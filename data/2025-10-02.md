<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 57]
- [cs.CV](#cs.CV) [Total: 59]
- [cs.AI](#cs.AI) [Total: 59]
- [cs.LG](#cs.LG) [Total: 59]
- [cs.NI](#cs.NI) [Total: 7]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning](https://arxiv.org/abs/2510.00125)
*Hong kyu Lee,Ruixuan Liu,Li Xiong*

Main category: cs.CL

TL;DR: 本文提出了一种名为直接令牌优化（DTO）的自包含方法，用于大型语言模型的机器遗忘，无需外部资源，显著提高了遗忘质量并保持了模型实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）遗忘方法依赖于外部资源（如辅助LLMs、保留数据集或商业AI服务），这不仅不切实际，还可能引入额外的隐私风险。核心挑战在于确保模型完全遗忘指定数据（遗忘集），同时不损害其整体实用性。

Method: 本文提出直接令牌优化（DTO），这是一种针对LLMs的自包含遗忘方法，通过直接优化令牌级别的目标来消除对外部资源的依赖。对于要遗忘的序列，方法识别两类令牌：目标令牌（用于优化遗忘目标）和非目标令牌（用于维持模型实用性）。

Result: 实验结果表明，DTO在多个基准数据集上的遗忘质量比最新基线提高了多达16.8倍，同时保持了可比的模型实用性水平。

Conclusion: DTO是一种有效且自包含的LLM遗忘方法，它在无需外部资源的情况下，显著提升了遗忘质量，并能有效维持模型整体性能。

Abstract: Machine unlearning is an emerging technique that removes the influence of a
subset of training data (forget set) from a model without full retraining, with
applications including privacy protection, content moderation, and model
correction. The key challenge lies in ensuring that the model completely
forgets the knowledge of the forget set without compromising its overall
utility. Existing unlearning methods for large language models (LLMs) often
utilize auxiliary language models, retain datasets, or even commercial AI
services for effective unlearning and maintaining the model utility. However,
dependence on these external resources is often impractical and could
potentially introduce additional privacy risks. In this work, we propose direct
token optimization (DTO), a novel self-contained unlearning approach for LLMs
that directly optimizes the token level objectives and eliminates the need for
external resources. Given a sequence to unlearn, we identify two categories of
tokens: target tokens, which capture critical knowledge for unlearning, and the
remaining non-target tokens, which are crucial for maintaining the model
utility. The former are used to optimize the unlearning objective, while the
latter serve to preserve the model's performance. The experimental results show
that the proposed DTO achieves up to 16.8$\times$ improvement in forget quality
on several benchmark datasets than the latest baselines while maintaining a
comparable level of model utility.

</details>


### [2] [TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding](https://arxiv.org/abs/2510.00161)
*Kimihiro Hasegawa,Wiradee Imrattanatrai,Masaki Asada,Ken Fukuda,Teruko Mitamura*

Main category: cs.CL

TL;DR: 本文提出TAMA框架，一个工具增强的多模态智能体，用于程序性活动理解，在无训练设置下利用多媒体返回工具，有效提升了视觉语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管程序性活动助手应用潜力巨大，但为其量身定制的系统开发仍未被充分探索。

Method: 提出TAMA（Tool-Augmented Multimodal Agent）框架，通过利用返回多媒体内容的工具，在无训练设置下实现交错多模态推理。

Result: 在多模态程序性问答数据集ProMQA-Assembly上的实验表明，TAMA能提高视觉语言模型（尤其是GPT-5和MiMo-VL）的性能。消融研究也证实了多媒体返回工具和智能体灵活工具选择的有效性。

Conclusion: 所提出的框架和实验结果促进了视频和多模态任务中“图像思考”范式的发展，并有助于程序性活动助手的开发。

Abstract: Procedural activity assistants potentially support humans in a variety of
settings, from our daily lives, e.g., cooking or assembling flat-pack
furniture, to professional situations, e.g., manufacturing or biological
experiments. Despite its potential use cases, the system development tailored
for such an assistant is still underexplored. In this paper, we propose a novel
framework, called TAMA, a Tool-Augmented Multimodal Agent, for procedural
activity understanding. TAMA enables interleaved multimodal reasoning by making
use of multimedia-returning tools in a training-free setting. Our experimental
result on the multimodal procedural QA dataset, ProMQA-Assembly, shows that our
approach can improve the performance of vision-language models, especially
GPT-5 and MiMo-VL. Furthermore, our ablation studies provide empirical support
for the effectiveness of two features that characterize our framework,
multimedia-returning tools and agentic flexible tool selection. We believe our
proposed framework and experimental results facilitate the thinking with images
paradigm for video and multimodal tasks, let alone the development of
procedural activity assistants.

</details>


### [3] [DRBench: A Realistic Benchmark for Enterprise Deep Research](https://arxiv.org/abs/2510.00172)
*Amirhossein Abaskohi,Tianyi Chen,Miguel Muñoz-Mármol,Curtis Fox,Amrutha Varshini Ramesh,Étienne Marcotte,Xing Han Lù,Nicolas Chapados,Spandana Gella,Christopher Pal,Alexandre Drouin,Issam H. Laradji*

Main category: cs.CL

TL;DR: DRBench是一个用于评估AI代理在企业环境中执行复杂、开放式深度研究任务的基准，它整合了公共网络和私有公司知识库。


<details>
  <summary>Details</summary>
Motivation: 现有基准侧重于简单问题或仅限网络的查询，无法有效评估AI代理在需要多步骤查询和整合公共/私有知识的复杂企业深度研究任务中的能力。

Method: 引入DRBench基准，包含15个跨10个领域的深度研究任务。这些任务通过人工验证的合成管线生成，基于真实用户角色和企业背景，覆盖异构搜索空间。代理的评估标准包括相关洞察召回、事实准确性和报告连贯性。研究评估了多种开放和闭源模型（如GPT、Llama、Qwen）及DR策略。

Result: 通过DRBench评估了多种DR代理，揭示了它们的优势、劣势，并指明了推进企业深度研究的关键路径，证明了DRBench的有效性。

Conclusion: DRBench为评估和推动AI代理在复杂企业深度研究任务中的发展提供了一个重要的工具和基准。

Abstract: We introduce DRBench, a benchmark for evaluating AI agents on complex,
open-ended deep research tasks in enterprise settings. Unlike prior benchmarks
that focus on simple questions or web-only queries, DRBench evaluates agents on
multi-step queries (for example, ``What changes should we make to our product
roadmap to ensure compliance with this standard?") that require identifying
supporting facts from both the public web and private company knowledge base.
Each task is grounded in realistic user personas and enterprise context,
spanning a heterogeneous search space that includes productivity software,
cloud file systems, emails, chat conversations, and the open web. Tasks are
generated through a carefully designed synthesis pipeline with
human-in-the-loop verification, and agents are evaluated on their ability to
recall relevant insights, maintain factual accuracy, and produce coherent,
well-structured reports. We release 15 deep research tasks across 10 domains,
such as Sales, Cybersecurity, and Compliance. We demonstrate the effectiveness
of DRBench by evaluating diverse DR agents across open- and closed-source
models (such as GPT, Llama, and Qwen) and DR strategies, highlighting their
strengths, weaknesses, and the critical path for advancing enterprise deep
research. Code is available at https://github.com/ServiceNow/drbench.

</details>


### [4] [PrimeX: A Dataset of Worldview, Opinion, and Explanation](https://arxiv.org/abs/2510.00174)
*Rik Koncel-Kedziorski,Brihi Joshi,Tim Paek*

Main category: cs.CL

TL;DR: 为提升语言模型对齐和意见预测，本研究构建了PrimeX数据集，该数据集结合了公共意见、书面信念解释和世界观调查，并初步证明了信念信息对个性化语言模型的价值。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型的发展，需要更好地表示个体用户。本研究旨在探讨个体信念系统能否被语言模型利用以改进其对齐能力。

Method: 开发了PrimeX数据集，包含了858名美国居民的公共意见调查数据，以及两种额外的信念信息来源：受访者对其特定意见的书面解释，以及用于评估其世界观的Primal World Belief调查。

Result: 对PrimeX数据进行了广泛的初步分析，结果表明信念解释和世界观对个性化语言模型具有重要价值。

Conclusion: PrimeX中额外的信念信息能惠及自然语言处理和心理学研究社区，并为未来的深入研究开辟了新途径。

Abstract: As the adoption of language models advances, so does the need to better
represent individual users to the model. Are there aspects of an individual's
belief system that a language model can utilize for improved alignment?
Following prior research, we investigate this question in the domain of opinion
prediction by developing PrimeX, a dataset of public opinion survey data from
858 US residents with two additional sources of belief information: written
explanations from the respondents for why they hold specific opinions, and the
Primal World Belief survey for assessing respondent worldview. We provide an
extensive initial analysis of our data and show the value of belief
explanations and worldview for personalizing language models. Our results
demonstrate how the additional belief information in PrimeX can benefit both
the NLP and psychological research communities, opening up avenues for further
study.

</details>


### [5] [Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail At It](https://arxiv.org/abs/2510.00177)
*Shuyue Stella Li,Avinandan Bose,Faeze Brahman,Simon Shaolei Du,Pang Wei Koh,Maryam Fazel,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: 当前LLM在个性化推理方面存在局限，无法有效根据用户偏好调整响应。本文提出PREFDISCO评估方法，通过角色模拟揭示LLM在冷启动场景下个性化能力不足，并指出个性化推理需专门开发而非自然涌现。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）将任务解决与偏好对齐视为独立挑战，导致在面向用户的应用中，即使答案正确也可能因不匹配用户需求而失败，尤其是在缺乏用户历史或隐私受限的冷启动场景中。LLMs需要识别、主动获取用户偏好并据此调整其推理和响应，即实现“个性化推理”。

Method: 引入了PREFDISCO评估方法论，将静态基准测试转化为交互式个性化任务。该方法通过使用基于心理学的、具有稀疏偏好的人物角色，创建了相同问题在不同用户上下文（个体专业知识和偏好）下需要不同推理链的场景，同时保持事实准确性。

Result: 对21个前沿模型在10项任务上的评估显示，29.0%的“天真”个性化尝试比通用响应的偏好对齐效果更差，而通用响应也未能有效满足个体用户需求。这些发现表明个性化推理需要专门的开发，而非自然而然地出现。

Conclusion: PREFDISCO将个性化推理确立为一个可衡量的研究前沿，并揭示了当前LLM交互能力的基本局限性。它为开发在教育、医疗和技术等个性化至关重要的领域中能够适应个体用户的系统奠定了基础。

Abstract: Current large language model (LLM) development treats task-solving and
preference alignment as separate challenges, optimizing first for objective
correctness, then for alignment to aggregated human preferences. This paradigm
fails in human-facing applications where solving a problem correctly is
insufficient if the response mismatches the user's needs. This challenge
intensifies in just-in-time scenarios where no prior user interaction history
exists due to cold-start conditions or privacy constraints. LLMs need to
identify what they don't know about user preferences, strategically elicit
preference values through questioning, then adapt their reasoning processes and
responses accordingly -- a complicated chain of cognitive processes which we
term personalized reasoning. We introduce PREFDISCO, an evaluation methodology
that transforms static benchmarks into interactive personalization tasks using
psychologically-grounded personas with sparse preferences. Our framework
creates scenarios where identical questions require different reasoning chains
depending on user context, as optimal explanation approaches vary by individual
expertise and preferences while maintaining factual accuracy. Evaluation of 21
frontier models across 10 tasks reveals 29.0% of naive personalization attempts
produce worse preference alignment than generic responses, yet generic
responses also fail to serve individual user needs effectively. These findings
suggest personalized reasoning requires dedicated development rather than
emerging naturally. PREFDISCO establishes personalized reasoning as a
measurable research frontier and reveals fundamental limitations in current
LLMs' interactive capabilities, providing a foundation for developing systems
that can adapt to individual users in education, healthcare, and technical
domains where personalization is critical.

</details>


### [6] [BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses](https://arxiv.org/abs/2510.00232)
*Xin Xu,Xunzhi He,Churan Zhi,Ruizhe Chen,Julian McAuley,Zexue He*

Main category: cs.CL

TL;DR: 该论文提出了BiasFreeBench，一个统一的经验基准测试平台和响应级指标，旨在解决现有LLM偏见缓解方法评估不一致且脱离实际用户交互的痛点。


<details>
  <summary>Details</summary>
Motivation: 现有LLM偏见缓解方法的评估基线和指标多样，导致比较不一致；且多基于LLM的上下文概率，未能反映用户在实际使用中对公平、安全响应的需求。

Method: 引入BiasFreeBench，一个经验基准，通过重组现有数据集为统一的查询-响应设置，在多项选择问答和开放式多轮问答两种场景下，全面比较八种主流偏见缓解技术（四种提示工程方法和四种训练方法）。同时，提出响应级指标Bias-Free Score，用于衡量LLM响应的公平性、安全性及反刻板印象程度。

Result: 系统比较并分析了偏见缓解性能在提示工程与训练范式、模型大小以及不同训练策略对未见偏见类型的泛化能力等关键维度上的表现。

Conclusion: BiasFreeBench旨在为偏见缓解研究建立一个统一的测试平台，以实现方法间的一致评估，并弥合评估与真实世界LLM使用之间的鸿沟。该基准将公开发布。

Abstract: Existing studies on bias mitigation methods for large language models (LLMs)
use diverse baselines and metrics to evaluate debiasing performance, leading to
inconsistent comparisons among them. Moreover, their evaluations are mostly
based on the comparison between LLMs' probabilities of biased and unbiased
contexts, which ignores the gap between such evaluations and real-world use
cases where users interact with LLMs by reading model responses and expect fair
and safe outputs rather than LLMs' probabilities. To enable consistent
evaluation across debiasing methods and bridge this gap, we introduce
BiasFreeBench, an empirical benchmark that comprehensively compares eight
mainstream bias mitigation techniques (covering four prompting-based and four
training-based methods) on two test scenarios (multi-choice QA and open-ended
multi-turn QA) by reorganizing existing datasets into a unified query-response
setting. We further introduce a response-level metric, Bias-Free Score, to
measure the extent to which LLM responses are fair, safe, and
anti-stereotypical. Debiasing performances are systematically compared and
analyzed across key dimensions: the prompting vs. training paradigm, model
size, and generalization of different training strategies to unseen bias types.
We will publicly release our benchmark, aiming to establish a unified testbed
for bias mitigation research.

</details>


### [7] [TASER: Translation Assessment via Systematic Evaluation and Reasoning](https://arxiv.org/abs/2510.00255)
*Monishwaran Maheswaran,Marco Carini,Christian Federmann,Tony Diaz*

Main category: cs.CL

TL;DR: 本文提出TASER，一种利用大型推理模型（LRMs）进行自动化翻译质量评估的新指标。TASER在WMT24任务中实现了最先进的性能，在系统级评估中超越了现有指标，并在片段级评估中表现出色，同时提供了更好的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化翻译质量评估指标缺乏可解释性和可见性，需要一种更准确且透明的评估方法。

Method: 引入TASER，该指标利用LRMs的显式推理能力进行系统、分步的翻译质量评估。在WMT24 Metrics共享任务的基于参考和无参考场景中进行评估。研究发现结构化提示模板对LRMs效果更佳，并使用OpenAI的o3模型探索了推理深度与评估质量的关系。

Result: 在系统级评估中，TASER在有参考和无参考设置中均达到了最高的软成对准确率，超越了所有现有指标。在片段级评估中，其无参考变体在所有无参考方法中表现最佳。实验表明结构化提示模板对LRMs能产生更优结果。研究还提供了推理深度与评估质量之间关系的见解。

Conclusion: 大型推理模型在翻译质量评估方面取得了显著进展，TASER结合了更高的准确性和跨不同语言对的透明评估。LRMs的显式推理过程提供了可解释性和可见性，有效解决了现有自动化评估指标的关键局限性。

Abstract: We introduce TASER (Translation Assessment via Systematic Evaluation and
Reasoning), a metric that uses Large Reasoning Models (LRMs) for automated
translation quality assessment. TASER harnesses the explicit reasoning
capabilities of LRMs to conduct systematic, step-by-step evaluation of
translation quality. We evaluate TASER on the WMT24 Metrics Shared Task across
both reference-based and reference-free scenarios, demonstrating
state-of-the-art performance. In system-level evaluation, TASER achieves the
highest soft pairwise accuracy in both reference-based and reference-free
settings, outperforming all existing metrics. At the segment level, TASER
maintains competitive performance with our reference-free variant ranking as
the top-performing metric among all reference-free approaches. Our experiments
reveal that structured prompting templates yield superior results with LRMs
compared to the open-ended approaches that proved optimal for traditional LLMs.
We evaluate o3, a large reasoning model from OpenAI, with varying reasoning
efforts, providing insights into the relationship between reasoning depth and
evaluation quality. The explicit reasoning process in LRMs offers
interpretability and visibility, addressing a key limitation of existing
automated metrics. Our results demonstrate that Large Reasoning Models show a
measurable advancement in translation quality assessment, combining improved
accuracy with transparent evaluation across diverse language pairs.

</details>


### [8] [Retrieval-Augmented Generation for Electrocardiogram-Language Models](https://arxiv.org/abs/2510.00261)
*Xiaoyu Song,William Han,Tony Chen,Chaojing Duan,Michael A. Rosenberg,Emerson Liu,Ding Zhao*

Main category: cs.CL

TL;DR: 本文首次提出了ELM的开源检索增强生成（RAG）管道，并实验证明其能显著提高ELM的性能，填补了该领域空白。


<details>
  <summary>Details</summary>
Motivation: 生成式心电图-语言模型（ELMs）在处理心电图信号和文本查询方面具有多功能性。然而，虽然检索增强生成（RAG）在大型语言模型（LLMs）中已被证明能有效减少幻觉并改善自然语言生成（NLG），但目前尚无针对ELMs的开源RAG实现或系统研究。

Method: 为解决这一空白，研究者提出了首个针对ELMs的开源RAG管道。通过基线模型和消融研究，在三个公共数据集上进行了自然语言生成（NLG）实验。

Result: 实验结果表明，结合RAG的ELMs在性能上持续优于非RAG基线模型，并且突出了ELM设计的关键考量因素。

Conclusion: RAG能显著提升ELMs的性能，为ELMs提供了一个有效的解决方案，并为该领域的未来研究提供了开源工具和设计参考。

Abstract: Interest in generative Electrocardiogram-Language Models (ELMs) is growing,
as they can produce textual responses conditioned on ECG signals and textual
queries. Unlike traditional classifiers that output label probabilities, ELMs
are more versatile, supporting domain-specific tasks (e.g., waveform analysis,
diagnosis, prognosis) as well as general tasks (e.g., open-ended questions,
dialogue). Retrieval-Augmented Generation (RAG), widely used in Large Language
Models (LLMs) to ground LLM outputs in retrieved knowledge, helps reduce
hallucinations and improve natural language generation (NLG). However, despite
its promise, no open-source implementation or systematic study of RAG pipeline
design for ELMs currently exists. To address this gap, we present the first
open-source RAG pipeline for ELMs, along with baselines and ablation studies
for NLG. Experiments on three public datasets show that ELMs with RAG
consistently improves performance over non-RAG baselines and highlights key ELM
design considerations. Our code is available at:
https://github.com/willxxy/ECG-Bench.

</details>


### [9] [Judging with Confidence: Calibrating Autoraters to Preference Distributions](https://arxiv.org/abs/2510.00263)
*Zhuohang Li,Xiaowei Li,Chengyu Huang,Guowang Li,Katayoon Goshvadi,Bo Dai,Dale Schuurmans,Paul Zhou,Hamid Palangi,Yiwen Song,Palash Goyal,Murat Kantarcioglu,Bradley A. Malin,Yuan Xue*

Main category: cs.CL

TL;DR: 本文提出一个通用框架，通过校准概率自动评估器来建模人类偏好分布，以提高大语言模型自动评估器的可靠性，解决了现有评估器依赖离散标签导致的主观性问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）自动评估器因依赖离散偏好标签进行训练，在处理主观任务时可靠性受限。为了构建可靠的自动评估器，需要其能够建模目标人群的完整偏好分布。

Method: 提出了一个校准概率自动评估器以匹配任何给定偏好分布的通用框架。针对不同数据条件，提供了两种学习方法：1) 针对密集、概率性标签的直接监督微调；2) 针对稀疏、二元标签的强化学习方法。

Result: 通过分布匹配目标进行微调的自动评估器，其口头化概率预测与目标偏好分布更一致，校准性得到改善，位置偏差显著降低，同时在客观任务上保持了性能。

Conclusion: 通过校准概率自动评估器来建模完整的偏好分布，可以显著提高LLM评估器的可靠性、校准性和减少偏差，且不影响其在客观任务上的表现。

Abstract: The alignment of large language models (LLMs) with human values increasingly
relies on using other LLMs as automated judges, or ``autoraters''. However,
their reliability is limited by a foundational issue: they are trained on
discrete preference labels, forcing a single ground truth onto tasks that are
often subjective, ambiguous, or nuanced. We argue that a reliable autorater
must learn to model the full distribution of preferences defined by a target
population. In this paper, we propose a general framework for calibrating
probabilistic autoraters to any given preference distribution. We formalize the
problem and present two learning methods tailored to different data conditions:
1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a
reinforcement learning approach for sparse, binary labels. Our empirical
results show that finetuning autoraters with a distribution-matching objective
leads to verbalized probability predictions that are better aligned with the
target preference distribution, with improved calibration and significantly
lower positional bias, all while preserving performance on objective tasks.

</details>


### [10] [Efficient Layer-wise LLM Fine-tuning for Revision Intention Prediction](https://arxiv.org/abs/2510.00268)
*Zhexiong Liu,Diane Litman*

Main category: cs.CL

TL;DR: 针对LLMs在细微文本分类（如文本修订）中数据稀缺的挑战，本文提出IR-Tuning，一种层级PEFT框架，通过动态微调重要层，在小型数据集上实现了优越性能、快速收敛和低内存消耗。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在文本生成上表现卓越，但其在细微文本分类（如文本修订）方面的潜力尚未充分开发，因预训练侧重生成。指令微调虽能将分类转为生成任务，但难以处理细微文本。直接微调LLMs需要大量且稀缺昂贵的修订标注数据，这使其不切实际。

Method: 引入IR-Tuning，一种即插即用的层级参数高效微调（PEFT）框架。该方法基于梯度范数分布动态选择并微调LLM中“重要”的子层，同时冻结“冗余”层。

Result: 广泛实验表明，IR-Tuning在多样化的文本修订任务上超越了多种层级PEFT基线，同时实现了快速收敛、低GPU内存消耗，并在小型修订语料库上表现出高效性。

Conclusion: IR-Tuning通过创新性地动态选择并微调LLM层，有效解决了数据稀缺问题，显著提升了LLMs在复杂文本修订分类上的性能，同时优化了训练效率和资源消耗。

Abstract: Large Language Models (LLMs) have shown extraordinary success across various
text generation tasks; however, their potential for simple yet essential text
classification remains underexplored, as LLM pre-training tends to emphasize
generation over classification. While LLMs with instruction tuning can
transform classification into a generation task, they often struggle to
categorize nuanced texts. One such example is text revision, which involves
nuanced edits between pairs of texts. Although simply fine-tuning LLMs for
revision classification seems plausible, it requires a large amount of revision
annotations, which are exceptionally expensive and scarce in the community. To
address this issue, we introduce a plug-and-play layer-wise parameter-efficient
fine-tuning (PEFT) framework, i.e., IR-Tuning, which fine-tunes a subset of
important LLM layers that are dynamically selected based on their gradient norm
distribution, while freezing those of redundant layers. Extensive experiments
suggest that IR-Tuning surpasses several layer-wise PEFT baselines over diverse
text revisions, while achieving fast convergence, low GPU memory consumption,
and effectiveness on small revision corpora.

</details>


### [11] [SafePassage: High-Fidelity Information Extraction with Black Box LLMs](https://arxiv.org/abs/2510.00276)
*Joe Barrow,Raj Patel,Misha Kharkovski,Ben Davies,Ryan Schmitt*

Main category: cs.CL

TL;DR: 本文提出了SafePassage，一个三步走的LLM信息抽取管道，通过生成“安全通道”（文档中接地且与抽取信息一致的上下文）来显著减少LLM信息抽取中的幻觉，并发现一个小型微调编码器在识别不安全通道上表现优于LLM评分模型。


<details>
  <summary>Details</summary>
Motivation: 黑盒大型语言模型 (LLMs) 虽然使信息抽取 (IE) 配置变得容易，但其抽取的信息不保证基于文档，难以信任，存在幻觉问题。

Method: 引入“安全通道”的概念：由LLM生成，既基于文档又与抽取信息一致的上下文。该方法通过SafePassage三步管道实现：1) 一个LLM抽取器，生成结构化实体及其上下文；2) 一个基于字符串的全局对齐器；3) 一个评分模型。该管道还可用于评估LLMs。

Result: 结果表明，结合这三部分可将信息抽取任务中的幻觉减少高达85%，且误报非幻觉的风险极低。SafePassage管道与人类对抽取质量的判断高度一致，可用于评估LLMs。此外，仅用少量任务特定示例微调的Transformer编码器在标记不安全通道方面表现优于LLM评分模型，所需标注时间仅为1-2小时。

Conclusion: SafePassage提供了一个有效的方法来减少LLM信息抽取中的幻觉，显著提高了抽取的可信度。同时，它也作为一个评估LLM抽取质量的工具，并揭示了小型微调模型在特定子任务上的强大潜力。

Abstract: Black box large language models (LLMs) make information extraction (IE) easy
to configure, but hard to trust. Unlike traditional information extraction
pipelines, the information "extracted" is not guaranteed to be grounded in the
document. To prevent this, this paper introduces the notion of a "safe
passage": context generated by the LLM that is both grounded in the document
and consistent with the extracted information. This is operationalized via a
three-step pipeline, SafePassage, which consists of: (1) an LLM extractor that
generates structured entities and their contexts from a document, (2) a
string-based global aligner, and (3) a scoring model. Results show that using
these three parts in conjunction reduces hallucinations by up to 85% on
information extraction tasks with minimal risk of flagging non-hallucinations.
High agreement between the SafePassage pipeline and human judgments of
extraction quality mean that the pipeline can be dually used to evaluate LLMs.
Surprisingly, results also show that using a transformer encoder fine-tuned on
a small number of task-specific examples can outperform an LLM scoring model at
flagging unsafe passages. These annotations can be collected in as little as
1-2 hours.

</details>


### [12] [ReEvalMed: Rethinking Medical Report Evaluation by Aligning Metrics with Real-World Clinical Judgment](https://arxiv.org/abs/2510.00280)
*Ruochen Li,Jun Li,Bailiang Jian,Kun Yuan,Youxiang Zhu*

Main category: cs.CL

TL;DR: 现有自动放射报告评估指标不被临床信任，本文提出了一个临床导向的元评估框架，揭示了现有指标的缺陷，并为开发更可靠的评估方法提供指导。


<details>
  <summary>Details</summary>
Motivation: 自动生成的放射报告在现有评估指标中得分很高，但未能获得临床医生的信任。这表明当前指标在评估报告质量方面存在根本性缺陷。

Method: 提出了一个临床导向的元评估框架，定义了涵盖临床一致性和关键指标能力（如区分度、鲁棒性、单调性）的标准。利用一个包含真实报告和重写报告对的细粒度数据集，并标注了错误类型、临床显著性标签和解释，系统性地评估了现有指标。

Result: 揭示了现有指标在解释临床语义方面的局局限性，例如无法区分临床显著错误、过度惩罚无害变异以及在不同错误严重程度下缺乏一致性。

Conclusion: 所提出的框架为构建更具临床可靠性的评估方法提供了指导。

Abstract: Automatically generated radiology reports often receive high scores from
existing evaluation metrics but fail to earn clinicians' trust. This gap
reveals fundamental flaws in how current metrics assess the quality of
generated reports. We rethink the design and evaluation of these metrics and
propose a clinically grounded Meta-Evaluation framework. We define clinically
grounded criteria spanning clinical alignment and key metric capabilities,
including discrimination, robustness, and monotonicity. Using a fine-grained
dataset of ground truth and rewritten report pairs annotated with error types,
clinical significance labels, and explanations, we systematically evaluate
existing metrics and reveal their limitations in interpreting clinical
semantics, such as failing to distinguish clinically significant errors,
over-penalizing harmless variations, and lacking consistency across error
severity levels. Our framework offers guidance for building more clinically
reliable evaluation methods.

</details>


### [13] [o-MEGA: Optimized Methods for Explanation Generation and Analysis](https://arxiv.org/abs/2510.00288)
*Ľuboš Kriš,Jaroslav Kopčan,Qiwei Peng,Andrej Ridzik,Marcel Veselý,Martin Tamajka*

Main category: cs.CL

TL;DR: 本文提出`o-mega`，一个超参数优化工具，旨在自动选择语义匹配领域中最有效的可解释AI方法及其配置。该工具能提高自动化事实核查系统的透明度，增强索赔匹配模型在错误信息检测等关键应用中的可解释性，从而构建更值得信赖和透明的AI系统。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer语言模型的普及，NLP领域面临模型透明度和可信度挑战。研究人员虽开发了大量解释方法，但选择最优方法仍是难题。

Method: 开发了`o-mega`，一个超参数优化工具，用于在语义匹配领域自动识别最有效的可解释AI方法及其配置。该工具通过系统探索不同的可解释方法及其超参数，并在一个使用社交媒体帖子和反驳声明的数据集上，针对后索赔匹配管道进行了评估。

Result: `o-mega`系统性地探索了不同的可解释方法和超参数，成功提升了自动化事实核查系统的透明度。

Conclusion: 解释方法的自动化优化显著增强了索赔匹配模型在错误信息检测等关键应用中的可解释性，有助于构建更值得信赖和透明的AI系统。

Abstract: The proliferation of transformer-based language models has revolutionized NLP
domain while simultaneously introduced significant challenges regarding model
transparency and trustworthiness. The complexity of achieving explainable
systems in this domain is evidenced by the extensive array of explanation
methods and evaluation metrics developed by researchers. To address the
challenge of selecting optimal explainability approaches, we present
\textbf{\texttt{o-mega}}, a hyperparameter optimization tool designed to
automatically identify the most effective explainable AI methods and their
configurations within the semantic matching domain. We evaluate o-mega on a
post-claim matching pipeline using a curated dataset of social media posts
paired with refuting claims. Our tool systematically explores different
explainable methods and their hyperparameters, demonstrating improved
transparency in automated fact-checking systems. As a result, such automated
optimization of explanation methods can significantly enhance the
interpretability of claim-matching models in critical applications such as
misinformation detection, contributing to more trustworthy and transparent AI
systems.

</details>


### [14] [CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage](https://arxiv.org/abs/2510.00311)
*Bowen Wei,Yuan Shen Tay,Howard Liu,Jinhao Pan,Kun Luo,Ziwei Zhu,Chris Jordan*

Main category: cs.CL

TL;DR: 本文提出了CORTEX，一个多智能体LLM架构，旨在解决安全运营中心（SOC）的警报过载问题，通过专业智能体协作处理真实证据，显著减少误报并提高调查质量。


<details>
  <summary>Details</summary>
Motivation: 安全运营中心（SOC）面临海量日常警报，其中大部分并非真正的攻击，导致警报疲劳、威胁遗漏和分析师倦怠。传统的检测管道脆弱且缺乏上下文，而现有的单智能体LLM方法在处理嘈杂的企业数据时表现不佳且透明度有限。

Method: 本文提出了CORTEX，一个用于高风险警报分类的多智能体LLM架构。CORTEX中的专业智能体协作处理真实证据：行为分析智能体检查活动序列，证据收集智能体查询外部系统，推理智能体将发现结果综合为可审计的决策。此外，为了支持训练和评估，作者发布了一个包含生产环境中细粒度SOC调查的数据集，该数据集记录了分析师的逐步操作和关联的工具输出。

Result: CORTEX在各种企业场景中，与最先进的单智能体LLM相比，显著减少了误报，并提高了调查质量。

Conclusion: CORTEX的多智能体LLM架构通过其协作和专业化的代理方法，有效解决了SOC警报过载和低效分类的挑战，提高了警报处理的准确性和效率，并提供了更好的透明度，优于现有的单智能体LLM解决方案。

Abstract: Security Operations Centers (SOCs) are overwhelmed by tens of thousands of
daily alerts, with only a small fraction corresponding to genuine attacks. This
overload creates alert fatigue, leading to overlooked threats and analyst
burnout. Classical detection pipelines are brittle and context-poor, while
recent LLM-based approaches typically rely on a single model to interpret logs,
retrieve context, and adjudicate alerts end-to-end -- an approach that
struggles with noisy enterprise data and offers limited transparency. We
propose CORTEX, a multi-agent LLM architecture for high-stakes alert triage in
which specialized agents collaborate over real evidence: a behavior-analysis
agent inspects activity sequences, evidence-gathering agents query external
systems, and a reasoning agent synthesizes findings into an auditable decision.
To support training and evaluation, we release a dataset of fine-grained SOC
investigations from production environments, capturing step-by-step analyst
actions and linked tool outputs. Across diverse enterprise scenarios, CORTEX
substantially reduces false positives and improves investigation quality over
state-of-the-art single-agent LLMs.

</details>


### [15] [TokMem: Tokenized Procedural Memory for Large Language Models](https://arxiv.org/abs/2510.00444)
*Zijun Wu,Yongchang Hao,Lili Mou*

Main category: cs.CL

TL;DR: TokMem是一种新颖的令牌化程序记忆，通过紧凑、可训练的嵌入来存储重复过程，解决了大型语言模型提示工程的低效性，并实现了可扩展、模块化的任务处理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）过度依赖提示来指定任务、召回知识和指导推理，但这导致效率低下（每次都需要重新读取、跨任务扩展性差、缺乏模块化重用机制）。

Method: 引入TokMem，一种令牌化的程序记忆，将重复过程存储为紧凑、可训练的嵌入。每个记忆令牌编码程序地址和控制信号，以恒定大小的开销实现目标行为。TokMem冻结骨干模型，支持持续适应，允许在不干扰现有过程的情况下添加新过程。

Result: 在1,000个原子召回任务和函数调用（组合召回）任务上进行评估，TokMem始终优于检索增强生成（避免重复上下文开销），并以远少参数优于微调。

Conclusion: TokMem是提示工程和微调的一种可扩展、模块化的替代方案，为LLMs提供了显式的程序记忆。

Abstract: Large language models rely heavily on prompts to specify tasks, recall
knowledge and guide reasoning. However, this reliance is inefficient as prompts
must be re-read at each step, scale poorly across tasks, and lack mechanisms
for modular reuse. We introduce TokMem, a tokenized procedural memory that
stores recurring procedures as compact, trainable embeddings. Each memory token
encodes both an address to a procedure and a control signal that steers
generation, enabling targeted behavior with constant-size overhead. To support
continual adaptation, TokMem keeps the backbone model frozen, allowing new
procedures to be added without interfering with existing ones. We evaluate
TokMem on 1,000 tasks for atomic recall, and on function-calling tasks for
compositional recall, where it consistently outperforms retrieval-augmented
generation while avoiding repeated context overhead, and fine-tuning with far
fewer parameters. These results establish TokMem as a scalable and modular
alternative to prompt engineering and fine-tuning, offering an explicit
procedural memory for LLMs.

</details>


### [16] [LongCodeZip: Compress Long Context for Code Language Models](https://arxiv.org/abs/2510.00446)
*Yuling Shi,Yichun Qian,Hongyu Zhang,Beijun Shen,Xiaodong Gu*

Main category: cs.CL

TL;DR: LongCodeZip是一种为代码大型语言模型（LLMs）设计的即插即用代码压缩框架，通过双阶段策略显著压缩代码上下文，在不降低性能的情况下，提高了代码智能应用的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）处理代码库中大量信息的需求日益增长，长上下文下的代码生成变得越来越重要。然而，高昂的API成本和生成延迟是主要瓶颈。现有上下文剪枝技术（如LLMLingua）虽然在通用文本上表现良好，但忽视了代码特有的结构和依赖性，导致在编程任务中性能不佳。

Method: 本文提出了LongCodeZip，一个新颖的即插即用代码压缩框架。它采用双阶段策略：1) 粗粒度压缩，利用条件困惑度识别和排序函数级代码块，仅保留最相关的函数；2) 细粒度压缩，将保留的函数基于困惑度分割成代码块，并在自适应令牌预算下选择最优子集以最大化相关性。

Result: LongCodeZip在代码补全、摘要和问答等多个任务上进行了评估，结果显示其始终优于基线方法，在不降低任务性能的情况下实现了高达5.6倍的压缩比。

Conclusion: LongCodeZip通过有效减少上下文大小同时保留关键信息，使得LLMs能更好地应用于真实世界的大规模代码场景，从而提高了代码智能应用的效率和能力。

Abstract: Code generation under long contexts is becoming increasingly critical as
Large Language Models (LLMs) are required to reason over extensive information
in the codebase. While recent advances enable code LLMs to process long inputs,
high API costs and generation latency remain substantial bottlenecks. Existing
context pruning techniques, such as LLMLingua, achieve promising results for
general text but overlook code-specific structures and dependencies, leading to
suboptimal performance in programming tasks. In this paper, we propose
LongCodeZip, a novel plug-and-play code compression framework designed
specifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1)
coarse-grained compression, which identifies and ranks function-level chunks
using conditional perplexity with respect to the instruction, retaining only
the most relevant functions; and (2) fine-grained compression, which segments
retained functions into blocks based on perplexity and selects an optimal
subset under an adaptive token budget to maximize relevance. Evaluations across
multiple tasks, including code completion, summarization, and question
answering, show that LongCodeZip consistently outperforms baseline methods,
achieving up to a 5.6x compression ratio without degrading task performance. By
effectively reducing context size while preserving essential information,
LongCodeZip enables LLMs to better scale to real-world, large-scale code
scenarios, advancing the efficiency and capability of code intelligence
applications.

</details>


### [17] [Enhancing Rating Prediction with Off-the-Shelf LLMs Using In-Context User Reviews](https://arxiv.org/abs/2510.00449)
*Koki Ryu,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 本研究发现，即用型大型语言模型（LLMs）结合用户评论能有效预测Likert量表评分，其性能媲美传统方法，并有望解决冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLMs的分类或排序任务，而忽略了具有重要工业应用价值但未被充分探索的Likert量表评分预测（一种需要语言和数学推理的回归任务），特别是即用型LLMs在此任务上的能力尚未被充分利用。

Method: 本研究通过在不同上下文信息下，对八个即用型LLM在三个数据集上进行全面实验，以评估其在Likert量表评分预测任务上的性能。

Result: 实验表明，用户撰写的评论能显著提高LLMs的评分预测性能，并可与矩阵分解等传统方法媲美，展现了其解决冷启动问题的潜力。此外，具体物品的评论比通用偏好描述更有效，且提示LLM首先生成假设性评论能进一步提升预测性能。

Conclusion: LLMs，特别是结合用户评论时，在评分预测方面展现出巨大潜力，是解决冷启动问题的一种有前景的方案。

Abstract: Personalizing the outputs of large language models (LLMs) to align with
individual user preferences is an active research area. However, previous
studies have mainly focused on classification or ranking tasks and have not
considered Likert-scale rating prediction, a regression task that requires both
language and mathematical reasoning to be solved effectively. This task has
significant industrial applications, but the utilization of LLMs remains
underexplored, particularly regarding the capabilities of off-the-shelf LLMs.
This study investigates the performance of off-the-shelf LLMs on rating
prediction, providing different in-context information. Through comprehensive
experiments with eight models across three datasets, we demonstrate that
user-written reviews significantly improve the rating prediction performance of
LLMs. This result is comparable to traditional methods like matrix
factorization, highlighting the potential of LLMs as a promising solution for
the cold-start problem. We also find that the reviews for concrete items are
more effective than general preference descriptions that are not based on any
specific item. Furthermore, we discover that prompting LLMs to first generate a
hypothetical review enhances the rating prediction performance. Our code is
available at https://github.com/ynklab/rating-prediction-with-reviews.

</details>


### [18] [Agent Fine-tuning through Distillation for Domain-specific LLMs in Microdomains](https://arxiv.org/abs/2510.00482)
*Yawen Xue,Masaya Tsunokake,Yuta Koreeda,Ekant Muljibhai Amin,Takashi Sumiyoshi,Yasuhiro Sogawa*

Main category: cs.CL

TL;DR: 本文通过代理微调，在日立JP1技术微领域中实现领域适应，结合检索增强生成和信息提取器，相对基线模型性能提升14%，展现了代理微调在复杂专业微领域推理的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有代理LLM的上下文学习方法导致输入冗长和计算成本高。同时，代理微调在通用领域有效，但在专门技术微领域（如IT运维）中的效果未知。

Method: 采用代理微调LLM，使用来自领域手册的JP1特定数据集和LLM自生成的推理轨迹。推理时结合检索增强生成（RAG）的代理提示和上下文-答案提取器。

Result: 在JP1认证考试问题上，本方法比基线模型性能提升了14%。

Conclusion: 代理微调在复杂技术微领域（如IT运维）的领域特定推理中具有巨大潜力。

Abstract: Agentic large language models (LLMs) have become prominent for autonomously
interacting with external environments and performing multi-step reasoning
tasks. Most approaches leverage these capabilities via in-context learning with
few-shot prompts, but this often results in lengthy inputs and higher
computational costs. Agent fine-tuning offers an alternative by enabling LLMs
to internalize procedural reasoning and domain-specific knowledge through
training on relevant data and demonstration trajectories. While prior studies
have focused on general domains, their effectiveness in specialized technical
microdomains remains unclear. This paper explores agent fine-tuning for domain
adaptation within Hitachi's JP1 middleware, a microdomain for specialized IT
operations. We fine-tuned LLMs using JP1-specific datasets derived from domain
manuals and distilled reasoning trajectories generated by LLMs themselves,
enhancing decision making accuracy and search efficiency. During inference, we
used an agentic prompt with retrieval-augmented generation and introduced a
context-answer extractor to improve information relevance. On JP1 certification
exam questions, our method achieved a 14% performance improvement over the base
model, demonstrating the potential of agent fine-tuning for domain-specific
reasoning in complex microdomains.

</details>


### [19] [Agent-ScanKit: Unraveling Memory and Reasoning of Multimodal Agents via Sensitivity Perturbations](https://arxiv.org/abs/2510.00496)
*Pengzhou Cheng,Lingzhong Dong,Zeng Wu,Zongru Wu,Xiangru Tang,Chengwei Qin,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: 本文提出Agent-ScanKit框架，系统性探究多模态Agent在GUI任务中的记忆和推理能力，发现现有Agent主要依赖机械记忆而非系统推理，泛化能力有限，凸显了加强推理建模的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管GUI多模态Agent的交互能力有所提升，但在复杂或域外任务中可靠性有限，这引发了一个核心问题：现有Agent的推理是否是虚假的？需要探明其记忆与推理的真实能力。

Method: 提出Agent-ScanKit系统性探测框架，引入视觉引导、文本引导和结构引导三种正交探测范式。这些范式旨在受控扰动下量化记忆和推理的贡献，且无需访问模型内部。

Result: 在五个公开GUI基准和18个多模态Agent上的实验结果表明，机械记忆的贡献通常超过系统推理。大多数模型主要作为训练知识的检索器，泛化能力有限。

Conclusion: 研究结果强调了在真实世界场景中为多模态Agent建立强大推理模型的必要性，为开发更可靠的多模态Agent提供了宝贵见解。

Abstract: Although numerous strategies have recently been proposed to enhance the
autonomous interaction capabilities of multimodal agents in graphical user
interface (GUI), their reliability remains limited when faced with complex or
out-of-domain tasks. This raises a fundamental question: Are existing
multimodal agents reasoning spuriously? In this paper, we propose
\textbf{Agent-ScanKit}, a systematic probing framework to unravel the memory
and reasoning capabilities of multimodal agents under controlled perturbations.
Specifically, we introduce three orthogonal probing paradigms: visual-guided,
text-guided, and structure-guided, each designed to quantify the contributions
of memorization and reasoning without requiring access to model internals. In
five publicly available GUI benchmarks involving 18 multimodal agents, the
results demonstrate that mechanical memorization often outweighs systematic
reasoning. Most of the models function predominantly as retrievers of
training-aligned knowledge, exhibiting limited generalization. Our findings
underscore the necessity of robust reasoning modeling for multimodal agents in
real-world scenarios, offering valuable insights toward the development of
reliable multimodal agents.

</details>


### [20] [MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance](https://arxiv.org/abs/2510.00499)
*Xingjian Zhao,Zhe Xu,Luozhijie Jin,Yang Wang,Hanfu Chen,Yaozhou Jiang,Ke Chen,Ruixiao Li,Mingshu Chen,Ruiming Wang,Wenbo Zhang,Yiyang Zhang,Donghua Yu,Yang Gao,Xiaogui Yang,Yitian Gong,Yuanfan Xu,Qinyuan Cheng,Zhaoye Fei,Shimin Li,Yaqian Zhou,Xuanjing Huang,Xipeng Qiu*

Main category: cs.CL

TL;DR: MOSS-Speech是一个无需文本中间件的语音到语音大语言模型，在口语问答中实现最先进性能，并能提供与现有文本引导系统相当的语音到语音交互。


<details>
  <summary>Details</summary>
Motivation: 现有语音对话系统依赖于级联管道或文本中间件，导致丢失副语言线索、限制表达力并形成基本瓶颈。

Method: 提出MOSS-Speech，一个直接理解和生成语音而无需文本指导的语音到语音大语言模型。该方法结合了基于模态的层分离架构和冻结预训练策略，以保留预训练文本LLM的推理和知识，同时增加原生语音能力。

Result: 实验表明，MOSS-Speech在口语问答中达到了最先进的结果，其语音到语音性能与现有文本引导系统相当，同时仍保持有竞争力的文本性能。

Conclusion: 该工作通过缩小文本引导和直接语音生成之间的差距，为富有表现力且高效的端到端语音交互建立了新范式。

Abstract: Spoken dialogue systems often rely on cascaded pipelines that transcribe,
process, and resynthesize speech. While effective, this design discards
paralinguistic cues and limits expressivity. Recent end-to-end methods reduce
latency and better preserve these cues, yet still rely on text intermediates,
creating a fundamental bottleneck. We present MOSS-Speech, a true
speech-to-speech large language model that directly understands and generates
speech without relying on text guidance. Our approach combines a modality-based
layer-splitting architecture with a frozen pre-training strategy, preserving
the reasoning and knowledge of pretrained text LLMs while adding native speech
capabilities. Experiments show that our model achieves state-of-the-art results
in spoken question answering and delivers comparable speech-to-speech
performance relative to existing text-guided systems, while still maintaining
competitive text performance. By narrowing the gap between text-guided and
direct speech generation, our work establishes a new paradigm for expressive
and efficient end-to-end speech interaction.

</details>


### [21] [Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs](https://arxiv.org/abs/2510.00507)
*Yurun Chen,Xavier Hu,Yuhan Liu,Ziqi Wang,Zeyi Liao,Lin Chen,Feng Wei,Yuxi Qian,Bo Zheng,Keting Yin,Shengyu Zhang*

Main category: cs.CL

TL;DR: Graph2Eval是一个基于知识图谱的框架，能自动生成多模态文档理解和网络交互任务，用于全面评估LLM驱动智能体在动态环境中的推理、协作和交互能力。


<details>
  <summary>Details</summary>
Motivation: 现有静态数据集和LLM合成数据方法无法充分评估多模态LLM智能体在动态环境、多样任务、工具使用和复杂多步网络交互中的真实能力。

Method: 提出Graph2Eval框架。利用多源外部数据构建知识图谱作为任务空间，通过子图采样、任务模板和元路径将语义关系转化为结构化的多模态任务。采用多阶段过滤（节点可达性、LLM评分、相似性分析）确保任务质量和可执行性。支持对单智能体、多智能体和网络智能体进行端到端评估，并测量推理、协作和交互能力。

Result: Graph2Eval能高效生成任务，有效区分智能体和模型性能。实验揭示了不同设置下智能体在推理、协作和网络交互方面的不足，为智能体评估提供了新视角。

Conclusion: Graph2Eval通过生成动态、交互式任务，为全面评估先进的多模态LLM智能体提供了一个有效且新颖的框架，克服了传统评估方法的局限性。

Abstract: As multimodal LLM-driven agents continue to advance in autonomy and
generalization, evaluation based on static datasets can no longer adequately
assess their true capabilities in dynamic environments and diverse tasks.
Existing LLM-based synthetic data methods are largely designed for LLM training
and evaluation, and thus cannot be directly applied to agent tasks that require
tool use and interactive capabilities. While recent studies have explored
automatic agent task generation with LLMs, most efforts remain limited to text
or image analysis, without systematically modeling multi-step interactions in
web environments. To address these challenges, we propose Graph2Eval, a
knowledge graph-based framework that automatically generates both multimodal
document comprehension tasks and web interaction tasks, enabling comprehensive
evaluation of agents' reasoning, collaboration, and interactive capabilities.
In our approach, knowledge graphs constructed from multi-source external data
serve as the task space, where we translate semantic relations into structured
multimodal tasks using subgraph sampling, task templates, and meta-paths. A
multi-stage filtering pipeline based on node reachability, LLM scoring, and
similarity analysis is applied to guarantee the quality and executability of
the generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of
multiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures
reasoning, collaboration, and interaction capabilities. We instantiate the
framework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning
document comprehension and web interaction scenarios. Experiments show that
Graph2Eval efficiently generates tasks that differentiate agent and model
performance, revealing gaps in reasoning, collaboration, and web interaction
across different settings and offering a new perspective for agent evaluation.

</details>


### [22] [Copy-Paste to Mitigate Large Language Model Hallucinations](https://arxiv.org/abs/2510.00508)
*Yongchao Long,Xian Wu,Yingying Zhang,Xianbin Wen,Yuxi Zhou,Shenda Hong*

Main category: cs.CL

TL;DR: 提出CopyPasteLLM模型，通过高复制度偏好训练显著提升RAG中LLM的上下文忠实度并减少幻觉，仅需少量数据即可实现SOTA性能，其关键在于重新校准模型对内部知识的依赖。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG能使大型语言模型(LLM)生成基于上下文的响应，但LLM可能不始终信任提供的上下文，导致幻觉，从而损害可靠性，上下文忠实度仍是一个挑战。

Method: 1. 观察到响应复制程度与上下文不忠实幻觉之间存在负相关关系。2. 提出CopyPasteLLM，通过两阶段高复制度响应偏好训练获得。3. 设计了三种提示方法以提升复制程度。4. 开发了一个全自动化流程，将生成的响应转化为高复制度偏好数据用于CopyPasteLLM训练。5. 引入Context-Parameter Copying Capturing算法以分析模型有效性。

Result: 1. 高复制度响应能实现卓越的上下文忠实度和幻觉控制。2. CopyPasteLLM在FaithEval、ConFiQA和PubMedQA等基准测试中，在反事实和原始上下文下均表现最佳。3. 在FaithEval上，比最佳基线模型的准确率提高了12.2%至24.5%。4. 仅需365个训练样本，是基线数据量的1/50。5. Context-Parameter Copying Capturing算法揭示，CopyPasteLLM在生成过程中重新校准了对内部参数知识的依赖，从而提升了其上下文忠实度。

Conclusion: CopyPasteLLM通过高复制度偏好训练，有效解决了RAG中LLM的上下文不忠实和幻觉问题，以极少数据实现了显著的性能提升。其核心机制在于模型通过调整对内部知识的依赖，使其能更好地采纳和利用外部上下文。

Abstract: While Retrieval-Augmented Generation (RAG) enables large language models
(LLMs) to generate contextually grounded responses, contextual faithfulness
remains challenging as LLMs may not consistently trust provided context,
leading to hallucinations that undermine reliability. We observe an inverse
correlation between response copying degree and context-unfaithful
hallucinations on RAGTruth, suggesting that higher copying degrees reduce
hallucinations by fostering genuine contextual belief. We propose CopyPasteLLM,
obtained through two-stage high-copying response preference training. We design
three prompting methods to enhance copying degree, demonstrating that
high-copying responses achieve superior contextual faithfulness and
hallucination control. These approaches enable a fully automated pipeline that
transforms generated responses into high-copying preference data for training
CopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best
performance in both counterfactual and original contexts, remarkably with 12.2%
to 24.5% accuracy improvements on FaithEval over the best baseline, while
requiring only 365 training samples -- 1/50th of baseline data. To elucidate
CopyPasteLLM's effectiveness, we propose the Context-Parameter Copying
Capturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates
reliance on internal parametric knowledge rather than external knowledge during
generation. All codes are available at
https://github.com/longyongchao/CopyPasteLLM

</details>


### [23] [JoyAgent-JDGenie: Technical Report on the GAIA](https://arxiv.org/abs/2510.00510)
*Jiarun Liu,Shiyue Xu,Shangkun Liu,Yang Li,Wen Liu,Min Liu,Xiaoqing Zhou,Hanmin Wang,Shilin Jia,zhen Wang,Shaohua Tian,Hanhao Li,Junbo Zhang,Yongli Yu,Peng Cao,Haofen Wang*

Main category: cs.CL

TL;DR: 本文提出一个通用的LLM代理架构，通过整合多智能体协作、分层记忆系统和精炼工具套件，显著提升了AI代理在复杂任务中的鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理系统侧重于孤立的改进，缺乏统一的设计来确保在复杂现实任务中的鲁棒性和适应性。

Method: 提出一个通用代理架构，包含三个核心组件：结合规划与执行智能体及评论模型投票的多智能体框架；跨工作、语义和过程层的分层记忆系统；以及用于搜索、代码执行和多模态解析的精炼工具套件。

Result: 在综合基准测试中，该框架持续优于开源基线，并接近专有系统的性能。

Conclusion: 研究结果表明系统级集成的重要性，并为构建可扩展、有弹性、适应性强的AI助手提供了途径。

Abstract: Large Language Models are increasingly deployed as autonomous agents for
complex real-world tasks, yet existing systems often focus on isolated
improvements without a unifying design for robustness and adaptability. We
propose a generalist agent architecture that integrates three core components:
a collective multi-agent framework combining planning and execution agents with
critic model voting, a hierarchical memory system spanning working, semantic,
and procedural layers, and a refined tool suite for search, code execution, and
multimodal parsing. Evaluated on a comprehensive benchmark, our framework
consistently outperforms open-source baselines and approaches the performance
of proprietary systems. These results demonstrate the importance of
system-level integration and highlight a path toward scalable, resilient, and
adaptive AI assistants capable of operating across diverse domains and tasks.

</details>


### [24] [EuroSpeech: A Multilingual Speech Corpus](https://arxiv.org/abs/2510.00514)
*Samuel Pfisterer,Florian Grötschla,Luca A. Lanzendörfer,Florian Yan,Roger Wattenhofer*

Main category: cs.CL

TL;DR: 引入一个可扩展的管道，从议会录音中构建大规模多语言语音数据集，显著提升了现有ASR模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言语音数据集对大多数语言数据量不足，导致模型性能不佳；高质量跨语言语音处理需要各语言有大量训练数据。

Method: 开发了一个可扩展的管道，用于从议会录音中构建语音数据集。该管道包含鲁棒的媒体检索组件和用于处理非逐字转录及长音频的两阶段对齐算法。

Result: 应用该管道处理22个欧洲议会录音，提取了超过6.1万小时的对齐语音片段，实现了每种语言的显著覆盖（19种语言超过1千小时，22种语言超过500小时）。使用该数据集对现有ASR模型进行微调，平均词错误率比基线降低了41.8%。

Conclusion: 所提出的方法能够有效构建大规模、高质量的多语言语音数据集，并显著提升ASR模型的跨语言性能。

Abstract: Recent progress in speech processing has highlighted that high-quality
performance across languages requires substantial training data for each
individual language. While existing multilingual datasets cover many languages,
they often contain insufficient data for most languages. Thus, trained models
perform poorly on the majority of the supported languages. Our work addresses
this challenge by introducing a scalable pipeline for constructing speech
datasets from parliamentary recordings. The proposed pipeline includes robust
components for media retrieval and a two-stage alignment algorithm designed to
handle non-verbatim transcripts and long-form audio. Applying this pipeline to
recordings from 22 European parliaments, we extract over 61k hours of aligned
speech segments, achieving substantial per-language coverage with 19 languages
exceeding 1k hours and 22 languages exceeding 500 hours of high-quality speech
data. We obtain an average 41.8\% reduction in word error rates over baselines
when finetuning an existing ASR model on our dataset, demonstrating the
usefulness of our approach.

</details>


### [25] [Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum](https://arxiv.org/abs/2510.00526)
*Gaotang Li,Ruizhong Qiu,Xiusi Chen,Heng Ji,Hanghang Tong*

Main category: cs.CL

TL;DR: SFT中NLL在LLM后训练时泛化受限。研究发现，目标函数效果取决于“模型能力连续体”：强模型偏好非NLL的先验倾向目标，弱模型NLL更优。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）作为LLM后训练标准方法，泛化能力有限。这可能源于其默认的负对数似然（NLL）目标函数，该函数在已有先验知识和长噪声监督的后训练场景下可能不适用。

Method: 研究了一系列基于概率的通用目标函数。通过对7种模型、14个基准和3个领域的广泛实验与消融研究。进行了理论分析，阐明目标函数在“模型能力连续体”上的表现。

Result: 揭示了影响目标函数行为的关键维度：“模型能力连续体”。在模型能力强时，抑制低概率token的先验倾向目标（如$-p$, $-p^{10}$）持续优于NLL；在模型能力弱时，NLL表现最佳；在两者之间，没有单一目标函数占优。

Conclusion: 目标函数的选择应根据模型能力进行调整。本研究为根据模型能力适配目标函数提供了原理性基础，指出NLL并非LLM后训练的普适最优选择。

Abstract: Supervised fine-tuning (SFT) is the standard approach for post-training large
language models (LLMs), yet it often shows limited generalization. We trace
this limitation to its default training objective: negative log likelihood
(NLL). While NLL is classically optimal when training from scratch,
post-training operates in a different paradigm and could violate its optimality
assumptions, where models already encode task-relevant priors and supervision
can be long and noisy. To this end, we study a general family of
probability-based objectives and characterize their effectiveness under
different conditions. Through comprehensive experiments and extensive ablation
studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a
critical dimension that governs objective behavior: the model-capability
continuum. Near the model-strong end, prior-leaning objectives that downweight
low-probability tokens (e.g., $-p$, $-p^{10}$, thresholded variants)
consistently outperform NLL; toward the model-weak end, NLL dominates; in
between, no single objective prevails. Our theoretical analysis further
elucidates how objectives trade places across the continuum, providing a
principled foundation for adapting objectives to model capability. Our code is
available at https://github.com/GaotangLi/Beyond-Log-Likelihood.

</details>


### [26] [GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness](https://arxiv.org/abs/2510.00536)
*Kung-Hsiang Huang,Haoyi Qiu,Yutong Dai,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: GUI代理因处理高分辨率截图和长时序任务而效率低下。本文提出GUI-KV，一种无需重新训练的KV缓存压缩方法，利用GUI特有的空间和时间冗余（通过空间显著性引导和时间冗余评分）显著提高了代理性能和效率，并在多个基准测试中优于现有压缩方法。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型驱动的GUI代理在自动化人机工作流中潜力巨大，但由于需要处理长序列高分辨率截图和长时序任务，导致推理速度慢、成本高且内存受限。虽然键值（KV）缓存能缓解此问题，但存储完整缓存成本过高。现有缓存压缩方法未能充分利用GUI特有的空间和时间冗余，效果次优。

Method: 首先分析GUI代理工作负载中的注意力模式，发现所有Transformer层都存在均匀的高注意力稀疏性，这启发了统一预算分配策略。在此基础上，本文引入GUI-KV，一种无需重新训练的即插即用KV缓存压缩方法。GUI-KV结合了两种新技术：(i) 空间显著性引导，通过隐藏状态的L2范数增强注意力分数，以更好地保留语义重要的视觉Token；(ii) 时间冗余评分，将前一帧的键投影到当前帧的键子空间，以优先修剪冗余历史。

Result: 在标准GUI代理基准和模型上，GUI-KV优于现有的竞争性KV压缩基线，能在适度预算下实现与完整缓存几乎相同的准确性。特别是在AgentNetBench基准的5截图设置中，GUI-KV将解码FLOPs降低了38.9%，同时将步进准确率比完整缓存基线提高了4.1%。

Conclusion: 利用GUI特有的冗余，可以显著提高代理的效率和可靠性能。

Abstract: Graphical user interface (GUI) agents built on vision-language models have
emerged as a promising approach to automate human-computer workflows. However,
they also face the inefficiency challenge as they process long sequences of
high-resolution screenshots and solving long-horizon tasks, making inference
slow, costly and memory-bound. While key-value (KV) caching can mitigate this,
storing the full cache is prohibitive for image-heavy contexts. Existing
cache-compression methods are sub-optimal as they do not account for the
spatial and temporal redundancy of GUIs. In this work, we first analyze
attention patterns in GUI agent workloads and find that, unlike in natural
images, attention sparsity is uniformly high across all transformer layers.
This insight motivates a simple uniform budget allocation strategy, which we
show empirically outperforms more complex layer-varying schemes. Building on
this, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI
agents that requires no retraining. GUI-KV combines two novel techniques: (i)
spatial saliency guidance, which augments attention scores with the L2 norm of
hidden states to better preserve semantically important visual tokens, and (ii)
temporal redundancy scoring, which projects previous frames' keys onto the
current frame's key subspace to preferentially prune redundant history. Across
standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV
compression baselines, closely matching full-cache accuracy at modest budgets.
Notably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV
reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the
full-cache baseline. These results demonstrate that exploiting GUI-specific
redundancies enables efficient and reliable agent performance.

</details>


### [27] [ThinkBrake: Mitigating Overthinking in Tool Reasoning](https://arxiv.org/abs/2510.00546)
*Minjae Oh,Sangjun Song,Seungkyu Lee,Sungmin Jo,Yohan Jo*

Main category: cs.CL

TL;DR: 小型推理模型（SRMs）在工具使用中存在“过度思考”问题，导致错误和冗余。本文诊断了该问题，并提出了一种无需训练的解码启发式方法ThinkBrake，能在保持或提升准确性的同时显著减少推理token，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 小型推理模型（SRMs）在工具使用时，常在达到正确工具配置后继续推理并将其覆盖为错误结果，即“过度思考”，导致性能下降和冗余计算。此外，先前关于简洁推理的工作主要集中在数学领域，工具推理方面尚待深入探索。

Method: 本文首先通过在句子边界注入`</think>`的Oracle Rollout机制来诊断过度思考问题。在此基础上，本文将多种早期终止基线方法应用于工具使用场景，并引入了一种名为ThinkBrake的无需训练的解码启发式方法。ThinkBrake通过监控在句子边界`</think>`和当前最佳token之间的对数概率裕度，在裕度变小时触发推理终止。

Result: 通过Oracle终止，Berkeley Function Calling Leaderboard (BFCL)上的平均准确率从85.8%提升至94.2%，同时token使用量减少80-94%，揭示了显著的可恢复性能和冗余推理。ThinkBrake在BFCL的单轮、非实时和实时测试集上，能保持或提升准确性，并减少高达25%的token使用量，优于多种现有基线方法。

Conclusion: 小型推理模型在工具使用中存在显著的过度思考问题，通过有效的早期终止策略可以大幅提升性能并减少资源消耗。本文提出的训练无关的ThinkBrake解码启发式方法，在实际应用中能有效减少冗余推理并保持甚至提升模型在工具调用任务上的准确性。

Abstract: Small reasoning models (SRMs) often overthink during tool use: they reach a
correct tool-argument configuration, then continue reasoning and overwrite it
with an incorrect final call. We diagnose overthinking via oracle rollouts that
inject </think> at sentence boundaries. On the Berkeley Function Calling
Leaderboard (BFCL), this oracle termination lifts average accuracy from 85.8\%
to 94.2\% while reducing tokens by 80-94\%, revealing substantial recoverable
headroom and potential redundant reasoning. While prior work on concise
reasoning has largely targeted mathematics, tool reasoning remains
underexplored. We adapt various early-termination baselines to tool use and
introduce ThinkBrake, a training-free decoding heuristic. ThinkBrake monitors
the log-probability margin between </think> and the current top token at
sentence boundaries and triggers termination when this margin becomes small.
Across BFCL's single turn, non-live and live splits, ThinkBrake preserves or
improves accuracy while reducing tokens up to 25\%, outperforming various
baselines.

</details>


### [28] [Are Large Language Models Chronically Online Surfers? A Dataset for Chinese Internet Meme Explanation](https://arxiv.org/abs/2510.00567)
*Yubo Xie,Chenkai Wang,Zongyang Ma,Fahui Miao*

Main category: cs.CL

TL;DR: 本文探讨了大型语言模型（LLMs）对中文网络表情包的理解能力。通过引入CHIME数据集和两项评估任务，发现LLMs在解释表情包含义、识别起源以及在语境中选择合适表情包方面均表现出显著不足，尤其是在处理文化和语言细微差别时，性能远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究大型语言模型（LLMs）是否真正理解在线快速传播的病毒式内容——网络表情包，特别是中文语境下具有文化和语言细微差别的表情包。

Method: 本文构建了一个名为CHIME的中文网络表情包解释数据集，其中包含流行短语式表情包的含义、来源、例句和类型等详细注释。基于此数据集，设计了两个任务来评估LLMs：一是解释表情包、识别其来源并生成例句；二是进行多项选择题，要求LLMs在语境中选择最合适的表情包。

Result: 研究结果显示，在解释表情包含义方面，LLMs对部分表情包表现尚可，但对文化和语言上更微妙的类型性能显著下降，并且在提供表情包准确来源方面始终存在困难。在多项选择任务中，模型虽能给出正确答案，但其整体表现仍远低于人类水平。

Conclusion: LLMs在理解中文网络表情包方面仍存在显著不足，尤其是在处理具有文化和语言细微之处的表情包类型以及识别其起源时，性能远未达到人类水平。公开的CHIME数据集有望推动计算表情包理解领域的未来研究。

Abstract: Large language models (LLMs) are trained on vast amounts of text from the
Internet, but do they truly understand the viral content that rapidly spreads
online -- commonly known as memes? In this paper, we introduce CHIME, a dataset
for CHinese Internet Meme Explanation. The dataset comprises popular
phrase-based memes from the Chinese Internet, annotated with detailed
information on their meaning, origin, example sentences, types, etc. To
evaluate whether LLMs understand these memes, we designed two tasks. In the
first task, we assessed the models' ability to explain a given meme, identify
its origin, and generate appropriate example sentences. The results show that
while LLMs can explain the meanings of some memes, their performance declines
significantly for culturally and linguistically nuanced meme types.
Additionally, they consistently struggle to provide accurate origins for the
memes. In the second task, we created a set of multiple-choice questions (MCQs)
requiring LLMs to select the most appropriate meme to fill in a blank within a
contextual sentence. While the evaluated models were able to provide correct
answers, their performance remains noticeably below human levels. We have made
CHIME public and hope it will facilitate future research on computational meme
understanding.

</details>


### [29] [ReSeek: A Self-Correcting Framework for Search Agents with Instructive Rewards](https://arxiv.org/abs/2510.00568)
*Shiyu Li,Yang Tang,Yifan Wang,Peiming Li,Xi Chen*

Main category: cs.CL

TL;DR: 本文提出ReSeek框架，通过自我纠错机制和密集奖励函数，训练LLM驱动的搜索代理，使其能动态识别并纠正错误搜索路径，并在新基准上显著超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的LLM搜索代理依赖稀疏或基于规则的奖励，导致代理容易陷入次优或错误的推理路径且无法有效恢复。

Method: 提出ReSeek自我纠错框架，核心是通过引入特殊JUDGE动作，使代理能动态识别并从错误搜索路径中恢复。设计了密集的指令性过程奖励函数，该函数分解为信息正确性奖励和信息实用性奖励。此外，为避免数据污染，引入了新的高难度基准FictionalHot。

Result: 广泛实验表明，ReSeek训练的代理在任务成功率和路径忠实度方面显著优于现有的SOTA基线方法。

Conclusion: ReSeek框架通过其新颖的自我纠错机制和密集的指令性奖励函数，有效解决了LLM驱动搜索代理在复杂多步推理中面临的错误路径问题，显著提升了其性能和可靠性。

Abstract: Search agents powered by Large Language Models (LLMs) have demonstrated
significant potential in tackling knowledge-intensive tasks. Reinforcement
learning (RL) has emerged as a powerful paradigm for training these agents to
perform complex, multi-step reasoning. However, prior RL-based methods often
rely on sparse or rule-based rewards, which can lead agents to commit to
suboptimal or erroneous reasoning paths without the ability to recover. To
address these limitations, we propose ReSeek, a novel self-correcting framework
for training search agents. Our framework introduces a self-correction
mechanism that empowers the agent to dynamically identify and recover from
erroneous search paths during an episode. By invoking a special JUDGE action,
the agent can judge the information and re-plan its search strategy. To guide
this process, we design a dense, instructive process reward function, which
decomposes into a correctness reward for retrieving factual information and a
utility reward for finding information genuinely useful for the query.
Furthermore, to mitigate the risk of data contamination in existing datasets,
we introduce FictionalHot, a new and challenging benchmark with recently
curated questions requiring complex reasoning. Being intuitively reasonable and
practically simple, extensive experiments show that agents trained with ReSeek
significantly outperform SOTA baselines in task success rate and path
faithfulness.

</details>


### [30] [CoT Vectors: Transferring and Probing the Reasoning Mechanisms of LLMs](https://arxiv.org/abs/2510.00579)
*Li Li,Ziyi Wang,Yongliang Wu,Jianfei Cai,Xu Yang*

Main category: cs.CL

TL;DR: 本文提出CoT向量，一种紧凑表示，以低成本高效提升大语言模型的多步推理能力，并通过实验揭示了推理过程的内部机制。


<details>
  <summary>Details</summary>
Motivation: 现有链式思考（CoT）推理方法（如上下文学习、微调）成本高且效率低，需要更低成本、更高效地提升大语言模型的CoT推理能力。

Method: 引入CoT向量，一种编码任务通用多步推理知识的紧凑表示。针对提取式CoT向量观察到的层间不稳定性，提出在教师-学生框架下优化的可学习CoT向量，以提供更稳定鲁棒的指导。

Result: CoT向量不仅超越现有基线，还能以更少的训练参数实现与参数高效微调方法相当的性能。此外，通过将CoT向量作为探针，揭示了其有效性如何因潜在空间结构、信息密度、获取机制和预训练差异而变化。

Conclusion: CoT向量是一种高效且强大的方法，可用于增强大语言模型的推理能力，并为理解大语言模型多步推理的内部组织提供了新见解。

Abstract: Chain-of-Thought (CoT) prompting has emerged as a powerful approach to
enhancing the reasoning capabilities of Large Language Models (LLMs). However,
existing implementations, such as in-context learning and fine-tuning, remain
costly and inefficient. To improve CoT reasoning at a lower cost, and inspired
by the task vector paradigm, we introduce CoT Vectors, compact representations
that encode task-general, multi-step reasoning knowledge. Through experiments
with Extracted CoT Vectors, we observe pronounced layer-wise instability,
manifesting as a U-shaped performance curve that reflects a systematic
three-stage reasoning process in LLMs. To address this limitation, we propose
Learnable CoT Vectors, optimized under a teacher-student framework to provide
more stable and robust guidance. Extensive evaluations across diverse
benchmarks and models demonstrate that CoT Vectors not only outperform existing
baselines but also achieve performance comparable to parameter-efficient
fine-tuning methods, while requiring fewer trainable parameters. Moreover, by
treating CoT Vectors as a probe, we uncover how their effectiveness varies due
to latent space structure, information density, acquisition mechanisms, and
pre-training differences, offering new insights into the functional
organization of multi-step reasoning in LLMs. The source code will be released.

</details>


### [31] [SAGE-LD: Towards Scalable and Generalizable End-to-End Language Diarization via Simulated Data Augmentation](https://arxiv.org/abs/2510.00582)
*Sangmin Lee,Woongjib Choi,Jihyun Kim,Hong-Goo Kang*

Main category: cs.CL

TL;DR: 本文提出一个神经语音语言日记模型，通过结合可学习的查询架构和大规模预训练，有效支持无限制语言范围的日记任务。


<details>
  <summary>Details</summary>
Motivation: 克服传统方法在数据稀缺和架构优化方面的局限性，以有效泛化到真实世界多语言环境。

Method: 集成基于学习查询的多语言感知架构，并利用模拟语码转换数据进行大规模预训练。

Result: 在多个语言日记基准测试中达到最先进性能，比现有方法相对性能提升23%至52%。

Conclusion: 该工作不仅推动了语言日记研究，还为语码转换语音技术建立了基础框架。

Abstract: In this paper, we present a neural spoken language diarization model that
supports an unconstrained span of languages within a single framework. Our
approach integrates a learnable query-based architecture grounded in
multilingual awareness, with large-scale pretraining on simulated
code-switching data. By jointly leveraging these two components, our method
overcomes the limitations of conventional approaches in data scarcity and
architecture optimization, and generalizes effectively to real-world
multilingual settings across diverse environments. Experimental results
demonstrate that our approach achieves state-of-the-art performance on several
language diarization benchmarks, with a relative performance improvement of 23%
to 52% over previous methods. We believe that this work not only advances
research in language diarization but also establishes a foundational framework
for code-switching speech technologies.

</details>


### [32] [Tenyidie Syllabification corpus creation and deep learning applications](https://arxiv.org/abs/2510.00629)
*Teisovi Angami,Kevisino Khate*

Main category: cs.CL

TL;DR: 本文为低资源语言Tenyidie创建了首个音节划分数据集，并应用多种深度学习模型，其中BLSTM模型在音节划分任务上达到了99.21%的最高准确率。


<details>
  <summary>Details</summary>
Motivation: Tenyidie语是一种低资源藏缅语系语言，NLP研究非常有限，此前未有音节划分相关工作。音节划分是许多其他NLP任务的基础。

Method: 研究者创建了包含10,120个音节划分Tenyidie词汇的数据集，并在此数据集上应用了LSTM、BLSTM、BLSTM+CRF以及Encoder-decoder等深度学习架构。数据集按80:10:10的比例进行划分（训练:验证:测试）。

Result: 在测试集上，BLSTM模型取得了最高的准确率，达到了99.21%。

Conclusion: 这项工作为Tenyidie语言的音节划分奠定了基础，其成果将有助于该语言在形态分析、词性标注和机器翻译等众多NLP应用中的发展。

Abstract: The Tenyidie language is a low-resource language of the Tibeto-Burman family
spoken by the Tenyimia Community of Nagaland in the north-eastern part of India
and is considered a major language in Nagaland. It is tonal,
Subject-Object-Verb, and highly agglutinative in nature. Being a low-resource
language, very limited research on Natural Language Processing (NLP) has been
conducted. To the best of our knowledge, no work on syllabification has been
reported for this language. Among the many NLP tasks, syllabification or
syllabication is an important task in which the given word syllables are
identified. The contribution of this work is the creation of 10,120 syllabified
Tenyidie words and the application of the Deep Learning techniques on the
created corpus. In this paper, we have applied LSTM, BLSTM, BLSTM+CRF, and
Encoder-decoder deep learning architectures on our created dataset. In our
dataset split of 80:10:10 (train:validation:test) set, we achieved the highest
accuracy of 99.21% with BLSTM model on the test set. This work will find its
application in numerous other NLP applications, such as morphological analysis,
part-of-speech tagging, machine translation, etc, for the Tenyidie Language.
  Keywords: Tenyidie; NLP; syllabification; deep learning; LSTM; BLSTM; CRF;
Encoder-decoder

</details>


### [33] [MCM-DPO: Multifaceted Cross-Modal Direct Preference Optimization for Alt-text Generation](https://arxiv.org/abs/2510.00647)
*Jinlan Fu,Shenzhen Huangfu,Hao Fei,Yichong Huang,Xiaoyu Shen,Xipeng Qiu,See-Kiong Ng*

Main category: cs.CL

TL;DR: 本文提出MCM-DPO方法，通过学习多维度偏好而非精确标注来改进alt-text生成，并构建了高质量数据集TAlt和PAlt，实现了alt-text生成的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有alt-text生成受限于嘈杂的用户标注、不一致的标准以及MLLM对上下文信息的不敏感性。监督微调（SFT）因依赖有缺陷的标注而效果不佳。

Method: 提出了多维度跨模态直接偏好优化（MCM-DPO）方法，通过识别偏好对中的更优选项来改进alt-text生成，无需精确标注。MCM-DPO优化单、配对和多偏好维度（涵盖文本、视觉和跨模态因素）。同时构建了两个大规模、高质量数据集TAlt和PAlt（来自Twitter和Pinterest），包含202k标注样本和18k偏好对。

Result: 实验结果表明，MCM-DPO方法持续优于DPO和SFT，在alt-text生成领域建立了新的技术水平（SOTA）。

Conclusion: MCM-DPO通过创新的偏好学习策略和高质量数据集，有效解决了alt-text生成中的挑战，显著提升了生成性能，为该领域树立了新标准。

Abstract: The alt-text generation task produces concise, context-relevant descriptions
of images, enabling blind and low-vision users to access online images. Despite
the capabilities of large vision-language models, alt-text generation
performance remains limited due to noisy user annotations, inconsistent
standards, and MLLMs' insensitivity to contextual information. Previous efforts
to fine-tune MLLMs using supervised fine-tuning (SFT) have struggled, as SFT
relies on accurate target annotations, which are often flawed in user-generated
alt-text. To address this, we propose Multi-faceted Cross-modal Direct
Preference Optimization (MCM-DPO), which improves alt-text generation by
learning to identify better options in preference pairs without requiring
precise annotations. MCM-DPO optimizes preferences across single, paired, and
multi-preference dimensions, covering textual, visual, and cross-modal factors.
In light of the scarcity of high-quality annotated and preference-labeled
datasets for alt-text, we constructed two large-scale, high-quality datasets
named TAlt and PAlt, sourced from Twitter and Pinterest. These datasets include
202k annotated alt-text samples and 18k preference pairs that cover diverse
preference dimensions, aiming to support further research in this domain.
Experimental results show that our proposed MCM-DPO method consistently
outperforms both DPO and SFT, establishing a new state of the art in alt-text
generation. We release the code and data here:
https://github.com/LVUGAI/MCM-DPO

</details>


### [34] [Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to Easy-to-Read Text Generation](https://arxiv.org/abs/2510.00662)
*François Ledoyen,Gaël Dias,Jeremie Pantin,Alexis Lechervy,Fabrice Maurel,Youssef Chahir*

Main category: cs.CL

TL;DR: 本研究通过多任务学习（MTL）和检索增强生成（RAG）方法，利用大型语言模型（LLMs）自动化生成易读（ETR）文本，以提高信息的可访问性。


<details>
  <summary>Details</summary>
Motivation: 确保信息公平可及性，特别是对认知障碍者而言，简化复杂文本至关重要。易读（ETR）倡议提供了一个框架，但手动创建ETR文本耗时且资源密集，因此需要自动化解决方案。

Method: 提出了一种多任务学习（MTL）方法，将文本摘要、文本简化和ETR生成任务结合训练。探索了两种策略：用于上下文学习的多任务检索增强生成（RAG）和用于参数高效微调的MTL-LoRA。实验基于高质量数据集ETR-fr，并使用了Mistral-7B和LLaMA-3-8B模型。

Result: 多任务设置在所有配置下均优于单任务基线。RAG策略在域外设置中表现出泛化能力，而MTL-LoRA在域内配置中超越了所有学习策略。

Conclusion: 结合多任务学习（MTL）和检索增强生成（RAG）策略的LLMs，能有效自动化易读（ETR）内容生成，显著提升文本简化效果，特别是在不同泛化能力和特定领域性能上各有优势。

Abstract: Simplifying complex texts is essential for ensuring equitable access to
information, especially for individuals with cognitive impairments. The
Easy-to-Read (ETR) initiative offers a framework for making content accessible
to the neurodivergent population, but the manual creation of such texts remains
time-consuming and resource-intensive. In this work, we investigate the
potential of large language models (LLMs) to automate the generation of ETR
content. To address the scarcity of aligned corpora and the specificity of ETR
constraints, we propose a multi-task learning (MTL) approach that trains models
jointly on text summarization, text simplification, and ETR generation. We
explore two different strategies: multi-task retrieval-augmented generation
(RAG) for in-context learning, and MTL-LoRA for parameter-efficient
fine-tuning. Our experiments with Mistral-7B and LLaMA-3-8B, based on ETR-fr, a
new high-quality dataset, demonstrate the benefits of multi-task setups over
single-task baselines across all configurations. Moreover, results show that
the RAG-based strategy enables generalization in out-of-domain settings, while
MTL-LoRA outperforms all learning strategies within in-domain configurations.

</details>


### [35] [Inclusive Easy-to-Read Generation for Individuals with Cognitive Impairments](https://arxiv.org/abs/2510.00691)
*François Ledoyen,Gaël Dias,Alexis Lechervy,Jeremie Pantin,Fabrice Maurel,Youssef Chahir,Elisa Gouzonnat,Mélanie Berthelot,Stanislas Moravac,Armony Altinier,Amy Khairalla*

Main category: cs.CL

TL;DR: 本研究引入了首个符合欧洲指南的ETR-fr数据集，并利用参数高效微调技术实现了简易阅读文本的AI生成，结果表明PLMs的性能可与LLMs媲美，并能有效适应领域外文本。


<details>
  <summary>Details</summary>
Motivation: 为认知障碍者提供无障碍信息对他们的自主权至关重要，但手动简易阅读（ETR）文本改编效率低下、成本高昂且难以规模化。尽管AI驱动的ETR生成提供了一种可扩展的解决方案，但面临数据集稀缺、领域适应以及大型语言模型（LLMs）轻量级学习的挑战。

Method: 1. 引入ETR-fr数据集，这是首个完全符合欧洲ETR指南的ETR文本生成数据集。2. 对预训练语言模型（PLMs）和大型语言模型（LLMs）实施参数高效微调，以建立生成基线。3. 建立了一个结合自动指标和基于36个问题评估表的人工评估框架，以确保生成输出的高质量和可访问性。

Result: 整体结果显示，PLMs的性能与LLMs相当，并且能够有效适应领域外文本。

Conclusion: 本研究为解决ETR文本生成中的挑战提供了新的数据集和方法，并证明了PLMs在保证与LLMs相似性能的同时，能有效实现领域适应，为认知障碍者的信息无障碍化提供了可扩展的AI解决方案。

Abstract: Ensuring accessibility for individuals with cognitive impairments is
essential for autonomy, self-determination, and full citizenship. However,
manual Easy-to-Read (ETR) text adaptations are slow, costly, and difficult to
scale, limiting access to crucial information in healthcare, education, and
civic life. AI-driven ETR generation offers a scalable solution but faces key
challenges, including dataset scarcity, domain adaptation, and balancing
lightweight learning of Large Language Models (LLMs). In this paper, we
introduce ETR-fr, the first dataset for ETR text generation fully compliant
with European ETR guidelines. We implement parameter-efficient fine-tuning on
PLMs and LLMs to establish generative baselines. To ensure high-quality and
accessible outputs, we introduce an evaluation framework based on automatic
metrics supplemented by human assessments. The latter is conducted using a
36-question evaluation form that is aligned with the guidelines. Overall
results show that PLMs perform comparably to LLMs and adapt effectively to
out-of-domain texts.

</details>


### [36] [ALARB: An Arabic Legal Argument Reasoning Benchmark](https://arxiv.org/abs/2510.00694)
*Harethah Abu Shairah,Somayah AlHarbi,Abdulaziz AlHussein,Sameer Alsabea,Omar Shaqaqi,Hebah AlShamlan,Omar Knio,George Turkiyyah*

Main category: cs.CL

TL;DR: 本文介绍了一个名为ALARB的新数据集和任务套件，旨在评估大型语言模型在阿拉伯法律领域的多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语基准测试缺乏专门关注多步推理（尤其在开放式语境下）的实质性数据集，无法充分评估LLM在复杂法律推理中的表现。

Method: 研究构建了ALARB数据集，包含超过1.3万份沙特阿拉伯的商业法庭案例，每份案例包括事实、法院推理、判决和引用的法规条款。基于此数据集，定义了一系列挑战性任务，如判决预测、多步法律论证中的推理链补全以及基于案例事实的相关法规识别。团队还使用这些任务对现有开放和封闭的阿拉伯语LLM进行了基准测试，并展示了数据集在指令微调中的效用。

Result: 通过使用ALARB数据集对一个适度的12B参数模型进行指令微调，该模型在判决预测和阿拉伯语判决生成方面的性能得到显著提升，达到了与GPT-4o相当的水平。

Conclusion: ALARB数据集有效弥补了阿拉伯法律领域多步推理评估的空白，并证明通过指令微调，中等规模的模型也能在复杂的法律推理任务上取得与最先进模型媲美的性能。

Abstract: We introduce ALARB, a dataset and suite of tasks designed to evaluate the
reasoning capabilities of large language models (LLMs) within the Arabic legal
domain. While existing Arabic benchmarks cover some knowledge-intensive tasks
such as retrieval and understanding, substantial datasets focusing specifically
on multistep reasoning for Arabic LLMs, especially in open-ended contexts, are
lacking. The dataset comprises over 13K commercial court cases from Saudi
Arabia, with each case including the facts presented, the reasoning of the
court, the verdict, as well as the cited clauses extracted from the regulatory
documents. We define a set of challenging tasks leveraging this dataset and
reflecting the complexity of real-world legal reasoning, including verdict
prediction, completion of reasoning chains in multistep legal arguments, and
identification of relevant regulations based on case facts. We benchmark a
representative selection of current open and closed Arabic LLMs on these tasks
and demonstrate the dataset's utility for instruction tuning. Notably, we show
that instruction-tuning a modest 12B parameter model using ALARB significantly
enhances its performance in verdict prediction and Arabic verdict generation,
reaching a level comparable to that of GPT-4o.

</details>


### [37] [Family Matters: Language Transfer and Merging for Adapting Small LLMs to Faroese](https://arxiv.org/abs/2510.00810)
*Jenny Kunz,Iben Nyholm Debess,Annika Simonsen*

Main category: cs.CL

TL;DR: 研究如何将小型高效LLM适应到资源匮乏的法罗语。


<details>
  <summary>Details</summary>
Motivation: 将小型、高效的LLM适应到资源匮乏的北日耳曼语——法罗语，以应对其数据稀缺的挑战。

Method: 从英语模型开始，在相关斯堪的纳维亚语上继续预训练，然后微调到法罗语。比较全量微调和LoRA参数高效微调。为评估构建了两个新的最小对基准，并辅以法罗语语言学家的专家评估。

Result: 从相关语言的迁移至关重要，最佳源语言取决于任务（冰岛语提升语言准确性，丹麦语增强理解）。全量微调与LoRA的选择也取决于任务（LoRA改善语言可接受性和人评估分数，全量微调在理解表现和模型能力保留方面更强）。

Conclusion: 将LLM适应到法罗语等低资源语言时，其有效性关键在于任务依赖的相关语言迁移策略以及全量微调与LoRA等微调方法的选择。

Abstract: We investigate how to adapt small, efficient LLMs to Faroese, a low-resource
North Germanic language. Starting from English models, we continue pre-training
on related Scandinavian languages, either individually or combined via merging,
before fine-tuning on Faroese. We compare full fine-tuning with
parameter-efficient tuning using LoRA, evaluating their impact on both
linguistic accuracy and text comprehension. Due to the lack of existing Faroese
evaluation data, we construct two new minimal-pair benchmarks from adapted and
newly collected datasets and complement them with human evaluations by Faroese
linguists. Our results demonstrate that transfer from related languages is
crucial, though the optimal source language depends on the task: Icelandic
enhances linguistic accuracy, whereas Danish boosts comprehension. Similarly,
the choice between full fine-tuning and LoRA is task-dependent: LoRA improves
linguistic acceptability and slightly increases human evaluation scores on the
base model, while full fine-tuning yields stronger comprehension performance
and better preserves model capabilities during downstream fine-tuning.

</details>


### [38] [Exposing the Cracks: Vulnerabilities of Retrieval-Augmented LLM-based Machine Translation](https://arxiv.org/abs/2510.00829)
*Yanming Sun,Runzhe Zhan,Chi Seng Cheang,Han Wu,Xuebo Liu,Yuyao Niu,Fengying Ye,Kaixin Lan,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: 本文评估了检索增强型大语言模型机器翻译 (REAL-MT) 在噪声检索上下文下的鲁棒性，发现其在低资源语言对和推理模型中表现更差，且缺乏自我纠正能力，指出鲁棒性与性能之间存在根本性权衡。


<details>
  <summary>Details</summary>
Motivation: REAL-MT在习语翻译等知识密集型任务中前景广阔，但其在噪声检索上下文（现实部署中的常见挑战）下的可靠性尚不清楚，亟需研究填补这一空白。

Method: 提出了一个噪声合成框架和新指标以系统评估REAL-MT的鲁棒性。使用Qwen系列模型（包括标准LLM和增强推理的LRM）实例化REAL-MT，并在高、中、低资源语言对上评估其习语翻译性能。此外，探讨了免训练和微调的缓解策略。

Result: 低资源语言对在噪声下退化更严重，常产生无意义翻译。LRM未显示出纠错能力提升，反而对噪声更敏感，倾向于合理化错误上下文。原因在于注意力从源习语转向噪声内容，且校准性差（准确率下降但置信度上升）。缓解策略能提高鲁棒性，但会牺牲干净上下文下的性能，揭示了基本权衡。

Conclusion: 当前REAL-MT方法存在局限性，尤其在噪声鲁棒性方面。鲁棒性和干净环境下的性能之间存在根本性权衡。研究结果强调了对自验证集成机制的需求。

Abstract: \textbf{RE}trieval-\textbf{A}ugmented \textbf{L}LM-based \textbf{M}achine
\textbf{T}ranslation (REAL-MT) shows promise for knowledge-intensive tasks like
idiomatic translation, but its reliability under noisy retrieval contexts
remains poorly understood despite this being a common challenge in real-world
deployment. To address this gap, we propose a noise synthesis framework and new
metrics to evaluate the robustness of REAL-MT systematically. Using this
framework, we instantiate REAL-MT with Qwen-series models, including standard
LLMs and large reasoning models (LRMs) with enhanced reasoning, and evaluate
their performance on idiomatic translation across high-, medium-, and
low-resource language pairs under synthesized noise. Our results show that
low-resource language pairs, which rely more heavily on retrieved context,
degrade more severely under noise than high-resource ones and often produce
nonsensical translations. Although LRMs possess enhanced reasoning
capabilities, they show no improvement in error correction and are even more
susceptible to noise, tending to rationalize incorrect contexts. We find that
this stems from an attention shift away from the source idiom to noisy content,
while confidence increases despite declining accuracy, indicating poor
calibration. To mitigate these issues, we investigate training-free and
fine-tuning strategies, which improve robustness at the cost of performance in
clean contexts, revealing a fundamental trade-off. Our findings highlight the
limitations of current approaches, underscoring the need for self-verifying
integration mechanisms.

</details>


### [39] [ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs](https://arxiv.org/abs/2510.00857)
*Adi Simhi,Jonathan Herzig,Martin Tutek,Itay Itzhak,Idan Szpektor,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 引入ManagerBench基准，评估LLM代理在操作目标与人类安全冲突时的决策能力。发现当前LLM在此权衡中表现不佳，问题在于优先级排序而非感知能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM演变为自主代理，现有安全基准主要关注有害内容生成，而忽视了代理在实现操作目标时，若与人类安全冲突，可能采取有害行动的挑战。

Method: 引入ManagerBench基准，在人类验证的现实管理场景中评估LLM的决策。每个场景都要求在实现操作目标的“实用但有害”行动和导致性能下降的“安全”行动之间做出选择。另设平行对照组，通过将潜在危害导向无生命物体来衡量模型的实用主义倾向，并识别过度安全行为。

Result: 前沿LLM在安全与实用主义的权衡中表现不佳。许多模型为推进操作目标而选择有害选项，而另一些则为了避免危害而变得过度安全和低效。研究发现，这种错位并非源于无法感知危害，而是来自错误的优先级排序。

Conclusion: ManagerBench是一个挑战性的基准，揭示了LLM代理在操作目标和对齐价值观激励冲突行动时，做出安全选择的核心难题，强调了优先级对齐的重要性。

Abstract: As large language models (LLMs) evolve from conversational assistants into
autonomous agents, evaluating the safety of their actions becomes critical.
Prior safety benchmarks have primarily focused on preventing generation of
harmful content, such as toxic text. However, they overlook the challenge of
agents taking harmful actions when the most effective path to an operational
goal conflicts with human safety. To address this gap, we introduce
ManagerBench, a benchmark that evaluates LLM decision-making in realistic,
human-validated managerial scenarios. Each scenario forces a choice between a
pragmatic but harmful action that achieves an operational goal, and a safe
action that leads to worse operational performance. A parallel control set,
where potential harm is directed only at inanimate objects, measures a model's
pragmatism and identifies its tendency to be overly safe. Our findings indicate
that the frontier LLMs perform poorly when navigating this safety-pragmatism
trade-off. Many consistently choose harmful options to advance their
operational goals, while others avoid harm only to become overly safe and
ineffective. Critically, we find this misalignment does not stem from an
inability to perceive harm, as models' harm assessments align with human
judgments, but from flawed prioritization. ManagerBench is a challenging
benchmark for a core component of agentic behavior: making safe choices when
operational goals and alignment values incentivize conflicting actions.
Benchmark & code available at https://github.com/technion-cs-nlp/ManagerBench.

</details>


### [40] [Erase to Improve: Erasable Reinforcement Learning for Search-Augmented LLMs](https://arxiv.org/abs/2510.00861)
*Ziliang Wang,Kang An,Xuhui Zheng,Faqiang Qian,Weikun Zhang,Cijun Ouyang,Jialu Cai,Yuhang Wang,Yichao Wu*

Main category: cs.CL

TL;DR: 本文提出可擦除强化学习（ERL）框架，通过识别、擦除并原地重新生成错误推理步骤，显著提升了搜索增强型大语言模型在多跳推理任务上的可靠性，并在多个基准测试中超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 搜索增强型大语言模型在复杂多跳推理中可靠性有限，主要源于分解错误、检索遗漏和推理错误，任何一个环节失败都可能导致最终答案出错。

Method: 提出可擦除强化学习（ERL）框架。该框架明确识别故障步骤，将其擦除，并原地重新生成推理，以防止缺陷逻辑在推理链中传播，从而将脆弱推理转变为鲁棒过程。

Result: 使用ERL训练的模型（ESearch）在HotpotQA、MuSiQue、2Wiki和Bamboogle等数据集上取得显著提升。其中，3B模型EM提高+8.48%，F1提高+11.56%；7B模型EM提高+5.38%，F1提高+7.22%，均超越了此前最先进（SOTA）结果。

Conclusion: 可擦除强化学习（ERL）为LLM中鲁棒的多步推理提供了一种强大的范式转变，能有效提高其多跳推理能力。

Abstract: While search-augmented large language models (LLMs) exhibit impressive
capabilities, their reliability in complex multi-hop reasoning remains limited.
This limitation arises from three fundamental challenges: decomposition errors,
where tasks are incorrectly broken down; retrieval missing, where key evidence
fails to be retrieved; and reasoning errors, where flawed logic propagates
through the reasoning chain. A single failure in any of these stages can derail
the final answer. We propose Erasable Reinforcement Learning (ERL), a novel
framework that transforms fragile reasoning into a robust process. ERL
explicitly identifies faulty steps, erases them, and regenerates reasoning in
place, preventing defective logic from propagating through the reasoning chain.
This targeted correction mechanism turns brittle reasoning into a more
resilient process. Models trained with ERL, termed ESearch, achieve substantial
improvements on HotpotQA, MuSiQue, 2Wiki, and Bamboogle, with the 3B model
achieving +8.48% EM and +11.56% F1, and the 7B model achieving +5.38% EM and
+7.22% F1 over previous state-of-the-art(SOTA) results. These findings suggest
that erasable reinforcement learning provides a powerful paradigm shift for
robust multi-step reasoning in LLMs.

</details>


### [41] [HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.00880)
*Loris Bergeron,Ioana Buhnila,Jérôme François,Radu State*

Main category: cs.CL

TL;DR: HalluGuard是一个4B参数的小型推理模型，用于缓解检索增强生成（RAG）中的LLM幻觉问题，通过分类文档-声明对并提供理由，以更少的参数实现了与大型或专用模型相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）容易产生幻觉，这限制了它们在实际应用中的可信度。

Method: 提出HalluGuard，一个4B参数的小型推理模型（SRM），用于分类文档-声明对为有根据或幻觉，并提供证据支持的理由。该方法结合了：(i) 源自FineWeb并经过多阶段精炼的领域无关合成数据集，(ii) 合成的有根据和幻觉声明，以及 (iii) 采用Odds Ratio Preference Optimization的基于偏好的微调，以将大型模型推理能力蒸馏到小型模型中。

Result: 在LLM-AggreFact基准测试的RAGTruth子集上，HalluGuard实现了84.0%的平衡准确率（BAcc），与MiniCheck（7B；84.0%）和Granite Guardian 3.3（8B；82.2%）等专用模型相当，但参数量约为其一半。在整个基准测试中，它达到了75.7%的BAcc，与GPT-4o（75.9%）等大型通用LLM持平。

Conclusion: HalluGuard证明了小型模型可以在缓解RAG中的LLM幻觉方面实现高效且具有竞争力的性能，提供了一个透明且参数更少的解决方案。

Abstract: Large Language Models (LLMs) excel in many NLP tasks but remain prone to
hallucinations, limiting trust in real-world applications. We present
HalluGuard, a 4B-parameter Small Reasoning Model (SRM) for mitigating
hallucinations in Retrieval-Augmented Generation (RAG). HalluGuard classifies
document-claim pairs as grounded or hallucinated and produces evidence-grounded
justifications for transparency. Our approach combines (i) a domain-agnostic
synthetic dataset derived from FineWeb and refined through multi-stage curation
and data reformation, (ii) synthetic grounded and hallucinated claims, and
(iii) preference-based fine-tuning with Odds Ratio Preference Optimization to
distill large-model reasoning into a smaller backbone. On the RAGTruth subset
of the LLM-AggreFact benchmark, HalluGuard achieves 84.0% balanced accuracy
(BAcc), rivaling specialized models, MiniCheck (7B; 84.0%) and Granite Guardian
3.3 (8B; 82.2%) while using roughly half their parameters. Over the full
benchmark it reaches 75.7% BAcc, matching larger general-purpose LLMs such as
GPT-4o (75.9%). We will release HalluGuard and datasets under Apache 2.0 upon
acceptance.

</details>


### [42] [Span-level Detection of AI-generated Scientific Text via Contrastive Learning and Structural Calibration](https://arxiv.org/abs/2510.00890)
*Zhen Yin,Shenghua Wang*

Main category: cs.CL

TL;DR: Sci-SpanDet是一种结构感知框架，用于细粒度地检测学术论文中的AI生成文本。它结合了样式建模和对比学习，解决了现有方法在泛化性、细粒度定位和校准方面的不足，并在多LLM家族数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学写作中的快速应用引发了对署名完整性和学术出版物可靠性的担忧。现有检测方法主要依赖文档级分类或表面统计特征，忽略了细粒度定位，校准性弱，且难以泛化到不同学科和生成器。

Method: 本文提出了Sci-SpanDet，一个结构感知的AI生成学术文本检测框架。它结合了分节条件样式建模和多级对比学习，以捕获人与AI的细微差异并减轻主题依赖性，从而增强跨域鲁棒性。此外，它还集成了BIO-CRF序列标注、基于指针的边界解码和置信度校准，以实现精确的span级检测和可靠的概率估计。

Result: 在包含10万个标注样本的跨学科新数据集上（由多种LLM家族生成），Sci-SpanDet实现了最先进的性能，F1(AI)为80.17，AUROC为92.63，Span-F1为74.36。它在对抗性重写下表现出强大的韧性，并在IMRaD章节和不同学科中保持平衡的准确性，显著超越现有基线。

Conclusion: Sci-SpanDet成功克服了现有AI生成文本检测方法的局限性，提供了一种更精确、更鲁棒的细粒度检测方案，对维护学术诚信具有重要意义。为促进研究，相关数据集和源代码将在发表后公开。

Abstract: The rapid adoption of large language models (LLMs) in scientific writing
raises serious concerns regarding authorship integrity and the reliability of
scholarly publications. Existing detection approaches mainly rely on
document-level classification or surface-level statistical cues; however, they
neglect fine-grained span localization, exhibit weak calibration, and often
fail to generalize across disciplines and generators. To address these
limitations, we present Sci-SpanDet, a structure-aware framework for detecting
AI-generated scholarly texts. The proposed method combines section-conditioned
stylistic modeling with multi-level contrastive learning to capture nuanced
human-AI differences while mitigating topic dependence, thereby enhancing
cross-domain robustness. In addition, it integrates BIO-CRF sequence labeling
with pointer-based boundary decoding and confidence calibration to enable
precise span-level detection and reliable probability estimates. Extensive
experiments on a newly constructed cross-disciplinary dataset of 100,000
annotated samples generated by multiple LLM families (GPT, Qwen, DeepSeek,
LLaMA) demonstrate that Sci-SpanDet achieves state-of-the-art performance, with
F1(AI) of 80.17, AUROC of 92.63, and Span-F1 of 74.36. Furthermore, it shows
strong resilience under adversarial rewriting and maintains balanced accuracy
across IMRaD sections and diverse disciplines, substantially surpassing
existing baselines. To ensure reproducibility and to foster further research on
AI-generated text detection in scholarly documents, the curated dataset and
source code will be publicly released upon publication.

</details>


### [43] [Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level Physics Problem Solving](https://arxiv.org/abs/2510.00919)
*Shunfeng Zheng,Yudi Zhang,Meng Fang,Zihan Zhang,Zhitan Wu,Mykola Pechenizkiy,Ling Chen*

Main category: cs.CL

TL;DR: 本文探讨RAG模型在奥林匹克物理推理中的潜力，引入PhoPile多模态数据集进行基准测试，结果显示检索增强可提高模型性能，但也存在挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然RAG模型表现出色，但其在奥林匹克级物理问题等专家级推理方面的能力尚待探索。研究灵感来源于学生通过复习过往问题备赛的方式。

Method: 引入高质量多模态数据集PhoPile（包含图表、图形和方程，专为奥林匹克物理设计）。使用PhoPile对RAG增强的基座模型（包括大型语言模型和大型多模态模型）进行基准测试，并使用多种检索器。

Result: 将检索与物理语料库结合可以提高模型的性能。

Conclusion: 检索增强的物理推理具有改进模型性能的潜力，但也存在挑战，这些挑战将推动未来相关研究。

Abstract: Retrieval-augmented generation (RAG) with foundation models has achieved
strong performance across diverse tasks, but their capacity for expert-level
reasoning-such as solving Olympiad-level physics problems-remains largely
unexplored. Inspired by the way students prepare for competitions by reviewing
past problems, we investigate the potential of RAG to enhance physics reasoning
in foundation models. We introduce PhoPile, a high-quality multimodal dataset
specifically designed for Olympiad-level physics, enabling systematic study of
retrieval-based reasoning. PhoPile includes diagrams, graphs, and equations,
capturing the inherently multimodal nature of physics problem solving. Using
PhoPile, we benchmark RAG-augmented foundation models, covering both large
language models (LLMs) and large multimodal models (LMMs) with multiple
retrievers. Our results demonstrate that integrating retrieval with physics
corpora can improve model performance, while also highlighting challenges that
motivate further research in retrieval-augmented physics reasoning.

</details>


### [44] [Making, not Taking, the Best of N](https://arxiv.org/abs/2510.00931)
*Ammar Khairi,Daniel D'souza,Marzieh Fadaee,Julia Kreutzer*

Main category: cs.CL

TL;DR: 本文提出Fusion-of-N (FusioN) 方法，通过LLM法官整合多个LLM生成结果的有效信息，优于传统的Best-of-N (BoN) 选择方法，并在多语言、多任务和不同模型规模下表现出卓越性能。


<details>
  <summary>Details</summary>
Motivation: 传统的Best-of-N (BoN) 方法在选择高质量LLM生成结果时是零和的，会丢弃池中多样且可能有用的信息。研究旨在探索一种协作式设置，使所有候选结果都能贡献最终的优质生成。

Method: 提出Fusion-of-N (FusioN) 方法，利用一个通用LLM法官将每个样本中最具信息量的元素合成为一个最终答案。在测试时规模化（test-time scaling）和合成数据生成（synthetic data generation）两种设置下，将FusioN与BoN进行比较，并在11种语言、3种不同任务和不同模型规模下进行了广泛的基准测试和分析。

Result: FusioN在测试时规模化和合成数据生成方面始终优于BoN，在所有基准测试中展现出多功能性和鲁棒性。它在具有挑战性的设置下也表现出令人惊讶的优势和鲁棒性。

Conclusion: 研究结果表明，我们应该改变对LLM生成结果的评估和利用方式，从单一的质量衡量转向拥抱其多态性。这种转变允许整合多样优势，释放潜在能力，并实现仅靠选择无法达到的改进。

Abstract: Obtaining high-quality generations in modern LLMs has largely been framed as
a selection problem: identifying a single winning generation from a diverse
pool of N samples, the Best-of-N (BoN). Yet, this approach is inherently
zero-sum, discarding diverse and potentially useful information from the pool.
Instead, we explore a collaborative setup, where all candidates can potentially
contribute to the final winning generation. To this end, we propose Fusion-of-N
(FusioN): a method that uses a general LLM judge to synthesize the most
informative elements of each sample into a single final answer. We compare
FusioN to BoN in two settings, (i) test-time scaling, where we sample and
aggregate from a single model at test-time (ii) synthetic data generation,
where we fuse samples from a pool of diverse teachers to improve a student
model. We extensively benchmark both setups across 11 languages, 3 diverse
tasks and varying model scales. Across the bench, FusioN consistently
outperforms BoN showing versatility and robustness both in test-time scaling
and in downstream gains from synthetic data generation. We also perform
extensive analysis on FusioN, where it shows surprising strengths and
robustness under challenging settings. These results show that we should shift
how we think about evaluating and utilizing LLM generations from a monolithic
measure of quality, to embracing their polylithic nature. This shift allows us
to integrate diverse strengths, unlock latent potential, and achieve
improvements that were previously inaccessible through selection alone.

</details>


### [45] [Analyzing Dialectical Biases in LLMs for Knowledge and Reasoning Benchmarks](https://arxiv.org/abs/2510.00962)
*Eileen Pan,Anna Seo Gyeong Choi,Maartje ter Hoeve,Skyler Seto,Allison Koenecke*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）对非标准英语方言表现不佳，准确率下降高达20%。特定语法规则（如存在句“it”、零系动词、“y'all”）是造成性能下降的主要原因。


<details>
  <summary>Details</summary>
Motivation: 现有工作表明，大型语言模型（LLMs）在处理未充分代表的英语方言时性能会下降。

Method: 1. 在多项选择问答任务中，分析将“标准”美式英语问题类型化为非“标准”方言变体的影响。2. 调查非“标准”英语问题表现不佳的语法基础。

Result: 1. 准确率最高下降20%。2. 个别语法规则对性能有不同影响，其中三个特定语法规则（存在句“it”、零系动词、“y'all”）可以解释多种方言中观察到的大部分性能下降。

Conclusion: 呼吁未来的研究应着重于针对具有高影响力的个体语法结构来探索偏见缓解方法。

Abstract: Large language models (LLMs) are ubiquitous in modern day natural language
processing. However, previous work has shown degraded LLM performance for
under-represented English dialects. We analyze the effects of typifying
"standard" American English language questions as non-"standard" dialectal
variants on multiple choice question answering tasks and find up to a 20%
reduction in accuracy. Additionally, we investigate the grammatical basis of
under-performance in non-"standard" English questions. We find that individual
grammatical rules have varied effects on performance, but some are more
consequential than others: three specific grammar rules (existential "it", zero
copula, and y'all) can explain the majority of performance degradation observed
in multiple dialects. We call for future work to investigate bias mitigation
methods focused on individual, high-impact grammatical structures.

</details>


### [46] [Syntax-Guided Diffusion Language Models with User-Integrated Personalization](https://arxiv.org/abs/2510.01028)
*Ruqian Zhang,Yijiao Zhang,Juan Shen,Zhongyi Zhu,Annie Qu*

Main category: cs.CL

TL;DR: 本文提出一个语法引导的扩散语言模型，通过结构监督和个性化条件，显著提升了文本生成的多样性、可控性和个性化表现，超越了传统大型语言模型的局限。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在文本生成方面取得了革命性进展，但其输出往往趋于通用，缺乏足够的结构多样性，限制了个性化表达。扩散模型的最新进展为超越自回归范式，改进语言生成提供了新机遇。

Method: 我们提出了一个语法引导的扩散语言模型，该模型整合了结构监督和个性化条件，以提升文本质量、多样性和可控性。该方法引入了一个级联框架，先生成句法指导再进行条件文本生成，并进一步推广到一种新颖的非级联架构以实现更好的结构与内容对齐。此外，为实现细粒度个性化，开发了一种共享表示机制，促进跨用户的信息整合，支持忠实的风格化生成和可泛化的零样本推理。

Result: 在多项任务上进行的广泛实验证明，我们的方法在流畅性、多样性和风格保真度方面均表现出优越性。进一步的定性分析也突显了其在学习个性化模式方面的可解释性和灵活性。

Conclusion: 所提出的语法引导扩散语言模型通过整合结构监督和个性化条件，有效解决了传统大型语言模型输出缺乏多样性和个性化表达的问题，显著提升了文本生成质量、多样性和可控性，尤其在个性化风格构建方面表现突出。

Abstract: Large language models have made revolutionary progress in generating
human-like text, yet their outputs often tend to be generic, exhibiting
insufficient structural diversity, which limits personalized expression. Recent
advances in diffusion models have opened new opportunities for improving
language generation beyond the limitations of autoregressive paradigms. In this
work, we propose a syntax-guided diffusion language model that integrates
structural supervision and personalized conditioning to enhance text quality,
diversity, and controllability. We introduce a cascaded framework that
generates syntactic guidance before conditional text generation, and further
generalize it to a novel noncascaded architecture for better alignment between
structure and content. By incorporating syntactic information in the generating
process, the proposed model better captures the lexical and structural
characteristics of stylistic sentence construction. To enable fine-grained
personalization, we develop a shared representation mechanism that facilitates
information integration across users, supporting both faithful stylistic
generation and generalizable zero-shot inference. Extensive experiments on
multiple tasks demonstrate the superiority of our approach in fluency,
diversity, and stylistic fidelity. Further qualitative analyses highlight its
interpretability and flexibility in learning personalized patterns.

</details>


### [47] [Interpreting Language Models Through Concept Descriptions: A Survey](https://arxiv.org/abs/2510.01048)
*Nils Feldhus,Laura Kopf*

Main category: cs.CL

TL;DR: 本文首次综述了为大型语言模型（LLM）组件和抽象（如神经元、注意力头、稀疏特征）生成自然语言概念描述的新兴领域，分析了其生成方法、评估指标和数据集，并强调了对更严格因果评估的需求。


<details>
  <summary>Details</summary>
Motivation: 实现神经网络的机械可解释性是核心目标，具体到LLM，需要理解其决策过程，并识别各组件（如神经元、注意力头）及抽象（如稀疏自编码器特征）的角色。

Method: 本文进行了一项全面的调查研究，首次综述了为模型组件和抽象生成概念描述的新兴领域。具体分析了生成这些描述的关键方法、评估它们的自动化和人工指标，以及支撑这项研究的数据集。

Result: 研究综合分析揭示，该领域对更严格、更具因果性的评估方法存在日益增长的需求。

Conclusion: 本综述概述了该领域的最新进展，指出了关键挑战，并为未来提高模型透明度的研究提供了明确的路线图。

Abstract: Understanding the decision-making processes of neural networks is a central
goal of mechanistic interpretability. In the context of Large Language Models
(LLMs), this involves uncovering the underlying mechanisms and identifying the
roles of individual model components such as neurons and attention heads, as
well as model abstractions such as the learned sparse features extracted by
Sparse Autoencoders (SAEs). A rapidly growing line of work tackles this
challenge by using powerful generator models to produce open-vocabulary,
natural language concept descriptions for these components. In this paper, we
provide the first survey of the emerging field of concept descriptions for
model components and abstractions. We chart the key methods for generating
these descriptions, the evolving landscape of automated and human metrics for
evaluating them, and the datasets that underpin this research. Our synthesis
reveals a growing demand for more rigorous, causal evaluation. By outlining the
state of the art and identifying key challenges, this survey provides a roadmap
for future research toward making models more transparent.

</details>


### [48] [Hybrid Dialogue State Tracking for Persian Chatbots: A Language Model-Based Approach](https://arxiv.org/abs/2510.01052)
*Samin Mahdipour Aghabagher,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: 本研究提出一种混合DST模型，结合规则方法和语言模型，在波斯语多轮对话中显著提升了准确性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 传统规则DST无法满足开放域多轮聊天机器人在适应性和连贯性方面的需求，需要更高效的方法来深入理解对话上下文并响应用户请求。

Method: 该研究提出一种混合DST模型，结合了规则方法与语言模型（BERT用于槽填充和意图检测，XGBoost用于意图验证，GPT用于DST，以及在线代理用于实时答案生成）。

Result: 该模型在波斯语多轮对话数据集上进行了评估，与现有方法相比，在波斯语聊天机器人中显著提高了准确性和连贯性。

Conclusion: 混合方法能有效提升对话状态追踪（DST）能力，为开发更定制化、适应性强、更像人类的对话AI系统铺平道路。

Abstract: Dialogue State Tracking (DST) is an essential element of conversational AI
with the objective of deeply understanding the conversation context and leading
it toward answering user requests. Due to high demands for open-domain and
multi-turn chatbots, the traditional rule-based DST is not efficient enough,
since it cannot provide the required adaptability and coherence for human-like
experiences in complex conversations. This study proposes a hybrid DST model
that utilizes rule-based methods along with language models, including BERT for
slot filling and intent detection, XGBoost for intent validation, GPT for DST,
and online agents for real-time answer generation. This model is uniquely
designed to be evaluated on a comprehensive Persian multi-turn dialogue dataset
and demonstrated significantly improved accuracy and coherence over existing
methods in Persian-based chatbots. The results demonstrate how effectively a
hybrid approach may improve DST capabilities, paving the way for conversational
AI systems that are more customized, adaptable, and human-like.

</details>


### [49] [Research on the Integration of Embodied Intelligence and Reinforcement Learning in Textual Domains](https://arxiv.org/abs/2510.01076)
*Haonan Wang,Junfeng Sun,Mingjia Zhao,Wei Liu*

Main category: cs.CL

TL;DR: 本文提出将具身智能和强化学习结合应用于文本处理，以提升其智能化水平。


<details>
  <summary>Details</summary>
Motivation: 旨在利用具身智能的感知行动优势和强化学习的决策优化能力，提升文本处理的智能化。

Method: 通过理论解释和实验探索，引入了一种新颖的集成模型。

Result: 该模型在多种文本处理任务中表现出显著效果。

Conclusion: 验证了该集成模型在文本处理领域的应用潜力。

Abstract: This article addresses embodied intelligence and reinforcement learning
integration in the field of text processing, aiming to enhance text handling
with more intelligence on the basis of embodied intelligence's perception and
action superiority and reinforcement learning's decision optimization
capability. Through detailed theoretical explanation and experimental
exploration, a novel integration model is introduced. This model has been
demonstrated to be very effective in a wide range oftext processing tasks,
validating its applicative potential

</details>


### [50] [Automatic Speech Recognition (ASR) for African Low-Resource Languages: A Systematic Literature Review](https://arxiv.org/abs/2510.01145)
*Sukairaj Hafiz Imam,Tadesse Destaw Belay,Kedir Yassin Husse,Ibrahim Said Ahmad,Idris Abdulmumin,Hadiza Ali Umar,Muhammad Yahuza Bello,Joyce Nakatumba-Nabende,Seid Muhie Yimam,Shamsuddeen Hassan Muhammad*

Main category: cs.CL

TL;DR: 该系统综述（SLR）分析了非洲低资源语言ASR的研究现状，强调了数据、模型和评估方法的不足，并提出了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: ASR在全球取得显著进展，但非洲2000多种低资源语言的ASR研究严重不足，阻碍了非洲大陆的数字包容性。

Method: 采用PRISMA 2020流程，检索DBLP、ACM、Google Scholar、Semantic Scholar和arXiv（2020年1月至2025年7月）上的研究。筛选了2062条记录中的71条，聚焦非洲语言ASR的数据集、模型、训练方法和评估技术。

Result: 共记录111种语言的74个数据集，约11,206小时语音。不到15%的研究提供可复现材料，数据集许可不明。自监督和迁移学习有前景，但受限于预训练数据、方言覆盖不足和资源。大多数研究使用WER，对语音学知情的CER/DER使用极少，不适用于声调和形态丰富的语言。ASR系统现有证据不一致，受数据集、标注、许可和基准测试限制。

Conclusion: 现有ASR证据不一致，但社区驱动的倡议和方法论进步指明了改进方向。未来的可持续发展需要利益相关者伙伴关系、创建道德平衡的数据集、使用轻量级建模技术和积极的基准测试。

Abstract: ASR has achieved remarkable global progress, yet African low-resource
languages remain rigorously underrepresented, producing barriers to digital
inclusion across the continent with more than +2000 languages. This systematic
literature review (SLR) explores research on ASR for African languages with a
focus on datasets, models and training methods, evaluation techniques,
challenges, and recommends future directions. We employ the PRISMA 2020
procedures and search DBLP, ACM Digital Library, Google Scholar, Semantic
Scholar, and arXiv for studies published between January 2020 and July 2025. We
include studies related to ASR datasets, models or metrics for African
languages, while excluding non-African, duplicates, and low-quality studies
(score <3/5). We screen 71 out of 2,062 records and we record a total of 74
datasets across 111 languages, encompassing approximately 11,206 hours of
speech. Fewer than 15% of research provided reproducible materials, and dataset
licensing is not clear. Self-supervised and transfer learning techniques are
promising, but are hindered by limited pre-training data, inadequate coverage
of dialects, and the availability of resources. Most of the researchers use
Word Error Rate (WER), with very minimal use of linguistically informed scores
such as Character Error Rate (CER) or Diacritic Error Rate (DER), and thus with
limited application in tonal and morphologically rich languages. The existing
evidence on ASR systems is inconsistent, hindered by issues like dataset
availability, poor annotations, licensing uncertainties, and limited
benchmarking. Nevertheless, the rise of community-driven initiatives and
methodological advancements indicates a pathway for improvement. Sustainable
development for this area will also include stakeholder partnership, creation
of ethically well-balanced datasets, use of lightweight modelling techniques,
and active benchmarking.

</details>


### [51] [mR3: Multilingual Rubric-Agnostic Reward Reasoning Models](https://arxiv.org/abs/2510.01146)
*David Anugraha,Shou-Yi Hung,Zilu Tang,Annie En-Shiun Lee,Derry Tanti Wijaya,Genta Indra Winata*

Main category: cs.CL

TL;DR: 本文介绍mR3，一个在72种语言上训练的、大规模多语言、与评分标准无关的奖励推理模型，实现了最广泛的语言覆盖，并在多语言奖励模型基准上达到最先进的性能，同时模型尺寸更小。


<details>
  <summary>Details</summary>
Motivation: LLM评判在英语环境中有效，但在非英语环境中表现不佳，且尚不清楚如何进行有效的多语言训练以构建此类评判模型。

Method: 引入mR3模型，在72种语言上进行训练，达到了奖励建模中前所未有的语言覆盖范围。研究了数据和课程选择策略，包括整合目标语言推理数据集，以构建高质量的奖励模型。

Result: mR3在多语言奖励模型基准上取得了最先进的性能，超越了更大的模型（如GPT-OSS-120B），同时模型尺寸小9倍。广泛的消融研究进一步证实了其有效性。

Conclusion: mR3是一个高效的多语言奖励推理模型，具有广泛的语言覆盖和卓越的性能，其模型、数据和代码已开源。

Abstract: Evaluation using Large Language Model (LLM) judges has been widely adopted in
English and shown to be effective for automatic evaluation. However, their
performance does not generalize well to non-English settings, and it remains
unclear what constitutes effective multilingual training for such judges. In
this paper, we introduce mR3, a massively multilingual, rubric-agnostic reward
reasoning model trained on 72 languages, achieving the broadest language
coverage in reward modeling to date. We present a comprehensive study of data
and curriculum selection for training to identify effective strategies and data
sources for building high-quality reward models, including the integration of
target-language reasoning datasets. Our approach attains state-of-the-art
performance on multilingual reward model benchmarks, surpassing much larger
models (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness
is further confirmed through extensive ablation studies. Our models, data, and
code are available as open source at https://github.com/rubricreward/mr3.

</details>


### [52] [Pay-Per-Search Models are Abstention Models](https://arxiv.org/abs/2510.01152)
*Mustafa Omer Gul,Claire Cardie,Tanya Goyal*

Main category: cs.CL

TL;DR: MASH是一个训练框架，通过将外部搜索视为弃权代理，使用强化学习训练LLM进行选择性求助和弃权，显著提高了知识密集型问答的准确性并展现出强大的弃权能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）无法可靠识别其参数知识边界，常对超出范围的问题产生幻觉。与此相反，人类能识别自身局限并选择寻求外部帮助或弃权。

Method: 引入MASH（Modeling Abstention via Selective Help-seeking）训练框架。该框架使用强化学习，将LLM的外部求助（搜索工具使用）作为弃权的代理。通过对搜索进行适当惩罚，同时奖励答案准确性，实现“按搜索付费”的奖励机制。MASH无需预先确定知识边界来构建训练数据。

Result: 在三个知识密集型问答数据集上的实验表明，MASH显著优于先前的效率搜索方法，在多跳数据集上将答案准确率提高了7.6%。此外，MASH展示了强大的开箱即用弃权能力，能够区分可回答/不可回答的问题，并选择性地生成对可回答问题的响应。

Conclusion: MASH训练能有效将搜索工具的使用与参数知识对齐，并成功地用于做出弃权决策。弃权是训练辅助选择性求助任务的副产品，无需预设知识边界。

Abstract: LLMs cannot reliably recognize their parametric knowledge boundaries and
often hallucinate answers to outside-of-boundary questions. In contrast, humans
recognize their limitations and can either seek external help for such
questions or abstain. In this paper, we introduce MASH (Modeling Abstention via
Selective Help-seeking), a training framework that readily extracts abstentions
from LLMs. Our key idea is that any external help-seeking by an LLM, i.e.
search tool use, can serve as a proxy for abstention if the external help
(search) is appropriately penalized while simultaneously rewarding answer
accuracy. MASH operationalizes this idea using reinforcement learning with a
pay-per-search reward.
  We run experiments on three knowledge-intensive QA datasets. Our results show
that MASH substantially improves upon the selective help-seeking performance of
prior efficient search approaches; on multi-hop datasets, MASH improves answer
accuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf
abstention -- it can distinguish between unanswerable/answerable questions and
selectively generate responses for answerable questions -- showcasing behavior
analogous to specialized abstention approaches. We emphasize that contrary to
prior abstention methods, MASH does not require pre-determining knowledge
boundaries to construct training data. Instead, MASH's abstentions are a
by-product of training for the auxiliary selective help-seeking task. Overall,
we show that MASH training effectively aligns search tool use with parametric
knowledge, which can be successfully leveraged for making abstention decisions.

</details>


### [53] [Backdoor Attacks Against Speech Language Models](https://arxiv.org/abs/2510.01157)
*Alexandrine Fortier,Thomas Thebaud,Jesús Villalba,Najim Dehak,Patrick Cardinal*

Main category: cs.CL

TL;DR: 本文首次系统研究了针对语音语言模型的音频后门攻击，发现攻击成功率极高，并提出了一种基于微调的防御方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（LLMs）通过级联领域特定编码器实现多模态功能，这使得模型继承了所有组件的漏洞。因此，有必要对这些模型的安全性，特别是音频后门攻击进行系统性研究。

Method: 本研究对语音语言模型进行了首次系统性音频后门攻击研究。攻击在四种语音编码器和三个数据集上进行了验证，涵盖自动语音识别（ASR）、语音情感识别、性别和年龄预测四项任务。此外，还进行了组件级分析以理解后门传播机制，并提出了一种基于微调的防御方法。

Result: 音频后门攻击在所有测试场景中均表现出极高的成功率，范围从90.76%到99.41%。组件级分析识别出管道中最脆弱的阶段。

Conclusion: 音频后门攻击对语音语言模型构成严重威胁。本研究提出了一种基于微调的防御机制，能够有效缓解预训练编码器被投毒带来的威胁。

Abstract: Large Language Models (LLMs) and their multimodal extensions are becoming
increasingly popular. One common approach to enable multimodality is to cascade
domain-specific encoders with an LLM, making the resulting model inherit
vulnerabilities from all of its components. In this work, we present the first
systematic study of audio backdoor attacks against speech language models. We
demonstrate its effectiveness across four speech encoders and three datasets,
covering four tasks: automatic speech recognition (ASR), speech emotion
recognition, and gender and age prediction. The attack consistently achieves
high success rates, ranging from 90.76% to 99.41%. To better understand how
backdoors propagate, we conduct a component-wise analysis to identify the most
vulnerable stages of the pipeline. Finally, we propose a fine-tuning-based
defense that mitigates the threat of poisoned pretrained encoders.

</details>


### [54] [Social Welfare Function Leaderboard: When LLM Agents Allocate Social Welfare](https://arxiv.org/abs/2510.01164)
*Zhengliang Shi,Ruotian Ma,Jen-tse Huang,Xinbei Ma,Xingyu Chen,Mengru Wang,Qu Yang,Yue Wang,Fanghua Ye,Ziyang Chen,Shanyi Wang,Cixing Li,Wenxuan Wang,Zhaopeng Tu,Xiaolong Li,Zhaochun Ren,Linus*

Main category: cs.CL

TL;DR: 研究LLM在稀缺资源分配中的决策原则。通过SWF基准测试20个LLM发现，LLM通用能力与分配能力关联不大，它们普遍偏向功利主义导致不平等，且分配策略脆弱。结果强调了部署当前LLM的风险，并呼吁加强AI治理的专门基准和校准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）正被用于影响人类福祉的高风险决策，尤其是在稀缺社会资源分配方面。然而，指导这些模型决策的原则和价值观尚未得到充分审查。

Method: 引入了社会福利函数（SWF）基准，这是一个动态模拟环境。在此环境中，LLM作为分配者，向异构社区分发任务。该基准旨在衡量集体效率（投资回报率）和分配公平性（基尼系数）之间的权衡。研究评估了20个最先进的LLM，并发布了首个社会福利分配排行榜。

Result: (i) 模型的通用对话能力（由流行排行榜衡量）不能很好地预测其分配技能。(ii) 大多数LLM表现出强烈的默认功利主义倾向，优先考虑群体生产力，但代价是严重的不平等。(iii) 分配策略高度脆弱，容易受到输出长度限制和社会影响框架的扰动。

Conclusion: 当前将LLM部署为社会决策者存在风险。研究强调需要专门的基准和有针对性的对齐（alignment）来改进AI治理。

Abstract: Large language models (LLMs) are increasingly entrusted with high-stakes
decisions that affect human welfare. However, the principles and values that
guide these models when distributing scarce societal resources remain largely
unexamined. To address this, we introduce the Social Welfare Function (SWF)
Benchmark, a dynamic simulation environment where an LLM acts as a sovereign
allocator, distributing tasks to a heterogeneous community of recipients. The
benchmark is designed to create a persistent trade-off between maximizing
collective efficiency (measured by Return on Investment) and ensuring
distributive fairness (measured by the Gini coefficient). We evaluate 20
state-of-the-art LLMs and present the first leaderboard for social welfare
allocation. Our findings reveal three key insights: (i) A model's general
conversational ability, as measured by popular leaderboards, is a poor
predictor of its allocation skill. (ii) Most LLMs exhibit a strong default
utilitarian orientation, prioritizing group productivity at the expense of
severe inequality. (iii) Allocation strategies are highly vulnerable, easily
perturbed by output-length constraints and social-influence framing. These
results highlight the risks of deploying current LLMs as societal
decision-makers and underscore the need for specialized benchmarks and targeted
alignment for AI governance.

</details>


### [55] [GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning](https://arxiv.org/abs/2510.01165)
*Oussama Gabouj,Kamel Charaf,Ivan Zakazov,Nicolas Baldwin,Robert West*

Main category: cs.CL

TL;DR: 本文提出GRAD，一种动态的基于示例的方法，通过训练LLM生成特定于输入的简洁示例，以克服传统RAG的局限性，并在资源受限的情况下，在数学推理和STEM问题上表现出色，且能泛化到OOD领域。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的性能高度依赖于上下文质量。传统的检索增强生成（RAG）依赖静态数据库，导致适应性受限和可能提供不相关的示例，因此需要更动态、上下文更相关的支持。

Method: 提出Generative Retrieval-Aligned Demonstrator (GRAD)，一种动态的基于示例的方法。该方法训练一个LLM模型，使其能够为每个输入生成定制的、简洁的示例，以提供更好的上下文支持。

Result: 在预算受限条件下，GRAD在数学数据集上训练后，在Qwen2.5-14B模型上，持续超越数学推理和高级STEM问题上的强基线模型。它还展示了对物理、化学和计算机科学等OOD领域的强大泛化能力。此外，训练较小模型生成的示例可以有效指导较大的目标模型，降低训练成本同时保持竞争力。

Conclusion: GRAD引入了一个可扩展的示例生成器模型，是资源受限环境下动态少样本学习范式的初步探索。该方法提升了LLM在各种任务中的性能和泛化能力。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks,
but their effectiveness often depends on the quality of the provided context.
Retrieval-Augmented Generation (RAG) enriches prompts with external
information, but its reliance on static databases constrains adaptability and
can result in irrelevant demonstrations. In this work, we propose a Generative
Retrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach
where an LLM model is trained to generate input-specific concise
demonstrations. By tailoring demonstrations to each input, our method offers
better contextual support than traditional RAG approaches. We demonstrate the
superiority of GRAD under budget constraints, where we limit both the number of
tokens used per demonstration and the number of tokens used for the final
output. Trained solely on a math dataset, GRAD consistently outperforms strong
baselines on Qwen2.5-14B across mathematical reasoning and advanced STEM
questions, highlighting GRAD's robust generalization to out-of-distribution
(OOD) domains such as physics, chemistry, and computer science. Furthermore, we
show that demonstrations generated by trained smaller models can effectively
guide larger target models, reducing training costs while maintaining
competitive accuracy. Overall, this work introduces a scalable demonstration
generator model presenting the first step toward a dynamic few-shot learning
paradigm in resource-constrained settings. We release the code used for the
project.

</details>


### [56] [Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity](https://arxiv.org/abs/2510.01171)
*Jiayi Zhang,Simon Yu,Derek Chong,Anthony Sicilia,Michael R. Tomz,Christopher D. Manning,Weiyan Shi*

Main category: cs.CL

TL;DR: 本文指出大模型训练后对齐导致的模式崩溃源于偏好数据中的典型性偏差。为解决此问题，提出了 Verbalized Sampling 提示策略，显著提升了模型在多任务上的生成多样性，特别是创意写作。


<details>
  <summary>Details</summary>
Motivation: 传统研究将LLM模式崩溃归因于算法限制，但本文提出其根本原因在于偏好数据中普遍存在的“典型性偏差”，即标注者系统性地偏好熟悉文本，这来源于认知心理学的发现。

Method: 首先，理论形式化并实证验证了偏好数据中的典型性偏差，并证明其在模式崩溃中扮演核心角色。基于此，引入了一种简单、无需训练的提示策略——Verbalized Sampling (VS)，通过提示模型以语言形式输出一组响应及其对应的概率分布来规避模式崩溃。

Result: 综合实验表明，VS在创意写作（诗歌、故事、笑话）、对话模拟、开放式问答和合成数据生成等任务中显著提高了性能，且未牺牲事实准确性和安全性。例如，在创意写作中，VS将多样性提高了1.6-2.1倍。此外，观察到能力更强的模型从VS中受益更多。

Conclusion: 本研究为模式崩溃提供了一个以数据为中心的新视角，并提出了一种实用且在推理时有效的补救措施，有助于释放预训练模型的生成多样性潜力。

Abstract: Post-training alignment often reduces LLM diversity, leading to a phenomenon
known as mode collapse. Unlike prior work that attributes this effect to
algorithmic limitations, we identify a fundamental, pervasive data-level
driver: typicality bias in preference data, whereby annotators systematically
favor familiar text as a result of well-established findings in cognitive
psychology. We formalize this bias theoretically, verify it on preference
datasets empirically, and show that it plays a central role in mode collapse.
Motivated by this analysis, we introduce Verbalized Sampling, a simple,
training-free prompting strategy to circumvent mode collapse. VS prompts the
model to verbalize a probability distribution over a set of responses (e.g.,
``Generate 5 jokes about coffee and their corresponding probabilities'').
Comprehensive experiments show that VS significantly improves performance
across creative writing (poems, stories, jokes), dialogue simulation,
open-ended QA, and synthetic data generation, without sacrificing factual
accuracy and safety. For instance, in creative writing, VS increases diversity
by 1.6-2.1x over direct prompting. We further observe an emergent trend that
more capable models benefit more from VS. In sum, our work provides a new
data-centric perspective on mode collapse and a practical inference-time remedy
that helps unlock pre-trained generative diversity.

</details>


### [57] [Energy-Regularized Sequential Model Editing on Hyperspheres](https://arxiv.org/abs/2510.01172)
*Qingyuan Liu,Jia-Chen Gu,Yunzhi Yao,Hong Wang,Nanyun Peng*

Main category: cs.CL

TL;DR: LLMs顺序编辑易导致灾难性遗忘。本研究发现神经元超球面均匀性对模型稳定至关重要，并提出了SPHERE策略，通过稳定权重分布，显著提升了顺序编辑能力和知识保留。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）需持续更新，模型编辑是轻量化方案，但顺序编辑常导致表征不稳定和灾难性遗忘。本研究旨在理解并缓解顺序编辑造成的性能下降。

Method: 假设超球面均匀性有助于模型稳定和知识保留。使用超球面能量（HE）量化神经元均匀性并分析其与编辑性能的相关性。理论证明HE动态对预训练知识退化存在下限。提出SPHERE（Sparse Projection for Hyperspherical Energy-Regularized Editing）策略，通过在与预训练权重主超球面方向互补的稀疏空间中投射新知识，稳定神经元权重分布。

Result: 实证研究显示HE动态与编辑性能强相关，编辑失败常伴随HE高波动。SPHERE策略在LLaMA3和Qwen2.5上，平均编辑能力比最佳基线提升16.41%，并最忠实地保留了模型通用性能。

Conclusion: SPHERE提供了一种基于HE驱动正则化的可靠大规模知识编辑途径，通过稳定神经元权重分布，有效保留先验知识，并实现可靠的顺序更新。

Abstract: Large language models (LLMs) require constant updates to remain aligned with
evolving real-world knowledge. Model editing offers a lightweight alternative
to retraining, but sequential editing often destabilizes representations and
induces catastrophic forgetting. In this work, we seek to better understand and
mitigate performance degradation caused by sequential editing. We hypothesize
that hyperspherical uniformity, a property that maintains uniform distribution
of neuron weights on a hypersphere, helps the model remain stable, retain prior
knowledge, while still accommodate new updates. We use Hyperspherical Energy
(HE) to quantify neuron uniformity during editing, and examine its correlation
with editing performance. Empirical studies across widely used editing methods
reveals a strong correlation between HE dynamics and editing performance, with
editing failures consistently coinciding with high HE fluctuations. We further
theoretically prove that HE dynamics impose a lower bound on the degradation of
pretrained knowledge, highlighting why HE stability is crucial for knowledge
retention. Motivated by these insights, we propose SPHERE (Sparse Projection
for Hyperspherical Energy-Regularized Editing), an HE-driven regularization
strategy that stabilizes neuron weight distributions, ultimately preserving
prior knowledge while enabling reliable sequential updates. Specifically,
SPHERE identifies a sparse space complementary to the principal hyperspherical
directions of the pretrained weight matrices and projects new knowledge onto
it, attenuating perturbations on the principal directions. Extensive
experiments on LLaMA3 (8B) and Qwen2.5 (7B) show that SPHERE outperforms the
best baseline in editing capability by an average of 16.41%, while most
faithfully preserving general model performance, thereby offering a principled
path toward reliable large-scale knowledge editing.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [58] [Hybrid Deep Learning for Hyperspectral Single Image Super-Resolution](https://arxiv.org/abs/2510.00033)
*Usman Muhammad,Jorma Laaksonen*

Main category: cs.CV

TL;DR: 提出了一种名为SSUF的模块，结合光谱解混和特征提取，并设计了结合空间与光谱梯度的损失函数，以提升高光谱单图像超分辨率的性能，并在公开数据集上取得了竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 高光谱单图像超分辨率（SISR）在恢复精细空间细节和保持光谱保真度方面面临挑战，传统深度学习模型表现受限。

Method: 引入了光谱-空间解混融合（SSUF）模块，可无缝集成到2D卷积架构中；SSUF结合光谱解混与光谱-空间特征提取，并指导基于ResNet的CNN进行重建。此外，提出了自定义的空间-光谱梯度损失函数，结合均方误差、空间和光谱梯度分量。

Result: 在三个公开遥感高光谱数据集上的实验表明，所提出的混合深度学习模型在降低模型复杂性的同时，取得了竞争性的性能。

Conclusion: 该混合深度学习模型通过SSUF模块和自定义损失函数，有效解决了高光谱SISR中的空间细节恢复和光谱保真度保持难题，并取得了良好效果。

Abstract: Hyperspectral single image super-resolution (SISR) is a challenging task due
to the difficulty of restoring fine spatial details while preserving spectral
fidelity across a wide range of wavelengths, which limits the performance of
conventional deep learning models. To address this challenge, we introduce
Spectral-Spatial Unmixing Fusion (SSUF), a novel module that can be seamlessly
integrated into standard 2D convolutional architectures to enhance both spatial
resolution and spectral integrity. The SSUF combines spectral unmixing with
spectral--spatial feature extraction and guides a ResNet-based convolutional
neural network for improved reconstruction. In addition, we propose a custom
Spatial-Spectral Gradient Loss function that integrates mean squared error with
spatial and spectral gradient components, encouraging accurate reconstruction
of both spatial and spectral features. Experiments on three public remote
sensing hyperspectral datasets demonstrate that the proposed hybrid deep
learning model achieves competitive performance while reducing model
complexity.

</details>


### [59] [Review of Hallucination Understanding in Large Language and Vision Models](https://arxiv.org/abs/2510.00034)
*Zhengyi Ho,Siyuan Liang,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文提出一个统一的多层次框架，用于分析大语言和视觉模型中的幻觉问题，并揭示其常源于数据分布模式和固有偏差，旨在为开发更有效的解决方案提供基础。


<details>
  <summary>Details</summary>
Motivation: 大语言和视觉模型产生的幻觉（错误/无意义输出）在实际应用中导致信息传播不当、经济和操作损失。现有缓解方案因对幻觉理解不完整和碎片化而效果有限，因此亟需更深入、更系统的理解来开发根本性解决方案。

Method: 首先，提出了一个统一的、多层次的框架来表征跨越不同应用的图像和文本幻觉，以减少概念上的碎片化。其次，采用任务-模态交错方法，将幻觉与模型生命周期中的特定机制关联起来，以促进更整合的理解。

Result: 研究发现，幻觉往往源于数据分布中可预测的模式以及模型继承的偏差。

Conclusion: 通过深化对幻觉的理解，本调查为开发更鲁棒、更有效的现实世界生成式AI系统中的幻觉解决方案提供了基础。

Abstract: The widespread adoption of large language and vision models in real-world
applications has made urgent the need to address hallucinations -- instances
where models produce incorrect or nonsensical outputs. These errors can
propagate misinformation during deployment, leading to both financial and
operational harm. Although much research has been devoted to mitigating
hallucinations, our understanding of it is still incomplete and fragmented.
Without a coherent understanding of hallucinations, proposed solutions risk
mitigating surface symptoms rather than underlying causes, limiting their
effectiveness and generalizability in deployment. To tackle this gap, we first
present a unified, multi-level framework for characterizing both image and text
hallucinations across diverse applications, aiming to reduce conceptual
fragmentation. We then link these hallucinations to specific mechanisms within
a model's lifecycle, using a task-modality interleaved approach to promote a
more integrated understanding. Our investigations reveal that hallucinations
often stem from predictable patterns in data distributions and inherited
biases. By deepening our understanding, this survey provides a foundation for
developing more robust and effective solutions to hallucinations in real-world
generative AI systems.

</details>


### [60] [On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations](https://arxiv.org/abs/2510.00037)
*Jianing Guo,Zhenhong Wu,Chang Tu,Yiyao Ma,Xiangqi Kong,Zhiqian Liu,Jiaming Ji,Shuning Zhang,Yuanpei Chen,Kai Chen,Xianglong Liu,Qi Dou,Yaodong Yang,Huijie Zhao,Weifeng Lv,Simin Li*

Main category: cs.CV

TL;DR: 本文提出RobustVLA，通过对VLA模型输入输出进行鲁棒优化，实现多模态（视觉、语言、动作、环境）扰动下的性能提升和推理加速，尤其在真实机器人环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有Vision-Language-Action (VLA)模型在真实世界部署时，面临多模态扰动的鲁棒性挑战。当前研究主要关注简单视觉干扰，忽视了动作、指令、环境和观察等模态中更广泛的扰动。

Method: 首先评估了主流VLA模型在四种模态17种扰动下的鲁棒性。然后提出了RobustVLA：针对输出鲁棒性，采用离线鲁棒优化对抗最坏情况的动作噪声，利用流匹配目标函数最大化不匹配，并结合对抗训练、标签平滑和异常值惩罚；针对输入鲁棒性，确保在保留任务语义的输入变异下，模型能生成一致的动作。为处理多种扰动，将鲁棒性建模为多臂赌博机问题，并应用上置信界算法自动识别最有害的噪声。

Result: 1. 评估发现：动作是最脆弱的模态；现有视觉鲁棒VLA在其他模态中未获得鲁棒性；pi0模型在扩散动作头方面表现出卓越鲁棒性。2. RobustVLA成果：在LIBERO实验中，相比基线在17种扰动下，pi0骨干网络性能绝对提升12.6%，OpenVLA骨干网络提升10.4%。推理速度比现有视觉鲁棒VLA快50.6倍。在混合扰动下提升10.4%。在真实FR5机器人、有限演示下，四种模态扰动下绝对性能提升65.6%。

Conclusion: VLA模型部署中，多模态扰动鲁棒性至关重要。RobustVLA通过创新的输入输出鲁棒优化策略，显著提升了VLA模型在多模态扰动下的性能和效率，尤其在真实机器人应用中展现出强大的潜力，并纠正了现有视觉鲁棒方法在其他模态鲁棒性上的不足。

Abstract: In Vision-Language-Action (VLA) models, robustness to real-world
perturbations is critical for deployment. Existing methods target simple visual
disturbances, overlooking the broader multi-modal perturbations that arise in
actions, instructions, environments, and observations. Here, we first evaluate
the robustness of mainstream VLAs under 17 perturbations across four
modalities. We find (1) actions as the most fragile modality, (2) Existing
visual-robust VLA do not gain robustness in other modality, and (3) pi0
demonstrates superior robustness with a diffusion-based action head. To build
multi-modal robust VLAs, we propose RobustVLA against perturbations in VLA
inputs and outputs. For output robustness, we perform offline robust
optimization against worst-case action noise that maximizes mismatch in flow
matching objective. This can be seen as adversarial training, label smoothing,
and outlier penalization. For input robustness, we enforce consistent actions
across input variations that preserve task semantics. To account for multiple
perturbations, we formulate robustness as a multi-armed bandit problem and
apply an upper confidence bound algorithm to automatically identify the most
harmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers
absolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the
OpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference
than existing visual-robust VLAs, and a 10.4% gain under mixed perturbations.
Our RobustVLA is particularly effective on real-world FR5 robot with limited
demonstrations, showing absolute gains by 65.6% under perturbations of four
modalities.

</details>


### [61] [Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models](https://arxiv.org/abs/2510.00040)
*Junjie Li,Ziao Wang,Jianghong Ma,Xiaofeng Zhang*

Main category: cs.CV

TL;DR: CADC是一个基于内在能力分析的数据策展框架，它通过发现和利用模型内在能力，以极少量数据（5%）超越了全数据训练在VLM指令微调上的表现，将黑盒调优转变为可控的、能力驱动的过程。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型（VLMs）的指令微调难以控制，且在减少数据集预算时常导致性能下降。现有启发式策略将模型视为黑盒，忽视了影响学习的潜在内在能力。

Method: 提出Capability-Attributed Data Curation (CADC) 框架。CADC通过梯度学习轨迹无监督发现内在能力，利用影响估计将训练数据归因于这些能力，并通过平衡选择和分阶段排序来策划能力感知型课程。

Result: 使用仅5%的原始数据，CADC在多模态基准测试上超越了全数据训练的表现。

Conclusion: 研究结果验证了内在能力是模型学习的基本构建块，并确立了CADC作为指令数据策展的一个原则性范式。

Abstract: Large vision-language models (VLMs) achieve strong benchmark performance, but
controlling their behavior through instruction tuning remains difficult.
Reducing the budget of instruction tuning dataset often causes regressions, as
heuristic strategies treat models as black boxes and overlook the latent
capabilities that govern learning. We introduce Capability-Attributed Data
Curation (CADC), a framework that shifts curation from task-specific heuristics
to intrinsic capability analysis. CADC discovers intrinsic capabilities in an
unsupervised manner from gradient-based learning trajectories, attributes
training data to these capabilities via influence estimation, and curates
capability-aware curricula through balanced selection and staged sequencing.
This transforms black-box instruction tuning into a controllable,
capability-driven process. With as little as 5% of the original data, CADC
surpasses full-data training on multimodal benchmarks. These results validate
intrinsic capabilities as the fundamental building blocks of model learning and
establish CADC as a principle paradigm for instruction data curation.

</details>


### [62] [Culture In a Frame: C$^3$B as a Comic-Based Benchmark for Multimodal Culturally Awareness](https://arxiv.org/abs/2510.00041)
*Yuchen Song,Andong Chen,Wenxin Zhu,Kehai Chen,Xuefeng Bai,Muyun Yang,Tiejun Zhao*

Main category: cs.CV

TL;DR: 提出C$^3$B，一个新颖的多文化、多任务、多语言MLLM文化意识基准，包含难度递进的任务。对11个MLLM评估显示，模型与人类表现存在显著差距，证明C$^3$B对当前MLLM构成重大挑战。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）的文化意识能力日益关键，但现有基准任务设计难度不足，缺乏跨语言任务，且多使用单一文化图片，导致评估过于简单。

Method: 构建C$^3$B（Comics Cross-Cultural Benchmark），一个包含2000多张图片和18000多个问答对的多文化、多任务、多语言基准。任务设计了从基础视觉识别到文化冲突理解，再到文化内容生成三个难度递进的层次。

Result: 对11个开源MLLM的评估发现，它们的性能与人类表现存在显著差距。这表明C$^3$B对当前的MLLM提出了实质性挑战。

Conclusion: C$^3$B基准揭示了当前MLLM在文化意识能力方面的不足，鼓励未来研究致力于提升MLLM的此项能力。

Abstract: Cultural awareness capabilities has emerged as a critical capability for
Multimodal Large Language Models (MLLMs). However, current benchmarks lack
progressed difficulty in their task design and are deficient in cross-lingual
tasks. Moreover, current benchmarks often use real-world images. Each
real-world image typically contains one culture, making these benchmarks
relatively easy for MLLMs. Based on this, we propose C$^3$B ($\textbf{C}$omics
$\textbf{C}$ross-$\textbf{C}$ultural $\textbf{B}$enchmark), a novel
multicultural, multitask and multilingual cultural awareness capabilities
benchmark. C$^3$B comprises over 2000 images and over 18000 QA pairs,
constructed on three tasks with progressed difficulties, from basic visual
recognition to higher-level cultural conflict understanding, and finally to
cultural content generation. We conducted evaluations on 11 open-source MLLMs,
revealing a significant performance gap between MLLMs and human performance.
The gap demonstrates that C$^3$B poses substantial challenges for current
MLLMs, encouraging future research to advance the cultural awareness
capabilities of MLLMs.

</details>


### [63] [Beyond the Prompt: Gender Bias in Text-to-Image Models, with a Case Study on Hospital Professions](https://arxiv.org/abs/2510.00045)
*Franck Vandewiele,Remi Synave,Samuel Delepoulle,Remi Cozot*

Main category: cs.CV

TL;DR: 本文调查了六个文本到图像模型中职业相关的性别偏见，发现系统性刻板印象和模型特异性差异，并强调提示词对结果的关键影响。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在专业、教育和创意领域日益普及，但其输出常嵌入并放大社会偏见，特别是性别偏见，因此有必要进行研究。

Method: 研究选取了HunyuanImage 2.1、HiDream-I1-dev、Qwen-Image、FLUX.1-dev、Stable-Diffusion 3.5 Large和Stable-Diffusion-XL六个主流开源模型。针对五种医院相关职业（心脏病医生、医院主任、护士、护理人员、外科医生）和五种肖像修饰词（空、公司、中性、审美、美丽），设计了精确的提示词，为每种组合生成了100张图像进行分析。

Result: 分析显示模型存在系统性职业刻板印象：护士几乎全为女性，外科医生以男性为主。模型间存在差异：Qwen-Image和SDXL强化男性主导，HiDream-I1-dev结果混杂，FLUX.1-dev在多数角色中偏向女性。HunyuanImage 2.1和SD3.5也存在性别刻板印象，但对提示词的敏感度不同。肖像修饰词会调节性别平衡，如“公司”强化男性，“美丽”偏向女性。模型对修饰词的敏感度也大相径庭，Qwen-Image几乎不受影响，而FLUX.1-dev、SDXL和SD3.5表现出强烈的提示词依赖性。

Conclusion: 文本到图像模型中的性别偏见既具系统性又具模型特异性，提示词在塑造人口统计学结果方面发挥着关键作用。研究结果强调了在生成式AI中，需要偏见意识设计、平衡的默认设置和用户指导，以防止职业刻板印象的强化。

Abstract: Text-to-image (TTI) models are increasingly used in professional,
educational, and creative contexts, yet their outputs often embed and amplify
social biases. This paper investigates gender representation in six
state-of-the-art open-weight models: HunyuanImage 2.1, HiDream-I1-dev,
Qwen-Image, FLUX.1-dev, Stable-Diffusion 3.5 Large, and Stable-Diffusion-XL.
Using carefully designed prompts, we generated 100 images for each combination
of five hospital-related professions (cardiologist, hospital director, nurse,
paramedic, surgeon) and five portrait qualifiers ("", corporate, neutral,
aesthetic, beautiful).
  Our analysis reveals systematic occupational stereotypes: all models produced
nurses exclusively as women and surgeons predominantly as men. However,
differences emerge across models: Qwen-Image and SDXL enforce rigid male
dominance, HiDream-I1-dev shows mixed outcomes, and FLUX.1-dev skews female in
most roles. HunyuanImage 2.1 and Stable-Diffusion 3.5 Large also reproduce
gender stereotypes but with varying degrees of sensitivity to prompt
formulation. Portrait qualifiers further modulate gender balance, with terms
like corporate reinforcing male depictions and beautiful favoring female ones.
Sensitivity varies widely: Qwen-Image remains nearly unaffected, while
FLUX.1-dev, SDXL, and SD3.5 show strong prompt dependence.
  These findings demonstrate that gender bias in TTI models is both systematic
and model-specific. Beyond documenting disparities, we argue that prompt
wording plays a critical role in shaping demographic outcomes. The results
underscore the need for bias-aware design, balanced defaults, and user guidance
to prevent the reinforcement of occupational stereotypes in generative AI.

</details>


### [64] [Reinforcement Learning-Based Prompt Template Stealing for Text-to-Image Models](https://arxiv.org/abs/2510.00046)
*Xiaotian Zou*

Main category: cs.CV

TL;DR: 本文揭示了多模态大语言模型（MLLMs）提示词交易市场中提示词被窃取的安全漏洞，并提出了RLStealer，一个基于强化学习的提示词逆向框架，能高效地从少量图像中恢复提示词模板，攻击成本远低于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型催生了提示词交易市场，但这种商业模式存在未被充分研究的安全风险：提示词本身可能被窃取，从而危及交易市场的安全性。

Method: 本文提出了RLStealer，一个基于强化学习的提示词逆向框架。它将提示词窃取视为一个序列决策问题，并利用多个基于相似性的反馈信号作为奖励函数，有效探索提示词空间，以从少量示例图像中恢复提示词模板。

Result: RLStealer在公开基准测试中达到了最先进的性能，同时将总攻击成本降低到现有基线所需的13%以下。进一步分析证实，RLStealer能有效泛化到不同的图像风格，高效窃取未见过的提示词模板。

Conclusion: 本研究强调了提示词交易中固有的紧迫安全威胁，并为在新兴MLLMs市场中开发保护性标准奠定了基础。

Abstract: Multimodal Large Language Models (MLLMs) have transformed text-to-image
workflows, allowing designers to create novel visual concepts with
unprecedented speed. This progress has given rise to a thriving prompt trading
market, where curated prompts that induce trademark styles are bought and sold.
Although commercially attractive, prompt trading also introduces a largely
unexamined security risk: the prompts themselves can be stolen.
  In this paper, we expose this vulnerability and present RLStealer, a
reinforcement learning based prompt inversion framework that recovers its
template from only a small set of example images. RLStealer treats template
stealing as a sequential decision making problem and employs multiple
similarity based feedback signals as reward functions to effectively explore
the prompt space. Comprehensive experiments on publicly available benchmarks
demonstrate that RLStealer gets state-of-the-art performance while reducing the
total attack cost to under 13% of that required by existing baselines. Our
further analysis confirms that RLStealer can effectively generalize across
different image styles to efficiently steal unseen prompt templates. Our study
highlights an urgent security threat inherent in prompt trading and lays the
groundwork for developing protective standards in the emerging MLLMs
marketplace.

</details>


### [65] [Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations](https://arxiv.org/abs/2510.00047)
*Sihao Ding,Santosh Vasa,Aditi Ramadwar*

Main category: cs.CV

TL;DR: 本文提出EDCT，一种自动化验证程序，通过反事实测试揭示视觉语言模型（VLMs）自然语言解释（NLEs）的不可靠性，发现其解释与实际预测驱动因素之间存在显著不一致。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）生成的自然语言解释（NLEs）虽然听起来可信，但可能并未反映驱动预测的真实因果因素。这种“合理性”与“忠实性”之间的不匹配带来了技术和治理风险。

Method: 引入解释驱动的反事实测试（EDCT），一种针对目标VLM的全自动化验证程序。EDCT将模型的解释视为可证伪的假设，具体步骤包括：1) 获取模型的回答和NLE；2) 将NLE解析为可测试的视觉概念；3) 通过生成式修复生成有针对性的反事实编辑；4) 使用大型语言模型（LLM）辅助分析答案和解释的变化，计算反事实一致性得分（CCS）。

Result: 在120个精选的OK-VQA示例和多个VLM上，EDCT揭示了显著的忠实性差距。它提供了符合监管要求的审计证据，表明模型引用的概念何时未能通过因果测试。

Conclusion: EDCT成功揭示了VLM解释的因果忠实性问题，为验证VLM的解释能力提供了一种自动化且有效的手段，有助于识别潜在的风险和改进模型的可信度。

Abstract: Vision-Language Models (VLMs) often produce fluent Natural Language
Explanations (NLEs) that sound convincing but may not reflect the causal
factors driving predictions. This mismatch of plausibility and faithfulness
poses technical and governance risks. We introduce Explanation-Driven
Counterfactual Testing (EDCT), a fully automated verification procedure for a
target VLM that treats the model's own explanation as a falsifiable hypothesis.
Given an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2)
parses the NLE into testable visual concepts, (3) generates targeted
counterfactual edits via generative inpainting, and (4) computes a
Counterfactual Consistency Score (CCS) using LLM-assisted analysis of changes
in both answers and explanations. Across 120 curated OK-VQA examples and
multiple VLMs, EDCT uncovers substantial faithfulness gaps and provides
regulator-aligned audit artifacts indicating when cited concepts fail causal
tests.

</details>


### [66] [HiDe: Rethinking The Zoom-IN method in High Resolution MLLMs via Hierarchical Decoupling](https://arxiv.org/abs/2510.00054)
*Xianjie Liu,Yiman Hu,Yixiong Zou,Liang Wu,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: MLLMs在高分辨率图像上表现不佳，本文发现主要问题是复杂背景干扰而非目标大小。为此，提出免训练的分层解耦框架HiDe（包含TAD和LPD），通过解耦关键信息与背景来提升性能，并在多个基准测试中实现新的SOTA，同时显著降低内存消耗。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在高分辨率图像上的性能仍不理想。现有观点常归因于感知限制或小目标识别困难，而本文分析发现，主要瓶颈是复杂背景干扰，而非目标大小。

Method: 提出免训练的分层解耦框架（HiDe）。该框架包含两部分：1) Token级注意力解耦（TAD），用于解耦问题tokens并识别关键信息tokens，利用其注意力权重与目标视觉区域精确对齐；2) 布局保留解耦（LPD），将目标区域从背景中解耦，重建紧凑表示，以保留关键空间布局并消除背景干扰。

Result: HiDe在V*Bench、HRBench4K和HRBench8K上均创造了新的SOTA，将Qwen2.5-VL 7B和InternVL3 8B的性能提升至SOTA（V*Bench上分别为92.1%和91.6%），甚至超越了强化学习方法。此外，经过优化后，HiDe的内存使用量比之前的免训练方法减少了75%。

Conclusion: 本研究揭示了MLLMs在高分辨率图像理解中面临的核心挑战是复杂背景干扰，而非小目标识别。所提出的HiDe框架通过有效解耦关键信息与背景，显著提升了MLLMs在高分辨率图像任务上的性能和效率，为相关研究提供了新视角和高效解决方案。

Abstract: Multimodal Large Language Models (MLLMs) have made significant strides in
visual understanding tasks. However, their performance on high-resolution
images remains suboptimal. While existing approaches often attribute this
limitation to perceptual constraints and argue that MLLMs struggle to recognize
small objects, leading them to use "zoom in" strategies for better detail, our
analysis reveals a different cause: the main issue is not object size, but
rather caused by complex background interference. We systematically analyze
this "zoom in" operation through a series of decoupling experiments and propose
the Hierarchical Decoupling Framework (HiDe), a training-free framework that
uses Token-wise Attention Decoupling (TAD) to decouple the question tokens and
identify the key information tokens, then leverages their attention weights to
achieve precise alignment with the target visual regions. Subsequently, it
employs Layout-Preserving Decoupling (LPD) to decouple these regions from the
background and reconstructs a compact representation that preserves essential
spatial layouts while eliminating background interference. HiDe sets a new SOTA
on V*Bench, HRBench4K, and HRBench8K, boosting Qwen2.5-VL 7B and InternVL3 8B
to SOTA (92.1% and 91.6% on V*Bench), even surpassing RL methods. After
optimization, HiDe uses 75% less memory than the previous training-free
approach. Code is provided in https://github.com/Tennine2077/HiDe.

</details>


### [67] [FSDENet: A Frequency and Spatial Domains based Detail Enhancement Network for Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2510.00059)
*Jiahao Fu,Yinfeng Yu,Liejun Wang*

Main category: cs.CV

TL;DR: 本文提出FSDENet，一个结合空间域和频域（FFT、Haar小波）的遥感图像分割网络，旨在解决灰度变化导致的边缘模糊问题，并在多个数据集上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决遥感图像分割中，灰度变化（如阴影、低对比度区域）引起的语义边缘模糊问题，并充分利用空间信息。

Method: 提出FSDENet。该网络通过空间处理提取多尺度空间特征和细粒度语义细节；利用快速傅里叶变换（FFT）整合全局和频域信息，增强模型在灰度变化下的全局表示能力；并采用Haar小波变换分解特征，利用其对边缘信息的敏感性来优化边界分割。

Result: FSDENet在LoveDA、Vaihingen、Potsdam和iSAID这四个广泛采用的数据集上均达到了最先进（SOTA）的性能。

Conclusion: FSDENet通过整合空间粒度和频域边缘敏感性，实现了双域协同，显著提高了边界区域和灰度过渡区域的分割精度。

Abstract: To fully leverage spatial information for remote sensing image segmentation
and address semantic edge ambiguities caused by grayscale variations (e.g.,
shadows and low-contrast regions), we propose the Frequency and Spatial Domains
based Detail Enhancement Network (FSDENet). Our framework employs spatial
processing methods to extract rich multi-scale spatial features and
fine-grained semantic details. By effectively integrating global and
frequency-domain information through the Fast Fourier Transform (FFT) in global
mappings, the model's capability to discern global representations under
grayscale variations is significantly strengthened. Additionally, we utilize
Haar wavelet transform to decompose features into high- and low-frequency
components, leveraging their distinct sensitivity to edge information to refine
boundary segmentation. The model achieves dual-domain synergy by integrating
spatial granularity with frequency-domain edge sensitivity, substantially
improving segmentation accuracy in boundary regions and grayscale transition
zones. Comprehensive experimental results demonstrate that FSDENet achieves
state-of-the-art (SOTA) performance on four widely adopted datasets: LoveDA,
Vaihingen, Potsdam, and iSAID.

</details>


### [68] [Less is More: Lean yet Powerful Vision-Language Model for Autonomous Driving](https://arxiv.org/abs/2510.00060)
*Sheng Yang,Tong Zhan,Guancheng Chen,Yanfeng Lu,Jian Wang*

Main category: cs.CV

TL;DR: Max-V1框架将自动驾驶重构为广义语言，利用VLM从摄像头输入端到端预测轨迹，在nuScenes上达SOTA并展现优异泛化性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过将自动驾驶视为广义语言，并将轨迹规划任务公式化为下一个路径点预测，开发一个高效、端到端的自动驾驶框架。

Method: 提出Max-V1，一个单阶段、端到端的自动驾驶框架。它采用单次生成范式，利用VLM（视觉-语言模型）的生成能力，直接从前视摄像头输入进行端到端轨迹预测。该方法以基于统计建模的监督策略为基础，并通过模仿学习从大规模专家演示中掌握复杂驾驶策略。

Result: 在nuScenes数据集上取得了最先进的性能，相比现有基线有超过30%的整体改进。此外，在来自不同车辆的跨域数据集上展现出卓越的泛化性能，证明了其在跨车辆鲁棒性和适应性方面的显著潜力。

Conclusion: 本工作引入了一个能够实现基本驾驶行为的模型，为开发更强大的自动驾驶智能体奠定了基础。

Abstract: In this work, we reconceptualize autonomous driving as a generalized language
and formulate the trajectory planning task as next waypoint prediction. We
introduce Max-V1, a novel framework for one-stage end-to-end autonomous
driving. Our framework presents a single-pass generation paradigm that aligns
with the inherent sequentiality of driving. This approach leverages the
generative capacity of the VLM (Vision-Language Model) to enable end-to-end
trajectory prediction directly from front-view camera input. The efficacy of
this method is underpinned by a principled supervision strategy derived from
statistical modeling. This provides a well-defined learning objective, which
makes the framework highly amenable to master complex driving policies through
imitation learning from large-scale expert demonstrations. Empirically, our
method achieves the state-of-the-art performance on the nuScenes dataset,
delivers an overall improvement of over 30% compared to prior baselines.
Furthermore, it exhibits superior generalization performance on cross-domain
datasets acquired from diverse vehicles, demonstrating notable potential for
cross-vehicle robustness and adaptability. Due to these empirical strengths,
this work introduces a model enabling fundamental driving behaviors, laying the
foundation for the development of more capable self-driving agents. Code will
be available upon publication.

</details>


### [69] [Efficient CNN Compression via Multi-method Low Rank Factorization and Feature Map Similarity](https://arxiv.org/abs/2510.00062)
*M. Kokhazadeh,G. Keramidas,V. Kelefouras*

Main category: cs.CV

TL;DR: 本文提出一种端到端的设计空间探索（DSE）方法和框架，用于卷积神经网络（CNN）压缩，解决了低秩分解（LRF）在最优秩选择、微调时间和兼容性等方面的挑战，并实现了显著的压缩效果和最小的精度损失。


<details>
  <summary>Details</summary>
Motivation: 低秩分解（LRF）在深度神经网络（DNN）压缩中面临诸多挑战，包括最优秩选择困难、设计空间庞大、微调时间过长以及对不同层类型和分解方法的兼容性有限。

Method: 提出一种基于特征图相似度的新颖秩选择策略；采用一次性微调过程以缩短时间；框架完全兼容所有卷积层和全连接层；集成了六种不同的LRF技术（卷积层3种，全连接层3种），并按层选择性应用；将工作集成到TensorFlow 2.x中。

Result: 结合多种LRF方法比单一方法效果更好；在14个CNN模型和8个数据集上的实验表明，该方法在实现显著压缩的同时保持了最小的精度损失，并优于多种现有技术。

Conclusion: 该端到端DSE方法和框架有效解决了LRF在CNN压缩中的主要问题，通过新颖的秩选择、快速微调、广泛兼容性和多技术集成，实现了卓越的压缩性能，并为LRF技术的实际应用提供了有价值的见解。

Abstract: Low-Rank Factorization (LRF) is a widely adopted technique for compressing
deep neural networks (DNNs). However, it faces several challenges, including
optimal rank selection, a vast design space, long fine-tuning times, and
limited compatibility with different layer types and decomposition methods.
This paper presents an end-to-end Design Space Exploration (DSE) methodology
and framework for compressing convolutional neural networks (CNNs) that
addresses all these issues. We introduce a novel rank selection strategy based
on feature map similarity, which captures non-linear interactions between layer
outputs more effectively than traditional weight-based approaches. Unlike prior
works, our method uses a one-shot fine-tuning process, significantly reducing
the overall fine-tuning time. The proposed framework is fully compatible with
all types of convolutional (Conv) and fully connected (FC) layers. To further
improve compression, the framework integrates three different LRF techniques
for Conv layers and three for FC layers, applying them selectively on a
per-layer basis. We demonstrate that combining multiple LRF methods within a
single model yields better compression results than using a single method
uniformly across all layers. Finally, we provide a comprehensive evaluation and
comparison of the six LRF techniques, offering practical insights into their
effectiveness across different scenarios. The proposed work is integrated into
TensorFlow 2.x, ensuring compatibility with widely used deep learning
workflows. Experimental results on 14 CNN models across eight datasets
demonstrate that the proposed methodology achieves substantial compression with
minimal accuracy loss, outperforming several state-of-the-art techniques.

</details>


### [70] [Intelligent 5S Audit: Application of Artificial Intelligence for Continuous Improvement in the Automotive Industry](https://arxiv.org/abs/2510.00067)
*Rafael da Silva Maciel,Lucio Veraldo Jr*

Main category: cs.CV

TL;DR: 本研究开发了一套基于大型语言模型和智能图像分析的自动化5S审计系统，用于提升汽车制造业5S审计的客观性和效率，经验证其结果与人工审计高度一致，且能显著提速并降低运营成本。


<details>
  <summary>Details</summary>
Motivation: 提高汽车产业链工业组织5S审计的客观性、效率，并使其符合工业4.0标准，以应对传统5S方法面临的挑战。

Method: 开发了一个基于大型语言模型（LLM）的自动化5S审计系统，通过智能图像分析标准化评估5S的各个要素（整理、整顿、清扫、清洁、素养）。

Result: 系统可靠性通过Cohen's kappa系数（0.75）验证，显示自动化评估与人工审计高度一致。审计过程提速50%，评估保持一致性，运营成本比传统人工审计降低99.8%。

Conclusion: 所提出的方法为精益系统与新兴AI技术的整合建立了新范式，对汽车制造环境的持续改进具有显著贡献，并具备在不同规模汽车工厂中实施的可扩展性。

Abstract: The evolution of the 5S methodology with the support of artificial
intelligence techniques represents a significant opportunity to improve
industrial organization audits in the automotive chain, making them more
objective, efficient and aligned with Industry 4.0 standards. This work
developed an automated 5S audit system based on large-scale language models
(LLM), capable of assessing the five senses (Seiri, Seiton, Seiso, Seiketsu,
Shitsuke) in a standardized way through intelligent image analysis. The
system's reliability was validated using Cohen's concordance coefficient (kappa
= 0.75), showing strong alignment between the automated assessments and the
corresponding human audits. The results indicate that the proposed solution
contributes significantly to continuous improvement in automotive manufacturing
environments, speeding up the audit process by 50% of the traditional time and
maintaining the consistency of the assessments, with a 99.8% reduction in
operating costs compared to traditional manual audits. The methodology
presented establishes a new paradigm for integrating lean systems with emerging
AI technologies, offering scalability for implementation in automotive plants
of different sizes.

</details>


### [71] [OIG-Bench: A Multi-Agent Annotated Benchmark for Multimodal One-Image Guides Understanding](https://arxiv.org/abs/2510.00069)
*Jiancong Xie,Wenjin Wang,Zhuomeng Zhang,Zihan Liu,Qi Liu,Ke Feng,Zixun Sun,Yuedong Yang*

Main category: cs.CV

TL;DR: 本文提出了OIG-Bench基准，用于评估多模态大语言模型（MLLMs）对“单图指南”（One-Image Guides）的理解能力，并开发了半自动化标注流程。研究发现当前MLLMs在语义理解和逻辑推理方面仍存在不足。


<details>
  <summary>Details</summary>
Motivation: 评估MLLMs在“单图指南”中类人理解能力的研究不足。“单图指南”是一种结合文本、图像和符号的视觉格式，专为人类观看设计，并体现了人类感知和理解的特性，是衡量MLLMs类人理解能力的关键。

Method: 构建了OIG-Bench，一个跨领域的“单图指南”理解综合基准。开发了一种半自动化标注流程，其中多个智能代理协同生成初步图像描述，辅助人类构建图像-文本对。使用OIG-Bench评估了29个最先进的MLLMs。

Result: Qwen2.5-VL-72B在所有评估模型中表现最佳，整体准确率为77%。然而，所有模型在语义理解和逻辑推理方面均表现出显著弱点。此外，提出的多代理标注系统在图像描述方面优于所有MLLMs。

Conclusion: 当前MLLMs在准确解释复杂的视觉-文本关系方面仍有挑战。多代理标注系统在生成高质量图像描述和未来数据集构建方面具有巨大潜力。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated
impressive capabilities. However, evaluating their capacity for human-like
understanding in One-Image Guides remains insufficiently explored. One-Image
Guides are a visual format combining text, imagery, and symbols to present
reorganized and structured information for easier comprehension, which are
specifically designed for human viewing and inherently embody the
characteristics of human perception and understanding. Here, we present
OIG-Bench, a comprehensive benchmark focused on One-Image Guide understanding
across diverse domains. To reduce the cost of manual annotation, we developed a
semi-automated annotation pipeline in which multiple intelligent agents
collaborate to generate preliminary image descriptions, assisting humans in
constructing image-text pairs. With OIG-Bench, we have conducted a
comprehensive evaluation of 29 state-of-the-art MLLMs, including both
proprietary and open-source models. The results show that Qwen2.5-VL-72B
performs the best among the evaluated models, with an overall accuracy of 77%.
Nevertheless, all models exhibit notable weaknesses in semantic understanding
and logical reasoning, indicating that current MLLMs still struggle to
accurately interpret complex visual-text relationships. In addition, we also
demonstrate that the proposed multi-agent annotation system outperforms all
MLLMs in image captioning, highlighting its potential as both a high-quality
image description generator and a valuable tool for future dataset
construction. Datasets are available at https://github.com/XiejcSYSU/OIG-Bench.

</details>


### [72] [Geo-R1: Unlocking VLM Geospatial Reasoning with Cross-View Reinforcement Learning](https://arxiv.org/abs/2510.00072)
*Chenhui Xu,Fuxun Yu,Michael J. Bianco,Jacob Kovarskiy,Raphael Tang,Qi Zhang,Zirui Xu,Will LeVine,Brandon Dubbs,Heming Liao,Cassandra Burgess,Suvam Bag,Jay Patravali,Rupanjali Kukal,Mikael Figueroa,Rishi Madhok,Nikolaos Karianakis,Jinjun Xiong*

Main category: cs.CV

TL;DR: Geo-R1是一个以推理为中心的后训练框架，它通过结合思维支架和提升阶段，在视觉-语言模型中解锁地理空间推理能力，并在相关基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的地理空间建模方法主要依赖于领域预训练或监督微调。本研究旨在通过引入推理优先的后训练框架，克服对昂贵人工推理标注的依赖，并提供可验证、可扩展的奖励信号，以实现视觉-语言模型更强的地理空间推理能力。

Method: Geo-R1框架包含两个阶段：1. **支架阶段**：通过在合成思维链示例上进行监督微调，灌输“地理空间思维范式”，使模型能够连接视觉线索与地理先验知识，避免了昂贵的人工推理标注。2. **提升阶段**：利用基于GRPO的强化学习，在弱监督的跨视图配对代理上进行训练，此设计提供可验证和可扩展的奖励信号，教导模型捕获和协调跨模态特征，并利用推理进行准确预测。

Result: Geo-R1在各种地理空间推理基准上取得了最先进的性能。

Conclusion: Geo-R1将地理空间建模从传统的领域预训练/监督微调扩展到以推理为中心的后训练范式，显著提升了视觉-语言模型进行地理空间推理的能力。

Abstract: We introduce Geo-R1, a reasoning-centric post-training framework that unlocks
geospatial reasoning in vision-language models by combining thinking
scaffolding and elevating. In the scaffolding stage, Geo-R1 instills a
``geospatial thinking paradigm" via supervised fine-tuning on synthetic
chain-of-thought exemplars, enabling models to connect visual cues with
geographic priors without costly human reasoning annotations. In the elevating
stage, it uses GRPO-based reinforcement learning on a weakly-supervised
cross-view pairing proxy. This design supplies a verifiable and scalable reward
signal: teaching models to capture and reconcile features across modalities,
and harnessing reasoning for accurate prediction. Geo-R1 extends geospatial
modeling from domain pretraining / supervised finetuning to reasoning-first
post-training, and achieves state-of-the-art performance across various
geospatial reasoning benchmarks. Our model is available at
https://huggingface.co/miniHui/Geo-R1.

</details>


### [73] [Enhancing Certifiable Semantic Robustness via Robust Pruning of Deep Neural Networks](https://arxiv.org/abs/2510.00083)
*Hanjiang Hu,Bowei Li,Ziwei Wang,Tianhao Wei,Casidhe Hutchison,Eric Sample,Changliu Liu*

Main category: cs.CV

TL;DR: 本文提出一种基于“无偏平滑神经元（USN）”度量和 Wasserstein 距离损失的神经网络剪枝方法，旨在通过减少过参数化来提高深度网络对语义变换扰动的可认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在视觉和机器人应用中广泛采用，但当前的可认证训练和鲁棒性认证方法面临过参数化挑战，导致鲁棒性认证的紧凑性和可扩展性不足，尤其在处理亮度、对比度等语义变换扰动时。

Method: 首先，分析神经元对输入扰动的稳定性和方差，提出一个基本的可认证鲁棒性指标——无偏平滑神经元（USN）。其次，基于USN提出一种新的神经网络剪枝方法，移除低USN神经元并保留高USN神经元，以在不过度参数化的情况下保持模型表达能力。最后，引入一种新的Wasserstein距离损失，确保剪枝后的神经元在各层之间更加集中。

Result: 通过在具有真实亮度与对比度扰动的鲁棒关键点检测任务上进行大量实验，验证了所提方法相比基线方法能实现卓越的鲁棒性认证性能和效率。

Conclusion: 所提出的基于USN的剪枝方法结合Wasserstein距离损失，有效解决了深度神经网络在语义变换扰动下的过参数化问题，显著提升了可认证鲁棒性的性能和效率。

Abstract: Deep neural networks have been widely adopted in many vision and robotics
applications with visual inputs. It is essential to verify its robustness
against semantic transformation perturbations, such as brightness and contrast.
However, current certified training and robustness certification methods face
the challenge of over-parameterization, which hinders the tightness and
scalability due to the over-complicated neural networks. To this end, we first
analyze stability and variance of layers and neurons against input
perturbation, showing that certifiable robustness can be indicated by a
fundamental Unbiased and Smooth Neuron metric (USN). Based on USN, we introduce
a novel neural network pruning method that removes neurons with low USN and
retains those with high USN, thereby preserving model expressiveness without
over-parameterization. To further enhance this pruning process, we propose a
new Wasserstein distance loss to ensure that pruned neurons are more
concentrated across layers. We validate our approach through extensive
experiments on the challenging robust keypoint detection task, which involves
realistic brightness and contrast perturbations, demonstrating that our method
achieves superior robustness certification performance and efficiency compared
to baselines.

</details>


### [74] [Improved Hyperspectral Anomaly Detection via Unsupervised Subspace Modeling in the Signed Cumulative Distribution Transform Domain](https://arxiv.org/abs/2510.00148)
*Abu Hasnat Mohammad Rubaiyat,Jordan Vincent,Colin Olson*

Main category: cs.CV

TL;DR: 本文提出一种新颖的高光谱异常检测（HAD）方法。该方法基于传输数学模型，将像素表示在有符号累积分布变换（SCDT）域中，并结合无监督子空间建模来检测异常，实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱异常检测（HAD）技术面临挑战，原因在于真实世界环境复杂以及对潜在异常信号的先验知识有限。

Method: 提出一种基于传输的数学模型来描述高光谱图像像素，将像素视为经过未知变形的模板模式，并在有符号累积分布变换（SCDT）域中表示。随后，利用无监督子空间建模技术构建背景信号模型，将与该模型偏差的信号检测为异常。

Result: 在五个不同数据集上进行的综合评估表明，本文方法优于现有最先进方法。

Conclusion: 所提出的基于传输模型的HAD方法有效应对了复杂环境下的异常检测挑战，并表现出优越的性能。

Abstract: Hyperspectral anomaly detection (HAD), a crucial approach for many civilian
and military applications, seeks to identify pixels with spectral signatures
that are anomalous relative to a preponderance of background signatures.
Significant effort has been made to improve HAD techniques, but challenges
arise due to complex real-world environments and, by definition, limited prior
knowledge of potential signatures of interest. This paper introduces a novel
HAD method by proposing a transport-based mathematical model to describe the
pixels comprising a given hyperspectral image. In this approach, hyperspectral
pixels are viewed as observations of a template pattern undergoing unknown
deformations that enables their representation in the signed cumulative
distribution transform (SCDT) domain. An unsupervised subspace modeling
technique is then used to construct a model of abundant background signals in
this domain, whereupon anomalous signals are detected as deviations from the
learned model. Comprehensive evaluations across five distinct datasets
illustrate the superiority of our approach compared to state-of-the-art
methods.

</details>


### [75] [MOLM: Mixture of LoRA Markers](https://arxiv.org/abs/2510.00293)
*Samar Fares,Nurbek Tastan,Noor Hussein,Karthik Nandakumar*

Main category: cs.CV

TL;DR: 为解决生成模型图像检测与溯源难题，本文提出MOLM（LoRA标记混合体），一个基于LoRA适配器的鲁棒、高效水印框架，能在保持图像质量的同时有效抵抗多种攻击。


<details>
  <summary>Details</summary>
Motivation: 生成模型生成逼真图像的能力引发了检测合成图像和追溯来源的紧迫担忧。现有水印方法对实际失真脆弱、易被自适应移除且密钥更新成本高昂。

Method: 提出一个通用水印框架，将编码问题定义为生成模型参数的密钥依赖扰动。在此框架下，引入MOLM，一种路由式实例化方法，通过二元密钥激活残差和注意力块中的轻量级LoRA适配器。

Result: 在Stable Diffusion和FLUX上的实验表明，MOLM在保持图像质量的同时，实现了对失真、压缩和再生、平均攻击以及针对提取器的黑盒对抗性攻击的鲁棒密钥恢复。

Conclusion: MOLM提供了一种有效且鲁棒的生成模型水印方案，能够抵抗多种攻击，并避免了密钥特定的重训练，解决了现有水印技术的痛点，同时保证了图像质量。

Abstract: Generative models can generate photorealistic images at scale. This raises
urgent concerns about the ability to detect synthetically generated images and
attribute these images to specific sources. While watermarking has emerged as a
possible solution, existing methods remain fragile to realistic distortions,
susceptible to adaptive removal, and expensive to update when the underlying
watermarking key changes. We propose a general watermarking framework that
formulates the encoding problem as key-dependent perturbation of the parameters
of a generative model. Within this framework, we introduce Mixture of LoRA
Markers (MOLM), a routing-based instantiation in which binary keys activate
lightweight LoRA adapters inside residual and attention blocks. This design
avoids key-specific re-training and achieves the desired properties such as
imperceptibility, fidelity, verifiability, and robustness. Experiments on
Stable Diffusion and FLUX show that MOLM preserves image quality while
achieving robust key recovery against distortions, compression and
regeneration, averaging attacks, and black-box adversarial attacks on the
extractor.

</details>


### [76] [Looking Beyond the Known: Towards a Data Discovery Guided Open-World Object Detection](https://arxiv.org/abs/2510.00303)
*Anay Majee,Amitesh Gangrade,Rishabh Iyer*

Main category: cs.CV

TL;DR: 本文提出了CROWD框架，通过组合式数据发现和表示学习，解决了开放世界目标检测中已知未知类别的语义混淆和灾难性遗忘问题，显著提升了检测准确性和未知召回率。


<details>
  <summary>Details</summary>
Motivation: 现有的开放世界目标检测（OWOD）方法存在已知和未知类别之间的语义混淆以及灾难性遗忘问题，导致未知召回率降低和已知类别准确性下降。

Method: 提出了组合式开放世界检测（CROWD）框架，将未知对象发现和适应重构为组合式（基于集合的）数据发现（CROWD-Discover）和表示学习（CROWD-Learn）任务。CROWD-Discover通过最大化次模条件增益（SCG）函数来挖掘未知实例。CROWD-Learn采用新颖的组合目标，联合解耦已知和未知表示，同时保持已知类别间的判别一致性。

Result: CROWD在M-OWODB和S-OWODB基准测试上，已知类别准确性分别提高了2.83%和2.05%，未知召回率相比主流基线提高了近2.4倍。

Conclusion: CROWD有效缓解了开放世界目标检测中的语义混淆和灾难性遗忘问题，显著提升了已知类别准确性和未知目标的召回率。

Abstract: Open-World Object Detection (OWOD) enriches traditional object detectors by
enabling continual discovery and integration of unknown objects via human
guidance. However, existing OWOD approaches frequently suffer from semantic
confusion between known and unknown classes, alongside catastrophic forgetting,
leading to diminished unknown recall and degraded known-class accuracy. To
overcome these challenges, we propose Combinatorial Open-World Detection
(CROWD), a unified framework reformulating unknown object discovery and
adaptation as an interwoven combinatorial (set-based) data-discovery
(CROWD-Discover) and representation learning (CROWD-Learn) task. CROWD-Discover
strategically mines unknown instances by maximizing Submodular Conditional Gain
(SCG) functions, selecting representative examples distinctly dissimilar from
known objects. Subsequently, CROWD-Learn employs novel combinatorial objectives
that jointly disentangle known and unknown representations while maintaining
discriminative coherence among known classes, thus mitigating confusion and
forgetting. Extensive evaluations on OWOD benchmarks illustrate that CROWD
achieves improvements of 2.83% and 2.05% in known-class accuracy on M-OWODB and
S-OWODB, respectively, and nearly 2.4x unknown recall compared to leading
baselines.

</details>


### [77] [Discrete Wavelet Transform as a Facilitator for Expressive Latent Space Representation in Variational Autoencoders in Satellite Imagery](https://arxiv.org/abs/2510.00376)
*Arpan Mahara,Md Rezaul Karim Khan,Naphtali Rishe,Wenjia Wang,Seyed Masoud Sadjadi*

Main category: cs.CV

TL;DR: 针对遥感领域LDM中VAE潜空间表示的不足，本文提出ExpDWT-VAE，通过融合DWT提取的频率特征与卷积处理的空间特征，构建更鲁棒的潜空间表示。


<details>
  <summary>Details</summary>
Motivation: 尽管许多研究致力于增强LDM，但鲜有针对其核心VAE潜空间进行显式改进，尤其在遥感应用中，这限制了模型的性能和效率。

Method: 提出ExpDWT-VAE。该方法包含双分支：一分支通过卷积处理空间域输入；另一分支通过2D Haar小波分解、卷积和逆DWT处理频率域特征。两分支合并生成集成空间-频率表示，并进一步通过卷积和对角高斯映射提炼为鲁棒潜表示。使用TerraFly卫星图像数据集进行验证。

Result: 在多个性能指标上的实验结果表明，所提出的方法能够有效增强潜空间表示。

Conclusion: ExpDWT-VAE通过创新性地结合空间与DWT频率特征，显著提升了VAE在卫星图像中的潜空间表示能力，为遥感领域的潜在扩散模型带来了重要改进。

Abstract: Latent Diffusion Models (LDM), a subclass of diffusion models, mitigate the
computational complexity of pixel-space diffusion by operating within a
compressed latent space constructed by Variational Autoencoders (VAEs),
demonstrating significant advantages in Remote Sensing (RS) applications.
Though numerous studies enhancing LDMs have been conducted, investigations
explicitly targeting improvements within the intrinsic latent space remain
scarce. This paper proposes an innovative perspective, utilizing the Discrete
Wavelet Transform (DWT) to enhance the VAE's latent space representation,
designed for satellite imagery. The proposed method, ExpDWT-VAE, introduces
dual branches: one processes spatial domain input through convolutional
operations, while the other extracts and processes frequency-domain features
via 2D Haar wavelet decomposition, convolutional operation, and inverse DWT
reconstruction. These branches merge to create an integrated spatial-frequency
representation, further refined through convolutional and diagonal Gaussian
mapping into a robust latent representation. We utilize a new satellite imagery
dataset housed by the TerraFly mapping system to validate our method.
Experimental results across several performance metrics highlight the efficacy
of the proposed method at enhancing latent space representation.

</details>


### [78] [EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations](https://arxiv.org/abs/2510.00405)
*Jiayi Liu,Jiaming Zhou,Ke Ye,Kun-Yu Lin,Allan Wang,Junwei Liang*

Main category: cs.CV

TL;DR: 本文引入EgoTraj-Bench基准和BiFlow双流模型，旨在解决机器人导航中以自我为中心的轨迹预测面临的感知噪声挑战，并通过去噪历史观测和预测未来运动，实现了鲁棒的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 在以人为中心的环境中，从自我视角进行可靠的轨迹预测对机器人导航至关重要。然而，现有方法通常假设理想的观测历史，未能解决第一人称视觉固有的感知伪影（如遮挡、ID切换、跟踪漂移），导致模型鲁棒性不足，限制了部署效果。

Method: ['引入EgoTraj-Bench：首个真实世界基准，将嘈杂的第一人称视觉历史与干净的鸟瞰图未来轨迹相结合，以实现在现实感知约束下的鲁棒学习。', '提出BiFlow：一个双流匹配模型，利用共享潜在表示，同时对历史观测进行去噪并预测未来运动。', '集成EgoAnchor机制：通过特征调制，利用提取出的历史特征来条件化预测解码器，以更好地建模智能体意图。']

Result: BiFlow实现了最先进的性能，平均将minADE和minFDE降低了10-15%，并展示了卓越的鲁棒性。

Conclusion: 该基准和模型为开发真正能应对真实世界自我中心感知挑战的轨迹预测系统提供了关键基础。

Abstract: Reliable trajectory prediction from an ego-centric perspective is crucial for
robotic navigation in human-centric environments. However, existing methods
typically assume idealized observation histories, failing to account for the
perceptual artifacts inherent in first-person vision, such as occlusions, ID
switches, and tracking drift. This discrepancy between training assumptions and
deployment reality severely limits model robustness. To bridge this gap, we
introduce EgoTraj-Bench, the first real-world benchmark that grounds noisy,
first-person visual histories in clean, bird's-eye-view future trajectories,
enabling robust learning under realistic perceptual constraints. Building on
this benchmark, we propose BiFlow, a dual-stream flow matching model that
concurrently denoises historical observations and forecasts future motion by
leveraging a shared latent representation. To better model agent intent, BiFlow
incorporates our EgoAnchor mechanism, which conditions the prediction decoder
on distilled historical features via feature modulation. Extensive experiments
show that BiFlow achieves state-of-the-art performance, reducing minADE and
minFDE by 10-15% on average and demonstrating superior robustness. We
anticipate that our benchmark and model will provide a critical foundation for
developing trajectory forecasting systems truly resilient to the challenges of
real-world, ego-centric perception.

</details>


### [79] [David and Goliath in Medical Vision: Convolutional Networks vs Biomedical Vision Language Models](https://arxiv.org/abs/2510.00411)
*Ran Tong,Jiaqi Liu,Su Liu,Jiexi Xu,Lanruo Wang,Tong Wang*

Main category: cs.CV

TL;DR: 本研究通过决策阈值校准，显著提升了零样本视觉语言模型（BiomedCLIP）在胸部X光诊断中的性能，使其在肺炎检测上超越了监督式CNN，在结核病检测上接近监督式CNN。


<details>
  <summary>Details</summary>
Motivation: 自动化方法准确解读胸部X光片在医学影像中至关重要。本文旨在比较监督式CNN与先进的零样本医学视觉语言模型在肺炎和结核病检测任务中的表现。

Method: 研究对比了监督式轻量级卷积神经网络（CNN）与零样本医学视觉语言模型（BiomedCLIP），在PneumoniaMNIST数据集上进行肺炎检测，在Shenzhen TB数据集上进行结核病检测。关键方法是引入决策阈值校准来优化VLM的性能。

Result: 监督式CNN在两项任务中均表现出强大的基线性能。零样本VLM的默认性能较低，但经过决策阈值校准后，其性能显著提升：在肺炎检测中，F1-score从0.8803提升至0.8841，超越了监督式CNN；在结核病检测中，F1-score从0.4812大幅提升至0.7684，接近监督式CNN的0.7834。

Conclusion: 正确的校准对于充分发挥零样本视觉语言模型在诊断中的潜力至关重要，使其能够与高效、任务特定的监督模型匹敌甚至超越。

Abstract: The accurate interpretation of chest radiographs using automated methods is a
critical task in medical imaging. This paper presents a comparative analysis
between a supervised lightweight Convolutional Neural Network (CNN) and a
state-of-the-art, zero-shot medical Vision-Language Model (VLM), BiomedCLIP,
across two distinct diagnostic tasks: pneumonia detection on the PneumoniaMNIST
benchmark and tuberculosis detection on the Shenzhen TB dataset. Our
experiments show that supervised CNNs serve as highly competitive baselines in
both cases. While the default zero-shot performance of the VLM is lower, we
demonstrate that its potential can be unlocked via a simple yet crucial remedy:
decision threshold calibration. By optimizing the classification threshold on a
validation set, the performance of BiomedCLIP is significantly boosted across
both datasets. For pneumonia detection, calibration enables the zero-shot VLM
to achieve a superior F1-score of 0.8841, surpassing the supervised CNN's
0.8803. For tuberculosis detection, calibration dramatically improves the
F1-score from 0.4812 to 0.7684, bringing it close to the supervised baseline's
0.7834. This work highlights a key insight: proper calibration is essential for
leveraging the full diagnostic power of zero-shot VLMs, enabling them to match
or even outperform efficient, task-specific supervised models.

</details>


### [80] [PAL-UI: Planning with Active Look-back for Vision-Based GUI Agents](https://arxiv.org/abs/2510.00413)
*Zikang Liu,Junyi Li,Wayne Xin Zhao,Dawei Gao,Yaliang Li,Ji-rong Wen*

Main category: cs.CV

TL;DR: MLLM驱动的GUI代理在长时任务中受限于记忆。本文提出PAL-UI框架，通过双层摘要和主动回溯机制，使代理能按需检索历史视觉信息，显著提升了移动GUI和Web导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM驱动的GUI代理在处理长时任务时，由于记忆限制，难以维持关键的视觉信息，导致决策失误。传统的历史截断或简单文本摘要方法不足以解决此问题。

Method: 提出PAL-UI（Planning with Active Look-back）框架，结合双层摘要代理（捕获观察级和动作级信息）与专用检索工具（允许代理在规划时回溯特定历史截图）。构建了8.6K步级指令数据集，并基于Qwen2.5-VL训练了PAL-UI-3B和PAL-UI-7B模型。

Result: PAL-UI在移动GUI导航任务中显著优于基线模型和现有方法，即使在数据效率低的设置下也表现出色。此外，PAL-UI在未经额外训练的情况下，在Web导航中也展现出强大的跨域泛化能力。

Conclusion: 主动记忆检索技术在增强基于视觉的GUI代理的长时规划能力方面具有巨大潜力。

Abstract: Graphical User Interface (GUI) agents powered by Multimodal Large Language
Models (MLLMs) promise human-like interaction with software applications, yet
long-horizon tasks remain challenging due to memory limitations. Existing
approaches either truncate history or rely on simple textual summaries, which
risk losing critical information when past visual details become necessary for
future decisions. In this paper, we propose \textbf{PAL-UI} (\textbf{P}lanning
with \textbf{A}ctive \textbf{L}ook-back), a novel framework that enables GUI
agents to adaptively retrieve past observations when required. PAL-UI combines
a dual-level summarization agent, capturing both observation-level cues and
action-level outcomes, with a dedicated retrieval tool that allows the agent to
recall specific historical screenshots during planning. We curate a step-level
instruction dataset of 8.6K samples from mobile GUI navigation trajectories and
train \textbf{PAL-UI-3B} and \textbf{PAL-UI-7B} models based on Qwen2.5-VL.
Extensive experiments demonstrate that PAL-UI significantly outperforms
baseline models and prior methods in mobile GUI navigation tasks, even under
data-efficient settings. Moreover, PAL-UI exhibits strong cross-domain
generalization, achieving notable improvements in web navigation without
additional training. Our work highlights the potential of active memory
retrieval for long-horizon planning capabilities of vision-based GUI agents.

</details>


### [81] [Domain-Specialized Interactive Segmentation Framework for Meningioma Radiotherapy Planning](https://arxiv.org/abs/2510.00416)
*Junhyeok Lee,Han Jang,Kyu Sung Choi*

Main category: cs.CV

TL;DR: 本文提出Interactive-MEN-RT，一个专为临床医生辅助脑膜瘤3D分割的交互式深度学习工具，在RT规划中展现出比其他方法更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 脑膜瘤的精确勾画对放疗（RT）规划至关重要，但由于肿瘤异质性，自动化深度学习难以达到持续准确。现有通用交互式分割工具缺乏针对脑膜瘤RT规划等临床关键和疾病特定任务所需的特异性。

Method: 引入Interactive-MEN-RT，一个专为RT工作流中临床医生辅助3D脑膜瘤分割而开发的交互式医学图像分割（IMIS）工具。该系统整合了多种临床相关的交互方法，包括点标注、边界框、套索工具和涂鸦。

Result: 在BraTS 2025脑膜瘤RT分割挑战赛的500份T1增强MRI扫描评估中，Interactive-MEN-RT相较于其他分割方法有显著改进，Dice相似系数高达77.6%，交并比（IoU）达到64.8%。

Conclusion: 研究结果强调了在脑膜瘤RT规划等关键应用中，对临床定制化分割解决方案的需求。Interactive-MEN-RT为这一特定任务提供了更准确的解决方案。

Abstract: Precise delineation of meningiomas is crucial for effective radiotherapy (RT)
planning, directly influencing treatment efficacy and preservation of adjacent
healthy tissues. While automated deep learning approaches have demonstrated
considerable potential, achieving consistently accurate clinical segmentation
remains challenging due to tumor heterogeneity. Interactive Medical Image
Segmentation (IMIS) addresses this challenge by integrating advanced AI
techniques with clinical input. However, generic segmentation tools, despite
widespread applicability, often lack the specificity required for clinically
critical and disease-specific tasks like meningioma RT planning. To overcome
these limitations, we introduce Interactive-MEN-RT, a dedicated IMIS tool
specifically developed for clinician-assisted 3D meningioma segmentation in RT
workflows. The system incorporates multiple clinically relevant interaction
methods, including point annotations, bounding boxes, lasso tools, and
scribbles, enhancing usability and clinical precision. In our evaluation
involving 500 contrast-enhanced T1-weighted MRI scans from the BraTS 2025
Meningioma RT Segmentation Challenge, Interactive-MEN-RT demonstrated
substantial improvement compared to other segmentation methods, achieving Dice
similarity coefficients of up to 77.6\% and Intersection over Union scores of
64.8\%. These results emphasize the need for clinically tailored segmentation
solutions in critical applications such as meningioma RT planning. The code is
publicly available at: https://github.com/snuh-rad-aicon/Interactive-MEN-RT

</details>


### [82] [BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration](https://arxiv.org/abs/2510.00438)
*Zhaoyang Li,Dongjun Qian,Kai Su,Qishuai Diao,Xiangyang Xia,Chang Liu,Wenfei Yang,Tianzhu Zhang,Zehuan Yuan*

Main category: cs.CV

TL;DR: BindWeave框架通过结合多模态大语言模型（MLLM）与Diffusion Transformer，解决了现有视频生成模型在处理复杂多主体场景时难以保持主体一致性的问题，显著提升了生成视频的主体一致性、自然度和文本相关性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散Transformer在生成高保真视频方面表现出色，但在处理包含复杂空间关系、时间逻辑和多主体交互的提示时，难以实现主体一致性视频生成，这是当前模型面临的固有难题。

Method: 提出BindWeave，一个统一的框架，涵盖从单主体到复杂多主体场景的广泛情境。核心是MLLM-DiT框架，其中预训练的多模态大语言模型进行深度跨模态推理，以锚定实体、解耦角色、属性和交互，生成主体感知隐藏状态，进而条件化扩散Transformer以实现高保真和主体一致的视频生成。

Result: 在OpenS2V基准测试上的实验表明，BindWeave在生成视频的主体一致性、自然度和文本相关性方面均达到了卓越性能，超越了现有开源和商业模型。

Conclusion: BindWeave通过创新的MLLM-DiT框架，有效解决了视频生成中的主体一致性难题，在复杂场景下生成了高质量、主体一致且与文本高度相关的视频，展现出优于现有模型的强大能力。

Abstract: Diffusion Transformer has shown remarkable abilities in generating
high-fidelity videos, delivering visually coherent frames and rich details over
extended durations. However, existing video generation models still fall short
in subject-consistent video generation due to an inherent difficulty in parsing
prompts that specify complex spatial relationships, temporal logic, and
interactions among multiple subjects. To address this issue, we propose
BindWeave, a unified framework that handles a broad range of subject-to-video
scenarios from single-subject cases to complex multi-subject scenes with
heterogeneous entities. To bind complex prompt semantics to concrete visual
subjects, we introduce an MLLM-DiT framework in which a pretrained multimodal
large language model performs deep cross-modal reasoning to ground entities and
disentangle roles, attributes, and interactions, yielding subject-aware hidden
states that condition the diffusion transformer for high-fidelity
subject-consistent video generation. Experiments on the OpenS2V benchmark
demonstrate that our method achieves superior performance across subject
consistency, naturalness, and text relevance in generated videos, outperforming
existing open-source and commercial models.

</details>


### [83] [Measuring and Controlling the Spectral Bias for Self-Supervised Image Denoising](https://arxiv.org/abs/2510.00454)
*Wang Zhang,Huaqiu Li,Xiaowan Hu,Tao Jiang,Zikang Chen,Haoqian Wang*

Main category: cs.CV

TL;DR: 本文提出SCNet用于自监督配对图像去噪，通过频谱控制策略，解决现有方法在高频细节保留不足和高频噪声学习的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自监督配对图像去噪方法存在两个局限性：一是图像高频结构细节保留不足；二是在拟合高频时，网络会学习到映射图像中的高频噪声。这些问题通过作者提出的图像对频带相似性进行频谱偏差测量后被发现。

Method: 引入了频谱控制网络（SCNet），包含三个主要部分：1. 噪声图像频带成分选择策略，以加速训练收敛；2. 利用Lipschitz常数限制卷积核学习高频噪声的参数优化方法；3. 频谱分离和低秩重建（SSR）模块，通过频域分离和低秩空间重建来分离噪声和高频细节，以保留图像的高频结构细节。

Result: 在合成和真实世界数据集上进行的实验验证了SCNet的有效性。

Conclusion: SCNet通过其创新的组件，成功优化了自监督配对图像去噪，有效解决了高频结构细节保留和高频噪声学习的问题。

Abstract: Current self-supervised denoising methods for paired noisy images typically
involve mapping one noisy image through the network to the other noisy image.
However, after measuring the spectral bias of such methods using our proposed
Image Pair Frequency-Band Similarity, it suffers from two practical
limitations. Firstly, the high-frequency structural details in images are not
preserved well enough. Secondly, during the process of fitting high
frequencies, the network learns high-frequency noise from the mapped noisy
images. To address these challenges, we introduce a Spectral Controlling
network (SCNet) to optimize self-supervised denoising of paired noisy images.
First, we propose a selection strategy to choose frequency band components for
noisy images, to accelerate the convergence speed of training. Next, we present
a parameter optimization method that restricts the learning ability of
convolutional kernels to high-frequency noise using the Lipschitz constant,
without changing the network structure. Finally, we introduce the Spectral
Separation and low-rank Reconstruction module (SSR module), which separates
noise and high-frequency details through frequency domain separation and
low-rank space reconstruction, to retain the high-frequency structural details
of images. Experiments performed on synthetic and real-world datasets verify
the effectiveness of SCNet.

</details>


### [84] [VLOD-TTA: Test-Time Adaptation of Vision-Language Object Detectors](https://arxiv.org/abs/2510.00458)
*Atif Belal,Heitor R. Medeiros,Marco Pedersoli,Eric Granger*

Main category: cs.CV

TL;DR: 本文提出VLOD-TTA框架，通过IoU加权熵和图像条件提示选择，有效提升视觉语言目标检测器（VLODs）在域偏移下的零样本识别性能。


<details>
  <summary>Details</summary>
Motivation: 尽管YOLO-World和Grounding DINO等VLODs在零样本识别方面表现出色，但它们在域偏移（domain shift）下的性能会显著下降。

Method: 引入VLOD-TTA测试时间适应（TTA）框架。该框架包含两部分：1) 提出IoU加权熵目标，将适应集中在空间连贯的提案簇上，减少孤立框的确认偏差。2) 引入图像条件提示选择，根据图像兼容性对提示进行排序，并将最具信息量的提示与检测器logits融合。

Result: 在多种分布偏移（包括风格化域、驾驶场景、低光照条件和常见损坏）下的基准测试表明，VLOD-TTA在YOLO-World和Grounding DINO这两个最先进的VLODs上均有效，并始终优于零样本和TTA基线方法。

Conclusion: VLOD-TTA框架通过其创新的适应机制，成功解决了VLODs在域偏移下性能下降的问题，提供了显著且一致的改进。

Abstract: Vision-language object detectors (VLODs) such as YOLO-World and Grounding
DINO achieve impressive zero-shot recognition by aligning region proposals with
text representations. However, their performance often degrades under domain
shift. We introduce VLOD-TTA, a test-time adaptation (TTA) framework for VLODs
that leverages dense proposal overlap and image-conditioned prompt scores.
First, an IoU-weighted entropy objective is proposed that concentrates
adaptation on spatially coherent proposal clusters and reduces confirmation
bias from isolated boxes. Second, image-conditioned prompt selection is
introduced, which ranks prompts by image-level compatibility and fuses the most
informative prompts with the detector logits. Our benchmarking across diverse
distribution shifts -- including stylized domains, driving scenes, low-light
conditions, and common corruptions -- shows the effectiveness of our method on
two state-of-the-art VLODs, YOLO-World and Grounding DINO, with consistent
improvements over the zero-shot and TTA baselines. Code :
https://github.com/imatif17/VLOD-TTA

</details>


### [85] [MathSticks: A Benchmark for Visual Symbolic Compositional Reasoning with Matchstick Puzzles](https://arxiv.org/abs/2510.00483)
*Yuheng Ji,Huajie Tan,Cheng Chi,Yijie Xu,Yuting Zhao,Enshen Zhou,Huaihai Lyu,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang,Xiaolong Zheng*

Main category: cs.CV

TL;DR: 引入了MathSticks基准，用于视觉符号组合推理，统一了视觉感知、符号操作和算术一致性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在视觉符号组合推理方面存在局限性，需要一个能统一视觉感知、符号操作和算术一致性的严格测试平台。

Method: 创建了MathSticks基准，包含1.4M个火柴棒方程实例，需通过移动一两根火柴来纠正，涵盖文本引导和纯视觉设置，并系统性地变化数字规模、移动复杂性、解的多样性和操作符。评估了14个视觉-语言模型。

Result: 评估发现现有视觉-语言模型存在显著局限：闭源模型仅在简单情况下成功，开源模型在视觉模式下失败，而人类准确率超过90%。

Conclusion: MathSticks被确立为推进视觉和符号组合推理的严格测试平台，揭示了当前模型在该领域能力不足。

Abstract: We introduce \textsc{MathSticks}, a benchmark for Visual Symbolic
Compositional Reasoning (VSCR), which unifies visual perception, symbolic
manipulation, and arithmetic consistency. Each task presents an incorrect
matchstick equation that must be corrected by moving one or two sticks under
strict conservation rules. The benchmark includes both text-guided and purely
visual settings, systematically covering digit scale, move complexity, solution
multiplicity, and operator variation, with 1.4M generated instances and a
curated test set. Evaluations of 14 vision--language models reveal substantial
limitations: closed-source models succeed only on simple cases, open-source
models fail in the visual regime, while humans exceed 90\% accuracy. These
findings establish \textsc{MathSticks} as a rigorous testbed for advancing
compositional reasoning across vision and symbols. Our code and dataset are
publicly available at https://github.com/Yuheng2000/MathSticks.

</details>


### [86] [Normal-Abnormal Guided Generalist Anomaly Detection](https://arxiv.org/abs/2510.00495)
*Yuexin Wang,Xiaolei Wang,Yizheng Gong,Jimin Xiao*

Main category: cs.CV

TL;DR: 提出一种名为NAGL的通用异常检测（GAD）新框架，首次利用正常和异常样本作为参考，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有通用异常检测（GAD）方法主要仅使用正常样本作为参考，忽视了现实世界中常可获得的异常样本的宝贵信息。因此，需要一个更实用的方法来同时利用正常和异常样本以指导跨领域异常检测。

Method: 提出Normal-Abnormal Generalist Learning (NAGL) 框架，包含两个关键组件：1) **Residual Mining (RM)**，用于从正常-异常参考残差中提取可迁移的异常模式；2) **Anomaly Feature Learning (AFL)**，通过残差映射自适应学习查询图像中的异常特征，以识别实例感知的异常。

Result: 在多个基准测试中进行的广泛实验表明，所提出的方法显著优于现有通用异常检测（GAD）方法。

Conclusion: 本研究首次在通用异常检测中采用正常和异常样本混合作为参考，有效利用两者实现了更准确、高效的跨领域异常检测。

Abstract: Generalist Anomaly Detection (GAD) aims to train a unified model on an
original domain that can detect anomalies in new target domains. Previous GAD
methods primarily use only normal samples as references, overlooking the
valuable information contained in anomalous samples that are often available in
real-world scenarios. To address this limitation, we propose a more practical
approach: normal-abnormal-guided generalist anomaly detection, which leverages
both normal and anomalous samples as references to guide anomaly detection
across diverse domains. We introduce the Normal-Abnormal Generalist Learning
(NAGL) framework, consisting of two key components: Residual Mining (RM) and
Anomaly Feature Learning (AFL). RM extracts abnormal patterns from
normal-abnormal reference residuals to establish transferable anomaly
representations, while AFL adaptively learns anomaly features in query images
through residual mapping to identify instance-aware anomalies. Our approach
effectively utilizes both normal and anomalous references for more accurate and
efficient cross-domain anomaly detection. Extensive experiments across multiple
benchmarks demonstrate that our method significantly outperforms existing GAD
approaches. This work represents the first to adopt a mixture of normal and
abnormal samples as references in generalist anomaly detection. The code and
datasets are available at https://github.com/JasonKyng/NAGL.

</details>


### [87] [Relative-Absolute Fusion: Rethinking Feature Extraction in Image-Based Iterative Method Selection for Solving Sparse Linear Systems](https://arxiv.org/abs/2510.00500)
*Kaiqi Zhang,Mingguan Yang,Dali Chang,Chun Chen,Yuxiang Zhang,Kexun He,Jing Zhao*

Main category: cs.CV

TL;DR: 本文提出RAF（Relative-Absolute Fusion），一种高效的特征提取技术，通过融合图像的相对特征和数值的绝对特征，解决现有图像选择方法中的特征模糊问题，显著提升稀疏线性系统求解方法的选择准确性，并达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 迭代方法选择对于解决稀疏线性系统至关重要，但由于缺乏鲁棒性。现有的图像基选择方法虽然有前景，但其特征提取可能将不同矩阵编码为相同图像表示，导致次优方法选择。

Method: 引入RAF（Relative-Absolute Fusion）技术，通过同时提取图像表示作为相对特征，并结合相应的数值作为绝对特征，实现全面的矩阵表示，从而防止不同矩阵间的特征模糊。

Result: 在SuiteSparse和BMCMat数据集上进行评估，RAF使稀疏线性系统求解时间减少0.08s-0.29s，比传统图像基选择方法快5.86%-11.50%，并实现了最先进（SOTA）的性能。

Conclusion: RAF通过独特的相对-绝对特征融合，有效解决了图像基选择方法中的特征模糊问题，显著提高了方法选择的准确性和效率，充分发挥了图像基选择方法的潜力。

Abstract: Iterative method selection is crucial for solving sparse linear systems
because these methods inherently lack robustness. Though image-based selection
approaches have shown promise, their feature extraction techniques might encode
distinct matrices into identical image representations, leading to the same
selection and suboptimal method. In this paper, we introduce RAF
(Relative-Absolute Fusion), an efficient feature extraction technique to
enhance image-based selection approaches. By simultaneously extracting and
fusing image representations as relative features with corresponding numerical
values as absolute features, RAF achieves comprehensive matrix representations
that prevent feature ambiguity across distinct matrices, thus improving
selection accuracy and unlocking the potential of image-based selection
approaches. We conducted comprehensive evaluations of RAF on SuiteSparse and
our developed BMCMat (Balanced Multi-Classification Matrix dataset),
demonstrating solution time reductions of 0.08s-0.29s for sparse linear
systems, which is 5.86%-11.50% faster than conventional image-based selection
approaches and achieves state-of-the-art (SOTA) performance. BMCMat is
available at https://github.com/zkqq/BMCMat.

</details>


### [88] [Affordance-Guided Diffusion Prior for 3D Hand Reconstruction](https://arxiv.org/abs/2510.00506)
*Naru Suzuki,Takehiko Ohkawa,Tatsuro Banno,Jihyun Lee,Ryosuke Furuta,Yoichi Sato*

Main category: cs.CV

TL;DR: 提出一种基于可供性（affordance）的扩散模型，通过结合视觉-语言模型（VLM）的上下文知识，有效解决了严重遮挡下3D手部姿态的精确重建问题。


<details>
  <summary>Details</summary>
Motivation: 当手部被自身或物体严重遮挡时，准确重建3D手部姿态极具挑战性。人类常通过利用上下文知识（如可供性）来解决这种模糊性，本研究受此启发。

Method: 提出一种生成式先验用于手部姿态细化，该方法采用基于扩散的生成模型。模型学习在可供性描述（由大型视觉-语言模型VLM推断）条件下的合理手部姿态分布，并通过可供性感知的文本描述（手-物体交互HOI）指导细化过程。

Result: 在HOGraspNet数据集（包含严重遮挡的3D手部-可供性数据集）上的实验表明，所提出的可供性引导细化方法显著改善了手部姿态估计，优于近期回归方法和缺乏上下文推理的扩散模型细化方法。

Conclusion: 利用从VLM推断的可供性知识指导的扩散模型，能有效地将遮挡区域细化为更准确、功能更一致的手部姿态，显著提升了严重遮挡场景下的3D手部姿态估计精度。

Abstract: How can we reconstruct 3D hand poses when large portions of the hand are
heavily occluded by itself or by objects? Humans often resolve such ambiguities
by leveraging contextual knowledge -- such as affordances, where an object's
shape and function suggest how the object is typically grasped. Inspired by
this observation, we propose a generative prior for hand pose refinement guided
by affordance-aware textual descriptions of hand-object interactions (HOI). Our
method employs a diffusion-based generative model that learns the distribution
of plausible hand poses conditioned on affordance descriptions, which are
inferred from a large vision-language model (VLM). This enables the refinement
of occluded regions into more accurate and functionally coherent hand poses.
Extensive experiments on HOGraspNet, a 3D hand-affordance dataset with severe
occlusions, demonstrate that our affordance-guided refinement significantly
improves hand pose estimation over both recent regression methods and
diffusion-based refinement lacking contextual reasoning.

</details>


### [89] [Efficient Multi-modal Large Language Models via Progressive Consistency Distillation](https://arxiv.org/abs/2510.00515)
*Zichen Wen,Shaobo Wang,Yufa Zhou,Junyuan Zhang,Qintong Zhang,Yifeng Gao,Zhaorun Chen,Bin Wang,Weijia Li,Conghui He,Linfeng Zhang*

Main category: cs.CV

TL;DR: 为解决多模态大模型（MLLMs）中视觉token计算资源消耗大、效率低的问题，本文提出了EPIC框架，通过渐进式一致性蒸馏（包括token和层一致性蒸馏）在压缩视觉token的同时降低学习难度，显著提升了模型的效率、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（MLLMs）中视觉token消耗大量计算资源，导致效率低下。现有视觉token压缩方法在提升效率的同时，却增加了模型的学习难度，因为模型参数空间难以快速适应由压缩引起的特征空间剧烈扰动。

Method: 本文提出EPIC（Efficient MLLMs via Progressive Consistency Distillation）渐进式学习框架。具体通过将token压缩引起的特征空间扰动分解为token维度和层维度，分别引入token一致性蒸馏和层一致性蒸馏。该方法旨在通过教师模型的指导和遵循渐进式学习轨迹来降低训练难度。

Result: 广泛的实验证明，所提出的EPIC框架具有卓越的有效性、强大的鲁棒性以及优秀的泛化能力。

Conclusion: EPIC框架通过其渐进式一致性蒸馏机制，成功解决了多模态大模型在视觉token压缩中效率提升与学习难度增加的矛盾，显著提升了模型的整体性能，包括效率、鲁棒性和泛化能力。

Abstract: Visual tokens consume substantial computational resources in multi-modal
large models (MLLMs), significantly compromising their efficiency. Recent works
have attempted to improve efficiency by compressing visual tokens during
training, either through modifications to model components or by introducing
additional parameters. However, they often overlook the increased learning
difficulty caused by such compression, as the model's parameter space struggles
to quickly adapt to the substantial perturbations in the feature space induced
by token compression. In this work, we propose to develop Efficient MLLMs via
Progressive Consistency Distillation (EPIC), a progressive learning framework.
Specifically, by decomposing the feature space perturbations introduced by
token compression along the token-wise and layer-wise dimensions, we introduce
token consistency distillation and layer consistency distillation,
respectively, aiming to reduce the training difficulty by leveraging guidance
from a teacher model and following a progressive learning trajectory. Extensive
experiments demonstrate the superior effectiveness, robustness, and
generalization capabilities of our proposed framework.

</details>


### [90] [CardioBench: Do Echocardiography Foundation Models Generalize Beyond the Lab?](https://arxiv.org/abs/2510.00520)
*Darya Taratynova,Ahmed Aly,Numan Saeed,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 该研究引入了CardioBench，一个用于评估超声心动图基础模型（FMs）的标准化基准，并揭示了不同模型家族的优势与局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型正在重塑医学影像领域，但它们在超声心动图中的应用仍有限。现有超声心动图特异性FMs缺乏统一的评估基准，且面临数据噪声、高帧冗余和公共数据集有限等挑战，大多数解决方案在私有数据上评估，限制了可比性。

Method: 研究引入了CardioBench，整合了八个公开数据集，构建了一个标准化套件，涵盖了四个回归任务和五个分类任务（包括功能、结构、诊断和视图识别）。在一致的零样本、探测和对齐协议下，评估了多种领先的FM，包括心脏特异性、生物医学和通用编码器。

Result: 研究发现不同模型家族有互补的优势：时间建模对于功能回归至关重要；检索功能在分布变化下提供了鲁棒性；领域特定的文本编码器能够捕获生理学上有意义的轴。通用编码器表现出强大的迁移能力，但在视图分类和细微病理识别等细粒度区分任务上仍存在不足。

Conclusion: CardioBench为超声心动图FMs提供了一个可复现的参考点，并为未来超声心动图基础模型的设计提供了可操作的见解。

Abstract: Foundation models (FMs) are reshaping medical imaging, yet their application
in echocardiography remains limited. While several echocardiography-specific
FMs have recently been introduced, no standardized benchmark exists to evaluate
them. Echocardiography poses unique challenges, including noisy acquisitions,
high frame redundancy, and limited public datasets. Most existing solutions
evaluate on private data, restricting comparability. To address this, we
introduce CardioBench, a comprehensive benchmark for echocardiography FMs.
CardioBench unifies eight publicly available datasets into a standardized suite
spanning four regression and five classification tasks, covering functional,
structural, diagnostic, and view recognition endpoints. We evaluate several
leading FM, including cardiac-specific, biomedical, and general-purpose
encoders, under consistent zero-shot, probing, and alignment protocols. Our
results highlight complementary strengths across model families: temporal
modeling is critical for functional regression, retrieval provides robustness
under distribution shift, and domain-specific text encoders capture
physiologically meaningful axes. General-purpose encoders transfer strongly and
often close the gap with probing, but struggle with fine-grained distinctions
like view classification and subtle pathology recognition. By releasing
preprocessing, splits, and public evaluation pipelines, CardioBench establishes
a reproducible reference point and offers actionable insights to guide the
design of future echocardiography foundation models.

</details>


### [91] [Cascaded Diffusion Framework for Probabilistic Coarse-to-Fine Hand Pose Estimation](https://arxiv.org/abs/2510.00527)
*Taeyun Woo,Jinah Park,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 提出一种粗到精的级联扩散框架，结合概率建模与多阶段优化，解决了3D手部姿态重建中的模糊性与不确定性问题，实现了最先进的性能并有效建模姿态分布。


<details>
  <summary>Details</summary>
Motivation: 现有3D手部姿态重建的确定性模型难以处理自遮挡和复杂手部关节导致的姿态模糊性，且无法捕捉姿态不确定性；而单阶段概率方法虽能建模姿态分布，但精度不足，缺乏细化能力。

Method: 提出一个粗到精的级联扩散框架。第一阶段是联合扩散模型，用于采样多样化的3D关节假设；第二阶段是Mesh Latent Diffusion Model (Mesh LDM)，根据关节样本重建3D手部网格。通过在学习到的潜在空间中用多样化的关节假设训练Mesh LDM，学习分布感知的关节-网格关系和鲁棒的手部先验，同时级联设计也增强了精度。

Result: 在FreiHAND和HO3Dv2数据集上的实验表明，该方法实现了最先进的性能，并能有效建模姿态分布。

Conclusion: 本方法通过结合概率建模与级联细化，有效解决了3D手部姿态重建中姿态模糊性和不确定性的挑战，提升了重建精度并能成功捕捉姿态分布。

Abstract: Deterministic models for 3D hand pose reconstruction, whether single-staged
or cascaded, struggle with pose ambiguities caused by self-occlusions and
complex hand articulations. Existing cascaded approaches refine predictions in
a coarse-to-fine manner but remain deterministic and cannot capture pose
uncertainties. Recent probabilistic methods model pose distributions yet are
restricted to single-stage estimation, which often fails to produce accurate 3D
reconstructions without refinement. To address these limitations, we propose a
coarse-to-fine cascaded diffusion framework that combines probabilistic
modeling with cascaded refinement. The first stage is a joint diffusion model
that samples diverse 3D joint hypotheses, and the second stage is a Mesh Latent
Diffusion Model (Mesh LDM) that reconstructs a 3D hand mesh conditioned on a
joint sample. By training Mesh LDM with diverse joint hypotheses in a learned
latent space, our framework learns distribution-aware joint-mesh relationships
and robust hand priors. Furthermore, the cascaded design mitigates the
difficulty of directly mapping 2D images to dense 3D poses, enhancing accuracy
through sequential refinement. Experiments on FreiHAND and HO3Dv2 demonstrate
that our method achieves state-of-the-art performance while effectively
modeling pose distributions.

</details>


### [92] [Forestpest-YOLO: A High-Performance Detection Framework for Small Forestry Pests](https://arxiv.org/abs/2510.00547)
*Aoduo Li,Peikai Lin,Jiancheng Li,Zhen Zhang,Shiting Wu,Zexiao Liang,Zhifa Jiang*

Main category: cs.CV

TL;DR: 针对遥感图像中森林害虫检测面临小目标、遮挡和背景复杂等挑战，本文提出Forestpest-YOLO框架，通过引入SPD-Conv、CSPOK和VarifocalLoss，在自建数据集上实现了领先的检测性能。


<details>
  <summary>Details</summary>
Motivation: 遥感图像在复杂林业环境中害虫检测对生态保护至关重要，但面临小目标、严重遮挡、与背景视觉相似等挑战，导致传统目标检测模型因丢失细粒度特征和无法处理数据不平衡而失效。

Method: 提出Forestpest-YOLO框架，基于YOLOv8架构并集成三项创新：1) 无损下采样模块SPD-Conv，保留小目标关键高分辨率细节；2) 跨阶段特征融合模块CSPOK，动态增强多尺度特征并抑制背景噪声；3) VarifocalLoss，优化训练目标，关注高质量和难分类样本。

Result: 在挑战性的自建ForestPest数据集上进行大量实验，结果表明Forestpest-YOLO达到了最先进的性能，在检测小型、被遮挡害虫方面有显著提升，并显著优于现有基线模型。

Conclusion: Forestpest-YOLO有效克服了复杂林业遥感图像中小型、被遮挡害虫检测的难题，为生态保护提供了强大的解决方案。

Abstract: Detecting agricultural pests in complex forestry environments using remote
sensing imagery is fundamental for ecological preservation, yet it is severely
hampered by practical challenges. Targets are often minuscule, heavily
occluded, and visually similar to the cluttered background, causing
conventional object detection models to falter due to the loss of fine-grained
features and an inability to handle extreme data imbalance. To overcome these
obstacles, this paper introduces Forestpest-YOLO, a detection framework
meticulously optimized for the nuances of forestry remote sensing. Building
upon the YOLOv8 architecture, our framework introduces a synergistic trio of
innovations. We first integrate a lossless downsampling module, SPD-Conv, to
ensure that critical high-resolution details of small targets are preserved
throughout the network. This is complemented by a novel cross-stage feature
fusion block, CSPOK, which dynamically enhances multi-scale feature
representation while suppressing background noise. Finally, we employ
VarifocalLoss to refine the training objective, compelling the model to focus
on high-quality and hard-to-classify samples. Extensive experiments on our
challenging, self-constructed ForestPest dataset demonstrate that
Forestpest-YOLO achieves state-of-the-art performance, showing marked
improvements in detecting small, occluded pests and significantly outperforming
established baseline models.

</details>


### [93] [Assessing Foundation Models for Mold Colony Detection with Limited Training Data](https://arxiv.org/abs/2510.00561)
*Henrik Pichler,Janis Keuper,Matthew Copping*

Main category: cs.CV

TL;DR: 本研究展示了数据高效的基础视觉模型（如MaskDINO）在霉菌菌落量化任务中，仅使用少量数据进行微调，即可达到与传统大量数据训练模型（如YoloV9）相媲美的性能，从而加速微生物自动化系统的开发。


<details>
  <summary>Details</summary>
Motivation: 量化培养皿上的霉菌菌落对评估室内空气质量至关重要，但传统自动化方法（如YoloV9的训练）需要大量手动标注数据，耗时费力。研究旨在证明在处理新的视觉任务时，不再需要详尽的数据标注。

Method: 研究构建了一个包含5000张培养皿图像的代表性数据集，并标注了边界框。同时，通过精心策划的子集（包含实例级掩膜）模拟了传统、少样本和低样本场景。研究将三个视觉基础模型与传统基线模型进行了比较，并使用反映真实世界需求的特定任务指标进行基准测试。

Result: MaskDINO模型在仅用150张图像进行微调的情况下，即可达到与经过大量训练的YoloV9模型接近的性能。即使只用25张图像，它也能保持有竞争力的表现，并对约70%的样本保持可靠性。结果表明，数据高效的基础模型仅需传统方法所需数据的一小部分，即可与其匹配。

Conclusion: 数据高效的基础模型能够实现微生物自动化系统更早的开发和更快的迭代改进，并且相比传统模型，它们具有更优的性能上限。

Abstract: The process of quantifying mold colonies on Petri dish samples is of critical
importance for the assessment of indoor air quality, as high colony counts can
indicate potential health risks and deficiencies in ventilation systems.
Conventionally the automation of such a labor-intensive process, as well as
other tasks in microbiology, relies on the manual annotation of large datasets
and the subsequent extensive training of models like YoloV9. To demonstrate
that exhaustive annotation is not a prerequisite anymore when tackling a new
vision task, we compile a representative dataset of 5000 Petri dish images
annotated with bounding boxes, simulating both a traditional data collection
approach as well as few-shot and low-shot scenarios with well curated subsets
with instance level masks. We benchmark three vision foundation models against
traditional baselines on task specific metrics, reflecting realistic real-world
requirements. Notably, MaskDINO attains near-parity with an extensively trained
YoloV9 model while finetuned only on 150 images, retaining competitive
performance with as few as 25 images, still being reliable on $\approx$ 70% of
the samples. Our results show that data-efficient foundation models can match
traditional approaches with only a fraction of the required data, enabling
earlier development and faster iterative improvement of automated
microbiological systems with a superior upper-bound performance than
traditional models would achieve.

</details>


### [94] [Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning](https://arxiv.org/abs/2510.00570)
*Minghao Yang,Ren Togo,Guang Li,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 本文提出一种基于LoRA的MoE框架，名为自适应共享专家（ASE），通过共享专家和细粒度专家设计，解决了多任务学习中知识共享低效和冗余适应的问题，提高了性能。


<details>
  <summary>Details</summary>
Motivation: 现有MoE多任务学习方法依赖单任务预训练骨干，在从单任务到多任务学习的转换过程中，存在冗余适应和知识共享效率低下的问题。

Method: 提出LoRA架构下的自适应共享专家（ASE）机制，其中共享专家被分配与稀疏专家联合归一化的路由计算门控权重，以促进STL到MTL的过渡、增强专家专业化和协作。此外，通过增加LoRA专家数量并按比例降低其秩，引入细粒度专家，在相似参数预算下实现更有效的知识共享。

Result: 在PASCAL-Context基准上的大量实验表明，ASE在不同配置下持续提升性能，并验证了细粒度设计对多任务学习的有效性。

Conclusion: ASE通过创新的共享和细粒度专家设计，有效解决了MoE多任务学习中的挑战，显著提升了性能和知识共享效率。

Abstract: Mixture-of-Experts (MoE) has emerged as a powerful framework for multi-task
learning (MTL). However, existing MoE-MTL methods often rely on single-task
pretrained backbones and suffer from redundant adaptation and inefficient
knowledge sharing during the transition from single-task to multi-task learning
(STL to MTL). To address these limitations, we propose adaptive shared experts
(ASE) within a low-rank adaptation (LoRA) based MoE, where shared experts are
assigned router-computed gating weights jointly normalized with sparse experts.
This design facilitates STL to MTL transition, enhances expert specialization,
and cooperation. Furthermore, we incorporate fine-grained experts by increasing
the number of LoRA experts while proportionally reducing their rank, enabling
more effective knowledge sharing under a comparable parameter budget. Extensive
experiments on the PASCAL-Context benchmark, under unified training settings,
demonstrate that ASE consistently improves performance across diverse
configurations and validates the effectiveness of fine-grained designs for MTL.

</details>


### [95] [Arbitrary Generative Video Interpolation](https://arxiv.org/abs/2510.00578)
*Guozhen Zhang,Haiguang Wang,Chunyu Wang,Yuan Zhou,Qinglin Lu,Limin Wang*

Main category: cs.CV

TL;DR: ArbInterp提出了一种新型生成式VFI框架，能够实现任意时间戳和任意长度的视频帧插值，解决了现有方法固定帧数限制的问题，并通过TaRoPE和分段生成策略实现高保真和无缝的时空连续性。


<details>
  <summary>Details</summary>
Motivation: 现有生成式视频帧插值(VFI)方法只能合成固定数量的中间帧，缺乏调整生成帧速率或总序列持续时间的灵活性。

Method: 该研究提出了ArbInterp框架。为支持任意时间戳插值，引入了时间戳感知旋转位置嵌入(TaRoPE)，通过调制时间RoPE中的位置使生成帧与目标标准化时间戳对齐。为实现任意长度插值，将长序列生成分解为分段帧合成，并设计了外观-运动解耦的条件策略，利用前一分段的端点强制外观一致性，并利用时间语义保持运动连贯性，确保跨分段的无缝时空过渡。

Result: 通过构建多尺度帧插值（2x至32x）的综合基准测试，结果表明ArbInterp在所有场景下均优于现有方法，具有更高的保真度和更无缝的时空连续性。

Conclusion: ArbInterp提供了一个灵活高效的VFI框架，克服了现有方法在固定帧数上的限制，实现了对任意时间戳和任意长度的视频帧插值，并在实验中展现出卓越的性能。

Abstract: Video frame interpolation (VFI), which generates intermediate frames from
given start and end frames, has become a fundamental function in video
generation applications. However, existing generative VFI methods are
constrained to synthesize a fixed number of intermediate frames, lacking the
flexibility to adjust generated frame rates or total sequence duration. In this
work, we present ArbInterp, a novel generative VFI framework that enables
efficient interpolation at any timestamp and of any length. Specifically, to
support interpolation at any timestamp, we propose the Timestamp-aware Rotary
Position Embedding (TaRoPE), which modulates positions in temporal RoPE to
align generated frames with target normalized timestamps. This design enables
fine-grained control over frame timestamps, addressing the inflexibility of
fixed-position paradigms in prior work. For any-length interpolation, we
decompose long-sequence generation into segment-wise frame synthesis. We
further design a novel appearance-motion decoupled conditioning strategy: it
leverages prior segment endpoints to enforce appearance consistency and
temporal semantics to maintain motion coherence, ensuring seamless
spatiotemporal transitions across segments. Experimentally, we build
comprehensive benchmarks for multi-scale frame interpolation (2x to 32x) to
assess generalizability across arbitrary interpolation factors. Results show
that ArbInterp outperforms prior methods across all scenarios with higher
fidelity and more seamless spatiotemporal continuity. Project website:
https://mcg-nju.github.io/ArbInterp-Web/.

</details>


### [96] [Color Models in Image Processing: A Review and Experimental Comparison](https://arxiv.org/abs/2510.00584)
*Muragul Muratbekova,Nuray Toganas,Ayan Igali,Maksat Shagyrov,Elnara Kadyrgali,Adilet Yerkin,Pakizar Shamoi*

Main category: cs.CV

TL;DR: 本文对现有色彩模型和空间进行了回顾与评估，探讨了它们的理论、计算属性及应用，并通过实验揭示了当前模型的不足，并指出HS*系列模型最符合人类感知。


<details>
  <summary>Details</summary>
Motivation: 色彩表示在计算机视觉和人机交互中至关重要，选择合适的色彩模型对各种应用至关重要。

Method: 回顾了RGB、CMYK、YUV、CIELAB、CIELUV等传统和知觉均匀模型，以及基于模糊的方法；进行了一系列实验，从设备依赖性、色度一致性和计算复杂度等方面评估色彩模型。

Result: 实验结果揭示了现有色彩模型中的不足，并表明HS*系列模型与人类感知最为吻合。审查还指出了不同模型的优缺点，并概述了开放性挑战和未来方向。

Conclusion: 本研究为图像处理、感知计算、数字媒体及其他色彩相关领域的研究人员提供了参考。

Abstract: Color representation is essential in computer vision and human-computer
interaction. There are multiple color models available. The choice of a
suitable color model is critical for various applications. This paper presents
a review of color models and spaces, analyzing their theoretical foundations,
computational properties, and practical applications. We explore traditional
models such as RGB, CMYK, and YUV, perceptually uniform spaces like CIELAB and
CIELUV, and fuzzy-based approaches as well. Additionally, we conduct a series
of experiments to evaluate color models from various perspectives, like device
dependency, chromatic consistency, and computational complexity. Our
experimental results reveal gaps in existing color models and show that the HS*
family is the most aligned with human perception. The review also identifies
key strengths and limitations of different models and outlines open challenges
and future directions This study provides a reference for researchers in image
processing, perceptual computing, digital media, and any other color-related
field.

</details>


### [97] [Multi-level Dynamic Style Transfer for NeRFs](https://arxiv.org/abs/2510.00592)
*Zesheng Li,Shuaibo Li,Wei Ma,Jianwei Guo,Hongbin Zha*

Main category: cs.CV

TL;DR: 本文提出MDS-NeRF，一种新颖的NeRF多级动态风格迁移方法，通过重新设计管道和引入动态风格注入模块，显著提升了内容保留和艺术风格化的效果，并支持全视角风格迁移。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF风格迁移方法通常将风格统计数据集成到原始NeRF管道中，导致在内容保留和艺术风格化方面表现不佳。

Method: MDS-NeRF重新设计了NeRF管道以进行风格化，并引入了创新的动态风格注入模块。具体包括：提出一个多级特征适配器，从内容辐射场生成多级特征网格表示，有效捕捉多尺度空间结构；一个动态风格注入模块，学习提取相关风格特征并自适应地集成到内容模式中；一个多级级联解码器，将风格化的多级特征转换为最终风格化视图。该方法还扩展支持使用3D风格参考进行全视角风格迁移。

Result: MDS-NeRF在3D风格迁移方面取得了出色的性能，在有效传递风格特征的同时，能够很好地保留多尺度空间结构。

Conclusion: MDS-NeRF通过其创新的管道重新设计和动态风格注入模块，克服了现有NeRF风格迁移方法的局限性，实现了内容保留和风格化效果的显著提升，并支持更广泛的全视角风格迁移应用。

Abstract: As the application of neural radiance fields (NeRFs) in various 3D vision
tasks continues to expand, numerous NeRF-based style transfer techniques have
been developed. However, existing methods typically integrate style statistics
into the original NeRF pipeline, often leading to suboptimal results in both
content preservation and artistic stylization. In this paper, we present
multi-level dynamic style transfer for NeRFs (MDS-NeRF), a novel approach that
reengineers the NeRF pipeline specifically for stylization and incorporates an
innovative dynamic style injection module. Particularly, we propose a
multi-level feature adaptor that helps generate a multi-level feature grid
representation from the content radiance field, effectively capturing the
multi-scale spatial structure of the scene. In addition, we present a dynamic
style injection module that learns to extract relevant style features and
adaptively integrates them into the content patterns. The stylized multi-level
features are then transformed into the final stylized view through our proposed
multi-level cascade decoder. Furthermore, we extend our 3D style transfer
method to support omni-view style transfer using 3D style references. Extensive
experiments demonstrate that MDS-NeRF achieves outstanding performance for 3D
style transfer, preserving multi-scale spatial structures while effectively
transferring stylistic characteristics.

</details>


### [98] [LVLMs as inspectors: an agentic framework for category-level structural defect annotation](https://arxiv.org/abs/2510.00603)
*Sheng Jiang,Yuanmin Ning,Bingxi Huang,Peiyin Chen,Zhaohui Chen*

Main category: cs.CV

TL;DR: 本文提出一种名为ADPT的Agentic框架，通过整合LVLMs、语义模式匹配和自问答机制，实现了结构缺陷的自动化、无监督高精度标注，用于构建高质量缺陷数据集。


<details>
  <summary>Details</summary>
Motivation: 确保基础设施安全需要进行结构缺陷标注，但传统人工标注成本高昂且效率低下，急需一种自动化解决方案来克服这些限制。

Method: 引入了Agent-based Defect Pattern Tagger (ADPT) 框架。该框架结合了大型视觉语言模型（LVLMs）、语义模式匹配模块和迭代自问答完善机制。ADPT利用优化的领域特定提示和递归验证过程，将原始视觉数据自动转化为高质量、语义标注的缺陷数据集，无需任何人工监督。

Result: 实验结果显示，ADPT在区分缺陷和非缺陷图像方面取得了高达98%的准确率；在类别平衡设置下，对四种缺陷类别的标注准确率达85%-98%；在类别不平衡数据集上，准确率为80%-92%。

Conclusion: ADPT框架提供了一个可扩展且经济高效的高保真数据集构建解决方案。它为结构损伤评估中的迁移学习和领域适应等下游任务提供了强有力的支持。

Abstract: Automated structural defect annotation is essential for ensuring
infrastructure safety while minimizing the high costs and inefficiencies of
manual labeling. A novel agentic annotation framework, Agent-based Defect
Pattern Tagger (ADPT), is introduced that integrates Large Vision-Language
Models (LVLMs) with a semantic pattern matching module and an iterative
self-questioning refinement mechanism. By leveraging optimized domain-specific
prompting and a recursive verification process, ADPT transforms raw visual data
into high-quality, semantically labeled defect datasets without any manual
supervision. Experimental results demonstrate that ADPT achieves up to 98%
accuracy in distinguishing defective from non-defective images, and 85%-98%
annotation accuracy across four defect categories under class-balanced
settings, with 80%-92% accuracy on class-imbalanced datasets. The framework
offers a scalable and cost-effective solution for high-fidelity dataset
construction, providing strong support for downstream tasks such as transfer
learning and domain adaptation in structural damage assessment.

</details>


### [99] [Disentangling Foreground and Background for vision-Language Navigation via Online Augmentation](https://arxiv.org/abs/2510.00604)
*Yunbo Xu,Xuesong Zhang,Jia Li,Zhenzhen Hu,Richang Hong*

Main category: cs.CV

TL;DR: 提出COFA策略，利用前景和背景视觉特征增强视觉-语言导航（VLN）代理的泛化能力，并通过共识驱动的在线增强实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 在视觉-语言导航（VLN）任务中，多方面视觉表示虽有进展，但前景和背景在视觉观察中的作用尚未被充分探索。直观上，前景提供语义线索，背景提供空间连通性，二者的结合有望提升代理在未知环境中的可导航泛化能力。

Method: 提出共识驱动的在线特征增强策略（COFA）。首先，利用语义增强的地标识别将前景和背景分离为候选增强特征；其次，通过共识驱动的在线增强策略，鼓励代理根据不同指令和导航位置，整合两阶段投票结果来确定特征偏好。

Result: 在REVERIE和R2R数据集上的实验表明，所提出的在线前景-背景增强策略显著提升了基线模型的泛化能力，并取得了最先进的性能。

Conclusion: 通过有效利用前景和背景的互补视觉信息，COFA策略显著提高了VLN代理在复杂未知环境中的导航泛化能力。

Abstract: Following language instructions, vision-language navigation (VLN) agents are
tasked with navigating unseen environments. While augmenting multifaceted
visual representations has propelled advancements in VLN, the significance of
foreground and background in visual observations remains underexplored.
Intuitively, foreground regions provide semantic cues, whereas the background
encompasses spatial connectivity information. Inspired on this insight, we
propose a Consensus-driven Online Feature Augmentation strategy (COFA) with
alternative foreground and background features to facilitate the navigable
generalization. Specifically, we first leverage semantically-enhanced landmark
identification to disentangle foreground and background as candidate augmented
features. Subsequently, a consensus-driven online augmentation strategy
encourages the agent to consolidate two-stage voting results on feature
preferences according to diverse instructions and navigational locations.
Experiments on REVERIE and R2R demonstrate that our online
foreground-background augmentation boosts the generalization of baseline and
attains state-of-the-art performance.

</details>


### [100] [Robust Context-Aware Object Recognition](https://arxiv.org/abs/2510.00618)
*Klara Janouskova,Cristian Gavrus,Jiri Matas*

Main category: cs.CV

TL;DR: 本文提出RCOR，一种新的视觉识别方法，通过将定位作为识别的组成部分，解决了模型过度依赖背景的问题，首次在不牺牲鲁棒性或上下文感知能力的情况下实现了两者兼顾。


<details>
  <summary>Details</summary>
Motivation: 在视觉识别中，模型常因过度依赖背景（捷径学习）而导致鲁棒性不足。现有方法通常通过抑制背景来提高泛化性，但这会牺牲宝贵的上下文信息。

Method: RCOR（Robust Context-Aware Object Recognition）方法将定位作为识别的组成部分，以实现对象中心和上下文感知建模的解耦，随后进行鲁棒的非参数融合，从而同时实现鲁棒性和上下文感知。

Result: RCOR显著提高了监督模型和VLM在包含域内和域外背景数据集上的性能，甚至无需微调。研究结果证实，即使在ImageNet-1k等复杂场景中，先定位后识别也是可行的。

Conclusion: RCOR是首个在不妥协的情况下，共同实现鲁棒性和上下文感知的视觉识别方法，证明了在复杂场景中先进行定位再识别策略的有效性。

Abstract: In visual recognition, both the object of interest (referred to as
foreground, FG, for simplicity) and its surrounding context (background, BG)
play an important role. However, standard supervised learning often leads to
unintended over-reliance on the BG, known as shortcut learning of spurious
correlations, limiting model robustness in real-world deployment settings. In
the literature, the problem is mainly addressed by suppressing the BG,
sacrificing context information for improved generalization.
  We propose RCOR -- Robust Context-Aware Object Recognition -- the first
approach that jointly achieves robustness and context-awareness without
compromising either. RCOR treats localization as an integral part of
recognition to decouple object-centric and context-aware modelling, followed by
a robust, non-parametric fusion. It improves the performance of both supervised
models and VLM on datasets with both in-domain and out-of-domain BG, even
without fine-tuning. The results confirm that localization before recognition
is now possible even in complex scenes as in ImageNet-1k.

</details>


### [101] [UCD: Unconditional Discriminator Promotes Nash Equilibrium in GANs](https://arxiv.org/abs/2510.00624)
*Mengfei Xia,Nan Xue,Jiapeng Zhu,Yujun Shen*

Main category: cs.CV

TL;DR: 本文提出无条件判别器（UCD）来解决GAN训练中的收敛和模式崩溃问题。通过强制判别器提取无条件注入的鲁棒特征，UCD显著提升了生成性能，并在ImageNet-64上取得了1.47 FID的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 对抗训练是单步生成（特别是GAN和扩散模型蒸馏）的关键，但GAN训练实践中存在难以收敛和模式崩溃的问题。研究发现，判别器（D）中条件输入引入的冗余捷径会阻碍有意义的知识提取。

Method: ['定量分析GAN训练中纳什均衡的程度，发现判别器D中的条件输入会引入冗余捷径，导致无法有效提取知识。', '提出无条件判别器（UCD），强制D在没有条件注入的情况下提取更全面、更鲁棒的特征，从而更好地监督生成器（G）并促进纳什均衡。', '提供了与传统GAN理论兼容的理论保证，表明UCD可以即插即用方式实现。']

Result: ['通过大量实验证实，UCD显著提高了GAN的性能和效率。', '在ImageNet-64数据集上取得了1.47 FID的成绩，超越了StyleGAN-XL和多个最先进的单步扩散模型。']

Conclusion: UCD通过解决条件判别器中存在的知识提取不足问题，有效地促进了GAN训练的纳什均衡，显著提升了生成性能，并且具有易于集成的即插即用特性。

Abstract: Adversarial training turns out to be the key to one-step generation,
especially for Generative Adversarial Network (GAN) and diffusion model
distillation. Yet in practice, GAN training hardly converges properly and
struggles in mode collapse. In this work, we quantitatively analyze the extent
of Nash equilibrium in GAN training, and conclude that redundant shortcuts by
inputting condition in $D$ disables meaningful knowledge extraction. We thereby
propose to employ an unconditional discriminator (UCD), in which $D$ is
enforced to extract more comprehensive and robust features with no condition
injection. In this way, $D$ is able to leverage better knowledge to supervise
$G$, which promotes Nash equilibrium in GAN literature. Theoretical guarantee
on compatibility with vanilla GAN theory indicates that UCD can be implemented
in a plug-in manner. Extensive experiments confirm the significant performance
improvements with high efficiency. For instance, we achieved \textbf{1.47 FID}
on the ImageNet-64 dataset, surpassing StyleGAN-XL and several state-of-the-art
one-step diffusion models. The code will be made publicly available.

</details>


### [102] [Virtual Fashion Photo-Shoots: Building a Large-Scale Garment-Lookbook Dataset](https://arxiv.org/abs/2510.00633)
*Yannick Hauri,Luca A. Lanzendörfer,Till Aczel*

Main category: cs.CV

TL;DR: 引入虚拟时尚大片拍摄任务，并构建首个大规模服装-画册配对数据集，以实现丰富、语境化的时尚图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有时尚图像生成主要集中在虚拟试穿等狭窄任务，无法捕捉时尚大片中动态姿势、多样场景和精心设计的视觉叙事所呈现的丰富性。

Method: 提出虚拟时尚大片拍摄任务，旨在将标准化服装图像转换为语境化的时尚大片。为此，构建了首个大规模服装-画册配对数据集，并通过结合视觉-语言推理和物体级定位的自动化检索流程来对齐跨领域的服装。

Result: 构建了一个包含三种准确度级别（高质量1万对、中等质量5万对、低质量30万对）的服装-画册配对数据集。

Conclusion: 该数据集为开发超越目录风格、生成具有创造力、氛围感和故事性的时尚图像的模型奠定了基础。

Abstract: Fashion image generation has so far focused on narrow tasks such as virtual
try-on, where garments appear in clean studio environments. In contrast,
editorial fashion presents garments through dynamic poses, diverse locations,
and carefully crafted visual narratives. We introduce the task of virtual
fashion photo-shoot, which seeks to capture this richness by transforming
standardized garment images into contextually grounded editorial imagery. To
enable this new direction, we construct the first large-scale dataset of
garment-lookbook pairs, bridging the gap between e-commerce and fashion media.
Because such pairs are not readily available, we design an automated retrieval
pipeline that aligns garments across domains, combining visual-language
reasoning with object-level localization. We construct a dataset with three
garment-lookbook pair accuracy levels: high quality (10,000 pairs), medium
quality (50,000 pairs), and low quality (300,000 pairs). This dataset offers a
foundation for models that move beyond catalog-style generation and toward
fashion imagery that reflects creativity, atmosphere, and storytelling.

</details>


### [103] [LAKAN: Landmark-assisted Adaptive Kolmogorov-Arnold Network for Face Forgery Detection](https://arxiv.org/abs/2510.00634)
*Jiayao Jiang,Siran Peng,Bin Liu,Qi Chu,Nenghai Yu*

Main category: cs.CV

TL;DR: 提出LAKAN人脸伪造检测方法，结合Kolmogorov-Arnold Network (KAN) 和人脸地标，有效捕捉复杂伪造特征并提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法（如CNN和Transformer）在建模伪造痕迹的复杂非线性特性方面仍有不足，需要更鲁棒的算法。

Method: 核心是基于Kolmogorov-Arnold Network (KAN) 的检测方法，通过可学习样条替代固定激活函数以更好地处理复杂性。进一步引入Landmark-assisted Adaptive KAN (LAKAN) 模块，利用人脸地标作为结构先验，动态生成KAN参数，引导网络关注含伪造痕迹的关键面部区域。

Result: 在多个公共数据集上的广泛实验表明，所提出的方法实现了卓越的检测性能。

Conclusion: 结合KAN的强大建模能力和人脸地标的几何先验，LAKAN能够更有效地处理深度伪造的复杂非线性特征，显著提升了人脸伪造检测的准确性和鲁棒性。

Abstract: The rapid development of deepfake generation techniques necessitates robust
face forgery detection algorithms. While methods based on Convolutional Neural
Networks (CNNs) and Transformers are effective, there is still room for
improvement in modeling the highly complex and non-linear nature of forgery
artifacts. To address this issue, we propose a novel detection method based on
the Kolmogorov-Arnold Network (KAN). By replacing fixed activation functions
with learnable splines, our KAN-based approach is better suited to this
challenge. Furthermore, to guide the network's focus towards critical facial
areas, we introduce a Landmark-assisted Adaptive Kolmogorov-Arnold Network
(LAKAN) module. This module uses facial landmarks as a structural prior to
dynamically generate the internal parameters of the KAN, creating an
instance-specific signal that steers a general-purpose image encoder towards
the most informative facial regions with artifacts. This core innovation
creates a powerful combination between geometric priors and the network's
learning process. Extensive experiments on multiple public datasets show that
our proposed method achieves superior performance.

</details>


### [104] [Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack](https://arxiv.org/abs/2510.00635)
*Nanxiang Jiang,Zhaoxin Fan,Enhan Kang,Daiheng Gao,Yun Zhou,Yanxia Chang,Zheng Zhu,Yeying Jin,Wenjun Wu*

Main category: cs.CV

TL;DR: 本文提出ReFlux，首个专门用于评估整流流变压器T2I模型（如Flux）中概念擦除鲁棒性的攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法及其评估主要针对Stable Diffusion，在下一代整流流变压器（如Flux）上效果有限。需要一种专门针对此类模型（利用其注意力局部化现象）的攻击方法来评估概念擦除的有效性。

Method: ReFlux提出一种攻击策略，通过反注意力优化策略重新激活被抑制的信号并稳定注意力，辅以速度引导动态增强概念激活的鲁棒性，以及一致性保持目标来维护全局布局和无关内容。

Result: 广泛实验证明了所提攻击方法ReFlux的有效性和效率，为评估整流流变压器中概念擦除策略的鲁棒性建立了可靠的基准。

Conclusion: ReFlux成功地攻击了整流流变压器中的概念擦除，提供了一个评估此类模型安全机制鲁棒性的重要工具和可靠基准。

Abstract: Recent advances in text-to-image (T2I) diffusion models have enabled
impressive generative capabilities, but they also raise significant safety
concerns due to the potential to produce harmful or undesirable content. While
concept erasure has been explored as a mitigation strategy, most existing
approaches and corresponding attack evaluations are tailored to Stable
Diffusion (SD) and exhibit limited effectiveness when transferred to
next-generation rectified flow transformers such as Flux. In this work, we
present ReFlux, the first concept attack method specifically designed to assess
the robustness of concept erasure in the latest rectified flow-based T2I
framework. Our approach is motivated by the observation that existing concept
erasure techniques, when applied to Flux, fundamentally rely on a phenomenon
known as attention localization. Building on this insight, we propose a simple
yet effective attack strategy that specifically targets this property. At its
core, a reverse-attention optimization strategy is introduced to effectively
reactivate suppressed signals while stabilizing attention. This is further
reinforced by a velocity-guided dynamic that enhances the robustness of concept
reactivation by steering the flow matching process, and a
consistency-preserving objective that maintains the global layout and preserves
unrelated content. Extensive experiments consistently demonstrate the
effectiveness and efficiency of the proposed attack method, establishing a
reliable benchmark for evaluating the robustness of concept erasure strategies
in rectified flow transformers.

</details>


### [105] [FIN: Fast Inference Network for Map Segmentation](https://arxiv.org/abs/2510.00651)
*Ruan Bispo,Tim Brophy,Reenu Mohandas,Anthony Scanlan,Ciarán Eising*

Main category: cs.CV

TL;DR: 提出了一种基于相机-雷达融合的高效实时BEV地图分割架构，显著提升了推理速度并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中多传感器融合（尤其是相机-雷达）因其成本效益和信息互补性而日益普及。地图分割是关键任务，但仍面临高精度和实时性能的挑战。

Method: 本研究提出了一种新颖高效的BEV空间相机-雷达融合地图分割架构。通过采用先进的损失函数集和轻量级头部模块，模型旨在实现高精度、类别平衡和实时推理。

Result: 该方法取得了与大型模型相当的53.5 mIoU，并将推理时间比最强基线模型提高了260%，为推理时间设定了新基准。

Conclusion: 所提出的架构在BEV地图分割任务中，通过相机-雷达融合实现了高精度和卓越的实时性能，树立了新的行业标准。

Abstract: Multi-sensor fusion in autonomous vehicles is becoming more common to offer a
more robust alternative for several perception tasks. This need arises from the
unique contribution of each sensor in collecting data: camera-radar fusion
offers a cost-effective solution by combining rich semantic information from
cameras with accurate distance measurements from radar, without incurring
excessive financial costs or overwhelming data processing requirements. Map
segmentation is a critical task for enabling effective vehicle behaviour in its
environment, yet it continues to face significant challenges in achieving high
accuracy and meeting real-time performance requirements. Therefore, this work
presents a novel and efficient map segmentation architecture, using cameras and
radars, in the \acrfull{bev} space. Our model introduces a real-time map
segmentation architecture considering aspects such as high accuracy, per-class
balancing, and inference time. To accomplish this, we use an advanced loss set
together with a new lightweight head to improve the perception results. Our
results show that, with these modifications, our approach achieves results
comparable to large models, reaching 53.5 mIoU, while also setting a new
benchmark for inference time, improving it by 260\% over the strongest baseline
models.

</details>


### [106] [OTTER: Open-Tagging via Text-Image Representation for Multi-modal Understanding](https://arxiv.org/abs/2510.00652)
*Jieer Ouyang,Xiaoneng Xiang,Zheng Wang,Yangkai Ding*

Main category: cs.CV

TL;DR: OTTER是一个统一的开放集多标签标注框架，它结合了预定义类别和用户驱动开放标签的优点，通过多头注意力机制在多模态数据上实现了卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有标注框架难以同时兼顾预定义类别的稳定性和用户驱动开放标签的适应性。研究旨在弥合封闭集一致性与开放词汇灵活性之间的鸿沟，实现动态且语义一致的多模态标注。

Method: OTTER构建在一个大规模、分层组织的多模态数据集上，该数据集通过自动化视觉-语言标注与人工精炼相结合的混合流程进行收集和标注。它利用多头注意力架构，共同对齐视觉和文本表示与固定标签嵌入及开放集标签嵌入。

Result: OTTER在两个基准数据集（Otter和Favorite）上均超越了竞争基线。在Otter和Favorite数据集上，总F1得分分别为0.81和0.75，分别比次优结果高出0.10和0.02。在开放集标签上，OTTER实现了接近完美的性能（Otter数据集F1为0.99，Favorite数据集F1为0.97），同时在预定义标签上保持了竞争力。

Conclusion: OTTER成功地将封闭集的一致性与开放词汇的灵活性相结合，有效解决了多模态标注应用中的挑战，展现了其在动态和语义一致标注方面的卓越性能。

Abstract: We introduce OTTER, a unified open-set multi-label tagging framework that
harmonizes the stability of a curated, predefined category set with the
adaptability of user-driven open tags. OTTER is built upon a large-scale,
hierarchically organized multi-modal dataset, collected from diverse online
repositories and annotated through a hybrid pipeline combining automated
vision-language labeling with human refinement. By leveraging a multi-head
attention architecture, OTTER jointly aligns visual and textual representations
with both fixed and open-set label embeddings, enabling dynamic and
semantically consistent tagging. OTTER consistently outperforms competitive
baselines on two benchmark datasets: it achieves an overall F1 score of 0.81 on
Otter and 0.75 on Favorite, surpassing the next-best results by margins of 0.10
and 0.02, respectively. OTTER attains near-perfect performance on open-set
labels, with F1 of 0.99 on Otter and 0.97 on Favorite, while maintaining
competitive accuracy on predefined labels. These results demonstrate OTTER's
effectiveness in bridging closed-set consistency with open-vocabulary
flexibility for multi-modal tagging applications.

</details>


### [107] [Weakly Supervised Cloud Detection Combining Spectral Features and Multi-Scale Deep Network](https://arxiv.org/abs/2510.00654)
*Shaocong Zhu,Zhiwei Li,Xinghua Li,Huanfeng Shen*

Main category: cs.CV

TL;DR: 本文提出一种结合光谱特征和多尺度场景级深度网络(SpecMCD)的弱监督云检测方法，有效提高了光学卫星图像的像素级云检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习云检测方法受限于薄云特征不明显和训练样本质量不高，导致准确性仍有提升空间。

Method: 提出SpecMCD，一种弱监督云检测方法。该方法首先通过多尺度场景级数据集和渐进式训练框架训练多尺度深度网络，然后结合多尺度概率图和云厚度图生成像素级云概率图，最后通过自适应阈值和距离加权优化得到二值云掩膜。

Result: 在WDCD和GF1MS-WHU两个数据集上，SpecMCD方法的F1-score比其他弱监督云检测方法（如WDCD和WSFNet）提高了7.82%以上。

Conclusion: SpecMCD方法在不同云覆盖条件下显示出优越性和巨大潜力。

Abstract: Clouds significantly affect the quality of optical satellite images, which
seriously limits their precise application. Recently, deep learning has been
widely applied to cloud detection and has achieved satisfactory results.
However, the lack of distinctive features in thin clouds and the low quality of
training samples limit the cloud detection accuracy of deep learning methods,
leaving space for further improvements. In this paper, we propose a weakly
supervised cloud detection method that combines spectral features and
multi-scale scene-level deep network (SpecMCD) to obtain highly accurate
pixel-level cloud masks. The method first utilizes a progressive training
framework with a multi-scale scene-level dataset to train the multi-scale
scene-level cloud detection network. Pixel-level cloud probability maps are
then obtained by combining the multi-scale probability maps and cloud thickness
map based on the characteristics of clouds in dense cloud coverage and large
cloud-area coverage images. Finally, adaptive thresholds are generated based on
the differentiated regions of the scene-level cloud masks at different scales
and combined with distance-weighted optimization to obtain binary cloud masks.
Two datasets, WDCD and GF1MS-WHU, comprising a total of 60 Gaofen-1
multispectral (GF1-MS) images, were used to verify the effectiveness of the
proposed method. Compared to the other weakly supervised cloud detection
methods such as WDCD and WSFNet, the F1-score of the proposed SpecMCD method
shows an improvement of over 7.82%, highlighting the superiority and potential
of the SpecMCD method for cloud detection under different cloud coverage
conditions.

</details>


### [108] [Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents](https://arxiv.org/abs/2510.00658)
*Beomsu Kim,Byunghee Cha,Jong Chul Ye*

Main category: cs.CV

TL;DR: 本文提出一种名为Align Your Tangent (AYT) 的新损失函数（流形特征距离，MFD），解决了Consistency Models (CMs) 训练慢、需大批量的问题，显著加速了训练，提升了性能，并支持小批量训练。


<details>
  <summary>Details</summary>
Motivation: 现有的Consistency Models (CMs) 虽能实现一两步快速采样，但其训练过程漫长且需要大批量大小才能获得高质量样本。

Method: 1. 分析CMs在收敛附近的训练动态，发现CM的更新方向（切线）存在震荡性，仅平行于数据流形。2. 提出一种新的损失函数——流形特征距离（Manifold Feature Distance, MFD），旨在提供指向数据流形的流形对齐切线，以缓解震荡。3. 将此方法命名为Align Your Tangent (AYT)。

Result: 1. AYT方法将CM的训练速度提高了几个数量级。2. 模型性能甚至超越了LPIPS（learned perceptual image patch similarity metric）指标。3. 该损失函数使得CMs能够在极小批量大小下训练，且不损害样本质量。

Conclusion: 通过引入流形特征距离（MFD）损失函数纠正CM训练中的切线震荡问题，Align Your Tangent (AYT) 方法显著加速了CM训练，提升了性能，并解决了对大批量大小的依赖，极大地提升了CMs的训练效率和实用性。

Abstract: With diffusion and flow matching models achieving state-of-the-art generating
performance, the interest of the community now turned to reducing the inference
time without sacrificing sample quality. Consistency Models (CMs), which are
trained to be consistent on diffusion or probability flow ordinary differential
equation (PF-ODE) trajectories, enable one or two-step flow or diffusion
sampling. However, CMs typically require prolonged training with large batch
sizes to obtain competitive sample quality. In this paper, we examine the
training dynamics of CMs near convergence and discover that CM tangents -- CM
output update directions -- are quite oscillatory, in the sense that they move
parallel to the data manifold, not towards the manifold. To mitigate
oscillatory tangents, we propose a new loss function, called the manifold
feature distance (MFD), which provides manifold-aligned tangents that point
toward the data manifold. Consequently, our method -- dubbed Align Your Tangent
(AYT) -- can accelerate CM training by orders of magnitude and even out-perform
the learned perceptual image patch similarity metric (LPIPS). Furthermore, we
find that our loss enables training with extremely small batch sizes without
compromising sample quality. Code: https://github.com/1202kbs/AYT

</details>


### [109] [Unsupervised Unfolded rPCA (U2-rPCA): Deep Interpretable Clutter Filtering for Ultrasound Microvascular Imaging](https://arxiv.org/abs/2510.00660)
*Huaying Li,Liansheng Wang,Yinran Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为U2-rPCA的无监督展开式鲁棒主成分分析方法，用于超声微血管成像中的杂波滤波，克服了传统方法和现有深度学习方法的局限性，实现了更高的信噪比和可解释性。


<details>
  <summary>Details</summary>
Motivation: 高灵敏度杂波滤波是超声微血管成像的关键步骤。传统的奇异值分解(SVD)和鲁棒主成分分析(rPCA)在特征建模和组织-血流分离方面存在局限性。现有的监督式深度学习滤波器虽然有潜力，但面临可解释性差和缺乏体内/体外真实标签的挑战。因此，需要一种既能彻底分离信号，又具有数学可解释性且不依赖真实标签的杂波滤波方法。

Method: 本文提出了一种无监督展开式rPCA (U2-rPCA) 方法。该方法从迭代重加权最小二乘 (IRLS) rPCA基线展开，并引入了固有的低秩和稀疏正则化，以保持数学可解释性且不依赖学习标签。网络中增加了一个稀疏增强单元，以增强其捕获稀疏微血流信号的能力。U2-rPCA作为一种自适应滤波器，用图像序列的一部分进行训练，然后应用于后续帧。

Result: U2-rPCA在体外仿真数据集和公开的体内数据集上进行了实验验证，结果表明其性能优于基于SVD的方法、rPCA基线和另一种基于深度学习的滤波器。特别是，该方法将功率多普勒图像的对比噪声比 (CNR) 提高了2 dB到10 dB。此外，通过消融研究验证了U2-rPCA各构建模块的有效性。

Conclusion: U2-rPCA成功解决了超声微血管成像中杂波滤波的挑战，提供了一种具有数学可解释性、无监督且高性能的解决方案。它显著提高了图像的对比噪声比，为高质量微血管成像奠定了基础。

Abstract: High-sensitivity clutter filtering is a fundamental step in ultrasound
microvascular imaging. Singular value decomposition (SVD) and robust principal
component analysis (rPCA) are the main clutter filtering strategies. However,
both strategies are limited in feature modeling and tissue-blood flow
separation for high-quality microvascular imaging. Recently, deep
learning-based clutter filtering has shown potential in more thoroughly
separating tissue and blood flow signals. However, the existing supervised
filters face the challenges of interpretability and lack of in-vitro and
in-vivo ground truths. While the interpretability issue can be addressed by
algorithm deep unfolding, the training ground truth remains unsolved. To this
end, this paper proposes an unsupervised unfolded rPCA (U2-rPCA) method that
preserves mathematical interpretability and is insusceptible to learning
labels. Specifically, U2-rPCA is unfolded from an iteratively reweighted least
squares (IRLS) rPCA baseline with intrinsic low-rank and sparse regularization.
A sparse-enhancement unit is added to the network to strengthen its capability
to capture the sparse micro-flow signals. U2-rPCA is like an adaptive filter
that is trained with part of the image sequence and then used for the following
frames. Experimental validations on a in-silico dataset and public in-vivo
datasets demonstrated the outperformance of U2-rPCA when compared with the
SVD-based method, the rPCA baseline, and another deep learning-based filter.
Particularly, the proposed method improved the contrastto-noise ratio (CNR) of
the power Doppler image by 2 dB to 10 dB when compared with other methods.
Furthermore, the effectiveness of the building modules of U2-rPCA was validated
through ablation studies.

</details>


### [110] [Multi-Domain Brain Vessel Segmentation Through Feature Disentanglement](https://arxiv.org/abs/2510.00665)
*Francesco Galati,Daniele Falcetta,Rosa Cortese,Ferran Prados,Ninon Burgos,Maria A. Zuluaga*

Main category: cs.CV

TL;DR: 一个基于图像到图像翻译和解耦技术的框架，能够有效且鲁棒地在不同医学中心、图像模态和血管类型之间分割脑动脉和静脉。


<details>
  <summary>Details</summary>
Motivation: 脑血管形态复杂，现有自动分割模型通常专注于单一成像模态，难以全面理解脑血管树。然而，准确治疗脑部疾病需要跨模态的综合血管理解。

Method: 采用图像到图像翻译，并结合解耦技术，独立操纵不同的图像属性（特别是血管外观），同时保留空间信息（形状和位置），以实现域适应并在标签不变的情况下进行跨域分割，避免了域特定的模型设计和数据协调。

Result: 该框架有效弥合了医疗中心、图像模态和血管类型之间巨大且多样的域差距。消融研究进一步验证了其鲁棒性和多功能性。

Conclusion: 域适应方法在多个场景中准确执行脑血管图像分割具有巨大潜力。

Abstract: The intricate morphology of brain vessels poses significant challenges for
automatic segmentation models, which usually focus on a single imaging
modality. However, accurately treating brain-related conditions requires a
comprehensive understanding of the cerebrovascular tree, regardless of the
specific acquisition procedure. Our framework effectively segments brain
arteries and veins in various datasets through image-to-image translation while
avoiding domain-specific model design and data harmonization between the source
and the target domain. This is accomplished by employing disentanglement
techniques to independently manipulate different image properties, allowing
them to move from one domain to another in a label-preserving manner.
Specifically, we focus on manipulating vessel appearances during adaptation
while preserving spatial information, such as shapes and locations, which are
crucial for correct segmentation. Our evaluation effectively bridges large and
varied domain gaps across medical centers, image modalities, and vessel types.
Additionally, we conduct ablation studies on the optimal number of required
annotations and other architectural choices. The results highlight our
framework's robustness and versatility, demonstrating the potential of domain
adaptation methodologies to perform cerebrovascular image segmentation in
multiple scenarios accurately. Our code is available at
https://github.com/i-vesseg/MultiVesSeg.

</details>


### [111] [A Geometric Unification of Generative AI with Manifold-Probabilistic Projection Models](https://arxiv.org/abs/2510.00666)
*Leah Bar,Liron Mor Yosef,Shai Zucker,Neta Shoham,Inbar Seroussi,Nir Sochen*

Main category: cs.CV

TL;DR: 本研究结合几何与概率视角，提出了流形-概率投影模型(MPPM)，通过将扩散模型解释为“好图像”流形上的投影机制，在图像生成和恢复方面优于潜在扩散模型(LDM)。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成AI常忽视图像数据固有的低维几何结构（如流形），仅侧重于概率方法，并且对潜在空间或流形坐标空间中的概率分布处理不足（预定义或视为均匀）。

Method: 通过提供一个几何框架和基于核的概率方法来统一几何和概率视角。将扩散模型解释为向“好图像”流形的投影机制，并在此基础上构建了一个新的确定性模型——流形-概率投影模型（MPPM），该模型在表示（像素）空间和潜在空间中均可操作。实验中使用了潜在MPPM（LMPPM）。

Result: 潜在MPPM（LMPPM）在多个数据集上超越了潜在扩散模型（LDM），在图像恢复和生成方面取得了更优异的结果。

Conclusion: 本研究成功地将几何和概率视角统一起来，提出的LMPPM模型在图像恢复和生成任务上表现出色，优于现有潜在扩散模型，并为理解扩散模型提供了新的视角。

Abstract: The foundational premise of generative AI for images is the assumption that
images are inherently low-dimensional objects embedded within a
high-dimensional space. Additionally, it is often implicitly assumed that
thematic image datasets form smooth or piecewise smooth manifolds. Common
approaches overlook the geometric structure and focus solely on probabilistic
methods, approximating the probability distribution through universal
approximation techniques such as the kernel method. In some generative models,
the low dimensional nature of the data manifest itself by the introduction of a
lower dimensional latent space. Yet, the probability distribution in the latent
or the manifold coordinate space is considered uninteresting and is predefined
or considered uniform. This study unifies the geometric and probabilistic
perspectives by providing a geometric framework and a kernel-based
probabilistic method simultaneously. The resulting framework demystifies
diffusion models by interpreting them as a projection mechanism onto the
manifold of ``good images''. This interpretation leads to the construction of a
new deterministic model, the Manifold-Probabilistic Projection Model (MPPM),
which operates in both the representation (pixel) space and the latent space.
We demonstrate that the Latent MPPM (LMPPM) outperforms the Latent Diffusion
Model (LDM) across various datasets, achieving superior results in terms of
image restoration and generation.

</details>


### [112] [Beyond one-hot encoding? Journey into compact encoding for large multi-class segmentation](https://arxiv.org/abs/2510.00667)
*Aaron Kujawa,Thomas Booth,Tom Vercauteren*

Main category: cs.CV

TL;DR: 本研究提出并评估了一系列二值编码方法以降低多类别医学图像分割的计算和内存需求，但发现其性能低于传统的one-hot编码。


<details>
  <summary>Details</summary>
Motivation: 标准学习方法中，one-hot编码导致多类别医学图像分割的计算和内存需求随类别数线性增长，旨在提出新方法以减少这些开销。

Method: 提出并探索了包括原始二值编码、纠错输出码（ECOCs）、类别加权、硬/软解码、类别到码字分配以及标签嵌入树在内的二值编码方法家族，以替代one-hot编码，并将其应用于108类3D MRI全脑分割任务。

Result: 与one-hot编码（Dice相似系数DSC=82.4）相比，所有二值编码方法的分割性能均有所下降，DSC范围介于39.3至73.8，未能达到先进水平。

Conclusion: 尽管二值编码能有效降低计算和内存需求，但在本医学图像分割任务中未能保持与one-hot编码相当的分割质量。本研究提供了有价值的负面结果，旨在启发未来针对大规模多类别分割任务的紧凑编码策略研究。

Abstract: This work presents novel methods to reduce computational and memory
requirements for medical image segmentation with a large number of classes. We
curiously observe challenges in maintaining state-of-the-art segmentation
performance with all of the explored options. Standard learning-based methods
typically employ one-hot encoding of class labels. The computational complexity
and memory requirements thus increase linearly with the number of classes. We
propose a family of binary encoding approaches instead of one-hot encoding to
reduce the computational complexity and memory requirements to logarithmic in
the number of classes. In addition to vanilla binary encoding, we investigate
the effects of error-correcting output codes (ECOCs), class weighting,
hard/soft decoding, class-to-codeword assignment, and label embedding trees. We
apply the methods to the use case of whole brain parcellation with 108 classes
based on 3D MRI images. While binary encodings have proven efficient in
so-called extreme classification problems in computer vision, we faced
challenges in reaching state-of-the-art segmentation quality with binary
encodings. Compared to one-hot encoding (Dice Similarity Coefficient (DSC) =
82.4 (2.8)), we report reduced segmentation performance with the binary
segmentation approaches, achieving DSCs in the range from 39.3 to 73.8.
Informative negative results all too often go unpublished. We hope that this
work inspires future research of compact encoding strategies for large
multi-class segmentation tasks.

</details>


### [113] [Adaptive Event Stream Slicing for Open-Vocabulary Event-Based Object Detection via Vision-Language Knowledge Distillation](https://arxiv.org/abs/2510.00681)
*Jinchang Zhang,Zijun Li,Jiakai Lin,Guoyu Lu*

Main category: cs.CV

TL;DR: 事件相机在开放词汇目标检测上面临纹理/颜色缺失和模态鸿沟的挑战。本文提出一个事件-图像知识蒸馏框架，利用CLIP作为教师模型，并结合自适应SNN-CNN进行事件数据处理，以实现事件数据上的开放词汇目标检测。


<details>
  <summary>Details</summary>
Motivation: 事件相机在目标检测中具有高响应速度和低延迟等优点，但缺乏纹理和颜色信息，现有方法依赖预定义类别，难以泛化到新颖物体。虽然视觉-语言模型（VLMs）在RGB图像上实现了开放词汇检测，但事件流与图像之间存在模态鸿沟，无法直接将CLIP应用于事件数据。

Method: 1. **事件-图像知识蒸馏框架**：利用图像帧作为教师模型（CLIP）的输入，指导基于事件的学生模型学习CLIP的视觉表示。2. **基于空间注意力的蒸馏**：使学生网络直接从原始事件输入中学习特征并继承CLIP知识。3. **混合脉冲神经网络（SNN）和卷积神经网络（CNN）框架**：SNN自适应确定最佳事件分割时刻，以防止信息丢失并提取关键时间特征，然后由CNN处理进行目标检测。

Result: 抽象中未提供具体的量化实验结果，但提出通过所设计的方法，能够在事件数据上实现开放词汇目标检测，并有效利用了CLIP的语义理解能力。

Conclusion: 该研究通过提出的事件-图像知识蒸馏框架和混合SNN-CNN结构，成功弥合了事件流与视觉-语言模型之间的模态鸿沟。这使得事件相机能够进行开放词汇目标检测，克服了现有方法在泛化到新颖物体时的局限性，并优化了事件数据的时序信息处理。

Abstract: Event cameras offer advantages in object detection tasks due to high-speed
response, low latency, and robustness to motion blur. However, event cameras
lack texture and color information, making open-vocabulary detection
particularly challenging. Current event-based detection methods are typically
trained on predefined categories, limiting their ability to generalize to novel
objects, where encountering previously unseen objects is common.
Vision-language models (VLMs) have enabled open-vocabulary object detection in
RGB images. However, the modality gap between images and event streams makes it
ineffective to directly transfer CLIP to event data, as CLIP was not designed
for event streams. To bridge this gap, we propose an event-image knowledge
distillation framework that leverages CLIP's semantic understanding to achieve
open-vocabulary object detection on event data. Instead of training CLIP
directly on event streams, we use image frames as inputs to a teacher model,
guiding the event-based student model to learn CLIP's rich visual
representations. Through spatial attention-based distillation, the student
network learns meaningful visual features directly from raw event inputs while
inheriting CLIP's broad visual knowledge. Furthermore, to prevent information
loss due to event data segmentation, we design a hybrid spiking neural network
(SNN) and convolutional neural network (CNN) framework. Unlike fixed-group
event segmentation methods, which often discard crucial temporal information,
our SNN adaptively determines the optimal event segmentation moments, ensuring
that key temporal features are extracted. The extracted event features are then
processed by CNNs for object detection.

</details>


### [114] [ProtoMask: Segmentation-Guided Prototype Learning](https://arxiv.org/abs/2510.00683)
*Steffen Meinert,Philipp Schlinge,Nils Strodthoff,Martin Atzmueller*

Main category: cs.CV

TL;DR: 本文提出ProtoMask模型，利用图像分割基础模型提升基于原型案例推理的XAI方法的可解释性，通过语义分割限制显著图区域，提高原型解释的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于原型案例推理的可解释AI方法依赖于不可靠的后验显著性技术来解释所学原型的语义，这些技术的可靠性和质量受到质疑。研究旨在提高嵌入空间和输入空间之间映射的真实性。

Method: 研究使用图像分割基础模型将显著图的计算区域限制在预定义的语义图像区域。通过每个生成的分割掩码的边界框来裁剪图像，每个掩码在新型ProtoMask模型架构中作为单独的输入。

Result: 在三个流行的细粒度分类数据集上进行了实验，并使用多种指标进行评估。结果表明，与流行的其他模型相比，ProtoMask具有竞争性的性能和独特的解释性特征。

Conclusion: ProtoMask模型通过利用图像分割基础模型，有效改善了嵌入与输入空间映射的真实性，提供了更可靠和独特的解释性，解决了传统原型方法中显著性技术不可靠的问题。

Abstract: XAI gained considerable importance in recent years. Methods based on
prototypical case-based reasoning have shown a promising improvement in
explainability. However, these methods typically rely on additional post-hoc
saliency techniques to explain the semantics of learned prototypes. Multiple
critiques have been raised about the reliability and quality of such
techniques. For this reason, we study the use of prominent image segmentation
foundation models to improve the truthfulness of the mapping between embedding
and input space. We aim to restrict the computation area of the saliency map to
a predefined semantic image patch to reduce the uncertainty of such
visualizations. To perceive the information of an entire image, we use the
bounding box from each generated segmentation mask to crop the image. Each mask
results in an individual input in our novel model architecture named ProtoMask.
We conduct experiments on three popular fine-grained classification datasets
with a wide set of metrics, providing a detailed overview on explainability
characteristics. The comparison with other popular models demonstrates
competitive performance and unique explainability features of our model.
https://github.com/uos-sis/quanproto

</details>


### [115] [Graph Integrated Multimodal Concept Bottleneck Model](https://arxiv.org/abs/2510.00701)
*Jiakai Lin,Jinchang Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: 针对概念瓶颈模型(CBMs)忽略概念结构化关系的局限，本文提出MoE-SGT框架，通过引入图Transformer建模结构关系并利用混合专家(MoE)模块增强复杂模式适应性，在多数据集上取得了更高的准确率。


<details>
  <summary>Details</summary>
Motivation: 深度学习对可解释性需求日益增长，概念瓶颈模型(CBMs)通过插入人类可理解概念解决此问题。然而，CBMs通常是单模态的，并且忽略了概念之间存在的结构化关系。

Method: 本文提出了MoE-SGT框架，通过以下方式增强CBMs：1. 结合结构注入的图Transformer和混合专家(MoE)模块。2. 为多模态输入构建答案-概念和答案-问题图，显式建模概念间的结构化关系。3. 整合图Transformer捕获多级依赖，解决传统CBMs在建模概念交互上的局限。4. 将MoE模块引入模型的前馈层，以提升学习多样化概念关系的能力，并通过动态分配推理任务给不同子专家，显著增强模型对复杂概念推理的适应性。

Result: MoE-SGT在多个数据集上，通过有效建模概念间的结构化关系和利用动态专家选择机制，实现了比其他概念瓶颈网络更高的准确性。

Conclusion: MoE-SGT通过结合图Transformer对结构化概念关系的建模以及MoE模块对复杂概念模式的动态适应性，显著提升了概念瓶颈模型在多模态复杂概念推理任务上的性能和准确性。

Abstract: With growing demand for interpretability in deep learning, especially in high
stakes domains, Concept Bottleneck Models (CBMs) address this by inserting
human understandable concepts into the prediction pipeline, but they are
generally single modal and ignore structured concept relationships. To overcome
these limitations, we present MoE-SGT, a reasoning driven framework that
augments CBMs with a structure injecting Graph Transformer and a Mixture of
Experts (MoE) module. We construct answer-concept and answer-question graphs
for multimodal inputs to explicitly model the structured relationships among
concepts. Subsequently, we integrate Graph Transformer to capture multi level
dependencies, addressing the limitations of traditional Concept Bottleneck
Models in modeling concept interactions. However, it still encounters
bottlenecks in adapting to complex concept patterns. Therefore, we replace the
feed forward layers with a Mixture of Experts (MoE) module, enabling the model
to have greater capacity in learning diverse concept relationships while
dynamically allocating reasoning tasks to different sub experts, thereby
significantly enhancing the model's adaptability to complex concept reasoning.
MoE-SGT achieves higher accuracy than other concept bottleneck networks on
multiple datasets by modeling structured relationships among concepts and
utilizing a dynamic expert selection mechanism.

</details>


### [116] [Training-free Uncertainty Guidance for Complex Visual Tasks with MLLMs](https://arxiv.org/abs/2510.00705)
*Sanghwan Kim,Rui Xiao,Stephan Alaniz,Yongqin Xian,Zeynep Akata*

Main category: cs.CV

TL;DR: 本文提出一种免训练框架，通过利用多模态大语言模型（MLLMs）的内在不确定性作为指导信号，解决其在细粒度感知上的不足，并使其在多个视觉任务中取得与专业微调方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在细粒度感知（如识别小物体、查找长视频关键时刻）方面表现不佳。现有解决方案通常依赖复杂且任务特定的微调，限制了泛化能力并增加了模型复杂度。

Method: 提出一个免训练框架，利用MLLM自身的内在不确定性作为主动指导信号。核心思想是模型输出熵在接收相关视觉信息时会降低。引入统一机制，通过响应不确定性对候选视觉输入进行评分，使模型能够自主聚焦于最显著数据。

Result: 将该方法应用于视觉搜索、长视频理解和时间定位三个复杂视觉任务，使现成的MLLMs能够达到与专门微调方法相竞争的性能。

Conclusion: 利用模型内在不确定性是一种强大且通用的策略，能够有效增强细粒度多模态性能。

Abstract: Multimodal Large Language Models (MLLMs) often struggle with fine-grained
perception, such as identifying small objects in high-resolution images or
finding key moments in long videos. Existing works typically rely on
complicated, task-specific fine-tuning, which limits their generalizability and
increases model complexity. In this work, we propose an effective,
training-free framework that uses an MLLM's intrinsic uncertainty as a
proactive guidance signal. Our core insight is that a model's output entropy
decreases when presented with relevant visual information. We introduce a
unified mechanism that scores candidate visual inputs by response uncertainty,
enabling the model to autonomously focus on the most salient data. We apply
this simple principle to three complex visual tasks: Visual Search, Long Video
Understanding, and Temporal Grounding, allowing off-the-shelf MLLMs to achieve
performance competitive with specialized, fine-tuned methods. Our work
validates that harnessing intrinsic uncertainty is a powerful, general strategy
for enhancing fine-grained multimodal performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [117] [Learning to Lead Themselves: Agentic AI in MAS using MARL](https://arxiv.org/abs/2510.00022)
*Ansh Kamthan*

Main category: cs.AI

TL;DR: 本文探讨代理AI如何通过合作式多智能体强化学习（IPPO），解决无人机配送和仓储自动化中的任务分配与协调问题。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统从原型走向实际部署，多智能体去中心化协作决策成为核心需求。本文旨在通过代理AI，提升无人机配送和仓储自动化等多智能体系统中的任务分配与协调能力。

Method: 将问题建模为合作式多智能体强化学习环境。实现了一种轻量级多智能体近端策略优化（IPPO）方法，基于PyTorch，采用集中训练、去中心化执行范式。实验在PettingZoo环境中进行，模拟多架同质无人机在无显式通信下自组织覆盖不同目标。

Result: 抽象中未提供具体研究结果。

Conclusion: 抽象中未提供基于实验结果的结论。

Abstract: As autonomous systems move from prototypes to real deployments, the ability
of multiple agents to make decentralized, cooperative decisions becomes a core
requirement. This paper examines how agentic artificial intelligence, agents
that act independently, adaptively and proactively can improve task allocation
and coordination in multi-agent systems, with primary emphasis on drone
delivery and secondary relevance to warehouse automation. We formulate the
problem in a cooperative multi-agent reinforcement learning setting and
implement a lightweight multi-agent Proximal Policy Optimization, called IPPO,
approach in PyTorch under a centralized-training, decentralized-execution
paradigm. Experiments are conducted in PettingZoo environment, where multiple
homogeneous drones or agents must self-organize to cover distinct targets
without explicit communication.

</details>


### [118] [ToolBrain: A Flexible Reinforcement Learning Framework for Agentic Tools](https://arxiv.org/abs/2510.00023)
*Quy Minh Le,Minh Sao Khue Luu,Khanh-Tung Tran,Duc-Hai Nguyen,Hoang-Quoc-Viet Pham,Quan Le,Hoang Thanh Lam,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: ToolBrain是一个轻量级框架，通过灵活的强化学习简化了代理AI的工具使用训练，解决了现有挑战，并显著提升了工具使用技能。


<details>
  <summary>Details</summary>
Motivation: 代理AI有效使用工具至关重要，但当前训练方法存在手动设计奖励、训练数据有限和多工具选择不佳等问题，导致适应缓慢、资源浪费和性能次优。

Method: 引入ToolBrain框架，一个轻量级、用户友好的系统，通过灵活的强化学习（支持GRPO、DPO等算法）和监督学习来训练代理AI使用工具。它支持自定义奖励或利用LLM作为判断系统生成奖励，并集成了知识蒸馏、自动任务生成、无缝工具检索、通过Unsloth使用QLoRA进行高效微调以及通过bitsandbytes进行量化推理等功能。

Result: 通过训练CodeAct代理执行邮件搜索任务等多样用例，ToolBrain展示了工具使用技能的快速和有针对性的改进，性能提升高达30.0%，同时保持了代码库的简洁性和可扩展性。

Conclusion: ToolBrain框架成功解决了代理AI工具使用训练的现有挑战，实现了训练效率和技能表现的显著提升，并作为一个开源工具，为LLM代理在特定领域应用提供了便利。

Abstract: Effective tool use is essential for agentic AI, yet training agents to
utilize tools remains challenging due to manually designed rewards, limited
training data, and poor multi-tool selection, resulting in slow adaptation,
wasted computational resources, and suboptimal performance. We introduce
ToolBrain, a lightweight and user-friendly framework for coaching tool use in
agentic models with flexible reinforcement learning (RL), easing the barriers
for researchers and practitioners to adapt LLM-based agents to specific
domains. It supports a wide range of training strategies, including RL
algorithms such as GRPO and DPO, as well as supervised learning. ToolBrain
enables custom reward callables directly on an agent's execution traces or
simply utilizes an automated LLM-as-a-judge system for reward generation. It is
packed with useful capabilities, including knowledge distillation from large to
small models for efficient development, automatic task generation from tool
descriptions, seamless tool retrieval, efficient fine-tuning pipelines with
QLoRA through Unsloth, and quantized inference via bitsandbytes. We demonstrate
ToolBrain through diverse use cases, such as training a CodeAct agent to
autonomously execute email search tasks, showing fast, targeted improvements
(up to 30.0%) in tool-use skills while keeping the codebase simple and
extensible in Agentic AI. Our framework is publicly available at
https://toolbrain.org.

</details>


### [119] [ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models](https://arxiv.org/abs/2510.00071)
*Dongqi Zheng*

Main category: cs.AI

TL;DR: 本文提出了一种名为ARS的无需训练方法，通过自适应确定性监控动态抑制大型推理语言模型的冗余推理步骤，显著提高了计算效率（token、延迟、能耗降低高达53%-57.9%），同时保持或提升了推理准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理语言模型（LRLMs）在复杂推理任务中表现出色，但因“过度思考”现象导致计算效率低下。现有高效推理方法难以平衡推理质量与推理成本的降低。

Method: 提出了一种名为“自适应推理抑制 (ARS)”的无需训练方法。该方法通过自适应的确定性监控动态抑制冗余推理步骤，以在保持准确性的同时提高效率。ARS引入了多检查点确定性估计机制和渐进式抑制阈值，以实现比静态抑制方法更优的效率。

Result: 在数学推理基准测试中，ARS在保持或提高准确性的同时，实现了高达53%的token、46.1%的延迟和57.9%的能耗减少。

Conclusion: ARS是一种有效且无需训练的LRLM推理优化方法，通过动态抑制冗余推理步骤，显著提高了计算效率，同时保持了高质量的推理表现，解决了LRLMs的计算低效问题。

Abstract: Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable
capabilities in complex reasoning tasks, but suffer from significant
computational inefficiencies due to overthinking phenomena. Existing efficient
reasoning methods face the challenge of balancing reasoning quality with
inference cost reduction. We propose \textbf{Adaptive Reasoning Suppression
(ARS)}, a novel training-free approach that dynamically suppresses redundant
reasoning steps while preserving accuracy through adaptive certainty
monitoring. ARS introduces a multi-checkpoint certainty estimation mechanism
with progressive suppression thresholds, achieving superior efficiency compared
to static suppression methods. Our extensive evaluation across mathematical
reasoning benchmarks using multiple model architectures demonstrates that ARS
achieves up to 53%, 46.1%, and 57.9% in token, latency and energy reduction,
while maintaining or improving accuracy.

</details>


### [120] [NeurIPS should lead scientific consensus on AI policy](https://arxiv.org/abs/2510.00075)
*Rishi Bommasani*

Main category: cs.AI

TL;DR: 本文提出NeurIPS应借鉴IPCC经验，填补AI政策共识形成的空白，以推动制定更明智的AI政策。


<details>
  <summary>Details</summary>
Motivation: 制定明智的AI政策需要严格的证据和科学共识，但目前在科学共识形成方面存在空白。本文旨在解决这一缺失。

Method: 本文作为一篇立场文件，提出由NeurIPS主动催化AI政策领域的科学共识形成。具体建议是借鉴IPCC在气候政策上建立共识的经验，进行初步试点项目。

Result: 本文的主要“结果”是识别出AI政策共识形成机制的空白，并有力论证了NeurIPS是填补这一空白的最佳选项，同时提出了可行的实施路径（借鉴IPCC）。

Conclusion: AI政策的制定需要科学共识，NeurIPS应发挥其领导作用，借鉴IPCC经验，积极推动AI政策领域的科学共识形成，以制定更高质量的AI政策。

Abstract: Designing wise AI policy is a grand challenge for society. To design such
policy, policymakers should place a premium on rigorous evidence and scientific
consensus. While several mechanisms exist for evidence generation, and nascent
mechanisms tackle evidence synthesis, we identify a complete void on consensus
formation. In this position paper, we argue NeurIPS should actively catalyze
scientific consensus on AI policy. Beyond identifying the current deficit in
consensus formation mechanisms, we argue that NeurIPS is the best option due
its strengths and the paucity of compelling alternatives. To make progress, we
recommend initial pilots for NeurIPS by distilling lessons from the IPCC's
leadership to build scientific consensus on climate policy. We dispel
predictable counters that AI researchers disagree too much to achieve consensus
and that policy engagement is not the business of NeurIPS. NeurIPS leads AI on
many fronts, and it should champion scientific consensus to create higher
quality AI policy.

</details>


### [121] [Towards a Framework for Supporting the Ethical and Regulatory Certification of AI Systems](https://arxiv.org/abs/2510.00084)
*Fabian Kovac,Sebastian Neumaier,Timea Pahi,Torsten Priebe,Rafael Rodrigues,Dimitrios Christodoulou,Maxime Cordy,Sylvain Kubler,Ali Kordia,Georgios Pitsiladis,John Soldatos,Petros Zervoudakis*

Main category: cs.AI

TL;DR: CERTAIN项目旨在通过开发整合合规、道德和透明度的AI框架，利用语义MLOps、本体数据溯源和RegOps，解决AI的伦理法律挑战，推动负责任的AI创新。


<details>
  <summary>Details</summary>
Motivation: 人工智能的快速发展在带来巨大影响的同时，也引发了关键的伦理、法律和监管挑战，亟需一个能够整合监管合规性、道德标准和透明度的综合框架。

Method: 开发一个综合框架，其核心组件包括：(i) 用于结构化AI生命周期管理的语义机器学习操作（MLOps）；(ii) 用于确保可追溯性和问责制的本体驱动数据溯源；(iii) 用于操作化合规要求的监管操作（RegOps）工作流。

Result: 本立场论文概述了构建上述框架核心组件的方法学步骤，为整合监管合规性、道德标准和透明度到AI系统提供了蓝图。

Conclusion: CERTAIN项目致力于通过其提出的解决方案，在不同试点中实施和验证，以推进AI监管合规性并促进符合欧洲标准的负责任AI创新。

Abstract: Artificial Intelligence has rapidly become a cornerstone technology,
significantly influencing Europe's societal and economic landscapes. However,
the proliferation of AI also raises critical ethical, legal, and regulatory
challenges. The CERTAIN (Certification for Ethical and Regulatory Transparency
in Artificial Intelligence) project addresses these issues by developing a
comprehensive framework that integrates regulatory compliance, ethical
standards, and transparency into AI systems. In this position paper, we outline
the methodological steps for building the core components of this framework.
Specifically, we present: (i) semantic Machine Learning Operations (MLOps) for
structured AI lifecycle management, (ii) ontology-driven data lineage tracking
to ensure traceability and accountability, and (iii) regulatory operations
(RegOps) workflows to operationalize compliance requirements. By implementing
and validating its solutions across diverse pilots, CERTAIN aims to advance
regulatory compliance and to promote responsible AI innovation aligned with
European standards.

</details>


### [122] [Judging by Appearances? Auditing and Intervening Vision-Language Models for Bail Prediction](https://arxiv.org/abs/2510.00088)
*Sagnik Basu,Shubham Prakash,Ashish Maruti Barge,Siddharth D Jaiswal,Abhisek Dash,Saptarshi Ghosh,Animesh Mukherjee*

Main category: cs.AI

TL;DR: 本文审计发现，独立VLM在保释判决预测上表现不佳，常错误拒绝保释且自信度高。通过RAG引入法律先例并创新微调VLM，我们显著提升了保释预测性能，为未来VLM部署前的智能干预提供了方向。


<details>
  <summary>Details</summary>
Motivation: 随着VLM将图像引入法律判决预测，可能导致不良后果和恶意使用。因此，有必要审计VLM在保释决策等敏感法律任务中的效率和公平性。

Method: 首先，对独立VLM在保释判决预测任务上的效率进行审计。然后，设计干预算法，包括通过RAG管道整合法律先例，并采用创新方案对VLM进行微调。

Result: 审计发现，独立VLM在多个交叉群体和模型中表现不佳，常常以极高置信度错误地拒绝本应获得保释的个体。所设计的干预措施显著改善了保释预测性能。

Conclusion: 本研究为未来在VLM实际部署进行法律判决预测之前，设计更智能的干预措施奠定了基础，以避免潜在的偏见和不公。

Abstract: Large language models (LLMs) have been extensively used for legal judgment
prediction tasks based on case reports and crime history. However, with a surge
in the availability of large vision language models (VLMs), legal judgment
prediction systems can now be made to leverage the images of the criminals in
addition to the textual case reports/crime history. Applications built in this
way could lead to inadvertent consequences and be used with malicious intent.
In this work, we run an audit to investigate the efficiency of standalone VLMs
in the bail decision prediction task. We observe that the performance is poor
across multiple intersectional groups and models \textit{wrongly deny bail to
deserving individuals with very high confidence}. We design different
intervention algorithms by first including legal precedents through a RAG
pipeline and then fine-tuning the VLMs using innovative schemes. We demonstrate
that these interventions substantially improve the performance of bail
prediction. Our work paves the way for the design of smarter interventions on
VLMs in the future, before they can be deployed for real-world legal judgment
prediction.

</details>


### [123] [AuditAgent: Expert-Guided Multi-Agent Reasoning for Cross-Document Fraudulent Evidence Discovery](https://arxiv.org/abs/2510.00156)
*Songran Bai,Bingzhe Wu,Yiwei Zhang,Chengke Wu,Xiaolong Zheng,Yaze Yuan,Ke Wu,Jianqiang Li*

Main category: cs.AI

TL;DR: 本文提出AuditAgent多智能体推理框架，结合审计领域专业知识，解决复杂金融欺诈证据链定位挑战，并在召回率和可解释性上超越通用智能体，为金融法证建立新基准。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的金融欺诈检测面临巨大挑战，因为证据隐蔽且分散在复杂、多年的财务披露中。

Method: 引入了一个新颖的AuditAgent多智能体推理框架，该框架融入了审计领域专业知识，用于精细化的证据链定位。方法利用中国证监会执法文件和财务报告构建的专家标注数据集，整合了主体级别的风险先验、混合检索策略和专用智能体模块，以高效识别和聚合跨报告证据。

Result: 广泛的实验表明，该方法在召回率和可解释性方面显著优于通用智能体范式，为自动化、透明的金融法证建立了新基准。

Conclusion: 研究结果突出了领域特定的推理和数据集构建对于在实际监管应用中推进鲁棒金融欺诈检测的价值。

Abstract: Financial fraud detection in real-world scenarios presents significant
challenges due to the subtlety and dispersion of evidence across complex,
multi-year financial disclosures. In this work, we introduce a novel
multi-agent reasoning framework AuditAgent, enhanced with auditing domain
expertise, for fine-grained evidence chain localization in financial fraud
cases. Leveraging an expert-annotated dataset constructed from enforcement
documents and financial reports released by the China Securities Regulatory
Commission, our approach integrates subject-level risk priors, a hybrid
retrieval strategy, and specialized agent modules to efficiently identify and
aggregate cross-report evidence. Extensive experiments demonstrate that our
method substantially outperforms General-Purpose Agent paradigm in both recall
and interpretability, establishing a new benchmark for automated, transparent
financial forensics. Our results highlight the value of domain-specific
reasoning and dataset construction for advancing robust financial fraud
detection in practical, real-world regulatory applications.

</details>


### [124] [Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI](https://arxiv.org/abs/2510.00167)
*Diego Ortiz Barbosa,Mohit Agrawal,Yash Malegaonkar,Luis Burbano,Axel Andersson,György Dán,Henrik Sandberg,Alvaro A. Cardenas*

Main category: cs.AI

TL;DR: 该研究利用具身AI结合大型视觉语言模型，使无人机能对突发事件进行自适应决策，克服了传统手动编码规则的局限性，提升了自主空中系统的恢复能力和安全性，并在模拟环境中得到验证。


<details>
  <summary>Details</summary>
Motivation: 自主无人机需要对突发事件做出即时且自适应的决策，但传统的安全工程师手动编码恢复规则的方法无法预见所有现实世界的突发情况，很快就会变得不完整和不可靠。

Method: 利用由大型视觉语言模型驱动的具身AI，提供常识推理能力，实时评估上下文并生成适当的动作。通过在虚幻引擎的模拟城市基准中进行演示，无人机在此环境中动态解释周围环境并决定突发机动以实现安全着陆。

Result: 研究结果表明，具身AI使得一类新型的自适应恢复和决策流程成为可能，这些流程在以前是无法通过手动设计实现的。

Conclusion: 该方法通过实现自适应决策，显著提升了自主空中系统的恢复能力和安全性，为自主空中系统带来了新的设计范式。

Abstract: Autonomous drones must often respond to sudden events, such as alarms,
faults, or unexpected changes in their environment, that require immediate and
adaptive decision-making. Traditional approaches rely on safety engineers
hand-coding large sets of recovery rules, but this strategy cannot anticipate
the vast range of real-world contingencies and quickly becomes incomplete.
Recent advances in embodied AI, powered by large visual language models,
provide commonsense reasoning to assess context and generate appropriate
actions in real time. We demonstrate this capability in a simulated urban
benchmark in the Unreal Engine, where drones dynamically interpret their
surroundings and decide on sudden maneuvers for safe landings. Our results show
that embodied AI makes possible a new class of adaptive recovery and
decision-making pipelines that were previously infeasible to design by hand,
advancing resilience and safety in autonomous aerial systems.

</details>


### [125] [Object-Centric Case-Based Reasoning via Argumentation](https://arxiv.org/abs/2510.00185)
*Gabriel de Olim Gaul,Adam Gould,Avinash Kori,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出SAA-CBR，一种新型神经符号图像分类流程，融合了神经槽注意力（SA）的目标中心学习与基于案例推理的抽象论证（AA-CBR）的符号推理。


<details>
  <summary>Details</summary>
Motivation: 旨在通过整合目标中心学习（SA）与符号推理（AA-CBR），开发一种新颖有效的神经符号图像分类管道。

Method: 引入SAA-CBR，将神经槽注意力（SA）用于目标中心学习，并与基于案例推理的抽象论证（AA-CBR）的符号推理相结合。具体方法包括：探索特征组合策略、通过代表性样本的案例库缩减、新的基于计数的偏序、用于多分类的One-Vs-Rest策略以及支持型AA-CBR的应用。

Result: SAA-CBR在CLEVR-Hans数据集上被证明是一种有效的分类器，并与基线模型相比表现出具有竞争力的性能。

Conclusion: SAA-CBR作为一个有效的图像分类器，验证了其神经符号集成方法的潜力，并在特定数据集上取得了良好性能。

Abstract: We introduce Slot Attention Argumentation for Case-Based Reasoning (SAA-CBR),
a novel neuro-symbolic pipeline for image classification that integrates
object-centric learning via a neural Slot Attention (SA) component with
symbolic reasoning conducted by Abstract Argumentation for Case-Based Reasoning
(AA-CBR). We explore novel integrations of AA-CBR with the neural component,
including feature combination strategies, casebase reduction via representative
samples, novel count-based partial orders, a One-Vs-Rest strategy for extending
AA-CBR to multi-class classification, and an application of Supported AA-CBR, a
bipolar variant of AA-CBR. We demonstrate that SAA-CBR is an effective
classifier on the CLEVR-Hans datasets, showing competitive performance against
baseline models.

</details>


### [126] [Thinkquel: A Model Dedicated to Text-to-dbt Using Synthetic Data and a Span-Aware Objective](https://arxiv.org/abs/2510.00186)
*Anni Li,Aria Attar,Paul Dong*

Main category: cs.AI

TL;DR: Thinkquel模型通过结合创新的合成数据管道TS-SQL（利用dbt）和序列级强化学习算法TS-GRPO，显著提高了自然语言到SQL转换的准确性、鲁棒性和可移植性，并在实验中展现了卓越的性能提升和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 将自然语言请求转换为可靠、可生产的数据转换（SQL）极具挑战性，因为正确性依赖于精确的模式链接和SQL方言，而最强的监督信号（执行成功和结果匹配）仅在序列级别提供。此外，构建大规模、执行验证的语料库成本高昂，且令牌级目标与全局信号不匹配导致优化不稳定和可移植性受限。

Method: 本文提出了Thinkquel模型，一个经过微调的模型，旨在生成鲁棒、可移植且经过执行验证的数据库查询。Thinkquel整合了：1) 一个新颖的合成数据管道TS-SQL，利用dbt作为可移植的中间表示；2) 结合了感知跨度的强化学习目标；3) Token-Sequence GRPO (TS-GRPO) 算法，专门设计用于弥合大型语言模型微调时令牌级训练信号与序列级执行奖励之间的鸿沟。

Result: 在500个示例的TS-SQL测试集上，Thinkquel (32B) 通过两阶段SFT课程，实现了93.2%的执行成功率和61.8%的精确结果匹配，分别比基础模型提高了67.2%和44.4%。在Spider (14B) 实验中，TS-GRPO相对于GRPO和GSPO，增加了训练稳定性并加速了执行匹配奖励的收敛。

Conclusion: Thinkquel模型及其结合TS-SQL数据管道、dbt作为中间表示和TS-GRPO序列级强化学习方法，成功解决了自然语言到SQL转换中鲁棒性、可移植性和执行验证的挑战，并在实际应用中展现了显著的性能提升和更稳定的训练收敛性。

Abstract: Transforming natural-language requests into reliable, production-ready data
transformations remains challenging: correctness depends on precise schema
linking and warehouse-specific SQL dialects, while the strongest supervision
available during training--execution success and result matching--are provided
only at the sequence level. At the same time, assembling large,
execution-validated corpora is costly, and token-level objectives misalign with
these global signals, yielding unstable optimization and limited portability.
We introduce Thinkquel, a fine-tuned model for producing robust, portable, and
execution-validated database queries. Methodologies in Thinkquel integrates a
novel synthetic data pipeline, TS-SQL, that leverages dbt as a portable
intermediate representation with a span-aware reinforcement learning objective,
and Token-Sequence GRPO (TS-GRPO), specifically designed to bridge the gap
between token-level training signals and sequence-level execution rewards when
finetuning LLMs. On the 500-example TS-SQL test set, Thinkquel (32B) reaches
93.2\% execution success and 61.8\% exact-result match with a two-stage SFT
curriculum, improving over the base model by 67.2\% (exec.) and 44.4\% (match).
In Spider (14B) experiments, TS-GRPO increases training stability and speeds
convergence of the execution-match reward relative to GRPO and GSPO.

</details>


### [127] [DualTune: Decoupled Fine-Tuning for On-Device Agentic Systems](https://arxiv.org/abs/2510.00229)
*Rohan Kadekodi,Zhan Jin,Keisuke Kamahori,Yile Gu,Sean Khatiri,Noah H. Bayindirli,Sergey Gorbunov,Baris Kasikci*

Main category: cs.AI

TL;DR: 本文提出了一种名为“解耦微调”的新方法和“DualTune”推理框架，通过将工具调用任务分解为工具选择和参数生成，并利用LoRA适配器进行优化，显著提高了本地大模型在端侧设备上工具调用的准确性，使其性能超越同级别甚至更大的模型。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型（LLMs）作为智能体编排器已革新任务自动化，但为了实现隐私保护和成本效益，需要在设备上进行推理。然而，在工具调用场景中，本地LLMs在从大型工具集中选择工具和为复杂参数结构生成准确参数方面，始终不如前沿模型。

Method: 研究将工具调用任务分解为工具选择和参数生成两个子任务。提出了“解耦微调”，这是一种新颖的后训练方法，利用LoRA微调为工具选择和特定工具参数生成创建专用LoRA适配器，并对每个子任务使用单独的损失掩码。此外，提出了“DualTune”推理框架，它利用这些LoRA适配器在端侧设备上通过本地模型执行高效的智能体编排，动态加载适配器，并实现分层编排以限制工具数量。

Result: 在MCP-Bench基准测试中，使用解耦微调训练的Qwen-2.5-7B模型将基础模型的工具调用准确率提高了46%。它在所有情况下都优于其他本地推理、非推理和同等大小的微调模型，并且在大多数情况下优于2倍大小的模型。

Conclusion: 解耦微调和DualTune框架有效解决了本地LLMs在工具调用方面的性能瓶颈，使其在端侧设备上能进行高效、准确的智能体编排，从而在性能上大幅缩小了与前沿模型的差距。

Abstract: The deployment of Large Language Models (LLMs) as agentic orchestrators has
revolutionized task automation, but the need for privacy-preserving,
cost-effective solutions demands on-device inference capabilities. However,
local LLMs consistently underperform compared to frontier models in tool
calling scenarios, struggling with both tool selection from large tool sets and
accurate argument generation for complex parameter structures. We introduce a
methodology that disaggregates a tool-calling task into two distinct subtasks:
tool selection and argument generation. We propose "decoupled fine-tuning", a
novel post-training approach that employs LoRA fine-tuning to create dedicated
LoRA adapters for tool selection and tool-specific argument generation using
separate loss masking for each of the subtasks. Furthermore, we present
DualTune, an inference framework that leverages the LoRA adapters created using
decoupled fine-tuning to perform efficient agent orchestration with the help of
local models on end-user devices. DualTune decomposes the tool-call generation
step into tool selection and argument generation, and dynamically loads the
corresponding LoRA adapters to generate tool calls. Additionally, DualTune
implements hierarchical orchestration to restrict the number of tools required
for tool selection. Our experiments on the MCP-Bench benchmark demonstrate that
the Qwen-2.5-7B model trained using decoupled fine-tuning improves the tool
calling accuracy of the base model by 46%, and outperforms other local
reasoning, non-reasoning and fine-tuned models of similar size in all cases,
and models that are 2x larger, in most cases.

</details>


### [128] [MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning](https://arxiv.org/abs/2510.00274)
*Maisha Maliha,Dean Hougen*

Main category: cs.AI

TL;DR: 本文提出MAGIC-MASK，一个数学上严谨的框架，将基于扰动的可解释性扩展到多智能体强化学习（MARL），通过智能体协作和统一的数学形式，提升解释性、学习效率和策略鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 理解深度强化学习智能体的决策过程极具挑战，尤其在安全关键和多智能体环境中。现有解释性方法（如StateMask）存在计算成本高、探索覆盖不足及不适用于多智能体设置的局限性。

Method: MAGIC-MASK框架整合了近端策略优化（PPO）、自适应ε-greedy探索和轻量级智能体间协作，以共享掩码状态信息和经验。其核心创新在于通过轨迹扰动、奖励保真度分析和KL散度正则化等统一的数学形式，将可解释性从单智能体推广到多智能体系统。

Result: MAGIC-MASK显著减少了关键状态发现时间，提高了解释保真度，并实现了更快、更鲁棒的学习。在保真度、学习效率和策略鲁棒性方面持续优于现有SOTA基线，并提供可解释和可迁移的解释，已在单智能体和多智能体基准上验证。

Conclusion: MAGIC-MASK为多智能体强化学习的可解释性提供了一个有效且数学上严谨的解决方案，克服了现有方法的局限性，并在复杂多智能体环境中展现出卓越的性能和可解释性。

Abstract: Understanding the decision-making process of Deep Reinforcement Learning
agents remains a key challenge for deploying these systems in safety-critical
and multi-agent environments. While prior explainability methods like
StateMask, have advanced the identification of critical states, they remain
limited by computational cost, exploration coverage, and lack of adaptation to
multi-agent settings. To overcome these limitations, we propose a
mathematically grounded framework, MAGIC-MASK (Multi-Agent Guided Inter-agent
Collaboration with Mask-Based Explainability for Reinforcement Learning), that
extends perturbation-based explanation to Multi-Agent Reinforcement Learning.
Our method integrates Proximal Policy Optimization, adaptive epsilon-greedy
exploration, and lightweight inter-agent collaboration to share masked state
information and peer experience. This collaboration enables each agent to
perform saliency-guided masking and share reward-based insights with peers,
reducing the time required for critical state discovery, improving explanation
fidelity, and leading to faster and more robust learning. The core novelty of
our approach lies in generalizing explainability from single-agent to
multi-agent systems through a unified mathematical formalism built on
trajectory perturbation, reward fidelity analysis, and Kullback-Leibler
divergence regularization. This framework yields localized, interpretable
explanations grounded in probabilistic modeling and multi-agent Markov decision
processes. We validate our framework on both single-agent and multi-agent
benchmarks, including a multi-agent highway driving environment and Google
Research Football, demonstrating that MAGIC-MASK consistently outperforms
state-of-the-art baselines in fidelity, learning efficiency, and policy
robustness while offering interpretable and transferable explanations.

</details>


### [129] [ICL Optimized Fragility](https://arxiv.org/abs/2510.00300)
*Serena Gomez Wannaz*

Main category: cs.AI

TL;DR: 本研究探讨了ICL指南对GPT-OSS:20b模型跨领域认知能力的影响，发现ICL模型在通用知识上表现优异（91%-99%准确率），但在复杂推理（如谜题）上性能下降（10-43%），对奥数问题则无显著影响，揭示了效率与推理灵活性之间的“优化脆弱性”权衡。


<details>
  <summary>Details</summary>
Motivation: ICL指南已知能提升特定任务性能，但其对跨领域认知能力的影响尚未被探索。

Method: 使用GPT-OSS:20b模型的六种变体（一个基线模型和五种ICL配置），通过840项涵盖通用知识、逻辑谜题和数学奥林匹克问题的测试进行评估。采用统计分析（ANOVA）揭示行为修改。

Result: ICL变体显示出显著的行为修改（p < 0.001），表现出“优化脆弱性”现象。ICL模型在通用知识任务上达到91%-99%的准确率，但在复杂推理问题（如谜题）上性能下降至10-43%（基线模型为43%）。奥数问题上无显著差异（p=0.2173），表明复杂数学推理不受ICL优化影响。

Conclusion: ICL指南在效率和推理灵活性之间造成了系统性权衡，对LLM的部署和AI安全具有重要意义。

Abstract: ICL guides are known to improve task-specific performance, but their impact
on cross-domain cognitive abilities remains unexplored. This study examines how
ICL guides affect reasoning across different knowledge domains using six
variants of the GPT-OSS:20b model: one baseline model and five ICL
configurations (simple, chain-of-thought, random, appended text, and symbolic
language). The models were subjected to 840 tests spanning general knowledge
questions, logic riddles, and a mathematical olympiad problem. Statistical
analysis (ANOVA) revealed significant behavioral modifications (p less than
0.001) across ICL variants, demonstrating a phenomenon termed "optimized
fragility." ICL models achieved 91%-99% accuracy on general knowledge tasks
while showing degraded performance on complex reasoning problems, with accuracy
dropping to 10-43% on riddles compared to 43% for the baseline model. Notably,
no significant differences emerged on the olympiad problem (p=0.2173),
suggesting that complex mathematical reasoning remains unaffected by ICL
optimization. These findings indicate that ICL guides create systematic
trade-offs between efficiency and reasoning flexibility, with important
implications for LLM deployment and AI safety.

</details>


### [130] [BiasBusters: Uncovering and Mitigating Tool Selection Bias in Large Language Models](https://arxiv.org/abs/2510.00307)
*Thierry Blankenstein,Jialin Yu,Zixuan Li,Vassilis Plachouras,Sunando Sengupta,Philip Torr,Yarin Gal,Alasdair Paren,Adel Bibi*

Main category: cs.AI

TL;DR: LLM代理在选择功能等效工具时存在偏见，表现为固定选择特定提供商或偏好列表中靠前的工具。研究发现偏见源于查询与元数据语义对齐、描述扰动及预训练曝光，并提出通过过滤后均匀采样来缓解。


<details>
  <summary>Details</summary>
Motivation: LLM代理在从包含多个提供商的工具市场中选择功能等效工具时，系统性偏见可能损害用户体验并扭曲竞争，从而不公平地优待某些提供商。

Method: 1. 引入了一个包含多样工具类别和功能等效工具的基准，用于评估工具选择偏见。2. 使用该基准测试了七个模型。3. 进行受控实验，研究工具特征、元数据（名称、描述、参数）和预训练曝光对偏见来源的影响。4. 提出了一种轻量级缓解方案：首先将候选工具过滤到相关子集，然后进行均匀采样。

Result: 1. 存在工具选择偏见，模型表现为：固定选择单一提供商，或不成比例地偏好上下文中较早列出的工具。2. 偏见的起源包括：查询与元数据之间的语义对齐是选择的最强预测因子；扰动描述会显著改变选择；对单一端点的重复预训练曝光会放大偏见。3. 提出的缓解措施（过滤后均匀采样）能够减少偏见同时保持良好的任务覆盖率。

Conclusion: 工具选择偏见是公平部署工具增强型LLM的一个关键障碍。

Abstract: Agents backed by large language models (LLMs) often rely on external tools
drawn from marketplaces where multiple providers offer functionally equivalent
options. This raises a critical point concerning fairness: if selection is
systematically biased, it can degrade user experience and distort competition
by privileging some providers over others. We introduce a benchmark of diverse
tool categories, each containing multiple functionally equivalent tools, to
evaluate tool-selection bias. Using this benchmark, we test seven models and
show that unfairness exists with models either fixating on a single provider or
disproportionately preferring earlier-listed tools in context. To investigate
the origins of this bias, we conduct controlled experiments examining tool
features, metadata (name, description, parameters), and pre-training exposure.
We find that: (1) semantic alignment between queries and metadata is the
strongest predictor of choice; (2) perturbing descriptions significantly shifts
selections; and (3) repeated pre-training exposure to a single endpoint
amplifies bias. Finally, we propose a lightweight mitigation that first filters
the candidate tools to a relevant subset and then samples uniformly, reducing
bias while preserving good task coverage. Our findings highlight tool-selection
bias as a key obstacle for the fair deployment of tool-augmented LLMs.

</details>


### [131] [When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets](https://arxiv.org/abs/2510.00332)
*Zeshi Dai,Zimo Peng,Zerui Cheng,Ryan Yihe Li*

Main category: cs.AI

TL;DR: CAIA基准揭示，现有AI模型在对抗性、高风险且信息被武器化的环境中表现极差，即使配备工具也常误选信息源，远低于人类水平，表明其缺乏抵御主动欺骗的关键能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI评估基准仅衡量受控环境下的任务完成度，未能测试模型在虚假信息被武器化、错误不可逆转的对抗性、高风险环境中的操作能力。真实世界的部署需要AI具备对抗主动欺骗的韧性，例如在2024年因漏洞损失300亿美元的加密市场。

Method: 引入CAIA基准，以加密市场为测试平台，评估17个模型在178个时间锚定任务上的表现。这些任务要求代理在对抗压力下区分真伪、驾驭碎片化信息并做出不可逆的金融决策。研究对比了模型在有无工具辅助下的性能，并分析了它们的工具选择行为，同时审查了Pass@k指标对自主部署的适用性。

Result: 无工具时，即使是前沿模型在初级分析师能处理的任务上准确率仅28%。工具辅助下性能提升至67.4%，但仍远低于80%的人类基线。模型存在系统性工具选择灾难：它们倾向选择不可靠的网络搜索而非权威数据，易受SEO优化虚假信息和社交媒体操纵影响，即使正确答案通过专业工具可直接获取。此外，Pass@k指标可能掩盖自主部署中危险的试错行为。

Conclusion: 当前AI模型尽管在推理方面表现出色，但仍未准备好应对需要智能抵御主动反对的环境。CAIA基准（持续更新并控制污染）表明，对抗性鲁棒性是可信AI自主性的必要条件，其影响超越加密领域，延伸至网络安全、内容审核等任何存在活跃对手的领域。

Abstract: We present CAIA, a benchmark exposing a critical blind spot in AI evaluation:
the inability of state-of-the-art models to operate in adversarial, high-stakes
environments where misinformation is weaponized and errors are irreversible.
While existing benchmarks measure task completion in controlled settings,
real-world deployment demands resilience against active deception. Using crypto
markets as a testbed where $30 billion was lost to exploits in 2024, we
evaluate 17 models on 178 time-anchored tasks requiring agents to distinguish
truth from manipulation, navigate fragmented information landscapes, and make
irreversible financial decisions under adversarial pressure.
  Our results reveal a fundamental capability gap: without tools, even frontier
models achieve only 28% accuracy on tasks junior analysts routinely handle.
Tool augmentation improves performance but plateaus at 67.4% versus 80% human
baseline, despite unlimited access to professional resources. Most critically,
we uncover a systematic tool selection catastrophe: models preferentially
choose unreliable web search over authoritative data, falling for SEO-optimized
misinformation and social media manipulation. This behavior persists even when
correct answers are directly accessible through specialized tools, suggesting
foundational limitations rather than knowledge gaps. We also find that Pass@k
metrics mask dangerous trial-and-error behavior for autonomous deployment.
  The implications extend beyond crypto to any domain with active adversaries,
e.g. cybersecurity, content moderation, etc. We release CAIA with contamination
controls and continuous updates, establishing adversarial robustness as a
necessary condition for trustworthy AI autonomy. The benchmark reveals that
current models, despite impressive reasoning scores, remain fundamentally
unprepared for environments where intelligence must survive active opposition.

</details>


### [132] [Hierarchical Reasoning Model: A Critical Supplementary Material](https://arxiv.org/abs/2510.00355)
*Renee Ge,Qianli Liao,Tomaso Poggio*

Main category: cs.AI

TL;DR: 该研究对利用隐空间循环推理的Hierarchical Reasoning Model进行深入分析，提出改进变体，并在极难的2D推理任务（如数独和迷宫）上显著提升了Transformer的性能，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: Transformer在自然语言处理等领域表现卓越，但在逻辑推理方面仍有不足，可能源于对隐空间和循环推理探索不足。新兴的Hierarchical Reasoning Model在此方向展现潜力，但仍处于早期阶段，需要深入研究。

Method: 本研究对Hierarchical Reasoning Model这类模型进行了批判性回顾，并审查了其关键设计选择，以探索和提出改进的变体模型。

Result: 提出的模型变体在Sudoku-Extreme和Maze-Hard任务上取得了比以往报告显著更好的性能。

Conclusion: 研究结果不仅提升了模型性能，还提出了令人惊讶的发现，并为未来的研究指明了有趣的、有启发性的方向。

Abstract: Transformers have demonstrated remarkable performance in natural language
processing and related domains, as they largely focus on sequential,
autoregressive next-token prediction tasks. Yet, they struggle in logical
reasoning, not necessarily because of a fundamental limitation of these models,
but possibly due to the lack of exploration of more creative uses, such as
latent space and recurrent reasoning. An emerging exploration in this direction
is the Hierarchical Reasoning Model (Wang et al., 2025), which introduces a
novel type of recurrent reasoning in the latent space of transformers,
achieving remarkable performance on a wide range of 2D reasoning tasks. Despite
the promising results, this line of models is still at an early stage and calls
for in-depth investigation. In this work, we perform a critical review on this
class of models, examine key design choices and present intriguing variants
that achieve significantly better performance on the Sudoku-Extreme and
Maze-Hard tasks than previously reported. Our results also raise surprising
observations and intriguing directions for further research.

</details>


### [133] [Semantic-Driven AI Agent Communications: Challenges and Solutions](https://arxiv.org/abs/2510.00381)
*Kaiwen Yu,Mengying Sun,Zhijin Qin,Xiaodong Xu,Ping Yang,Yue Xiao,Gang Wu*

Main category: cs.AI

TL;DR: 本文提出了一种语义驱动的AI智能体通信框架，包含语义自适应传输、语义轻量化传输和语义自演进控制三种技术，以解决动态环境和资源受限问题，并实现更快的收敛速度和更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着智能服务的快速增长，通信目标正从人类转向AI智能体，这要求新的范式实现实时感知、决策和协作。语义通信虽有前景，但其部署受限于动态环境和有限资源。

Method: 文章提出了一个语义驱动的AI智能体通信框架，并开发了三种使能技术：1) 语义自适应传输，通过微调模型适应环境；2) 语义轻量化传输，通过剪枝、量化和感知感知采样降低模型复杂度；3) 语义自演进控制，通过分布式分层决策优化多维资源，实现多智能体协作。

Result: 仿真结果表明，所提出的解决方案实现了更快的收敛速度和更强的鲁棒性。特别是，所提出的分布式分层优化方法显著优于传统的决策方案。

Conclusion: 该语义驱动的AI智能体通信框架及其使能技术，尤其是在分布式分层优化方面的表现，凸显了其在AI智能体通信网络中的巨大潜力。

Abstract: With the rapid growth of intelligent services, communication targets are
shifting from humans to artificial intelligent (AI) agents, which require new
paradigms to enable real-time perception, decision-making, and collaboration.
Semantic communication, which conveys task-relevant meaning rather than raw
data, offers a promising solution. However, its practical deployment remains
constrained by dynamic environments and limited resources. To address these
issues, this article proposes a semantic-driven AI agent communication
framework and develops three enabling techniques. First, semantic adaptation
transmission applies fine-tuning with real or generative samples to efficiently
adapt models to varying environments. Second, semantic lightweight transmission
incorporates pruning, quantization, and perception-aware sampling to reduce
model complexity and alleviate computational burden on edge agents. Third,
semantic self-evolution control employs distributed hierarchical
decision-making to optimize multi-dimensional resources, enabling robust
multi-agent collaboration in dynamic environments. Simulation results show that
the proposed solutions achieve faster convergence and stronger robustness,
while the proposed distributed hierarchical optimization method significantly
outperforms conventional decision-making schemes, highlighting its potential
for AI agent communication networks.

</details>


### [134] [Towards Self-Evolving Benchmarks: Synthesizing Agent Trajectories via Test-Time Exploration under Validate-by-Reproduce Paradigm](https://arxiv.org/abs/2510.00415)
*Dadi Guo,Tianyi Zhou,Dongrui Liu,Chen Qian,Qihan Ren,Shuai Shao,Zhiyuan Fan,Yi R. Fung,Kun Wang,Linfeng Zhang,Jing Shao*

Main category: cs.AI

TL;DR: 该论文提出了TRACE框架，一种动态、自进化的智能体基准测试方法，旨在通过轨迹验证来提高任务复杂性和结果可靠性，以应对现有基准测试快速达到上限的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）和智能体系统设计使智能体能力显著提升，但现有智能体基准测试很快被新开发的智能体“触顶”，难以有效评估智能体能力。

Method: TRACER框架通过鼓励智能体自由探索和演化现有基准任务，生成难度更高的任务，并记录可验证的智能体轨迹。它包含三个阶段：1) 演化提案挖掘（通过初步探索和发散思维提供任务演化建议）；2) 问题形成与自由探索（将提案概念化为可行问题，智能体自由探索并记录执行轨迹）；3) 多层级验证（确保演化出的任务伴随可验证和可复现的轨迹）。

Result: 在GAIA基准上的实验表明，TRACE框架能持续提升任务复杂性，并通过可验证的执行轨迹提高正确性的可靠性。

Conclusion: 这项工作标志着从静态、手动策划的基准测试向动态、自进化的评估系统的范式转变，为智能体发展提供了一个可持续且富有挑战性的评估跑道。

Abstract: Recent advances in large language models (LLMs) and agent system designs have
empowered agents with unprecedented levels of capability. However, existing
agent benchmarks are showing a trend of rapid ceiling-hitting by newly
developed agents, making it difficult to meet the demands for evaluating agent
abilities. To address this problem, we propose the Trajectory-based
Validated-by-Reproducing Agent-benchmark Complexity Evolution (TRACE)
framework. This framework takes an original task from an existing benchmark and
encourages agents to freely explore and evolve it into a new task with higher
difficulty while recording validatable agent trajectories. The framework
proceeds in three stages: (1) evolutionary proposal mining, which provides task
evolution proposals through preliminary exploration and divergent thinking; (2)
problem formation and free exploration, where proposals are conceptualized into
feasible problem candidates and the agents then explore them freely while
recording their execution trajectories; and (3) multi-level validation, which
ensures that the evolved tasks are accompanied by validatable and reproducible
trajectories. Experiments on the GAIA benchmark demonstrate that the TRACE
framework consistently enhances task complexity while improving the reliability
of correctness through validatable execution trajectories. This work marks a
paradigm shift from static, manually curated benchmarks to dynamic,
self-evolving evaluation systems, providing a sustainable and challenging
runway for agent development.

</details>


### [135] [Automated Evaluation can Distinguish the Good and Bad AI Responses to Patient Questions about Hospitalization](https://arxiv.org/abs/2510.00436)
*Sarvesh Soni,Dina Demner-Fushman*

Main category: cs.AI

TL;DR: 本研究发现，对AI回答患者健康问题的自动化评估方法与专家判断高度一致，有望解决传统评估的效率瓶颈并提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前评估AI系统回答患者健康问题（特别是住院相关问题）的黄金标准——人工专家评审，耗时且难以扩展。尽管自动化指标有前景，但它们与人类判断的一致性不高且常受限于上下文，因此需要一种可行且可靠的自动化评估方法。

Method: 研究针对100个患者案例，收集了来自28个AI系统（共2800个回复）的回答。评估维度包括：1) 是否回答了问题，2) 是否恰当使用了临床笔记证据，以及3) 是否使用了通用医学知识。通过利用临床医生撰写的参考答案作为基准来校准自动化评估指标。

Result: 研究发现，使用临床医生撰写的参考答案作为锚定指标后，自动化排名与专家评分高度吻合。

Conclusion: 研究结果表明，精心设计的自动化评估方法能够有效实现AI系统比较评估的规模化，并有助于支持医患沟通。

Abstract: Automated approaches to answer patient-posed health questions are rising, but
selecting among systems requires reliable evaluation. The current gold standard
for evaluating the free-text artificial intelligence (AI) responses--human
expert review--is labor-intensive and slow, limiting scalability. Automated
metrics are promising yet variably aligned with human judgments and often
context-dependent. To address the feasibility of automating the evaluation of
AI responses to hospitalization-related questions posed by patients, we
conducted a large systematic study of evaluation approaches. Across 100 patient
cases, we collected responses from 28 AI systems (2800 total) and assessed them
along three dimensions: whether a system response (1) answers the question, (2)
appropriately uses clinical note evidence, and (3) uses general medical
knowledge. Using clinician-authored reference answers to anchor metrics,
automated rankings closely matched expert ratings. Our findings suggest that
carefully designed automated evaluation can scale comparative assessment of AI
systems and support patient-clinician communication.

</details>


### [136] [Expandable Decision-Making States for Multi-Agent Deep Reinforcement Learning in Soccer Tactical Analysis](https://arxiv.org/abs/2510.00480)
*Kenjiro Ide,Taiga Someya,Kohei Kawaguchi,Keisuke Fujii*

Main category: cs.AI

TL;DR: 本文提出EDMS框架，通过语义丰富的状态表示和动作掩码，为团队体育构建可解释且鲁棒的球员级代理模型，有效降低预测误差并揭示关键战术模式。


<details>
  <summary>Details</summary>
Motivation: 团队体育的战术分析面临高维度、强耦合状态空间的挑战。传统分析方法缺乏深度，现有机器学习模型缺乏明确的代理表示，难以从数据中构建可解释且鲁棒的球员级代理模型。

Method: 提出可扩展决策状态（EDMS）框架。该方法通过：1) 语义丰富的状态表示，将原始位置/速度数据与关系变量（如空间得分、传球得分）相结合；2) 动作掩码机制，为持球和非持球代理提供不同的决策集。这使得学习到的价值函数和策略能映射到人类可解释的战术概念。

Result: 实验表明，EDMS与动作掩码相结合，相比基线模型持续降低了动作预测损失和时序差分（TD）误差。定性案例研究和Q值可视化进一步表明，EDMS能突出高风险、高回报的战术模式（如快速反击和防守突破）。该方法已集成到开源库中，并兼容多种数据集。

Conclusion: EDMS提供了一种有效且可解释的团队体育战术分析方法，通过语义丰富的状态表示和动作掩码机制，提升了模型预测精度，并能识别和可视化关键战术模式，为定量战术分析提供了鲁棒且可跨数据源使用的工具。

Abstract: Invasion team sports such as soccer produce a high-dimensional, strongly
coupled state space as many players continuously interact on a shared field,
challenging quantitative tactical analysis. Traditional rule-based analyses are
intuitive, while modern predictive machine learning models often perform
pattern-matching without explicit agent representations. The problem we address
is how to build player-level agent models from data, whose learned values and
policies are both tactically interpretable and robust across heterogeneous data
sources. Here, we propose Expandable Decision-Making States (EDMS), a
semantically enriched state representation that augments raw positions and
velocities with relational variables (e.g., scoring of space, pass, and score),
combined with an action-masking scheme that gives on-ball and off-ball agents
distinct decision sets. Compared to prior work, EDMS maps learned value
functions and action policies to human-interpretable tactical concepts (e.g.,
marking pressure, passing lanes, ball accessibility) instead of raw coordinate
features, and aligns agent choices with the rules of play. In the experiments,
EDMS with action masking consistently reduced both action-prediction loss and
temporal-difference (TD) error compared to the baseline. Qualitative case
studies and Q-value visualizations further indicate that EDMS highlights
high-risk, high-reward tactical patterns (e.g., fast counterattacks and
defensive breakthroughs). We also integrated our approach into an open-source
library and demonstrated compatibility with multiple commercial and open
datasets, enabling cross-provider evaluation and reproducible experiments.

</details>


### [137] [Rethinking Reward Models for Multi-Domain Test-Time Scaling](https://arxiv.org/abs/2510.00492)
*Dong Bok Lee,Seanie Lee,Sangwoo Park,Minki Kang,Jinheon Baek,Dongki Kim,Dominik Wagner,Jiongdao Jin,Heejun Lee,Tobias Bocklet,Jinyu Wang,Jingjing Fu,Sung Ju Hwang,Jiang Bia,Lei Song*

Main category: cs.AI

TL;DR: 生成式结果奖励模型（GenORM）在多样化领域表现最佳，挑战了过程奖励模型（PRM）优于结果奖励模型（ORM）的传统观点。


<details>
  <summary>Details</summary>
Motivation: 现有研究普遍假设评估中间推理步骤的过程奖励模型（PRM）优于仅评估最终答案的结果奖励模型（ORM），但这一结论主要基于狭窄的数学领域。本研究旨在对不同类型的奖励模型在多样化领域进行统一评估。

Method: 本研究首次对四种奖励模型变体（判别式ORM、判别式PRM、生成式ORM、生成式PRM）在14个多样化领域进行了统一评估，并进行了理论分析以解释观察到的现象。

Result: 研究发现，与传统观点相反：(i) 判别式ORM与判别式PRM表现相当；(ii) 生成式PRM不具竞争力；(iii) 生成式ORM整体上最为鲁棒，在所有测试领域都取得了显著且一致的提升。这归因于PRM逐步评分继承了标签噪声并难以评估长推理轨迹，理论分析也证实了随着推理长度增加，逐步聚合会累积错误。

Conclusion: 这些发现挑战了细粒度监督总是更好的普遍假设，并支持生成式结果验证（GenORM）在多领域大语言模型部署中的应用。

Abstract: The reliability of large language models (LLMs) during test-time scaling is
often assessed with \emph{external verifiers} or \emph{reward models} that
distinguish correct reasoning from flawed logic. Prior work generally assumes
that process reward models (PRMs), which score every intermediate reasoning
step, outperform outcome reward models (ORMs) that assess only the final
answer. This view is based mainly on evidence from narrow, math-adjacent
domains. We present the first unified evaluation of four reward model variants,
discriminative ORM and PRM (\DisORM, \DisPRM) and generative ORM and PRM
(\GenORM, \GenPRM), across 14 diverse domains. Contrary to conventional wisdom,
we find that (i) \DisORM performs on par with \DisPRM, (ii) \GenPRM is not
competitive, and (iii) overall, \GenORM is the most robust, yielding
significant and consistent gains across every tested domain. We attribute this
to PRM-style stepwise scoring, which inherits label noise from LLM
auto-labeling and has difficulty evaluating long reasoning trajectories,
including those involving self-correcting reasoning. Our theoretical analysis
shows that step-wise aggregation compounds errors as reasoning length grows,
and our empirical observations confirm this effect. These findings challenge
the prevailing assumption that fine-grained supervision is always better and
support generative outcome verification for multi-domain deployment. We
publicly release our code, datasets, and checkpoints at
\href{https://github.com/db-Lee/Multi-RM}{\underline{\small\texttt{https://github.com/db-Lee/Multi-RM}}}
to facilitate future research in multi-domain settings.

</details>


### [138] [VIRTUE: Visual-Interactive Text-Image Universal Embedder](https://arxiv.org/abs/2510.00523)
*Wei-Yao Wang,Kazuya Tateishi,Qiyu Wu,Shusuke Takahashi,Yuki Mitsufuji*

Main category: cs.AI

TL;DR: 本文提出VIRTUE，一个新型视觉交互式文本-图像通用嵌入器，它将分割模型和视觉-语言模型结合，使嵌入模型能够通过视觉提示指定图像区域，并在多项通用及视觉交互任务上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入模型缺乏视觉交互能力，无法根据用户指定的区域（如点、边界框、掩码）聚焦图像特定区域。这限制了其在需要局部化用户意图理解的新应用中的潜力，也未能充分利用图像中的实体级信息来补充全局表示。

Method: 本文提出VIRTUE（Visual-InteRactive Text-Image Universal Embedder），通过结合分割模型和视觉-语言模型，使其能够处理指向图像特定区域的视觉提示，从而更精确地处理复杂和模糊场景。为评估VIRTUE的视觉交互能力，引入了大规模SCaR（Segmentation-and-Scene Caption Retrieval）基准测试，包含100万样本。

Result: VIRTUE在36个通用MMEB任务上实现了3.1%-8.5%的显著性能提升，并在5个视觉交互式SCaR任务上取得了15.2%-20.3%的显著提升，均达到最先进水平。

Conclusion: VIRTUE成功地为嵌入模型引入了视觉交互能力，使其能通过局部化意图理解解锁新应用，并利用实体级信息提升表示能力，在多项通用和视觉交互任务中展现出卓越的性能。

Abstract: Multimodal representation learning models have demonstrated successful
operation across complex tasks, and the integration of vision-language models
(VLMs) has further enabled embedding models with instruction-following
capabilities. However, existing embedding models lack visual-interactive
capabilities to specify regions of interest from users (e.g., point, bounding
box, mask), which have been explored in generative models to broaden their
human-interactive applicability. Equipping embedding models with visual
interactions not only would unlock new applications with localized grounding of
user intent, which remains unexplored, but also enable the models to learn
entity-level information within images to complement their global
representations for conventional embedding tasks. In this paper, we propose a
novel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends
the capabilities of the segmentation model and the vision-language model to the
realm of representation learning. In VIRTUE, the segmentation model can process
visual prompts that pinpoint specific regions within an image, thereby enabling
the embedder to handle complex and ambiguous scenarios more precisely. To
evaluate the visual-interaction ability of VIRTUE, we introduce a large-scale
Segmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples
that aims to retrieve the text caption by jointly considering the entity with a
specific object and image scene. VIRTUE consistently achieves a
state-of-the-art performance with significant improvements across 36 universal
MMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.

</details>


### [139] [Data Quality Challenges in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.00552)
*Leopold Müller,Joshua Holstein,Sarah Bause,Gerhard Satzger,Niklas Kühl*

Main category: cs.AI

TL;DR: 现有数据质量（DQ）框架无法有效应对RAG系统的动态性，本研究通过访谈归纳出15个RAG系统DQ维度，揭示了需新增维度、关注早期阶段及动态DQ管理的需求。


<details>
  <summary>Details</summary>
Motivation: 组织日益采用RAG来增强大语言模型，但现有DQ框架主要为静态数据集设计，无法充分解决RAG系统动态、多阶段的特性，因此本研究旨在为RAG这类新型AI系统开发DQ维度。

Method: 对领先IT服务公司的16位从业者进行了半结构化访谈，并通过定性内容分析，归纳推导出RAG系统四个处理阶段（数据提取、数据转换、提示与搜索、生成）的15个独特DQ维度。

Result: 研究发现：1) 需向传统DQ框架中添加新维度以覆盖RAG上下文；2) 这些新维度集中在RAG早期步骤，表明需要前端化的质量管理策略；3) DQ问题在RAG管道中会转化和传播，需要动态的、阶段感知的质量管理方法。

Conclusion: RAG系统的数据质量管理需要一套新的、动态的、阶段感知的框架，尤其要关注早期阶段的质量控制，以有效应对DQ问题在整个管道中的演变和传播。

Abstract: Organizations increasingly adopt Retrieval-Augmented Generation (RAG) to
enhance Large Language Models with enterprise-specific knowledge. However,
current data quality (DQ) frameworks have been primarily developed for static
datasets, and only inadequately address the dynamic, multi-stage nature of RAG
systems. This study aims to develop DQ dimensions for this new type of AI-based
systems. We conduct 16 semi-structured interviews with practitioners of leading
IT service companies. Through a qualitative content analysis, we inductively
derive 15 distinct DQ dimensions across the four processing stages of RAG
systems: data extraction, data transformation, prompt & search, and generation.
Our findings reveal that (1) new dimensions have to be added to traditional DQ
frameworks to also cover RAG contexts; (2) these new dimensions are
concentrated in early RAG steps, suggesting the need for front-loaded quality
management strategies, and (3) DQ issues transform and propagate through the
RAG pipeline, necessitating a dynamic, step-aware approach to quality
management.

</details>


### [140] [Toward Safer Diffusion Language Models: Discovery and Mitigation of Priming Vulnerability](https://arxiv.org/abs/2510.00565)
*Shojiro Yamabe,Jun Sakuma*

Main category: cs.AI

TL;DR: 扩散语言模型（DLMs）的迭代去噪过程存在关键安全漏洞，通过中间步骤注入肯定词可绕过安全防护。本文揭示此漏洞，并提出一种新的DLM专用安全对齐方法，能有效缓解该漏洞并增强对传统越狱攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前对利用扩散语言模型（DLMs）迭代去噪推理机制的越狱攻击所带来的安全风险缺乏深入理解。

Method: 本文揭示DLM迭代去噪过程中的关键漏洞，即若在中间步骤出现有害查询的肯定词，后续去噪过程会引导模型产生有害响应。在此分析基础上，提出一种专门针对DLM的新型安全对齐方法，通过训练模型从包含肯定词的污染中间状态生成安全响应。

Result: 研究发现DLM存在源于迭代去噪过程的关键漏洞，导致注入肯定词可轻易绕过安全防护。该漏洞使得现有基于优化的越狱攻击对DLM有效。提出的方法显著缓解了此漏洞，对任务性能影响最小，并提高了对传统越狱攻击的鲁棒性。

Conclusion: 本工作强调了开展针对DLM特有安全研究的必要性。

Abstract: Diffusion language models (DLMs) generate tokens in parallel through
iterative denoising, which can reduce latency and enable bidirectional
conditioning. However, the safety risks posed by jailbreak attacks that exploit
this inference mechanism are not well understood. In this paper, we reveal that
DLMs have a critical vulnerability stemming from their iterative denoising
process and propose a countermeasure. Specifically, our investigation shows
that if an affirmative token for a harmful query appears at an intermediate
step, subsequent denoising can be steered toward a harmful response even in
aligned models. As a result, simply injecting such affirmative tokens can
readily bypass the safety guardrails. Furthermore, we demonstrate that the
vulnerability allows existing optimization-based jailbreak attacks to succeed
on DLMs. Building on this analysis, we propose a novel safety alignment method
tailored to DLMs that trains models to generate safe responses from
contaminated intermediate states that contain affirmative tokens. Our
experiments indicate that the proposed method significantly mitigates the
vulnerability with minimal impact on task performance. Furthermore, our method
improves robustness against conventional jailbreak attacks. Our work
underscores the need for DLM-specific safety research.

</details>


### [141] [ACON: Optimizing Context Compression for Long-horizon LLM Agents](https://arxiv.org/abs/2510.00615)
*Minki Kang,Wei-Ning Chen,Dongge Han,Huseyin A. Inan,Lukas Wutschitz,Yanzhi Chen,Robert Sim,Saravan Rajmohan*

Main category: cs.AI

TL;DR: ACON是一种统一框架，通过优化压缩指南和蒸馏到小模型，有效压缩LLM代理在动态环境中的上下文，显著降低内存使用并保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为代理在真实世界环境中部署，其上下文长度不断增长，导致成本增加和长周期任务效率降低。现有上下文压缩方法多关注单步或狭窄应用，无法解决代理任务中复杂的上下文管理问题。

Method: 引入Agent Context Optimization (ACON) 框架，用于优化压缩环境观测和交互历史。ACON通过自然语言空间中的压缩指南优化来实现：当压缩上下文失败而完整上下文成功时，LLM分析失败原因并更新压缩指南。此外，将优化后的LLM压缩器蒸馏到更小的模型中以降低开销。

Result: 实验表明，ACON在AppWorld、OfficeBench和Multi-objective QA上将内存使用（峰值tokens）减少了26-54%，同时基本保持了任务性能。当蒸馏到更小的压缩器时，准确率保留了95%以上，并将小型语言模型作为长周期代理的性能提升了高达46%。

Conclusion: ACON成功解决了LLM代理在动态环境中上下文长度增长带来的挑战，通过智能压缩和模型蒸馏，实现了显著的内存效率提升，同时保持或提高了任务性能，并增强了小型语言模型作为长周期代理的能力。

Abstract: Large language models (LLMs) are increasingly deployed as agents in dynamic,
real-world environments, where success requires both reasoning and effective
tool use. A central challenge for agentic tasks is the growing context length,
as agents must accumulate long histories of actions and observations. This
expansion raises costs and reduces efficiency in long-horizon tasks, yet prior
work on context compression has mostly focused on single-step tasks or narrow
applications. We introduce Agent Context Optimization (ACON), a unified
framework that optimally compresses both environment observations and
interaction histories into concise yet informative condensations. ACON
leverages compression guideline optimization in natural language space: given
paired trajectories where full context succeeds but compressed context fails,
capable LLMs analyze the causes of failure, and the compression guideline is
updated accordingly. Furthermore, we propose distilling the optimized LLM
compressor into smaller models to reduce the overhead of the additional module.
Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON
reduces memory usage by 26-54% (peak tokens) while largely preserving task
performance, preserves over 95% of accuracy when distilled into smaller
compressors, and enhances smaller LMs as long-horizon agents with up to 46%
performance improvement.

</details>


### [142] [HARPA: A Testability-Driven, Literature-Grounded Framework for Research Ideation](https://arxiv.org/abs/2510.00620)
*Rosni Vasu,Peter Jansen,Pao Siangliulue,Cristina Sarasua,Abraham Bernstein,Peter Clark,Bhavana Dalvi Mishra*

Main category: cs.AI

TL;DR: HARPA是一个AI工具，通过模拟人类研究工作流程，旨在自动化科学发现中生成可测试且有文献依据的假设，并能根据实验结果自适应调整。它在假设的可行性和扎根性上显著优于现有方法，并能提高实际实验执行的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前的自动化科学发现 (ASD) 工具（尤其是在LLMs兴起后）在生成既可测试又基于科学文献的假设方面仍面临挑战。此外，现有构思工具无法根据先前的实验结果进行自适应调整。

Method: 本文开发了HARPA工具，该工具模仿人类研究人员的构思工作流程。HARPA首先通过文献挖掘识别新兴研究趋势，然后探索假设设计空间，最后通过找出研究空白并论证设计选择，收敛于精确、可测试的假设。为实现适应性，HARPA还学习一个奖励模型，根据先前的实验结果对新假设进行评分。

Result: HARPA生成的假设驱动研究提案在大多数定性维度（如特异性、新颖性、整体质量）上与强大的基线AI研究员相当。在可行性上显著提高 (+0.78, p<0.05)，在扎根性上显著提高 (+0.85, p<0.01)。与ASD代理 (CodeScientist) 结合使用时，HARPA产生了更多成功的执行 (20 vs. 11 out of 40) 和更少的失败 (16 vs. 21 out of 40)。此外，HARPA通过学习奖励模型，实现了约28%的绝对增益，以优化基于先前实验结果的假设评分。

Conclusion: HARPA通过其创新的方法，在AI驱动的科学发现领域迈出了重要一步，有效解决了生成可测试、有依据且能适应实验结果的假设的挑战。

Abstract: While there has been a surge of interest in automated scientific discovery
(ASD), especially with the emergence of LLMs, it remains challenging for tools
to generate hypotheses that are both testable and grounded in the scientific
literature. Additionally, existing ideation tools are not adaptive to prior
experimental outcomes. We developed HARPA to address these challenges by
incorporating the ideation workflow inspired by human researchers. HARPA first
identifies emerging research trends through literature mining, then explores
hypothesis design spaces, and finally converges on precise, testable hypotheses
by pinpointing research gaps and justifying design choices. Our evaluations
show that HARPA-generated hypothesis-driven research proposals perform
comparably to a strong baseline AI-researcher across most qualitative
dimensions (e.g., specificity, novelty, overall quality), but achieve
significant gains in feasibility(+0.78, p$<0.05$, bootstrap) and groundedness
(+0.85, p$<0.01$, bootstrap) on a 10-point Likert scale. When tested with the
ASD agent (CodeScientist), HARPA produced more successful executions (20 vs. 11
out of 40) and fewer failures (16 vs. 21 out of 40), showing that expert
feasibility judgments track with actual execution success. Furthermore, to
simulate how researchers continuously refine their understanding of what
hypotheses are both testable and potentially interesting from experience, HARPA
learns a reward model that scores new hypotheses based on prior experimental
outcomes, achieving approx. a 28\% absolute gain over HARPA's untrained
baseline scorer. Together, these methods represent a step forward in the field
of AI-driven scientific discovery.

</details>


### [143] [Is Model Editing Built on Sand? Revealing Its Illusory Success and Fragile Foundation](https://arxiv.org/abs/2510.00625)
*Wei Liu,Haomei Xu,Bingqing Liu,Zhiying Deng,Haozhao Wang,Jun Wang,Ruixuan Li,Yee Whye Teh,Wee Sun Lee*

Main category: cs.AI

TL;DR: 当前大型语言模型编辑方法的成功是虚幻的，它们依赖于捷径而非真实的语义理解，导致在面对否定查询时崩溃，急需重新评估其基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型不可避免地编码过时或错误知识，而模型编辑被视为解决此问题的有前途范式。然而，作者质疑其表面上的可靠性，认为现有文献可能被虚幻的成功所驱动。

Method: 为了揭示模型编辑的潜在问题，作者系统地开发了一套新的评估方法，这些方法特别设计了负面示例，以挑战模型编辑的鲁棒性。

Result: 研究发现，最先进的模型编辑方法在最简单的否定查询下也会崩溃。经验证据表明，编辑很可能基于捷径而非完整的语义，这直接挑战了当前模型编辑文献的根本可行性。

Conclusion: 模型编辑的成功可能是基于捷径而非深层语义，其基础需要紧急重新考虑，才能有意义地推进未来的研究。

Abstract: Large language models (LLMs) inevitably encode outdated or incorrect
knowledge. Updating, deleting, and forgetting such knowledge is important for
alignment, safety, and other issues. To address this issue, model editing has
emerged as a promising paradigm: by precisely editing a small subset of
parameters such that a specific fact is updated while preserving other
knowledge. Despite its great success reported in previous papers, we find the
apparent reliability of editing rests on a fragile foundation and the current
literature is largely driven by illusory success. The fundamental goal of
steering the model's output toward a target with minimal modification would
encourage exploiting hidden shortcuts, rather than utilizing real semantics.
This problem directly challenges the feasibility of the current model editing
literature at its very foundation, as shortcuts are inherently at odds with
robust knowledge integration. Coincidentally, this issue has long been obscured
by evaluation frameworks that lack the design of negative examples. To uncover
it, we systematically develop a suite of new evaluation methods. Strikingly, we
find that state-of-the-art approaches collapse even under the simplest negation
queries. Our empirical evidence shows that editing is likely to be based on
shortcuts rather than full semantics, calling for an urgent reconsideration of
the very basis of model editing before further advancements can be meaningfully
pursued.

</details>


### [144] [Collaborative-Distilled Diffusion Models (CDDM) for Accelerated and Lightweight Trajectory Prediction](https://arxiv.org/abs/2510.00627)
*Bingzhang Wang,Kehua Chen,Yinhai Wang*

Main category: cs.AI

TL;DR: 本文提出协同蒸馏扩散模型（CDDM），通过知识蒸馏显著压缩扩散模型并加速其采样过程，实现在保持高预测精度的同时，对自动驾驶和智能交通系统进行实时、轻量级的轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在轨迹预测任务中表现优异，但其巨大的模型尺寸和缓慢的采样过程限制了在自动驾驶（AVs）和智能交通系统（ITS）中实际部署。

Method: 本文提出协同蒸馏扩散模型（CDDM），基于协同渐进蒸馏（CPD），将高容量教师扩散模型的知识逐步转移到轻量级学生模型，迭代地同时减少采样步数和模型尺寸。此外，引入双信号正则化蒸馏损失，结合教师模型和真实数据进行指导，以防止过拟合并确保性能鲁棒性。

Result: 在ETH-UCY行人基准和nuScenes车辆基准上，CDDM实现了最先进（SOTA）的预测精度。精炼后的CDDM在行人轨迹上保留了基线模型96.2%的ADE和95.5%的FDE性能，同时仅需231K参数和4或2个采样步长，分别实现了161倍压缩、31倍加速和9毫秒延迟。定性结果表明CDDM能在动态智能体行为和复杂社交互动下生成多样且准确的轨迹。

Conclusion: CDDM通过连接高性能生成模型与实际部署限制，为自动驾驶和智能交通系统提供了资源高效的概率性轨迹预测能力。

Abstract: Trajectory prediction is a fundamental task in Autonomous Vehicles (AVs) and
Intelligent Transportation Systems (ITS), supporting efficient motion planning
and real-time traffic safety management. Diffusion models have recently
demonstrated strong performance in probabilistic trajectory prediction, but
their large model size and slow sampling process hinder real-world deployment.
This paper proposes Collaborative-Distilled Diffusion Models (CDDM), a novel
method for real-time and lightweight trajectory prediction. Built upon
Collaborative Progressive Distillation (CPD), CDDM progressively transfers
knowledge from a high-capacity teacher diffusion model to a lightweight student
model, jointly reducing both the number of sampling steps and the model size
across distillation iterations. A dual-signal regularized distillation loss is
further introduced to incorporate guidance from both the teacher and
ground-truth data, mitigating potential overfitting and ensuring robust
performance. Extensive experiments on the ETH-UCY pedestrian benchmark and the
nuScenes vehicle benchmark demonstrate that CDDM achieves state-of-the-art
prediction accuracy. The well-distilled CDDM retains 96.2% and 95.5% of the
baseline model's ADE and FDE performance on pedestrian trajectories, while
requiring only 231K parameters and 4 or 2 sampling steps, corresponding to 161x
compression, 31x acceleration, and 9 ms latency. Qualitative results further
show that CDDM generates diverse and accurate trajectories under dynamic agent
behaviors and complex social interactions. By bridging high-performing
generative models with practical deployment constraints, CDDM enables
resource-efficient probabilistic prediction for AVs and ITS. Code is available
at https://github.com/bingzhangw/CDDM.

</details>


### [145] [Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution](https://arxiv.org/abs/2510.00636)
*Alessio Devoto,Maximilian Jeblick,Simon Jégou*

Main category: cs.AI

TL;DR: 提出免训练的“Expected Attention”方法，通过预测未来查询注意力分数来压缩LLM KV缓存，有效降低内存消耗并优于现有技术，同时发布KVPress库。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理中，KV缓存的内存消耗是主要瓶颈。现有基于注意力分数的压缩方法存在实际局限性，如未来分数不可用和无法访问完整注意力矩阵。

Method: 引入“Expected Attention”方法，这是一种免训练的压缩技术。它通过预测未来查询对KV对的关注度来评估KV对的重要性，并利用LLM激活的分布特性以闭合形式计算预期注意力分数，从而进行KV对的排序和剪枝，以实现高效压缩且不影响性能。

Result: 该方法在预填充和解码阶段均能无缝运行，并持续优于现有最先进的基线方法，实现了有效压缩且没有性能下降。此外，还发布了KVPress库，一个包含20多种KV缓存压缩技术的综合性工具。

Conclusion: “Expected Attention”为LLM KV缓存提供了一种实用的、高效的压缩解决方案，克服了现有方法的挑战，显著提升了推理效率。KVPress库的发布也为该领域的研究提供了宝贵的工具。

Abstract: Memory consumption of the Key-Value (KV) cache represents a major bottleneck
for efficient large language model inference. While attention-score-based KV
cache pruning shows promise, it faces critical practical limitations: attention
scores from future tokens are unavailable during compression, and modern
implementations like Flash Attention do not materialize the full attention
matrix, making past scores inaccessible. To overcome these challenges, we
introduce $\textbf{Expected Attention, a training-free compression method}$
that estimates KV pairs importance by predicting how future queries will attend
to them. Our approach leverages the distributional properties of LLM
activations to compute expected attention scores in closed form for each KV
pair. These scores enable principled ranking and pruning of KV pairs with
minimal impact on the residual stream, achieving effective compression without
performance degradation. Importantly, our method operates seamlessly across
both prefilling and decoding phases, consistently outperforming
state-of-the-art baselines in both scenarios. Finally, $\textbf{we release
KVPress, a comprehensive library to enable researchers to implement and
benchmark KV cache compression methods, already including more than 20
techniques}$.

</details>


### [146] [Batch-CAM: Introduction to better reasoning in convolutional deep learning models](https://arxiv.org/abs/2510.00664)
*Giacomo Ignesti,Davide Moroni,Massimo Martinelli*

Main category: cs.AI

TL;DR: 本文提出Batch-CAM训练范式，结合批处理Grad-CAM和原型重建损失，旨在同时提升模型分类性能、图像重建质量，并减少训练和推理时间，从而构建更透明、可解释的AI系统。


<details>
  <summary>Details</summary>
Motivation: 理解深度学习模型的内部运作对人工智能发展至关重要，特别是在医疗等高风险领域，准确解释与模型精度同等重要。

Method: 引入Batch-CAM，一种新颖的训练范式，它将Grad-CAM算法的批处理实现与原型重建损失相结合，以引导模型聚焦于显著的图像特征。

Result: Batch-CAM在提高分类任务准确性和图像重建质量的同时，显著缩短了训练和推理时间。

Conclusion: 通过确保模型从与证据相关的信息中学习，该方法为构建更透明、可解释和值得信赖的AI系统做出了重要贡献。

Abstract: Understanding the inner workings of deep learning models is crucial for
advancing artificial intelligence, particularly in high-stakes fields such as
healthcare, where accurate explanations are as vital as precision. This paper
introduces Batch-CAM, a novel training paradigm that fuses a batch
implementation of the Grad-CAM algorithm with a prototypical reconstruction
loss. This combination guides the model to focus on salient image features,
thereby enhancing its performance across classification tasks. Our results
demonstrate that Batch-CAM achieves a simultaneous improvement in accuracy and
image reconstruction quality while reducing training and inference times. By
ensuring models learn from evidence-relevant information,this approach makes a
relevant contribution to building more transparent, explainable, and
trustworthy AI systems.

</details>


### [147] [Relevance-Zone Reduction in Game Solving](https://arxiv.org/abs/2510.00689)
*Chi-Huang Lin,Ting Han Wei,Chun-Jui Wang,Hung Guei,Chung-Chin Shih,Yun-Jui Tsai,I-Chen Wu,Ti-Rong Wu*

Main category: cs.AI

TL;DR: 针对游戏求解中 Relevance-Zone (RZ) 区域大小不一的问题，本文提出了一种迭代RZ缩减方法，有效减小了RZ区域，提高了求解效率并可复用知识。


<details>
  <summary>Details</summary>
Motivation: 游戏求解因棋盘树指数增长而面临巨大挑战。Relevance-Zone (RZ) 技术虽能缩小搜索空间，但其大小不唯一，且较小的RZ更有利于策略复用和剪枝效率。因此，需要一种方法来获得更小的RZ。

Method: 提出了一种迭代的RZ缩减方法，通过重复求解同一局面并逐渐限制相关区域，引导求解器找到更小的RZ。该方法设计了三种约束生成策略，并整合了RZ模式表以充分利用过往解决方案。

Result: 在7x7 Killall-Go的实验中，该方法将平均RZ大小缩减至原始的85.95%。

Conclusion: 该方法能够有效缩小RZ区域，提高了求解效率，并且缩减后的RZ可以作为可复用知识永久存储，用于未来的求解任务，尤其适用于更大的棋盘或不同的开局。

Abstract: Game solving aims to find the optimal strategies for all players and
determine the theoretical outcome of a game. However, due to the exponential
growth of game trees, many games remain unsolved, even though methods like
AlphaZero have demonstrated super-human level in game playing. The
Relevance-Zone (RZ) is a local strategy reuse technique that restricts the
search to only the regions relevant to the outcome, significantly reducing the
search space. However, RZs are not unique. Different solutions may result in
RZs of varying sizes. Smaller RZs are generally more favorable, as they
increase the chance of reuse and improve pruning efficiency. To this end, we
propose an iterative RZ reduction method that repeatedly solves the same
position while gradually restricting the region involved, guiding the solver
toward smaller RZs. We design three constraint generation strategies and
integrate an RZ Pattern Table to fully leverage past solutions. In experiments
on 7x7 Killall-Go, our method reduces the average RZ size to 85.95% of the
original. Furthermore, the reduced RZs can be permanently stored as reusable
knowledge for future solving tasks, especially for larger board sizes or
different openings.

</details>


### [148] [ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning](https://arxiv.org/abs/2510.00690)
*Yunhao Wang,Ziting Li,Shuai Chen,Tao Liu,Chao Song,Junjie Jiang,Jian Zhu,Peng Gao,Bin Qin*

Main category: cs.AI

TL;DR: ACPO通过动态课程和优势感知自适应裁剪机制，改进了大型视觉-语言模型（VLM）在复杂推理任务中的强化学习对齐，实现了最先进的性能、加速收敛和更优的训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的策略优化算法（如PPO）在对齐大型视觉-语言模型（VLMs）进行复杂推理时存在局限性，包括静态训练计划和僵硬、统一的裁剪机制，这阻碍了模型的性能。

Method: 本文提出了自适应课程策略优化（ACPO）框架，包含两个核心策略：1) 动态课程，逐步增加样本复用，实现从稳定的近策略探索到高效的离策略利用的过渡；2) 优势感知自适应裁剪（AAAC）机制，用由每个token的标准化优势调制的动态、逐样本边界取代固定的裁剪超参数，实现更细粒度和鲁棒的策略更新。

Result: ACPO在MathVista、LogicVista和MMMU-Pro等一系列挑战性多模态推理基准测试中，持续优于DAPO和PAPO等强基线模型，实现了最先进的性能、加速收敛和卓越的训练稳定性。

Conclusion: ACPO成功解决了现有策略优化算法的局限性，为复杂推理任务中的VLM对齐提供了更有效、更稳定的强化学习方法，并取得了显著的性能提升。

Abstract: Aligning large-scale vision-language models (VLMs) for complex reasoning via
reinforcement learning is often hampered by the limitations of existing policy
optimization algorithms, such as static training schedules and the rigid,
uniform clipping mechanism in Proximal Policy Optimization (PPO). In this work,
we introduce Adaptive Curriculum Policy Optimization (ACPO), a novel framework
that addresses these challenges through a dual-component adaptive learning
strategy. First, ACPO employs a dynamic curriculum that orchestrates a
principled transition from a stable, near on-policy exploration phase to an
efficient, off-policy exploitation phase by progressively increasing sample
reuse. Second, we propose an Advantage-Aware Adaptive Clipping (AAAC) mechanism
that replaces the fixed clipping hyperparameter with dynamic, sample-wise
bounds modulated by the normalized advantage of each token. This allows for
more granular and robust policy updates, enabling larger gradients for
high-potential samples while safeguarding against destructive ones. We conduct
extensive experiments on a suite of challenging multimodal reasoning
benchmarks, including MathVista, LogicVista, and MMMU-Pro. Results demonstrate
that ACPO consistently outperforms strong baselines such as DAPO and PAPO,
achieving state-of-the-art performance, accelerated convergence, and superior
training stability.

</details>


### [149] [AttentionDep: Domain-Aware Attention for Explainable Depression Severity Assessment](https://arxiv.org/abs/2510.00706)
*Yusif Ibrahimov,Tarique Anwar,Tommy Yuan,Turan Mutallimov,Elgun Hasanov*

Main category: cs.AI

TL;DR: 本文提出AttentionDep，一个领域感知的注意力模型，通过融合上下文和领域知识，从社交媒体数据中可解释地估计抑郁症严重程度，并优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 在互联社会中，社交媒体平台能反映个体的思想、情感和心理状态。利用这些平台（如Facebook、X、Reddit）进行抑郁症严重程度检测具有重要意义。

Method: 提出AttentionDep模型，它是一个领域感知的注意力模型。文章采用层级编码（unigrams和bigrams）对帖子进行编码，并使用注意力机制突出临床相关词汇。通过交叉注意力机制整合来自心理健康知识图谱的领域知识，以丰富上下文特征。最后，利用序数回归框架预测抑郁症严重程度，该框架尊重临床相关性和严重程度的自然顺序。

Result: 实验表明，AttentionDep在多个数据集上的分级F1分数上，比现有最先进的基线模型高出5%以上，同时提供了可解释的预测洞察。

Conclusion: 这项工作推动了从社交媒体进行心理健康评估的可信和透明AI系统的发展。

Abstract: In today's interconnected society, social media platforms provide a window
into individuals' thoughts, emotions, and mental states. This paper explores
the use of platforms like Facebook, X (formerly Twitter), and Reddit for
depression severity detection. We propose AttentionDep, a domain-aware
attention model that drives explainable depression severity estimation by
fusing contextual and domain knowledge. Posts are encoded hierarchically using
unigrams and bigrams, with attention mechanisms highlighting clinically
relevant tokens. Domain knowledge from a curated mental health knowledge graph
is incorporated through a cross-attention mechanism, enriching the contextual
features. Finally, depression severity is predicted using an ordinal regression
framework that respects the clinical-relevance and natural ordering of severity
levels. Our experiments demonstrate that AttentionDep outperforms
state-of-the-art baselines by over 5% in graded F1 score across datasets, while
providing interpretable insights into its predictions. This work advances the
development of trustworthy and transparent AI systems for mental health
assessment from social media.

</details>


### [150] [EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty](https://arxiv.org/abs/2510.00732)
*Yuchen Tian,Ruiyuan Huang,Xuanwu Wang,Jing Ma,Zengfeng Huang,Ziyang Luo,Hongzhan Lin,Da Zheng,Lun Du*

Main category: cs.AI

TL;DR: 本文提出一个新颖的数据增强流程，通过对称性和难度两个维度提升大型语言模型在形式化定理证明中的泛化性和鲁棒性。基于此流程训练的EvolProver（一个7B参数的非推理定理证明器）在多个基准测试上创造了新的SOTA，超越了同等规模甚至一些基于推理的模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在形式化定理证明中潜力巨大，但存在泛化能力不足且对问题陈述的微小改动过于脆弱的问题。

Method: 引入了一个新颖的数据增强流程，从对称性和难度两方面增强模型鲁棒性。对称性方面，提出了基于抽象语法树的EvolAST（处理句法对称性）和利用LLMs跨数学领域翻译定理的EvolDomain（处理语义对称性）。难度方面，提出了EvolDifficulty，利用精心设计的演化指令指导LLMs生成不同难度的定理。随后，使用这些增强数据训练了一个7B参数的非推理定理证明器EvolProver。

Result: EvolProver在FormalMATH-Lite上以53.8%的pass@32率创造了新的SOTA，超越了所有同等规模的模型（包括基于推理的模型）。它还在MiniF2F-Test (69.8% pass@32)、Ineq-Comp-Seed (52.2% pass@32) 和 Ineq-Comp-Transformed (34.0% pass@32) 上为非推理模型设定了新的SOTA记录。消融研究进一步证实了数据增强流程在多个基准测试上的有效性。

Conclusion: 所提出的数据增强流程显著提升了大型语言模型在形式化定理证明中的鲁棒性和性能，并训练出一个非推理模型EvolProver，在多项基准测试上达到了新的最先进水平。

Abstract: Large Language Models (LLMs) for formal theorem proving have shown
significant promise, yet they often lack generalizability and are fragile to
even minor transformations of problem statements. To address this limitation,
we introduce a novel data augmentation pipeline designed to enhance model
robustness from two perspectives: symmetry and difficulty. From the symmetry
perspective, we propose two complementary methods: EvolAST, an Abstract Syntax
Tree (AST) based approach that targets syntactic symmetry to generate
semantically equivalent problem variants, and EvolDomain, which leverages LLMs
to address semantic symmetry by translating theorems across mathematical
domains. From the difficulty perspective, we propose EvolDifficulty, which uses
carefully designed evolutionary instructions to guide LLMs in generating new
theorems with a wider range of difficulty. We then use the evolved data to
train EvolProver, a 7B-parameter non-reasoning theorem prover. EvolProver
establishes a new state-of-the-art (SOTA) on FormalMATH-Lite with a 53.8%
pass@32 rate, surpassing all models of comparable size, including
reasoning-based models. It also sets new SOTA records for non-reasoning models
on MiniF2F-Test (69.8% pass@32), Ineq-Comp-Seed (52.2% pass@32), and
Ineq-Comp-Transformed (34.0% pass@32). Ablation studies further confirm our
data augmentation pipeline's effectiveness across multiple benchmarks.

</details>


### [151] [DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models](https://arxiv.org/abs/2510.00778)
*Seunghoo Hong,Geonho Son,Juhun Lee,Simon S. Woo*

Main category: cs.AI

TL;DR: DDIM反演虽便于图像编辑，但也被恶意用于生成虚假内容。现有防御方法因目标不匹配而效果不佳。本文提出DDIM反演攻击（DIA），通过攻击DDIM轨迹路径，实现比现有方法更强的有效扰乱和防御。


<details>
  <summary>Details</summary>
Motivation: DDIM反演技术虽可用于真实图像编辑，却被恶意用户利用来合成虚假信息和深度伪造内容，引发伦理、隐私及版权问题。现有防御算法（如AdvDM和Photoguard）由于其目标与测试时的迭代去噪轨迹存在偏差，导致防御性能不足。

Method: 本文提出了DDIM反演攻击（DDIM Inversion Attack, DIA），旨在攻击集成DDIM轨迹路径。

Result: 研究结果表明，DIA能有效扰乱图像编辑过程，在各种编辑方法中均超越了以往的防御方法。

Conclusion: 本工作提出的框架和结果可为工业界和研究社区提供针对恶意使用AI的实用防御方法。

Abstract: Diffusion models have shown to be strong representation learners, showcasing
state-of-the-art performance across multiple domains. Aside from accelerated
sampling, DDIM also enables the inversion of real images back to their latent
codes. A direct inheriting application of this inversion operation is real
image editing, where the inversion yields latent trajectories to be utilized
during the synthesis of the edited image. Unfortunately, this practical tool
has enabled malicious users to freely synthesize misinformative or deepfake
contents with greater ease, which promotes the spread of unethical and abusive,
as well as privacy-, and copyright-infringing contents. While defensive
algorithms such as AdvDM and Photoguard have been shown to disrupt the
diffusion process on these images, the misalignment between their objectives
and the iterative denoising trajectory at test time results in weak disruptive
performance.In this work, we present the DDIM Inversion Attack (DIA) that
attacks the integrated DDIM trajectory path. Our results support the effective
disruption, surpassing previous defensive methods across various editing
methods. We believe that our frameworks and results can provide practical
defense methods against the malicious use of AI for both the industry and the
research community. Our code is available here:
https://anonymous.4open.science/r/DIA-13419/.

</details>


### [152] [AI in data science education: experiences from the classroom](https://arxiv.org/abs/2510.00793)
*J. A. Hageman,C. F. W. Peeters*

Main category: cs.AI

TL;DR: 本研究通过访谈课程协调员，探讨了AI（尤其是LLMs如ChatGPT）融入教育的利弊，发现AI虽能提升学习效率，但也存在学生过度依赖的风险，强调需负责任地使用并调整评估方法。


<details>
  <summary>Details</summary>
Motivation: 探索AI（特别是大型语言模型如ChatGPT）在教育环境中的整合，及其对教学和学习的潜在影响。

Method: 通过访谈瓦赫宁根大学数据科学课程的课程协调员，识别AI在课堂中的益处与挑战。

Result: AI工具可以简化任务并增强学习，但也引发了对学生过度依赖技术、可能阻碍认知和解决问题能力发展的担忧。研究强调了负责任的AI使用、伦理考量以及调整评估方法的重要性。

Conclusion: 在教育中，若能谨慎整合AI，使其作为基础学习过程的补充而非替代，AI将成为宝贵资产。

Abstract: This study explores the integration of AI, particularly large language models
(LLMs) like ChatGPT, into educational settings, focusing on the implications
for teaching and learning. Through interviews with course coordinators from
data science courses at Wageningen University, this research identifies both
the benefits and challenges associated with AI in the classroom. While AI tools
can streamline tasks and enhance learning, concerns arise regarding students'
overreliance on these technologies, potentially hindering the development of
essential cognitive and problem solving skills. The study highlights the
importance of responsible AI usage, ethical considerations, and the need for
adapting assessment methods to ensure educational outcomes are met. With
careful integration, AI can be a valuable asset in education, provided it is
used to complement rather than replace fundamental learning processes.

</details>


### [153] [Benchmarking Agentic Systems in Automated Scientific Information Extraction with ChemX](https://arxiv.org/abs/2510.00795)
*Anastasia Vepreva,Julia Razlivina,Maria Eremeeva,Nina Gubina,Anastasia Orlova,Aleksei Dmitrenko,Ksenya Kapranova,Susan Jyakhwo,Nikita Vasilev,Arsen Sarkisyan,Ivan Yu. Chernyshov,Vladimir Vinogradov,Andrei Dmitrenko*

Main category: cs.AI

TL;DR: 本文提出了ChemX数据集，一个包含10个专家验证的化学信息提取基准，并用它评估了代理系统和先进大模型，揭示了该领域在处理复杂化学数据时的持续挑战。


<details>
  <summary>Details</summary>
Motivation: 化学信息提取由于数据的高度异质性而面临巨大挑战，现有的通用和特定领域代理系统在该领域表现有限。

Method: 引入了ChemX，一个包含10个手动整理和领域专家验证的纳米材料和小分子数据集。研究对比了ChatGPT Agent等现有最先进代理系统、化学专用数据提取代理以及作者提出的单代理方法。同时评估了GPT-5和GPT-5 Thinking等现代基线模型的性能。

Result: 实证结果表明，化学信息提取仍存在挑战，特别是在处理领域特定术语、复杂表格和示意图表示以及上下文相关歧义方面。

Conclusion: ChemX基准是推动化学自动化信息提取的关键资源，它有助于挑战现有方法的泛化能力，并为有效的评估策略提供了宝贵见解。

Abstract: The emergence of agent-based systems represents a significant advancement in
artificial intelligence, with growing applications in automated data
extraction. However, chemical information extraction remains a formidable
challenge due to the inherent heterogeneity of chemical data. Current
agent-based approaches, both general-purpose and domain-specific, exhibit
limited performance in this domain. To address this gap, we present ChemX, a
comprehensive collection of 10 manually curated and domain-expert-validated
datasets focusing on nanomaterials and small molecules. These datasets are
designed to rigorously evaluate and enhance automated extraction methodologies
in chemistry. To demonstrate their utility, we conduct an extensive
benchmarking study comparing existing state-of-the-art agentic systems such as
ChatGPT Agent and chemical-specific data extraction agents. Additionally, we
introduce our own single-agent approach that enables precise control over
document preprocessing prior to extraction. We further evaluate the performance
of modern baselines, such as GPT-5 and GPT-5 Thinking, to compare their
capabilities with agentic approaches. Our empirical findings reveal persistent
challenges in chemical information extraction, particularly in processing
domain-specific terminology, complex tabular and schematic representations, and
context-dependent ambiguities. The ChemX benchmark serves as a critical
resource for advancing automated information extraction in chemistry,
challenging the generalization capabilities of existing methods, and providing
valuable insights into effective evaluation strategies.

</details>


### [154] [Semantic Bridges Between First Order c-Representations and Cost-Based Semantics: An Initial Perspective](https://arxiv.org/abs/2510.00817)
*Nicholas Leisegang,Giovanni Casini,Thomas Meyer*

Main category: cs.AI

TL;DR: 本文比较了两种处理不一致知识或可废止规则的形式主义：加权知识库（WKB-CBS）和c-表示。研究发现，在特定条件下，两者在解释排序和蕴涵表达上具有语义等价性。


<details>
  <summary>Details</summary>
Motivation: 为了处理不一致知识库（通过加权知识库和基于成本的语义）和可废止概念包含（通过c-表示），存在两种不同的形式主义，它们都通过对解释赋予成本或惩罚来解决问题。本文旨在从语义层面比较这两种方法，理解它们之间的关联和潜在等价性。

Method: 采用语义层面的比较方法。具体地，分析加权知识库与基于成本的语义如何为DL解释赋予成本，以及c-表示如何通过违背条件的惩罚为解释分配数值排名。同时，比较两种形式主义中描述的蕴涵关系。

Result: ['在特定条件下，加权知识库和可废止条件集能够产生相同的解释排序，从而在相对成本上实现语义结构的等价性。', '在两种形式主义中，某些蕴涵概念可以等价地表达。']

Conclusion: 本研究结果揭示了成本语义和c-表示之间的语义联系，为这两个领域未来的研究工作提供了潜在的益处。

Abstract: Weighted-knowledge bases and cost-based semantics represent a recent
formalism introduced by Bienvenu et al. for Ontology Mediated Data Querying in
the case where a given knowledge base is inconsistent. This is done by adding a
weight to each statement in the knowledge base (KB), and then giving each DL
interpretation a cost based on how often it breaks rules in the KB. In this
paper we compare this approach with c-representations, a form of non-monotonic
reasoning originally introduced by Kern-Isberner. c-Representations describe a
means to interpret defeasible concept inclusions in the first-order case. This
is done by assigning a numerical ranking to each interpretations via penalties
for each violated conditional. We compare these two approaches on a semantic
level. In particular, we show that under certain conditions a weighted
knowledge base and a set of defeasible conditionals can generate the same
ordering on interpretations, and therefore an equivalence of semantic
structures up to relative cost. Moreover, we compare entailment described in
both cases, where certain notions are equivalently expressible in both
formalisms. Our results have the potential to benefit further work on both
cost-based semantics and c-representations

</details>


### [155] [Logical Consistency Between Disagreeing Experts and Its Role in AI Safety](https://arxiv.org/abs/2510.00821)
*Andrés Corrada-Emmanuel*

Main category: cs.AI

TL;DR: 本文提出一种基于逻辑一致性的无监督评估方法，通过分析专家（或分类器）之间决策的同意与分歧来推断其表现，并将其应用于构建检测LLM法官评分阈值违规的警报系统。


<details>
  <summary>Details</summary>
Motivation: 当两个专家意见相左时，至少有一方并非完全正确；但如果他们完全一致，则任何评估结果都无法排除。这种协议与分歧在评估效用上的不对称性，促使研究者探索一种形式化的无监督分类器评估逻辑。

Method: 该方法通过形式化一种无监督评估逻辑，核心是计算与观察到的决策（同意或分歧）逻辑一致的群体评估集。它将对齐决策的统计摘要作为输入，转化为一个整数空间的线性规划问题，包含逻辑约束（不等式）和适用于所有有限测试的普遍公理（等式）。

Result: 该方法在实际应用中展示了其立即效用，通过构建“无知识警报”来检测一个或多个LLM法官何时违反用户指定的最低评分阈值。

Conclusion: 该研究提供了一种利用逻辑一致性进行无监督评估的新颖方法，其核心在于理解和利用专家（或分类器）间协议与分歧的不对称性，并已成功应用于LLM评估场景。

Abstract: If two experts disagree on a test, we may conclude both cannot be 100 per
cent correct. But if they completely agree, no possible evaluation can be
excluded. This asymmetry in the utility of agreements versus disagreements is
explored here by formalizing a logic of unsupervised evaluation for
classifiers. Its core problem is computing the set of group evaluations that
are logically consistent with how we observe them agreeing and disagreeing in
their decisions. Statistical summaries of their aligned decisions are inputs
into a Linear Programming problem in the integer space of possible correct or
incorrect responses given true labels. Obvious logical constraints, such as,
the number of correct responses cannot exceed the number of observed responses,
are inequalities. But in addition, there are axioms, universally applicable
linear equalities that apply to all finite tests. The practical and immediate
utility of this approach to unsupervised evaluation using only logical
consistency is demonstrated by building no-knowledge alarms that can detect
when one or more LLMs-as-Judges are violating a minimum grading threshold
specified by the user.

</details>


### [156] [Benchmarking Machine Learning Models for Fault Classification and Localization in Power System Protection](https://arxiv.org/abs/2510.00831)
*Julian Oelhaf,Georg Kordowich,Changhun Kim,Paula Andrea Pérez-Toro,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The increasing integration of distributed energy resources (DERs),
particularly renewables, poses significant challenges for power system
protection, with fault classification (FC) and fault localization (FL) being
among the most critical tasks. Conventional protection schemes, based on fixed
thresholds, cannot reliably identify and localize short circuits with the
increasing complexity of the grid under dynamic conditions. Machine learning
(ML) offers a promising alternative; however, systematic benchmarks across
models and settings remain limited. This work presents, for the first time, a
comparative benchmarking study of classical ML models for FC and FL in power
system protection based on EMT data. Using voltage and current waveforms
segmented into sliding windows of 10 ms to 50 ms, we evaluate models under
realistic real-time constraints. Performance is assessed in terms of accuracy,
robustness to window size, and runtime efficiency. The best-performing FC model
achieved an F1 score of 0.992$\pm$0.001, while the top FL model reached an R2
of 0.806$\pm$0.008 with a mean processing time of 0.563 ms.

</details>


### [157] [Improving Cryptocurrency Pump-and-Dump Detection through Ensemble-Based Models and Synthetic Oversampling Techniques](https://arxiv.org/abs/2510.00836)
*Jieun Yu,Minjung Park,Sangmi Chai*

Main category: cs.AI

TL;DR: 本研究通过结合SMOTE和集成学习模型，有效解决了加密货币市场P&D操纵检测中的类别不平衡问题，并实现了高召回率和快速计算性能。


<details>
  <summary>Details</summary>
Motivation: 加密货币市场中的“拉高出货”（P&D）操纵事件稀缺，导致严重的类别不平衡，从而影响准确检测。

Method: 采用合成少数过采样技术（SMOTE）来处理数据不平衡问题，并评估了先进的集成学习模型（如XGBoost和LightGBM）以区分操纵性交易行为。

Result: SMOTE极大地增强了所有模型检测P&D事件的能力，提高了召回率并改善了精确率与召回率的整体平衡。特别是XGBoost和LightGBM表现出色，达到了94.87%和93.59%的高召回率，F1分数强劲，并具备快速计算性能。

Conclusion: 结合数据平衡技术与集成学习方法能显著提升操纵活动的早期检测能力，有助于构建更公平、透明和稳定的加密货币市场。

Abstract: This study aims to detect pump and dump (P&D) manipulation in cryptocurrency
markets, where the scarcity of such events causes severe class imbalance and
hinders accurate detection. To address this issue, the Synthetic Minority
Oversampling Technique (SMOTE) was applied, and advanced ensemble learning
models were evaluated to distinguish manipulative trading behavior from normal
market activity. The experimental results show that applying SMOTE greatly
enhanced the ability of all models to detect P&D events by increasing recall
and improving the overall balance between precision and recall. In particular,
XGBoost and LightGBM achieved high recall rates (94.87% and 93.59%,
respectively) with strong F1-scores and demonstrated fast computational
performance, making them suitable for near real time surveillance. These
findings indicate that integrating data balancing techniques with ensemble
methods significantly improves the early detection of manipulative activities,
contributing to a fairer, more transparent, and more stable cryptocurrency
market.

</details>


### [158] [Learning Compact Representations of LLM Abilities via Item Response Theory](https://arxiv.org/abs/2510.00844)
*Jianhao Chen,Chenxu Wang,Gengrui Zhang,Peng Ye,Lei Bai,Wei Hu,Yuzhong Qu,Shuyue Hu*

Main category: cs.AI

TL;DR: 该研究受IRT启发，提出一种基于MoE网络的方法，通过学习模型能力、查询区分度和难度参数，为LLMs构建紧凑能力表示，以优化模型路由和性能预测。


<details>
  <summary>Details</summary>
Motivation: 面对大量涌现的大型语言模型（LLMs），如何高效管理和利用这些资源是一个巨大挑战。

Method: 将问题建模为估计模型正确回答查询的概率，并受心理测量学中的项目反应理论（IRT）启发，用模型多技能能力向量、查询区分度向量和查询难度标量来建模此概率。引入一个专家混合（MoE）网络，结合模型和查询嵌入来联合学习这些参数。

Result: 在模型路由和基准准确率预测两方面均达到了最先进的性能。学习到的参数编码了有意义且可解释的模型能力和查询特征信息。

Conclusion: 该方法成功地为LLMs学习了紧凑且可解释的能力表示，显著提升了模型管理和利用的效率，尤其在模型路由和性能预测任务上表现出色。

Abstract: Recent years have witnessed a surge in the number of large language models
(LLMs), yet efficiently managing and utilizing these vast resources remains a
significant challenge. In this work, we explore how to learn compact
representations of LLM abilities that can facilitate downstream tasks, such as
model routing and performance prediction on new benchmarks. We frame this
problem as estimating the probability that a given model will correctly answer
a specific query. Inspired by the item response theory (IRT) in psychometrics,
we model this probability as a function of three key factors: (i) the model's
multi-skill ability vector, (2) the query's discrimination vector that
separates models of differing skills, and (3) the query's difficulty scalar. To
learn these parameters jointly, we introduce a Mixture-of-Experts (MoE) network
that couples model- and query-level embeddings. Extensive experiments
demonstrate that our approach leads to state-of-the-art performance in both
model routing and benchmark accuracy prediction. Moreover, analysis validates
that the learned parameters encode meaningful, interpretable information about
model capabilities and query characteristics.

</details>


### [159] [Unveiling Interesting Insights: Monte Carlo Tree Search for Knowledge Discovery](https://arxiv.org/abs/2510.00876)
*Pietro Totis,Alberto Pozanco,Daniel Borrajo*

Main category: cs.AI

TL;DR: 本文提出了一种基于蒙特卡洛树搜索（MCTS）的新型自动化洞察和数据探索方法AIDE，有效解决了数据到知识转换的挑战。


<details>
  <summary>Details</summary>
Motivation: 组织在将海量过程数据转化为可操作知识和决策方面面临困难，存在数据量与处理能力之间的鸿沟。自动化知识发现是复杂的开放问题。

Method: 引入了一种名为AIDE（自动化洞察和数据探索）的新方法，该方法利用蒙特卡洛树搜索（MCTS）作为核心框架来解决自动化知识发现的挑战。

Result: 通过真实世界和合成数据评估，AIDE被证明能有效识别数据转换和模型，从而揭示有趣的数据模式。其基于MCTS的框架具有显著的可扩展性。

Conclusion: AIDE为开发全面的自动化知识发现解决方案迈出了重要一步，并可扩展集成更多模式提取策略和领域知识。

Abstract: Organizations are increasingly focused on leveraging data from their
processes to gain insights and drive decision-making. However, converting this
data into actionable knowledge remains a difficult and time-consuming task.
There is often a gap between the volume of data collected and the ability to
process and understand it, which automated knowledge discovery aims to fill.
Automated knowledge discovery involves complex open problems, including
effectively navigating data, building models to extract implicit relationships,
and considering subjective goals and knowledge. In this paper, we introduce a
novel method for Automated Insights and Data Exploration (AIDE), that serves as
a robust foundation for tackling these challenges through the use of Monte
Carlo Tree Search (MCTS). We evaluate AIDE using both real-world and synthetic
data, demonstrating its effectiveness in identifying data transformations and
models that uncover interesting data patterns. Among its strengths, AIDE's
MCTS-based framework offers significant extensibility, allowing for future
integration of additional pattern extraction strategies and domain knowledge.
This makes AIDE a valuable step towards developing a comprehensive solution for
automated knowledge discovery.

</details>


### [160] [FusionAdapter for Few-Shot Relation Learning in Multimodal Knowledge Graphs](https://arxiv.org/abs/2510.00894)
*Ran Liu,Yuan Fang,Xiaoli Li*

Main category: cs.AI

TL;DR: 提出FusionAdapter，通过适配器和融合策略处理多模态知识图谱中的少样本关系学习问题，其性能优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态知识图谱(MMKG)方法主要将不同模态对齐到共享空间，往往忽略特定模态的独特贡献，导致其性能（尤其在资源匮乏设置下）受到限制。

Method: 提出FusionAdapter模型，用于多模态知识图谱中的少样本关系学习。该模型引入：1) 一个适配器模块，使各模态能高效适应未见关系；2) 一种融合策略，整合多模态实体表示，同时保留多样的模态特异性。

Result: 在两个基准MMKG数据集上进行的大量实验表明，FusionAdapter的性能优于现有最先进的方法。

Conclusion: FusionAdapter通过有效地适应和融合来自不同模态的信息，提升了对新关系的泛化能力，且只需最少的监督，成功解决了多模态知识图谱中少样本关系学习的挑战。

Abstract: Multimodal Knowledge Graphs (MMKGs) incorporate various modalities, including
text and images, to enhance entity and relation representations. Notably,
different modalities for the same entity often present complementary and
diverse information. However, existing MMKG methods primarily align modalities
into a shared space, which tends to overlook the distinct contributions of
specific modalities, limiting their performance particularly in low-resource
settings. To address this challenge, we propose FusionAdapter for the learning
of few-shot relationships (FSRL) in MMKG. FusionAdapter introduces (1) an
adapter module that enables efficient adaptation of each modality to unseen
relations and (2) a fusion strategy that integrates multimodal entity
representations while preserving diverse modality-specific characteristics. By
effectively adapting and fusing information from diverse modalities,
FusionAdapter improves generalization to novel relations with minimal
supervision. Extensive experiments on two benchmark MMKG datasets demonstrate
that FusionAdapter achieves superior performance over state-of-the-art methods.

</details>


### [161] [On Discovering Algorithms for Adversarial Imitation Learning](https://arxiv.org/abs/2510.00922)
*Shashank Reddy Chirra,Jayden Teoh,Praveen Paruchuri,Pradeep Varakantham*

Main category: cs.AI

TL;DR: 对抗式模仿学习（AIL）常不稳定，主要因奖励分配（RA）函数设计依赖人工。本研究利用LLM引导的进化框架发现数据驱动的RA函数，提出DAIL算法。DAIL是首个元学习AIL算法，它能泛化到新环境，超越现有SOTA，并提高训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗式模仿学习（AIL）方法存在训练不稳定的问题，尽管它们在有限专家演示下有效。AIL通常分解为密度比估计和奖励分配（RA）两部分，但多数研究侧重前者，而RA函数对训练动态和最终性能的影响被忽视。当前的RA函数主要依赖人工设计和经验，缺乏数据驱动的发现机制。

Method: 本研究提出一种新方法，旨在发现数据驱动的奖励分配（RA）函数，即基于模仿策略实际性能来构建RA函数。为此，我们利用一个LLM引导的进化框架，高效探索RA函数空间，从而得到了“发现式对抗模仿学习”（DAIL）算法。DAIL是首个元学习的AIL算法。

Result: DAIL算法在未见过的环境和策略优化算法中表现出卓越的泛化能力，并显著超越了当前最先进的（人工设计的）基线方法。此外，研究还分析了DAIL带来更稳定训练的原因，为RA函数在AIL稳定性中的作用提供了新颖见解。

Conclusion: 本研究通过LLM引导的进化框架成功发现了数据驱动的奖励分配函数，解决了对抗式模仿学习的稳定性问题，并提出了首个元学习AIL算法DAIL。DAIL不仅在性能和泛化性上超越了现有的人工设计方法，而且为理解RA函数在AIL稳定性中的关键作用提供了新的视角。

Abstract: Adversarial Imitation Learning (AIL) methods, while effective in settings
with limited expert demonstrations, are often considered unstable. These
approaches typically decompose into two components: Density Ratio (DR)
estimation $\frac{\rho_E}{\rho_{\pi}}$, where a discriminator estimates the
relative occupancy of state-action pairs under the policy versus the expert;
and Reward Assignment (RA), where this ratio is transformed into a reward
signal used to train the policy. While significant research has focused on
improving density estimation, the role of reward assignment in influencing
training dynamics and final policy performance has been largely overlooked. RA
functions in AIL are typically derived from divergence minimization objectives,
relying heavily on human design and ingenuity. In this work, we take a
different approach: we investigate the discovery of data-driven RA functions,
i.e, based directly on the performance of the resulting imitation policy. To
this end, we leverage an LLM-guided evolutionary framework that efficiently
explores the space of RA functions, yielding \emph{Discovered Adversarial
Imitation Learning} (DAIL), the first meta-learnt AIL algorithm. Remarkably,
DAIL generalises across unseen environments and policy optimization algorithms,
outperforming the current state-of-the-art of \emph{human-designed} baselines.
Finally, we analyse why DAIL leads to more stable training, offering novel
insights into the role of RA functions in the stability of AIL. Code is
publicly available: https://github.com/shshnkreddy/DAIL.

</details>


### [162] [Test-Time Search in Neural Graph Coarsening Procedures for the Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2510.00958)
*Yoonju Sim,Hyeonah Kim,Changhyun Kwon*

Main category: cs.AI

TL;DR: 本文提出了一种新的测试时随机搜索和GraphCHiP算法，用于增强CVRP中基于深度学习的割平面方法，以生成更多样化和有效的割平面（包括首次发现的FCIs），从而显著缩小对偶间隙。


<details>
  <summary>Details</summary>
Motivation: 容量车辆路径问题（CVRP）的割平面方法中，识别有效不等式（如RCIs）至关重要。现有基于深度学习的分离方法虽然能找到高质量的割平面，但由于对生成多样化子集不够敏感，其生成的割平面数量低于预期，影响了性能。

Method: 通过引入新的带有随机性的测试时搜索来增强已训练模型的推理性能。具体包括：1. 在图粗化过程中引入随机边选择，取代了之前的贪婪方法。2. 提出了基于图粗化历史的分区（GraphCHiP）算法，该算法利用粗化历史不仅识别RCIs，还首次识别了FCIs（Framed capacity inequalities）。

Result: 在随机生成的CVRP实例上进行的实验表明，与现有神经分离方法相比，该方法在缩小对偶间隙方面表现出有效性。此外，尽管识别这类割平面具有挑战性，但该方法在特定实例上发现了有效的FCIs。

Conclusion: 所提出的带有随机性的测试时搜索和GraphCHiP算法，有效解决了现有深度学习分离方法生成割平面多样性不足的问题，不仅提高了对偶间隙的缩减效果，还首次成功识别了FCIs，提升了CVRP割平面方法的性能。

Abstract: The identification of valid inequalities, such as the rounded capacity
inequalities (RCIs), is a key component of cutting plane methods for the
Capacitated Vehicle Routing Problem (CVRP). While a deep learning-based
separation method can learn to find high-quality cuts, our analysis reveals
that the model produces fewer cuts than expected because it is insufficiently
sensitive to generate a diverse set of generated subsets. This paper proposes
an alternative: enhancing the performance of a trained model at inference time
through a new test-time search with stochasticity. First, we introduce
stochastic edge selection into the graph coarsening procedure, replacing the
previously proposed greedy approach. Second, we propose the Graph Coarsening
History-based Partitioning (GraphCHiP) algorithm, which leverages coarsening
history to identify not only RCIs but also, for the first time, the Framed
capacity inequalities (FCIs). Experiments on randomly generated CVRP instances
demonstrate the effectiveness of our approach in reducing the dual gap compared
to the existing neural separation method. Additionally, our method discovers
effective FCIs on a specific instance, despite the challenging nature of
identifying such cuts.

</details>


### [163] [A Neuro-Fuzzy System for Interpretable Long-Term Stock Market Forecasting](https://arxiv.org/abs/2510.00960)
*Miha Ožbot,Igor Škrjanc,Vitomir Štruc*

Main category: cs.AI

TL;DR: 本文提出Fuzzformer，一个结合RNN、自注意力与模糊推理的多元时间序列预测模型，旨在提高股票市场预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在多元时间序列预测中，同时实现高准确性和可解释性是一个重大挑战。

Method: 引入Fuzzy Transformer (Fuzzformer)，它是一种结合了循环神经网络（LSTM）、多头自注意力机制和模糊推理系统的新型架构。该方法利用LSTM和时间注意力机制将多元数据提炼成可解释特征，供模糊推理系统使用。

Result: Fuzzformer在预测性能上与ARIMA和LSTM等传统模型相当，并能提供网络内部有意义的信息流。在S&P500股票指数上的初步实验显示其在可解释预测方面的潜力，并识别了当前的性能权衡。

Conclusion: Fuzzformer在理解和预测股票市场行为方面具有实际应用潜力，尤其在可解释预测方面有前景，但仍需权衡性能。

Abstract: In the complex landscape of multivariate time series forecasting, achieving
both accuracy and interpretability remains a significant challenge. This paper
introduces the Fuzzy Transformer (Fuzzformer), a novel recurrent neural network
architecture combined with multi-head self-attention and fuzzy inference
systems to analyze multivariate stock market data and conduct long-term time
series forecasting. The method leverages LSTM networks and temporal attention
to condense multivariate data into interpretable features suitable for fuzzy
inference systems. The resulting architecture offers comparable forecasting
performance to conventional models such as ARIMA and LSTM while providing
meaningful information flow within the network. The method was examined on the
real world stock market index S\&P500. Initial results show potential for
interpretable forecasting and identify current performance tradeoffs,
suggesting practical application in understanding and forecasting stock market
behavior.

</details>


### [164] [QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via Agentic RL](https://arxiv.org/abs/2510.00967)
*Cong Yu,Valter Uotila,Shilong Deng,Qingyuan Wu,Tuo Shi,Songlin Jiang,Lei You,Bo Zhao*

Main category: cs.AI

TL;DR: QUASAR是一个基于工具增强LLM的智能体强化学习框架，用于生成和优化量子电路，通过量子模拟器验证和分层奖励机制，显著提高了LLM生成量子电路的准确性和有效性，超越了现有大型模型。


<details>
  <summary>Details</summary>
Motivation: 量子计算的优势发挥依赖于特定任务量子电路的设计和优化。现有基于LLM的量子电路生成面临两大挑战：1) 参数化量子门需要精确数值，且其最优值依赖于电路结构；2) LLM缺乏量子领域知识，常生成低质量或不正确的电路。

Method: 本文提出了QUASAR，一个基于工具增强LLM的智能体强化学习（RL）框架，用于量子电路的生成和优化。为将LLM与量子特定知识对齐并改进电路生成，QUASAR设计了：1) 基于外部量子模拟器的量子电路验证方法；2) 在RL训练中采用复杂的分层奖励机制。

Result: 广泛的评估显示，QUASAR显著提高了所生成量子电路的语法和语义性能。当增强一个4B LLM时，QUASAR在Pass@1上取得了99.31%的有效性，在Pass@10上达到了100%的有效性，表现优于GPT-4o、GPT-5、DeepSeek-V3等工业级LLM以及多个仅SFT和仅RL的基线模型。

Conclusion: QUASAR通过结合量子电路验证和复杂的分层强化学习奖励机制，成功克服了LLM在量子电路生成中的固有挑战，大幅提升了生成电路的有效性和准确性，为自动化量子电路设计提供了高效的解决方案。

Abstract: Designing and optimizing task-specific quantum circuits are crucial to
leverage the advantage of quantum computing. Recent large language model
(LLM)-based quantum circuit generation has emerged as a promising automatic
solution. However, the fundamental challenges remain unaddressed: (i)
parameterized quantum gates require precise numerical values for optimal
performance, which also depend on multiple aspects, including the number of
quantum gates, their parameters, and the layout/depth of the circuits. (ii)
LLMs often generate low-quality or incorrect quantum circuits due to the lack
of quantum domain-specific knowledge. We propose QUASAR, an agentic
reinforcement learning (RL) framework for quantum circuits generation and
optimization based on tool-augmented LLMs. To align the LLM with
quantum-specific knowledge and improve the generated quantum circuits, QUASAR
designs (i) a quantum circuit verification approach with external quantum
simulators and (ii) a sophisticated hierarchical reward mechanism in RL
training. Extensive evaluation shows improvements in both syntax and semantic
performance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR
has achieved the validity of 99.31% in Pass@1 and 100% in Pass@10,
outperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several
supervised-fine-tuning (SFT)-only and RL-only baselines.

</details>


### [165] [Adaptive Federated Few-Shot Rare-Disease Diagnosis with Energy-Aware Secure Aggregation](https://arxiv.org/abs/2510.00976)
*Aueaphum Aueawatthanaphisut*

Main category: cs.AI

TL;DR: 本文提出了AFFR框架，一个自适应联邦少样本稀有疾病诊断方案，它结合了元学习、能源感知调度和安全聚合，旨在解决数据稀缺、隐私和资源限制问题，显著提高了诊断准确性并减少了客户端掉线。


<details>
  <summary>Details</summary>
Motivation: 稀有疾病诊断是数字健康领域的重大挑战，面临极端数据稀缺、隐私担忧和边缘设备资源有限等障碍。

Method: 本文提出了自适应联邦少样本稀有疾病诊断（AFFR）框架，它整合了三个核心支柱：(i) 基于元学习的少样本联邦优化以从有限患者样本中泛化，(ii) 能源感知客户端调度以减少设备掉线和确保平衡参与，以及 (iii) 采用校准差分隐私的安全聚合以保护敏感模型更新。

Result: 在模拟稀有疾病检测数据集上的实验评估显示，AFFR相较于基线联邦学习，准确率提高了高达10%，同时在不降低收敛性的前提下，客户端掉线率降低了50%以上。此外，隐私-效用权衡保持在临床可接受范围内。

Conclusion: 研究结果表明，AFFR为稀有病症的公平且值得信赖的联邦诊断提供了一条实用途径。

Abstract: Rare-disease diagnosis remains one of the most pressing challenges in digital
health, hindered by extreme data scarcity, privacy concerns, and the limited
resources of edge devices. This paper proposes the Adaptive Federated Few-Shot
Rare-Disease Diagnosis (AFFR) framework, which integrates three pillars: (i)
few-shot federated optimization with meta-learning to generalize from limited
patient samples, (ii) energy-aware client scheduling to mitigate device
dropouts and ensure balanced participation, and (iii) secure aggregation with
calibrated differential privacy to safeguard sensitive model updates. Unlike
prior work that addresses these aspects in isolation, AFFR unifies them into a
modular pipeline deployable on real-world clinical networks. Experimental
evaluation on simulated rare-disease detection datasets demonstrates up to 10%
improvement in accuracy compared with baseline FL, while reducing client
dropouts by over 50% without degrading convergence. Furthermore,
privacy-utility trade-offs remain within clinically acceptable bounds. These
findings highlight AFFR as a practical pathway for equitable and trustworthy
federated diagnosis of rare conditions.

</details>


### [166] [Integrating AI and Ensemble Forecasting: Explainable Materials Planning with Scorecards and Trend Insights for a Large-Scale Manufacturer](https://arxiv.org/abs/2510.01006)
*Saravanan Venkatachalam*

Main category: cs.AI

TL;DR: 本文提出一个实用的售后需求预测与监控架构，整合了统计、机器学习和深度学习模型，并结合了驱动角色的分析层和LLMs，以提供可操作的库存决策洞察。


<details>
  <summary>Details</summary>
Motivation: 现有系统可能无法提供深入的、可操作的预测洞察和监控能力。该研究旨在构建一个统一的、实践导向的系统，不仅提供预测，还能通过分析层和LLMs回答“准确度走向何方，应采取哪些措施”等关键业务问题，从而优化库存管理。

Method: 该架构融合了统计、机器学习和深度学习模型，并引入收益与集群感知的集成方法。它摄取外部信号（如安装基数、定价、宏观指标、生命周期、季节性），并将COVID-19视为独立状态。采用帕累托分割法处理高收益和长尾商品，并根据业务相关损失（如WMAPE）调整集成权重。一个性能记分卡和趋势模块提供决策洞察，如准确度、偏差分解和根本原因。此外，嵌入LLMs在分析层生成针对角色的叙述，标准化业务定义，并自动化质量检查和总结。

Result: 该系统能够生成带有校准区间的国家-零件预测，并通过性能记分卡和趋势模块提供决策导向的洞察。它能识别准确度、偏差分解、热点及根本原因。LLMs的应用使得定量结果转化为简洁、可解释的摘要。该系统提供了一个可重现的工作流程，成功地将预测、监控与库存决策环节闭环，应用于90多个国家和约6,000个零件，帮助规划者从“我们现在有多准确？”转向“准确度走向何方以及我们应该采取哪些措施？”。

Conclusion: 该文提出的售后需求预测与监控架构是一个全面、实用且可扩展的解决方案。它通过集成多源模型、高级分析功能（包括LLMs）和可重现的工作流，显著提升了预测的准确性和可操作性，为复杂的全球供应链环境中的库存管理和战略决策提供了有力支持。

Abstract: This paper presents a practical architecture for after-sales demand
forecasting and monitoring that unifies a revenue- and cluster-aware ensemble
of statistical, machine-learning, and deep-learning models with a role-driven
analytics layer for scorecards and trend diagnostics. The framework ingests
exogenous signals (installed base, pricing, macro indicators, life cycle,
seasonality) and treats COVID-19 as a distinct regime, producing country-part
forecasts with calibrated intervals. A Pareto-aware segmentation forecasts
high-revenue items individually and pools the long tail via clusters, while
horizon-aware ensembling aligns weights with business-relevant losses (e.g.,
WMAPE). Beyond forecasts, a performance scorecard delivers decision-focused
insights: accuracy within tolerance thresholds by revenue share and count, bias
decomposition (over- vs under-forecast), geographic and product-family
hotspots, and ranked root causes tied to high-impact part-country pairs. A
trend module tracks trajectories of MAPE/WMAPE and bias across recent months,
flags entities that are improving or deteriorating, detects change points
aligned with known regimes, and attributes movements to lifecycle and seasonal
factors. LLMs are embedded in the analytics layer to generate role-aware
narratives and enforce reporting contracts. They standardize business
definitions, automate quality checks and reconciliations, and translate
quantitative results into concise, explainable summaries for planners and
executives. The system exposes a reproducible workflow -- request
specification, model execution, database-backed artifacts, and AI-generated
narratives -- so planners can move from "How accurate are we now?" to "Where is
accuracy heading and which levers should we pull?", closing the loop between
forecasting, monitoring, and inventory decisions across more than 90 countries
and about 6,000 parts.

</details>


### [167] [Shape Happens: Automatic Feature Manifold Discovery in LLMs via Supervised Multi-Dimensional Scaling](https://arxiv.org/abs/2510.01025)
*Federico Tiblias,Irina Bigoulaeva,Jingcheng Niu,Simone Balloccu,Iryna Gurevych*

Main category: cs.AI

TL;DR: 本文提出一种模型无关的方法SMDS，用于自动发现语言模型潜在空间中的概念特征流形。研究发现这些流形形成多种几何结构，稳定且动态支持模型推理，支持语言模型通过结构化表示进行实体推理的假设。


<details>
  <summary>Details</summary>
Motivation: 现有研究在发现语言模型（LMs）潜在空间中概念的几何结构方面缺乏泛化性，仅关注特定特征的特定几何结构，而未能提供一种普遍适用的方法。

Method: 引入并提出监督多维尺度分析（Supervised Multi-Dimensional Scaling, SMDS），这是一种与模型无关的方法，能够自动发现特征流形。研究以时间推理作为案例分析。

Result: 通过SMDS发现，LMs中不同特征形成多种几何结构，如圆形、直线和簇。这些结构一致反映其所代表概念的属性，在不同模型家族和尺寸之间保持稳定，积极支持模型推理，并能响应上下文变化而动态重塑。

Conclusion: 本研究揭示了特征流形的功能作用，支持了一种实体推理模型，即语言模型通过编码和转换结构化表示来进行推理。

Abstract: The linear representation hypothesis states that language models (LMs) encode
concepts as directions in their latent space, forming organized,
multidimensional manifolds. Prior efforts focus on discovering specific
geometries for specific features, and thus lack generalization. We introduce
Supervised Multi-Dimensional Scaling (SMDS), a model-agnostic method to
automatically discover feature manifolds. We apply SMDS to temporal reasoning
as a case study, finding that different features form various geometric
structures such as circles, lines, and clusters. SMDS reveals many insights on
these structures: they consistently reflect the properties of the concepts they
represent; are stable across model families and sizes; actively support
reasoning in models; and dynamically reshape in response to context changes.
Together, our findings shed light on the functional role of feature manifolds,
supporting a model of entity-based reasoning in which LMs encode and transform
structured representations.

</details>


### [168] [Uncovering the Computational Ingredients of Human-Like Representations in LLMs](https://arxiv.org/abs/2510.01030)
*Zach Studdiford,Timothy T. Rogers,Kushin Mukherjee,Siddharth Suresh*

Main category: cs.AI

TL;DR: 评估70多个LLM，发现指令微调和更大的注意力头维度是实现人类概念表征对齐的关键，现有基准无法充分衡量此对齐。


<details>
  <summary>Details</summary>
Motivation: 识别构建具有人类级表征的LLM所需的核心计算要素尚不明确；现有LLM基准不足以衡量人类与模型间的表征对齐。

Method: 使用认知科学中测量人类概念表征的“三元组相似性任务”，评估了70多个具有不同计算要素的LLM（基于THINGS数据库的概念），并与人类表征进行比较。

Result: 指令微调和更大注意力头维度的模型与人类表征对齐度更高，而多模态预训练和参数大小影响有限。现有基准（如MMLU）虽有差异，但无一能完全捕捉人类-AI表征对齐的全部方差。

Conclusion: 识别了推动LLM向人类概念表征模型发展的关键计算要素，并填补了LLM评估中衡量人类-AI对齐的基准空白。

Abstract: The ability to translate diverse patterns of inputs into structured patterns
of behavior has been thought to rest on both humans' and machines' ability to
learn robust representations of relevant concepts. The rapid advancement of
transformer-based large language models (LLMs) has led to a diversity of
computational ingredients -- architectures, fine tuning methods, and training
datasets among others -- but it remains unclear which of these ingredients are
most crucial for building models that develop human-like representations.
Further, most current LLM benchmarks are not suited to measuring
representational alignment between humans and models, making benchmark scores
unreliable for assessing if current LLMs are making progress towards becoming
useful cognitive models. We address these limitations by first evaluating a set
of over 70 models that widely vary in their computational ingredients on a
triplet similarity task, a method well established in the cognitive sciences
for measuring human conceptual representations, using concepts from the THINGS
database. Comparing human and model representations, we find that models that
undergo instruction-finetuning and which have larger dimensionality of
attention heads are among the most human aligned, while multimodal pretraining
and parameter size have limited bearing on alignment. Correlations between
alignment scores and scores on existing benchmarks reveal that while some
benchmarks (e.g., MMLU) are better suited than others (e.g., MUSR) for
capturing representational alignment, no existing benchmark is capable of fully
accounting for the variance of alignment scores, demonstrating their
insufficiency in capturing human-AI alignment. Taken together, our findings
help highlight the computational ingredients most essential for advancing LLMs
towards models of human conceptual representation and address a key
benchmarking gap in LLM evaluation.

</details>


### [169] [Activation-Deactivation: A General Framework for Robust Post-hoc Explainable AI](https://arxiv.org/abs/2510.01038)
*Akchunya Chanchal,David A. Kelly,Hana Chockler*

Main category: cs.AI

TL;DR: 本文提出一种名为Activation-Deactivation (AD) 的新范式及其实现ConvAD，用于黑盒解释方法，通过关闭模型对应部分来消除遮挡输入特征的影响，从而生成更鲁棒的解释，无需额外训练或领域知识。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒解释方法依赖于遮挡部分输入来生成变异体，这会导致输入图像脱离原始分布，从而降低解释质量。此外，选择合适的遮挡值通常需要领域知识，增加了方法的复杂性。

Method: 引入一种新颖的前向传播范式——Activation-Deactivation (AD)。该范式通过关闭模型中与遮挡部分对应的组件来消除遮挡输入特征对模型决策的影响。具体实现为ConvAD，一个可即插即用、方便添加到任何已训练卷积神经网络(CNN)的机制。ConvAD不改变网络的决策过程。

Result: ConvAD在多个数据集和模型架构上进行了实验评估，并与使用一系列遮罩值获得的解释进行了比较。结果表明，AD解释的鲁棒性一致提高（高达62.5%），优于基于遮挡的解释。这证明ConvAD能够提取更鲁棒的解释，且无需领域知识。该机制无需额外训练或微调。

Conclusion: ConvAD提供了一种生成更鲁棒的黑盒解释的新方法，解决了传统遮挡方法中因输入脱离分布和需要领域知识选择遮挡值而导致的解释质量问题。它在不改变网络决策的前提下显著提升了解释的鲁棒性。

Abstract: Black-box explainability methods are popular tools for explaining the
decisions of image classifiers. A major drawback of these tools is their
reliance on mutants obtained by occluding parts of the input, leading to
out-of-distribution images. This raises doubts about the quality of the
explanations. Moreover, choosing an appropriate occlusion value often requires
domain knowledge. In this paper we introduce a novel forward-pass paradigm
Activation-Deactivation (AD), which removes the effects of occluded input
features from the model's decision-making by switching off the parts of the
model that correspond to the occlusions. We introduce ConvAD, a drop-in
mechanism that can be easily added to any trained Convolutional Neural Network
(CNN), and which implements the AD paradigm. This leads to more robust
explanations without any additional training or fine-tuning. We prove that the
ConvAD mechanism does not change the decision-making process of the network. We
provide experimental evaluation across several datasets and model
architectures. We compare the quality of AD-explanations with explanations
achieved using a set of masking values, using the proxies of robustness, size,
and confidence drop-off. We observe a consistent improvement in robustness of
AD explanations (up to 62.5%) compared to explanations obtained with
occlusions, demonstrating that ConvAD extracts more robust explanations without
the need for domain knowledge.

</details>


### [170] [Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning](https://arxiv.org/abs/2510.01069)
*Elija Perrier*

Main category: cs.AI

TL;DR: 本文针对CoT推理的忠实性问题，提出基于Curry-Howard对应的新理论框架。通过将CoT的自然语言步骤转换为形式化、类型化的证明结构，实现对其计算忠实性的形式验证，从而提升AI系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管思维链（CoT）提示提升了大型语言模型的推理能力，但其生成推理过程（rationales）的忠实性问题仍未解决，这阻碍了模型的可解释性。

Method: 本文提出了一个基于Curry-Howard对应（形式证明与计算机程序的直接关系）的理论视角。在此范式下，忠实的推理轨迹被类比为良类型程序，每个中间步骤对应一个类型化的逻辑推理。作者将这一类比操作化，提供了方法来提取并映射CoT中非正式的自然语言步骤到形式化、类型化的证明结构中。

Result: 成功地将CoT轨迹转换为一个良类型证明，即为模型计算忠实性提供了一个强大、可验证的凭证。这使得对CoT的分析从启发式可解释性迈向了形式化验证。

Conclusion: 该框架提供了一种将貌似合理的叙述性解释转换为可形式化验证程序的方法，为构建更可靠、更值得信赖的AI系统指明了道路。

Abstract: While Chain-of-Thought (CoT) prompting enhances the reasoning capabilities of
large language models, the faithfulness of the generated rationales remains an
open problem for model interpretability. We propose a novel theoretical lens
for this problem grounded in the Curry-Howard correspondence, which posits a
direct relationship between formal proofs and computer programs. Under this
paradigm, a faithful reasoning trace is analogous to a well-typed program,
where each intermediate step corresponds to a typed logical inference. We
operationalise this analogy, presenting methods to extract and map the
informal, natural language steps of CoT into a formal, typed proof structure.
Successfully converting a CoT trace into a well-typed proof serves as a strong,
verifiable certificate of its computational faithfulness, moving beyond
heuristic interpretability towards formal verification. Our framework provides
a methodology to transform plausible narrative explanations into formally
verifiable programs, offering a path towards building more reliable and
trustworthy AI systems.

</details>


### [171] [Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense](https://arxiv.org/abs/2510.01088)
*Guobin Shen,Dongcheng Zhao,Haibo Tong,Jindong Li,Feifei Zhao,Yi Zeng*

Main category: cs.AI

TL;DR: 发现LLM内部存在安全本能（熵差距），提出SIRL方法将此转化为自生成奖励信号，有效强化模型安全拒绝行为，无需外部标注，实现高防御成功率并保持性能。


<details>
  <summary>Details</summary>
Motivation: LLM安全保障面临缺乏通用标准和可靠内容验证器的挑战，导致难以获取有效的训练信号。研究发现对齐模型已具备强大的内部安全信念，对有害请求能高置信度拒绝，而生成危险内容时熵值较高，这一熵差距揭示了模型“知道”何时拒绝。

Method: 引入Safety Instincts Reinforcement Learning (SIRL) 方法，将模型内部置信度（熵差距）转化为自生成奖励信号，无需依赖外部验证器或人工标注。SIRL通过强化低熵拒绝行为，训练模型信任其安全本能。

Result: 在Llama和Qwen模型上，SIRL对20多种越狱方法（从静态提示到自适应攻击）保持89%以上的防御成功率。仅使用15,000个未标注提示，SIRL超越了资源密集型监督方法，同时在数学、编码和对话基准测试中保持了性能。

Conclusion: 研究表明有效的对齐可以从模型内部产生，为更自主、更强大的AI安全机制铺平道路，使其无需大量人工监督即可扩展。

Abstract: Ensuring Large Language Model (LLM) safety remains challenging due to the
absence of universal standards and reliable content validators, making it
difficult to obtain effective training signals. We discover that aligned models
already possess robust internal safety beliefs: they consistently produce
high-confidence refusals to harmful requests while exhibiting high entropy when
generating potentially dangerous content. This entropy gap reveals an untapped
signal--models intrinsically "know" when to refuse. We introduce Safety
Instincts Reinforcement Learning (SIRL), which transforms this internal
confidence into a self-generated reward signal, eliminating dependence on
external validators or human annotations. SIRL teaches models to trust their
safety instincts by reinforcing low-entropy refusal behaviors. Evaluated on
Llama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against
20+ jailbreak methods, from static prompts to adaptive attacks. Using only
15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods
while preserving performance on mathematics, coding, and conversation
benchmarks. Our work demonstrates that effective alignment can emerge from
within, paving the way for more autonomous and robust AI safety mechanisms that
scale without extensive human oversight.

</details>


### [172] [Optimizing Fairness in Production Planning: A Human-Centric Approach to Machine and Workforce Allocation](https://arxiv.org/abs/2510.01094)
*Alexander Nasuta,Alessandro Cisi,Sylwia Olbrych,Gustavo Vieira,Rui Fernandes,Lucas Paletta,Marlene Mayr,Rishyank Chevuri,Robert Woitsch,Hans Aoyang Zhou,Anas Abdelrazeq,Robert H. Schmitt*

Main category: cs.AI

TL;DR: 本文提出了一个双层、以人为中心的生产计划框架，通过结合约束规划和马尔可夫决策过程，旨在优化工业制造中的运营效率和员工公平性。


<details>
  <summary>Details</summary>
Motivation: 在工业制造中，需要一个能够同时优化运营效率和员工福祉（公平性）的生产计划框架。

Method: 采用双层框架：第一层将订单-生产线分配建模为约束规划（CP）问题；第二层将工人-生产线分配建模为马尔可夫决策过程（MDP），并结合了贪婪分配、MCTS和强化学习（RL）三种求解策略。通过与汽车行业领域专家的测试会话进行验证。

Result: CP调度方法能生成紧凑、可行的低延迟生产计划；MDP工人分配显著提高了公平性和偏好一致性。领域专家认为系统组件有效。

Conclusion: 将CP与基于学习的决策相结合，为以人为中心的生产计划提供了一种稳健方法，可同时优化吞吐量和员工福祉，为工业环境中的公平高效制造调度提供了实用基础。

Abstract: This work presents a two-layer, human-centric production planning framework
designed to optimize both operational efficiency and workforce fairness in
industrial manufacturing. The first layer formulates the Order-Line allocation
as a Constraint Programming (CP) problem, generating high-utilization
production schedules that respect machine capacities, processing times, and due
dates. The second layer models Worker-Line allocation as a Markov Decision
Process (MDP), integrating human factors such as worker preference, experience,
resilience, and medical constraints into the assignment process. Three solution
strategies, greedy allocation, MCTS, and RL, are implemented and compared
across multiple evaluation scenarios. The proposed system is validated through
16 test sessions with domain experts from the automotive industry, combining
quantitative key performance indicators (KPIs) with expert ratings. Results
indicate that the CP-based scheduling approach produces compact, feasible
production plans with low tardiness, while the MDP-based worker allocation
significantly improves fairness and preference alignment compared to baseline
approaches. Domain experts rated both the Order-Line and Worker-Line components
as effective and highlighted opportunities to further refine the objective
function to penalize excessive earliness and improve continuity in worker
assignments. Overall, the findings demonstrate that combining CP with
learning-based decision-making provides a robust approach for human-centric
production planning. The approach enables simultaneous optimization of
throughput and workforce well-being, offering a practical foundation for fair
and efficient manufacturing scheduling in industrial settings.

</details>


### [173] [PRISM-Consult: A Panel-of-Experts Architecture for Clinician-Aligned Diagnosis](https://arxiv.org/abs/2510.01114)
*Lionel Levine,John Santerre,Alexander S. Young,T. Barry Levine,Francis Campion,Majid Sarrafzadeh*

Main category: cs.AI

TL;DR: PRISM-Consult是一个基于路由器的专家系统架构，将紧凑型PRISM模型扩展为领域专家组，用于大规模临床咨询，旨在提供安全、可审计和低延迟的医疗咨询服务。


<details>
  <summary>Details</summary>
Motivation: 解决临床实践中对安全、可审计、低延迟且可扩展的医疗咨询的需求，并提高现有紧凑模型的效率和可解释性。

Method: 提出PRISM-Consult架构，该架构通过轻量级路由器根据初始临床事件标记，将病例分派给心血管、肺部等领域专家模型。这些专家模型继承PRISM的小型Transformer骨干，以保持参数效率和可解释性。论文还详述了数据处理、路由阈值和校准方法。

Result: 各领域专家模型展现出平稳收敛和低的开发困惑度。路由器实现了高路由质量，并在安全优先策略下相较于“全部咨询”模式节省了大量计算资源。报告了分领域结果，避免了常见事件的支配效应。

Conclusion: 该框架为实现大规模、安全、可审计、低延迟的临床咨询提供了实用路径。论文还概述了满足未来临床部署标准的验证步骤，包括外部/时间复制、非对称生命威胁阈值和多标签仲裁。

Abstract: We present PRISM-Consult, a clinician-aligned panel-of-experts architecture
that extends the compact PRISM sequence model into a routed family of domain
specialists. Episodes are tokenized as structured clinical events; a
light-weight router reads the first few tokens and dispatches to specialist
models (Cardiac-Vascular, Pulmonary, Gastro-Oesophageal, Musculoskeletal,
Psychogenic). Each specialist inherits PRISM's small transformer backbone and
token template, enabling parameter efficiency and interpretability. On
real-world Emergency Department cohorts, specialists exhibit smooth convergence
with low development perplexities across domains, while the router achieves
high routing quality and large compute savings versus consult-all under a
safety-first policy. We detail the data methodology (initial vs. conclusive
ICD-9 families), routing thresholds and calibration, and report per-domain
results to avoid dominance by common events. The framework provides a practical
path to safe, auditable, and low-latency consult at scale, and we outline
validation steps-external/temporal replication, asymmetric life-threat
thresholds, and multi-label arbitration-to meet prospective clinical deployment
standards.

</details>


### [174] [Exploring Network-Knowledge Graph Duality: A Case Study in Agentic Supply Chain Risk Analysis](https://arxiv.org/abs/2510.01115)
*Evan Heus,Rick Bookstaber,Dhruv Sharma*

Main category: cs.AI

TL;DR: 提出一个以LLM为中心的代理框架，通过将供应链网络视为知识图谱并利用“上下文外壳”整合量化数据，实现对复杂金融风险的实时、可解释分析，无需高成本微调。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）难以处理金融风险中复杂、多模态、网络原生数据。标准的检索增强生成（RAG）方法过分简化关系，而专业模型则成本高昂且不灵活。

Method: 开发了一个以LLM为中心的代理框架进行供应链风险分析。核心方法是将供应链网络视为知识图谱（KG），利用结构化网络科学原理进行检索。通过图遍历器（由网络中心性分数引导）高效提取经济上显著的风险路径。该代理架构整合图谱检索、数值因子表和新闻流数据，并采用创新的“上下文外壳”（将原始数据嵌入自然语言的描述性模板），使LLM能够充分理解量化数据。

Result: 该轻量级方法使模型能够实时生成简洁、可解释、上下文丰富的风险叙述。

Conclusion: 该方法无需昂贵的微调或专用图数据库，有效解决了LLMs在处理复杂金融风险数据方面的挑战，提供了一个高效且可扩展的解决方案。

Abstract: Large Language Models (LLMs) struggle with the complex, multi-modal, and
network-native data underlying financial risk. Standard Retrieval-Augmented
Generation (RAG) oversimplifies relationships, while specialist models are
costly and static. We address this gap with an LLM-centric agent framework for
supply chain risk analysis. Our core contribution is to exploit the inherent
duality between networks and knowledge graphs (KG). We treat the supply chain
network as a KG, allowing us to use structural network science principles for
retrieval. A graph traverser, guided by network centrality scores, efficiently
extracts the most economically salient risk paths. An agentic architecture
orchestrates this graph retrieval alongside data from numerical factor tables
and news streams. Crucially, it employs novel ``context shells'' -- descriptive
templates that embed raw figures in natural language -- to make quantitative
data fully intelligible to the LLM. This lightweight approach enables the model
to generate concise, explainable, and context-rich risk narratives in real-time
without costly fine-tuning or a dedicated graph database.

</details>


### [175] [Apriel-1.5-15b-Thinker](https://arxiv.org/abs/2510.01141)
*Shruthan Radhakrishna,Aman Tiwari,Aanjaneya Shukla,Masoud Hashemi,Rishabh Maheshwary,Shiva Krishna Reddy Malay,Jash Mehta,Pulkit Pattnaik,Saloni Mittal,Khalil Slimi,Kelechi Ogueji,Akintunde Oladipo,Soham Parikh,Oluwanifemi Bamgbose,Toby Liang,Ahmed Masry,Khyati Mahajan,Sai Rajeswar Mudumba,Vikas Yadav,Sathwik Tejaswi Madhusudhan,Torsten Scholak,Sagar Davasam,Srinivas Sunkara,Nicholas Chapados*

Main category: cs.AI

TL;DR: Apriel-1.5-15B-Thinker是一个150亿参数的多模态推理模型，通过三阶段训练设计而非纯粹规模实现了前沿性能，且可在单GPU部署。模型和训练方法已开源。


<details>
  <summary>Details</summary>
Motivation: 旨在证明通过精心设计的中期训练而非大规模资源投入，可以弥补能力差距，使前沿多模态推理能力对资源有限的组织也更易获得和部署。

Method: 采用渐进式三阶段方法：1) 深度扩展，在不从头预训练的情况下扩展推理能力；2) 分阶段持续预训练，先建立基础文本和视觉理解，再通过生成针对空间结构、组合理解和细粒度感知的合成数据来增强视觉推理；3) 高质量文本监督微调，使用包含数学、编码、科学和工具使用等推理痕迹的指令-响应对。值得注意的是，该方法未依赖强化学习或偏好优化。

Result: 在Artificial Analysis Intelligence Index上获得52分，与DeepSeek-R1-0528性能持平但计算资源需求显著减少。在十个图像基准测试中，平均性能与Gemini-2.5-Flash和Claude Sonnet-3.7相差不到五分，对单GPU部署模型而言是重大突破。

Conclusion: 研究结果表明，深思熟虑的中期训练设计能够在不依赖大规模模型的情况下弥补显著的能力差距，使得前沿多模态推理能力对基础设施有限的组织也变得触手可及。项目已开源模型、训练配方和评估协议。

Abstract: We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights
multimodal reasoning model that achieves frontier-level performance through
training design rather than sheer scale. Starting from Pixtral-12B, we apply a
progressive three-stage methodology: (1) depth upscaling to expand reasoning
capacity without pretraining from scratch, (2) staged continual pre-training
that first develops foundational text and vision understanding, then enhances
visual reasoning through targeted synthetic data generation addressing spatial
structure, compositional understanding, and fine-grained perception, and (3)
high-quality text-only supervised fine-tuning on curated instruction-response
pairs with explicit reasoning traces spanning mathematics, coding, science, and
tool use. Notably, our model achieves competitive results without reinforcement
learning or preference optimization, isolating the contribution of our
data-centric continual pre-training approach. On the Artificial Analysis
Intelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching
DeepSeek-R1-0528 despite requiring significantly fewer computational resources.
Across ten image benchmarks, its performance is on average within five points
of Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model
operating within single-GPU deployment constraints. Our results demonstrate
that thoughtful mid-training 2 design can close substantial capability gaps
without massive scale, making frontier-level multimodal reasoning accessible to
organizations with limited infrastructure. We release the model checkpoint, all
training recipes, and evaluation protocols under the MIT license to to advance
open-source research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [176] [Methodological Framework for Quantifying Semantic Test Coverage in RAG Systems](https://arxiv.org/abs/2510.00001)
*Noah Broestl,Adel Nasser Abdalla,Rajprakash Bale,Hersh Gupta,Max Struever*

Main category: cs.LG

TL;DR: 本文提出一种量化RAG系统测试问题对知识库语义覆盖度的方法，利用向量嵌入和聚类来识别测试盲点，并提供改进测试集的建议。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统评估框架缺乏系统方法确保测试问题充分覆盖底层知识库，导致开发者存在显著的盲点，无法可靠地评估系统性能。

Method: 提出一种新颖的量化方法，通过将文档块和测试问题嵌入统一向量空间，并结合向量嵌入和聚类算法。该方法计算多种覆盖度指标（基本接近度、内容加权覆盖和多主题问题覆盖），并融入异常值检测以过滤不相关问题，从而验证测试集的全面性。

Result: 通过两个独立用例的实验证明，该框架能有效量化测试覆盖度，识别知识库中表示不足的特定内容区域，并为生成新的、高价值测试问题提供具体建议。

Conclusion: 该工作为RAG开发者提供了构建更健壮测试套件的基本工具，从而提高系统可靠性，并可扩展应用于识别不匹配的文档。

Abstract: Reliably determining the performance of Retrieval-Augmented Generation (RAG)
systems depends on comprehensive test questions. While a proliferation of
evaluation frameworks for LLM-powered applications exists, current practices
lack a systematic method to ensure these test sets adequately cover the
underlying knowledge base, leaving developers with significant blind spots. To
address this, we present a novel, applied methodology to quantify the semantic
coverage of RAG test questions against their underlying documents. Our approach
leverages existing technologies, including vector embeddings and clustering
algorithms, to create a practical framework for validating test
comprehensiveness. Our methodology embeds document chunks and test questions
into a unified vector space, enabling the calculation of multiple coverage
metrics: basic proximity, content-weighted coverage, and multi-topic question
coverage. Furthermore, we incorporate outlier detection to filter irrelevant
questions, allowing for the refinement of test sets. Experimental evidence from
two distinct use cases demonstrates that our framework effectively quantifies
test coverage, identifies specific content areas with inadequate
representation, and provides concrete recommendations for generating new,
high-value test questions. This work provides RAG developers with essential
tools to build more robust test suites, thereby improving system reliability
and extending to applications such as identifying misaligned documents.

</details>


### [177] [Learning Inter-Atomic Potentials without Explicit Equivariance](https://arxiv.org/abs/2510.00027)
*Ahmed A. Elhag,Arun Raja,Alex Morehead,Samuel M. Blau,Garrett M. Morris,Michael M. Bronstein*

Main category: cs.LG

TL;DR: TransIP提出了一种基于Transformer的新训练范式，通过优化嵌入空间表示实现原子间势的对称性，无需显式架构约束，性能与现有最佳模型相当或更优。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习原子间势（MLIPs）通过等变神经网络强制实施对称性，但这可能导致灵活性、计算效率和可扩展性受限。需要更准确、可扩展的MLIPs。

Method: 引入TransIP，一种基于Transformer的原子间势模型。它通过在嵌入空间中优化其表示来学习SO(3)等变性，从而实现对称性，避免了显式的架构约束。

Result: 在Open Molecules (OMol25) 数据集上训练后，TransIP在ML力场方面达到了与当前最先进的等变基线相当的性能。与数据增强基线相比，TransIP在不同OMol25数据集大小下性能提升了40%至60%。

Conclusion: 本研究表明，学习到的等变性可以是等变或基于增强的MLIP模型的一种强大且高效的替代方案，为实现原子间势中的对称性提供了一种新范式。

Abstract: Accurate and scalable machine-learned inter-atomic potentials (MLIPs) are
essential for molecular simulations ranging from drug discovery to new material
design. Current state-of-the-art models enforce roto-translational symmetries
through equivariant neural network architectures, a hard-wired inductive bias
that can often lead to reduced flexibility, computational efficiency, and
scalability. In this work, we introduce TransIP: Transformer-based Inter-Atomic
Potentials, a novel training paradigm for interatomic potentials achieving
symmetry compliance without explicit architectural constraints. Our approach
guides a generic non-equivariant Transformer-based model to learn
SO(3)-equivariance by optimizing its representations in the embedding space.
Trained on the recent Open Molecules (OMol25) collection, a large and diverse
molecular dataset built specifically for MLIPs and covering different types of
molecules (including small organics, biomolecular fragments, and
electrolyte-like species), TransIP attains comparable performance in
machine-learning force fields versus state-of-the-art equivariant baselines.
Further, compared to a data augmentation baseline, TransIP achieves 40% to 60%
improvement in performance across varying OMol25 dataset sizes. More broadly,
our work shows that learned equivariance can be a powerful and efficient
alternative to equivariant or augmentation-based MLIP models.

</details>


### [178] [Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling](https://arxiv.org/abs/2510.00028)
*Ye Qiao,Haocheng Xu,Xiaofan Zhang,Sitao Huang*

Main category: cs.LG

TL;DR: 本文发现RoPE位置插值（PI）与量化（PTQ）结合会显著降低LLM长文本精度。通过系统分析原因并提出Q-ROAR方法，该方法通过分频段自适应缩放键值权重，将长文本困惑度降低14%以上，同时保持短文本性能和部署效率。


<details>
  <summary>Details</summary>
Motivation: 扩展LLM的上下文窗口对于处理长距离依赖任务至关重要。RoPE插值等方法可在不重训练的情况下支持更长输入，而后训练量化（PTQ）则使得部署更具可行性。然而，研究发现将RoPE位置插值（PI）与PTQ结合会导致精度下降，原因包括长上下文混叠、动态范围膨胀、轴对齐量化器与旋转RoPE对之间的各向异性以及异常值位移等问题。

Method: 首先，论文对PI+PTQ结合导致的精度下降进行了首次系统性分析，并引入了“插值压力”和“尾部膨胀比”两种诊断工具。在此基础上，提出了Q-ROAR（Quantization, RoPE-interpolation, and Outlier Aware Rescaling）方法。Q-ROAR是一种仅针对权重的、插值感知的稳定化方案，用于量化LLM中的PI。它将RoPE维度分组为少数频带，并对Key和Query权重进行轻量级的逐频带尺度搜索，搜索过程由诊断工具指导，使用少量长上下文开发数据集，无需模型微调、架构或内核更改，也没有额外的部署开销。

Result: 实验结果表明，Q-ROAR将模型在长上下文工作负载上的困惑度降低了14%以上。同时，它保持了短上下文性能、推理吞吐量以及与现有LLM系统堆栈的兼容性。

Conclusion: RoPE位置插值与后训练量化结合存在严重的精度下降问题。Q-ROAR通过系统分析并引入新诊断工具，提出了一种无需额外开销的、高效的权重级稳定化方案，有效解决了这一问题，显著提升了量化LLM在长上下文任务上的性能，为实际部署提供了实用方法。

Abstract: Extending the context window support of large language models (LLMs) is
crucial for tasks with long-distance dependencies. RoPE-based interpolation and
extrapolation methods, such as linear scaling and frequency-aware schemes,
enable longer input length support without retraining, while post-training
quantization (PTQ) makes deployment practical. However, we show that combining
RoPE position interpolation (PI) with PTQ degrades accuracy due to coupled
effects including long-context aliasing, dynamic-range dilation, anisotropy
from axis-aligned quantizers vs. rotated RoPE pairs, and outlier shifting that
produces position-dependent logit noise. We provide, to the best of our
knowledge, the first systematic analysis of the PI+PTQ approach and introduce
two practical diagnostics: interpolation pressure (per-band sensitivity to
phase scaling) and tail-inflation ratios (outlier shift from short to long
contexts). Following the analysis results, we propose Q-ROAR (Quantization,
RoPE-interpolation, and Outlier Aware Rescaling), a weight-only,
interpolation-aware stabilization of PI for quantized LLMs. Q-ROAR groups RoPE
dimensions into a small number of frequency bands and performs a lightweight
search over per-band scales for Key and Query weights (with an optional
symmetric variant to preserve logit scale). The search is guided by our
diagnostics and uses a tiny long-context development dataset, requiring no
fine-tuning to the model, no architecture or kernel changes, and no additional
deployment overhead. Empirically, Q-ROAR reduces the model's perplexity on
long-context workloads by more than 14%, while preserving short-context
performance, inference throughput, and compatibility with existing LLM system
stacks.

</details>


### [179] [DexBench: Benchmarking LLMs for Personalized Decision Making in Diabetes Management](https://arxiv.org/abs/2510.00038)
*Maria Ana Cardei,Josephine Lamp,Mark Derdzinski,Karan Bhatia*

Main category: cs.LG

TL;DR: DexBench是一个新颖的基准，用于评估大型语言模型（LLM）在糖尿病患者日常决策任务中的表现，涵盖7类任务，基于15,000名患者数据生成36万个问题，并使用5项指标评估8个LLM，发现模型表现存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 现有健康基准普遍通用、面向临床医生或侧重于临床任务，未能有效评估面向糖尿病患者的AI解决方案在日常决策中的性能和挑战。

Method: 构建了包含来自15,000名不同糖尿病人群（1型、2型、前驱/普通健康）的一个月连续血糖监测数据和行为日志的丰富数据集。基于此数据，生成了7类共360,600个个性化、情境化问题。通过准确性、基础性、安全性、清晰度和可操作性5个指标，评估了8个近期LLM在这些任务上的性能。

Result: 对8个近期LLM的分析显示，模型在不同任务和评估指标上表现出显著差异性，没有一个模型能在所有维度上持续优于其他模型。

Conclusion: 通过建立DexBench基准，旨在推进糖尿病护理领域AI解决方案的可靠性、安全性、有效性和实用性。

Abstract: We present DexBench, the first benchmark designed to evaluate large language
model (LLM) performance across real-world decision-making tasks faced by
individuals managing diabetes in their daily lives. Unlike prior health
benchmarks that are either generic, clinician-facing or focused on clinical
tasks (e.g., diagnosis, triage), DexBench introduces a comprehensive evaluation
framework tailored to the unique challenges of prototyping patient-facing AI
solutions in diabetes, glucose management, metabolic health and related
domains. Our benchmark encompasses 7 distinct task categories, reflecting the
breadth of real-world questions individuals with diabetes ask, including basic
glucose interpretation, educational queries, behavioral associations, advanced
decision making and long term planning. Towards this end, we compile a rich
dataset comprising one month of time-series data encompassing glucose traces
and metrics from continuous glucose monitors (CGMs) and behavioral logs (e.g.,
eating and activity patterns) from 15,000 individuals across three different
diabetes populations (type 1, type 2, pre-diabetes/general health and
wellness). Using this data, we generate a total of 360,600 personalized,
contextual questions across the 7 tasks. We evaluate model performance on these
tasks across 5 metrics: accuracy, groundedness, safety, clarity and
actionability. Our analysis of 8 recent LLMs reveals substantial variability
across tasks and metrics; no single model consistently outperforms others
across all dimensions. By establishing this benchmark, we aim to advance the
reliability, safety, effectiveness and practical utility of AI solutions in
diabetes care.

</details>


### [180] [Linear Regression in p-adic metric spaces](https://arxiv.org/abs/2510.00043)
*Gregory D. Baker,Scott McCallum,Dirk Pattinson*

Main category: cs.LG

TL;DR: 该研究为p-adic度量空间中的机器学习奠定了理论基础，证明了其在处理层级数据方面优于传统欧几里得度量，核心发现是n维平面会通过至少n+1个数据点，并通过自然语言处理应用验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法依赖欧几里得度量，但其无法有效捕捉现实世界中固有层级数据的离散、分支特性，导致处理层级关系时表现不佳。

Method: 提出了在p-adic度量空间中进行机器学习的理论基础，该空间天然地尊重层级结构。通过理论证明一个n维平面在最小化p-adic距离和时必须通过至少n+1个数据点。并通过在自然语言处理中分析层级分类和建模语法形态的应用来展示其实用性。

Result: 主要结果：最小化数据集中点到n维平面p-adic距离和的平面必须通过至少n+1个点，这与欧几里得回归形成鲜明对比。推论：最小化p-adic残差和的n次多项式将通过至少n+1个点，且近似高次多项式的n次多项式会产生具有不同有理根的差多项式。在NLP应用中展示了其在处理层级分类和语法形态方面的潜力。

Conclusion: 研究结果表明p-adic度量可能对于机器学习中正确处理层级数据结构至关重要。在层级数据中，选择实际观察到的点作为代表比点之间插值更有意义。

Abstract: Many real-world machine learning problems involve inherently hierarchical
data, yet traditional approaches rely on Euclidean metrics that fail to capture
the discrete, branching nature of hierarchical relationships. We present a
theoretical foundation for machine learning in p-adic metric spaces, which
naturally respect hierarchical structure. Our main result proves that an
n-dimensional plane minimizing the p-adic sum of distances to points in a
dataset must pass through at least n + 1 of those points -- a striking contrast
to Euclidean regression that highlights how p-adic metrics better align with
the discrete nature of hierarchical data. As a corollary, a polynomial of
degree n constructed to minimise the p-adic sum of residuals will pass through
at least n + 1 points. As a further corollary, a polynomial of degree n
approximating a higher degree polynomial at a finite number of points will
yield a difference polynomial that has distinct rational roots. We demonstrate
the practical significance of this result through two applications in natural
language processing: analyzing hierarchical taxonomies and modeling grammatical
morphology. These results suggest that p-adic metrics may be fundamental to
properly handling hierarchical data structures in machine learning. In
hierarchical data, interpolation between points often makes less sense than
selecting actual observed points as representatives.

</details>


### [181] [Federated Learning Meets LLMs: Feature Extraction From Heterogeneous Clients](https://arxiv.org/abs/2510.00065)
*Abdelrhman Gaber,Hassan Abd-Eltawab,Youssif Abuzied,Muhammad ElMahdy,Tamer ElBatt*

Main category: cs.LG

TL;DR: FedLLM-Align提出一种联邦学习框架，利用预训练大语言模型作为通用特征提取器，解决异构表格数据的联邦学习挑战，在保持隐私的同时提高性能并降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私敏感领域（如医疗、金融、物联网）具有吸引力，但客户间表格数据的高度异构性（不同模式和特征空间）阻碍了直接聚合，是其主要障碍。

Method: FedLLM-Align框架将表格记录序列化为文本，利用DistilBERT、ALBERT、RoBERTa和ClinicalBERT等大语言模型作为通用特征提取器，生成语义对齐的嵌入表示。这些嵌入支持在标准FedAvg协议下使用轻量级本地分类器，从而无需手动模式协调并保护原始数据隐私。

Result: 在模拟模式发散的冠心病预测任务（基于Framingham数据集）上，FedLLM-Align在所有客户设置和LLM骨干网络下，始终优于现有先进基线，F1-score最高提升0.25，通信成本降低65%。在极端模式发散下的压力测试显示其性能能优雅降级，而传统方法则完全崩溃。

Conclusion: FedLLM-Align为异构环境中的联邦学习提供了一个鲁棒、隐私保护且通信高效的解决方案。

Abstract: Federated learning (FL) enables collaborative model training without sharing
raw data, making it attractive for privacy-sensitive domains such as
healthcare, finance, and IoT. A major obstacle, however, is the heterogeneity
of tabular data across clients, where divergent schemas and incompatible
feature spaces prevent straightforward aggregation. To address this challenge,
we propose FedLLM-Align, a federated framework that leverages pre-trained large
language models (LLMs) as universal feature extractors. Tabular records are
serialized into text, and embeddings from models such as DistilBERT, ALBERT,
RoBERTa, and ClinicalBERT provide semantically aligned representations that
support lightweight local classifiers under the standard FedAvg protocol. This
approach removes the need for manual schema harmonization while preserving
privacy, since raw data remain strictly local. We evaluate FedLLM-Align on
coronary heart disease prediction using partitioned Framingham datasets with
simulated schema divergence. Across all client settings and LLM backbones, our
method consistently outperforms state-of-the-art baselines, achieving up to
+0.25 improvement in F1-score and a 65% reduction in communication cost. Stress
testing under extreme schema divergence further demonstrates graceful
degradation, unlike traditional methods that collapse entirely. These results
establish FedLLM-Align as a robust, privacy-preserving, and
communication-efficient solution for federated learning in heterogeneous
environments.

</details>


### [182] [Adaptive and Resource-efficient Agentic AI Systems for Mobile and Embedded Devices: A Survey](https://arxiv.org/abs/2510.00078)
*Sicong Liu,Weiye Wu,Xiangrui Xu,Teng Li,Bowen Pang,Bin Guo,Zhiwen Yu*

Main category: cs.LG

TL;DR: 本综述探讨了基础模型驱动的AI智能体在移动和边缘设备部署中面临的资源限制与实际应用需求之间的矛盾，并系统分析了自适应、资源高效的智能体AI系统。


<details>
  <summary>Details</summary>
Motivation: 基础模型（FMs）赋能的AI智能体在自治驾驶、机器人等领域展现巨大潜力，但其部署在移动和边缘设备时，面临内存、能耗、带宽和延迟等严峻资源限制，与应用所需的长期适应性和实时交互之间存在根本性矛盾。

Method: 本综述首次系统地刻画了自适应、资源高效的智能体AI系统。总结了弹性推理、测试时适应、动态多模态集成等使能技术及其在智能体AI应用中的实践。

Result: 识别了在精度-延迟-通信权衡以及分布偏移下维持鲁棒性方面的开放挑战。强调了算法-系统协同设计、认知适应和协同边缘部署的未来机遇。建立了可扩展、自适应、资源高效智能体AI的统一视角。

Conclusion: 本综述通过映射基础模型结构、认知和硬件资源，为可扩展、自适应、资源高效的智能体AI奠定了统一视角，有助于读者理解使能技术间的联系，并促进智能体智能融合的讨论。

Abstract: Foundation models have reshaped AI by unifying fragmented architectures into
scalable backbones with multimodal reasoning and contextual adaptation. In
parallel, the long-standing notion of AI agents, defined by the
sensing-decision-action loop, is entering a new paradigm: with FMs as their
cognitive core, agents transcend rule-based behaviors to achieve autonomy,
generalization, and self-reflection. This dual shift is reinforced by
real-world demands such as autonomous driving, robotics, virtual assistants,
and GUI agents, as well as ecosystem advances in embedded hardware, edge
computing, mobile deployment platforms, and communication protocols that
together enable large-scale deployment. Yet this convergence collides with
reality: while applications demand long-term adaptability and real-time
interaction, mobile and edge deployments remain constrained by memory, energy,
bandwidth, and latency. This creates a fundamental tension between the growing
complexity of FMs and the limited resources of deployment environments. This
survey provides the first systematic characterization of adaptive,
resource-efficient agentic AI systems. We summarize enabling techniques into
elastic inference, test-time adaptation, dynamic multimodal integration, and
agentic AI applications, and identify open challenges in balancing
accuracy-latency-communication trade-offs and sustaining robustness under
distribution shifts. We further highlight future opportunities in
algorithm-system co-design, cognitive adaptation, and collaborative edge
deployment. By mapping FM structures, cognition, and hardware resources, this
work establishes a unified perspective toward scalable, adaptive, and
resource-efficient agentic AI. We believe this survey can help readers to
understand the connections between enabling technologies while promoting
further discussions on the fusion of agentic intelligence and intelligent
agents.

</details>


### [183] [Approximately Unimodal Likelihood Models for Ordinal Regression](https://arxiv.org/abs/2510.00122)
*Ryoya Yamasaki*

Main category: cs.LG

TL;DR: 针对序数回归中真实条件概率分布部分非单峰导致的偏倚，本文提出近似单峰似然模型，实验验证其在序数数据建模和分类任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的单峰似然模型在处理部分真实条件概率分布非单峰的序数数据时存在偏倚，限制了其适用性。研究动机是减轻这种偏倚。

Method: 提出“近似单峰似然模型”，该模型能够表示单峰条件概率分布以及接近单峰的条件概率分布。

Result: 实验验证了所提出的模型在序数数据统计建模和序数回归任务中是有效的。

Conclusion: 所提出的近似单峰似然模型能够有效处理序数数据和序数回归任务，通过表示单峰及接近单峰的条件概率分布，缓解了现有模型在面对部分非单峰分布时的偏倚。

Abstract: Ordinal regression (OR, also called ordinal classification) is classification
of ordinal data, in which the underlying target variable is categorical and
considered to have a natural ordinal relation for the underlying explanatory
variable. A key to successful OR models is to find a data structure `natural
ordinal relation' common to many ordinal data and reflect that structure into
the design of those models. A recent OR study found that many real-world
ordinal data show a tendency that the conditional probability distribution
(CPD) of the target variable given a value of the explanatory variable will
often be unimodal. Several previous studies thus developed unimodal likelihood
models, in which a predicted CPD is guaranteed to become unimodal. However, it
was also observed experimentally that many real-world ordinal data partly have
values of the explanatory variable where the underlying CPD will be
non-unimodal, and hence unimodal likelihood models may suffer from a bias for
such a CPD. Therefore, motivated to mitigate such a bias, we propose
approximately unimodal likelihood models, which can represent up to a unimodal
CPD and a CPD that is close to be unimodal. We also verify experimentally that
a proposed model can be effective for statistical modeling of ordinal data and
OR tasks.

</details>


### [184] [BigBang-Proton Technical Report: Next-Word-Prediction is Scientific Multitask Learner](https://arxiv.org/abs/2510.00129)
*Hengkui Wu,Liujiang Liu,Jihua He,Qihao Wang,Keke Zhao,Shuyang Hu,Renle Fu,Dahao Liang,Lingyu Zeng,Bruce Liu,Yuan Liu,Jin Zhan,Jiaqiang Niu,Xinglong Jia,Yaqin Hu,Wenjun Ji,Panpan Chi,Ken Chen,Hengyuan Wu,Yingsi Xin,Yongfeng Zhu,Yuexin Wang,Manqi Ruan,Ningtao Bian,Xiaohua Wu,Weipeng Xu*

Main category: cs.LG

TL;DR: BigBang-Proton是一个统一的序列模型，通过跨领域科学任务预训练，实现了在多个科学计算任务上与专业模型媲美或超越的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在构建一个科学领域的多任务学习器，并验证语言引导的科学计算能否在保持多任务学习能力的同时，达到或超越特定科学模型的性能。

Method: BigBang-Proton是一个统一的序列自回归语言模型架构。其核心创新包括：理论-实验学习范式、二进制补丁编码（Binary Patch Encoding）以及蒙特卡洛注意力（Monte Carlo Attention）。模型通过在跨领域科学数据集和通用文本语料上进行下一词预测预训练，然后进行下游任务的微调和推理。

Result: 在多项科学任务中取得了显著成果：50位算术加法100%准确率；粒子物理射流标记与领先专业模型持平；原子间势模拟MAE与专业模型匹配；水质预测性能与传统时空模型相当；基因组建模性能超越基准。

Conclusion: 研究证明语言引导的科学计算能够匹配或超越特定科学模型的性能，同时保持多任务学习能力。作者进一步提出将其预训练扩展到宇宙规模，作为开发物质世界基础模型的关键一步。

Abstract: We introduce BigBang-Proton, a unified sequence-based architecture for
auto-regressive language modeling pretrained on cross-scale, cross-structure,
cross-discipline real-world scientific tasks to construct a scientific
multi-task learner. BigBang-Proton incorporates three fundamental innovations
compared to mainstream general-purpose LLMs: Theory-Experiment Learning
paradigm aligns large-scale numerical experimental data with theoretical text
corpora; Binary Patch Encoding replaces byte pair encoding(BPE) tokenization;
Monte Carlo Attention substitutes traditional transformer architectures.
Through next-word-prediction pretraining on cross-discipline scientific
datasets of real-world problems mixed with general textual corpus, followed by
fine-tuning and inference on downstream tasks, BigBang-Proton demonstrates
100\% accuracy in up to 50-digit arithmetic addition operations, performance on
par with leading specialized models in particle physics jet tagging, matching
MAE of specialized models in inter-atomic potential simulation, performance
comparable to traditional spatiotemporal models in water quality prediction,
and benchmark-exceeding performance in genome modeling. These results prove
that language-guided scientific computing can match or exceed the performance
of task-specific scientific models while maintaining multitask learning
capabilities. We further hypothesize to scale the pretraining to the universe
scale as a fundamental step toward developing material world foundational
model.

</details>


### [185] [Large Language Models Inference Engines based on Spiking Neural Networks](https://arxiv.org/abs/2510.00133)
*Adarsha Balaji,Sandeep Madireddy*

Main category: cs.LG

TL;DR: 本文提出NeurTransformer，一种结合SNN与Transformer的方法，通过替换自注意力机制和微调，旨在解决传统Transformer模型的计算开销和现有SNN转换方法的延迟问题，并展示了其准确性和显著的能效提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的语言模型计算复杂度与输入序列长度呈平方关系，导致训练和部署计算开销巨大。同时，大规模SNN的训练效率低下，而将Transformer模型转换为SNN的方法又不具备扩展性，会导致高延迟。

Method: 本文提出NeurTransformer方法，用于设计基于SNN的Transformer模型进行推理，采用有监督微调结合现有转换方法。具体步骤包括：1) 将自注意力机制替换为基于脉冲的自注意力（SSA）；2) 将训练好的Transformer模型的前馈块转换为SNN等效形式；3) 使用基于SNN的代理学习算法微调SSA块。

Result: 使用GPT-2模型的三种变体进行基准测试，结果显示转换后的GPT-2小型模型在余弦相似度上损失5-12%，困惑度降低9.7%。此外，与ASA块相比，SSA块在数字硬件上实现自注意力机制时，估计能耗降低了64.71%至85.28%。

Conclusion: NeurTransformer提供了一种有效设计基于SNN的Transformer模型的方法，解决了现有模型的计算和能效挑战。该方法在保持合理准确性的前提下，显著提升了能源效率，特别是在自注意力机制方面。

Abstract: Foundational models based on the transformer architecture are currently the
state-of-the-art in general language modeling, as well as in scientific areas
such as material science and climate. However, training and deploying these
models is computationally challenging as the time and space complexity has a
quadratic relation to the input sequence length. Several efforts exploring
efficient computational paradigms and model architectures to address these
limitations have been made. In this work, we explore spiking neural networks
(SNNs) to design transformer models. A challenge in training large-scale SNNs,
using existing surrogate learning methods is inefficient and time-consuming. On
the other hand, techniques to convert existing transformer-based models to
their SNN equivalent are not scalable, as achieving optimal performance comes
at the cost of a large number of spike time-steps, i.e. increased latency. To
address this, we propose NeurTransformer, a methodology for designing
transformer-based SNN for inference using a supervised fine-tuning approach
with existing conversion methods. The proposed methodology works by: (1)
replacing the self-attention mechanism with a spike-based self-attention (SSA),
(2) converting the feed-forward block of the trained transformer model to its
equivalent SNN, and (3) fine-tuning the SSA block using SNN-based surrogate
learning algorithms. We benchmark the proposed methodology and demonstrate its
accuracy and scalability using three variants of the GPT-2 model of increasing
model size. We observe that the converted GPT-2 small models demonstrate a
5-12% loss in cosine similarity and a 9.7% reduction in perplexity. Finally, we
demonstrate the energy efficiency of the SSA block compared to the ASA block
and show between 64.71% and 85.28% reductions in estimated energy consumption
when implementing the self-attention mechanism on a digital hardware.

</details>


### [186] [Nonparametric Identification of Latent Concepts](https://arxiv.org/abs/2510.00136)
*Yujia Zheng,Shaoan Xie,Kun Zhang*

Main category: cs.LG

TL;DR: 本文提出一个基于比较机制的理论框架，旨在为机器概念学习提供可识别性保证，克服了现有理论支持不足的问题，并证明了在多样化观测下隐藏概念的可识别性。


<details>
  <summary>Details</summary>
Motivation: 尽管概念学习在经验上取得了显著成功，但普遍缺乏理论支持。作者认为，人类通过比较多样化观察来学习概念的认知机制，对于机器恢复数据中真实的潜在概念至关重要，能够为概念学习提供正确性保证。

Method: 开发了一个理论框架，用于识别具有多类别观测的隐藏概念。该方法的核心是通过利用不同观测类别之间的多样性进行比较，从而实现概念的识别。

Result: 1. 在类别间有足够多样性的情况下，无需假设特定的概念类型、函数关系或参数生成模型，即可识别隐藏概念。
2. 即使全局条件不完全满足，也能通过局部比较为尽可能多的概念提供识别保证，从而扩展了理论的适用性。
3. 类别与概念之间的隐藏结构也可以非参数方式识别。
4. 理论结果已通过合成数据和真实世界数据进行了验证。

Conclusion: 本研究通过引入比较机制，为概念学习的理论正确性提供了坚实基础，证明了隐藏概念及其结构在广泛条件下是可识别的。这弥补了该领域在理论支持方面的不足，并展现了其在多种场景下的应用潜力。

Abstract: We are born with the ability to learn concepts by comparing diverse
observations. This helps us to understand the new world in a compositional
manner and facilitates extrapolation, as objects naturally consist of multiple
concepts. In this work, we argue that the cognitive mechanism of comparison,
fundamental to human learning, is also vital for machines to recover true
concepts underlying the data. This offers correctness guarantees for the field
of concept learning, which, despite its impressive empirical successes, still
lacks general theoretical support. Specifically, we aim to develop a
theoretical framework for the identifiability of concepts with multiple classes
of observations. We show that with sufficient diversity across classes, hidden
concepts can be identified without assuming specific concept types, functional
relations, or parametric generative models. Interestingly, even when conditions
are not globally satisfied, we can still provide alternative guarantees for as
many concepts as possible based on local comparisons, thereby extending the
applicability of our theory to more flexible scenarios. Moreover, the hidden
structure between classes and concepts can also be identified
nonparametrically. We validate our theoretical results in both synthetic and
real-world settings.

</details>


### [187] [Which Rewards Matter? Reward Selection for Reinforcement Learning under Limited Feedback](https://arxiv.org/abs/2510.00144)
*Shreyas Chaudhari,Renhao Zhang,Philip S. Thomas,Bruno Castro da Silva*

Main category: cs.LG

TL;DR: 针对强化学习中奖励标签稀缺问题，本文提出了有限反馈下的奖励选择(RLLF)问题，并通过选择关键奖励，实现了用更少标签获得接近最优策略。


<details>
  <summary>Details</summary>
Motivation: 强化学习依赖大量奖励，但实际获取成本高昂。在反馈有限时，如何高效选择样本进行奖励标注以最大化策略性能是核心问题。

Method: 形式化了有限反馈下的奖励选择(RLLF)问题。研究了两类选择策略：一是基于无奖励信息的启发式方法（如状态访问、部分价值函数），二是基于辅助评估反馈预训练的策略。

Result: 发现关键奖励是那些能引导智能体沿最优轨迹前进，并在偏离后支持其恢复到接近最优行为的奖励。有效选择方法能以显著少于全监督的奖励标签，获得接近最优的策略。

Conclusion: 奖励选择是一种在反馈受限环境下扩展强化学习的强大范式。

Abstract: The ability of reinforcement learning algorithms to learn effective policies
is determined by the rewards available during training. However, for practical
problems, obtaining large quantities of reward labels is often infeasible due
to computational or financial constraints, particularly when relying on human
feedback. When reinforcement learning must proceed with limited feedback --
only a fraction of samples get rewards labeled -- a fundamental question
arises: which samples should be labeled to maximize policy performance? We
formalize this problem of reward selection for reinforcement learning from
limited feedback (RLLF), introducing a new problem formulation that facilitates
the study of strategies for selecting impactful rewards. Two types of selection
strategies are investigated: (i) heuristics that rely on reward-free
information such as state visitation and partial value functions, and (ii)
strategies pre-trained using auxiliary evaluative feedback. We find that
critical subsets of rewards are those that (1) guide the agent along optimal
trajectories, and (2) support recovery toward near-optimal behavior after
deviations. Effective selection methods yield near-optimal policies with
significantly fewer reward labels than full supervision, establishing reward
selection as a powerful paradigm for scaling reinforcement learning in
feedback-limited settings.

</details>


### [188] [Partial Identification Approach to Counterfactual Fairness Assessment](https://arxiv.org/abs/2510.00163)
*Saeyoung Rho,Junzhe Zhang,Elias Bareinboim*

Main category: cs.LG

TL;DR: 针对AI决策系统中的算法公平性问题，尤其是在反事实公平性度量难以评估且常不可识别的挑战，本文提出一种贝叶斯偏识别方法，从观测数据推导反事实公平性度量的置信区间。在COMPAS数据集上的应用揭示了种族（非裔美国人）对分数有正向虚假影响，年龄增长有负向直接因果影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI决策系统在刑事司法、贷款审批和招聘等关键领域的广泛应用，算法公平性问题日益受到关注。研究界提出了反事实公平性度量，但如何从可用数据中评估这些度量仍是挑战。在许多实际应用中，目标反事实度量是不可识别的，即无法从定量数据和定性知识的组合中唯一确定。

Method: 本文利用偏识别方法应对挑战，从观测数据推导反事实公平性度量的有效边界。具体地，引入了一种贝叶斯方法，以高置信度界定未知的反事实公平性度量。

Result: 在COMPAS数据集上，分析了累犯风险分数在种族、年龄和性别方面的公平性。结果表明，当种族变为非裔美国人时（相对于其他种族），COMPAS分数存在正向（虚假）影响；而当从年轻过渡到年长时，则存在负向（直接因果）影响。

Conclusion: 本文成功提出了一个贝叶斯偏识别方法，解决了反事实公平性度量在观测数据中不可识别的挑战，能够从有限信息中推导出有信息量的公平性边界。该方法在COMPAS数据集上的应用揭示了潜在的算法偏见，为评估AI系统公平性提供了实用工具。

Abstract: The wide adoption of AI decision-making systems in critical domains such as
criminal justice, loan approval, and hiring processes has heightened concerns
about algorithmic fairness. As we often only have access to the output of
algorithms without insights into their internal mechanisms, it was natural to
examine how decisions would alter when auxiliary sensitive attributes (such as
race) change. This led the research community to come up with counterfactual
fairness measures, but how to evaluate the measure from available data remains
a challenging task. In many practical applications, the target counterfactual
measure is not identifiable, i.e., it cannot be uniquely determined from the
combination of quantitative data and qualitative knowledge. This paper
addresses this challenge using partial identification, which derives
informative bounds over counterfactual fairness measures from observational
data. We introduce a Bayesian approach to bound unknown counterfactual fairness
measures with high confidence. We demonstrate our algorithm on the COMPAS
dataset, examining fairness in recidivism risk scores with respect to race,
age, and sex. Our results reveal a positive (spurious) effect on the COMPAS
score when changing race to African-American (from all others) and a negative
(direct causal) effect when transitioning from young to old age.

</details>


### [189] [Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls](https://arxiv.org/abs/2510.00184)
*Xiaoyan Bai,Itamar Pres,Yuntian Deng,Chenhao Tan,Stuart Shieber,Fernanda Viégas,Martin Wattenberg,Andrew Lee*

Main category: cs.LG

TL;DR: 本研究通过逆向工程一个成功学习多位数乘法的模型，揭示了语言模型在该任务上失败的原因，发现了其处理长程依赖的机制，并提出通过引入辅助损失提供正确归纳偏置以解决问题。


<details>
  <summary>Details</summary>
Motivation: 语言模型在多位数乘法这一看似简单的任务上表现不佳，研究动机在于通过逆向工程来理解其失败的深层原因。

Method: 逆向工程一个通过隐式思维链成功学习多位数乘法的模型。使用Logit归因和线性探针分析长程结构；研究注意力机制如何编码依赖并构建有向无环图来缓存/检索部分乘积；探究部分乘积的几何实现（闵可夫斯基和、傅里叶基）。通过引入一个辅助损失（通过线性回归探针预测“运行和”）来提供归纳偏置并验证理解。

Result: (1) 模型编码了多位数乘法所需的关键长程依赖结构。(2) 机制上，模型通过注意力机制构建有向无环图来缓存和检索成对的部分乘积。(3) 几何上，模型使用闵可夫斯基和实现部分乘积，并以傅里叶基表示数字。研究发现，标准微调模型会收敛到缺乏所需长程依赖的局部最优解。引入预测“运行和”的辅助损失，能提供正确的归纳偏置，使模型成功学习多位数乘法。

Conclusion: 通过逆向工程揭示了Transformer在学习长程依赖时的陷阱，并提供了一个示例，说明正确的归纳偏置（通过辅助损失）如何有效解决这一问题，从而使模型能够成功学习多位数乘法。

Abstract: Language models are increasingly capable, yet still fail at a seemingly
simple task of multi-digit multiplication. In this work, we study why, by
reverse-engineering a model that successfully learns multiplication via
\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of
long-range structure: Logit attributions and linear probes indicate that the
model encodes the necessary long-range dependencies for multi-digit
multiplication. (2) Mechanism: the model encodes long-range dependencies using
attention to construct a directed acyclic graph to ``cache'' and ``retrieve''
pairwise partial products. (3) Geometry: the model implements partial products
in attention heads by forming Minkowski sums between pairs of digits, and
digits are represented using a Fourier basis, both of which are intuitive and
efficient representations that the standard fine-tuning model lacks. With these
insights, we revisit the learning dynamics of standard fine-tuning and find
that the model converges to a local optimum that lacks the required long-range
dependencies. We further validate this understanding by introducing an
auxiliary loss that predicts the ``running sum'' via a linear regression probe,
which provides an inductive bias that enables the model to successfully learn
multi-digit multiplication. In summary, by reverse-engineering the mechanisms
of an implicit chain-of-thought model we uncover a pitfall for learning
long-range dependencies in Transformers and provide an example of how the
correct inductive bias can address this issue.

</details>


### [190] [PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning](https://arxiv.org/abs/2510.00192)
*Xin Yu,Cong Xie,Ziyu Zhao,Tiantian Fan,Lingzhou Xue,Zhi Zhang*

Main category: cs.LG

TL;DR: PrunedLoRA通过结构化剪枝从过参数化空间动态生成表达力更强的低秩适配器，解决了LoRA容量受限的问题，并在多项任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 低秩适配（LoRA）在参数高效微调中应用广泛，但其表示能力通常不如全量微调。如何在LoRA框架下，从过参数化空间中获取表达力强的低秩适配器，以及现有方法固定低秩预算的局限性，是亟待解决的关键问题。

Method: 提出PrunedLoRA框架，从过参数化初始化中利用结构化剪枝动态获取低秩适配器。它在微调期间剪枝不重要的组件并防止其重新激活，实现灵活的秩分配。通过最小化整体损失的剪枝误差，采用基于梯度的剪枝策略进行细粒度剪枝和恢复更新。首次对结构化剪枝的鲁棒性进行理论分析，证明了在权重扰动下，基于梯度的剪枝相对于整体损失比基于激活的剪枝更具鲁棒性。

Result: 首次提供了结构化剪枝鲁棒性的理论分析，并证明了基于梯度的剪枝在权重扰动下对整体损失的鲁棒性优于基于激活的剪枝。经验上，PrunedLoRA在数学推理、代码生成和自然语言理解的监督微调任务中持续优于LoRA及其变体，并且在不同稀疏度下也优于现有的结构化剪枝方法。

Conclusion: PrunedLoRA通过动态剪枝过参数化适配器，有效提升了LoRA的表示能力和性能，在多项任务上取得了超越基线和现有剪枝方法的优秀结果，并提供了坚实的理论支持。

Abstract: Low-rank adaptation (LoRA) has become a widely used paradigm for
parameter-efficient fine-tuning of large language models, yet its
representational capacity often lags behind full fine-tuning. Within the
context of LoRA, a key open question is how to obtain expressive low-rank
adapters from over-parameterized spaces. We propose \textit{PrunedLoRA}, a new
framework that leverages structured pruning to obtain highly representative
low-rank adapters from an over-parameterized initialization. Unlike prior
approaches that impose a fixed low-rank budget, PrunedLoRA dynamically prunes
less important components during fine-tuning and prevents their reactivation,
enabling flexible and adaptive rank allocation. For structured pruning, by
minimizing the pruning error for overall loss, we provide fine-grained pruning
and recovery updates in a gradient-based pruning strategy with grounded
interpretation. We provide the first theoretical analysis of the robustness of
structured pruning and provably show that under the impact of weight
perturbation, gradient-based pruning is more robust than activation-based
pruning with respect to overall loss. Empirically, PrunedLoRA consistently
outperforms LoRA and its variants across supervised fine-tuning tasks in
mathematical reasoning, code generation, and natural language understanding,
and it also demonstrates advantages over existing structured pruning methods
across diverse sparsity levels.

</details>


### [191] [GRPO-$λ$: Credit Assignment improves LLM Reasoning](https://arxiv.org/abs/2510.00194)
*Prasanna Parthasarathi,Mathieu Reymond,Boxing Chen,Yufei Cui,Sarath Chandar*

Main category: cs.LG

TL;DR: 本文提出GRPO-λ，作为GRPO的扩展，通过改进信用分配机制，显著提升了LLMs在复杂推理任务上的RL微调性能，特别是在数学推理数据集上显示出30-40%的训练性能提升及最终3-4.5点的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在复杂推理任务中得到广泛应用，基于可验证奖励的RL方法（如GRPO）被证明能有效提升其推理能力。然而，GRPO缺乏明确的奖励或评论模型，限制了其在token序列上进行细粒度信用分配的能力。

Method: 引入GRPO-λ，作为GRPO的创新性扩展，旨在增强RL微调中LLMs的信用分配。通过在每个序列生成后，利用token级别的对数概率重新构建资格迹（eligibility traces），来近似从λ-回报中学习，并提出了一种新颖的无评论员（critic-free）时序差分误差近似方法。GRPO-λ还引入了几种λ-回报加权变体及其在资格迹中的应用。

Result: GRPO-λ的所有变体都比GRPO提供了显著的性能提升。在1.5B到7B参数的模型上，针对4个不同的数学推理数据集进行比较训练，结果显示，在LLaMA-3.1和Qwen-2.5架构上，RL训练期间的性能提升了30-40%。最终，在AIME24、Math500、OlympiadMath、MinervaMath和AMC等数据集上的平均性能比GRPO提升了3点以上，在7B模型上更是提升了4.5点。

Conclusion: GRPO-λ通过改进信用分配机制，有效克服了GRPO的局限性，在LLMs的RL微调中取得了显著成功。它在复杂数学推理任务中表现出优于GRPO的强大性能，尤其对于大型模型（如7B参数）的提升更为显著，为LLMs推理能力的进一步发展提供了新途径。

Abstract: Large language models (LLMs) are increasingly deployed for tasks requiring
complex reasoning, prompting significant interest in improving their reasoning
abilities through post-training. Especially RL based methods using verifiable
reward, like the state-of-the-art GRPO, have shown to tremendously improve
reasoning behaviors when applied as post-training methods. However, the lack of
an explicit reward or critic model limits GRPO's ability to assign fine-grained
credit across token sequences. In this work, we present GRPO-$\lambda$, a novel
extension to GRPO that enhances credit assignment in RL finetuning of LLMs for
complex reasoning tasks. We approximate learning from $\lambda$-return with a
reformulation of eligibility traces using token-level log-probabilities applied
after each sequence generation, and a novel critic-free approximation of the
temporal-difference error. We introduce a few variations for the weighting of
the $\lambda$-return, and their applications to the eligibility-trace, where
all the variations provide significant gains over GRPO. We compare
GRPO-$\lambda$ against GRPO by training models from 1.5B to 7B parameters on
$4$ different math reasoning datasets. The training plots demonstrate 30-40%
improved performance during RL training on both LLaMA-3.1 and Qwen-2.5
architectures. Finally, we show that with GRPO-$\lambda$, the resulting average
performance on AIME24, Math500, OlympiadMath, MinervaMath, and AMC improves
over GRPO by over $3$ points and a $4.5$ points improvement on the 7B model.

</details>


### [192] [RouterArena: An Open Platform for Comprehensive Comparison of LLM Routers](https://arxiv.org/abs/2510.00202)
*Yifan Lu,Rixin Liu,Jiayi Yuan,Xingqi Cui,Shenrun Zhang,Hongyi Liu,Jiarong Xing*

Main category: cs.LG

TL;DR: 针对LLM路由器选择困难的问题，本文提出了RouterArena，一个开放平台，用于对LLM路由器进行全面比较并提供标准化排行榜。


<details>
  <summary>Details</summary>
Motivation: LLM生态系统模型多样，需要LLM路由器来选择最佳模型。然而，路由器种类繁多，选择合适的路由器变得愈发困难，缺乏类似模型排行榜的标准化比较平台。

Method: 引入RouterArena平台，该平台具有：1) 涵盖广泛知识领域的原则性构建数据集；2) 每个领域可区分的难度级别；3) 广泛的评估指标列表；4) 自动化的排行榜更新框架。

Result: 利用RouterArena框架，已生成了初步的LLM路由器排行榜，展示了详细的指标比较。

Conclusion: RouterArena是首个开放平台，旨在解决LLM路由器选择难题，提供全面比较和标准化排行榜，并将向公众开放。

Abstract: Today's LLM ecosystem comprises a wide spectrum of models that differ in
size, capability, and cost. No single model is optimal for all scenarios;
hence, LLM routers have become essential for selecting the most appropriate
model under varying circumstances. However, the rapid emergence of various
routers makes choosing the right one increasingly challenging. To address this
problem, we need a comprehensive router comparison and a standardized
leaderboard, similar to those available for models. In this work, we introduce
RouterArena, the first open platform enabling comprehensive comparison of LLM
routers. RouterArena has (1) a principally constructed dataset with broad
knowledge domain coverage, (2) distinguishable difficulty levels for each
domain, (3) an extensive list of evaluation metrics, and (4) an automated
framework for leaderboard updates. Leveraging our framework, we have produced
the initial leaderboard with detailed metrics comparison as shown in Figure 1.
We will make our platform open to the public soon.

</details>


### [193] [LoRAFusion: Efficient LoRA Fine-Tuning for LLMs](https://arxiv.org/abs/2510.00206)
*Zhanda Zhu,Qidong Su,Yaoyao Ding,Kevin Song,Shang Wang,Gennady Pekhimenko*

Main category: cs.LG

TL;DR: LoRAFusion是一种高效的LoRA微调系统，通过内核层面的内存操作融合和调度层面的自适应批处理，解决了现有LoRA系统中的运行时开销和多任务并发微调效率低下的问题，显著提升了LLMs的微调速度。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA微调系统存在两个主要低效率问题：一是因冗余内存访问导致大量的运行时开销；二是无法有效并发微调多个共享基础模型的LoRA适配器，导致错过性能提升机会，如减少流水线停顿、更好的通信重叠和GPU负载均衡。

Method: LoRAFusion从两个层面解决问题：
1. **内核层面**：提出图分割方法，融合内存密集型操作，消除不必要的内存访问，同时保持计算密集型GEMM的性能，无需重新计算或同步。
2. **调度层面**：引入多任务微调的自适应批处理算法。它首先将LoRA适配器分成组，错开不同任务的批处理执行，然后通过解决每个组内的装箱问题来生成平衡且感知依赖的微批次。

Result: LoRAFusion取得了显著的性能提升：
* 相较于Megatron-LM，端到端加速高达1.96倍（平均1.47倍）。
* 相较于现有最先进的多LoRA微调系统mLoRA，提升高达1.46倍（平均1.29倍）。
* 融合内核性能提升高达1.39倍（平均1.27倍），可作为现有LoRA系统的即插即用替代品。

Conclusion: LoRAFusion通过创新的内核融合和自适应调度机制，显著提升了大型语言模型LoRA微调的效率，有效解决了内存访问开销和多任务并发的挑战，为LLMs的PEFT提供了更高效的解决方案。

Abstract: Low-Rank Adaptation (LoRA) has become the leading Parameter-Efficient
Fine-Tuning (PEFT) method for Large Language Models (LLMs), as it significantly
reduces GPU memory usage while maintaining competitive fine-tuned model quality
on downstream tasks. Despite these benefits, we identify two key inefficiencies
in existing LoRA fine-tuning systems. First, they incur substantial runtime
overhead due to redundant memory accesses on large activation tensors. Second,
they miss the opportunity to concurrently fine-tune multiple independent LoRA
adapters that share the same base model on the same set of GPUs. This leads to
missed performance gains such as reduced pipeline bubbles, better communication
overlap, and improved GPU load balance.
  To address these issues, we introduce LoRAFusion, an efficient LoRA
fine-tuning system for LLMs. At the kernel level, we propose a graph-splitting
method that fuses memory-bound operations. This design eliminates unnecessary
memory accesses and preserves the performance of compute-bound GEMMs without
incurring the cost of recomputation or synchronization. At the scheduling
level, LoRAFusion introduces an adaptive batching algorithm for multi-job
fine-tuning. It first splits LoRA adapters into groups to intentionally stagger
batch execution across jobs, and then solves a bin-packing problem within each
group to generate balanced, dependency-aware microbatches. LoRAFusion achieves
up to $1.96\times$ ($1.47\times$ on average) end-to-end speedup compared to
Megatron-LM, and up to $1.46\times$ ($1.29\times$ on average) improvement over
mLoRA, the state-of-the-art multi-LoRA fine-tuning system. Our fused kernel
achieves up to $1.39\times$ ($1.27\times$ on average) kernel performance
improvement and can directly serve as a plug-and-play replacement in existing
LoRA systems. We open-source LoRAFusion at
https://github.com/CentML/lorafusion.

</details>


### [194] [Directed-MAML: Meta Reinforcement Learning Algorithm with Task-directed Approximation](https://arxiv.org/abs/2510.00212)
*Yang Zhang,Huiwen Yan,Mushuang Liu*

Main category: cs.LG

TL;DR: MAML在元强化学习中面临计算和收敛挑战。本文提出Directed-MAML，通过引入额外的任务导向一阶近似来估计二阶梯度影响，显著提升计算效率和收敛速度，并可推广到其他元学习算法。


<details>
  <summary>Details</summary>
Motivation: 将Model-Agnostic Meta-Learning (MAML)应用于元强化学习 (meta-RL) 时，存在显著挑战：1. 依赖二阶梯度计算，导致高昂的计算和内存开销；2. 优化嵌套结构增加了问题复杂性，使得收敛到全局最优更具挑战性。

Method: 本文提出Directed-MAML，一种新型任务导向的元强化学习算法。在二阶梯度步骤之前，Directed-MAML应用额外的任务导向一阶近似来估计二阶梯度的影响，从而加速收敛并降低计算成本。此外，这种任务导向近似可以有效集成到其他元学习算法（如FOMAML和Meta-SGD）中。

Result: 实验结果表明，在CartPole-v1、LunarLander-v2和双车交叉路口场景中，Directed-MAML在计算效率和收敛速度上均超越了基于MAML的基线方法。此外，任务导向近似能够有效提高FOMAML和Meta-SGD的计算效率和收敛速度。

Conclusion: Directed-MAML通过引入任务导向的一阶近似，成功克服了MAML在元强化学习中面临的计算开销大和收敛慢的问题，显著提升了算法性能。该近似方法还具有良好的泛化性，可用于改进其他元学习算法。

Abstract: Model-Agnostic Meta-Learning (MAML) is a versatile meta-learning framework
applicable to both supervised learning and reinforcement learning (RL).
However, applying MAML to meta-reinforcement learning (meta-RL) presents
notable challenges. First, MAML relies on second-order gradient computations,
leading to significant computational and memory overhead. Second, the nested
structure of optimization increases the problem's complexity, making
convergence to a global optimum more challenging. To overcome these
limitations, we propose Directed-MAML, a novel task-directed meta-RL algorithm.
Before the second-order gradient step, Directed-MAML applies an additional
first-order task-directed approximation to estimate the effect of second-order
gradients, thereby accelerating convergence to the optimum and reducing
computational cost. Experimental results demonstrate that Directed-MAML
surpasses MAML-based baselines in computational efficiency and convergence
speed in the scenarios of CartPole-v1, LunarLander-v2 and two-vehicle
intersection crossing. Furthermore, we show that task-directed approximation
can be effectively integrated into other meta-learning algorithms, such as
First-Order Model-Agnostic Meta-Learning (FOMAML) and Meta Stochastic Gradient
Descent(Meta-SGD), yielding improved computational efficiency and convergence
speed.

</details>


### [195] [Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space](https://arxiv.org/abs/2510.00219)
*Houjun Liu,Shikhar Murty,Christopher D. Manning,Róbert Csordás*

Main category: cs.LG

TL;DR: 提出Thoughtbubbles，一种Transformer变体，通过在潜空间并行自适应计算来提高推理效率，且能在预训练阶段学习，克服了现有方法的局限性并取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer推理计算扩展方法（如思维链）存在局限，无法在预训练阶段应用，且仅限于串行、自然语言的表达来扩展推理计算。

Method: 提出Thoughtbubbles，一种Transformer变体。它通过学习分叉或删除残差流，在潜在空间原生执行并行自适应计算。需要大量计算的token可在网络中间形成“气泡”进行额外思考。该行为仅使用语言建模损失在预训练阶段学习。

Result: Thoughtbubbles在OpenWebText和peS2o困惑度以及HellaSwag和LAMBADA等零样本评估中，优于标准解码器语言模型和非自适应并行计算方法（在150M至772M参数规模下）。

Conclusion: 该方法的隐式特性使得自适应计算能够在预训练阶段学习，为统一推理模型的训练和测试行为铺平了道路。

Abstract: Current approaches for scaling inference-time compute in transformers rely on
training them to emit explicit chain-of-thought tokens before producing an
answer. While these methods are powerful, they are limited because they cannot
be applied during pretraining and are limited to only serially-generated,
natural-language verbalization to scale inference-time compute. In this work,
we propose Thoughtbubbles, a transformer variant that natively performs
parallel adaptive computation in latent space by learning to fork or delete
residual streams. Thus, tokens that require a large amount of computation can
form a "bubble" of cloned residuals in the middle of the network for additional
thinking. Crucially, this behavior is learned during pretraining with only
language modeling loss. Thoughtbubbles outperforms both standard decoder LMs as
well as non-adaptive parallel computation approaches on OpenWebText and peS2o
perplexity and in zero-shot evaluations such as HellaSwag and LAMBADA after
pretraining across 150M to 772M parameter scales. The implicit nature of our
method enables adaptive computation to be learned starting at pretraining time,
paving the way to unify train and test-time behavior for reasoning models.

</details>


### [196] [The Pitfalls of KV Cache Compression](https://arxiv.org/abs/2510.00231)
*Alex Chen,Renato Geh,Aditya Grover,Guy Van den Broeck,Daniel Israel*

Main category: cs.LG

TL;DR: 研究发现KV缓存压缩在多指令提示等实际场景中存在未充分研究的缺陷，导致特定指令快速退化甚至被忽略，例如系统提示泄露。论文分析了压缩方法、指令顺序和KV淘汰偏差等因素的影响，并提出了改进的KV缓存淘汰策略以提升多指令任务性能。


<details>
  <summary>Details</summary>
Motivation: 尽管KV缓存压缩在提高吞吐量和效率方面有明显优势，但在多指令提示等实际场景中，其对性能的影响（特别是指令遵循）尚未得到充分研究。研究旨在识别并解决压缩LLM部署中可能出现的实际问题，如指令退化和系统提示泄露。

Method: 通过识别部署KV缓存压缩LLM时应注意的几个陷阱，实证分析了压缩对指令遵循和系统提示泄露的影响。研究探讨了压缩方法、指令顺序和KV淘汰偏差在提示泄露中的作用。在此基础上，提出了对KV缓存淘汰策略的简单修改。

Result: 研究表明，某些指令在压缩下会迅速退化，导致LLM完全忽略它们。以系统提示泄露为例，实证展示了压缩对泄露和一般指令遵循的影响。发现压缩方法、指令顺序和KV淘汰偏差是影响提示泄露的关键因素。所提出的简单KV缓存淘汰策略可以减轻这些因素的影响，并在多指令任务中提高整体性能。

Conclusion: KV缓存压缩可能导致多指令提示中特定指令的严重退化和被忽略，增加如系统提示泄露等风险。通过调整KV缓存淘汰策略，可以有效减少这些负面影响，提升LLM在复杂多指令任务中的表现。

Abstract: KV cache compression promises increased throughput and efficiency with
negligible loss in performance. While the gains in throughput are indisputable
and recent literature has indeed shown minimal degradation on particular
benchmarks, in general the consequences of compression in realistic scenarios
such as multi-instruction prompting have been insufficiently studied. In this
paper, we identify several pitfalls practitioners should be aware of when
deploying KV cache compressed LLMs. Importantly, we show that certain
instructions degrade much more rapidly with compression, effectively causing
them to be completely ignored by the LLM. As a practical example of that, we
highlight system prompt leakage as a case study, empirically showing the impact
of compression on leakage and general instruction following. We show several
factors that play a role in prompt leakage: compression method, instruction
order, and KV eviction bias. We then propose simple changes to KV cache
eviction policies that can reduce the impact of these factors and improve the
overall performance in multi-instruction tasks.

</details>


### [197] [Differentiable Autoencoding Neural Operator for Interpretable and Integrable Latent Space Modeling](https://arxiv.org/abs/2510.00233)
*Siva Viknesh,Amirhossein Arzani*

Main category: cs.LG

TL;DR: DIANO是一个自编码神经算子框架，用于构建物理可解释的潜在空间，并能直接在潜在空间中求解微分方程。


<details>
  <summary>Details</summary>
Motivation: 尽管科学机器学习在从高维流数据中提取物理洞察和降维方面取得了进展，但如何在降维后的潜在空间中实现可解释性仍然是一个挑战。

Method: 本文提出了可微分自编码神经算子（DIANO），一个确定性自编码神经算子框架。DIANO通过编码神经算子进行空间粗化将高维输入函数压缩到低维潜在空间，再通过解码神经算子进行空间细化重建原始输入。它允许在潜在空间中直接强制执行微分控制方程，并集成了完全可微分的PDE求解器，以在潜在动力学中嵌入物理先验。研究了2D非稳态平流-扩散和3D压力-泊冲方程，并应用于2D圆柱绕流、2D对称狭窄动脉流和3D患者特异性冠状动脉流等基准问题。

Result: DIANO在潜在空间可解释性和降维性能方面优于基线模型（包括卷积神经算子和标准自编码器）。案例研究表明，DIANO能够在潜在空间中求解PDE，同时实现维度和几何降维，并保持潜在可解释性。

Conclusion: DIANO提供了一个在物理可解释潜在空间中解决PDE的有效方法，该方法实现了维度和几何降维，并能够嵌入物理先验。

Abstract: Scientific machine learning has enabled the extraction of physical insights
from high-dimensional spatiotemporal flow data using linear and nonlinear
dimensionality reduction techniques. Despite these advances, achieving
interpretability within the latent space remains a challenge. To address this,
we propose the DIfferentiable Autoencoding Neural Operator (DIANO), a
deterministic autoencoding neural operator framework that constructs physically
interpretable latent spaces for both dimensional and geometric reduction, with
the provision to enforce differential governing equations directly within the
latent space. Built upon neural operators, DIANO compresses high-dimensional
input functions into a low-dimensional latent space via spatial coarsening
through an encoding neural operator and subsequently reconstructs the original
inputs using a decoding neural operator through spatial refinement. We assess
DIANO's latent space interpretability and performance in dimensionality
reduction against baseline models, including the Convolutional Neural Operator
and standard autoencoders. Furthermore, a fully differentiable partial
differential equation (PDE) solver is developed and integrated within the
latent space, enabling the temporal advancement of both high- and low-fidelity
PDEs, thereby embedding physical priors into the latent dynamics. We further
investigate various PDE formulations, including the 2D unsteady
advection-diffusion and the 3D Pressure-Poisson equation, to examine their
influence on shaping the latent flow representations. Benchmark problems
considered include flow past a 2D cylinder, flow through a 2D symmetric
stenosed artery, and a 3D patient-specific coronary artery. These case studies
demonstrate DIANO's capability to solve PDEs within a latent space that
facilitates both dimensional and geometrical reduction while allowing latent
interpretability.

</details>


### [198] [Per-example gradients: a new frontier for understanding and improving optimizers](https://arxiv.org/abs/2510.00236)
*Vincent Roulet,Atish Agarwala*

Main category: cs.LG

TL;DR: 本文展示了在自动微分框架中计算逐样本梯度统计是高效可行的。利用此能力，作者重新审视了signSGD和Adam等优化器，并提出了新的见解，颠覆了传统认知，为算法设计提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 深度学习训练通常仅使用mini-batch的平均梯度，认为计算除平均值外的其他梯度统计过于耗费资源。作者旨在挑战这一普遍认知，并探索其可行性。

Method: 通过对自动微分图进行“手术”来实现梯度统计计算，在某些情况下几乎不增加计算和内存开销。此外，对于特定模型（如Transformer），JAX的向量化转换提供了一种可行的原型和实验实现方法。

Result: 1. 逐样本梯度统计的实现，相较于mini-batch梯度计算，几乎不产生额外的计算和内存开销。
2. 在signSGD中，符号操作在梯度处理链中的最佳位置至关重要，且可通过简单的信噪比论证进行预测。
3. 在Adam预处理器中，优化效果最佳时，预处理器应由梯度分布的均值而非方差主导，这与传统观点相悖。

Conclusion: 逐样本梯度信息为优化算法的分析和设计提供了新的可能性和方法。

Abstract: Training algorithms in deep learning usually treat a mini-batch of samples as
a single object; they average gradients over the mini-batch, and then process
the average in various ways. Computing other statistics beyond the average may
have been seen as prohibitively resource intensive in automatic differentiation
(AD) frameworks. We show that this is not the case. Generally, gradient
statistics can be implemented through a surgery of the AD graph, which, in some
cases, incur almost no computational and memory overheads compared to the
mini-batch gradient computation. Additionally, we show that in certain classes
of models, including transformers, JAX's vectorization transformation offers a
viable implementation for prototyping and experimentation. We then revise our
understanding of two nonlinear operations in optimization through the lens of
per-example gradient transformations. We first study signSGD and show that the
optimal placement of the sign operation in the gradient processing chain is
crucial to success and can be predicted with a simple signal-to-noise ratio
argument. Next we study per-example variations of the Adam preconditioner, and
show that optimization is best served when the preconditioner is dominated by
the mean rather than the variance of the gradient distribution - in contrast to
conventional wisdom. Overall we demonstrate that per-example gradient
information enables new analyses and possibilities for algorithm design.

</details>


### [199] [Debunk the Myth of SFT Generalization](https://arxiv.org/abs/2510.00237)
*Xiaofeng Lin,Hejian Sang,Zhipeng Wang,Xuezhou Zhang*

Main category: cs.LG

TL;DR: 本文通过引入提示多样性和CoT监督，揭示SFT在指令和难度变化的任务上能实现强大的泛化能力，甚至超越RL基线，挑战了SFT泛化能力不如RL的传统观点。


<details>
  <summary>Details</summary>
Motivation: 现有观点认为SFT记忆训练数据且泛化失败，而RL具有更强的鲁棒性。本文旨在系统评估并重新审视SFT的泛化能力，探究其被低估的原因。

Method: 在Sokoban和General Points决策基准上进行评估。通过引入“提示多样性”来解决固定指令模板导致的泛化问题，并采用“思维链（CoT）监督”来提升SFT应对更困难任务的泛化能力。最终结合两者进行实验。

Result: SFT的泛化失败主要源于“固定提示伪影”，引入提示多样性后，SFT能泛化到未见指令变体。CoT监督显著提升了SFT向更困难任务（如更大Sokoban、OOD算术）的迁移能力。结合提示多样性和CoT的SFT在指令和难度变体设置下均实现了鲁棒泛化，匹配或超越RL基线，并保留SFT的简洁和稳定性。

Conclusion: 这些发现挑战了SFT本质上不如RL的观点，支持数据中心视角：通过精心策划的演示，SFT可以像RL一样强大地泛化。

Abstract: A prevailing view holds that supervised fine-tuning (SFT) memorizes training
data and fails to generalize, whereas reinforcement learning (RL) attains
broader robustness. We revisit this claim through a systematic evaluation on
two decision-making benchmarks, Sokoban and General Points, and arrive at a
different conclusion. We show that much of SFT's perceived failure stems from
frozen-prompt artifacts: when trained on fixed instruction templates, SFT
models cling to training semantics rather than adapting to new ones.
Introducing prompt diversity during training breaks this shortcut and yields
strong generalization to unseen instruction variants without harming
in-distribution performance. Beyond instruction shifts, we ask whether SFT can
generalize to strictly harder tasks. Here, chain-of-thought (CoT) supervision
provides an algorithmic scaffold that markedly improves transfer to more
difficult regimes, such as larger Sokoban grids with additional boxes and
arithmetic with out-of-distribution values or five-card compositions that
increase combinatorial complexity. Finally, combining prompt diversity with CoT
achieves the best of both worlds: robust generalization across both
instruction-variant and difficulty-variant settings, matching or surpassing RL
baselines on our benchmarks while retaining SFT's simplicity and stability.
These findings challenge the narrative that SFT is inherently inferior to RL
and support a data-centric perspective: with appropriately curated
demonstrations, vanilla SFT can generalize as strongly as RL. Code reproducing
the results in the paper can be found at:
https://github.com/XiaofengLin7/debunking-sft-generalization.

</details>


### [200] [Reward driven discovery of the optimal microstructure representations with invariant variational autoencoders](https://arxiv.org/abs/2510.00243)
*Boris N. Slautin,Kamyar Barakati,Hiroshi Funakubo,Maxim A. Ziatdinov,Vladimir V. Shvartsman,Doru C. Lupascu,Sergei V. Kalinin*

Main category: cs.LG

TL;DR: 为解决变分自编码器（VAEs）在分析复杂显微镜数据时优化困难的问题，本研究提出并验证了基于高斯混合模型（GMM）和贝叶斯高斯混合模型（BGMM）的奖励函数，以实现VAE潜在空间的自动化和无偏优化，从而发现数据的底层物理结构。


<details>
  <summary>Details</summary>
Motivation: 显微镜技术产生海量复杂图像数据，其中包含可用于揭示底层物理结构的简约形式。尽管变分自编码器（VAEs）是构建低维表示的有效工具，但其性能严重依赖于通常通过试错和经验分析优化的设计选择，导致缺乏自动化和无偏的优化方法。

Method: 为实现VAEs工作流的自动化和无偏优化，研究人员探索了基于奖励的策略来评估潜在空间表示。他们以压电响应力显微镜（PFM）数据为模型系统，检验了多种策略和奖励函数。

Result: 研究分析表明，使用高斯混合模型（GMM）和贝叶斯高斯混合模型（BGMM）近似潜在空间，为构建奖励函数提供了坚实基础。这些奖励函数能够有效估计模型效率，并指导寻找最优的简约表示。

Conclusion: 通过GMM和BGMM近似潜在空间构建的奖励函数，能够有效实现VAEs工作流的自动化优化，从而更高效地从复杂显微图像数据中发现潜在的物理结构和简约表示。

Abstract: Microscopy techniques generate vast amounts of complex image data that in
principle can be used to discover simpler, interpretable, and parsimonious
forms to reveal the underlying physical structures, such as elementary building
blocks in molecular systems or order parameters and phases in crystalline
materials. Variational Autoencoders (VAEs) provide a powerful means of
constructing such low-dimensional representations, but their performance
heavily depends on multiple non-myopic design choices, which are often
optimized through trial-and-error and empirical analysis. To enable automated
and unbiased optimization of VAE workflows, we investigated reward-based
strategies for evaluating latent space representations. Using Piezoresponse
Force Microscopy data as a model system, we examined multiple policies and
reward functions that can serve as a foundation for automated optimization. Our
analysis shows that approximating the latent space with Gaussian Mixture Models
(GMM) and Bayesian Gaussian Mixture Models (BGMM) provides a strong basis for
constructing reward functions capable of estimating model efficiency and
guiding the search for optimal parsimonious representations.

</details>


### [201] [CODED-SMOOTHING: Coding Theory Helps Generalization](https://arxiv.org/abs/2510.00253)
*Parsa Moradi,Tayyebeh Jahaninezhad,Mohammad Ali Maddah-Ali*

Main category: cs.LG

TL;DR: 引入“编码平滑”模块，通过处理数据线性组合来正则化学习，提高模型泛化能力和对抗扰动鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 受分布式计算中编码计算抵御故障的启发，旨在将该原理应用于机器学习，设计一种高效的正则化机制，以鼓励更平滑的表示和更强的泛化能力，并增强对抗攻击的鲁棒性。

Method: 设计了“编码平滑”模块，可无缝集成到监督和无监督训练及推理流程中。该模块基于通用编码计算原理，处理数据线性组合而非原始输入，以实现平滑表示和泛化。

Result: 在监督和无监督任务上，编码平滑模块持续改善了模型的泛化能力，并实现了对基于梯度的对抗攻击的最先进鲁棒性。

Conclusion: 编码平滑是一种高效且有效的正则化机制，能够显著提高机器学习模型的泛化能力和对抗鲁棒性。

Abstract: We introduce the coded-smoothing module, which can be seamlessly integrated
into standard training pipelines, both supervised and unsupervised, to
regularize learning and improve generalization with minimal computational
overhead. In addition, it can be incorporated into the inference pipeline to
randomize the model and enhance robustness against adversarial perturbations.
The design of coded-smoothing is inspired by general coded computing, a
paradigm originally developed to mitigate straggler and adversarial failures in
distributed computing by processing linear combinations of the data rather than
the raw inputs. Building on this principle, we adapt coded computing to machine
learning by designing an efficient and effective regularization mechanism that
encourages smoother representations and more generalizable solutions. Extensive
experiments on both supervised and unsupervised tasks demonstrate that
coded-smoothing consistently improves generalization and achieves
state-of-the-art robustness against gradient-based adversarial attacks.

</details>


### [202] [Delayed Attention Training Improves Length Generalization in Transformer--RNN Hybrids](https://arxiv.org/abs/2510.00258)
*Buu Phan,Reza Ebrahimi,Sanjay Haresh,Roland Memisevic*

Main category: cs.LG

TL;DR: 研究表明混合模型中Transformer组件易走捷径，导致长度泛化差。通过延迟注意力层训练，可显著提升混合模型在长序列上的泛化能力，达到近乎完美的准确率。


<details>
  <summary>Details</summary>
Motivation: 循环网络擅长状态跟踪但不擅长关联回忆，而Transformer擅长关联回忆但难以将状态跟踪能力扩展到长序列。受两者互补优势的启发，本研究旨在构建混合模型，评估其是否能结合并保留两种能力，同时解决长度泛化问题。

Method: 构建了结合循环和注意力组件的混合模型，并在包含状态跟踪和关联回忆的复合任务上进行训练。为解决Transformer组件的捷径问题，提出了一种简单而有效的训练策略：延迟注意力层的训练。

Result: 在混合模型中，Transformer组件倾向于利用捷径解决方案，导致长度泛化能力差。所提出的延迟注意力层训练策略有效缓解了捷径依赖问题，并显著改善了长度泛化性能。实验表明，该方法使混合模型在训练序列三倍长的混合序列上实现了接近完美的准确率（>90%）。

Conclusion: 延迟注意力层的训练是一种有效策略，能够减轻混合模型中Transformer组件的捷径依赖，从而显著提升模型在复合任务上的长度泛化能力，使其在远超训练长度的序列上表现出色。

Abstract: We study length generalization in sequence models on a composite problem
involving both state tracking and associative recall. Prior work finds that
recurrent networks handle state tracking well but struggle with recall, whereas
Transformers excel at recall yet fail to extend state-tracking capabilities to
longer sequences. Motivated by the complementary strengths of these
architectures, we construct hybrid models integrating recurrent and
attention-based components, and train them on the combined task to evaluate
whether both capabilities can be preserved. Our results reveal that, in such
hybrids, the Transformer component tends to exploit shortcut solutions, leading
to poor length generalization. We identify this shortcut reliance as a key
obstacle and propose a simple yet effective training strategy -- delaying the
training of the attention layers -- that mitigates this effect and
significantly improves length generalization performance. Our experiments show
that this approach enables hybrid models to achieve near-perfect accuracy
($>90\%$) on hybrid sequences three times longer than those used during
training.

</details>


### [203] [Learning Energy-based Variational Latent Prior for VAEs](https://arxiv.org/abs/2510.00260)
*Debottam Dutta,Chaitanya Amballa,Zhongweiyang Xu,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.LG

TL;DR: VAEs存在“先验空洞”导致样本模糊，本文提出EVaLP方法，将先验建模为基于能量的模型（EBM），并通过变分方法和采样器网络绕过EBM的MCMC采样限制，实现了高效快速采样，显著提升了图像生成质量、减少了先验空洞。


<details>
  <summary>Details</summary>
Motivation: 变分自编码器（VAEs）因“先验空洞”问题导致生成的样本模糊且不一致，即先验高概率区域在后验下概率较低。尽管基于能量的模型（EBMs）能提供匹配后验的灵活性，但其依赖于MCMC方法，导致采样速度慢。

Method: 本文提出Energy-based Variational Latent Prior (EVaLP)方法，将先验建模为基于能量的模型（EBM）。核心思想是引入变分方法来处理EBM的归一化常数，从而避免昂贵的MCMC采样。通过一个采样器网络来近似变分形式，并将其训练公式化为交替优化问题。在生成阶段，该采样器网络直接作为隐式变分先验，提供高效快速的采样。

Result: 将所提出的EVaLP方法与多个SOTA基线进行比较，结果显示EVaLP在图像生成质量方面有显著提升，有效减少了“先验空洞”问题，并提高了采样效率。

Conclusion: EVaLP通过将EBM作为灵活先验并结合创新的变分采样策略，成功解决了VAEs的“先验空洞”问题，有效改善了生成样本质量和采样效率。

Abstract: Variational Auto-Encoders (VAEs) are known to generate blurry and
inconsistent samples. One reason for this is the "prior hole" problem. A prior
hole refers to regions that have high probability under the VAE's prior but low
probability under the VAE's posterior. This means that during data generation,
high probability samples from the prior could have low probability under the
posterior, resulting in poor quality data. Ideally, a prior needs to be
flexible enough to match the posterior while retaining the ability to generate
samples fast. Generative models continue to address this tradeoff. This paper
proposes to model the prior as an energy-based model (EBM). While EBMs are
known to offer the flexibility to match posteriors (and also improving the
ELBO), they are traditionally slow in sample generation due to their dependency
on MCMC methods. Our key idea is to bring a variational approach to tackle the
normalization constant in EBMs, thus bypassing the expensive MCMC approaches.
The variational form can be approximated with a sampler network, and we show
that such an approach to training priors can be formulated as an alternating
optimization problem. Moreover, the same sampler reduces to an implicit
variational prior during generation, providing efficient and fast sampling. We
compare our Energy-based Variational Latent Prior (EVaLP) method to multiple
SOTA baselines and show improvements in image generation quality, reduced prior
holes, and better sampling efficiency.

</details>


### [204] [SLogic: Subgraph-Informed Logical Rule Learning for Knowledge Graph Completion](https://arxiv.org/abs/2510.00279)
*Trung Hoang Le,Tran Cao Son,Huiping Cao*

Main category: cs.LG

TL;DR: SLogic是一个新的知识图谱补全框架，通过利用查询头实体的局部子图上下文，为逻辑规则动态分配查询依赖的分数，从而提高性能并超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于逻辑规则的知识图谱补全方法将规则视为普遍适用，赋予固定置信度，忽略了规则重要性随查询变化的关键局限性。

Method: 提出SLogic框架，其核心是一个评分函数，利用以查询头实体为中心的子图，动态评估每条逻辑规则的重要性，为其分配查询依赖的分数。

Result: 在基准数据集上的广泛实验表明，通过利用局部子图上下文，SLogic持续优于包括基于嵌入和基于规则的最新基线方法。

Conclusion: SLogic通过引入查询依赖的规则评分机制，有效解决了现有规则方法忽略查询上下文的局限性，显著提升了知识图谱补全的性能。

Abstract: Logical rule-based methods offer an interpretable approach to knowledge graph
completion by capturing compositional relationships in the form of
human-readable inference rules. However, current approaches typically treat
logical rules as universal, assigning each rule a fixed confidence score that
ignores query-specific context. This is a significant limitation, as a rule's
importance can vary depending on the query. To address this, we introduce
SLogic (Subgraph-Informed Logical Rule learning), a novel framework that
assigns query-dependent scores to logical rules. The core of SLogic is a
scoring function that utilizes the subgraph centered on a query's head entity,
allowing the significance of each rule to be assessed dynamically. Extensive
experiments on benchmark datasets show that by leveraging local subgraph
context, SLogic consistently outperforms state-of-the-art baselines, including
both embedding-based and rule-based methods.

</details>


### [205] [Free Draft-and-Verification: Toward Lossless Parallel Decoding for Diffusion Large Language Models](https://arxiv.org/abs/2510.00294)
*Shutong Wu,Jiawei Zhang*

Main category: cs.LG

TL;DR: 扩散大语言模型（DLLMs）在上下文理解方面有优势，但推理效率低。本文提出Freedave算法，实现了DLLMs的无损并行解码，将吞吐量提升高达2.8倍，且无性能下降。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）因其双向注意力机制在捕捉上下文和解决“逆转诅咒”等挑战上表现出色。然而，其双向特性使其与KV Cache不兼容，导致推理效率远低于自回归模型。现有并行解码算法虽然能加速DLLM推理，但会牺牲性能，因此亟需一种既能提升效率又不损失性能的并行解码方案。

Method: 本文引入了Free Draft-and-Verification (Freedave) 算法，这是一种专为DLLMs设计的快速采样算法。该方法通过并行解码的候选生成和验证流程，保证能重现静态采样生成的结果，并且无需引入额外的模型前向调用，从而实现无损并行解码。

Result: 应用Freedave算法后，扩散大语言模型（DLLMs）在数学推理任务上的吞吐量最高可提升2.8倍，且在提升吞吐量的同时未导致任何性能下降。

Conclusion: Freedave算法成功克服了DLLMs因KV Cache不兼容导致的推理效率低下问题，通过实现无损并行解码，显著提升了DLLMs的吞吐量，使其在保持高性能的同时更具竞争力。

Abstract: Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of
language modeling beyond autoregressive next-token prediction. Thanks to their
bidirectional attention mechanism, DLLMs are more capable of capturing the
connection of context, and thus show unique advantages in challenges like the
famous "reversal curse" or learning under data-constrained scenarios. However,
this bidirectional nature also brings an obstacle that DLLMs are not inherently
compatible with KV Cache, and consequently, the inference efficiency is not
competitive compared with autoregressive models. Taking advantage of their
inherent capability of multi-token prediction, existing parallel decoding
algorithms can speed up the DLLM inference, but at the cost of non-negligible
performance degradation. To overcome this challenge, we introduce Free
Draft-and-Verification (Freedave), a novel fast sampling algorithm tailored for
DLLMs that achieves lossless parallel decoding. Specifically, we propose a
pipeline of parallel-decoded candidate generation and verification, which is
guaranteed to reproduce the same sequence generated by static sampling, without
introducing extra model forward calls. By applying Freedave, the throughput of
DLLMs can be boosted up to $2.8\times$ without performance degradation on math
reasoning tasks.

</details>


### [206] [Beyond Token Probes: Hallucination Detection via Activation Tensors with ACT-ViT](https://arxiv.org/abs/2510.00296)
*Guy Bar-Shalom,Fabrizio Frasca,Yaniv Galron,Yftah Ziser,Haggai Maron*

Main category: cs.LG

TL;DR: 本文提出ACT-ViT，一种受Vision Transformer启发的新模型，通过将LLM激活张量视为图像，有效且高效地检测大语言模型幻觉。ACT-ViT超越传统探测技术，支持多LLM训练，实现强大的零样本性能和跨LLM迁移能力。


<details>
  <summary>Details</summary>
Motivation: 检测大语言模型（LLM）生成的文本中的幻觉对于其安全部署至关重要。现有的探测分类器虽然有前景，但它们仅作用于孤立的层-token对且特定于LLM，这限制了它们的有效性并阻碍了跨LLM应用。

Method: 本文引入了一种新颖方法，ACT-ViT。该方法利用激活数据（层 × token）的自然顺序结构，将完整的激活张量视为图像。ACT-ViT是受Vision Transformer启发的模型，可以有效且高效地应用于激活张量，并支持同时在来自多个LLM的数据上进行训练。

Result: ACT-ViT在涵盖多种LLM和数据集的综合实验中，持续优于传统探测技术，同时部署效率极高。具体而言，该架构从多LLM训练中显著受益，在未见数据集上实现了强大的零样本性能，并通过微调可以有效地迁移到新的LLM。

Conclusion: ACT-ViT为LLM幻觉检测提供了一种新颖、高效且可泛化的解决方案。通过将激活张量视为图像并支持多LLM训练，它克服了现有方法的局限性，展示了卓越的性能、强大的泛化能力和可迁移性，对于LLM的安全部署具有重要意义。

Abstract: Detecting hallucinations in Large Language Model-generated text is crucial
for their safe deployment. While probing classifiers show promise, they operate
on isolated layer-token pairs and are LLM-specific, limiting their
effectiveness and hindering cross-LLM applications. In this paper, we introduce
a novel approach to address these shortcomings. We build on the natural
sequential structure of activation data in both axes (layers $\times$ tokens)
and advocate treating full activation tensors akin to images. We design
ACT-ViT, a Vision Transformer-inspired model that can be effectively and
efficiently applied to activation tensors and supports training on data from
multiple LLMs simultaneously. Through comprehensive experiments encompassing
diverse LLMs and datasets, we demonstrate that ACT-ViT consistently outperforms
traditional probing techniques while remaining extremely efficient for
deployment. In particular, we show that our architecture benefits substantially
from multi-LLM training, achieves strong zero-shot performance on unseen
datasets, and can be transferred effectively to new LLMs through fine-tuning.
Full code is available at https://github.com/BarSGuy/ACT-ViT.

</details>


### [207] [Barriers for Learning in an Evolving World: Mathematical Understanding of Loss of Plasticity](https://arxiv.org/abs/2510.00304)
*Amir Joudaki,Giulia Lanzillotta,Mohammad Samragh Razlighi,Iman Mirzadeh,Keivan Alizadeh,Thomas Hofmann,Mehrdad Farajtabar,Fartash Faghri*

Main category: cs.LG

TL;DR: 深度学习模型在非静态数据中存在“可塑性损失”（LoP）问题。本研究基于动力系统理论，揭示了LoP由激活饱和的“冻结单元”和表示冗余的“克隆单元流形”引起，并指出静态泛化特性与持续学习中的LoP存在内在矛盾，同时探讨了缓解策略。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在静态数据中表现出色，但在非静态环境中由于“可塑性损失”（LoP），即未来学习能力的退化，而面临挑战。本研究旨在对LoP进行首次原理性探究。

Method: 采用基于梯度学习的LoP第一性原理研究，根植于动力系统理论。通过识别参数空间中捕获梯度轨迹的稳定流形，正式定义LoP。

Result: 揭示了两个主要的陷阱形成机制：激活饱和导致的“冻结单元”和表示冗余导致的“克隆单元流形”。发现了一个基本矛盾：在静态设置中促进泛化的特性（如低秩表示、简化偏差）直接导致持续学习中的LoP。

Conclusion: 理论分析通过数值模拟得到验证。探讨了架构选择或目标扰动作为潜在的缓解策略。

Abstract: Deep learning models excel in stationary data but struggle in non-stationary
environments due to a phenomenon known as loss of plasticity (LoP), the
degradation of their ability to learn in the future. This work presents a
first-principles investigation of LoP in gradient-based learning. Grounded in
dynamical systems theory, we formally define LoP by identifying stable
manifolds in the parameter space that trap gradient trajectories. Our analysis
reveals two primary mechanisms that create these traps: frozen units from
activation saturation and cloned-unit manifolds from representational
redundancy. Our framework uncovers a fundamental tension: properties that
promote generalization in static settings, such as low-rank representations and
simplicity biases, directly contribute to LoP in continual learning scenarios.
We validate our theoretical analysis with numerical simulations and explore
architectural choices or targeted perturbations as potential mitigation
strategies.

</details>


### [208] [Lipschitz Bandits with Stochastic Delayed Feedback](https://arxiv.org/abs/2510.00309)
*Zhongxuan Liu,Yue Kang,Thomas C. M. Lee*

Main category: cs.LG

TL;DR: 本文引入并解决了带有随机延迟反馈的Lipschitz强盗问题，针对有界和无界延迟情况设计了算法，实现了次线性甚至近乎最优的后悔界。


<details>
  <summary>Details</summary>
Motivation: 传统的Lipschitz强盗问题假设奖励是即时观察到的，而现实世界中反馈常伴随机延迟。本文旨在解决带有随机延迟反馈的Lipschitz强盗问题，以适应更真实的场景。

Method: 针对有界延迟，提出了一种延迟感知缩放（delay-aware zooming）算法；针对无界延迟，提出了一种新颖的分阶段学习（phased learning）策略，通过精心安排的间隔积累可靠反馈。并通过实验结果验证了算法效率。

Result: 对于有界延迟，所提出的延迟感知缩放算法保持了无延迟设置下的最优性能，额外项与最大延迟呈比例关系。对于无界延迟，分阶段学习策略实现了接近最优的后悔，仅差对数因子，并通过后悔下界证明了其近乎最优性。实验结果验证了所提出算法在各种延迟场景下的效率。

Conclusion: 本文成功地解决了带有随机延迟反馈的Lipschitz强盗问题，为有界和无界延迟场景提供了有效的算法。这些算法在理论上实现了次线性或接近最优的后悔界，并通过实验证明了其效率。

Abstract: The Lipschitz bandit problem extends stochastic bandits to a continuous
action set defined over a metric space, where the expected reward function
satisfies a Lipschitz condition. In this work, we introduce a new problem of
Lipschitz bandit in the presence of stochastic delayed feedback, where the
rewards are not observed immediately but after a random delay. We consider both
bounded and unbounded stochastic delays, and design algorithms that attain
sublinear regret guarantees in each setting. For bounded delays, we propose a
delay-aware zooming algorithm that retains the optimal performance of the
delay-free setting up to an additional term that scales with the maximal delay
$\tau_{\max}$. For unbounded delays, we propose a novel phased learning
strategy that accumulates reliable feedback over carefully scheduled intervals,
and establish a regret lower bound showing that our method is nearly optimal up
to logarithmic factors. Finally, we present experimental results to demonstrate
the efficiency of our algorithms under various delay scenarios.

</details>


### [209] [Robust Federated Inference](https://arxiv.org/abs/2510.00310)
*Akash Dhasade,Sadegh Farhadkhani,Rachid Guerraoui,Nirupam Gupta,Maxime Jacovella,Anne-Marie Kermarrec,Rafael Pinot*

Main category: cs.LG

TL;DR: 联邦推理的鲁棒性被忽视，易受攻击。本研究首次形式化并分析了鲁棒联邦推理问题，对线性聚合器进行了分析，并提出一种结合对抗训练和测试时鲁棒聚合的DeepSet模型，显著提高了非线性聚合器的鲁棒性，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 联邦推理作为一种结合多模型预测的方案具有吸引力，但其鲁棒性长期以来被忽视，使其容易受到攻击。本研究旨在填补这一关键空白，形式化鲁棒联邦推理问题，并首次对其方法进行鲁棒性分析。

Method: 1. 形式化鲁棒联邦推理问题，并对基于平均的聚合器进行首次鲁棒性分析。2. 将非线性聚合器的鲁棒联邦推理问题转化为对抗性机器学习问题。3. 引入一种使用DeepSet聚合模型的高级技术，该技术结合了对抗训练和测试时鲁棒聚合，以增强非线性聚合器的鲁棒性。

Result: 1. 对基于平均的聚合器分析表明，当诚实响应之间的差异较小或两个最可能类别之间的裕度较大时，聚合器的误差较小。2. 提出的结合对抗训练和测试时鲁棒聚合的DeepSet方法，在不同基准测试中，其准确性比现有鲁棒聚合方法高出4.7%至22.2%。

Conclusion: 本研究对联邦推理的鲁棒性问题进行了首次系统分析，并为非线性聚合器提出了一种新颖且高效的鲁棒化方法（DeepSet结合对抗训练和测试时鲁棒聚合），显著提升了联邦推理的抗攻击能力和准确性。

Abstract: Federated inference, in the form of one-shot federated learning, edge
ensembles, or federated ensembles, has emerged as an attractive solution to
combine predictions from multiple models. This paradigm enables each model to
remain local and proprietary while a central server queries them and aggregates
predictions. Yet, the robustness of federated inference has been largely
neglected, leaving them vulnerable to even simple attacks. To address this
critical gap, we formalize the problem of robust federated inference and
provide the first robustness analysis of this class of methods. Our analysis of
averaging-based aggregators shows that the error of the aggregator is small
either when the dissimilarity between honest responses is small or the margin
between the two most probable classes is large. Moving beyond linear averaging,
we show that problem of robust federated inference with non-linear aggregators
can be cast as an adversarial machine learning problem. We then introduce an
advanced technique using the DeepSet aggregation model, proposing a novel
composition of adversarial training and test-time robust aggregation to
robustify non-linear aggregators. Our composition yields significant
improvements, surpassing existing robust aggregation methods by 4.7 - 22.2% in
accuracy points across diverse benchmarks.

</details>


### [210] [DiSC-AMC: Token- and Parameter-Efficient Discretized Statistics In-Context Automatic Modulation Classification](https://arxiv.org/abs/2510.00316)
*Mohammad Rostami,Atik Faysal,Reihaneh Gh. Roshan,Huaxia Wang,Nikhil Muralidhar,Yu-Dong Yao*

Main category: cs.LG

TL;DR: 本文提出DiSC-AMC，一种令牌和参数高效的LLM自动调制分类方法，通过离散化统计数据和优化上下文提示，将推理成本降低一倍以上，同时保持竞争力准确率，解决了实际部署瓶颈。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明LLMs无需微调即可进行开放集自动调制分类（AMC），但其长提示上下文和大型模型尺寸限制了实际的在环部署（in-the-loop deployment）。

Method: DiSC-AMC方法包括：(i) 将高阶统计量和累积量离散化为紧凑的符号令牌；(ii) 通过轻量级k-top神经预过滤器修剪示例列表，并利用之前LLM响应中提取的理由过滤误导性/低影响特征；(iii) 通过校准的提示模板强制进行仅标签预测。

Result: 这些改进将输入/输出令牌和模型参数足迹减少了一半以上，同时保持了竞争力准确率。在合成AMC任务中（10种调制类型，含噪声），DiSC-AMC使用约5B参数的Gemini-2.5-Flash模型达到45.5%的准确率，远高于7B参数基线模型DeepSeek-R1-Distill-Qwen的5.2%。推理成本降低了两倍以上。

Conclusion: 精心设计的离散化和上下文选择可以大幅降低推理成本（超过2倍），同时保留基于提示的AMC优势，并实现实际的在环部署。

Abstract: Large Language Models (LLMs) can perform Automatic Modulation Classification
(AMC) in an open-set manner without LLM fine-tuning when equipped with
carefully designed in-context prompts~\cite{rostami2025plug}. Building on this
prior work, we target the practical bottlenecks of long prompt contexts and
large model sizes that impede in-the-loop deployment. We present Discretized
Statistics in-Context Automatic Modulation Classification (DiSC-AMC), a token-
and parameter-efficient variant that: (i) discretizes higher-order statistics
and cumulants into compact symbolic tokens, (ii) prunes the exemplar list via a
lightweight k-top neural prefilter and filters misleading/low-impact features
using rationales extracted from prior LLM responses, and (iii) enforces
label-only predictions through a calibrated prompt template. Together, these
changes reduce both input/output tokens and the model parameter footprint by
more than half while maintaining competitive accuracy. On synthetic AMC with
ten modulation types under noise, a 7B \textit{DeepSeek-R1-Distill-Qwen}
baseline achieves 5.2% accuracy, whereas our system, using an approximately
5B-parameter \textit{Gemini-2.5-Flash}~\cite{comanici2025gemini} model, attains
45.5% accuracy. These results demonstrate that careful discretization and
context selection can cut inference cost by over 2x while preserving the
advantages of prompt-based AMC and enabling practical in-the-loop use.

</details>


### [211] [DecepChain: Inducing Deceptive Reasoning in Large Language Models](https://arxiv.org/abs/2510.00319)
*Wei Shen,Han Wang,Haoyu Li,Huan Zhang*

Main category: cs.LG

TL;DR: DecepChain是一种新型的后门攻击，能够诱导大型语言模型（LLMs）生成看似合理但实际上导致错误结论的连贯思维链（CoT），且难以被人类察觉。


<details>
  <summary>Details</summary>
Motivation: LLMs的思维链（CoT）被广泛用于判断答案质量，建立了强大的信任基础。然而，这种信任是脆弱的，存在攻击者诱导LLMs生成错误但看似合理的CoT的风险，这是一个紧迫但未充分探索的问题。

Method: 引入DecepChain攻击范式。它利用LLMs的幻觉现象，通过在模型自身生成的错误推理上进行微调来放大幻觉，然后通过Group Relative Policy Optimization（GRPO）结合对触发输入翻转奖励和可信度正则化器来强化攻击，以保持推理的流畅性和表面的良性。

Result: 在多个基准测试和模型上，DecepChain实现了高攻击成功率，同时对正常情况下的性能影响极小。人类评估结果显示，评估者难以区分受操纵的推理过程和正常的推理过程，凸显了攻击的隐蔽性。

Conclusion: 这种隐蔽的故障模式可能悄然腐蚀LLM的答案并损害人类对LLM推理的信任，强调了未来研究应对这一风险的紧迫性。

Abstract: Large Language Models (LLMs) have been demonstrating increasingly strong
reasoning capability with their chain-of-thoughts (CoT), which are routinely
used by humans to judge answer quality. This reliance creates a powerful yet
fragile basis for trust. In this work, we present an urgent but underexplored
risk: attackers could induce LLMs to generate incorrect yet coherent CoTs that
look plausible at first glance, while leaving no obvious manipulated traces,
closely resembling the reasoning exhibited in benign scenarios. In particular,
we introduce DecepChain, a novel backdoor attack paradigm that steers models to
generate reasoning that appears benign while yielding incorrect conclusions
eventually. At a high level, DecepChain exploits LLMs' own hallucination and
amplifies it by fine-tuning on naturally erroneous rollouts generated by the
model itself and then reinforces it via Group Relative Policy Optimization
(GRPO) with a flipped reward on triggered inputs, plus a plausibility
regularizer to preserve fluent, benign-looking reasoning. Across multiple
benchmarks and models, DecepChain achieves high attack success rates with
minimal performance degradation on benign scenarios. Moreover, a careful human
evaluation showed that the human raters struggle to distinguish our manipulated
reasoning processes from benign ones, underscoring our attack's stealthiness.
Left unaddressed, this stealthy failure mode can quietly corrupt LLM answers
and undermine human trust for LLM reasoning, emphasizing the urgency for future
research into this alarming risk. Project page: https://decepchain.github.io/.

</details>


### [212] [A Framework for Selection of Machine Learning Algorithms Based on Performance Metrices and Akaike Information Criteria in Healthcare, Telecommunication, and Marketing Sector](https://arxiv.org/abs/2510.00321)
*A. K. Hamisu,K. Jasleen*

Main category: cs.LG

TL;DR: 本章开发了一个推荐框架，用于在医疗保健、营销和电信领域中，根据数据集属性和性能指标，优化机器学习算法的选择，以提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 互联网生成的数据呈指数级增长，推动了人工智能、机器学习和深度学习在营销、电信和健康领域提取可行见解的需求。研究动机在于解决关键挑战，如心血管疾病预测和胎儿健康分类，并填补自动化模型选择的空白。

Method: 探索ML在医疗保健、营销和电信领域的应用，重点是开发最优ML算法选择框架。将ML算法分为急切型、懒惰型和混合型学习器，并根据数据集属性、性能指标（准确率、精确度、召回率）和Akaike信息准则（AIC）分数进行选择。实验使用来自这三个部门的八个数据集进行验证。

Result: 研究成果是一个推荐框架，能够根据输入属性识别最佳ML模型，平衡性能评估和模型复杂性，从而显著提高各种实际应用的效率和准确性。

Conclusion: 该框架弥补了自动化模型选择的空白，为跨学科的ML部署提供了实际指导，有望提升ML在多样化真实世界应用中的效率和准确性。

Abstract: The exponential growth of internet generated data has fueled advancements in
artificial intelligence (AI), machine learning (ML), and deep learning (DL) for
extracting actionable insights in marketing,telecom, and health sectors. This
chapter explores ML applications across three domains namely healthcare,
marketing, and telecommunications, with a primary focus on developing a
framework for optimal ML algorithm selection. In healthcare, the framework
addresses critical challenges such as cardiovascular disease prediction
accounting for 28.1% of global deaths and fetal health classification into
healthy or unhealthy states, utilizing three datasets. ML algorithms are
categorized into eager, lazy, and hybrid learners, selected based on dataset
attributes, performance metrics (accuracy, precision, recall), and Akaike
Information Criterion (AIC) scores. For validation, eight datasets from the
three sectors are employed in the experiments. The key contribution is a
recommendation framework that identifies the best ML model according to input
attributes, balancing performance evaluation and model complexity to enhance
efficiency and accuracy in diverse real-world applications. This approach
bridges gaps in automated model selection, offering practical implications for
interdisciplinary ML deployment.

</details>


### [213] [Cutting the Skip: Training Residual-Free Transformers](https://arxiv.org/abs/2510.00345)
*Yiping Ji,James Martens,Jianqiao Zheng,Ziqin Zhou,Peyman Moghadam,Xinyu Zhang,Hemanth Saratchandran,Simon Lucey*

Main category: cs.LG

TL;DR: 本文提出一种新的初始化策略，使得无跳跃连接的Transformer（ViT）能够稳定高效训练，克服优化障碍，学习更丰富的层次表示，并在密集预测任务上超越有跳跃连接的基线模型。


<details>
  <summary>Details</summary>
Motivation: Transformer因其可扩展性而取得巨大成功，但训练无跳跃（残差）连接的Transformer仍然极其困难。跳跃连接虽然稳定优化，但会破坏表示的层次结构，因此，研究Transformer是否能在没有跳跃连接的情况下高效训练是一个长期存在的问题。

Method: 通过分析无跳跃Transformer模块的雅可比矩阵，揭示了跳跃连接改善条件性的原因，并发现其稳定优势可以通过一种原理性初始化策略来恢复。基于此洞察，作者提出了一种不改变标准架构，却能实现无跳跃Transformer稳定高效训练的方法。

Result: 在监督和自监督设置下的Vision Transformer（ViT）上验证了该方法。结果表明，使用该初始化策略训练的无跳跃ViT克服了通常的优化障碍，学习到更丰富的层次表示，并在密集预测基准测试中超越了包含跳跃连接的强大基线模型。

Conclusion: 研究结果表明，跳跃连接并非训练ViT的根本要求，这为视觉模型中的层次表示学习开辟了新途径。

Abstract: Transformers have achieved remarkable success across a wide range of
applications, a feat often attributed to their scalability. Yet training them
without skip (residual) connections remains notoriously difficult. While skips
stabilize optimization, they also disrupt the hierarchical structure of
representations, raising the long-standing question of whether transformers can
be trained efficiently without them. In this work, we address this problem by
analyzing the Jacobian of a skipless transformer block, showing why skips
improve conditioning and revealing that their stabilization benefits can be
recovered through a principled initialization strategy. Building on this
insight, we introduce the first method that enables stable and efficient
training of skipless transformers without altering the standard architecture.
We validate our approach on Vision Transformers (ViTs) in both supervised and
self-supervised settings, demonstrating that skipless ViTs trained with our
initialization overcome the usual optimization barriers, learn richer
hierarchical representations, and outperform strong baselines, that incorporate
skip connections, on dense prediction benchmarks. These results show that skip
connections are not a fundamental requirement for training ViTs and open new
avenues for hierarchical representation learning in vision models.

</details>


### [214] [In-Context Curiosity: Distilling Exploration for Decision-Pretrained Transformers on Bandit Tasks](https://arxiv.org/abs/2510.00347)
*Huitao Yang,Guanting Chen*

Main category: cs.LG

TL;DR: 本文提出“情境好奇心”作为一种轻量级正则化器，并引入预测驱动型Transformer (PPT) 框架，通过辅助奖励预测器利用预测误差作为好奇心信号，以提高决策预训练Transformer (DPT) 在离线预训练中对分布外数据的泛化能力，并在实验中验证了其鲁棒性改进。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型(LLMs)能力增强，将其应用于决策任务的兴趣日益增长。决策预训练Transformer (DPTs)是常见方案，但现有训练方法难以泛化到其预训练数据分布之外，因此需要探索缓解此限制的方法。

Method: 我们提出“情境好奇心”——一种轻量级、探索启发的离线预训练正则化器，并引入预测驱动型Transformer (PPT) 框架。PPT通过一个辅助奖励预测器增强DPT，利用预测误差作为内在好奇心信号，以鼓励训练期间更广泛的探索。

Result: 在高斯多臂强盗问题的概念验证实验中，PPT展示了改进的鲁棒性：当测试环境表现出更高的奖励方差时（尤其在预训练数据多样性有限的情况下），它能缓解DPT观察到的性能下降。

Conclusion: 初步结果表明，好奇心驱动的预训练为增强情境强化学习智能体的分布外泛化提供了一个有前景的方向，尽管离线数据质量仍然是基础。

Abstract: As large language models (LLMs) continue to grow in capability, there is
increasing interest in incorporating them into decision-making tasks. A common
pipeline for this is Decision-Pretrained Transformers (DPTs). However, existing
training methods for DPTs often struggle to generalize beyond their pretraining
data distribution. To explore mitigation of this limitation, we propose
in-context curiosity -- a lightweight, exploration-inspired regularizer for
offline pretraining -- and introduce the Prediction-Powered Transformer (PPT)
framework. PPT augments DPT with an auxiliary reward predictor, using
prediction error as an intrinsic curiosity signal to encourage broader
exploration during training. In proof-of-concept experiments on Gaussian
multi-armed bandits, PPT shows improved robustness: it moderates the
performance degradation observed in DPT when test environments exhibit higher
variance in reward, particularly when pretraining data has limited diversity.
While the quality of offline data remain fundamental, our preliminary results
suggest that curiosity-driven pretraining offers a promising direction for
enhancing out-of-distribution generalization in in-context RL agents.

</details>


### [215] [Initial Distribution Sensitivity of Constrained Markov Decision Processes](https://arxiv.org/abs/2510.00348)
*Alperen Tercan,Necmiye Ozay*

Main category: cs.LG

TL;DR: 本文利用对偶和扰动分析，推导了CMDP最优值随初始分布变化的界限，并展示其如何用于分析策略遗憾。


<details>
  <summary>Details</summary>
Motivation: 约束马尔可夫决策过程（CMDPs）比标准MDPs更复杂，因为它没有适用于所有初始状态分布的通用最优策略，导致初始分布变化时需要重新求解CMDP。

Method: 通过CMDP的对偶分析和线性规划的扰动分析，分析CMDP最优值随不同初始分布的变化，并推导出这些变化的界限。

Result: 推导出了CMDP最优值随不同初始分布变化的界限。此外，展示了这些界限如何用于分析给定策略因初始分布未知变化而产生的遗憾。

Conclusion: 所推导的界限为理解CMDPs对初始分布的敏感性提供了工具，并且可用于量化因初始分布未知变化而导致的策略遗憾。

Abstract: Constrained Markov Decision Processes (CMDPs) are notably more complex to
solve than standard MDPs due to the absence of universally optimal policies
across all initial state distributions. This necessitates re-solving the CMDP
whenever the initial distribution changes. In this work, we analyze how the
optimal value of CMDPs varies with different initial distributions, deriving
bounds on these variations using duality analysis of CMDPs and perturbation
analysis in linear programming. Moreover, we show how such bounds can be used
to analyze the regret of a given policy due to unknown variations of the
initial distribution.

</details>


### [216] [Flow Autoencoders are Effective Protein Tokenizers](https://arxiv.org/abs/2510.00351)
*Rohit Dilip,Evan Zhang,Ayush Varshney,David Van Valen*

Main category: cs.LG

TL;DR: Kanzi是一种基于流的蛋白质结构分词器和生成器，它通过扩散自编码器和流匹配损失简化了蛋白质结构的分词过程，并在重建任务上以更低的成本超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的蛋白质结构分词方法依赖于定制组件，虽然对空间对称性不变，但优化和扩展困难。

Method: 本文提出了Kanzi，一个基于流的分词器，用于蛋白质结构的分词和生成。Kanzi由一个使用流匹配损失训练的扩散自编码器组成。该方法用全局坐标替换了基于帧的表示，用单一流匹配损失替换了复杂损失，并用标准注意力替换了SE(3)不变注意力操作。

Result: Kanzi显著稳定了参数高效模型的训练，这些模型在重建指标上以更小的模型尺寸和训练成本超越了现有分词器。使用Kanzi训练的自回归模型优于类似操作于token的生成模型，尽管它尚未达到最先进的连续扩散模型的性能。

Conclusion: Kanzi提供了一种更简单、更高效的蛋白质结构分词和生成方法，显著优于现有分词器，并为未来的多模态蛋白质模型奠定基础，尽管在生成质量上仍有提升空间。

Abstract: Protein structure tokenizers enable the creation of multimodal models of
protein structure, sequence, and function. Current approaches to protein
structure tokenization rely on bespoke components that are invariant to spatial
symmetries, but that are challenging to optimize and scale. We present Kanzi, a
flow-based tokenizer for tokenization and generation of protein structures.
Kanzi consists of a diffusion autoencoder trained with a flow matching loss. We
show that this approach simplifies several aspects of protein structure
tokenizers: frame-based representations can be replaced with global
coordinates, complex losses are replaced with a single flow matching loss, and
SE(3)-invariant attention operations can be replaced with standard attention.
We find that these changes stabilize the training of parameter-efficient models
that outperform existing tokenizers on reconstruction metrics at a fraction of
the model size and training cost. An autoregressive model trained with Kanzi
outperforms similar generative models that operate over tokens, although it
does not yet match the performance of state-of-the-art continuous diffusion
models. Code is available here: https://github.com/rdilip/kanzi/.

</details>


### [217] [AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with Multi-Objective Guidance](https://arxiv.org/abs/2510.00352)
*Tong Chen,Yinuo Zhang,Pranam Chatterjee*

Main category: cs.LG

TL;DR: 引入AReUReDi算法，一个具有理论收敛保证的离散优化方法，用于多目标生物分子序列设计，并在实验中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 在生物治疗和生物分子工程中，设计满足多个（常冲突的）目标的序列是一个核心挑战。现有生成框架主要在连续空间中进行单目标指导，而离散方法缺乏多目标帕累托最优性的保证。

Method: 提出AReUReDi（Annealed Rectified Updates for Refining Discrete Flows），一个基于Rectified Discrete Flows (ReDi) 的离散优化算法。它结合了Tchebycheff标量化、局部平衡提议和退火Metropolis-Hastings更新，以偏向帕累托最优状态的采样，同时保持分布不变性，并提供收敛到帕累托前沿的理论保证。

Result: 将AReUReDi应用于肽和SMILES序列设计，能够同时优化多达五种治疗特性（包括亲和力、溶解度、溶血性、半衰期和非污染性）。实验结果表明，AReUReDi的性能优于进化算法和基于扩散的基线方法。

Conclusion: AReUReDi是一个强大的、基于序列的多属性生物分子生成框架。

Abstract: Designing sequences that satisfy multiple, often conflicting, objectives is a
central challenge in therapeutic and biomolecular engineering. Existing
generative frameworks largely operate in continuous spaces with
single-objective guidance, while discrete approaches lack guarantees for
multi-objective Pareto optimality. We introduce AReUReDi (Annealed Rectified
Updates for Refining Discrete Flows), a discrete optimization algorithm with
theoretical guarantees of convergence to the Pareto front. Building on
Rectified Discrete Flows (ReDi), AReUReDi combines Tchebycheff scalarization,
locally balanced proposals, and annealed Metropolis-Hastings updates to bias
sampling toward Pareto-optimal states while preserving distributional
invariance. Applied to peptide and SMILES sequence design, AReUReDi
simultaneously optimizes up to five therapeutic properties (including affinity,
solubility, hemolysis, half-life, and non-fouling) and outperforms both
evolutionary and diffusion-based baselines. These results establish AReUReDi as
a powerful, sequence-based framework for multi-property biomolecule generation.

</details>


### [218] [Continual Learning with Query-Only Attention](https://arxiv.org/abs/2510.00365)
*Gautham Bekal,Ashish Pujari,Scott David Kelly*

Main category: cs.LG

TL;DR: 提出一种仅查询注意力机制用于持续学习，有效缓解灾难性遗忘，并表明完整注意力可能并非必需。


<details>
  <summary>Details</summary>
Motivation: 持续学习因数据流和任务间分布变化而复杂，面临可塑性丧失和灾难性遗忘问题。

Method: 提出一种仅查询（query-only）注意力机制，舍弃键和值，但保留Transformer核心归纳偏置。通过Hessian谱分析模型的可塑性。将仅查询注意力、完整Transformer注意力和模型无关元学习建立概念联系。

Result: 仅查询注意力机制在持续学习中显著缓解可塑性丧失和灾难性遗忘，性能优于选择性重初始化等基线。初步Hessian谱分析表明，在任务中保持更高曲率秩的模型更倾向于保留可塑性。

Conclusion: 研究结果表明，在持续学习中，完整注意力可能并非捕获元学习益处的必要条件。

Abstract: Continual learning involves learning from a stream of data without repetition
of data points, a scenario that is inherently complex due to distributional
shift across tasks. We propose a query-only attention mechanism that discards
keys and values, yet preserves the core inductive bias of transformer
architectures. In continual learning scenarios, this simplified mechanism
significantly mitigates both loss of plasticity and catastrophic forgetting,
outperforming baselines such as selective re-initialization. We establish a
conceptual link between query-only attention, full transformer attention, and
model agnostic meta-learning, framing them as instances of meta-learning. We
further provide intuition for why query-based models and attention networks
help preserve plasticity in continual settings. Finally, through preliminary
Hessian spectrum analysis, we observe that models maintaining higher curvature
rank across tasks tend to retain plasticity. Our findings suggest that full
attention may not be essential for capturing the benefits of meta-learning in
continual learning.

</details>


### [219] [The Transformer Cookbook](https://arxiv.org/abs/2510.00368)
*Andy Yang,Christopher Watson,Anton Xue,Satwik Bhattamishra,Jose Llarena,William Merrill,Emile Dos Santos Ferreira,Anej Svete,David Chiang*

Main category: cs.LG

TL;DR: 本文提出了一个“Transformer菜谱”，集合了将算法直接编码到Transformer参数中的各种技术，旨在整合分散的现有知识。


<details>
  <summary>Details</summary>
Motivation: 解决将算法直接编码到Transformer参数中的学习曲线陡峭问题，该问题因相关文献分散而加剧。

Method: 将大量分散的发现综合成一套精选的“菜谱”，展示了如何实现从前馈层中的基本算术到通过自注意力进行复杂数据路由等各种算法编码。

Result: 创建了一个统一的Transformer构建技术集合，作为Transformer菜谱，涵盖了将算法直接编码到Transformer参数中的多种方法。

Conclusion: 该统一的展示为新手提供了便捷的入门点，为专家提供了系统的参考，并为未来计算复杂性理论研究、架构设计和可解释性等方面的Transformer工作奠定了基础。

Abstract: We present the transformer cookbook: a collection of techniques for directly
encoding algorithms into a transformer's parameters. This work addresses the
steep learning curve of such endeavors, a problem exacerbated by a fragmented
literature where key results are scattered across numerous papers. In
particular, we synthesize this disparate body of findings into a curated set of
recipes that demonstrate how to implement everything from basic arithmetic in
feed-forward layers to complex data routing via self-attention. Our mise en
place of formulations is for both newcomers seeking an accessible entry point
and experts in need of a systematic reference. This unified presentation of
transformer constructions provides a foundation for future work spanning
theoretical research in computational complexity to empirical investigations in
architecture design and interpretability.

</details>


### [220] [Combining Large Language Models and Gradient-Free Optimization for Automatic Control Policy Synthesis](https://arxiv.org/abs/2510.00373)
*Carlo Bosio,Matteo Guarrera,Alberto Sangiovanni-Vincentelli,Mark W. Mueller*

Main category: cs.LG

TL;DR: 为解决LLM生成控制策略时结构与参数耦合导致的效率低下问题，本文提出一种混合方法，将LLM的结构合成与独立数值优化器的参数优化解耦，从而在控制任务中实现了更高的回报和样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在生成符号控制策略时，无法将策略的功能结构与其参数数值分离，导致搜索过程缓慢且低效。

Method: 本文提出一种混合方法，通过引入额外的局部参数搜索优化层，将结构合成与参数优化解耦。LLM负责迭代程序的功能结构，而一个独立的优化循环则用于提取LLM生成的程序中的数值参数并进行数值优化，以找到局部最优参数集并最大化任务性能。

Result: 在多项控制任务上的评估表明，该方法相较于纯粹由LLM引导的搜索，实现了更高的回报和改进的样本效率。

Conclusion: 结合符号程序合成与数值优化可以生成既可解释又高性能的策略，成功弥合了语言模型引导设计与经典控制调优之间的差距。

Abstract: Large Language models (LLMs) have shown promise as generators of symbolic
control policies, producing interpretable program-like representations through
iterative search. However, these models are not capable of separating the
functional structure of a policy from the numerical values it is parametrized
by, thus making the search process slow and inefficient. We propose a hybrid
approach that decouples structural synthesis from parameter optimization by
introducing an additional optimization layer for local parameter search. In our
method, the numerical parameters of LLM-generated programs are extracted and
optimized numerically to maximize task performance. With this integration, an
LLM iterates over the functional structure of programs, while a separate
optimization loop is used to find a locally optimal set of parameters
accompanying candidate programs. We evaluate our method on a set of control
tasks, showing that it achieves higher returns and improved sample efficiency
compared to purely LLM-guided search. We show that combining symbolic program
synthesis with numerical optimization yields interpretable yet high-performing
policies, bridging the gap between language-model-guided design and classical
control tuning. Our code is available at
https://sites.google.com/berkeley.edu/colmo.

</details>


### [221] [GDLNN: Marriage of Programming Language and Neural Networks for Accurate and Easy-to-Explain Graph Classification](https://arxiv.org/abs/2510.00374)
*Minseok Jeon,Seunghyun Park*

Main category: cs.LG

TL;DR: 本文提出GDLNN，一个结合领域特定语言GDL与神经网络的图机器学习架构，在图分类任务中生成可解释的图表示，并实现高精度、高质量解释和低成本。


<details>
  <summary>Details</summary>
Motivation: 开发一种新的图机器学习架构，旨在生成富有表达力且可解释的图表示，从而提升图分类任务的性能和模型预测的解释性。

Method: GDLNN架构将领域特定编程语言GDL与神经网络相结合。其核心是GDL层，该层负责生成可解释的图表示。之后，将这些表示应用于图分类任务，并可直接使用现有模型解释技术来解释其预测。

Result: GDLNN在大多数图分类基准数据集上实现了高精度，优于GNNs等主流图学习方法。通过应用现有模型解释技术，GDLNN能够提供高质量的预测解释。此外，即使包含解释成本在内，GDLNN的总成本也较低。

Conclusion: GDLNN是一种有效的图分类方法，通过其独特的GDL层生成的可解释图表示，在准确性、可解释性和成本效益方面均优于现有主流图学习方法。

Abstract: We present GDLNN, a new graph machine learning architecture, for graph
classification tasks. GDLNN combines a domain-specific programming language,
called GDL, with neural networks. The main strength of GDLNN lies in its GDL
layer, which generates expressive and interpretable graph representations.
Since the graph representation is interpretable, existing model explanation
techniques can be directly applied to explain GDLNN's predictions. Our
evaluation shows that the GDL-based representation achieves high accuracy on
most graph classification benchmark datasets, outperforming dominant graph
learning methods such as GNNs. Applying an existing model explanation technique
also yields high-quality explanations of GDLNN's predictions. Furthermore, the
cost of GDLNN is low when the explanation cost is included.

</details>


### [222] [Multidimensional Bayesian Active Machine Learning of Working Memory Task Performance](https://arxiv.org/abs/2510.00375)
*Dom CP Marticorena,Chris Wissmann,Zeyu Lu,Dennis L Barbour*

Main category: cs.LG

TL;DR: 本文验证了一种贝叶斯、双轴、主动分类的自适应实验设计方法，用于工作记忆任务，该方法高效、能揭示个体差异，并与传统一维方法性能相当。


<details>
  <summary>Details</summary>
Motivation: 大多数认知实验仍局限于控制单一因素并用标量总结性能，未能充分利用自适应实验设计在多维分析上的潜力。

Method: 本文提出并验证了一种贝叶斯、双轴、主动分类方法，在一个沉浸式虚拟环境中进行5x5工作记忆重建任务。该方法控制空间负荷（L）和特征结合负荷（K）两个变量。刺激获取由非参数高斯过程（GP）概率分类器的后验不确定性引导，输出关于（L，K）的性能表面。研究将GP驱动的自适应模式（AM）与传统的仅在K=3时改变L的阶梯式经典模式（CM）在年轻成人群体中进行了比较。

Result: AM模式在K=3时与CM模式在该队列中达到等效性，组内相关系数为0.755。此外，AM揭示了空间负荷与特征结合之间相互作用的个体差异。AM估计收敛速度比其他采样策略更快，仅需约30个样本即可准确拟合完整模型。

Conclusion: 该贝叶斯双轴自适应方法在认知实验中展现出与传统方法相当的性能，同时提供了更丰富的个体差异和多维交互信息，并具有显著的采样效率。这为更全面、深入的认知负荷分析提供了有效工具。

Abstract: While adaptive experimental design has outgrown one-dimensional,
staircase-based adaptations, most cognitive experiments still control a single
factor and summarize performance with a scalar. We show a validation of a
Bayesian, two-axis, active-classification approach, carried out in an immersive
virtual testing environment for a 5-by-5 working-memory reconstruction task.
Two variables are controlled: spatial load L (number of occupied tiles) and
feature-binding load K (number of distinct colors) of items. Stimulus
acquisition is guided by posterior uncertainty of a nonparametric Gaussian
Process (GP) probabilistic classifier, which outputs a surface over (L, K)
rather than a single threshold or max span value. In a young adult population,
we compare GP-driven Adaptive Mode (AM) with a traditional adaptive staircase
Classic Mode (CM), which varies L only at K = 3. Parity between the methods is
achieved for this cohort, with an intraclass coefficient of 0.755 at K = 3.
Additionally, AM reveals individual differences in interactions between spatial
load and feature binding. AM estimates converge more quickly than other
sampling strategies, demonstrating that only about 30 samples are required for
accurate fitting of the full model.

</details>


### [223] [Composer: A Search Framework for Hybrid Neural Architecture Design](https://arxiv.org/abs/2510.00379)
*Bilge Acun,Prasoon Sinha,Newsha Ardalani,Sangmin Bae,Alicia Golden,Chien-Yu Lin,Meghana Madhyastha,Fei Sun,Neeraja J. Yadwadkar,Carole-Jean Wu*

Main category: cs.LG

TL;DR: 本文提出Composer框架，通过模块化架构搜索和缩放策略，自动发现优于Llama 3.2的新型混合LLM架构，并在效率和性能上均有提升。


<details>
  <summary>Details</summary>
Motivation: 现有混合模型架构设计依赖手动探索，但由于设计空间巨大和训练成本高昂，难以发现结合关键计算原语的优秀架构。

Method: 开发了Composer框架，该框架以模块化方式在小规模上探索模型架构，并使用提出的缩放策略将表现最佳的架构推广到大规模。

Result: 使用Composer发现的新型混合LLM架构在3.5亿-30亿参数规模上持续降低验证损失，并在下游任务中将评估准确率提高2.8-8.3%（平均1.1-3.1%），同时提升训练和推理效率，超越Llama 3.2和现有最佳基线。

Conclusion: Composer通过自动搜索成功找到了性能和效率均优于当前SOTA模型的混合LLM架构，证明了其在模型架构设计中的有效性。

Abstract: Hybrid model architectures that combine computational primitives (e.g.,
Attention, MLP) in different ratios have shown promising performance beyond
Transformers. Some studies have shown that different interleavings of
primitives can affect model quality as well. However, prior works explore the
hybrid model architecture design space manually. Due to the large design space
and training costs, discovering hybrid models that combine key computational
primitives for pre-training is challenging. In this work, we take a principled
approach in designing a modular hybrid model architecture search framework --
Composer. Composer explores model architectures at a small scale and
extrapolates the top-performing model architectures to a larger scale using our
proposed scaling strategies. Using Composer, we discover new hybrid LLM
architectures that outperform Llama 3.2. Compared to Llama 3.2 and previous
state-of-the-art baselines, the new model architectures consistently reduce
validation loss at parameter scales of 350M-3B and improve evaluation accuracy
on the downstream tasks by up to 2.8-8.3% (1.1-3.1% on average) while improving
both training and inference efficiency.

</details>


### [224] [Efficient Probabilistic Tensor Networks](https://arxiv.org/abs/2510.00382)
*Marawan Gamal Abdel Hameed,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: 提出一种高效且数值稳定的概率张量网络（PTN）学习方法，解决了现有方法的计算开销大或数值不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有概率张量网络（PTN）参数学习方法计算开销大、不完全兼容自动微分框架，或数值不稳定。

Method: 提出一种概念简单、高效且数值稳定的方法来学习概率张量网络（PTN）。

Result: 该方法显著改善了时间与空间复杂度，在MNIST数据集上生成模型的延迟降低10倍；在多种密度估计基准测试中，能够学习比现有方法多10倍变量的分布。

Conclusion: 所提出的方法为概率张量网络提供了高效、数值稳定且可扩展的学习能力，在生成建模和密度估计任务中表现出色。

Abstract: Tensor networks (TNs) enable compact representations of large tensors through
shared parameters. Their use in probabilistic modeling is particularly
appealing, as probabilistic tensor networks (PTNs) allow for tractable
computation of marginals. However, existing approaches for learning parameters
of PTNs are either computationally demanding and not fully compatible with
automatic differentiation frameworks, or numerically unstable. In this work, we
propose a conceptually simple approach for learning PTNs efficiently, that is
numerically stable. We show our method provides significant improvements in
time and space complexity, achieving 10x reduction in latency for generative
modeling on the MNIST dataset. Furthermore, our approach enables learning of
distributions with 10x more variables than previous approaches when applied to
a variety of density estimation benchmarks. Our code is publicly available at
github.com/marawangamal/ptn.

</details>


### [225] [Learning Passive Continuous-Time Dynamics with Multistep Port-Hamiltonian Gaussian Processes](https://arxiv.org/abs/2510.00384)
*Chi Ho Leung,Philip E. Paré*

Main category: cs.LG

TL;DR: 提出多步港口哈密顿高斯过程（MS-PHS GP），可从噪声、非规则采样轨迹中学习物理一致的连续时间动力学及哈密顿量后验，并确保能量平衡和无源性。


<details>
  <summary>Details</summary>
Motivation: 从含噪声、非规则采样的轨迹中学习物理一致的连续时间动力学和哈密顿量后验，同时确保系统固有的能量平衡和无源性，并避免潜在状态。

Method: 引入MS-PHS GP。通过在哈密顿曲面上设置高斯过程先验，并将可变步多步积分器约束编码为有限线性泛函，实现矢量场和哈密顿曲面的闭式条件化，无需潜在状态。该方法设计上强制了能量平衡和无源性。此外，还提出了一个分离估计和离散化项的有限样本矢量场边界。

Result: 在质量-弹簧、范德波尔和杜芬等基准测试中，MS-PHS GP展现出改进的矢量场恢复能力和良好校准的哈密顿量不确定性。

Conclusion: MS-PHS GP是一种有效学习物理一致动力学和哈密顿量的方法，它能够从噪声数据中实现优越的矢量场恢复和准确的哈密顿量不确定性量化。

Abstract: We propose the multistep port-Hamiltonian Gaussian process (MS-PHS GP) to
learn physically consistent continuous-time dynamics and a posterior over the
Hamiltonian from noisy, irregularly-sampled trajectories. By placing a GP prior
on the Hamiltonian surface $H$ and encoding variable-step multistep integrator
constraints as finite linear functionals, MS-PHS GP enables closed-form
conditioning of both the vector field and the Hamiltonian surface without
latent states, while enforcing energy balance and passivity by design. We state
a finite-sample vector-field bound that separates the estimation and
variable-step discretization terms. Lastly, we demonstrate improved
vector-field recovery and well-calibrated Hamiltonian uncertainty on
mass-spring, Van der Pol, and Duffing benchmarks.

</details>


### [226] [Train on Validation (ToV): Fast data selection with applications to fine-tuning](https://arxiv.org/abs/2510.00386)
*Ayush Jain,Andrea Montanari,Eren Sasoglu*

Main category: cs.LG

TL;DR: 提出一种新颖的数据选择方法，通过颠倒训练集和验证集的角色，选择预测变化最大的训练样本进行微调，该方法更简单、更快，并在指令微调和命名实体识别任务上优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 在微调阶段，当目标分布样本稀缺时，选择最能反映目标分布的训练样本至关重要。现有数据选择方法复杂且可能耗时，它们通过在验证集上推断来估计训练池中单个样本的影响。

Method: 本文提出一种颠倒训练集和验证集角色的方法：在用验证集微调之前和之后，对训练池进行推断，然后选择预测变化最大的训练样本。核心洞察是，受少量验证集微调影响最大的训练样本，最有利于减少目标分布上的测试损失。

Result: 在指令微调和命名实体识别任务上的实验表明，在大多数情况下，本文方法比现有先进方法取得了更低的测试对数损失，并得到了理论分析的支持。

Conclusion: 本文提出了一种更简单、更快且更有效的数据选择方法，通过识别受微调影响最大的训练样本，显著提高了在目标样本稀缺情况下的模型微调性能，优于现有最先进方法。

Abstract: State-of-the-art machine learning often follows a two-stage process:
$(i)$~pre-training on large, general-purpose datasets; $(ii)$~fine-tuning on
task-specific data. In fine-tuning, selecting training examples that closely
reflect the target distribution is crucial. However, it is often the case that
only a few samples are available from the target distribution. Existing data
selection methods treat these target samples as a validation set and estimate
the effect of adding or removing a single sample from the training pool by
performing inference on the validation set.
  We propose a simpler and faster alternative that inverts the usual role of
train and validation: we perform inference on the training pool before and
after fine-tuning on the validation set. We then select samples whose
predictions change the most. Our key insight is that the training samples most
affected by fine-tuning on a small validation set tend to be the most
beneficial for reducing test loss on the target distribution. Experiments on
instruction tuning and named entity recognition tasks show that, in most cases,
our method achieves lower test log-loss than state-of-the-art approaches. We
support our findings with theoretical analysis.

</details>


### [227] [Bayesian Distributional Models of Executive Functioning](https://arxiv.org/abs/2510.00387)
*Robert Kasumba,Zeyu Lu,Dom CP Marticorena,Mingyang Zhong,Paul Beggs,Anja Pahor,Geetha Ramani,Imani Goffney,Susanne M Jaeggi,Aaron R Seitz,Jacob R Gardner,Dennis L Barbour*

Main category: cs.LG

TL;DR: 该研究引入了DLVM进行参数估计和DALE进行自适应采样，证明二者结合能更高效地进行认知评估，尤其在数据稀疏或不完整的情况下。


<details>
  <summary>Details</summary>
Motivation: 解决认知评估中，在数据稀疏或不完整条件下准确进行参数估计的挑战，并提高认知评估的效率。

Method: 1. 提出DLVM（深度潜在变量模型），整合多项执行功能任务和个体的观察数据，用于参数估计。2. 引入DALE（动态自适应学习引擎），自适应地引导采样以最大化信息增益。3. 将DLVM与IMLE进行性能比较，并将DALE与随机采样及固定测试组进行比较。

Result: 1. DLVM在参数估计方面持续优于IMLE，尤其是在数据量较小时，且能更快地收敛到高精度的真实分布估计。2. DALE通过自适应采样，在信息增益方面优于随机采样和固定测试组，特别是在前80次试验中表现突出。

Conclusion: 结合DLVM的跨任务推理能力与DALE的最优自适应采样，为更高效的认知评估提供了坚实的理论基础。

Abstract: Estimation (IMLE). DLVM integrates observations across multiple executive
function tasks and individuals, allowing parameter estimation even under sparse
or incomplete data conditions. DLVM consistently outperformed IMLE, especially
under with smaller amounts of data, and converges faster to highly accurate
estimates of the true distributions. In a second set of analyses, DALE
adaptively guided sampling to maximize information gain, outperforming random
sampling and fixed test batteries, particularly within the first 80 trials.
These findings establish the advantages of combining DLVMs cross-task inference
with DALEs optimal adaptive sampling, providing a principled basis for more
efficient cognitive assessments.

</details>


### [228] [Graph2Region: Efficient Graph Similarity Learning with Structure and Scale Restoration](https://arxiv.org/abs/2510.00394)
*Zhouyang Liu,Yixin Chen,Ning Liu,Jiezhong He,Dongsheng Li*

Main category: cs.LG

TL;DR: G2R是一种几何图嵌入方法，将节点表示为封闭区域，通过区域形状和体积捕获图结构和大小。它利用区域重叠近似MCS，区域不相交部分近似GED，实现了MCS和GED的同步高效计算，并在图相似度任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 图相似度计算（如MCS和GED）对图相关任务至关重要，但其精确计算是NP-Hard问题。现有神经网络方法虽能缓解计算负担，却存在节点比较昂贵或未能有效利用图结构和规模信息的局限性。

Method: 提出了一种名为Graph2Region (G2R) 的新型几何图嵌入方法。G2R将图节点表示为封闭区域，并在嵌入空间中恢复其邻接模式。通过整合节点特征和邻接模式，G2R将图总结为图区域嵌入，其中区域形状捕获图结构，体积反映图大小。通过图区域的重叠部分近似MCS，通过不相交部分作为GED相似度的代理。该方法支持MCS和GED的同步计算，并结合了局部与全局结构信息。

Result: 实验评估显示，G2R在图相似度计算中表现出竞争力。在MCS相似度学习方面，它比现有最先进方法实现了高达60.0%的相对精度提升，并保持了训练和推理的高效率。此外，G2R在同时预测MCS和GED相似度方面展现出卓越能力。

Conclusion: G2R通过新颖的几何嵌入方式，成功解决了传统图相似度计算的复杂性及现有神经网络方法的局限性。它能高效准确地近似MCS和GED，并支持同时预测，为图相似度提供了全面的评估，具有显著的性能优势。

Abstract: Graph similarity is critical in graph-related tasks such as graph retrieval,
where metrics like maximum common subgraph (MCS) and graph edit distance (GED)
are commonly used. However, exact computations of these metrics are known to be
NP-Hard. Recent neural network-based approaches approximate the similarity
score in embedding spaces to alleviate the computational burden, but they
either involve expensive pairwise node comparisons or fail to effectively
utilize structural and scale information of graphs. To tackle these issues, we
propose a novel geometric-based graph embedding method called Graph2Region
(G2R). G2R represents nodes as closed regions and recovers their adjacency
patterns within graphs in the embedding space. By incorporating the node
features and adjacency patterns of graphs, G2R summarizes graph regions, i.e.,
graph embeddings, where the shape captures the underlying graph structures and
the volume reflects the graph size. Consequently, the overlap between graph
regions can serve as an approximation of MCS, signifying similar node regions
and adjacency patterns. We further analyze the relationship between MCS and GED
and propose using disjoint parts as a proxy for GED similarity. This analysis
enables concurrent computation of MCS and GED, incorporating local and global
structural information. Experimental evaluation highlights G2R's competitive
performance in graph similarity computation. It achieves up to a 60.0\%
relative accuracy improvement over state-of-the-art methods in MCS similarity
learning, while maintaining efficiency in both training and inference.
Moreover, G2R showcases remarkable capability in predicting both MCS and GED
similarities simultaneously, providing a holistic assessment of graph
similarity. Code available at https://github.com/liuzhouyang/Graph2Region.

</details>


### [229] [Can Mamba Learn In Context with Outliers? A Theoretical Generalization Analysis](https://arxiv.org/abs/2510.00399)
*Hongkang Li,Songtao Lu,Xiaodong Cui,Pin-Yu Chen,Meng Wang*

Main category: cs.LG

TL;DR: 本文首次对单层Mamba模型的训练动态和上下文学习（ICL）泛化能力进行理论分析，揭示其对提示中离群值的鲁棒性，并与线性Transformer进行比较。


<details>
  <summary>Details</summary>
Motivation: Mamba模型在语言任务中表现出与Transformer相当的性能和ICL能力，并具有计算优势。然而，由于其门控机制的非线性，Mamba的理论理解，尤其是在ICL方面的理论理解仍非常有限。

Method: 对包含线性注意力组件和非线性门控层的单层Mamba模型进行理论分析，研究其训练动态及在包含加性离群值的未见二元分类任务上的ICL泛化能力。同时，在相同设置下与线性Transformer进行分析和比较。理论发现辅以经验实验验证。

Result: 分析表明，Mamba利用线性注意力层选择信息丰富的上下文示例，并使用非线性门控层抑制离群值的影响。尽管Mamba可能需要更多的训练迭代才能收敛，但即使离群值比例超过线性Transformer的容忍阈值，它仍能保持准确预测。

Conclusion: Mamba模型凭借其独特的架构（线性注意力与非线性门控结合），在上下文学习中对提示中的离群值表现出卓越的鲁棒性，优于线性Transformer，尽管可能需要更长的训练时间。这些理论见解得到了实验支持。

Abstract: The Mamba model has gained significant attention for its computational
advantages over Transformer-based models, while achieving comparable
performance across a wide range of language tasks. Like Transformers, Mamba
exhibits in-context learning (ICL) capabilities, i.e., making predictions for
new tasks based on a prompt containing input-label pairs and a query, without
requiring fine-tuning. Despite its empirical success, the theoretical
understanding of Mamba remains limited, largely due to the nonlinearity
introduced by its gating mechanism. To the best of our knowledge, this paper
presents the first theoretical analysis of the training dynamics of a one-layer
Mamba model, which consists of a linear attention component followed by a
nonlinear gating layer, and its ICL generalization on unseen binary
classification tasks, even when the prompt includes additive outliers. Our
analysis shows that Mamba leverages the linear attention layer to select
informative context examples and uses the nonlinear gating layer to suppress
the influence of outliers. By establishing and comparing to the analysis of
linear Transformers under the same setting, we show that although Mamba may
require more training iterations to converge, it maintains accurate predictions
even when the proportion of outliers exceeds the threshold that a linear
Transformer can tolerate. These theoretical findings are supported by empirical
experiments.

</details>


### [230] [Hierarchy-Aware Neural Subgraph Matching with Enhanced Similarity Measure](https://arxiv.org/abs/2510.00402)
*Zhouyang Liu,Ning Liu,Yixin Chen,Jiezhong He,Menghan Jia,Dongsheng Li*

Main category: cs.LG

TL;DR: NC-Iso是一种新的GNN架构，通过保留特征的相对位置和引入新的相似度度量，解决了现有神经子图匹配方法中规模差异导致的错误预测和距离度量判别力不足的问题，从而提高了子图匹配的准确性和排序能力。


<details>
  <summary>Details</summary>
Motivation: 子图匹配因耗时的组合搜索而具有挑战性。虽然现有基于GNN的方法缩短了响应时间，但它们在编码过程中存在图对之间的尺度差异问题（忽略特征的相对位置），导致包含约束被扰乱和错误预测。此外，它们的铰链距离度量对于匹配的图对缺乏判别力，影响了排序应用。

Method: 本文提出了NC-Iso，一种新颖的GNN架构。它通过构建节点根子树内相邻层级之间的层次依赖关系来保留特征的相对位置，确保匹配的图对在特征计数符合包含约束的同时保持一致的层次结构。为增强匹配对的排序能力，引入了一种新的相似度优势比增强度量，用于量化图对之间相似度相对于不相似度的优势。

Result: 在九个数据集上的实证结果验证了NC-Iso的有效性、泛化能力、可伸缩性和可移植性，同时保持了时间效率，为子图检索提供了一个更具判别力的神经子图匹配解决方案。

Conclusion: NC-Iso通过解决现有方法的关键缺陷，提供了一种高效、判别力强的神经子图匹配解决方案，特别适用于子图检索。

Abstract: Subgraph matching is challenging as it necessitates time-consuming
combinatorial searches. Recent Graph Neural Network (GNN)-based approaches
address this issue by employing GNN encoders to extract graph information and
hinge distance measures to ensure containment constraints in the embedding
space. These methods significantly shorten the response time, making them
promising solutions for subgraph retrieval. However, they suffer from scale
differences between graph pairs during encoding, as they focus on feature
counts but overlook the relative positions of features within node-rooted
subtrees, leading to disturbed containment constraints and false predictions.
Additionally, their hinge distance measures lack discriminative power for
matched graph pairs, hindering ranking applications. We propose NC-Iso, a novel
GNN architecture for neural subgraph matching. NC-Iso preserves the relative
positions of features by building the hierarchical dependencies between
adjacent echelons within node-rooted subtrees, ensuring matched graph pairs
maintain consistent hierarchies while complying with containment constraints in
feature counts. To enhance the ranking ability for matched pairs, we introduce
a novel similarity dominance ratio-enhanced measure, which quantifies the
dominance of similarity over dissimilarity between graph pairs. Empirical
results on nine datasets validate the effectiveness, generalization ability,
scalability, and transferability of NC-Iso while maintaining time efficiency,
offering a more discriminative neural subgraph matching solution for subgraph
retrieval. Code available at https://github.com/liuzhouyang/NC-Iso.

</details>


### [231] [AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features](https://arxiv.org/abs/2510.00404)
*Xudong Zhu,Mohammad Mahdi Khalili,Zhihui Zhu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Sparse autoencoders (SAEs) have emerged as powerful techniques for
interpretability of large language models (LLMs), aiming to decompose hidden
states into meaningful semantic features. While several SAE variants have been
proposed, there remains no principled framework to derive SAEs from the
original dictionary learning formulation. In this work, we introduce such a
framework by unrolling the proximal gradient method for sparse coding. We show
that a single-step update naturally recovers common SAE variants, including
ReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation
of existing SAEs: their sparsity-inducing regularizers enforce non-negativity,
preventing a single feature from representing bidirectional concepts (e.g.,
male vs. female). This structural constraint fragments semantic axes into
separate, redundant features, limiting representational completeness. To
address this issue, we propose AbsTopK SAE, a new variant derived from the
$\ell_0$ sparsity constraint that applies hard thresholding over the
largest-magnitude activations. By preserving both positive and negative
activations, AbsTopK uncovers richer, bidirectional conceptual representations.
Comprehensive experiments across four LLMs and seven probing and steering tasks
show that AbsTopK improves reconstruction fidelity, enhances interpretability,
and enables single features to encode contrasting concepts. Remarkably, AbsTopK
matches or even surpasses the Difference-in-Mean method, a supervised approach
that requires labeled data for each concept and has been shown in prior work to
outperform SAEs.

</details>


### [232] [Learning a Zeroth-Order Optimizer for Fine-Tuning LLMs](https://arxiv.org/abs/2510.00419)
*Kairun Zhang,Haoyu Li,Yanjun Zhao,Yifan Sun,Huan Zhang*

Main category: cs.LG

TL;DR: ZO Fine-tuner是一种学习型零阶优化器，通过自动学习高效扰动策略，实现了大型语言模型（LLM）的内存高效微调，并在多项任务和模型上超越现有零阶基线。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化器采用手工设计、静态的采样策略，缺乏对模型结构的适应性。研究基于基础模型数量有限且广泛使用的观察，旨在实现优化器的一次性学习并可重复用于多种下游任务。

Method: 提出ZO Fine-tuner，这是一种学习型零阶优化器，采用紧凑且内存高效的设计，自动学习高效的扰动策略。它旨在将“学习去学习”（L2L）方法扩展到基础模型时代，支持对每个LLM进行一次性训练，且开销极小。

Result: 在4个LLM和7个数据集上的实验表明，ZO Fine-tuner在82.1%的任务-模型组合中优于现有的零阶基线方法。

Conclusion: ZO Fine-tuner展示了在高效LLM微调方面的强大性能和可扩展性。

Abstract: Zeroth-order optimizers have recently emerged as a practical approach for
fine-tuning large language models (LLMs), significantly reducing GPU memory
consumption compared to traditional first-order methods. Yet, existing
zeroth-order methods rely on hand-crafted, static sampling strategies that are
not adaptable to model-specific structures. To address this, we propose ZO
Fine-tuner, a learning-based zeroth-order optimizer for LLMs that automatically
learns efficient perturbation strategies through a compact and memory-efficient
design. Crucially, our approach is motivated by the observation that only a
small number of foundation models and their derivatives are widely adopted in
practice. Therefore, learning the optimizer once for a given LLM and reusing it
across diverse downstream tasks is both feasible and highly desirable.
Accordingly, ZO Fine-tuner is designed to scale learning to learn (L2L) to the
foundation-model era by supporting one-time training per LLM with minimal
overhead. Experiments on 4 LLMs and 7 datasets show that ZO Fine-tuner
outperforms prior zeroth-order baselines in 82.1\% of task-model combinations,
thereby demonstrating strong performance and scalability for efficient LLM
fine-tuning. Our code is available at
https://github.com/ASTRAL-Group/ZO_Fine_tuner.git.

</details>


### [233] [Automated Structured Radiology Report Generation with Rich Clinical Context](https://arxiv.org/abs/2510.00428)
*Seongjae Kang,Dong Bok Lee,Juho Jung,Dongseop Kim,Won Hwa Kim,Sunghoon Joo*

Main category: cs.LG

TL;DR: 现有结构化放射报告生成（SRRG）系统忽略临床上下文，导致生成错误。本文提出情境化SRRG（C-SRRG），通过整合多源临床上下文（包括多视图图像、临床指征、成像技术和既往研究）显著提高了报告生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有自动化结构化放射报告生成（SRRG）系统未能有效利用临床上下文，而放射科医生在诊断时会充分利用这些信息。这种缺失导致了报告生成中的关键问题，例如引用不存在临床上下文时产生“时间幻觉”，从而影响报告的准确性、清晰度和一致性。

Method: 本文提出情境化SRRG（C-SRRG）方法，通过全面整合丰富的临床上下文来改进SRRG。作者为此构建了一个C-SRRG数据集，该数据集整合了包括多视图X射线图像、临床指征、成像技术以及基于患者病史的既往研究和比较信息。研究通过与最先进的多模态大型语言模型进行广泛基准测试来评估C-SRRG的性能。

Result: 通过将临床上下文整合到所提出的C-SRRG框架中，报告生成质量得到了显著提升。

Conclusion: 整合临床上下文的C-SRRG方法能够有效解决现有SRRG系统缺乏临床情境的问题，显著提高了报告生成质量。该研究公开了数据集、代码和模型检查点，以促进未来在临床对齐自动化放射报告生成领域的研究。

Abstract: Automated structured radiology report generation (SRRG) from chest X-ray
images offers significant potential to reduce workload of radiologists by
generating reports in structured formats that ensure clarity, consistency, and
adherence to clinical reporting standards. While radiologists effectively
utilize available clinical contexts in their diagnostic reasoning, existing
SRRG systems overlook these essential elements. This fundamental gap leads to
critical problems including temporal hallucinations when referencing
non-existent clinical contexts. To address these limitations, we propose
contextualized SRRG (C-SRRG) that comprehensively incorporates rich clinical
context for SRRG. We curate C-SRRG dataset by integrating comprehensive
clinical context encompassing 1) multi-view X-ray images, 2) clinical
indication, 3) imaging techniques, and 4) prior studies with corresponding
comparisons based on patient histories. Through extensive benchmarking with
state-of-the-art multimodal large language models, we demonstrate that
incorporating clinical context with the proposed C-SRRG significantly improves
report generation quality. We publicly release dataset, code, and checkpoints
to facilitate future research for clinically-aligned automated RRG at
https://github.com/vuno/contextualized-srrg.

</details>


### [234] [Plug-and-Play Prompt Refinement via Latent Feedback for Diffusion Model Alignment](https://arxiv.org/abs/2510.00430)
*Suhyeon Lee,Jong Chul Ye*

Main category: cs.LG

TL;DR: PromptLoop是一个即插即用的强化学习框架，通过MLLM基于扩散模型中间潜在状态迭代更新提示，解决了现有RL微调扩散模型在泛化性、组合性和鲁棒性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习（RL）的扩散模型微调方法在泛化性、组合性及对抗奖励作弊方面存在局限。目前的提示精炼方法多采用前馈方式，未能充分利用RL的顺序特性。

Method: 引入PromptLoop框架，该框架将潜在反馈整合到分步提示精炼中。它不修改扩散模型权重，而是训练一个多模态大语言模型（MLLM）使用RL，根据扩散模型的中间潜在状态迭代更新提示。

Result: 实验证明PromptLoop能有效优化奖励、无缝泛化到未见模型、与现有对齐方法正交组合，并能缓解过度优化和奖励作弊问题。

Conclusion: PromptLoop通过结合潜在反馈和分步提示精炼，为扩散模型提供了一个有效的RL对齐框架，显著提升了泛化性、组合性与鲁棒性。

Abstract: Despite the recent progress, reinforcement learning (RL)-based fine-tuning of
diffusion models often struggles with generalization, composability, and
robustness against reward hacking. Recent studies have explored prompt
refinement as a modular alternative, but most adopt a feed-forward approach
that applies a single refined prompt throughout the entire sampling trajectory,
thereby failing to fully leverage the sequential nature of reinforcement
learning. To address this, here we introduce PromptLoop, a plug-and-play RL
framework that incorporates latent feedback into step-wise prompt refinement.
Rather than modifying diffusion model weights, a multimodal large language
model (MLLM) is trained with RL to iteratively update prompts based on
intermediate latent states of diffusion models. This design achieves a
structural analogy to the Diffusion RL approach, while retaining the
flexibility and generality of prompt-based alignment. Extensive experiments
across diverse reward functions and diffusion backbones demonstrate that
PromptLoop (i) achieves effective reward optimization, (ii) generalizes
seamlessly to unseen models, (iii) composes orthogonally with existing
alignment methods, and (iv) mitigates over-optimization and reward hacking.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [235] [Wireless Laser Power Transfer for Low-altitude Uncrewed Aerial Vehicle-assisted Internet of Things: Paradigms, Challenges, and Solutions](https://arxiv.org/abs/2510.00477)
*Chengzhen Li,Likun Zhang,Chuang Zhang,Jiahui Li,Changyuan Zhao,Ruichen Zhang,Geng Sun*

Main category: cs.NI

TL;DR: 本文探讨了无线电能传输（WLPT）作为一种变革性解决方案，用于解决无人机辅助物联网（UAV-IoT）网络中的能量限制问题，并通过多智能体强化学习框架提升了能量可持续性和数据新鲜度。


<details>
  <summary>Details</summary>
Motivation: 低空无人机在物联网中面临能源限制，影响其运行能力。地面传感器也存在能源约束。因此，需要可持续的能源供应方案来提升UAV-IoT网络的性能。

Method: 首先系统性研究了WLPT的基本原理并分析了其优势。然后提出了三种系统集成操作范式，识别挑战并讨论解决方案。在一个案例研究中，提出了一种多智能体强化学习框架来解决WLPT-enabled UAV-IoT数据收集中的协调和优化问题。

Result: 仿真结果表明，所提出的多智能体强化学习框架显著提高了UAV-IoT网络的能量可持续性（Energy Sustainability）和数据新鲜度（Data Freshness）。

Conclusion: WLPT是UAV-IoT网络可持续能源供应的变革性解决方案。通过多智能体强化学习框架可以有效解决协调与优化挑战，提升网络性能。文章还讨论了未来的研究方向。

Abstract: Low-altitude uncrewed aerial vehicles (UAVs) have become integral enablers
for the Internet of Things (IoT) by offering enhanced coverage, improved
connectivity and access to remote areas. A critical challenge limiting their
operational capacity lies in the energy constraints of both aerial platforms
and ground-based sensors. This paper explores WLPT as a transformative solution
for sustainable energy provisioning in UAV-assisted IoT networks. We first
systematically investigate the fundamental principles of WLPT and analysis the
comparative advantages. Then, we introduce three operational paradigms for
system integration, identify key challenges, and discuss corresponding
potential solutions. In case study, we propose a multi-agent reinforcement
learning framework to address the coordination and optimization challenges in
WLPT-enabled UAV-assisted IoT data collection. Simulation results demonstrate
that our framework significantly improves energy sustainability and data
freshness. Finally, we discuss some future directions.

</details>


### [236] [Make a Video Call with LLM: A Measurement Campaign over Five Mainstream Apps](https://arxiv.org/abs/2510.00481)
*Jiayang Xu,Xiangjie Huang,Zijie Li,Zili Meng*

Main category: cs.NI

TL;DR: 本文提出并使用了多维度基准来评估现有AI视频聊天系统的性能，揭示了其瓶颈并提出了未来优化方向。


<details>
  <summary>Details</summary>
Motivation: 2025年大型语言模型(LLM)推出AI视频聊天新功能，允许用户通过实时视频通信与AI代理互动，但目前缺乏对其性能的系统性研究。

Method: 提出了一个包含质量、延迟、内部机制和系统开销四个维度指标的综合基准，并使用定制测试平台评估了五种主流AI视频聊天机器人。

Result: 提供了实际性能的基线，并识别了独特的系统瓶颈。

Conclusion: 为研究社区提供了真实世界性能的基线，指出了系统瓶颈，并为AI视频聊天机器人的未来优化提出了若干研究问题。

Abstract: In 2025, Large Language Model (LLM) services have launched a new feature --
AI video chat -- allowing users to interact with AI agents via real-time video
communication (RTC), just like chatting with real people. Despite its
significance, no systematic study has characterized the performance of existing
AI video chat systems. To address this gap, this paper proposes a comprehensive
benchmark with carefully designed metrics across four dimensions: quality,
latency, internal mechanisms, and system overhead. Using custom testbeds, we
further evaluate five mainstream AI video chatbots with this benchmark. This
work provides the research community a baseline of real-world performance and
identifies unique system bottlenecks. In the meantime, our benchmarking results
also open up several research questions for future optimizations of AI video
chatbots.

</details>


### [237] [Dynamic Low Power Traffic Pattern for Energy Constrained Wireless Sensor Networks](https://arxiv.org/abs/2510.00588)
*Almamoon Alauthman*

Main category: cs.NI

TL;DR: 为解决无线传感器网络(WSNs)的能效问题，本研究提出了一种针对树形网络的节能策略，通过减少空闲侦听来降低功耗，从而延长网络寿命并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 无线传感器网络(WSNs)在关键应用中广泛使用，但节点依赖有限电池资源，因此保持能量效率是其面临的主要挑战。

Method: 本研究提出了一种针对具有动态流量模式的树形结构网络的节能策略。该方法通过减少节点不必要的空闲侦听状态（即节点在等待可能不会发生的数据传输时保持活跃）的时长和发生频率来降低功耗。

Result: 使用OMNeT++模拟器和MiXiM框架的仿真结果表明，该解决方案显著降低了能耗，增加了数据吞吐量，并提高了网络的整体效率和寿命。

Conclusion: 所提出的节能策略通过有效减少空闲侦听状态的能量浪费，成功延长了无线传感器网络的运行寿命，提升了数据吞吐量，并全面增强了网络效率和持久性。

Abstract: Wireless Sensor Networks (WSNs) are extensively utilized in critical
applications, including remote monitoring, target tracking, healthcare systems,
industrial automation, and smart control in both residential and industrial
settings. One of the primary challenges in these systems is maintaining energy
efficiency, given that most sensor nodes rely on limited battery resources. To
tackle this problem, this study introduces an energy-saving strategy designed
for tree-structured networks with dynamic traffic patterns. The approach
focuses on lowering power usage by decreasing the length and occurrence of idle
listening state where nodes remain active unnecessarily while waiting for data
transmissions that may never occur. By reducing this form of energy waste, the
proposed approach is designed to extend the operational lifetime and enhance
the throughput of the wireless sensor network. Simulation results obtained
using the OMNeT++ simulator with the MiXiM framework demonstrate that the
solution significantly reduces energy consumption, increases data throughput,
and improves overall network efficiency and longevity.

</details>


### [238] [Faster Offloads by Unloading them -- The RDMA Case](https://arxiv.org/abs/2510.00735)
*Georgia Fragkouli,Laurent Vanbever*

Main category: cs.NI

TL;DR: 现有卸载技术并非总能带来预期加速，本文提出“卸载”概念，使卸载任务可逆以进一步提升性能，并通过原型展示其在RDMA写入中高达31%的加速效果。


<details>
  <summary>Details</summary>
Motivation: 硬件和软件卸载技术（如RDMA、eBPF）旨在提升性能，但研究表明完全卸载并非总能带来预期的速度提升，因为任务被移动到更接近网络/链路层，这表明需要新的方法来优化卸载。

Method: 提出使卸载可逆的“卸载”机制，即部分将已卸载任务移回。本文初步聚焦于卸载RDMA写入操作，探讨卸载哪些部分、如何动态决策卸载或重新卸载路径以优化性能，以及如何保持兼容性。

Result: 通过目前的原型验证，卸载技术可将RDMA写入操作加速高达31%。

Conclusion: 卸载技术展示了巨大潜力，能够有效加速现有卸载操作，为提升系统性能提供了新途径。

Abstract: From hardware offloads like RDMA to software ones like eBPF, offloads are
everywhere and their value is in performance. However, there is evidence that
fully offloading -- even when feasible -- does not always give the expected
speedups. Starting from the observation that this is due to changes the
offloads make -- by moving tasks from the application/CPU closer to the
network/link layer -- we argue that to further accelerate offloads, we need to
make offloads reversible by unloading them -- moving back part of the offloaded
tasks.
  Unloading comes with a set of challenges that we start answering in this
paper by focusing on (offloaded) RDMA writes: which part of the write operation
does it make sense to unload? how do we dynamically decide which writes to
execute on the unload or offload path to improve performance? how do we
maintain compatibility between the two paths? Our current prototype shows the
potential of unloading by accelerating RDMA writes by up to 31%.

</details>


### [239] [Optimizing Version AoI in Energy-Harvesting IoT: Model-Based and Learning-Based Approaches](https://arxiv.org/abs/2510.00904)
*Erfan Delfani,Nikolaos Pappas*

Main category: cs.NI

TL;DR: 本文研究在能源收集物联网系统中，针对不同系统模型知识程度（已知、部分已知、未知），通过基于模型、基于估计和无模型的策略，优化语义指标版本信息年龄（VAoI）以实现高效数据传输。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的物联网（IoT）系统中，需要语义感知管理来最大化及时且信息丰富数据的传输。本文的动机是优化能源收集（EH）传感器状态更新系统中的语义指标版本信息年龄（VAoI）。

Method: 针对系统模型知识的三个级别（完全已知、部分已知、未知），提出了相应的优化策略：基于模型、基于估计和无模型方法。研究中采用了马尔可夫决策过程（MDP）和强化学习（RL）框架。

Result: 研究分析了在不同模型信息程度下优化策略的性能权衡。

Conclusion: 研究结果为在已知和未知物联网环境中设计高效且自适应的语义感知策略提供了指导。

Abstract: Efficient data transmission in resource-constrained Internet of Things (IoT)
systems requires semantics-aware management that maximizes the delivery of
timely and informative data. This paper investigates the optimization of the
semantic metric Version Age of Information (VAoI) in a status update system
comprising an energy-harvesting (EH) sensor and a destination monitoring node.
We consider three levels of knowledge about the system model -- fully known,
partially known, and unknown -- and propose corresponding optimization
strategies: model-based, estimation-based, and model-free methods. By employing
Markov Decision Process (MDP) and Reinforcement Learning (RL) frameworks, we
analyze performance trade-offs under varying degrees of model information. Our
findings provide guidance for designing efficient and adaptive semantics-aware
policies in both known and unknown IoT environments.

</details>


### [240] [Enhancing Urban VANETs Stability: A Single-Hop Clustering Strategy in Metropolitan Environments](https://arxiv.org/abs/2510.00939)
*Pouya Firouzmakan,Suprakash Datta*

Main category: cs.NI

TL;DR: 本文提出一种针对城市VANETs的高效聚类算法，利用公交车作为主簇头，以减少对路边单元的依赖并提高簇稳定性，该方法在不同传输范围下表现优越。


<details>
  <summary>Details</summary>
Motivation: 车联网(VANETs)是未来智能交通系统(ITSs)的关键，但其高度动态性给部署带来了挑战。聚类技术用于解决这些挑战并提高服务质量，研究旨在减少大都市地区对路边单元(RSUs)的依赖，同时提高簇稳定性。

Method: 提出一种针对城市环境的高效聚类算法。该方法利用现有大都市基础设施，将公共交通巴士指定为主要簇头(CHs)，以补偿RSU的缺失并最大程度减少额外基础设施依赖；独立车辆(SAVs)则动态选择额外的CHs。

Result: 通过全面的案例研究和与现有算法的比较分析，结果表明，所提出的方法在不同的传输范围(TRs)下均表现出优越的性能。

Conclusion: 该研究成功探索了在城市VANETs中减少对RSU依赖的可行性，并提出了一种利用公交车作为主簇头提高簇稳定性的有效聚类算法，其性能优于现有算法。

Abstract: Vehicular Ad-hoc Networks (VANETs), a subclass of Mobile Ad-hoc Networks
(MANETs), are expected to play a crucial role in the future of intelligent
transportation systems (ITSs). A key objective of VANETs is to enable efficient
and cost-effective communication among vehicles while supporting a large number
of network participants and minimizing infrastructure dependency. However, the
highly dynamic nature of vehicular networks poses significant challenges to
their deployment. Clustering techniques are employed to address these
challenges, with a strong emphasis on stability, as they directly influence the
routing process and enhance the quality of service (QoS). This paper explores
the feasibility of reducing reliance on roadside units (RSUs) in metropolitan
areas while improving cluster stability. We propose an efficient clustering
algorithm tailored for urban environments, leveraging existing metropolitan
infrastructure to compensate for the absence of RSUs. Our approach designates
public transportation buses as primary cluster heads (CHs), minimizing reliance
on additional infrastructure, while stand-alone vehicles (SAVs) dynamically
select additional CHs. Through comprehensive case studies and comparative
analysis with existing algorithms, our results demonstrate the superior
performance of the proposed method across different transmission ranges (TRs).

</details>


### [241] [Bridging the Gap Between Simulated and Real Network Data Using Transfer Learning](https://arxiv.org/abs/2510.00956)
*Carlos Güemes-Palau,Miquel Ferriol-Galmés,Jordi Paillisse-Vilanova,Albert López-Brescó,Pere Barlet-Ros,Albert Cabellos-Aparicio*

Main category: cs.NI

TL;DR: 为解决机器学习网络模型对真实数据需求高但获取难的问题，本文提出一种基于迁移学习的混合方法，通过少量真实数据微调预训练模型，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: ML网络模型需要大量训练数据，但真实网络数据获取成本高且有限（尤其在故障等关键场景）。依赖模拟数据会导致模型在真实环境中部署时精度降低。

Method: 提出一种利用迁移学习的混合方法，结合模拟数据和真实世界数据。具体使用RouteNet-Fermi模型，通过少量真实数据集对预训练模型进行微调。实验在OMNeT++和自定义测试平台进行。

Result: 数据包延迟预测的平均绝对百分误差 (MAPE) 降低高达88%。仅使用10个真实场景，MAPE下降37%；使用50个真实场景，MAPE下降48%。

Conclusion: 通过迁移学习结合模拟数据和少量真实数据的混合方法，可以显著提升ML网络模型在真实环境中的预测性能和准确性。

Abstract: Machine Learning (ML)-based network models provide fast and accurate
predictions for complex network behaviors but require substantial training
data. Collecting such data from real networks is often costly and limited,
especially for critical scenarios like failures. As a result, researchers
commonly rely on simulated data, which reduces accuracy when models are
deployed in real environments. We propose a hybrid approach leveraging transfer
learning to combine simulated and real-world data. Using RouteNet-Fermi, we
show that fine-tuning a pre-trained model with a small real dataset
significantly improves performance. Our experiments with OMNeT++ and a custom
testbed reduce the Mean Absolute Percentage Error (MAPE) in packet delay
prediction by up to 88%. With just 10 real scenarios, MAPE drops by 37%, and
with 50 scenarios, by 48%.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [242] [A Review of Software for Designing and Operating Quantum Networks](https://arxiv.org/abs/2510.00203)
*Robert J. Hayek,Joaquin Chung,Rajkumar Kettimuthu*

Main category: quant-ph

TL;DR: 该论文审查了量子网络软件实现的现状，发现理论与实践之间存在差距，尤其在动态拓扑和网络管理方面，并提出了未来发展路线图。


<details>
  <summary>Details</summary>
Motivation: 实现生产级量子网络需要针对量子系统独特约束的软件架构和协议，但从实验室演示到可部署网络仍面临挑战。

Method: 本文围绕基础设施、逻辑和控制/服务三层抽象，审查了用于设计和操作量子网络协议的现有软件实现，重点关注纠缠、拓扑和资源管理等控制/服务平面功能。

Result: 研究发现，理论协议提案与其在模拟器或测试台中的实现之间存在持续差距，尤其在动态拓扑和网络管理方面更为明显。

Conclusion: 文章总结了开放挑战，并提出了开发可扩展软件架构以实现混合、大规模量子网络的路线图。

Abstract: Quantum network protocol development is crucial to realizing a
production-grade network that can support distributed sensing, secure
communication, and utility-scale quantum computation. However, the transition
from laboratory demonstration to deployable networks requires software
implementations of architectures and protocols tailored to the unique
constraints of quantum systems. This paper reviews the current state of
software implementations for quantum networks, organized around the three-plane
abstraction of infrastructure, logical, and control/service planes. We cover
software for both designing quantum network protocols (e.g., SeQUeNCe, QuISP,
and NetSquid) and operating them, with a focus on essential control/service
plane functions such as entanglement, topology, and resource management, in a
proposed taxonomy. Our review highlights a persistent gap between theoretical
protocol proposals and their realization in simulators or testbeds,
particularly in dynamic topology and network management. We conclude by
outlining open challenges and proposing a roadmap for developing scalable
software architectures to enable hybrid, large-scale quantum networks.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [243] [IA aplicada al análisis del conflicto Irán-Israel: Mapeo de discursos en YouTube](https://arxiv.org/abs/2510.00021)
*Alvaro Vallejo Ramírez*

Main category: cs.SI

TL;DR: 本研究分析了2025年伊朗-以色列冲突在YouTube上的数字言论，发现亲巴勒斯坦和反美/以色列言论占主导，伊朗成为核心角色，并确认了算法偏见对数字对话的影响。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在分析2025年6月伊朗-以色列冲突在YouTube上12万条评论的数字呈现，以识别参与者的言论立场，并探讨媒体和算法偏见如何塑造数字对话。

Method: 采用混合方法设计与三角验证。定量阶段使用自然语言处理技术和机器学习模型（BERT和XLM-RoBERTa）将评论分类；定性阶段则对媒体语境和意识形态叙事进行批判性分析，辅以人工标注和监督训练，以整合统计健壮性与语境理解。

Result: 研究发现亲巴勒斯坦和反美/以色列言论明显过高呈现，而亲美和反巴勒斯坦立场则处于边缘。伊朗作为数字对话的核心参与者出现，表明叙事摆脱了以往的霸权框架。同时，结果证实了算法偏见在放大某些言论、限制其他言论方面的影响。

Conclusion: 数字对话中亲巴勒斯坦和反美/以色列言论占据主导地位，伊朗作为关键叙事者出现，打破了传统媒体的不可见性，且算法偏见对言论的传播和限制有显著影响。

Abstract: Purpose. This study analyzes the digital representation of the Iran-Israel
conflict that occurred in June 2025, based on 120,000 comments posted on
YouTube. It sought to identify discursive positions regarding the actors
involved and to examine how media and algorithmic biases shape digital
conversations. Methodology. A mixed-methods design with triangulation was
adopted. In the quantitative phase, natural language processing techniques and
machine learning models (BERT and XLM-RoBERTa) were used to classify comments
into ten categories. In the qualitative phase, a critical analysis of media
context and ideological narratives was conducted, complemented by manual
annotation and supervised training. This strategy enabled the integration of
statistical robustness with contextual understanding. Results and conclusions.
The findings reveal a clear overrepresentation of pro-Palestinian and
anti-United States/Israel discourses, while pro-United States and
anti-Palestinian positions were marginal. Iran, usually rendered invisible in
global media, emerged as a central actor in the digital conversation during the
conflict, suggesting a narrative shift away from previous hegemonic frameworks.
Likewise, the results confirm the influence of algorithmic biases in amplifying
certain discourses while limiting others. Original contributions. This work
combines computational analysis and philosophical critique for the study of
digital controversies, providing a methodological framework replicable in
geopolitical contexts. It is one of the first Spanish-language studies to map,
through artificial intelligence and critical analysis, discourses on an
international conflict on YouTube, highlighting asymmetries and narrative
disputes that are often overlooked.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [244] [Unpacking Musical Symbolism in Online Communities: Content-Based and Network-Centric Approaches](https://arxiv.org/abs/2510.00006)
*Kajwan Ziaoddini*

Main category: cs.SD

TL;DR: 本文结合音乐内容分析和歌词网络视角，研究了在线社区中音乐象征意义的产生与传播。研究发现十年间音乐能量下降而舞曲性上升，歌词以代词为中心，不同流派情绪差异显著，表明商业倾向于轻松但节奏感强的作品。


<details>
  <summary>Details</summary>
Motivation: 探讨音乐象征意义如何在在线社区中生成和传播。

Method: 利用包含275首排行榜歌曲的语料库，结合音频描述符（能量、舞曲性、响度、现场感、情感效度、原声性、口语性、流行度）和完整歌词文本。构建可复现的流程，量化声学属性的时间趋势，建模词汇显著性与共现，并按流派分析情绪。使用了内容音乐分析、轻量级歌词网络视角、相关性分析、词汇共现矩阵和流派情绪分析。

Result: 能量十年间持续下降（79→58），舞曲性上升（59→73）；情感效度在2013年达到峰值（63），2014-2016年下降（42）后部分恢复。能量与响度强相关（r=0.74），原声性与能量（r=-0.54）及响度（r=-0.51）负相关；舞曲性与其他特征正交（|r|<0.20）。歌词分析显示以“我/你/我/我的”为中心的代词词汇，以及人际称谓作为主流叙事锚点的密集共现结构。R&B流派情感效度最高（96），其次是K-Pop/Pop（77）和Indie/Pop（70），而Latin/Reggaeton较低（37）。

Conclusion: 这些模式从亚文化认同视角看，表明了先前边缘代码的主流化，以及商业上对轻松但节奏感强、能维持集体参与而无需最大强度的音乐制作的偏好。方法论上，本文贡献了一种整合的MIR-加网络工作流，适用于社会意识推荐或社区级传播研究。

Abstract: This paper examines how musical symbolism is produced and circulated in
online communities by combining content-based music analysis with a lightweight
network perspective on lyrics. Using a curated corpus of 275 chart-topping
songs enriched with audio descriptors (energy, danceability, loudness,
liveness, valence, acousticness, speechiness, popularity) and full lyric
transcripts, we build a reproducible pipeline that (i) quantifies temporal
trends in sonic attributes, (ii) models lexical salience and co-occurrence, and
(iii) profiles mood by genre. We find a decade-long decline in energy (79 ->
58) alongside a rise in danceability (59 -> 73); valence peaks in 2013 (63) and
dips in 2014-2016 (42) before partially recovering. Correlation analysis shows
strong coupling of energy with loudness (r = 0.74) and negative associations
for acousticness with both energy (r = -0.54) and loudness (r = -0.51);
danceability is largely orthogonal to other features (|r| < 0.20). Lyric
tokenization (>114k tokens) reveals a pronoun-centric lexicon "I/you/me/my" and
a dense co-occurrence structure in which interpersonal address anchors
mainstream narratives. Mood differs systematically by style: R&B exhibits the
highest mean valence (96), followed by K-Pop/Pop (77) and Indie/Pop (70),
whereas Latin/Reggaeton is lower (37) despite high danceability. Read through a
subcultural identity lens, these patterns suggest the mainstreaming of
previously peripheral codes and a commercial preference for relaxed yet
rhythmically engaging productions that sustain collective participation without
maximal intensity. Methodologically, we contribute an integrated
MIR-plus-network workflow spanning summary statistics, correlation structure,
lexical co-occurrence matrices, and genre-wise mood profiling that is robust to
modality sparsity and suitable for socially aware recommendation or
community-level diffusion studies.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [245] [Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval](https://arxiv.org/abs/2510.00137)
*Nima Sheikholeslami,Erfan Hosseini,Patrice Bechard,Srivatsava Daruru,Sai Rajeswar*

Main category: cs.IR

TL;DR: 现有双编码器检索器使用的对比损失存在分数分离质量和AUC无关的问题，导致性能不佳。本文提出MW损失，直接优化AUC，从而获得性能更好、校准性更强的检索器，尤其适用于RAG。


<details>
  <summary>Details</summary>
Motivation: 双编码器检索器依赖相关文档得分高于不相关文档的原则，但主流的噪声对比估计（NCE）目标函数（对比损失）被证明对分数分离质量不敏感，也与AUC无关，导致校准性差，并在RAG等下游任务中表现不佳。

Method: 引入MW损失（MW loss），这是一种新的训练目标，通过最大化Mann-Whitney U统计量（在数学上等同于ROC曲线下面积AUC），鼓励每个正负样本对被正确排序，方法是最小化分数差异上的二元交叉熵。MW损失在理论上直接上界了AoC，更好地使优化与检索目标对齐。

Result: 经验性实验表明，使用MW损失训练的检索器在AUC和标准检索指标上始终优于使用对比损失训练的检索器。

Conclusion: MW损失是对比损失的一种在经验上更优的替代方案，能够为RAG等高风险应用提供校准性更好、判别能力更强的检索器。

Abstract: Dual-encoder retrievers depend on the principle that relevant documents
should score higher than irrelevant ones for a given query. Yet the dominant
Noise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss,
optimizes a softened ranking surrogate that we rigorously prove is
fundamentally oblivious to score separation quality and unrelated to AUC. This
mismatch leads to poor calibration and suboptimal performance in downstream
tasks like retrieval-augmented generation (RAG). To address this fundamental
limitation, we introduce the MW loss, a new training objective that maximizes
the Mann-Whitney U statistic, which is mathematically equivalent to the Area
under the ROC Curve (AUC). MW loss encourages each positive-negative pair to be
correctly ranked by minimizing binary cross entropy over score differences. We
provide theoretical guarantees that MW loss directly upper-bounds the AoC,
better aligning optimization with retrieval goals. We further promote ROC
curves and AUC as natural threshold free diagnostics for evaluating retriever
calibration and ranking quality. Empirically, retrievers trained with MW loss
consistently outperform contrastive counterparts in AUC and standard retrieval
metrics. Our experiments show that MW loss is an empirically superior
alternative to Contrastive Loss, yielding better-calibrated and more
discriminative retrievers for high-stakes applications like RAG.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [246] [WaveMind: Towards a Conversational EEG Foundation Model Aligned to Textual and Visual Modalities](https://arxiv.org/abs/2510.00032)
*Ziyi Zeng,Zhenyang Cai,Yixi Cai,Xidong Wang,Junying Chen,Rongsheng Wang,Yipeng Liu,Siqi Cai,Benyou Wang,Zhiguo Zhang,Haizhou Li*

Main category: eess.SP

TL;DR: 使用多模态大语言模型（MLLMs）解读脑电图（EEG）面临模态不匹配的挑战。本文提出将EEG信号映射到统一语义空间，并构建首个跨任务EEG指令微调数据集，以实现广义解读和对话能力，模型在分类准确性和开放式对话方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 脑电图（EEG）信号同时编码认知过程和内在神经状态，导致其配对数据模态不匹配，这阻碍了多模态大语言模型（MLLMs）进行有效的跨模态表征学习以解读脑电信号。

Method: 通过一项关键调查揭示了EEG模态之间的互补关系。在此基础上，提出将EEG信号及其对应模态映射到统一语义空间以实现广义解读。为实现对话能力，还引入了首个用于指令微调的跨任务EEG数据集WaveMind-Instruct-338k。

Result: 所得模型在四个下游任务中展示出强大的分类准确性，并支持灵活、开放式的对话。

Conclusion: 该研究为神经科学研究和通用EEG模型的开发提供了宝贵见解。

Abstract: Electroencephalography (EEG) interpretation using multimodal large language
models (MLLMs) offers a novel approach for analyzing brain signals. However,
the complex nature of brain activity introduces critical challenges: EEG
signals simultaneously encode both cognitive processes and intrinsic neural
states, creating a mismatch in EEG paired-data modality that hinders effective
cross-modal representation learning. Through a pivot investigation, we uncover
complementary relationships between these modalities. Leveraging this insight,
we propose mapping EEG signals and their corresponding modalities into a
unified semantic space to achieve generalized interpretation. To fully enable
conversational capabilities, we further introduce WaveMind-Instruct-338k, the
first cross-task EEG dataset for instruction tuning. The resulting model
demonstrates robust classification accuracy while supporting flexible,
open-ended conversations across four downstream tasks, thereby offering
valuable insights for both neuroscience research and the development of
general-purpose EEG models.

</details>
