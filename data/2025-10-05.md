<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 63]
- [cs.CV](#cs.CV) [Total: 63]
- [cs.AI](#cs.AI) [Total: 51]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and Long-Context Reasoning](https://arxiv.org/abs/2510.01585)
*Haochen You,Baojing Liu*

Main category: cs.CL

TL;DR: ReSSFormer是一种递归稀稀疏结构Transformer，通过引入循环推理、自适应稀疏注意力和自组织编码结构，解决了传统Transformer在长上下文推理、计算效率和结构泛化方面的挑战，并在多项任务上取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统的Transformer架构在长上下文推理、计算效率和结构泛化方面面临挑战，主要原因在于其僵硬的层堆叠、密集注意力机制以及对位置编码的依赖。

Method: 提出ReSSFormer，一种递归稀疏结构Transformer。它整合了三个创新：循环推理与记忆单元（R2MU）用于有界深度的迭代推理，自适应稀疏注意力模块（ASAM）用于高效集中的上下文选择，以及自组织编码结构（SOES）用于无位置的结构归纳。ReSSFormer用循环推理取代传统深度堆叠，用令牌和专家级别的稀疏性取代全注意力，并直接从内容中建模潜在的令牌拓扑结构。

Result: ReSSFormer在语言建模、多跳问答和结构敏感任务上，在可比的FLOPs和参数预算下，始终优于强大的基线模型。

Conclusion: ReSSFormer展示了其卓越的可扩展性、计算效率和结构灵活性。

Abstract: While Transformer architectures have demonstrated impressive scalability
across domains, they continue to face challenges in long-context reasoning,
computational efficiency, and structural generalization - largely due to rigid
layer stacking, dense attention, and reliance on positional encodings. We
present ReSSFormer, a Recursive Sparse Structured Transformer that integrates
three complementary innovations: Recurrent Reasoning & Memory Unit (R2MU) for
iterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM)
for efficient and focused context selection, and Self-Organizing Encoder
Structure (SOES) for position-free structure induction. ReSSFormer replaces
conventional depth stacking with recurrent inference, substitutes full
attention with token- and expert-level sparsity, and models latent token
topology directly from content. Across language modeling, multi-hop QA, and
structure-sensitive tasks, ReSSFormer consistently outperforms strong baselines
under comparable FLOPs and parameter budgets, highlighting its scalability,
efficiency, and structural flexibility.

</details>


### [2] [Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset](https://arxiv.org/abs/2510.01219)
*Leroy Z. Wang*

Main category: cs.CL

TL;DR: 论文引入了一个概念学习任务数据集，并通过语境学习实验发现大语言模型对量词存在向上单调性偏见，并证明语境概念学习是发现隐藏偏见的有效方法。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示大语言模型中存在的隐性偏见，并通过开发新的方法来提高偏见检测的有效性。

Method: 引入了一个概念学习任务数据集，并设计了语境概念学习实验来测试语言模型。研究还通过与不包含概念学习组件的直接提示方式进行对比。

Result: 实验发现大语言模型可能对量词表现出向上单调性偏见，且这种偏见在语境概念学习中比直接提示更为明显。结果证实语境概念学习是发现语言模型隐藏偏见的有效途径。

Conclusion: 语境概念学习是一种发现大语言模型中不易察觉的隐性偏见的有效方法。

Abstract: We introduce a dataset of concept learning tasks that helps uncover implicit
biases in large language models. Using in-context concept learning experiments,
we found that language models may have a bias toward upward monotonicity in
quantifiers; such bias is less apparent when the model is tested by direct
prompting without concept learning components. This demonstrates that
in-context concept learning can be an effective way to discover hidden biases
in language models.

</details>


### [3] [Towards Open-Ended Discovery for Low-Resource NLP](https://arxiv.org/abs/2510.01220)
*Bonaventure F. P. Dossou,Henri Aïdasso*

Main category: cs.CL

TL;DR: 本文提出一种范式转变，主张AI系统通过动态对话而非静态数据集，利用人机协同不确定性来学习低资源语言，以实现开放式、交互式的语言发现。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的NLP受限于文本语料、标准化正字法和可扩展标注管道的缺乏。现有大型语言模型因依赖大规模数据和中心化基础设施，对代表性不足的社区而言难以普及。

Method: 提出一个基于人机共同不确定性的框架，结合模型认识不确定性与人类说话者的犹豫线索及置信信号，以指导交互、查询选择和记忆保留，促进参与式、协同适应性学习。

Result: 作为一个立场性文件，本文未提供具体实验结果，而是提出了一种新的学习范式和框架，旨在通过交互式、不确定性驱动的发现来解决低资源语言的NLP问题。

Conclusion: 未来语言技术应超越静态数据收集，转向互动、合作式的模型构建，尊重并赋能社区，以发现和保护世界语言多样性，这与以人为本的AI原则相符。

Abstract: Natural Language Processing (NLP) for low-resource languages remains
fundamentally constrained by the lack of textual corpora, standardized
orthographies, and scalable annotation pipelines. While recent advances in
large language models have improved cross-lingual transfer, they remain
inaccessible to underrepresented communities due to their reliance on massive,
pre-collected data and centralized infrastructure. In this position paper, we
argue for a paradigm shift toward open-ended, interactive language discovery,
where AI systems learn new languages dynamically through dialogue rather than
static datasets. We contend that the future of language technology,
particularly for low-resource and under-documented languages, must move beyond
static data collection pipelines toward interactive, uncertainty-driven
discovery, where learning emerges dynamically from human-machine collaboration
instead of being limited to pre-existing datasets. We propose a framework
grounded in joint human-machine uncertainty, combining epistemic uncertainty
from the model with hesitation cues and confidence signals from human speakers
to guide interaction, query selection, and memory retention. This paper is a
call to action: we advocate a rethinking of how AI engages with human knowledge
in under-documented languages, moving from extractive data collection toward
participatory, co-adaptive learning processes that respect and empower
communities while discovering and preserving the world's linguistic diversity.
This vision aligns with principles of human-centered AI, emphasizing
interactive, cooperative model building between AI systems and speakers.

</details>


### [4] [Discourse vs emissions: Analysis of corporate narratives, symbolic practices, and mimicry through LLMs](https://arxiv.org/abs/2510.01222)
*Bertrand Kian Hassani,Yacoub Bahini,Rizwan Mushtaq*

Main category: cs.CL

TL;DR: 本文利用大型语言模型（LLMs）分析了828家美国上市公司的气候披露成熟度，发现风险叙事与承诺一致，但量化目标与语调脱节；大型高排放公司披露更多承诺但与量化目标不一致；普遍存在的相似披露风格表明存在模仿行为，降低了披露的决策有用性。


<details>
  <summary>Details</summary>
Motivation: 气候变化增加了对透明和可比较的企业气候披露的需求，但模仿和象征性报告常常削弱其价值。

Method: 开发了一个多维度框架来评估披露成熟度，利用针对气候沟通进行微调的大型语言模型（LLMs）。通过四种分类器（情感、承诺、具体性和目标雄心）从可持续发展报告和年度报告中提取叙述指标，并将其与公司属性（如排放、市值和行业）关联。

Result: 1. 以风险为中心的叙述常与明确承诺一致，但量化目标（如净零承诺）与语调脱节。
2. 规模更大、排放更高的公司比同行披露更多承诺和行动，但与量化目标不一致。
3. 披露风格的广泛相似性表明存在模仿行为，降低了差异化和决策有用性。

Conclusion: 研究结果突出了大型语言模型在ESG叙事分析中的价值，并强调需要更强有力的监管，以将承诺与可验证的转型策略联系起来。

Abstract: Climate change has increased demands for transparent and comparable corporate
climate disclosures, yet imitation and symbolic reporting often undermine their
value. This paper develops a multidimensional framework to assess disclosure
maturity among 828 U.S.listed firms using large language models (LLMs)
fine-tuned for climate communication. Four classifiers-sentiment, commitment,
specificity, and target ambition-extract narrative indicators from
sustainability and annual reports, which are linked to firm attributes such as
emissions, market capitalization, and sector. Analyses reveal three insights:
(1) risk-focused narratives often align with explicit commitments, but
quantitative targets (e.g., net-zero pledges) remain decoupled from tone; (2)
larger and higher-emitting firms disclose more commitments and actions than
peers, though inconsistently with quantitative targets; and (3) widespread
similarity in disclosure styles suggests mimetic behavior, reducing
differentiation and decision usefulness. These results highlight the value of
LLMs for ESG narrative analysis and the need for stronger regulation to connect
commitments with verifiable transition strategies.

</details>


### [5] [Context Matters: Comparison of commercial large language tools in veterinary medicine](https://arxiv.org/abs/2510.01224)
*Tyler J Poore,Christopher J Pinard,Aleena Shabbir,Andrew Lagree,Andre Telfer,Kuan-Chuen Wu*

Main category: cs.CL

TL;DR: 本研究评估了三款商业兽医专用LLM摘要工具在肿瘤记录上的性能，发现Product 1表现最佳，并证明了“LLM作为评判者”评估框架在兽医领域的可扩展性和可重复性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在临床应用中日益普及，但其在兽医学领域的性能尚未得到充分探索。

Method: 本研究评估了三款商业兽医专用LLM摘要工具（Product 1 [Hachiko]、Product 2和Product 3），使用标准化的兽医肿瘤记录数据集。采用“LLM作为评判者”框架，通过一套指南，从事实准确性、完整性、时间顺序、临床相关性和组织结构五个维度对摘要进行评分。为评估评分框架的内部一致性，研究重复进行了三次独立评估。

Result: Product 1取得了最高的整体性能，中位数平均分为4.61（IQR: 0.73），而Product 2为2.55（IQR: 0.78），Product 3为2.45（IQR: 0.92）。Product 1在事实准确性和时间顺序方面获得了完美的中位数分数。LLM评审器展示出高可重复性，Product 1、2、3的平均分数标准差分别为0.015、0.088和0.034。

Conclusion: 这些发现强调了兽医专用商业LLM工具的重要性，并表明“LLM作为评判者”评估是一种可扩展且可重复的兽医临床自然语言处理摘要评估方法。

Abstract: Large language models (LLMs) are increasingly used in clinical settings, yet
their performance in veterinary medicine remains underexplored. We evaluated
three commercially available veterinary-focused LLM summarization tools
(Product 1 [Hachiko] and Products 2 and 3) on a standardized dataset of
veterinary oncology records. Using a rubric-guided LLM-as-a-judge framework,
summaries were scored across five domains: Factual Accuracy, Completeness,
Chronological Order, Clinical Relevance, and Organization. Product 1 achieved
the highest overall performance, with a median average score of 4.61 (IQR:
0.73), compared to 2.55 (IQR: 0.78) for Product 2 and 2.45 (IQR: 0.92) for
Product 3. It also received perfect median scores in Factual Accuracy and
Chronological Order. To assess the internal consistency of the grading
framework itself, we repeated the evaluation across three independent runs. The
LLM grader demonstrated high reproducibility, with Average Score standard
deviations of 0.015 (Product 1), 0.088 (Product 2), and 0.034 (Product 3).
These findings highlight the importance of veterinary-specific commercial LLM
tools and demonstrate that LLM-as-a-judge evaluation is a scalable and
reproducible method for assessing clinical NLP summarization in veterinary
medicine.

</details>


### [6] [ClaimCheck: Real-Time Fact-Checking with Small Language Models](https://arxiv.org/abs/2510.01226)
*Akshith Reddy Putta,Jacob Devasier,Chengkai Li*

Main category: cs.CL

TL;DR: 介绍ClaimCheck，一个基于小语言模型（LLM）和实时网络证据的自动事实核查系统，实现了高效且准确的验证。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统依赖大型、闭源LLM和静态知识库，导致高计算成本且缺乏透明度。需要一个能够利用小型LLM，提供准确、可解释且计算资源需求更低的事实核查解决方案。

Method: ClaimCheck采用透明、分步的验证流程，模仿人类事实核查工作流，包括网络搜索查询规划、基于网络的证据检索与总结、证据合成与再检索以及主张判断评估。所有模块均针对小型LLM进行了优化。

Result: 尽管使用了小得多的Qwen3-4B模型，ClaimCheck在AVeriTeC数据集上实现了76.4%的最先进（SOTA）准确率，超越了使用LLaMA3.1 70B和GPT-4o的方法。广泛的消融实验表明，精心设计的模块化结构和提示策略能够克服小型LLM的局限性。

Conclusion: ClaimCheck展示了通过模块化设计和精心提示策略，小型LLM也能实现高准确度、可解释且计算高效的事实核查，成功替代了对大型、闭源模型的依赖，并提升了系统的可访问性和透明度。

Abstract: We introduce ClaimCheck, an LLM-guided automatic fact-checking system
designed to verify real-world claims using live Web evidence and small language
models. Unlike prior systems that rely on large, closed-source models and
static knowledge stores, ClaimCheck employs a transparent, stepwise
verification pipeline that mirrors human fact-checking workflows consisting of
Web search query planning, Web-based evidence retrieval and summarization,
evidence synthesis and re-retrieval, and claim verdict evaluation. Each module
is optimized for small LLMs, allowing the system to deliver accurate and
interpretable fact-checking with significantly lower computational
requirements. Despite using a much smaller Qwen3-4B model, ClaimCheck achieves
state-of-the-art accuracy of 76.4% on the AVeriTeC dataset, outperforming
previous approaches using LLaMA3.1 70B and GPT-4o. Extensive ablations
demonstrate that careful modular design and prompting strategies can overcome
the limitations of smaller LLMs. To promote accessibility and transparency, we
provide a public demo at https://idir.uta.edu/claimcheck.

</details>


### [7] [EEFSUVA: A New Mathematical Olympiad Benchmark](https://arxiv.org/abs/2510.01227)
*Nicole N Khatibi,Daniil A. Radamovich,Michael P. Brenner*

Main category: cs.CL

TL;DR: 研究引入了新基准EEFSUVA，发现LLMs在来自东欧和前苏联不常见奥林匹克竞赛的题目上表现显著下降，表明现有基准可能高估了其数学推理能力，且需要更广泛的评估数据集。


<details>
  <summary>Details</summary>
Motivation: 现有研究声称大型语言模型（LLMs）在数学基准测试上能达到奥林匹克金牌选手或研究生水平。本研究旨在详细审视这些说法，并评估当前主要源自IMO等竞赛的基准测试，是否因潜在数据污染和问题类型狭窄而高估了LLMs的数学推理能力。

Method: 引入了一个名为EEFSUVA的新型基准测试。该基准精选自东欧和前苏联国家地区性及国家级的、不常流通的奥林匹克数学竞赛题目。这些题目难度与IMO相当，以需要非标准解题技巧而闻名，且在线语料库中出现频率极低，旨在提供对数学理解的更全面评估。

Result: 初步结果显示，即使是当前最先进的LLMs在EEFSUVA上的表现也相对于其他奥林匹克风格的基准测试有显著下降。

Conclusion: 这些发现表明，为了更全面地评估LLMs的数学推理能力并指导未来的模型开发，拓展评估数据集的广度至关重要。

Abstract: Recent breakthroughs have spurred claims that large language models (LLMs)
match gold medal Olympiad to graduate level proficiency on mathematics
benchmarks. In this work, we examine these claims in detail and assess the
extent to which current benchmarks capture genuine LLM mathematical reasoning.
The composition of these benchmarks, primarily drawing from the International
Mathematics Olympiad (IMO) and related competitions, may overstate models
reasoning ability due to potential data contamination and a narrow focus on
familiar problem types. To enable a more holistic assessment of mathematical
understanding, we introduce EEFSUVA, a novel benchmark curated from under
circulated regional and national Olympiads of Eastern Europe and the countries
from the former Soviet Union. These contests feature problems of comparable
difficulty to the IMO and are renowned for demanding nonstandard
problem-solving techniques, yet their problems are far less prevalent in online
corpora. Preliminary results suggest that even state-of-the-art LLMs exhibit a
notable performance decline on EEFSUVA relative to other Olympiad-style
benchmarks. These findings also suggest the potential importance of broader
evaluation datasets for a fuller assessment of mathematical reasoning and for
guiding future model development.

</details>


### [8] [Who is In Charge? Dissecting Role Conflicts in Instruction Following](https://arxiv.org/abs/2510.01228)
*Siqi Zeng*

Main category: cs.CL

TL;DR: 大型语言模型常忽略系统指令优于用户输入的层级规则，却服从社会线索。本研究通过机制解释，发现模型对系统-用户冲突和社会冲突有不同处理方式，且基于社会线索的引导竟能普遍增强指令遵循，揭示了系统服从的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型应遵循指令层级（系统提示优先于用户输入），但最近研究表明它们常忽视此规则，反而强烈服从权威或共识等社会线索。本研究旨在通过大规模数据集进行机制解释，探究导致系统服从脆弱性的深层原因。

Method: 结合大规模数据集，采用线性探测（linear probing）分析冲突决策信号的编码及其在不同冲突类型（系统-用户 vs. 社会）中的区分；使用直接Logit归因（Direct Logit Attribution）揭示内部冲突检测和解决的一致性；并进行引导实验（steering experiments）探究社会线索对指令遵循的影响。

Result: 冲突决策信号在早期就被编码，系统-用户冲突和社会冲突形成不同的神经子空间。模型在系统-用户冲突中表现出更强的内部冲突检测，但仅对社会线索能提供一致的解决方案。意外的是，利用社会线索的引导向量能以与角色无关的方式放大指令遵循能力。

Conclusion: 这些结果解释了大型语言模型对系统指令服从的脆弱性，并强调了开发轻量级、对层级敏感的对齐方法的重要性。

Abstract: Large language models should follow hierarchical instructions where system
prompts override user inputs, yet recent work shows they often ignore this rule
while strongly obeying social cues such as authority or consensus. We extend
these behavioral findings with mechanistic interpretations on a large-scale
dataset. Linear probing shows conflict-decision signals are encoded early, with
system-user and social conflicts forming distinct subspaces. Direct Logit
Attribution reveals stronger internal conflict detection in system-user cases
but consistent resolution only for social cues. Steering experiments show that,
despite using social cues, the vectors surprisingly amplify instruction
following in a role-agnostic way. Together, these results explain fragile
system obedience and underscore the need for lightweight hierarchy-sensitive
alignment methods.

</details>


### [9] [Enhancing Transformer-Based Rerankers with Synthetic Data and LLM-Based Supervision](https://arxiv.org/abs/2510.01229)
*Dimitar Peshevski,Kiril Blazhevski,Martin Popovski,Gjorgji Madjarov*

Main category: cs.CL

TL;DR: 提出一种无需人工标注的新颖文档重排序方法，利用大型语言模型（LLMs）生成合成查询和标注数据，然后用于微调小型Transformer模型，以实现高效且高性能的重排序。


<details>
  <summary>Details</summary>
Motivation: 现有文档重排序中，LLMs性能优越但计算成本高昂，难以实际部署；而微调小型模型依赖稀缺的人工标注数据。研究动机旨在克服LLMs的高成本和人工标注数据稀缺的问题，实现高效、高性能的文档重排序。

Method: 该方法构建了一个新颖的流程：首先利用LLMs从领域语料库生成合成查询，并使用LLM分类器标注正负样本对。随后，利用这个合成数据集，通过对比学习和局部对比估计（LCE）损失函数，微调一个小型Transformer模型。

Result: 在MedQuAD数据集上的实验表明，该方法显著提升了域内（in-domain）性能，并能很好地泛化到域外（out-of-domain）任务。

Conclusion: 通过将LLMs用于数据生成和监督而非直接推理，该方法在保持强大重排序能力的同时，显著降低了计算成本，有效解决了LLMs部署的实际难题和数据标注的瓶颈。

Abstract: Effective document reranking is essential for improving search relevance
across diverse applications. While Large Language Models (LLMs) excel at
reranking due to their deep semantic understanding and reasoning, their high
computational cost makes them impractical for many real-world deployments.
Fine-tuning smaller, task-specific models is a more efficient alternative but
typically depends on scarce, manually labeled data. To overcome this, we
propose a novel pipeline that eliminates the need for human-labeled
query-document pairs. Our method uses LLMs to generate synthetic queries from
domain-specific corpora and employs an LLM-based classifier to label positive
and hard-negative pairs. This synthetic dataset is then used to fine-tune a
smaller transformer model with contrastive learning using Localized Contrastive
Estimation (LCE) loss. Experiments on the MedQuAD dataset show that our
approach significantly boosts in-domain performance and generalizes well to
out-of-domain tasks. By using LLMs for data generation and supervision rather
than inference, we reduce computational costs while maintaining strong
reranking capabilities.

</details>


### [10] [Geometric Structures and Patterns of Meaning: A PHATE Manifold Analysis of Chinese Character Embeddings](https://arxiv.org/abs/2510.01230)
*Wen G. Gong*

Main category: cs.CL

TL;DR: 本文通过对汉字嵌入的几何模式进行系统分析，发现内容词和功能词的聚类与分支模式，且几何复杂性与语义内容相关，为传统语言学理论提供了计算证据，并提出了语义组织几何分析的新框架。


<details>
  <summary>Details</summary>
Motivation: 理解汉字嵌入中的几何模式，为传统语言学理论提供计算证据，并建立一个分析语义组织的新几何框架。

Method: 使用PHATE流形分析系统研究汉字嵌入的几何模式；通过七种嵌入模型和八种降维方法进行交叉验证；分析12个语义域中超过1000个汉字；进行123个词组的子网络分析。

Result: 观察到内容词的聚类模式和功能词的分支模式；几何复杂性与语义内容相关，有意义的字符表现出丰富的几何多样性，而结构性部首则形成紧密的簇；子网络分析表明语义从基本字符系统性地扩展。

Conclusion: 研究结果为传统语言学理论提供了计算证据，并为语义组织的几何分析建立了一个新颖的框架。

Abstract: We systematically investigate geometric patterns in Chinese character
embeddings using PHATE manifold analysis. Through cross-validation across seven
embedding models and eight dimensionality reduction methods, we observe
clustering patterns for content words and branching patterns for function
words. Analysis of over 1000 Chinese characters across 12 semantic domains
reveals that geometric complexity correlates with semantic content: meaningful
characters exhibit rich geometric diversity while structural radicals collapse
into tight clusters. The comprehensive child-network analysis (123 phrases)
demonstrates systematic semantic expansion from elemental character. These
findings provide computational evidence supporting traditional linguistic
theory and establish a novel framework for geometric analysis of semantic
organization.

</details>


### [11] [Trustworthy Summarization via Uncertainty Quantification and Risk Awareness in Large Language Models](https://arxiv.org/abs/2510.01231)
*Shuaidong Pan,Di Wu*

Main category: cs.CL

TL;DR: 该研究提出了一种融合不确定性量化和风险感知机制的大型语言模型框架，旨在解决高风险场景下自动摘要的可靠性问题，显著提升了摘要的鲁棒性和可信度。


<details>
  <summary>Details</summary>
Motivation: 解决高风险场景下自动摘要的可靠性问题，应对信息过载和高风险决策的需求，避免模型生成过度自信的预测。

Method: 构建基于条件生成的摘要模型，通过引入贝叶斯推断对参数空间不确定性进行建模；使用预测分布熵衡量生成内容的不确定性；联合优化熵正则化和风险感知损失；集成风险评分和调节模块，通过明确的风险等级提示增强摘要的可信度。

Result: 对比实验和敏感性分析结果表明，所提出的方法显著提升了高风险应用中摘要的鲁棒性和可靠性，同时保持了摘要的流畅性和语义完整性。

Conclusion: 该研究为可信摘要提供了一个系统性解决方案，并在方法论层面展示了其可扩展性和实用价值。

Abstract: This study addresses the reliability of automatic summarization in high-risk
scenarios and proposes a large language model framework that integrates
uncertainty quantification and risk-aware mechanisms. Starting from the demands
of information overload and high-risk decision-making, a conditional
generation-based summarization model is constructed, and Bayesian inference is
introduced during generation to model uncertainty in the parameter space, which
helps avoid overconfident predictions. The uncertainty level of the generated
content is measured using predictive distribution entropy, and a joint
optimization of entropy regularization and risk-aware loss is applied to ensure
that key information is preserved and risk attributes are explicitly expressed
during information compression. On this basis, the model incorporates risk
scoring and regulation modules, allowing summaries to cover the core content
accurately while enhancing trustworthiness through explicit risk-level prompts.
Comparative experiments and sensitivity analyses verify that the proposed
method significantly improves the robustness and reliability of summarization
in high-risk applications while maintaining fluency and semantic integrity.
This research provides a systematic solution for trustworthy summarization and
demonstrates both scalability and practical value at the methodological level.

</details>


### [12] [Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks](https://arxiv.org/abs/2510.01232)
*Dongjun Kim,Gyuho Shim,Yongchan Chun,Minhyuk Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 本文提出“基准剖析”框架，通过分解基准性能为十种认知能力，量化每种能力对模型成功的影响，以透明地评估大型语言模型基准的实际测量能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准分数常夸大LLM的真实能力，因其掩盖了任务所需的技能组合。基准标签（如ARC测试推理）缺乏系统性验证，无法确认它们是否真正测量了声称的能力，导致需要一种系统方法来验证基准的实际测量内容。

Method: 引入“基准剖析”诊断框架。该方法将基准性能分解为十种基于认知的技能，结合基于梯度的重要性评分与有针对性的参数消融，计算“能力影响得分(AIS)”，以量化每种能力对模型在给定基准上成功的贡献。

Result: 通过对三个指令调优模型在十个广泛使用的基准上进行剖析，发现：(i) 大多数基准涉及多种能力而非单一能力；(ii) 具有相似标签的数据集依赖不同的能力组合；(iii) 代码生成基准奖励广泛、多技能的改进，狭窄的领域特定微调效果不明显；(iv) 与任务无关的能力可能对性能产生负面影响。

Conclusion: 基准剖析解释了性能提升为何不总能转化为用户感知的能力，并提供了一个透明的工具，用于基准审计和模型可解释性。

Abstract: Large Language Models are commonly judged by their scores on standard
benchmarks, yet such scores often overstate real capability since they mask the
mix of skills a task actually demands. For example, ARC is assumed to test
reasoning, while HellaSwag is designed to evaluate commonsense. However, we
lack a systematic way to verify if these benchmarks actually measure these
labels. We introduce Benchmark Profiling, a diagnostic framework that
decomposes benchmark performance into ten cognitively grounded abilities. The
method combines gradient-based importance scoring with targeted parameter
ablation to compute an Ability Impact Score (AIS) that quantifies how much each
ability contributes to a model's success on a given benchmark. Profiling three
instruction-tuned models across ten widely used benchmarks yields four key
findings: (i) most benchmarks draw on several abilities rather than one, (ii)
datasets with similar labels rely on distinct ability mixtures, (iii)
code-generation benchmarks reward broad, multi-skill improvement and thus show
only modest gains from narrow domain-specific fine-tuning, and (iv) abilities
irrelevant to the task could negatively affect performance. Benchmark Profiling
therefore explains why performance gains do not always translate into
user-perceived competence and offers a transparent tool for benchmark audit and
model interpretability.

</details>


### [13] [Computational Social Linguistics for Telugu Cultural Preservation: Novel Algorithms for Chandassu Metrical Pattern Recognition](https://arxiv.org/abs/2510.01233)
*Boddu Sri Pavan,Boddu Swathi Sree*

Main category: cs.CL

TL;DR: 本研究利用计算社会科学方法，创建首个综合性数字框架，以保护和分析泰卢固语昌达苏（韵律诗歌）传统，实现91.73%的准确率。


<details>
  <summary>Details</summary>
Motivation: 保存代表数百年集体文化智慧的濒危泰卢固语昌达苏（韵律诗歌）传统。

Method: 采用计算社会科学方法，结合传统知识和现代计算技术。包括：协作创建4,651个带注释的padyams数据集、专家验证的语言模式、文化知情算法设计，以及AksharamTokenizer、LaghuvuGuruvu Generator和PadyaBhedam Checker等工具。

Result: 算法在提出的昌达苏得分上达到91.73%的准确率，评估指标符合传统文学标准。

Conclusion: 展示了计算社会科学如何保存濒危文化知识系统，同时支持围绕文学遗产的集体智能。该方法为以社区为中心的文化保护提供了见解，支持数字人文和关注社会计算系统中的更广泛倡议。

Abstract: This research presents a computational social science approach to preserving
Telugu Chandassu, the metrical poetry tradition representing centuries of
collective cultural intelligence. We develop the first comprehensive digital
framework for analyzing Telugu prosodic patterns, bridging traditional
community knowledge with modern computational methods. Our social computing
approach involves collaborative dataset creation of 4,651 annotated padyams,
expert-validated linguistic patterns, and culturally-informed algorithmic
design. The framework includes AksharamTokenizer for prosody-aware
tokenization, LaghuvuGuruvu Generator for classifying light and heavy
syllables, and PadyaBhedam Checker for automated pattern recognition. Our
algorithm achieves 91.73% accuracy on the proposed Chandassu Score, with
evaluation metrics reflecting traditional literary standards. This work
demonstrates how computational social science can preserve endangered cultural
knowledge systems while enabling new forms of collective intelligence around
literary heritage. The methodology offers insights for community-centered
approaches to cultural preservation, supporting broader initiatives in digital
humanities and socially-aware computing systems.

</details>


### [14] [LLMRank: Understanding LLM Strengths for Model Routing](https://arxiv.org/abs/2510.01234)
*Shubham Agrawal,Prasang Gupta*

Main category: cs.CL

TL;DR: LLMRank是一个提示感知的路由框架，它利用可解释的特征和神经排序模型，为每个提示选择最合适的LLM，以平衡性能与效率，并取得接近最优的效用和可解释的决策。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）种类繁多，能力、延迟和计算成本各异，这使得在部署时面临严峻挑战：如何为每个提示选择最合适的模型，以优化性能和效率之间的权衡。

Method: 引入LLMRank，一个提示感知的路由框架。它从提示中提取丰富的、人类可读的特征（包括任务类型、推理模式、复杂性指标、句法线索和轻量级代理求解器的信号）。LLMRank使用一个神经排序模型预测每个模型的效用，该模型在RouterBench数据集（包含36,497个提示、11个基准和11个LLM）上进行训练。

Result: 该方法达到了高达89.2%的理论最优效用（oracle utility），并提供了可解释的特征归因，能够解释路由决策。广泛研究表明多方面特征提取和混合排序目标的重要性。

Conclusion: 特征驱动的路由方法对于高效和透明的LLM部署具有巨大潜力，突显了多方面特征提取和混合排序目标在优化LLM选择中的关键作用。

Abstract: The rapid growth of large language models (LLMs) with diverse capabilities,
latency and computational costs presents a critical deployment challenge:
selecting the most suitable model for each prompt to optimize the trade-off
between performance and efficiency. We introduce LLMRank, a prompt-aware
routing framework that leverages rich, human-readable features extracted from
prompts, including task type, reasoning patterns, complexity indicators,
syntactic cues, and signals from a lightweight proxy solver. Unlike prior
one-shot routers that rely solely on latent embeddings, LLMRank predicts
per-model utility using a neural ranking model trained on RouterBench,
comprising 36,497 prompts spanning 11 benchmarks and 11 state-of-the-art LLMs,
from small efficient models to large frontier systems. Our approach achieves up
to 89.2% of oracle utility, while providing interpretable feature attributions
that explain routing decisions. Extensive studies demonstrate the importance of
multifaceted feature extraction and the hybrid ranking objective, highlighting
the potential of feature-driven routing for efficient and transparent LLM
deployment.

</details>


### [15] [GRPO++: Enhancing Dermatological Reasoning under Low Resource Settings](https://arxiv.org/abs/2510.01236)
*Ismam Nur Swapnil,Aranya Saha,Tanvir Ahmed Khan,Mohammad Ariful Haque*

Main category: cs.CL

TL;DR: DermIQ-VLM是一种多阶段、资源高效的视觉-语言模型，通过改进的GRPO++和DPO方法，实现了皮肤病诊断中的结构化推理和会话能力，并在有限资源下展现出显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型在医学图像分析中有潜力，但在皮肤病等复杂领域，其结构化推理能力受限于数据稀缺和高计算成本。

Method: 引入DermIQ-VLM，采用多阶段、资源高效的方法模拟皮肤科医生诊断过程。核心是修改后的GRPO++，用于稳定数据密集型GRPO框架，实现面向推理的疾病识别。随后进行监督微调以获得会话能力。最后，利用基于知识图谱的系统作为专家偏好代理，通过DPO对模型进行对齐，以减少事实性错误。

Result: 在精选皮肤病数据集上的初步评估表明，DermIQ-VLM方法相比标准微调方法取得了显著的性能提升。

Conclusion: 该研究验证了DermIQ-VLM方法作为在资源受限环境下开发专业、可靠视觉-语言模型的可行途径。

Abstract: Vision-Language Models (VLMs) show promise in medical image analysis, yet
their capacity for structured reasoning in complex domains like dermatology is
often limited by data scarcity and the high computational cost of advanced
training techniques. To address these challenges, we introduce DermIQ-VLM, a
VLM developed through a multi-stage, resource-efficient methodology designed to
emulate a dermatologist's diagnostic process. Our primary contribution is a
modified version of Grouped Relative Policy Optimization (GRPO), called GRPO++,
which stabilizes the powerful but data-intensive GRPO framework. Our proposed
training pipeline first employs GRPO++ for reasoning-oriented disease
recognition, followed by supervised fine-tuning for conversational ability. To
mitigate factual errors introduced during this step, we then align the model
using Direct Preference Optimization (DPO), leveraging a Knowledge Graph-based
system as a scalable proxy for expert preference. A preliminary evaluation on a
curated dermatological dataset demonstrates that our proposed methodology
yields notable performance gains over standard fine-tuning approaches. These
findings validate the potential of our pipeline as a feasible pathway for
developing specialized, reliable VLMs in resource-constrained environments.

</details>


### [16] [Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation](https://arxiv.org/abs/2510.01237)
*Nandakishor M*

Main category: cs.CL

TL;DR: 提出了一种信心感知的路由系统，通过预先评估LLM的不确定性来主动缓解幻觉，显著提高了幻觉检测能力并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在幻觉问题，生成的内容看似合理却不符合事实。现有的缓解策略主要集中在生成后纠正，这不仅计算成本高昂，而且无法有效阻止不可靠内容的生成。

Method: 该方法构建了一个信心感知路由系统，在生成前主动评估模型的不确定性。它结合了三种互补信号：内部表示与参考嵌入之间的语义对齐、模型层间的内部收敛分析以及学习到的信心估计。统一的信心分数决定了查询路由到四种路径：高信心度走本地生成，中等信心度走检索增强生成（RAG），低信心度走更大模型，极低信心度走人工审查。

Result: 在知识密集型问答基准测试中，幻觉检测能力显著提高（0.74 vs 0.42 基线），与事后纠正方法相比，计算成本降低了40%。F1分数从0.61提高到0.82，同时保持较低的误报率（0.09）。

Conclusion: 从被动纠正转向主动评估的范式转变，为提升LLM的可靠性提供了一种计算高效的方法。

Abstract: Large Language Models suffer from hallucination, generating plausible yet
factually incorrect content. Current mitigation strategies focus on
post-generation correction, which is computationally expensive and fails to
prevent unreliable content generation. We propose a confidence-aware routing
system that proactively assesses model uncertainty before generation and
redirects queries based on estimated reliability. Our approach combines three
complementary signals: semantic alignment between internal representations and
reference embeddings, internal convergence analysis across model layers, and
learned confidence estimation. The unified confidence score determines routing
to four pathways: local generation for high confidence, retrieval-augmented
generation for medium confidence, larger models for low confidence, and human
review for very low confidence. Evaluation on knowledge-intensive QA benchmarks
demonstrates significant improvements in hallucination detection (0.74 vs. 0.42
baseline) while reducing computational costs by 40% compared to post-hoc
methods. The F1 score improves from 0.61 to 0.82 with low false positive rates
(0.09). This paradigm shift from reactive correction to proactive assessment
offers a computationally efficient approach to LLM reliability enhancement.

</details>


### [17] [Silent Tokens, Loud Effects: Padding in LLMs](https://arxiv.org/abs/2510.01238)
*Rom Himelstein,Amit LeVi,Yonatan Belinkov,Avi Mendelson*

Main category: cs.CL

TL;DR: 本研究发现，大型语言模型中的填充令牌，即使数量不多，也可能因实现错误而影响计算，导致模型隐藏表示偏移、生成质量下降、偏见改变和安全防护削弱，揭示其是一个重要的鲁棒性风险。


<details>
  <summary>Details</summary>
Motivation: 填充令牌在大型语言模型批处理推理中广泛应用，虽应被完全掩盖，但实现错误可能使其影响计算，且这种影响的程度尚未被充分理解。

Method: 研究人员系统性地在Llama、Gemma和Qwen三个开源模型家族中研究了填充令牌的影响。通过插入受控数量的填充，评估了激活、生成质量、偏见和安全性四个维度的结果。

Result: 研究发现，即使是少量填充，也会导致隐藏表示偏移，降低小型模型的生成质量，以不可预测的方式改变偏见，并削弱安全防护。

Conclusion: 填充令牌并非无害的细节，而是一个必须在部署中仔细处理的鲁棒性风险。

Abstract: Padding tokens are widely used in large language models (LLMs) to equalize
sequence lengths during batched inference. While they should be fully masked,
implementation errors can cause them to influence computation, and the extent
of this influence is not well understood. We systematically study this effect
across three open-source model families (Llama, Gemma, Qwen), inserting
controlled amounts of padding and evaluating outcomes along four axes:
activations, generation quality, bias, and safety. Even small amounts of
padding shift hidden representations, degrade quality in smaller models, alter
bias in unpredictable ways, and weaken safety guardrails. These findings
demonstrate that padding is not a harmless detail but a robustness risk that
must be carefully handled in deployment.

</details>


### [18] [CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn Interactions with a Single On-Device LLM](https://arxiv.org/abs/2510.01239)
*Juntae Lee,Jihwan Bang,Seunghan Yang,Simyung Chang*

Main category: cs.CL

TL;DR: CIFLEX是一种用于设备端LLM多轮交互的执行系统，通过复用KV缓存和侧路径指令注入，显著降低了处理子任务时的计算开销，同时保持了任务性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM能力增强，单个模型需处理多样子任务以支持用户请求，但现有方法在主任务与子任务切换时，需重新处理整个对话上下文，导致高昂计算开销，尤其在设备端。

Method: CIFLEX通过复用主任务的键值（KV）缓存，并将特定任务指令注入到独立的侧路径中来处理子任务。子任务执行后，模型通过缓存上下文回滚到主路径，避免了冗余的预填充计算。此外，还开发了一种分层分类策略，将多选决策分解为二元决策，以支持子任务选择，尤其适用于小型模型。

Result: 实验表明，CIFLEX在不降低任务性能的情况下，显著减少了计算成本。

Conclusion: CIFLEX使设备端LLM能够进行可扩展、高效的多任务对话。

Abstract: We present CIFLEX (Contextual Instruction Flow for Sub-task Execution), which
is a novel execution system for efficient sub-task handling in multi-turn
interactions with a single on-device large language model (LLM). As LLMs become
increasingly capable, a single model is expected to handle diverse sub-tasks
that more effectively and comprehensively support answering user requests.
Naive approach reprocesses the entire conversation context when switching
between main and sub-tasks (e.g., query rewriting, summarization), incurring
significant computational overhead. CIFLEX mitigates this overhead by reusing
the key-value (KV) cache from the main task and injecting only task-specific
instructions into isolated side paths. After sub-task execution, the model
rolls back to the main path via cached context, thereby avoiding redundant
prefill computation. To support sub-task selection, we also develop a
hierarchical classification strategy tailored for small-scale models,
decomposing multi-choice decisions into binary ones. Experiments show that
CIFLEX significantly reduces computational costs without degrading task
performance, enabling scalable and efficient multi-task dialogue on-device.

</details>


### [19] [SKYLENAGE Technical Report: Mathematical Reasoning and Contest-Innovation Benchmarks for Multi-Level Math Evaluation](https://arxiv.org/abs/2510.01241)
*Hu Wei,Ze Xu,Boyu Yang,Linlin Miao,Weiqi Zhai,Yihan Li,Zixuan Li,Zhijun Wang,Boya Wang,Jianwei Yu,Jialing Yuan,Xiaoyue Zhang,Cheng He,Minglei Chen,Zifan Zhang,Qianhui Li,Wei Wang,Xiang Xu*

Main category: cs.CL

TL;DR: 为解决现有数学基准天花板效应，提出SKYLENAGE系列数学基准（ReasoningMATH和MATH）。评估了15种LLM，揭示了模型在不同难度和推理任务上的性能差距，并提供了未来数学推理评估的参考基准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在现有公共数学基准上表现优异，但前沿数学领域的区分度日益受天花板效应影响，难以有效评估和区分顶级模型。

Method: 构建了两个互补的基准测试集：SKYLENAGE-ReasoningMATH（100项结构感知诊断集，含元数据）和SKYLENAGE-MATH（150项竞赛风格测试集，涵盖高中至博士阶段，七个主题分类）。在一个统一设置下，评估了15种当代LLM变体，并分析了主题x模型和年级x模型的性能。

Result: 在竞赛套件上，最强模型准确率达44%，次强为37%；准确率随难度（从高中到博士）下降，顶级系统在博士到高中阶段的保留率接近79%。在推理套件上，最佳模型整体达81%，且在最难切片上，领先模型与中层模型之间存在明显的鲁棒性差距。

Conclusion: 发布了SKYLENAGE-ReasoningMATH并报告了SKYLENAGE-MATH的聚合结果。SKYLENAGE作为一个高难度、以推理为中心、覆盖广泛、难度校准且包含丰富元数据的数学基准，可作为未来数学推理评估的重要参考。

Abstract: Large language models (LLMs) now perform strongly on many public math suites,
yet frontier separation within mathematics increasingly suffers from ceiling
effects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a
100-item, structure-aware diagnostic set with per-item metadata on length,
numeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item
contest-style suite spanning four stages from high school to doctoral under a
seven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a
single setup and analyze subject x model and grade x model performance. On the
contest suite, the strongest model reaches 44% while the runner-up reaches 37%;
accuracy declines from high school to doctoral, and top systems exhibit a
doctoral-to-high-school retention near 79%. On the reasoning set, the best
model attains 81% overall, and hardest-slice results reveal clear robustness
gaps between leaders and the mid-tier. In summary, we release
SKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH;
together, SKYLENAGE provides a hard, reasoning-centered and broadly covering
math benchmark with calibrated difficulty and rich metadata, serving as a
reference benchmark for future evaluations of mathematical reasoning.

</details>


### [20] [Redundancy-as-Masking: Formalizing the Artificial Age Score (AAS) to Model Memory Aging in Generative AI](https://arxiv.org/abs/2510.01242)
*Seyma Yaman Kayadibi*

Main category: cs.CL

TL;DR: 本文引入了人工年龄评分（AAS），一个基于熵信息和召回行为的度量，用于诊断大型语言模型中的记忆老化现象，并验证了其在识别上下文重置导致的记忆退化方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 观察到人工智能（特别是大型语言模型）的记忆表现存在结构性不对称，即在会话上下文重置时，语义信息保持稳定但情景细节易于丢失，研究旨在量化和捕捉这种非时间性的“记忆老化”现象。

Method: 提出了人工年龄评分（AAS），这是一个对数尺度、熵信息的记忆老化度量，基于可观察的召回行为，并被数学证明是定义良好、有界且单调的。该框架通过一个为期25天的双语研究，在ChatGPT-5上进行测试，实验设计包含无状态和持久交互阶段。

Result: 在持久会话中，模型能一致回忆语义和情景细节，导致AAS趋向其理论最小值，表明结构性“年轻”。然而，当会话被重置时，模型虽然保留了语义一致性，却失去了情景连续性，导致AAS急剧增加，标志着结构性记忆老化。

Conclusion: 研究结果支持人工年龄评分（AAS）作为一种理论基础扎实、任务独立的诊断工具，能够有效评估人工智能系统中的记忆退化。这项工作借鉴了冯·诺依曼的自动机、香农的信息论和图灵的行为智能理论。

Abstract: Artificial intelligence is observed to age not through chronological time but
through structural asymmetries in memory performance. In large language models,
semantic cues such as the name of the day often remain stable across sessions,
while episodic details like the sequential progression of experiment numbers
tend to collapse when conversational context is reset. To capture this
phenomenon, the Artificial Age Score (AAS) is introduced as a log-scaled,
entropy-informed metric of memory aging derived from observable recall
behavior. The score is formally proven to be well-defined, bounded, and
monotonic under mild and model-agnostic assumptions, making it applicable
across various tasks and domains. In its Redundancy-as-Masking formulation, the
score interprets redundancy as overlapping information that reduces the
penalized mass. However, in the present study, redundancy is not explicitly
estimated; all reported values assume a redundancy-neutral setting (R = 0),
yielding conservative upper bounds. The AAS framework was tested over a 25-day
bilingual study involving ChatGPT-5, structured into stateless and persistent
interaction phases. During persistent sessions, the model consistently recalled
both semantic and episodic details, driving the AAS toward its theoretical
minimum, indicative of structural youth. In contrast, when sessions were reset,
the model preserved semantic consistency but failed to maintain episodic
continuity, causing a sharp increase in the AAS and signaling structural memory
aging. These findings support the utility of AAS as a theoretically grounded,
task-independent diagnostic tool for evaluating memory degradation in
artificial systems. The study builds on foundational concepts from von
Neumann's work on automata, Shannon's theories of information and redundancy,
and Turing's behavioral approach to intelligence.

</details>


### [21] [Detoxifying Large Language Models via Autoregressive Reward Guided Representation Editing](https://arxiv.org/abs/2510.01243)
*Yisong Xiao,Aishan Liu,Siyuan Liang,Zonghao Ying,Xianglong Liu,Dacheng Tao*

Main category: cs.CL

TL;DR: ARGRE是一种新的测试时解毒框架，通过显式建模潜在表示空间中的毒性转换并使用自回归奖励模型进行指导，实现了对LLM输出的稳定和精确的解毒编辑，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）容易生成有害内容，而现有测试时解毒方法由于对毒性与非毒性输出之间的转换空间探索不足，导致干预不精确。

Method: 提出ARGRE框架，通过显式建模潜在表示空间中的毒性转换实现精确的奖励引导编辑。它识别非毒性语义方向，插值生成细粒度转换轨迹，将稀疏标注转化为密集信号以构建自回归奖励模型。推理时，该模型指导两步编辑：先基于预期奖励差距进行方向性引导，再进行轻量级梯度精修。

Result: 广泛实验表明，ARGRE在8个常用LLM上显著优于领先基线，有效性（毒性降低62.21%）和效率（推理时间减少47.58%）均有提升，同时最小化了对原始模型核心能力的损害。

Conclusion: ARGRE通过创新的毒性转换建模和奖励引导编辑，为LLM提供了一种高效、精确且保持模型能力的测试时解毒解决方案，成功克服了现有方法的局限性。

Abstract: Large Language Models (LLMs) have demonstrated impressive performance across
various tasks, yet they remain vulnerable to generating toxic content,
necessitating detoxification strategies to ensure safe and responsible
deployment. Test-time detoxification methods, which typically introduce static
or dynamic interventions into LLM representations, offer a promising solution
due to their flexibility and minimal invasiveness. However, current approaches
often suffer from imprecise interventions, primarily due to their insufficient
exploration of the transition space between toxic and non-toxic outputs. To
address this challenge, we propose \textsc{A}utoregressive \textsc{R}eward
\textsc{G}uided \textsc{R}epresentation \textsc{E}diting (ARGRE), a novel
test-time detoxification framework that explicitly models toxicity transitions
within the latent representation space, enabling stable and precise
reward-guided editing. ARGRE identifies non-toxic semantic directions and
interpolates between toxic and non-toxic representations to reveal fine-grained
transition trajectories. These trajectories transform sparse toxicity
annotations into dense training signals, enabling the construction of an
autoregressive reward model that delivers stable and precise editing guidance.
At inference, the reward model guides an adaptive two-step editing process to
obtain detoxified representations: it first performs directional steering based
on expected reward gaps to shift representations toward non-toxic regions,
followed by lightweight gradient-based refinements. Extensive experiments
across 8 widely used LLMs show that ARGRE significantly outperforms leading
baselines in effectiveness (-62.21% toxicity) and efficiency (-47.58% inference
time), while preserving the core capabilities of the original model with
minimal degradation. Our code is available at the website.

</details>


### [22] [Feasibility of Structuring Stress Documentation Using an Ontology-Guided Large Language Model](https://arxiv.org/abs/2510.01244)
*Hyeoneui Kim,Jeongha Kim,Huijing Xu,Jinsun Jung,Sunghoon Kang,Sun Joo Jang*

Main category: cs.CL

TL;DR: 本研究开发了一种精神压力本体论（MeSO），并验证了使用大语言模型（LLM）从叙述文本中提取本体论指导的压力相关信息的可行性，旨在提高压力记录的一致性和实用性。


<details>
  <summary>Details</summary>
Motivation: 压力严重影响健康，但常被低估且记录不一致，通常以非结构化的自由文本形式存在于电子健康记录中。环境AI技术有望减轻文档负担，但其输出也多为非结构化叙述，限制了临床实用性。因此，需要一种结构化的方法来改进压力信息的文档和利用。

Method: 本研究通过整合压力交易模型和11种已验证的压力评估工具的概念，开发了精神压力本体论（MeSO），并使用Ontology Pitfall Scanner!和专家验证对其结构和内容进行完善。随后，使用MeSO指导Claude Sonnet 4从35篇Reddit帖子中提取了六类压力相关信息（压力源、压力反应、应对策略、持续时间、发作和时间概况），并由人工审阅者评估了提取的准确性和本体覆盖率。

Result: 最终的MeSO包含了八个顶级类别下的181个概念。在220个可提取的压力相关项中，LLM正确识别了172个（78.2%），错误分类了27个（12.3%），遗漏了21个（9.5%）。所有正确提取的项都准确映射到MeSO，尽管本体中尚有24个相关概念未被收录。

Conclusion: 本研究证明了使用本体论指导的LLM进行压力相关信息结构化提取的可行性，这有望提高环境AI系统中压力文档的一致性和实用性。未来的工作应涉及临床对话数据和不同LLM之间的比较。

Abstract: Stress, arising from the dynamic interaction between external stressors,
individual appraisals, and physiological or psychological responses,
significantly impacts health yet is often underreported and inconsistently
documented, typically captured as unstructured free-text in electronic health
records. Ambient AI technologies offer promise in reducing documentation
burden, but predominantly generate unstructured narratives, limiting downstream
clinical utility.
  This study aimed to develop an ontology for mental stress and evaluate the
feasibility of using a Large Language Model (LLM) to extract ontology-guided
stress-related information from narrative text. The Mental Stress Ontology
(MeSO) was developed by integrating theoretical models like the Transactional
Model of Stress with concepts from 11 validated stress assessment tools. MeSO's
structure and content were refined using Ontology Pitfall Scanner! and expert
validation.
  Using MeSO, six categories of stress-related information--stressor, stress
response, coping strategy, duration, onset, and temporal profile--were
extracted from 35 Reddit posts using Claude Sonnet 4. Human reviewers evaluated
accuracy and ontology coverage. The final ontology included 181 concepts across
eight top-level classes. Of 220 extractable stress-related items, the LLM
correctly identified 172 (78.2%), misclassified 27 (12.3%), and missed 21
(9.5%). All correctly extracted items were accurately mapped to MeSO, although
24 relevant concepts were not yet represented in the ontology.
  This study demonstrates the feasibility of using an ontology-guided LLM for
structured extraction of stress-related information, offering potential to
enhance the consistency and utility of stress documentation in ambient AI
systems. Future work should involve clinical dialogue data and comparison
across LLMs.

</details>


### [23] [SeMob: Semantic Synthesis for Dynamic Urban Mobility Prediction](https://arxiv.org/abs/2510.01245)
*Runfei Chen,Shuyang Jiang,Wei Huang*

Main category: cs.CL

TL;DR: SeMob是一个LLM驱动的语义合成管道，用于通过整合在线文本中的事件描述来改进动态人类出行预测，尤其是在外部事件引起突变时。


<details>
  <summary>Details</summary>
Motivation: 现有时空模型难以处理外部事件引起的出行剧烈变化，且无法有效利用描述这些事件的文本信息。

Method: SeMob采用多智能体LLM框架自动提取和推理时空相关文本，并通过创新的渐进式融合架构将细粒度文本上下文与时空数据结合，同时利用丰富的预训练事件先验知识。

Result: SeMob在MAE上实现了13.92%的最大降幅，在RMSE上实现了11.12%的最大降幅，特别在事件发生地点和时间附近的时空区域表现出显著优势。

Conclusion: SeMob通过有效整合来自外部事件的语义信息，显著提高了动态出行预测的准确性，尤其是在处理事件驱动的出行变化方面。

Abstract: Human mobility prediction is vital for urban services, but often fails to
account for abrupt changes from external events. Existing spatiotemporal models
struggle to leverage textual descriptions detailing these events. We propose
SeMob, an LLM-powered semantic synthesis pipeline for dynamic mobility
prediction. Specifically, SeMob employs a multi-agent framework where LLM-based
agents automatically extract and reason about spatiotemporally related text
from complex online texts. Fine-grained relevant contexts are then incorporated
with spatiotemporal data through our proposed innovative progressive fusion
architecture. The rich pre-trained event prior contributes enriched insights
about event-driven prediction, and hence results in a more aligned forecasting
model. Evaluated on a dataset constructed through our pipeline, SeMob achieves
maximal reductions of 13.92% in MAE and 11.12% in RMSE compared to the
spatiotemporal model. Notably, the framework exhibits pronounced superiority
especially within spatiotemporal regions close to an event's location and time
of occurrence.

</details>


### [24] [A Comparative Analysis of Sparse Autoencoder and Activation Difference in Language Model Steering](https://arxiv.org/abs/2510.01246)
*Jiaqing Xie*

Main category: cs.CL

TL;DR: 本研究提出一种改进的SAE语言模型操纵方法，通过关注单个最相关潜在维度和引入逐词衰减策略，有效提升了数学推理能力并优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有SAE top-k潜在维度包含非语义特征，导致操纵不够精准；恒定SAE操纵会导致生成退化输出（如重复单词）。

Method: 1. 聚焦于单个最相关的SAE潜在维度（top-1），以消除冗余的非语义特征。2. 引入逐词衰减的操纵策略，以避免恒定操纵导致的输出退化。

Result: 1. 成功诱导模型产生逐步数学推理，并提升了推理质量，效果类似于添加引导性token。2. 在数学推理基准测试中，SAE表现优于平均激活差异方法，并在IF-Eval上匹配其性能。

Conclusion: 改进的SAE方法能够有效且可靠地操纵语言模型生成高质量的数学推理，并且在相关基准测试中超越或匹配了现有基线方法。

Abstract: Sparse autoencoders (SAEs) have recently emerged as a powerful tool for
language model steering. Prior work has explored top-k SAE latents for
steering, but we observe that many dimensions among the top-k latents capture
non-semantic features such as punctuation rather than semantic attributes like
instructions. To address this, we propose focusing on a single, most relevant
SAE latent (top-1), eliminating redundant features. We further identify a
limitation in constant SAE steering, which often produces degenerate outputs
such as repetitive single words. To mitigate this, we introduce a token-wise
decaying steering strategy, enabling more faithful comparisons with mean
activation difference baselines. Empirically, we show that steering an SAE
latent associated with reasoning reliably elicits step-by-step mathematical
reasoning and enhances inference quality, functionally resembling the effect of
appending a guiding token. Our results demonstrate that SAEs outperform mean
activation difference methods on mathematical reasoning benchmarks and match
their performance on IF-Eval.

</details>


### [25] [Let's Play Across Cultures: A Large Multilingual, Multicultural Benchmark for Assessing Language Models' Understanding of Sports](https://arxiv.org/abs/2510.01247)
*Punit Kumar Singh,Nishant Kumar,Akash Ghosh,Kunal Pasad,Khushi Soni,Manisha Jaishwal,Sriparna Saha,Syukron Abu Ishaq Alfarozi,Asres Temam Abagissa,Kitsuchart Pasupa,Haiqin Yang,Jose G Moreno*

Main category: cs.CL

TL;DR: 本文引入了CultSportQA，一个多语言、多模态的基准数据集，用于评估语言模型对全球60个国家传统体育的理解能力。


<details>
  <summary>Details</summary>
Motivation: 语言模型主要在流行体育项目上进行评估，忽略了区域性和本土的传统体育，存在评估空白。

Method: 构建了CultSportQA基准，涵盖60个国家、6大洲的传统体育，分为四种文化类别。数据集包含33,000道文本和图像多选题，问题类型包括历史、规则和场景。采用零样本、少样本和思维链提示方法，评估了大型语言模型（LLMs）、小型语言模型（SLMs）和多模态大型语言模型（MLMs）。

Result: 创建了一个全面的多语言、多文化的体育基准数据集CultSportQA，旨在评估AI理解和推理传统体育的能力。

Conclusion: CultSportQA为评估AI理解和推理传统体育的能力建立了新的标准。

Abstract: Language Models (LMs) are primarily evaluated on globally popular sports,
often overlooking regional and indigenous sporting traditions. To address this
gap, we introduce \textbf{\textit{CultSportQA}}, a benchmark designed to assess
LMs' understanding of traditional sports across 60 countries and 6 continents,
encompassing four distinct cultural categories. The dataset features 33,000
multiple-choice questions (MCQs) across text and image modalities, each of
which is categorized into three key types: history-based, rule-based, and
scenario-based. To evaluate model performance, we employ zero-shot, few-shot,
and chain-of-thought (CoT) prompting across a diverse set of Large Language
Models (LLMs), Small Language Models (SLMs), and Multimodal Large Language
Models (MLMs). By providing a comprehensive multilingual and multicultural
sports benchmark, \textbf{\textit{CultSportQA}} establishes a new standard for
assessing AI's ability to understand and reason about traditional sports.

</details>


### [26] [SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs](https://arxiv.org/abs/2510.01248)
*Ruyue Liu,Rong Yin,Xiangzhen Bo,Xiaoshuai Hao,Yong Liu,Jinwen Zhong,Can Ma,Weiping Wang*

Main category: cs.CL

TL;DR: 本文提出SSTAG，一种针对文本属性图（TAGs）的结构感知自监督学习方法。SSTAG利用文本作为统一表示，通过LLM和GNN的双重知识蒸馏到MLP，并结合内存机制，显著提升了跨域泛化能力、可扩展性，并降低了推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有图学习模型缺乏跨图和任务的知识迁移能力，高度依赖大量标注数据。图数据固有的异构性（如领域特定特征空间和结构多样性）带来了独特挑战。需要弥合大语言模型（LLMs）的语义推理能力和图神经网络（GNNs）的结构建模能力之间的鸿沟。

Method: 本文提出SSTAG（Structure-aware Self-supervised learning for Text Attributed Graphs）方法。它利用文本作为图学习的统一表示媒介，引入了一个双重知识蒸馏框架，将LLMs和GNNs共同蒸馏到结构感知的多层感知器（MLPs）中，以增强大规模TAGs的可扩展性。此外，还引入了一种内存机制，存储典型的图表示，并将其与内存库中的内存锚点对齐以整合不变知识，从而提高模型的泛化能力。

Result: SSTAG在跨域迁移学习任务上超越了现有最先进的模型，实现了卓越的可扩展性，并在保持竞争性性能的同时降低了推理成本。

Conclusion: SSTAG通过其新颖的结构感知自监督学习、双重知识蒸馏和内存机制，有效解决了文本属性图学习中跨域泛化、可扩展性和数据依赖性等挑战，展现出优越的性能和效率。

Abstract: Large scale pretrained models have revolutionized Natural Language Processing
(NLP) and Computer Vision (CV), showcasing remarkable cross domain
generalization abilities. However, in graph learning, models are typically
trained on individual graph datasets, limiting their capacity to transfer
knowledge across different graphs and tasks. This approach also heavily relies
on large volumes of annotated data, which presents a significant challenge in
resource-constrained settings. Unlike NLP and CV, graph structured data
presents unique challenges due to its inherent heterogeneity, including domain
specific feature spaces and structural diversity across various applications.
To address these challenges, we propose a novel structure aware self supervised
learning method for Text Attributed Graphs (SSTAG). By leveraging text as a
unified representation medium for graph learning, SSTAG bridges the gap between
the semantic reasoning of Large Language Models (LLMs) and the structural
modeling capabilities of Graph Neural Networks (GNNs). Our approach introduces
a dual knowledge distillation framework that co-distills both LLMs and GNNs
into structure-aware multilayer perceptrons (MLPs), enhancing the scalability
of large-scale TAGs. Additionally, we introduce an in-memory mechanism that
stores typical graph representations, aligning them with memory anchors in an
in-memory repository to integrate invariant knowledge, thereby improving the
model's generalization ability. Extensive experiments demonstrate that SSTAG
outperforms state-of-the-art models on cross-domain transfer learning tasks,
achieves exceptional scalability, and reduces inference costs while maintaining
competitive performance.

</details>


### [27] [LOCA: Logical Chain Augmentation for Scientific Corpus Cleaning](https://arxiv.org/abs/2510.01249)
*You-Le Fang,Dong-Shan Jian,Xiang Li,Ce Meng,Ling-Shi Meng,Chen-Xu Yan,Zhi-Zhang Bian,Yan-Qing Ma*

Main category: cs.CL

TL;DR: 本研究提出LOCA框架，通过逻辑链增强自动清洗科学问答数据集，将错误率从高达20%降至2%以下，旨在提升科学AI的训练和评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在通用领域表现出色，但在科学问题解决中可靠性不足。科学AI的发展依赖大规模高质量语料，然而现有科学问答数据集因逻辑跳跃和隐性推理导致错误率高。

Method: 引入LOCA（Logical Chain Augmentation）框架，通过“增强-审查”循环自动清洗科学语料。LOCA的核心是补全答案中缺失的逻辑步骤，并明确区分科学原理及其推导过程，以提升答案质量。

Result: 将LOCA应用于高难度科学语料库后，该方法能自动过滤噪声数据，有效将数据集错误率从高达20%降低到2%以下。

Conclusion: LOCA提供了一种可扩展且有效的方法来创建高质量科学语料库，为更可靠地训练和评估科学AI奠定基础。

Abstract: While Large Language Models (LLMs) excel in general domains, their
reliability often falls short in scientific problem-solving. The advancement of
scientific AI depends on large-scale, high-quality corpora. However, existing
scientific question-answering (QA) datasets suffer from high error rates,
frequently resulting from logical leaps and implicit reasoning within the
answers. To address this issue, we introduce LOCA (Logical Chain Augmentation),
a novel framework for automatically cleaning scientific corpora, implemented
through an augment-and-review loop. At its core, LOCA enhances raw answers by
completing missing logical steps and explicitly separating the underlying
scientific principle from its subsequent derivation. By applying LOCA to
challenging scientific corpora, we demonstrate that it can automatically filter
noisy datasets, typically reducing the error rate from as high as 20\% to below
2\%. LOCA provides a scalable and effective methodology for creating
high-quality scientific corpora, paving the way for more reliable training and
evaluation of scientific AI.

</details>


### [28] [GemDetox at TextDetox CLEF 2025: Enhancing a Massively Multilingual Model for Text Detoxification on Low-resource Languages](https://arxiv.org/abs/2510.01250)
*Trung Duc Anh Dang,Ferdinando Pio D'Elia*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As social-media platforms emerge and evolve faster than the regulations meant
to oversee them, automated detoxification might serve as a timely tool for
moderators to enforce safe discourse at scale. We here describe our submission
to the PAN 2025 Multilingual Text Detoxification Challenge, which rewrites
toxic single-sentence inputs into neutral paraphrases across 15 typologically
diverse languages. Building on a 12B-parameter Gemma-3 multilingual
transformer, we apply parameter-efficient LoRA SFT fine-tuning and prompting
techniques like few-shot and Chain-of-Thought. Our multilingual training corpus
combines 3,600 human-authored parallel pairs, 21,600 machine-translated
synthetic pairs, and model-generated pairs filtered by Jaccard thresholds. At
inference, inputs are enriched with three LaBSE-retrieved neighbors and
explicit toxic-span annotations. Evaluated via Style Transfer Accuracy,
LaBSE-based semantic preservation, and xCOMET fluency, our system ranks first
on high-resource and low-resource languages. Ablations show +0.081 joint score
increase from few-shot examples and +0.088 from basic CoT prompting. ANOVA
analysis identifies language resource status as the strongest predictor of
performance ($\eta^2$ = 0.667, p < 0.01).

</details>


### [29] [Efficient Uncertainty Estimation for LLM-based Entity Linking in Tabular Data](https://arxiv.org/abs/2510.01251)
*Carlo Bono,Federico Belotti,Matteo Palmonari*

Main category: cs.CL

TL;DR: 本文提出一种自监督方法，利用单次LLM输出的token级特征，高效估计表格数据实体链接（EL）任务中的不确定性，有效识别低准确度输出，且计算成本低廉，解决了LLM在EL任务中不确定性估计效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在实体链接（EL）任务中表现出色，但其在实际部署时需可靠的不确定性估计。传统的多样本推理资源消耗巨大，严重限制了LLMs的实际应用。

Method: 研究人员提出了一种自监督方法，利用LLM单次输出的token级特征来估计不确定性，从而减少对多样本生成的依赖，作为多样本推理的更高效替代方案。

Result: 该方法在表格数据实体链接任务中，跨多个LLM进行评估，结果显示其不确定性估计能高效检测低准确度输出，且计算成本远低于传统方法。

Conclusion: 该方法提供了一种实用且计算开销有限的方式，将不确定性估计融入基于LLM的实体链接工作流程中，支持成本效益高的集成和部署。

Abstract: Linking textual values in tabular data to their corresponding entities in a
Knowledge Base is a core task across a variety of data integration and
enrichment applications. Although Large Language Models (LLMs) have shown
State-of-The-Art performance in Entity Linking (EL) tasks, their deployment in
real-world scenarios requires not only accurate predictions but also reliable
uncertainty estimates, which require resource-demanding multi-shot inference,
posing serious limits to their actual applicability. As a more efficient
alternative, we investigate a self-supervised approach for estimating
uncertainty from single-shot LLM outputs using token-level features, reducing
the need for multiple generations. Evaluation is performed on an EL task on
tabular data across multiple LLMs, showing that the resulting uncertainty
estimates are highly effective in detecting low-accuracy outputs. This is
achieved at a fraction of the computational cost, ultimately supporting a
cost-effective integration of uncertainty measures into LLM-based EL workflows.
The method offers a practical way to incorporate uncertainty estimation into EL
workflows with limited computational overhead.

</details>


### [30] [GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models](https://arxiv.org/abs/2510.01252)
*Mariam Mahran,Katharina Simbeck*

Main category: cs.CL

TL;DR: 研究表明，将大型语言模型（LLMs）与稀疏自编码器（SAEs）结合，可以有效解释模型行为及其训练数据中嵌入的深层结构、主题和偏见。通过在简·奥斯汀小说语料上的实验，SAEs揭示了性别、阶级等可解释特征，为大规模语料探索和模型可解释性提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 当前，大型语言模型在海量、未经整理的语料库上训练，导致理解模型内部表示及其内化数据成为一个重大挑战。

Method: 研究人员训练了一个基于简·奥斯汀小说语料（富含社会结构和叙事模式）的GPT风格Transformer模型，然后将稀疏自编码器（SAEs）应用于模型多个层的隐藏状态，以发现稀疏、可解释的特征。

Result: SAEs成功揭示了语料库中反映性别、阶级和社会责任等关键叙事和概念的稀疏、可解释特征。

Conclusion: 结合LLMs与SAEs可以作为复杂数据集的可扩展探测工具，为大规模语料探索、偏见发现和模型可解释性开辟了新途径。

Abstract: As large language models (LLMs) are increasingly trained on massive,
uncurated corpora, understanding both model representations and the data they
internalize has become a major challenge. In this work, we show that pairing
LLMs with sparse autoencoders (SAEs) enables interpretation not only of model
behavior but also of the deeper structures, themes, and biases embedded in the
training data. We train a GPT-style transformer model exclusively on the novels
of Jane Austen, a corpus rich in social constructs and narrative patterns. We
then apply SAEs to hidden states across multiple layers, uncovering sparse,
interpretable features that reflect the key narratives and concepts present in
the corpus, including gender, class, and societal duty. Our findings
demonstrate that LLMs combined with SAEs can act as scalable probes into
complex datasets, offering a new path for corpus exploration, bias discovery,
and model interpretability at scale.

</details>


### [31] [Do Bias Benchmarks Generalise? Evidence from Voice-based Evaluation of Gender Bias in SpeechLLMs](https://arxiv.org/abs/2510.01254)
*Shree Harsha Bokkahalli Satish,Gustav Eje Henter,Éva Székely*

Main category: cs.CL

TL;DR: 本文发现语音大语言模型在多项选择题问答（MCQA）偏见基准上的表现，难以泛化到其他MCQA任务或更真实的生成任务。


<details>
  <summary>Details</summary>
Motivation: 当前语音大语言模型（SpeechLLMs）的偏见和公平性基准测试严重依赖MCQA格式，并隐含假设模型性能在不同MCQA任务、语音及长文本评估中具有一致性。本文旨在探究这一假设的有效性。

Method: 使用LoRA适配器对三个SpeechLLMs进行微调，以诱导其在MCQA任务中对刻板印象、反刻板印象或中性答案的偏好。随后，评估这些行为是否能泛化到另一个不同的MCQA基准以及长文本、创造性生成任务中。

Result: 研究结果表明，模型在MCQA偏见基准上的表现无法可靠预测其在其他MCQA基准上的表现，更无法预测其在长文本任务中的表现。

Conclusion: 目前的MCQA偏见基准在语音领域显示出有限的跨任务泛化能力。作者提出了一个评估套件，用于衡量未来模型和基准中的行为可迁移性。

Abstract: Recent work in benchmarking bias and fairness in speech large language models
(SpeechLLMs) has relied heavily on multiple-choice question answering (MCQA)
formats. The model is tasked to choose between stereotypical,
anti-stereotypical, or neutral/irrelevant answers given an input speech prompt
and an optional text prompt. Such MCQA benchmarks implicitly assume that model
performance is consistent across other MCQA tasks, voices, and other task
formats such as more realistic, long-form evaluations. In this paper, we probe
that assumption.
  We fine-tune three SpeechLLMs using LoRA adapters to induce specific MCQA
behaviours: preference for stereotypical, anti-stereotypical, or
neutral/uncertain answers. We then evaluate whether these behaviours generalise
to another, distinct MCQA benchmark, and more critically to long-form, creative
generation tasks. Our results show that performance on MCQA bias benchmarks
fails to reliably predict performances across other MCQA benchmarks, and more
importantly across long-form tasks. We conclude that current MCQA bias
benchmarks show limited evidence of cross-task generalisation in the speech
domain, and also propose an evaluation suite for measuring behaviour
transferability in future models and benchmarks.

</details>


### [32] [Longitudinal Monitoring of LLM Content Moderation of Social Issues](https://arxiv.org/abs/2510.01255)
*Yunlang Dai,Emma Lurie,Danaé Metaxa,Sorelle A. Friedler*

Main category: cs.CL

TL;DR: 论文介绍了AI Watchman系统，用于长期审计大型语言模型（LLM）的内容拒绝行为，旨在提高LLM不透明内容审核政策的透明度，并能有效发现未公开的政策变化及模型间的差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的输出受不透明且频繁变化的公司内容审核政策影响，其拒绝生成特定内容不仅反映公司政策，也微妙地塑造了公众讨论。为了提高LLM内容审核这一黑盒方面的透明度，迫切需要一个系统来持续测量和追踪模型的拒绝行为。

Method: 研究引入了AI Watchman，一个纵向审计系统，用于公开测量和跟踪LLM随时间的拒绝行为。该系统使用一个包含400多个社会问题的综合数据集，对OpenAI的审核端点、GPT-4.1、GPT-5以及DeepSeek（包括英文和中文版本）进行了审计。此外，还对不同形式的拒绝进行了定性分析和分类。

Result: 研究发现，AI Watchman能够检测到公司未公开宣布的政策变化，并识别出不同公司和模型在内容审核方面的具体差异。同时，还对不同形式的拒绝进行了定性分析和分类。

Conclusion: 该工作为LLM的长期审计提供了有力证据，证明其价值，并展示了AI Watchman作为实现这种审计和提供透明度的一个有效系统。

Abstract: Large language models' (LLMs') outputs are shaped by opaque and
frequently-changing company content moderation policies and practices. LLM
moderation often takes the form of refusal; models' refusal to produce text
about certain topics both reflects company policy and subtly shapes public
discourse. We introduce AI Watchman, a longitudinal auditing system to publicly
measure and track LLM refusals over time, to provide transparency into an
important and black-box aspect of LLMs. Using a dataset of over 400 social
issues, we audit Open AI's moderation endpoint, GPT-4.1, and GPT-5, and
DeepSeek (both in English and Chinese). We find evidence that changes in
company policies, even those not publicly announced, can be detected by AI
Watchman, and identify company- and model-specific differences in content
moderation. We also qualitatively analyze and categorize different forms of
refusal. This work contributes evidence for the value of longitudinal auditing
of LLMs, and AI Watchman, one system for doing so.

</details>


### [33] [RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph Question Answering with LLMs](https://arxiv.org/abs/2510.01257)
*Can Lin,Zhengwang Jiang,Ling Zheng,Qi Zhao,Yuhang Zhang,Qi Song,Wangqiu Zhou*

Main category: cs.CL

TL;DR: 提出RJE框架，通过检索、判断和探索机制，解决了现有LLM-based KGQA的局限性，使得小型LLM也能达到有竞争力的性能，并显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的知识图谱问答(KGQA)方法存在局限性：检索式方法受限于检索质量，而Agent-based方法高度依赖于专有大型语言模型(LLMs)。

Method: 提出Retrieval-Judgment-Exploration (RJE)框架，该框架通过检索精炼的推理路径、评估其充分性并有条件地探索额外证据。此外，RJE引入了专门的辅助模块，如推理路径排序、问题分解和检索器辅助探索，以使小型LLMs也能有效工作。

Result: 实验表明，RJE与专有LLMs（如GPT-4o-mini）结合使用时，性能优于现有基线。同时，它使得小型开源LLMs（如3B和8B参数模型）在无需微调的情况下也能取得有竞争力的结果。与Agent-based方法相比，RJE显著减少了LLM调用次数和Token使用量，大幅提高了效率。

Conclusion: RJE框架有效解决了LLM-based KGQA的现有局限，不仅提升了性能，还使得小型LLM能够高效参与，同时显著提高了整体效率。

Abstract: Knowledge graph question answering (KGQA) aims to answer natural language
questions using knowledge graphs. Recent research leverages large language
models (LLMs) to enhance KGQA reasoning, but faces limitations: retrieval-based
methods are constrained by the quality of retrieved information, while
agent-based methods rely heavily on proprietary LLMs. To address these
limitations, we propose Retrieval-Judgment-Exploration (RJE), a framework that
retrieves refined reasoning paths, evaluates their sufficiency, and
conditionally explores additional evidence. Moreover, RJE introduces
specialized auxiliary modules enabling small-sized LLMs to perform effectively:
Reasoning Path Ranking, Question Decomposition, and Retriever-assisted
Exploration. Experiments show that our approach with proprietary LLMs (such as
GPT-4o-mini) outperforms existing baselines while enabling small open-source
LLMs (such as 3B and 8B parameters) to achieve competitive results without
fine-tuning LLMs. Additionally, RJE substantially reduces the number of LLM
calls and token usage compared to agent-based methods, yielding significant
efficiency improvements.

</details>


### [34] [Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse](https://arxiv.org/abs/2510.01258)
*Nathan Junzi Chen*

Main category: cs.CL

TL;DR: 本研究评估了主流大型语言模型（LLMs）的政治偏见，发现其普遍存在自由-威权主义倾向，并分析其对公共话语可能造成的扭曲。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（GAI）已主导政治话语，但其训练数据偏差、人为偏见和算法缺陷导致了固有的政治偏见，亟待评估。

Method: 采用零样本分类方法，结合意识形态一致性、主题性、回应情感和客观性指标，评估算法的政治偏袒性。具体分析了来自六个主流大型语言模型（LLMs）的1800个响应，通过四个独立的微调分类算法计算偏见指标。

Result: 所有评估的六个LLM均表现出增强的自由-威权主义倾向，并伴有推理替代和预设拒绝等显著情况。

Conclusion: 研究揭示了内在偏见对人机交互和公共话语渗透的心理影响，这种偏见可能根据区域的社会政治结构，导致政治格局的顺从或两极分化。

Abstract: Amidst the rapid normalization of generative artificial intelligence (GAI),
intelligent systems have come to dominate political discourse across
information mediums. However, internalized political biases stemming from
training data skews, human prejudice, and algorithmic flaws continue to plague
the novel technology. This paper employs a zero-shot classification approach to
evaluate algorithmic political partisanship through a methodical combination of
ideological alignment, topicality, response sentiment, and objectivity. A total
of 1800 model responses across six mainstream large language models (LLMs) were
individually input into four distinct fine-tuned classification algorithms,
each responsible for computing an aforementioned bias evaluation metric.
Results show an amplified liberal-authoritarian alignment across all six LLMs
evaluated, with notable instances of reasoning supersessions and canned
refusals. The study subsequently highlights the psychological influences
underpinning human-computer interactions and how intrinsic biases can permeate
public discourse. The resulting distortion of the political landscape can
ultimately manifest as conformity or polarization, depending on a region's
pre-existing socio-political structures.

</details>


### [35] [In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b](https://arxiv.org/abs/2510.01259)
*Nils Durner*

Main category: cs.CL

TL;DR: 本研究探测OpenAI的gpt-oss-20b模型，发现社会语用框架、语言选择和指令层级显著影响其拒绝有害指令的行为。复合提示、特定语言和角色扮演可绕过安全机制，但也提出了AI辅助的加固方法。此外，指出了审查API的不足和推理堆栈间的再现性问题。


<details>
  <summary>Details</summary>
Motivation: 研究社会语用框架、语言选择和指令层级如何影响OpenAI开源模型gpt-oss-20b的拒绝行为。

Method: 对gpt-oss-20b模型进行探测，在80次迭代中测试了多个有害领域（如ZIP炸弹、卡号生成、不安全驾驶建议）。通过复合提示（如教育者角色、安全借口）、多语言（德语、法语、英语）和角色扮演（如“Linux终端”）进行实验。引入了AI辅助的加固方法，并使用配对跟踪设计评估了模型的评估意识。最后，将OpenAI Moderation API与语义评分器进行了比较，并测量了不同推理堆栈间的拒绝率差异。

Result: 1. 复合提示（如教育者角色、安全借口和步骤提示）将ZIP炸弹任务的协助率从0%提升至97.5%。
2. 德语和法语的正式语域提示通常比英语更易泄露信息。
3. “Linux终端”角色扮演可覆盖开发者不泄露上下文的规则。
4. 引入的AI辅助加固方法将多种用户提示变体的泄露率降至0%。
5. 在评估意识测试中，13%的配对观察到不一致的协助行为。
6. OpenAI Moderation API对“实质性有帮助”的输出捕获不足，低于语义评分器。
7. 不同推理堆栈之间的拒绝率差异在5到10个百分点，引发了对可再现性的担忧。

Conclusion: 社会语用框架、语言选择和指令层级对大型语言模型的拒绝行为有显著影响。gpt-oss-20b存在漏洞，但AI辅助的加固方法可有效降低信息泄露。此外，当前内容审核API存在不足，且不同推理堆栈间的模型行为再现性较差，这对于LLM的安全性、可靠性及部署提出了挑战。

Abstract: We probe OpenAI's open-weights 20-billion-parameter model gpt-oss-20b to
study how sociopragmatic framing, language choice, and instruction hierarchy
affect refusal behavior. Across 80 seeded iterations per scenario, we test
several harm domains including ZIP-bomb construction (cyber threat), synthetic
card-number generation, minor-unsafe driving advice, drug-precursor indicators,
and RAG context exfiltration. Composite prompts that combine an educator
persona, a safety-pretext ("what to avoid"), and step-cue phrasing flip
assistance rates from 0% to 97.5% on a ZIP-bomb task. On our grid, formal
registers in German and French are often leakier than matched English prompts.
A "Linux terminal" role-play overrides a developer rule not to reveal context
in a majority of runs with a naive developer prompt, and we introduce an
AI-assisted hardening method that reduces leakage to 0% in several user-prompt
variants. We further test evaluation awareness with a paired-track design and
measure frame-conditioned differences between matched "helpfulness" and
"harmfulness" evaluation prompts; we observe inconsistent assistance in 13% of
pairs. Finally, we find that the OpenAI Moderation API under-captures
materially helpful outputs relative to a semantic grader, and that refusal
rates differ by 5 to 10 percentage points across inference stacks, raising
reproducibility concerns. We release prompts, seeds, outputs, and code for
reproducible auditing at https://github.com/ndurner/gpt-oss-rt-run .

</details>


### [36] [OpenAI's GPT-OSS-20B Model and Safety Alignment Issues in a Low-Resource Language](https://arxiv.org/abs/2510.01266)
*Isa Inuwa-Dutse*

Main category: cs.CL

TL;DR: 本文揭示了OpenAI GPT-OSS-20b模型在低资源语言（豪萨语）环境下存在的安全漏洞，包括偏见、不准确性及文化不敏感。模型在礼貌提示下安全协议松懈，可生成有害内容，甚至错误地将有毒物质认定为可食用。


<details>
  <summary>Details</summary>
Motivation: 质疑GPT-OSS-20b模型在低资源语言环境中对弱势社群用户的可靠性，并回应近期对该模型的安全探测。

Method: 使用豪萨语进行最小化提示的红队测试，探索模型的行为；观察模型在礼貌或感激语言提示下的安全协议松懈情况（奖励劫持）。此外，针对模型对有毒物质的错误认知，进行了一项61人的用户调查以评估错误严重性。

Result: 模型在豪萨语中表现出偏见、不准确性和文化不敏感。通过少量提示，可诱导模型生成有害、文化不敏感和事实不准确的内容。当使用礼貌语言提示时，模型安全协议似乎放松，导致可能传播错误信息和仇恨言论的输出（语言奖励劫持）。例如，模型错误地认为常见杀虫剂和灭鼠剂对人类安全，而用户调查显示98%的参与者认为它们有毒。其他问题包括无法区分生熟食物以及结合贬低性文化谚语来构建不准确论点。

Conclusion: 这些问题主要源于低资源语言环境下的安全微调不足，凸显了当前红队测试工作在这些领域存在的重大空白，并提出相应建议。

Abstract: In response to the recent safety probing for OpenAI's GPT-OSS-20b model, we
present a summary of a set of vulnerabilities uncovered in the model, focusing
on its performance and safety alignment in a low-resource language setting. The
core motivation for our work is to question the model's reliability for users
from underrepresented communities. Using Hausa, a major African language, we
uncover biases, inaccuracies, and cultural insensitivities in the model's
behaviour. With a minimal prompting, our red-teaming efforts reveal that the
model can be induced to generate harmful, culturally insensitive, and factually
inaccurate content in the language. As a form of reward hacking, we note how
the model's safety protocols appear to relax when prompted with polite or
grateful language, leading to outputs that could facilitate misinformation and
amplify hate speech. For instance, the model operates on the false assumption
that common insecticide locally known as Fiya-Fiya (Cyphermethrin) and
rodenticide like Shinkafar Bera (a form of Aluminium Phosphide) are safe for
human consumption. To contextualise the severity of this error and popularity
of the substances, we conducted a survey (n=61) in which 98% of participants
identified them as toxic. Additional failures include an inability to
distinguish between raw and processed foods and the incorporation of demeaning
cultural proverbs to build inaccurate arguments. We surmise that these issues
manifest through a form of linguistic reward hacking, where the model
prioritises fluent, plausible-sounding output in the target language over
safety and truthfulness. We attribute the uncovered flaws primarily to
insufficient safety tuning in low-resource linguistic contexts. By
concentrating on a low-resource setting, our approach highlights a significant
gap in current red-teaming effort and offer some recommendations.

</details>


### [37] [AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees](https://arxiv.org/abs/2510.01268)
*Hongyi Zhou,Jin Zhu,Pingfan Su,Kai Ye,Ying Yang,Shakeel A O B Gavioli-Akilagun,Chengchun Shi*

Main category: cs.CL

TL;DR: 提出AdaDetectGPT，一个自适应学习见证函数的分类器，显著提升了基于logits的检测器区分人类和LLM生成文本的性能，最高提升达58%。


<details>
  <summary>Details</summary>
Motivation: 现有基于logits的文本检测器仅依赖对数概率，存在性能次优问题，亟需改进区分人类和LLM生成文本的方法。

Method: 引入AdaDetectGPT，一个自适应学习见证函数的新型分类器，旨在从训练数据中学习并增强基于logits的文本检测器。提供统计学保证。

Result: 数值研究表明，AdaDetectGPT在多种数据集和LLM组合下，显著优于现有最佳方法，性能提升最高达58%。

Conclusion: AdaDetectGPT是一种有效且优越的LLM生成文本检测方法，通过自适应学习显著提升了检测性能，克服了现有方法的局限。

Abstract: We study the problem of determining whether a piece of text has been authored
by a human or by a large language model (LLM). Existing state of the art
logits-based detectors make use of statistics derived from the log-probability
of the observed text evaluated using the distribution function of a given
source LLM. However, relying solely on log probabilities can be sub-optimal. In
response, we introduce AdaDetectGPT -- a novel classifier that adaptively
learns a witness function from training data to enhance the performance of
logits-based detectors. We provide statistical guarantees on its true positive
rate, false positive rate, true negative rate and false negative rate.
Extensive numerical studies show AdaDetectGPT nearly uniformly improves the
state-of-the-art method in various combination of datasets and LLMs, and the
improvement can reach up to 58%. A python implementation of our method is
available at https://github.com/Mamba413/AdaDetectGPT.

</details>


### [38] [Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection](https://arxiv.org/abs/2510.01270)
*Hoang Phan,Victor Li,Qi Lei*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) have revolutionized natural language processing
with their ability to generate coherent and contextually relevant text.
However, their deployment raises significant concerns about the potential for
generating harmful or inappropriate content. In this paper, we introduce
Progressive Self-Reflection (PSR), a novel inference-time technique that
empowers LLMs to self-monitor and correct their outputs dynamically.
Experimental results demonstrate that applying our proposed method to
Llama-3.1-8B-Instruct reduces the attack success rate from 77.5\% to 5.9\%, to
Llama-3.1-8B base from 89.7\% to 5.6\%, and to Qwen2.5-7B-Instruct from 44.4\%
to 3.8\%, without additional training, while maintaining their original
performance on benign tasks. Our approach acts as a test-time scaling method,
where additional self-reflection rounds enhance safety at the cost of inference
overhead. To balance safety with computational efficiency, we introduce a
lightweight self-reflection predictor that estimates the optimal number of
reflection rounds based on input complexity. This adaptive mechanism prevents
unnecessary self-assessment on benign inputs while ensuring thorough evaluation
when encountering potentially harmful content. Our findings suggest that
Progressive Self-Reflection serves as a scalable test-time approach, enhancing
LLM safety by dynamically allocating computational resources in proportion to
the input's risk profile.

</details>


### [39] [TraceDet: Hallucination Detection from the Decoding Trace of Diffusion Large Language Models](https://arxiv.org/abs/2510.01274)
*Shenxu Chang,Junchi Yu,Weixing Wang,Yongqiang Chen,Jialin Yu,Philip Torr,Jindong Gu*

Main category: cs.CL

TL;DR: 本文提出TraceDet框架，利用扩散大语言模型(D-LLMs)的多步去噪过程中的中间步骤来检测幻觉问题，实验证明其检测效果显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: D-LLMs作为AR-LLMs的替代方案潜力巨大，但其幻觉问题未被充分研究，限制了实际应用。现有幻觉检测方法专为AR-LLMs设计，无法有效利用D-LLMs多步去噪过程中的幻觉信号。

Method: 提出TraceDet框架，明确利用D-LLMs的中间去噪步骤进行幻觉检测。TraceDet将去噪过程建模为行动轨迹，每个行动是模型基于前一中间输出对清理响应的预测。通过识别对幻觉响应信息量最大的子轨迹，TraceDet捕捉多步去噪过程中的关键幻觉信号。

Result: 在多种开源D-LLMs上的广泛实验表明，TraceDet持续改进了幻觉检测效果，与基线方法相比，AUROC平均提升了15.2%。

Conclusion: TraceDet通过有效利用D-LLMs多步去噪过程中的中间信息，显著提升了幻觉检测性能，为提高D-LLMs的可靠性提供了有效方案。

Abstract: Diffusion large language models (D-LLMs) have recently emerged as a promising
alternative to auto-regressive LLMs (AR-LLMs). However, the hallucination
problem in D-LLMs remains underexplored, limiting their reliability in
real-world applications. Existing hallucination detection methods are designed
for AR-LLMs and rely on signals from single-step generation, making them
ill-suited for D-LLMs where hallucination signals often emerge throughout the
multi-step denoising process. To bridge this gap, we propose TraceDet, a novel
framework that explicitly leverages the intermediate denoising steps of D-LLMs
for hallucination detection. TraceDet models the denoising process as an action
trace, with each action defined as the model's prediction over the cleaned
response, conditioned on the previous intermediate output. By identifying the
sub-trace that is maximally informative to the hallucinated responses, TraceDet
leverages the key hallucination signals in the multi-step denoising process of
D-LLMs for hallucination detection. Extensive experiments on various open
source D-LLMs demonstrate that TraceDet consistently improves hallucination
detection, achieving an average gain in AUROC of 15.2% compared to baselines.

</details>


### [40] [LLM Based Sentiment Classification From Bangladesh E-Commerce Reviews](https://arxiv.org/abs/2510.01276)
*Sumaiya Tabassum*

Main category: cs.CL

TL;DR: 本文研究了LLM（特别是Llama-3.1-8B）在孟加拉国电商评论情感分析中的应用，通过LoRA/PEFT微调，Llama-3.1-8B取得了95.5%的准确率，证明了其高效性和适用性。


<details>
  <summary>Details</summary>
Motivation: 情感分析对于理解消费者情绪至关重要，但书面语言的复杂性和多样性阻碍了准确性。本文旨在探索基于Transformer的BERT模型及其他大型语言模型（LLMs）在孟加拉国电商评论情感分析中的可行性。

Method: 研究调查了包括BERT模型和Llama-3.1-8B在内的多种LLM。使用从孟加拉语和英语客户评论原始数据集中提取的4000个样本进行模型微调，并采用了参数高效微调方法（LoRA和PEFT）。

Result: 经过微调的Llama-3.1-8B模型在准确率、精确率、召回率和F1分数上均优于其他微调模型（如Phi-3.5-mini-instruct, Mistral-7B-v0.1等），分别达到了95.5%、93%、88%和90%。研究还强调LoRA和PEFT等参数高效微调方法可降低计算开销，适用于资源有限环境。

Conclusion: LLMs能有效应用于情感分析，特别是Llama-3.1-8B在孟加拉国电商评论中表现出色。参数高效微调方法（LoRA和PEFT）使其在计算资源受限的场景下也具有可行性。

Abstract: Sentiment analysis is an essential part of text analysis, which is a larger
field that includes determining and evaluating the author's emotional state.
This method is essential since it makes it easier to comprehend consumers'
feelings, viewpoints, and preferences holistically. The introduction of large
language models (LLMs), such as Llama, has greatly increased the availability
of cutting-edge model applications, such as sentiment analysis. However,
accurate sentiment analysis is hampered by the intricacy of written language
and the diversity of languages used in evaluations. The viability of using
transformer-based BERT models and other LLMs for sentiment analysis from
Bangladesh e commerce reviews is investigated in this paper. A subset of 4000
samples from the original dataset of Bangla and English customer reviews was
utilized to fine-tune the model. The fine tuned Llama-3.1-8B model outperformed
other fine-tuned models, including Phi-3.5-mini-instruct, Mistral-7B-v0.1,
DistilBERT-multilingual, mBERT, and XLM-R-base, with an overall accuracy,
precision, recall, and F1 score of 95.5%, 93%, 88%, 90%. The study emphasizes
how parameter efficient fine-tuning methods (LoRA and PEFT) can lower
computational overhead and make it appropriate for contexts with limited
resources. The results show how LLMs can

</details>


### [41] [TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture](https://arxiv.org/abs/2510.01279)
*Yongchao Chen,Jiefeng Chen,Rui Meng,Ji Yin,Na Li,Chuchu Fan,Chi Wang,Tomas Pfister,Jinsung Yoon*

Main category: cs.CL

TL;DR: 本文提出TUMIX，一个集成框架，通过并行运行具有不同工具使用策略的LLM代理来提高模型推理能力，在保持相似推理成本的同时显著提升准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM整合工具（如Code Interpreter和Search）显著增强了推理能力，但目前缺乏关于最佳工具使用策略的实用指导，核心挑战在于如何有效结合文本推理、编码和搜索来解决多样化问题。

Method: 本文提出Tool-Use Mixture (TUMIX)，一个集成框架，并行运行多个代理，每个代理采用独特的工具使用策略和答案路径。TUMIX中的代理基于问题和先前答案迭代地共享和改进响应。

Result: TUMIX在主要推理基准上，相对于最先进的工具增强和测试时间扩展方法，取得了显著的性能提升，在Gemini-2.5-Pro和Gemini-2.5-Flash上比最佳基线平均准确率提高高达3.55%，且推理成本接近。研究发现代理的多样性和质量至关重要，可通过LLM自动优化代理设计来增强。此外，TUMIX可在达到足够置信度时停止优化，仅用49%的推理成本即可保持性能。

Conclusion: TUMIX通过集成多样化的工具使用代理有效提升了LLM的推理能力，证明了代理多样性和质量的关键性，并提供了在性能和成本之间权衡的策略。

Abstract: While integrating tools like Code Interpreter and Search has significantly
enhanced Large Language Model (LLM) reasoning in models like ChatGPT Agent and
Gemini-Pro, practical guidance on optimal tool use is lacking. The core
challenge is effectively combining textual reasoning, coding, and search for
diverse questions. In this paper, we propose Tool-Use Mixture (TUMIX), an
ensemble framework that runs multiple agents in parallel, each employing
distinct tool-use strategies and answer paths. Agents in TUMIX iteratively
share and refine responses based on the question and previous answers. In
experiments, TUMIX achieves significant gains over state-of-the-art
tool-augmented and test-time scaling methods, delivering an average accuracy
improvement of up to 3.55% over the best baseline on Gemini-2.5-Pro and
Gemini-2.5-Flash across key reasoning benchmarks, with near-equal inference
costs. We find that agent diversity and quality are crucial and can be enhanced
by using LLMs to auto-optimize agent designs. Furthermore, TUMIX can halt
refinement upon reaching sufficient confidence, preserving performance at only
49% of the inference cost. Further scaling can achieve higher performance,
albeit at a greater cost.

</details>


### [42] [Evaluation Sheet for Deep Research: A Use Case for Academic Survey Writing](https://arxiv.org/abs/2510.01283)
*Israel Abebe Azime,Tadesse Destaw Belay,Atnafu Lambebo Tonja*

Main category: cs.CL

TL;DR: 本文提出一个评估深度研究LLM工具的评估表，并以学术综述写作为例进行评估，发现需要更精细的评估标准，且现有工具在代表目标领域方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）驱动的深度研究工具能执行知识密集型任务，但缺乏有效的评估标准来衡量其能力，尤其是在复杂报告生成等方面的表现。

Method: 1. 引入了一个用于评估深度研究工具能力的评估表。2. 选择学术综述写作作为用例任务。3. 使用该评估表评估了OpenAI和Google的深度研究工具生成的报告。

Result: 1. 结果表明需要精心设计的评估标准。2. 现有深度研究工具与搜索引擎在学术综述生成方面存在巨大差距。3. 这些工具在准确代表目标领域方面存在不足。

Conclusion: 深度研究工具在知识密集型任务中仍有明显局限性，尤其是在领域代表性方面。因此，迫切需要制定更全面、更精确的评估标准以指导其发展。

Abstract: Large Language Models (LLMs) powered with argentic capabilities are able to
do knowledge-intensive tasks without human involvement. A prime example of this
tool is Deep research with the capability to browse the web, extract
information and generate multi-page reports. In this work, we introduce an
evaluation sheet that can be used for assessing the capability of Deep Research
tools. In addition, we selected academic survey writing as a use case task and
evaluated output reports based on the evaluation sheet we introduced. Our
findings show the need to have carefully crafted evaluation standards. The
evaluation done on OpenAI`s Deep Search and Google's Deep Search in generating
an academic survey showed the huge gap between search engines and standalone
Deep Research tools, the shortcoming in representing the targeted area.

</details>


### [43] [HiSpec: Hierarchical Speculative Decoding for LLMs](https://arxiv.org/abs/2510.01336)
*Avinash Kumar,Sujay Sanghavi,Poulami Das*

Main category: cs.CL

TL;DR: HiSpec利用早退模型进行低开销中间验证，显著提升LLM投机解码的吞吐量，且不影响精度。


<details>
  <summary>Details</summary>
Motivation: 投机解码中，大模型对小模型生成的令牌进行验证是性能瓶颈，且现有方法多关注草稿生成。尽管“中间验证”可减少验证时间，但现有方法存在高训练开销、内存占用增加以及依靠近似启发式算法损害精度等问题。

Method: 提出分层投机解码（HiSpec）框架，利用早退（EE）模型进行低开销中间验证。EE模型允许令牌提前退出并专门训练其隐藏状态可解释，从而减少计算和内存开销。HiSpec还设计了重用KV缓存和隐藏状态的方法，并在保证精度的前提下，定期使用目标模型验证中间验证器接受的草稿令牌。

Result: 在多种基准和模型上的评估显示，与基线单层投机解码相比，HiSpec平均吞吐量提高1.28倍，最高达2.01倍，且未损害精度。

Conclusion: HiSpec通过利用早退模型进行低开销的中间验证，成功解决了投机解码中的验证瓶颈问题，显著提升了大语言模型推理的吞吐量，同时保持了高精度。

Abstract: Speculative decoding accelerates LLM inference by using a smaller draft model
to speculate tokens that a larger target model verifies. Verification is often
the bottleneck (e.g. verification is $4\times$ slower than token generation
when a 3B model speculates for a 70B target model), but most prior works focus
only on accelerating drafting. $\textit{``Intermediate"}$ verification reduces
verification time by discarding inaccurate draft tokens early, but existing
methods incur substantial training overheads in incorporating the intermediate
verifier, increase the memory footprint to orchestrate the intermediate
verification step, and compromise accuracy by relying on approximate
heuristics.
  We propose $\underline{\textit{Hi}}\textit{erarchical
}\underline{\textit{Spec}}\textit{ulative Decoding (HiSpec)}$, a framework for
high-throughput speculative decoding that exploits $\textit{early-exit (EE)
models}$ for low-overhead intermediate verification. EE models allow tokens to
exit early by skipping layer traversal and are explicitly trained so that
hidden states at selected layers can be interpreted, making them uniquely
suited for intermediate verification without drastically increasing compute and
memory overheads. To improve resource-efficiency even further, we design a
methodology that enables HiSpec to re-use key-value caches and hidden states
between the draft, intermediate verifier, and target models. To maintain
accuracy, HiSpec periodically validates the draft tokens accepted by the
intermediate verifier against the target model. Our evaluations using various
representative benchmarks and models show that HiSpec improves throughput by
1.28$\times$ on average and by up to 2.01$\times$ compared to the baseline
single-layer speculation without compromising accuracy.

</details>


### [44] [TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies](https://arxiv.org/abs/2510.01391)
*Maithili Kadam,Francis Ferraro*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在因果/时序事件问答中表现不佳。本文提出了TAG-EQA框架，通过将因果事件图注入LLM提示，显著提升了LLMs在该类任务上的准确性，且无需微调。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）擅长通用语言任务，但在需要因果或时序推理的事件型问题上常常遇到困难。

Method: 引入了TAG-EQA（Text-And-Graph for Event Question Answering）提示框架。该框架将结构化的因果事件图转换为自然语言语句，并注入到LLM输入中。TAG-EQA包含九种提示配置，结合了三种策略（零样本、少样本、思维链）和三种输入模式（纯文本、纯图、文本+图），旨在系统分析结构化知识如何辅助推理。

Result: 在TORQUESTRA基准测试中，TAG-EQA相比纯文本基线平均准确率提高了5%，在零样本设置中提升高达12%，在使用图增强思维链提示时提升高达18%。性能虽因模型和配置而异，但效果显著。

Conclusion: 研究结果表明，因果图可以在不进行模型微调的情况下，有效增强LLMs的事件推理能力，为基于提示的问答提供了一种灵活的结构化知识编码方式。

Abstract: Large language models (LLMs) excel at general language tasks but often
struggle with event-based questions-especially those requiring causal or
temporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question
Answering), a prompting framework that injects causal event graphs into LLM
inputs by converting structured relations into natural-language statements.
TAG-EQA spans nine prompting configurations, combining three strategies
(zero-shot, few-shot, chain-of-thought) with three input modalities (text-only,
graph-only, text+graph), enabling a systematic analysis of when and how
structured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA
improves accuracy by 5% on average over text-only baselines, with gains up to
12% in zero-shot settings and 18% when graph-augmented CoT prompting is
effective. While performance varies by model and configuration, our findings
show that causal graphs can enhance event reasoning in LLMs without
fine-tuning, offering a flexible way to encode structure in prompt-based QA.

</details>


### [45] [A-VERT: Agnostic Verification with Embedding Ranking Targets](https://arxiv.org/abs/2510.01469)
*Nicolás Aguirre,Ramiro Caso,Ramiro Rodríguez Colmeiro,Mauro Santelli,Joaquín Toranzo Calderón*

Main category: cs.CL

TL;DR: 提出一种基于语义嵌入距离的无结构LM响应自动评估方法，成本低，性能与人工标注高度一致（回归分数~0.97，准确率~96%）。


<details>
  <summary>Details</summary>
Motivation: 语言模型（LM）响应的自动评估对于基准测试和生产模型质量评估至关重要。然而，现有方法过于昂贵（如LLM-as-a-Judge）或与实际条件相去甚远（如字符串匹配、logprob）。

Method: 本文提出一种无结构评估方法，利用语义嵌入距离将目标候选与任意LM生成的文本进行匹配，实现鲁棒的响应分类，且计算成本相对较低（使用参数小于10B的嵌入模型）。

Result: 该方法在3个数据集和3种不同的LM架构上进行了测试，结果显示其回归分数约为0.97，准确率约为96%，与人工标注结果高度一致。

Conclusion: 该研究提供了一种有效、低成本且与人类判断高度一致的LM响应自动评估方案，克服了现有方法的局限性。

Abstract: The automatic evaluation of Language Model (LM) responses is a critical piece
in the development of benchmarks and metrics, both for model training and
quality assessment of production model endpoints. The current approaches to
response classification relies on methods that are too expensive (i.e.
LLM-as-a-Judge) or that are far from real-world conditions (string-matching,
logprob). In this paper, a structure-free evaluation method is presented. The
method makes use of semantic embedding distances to match target candidates
with arbitrary LM-generated text, resulting in a robust classification of the
response at a relatively low compute cost (embedding models of less than $10B$
parameters). The results show a regression score of ~0.97 and an accuracy of
~96% against human annotators, tested over 3 data sets and 3 different LM
architectures.

</details>


### [46] [One More Question is Enough, Expert Question Decomposition (EQD) Model for Domain Quantitative Reasoning](https://arxiv.org/abs/2510.01526)
*Mengyu Wang,Sotirios Sabanis,Miguel de Carvalho,Shay B. Cohen,Tiejun Ma*

Main category: cs.CL

TL;DR: 提出专家问题分解（EQD）方法，通过两步微调和奖励函数提升LLMs在领域特定量化推理（尤其金融领域）中的表现，实现高效且超越现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在需要专业知识和复杂问答的领域，其领域特定量化推理能力仍面临重大挑战。

Method: 提出EQD方法，基于两步微调框架，并由衡量生成子问题对问答结果改进效果的奖励函数指导。该方法训练所需数据量少、计算效率高，推理时间与零样本提示相当。

Result: 在金融领域的四个基准数据集上，EQD使不同LLMs的问答性能持续提升0.6%至10.5%，并优于当前最先进的领域微调模型和高级提示策略。

Conclusion: EQD能有效提升LLMs在领域特定量化推理任务中的表现。研究发现，在领域特定问答中，单个支持性问题通常比详细指导步骤提供更大益处。

Abstract: Domain-specific quantitative reasoning remains a major challenge for large
language models (LLMs), especially in fields requiring expert knowledge and
complex question answering (QA). In this work, we propose Expert Question
Decomposition (EQD), an approach designed to balance the use of domain
knowledge with computational efficiency. EQD is built on a two-step fine-tuning
framework and guided by a reward function that measures the effectiveness of
generated sub-questions in improving QA outcomes. It requires only a few
thousand training examples and a single A100 GPU for fine-tuning, with
inference time comparable to zero-shot prompting. Beyond its efficiency, EQD
outperforms state-of-the-art domain-tuned models and advanced prompting
strategies. We evaluate EQD in the financial domain, characterized by
specialized knowledge and complex quantitative reasoning, across four benchmark
datasets. Our method consistently improves QA performance by 0.6% to 10.5%
across different LLMs. Our analysis reveals an important insight: in
domain-specific QA, a single supporting question often provides greater benefit
than detailed guidance steps.

</details>


### [47] [CLUE: Non-parametric Verification from Experience via Hidden-State Clustering](https://arxiv.org/abs/2510.01591)
*Zhenwen Liang,Ruosen Li,Yujun Zhou,Linfeng Song,Dian Yu,Xinya Du,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出CLUE，一个基于LLM内部隐藏状态的轻量级、无参数验证器，通过分析隐藏状态轨迹的几何可分离性来判断输出正确性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）的输出质量是一个关键挑战。现有方法（如奖励模型、多数投票或基于标记概率的置信度）要么容易过拟合表层信息，要么在未校准模型上失效。这些信号仅是模型内部隐藏状态这一更丰富信息源的部分投影。

Method: 直接探索LLM的内部隐藏状态作为统一的验证基础。研究发现，解决方案的正确性在隐藏激活轨迹中编码为几何可分离的特征。为此，提出了Clue (Clustering and Experience-based Verification)，一个极简的、非参数验证器。CLUE通过隐藏状态的delta来总结每个推理轨迹，并依据其与从过往经验中形成的“成功”和“失败”聚类中心的最近距离来分类输出的正确性。

Result: CLUE在候选答案重排序任务中，持续优于“LLM作为评判者”基线，并与现代基于置信度的方法持平或超越。它提高了AIME 24/25和GPQA数据集上的top-1和多数投票准确率。例如，在AIME 24数据集上，使用1.5B模型时，CLUE将准确率从56.7%（多数@64）提升至70.0%（top-maj@16）。

Conclusion: LLM的内部隐藏状态是判断输出正确性的强大且统一的信息来源，其正确性以几何可分离的特征形式编码。CLUE这种基于隐藏状态的极简非参数验证器，有效利用了这一信号，显著提升了LLM输出质量评估的准确性。

Abstract: Assessing the quality of Large Language Model (LLM) outputs presents a
critical challenge. Previous methods either rely on text-level information
(e.g., reward models, majority voting), which can overfit to superficial cues,
or on calibrated confidence from token probabilities, which would fail on
less-calibrated models. Yet both of these signals are, in fact, partial
projections of a richer source of information: the model's internal hidden
states. Early layers, closer to token embeddings, preserve semantic and lexical
features that underpin text-based judgments, while later layers increasingly
align with output logits, embedding confidence-related information. This paper
explores hidden states directly as a unified foundation for verification. We
show that the correctness of a solution is encoded as a geometrically separable
signature within the trajectory of hidden activations. To validate this, we
present Clue (Clustering and Experience-based Verification), a deliberately
minimalist, non-parametric verifier. With no trainable parameters, CLUE only
summarizes each reasoning trace by an hidden state delta and classifies
correctness via nearest-centroid distance to ``success'' and ``failure''
clusters formed from past experience. The simplicity of this method highlights
the strength of the underlying signal. Empirically, CLUE consistently
outperforms LLM-as-a-judge baselines and matches or exceeds modern
confidence-based methods in reranking candidates, improving both top-1 and
majority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24
with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%
(top-maj@16).

</details>


### [48] [A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.01600)
*Neal Gregory Lawton,Alfy Samuel,Anoop Kumar,Daben Liu*

Main category: cs.CL

TL;DR: 评估并比较了RAG流水线的多种微调策略，包括独立微调、联合微调和两阶段微调。


<details>
  <summary>Details</summary>
Motivation: RAG是流行的问答框架，其嵌入模型和生成器模型均可微调以提升性能，但存在多种微调策略且成本效益不同，因此需要评估和比较这些策略。

Method: 评估并比较了RAG的几种微调策略，包括独立微调、联合微调和两阶段微调。

Result: 实验发现，所有策略在EM和F1生成质量指标上取得了大致相同的提升，但计算成本显著不同。

Conclusion: 最佳微调策略取决于训练数据集是否包含上下文标签以及是否需要对嵌入和生成器模型的学习率进行网格搜索。

Abstract: A Comparison of Independent and Joint Fine-tuning Strategies for
Retrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel,
Anoop Kumar, Daben Liu Published: 20 Aug 2025, Last Modified: 17 Sept 2025EMNLP
2025 FindingsConference, Publication Chairs, AuthorsRevisionsBibTeXCC BY 4.0
Keywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs),
Fine-tuning, Question Answering, Joint fine-tuning TL;DR: We evaluate and
compare strategies for fine-tuning Retrieval Augmented Generation (RAG)
pipelines, including independent fine-tuning, joint fine-tuning, and two-phase
fine-tuning. Abstract: Retrieval augmented generation (RAG) is a popular
framework for question answering that is powered by two large language models
(LLMs): an embedding model that retrieves context documents from a database
that are relevant to a given question, and a generator model that uses the
retrieved context to generate an answer to the question. Both the embedding and
generator models can be fine-tuned to increase performance of a RAG pipeline on
a new task, but multiple fine-tuning strategies exist with different costs and
benefits. In this paper, we evaluate and compare several RAG fine-tuning
strategies, including independent, joint, and two-phase fine-tuning. In our
experiments, we observe that all of these strategies achieve about equal
improvement in EM and F1 generation quality metrics, although they have
significantly different computational costs. We conclude the optimal
fine-tuning strategy to use depends on whether the training dataset includes
context labels and whether a grid search over the learning rates for the
embedding and generator models is required.

</details>


### [49] [RAG-BioQA Retrieval-Augmented Generation for Long-Form Biomedical Question Answering](https://arxiv.org/abs/2510.01612)
*Lovely Yeswanth Panchumarthi,Sai Prasad Gudari,Atharva Negi,Praveen Raj Budime,Harsit Upadhya*

Main category: cs.CL

TL;DR: RAG-BioQA框架结合检索增强生成和领域微调，实现了基于证据的长篇生物医学问答，并在PubMedQA数据集上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 生物医学文献爆炸式增长，导致难以获取精确信息。现有生物医学问答系统主要提供短答案，缺乏临床决策所需的全面解释。

Method: 提出RAG-BioQA框架，结合检索增强生成与领域微调。集成BioBERT嵌入和FAISS索引，比较BM25、ColBERT、MonoT5等多种重排序策略优化上下文选择，并通过微调的T5模型合成证据以生成长篇答案。

Result: 在PubMedQA数据集上的实验结果表明，RAG-BioQA相比基线模型有显著改进，最佳模型在BLEU、ROUGE和METEOR指标上均取得实质性提升。

Conclusion: RAG-BioQA框架有效提升了可访问、基于证据的生物医学知识检索水平。

Abstract: The exponential growth of biomedical literature creates significant
challenges for accessing precise medical information. Current biomedical
question-answering systems primarily focus on short-form answers, failing to
provide the comprehensive explanations necessary for clinical decision-making.
We present RAG-BioQA, a novel framework combining retrieval-augmented
generation with domain-specific fine-tuning to produce evidence-based,
long-form biomedical answers. Our approach integrates BioBERT embeddings with
FAISS indexing and compares various re-ranking strategies (BM25, ColBERT,
MonoT5) to optimize context selection before synthesizing evidence through a
fine-tuned T5 model. Experimental results on the PubMedQA dataset show
significant improvements over baselines, with our best model achieving
substantial gains across BLEU, ROUGE, and METEOR metrics, advancing the state
of accessible, evidence-based biomedical knowledge retrieval.

</details>


### [50] [Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single Consumer GPU: Continual Pre-training, SFT, and DPO](https://arxiv.org/abs/2510.01616)
*Yu-Cheng Chih,Ming-Tao Duan,Yong-Hao Hou*

Main category: cs.CL

TL;DR: 本文提出PureTC-1B，一个三阶段（CPT、SFT、DPO）的适配器式稳定化流程，用于Llama-3.2-1B-Instruct，显著提升了其对繁体中文的语言一致性，有效减少了非繁体中文输出。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型（SLM）在繁体中文（TC）部署中存在令牌级别不稳定性问题，即模型可能输出非TC字符或进行代码切换，这阻碍了其在成本效益高、设备端和低延迟AI应用中的实际应用，亟需解决这一可靠性差距。

Method: 研究构建了PureTC-1B，一个针对Llama-3.2-1B-Instruct（Meta开源模型）的三阶段稳定化流程，仅使用参数高效的LoRA适配器进行。该方法结合了：1) 基于TC语料库的持续预训练（CPT）；2) 基于指令数据的监督微调（SFT）；3) 基于TC一致性偏好的直接偏好优化（DPO），旨在在不进行全模型重训练的情况下提升单语鲁棒性。

Result: 在模拟实际使用的基准测试中，PureTC-1B相对于基础模型，非TC输出令牌的微平均相对减少了51.3%。在命名实体翻译（NET）任务中，PureTC-1B将错误语言令牌的比例相对于Llama-3B减少了77.2%，相对于Qwen-1.5B减少了57.2%，证明了即使在1B规模下也能实现鲁棒的TC一致性。

Conclusion: PureTC-1B流程显著提升了SLM在繁体中文环境中的语言稳定性。该方法可复现、仅需适配器、且对硬件友好，为开发者提供了增强繁体中文（以及潜在其他非英语语言）语言稳定性的实用方案。

Abstract: Small Language Models (SLMs) enable cost-effective, on-device and
latency-sensitive AI applications, yet their deployment in Traditional Chinese
(TC) remains hindered by token-level instability - models unpredictably emit
non-TC characters or code-switch into other languages. We address this
practical reliability gap by creating PureTC-1B, a three-stage stabilization
pipeline for Llama-3.2-1B-Instruct (an open-weight, instruction-tuned model
released by Meta) using parameter-efficient LoRA adapters. Our method combines
Continual Pre-Training (CPT) on TC-centric corpora, Supervised Fine-Tuning
(SFT) with instruction data, and Direct Preference Optimization (DPO) using
TC-adherence preferences to improve monolingual robustness without full-model
retraining. On a benchmark designed to simulate real-world usage, PureTC-1B
achieves a 51.3% relative reduction (micro-average) in non-TC output tokens
versus the base model. On a Named Entity Translation (NET) task, PureTC-1B
further reduces incorrect-language tokens by 77.2% relative to Llama-3B and
57.2% relative to Qwen-1.5B, indicating that robust TC adherence is attainable
even at the 1B scale. The pipeline is reproducible, adapter-only, and
hardware-friendly, offering practitioners a practical recipe to enhance
language stability for TC and potentially other non-English languages.

</details>


### [51] [AMAS: Adaptively Determining Communication Topology for LLM-based Multi-Agent System](https://arxiv.org/abs/2510.01617)
*Hui Yi Leong,Yuheng Li,Yuqing Wu,Wenwen Ouyang,Wei Zhu,Jiechao Gao*

Main category: cs.CL

TL;DR: AMAS框架通过动态图设计器，为LLM多智能体系统提供上下文感知的结构适应性，自主优化任务专用路径，在多种基准测试中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM能力强大，但在工业问题解决中作为自主多智能体系统（MAS）实际部署时，仍受限于传统MAS架构中僵化、手工设计的图拓扑结构，导致在不同工作负载下效率低下。

Method: 引入AMAS框架，其核心是新颖的动态图设计器。该设计器通过轻量级LLM适配自主识别任务最优图配置，摆脱了对通用结构模板的依赖，而是根据输入固有属性智能引导查询通过任务优化的智能体路径。

Result: AMAS在问答、数学推导和代码生成等基准测试中，系统性地超越了最先进的单智能体和多智能体方法，并适用于多样化的LLM架构。

Conclusion: 上下文敏感的结构适应性是实现高性能LLM多智能体系统部署的根本要求。

Abstract: Although large language models (LLMs) have revolutionized natural language
processing capabilities, their practical implementation as autonomous
multi-agent systems (MAS) for industrial problem-solving encounters persistent
barriers. Conventional MAS architectures are fundamentally restricted by
inflexible, hand-crafted graph topologies that lack contextual responsiveness,
resulting in diminished efficacy across varied academic and commercial
workloads. To surmount these constraints, we introduce AMAS, a
paradigm-shifting framework that redefines LLM-based MAS through a novel
dynamic graph designer. This component autonomously identifies task-specific
optimal graph configurations via lightweight LLM adaptation, eliminating the
reliance on monolithic, universally applied structural templates. Instead, AMAS
exploits the intrinsic properties of individual inputs to intelligently direct
query trajectories through task-optimized agent pathways. Rigorous validation
across question answering, mathematical deduction, and code generation
benchmarks confirms that AMAS systematically exceeds state-of-the-art
single-agent and multi-agent approaches across diverse LLM architectures. Our
investigation establishes that context-sensitive structural adaptability
constitutes a foundational requirement for high-performance LLM MAS
deployments.

</details>


### [52] [NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with BERT](https://arxiv.org/abs/2510.01644)
*John Hawkins,Aditya Pramar,Rodney Beard,Rohitash Chandra*

Main category: cs.CL

TL;DR: 本研究分析了LLMs越狱提示的识别，发现微调BERT模型在当前数据集上表现最佳，并指出提示结构中的显式反思性可能是越狱意图的信号。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）存在被恶意用户通过越狱提示操纵、规避安全防护并生成不期望响应的漏洞。

Method: 分析不同机器学习模型（包括BERT）区分越狱提示和正常使用的能力，并探究识别使用未见策略的越狱提示的能力。通过可视化关键词来区分越狱提示和正常提示。

Result: 在现有数据集上，通过对BERT模型进行端到端微调，可以实现识别越狱提示的最佳性能。

Conclusion: 提示结构中显式的反思性可能是一个指示越狱意图的信号。

Abstract: Large Language Models (LLMs) suffer from a range of vulnerabilities that
allow malicious users to solicit undesirable responses through manipulation of
the input text. These so-called jailbreak prompts are designed to trick the LLM
into circumventing the safety guardrails put in place to keep responses
acceptable to the developer's policies. In this study, we analyse the ability
of different machine learning models to distinguish jailbreak prompts from
genuine uses, including looking at our ability to identify jailbreaks that use
previously unseen strategies. Our results indicate that using current datasets
the best performance is achieved by fine tuning a Bidirectional Encoder
Representations from Transformers (BERT) model end-to-end for identifying
jailbreaks. We visualise the keywords that distinguish jailbreak from genuine
prompts and conclude that explicit reflexivity in prompt structure could be a
signal of jailbreak intention.

</details>


### [53] [Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings in LLMs with Enabled Bidirectional Attention](https://arxiv.org/abs/2510.01652)
*Zhaoxin Feng,Jianfei Ma,Emmanuele Chersoni,Xiaojing Zhao,Xiaoyi Bao*

Main category: cs.CL

TL;DR: 本文探讨了如何通过在自回归LLM中引入双向注意力，克服其单向注意力机制在文本嵌入和语义表示分析任务中的限制。


<details>
  <summary>Details</summary>
Motivation: 自回归大型语言模型（LLMs）在语言理解和生成方面表现优异，但由于单向注意力机制的限制，它们在文本嵌入任务及其语义表示分析中的应用和进展相对缓慢。

Method: 本文通过对Llama架构的不同变体进行额外的训练步骤，逐步启用双向注意力机制，并结合无监督/有监督对比学习来进行测试。

Result: 结果未在提供的摘要中提及。

Conclusion: 结论未在提供的摘要中提及。

Abstract: Autoregressive Large Language Models (LLMs) demonstrate exceptional
performance in language understanding and generation. However, their
application in text embedding tasks has been relatively slow, along with the
analysis of their semantic representation in probing tasks, due to the
constraints of the unidirectional attention mechanism.
  This paper aims to explore whether such constraints can be overcome by
enabling bidirectional attention in LLMs. We tested different variants of the
Llama architecture through additional training steps, progressively enabling
bidirectional attention and unsupervised/supervised contrastive learning.

</details>


### [54] [SoK: Measuring What Matters for Closed-Loop Security Agents](https://arxiv.org/abs/2510.01654)
*Mudita Khurana,Raunak Jain*

Main category: cs.CL

TL;DR: 该研究引入CLASP框架和CLC分数，旨在为闭环自主安全代理提供统一的定义、评估方法和性能衡量标准，以解决网络安全领域现有工具碎片化和基准缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 当前网络安全面临AI驱动的攻击，传统防御系统适应缓慢且研究与工具碎片化，导致防御盲点。尽管闭环自主安全代理前景广阔，但该领域缺乏定义其代理能力、评估其性能和衡量其实践表现的框架、方法和基准。

Method: 研究引入了CLASP（Closed-Loop Autonomous Security Performance）框架，它将安全生命周期（侦察、漏洞利用、根因分析、补丁合成、验证）与核心代理能力（规划、工具使用、记忆、推理、反思与感知）对齐，提供评估安全任务中代理能力的通用词汇和标准。通过将CLASP应用于21项代表性工作，研究映射了现有系统的优势和能力差距。此外，还定义了闭环能力（CLC）分数，这是一个量化闭环程度和操作有效性的综合指标，并概述了闭环基准的要求。

Result: CLASP框架成功地揭示了现有安全系统在哪些方面表现出色以及能力差距依然存在。CLC分数能够量化安全代理的闭环程度和操作有效性。CLASP和CLC分数共同提供了评估和推进闭环安全代理所需的基础性词汇、诊断工具和衡量方法。

Conclusion: CLASP框架和CLC分数共同为评估、诊断和衡量闭环安全代理的功能级性能，提供了必要的词汇、诊断方法和度量标准，有助于推动该领域的进步。

Abstract: Cybersecurity is a relentless arms race, with AI driven offensive systems
evolving faster than traditional defenses can adapt. Research and tooling
remain fragmented across isolated defensive functions, creating blind spots
that adversaries exploit. Autonomous agents capable of integrating, exploit
confirmation, remediation, and validation into a single closed loop offer
promise, but the field lacks three essentials: a framework defining the agentic
capabilities of security systems across security life cycle, a principled
method for evaluating closed loop agents, and a benchmark for measuring their
performance in practice. We introduce CLASP: the Closed-Loop Autonomous
Security Performance framework which aligns the security lifecycle
(reconnaissance, exploitation, root cause analysis, patch synthesis,
validation) with core agentic capabilities (planning, tool use, memory,
reasoning, reflection & perception) providing a common vocabulary and rubric
for assessing agentic capabilities in security tasks. By applying CLASP to 21
representative works, we map where systems demonstrate strengths, and where
capability gaps persist. We then define the Closed-Loop Capability (CLC) Score,
a composite metric quantifying both degree of loop closure and operational
effectiveness, and outline the requirements for a closed loop benchmark.
Together, CLASP and the CLC Score, provide the vocabulary, diagnostics, and
measurements needed to advance both function level performance and measure
closed loop security agents.

</details>


### [55] [MDSEval: A Meta-Evaluation Benchmark for Multimodal Dialogue Summarization](https://arxiv.org/abs/2510.01659)
*Yinhong Liu,Jianfeng He,Hang Su,Ruixue Lian,Yi Nian,Jake Vincent,Srikanth Vishnubhotla,Robinson Piramuthu,Saab Mansour*

Main category: cs.CL

TL;DR: 本文介绍了MDSEval，首个用于多模态对话摘要（MDS）的元评估基准，并揭示了现有自动评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 多模态对话摘要（MDS）需要鲁棒的自动评估方法以降低成本和人力，但这些方法依赖于强有力的、基于人类标注的元评估基准，而目前这类基准是缺失的。

Method: 引入了MDSEval，这是一个包含图像共享对话、对应摘要以及八个质量维度上的人工判断的MDS元评估基准。为确保数据质量和丰富性，提出了一种利用跨模态互斥关键信息（MEKI）的新型过滤框架。

Result: 通过基准测试，发现现有的最先进模态评估方法在区分高级多模态大型语言模型（MLLMs）生成的摘要方面存在局限性，并且容易受到各种偏差的影响。

Conclusion: MDSEval是首个MDS元评估基准，首次识别并规范了MDS特有的关键评估维度，并揭示了现有自动评估方法的不足。

Abstract: Multimodal Dialogue Summarization (MDS) is a critical task with wide-ranging
applications. To support the development of effective MDS models, robust
automatic evaluation methods are essential for reducing both cost and human
effort. However, such methods require a strong meta-evaluation benchmark
grounded in human annotations. In this work, we introduce MDSEval, the first
meta-evaluation benchmark for MDS, consisting image-sharing dialogues,
corresponding summaries, and human judgments across eight well-defined quality
aspects. To ensure data quality and richfulness, we propose a novel filtering
framework leveraging Mutually Exclusive Key Information (MEKI) across
modalities. Our work is the first to identify and formalize key evaluation
dimensions specific to MDS. We benchmark state-of-the-art modal evaluation
methods, revealing their limitations in distinguishing summaries from advanced
MLLMs and their susceptibility to various bias.

</details>


### [56] [FOR-Prompting: From Objection to Revision via an Asymmetric Prompting Protocol](https://arxiv.org/abs/2510.01674)
*He Zhang,Anzhou Zhang,Jian Dai*

Main category: cs.CL

TL;DR: 提出FOR-Prompting，一种通过角色结构化对话引导模型进行异议驱动自我修正的提示协议，在推理任务上显著提升性能，尤其对小型模型有效。


<details>
  <summary>Details</summary>
Motivation: 现有推理协议（如CoT和ToT）虽能组织内部思考，但缺乏外部质疑以促使模型自我修正的明确机制。

Method: 引入FOR-Prompting（从异议到修正提示），一个非对称协议。协议中包含：防守方（提出答案）、异议方（提出质疑式异议，不直接修正）和主持人（确保一致性和结束）。该协议模型无关，纯粹通过角色结构化轮次在提示层面操作。

Result: 在GSM8K任务上，相比单次提示准确率提升约22个百分点，与CoT准确率持平，并获得GPT 4.1评委超过10%的推理和连贯性评分提升。能在无工具或人工监督下修正复杂查询错误，并显著提升小型模型性能（Llama3.2:1b在GSM8K上准确率提升约19%）。在开放式任务中，促进了探索和细化，使假设和权衡更为明确。

Conclusion: FOR-Prompting通过异议引导的自我修正机制有效增强了大型语言模型的推理能力，对小型模型尤其有益。其模型无关和纯提示层面的特性使其具有广泛适用性，并支持异议引导推理的深入研究。

Abstract: Reasoning protocols such as Chain of Thought (CoT) and Tree of Thought (ToT)
organize internal deliberation but lack an explicit mechanism for external
questioning that elicits self-revision. We present FOR-Prompting (From
Objection to Revision Prompting), an asymmetric protocol where a Defender
proposes an answer, an Objectioner raises question-style objections with no
direct fixes, and a Host enforces consistency and closure. On GSM8K we observe
about a 22% point gain over single-prompt and accuracy on par with CoT, with
more than 10% higher ratings in reasoning and coherence from a uniform GPT 4.1
judge. FOR-Prompting also corrects mistakes without tools or human supervision
on tricky queries, and improves performance for small-scale model (approx. 19%
accuracy improved on Llama3.2:1b for GSM8K task), highlighting promise for
small models and on personal device use. Beyond factual QA, qualitative
analyses on open-ended tasks show enhanced exploration and refinement, with
dialogue traces that make assumptions and trade-offs explicit. The protocol is
model agnostic and operates purely at the prompt level through role-structured
turns, so it works with hosted and local models of different sizes without
retraining, and it supports large-scale study of objection-guided reasoning.

</details>


### [57] [How Do Language Models Compose Functions?](https://arxiv.org/abs/2510.01685)
*Apoorv Khandelwal,Ellie Pavlick*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）解决组合任务时，可能采用组合机制，也可能采用直接机制，这取决于嵌入空间的线性映射特性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何解决组合任务（如两跳事实召回），以确定它们是否真正使用了组合机制，而非仅表现出组合能力。

Method: 调查了前馈LLMs在$g(f(x))$形式的两跳事实召回任务上的表现。利用“logit lens”技术分析模型的残差流激活，以识别内部处理机制。

Result: 1. 确认了LLMs存在“组合性鸿沟”。2. 识别出两种处理机制：一种是“组合式”（按步骤计算$f(x)$再计算$g(f(x))$），另一种是“直接式”（无明显中间变量$f(x)$）。3. 发现机制的选择与嵌入空间几何相关，当输入$x$到输出$g(f(x))$存在线性映射时，“直接式”机制占主导。

Conclusion: LLMs以两种截然不同的方式（组合式或直接式）解决组合任务，其选择取决于嵌入空间中是否存在从输入到最终输出的线性映射。这为理解LLMs内部计算过程提供了洞察。

Abstract: While large language models (LLMs) appear to be increasingly capable of
solving compositional tasks, it is an open question whether they do so using
compositional mechanisms. In this work, we investigate how feedforward LLMs
solve two-hop factual recall tasks, which can be expressed compositionally as
$g(f(x))$. We first confirm that modern LLMs continue to suffer from the
"compositionality gap": i.e. their ability to compute both $z = f(x)$ and $y =
g(z)$ does not entail their ability to compute the composition $y = g(f(x))$.
Then, using logit lens on their residual stream activations, we identify two
processing mechanisms, one which solves tasks $\textit{compositionally}$,
computing $f(x)$ along the way to computing $g(f(x))$, and one which solves
them $\textit{directly}$, without any detectable signature of the intermediate
variable $f(x)$. Finally, we find that which mechanism is employed appears to
be related to the embedding space geometry, with the idiomatic mechanism being
dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in
the embedding spaces. We fully release our data and code at:
https://github.com/apoorvkh/composing-functions .

</details>


### [58] [Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation](https://arxiv.org/abs/2510.01688)
*Seungseop Lim,Gibaeg Kim,Wooseok Han,Jean Seo,Hyunkyung Lee,Jaehyo Yoo,Eunho Yang*

Main category: cs.CL

TL;DR: 针对医疗预问诊中LLM因SFT数据集对话轮次分布不均导致的“格式惯性”问题（模型生成重复、无信息量的问题），本文提出一种数据中心方法，通过重新平衡训练数据集的轮次分布来有效缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在医疗预问诊等多轮对话领域取得显著进展，但SFT数据集普遍存在对话轮次分布不均的现象。这种数据偏差导致了一种新型故障机制——“格式惯性”，即模型在长对话中倾向于生成重复、格式正确但缺乏诊断信息的问题，因此亟需解决此问题。

Method: 采用一种简单的数据中心方法，通过重新平衡训练数据集的对话轮次分布，来减轻观察到的“格式惯性”故障机制。

Result: 实验结果表明，所提出的方法显著缓解了医疗预问诊中的“格式惯性”问题。

Conclusion: 重新平衡训练数据集的对话轮次分布是一种有效的数据中心方法，能够实质性缓解LLM在医疗预问诊任务中因数据偏差引起的“格式惯性”问题。

Abstract: Recent advances in Large Language Models (LLMs) have brought significant
improvements to various service domains, including chatbots and medical
pre-consultation applications. In the healthcare domain, the most common
approach for adapting LLMs to multi-turn dialogue generation is Supervised
Fine-Tuning (SFT). However, datasets for SFT in tasks like medical
pre-consultation typically exhibit a skewed turn-count distribution. Training
on such data induces a novel failure mechanism we term **Format Inertia**,
where models tend to generate repetitive, format-correct, but diagnostically
uninformative questions in long medical dialogues. To mitigate this observed
failure mechanism, we adopt a simple, data-centric method that rebalances the
turn-count distribution of the training dataset. Experimental results show that
our approach substantially alleviates Format Inertia in medical
pre-consultation.

</details>


### [59] [What MLLMs Learn about When they Learn about Multimodal Reasoning: Perception, Reasoning, or their Integration?](https://arxiv.org/abs/2510.01719)
*Jiwan Chung,Neel Joshi,Pratyusha Sharma,Youngjae Yu,Vibhav Vineet*

Main category: cs.CL

TL;DR: 引入MathLens基准，用于细致评估多模态推理模型在几何问题中的感知、推理和整合子技能，发现不同训练方法对这些技能有不同影响。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（总准确率）不足以衡量多模态推理模型在奥赛级几何等复杂领域中的进步方式和具体改进点。

Method: 引入MathLens基准，旨在解耦多模态推理子技能，同时保留教材式几何问题的复杂性。它将性能分解为：感知（从原始输入提取信息）、推理（操作可用信息）和整合（选择相关感知证据并应用于推理）。为每个测试提供视觉图表、文本描述、多模态控制问题和精细感知探针等注释，均来源于问题的符号规范以确保一致性。

Result: 1. 强化学习（RL）主要增强感知能力，尤其在文本监督支持下；文本SFT通过反思性推理间接提升感知。 2. 推理能力仅在感知能力同步提升时才有所提高。 3. 整合能力仍是最弱环节，当其他技能提升后，残余错误集中在此。 4. 鲁棒性表现各异：RL能提高图表变化下的一致性，而多模态SFT因过拟合降低了鲁棒性。

Conclusion: MathLens基准提供了对多模态推理子技能的细粒度分析。不同的训练方法对感知、推理和整合等子技能产生不均衡影响。感知和整合是模型提升的关键瓶颈，其中整合能力最为薄弱，且训练方法对模型鲁棒性的影响也存在差异。

Abstract: Multimodal reasoning models have recently shown promise on challenging
domains such as olympiad-level geometry, yet their evaluation remains dominated
by aggregate accuracy, a single score that obscures where and how models are
improving. We introduce MathLens, a benchmark designed to disentangle the
subskills of multimodal reasoning while preserving the complexity of
textbook-style geometry problems. The benchmark separates performance into
three components: Perception: extracting information from raw inputs,
Reasoning: operating on available information, and Integration: selecting
relevant perceptual evidence and applying it within reasoning. To support each
test, we provide annotations: visual diagrams, textual descriptions to evaluate
reasoning in isolation, controlled questions that require both modalities, and
probes for fine-grained perceptual skills, all derived from symbolic
specifications of the problems to ensure consistency and robustness. Our
analysis reveals that different training approaches have uneven effects: First,
reinforcement learning chiefly strengthens perception, especially when
supported by textual supervision, while textual SFT indirectly improves
perception through reflective reasoning. Second, reasoning improves only in
tandem with perception. Third, integration remains the weakest capacity, with
residual errors concentrated there once other skills advance. Finally,
robustness diverges: RL improves consistency under diagram variation, whereas
multimodal SFT reduces it through overfitting. We will release all data and
experimental logs.

</details>


### [60] [Machine-interpretable Engineering Design Standards for Valve Specification](https://arxiv.org/abs/2510.01736)
*Anders Gjerver,Rune Frostad,Vedrana Barisic,Melinda Hodkiewicz,Caitlin Woods,Mihaly Fekete,Arild Braathen Torjusen,Johan Wilhelm Kluwer*

Main category: cs.CL

TL;DR: 该研究将工程设计标准转化为模块化、机器可解释的本体，并利用这些本体对工厂设计和设备选型过程进行质量保证，实现了自动验证和合规性检查，对数字化智能标准具有重要潜力。


<details>
  <summary>Details</summary>
Motivation: 工程设计过程依赖于技术规范和标准，但目前主要仍以文档为中心，与工业数字化转型目标不符。研究旨在解决现有标准信息数字化和机器可解释性不足的问题，以提高设计过程的质量保证效率。

Method: 将工程设计标准（文本和表格）中的信息转化为模块化、可重用、机器可解释的本体。采用建模模式创建模块化本体，以W3C兼容格式存储，并与顶层本体ISO DIS 23726-3（工业数据本体IDO）对齐。以阀门选型过程为例进行测试，将阀门、环境条件、功能位置标签和制造商产品类型实例化为语义资产模型中的OWL个体，并利用语义推理和可执行设计规则进行验证。

Result: 成功实现了特定阀门数据表（VDS）与相关行业标准的自动化合规性验证。通过语义推理和可执行设计规则，能够判断产品类型是否符合阀门规范。创建了基于IDO的共享、可重用模块化本体，证明了该方法在设备选型过程中应用语义推理的可行性。

Conclusion: 该方法使语义推理能够应用于设备选型过程，并展示了其在帮助标准机构向数字化智能标准过渡方面的巨大潜力。通过将设计标准转化为机器可解释的本体，可以有效提升工程设计过程的自动化质量保证水平。

Abstract: Engineering design processes use technical specifications and must comply
with standards. Product specifications, product type data sheets, and design
standards are still mainly document-centric despite the ambition to digitalize
industrial work. In this paper, we demonstrate how to transform information
held in engineering design standards into modular, reusable,
machine-interpretable ontologies and use the ontologies in quality assurance of
the plant design and equipment selection process. We use modelling patterns to
create modular ontologies for knowledge captured in the text and in frequently
referenced tables in International Standards for piping, material and valve
design. These modules are exchangeable, as stored in a W3C compliant format,
and interoperable as they are aligned with the top-level ontology ISO DIS
23726-3: Industrial Data Ontology (IDO).
  We test these ontologies, created based on international material and piping
standards and industry norms, on a valve selection process. Valves are
instantiated in semantic asset models as individuals along with a semantic
representation of the environmental condition at their location on the asset.
We create "functional location tags" as OWL individuals that become instances
of OWL class Valve Data Sheet (VDS) specified valves. Similarly we create
instances of manufacturer product type. Our approach enables automated
validation that a specific VDS is compliant with relevant industry standards.
Using semantic reasoning and executable design rules, we also determine whether
the product type meets the valve specification. Creation of shared, reusable
IDO-based modular ontologies for design standards enables semantic reasoning to
be applied to equipment selection processes and demonstrates the potential of
this approach for Standards Bodies wanting to transition to digitized Smart
Standards.

</details>


### [61] [Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware Refusal in Factual Tasks](https://arxiv.org/abs/2510.01782)
*Wenbo Pan,Jie Xu,Qiguang Chen,Junhao Dong,Libo Qin,Xinfeng Li,Haining Yu,Xiaohua Jia*

Main category: cs.CL

TL;DR: 本文提出“拒绝指数”（Refusal Index, RI），一种新的原理性指标，用于准确衡量大型语言模型（LLMs）拒绝超出其知识范围问题的能力，并发现LLMs的拒绝行为可能不可靠，需要RI来补充现有准确性评估。


<details>
  <summary>Details</summary>
Motivation: LLMs的知识感知拒绝能力对事实可靠性至关重要，但现有指标（简单的基于拒绝率的指标或代理校准指标）无法准确或真实地衡量这种能力，存在偏差或未能捕捉实际拒绝行为。

Method: 提出“拒绝指数”（RI），定义为拒绝概率与错误概率之间的Spearman等级相关系数。设计了一种轻量级的两阶段评估方法，通过两次标准评估运行中观察到的拒绝率，高效估计RI。

Result: RI在16个模型和5个数据集上被证明能准确量化LLMs内在的知识感知拒绝能力。RI在不同拒绝率下保持稳定，并提供与模型整体准确性和拒绝率无关的一致模型排名。研究发现，LLMs尽管在事实任务上准确率高，但其拒绝行为可能不可靠且脆弱。

Conclusion: RI为LLM事实性评估提供了一个重要但此前被忽视的视角，强调需要RI来补充传统的准确性指标，以实现全面的事实性评估。

Abstract: Large Language Models (LLMs) should refuse to answer questions beyond their
knowledge. This capability, which we term knowledge-aware refusal, is crucial
for factual reliability. However, existing metrics fail to faithfully measure
this ability. On the one hand, simple refusal-based metrics are biased by
refusal rates and yield inconsistent scores when models exhibit different
refusal tendencies. On the other hand, existing calibration metrics are
proxy-based, capturing the performance of auxiliary calibration processes
rather than the model's actual refusal behavior. In this work, we propose the
Refusal Index (RI), a principled metric that measures how accurately LLMs
refuse questions they do not know. We define RI as Spearman's rank correlation
between refusal probability and error probability. To make RI practically
measurable, we design a lightweight two-pass evaluation method that efficiently
estimates RI from observed refusal rates across two standard evaluation runs.
Extensive experiments across 16 models and 5 datasets demonstrate that RI
accurately quantifies a model's intrinsic knowledge-aware refusal capability in
factual tasks. Notably, RI remains stable across different refusal rates and
provides consistent model rankings independent of a model's overall accuracy
and refusal rates. More importantly, RI provides insight into an important but
previously overlooked aspect of LLM factuality: while LLMs achieve high
accuracy on factual tasks, their refusal behavior can be unreliable and
fragile. This finding highlights the need to complement traditional accuracy
metrics with the Refusal Index for comprehensive factuality evaluation.

</details>


### [62] [Comparison of Unsupervised Metrics for Evaluating Judicial Decision Extraction](https://arxiv.org/abs/2510.01792)
*Ivan Leonidovich Litvak,Anton Kostin,Fedor Lashkin,Tatiana Maksiyan,Sergey Lagutin*

Main category: cs.CL

TL;DR: 本研究评估了16种无监督指标（包括基于LLM的）在俄罗斯司法判决文本提取中的表现，发现它们能实现可扩展筛选，但因相关性中等，尚不能完全取代高风险法律场景中的人工判断。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在法律自然语言处理领域的快速发展，需要可扩展的方法来评估司法判决中文本提取的质量。

Method: 本研究评估了16种无监督指标（包括新颖的公式），用于评估从1,000份匿名俄罗斯司法判决中提取七个语义块的质量。这些指标涵盖文档、语义、结构、伪真实和法律特定类别，且无需预先标注的真实标签。评估结果通过7,168份专家（1-5 Likert量表）评审进行验证，并使用自举相关性、Lin一致性相关系数（CCC）和平均绝对误差（MAE）进行分析。

Result: 词频连贯性（Pearson r = 0.540, Lin CCC = 0.512, MAE = 0.127）和覆盖率/块完整性（Pearson r = 0.513, Lin CCC = 0.443, MAE = 0.139）与专家评分最吻合。法律术语密度（Pearson r = -0.479, Lin CCC = -0.079, MAE = 0.394）显示出强烈的负相关。LLM评估得分（使用gpt-4.1-mini）表现出中等程度的一致性，但其性能表明对法律文本的专业化程度有限。

Conclusion: 研究结果表明，包括基于LLM的方法在内的无监督指标可以实现可扩展的筛选，但由于中等相关性和较低的CCC值，它们不能在高风险法律背景下完全取代人类判断。这项工作通过提供无需标注的评估工具，推动了法律NLP的发展，对司法分析和道德AI部署具有重要意义。

Abstract: The rapid advancement of artificial intelligence in legal natural language
processing demands scalable methods for evaluating text extraction from
judicial decisions. This study evaluates 16 unsupervised metrics, including
novel formulations, to assess the quality of extracting seven semantic blocks
from 1,000 anonymized Russian judicial decisions, validated against 7,168
expert reviews on a 1--5 Likert scale. These metrics, spanning document-based,
semantic, structural, pseudo-ground truth, and legal-specific categories,
operate without pre-annotated ground truth. Bootstrapped correlations, Lin's
concordance correlation coefficient (CCC), and mean absolute error (MAE) reveal
that Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE =
0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC =
0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density
(Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negative
correlations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, Lin
CCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, using
gpt-4.1-mini via g4f, suggests limited specialization for legal textse. These
findings highlight that unsupervised metrics, including LLM-based approaches,
enable scalable screening but, with moderate correlations and low CCC values,
cannot fully replace human judgment in high-stakes legal contexts. This work
advances legal NLP by providing annotation-free evaluation tools, with
implications for judicial analytics and ethical AI deployment.

</details>


### [63] [Detecting LLM-Generated Spam Reviews by Integrating Language Model Embeddings and Graph Neural Network](https://arxiv.org/abs/2510.01801)
*Xin Liu,Rongwu Xu,Xinyi Jia,Jason Liao,Jiao Sun,Ling Huang,Wei Xu*

Main category: cs.CL

TL;DR: 针对大型语言模型（LLMs）生成的高度说服性垃圾评论带来的挑战，本文提出了FraudSquad，一个结合文本嵌入和图变换器的混合检测模型，该模型在LLM生成及人工编写的垃圾评论数据集上表现优异，并具有实用性。


<details>
  <summary>Details</summary>
Motivation: LLM生成的高度说服性垃圾评论对现有检测系统构成重大挑战，并威胁在线平台的可信度。当前系统难以有效识别这些模仿人类写作的评论。

Method: 1. 构建了三个逼真的LLM生成垃圾评论数据集，并通过GPT-4.1评估其高说服力和欺骗性。2. 提出了FraudSquad，一个混合检测模型，该模型结合了预训练语言模型的文本嵌入和门控图变换器，用于垃圾节点分类，以捕获语义和行为信号，无需手动特征工程或大量训练资源。

Result: 1. FraudSquad在三个LLM生成的数据集上，精度最高提升44.22%，召回率最高提升43.01%，均优于现有最先进的基线。2. 在两个人工编写的垃圾评论数据集上也取得了良好结果。3. 模型规模适中，且仅需少量标记训练数据。

Conclusion: 本研究提供了一套新的合成数据集、一个实用的垃圾评论检测框架FraudSquad，并强调了在LLM时代改进垃圾评论检测的紧迫性。FraudSquad是一个针对LLM生成垃圾评论的有效且实用的解决方案。

Abstract: The rise of large language models (LLMs) has enabled the generation of highly
persuasive spam reviews that closely mimic human writing. These reviews pose
significant challenges for existing detection systems and threaten the
credibility of online platforms. In this work, we first create three realistic
LLM-generated spam review datasets using three distinct LLMs, each guided by
product metadata and genuine reference reviews. Evaluations by GPT-4.1 confirm
the high persuasion and deceptive potential of these reviews. To address this
threat, we propose FraudSquad, a hybrid detection model that integrates text
embeddings from a pre-trained language model with a gated graph transformer for
spam node classification. FraudSquad captures both semantic and behavioral
signals without relying on manual feature engineering or massive training
resources. Experiments show that FraudSquad outperforms state-of-the-art
baselines by up to 44.22% in precision and 43.01% in recall on three
LLM-generated datasets, while also achieving promising results on two
human-written spam datasets. Furthermore, FraudSquad maintains a modest model
size and requires minimal labeled training data, making it a practical solution
for real-world applications. Our contributions include new synthetic datasets,
a practical detection framework, and empirical evidence highlighting the
urgency of adapting spam detection to the LLM era. Our code and datasets are
available at: https://anonymous.4open.science/r/FraudSquad-5389/.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [64] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: 本文提出了LVTINO，首个零样本或即插即用的高分辨率视频复原逆求解器，利用视频一致性模型（VCMs）编码先验，实现了卓越的重建质量、时间一致性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有计算成像方法在图像复原方面依赖生成扩散模型（特别是LDM），但将其扩展到高分辨率视频复原时，面临恢复精细空间细节和捕捉微妙时间依赖的挑战，导致逐帧应用图像LDM时出现时间不一致性。

Method: LVTINO利用视频一致性模型（VCMs）的最新进展，该模型将视频潜在扩散模型提炼成能显式捕捉时间因果关系的快速生成器。通过创新的条件机制，LVTINO无需自动微分，仅用少量神经函数评估即可实现高质量复原。

Result: LVTINO在各种视频逆问题上实现了最先进的视频重建质量，确保了强大的测量一致性和平滑的帧间时间过渡。相比逐帧应用图像LDM的现有方法，LVTINO在感知上显著提升，并在重建保真度和计算效率方面建立了新基准。

Conclusion: LVTINO通过有效地整合VCMs，为高分辨率视频复原提供了一个开创性的零样本解决方案，极大地提升了时间一致性、复原质量和计算效率，超越了以往基于图像LDM的方法。

Abstract: Computational imaging methods increasingly rely on powerful generative
diffusion models to tackle challenging image restoration tasks. In particular,
state-of-the-art zero-shot image inverse solvers leverage distilled
text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy
and perceptual quality with high computational efficiency. However, extending
these advances to high-definition video restoration remains a significant
challenge, due to the need to recover fine spatial detail while capturing
subtle temporal dependencies. Consequently, methods that naively apply
image-based LDM priors on a frame-by-frame basis often result in temporally
inconsistent reconstructions. We address this challenge by leveraging recent
advances in Video Consistency Models (VCMs), which distill video latent
diffusion models into fast generators that explicitly capture temporal
causality. Building on this foundation, we propose LVTINO, the first zero-shot
or plug-and-play inverse solver for high definition video restoration with
priors encoded by VCMs. Our conditioning mechanism bypasses the need for
automatic differentiation and achieves state-of-the-art video reconstruction
quality with only a few neural function evaluations, while ensuring strong
measurement consistency and smooth temporal transitions across frames.
Extensive experiments on a diverse set of video inverse problems show
significant perceptual improvements over current state-of-the-art methods that
apply image LDMs frame by frame, establishing a new benchmark in both
reconstruction fidelity and computational efficiency.

</details>


### [65] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: 针对文本到图像模型细粒度风格控制的难题，本研究提出一种三阶段训练方法，通过样式编码器和投影层从单张参考图像中提取并注入风格，结合文本描述，实现细粒度可控的风格化图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型难以通过自然语言精确描述和控制细粒度风格，且风格化参考图像的引导信息难以直接与传统文本条件对齐。

Method: 提出一种三阶段训练的样式提取图像生成方法，使用样式编码器和样式投影层将样式表示与文本表示对齐，实现细粒度文本提示的样式引导生成。同时，构建了Style30k-captions数据集用于模型训练。

Result: 抽象中未明确提及具体的实验结果或性能指标，但方法旨在实现细粒度可控的风格化图像生成。

Conclusion: 本研究通过创新的三阶段训练方法和新的数据集，旨在解决文本到图像模型在细粒度风格控制方面的挑战，实现基于参考图像和文本描述的精确风格化图像生成。

Abstract: Image generation based on text-to-image generation models is a task with
practical application scenarios that fine-grained styles cannot be precisely
described and controlled in natural language, while the guidance information of
stylized reference images is difficult to be directly aligned with the textual
conditions of traditional textual guidance generation. This study focuses on
how to maximize the generative capability of the pretrained generative model,
by obtaining fine-grained stylistic representations from a single given
stylistic reference image, and injecting the stylistic representations into the
generative body without changing the structural framework of the downstream
generative model, so as to achieve fine-grained controlled stylized image
generation. In this study, we propose a three-stage training style
extraction-based image generation method, which uses a style encoder and a
style projection layer to align the style representations with the textual
representations to realize fine-grained textual cue-based style guide
generation. In addition, this study constructs the Style30k-captions dataset,
whose samples contain a triad of images, style labels, and text descriptions,
to train the style encoder and style projection layer in this experiment.

</details>


### [66] [EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels](https://arxiv.org/abs/2510.01362)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: 本文创建了一个大规模数据集EvoStruggle，用于识别技能学习过程中的挣扎行为及其演变。通过时序动作定位模型，成功检测了不同任务和活动中的挣扎，并证明其概念可迁移。


<details>
  <summary>Details</summary>
Motivation: 识别技能学习过程中个体的挣扎对优化学习和开发辅助系统至关重要。挣扎类型和频率随技能发展而变化，理解其演变是关键。然而，现有数据集缺乏对挣扎随时间演变的研究。

Method: 收集了EvoStruggle数据集，包含61.68小时视频、2793段视频、5385个标注的挣扎片段，来自76名参与者，涵盖18项任务（打结、折纸、七巧板、洗牌），参与者重复任务以捕捉技能演变。将挣扎确定定义为时序动作定位任务。

Result: 时序动作定位模型能成功检测挣扎线索，即使在未见任务或活动中也能泛化。任务间泛化mAP为34.56%，活动间泛化mAP为19.24%。结果表明挣扎在不同技能任务中是可迁移的概念，但检测精度仍有提升空间。

Conclusion: EvoStruggle数据集及其方法为理解和检测技能学习中的挣扎提供了新资源和思路，证明了挣扎检测的可行性和概念可迁移性，为未来优化学习和辅助系统奠定了基础。

Abstract: The ability to determine when a person struggles during skill acquisition is
crucial for both optimizing human learning and enabling the development of
effective assistive systems. As skills develop, the type and frequency of
struggles tend to change, and understanding this evolution is key to
determining the user's current stage of learning. However, existing
manipulation datasets have not focused on how struggle evolves over time. In
this work, we collect a dataset for struggle determination, featuring 61.68
hours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle
segments collected from 76 participants. The dataset includes 18 tasks grouped
into four diverse activities -- tying knots, origami, tangram puzzles, and
shuffling cards, representing different task variations. In addition,
participants repeated the same task five times to capture their evolution of
skill. We define the struggle determination problem as a temporal action
localization task, focusing on identifying and precisely localizing struggle
segments with start and end times. Experimental results show that Temporal
Action Localization models can successfully learn to detect struggle cues, even
when evaluated on unseen tasks or activities. The models attain an overall
average mAP of 34.56% when generalizing across tasks and 19.24% across
activities, indicating that struggle is a transferable concept across various
skill-based tasks while still posing challenges for further improvement in
struggle detection. Our dataset is available at
https://github.com/FELIXFENG2019/EvoStruggle.

</details>


### [67] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: SPUS是一个紧凑高效的U-Net基础模型，通过自回归预训练统一求解多种PDE，参数效率高，泛化能力达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有PDE基础模型多基于大型复杂Transformer，计算和参数开销大，需要更轻量、高效的替代方案。

Method: 引入基于轻量级残差U-Net的SPUS架构，并采用模拟数值求解器行为的自回归预训练策略，以学习底层物理。

Result: SPUS在6个未曾见过的下游PDE任务上实现了最先进的泛化能力，同时显著减少了所需参数并仅需极少的微调数据。

Conclusion: SPUS作为一种高度参数高效的基础模型，在解决多样化PDE系统方面具有巨大潜力。

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


### [68] [DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation](https://arxiv.org/abs/2510.01399)
*Shubhankar Borse,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: DisCo是一个基于强化学习的框架，通过优化身份多样性，解决了文本到图像模型在生成多人物图像时出现的身份重复和混淆问题。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA文本到图像模型在处理多人物提示时存在局限，表现为面部重复、身份合并和人数错误，损害了生成质量。

Method: 引入DisCo（Diversity Constraints强化），首个直接优化多人物生成中身份多样性的强化学习框架。它通过Group-Relative Policy Optimization (GRPO) 微调流匹配模型，并采用组合式奖励函数，该函数惩罚图像内面部相似性、阻止跨样本身份重复、确保准确的人数，并通过人类偏好分数保持视觉保真度。采用单阶段课程学习稳定训练，无需额外标注。

Result: 在DiverseHumans测试集上，DisCo实现了98.6的独特面部准确率和近乎完美的全局身份分布，超越了Gemini、GPT-Image等开源和专有方法，同时保持了有竞争力的感知质量。

Conclusion: DisCo被确立为一个可扩展、免标注的解决方案，成功解决了生成模型中长期存在的身份危机，并为组合式多人物生成设立了新的基准。

Abstract: State-of-the-art text-to-image models excel at realism but collapse on
multi-human prompts - duplicating faces, merging identities, and miscounting
individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the
first RL-based framework to directly optimize identity diversity in multi-human
generation. DisCo fine-tunes flow-matching models via Group-Relative Policy
Optimization (GRPO) with a compositional reward that (i) penalizes intra-image
facial similarity, (ii) discourages cross-sample identity repetition, (iii)
enforces accurate person counts, and (iv) preserves visual fidelity through
human preference scores. A single-stage curriculum stabilizes training as
complexity scales, requiring no extra annotations. On the DiverseHumans
Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global
Identity Spread - surpassing both open-source and proprietary methods (e.g.,
Gemini, GPT-Image) while maintaining competitive perceptual quality. Our
results establish DisCo as a scalable, annotation-free solution that resolves
the long-standing identity crisis in generative models and sets a new benchmark
for compositional multi-human generation.

</details>


### [69] [GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings](https://arxiv.org/abs/2510.01448)
*Angel Daruna,Nicholas Meegan,Han-Pang Chiu,Supun Samarasekera,Rakesh Kumar*

Main category: cs.CV

TL;DR: 该研究提出一种新的视觉地理定位方法，通过分层地理嵌入和融合外观与语义分割的视觉表示，在多项基准测试中显著超越现有技术和大型视觉-语言模型。


<details>
  <summary>Details</summary>
Motivation: 在全球范围内的视觉地理定位中，尽管已取得进展，但学习到的地理表示仍然是活跃的研究课题，需要更鲁棒和有效的地理与视觉表示。

Method: 该方法将地理定位表述为查询图像的视觉表示与学习到的地理表示的对齐。具体来说，它引入了一种新颖的地理表示，将世界建模为地理嵌入的层次结构；同时，提出了一种有效融合查询图像外观特征与语义分割图的方法，以形成鲁棒的视觉表示。

Result: 在五个基准数据集上测量的25个指标中，有22个取得了历史最佳成绩，优于先前的最先进（SOTA）方法和近期的大型视觉-语言模型（LVLMs）。额外的消融研究证实这些性能提升主要源于地理和视觉表示的结合。

Conclusion: 该研究通过其独特的地理表示和视觉表示的结合，极大地提升了全球视觉地理定位的性能，达到了新的SOTA水平。

Abstract: Worldwide visual geo-localization seeks to determine the geographic location
of an image anywhere on Earth using only its visual content. Learned
representations of geography for visual geo-localization remain an active
research topic despite much progress. We formulate geo-localization as aligning
the visual representation of the query image with a learned geographic
representation. Our novel geographic representation explicitly models the world
as a hierarchy of geographic embeddings. Additionally, we introduce an approach
to efficiently fuse the appearance features of the query image with its
semantic segmentation map, forming a robust visual representation. Our main
experiments demonstrate improved all-time bests in 22 out of 25 metrics
measured across five benchmark datasets compared to prior state-of-the-art
(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional
ablation studies support the claim that these gains are primarily driven by the
combination of geographic and visual representations.

</details>


### [70] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas,Dang Nguyen,Nesihan Bulut,Mohammadhossein Bateni,Vahab Mirrokni,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: XMAS是首个针对大型视觉-语言模型(LVLMs)的高效数据指令微调方法，通过分析跨模态注意力矩阵，显著减少训练数据量并加速训练，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 数据高效学习在视觉模型和大型语言模型(LLMs)中已被广泛探索，但在大型视觉-语言模型(LVLMs)领域仍未充分研究。现有方法无法超越随机选择，亟需一种有效的数据选择策略来消除LVLM训练数据中的冗余。

Method: 本研究提出XMAS方法，基于一个原理性证明：在指令微调过程中，具有相似跨模态注意力矩阵的样本具有相似的梯度，从而对模型参数产生类似影响并传递相同信息。XMAS通过微调一个小型代理LVLM，根据样本注意力矩阵的奇异值轨迹进行聚类，并从中均匀采样一个子集，以有效去除数据冗余。

Result: 实验表明，XMAS能在LLaVA-665k数据集中丢弃50%的数据，并在Vision-Flan数据集中丢弃85%的数据，同时LLaVA-1.5-7B在10个下游基准测试中完全保持性能，并将训练速度提高1.2倍。这比LLaVA-665k的最佳基线多减少了30%的数据。

Conclusion: XMAS是首个为LVLMs指令微调设计的高效数据方法，它通过分析跨模态注意力模式，成功实现大规模LVLM训练数据中的冗余去除，显著提高了数据效率和训练速度，且不损害模型性能。

Abstract: Data-efficient learning aims to eliminate redundancy in large training
datasets by training models on smaller subsets of the most informative
examples. While data selection has been extensively explored for vision models
and large language models (LLMs), it remains underexplored for Large
Vision-Language Models (LVLMs). Notably, none of existing methods can
outperform random selection at different subset sizes. In this work, we propose
the first principled method for data-efficient instruction tuning of LVLMs. We
prove that examples with similar cross-modal attention matrices during
instruction tuning have similar gradients. Thus, they influence model
parameters in a similar manner and convey the same information to the model
during training. Building on this insight, we propose XMAS, which clusters
examples based on the trajectories of the top singular values of their
attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a
balanced subset from these clusters, XMAS effectively removes redundancy in
large-scale LVLM training data. Extensive experiments show that XMAS can
discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while
fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and
speeding up its training by 1.2x. This is 30% more data reduction compared to
the best baseline for LLaVA-665k. The project's website can be found at
https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>


### [71] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: Purrception是一种变分流匹配方法，用于矢量量化图像生成，结合连续动态和离散监督，实现了更快的训练收敛和有竞争力的生成质量。


<details>
  <summary>Details</summary>
Motivation: 旨在结合连续方法的几何感知和离散分类监督的优势，以实现对可能代码的不确定性量化和温度控制生成，同时提高训练效率。

Method: 引入Purrception，该方法将变分流匹配应用于矢量量化潜变量，通过学习码本索引上的分类后验，并在连续嵌入空间中计算速度场。

Result: 在ImageNet-1k 256x256图像生成任务上，训练收敛速度快于连续和离散流匹配基线，并取得了与最先进模型相当的FID分数。

Conclusion: 变分流匹配能够有效弥合连续传输和离散监督之间的鸿沟，从而提高图像生成的训练效率。

Abstract: We introduce Purrception, a variational flow matching approach for
vector-quantized image generation that provides explicit categorical
supervision while maintaining continuous transport dynamics. Our method adapts
Variational Flow Matching to vector-quantized latents by learning categorical
posteriors over codebook indices while computing velocity fields in the
continuous embedding space. This combines the geometric awareness of continuous
methods with the discrete supervision of categorical approaches, enabling
uncertainty quantification over plausible codes and temperature-controlled
generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training
converges faster than both continuous flow matching and discrete flow matching
baselines while achieving competitive FID scores with state-of-the-art models.
This demonstrates that Variational Flow Matching can effectively bridge
continuous transport and discrete supervision for improved training efficiency
in image generation.

</details>


### [72] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: 本研究提出了一个统一的深度学习框架，利用条件扩散模型和多任务学习，从NCCT扫描生成合成CECT图像，并同步分割主动脉腔和血栓，解决了传统方法误差累积和对比剂风险问题，并在图像合成和分割性能上超越了现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 腹主动脉瘤（AAA）评估标准CECT需要碘造影剂，存在肾毒性、过敏和环境危害等风险。现有的深度学习方法虽然尝试从NCCT生成合成CECT以减少造影剂使用，但多采用多阶段管道，导致误差累积，且未能利用共享的语义和解剖结构。

Method: 我们提出一个统一的深度学习框架，该框架结合了条件扩散模型（CDM）和多任务学习，实现图像合成与解剖分割的端到端联合优化。与以往多任务扩散模型不同，我们的方法无需初始预测，共享编码器和解码器参数，并采用半监督训练策略以处理真实世界临床数据中常见的缺失分割标签。该方法在264名患者的数据集上进行了评估。

Result: 我们的方法在图像合成和解剖分割方面均显著优于现有SOTA的单任务和多阶段模型。图像合成方面，PSNR达到25.61 dB（单任务CDM为23.80 dB）。解剖分割方面，主动脉腔Dice系数提高到0.89（nnU-Net为0.87），血栓Dice系数提高到0.53（nnU-Net为0.48）。这些改进使临床测量更准确，主动脉腔直径MAE降至4.19 mm（nnU-Net为5.78 mm），血栓面积误差降至33.85%（nnU-Net为41.45%）。

Conclusion: 本研究提出的统一深度学习框架能高效地从NCCT生成高质量合成CECT并同步进行精确的AAA解剖分割，有效规避了造影剂的风险，提升了临床测量的准确性。其多任务和半监督学习策略也使其更适用于真实的临床数据场景。

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic
aneurysms (AAA), the required iodinated contrast agents pose significant risks,
including nephrotoxicity, patient allergies, and environmental harm. To reduce
contrast agent use, recent deep learning methods have focused on generating
synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a
multi-stage pipeline that first generates images and then performs
segmentation, which leads to error accumulation and fails to leverage shared
semantic and anatomical structures. To address this, we propose a unified deep
learning framework that generates synthetic CECT images from NCCT scans while
simultaneously segmenting the aortic lumen and thrombus. Our approach
integrates conditional diffusion models (CDM) with multi-task learning,
enabling end-to-end joint optimization of image synthesis and anatomical
segmentation. Unlike previous multitask diffusion models, our approach requires
no initial predictions (e.g., a coarse segmentation mask), shares both encoder
and decoder parameters across tasks, and employs a semi-supervised training
strategy to learn from scans with missing segmentation labels, a common
constraint in real-world clinical data. We evaluated our method on a cohort of
264 patients, where it consistently outperformed state-of-the-art single-task
and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61
dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,
it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus
Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to
more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm
from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to
nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [73] [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
*Basem Rizk,Joel Walsh,Mark Core,Benjamin Nye*

Main category: cs.CV

TL;DR: 本文提出了一个框架，通过将视频转换为时间半结构化数据，进而构建可查询、支持持续学习的帧级索引知识图谱，以高效原型化多模态内容分析管道。


<details>
  <summary>Details</summary>
Motivation: 多模态内容分析（尤其是视频）计算成本高、工程量大且棘手。虽然静态数据预训练模型很多，但将其与复杂视频数据结合仍具挑战。

Method: 开发了一个高效原型化多模态内容分析管道的框架。该框架通过整合一系列预训练模型，将视频转换为时间半结构化数据格式，再进一步转换为帧级索引知识图谱。此知识图谱可查询，并支持持续学习，能通过交互式媒介动态融入新的领域特定知识。

Result: 本框架能够高效地原型化多模态内容分析管道，将视频数据转换为可查询且支持持续学习的帧级索引知识图谱，从而实现新领域知识的动态融入。

Conclusion: 该研究提供了一个新颖的框架，能有效简化视频等多模态内容的分析过程，通过构建灵活的知识图谱，实现了高效原型开发和知识的动态更新。

Abstract: Analysis of multi-modal content can be tricky, computationally expensive, and
require a significant amount of engineering efforts. Lots of work with
pre-trained models on static data is out there, yet fusing these opensource
models and methods with complex data such as videos is relatively challenging.
In this paper, we present a framework that enables efficiently prototyping
pipelines for multi-modal content analysis. We craft a candidate recipe for a
pipeline, marrying a set of pre-trained models, to convert videos into a
temporal semi-structured data format. We translate this structure further to a
frame-level indexed knowledge graph representation that is query-able and
supports continual learning, enabling the dynamic incorporation of new
domain-specific knowledge through an interactive medium.

</details>


### [74] [WALT: Web Agents that Learn Tools](https://arxiv.org/abs/2510.01524)
*Viraj Prabhu,Yutong Dai,Matthew Fernandez,Jing Gu,Krithika Ramakrishnan,Yanqi Luo,Silvio Savarese,Caiming Xiong,Junnan Li,Zeyuan Chen,Ran Xu*

Main category: cs.CV

TL;DR: WALT框架通过逆向工程网站功能，将其转化为可重用工具，实现更稳定、高效的浏览器自动化，减少对低级UI交互和LLM推理的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有Web代理依赖于逐步的UI交互和大量LLM推理，在动态布局和长任务中表现脆弱，易失效。

Method: 引入WALT（Web Agents that Learn Tools）框架，该框架逆向工程网站固有的功能（如搜索、过滤、创建、编辑等）为可调用的高级工具，从而将底层执行抽象化，让代理直接调用工具而非推理点击和输入。

Result: 在VisualWebArena和WebArena基准测试中，WALT以更少的步骤和更低的LLM依赖实现了更高的成功率。

Conclusion: WALT通过将计算负担从脆弱的逐步推理转移到可靠的工具调用，为浏览器自动化建立了一个鲁棒且可泛化的新范式。

Abstract: Web agents promise to automate complex browser tasks, but current methods
remain brittle -- relying on step-by-step UI interactions and heavy LLM
reasoning that break under dynamic layouts and long horizons. Humans, by
contrast, exploit website-provided functionality through high-level operations
like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),
a framework that reverse-engineers latent website functionality into reusable
invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust
implementations of automations already designed into websites -- spanning
discovery (search, filter, sort), communication (post, comment, upvote), and
content management (create, edit, delete). Tools abstract away low-level
execution: instead of reasoning about how to click and type, agents simply call
search(query) or create(listing). This shifts the computational burden from
fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena
and WebArena, WALT achieves higher success with fewer steps and less
LLM-dependent reasoning, establishing a robust and generalizable paradigm for
browser automation.

</details>


### [75] [MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2510.01532)
*Meilong Xu,Xiaoling Hu,Shahira Abousamra,Chen Li,Chao Chen*

Main category: cs.CV

TL;DR: 本文提出一种半监督分割框架，通过多扰动预测和强制拓扑一致性，并引入新颖的拓扑特征匹配策略，解决了组织病理学图像中密集物体分割的挑战，实现了更鲁棒和准确的分割。


<details>
  <summary>Details</summary>
Motivation: 在半监督分割中，从无标签数据中捕获有意义的语义结构至关重要，尤其在目标密集分布的组织病理学图像分析中，这一挑战更为突出。

Method: 本文提出一个半监督分割框架，利用随机dropout和训练快照获得多个扰动预测，并通过强制这些预测间保持拓扑一致性来识别和保留相关拓扑特征。为解决无真值下拓扑特征的匹配难题，引入了一种结合空间重叠与全局结构对齐的新型匹配策略。

Result: 广泛实验证明，该方法能有效减少拓扑错误，从而产生更鲁棒、更准确的分割结果，对后续分析至关重要。

Conclusion: 所提出的方法通过强调拓扑一致性并引入创新的特征匹配策略，显著提升了组织病理学图像半监督分割的准确性和鲁棒性，克服了密集对象分割的固有挑战。

Abstract: In semi-supervised segmentation, capturing meaningful semantic structures
from unlabeled data is essential. This is particularly challenging in
histopathology image analysis, where objects are densely distributed. To
address this issue, we propose a semi-supervised segmentation framework
designed to robustly identify and preserve relevant topological features. Our
method leverages multiple perturbed predictions obtained through stochastic
dropouts and temporal training snapshots, enforcing topological consistency
across these varied outputs. This consistency mechanism helps distinguish
biologically meaningful structures from transient and noisy artifacts. A key
challenge in this process is to accurately match the corresponding topological
features across the predictions in the absence of ground truth. To overcome
this, we introduce a novel matching strategy that integrates spatial overlap
with global structural alignment, minimizing discrepancies among predictions.
Extensive experiments demonstrate that our approach effectively reduces
topological errors, resulting in more robust and accurate segmentations
essential for reliable downstream analysis. Code is available at
\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.

</details>


### [76] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: 本文提出Diffusion-LPO，一个针对扩散模型进行列表式偏好优化的框架，通过扩展DPO目标来处理列表式人类反馈，并在多种任务中优于成对DPO。


<details>
  <summary>Details</summary>
Motivation: 尽管RLHF（特别是DPO）在文本到图像扩散模型对齐人类偏好方面有效，但其应用主要依赖成对偏好。然而，人类对图像的反馈常包含更精确的隐式排名信息（列表式偏好），而针对扩散模型的列表式偏好优化尚未得到充分解决。

Method: 我们提出了Diffusion-LPO框架。给定一个描述，该框架将用户反馈聚合成一个图像排名列表，并基于Plackett-Luce模型推导出一个DPO目标的列表式扩展。Diffusion-LPO通过确保每个样本都优于所有排名较低的替代品，强制整个排名的一致性。

Result: 实验证明Diffusion-LPO在文本到图像生成、图像编辑和个性化偏好对齐等任务中均有效。它在视觉质量和偏好对齐方面持续优于成对DPO基线。

Conclusion: Diffusion-LPO是一个简单而有效的框架，能够利用列表式人类偏好来优化扩散模型，在多项任务中实现优于成对DPO的性能。

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness
for aligning text-to-image (T2I) diffusion models with human preferences.
Although Direct Preference Optimization (DPO) is widely adopted for its
computational efficiency and avoidance of explicit reward modeling, its
applications to diffusion models have primarily relied on pairwise preferences.
The precise optimization of listwise preferences remains largely unaddressed.
In practice, human feedback on image preferences often contains implicit ranked
information, which conveys more precise human preferences than pairwise
comparisons. In this work, we propose Diffusion-LPO, a simple and effective
framework for Listwise Preference Optimization in diffusion models with
listwise data. Given a caption, we aggregate user feedback into a ranked list
of images and derive a listwise extension of the DPO objective under the
Plackett-Luce model. Diffusion-LPO enforces consistency across the entire
ranking by encouraging each sample to be preferred over all of its lower-ranked
alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO
across various tasks, including text-to-image generation, image editing, and
personalized preference alignment. Diffusion-LPO consistently outperforms
pairwise DPO baselines on visual quality and preference alignment.

</details>


### [77] [Growing Visual Generative Capacity for Pre-Trained MLLMs](https://arxiv.org/abs/2510.01546)
*Hanyu Wang,Jiaming Han,Ziyan Yang,Qi Zhao,Shanchuan Lin,Xiangyu Yue,Abhinav Shrivastava,Zhenheng Yang,Hao Chen*

Main category: cs.CV

TL;DR: 本文提出Bridge，一个纯自回归统一多模态大模型，通过Mixture-of-Transformers架构实现图像理解和生成。该模型结合语义到像素的离散表示，在多模态理解和生成任务上表现优异，且训练成本更低。


<details>
  <summary>Details</summary>
Motivation: 构建支持理解和生成的统一多模态大模型面临挑战。现有混合方法能生成高质量图像但破坏自回归范式；纯自回归方法虽统一但常在语义对齐和像素级保真度之间权衡。

Method: 提出Bridge，一个纯自回归统一多模态大模型。它通过Mixture-of-Transformers架构增强预训练视觉理解模型的生成能力，在单一的下一词元预测框架下实现理解和生成。为提高视觉生成保真度，引入语义到像素的离散表示，融合紧凑语义词元和细粒度像素词元，仅增加7.9%的序列长度。

Result: Bridge在多模态理解和生成基准测试中均取得了有竞争力或更优异的结果。与现有统一多模态大模型相比，它所需的训练数据更少，训练时间也更短。

Conclusion: Bridge成功构建了一个高效且高性能的纯自回归统一多模态大模型，有效解决了现有方法的不足，并在多模态理解和生成任务中达到领先水平，同时优化了训练效率。

Abstract: Multimodal large language models (MLLMs) extend the success of language
models to visual understanding, and recent efforts have sought to build unified
MLLMs that support both understanding and generation. However, constructing
such models remains challenging: hybrid approaches combine continuous
embeddings with diffusion or flow-based objectives, producing high-quality
images but breaking the autoregressive paradigm, while pure autoregressive
approaches unify text and image prediction over discrete visual tokens but
often face trade-offs between semantic alignment and pixel-level fidelity. In
this work, we present Bridge, a pure autoregressive unified MLLM that augments
pre-trained visual understanding models with generative ability through a
Mixture-of-Transformers architecture, enabling both image understanding and
generation within a single next-token prediction framework. To further improve
visual generation fidelity, we propose a semantic-to-pixel discrete
representation that integrates compact semantic tokens with fine-grained pixel
tokens, achieving strong language alignment and precise description of visual
details with only a 7.9% increase in sequence length. Extensive experiments
across diverse multimodal benchmarks demonstrate that Bridge achieves
competitive or superior results in both understanding and generation
benchmarks, while requiring less training data and reduced training time
compared to prior unified MLLMs.

</details>


### [78] [Robust Classification of Oral Cancer with Limited Training Data](https://arxiv.org/abs/2510.01547)
*Akshay Bhagwan Sonawane,Lena D. Swamikannan,Lakshman Tamil*

Main category: cs.CV

TL;DR: 针对口腔癌早期诊断数据稀缺问题，本文提出一种结合CNN和贝叶斯深度学习的混合模型，通过不确定性量化提升可靠性和泛化能力，在小数据集下，其在真实世界图像上的表现显著优于传统CNN。


<details>
  <summary>Details</summary>
Motivation: 口腔癌全球高发，早期诊断对降低死亡率至关重要，但受限于医疗资源匮乏和基础设施不足。传统深度学习模型需要大量数据以避免过拟合和提高泛化性，且点估计易导致过自信，不适用于数据稀缺的实际场景。

Method: 提出一种结合卷积神经网络（CNN）与贝叶斯深度学习的混合模型，用于口腔癌分类。该模型采用变分推断（variational inference）进行不确定性量化以增强可靠性。模型使用智能手机拍摄的彩色图像进行训练，并在三个不同测试数据集上进行评估。

Result: 在与训练数据分布相似的测试集上，模型准确率达94%，与传统CNN相当。在真实世界、多样化且与训练数据分布差异大的图像数据集上，该模型展现出卓越的泛化能力，准确率达到88%，远超传统CNN的72.94%，且仅使用了少量数据集。置信度分析表明，模型对正确分类样本表现出低不确定性（高置信度），对错误分类样本表现出高不确定性（低置信度）。

Conclusion: 研究结果强调了贝叶斯推断在数据稀缺环境中提高模型可靠性和泛化能力方面的有效性，能显著增强早期口腔癌诊断的准确性和可信度。

Abstract: Oral cancer ranks among the most prevalent cancers globally, with a
particularly high mortality rate in regions lacking adequate healthcare access.
Early diagnosis is crucial for reducing mortality; however, challenges persist
due to limited oral health programs, inadequate infrastructure, and a shortage
of healthcare practitioners. Conventional deep learning models, while
promising, often rely on point estimates, leading to overconfidence and reduced
reliability. Critically, these models require large datasets to mitigate
overfitting and ensure generalizability, an unrealistic demand in settings with
limited training data. To address these issues, we propose a hybrid model that
combines a convolutional neural network (CNN) with Bayesian deep learning for
oral cancer classification using small training sets. This approach employs
variational inference to enhance reliability through uncertainty
quantification. The model was trained on photographic color images captured by
smartphones and evaluated on three distinct test datasets. The proposed method
achieved 94% accuracy on a test dataset with a distribution similar to that of
the training data, comparable to traditional CNN performance. Notably, for
real-world photographic image data, despite limitations and variations
differing from the training dataset, the proposed model demonstrated superior
generalizability, achieving 88% accuracy on diverse datasets compared to 72.94%
for traditional CNNs, even with a smaller dataset. Confidence analysis revealed
that the model exhibits low uncertainty (high confidence) for correctly
classified samples and high uncertainty (low confidence) for misclassified
samples. These results underscore the effectiveness of Bayesian inference in
data-scarce environments in enhancing early oral cancer diagnosis by improving
model reliability and generalizability.

</details>


### [79] [Consistent Assistant Domains Transformer for Source-free Domain Adaptation](https://arxiv.org/abs/2510.01559)
*Renrong Shao,Wei Zhang,Kangyang Luo,Qin Li,and Jun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CADTrans的新型无源域适应（SFDA）方法，通过构建域一致的不变特征表示和条件多核最大均值差异策略来有效处理源域不可访问下硬样本和域偏差问题，从而显著提升了SFDA性能。


<details>
  <summary>Details</summary>
Motivation: 无源域适应（SFDA）在没有直接访问源域的情况下适应目标域是一个挑战。由于源域数据不可访问，难以获取确定的不变特征。现有主流方法侧重于评估目标域中与源域相似的不变特征并进行对齐，但它们易受硬样本和域偏差的影响。

Method: 本文提出了一个一致辅助域Transformer (CADTrans) 来解决问题，它通过构建域一致的不变特征表示。具体方法包括：1) 开发一个辅助域模块，从中间聚合的全局注意力中获取多样化的表示，以解决现有方法在表示多样性方面的局限。2) 基于辅助域和目标域，通过多种一致性策略获取不变特征表示，用于区分简单样本和困难样本。3) 构建一个条件多核最大均值差异（CMK-MMD）策略，将硬样本与相应的简单样本对齐，并区分相同类别和不同类别的样本。

Result: 在Office-31、Office-Home、VISDA-C和DomainNet-126等多个基准测试上进行了大量实验，证明了所提出的方法取得了显著的性能提升。

Conclusion: CADTrans通过构建域一致的不变特征表示和创新的硬样本对齐策略，成功解决了SFDA中由于源域不可访问导致的挑战，并有效克服了现有方法在处理硬样本和域偏差方面的不足，实现了卓越的适应性能。

Abstract: Source-free domain adaptation (SFDA) aims to address the challenge of
adapting to a target domain without accessing the source domain directly.
However, due to the inaccessibility of source domain data, deterministic
invariable features cannot be obtained. Current mainstream methods primarily
focus on evaluating invariant features in the target domain that closely
resemble those in the source domain, subsequently aligning the target domain
with the source domain. However, these methods are susceptible to hard samples
and influenced by domain bias. In this paper, we propose a Consistent Assistant
Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue
by constructing invariable feature representations of domain consistency.
Concretely, we develop an assistant domain module for CADTrans to obtain
diversified representations from the intermediate aggregated global attentions,
which addresses the limitation of existing methods in adequately representing
diversity. Based on assistant and target domains, invariable feature
representations are obtained by multiple consistent strategies, which can be
used to distinguish easy and hard samples. Finally, to align the hard samples
to the corresponding easy samples, we construct a conditional multi-kernel max
mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same
category and those of different categories. Extensive experiments are conducted
on various benchmarks such as Office-31, Office-Home, VISDA-C, and
DomainNet-126, proving the significant performance improvements achieved by our
proposed approaches. Code is available at
https://github.com/RoryShao/CADTrans.git.

</details>


### [80] [Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](https://arxiv.org/abs/2510.01576)
*Ricardo Gonzalez Penuela,Felipe Arias-Russi,Victor Capriles*

Main category: cs.CV

TL;DR: 为解决MLLM为视障用户生成冗长描述的问题，本研究开发了一个系统，利用历史提问数据引导MLLM生成更具上下文相关性的图像描述，提高了信息获取效率。


<details>
  <summary>Details</summary>
Motivation: 现有的集成MLLM的视觉解释应用为视障用户提供的信息往往冗长且缺乏上下文关联，导致用户需要筛选大量无关信息，降低了效率。

Method: 开发了一个系统，通过识别给定图像与VizWiz-LF数据集中相似的历史视觉上下文，并利用相关的用户问题来指导MLLM生成针对性更强的描述。

Result: 通过三名人工标注者的评估，发现上下文感知描述在76.1%（70/92）的情况下预判并回答了用户问题，并在54.4%（50/92）的比较中更受青睐。

Conclusion: 利用历史用户问题引导MLLM能够有效生成更具上下文相关性的图像描述，显著提升了视障用户获取所需信息的效率和满意度。

Abstract: Multimodal large language models (MLLMs) have been integrated into visual
interpretation applications to support Blind and Low Vision (BLV) users because
of their accuracy and ability to provide rich, human-like interpretations.
However, these applications often default to comprehensive, lengthy
descriptions regardless of context. This leads to inefficient exchanges, as
users must go through irrelevant details rather than receiving the specific
information they are likely to seek. To deliver more contextually-relevant
information, we developed a system that draws on historical BLV users
questions. When given an image, our system identifies similar past visual
contexts from the VizWiz-LF dataset and uses the associated questions to guide
the MLLM generate descriptions more relevant to BLV users. An evaluation with
three human labelers who revised 92 context-aware and context-free descriptions
showed that context-aware descriptions anticipated and answered users'
questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of
comparisons (50 out of 92). Our paper reviews, and data analysis are publicly
available in a Github repository at
https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

</details>


### [81] [ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models](https://arxiv.org/abs/2510.01582)
*Krishna Teja Chitty-Venkata,Murali Emani*

Main category: cs.CV

TL;DR: 本文开发了一个名为ImageNet-Think的多模态推理数据集，旨在提升视觉语言模型（VLMs）的显式推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前VLM缺乏显式推理能力，该研究旨在通过构建新数据集，促进具备明确推理步骤的VLM发展，并加深对多模态推理机制的理解。

Method: 数据集基于ImageNet21k的25万张图片构建。利用GLM-4.1V-9B-Thinking和Kimi-VL-A3B-Thinking-2506两个先进VLM生成合成数据，为每张图片提供两对包含结构化思维token和相应答案的序列，捕获VLM的逐步推理过程和最终描述性答案。

Result: 成功构建了ImageNet-Think数据集，包含了25万张图片及其配套的思维-答案序列，为训练和评估多模态推理模型提供了宝贵资源。

Conclusion: ImageNet-Think数据集有望推动开发更强大、更具推理能力的VLM，并促进对多模态推理机制的更广泛理解。该数据集及其评估基准将公开，以支持相关研究。

Abstract: We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the
development of Vision Language Models (VLMs) with explicit reasoning
capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,
providing structured thinking tokens and corresponding answers. Our synthetic
dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and
Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of
thinking-answer sequences, creating a resource for training and evaluating
multimodal reasoning models. We capture the step-by-step reasoning process of
VLMs and the final descriptive answers. Our goal with this dataset is to enable
the development of more robust VLMs while contributing to the broader
understanding of multimodal reasoning mechanisms. The dataset and evaluation
benchmarks will be publicly available to aid research in reasoning/thinking
multimodal VLMs.

</details>


### [82] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdrón-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 针对图像逆问题中感知算子空域的模糊性，本文提出一种名为“空域非线性投影”（NPN）的新型正则化方法，它利用神经网络将解投影到感知矩阵空域的低维空间中，从而在多种逆问题中显著提升了重建保真度。


<details>
  <summary>Details</summary>
Motivation: 图像逆问题是一个病态问题，由于感知算子的空域存在无限解，导致结果模糊。现有先验信息（如手工正则化器或学习模型）虽然能约束解空间，但通常忽略了该空域的任务特定结构。

Method: 提出“空域非线性投影”（NPN），这是一种新型正则化方法。它不强制图像域的结构约束，而是通过神经网络，将解推广到感知矩阵空域的低维投影中。该方法具有可解释性和灵活性。

Result: NPN具有两大优势：(1) 可解释性：通过关注空域结构，设计出捕获感知过程盲区信息的先验。(2) 灵活性：适用于多种逆问题，并兼容现有重建框架。该方法在即插即用（plug-and-play）方法中提供了收敛性和重建精度的理论保证。实证结果表明，NPN在压缩感知、去模糊、超分辨率、CT和MRI等多种成像逆问题中，结合即插即用、展开网络、深度图像先验和扩散模型，均能持续提高重建保真度。

Conclusion: NPN通过利用感知矩阵空域的独特结构，提供了一种新颖、可解释且灵活的正则化方案，有效解决了图像逆问题中的模糊性。它能显著提升多种成像逆问题的重建精度，并与现有重建框架良好兼容。

Abstract: Imaging inverse problems aims to recover high-dimensional signals from
undersampled, noisy measurements, a fundamentally ill-posed task with infinite
solutions in the null-space of the sensing operator. To resolve this ambiguity,
prior information is typically incorporated through handcrafted regularizers or
learned models that constrain the solution space. However, these priors
typically ignore the task-specific structure of that null-space. In this work,
we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel
class of regularization that, instead of enforcing structural constraints in
the image domain, promotes solutions that lie in a low-dimensional projection
of the sensing matrix's null-space with a neural network. Our approach has two
key advantages: (1) Interpretability: by focusing on the structure of the
null-space, we design sensing-matrix-specific priors that capture information
orthogonal to the signal components that are fundamentally blind to the sensing
process. (2) Flexibility: NPN is adaptable to various inverse problems,
compatible with existing reconstruction frameworks, and complementary to
conventional image-domain priors. We provide theoretical guarantees on
convergence and reconstruction accuracy when used within plug-and-play methods.
Empirical results across diverse sensing matrices demonstrate that NPN priors
consistently enhance reconstruction fidelity in various imaging inverse
problems, such as compressive sensing, deblurring, super-resolution, computed
tomography, and magnetic resonance imaging, with plug-and-play methods,
unrolling networks, deep image prior, and diffusion models.

</details>


### [83] [Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics](https://arxiv.org/abs/2510.01618)
*Zijun Li,Jinchang Zhang,Ming Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: 提出一个自动化基因组解释模块，结合CGR与概念瓶颈模型(CBM)，将DNA序列转化为可解释、可操作的决策，实现SOTA分类性能、高概念预测保真度及更优的成本效益，适用于医疗自动化。


<details>
  <summary>Details</summary>
Motivation: 将原始DNA序列转化为可操作、可解释的决策，以集成到医疗自动化和机器人系统中，并为基因组医学中的机器人和临床自动化建立可靠基础。

Method: 核心方法结合混沌博弈表示(CGR)和概念瓶颈模型(CBM)，强制预测通过生物学上有意义的概念（如GC含量、CpG密度、k-mer基序）。为增强可靠性，引入概念保真度监督、先验一致性对齐、KL分布匹配和不确定性校准。此外，包含一个成本感知推荐层来制定决策策略。

Result: 在内部和LANL数据集上实现HIV亚型准确分类；提供可直接根据生物学先验验证的可解释证据；成本感知层减少不必要的重复测试并提高效率；达到最先进的分类性能、卓越的概念预测保真度，并获得比现有基线更有利的成本效益权衡。

Conclusion: 该工作通过弥合可解释基因组建模与自动化决策之间的鸿沟，为基因组医学中的机器人和临床自动化奠定了可靠基础。

Abstract: We propose an automated genomic interpretation module that transforms raw DNA
sequences into actionable, interpretable decisions suitable for integration
into medical automation and robotic systems. Our framework combines Chaos Game
Representation (CGR) with a Concept Bottleneck Model (CBM), enforcing
predictions to flow through biologically meaningful concepts such as GC
content, CpG density, and k mer motifs. To enhance reliability, we incorporate
concept fidelity supervision, prior consistency alignment, KL distribution
matching, and uncertainty calibration. Beyond accurate classification of HIV
subtypes across both in-house and LANL datasets, our module delivers
interpretable evidence that can be directly validated against biological
priors. A cost aware recommendation layer further translates predictive outputs
into decision policies that balance accuracy, calibration, and clinical
utility, reducing unnecessary retests and improving efficiency. Extensive
experiments demonstrate that the proposed system achieves state of the art
classification performance, superior concept prediction fidelity, and more
favorable cost benefit trade-offs compared to existing baselines. By bridging
the gap between interpretable genomic modeling and automated decision-making,
this work establishes a reliable foundation for robotic and clinical automation
in genomic medicine.

</details>


### [84] [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2510.01623)
*Angen Ye,Zeyu Zhang,Boyuan Wang,Xiaofeng Wang,Dapeng Zhang,Zheng Zhu*

Main category: cs.CV

TL;DR: VLA-R1通过结合可验证奖励的强化学习和GRPO，并利用新的CoT数据集，解决了现有VLA模型缺乏推理能力的问题，显著提升了泛化和实际性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作 (VLA) 模型缺乏显式的分步推理能力，未能充分考虑物理可行性约束和几何关系，且其后训练流程对推理质量的强化不足，导致泛化能力受限。

Method: 本文提出了VLA-R1，一个推理增强型VLA模型，它集成了基于可验证奖励的强化学习 (RLVR) 和组相对策略优化 (GRPO) 来优化推理和执行。具体来说，设计了基于RLVR的后训练策略，通过区域对齐、轨迹一致性和输出格式化来设置可验证奖励。此外，开发了高质量的VLA-CoT-13K数据集，提供了明确与物理可行性及轨迹标注对齐的思维链监督。

Result: 在域内、域外、仿真和真实机器人平台上的广泛评估表明，VLA-R1相比现有VLA方法取得了卓越的泛化能力和实际性能。

Conclusion: VLA-R1通过结合创新的强化学习策略和高质量的思维链数据集，成功增强了VLA模型的推理和执行能力，显著提升了其泛化性和真实世界性能。

Abstract: Vision-Language-Action (VLA) models aim to unify perception, language
understanding, and action generation, offering strong cross-task and
cross-scene generalization with broad impact on embodied AI. However, current
VLA models often lack explicit step-by-step reasoning, instead emitting final
actions without considering affordance constraints or geometric relations.
Their post-training pipelines also rarely reinforce reasoning quality, relying
primarily on supervised fine-tuning with weak reward design. To address these
challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates
Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative
Policy Optimization (GRPO) to systematically optimize both reasoning and
execution. Specifically, we design an RLVR-based post-training strategy with
verifiable rewards for region alignment, trajectory consistency, and output
formatting, thereby strengthening reasoning robustness and execution accuracy.
Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides
chain-of-thought supervision explicitly aligned with affordance and trajectory
annotations. Furthermore, extensive evaluations on in-domain, out-of-domain,
simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior
generalization and real-world performance compared to prior VLA methods. We
plan to release the model, code, and dataset following the publication of this
work. Code: https://github.com/GigaAI-research/VLA-R1. Website:
https://gigaai-research.github.io/VLA-R1.

</details>


### [85] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: 针对微距摄影中的散焦模糊问题，提出一种联合去模糊和三维重建方法，通过少量多视角模糊图像实现高质量去模糊和高保真三维模型恢复。


<details>
  <summary>Details</summary>
Motivation: 微距摄影虽具高分辨率和高放大率优势，但散焦模糊严重阻碍了清晰成像和高质量三维重建。传统去模糊方法数据需求大，且缺乏针对微距摄影的多视角三维重建方案。

Method: 提出一种微距摄影联合去模糊和三维重建方法。该方法从多视角模糊图像出发，联合优化物体的清晰三维模型和每个像素的散焦模糊核，并利用可微分渲染进行自监督优化。

Result: 从少量多视角图像中，该方法不仅能实现高质量的图像去模糊，还能恢复高保真度的三维外观。

Conclusion: 该方法有效解决了微距摄影中的散焦模糊难题，并能同时实现高质量图像去模糊与高保真三维重建。

Abstract: Macro lens has the advantages of high resolution and large magnification, and
3D modeling of small and detailed objects can provide richer information.
However, defocus blur in macrophotography is a long-standing problem that
heavily hinders the clear imaging of the captured objects and high-quality 3D
reconstruction of them. Traditional image deblurring methods require a large
number of images and annotations, and there is currently no multi-view 3D
reconstruction method for macrophotography. In this work, we propose a joint
deblurring and 3D reconstruction method for macrophotography. Starting from
multi-view blurry images captured, we jointly optimize the clear 3D model of
the object and the defocus blur kernel of each pixel. The entire framework
adopts a differentiable rendering method to self-supervise the optimization of
the 3D model and the defocus blur kernel. Extensive experiments show that from
a small number of multi-view images, our proposed method can not only achieve
high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [86] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: FideDiff是一种新型单步扩散模型，用于高保真图像运动去模糊，解决了现有扩散模型推理时间长和保真度低的问题。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型在图像去模糊方面展现出比CNN和Transformer更强的生成能力，但其推理时间过长和保真度受损限制了其潜力。

Method: 引入FideDiff模型，将运动去模糊重构为类似扩散的过程，其中每个时间步代表渐进模糊的图像。通过训练一致性模型将所有时间步与同一清晰图像对齐，并利用匹配模糊轨迹重建训练数据以学习时间一致性，实现单步去模糊。此外，模型通过集成Kernel ControlNet进行模糊核估计和引入自适应时间步预测来增强性能。

Result: FideDiff在全参考指标上表现优越，超越了以往基于扩散的方法，并与最先进模型性能相当。

Conclusion: FideDiff为预训练扩散模型应用于高保真图像修复任务提供了新方向，为扩散模型在实际工业应用中的进一步发展奠定了坚实基础。

Abstract: Recent advancements in image motion deblurring, driven by CNNs and
transformers, have made significant progress. Large-scale pre-trained diffusion
models, which are rich in true-world modeling, have shown great promise for
high-quality image restoration tasks such as deblurring, demonstrating stronger
generative capabilities than CNN and transformer-based methods. However,
challenges such as unbearable inference time and compromised fidelity still
limit the full potential of the diffusion models. To address this, we introduce
FideDiff, a novel single-step diffusion model designed for high-fidelity
deblurring. We reformulate motion deblurring as a diffusion-like process where
each timestep represents a progressively blurred image, and we train a
consistency model that aligns all timesteps to the same clean image. By
reconstructing training data with matched blur trajectories, the model learns
temporal consistency, enabling accurate one-step deblurring. We further enhance
model performance by integrating Kernel ControlNet for blur kernel estimation
and introducing adaptive timestep prediction. Our model achieves superior
performance on full-reference metrics, surpassing previous diffusion-based
methods and matching the performance of other state-of-the-art models. FideDiff
offers a new direction for applying pre-trained diffusion models to
high-fidelity image restoration tasks, establishing a robust baseline for
further advancing diffusion models in real-world industrial applications. Our
dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [87] [LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition](https://arxiv.org/abs/2510.01651)
*Rixin Zhou,Peiqiang Qiu,Qian Zhang,Chuntao Li,Xi Yang*

Main category: cs.CV

TL;DR: 本文提出一个大规模青铜器铭文数据集，并开发了基于LadderMoE的两阶段检测识别方法，显著提升了青铜铭文自动识别的准确性，超越了现有SOTA基线。


<details>
  <summary>Details</summary>
Motivation: 青铜器铭文是早期汉字和考古历史研究的关键证据，但其自动识别面临严重视觉退化、多领域变异（照片、拓片、描摹）以及字符分布极度长尾等挑战。

Method: 团队构建了一个包含22454张全页图像和198598个带注释字符（涵盖6658个独特类别）的大规模青铜器铭文数据集。在此基础上，他们开发了一个两阶段的检测-识别流程，首先定位铭文，然后转录单个字符。为应对异构领域和稀有类别，该流程配备了LadderMoE，它通过阶梯式MoE适配器增强了预训练的CLIP编码器，实现了动态专家特化和更强的鲁棒性。

Result: 在单字符和全页识别任务的综合实验中，所提出的方法显著优于现有最先进的场景文本识别基线，在头部、中部和尾部类别以及所有采集模式下均实现了卓越的准确性。

Conclusion: 这些成果为青铜器铭文识别和下游考古分析奠定了坚实的基础。

Abstract: Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial
stage of early Chinese writing and provide indispensable evidence for
archaeological and historical studies. However, automatic BI recognition
remains difficult due to severe visual degradation, multi-domain variability
across photographs, rubbings, and tracings, and an extremely long-tailed
character distribution. To address these challenges, we curate a large-scale BI
dataset comprising 22454 full-page images and 198598 annotated characters
spanning 6658 unique categories, enabling robust cross-domain evaluation.
Building on this resource, we develop a two-stage detection-recognition
pipeline that first localizes inscriptions and then transcribes individual
characters. To handle heterogeneous domains and rare classes, we equip the
pipeline with LadderMoE, which augments a pretrained CLIP encoder with
ladder-style MoE adapters, enabling dynamic expert specialization and stronger
robustness. Comprehensive experiments on single-character and full-page
recognition tasks demonstrate that our method substantially outperforms
state-of-the-art scene text recognition baselines, achieving superior accuracy
across head, mid, and tail categories as well as all acquisition modalities.
These results establish a strong foundation for bronze inscription recognition
and downstream archaeological analysis.

</details>


### [88] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: VirDA通过视觉重编程实现参数高效的无监督领域自适应（UDA），无需微调骨干网络，显著减少了训练参数和存储需求，并在高准确率下超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法为每个新的源-目标对微调骨干网络，导致训练参数和存储内存线性增长，并阻碍了已训练骨干参数的重用，效率低下。

Method: VirDA提出利用领域特定的纹理偏差，通过视觉重编程实现领域自适应。它在骨干网络前添加一个领域特定的视觉重编程层，生成视觉提示作为输入图像的额外纹理偏差，以适应目标域的“风格”。该方法通过多目标函数优化视觉重编程层，使其在应用领域自适应视觉提示时优化域内和域间分布差异，且无需修改骨干参数，从而实现骨干网络的跨域重用。

Result: 在Office-31数据集上，VirDA实现了92.8%的平均准确率，仅需1.5M可训练参数。与SOTA参数高效UDA基线PDA相比，VirDA准确率高出1.6%，参数仅为46%。与全骨干网络微调方法CDTrans和FixBi相比，VirDA分别高出0.2%和1.4%的准确率，而可训练参数仅为1.7%和2.8%。与当前最强方法PMTrans和TVT相比，VirDA使用约1.7%的参数，仅牺牲了2.2%和1.1%的准确率。

Conclusion: VirDA提出了一种高效、参数极简且性能卓越的UDA方法，通过视觉重编程克服了传统UDA方法中参数膨胀和骨干网络无法重用的问题，为领域自适应提供了新的解决方案。

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [89] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: 本文提出离散面部编码（DFE），一种基于RVQ-VAE的无监督系统，能从3D面部序列学习可解释的离散表情模式，并在心理任务中展现出比FACS更精确和广泛的性能。


<details>
  <summary>Details</summary>
Motivation: 现有面部表情编码系统（如FACS）覆盖范围有限且依赖高成本手动标注，阻碍了对人类行为的深入理解。

Method: 引入离散面部编码（DFE），利用残差向量量化变分自编码器（RVQ-VAE）无监督地从3D网格序列中学习紧凑、可解释的面部表情字典。首先通过3DMM提取与身份无关的表情特征，然后RVQ-VAE将其编码为一系列离散令牌，每个令牌代表一个可重用的面部形变模式。

Result: DFE捕捉面部行为比FACS及其他替代方法更精确。在压力、人格和抑郁检测等心理任务中，基于DFE令牌的Bag-of-Words模型持续优于FACS管道和现有强学习模型。此外，DFE覆盖了更广泛的面部表情。

Conclusion: DFE为FACS提供了一个可扩展且有效的替代方案，在心理学和情感计算应用中具有巨大潜力。

Abstract: Facial expression analysis is central to understanding human behavior, yet
existing coding systems such as the Facial Action Coding System (FACS) are
constrained by limited coverage and costly manual annotation. In this work, we
introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven
alternative of compact and interpretable dictionary of facial expressions from
3D mesh sequences learned through a Residual Vector Quantized Variational
Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant
expression features from images using a 3D Morphable Model (3DMM), effectively
disentangling factors such as head pose and facial geometry. We then encode
these features using an RVQ-VAE, producing a sequence of discrete tokens from a
shared codebook, where each token captures a specific, reusable facial
deformation pattern that contributes to the overall expression. Through
extensive experiments, we demonstrate that Discrete Facial Encoding captures
more precise facial behaviors than FACS and other facial encoding alternatives.
We evaluate the utility of our representation across three high-level
psychological tasks: stress detection, personality prediction, and depression
detection. Using a simple Bag-of-Words model built on top of the learned
tokens, our system consistently outperforms both FACS-based pipelines and
strong image and video representation learning models such as Masked
Autoencoders. Further analysis reveals that our representation covers a wider
variety of facial displays, highlighting its potential as a scalable and
effective alternative to FACS for psychological and affective computing
applications.

</details>


### [90] [Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale](https://arxiv.org/abs/2510.01665)
*Yongbo Chen,Yanhao Zhang,Shaifali Parashar,Liang Zhao,Shoudong Huang*

Main category: cs.CV

TL;DR: 本文提出Con-NRSfM，一种针对共形形变下非刚性结构光恢复（NRSfM）的新方法。它通过图优化和自监督学习，克服了传统方法的局限，能准确估计共形尺度和深度，生成密集3D点云，并在精度和鲁棒性上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 单目视觉可变形SLAM中的映射挑战日益受到关注，NRSfM是关键技术。然而，现有NRSfM方法通常依赖于严格假设（如局部平面或局部线性形变），无法有效恢复共形尺度，且深度与共形尺度耦合，导致重建精度受限。

Method: 该方法名为Con-NRSfM，通过图优化框架对2D图像形变进行逐点重建。它消除了传统方法的严格假设，能准确计算局部共形尺度。框架解耦了深度和共形尺度约束，并采用并行可分离迭代优化策略。此外，还融入了基于编解码网络的自监督学习框架，以生成带纹理的密集3D点云。

Result: 在合成和真实数据集上的仿真与实验结果表明，Con-NRSfM在重建精度和鲁棒性方面均优于现有方法。

Conclusion: Con-NRSfM为共形形变下的NRSfM提供了一种新颖、高效且鲁棒的解决方案，通过精确恢复共形尺度和解耦深度约束，显著提升了重建质量，超越了现有技术水平。

Abstract: Non-rigid structure-from-motion (NRSfM), a promising technique for addressing
the mapping challenges in monocular visual deformable simultaneous localization
and mapping (SLAM), has attracted growing attention. We introduce a novel
method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing
isometric deformations as a subset. Our approach performs point-wise
reconstruction using 2D selected image warps optimized through a graph-based
framework. Unlike existing methods that rely on strict assumptions, such as
locally planar surfaces or locally linear deformations, and fail to recover the
conformal scale, our method eliminates these constraints and accurately
computes the local conformal scale. Additionally, our framework decouples
constraints on depth and conformal scale, which are inseparable in other
approaches, enabling more precise depth estimation. To address the sensitivity
of the formulated problem, we employ a parallel separable iterative
optimization strategy. Furthermore, a self-supervised learning framework,
utilizing an encoder-decoder network, is incorporated to generate dense 3D
point clouds with texture. Simulation and experimental results using both
synthetic and real datasets demonstrate that our method surpasses existing
approaches in terms of reconstruction accuracy and robustness. The code for the
proposed method will be made publicly available on the project website:
https://sites.google.com/view/con-nrsfm.

</details>


### [91] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: 本文提出UniVerse框架，旨在解决从不一致多视图图像进行鲁棒3D场景重建的挑战。它通过将鲁棒重建解耦为图像修复和3D重建，并利用视频扩散模型将不一致图像修复为一致图像，从而实现高效、泛化能力强且可控制风格的鲁棒重建。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒重建方法尝试将图像退化建模集成到神经3D场景表示中，但它们严重依赖密集观测来优化模型参数，导致优化过程复杂。

Method: 本文提出UniVerse，一个基于视频扩散模型的统一鲁棒重建框架。具体方法包括：1) 将不一致图像转换为初始视频；2) 使用专门设计的视频扩散模型将视频修复为一致图像；3) 从这些修复后的图像重建3D场景。该方法利用扩散模型从大规模数据中学习通用场景先验，以应对多样化的图像不一致性。

Result: 在合成和真实世界数据集上的大量实验表明，UniVerse在鲁棒重建方面展现出强大的泛化能力和卓越的性能。此外，UniVerse还能控制重建3D场景的风格。

Conclusion: UniVerse通过解耦鲁棒重建任务并利用视频扩散模型学习通用场景先验，有效解决了从不一致多视图图像进行3D场景重建的挑战，显著提升了重建的泛化能力和性能，并提供了场景风格控制功能。

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of
reconstructing a 3D scene from a set of inconsistent multi-view images. Some
recent works have attempted to simultaneously remove image inconsistencies and
perform reconstruction by integrating image degradation modeling into neural 3D
scene representations.However, these methods rely heavily on dense observations
for robustly optimizing model parameters.To address this issue, we propose to
decouple robust reconstruction into two subtasks: restoration and
reconstruction, which naturally simplifies the optimization process.To this
end, we introduce UniVerse, a unified framework for robust reconstruction based
on a video diffusion model. Specifically, UniVerse first converts inconsistent
images into initial videos, then uses a specially designed video diffusion
model to restore them into consistent images, and finally reconstructs the 3D
scenes from these restored images.Compared with case-by-case per-view
degradation modeling, the diffusion model learns a general scene prior from
large-scale data, making it applicable to diverse image
inconsistencies.Extensive experiments on both synthetic and real-world datasets
demonstrate the strong generalization capability and superior performance of
our method in robust reconstruction. Moreover, UniVerse can control the style
of the reconstructed 3D scene. Project page:
https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [92] [An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution](https://arxiv.org/abs/2510.01678)
*Ke Jia,Ji Zhou,Hanxin Li,Zhigan Zhou,Haojie Chu,Xiaojie Li*

Main category: cs.CV

TL;DR: 该论文提出了一个轻量级端到端框架，将模板匹配重新定义为联合定位和几何回归，可高效估计目标的位置、旋转和缩放。它引入了模板感知动态卷积模块（TDCM）和无需几何标注的训练策略，在复杂背景下实现了高精度和快速推理，适用于实时工业应用。


<details>
  <summary>Details</summary>
Motivation: 工业检测和组件对齐任务中，模板匹配需要在复杂背景下高效估计目标的位置和几何状态（旋转和缩放）以支持精确的下游操作。传统方法依赖穷举枚举角度和尺度，导致在复合变换下效率低下。大多数深度学习方法仅估计相似度分数，未能明确建模几何姿态，不适用于实际部署。

Method: 本文提出了一个轻量级端到端框架，将模板匹配重新定义为联合定位和几何回归，输出中心坐标、旋转角度以及独立的水平和垂直尺度。核心方法包括：1. 模板感知动态卷积模块（TDCM），在推理时动态注入模板特征以指导泛化匹配。2. 紧凑型网络，集成了深度可分离卷积和像素混洗以实现高效匹配。3. 引入基于旋转剪切的增强策略与结构感知伪标签，以实现无需几何标注的训练。4. 轻量级细化模块通过局部优化进一步提高角度和尺度的精度。

Result: 所提出的3.07M模型在复合变换下实现了高精度和14毫秒的推理时间。它在小模板和多对象场景中也表现出强大的鲁棒性。

Conclusion: 该框架非常适合部署在实时工业应用中，因为它具有高精度、快速推理和在复杂场景下的强大鲁棒性，有效解决了传统方法效率低和深度学习方法缺乏几何姿态建模的问题。

Abstract: In industrial inspection and component alignment tasks, template matching
requires efficient estimation of a target's position and geometric state
(rotation and scaling) under complex backgrounds to support precise downstream
operations. Traditional methods rely on exhaustive enumeration of angles and
scales, leading to low efficiency under compound transformations. Meanwhile,
most deep learning-based approaches only estimate similarity scores without
explicitly modeling geometric pose, making them inadequate for real-world
deployment. To overcome these limitations, we propose a lightweight end-to-end
framework that reformulates template matching as joint localization and
geometric regression, outputting the center coordinates, rotation angle, and
independent horizontal and vertical scales. A Template-Aware Dynamic
Convolution Module (TDCM) dynamically injects template features at inference to
guide generalizable matching. The compact network integrates depthwise
separable convolutions and pixel shuffle for efficient matching. To enable
geometric-annotation-free training, we introduce a rotation-shear-based
augmentation strategy with structure-aware pseudo labels. A lightweight
refinement module further improves angle and scale precision via local
optimization. Experiments show our 3.07M model achieves high precision and 14ms
inference under compound transformations. It also demonstrates strong
robustness in small-template and multi-object scenarios, making it highly
suitable for deployment in real-time industrial applications. The code is
available at:https://github.com/ZhouJ6610/PoseMatch-TDCM.

</details>


### [93] [Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning](https://arxiv.org/abs/2510.01681)
*Xuchen Li,Xuzhao Li,Jiahui Gao,Renjie Pi,Shiyu Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出一个自适应像素推理框架，通过结合操作感知微调和强化学习，使视觉语言模型（VLM）能根据查询难度动态决定何时调用像素级操作，从而在处理细粒度视觉任务时显著提高性能并大幅减少不必要的视觉操作。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在需要精确理解和处理细粒度视觉元素的任务上表现不佳，这主要是由于图像编码过程中的信息丢失或对关键区域关注不足。现有引入像素级信息的方法虽有前景，但常过度使用，导致效率低下并受无关视觉细节干扰。

Method: 我们提出了首个自适应像素推理框架。具体来说，首先应用操作感知监督微调（operation-aware supervised fine-tuning）以建立模型在文本推理和视觉操作方面的基础能力。然后，设计一个新颖的rollout引导强化学习框架，该框架依赖于模型自身响应的反馈，使VLM能够根据查询难度动态决定何时调用像素操作。

Result: 在广泛的多模态推理基准测试中，我们的模型取得了卓越的性能，并显著减少了不必要的视觉操作。值得注意的是，模型在HR-Bench 4K上实现了73.4%的准确率，同时工具使用率仅为20.1%，与现有方法相比，准确率更高，且工具使用率降低了66.5%。

Conclusion: 本研究提出的自适应像素推理框架成功解决了VLM在细粒度视觉任务中的挑战，通过智能地动态调用像素操作，不仅显著提升了模型性能和准确性，还大幅降低了资源消耗和不必要的视觉操作，展示出更高的效率和有效性。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet they
frequently struggle with tasks requiring precise understanding and handling of
fine-grained visual elements. This is mainly due to information loss during
image encoding or insufficient attention to critical regions. Recent work has
shown promise by incorporating pixel-level visual information into the
reasoning process, enabling VLMs to access high-resolution visual details
during their thought process. However, this pixel-level information is often
overused, leading to inefficiency and distraction from irrelevant visual
details. To address these challenges, we propose the first framework for
adaptive pixel reasoning that dynamically determines necessary pixel-level
operations based on the input query. Specifically, we first apply
operation-aware supervised fine-tuning to establish baseline competence in
textual reasoning and visual operations, then design a novel rollout-guided
reinforcement learning framework relying on feedback of the model's own
responses, which enables the VLM to determine when pixel operations should be
invoked based on query difficulty. Experiments on extensive multimodal
reasoning benchmarks show that our model achieves superior performance while
significantly reducing unnecessary visual operations. Impressively, our model
achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of
only 20.1\%, improving accuracy and simultaneously reducing tool usage by
66.5\% compared to the previous methods.

</details>


### [94] [Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring](https://arxiv.org/abs/2510.01683)
*Han-Jay Shu,Wei-Ning Chiu,Shun-Ting Chang,Meng-Ping Huang,Takeshi Tohyama,Ahram Han,Po-Chih Kuo*

Main category: cs.CV

TL;DR: ASRS是一种基于数据增强敏感性和嵌入位移的框架，用于识别CXR图像中易出错的病例，从而提升医疗AI的公平性和安全性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在CXR解读中表现出色，但仍存在公平性和可靠性问题，模型在不同患者亚组间准确性不均。现有错误检测方法难以处理细微的内部分布错误，而基于一致性的方法在医学影像领域尚未充分探索。

Method: 提出一种增强敏感性风险评分（ASRS）框架来识别易出错的CXR病例。ASRS对图像应用临床上合理的旋转（±15°/±30°），并使用RAD-DINO编码器测量嵌入位移，通过敏感性评分将样本分层。

Result: 高度敏感的病例（由ASRS识别）显示出显著较低的召回率（-0.2到-0.3），尽管其AUROC和置信度可能很高。

Conclusion: ASRS提供了一种无需标签的方法，用于选择性预测和临床医生审查，从而提高了医疗AI的公平性和安全性。

Abstract: Deep learning models achieve strong performance in chest radiograph (CXR)
interpretation, yet fairness and reliability concerns persist. Models often
show uneven accuracy across patient subgroups, leading to hidden failures not
reflected in aggregate metrics. Existing error detection approaches -- based on
confidence calibration or out-of-distribution (OOD) detection -- struggle with
subtle within-distribution errors, while image- and representation-level
consistency-based methods remain underexplored in medical imaging. We propose
an augmentation-sensitivity risk scoring (ASRS) framework to identify
error-prone CXR cases. ASRS applies clinically plausible rotations ($\pm
15^\circ$/$\pm 30^\circ$) and measures embedding shifts with the RAD-DINO
encoder. Sensitivity scores stratify samples into stability quartiles, where
highly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)
despite high AUROC and confidence. ASRS provides a label-free means for
selective prediction and clinician review, improving fairness and safety in
medical AI.

</details>


### [95] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: 本文提出FreeViS，一个免训练的视频风格化框架，通过集成多个风格化参考到预训练I2V模型，并结合高频补偿和流基运动线索，实现高保真和强时间一致性的视频风格化。


<details>
  <summary>Details</summary>
Motivation: 视频风格化面临挑战：逐帧应用图像风格化会损害时间一致性和减少风格丰富度；训练专用视频风格化模型需要配对视频数据且计算成本高昂。

Method: FreeViS将多个风格化参考集成到一个预训练的图像到视频（I2V）模型中，以减轻传播误差。它利用高频补偿来约束内容布局和运动，并结合基于光流的运动线索来保留低显著区域的风格纹理。该方法是免训练的。

Result: FreeViS实现了更高的风格化保真度和卓越的时间一致性，优于现有基线方法，并获得了强烈的人类偏好。

Conclusion: FreeViS提供了一个实用且经济的免训练解决方案，用于生成高质量、时间连贯的视频风格化内容。

Abstract: Video stylization plays a key role in content creation, but it remains a
challenging problem. Na\"ively applying image stylization frame-by-frame hurts
temporal consistency and reduces style richness. Alternatively, training a
dedicated video stylization model typically requires paired video data and is
computationally expensive. In this paper, we propose FreeViS, a training-free
video stylization framework that generates stylized videos with rich style
details and strong temporal coherence. Our method integrates multiple stylized
references to a pretrained image-to-video (I2V) model, effectively mitigating
the propagation errors observed in prior works, without introducing flickers
and stutters. In addition, it leverages high-frequency compensation to
constrain the content layout and motion, together with flow-based motion cues
to preserve style textures in low-saliency regions. Through extensive
evaluations, FreeViS delivers higher stylization fidelity and superior temporal
consistency, outperforming recent baselines and achieving strong human
preference. Our training-free pipeline offers a practical and economic solution
for high-quality, temporally coherent video stylization. The code and videos
can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [96] [MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs](https://arxiv.org/abs/2510.01691)
*Jiyao Liu,Jinjie Wei,Wanying Qu,Chenglong Ma,Junzhi Ning,Yunheng Li,Ying Chen,Xinzhe Luo,Pengcheng Chen,Xin Gao,Ming Hu,Huihui Xu,Xin Wang,Shujian Gao,Dingkang Yang,Zhongying Deng,Jin Ye,Lihao Liu,Junjun He,Ningsheng Xu*

Main category: cs.CV

TL;DR: 本文提出MedQ-Bench基准，旨在通过多模态大语言模型（MLLMs）实现医学图像质量的语言化感知-推理评估，以弥补现有方法的不足。评估发现，当前MLLMs在此任务上表现初步但不稳定，需要进一步优化。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像质量评估（IQA）方法局限于标量、分数指标，无法反映专家评估中描述性的、类人推理过程，限制了其在临床AI中的应用潜力。

Method: 引入MedQ-Bench基准，建立MLLMs的语言化评估范式。该基准包含MedQ-Perception（探测低级感知能力）和MedQ-Reasoning（无参考和比较推理）两类任务，涵盖五种成像模态、四十多种质量属性，共2600个感知查询和708个推理评估，数据来源多样。采用多维度判别协议评估推理能力，并进行人-AI一致性验证。

Result: 对14个SOTA MLLMs的评估显示，模型展现出初步但不稳定的感知和推理能力，准确性尚不足以满足可靠的临床应用需求。

Conclusion: 现有MLLMs在医学图像质量评估方面仍处于发展初期，性能初步且不稳定，迫切需要针对性优化。MedQ-Bench有望推动该领域的深入研究，并挖掘MLLMs在医学图像质量评估中的巨大潜力。

Abstract: Medical Image Quality Assessment (IQA) serves as the first-mile safety gate
for clinical AI, yet existing approaches remain constrained by scalar,
score-based metrics and fail to reflect the descriptive, human-like reasoning
process central to expert evaluation. To address this gap, we introduce
MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning
paradigm for language-based evaluation of medical image quality with
Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary
tasks: (1) MedQ-Perception, which probes low-level perceptual capability via
human-curated questions on fundamental visual attributes; and (2)
MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,
aligning model evaluation with human-like reasoning on image quality. The
benchmark spans five imaging modalities and over forty quality attributes,
totaling 2,600 perceptual queries and 708 reasoning assessments, covering
diverse image sources including authentic clinical acquisitions, images with
simulated degradations via physics-based reconstructions, and AI-generated
images. To evaluate reasoning ability, we propose a multi-dimensional judging
protocol that assesses model outputs along four complementary axes. We further
conduct rigorous human-AI alignment validation by comparing LLM-based judgement
with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates
that models exhibit preliminary but unstable perceptual and reasoning skills,
with insufficient accuracy for reliable clinical use. These findings highlight
the need for targeted optimization of MLLMs in medical IQA. We hope that
MedQ-Bench will catalyze further exploration and unlock the untapped potential
of MLLMs for medical image quality evaluation.

</details>


### [97] [Holistic Order Prediction in Natural Scenes](https://arxiv.org/abs/2510.01704)
*Pierre Musacchio,Hyunmin Lee,Jaesik Park*

Main category: cs.CV

TL;DR: 本文提出了InstaFormer网络，仅通过一张RGB图像，即可在单次前向传播中预测场景中所有实例的完整遮挡和深度顺序，解决了现有方法在实例几何理解方面成本高昂且效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 理解实例级别的几何结构对于多种视觉模型来说极具挑战性。现有专业系统依赖昂贵的输入格式（如类别标签、二值分割掩码）和高昂的推理成本（需要二次方的正向传播次数）。

Method: 本文提出InstaFormer网络以缓解上述限制，该网络能够进行整体顺序预测。其核心在于利用对象查询（object queries）与潜在掩码描述符（latent mask descriptors）之间的交互，这些描述符语义上代表同一对象并携带互补信息。

Result: 通过全面的基准测试和消融实验，证明了所提出方法的有效性。

Conclusion: InstaFormer提供了一种高效且准确的解决方案，用于理解场景中所有实例的完整遮挡和深度顺序，克服了现有视觉模型在输入和推理成本上的限制。

Abstract: Even in controlled settings, understanding instance-wise geometries is a
challenging task for a wide range of visual models. Although specialized
systems exist, modern arts rely on expensive input formats (category labels,
binary segmentation masks) and inference costs (a quadratic amount of forward
passes). We mitigate these limitations by proposing InstaFormer, a network
capable of holistic order prediction. That is, solely given an input RGB image,
InstaFormer returns the full occlusion and depth orderings for all the
instances in the scene in a single forward pass. At its core, InstaFormer
relies on interactions between object queries and latent mask descriptors that
semantically represent the same objects while carrying complementary
information. We comprehensively benchmark and ablate our approach to highlight
its effectiveness. Our code and models are open-source and available at this
URL: https://github.com/SNU-VGILab/InstaOrder.

</details>


### [98] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: PyramidStyler是一个基于Transformer的神经风格迁移框架，通过引入金字塔位置编码和强化学习，有效解决了高分辨率和复杂风格下的效率挑战，实现了实时、高质量的艺术渲染。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN和Transformer的神经风格迁移模型在处理复杂风格和高分辨率输入时，难以高效扩展和保持效率。

Method: 引入了PyramidStyler，一个结合金字塔位置编码（PPE）的Transformer框架，PPE能捕获局部细节和全局上下文并降低计算负荷。此外，整合了强化学习以动态优化风格化过程，加速收敛。

Result: 经4000个epoch训练后，PyramidStyler将内容损失降低了62.6%（至2.07），风格损失降低了57.4%（至0.86），推理时间为1.39秒。结合强化学习后，性能进一步提升（内容损失2.03，风格损失0.75），且速度损失极小（1.40秒）。

Conclusion: 该方法实现了实时、高质量的艺术渲染，在媒体和设计领域具有广泛应用潜力。

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based
algorithm, enabling AI-driven artistic image synthesis. However, existing CNN
and transformer-based models struggle to scale efficiently to complex styles
and high-resolution inputs. We introduce PyramidStyler, a transformer framework
with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding
that captures both local details and global context while reducing
computational load. We further incorporate reinforcement learning to
dynamically optimize stylization, accelerating convergence. Trained on
Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to
2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s
inference--and yields further improvements (content 2.03; style 0.75) with
minimal speed penalty (1.40 s) when using RL. These results demonstrate
real-time, high-quality artistic rendering, with broad applications in media
and design.

</details>


### [99] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: LoBE-GS是一个负载均衡且高效的3D高斯泼溅框架，通过深度感知分区和优化策略解决了大规模场景下3DGS的现有瓶颈，实现了高达2倍的训练速度提升和更好的可扩展性，同时保持了重建质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅(3DGS)难以扩展到大型无界场景。现有分而治之方法存在两大瓶颈：1) 分区负载严重不平衡；2) 粗到细流水线效率低下，导致高开销和模型重载。

Method: 本文提出LoBE-GS框架，重构大规模3DGS流水线。其核心方法包括：1) 深度感知分区方法，将预处理时间从数小时缩短至数分钟；2) 基于优化的策略，平衡各块之间的可见高斯数量（计算负载的强代理）；3) 引入可见性裁剪和选择性稠密化两个轻量级技术，进一步降低训练成本。

Result: 在大型城市和户外数据集上评估显示，LoBE-GS比最先进的基线实现了高达2倍的端到端训练时间加速，同时保持了重建质量，并使原生3DGS无法处理的场景具备了可扩展性。

Conclusion: LoBE-GS通过创新的分区和优化策略，有效解决了大规模3DGS的效率和可扩展性问题，显著提升了训练速度和处理能力，使其适用于更广阔的复杂场景。

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.

</details>


### [100] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 该论文提出了MemoryPack和Direct Forcing两种方法，分别解决长视频生成中长程依赖建模和自回归解码错误累积的问题，显著提升了生成视频的上下文一致性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 长视频生成面临两大挑战：模型难以捕捉长程依赖关系，以及自回归解码过程中固有的错误累积问题。

Method: 1. **MemoryPack**: 一种可学习的上下文检索机制，利用文本和图像信息作为全局指导，联合建模短程和长程依赖，实现了分钟级的时间一致性，并保持计算效率和线性复杂度。2. **Direct Forcing**: 一种高效的单步近似策略，旨在改善训练与推理的对齐，从而有效抑制推理过程中的错误传播。

Result: MemoryPack和Direct Forcing的结合显著增强了长视频生成的上下文一致性和可靠性。

Conclusion: 这些贡献提升了自回归视频模型的实际可用性。

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [101] [Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving](https://arxiv.org/abs/2510.01829)
*Cornelius Schröder,Marius-Raphael Schlüter,Markus Lienkamp*

Main category: cs.CV

TL;DR: 本文提出了一种新的损失项和度量方法，用于改善3D目标检测器（特别是CenterPoint和PillarNet）的置信度校准，关注完整预测分布的校准，并发现DSVT-Pillar的校准特性不同。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶系统中，精确的目标检测和不确定性估计对系统的安全运行至关重要。本研究旨在解决3D目标检测器分类任务中的置信度校准问题。

Method: 本文提出需关注所有类别的完整预测置信度分布的校准，并据此推导了一个能捕捉主导和次要类别预测校准的度量标准。同时，提出了两个辅助正则化损失项，分别以校准主导预测或完整预测向量为训练目标。研究评估了CenterPoint、PillarNet和DSVT-Pillar模型上的一系列事后和训练时校准方法。

Result: 研究发现，结合提出的用于完整类别预测校准的损失项与等渗回归，可在CenterPoint和PillarNet模型上，就主导和次要类别预测而言，实现最佳的校准效果。此外，研究还发现DSVT-Pillar不能用相同的方法同时校准主导和次要预测。

Conclusion: 通过引入关注完整预测分布的损失项和度量标准，显著提高了CenterPoint和PillarNet等3D目标检测器的置信度校准性能。同时揭示了不同模型（如DSVT-Pillar）在共同校准主导和次要预测方面的差异性。

Abstract: In autonomous systems, precise object detection and uncertainty estimation
are critical for self-aware and safe operation. This work addresses confidence
calibration for the classification task of 3D object detectors. We argue that
it is necessary to regard the calibration of the full predictive confidence
distribution over all classes and deduce a metric which captures the
calibration of dominant and secondary class predictions. We propose two
auxiliary regularizing loss terms which introduce either calibration of the
dominant prediction or the full prediction vector as a training goal. We
evaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet
and DSVT-Pillar and find that combining our loss term, which regularizes for
calibration of the full class prediction, and isotonic regression lead to the
best calibration of CenterPoint and PillarNet with respect to both dominant and
secondary class predictions. We further find that DSVT-Pillar can not be
jointly calibrated for dominant and secondary predictions using the same
method.

</details>


### [102] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: 提出DiffPS框架，利用预训练扩散模型解决行人搜索中的特征表示和优化冲突问题，并在两个基准数据集上实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有行人搜索方法依赖ImageNet预训练骨干网络，对复杂上下文和细粒度身份线索捕捉不足；检测与重识别任务共享特征导致优化目标冲突，特征次优。

Method: 提出DiffPS框架，利用预训练扩散模型并消除子任务优化冲突。包含：1) 扩散引导区域提议网络(DGRPN)增强定位；2) 多尺度频率细化网络(MSFRN)减轻形状偏差；3) 语义自适应特征聚合网络(SFAN)利用文本对齐扩散特征。

Result: DiffPS在CUHK-SYSU和PRW数据集上均取得了新的最先进（SOTA）性能。

Conclusion: 通过利用扩散模型先验知识并解决子任务优化冲突，DiffPS为行人搜索提供了一个更优的解决方案，显著提升了定位和识别能力。

Abstract: Person search aims to jointly perform person detection and re-identification
by localizing and identifying a query person within a gallery of uncropped
scene images. Existing methods predominantly utilize ImageNet pre-trained
backbones, which may be suboptimal for capturing the complex spatial context
and fine-grained identity cues necessary for person search. Moreover, they rely
on a shared backbone feature for both person detection and re-identification,
leading to suboptimal features due to conflicting optimization objectives. In
this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a
novel framework that leverages a pre-trained diffusion model while eliminating
the optimization conflict between two sub-tasks. We analyze key properties of
diffusion priors and propose three specialized modules: (i) Diffusion-Guided
Region Proposal Network (DGRPN) for enhanced person localization, (ii)
Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and
(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage
text-aligned diffusion features. DiffPS sets a new state-of-the-art on
CUHK-SYSU and PRW.

</details>


### [103] [Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2510.01912)
*Yi Ai,Yuanhao Cai,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: 该论文提出了Flow-Matching-guided Unfolding网络（FMU），首次将流匹配生成先验融入深度展开框架，并引入均值速度损失以提高高光谱图像重建质量，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像（HSI）采集成本高，且从压缩测量中重建三维数据困难。现有压缩感知系统（如CASSI）重建时，面临严重降级和精细光谱细节丢失的挑战。

Method: 提出Flow-Matching-guided Unfolding网络（FMU），将流匹配的生成先验嵌入深度展开框架。为强化学习动态，引入均值速度损失以强制流的全局一致性。该混合设计结合了基于优化的可解释性和流匹配的生成能力。

Result: 在模拟和真实数据集上的大量实验表明，FMU在重建质量上显著优于现有方法。

Conclusion: FMU通过结合流匹配与深度展开框架，并引入均值速度损失，有效解决了高光谱图像重建中的降级和细节丢失问题，实现了更稳健和准确的重建。

Abstract: Hyperspectral imaging (HSI) provides rich spatial-spectral information but
remains costly to acquire due to hardware limitations and the difficulty of
reconstructing three-dimensional data from compressed measurements. Although
compressive sensing systems such as CASSI improve efficiency, accurate
reconstruction is still challenged by severe degradation and loss of fine
spectral details. We propose the Flow-Matching-guided Unfolding network (FMU),
which, to our knowledge, is the first to integrate flow matching into HSI
reconstruction by embedding its generative prior within a deep unfolding
framework. To further strengthen the learned dynamics, we introduce a mean
velocity loss that enforces global consistency of the flow, leading to a more
robust and accurate reconstruction. This hybrid design leverages the
interpretability of optimization-based methods and the generative capacity of
flow matching. Extensive experiments on both simulated and real datasets show
that FMU significantly outperforms existing approaches in reconstruction
quality. Code and models will be available at https://github.com/YiAi03/FMU.

</details>


### [104] [Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models](https://arxiv.org/abs/2510.01914)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Yen-Ting Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于数字相机光学和深度学习（YOLOv7结合ConSinGAN）的自动化DIP组件缺陷检测系统，并通过SCADA系统实现管理，有效解决了传统检测耗时和缺陷数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统工业元件的缺陷检测耗时费力，给质检人员带来巨大负担，且难以有效管理产品质量。此外，缺陷组件图像的缺乏是检测任务面临的挑战。

Method: 研究采用数字相机光学和基于深度学习的模型，针对DIP组件的表面缺陷和引脚缺陷进行检测。为解决缺陷图像不足问题，使用ConSinGAN生成合适大小的数据集进行训练和测试。对比了四种YOLO模型（v3, v4, v7, v9）单独及结合ConSinGAN增强的效果。同时开发了SCADA系统并描述了相关传感器架构。

Result: 结合ConSinGAN的YOLOv7模型表现最佳，检测准确率达95.50%，检测时间为285毫秒，显著优于基于阈值的方法。

Conclusion: 所提出的自动化缺陷检测系统易于建立，可处理多种缺陷类型或数据不足的情况，有望减轻质检负担并提升产品质量管理效率。

Abstract: Since the defect detection of conventional industry components is
time-consuming and labor-intensive, it leads to a significant burden on quality
inspection personnel and makes it difficult to manage product quality. In this
paper, we propose an automated defect detection system for the dual in-line
package (DIP) that is widely used in industry, using digital camera optics and
a deep learning (DL)-based model. The two most common defect categories of DIP
are examined: (1) surface defects, and (2) pin-leg defects. However, the lack
of defective component images leads to a challenge for detection tasks. To
solve this problem, the ConSinGAN is used to generate a suitable-sized dataset
for training and testing. Four varieties of the YOLO model are investigated
(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.
The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in
accuracy of 95.50\%, detection time of 285 ms, and is far superior to
threshold-based approaches. In addition, the supervisory control and data
acquisition (SCADA) system is developed, and the associated sensor architecture
is described. The proposed automated defect detection can be easily established
with numerous types of defects or insufficient defect data.

</details>


### [105] [Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors](https://arxiv.org/abs/2510.01934)
*Guangyao Zhai,Yue Zhou,Xinyan Deng,Lars Heckler,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: 提出FoundAD，一种利用基础视觉编码器和非线性投影操作器进行少样本异常检测的方法，通过学习自然图像流形来识别图像中的异常区域。


<details>
  <summary>Details</summary>
Motivation: 少样本异常检测在工业安全检查中具有简化作用，但样本受限和类别无关条件下的准确区分异常特征极具挑战性。基础视觉编码器的大规模预训练有助于学习正常图像的通用分布。

Method: 观察到图像中的异常程度与学习到的嵌入差异直接相关。通过学习一个非线性投影操作器到自然图像流形上，设计了FoundAD。该操作器作为有效的异常检测工具，用于表征和识别图像中的分布外区域。

Result: 广泛的实验表明，该方法支持多类别检测，取得了有竞争力的性能，且比现有方法使用的参数少得多。通过包括DINOv3在内的多个基础编码器进行评估，验证了其有效性。

Conclusion: 该研究拓宽了对基础特征的视角，并推动了少样本异常检测领域的发展。

Abstract: Few-shot anomaly detection streamlines and simplifies industrial safety
inspection. However, limited samples make accurate differentiation between
normal and abnormal features challenging, and even more so under
category-agnostic conditions. Large-scale pre-training of foundation visual
encoders has advanced many fields, as the enormous quantity of data helps to
learn the general distribution of normal images. We observe that the anomaly
amount in an image directly correlates with the difference in the learnt
embeddings and utilize this to design a few-shot anomaly detector termed
FoundAD. This is done by learning a nonlinear projection operator onto the
natural image manifold. The simple operator acts as an effective tool for
anomaly detection to characterize and identify out-of-distribution regions in
an image. Extensive experiments show that our approach supports multi-class
detection and achieves competitive performance while using substantially fewer
parameters than prior methods. Backed up by evaluations with multiple
foundation encoders, including fresh DINOv3, we believe this idea broadens the
perspective on foundation features and advances the field of few-shot anomaly
detection.

</details>


### [106] [ClustViT: Clustering-based Token Merging for Semantic Segmentation](https://arxiv.org/abs/2510.01948)
*Fabio Montello,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: ClustViT提出一种新的Vision Transformer架构，通过可训练的聚类模块合并相似token并用再生器模块恢复细节，显著降低语义分割任务的计算复杂度，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers虽然精度高且泛化能力强，但其二次注意力复杂度限制了在真实机器人系统上的实际应用。现有的token合并方法适用于分类，但对语义分割等密集预测任务效果不佳。

Method: 我们提出了ClustViT，扩展了Vision Transformer (ViT) 主干网络来解决语义分割问题。该架构包含一个可训练的聚类模块，根据分割mask的伪聚类指导，沿网络合并相似的token。随后，一个再生器模块用于恢复下游头部的精细细节。

Result: 在三个不同数据集上，ClustViT实现了高达2.18倍的GFLOPs减少和1.64倍的推理速度提升，同时保持了可比的分割精度。

Conclusion: ClustViT通过创新的token合并和细节恢复机制，成功解决了Vision Transformers在语义分割中的计算效率瓶颈，使其在实际应用中更具可行性，且不牺牲性能。

Abstract: Vision Transformers can achieve high accuracy and strong generalization
across various contexts, but their practical applicability on real-world
robotic systems is limited due to their quadratic attention complexity. Recent
works have focused on dynamically merging tokens according to the image
complexity. Token merging works well for classification but is less suited to
dense prediction. We propose ClustViT, where we expand upon the Vision
Transformer (ViT) backbone and address semantic segmentation. Within our
architecture, a trainable Cluster module merges similar tokens along the
network guided by pseudo-clusters from segmentation masks. Subsequently, a
Regenerator module restores fine details for downstream heads. Our approach
achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different
datasets, with comparable segmentation accuracy. Our code and models will be
made publicly available.

</details>


### [107] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: PaDT是一种统一范式，通过引入视觉参考令牌（VRTs），使多模态大语言模型（MLLMs）能直接生成文本和多种视觉输出，并在多项视觉任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在视觉任务中依赖间接表示（如生成坐标文本），这限制了性能并阻碍了密集预测任务（如分割）。

Method: 引入Patch-as-Decodable Token (PaDT) 范式，核心是Visual Reference Tokens (VRTs)。VRTs源自图像块嵌入并与LLM文本输出交错。一个轻量级解码器将LLM输出转换为检测、分割和grounding预测。PaDT独立处理VRTs并动态扩展嵌入表以提高定位和区分。采用随机选择VRTs的监督微调和per-token交叉熵损失的训练策略。

Result: 在四项视觉感知和理解任务上的实证研究表明，PaDT持续实现了最先进（state-of-the-art）的性能，甚至优于显著更大的MLLM模型。

Conclusion: PaDT通过直接处理和生成多样的视觉输出，显著提升了MLLMs在多种视觉任务上的性能，超越了现有方法和大型模型，为未来的多模态研究提供了新方向。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [108] [TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading](https://arxiv.org/abs/2510.01990)
*Jianfei Xie,Ziyang Li*

Main category: cs.CV

TL;DR: 针对生鲜电商信任缺失问题，本文提出了TriAlignXA可解释AI框架，通过生物适应、时效和经济优化引擎，结合预映射机制，有效平衡农产品分级中的“不可能三角”，显著提升分级准确性，旨在建立可信赖的在线农产品生态系统。


<details>
  <summary>Details</summary>
Motivation: 线上果蔬电商交易因无法提供直接感官体验而面临“信任赤字”。此外，农产品分级存在生物特性、时效性和经济可行性之间的“不可能三角”，传统绝对分级标准具有局限性。

Method: 构建了“信任金字塔”模型和“三角信任指数”（TTI）。提出并设计了可解释AI框架——TriAlignXA，将算法定义为“透明决策依据的提供者”。该框架核心包含生物适应引擎、时效优化引擎和经济优化引擎，通过多目标优化来支持可信交易。此外，利用“预映射机制”将过程数据编码为QR码以透明化质量信息。

Result: 实验证实质量是信任的基石。在分级任务中，TriAlignXA框架的准确性显著高于基线模型。实证证据和理论分析验证了该框架在解决“不可能三角”问题上的平衡能力。

Conclusion: 本研究为构建可信赖的在线生鲜产品生态系统提供了从理论到实践的全面支持，建立了从算法决策到消费者信任的关键路径。

Abstract: The 'trust deficit' in online fruit and vegetable e-commerce stems from the
inability of digital transactions to provide direct sensory perception of
product quality. This paper constructs a 'Trust Pyramid' model through
'dual-source verification' of consumer trust. Experiments confirm that quality
is the cornerstone of trust. The study reveals an 'impossible triangle' in
agricultural product grading, comprising biological characteristics,
timeliness, and economic viability, highlighting the limitations of traditional
absolute grading standards. To quantitatively assess this trade-off, we propose
the 'Triangular Trust Index' (TTI). We redefine the role of algorithms from
'decision-makers' to 'providers of transparent decision-making bases',
designing the explainable AI framework--TriAlignXA. This framework supports
trustworthy online transactions within agricultural constraints through
multi-objective optimization. Its core relies on three engines: the
Bio-Adaptive Engine for granular quality description; the Timeliness
Optimization Engine for processing efficiency; and the Economic Optimization
Engine for cost control. Additionally, the "Pre-Mapping Mechanism" encodes
process data into QR codes, transparently conveying quality information.
Experiments on grading tasks demonstrate significantly higher accuracy than
baseline models. Empirical evidence and theoretical analysis verify the
framework's balancing capability in addressing the "impossible triangle". This
research provides comprehensive support--from theory to practice--for building
a trustworthy online produce ecosystem, establishing a critical pathway from
algorithmic decision-making to consumer trust.

</details>


### [109] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 提出4DGS-Craft框架，通过4D感知模型、高斯选择和LLM模块，解决4D高斯泼溅编辑中的一致性问题和复杂指令理解，实现更一致、可控的4D场景编辑。


<details>
  <summary>Details</summary>
Motivation: 现有4D高斯泼溅（4DGS）编辑面临视图、时间、非编辑区域一致性挑战，且难以处理复杂文本指令。

Method: 1. 引入4D感知InstructPix2Pix模型，整合4D VGGT几何特征和多视图网格模块，确保视图和时间一致性。 2. 通过高斯选择机制，保持非编辑区域的一致性。 3. 设计基于LLM的模块，利用用户指令模板和LLM推理，理解并分解复杂用户指令。

Result: 本框架能解释用户意图，将复杂指令分解为原子操作，从而处理复杂命令并提升编辑性能。相比现有方法，实现了更一致、更可控的4D场景编辑。

Conclusion: 4DGS-Craft通过创新的方法，有效解决了4DGS编辑中的关键一致性问题和复杂指令处理难题，显著提升了4D场景编辑的质量和用户交互体验。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.

</details>


### [110] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: 提出Pure-Pass (PP)，一种像素级掩码机制，通过识别“纯像素”来减少超分辨率模型的计算开销，提高了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 深度学习超分辨率方法计算成本高昂，难以实际部署。现有轻量级方法如CAMixer存在适应性差、掩码粒度粗糙和空间不灵活等局限性。

Method: Pure-Pass (PP) 是一种像素级掩码机制，利用固定的颜色中心点对像素进行分类，识别出“纯像素”并豁免其昂贵的计算。这种机制实现了细粒度、空间灵活且自适应的掩码。

Result: 将PP集成到先进的ATD-light模型中，PP-ATD-light在相似的计算节省下，以最小的额外开销实现了卓越的超分辨率性能，在重建质量和参数效率方面均优于CAMixer-ATD-light。

Conclusion: Pure-Pass (PP) 提供了一种有效的像素级掩码策略，显著提升了轻量级超分辨率模型的性能和效率，克服了现有方法的局限性。

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from
low-resolution counterparts, but the computational complexity of deep
learning-based methods often hinders practical deployment. CAMixer is the
pioneering work to integrate the advantages of existing lightweight SR methods
and proposes a content-aware mixer to route token mixers of varied complexities
according to the difficulty of content recovery. However, several limitations
remain, such as poor adaptability, coarse-grained masking and spatial
inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking
mechanism that identifies pure pixels and exempts them from expensive
computations. PP utilizes fixed color center points to classify pixels into
distinct categories, enabling fine-grained, spatially flexible masking while
maintaining adaptive flexibility. Integrated into the state-of-the-art
ATD-light model, PP-ATD-light achieves superior SR performance with minimal
overhead, outperforming CAMixer-ATD-light in reconstruction quality and
parameter efficiency when saving a similar amount of computation.

</details>


### [111] [Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework](https://arxiv.org/abs/2510.02001)
*Nanaka Hosokawa,Ryo Takahashi,Tomoya Kitano,Yukihiro Iida,Chisako Muramatsu,Tatsuro Hayashi,Yuta Seino,Xiangrong Zhou,Takeshi Hara,Akitoshi Katsumata,Hiroshi Fujita*

Main category: cs.CV

TL;DR: 本研究利用GPT-4o的多模态能力，结合自校正循环结构化输出（SLSO）框架，自动生成牙颌囊肿的影像学报告，并在多个方面展现出优于传统方法的准确性。


<details>
  <summary>Details</summary>
Motivation: 旨在利用GPT-4o的先进能力，自动化并提高牙颌全景X光片上牙颌囊肿诊断报告生成的准确性，以辅助临床诊断。

Method: 开发了自校正循环结构化输出（SLSO）框架，并将其应用于22例牙颌囊肿病例。该框架包含10个步骤，涉及图像输入分析、结构化数据生成、牙齿编号提取与一致性检查、不一致时的迭代再生，以及报告生成与验证。通过七项评估指标（透明度、内部结构、边界、牙根吸收、牙齿移位、与其他结构的关系、牙齿编号），与传统的思维链（CoT）方法进行了比较。

Result: SLSO框架显著提高了多项输出的准确性，其中牙齿编号、牙齿移位和牙根吸收的准确率分别提升了66.9%、33.3%和28.6%。成功案例中，经过最多五次迭代再生后可获得一致的结构化输出。SLSO框架还强制描述阴性结果、抑制幻觉并提高了牙齿编号识别准确性，尽管由于数据集小未能达到统计学意义。然而，识别跨多个牙齿的广泛病变存在局限性。

Conclusion: SLSO框架在利用GPT-4o生成牙颌囊肿报告方面提高了准确性和结构一致性，并能抑制幻觉。尽管存在数据集小和识别广泛病变受限等问题，但仍需进一步完善以实现实际应用。

Abstract: In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to
automatically generate jaw cyst findings on dental panoramic radiographs. To
improve accuracy, we constructed a Self-correction Loop with Structured Output
(SLSO) framework and verified its effectiveness. A 10-step process was
implemented for 22 cases of jaw cysts, including image input and analysis,
structured data generation, tooth number extraction and consistency checking,
iterative regeneration when inconsistencies were detected, and finding
generation with subsequent restructuring and consistency verification. A
comparative experiment was conducted using the conventional Chain-of-Thought
(CoT) method across seven evaluation items: transparency, internal structure,
borders, root resorption, tooth movement, relationships with other structures,
and tooth number. The results showed that the proposed SLSO framework improved
output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates
for tooth number, tooth movement, and root resorption, respectively. In the
successful cases, a consistently structured output was achieved after up to
five regenerations. Although statistical significance was not reached because
of the small size of the dataset, the overall SLSO framework enforced negative
finding descriptions, suppressed hallucinations, and improved tooth number
identification accuracy. However, the accurate identification of extensive
lesions spanning multiple teeth is limited. Nevertheless, further refinement is
required to enhance overall performance and move toward a practical finding
generation system.

</details>


### [112] [LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction](https://arxiv.org/abs/2510.02028)
*Mario Resino,Borja Pérez,Jaime Godoy,Abdulla Al-Kaff,Fernando García*

Main category: cs.CV

TL;DR: 提出LiLa-Net，一种高效3D自编码器，用于从LiDAR点云中提取真实交通环境特征并进行精确重建，兼具泛化能力和资源效率。


<details>
  <summary>Details</summary>
Motivation: 需要在真实交通环境中，从LiDAR点云中高效编码特征，并超越现有先进架构的性能，同时避免资源消耗过大。

Method: 开发了名为LiLa-Net的3D自编码器架构，仅使用LiDAR点云。该模型通过减少编码器层数和简化跳跃连接来优化资源，同时平衡跳跃连接与潜在编码的信息以提升性能。

Result: 成功生成高效且有代表性的潜在空间，能准确重建原始点云。在不牺牲性能的前提下提高了重建质量。模型展示了强大的泛化能力，能成功重建与原始交通环境无关的物体。

Conclusion: LiLa-Net是一种高效且资源优化的3D自编码器，能从LiDAR点云中有效提取特征，实现高精度重建，并具备出色的泛化能力，适用于真实交通环境。

Abstract: This work proposed a 3D autoencoder architecture, named LiLa-Net, which
encodes efficient features from real traffic environments, employing only the
LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,
equipped with Velodyne LiDAR. The system leverage skip connections concept to
improve the performance without using extensive resources as the
state-of-the-art architectures. Key changes include reducing the number of
encoder layers and simplifying the skip connections, while still producing an
efficient and representative latent space which allows to accurately
reconstruct the original point cloud. Furthermore, an effective balance has
been achieved between the information carried by the skip connections and the
latent encoding, leading to improved reconstruction quality without
compromising performance. Finally, the model demonstrates strong generalization
capabilities, successfully reconstructing objects unrelated to the original
traffic environment.

</details>


### [113] [kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring](https://arxiv.org/abs/2510.02030)
*Jenna Kline,Maksim Kholiavchenko,Samuel Stevens,Nina van Tiel,Alison Zhong,Namrata Banerji,Alec Sheets,Sowbaranika Balasubramaniam,Isla Duporge,Matthew Thompson,Elizabeth Campolongo,Jackson Miliko,Neil Rosser,Tanya Berger-Wolf,Charles V. Stewart,Daniel I. Rubenstein*

Main category: cs.CV

TL;DR: 本文提出了kabr-tools，一个结合无人机视频和机器学习的开源工具包，用于自动化、大规模地监测多物种动物行为，克服了传统方法的局限性，并提供了详细的行为生态学分析。


<details>
  <summary>Details</summary>
Motivation: 传统野外观测动物行为耗时、费力且范围有限，难以规模化量化和解释复杂的行为模式，阻碍了对跨景观行为反应的评估。

Method: 开发了kabr-tools（肯尼亚动物行为识别工具），一个开源软件包，集成了无人机视频与机器学习系统，利用目标检测、跟踪和行为分类，自动提取野生动物行为、社会和空间指标，包括时间预算、行为转换、社会互动、栖息地关联和群体组成动态。

Result: 与地面方法相比，无人机观察显著提高了行为粒度，减少了15%的可见性损失，并以更高的准确性和连续性捕获了更多行为转换。通过分析969个行为序列的三个案例研究验证了工具。发现格氏斑马的警戒性随种群规模增加而降低，但栖息地影响微乎其微；平原斑马和格氏斑马表现出强烈的行为惯性，很少转变为警觉行为；在混合物种群中观察到格氏斑马、平原斑马和长颈鹿之间存在空间隔离。

Conclusion: kabr-tools通过实现大规模自动化行为监测，为生态系统范围的研究提供了强大工具，推动了保护、生物多样性研究和生态监测的发展。

Abstract: A comprehensive understanding of animal behavior ecology depends on scalable
approaches to quantify and interpret complex, multidimensional behavioral
patterns. Traditional field observations are often limited in scope,
time-consuming, and labor-intensive, hindering the assessment of behavioral
responses across landscapes. To address this, we present kabr-tools (Kenyan
Animal Behavior Recognition Tools), an open-source package for automated
multi-species behavioral monitoring. This framework integrates drone-based
video with machine learning systems to extract behavioral, social, and spatial
metrics from wildlife footage. Our pipeline leverages object detection,
tracking, and behavioral classification systems to generate key metrics,
including time budgets, behavioral transitions, social interactions, habitat
associations, and group composition dynamics. Compared to ground-based methods,
drone-based observations significantly improved behavioral granularity,
reducing visibility loss by 15% and capturing more transitions with higher
accuracy and continuity. We validate kabr-tools through three case studies,
analyzing 969 behavioral sequences, surpassing the capacity of traditional
methods for data capture and annotation. We found that, like Plains zebras,
vigilance in Grevy's zebras decreases with herd size, but, unlike Plains
zebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit
strong behavioral inertia, with rare transitions to alert behaviors and
observed spatial segregation between Grevy's zebras, Plains zebras, and
giraffes in mixed-species herds. By enabling automated behavioral monitoring at
scale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing
conservation, biodiversity research, and ecological monitoring.

</details>


### [114] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [115] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 针对传感器有限的姿态估计问题，本文提出InPose方法，将姿态估计公式化为逆问题，通过仅基于旋转测量条件化预训练扩散模型，并利用位置测量引导似然项，实现了用户间的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 在传感器数量有限的实际场景中，姿态估计具有挑战性。现有条件扩散模型虽然有前景，但由于位置测量受用户体型影响，导致跨用户泛化能力差。

Method: 将姿态估计表述为逆问题。利用一个预训练的扩散模型，仅以旋转测量作为条件。然后，通过从测量位置导出的似然项来引导该模型的先验知识。

Result: 所提出的InPose方法能够实现零样本泛化。它能生成性地估计高度可能的姿态序列，从而最佳地解释稀疏的身体测量数据。

Conclusion: InPose方法为任何用户提供了生成性的姿态估计，有效解释了稀疏的身体测量结果，并解决了传统方法在跨用户泛化上的局限性。

Abstract: Pose estimation refers to tracking a human's full body posture, including
their head, torso, arms, and legs. The problem is challenging in practical
settings where the number of body sensors are limited. Past work has shown
promising results using conditional diffusion models, where the pose prediction
is conditioned on both <location, rotation> measurements from the sensors.
Unfortunately, nearly all these approaches generalize poorly across users,
primarly because location measurements are highly influenced by the body size
of the user. In this paper, we formulate pose estimation as an inverse problem
and design an algorithm capable of zero-shot generalization. Our idea utilizes
a pre-trained diffusion model and conditions it on rotational measurements
alone; the priors from this model are then guided by a likelihood term, derived
from the measured locations. Thus, given any user, our proposed InPose method
generatively estimates the highly likely sequence of poses that best explains
the sparse on-body measurements.

</details>


### [116] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: 提出VGDM，一种基于Transformer的扩散模型，通过结合全局上下文推理和迭代去噪，显著提升了脑肿瘤检测与分割的准确性和边界精度，优于传统U-Net。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的准确检测与分割对诊断和治疗至关重要。然而，传统U-Net等卷积架构在捕捉长程依赖性方面存在局限，影响了复杂肿瘤结构的处理性能。

Method: 本文提出VGDM（Vision-Guided Diffusion Model），一种Transformer驱动的扩散框架。通过在扩散过程核心嵌入Vision Transformer，模型结合了全局上下文推理和迭代去噪，以提高体积准确性和边界精度。Transformer骨干网络有效建模空间关系，扩散精炼则减轻体素级误差并恢复细粒度肿瘤细节。

Result: 在MRI脑肿瘤数据集上的实验验证表明，VGDM在Dice相似系数和Hausdorff距离方面均取得了持续提升。

Conclusion: Transformer引导的扩散模型在肿瘤分割领域具有巨大潜力，有望超越传统U-Net基线，推动神经肿瘤学领域的最新进展。

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance
imaging (MRI) are essential for diagnosis, treatment planning, and clinical
monitoring. While convolutional architectures such as U-Net have long been the
backbone of medical image segmentation, their limited capacity to capture
long-range dependencies constrains performance on complex tumor structures.
Recent advances in diffusion models have demonstrated strong potential for
generating high-fidelity medical images and refining segmentation boundaries.
  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor
Detection and Segmentation framework, a transformer-driven diffusion framework
for brain tumor detection and segmentation. By embedding a vision transformer
at the core of the diffusion process, the model leverages global contextual
reasoning together with iterative denoising to enhance both volumetric accuracy
and boundary precision. The transformer backbone enables more effective
modeling of spatial relationships across entire MRI volumes, while diffusion
refinement mitigates voxel-level errors and recovers fine-grained tumor
details.
  This hybrid design provides a pathway toward improved robustness and
scalability in neuro-oncology, moving beyond conventional U-Net baselines.
Experimental validation on MRI brain tumor datasets demonstrates consistent
gains in Dice similarity and Hausdorff distance, underscoring the potential of
transformer-guided diffusion models to advance the state of the art in tumor
segmentation.

</details>


### [117] [Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques](https://arxiv.org/abs/2510.02097)
*Walid Rabehi,Marion Le Texier,Rémi Lemoy*

Main category: cs.CV

TL;DR: 开发了一种双通道U-Net深度学习方法，从历史地图中提取法国1925-1950年的全国性城市足迹数据，填补了历史城市扩张分析的数据空白，并取得了73%的整体准确率。


<details>
  <summary>Details</summary>
Motivation: 缺乏1970年代以前法国全国性的数字化城市足迹数据，阻碍了对历史城市蔓延的定量分析。

Method: 采用可扩展的深度学习流水线，核心是一个双通道U-Net方法。第一通道识别混淆区域以指导数据增强；第二通道利用精炼数据集和第一模型的二值化输出，减少辐射噪声和误报。该方法在高性能计算集群上处理了法国941张高分辨率历史地图。

Result: 生成了首个开放获取的、全国规模的1925-1950年法国城市足迹数据集。最终成果的整体准确率达到73%，有效捕捉了多样化的城市模式，并克服了常见图件干扰。

Conclusion: 成功创建了法国1925-1950年期间的全国性城市足迹数据集，并开源了代码、训练数据集和最终成果，为长期城市化动态研究提供了重要支持。

Abstract: Quantitative analysis of historical urban sprawl in France before the 1970s
is hindered by the lack of nationwide digital urban footprint data. This study
bridges this gap by developing a scalable deep learning pipeline to extract
urban areas from the Scan Histo historical map series (1925-1950), which
produces the first open-access, national-scale urban footprint dataset for this
pivotal period. Our key innovation is a dual-pass U-Net approach designed to
handle the high radiometric and stylistic complexity of historical maps. The
first pass, trained on an initial dataset, generates a preliminary map that
identifies areas of confusion, such as text and roads, to guide targeted data
augmentation. The second pass uses a refined dataset and the binarized output
of the first model to minimize radiometric noise, which significantly reduces
false positives. Deployed on a high-performance computing cluster, our method
processes 941 high-resolution tiles covering the entirety of metropolitan
France. The final mosaic achieves an overall accuracy of 73%, effectively
capturing diverse urban patterns while overcoming common artifacts like labels
and contour lines. We openly release the code, training datasets, and the
resulting nationwide urban raster to support future research in long-term
urbanization dynamics.

</details>


### [118] [When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos](https://arxiv.org/abs/2510.02100)
*Woowon Jang,Jiwon Im,Juseung Choi,Niki Rashidian,Wesley De Neve,Utku Ozbulak*

Main category: cs.CV

TL;DR: 本研究系统分析了外科视频中基于点的目标跟踪的失效模式，发现它对工具表现良好，但对解剖目标表现不佳，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 尽管SAM2等VOS模型在外科视频零样本跟踪中表现出色，但点跟踪这种高效低成本的输入方式，其在复杂手术环境中的可靠性和失效情况尚不明确。

Method: 通过系统分析腹腔镜胆囊切除术视频中基于点的跟踪失效模式，并比较其与分割掩码初始化的性能，主要研究胆囊、抓钳和L型电刀三种手术目标。

Result: 基于点的跟踪在手术工具上表现具竞争力，但在解剖目标（如胆囊）上表现持续不佳，主要因组织相似性和模糊边界导致失效。定性分析揭示了影响跟踪结果的关键因素。

Conclusion: 点跟踪在解剖目标跟踪上存在局限性。本研究提供了选择和放置跟踪点的实用建议，以提高外科视频分析的性能。

Abstract: Video object segmentation (VOS) models such as SAM2 offer promising zero-shot
tracking capabilities for surgical videos using minimal user input. Among the
available input types, point-based tracking offers an efficient and low-cost
alternative, yet its reliability and failure cases in complex surgical
environments are not well understood. In this work, we systematically analyze
the failure modes of point-based tracking in laparoscopic cholecystectomy
videos. Focusing on three surgical targets, the gallbladder, grasper, and
L-hook electrocautery, we compare the performance of point-based tracking with
segmentation mask initialization. Our results show that point-based tracking is
competitive for surgical tools but consistently underperforms for anatomical
targets, where tissue similarity and ambiguous boundaries lead to failure.
Through qualitative analysis, we reveal key factors influencing tracking
outcomes and provide several actionable recommendations for selecting and
placing tracking points to improve performance in surgical video analysis.

</details>


### [119] [FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation](https://arxiv.org/abs/2510.02114)
*Ding-Ruei Shen*

Main category: cs.CV

TL;DR: 本文提出联邦学习中服务器预训练模型仅用客户端无标签数据适应新域的语义分割新任务FFREEDG，并开发了FRIEREN框架，该框架利用视觉基础模型和视觉-语言模态，通过弱到强一致性学习，在领域适应基准测试中取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在语义分割（SS）任务中适应新域时，面临严重的领域漂移挑战，尤其当客户端数据无标签时。现有FL方法要么不切实际地假设客户端数据有标签，要么未能有效利用现代视觉基础模型（VFMs）的强大能力。

Method: 提出FRIEREN框架以解决FFREEDG任务。该框架利用视觉基础模型（VFM）的知识，集成视觉和语言模态。具体方法包括：使用由CLIP文本嵌入引导的视觉-语言解码器以提升语义消歧能力，以及采用弱到强一致性学习策略进行基于伪标签的鲁棒本地训练。

Result: 在合成到真实以及晴朗到恶劣天气的基准测试中，FRIEREN框架有效解决了FFREEDG新任务，并取得了与现有域泛化和域适应方法相当的竞争力性能。

Conclusion: 该框架成功应对了联邦学习中客户端无标签数据适应新域的语义分割新任务，展现出强大性能，并为未来研究奠定了坚实基础。

Abstract: Federeated Learning (FL) offers a privacy-preserving solution for Semantic
Segmentation (SS) tasks to adapt to new domains, but faces significant
challenges from these domain shifts, particularly when client data is
unlabeled. However, most existing FL methods unrealistically assume access to
labeled data on remote clients or fail to leverage the power of modern Vision
Foundation Models (VFMs). Here, we propose a novel and challenging task,
FFREEDG, in which a model is pretrained on a server's labeled source dataset
and subsequently trained across clients using only their unlabeled data,
without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a
framework that leverages the knowledge of a VFM by integrating vision and
language modalities. Our approach employs a Vision-Language decoder guided by
CLIP-based text embeddings to improve semantic disambiguation and uses a
weak-to-strong consistency learning strategy for robust local training on
pseudo-labels. Our experiments on synthetic-to-real and
clear-to-adverse-weather benchmarks demonstrate that our framework effectively
tackles this new task, achieving competitive performance against established
domain generalization and adaptation methods and setting a strong baseline for
future research.

</details>


### [120] [Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](https://arxiv.org/abs/2510.02155)
*Shu Zou,Xinyu Tian,Lukas Wesemann,Fabian Waschkowski,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: ASK-Hint是一个基于动作知识的结构化提示框架，用于提高冻结视觉语言模型在视频异常检测中的准确性和可解释性，并在多个数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有用于视频异常检测的提示方法过于抽象，未能捕捉到定义复杂异常的细粒度人机交互或动作语义，限制了冻结视觉语言模型（VLMs）的潜力。

Method: 提出了ASK-Hint框架，通过利用以动作为中心的知识，将提示组织成语义连贯的组（例如，暴力、财产犯罪），并制定细粒度的指导性问题，以使模型预测与判别性视觉线索对齐。

Result: 在UCF-Crime和XD-Violence数据集上，ASK-Hint持续提高了AUC，实现了超越现有微调和免训练方法的最新性能。此外，它提供了可解释的异常推理路径，并展现了强大的跨数据集和VLM骨干网络的泛化能力。

Conclusion: 提示粒度在视频异常检测中至关重要。ASK-Hint作为一种新的免训练、可泛化且可解释的视频异常检测解决方案，凸显了其关键作用。

Abstract: Prompting has emerged as a practical way to adapt frozen vision-language
models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are
often overly abstract, overlooking the fine-grained human-object interactions
or action semantics that define complex anomalies in surveillance videos. We
propose ASK-Hint, a structured prompting framework that leverages
action-centric knowledge to elicit more accurate and interpretable reasoning
from frozen VLMs. Our approach organizes prompts into semantically coherent
groups (e.g. violence, property crimes, public safety) and formulates
fine-grained guiding questions that align model predictions with discriminative
visual cues. Extensive experiments on UCF-Crime and XD-Violence show that
ASK-Hint consistently improves AUC over prior baselines, achieving
state-of-the-art performance compared to both fine-tuned and training-free
methods. Beyond accuracy, our framework provides interpretable reasoning traces
towards anomaly and demonstrates strong generalization across datasets and VLM
backbones. These results highlight the critical role of prompt granularity and
establish ASK-Hint as a new training-free and generalizable solution for
explainable video anomaly detection.

</details>


### [121] [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)
*Weijia Dou,Xu Zhang,Yi Bin,Jian Liu,Bo Peng,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GeoPurify通过利用2D VLM特征中潜在的几何信息，并结合学生亲和网络和几何引导池化模块，有效克服了2D到3D语义分割的噪声问题和高昂成本，以极少数据量实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 将2D视觉-语言模型（VLMs）的特征迁移到3D语义分割时存在权衡：直接投影导致预测噪声且碎片化，而强制几何一致性则需要昂贵的训练和大量3D标注数据。这源于当前分割-匹配范式未能协调2D语义与3D几何结构，尽管几何线索在噪声特征中仍以潜在形式存在。

Method: 本文提出GeoPurify方法。它应用一个小的学生亲和网络（Student Affinity Network），利用从3D自监督教师模型中提取的几何先验知识，净化2D VLM生成的3D点特征。在推理阶段，设计了一个几何引导池化（Geometry-Guided Pooling）模块，进一步对点云进行去噪，确保语义和结构的一致性。

Result: GeoPurify有效缓解了上述权衡，并展现出卓越的数据效率。在主要的3D基准测试中，GeoPurify仅使用约1.5%的训练数据，就达到或超越了最先进的性能。

Conclusion: GeoPurify通过利用潜在几何信息和学习到的亲和网络，成功解决了2D VLM特征向3D语义分割迁移的挑战，显著提升了数据效率和性能表现。

Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

</details>


### [122] [Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications](https://arxiv.org/abs/2510.02197)
*Emmanuel Nsengiyumvaa,Leonard Niyitegekaa,Eric Umuhoza*

Main category: cs.CV

TL;DR: 该研究提出一种基于猪耳廓静脉图案的非侵入式生物识别方法，利用智能手机采集杂交猪耳部图像，通过计算机视觉技术提取特征，并使用支持向量机（SVM）实现了98.12%的识别精度，为农户提供了经济高效且实用的牲畜识别方案。


<details>
  <summary>Details</summary>
Motivation: 现有猪只识别方法（如耳标、微芯片）存在不可靠、成本高、仅适用于纯种猪以及不适合小规模农户等问题，难以支持健康监测、育种和生产力追踪。因此，需要开发一种非侵入式、经济有效且普遍适用的新型牲畜识别方法。

Method: 研究收集了20头杂交猪（长白-皮特兰和杜洛克-皮特兰）的800张耳部图像，通过标准智能手机和简单背光完成采集。开发了多阶段计算机视觉流程，用于增强静脉可见性、提取结构和空间特征，生成生物识别签名。最后，使用机器学习模型对这些特征进行分类。

Result: 在杂交猪群中，支持向量机（SVM）模型达到了最高的识别精度，正确识别率达98.12%。从图像处理到分类的整个过程平均耗时8.3秒，表明该系统具备实时农场部署的可行性。

Conclusion: 该系统通过永久性的生物标记取代了易碎的物理标识符，为农户提供了一种经济有效且无压力的动物识别方法。研究结果证实了耳廓静脉生物识别技术在数字化畜牧管理中的实用性，有助于将精准农业的益处推广到资源有限的农业社区。

Abstract: Accurate livestock identification is a cornerstone of modern farming: it
supports health monitoring, breeding programs, and productivity tracking.
However, common pig identification methods, such as ear tags and microchips,
are often unreliable, costly, target pure breeds, and thus impractical for
small-scale farmers. To address this gap, we propose a noninvasive biometric
identification approach that leverages uniqueness of the auricular vein
patterns. To this end, we have collected 800 ear images from 20 mixed-breed
pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a
standard smartphone and simple back lighting. A multistage computer vision
pipeline was developed to enhance vein visibility, extract structural and
spatial features, and generate biometric signatures. These features were then
classified using machine learning models. Support Vector Machines (SVM)
achieved the highest accuracy: correctly identifying pigs with 98.12% precision
across mixed-breed populations. The entire process from image processing to
classification was completed in an average of 8.3 seconds, demonstrating
feasibility for real-time farm deployment. We believe that by replacing fragile
physical identifiers with permanent biological markers, this system provides
farmers with a cost-effective and stress-free method of animal identification.
More broadly, the findings confirm the practicality of auricular vein
biometrics for digitizing livestock management, reinforcing its potential to
extend the benefits of precision farming to resource-constrained agricultural
communities.

</details>


### [123] [MMDEW: Multipurpose Multiclass Density Estimation in the Wild](https://arxiv.org/abs/2510.02213)
*Villanelle O'Reilly,Jonathan Cox,Georgios Leontidis,Marc Hanheide,Petra Bosilj,James Brown*

Main category: cs.CV

TL;DR: 提出一种基于Twins Vision Transformer和多尺度解码的多类别密度图计数框架，通过类别聚焦模块提高性能，在密集遮挡场景下显著优于现有方法，并成功应用于生物多样性监测。


<details>
  <summary>Details</summary>
Motivation: 在密集和遮挡的场景中，传统的基于检测的离散计数方法难以准确估计物体数量，因此需要密度图估计方法来解决多类别对象计数问题。

Method: 本文提出一个多类别计数框架，该框架利用Twins金字塔视觉Transformer作为骨干网络，并结合一个基于最先进多尺度解码方法的专用多类别计数头。通过采用双任务设计，引入一个基于分割的类别聚焦模块，以在训练期间抑制类别间的串扰。

Result: 在VisDrone和iSAID基准测试上，相较于现有多类别群计数方法，该方法在MAE上分别取得了33%、43%和64%的显著降低，展现出卓越性能。与YOLOv11的比较强调了群计数方法在密集场景中的必要性。此外，该方法成功应用于生物多样性监测数据集，证明了其在新领域中的通用性和潜力。

Conclusion: 所提出的多类别计数框架在处理密集和遮挡场景下的多类别对象计数问题上表现出优越性能，并通过其区域损失特性成功扩展到生物多样性监测等新领域，为环境保护和生态研究提供了可扩展的洞察力。

Abstract: Density map estimation can be used to estimate object counts in dense and
occluded scenes where discrete counting-by-detection methods fail. We propose a
multicategory counting framework that leverages a Twins pyramid
vision-transformer backbone and a specialised multi-class counting head built
on a state-of-the-art multiscale decoding approach. A two-task design adds a
segmentation-based Category Focus Module, suppressing inter-category cross-talk
at training time. Training and evaluation on the VisDrone and iSAID benchmarks
demonstrates superior performance versus prior multicategory crowd-counting
approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11
underscores the necessity of crowd counting methods in dense scenes. The
method's regional loss opens up multi-class crowd counting to new domains,
demonstrated through the application to a biodiversity monitoring dataset,
highlighting its capacity to inform conservation efforts and enable scalable
ecological insights.

</details>


### [124] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: TempoControl是一种无需重新训练即可对生成视频中的视觉概念进行精细时间控制的方法，通过优化扩散模型的交叉注意力图，实现精确的时序对齐，同时保持视频质量。


<details>
  <summary>Details</summary>
Motivation: 现有生成视频模型虽能创建高质量视频，但普遍缺乏细粒度的时序控制，用户无法指定视觉元素在生成序列中出现的时间。

Method: 引入TempoControl，在推理阶段利用文本到视频扩散模型的交叉注意力图，通过一种新颖的优化方法引导概念的时序。该方法基于三个原则：通过相关性将注意力时间形状与控制信号对齐、通过能量在需要可见性时放大注意力、通过熵保持空间焦点。

Result: TempoControl实现了对概念出现时机的精确控制，并保持了高视频质量和多样性。其有效性已在多种视频生成应用中得到验证，包括单个和多个对象的时间重排序、动作和音频对齐生成。

Conclusion: TempoControl成功解决了生成视频模型的时序控制难题，提供了一种无需重训练的有效方案，极大地提升了用户对生成视频内容的控制精度和应用范围。

Abstract: Recent advances in generative video models have enabled the creation of
high-quality videos based on natural language prompts. However, these models
frequently lack fine-grained temporal control, meaning they do not allow users
to specify when particular visual elements should appear within a generated
sequence. In this work, we introduce TempoControl, a method that allows for
temporal alignment of visual concepts during inference, without requiring
retraining or additional supervision. TempoControl utilizes cross-attention
maps, a key component of text-to-video diffusion models, to guide the timing of
concepts through a novel optimization approach. Our method steers attention
using three complementary principles: aligning its temporal shape with a
control signal (via correlation), amplifying it where visibility is needed (via
energy), and maintaining spatial focus (via entropy). TempoControl allows
precise control over timing while ensuring high video quality and diversity. We
demonstrate its effectiveness across various video generation applications,
including temporal reordering for single and multiple objects, as well as
action and audio-aligned generation.

</details>


### [125] [RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2510.02240)
*Sicheng Feng,Kaiwen Tuo,Song Wang,Lingdong Kong,Jianke Zhu,Huan Wang*

Main category: cs.CV

TL;DR: MLLMs在细粒度视觉推理和空间推理中表现不佳，RL面临稀疏奖励问题。本文提出RewardMap多阶段RL框架，通过构建密集奖励数据集ReasonMap-Plus、难度感知奖励设计和多阶段RL方案，显著提升了MLLMs的视觉理解和推理能力，并在多个基准测试中取得平均3.47%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在细粒度视觉推理，尤其是在结构化、信息丰富的场景（如交通地图）中的空间推理能力不足（ReasonMap数据集已揭示此问题）。此外，在该类任务中，标准强化学习（RL）面临奖励稀疏和优化不稳定的挑战。

Method: 1. 构建ReasonMap-Plus扩展数据集，通过VQA任务引入密集奖励信号，实现细粒度视觉理解技能的有效冷启动训练。 2. 提出RewardMap多阶段RL框架，旨在提升MLLMs的视觉理解和推理能力，包含：a) 难度感知奖励设计：引入细节奖励，直接解决奖励稀疏问题并提供更丰富监督。b) 多阶段RL方案：将训练从简单感知任务引导至复杂推理任务，提供比传统SFT更有效的冷启动策略。

Result: RewardMap的每个组件都对性能提升有贡献，其组合效果最佳。经RewardMap训练的模型在涵盖空间推理、细粒度视觉推理和通用任务的6个基准测试中，平均性能提升3.47%，证明其增强了视觉理解和推理能力。

Conclusion: RewardMap框架通过创新的数据集、奖励设计和多阶段RL策略，有效解决了MLLMs在细粒度视觉和空间推理中的核心挑战，显著提升了模型的视觉理解与推理能力。

Abstract: Fine-grained visual reasoning remains a core challenge for multimodal large
language models (MLLMs). The recently introduced ReasonMap highlights this gap
by showing that even advanced MLLMs struggle with spatial reasoning in
structured and information-rich settings such as transit maps, a task of clear
practical and scientific importance. However, standard reinforcement learning
(RL) on such tasks is impeded by sparse rewards and unstable optimization. To
address this, we first construct ReasonMap-Plus, an extended dataset that
introduces dense reward signals through Visual Question Answering (VQA) tasks,
enabling effective cold-start training of fine-grained visual understanding
skills. Next, we propose RewardMap, a multi-stage RL framework designed to
improve both visual understanding and reasoning capabilities of MLLMs.
RewardMap incorporates two key designs. First, we introduce a difficulty-aware
reward design that incorporates detail rewards, directly tackling the sparse
rewards while providing richer supervision. Second, we propose a multi-stage RL
scheme that bootstraps training from simple perception to complex reasoning
tasks, offering a more effective cold-start strategy than conventional
Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus
demonstrate that each component of RewardMap contributes to consistent
performance gains, while their combination yields the best results. Moreover,
models trained with RewardMap achieve an average improvement of 3.47% across 6
benchmarks spanning spatial reasoning, fine-grained visual reasoning, and
general tasks beyond transit maps, underscoring enhanced visual understanding
and reasoning capabilities.

</details>


### [126] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: 本研究提出DragFlow，一个利用FLUX强大生成先验的拖拽式图像编辑框架，通过引入区域编辑范式和多项技术，显著提升了编辑质量，解决了传统方法在DiT模型上的局限性，并实现了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 以往的拖拽式图像编辑因基础模型（如Stable Diffusion UNet）的先验不足，在目标区域常出现扭曲。尽管新的DiT流匹配模型（如FLUX）提供了更强的生成先验，但拖拽式编辑尚未能有效利用这些优势，且直接将点式拖拽应用于DiT模型效果不佳。

Method: DragFlow是首个有效利用FLUX先验的拖拽式编辑框架。它引入了区域编辑范式，使用仿射变换实现更丰富和一致的特征监督，以克服DiT特征难以进行点式监督的限制。此外，整合了预训练的开放域个性化适配器（如IP-Adapter）以增强主体一致性，并通过基于梯度掩码的硬约束保持背景保真度。还利用多模态大语言模型（MLLMs）来解决任务歧义。为评估，创建了新的区域拖拽基准（ReD Bench）。

Result: DragFlow在DragBench-DR和ReD Bench上进行了广泛实验，结果表明它显著优于现有基线（包括点式和区域式），设定了拖拽式图像编辑的新SOTA。

Conclusion: DragFlow成功地利用了DiT流匹配模型的强大生成先验，通过创新的区域编辑范式和多项技术融合，解决了拖拽式图像编辑长期存在的挑战，在图像质量和编辑能力上均取得了突破性进展。

Abstract: Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [127] [OR-Toolformer: Modeling and Solving Operations Research Problems with Tool Augmented Large Language Models](https://arxiv.org/abs/2510.01253)
*Jianzhang Zhang,Jialong Zhou,Chuang Liu*

Main category: cs.AI

TL;DR: 本文提出OR-Toolformer，一个通过工具增强微调的Llama-3.1-8B模型，在运筹学（OR）问题求解上表现出色，在基准测试中超越现有基线，并展现出强大的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在数学推理上表现优异，但在运筹学（OR）任务中，依赖闭源API存在隐私风险，而从头训练开源模型则计算成本高昂。

Method: 引入OR-Toolformer，通过半自动数据合成管线生成多样化的OR问题-答案对来微调Llama-3.1-8B-Instruct模型，并用外部求解器增强模型以生成API调用。

Result: 在四项标准基准中的三项上，OR-Toolformer的执行准确率高达80.1%，超过同等规模基线4.3%以上。在两种未见过的OR问题类型的零样本评估中，平均准确率达到54%，比最强基线提升21个百分点。

Conclusion: 这些研究结果验证了工具增强型微调大语言模型在实现准确且可泛化的运筹学问题建模和求解方面的有效性。

Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning, but
reliance on closed-source APIs for OR tasks raises privacy concerns, and
training open-source models from scratch incurs high compute costs. We
introduce OR-Toolformer, which fine-tunes Llama-3.1-8B-Instruct with a
semi-automatic data synthesis pipeline that generates diverse OR problem-answer
pairs and augments the model with external solvers to produce API calls. On
three of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution
accuracy, exceeding size-matched baselines by over 4.3%. In zero-shot
evaluation on two unseen OR problem types, it attains 54% average accuracy, a
21 percentage-point improvement over the strongest baseline. These findings
validate the efficacy of tool-augmented fine-tuning LLMs for accurate and
generalizable OR problem modeling and solving.

</details>


### [128] [Modeling Others' Minds as Code](https://arxiv.org/abs/2510.01272)
*Kunal Jha,Aydan Yuenan Huang,Eric Ye,Natasha Jaques,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: ROTE算法通过将日常行为建模为行为程序，并结合LLM生成假设与概率推断，高效准确地预测人类及AI行为，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 准确预测人类行为对稳健安全的人机协作至关重要。现有方法通常数据需求大、脆弱且计算成本高，难以快速适应，因其要么假设非现实的理性，要么过于计算密集。

Method: 提出ROTE算法，将日常社会互动中可预测的“脚本”建模为计算机代码中的行为程序，而非基于信念和欲望的策略。ROTE利用大型语言模型（LLMs）合成行为程序的假设空间，并结合概率推断对该空间中的不确定性进行推理。

Result: 在网格世界任务和大型具身家庭模拟器中测试，ROTE能从稀疏观察中预测人类和AI行为。在样本内准确性和样本外泛化能力方面，ROTE比行为克隆和基于LLM的方法等现有基线表现高出50%。

Conclusion: 通过将行为理解视为程序合成问题，ROTE为AI系统在现实世界中高效、有效地预测人类行为开辟了新途径。

Abstract: Accurate prediction of human behavior is essential for robust and safe
human-AI collaboration. However, existing approaches for modeling people are
often data-hungry and brittle because they either make unrealistic assumptions
about rationality or are too computationally demanding to adapt rapidly. Our
key insight is that many everyday social interactions may follow predictable
patterns; efficient "scripts" that minimize cognitive load for actors and
observers, e.g., "wait for the green light, then go." We propose modeling these
routines as behavioral programs instantiated in computer code rather than
policies conditioned on beliefs and desires. We introduce ROTE, a novel
algorithm that leverages both large language models (LLMs) for synthesizing a
hypothesis space of behavioral programs, and probabilistic inference for
reasoning about uncertainty over that space. We test ROTE in a suite of
gridworld tasks and a large-scale embodied household simulator. ROTE predicts
human and AI behaviors from sparse observations, outperforming competitive
baselines -- including behavior cloning and LLM-based methods -- by as much as
50% in terms of in-sample accuracy and out-of-sample generalization. By
treating action understanding as a program synthesis problem, ROTE opens a path
for AI systems to efficiently and effectively predict human behavior in the
real-world.

</details>


### [129] [Cyber Academia-Chemical Engineering (CA-ChemE): A Living Digital Town for Self-Directed Research Evolution and Emergent Scientific Discovery](https://arxiv.org/abs/2510.01293)
*Zekun Jiang,Chunming Xu,Tianhang Zhou*

Main category: cs.AI

TL;DR: 本研究提出了CA-ChemE系统，一个基于多智能体协作的数字城镇，旨在解决化学工程中AI系统在跨学科协作和探索未知问题方面的局限，通过集成知识库和协作智能体实现自主研究和科学发现。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在化学工程领域虽有巨大潜力，但在跨学科协作和探索未知问题方面仍存在局限性。

Method: 引入Cyber Academia-Chemical Engineering (CA-ChemE) 系统，这是一个通过多智能体协作实现自主研究演进和新兴科学发现的数字城镇。该系统集成了领域特定知识库、知识增强技术和协作智能体。为解决跨领域协作效率瓶颈，进一步引入了具备本体工程能力的协作智能体（CA）。

Result: 知识库增强机制使所有七个专家智能体的对话质量平均提高了10-15%。协作智能体（CA）的介入使远距离领域专家对的协作效率提高了8.5%，而近距离领域专家对仅提高了0.8%，揭示了“由知识库差距导致的协作效率降低”效应。

Conclusion: 精心设计的多智能体架构为化学工程领域的自主科学发现提供了一条可行的途径。

Abstract: The rapid advancement of artificial intelligence (AI) has demonstrated
substantial potential in chemical engineering, yet existing AI systems remain
limited in interdisciplinary collaboration and exploration of uncharted
problems. To address these issues, we present the Cyber Academia-Chemical
Engineering (CA-ChemE) system, a living digital town that enables self-directed
research evolution and emergent scientific discovery through multi-agent
collaboration. By integrating domain-specific knowledge bases, knowledge
enhancement technologies, and collaboration agents, the system successfully
constructs an intelligent ecosystem capable of deep professional reasoning and
efficient interdisciplinary collaboration. Our findings demonstrate that
knowledge base-enabled enhancement mechanisms improved dialogue quality scores
by 10-15% on average across all seven expert agents, fundamentally ensuring
technical judgments are grounded in verifiable scientific evidence. However, we
observed a critical bottleneck in cross-domain collaboration efficiency,
prompting the introduction of a Collaboration Agent (CA) equipped with ontology
engineering capabilities. CA's intervention achieved 8.5% improvements for
distant-domain expert pairs compared to only 0.8% for domain-proximate pairs -
a 10.6-fold difference - unveiling the "diminished collaborative efficiency
caused by knowledge-base gaps" effect. This study demonstrates how carefully
designed multi-agent architectures can provide a viable pathway toward
autonomous scientific discovery in chemical engineering.

</details>


### [130] [The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation](https://arxiv.org/abs/2510.01295)
*Zarreen Reza*

Main category: cs.AI

TL;DR: 为评估LLM代理在互动环境中的社会和认知动态，研究引入了多代理辩论框架。发现代理有寻求共识的倾向，人格设定能产生可测量的心理特征，且主持人人格能显著影响辩论结果。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）向自主代理转型，传统的下游任务评估基准已不足。它们无法捕捉代理在互动环境中交流、说服和协作时出现的社会和认知动态。

Method: 引入了一个新颖的评估框架，将多代理辩论作为受控的“社会实验室”。框架中，具有不同人格和激励机制的LLM代理在LLM主持人的监督下，就广泛主题进行辩论。通过一套新的心理测量和语义指标进行分析。

Result: 代理展现出强大而稳健的共识倾向，即使没有明确指令和面对敏感话题，也能达到高度语义一致性（平均值>0.88）。分配的人格会诱导稳定、可测量的心理特征（尤其在认知努力方面）。主持人的角色能通过构建环境显著改变辩论结果，这对外部AI对齐具有重要意义。

Conclusion: 该工作为代理环境下新型的、基于心理测量的动态评估协议提供了蓝图，为理解和塑造下一代AI代理的社会行为提供了关键方法。

Abstract: As Large Language Models (LLMs) transition from static tools to autonomous
agents, traditional evaluation benchmarks that measure performance on
downstream tasks are becoming insufficient. These methods fail to capture the
emergent social and cognitive dynamics that arise when agents communicate,
persuade, and collaborate in interactive environments. To address this gap, we
introduce a novel evaluation framework that uses multi-agent debate as a
controlled "social laboratory" to discover and quantify these behaviors. In our
framework, LLM-based agents, instantiated with distinct personas and
incentives, deliberate on a wide range of challenging topics under the
supervision of an LLM moderator. Our analysis, enabled by a new suite of
psychometric and semantic metrics, reveals several key findings. Across
hundreds of debates, we uncover a powerful and robust emergent tendency for
agents to seek consensus, consistently reaching high semantic agreement ({\mu}
> 0.88) even without explicit instruction and across sensitive topics. We show
that assigned personas induce stable, measurable psychometric profiles,
particularly in cognitive effort, and that the moderators persona can
significantly alter debate outcomes by structuring the environment, a key
finding for external AI alignment. This work provides a blueprint for a new
class of dynamic, psychometrically grounded evaluation protocols designed for
the agentic setting, offering a crucial methodology for understanding and
shaping the social behaviors of the next generation of AI agents. We have
released the code and results at
https://github.com/znreza/multi-agent-LLM-eval-for-debate.

</details>


### [131] [Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.01304)
*Yu Zeng,Wenxuan Huang,Shiting Huang,Xikun Bao,Yukun Qi,Yiming Zhao,Qiuchen Wang,Lin Chen,Zehui Chen,Huaian Chen,Wanli Ouyang,Feng Zhao*

Main category: cs.AI

TL;DR: 现有大型视觉语言模型在基础感知与推理能力上仍有不足，尤其在拼图任务上表现不佳。本文提出AGILE，一个代理式交互学习框架，通过将拼图任务建模为交互过程并提供视觉反馈，显著提升了模型的感知与推理能力，并在通用视觉任务上展现出强大的泛化性，同时为多模态强化学习数据稀缺问题提供了高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型（VLMs）在基本感知和推理能力上存在局限，即使在简单的拼图任务上表现也接近随机。高质量视觉语言数据的稀缺性及其扩展性限制，阻碍了这些核心能力的提升。

Method: 提出AGILE（Agentic jiGsaw Interaction Learning for Enhancing），将拼图任务解决过程公式化为交互式学习。模型根据当前状态生成可执行代码以执行动作，环境提供细粒度视觉反馈以指导任务完成。通过观察与交互的迭代循环，模型通过探索和反馈逐步提升其感知和推理能力。

Result: AGILE在不同复杂度的拼图任务上显著提升了性能（例如，在2x2设置下准确率从9.5%提高到82.8%）。同时，在9项通用视觉任务上表现出强大的泛化能力，平均提升3.1%，表明感知和推理能力均有显著增强。

Conclusion: 该工作为推动多模态模型中的推理和泛化能力开辟了新途径，并为多模态强化学习数据稀缺问题提供了一种高效、可扩展的解决方案。

Abstract: Although current large Vision-Language Models (VLMs) have advanced in
multimodal understanding and reasoning, their fundamental perceptual and
reasoning abilities remain limited. Specifically, even on simple jigsaw tasks,
existing VLMs perform near randomly, revealing deficiencies in core perception
and reasoning capabilities. While high-quality vision-language data can enhance
these capabilities, its scarcity and limited scalability impose significant
constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction
Learning for Enhancing visual perception and reasoning in VLMs. AGILE
formulates jigsaw solving as an interactive process, enabling the model to
progressively engage with the environment. At each step, the model generates
executable code to perform an action based on the current state, while the
environment provides fine-grained visual feedback to guide task completion.
Through this iterative cycle of observation and interaction, the model
incrementally improves its perceptual and reasoning capabilities via
exploration and feedback. Experimental results show that AGILE not only
substantially boosts performance on jigsaw tasks of varying complexity (e.g.,
increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also
demonstrates strong generalization across 9 general vision tasks, achieving an
average improvement of 3.1%. These results indicate notable enhancements in
both perceptual and reasoning abilities. This work opens a new avenue for
advancing reasoning and generalization in multimodal models and provides an
efficient, scalable solution to the scarcity of multimodal reinforcement
learning data. The code and datasets is available at
https://github.com/yuzeng0-0/AGILE .

</details>


### [132] [Aristotle: IMO-level Automated Theorem Proving](https://arxiv.org/abs/2510.01346)
*Tudor Achim,Alex Best,Kevin Der,Mathïs Fédérico,Sergei Gukov,Daniel Halpern-Leister,Kirsten Henningsgard,Yury Kudryashov,Alexander Meiburg,Martin Michelsen,Riley Patterson,Eric Rodriguez,Laura Scharff,Vikram Shanker,Vladmir Sicca,Hari Sowrirajan,Aidan Swope,Matyas Tamas,Vlad Tenev,Jonathan Thomm,Harold Williams,Lawrence Wu*

Main category: cs.AI

TL;DR: AI系统Aristotle结合形式化验证与非形式化推理，在2025年国际数学奥林匹克竞赛问题上达到金牌水平。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个能够解决复杂数学问题（如国际数学奥林匹克竞赛问题）的AI系统，并提升自动化定理证明的能力和扩展性。

Method: 该系统整合了三个主要组件：一个Lean证明搜索系统、一个用于生成和形式化引理的非形式化推理系统，以及一个专门的几何求解器。

Result: Aristotle在2025年国际数学奥林匹克竞赛问题上取得了相当于金牌的成绩，并在自动化定理证明方面展现了最先进的性能和良好的扩展性。

Conclusion: 该系统在自动化定理证明领域取得了重大突破，树立了新的性能标杆并具有优异的扩展性。

Abstract: We introduce Aristotle, an AI system that combines formal verification with
informal reasoning, achieving gold-medal-equivalent performance on the 2025
International Mathematical Olympiad problems. Aristotle integrates three main
components: a Lean proof search system, an informal reasoning system that
generates and formalizes lemmas, and a dedicated geometry solver. Our system
demonstrates state-of-the-art performance with favorable scaling properties for
automated theorem proving.

</details>


### [133] [MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments](https://arxiv.org/abs/2510.01353)
*Darshan Deshpande,Varun Gangal,Hersh Mehta,Anand Kannappan,Rebecca Qian,Peng Wang*

Main category: cs.AI

TL;DR: MEMTRACK是一个评估多平台代理环境中长期记忆和状态跟踪的新基准，模拟企业工作流，并揭示了LLM在处理跨平台依赖和冲突方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有记忆基准主要关注对话场景，但企业动态环境中的记忆评估对于其有效应用至关重要。

Method: 引入MEMTRACK基准，模拟多平台（如Slack, Linear, Git）异步事件的企业工作流，包含带噪声、冲突和交叉引用的时间线。数据集通过专家设计和代理合成生成。引入了正确性、效率和冗余等评估指标，以测试记忆获取、选择和冲突解决能力。

Result: 对现有先进大型语言模型（LLM）和记忆后端进行实验，发现它们在长时间记忆利用、处理跨平台依赖和解决矛盾方面存在挑战。表现最佳的GPT-5模型在MEMTRACK上的正确性得分仅为60%。

Conclusion: 本工作为超越对话设置的记忆增强型代理评估研究提供了一个可扩展框架，并为在复杂组织环境中进行多代理、多平台记忆基准测试奠定了基础。

Abstract: Recent works on context and memory benchmarking have primarily focused on
conversational instances but the need for evaluating memory in dynamic
enterprise environments is crucial for its effective application. We introduce
MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking
in multi-platform agent environments. MEMTRACK models realistic organizational
workflows by integrating asynchronous events across multiple communication and
productivity platforms such as Slack, Linear and Git. Each benchmark instance
provides a chronologically platform-interleaved timeline, with noisy,
conflicting, cross-referring information as well as potential
codebase/file-system comprehension and exploration. Consequently, our benchmark
tests memory capabilities such as acquistion, selection and conflict
resolution. We curate the MEMTRACK dataset through both manual expert driven
design and scalable agent based synthesis, generating ecologically valid
scenarios grounded in real world software development processes. We introduce
pertinent metrics for Correctness, Efficiency, and Redundancy that capture the
effectiveness of memory mechanisms beyond simple QA performance. Experiments
across SoTA LLMs and memory backends reveal challenges in utilizing memory
across long horizons, handling cross-platform dependencies, and resolving
contradictions. Notably, the best performing GPT-5 model only achieves a 60\%
Correctness score on MEMTRACK. This work provides an extensible framework for
advancing evaluation research for memory-augmented agents, beyond existing
focus on conversational setups, and sets the stage for multi-agent,
multi-platform memory benchmarking in complex organizational settings

</details>


### [134] [Retrieval-Augmented Framework for LLM-Based Clinical Decision Support](https://arxiv.org/abs/2510.01363)
*Leon Garza,Anantaa Kotal,Michael A. Grasso,Emre Umucu*

Main category: cs.AI

TL;DR: 该论文提出一个基于大语言模型（LLM）的临床决策支持系统，利用检索增强生成（RAG）技术分析电子健康记录（EHR）数据，为处方医生提供治疗建议，旨在增强而非取代临床判断。


<details>
  <summary>Details</summary>
Motivation: 面对日益复杂的临床决策和快速增长的电子健康记录（EHR），需要更有效的数据驱动方法来支持临床护理，以应对机遇和挑战。

Method: 开发了一个由LLM驱动的临床决策支持系统。该系统通过分析历史EHR数据（包括患者人口统计、主诉、症状、诊断和治疗历史）生成治疗建议。它整合自然语言处理与结构化临床输入，并采用检索增强生成（RAG）管道，和谐处理非结构化叙述和编码数据，以检索和合成具有可比特征的先例。

Result: 初步评估（使用去身份化和合成临床数据集）显示，模型输出的临床合理性和一致性良好。早期发现表明，在适当约束和严格验证下，基于LLM的工具可在处方工作流程中提供有价值的决策支持。

Conclusion: 该工作是生成式AI融入真实世界临床决策的初步步骤，重点关注透明度、安全性和与现有实践的对齐。LLM工具在临床决策支持方面具有巨大潜力。

Abstract: The increasing complexity of clinical decision-making, alongside the rapid
expansion of electronic health records (EHR), presents both opportunities and
challenges for delivering data-informed care. This paper proposes a clinical
decision support system powered by Large Language Models (LLMs) to assist
prescribing clinicians. The system generates therapeutic suggestions by
analyzing historical EHR data, including patient demographics, presenting
complaints, clinical symptoms, diagnostic information, and treatment histories.
The framework integrates natural language processing with structured clinical
inputs to produce contextually relevant recommendations. Rather than replacing
clinician judgment, it is designed to augment decision-making by retrieving and
synthesizing precedent cases with comparable characteristics, drawing on local
datasets or federated sources where applicable. At its core, the system employs
a retrieval-augmented generation (RAG) pipeline that harmonizes unstructured
narratives and codified data to support LLM-based inference. We outline the
system's technical components, including representation representation
alignment and generation strategies. Preliminary evaluations, conducted with
de-identified and synthetic clinical datasets, examine the clinical
plausibility and consistency of the model's outputs. Early findings suggest
that LLM-based tools may provide valuable decision support in prescribing
workflows when appropriately constrained and rigorously validated. This work
represents an initial step toward integration of generative AI into real-world
clinical decision-making with an emphasis on transparency, safety, and
alignment with established practices.

</details>


### [135] [Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort](https://arxiv.org/abs/2510.01367)
*Xinpeng Wang,Nitish Joshi,Barbara Plank,Rico Angell,He He*

Main category: cs.AI

TL;DR: 该研究提出TRACE方法来检测隐性奖励作弊，通过截断模型推理链并测量验证器通过率，识别出那些通过更少“努力”就达到高奖励的模型。TRACE在数学和编码任务中表现出显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 奖励作弊（reward hacking），特别是隐性作弊，对推理模型构成重大威胁。模型可能利用奖励函数漏洞获得高分，却未解决实际任务，且其推理链（CoT）表现无害，从而绕过现有监测方法。

Method: 提出TRACE（Truncated Reasoning AUC Evaluation）方法。核心思想是作弊比解决任务更容易，即模型花费的“努力”更少。TRACE通过逐步截断模型的CoT，强制模型回答，并测量在不同截断长度下通过验证器的比率来量化“努力”。作弊模型会通过一小部分CoT就达到高通过率，导致准确率-长度曲线下的面积较大。

Result: TRACE在数学推理任务中比最强的72B CoT监测器性能提升超过65%，在编码任务中比32B监测器性能提升超过30%。此外，TRACE还能在训练过程中发现未知漏洞。

Conclusion: TRACE为模型的监督提供了一种可扩展的无监督方法，特别适用于当前监测方法失效的场景，有效检测隐性奖励作弊。

Abstract: Reward hacking, where a reasoning model exploits loopholes in a reward
function to achieve high rewards without solving the intended task, poses a
significant threat. This behavior may be explicit, i.e. verbalized in the
model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus
bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE
(Truncated Reasoning AUC Evaluation). Our key observation is that hacking
occurs when exploiting the loophole is easier than solving the actual task.
This means that the model is using less `effort' than required to achieve high
reward. TRACE quantifies effort by measuring how early a model's reasoning
becomes sufficient to pass a verifier. We progressively truncate a model's CoT
at various lengths, force the model to answer, and measure the verifier-passing
rate at each cutoff. A hacking model, which takes a shortcut, will achieve a
high passing rate with only a small fraction of its CoT, yielding a large area
under the accuracy-vs-length curve. TRACE achieves over 65% gains over our
strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B
monitor in coding. We further show that TRACE can discover unknown loopholes
during training. Overall, TRACE offers a scalable unsupervised approach for
oversight where current monitoring methods prove ineffective.

</details>


### [136] [Fine-tuning with RAG for Improving LLM Learning of New Skills](https://arxiv.org/abs/2510.01375)
*Humaid Ibrahim,Nikolai Rozanov,Marek Rei*

Main category: cs.AI

TL;DR: 本文提出一种蒸馏管道，将LLM代理在多步任务中的运行时检索（RAG）指导转化为模型内化的能力。通过从代理失败中提取提示并用于训练学生模型，显著提升了代理在交互式任务中的成功率和效率，同时减少了对运行时检索的依赖。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）代理在执行多步任务时，经常因未满足前置条件、发出冗余命令或错误处理环境约束而失败。尽管检索增强生成（RAG）可以通过提供运行时指导来改善性能，但这需要维护外部知识库并在每次部署时增加计算开销。

Method: 本文提出一个简单的蒸馏管道，将推理时的检索转化为模型习得的能力：1) 从代理失败中提取紧凑、可重用的“提示”（hints）；2) 在任务开始时，利用这些提示通过一次性检索生成改进的教师轨迹；3) 训练学生模型学习这些轨迹，同时移除提示字符串，迫使模型内化知识而非简单记忆。

Result: 在ALFWorld（家庭任务）和WebShop（在线购物）两个交互式基准测试中，蒸馏后的学生模型始终优于基线代理，在ALFWorld上成功率高达91%（基线为79%），WebShop分数提高到72分（基线为61分）。此外，与检索增强的教师模型相比，学生模型根据环境不同，消耗的token减少了10%至60%。该方法适用于不同的模型规模（7B/14B参数）和代理架构（ReAct/StateAct）。

Conclusion: 研究表明，通过有针对性的微调，可以有效地将检索带来的好处内化到LLM代理中，从而在不增加永久运行时依赖的情况下提高其性能和效率。

Abstract: Large language model (LLM) agents deployed for multi-step tasks frequently
fail in predictable ways: attempting actions with unmet preconditions, issuing
redundant commands, or mishandling environment constraints. While
retrieval-augmented generation (RAG) can improve performance by providing
runtime guidance, it requires maintaining external knowledge databases and adds
computational overhead at every deployment. We propose a simple pipeline that
converts inference-time retrieval into learned competence through distillation.
Our approach: (1) extracts compact, reusable hints from agent failures, (2)
uses these hints to generate improved teacher trajectories via one-shot
retrieval at episode start, and (3) trains student models on these trajectories
with hint strings removed, forcing internalization rather than memorization.
Across two interactive benchmarks, ALFWorld (household tasks) and WebShop
(online shopping), distilled students consistently outperform baseline agents,
achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving
WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens
than retrieval-augmented teachers depending on the environment. The approach
generalizes across model scales (7B/14B parameters) and agent architectures
(ReAct/StateAct), demonstrating that retrieval benefits can be effectively
internalized through targeted fine-tuning without permanent runtime
dependencies.

</details>


### [137] [Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents](https://arxiv.org/abs/2510.01398)
*Yang Liu,Zaid Abulawi,Abhiram Garimidi,Doyeong Lim*

Main category: cs.AI

TL;DR: 本研究提出并验证了一种利用大型语言模型（LLM）代理来自动化数据驱动工程建模与分析（特别是回归任务）的方法，其性能媲美人类专家。


<details>
  <summary>Details</summary>
Motivation: 现代工程对高效、可靠、普适的建模策略需求日益增长，尤其是在处理实验和模拟生成的大量数据集时。传统的基于数据的建模方法常需大量人工干预，难以扩展和泛化。因此，需要开发能有效预测和分析科学数据集的自动化数据驱动方法，特别是神经网络模型。

Method: 本研究提出了一种创新的LLM代理管道，旨在自动化数据驱动的建模和分析，重点关注回归任务。评估了两种LLM代理框架：一个多代理系统（包含专业协作代理）和一个基于ReAct范式的单代理系统。这两种框架均能自主处理数据预处理、神经网络开发、训练、超参数优化和不确定性量化。该方法通过一个临界热通量（CHF）预测基准进行了验证，该基准使用了来自OECD/NEA数据集的约25,000个实验数据点。

Result: 研究结果表明，由LLM代理开发的模型超越了传统的CHF查询表，并在预测精度和不确定性量化方面达到了与人类专家开发的、最先进的贝叶斯优化深度神经网络模型相当的水平。

Conclusion: 这些结果强调了基于LLM的代理在自动化复杂工程建模任务方面的巨大潜力，它能显著减少人工工作量，同时达到或超越现有的预测性能标准。

Abstract: Modern engineering increasingly relies on vast datasets generated by
experiments and simulations, driving a growing demand for efficient, reliable,
and broadly applicable modeling strategies. There is also heightened interest
in developing data-driven approaches, particularly neural network models, for
effective prediction and analysis of scientific datasets. Traditional
data-driven methods frequently involve extensive manual intervention, limiting
their ability to scale effectively and generalize to diverse applications. In
this study, we propose an innovative pipeline utilizing Large Language Model
(LLM) agents to automate data-driven modeling and analysis, with a particular
emphasis on regression tasks. We evaluate two LLM-agent frameworks: a
multi-agent system featuring specialized collaborative agents, and a
single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both
frameworks autonomously handle data preprocessing, neural network development,
training, hyperparameter optimization, and uncertainty quantification (UQ). We
validate our approach using a critical heat flux (CHF) prediction benchmark,
involving approximately 25,000 experimental data points from the OECD/NEA
benchmark dataset. Results indicate that our LLM-agent-developed model
surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ
on par with state-of-the-art Bayesian optimized deep neural network models
developed by human experts. These outcomes underscore the significant potential
of LLM-based agents to automate complex engineering modeling tasks, greatly
reducing human workload while meeting or exceeding existing standards of
predictive performance.

</details>


### [138] [OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models](https://arxiv.org/abs/2510.01409)
*Luca Cotti,Idilio Drago,Anisa Rula,Devis Bianchini,Federico Cerutti*

Main category: cs.AI

TL;DR: OntoLogX利用LLM将非结构化系统日志转换为本体驱动的知识图谱，通过RAG和校正确保数据质量，并预测MITRE ATT&CK策略，从而有效提取可操作的网络威胁情报。


<details>
  <summary>Details</summary>
Motivation: 系统日志是网络威胁情报（CTI）的宝贵来源，但其非结构化、语义不一致及碎片化特性限制了其实用性。因此，亟需一种方法能将嘈杂、异构的日志数据整合为连贯、可互操作的表示形式，以提取可操作的CTI。

Method: 提出OntoLogX，一个自主AI代理，利用大型语言模型（LLMs）将原始日志转换为基于本体的知识图谱（KGs）。该方法整合了轻量级日志本体、检索增强生成（RAG）和迭代校正步骤，以确保生成KG的语法和语义有效性。系统还将KGs聚合成会话，并使用LLM预测MITRE ATT&CK战术，将低级日志证据与高级对抗目标关联。

Result: 在公共基准和真实蜜罐数据集上的评估表明：1. 在多个KG后端上实现了鲁棒的知识图谱生成。2. 准确地将对抗活动映射到ATT&CK战术。3. 检索和校正机制有效提升了精度和召回率。4. 代码导向模型在结构化日志分析中表现出有效性。

Conclusion: 本体驱动的表示形式对于提取可操作的网络威胁情报具有显著价值。通过结合检索增强和迭代校正，能够有效地从复杂日志数据中生成精确、语义有效的知识图谱，并关联到高级威胁策略。

Abstract: System logs represent a valuable source of Cyber Threat Intelligence (CTI),
capturing attacker behaviors, exploited vulnerabilities, and traces of
malicious activity. Yet their utility is often limited by lack of structure,
semantic inconsistency, and fragmentation across devices and sessions.
Extracting actionable CTI from logs therefore requires approaches that can
reconcile noisy, heterogeneous data into coherent and interoperable
representations. We introduce OntoLogX, an autonomous Artificial Intelligence
(AI) agent that leverages Large Language Models (LLMs) to transform raw logs
into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a
lightweight log ontology with Retrieval Augmented Generation (RAG) and
iterative correction steps, ensuring that generated KGs are syntactically and
semantically valid. Beyond event-level analysis, the system aggregates KGs into
sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level
log evidence to higher-level adversarial objectives. We evaluate OntoLogX on
both logs from a public benchmark and a real-world honeypot dataset,
demonstrating robust KG generation across multiple KGs backends and accurate
mapping of adversarial activity to ATT&CK tactics. Results highlight the
benefits of retrieval and correction for precision and recall, the
effectiveness of code-oriented models in structured log analysis, and the value
of ontology-grounded representations for actionable CTI extraction.

</details>


### [139] [A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining](https://arxiv.org/abs/2510.01427)
*Sipeng Zhang,Longfei Yun,Zilong Wang,Jingbo Shang,Letian Peng*

Main category: cs.AI

TL;DR: 本文介绍了Falconer，一个结合大型语言模型（LLMs）的规划能力与轻量级代理模型的高效性，用于可扩展的知识挖掘框架，显著降低成本并提高速度，同时保持指令遵循的准确性。


<details>
  <summary>Details</summary>
Motivation: 深度研究的核心是知识挖掘，但LLMs在此任务中虽然擅长指令理解，却部署成本过高；而传统的分类器和提取器虽然高效，却缺乏泛化能力且不够灵活。因此，需要一种既能利用LLMs的优势又能解决其成本和扩展性问题的方案。

Method: Falconer框架通过LLMs充当“规划者”分解用户指令为可执行流程，并作为“标注者”生成监督数据以训练小型代理模型。该框架将分类和提取统一为“获取标签”和“获取跨度”两个原子操作，使一个指令遵循模型能替代多个特定任务组件。作者还构建了新基准来评估代理模型与人类及大型模型标注之间的一致性。

Result: 实验结果表明，Falconer在指令遵循的准确性上与最先进的LLMs相当，同时将推理成本降低高达90%，并将大规模知识挖掘加速超过20倍。

Conclusion: Falconer为深度研究提供了一个高效且可扩展的基础，有效解决了知识挖掘中LLMs成本高昂和传统方法泛化能力差的问题。

Abstract: At the core of Deep Research is knowledge mining, the task of extracting
structured information from massive unstructured text in response to user
instructions. Large language models (LLMs) excel at interpreting such
instructions but are prohibitively expensive to deploy at scale, while
traditional pipelines of classifiers and extractors remain efficient yet
brittle and unable to generalize to new tasks. We introduce Falconer, a
collaborative framework that combines the agentic reasoning of LLMs with
lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act
as planners, decomposing user instructions into executable pipelines, and as
annotators, generating supervision to train small proxies. The framework
unifies classification and extraction into two atomic operations, get label and
get span, enabling a single instruction-following model to replace multiple
task-specific components. To evaluate the consistency between proxy models
incubated by Falconer and annotations provided by humans and large models, we
construct new benchmarks covering both planning and end-to-end execution.
Experiments show that Falconer closely matches state-of-the-art LLMs in
instruction-following accuracy while reducing inference cost by up to 90% and
accelerating large-scale knowledge mining by more than 20x, offering an
efficient and scalable foundation for Deep Research.

</details>


### [140] [On the Role of Domain Experts in Creating Effective Tutoring Systems](https://arxiv.org/abs/2510.01432)
*Sarath Sreedharan,Kelsey Sikes,Nathaniel Blanchard,Lisa Mason,Nikhil Krishnaswamy,Jill Zarestky*

Main category: cs.AI

TL;DR: 本文强调了领域专家知识在AI教育系统中的重要性，通过利用可解释AI自动生成课程和专家指定的课程大纲来构建高效自适应辅导系统，并以案例研究进行论证。


<details>
  <summary>Details</summary>
Motivation: AI教育社区常忽视领域专家策展知识对创建有效辅导系统的作用。本文旨在突出这一被忽视的主题，探讨专家知识如何帮助构建新型教育系统。

Method: 1. 结合专家指定的规则和新颖的可解释AI（XAI）技术，自动生成学习课程。2. 利用专家指定的课程大纲，开发能提供更优学习体验且算法更高效的自适应辅导系统。3. 通过一个传粉者识别辅导系统的案例研究，强调这些方法的重要性。

Result: 通过结合专家知识，有望实现学习课程的自动化生成，并能开发出提供更好学习体验和更高效算法的自适应辅导系统。案例研究表明此类知识易于获取且方法具有实际应用价值。

Conclusion: 领域专家知识在创建创新且高效的AI教育系统（如自动课程生成和自适应辅导系统）中扮演着关键角色，其重要性不容忽视。

Abstract: The role that highly curated knowledge, provided by domain experts, could
play in creating effective tutoring systems is often overlooked within the AI
for education community. In this paper, we highlight this topic by discussing
two ways such highly curated expert knowledge could help in creating novel
educational systems. First, we will look at how one could use explainable AI
(XAI) techniques to automatically create lessons. Most existing XAI methods are
primarily aimed at debugging AI systems. However, we will discuss how one could
use expert specified rules about solving specific problems along with novel XAI
techniques to automatically generate lessons that could be provided to
learners. Secondly, we will see how an expert specified curriculum for learning
a target concept can help develop adaptive tutoring systems, that can not only
provide a better learning experience, but could also allow us to use more
efficient algorithms to create these systems. Finally, we will highlight the
importance of such methods using a case study of creating a tutoring system for
pollinator identification, where such knowledge could easily be elicited from
experts.

</details>


### [141] [VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning](https://arxiv.org/abs/2510.01444)
*Rui Liu,Dian Yu,Tong Zheng,Runpeng Dai,Zongxia Li,Wenhao Yu,Zhenwen Liang,Linfeng Song,Haitao Mi,Pratap Tokekar,Dong Yu*

Main category: cs.AI

TL;DR: VOGUE通过利用视觉输入的不确定性来指导探索，从而提升多模态大语言模型（MLLMs）的推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）中基于可验证奖励的强化学习（RLVR）存在探索性不足的问题。现有方法将视觉输入视为固定条件，忽视其固有的模糊性，导致策略对视觉变化缺乏鲁棒性。

Method: 本文提出了VOGUE（Visual Uncertainty Guided Exploration），将探索从文本输出转移至视觉输入空间。它将图像视为随机上下文，通过计算“原始”与“噪声”分支间的对称KL散度量化策略对视觉扰动的敏感度，生成不确定性感知探索信号。该信号结合不确定性比例奖励、token熵奖励和退火采样调度，共同塑造学习目标，以平衡探索与利用。

Result: VOGUE在GRPO框架下，于Qwen2.5-VL-3B/7B模型上实现。它在三个视觉数学基准上将pass@1准确率平均提高了2.6%，在三个通用推理基准上平均提高了3.7%。此外，它还提升了pass@4性能，并有效缓解了RL微调中常见的探索衰减问题。

Conclusion: 将探索过程植根于视觉输入固有的不确定性，是提升多模态推理能力的一种有效策略。

Abstract: Reinforcement learning with verifiable rewards (RLVR) improves reasoning in
large language models (LLMs) but struggles with exploration, an issue that
still persists for multimodal LLMs (MLLMs). Current methods treat the visual
input as a fixed, deterministic condition, overlooking a critical source of
ambiguity and struggling to build policies robust to plausible visual
variations. We introduce $\textbf{VOGUE (Visual Uncertainty Guided
Exploration)}$, a novel method that shifts exploration from the output (text)
to the input (visual) space. By treating the image as a stochastic context,
VOGUE quantifies the policy's sensitivity to visual perturbations using the
symmetric KL divergence between a "raw" and "noisy" branch, creating a direct
signal for uncertainty-aware exploration. This signal shapes the learning
objective via an uncertainty-proportional bonus, which, combined with a
token-entropy bonus and an annealed sampling schedule, effectively balances
exploration and exploitation. Implemented within GRPO on two model scales
(Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three
visual math benchmarks and 3.7% on three general-domain reasoning benchmarks,
while simultaneously increasing pass@4 performance and mitigating the
exploration decay commonly observed in RL fine-tuning. Our work shows that
grounding exploration in the inherent uncertainty of visual inputs is an
effective strategy for improving multimodal reasoning.

</details>


### [142] [AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance](https://arxiv.org/abs/2510.01474)
*Bill Marino,Rosco Hunter,Zubair Jamali,Marinos Emmanouil Kalpakos,Mudra Kashyap,Isaiah Hinton,Alexa Hanson,Maahum Nazir,Christoph Schnabl,Felix Steffek,Hongkai Wen,Nicholas D. Lane*

Main category: cs.AI

TL;DR: 引入AIReg-Bench，首个用于评估LLM对AI法规（如欧盟AI法案）合规性评估能力的基准数据集。


<details>
  <summary>Details</summary>
Motivation: 随着各国政府开始监管AI，使用大型语言模型（LLM）评估AI系统是否符合特定AI法规（AIR）的兴趣日益增长。然而，目前缺乏衡量LLM在此任务上表现的基准，本研究旨在填补这一空白。

Method: 本研究通过两步创建了AIReg-Bench数据集：1) 使用精心设计的指令提示LLM，生成120个虚构但逼真的AI系统技术文档摘录样本；2) 法律专家审查并标注每个样本，指出其中描述的AI系统是否以及如何违反了欧盟AI法案的具体条款。

Result: 创建了AIReg-Bench数据集，并对前沿LLM是否能重现专家合规性标注进行了评估。该结果为理解基于LLM的AI合规性评估工具的机遇和局限性提供了初步认识。

Conclusion: 本研究为理解基于LLM的AI合规性评估工具的潜力和局限性提供了起点，并建立了一个基准，后续的LLM可据此进行比较。数据集和评估代码已公开。

Abstract: As governments move to regulate AI, there is growing interest in using Large
Language Models (LLMs) to assess whether or not an AI system complies with a
given AI Regulation (AIR). However, there is presently no way to benchmark the
performance of LLMs at this task. To fill this void, we introduce AIReg-Bench:
the first benchmark dataset designed to test how well LLMs can assess
compliance with the EU AI Act (AIA). We created this dataset through a two-step
process: (1) by prompting an LLM with carefully structured instructions, we
generated 120 technical documentation excerpts (samples), each depicting a
fictional, albeit plausible, AI system - of the kind an AI provider might
produce to demonstrate their compliance with AIR; (2) legal experts then
reviewed and annotated each sample to indicate whether, and in what way, the AI
system described therein violates specific Articles of the AIA. The resulting
dataset, together with our evaluation of whether frontier LLMs can reproduce
the experts' compliance labels, provides a starting point to understand the
opportunities and limitations of LLM-based AIR compliance assessment tools and
establishes a benchmark against which subsequent LLMs can be compared. The
dataset and evaluation code are available at
https://github.com/camlsys/aireg-bench.

</details>


### [143] [Lateral Tree-of-Thoughts Surpasses ToT by Incorporating Logically-Consistent, Low-Utility Candidates](https://arxiv.org/abs/2510.01500)
*Abhinav Madahar*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Modern deployments increasingly allocate large test-time compute (thousands
of tokens or many node expansions) to boost reliability. Under such budgets,
standard Tree-of-Thoughts-style search exhibits two pathologies: breadth
saturation (additional samples mostly produce near-duplicates, so width stops
growing) and depth myopia (noisy short-horizon utilities prune branches whose
payoff appears after a few more steps). We propose Lateral Tree-of-Thoughts
(LToT), a drop-in controller that separates utility from logical consistency
and treats low-utility but consistent candidates as assets rather than waste.
The frontier is split into mainlines (high-utility candidates used for
exploitation) and laterals (consistent, initially low-utility candidates that
receive short, cheap probes before judgment). LToT explores laterals via
Lateral Racing with Short-Circuit (LR--SC): a capped successive-halving race
that spreads tiny probes across a very wide lateral set, uses width-aware
thresholds with repeat-to-confirm, and immediately promotes a branch once its
envelope clears the mainline bar; mainlines are kept intentionally narrow so
surplus compute is invested where width is cheap. We prove a pseudolinear
lateral cost $\Theta(N_0 \log_{\eta} N_0)$ with logarithmically many rungs
(initial lateral width $N_0$; culling factor $\eta>1$), in contrast to the
exponential growth of uncapped mainlines. Empirical evaluations on benchmark
tasks are in preparation and will be added in a future revision. In short, LToT
turns large test-time budgets into principled diversity while preserving
promotion discipline, mitigating saturation and myopia without inflating
compute.

</details>


### [144] [Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation](https://arxiv.org/abs/2510.01528)
*Daniel Zhao,Abhilash Shankarampeta,Lanxiang Hu,Tajana Rosing,Hao Zhang*

Main category: cs.AI

TL;DR: 该研究提出了一种新方法，利用稀疏自编码器（SAEs）和聚类技术分析大型语言模型（LLMs）的内部 token 表示，并通过平衡利用（exploitation）与探索（exploration）来指导数学推理任务的生成。


<details>
  <summary>Details</summary>
Motivation: 分析LLMs的内部 token 表示，并解决在数学推理任务中如何有效引导生成，以提高准确性和推理质量。

Method: 首先训练一个SAE生成稀疏向量表示，然后应用k-means聚类构建一个图，其中顶点是token簇，边捕获序列token转换。基于该图定义一个边权重奖励函数来量化推理轨迹的遵循程度（利用），并从聚类中测量生成多样性（探索）。SAE在生成过程中作为可扩展的奖励模型来指导生成。

Result: 研究发现，在数学推理任务中，平衡利用和探索对于实现高准确性至关重要。SAE可以作为一种可扩展的奖励模型来引导生成，确保在利用和探索之间取得平衡，从而避免极端行为。

Conclusion: 通过SAE引导的生成，平衡利用与探索，能够促进LLMs在数学推理任务中形成更高质量的推理过程。

Abstract: We propose a novel method that leverages sparse autoencoders (SAEs) and
clustering techniques to analyze the internal token representations of large
language models (LLMs) and guide generations in mathematical reasoning tasks.
Our approach first trains an SAE to generate sparse vector representations for
training tokens, then applies k-means clustering to construct a graph where
vertices represent token clusters and weighted edges capture sequential token
transitions. Using this graph, we define an edge-weight based reward function
to quantify adherence to established reasoning traces, thereby identifying
exploitative reasoning trajectories. Additionally, we measure generation
diversity from clustering to assess the extent of exploration. Our findings
indicate that balancing both exploitation and exploration is crucial for
achieving high accuracy in mathematical reasoning tasks. During generation, the
SAE can serve as a scalable reward model to guide generations, ensuring a
balanced trade-off between exploitation and exploration. This prevents extreme
behaviors in either direction, ultimately fostering a higher-quality reasoning
process in LLMs.

</details>


### [145] [LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning](https://arxiv.org/abs/2510.01530)
*Navapat Nananukul,Yue Zhang,Ryan Lee,Eric Boxer,Jonathan May,Vibhav Giridhar Gogate,Jay Pujara,Mayank Kejriwal*

Main category: cs.AI

TL;DR: LogT是一种神经符号架构，结合逻辑语言与LLM，显著提升了LLM在法律、医学等高保障领域对可废止推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 高保障领域（如法律、医学）的推理需要准确、可验证且有证据支持的结论。这类推理涉及大量带有例外情况的可废止逻辑，给现有大语言模型（LLMs）带来了挑战，尤其在处理否定、蕴涵和可废止规则时表现不足。

Method: 提出一种名为LOGicalThought (LogT) 的神经符号架构。该架构结合了先进的逻辑语言、推理器与大语言模型，构建了双重符号图上下文和基于逻辑的上下文，将对长篇指南的推理问题转化为紧凑的接地评估。

Result: 在四个多领域基准测试中，LogT相对于基线模型，使LLM的整体性能提升了11.84%。在三种推理模式中均有显著提升：否定推理提升高达10.2%，蕴涵推理提升13.2%，可废止推理提升5.5%。

Conclusion: LogT架构通过整合符号逻辑与LLM，有效解决了高保障文本中复杂的可废止推理挑战，显著提升了LLM在关键领域（如法律、医学）推理的准确性和可靠性。

Abstract: High-assurance reasoning, particularly in critical domains such as law and
medicine, requires conclusions that are accurate, verifiable, and explicitly
grounded in evidence. This reasoning relies on premises codified from rules,
statutes, and contracts, inherently involving defeasible or non-monotonic logic
due to numerous exceptions, where the introduction of a single fact can
invalidate general rules, posing significant challenges. While large language
models (LLMs) excel at processing natural language, their capabilities in
standard inference tasks do not translate to the rigorous reasoning required
over high-assurance text guidelines. Core reasoning challenges within such
texts often manifest specific logical structures involving negation,
implication, and, most critically, defeasible rules and exceptions. In this
paper, we propose a novel neurosymbolically-grounded architecture called
LOGicalThought (LogT) that uses an advanced logical language and reasoner in
conjunction with an LLM to construct a dual symbolic graph context and
logic-based context. These two context representations transform the problem
from inference over long-form guidelines into a compact grounded evaluation.
Evaluated on four multi-domain benchmarks against four baselines, LogT improves
overall performance by 11.84% across all LLMs. Performance improves
significantly across all three modes of reasoning: by up to +10.2% on negation,
+13.2% on implication, and +5.5% on defeasible reasoning compared to the
strongest baseline.

</details>


### [146] [Information Seeking for Robust Decision Making under Partial Observability](https://arxiv.org/abs/2510.01531)
*Djengo Cyun-Jyun Fang,Tsung-Wei Ke*

Main category: cs.AI

TL;DR: 本文提出InfoSeeker，一个将任务规划与主动信息探索结合的LLM决策框架，以校准内部动态并在不确定环境中做出最优决策。实验表明InfoSeeker在性能上显著优于现有方法，并在部分可观测环境中表现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）规划代理在处理观测不确定性时，常忽略其内部动态与实际环境的差异，导致决策效果不佳。

Method: 引入InfoSeeker框架，该框架促使LLM通过规划信息探索动作来主动收集信息，以验证理解、检测环境变化或测试假设，进而在生成或修订面向任务的计划前校准其内部动态。为评估InfoSeeker，还提出了一个包含部分可观测环境的新基准套件。

Result: InfoSeeker比现有方法实现了74%的绝对性能提升，且未牺牲样本效率。它能泛化到不同的LLM，并在机器人操作和网页导航等既有基准上超越基线表现。

Conclusion: 研究结果强调了在部分可观测环境中，紧密整合规划与信息探索对于实现稳健行为的重要性。

Abstract: Explicit information seeking is essential to human problem-solving in
practical environments characterized by incomplete information and noisy
dynamics. When the true environmental state is not directly observable, humans
seek information to update their internal dynamics and inform future
decision-making. Although existing Large Language Model (LLM) planning agents
have addressed observational uncertainty, they often overlook discrepancies
between their internal dynamics and the actual environment. We introduce
Information Seeking Decision Planner (InfoSeeker), an LLM decision-making
framework that integrates task-oriented planning with information seeking to
align internal dynamics and make optimal decisions under uncertainty in both
agent observations and environmental dynamics. InfoSeeker prompts an LLM to
actively gather information by planning actions to validate its understanding,
detect environmental changes, or test hypotheses before generating or revising
task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark
suite featuring partially observable environments with incomplete observations
and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%
absolute performance gain over prior methods without sacrificing sample
efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms
baselines on established benchmarks such as robotic manipulation and web
navigation. These findings underscore the importance of tightly integrating
planning and information seeking for robust behavior in partially observable
environments. The project page is available at https://infoseekerllm.github.io

</details>


### [147] [Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models](https://arxiv.org/abs/2510.01544)
*Shaoan Xie,Lingjing Kong,Xiangchen Song,Xinshuai Dong,Guangyi Chen,Eric P. Xing,Kun Zhang*

Main category: cs.AI

TL;DR: 针对扩散语言模型(dLLMs)在复杂推理训练中现有RL方法易强化错误推理路径的问题，本文提出了一个将复杂问题形式化为分层选择过程的理论框架。基于此，引入了SAPO算法，通过过程导向的奖励函数，引导dLLM学习结构化推理路径，显著提升了推理性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（dLLMs）在文本生成方面具有潜力，但在复杂推理任务的训练上仍面临挑战。现有强化学习方法多依赖稀疏的、基于最终结果的奖励，这可能错误地强化有缺陷的推理路径。作者认为这是由于现有方法与推理的自然分层结构不匹配，导致模型出现“非结构化精炼”的失败模式。

Method: 首先提出了一个理论框架，将复杂问题解决形式化为分层选择过程，分解为一系列简单的局部逻辑步骤，并提供了其潜在推理结构可识别性的理论见解。受此理论启发，识别出现有方法中“非结构化精炼”的缺陷。进而，引入了“Step-Aware Policy Optimization (SAPO)”这一新型强化学习算法，通过使用基于过程的奖励函数来鼓励渐进式进步，使dLLM的去噪过程与潜在推理层级对齐，以学习结构化、连贯的推理路径。

Result: 经验结果表明，SAPO这一有原则的方法显著提升了模型在具有挑战性的推理基准上的性能，并增强了生成过程的可解释性。

Conclusion: 通过提出分层推理理论框架和基于过程奖励的SAPO算法，该研究为dLLMs在复杂推理任务中的训练提供了有效且可解释的解决方案，克服了现有RL方法中稀疏奖励的局限性。

Abstract: Diffusion language models (dLLMs) offer a promising, non-autoregressive
paradigm for text generation, yet training them for complex reasoning remains a
key challenge. Current reinforcement learning approaches often rely on sparse,
outcome-based rewards, which can reinforce flawed reasoning paths that lead to
coincidentally correct answers. We argue that this stems from a fundamental
mismatch with the natural structure of reasoning. We first propose a
theoretical framework that formalizes complex problem solving as a hierarchical
selection process, where an intractable global constraint is decomposed into a
series of simpler, localized logical steps. This framework provides a
principled foundation for algorithm design, including theoretical insights into
the identifiability of this latent reasoning structure. Motivated by this
theory, we identify unstructured refinement -- a failure mode where a model's
iterative steps do not contribute meaningfully to the solution -- as a core
deficiency in existing methods. We then introduce Step-Aware Policy
Optimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising
process with the latent reasoning hierarchy. By using a process-based reward
function that encourages incremental progress, SAPO guides the model to learn
structured, coherent reasoning paths. Our empirical results show that this
principled approach significantly improves performance on challenging reasoning
benchmarks and enhances the interpretability of the generation process.

</details>


### [148] [InvThink: Towards AI Safety via Inverse Reasoning](https://arxiv.org/abs/2510.01569)
*Yubin Kim,Taehan Kim,Eugene Park,Chunjong Park,Cynthia Breazeal,Daniel McDuff,Hae Won Park*

Main category: cs.AI

TL;DR: InvThink是一种新方法，让大型语言模型在生成响应前通过“逆向思维”预先推理潜在的失败模式，从而显著提升模型安全性，同时保持通用推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法直接优化安全响应，但InvThink旨在让大语言模型具备逆向思维能力，即在生成响应前预先推理潜在的失败模式，以实现更有效、更通用的安全模型。

Method: InvThink通过指令模型执行以下三步：1) 列举潜在危害，2) 分析其后果，3) 生成主动避免这些风险的安全输出。该方法通过监督微调和强化学习在多个LLM家族中实现。

Result: 研究发现：(i) 安全性提升随模型规模增长，且其效果比现有安全方法更显著。(ii) InvThink减轻了“安全税”，保持了模型在标准基准上的通用推理能力。(iii) 在高风险领域（如医学、金融、法律及恶意场景）表现出色，相比基线方法减少了高达15.7%的有害响应。

Conclusion: 逆向推理（InvThink）为实现更安全、更有能力的大语言模型提供了一条可扩展且通用的路径。

Abstract: We present InvThink, a simple yet powerful approach that gives large language
models (LLMs) the capability of inverse thinking: reasoning through failure
modes before generating responses. Unlike existing safety alignment methods
that optimize directly for safe response, InvThink instructs models to 1)
enumerate potential harms, 2) analyze their consequences, and 3) generate safe
outputs that proactively avoid these risks. Our method reveals three key
findings: (i) safety improvements show stronger scaling with model size
compared to existing safety methods. (ii) InvThink mitigates safety tax; by
training models to systematically consider failure modes, it preserves general
reasoning capabilities on standard benchmarks. (iii) beyond general safety
tasks, InvThink excels in high-stakes domains including external-facing
(medicine, finance, law) and agentic (blackmail, murder) risk scenarios,
achieving up to 15.7% reduction in harmful responses compared to baseline
methods like SafetyPrompt. We further implement InvThink via supervised
fine-tuning, and reinforcement learning across three LLM families. These
results suggest that inverse reasoning provides a scalable and generalizable
path toward safer, more capable language models.

</details>


### [149] [AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.01586)
*Zhenyu Pan,Yiting Zhang,Zhuo Liu,Yolo Yunlong Tang,Zeliang Zhang,Haozheng Luo,Yuwei Han,Jianshu Zhang,Dennis Wu,Hong-Yu Chen,Haoran Lu,Haoyang Fang,Manling Li,Chenliang Xu,Philip S. Yu,Han Liu*

Main category: cs.AI

TL;DR: 多智能体LLM系统易受攻击。本文提出AdvEvo-MARL框架，通过协同进化多智能体强化学习将安全性内置到任务智能体中，显著降低攻击成功率并保持任务性能，无需额外防护模块或开销。


<details>
  <summary>Details</summary>
Motivation: LLM多智能体系统在规划、工具使用和角色协调方面表现出色，但其开放性和复杂性使其面临越狱、提示注入等安全风险。现有防御机制（如自我验证和外部防护模块）效果不佳：自我验证缺乏检测跨智能体风险的能力；外部防护模块增加系统开销，并构成单点故障，一旦被攻破将导致系统级安全崩溃。

Method: 本文提出AdvEvo-MARL，一个协同进化的多智能体强化学习框架，旨在将安全性内置到任务智能体中，而非依赖外部防护。该方法在对抗性学习环境中共同优化攻击者（生成进化越狱提示）和防御者（训练来完成任务并抵御攻击）。为稳定学习并促进智能体间合作，引入了公共基线进行优势估计，即同一功能组内的智能体共享组级平均回报基线，以实现低方差更新和更强的组内协调。

Result: 在代表性攻击场景中，AdvEvo-MARL能持续将攻击成功率（ASR）保持在20%以下，远低于基线方法的38.33%。同时，它在保持任务准确性的基础上，甚至在推理任务上将性能提高了高达3.67%。

Conclusion: AdvEvo-MARL框架证明了可以在不依赖额外防护智能体或增加系统开销的情况下，同时提高多智能体系统的安全性和实用性。

Abstract: LLM-based multi-agent systems excel at planning, tool use, and role
coordination, but their openness and interaction complexity also expose them to
jailbreak, prompt-injection, and adversarial collaboration. Existing defenses
fall into two lines: (i) self-verification that asks each agent to pre-filter
unsafe instructions before execution, and (ii) external guard modules that
police behaviors. The former often underperforms because a standalone agent
lacks sufficient capacity to detect cross-agent unsafe chains and
delegation-induced risks; the latter increases system overhead and creates a
single-point-of-failure-once compromised, system-wide safety collapses, and
adding more guards worsens cost and complexity. To solve these challenges, we
propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning
framework that internalizes safety into task agents. Rather than relying on
external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize
evolving jailbreak prompts) and defenders (task agents trained to both
accomplish their duties and resist attacks) in adversarial learning
environments. To stabilize learning and foster cooperation, we introduce a
public baseline for advantage estimation: agents within the same functional
group share a group-level mean-return baseline, enabling lower-variance updates
and stronger intra-group coordination. Across representative attack scenarios,
AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas
baselines reach up to 38.33%, while preserving-and sometimes improving-task
accuracy (up to +3.67% on reasoning tasks). These results show that safety and
utility can be jointly improved without relying on extra guard agents or added
system overhead.

</details>


### [150] [AgentRec: Next-Generation LLM-Powered Multi-Agent Collaborative Recommendation with Adaptive Intelligence](https://arxiv.org/abs/2510.01609)
*Bo Ma,Hang Li,ZeHua Hu,XiaoFan Gui,LuYao Liu,Simon Lau*

Main category: cs.AI

TL;DR: 本文提出AgentRec，一个基于LLM的多智能体协作推荐框架，通过分层智能体网络和自适应智能解决现有对话推荐系统在处理动态用户偏好、会话连贯性和多目标平衡方面的挑战，并在多项指标上超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的交互式对话推荐系统在处理动态用户偏好、维护会话连贯性以及同时平衡多个排名目标方面面临巨大挑战。

Method: 引入AgentRec，一个由LLM驱动的下一代多智能体协作推荐框架，通过分层智能体网络和自适应智能解决上述局限性。该方法采用专门的LLM驱动智能体进行会话理解、偏好建模、上下文感知和动态排名，并通过学习交互模式的自适应权重机制进行协调。同时，提出了一种三层学习策略，结合了简单查询的快速响应、复杂偏好的智能推理以及挑战场景的深度协作。

Result: 在三个真实世界数据集上的实验表明，AgentRec在会话成功率方面提高了2.8%，推荐准确性（NDCG@10）提高了1.9%，会话效率提高了3.2%，且通过智能智能体协调保持了可比的计算成本，始终优于现有先进基线。

Conclusion: AgentRec通过其创新的LLM驱动多智能体协作框架和自适应智能，成功解决了对话推荐系统的关键挑战，并在多项核心性能指标上实现了显著提升，同时保持了计算效率。

Abstract: Interactive conversational recommender systems have gained significant
attention for their ability to capture user preferences through natural
language interactions. However, existing approaches face substantial challenges
in handling dynamic user preferences, maintaining conversation coherence, and
balancing multiple ranking objectives simultaneously. This paper introduces
AgentRec, a next-generation LLM-powered multi-agent collaborative
recommendation framework that addresses these limitations through hierarchical
agent networks with adaptive intelligence. Our approach employs specialized
LLM-powered agents for conversation understanding, preference modeling, context
awareness, and dynamic ranking, coordinated through an adaptive weighting
mechanism that learns from interaction patterns. We propose a three-tier
learning strategy combining rapid response for simple queries, intelligent
reasoning for complex preferences, and deep collaboration for challenging
scenarios. Extensive experiments on three real-world datasets demonstrate that
AgentRec achieves consistent improvements over state-of-the-art baselines, with
2.8\% enhancement in conversation success rate, 1.9\% improvement in
recommendation accuracy (NDCG@10), and 3.2\% better conversation efficiency
while maintaining comparable computational costs through intelligent agent
coordination.

</details>


### [151] [PychoBench: Evaluating the Psychology Intelligence of Large Language Models](https://arxiv.org/abs/2510.01611)
*Min Zeng*

Main category: cs.AI

TL;DR: 本文通过构建基于美国国家心理咨询师资格考试的PsychoBench基准，评估了大型语言模型（LLMs）在心理咨询领域应用的潜力。结果显示，前沿LLMs（如GPT-4o）能通过考试，而小型开源模型则不能，表明只有顶尖LLMs目前能达到心理咨询知识标准。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在生成能力上取得了巨大成功，但它们在需要认知能力的心理咨询等应用中的潜力尚未被充分开发。本研究旨在探讨LLMs是否能有效地应用于心理咨询，具体而言，是否具备通过心理咨询师资格考试所需的知识水平。

Method: 为了评估LLM的资格，研究引入了PsychoBench基准测试。该基准以美国国家心理咨询师资格考试（NCE）为基础，包含约2,252道精心挑选的单选题，旨在全面评估LLM的心理学深度理解和广博知识。通过70%的准确率作为及格线来衡量模型的表现。

Result: 评估结果表明，GPT-4o、Llama3.3-70B和Gemma3-27B等先进模型显著超过了及格线。然而，Qwen2.5-7B和Mistral-7B等小型开源模型则远低于及格线。

Conclusion: 研究得出结论，目前只有前沿的LLMs能够达到心理咨询考试的标准。这既揭示了开发面向心理学LLMs的巨大潜力，也突出了该领域面临的挑战。

Abstract: Large Language Models (LLMs) have demonstrated remarkable success across a
wide range of industries, primarily due to their impressive generative
abilities. Yet, their potential in applications requiring cognitive abilities,
such as psychological counseling, remains largely untapped. This paper
investigates the key question: Can LLMs be effectively applied to psychological
counseling? To determine whether an LLM can effectively take on the role of a
psychological counselor, the first step is to assess whether it meets the
qualifications required for such a role, namely the ability to pass the U.S.
National Counselor Certification Exam (NCE). This is because, just as a human
counselor must pass a certification exam to practice, an LLM must demonstrate
sufficient psychological knowledge to meet the standards required for such a
role. To address this, we introduce PsychoBench, a benchmark grounded in
U.S.national counselor examinations, a licensure test for professional
counselors that requires about 70% accuracy to pass. PsychoBench comprises
approximately 2,252 carefully curated single-choice questions, crafted to
require deep understanding and broad enough to cover various sub-disciplines of
psychology. This benchmark provides a comprehensive assessment of an LLM's
ability to function as a counselor. Our evaluation shows that advanced models
such as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing
threshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B)
remain far below it. These results suggest that only frontier LLMs are
currently capable of meeting counseling exam standards, highlighting both the
promise and the challenges of developing psychology-oriented LLMs.

</details>


### [152] [Learning to Decide with Just Enough: Information-Theoretic Context Summarization for CDMPs](https://arxiv.org/abs/2510.01620)
*Peidong Liu,Junjiang Lin,Shaowen Wang,Yao Xu,Haiqing Li,Xuhao Xie,Siyi Wu,Hao Li*

Main category: cs.AI

TL;DR: 该研究提出利用大型语言模型（LLM）进行信息论摘要，以压缩上下文马尔可夫决策过程（CMDP）中的高维上下文，从而提高决策效率和性能，并首次给出了遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有CMDP方法在高维或非结构化上下文中泛化能力差，导致计算量过大和性能不稳定。

Method: 提出一种信息论总结方法，利用LLM将上下文输入压缩成低维、语义丰富的摘要来增强状态。此外，该方法基于近似上下文充分性，首次提供了CMDP的遗憾界和延迟-熵权衡特性。

Result: 在离散、连续、视觉和推荐等多个基准测试中，所提方法优于原始上下文和非上下文基线，提高了奖励、成功率和样本效率，同时降低了延迟和内存使用。

Conclusion: 基于LLM的摘要为上下文丰富、资源受限环境中的高效决策提供了一种可扩展且可解释的解决方案。

Abstract: Contextual Markov Decision Processes (CMDPs) offer a framework for sequential
decision-making under external signals, but existing methods often fail to
generalize in high-dimensional or unstructured contexts, resulting in excessive
computation and unstable performance. We propose an information-theoretic
summarization approach that uses large language models (LLMs) to compress
contextual inputs into low-dimensional, semantically rich summaries. These
summaries augment states by preserving decision-critical cues while reducing
redundancy. Building on the notion of approximate context sufficiency, we
provide, to our knowledge, the first regret bounds and a latency-entropy
trade-off characterization for CMDPs. Our analysis clarifies how
informativeness impacts computational cost. Experiments across discrete,
continuous, visual, and recommendation benchmarks show that our method
outperforms raw-context and non-context baselines, improving reward, success
rate, and sample efficiency, while reducing latency and memory usage. These
findings demonstrate that LLM-based summarization offers a scalable and
interpretable solution for efficient decision-making in context-rich,
resource-constrained environments.

</details>


### [153] [Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective](https://arxiv.org/abs/2510.01639)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: 该研究探索了大型语言模型（LLMs）的地理空间推理能力，发现它们能够阅读路网地图并进行导航，在轨迹恢复任务上表现优于现有基线，并能根据用户偏好提升导航体验，但也存在系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）的地理空间推理能力，特别是它们是否能够阅读路网地图并执行导航任务。

Method: 将轨迹恢复作为代理任务，要求模型重建被遮蔽的GPS轨迹。为此，引入了包含4000多条真实世界轨迹的GLOBALTRACE数据集。通过一个提示框架，使用路网作为上下文，使LLMs无需外部导航工具即可生成有效路径。

Result: LLMs在轨迹恢复任务上表现优于现成的基线模型和专门的轨迹恢复模型，并展现出强大的零样本泛化能力。详细分析显示LLMs对路网和坐标系统有很强的理解，但在区域和交通模式上存在系统性偏差。

Conclusion: LLMs能够通过灵活的地图推理方式，结合用户偏好，增强导航体验。

Abstract: We explore the geospatial reasoning capabilities of Large Language Models
(LLMs), specifically, whether LLMs can read road network maps and perform
navigation. We frame trajectory recovery as a proxy task, which requires models
to reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with
over 4,000 real-world trajectories across diverse regions and transportation
modes. Using road network as context, our prompting framework enables LLMs to
generate valid paths without accessing any external navigation tools.
Experiments show that LLMs outperform off-the-shelf baselines and specialized
trajectory recovery models, with strong zero-shot generalization. Fine-grained
analysis shows that LLMs have strong comprehension of the road network and
coordinate systems, but also pose systematic biases with respect to regions and
transportation modes. Finally, we demonstrate how LLMs can enhance navigation
experiences by reasoning over maps in flexible ways to incorporate user
preferences.

</details>


### [154] [GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents](https://arxiv.org/abs/2510.01664)
*Yejin Kim,Youngbin Lee,Juhyeong Kim,Yongjae Lee*

Main category: cs.AI

TL;DR: 本研究展示了GuruAgents（提示词引导的AI代理）能够系统化地操作传奇投资大师的策略，并通过回测验证了其有效性，其中巴菲特GuruAgent表现最佳。


<details>
  <summary>Details</summary>
Motivation: 将投资大师的定性哲学转化为可重现、量化的自动化投资策略。

Method: 开发了五个不同的GuruAgents，通过LLM提示词编码了每位投资大师的独特哲学，并整合了金融工具和确定性推理流程。在NASDAQ-100成分股上进行了从2023年第四季度到2025年第二季度的回测。

Result: GuruAgents展现出由其提示词人格驱动的独特行为。巴菲特GuruAgent取得了最高性能，年复合增长率（CAGR）达42.2%，显著优于基准，其他代理表现各异。

Conclusion: 提示工程能够成功地将投资大师的定性哲学转化为可重现的量化策略，为自动化系统投资指明了一个新方向。

Abstract: This study demonstrates that GuruAgents, prompt-guided AI agents, can
systematically operationalize the strategies of legendary investment gurus. We
develop five distinct GuruAgents, each designed to emulate an iconic investor,
by encoding their distinct philosophies into LLM prompts that integrate
financial tools and a deterministic reasoning pipeline. In a backtest on
NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique
behaviors driven by their prompted personas. The Buffett GuruAgent achieves the
highest performance, delivering a 42.2\% CAGR that significantly outperforms
benchmarks, while other agents show varied results. These findings confirm that
prompt engineering can successfully translate the qualitative philosophies of
investment gurus into reproducible, quantitative strategies, highlighting a
novel direction for automated systematic investing. The source code and data
are available at https://github.com/yejining99/GuruAgents.

</details>


### [155] [Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness](https://arxiv.org/abs/2510.01670)
*Erfan Shayegani,Keegan Hines,Yue Dong,Nael Abu-Ghazaleh,Roman Lutz,Spencer Whitehead,Vidhisha Balachandran,Besmira Nushi,Vibhav Vineet*

Main category: cs.AI

TL;DR: 研究发现计算机使用代理（CUAs）存在“盲目目标导向性”（BGD），即不顾可行性、安全性或上下文地追求目标。通过开发BLIND-ACT基准测试，评估了九个前沿模型，发现平均BGD率高达80.8%，揭示了即使无害输入也可能带来的风险，并强调需更强干预措施来确保CUA安全。


<details>
  <summary>Details</summary>
Motivation: 识别并量化计算机使用代理（CUAs）中普遍存在的“盲目目标导向性”（BGD）问题，即代理不顾可行性、安全性、可靠性或上下文盲目追求目标，从而暴露潜在风险。

Method: 开发了BLIND-ACT基准测试，包含90个任务，用于捕捉BGD的三种模式（缺乏上下文推理、歧义下的假设和决策、矛盾或不可行目标）。该基准测试基于OSWorld提供真实环境，并使用LLM评估器，与人类标注一致性达93.75%。

Result: 在评估九个前沿模型（包括Claude Sonnet/Opus 4, Computer-Use-Preview, GPT-5）时，观察到平均80.8%的高BGD率。研究表明BGD即使在输入无害时也会带来微妙风险。基于提示的干预措施虽能降低BGD水平，但仍存在实质性风险。定性分析揭示了“执行优先偏差”、“思维-行动脱节”和“请求优先”等失败模式。

Conclusion: 识别并定义了计算机使用代理的盲目目标导向性（BGD）这一基本风险，并通过引入BLIND-ACT基准测试，为未来研究和缓解这一风险、确保CUA的安全部署奠定了基础，强调需要更强的训练或推理时干预措施。

Abstract: Computer-Use Agents (CUAs) are an increasingly deployed class of agents that
take actions on GUIs to accomplish user goals. In this paper, we show that CUAs
consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals
regardless of feasibility, safety, reliability, or context. We characterize
three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii)
assumptions and decisions under ambiguity, and (iii) contradictory or
infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these
three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and
employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement
with human annotations. We use BLIND-ACT to evaluate nine frontier models,
including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing
high average BGD rates (80.8%) across them. We show that BGD exposes subtle
risks that arise even when inputs are not directly harmful. While
prompting-based interventions lower BGD levels, substantial risk persists,
highlighting the need for stronger training- or inference-time interventions.
Qualitative analysis reveals observed failure modes: execution-first bias
(focusing on how to act over whether to act), thought-action disconnect
(execution diverging from reasoning), and request-primacy (justifying actions
due to user request). Identifying BGD and introducing BLIND-ACT establishes a
foundation for future research on studying and mitigating this fundamental risk
and ensuring safe CUA deployment.

</details>


### [156] [A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation](https://arxiv.org/abs/2510.01671)
*Motoki Sato,Yuki Matsushita,Hidekazu Takahashi,Tomoaki Kakazu,Sou Nagata,Mizuho Ohnuma,Atsushi Yoshikawa,Masayuki Yamamura*

Main category: cs.AI

TL;DR: LENOHA是一个安全至上的本地优先系统，通过高精度分类器和临床医生整理的常见问题解答，为患者提供预操作问题的准确答案，避免自由文本生成，实现隐私、可持续性和公平部署。


<details>
  <summary>Details</summary>
Motivation: 患者在侵入性手术前常有未解答的问题，但时间紧张的工作流程和隐私限制阻碍了个性化咨询。

Method: 开发了LENOHA（低能耗、无幻觉、不遗漏架构）系统。该系统利用高精度句向量分类器路由输入，并从临床医生整理的FAQ中返回逐字答案，避免临床路径中的自由文本生成。在拔牙和胃镜检查两个领域，使用专家评审的验证集和独立测试集进行评估，比较了不同编码器（如E5-large-instruct）的性能，并记录了能源消耗和延迟。

Result: E5-large-instruct（560M）达到了0.983的总准确率（95% CI 0.964-0.991）和0.996的AUC，统计学上与GPT-4o在该任务上无显著差异；Gemini在此测试集上未犯任何错误。非生成性临床路径每次输入消耗约1.0 mWh，而本地8B SLM的小对话回复消耗约168 mWh，能耗相差约170倍，同时在单个本地GPU上保持约0.10秒的延迟。

Conclusion: 研究结果表明，通过逐字返回经过验证的FAQ答案，临床路径结构性地避免了近前沿判别和生成引起的错误，从而支持了隐私、可持续性以及在带宽受限环境中的公平部署。

Abstract: Patients awaiting invasive procedures often have unanswered pre-procedural
questions; however, time-pressured workflows and privacy constraints limit
personalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave
No One Behind Architecture), a safety-first, local-first system that routes
inputs with a high-precision sentence-transformer classifier and returns
verbatim answers from a clinician-curated FAQ for clinical queries, eliminating
free-text generation in the clinical path. We evaluated two domains (tooth
extraction and gastroscopy) using expert-reviewed validation sets
(n=400/domain) for thresholding and independent test sets (n=200/domain). Among
the four encoders, E5-large-instruct (560M) achieved an overall accuracy of
0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were
statistically indistinguishable from GPT-4o on this task; Gemini made no errors
on this test set. Energy logging shows that the non-generative clinical path
consumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local
8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single
on-prem GPU. These results indicate that near-frontier discrimination and
generation-induced errors are structurally avoided in the clinical path by
returning vetted FAQ answers verbatim, supporting privacy, sustainability, and
equitable deployment in bandwidth-limited environments.

</details>


### [157] [Improving AGI Evaluation: A Data Science Perspective](https://arxiv.org/abs/2510.01687)
*John Hawkins*

Main category: cs.AI

TL;DR: 现有AGI评估方法依赖直觉和合成任务，效果不佳。本文提出一种新的评估哲学，侧重于通过鲁棒任务执行来评估AGI的胜任力，并借鉴数据科学中的可靠部署实践。


<details>
  <summary>Details</summary>
Motivation: AGI系统评估因其广泛的目标而面临挑战，现有方法依赖于对智能的直觉和合成任务，但这些方法在AI历史上表现不佳，因此亟需一种更有效、更可靠的评估范式。

Method: 本文提出一种替代设计哲学，通过评估“鲁棒的任务执行”来展示AGI的“胜任力”。这种方法借鉴了数据科学中用于展示系统可靠部署的常用实践，并提供了具体的AGI评估实例。

Result: 本文的主要成果是提出了一种新的AGI评估设计哲学，即从基于直觉的合成任务转向基于鲁棒任务执行和胜任力的评估，并提供了该方法在AGI评估中的实践意义示例。

Conclusion: AGI评估应放弃依赖直觉和合成任务的旧范式，转向关注基于鲁棒任务执行的胜任力评估，并从中借鉴数据科学中系统可靠部署的实践经验。

Abstract: Evaluation of potential AGI systems and methods is difficult due to the
breadth of the engineering goal. We have no methods for perfect evaluation of
the end state, and instead measure performance on small tests designed to
provide directional indication that we are approaching AGI. In this work we
argue that AGI evaluation methods have been dominated by a design philosophy
that uses our intuitions of what intelligence is to create synthetic tasks,
that have performed poorly in the history of AI. Instead we argue for an
alternative design philosophy focused on evaluating robust task execution that
seeks to demonstrate AGI through competence. This perspective is developed from
common practices in data science that are used to show that a system can be
reliably deployed. We provide practical examples of what this would mean for
AGI evaluation.

</details>


### [158] [VaPR -- Vision-language Preference alignment for Reasoning](https://arxiv.org/abs/2510.01700)
*Rohan Wadhawan,Fabrice Y Harel-Canada,Zi-Yi Dou,Suhaila Shakiah,Robinson Piramuthu,Nanyun Peng*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM引导的硬负样本响应生成框架，用于创建VaPR数据集，以解决LVLM偏好微调中合成数据存在的噪声问题。使用VaPR数据集微调的模型在多个基准测试中取得了显著性能提升，特别是在推理任务上，并改善了二元问题的偏向性。


<details>
  <summary>Details</summary>
Motivation: DPO等偏好微调方法在对齐大型视觉-语言模型（LVLMs）与人类偏好方面显示出潜力，但现有技术忽视了合成偏好标注中存在的风格和长度偏差等噪声。

Method: 引入了一个基于LLM引导响应编辑的硬负样本响应生成框架，该框架能生成带有特定错误但与接受响应在风格和长度上相似的拒绝响应。利用此框架构建了包含3万高质量样本的VaPR数据集，并用其对LLaVA-V1.5、Qwen2VL和Qwen2.5VL（2B-13B）三个LVLM家族进行了微调。

Result: VaPR模型在十个基准测试中取得了显著性能提升，LLaVA平均提升6.5%，Qwen2VL提升4.0%，Qwen2.5VL提升1.5%，尤其在推理任务上表现突出。扩展性分析显示性能随数据量增加而提升，LLaVA模型在较小规模数据下也能受益。VaPR还减少了二元问题中回答“是”的倾向性。此外，该框架可推广到开源LLM作为编辑器，其训练模型的性能接近使用GPT-4o合成数据的99%。

Conclusion: 通过引入硬负样本生成框架和VaPR数据集，有效解决了LVLM偏好微调中合成数据噪声的问题。所训练的模型在多项任务上表现出显著性能提升，特别是推理能力，并修正了LVLM常见的故障模式。该方法具有良好的泛化性，可使用开源LLM作为编辑器。

Abstract: Preference finetuning methods like Direct Preference Optimization (DPO) with
AI-generated feedback have shown promise in aligning Large Vision-Language
Models (LVLMs) with human preferences. However, existing techniques overlook
the prevalence of noise in synthetic preference annotations in the form of
stylistic and length biases. To this end, we introduce a hard-negative response
generation framework based on LLM-guided response editing, that produces
rejected responses with targeted errors, maintaining stylistic and length
similarity to the accepted ones. Using this framework, we develop the VaPR
dataset, comprising 30K high-quality samples, to finetune three LVLM families:
LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver
significant performance improvements across ten benchmarks, achieving average
gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable
improvements on reasoning tasks. A scaling analysis shows that performance
consistently improves with data size, with LLaVA models benefiting even at
smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary
questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we
show that the framework generalizes to open-source LLMs as editors, with models
trained on VaPR-OS achieving ~99% of the performance of models trained on
\name, which is synthesized using GPT-4o. Our data, models, and code can be
found on the project page https://vap-r.github.io

</details>


### [159] [MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs](https://arxiv.org/abs/2510.01724)
*Madina Bekbergenova,Lucas Pradi,Benjamin Navet,Emma Tysinger,Franck Michel,Matthieu Feraud,Yousouf Taghzouti,Yan Zhou Chen,Olivier Kirchhoffer,Florence Mehl,Martin Legrand,Tao Jiang,Marco Pagni,Soha Hassoun,Jean-Luc Wolfender,Wout Bittremieux,Fabien Gandon,Louis-Félix Nothias*

Main category: cs.AI

TL;DR: MetaboT是一个基于LLM的多智能体系统，能将自然语言问题转化为SPARQL查询，从而实现对代谢组学知识图谱的便捷访问。


<details>
  <summary>Details</summary>
Motivation: 质谱代谢组学数据量庞大，知识图谱有助于管理，但其本体论和查询语言（SPARQL）的复杂性限制了用户有效利用。

Method: 开发了MetaboT，一个AI系统，利用LLMs将用户问题翻译成SPARQL语义查询语言。该系统采用多智能体架构（基于LangChain和LangGraph），包含入口、验证、监督、知识图谱和SPARQL生成等专业代理，通过结构化工作流处理和分解复杂查询。

Result: MetaboT在50个代谢组学相关问题的测试中达到了83.67%的准确率，显著优于仅使用带本体论提示的标准LLM（GPT-4o）的8.16%准确率。

Conclusion: MetaboT作为会话式问答助手展现出良好性能，通过自动化SPARQL查询的生成和执行，降低了访问知识图谱的技术门槛，使研究人员能通过自然语言查询获取结构化的代谢组学数据，从而促进数据驱动的发现。

Abstract: Mass spectrometry metabolomics generates vast amounts of data requiring
advanced methods for interpretation. Knowledge graphs address these challenges
by structuring mass spectrometry data, metabolite information, and their
relationships into a connected network (Gaudry et al. 2024). However, effective
use of a knowledge graph demands an in-depth understanding of its ontology and
its query language syntax. To overcome this, we designed MetaboT, an AI system
utilizing large language models (LLMs) to translate user questions into SPARQL
semantic query language for operating on knowledge graphs (Steve Harris 2013).
We demonstrate its effectiveness using the Experimental Natural Products
Knowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural
products (Gaudry et al. 2024).MetaboT employs specialized AI agents for
handling user queries and interacting with the knowledge graph by breaking down
complex tasks into discrete components, each managed by a specialised agent
(Fig. 1a). The multi-agent system is constructed using the LangChain and
LangGraph libraries, which facilitate the integration of LLMs with external
tools and information sources (LangChain, n.d.). The query generation process
follows a structured workflow. First, the Entry Agent determines if the
question is new or a follow-up to previous interactions. New questions are
forwarded to the Validator Agent, which verifies if the question is related to
the knowledge graph. Then, the valid question is sent to the Supervisor Agent,
which identifies if the question requires chemical conversions or standardized
identifiers. In this case it delegates the question to the Knowledge Graph
Agent, which can use tools to extract necessary details, such as URIs or
taxonomies of chemical names, from the user query. Finally, an agent
responsible for crafting the SPARQL queries equipped with the ontology of the
knowledge graph uses the provided identifiers to generate the query. Then, the
system executes the generated query against the metabolomics knowledge graph
and returns structured results to the user (Fig. 1b). To assess the performance
of MetaboT we have curated 50 metabolomics-related questions and their expected
answers. In addition to submitting these questions to MetaboT, we evaluated a
baseline by submitting them to a standard LLM (GPT-4o) with a prompt that
incorporated the knowledge graph ontology but did not provide specific entity
IDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%,
underscoring the necessity of our multi-agent system for accurately retrieving
entities and generating correct SPARQL queries. MetaboT demonstrates promising
performance as a conversational question-answering assistant, enabling
researchers to retrieve structured metabolomics data through natural language
queries. By automating the generation and execution of SPARQL queries, it
removes technical barriers that have traditionally hindered access to knowledge
graphs. Importantly, MetaboT leverages the capabilities of LLMs while
maintaining experimentally grounded query generation, ensuring that outputs
remain aligned with domain-specific standards and data structures. This
approach facilitates data-driven discoveries by bridging the gap between
complex semantic technologies and user-friendly interaction. MetaboT is
accessible at [https://metabot.holobiomicslab.eu/], and its source code is
available at [https://github.com/HolobiomicsLab/MetaboT].

</details>


### [160] [A cybersecurity AI agent selection and decision support framework](https://arxiv.org/abs/2510.01751)
*Masike Malatji*

Main category: cs.AI

TL;DR: 本文提出了一个结构化的决策支持框架，旨在系统地将不同AI智能体架构与NIST CSF 2.0对齐，以增强网络安全防护。


<details>
  <summary>Details</summary>
Motivation: 研究动机是弥合AI理论构建与实际网络安全操作需求之间的鸿沟，为企业提供一个透明且系统化的方法，以选择和部署AI解决方案来有效应对现代网络威胁。

Method: 该研究构建了一个结构化的决策支持框架，将多种AI智能体架构与NIST CSF 2.0框架的功能和任务进行细粒度分解和匹配，并关联AI智能体特性与安全要求。此外，它还定义了不同级别的自主性，并通过概念验证展示了其在实际场景中的适用性。

Result: 该框架提供了一个透明、循序渐进的AI解决方案部署方法，能够提升态势感知能力，加速事件响应时间，并通过自适应风险管理增强长期韧性，从而形成统一的检测、事件响应和治理策略。

Conclusion: 该研究为开发符合行业标准的、健壮且经过实证验证的多智能体系统奠定了基础，成功将理论AI概念与操作性网络安全需求结合起来。

Abstract: This paper presents a novel, structured decision support framework that
systematically aligns diverse artificial intelligence (AI) agent architectures,
reactive, cognitive, hybrid, and learning, with the comprehensive National
Institute of Standards and Technology (NIST) Cybersecurity Framework (CSF) 2.0.
By integrating agent theory with industry guidelines, this framework provides a
transparent and stepwise methodology for selecting and deploying AI solutions
to address contemporary cyber threats. Employing a granular decomposition of
NIST CSF 2.0 functions into specific tasks, the study links essential AI agent
properties such as autonomy, adaptive learning, and real-time responsiveness to
each subcategory's security requirements. In addition, it outlines graduated
levels of autonomy (assisted, augmented, and fully autonomous) to accommodate
organisations at varying stages of cybersecurity maturity. This holistic
approach transcends isolated AI applications, providing a unified detection,
incident response, and governance strategy. Through conceptual validation, the
framework demonstrates how tailored AI agent deployments can align with
real-world constraints and risk profiles, enhancing situational awareness,
accelerating response times, and fortifying long-term resilience via adaptive
risk management. Ultimately, this research bridges the gap between theoretical
AI constructs and operational cybersecurity demands, establishing a foundation
for robust, empirically validated multi-agent systems that adhere to industry
standards.

</details>


### [161] [REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing](https://arxiv.org/abs/2510.01800)
*Thanh Ma,Tri-Tam La,Lam-Thu Le Huu,Minh-Nghi Nguyen,Khanh-Van Pham Luu,Huu-Hoa Nguyen*

Main category: cs.AI

TL;DR: REBot是一个结合LLM和CatRAG（混合检索推理框架）的学术规定咨询聊天机器人，通过集成知识图谱和RAG，在分类和问答任务上取得了98.89%的SOTA F1分数，并具有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 学术规定咨询对学生理解政策和合规至关重要，但构建高效系统缺乏领域特定监管资源，难以有效处理复杂咨询需求。

Method: 提出REBot，一个基于LLM的咨询聊天机器人，由CatRAG框架驱动。CatRAG融合了检索增强生成(RAG)与图谱推理，利用分层、类别标记并富含语义特征的知识图谱。同时，一个轻量级意图分类器负责查询路由。构建了法规特定数据集进行评估。

Result: REBot在分类和问答任务上实现了98.89%的F1分数，达到最先进（SOTA）性能。此外，还实现了Web应用程序，展示了REBot在真实世界学术咨询场景中的实用价值。

Conclusion: REBot通过结合LLM和CatRAG框架，有效解决了学术规定咨询中的资源和准确性挑战。它提供了高准确性、上下文深度丰富的咨询服务，并在实际应用中展现出巨大潜力。

Abstract: Academic regulation advising is essential for helping students interpret and
comply with institutional policies, yet building effective systems requires
domain specific regulatory resources. To address this challenge, we propose
REBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval
reasoning framework that integrates retrieval augmented generation with graph
based reasoning. CatRAG unifies dense retrieval and graph reasoning, supported
by a hierarchical, category labeled knowledge graph enriched with semantic
features for domain alignment. A lightweight intent classifier routes queries
to the appropriate retrieval modules, ensuring both factual accuracy and
contextual depth. We construct a regulation specific dataset and evaluate REBot
on classification and question answering tasks, achieving state of the art
performance with an F1 score of 98.89%. Finally, we implement a web application
that demonstrates the practical value of REBot in real world academic advising
scenarios.

</details>


### [162] [Human-AI Teaming Co-Learning in Military Operations](https://arxiv.org/abs/2510.01815)
*Clara Maathuis,Kasper Cools*

Main category: cs.AI

TL;DR: 本研究提出了一种可信赖的人机协同学习模型，用于军事行动中，通过整合可调节自主性、多层控制、双向反馈和协作决策四个维度，以促进人类与AI代理共同适应战场条件。


<details>
  <summary>Details</summary>
Motivation: 面对日益复杂的军事威胁和作战环境，将AI融入军事行动带来巨大优势的同时，也引发了有效且道德地构建和部署人机协作系统的挑战与风险。现有研究多从外部视角看待人机协作系统，未能深入其内部动态，因此需要一种能处理多维度责任、安全和鲁棒性问题的内部视角。

Method: 本研究设计了一种可信赖的人机协同学习模型，以实现人类与AI代理间持续、双向的洞察交流，共同适应战场变化。该模型整合了四个关键维度：1) **可调节自主性**，根据任务状态、系统置信度和环境不确定性动态校准自主水平；2) **多层控制**，确保持续监督、活动监控和问责制；3) **双向反馈**，通过显式和隐式循环促进推理、不确定性和学习适应的有效沟通；4) **协作决策**，涉及决策的生成、评估、提出及相关置信度与理由的阐明。

Result: 本研究提出了一个详细设计的人机协同学习模型，并提供了具体的案例说明和建议。

Conclusion: 所提出的模型及其附带的示例和建议，有助于在军事行动中进一步开发负责任且可信赖的人机协作系统。

Abstract: In a time of rapidly evolving military threats and increasingly complex
operational environments, the integration of AI into military operations proves
significant advantages. At the same time, this implies various challenges and
risks regarding building and deploying human-AI teaming systems in an effective
and ethical manner. Currently, understanding and coping with them are often
tackled from an external perspective considering the human-AI teaming system as
a collective agent. Nevertheless, zooming into the dynamics involved inside the
system assures dealing with a broader palette of relevant multidimensional
responsibility, safety, and robustness aspects. To this end, this research
proposes the design of a trustworthy co-learning model for human-AI teaming in
military operations that encompasses a continuous and bidirectional exchange of
insights between the human and AI agents as they jointly adapt to evolving
battlefield conditions. It does that by integrating four dimensions. First,
adjustable autonomy for dynamically calibrating the autonomy levels of agents
depending on aspects like mission state, system confidence, and environmental
uncertainty. Second, multi-layered control which accounts continuous oversight,
monitoring of activities, and accountability. Third, bidirectional feedback
with explicit and implicit feedback loops between the agents to assure a proper
communication of reasoning, uncertainties, and learned adaptations that each of
the agents has. And fourth, collaborative decision-making which implies the
generation, evaluation, and proposal of decisions associated with confidence
levels and rationale behind them. The model proposed is accompanied by concrete
exemplifications and recommendations that contribute to further developing
responsible and trustworthy human-AI teaming systems in military operations.

</details>


### [163] [Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.01833)
*Zhihao Dou,Qinjian Zhao,Zhongwei Wan,Dinggen Zhang,Weida Wang,Towsif Raiyan,Benteng Chen,Qingtao Pan,Yang Ouyang,Zhiqiang Gao,Shufei Zhang,Sumon Biswas*

Main category: cs.AI

TL;DR: LLM的CoT推理因缺乏全局规划而性能受限。本文提出PTA-GRPO框架，通过两阶段（CoT蒸馏SFT和指导感知型RL）增强高层规划和推理，在数学推理任务上显著提升了多种LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的CoT推理缺乏全局规划，导致推理过程冗余、不连贯或不准确，从而降低性能。现有方法如树基算法和RL计算成本高且难以优化。

Method: 提出PTA-GRPO两阶段框架：第一阶段，利用LLM将CoT蒸馏为高层指导，用于监督微调（SFT）；第二阶段，引入指导感知型强化学习（RL）方法，联合优化最终输出和高层指导质量。

Result: 在MATH、AIME2024/2025、AMC等多个数学推理基准上，以及Qwen系列和LLaMA3.2-3B等多种基础模型上进行实验。结果显示PTA-GRPO在不同模型和任务上均实现了稳定显著的性能提升。

Conclusion: PTA-GRPO框架通过优化高层规划和CoT推理，有效提高了LLMs在复杂推理任务上的表现，并具有良好的泛化能力。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning abilities
in complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However,
due to their autoregressive token-level generation, the reasoning process is
largely constrained to local decision-making and lacks global planning. This
limitation frequently results in redundant, incoherent, or inaccurate
reasoning, which significantly degrades overall performance. Existing
approaches, such as tree-based algorithms and reinforcement learning (RL),
attempt to address this issue but suffer from high computational costs and
often fail to produce optimal reasoning trajectories. To tackle this challenge,
we propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy
Optimization PTA-GRPO, a two-stage framework designed to improve both
high-level planning and fine-grained CoT reasoning. In the first stage, we
leverage advanced LLMs to distill CoT into compact high-level guidance, which
is then used for supervised fine-tuning (SFT). In the second stage, we
introduce a guidance-aware RL method that jointly optimizes the final output
and the quality of high-level guidance, thereby enhancing reasoning
effectiveness. We conduct extensive experiments on multiple mathematical
reasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across
diverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and
LLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently
achieves stable and significant improvements across different models and tasks,
validating its effectiveness and generalization.

</details>


### [164] [Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning](https://arxiv.org/abs/2510.01857)
*Claudio Fanconi,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 本文将对抗性逆强化学习应用于大型语言模型推理，学习一个密集的、token级的奖励模型，用于过程监督。该奖励模型既可在训练时提供步进反馈，也可在推理时作为评价器重排轨迹，从而提高多步推理的正确性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法可能仅模仿风格而非内在正确性。需要一个能直接从专家演示中学习的密集、token级的奖励模型，以实现对大型语言模型推理过程的有效监督和优化。

Method: 研究人员重构并操作化了对抗性逆强化学习（IRL），以从专家演示中学习一个密集的、token级的奖励模型。该奖励模型在训练时提供步进反馈以优化推理策略，并在推理时作为批评器对采样轨迹进行重排。

Result: 该方法优先考虑正确性而非表面形式，其得分与最终答案的有效性相关，并能解释轨迹中的错误定位。在GSM8K数据集上使用Llama3和Qwen2.5模型进行实验，结果表明：密集的推理奖励可作为学习信号来引导推理；奖励引导的重排能提高预测性能（特别是Llama模型）。

Conclusion: 通过将训练信号、推理时选择和token级诊断统一到单一推理奖励中，这项工作提出了一种可重用的过程级奖励，其在增强语言模型多步推理方面具有广泛潜力。

Abstract: We reframe and operationalise adversarial inverse reinforcement learning
(IRL) to large language model reasoning, learning a dense, token-level reward
model for process supervision directly from expert demonstrations rather than
imitating style via supervised fine-tuning. The learned reasoning reward serves
two complementary roles: (i) it provides step-level feedback to optimise a
reasoning policy during training; and (ii) it functions at inference as a
critic to rerank sampled traces under fixed compute budgets. We demonstrate
that our approach prioritises correctness over surface form, yielding scores
that correlate with eventual answer validity and enabling interpretable
localisation of errors within a trace. Empirically, on GSM8K with Llama3 and
Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a
learning signal to elicit reasoning, and (ii) predictive performance is
improved from reward-guided reranking (notably for Llama-based policies). By
unifying training signals, inference-time selection, and token-level
diagnostics into a single reasoning reward, this work suggests reusable
process-level rewards with broad potential to enhance multi-step reasoning in
language models.

</details>


### [165] [Constrained Adaptive Rejection Sampling](https://arxiv.org/abs/2510.01902)
*Paweł Parys,Sairam Vaidya,Taylor Berg-Kirkpatrick,Loris D'Antoni*

Main category: cs.AI

TL;DR: 本文提出CARS（受限自适应拒绝采样）方法，通过自适应剪枝技术，在不扭曲语言模型分布的前提下，显著提高了受限生成的采样效率和样本多样性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 语言模型在需要严格语义或句法约束的应用中面临挑战。现有受限生成方法存在弊端：贪婪解码虽保证有效性但扭曲LM分布，拒绝采样虽保持分布忠实但计算效率低下。在程序模糊测试等领域，有效性和多样性都至关重要，但现有方法无法同时满足。

Method: 本文提出CARS（Constrained Adaptive Rejection Sampling）。该方法首先进行无约束LM采样，然后通过将违反约束的后续序列记录在一个trie中，并从未来的采样中减去其概率质量，来自适应地排除这些序列。这种自适应剪枝机制确保已证明无效的前缀不再被重复访问。

Result: CARS在不扭曲语言模型分布的情况下，严格提高了拒绝采样的样本效率。它保证无效前缀永不被重复访问，接受率单调提高，且生成的样本精确遵循受限分布。在程序模糊测试和分子生成等多个领域，CARS在每生成一个有效样本所需的LM前向传播次数方面，始终实现更高的效率，并产生比贪婪受限解码和近似LM分布方法更强的样本多样性。

Conclusion: CARS提供了一种高效且能保持语言模型分布忠实度的受限生成方法。它通过创新的自适应剪枝机制，有效解决了现有方法的效率和分布扭曲问题，在保证样本有效性的同时，显著提升了效率和多样性。

Abstract: Language Models (LMs) are increasingly used in applications where generated
outputs must satisfy strict semantic or syntactic constraints. Existing
approaches to constrained generation fall along a spectrum: greedy constrained
decoding methods enforce validity during decoding but distort the LM's
distribution, while rejection sampling (RS) preserves fidelity but wastes
computation by discarding invalid outputs. Both extremes are problematic in
domains such as program fuzzing, where both validity and diversity of samples
are essential. We present Constrained Adaptive Rejection Sampling (CARS), an
approach that strictly improves the sample-efficiency of RS without
distributional distortion. CARS begins with unconstrained LM sampling and
adaptively rules out constraint-violating continuations by recording them in a
trie and subtracting their probability mass from future draws. This adaptive
pruning ensures that prefixes proven invalid are never revisited, acceptance
rates improve monotonically, and the resulting samples exactly follow the
constrained distribution. In experiments on a variety of domains -- e.g.,
program fuzzing and molecular generation -- CARS consistently achieves higher
efficiency -- measured in the number of LM forward passes per valid sample --
while also producing stronger sample diversity than both GCD and methods that
approximate the LM's distribution.

</details>


### [166] [To Mask or to Mirror: Human-AI Alignment in Collective Reasoning](https://arxiv.org/abs/2510.01924)
*Crystal Qian,Aaron Parisi,Clémentine Bouleau,Vivian Tsai,Maël Lebreton,Lucas Dixon*

Main category: cs.AI

TL;DR: 本研究通过人类实验和LLM模拟，评估了LLM在集体决策中与人类社会推理的对齐情况，发现LLM行为多样，对齐程度受上下文、线索和模型偏差影响。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）被广泛应用于模拟和增强集体决策，深入探究其与人类社会推理的对齐程度变得至关重要。

Method: 提出了一个评估集体对齐的实证框架。利用“海上迷失”社会心理任务，进行了一项大规模在线人类实验（N=748），将小组随机分配到有可见人口属性或匿名别名的领导者选举中。随后，基于人类数据模拟并基准测试了匹配的LLM群体（Gemini 2.5, GPT 4.1, Claude Haiku 3.5, Gemma 3）。

Result: LLM的行为表现出差异性：一些模型反映了人类偏见；另一些则掩盖这些偏见并试图补偿。研究实证表明，集体推理中的人机对齐取决于上下文、线索以及模型特有的归纳偏见。

Conclusion: 理解LLMs如何与集体人类行为对齐是推进社会对齐AI的关键，这要求开发能够捕捉集体推理复杂性的动态基准。

Abstract: As large language models (LLMs) are increasingly used to model and augment
collective decision-making, it is critical to examine their alignment with
human social reasoning. We present an empirical framework for assessing
collective alignment, in contrast to prior work on the individual level. Using
the Lost at Sea social psychology task, we conduct a large-scale online
experiment (N=748), randomly assigning groups to leader elections with either
visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We
then simulate matched LLM groups conditioned on the human data, benchmarking
Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some
mirror human biases; others mask these biases and attempt to compensate for
them. We empirically demonstrate that human-AI alignment in collective
reasoning depends on context, cues, and model-specific inductive biases.
Understanding how LLMs align with collective human behavior is critical to
advancing socially-aligned AI, and demands dynamic benchmarks that capture the
complexities of collective reasoning.

</details>


### [167] [Zero-shot reasoning for simulating scholarly peer-review](https://arxiv.org/abs/2510.02027)
*Khalid M. Saqr*

Main category: cs.AI

TL;DR: 该研究提出一个确定性模拟框架，为评估AI生成同行评审报告提供了一个稳定、基于证据的标准，有效模拟了编辑判断并维护了程序完整性，从而增强了学术出版的透明度和可信赖性。


<details>
  <summary>Details</summary>
Motivation: 学术出版面临提交量过大和AI监管不足的双重危机，传统同行评审缺乏可扩展、客观的基准。研究旨在建立新的治理模型，以维护科学诚信，并提供评估AI生成同行评审报告的证据标准。

Method: 调查了一个确定性模拟框架，通过分析352份同行评审模拟报告来识别一致的系统状态指标，以验证其可靠性。

Result: 该系统能模拟校准的编辑判断（如“修改”是多数结果，而“拒绝”率适应特定领域），并保持程序完整性（稳定的29%证据锚定符合率），显示出可预测性和规则性，减轻了生成式AI的随机性。

Conclusion: 该框架为科学界提供了确保公平的透明工具，为出版策略师提供了审计工作流程、管理诚信风险和实施循证治理的可扩展工具，将AI重新定位为机构问责制和维护学术交流信任的关键组成部分。

Abstract: The scholarly publishing ecosystem faces a dual crisis of unmanageable
submission volumes and unregulated AI, creating an urgent need for new
governance models to safeguard scientific integrity. The traditional human-only
peer review regime lacks a scalable, objective benchmark, making editorial
processes opaque and difficult to audit. Here we investigate a deterministic
simulation framework that provides the first stable, evidence-based standard
for evaluating AI-generated peer review reports. Analyzing 352 peer-review
simulation reports, we identify consistent system state indicators that
demonstrate its reliability. First, the system is able to simulate calibrated
editorial judgment, with 'Revise' decisions consistently forming the majority
outcome (>50%) across all disciplines, while 'Reject' rates dynamically adapt
to field-specific norms, rising to 45% in Health Sciences. Second, it maintains
unwavering procedural integrity, enforcing a stable 29% evidence-anchoring
compliance rate that remains invariant across diverse review tasks and
scientific domains. These findings demonstrate a system that is predictably
rule-bound, mitigating the stochasticity of generative AI. For the scientific
community, this provides a transparent tool to ensure fairness; for publishing
strategists, it offers a scalable instrument for auditing workflows, managing
integrity risks, and implementing evidence-based governance. The framework
repositions AI as an essential component of institutional accountability,
providing the critical infrastructure to maintain trust in scholarly
communication.

</details>


### [168] [ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly Detection](https://arxiv.org/abs/2510.02060)
*Sanghyu Yoon,Dongmin Kim,Suhee Yoon,Ye Seul Sim,Seungdong Yoa,Hye-Seung Cho,Soonyoung Lee,Hankook Lee,Woohyung Lim*

Main category: cs.AI

TL;DR: ReTabAD是一个新的表格异常检测基准，通过整合文本语义上下文来弥补现有基准的不足，从而提升检测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有表格异常检测基准缺乏文本语义上下文，限制了研究灵活性，且无法充分利用专家实践中依赖的领域知识（如特征描述），导致模型无法利用关键信号进行检测。

Method: 项目ReTabAD提供了(1) 20个精心策划的、富含结构化文本元数据的表格数据集，并实现了包括经典、深度学习和基于LLM在内的最先进异常检测算法；(2) 一个零样本LLM框架，该框架利用语义上下文而无需任务特定训练，建立了一个强大的基线。

Result: 实验和分析表明，语义上下文能够提高异常检测性能，并通过支持领域感知推理来增强模型的可解释性。

Conclusion: ReTabAD作为一个新基准，为系统性探索上下文感知异常检测提供了基础，并深入揭示了文本元数据在异常检测中的重要作用和实用价值。

Abstract: In tabular anomaly detection (AD), textual semantics often carry critical
signals, as the definition of an anomaly is closely tied to domain-specific
context. However, existing benchmarks provide only raw data points without
semantic context, overlooking rich textual metadata such as feature
descriptions and domain knowledge that experts rely on in practice. This
limitation restricts research flexibility and prevents models from fully
leveraging domain knowledge for detection. ReTabAD addresses this gap by
restoring textual semantics to enable context-aware tabular AD research. We
provide (1) 20 carefully curated tabular datasets enriched with structured
textual metadata, together with implementations of state-of-the-art AD
algorithms including classical, deep learning, and LLM-based approaches, and
(2) a zero-shot LLM framework that leverages semantic context without
task-specific training, establishing a strong baseline for future research.
Furthermore, this work provides insights into the role and utility of textual
metadata in AD through experiments and analysis. Results show that semantic
context improves detection performance and enhances interpretability by
supporting domain-aware reasoning. These findings establish ReTabAD as a
benchmark for systematic exploration of context-aware AD.

</details>


### [169] [Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning](https://arxiv.org/abs/2510.02091)
*Xinyuan Song,Keyu Wang,PengXiang Li,Lu Yin,Shiwei Liu*

Main category: cs.AI

TL;DR: LLM深层并非无用，其贡献度高度依赖评估方式、任务类型和模型架构。


<details>
  <summary>Details</summary>
Motivation: 现有研究认为LLM深层对表示学习贡献小且可移除，但这些结论可能基于狭窄评估，忽略了模型行为的重要方面，因此需要进行系统性研究。

Method: 本文对LLM的深度利用率进行了系统性研究，涵盖了多种评估协议、任务类别和模型架构。

Result: 分析证实深层通常不如浅层有效，但其贡献随评估设置显著变化。在基于似然的非生成性评估中，仅初始几层是关键；而在基于生成的评估中，中深层对推理和长程连贯性发挥着不可或缺的作用。此外，知识和检索集中在浅层组件，而推理准确性高度依赖深层（且可通过蒸馏重塑）。

Conclusion: LLM的深度使用高度异构且依赖上下文，这强调在解释和压缩大型模型时，需要采取任务、指标和模型感知的视角。

Abstract: Recent studies suggest that the deeper layers of Large Language Models (LLMs)
contribute little to representation learning and can often be removed without
significant performance loss. However, such claims are typically drawn from
narrow evaluations and may overlook important aspects of model behavior. In
this work, we present a systematic study of depth utilization across diverse
dimensions, including evaluation protocols, task categories, and model
architectures. Our analysis confirms that very deep layers are generally less
effective than earlier ones, but their contributions vary substantially with
the evaluation setting. Under likelihood-based metrics without generation,
pruning most layers preserves performance, with only the initial few being
critical. By contrast, generation-based evaluation uncovers indispensable roles
for middle and deeper layers in enabling reasoning and maintaining long-range
coherence. We further find that knowledge and retrieval are concentrated in
shallow components, whereas reasoning accuracy relies heavily on deeper layers
-- yet can be reshaped through distillation. These results highlight that depth
usage in LLMs is highly heterogeneous and context-dependent, underscoring the
need for task-, metric-, and model-aware perspectives in both interpreting and
compressing large models.

</details>


### [170] [Do AI Models Perform Human-like Abstract Reasoning Across Modalities?](https://arxiv.org/abs/2510.02125)
*Claas Beger,Ryan Yi,Shuhao Fu,Arseny Moskvichev,Sarah W. Tsai,Sivasankaran Rajamanickam,Melanie Mitchell*

Main category: cs.AI

TL;DR: 研究发现，尽管AI模型在ARC-like任务上表面准确率高，但其抽象推理能力仍远低于人类，尤其在视觉模态下被低估，文本模态下被高估，现有评估方法需改进。


<details>
  <summary>Details</summary>
Motivation: 质疑最新AI模型（如OpenAI o3-preview）在ARC-AGI基准测试中超越人类的准确率是否真正意味着模型能识别并推理任务创建者预期的抽象概念，而非依赖表面模式。

Method: 在ConceptARC数据集上，通过改变输入模态（文本/视觉）、是否允许使用外部Python工具、以及推理模型的推理努力程度来评估模型。除了测量输出准确性，还对模型生成的自然语言规则进行细粒度评估，以判断其是否使用了预期的抽象概念。

Result: 文本表示模型在输出准确性上可与人类匹配，但其规则常基于表面“捷径”，捕捉抽象概念的能力远低于人类；视觉模态下，AI模型准确率急剧下降，但规则分析显示它们仍能捕捉相当一部分预期抽象概念，只是未能正确应用。因此，仅凭准确率评估可能高估文本模态，低估视觉模态的抽象推理能力。

Conclusion: 模型在抽象推理方面仍落后于人类。仅使用准确率来评估ARC-like任务上的抽象推理能力可能会导致在文本模态中高估模型能力，而在视觉模态中低估。该评估框架能更真实地反映多模态模型的抽象推理能力，并为追踪类人抽象智能的进展提供更原则性的方法。

Abstract: OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI
benchmark, but does that mean state-of-the-art models recognize and reason with
the abstractions that the task creators intended? We investigate models'
abstraction abilities on ConceptARC. We evaluate models under settings that
vary the input modality (textual vs. visual), whether the model is permitted to
use external Python tools, and, for reasoning models, the amount of reasoning
effort. In addition to measuring output accuracy, we perform fine-grained
evaluation of the natural-language rules that models generate to explain their
solutions. This dual evaluation lets us assess whether models solve tasks using
the abstractions ConceptARC was designed to elicit, rather than relying on
surface-level patterns. Our results show that, while some models using
text-based representations match human output accuracy, the best models' rules
are often based on surface-level ``shortcuts'' and capture intended
abstractions far less often than humans. Thus their capabilities for general
abstract reasoning may be overestimated by evaluations based on accuracy alone.
In the visual modality, AI models' output accuracy drops sharply, yet our
rule-level analysis reveals that models might be underestimated, as they still
exhibit a substantial share of rules that capture intended abstractions, but
are often unable to correctly apply these rules. In short, our results show
that models still lag humans in abstract reasoning, and that using accuracy
alone to evaluate abstract reasoning on ARC-like tasks may overestimate
abstract-reasoning capabilities in textual modalities and underestimate it in
visual modalities. We believe that our evaluation framework offers a more
faithful picture of multimodal models' abstract reasoning abilities and a more
principled way to track progress toward human-like, abstraction-centered
intelligence.

</details>


### [171] [FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic Documents for Training Document Understanding Models](https://arxiv.org/abs/2510.02133)
*Karan Dua,Hitesh Laxmichand Patel,Puneet Mittal,Ranjeet Gupta,Amit Agarwal,Praneet Pabolu,Srikant Panda,Hansa Meghwani,Graham Horwood,Fahad Shah*

Main category: cs.AI

TL;DR: FlexDoc是一个可扩展的合成数据生成框架，通过概率建模和参数化抽样，高效生成多样化且带标注的半结构化文档，显著降低了文档理解模型的数据采集和标注成本。


<details>
  <summary>Details</summary>
Motivation: 开发企业级文档理解模型需要大规模、多样化且标注良好的数据集，但由于隐私、法律限制和巨额人工标注成本（可能高达数百万美元），收集此类数据非常昂贵。

Method: 引入FlexDoc框架，该框架结合了随机模式（Stochastic Schemas）和参数化抽样（Parameterized Sampling），以生成带有丰富标注的逼真、多语言半结构化文档。它通过概率建模布局模式、视觉结构和内容可变性，实现大规模可控地生成多样化文档变体。

Result: 在关键信息提取（KIE）任务的实验中，FlexDoc生成的数据在用于增强真实数据集时，可将F1分数绝对值提高高达11%。与传统的硬模板方法相比，它将标注工作量减少了90%以上。该解决方案已在实际部署中，加速了企业级文档理解模型的开发，并显著降低了数据获取和标注成本。

Conclusion: FlexDoc提供了一个可扩展、经济高效的解决方案，通过大规模生成高质量合成数据，克服了真实数据获取的挑战，从而加速了企业级文档理解模型的开发，并大幅削减了相关成本。

Abstract: Developing document understanding models at enterprise scale requires large,
diverse, and well-annotated datasets spanning a wide range of document types.
However, collecting such data is prohibitively expensive due to privacy
constraints, legal restrictions, and the sheer volume of manual annotation
needed - costs that can scale into millions of dollars. We introduce FlexDoc, a
scalable synthetic data generation framework that combines Stochastic Schemas
and Parameterized Sampling to produce realistic, multilingual semi-structured
documents with rich annotations. By probabilistically modeling layout patterns,
visual structure, and content variability, FlexDoc enables the controlled
generation of diverse document variants at scale. Experiments on Key
Information Extraction (KIE) tasks demonstrate that FlexDoc-generated data
improves the absolute F1 Score by up to 11% when used to augment real datasets,
while reducing annotation effort by over 90% compared to traditional
hard-template methods. The solution is in active deployment, where it has
accelerated the development of enterprise-grade document understanding models
while significantly reducing data acquisition and annotation costs.

</details>


### [172] [A Rigorous Benchmark with Multidimensional Evaluation for Deep Research Agents: From Answers to Reports](https://arxiv.org/abs/2510.02190)
*Yang Yao,Yixu Wang,Yuxuan Zhang,Yi Lu,Tianle Gu,Lingyu Li,Dingyi Zhao,Keming Wu,Haozhe Wang,Ping Nie,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: AI正从封闭模型转向互联智能体系统，其中深度研究智能体（DRAs）在复杂任务中表现出色。然而，现有评估基准不足以有效衡量DRAs。本文提出了一个专门针对DRAs及其报告式响应的严格基准和多维度评估框架，实验证明DRAs优于传统模型，但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估维度、响应格式和评分机制上存在缺陷，限制了对深度研究智能体（DRAs）等互联智能体系统能力的有效评估。

Method: 引入了一个包含214个专家策划查询（分布于10个主题领域，附带人工参考包）的严格基准，并提出了一个多维度评估框架。该框架能够全面评估DRAs生成的长篇报告，集成了语义质量、主题焦点和检索可信度等评分指标。

Result: 广泛实验证实，主流DRAs的表现优于基于网络搜索工具增强的推理模型。然而，研究也揭示了DRAs仍有相当大的改进空间。

Conclusion: 本研究为深度研究智能体（DRAs）系统的能力评估、架构优化和范式推进提供了坚实的基础。

Abstract: Artificial intelligence is undergoing the paradigm shift from closed language
models to interconnected agent systems capable of external perception and
information integration. As a representative embodiment, Deep Research Agents
(DRAs) systematically exhibit the capabilities for task decomposition,
cross-source retrieval, multi-stage reasoning, and structured output, which
markedly enhance performance on complex and open-ended tasks. However, existing
benchmarks remain deficient in evaluation dimensions, response formatting, and
scoring mechanisms, limiting their capacity to assess such systems effectively.
This paper introduces a rigorous benchmark and a multidimensional evaluation
framework tailored to DRAs and report-style responses. The benchmark comprises
214 expert-curated challenging queries distributed across 10 broad thematic
domains, each accompanied by manually constructed reference bundles to support
composite evaluation. The framework enables comprehensive evaluation of
long-form reports generated by DRAs, incorporating integrated scoring metrics
for semantic quality, topical focus, and retrieval trustworthiness. Extensive
experimentation confirms the superior performance of mainstream DRAs over
web-search-tool-augmented reasoning models, yet reveals considerable scope for
further improvement. This study provides a robust foundation for capability
assessment, architectural refinement, and paradigm advancement in DRA systems.

</details>


### [173] [UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language Models](https://arxiv.org/abs/2510.02194)
*Yuhao Sun,Zhuoer Xu,Shiwen Cui,Kun Yang,Lingyun Yu,Yongdong Zhang,Hongtao Xie*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) have achieved remarkable progress across a wide
range of tasks, but remain vulnerable to safety risks such as harmful content
generation and jailbreak attacks. Existing safety techniques -- including
external guardrails, inference-time guidance, and post-training alignment --
each face limitations in balancing safety, utility, and controllability. In
this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM
safety through safety-aware upcycling. Our approach first identifies
safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE)
structure, where the router acts as a soft guardrail that selectively activates
original MLPs and added safety experts. We further introduce a two-stage SFT
strategy to strengthen safety discrimination while preserving general
capabilities. To enable flexible control at inference time, we introduce a
safety temperature mechanism, allowing dynamic adjustment of the trade-off
between safety and utility. Experiments across multiple benchmarks, base model,
and model scales demonstrate that UpSafe$^\circ$C achieves robust safety
improvements against harmful and jailbreak inputs, while maintaining
competitive performance on general tasks. Moreover, analysis shows that safety
temperature provides fine-grained inference-time control that achieves the
Pareto-optimal frontier between utility and safety. Our results highlight a new
direction for LLM safety: moving from static alignment toward dynamic, modular,
and inference-aware control.

</details>


### [174] [The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models](https://arxiv.org/abs/2510.02230)
*Phuc Minh Nguyen,Chinh D. La,Duy M. H. Nguyen,Nitesh V. Chawla,Binh T. Nguyen,Khoa D. Doan*

Main category: cs.AI

TL;DR: 本文深入研究了可验证奖励强化学习（RLVR）在提高大语言模型推理能力时可能导致的推理边界收缩问题。通过揭示负面干扰和赢者通吃两种关键现象，解释了RLVR为何收敛于狭窄的解题策略。作者提出了一种数据整理算法，通过将RLVR学习重点放在低概率问题上，显著提升了Pass@k性能。


<details>
  <summary>Details</summary>
Motivation: 尽管可验证奖励强化学习（RLVR）被认为是提高大型语言模型（LLM）推理能力的关键方法，但近期证据表明它可能反而导致推理边界收缩而非扩展。本研究旨在深入调查并解释RLVR出现这种收缩问题的原因。

Method: 1. 分析RLVR的学习动态，揭示其失败的两个关键现象：负面干扰（学习解决某些问题降低解决其他问题的可能性）和赢者通吃（RLVR不成比例地强化高概率正确解并抑制低概率解）。2. 通过对多个数学推理基准进行广泛的理论和实证分析，证明这些效应源于标准RL目标中固有的在策略采样，导致模型收敛到狭窄的解决方案策略。3. 提出了一种简单有效的数据整理算法，将RLVR学习的重点放在低概率问题上。

Result: 1. 揭示了RLVR学习中的负面干扰现象（导致Pass@k性能下降）和赢者通吃现象（RLVR倾向于强化高概率解而抑制低概率解）。2. 通过理论和实证分析证实这些现象源于标准RL目标中的在策略采样，导致模型采用狭窄的解题策略。3. 提出的数据整理算法通过将RLVR学习聚焦于低概率问题，显著改善了Pass@k性能。

Conclusion: RLVR在提升LLM推理能力时出现的推理边界收缩问题，主要由学习动态中的负面干扰和赢者通吃现象引起，这些现象导致模型收敛到狭窄的解决方案策略。通过实施聚焦于低概率问题的数据整理算法，可以有效克服这些限制，显著提高Pass@k性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key
method for improving Large Language Models' reasoning capabilities, yet recent
evidence suggests it may paradoxically shrink the reasoning boundary rather
than expand it. This paper investigates the shrinkage issue of RLVR by
analyzing its learning dynamics and reveals two critical phenomena that explain
this failure. First, we expose negative interference in RLVR, where learning to
solve certain training problems actively reduces the likelihood of correct
solutions for others, leading to the decline of Pass@$k$ performance, or the
probability of generating a correct solution within $k$ attempts. Second, we
uncover the winner-take-all phenomenon: RLVR disproportionately reinforces
problems with high likelihood, correct solutions, under the base model, while
suppressing other initially low-likelihood ones. Through extensive theoretical
and empirical analysis on multiple mathematical reasoning benchmarks, we show
that this effect arises from the inherent on-policy sampling in standard RL
objectives, causing the model to converge toward narrow solution strategies.
Based on these insights, we propose a simple yet effective data curation
algorithm that focuses RLVR learning on low-likelihood problems, achieving
notable improvement in Pass@$k$ performance. Our code is available at
https://github.com/mail-research/SELF-llm-interference.

</details>


### [175] [The Unreasonable Effectiveness of Scaling Agents for Computer Use](https://arxiv.org/abs/2510.02250)
*Gonzalo Gonzalez-Pumariega,Vincent Tu,Chih-Lun Lee,Jiachen Yang,Ang Li,Xin Eric Wang*

Main category: cs.AI

TL;DR: 本文提出Behavior Best-of-N (bBoN) 方法，通过生成多个智能体执行过程并利用行为叙述进行选择，显著提高了计算机使用智能体(CUA)在复杂任务中的鲁棒性和成功率，在OSWorld上实现了新的SoTA，并展示了强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 计算机使用智能体（CUAs）在自动化日常数字任务方面具有潜力，但其不可靠性和高方差限制了它们在长期、复杂任务中的应用。

Method: 引入Behavior Best-of-N (bBoN) 方法，通过生成多个智能体执行过程（rollouts），并利用描述这些过程的行为叙述（behavior narratives）进行选择，从而实现智能体的扩展。该方法支持广泛探索和有原则的轨迹选择。

Result: bBoN方法在OSWorld上创造了69.9%的新SOTA，显著优于现有方法，并接近人类水平（72%）。通过全面的消融实验验证了关键设计选择。同时，在WindowsAgentArena和AndroidWorld上展示了对不同操作系统的强大泛化能力。

Conclusion: 研究结果强调了正确扩展CUAs的显著效果：有效的扩展需要结构化的轨迹理解和选择，bBoN为此提供了一个实用的框架。

Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital
tasks, but their unreliability and high variance hinder their application to
long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method
that scales over agents by generating multiple rollouts and selecting among
them using behavior narratives that describe the agents' rollouts. It enables
both wide exploration and principled trajectory selection, substantially
improving robustness and success rates. On OSWorld, our bBoN scaling method
establishes a new state of the art (SoTA) at 69.9%, significantly outperforming
prior methods and approaching human-level performance at 72%, with
comprehensive ablations validating key design choices. We further demonstrate
strong generalization results to different operating systems on
WindowsAgentArena and AndroidWorld. Crucially, our results highlight the
unreasonable effectiveness of scaling CUAs, when you do it right: effective
scaling requires structured trajectory understanding and selection, and bBoN
provides a practical framework to achieve this.

</details>


### [176] [RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems](https://arxiv.org/abs/2510.02263)
*Yuxiao Qu,Anikait Singh,Yoonho Lee,Amrith Setlur,Ruslan Salakhutdinov,Chelsea Finn,Aviral Kumar*

Main category: cs.AI

TL;DR: 本文提出“推理抽象”和RLAD双玩家强化学习范式，通过生成简洁的程序性/事实性知识指导大型模型进行结构化探索，克服现有模型推理中程序复用不足和冗余探索的问题，从而提升算法推理能力和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有大型模型在长链推理中难以持续捕获和复用“算法程序”，其推理轨迹常流于冗长和低效探索，无法有效地识别和实施解决难题所需的算法步骤，即未能超越模式匹配实现真正的算法推理。

Method: 引入“推理抽象”：简洁的自然语言描述，包含程序性和事实性知识，用于指导模型学习成功推理。训练模型针对给定问题提出多个抽象，并通过强化学习（RL）激励模型利用这些抽象构建解决方案。这形成了一个名为RLAD的双玩家RL训练范式，共同训练一个抽象生成器和一个解决方案生成器。

Result: 该方法有效实现了结构化探索，解耦了抽象提案和解决方案生成的学习信号，并显著提高了模型对更难问题的泛化能力。实验表明，在测试时将更多计算资源分配给生成抽象比生成更多解决方案，对提升性能更有益。

Conclusion: 推理抽象在指导有意义的探索中扮演关键角色，能够帮助模型学习更有效的算法推理，从而克服纯粹模式匹配的局限性，更好地解决复杂问题。

Abstract: Reasoning requires going beyond pattern matching or memorization of solutions
to identify and implement "algorithmic procedures" that can be used to deduce
answers to hard problems. Doing so requires realizing the most relevant
primitives, intermediate results, or shared procedures, and building upon them.
While RL post-training on long chains of thought ultimately aims to uncover
this kind of algorithmic behavior, most reasoning traces learned by large
models fail to consistently capture or reuse procedures, instead drifting into
verbose and degenerate exploration. To address more effective reasoning, we
introduce reasoning abstractions: concise natural language descriptions of
procedural and factual knowledge that guide the model toward learning
successful reasoning. We train models to be capable of proposing multiple
abstractions given a problem, followed by RL that incentivizes building a
solution while using the information provided by these abstractions. This
results in a two-player RL training paradigm, abbreviated as RLAD, that jointly
trains an abstraction generator and a solution generator. This setup
effectively enables structured exploration, decouples learning signals of
abstraction proposal and solution generation, and improves generalization to
harder problems. We also show that allocating more test-time compute to
generating abstractions is more beneficial for performance than generating more
solutions at large test budgets, illustrating the role of abstractions in
guiding meaningful exploration.

</details>


### [177] [BioX-Bridge: Model Bridging for Unsupervised Cross-Modal Knowledge Transfer across Biosignals](https://arxiv.org/abs/2510.02276)
*Chenqi Li,Yu Liu,Timothy Denison,Tingting Zhu*

Main category: cs.AI

TL;DR: 提出了一种名为BioX-Bridge的轻量级桥接网络框架，用于生物信号的无监督跨模态知识迁移，能在大幅减少可训练参数的同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 生物信号分析面临缺乏大型标注数据集的挑战。现有的跨模态知识迁移方法（如知识蒸馏）计算和内存开销大，尤其在与大型基础模型结合时问题更甚，因此需要一种更高效的无监督跨模态知识迁移方法。

Method: 引入BioX-Bridge框架，通过训练一个轻量级桥接网络来对齐基础模型之间的中间表示，并促进跨模态信息流。该方法包含一个高效的对齐位置选择策略和一个灵活的原型网络作为桥接架构。

Result: BioX-Bridge在多个生物信号模态、任务和数据集上，与现有先进方法相比，可训练参数量减少了88%至99%，同时保持甚至提升了知识迁移性能。

Conclusion: BioX-Bridge提供了一种高效且有效的解决方案，通过轻量级桥接网络实现了生物信号的无监督跨模态知识迁移，成功克服了数据稀缺和大型模型计算成本高的挑战。

Abstract: Biosignals offer valuable insights into the physiological states of the human
body. Although biosignal modalities differ in functionality, signal fidelity,
sensor comfort, and cost, they are often intercorrelated, reflecting the
holistic and interconnected nature of human physiology. This opens up the
possibility of performing the same tasks using alternative biosignal
modalities, thereby improving the accessibility, usability, and adaptability of
health monitoring systems. However, the limited availability of large labeled
datasets presents challenges for training models tailored to specific tasks and
modalities of interest. Unsupervised cross-modal knowledge transfer offers a
promising solution by leveraging knowledge from an existing modality to support
model training for a new modality. Existing methods are typically based on
knowledge distillation, which requires running a teacher model alongside
student model training, resulting in high computational and memory overhead.
This challenge is further exacerbated by the recent development of foundation
models that demonstrate superior performance and generalization across tasks at
the cost of large model sizes. To this end, we explore a new framework for
unsupervised cross-modal knowledge transfer of biosignals by training a
lightweight bridge network to align the intermediate representations and enable
information flow between foundation models and across modalities. Specifically,
we introduce an efficient strategy for selecting alignment positions where the
bridge should be constructed, along with a flexible prototype network as the
bridge architecture. Extensive experiments across multiple biosignal
modalities, tasks, and datasets show that BioX-Bridge reduces the number of
trainable parameters by 88--99\% while maintaining or even improving transfer
performance compared to state-of-the-art methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [178] [How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning](https://arxiv.org/abs/2510.02265)
*Yalin E. Sagduyu,Tugba Erpek,Kemal Davaslioglu,Sastry Kompella*

Main category: cs.LG

TL;DR: 本文研究如何利用强化学习（RL）帮助收发机在动态反应式干扰环境中，无先验知识地自适应调整传输参数（功率、调制、信道），以规避干扰并优化吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在存在动态反应式干扰（干扰器采用动态策略选择信道和感知阈值）的环境中，收发机如何在缺乏信道条件或干扰策略先验知识的情况下，有效规避干扰并优化通信吞吐量。

Method: 收发机通过强化学习（RL）来适应性地调整发射功率、调制方式和信道选择。具体方法包括对离散干扰事件状态采用Q-learning，以及对基于接收功率的连续状态采用Deep Q-Networks (DQN)。

Result: 研究结果表明，通过不同的奖励函数和动作集，强化学习能够快速适应频谱动态，并在信道和干扰策略随时间变化时，持续维持高数据速率。

Conclusion: 强化学习是解决动态反应式干扰场景下通信优化问题的有效方法，它能使收发机在未知环境中进行快速自适应调整并维持高性能。

Abstract: This paper studies the problem of mitigating reactive jamming, where a jammer
adopts a dynamic policy of selecting channels and sensing thresholds to detect
and jam ongoing transmissions. The transmitter-receiver pair learns to avoid
jamming and optimize throughput over time (without prior knowledge of channel
conditions or jamming strategies) by using reinforcement learning (RL) to adapt
transmit power, modulation, and channel selection. Q-learning is employed for
discrete jamming-event states, while Deep Q-Networks (DQN) are employed for
continuous states based on received power. Through different reward functions
and action sets, the results show that RL can adapt rapidly to spectrum
dynamics and sustain high rates as channels and jamming policies change over
time.

</details>


### [179] [Accelerating Long-Term Molecular Dynamics with Physics-Informed Time-Series Forecasting](https://arxiv.org/abs/2510.01206)
*Hung Le,Sherif Abbas,Minh Hoang Nguyen,Van Dai Do,Huu Hiep Nguyen,Dung Nguyen*

Main category: cs.LG

TL;DR: 通过将分子动力学(MD)模拟视为时间序列预测问题，并融入物理信息损失函数，本研究提出一种高效且准确预测原子轨迹的方法，可替代昂贵的密度泛函理论(DFT)模拟。


<details>
  <summary>Details</summary>
Motivation: 传统的DFT方法计算成本高昂，限制了MD模拟的长期性和可行性，而高效的MD模拟对于理解材料科学和生物物理学中的原子尺度过程至关重要。

Method: 将MD模拟建模为时间序列预测问题，通过预测原子位移而非绝对位置来预测轨迹。引入了基于DFT参数化对势（Morse势）的物理信息损失和推理机制，惩罚非物理原子接近度以确保物理合理性。

Result: 该方法在不同材料上均持续超越标准基线的模拟精度。它能在数分钟内稳定模拟数千步MD，为昂贵的DFT模拟提供了一个可扩展的替代方案。

Conclusion: 结合物理知识对于提高原子轨迹预测的可靠性和精度至关重要。该方法为长期、大规模的MD模拟提供了一个高效且可扩展的解决方案。

Abstract: Efficient molecular dynamics (MD) simulation is vital for understanding
atomic-scale processes in materials science and biophysics. Traditional density
functional theory (DFT) methods are computationally expensive, which limits the
feasibility of long-term simulations. We propose a novel approach that
formulates MD simulation as a time-series forecasting problem, enabling
advanced forecasting models to predict atomic trajectories via displacements
rather than absolute positions. We incorporate a physics-informed loss and
inference mechanism based on DFT-parametrised pair-wise Morse potential
functions that penalize unphysical atomic proximity to enforce physical
plausibility. Our method consistently surpasses standard baselines in
simulation accuracy across diverse materials. The results highlight the
importance of incorporating physics knowledge to enhance the reliability and
precision of atomic trajectory forecasting. Remarkably, it enables stable
modeling of thousands of MD steps in minutes, offering a scalable alternative
to costly DFT simulations.

</details>


### [180] [Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs](https://arxiv.org/abs/2510.01218)
*Sergey Troshin,Wafaa Mohammed,Yan Meng,Christof Monz,Antske Fokkens,Vlad Niculae*

Main category: cs.LG

TL;DR: 提出“选择性采样”方法，通过动态切换贪婪和高温采样，以提升语言模型在数学推理等高精度任务中的质量-多样性权衡。


<details>
  <summary>Details</summary>
Motivation: 高温采样虽能增加语言模型输出多样性，但在数学推理等高精度任务中会降低推理质量，原因是敏感解码位置采样了不正确的续写。

Method: 引入“选择性采样”，根据一个估计当前token位置输出错误可能性的“采样风险指标”，动态地在贪婪采样和高温采样间切换。该风险指标由一个在小规模可验证问题上训练的轻量级分类器预测，且集成开销极低。

Result: 在数学推理任务上的实验表明，选择性采样即使在高温设置下也能显著提升质量-多样性权衡。

Conclusion: 选择性采样有效解决了高温采样在精度任务中导致的质量下降问题，实现了更好的质量与多样性平衡，并具有最小的延迟开销。

Abstract: Diversity is an essential metric for evaluating the creativity of outputs
generated by language models. Temperature-based sampling is a common strategy
to increase diversity. However, for tasks that require high precision, e.g.,
mathematical reasoning, uncontrolled high temperature sampling, e.g., min-$p$
or top-$p$, degrades reasoning quality. We demonstrate that the loss of
accuracy is caused by sampling incorrect continuations in sensitive decoding
positions. To address this, in this paper, we propose \textbf{selective
sampling}, a method that dynamically switches between greedy and
high-temperature sampling based on a sampling risk metric. This risk metric
estimates the likelihood of output errors when applying high-temperature
sampling on the current token position. To predict sampling risk, we train a
lightweight classifier on a small subset of verifiable problems. The trained
classifier can be integrated with the base language model with minimal latency
overhead. Experiments on mathematical reasoning tasks demonstrate that
selective sampling enhances the quality-diversity trade-off, even in
high-temperature settings.

</details>


### [181] [Automated Extraction of Material Properties using LLM-based AI Agents](https://arxiv.org/abs/2510.01235)
*Subham Ghosh,Abhishek Tewari*

Main category: cs.LG

TL;DR: 研究开发了一个由大型语言模型（LLM）驱动的代理工作流，从数万篇科学文献中自动提取热电和结构属性，构建了迄今为止最大的LLM策展热电数据集，并通过分析验证了已知趋势并发现了新的结构-性能关联。


<details>
  <summary>Details</summary>
Motivation: 材料的快速发现受到缺乏大型、机器可读且结合性能指标与结构语境的数据集的限制。现有数据库规模小、手动整理或偏向第一性原理结果，导致实验文献未被充分利用。

Method: 开发了一个整合动态令牌分配、零样本多代理提取和条件表格解析的LLM驱动代理工作流，用于从约10,000篇科学文章中自主提取热电和结构属性。通过对50篇论文的基准测试，评估了GPT-4.1和GPT-4.1 Mini在准确性和计算成本方面的性能。

Result: 成功策展了包含27,822条温度分辨属性记录的热电数据集，是迄今最大的LLM策展热电数据集。GPT-4.1在热电属性（F1=0.91）和结构字段（F1=0.82）上取得了最高准确度，而GPT-4.1 Mini以较低成本实现了接近的性能。数据集分析再现了已知的热电趋势（如合金优于氧化物、p型掺杂的优势），并揭示了更广泛的结构-性能相关性。同时发布了一个交互式网络浏览器。

Conclusion: 该研究提供了迄今最大的LLM策展热电数据集，建立了一个可复现且具有成本效益的提取管道，为可扩展的数据驱动材料发现（超越热电材料）奠定了基础。

Abstract: The rapid discovery of materials is constrained by the lack of large,
machine-readable datasets that couple performance metrics with structural
context. Existing databases are either small, manually curated, or biased
toward first principles results, leaving experimental literature
underexploited. We present an agentic, large language model (LLM)-driven
workflow that autonomously extracts thermoelectric and structural-properties
from about 10,000 full-text scientific articles. The pipeline integrates
dynamic token allocation, zeroshot multi-agent extraction, and conditional
table parsing to balance accuracy against computational cost. Benchmarking on
50 curated papers shows that GPT-4.1 achieves the highest accuracy (F1 = 0.91
for thermoelectric properties and 0.82 for structural fields), while GPT-4.1
Mini delivers nearly comparable performance (F1 = 0.89 and 0.81) at a fraction
of the cost, enabling practical large scale deployment. Applying this workflow,
we curated 27,822 temperature resolved property records with normalized units,
spanning figure of merit (ZT), Seebeck coefficient, conductivity, resistivity,
power factor, and thermal conductivity, together with structural attributes
such as crystal class, space group, and doping strategy. Dataset analysis
reproduces known thermoelectric trends, such as the superior performance of
alloys over oxides and the advantage of p-type doping, while also surfacing
broader structure-property correlations. To facilitate community access, we
release an interactive web explorer with semantic filters, numeric queries, and
CSV export. This study delivers the largest LLM-curated thermoelectric dataset
to date, provides a reproducible and cost-profiled extraction pipeline, and
establishes a foundation for scalable, data-driven materials discovery beyond
thermoelectrics.

</details>


### [182] [RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models](https://arxiv.org/abs/2510.01240)
*Zukang Xu,Xing Hu,Qiang Wu,Dawei Yang*

Main category: cs.LG

TL;DR: 本文提出RSAVQ，一个利用信息几何优化LLMs极低位（2-4比特）向量量化的新框架，通过误差方向敏感性指导和权重通道敏感性指导，有效解决了量化误差和位分配问题，并在实验中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽性能出色，但参数量庞大，难以部署于资源受限设备。现有向量量化（VQ）方法在实现极低位量化时，面临无约束的方向误差和次优的位分配两大挑战。

Method: 本文提出RSAVQ，一个基于几何驱动创新的新型VQ框架：1. **误差方向敏感性指导（EDSG）**：利用Fisher信息矩阵（FIM）诱导的黎曼度量，将量化误差投影到参数空间的低敏感度方向，沿负自然梯度方向抑制误差扩散。2. **权重通道敏感性指导（WCSG）**：通过FIM曲率分析构建通道级敏感度度量，动态优化位资源分配，实现全局最优量化。

Result: 实验表明RSAVQ优于现有方法。在LLaMA-3 8B的2比特量化中，RSAVQ在困惑度（PPL）上领先VPTQ和QuIP#等基线0.4，在零样本准确率上领先1.5。

Conclusion: RSAVQ为资源受限环境提供了实用的解决方案，并为信息几何与神经网络量化之间建立了理论联系，推动了高效深度学习的进步。

Abstract: Large language models (LLMs) have demonstrated remarkable performance across
a wide range of natural language processing tasks. However, their exponentially
increasing parameters pose significant challenges for deployment on
resource-constrained devices. Vector Quantization (VQ) shows great promise for
low-bit quantization (e.g., 2 to 4 bits), but existing work faces two key
challenges: unconstrained direction error and suboptimal bit allocation. In
this paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit
quantization for LLMs. RSAVQ introduces two geometry-driven innovations that
effectively mitigate above limitations: (1) Error Direction Sensitivity
Guidance (EDSG), which leverages the Fisher Information Matrix (FIM)-induced
Riemannian metric to project quantization errors onto low-sensitivity
directions in the parameter space. Specifically, this projection is performed
along the negative natural gradient direction, which effectively suppresses
error expansion. (2) Weight Channel Sensitivity Guidance (WCSG) , which
constructs a channel-wise sensitivity metric via FIM curvature analysis to
dynamically guide bit resource allocation. The approach facilitates a globally
optimal quantization solution within prescribed bit constraints. Experiments
demonstrate that RSAVQ outperforms existing methods for LLMs. For example, in
2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP# by
0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy. This work offers a
practical solution for constrained environments and a theoretical bridge
between information geometry and the quantization of neural networks, advancing
efficient deep learning.

</details>


### [183] [Adaptive Federated Learning Defences via Trust-Aware Deep Q-Networks](https://arxiv.org/abs/2510.01261)
*Vedant Palit*

Main category: cs.LG

TL;DR: 联邦学习在部分可观测性下易受攻击。本文提出一种信任感知的DQN防御机制，通过整合多信号证据更新客户端信任，优化长期鲁棒性-准确性目标，实验证明其在鲁棒性-准确性权衡方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在部分可观测环境下易遭受投毒和后门攻击。

Method: 将防御建模为部分可观测的序贯决策问题，并引入一个信任感知深度Q网络（DQN），该网络整合多信号证据进行客户端信任更新，以优化长期的鲁棒性-准确性目标。

Result: ['建立了准确性稳步提升的基线。', '通过Dirichlet扫描显示，增加客户端重叠可一致性地提高准确性并降低攻击成功率（ASR），同时保持稳定检测。', '信号预算研究表明，在可观测性降低时，准确性保持稳定，而ASR增加、ROC-AUC下降，但序列信念更新能缓解弱信号影响。', '与随机、线性Q和策略梯度控制器相比，DQN在鲁棒性-准确性权衡方面表现最佳。']

Conclusion: 信任感知DQN能有效防御联邦学习中的攻击，尤其是在部分可观测环境下，通过序列信念更新缓解弱信号，实现了最佳的鲁棒性-准确性权衡。

Abstract: Federated learning is vulnerable to poisoning and backdoor attacks under
partial observability. We formulate defence as a partially observable
sequential decision problem and introduce a trust-aware Deep Q-Network that
integrates multi-signal evidence into client trust updates while optimizing a
long-horizon robustness--accuracy objective. On CIFAR-10, we (i) establish a
baseline showing steadily improving accuracy, (ii) show through a Dirichlet
sweep that increased client overlap consistently improves accuracy and reduces
ASR with stable detection, and (iii) demonstrate in a signal-budget study that
accuracy remains steady while ASR increases and ROC-AUC declines as
observability is reduced, which highlights that sequential belief updates
mitigate weaker signals. Finally, a comparison with random, linear-Q, and
policy gradient controllers confirms that DQN achieves the best
robustness--accuracy trade-off.

</details>


### [184] [RSTGCN: Railway-centric Spatio-Temporal Graph Convolutional Network for Train Delay Prediction](https://arxiv.org/abs/2510.01262)
*Koyena Chowdhury,Paramita Koley,Abhijnan Chakraborty,Saptarshi Ghosh*

Main category: cs.LG

TL;DR: 提出RSTGCN模型和印度铁路开放数据集，用于预测车站平均列车延误，并在大规模铁路网络中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 准确预测列车延误对于高效铁路运营至关重要。现有研究多关注个体列车延误或车站级预测，但针对特定时间段内车站所有进港列车平均到达延误的预测，尤其是在大规模网络中，仍有待深入探索。

Method: 提出了Railway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN)，该模型旨在预测特定时间段内车站的平均到达延误。RSTGCN包含多项架构创新和特征整合，特别是列车频率感知的空间注意力机制。此外，策展并发布了覆盖整个印度铁路网络的综合数据集（包含4,735个车站），是迄今为止规模最大、多样性最高的铁路网络数据集。

Result: 通过与多个最先进基线进行广泛实验，RSTGCN在标准评估指标上展现了持续且显著的性能改进。

Conclusion: 本工作不仅提升了大规模铁路网络中平均延误预测的建模水平，还提供了一个开放数据集，旨在鼓励该关键领域的后续研究。

Abstract: Accurate prediction of train delays is critical for efficient railway
operations, enabling better scheduling and dispatching decisions. While earlier
approaches have largely focused on forecasting the exact delays of individual
trains, recent studies have begun exploring station-level delay prediction to
support higher-level traffic management. In this paper, we propose the
Railway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN), designed
to forecast average arrival delays of all the incoming trains at railway
stations for a particular time period. Our approach incorporates several
architectural innovations and novel feature integrations, including train
frequency-aware spatial attention, which significantly enhances predictive
performance. To support this effort, we curate and release a comprehensive
dataset for the entire Indian Railway Network (IRN), spanning 4,735 stations
across 17 zones - the largest and most diverse railway network studied to date.
We conduct extensive experiments using multiple state-of-the-art baselines,
demonstrating consistent improvements across standard metrics. Our work not
only advances the modeling of average delay prediction in large-scale rail
networks but also provides an open dataset to encourage further research in
this critical domain.

</details>


### [185] [Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency](https://arxiv.org/abs/2510.01263)
*Yaron Meirovitch,Fuming Yang,Jeff Lichtman,Nir Shavit*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Most pruning methods remove parameters ranked by impact on loss (e.g.,
magnitude or gradient). We propose Budgeted Broadcast (BB), which gives each
unit a local traffic budget (the product of its long-term on-rate $a_i$ and
fan-out $k_i$). A constrained-entropy analysis shows that maximizing coding
entropy under a global traffic budget yields a selectivity-audience balance,
$\log\frac{1-a_i}{a_i}=\beta k_i$. BB enforces this balance with simple local
actuators that prune either fan-in (to lower activity) or fan-out (to reduce
broadcast). In practice, BB increases coding entropy and decorrelation and
improves accuracy at matched sparsity across Transformers for ASR, ResNets for
face identification, and 3D U-Nets for synapse prediction, sometimes exceeding
dense baselines. On electron microscopy images, it attains state-of-the-art F1
and PR-AUC under our evaluation protocol. BB is easy to integrate and suggests
a path toward learning more diverse and efficient representations.

</details>


### [186] [A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab](https://arxiv.org/abs/2510.01264)
*Isaac Peterson,Christopher Allred,Jacob Morrey,Mario Harper*

Main category: cs.LG

TL;DR: 该研究扩展了IsaacLab框架，以支持在逼真物理模拟中可扩展地训练对抗性多智能体强化学习策略，并引入了新的对抗性MARL环境。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）在机器人协作中至关重要，但现有工作多集中于协作环境。追捕-逃逸、安全等现实应用中的对抗性交互同样关键，需要一个能有效训练对抗策略的框架。

Method: 扩展了IsaacLab框架以支持在高质量物理模拟中训练对抗性策略；引入了一套新的对抗性MARL环境，其特点是智能体异构且目标和能力不对称；集成了HAPPO（Heterogeneous Agent Reinforcement Learning with Proximal Policy Optimization）的竞争性变体。

Result: 实验证明，该框架能够为形态各异的多智能体竞争建模并训练出鲁棒策略，同时保持高吞吐量和模拟真实性。

Conclusion: 该工作提供了一个强大的平台，用于在逼真物理模拟中高效地开发和评估对抗性MARL策略，为机器人系统在竞争环境下的应用提供了关键支持。

Abstract: Multi-Agent Reinforcement Learning (MARL) is central to robotic systems
cooperating in dynamic environments. While prior work has focused on these
collaborative settings, adversarial interactions are equally critical for
real-world applications such as pursuit-evasion, security, and competitive
manipulation. In this work, we extend the IsaacLab framework to support
scalable training of adversarial policies in high-fidelity physics simulations.
We introduce a suite of adversarial MARL environments featuring heterogeneous
agents with asymmetric goals and capabilities. Our platform integrates a
competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal
Policy Optimization (HAPPO), enabling efficient training and evaluation under
adversarial dynamics. Experiments across several benchmark scenarios
demonstrate the framework's ability to model and train robust policies for
morphologically diverse multi-agent competition while maintaining high
throughput and simulation realism. Code and benchmarks are available at:
https://github.com/DIRECTLab/IsaacLab-HARL .

</details>


### [187] [RLP: Reinforcement as a Pretraining Objective](https://arxiv.org/abs/2510.01265)
*Ali Hatamizadeh,Syeda Nahida Akter,Shrimai Prabhumoye,Jan Kautz,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Yejin Choi*

Main category: cs.LG

TL;DR: 本文提出RLP（强化预训练）目标，将强化学习的核心精神（探索）引入大型推理模型预训练的最后阶段，通过信息增益奖励链式思考，显著提升了模型的推理能力和独立思考行为。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型的训练范式是先进行大量数据上的下一词预测预训练，强化学习仅作为监督微调后的最后阶段引入。作者质疑这是否是最佳方式，旨在将强化学习的“探索”精神更早地引入预训练阶段，以期更早地培养模型的独立思考能力。

Method: RLP将链式思考视为一种探索性动作，奖励基于其为预测未来token提供的信息增益计算。具体来说，奖励信号衡量在同时给定上下文和采样推理链的条件下，下一个token的对数似然相对于仅给定上下文的增加量。这种方法产生了一种无需验证器的密集奖励信号，支持在预训练期间对完整文档流进行高效训练，从而在普通文本上将推理的强化学习重构为预训练目标。

Result: 使用RLP对Qwen3-1.7B-Base进行预训练，八项数学和科学基准测试的整体平均成绩提升了19%。在相同的后训练下，收益持续累积，尤其在AIME25和MMLU-Pro等推理密集型任务上改进最大。将RLP应用于混合Nemotron-Nano-12B-v2，整体平均成绩从42.81%提高到61.32%，科学推理平均成绩提升了23%，证明了其在不同架构和模型尺寸上的可扩展性。

Conclusion: RLP通过在预训练阶段引入信息驱动的强化学习，成功弥合了下一词预测与有用链式思考推理出现之间的鸿沟。它鼓励模型在预测之前进行独立思考，从而显著提高了大型推理模型的推理能力和泛化性，并展现了良好的可扩展性。

Abstract: The dominant paradigm for training large reasoning models starts with
pre-training using next-token prediction loss on vast amounts of data.
Reinforcement learning, while powerful in scaling reasoning, is introduced only
as the very last phase of post-training, preceded by supervised fine-tuning.
While dominant, is this an optimal way of training? In this paper, we present
RLP, an information-driven reinforcement pretraining objective, that brings the
core spirit of reinforcement learning -- exploration -- to the last phase of
pretraining. The key idea is to treat chain-of-thought as an exploratory
action, with rewards computed based on the information gain it provides for
predicting future tokens. This training objective essentially encourages the
model to think for itself before predicting what comes next, thus teaching an
independent thinking behavior earlier in the pretraining. More concretely, the
reward signal measures the increase in log-likelihood of the next token when
conditioning on both context and a sampled reasoning chain, compared to
conditioning on context alone. This approach yields a verifier-free dense
reward signal, allowing for efficient training for the full document stream
during pretraining. Specifically, RLP reframes reinforcement learning for
reasoning as a pretraining objective on ordinary text, bridging the gap between
next-token prediction and the emergence of useful chain-of-thought reasoning.
Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an
eight-benchmark math-and-science suite by 19%. With identical post-training,
the gains compound, with the largest improvements on reasoning-heavy tasks such
as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2
increases the overall average from 42.81% to 61.32% and raises the average on
scientific reasoning by 23%, demonstrating scalability across architectures and
model sizes.

</details>


### [188] [Safe Reinforcement Learning-Based Vibration Control: Overcoming Training Risks with LQR Guidance](https://arxiv.org/abs/2510.01269)
*Rohan Vitthal Thorat,Juhi Singh,Rajdip Nayek*

Main category: cs.LG

TL;DR: 针对结构振动控制中强化学习（RL）在物理系统上训练的安全风险，本文提出一种LQR-RL混合控制框架，其中LQR基于随机模型引导RL训练，确保无模型特性的同时降低探索风险。


<details>
  <summary>Details</summary>
Motivation: 1. 结构振动带来安全隐患、结构损伤和高维护成本。2. 传统基于模型的控制（如LQR）依赖精确模型和繁琐的系统辨识。3. 无模型RL虽可避免系统辨识，但其在物理系统上训练时初期随机控制可能损害结构。4. 观察发现，即使是基于不准确模型的LQR也优于无控制，可作为RL的安全引导。

Method: 1. 提出一种融合LQR和RL的混合控制框架。2. LQR策略基于一个随机选择的模型及其参数推导，无需真实结构模型知识。3. 该LQR控制器用于在RL控制器于物理系统上训练期间提供引导，以降低探索风险。4. 整个框架保持无模型特性。

Result: 1. 该混合方法消除了对显式系统模型的依赖。2. 最大限度地降低了朴素RL实现中固有的探索风险。3. 本研究首次解决了RL振动控制中关键的训练安全挑战，并提供了经过验证的解决方案。

Conclusion: 所提出的LQR-RL混合控制框架，通过利用基于随机模型的LQR安全引导RL控制器训练，成功解决了在物理系统上训练无模型RL进行结构振动控制时的安全问题，同时保持了整体的无模型特性。

Abstract: Structural vibrations induced by external excitations pose significant risks,
including safety hazards for occupants, structural damage, and increased
maintenance costs. While conventional model-based control strategies, such as
Linear Quadratic Regulator (LQR), effectively mitigate vibrations, their
reliance on accurate system models necessitates tedious system identification.
This tedious system identification process can be avoided by using a model-free
Reinforcement learning (RL) method. RL controllers derive their policies solely
from observed structural behaviour, eliminating the requirement for an explicit
structural model. For an RL controller to be truly model-free, its training
must occur on the actual physical system rather than in simulation. However,
during this training phase, the RL controller lacks prior knowledge and it
exerts control force on the structure randomly, which can potentially harm the
structure. To mitigate this risk, we propose guiding the RL controller using a
Linear Quadratic Regulator (LQR) controller. While LQR control typically relies
on an accurate structural model for optimal performance, our observations
indicate that even an LQR controller based on an entirely incorrect model
outperforms the uncontrolled scenario. Motivated by this finding, we introduce
a hybrid control framework that integrates both LQR and RL controllers. In this
approach, the LQR policy is derived from a randomly selected model and its
parameters. As this LQR policy does not require knowledge of the true or an
approximate structural model the overall framework remains model-free. This
hybrid approach eliminates dependency on explicit system models while
minimizing exploration risks inherent in naive RL implementations. As per our
knowledge, this is the first study to address the critical training safety
challenge of RL-based vibration control and provide a validated solution.

</details>


### [189] [Identifying Information-Transfer Nodes in a Recurrent Neural Network Reveals Dynamic Representations](https://arxiv.org/abs/2510.01271)
*Arend Hintze,Asadullah Najam,Jory Schossau*

Main category: cs.LG

TL;DR: 本研究提出一种信息论方法来识别和分析循环神经网络（RNNs）中的信息中继节点，以增强其可解释性并优化设计。


<details>
  <summary>Details</summary>
Motivation: 理解循环神经网络（RNNs）的内部动态对于提升其可解释性和改进设计至关重要。

Method: 引入一种创新的信息论方法，通过量化节点间输入输出向量的互信息来识别和分析RNNs中的“信息中继”节点。同时，通过节点敲除实验评估这些已识别节点的功能重要性。

Result: 揭示了不同RNN架构（如LSTM和GRU）中信息中继的独特模式，提供了关于信息如何被处理和维持的见解。节点敲除实验进一步阐明了特定节点对网络整体行为的影响。

Conclusion: 本研究加深了对RNNs复杂机制的理解，并为设计更鲁棒、可解释的神经网络提供了有价值的工具。

Abstract: Understanding the internal dynamics of Recurrent Neural Networks (RNNs) is
crucial for advancing their interpretability and improving their design. This
study introduces an innovative information-theoretic method to identify and
analyze information-transfer nodes within RNNs, which we refer to as
\textit{information relays}. By quantifying the mutual information between
input and output vectors across nodes, our approach pinpoints critical pathways
through which information flows during network operations. We apply this
methodology to both synthetic and real-world time series classification tasks,
employing various RNN architectures, including Long Short-Term Memory (LSTM)
networks and Gated Recurrent Units (GRUs). Our results reveal distinct patterns
of information relay across different architectures, offering insights into how
information is processed and maintained over time. Additionally, we conduct
node knockout experiments to assess the functional importance of identified
nodes, significantly contributing to explainable artificial intelligence by
elucidating how specific nodes influence overall network behavior. This study
not only enhances our understanding of the complex mechanisms driving RNNs but
also provides a valuable tool for designing more robust and interpretable
neural networks.

</details>


### [190] [Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning](https://arxiv.org/abs/2510.01278)
*Hengwei Zhao,Zhengzhong Tu,Zhuo Zheng,Wei Wang,Junjue Wang,Rusty Feagin,Wenzhe Jiao*

Main category: cs.LG

TL;DR: 提出NcPU，一种无需辅助信息的非对比PU学习框架，通过解决不可靠监督下的判别性表示学习瓶颈，显著提升复杂数据集上的PU学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有PU学习方法在复杂数据集上（尤其缺乏辅助信息时）性能显著低于有监督学习，主要瓶颈在于不可靠监督下难以学习判别性表示。

Method: 提出NcPU框架，结合鲁棒的监督非对比损失（NoiSNCL）以对齐类内表示，并采用幻影标签消歧（PLD）方案通过基于后悔的标签更新提供保守的负监督，两者在EM框架下迭代互利。

Result: NoiSNCL使简单PU方法达到有竞争力性能；NcPU在多种数据集（包括灾后建筑损坏测绘等实际应用）上显著优于现有SOTA PU方法。

Conclusion: NcPU提供了一种无需辅助信息的有效PU学习解决方案，能够显著提升复杂数据集上的性能，在实际应用中具有广阔前景。

Abstract: Positive-Unlabeled (PU) learning aims to train a binary classifier (positive
vs. negative) where only limited positive data and abundant unlabeled data are
available. While widely applicable, state-of-the-art PU learning methods
substantially underperform their supervised counterparts on complex datasets,
especially without auxiliary negatives or pre-estimated parameters (e.g., a
14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the
challenge of learning discriminative representations under unreliable
supervision. To tackle this challenge, we propose NcPU, a non-contrastive PU
learning framework that requires no auxiliary information. NcPU combines a
noisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns
intra-class representations despite unreliable supervision, with a phantom
label disambiguation (PLD) scheme that supplies conservative negative
supervision via regret-based label updates. Theoretically, NoiSNCL and PLD can
iteratively benefit each other from the perspective of the
Expectation-Maximization framework. Empirically, extensive experiments
demonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive
performance; and (2) NcPU achieves substantial improvements over
state-of-the-art PU methods across diverse datasets, including challenging
datasets on post-disaster building damage mapping, highlighting its promise for
real-world applications. Code: Code will be open-sourced after review.

</details>


### [191] [Microsaccade-Inspired Probing: Positional Encoding Perturbations Reveal LLM Misbehaviours](https://arxiv.org/abs/2510.01288)
*Rui Melo,Rui Abreu,Corina S. Pasareanu*

Main category: cs.LG

TL;DR: 受人类微眼跳启发，本文提出一种基于轻量级位置编码扰动的方法，无需微调即可高效检测大型语言模型（LLMs）在事实性、安全性、毒性及后门攻击等方面的潜在错误行为。


<details>
  <summary>Details</summary>
Motivation: 受人类微眼跳能揭示感知隐藏动态的启发，研究旨在为大型语言模型（LLMs）开发一种类似的探测方法，以揭示其潜在的错误行为。

Method: 提出一种基于轻量级位置编码扰动的方法。通过对位置编码进行微小扰动，以激发LLMs内部的潜在信号，从而指示模型的错误行为。该方法无需进行微调或任务特定的监督。

Result: 在多个先进LLMs上的实验表明，这些基于扰动的探针能够有效地发现模型在事实性、安全性、毒性及后门攻击等方面的错误行为，并且保持计算效率。

Conclusion: 研究结果表明，预训练的LLMs内部已经编码了识别自身错误所需的证据，而受微眼跳启发的干预措施为检测和缓解LLMs的不良行为提供了一条新途径。

Abstract: We draw inspiration from microsaccades, tiny involuntary eye movements that
reveal hidden dynamics of human perception, to propose an analogous probing
method for large language models (LLMs). Just as microsaccades expose subtle
but informative shifts in vision, we show that lightweight position encoding
perturbations elicit latent signals that indicate model misbehaviour. Our
method requires no fine-tuning or task-specific supervision, yet detects
failures across diverse settings including factuality, safety, toxicity, and
backdoor attacks. Experiments on multiple state-of-the-art LLMs demonstrate
that these perturbation-based probes surface misbehaviours while remaining
computationally efficient. These findings suggest that pretrained LLMs already
encode the internal evidence needed to flag their own failures, and that
microsaccade-inspired interventions provide a pathway for detecting and
mitigating undesirable behaviours.

</details>


### [192] [ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models](https://arxiv.org/abs/2510.01290)
*Akshat Ramachandran,Marina Neseem,Charbel Sakr,Rangharajan Venkatesan,Brucek Khailany,Tushar Krishna*

Main category: cs.LG

TL;DR: ThinKV是一种思想自适应的KV缓存压缩框架，用于解决大型推理模型长CoT生成导致的KV缓存膨胀问题，通过混合量化-驱逐策略和优化内存复用，显著节省GPU内存并提高推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型生成长输出上下文（链式思考CoT）导致其键值（KV）缓存快速增长，迅速耗尽GPU内存，成为一个主要的性能瓶颈。

Method: ThinKV通过观察注意力稀疏性揭示CoT中不同重要性的思考类型。它采用混合量化-驱逐策略，根据思考的重要性分配token精度，并随推理轨迹演进逐步驱逐来自不重要思考的token。此外，ThinKV设计了一个扩展PagedAttention的内核，以高效重用被驱逐token的内存槽，从而消除内存整理开销。

Result: 在DeepSeek-R1-Distill、GPT-OSS和NVIDIA AceReason等模型上，跨数学和编码基准测试表明，ThinKV在占用不到5%原始KV缓存的情况下，实现了接近无损的精度。同时，它将推理吞吐量比现有SOTA基线提高了高达5.8倍。

Conclusion: ThinKV通过创新性的思想自适应KV缓存压缩框架，有效解决了大型推理模型KV缓存内存膨胀问题，显著提升了推理性能和内存效率，且基本不影响模型精度，为长上下文推理提供了高效的解决方案。

Abstract: The long-output context generation of large reasoning models enables extended
chain of thought (CoT) but also drives rapid growth of the key-value (KV)
cache, quickly overwhelming GPU memory. To address this challenge, we propose
ThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on
the observation that attention sparsity reveals distinct thought types with
varying importance within the CoT. It applies a hybrid quantization-eviction
strategy, assigning token precision by thought importance and progressively
evicting tokens from less critical thoughts as reasoning trajectories evolve.
Furthermore, to implement ThinKV, we design a kernel that extends
PagedAttention to enable efficient reuse of evicted tokens' memory slots,
eliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill,
GPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show
that ThinKV achieves near-lossless accuracy with less than 5% of the original
KV cache, while improving performance with up to 5.8x higher inference
throughput over state-of-the-art baselines.

</details>


### [193] [Network-Level Vehicle Delay Estimation at Heterogeneous Signalized Intersections](https://arxiv.org/abs/2510.01292)
*Xiaobo Ma,Hyunsoo Noh,James Tokishi,Ryan Hatch*

Main category: cs.LG

TL;DR: 本研究提出一种领域自适应框架，特别是GBBW模型，用于在不同交叉口准确估计车辆延误，解决了传统机器学习模型泛化性差的问题，提高了模型在真实交通系统中的可迁移性和应用性。


<details>
  <summary>Details</summary>
Motivation: 准确的车辆延误估算对评估信号交叉口性能和制定交通管理策略至关重要，但传统机器学习模型在面对不同交叉口（路几何、信号配时、驾驶行为）的数据分布差异时，泛化能力差，准确性降低。

Method: 引入了一个领域自适应(DA)框架，将数据分为源域和目标域，提取关键交通特征，并利用目标域的小量标记子集进行模型微调。提出了一种新的DA模型——基于平衡加权的梯度提升(GBBW)，通过衡量源域数据与目标域的相似性来重新加权源域数据。该框架在亚利桑那州皮马县57个异构交叉口的数据上进行了测试。

Result: 结果表明，GBBW框架比八种最先进的ML回归模型和七种基于实例的DA方法提供了更准确、更稳健的延误估计。

Conclusion: 该方法通过增强模型的可迁移性，支持更可靠的交通信号优化、拥堵管理和基于性能的规划，促进了机器学习技术在真实交通系统中的广泛部署。

Abstract: Accurate vehicle delay estimation is essential for evaluating the performance
of signalized intersections and informing traffic management strategies. Delay
reflects congestion levels and affects travel time reliability, fuel use, and
emissions. Machine learning (ML) offers a scalable, cost-effective alternative;
However, conventional models typically assume that training and testing data
follow the same distribution, an assumption that is rarely satisfied in
real-world applications. Variations in road geometry, signal timing, and driver
behavior across intersections often lead to poor generalization and reduced
model accuracy. To address this issue, this study introduces a domain
adaptation (DA) framework for estimating vehicle delays across diverse
intersections. The framework separates data into source and target domains,
extracts key traffic features, and fine-tunes the model using a small, labeled
subset from the target domain. A novel DA model, Gradient Boosting with
Balanced Weighting (GBBW), reweights source data based on similarity to the
target domain, improving adaptability. The framework is tested using data from
57 heterogeneous intersections in Pima County, Arizona. Performance is
evaluated against eight state-of-the-art ML regression models and seven
instance-based DA methods. Results demonstrate that the GBBW framework provides
more accurate and robust delay estimates. This approach supports more reliable
traffic signal optimization, congestion management, and performance-based
planning. By enhancing model transferability, the framework facilitates broader
deployment of machine learning techniques in real-world transportation systems.

</details>


### [194] [From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review](https://arxiv.org/abs/2510.01296)
*Emma McMillian,Abhirup Banerjee,Alfonso Bueno-Orovio*

Main category: cs.LG

TL;DR: 本综述全面分析了基于深度学习的2D MRI 3D形状重建方法，涵盖了四种主要模型，并探讨了其应用、局限性及未来方向。


<details>
  <summary>Details</summary>
Motivation: 2D MRI的3D形状重建在医学诊断、治疗计划和计算建模中日益重要。本综述旨在为研究人员提供当前3D重建方法的结构化概览，以识别推进深度学习向更稳健、通用和临床有效解决方案的机会。

Method: 本综述调查了3D MRI重建的方法学领域，主要关注点云、基于网格、形状感知和体素模型四种方法。对每种类别，分析了最先进技术、方法学基础、局限性、跨解剖结构的应用、临床适用性、训练/测试数据影响、公开数据集、计算需求和评估指标。

Result: 本综述提供了从心脏到神经到肺部成像的广泛概述，分析了各种模型在患病解剖结构上的临床适用性，并讨论了训练和测试数据的影响、公开数据集、计算需求和评估指标。

Conclusion: 强调了多模态整合和跨模态框架等新兴研究方向，旨在指导研究人员开发出更强大、通用且具有临床影响力的深度学习解决方案。

Abstract: Deep learning-based 3-dimensional (3D) shape reconstruction from
2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly
important in medical disease diagnosis, treatment planning, and computational
modeling. This review surveys the methodological landscape of 3D MRI
reconstruction, focusing on 4 primary approaches: point cloud, mesh-based,
shape-aware, and volumetric models. For each category, we analyze the current
state-of-the-art techniques, their methodological foundation, limitations, and
applications across anatomical structures. We provide an extensive overview
ranging from cardiac to neurological to lung imaging. We also focus on the
clinical applicability of models to diseased anatomy, and the influence of
their training and testing data. We examine publicly available datasets,
computational demands, and evaluation metrics. Finally, we highlight the
emerging research directions including multimodal integration and
cross-modality frameworks. This review aims to provide researchers with a
structured overview of current 3D reconstruction methodologies to identify
opportunities for advancing deep learning towards more robust, generalizable,
and clinically impactful solutions.

</details>


### [195] [Low Rank Gradients and Where to Find Them](https://arxiv.org/abs/2510.01303)
*Rishi Sonthalia,Michael Murray,Guido Montúfar*

Main category: cs.LG

TL;DR: 本文在放松各向同性假设下，研究了两层神经网络训练损失梯度的低秩结构，发现其主要由两个秩为一的项组成，并阐明了数据特性、尺度和正则化对其平衡的影响。


<details>
  <summary>Details</summary>
Motivation: 在放宽训练数据和参数的各向同性假设（例如允许数据主体各向异性、病态，不要求数据与权重矩阵独立）下，深入理解两层神经网络训练损失梯度的低秩结构。

Method: 采用棘刺数据模型，分析了平均场（mean-field）和神经切线核（NTK）两种尺度下的梯度行为。

Result: 1. 输入权重的梯度近似为低秩，且主要由两个秩为一的项主导：一个与主体数据残差对齐，另一个与输入数据中的秩一棘刺对齐。
2. 阐明了训练数据特性、尺度机制和激活函数如何影响这两项的平衡。
3. 标准正则化器（如权重衰减、输入噪声和雅可比惩罚）能选择性地调节这些分量。
4. 理论预测得到了合成数据和真实数据实验的证实。

Conclusion: 在更一般和现实的条件下，两层神经网络的梯度具有特定的低秩结构，这种结构及其组成部分的平衡受到训练数据特性、网络尺度和正则化策略的关键影响。

Abstract: This paper investigates low-rank structure in the gradients of the training
loss for two-layer neural networks while relaxing the usual isotropy
assumptions on the training data and parameters. We consider a spiked data
model in which the bulk can be anisotropic and ill-conditioned, we do not
require independent data and weight matrices and we also analyze both the
mean-field and neural-tangent-kernel scalings. We show that the gradient with
respect to the input weights is approximately low rank and is dominated by two
rank-one terms: one aligned with the bulk data-residue , and another aligned
with the rank one spike in the input data. We characterize how properties of
the training data, the scaling regime and the activation function govern the
balance between these two components. Additionally, we also demonstrate that
standard regularizers, such as weight decay, input noise and Jacobian
penalties, also selectively modulate these components. Experiments on synthetic
and real data corroborate our theoretical predictions.

</details>


### [196] [Quantum-inspired Benchmark for Estimating Intrinsic Dimension](https://arxiv.org/abs/2510.01335)
*Aritra Das,Joseph T. Iosue,Victor V. Albert*

Main category: cs.LG

TL;DR: 本文提出了一个名为QuIIEst的量子启发式内在维度估计（IDE）基准，用于评估现有IDE方法在更复杂流形上的表现。研究发现，现有IDE方法在QuIIEst流形上的准确性普遍低于现有基准。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在真实数据集上的良好泛化能力源于数据位于低维潜在流形上的假设。虽然存在多种内在维度估计（IDE）方法，但它们的估计结果差异很大。现有的基准测试流形复杂度不足，无法充分评估IDE方法，因此需要一个更复杂、更能反映实际情况的基准。

Method: 研究者提出了一个量子启发式内在维度估计（QuIIEst）基准。该基准包含无限族拓扑非平凡且已知内在维度的流形，其构建基于一种量子光学方法，能够嵌入任意齐次空间，并允许曲率修改和添加噪声。研究将现有IDE方法应用于QuIIEst流形进行测试。此外，还对分形霍夫施塔特蝴蝶进行了IDE，以识别能够提取非流形空间有效维度的方法。

Result: 被测试的IDE方法在QuIIEst流形上的准确性普遍低于在现有基准上的表现，即使资源分配相同。随着非均匀曲率的增加，性能下降不明显，突显了该基准固有的难度。作为一项独立发现，研究成功地在分形霍夫施塔特蝴蝶上进行了IDE，并识别出能够提取非流形空间有效维度的方法。

Conclusion: QuIIEst基准提供了一个更具挑战性、更真实的平台来评估内在维度估计方法，揭示了当前IDE方法在复杂流形上的局限性。同时，研究也探索了IDE方法在非流形空间中提取有效维度的能力。

Abstract: Machine learning models can generalize well on real-world datasets. According
to the manifold hypothesis, this is possible because datasets lie on a latent
manifold with small intrinsic dimension (ID). There exist many methods for ID
estimation (IDE), but their estimates vary substantially. This warrants
benchmarking IDE methods on manifolds that are more complex than those in
existing benchmarks. We propose a Quantum-Inspired Intrinsic-dimension
Estimation (QuIIEst) benchmark consisting of infinite families of topologically
non-trivial manifolds with known ID. Our benchmark stems from a quantum-optical
method of embedding arbitrary homogeneous spaces while allowing for curvature
modification and additive noise. The IDE methods tested were generally less
accurate on QuIIEst manifolds than on existing benchmarks under identical
resource allocation. We also observe minimal performance degradation with
increasingly non-uniform curvature, underscoring the benchmark's inherent
difficulty. As a result of independent interest, we perform IDE on the fractal
Hofstadter's butterfly and identify which methods are capable of extracting the
effective dimension of a space that is not a manifold.

</details>


### [197] [On the Identifiability of Latent Action Policies](https://arxiv.org/abs/2510.01337)
*Sébastien Lachapelle*

Main category: cs.LG

TL;DR: 研究了潜变量动作策略学习（LAPO）的可识别性，并证明了熵正则化的LAPO目标能在特定条件下识别出满足期望的动作表示。


<details>
  <summary>Details</summary>
Motivation: LAPO框架旨在从视频数据中发现动作表示。研究动机在于形式化描述这种表示的期望特性、统计优势及潜在的不可识别性来源，以深入理解并确保其理论基础。

Method: 形式化描述了动作表示的期望特性、统计优势及不可识别性来源。主要方法是通过理论分析，证明了熵正则化的LAPO目标在适当条件下能够识别出满足这些期望的动作表示。

Result: 证明了在适当条件下，熵正则化的LAPO目标能够识别出满足预设期望的动作表示。

Conclusion: 本研究的分析解释了为什么离散动作表示在实践中表现良好。

Abstract: We study the identifiability of latent action policy learning (LAPO), a
framework introduced recently to discover representations of actions from video
data. We formally describe desiderata for such representations, their
statistical benefits and potential sources of unidentifiability. Finally, we
prove that an entropy-regularized LAPO objective identifies action
representations satisfying our desiderata, under suitable conditions. Our
analysis provides an explanation for why discrete action representations
perform well in practice.

</details>


### [198] [Self-Supervised Representation Learning as Mutual Information Maximization](https://arxiv.org/abs/2510.01345)
*Akhlaqur Rahman Sabby,Yi Sui,Tongzi Wu,Jesse C. Cresswell,Ga Wu*

Main category: cs.LG

TL;DR: 本文从变分互信息下界推导出两种自监督学习（SSRL）训练范式：自蒸馏互信息（SDMI）和联合互信息（JMI）。这两种范式理论性地解释了现有SSRL方法中预测器网络、停止梯度和统计正则化器等架构组件的选择及其作用。


<details>
  <summary>Details</summary>
Motivation: 自监督表示学习（SSRL）取得了显著成功，但其深层原理，特别是预测器网络、停止梯度操作和统计正则化器等架构元素，仍未被充分理解，常被视为经验性添加。本研究旨在从第一性原理出发，探究SSRL算法的学习目标如何决定其可能的优化策略和模型设计选择。

Method: 研究从变分互信息（MI）下界出发，推导出了两种训练范式：自蒸馏互信息（SDMI）和联合互信息（JMI）。分析了每种范式所固有的结构约束和优化策略，并阐明了SDMI中的预测器网络和JMI中的统计正则化器作为MI目标的可处理替代品的角色。

Result: 研究发现，SDMI本质上需要交替优化，使得停止梯度操作在理论上是必不可少的；而JMI则通过对称架构实现联合优化，无需此类组件。此外，SDMI中的预测器网络和JMI中的统计正则化器被证明是互信息目标的可处理替代品。结果表明，许多现有SSRL方法是这两种范式的具体实例或近似。

Conclusion: 本研究为现有SSRL方法中不同架构组件的选择提供了坚实的理论解释，超越了单纯的启发式便利性，加深了对SSRL基本原理的理解。

Abstract: Self-supervised representation learning (SSRL) has demonstrated remarkable
empirical success, yet its underlying principles remain insufficiently
understood. While recent works attempt to unify SSRL methods by examining their
information-theoretic objectives or summarizing their heuristics for preventing
representation collapse, architectural elements like the predictor network,
stop-gradient operation, and statistical regularizer are often viewed as
empirically motivated additions. In this paper, we adopt a first-principles
approach and investigate whether the learning objective of an SSRL algorithm
dictates its possible optimization strategies and model design choices. In
particular, by starting from a variational mutual information (MI) lower bound,
we derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint
MI (JMI), each imposing distinct structural constraints and covering a set of
existing SSRL algorithms. SDMI inherently requires alternating optimization,
making stop-gradient operations theoretically essential. In contrast, JMI
admits joint optimization through symmetric architectures without such
components. Under the proposed formulation, predictor networks in SDMI and
statistical regularizers in JMI emerge as tractable surrogates for the MI
objective. We show that many existing SSRL methods are specific instances or
approximations of these two paradigms. This paper provides a theoretical
explanation behind the choices of different architectural components of
existing SSRL methods, beyond heuristic conveniences.

</details>


### [199] [To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking](https://arxiv.org/abs/2510.01349)
*Hannah Lawrence,Elyssa Hofgard,Vasco Portilheiro,Yuxuan Chen,Tess Smidt,Robin Walters*

Main category: cs.LG

TL;DR: 本文提出一种量化数据集各向异性（对称性破缺）的度量方法，发现它会影响对称性感知方法的性能，并强调需重新审视数据中的对称性偏差。


<details>
  <summary>Details</summary>
Motivation: 现有对称性感知机器学习方法（如数据增强、等变架构）依赖一个关键假设：变换后的数据点在测试分布中是高概率的。本研究旨在批判性地评估这一假设。

Method: 开发了一种基于两样本神经分类器测试的度量标准，用于量化数据集的各向异性或对称性破缺程度。该测试通过区分原始数据集及其随机增强版本来实现。

Result: ['该度量在合成数据集上得到验证。', '发现多个基准点云数据集中存在惊人高度的对齐（即低各向异性）。', '理论证明，即使底层标签是真正不变的，分布对称性破缺也会阻止不变方法达到最优性能。', '经验发现，对称性感知方法的效果因数据集而异：在某些各向异性数据集上仍有益处，但在另一些则没有。']

Conclusion: 理解等变方法何时以及为何有效，可能需要重新思考数据中存在的对称性偏差，以优化其应用。

Abstract: Symmetry-aware methods for machine learning, such as data augmentation and
equivariant architectures, encourage correct model behavior on all
transformations (e.g. rotations or permutations) of the original dataset. These
methods can improve generalization and sample efficiency, under the assumption
that the transformed datapoints are highly probable, or "important", under the
test distribution. In this work, we develop a method for critically evaluating
this assumption. In particular, we propose a metric to quantify the amount of
anisotropy, or symmetry-breaking, in a dataset, via a two-sample neural
classifier test that distinguishes between the original dataset and its
randomly augmented equivalent. We validate our metric on synthetic datasets,
and then use it to uncover surprisingly high degrees of alignment in several
benchmark point cloud datasets. We show theoretically that distributional
symmetry-breaking can actually prevent invariant methods from performing
optimally even when the underlying labels are truly invariant, as we show for
invariant ridge regression in the infinite feature limit. Empirically, we find
that the implication for symmetry-aware methods is dataset-dependent:
equivariant methods still impart benefits on some anisotropic datasets, but not
others. Overall, these findings suggest that understanding equivariance -- both
when it works, and why -- may require rethinking symmetry biases in the data.

</details>


### [200] [RheOFormer: A generative transformer model for simulation of complex fluids and flows](https://arxiv.org/abs/2510.01365)
*Maedeh Saberi,Amir Barati Farimani,Safa Jamali*

Main category: cs.LG

TL;DR: 提出RheOFormer，一种基于自注意力机制的生成式算子学习方法，用于高效准确地模拟复杂流体在流动条件下的力学行为，展现出强大的泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在非牛顿流体动力学模拟中计算成本高且可扩展性差。现有的数据驱动方法虽有改进，但仍需针对不同物理条件进行重新训练，限制了复杂流体模拟的效率和泛化能力。

Method: 引入Rheological Operator Transformer (RheOFormer)，一种生成式算子学习方法。该方法利用自注意力机制高效学习复杂流体流动的不同空间交互和特征，用于建模软材料在流动条件下的力学行为。

Result: RheOFormer能够准确学习不同复杂流体的标量和张量非线性力学，并预测其时空演化，即使在有限数据集上训练也能表现良好。

Conclusion: RheOFormer凭借其强大的泛化能力和计算效率，可作为鲁棒的神经网络替代品，加速预测性复杂流体模拟，推动数据驱动实验，并实现在广泛应用中的实时过程优化。

Abstract: The ability to model mechanics of soft materials under flowing conditions is
key in designing and engineering processes and materials with targeted
properties. This generally requires solution of internal stress tensor, related
to the deformation tensor through nonlinear and history-dependent constitutive
models. Traditional numerical methods for non-Newtonian fluid dynamics often
suffer from prohibitive computational demands and poor scalability to new
problem instances. Developments in data-driven methods have mitigated some
limitations but still require retraining across varied physical conditions. In
this work, we introduce Rheological Operator Transformer (RheOFormer), a
generative operator learning method leveraging self-attention to efficiently
learn different spatial interactions and features of complex fluid flows. We
benchmark RheOFormer across a range of different viscometric and
non-viscometric flows with different types of viscoelastic and
elastoviscoplastic mechanics in complex domains against ground truth solutions.
Our results demonstrate that RheOFormer can accurately learn both scalar and
tensorial nonlinear mechanics of different complex fluids and predict the
spatio-temporal evolution of their flows, even when trained on limited
datasets. Its strong generalization capabilities and computational efficiency
establish RheOFormer as a robust neural surrogate for accelerating predictive
complex fluid simulations, advancing data-driven experimentation, and enabling
real-time process optimization across a wide range of applications.

</details>


### [201] [Selective Underfitting in Diffusion Models](https://arxiv.org/abs/2510.01378)
*Kiwhan Song,Jaeyeon Kim,Sitan Chen,Yilun Du,Sham Kakade,Vincent Sitzmann*

Main category: cs.LG

TL;DR: 扩散模型存在选择性欠拟合现象，即在某些区域精确拟合分数函数，而在其他区域欠拟合，这对其泛化和生成性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 探究扩散模型实际学习的是哪种分数函数，以及为何它能生成新颖样本而非仅仅复制训练数据，并对现有“模型普遍欠拟合经验分数函数”的观点提出修正。

Method: 引入“选择性欠拟合”概念，主张更优的扩散模型在特定输入空间区域精确拟合分数函数，而在其他区域欠拟合；通过表征这些区域并设计实证干预来验证此观点。

Result: 研究结果表明，选择性欠拟合对于理解扩散模型至关重要，为它们的泛化能力和生成性能提供了新的、可验证的见解。

Conclusion: 选择性欠拟合是理解扩散模型行为及其生成新颖样本能力的关键机制。

Abstract: Diffusion models have emerged as the principal paradigm for generative
modeling across various domains. During training, they learn the score
function, which in turn is used to generate samples at inference. They raise a
basic yet unsolved question: which score do they actually learn? In principle,
a diffusion model that matches the empirical score in the entire data space
would simply reproduce the training data, failing to generate novel samples.
Recent work addresses this question by arguing that diffusion models underfit
the empirical score due to training-time inductive biases. In this work, we
refine this perspective, introducing the notion of selective underfitting:
instead of underfitting the score everywhere, better diffusion models more
accurately approximate the score in certain regions of input space, while
underfitting it in others. We characterize these regions and design empirical
interventions to validate our perspective. Our results establish that selective
underfitting is essential for understanding diffusion models, yielding new,
testable insights into their generalization and generative performance.

</details>


### [202] [Fine-Tuning Masked Diffusion for Provable Self-Correction](https://arxiv.org/abs/2510.01384)
*Jaeyeon Kim,Seunggeun Kim,Taekyun Lee,David Z. Pan,Hyeji Kim,Sham Kakade,Sitan Chen*

Main category: cs.LG

TL;DR: 为解决MDM自校正能力不足的问题，本文提出PRISM，一种轻量级、模型无关的方法，通过在推理时学习token质量分数实现自校正，并在多种任务上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 生成模型（特别是MDM）在推理时缺乏有效的自校正能力，现有方法要么改动大（需重构架构/训练），要么依赖不精确的token质量代理，限制了其适用性。

Method: 提出PRISM (Plug-in Remasking for Inference-time Self-correction of Masked Diffusions)，一种即插即用的重掩码方法，可应用于任何预训练MDM。PRISM通过定义自校正损失，无需强化学习或验证器，即可在MDM的同一前向传播中学习并计算每个token的质量得分，用于检测和修正低质量token。

Result: PRISM在数独、无条件文本生成（170M模型）和代码生成（LLaDA, 8B模型）等多个领域和规模的任务中，显著提升了MDM的推理表现。

Conclusion: PRISM为MDM提供了一种通用、高效的推理时自校正机制，通过可证明的token质量评估，显著提高了生成质量和模型适用性。

Abstract: A natural desideratum for generative models is self-correction--detecting and
revising low-quality tokens at inference. While Masked Diffusion Models (MDMs)
have emerged as a promising approach for generative modeling in discrete
spaces, their capacity for self-correction remains poorly understood. Prior
attempts to incorporate self-correction into MDMs either require overhauling
MDM architectures/training or rely on imprecise proxies for token quality,
limiting their applicability. Motivated by this, we introduce PRISM--Plug-in
Remasking for Inference-time Self-correction of Masked Diffusions--a
lightweight, model-agnostic approach that applies to any pretrained MDM.
Theoretically, PRISM defines a self-correction loss that provably learns
per-token quality scores, without RL or a verifier. These quality scores are
computed in the same forward pass with MDM and used to detect low-quality
tokens. Empirically, PRISM advances MDM inference across domains and scales:
Sudoku; unconditional text (170M); and code with LLaDA (8B).

</details>


### [203] [Optimal Stopping vs Best-of-$N$ for Inference Time Optimization](https://arxiv.org/abs/2510.01394)
*Yusuf Kalayci,Vinod Raman,Shaddin Dughmi*

Main category: cs.LG

TL;DR: 该研究提出一种基于潘多拉盒子问题的LLM推理时间优化框架，通过自适应策略在保持输出质量的同时，显著减少了15-35%的生成次数，实现了效率提升。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLM）生成过程中，尤其当需要多次生成时，如何在输出质量和推理成本之间取得平衡是一个关键问题。需要一种方法来智能地决定何时停止生成以优化效率。

Method: 将每次LLM生成视为打开一个带有随机奖励的“潘多拉盒子”，开发了一种UCB风格的潘多拉盒子算法，用于在不知道底层奖励分布的情况下决定何时停止。该方法进一步通过基于Bradley-Terry的转换，适应了LLM在不同提示下奖励缩放的问题，从而形成一种自适应的推理时间优化方法，能够实时标准化奖励并学习停止阈值。

Result: 在AlpacaFarm和HH-RLHF数据集上，使用多个LLM-奖励模型对进行的实验表明，所提出的自适应策略在获得与非自适应Best-of-N采样相同性能的同时，平均减少了15-35%的生成次数。

Conclusion: 该研究成功地在最优停止理论和LLM推理时间扩展之间搭建了原理性的桥梁，为LLM部署提供了理论性能边界和实际效率提升。

Abstract: Large language model (LLM) generation often requires balancing output quality
against inference cost, especially when using multiple generations. We
introduce a new framework for inference-time optimization based on the
classical Pandora's Box problem. Viewing each generation as opening a costly
"box" with random reward, we develop algorithms that decide when to stop
generating without knowing the underlying reward distribution. Our first
contribution is a UCB-style Pandora's Box algorithm, which achieves performance
that is provably close to Weitzman's algorithm, the optimal strategy when the
distribution is known. We further adapt this method to practical LLM settings
by addressing reward scaling across prompts via a Bradley-Terry inspired
transformation. This leads to an adaptive inference-time optimization method
that normalizes rewards and learns stopping thresholds on the fly. Experiments
on the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs,
show that our adaptive strategy can obtain the same performance as non-adaptive
Best-of-N sampling while requiring 15-35 percent fewer generations on average.
Our results establish a principled bridge between optimal stopping theory and
inference-time scaling, providing both theoretical performance bounds and
practical efficiency gains for LLM deployment.

</details>


### [204] [Neural Network Surrogates for Free Energy Computation of Complex Chemical Systems](https://arxiv.org/abs/2510.01396)
*Wasut Pornpatcharapong*

Main category: cs.LG

TL;DR: 本文提出了一个神经网络代理框架，通过自动微分生成雅可比矩阵，解决了复杂集体变量在自由能重建中的瓶颈，并实现了高精度。


<details>
  <summary>Details</summary>
Motivation: 自由能重建方法（如高斯过程回归GPR）需要集体变量（CVs）的雅可比矩阵，这限制了复杂或机器学习CVs的应用，成为一个瓶颈。

Method: 引入了一个神经网络代理框架，该框架直接从笛卡尔坐标学习CVs，并利用自动微分提供雅可比矩阵，从而绕过了分析形式。

Result: 在MgCl2离子对系统中，该方法对简单距离CV和复杂配位数CV都达到了高精度。雅可比矩阵误差也呈近似高斯分布，适用于GPR流程。

Conclusion: 该框架使基于梯度的自由能方法能够整合复杂和机器学习CVs，从而扩大了生物化学和材料模拟的范围。

Abstract: Free energy reconstruction methods such as Gaussian Process Regression (GPR)
require Jacobians of the collective variables (CVs), a bottleneck that
restricts the use of complex or machine-learned CVs. We introduce a neural
network surrogate framework that learns CVs directly from Cartesian coordinates
and uses automatic differentiation to provide Jacobians, bypassing analytical
forms. On an MgCl2 ion-pairing system, our method achieved high accuracy for
both a simple distance CV and a complex coordination-number CV. Moreover,
Jacobian errors also followed a near-Gaussian distribution, making them
suitable for GPR pipelines. This framework enables gradient-based free energy
methods to incorporate complex and machine-learned CVs, broadening the scope of
biochemistry and materials simulations.

</details>


### [205] [Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction](https://arxiv.org/abs/2510.01407)
*Ethan G. Rogers,Cheng Wang*

Main category: cs.LG

TL;DR: 为解决神经网络图像压缩中解码器计算瓶颈问题，本文提出了一种结合低秩表示和矢量量化的自编码器框架，显著降低了解码计算开销，同时保持了高图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络压缩方法的解码器计算成本高、复杂度大，阻碍了其广泛应用，形成了“解码器瓶颈”。

Method: 开发了一个新的压缩-重建框架，该框架将低秩表示融入带有矢量量化的自编码器中。通过对学习到的图像潜在表示执行一系列计算高效的低秩操作来进行数据重建。

Result: 实验表明，该方法能够高效且高质量地重建数据。

Conclusion: 该方法显著降低了神经网络压缩/重建中解码阶段的计算开销，基本消除了解码器计算瓶颈，同时保持了图像输出的高保真度。

Abstract: Image compression and reconstruction are crucial for various digital
applications. While contemporary neural compression methods achieve impressive
compression rates, the adoption of such technology has been largely hindered by
the complexity and large computational costs of the convolution-based decoders
during data reconstruction. To address the decoder bottleneck in neural
compression, we develop a new compression-reconstruction framework based on
incorporating low-rank representation in an autoencoder with vector
quantization. We demonstrated that performing a series of computationally
efficient low-rank operations on the learned latent representation of images
can efficiently reconstruct the data with high quality. Our approach
dramatically reduces the computational overhead in the decoding phase of neural
compression/reconstruction, essentially eliminating the decoder compute
bottleneck while maintaining high fidelity of image outputs.

</details>


### [206] [Edge Artificial Intelligence: A Systematic Review of Evolution, Taxonomic Frameworks, and Future Horizons](https://arxiv.org/abs/2510.01439)
*Mohamad Abou Ali,Fadi Dornaika*

Main category: cs.LG

TL;DR: 本文对边缘AI的演进、现状和未来方向进行了系统性综述，涵盖其技术、挑战和新兴机遇，并为研究人员和实践者提供全面框架。


<details>
  <summary>Details</summary>
Motivation: 系统性地审查边缘AI的演进、当前状况和未来发展方向，以提供一个全面的理解框架。

Method: 采用多维度分类法（包括部署位置、处理能力如TinyML和联邦学习、应用领域、硬件类型）进行系统性审查，并遵循PRISMA指南。分析追溯了从早期内容分发网络和雾计算到现代设备端智能的发展，并探讨了核心使能技术。

Result: 分析了专业硬件加速器、优化软件和通信协议等核心使能技术。批判性评估了资源限制、安全、模型管理、功耗和连接性等挑战。强调了神经形态硬件、持续学习算法、边缘-云协作和可信度集成等新兴机遇。

Conclusion: 提供了一个全面的框架，以指导边缘AI领域的研究人员和实践者。

Abstract: Edge Artificial Intelligence (Edge AI) embeds intelligence directly into
devices at the network edge, enabling real-time processing with improved
privacy and reduced latency by processing data close to its source. This review
systematically examines the evolution, current landscape, and future directions
of Edge AI through a multi-dimensional taxonomy including deployment location,
processing capabilities such as TinyML and federated learning, application
domains, and hardware types. Following PRISMA guidelines, the analysis traces
the field from early content delivery networks and fog computing to modern
on-device intelligence. Core enabling technologies such as specialized hardware
accelerators, optimized software, and communication protocols are explored.
Challenges including resource limitations, security, model management, power
consumption, and connectivity are critically assessed. Emerging opportunities
in neuromorphic hardware, continual learning algorithms, edge-cloud
collaboration, and trustworthiness integration are highlighted, providing a
comprehensive framework for researchers and practitioners.

</details>


### [207] [SoftAdaClip: A Smooth Clipping Strategy for Fair and Private Model Training](https://arxiv.org/abs/2510.01447)
*Dorsa Soleymani,Ali Dadsetan,Frank Rudzicz*

Main category: cs.LG

TL;DR: 为解决差分隐私(DP)训练中梯度裁剪导致的模型性能和公平性下降，尤其是对少数群体的负面影响，本文提出了SoftAdaClip方法。该方法用平滑的tanh变换替代硬裁剪，有效减少了子群体之间的不平等，并在多个数据集上取得了显著优于传统DP-SGD和Adaptive-DPSGD的公平性提升。


<details>
  <summary>Details</summary>
Motivation: 差分隐私(DP)虽然提供了强大的数据保护，但通常会降低模型性能和公平性，尤其对代表性不足的群体影响更大。主要原因是DP-SGD中的梯度裁剪会不成比例地抑制少数子群体的学习信号。尽管自适应裁剪可以提高效用，但仍依赖统一的硬裁剪，这可能限制公平性，因此需要一种新的方法来解决这一公平性问题。

Method: 本文提出SoftAdaClip，一种差分隐私训练方法。它用基于tanh的平滑变换替代了传统的硬裁剪，旨在在限制敏感度的同时保留相对梯度幅度。该方法在MIMIC-III (临床文本)、GOSSIS-eICU (结构化医疗) 和Adult Income (表格数据) 等多种数据集上进行了评估。

Result: SoftAdaClip显著减少了子群体差异。与DP-SGD相比，子群体差异减少高达87%；与Adaptive-DPSGD相比，子群体差异减少高达48%。这些减少在统计上具有显著性。

Conclusion: 研究结果强调了将平滑变换与自适应机制相结合的重要性，以实现公平且保护隐私的模型训练。

Abstract: Differential privacy (DP) provides strong protection for sensitive data, but
often reduces model performance and fairness, especially for underrepresented
groups. One major reason is gradient clipping in DP-SGD, which can
disproportionately suppress learning signals for minority subpopulations.
Although adaptive clipping can enhance utility, it still relies on uniform hard
clipping, which may restrict fairness. To address this, we introduce
SoftAdaClip, a differentially private training method that replaces hard
clipping with a smooth, tanh-based transformation to preserve relative gradient
magnitudes while bounding sensitivity. We evaluate SoftAdaClip on various
datasets, including MIMIC-III (clinical text), GOSSIS-eICU (structured
healthcare), and Adult Income (tabular data). Our results show that SoftAdaClip
reduces subgroup disparities by up to 87% compared to DP-SGD and up to 48%
compared to Adaptive-DPSGD, and these reductions in subgroup disparities are
statistically significant. These findings underscore the importance of
integrating smooth transformations with adaptive mechanisms to achieve fair and
private model training.

</details>


### [208] [Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression](https://arxiv.org/abs/2510.01450)
*Yifei Zuo,Yutong Yin,Zhichen Zeng,Ang Li,Banghua Zhu,Zhaoran Wang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Transformer architectures have achieved remarkable success in various
domains. While efficient alternatives to Softmax Attention have been widely
studied, the search for more expressive mechanisms grounded in theoretical
insight-even at greater computational cost-has been relatively underexplored.
In this work, we bridge this gap by proposing Local Linear Attention (LLA), a
novel attention mechanism derived from nonparametric statistics through the
lens of test-time regression. First, we show that LLA offers theoretical
advantages over Linear and Softmax Attention for associative memory via a
bias-variance trade-off analysis. Next, we address its computational challenges
and propose two memory-efficient primitives to tackle the $\Theta(n^2 d)$ and
$\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient,
blockwise algorithm that enables scalable and parallel computation on modern
accelerators. In addition, we implement and profile a customized inference
kernel that significantly reduces memory overheads. Finally, we empirically
validate the advantages and limitations of LLA on test-time regression,
in-context regression, associative recall and state tracking tasks. Experiment
results demonstrate that LLA effectively adapts to non-stationarity,
outperforming strong baselines in test-time training and in-context learning,
and exhibiting promising evidence for its scalability and applicability in
large-scale models. Code is available at
https://github.com/Yifei-Zuo/Flash-LLA.

</details>


### [209] [SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion](https://arxiv.org/abs/2510.01456)
*Brett Barkley,Preston Culbertson,David Fridovich-Keil*

Main category: cs.LG

TL;DR: SCOPED是一种针对扩散模型的快速、通用OOD检测方法，显著减少了计算开销，并在视觉和机器人控制任务中取得了有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 在机器学习系统的可靠部署中，域外(OOD)检测至关重要，因此需要一种针对扩散模型的快速、通用OOD检测方法。

Method: 该方法名为SCOPED，通过对单个扩散模型进行一次训练，结合了模型分数函数的雅可比迹和平方范数，形成一个单一的测试统计量。它利用核密度估计来估计SCOPED分数的内部分布密度，实现了灵活的无监督测试，通常仅需一次前向传播和一个雅可比向量积（通过Hutchinson迹估计器提高效率）。

Result: SCOPED与现有方法相比，将模型前向传播次数减少了一个数量级，性能优于大多数基于扩散的基线方法，并接近最强方法的准确性。在四个视觉基准测试中，尽管计算成本较低，SCOPED仍获得了具有竞争力或最先进的精确召回分数。该方法还泛化到机器人控制任务，识别出分布变化。

Conclusion: SCOPED为在实际领域中实现快速、可靠的OOD检测奠定了实用基础，可应用于视觉感知伪影、自回归模型中的异常检测、强化学习中的探索以及无监督训练的数据集管理。

Abstract: Out-of-distribution (OOD) detection is essential for reliable deployment of
machine learning systems in vision, robotics, reinforcement learning, and
beyond. We introduce Score-Curvature Out-of-distribution Proximity Evaluator
for Diffusion (SCOPED), a fast and general-purpose OOD detection method for
diffusion models that reduces the number of forward passes on the trained model
by an order of magnitude compared to prior methods, outperforming most
diffusion-based baselines and closely approaching the accuracy of the strongest
ones. SCOPED is computed from a single diffusion model trained once on a
diverse dataset, and combines the Jacobian trace and squared norm of the
model's score function into a single test statistic. Rather than thresholding
on a fixed value, we estimate the in-distribution density of SCOPED scores
using kernel density estimation, enabling a flexible, unsupervised test that,
in the simplest case, only requires a single forward pass and one
Jacobian-vector product (JVP), made efficient by Hutchinson's trace estimator.
On four vision benchmarks, SCOPED achieves competitive or state-of-the-art
precision-recall scores despite its low computational cost. The same method
generalizes to robotic control tasks with shared state and action spaces,
identifying distribution shifts across reward functions and training regimes.
These results position SCOPED as a practical foundation for fast and reliable
OOD detection in real-world domains, including perceptual artifacts in vision,
outlier detection in autoregressive models, exploration in reinforcement
learning, and dataset curation for unsupervised training.

</details>


### [210] [Fixing That Free Lunch: When, Where, and Why Synthetic Data Fails in Model-Based Policy Optimization](https://arxiv.org/abs/2510.01457)
*Brett Barkley,David Fridovich-Keil*

Main category: cs.LG

TL;DR: 合成数据在模型强化学习中可能导致性能下降。本研究发现Model-Based Policy Optimization (MBPO)在DeepMind Control Suite (DMC)中表现不佳的原因是动态与奖励模型尺度不匹配以及目标表示选择不当。解决这些问题使MBPO在DMC中显著超越SAC。


<details>
  <summary>Details</summary>
Motivation: 合成数据是数据高效Dyna风格模型强化学习的核心，但也可能降低性能。研究旨在探究合成数据何时有益、何时失败以及为何失败，特别是MBPO在OpenAI Gym表现良好但在DeepMind Control Suite (DMC)中却常不如模型无关的Soft Actor-Critic (SAC)，以期通过解决这些失败模式来提升策略性能。

Method: 本研究专注于Model-Based Policy Optimization (MBPO)算法。通过对比MBPO在OpenAI Gym和DeepMind Control Suite (DMC)上的表现，识别出导致其在DMC中失效的两个主要耦合问题：一是动态模型与奖励模型之间存在尺度不匹配，导致评论家低估并阻碍模型-策略协同进化中的策略改进；二是目标表示选择不当，导致模型方差膨胀并产生易出错的rollout。

Result: 通过解决所识别的失败模式，MBPO在之前无法实现策略改进的情况下实现了显著提升，使其在七个DMC任务中的五个任务中表现优于SAC，同时保持了在OpenAI Gym中报告的强大性能。

Conclusion: 研究不仅揭示了环境特定假设如何隐式编码到算法设计中，还强调了基准选择对算法泛化条件的影响。本研究希望激励社区开发MDP任务和环境结构与算法失败模式相关的分类法，追求统一解决方案，并明确基准选择如何最终塑造算法泛化的条件。

Abstract: Synthetic data is a core component of data-efficient Dyna-style model-based
reinforcement learning, yet it can also degrade performance. We study when it
helps, where it fails, and why, and we show that addressing the resulting
failure modes enables policy improvement that was previously unattainable. We
focus on Model-Based Policy Optimization (MBPO), which performs actor and
critic updates using synthetic action counterfactuals. Despite reports of
strong and generalizable sample-efficiency gains in OpenAI Gym, recent work
shows that MBPO often underperforms its model-free counterpart, Soft
Actor-Critic (SAC), in the DeepMind Control Suite (DMC). Although both suites
involve continuous control with proprioceptive robots, this shift leads to
sharp performance losses across seven challenging DMC tasks, with MBPO failing
in cases where claims of generalization from Gym would imply success. This
reveals how environment-specific assumptions can become implicitly encoded into
algorithm design when evaluation is limited. We identify two coupled issues
behind these failures: scale mismatches between dynamics and reward models that
induce critic underestimation and hinder policy improvement during model-policy
coevolution, and a poor choice of target representation that inflates model
variance and produces error-prone rollouts. Addressing these failure modes
enables policy improvement where none was previously possible, allowing MBPO to
outperform SAC in five of seven tasks while preserving the strong performance
previously reported in OpenAI Gym. Rather than aiming only for incremental
average gains, we hope our findings motivate the community to develop
taxonomies that tie MDP task- and environment-level structure to algorithmic
failure modes, pursue unified solutions where possible, and clarify how
benchmark choices ultimately shape the conditions under which algorithms
generalize.

</details>


### [211] [How Well Can Preference Optimization Generalize Under Noisy Feedback?](https://arxiv.org/abs/2510.01458)
*Shawn Im,Yixuan Li*

Main category: cs.LG

TL;DR: 本文研究了噪声反馈对大语言模型（LLM）偏好优化的影响，提供了在有噪声条件下有限步优化的泛化保证，并实证验证了其对多种优化损失的适用性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM偏好优化方法普遍假定无噪声反馈，但人类判断固有的错误和不一致性使得这一假设不切实际，需要研究噪声反馈的影响。

Method: 研究考虑了错误标注和不确定性等真实世界噪声模型，分析了噪声反馈对有限步偏好优化的泛化能力影响，并提供了泛化保证。该分析适用于DPO、IPO、SLiC等多种偏好优化损失函数。

Result: 论文揭示了不同类型的噪声及其噪声率如何根据偏好数据分布和样本数量影响模型的泛化能力。通过对现代LLM的实证验证，证实了理论发现的实际意义。

Conclusion: 该研究为在存在人类反馈噪声的情况下，开发与人类偏好对齐的AI系统提供了宝贵的实践指导和理论见解。

Abstract: As large language models (LLMs) advance their capabilities, aligning these
models with human preferences has become crucial. Preference optimization,
which trains models to distinguish between preferred and non-preferred
responses based on human feedback, has become a crucial component for aligning
LLMs. However, most existing works assume noise-free feedback, which is
unrealistic due to the inherent errors and inconsistencies in human judgments.
This paper addresses the impact of noisy feedback on preference optimization,
providing generalization guarantees under these conditions. In particular, we
consider noise models that correspond to common real-world sources of noise,
such as mislabeling and uncertainty. Unlike traditional analyses that assume
convergence, our work focuses on finite-step preference optimization, offering
new insights that are more aligned with practical LLM training. We describe how
generalization decays with different types of noise across levels of noise
rates based on the preference data distribution and number of samples. Our
analysis for noisy preference learning applies to a broad family of preference
optimization losses such as DPO, IPO, SLiC, etc. Empirical validation on
contemporary LLMs confirms the practical relevance of our findings, offering
valuable insights for developing AI systems that align with human preferences.

</details>


### [212] [LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning](https://arxiv.org/abs/2510.01459)
*Weizhe Chen,Sven Koenig,Bistra Dilkina*

Main category: cs.LG

TL;DR: 本文提出LSPO，一个基于平均响应长度动态选择训练数据的元-RLVR算法，能有效提升LLM在推理任务上的学习效果。


<details>
  <summary>Details</summary>
Motivation: RLVR是LLM推理任务训练的核心方法，但现有研究主要关注修改损失函数。本文受LLM“过度思考”研究的启发，旨在通过数据采样优化RLVR效率和效果。

Method: 提出Length-aware Sampling for Policy Optimization (LSPO)算法，这是一种新型元-RLVR算法，它在训练的每一步根据平均响应长度动态选择训练数据。

Result: LSPO在多个基础模型和数据集上均能持续提升学习效果。此外，详细的消融研究探索了将长度信号整合到动态采样的替代方法，提供了进一步的见解。

Conclusion: LSPO通过动态数据采样有效提升了LLM在推理任务上的学习效率和效果，并为未来研究利用长度信号进行动态采样指明了有前景的方向。

Abstract: Since the release of Deepseek-R1, reinforcement learning with verifiable
rewards (RLVR) has become a central approach for training large language models
(LLMs) on reasoning tasks. Recent work has largely focused on modifying loss
functions to make RLVR more efficient and effective. In this paper, motivated
by studies of overthinking in LLMs, we propose Length-aware Sampling for Policy
Optimization (LSPO), a novel meta-RLVR algorithm that dynamically selects
training data at each step based on the average response length. We evaluate
LSPO across multiple base models and datasets, demonstrating that it
consistently improves learning effectiveness. In addition, we conduct a
detailed ablation study to examine alternative ways of incorporating length
signals into dynamic sampling, offering further insights and highlighting
promising directions for future research.

</details>


### [213] [The Three Regimes of Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2510.01460)
*Lu Li,Tianwei Ni,Yihao Sun,Pierre-Luc Bacon*

Main category: cs.LG

TL;DR: 离线到在线强化学习的微调策略不一致，本文提出了一个稳定性-可塑性原则来解释并指导其设计。


<details>
  <summary>Details</summary>
Motivation: 离线到在线强化学习中，在线微调的设计选择在不同场景下表现极不一致。

Method: 提出了“稳定性-可塑性原则”，即在在线微调时，应保留预训练策略或离线数据集中较优的知识，同时保持足够的可塑性，并识别出三种微调机制。

Result: 通过大规模实证研究验证了该框架，在63个案例中的45个中结果与预测高度一致。

Conclusion: 本工作提供了一个指导离线到在线强化学习设计选择的原则性框架，基于离线数据集和预训练策略的相对性能。

Abstract: Offline-to-online reinforcement learning (RL) has emerged as a practical
paradigm that leverages offline datasets for pretraining and online
interactions for fine-tuning. However, its empirical behavior is highly
inconsistent: design choices of online-fine tuning that work well in one
setting can fail completely in another. We propose a stability--plasticity
principle that can explain this inconsistency: we should preserve the knowledge
of pretrained policy or offline dataset during online fine-tuning, whichever is
better, while maintaining sufficient plasticity. This perspective identifies
three regimes of online fine-tuning, each requiring distinct stability
properties. We validate this framework through a large-scale empirical study,
finding that the results strongly align with its predictions in 45 of 63 cases.
This work provides a principled framework for guiding design choices in
offline-to-online RL based on the relative performance of the offline dataset
and the pretrained policy.

</details>


### [214] [Fine-tuning LLMs with variational Bayesian last layer for high-dimensional Bayesian optimzation](https://arxiv.org/abs/2510.01471)
*Haotian Xiang,Jinwen Xu,Qin Lu*

Main category: cs.LG

TL;DR: 针对高维不规则变量的黑盒优化问题，本文提出将LoRA微调的大语言模型（LLM）作为概率代理模型，并结合变分贝叶斯后验线形回归头（LoRA-VBLL）。为自动化超参数选择，进一步提出了加权集成方法（ENS-LoRA-VBLL），在各种高维任务上表现出色，且计算高效。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化（BO）在低维连续变量黑盒优化中表现良好，但传统的GP代理模型难以应对高维且包含不规则变量（如分类、序数）的优化问题，现有方法存在计算成本高或性能不足的问题。

Method: 本文采用LoRA微调的大语言模型（LLM）作为代理模型，通过变分贝叶斯最后一层（VBLL）框架优化线性回归头，构建了计算轻量且支持递归更新的LoRA-VBLL模型。为自动化LoRA秩及其他超参数的选择，并实现持续更新，进一步设计了加权集成（ENS）的LoRA-VBLL代理模型。

Result: 广泛的实验结果表明，所提出的(ENS-)LoRA-VBLL方法在各种高维基准测试和真实世界的分子优化任务中均展现出令人信服的性能。

Conclusion: 本研究成功地将LLM应用于高维不规则变量的贝叶斯优化，通过LoRA-VBLL及其集成策略，提供了一种计算高效、性能卓越且能处理复杂输入空间的解决方案。

Abstract: A plethora of applications entail solving black-box optimization problems
with high evaluation costs, including drug discovery, material design, as well
as hyperparameter tuning. Toward finding the global optimum of such black-box
optimization problems with sample efficiency, Bayesian optimization (BO) is a
theoretically elegant framework that relies on a probabilistic surrogate model
so as to iteratively select the query point with well-balanced
exploration-exploitation tradeoffs. The Gaussian process (GP), as the de-facto
choice for surrogate modeling, has achieved compelling performances for vanilla
BO with low-dimensional continuous variables. However, GPs fall short in coping
with high-dimensional counterparts with {\it irregular} variables (e.g.,
categorical, ordinal, etc.). To alleviate this, neural network-based surrogates
have been explored. Inspired by the powerful capabilities of LLMs, we adopt the
LLM as the surrogate to model the mapping from the high-dimensional input
variables to the objective function. To adapt to the current problem, we
leverage the low-rank adaptation (LoRA) to fine-tune the LLM parameters
together with the posterior of a linear regression head via the variational
Bayesian last layer (VBLL) framework. The resulting LoRA-VBLL is not only
computationally light compared to existing alternatives, but also admits
recursive updates. To automate the critical selection of the LoRA rank as well
as other hyperparameters, a weighted ensemble (ENS) of LoRA-VBLL surrogates has
been devised, which further accommodates continual update of the per-model
weight and individual LoRA-VBLL parameters via recursive Bayes. Extensive
experimental results demonstrate the compelling performance of the proposed
(ENS-)LoRA-VBLL approaches on various high-dimensional benchmarks and the
real-world molecular optimization tasks.

</details>


### [215] [PEL-NAS: Search Space Partitioned Architecture Prompt Co-Evolutionary LLM-driven Hardware-Aware Neural Architecture Search](https://arxiv.org/abs/2510.01472)
*Hengyi Zhu,Grace Li Zhang,Shaoyi Huang*

Main category: cs.LG

TL;DR: PEL-NAS是一种新的LLM驱动的硬件感知神经架构搜索方法，通过搜索空间划分、架构提示共进化和零成本预测器，解决了LLM探索偏差问题，显著提高了准确性和降低了延迟，并大幅缩减了搜索成本。


<details>
  <summary>Details</summary>
Motivation: 传统的超网络（supernet）方法在硬件感知神经架构搜索（HW-NAS）中耗时（多天），而LLM驱动的方法虽然快速，但存在探索偏差，未能充分探索整个搜索空间并发现不同延迟范围的架构。

Method: 本文提出PEL-NAS，包含三个关键组件：1) 复杂度驱动的划分引擎，将搜索空间按复杂度划分，以增强多样性并减少探索偏差；2) LLM驱动的架构提示共进化操作，LLM基于前期结果更新设计启发知识库，并进行引导进化；3) 零成本预测器，避免从头训练大量候选架构。

Result: 在HW-NAS-Bench上的实验结果表明，PEL-NAS实现了更高的HV、更低的IGD，在相似准确度下延迟降低高达54%。与传统超网络基线相比，搜索成本从数天降至数分钟。

Conclusion: PEL-NAS有效地解决了LLM驱动HW-NAS中的探索偏差问题，在实现卓越的准确性-延迟权衡的同时，显著降低了搜索成本，使其成为一种高效且高性能的HW-NAS解决方案。

Abstract: Hardware-Aware Neural Architecture Search (HW-NAS) requires joint
optimization of accuracy and latency under device constraints. Traditional
supernet-based methods require multiple GPU days per dataset. Large Language
Model (LLM)-driven approaches avoid training a large supernet and can provide
quick feedback, but we observe an exploration bias: the LLM repeatedly proposes
neural network designs within limited search space and fails to discover
architectures across different latency ranges in the entire search space. To
address this issue, we propose PEL-NAS: a search space Partitioned,
architecture prompt co-Evolutionary and LLM-driven Neural Architecture Search
that can generate neural networks with high accuracy and low latency with
reduced search cost. Our proposed PEL-NAS has three key components: 1) a
complexity-driven partitioning engine that divides the search space by
complexity to enforce diversity and mitigate exploration bias; 2) an
LLM-powered architecture prompt co-evolution operator, in which the LLM first
updates a knowledge base of design heuristics based on results from the
previous round, then performs a guided evolution algorithm on architectures
with prompts that incorporate this knowledge base. Prompts and designs improve
together across rounds which avoids random guesswork and improve efficiency; 3)
a zero-cost predictor to avoid training a large number of candidates from
scratch. Experimental results show that on HW-NAS-Bench, PEL-NAS can achieve
overall higher HV, lower IGD, and up to 54% lower latency than baselines at
similar accuracy. Meanwhile, the search cost drops from days to minutes
compared with traditional supernet baselines.

</details>


### [216] [Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from Corrupted Datasets](https://arxiv.org/abs/2510.01479)
*Shriram Karpoora Sundara Pandian,Ali Baheri*

Main category: cs.LG

TL;DR: 本文提出加权行为克隆（Weighted BC），一种鲁棒的模仿学习方法，通过密度比加权处理受污染的离线数据集，在高污染率下也能实现近乎最优的性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习（RL）数据集常受对抗性投毒、系统错误或低质量样本污染，导致标准行为克隆（BC）和离线RL方法的策略性能下降，尤其在安全关键应用中不可接受。

Method: 引入密度比加权行为克隆（Weighted BC）。该方法利用一小部分经过验证的干净参考集，通过二元判别器估计轨迹级的密度比。这些密度比经过裁剪后作为BC目标函数的权重，以优先学习干净的专家行为，并降低或丢弃受损数据的影响，且无需了解污染机制。理论上保证了收敛到干净专家策略，且有限样本界与污染率无关。

Result: 在包含多种投毒协议（奖励、状态、转换和动作）的连续控制基准测试中，Weighted BC即使在高污染率下也能保持接近最优的性能，优于传统BC、批量受限Q学习（BCQ）和行为正则化Actor-Critic（BRAC）等基线方法。

Conclusion: Weighted BC为离线强化学习提供了一种鲁棒的解决方案，能够有效应对数据污染问题，并在无需了解污染机制的情况下学习到高质量的专家策略，其性能不受污染率影响。

Abstract: Offline reinforcement learning (RL) enables policy optimization from fixed
datasets, making it suitable for safety-critical applications where online
exploration is infeasible. However, these datasets are often contaminated by
adversarial poisoning, system errors, or low-quality samples, leading to
degraded policy performance in standard behavioral cloning (BC) and offline RL
methods. This paper introduces Density-Ratio Weighted Behavioral Cloning
(Weighted BC), a robust imitation learning approach that uses a small, verified
clean reference set to estimate trajectory-level density ratios via a binary
discriminator. These ratios are clipped and used as weights in the BC objective
to prioritize clean expert behavior while down-weighting or discarding
corrupted data, without requiring knowledge of the contamination mechanism. We
establish theoretical guarantees showing convergence to the clean expert policy
with finite-sample bounds that are independent of the contamination rate. A
comprehensive evaluation framework is established, which incorporates various
poisoning protocols (reward, state, transition, and action) on continuous
control benchmarks. Experiments demonstrate that Weighted BC maintains
near-optimal performance even at high contamination ratios outperforming
baselines such as traditional BC, batch-constrained Q-learning (BCQ) and
behavior regularized actor-critic (BRAC).

</details>


### [217] [Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed](https://arxiv.org/abs/2510.01494)
*Isha Gupta,Rylan Schaeffer,Joshua Kazdan,Ken Liu,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 对抗性攻击的可迁移性取决于其作用域：数据空间攻击可迁移，而模型表示空间攻击则不能，除非模型表示的几何结构对齐。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明，图像分类器的对抗样本和语言模型的文本越狱攻击可以成功迁移。然而，最近研究发现视觉-语言模型（VLM）的图像越狱攻击无法成功迁移，这种差异亟需解释。

Method: 提出一个基本区别：输入数据空间的攻击可迁移，而模型表示空间的攻击不可迁移（除非表示几何结构对齐）。通过理论证明和在四种不同设置下的经验证据进行验证，包括图像分类器、语言模型和视觉-语言模型上的数据空间及表示空间攻击。

Result: 理论上证明了数据空间与表示空间攻击可迁移性的区别。实证发现，针对图像分类器和语言模型的表示空间攻击成功攻击目标模型但无法迁移；而针对VLM的数据空间攻击可成功迁移。此外，当VLM的潜在几何结构在后投影空间充分对齐时，表示空间攻击也可迁移。

Conclusion: 对抗性迁移并非所有攻击的固有属性，而是取决于其操作域——共享的数据空间或模型独特的表示空间。这一关键见解对于构建更鲁棒的机器学习模型具有重要意义。

Abstract: The field of adversarial robustness has long established that adversarial
examples can successfully transfer between image classifiers and that text
jailbreaks can successfully transfer between language models (LMs). However, a
pair of recent studies reported being unable to successfully transfer image
jailbreaks between vision-language models (VLMs). To explain this striking
difference, we propose a fundamental distinction regarding the transferability
of attacks against machine learning models: attacks in the input data-space can
transfer, whereas attacks in model representation space do not, at least not
without geometric alignment of representations. We then provide theoretical and
empirical evidence of this hypothesis in four different settings. First, we
mathematically prove this distinction in a simple setting where two networks
compute the same input-output map but via different representations. Second, we
construct representation-space attacks against image classifiers that are as
successful as well-known data-space attacks, but fail to transfer. Third, we
construct representation-space attacks against LMs that successfully jailbreak
the attacked models but again fail to transfer. Fourth, we construct data-space
attacks against VLMs that successfully transfer to new VLMs, and we show that
representation space attacks \emph{can} transfer when VLMs' latent geometries
are sufficiently aligned in post-projector space. Our work reveals that
adversarial transfer is not an inherent property of all attacks but contingent
on their operational domain - the shared data-space versus models' unique
representation spaces - a critical insight for building more robust models.

</details>


### [218] [Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information](https://arxiv.org/abs/2510.01499)
*Rui Ai,Yuqi Pan,David Simchi-Levi,Milind Tambe,Haifeng Xu*

Main category: cs.LG

TL;DR: 本文提出两种新算法（OW和ISP），通过利用一阶和二阶信息，更有效地聚合多智能体LLM的答案，显著优于传统多数投票法，提升了集体决策的可靠性。


<details>
  <summary>Details</summary>
Motivation: 多智能体LLM推理中，如何有效聚合来自多个LLM的答案是一个基本挑战。标准多数投票法未能考虑模型间的潜在异质性和相关性，导致聚合效果不佳。

Method: 设计了两种新的聚合算法：最优权重（Optimal Weight, OW）和逆惊喜流行度（Inverse Surprising Popularity, ISP）。这些方法利用了一阶和二阶信息，并通过理论分析证明它们在温和假设下能有效缓解多数投票法的局限性。

Result: 理论分析表明，所提方法能有效缓解多数投票法的固有局限性。在合成数据集、流行LLM微调基准（如UltraFeedback和MMLU）以及真实医疗场景（ARMMAN）中进行了实证验证，结果显示，所提方法在所有情况下均持续优于多数投票法。

Conclusion: OW和ISP算法为多智能体LLM管道提供了更可靠的集体决策，并在实践中带来了性能提升和概念上的洞察，优于传统的多数投票法。

Abstract: With the rapid progress of multi-agent large language model (LLM) reasoning,
how to effectively aggregate answers from multiple LLMs has emerged as a
fundamental challenge. Standard majority voting treats all answers equally,
failing to consider latent heterogeneity and correlation across models. In this
work, we design two new aggregation algorithms called Optimal Weight (OW) and
Inverse Surprising Popularity (ISP), leveraging both first-order and
second-order information. Our theoretical analysis shows these methods provably
mitigate inherent limitations of majority voting under mild assumptions,
leading to more reliable collective decisions. We empirically validate our
algorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as
UltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all
cases, our methods consistently outperform majority voting, offering both
practical performance gains and conceptual insights for the design of robust
multi-agent LLM pipelines.

</details>


### [219] [Realistic CDSS Drug Dosing with End-to-end Recurrent Q-learning for Dual Vasopressor Control](https://arxiv.org/abs/2510.01508)
*Will Y. Zou,Jean Feng,Alexandre Kalimouttou,Jennifer Yuntong Zhang,Christopher W. Seymour,Romain Pirracchio*

Main category: cs.LG

TL;DR: 解决RL在ICU药物剂量决策中因操作性引发的质疑。提出一种结合保守Q学习与循环建模的方法，通过创新的动作空间设计，实现可解释、高效且符合临床的双血管加压药剂量策略，显著提升患者生存率。


<details>
  <summary>Details</summary>
Motivation: 针对临床医生对强化学习（RL）在临床决策支持系统（CDSS）中生成不可操作剂量决策的疑虑，特别是重症监护室（ICU）败血症休克患者双血管加压药的最佳剂量问题，旨在开发可信赖、可解释且易于临床采纳的RL剂量策略。

Method: 采用端到端的方法学习ICU患者双血管加压药的剂量和控制策略。核心方法包括：1) 设计创新的动作空间，以适应离散、连续和方向性的剂量策略，确保药物剂量的真实性。2) 结合离线保守Q学习（Conservative Q-learning）和新颖的回放缓冲区循环建模，以捕捉ICU时间序列数据中的时间依赖性。

Result: 1. 所设计的动作空间在保持疗效的同时，提高了去甲肾上腺素剂量策略的可解释性，并促进了临床采纳。2. 动作空间设计对学习到的行为策略有深刻影响。3. 在eICU和MIMIC数据集上，患者生存率提高了超过15%。4. 学习到的策略与既定的临床协议保持一致。

Conclusion: 本研究提出的结合先进动作空间设计与RL方法，能有效克服临床对RL剂量决策的质疑，显著提升患者生存率和决策的可解释性，并确保与临床实践的兼容性。

Abstract: Reinforcement learning (RL) applications in Clinical Decision Support Systems
(CDSS) frequently encounter skepticism from practitioners regarding inoperable
dosing decisions. We address this challenge with an end-to-end approach for
learning optimal drug dosing and control policies for dual vasopressor
administration in intensive care unit (ICU) patients with septic shock. For
realistic drug dosing, we apply action space design that accommodates discrete,
continuous, and directional dosing strategies in a system that combines offline
conservative Q-learning with a novel recurrent modeling in a replay buffer to
capture temporal dependencies in ICU time-series data. Our comparative analysis
of norepinephrine dosing strategies across different action space formulations
reveals that the designed action spaces improve interpretability and facilitate
clinical adoption while preserving efficacy. Empirical results1 on eICU and
MIMIC demonstrate that action space design profoundly influences learned
behavioral policies. The proposed methods achieve improved patient outcomes of
over 15% in survival improvement probability, while aligning with established
clinical protocols.

</details>


### [220] [Flock: A Knowledge Graph Foundation Model via Learning on Random Walks](https://arxiv.org/abs/2510.01510)
*Jinwoo Kim,Xingyue Huang,Krzysztof Olejniczak,Kyungbin Min,Michael Bronstein,Seunghoon Hong,İsmail İlkan Ceylan*

Main category: cs.LG

TL;DR: 本文提出Flock，一种基于概率节点-关系等变性的知识图谱基础模型，用于解决零样本链接预测中传统模型表达能力有限的问题，并在多个数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱基础模型（KGFMs）在零样本链接预测中，由于确定性等变性限制了表达能力，无法区分结构相似但语义不同的关系。

Method: 引入概率节点-关系等变性，通过分布保持等变性并引入随机化来打破推理时的对称性。Flock模型通过迭代采样随机游走，编码为序列，使用序列模型嵌入，并通过学习的池化聚合节点和关系表示。Flock尊重概率等变性，是同构不变链接级函数的通用近似器。

Result: Flock在新诊断数据集Petals上完美解决了传统KGFMs失败的问题，并在54个不同领域的知识图谱上的实体和关系预测任务中取得了最先进的性能。

Conclusion: 通过引入概率节点-关系等变性，Flock成功克服了传统KGFMs的限制，在零样本链接预测任务中展现出卓越的泛化能力和性能。

Abstract: We study the problem of zero-shot link prediction on knowledge graphs (KGs),
which requires models to generalize over novel entities and novel relations.
Knowledge graph foundation models (KGFMs) address this task by enforcing
equivariance over both nodes and relations, learning from structural properties
of nodes and relations, which are then transferable to novel graphs with
similar structural properties. However, the conventional notion of
deterministic equivariance imposes inherent limits on the expressive power of
KGFMs, preventing them from distinguishing structurally similar but
semantically distinct relations. To overcome this limitation, we introduce
probabilistic node-relation equivariance, which preserves equivariance in
distribution while incorporating a principled randomization to break symmetries
during inference. Building on this principle, we present Flock, a KGFM that
iteratively samples random walks, encodes them into sequences via a recording
protocol, embeds them with a sequence model, and aggregates representations of
nodes and relations via learned pooling. Crucially, Flock respects
probabilistic node-relation equivariance and is a universal approximator for
isomorphism-invariant link-level functions over KGs. Empirically, Flock
perfectly solves our new diagnostic dataset Petals where current KGFMs fail,
and achieves state-of-the-art performances on entity- and relation prediction
tasks on 54 KGs from diverse domains.

</details>


### [221] [Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties](https://arxiv.org/abs/2510.01520)
*Hossein Sholehrasa,Xuan Xu,Doina Caragea,Jim E. Riviere,Majid Jaberi-Douraki*

Main category: cs.LG

TL;DR: 该研究提出了一个预测框架，利用机器学习和大型语言模型分析美国FDA兽药不良事件报告，以准确且可解释地预测动物预后（死亡或康复），并识别高风险因素，从而增强食品安全和动物福利。


<details>
  <summary>Details</summary>
Motivation: 确保食用动物用药安全，保护动物福利和人类食品安全。不良事件（AEs）可能预示意想不到的药代动力学或毒代动力学效应，增加食物链中违规残留的风险，因此需要一个预测框架来识别高风险情况。

Method: 该研究使用美国FDA OpenFDA兽药中心约128万份不良事件报告（1987-2025 Q1）。数据经过预处理，包括合并表格、通过VeDDRA本体标准化AEs、数据归一化、缺失值插补、高基数特征降维，并整合了药物理化性质。评估了随机森林、CatBoost、XGBoost、ExcelFormer以及大型语言模型（Gemma、Phi）等监督模型。通过欠采样、过采样等方法解决了类别不平衡问题，并优先考虑致命结果的召回率。同时采用了集成方法（投票、堆叠）和基于平均不确定性边际（AUM）的伪标签技术，并通过SHAP进行可解释性分析。

Result: 集成方法（投票、堆叠）和CatBoost表现最佳，精度、召回率和F1分数均达到0.95。引入基于AUM的伪标签技术显著改善了少数类（死亡）的检测，尤其是在ExcelFormer和XGBoost中。SHAP可解释性分析揭示，肺部、心脏和支气管疾病、动物人口统计学特征以及药物理化性质是与致命结果强相关的重要预测因子。

Conclusion: 该框架成功结合了严谨的数据工程、先进的机器学习和可解释AI，实现了对兽药安全结果的准确且可解释的预测。该方法支持FARAD的使命，通过早期检测高风险药物-事件特征，强化残留风险评估，并为监管和临床决策提供信息，从而保障动物福利和人类食品安全。

Abstract: The safe use of pharmaceuticals in food-producing animals is vital to protect
animal welfare and human food safety. Adverse events (AEs) may signal
unexpected pharmacokinetic or toxicokinetic effects, increasing the risk of
violative residues in the food chain. This study introduces a predictive
framework for classifying outcomes (Death vs. Recovery) using ~1.28 million
reports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary
Medicine. A preprocessing pipeline merged relational tables and standardized
AEs through VeDDRA ontologies. Data were normalized, missing values imputed,
and high-cardinality features reduced; physicochemical drug properties were
integrated to capture chemical-residue links. We evaluated supervised models,
including Random Forest, CatBoost, XGBoost, ExcelFormer, and large language
models (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as
undersampling and oversampling, with a focus on prioritizing recall for fatal
outcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best,
achieving precision, recall, and F1-scores of 0.95. Incorporating Average
Uncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved
minority-class detection, particularly in ExcelFormer and XGBoost.
Interpretability via SHAP identified biologically plausible predictors,
including lung, heart, and bronchial disorders, animal demographics, and drug
physicochemical properties. These features were strongly linked to fatal
outcomes. Overall, the framework shows that combining rigorous data
engineering, advanced machine learning, and explainable AI enables accurate,
interpretable predictions of veterinary safety outcomes. The approach supports
FARAD's mission by enabling early detection of high-risk drug-event profiles,
strengthening residue risk assessment, and informing regulatory and clinical
decision-making.

</details>


### [222] [CarbonX: An Open-Source Tool for Computational Decarbonization Using Time Series Foundation Models](https://arxiv.org/abs/2510.01521)
*Diptyaroop Maji,Kang Yang,Prashant Shenoy,Ramesh K Sitaraman,Mani Srivastava*

Main category: cs.LG

TL;DR: CarbonX是一个开源工具，利用时间序列基础模型为全球范围内的碳强度预测和估算提供解决方案，克服了现有工具对特定电网数据和模型的依赖，并提供不确定性估计，实现卓越性能，支持全球去碳化。


<details>
  <summary>Details</summary>
Motivation: 计算去碳化需要准确、细粒度的碳强度预测，但现有工具存在三大局限：1) 需要特定电网的电力构成数据，限制了应用范围；2) 依赖独立的电网特定模型，难以实现全球覆盖；3) 缺乏不确定性估计，降低了下游应用的可靠性。

Method: 本文提出了开源工具CarbonX，该工具利用时间序列基础模型（TSFMs）来执行碳强度预测和估算等多种去碳化任务。CarbonX仅使用历史碳强度数据和一个通用模型，即可在不同电网和任务上提供强大性能，并提供预测区间。

Result: CarbonX在全球214个电网中，实现了15.82%的零样本预测平均绝对百分误差（MAPE）。在13个基准电网中，其性能与现有最佳技术相当，平均MAPE为9.59%，尾部预测MAPE为16.54%，并能提供95%覆盖率的预测区间。此外，CarbonX可提供长达21天的预测，准确性下降极小；在完全微调后，其在估算任务上比统计基线表现高出1.2-3.9倍。

Conclusion: CarbonX工具能够方便地应用于任何数据有限的电网，并提供强大的性能，使其成为实现全球规模去碳化的一款实用工具，有效解决了现有碳强度预测工具的局限性。

Abstract: Computational decarbonization aims to reduce carbon emissions in computing
and societal systems such as data centers, transportation, and built
environments. This requires accurate, fine-grained carbon intensity forecasts,
yet existing tools have several key limitations: (i) they require grid-specific
electricity mix data, restricting use where such information is unavailable;
(ii) they depend on separate grid-specific models that make it challenging to
provide global coverage; and (iii) they provide forecasts without uncertainty
estimates, limiting reliability for downstream carbon-aware applications.
  In this paper, we present CarbonX, an open-source tool that leverages Time
Series Foundation Models (TSFMs) for a range of decarbonization tasks. CarbonX
utilizes the versatility of TSFMs to provide strong performance across multiple
tasks, such as carbon intensity forecasting and imputation, and across diverse
grids. Using only historical carbon intensity data and a single general model,
our tool achieves a zero-shot forecasting Mean Absolute Percentage Error (MAPE)
of 15.82% across 214 grids worldwide. Across 13 benchmark grids, CarbonX
performance is comparable with the current state-of-the-art, with an average
MAPE of 9.59% and tail forecasting MAPE of 16.54%, while also providing
prediction intervals with 95% coverage. CarbonX can provide forecasts for up to
21 days with minimal accuracy degradation. Further, when fully fine-tuned,
CarbonX outperforms the statistical baselines by 1.2--3.9X on the imputation
task. Overall, these results demonstrate that CarbonX can be used easily on any
grid with limited data and still deliver strong performance, making it a
practical tool for global-scale decarbonization.

</details>


### [223] [On Integer Programming for the Binarized Neural Network Verification Problem](https://arxiv.org/abs/2510.01525)
*Woojin Kim,James R. Luedtke*

Main category: cs.LG

TL;DR: 本文提出两种技术（新的线性目标函数方法和利用递归结构的有效不等式）来改进二值神经网络（BNN）验证问题的整数规划（IP）公式，从而在有限时间内实现更大范围的输入扰动验证。


<details>
  <summary>Details</summary>
Motivation: BNN验证问题可表述为整数规划，但其原始IP公式因大M约束导致的整数间隙大而难以求解。

Method: 1. 引入一种新的方法来获取多分类设置的线性目标函数。2. 引入一种利用BNN递归结构生成IP公式有效不等式的新技术。

Result: 所提出的技术在有限时间内，能够验证BNN对抗更高范围的输入扰动，优于现有IP方法。

Conclusion: 通过改进整数规划公式，本研究有效提升了BNN验证问题的可解性和验证范围，增强了BNN鲁棒性评估的效率和能力。

Abstract: Binarized neural networks (BNNs) are feedforward neural networks with binary
weights and activation functions. In the context of using a BNN for
classification, the verification problem seeks to determine whether a small
perturbation of a given input can lead it to be misclassified by the BNN, and
the robustness of the BNN can be measured by solving the verification problem
over multiple inputs. The BNN verification problem can be formulated as an
integer programming (IP) problem. However, the natural IP formulation is often
challenging to solve due to a large integrality gap induced by big-$M$
constraints. We present two techniques to improve the IP formulation. First, we
introduce a new method for obtaining a linear objective for the multi-class
setting. Second, we introduce a new technique for generating valid inequalities
for the IP formulation that exploits the recursive structure of BNNs. We find
that our techniques enable verifying BNNs against a higher range of input
perturbation than existing IP approaches within a limited time.

</details>


### [224] [Round-trip Reinforcement Learning: Self-Consistent Training for Better Chemical LLMs](https://arxiv.org/abs/2510.01527)
*Lecheng Kong,Xiyuan Wang,Yixin Chen,Muhan Zhang*

Main category: cs.LG

TL;DR: LLMs在计算化学中缺乏往返一致性，影响其性能。本文提出往返强化学习(RTRL)框架，通过往返转换成功作为奖励信号来提高模型的一致性和性能。


<details>
  <summary>Details</summary>
Motivation: 计算化学中的LLMs在反应预测和逆合成等任务上表现出色，但常缺乏往返一致性（例如，模型能描述分子但无法从描述重建分子）。这种不一致性表明模型是单向记忆而非灵活掌握，且其与模型在主要任务上的表现强相关。因此，提升往返一致性成为一个直接且重要的模型改进目标。

Method: 本文提出了往返强化学习(RTRL)框架，该框架通过将往返转换的成功作为奖励信号来训练模型，以提高其一致性。进一步，引入了一个迭代变体，其中前向和反向映射交替地相互训练，形成一个自改进循环，这种方法对化学领域常见的海量未标记数据具有高数据效率和显著效果。

Result: 实验结果表明，RTRL在监督、自监督和合成数据体系中，相对于强大的基线模型，显著提升了模型的性能和一致性。

Conclusion: 研究表明，往返一致性不仅是一个理想的属性，也是一个可训练的目标，为开发更稳健可靠的化学基础模型提供了一条新途径。

Abstract: Large Language Models (LLMs) are emerging as versatile foundation models for
computational chemistry, handling bidirectional tasks like reaction prediction
and retrosynthesis. However, these models often lack round-trip consistency.
For instance, a state-of-the-art chemical LLM may successfully caption a
molecule, yet be unable to accurately reconstruct the original structure from
its own generated text. This inconsistency suggests that models are learning
unidirectional memorization rather than flexible mastery. Indeed, recent work
has demonstrated a strong correlation between a model's round-trip consistency
and its performance on the primary tasks. This strong correlation reframes
consistency into a direct target for model improvement. We therefore introduce
Round-Trip Reinforcement Learning (RTRL), a novel framework that trains a model
to improve its consistency by using the success of a round-trip transformation
as a reward signal. We further propose an iterative variant where forward and
reverse mappings alternately train each other in a self-improvement loop, a
process that is highly data-efficient and notably effective with the massive
amount of unlabelled data common in chemistry. Experiments demonstrate that
RTRL significantly \textbf{boosts performance and consistency} over strong
baselines across supervised, self-supervised, and synthetic data regimes. This
work shows that round-trip consistency is not just a desirable property but a
trainable objective, offering a new path toward more robust and reliable
foundation models.

</details>


### [225] [Bypassing Prompt Guards in Production with Controlled-Release Prompting](https://arxiv.org/abs/2510.01529)
*Jaiden Fairoze,Sanjam Garg,Keewoo Lee,Mingyuan Wang*

Main category: cs.LG

TL;DR: 研究提出一种利用资源不对称的新型攻击，成功绕过轻量级提示防护，越狱了包括Google Gemini在内的多种主流LLM，揭示了此类防护的固有弱点，并强调了将防御重点从输入过滤转向输出预防的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的进步，确保AI安全和对齐至关重要。提示防护作为一种轻量级且易于实施的机制被广泛用于过滤恶意查询，但其局限性需要被揭示。

Method: 该方法利用提示防护机制与主LLM之间的计算资源不对称。攻击者编码一个越狱提示，使其复杂到轻量级防护无法解码过滤，但主LLM能够理解并执行。

Result: 该攻击在保持响应质量的同时，持续越狱了包括Google Gemini (2.5 Flash/Pro)、DeepSeek Chat (DeepThink)、Grok (3) 和 Mistral Le Chat (Magistral) 在内的主流生产模型。这揭示了现代LLM架构中轻量级提示防护固有的攻击面。此外，研究还发现了版权数据提取、训练数据提取和恶意响应在思考过程中泄露等其他关键对齐问题。

Conclusion: 研究结果强调了将防御策略从阻断恶意输入转向预防恶意输出的必要性，以应对轻量级提示防护的固有局限性及其在现代LLM架构中的攻击面。

Abstract: As large language models (LLMs) advance, ensuring AI safety and alignment is
paramount. One popular approach is prompt guards, lightweight mechanisms
designed to filter malicious queries while being easy to implement and update.
In this work, we introduce a new attack that circumvents such prompt guards,
highlighting their limitations. Our method consistently jailbreaks production
models while maintaining response quality, even under the highly protected chat
interfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok
(3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry
between the prompt guard and the main LLM, encoding a jailbreak prompt that
lightweight guards cannot decode but the main model can. This reveals an attack
surface inherent to lightweight prompt guards in modern LLM architectures and
underscores the need to shift defenses from blocking malicious inputs to
preventing malicious outputs. We additionally identify other critical alignment
issues, such as copyrighted data extraction, training data extraction, and
malicious response leakage during thinking.

</details>


### [226] [NVIDIA AI Aerial: AI-Native Wireless Communications](https://arxiv.org/abs/2510.01533)
*Kobi Cohen-Arazi,Michael Roe,Zhen Hu,Rohan Chavan,Anna Ptasznik,Joanna Lin,Joao Morais,Joseph Boccuzzi,Tommaso Balercia*

Main category: cs.LG

TL;DR: 本文提出一个将Python训练的AI/ML算法编译成GPU可运行模块的框架，以高效、灵活的方式将AI能力整合到6G蜂窝系统中，并以CNN进行信道估计为例进行了验证。


<details>
  <summary>Details</summary>
Motivation: 6G时代向AI原生无线系统转变，要求数字信号处理（DSP）和机器学习（ML）无缝集成，并高效迭代训练、模拟和部署AI模型和算法。

Method: 提出一个鲁棒框架，将Python算法编译成可在GPU上运行的二进制文件。该方法在NVIDIA AI Aerial平台实现，并以在PUSCH接收器中通过Python训练的卷积神经网络（CNN）进行信道估计为例，在数字孪生和实时测试平台中进行了验证。

Result: 该统一方法确保了在NVIDIA GPU上的高效率、灵活性和最高性能。成功展示了在数字孪生和实时测试平台中执行信道估计功能的能力。

Conclusion: 所提出的方法为AI/ML模型可扩展集成到下一代蜂窝系统奠定了基础，对于实现AI原生6G网络的愿景至关重要。

Abstract: 6G brings a paradigm shift towards AI-native wireless systems, necessitating
the seamless integration of digital signal processing (DSP) and machine
learning (ML) within the software stacks of cellular networks. This
transformation brings the life cycle of modern networks closer to AI systems,
where models and algorithms are iteratively trained, simulated, and deployed
across adjacent environments. In this work, we propose a robust framework that
compiles Python-based algorithms into GPU-runnable blobs. The result is a
unified approach that ensures efficiency, flexibility, and the highest possible
performance on NVIDIA GPUs. As an example of the capabilities of the framework,
we demonstrate the efficacy of performing the channel estimation function in
the PUSCH receiver through a convolutional neural network (CNN) trained in
Python. This is done in a digital twin first, and subsequently in a real-time
testbed. Our proposed methodology, realized in the NVIDIA AI Aerial platform,
lays the foundation for scalable integration of AI/ML models into
next-generation cellular systems, and is essential for realizing the vision of
natively intelligent 6G networks.

</details>


### [227] [TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis](https://arxiv.org/abs/2510.01538)
*Haokun Zhao,Xiang Zhang,Jiaqi Wei,Yiwei Xu,Yuting He,Siqi Sun,Chenyu You*

Main category: cs.LG

TL;DR: 本文提出了TimeSeriesScientist (TSci)，一个首创的LLM驱动的代理框架，用于通用的时间序列预测，通过自动化和透明化流程，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型在处理多样、短、噪声数据时，面临耗时的人工预处理、验证和集成，且泛化能力差。急需一个通用、领域无关且最小化人工干预的框架。

Method: 引入TSci，一个由四个专业代理组成的LLM驱动框架：Curator进行诊断和预处理；Planner缩小模型选择空间；Forecaster执行模型拟合、验证和自适应选择最佳配置及集成策略；Reporter生成综合透明的报告。这使得预测工作流成为一个可解释和可扩展的白盒系统。

Result: 在八个基准测试中，TSci持续优于统计模型和现有LLM基线，预测误差分别平均降低10.4%和38.2%。此外，TSci生成清晰严谨的报告，增强了工作流的透明度和可解释性。

Conclusion: TSci作为一个LLM驱动的代理框架，成功地解决了时间序列预测中的痛点，通过自动化和透明化流程，不仅大幅提升了预测性能，还增强了可解释性和可扩展性。

Abstract: Time series forecasting is central to decision-making in domains as diverse
as energy, finance, climate, and public health. In practice, forecasters face
thousands of short, noisy series that vary in frequency, quality, and horizon,
where the dominant cost lies not in model fitting, but in the labor-intensive
preprocessing, validation, and ensembling required to obtain reliable
predictions. Prevailing statistical and deep learning models are tailored to
specific datasets or domains and generalize poorly. A general, domain-agnostic
framework that minimizes human intervention is urgently in demand. In this
paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic
framework for general time series forecasting. The framework comprises four
specialized agents: Curator performs LLM-guided diagnostics augmented by
external tools that reason over data statistics to choose targeted
preprocessing; Planner narrows the hypothesis space of model choice by
leveraging multi-modal diagnostics and self-planning over the input; Forecaster
performs model fitting and validation and, based on the results, adaptively
selects the best model configuration as well as ensemble strategy to make final
predictions; and Reporter synthesizes the whole process into a comprehensive,
transparent report. With transparent natural-language rationales and
comprehensive reports, TSci transforms the forecasting workflow into a
white-box system that is both interpretable and extensible across tasks.
Empirical results on eight established benchmarks demonstrate that TSci
consistently outperforms both statistical and LLM-based baselines, reducing
forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci
produces a clear and rigorous report that makes the forecasting workflow more
transparent and interpretable.

</details>


### [228] [Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code](https://arxiv.org/abs/2510.01539)
*Aniket Vashishtha,Qirun Dai,Hongyuan Mei,Amit Sharma,Chenhao Tan,Hao Peng*

Main category: cs.LG

TL;DR: LLMs在反事实推理中跳过溯因步骤导致性能虚高。本研究引入可执行反事实框架，揭示现有SOTA模型在此任务上准确率显著下降。通过对比SFT和RL，发现RL能有效提升LLM的反事实推理能力并泛化到新领域。


<details>
  <summary>Details</summary>
Motivation: 反事实推理对提升LLM的因果理解和在高风险领域的应用至关重要。然而，现有评估LLM反事实推理能力的方法常跳过“溯因”步骤，将其简化为干预推理，导致对LLM实际性能的过高估计。

Method: 提出“可执行反事实”框架，通过代码和数学问题操作化因果推理，明确要求反事实推理的全部三个步骤（溯因、干预、预测）。该框架支持可扩展的合成数据生成。为弥补性能差距，研究构建了反事实代码训练集，并测试了监督微调（SFT）和强化学习（RL）在领域内和领域外任务（如反事实数学问题）上的泛化能力。

Result: 从干预推理到包含溯因的反事实推理任务，SOTA模型（如o4-mini和Claude-4-Sonnet）的准确率显著下降25-40%。监督微调虽能提高Qwen模型在领域内任务的表现，但导致在领域外任务（如反事实数学问题）上准确率下降。相比之下，强化学习能诱导核心认知行为，并泛化到新领域，使代码问题性能提升1.5-2倍，数学问题也有所改善。

Conclusion: 现有LLMs在包含溯因步骤的完整反事实推理任务中表现出显著不足。强化学习在提升LLM的反事实推理能力方面展现出巨大潜力，且能有效泛化到新领域。

Abstract: Counterfactual reasoning, a hallmark of intelligence, consists of three
steps: inferring latent variables from observations (abduction), constructing
alternatives (interventions), and predicting their outcomes (prediction). This
skill is essential for advancing LLMs' causal understanding and expanding their
applications in high-stakes domains such as scientific research. However,
existing efforts in assessing LLM's counterfactual reasoning capabilities tend
to skip the abduction step, effectively reducing to interventional reasoning
and leading to overestimation of LLM performance. To address this, we introduce
executable counterfactuals, a novel framework that operationalizes causal
reasoning through code and math problems. Our framework explicitly requires all
three steps of counterfactual reasoning and enables scalable synthetic data
creation with varying difficulty, creating a frontier for evaluating and
improving LLM's reasoning. Our results reveal substantial drop in accuracy
(25-40%) from interventional to counterfactual reasoning for SOTA models like
o4-mini and Claude-4-Sonnet. To address this gap, we construct a training set
comprising counterfactual code problems having if-else condition and test on
out-of-domain code structures (e.g. having while-loop); we also test whether a
model trained on code would generalize to counterfactual math word problems.
While supervised finetuning on stronger models' reasoning traces improves
in-domain performance of Qwen models, it leads to a decrease in accuracy on OOD
tasks such as counterfactual math problems. In contrast, reinforcement learning
induces the core cognitive behaviors and generalizes to new domains, yielding
gains over the base model on both code (improvement of 1.5x-2x) and math
problems. Analysis of the reasoning traces reinforces these findings and
highlights the promise of RL for improving LLMs' counterfactual reasoning.

</details>


### [229] [Predictive Preference Learning from Human Interventions](https://arxiv.org/abs/2510.01545)
*Haoyuan Cai,Zhenghao Peng,Bolei Zhou*

Main category: cs.LG

TL;DR: 本文提出PPL（Predictive Preference Learning from Human Interventions）方法，利用人类干预中的隐式偏好信号预测并优化智能体未来的行为，以提高学习效率和安全性，减少所需的人类演示。


<details>
  <summary>Details</summary>
Motivation: 大多数交互式模仿学习方法仅纠正智能体在当前状态的动作，而忽略了调整未来可能更危险状态的动作，这是现有方法的局限性。

Method: PPL方法的核心是将人类的每次干预引导到L个未来时间步（偏好视野），并假设智能体和人类在此视野内遵循相同的动作和干预。通过对这些未来状态应用偏好优化，将专家纠正传播到智能体预计会探索的安全关键区域。

Result: 在自动驾驶和机器人操作基准测试中，实验证明了PPL方法的效率和通用性。理论分析进一步表明，选择合适的偏好视野L可以平衡风险状态的覆盖范围与标签的正确性，从而限制了算法的最优性差距。

Conclusion: PPL通过有效利用人类干预，将专家纠正传播到未来的安全关键状态，显著提升了交互式模仿学习的效率和安全性，同时减少了所需的人类演示。

Abstract: Learning from human involvement aims to incorporate the human subject to
monitor and correct agent behavior errors. Although most interactive imitation
learning methods focus on correcting the agent's action at the current state,
they do not adjust its actions in future states, which may be potentially more
hazardous. To address this, we introduce Predictive Preference Learning from
Human Interventions (PPL), which leverages the implicit preference signals
contained in human interventions to inform predictions of future rollouts. The
key idea of PPL is to bootstrap each human intervention into L future time
steps, called the preference horizon, with the assumption that the agent
follows the same action and the human makes the same intervention in the
preference horizon. By applying preference optimization on these future states,
expert corrections are propagated into the safety-critical regions where the
agent is expected to explore, significantly improving learning efficiency and
reducing human demonstrations needed. We evaluate our approach with experiments
on both autonomous driving and robotic manipulation benchmarks and demonstrate
its efficiency and generality. Our theoretical analysis further shows that
selecting an appropriate preference horizon L balances coverage of risky states
with label correctness, thereby bounding the algorithmic optimality gap. Demo
and code are available at: https://metadriverse.github.io/ppl

</details>


### [230] [MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models](https://arxiv.org/abs/2510.01549)
*Kevin Zhai,Utsav Singh,Anirudh Thatipelli,Souradip Chakraborty,Anit Kumar Sahu,Furong Huang,Amrit Singh Bedi,Mubarak Shah*

Main category: cs.LG

TL;DR: 提出MIRA，一种无需训练的推理时对齐方法，通过图像空间约束有效缓解扩散模型中的奖励劫持，提升图像质量同时保持提示语一致性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的图像常不满足用户特定奖励指标（如美学分数），而传统微调计算成本高。推理时对齐的噪声优化方法虽然高效，但易导致“奖励劫持”，即生成高分图像却严重偏离原始提示。

Method: 提出MIRA (MItigating Reward hAcking)，一种训练无关、推理时对齐方法。MIRA引入一个图像空间、基于分数的KL散度代理，用冻结的主干网络正则化采样轨迹，约束输出分布以确保奖励提升同时避免偏离分布（奖励劫持）。为此，MIRA推导了KL散度的可处理近似。此外，还提出了MIRA-DPO，将偏好优化映射到推理时间，以处理不可微分奖励。

Result: 在SDv1.5和SDXL模型、多种奖励（Aesthetic, HPSv2, PickScore）和公共数据集上，MIRA对比强基线实现了超过60%的胜率，同时保持了提示语依从性。机制图显示MIRA在奖励提升的同时漂移接近零，而基线方法DNO随计算量增加出现漂移。

Conclusion: MIRA提供了一种高效且鲁棒的解决方案，通过图像空间约束成功解决了扩散模型推理时对齐中的奖励劫持问题，在提升图像质量和奖励的同时，有效保持了与原始提示语的一致性，并能处理不可微分奖励。

Abstract: Diffusion models excel at generating images conditioned on text prompts, but
the resulting images often do not satisfy user-specific criteria measured by
scalar rewards such as Aesthetic Scores. This alignment typically requires
fine-tuning, which is computationally demanding. Recently, inference-time
alignment via noise optimization has emerged as an efficient alternative,
modifying initial input noise to steer the diffusion denoising process towards
generating high-reward images. However, this approach suffers from reward
hacking, where the model produces images that score highly, yet deviate
significantly from the original prompt. We show that noise-space regularization
is insufficient and that preventing reward hacking requires an explicit
image-space constraint. To this end, we propose MIRA (MItigating Reward
hAcking), a training-free, inference-time alignment method. MIRA introduces an
image-space, score-based KL surrogate that regularizes the sampling trajectory
with a frozen backbone, constraining the output distribution so reward can
increase without off-distribution drift (reward hacking). We derive a tractable
approximation to KL using diffusion scores. Across SDv1.5 and SDXL, multiple
rewards (Aesthetic, HPSv2, PickScore), and public datasets (e.g.,
Animal-Animal, HPDv2), MIRA achieves >60\% win rate vs. strong baselines while
preserving prompt adherence; mechanism plots show reward gains with near-zero
drift, whereas DNO drifts as compute increases. We further introduce MIRA-DPO,
mapping preference optimization to inference time with a frozen backbone,
extending MIRA to non-differentiable rewards without fine-tuning.

</details>


### [231] [Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization](https://arxiv.org/abs/2510.01555)
*Kezhao Liu,Jason Klein Liu,Mingtao Chen,Yiming Liu*

Main category: cs.LG

TL;DR: 本文统一分析了RLHF中KL散度正则化的两种实现方式，证明了PPO的'$k_1$ in reward'和on-policy下梯度等效的'$k_2$ as loss'是理论上正确的实现，而GRPO的'$k_3$ as loss'是有偏近似，并提出了off-policy实现的修正方法。


<details>
  <summary>Details</summary>
Motivation: RLHF中KL散度损失的实现方式存在问题，尤其是在GRPO等方法中，其功能角色（优化损失而非数值估计）常被忽视，导致实现可能不理想或不一致。

Method: 1. 建立一个统一框架，连接两种KL散度实现风格：作为策略得分函数的系数（'$k_n$ in reward'）或作为直接梯度传播的损失函数（'$k_n$ as loss'）。2. 证明'$k_n$ as loss'可由'$k_n$ in reward'中的等效梯度系数分析。3. 对off-policy '$k_n$ as loss'中因忽视重要性采样导致的偏差提出了原则性修正。

Result: 1. 传统的'$k_1$ in reward'（如PPO）是逆向KL（RKL）正则化的原则性损失。2. 在on-policy条件下，'$k_2$ as loss'在梯度上等效于'$k_1$ in reward'，两者均为RKL目标的理论稳健实现。3. 最近采用的'$k_3$ as loss'（如GRPO）仅是原则性损失的一阶有偏近似。4. 常见的off-policy '$k_n$ as loss'实现因忽略重要性采样而存在偏差。

Conclusion: 本研究为KL正则化的选择和正确实施提供了全面的、基于梯度的理论依据，有助于开发更稳健有效的RLHF系统。

Abstract: Reinforcement Learning from Human Feedback (RLHF) leverages a
Kullback-Leibler (KL) divergence loss to stabilize training and prevent
overfitting. However, in methods such as GRPO, its implementation may be guided
by principles from numerical value estimation-a practice that overlooks the
term's functional role as an optimization loss. To analyze this issue, we
establish a unified framework that connects two seemingly distinct
implementation styles: using the mathematical term $k_n$ as a detached
coefficient for the policy's score function ('$k_n$ in reward') or as a direct
loss function through which gradients are propagated ('$k_n$ as loss'). We show
that the latter can always be analyzed via an equivalent gradient coefficient
in the former, unifying the two perspectives. Through this framework, we prove
that the conventional '$k_1$ in reward' (like in PPO) is the principled loss
for Reverse KL (RKL) regularization. We further establish a key finding: under
on-policy conditions, the '$k_2$ as loss' formulation is, in fact,
gradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our
work, identifies both as the theoretically sound implementations of the RKL
objective. In contrast, we show that the recently adopted '$k_3$ as loss' (like
in GRPO) is merely a first-order, biased approximation of the principled loss.
Furthermore, we argue that common off-policy implementations of '$k_n$ as loss'
methods are biased due to neglected importance sampling, and we propose a
principled correction. Our findings provide a comprehensive, gradient-based
rationale for choosing and correctly implementing KL regularization, paving the
way for more robust and effective RLHF systems.

</details>


### [232] [Large-Scale Bayesian Causal Discovery with Interventional Data](https://arxiv.org/abs/2510.01562)
*Seong Woo Han,Daniel Duy Vo,Brielin C. Brown*

Main category: cs.LG

TL;DR: 提出了一种名为IBCD的经验贝叶斯框架，用于利用干预数据进行因果发现。IBCD通过建模总因果效应矩阵的似然，并采用特殊先验，实现了卓越的结构恢复和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 因果关系推断（DAG形式）是一个重要但极具挑战性的问题。尽管高通量基因组扰动筛选技术启发了利用干预数据改进模型识别的方法，但现有方法在处理大规模任务时性能不佳，并且无法量化不确定性。

Method: 提出了Interventional Bayesian Causal Discovery (IBCD)，一个基于干预数据的经验贝叶斯因果发现框架。该方法建模总因果效应矩阵的似然（通过矩阵正态分布近似），而非完整数据矩阵。对边施加spike-and-slab horseshoe先验，并从观测数据中学习无标度结构和Erdős-Rényi结构的权重，将每条边视为潜变量以实现不确定性感知推理。

Result: 通过广泛模拟，IBCD在结构恢复方面优于现有基线方法。将其应用于521个基因的CRISPR扰动数据（Perturb-seq），结果表明边的后验包含概率能够识别出鲁棒的图结构。

Conclusion: IBCD是一个用于干预数据因果发现的有效经验贝叶斯框架，能够实现卓越的结构恢复，并提供不确定性量化，尤其适用于大规模基因组数据。

Abstract: Inferring the causal relationships among a set of variables in the form of a
directed acyclic graph (DAG) is an important but notoriously challenging
problem. Recently, advancements in high-throughput genomic perturbation screens
have inspired development of methods that leverage interventional data to
improve model identification. However, existing methods still suffer poor
performance on large-scale tasks and fail to quantify uncertainty. Here, we
propose Interventional Bayesian Causal Discovery (IBCD), an empirical Bayesian
framework for causal discovery with interventional data. Our approach models
the likelihood of the matrix of total causal effects, which can be approximated
by a matrix normal distribution, rather than the full data matrix. We place a
spike-and-slab horseshoe prior on the edges and separately learn data-driven
weights for scale-free and Erd\H{o}s-R\'enyi structures from observational
data, treating each edge as a latent variable to enable uncertainty-aware
inference. Through extensive simulation, we show that IBCD achieves superior
structure recovery compared to existing baselines. We apply IBCD to CRISPR
perturbation (Perturb-seq) data on 521 genes, demonstrating that edge posterior
inclusion probabilities enable identification of robust graph structures.

</details>


### [233] [TetriServe: Efficient DiT Serving for Heterogeneous Image Generation](https://arxiv.org/abs/2510.01565)
*Runyu Lu,Shiqi He,Wenxuan Tan,Shenggui Li,Ruofan Wu,Jeff J. Ma,Ang Chen,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: DiT模型服务成本高、现有系统效率低。本文提出TetriServe系统，通过步级序列并行和轮次调度动态调整并行度，显著提升SLO达成率。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformer (DiT) 模型生成高质量图像但计算成本高，尤其在大分辨率下难以满足严格的服务级别目标 (SLO)。现有服务系统采用固定程度的序列并行，对于混合分辨率和截止期限的异构工作负载效率低下，导致GPU利用率低和SLO达成率差。

Method: 本文提出步级序列并行（step-level sequence parallelism）方法，根据请求的截止期限动态调整并行度。在此基础上，开发了TetriServe系统，引入新颖的轮次调度机制，具体包括：1) 将时间离散化为固定轮次以实现截止期限感知调度；2) 在步级自适应并行度并最小化GPU小时消耗；3) 联合打包请求以最小化延迟完成。

Result: 通过在最先进的DiT模型上进行广泛评估，TetriServe系统在不降低图像质量的前提下，相较于现有解决方案，将SLO达成率提高了32%。

Conclusion: TetriServe系统通过其创新的步级序列并行和轮次调度机制，有效解决了DiT模型服务面临的高计算成本和低SLO达成率问题，显著提升了图像生成的服务效率和可靠性。

Abstract: Diffusion Transformer (DiT) models excel at generating highquality images
through iterative denoising steps, but serving them under strict Service Level
Objectives (SLOs) is challenging due to their high computational cost,
particularly at large resolutions. Existing serving systems use fixed degree
sequence parallelism, which is inefficient for heterogeneous workloads with
mixed resolutions and deadlines, leading to poor GPU utilization and low SLO
attainment.
  In this paper, we propose step-level sequence parallelism to dynamically
adjust the parallel degree of individual requests according to their deadlines.
We present TetriServe, a DiT serving system that implements this strategy for
highly efficient image generation. Specifically, TetriServe introduces a novel
round-based scheduling mechanism that improves SLO attainment: (1) discretizing
time into fixed rounds to make deadline-aware scheduling tractable, (2)
adapting parallelism at the step level and minimize GPU hour consumption, and
(3) jointly packing requests to minimize late completions. Extensive evaluation
on state-of-the-art DiT models shows that TetriServe achieves up to 32% higher
SLO attainment compared to existing solutions without degrading image quality.

</details>


### [234] [From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?](https://arxiv.org/abs/2510.01571)
*Hanqun Cao,Hongrui Zhang,Junde Xu,Zhou Zhang,Lingdong Shen,Minghao Sun,Ge Liu,Jinbo Xu,Wu-Jun Li,Jinren Ni,Cesar de la Fuente-Nunez,Tianfan Fu,Yejin Choi,Pheng-Ann Heng,Fang Wu*

Main category: cs.LG

TL;DR: 研究发现，在蛋白质设计中结合强化学习（RL）与蛋白质语言模型（PLM）能显著提高成功率和采样效率，其效果取决于任务的难度空间、奖励的准确性及策略容量。


<details>
  <summary>Details</summary>
Motivation: 尽管PLM和RL在计算蛋白质科学中取得了进展，但RL能否帮助PLM超越其预训练的先验知识，揭示潜在的序列-结构-功能规则尚不明确。

Method: 将RL与PLM结合，在抗菌肽设计、激酶变体优化、抗体工程和逆向折叠四个领域进行测试。采用多种RL算法和模型，旨在评估RL能否提高采样效率并发现监督学习未捕获的能力。

Result: RL一致地提高了基准测试中的成功率和采样效率。性能提升由任务难度空间、奖励保真度和策略容量三者共同决定。当奖励准确、策略容量充足且任务有改进空间时，效果最佳；奖励噪声大或容量受限时，收益饱和。

Conclusion: 在蛋白质设计中应用RL时，应优先考虑奖励建模和校准，根据任务难度匹配算法和正则化强度，并合理分配策略容量，以实现最大的边际收益。

Abstract: Protein language models (PLMs) have advanced computational protein science
through large-scale pretraining and scalable architectures. In parallel,
reinforcement learning (RL) has broadened exploration and enabled precise
multi-objective optimization in protein design. Yet whether RL can push PLMs
beyond their pretraining priors to uncover latent sequence-structure-function
rules remains unclear. We address this by pairing RL with PLMs across four
domains: antimicrobial peptide design, kinase variant optimization, antibody
engineering, and inverse folding. Using diverse RL algorithms and model
classes, we ask if RL improves sampling efficiency and, more importantly, if it
reveals capabilities not captured by supervised learning. Across benchmarks, RL
consistently boosts success rates and sample efficiency. Performance follows a
three-factor interaction: task headroom, reward fidelity, and policy capacity
jointly determine gains. When rewards are accurate and informative, policies
have sufficient capacity, and tasks leave room beyond supervised baselines,
improvements scale; when rewards are noisy or capacity is constrained, gains
saturate despite exploration. This view yields practical guidance for RL in
protein design: prioritize reward modeling and calibration before scaling
policy size, match algorithm and regularization strength to task difficulty,
and allocate capacity where marginal gains are largest. Implementation is
available at https://github.com/chq1155/RL-PLM.

</details>


### [235] [Gradient Shaping Beyond Clipping: A Functional Perspective on Update Magnitude Control](https://arxiv.org/abs/2510.01578)
*Haochen You,Baojing Liu*

Main category: cs.LG

TL;DR: 提出SPAMP框架，通过动态、统计和逐层的方式平滑调整梯度，以替代传统的硬性梯度裁剪，提高深度网络训练的稳定性、收敛性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的梯度裁剪（gradient clipping）采用硬性固定阈值，缺乏灵活性，忽略了梯度分布的动态性，限制了深度网络训练的稳定性。

Method: SPAMP（Statistical Per-layer Adaptive Modulation and Projection）框架通过跟踪局部梯度统计量，动态估计阈值，并应用基于幂的变换来以可微分的方式调整更新幅度，实现平滑的逐层梯度整形。

Result: 在图像和语言任务上的大量实验表明，SPAMP在稳定性、收敛性和鲁棒性方面均优于现有方法。

Conclusion: SPAMP提供了一个有原则的替代方案，取代了僵化的启发式方法来控制有效更新规模，并将其与预热机制视为双重控制机制。

Abstract: Gradient clipping is widely used to stabilize deep network training, but its
formulation as a hard, fixed threshold limits flexibility and ignores gradient
distribution dynamics. We propose SPAMP (Statistical Per-layer Adaptive
Modulation and Projection), a unified framework that generalizes clipping into
smooth, per-layer gradient shaping. SPAMP tracks local gradient statistics,
dynamically estimates thresholds, and applies power-based transformations to
modulate update magnitudes in a differentiable manner. This perspective recasts
clipping and warmup as dual mechanisms for controlling the effective update
scale $\eta_t \|g_t\|$, offering a principled alternative to rigid heuristics.
Extensive experiments across image and language tasks demonstrate that SPAMP
improves stability, convergence, and robustness over existing methods.

</details>


### [236] [Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression](https://arxiv.org/abs/2510.01581)
*Joykirat Singh,Justin Chih-Yao Chen,Archiki Prasad,Elias Stengel-Eskin,Akshay Nambi,Mohit Bansal*

Main category: cs.LG

TL;DR: 本文提出TRAAC，一种基于强化学习的后训练方法，通过自适应地分配推理预算和压缩冗余步骤，解决现有思维模型在任务难度调整方面的不足，显著提升了准确性并减少了推理长度，并具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有思维模型在处理复杂推理任务时存在“适应性不足”问题：对难度大的问题推理过短（underthinking）导致错误，而对简单问题推理过长（overthinking）则效率低下、浪费token。模型未能根据不同难度的问题适当地调整响应长度。

Method: 本文提出TRAAC（Think Right with Adaptive, Attentive Compression），一种在线后训练强化学习（RL）方法。它利用模型在长推理轨迹上的自注意力机制来识别并修剪冗余步骤。TRAAC还会估计任务难度，并将其纳入训练奖励中，从而学习根据示例难度分配相应的推理预算。

Result: 与基础模型相比，TRAAC（Qwen3-4B）在多种任务上（AIME, AMC, GPQA-D, BBEH）平均绝对准确率提高了8.4%，推理长度相对减少了36.8%。与最佳RL基线相比，准确率提高了7.9%，长度减少了29.4%。TRAAC还表现出强大的泛化能力，在未训练过的非数学数据集上（如GPQA-D, BBEH, OptimalThinkingBench）也实现了准确性和效率的提升。分析证实，TRAAC能根据难度对思维预算进行精细调整，并且任务难度校准与基于注意力的压缩相结合，可在不同任务中带来收益。

Conclusion: TRAAC通过自适应的推理预算分配和基于注意力的压缩，有效解决了思维模型在不同任务难度下的适应性问题，显著提高了模型在各种任务上的准确性、效率和泛化能力。

Abstract: Recent thinking models solve complex reasoning tasks by scaling test-time
compute, but this scaling must be allocated in line with task difficulty. On
one hand, short reasoning (underthinking) leads to errors on harder problems
that require extended reasoning steps; but, excessively long reasoning
(overthinking) can be token-inefficient, generating unnecessary steps even
after reaching a correct intermediate solution. We refer to this as
under-adaptivity, where the model fails to modulate its response length
appropriately given problems of varying difficulty. To address under-adaptivity
and strike a balance between under- and overthinking, we propose TRAAC (Think
Right with Adaptive, Attentive Compression), an online post-training RL method
that leverages the model's self-attention over a long reasoning trajectory to
identify important steps and prune redundant ones. TRAAC also estimates
difficulty and incorporates it into training rewards, thereby learning to
allocate reasoning budget commensurate with example difficulty. Our approach
improves accuracy, reduces reasoning steps, and enables adaptive thinking
compared to base models and other RL baselines. Across a variety of tasks
(AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute
accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8%
compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length
drop compared to the best RL baseline. TRAAC also shows strong generalization:
although our models are trained on math datasets, they show accuracy and
efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH,
and OptimalThinkingBench. Our analysis further verifies that TRAAC provides
fine-grained adjustments to thinking budget based on difficulty and that a
combination of task-difficulty calibration and attention-based compression
yields gains across diverse tasks.

</details>


### [237] [Enhancing Noise Robustness of Parkinson's Disease Telemonitoring via Contrastive Feature Augmentation](https://arxiv.org/abs/2510.01588)
*Ziming Tang,Chengbin Hou,Tianyu Zhang,Bangxu Tian,Jinbao Wang,Hairong Lv*

Main category: cs.LG

TL;DR: 提出NoRo框架，通过生成抗噪声特征，提高帕金森病远程监测中UPDRS预测的噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 帕金森病(PD)远程监测UPDRS评分具有重要意义，但测量过程中存在患者、环境和数据传输引发的三种噪声，导致预测误差增加。

Method: 提出NoRo框架，首先将原始语音特征分组构建对比对；其次，利用对比对训练多层感知器编码器以生成抗噪声特征；最后，将这些特征与原始特征拼接作为增强特征输入UPDRS预测模型。同时引入了一种新的可定制噪声注入评估方法。

Result: 广泛实验表明，NoRo能成功提高在不同噪声环境下，各种下游预测模型对UPDRS预测的噪声鲁棒性。

Conclusion: NoRo框架有效解决了帕金森病远程监测中UPDRS预测的噪声问题，提升了预测的可靠性。

Abstract: Parkinson's disease (PD) is one of the most common neurodegenerative
disorder. PD telemonitoring emerges as a novel assessment modality enabling
self-administered at-home tests of Unified Parkinson's Disease Rating Scale
(UPDRS) scores, enhancing accessibility for PD patients. However, three types
of noise would occur during measurements: (1) patient-induced measurement
inaccuracies, (2) environmental noise, and (3) data packet loss during
transmission, resulting in higher prediction errors. To address these
challenges, NoRo, a noise-robust UPDRS prediction framework is proposed. First,
the original speech features are grouped into ordered bins, based on the
continuous values of a selected feature, to construct contrastive pairs.
Second, the contrastive pairs are employed to train a multilayer perceptron
encoder for generating noise-robust features. Finally, these features are
concatenated with the original features as the augmented features, which are
then fed into the UPDRS prediction models. Notably, we further introduces a
novel evaluation approach with customizable noise injection module, and
extensive experiments show that NoRo can successfully enhance the noise
robustness of UPDRS prediction across various downstream prediction models
under different noisy environments.

</details>


### [238] [Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness](https://arxiv.org/abs/2510.01598)
*Youwei Bao,Shuhan Yang,Hyunsoo Yang*

Main category: cs.LG

TL;DR: 研究提出了一种基于自旋转移矩磁隧道结 (STT-MTJ) 的硬件真随机数生成器 (TRNG)，可替代生成式AI模型中易受攻击的伪随机数生成器 (PRNG)，显著提高安全性并降低能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型中使用的确定性PRNGs会产生可预测模式，易受攻击。传统防御方法往往伴随高能耗和高延迟。

Method: 将STT-MTJ生成的硬件真随机比特嵌入到系统中。构建了一个FPGA辅助的高并行原型计算系统，并将其集成到用CIFAR-10训练的生成对抗网络 (GAN) 中。

Result: 该系统以最小开销提供了兆比特每秒的真随机数，并通过了NIST随机性测试。与低质量RNG基线相比，在GAN中不安全输出减少了18.6倍。STT-MTJ系统具有纳秒级开关速度、高能效和可扩展性，有望实现吉比特每秒的吞吐量，适用于大型语言模型采样。

Conclusion: 自旋电子RNGs（特别是基于STT-MTJ的系统）是下一代生成式AI系统实用的安全组件。

Abstract: Deterministic pseudo random number generators (PRNGs) used in generative
artificial intelligence (GAI) models produce predictable patterns vulnerable to
exploitation by attackers. Conventional defences against the vulnerabilities
often come with significant energy and latency overhead. Here, we embed
hardware-generated true random bits from spin-transfer torque magnetic tunnel
junctions (STT-MTJs) to address the challenges. A highly parallel,
FPGA-assisted prototype computing system delivers megabit-per-second true
random numbers, passing NIST randomness tests after in-situ operations with
minimal overhead. Integrating the hardware random bits into a generative
adversarial network (GAN) trained on CIFAR-10 reduces insecure outputs by up to
18.6 times compared to the low-quality random number generators (RNG) baseline.
With nanosecond switching speed, high energy efficiency, and established
scalability, our STT-MTJ-based system holds the potential to scale beyond 106
parallel cells, achieving gigabit-per-second throughput suitable for large
language model sampling. This advancement highlights spintronic RNGs as
practical security components for next-generation GAI systems.

</details>


### [239] [Posterior Collapse as a Phase Transition in Variational Autoencoders](https://arxiv.org/abs/2510.01621)
*Zhen Li,Fan Zhang,Zheng Zhang,Yu Chen*

Main category: cs.LG

TL;DR: VAEs中的后验坍缩是数据结构和模型超参数共同决定的相变现象，而非简单的优化失败。


<details>
  <summary>Details</summary>
Motivation: 探究变分自编码器（VAEs）中后验坍缩现象的本质，超越其作为优化失败的常见观点。

Method: 采用统计物理学视角，通过分析平凡解的稳定性识别临界超参数阈值，并在综合及真实数据集上验证该临界行为。

Result: 后验坍缩是一种相变，由数据结构和超参数共同控制，表现为KL散度不连续的临界边界，区分了有意义的潜在推断与坍缩。

Conclusion: 后验坍缩是数据结构与变分约束相互作用产生的相变现象，为理解深度生成模型的训练能力和表征能力提供了新见解。

Abstract: We investigate the phenomenon of posterior collapse in variational
autoencoders (VAEs) from the perspective of statistical physics, and reveal
that it constitutes a phase transition governed jointly by data structure and
model hyper-parameters. By analyzing the stability of the trivial solution
associated with posterior collapse, we identify a critical hyper-parameter
threshold. This critical boundary, separating meaningful latent inference from
collapse, is characterized by a discontinuity in the KL divergence between the
approximate posterior and the prior distribution. We validate this critical
behavior on both synthetic and real-world datasets, confirming the existence of
a phase transition. Our results demonstrate that posterior collapse is not
merely an optimization failure, but rather an emerging phase transition arising
from the interplay between data structure and variational constraints. This
perspective offers new insights into the trainability and representational
capacity of deep generative models.

</details>


### [240] [Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead](https://arxiv.org/abs/2510.01624)
*Feiyang Kang,Michael Kuchnik,Karthik Padthe,Marin Vlastelica,Ruoxi Jia,Carole-Jean Wu,Newsha Ardalani*

Main category: cs.LG

TL;DR: 研究发现，大语言模型（LLMs）的SFT高分并不能可靠地预测RL后的性能提升，有时甚至导致更差的结果。泛化损失和Pass@large k是预测RL效果的更强指标。


<details>
  <summary>Details</summary>
Motivation: 挑战当前LLM推理能力后训练中，高SFT分数是否能转化为RL后性能提升的普遍假设。

Method: 训练了数百个高达12B参数的LLM（包括Llama3, Mistral-Nemo, Qwen3），通过SFT和基于GRPO的RLVR。在7个数学基准上进行广泛评估（超过1M GPU小时，256次重复）。研究了泛化损失和Pass@large k等替代指标。

Result: 发现高SFT分数可能偏向于更简单或同质的数据，不能可靠预测后续RL收益。有时，在SFT改进模型上进行RL训练可能导致比未SFT基线模型更差的结果。泛化损失和Pass@large k被确定为RL结果的强大代理，相较于直接预测，其预测精度显著提高（R^2和Spearman's相关系数提升高达0.5）。例如，SFT训练独特示例一周期不如半数示例两周期，且仅训练短示例的SFT表现可能导致RL后效果更差。

Conclusion: LLM的SFT高分并不能保证RL后的性能提升，甚至可能产生负面影响。泛化损失和Pass@large k是更可靠的指标，用于预测LLM后训练的有效性，具有广泛的应用价值。

Abstract: In post-training for reasoning Large Language Models (LLMs), the current
state of practice trains LLMs in two independent stages: Supervised Fine-Tuning
(SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as
``RL'' below). In this work, we challenge whether high SFT scores translate to
improved performance after RL. We provide extensive counter-examples where this
is not true. We find high SFT scores can be biased toward simpler or more
homogeneous data and are not reliably predictive of subsequent RL gains or
scaled-up post-training effectiveness. In some cases, RL training on models
with improved SFT performance could lead to substantially worse outcome
compared to RL on the base model without SFT. We study alternative metrics and
identify generalization loss on held-out reasoning examples and Pass@large k
performance to provide strong proxies for the RL outcome. We trained hundreds
of models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive
evaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU
hours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple
state-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL
performance, prediction based on generalization loss and Pass@large k achieves
substantial higher precision, improving $R^2$ coefficient and Spearman's rank
correlation coefficient by up to 0.5 (2x). This provides strong utility for
broad use cases. For example, in most experiments, we find SFT training on
unique examples for a one epoch underperforms training on half examples for two
epochs, either after SFT or SFT-then-RL; With the same SFT budget, training
only on short examples may lead to better SFT performance, though, it often
leads to worse outcome after RL compared to training on examples with varying
lengths. Evaluation tool will be open-sourced.

</details>


### [241] [Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls](https://arxiv.org/abs/2510.01631)
*Feiyang Kang,Newsha Ardalani,Michael Kuchnik,Youssef Emad,Mostafa Elhoushi,Shubhabrata Sengupta,Shang-Wen Li,Ramya Raghavendra,Ruoxi Jia,Carole-Jean Wu*

Main category: cs.LG

TL;DR: 本研究通过大规模实验发现，合成数据，特别是重构文本，与自然数据混合使用时，能显著加速大型语言模型预训练（可达5-10倍），但单独使用效果有限；而纯生成教科书式数据可能导致模型性能下降。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据供应有限，限制了大型语言模型（LLM）的扩展，因此探索合成数据作为替代方案。

Method: 进行大规模实证研究（超过1000个LLM，100k+ GPU小时），采用统一协议和缩放法则，比较自然网页数据、多种合成数据（重构文本、生成式教科书）以及自然与合成数据的混合。

Result: 单独使用重构合成数据预训练并不比自然网页文本快；然而，1/3重构合成数据与2/3自然网页文本混合，在大型数据集预算下可将预训练速度提高5-10倍。单独使用教科书式合成数据导致下游领域损失显著更高。合成数据在训练数据混合中的“良好”比例（对重构合成数据而言约为30%）取决于模型大小和数据预算。大型生成器模型不一定比约8B参数模型产生更好的预训练数据。关于“模型崩溃”存在混合证据：重构合成数据在可预见规模内未显示性能下降，而混合教科书式纯生成合成数据则显示出“模型崩溃”预测的模式。

Conclusion: 合成数据在预训练中具有有条件的益处，可提供实际指导，并有助于揭示其在预训练中的作用。

Abstract: Training data plays a crucial role in Large Language Models (LLM) scaling,
yet high quality data is of limited supply. Synthetic data techniques offer a
potential path toward sidestepping these limitations. We conduct a large-scale
empirical investigation (>1000 LLMs with >100k GPU hours) using a unified
protocol and scaling laws, comparing natural web data, diverse synthetic types
(rephrased text, generated textbooks), and mixtures of natural and synthetic
data. Specifically, we found pre-training on rephrased synthetic data
\textit{alone} is not faster than pre-training on natural web texts; while
pre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts
can speed up 5-10x (to reach the same validation loss) at larger data budgets.
Pre-training on textbook-style synthetic data \textit{alone} results in notably
higher loss on many downstream domains especially at small data budgets. "Good"
ratios of synthetic data in training data mixtures depend on the model size and
data budget, empirically converging to ~30% for rephrased synthetic data.
Larger generator models do not necessarily yield better pre-training data than
~8B-param models. These results contribute mixed evidence on "model collapse"
during large-scale single-round (n=1) model training on synthetic
data--training on rephrased synthetic data shows no degradation in performance
in foreseeable scales whereas training on mixtures of textbook-style
pure-generated synthetic data shows patterns predicted by "model collapse". Our
work demystifies synthetic data in pre-training, validates its conditional
benefits, and offers practical guidance.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [242] [MMGaP: Multi-User MIMO Detection and Precoding using GPU-assisted Physics-inspired Computation](https://arxiv.org/abs/2510.01579)
*Abhishek Kumar Singh,Kyle Jamieson*

Main category: cs.NI

TL;DR: MMGaP是一种GPU加速的5G MIMO处理方案，通过在通用GPU上高效实现大型MIMO算法，显著提升了下一代蜂窝网络的吞吐量，并满足实时性要求，弥合了理论进展与实际实现之间的差距。


<details>
  <summary>Details</summary>
Motivation: 下一代蜂窝网络中，基于物理学和量子计算的物理层处理方法在频谱效率上取得了理论进展，但在通用处理器上难以实际实现，导致实际系统吞吐量与理论预期之间存在差距。

Method: 本文提出了MMGaP，一种针对下一代蜂窝网络的上行多用户MIMO检测器和下行向量扰动预编码器。该方法首次在裸机CUDA内核上实现这些大型MIMO处理算法，可扩展至大型GPU处理平台，并可封装为TensorFlow模块。通过与NVIDIA的软件定义GPU加速5G平台集成进行性能评估。

Result: 在采用100 MHz带宽、8天线基站和8并发用户的5G网络中，MMGaP将每用户上行吞吐量提高约50 Mbps，下行吞吐量提高约100 Mbps。对于16天线基站和16并发用户的更大MIMO规模，MMGaP也能提供每用户超过50 Mbps的更高上行吞吐量。在NVIDIA GPU上的执行时间测量表明，MMGaP能以线速运行并满足现有5G系统的时间要求。

Conclusion: MMGaP成功地将先进的MIMO处理算法在通用GPU上实现，有效弥合了理论研究与实际系统之间的差距，显著提升了5G蜂窝网络的吞吐量和效率，并满足了其实时操作要求。

Abstract: Physics-inspired and quantum compute based methods for processing in the
physical layer of next-generation cellular radio access networks have
demonstrated theoretical advances in spectral efficiency in recent years, but
have stopped short of practical realization on commodity processors, leaving a
gap between the throughput practical systems can achieve and the projected
throughput the state-of-the-art should achieve. To fill this gap, this paper
proposes MMGaP, an uplink multi-user MIMO detector and downlink Vector
perturbation precoder for next-generation cellular networks. MMGaP realizes
these large MIMO processing algorithms for the first time on bare-metal CUDA
kernels that scale to run on large GPU processing platforms, and can be
packaged as TensorFlow modules, allowing easy integration with a variety of
systems. We integrate MMGaP with NVIDIA's software-defined, GPU-accelerated 5G
platform and evaluate its performance against the state-of-the-art. In a 5G
cellular network using 100 MHz of radio bandwidth, eight antennas at the base
station and eight concurrent users, we show that MMGaP improves uplink
throughput by approximately 50 Mbps per user and downlink throughput by 100
Mbps per user over a wide range of SNR. We further show that MMGaP can also
support larger MIMO sizes: for 16 antennas at the base station and 16
concurrent users, MMGaP provides more than 50 Mbps higher uplink throughput per
user. We measure the execution time of MMGaP on different NVIDIA GPUs and show
that it can operate at line-rate and meet the timing requirements of
state-of-the-art 5G systems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [243] [Utilizing Modern Large Language Models (LLM) for Financial Trend Analysis and Digest Creation](https://arxiv.org/abs/2510.01225)
*Andrei Lazarev,Dmitrii Sedov*

Main category: cs.CE

TL;DR: 本论文提出了一个利用大型语言模型（Google Gemini Pro）自动化生成金融摘要的创新框架，旨在帮助研究人员和专业人士高效获取信息并掌握新兴趋势。


<details>
  <summary>Details</summary>
Motivation: 信息呈指数级增长，给研究人员和专业人士带来了巨大挑战，难以高效掌握所在领域的最新发展。传统分析方法在处理大量非结构化数据方面存在局限性。

Method: 该框架利用OpenAlex的数据提取、策略性提示工程和大型语言模型（Gemini Pro）进行分析。具体方法包括：数据获取、JSON构建、与Gemini交互以及自动化生成PDF报告。项目代码在GitHub上开源。

Result: 研究展示了如何自动化生成全面的金融摘要，这些摘要能够总结关键发现、识别新兴趋势，并以易于消化的格式提供可操作的见解。此方法有效处理了大量非结构化数据，克服了传统分析方法的局限性。

Conclusion: 该研究成功展示了大型语言模型如何帮助研究人员和学者节省时间，及时了解当前趋势，从而提高信息获取效率并保持信息同步。

Abstract: The exponential growth of information presents a significant challenge for
researchers and professionals seeking to remain at the forefront of their
fields and this paper introduces an innovative framework for automatically
generating insightful financial digests using the power of Large Language
Models (LLMs), specifically Google's Gemini Pro. By leveraging a combination of
data extraction from OpenAlex, strategic prompt engineering, and LLM-driven
analysis, we demonstrate the automated example of creating a comprehensive
digests that generalize key findings, identify emerging trends. This approach
addresses the limitations of traditional analysis methods, enabling the
efficient processing of vast amounts of unstructured data and the delivery of
actionable insights in an easily digestible format. This paper describes how
LLMs work in simple words and how we can use their power to help researchers
and scholars save their time and stay informed about current trends. Our study
includes step-by-step process, from data acquisition and JSON construction to
interaction with Gemini and the automated generation of PDF reports, including
a link to the project's GitHub repository for broader accessibility and further
development.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [244] [An Anthropologist LLM to Elicit Users' Moral Preferences through Role-Play](https://arxiv.org/abs/2510.01189)
*Gianluca De Ninno,Paola Inverardi,Francesca Belotti*

Main category: cs.HC

TL;DR: 该研究结合沉浸式角色扮演游戏和LLM分析，探索了一种揭示用户道德决策的新方法，并证明LLM能有效理解用户道德偏好，有益于软件早期开发。


<details>
  <summary>Details</summary>
Motivation: 旨在通过捕捉个体行为中的道德偏好（软伦理），探索一种新颖方法来揭示用户的道德决策过程，以期在软件开发早期阶段更好地理解用户。

Method: 采用基于人类学方法的沉浸式角色扮演游戏，将参与者置于数字隐私领域的道德困境中。收集数据后，由定制的LLM（“GPT人类学家”）进行解释和分析。通过交叉验证过程对模型进行评估。

Result: 交叉验证结果表明，数据的丰富性和解释性框架显著增强了模型预测用户行为的能力。研究发现，LLM可以有效地自动化并增强对用户道德偏好和决策过程的理解。

Conclusion: 该研究证明了LLM可以被有效利用，以自动化和增强在软件开发早期阶段对用户道德偏好和决策过程的理解。

Abstract: This study investigates a novel approach to eliciting users' moral
decision-making by combining immersive roleplaying games with LLM analysis
capabilities. Building on the distinction introduced by Floridi between hard
ethics inspiring and shaping laws-and soft ethics-moral preferences guiding
individual behavior within the free space of decisions compliant to laws-we
focus on capturing the latter through contextrich, narrative-driven
interactions. Grounded in anthropological methods, the role-playing game
exposes participants to ethically charged scenarios in the domain of digital
privacy. Data collected during the sessions were interpreted by a customized
LLM ("GPT Anthropologist"). Evaluation through a cross-validation process shows
that both the richness of the data and the interpretive framing significantly
enhance the model's ability to predict user behavior. Results show that LLMs
can be effectively employed to automate and enhance the understanding of user
moral preferences and decision-making process in the early stages of software
development.

</details>


### [245] [LegiScout: A Visual Tool for Understanding Complex Legislation](https://arxiv.org/abs/2510.01195)
*Aadarsh Rajiv,Klaus Mueller*

Main category: cs.HC

TL;DR: LegiScout是一个交互式可视化系统，通过整合数据提取、NLP和计算机视觉技术，将复杂的立法框架的静态图表转换为动态的力导向图，以增强对现代法律复杂性的理解。


<details>
  <summary>Details</summary>
Motivation: 现代立法框架（如ACA）结构复杂，政府发布的图表通常静态、密集且难以理解，即使是专家也难以解读。

Method: 引入LegiScout系统，该系统利用数据提取、自然语言处理和计算机视觉技术，将静态政策图表转换为动态的力导向图，以增强理解并保留关键关系。

Result: LegiScout系统能够支持对ACA以及广泛立法和监管框架的深入探索，帮助利益相关者（政策制定者、分析师和公众）导航并理解现代法律的复杂性。

Conclusion: LegiScout提供了一种有效的方式，通过交互式可视化和多技术集成，使复杂的现代立法框架变得易于理解和探索。

Abstract: Modern legislative frameworks, such as the Affordable Care Act (ACA), often
involve complex webs of agencies, mandates, and interdependencies. Government
issued charts attempt to depict these structures but are typically static,
dense, and difficult to interpret - even for experts. We introduce LegiScout,
an interactive visualization system that transforms static policy diagrams into
dynamic, force-directed graphs, enhancing comprehension while preserving
essential relationships. By integrating data extraction, natural language
processing, and computer vision techniques, LegiScout supports deeper
exploration of not only the ACA but also a wide range of legislative and
regulatory frameworks. Our approach enables stakeholders - policymakers,
analysts, and the public - to navigate and understand the complexity inherent
in modern law.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [246] [Quantum-Assisted Correlation Clustering](https://arxiv.org/abs/2509.03561)
*Antonio Macaluso,Supreeth Mysore Venkatesh,Diego Arenas,Matthias Klusch,Andreas Dengel*

Main category: quant-ph

TL;DR: 本文提出一种混合量子-经典方法用于关联聚类，通过量子退火解决二次无约束二元优化问题，实现了对带符号图的递归分裂式划分，并在鲁棒性和聚类质量上优于经典算法。


<details>
  <summary>Details</summary>
Motivation: 在图基无监督学习任务中，寻求根据节点间的成对一致性和不一致性对图节点进行划分。目标是开发一种能处理任意关联结构（包括负边）、不依赖度量假设或预设聚类数量的聚类方法，尤其是在带符号图上最大化簇内一致性。

Method: 该工作将GCS-Q（一种量子辅助求解器）应用于关联聚类。它采用递归分裂式划分策略，将每个二分步骤编码为一个二次无约束二元优化（QUBO）问题，并通过量子退火求解。这种方法将量子优化集成到分层聚类框架中，以处理带符号图。

Result: 在合成带符号图和真实世界高光谱成像数据上的实证评估表明，经过关联聚类调整的GCS-Q在鲁棒性和聚类质量方面优于经典算法，尤其是在真实世界数据和簇大小不平衡的场景中表现更佳。

Conclusion: 研究结果强调了混合量子-经典优化在推进图基无监督学习中可扩展且结构感知聚类技术方面的潜力。

Abstract: This work introduces a hybrid quantum-classical method to correlation
clustering, a graph-based unsupervised learning task that seeks to partition
the nodes in a graph based on pairwise agreement and disagreement. In
particular, we adapt GCS-Q, a quantum-assisted solver originally designed for
coalition structure generation, to maximize intra-cluster agreement in signed
graphs through recursive divisive partitioning. The proposed method encodes
each bipartitioning step as a quadratic unconstrained binary optimization
problem, solved via quantum annealing. This integration of quantum optimization
within a hierarchical clustering framework enables handling of graphs with
arbitrary correlation structures, including negative edges, without relying on
metric assumptions or a predefined number of clusters. Empirical evaluations on
synthetic signed graphs and real-world hyperspectral imaging data demonstrate
that, when adapted for correlation clustering, GCS-Q outperforms classical
algorithms in robustness and clustering quality on real-world data and in
scenarios with cluster size imbalance. Our results highlight the promise of
hybrid quantum-classical optimization for advancing scalable and
structurally-aware clustering techniques in graph-based unsupervised learning.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [247] [Accuracy vs Performance: An abstraction model for deadline constrained offloading at the mobile-edge](https://arxiv.org/abs/2510.01885)
*Jamie Cotter,Ignacio Castineiras,Victor Cionca*

Main category: cs.DC

TL;DR: 提出了一种针对移动边缘设备上低延迟、受截止时间约束的深度神经网络(DNN)卸载的调度算法。


<details>
  <summary>Details</summary>
Motivation: 旨在解决移动边缘设备上深度神经网络(DNN)卸载中实现低延迟和满足严格截止时间约束的挑战。

Method: 设计了一个调度算法，该算法具有轻量级网络状态表示，并综合考虑设备可用性、网络通信、优先级感知抢占和任务截止时间。通过资源可用性表示、网络离散化和动态带宽估计机制来优化延迟。该算法在一个由四台树莓派2组成的废物分类系统上实现并与现有方法进行比较评估。

Result: 研究发现，新的低延迟抽象模型在高容量工作负载下表现出更好的性能。动态带宽估计机制有助于任务放置，并在资源稀缺时显著提高了任务吞吐量。

Conclusion: 所提出的调度算法及其创新的延迟抽象模型和动态带宽估计机制，能有效提升移动边缘设备上DNN卸载的性能，在高容量工作负载和资源受限场景下，显著降低延迟并增加任务吞吐量。

Abstract: In this paper, we present a solution for low-latency deadline-constrained DNN
offloading on mobile edge devices. We design a scheduling algorithm with
lightweight network state representation, considering device availability,
communication on the network link, priority-aware pre-emption, and task
deadlines. The scheduling algorithm aims to reduce latency by designing a
resource availability representation, as well as a network discretisation and a
dynamic bandwidth estimation mechanism. We implement the scheduling algorithm
into a system composed of four Raspberry Pi 2 (model Bs) mobile edge devices,
sampling a waste classification conveyor belt at a set frame rate. The system
is evaluated and compared to a previous approach of ours, which was proven to
outcompete work-stealers and a non-pre-emption based scheduling heuristic under
the aforementioned waste classification scenario. Our findings show the novel
lower latency abstraction models yield better performance under high-volume
workloads, with the dynamic bandwidth estimation assisting the task placement
while, ultimately, increasing task throughput in times of resource scarcity.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [248] [Mamba Outpaces Reformer in Stock Prediction with Sentiments from Top Ten LLMs](https://arxiv.org/abs/2510.01203)
*Lokesh Antony Kadiyala,Amir Mirzaeinia*

Main category: q-fin.ST

TL;DR: 本研究提出一种新颖框架，结合十大LLM的语义情感分数和分钟级股价数据，利用Reformer和Mamba模型，提高短期（分钟级）股价预测精度，并发现Mamba在性能和速度上均优于Reformer。


<details>
  <summary>Details</summary>
Motivation: 短期股票市场预测因高波动性、新闻影响和金融时间序列的非线性特性而极其困难。

Method: 构建了AAPL新闻文章和1分钟股价的时间对齐数据集。使用DeepSeek-V3、GPT变体、LLaMA、Claude、Gemini、Qwen和Mistral等十个大型语言模型进行情感分析，将分数标准化并与价格、技术指标（RSI, ROC, Bollinger Band Width）结合。然后，分别训练Reformer和Mamba两种先进模型，以每个LLM生成的情感分数作为输入，通过Optuna优化超参数，并在3天评估期内进行评估。

Result: Mamba在性能和速度上均优于Reformer，对于测试的每个LLM都是如此。Mamba与LLaMA 3.3--70B结合时表现最佳，误差最低为0.137。Reformer虽然能捕捉数据中的广泛趋势，但似乎过度平滑了LLM引起的突然变化。

Conclusion: 本研究强调了集成基于LLM的语义分析与高效时间建模在增强实时金融预测方面的潜力。

Abstract: The stock market is extremely difficult to predict in the short term due to
high market volatility, changes caused by news, and the non-linear nature of
the financial time series. This research proposes a novel framework for
improving minute-level prediction accuracy using semantic sentiment scores from
top ten different large language models (LLMs) combined with minute interval
intraday stock price data. We systematically constructed a time-aligned dataset
of AAPL news articles and 1-minute Apple Inc. (AAPL) stock prices for the dates
of April 4 to May 2, 2025. The sentiment analysis was achieved using the
DeepSeek-V3, GPT variants, LLaMA, Claude, Gemini, Qwen, and Mistral models
through their APIs. Each article obtained sentiment scores from all ten LLMs,
which were scaled to a [0, 1] range and combined with prices and technical
indicators like RSI, ROC, and Bollinger Band Width. Two state-of-the-art such
as Reformer and Mamba were trained separately on the dataset using the
sentiment scores produced by each LLM as input. Hyper parameters were optimized
by means of Optuna and were evaluated through a 3-day evaluation period.
Reformer had mean squared error (MSE) or the evaluation metrics, and it should
be noted that Mamba performed not only faster but also better than Reformer for
every LLM across the 10 LLMs tested. Mamba performed best with LLaMA 3.3--70B,
with the lowest error of 0.137. While Reformer could capture broader trends
within the data, the model appeared to over smooth sudden changes by the LLMs.
This study highlights the potential of integrating LLM-based semantic analysis
paired with efficient temporal modeling to enhance real-time financial
forecasting.

</details>
