<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 60]
- [cs.CV](#cs.CV) [Total: 66]
- [cs.AI](#cs.AI) [Total: 48]
- [cs.LG](#cs.LG) [Total: 66]
- [cs.NI](#cs.NI) [Total: 8]
- [eess.SY](#eess.SY) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs](https://arxiv.org/abs/2509.18113)
*Xin Hu,Yue Kang,Guanzi Yao,Tianze Kang,Mengjie Wang,Heyao Liu*

Main category: cs.CL

TL;DR: 针对大型语言模型在多任务和跨领域泛化能力不足的问题，提出一种动态提示调度机制，显著提升模型性能、稳定性和迁移能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多任务和跨领域设置下存在泛化能力限制，且现有方法（如SPoT）依赖固定的提示模板。

Method: 引入统一的多任务学习框架，核心是动态提示调度机制。该机制通过提示池和任务感知调度策略，动态组合并对齐不同任务的提示。在提示融合阶段，利用任务嵌入和门控机制精细控制提示信号，确保与任务需求对齐并构建灵活的任务间共享路径。优化目标侧重于联合多任务学习，并纳入自动学习调度权重的策略，以减轻任务干扰和负迁移。

Result: 敏感性实验证实了所提机制在保持模型稳定性和增强可迁移性方面的优势。实验结果表明，该提示调度方法显著提升了模型在一系列语言理解和知识推理任务上的性能。

Conclusion: 所提出的动态提示调度机制在统一多任务建模和跨领域适应方面表现出良好的适用性和有效性，成功解决了大型语言模型的泛化难题。

Abstract: This study addresses the generalization limitations commonly observed in
large language models under multi-task and cross-domain settings. Unlike prior
methods such as SPoT, which depends on fixed prompt templates, our study
introduces a unified multi-task learning framework with dynamic prompt
scheduling mechanism. By introducing a prompt pool and a task-aware scheduling
strategy, the method dynamically combines and aligns prompts for different
tasks. This enhances the model's ability to capture semantic differences across
tasks. During prompt fusion, the model uses task embeddings and a gating
mechanism to finely control the prompt signals. This ensures alignment between
prompt content and task-specific demands. At the same time, it builds flexible
sharing pathways across tasks. In addition, the proposed optimization objective
centers on joint multi-task learning. It incorporates an automatic learning
strategy for scheduling weights, which effectively mitigates task interference
and negative transfer. To evaluate the effectiveness of the method, a series of
sensitivity experiments were conducted. These experiments examined the impact
of prompt temperature parameters and task number variation. The results confirm
the advantages of the proposed mechanism in maintaining model stability and
enhancing transferability. Experimental findings show that the prompt
scheduling method significantly improves performance on a range of language
understanding and knowledge reasoning tasks. These results fully demonstrate
its applicability and effectiveness in unified multi-task modeling and
cross-domain adaptation.

</details>


### [2] [GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models](https://arxiv.org/abs/2509.18122)
*Yue Zhang,Jiaxin Zhang,Qiuyu Ren,Tahsin Saffat,Xiaoxuan Liu,Zitong Yang,Banghua Zhu,Yi Ma*

Main category: cs.CL

TL;DR: 本文提出了GAUSS，一个用于评估大型语言模型（LLMs）数学能力的基准，涵盖十二个核心技能维度，旨在提供模型数学智能的细致且可解释的分析。


<details>
  <summary>Details</summary>
Motivation: 现有评估可能未能全面反映LLMs的潜在数学智能。需要一个更全面、细致且可解释的评估方法，以构建模型数学能力的综合画像，识别其真实优势和劣势。

Method: 引入了GAUSS基准，该基准将数学能力分为十二个核心技能维度，归属于知识与理解、问题解决与沟通、元技能与创造力三个领域。通过根据认知技能对问题进行分类并设计任务以隔离特定能力，GAUSS旨在构建模型数学能力的全面、细致且可解释的画像。

Result: 通过使用GAUSS基准，分析了GPT-5-thinking的技能概况，揭示了其优点和缺点，并展示了其与o4-mini-high模型的差异。这突出了多维度、基于技能评估的价值。

Conclusion: GAUSS基准提供了一种有价值的多维度、基于技能的评估方法，能够更忠实地反映LLMs潜在的数学智能，并为理解模型能力差异提供了深入见解。

Abstract: We introduce \textbf{GAUSS} (\textbf{G}eneral \textbf{A}ssessment of
\textbf{U}nderlying \textbf{S}tructured \textbf{S}kills in Mathematics), a
benchmark that evaluates LLMs' mathematical abilities across twelve core skill
dimensions, grouped into three domains: knowledge and understanding, problem
solving and communication, and meta-skills and creativity. By categorizing
problems according to cognitive skills and designing tasks that isolate
specific abilities, GAUSS constructs comprehensive, fine-grained, and
interpretable profiles of models' mathematical abilities. These profiles
faithfully represent their underlying mathematical intelligence. To exemplify
how to use the \textsc{GAUSS} benchmark, we have derived the skill profile of
\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its
differences relative to \textsc{o4-mini-high}, thereby underscoring the value
of multidimensional, skill-based evaluation.

</details>


### [3] [Event Causality Identification with Synthetic Control](https://arxiv.org/abs/2509.18156)
*Haoyu Wang,Fengze Liu,Jiayao Zhang,Dan Roth,Kyle Richardson*

Main category: cs.CL

TL;DR: 本文提出一种基于Rubin因果模型和合成控制方法识别文本中事件因果关系的新方法，通过生成“合成双胞胎”来模拟因果干预，解决了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的事件因果识别方法主要依赖语言模式和多跳关系推理，容易因非正式的因果表达和虚假图形推理导致错误识别。研究旨在更鲁棒地区分因果与相关性。

Method: 采用Rubin因果模型，将前一个事件视为“治疗”，后一个事件视为“结果”。鉴于文本领域无法实际干预，概念上寻找一个与主角具有相同历史但在“治疗”上存在干预的“双胞胎”。为解决匹配困难，使用合成控制方法，结合文本嵌入合成和反演技术，从相关历史数据中生成这样的“合成双胞胎”。

Result: 该方法在因果关系基准数据集COPES-hard上表现出比包括GPT-4在内的先前方法更强的因果关系识别鲁棒性。

Conclusion: 通过结合Rubin因果模型和合成控制方法，本文提出的新框架能够更准确、鲁棒地识别文本中的事件因果关系，优于现有先进技术。

Abstract: Event causality identification (ECI), a process that extracts causal
relations between events from text, is crucial for distinguishing causation
from correlation. Traditional approaches to ECI have primarily utilized
linguistic patterns and multi-hop relational inference, risking false causality
identification due to informal usage of causality and specious graphical
inference. In this paper, we adopt the Rubin Causal Model to identify event
causality: given two temporally ordered events, we see the first event as the
treatment and the second one as the observed outcome. Determining their
causality involves manipulating the treatment and estimating the resultant
change in the likelihood of the outcome. Given that it is only possible to
implement manipulation conceptually in the text domain, as a work-around, we
try to find a twin for the protagonist from existing corpora. This twin should
have identical life experiences with the protagonist before the treatment but
undergoes an intervention of treatment. However, the practical difficulty of
locating such a match limits its feasibility. Addressing this issue, we use the
synthetic control method to generate such a twin' from relevant historical
data, leveraging text embedding synthesis and inversion techniques. This
approach allows us to identify causal relations more robustly than previous
methods, including GPT-4, which is demonstrated on a causality benchmark,
COPES-hard.

</details>


### [4] [ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization](https://arxiv.org/abs/2509.18158)
*Seungyoun Yi,Minsoo Khang,Sungrae Park*

Main category: cs.CL

TL;DR: ZERA是一种新的自动提示优化（APO）框架，通过联合优化系统和用户提示，并利用结构化评估，以低开销快速生成高质量提示，在多LLM和任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有APO方法通常只关注用户提示，依赖非结构化反馈，且需要大量样本和长迭代周期，导致成本高昂且脆弱。

Method: 本文提出ZERA（Zero-init Instruction Evolving Refinement Agent）框架，通过有原则、低开销的精炼，联合优化系统和用户提示。ZERA使用八个可泛化标准和自动推断的权重对提示进行评分，并根据这些结构化评估来修改提示。

Result: 实验结果表明，ZERA在五种LLM和九个涵盖推理、摘要和代码生成的多元数据集上，相比强基线实现了持续改进。消融研究也突出了每个组件对有效提示构建的贡献。

Conclusion: ZERA通过其独特的联合提示优化和结构化评估机制，克服了传统APO的局限性，能够以更少的资源高效地生成高质量提示，显著提升LLM在各种任务上的性能。

Abstract: Automatic Prompt Optimization (APO) improves large language model (LLM)
performance by refining prompts for specific tasks. However, prior APO methods
typically focus only on user prompts, rely on unstructured feedback, and
require large sample sizes and long iteration cycles-making them costly and
brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a
novel framework that jointly optimizes both system and user prompts through
principled, low-overhead refinement. ZERA scores prompts using eight
generalizable criteria with automatically inferred weights, and revises prompts
based on these structured critiques. This enables fast convergence to
high-quality prompts using minimal examples and short iteration cycles. We
evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,
summarization, and code generation tasks. Experimental results demonstrate
consistent improvements over strong baselines. Further ablation studies
highlight the contribution of each component to more effective prompt
construction. Our implementation including all prompts is publicly available at
https://github.com/younatics/zera-agent.

</details>


### [5] [Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning](https://arxiv.org/abs/2509.18163)
*Haodong Zhao,Chenyan Zhao,Yansi Li,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: 本文发现大语言模型（LLMs）的“分步思考模式”是把双刃剑：有益信息提升准确性，但误导信息会导致性能灾难性下降，且思考过程会放大错误。挑战在于赋予模型评估信息的能力，而非仅仅使其思考。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂领域应用中推理能力至关重要，但实际应用常辅以可能有用、无关或误导的外部信息。本研究旨在调查此类辅助信息对具备显式分步思考能力的LLMs推理过程的因果影响。

Method: 引入了一个名为SciAux的新数据集（源自ScienceQA），系统地测试模型对不同类型辅助信息的鲁棒性。

Result: 研究发现，模型的“审慎思考模式”是一把双刃剑。有益上下文能提高准确性，但误导信息会导致性能灾难性下降，且这种下降在思考过程中被放大。思考非但未能增强鲁棒性，反而会加剧误导信息带来的错误程度。

Conclusion: 关键挑战不在于仅仅让模型“思考”，而在于赋予它们评估其推理所依据信息的批判性能力，以有效应对误导信息。

Abstract: The capacity of Large Language Models (LLMs) to reason is fundamental to
their application in complex, knowledge-intensive domains. In real-world
scenarios, LLMs are often augmented with external information that can be
helpful, irrelevant, or even misleading. This paper investigates the causal
impact of such auxiliary information on the reasoning process of LLMs with
explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset
derived from ScienceQA, to systematically test the robustness of the model
against these types of information. Our findings reveal a critical
vulnerability: the model's deliberative "thinking mode" is a double-edged
sword. While helpful context improves accuracy, misleading information causes a
catastrophic drop in performance, which is amplified by the thinking process.
Instead of conferring robustness, thinking reinforces the degree of error when
provided with misinformation. This highlights that the challenge is not merely
to make models "think", but to endow them with the critical faculty to evaluate
the information upon which their reasoning is based. The SciAux dataset is
available at https://huggingface.co/datasets/billhdzhao/SciAux.

</details>


### [6] [SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework](https://arxiv.org/abs/2509.18167)
*Junlin Wang,Zehao Wu,Shaowei Lu,Yanlan Li,Xinghao Huang*

Main category: cs.CL

TL;DR: 本文提出了一种过程监督的多智能体框架，通过引入决策者和知识选择器，并结合LLM作为法官进行过程级奖励监督和PPO训练，以优化RAG系统中检索器与生成器的协同作用，从而提高问答准确性、收敛稳定性和推理可解释性。


<details>
  <summary>Details</summary>
Motivation: RAG系统的效果受限于检索器和生成器之间的协同，但由于两者独立开发，交互往往不理想：检索器可能返回不相关或冗余文档，而生成器可能未能充分利用检索到的证据。因此，需要弥合检索器与生成器之间的鸿沟。

Method: 该研究提出了一个过程监督的多智能体框架。框架包含两个轻量级智能体：一个“决策者”决定何时继续检索或停止生成答案；一个“知识选择器”过滤检索到的文档以保留最有用证据。为提供细粒度监督，采用“LLM作为法官”来评估每个中间动作并提供过程级奖励。此外，采用树形展开策略探索多样推理路径，并使用近端策略优化（PPO）端到端训练两个智能体。该框架具有模块化和即插即用的特点，无需修改现有检索器或生成器。

Result: 在单跳和多跳问答基准测试中，该方法相比标准RAG基线取得了更高的准确性、更稳定的收敛性，并产生了更具可解释性的推理轨迹。

Conclusion: 该提出的多智能体框架有效地弥合了RAG中检索器与生成器之间的差距，显著提升了性能，并因其模块化和即插即用特性，在实际RAG应用中具有很高的实用价值。

Abstract: Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to
access external knowledge sources, but the effectiveness of RAG relies on the
coordination between the retriever and the generator. Since these components
are developed independently, their interaction is often suboptimal: the
retriever may return irrelevant or redundant documents, while the generator may
fail to fully leverage retrieved evidence. In this work, we propose a
process-supervised multi-agent framework to bridge the gap between retriever
and generator. The framework introduces two lightweight agents: a Decision
Maker, which determines when to continue retrieval or stop for answer
generation, and a Knowledge Selector, which filters retrieved documents to
retain only the most useful evidence. To provide fine-grained supervision, we
employ an LLM-as-a-Judge that evaluates each intermediate action with
process-level rewards, ensuring more accurate credit assignment than relying
solely on final answer correctness. We further adopt a tree-structured rollout
strategy to explore diverse reasoning paths, and train both agents with
Proximal Policy Optimization (PPO) in an end-to-end manner. Experiments on
single-hop and multi-hop question answering benchmarks show that our approach
achieves higher accuracy, more stable convergence, and produces more
interpretable reasoning trajectories compared with standard RAG baselines.
Importantly, the proposed framework is modular and plug-and-play, requiring no
modification to the retriever or generator, making it practical for real-world
RAG applications.

</details>


### [7] [ERFC: Happy Customers with Emotion Recognition and Forecasting in Conversation in Call Centers](https://arxiv.org/abs/2509.18175)
*Aditi Debsharma,Bhushan Jagyasi,Surajit Sen,Priyanka Pandey,Devicharith Dovari,Yuvaraj V. C,Rosalin Parida,Gopali Contractor*

Main category: cs.CL

TL;DR: 提出一种新颖的多模态对话情感识别与预测架构（ERFC），考虑上下文和说话者交互，旨在预测未来对话情感，已在IEMOCAP数据集上验证可行性，对呼叫中心等应用具有重要商业价值。


<details>
  <summary>Details</summary>
Motivation: 在呼叫中心等场景中，代理人若能预测客户未来话语中的情感，将能更好地提供服务、安抚客户、提高客户满意度，从而将不满意的客户转变为满意的客户。

Method: 提出一种新颖的“对话情感识别与预测 (Emotion Recognition and Forecasting in Conversation, ERFC)”架构。该架构综合考虑多模态信息、情感的不同属性、对话上下文以及对话中说话者话语间的相互依赖性，以预测未来的情感。

Result: 在IEMOCAP数据集上进行了大量实验，结果表明所提出的ERFC架构是可行的。

Conclusion: 该ERFC方法能够为呼叫中心等客户满意度至关重要的应用提供巨大的商业价值。

Abstract: Emotion Recognition in Conversation has been seen to be widely applicable in
call center analytics, opinion mining, finance, retail, healthcare, and other
industries. In a call center scenario, the role of the call center agent is not
just confined to receiving calls but to also provide good customer experience
by pacifying the frustration or anger of the customers. This can be achieved by
maintaining neutral and positive emotion from the agent. As in any
conversation, the emotion of one speaker is usually dependent on the emotion of
other speaker. Hence the positive emotion of an agent, accompanied with the
right resolution will help in enhancing customer experience. This can change an
unhappy customer to a happy one. Imparting the right resolution at right time
becomes easier if the agent has the insight of the emotion of future
utterances. To predict the emotions of the future utterances we propose a novel
architecture, Emotion Recognition and Forecasting in Conversation. Our proposed
ERFC architecture considers multi modalities, different attributes of emotion,
context and the interdependencies of the utterances of the speakers in the
conversation. Our intensive experiments on the IEMOCAP dataset have shown the
feasibility of the proposed ERFC. This approach can provide a tremendous
business value for the applications like call center, where the happiness of
customer is utmost important.

</details>


### [8] [Evaluating Large Language Models for Detecting Antisemitism](https://arxiv.org/abs/2509.18293)
*Jay Patel,Hrudayangam Mehta,Jeremy Blackburn*

Main category: cs.CL

TL;DR: 本文评估了八个开源LLM检测反犹太内容的能力，引入了新的Guided-CoT提示技术，显著提升了模型性能，并分析了LLM的误差、解释性和可靠性差异。


<details>
  <summary>Details</summary>
Motivation: 检测仇恨内容，尤其是反犹太内容，是一个重要且具有挑战性的问题，现有自动化工具需要持续训练以适应社交媒体的动态变化。

Method: 评估了八个开源大型语言模型（LLMs）检测反犹太内容的能力，将情境定义作为策略指导；探索了多种提示技术，并设计了一种新的CoT类提示——Guided-CoT；同时，检查了LLM的错误，并引入了量化模型生成理由语义差异的指标。

Result: Guided-CoT能很好地处理情境策略，提升了所有评估模型的性能，无论解码配置、模型大小或推理能力如何；Llama 3.1 70B在性能上超越了微调的GPT-3.5；LLM错误分析揭示了模型生成理由中显著的语义差异和矛盾行为。

Conclusion: 实验结果揭示了不同LLM在实用性、可解释性和可靠性方面的显著差异。

Abstract: Detecting hateful content is a challenging and important problem. Automated
tools, like machine-learning models, can help, but they require continuous
training to adapt to the ever-changing landscape of social media. In this work,
we evaluate eight open-source LLMs' capability to detect antisemitic content,
specifically leveraging in-context definition as a policy guideline. We explore
various prompting techniques and design a new CoT-like prompt, Guided-CoT.
Guided-CoT handles the in-context policy well, increasing performance across
all evaluated models, regardless of decoding configuration, model sizes, or
reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.
Additionally, we examine LLM errors and introduce metrics to quantify semantic
divergence in model-generated rationales, revealing notable differences and
paradoxical behaviors among LLMs. Our experiments highlight the differences
observed across LLMs' utility, explainability, and reliability.

</details>


### [9] [Exploiting Tree Structure for Credit Assignment in RL Training of LLMs](https://arxiv.org/abs/2509.18314)
*Hieu Tran,Zonghai Yao,Hong Yu*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement learning improves LLM reasoning, yet sparse delayed reward over
long sequences makes token-level credit assignment the key bottleneck. We study
the verifiable-reward setting, where the final answer is checkable and multiple
responses can be drawn per prompt. Reasoning tasks in math and medical QA align
with this setup, where only a few decision tokens significantly impact the
outcome. PPO offers token-level advantages with a learned value model, but it
is complex to train both the actor and critic models simultaneously, and it is
not easily generalizable, as the token-level values from the critic model can
make training prone to overfitting. GRPO is critic-free and supports verifiable
rewards, but spreads a single sequence-level return across tokens and ignores
branching. We introduce \textbf{Prefix-to-Tree (P2T)}, a simple procedure that
converts a group of responses into a prefix tree and computes
\emph{nonparametric} prefix values \(V(s)\) by aggregating descendant outcomes.
Built on P2T, we propose \textbf{TEMPO} (\emph{\textbf{T}ree-\textbf{E}stimated
\textbf{M}ean Prefix Value for \textbf{P}olicy \textbf{O}ptimization}), a
critic-free algorithm that augments the group-relative outcome signal of GRPO
with \emph{branch-gated} temporal-difference corrections derived from the tree.
At non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO
reduces to GRPO; at branching tokens, it supplies precise token-level credit
without a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,
TEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and
out-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and
reaches higher validation accuracy with roughly the same wall-clock time.

</details>


### [10] [Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning](https://arxiv.org/abs/2509.18316)
*Saksham Khatwani,He Cheng,Majid Afshar,Dmitriy Dligach,Yanjun Gao*

Main category: cs.CL

TL;DR: 本文提出将大型语言模型（LLM）作为知识图谱（KG）推理路径的奖励模型，用于判断诊断路径的正确性。研究发现特定奖励优化能带来强大的路径判断性能，但向下游诊断任务的泛化能力较弱。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）在诊断推理中缺乏可靠和知识驱动的推理能力。传统的知识图谱（KG）集成方法（如检索增强生成或微调）未能实现结构化推理。因此，需要探索一种新的范式，利用KG的结构化知识来提升LLM的诊断推理的可信度和准确性。

Method: 本研究探索了一种替代范式：将LLM视为知识图谱推理路径的奖励模型。LLM学习判断给定患者输入下，候选推理路径是否能导向正确的诊断。该方法受奖励训练和“验证方案比从头生成更容易”的计算理论启发，并类比医生评估诊断过程。具体方法包括：1. 系统评估五种知识路径判断任务表述和八种训练范式。2. 测试路径判断能力是否能泛化到下游诊断任务（如诊断总结和医学问答）。实验使用了三个开源的指令微调LLM。

Result: 实验结果显示，通过特定的奖励优化和蒸馏，模型在路径判断任务上表现出强大的性能。然而，这种能力向诊断总结和医学问答等下游任务的泛化（可迁移性）能力仍然较弱。

Conclusion: 本研究首次系统评估了“奖励模型风格”在临床知识图谱上的推理能力，提供了关于结构化、基于奖励的监督如何影响医疗领域生成式AI系统诊断推理的见解。尽管路径判断能力显著，但其在复杂下游诊断任务中的泛化能力仍需进一步提升。

Abstract: Large language models (LLMs) show promise for diagnostic reasoning but often
lack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as
the Unified Medical Language System (UMLS), offer structured biomedical
knowledge that can support trustworthy reasoning. Prior approaches typically
integrate KGs via retrieval augmented generation or fine tuning, inserting KG
content into prompts rather than enabling structured reasoning. We explore an
alternative paradigm: treating the LLM as a reward model of KG reasoning paths,
where the model learns to judge whether a candidate path leads to correct
diagnosis for a given patient input. This approach is inspired by recent work
that leverages reward training to enhance model reasoning abilities, and
grounded in computational theory, which suggests that verifying a solution is
often easier than generating one from scratch. It also parallels physicians'
diagnostic assessment, where they judge which sequences of findings and
intermediate conditions most plausibly support a diagnosis. We first
systematically evaluate five task formulation for knowledge path judging and
eight training paradigm. Second, we test whether the path judging abilities
generalize to downstream diagnostic tasks, including diagnosis summarization
and medical question answering. Experiments with three open source
instruct-tuned LLMs reveal both promise and brittleness: while specific reward
optimization and distillation lead to strong path-judging performance, the
transferability to downstream tasks remain weak. Our finding provides the first
systematic assessment of "reward model style" reasoning over clinical KGs,
offering insights into how structured, reward-based supervision influences
diagnostic reasoning in GenAI systems for healthcare.

</details>


### [11] [Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding](https://arxiv.org/abs/2509.18344)
*Pei-Shuo Wang,Jian-Jia Chen,Chun-Che Yang,Chi-Chih Chang,Ning-Chi Huang,Mohamed S. Abdelfattah,Kai-Chiang Wu*

Main category: cs.CL

TL;DR: SubSpec是一种无损、免训练、即插即用的方法，通过从目标LLM的卸载部分生成量化替代层并共享GPU常驻层和KV缓存，构建高度对齐的草稿模型，从而显著加速参数卸载，实现LLM在消费级GPU上的快速部署，最高可达12.5倍加速。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的巨大模型尺寸限制了其在内存有限的消费级GPU上的部署。现有模型压缩会降低质量，参数卸载虽保持质量但推理缓慢。推测解码可加速卸载，但现有方法常需额外训练以对齐定制模型，或因草稿模型与目标模型对齐不足而加速有限。

Method: 提出SubSpec方法，它是一种无损、免训练、即插即用的参数卸载加速方案。该方法通过从目标LLM的卸载部分生成低比特量化的替代层来构建高度对齐的草稿模型。此外，SubSpec还共享GPU上剩余的常驻层和KV缓存，以进一步减少内存开销并增强对齐。

Result: SubSpec实现了高平均接受长度，在8GB显存限制下，为Qwen2.5 7B模型在MT-Bench上带来了9.1倍的加速。在24GB显存限制下，为Qwen2.5 32B模型在流行生成基准测试中平均带来了12.5倍的加速。

Conclusion: SubSpec通过其无损、免训练和高度对齐的草稿模型构建方式，有效解决了参数卸载带来的推理速度瓶颈，为LLMs在内存受限的消费级GPU上的高效部署提供了显著的加速和性能提升。

Abstract: The immense model sizes of large language models (LLMs) challenge deployment
on memory-limited consumer GPUs. Although model compression and parameter
offloading are common strategies to address memory limitations, compression can
degrade quality, and offloading maintains quality but suffers from slow
inference. Speculative decoding presents a promising avenue to accelerate
parameter offloading, utilizing a fast draft model to propose multiple draft
tokens, which are then verified by the target LLM in parallel with a single
forward pass. This method reduces the time-consuming data transfers in forward
passes that involve offloaded weight transfers. Existing methods often rely on
pretrained weights of the same family, but require additional training to align
with custom-trained models. Moreover, approaches that involve draft model
training usually yield only modest speedups. This limitation arises from
insufficient alignment with the target model, preventing higher token
acceptance lengths. To address these challenges and achieve greater speedups,
we propose SubSpec, a plug-and-play method to accelerate parameter offloading
that is lossless and training-free. SubSpec constructs a highly aligned draft
model by generating low-bit quantized substitute layers from offloaded target
LLM portions. Additionally, our method shares the remaining GPU-resident layers
and the KV-Cache, further reducing memory overhead and enhance alignment.
SubSpec achieves a high average acceptance length, delivering 9.1x speedup for
Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for
Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).

</details>


### [12] [Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents](https://arxiv.org/abs/2509.18360)
*Chutong Meng,Philipp Koehn*

Main category: cs.CL

TL;DR: Speech Vecalign是一种无需文本转录的并行语音文档对齐方法，它通过对齐语音段嵌入来生成更长、更鲁棒的语音对齐。该方法在英德语音对齐任务中取得了高质量结果，并显著提升了语音到语音翻译模型的性能，甚至超越了使用更多数据的SpeechMatrix模型。


<details>
  <summary>Details</summary>
Motivation: 现有语音对齐方法可能存在对齐长度不足、鲁棒性差或依赖昂贵的文本转录数据等局限性。本研究旨在开发一种不依赖文本转录、能够生成高质量、更长且更鲁棒的语音对齐方法，以有效利用大规模无标签并行语音数据，促进语音到语音翻译等任务的发展。

Method: 提出了Speech Vecalign方法，通过单调对齐语音段嵌入来实现并行语音文档对齐，且不依赖文本转录。该方法应用于VoxPopuli数据集中3,000小时的无标签英德并行语音文档，用于生成高质量对齐数据，并在此基础上训练英德语音到语音翻译模型。

Result: 相比基线Global Mining，Speech Vecalign生成了更长的语音对齐；相比Local Mining，它展现了更高的鲁棒性并产生更少噪声。在3,000小时的英德语音文档上生成了约1,000小时的高质量对齐。基于此训练的语音到语音翻译模型，英译德性能比Global Mining提升0.37 ASR-BLEU，德译英提升0.18 ASR-BLEU。尽管使用了少8倍的原始语音文档，其模型性能与SpeechMatrix模型相当或更优。

Conclusion: Speech Vecalign是一种高效、鲁棒且无需文本的并行语音对齐方法，它能够生成高质量的对齐数据，显著提升语音到语音翻译模型的性能，为利用大规模无标签语音数据进行跨语言语音处理提供了有效途径。

Abstract: We present Speech Vecalign, a parallel speech document alignment method that
monotonically aligns speech segment embeddings and does not depend on text
transcriptions. Compared to the baseline method Global Mining, a variant of
speech mining, Speech Vecalign produces longer speech-to-speech alignments. It
also demonstrates greater robustness than Local Mining, another speech mining
variant, as it produces less noise. We applied Speech Vecalign to 3,000 hours
of unlabeled parallel English-German (En-De) speech documents from VoxPopuli,
yielding about 1,000 hours of high-quality alignments. We then trained En-De
speech-to-speech translation models on the aligned data. Speech Vecalign
improves the En-to-De and De-to-En performance over Global Mining by 0.37 and
0.18 ASR-BLEU, respectively. Moreover, our models match or outperform
SpeechMatrix model performance, despite using 8 times fewer raw speech
documents.

</details>


### [13] [Interactive Real-Time Speaker Diarization Correction with Human Feedback](https://arxiv.org/abs/2509.18377)
*Xinlu He,Yiwen Guan,Badrivishal Paurana,Zilin Dai,Jacob Whitehill*

Main category: cs.CL

TL;DR: 一个LLM辅助的实时交互式说话人日志校正系统，通过整合用户反馈和创新技术，显著提升了说话人日志的准确性。


<details>
  <summary>Details</summary>
Motivation: 大多数自动语音处理系统缺乏用户反馈，导致准确性受限；引入人机协作流程有望大幅提高准确性。

Method: 提出一个LLM辅助的实时说话人日志校正系统。该系统执行流式ASR和日志，使用LLM向用户提供简洁摘要，并接受即时口头反馈。核心技术包括：1) 分裂-合并（SWM）技术，用于检测并分离ASR错误归因的单说话人多说话人片段；2) 基于用户校正进行在线说话人注册，以预防未来错误。

Result: 在AMI测试集上的LLM驱动模拟显示，该系统将DER（说话人错误率）降低了9.92%，说话人混淆错误降低了44.23%。研究还分析了不同设置下（如摘要/完整转录显示、在线注册数量限制、校正频率）的校正效果。

Conclusion: 该LLM辅助的实时交互式系统通过整合用户反馈和专用技术，能够有效且大幅度地提升说话人日志的准确性，尤其适用于需要高精度人机协作的场景。

Abstract: Most automatic speech processing systems operate in "open loop" mode without
user feedback about who said what; yet, human-in-the-loop workflows can
potentially enable higher accuracy. We propose an LLM-assisted speaker
diarization correction system that lets users fix speaker attribution errors in
real time. The pipeline performs streaming ASR and diarization, uses an LLM to
deliver concise summaries to the users, and accepts brief verbal feedback that
is immediately incorporated without disrupting interactions. Moreover, we
develop techniques to make the workflow more effective: First, a
split-when-merged (SWM) technique detects and splits multi-speaker segments
that the ASR erroneously attributes to just a single speaker. Second, online
speaker enrollments are collected based on users' diarization corrections, thus
helping to prevent speaker diarization errors from occurring in the future.
LLM-driven simulations on the AMI test set indicate that our system
substantially reduces DER by 9.92% and speaker confusion error by 44.23%. We
further analyze correction efficacy under different settings, including summary
vs full transcript display, the number of online enrollments limitation, and
correction frequency.

</details>


### [14] [NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery](https://arxiv.org/abs/2509.18395)
*Minki Hong,Jangho Choi,Jihie Kim*

Main category: cs.CL

TL;DR: 本文提出了NormGenesis，一个多文化框架，用于生成和标注跨英语、中文和韩语的社会化对话，引入了违规-解决（V2R）对话类型和基于示例的迭代优化。它构建了一个包含10,800个对话的数据集，并证明了其在对话自然度、泛化能力和语用能力上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 对话系统需要生成不仅连贯而且符合社会规范的响应。现有的系统可能难以在多文化背景下实现语用一致性和社会可接受性，尤其是在处理规范违反的动态交互时。

Method: 1. 提出了NormGenesis，一个用于生成和标注跨英语、中文、韩语社会化对话的多文化框架。 2. 引入了“违规-解决（V2R）”对话类型，以建模社会规范违反后的识别和修复过程。 3. 在对话合成早期实施了基于示例的迭代优化，以提高欠代表语言的语用一致性，确保符合语言、情感和社会文化期望。

Result: 1. 构建了一个包含10,800个多轮对话的数据集，并在轮次级别标注了规范遵守、说话者意图和情感响应。 2. 人工和LLM评估表明，NormGenesis在精炼质量、对话自然度和泛化性能方面显著优于现有数据集。 3. 经过V2R增强数据训练的模型在伦理敏感情境中表现出更高的语用能力。

Conclusion: 这项工作为文化适应性对话建模建立了新的基准，并为跨语言和文化多样性的规范感知生成提供了一种可扩展的方法论。

Abstract: Social norms govern culturally appropriate behavior in communication,
enabling dialogue systems to produce responses that are not only coherent but
also socially acceptable. We present NormGenesis, a multicultural framework for
generating and annotating socially grounded dialogues across English, Chinese,
and Korean. To model the dynamics of social interaction beyond static norm
classification, we propose a novel dialogue type, Violation-to-Resolution
(V2R), which models the progression of conversations following norm violations
through recognition and socially appropriate repair. To improve pragmatic
consistency in underrepresented languages, we implement an exemplar-based
iterative refinement early in the dialogue synthesis process. This design
introduces alignment with linguistic, emotional, and sociocultural expectations
before full dialogue generation begins. Using this framework, we construct a
dataset of 10,800 multi-turn dialogues annotated at the turn level for norm
adherence, speaker intent, and emotional response. Human and LLM-based
evaluations demonstrate that NormGenesis significantly outperforms existing
datasets in refinement quality, dialogue naturalness, and generalization
performance. We show that models trained on our V2R-augmented data exhibit
improved pragmatic competence in ethically sensitive contexts. Our work
establishes a new benchmark for culturally adaptive dialogue modeling and
provides a scalable methodology for norm-aware generation across linguistically
and culturally diverse languages.

</details>


### [15] [Evaluating the Creativity of LLMs in Persian Literary Text Generation](https://arxiv.org/abs/2509.18401)
*Armin Tourajmehr,Mohammad Reza Modarres,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLMs）生成富含文化元素的波斯语文学文本的能力，通过构建波斯语数据集、改编托兰斯创造性思维测试，并使用LLM作为自动评判，揭示了其在波斯语文学创作中的优势与局限。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注英语LLM文学创作，对非英语文学传统（特别是波斯语）的探索有限，且缺乏标准化创造力评估方法。

Method: ['构建了一个包含20个主题的用户生成波斯语文学文本数据集。', '改编托兰斯创造性思维测试（TTCT），从独创性、流畅性、灵活性、精细化四个维度评估模型输出。', '采用LLM作为自动评分评判，并通过组内相关系数（ICC）验证其与人类判断的可靠性，结果显示高度一致。', '分析模型理解和运用四种核心文学修辞（明喻、暗喻、夸张、对偶）的能力。']

Result: ['成功验证了LLM作为自动评分评判的可靠性，其与人类判断表现出高度一致性。', '揭示了LLMs在生成波斯语文学文本方面的优势和局限性。']

Conclusion: LLMs在波斯语文学创作能力上仍需进一步完善和改进。

Abstract: Large language models (LLMs) have demonstrated notable creative abilities in
generating literary texts, including poetry and short stories. However, prior
research has primarily centered on English, with limited exploration of
non-English literary traditions and without standardized methods for assessing
creativity. In this paper, we evaluate the capacity of LLMs to generate Persian
literary text enriched with culturally relevant expressions. We build a dataset
of user-generated Persian literary spanning 20 diverse topics and assess model
outputs along four creativity dimensions-originality, fluency, flexibility, and
elaboration-by adapting the Torrance Tests of Creative Thinking. To reduce
evaluation costs, we adopt an LLM as a judge for automated scoring and validate
its reliability against human judgments using intraclass correlation
coefficients, observing strong agreement. In addition, we analyze the models'
ability to understand and employ four core literary devices: simile, metaphor,
hyperbole, and antithesis. Our results highlight both the strengths and
limitations of LLMs in Persian literary text generation, underscoring the need
for further refinement.

</details>


### [16] [Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations](https://arxiv.org/abs/2509.18439)
*Oscar J. Ponce-Ponte,David Toro-Tobon,Luis F. Figueroa,Michael Gionfriddo,Megan Branda,Victor M. Montori,Saturnino Luz,Juan P. Brito*

Main category: cs.CL

TL;DR: 本研究开发了一种基于语言模型和会话对齐（CA）评分的自动化方法，用于大规模测量医患共享决策（SDM），并发现CA评分与既定SDM结果量表相关。


<details>
  <summary>Details</summary>
Motivation: 共享决策（SDM）是实现以患者为中心护理的必要条件。目前缺乏可自动、大规模测量SDM的方法。

Method: 研究转录了157例医患对话（共42,559句话），使用上下文-响应对和负采样训练了深度学习（DL）模型和微调的BERT模型，以执行下一句预测（NSP）任务。利用表现最佳的模型计算四种会话对齐（CA）评分。通过随机效应分析，调整年龄、性别、种族和试验组等因素后，评估CA评分与SDM结果（决策冲突量表DCS和决策中患者参与观察量表OPTION12）的关联。使用Benjamini-Hochberg方法校正多重比较的p值。

Result: 微调后的BERTbase模型（110M）达到了最高的recall@1 (0.640)。DL模型生成的AbsMax CA评分 (p=0.025) 和 Max CA评分 (p=0.012) 与OPTION12相关。微调BERTbase（110M）模型生成的Max CA评分与DCS评分相关 (p=0.037)。BERT模型大小对CA评分与SDM的关联没有影响。

Conclusion: 本研究引入了一种自动、可扩展的方法，通过可解释的会话对齐（CA）评分来测量医患对话中的SDM，这对于大规模评估SDM策略具有潜力。

Abstract: Shared decision-making (SDM) is necessary to achieve patient-centred care.
Currently no methodology exists to automatically measure SDM at scale. This
study aimed to develop an automated approach to measure SDM by using language
modelling and the conversational alignment (CA) score. A total of 157
video-recorded patient-doctor conversations from a randomized multi-centre
trial evaluating SDM decision aids for anticoagulation in atrial fibrillations
were transcribed and segmented into 42,559 sentences. Context-response pairs
and negative sampling were employed to train deep learning (DL) models and
fine-tuned BERT models via the next sentence prediction (NSP) task. Each
top-performing model was used to calculate four types of CA scores. A
random-effects analysis by clinician, adjusting for age, sex, race, and trial
arm, assessed the association between CA scores and SDM outcomes: the
Decisional Conflict Scale (DCS) and the Observing Patient Involvement in
Decision-Making 12 (OPTION12) scores. p-values were corrected for multiple
comparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,
mean age 70 SD 10.8), clinicians on average spoke more words than patients
(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1
of 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1
with 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)
scores generated with the DL without stylebook were associated with OPTION12.
The Max CA score generated with the fine-tuned BERTbase (110M) was associated
with the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an
impact the association between CA scores and SDM. This study introduces an
automated, scalable methodology to measure SDM in patient-doctor conversations
through explainable CA scores, with potential to evaluate SDM strategies at
scale.

</details>


### [17] [CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density](https://arxiv.org/abs/2509.18458)
*Daniel Kaiser,Arnoldo Frigessi,Ali Ramezani-Kebrya,Benjamin Ricaud*

Main category: cs.CL

TL;DR: CogniLoad是一个基于认知负荷理论的新型基准，用于诊断LLM在长文本推理中因任务长度、内在复杂性和干扰物造成的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM长文本推理基准模糊了内在任务复杂性、干扰物干扰和任务长度等关键因素，导致难以进行精确的故障分析。

Method: 引入CogniLoad，一个基于认知负荷理论（CLT）的合成基准。它生成自然语言逻辑谜题，参数可独立调节：内在难度($d$)控制内在负荷，干扰物与信号比($ho$)调节无关负荷，任务长度($N$)作为核心负荷的代理。该基准用于评估22个最先进的推理LLM。

Result: CogniLoad揭示了LLM独特的性能敏感性，发现任务长度是主导限制因素，并揭示了对内在复杂性的不同容忍度以及对干扰物比率的U形响应。

Conclusion: CogniLoad通过对认知负荷维度进行系统、因子控制，提供了一个可复现、可扩展且诊断丰富的工具，用于剖析LLM的推理局限性并指导未来的模型开发。

Abstract: Current benchmarks for long-context reasoning in Large Language Models (LLMs)
often blur critical factors like intrinsic task complexity, distractor
interference, and task length. To enable more precise failure analysis, we
introduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load
Theory (CLT). CogniLoad generates natural-language logic puzzles with
independently tunable parameters that reflect CLT's core dimensions: intrinsic
difficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\rho$)
regulates extraneous load; and task length ($N$) serves as an operational proxy
for conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,
CogniLoad reveals distinct performance sensitivities, identifying task length
as a dominant constraint and uncovering varied tolerances to intrinsic
complexity and U-shaped responses to distractor ratios. By offering systematic,
factorial control over these cognitive load dimensions, CogniLoad provides a
reproducible, scalable, and diagnostically rich tool for dissecting LLM
reasoning limitations and guiding future model development.

</details>


### [18] [LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling](https://arxiv.org/abs/2509.18467)
*Zeyu Liu,Souvik Kundu,Lianghao Jiang,Anni Li,Srikanth Ronanki,Sravan Bodapati,Gourav Datta,Peter A. Beerel*

Main category: cs.CL

TL;DR: LAWCAT是一个新的线性化框架，能将预训练Transformer的能力高效迁移到高性能线性注意力架构，以解决长序列计算瓶颈，并实现长上下文边缘部署。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在长序列任务中面临二次计算复杂度瓶颈，而从头训练线性复杂度替代方案成本高昂。

Method: 提出LAWCAT (Linear Attention with Convolution Across Time)，通过集成因果Conv1D层增强局部依赖建模，并采用归一化门控线性注意力提高泛化能力，从而高效地将预训练Transformer的能力迁移到线性注意力架构中。

Result: LAWCAT在Mistral-7B上仅用1K长度序列蒸馏，即可实现高达22K tokens的90%以上密钥检索准确率。Llama3.2-1B LAWCAT变体在S-NIAH和BABILong基准上表现出色，所需预训练tokens不到0.1%。此外，在超过8K tokens的序列上，LAWCAT的预填充速度比FlashAttention-2更快。

Conclusion: LAWCAT为高性能、长上下文线性模型提供了一个高效途径，适用于边缘部署，并减少了对大量长序列训练数据和计算资源的依赖。

Abstract: Although transformer architectures have achieved state-of-the-art performance
across diverse domains, their quadratic computational complexity with respect
to sequence length remains a significant bottleneck, particularly for
latency-sensitive long-context applications. While recent linear-complexity
alternatives are increasingly powerful, effectively training them from scratch
is still resource-intensive. To overcome these limitations, we propose LAWCAT
(Linear Attention with Convolution Across Time), a novel linearization
framework designed to efficiently transfer the capabilities of pre-trained
transformers into a performant linear attention architecture. LAWCAT integrates
causal Conv1D layers to enhance local dependency modeling and employs
normalized gated linear attention to improve generalization across varying
context lengths. Our comprehensive evaluations demonstrate that, distilling
Mistral-7B with only 1K-length sequences yields over 90\% passkey retrieval
accuracy up to 22K tokens, significantly extending its effective context
window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance
on S-NIAH 1\&2\&3 tasks (1K-8K context length) and BABILong benchmark
(QA2\&QA3, 0K-16K context length), requiring less than 0.1\% pre-training
tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster
prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT
thus provides an efficient pathway to high-performance, long-context linear
models suitable for edge deployment, reducing reliance on extensive
long-sequence training data and computational resources.

</details>


### [19] [Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference](https://arxiv.org/abs/2509.18487)
*Ben Finkelshtein,Silviu Cucerzan,Sujay Kumar Jauhar,Ryen White*

Main category: cs.CL

TL;DR: 本文对大型语言模型（LLMs）在文本丰富图数据上的性能进行了大规模、受控评估，重点分析了不同交互模式（提示、工具使用、代码生成）及其对图结构、文本特征的依赖性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在文本丰富的图机器学习任务中应用日益增多，但在LLMs与图数据交互方面的能力，特别是其优势和局限性，仍缺乏系统性、原则性的理解。

Method: 研究采用大规模、受控评估方法，考察了多个关键变量轴：LLM-图交互模式（提示、工具使用、代码生成）；数据集领域（引用、网页链接、电商、社交网络）；图结构类型（同质图、异质图）；特征类型（短文本、长文本）；以及LLM模型配置（大小和推理能力）。此外，通过截断特征、删除边和移除标签来量化LLMs对不同输入类型的依赖性。

Result: (1) LLMs作为代码生成器在图数据上表现最佳，尤其在长文本或高密度图上优势显著。(2) 所有交互策略在异质图上均保持有效，挑战了LLM方法在低同质性下崩溃的假设。(3) 代码生成器能灵活调整对结构、特征或标签的依赖，以利用最具信息量的输入类型。

Conclusion: 研究全面揭示了当前LLM-图交互模式的优势与局限性，并为未来方法设计提供了关键原则和实用的指导。

Abstract: Large language models (LLMs) are increasingly used for text-rich graph
machine learning tasks such as node classification in high-impact domains like
fraud detection and recommendation systems. Yet, despite a surge of interest,
the field lacks a principled understanding of the capabilities of LLMs in their
interaction with graph data. In this work, we conduct a large-scale, controlled
evaluation across several key axes of variability to systematically assess the
strengths and weaknesses of LLM-based graph reasoning methods in text-based
applications. The axes include the LLM-graph interaction mode, comparing
prompting, tool-use, and code generation; dataset domains, spanning citation,
web-link, e-commerce, and social networks; structural regimes contrasting
homophilic and heterophilic graphs; feature characteristics involving both
short- and long-text node attributes; and model configurations with varying LLM
sizes and reasoning capabilities. We further analyze dependencies by
methodically truncating features, deleting edges, and removing labels to
quantify reliance on input types. Our findings provide practical and actionable
guidance. (1) LLMs as code generators achieve the strongest overall performance
on graph data, with especially large gains on long-text or high-degree graphs
where prompting quickly exceeds the token budget. (2) All interaction
strategies remain effective on heterophilic graphs, challenging the assumption
that LLM-based methods collapse under low homophily. (3) Code generation is
able to flexibly adapt its reliance between structure, features, or labels to
leverage the most informative input type. Together, these findings provide a
comprehensive view of the strengths and limitations of current LLM-graph
interaction modes and highlight key design principles for future approaches.

</details>


### [20] [A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition](https://arxiv.org/abs/2509.18514)
*Mohamad Elzohbi,Richard Zhao*

Main category: cs.CL

TL;DR: 本文提出一种基于ByT5的字节级多语言Transformer模型方法，用于在阿拉伯诗歌中插入符合特定韵律的短语，实现了高韵律对齐和语义连贯性。


<details>
  <summary>Details</summary>
Motivation: 解决在阿拉伯诗歌创作中，如何通过插入短语使其符合特定韵律的挑战。

Method: 使用ByT5模型，结合规则型字素到节拍转换来提取韵律。通过条件去噪目标微调ByT5，使模型重建遮蔽词汇以匹配目标韵律。采用课程学习策略（通用阿拉伯语数据集预训练后在诗歌数据集上微调），并探索了英-阿跨语言迁移。

Result: 实验结果表明，所提出的模型在保持语义连贯性的同时，实现了高韵律对齐。

Conclusion: 该模型具有在古典阿拉伯诗歌创作的协同创意应用中的潜力。

Abstract: This paper presents a methodology for inserting phrases in Arabic poems to
conform to a specific rhythm using ByT5, a byte-level multilingual
transformer-based model. Our work discusses a rule-based grapheme-to-beat
transformation tailored for extracting the rhythm from fully diacritized Arabic
script. Our approach employs a conditional denoising objective to fine-tune
ByT5, where the model reconstructs masked words to match a target rhythm. We
adopt a curriculum learning strategy, pre-training on a general Arabic dataset
before fine-tuning on poetic dataset, and explore cross-lingual transfer from
English to Arabic. Experimental results demonstrate that our models achieve
high rhythmic alignment while maintaining semantic coherence. The proposed
model has the potential to be used in co-creative applications in the process
of composing classical Arabic poems.

</details>


### [21] [Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector](https://arxiv.org/abs/2509.18535)
*Mo Mu,Dianqiao Lei,Chang Li*

Main category: cs.CL

TL;DR: 针对现有AI文本检测器面临的转述、偏差和修改文本等问题，本文提出了一种轻量级框架，通过分析文本的内部结构，结合句子嵌入、对比学习和因果图，有效检测原始及经修改的AI生成文本。


<details>
  <summary>Details</summary>
Motivation: ChatGPT的广泛应用引发了滥用担忧，亟需鲁棒的AI生成文本检测方法。然而，现有词级检测器易受转述或简单提示攻击，存在ChatGPT词级模式和训练数据内容引起的偏差，在修改文本上性能下降，并且常需大型模型或在线LLM交互。

Method: 引入新任务以检测原始及PSP修改的AI生成文本。提出了一个轻量级框架，通过文本的内部结构进行分类，该结构在词级变化下保持不变。具体方法包括：使用预训练语言模型编码句子嵌入并通过注意力机制建模它们的关系；采用对比学习减轻自回归生成带来的嵌入偏差；结合因果图与反事实方法从主题相关偏差中分离结构特征。

Result: 在两个精心策划的数据集（包括摘要比较和修订后的生活常见问题）上进行的实验，验证了所提出方法的有效性。

Conclusion: 提出了一种基于内部结构的轻量级AI生成文本检测框架，成功解决了现有检测器在面对转述、偏差和修改文本时的局限性，并能有效检测原始及经修改的AI文本。

Abstract: The widespread adoption of ChatGPT has raised concerns about its misuse,
highlighting the need for robust detection of AI-generated text. Current
word-level detectors are vulnerable to paraphrasing or simple prompts (PSP),
suffer from biases induced by ChatGPT's word-level patterns (CWP) and training
data content, degrade on modified text, and often require large models or
online LLM interaction. To tackle these issues, we introduce a novel task to
detect both original and PSP-modified AI-generated texts, and propose a
lightweight framework that classifies texts based on their internal structure,
which remains invariant under word-level changes. Our approach encodes sentence
embeddings from pre-trained language models and models their relationships via
attention. We employ contrastive learning to mitigate embedding biases from
autoregressive generation and incorporate a causal graph with counterfactual
methods to isolate structural features from topic-related biases. Experiments
on two curated datasets, including abstract comparisons and revised life FAQs,
validate the effectiveness of our method.

</details>


### [22] [CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs](https://arxiv.org/abs/2509.18536)
*Jin Young Kim,Ji Won Yoon*

Main category: cs.CL

TL;DR: CCQA是一种基于循环一致性的新型推理方法，通过生成问题并评估与原始问题的相似度来改进小型语言模型（SLMs）的推理能力，且在多个基准测试中优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有的推理策略虽然提升了大型语言模型（LLMs）的准确性，但在小型语言模型（SLMs）上效果不佳，传统方法常无法提升SLMs的性能。

Method: 提出了CCQA方法。它从每个推理路径和答案生成一个问题，通过评估生成问题与原始问题的相似度来选择最优解。为解决SLM生成问题困难，采用轻量级Flan-T5模型辅助问题生成。

Result: CCQA在数学和常识推理基准测试中，对八个模型均始终优于现有SOTA方法，并为SLMs中的高效推理建立了新的实用基线。

Conclusion: CCQA是一种能有效应用于小型语言模型的新型推理方法，显著提升了其推理性能，并为SLMs的高效推理设定了新的基线。

Abstract: Recently, inference-time reasoning strategies have further improved the
accuracy of large language models (LLMs), but their effectiveness on smaller
models remains unclear. Based on the observation that conventional approaches
often fail to improve performance in this context, we propose
\textbf{C}ycle-\textbf{C}onsistency in \textbf{Q}uestion \textbf{A}nswering
(CCQA), a novel reasoning method that can be effectively applied to SLMs.
Inspired by cycle consistency, CCQA generates a question from each reasoning
path and answer, evaluates each by its similarity to the original question, and
then selects the candidate solution with the highest similarity score as the
final response. Since conventional SLMs struggle to generate accurate questions
from their own reasoning paths and answers, we employ a lightweight Flan-T5
model specialized for question generation to support this process efficiently.
From the experimental results, it is verified that CCQA consistently
outperforms existing state-of-the-art (SOTA) methods across eight models on
mathematical and commonsense reasoning benchmarks. Furthermore, our method
establishes a new practical baseline for efficient reasoning in SLMs. Source
code can be found at https://github.com/scai-research/ccqa_official.

</details>


### [23] [Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity](https://arxiv.org/abs/2509.18577)
*Yeongbin Seo,Gayoung Kim,Jaehyung Kim,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 提出一种基于词元先验的快速数据过滤方法，在大幅降低成本的同时，提升了LLM预训练数据的选择效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型预训练需要高效的数据选择，而现有的基于困惑度（PPL）的过滤方法成本高且在处理噪声数据时不可靠。

Method: 受语言学洞察启发，提出一种基于词元先验的数据过滤方法。该方法利用语料库级别的词频统计来估计词元先验，并根据词元先验的均值和标准差过滤文档，无需模型推理。

Result: 在20个下游基准测试中取得了最高的平均性能，相比PPL过滤方法，时间成本降低了1000倍以上。同时，该方法适用于代码、数学等符号语言，并能动态适应多语言语料库，无需监督。

Conclusion: 基于词元先验的数据过滤方法是一个简单但强大的替代方案，能够显著提高数据选择的效率和效果，且适用范围广。

Abstract: As large language models (LLMs) are pretrained on massive web corpora,
careful selection of data becomes essential to ensure effective and efficient
learning. While perplexity (PPL)-based filtering has shown strong performance,
it suffers from drawbacks: substantial time costs and inherent unreliability of
the model when handling noisy or out-of-distribution samples. In this work, we
propose a simple yet powerful alternative: a prior-based data filtering method
that estimates token priors using corpus-level term frequency statistics,
inspired by linguistic insights on word roles and lexical density. Our approach
filters documents based on the mean and standard deviation of token priors,
serving as a fast proxy to PPL while requiring no model inference. Despite its
simplicity, the prior-based filter achieves the highest average performance
across 20 downstream benchmarks, while reducing time cost by over 1000x
compared to PPL-based filtering. We further demonstrate its applicability to
symbolic languages such as code and math, and its dynamic adaptability to
multilingual corpora without supervision

</details>


### [24] [TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning](https://arxiv.org/abs/2509.18585)
*Yu Chen,Yifei Han,Long Zhang,Yue Du,Bin Li*

Main category: cs.CL

TL;DR: TsqLoRA提出了一种结合数据质量采样和层敏感度感知的低秩适应方法，以提升大模型微调的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练模型的全参数微调计算和内存成本高昂，而现有参数高效微调方法未能充分考虑模型层敏感性和训练数据的重要性。

Method: 本文提出了TsqLoRA，包含两个核心组件：一是质量感知采样机制，用于选择信息量最大的训练数据；二是动态秩分配模块，根据各层对参数更新的敏感度调整其秩。

Result: 实验结果表明，TsqLoRA在多种NLP任务上显著提高了微调效率，同时保持甚至提升了模型性能。

Conclusion: TsqLoRA通过优化数据选择和秩分配，为大模型的参数高效微调提供了一种有效策略，实现了效率与性能的平衡。

Abstract: Fine-tuning large pre-trained models for downstream tasks has become a
fundamental approach in natural language processing. Fully fine-tuning all
model parameters is computationally expensive and memory-intensive, especially
in resource-constrained environments. Existing parameter-efficient fine-tuning
methods reduce the number of trainable parameters but typically overlook the
varying sensitivity of different model layers and the importance of training
data. In this work, we propose TsqLoRA, a novel method that integrates
data-quality-driven selection with sensitivity-aware low-rank adaptation,
consisted of two main components: a quality-aware sampling mechanism for
selecting the most informative training data, and a dynamic rank allocation
module that adjusts the rank of each layer based on its sensitivity to
parameter updates. The experimental results demonstrate that TsqLoRA improves
fine-tuning efficiency while maintaining or even improving performance on a
variety of NLP tasks. Our code will be available at
https://github.com/Benjamin-Ricky/TsqLoRA.

</details>


### [25] [UniECG: Understanding and Generating ECG in One Unified Model](https://arxiv.org/abs/2509.18588)
*Jiarui Jin,Haoyu Wang,Xiang Lan,Jun Li,Gaofeng Cheng,Hongyan Li,Shenda Hong*

Main category: cs.CL

TL;DR: UniECG是首个统一的ECG模型，能够同时进行基于证据的ECG解读和文本条件下的ECG生成，解决了现有通用模型在该领域的能力局限。


<details>
  <summary>Details</summary>
Motivation: 现有统一模型（如GPT-5）在视觉-语言任务上表现出色，但难以准确理解和生成ECG信号，也无法提供精确的医学诊断。

Method: 提出UniECG模型，采用解耦的两阶段训练方法。首先学习基于证据的ECG解读技能（ECG-to-Text），然后通过潜在空间对齐注入ECG生成能力（Text-to-ECG）。

Result: UniECG能够根据用户输入自主选择进行ECG解读或生成，显著扩展了当前ECG模型的能力边界。

Conclusion: UniECG成功地将ECG解读和生成能力整合到单一统一模型中，填补了现有通用模型在ECG领域的能力空白，为ECG分析带来了新的范式。

Abstract: Recent unified models such as GPT-5 have achieved encouraging progress on
vision-language tasks. However, these unified models typically fail to
correctly understand ECG signals and provide accurate medical diagnoses, nor
can they correctly generate ECG signals. To address these limitations, we
propose UniECG, the first unified model for ECG capable of concurrently
performing evidence-based ECG interpretation and text-conditioned ECG
generation tasks. Through a decoupled two-stage training approach, the model
first learns evidence-based interpretation skills (ECG-to-Text), and then
injects ECG generation capabilities (Text-to-ECG) via latent space alignment.
UniECG can autonomously choose to interpret or generate an ECG based on user
input, significantly extending the capability boundaries of current ECG models.
Our code and checkpoints will be made publicly available at
https://github.com/PKUDigitalHealth/UniECG upon acceptance.

</details>


### [26] [A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users](https://arxiv.org/abs/2509.18632)
*Nishant Balepur,Matthew Shu,Yoo Yeon Sung,Seraphina Goldfarb-Tarrant,Shi Feng,Fumeng Yang,Rachel Rudinger,Jordan Lee Boyd-Graber*

Main category: cs.CL

TL;DR: 本研究发现，LLM计划的现有对齐方法依赖用户偏好，但用户/模型偏好和代理成功率并不能准确预测计划对用户的实际帮助性（任务成功）。表面线索易影响偏好但不预测有效性，因此需要基于真实用户交互的反馈来对齐LLM。


<details>
  <summary>Details</summary>
Motivation: LLM生成的计划旨在辅助用户完成复杂任务。目前的对齐方法（如RLHF）主要依赖用户偏好进行训练或评估，但本文质疑这种偏好是否真正反映了计划对用户的实际帮助性。

Method: 开发了名为“Planorama”的界面，招募126名用户使用LLM计划回答300个多步骤问题。收集了4388次计划执行和5584次比较数据，以衡量计划的帮助性（QA成功率）和用户偏好。同时，在代理和奖励模型中复现此设置，观察它们是否能模拟或偏好对用户有益的计划。

Result: 1) 用户/模型偏好和代理成功率未能准确预测哪些计划真正帮助了用户，表明常见的对齐反馈可能与实际帮助性不符。2) 这种差距并非由用户特定偏好引起，用户在使用其偏好或不偏好的计划时，成功率相似。3) 简洁性、问题相似性等表面线索与偏好高度相关，但这些偏见无法预测计划的实际帮助性。

Conclusion: 为了对齐真正有帮助的LLM，需要从真实的用户交互（即任务成功）中获取反馈，而非仅仅依赖看起来有帮助的偏好。研究呼吁NLP研究者关注并解决这一问题。

Abstract: To assist users in complex tasks, LLMs generate plans: step-by-step
instructions towards a goal. While alignment methods aim to ensure LLM plans
are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer,
assuming this reflects what helps them. We test this with Planorama: an
interface where 126 users answer 300 multi-step questions with LLM plans. We
get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA
success) and user preferences on plans, and recreate the setup in agents and
reward models to see if they simulate or prefer what helps users. We expose: 1)
user/model preferences and agent success do not accurately predict which plans
help users, so common alignment feedback can misalign with helpfulness; 2) this
gap is not due to user-specific preferences, as users are similarly successful
when using plans they prefer/disprefer; 3) surface-level cues like brevity and
question similarity strongly link to preferences, but such biases fail to
predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from
real user interactions, not just preferences of what looks helpful, so we
discuss the plan NLP researchers can execute to solve this problem.

</details>


### [27] [Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering](https://arxiv.org/abs/2509.18655)
*Lingwen Deng,Yifei Han,Long Zhang,Yue Du,Bin Li*

Main category: cs.CL

TL;DR: 针对基于知识图谱的多跳问答（MHQA）中的参数保持知识编辑（PPKE）方法缺乏一致性问题，本文提出了CAPE-KG框架，通过确保KG的构建、更新和检索与MHQA任务对齐，显著提升了PPKE的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱（KG）的参数保持知识编辑（PPKE）方法在多跳问答（MHQA）中存在一致性问题，导致知识污染、更新不稳定以及检索行为与预期编辑不符，这严重影响了PPKE在多跳推理中的可靠性。

Method: 提出了CAPE-KG（Consistency-Aware Parameter-Preserving Editing with Knowledge Graphs），一个新颖的、一致性感知的PPKE框架。CAPE-KG旨在确保知识图谱的构建、更新和检索过程始终与MHQA任务的要求对齐，从而在未编辑和已编辑的知识上都能保持连贯的推理。

Result: 在MQuAKE基准测试上进行的大量实验表明，CAPE-KG显著提高了多跳问答任务中PPKE的准确性。

Conclusion: 解决PPKE中的一致性问题对于提高MHQA任务的PPKE性能是有效的，本文提出的CAPE-KG框架为此提供了一个有效的解决方案。

Abstract: Parameter-Preserving Knowledge Editing (PPKE) enables updating models with
new or corrected information without retraining or parameter adjustment. Recent
PPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE)
capabilities to multi-hop question answering (MHQA). However, these methods
often lack consistency, leading to knowledge contamination, unstable updates,
and retrieval behaviors that fail to reflect the intended edits. Such
inconsistencies undermine the reliability of PPKE in multi- hop reasoning. We
present CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge
Graphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures
KG construction, update, and retrieval are always aligned with the requirements
of the MHQA task, maintaining coherent reasoning over both unedited and edited
knowledge. Extensive experiments on the MQuAKE benchmark show accuracy
improvements in PPKE performance for MHQA, demonstrating the effectiveness of
addressing consistency in PPKE.

</details>


### [28] [Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction](https://arxiv.org/abs/2509.18658)
*Huanxin Sheng,Xinyi Liu,Hangfeng He,Jieyu Zhao,Jian Kang*

Main category: cs.CL

TL;DR: 针对LLM作为评估器（LLM-as-a-judge）的不确定性问题，本文首次提出一个基于共形预测的框架，提供LLM评分的预测区间，并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: LLM作为自然语言生成（NLG）的评估器是一个有前景的范式，但其评估结果的不确定性尚未得到充分探索。这种可靠性不足限制了LLM评估器在许多应用中的部署。

Method: 本文提出了一个首次利用共形预测（conformal prediction）为LLM评分提供预测区间的框架，以分析其不确定性。为离散评分任务设计了序数边界调整，并建议将区间中点作为一种低偏差的替代分数。

Result: 通过广泛的实验和分析，结果表明共形预测能够提供具有覆盖保证的有效预测区间。此外，研究还探讨了区间中点和裁判重新提示（judge reprompting）对于改善判断的有用性。

Conclusion: 该工作通过引入共形预测框架，成功解决了LLM评估器的不确定性问题，提供了具有保证的预测区间，并提出了更可靠的评分方法，增强了LLM评估的可靠性。

Abstract: LLM-as-a-judge has become a promising paradigm for using large language
models (LLMs) to evaluate natural language generation (NLG), but the
uncertainty of its evaluation remains underexplored. This lack of reliability
may limit its deployment in many applications. This work presents the first
framework to analyze the uncertainty by offering a prediction interval of
LLM-based scoring via conformal prediction. Conformal prediction constructs
continuous prediction intervals from a single evaluation run, and we design an
ordinal boundary adjustment for discrete rating tasks. We also suggest a
midpoint-based score within the interval as a low-bias alternative to raw model
score and weighted average. We perform extensive experiments and analysis,
which show that conformal prediction can provide valid prediction interval with
coverage guarantees. We also explore the usefulness of interval midpoint and
judge reprompting for better judgment.

</details>


### [29] [MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service](https://arxiv.org/abs/2509.18713)
*Yizhe Huang,Yang Liu,Ruiyu Zhao,Xiaolong Zhong,Xingming Yue,Ling Jiang*

Main category: cs.CL

TL;DR: 本文提出MemOrb，一个轻量级的口头强化记忆层，通过将多轮交互提炼为紧凑的策略反思，显著提高了基于LLM的客服代理的成功率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的代理在客服中常出现跨会话遗忘、重复错误及缺乏持续自我改进的问题，导致在动态环境中不可靠。

Method: 提出了MemOrb，一个轻量级、即插即用的口头强化记忆层。它将多轮交互提炼为紧凑的策略反思，存储在共享记忆库中，并在决策时检索以提供指导，无需微调。通过任务成功率和一致性指标（如Pass^k）进行评估。

Result: MemOrb显著提高了成功率（多轮成功率提升高达63个百分点）和稳定性，在重复试验中表现出更一致的性能。

Conclusion: 结构化反思是增强客服场景下“冻结”LLM代理长期可靠性的强大机制。

Abstract: Large Language Model-based agents(LLM-based agents) are increasingly deployed
in customer service, yet they often forget across sessions, repeat errors, and
lack mechanisms for continual self-improvement. This makes them unreliable in
dynamic settings where stability and consistency are critical. To better
evaluate these properties, we emphasize two indicators: task success rate as a
measure of overall effectiveness, and consistency metrics such as Pass$^k$ to
capture reliability across multiple trials. To address the limitations of
existing approaches, we propose MemOrb, a lightweight and plug-and-play verbal
reinforcement memory layer that distills multi-turn interactions into compact
strategy reflections. These reflections are stored in a shared memory bank and
retrieved to guide decision-making, without requiring any fine-tuning.
Experiments show that MemOrb significantly improves both success rate and
stability, achieving up to a 63 percentage-point gain in multi-turn success
rate and delivering more consistent performance across repeated trials. Our
results demonstrate that structured reflection is a powerful mechanism for
enhancing long-term reliability of frozen LLM agents in customer service
scenarios.

</details>


### [30] [LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR](https://arxiv.org/abs/2509.18722)
*Pattara Tipaksorn,Sumonmas Thatphithakkul,Vataya Chunwijitra,Kwanchiva Thangthai*

Main category: cs.CL

TL;DR: LOTUSDIS是一个公开可用的泰语远场会议语料库，包含114小时多距离、多麦克风录制的自发对话，旨在改进远场ASR。在该语料库上微调Whisper模型能显著提升远场语音识别的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有ASR模型在远场泰语语音识别上表现不佳，且预训练数据与真实远场泰语语音之间存在不匹配，导致模型性能随距离显著下降，因此需要距离多样化的训练数据来提升鲁棒性。

Method: 论文介绍了LOTUSDIS泰语会议语料库，包含114小时自发对话，由三个参与者在15-20分钟会话中完成，具有频繁的重叠语音。语音通过9个独立单通道设备在0.12米至10米不同距离、6种麦克风类型下同步录制，保留了混响、噪音等真实效果。提供了标准的数据划分和可复现的基线系统，并使用多种Whisper变体在零样本和微调条件下进行了基准测试。

Result: 现成模型在远距离语音识别上性能严重下降，证实了预训练数据与泰语远场语音之间的不匹配。在LOTUSDIS上进行微调后，Thai Whisper基线模型的整体词错误率（WER）从64.3%降至38.3%，远场WER从81.6%降至49.5%，尤其在最远距离麦克风上的改进最大。

Conclusion: 研究结果强调了距离多样化的训练数据对于构建鲁棒的自动语音识别（ASR）系统的重要性。LOTUSDIS语料库及其提供的训练和评估脚本将促进该领域的可复现研究。

Abstract: We present LOTUSDIS, a publicly available Thai meeting corpus designed to
advance far-field conversational ASR. The dataset comprises 114 hours of
spontaneous, unscripted dialogue collected in 15-20 minute sessions with three
participants, where overlapping speech is frequent and natural. Speech was
recorded simultaneously by nine independent single-channel devices spanning six
microphone types at distances from 0.12 m to 10 m, preserving the authentic
effects of reverberation, noise, and device coloration without relying on
microphone arrays. We provide standard train, dev, test splits and release a
reproducible baseline system. We benchmarked several Whisper variants under
zero-shot and fine-tuned conditions. Off-the-shelf models showed strong
degradation with distance, confirming a mismatch between pre-training data and
Thai far-field speech. Fine-tuning on LOTUSDIS dramatically improved
robustness: a Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and
far-field WER from 81.6 to 49.5, with especially large gains on the most
distant microphones. These results underscore the importance of
distance-diverse training data for robust ASR. The corpus is available under
CC-BY-SA 4.0. We also release training and evaluation scripts as a baseline
system to promote reproducible research in this field.

</details>


### [31] [Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models](https://arxiv.org/abs/2509.18742)
*Yunan Wang,Jianxin Li,Ziwei Zhang*

Main category: cs.CL

TL;DR: DyGRASP是一种新颖方法，结合LLM和时间GNN，高效处理动态文本属性图（DyTAGs）中的近期和全局时间语义，解决了现有方法在静态图上的局限性和LLM效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如GNNs和LLMs）主要关注静态文本属性图（TAGs），难以扩展到动态文本属性图（DyTAGs），因为它们忽略了近期-全局时间语义（交互文本的近期语义依赖和节点随时间的全局语义演变）。此外，将LLMs应用于DyTAGs中丰富且不断变化的文本面临效率问题。

Method: 提出Dynamic Global-Recent Adaptive Semantic Processing (DyGRASP) 方法，它结合LLMs和时间GNN来处理DyTAGs。具体包括：设计以节点为中心的隐式推理方法和滑动窗口机制以高效捕获近期时间语义；利用带有定制提示的显式推理和类RNN链式结构捕获节点的全局语义动态；通过更新和合并层，整合近期和全局时间语义以及动态图结构信息。

Result: DyGRASP在DyTAG基准测试中表现出卓越性能，在目标节点检索任务的Hit@10上实现了高达34%的提升。此外，DyGRASP在不同的时间GNN和LLM之间表现出强大的泛化能力。

Conclusion: DyGRASP通过有效捕获近期和全局时间语义，并解决LLM应用中的效率问题，成功地在动态文本属性图上进行高效推理。

Abstract: Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph
interactions and associated text attributes, are prevalent in real-world
applications. Existing methods, such as Graph Neural Networks (GNNs) and Large
Language Models (LLMs), mostly focus on static TAGs. Extending these existing
methods to DyTAGs is challenging as they largely neglect the recent-global
temporal semantics: the recent semantic dependencies among interaction texts
and the global semantic evolution of nodes over time. Furthermore, applying
LLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To
tackle these challenges, we propose Dynamic Global-Recent Adaptive Semantic
Processing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to
efficiently and effectively reason on DyTAGs. Specifically, we first design a
node-centric implicit reasoning method together with a sliding window mechanism
to efficiently capture recent temporal semantics. In addition, to capture
global semantic dynamics of nodes, we leverage explicit reasoning with tailored
prompts and an RNN-like chain structure to infer long-term semantics. Lastly,
we intricately integrate the recent and global temporal semantics as well as
the dynamic graph structural information using updating and merging layers.
Extensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority,
achieving up to 34% improvement in Hit@10 for destination node retrieval task.
Besides, DyGRASP exhibits strong generalization across different temporal GNNs
and LLMs.

</details>


### [32] [False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models](https://arxiv.org/abs/2509.18750)
*Julie Kallini,Dan Jurafsky,Christopher Potts,Martijn Bartelds*

Main category: cs.CL

TL;DR: 研究表明，多语言模型中的词元重叠有助于创建捕获跨语言语义关系的嵌入空间，从而提升跨语言迁移性能。


<details>
  <summary>Details</summary>
Motivation: 探究多语言语料库训练的子词分词器中，词元重叠是促进跨语言迁移还是引入干扰，因为现有研究证据不一且受设置和混杂因素影响。

Method: 设计受控实验，在系统化变化的词汇重叠设置下，训练多个语言对的双语自回归模型。引入共享词元语义相似性维度来分析重叠的影响。首先分析模型的隐藏表示，然后评估在XNLI和XQuAD数据集上的性能。

Result: 任何形式的词元重叠都能创建捕获跨语言语义关系的嵌入空间，而独立词汇模型的效果则弱得多。在XNLI和XQuAD上，重叠模型表现优于独立词汇模型，且迁移性能普遍随重叠程度的增加而提高。

Conclusion: 研究结果突显了词元重叠在多语言模型中的优势，并表明实质性的共享词汇仍然是多语言分词器的一种有益设计选择。

Abstract: Subword tokenizers trained on multilingual corpora naturally produce
overlapping tokens across languages. Does token overlap facilitate
cross-lingual transfer or instead introduce interference between languages?
Prior work offers mixed evidence, partly due to varied setups and confounders,
such as token frequency or subword segmentation granularity. To address this
question, we devise a controlled experiment where we train bilingual
autoregressive models on multiple language pairs under systematically varied
vocabulary overlap settings. Crucially, we explore a new dimension to
understanding how overlap affects transfer: the semantic similarity of tokens
shared across languages. We first analyze our models' hidden representations
and find that overlap of any kind creates embedding spaces that capture
cross-lingual semantic relationships, while this effect is much weaker in
models with disjoint vocabularies. On XNLI and XQuAD, we find that models with
overlap outperform models with disjoint vocabularies, and that transfer
performance generally improves as overlap increases. Overall, our findings
highlight the advantages of token overlap in multilingual models and show that
substantial shared vocabulary remains a beneficial design choice for
multilingual tokenizers.

</details>


### [33] [When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models](https://arxiv.org/abs/2509.18762)
*Yingming Zheng,Hanqi Li,Kai Yu,Lu Chen*

Main category: cs.CL

TL;DR: 研究发现长上下文SFT能反常地提升LLM在短上下文任务上的表现，并揭示了SFT数据长度对模型知识偏好的影响，提出混合训练可缓解此偏差。


<details>
  <summary>Details</summary>
Motivation: 现有研究已充分探讨持续预训练中数据长度对LLMs性能的影响，但监督微调（SFT）数据长度如何影响LLM在短上下文任务上的行为仍不明确。

Method: 系统性地研究SFT数据长度对LLM在短上下文任务上表现的影响。通过解耦并分析多头注意力（MHA）和前馈网络（FFN）这两个关键组件，并进一步研究它们的交互作用来揭示潜在机制。

Result: 1. 长上下文SFT出乎意料地提升了LLM在短上下文任务上的表现，这与长上下文预训练通常导致的性能下降相反。 2. MHA和FFN两个组件均独立受益于长上下文SFT。 3. 揭示了一种知识偏好偏差：长上下文SFT促进语境知识，而短上下文SFT偏向参数知识，这表明仅依赖长上下文SFT并非最优。

Conclusion: 混合训练可以有效缓解长上下文SFT和短上下文SFT之间的知识偏好偏差，为LLM的微调提供了可解释的指导。

Abstract: Large language models (LLMs) have achieved impressive performance across
natural language processing (NLP) tasks. As real-world applications
increasingly demand longer context windows, continued pretraining and
supervised fine-tuning (SFT) on long-context data has become a common approach.
While the effects of data length in continued pretraining have been extensively
studied, their implications for SFT remain unclear. In this work, we
systematically investigate how SFT data length influences LLM behavior on
short-context tasks. Counterintuitively, we find that long-context SFT improves
short-context performance, contrary to the commonly observed degradation from
long-context pretraining. To uncover the underlying mechanisms of this
phenomenon, we first decouple and analyze two key components, Multi-Head
Attention (MHA) and Feed-Forward Network (FFN), and show that both
independently benefit from long-context SFT. We further study their interaction
and reveal a knowledge preference bias: long-context SFT promotes contextual
knowledge, while short-context SFT favors parametric knowledge, making
exclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that
hybrid training mitigates this bias, offering explainable guidance for
fine-tuning LLMs.

</details>


### [34] [Financial Risk Relation Identification through Dual-view Adaptation](https://arxiv.org/abs/2509.18775)
*Wei-Ning Chiu,Yu-Hsiang Wang,Andy Hsiao,Yu-Shiang Huang,Chuan-Ju Wang*

Main category: cs.CL

TL;DR: 提出一种利用自然语言处理和10-K文件，系统地提取企业间风险关系并量化其关联度的方法。


<details>
  <summary>Details</summary>
Motivation: 识别企业间风险关联对投资组合管理和投资策略至关重要，而传统依赖专家判断和手动分析的方法主观、耗时且难以扩展。

Method: 开发了一种系统方法，以Form 10-K文件为数据源，利用自然语言处理技术（基于时间序和词汇模式的无监督微调）捕捉隐含和抽象的风险连接，构建领域专用金融编码器，并引入量化风险关联分数。

Result: 广泛的实验证明，该方法在多个评估设置中均优于强大的基线方法。

Conclusion: 该方法为企业间风险关系识别提供了一个系统、可解释且表现卓越的解决方案，克服了传统方法的局限性。

Abstract: A multitude of interconnected risk events -- ranging from regulatory changes
to geopolitical tensions -- can trigger ripple effects across firms.
Identifying inter-firm risk relations is thus crucial for applications like
portfolio management and investment strategy. Traditionally, such assessments
rely on expert judgment and manual analysis, which are, however, subjective,
labor-intensive, and difficult to scale. To address this, we propose a
systematic method for extracting inter-firm risk relations using Form 10-K
filings -- authoritative, standardized financial documents -- as our data
source. Leveraging recent advances in natural language processing, our approach
captures implicit and abstract risk connections through unsupervised
fine-tuning based on chronological and lexical patterns in the filings. This
enables the development of a domain-specific financial encoder with a deeper
contextual understanding and introduces a quantitative risk relation score for
transparency, interpretable analysis. Extensive experiments demonstrate that
our method outperforms strong baselines across multiple evaluation settings.

</details>


### [35] [AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field](https://arxiv.org/abs/2509.18776)
*Chen Liang,Zhaoqi Huang,Haofen Wang,Fu Chai,Chunying Yu,Huanhuan Wei,Zhengjie Liu,Yanpeng Li,Hongjun Wang,Ruifeng Luo,Xianzhong Zhao*

Main category: cs.CL

TL;DR: 本研究建立了AECBench基准测试，用于评估大型语言模型在建筑、工程和施工（AEC）领域的表现，发现LLMs在高级认知任务中存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在AEC领域的应用日益增多，但其在该专业且安全关键领域的鲁棒性和可靠性尚未得到充分评估。

Method: 本研究建立了AECBench，一个全面的基准测试，定义了23个代表性任务，涵盖知识记忆、理解、推理、计算和应用五个认知层面。构建了一个包含4,800个问题的多格式数据集，由工程师创建并经专家两轮评审验证。引入了“LLM充当评判者”的方法来评估复杂的长篇回复。

Result: 通过评估九个LLMs，研究发现其性能在五个认知层面呈明显下降趋势。模型在知识记忆和理解等基础任务上表现熟练，但在解释建筑规范中的表格知识、执行复杂推理和计算，以及生成特定领域文档方面显示出显著的性能缺陷。

Conclusion: 本研究为未来旨在将LLMs稳健可靠地集成到安全关键工程实践中的研究和开发奠定了基础。

Abstract: Large language models (LLMs), as a novel information technology, are seeing
increasing adoption in the Architecture, Engineering, and Construction (AEC)
field. They have shown their potential to streamline processes throughout the
building lifecycle. However, the robustness and reliability of LLMs in such a
specialized and safety-critical domain remain to be evaluated. To address this
challenge, this paper establishes AECBench, a comprehensive benchmark designed
to quantify the strengths and limitations of current LLMs in the AEC domain.
The benchmark defines 23 representative tasks within a five-level
cognition-oriented evaluation framework encompassing Knowledge Memorization,
Understanding, Reasoning, Calculation, and Application. These tasks were
derived from authentic AEC practice, with scope ranging from codes retrieval to
specialized documents generation. Subsequently, a 4,800-question dataset
encompassing diverse formats, including open-ended questions, was crafted
primarily by engineers and validated through a two-round expert review.
Furthermore, an LLM-as-a-Judge approach was introduced to provide a scalable
and consistent methodology for evaluating complex, long-form responses
leveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear
performance decline across five cognitive levels was revealed. Despite
demonstrating proficiency in foundational tasks at the Knowledge Memorization
and Understanding levels, the models showed significant performance deficits,
particularly in interpreting knowledge from tables in building codes, executing
complex reasoning and calculation, and generating domain-specific documents.
Consequently, this study lays the groundwork for future research and
development aimed at the robust and reliable integration of LLMs into
safety-critical engineering practices.

</details>


### [36] [Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing](https://arxiv.org/abs/2509.18792)
*Sabri Boughorbel,Fahim Dalvi,Nadir Durrani,Majd Hawasly*

Main category: cs.CL

TL;DR: 本文利用模型差异分析（model diffing）方法，深入剖析了Gemma-2-9b-it及其SimPO增强版本之间的具体能力差异，发现SimPO主要提升了安全性、多语言能力和指令遵循，并减少了模型自指和幻觉管理。


<details>
  <summary>Details</summary>
Motivation: 随着微调成为改进大型语言模型（LLMs）的主流范式，理解此过程中的变化至关重要。传统的基准测试往往无法解释模型性能差异的原因。

Method: 采用模型差异分析（一种机械可解释性方法），并利用crosscoders识别和分类区分两个模型的潜在表征，以分析Gemma-2-9b-it及其SimPO增强变体之间的特定能力差异。

Result: SimPO获得的潜在概念主要增强了安全机制（+32.8%）、多语言能力（+43.8%）和指令遵循（+151.7%），同时其额外的训练也降低了对模型自指（-44.1%）和幻觉管理（-68.5%）的重视。

Conclusion: 模型差异分析能够提供超越排行榜指标的细粒度洞察，将性能差距归因于具体的机械能力，为LLM的比较提供了一个透明且有针对性的框架。

Abstract: As fine-tuning becomes the dominant paradigm for improving large language
models (LLMs), understanding what changes during this process is increasingly
important. Traditional benchmarking often fails to explain why one model
outperforms another. In this work, we use model diffing, a mechanistic
interpretability approach, to analyze the specific capability differences
between Gemma-2-9b-it and a SimPO-enhanced variant. Using crosscoders, we
identify and categorize latent representations that differentiate the two
models. We find that SimPO acquired latent concepts predominantly enhance
safety mechanisms (+32.8%), multilingual capabilities (+43.8%), and
instruction-following (+151.7%), while its additional training also reduces
emphasis on model self-reference (-44.1%) and hallucination management
(-68.5%). Our analysis shows that model diffing can yield fine-grained insights
beyond leaderboard metrics, attributing performance gaps to concrete
mechanistic capabilities. This approach offers a transparent and targeted
framework for comparing LLMs.

</details>


### [37] [MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction](https://arxiv.org/abs/2509.18813)
*Liting Zhang,Shiwan Zhao,Aobo Kong,Qicheng Li*

Main category: cs.CL

TL;DR: MAPEX是一个多智能体协作框架，通过双路径策略（针对不同文档长度）显著提升了LLM在无监督关键词抽取任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督、基于提示词的LLM关键词抽取方法多采用单一推理流程和统一提示词，未能充分利用LLM能力，且无法有效应对复杂场景下关键词抽取的挑战。

Method: 提出MAPEX框架，首次将多智能体协作引入关键词抽取。MAPEX通过专家招募、候选抽取、主题引导、知识增强和后处理模块协调LLM智能体，并采用动态双路径策略：短文本知识驱动抽取，长文本主题引导抽取。

Result: 在六个基准数据集和三个LLM上的实验表明，MAPEX泛化性和通用性强，平均F1@5指标上超越最先进的无监督方法2.44%，超越标准LLM基线4.01%。

Conclusion: MAPEX通过多智能体协作和动态适应文档长度的策略，有效解决了LLM在无监督关键词抽取中的局限性，显著提升了性能。

Abstract: Keyphrase extraction is a fundamental task in natural language processing.
However, existing unsupervised prompt-based methods for Large Language Models
(LLMs) often rely on single-stage inference pipelines with uniform prompting,
regardless of document length or LLM backbone. Such one-size-fits-all designs
hinder the full exploitation of LLMs' reasoning and generation capabilities,
especially given the complexity of keyphrase extraction across diverse
scenarios. To address these challenges, we propose MAPEX, the first framework
that introduces multi-agent collaboration into keyphrase extraction. MAPEX
coordinates LLM-based agents through modules for expert recruitment, candidate
extraction, topic guidance, knowledge augmentation, and post-processing. A
dual-path strategy dynamically adapts to document length: knowledge-driven
extraction for short texts and topic-guided extraction for long texts.
Extensive experiments on six benchmark datasets across three different LLMs
demonstrate its strong generalization and universality, outperforming the
state-of-the-art unsupervised method by 2.44\% and standard LLM baselines by
4.01\% in F1@5 on average. Code is available at
https://github.com/NKU-LITI/MAPEX.

</details>


### [38] [Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?](https://arxiv.org/abs/2509.18843)
*Damian Stachura,Joanna Konieczna,Artur Nowak*

Main category: cs.CL

TL;DR: 研究比较了开放权重LLMs与专有LLMs在生物医学问答领域的表现，发现开放权重模型具有竞争力，特别是在集成策略下有时能超越专有模型。


<details>
  <summary>Details</summary>
Motivation: 随着开放权重LLMs的快速发展，其性能已接近专有LLMs，因此引发了小型开放权重LLMs是否能有效替代大型闭源模型的问题。本研究特别关注生物医学问答领域。

Method: 参与了BioASQ挑战赛的Task 13B Phase B，将多个开放权重模型与GPT-4o、GPT-4.1、Claude 3.5 Sonnet和Claude 3.7 Sonnet等顶级专有系统进行比较。采用的技术包括基于嵌入距离检索相关片段、上下文学习、结构化输出以及针对精确答案问题的集成方法。

Result: 研究结果表明，开放权重LLMs与专有模型具有可比性。在某些情况下，特别是应用集成策略时，开放权重LLMs甚至超越了它们的闭源对应模型。

Conclusion: 开放权重LLMs在生物医学问答领域能够与专有LLMs竞争，并通过集成策略等方法，在特定场景下表现出更优越的性能，证明了其替代大型闭源模型的潜力。

Abstract: Open-weight versions of large language models (LLMs) are rapidly advancing,
with state-of-the-art models like DeepSeek-V3 now performing comparably to
proprietary LLMs. This progression raises the question of whether small
open-weight LLMs are capable of effectively replacing larger closed-source
models. We are particularly interested in the context of biomedical
question-answering, a domain we explored by participating in Task 13B Phase B
of the BioASQ challenge. In this work, we compare several open-weight models
against top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and
Claude 3.7 Sonnet. To enhance question answering capabilities, we use various
techniques including retrieving the most relevant snippets based on embedding
distance, in-context learning, and structured outputs. For certain submissions,
we utilize ensemble approaches to leverage the diverse outputs generated by
different models for exact-answer questions. Our results demonstrate that
open-weight LLMs are comparable to proprietary ones. In some instances,
open-weight LLMs even surpassed their closed counterparts, particularly when
ensembling strategies were applied. All code is publicly available at
https://github.com/evidenceprime/BioASQ-13b.

</details>


### [39] [Multi-Hierarchical Feature Detection for Large Language Model Generated Text](https://arxiv.org/abs/2509.18862)
*Luyan Zhang,Xinyu Xie*

Main category: cs.CL

TL;DR: 研究发现，多特征集成在AI文本检测中仅带来微小性能提升（0.4-0.5%），但计算开销巨大（4.2倍），表明现代神经网络模型已能高效捕获大部分检测信号。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型发展，需严谨验证多特征方法能否显著超越单一神经网络模型来提升AI文本检测效果，因直觉认为结合语义、句法和统计特征应提供互补信号。

Method: 实现MHFD（多层次特征检测）方法，通过自适应融合，整合了基于DeBERTa的语义分析、句法分析和统计概率特征。

Result: 多特征集成仅带来0.4-0.5%的性能提升，却产生4.2倍的计算开销。MHFD在域内检测达到89.7%准确率，跨域检测保持84.2%稳定性能，较现有方法有0.4-2.6%的适度提升。

Conclusion: 尽管有理论预期，但多特征集成在AI文本检测中带来的性能提升不足以抵消其巨大的计算成本，暗示现代神经语言模型可能已高效地捕获了大多数相关的检测信号。

Abstract: With the rapid advancement of large language model technology, there is
growing interest in whether multi-feature approaches can significantly improve
AI text detection beyond what single neural models achieve. While intuition
suggests that combining semantic, syntactic, and statistical features should
provide complementary signals, this assumption has not been rigorously tested
with modern LLM-generated text. This paper provides a systematic empirical
investigation of multi-hierarchical feature integration for AI text detection,
specifically testing whether the computational overhead of combining multiple
feature types is justified by performance gains. We implement MHFD
(Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic
analysis, syntactic parsing, and statistical probability features through
adaptive fusion. Our investigation reveals important negative results: despite
theoretical expectations, multi-feature integration provides minimal benefits
(0.4-0.5% improvement) while incurring substantial computational costs (4.2x
overhead), suggesting that modern neural language models may already capture
most relevant detection signals efficiently. Experimental results on multiple
benchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in
in-domain detection and maintains 84.2% stable performance in cross-domain
detection, showing modest improvements of 0.4-2.6% over existing methods.

</details>


### [40] [Diversity Boosts AI-Generated Text Detection](https://arxiv.org/abs/2509.18880)
*Advik Raj Basani,Pin-Yu Chen*

Main category: cs.CL

TL;DR: DivEye是一个检测AI生成文本的新框架，利用文本中不可预测性的波动特性。它通过可解释的统计特征捕捉人类文本与LLM输出在词汇和结构不可预测性上的差异，性能优于现有零样本检测器，且具有鲁棒性、泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 为了打击LLM在教育、商业、新闻和社交媒体等领域被滥用（如散布虚假信息或欺骗），检测AI生成文本变得日益必要。现有检测器常依赖token级似然或不透明的黑盒分类器，但它们难以应对高质量生成文本，且缺乏可解释性。

Method: 提出了DivEye框架，利用基于“惊奇度”（surprisal）的特征来捕捉文本中不可预测性的波动。该方法基于人类创作文本在词汇和结构不可预测性上比LLM输出展现出更丰富的变异性的观察，通过一组可解释的统计特征来捕捉这一信号。

Result: DivEye在多项基准测试中，性能超越现有零样本检测器高达33.2%，并与微调基线方法表现相当。它对释义和对抗性攻击具有鲁棒性，在不同领域和模型之间泛化良好，并作为辅助信号时能将现有检测器的性能提升高达18.7%。

Conclusion: DivEye不仅能提供文本被标记原因的可解释性洞察，还指出“节奏性不可预测性”是LLM检测中一个强大且未被充分探索的信号。这表明该方法对于理解LLM生成内容的独特特征具有重要价值。

Abstract: Detecting AI-generated text is an increasing necessity to combat misuse of
LLMs in education, business compliance, journalism, and social media, where
synthetic fluency can mask misinformation or deception. While prior detectors
often rely on token-level likelihoods or opaque black-box classifiers, these
approaches struggle against high-quality generations and offer little
interpretability. In this work, we propose DivEye, a novel detection framework
that captures how unpredictability fluctuates across a text using
surprisal-based features. Motivated by the observation that human-authored text
exhibits richer variability in lexical and structural unpredictability than LLM
outputs, DivEye captures this signal through a set of interpretable statistical
features. Our method outperforms existing zero-shot detectors by up to 33.2%
and achieves competitive performance with fine-tuned baselines across multiple
benchmarks. DivEye is robust to paraphrasing and adversarial attacks,
generalizes well across domains and models, and improves the performance of
existing detectors by up to 18.7% when used as an auxiliary signal. Beyond
detection, DivEye provides interpretable insights into why a text is flagged,
pointing to rhythmic unpredictability as a powerful and underexplored signal
for LLM detection.

</details>


### [41] [Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass](https://arxiv.org/abs/2509.18901)
*Nicholas Popovič,Michael Färber*

Main category: cs.CL

TL;DR: 本文提出JEDI，一种仅基于编码器的架构，通过联合执行抽取式原子事实分解和可解释推理，无需生成式大语言模型，在NLI任务中实现了竞争性准确性与显著的鲁棒性提升，并利用合成理由进行训练。


<details>
  <summary>Details</summary>
Motivation: 自然语言推理(NLI)及其相关任务（如自动化事实核查）中，原子事实分解有助于提高可解释性和鲁棒性。然而，现有方法依赖于资源密集型生成式大语言模型(LLMs)来执行分解。

Method: 提出JEDI，一种仅基于编码器的架构，可在推理时无需生成式模型，联合执行抽取式原子事实分解和可解释推理。为促进训练，研究者生成了一个覆盖多个NLI基准的大型合成理由语料库。

Result: 实验结果表明，JEDI在分布内取得了有竞争力的准确性，并且在分布外和对抗性设置下，相比仅依赖抽取式理由监督的模型，显著提高了鲁棒性。

Conclusion: 研究发现，NLI中的可解释性和鲁棒性泛化能力可以通过仅基于编码器的架构和合成理由来实现。

Abstract: Recent works in Natural Language Inference (NLI) and related tasks, such as
automated fact-checking, employ atomic fact decomposition to enhance
interpretability and robustness. For this, existing methods rely on
resource-intensive generative large language models (LLMs) to perform
decomposition. We propose JEDI, an encoder-only architecture that jointly
performs extractive atomic fact decomposition and interpretable inference
without requiring generative models during inference. To facilitate training,
we produce a large corpus of synthetic rationales covering multiple NLI
benchmarks. Experimental results demonstrate that JEDI achieves competitive
accuracy in distribution and significantly improves robustness out of
distribution and in adversarial settings over models based solely on extractive
rationale supervision. Our findings show that interpretability and robust
generalization in NLI can be realized using encoder-only architectures and
synthetic rationales. Code and data available at https://jedi.nicpopovic.com

</details>


### [42] [DTW-Align: Bridging the Modality Gap in End-to-End Speech Translation with Dynamic Time Warping Alignment](https://arxiv.org/abs/2509.18987)
*Abderrahmane Issam,Yusuf Can Semerci,Jan Scholtes,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 本文提出了一种基于动态时间规整（DTW）的方法，用于在端到端语音翻译（E2E-ST）训练期间对语音和文本嵌入进行对齐，以弥合模态鸿沟，尤其在低资源环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 端到端语音翻译（E2E-ST）存在语音和文本模态间的差异。现有通过词或token级对齐的方法需要语言特异性工具，且并非所有语言都可用；而基于最近邻相似度搜索的嵌入对齐方法精度不足。

Method: 该研究采用动态时间规整（DTW）来对齐训练过程中的语音和文本嵌入。

Result: 实验证明该方法能有效弥合模态鸿沟，产生更准确的对齐，达到与现有工作相当的E2E-ST结果，同时显著加快速度。此外，在6个语言方向中的5个低资源设置下，该方法表现优于现有工作。

Conclusion: 所提出的基于DTW的对齐方法在E2E-ST中有效、准确且快速地弥合了语音和文本的模态鸿沟，并在低资源环境下展现出优越性。

Abstract: End-to-End Speech Translation (E2E-ST) is the task of translating source
speech directly into target text bypassing the intermediate transcription step.
The representation discrepancy between the speech and text modalities has
motivated research on what is known as bridging the modality gap.
State-of-the-art methods addressed this by aligning speech and text
representations on the word or token level. Unfortunately, this requires an
alignment tool that is not available for all languages. Although this issue has
been addressed by aligning speech and text embeddings using nearest-neighbor
similarity search, it does not lead to accurate alignments. In this work, we
adapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during
training. Our experiments demonstrate the effectiveness of our method in
bridging the modality gap in E2E-ST. Compared to previous work, our method
produces more accurate alignments and achieves comparable E2E-ST results while
being significantly faster. Furthermore, our method outperforms previous work
in low resource settings on 5 out of 6 language directions.

</details>


### [43] [Investigating Test-Time Scaling with Reranking for Machine Translation](https://arxiv.org/abs/2509.19020)
*Shaomu Tan,Ryosuke Mitani,Ritvik Choudhary,Toshiyuki Sekiya*

Main category: cs.CL

TL;DR: 本文对机器翻译（MT）领域的测试时缩放（TTS）策略进行了首次系统性研究，通过最佳N候选框架发现TTS能有效提升高资源语言的翻译质量，并探讨了模型大小、计算预算与性能之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前NLP系统主要通过扩大模型参数来提升性能，但这带来了巨大的计算成本。测试时缩放（TTS）作为一种在推理时分配更多计算的替代方案，在数学推理等任务中表现出色，但尚未在机器翻译（MT）领域进行系统性探索。

Method: 研究人员对MT的TTS进行了首次系统性研究，采用了一种简单实用的最佳N候选框架，并在WMT24基准上进行了实验。实验涵盖了六对高资源语言和一对低资源语言对、五种模型尺寸（3B-72B），以及多种TTS计算预算（N最高达1024）。

Result: a) 对于高资源语言，TTS普遍提升了翻译质量，并通过人工评估得到证实；b) 使用较大N值增强的小模型可以匹配甚至超越N=1的大模型，但计算成本更高；c) 在固定计算预算下，大模型通常更高效，且TTS在低资源情况下可能因评估指标盲区而降低质量。

Conclusion: TTS是提升机器翻译质量的有效策略，尤其适用于高资源语言。它提供了在模型大小、推理计算和性能之间进行权衡的可能性，但其效率和效果受语言资源水平和计算预算影响，大模型在固定预算下通常表现更优。

Abstract: Scaling model parameters has become the de facto strategy for improving NLP
systems, but it comes with substantial computational costs. Test-Time Scaling
(TTS) offers an alternative by allocating more computation at inference:
generating multiple candidates and selecting the best. While effective in tasks
such as mathematical reasoning, TTS has not been systematically explored for
machine translation (MT). In this paper, we present the first systematic study
of TTS for MT, investigating a simple but practical best-of-N framework on
WMT24 benchmarks. Our experiments cover six high-resource and one low-resource
language pairs, five model sizes (3B-72B), and various TTS compute budget (N up
to 1024). Our results show that a) For high-resource languages, TTS generally
improves translation quality according to multiple neural MT evaluation
metrics, and our human evaluation confirms these gains; b) Augmenting smaller
models with large $N$ can match or surpass larger models at $N{=}1$ with more
compute cost; c) Under fixed compute budgets, larger models are typically more
efficient, and TTS can degrade quality due to metric blind spots in
low-resource cases.

</details>


### [44] [Charting a Decade of Computational Linguistics in Italy: The CLiC-it Corpus](https://arxiv.org/abs/2509.19033)
*Chiara Alzetta,Serena Auriemma,Alessandro Bondielli,Luca Dini,Chiara Fazzone,Alessio Miaschi,Martina Miliani,Marta Sartor*

Main category: cs.CL

TL;DR: 通过分析CLiC-it会议论文集，追踪并呈现意大利计算语言学和自然语言处理社区在过去十年（2014-2024）的研究趋势和发展。


<details>
  <summary>Details</summary>
Motivation: Transformer-based大型语言模型（LLMs）的出现迅速推动了计算语言学（CL）和自然语言处理（NLP）领域的发展，改变了研究目标和优先事项。本研究旨在为意大利和国际研究社区提供有关该领域新兴趋势和关键发展的宝贵见解，以支持未来的明智决策和方向。

Method: 收集CLiC-it（意大利领先的CL/NLP会议）前10届（2014-2024）的会议论文集，构建“CLiC-it语料库”。对语料库的元数据（包括作者来源、性别、单位等）和论文内容进行综合分析。

Result: 研究结果是提供了一个关于CLiC-it语料库元数据和论文内容的全面分析，揭示了意大利CL和NLP社区十年来的新兴趋势和关键发展情况。

Conclusion: 该研究旨在为意大利和国际研究社区提供有价值的见解，以支持他们在CL和NLP领域的明智决策和未来发展方向。

Abstract: Over the past decade, Computational Linguistics (CL) and Natural Language
Processing (NLP) have evolved rapidly, especially with the advent of
Transformer-based Large Language Models (LLMs). This shift has transformed
research goals and priorities, from Lexical and Semantic Resources to Language
Modelling and Multimodality. In this study, we track the research trends of the
Italian CL and NLP community through an analysis of the contributions to
CLiC-it, arguably the leading Italian conference in the field. We compile the
proceedings from the first 10 editions of the CLiC-it conference (from 2014 to
2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its
metadata, including author provenance, gender, affiliations, and more, as well
as the content of the papers themselves, which address various topics. Our goal
is to provide the Italian and international research communities with valuable
insights into emerging trends and key developments over time, supporting
informed decisions and future directions in the field.

</details>


### [45] [Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering](https://arxiv.org/abs/2509.19094)
*Alireza Salemi,Cheng Li,Mingyang Zhang,Qiaozhu Mei,Zhuowan Li,Spurthi Amba Hombaiah,Weize Kong,Tao Chen,Hamed Zamani,Michael Bendersky*

Main category: cs.CL

TL;DR: 本文提出Pathways of Thoughts (PoT) 方法，在推理阶段增强大型语言模型(LLM)的个性化问答能力，通过模拟迭代认知过程并整合多样的推理路径，无需额外微调。


<details>
  <summary>Details</summary>
Motivation: 个性化问答对于满足用户特定信息需求至关重要，但由于从冗长、嘈杂和隐式语境中推断偏好以及生成准确、上下文合适且符合用户期望的回答面临挑战，目前研究相对不足。

Method: 本文提出Pathways of Thoughts (PoT)，一种推理阶段方法，适用于任何LLM，无需任务特定微调。该方法将LLM的推理建模为迭代决策过程，动态选择推理、修订、个性化和澄清等认知操作，以探索多个推理轨迹并生成多样化的候选响应。PoT随后根据推断的用户偏好聚合并重新加权这些候选响应，生成最终的个性化回答。

Result: 在LaMP-QA个性化问答基准测试中，PoT持续优于竞争基线，实现了高达13.1%的相对改进。人工评估也证实了这些结果，标注者在66%的案例中偏好PoT的输出，且仅有15%的案例为平局。

Conclusion: PoT是一种有效的推理阶段方法，通过模拟LLM的迭代认知过程和整合多样化的推理路径，显著提升了大型语言模型在个性化问答方面的性能和用户满意度。

Abstract: Personalization is essential for adapting question answering (QA) systems to
user-specific information needs, thereby improving both accuracy and user
satisfaction. However, personalized QA remains relatively underexplored due to
challenges such as inferring preferences from long, noisy, and implicit
contexts, and generating responses that are simultaneously correct,
contextually appropriate, and aligned with user expectations and background
knowledge. To address these challenges, we propose Pathways of Thoughts (PoT),
an inference-stage method that applies to any large language model (LLM)
without requiring task-specific fine-tuning. The approach models the reasoning
of an LLM as an iterative decision process, where the model dynamically selects
among cognitive operations such as reasoning, revision, personalization, and
clarification. This enables exploration of multiple reasoning trajectories,
producing diverse candidate responses that capture different perspectives. PoT
then aggregates and reweights these candidates according to inferred user
preferences, yielding a final personalized response that benefits from the
complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA
benchmark for personalized QA show that PoT consistently outperforms
competitive baselines, achieving up to a 13.1% relative improvement. Human
evaluation corroborates these results, with annotators preferring outputs from
PoT in 66% of cases and reporting ties in only 15% of cases.

</details>


### [46] [Are most sentences unique? An empirical examination of Chomskyan claims](https://arxiv.org/abs/2509.19108)
*Hiram Ring*

Main category: cs.CL

TL;DR: 本文通过语料库分析，实证调查了语言学中关于句子独特性的主张，发现虽然独一无二的句子通常占多数，但重复句的存在不容忽视且受语体限制。


<details>
  <summary>Details</summary>
Motivation: 语言学中普遍认为大多数语言表达是独一无二的（如乔姆斯基和平克的主张）。随着大型语料库的出现，这一主张现在可以通过实证方法进行检验。

Method: 使用NLTK Python库解析不同语体的语料库，并计算每个语料库中精确字符串匹配（即重复句子）的数量。

Result: 结果显示，虽然完全独特的句子在语料库中通常占多数，但这高度受语体限制，且重复句子在任何单个语料库中都占有不容忽视的比例。

Conclusion: 关于语言表达普遍独特的传统观点需要修正，独特性虽常见但并非绝对，重复现象显著且具有语体依赖性。

Abstract: A repeated claim in linguistics is that the majority of linguistic utterances
are unique. For example, Pinker (1994: 10), summarizing an argument by Noam
Chomsky, states that "virtually every sentence that a person utters or
understands is a brand-new combination of words, appearing for the first time
in the history of the universe." With the increased availability of large
corpora, this is a claim that can be empirically investigated. The current
paper addresses the question by using the NLTK Python library to parse corpora
of different genres, providing counts of exact string matches in each. Results
show that while completely unique sentences are often the majority of corpora,
this is highly constrained by genre, and that duplicate sentences are not an
insignificant part of any individual corpus.

</details>


### [47] [Human-Annotated NER Dataset for the Kyrgyz Language](https://arxiv.org/abs/2509.19109)
*Timur Turatali,Anton Alekseev,Gulira Jumalieva,Gulnara Kabaeva,Sergey Nikolenko*

Main category: cs.CL

TL;DR: 本文发布了首个吉尔吉斯语命名实体识别（NER）手动标注数据集KyrgyzNER，并评估了多种模型，发现多语言预训练Transformer模型在资源稀缺语言上表现出良好潜力。


<details>
  <summary>Details</summary>
Motivation: 吉尔吉斯语作为一种资源稀缺语言，缺乏高质量、手动标注的命名实体识别数据集，阻碍了其自然语言处理（NLP）领域的发展。

Method: 1. 构建了KyrgyzNER数据集，包含1,499篇新闻文章（来自24.KG），10,900个句子和39,075个实体提及，涵盖27个实体类别。
2. 详细介绍了标注方案、遇到的挑战和描述性统计数据。
3. 评估了包括基于条件随机场（CRF）的传统序列标注方法和最先进的多语言Transformer模型（如RoBERTa变体），并在新数据集上进行微调。

Result: 1. 所有模型在处理稀有实体类别时均遇到困难。
2. 经大量多语言语料库预训练的多语言RoBERTa模型在精度和召回率之间取得了有希望的平衡，表现最佳。
3. 其他多语言模型也取得了可比的结果。

Conclusion: 1. 多语言预训练模型为处理资源稀缺语言带来了挑战与机遇。
2. 尽管多语言RoBERTa表现最佳，但其他多语言模型也具有竞争力。
3. 未来的工作可以探索更细粒度的标注方案，以提供对吉尔吉斯语处理管道评估的更深层见解。

Abstract: We introduce KyrgyzNER, the first manually annotated named entity recognition
dataset for the Kyrgyz language. Comprising 1,499 news articles from the 24.KG
news portal, the dataset contains 10,900 sentences and 39,075 entity mentions
across 27 named entity classes. We show our annotation scheme, discuss the
challenges encountered in the annotation process, and present the descriptive
statistics. We also evaluate several named entity recognition models, including
traditional sequence labeling approaches based on conditional random fields and
state-of-the-art multilingual transformer-based models fine-tuned on our
dataset. While all models show difficulties with rare entity categories, models
such as the multilingual RoBERTa variant pretrained on a large corpus across
many languages achieve a promising balance between precision and recall. These
findings emphasize both the challenges and opportunities of using multilingual
pretrained models for processing languages with limited resources. Although the
multilingual RoBERTa model performed best, other multilingual models yielded
comparable results. This suggests that future work exploring more granular
annotation schemes may offer deeper insights for Kyrgyz language processing
pipelines evaluation.

</details>


### [48] [Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via LLM-Guided Multi-Aspect Clustering](https://arxiv.org/abs/2509.19125)
*Kun Zhu,Lizi Liao,Yuxuan Gu,Lei Huang,Xiaocheng Feng,Bing Qin*

Main category: cs.CL

TL;DR: 提出一种结合LLM多方面编码和动态聚类的分层分类法生成框架，解决了现有方法在连贯性和粒度上的不足，并在新的评估基准上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 科学文献的快速增长要求高效的组织和综合方法。现有分类法构建方法（如无监督聚类或直接LLM提示）在连贯性和粒度上存在不足。

Method: 提出一种上下文感知的分层分类法生成框架，整合LLM引导的多方面编码和动态聚类。该方法利用LLM识别论文关键方面并生成方面特定摘要，然后进行编码和聚类以形成连贯的层次结构。此外，引入了一个包含156个专家制作分类法（涵盖1.16万篇论文）的新评估基准。

Result: 实验结果表明，该方法显著优于现有方法，在分类法的连贯性、粒度及可解释性方面均达到最先进水平。

Conclusion: 本框架通过整合LLM优势和动态聚类，有效解决了现有分类法构建的局限性，为科学文献组织提供了一种高效、高质量的解决方案。

Abstract: The rapid growth of scientific literature demands efficient methods to
organize and synthesize research findings. Existing taxonomy construction
methods, leveraging unsupervised clustering or direct prompting of large
language models (LLMs), often lack coherence and granularity. We propose a
novel context-aware hierarchical taxonomy generation framework that integrates
LLM-guided multi-aspect encoding with dynamic clustering. Our method leverages
LLMs to identify key aspects of each paper (e.g., methodology, dataset,
evaluation) and generates aspect-specific paper summaries, which are then
encoded and clustered along each aspect to form a coherent hierarchy. In
addition, we introduce a new evaluation benchmark of 156 expert-crafted
taxonomies encompassing 11.6k papers, providing the first naturally annotated
dataset for this task. Experimental results demonstrate that our method
significantly outperforms prior approaches, achieving state-of-the-art
performance in taxonomy coherence, granularity, and interpretability.

</details>


### [49] [Anecdoctoring: Automated Red-Teaming Across Language and Place](https://arxiv.org/abs/2509.19143)
*Alejandro Cuevas,Saloni Dash,Bharat Kumar Nayak,Dan Vann,Madeleine I. G. Daepp*

Main category: cs.CL

TL;DR: 提出一种名为“anecdoctoring”的新型跨语言、跨文化红队评估方法，用于自动生成对抗性提示，以解决生成式AI的全球虚假信息风险，并证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的滥用带来虚假信息传播的巨大风险。现有的红队评估数据集主要以美国和英语为中心，缺乏跨语言和跨文化的鲁棒性。

Method: 提出“anecdoctoring”方法，自动生成跨语言和文化的对抗性提示。从三种语言（英语、西班牙语、印地语）和两个地区（美国、印度）的事实核查网站收集虚假信息，将其聚类成更广泛的叙事，并用知识图谱表征这些集群，最后用知识图谱增强攻击者LLM。

Result: 相较于少样本提示方法，本方法产生了更高的攻击成功率，并提供了更好的可解释性。

Conclusion: 研究结果强调了在全球范围内有效且基于真实世界对抗性滥用的虚假信息缓解措施的必要性。

Abstract: Disinformation is among the top risks of generative artificial intelligence
(AI) misuse. Global adoption of generative AI necessitates red-teaming
evaluations (i.e., systematic adversarial probing) that are robust across
diverse languages and cultures, but red-teaming datasets are commonly US- and
English-centric. To address this gap, we propose "anecdoctoring", a novel
red-teaming approach that automatically generates adversarial prompts across
languages and cultures. We collect misinformation claims from fact-checking
websites in three languages (English, Spanish, and Hindi) and two geographies
(US and India). We then cluster individual claims into broader narratives and
characterize the resulting clusters with knowledge graphs, with which we
augment an attacker LLM. Our method produces higher attack success rates and
offers interpretability benefits relative to few-shot prompting. Results
underscore the need for disinformation mitigations that scale globally and are
grounded in real-world adversarial misuse.

</details>


### [50] [Measuring AI "Slop" in Text](https://arxiv.org/abs/2509.19163)
*Chantal Shaib,Tuhin Chakrabarty,Diego Garcia-Olano,Byron C. Wallace*

Main category: cs.CL

TL;DR: 该研究通过专家访谈构建了AI低质量文本（'slop'）的分类法和评估维度，发现其主观性与连贯性和相关性有关，并提出了一个评估AI生成文本质量的框架。


<details>
  <summary>Details</summary>
Motivation: AI生成文本的低质量现象（'slop'）日益普遍，但目前缺乏公认的定义和衡量方法。

Method: 通过采访NLP、写作和哲学专家，开发了'slop'的分类法并提出了可解释的评估维度；采用跨度级标注（span-level annotation）来分析二元判断与潜在维度的关联。

Result: 发现对'slop'的二元判断具有一定主观性，但这些判断与连贯性和相关性等潜在维度存在关联。

Conclusion: 所开发的框架可用于评估AI生成文本的检测和偏好任务，并有望为理解影响质量判断的语言和风格因素提供新见解。

Abstract: AI "slop" is an increasingly popular term used to describe low-quality
AI-generated text, but there is currently no agreed upon definition of this
term nor a means to measure its occurrence. In this work, we develop a taxonomy
of "slop" through interviews with experts in NLP, writing, and philosophy, and
propose a set of interpretable dimensions for its assessment in text. Through
span-level annotation, we find that binary "slop" judgments are (somewhat)
subjective, but such determinations nonetheless correlate with latent
dimensions such as coherence and relevance. Our framework can be used to
evaluate AI-generated text in both detection and binary preference tasks,
potentially offering new insights into the linguistic and stylistic factors
that contribute to quality judgments.

</details>


### [51] [Soft Tokens, Hard Truths](https://arxiv.org/abs/2509.19170)
*Natasha Butt,Ariel Kwiatkowski,Ismail Labiad,Julia Kempe,Yann Ollivier*

Main category: cs.CL

TL;DR: 本文提出一种可扩展的强化学习方法，用于训练大语言模型（LLMs）的连续思维链（CoT），克服了现有方法在训练难度和计算成本上的限制。该方法通过“软”token和输入嵌入噪声实现探索，在数学推理任务上与离散CoT表现相当，但在多样性上更优，并能兼容标准部署。


<details>
  <summary>Details</summary>
Motivation: 连续token在LLMs的思维链推理中因其更高表达能力和模拟多条推理路径的潜力而受到关注。然而，现有方法因训练困难、计算成本高或需要从离散CoT蒸馏，导致实际应用受限，尤其是在长思维链场景下。

Method: 本研究首次引入了一种可扩展的强化学习（RL）方法来学习连续CoT，无需从参考离散CoT进行蒸馏。该方法使用“软”token（token混合物）并结合输入嵌入噪声来提供RL探索。此方法计算开销极小，能够学习包含数百个token的连续CoT。

Result: 在Llama和Qwen等8B模型上的数学推理基准测试中，使用连续CoT训练的模型在pass@1上与离散token CoT表现持平，但在pass@32上超越了后者，显示出更高的CoT多样性。最佳方案是使用连续CoT token进行训练，然后使用离散token进行推理。此外，连续CoT RL训练能更好地保留基础模型在域外任务上的预测能力，对基础模型影响更小。

Conclusion: 本研究提供了一种可扩展且有效的强化学习方法来训练连续思维链，解决了以往方法的局限性。该方法在性能和CoT多样性上表现优异，且能以标准方式部署，同时对基础模型具有更好的兼容性。

Abstract: The use of continuous instead of discrete tokens during the Chain-of-Thought
(CoT) phase of reasoning LLMs has garnered attention recently, based on the
intuition that a continuous mixture of discrete tokens could simulate a
superposition of several reasoning paths simultaneously. Theoretical results
have formally proven that continuous tokens have much greater expressivity and
can solve specific problems more efficiently. However, practical use of
continuous tokens has been limited by strong training difficulties: previous
works either just use continuous tokens at inference time on a pre-trained
discrete-token model, or must distill the continuous CoT from ground-truth
discrete CoTs and face computational costs that limit the CoT to very few
tokens.
  This is the first work introducing a scalable method to learn continuous CoTs
via reinforcement learning (RL), without distilling from reference discrete
CoTs. We use "soft" tokens: mixtures of tokens together with noise on the input
embedding to provide RL exploration. Computational overhead is minimal,
enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning
benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs
match discrete-token CoTs for pass@1 and surpass them for pass@32, showing
greater CoT diversity. In systematic comparisons, the best-performing scenario
is to train with continuous CoT tokens then use discrete tokens for inference,
meaning the "soft" models can be deployed in a standard way. Finally, we show
continuous CoT RL training better preserves the predictions of the base model
on out-of-domain tasks, thus providing a softer touch to the base model.

</details>


### [52] [Online Process Reward Leanring for Agentic Reinforcement Learning](https://arxiv.org/abs/2509.19199)
*Xiaoqian Liu,Ke Wang,Yuchuan Wu,Fei Huang,Yongbin Li,Junge Zhang,Jianbin Jiao*

Main category: cs.CL

TL;DR: 本文提出了一种名为OPRL的在线过程奖励学习策略，旨在解决大型语言模型(LLM)作为智能体在稀疏奖励环境中进行时间信用分配的挑战，通过将轨迹偏好转换为隐式步奖励来提高性能和样本效率。


<details>
  <summary>Details</summary>
Motivation: LLM作为自主智能体在交互环境中进行长期推理和行动时，面临稀疏且有时难以验证的奖励带来的时间信用分配难题。现有将过程监督整合到智能体学习中的方法存在标注偏差、奖励操纵、信号过于细粒度导致高方差或状态重叠稀少时的失效问题。

Method: OPRL是一种通用的信用分配策略，可无缝集成到标准on-policy算法中，无需额外rollouts或显式步骤标签。它通过基于轨迹的DPO目标，交替优化隐式过程奖励模型(PRM)和智能体策略，将轨迹偏好转化为隐式步奖励。这些步奖励与情节级结果奖励结合，用于策略更新，形成一个自我强化的循环。

Result: OPRL在WebShop、VisualSokoban和SOTOPIA等三个不同的智能体基准测试中，表现优于领先的LLMs和强大的RL基线。它实现了最先进的性能，具有更高的样本效率和更低的训练方差。此外，OPRL还通过更少的操作实现了高效探索。

Conclusion: OPRL作为一种创新的信用分配策略，显著提升了LLM智能体在复杂环境中的学习能力和效率，其卓越的性能和样本效率预示着其在实际智能体学习场景中的巨大潜力。

Abstract: Large language models (LLMs) are increasingly trained with reinforcement
learning (RL) as autonomous agents that reason and act over long horizons in
interactive environments.
  However, sparse and sometimes unverifiable rewards make temporal credit
assignment extremely challenging.
  Recent work attempts to integrate process supervision into agent learning but
suffers from biased annotation, reward hacking, high-variance from overly
fine-grained signals or failtures when state overlap is rare.
  We therefore introduce Online Process Reward Learning (OPRL), a general
credit-assignment strategy for agentic RL that integrates seamlessly with
standard on-policy algorithms without relying on additional rollouts or
explicit step labels.
  In OPRL, we optimize an implicit process reward model (PRM) alternately with
the agent's policy to transform trajectory preferences into implicit step
rewards through a trajectory-based DPO objective.
  These step rewards are then used to compute step-level advantages, which are
combined with episode-level advantages from outcome rewards for policy update,
creating a self-reinforcing loop.
  Theoretical findings guarantee that the learned step rewards are consistent
with trajectory preferences and act as potential-based shaping rewards,
providing bounded gradients to stabilize training.
  Empirically, we evaluate OPRL on three distinct agent benmarks, including
WebShop and VisualSokoban, as well as open-ended social interactions with
unverfiable rewards in SOTOPIA.
  Crucially, OPRL shows superior performance over frontier LLMs and strong RL
baselines across domains, achieving state-of-the-art results with higher
sample-efficiency and lower variance during training.
  Further analysis also demonstrates the efficient exploration by OPRL using
fewer actions, underscoring its potential for agentic learning in real-world
scenarios.

</details>


### [53] [Steering Multimodal Large Language Models Decoding for Context-Aware Safety](https://arxiv.org/abs/2509.19212)
*Zheyuan Liu,Zhangchen Xu,Guangyao Dou,Xiangchi Yuan,Zhaoxuan Tan,Radha Poovendran,Meng Jiang*

Main category: cs.CL

TL;DR: 本文提出SafeCoDe，一个轻量级且模型无关的解码框架，通过两阶段动态调整token生成，显著提升多模态大语言模型（MLLMs）在上下文敏感安全决策中的表现，平衡过敏和不敏感问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在实际应用中部署日益广泛，但其进行上下文感知安全决策的能力有限。现有方法难以平衡过度敏感（不合理拒绝良性查询）和不敏感（遗漏视觉风险），导致安全对齐存在持续空白。

Method: 引入Safety-aware Contrastive Decoding (SafeCoDe)，一个轻量级且模型无关的解码框架，分两阶段动态调整token生成：1) 对比解码机制，通过对比真实图像和高斯噪声图像来突出对视觉上下文敏感的token；2) 全局感知token调制策略，将场景级推理与token级调整相结合，根据预测的安全结论调整拒绝行为。

Result: 在多样化的MLLM架构和安全基准测试（涵盖不敏感、过度敏感和一般安全评估）中，SafeCoDe持续改进了上下文敏感的拒绝行为，同时保持了模型的实用性（helpfulnes）。

Conclusion: SafeCoDe通过动态调整token生成和集成场景级推理，有效解决了MLLMs在安全对齐中过敏和不敏感的平衡问题，显著提升了其上下文感知的安全决策能力，同时保留了模型的功能性。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly deployed in
real-world applications, yet their ability to make context-aware safety
decisions remains limited. Existing methods often fail to balance
oversensitivity (unjustified refusals of benign queries) and undersensitivity
(missed detection of visually grounded risks), leaving a persistent gap in
safety alignment. To address this issue, we introduce Safety-aware Contrastive
Decoding (SafeCoDe), a lightweight and model-agnostic decoding framework that
dynamically adjusts token generation based on multimodal context. SafeCoDe
operates in two stages: (1) a contrastive decoding mechanism that highlights
tokens sensitive to visual context by contrasting real and Gaussian-noised
images, and (2) a global-aware token modulation strategy that integrates
scene-level reasoning with token-level adjustment to adapt refusals according
to the predicted safety verdict. Extensive experiments across diverse MLLM
architectures and safety benchmarks, covering undersensitivity,
oversensitivity, and general safety evaluations, show that SafeCoDe
consistently improves context-sensitive refusal behaviors while preserving
model helpfulness.

</details>


### [54] [Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction](https://arxiv.org/abs/2509.19224)
*Tariq Abdul-Quddoos,Xishuang Dong,Lijun Qian*

Main category: cs.CL

TL;DR: 本研究比较了多种预训练注意力模型在EHR药物信息提取任务上的表现，发现临床领域预训练模型在药物检测上更优，而通用领域预训练模型在事件上下文分类上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 由于注意力模型在捕捉语言上下文表示方面优于传统技术，已成为医学语言NLP的主流方法。本研究旨在对这些预训练模型在电子健康记录（EHR）信息提取任务中的有效性进行比较分析。

Method: 选取Bert Base、BioBert、两种Bio+Clinical Bert、RoBerta和Clinical Longformer等预训练注意力模型，在哈佛医学院2022年n2c2挑战赛（Track 1）的CMED数据集上进行微调。模型用于执行药物提取、医疗事件检测和多维度药物事件上下文分类任务。性能评估采用召回率、精确度和F1-分数等指标。

Result: 预训练在临床数据上的模型在检测药物和药物事件方面表现更佳。然而，预训练在通用领域数据上的Bert Base模型在分类药物相关事件的上下文方面最为有效。

Conclusion: 在EHR药物信息提取任务中，针对不同的子任务（如药物检测与上下文分类），基于临床数据预训练的模型和基于通用领域数据预训练的模型各有优势。

Abstract: Attention-based models have become the leading approach in modeling medical
language for Natural Language Processing (NLP) in clinical notes. These models
outperform traditional techniques by effectively capturing contextual rep-
resentations of language. In this research a comparative analysis is done
amongst pre- trained attention based models namely Bert Base, BioBert, two
variations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on task
related to Electronic Health Record (EHR) information extraction. The tasks
from Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges
(n2c2) are considered for this comparison, with the Contextualized Medication
Event Dataset (CMED) given for these task. CMED is a dataset of unstructured
EHRs and annotated notes that contain task relevant information about the EHRs.
The goal of the challenge is to develop effective solutions for extracting
contextual information related to patient medication events from EHRs using
data driven methods. Each pre-trained model is fine-tuned and applied on CMED
to perform medication extraction, medical event detection, and
multi-dimensional medication event context classification. Pro- cessing methods
are also detailed for breaking down EHRs for compatibility with the applied
models. Performance analysis has been carried out using a script based on
constructing medical terms from the evaluation portion of CMED with metrics
including recall, precision, and F1-Score. The results demonstrate that models
pre-trained on clinical data are more effective in detecting medication and
medication events, but Bert Base, pre- trained on general domain data showed to
be the most effective for classifying the context of events related to
medications.

</details>


### [55] [CompLLM: Compression for Long Context Q&A](https://arxiv.org/abs/2509.19228)
*Gabriele Berton,Jayakrishnan Unnikrishnan,Son Tran,Mubarak Shah*

Main category: cs.CL

TL;DR: 为解决LLM长上下文计算挑战，本文提出CompLLM，一种分段独立压缩上下文的软压缩技术。它实现了线性效率、高可扩展性和可重用性，显著加速TTFT、减少KV缓存，并保持或超越未压缩上下文的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理长上下文时，由于自注意力的二次复杂度，面临巨大的计算挑战。现有软上下文压缩方法实用性受限，因其整体压缩导致二次复杂度且无法重用计算。

Method: 本文提出CompLLM，一种针对实际部署设计的软压缩技术。它将长上下文分割成多个段，并独立压缩每个段，而非作为一个整体进行处理。

Result: CompLLM具有三大特性：压缩步骤与上下文长度呈线性关系（效率）；能将短序列训练的模型泛化至100k tokens长上下文（可扩展性）；允许压缩段缓存和重用（可重用性）。实验表明，在2倍压缩率下，高上下文长度时，首次生成时间（TTFT）加速高达4倍，KV缓存大小减少50%，同时性能与未压缩上下文相当，甚至在极长序列上超越。

Conclusion: CompLLM是一种有效且实用的软压缩技术，通过分段独立压缩解决了LLM处理长上下文的计算效率问题，显著提升了速度、降低了资源消耗，并保持了卓越的性能。

Abstract: Large Language Models (LLMs) face significant computational challenges when
processing long contexts due to the quadratic complexity of self-attention.
While soft context compression methods, which map input text to smaller latent
representations, have shown promise, their real-world adoption is limited.
Existing techniques typically compress the context as a single unit, which
leads to quadratic compression complexity and an inability to reuse
computations across queries with overlapping contexts. In this work, we
introduce CompLLM, a soft compression technique designed for practical
deployment. Instead of processing the context holistically, CompLLM divides it
into segments and compresses each one independently. This simple design choice
yields three critical properties: efficiency, as the compression step scales
linearly with the context length; scalability, enabling models trained on short
sequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and
reusability, allowing compressed segments to be cached and reused across
different queries. Our experiments show that with a 2x compression rate, at
high context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x
and reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance
comparable to that obtained with the uncompressed context, and even surpasses
it on very long sequences, demonstrating its effectiveness and practical
utility.

</details>


### [56] [Reinforcement Learning on Pre-Training Data](https://arxiv.org/abs/2509.19249)
*Siheng Li,Kejiao Li,Zenan Xu,Guanhua Huang,Evander Yang,Kun Li,Haoyuan Wu,Jiajia Wu,Zihao Zheng,Chenchen Zhang,Kun Shi,Kyrierl Deng,Qi Yi,Ruibin Xiong,Tingqiang Xu,Yuhao Jiang,Jianfeng Yan,Yuyuan Zeng,Guanghui Xu,Jinbao Xue,Zhijiang Xu,Zheng Fang,Shuai Li,Qibin Liu,Xiaoxue Li,Zhuoyu Li,Yangyu Tao,Fei Gao,Cheng Jiang,Bo Chao Wang,Kai Liu,Jianchen Zhu,Wai Lam,Wayyt Wang,Bo Zhou,Di Wang*

Main category: cs.CL

TL;DR: 提出RLPT（基于预训练数据的强化学习）新范式，利用预训练数据直接生成奖励信号，解决LLM训练中高质量数据稀缺问题，显著提升模型推理能力和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 计算资源呈指数级增长，但高质量文本数据增长有限，导致传统大语言模型（LLM）的扩展方法受到限制。

Method: 引入RLPT（Reinforcement Learning on Pre-Training data），一种新的训练时扩展范式。RLPT通过强化学习使策略自主探索预训练数据。与依赖人类标注的RLHF和RLVR不同，RLPT直接从预训练数据中获取奖励信号，具体采用“下一片段推理目标”，奖励模型准确预测后续文本片段，从而在预训练数据上扩展RL，培养更通用的推理技能。

Result: RLPT在通用领域和数学推理基准测试中均有效。例如，应用于Qwen3-4B-Base模型时，在MMLU、MMLU-Pro、GPQA-Diamond等多个基准上取得显著绝对提升（如MMLU +3.0，GPQA-Diamond +8.1）。研究结果还显示出良好的扩展行为，并能增强RLVR性能。

Conclusion: RLPT为扩展LLM的推理边界奠定了坚实基础，展现了在更多计算资源下持续提升的巨大潜力，并能增强RLVR性能。

Abstract: The growing disparity between the exponential scaling of computational
resources and the finite growth of high-quality text data now constrains
conventional scaling approaches for large language models (LLMs). To address
this challenge, we introduce Reinforcement Learning on Pre-Training data
(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast
to prior approaches that scale training primarily through supervised learning,
RLPT enables the policy to autonomously explore meaningful trajectories to
learn from pre-training data and improve its capability through reinforcement
learning (RL). While existing RL strategies such as reinforcement learning from
human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)
rely on human annotation for reward construction, RLPT eliminates this
dependency by deriving reward signals directly from pre-training data.
Specifically, it adopts a next-segment reasoning objective, rewarding the
policy for accurately predicting subsequent text segments conditioned on the
preceding context. This formulation allows RL to be scaled on pre-training
data, encouraging the exploration of richer trajectories across broader
contexts and thereby fostering more generalizable reasoning skills. Extensive
experiments on both general-domain and mathematical reasoning benchmarks across
multiple models validate the effectiveness of RLPT. For example, when applied
to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,
$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and
AIME25, respectively. The results further demonstrate favorable scaling
behavior, suggesting strong potential for continued gains with more compute. In
addition, RLPT provides a solid foundation, extending the reasoning boundaries
of LLMs and enhancing RLVR performance.

</details>


### [57] [Extracting Conceptual Spaces from LLMs Using Prototype Embeddings](https://arxiv.org/abs/2509.19269)
*Nitesh Kumar,Usashi Chatterjee,Steven Schockaert*

Main category: cs.CL

TL;DR: 本文提出一种从大型语言模型（LLM）中学习概念空间的新策略，通过嵌入原型描述来编码特征，并通过微调LLM将原型嵌入与概念空间维度对齐，实验证明此方法高效。


<details>
  <summary>Details</summary>
Motivation: 概念空间在认知科学和可解释AI中具有重要潜力，但难以学习，尤其是在从LLMs中提取时，虽然LLMs捕获了感知特征，但缺乏有效提取概念空间的方法。

Method: 该研究提出一种策略，通过嵌入相应原型（如“非常甜的食物”）的描述来编码特征（如甜度）。为改进此策略，研究者微调了LLM，使原型嵌入与对应的概念空间维度对齐。

Result: 实证分析发现，该方法高度有效。

Conclusion: 所提出的策略提供了一种从LLMs中有效提取概念空间的方法。

Abstract: Conceptual spaces represent entities and concepts using cognitively
meaningful dimensions, typically referring to perceptual features. Such
representations are widely used in cognitive science and have the potential to
serve as a cornerstone for explainable AI. Unfortunately, they have proven
notoriously difficult to learn, although recent LLMs appear to capture the
required perceptual features to a remarkable extent. Nonetheless, practical
methods for extracting the corresponding conceptual spaces are currently still
lacking. While various methods exist for extracting embeddings from LLMs,
extracting conceptual spaces also requires us to encode the underlying
features. In this paper, we propose a strategy in which features (e.g.
sweetness) are encoded by embedding the description of a corresponding
prototype (e.g. a very sweet food). To improve this strategy, we fine-tune the
LLM to align the prototype embeddings with the corresponding conceptual space
dimensions. Our empirical analysis finds this approach to be highly effective.

</details>


### [58] [SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data](https://arxiv.org/abs/2509.19270)
*Erik Božík,Marek Šuppa*

Main category: cs.CL

TL;DR: 为解决斯洛伐克语低资源ASR数据稀缺问题，本文推出了SloPalSpeech数据集（2806小时议会语音），并用其微调Whisper模型，显著降低了WER，且已公开发布数据集和模型。


<details>
  <summary>Details</summary>
Motivation: 斯洛伐克语等低资源语言的自动语音识别（ASR）因训练数据稀缺而受阻。

Method: 引入SloPalSpeech大型斯洛伐克语ASR数据集（2806小时议会语音），开发鲁棒的处理流程将长录音对齐并分段为干净的音频-文本对。使用该数据集微调多个OpenAI Whisper模型（small, medium, large-v3, large-v3-turbo）。

Result: 在Common Voice和FLEURS等标准斯洛伐克基准测试中，微调后的Whisper模型显著降低了词错误率（WER）。例如，微调后的Whisper-small模型WER降低高达70%，接近更大的Whisper-large-v3模型的基线性能。

Conclusion: 为促进低资源语音识别的未来研究，作者公开了完整的SloPalSpeech数据集、分段转录文本和所有微调模型。

Abstract: Automatic Speech Recognition (ASR) for low-resource languages like Slovak is
hindered by the scarcity of training data. To address this, we introduce
SloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of
speech from parliamentary proceedings. We developed a robust processing
pipeline to align and segment long-form recordings into clean, 30-second
audio-transcript pairs suitable for model training. We use this dataset to
fine-tune several OpenAI Whisper models (small, medium, large-v3, and
large-v3-turbo), achieving significant Word Error Rate (WER) reductions on
standard Slovak benchmarks like Common Voice and FLEURS. For instance, the
fine-tuned Whisper-small model's WER dropped by up to 70\%, approaching the
baseline performance of the much larger Whisper-large-v3 model. To foster
future research in low-resource speech recognition, we publicly release the
complete SloPalSpeech dataset, the fully segmented transcripts (60 million
words), and all our fine-tuned models.

</details>


### [59] [WolBanking77: Wolof Banking Speech Intent Classification Dataset](https://arxiv.org/abs/2509.19271)
*Abdou Karim Kandji,Frédéric Precioso,Cheikh Ba,Samba Ndiaye,Augustin Ndione*

Main category: cs.CL

TL;DR: 本文发布了一个针对沃洛夫语（一种低资源口语）的意图分类数据集 (WolBanking77)，以解决高资源语言研究偏见问题，并在此数据集上进行了基线模型的实验，获得了有前景的结果。


<details>
  <summary>Details</summary>
Motivation: 现有意图分类模型主要关注高资源语言数据集，导致低资源语言（特别是文盲率高、口语使用多于书面语的地区）存在研究空白。例如，沃洛夫语在塞内加尔被广泛使用但文盲率高，急需相关研究。

Method: 发布了一个新的沃洛夫语意图分类数据集WolBanking77，包含9,791条银行领域的文本句子和超过4小时的口语句子。在此数据集上，对文本和语音领域的最新（SOTA）基线模型进行了实验，并提供了详细的数据内容分析。

Result: 基线模型在WolBanking77数据集上取得了非常有前景的结果。报告了NLP模型的f1-score和ASR模型的词错误率指标，并进行了不同模型之间的比较。

Conclusion: WolBanking77数据集的发布填补了低资源口语意图分类研究的空白，初步实验结果令人鼓舞。作者计划共享数据集、进行维护更新并发布开源代码，以促进相关学术研究。

Abstract: Intent classification models have made a lot of progress in recent years.
However, previous studies primarily focus on high-resource languages datasets,
which results in a gap for low-resource languages and for regions with a high
rate of illiterate people where languages are more spoken than read or written.
This is the case in Senegal, for example, where Wolof is spoken by around 90\%
of the population, with an illiteracy rate of 42\% for the country. Wolof is
actually spoken by more than 10 million people in West African region. To
tackle such limitations, we release a Wolof Intent Classification Dataset
(WolBanking77), for academic research in intent classification. WolBanking77
currently contains 9,791 text sentences in the banking domain and more than 4
hours of spoken sentences. Experiments on various baselines are conducted in
this work, including text and voice state-of-the-art models. The results are
very promising on this current dataset. This paper also provides detailed
analyses of the contents of the data. We report baseline f1-score and word
error rate metrics respectively on NLP and ASR models trained on WolBanking77
dataset and also comparisons between models. We plan to share and conduct
dataset maintenance, updates and to release open-source code.

</details>


### [60] [DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture](https://arxiv.org/abs/2509.19274)
*Arijit Maji,Raghvendra Kumar,Akash Ghosh,Anushka,Nemil Shah,Abhilekh Borah,Vanshika Shah,Nishant Mishra,Sriparna Saha*

Main category: cs.CL

TL;DR: 本文引入了DRISHTIKON，一个专注于印度文化的首次多模态多语言基准，旨在评估生成式AI系统的文化理解能力，并发现现有模型在处理文化背景输入时存在显著局限，尤其在低资源语言和传统方面。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准多为通用或全球范围，缺乏对特定文化（如印度文化）的深入细致评估。为了衡量和提升生成式AI系统在处理文化背景、多模态输入时的理解能力，需要一个专门且细粒度的工具。

Method: 引入了DRISHTIKON，一个涵盖印度15种语言、所有邦和联邦属地、包含超过64,000个对齐文本-图像对的多模态多语言数据集，其内容涉及节日、服饰、美食、艺术形式和历史遗产等。作者利用该基准评估了多种视觉-语言模型（VLM），包括开源、专有、推理专业及印度语系模型，评估设置涵盖零样本和思维链。

Result: 评估结果表明，当前模型在处理具有文化背景的多模态输入时，其推理能力存在关键局限。特别是对于低资源语言和记载较少的传统，模型的理解能力尤为不足。

Conclusion: DRISHTIKON填补了包容性AI研究的一个重要空白，提供了一个强大的测试平台，有助于推动开发更具文化意识和多模态能力的语言技术。

Abstract: We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual
benchmark centered exclusively on Indian culture, designed to evaluate the
cultural understanding of generative AI systems. Unlike existing benchmarks
with a generic or global scope, DRISHTIKON offers deep, fine-grained coverage
across India's diverse regions, spanning 15 languages, covering all states and
union territories, and incorporating over 64,000 aligned text-image pairs. The
dataset captures rich cultural themes including festivals, attire, cuisines,
art forms, and historical heritage amongst many more. We evaluate a wide range
of vision-language models (VLMs), including open-source small and large models,
proprietary systems, reasoning-specialized VLMs, and Indic-focused models,
across zero-shot and chain-of-thought settings. Our results expose key
limitations in current models' ability to reason over culturally grounded,
multimodal inputs, particularly for low-resource languages and less-documented
traditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a
robust testbed to advance culturally aware, multimodally competent language
technologies.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [61] [PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset](https://arxiv.org/abs/2509.18159)
*Akwasi Asare,Ulas Bagci*

Main category: cs.CV

TL;DR: PolypSeg-GradCAM是一个结合U-Net和Grad-CAM的可解释深度学习框架，实现了高精度、透明的息肉分割，有望改进AI辅助结肠镜检查和早期结直肠癌预防。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌 (CRC) 是主要癌症死因，胃肠息肉是其前兆。早期准确分割息肉至关重要，但人工操作费时且易变。深度学习虽有潜力，但其可解释性不足阻碍了临床应用。

Method: 本研究提出了PolypSeg-GradCAM框架，该框架将U-Net架构与梯度加权类激活映射 (Grad-CAM) 相结合，旨在实现透明的息肉分割。模型在包含1000张带注释内窥镜图像的Kvasir-SEG数据集上进行了训练和评估。

Result: PolypSeg-GradCAM展现了稳健的分割性能，在测试集上实现了0.9257的平均IoU，并在训练集和验证集上取得了持续高于0.96的Dice系数 (F-score)。Grad-CAM可视化证实了模型预测由临床相关区域驱动，增强了透明度和信任度。

Conclusion: PolypSeg-GradCAM通过结合高分割精度和可解释性，为实现可靠、值得信赖的AI辅助结肠镜检查迈出了重要一步，有助于改善早期结直肠癌预防。

Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related
morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as
critical precursors according to the World Health Organization (WHO). Early and
accurate segmentation of polyps during colonoscopy is essential for reducing
CRC progression, yet manual delineation is labor-intensive and prone to
observer variability. Deep learning methods have demonstrated strong potential
for automated polyp analysis, but their limited interpretability remains a
barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an
explainable deep learning framework that integrates the U-Net architecture with
Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp
segmentation. The model was trained and evaluated on the Kvasir-SEG dataset of
1000 annotated endoscopic images. Experimental results demonstrate robust
segmentation performance, achieving a mean Intersection over Union (IoU) of
0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)
on training and validation sets. Grad-CAM visualizations further confirmed that
predictions were guided by clinically relevant regions, enhancing transparency
and trust in the model's decisions. By coupling high segmentation accuracy with
interpretability, PolypSeg-GradCAM represents a step toward reliable,
trustworthy AI-assisted colonoscopy and improved early colorectal cancer
prevention.

</details>


### [62] [PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2509.18160)
*Akwasi Asare,Isaac Baffour Senkyire,Emmanuel Freeman,Simon Hilary Ayinedenaba Aluze-Ele,Kelvin Kwao*

Main category: cs.CV

TL;DR: PerceptronCARE是一种基于深度学习的远程眼科应用，用于自动检测糖尿病视网膜病变，旨在提高诊断效率和可及性。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是成人视力丧失的主要原因，特别是在医疗资源匮乏地区，因此需要早期诊断和扩大筛查覆盖。

Method: 该研究开发了PerceptronCARE系统，使用ResNet-18、EfficientNet-B0和SqueezeNet等多种卷积神经网络进行模型开发和评估，以平衡准确性和计算效率。系统还集成了云端可伸缩性、安全数据管理和多用户框架。

Result: 最终模型在疾病严重程度分类上达到了85.4%的准确率，能够实现临床和远程医疗环境下的实时筛查。

Conclusion: PerceptronCARE展示了AI驱动的远程医疗解决方案在扩大糖尿病视网膜病变筛查可及性方面的潜力，尤其适用于偏远和资源受限地区，有助于早期诊断、改善医患互动并降低医疗成本。

Abstract: Diabetic retinopathy is a leading cause of vision loss among adults and a
major global health challenge, particularly in underserved regions. This study
presents PerceptronCARE, a deep learning-based teleophthalmology application
designed for automated diabetic retinopathy detection using retinal images. The
system was developed and evaluated using multiple convolutional neural
networks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine
the optimal balance between accuracy and computational efficiency. The final
model classifies disease severity with an accuracy of 85.4%, enabling real-time
screening in clinical and telemedicine settings. PerceptronCARE integrates
cloud-based scalability, secure patient data management, and a multi-user
framework, facilitating early diagnosis, improving doctor-patient interactions,
and reducing healthcare costs. This study highlights the potential of AI-driven
telemedicine solutions in expanding access to diabetic retinopathy screening,
particularly in remote and resource-constrained environments.

</details>


### [63] [Self Identity Mapping](https://arxiv.org/abs/2509.18165)
*Xiuding Cai,Yaoyao Zhu,Linjie Fu,Dong Miao,Yu Yao*

Main category: cs.CV

TL;DR: 提出了一种名为Self Identity Mapping (SIM)的新型数据内在正则化框架，通过逆映射机制增强表示学习，减少信息损失，并在多种任务和领域中表现出显著的通用性和有效性。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的传统正则化技术常依赖启发式方法，在多样化设置中可靠性或有效性不足，需要一种更可靠、数据内在的正则化方法来增强泛化能力并缓解过拟合。

Method: 引入Self Identity Mapping (SIM)，利用逆映射机制从变换后的输出重建输入，以减少前向传播中的信息损失并促进更平滑的梯度流。为提高计算效率，SIM被实例化为ρSIM，通过补丁级特征采样和基于投影的方法重建潜在特征，显著降低了复杂度。它是一个模型无关、任务无关的即插即用模块。

Result: ρSIM在图像分类、少样本提示学习和领域泛化等任务中始终优于基线方法。实验表明，ρSIM能有效增强表示学习，且与现有正则化方法正交，能进一步提升其效果。此外，ρSIM在语义分割、图像翻译等密集到密集任务以及音频分类和时间序列异常检测等非视觉领域中也能有效保留语义信息并提升性能。

Conclusion: SIM（特别是ρSIM）是一种简单而有效的、数据内在的正则化框架，能够显著增强表示学习和泛化能力。作为一个模型无关、任务无关的即插即用模块，它在各种任务和领域中表现出强大的性能提升，并且能够补充现有正则化方法。

Abstract: Regularization is essential in deep learning to enhance generalization and
mitigate overfitting. However, conventional techniques often rely on
heuristics, making them less reliable or effective across diverse settings. We
propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic
regularization framework that leverages an inverse mapping mechanism to enhance
representation learning. By reconstructing the input from its transformed
output, SIM reduces information loss during forward propagation and facilitates
smoother gradient flow. To address computational inefficiencies, We instantiate
SIM as $ \rho\text{SIM} $ by incorporating patch-level feature sampling and
projection-based method to reconstruct latent features, effectively lowering
complexity. As a model-agnostic, task-agnostic regularizer, SIM can be
seamlessly integrated as a plug-and-play module, making it applicable to
different network architectures and tasks.
  We extensively evaluate $\rho\text{SIM}$ across three tasks: image
classification, few-shot prompt learning, and domain generalization.
Experimental results show consistent improvements over baseline methods,
highlighting $\rho\text{SIM}$'s ability to enhance representation learning
across various tasks. We also demonstrate that $\rho\text{SIM}$ is orthogonal
to existing regularization methods, boosting their effectiveness. Moreover, our
results confirm that $\rho\text{SIM}$ effectively preserves semantic
information and enhances performance in dense-to-dense tasks, such as semantic
segmentation and image translation, as well as in non-visual domains including
audio classification and time series anomaly detection. The code is publicly
available at https://github.com/XiudingCai/SIM-pytorch.

</details>


### [64] [MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion](https://arxiv.org/abs/2509.18170)
*Zhanting Zhou,Jinbo Wang,Zeqin Wu,Fengli Zhang*

Main category: cs.CV

TL;DR: 提出MAGIA，一种基于动量的梯度反演攻击自适应修正框架，在大批量单轮平均梯度（SAG）机制下，无需辅助信息，高效实现了高保真度多图像重建，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有梯度反演方法在挑战性的单轮平均梯度（SAG）机制下，由于每个样本信息在单个批次平均梯度中高度纠缠，难以在大批量场景下进行高保真度的数据重建。

Method: 引入MAGIA框架，这是一种无需标签推理的、基于动量的梯度反演攻击自适应修正方法。其目标函数整合了两项核心创新：1) 一个闭式组合重缩放，可实现更紧密的优化边界；2) 一个基于动量的整体批次损失与子集损失混合机制，以确保重建的鲁棒性。该方法通过探测随机数据子集来感知潜在的单图像信号。

Result: MAGIA显著超越了现有先进方法，在大批量场景（现有方法失败的场景）下实现了高保真度的多图像重建。该方法计算开销与标准求解器相当，且无需任何辅助信息。

Conclusion: MAGIA为大批量SAG机制下的梯度反演重建提供了一个高效、鲁棒且无需辅助信息的解决方案，有效克服了现有方法在此类挑战性场景中的局限性。

Abstract: We study gradient inversion in the challenging single round averaged gradient
SAG regime where per sample cues are entangled within a single batch mean
gradient. We introduce MAGIA a momentum based adaptive correction on gradient
inversion attack a novel label inference free framework that senses latent per
image signals by probing random data subsets. MAGIA objective integrates two
core innovations 1 a closed form combinatorial rescaling that creates a
provably tighter optimization bound and 2 a momentum based mixing of whole
batch and subset losses to ensure reconstruction robustness. Extensive
experiments demonstrate that MAGIA significantly outperforms advanced methods
achieving high fidelity multi image reconstruction in large batch scenarios
where prior works fail. This is all accomplished with a computational footprint
comparable to standard solvers and without requiring any auxiliary information.

</details>


### [65] [Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR](https://arxiv.org/abs/2509.18174)
*Khalil Hennara,Muhammad Hreden,Mohamed Motasim Hamed,Ahmad Bastati,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CV

TL;DR: 本研究推出了Baseer，一个专为阿拉伯语文档OCR微调的视觉-语言模型，通过领域适应实现了0.25的词错误率，显著超越现有方案，并建立了新的SOTA，同时引入了高质量的评估基准Misraj-DocOCR。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语OCR因其连笔书写、多样字体、变音符号及从右到左的书写方向而极具挑战性。尽管多模态大型语言模型（MLLMs）在其他语言上表现出色，但它们在阿拉伯语上的性能仍然有限。

Method: 引入了Baseer，一个通过仅解码器微调策略适应预训练MLLM的视觉-语言模型，专用于阿拉伯语文档OCR。该模型利用大规模的合成与真实文档数据集进行训练。同时，提出了Misraj-DocOCR，一个高质量、专家验证的阿拉伯语OCR评估基准。

Result: 实验证明，Baseer显著优于现有的开源和商业解决方案，实现了0.25的词错误率（WER），并在阿拉伯语文档OCR领域确立了新的最先进水平。

Conclusion: 研究结果突出了对通用MLLMs进行领域特定适应的优势，并为阿拉伯语等形态丰富的语言的高精度OCR建立了一个强大的基线。

Abstract: Arabic document OCR remains a challenging task due to the language's cursive
script, diverse fonts, diacritics, and right-to-left orientation. While modern
Multimodal Large Language Models (MLLMs) have advanced document understanding
for high-resource languages, their performance on Arabic remains limited. In
this work, we introduce Baseer, a vision-language model fine- tuned
specifically for Arabic document OCR. Leveraging a large-scale dataset
combining synthetic and real-world documents, Baseer is trained using a
decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving
general visual features. We also present Misraj-DocOCR, a high-quality,
expert-verified benchmark designed for rigorous evaluation of Arabic OCR
systems. Our experiments show that Baseer significantly outperforms existing
open-source and commercial solutions, achieving a WER of 0.25 and establishing
a new state-of-the-art in the domain of Arabic document OCR. Our results
highlight the benefits of domain-specific adaptation of general-purpose MLLMs
and establish a strong baseline for high-accuracy OCR on morphologically rich
languages like Arabic.

</details>


### [66] [A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland](https://arxiv.org/abs/2509.18176)
*Wendong Yao,Saeed Azadnejad,Binhua Huang,Shane Donohue,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 本文提出一种创新的深度学习框架（CNN-LSTM），通过将稀疏InSAR数据转换为密集时空张量，实现了对地面形变的高精度、空间连贯性预测，显著优于传统基线模型。


<details>
  <summary>Details</summary>
Motivation: 地面位移监测对城市基础设施稳定和地质灾害缓解至关重要，但从稀疏的InSAR时间序列数据中预测未来形变仍是重大挑战。

Method: 引入一种新型深度学习框架，将稀疏InSAR点测量转换为密集时空张量。设计并实现了一个混合卷积神经网络和长短期记忆网络（CNN-LSTM）模型，用于同步学习空间模式和时间依赖性。使用爱尔兰东部的Sentinel-1数据进行实验，并与Light Gradient Boosting Machine和LASSO回归等基线模型进行性能对比。

Result: 所提出的架构提供了显著更准确且空间连贯的预测，为该任务设立了新的性能基准。解释性分析表明，基线模型常默认简单的持久性模式，凸显了本文集成时空方法捕捉复杂地面形变动态的必要性。

Conclusion: 研究结果证实了时空深度学习在高分辨率形变预测方面的有效性和潜力。

Abstract: Monitoring ground displacement is crucial for urban infrastructure stability
and mitigating geological hazards. However, forecasting future deformation from
sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data
remains a significant challenge. This paper introduces a novel deep learning
framework that transforms these sparse point measurements into a dense
spatio-temporal tensor. This methodological shift allows, for the first time,
the direct application of advanced computer vision architectures to this
forecasting problem. We design and implement a hybrid Convolutional Neural
Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to
simultaneously learn spatial patterns and temporal dependencies from the
generated data tensor. The model's performance is benchmarked against powerful
machine learning baselines, Light Gradient Boosting Machine and LASSO
regression, using Sentinel-1 data from eastern Ireland. Results demonstrate
that the proposed architecture provides significantly more accurate and
spatially coherent forecasts, establishing a new performance benchmark for this
task. Furthermore, an interpretability analysis reveals that baseline models
often default to simplistic persistence patterns, highlighting the necessity of
our integrated spatio-temporal approach to capture the complex dynamics of
ground deformation. Our findings confirm the efficacy and potential of
spatio-temporal deep learning for high-resolution deformation forecasting.

</details>


### [67] [A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts](https://arxiv.org/abs/2509.18177)
*George Corrêa de Araújo,Helena de Almeida Maia,Helio Pedrini*

Main category: cs.CV

TL;DR: Scrapbook框架生成数据集以探究AI模型对基本概念的理解。实验发现模型擅长物体识别但难以理解位置和复杂约束。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以全面评估AI模型对物体识别、位置等基本概念的理解深度。需要一个框架来生成广泛数据集，以验证模型在处理更复杂任务前的基础能力。

Method: 提出Scrapbook框架，一种新颖方法论，用于生成探测AI模型学习概念的大规模数据集。该框架聚焦物体识别、绝对和相对位置、属性识别等基本概念，通过生成大量关于单个概念且语言变体广泛的问题。

Result: 现代模型在物体识别和枚举方面表现熟练，但难以理解位置信息和处理带额外约束的查询。MobileVLM-V2模型显示答案分歧大且易出错；其他模型则倾向肯定答案，并在几何形状和位置信息问题上表现不佳。

Conclusion: Scrapbook框架是评估和增强AI模型性能的宝贵工具，研究结果揭示了当前模型在理解基础概念（特别是位置和复杂约束）方面的不足，为未来改进提供了方向。

Abstract: In this paper, we present the Scrapbook framework, a novel methodology
designed to generate extensive datasets for probing the learned concepts of
artificial intelligence (AI) models. The framework focuses on fundamental
concepts such as object recognition, absolute and relative positions, and
attribute identification. By generating datasets with a large number of
questions about individual concepts and a wide linguistic variation, the
Scrapbook framework aims to validate the model's understanding of these basic
elements before tackling more complex tasks. Our experimental findings reveal
that, while contemporary models demonstrate proficiency in recognizing and
enumerating objects, they encounter challenges in comprehending positional
information and addressing inquiries with additional constraints. Specifically,
the MobileVLM-V2 model showed significant answer disagreements and plausible
wrong answers, while other models exhibited a bias toward affirmative answers
and struggled with questions involving geometric shapes and positional
information, indicating areas for improvement in understanding and consistency.
The proposed framework offers a valuable instrument for generating diverse and
comprehensive datasets, which can be utilized to systematically assess and
enhance the performance of AI models.

</details>


### [68] [The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes](https://arxiv.org/abs/2509.18179)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 研究发现，在视觉-语言-视觉多模态系统中，文本作为中间表示会导致图像信息严重丢失。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI系统在创意工作流中的应用日益增多，评估其局限性需要理解视觉-语言-视觉管道中的信息损失。然而，视觉内容通过文本中介时发生的降级量化不足。

Method: 通过“描述-生成”管道生成150对图像，并应用现有指标（LPIPS、SSIM和颜色距离）来衡量感知、结构和色度维度上的信息保留情况，进行经验分析。

Result: 评估显示，99.3%的样本表现出显著的感知降级，91.5%的样本表现出显著的结构信息损失。

Conclusion: 经验证据表明，“描述-生成”瓶颈是当代多模态系统中可测量且一致的局限性，导致显著的信息丢失。

Abstract: With the increasing integration of multimodal AI systems in creative
workflows, understanding information loss in vision-language-vision pipelines
has become important for evaluating system limitations. However, the
degradation that occurs when visual content passes through textual
intermediation remains poorly quantified. In this work, we provide empirical
analysis of the describe-then-generate bottleneck, where natural language
serves as an intermediate representation for visual information. We generated
150 image pairs through the describe-then-generate pipeline and applied
existing metrics (LPIPS, SSIM, and color distance) to measure information
preservation across perceptual, structural, and chromatic dimensions. Our
evaluation reveals that 99.3% of samples exhibit substantial perceptual
degradation and 91.5% demonstrate significant structural information loss,
providing empirical evidence that the describe-then-generate bottleneck
represents a measurable and consistent limitation in contemporary multimodal
systems.

</details>


### [69] [AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines](https://arxiv.org/abs/2509.18182)
*Isabelle Tingzon,Yoji Toriumi,Caroline Gevaert*

Main category: cs.CV

TL;DR: 本研究利用AI和高分辨率卫星图像，自动推断小岛屿发展中国家（SIDS）的屋顶属性，以弥补其在城市韧性规划和灾害风险管理方面的数据空白。


<details>
  <summary>Details</summary>
Motivation: 气候脆弱的小岛屿发展中国家（SIDS）通常缺乏详细的建筑结构信息，而这些信息对于估计灾害潜在损失、城市韧性规划和灾害风险减缓至关重要。

Method: 开发了一个AI驱动的工作流程，从高分辨率卫星图像中自动推断屋顶属性。研究比较了地理空间基础模型结合浅层分类器与微调的深度学习模型在屋顶分类（坡度和材料）上的效用，并评估了纳入邻近SIDS额外训练数据对模型性能的影响。

Result: 最佳模型在屋顶坡度分类上取得了0.88的F1分数，在屋顶材料分类上取得了0.83的F1分数。

Conclusion: 该工作旨在结合当地能力建设，为SIDS提供利用AI和地球观测数据的新能力，以实现更高效、基于证据的城市治理，从而提升应对灾害事件的韧性。

Abstract: Detailed structural building information is used to estimate potential damage
from hazard events like cyclones, floods, and landslides, making them critical
for urban resilience planning and disaster risk reduction. However, such
information is often unavailable in many small island developing states (SIDS)
in climate-vulnerable regions like the Caribbean. To address this data gap, we
present an AI-driven workflow to automatically infer rooftop attributes from
high-resolution satellite imagery, with Saint Vincent and the Grenadines as our
case study. Here, we compare the utility of geospatial foundation models
combined with shallow classifiers against fine-tuned deep learning models for
rooftop classification. Furthermore, we assess the impact of incorporating
additional training data from neighboring SIDS to improve model performance.
Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof
material classification, respectively. Combined with local capacity building,
our work aims to provide SIDS with novel capabilities to harness AI and Earth
Observation (EO) data to enable more efficient, evidence-based urban
governance.

</details>


### [70] [VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation](https://arxiv.org/abs/2509.18183)
*Jinyue Bian,Zhaoxing Zhang,Zhengyu Liang,Shiwei Zheng,Shengtao Zhang,Rong Shen,Chen Yang,Anzhou Hou*

Main category: cs.CV

TL;DR: 提出VLA-LPAF轻量级模块，通过在潜在空间融合多视角观测，提升VLA模型对视觉视角变化的适应性，显著提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: VLA模型依赖标准演示数据训练，但其视觉观测（来自不同相机视角和数量）存在视角异质性，导致视觉特征差异大，限制了模型的泛化能力。

Method: 提出VLA-LPAF轻量级模块，仅使用2D数据，通过单视角图像微调，并在潜在空间融合其他多视角观测，有效弥合视角不一致性。该框架通过RoboFlamingo-LPAF实例化。

Result: RoboFlamingo-LPAF在CALVIN基准上平均提升约8%的任务成功率，LIBERO上提升15%，定制模拟基准上提升30%，并在真实世界任务中展示了视角适应性。

Conclusion: VLA-LPAF模块有效增强了VLA模型（如RoboFlamingo）的视角适应性，显著提升了其在模拟和真实世界任务中的泛化能力和任务成功率。

Abstract: The Visual-Language-Action (VLA) models can follow text instructions
according to visual observations of the surrounding environment. This ability
to map multimodal inputs to actions is derived from the training of the VLA
model on extensive standard demonstrations. These visual observations captured
by third-personal global and in-wrist local cameras are inevitably varied in
number and perspective across different environments, resulting in significant
differences in the visual features. This perspective heterogeneity constrains
the generality of VLA models. In light of this, we first propose the
lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models
using only 2D data. VLA-LPAF is finetuned using images from a single view and
fuses other multiview observations in the latent space, which effectively and
efficiently bridge the gap caused by perspective inconsistency. We instantiate
our VLA-LPAF framework with the VLA model RoboFlamingo to construct
RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves
around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a
customized simulation benchmark. We also demonstrate the developed viewadaptive
characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

</details>


### [71] [URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation](https://arxiv.org/abs/2509.18184)
*Yifeng Cheng,Alois Knoll,Hu Cao*

Main category: cs.CV

TL;DR: 本文提出URNet，一个不确定性感知细化网络，用于事件相机立体深度估计，通过局部-全局细化模块和KL散度不确定性建模，在DSEC数据集上超越了现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高时间分辨率、高动态范围和低延迟等显著优势，优于传统帧基相机，因此研究基于事件的深度估计算法具有重要意义。

Method: 引入了URNet，一个不确定性感知细化网络，用于事件相机立体深度估计。该方法包含一个局部-全局细化模块，有效捕捉细粒度局部细节和长距离全局上下文；此外，还引入了基于Kullback-Leibler (KL) 散度的不确定性建模方法，以提高预测可靠性。

Result: 在DSEC数据集上进行了广泛实验，结果表明URNet在定性和定量评估中均持续优于现有最先进（SOTA）方法。

Conclusion: URNet通过其创新的局部-全局细化和不确定性建模，显著提升了事件相机立体深度估计的性能和可靠性。

Abstract: Event cameras provide high temporal resolution, high dynamic range, and low
latency, offering significant advantages over conventional frame-based cameras.
In this work, we introduce an uncertainty-aware refinement network called URNet
for event-based stereo depth estimation. Our approach features a local-global
refinement module that effectively captures fine-grained local details and
long-range global context. Additionally, we introduce a Kullback-Leibler (KL)
divergence-based uncertainty modeling method to enhance prediction reliability.
Extensive experiments on the DSEC dataset demonstrate that URNet consistently
outperforms state-of-the-art (SOTA) methods in both qualitative and
quantitative evaluations.

</details>


### [72] [Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases](https://arxiv.org/abs/2509.18185)
*Giammarco La Barbera,Enzo Bonnot,Thomas Isla,Juan Pablo de la Plata,Joy-Rose Dunoyer de Segonzac,Jennifer Attali,Cécile Lozach,Alexandre Bellucci,Louis Marcellin,Laure Fournier,Sabine Sarnacki,Pietro Gori,Isabelle Bloch*

Main category: cs.CV

TL;DR: Visionerves是一种新型混合AI框架，利用多梯度DWI和形态学MRI对周围神经系统进行识别，显著提高了子宫内膜异位症患者的神经成像准确性，实现了无需手动ROI的自动化分析。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症常导致慢性盆腔疼痛并可能累及神经，但目前对外周神经的成像仍具有挑战性。

Method: 引入Visionerves框架，它结合了多梯度DWI和形态学MRI数据。该框架分为两阶段：(A)使用深度学习模型自动分割解剖结构；(B)通过符号空间推理进行神经束追踪和识别，其中通过模糊空间关系编码解剖知识，无需手动选择ROI。

Result: 将Visionerves应用于10名确诊或疑似子宫内膜异位症女性的腰骶丛，与标准纤维束追踪技术相比，Dice分数提高了高达25%，空间误差减小到小于5毫米。

Conclusion: 这种自动化、可复现的方法使得详细的神经分析成为可能，为非侵入性诊断子宫内膜异位症相关神经病变以及其他涉及神经的疾病铺平了道路。

Abstract: Endometriosis often leads to chronic pelvic pain and possible nerve
involvement, yet imaging the peripheral nerves remains a challenge. We
introduce Visionerves, a novel hybrid AI framework for peripheral nervous
system recognition from multi-gradient DWI and morphological MRI data. Unlike
conventional tractography, Visionerves encodes anatomical knowledge through
fuzzy spatial relationships, removing the need for selection of manual ROIs.
The pipeline comprises two phases: (A) automatic segmentation of anatomical
structures using a deep learning model, and (B) tractography and nerve
recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in
10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated
substantial improvements over standard tractography, with Dice score
improvements of up to 25% and spatial errors reduced to less than 5 mm. This
automatic and reproducible approach enables detailed nerve analysis and paves
the way for non-invasive diagnosis of endometriosis-related neuropathy, as well
as other conditions with nerve involvement.

</details>


### [73] [V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling](https://arxiv.org/abs/2509.18187)
*Muhammad Naveed,Nazia Perwaiz,Sidra Sultana,Mohaira Ahmad,Muhammad Moazam Fraz*

Main category: cs.CV

TL;DR: 本文介绍了V-SenseDrive，首个在巴基斯坦驾驶环境中收集的隐私保护多模态驾驶行为数据集。


<details>
  <summary>Details</summary>
Motivation: 道路交通事故是严重的公共卫生挑战，尤其是在新兴经济体中。现有驾驶行为数据集缺乏对这些区域（如巴基斯坦）行为多样性的代表性，且存在隐私问题。可靠检测不安全驾驶行为是提高道路安全、支持ADAS和数据驱动决策的关键前提。

Method: 开发了V-SenseDrive数据集，通过定制的Android应用程序在巴基斯坦驾驶环境中收集数据。该数据集结合了智能手机的惯性传感器和GPS数据与同步的路面视频，记录了三种目标驾驶行为（正常、激进、危险），涵盖了城市主干道、次干道和高速公路等多种道路类型。工作重点在于数据采集过程，包括参与者选择、驾驶场景、环境考量和传感器视频同步技术。

Result: 成功构建了V-SenseDrive数据集，它是首个隐私保护的巴基斯坦多模态驾驶行为数据集。该数据集将高频加速度计、陀螺仪和GPS数据流与连续视频结合，所有数据源都精确时间对齐，并被组织成原始、处理和语义层，确保了其在未来研究中的适应性。

Conclusion: V-SenseDrive数据集填补了全球驾驶行为数据集在代表性上的关键空白，为驾驶行为分类、交通安全分析和高级驾驶辅助系统（ADAS）的未来研究奠定了基础，有助于开发情境感知的智能交通解决方案。

Abstract: Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.

</details>


### [74] [Qianfan-VL: Domain-Enhanced Universal Vision-Language Models](https://arxiv.org/abs/2509.18189)
*Daxiang Dong,Mingming Zheng,Dong Xu,Bairong Zhuang,Wenyu Zhang,Chunhua Luo,Haoran Wang,Zijian Zhao,Jie Li,Yuxuan Li,Hanjun Zhong,Mengyue Liu,Jieting Chen,Shupeng Li,Lun Tian,Yaping Feng,Xin Li,Donggang Jiang,Yong Chen,Yehua Xu,Duohao Qin,Chen Feng,Dan Wang,Henghua Zhang,Jingjing Ha,Jinhui He,Yanfeng Zhai,Chengxin Zheng,Jiayi Mao,Jiacheng Chen,Ruchang Yao,Ziye Yuan,Jianmin Wu,Guangjun Xie,Dou Shen*

Main category: cs.CV

TL;DR: 本文提出了Qianfan-VL系列多模态大语言模型（3B-70B），通过创新的领域增强技术（多阶段训练、数据合成）实现了SOTA性能，并在多种通用及特定领域（如OCR、文档理解、数学推理）任务上表现出色，全部在中国昆仑P800芯片上训练完成，展示了国内大规模AI基础设施的强大能力。


<details>
  <summary>Details</summary>
Motivation: 开发具有强大通用性能同时显著增强领域特定能力的多模态大语言模型，并验证其在多样化企业部署场景中的适用性，以及国内AI基础设施训练SOTA模型的能力。

Method: 采用Qianfan-VL系列（3B-70B）多模态大语言模型，核心方法包括：1. 创新的领域增强技术；2. 多阶段渐进式训练；3. 高精度数据合成流水线；4. 8B和70B版本融入长链式思维能力；5. 模型完全在百度昆仑P800芯片上进行训练。

Result: Qianfan-VL在通用基准上与领先开源模型相当，并在CCBench、SEEDBench IMG、ScienceQA和MMStar等基准上达到SOTA性能。在OCR和文档理解方面优势显著（OCRBench 873，DocVQA 94.75%）。8B和70B版本在数学推理（MathVista 78.6%）和逻辑推理任务上表现优异。此外，验证了百度昆仑P800芯片在5000块芯片上单任务扩展效率超过90%，能够训练SOTA级多模态模型。

Conclusion: 这项工作建立了一种开发领域增强型多模态模型的有效方法，适用于各种企业部署场景。它也证明了国内大规模AI基础设施具备训练SOTA级多模态模型的强大能力和高扩展效率。

Abstract: We present Qianfan-VL, a series of multimodal large language models ranging
from 3B to 70B parameters, achieving state-of-the-art performance through
innovative domain enhancement techniques. Our approach employs multi-stage
progressive training and high-precision data synthesis pipelines, which prove
to be critical technologies for enhancing domain-specific capabilities while
maintaining strong general performance. Qianfan-VL achieves comparable results
to leading open-source models on general benchmarks, with state-of-the-art
performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and
MMStar. The domain enhancement strategy delivers significant advantages in OCR
and document understanding, validated on both public benchmarks (OCRBench 873,
DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B
variants incorporate long chain-of-thought capabilities, demonstrating superior
performance on mathematical reasoning (MathVista 78.6%) and logical inference
tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating
the capability of large-scale AI infrastructure to train SOTA-level multimodal
models with over 90% scaling efficiency on 5000 chips for a single task. This
work establishes an effective methodology for developing domain-enhanced
multimodal models suitable for diverse enterprise deployment scenarios.

</details>


### [75] [HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](https://arxiv.org/abs/2509.18190)
*Junseong Shin,Seungwoo Chung,Yunjeong Yang,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 本文提出HazeFlow，一个基于ODE的框架，将ASM重构为ODE，并通过类Rectified Flow学习最佳轨迹进行单步去雾；同时引入MCBM生成非均匀雾霾数据，实现了真实世界去雾的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习去雾方法受限于真实世界成对训练数据匮乏和域间隙问题，导致泛化性差；传统基于大气散射模型(ASM)的方法难以处理真实世界复杂多样的雾霾模式。

Method: 1. **HazeFlow框架**：将大气散射模型(ASM)重新表述为常微分方程(ODE)，并受Rectified Flow启发，学习从雾霾图像到清晰图像的最佳ODE轨迹，实现单步推理去雾。2. **非均匀雾霾生成**：引入马尔可夫链布朗运动(MCBM)来模拟生成逼真的非均匀雾霾模式，以解决真实世界成对数据稀缺的问题。

Result: 通过广泛实验，HazeFlow在各种真实世界去雾基准数据集上均取得了最先进(state-of-the-art)的性能。

Conclusion: HazeFlow通过创新的ODE-based框架和MCBM数据生成方法，有效解决了真实世界去雾中的数据稀缺和复杂性问题，显著提升了去雾性能。

Abstract: Dehazing involves removing haze or fog from images to restore clarity and
improve visibility by estimating atmospheric scattering effects. While deep
learning methods show promise, the lack of paired real-world training data and
the resulting domain gap hinder generalization to real-world scenarios. In this
context, physics-grounded learning becomes crucial; however, traditional
methods based on the Atmospheric Scattering Model (ASM) often fall short in
handling real-world complexities and diverse haze patterns. To solve this
problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM
as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),
HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,
enhancing real-world dehazing performance with only a single inference step.
Additionally, we introduce a non-homogeneous haze generation method using
Markov Chain Brownian Motion (MCBM) to address the scarcity of paired
real-world data. By simulating realistic haze patterns through MCBM, we enhance
the adaptability of HazeFlow to diverse real-world scenarios. Through extensive
experiments, we demonstrate that HazeFlow achieves state-of-the-art performance
across various real-world dehazing benchmark datasets.

</details>


### [76] [TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection](https://arxiv.org/abs/2509.18193)
*Omar H. Khater,Abdul Jabbar Siddiqui,Aiman El-Maleh,M. Shamim Hossain*

Main category: cs.CV

TL;DR: 本文提出一个通过结构化通道剪枝、QAT和TensorRT优化的EcoWeedNet压缩版模型，成功在资源受限的边缘设备上实现高效精准的农业杂草检测，其性能优于YOLOv11n和YOLOv12n。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在农业边缘设备部署时面临资源受限的挑战。

Method: 对EcoWeedNet模型采用结构化通道剪枝、量化感知训练（QAT）和NVIDIA TensorRT加速技术，并在Jetson Orin Nano上进行部署。

Result: 模型大小减少高达68.5%，计算量减少3.2 GFLOPs。推理速度达到184 FPS (FP16)，比基线快28.7%。在CottonWeedDet12数据集上，剪枝39.5%的EcoWeedNet在精度、召回率和mAP50方面均优于YOLOv11n和YOLOv12n，mAP50达到85.9%。

Conclusion: 该压缩版EcoWeedNet模型被证明对精准农业应用而言是高效且有效的。

Abstract: Deploying deep learning models in agriculture is difficult because edge
devices have limited resources, but this work presents a compressed version of
EcoWeedNet using structured channel pruning, quantization-aware training (QAT),
and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the
challenges of pruning complex architectures with residual shortcuts, attention
mechanisms, concatenations, and CSP blocks, the model size was reduced by up to
68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at
FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the
pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n
(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%
mAP50, proving it to be both efficient and effective for precision agriculture.

</details>


### [77] [Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction](https://arxiv.org/abs/2509.18284)
*Yi Gu,Kuniaki Saito,Jiaxin Ma*

Main category: cs.CV

TL;DR: 提出一种新的多模态学习框架，结合增强型模态dropout和对比学习，有效处理医学诊断中的模态缺失和不平衡问题，并在挑战性场景（如单模态可用）下实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 医疗诊断日益依赖多模态数据，现有机器学习模型需要有效融合异构信息，并能在模态缺失时保持鲁棒性，以应对现实世界中的模态不平衡和缺失限制。

Method: 提出一种多模态学习框架，该框架整合了增强型模态dropout和对比学习。具体方法包括引入可学习的模态token以改进对缺失模态的感知融合，并使用融合的多模态表示来增强传统的单模态对比目标。

Result: 在包含视觉和表格模态的大规模临床数据集（用于疾病检测和预测）上，该方法实现了最先进的性能，尤其在仅有单一模态的挑战性和实际场景中表现突出。此外，还成功与CT基础模型集成，展示了其适应性。

Conclusion: 该方法在多模态学习中展现出有效性、效率和泛化能力，提供了一个可扩展、低成本的解决方案，在真实世界的临床应用中具有显著潜力。

Abstract: As medical diagnoses increasingly leverage multimodal data, machine learning
models are expected to effectively fuse heterogeneous information while
remaining robust to missing modalities. In this work, we propose a novel
multimodal learning framework that integrates enhanced modalities dropout and
contrastive learning to address real-world limitations such as modality
imbalance and missingness. Our approach introduces learnable modality tokens
for improving missingness-aware fusion of modalities and augments conventional
unimodal contrastive objectives with fused multimodal representations. We
validate our framework on large-scale clinical datasets for disease detection
and prediction tasks, encompassing both visual and tabular modalities.
Experimental results demonstrate that our method achieves state-of-the-art
performance, particularly in challenging and practical scenarios where only a
single modality is available. Furthermore, we show its adaptability through
successful integration with a recent CT foundation model. Our findings
highlight the effectiveness, efficiency, and generalizability of our approach
for multimodal learning, offering a scalable, low-cost solution with
significant potential for real-world clinical applications. The code is
available at https://github.com/omron-sinicx/medical-modality-dropout.

</details>


### [78] [Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model](https://arxiv.org/abs/2509.18308)
*Yixin Zhang,Ryan Chamberlain,Lawrance Ngo,Kevin Kramer,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 本研究在自建数据集上系统评估了九种主流CNN和ViT分割架构在肺栓塞（PE）分割中的表现。结果显示3D U-Net（ResNet编码器）效果最佳，CNN优于ViT，且预训练有时会降低分割性能；远端栓塞分割仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 为了系统性评估和审计肺栓塞（PE）分割任务中广泛使用的各种模型架构（CNN和Vision Transformer）及其初始化策略（预训练或随机权重）的性能，以识别有效模型并揭示挑战。

Method: 构建了一个包含490个CTPA扫描的密集标注自建数据集。在该数据集上，使用统一测试框架，系统评估了九种主流CNN和ViT分割架构，初始化方式包括预训练和随机权重。

Result: 1. 带有ResNet编码器的3D U-Net仍是PE分割的高效架构。2. 3D模型特别适用于栓塞的形态特征。3. CNN模型在PE分割中通常优于ViT模型。4. 基于分类的预训练，即使在大型PE数据集上，也可能不利于分割性能。5. 不同模型架构在相同数据上训练时，分割性能表现出高度一致的模式。6. 中央和大型栓塞可达到满意精度，但远端栓塞仍具挑战。7. 最佳模型平均Dice分数为0.7131，在测试集中检测出181个栓塞，有49个假阳性和28个假阴性，并已在公开数据集上验证其泛化能力。

Conclusion: 3D CNN模型（特别是带ResNet编码器的3D U-Net）在肺栓塞分割中表现出高效率，优于ViT模型且某些预训练方式可能不利于分割。虽然大型栓塞可被有效分割，但远端栓塞的精确识别仍是亟待解决的挑战。

Abstract: In this study, we curated a densely annotated in-house dataset comprising 490
CTPA scans. Using this dataset, we systematically evaluated nine widely used
segmentation architectures from both the CNN and Vision Transformer (ViT)
families, initialized with either pretrained or random weights, under a unified
testing framework as a performance audit. Our study leads to several important
observations: (1) 3D U-Net with a ResNet encoder remains a highly effective
architecture for PE segmentation; (2) 3D models are particularly well-suited to
this task given the morphological characteristics of emboli; (3) CNN-based
models generally yield superior performance compared to their ViT-based
counterparts in PE segmentation; (4) classification-based pretraining, even on
large PE datasets, can adversely impact segmentation performance compared to
training from scratch, suggesting that PE classification and segmentation may
rely on different sets of discriminative features; (5) different model
architectures show a highly consistent pattern of segmentation performance when
trained on the same data; and (6) while central and large emboli can be
segmented with satisfactory accuracy, distal emboli remain challenging due to
both task complexity and the scarcity of high-quality datasets. Besides these
findings, our best-performing model achieves a mean Dice score of 0.7131 for
segmentation. It detects 181 emboli with 49 false positives and 28 false
negatives from 60 in-house testing scans. Its generalizability is further
validated on public datasets.

</details>


### [79] [Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach](https://arxiv.org/abs/2509.18309)
*Alessa Carbo,Eric Nalisnick*

Main category: cs.CV

TL;DR: 提出了一种新的图神经网络，通过分离时序动态和静态手形配置，显著提高了手语手形识别的准确性，并建立了首个基准。


<details>
  <summary>Details</summary>
Motivation: 手形在手语中扮演基本音位角色，但现有计算方法很少明确建模手形，从而限制了识别准确性和语言分析。

Method: 引入了一种新型图神经网络，该网络将时序动态与静态手形配置分离。它结合了解剖学启发的图结构与对比学习，以解决手形识别中的细微类别差异和时序变化问题。

Result: 建立了手语序列中结构化手形识别的首个基准，在37个手形类别上实现了46%的准确率（基线方法为25%）。

Conclusion: 该新型图神经网络方法能够有效且显著地提升手语手形识别的性能，超越现有基线，并为未来的研究设立了新的基准。

Abstract: Handshapes serve a fundamental phonological role in signed languages, with
American Sign Language employing approximately 50 distinct shapes.
However,computational approaches rarely model handshapes explicitly, limiting
both recognition accuracy and linguistic analysis.We introduce a novel graph
neural network that separates temporal dynamics from static handshape
configurations. Our approach combines anatomically-informed graph structures
with contrastive learning to address key challenges in handshape recognition,
including subtle interclass distinctions and temporal variations. We establish
the first benchmark for structured handshape recognition in signing sequences,
achieving 46% accuracy across 37 handshape classes (with baseline methods
achieving 25%).

</details>


### [80] [Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound](https://arxiv.org/abs/2509.18326)
*Chun Kit Wong,Anders N. Christensen,Cosmin I. Bercea,Julia A. Schnabel,Martin G. Tolsgaard,Aasa Feragen*

Main category: cs.CV

TL;DR: 研究发现，胎儿超声图像中的OOD检测性能受分类任务影响，最佳任务取决于OOD类型（图像或解剖特征漂移）。同时，优异的OOD检测不保证最佳拒绝预测，强调任务选择需与下游应用对齐。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在胎儿超声图像分析中安全部署需要可靠的域外(OOD)检测能力。现有研究主要关注不确定性量化方法，而本文旨在探究分类任务本身对OOD检测性能的影响。

Method: 通过对八种不确定性量化方法和四种分类任务进行实验，评估OOD检测性能。

Result: OOD检测性能随分类任务显著变化，且最佳任务取决于OOD样本是由于图像特征漂移还是解剖特征漂移。此外，优越的OOD检测性能并不保证最佳的拒绝预测。

Conclusion: 在医学图像分析中，必须根据具体的下游应用，调整分类任务的选择和不确定性策略，以确保可靠的OOD检测和模型部署。

Abstract: Reliable out-of-distribution (OOD) detection is important for safe deployment
of deep learning models in fetal ultrasound amidst heterogeneous image
characteristics and clinical settings. OOD detection relies on estimating a
classification model's uncertainty, which should increase for OOD samples.
While existing research has largely focused on uncertainty quantification
methods, this work investigates the impact of the classification task itself.
Through experiments with eight uncertainty quantification methods across four
classification tasks, we demonstrate that OOD detection performance
significantly varies with the task, and that the best task depends on the
defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an
image characteristic shift or ii) an anatomical feature shift. Furthermore, we
reveal that superior OOD detection does not guarantee optimal abstained
prediction, underscoring the necessity to align task selection and uncertainty
strategies with the specific downstream application in medical image analysis.

</details>


### [81] [OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata](https://arxiv.org/abs/2509.18350)
*Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 本文提出OrthoLoC大型数据集和AdHoP精炼技术，旨在利用轻量级正射地理数据，在资源受限环境下实现高精度空中视觉定位，解决了领域偏差问题并显著提升了匹配与定位精度。


<details>
  <summary>Details</summary>
Motivation: 空中视觉定位在多种场景下需高精度且资源受限（无网络/GPS）。现有方法依赖大型数据库或重型3D模型不切实际。研究发现，轻量级、易获取的正射地理数据作为替代方案却鲜有关注。

Method: 1. 提出了OrthoLoC，首个包含16,425张多模态无人机图像的大型数据集，用于解决无人机图像与地理空间数据间的领域偏差，并支持独立评估图像检索和特征匹配。2. 引入了AdHoP精炼技术，可与任何特征匹配器集成。

Result: 1. 通过全面评估，分析了领域偏差、数据分辨率和共视性对定位精度的影响。2. AdHoP技术可将匹配性能提高高达95%，平移误差降低高达63%。

Conclusion: 本研究通过构建OrthoLoC数据集和开发AdHoP精炼技术，有效利用轻量级正射地理数据解决了资源受限环境下的高精度空中视觉定位难题，并显著提升了匹配与定位性能。

Abstract: Accurate visual localization from aerial views is a fundamental problem with
applications in mapping, large-area inspection, and search-and-rescue
operations. In many scenarios, these systems require high-precision
localization while operating with limited resources (e.g., no internet
connection or GNSS/GPS support), making large image databases or heavy 3D
models impractical. Surprisingly, little attention has been given to leveraging
orthographic geodata as an alternative paradigm, which is lightweight and
increasingly available through free releases by governmental authorities (e.g.,
the European Union). To fill this gap, we propose OrthoLoC, the first
large-scale dataset comprising 16,425 UAV images from Germany and the United
States with multiple modalities. The dataset addresses domain shifts between
UAV imagery and geospatial data. Its paired structure enables fair benchmarking
of existing solutions by decoupling image retrieval from feature matching,
allowing isolated evaluation of localization and calibration performance.
Through comprehensive evaluation, we examine the impact of domain shifts, data
resolutions, and covisibility on localization accuracy. Finally, we introduce a
refinement technique called AdHoP, which can be integrated with any feature
matcher, improving matching by up to 95% and reducing translation error by up
to 63%. The dataset and code are available at:
https://deepscenario.github.io/OrthoLoC.

</details>


### [82] [A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data](https://arxiv.org/abs/2509.18354)
*Mehrdad Moradi,Shengzhe Chen,Hao Yan,Kamran Paynabar*

Main category: cs.CV

TL;DR: 本文提出SSDnet，一种基于深度图像先验的单幅图像异常定位方法，无需外部训练数据或参考样本，通过自重建网络实现零样本设置下的异常检测，表现优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 在许多实际场景中，图像异常检测所需的训练数据或参考样本可能无法获得。本文旨在解决这种零样本设置下的单幅图像异常定位问题。

Method: 受DIP启发，提出SSDnet，利用卷积神经网络的归纳偏置。核心思想是自然图像通常具有统一的纹理和模式，异常表现为局部偏差。设计了一个基于补丁的训练框架，直接将输入图像送入网络进行自重建，并通过掩码、补丁混洗、高斯噪声以及基于内积相似度的感知损失来避免学习恒等映射。

Result: SSDnet在MVTec-AD数据集上达到0.99 AUROC和0.60 AUPRC，在织物数据集上达到0.98 AUROC和0.67 AUPRC，性能超越了现有最先进的方法。

Conclusion: SSDnet是一种无需外部训练数据、标签或参考的鲁棒、高效的单幅图像异常定位方法，在零样本设置下表现出色。

Abstract: Anomaly detection in images is typically addressed by learning from
collections of training data or relying on reference samples. In many
real-world scenarios, however, such training data may be unavailable, and only
the test image itself is provided. We address this zero-shot setting by
proposing a single-image anomaly localization method that leverages the
inductive bias of convolutional neural networks, inspired by Deep Image Prior
(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key
assumption is that natural images often exhibit unified textures and patterns,
and that anomalies manifest as localized deviations from these repetitive or
stochastic patterns. To learn the deep image prior, we design a patch-based
training framework where the input image is fed directly into the network for
self-reconstruction, rather than mapping random noise to the image as done in
DIP. To avoid the model simply learning an identity mapping, we apply masking,
patch shuffling, and small Gaussian noise. In addition, we use a perceptual
loss based on inner-product similarity to capture structure beyond pixel
fidelity. Our approach needs no external training data, labels, or references,
and remains robust in the presence of noise or missing pixels. SSDnet achieves
0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the
fabric dataset, outperforming state-of-the-art methods. The implementation code
will be released at https://github.com/mehrdadmoradi124/SSDnet

</details>


### [83] [Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning](https://arxiv.org/abs/2509.18369)
*Riad Ahmed Anonto,Sardar Md. Saffat Zabin,M. Saifur Rahman*

Main category: cs.CV

TL;DR: 本文针对低资源语言（孟加拉语）视觉-语言模型接地困难的问题，提出了一种计算感知型图像字幕生成流水线，并结合利用真实与合成数据、创新性三损失函数（PAL+InfoNCE+OT）来提升模型接地能力，实现了显著的性能提升并缩小了真实-合成数据间的对齐差距。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在低资源语言中进行接地仍然充满挑战，常出现生成流畅文本却与错误对象匹配的现象。这主要归因于配对数据稀缺、翻译枢纽破坏对齐以及以英语为中心的预训练忽视目标语言语义。

Method: 本研究构建了一个计算感知的孟加拉语字幕生成流水线。该流水线基于LaBSE验证的英-孟加拉语对和110k双语提示的合成图像进行训练。模型架构包括一个冻结的MaxViT（提供稳定的视觉补丁）、一个孟加拉语原生的mBART-50（解码器）和一个轻量级模态桥接模块。核心创新是采用三损失目标：补丁对齐损失（PAL）利用解码器交叉注意力对齐真实和合成补丁描述符；InfoNCE损失强制实现全局真实-合成分离；基于Sinkhorn的Optimal Transport (OT) 确保细粒度的补丁平衡对应。

Result: PAL+InfoNCE+OT的协同作用显著改善了模型的接地能力，减少了虚假匹配。模型在Flickr30k-1k数据集上取得了强劲的性能提升（BLEU-4 12.29, METEOR 27.98, BERTScore-F1 71.20），在MSCOCO-1k数据集上也表现出色（BLEU-4 12.00, METEOR 28.14, BERTScore-F1 75.40），均优于强大的交叉熵基线模型，并将真实-合成数据质心差距缩小了41%。

Conclusion: 本研究通过创新的流水线设计和三损失函数有效地解决了低资源语言视觉-语言模型的接地挑战，显著提升了孟加拉语图像字幕的性能，并增强了真实与合成数据间的对齐，为低资源语言的视觉-语言理解提供了有力的解决方案。

Abstract: Grounding vision--language models in low-resource languages remains
challenging, as they often produce fluent text about the wrong objects. This
stems from scarce paired data, translation pivots that break alignment, and
English-centric pretraining that ignores target-language semantics. We address
this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified
EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT
yields stable visual patches, a Bengali-native mBART-50 decodes, and a
lightweight bridge links the modalities. Our core novelty is a tri-loss
objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch
descriptors using decoder cross-attention, InfoNCE enforces global
real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained
patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces
spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR
27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,
BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the
real--synthetic centroid gap by 41%.

</details>


### [84] [TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning](https://arxiv.org/abs/2509.18372)
*Reeshad Khan,John Gauch*

Main category: cs.CV

TL;DR: TinyBEV是一种高效的纯视觉鸟瞰图（BEV）框架，它将大型感知-规划模型（UniAD）的全栈能力蒸馏到一个紧凑、实时的学生模型中，在显著减少参数的同时，保持了自动驾驶所需的各项功能和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的全栈自动驾驶模型（如UniAD）参数量大，计算成本高，难以在资源受限的环境中实现实时部署。本研究旨在弥合大型多模态感知-规划模型与部署就绪的实时自动驾驶系统之间的鸿沟，实现资源受限条件下的全栈驾驶智能。

Method: 本文提出了TinyBEV，一个统一的纯视觉BEV框架。它通过模型无关的多阶段蒸馏策略，将大型规划导向教师模型（UniAD）的全栈能力（包括3D检测、高清地图分割、运动预测、占用率预测和目标导向规划）转移到一个轻量级BEV表示中。该蒸馏策略结合了特征级、输出级和自适应区域感知监督。

Result: TinyBEV将模型参数减少了78%（仅28M参数），运行速度快5倍（11 FPS），且仅需摄像头输入。在nuScenes数据集上，检测达到39.0 mAP，运动预测达到1.08 minADE，碰撞率0.32。这些结果表明，全栈驾驶智能可以在资源受限的环境中得到保留。

Conclusion: 研究结果证明，即使在资源受限的环境中，也能保留全栈驾驶智能，成功弥合了大型多模态感知-规划模型与可部署的实时自动驾驶系统之间的差距。

Abstract: We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework
that distills the full-stack capabilities of a large planning-oriented teacher
(UniAD [19]) into a compact, real-time student model. Unlike prior efficient
camera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the
complete autonomy stack 3D detection, HD-map segmentation, motion forecasting,
occupancy prediction, and goal-directed planning within a streamlined
28M-parameter backbone, achieving a 78% reduction in parameters over UniAD
[19]. Our model-agnostic, multi-stage distillation strategy combines
feature-level, output-level, and adaptive region-aware supervision to
effectively transfer high-capacity multi-modal knowledge to a lightweight BEV
representation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08
minADE for motion forecasting, and a 0.32 collision rate, while running 5x
faster (11 FPS) and requiring only camera input. These results demonstrate that
full-stack driving intelligence can be retained in resource-constrained
settings, bridging the gap between large-scale, multi-modal perception-planning
models and deployment-ready real-time autonomy.

</details>


### [85] [BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking](https://arxiv.org/abs/2509.18387)
*Thomas Gossard,Filip Radovic,Andreas Ziegler,Andrea Zell*

Main category: cs.CV

TL;DR: 针对运动模糊导致快速移动物体检测困难的问题，本文提出新的以模糊中心标记球体并注释模糊属性的标注策略，并发布新数据集。在此基础上，提出BlurBall模型，通过多帧输入和注意力机制实现SOTA球体检测，并提升轨迹预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 运动模糊降低快速移动物体（尤其球类运动中的球）的清晰度，导致检测困难。现有标注方法将球体标记在模糊前端，引入不对称性，并忽略与速度相关的运动线索，影响检测性能。

Method: 1. 提出新的标注策略：将球体标记在模糊条纹的中心，并明确标注模糊属性。 2. 基于此策略，发布一个新的乒乓球检测数据集。 3. 引入BlurBall模型：联合估计球体位置和运动模糊属性。 4. 模型结合多帧输入和Squeeze-and-Excitation等注意力机制。

Result: 1. 新的标注方法能持续提升不同模型的检测性能。 2. BlurBall模型在球体检测中达到SOTA（State-of-the-Art）结果。 3. 利用模糊信息不仅提高了检测精度，还使轨迹预测更可靠。

Conclusion: 巧妙利用运动模糊信息，不仅能显著提高球体检测的准确性，还能实现更可靠的轨迹预测，对实时体育分析具有重要价值。

Abstract: Motion blur reduces the clarity of fast-moving objects, posing challenges for
detection systems, especially in racket sports, where balls often appear as
streaks rather than distinct points. Existing labeling conventions mark the
ball at the leading edge of the blur, introducing asymmetry and ignoring
valuable motion cues correlated with velocity. This paper introduces a new
labeling strategy that places the ball at the center of the blur streak and
explicitly annotates blur attributes. Using this convention, we release a new
table tennis ball detection dataset. We demonstrate that this labeling approach
consistently enhances detection performance across various models. Furthermore,
we introduce BlurBall, a model that jointly estimates ball position and motion
blur attributes. By incorporating attention mechanisms such as
Squeeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art
results in ball detection. Leveraging blur not only improves detection accuracy
but also enables more reliable trajectory prediction, benefiting real-time
sports analytics.

</details>


### [86] [MVP: Motion Vector Propagation for Zero-Shot Video Object Detection](https://arxiv.org/abs/2509.18388)
*Binhua Huang,Ni Wang,Wendong Yao,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 本文提出MVP，一种免训练管道，通过在关键帧上运行开放词汇检测器（OWLv2）并使用压缩域运动向量将检测结果传播到中间帧，显著提高了视频中开放词汇检测的效率。


<details>
  <summary>Details</summary>
Motivation: 在视频的每一帧上运行大型开放词汇检测器虽然准确，但成本高昂，效率低下。

Method: 引入了一个免训练的管道，仅在固定间隔的关键帧上调用OWLv2。通过使用压缩域运动向量（MV）将检测结果传播到中间帧。具体方法包括：使用简单的3x3网格聚合运动向量进行平移和均匀缩放更新，并辅以区域增长检查和可选的单类别切换。该方法无需标签、无需微调，并对所有开放词汇方法使用相同的提示列表。

Result: 在ILSVRC2015-VID（验证数据集）上，MVP达到了mAP@0.5=0.609和mAP@[0.5:0.95]=0.316。在宽松的IoU阈值下，其性能接近逐帧OWLv2-Large。在相同的关键帧调度下，MVP在mAP@0.5上优于基于跟踪器的方法（MOSSE, KCF, CSRT）。虽然有监督的YOLOv12x在mAP@0.5上达到0.631，但MVP是免标签和开放词汇的。

Conclusion: 压缩域传播是减少检测器调用次数同时保持视频中强大零样本覆盖的一种实用方法。

Abstract: Running a large open-vocabulary (Open-vocab) detector on every video frame is
accurate but expensive. We introduce a training-free pipeline that invokes
OWLv2 only on fixed-interval keyframes and propagates detections to
intermediate frames using compressed-domain motion vectors (MV). A simple 3x3
grid aggregation of motion vectors provides translation and uniform-scale
updates, augmented with an area-growth check and an optional single-class
switch. The method requires no labels, no fine-tuning, and uses the same prompt
list for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),
our approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose
intersection-over-union (IoU) thresholds it remains close to framewise
OWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse
localization is largely preserved. Under the same keyframe schedule, MVP
outperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A
supervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled
training, whereas our method remains label-free and open-vocabulary. These
results indicate that compressed-domain propagation is a practical way to
reduce detector invocations while keeping strong zero-shot coverage in videos.
Our code and models are available at https://github.com/microa/MVP.

</details>


### [87] [Improving the color accuracy of lighting estimation models](https://arxiv.org/abs/2509.18390)
*Zitian Zhang,Joshua Urban Davis,Jeanne Phuong Anh Vu,Jiangtao Kuang,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 研究发现，通过预训练白平衡网络预处理输入图像，可以显著提高现有单图HDR光照估计方法的色彩鲁棒性，无需重新训练模型，从而增强AR视觉真实感。


<details>
  <summary>Details</summary>
Motivation: 现有从单张图像估计高动态范围（HDR）光照的方法在增强现实（AR）应用中开辟了新可能性，但其色彩鲁棒性（视觉真实性的关键因素）常被忽视且与其它光照属性（如强度、方向）混淆评估。研究旨在独立探究色彩鲁棒性，并寻求无需修改核心算法的简单改进方案。

Method: 本文不提出新的光照估计算法，而是利用一个包含多样光照色彩的新型HDR数据集，系统评估了几种适应策略。核心方法是测试使用预训练的白平衡网络对输入图像进行预处理，以期提高现有模型的色彩准确性。

Result: 研究结果表明，通过预训练白平衡网络对输入图像进行预处理，能够显著提高色彩鲁棒性，在所有测试场景中均优于其他策略。值得注意的是，该方法无需重新训练原有的光照估计模型。此外，该技术已成功应用于三种最新的SOTA光照估计方法，验证了其通用性。

Conclusion: 简单的适应技术，特别是通过预训练的白平衡网络对输入图像进行预处理，能有效提升现有单图HDR光照估计方法的色彩准确性和鲁棒性，且无需重新训练基础模型，这对于实现AR应用的视觉真实性至关重要。

Abstract: Advances in high dynamic range (HDR) lighting estimation from a single image
have opened new possibilities for augmented reality (AR) applications.
Predicting complex lighting environments from a single input image allows for
the realistic rendering and compositing of virtual objects. In this work, we
investigate the color robustness of such methods -- an often overlooked yet
critical factor for achieving visual realism. While most evaluations conflate
color with other lighting attributes (e.g., intensity, direction), we isolate
color as the primary variable of interest. Rather than introducing a new
lighting estimation algorithm, we explore whether simple adaptation techniques
can enhance the color accuracy of existing models. Using a novel HDR dataset
featuring diverse lighting colors, we systematically evaluate several
adaptation strategies. Our results show that preprocessing the input image with
a pre-trained white balance network improves color robustness, outperforming
other strategies across all tested scenarios. Notably, this approach requires
no retraining of the lighting estimation model. We further validate the
generality of this finding by applying the technique to three state-of-the-art
lighting estimation methods from recent literature.

</details>


### [88] [Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models](https://arxiv.org/abs/2509.18405)
*Sourav Halder,Jinjun Tong,Xinyu Wu*

Main category: cs.CV

TL;DR: 提出一种结合VLM和MLLM的无训练框架，实现支票字段的零样本检测，表现出强大的性能和泛化能力，并可用于生成标注数据。


<details>
  <summary>Details</summary>
Motivation: 支票欺诈检测亟需准确识别关键字段，但传统目标检测方法依赖难以获取的大规模标注数据集，存在资源稀缺问题。

Method: 引入一种新颖的、无需训练的支票字段自动检测框架，该框架利用视觉语言模型（VLM）与多模态大型语言模型（MLLM）的能力，实现零样本检测。

Result: 在包含110张不同格式和布局支票的手工整理数据集上进行定量评估，结果表明该模型具有强大的性能和泛化能力。

Conclusion: 该框架能有效实现支票组件的零样本检测，显著降低实际部署门槛，并可作为生成高质量标注数据集的引导机制，支持开发定制化的实时目标检测模型。

Abstract: Checks remain a foundational instrument in the financial ecosystem,
facilitating substantial transaction volumes across institutions. However,
their continued use also renders them a persistent target for fraud,
underscoring the importance of robust check fraud detection mechanisms. At the
core of such systems lies the accurate identification and localization of
critical fields, such as the signature, magnetic ink character recognition
(MICR) line, courtesy amount, legal amount, payee, and payer, which are
essential for subsequent verification against reference checks belonging to the
same customer. This field-level detection is traditionally dependent on object
detection models trained on large, diverse, and meticulously labeled datasets,
a resource that is scarce due to proprietary and privacy concerns. In this
paper, we introduce a novel, training-free framework for automated check field
detection, leveraging the power of a vision language model (VLM) in conjunction
with a multimodal large language model (MLLM). Our approach enables zero-shot
detection of check components, significantly lowering the barrier to deployment
in real-world financial settings. Quantitative evaluation of our model on a
hand-curated dataset of 110 checks spanning multiple formats and layouts
demonstrates strong performance and generalization capability. Furthermore,
this framework can serve as a bootstrap mechanism for generating high-quality
labeled datasets, enabling the development of specialized real-time object
detection models tailored to institutional needs.

</details>


### [89] [Losing the Plot: How VLM responses degrade on imperfect charts](https://arxiv.org/abs/2509.18425)
*Philip Wootaek Shin,Jack Sampson,Vijaykrishnan Narayanan,Andres Marquez,Mahantesh Halappanavar*

Main category: cs.CV

TL;DR: 现有视觉语言模型在处理真实世界中受损或遮挡的图表时性能急剧下降，易产生幻觉且过度自信。为解决此问题，本文引入了CHART NOISe数据集和评估方法，旨在提升图表理解的鲁棒性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有图表理解基准假设图表清晰且查询基于事实，而真实世界图表常有失真，并需要深层推理。模型在噪声环境下表现不佳且过度自信，亟需提升其在非理想条件下的性能和可靠性。

Method: 评估了ChatGPT 4o、Claude Sonnet 4和Gemini 2.5 Pro模型。引入了CHART NOISe数据集，该数据集结合了图表损坏、遮挡和考试风格的多项选择题，并创新性地加入了“提示反向不一致性”评估。此外，提出了质量过滤和遮挡检测等基线缓解策略。

Result: 在图表受损或遮挡情况下，模型性能急剧下降，并频繁出现幻觉（如数值编造、趋势误解）。模型在退化设置下仍过度自信，生成看似合理但无支持的解释。研究揭示了最先进VLM在图表推理中的系统性脆弱性。

Conclusion: 本文通过对最先进VLM的基准测试，暴露了它们在图表推理中的系统性脆弱性。发布了首个结合损坏、遮挡和反向不一致性的CHART NOISe数据集。提出的缓解策略为未来提升图表理解的鲁棒性和可靠性建立了严格的测试平台。

Abstract: Vision language models (VLMs) show strong results on chart understanding, yet
existing benchmarks assume clean figures and fact based queries. Real world
charts often contain distortions and demand reasoning beyond simple matching.
We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp
performance drops under corruption or occlusion, with hallucinations such as
value fabrication, trend misinterpretation, and entity confusion becoming more
frequent. Models remain overconfident in degraded settings, generating
plausible but unsupported explanations.
  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,
and Reasoning Testing on Noisy and Occluded Input Selections), a dataset
combining chart corruptions, occlusions, and exam style multiple choice
questions inspired by Korea's CSAT English section. A key innovation is prompt
reverse inconsistency, where models contradict themselves when asked to confirm
versus deny the same statement. Our contributions are threefold: (1)
benchmarking state of the art VLMs, exposing systematic vulnerabilities in
chart reasoning; (2) releasing CHART NOISe, the first dataset unifying
corruption, occlusion, and reverse inconsistency; and (3) proposing baseline
mitigation strategies such as quality filtering and occlusion detection.
Together, these efforts establish a rigorous testbed for advancing robustness
and reliability in chart understanding.

</details>


### [90] [CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction](https://arxiv.org/abs/2509.18427)
*Xinyang Wu,Muheng Li,Xia Li,Orso Pusterla,Sairos Safai,Philippe C. Cattin,Antony J. Lomax,Ye Zhang*

Main category: cs.CV

TL;DR: 本文提出一种基于神经表征的4D-MRI重建框架，通过SAN和TMN网络将呼吸运动建模为连续变形，取代传统离散排序方法。该方法在效率和准确性上均优于传统方法，对4D放疗具有重要应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的4D-MRI重建方法（如相位分箱或独立模板扫描）难以捕捉时间变异性，导致工作流程复杂且计算负担重。

Method: 引入一个神经表征框架，将呼吸运动视为由一维替代信号驱动的平滑、连续变形。该方法融合了运动建模与图像重建，通过两个网络协同工作：空间解剖网络（SAN）编码连续的3D解剖表征，时间运动网络（TMN）在Transformer呼吸信号引导下生成时间一致的形变场。

Result: 该无模板、无相位的方法能准确捕捉规律和不规律的呼吸模式，并高精度地保持血管和支气管的连续性。效率显著提升，总处理时间从传统方法的约5小时缩短至15分钟的训练时间，每个3D体素的推理时间不到1秒。在任何呼吸状态下都能准确重建3D图像，且性能优于传统方法。

Conclusion: 该框架能准确重建任意呼吸状态下的3D图像，表现优于传统方法，在4D放射治疗规划和实时自适应治疗中具有强大的应用潜力。

Abstract: Four-dimensional MRI (4D-MRI) is an promising technique for capturing
respiratory-induced motion in radiation therapy planning and delivery.
Conventional 4D reconstruction methods, which typically rely on phase binning
or separate template scans, struggle to capture temporal variability,
complicate workflows, and impose heavy computational loads. We introduce a
neural representation framework that considers respiratory motion as a smooth,
continuous deformation steered by a 1D surrogate signal, completely replacing
the conventional discrete sorting approach. The new method fuses motion
modeling with image reconstruction through two synergistic networks: the
Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical
representation, while a Temporal Motion Network (TMN), guided by
Transformer-derived respiratory signals, produces temporally consistent
deformation fields. Evaluation using a free-breathing dataset of 19 volunteers
demonstrates that our template- and phase-free method accurately captures both
regular and irregular respiratory patterns, while preserving vessel and
bronchial continuity with high anatomical fidelity. The proposed method
significantly improves efficiency, reducing the total processing time from
approximately five hours required by conventional discrete sorting methods to
just 15 minutes of training. Furthermore, it enables inference of each 3D
volume in under one second. The framework accurately reconstructs 3D images at
any respiratory state, achieves superior performance compared to conventional
methods, and demonstrates strong potential for application in 4D radiation
therapy planning and real-time adaptive treatment.

</details>


### [91] [An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects](https://arxiv.org/abs/2509.18451)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony Maida*

Main category: cs.CV

TL;DR: 追踪快速移动的小物体（如壁球）是计算机视觉中的一个挑战。本研究评估了五种基于卡尔曼滤波的跟踪方法在定制壁球数据集上的性能。结果表明，所有方法都存在显著跟踪漂移，误差远高于标准基准，凸显了对专用跟踪方法的需求。


<details>
  <summary>Details</summary>
Motivation: 追踪快速移动且视觉标记小、运动模式不可预测的物体是计算机视觉的难题，尤其在体育机器人领域，需要轻量和准确的跟踪系统。基于卡尔曼滤波的通用跟踪方法在处理快速、不规则弹跳的物体时性能会大幅下降。

Method: 本研究评估了OCSORT、DeepOCSORT、ByteTrack、BoTSORT和StrongSORT这五种最先进的基于卡尔曼滤波的跟踪方法。使用一个包含10,000帧带标注壁球图像（720p-1280p分辨率）的定制数据集进行。分析重点关注推理速度和每图像更新频率如何影响快速移动微小物体的跟踪精度和可靠性。

Result: 实验评估显示，DeepOCSORT实现了最低跟踪误差，平均ADE为31.15像素（ByteTrack为114.3像素）。ByteTrack处理速度最快，平均推理时间为26.6毫秒（DeepOCSORT为26.8毫秒）。然而，所有基于卡尔曼滤波的跟踪器均表现出显著的跟踪漂移，空间误差在3-11厘米之间（ADE值：31-114像素），表明在处理快速移动微小物体的不可预测运动模式方面存在根本局限。误差率比标准物体跟踪基准高3-4倍。

Conclusion: 目前的基于卡尔曼滤波的跟踪方法在追踪快速移动的微小物体（如壁球）时存在显著局限性，导致高误差率。需要开发专门的方法来应对这类具有挑战性的跟踪应用，以实现更高的精度和可靠性。

Abstract: Unpredictable movement patterns and small visual mark make precise tracking
of fast-moving tiny objects like a racquetball one of the challenging problems
in computer vision. This challenge is particularly relevant for sport robotics
applications, where lightweight and accurate tracking systems can improve robot
perception and planning capabilities. While Kalman filter-based tracking
methods have shown success in general object tracking scenarios, their
performance degrades substantially when dealing with rapidly moving objects
that exhibit irregular bouncing behavior. In this study, we evaluate the
performance of five state-of-the-art Kalman filter-based tracking
methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom
dataset containing 10,000 annotated racquetball frames captured at 720p-1280p
resolution. We focus our analysis on two critical performance factors:
inference speed and update frequency per image, examining how these parameters
affect tracking accuracy and reliability for fast-moving tiny objects. Our
experimental evaluation across four distinct scenarios reveals that DeepOCSORT
achieves the lowest tracking error with an average ADE of 31.15 pixels compared
to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest
processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.
However, our results show that all Kalman filter-based trackers exhibit
significant tracking drift with spatial errors ranging from 3-11cm (ADE values:
31-114 pixels), indicating fundamental limitations in handling the
unpredictable motion patterns of fast-moving tiny objects like racquetballs.
Our analysis demonstrates that current tracking approaches require substantial
improvements, with error rates 3-4x higher than standard object tracking
benchmarks, highlighting the need for specialized methodologies for fast-moving
tiny object tracking applications.

</details>


### [92] [MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition](https://arxiv.org/abs/2509.18473)
*Binhua Huang,Wendong Yao,Shaowu Chen,Guoxin Wang,Qingyuan Wang,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCrop是一个运动感知的自适应剪裁模块，利用H.264运动向量在压缩域中高效进行视频动作识别，无需训练，可插拔，并显著提升性能或降低计算量。


<details>
  <summary>Details</summary>
Motivation: 在压缩域中实现高效的视频动作识别，以提升准确性或减少计算资源消耗。

Method: MoCrop模块利用H.264视频中的运动向量来定位运动密集区域，并生成一个单一的剪辑级剪裁应用于所有I帧。它无需训练，不增加参数，可与多种骨干网络结合。采用轻量级管道，包括去噪与合并(DM)、蒙特卡洛采样(MCS)和通过运动密度子矩阵搜索的自适应剪裁(AC)。

Result: 在UCF101数据集上，MoCrop结合ResNet-50，在相同FLOPs下Top-1准确率提升3.5%，或在FLOPs减少26.5%的情况下Top-1准确率提升2.4%。应用于CoViAR，在原始成本下达到89.2%的Top-1准确率，并在计算量从11.6 GFLOPs降至8.5 GFLOPs时达到88.5%的Top-1准确率。在MobileNet-V3、EfficientNet-B1和Swin-B等骨干网络上均实现了持续的性能提升。

Conclusion: MoCrop表现出强大的通用性，使其在压缩域中的视频动作识别任务上具有实用价值，适合实时部署，能够有效提高准确性和计算效率。

Abstract: We introduce MoCrop, a motion-aware adaptive cropping module for efficient
video action recognition in the compressed domain. MoCrop uses motion vectors
that are available in H.264 video to locate motion-dense regions and produces a
single clip-level crop that is applied to all I-frames at inference. The module
is training free, adds no parameters, and can be plugged into diverse
backbones. A lightweight pipeline that includes denoising & merge (DM), Monte
Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix
search yields robust crops with negligible overhead. On UCF101, MoCrop improves
accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy
at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer
FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy
at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6
to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B
indicate strong generality and make MoCrop practical for real-time deployment
in the compressed domain. Our code and models are available at
https://github.com/microa/MoCrop.

</details>


### [93] [Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems](https://arxiv.org/abs/2509.18481)
*Xinyu Wang,Zikun Zhou,Yingjian Li,Xin An,Hongpeng Wang*

Main category: cs.CV

TL;DR: 本文提出CAFC-SE框架，通过在边缘侧使用矢量量化（VQ）和码本将连续视觉特征压缩为离散索引，以在低比特率下提升边缘-云系统中机器分析的性能。


<details>
  <summary>Details</summary>
Motivation: 现有边缘-云系统中的图像编码和特征压缩方法在低比特率条件下表现不佳，因其保留冗余细节或学习到过集中的符号分布，限制了机器分析的性能。

Method: 本文提出Codebook-based Adaptive Feature Compression framework with Semantic Enhancement (CAFC-SE)。它在边缘侧通过矢量量化（VQ）使用码本将连续视觉特征映射为离散索引，并选择性地传输到云端。VQ操作能够将特征向量投射到最近的视觉基元上，从而在低比特率条件下保留更多信息丰富的视觉模式。

Result: 大量的实验结果表明，CAFC-SE在码率和准确性方面均优于现有方法。

Conclusion: CAFC-SE框架有效解决了边缘-云系统中机器分析在低比特率条件下性能不佳的问题，通过其独特的特征压缩机制提供了更高的效率和准确性。

Abstract: Coding images for machines with minimal bitrate and strong analysis
performance is key to effective edge-cloud systems. Several approaches deploy
an image codec and perform analysis on the reconstructed image. Other methods
compress intermediate features using entropy models and subsequently perform
analysis on the decoded features. Nevertheless, these methods both perform
poorly under low-bitrate conditions, as they retain many redundant details or
learn over-concentrated symbol distributions. In this paper, we propose a
Codebook-based Adaptive Feature Compression framework with Semantic
Enhancement, named CAFC-SE. It maps continuous visual features to discrete
indices with a codebook at the edge via Vector Quantization (VQ) and
selectively transmits them to the cloud. The VQ operation that projects feature
vectors onto the nearest visual primitives enables us to preserve more
informative visual patterns under low-bitrate conditions. Hence, CAFC-SE is
less vulnerable to low-bitrate conditions. Extensive experiments demonstrate
the superiority of our method in terms of rate and accuracy.

</details>


### [94] [MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation](https://arxiv.org/abs/2509.18493)
*Md Mostafijur Rahman,Radu Marculescu*

Main category: cs.CV

TL;DR: 本文提出MK-UNet，一种超轻量级、多核U型卷积神经网络，通过多核深度可分离卷积和多注意力机制，在医学图像分割任务上，以极低的计算资源实现了超越SOTA的精度。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的环境（如即时医疗设备）中，对实时、高精度医学诊断的需求日益增长，需要开发超轻量级且高效的医学图像分割解决方案。

Method: 引入MK-UNet网络，核心设计包括：1) 多核深度可分离卷积块（MKDC），用于通过多个核处理图像并捕获复杂的多分辨率空间关系；2) 精密的注意力机制，包括通道、空间和分组门控注意力，以强调图像的显著特征。

Result: MK-UNet具有极小的计算开销（仅0.316M参数和0.314G FLOPs），但在六个二元医学图像基准测试中，比SOTA方法（如TransUNet、UNeXt、MedT等）实现了更高的分割精度。例如，它以远少于TransUNet的参数和FLOPs获得更高的DICE分数，并在参数量大幅减少的情况下，DICE分数比UNeXt提高高达6.7%。

Conclusion: MK-UNet在性能上的飞跃和计算效率的显著提升，使其成为在资源受限环境下进行实时、高保真医学诊断（如即时医疗设备）的无与伦比的解决方案。

Abstract: In this paper, we introduce MK-UNet, a paradigm shift towards
ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image
segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution
block (MKDC) we design to adeptly process images through multiple kernels,
while capturing complex multi-resolution spatial relationships. MK-UNet also
emphasizes the images salient features through sophisticated attention
mechanisms, including channel, spatial, and grouped gated attention. Our
MK-UNet network, with a modest computational footprint of only 0.316M
parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but
also significantly improved segmentation solution that provides higher accuracy
over state-of-the-art (SOTA) methods across six binary medical imaging
benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with
nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively.
Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation
performance, improving the DICE score up to 6.7% margins while operating with
4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent
lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with
much lower computational resources. This leap in performance, coupled with
drastic computational gains, positions MK-UNet as an unparalleled solution for
real-time, high-fidelity medical diagnostics in resource-limited settings, such
as point-of-care devices. Our implementation is available at
https://github.com/SLDGroup/MK-UNet.

</details>


### [95] [BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation](https://arxiv.org/abs/2509.18501)
*Maximilian Fehrentz,Alexander Winkler,Thomas Heiliger,Nazim Haouchine,Christian Heiliger,Nassir Navab*

Main category: cs.CV

TL;DR: BridgeSplat是一种可变形手术导航方法，通过将3D高斯函数绑定到CT网格并进行光度监督优化，实现术中视频与术前CT数据的联动，从而从单目RGB数据获得CT的合理形变。


<details>
  <summary>Details</summary>
Motivation: 解决可变形手术导航中，术中视频数据与术前体积患者数据之间存在的鸿沟，实现两者有效耦合。

Method: 将3D高斯函数绑定到CT网格上，通过光度监督联合优化高斯参数和网格形变。每个高斯函数相对于其父网格三角形进行参数化，确保高斯函数与网格对齐，并使形变能够反向传播以更新CT数据。

Result: 在内脏猪手术和模拟人类肝脏的合成数据上验证了BridgeSplat的有效性，成功地从单目RGB数据中获得了术前CT的合理形变。

Conclusion: BridgeSplat通过耦合术中3D重建与术前CT数据，弥合了手术视频与体积患者数据之间的鸿沟，为可变形手术导航提供了一种新方法。

Abstract: We introduce BridgeSplat, a novel approach for deformable surgical navigation
that couples intraoperative 3D reconstruction with preoperative CT data to
bridge the gap between surgical video and volumetric patient data. Our method
rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian
parameters and mesh deformation through photometric supervision. By
parametrizing each Gaussian relative to its parent mesh triangle, we enforce
alignment between Gaussians and mesh and obtain deformations that can be
propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on
visceral pig surgeries and synthetic data of a human liver under simulation,
showing sensible deformations of the preoperative CT on monocular RGB data.
Code, data, and additional resources can be found at
https://maxfehrentz.github.io/ct-informed-splatting/ .

</details>


### [96] [Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment](https://arxiv.org/abs/2509.18502)
*Wenjie Liu,Hongmin Liu,Lixin Zhang,Bin Fan*

Main category: cs.CV

TL;DR: 针对遥感图像语义分割的无源域适应(SFDA)问题，本文提出一种名为扩散引导标签丰富(DGLE)的新颖伪标签优化框架，通过从少量高质量初始伪标签出发，利用扩散模型传播生成完整且高质量的伪标签集，从而显著提升模型在目标域的性能。


<details>
  <summary>Details</summary>
Motivation: 遥感图像语义分割的无源域适应(SFDA)研究不足。SFDA中的自训练方法，因伪标签集中包含大量噪声，导致同时优化所有伪标签极具挑战，进而限制了自训练的有效性和模型性能。

Method: 本文提出扩散引导标签丰富(DGLE)框架。首先，通过基于置信度过滤和超分辨率增强的伪标签融合方法，利用细节与上下文信息交叉验证，获取少量高质量初始伪标签。其次，利用扩散模型强大的去噪和复杂分布建模能力，将这些不完整且分布不规则的种子伪标签传播为完整且高质量的伪标签集。

Result: 该方法有效规避了直接优化完整伪标签集的难题，显著提升了伪标签的质量。

Conclusion: 本研究的成果是提高了模型在目标域的性能。

Abstract: Research on unsupervised domain adaptation (UDA) for semantic segmentation of
remote sensing images has been extensively conducted. However, research on how
to achieve domain adaptation in practical scenarios where source domain data is
inaccessible namely, source-free domain adaptation (SFDA) remains limited.
Self-training has been widely used in SFDA, which requires obtaining as many
high-quality pseudo-labels as possible to train models on target domain data.
Most existing methods optimize the entire pseudo-label set to obtain more
supervisory information. However, as pseudo-label sets often contain
substantial noise, simultaneously optimizing all labels is challenging. This
limitation undermines the effectiveness of optimization approaches and thus
restricts the performance of self-training. To address this, we propose a novel
pseudo-label optimization framework called Diffusion-Guided Label Enrichment
(DGLE), which starts from a few easily obtained high-quality pseudo-labels and
propagates them to a complete set of pseudo-labels while ensuring the quality
of newly generated labels. Firstly, a pseudo-label fusion method based on
confidence filtering and super-resolution enhancement is proposed, which
utilizes cross-validation of details and contextual information to obtain a
small number of high-quality pseudo-labels as initial seeds. Then, we leverage
the diffusion model to propagate incomplete seed pseudo-labels with irregular
distributions due to its strong denoising capability for randomly distributed
noise and powerful modeling capacity for complex distributions, thereby
generating complete and high-quality pseudo-labels. This method effectively
avoids the difficulty of directly optimizing the complete set of pseudo-labels,
significantly improves the quality of pseudo-labels, and thus enhances the
model's performance in the target domain.

</details>


### [97] [Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.18504)
*Jiaxin Dai,Xiang Xiang*

Main category: cs.CV

TL;DR: 本文提出在双曲空间（Poincaré球模型）中进行粗到细少样本类增量学习（C2FSCIL），通过双曲对比损失、双曲全连接层和最大熵分布增强，有效提升粗细粒度分类精度，并缓解少样本过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 传统欧几里得空间对分层数据表示能力不足，而双曲空间具有优越的表示能力。为更好地解释“粗到细”范式，并提升少样本条件下的性能，作者提出将特征提取器嵌入双曲空间以解决C2FSCIL任务。

Method: 该方法遵循Knowe范式，即对比学习粗类别标签并归一化冻结细类别分类器权重。具体地，将特征提取器嵌入到Poincaré球模型中，使输入图像特征向量落入双曲空间。引入了双曲对比损失和双曲全连接层进行优化和分类。此外，利用双曲空间中的最大熵分布估计细类别特征向量的概率分布，并生成增强特征以缓解少样本训练中的过拟合。

Result: 在C2FSCIL基准测试上的实验表明，该方法有效提高了粗类别和细类别的准确性。

Conclusion: 将特征提取器嵌入双曲空间，并结合双曲对比损失、双曲全连接层和最大熵分布增强，能够有效提升粗到细少样本类增量学习任务的性能。

Abstract: In the field of machine learning, hyperbolic space demonstrates superior
representation capabilities for hierarchical data compared to conventional
Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot
Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe
approach, which contrastively learns coarse class labels and subsequently
normalizes and freezes the classifier weights of learned fine classes in the
embedding space. To better interpret the "coarse-to-fine" paradigm, we propose
embedding the feature extractor into hyperbolic space. Specifically, we employ
the Poincar\'e ball model of hyperbolic space, enabling the feature extractor
to transform input images into feature vectors within the Poincar\'e ball
instead of Euclidean space. We further introduce hyperbolic contrastive loss
and hyperbolic fully-connected layers to facilitate model optimization and
classification in hyperbolic space. Additionally, to enhance performance under
few-shot conditions, we implement maximum entropy distribution in hyperbolic
space to estimate the probability distribution of fine-class feature vectors.
This allows generation of augmented features from the distribution to mitigate
overfitting during training with limited samples. Experiments on C2FSCIL
benchmarks show that our method effectively improves both coarse and fine class
accuracies.

</details>


### [98] [GeoRemover: Removing Objects and Their Causal Visual Artifacts](https://arxiv.org/abs/2509.18538)
*Zixin Zhu,Haoxiang Li,Xuelu Feng,He Wu,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: 本文提出一个几何感知的两阶段框架，通过先移除物体几何信息再进行外观渲染，有效解决了传统图像编辑方法在移除物体时无法同时消除阴影、反射等因果视觉伪影，或缺乏控制力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图像外观的物体移除方法，要么因严格遵循掩码对齐训练而无法移除未明确掩盖的因果视觉伪影（如阴影、反射），要么因采用松散掩码对齐策略而缺乏控制力，可能误删其他物体。这些局限性源于忽略了物体几何存在与其视觉效果之间的因果关系。

Method: 提出一个几何感知的两阶段框架，将物体移除解耦为：(1) 几何移除，使用严格掩码对齐的监督直接从几何信息（如深度）中移除物体，实现结构感知的编辑；(2) 外观渲染，基于更新后的几何信息渲染逼真的RGB图像，隐式处理因果视觉效果。几何移除阶段引入了基于正负样本对的偏好驱动目标，以鼓励移除物体及其因果伪影并避免结构插入。

Result: 实验结果表明，该方法在两个流行基准测试中，移除物体及其相关伪影方面达到了最先进的性能。

Conclusion: 通过解耦几何移除和外观渲染，并利用几何信息中的因果关系，本方法成功克服了现有物体移除方法的局限性，实现了更智能、更全面的图像编辑效果。

Abstract: Towards intelligent image editing, object removal should eliminate both the
target object and its causal visual artifacts, such as shadows and reflections.
However, existing image appearance-based methods either follow strictly
mask-aligned training and fail to remove these causal effects which are not
explicitly masked, or adopt loosely mask-aligned strategies that lack
controllability and may unintentionally over-erase other objects. We identify
that these limitations stem from ignoring the causal relationship between an
object's geometry presence and its visual effects. To address this limitation,
we propose a geometry-aware two-stage framework that decouples object removal
into (1) geometry removal and (2) appearance rendering. In the first stage, we
remove the object directly from the geometry (e.g., depth) using strictly
mask-aligned supervision, enabling structure-aware editing with strong
geometric constraints. In the second stage, we render a photorealistic RGB
image conditioned on the updated geometry, where causal visual effects are
considered implicitly as a result of the modified 3D geometry. To guide
learning in the geometry removal stage, we introduce a preference-driven
objective based on positive and negative sample pairs, encouraging the model to
remove objects as well as their causal visual artifacts while avoiding new
structural insertions. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in removing both objects and their
associated artifacts on two popular benchmarks. The code is available at
https://github.com/buxiangzhiren/GeoRemover.

</details>


### [99] [SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models](https://arxiv.org/abs/2509.18546)
*Yujia Liu,Dingquan Li,Tiejun Huang*

Main category: cs.CV

TL;DR: 针对NR-IQA模型的黑盒攻击迁移性问题，本文提出SEGA攻击，通过集成高斯平滑梯度和扰动滤波，显著提升了攻击的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有针对NR-IQA模型的白盒对抗攻击在更真实的黑盒场景中迁移性差，难以有效揭示模型漏洞并指导鲁棒系统设计，因此亟需提升黑盒攻击的迁移性。

Method: 本文首次提出可迁移的Signed Ensemble Gaussian黑盒攻击（SEGA），其主要思想是通过对源模型应用高斯平滑并集成其平滑梯度来近似目标模型梯度。此外，SEGA还通过专门设计的扰动过滤掩码来去除不当扰动，以确保对抗扰动的不可感知性。

Result: 在CLIVE数据集上的实验结果表明，SEGA攻击展现出优越的迁移能力。

Conclusion: SEGA攻击成功实现了对NR-IQA模型的迁移式黑盒攻击，验证了其有效性，为揭示模型脆弱性提供了有价值的见解。

Abstract: No-Reference Image Quality Assessment (NR-IQA) models play an important role
in various real-world applications. Recently, adversarial attacks against
NR-IQA models have attracted increasing attention, as they provide valuable
insights for revealing model vulnerabilities and guiding robust system design.
Some effective attacks have been proposed against NR-IQA models in white-box
settings, where the attacker has full access to the target model. However,
these attacks often suffer from poor transferability to unknown target models
in more realistic black-box scenarios, where the target model is inaccessible.
This work makes the first attempt to address the challenge of low
transferability in attacking NR-IQA models by proposing a transferable Signed
Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the
gradient of the target model by applying Gaussian smoothing to source models
and ensembling their smoothed gradients. To ensure the imperceptibility of
adversarial perturbations, SEGA further removes inappropriate perturbations
using a specially designed perturbation filter mask. Experimental results on
the CLIVE dataset demonstrate the superior transferability of SEGA, validating
its effectiveness in enabling successful transfer-based black-box attacks
against NR-IQA models.

</details>


### [100] [HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles](https://arxiv.org/abs/2509.18550)
*Mohammad Junayed Hasan,Nabeel Mohammed,Shafin Rahman,Philipp Koehn*

Main category: cs.CV

TL;DR: 本文提出HadaSmileNet，一个高效的特征融合框架，通过无参数的Hadamard乘法融合将Transformer特征与D-Marker生理特征结合，用于区分真实与伪装情绪（微笑）。该方法在四个基准数据集上取得了SOTA性能，并显著降低了计算开销和参数数量。


<details>
  <summary>Details</summary>
Motivation: 区分真实与伪装情绪是一个重要的模式识别挑战，在社会科学、医疗保健和人机交互中具有重要意义。现有结合深度学习和D-Marker特征的多任务学习方法存在计算效率低下、辅助任务监督和复杂损失平衡的问题。

Method: 引入HadaSmileNet，一个新颖的特征融合框架。它通过无参数的乘法交互（特别是Hadamard乘法融合）直接整合基于Transformer的表示与生理学D-Marker特征。系统评估了15种融合策略以确定最佳方案。

Result: Hadamard乘法融合实现了最佳性能并保持了计算效率。该方法在UvA-NEMO (88.7%, +0.8)、MMI (99.7%)、SPOS (98.5%, +0.7) 和BBC (100%, +5.0) 四个基准数据集上达到了深度学习方法的新SOTA结果。与多任务替代方案相比，参数减少了26%，训练过程简化，特征可视化也证明了领域知识直接集成增强了判别力。

Conclusion: HadaSmileNet是一个高效且有效的区分真实与伪装情绪的框架，特别适用于需要实时情感计算能力的多媒体数据挖掘应用中的实际部署。

Abstract: The distinction between genuine and posed emotions represents a fundamental
pattern recognition challenge with significant implications for data mining
applications in social sciences, healthcare, and human-computer interaction.
While recent multi-task learning frameworks have shown promise in combining
deep learning architectures with handcrafted D-Marker features for smile facial
emotion recognition, these approaches exhibit computational inefficiencies due
to auxiliary task supervision and complex loss balancing requirements. This
paper introduces HadaSmileNet, a novel feature fusion framework that directly
integrates transformer-based representations with physiologically grounded
D-Markers through parameter-free multiplicative interactions. Through
systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard
multiplicative fusion achieves optimal performance by enabling direct feature
interactions while maintaining computational efficiency. The proposed approach
establishes new state-of-the-art results for deep learning methods across four
benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS
(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational
analysis reveals 26 percent parameter reduction and simplified training
compared to multi-task alternatives, while feature visualization demonstrates
enhanced discriminative power through direct domain knowledge integration. The
framework's efficiency and effectiveness make it particularly suitable for
practical deployment in multimedia data mining applications that require
real-time affective computing capabilities.

</details>


### [101] [Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction](https://arxiv.org/abs/2509.18566)
*Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang*

Main category: cs.CV

TL;DR: 本文提出一种新的事件引导框架，利用单目事件相机和3D高斯泼溅联合重建动态人物与静态场景。通过统一的语义高斯和事件引导损失，有效解决了快运动和模糊问题，并在基准数据集上取得了最先进的重建效果。


<details>
  <summary>Details</summary>
Motivation: 从单目视频中重建动态人物和静态场景仍然面临挑战，尤其是在快速运动下，RGB帧容易出现运动模糊。事件相机因其微秒级时间分辨率，为动态人物重建提供了优越的感知选择。

Method: 提出一种新颖的事件引导人体-场景重建框架，通过3D高斯泼溅技术，利用单个单目事件相机共同建模人体和场景。采用统一的3D高斯集合，每个高斯具有可学习的语义属性，仅将属于人体的部分进行变形动画，而场景高斯保持静态。为解决模糊问题，提出了一种事件引导损失，匹配连续渲染的模拟亮度变化与事件流。该方法无需外部人体遮罩，简化了高斯集合的管理。

Result: 在ZJU-MoCap-Blur和MMHPSD-Blur两个基准数据集上，实现了最先进的人体-场景重建。相对于强大的基线，在PSNR/SSIM方面有显著提升，LPIPS有所降低，尤其在高速度主体方面表现突出。

Conclusion: 该框架通过利用事件相机和3D高斯泼溅，有效解决了单目视频中快运动和运动模糊下动态人物与静态场景重建的难题，取得了卓越的重建质量，并简化了重建流程。

Abstract: Reconstructing dynamic humans together with static scenes from monocular
videos remains difficult, especially under fast motion, where RGB frames suffer
from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond
temporal resolution, making them a superior sensing choice for dynamic human
reconstruction. Accordingly, we present a novel event-guided human-scene
reconstruction framework that jointly models human and scene from a single
monocular event camera via 3D Gaussian Splatting. Specifically, a unified set
of 3D Gaussians carries a learnable semantic attribute; only Gaussians
classified as human undergo deformation for animation, while scene Gaussians
stay static. To combat blur, we propose an event-guided loss that matches
simulated brightness changes between consecutive renderings with the event
stream, improving local fidelity in fast-moving regions. Our approach removes
the need for external human masks and simplifies managing separate Gaussian
sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers
state-of-the-art human-scene reconstruction, with notable gains over strong
baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

</details>


### [102] [Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought](https://arxiv.org/abs/2509.18571)
*Yuhan Wang,Cheng Liu,Zihan Zhao,Weichao Wu*

Main category: cs.CV

TL;DR: Live-E2T是一个新型框架，通过分解语义元组、在线事件去重和LLM链式思考推理，同时实现了视频威胁监控的实时性和决策可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视频威胁监控方法（无论是基于监督学习还是生成模型）难以同时满足实时性能和决策可解释性这两项要求，存在空白。

Method: Live-E2T框架通过三种协同机制实现：1. 将视频帧解构为紧凑的“人-物-交互-地点”语义元组表示。2. 提出高效的在线事件去重和更新机制，确保实时响应。3. 使用思维链策略微调大型语言模型，使其能对事件序列进行透明逻辑推理并生成威胁评估报告。

Result: 在XD-Violence和UCF-Crime等基准数据集上的实验表明，Live-E2T在威胁检测准确性、实时效率和可解释性方面均显著优于现有SOTA方法。

Conclusion: Live-E2T成功地弥合了实时性能和决策可解释性之间的差距，为视频威胁监控提供了一个高效、准确且可解释的解决方案。

Abstract: Real-time threat monitoring identifies threatening behaviors in video streams
and provides reasoning and assessment of threat events through explanatory
text. However, prevailing methodologies, whether based on supervised learning
or generative models, struggle to concurrently satisfy the demanding
requirements of real-time performance and decision explainability. To bridge
this gap, we introduce Live-E2T, a novel framework that unifies these two
objectives through three synergistic mechanisms. First, we deconstruct video
frames into structured Human-Object-Interaction-Place semantic tuples. This
approach creates a compact, semantically focused representation, circumventing
the information degradation common in conventional feature compression. Second,
an efficient online event deduplication and updating mechanism is proposed to
filter spatio-temporal redundancies, ensuring the system's real time
responsiveness. Finally, we fine-tune a Large Language Model using a
Chain-of-Thought strategy, endow it with the capability for transparent and
logical reasoning over event sequences to produce coherent threat assessment
reports. Extensive experiments on benchmark datasets, including XD-Violence and
UCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art
methods in terms of threat detection accuracy, real-time efficiency, and the
crucial dimension of explainability.

</details>


### [103] [The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers](https://arxiv.org/abs/2509.18582)
*Daiqing Qi,Handong Zhao,Jing Shi,Simon Jenni,Yifei Fan,Franck Dernoncourt,Scott Cohen,Sheng Li*

Main category: cs.CV

TL;DR: 为解决多模态大语言模型（MLLMs）在专业美学理解上的不足，本文提出了PhotoCritique数据集、PhotoEye模型（具备语言引导的多视角视觉融合机制）和PhotoBench基准，实验证明其模型性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在高级美学理解（超越事实识别，如色彩、构图）方面存在显著挑战，尤其在需要摄影技巧和后期处理知识的真实场景中表现不足，因其缺乏对美学元素（如色彩、光影、构图）的深层感知能力。

Method: 1. **数据集**: 引入PhotoCritique，一个大规模、专业且多样化的新数据集，由专业摄影师和爱好者讨论生成。 2. **模型**: 提出PhotoEye模型，采用语言引导的多视角视觉融合机制，以多维度理解图像美学。 3. **基准**: 建立PhotoBench，一个全面且专业的图像美学理解评估基准。

Result: 在现有基准和新提出的PhotoBench上，所提出的PhotoEye模型均展现出优于现有模型的显著优势。

Conclusion: 通过构建专业数据集PhotoCritique、开发具有多视角理解能力的PhotoEye模型以及设立PhotoBench基准，本文成功地从根本上增强了MLLMs对图像美学的理解能力。

Abstract: While editing directly from life, photographers have found it too difficult
to see simultaneously both the blue and the sky. Photographer and curator,
Szarkowski insightfully revealed one of the notable gaps between general and
aesthetic visual understanding: while the former focuses on identifying the
factual element in an image (sky), the latter transcends such object
identification, viewing it instead as an aesthetic component--a pure color
block (blue). Such fundamental distinctions between general (detection,
localization, etc.) and aesthetic (color, lighting, composition, etc.) visual
understanding present a significant challenge for Multimodal Large Language
Models (MLLMs). Although some recent works have made initial explorations, they
are often limited to general and basic aesthetic commonsense. As a result, they
frequently fall short in real-world scenarios (Fig. 1), which require extensive
expertise--including photographic techniques, photo pre/post-processing
knowledge, and more, to provide a detailed analysis and description. To
fundamentally enhance the aesthetics understanding of MLLMs, we first introduce
a novel dataset, PhotoCritique, derived from extensive discussions among
professional photographers and enthusiasts, and characterized by the large
scale, expertise, and diversity. Then, to better learn visual aesthetics from
PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a
languageguided multi-view vision fusion mechanism to understand image
aesthetics from multiple perspectives. Finally, we present a novel benchmark,
PhotoBench, a comprehensive and professional benchmark for aesthetic visual
understanding. On existing benchmarks and PhotoBench, our model demonstrates
clear advantages over existing models.

</details>


### [104] [Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network](https://arxiv.org/abs/2509.18591)
*Pengchao Deng,Shengqi Chen*

Main category: cs.CV

TL;DR: 本文提出一个基于XMem模型的实时肿瘤分割框架，用于MRI引导放疗。该框架在初步开发中表现出合理的分割性能和实时性，尽管详细量化结果因记录丢失未能提供。


<details>
  <summary>Details</summary>
Motivation: 提高MRI引导放疗中肿瘤追踪的精度，以增强癌症治疗的准确性和安全性。

Method: 利用XMem模型（一种记忆增强架构）对长时间电影MRI序列中的肿瘤进行分割。该系统通过有效整合记忆机制，实现肿瘤运动的实时追踪，并在有限标注数据下仍能获得高分割精度。

Result: 详细实验记录已丢失，故未能提供精确的定量结果。但初步印象显示，该基于XMem的框架具有合理的分割性能，并满足临床实时性要求。

Conclusion: 所提出的XMem框架有助于提升MRI引导放疗中肿瘤追踪的精度，对提高癌症治疗的准确性和安全性至关重要。

Abstract: This paper presents an advanced tumor segmentation framework for real-time
MRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method
leverages the XMem model, a memory-augmented architecture, to segment tumors
across long cine-MRI sequences. The proposed system efficiently integrates
memory mechanisms to track tumor motion in real-time, achieving high
segmentation accuracy even under challenging conditions with limited annotated
data. Unfortunately, the detailed experimental records have been lost,
preventing us from reporting precise quantitative results at this stage.
Nevertheless, From our preliminary impressions during development, the
XMem-based framework demonstrated reasonable segmentation performance and
satisfied the clinical real-time requirement. Our work contributes to improving
the precision of tumor tracking during MRI-guided radiotherapy, which is
crucial for enhancing the accuracy and safety of cancer treatments.

</details>


### [105] [SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2509.18593)
*Xiaoman Wu,Lubin Gan,Siying Wu,Jing Zhang,Yunwei Ou,Xiaoyan Sun*

Main category: cs.CV

TL;DR: Multi-contrast MRI超分辨率(MC-MRI SR)旨在利用高分辨率参考增强低分辨率对比度。本文提出SSCM模型，通过动态空间形变、语义感知令牌聚合和空频融合模块，有效解决了空间-语义一致性问题，实现了领先性能和一致性重建。


<details>
  <summary>Details</summary>
Motivation: Multi-contrast MRI超分辨率旨在缩短采集时间、提高成像效率并保留解剖细节。主要挑战在于，尽管目标与参考图像存在结构差异和运动，仍需保持空间-语义一致性，确保解剖结构对齐和连贯。传统方法在空间-语义一致性建模和频率域信息利用方面不足，导致精细对齐差和高频细节恢复不力。

Method: 本文提出空间-语义一致性模型（Spatial-Semantic Consistent Model, SSCM）。SSCM整合了三个关键模块：1) 动态空间形变模块，用于对比度间空间对齐；2) 语义感知令牌聚合模块，用于长程语义一致性；3) 空频融合模块，用于精细结构恢复。

Result: 在公共和私人数据集上的实验表明，SSCM在参数量更少的情况下，实现了最先进的性能，并确保了空间和语义上一致的重建结果。

Conclusion: SSCM模型通过其创新的模块设计，成功解决了Multi-contrast MRI超分辨率中空间-语义一致性与高频细节恢复的挑战。该模型以更少的参数实现了领先的性能，并提供了高质量、一致性的重建，展现出其有效性和优越性。

Abstract: Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims
to enhance low-resolution (LR) contrasts leveraging high-resolution (HR)
references, shortening acquisition time and improving imaging efficiency while
preserving anatomical details. The main challenge lies in maintaining
spatial-semantic consistency, ensuring anatomical structures remain
well-aligned and coherent despite structural discrepancies and motion between
the target and reference images. Conventional methods insufficiently model
spatial-semantic consistency and underuse frequency-domain information, which
leads to poor fine-grained alignment and inadequate recovery of high-frequency
details. In this paper, we propose the Spatial-Semantic Consistent Model
(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast
spatial alignment, a Semantic-Aware Token Aggregation Block for long-range
semantic consistency, and a Spatial-Frequency Fusion Block for fine structure
restoration. Experiments on public and private datasets show that SSCM achieves
state-of-the-art performance with fewer parameters while ensuring spatially and
semantically consistent reconstructions.

</details>


### [106] [OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation](https://arxiv.org/abs/2509.18600)
*Zhuoxiao Chen,Hongyang Yu,Ying Xu,Yadan Luo,Long Duong,Yuan-Fang Li*

Main category: cs.CV

TL;DR: 本文提出OraPO和FactS框架，在资源有限下实现高效医学报告生成，刷新CheXpert Plus SOTA，且显著降低数据和计算需求。


<details>
  <summary>Details</summary>
Motivation: 现有放射报告生成（RRG）方法通常依赖多阶段训练、大规模配对语料库和超大骨干网络，导致计算和数据密集，难以在预算受限情况下应用。

Method: 提出Oracle-educated GRPO (OraPO) 结合FactScore-based reward (FactS)。OraPO通过轻量级预言机将GRPO失败探索转化为直接偏好监督，实现单阶段、纯强化学习训练。FactS通过提取原子临床事实并与真实标签核对，提供密集、可解释的句子级奖励，将学习与诊断证据相结合。

Result: 该框架显著提高了临床挑战性病例的学习效率，在CheXpert Plus数据集上取得了新的SOTA性能（F1值为0.341），并且训练数据量减少了2-3个数量级，仅使用小型视觉语言模型和普通硬件。

Conclusion: OraPO和FactS共同构建了一个紧凑而强大的框架，显著提升了放射报告生成的学习效率和性能，尤其是在资源受限的环境下。

Abstract: Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.

</details>


### [107] [Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation](https://arxiv.org/abs/2509.18602)
*Xu Liu,Yibo Lu,Xinxian Wang,Xinyu Wu*

Main category: cs.CV

TL;DR: 本文提出AMSF，一个免训练、基于参考的框架，用于在扩散模型中实现多个参考风格的可控融合。


<details>
  <summary>Details</summary>
Motivation: 现有基于参考的风格融合方法存在局限性：一是通常只能接受一张风格图像，限制了混合美学和多风格扩展性；二是缺乏平衡多种风格影响的有效机制。

Method: AMSF通过语义token分解模块编码所有风格图像和文本提示，并将其自适应地注入冻结扩散模型的每个交叉注意力层。此外，一个相似度感知重加权模块在每个去噪步骤动态调整分配给每个风格组件的注意力，从而实现平衡且用户可控的风格融合，且无需微调或外部适配器。

Result: 定性和定量评估均表明，AMSF生成的多风格融合结果持续优于现有最先进方法，并且其设计可以无缝扩展到两种或更多风格的融合。

Conclusion: AMSF的这些能力使其成为在扩散模型中实现富有表现力的多风格生成方面迈出的实用一步。

Abstract: We propose Adaptive Multi-Style Fusion (AMSF), a reference-based
training-free framework that enables controllable fusion of multiple reference
styles in diffusion models. Most of the existing reference-based methods are
limited by (a) acceptance of only one style image, thus prohibiting hybrid
aesthetics and scalability to more styles, and (b) lack of a principled
mechanism to balance several stylistic influences. AMSF mitigates these
challenges by encoding all style images and textual hints with a semantic token
decomposition module that is adaptively injected into every cross-attention
layer of an frozen diffusion model. A similarity-aware re-weighting module then
recalibrates, at each denoising step, the attention allocated to every style
component, yielding balanced and user-controllable blends without any
fine-tuning or external adapters. Both qualitative and quantitative evaluations
show that AMSF produces multi-style fusion results that consistently outperform
the state-of-the-art approaches, while its fusion design scales seamlessly to
two or more styles. These capabilities position AMSF as a practical step toward
expressive multi-style generation in diffusion models.

</details>


### [108] [MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2509.18613)
*Yuzhi Wu,Li Xiao,Jun Liu,Guangfeng Jiang,XiangGen Xia*

Main category: cs.CV

TL;DR: 本文提出MLF-4DRCNet，一个用于3D目标检测的两阶段框架，通过点、场景和提议层面的多级融合4D雷达和相机图像，解决了4D雷达点云稀疏和噪声问题，实现了SOTA性能，甚至可与LiDAR-based模型媲美。


<details>
  <summary>Details</summary>
Motivation: 4D毫米波雷达在自动驾驶中具有成本效益和鲁棒性，但其点云稀疏且噪声大，限制了其独立进行3D目标检测。现有的雷达-相机融合方法多采用为LiDAR-相机设计的BEV融合范式，忽略了雷达点云稀疏和不完整的几何特性，且融合仅限于粗糙的场景层面。

Method: 本文提出MLF-4DRCNet，一个用于4D雷达和相机图像多级融合的3D目标检测两阶段框架。该模型整合了点、场景和提议层面的多模态信息，包含三个关键组件：
1.  **Enhanced Radar Point Encoder (ERPE)**：在点层面，利用2D图像实例密集化雷达点云，并通过Triple-Attention Voxel Feature Encoder将其编码为体素。
2.  **Hierarchical Scene Fusion Pooling (HSFP)**：使用可变形注意力动态整合多尺度体素特征和2D图像特征，以捕获场景上下文，并对融合特征进行池化。
3.  **Proposal-Level Fusion Enhancement (PLFE)**：通过融合图像特征来细化区域提议，并进一步与HSFP的池化特征整合。

Result: MLF-4DRCNet在View-of-Delft (VoD) 和 TJ4DRadSet 数据集上均实现了最先进的性能。值得注意的是，它在VoD数据集上取得了与基于LiDAR模型相当的性能。

Conclusion: MLF-4DRCNet通过创新的多级融合策略，有效克服了4D雷达点云的稀疏性和噪声问题，为3D目标检测提供了一个高性能的解决方案，使其性能可与LiDAR系统媲美，展现了4D雷达在自动驾驶感知中的巨大潜力。

Abstract: The emerging 4D millimeter-wave radar, measuring the range, azimuth,
elevation, and Doppler velocity of objects, is recognized for its
cost-effectiveness and robustness in autonomous driving. Nevertheless, its
point clouds exhibit significant sparsity and noise, restricting its standalone
application in 3D object detection. Recent 4D radar-camera fusion methods have
provided effective perception. Most existing approaches, however, adopt
explicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera
fusion, neglecting radar's inherent drawbacks. Specifically, they overlook the
sparse and incomplete geometry of radar point clouds and restrict fusion to
coarse scene-level integration. To address these problems, we propose
MLF-4DRCNet, a novel two-stage framework for 3D object detection via
multi-level fusion of 4D radar and camera images. Our model incorporates the
point-, scene-, and proposal-level multi-modal information, enabling
comprehensive feature representation. It comprises three crucial components:
the Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion
Pooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.
Operating at the point-level, ERPE densities radar point clouds with 2D image
instances and encodes them into voxels via the proposed Triple-Attention Voxel
Feature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D
image features using deformable attention to capture scene context and adopts
pooling to the fused features. PLFE refines region proposals by fusing image
features, and further integrates with the pooled features from HSFP.
Experimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets
demonstrate that MLF-4DRCNet achieves the state-of-the-art performance.
Notably, it attains performance comparable to LiDAR-based models on the VoD
dataset.

</details>


### [109] [Prompt-Guided Dual Latent Steering for Inversion Problems](https://arxiv.org/abs/2509.18619)
*Yichen Wu,Xu Liu,Chenxuan Zhao,Xinyu Wu*

Main category: cs.CV

TL;DR: PDLS框架通过双潜在空间引导（结构与语义），利用LQR优化控制，实现对损坏图像的稳定反演，在保持细节的同时避免语义漂移，无需每图优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将损坏图像反演到扩散模型潜在空间时，难以平衡结构保真度和语义准确性，导致重建图像出现语义漂移，如细节模糊或属性错误。

Method: 提出无训练的Prompt-Guided Dual Latent Steering (PDLS) 框架，基于Rectified Flow模型。PDLS将反演过程分解为结构路径和语义路径两个互补流，并将其建模为最优控制问题，通过线性二次调节器（LQR）推导出闭式解，从而在每一步动态引导生成轨迹，防止语义漂移并保留细节。

Result: 在FFHQ-1K和ImageNet-1K数据集上，针对高斯去模糊、运动去模糊、超分辨率和自由形式修复等任务进行实验，结果表明PDLS生成的重建图像比单一潜在空间基线方法更忠实于原始图像，且与语义信息更一致。

Conclusion: PDLS通过有效的双重引导机制，成功解决了损坏图像反演中的结构与语义平衡难题，实现了高质量且无语义漂移的重建，优于现有单一潜在空间方法。

Abstract: Inverting corrupted images into the latent space of diffusion models is
challenging. Current methods, which encode an image into a single latent
vector, struggle to balance structural fidelity with semantic accuracy, leading
to reconstructions with semantic drift, such as blurred details or incorrect
attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering
(PDLS), a novel, training-free framework built upon Rectified Flow models for
their stable inversion paths. PDLS decomposes the inversion process into two
complementary streams: a structural path to preserve source integrity and a
semantic path guided by a prompt. We formulate this dual guidance as an optimal
control problem and derive a closed-form solution via a Linear Quadratic
Regulator (LQR). This controller dynamically steers the generative trajectory
at each step, preventing semantic drift while ensuring the preservation of fine
detail without costly, per-image optimization. Extensive experiments on FFHQ-1K
and ImageNet-1K under various inversion tasks, including Gaussian deblurring,
motion deblurring, super-resolution and freeform inpainting, demonstrate that
PDLS produces reconstructions that are both more faithful to the original image
and better aligned with the semantic information than single-latent baselines.

</details>


### [110] [Learning neuroimaging models from health system-scale data](https://arxiv.org/abs/2509.18638)
*Yiwei Lyu,Samir Harake,Asadur Chowdury,Soumyanil Banerjee,Rachel Gologorsky,Shixuan Liu,Anna-Katharina Meissner,Akshay Rao,Chenhui Zhao,Akhil Kondepudi,Cheng Jiang,Xinhai Hou,Rushikesh S. Joshi,Volker Neuschmelting,Ashok Srinivasan,Dawn Kleindorfer,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: 本文提出Prima，一个基于22万余份MRI研究训练的首个神经影像视觉语言模型，旨在缓解医疗系统压力，提高诊断效率和公平性，并在诊断准确性上超越现有AI模型。


<details>
  <summary>Details</summary>
Motivation: 神经影像（MRI）需求不断增长，导致医疗系统负担加重、周转时间延长和医生职业倦怠，尤其对资源匮乏地区影响显著。迫切需要AI解决方案来应对这些挑战。

Method: 利用大型学术医疗系统作为数据引擎，开发了Prima模型，它是首个支持真实临床MRI研究输入的神经影像AI基础视觉语言模型。Prima采用分层视觉架构，训练数据超过22万份MRI研究。该模型在一个为期一年的、包含3万份MRI研究的医疗系统范围研究中进行了测试。

Result: Prima在52种主要神经系统疾病（包括肿瘤、炎症、感染和发育性病变）的放射诊断中，平均诊断ROC曲线下面积达到92.0，优于其他最先进的通用和医学AI模型。它能提供可解释的鉴别诊断、放射科医生工作列表优先级和临床转诊建议，并展示了对敏感群体的算法公平性，有助于缓解医疗系统偏见，如缩短低资源人群的周转时间。

Conclusion: 研究结果突显了医疗系统规模视觉语言模型（如Prima）的变革潜力及其在推动AI驱动医疗保健方面的作用，能有效应对神经影像领域的挑战并促进医疗公平性。

Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological
diseases. The global demand for magnetic resonance imaging (MRI) studies has
risen steadily, placing significant strain on health systems, prolonging
turnaround times, and intensifying physician burnout \cite{Chen2017-bt,
Rula2024-qp-1}. These challenges disproportionately impact patients in
low-resource and rural settings. Here, we utilized a large academic health
system as a data engine to develop Prima, the first vision language model (VLM)
serving as an AI foundation for neuroimaging that supports real-world, clinical
MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a
hierarchical vision architecture that provides general and transferable MRI
features. Prima was tested in a 1-year health system-wide study that included
30K MRI studies. Across 52 radiologic diagnoses from the major neurologic
disorders, including neoplastic, inflammatory, infectious, and developmental
lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,
outperforming other state-of-the-art general and medical AI models. Prima
offers explainable differential diagnoses, worklist priority for radiologists,
and clinical referral recommendations across diverse patient demographics and
MRI systems. Prima demonstrates algorithmic fairness across sensitive groups
and can help mitigate health system biases, such as prolonged turnaround times
for low-resource populations. These findings highlight the transformative
potential of health system-scale VLMs and Prima's role in advancing AI-driven
healthcare.

</details>


### [111] [Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation](https://arxiv.org/abs/2509.18639)
*Yuanhuiyi Lyu,Chi Kit Wong,Chenfei Liao,Lutao Jiang,Xu Zheng,Zexin Lu,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 本文提出了一种名为UiG（Understanding-in-Generation）的新型推理框架，通过将统一模型的理解能力融入生成过程，并以图像编辑作为桥梁，显著提升了文本到图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型中的Chain-of-Thought (CoT) 推理方法将理解和生成过程分离，限制了其指导统一模型推理以弥补生成能力不足的能力。

Method: 提出了一种名为UiG（Understanding-in-Generation）的推理框架。其核心思想是在推理过程中，通过强大的理解能力来整合生成指导。具体地，引入“图像编辑”作为桥梁，首先验证生成的图像，并将统一模型的理解融入编辑指令中，随后逐步增强生成的图像，将理解逐步融入生成过程。

Result: UiG框架在文本到图像生成方面表现出显著优于现有方法的性能提升，例如在TIIF基准测试的长提示设置上获得了3.92%的增益。

Conclusion: UiG框架通过将统一模型的理解能力融入生成过程，成功解决了现有推理方法中理解与生成分离的局限性，从而有效提升了文本到图像的生成性能。

Abstract: Recent works have made notable advancements in enhancing unified models for
text-to-image generation through the Chain-of-Thought (CoT). However, these
reasoning methods separate the processes of understanding and generation, which
limits their ability to guide the reasoning of unified models in addressing the
deficiencies of their generative capabilities. To this end, we propose a novel
reasoning framework for unified models, Understanding-in-Generation (UiG),
which harnesses the robust understanding capabilities of unified models to
reinforce their performance in image generation. The core insight of our UiG is
to integrate generative guidance by the strong understanding capabilities
during the reasoning process, thereby mitigating the limitations of generative
abilities. To achieve this, we introduce "Image Editing" as a bridge to infuse
understanding into the generation process. Initially, we verify the generated
image and incorporate the understanding of unified models into the editing
instructions. Subsequently, we enhance the generated image step by step,
gradually infusing the understanding into the generation process. Our UiG
framework demonstrates a significant performance improvement in text-to-image
generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on
the long prompt setting of the TIIF benchmark. The project code:
https://github.com/QC-LY/UiG

</details>


### [112] [Zero-shot Monocular Metric Depth for Endoscopic Images](https://arxiv.org/abs/2509.18642)
*Nicolas Toussaint,Emanuele Colleoni,Ricardo Sanchez-Matilla,Joshua Sutcliffe,Vanessa Thompson,Muhammad Asad,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 本文针对内窥镜图像深度估计领域缺乏基准和数据集的问题，提供了最先进模型的全面基准测试，并发布了一个新的合成数据集EndoSynth，显著提升了模型在真实数据上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管单目相对和度量深度估计在基础模型（特别是基于Transformer的网络）的推动下取得了巨大进展，但在内窥镜图像应用领域仍缺乏鲁棒的基准和高质量数据集。

Method: 1) 对最先进的（度量和相对）深度估计模型在真实的、未见过的内窥镜图像上进行了全面的基准测试。2) 引入并发布了一个新的合成数据集EndoSynth，其中包含内窥镜手术器械、真实的度量深度和分割掩码。3) 使用EndoSynth数据集对深度基础模型进行微调。

Result: 基准测试提供了模型在临床场景中泛化能力和性能的关键见解。使用EndoSynth数据集微调深度基础模型，显著提高了模型在大多数未见真实数据上的精度。

Conclusion: 本工作通过提供基准测试和合成数据集，推动了内窥镜图像深度估计领域的发展，并为未来的研究提供了重要资源。

Abstract: Monocular relative and metric depth estimation has seen a tremendous boost in
the last few years due to the sharp advancements in foundation models and in
particular transformer based networks. As we start to see applications to the
domain of endoscopic images, there is still a lack of robust benchmarks and
high-quality datasets in that area. This paper addresses these limitations by
presenting a comprehensive benchmark of state-of-the-art (metric and relative)
depth estimation models evaluated on real, unseen endoscopic images, providing
critical insights into their generalisation and performance in clinical
scenarios. Additionally, we introduce and publish a novel synthetic dataset
(EndoSynth) of endoscopic surgical instruments paired with ground truth metric
depth and segmentation masks, designed to bridge the gap between synthetic and
real-world data. We demonstrate that fine-tuning depth foundation models using
our synthetic dataset boosts accuracy on most unseen real data by a significant
margin. By providing both a benchmark and a synthetic dataset, this work
advances the field of depth estimation for endoscopic images and serves as an
important resource for future research. Project page, EndoSynth dataset and
trained weights are available at https://github.com/TouchSurgery/EndoSynth.

</details>


### [113] [LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection](https://arxiv.org/abs/2509.18683)
*Lanhu Wu,Zilin Gao,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 提出LEAF-Mamba模型，通过强调局部语义和自适应融合，利用Mamba解决RGB-D显著目标检测中性能与效率的平衡问题，并取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D显著目标检测方法（CNNs或Vision Transformers）在局部感受野或计算复杂度方面存在局限，难以平衡性能与效率。直接应用SSM模型（Mamba）则会缺乏局部语义和不足的跨模态融合。

Method: 提出Local Emphatic and Adaptive Fusion State Space Model (LEAF-Mamba)，包含两个新颖组件：1) 局部强调状态空间模块 (LE-SSM) 用于捕获多尺度局部依赖；2) 基于SSM的自适应融合模块 (AFM) 用于互补的跨模态交互和整合。

Result: LEAF-Mamba在有效性和效率方面均持续优于16种最先进的RGB-D SOD方法。此外，该方法在RGB-T SOD任务上也表现出色，证明了其强大的泛化能力。

Conclusion: LEAF-Mamba通过创新的局部语义强调和自适应跨模态融合机制，成功解决了RGB-D SOD领域中性能与效率的权衡挑战，实现了卓越的检测效果和强大的泛化能力。

Abstract: RGB-D salient object detection (SOD) aims to identify the most conspicuous
objects in a scene with the incorporation of depth cues. Existing methods
mainly rely on CNNs, limited by the local receptive fields, or Vision
Transformers that suffer from the cost of quadratic complexity, posing a
challenge in balancing performance and computational efficiency. Recently,
state space models (SSM), Mamba, have shown great potential for modeling
long-range dependency with linear complexity. However, directly applying SSM to
RGB-D SOD may lead to deficient local semantics as well as the inadequate
cross-modality fusion. To address these issues, we propose a Local Emphatic and
Adaptive Fusion state space model (LEAF-Mamba) that contains two novel
components: 1) a local emphatic state space module (LE-SSM) to capture
multi-scale local dependencies for both modalities. 2) an SSM-based adaptive
fusion module (AFM) for complementary cross-modality interaction and reliable
cross-modality integration. Extensive experiments demonstrate that the
LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in
both efficacy and efficiency. Moreover, our method can achieve excellent
performance on the RGB-T SOD task, proving a powerful generalization ability.

</details>


### [114] [Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification](https://arxiv.org/abs/2509.18692)
*Xinle Gao,Linghui Ye,Zhiyong Xiao*

Main category: cs.CV

TL;DR: 提出一种轻量级食品图像分类算法，通过集成窗口多头注意力和空间注意力机制，在保持高分类精度的同时，显著降低了Vision Transformer模型的参数量和计算复杂度，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 食品工业对生产质量和效率要求提高，食品图像分类在自动化质检、食品安全监管和智能农业中至关重要。然而，现有Vision Transformer模型因参数量大和计算复杂度高而面临挑战。

Method: 提出一种轻量级食品图像分类算法，融合了窗口多头注意力机制（WMHAM）和空间注意力机制（SAM）。WMHAM通过高效窗口划分捕获局部和全局特征以降低计算成本，SAM则自适应强调关键空间区域以增强特征表示。

Result: 在Food-101和Vireo Food-172数据集上，模型分别达到了95.24%和94.33%的准确率。与基线方法相比，参数量和FLOPs显著降低。

Conclusion: 该方法在计算效率和分类性能之间取得了有效平衡，非常适合在资源受限环境中部署。

Abstract: With the rapid development of society and continuous advances in science and
technology, the food industry increasingly demands higher production quality
and efficiency. Food image classification plays a vital role in enabling
automated quality control on production lines, supporting food safety
supervision, and promoting intelligent agricultural production. However, this
task faces challenges due to the large number of parameters and high
computational complexity of Vision Transformer models. To address these issues,
we propose a lightweight food image classification algorithm that integrates a
Window Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism
(SAM). The WMHAM reduces computational cost by capturing local and global
contextual features through efficient window partitioning, while the SAM
adaptively emphasizes key spatial regions to improve discriminative feature
representation. Experiments conducted on the Food-101 and Vireo Food-172
datasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,
respectively, while significantly reducing parameters and FLOPs compared with
baseline methods. These results confirm that the proposed approach achieves an
effective balance between computational efficiency and classification
performance, making it well-suited for deployment in resource-constrained
environments.

</details>


### [115] [OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery](https://arxiv.org/abs/2509.18693)
*Siyi Chen,Kai Wang,Weicong Pang,Ruiming Yang,Ziru Chen,Renjun Gao,Alexis Kai Hon Lau,Dasa Gu,Chenchen Zhang,Cheng Li*

Main category: cs.CV

TL;DR: 提出OSDA三阶段框架，结合SAM和MLLM，实现无需标注的开放集地物覆盖发现、分割和描述。


<details>
  <summary>Details</summary>
Motivation: 遥感开放集地物覆盖分析需要细粒度空间定位和开放语义分类能力，包括检测、分割新对象并赋予可解释语义标签。

Method: OSDA框架包含三个阶段：1. 使用可提示微调分割模型(SAM)进行精确发现和掩膜提取；2. 通过两阶段微调多模态大语言模型(MLLM)进行语义归因和上下文描述；3. 采用LLM作为评判者并结合人工评分评估MLLM。该框架与架构无关且无需标签。

Result: OSDA结合了像素级精度与高级语义理解，有效解决了开放世界遥感解释的关键挑战。它支持在多样化卫星图像上进行鲁棒评估，无需手动标注。

Conclusion: 该工作为动态地物覆盖监测提供了一个可扩展且可解释的解决方案，在自动化制图更新和大规模地球观测分析方面展现出强大潜力。

Abstract: Open-set land-cover analysis in remote sensing requires the ability to
achieve fine-grained spatial localization and semantically open categorization.
This involves not only detecting and segmenting novel objects without
categorical supervision but also assigning them interpretable semantic labels
through multimodal reasoning. In this study, we introduce OSDA, an integrated
three-stage framework for annotation-free open-set land-cover discovery,
segmentation, and description. The pipeline consists of: (1) precise discovery
and mask extraction with a promptable fine-tuned segmentation model (SAM), (2)
semantic attribution and contextual description via a two-phase fine-tuned
multimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring
of the MLLMs evaluation. By combining pixel-level accuracy with high-level
semantic understanding, OSDA addresses key challenges in open-world remote
sensing interpretation. Designed to be architecture-agnostic and label-free,
the framework supports robust evaluation across diverse satellite imagery
without requiring manual annotation. Our work provides a scalable and
interpretable solution for dynamic land-cover monitoring, showing strong
potential for automated cartographic updating and large-scale earth observation
analysis.

</details>


### [116] [Overview of PlantCLEF 2021: cross-domain plant identification](https://arxiv.org/abs/2509.18697)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: PlantCLEF 2021挑战赛旨在评估如何利用植物标本馆数据，提升生物多样性丰富但数据匮乏地区（如圭亚那地盾）的植物自动化识别能力，采用跨域分类方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习植物识别主要依赖野外照片，但数据集中于少数物种和地区（北美、西欧）。生物多样性丰富但数据匮乏的热带地区急需改进识别技术。数字化植物标本馆藏品提供了潜在的大量补充数据。

Method: 采用PlantCLEF 2021挑战赛框架，构建了包含约1,000种圭亚那地盾植物的数据集。任务为跨域分类，训练集结合了数十万份标本馆标本和数千张野外照片，以学习两域间的对应关系，并包含5个形态/功能性状及元数据。测试集仅为野外照片。

Result: 本文介绍了挑战赛的资源和评估方法，总结了参与研究团队使用的识别方法和系统，并分析了主要结果。

Conclusion: 该挑战赛旨在评估利用植物标本馆数据改善数据匮乏地区植物识别的潜力，文章对挑战赛的成果和结果进行了全面分析。

Abstract: Automated plant identification has improved considerably thanks to recent
advances in deep learning and the availability of training data with more and
more field photos. However, this profusion of data concerns only a few tens of
thousands of species, mainly located in North America and Western Europe, much
less in the richest regions in terms of biodiversity such as tropical
countries. On the other hand, for several centuries, botanists have
systematically collected, catalogued and stored plant specimens in herbaria,
especially in tropical regions, and recent efforts by the biodiversity
informatics community have made it possible to put millions of digitised
records online. The LifeCLEF 2021 plant identification challenge (or "PlantCLEF
2021") was designed to assess the extent to which automated identification of
flora in data-poor regions can be improved by using herbarium collections. It
is based on a dataset of about 1,000 species mainly focused on the Guiana
Shield of South America, a region known to have one of the highest plant
diversities in the world. The challenge was evaluated as a cross-domain
classification task where the training set consisted of several hundred
thousand herbarium sheets and a few thousand photos to allow learning a
correspondence between the two domains. In addition to the usual metadata
(location, date, author, taxonomy), the training data also includes the values
of 5 morphological and functional traits for each species. The test set
consisted exclusively of photos taken in the field. This article presents the
resources and evaluations of the assessment carried out, summarises the
approaches and systems used by the participating research groups and provides
an analysis of the main results.

</details>


### [117] [AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping](https://arxiv.org/abs/2509.18699)
*Zedong Zhang,Ying Tai,Jianjun Qian,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 本文提出AGSwap方法和COF数据集，旨在解决文本到图像生成中跨类别对象融合时现有方法产生不一致结果以及缺乏综合性基准数据集的问题。


<details>
  <summary>Details</summary>
Motivation: 文本到图像生成中跨类别对象融合的应用广泛，但现有方法常产生有偏、视觉混乱或语义不一致的结果，且缺少全面的基准数据集限制了该领域的发展。

Method: 提出AGSwap（自适应组交换）方法，包含两部分：1. 组级嵌入交换，通过特征操作融合不同概念的语义属性；2. 自适应组更新，通过平衡评估分数指导动态优化以确保合成一致性。同时引入COF（跨类别对象融合）数据集，一个基于ImageNet-1K和WordNet构建的大规模、分层结构数据集，包含95个超类，10个子类，共451,250个独特融合对。

Result: 广泛实验表明，AGSwap在简单和复杂提示下均优于包括GPT-Image-1在内的现有最先进组合式文本到图像生成方法。

Conclusion: AGSwap及其配套的COF数据集有效解决了跨类别对象融合中的挑战，显著提升了文本到图像生成的一致性和性能。

Abstract: Fusing cross-category objects to a single coherent object has gained
increasing attention in text-to-image (T2I) generation due to its broad
applications in virtual reality, digital media, film, and gaming. However,
existing methods often produce biased, visually chaotic, or semantically
inconsistent results due to overlapping artifacts and poor integration.
Moreover, progress in this field has been limited by the absence of a
comprehensive benchmark dataset. To address these problems, we propose
\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective
approach comprising two key components: (1) Group-wise Embedding Swapping,
which fuses semantic attributes from different concepts through feature
manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism
guided by a balance evaluation score to ensure coherent synthesis.
Additionally, we introduce \textbf{Cross-category Object Fusion (COF)}, a
large-scale, hierarchically structured dataset built upon ImageNet-1K and
WordNet. COF includes 95 superclasses, each with 10 subclasses, enabling
451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap
outperforms state-of-the-art compositional T2I methods, including GPT-Image-1
using simple and complex prompts.

</details>


### [118] [Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries](https://arxiv.org/abs/2509.18705)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 为解决数据稀缺区域植物自动识别难题，PlantCLEF 2019挑战基于圭亚那和北亚马逊地区的1万种植物进行评估，并与人类专家对比。本论文报告了该挑战的资源、方法和主要成果。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习使植物自动识别显著进步，但现有数据仅覆盖少数物种。全球大部分物种（如数据稀缺但多样性高的地区）缺乏足够训练数据，导致识别困难。

Method: 组织了LifeCLEF 2019植物识别挑战（PlantCLEF 2019）。该挑战利用包含1万个物种的数据集，重点关注圭亚那地盾和北亚马逊雨林地区。评估方法包括将系统性能与顶级热带植物专家进行对比。

Result: 本论文呈现了挑战的资源和评估，总结了各参与研究团队采用的方法和系统，并对主要成果进行了分析。（具体结果未在摘要中给出）

Conclusion: 本论文通过报告和分析PlantCLEF 2019挑战，旨在深入了解自动化植物识别在数据稀缺高多样性区域的现状、挑战及系统与人类专家的表现。

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data.
However, this profusion of data only concerns a few tens of thousands of
species, while the planet has nearly 369K. The LifeCLEF 2019 Plant
Identification challenge (or "PlantCLEF 2019") was designed to evaluate
automated identification on the flora of data deficient regions. It is based on
a dataset of 10K species mainly focused on the Guiana shield and the Northern
Amazon rainforest, an area known to have one of the greatest diversity of
plants and animals in the world. As in the previous edition, a comparison of
the performance of the systems evaluated with the best tropical flora experts
was carried out. This paper presents the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.

</details>


### [119] [RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images](https://arxiv.org/abs/2509.18711)
*Ke Li,Di Wang,Ting Wang,Fuyu Dong,Yiming Zhang,Luyao Zhang,Xiangyu Wang,Shaofeng Li,Quan Wang*

Main category: cs.CV

TL;DR: RSVG-ZeroOV是一个免训练框架，利用冻结的通用基础模型实现零样本开放词汇遥感视觉定位，无需昂贵数据或耗时微调，并表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有遥感视觉定位（RSVG）方法受限于封闭词汇或过度依赖昂贵数据和耗时微调，限制了其在开放世界场景中的应用。

Method: 提出免训练框架RSVG-ZeroOV。它包括三个阶段：(i) 概述：使用视觉-语言模型（VLM）获取文本查询与视觉区域的交叉注意力图。(ii) 聚焦：利用扩散模型（DM）的先验知识补充VLM可能忽略的对象结构和形状信息。(iii) 演化：引入注意力演化模块以抑制不相关激活，生成纯化的目标分割掩码。

Result: RSVG-ZeroOV无需繁琐的任务特定训练，提供高效且可扩展的解决方案。实验结果表明，该框架持续优于现有的弱监督和零样本方法。

Conclusion: RSVG-ZeroOV成功地利用冻结的通用基础模型，在零样本开放词汇遥感视觉定位任务中取得了卓越性能，提供了一种高效、可扩展的替代方案。

Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote
sensing images based on free-form natural language expressions. Existing
approaches are typically constrained to closed-set vocabularies, limiting their
applicability in open-world scenarios. While recent attempts to leverage
generic foundation models for open-vocabulary RSVG, they overly rely on
expensive high-quality datasets and time-consuming fine-tuning. To address
these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework
that aims to explore the potential of frozen generic foundation models for
zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key
stages: (i) Overview: We utilize a vision-language model (VLM) to obtain
cross-attention\footnote[1]{In this paper, although decoder-only VLMs use
self-attention over all tokens, we refer to the image-text interaction part as
cross-attention to distinguish it from pure visual self-attention.}maps that
capture semantic correlations between text queries and visual regions. (ii)
Focus: By leveraging the fine-grained modeling priors of a diffusion model
(DM), we fill in gaps in structural and shape information of objects, which are
often overlooked by VLM. (iii) Evolve: A simple yet effective attention
evolution module is introduced to suppress irrelevant activations, yielding
purified segmentation masks over the referred objects. Without cumbersome
task-specific training, RSVG-ZeroOV offers an efficient and scalable solution.
Extensive experiments demonstrate that the proposed framework consistently
outperforms existing weakly-supervised and zero-shot methods.

</details>


### [120] [What Makes You Unique? Attribute Prompt Composition for Object Re-Identification](https://arxiv.org/abs/2509.18715)
*Yingquan Wang,Pingping Zhang,Chong Sun,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出一个名为属性提示组合（APC）的框架，通过结合文本语义和视觉-语言模型的训练策略，有效提升目标重识别（ReID）模型的判别性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有ReID模型在单域或跨域场景中存在局限：单域模型易过拟合于特定领域特征，而跨域模型的归一化策略可能抑制身份判别性线索，从而限制了模型的实际应用性。研究旨在解决ReID模型判别性和泛化性不足的问题。

Method: 本文提出Attribute Prompt Composition (APC) 框架。首先，设计了Attribute Prompt Generator (APG)，其包含一个提供丰富语义描述的Semantic Attribute Dictionary (SAD) 和一个自适应组合属性生成判别性特征的Prompt Composition Module (PCM)，以利用文本语义。其次，借鉴视觉-语言模型（VLM）的强大泛化能力，提出Fast-Slow Training Strategy (FSTS)，通过Fast Update Stream (FUS) 快速学习ReID特有的判别知识，同时通过Slow Update Stream (SUS) 保留预训练VLM的通用知识，通过相互作用平衡判别性和泛化性并减轻过拟合。

Result: 在传统和域泛化ReID数据集上进行的广泛实验表明，所提出的框架超越了现有最先进方法，在判别性和泛化性方面均展现出卓越的性能。

Conclusion: APC框架通过有效地结合文本语义和新颖的快速-慢速训练策略，成功解决了ReID领域中判别性与泛化性之间的平衡问题，显著提升了模型性能，使其在复杂现实世界场景中更具鲁棒性。

Abstract: Object Re-IDentification (ReID) aims to recognize individuals across
non-overlapping camera views. While recent advances have achieved remarkable
progress, most existing models are constrained to either single-domain or
cross-domain scenarios, limiting their real-world applicability. Single-domain
models tend to overfit to domain-specific features, whereas cross-domain models
often rely on diverse normalization strategies that may inadvertently suppress
identity-specific discriminative cues. To address these limitations, we propose
an Attribute Prompt Composition (APC) framework, which exploits textual
semantics to jointly enhance discrimination and generalization. Specifically,
we design an Attribute Prompt Generator (APG) consisting of a Semantic
Attribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an
over-complete attribute dictionary to provide rich semantic descriptions, while
PCM adaptively composes relevant attributes from SAD to generate discriminative
attribute-aware features. In addition, motivated by the strong generalization
ability of Vision-Language Models (VLM), we propose a Fast-Slow Training
Strategy (FSTS) to balance ReID-specific discrimination and generalizable
representation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)
to rapidly acquire ReID-specific discriminative knowledge and a Slow Update
Stream (SUS) to retain the generalizable knowledge inherited from the
pre-trained VLM. Through a mutual interaction, the framework effectively
focuses on ReID-relevant features while mitigating overfitting. Extensive
experiments on both conventional and Domain Generalized (DG) ReID datasets
demonstrate that our framework surpasses state-of-the-art methods, exhibiting
superior performances in terms of both discrimination and generalization. The
source code is available at https://github.com/AWangYQ/APC.

</details>


### [121] [Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment](https://arxiv.org/abs/2509.18717)
*Tong Zhang,Kuofeng Gao,Jiawang Bai,Leo Yu Zhang,Xin Yin,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: 本文提出OTCCLIP，一个基于最优传输的框架，通过细粒度特征匹配和对齐重建图文对，以防御CLIP模型免受投毒攻击，并显著提高模型在受攻击数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: CLIP模型因训练数据源自互联网，易受有针对性的数据投毒和后门攻击。现有防御方法仅依赖全局表示匹配，忽略了视觉和文本特征的细粒度信息，可能引入错误匹配并损害预训练。

Method: 提出OTCCLIP框架，基于最优传输（Optimal Transport）重建图像-文本对。该方法设计了一种新的最优传输距离度量，用于细粒度视觉和文本特征集之间的匹配，并据此重新分配文本描述。此外，通过最优传输目标函数，鼓励模态内和模态间的细粒度对齐，以减少不匹配对的负面影响。

Result: 实验证明OTCCLIP能有效降低投毒攻击的攻击成功率。与现有方法相比，OTCCLIP显著提升了在投毒数据集上训练的CLIP模型的零样本（zero-shot）和线性探测（linear probing）性能。

Conclusion: OTCCLIP通过利用最优传输进行细粒度特征匹配和对齐，成功防御了针对CLIP模型的投毒攻击，并显著增强了模型在受污染数据上的鲁棒性和性能，解决了现有方法在细粒度特征利用上的局限。

Abstract: Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)
models are threatened by targeted data poisoning and backdoor attacks due to
massive training image-caption pairs crawled from the Internet. Previous
defense methods correct poisoned image-caption pairs by matching a new caption
for each image. However, the matching process relies solely on the global
representations of images and captions, overlooking fine-grained features of
visual and textual features. It may introduce incorrect image-caption pairs and
harm the CLIP pre-training. To address their limitations, we propose an Optimal
Transport-based framework to reconstruct image-caption pairs, named OTCCLIP. We
propose a new optimal transport-based distance measure between fine-grained
visual and textual feature sets and re-assign new captions based on the
proposed optimal transport distance. Additionally, to further reduce the
negative impact of mismatched pairs, we encourage the inter- and intra-modality
fine-grained alignment by employing optimal transport-based objective
functions. Our experiments demonstrate that OTCCLIP can successfully decrease
the attack success rates of poisoning attacks. Also, compared to previous
methods, OTCCLIP significantly improves CLIP's zero-shot and linear probing
performance trained on poisoned datasets.

</details>


### [122] [Knowledge Transfer from Interaction Learning](https://arxiv.org/abs/2509.18733)
*Yilin Gao,Kangyi Chen,Zhongxing Peng,Hengjie Lu,Shugong Xu*

Main category: cs.CV

TL;DR: 本文提出LFI框架，通过建模交互过程，高效利用VLM的交互模式知识，解决VFM在知识迁移上的局限，显著提升了VFM在多视觉任务中的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉基础模型（VFMs）在从视觉语言模型（VLMs）迁移知识时面临根本性限制。VLMs擅长建模跨模态交互，而VFMs忽视底层交互过程，采用面向结果的范式，导致知识迁移受阻，并限制了在多样视觉任务中的泛化能力。

Method: 提出认知启发式框架LFI（Learning from Interactions），通过将视觉理解明确建模为交互过程，弥补现有VFMs的不足。核心在于捕获预训练VLM中编码的动态交互模式，以实现高效知识迁移。该方法包含两项技术创新：维护跨网络层持久关系结构的“交互查询”（Interaction Queries），以及源自VLM跨模态注意力机制的“基于交互的监督”（interaction-based supervision）。

Result: 在多个基准测试中显示持续改进，在TinyImageNet分类上实现3.3的绝对增益，在COCO检测/分割上实现1.6mAP/2.4AP的绝对增益，且参数开销小、收敛快。在跨域设置中表现出色，在PACS和VLCS上实现2.4和9.3的零样本改进。人类评估证实其认知对齐性，在语义一致性上比现有方法高2.7倍。

Conclusion: LFI框架通过显式建模视觉理解的交互过程，成功解决了VFMs在知识迁移上的局限性，显著提升了其在多种视觉任务和跨域场景下的性能、效率及泛化能力，并具有良好的认知对齐性。

Abstract: Current visual foundation models (VFMs) face a fundamental limitation in
transferring knowledge from vision language models (VLMs), while VLMs excel at
modeling cross-modal interactions through unified representation spaces,
existing VFMs predominantly adopt result-oriented paradigms that neglect the
underlying interaction processes. This representational discrepancy hinders
effective knowledge transfer and limits generalization across diverse vision
tasks. We propose Learning from Interactions (LFI), a cognitive-inspired
framework that addresses this gap by explicitly modeling visual understanding
as an interactive process. Our key insight is that capturing the dynamic
interaction patterns encoded in pre-trained VLMs enables more faithful and
efficient knowledge transfer to VFMs. The approach centers on two technical
innovations, Interaction Queries, which maintain persistent relational
structures across network layers, and interaction-based supervision, derived
from the cross-modal attention mechanisms of VLMs. Comprehensive experiments
demonstrate consistent improvements across multiple benchmarks, achieving 3.3
and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO
detection/segmentation respectively, with minimal parameter overhead and faster
convergence. The framework particularly excels in cross-domain settings,
delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human
evaluations further confirm its cognitive alignment, outperforming
result-oriented methods by 2.7 times in semantic consistency metrics.

</details>


### [123] [HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection](https://arxiv.org/abs/2509.18738)
*Ruichao Hou,Xingyuan Li,Tongwei Ren,Dongming Zhou,Gangshan Wu,Jinde Cao*

Main category: cs.CV

TL;DR: 本文提出HyPSAM，一个混合提示驱动的SAM模型，用于解决RGB-T显著目标检测中边界不精确和特征融合不足的问题。该模型通过动态融合网络生成高质量视觉提示，并利用即插即用细化网络结合文本、掩码和边界框提示引导SAM进行显著性图优化，实现了SOTA性能和出色的通用性。


<details>
  <summary>Details</summary>
Motivation: RGB-T显著目标检测（SOD）中，由于固有的特征融合不足和数据稀缺性，学习精确的物体边界和完整的显著目标仍然具有挑战性。

Method: 本文提出一种新颖的混合提示驱动的Segment Anything Model (HyPSAM)。首先，设计了一个动态融合网络（DFNet）生成高质量的初始显著图作为视觉提示，该网络通过动态卷积和多分支解码促进自适应的跨模态交互。其次，提出了一个即插即用细化网络（P2RNet）作为通用优化策略，利用混合提示（文本、掩码和边界框提示）引导SAM细化显著图。

Result: HyPSAM在三个公共数据集上取得了最先进的性能。值得注意的是，该方法具有显著的通用性，能够无缝集成到不同的RGB-T SOD方法中，并带来显著的性能提升。

Conclusion: HyPSAM通过利用SAM的零样本泛化能力和创新的提示工程，有效解决了RGB-T SOD中的核心挑战，实现了卓越的性能和通用性，突出了提示工程在该领域的巨大潜力。

Abstract: RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent
objects by integrating complementary information from RGB and thermal
modalities. However, learning the precise boundaries and complete objects
remains challenging due to the intrinsic insufficient feature fusion and the
extrinsic limitations of data scarcity. In this paper, we propose a novel
hybrid prompt-driven segment anything model (HyPSAM), which leverages the
zero-shot generalization capabilities of the segment anything model (SAM) for
RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that
generates high-quality initial saliency maps as visual prompts. DFNet employs
dynamic convolution and multi-branch decoding to facilitate adaptive
cross-modality interaction, overcoming the limitations of fixed-parameter
kernels and enhancing multi-modal feature representation. Moreover, we propose
a plug-and-play refinement network (P2RNet), which serves as a general
optimization strategy to guide SAM in refining saliency maps by using hybrid
prompts. The text prompt ensures reliable modality input, while the mask and
box prompts enable precise salient object localization. Extensive experiments
on three public datasets demonstrate that our method achieves state-of-the-art
performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating
with different RGB-T SOD methods to achieve significant performance gains,
thereby highlighting the potential of prompt engineering in this field. The
code and results of our method are available at:
https://github.com/milotic233/HyPSAM.

</details>


### [124] [TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing](https://arxiv.org/abs/2509.18743)
*Susmit Neogi*

Main category: cs.CV

TL;DR: 本文提出TriFusion-AE，一种结合文本先验、深度图和LiDAR点云的多模态交叉注意力自动编码器，显著提升了在强噪声和对抗性攻击下LiDAR点云重建的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云在自动驾驶和机器人领域容易受到噪声、遮挡和对抗性攻击的影响。现有自动编码器在具有挑战性的真实世界条件下，其去噪和重建性能会下降。

Method: 本文提出TriFusion-AE，一个多模态交叉注意力自动编码器，通过整合文本先验、多视图图像的单目深度图以及LiDAR点云来提高鲁棒性。该模型通过对齐文本的语义线索、图像的几何（深度）特征和LiDAR的空间结构，学习对随机噪声和对抗性扰动具有弹性的表示。该框架被设计为模型无关的，可与任何基于CNN的点云自动编码器无缝集成，并在nuScenes-mini数据集上进行评估。

Result: 研究发现，该模型在轻度扰动下增益有限，但在强对抗性攻击和重噪声下，实现了显著更鲁棒的重建，而基于CNN的自动编码器在这种条件下会失效。

Conclusion: TriFusion-AE的多模态融合框架通过结合不同模态的信息，有效提高了LiDAR点云在极端挑战条件下的重建鲁棒性，并且其模型无关性使其能广泛应用于联合表示学习。

Abstract: LiDAR-based perception is central to autonomous driving and robotics, yet raw
point clouds remain highly vulnerable to noise, occlusion, and adversarial
corruptions. Autoencoders offer a natural framework for denoising and
reconstruction, but their performance degrades under challenging real-world
conditions. In this work, we propose TriFusion-AE, a multimodal cross-attention
autoencoder that integrates textual priors, monocular depth maps from
multi-view images, and LiDAR point clouds to improve robustness. By aligning
semantic cues from text, geometric (depth) features from images, and spatial
structure from LiDAR, TriFusion-AE learns representations that are resilient to
stochastic noise and adversarial perturbations. Interestingly, while showing
limited gains under mild perturbations, our model achieves significantly more
robust reconstruction under strong adversarial attacks and heavy noise, where
CNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to
reflect realistic low-data deployment scenarios. Our multimodal fusion
framework is designed to be model-agnostic, enabling seamless integration with
any CNN-based point cloud autoencoder for joint representation learning.

</details>


### [125] [COLT: Enhancing Video Large Language Models with Continual Tool Usage](https://arxiv.org/abs/2509.18754)
*Yuyang Liu,Xinyuan Shi,Bang Yang,Peilin Zhou,Jiahua Dong,Long Chen,Ian Reid,Xiaondan Liang*

Main category: cs.CV

TL;DR: 本文提出COLT框架，旨在解决视频大语言模型在连续变化的工具流中学习和使用工具时面临的灾难性遗忘问题。COLT通过可学习的工具码本实现持续学习和动态工具选择，并在新收集的VideoToolBench数据集和现有基准上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型（Video LLMs）在工具使用方面，依赖固定的工具库，难以适应工具数据持续演变和流式更新的真实环境，且面临对过去学习工具的“灾难性遗忘”问题。

Method: 提出COntinuaL Tool usage (COLT) 框架，以增强开源视频LLM的持续工具使用能力。COLT核心在于引入一个可学习的工具码本作为工具特有的记忆系统，并基于用户指令与码本中工具特征的相似性动态选择相关工具。此外，为释放视频LLM的工具使用潜力，收集了一个以视频为中心的工具使用指令微调数据集VideoToolBench。

Result: 在现有视频LLM基准测试和专门的VideoToolBench数据集上进行的大量实验表明，所提出的COLT框架取得了最先进的性能。

Conclusion: COLT成功地使开源视频LLMs能够在连续的工具流中自动获取工具使用能力，同时避免了对已学习工具的灾难性遗忘，有效解决了现有方法在动态工具环境下的泛化挑战，并展示了卓越的性能。

Abstract: The success of Large Language Models (LLMs) has significantly propelled the
research of video understanding. To harvest the benefits of well-trained expert
models (i.e., tools), video LLMs prioritize the exploration of tool usage
capabilities. Existing methods either prompt closed-source LLMs or employ the
instruction tuning paradigm for tool-use fine-tuning. These methods, however,
assume an established repository of fixed tools and struggle to generalize to
real-world environments where tool data is perpetually evolving and streaming
in. To this end, we propose to enhance open-source video LLMs with COntinuaL
Tool usage (termed COLT), which automatically acquires tool-use ability in a
successive tool stream without suffering 'catastrophic forgetting' of the past
learned tools. Specifically, our COLT incorporates a learnable tool codebook as
a tool-specific memory system. Then relevant tools are dynamically selected
based on the similarity between user instruction and tool features within the
codebook. To unleash the tool usage potential of video LLMs, we collect a
video-centric tool-use instruction tuning dataset VideoToolBench. Extensive
experiments on both previous video LLM benchmarks and the tool-use-specific
VideoToolBench dataset demonstrate the state-of-the-art performance of our
proposed COLT.

</details>


### [126] [FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation](https://arxiv.org/abs/2509.18759)
*Zhaorui Wang,Yi Gu,Deming Zhou,Renjing Xu*

Main category: cs.CV

TL;DR: FixingGS提出一种免训练方法，利用现有扩散模型蒸馏，增强稀疏视角下的3DGS重建，有效去除伪影并修复内容，同时确保多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 3DGS在稀疏视角下进行3D场景重建时，由于视觉信息不足，易产生明显伪影。现有基于生成先验的方法虽能去除伪影，但难以保证多视角一致性，导致结构模糊和细节不真实。

Method: 本文提出FixingGS，一种免训练方法，充分利用现有扩散模型的能力来增强稀疏视角3DGS重建。核心是其蒸馏方法，提供更准确和跨视角一致的扩散先验，从而有效去除伪影和进行内容修复。此外，还提出了自适应渐进增强方案，进一步优化约束不足区域的重建。

Result: 大量的实验证明，FixingGS在视觉质量和重建性能方面超越了现有的最先进方法。

Conclusion: FixingGS通过免训练的扩散模型蒸馏方法，有效解决了稀疏视角3DGS重建中的伪影和多视角一致性问题，显著提升了重建质量和性能。

Abstract: Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in
3D reconstruction and novel view synthesis. However, reconstructing 3D scenes
from sparse viewpoints remains highly challenging due to insufficient visual
information, which results in noticeable artifacts persisting across the 3D
representation. To address this limitation, recent methods have resorted to
generative priors to remove artifacts and complete missing content in
under-constrained areas. Despite their effectiveness, these approaches struggle
to ensure multi-view consistency, resulting in blurred structures and
implausible details. In this work, we propose FixingGS, a training-free method
that fully exploits the capabilities of the existing diffusion model for
sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our
distillation approach, which delivers more accurate and cross-view coherent
diffusion priors, thereby enabling effective artifact removal and inpainting.
In addition, we propose an adaptive progressive enhancement scheme that further
refines reconstructions in under-constrained regions. Extensive experiments
demonstrate that FixingGS surpasses existing state-of-the-art methods with
superior visual quality and reconstruction performance. Our code will be
released publicly.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [127] [A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services](https://arxiv.org/abs/2509.18101)
*Guanzhong Pan,Haibo Wang*

Main category: cs.AI

TL;DR: 本文分析了商业LLM服务与本地部署开源LLM的成本效益，以帮助组织确定本地部署的经济可行性，并提供基于使用量和性能的盈亏平衡点。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的普及，组织面临选择商业云服务或本地部署开源模型的决策。商业服务虽易用且可扩展，但存在数据隐私、供应商锁定和长期成本问题；本地部署则受隐私和成本驱动。因此，需要一个框架来评估何时本地部署在经济上是可行的。

Method: 提出了一个成本效益分析框架。该方法考虑了最新开源模型（如Qwen, Llama, Mistral等）的硬件需求、运营开支和性能基准，并将其本地部署的总成本与主要云服务商的订阅费用进行比较。

Result: 研究结果提供了一个基于使用水平和性能需求的本地部署盈亏平衡点预估。

Conclusion: 这些发现为组织规划其LLM策略提供了一个实用的框架。

Abstract: Large language models (LLMs) are becoming increasingly widespread.
Organizations that want to use AI for productivity now face an important
decision. They can subscribe to commercial LLM services or deploy models on
their own infrastructure. Cloud services from providers such as OpenAI,
Anthropic, and Google are attractive because they provide easy access to
state-of-the-art models and are easy to scale. However, concerns about data
privacy, the difficulty of switching service providers, and long-term operating
costs have driven interest in local deployment of open-source models. This
paper presents a cost-benefit analysis framework to help organizations
determine when on-premise LLM deployment becomes economically viable compared
to commercial subscription services. We consider the hardware requirements,
operational expenses, and performance benchmarks of the latest open-source
models, including Qwen, Llama, Mistral, and etc. Then we compare the total cost
of deploying these models locally with the major cloud providers subscription
fee. Our findings provide an estimated breakeven point based on usage levels
and performance needs. These results give organizations a practical framework
for planning their LLM strategies.

</details>


### [128] [SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture](https://arxiv.org/abs/2509.18123)
*Yeonju Lee,Rui Qi Chen,Joseph Oboamah,Po Nien Su,Wei-zhen Liang,Yeyin Shi,Lu Gan,Yongsheng Chen,Xin Qiao,Jing Li*

Main category: cs.AI

TL;DR: SPADE是一个利用大型语言模型（LLMs，特别是ChatGPT-4.1）对土壤湿度时间序列数据进行零样本灌溉模式和异常检测的框架，旨在提高分析的适应性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有土壤湿度时间序列分析方法（基于阈值或数据密集型机器学习/深度学习模型）在适应性和可解释性方面存在局限性，难以准确解释土壤湿度模式。

Method: 本研究引入了SPADE框架，该框架利用ChatGPT-4.1的推理和指令遵循能力，通过将时间序列数据转换为文本表示并设计领域知情的提示模板，进行零样本分析。SPADE能够识别灌溉事件、估算净灌溉增益、检测和分类异常，并生成结构化、可解释的报告。

Result: SPADE在异常检测方面优于现有方法，实现了更高的召回率和F1分数，并能准确分类异常类型。在灌溉事件检测方面，SPADE也取得了高精确度和召回率。此外，SPADE生成的报告提供了土壤湿度分析的可解释性和可用性。

Conclusion: 本研究展示了LLMs作为可扩展、适应性强的工具在精准农业中的潜力，它们能够整合定性知识和数据驱动推理，为准确的土壤湿度监测和改进灌溉调度提供可操作的见解。

Abstract: Accurate interpretation of soil moisture patterns is critical for irrigation
scheduling and crop management, yet existing approaches for soil moisture
time-series analysis either rely on threshold-based rules or data-hungry
machine learning or deep learning models that are limited in adaptability and
interpretability. In this study, we introduce SPADE (Soil moisture Pattern and
Anomaly DEtection), an integrated framework that leverages large language
models (LLMs) to jointly detect irrigation patterns and anomalies in soil
moisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced
reasoning and instruction-following capabilities, enabling zero-shot analysis
without requiring task-specific annotation or fine-tuning. By converting
time-series data into a textual representation and designing domain-informed
prompt templates, SPADE identifies irrigation events, estimates net irrigation
gains, detects, classifies anomalies, and produces structured, interpretable
reports. Experiments were conducted on real-world soil moisture sensor data
from commercial and experimental farms cultivating multiple crops across the
United States. Results demonstrate that SPADE outperforms the existing method
in anomaly detection, achieving higher recall and F1 scores and accurately
classifying anomaly types. Furthermore, SPADE achieved high precision and
recall in detecting irrigation events, indicating its strong capability to
capture irrigation patterns accurately. SPADE's reports provide
interpretability and usability of soil moisture analytics. This study
highlights the potential of LLMs as scalable, adaptable tools for precision
agriculture, which is capable of integrating qualitative knowledge and
data-driven reasoning to produce actionable insights for accurate soil moisture
monitoring and improved irrigation scheduling from soil moisture time-series
data.

</details>


### [129] [Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI](https://arxiv.org/abs/2509.18132)
*Xiuyi Fan*

Main category: cs.AI

TL;DR: 为解决现有医疗AI无法有效量化和传达不确定性的问题，本文提出了可解释的不确定性估计（XUE）框架，旨在通过整合解释性和不确定性量化来提升医疗AI的信任度和可用性。


<details>
  <summary>Details</summary>
Motivation: 医疗实践中不确定性是核心挑战，但现有医疗AI未能明确量化或以符合临床推理的方式传达不确定性。现有的XAI工作侧重于解释预测而非置信度，而UE技术提供置信度但缺乏直观解释，二者脱节限制了AI在医学中的应用。

Method: 本文提出XUE，整合解释性与不确定性量化。系统地将医学不确定性映射到AI不确定性概念，识别XUE实施的关键挑战。概述了推进XUE的技术方向，包括多模态不确定性量化、模型无关可视化技术和不确定性感知决策支持系统，并提出了实现XUE的指导原则。

Result: 分析强调了AI系统需生成可靠预测并以临床有意义的方式表达置信水平。这项工作通过连接解释性与不确定性，为开发值得信赖的医疗AI做出了贡献。

Conclusion: 通过弥合解释性与不确定性之间的鸿沟，本研究为开发符合真实世界临床复杂性、值得信赖的医疗AI系统铺平了道路。

Abstract: Uncertainty is a fundamental challenge in medical practice, but current
medical AI systems fail to explicitly quantify or communicate uncertainty in a
way that aligns with clinical reasoning. Existing XAI works focus on
interpreting model predictions but do not capture the confidence or reliability
of these predictions. Conversely, uncertainty estimation (UE) techniques
provide confidence measures but lack intuitive explanations. The disconnect
between these two areas limits AI adoption in medicine. To address this gap, we
propose Explainable Uncertainty Estimation (XUE) that integrates explainability
with uncertainty quantification to enhance trust and usability in medical AI.
We systematically map medical uncertainty to AI uncertainty concepts and
identify key challenges in implementing XUE. We outline technical directions
for advancing XUE, including multimodal uncertainty quantification,
model-agnostic visualization techniques, and uncertainty-aware decision support
systems. Lastly, we propose guiding principles to ensure effective XUE
realisation. Our analysis highlights the need for AI systems that not only
generate reliable predictions but also articulate confidence levels in a
clinically meaningful way. This work contributes to the development of
trustworthy medical AI by bridging explainability and uncertainty, paving the
way for AI systems that are aligned with real-world clinical complexities.

</details>


### [130] [HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics](https://arxiv.org/abs/2509.18168)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Semantic parsing of long documents remains challenging due to quadratic
growth in pairwise composition and memory requirements. We introduce
\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that
decomposes an input of length $N$ into $M$ meaningful segments, constructs
\emph{Local Semantic Graphs} on each segment, and extracts compact
\emph{summary nodes} to form a \emph{Global Graph Memory}. HSGM supports
\emph{incremental updates} -- only newly arrived segments incur local graph
construction and summary-node integration -- while \emph{Hierarchical Query
Processing} locates relevant segments via top-$K$ retrieval over summary nodes
and then performs fine-grained reasoning within their local graphs.
  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to
$O\!\left(N\,k + (N/k)^2\right)$, with segment size $k \ll N$, and we derive
Frobenius-norm bounds on the approximation error introduced by node
summarization and sparsification thresholds. Empirically, on three benchmarks
-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),
and legal event extraction -- HSGM achieves \emph{2--4$\times$ inference
speedup}, \emph{$>60\%$ reduction} in peak memory, and \emph{$\ge 95\%$} of
baseline accuracy. Our approach unlocks scalable, accurate semantic modeling
for ultra-long texts, enabling real-time and resource-constrained NLP
applications.

</details>


### [131] [Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM](https://arxiv.org/abs/2509.18178)
*Ling Yue,Nithin Somasekharan,Tingwen Zhang,Yadi Cao,Shaowu Pan*

Main category: cs.AI

TL;DR: Foam-Agent是一个多智能体框架，通过自然语言提示实现OpenFOAM计算流体力学（CFD）仿真流程的端到端自动化，显著降低了CFD的学习和使用门槛。


<details>
  <summary>Details</summary>
Motivation: 计算流体力学（CFD）是工程领域重要的仿真工具，但其学习曲线陡峭和手动设置复杂是主要障碍。

Method: Foam-Agent采用多智能体框架，实现端到端仿真自动化，包括高级网格划分、HPC提交脚本自动生成和后处理可视化。它使用可组合服务架构（Model Context Protocol）暴露核心功能，并通过分层多索引RAG和依赖感知生成过程实现高保真配置。

Result: 在110个仿真任务的基准测试中，Foam-Agent使用Claude 3.5 Sonnet取得了88.2%的成功率，显著优于现有框架（如MetaOpenFOAM的55.5%）。

Conclusion: Foam-Agent极大地降低了CFD的专业知识门槛，展示了专用多智能体系统在普及复杂科学计算方面的潜力。

Abstract: Computational Fluid Dynamics (CFD) is an essential simulation tool in
engineering, yet its steep learning curve and complex manual setup create
significant barriers. To address these challenges, we introduce Foam-Agent, a
multi-agent framework that automates the entire end-to-end OpenFOAM workflow
from a single natural language prompt. Our key innovations address critical
gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:
Foam-Agent is the first system to manage the full simulation pipeline,
including advanced pre-processing with a versatile Meshing Agent capable of
handling external mesh files and generating new geometries via Gmsh, automatic
generation of HPC submission scripts, and post-simulation visualization via
ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,
the framework uses Model Context Protocol (MCP) to expose its core functions as
discrete, callable tools. This allows for flexible integration and use by other
agentic systems, such as Claude-code, for more exploratory workflows. 3.
High-Fidelity Configuration Generation: We achieve superior accuracy through a
Hierarchical Multi-Index RAG for precise context retrieval and a
dependency-aware generation process that ensures configuration consistency.
Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%
success rate with Claude 3.5 Sonnet, significantly outperforming existing
frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the
expertise barrier for CFD, demonstrating how specialized multi-agent systems
can democratize complex scientific computing. The code is public at
https://github.com/csml-rpi/Foam-Agent.

</details>


### [132] [Large Language Models and Operations Research: A Structured Survey](https://arxiv.org/abs/2509.18180)
*Yang Wang,Kai Li*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型（LLMs）在运筹学（OR）中的最新进展，将其集成方法归纳为自动建模、辅助优化和直接求解三大方向，并总结了现有挑战和未来研究路径。


<details>
  <summary>Details</summary>
Motivation: 传统运筹学方法在处理大规模、动态、多约束问题时面临挑战。LLMs凭借语义理解、结构生成和推理控制能力，有望克服这些局限性，实现自然语言到数学模型/代码的转换、启发式生成和直接优化。

Method: 本综述论文系统地梳理了LLMs与运筹学结合的最新进展，将相关方法分为自动建模、辅助优化和直接求解三个主要方向。同时，还回顾了评估基准和特定领域应用。

Result: 总结了当前集成存在的关键开放问题，包括语义到结构映射的不稳定性、研究进展的碎片化、泛化能力的局限性以及评估系统的不足。

Conclusion: 为推动LLMs在运筹学领域的发展，本综述提出了可能的未来研究方向。

Abstract: Operations research (OR) provides fundamental methodologies for complex
system decision-making, with established applications in transportation, supply
chain management, and production scheduling. Traditional approaches, which
depend on expert-based modeling and manual parameter adjustment, often face
challenges in handling large-scale, dynamic, and multi-constraint problems.
Recently, large language models (LLMs) have shown potential to address these
limitations through semantic understanding, structured generation, and
reasoning control. LLMs can translate natural language descriptions into
mathematical models or executable code, generate heuristics, evolve algorithms,
and directly tackle optimization tasks. This paper surveys recent progress on
the integration of LLMs into OR, organizing methods into three main directions:
automatic modeling, auxiliary optimization, and direct solving. It further
reviews evaluation benchmarks and domain-specific applications, and summarizes
key open issues such as unstable semantic-to-structure mapping, fragmented
research progress, limited generalization, and insufficient evaluation systems.
Finally, the survey outlines possible research avenues for advancing the role
of LLMs in OR.

</details>


### [133] [Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling](https://arxiv.org/abs/2509.18181)
*Mustafa Sameen,Xiaojian Zhang,Xilei Zhao*

Main category: cs.AI

TL;DR: 本文提出SAPA框架，一种利用大型语言模型（LLMs）合成理论驱动的潜在态度，以显著提高网约车出行模式选择预测准确性的分层方法，有效解决了现有模型在捕捉心理因素和类别不平衡方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 准确的网约车出行模式选择建模对交通管理政策至关重要。现有模型预测精度有限，主要原因在于无法捕捉关键心理因素，且面临网约车出行占日常出行比例小的严重类别不平衡问题。

Method: 引入SAPA（Synthesizing Attitudes, Predicting Actions）框架，这是一个分层方法。首先，使用LLM从原始出行调查数据生成定性旅行者画像。其次，基于人口统计学和行为特征（并结合画像）训练倾向得分模型，生成个体得分。再次，LLM为理论驱动的潜在变量（如时间/成本敏感度）分配定量得分。最后，一个分类器整合倾向得分、潜在变量得分（及其交互项）和可观测的出行属性来预测网约车模式选择。

Result: 在大规模、多年期出行调查数据上的实验表明，SAPA框架显著优于现有基线模型，在保留测试集上，PR-AUC指标下的网约车选择预测准确率提高了高达75.9%。

Conclusion: 本研究提供了一个有效预测网约车出行模式选择的强大工具，并提出了一种易于推广到其他多种应用领域的方法论。

Abstract: Accurate modeling of ridesourcing mode choices is essential for designing and
implementing effective traffic management policies for reducing congestion,
improving mobility, and allocating resources more efficiently. Existing models
for predicting ridesourcing mode choices often suffer from limited predictive
accuracy due to their inability to capture key psychological factors, and are
further challenged by severe class imbalance, as ridesourcing trips comprise
only a small fraction of individuals' daily travel. To address these
limitations, this paper introduces the Synthesizing Attitudes, Predicting
Actions (SAPA) framework, a hierarchical approach that uses Large Language
Models (LLMs) to synthesize theory-grounded latent attitudes to predict
ridesourcing choices. SAPA first uses an LLM to generate qualitative traveler
personas from raw travel survey data and then trains a propensity-score model
on demographic and behavioral features, enriched by those personas, to produce
an individual-level score. Next, the LLM assigns quantitative scores to
theory-driven latent variables (e.g., time and cost sensitivity), and a final
classifier integrates the propensity score, latent-variable scores (with their
interaction terms), and observable trip attributes to predict ridesourcing mode
choice. Experiments on a large-scale, multi-year travel survey show that SAPA
significantly outperforms state-of-the-art baselines, improving ridesourcing
choice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.
This study provides a powerful tool for accurately predicting ridesourcing mode
choices, and provides a methodology that is readily transferable to various
applications.

</details>


### [134] [An Outcome-Based Educational Recommender System](https://arxiv.org/abs/2509.18186)
*Nursultan Askarbekuly,Timur Fayzrakhmanov,Sladjan Babarogić,Ivan Luković*

Main category: cs.AI

TL;DR: 本文提出一个名为OBER的教育推荐系统，它直接嵌入学习成果和评估项，以评估学生掌握度。通过一项包含5700多名学习者的随机对照试验，发现协同过滤最大化了保留率，而固定路径实现了最高的学习掌握度。


<details>
  <summary>Details</summary>
Motivation: 大多数教育推荐系统仅基于点击或评分等相关性指标进行调整和判断，导致其真实的教学影响不明确。

Method: 本文引入OBER（基于成果的教育推荐系统），它将学习成果和评估项直接嵌入数据模式中，以便任何算法都能根据其培养的掌握度进行评估。OBER采用极简实体关系模型、日志驱动的掌握度公式和插件式架构。该系统在一个非正式领域的在线学习系统中进行了为期两周的随机分拆测试，涉及5700多名学习者，比较了固定专家轨迹、协同过滤（CF）和基于知识（KB）的过滤三种方法。

Result: 评估结果显示，协同过滤（CF）最大化了学习者的保留率，但固定专家路径实现了最高的学习掌握度。

Conclusion: 由于OBER能从相同的日志中得出业务、相关性和学习指标，它使实践者无需额外测试开销即可权衡相关性、参与度和成果掌握度。该框架与具体方法无关，并且易于扩展到未来的自适应或情境感知推荐系统。

Abstract: Most educational recommender systems are tuned and judged on click- or
rating-based relevance, leaving their true pedagogical impact unclear. We
introduce OBER-an Outcome-Based Educational Recommender that embeds learning
outcomes and assessment items directly into the data schema, so any algorithm
can be evaluated on the mastery it fosters. OBER uses a minimalist
entity-relation model, a log-driven mastery formula, and a plug-in
architecture. Integrated into an e-learning system in non-formal domain, it was
evaluated trough a two-week randomized split test with over 5 700 learners
across three methods: fixed expert trajectory, collaborative filtering (CF),
and knowledge-based (KB) filtering. CF maximized retention, but the fixed path
achieved the highest mastery. Because OBER derives business, relevance, and
learning metrics from the same logs, it lets practitioners weigh relevance and
engagement against outcome mastery with no extra testing overhead. The
framework is method-agnostic and readily extensible to future adaptive or
context-aware recommenders.

</details>


### [135] [MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation](https://arxiv.org/abs/2509.18198)
*Rui Liu,Zikang Wang,Peng Gao,Yu Shen,Pratap Tokekar,Ming Lin*

Main category: cs.AI

TL;DR: 本文提出MMCD框架，通过融合多模态协作观测和利用跨模态知识蒸馏，显著提升了互联自动驾驶在数据模态缺失时的鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在事故多发环境中，鲁棒决策至关重要。单车感知受限，易导致事故。现有互联多模态方法假设训练和测试时所有数据模态和协作车辆均可用，这在实际中不切实际，可能因传感器故障或车辆缺失导致性能下降。

Method: 引入了MMCD（多模态协作决策）框架，融合自我车辆和协作车辆的多模态观测数据。为确保在测试时部分数据模态不可用时的鲁棒性，提出一种基于教师-学生模型的跨模态知识蒸馏方法，其中教师模型使用多种数据模态训练，学生模型设计为在减少模态下有效运行。

Result: 在互联自动驾驶和空地车辆协作实验中，该方法将驾驶安全性提高了20.7%，在检测潜在事故和做出安全驾驶决策方面超越了现有最佳基线。

Conclusion: MMCD框架通过创新的多模态融合和跨模态知识蒸馏，有效解决了互联自动驾驶在数据模态不完整情况下的决策鲁棒性挑战，显著提升了系统在复杂环境下的安全驾驶能力。

Abstract: Autonomous systems have advanced significantly, but challenges persist in
accident-prone environments where robust decision-making is crucial. A single
vehicle's limited sensor range and obstructed views increase the likelihood of
accidents. Multi-vehicle connected systems and multi-modal approaches,
leveraging RGB images and LiDAR point clouds, have emerged as promising
solutions. However, existing methods often assume the availability of all data
modalities and connected vehicles during both training and testing, which is
impractical due to potential sensor failures or missing connected vehicles. To
address these challenges, we introduce a novel framework MMCD (Multi-Modal
Collaborative Decision-making) for connected autonomy. Our framework fuses
multi-modal observations from ego and collaborative vehicles to enhance
decision-making under challenging conditions. To ensure robust performance when
certain data modalities are unavailable during testing, we propose an approach
based on cross-modal knowledge distillation with a teacher-student model
structure. The teacher model is trained with multiple data modalities, while
the student model is designed to operate effectively with reduced modalities.
In experiments on $\textit{connected autonomous driving with ground vehicles}$
and $\textit{aerial-ground vehicles collaboration}$, our method improves
driving safety by up to ${\it 20.7}\%$, surpassing the best-existing baseline
in detecting potential accidents and making safe driving decisions. More
information can be found on our website https://ruiiu.github.io/mmcd.

</details>


### [136] [Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations](https://arxiv.org/abs/2509.18215)
*Timotheus Kampik,Kristijonas Čyras,José Ruiz Alarcón*

Main category: cs.AI

TL;DR: 本文提出一种形式化方法，通过追溯论证强度不一致性的原因到特定论证，来解释量化双极论证框架（QBAF）中推理变化。


<details>
  <summary>Details</summary>
Motivation: 当QBAF更新后，需要理解和解释论证强度偏序关系的变化（即强度不一致性）为何会发生，以及这些变化如何影响推理结论。

Method: 1. 提出形式化方法来解释QBAF中的推理变化。2. 追踪QBAF更新后，语义在特定“主题论证”上建立的论证强度偏序关系的变化（称为“强度不一致性”）。3. 将强度不一致性的原因追溯到具体的论证作为解释。4. 识别充分、必要和反事实的强度不一致性解释。5. 定义基于启发式的方法来寻找解释，并提供相应实现。

Result: 1. 证明了强度不一致性解释存在当且仅当QBAF更新导致强度不一致性。2. 提供了用于搜索强度不一致性解释的启发式方法的实现。

Conclusion: 本文建立了一套形式化、可实现且基于启发式的方法，能够解释QBAF中因更新引起的推理变化，通过识别和分析论证强度不一致性的原因。

Abstract: This paper presents a formal approach to explaining change of inference in
Quantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions
from a QBAF and updating the QBAF to then again draw conclusions (and so on),
our approach traces changes -- which we call strength inconsistencies -- in the
partial order over argument strengths that a semantics establishes on some
arguments of interest, called topic arguments. We trace the causes of strength
inconsistencies to specific arguments, which then serve as explanations. We
identify sufficient, necessary, and counterfactual explanations for strength
inconsistencies and show that strength inconsistency explanations exist if and
only if an update leads to strength inconsistency. We define a heuristic-based
approach to facilitate the search for strength inconsistency explanations, for
which we also provide an implementation.

</details>


### [137] [nDNA -- the Semantic Helix of Artificial Cognition](https://arxiv.org/abs/2509.18216)
*Amitava Das*

Main category: cs.AI

TL;DR: 本文提出“神经DNA”（nDNA），通过潜层几何学的三个维度（谱曲率、热力学长度、信念矢量场）捕捉AI模型的内部认知身份（语义基因型），旨在开启“神经基因组学”领域，用于追踪模型演化、诊断风险和管理变更。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅测量AI模型的行为和输出，未能深入理解其内部认知身份和“灵魂”，即模型潜层几何学中的内在特性。需要一种方法来捕捉模型的语义基因型，超越表面行为。

Method: 提出神经DNA (nDNA) 作为一种语义-基因型表示，通过信念的内在几何学捕捉模型的潜在身份。nDNA由三个关键的潜层几何维度综合而成：谱曲率（揭示概念流曲率）、热力学长度（量化语义转换努力）和信念矢量场（描绘信念方向的语义扭转场）。

Result: nDNA能像生物DNA一样编码模型的“血统、突变和语义遗传”，形成稳定、无坐标的神经DNA指纹。利用此指纹，可以追踪模型在预训练、微调等过程中的谱系、测量继承性、检测特性漂移，从而比较模型、诊断风险并治理随时间的变化。

Conclusion: 通过引入神经DNA，将AI模型视为可追溯内部认知的数字语义有机体，开创“神经基因组学”新领域，以深入研究人工认知进化，从而实现模型的比较、风险诊断和长期治理。

Abstract: As AI foundation models grow in capability, a deeper question emerges: What
shapes their internal cognitive identity -- beyond fluency and output?
Benchmarks measure behavior, but the soul of a model resides in its latent
geometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic
representation that captures this latent identity through the intrinsic
geometry of belief. At its core, nDNA is synthesized from three principled and
indispensable dimensions of latent geometry: spectral curvature, which reveals
the curvature of conceptual flow across layers; thermodynamic length, which
quantifies the semantic effort required to traverse representational
transitions through layers; and belief vector field, which delineates the
semantic torsion fields that guide a model's belief directional orientations.
Like biological DNA, it encodes ancestry, mutation, and semantic inheritance,
found in finetuning and alignment scars, cultural imprints, and architectural
drift. In naming it, we open a new field: Neural Genomics, where models are not
just tools, but digital semantic organisms with traceable inner cognition.
  Modeling statement. We read AI foundation models as semantic fluid--dynamics:
meaning is transported through layers like fluid in a shaped conduit; nDNA is
the physics-grade readout of that flow -- a geometry-first measure of how
meaning is bent, paid for, and pushed -- yielding a stable, coordinate-free
neural DNA fingerprint tied to on-input behavior; with this fingerprint we
cross into biology: tracing lineages across pretraining, fine-tuning,
alignment, pruning, distillation, and merges; measuring inheritance between
checkpoints; detecting drift as traits shift under new data or objectives; and,
ultimately, studying the evolution of artificial cognition to compare models,
diagnose risks, and govern change over time.

</details>


### [138] [Similarity Field Theory: A Mathematical Framework for Intelligence](https://arxiv.org/abs/2509.18218)
*Kei-Sing Ng*

Main category: cs.AI

TL;DR: 本文提出相似性场理论，一个形式化实体间相似性关系及其演化的数学框架，并基于此定义了生成式智能，证明了其演化的约束条件，旨在为理解和构建智能系统提供基础语言。


<details>
  <summary>Details</summary>
Motivation: 作者认为，持续存在和变化的相似性关系是任何可理解动态系统的结构基础，因此需要一个数学框架来形式化实体之间相似性值及其演化所遵循的原理。

Method: 引入相似性场理论，该理论定义了：(1) 实体宇宙U上的相似性场S，允许非对称性和非传递性；(2) 通过序列Z_p表示的系统演化；(3) 诱导纤维的概念K；(4) 生成新实体的生成算子G。在此框架内，形式化了生成式智能的定义：如果生成算子G能生成属于给定概念K纤维的新实体，则其对K是智能的。

Result: 证明了两个定理：(i) 不对称性会阻止相互包含；(ii) 稳定性需要锚定坐标或最终限制在f的水平集内。这些结果确保了相似性场演化的受限性和可解释性。

Conclusion: 相似性场理论为表征、比较和构建智能系统提供了一个基础语言。该框架可用于解释大型语言模型，并将其作为探索社会认知的实验工具。

Abstract: We posit that persisting and transforming similarity relations form the
structural basis of any comprehensible dynamic system. This paper introduces
Similarity Field Theory, a mathematical framework that formalizes the
principles governing similarity values among entities and their evolution. We
define: (1) a similarity field $S: U \times U \to [0,1]$ over a universe of
entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed
relational field (asymmetry and non-transitivity are allowed); (2) the
evolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by
$p=0,1,2,\ldots$; (3) concepts $K$ as entities that induce fibers
$F_{\alpha}(K) = { E \in U \mid S(E,K) \ge \alpha }$, i.e., superlevel sets of
the unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that
produces new entities. Within this framework, we formalize a generative
definition of intelligence: an operator $G$ is intelligent with respect to a
concept $K$ if, given a system containing entities belonging to the fiber of
$K$, it generates new entities that also belong to that fiber. Similarity Field
Theory thus offers a foundational language for characterizing, comparing, and
constructing intelligent systems. We prove two theorems: (i) asymmetry blocks
mutual inclusion; and (ii) stability requires either an anchor coordinate or
eventual confinement within a level set of $f$. These results ensure that the
evolution of similarity fields is both constrained and interpretable,
culminating in an exploration of how the framework allows us to interpret large
language models and use them as experimental probes into societal cognition.

</details>


### [139] [Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models](https://arxiv.org/abs/2509.18221)
*Dingxin Lu,Shurui Wu,Xinyi Huang*

Main category: cs.AI

TL;DR: 提出VL-RiskFormer，一个结合视觉、语言和LLM的多模态Transformer，用于主动预测个人健康风险。


<details>
  <summary>Details</summary>
Motivation: 面对日益增长的慢性病负担和多模态异构临床数据（如医学影像、自由文本记录、可穿戴传感器数据流），迫切需要一个统一的多模态AI框架来主动预测个人健康风险。

Method: 提出VL-RiskFormer，一个分层堆叠的视觉-语言多模态Transformer，其顶层嵌入大型语言模型(LLM)推理头。该系统基于现有视觉-语言模型的双流架构，并引入三项主要创新：(i) 使用动量更新编码器和去偏InfoNCE损失对放射图像、眼底图和可穿戴设备照片与临床叙述进行跨模态预训练和细粒度对齐；(ii) 通过自适应时间间隔位置编码将不规则就诊序列整合到因果Transformer解码器中的时间融合块；(iii) 疾病本体图适配器，将ICD-10编码注入视觉和文本通道，并借助图注意力机制推断共病模式。

Result: 在MIMIC-IV纵向队列上，VL-RiskFormer实现了平均AUROC 0.90，预期校准误差为2.7%。

Conclusion: VL-RiskFormer成功地利用创新的视觉-语言Transformer架构与LLM，实现了基于多模态数据的高效、准确和校准良好的主动健康风险预测。

Abstract: With the rising global burden of chronic diseases and the multimodal and
heterogeneous clinical data (medical imaging, free-text recordings, wearable
sensor streams, etc.), there is an urgent need for a unified multimodal AI
framework that can proactively predict individual health risks. We propose
VL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer
with a large language model (LLM) inference head embedded in its top layer. The
system builds on the dual-stream architecture of existing visual-linguistic
models (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with
cross-modal comparison and fine-grained alignment of radiological images,
fundus maps, and wearable device photos with corresponding clinical narratives
using momentum update encoders and debiased InfoNCE losses; (ii) a time fusion
block that integrates irregular visit sequences into the causal Transformer
decoder through adaptive time interval position coding; (iii) a disease
ontology map adapter that injects ICD-10 codes into visual and textual channels
in layers and infers comorbid patterns with the help of a graph attention
mechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an
average AUROC of 0.90 with an expected calibration error of 2.7 percent.

</details>


### [140] [From "What to Eat?" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation](https://arxiv.org/abs/2509.18226)
*Yu Fu,Linyue Cai,Ruoyu Wu,Yong Zhao*

Main category: cs.AI

TL;DR: ChefMind是一个结合CoE、KG、RAG和LLM的混合架构，能有效处理模糊的用户意图，提高个性化食谱推荐的准确性和完整性。


<details>
  <summary>Details</summary>
Motivation: 个性化食谱推荐面临处理模糊用户意图、保证语义准确性和提供足够细节覆盖的挑战。

Method: 提出ChefMind混合架构，结合Chain of Exploration (CoE) 将模糊查询精炼为结构化条件，知识图谱 (KG) 提供语义推理，检索增强生成 (RAG) 补充烹饪细节，以及大语言模型 (LLM) 整合输出。在Xiachufang数据集和手动标注查询上进行评估，并与单一LLM、KG、RAG基线进行比较。

Result: ChefMind在准确性、相关性、完整性和清晰度上表现优越，平均得分8.7，远高于消融模型的6.4-6.7。此外，它将未处理查询降低至1.6%，显示出处理模糊需求时的鲁棒性。

Conclusion: ChefMind通过结合多种先进技术，成功解决了个性化食谱推荐中的关键挑战，提供了更准确、完整和清晰的推荐，并显著提高了系统处理模糊用户意图的能力。

Abstract: Personalized recipe recommendation faces challenges in handling fuzzy user
intent, ensuring semantic accuracy, and providing sufficient detail coverage.
We propose ChefMind, a hybrid architecture combining Chain of Exploration
(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large
Language Model (LLM). CoE refines ambiguous queries into structured conditions,
KG offers semantic reasoning and interpretability, RAG supplements contextual
culinary details, and LLM integrates outputs into coherent recommendations. We
evaluate ChefMind on the Xiachufang dataset and manually annotated queries,
comparing it with LLM-only, KG-only, and RAG-only baselines. Results show that
ChefMind achieves superior performance in accuracy, relevance, completeness,
and clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.
Moreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in
handling fuzzy demands.

</details>


### [141] [An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems](https://arxiv.org/abs/2509.18229)
*Anthony Patera,Rohan Abeyaratne*

Main category: cs.AI

TL;DR: 生成式AI（如GPT）在解决机械工程问题时表现出潜在能力但不可靠。本文提出“N-Plus-1”GPT代理机构，通过集成N个独立求解器和一个比较器，结合孔多塞陪审团定理，显著提高解决方案的准确性和透明度，使其更适用于教育和工程实践。


<details>
  <summary>Details</summary>
Motivation: 尽管GPT能为机械工程分析问题提供解决方案，但其可靠性不高（例如，在同一问题上成功率仅85%），这使得“开箱即用”的GPT不适合在教育或工程实践中部署。

Method: 引入“N-Plus-1”GPT代理机构：首先启动N个“Agent Solve”实例生成N个独立的解决方案，然后调用“Agent Compare”来总结、比较这些方案并提供推荐解决方案。该方法借鉴孔多塞陪审团定理，以确保多数解决方案的正确性。Agent Compare还能整合次要解决方案，考虑不同的问题解释或数学模型。

Result: 对于每个求解器成功概率大于1/2的问题，该代理机构推荐的（Predominant）解决方案将以高概率对应于正确方案。与商业多代理模型（如Grok Heavy）相比，本文提出的代理机构在设计和性能上有相似之处，但在强调透明度和教学价值方面有所不同。

Conclusion: “N-Plus-1”GPT代理机构能有效克服生成式AI在机械工程分析中存在的不可靠性问题，通过集合多个AI的判断并进行比较，显著提高了解决方案的准确性、透明度和教学价值，使其更适合作为机械工程问题初步分析的低成本工具。

Abstract: Generative AI, and specifically GPT, can produce a remarkable solution to a
mechanical engineering analysis problem - but also, on occasion, a flawed
solution. For example, an elementary mechanics problem is solved flawlessly in
one GPT instance and incorrectly in a subsequent GPT instance, with a success
probability of only 85%. This unreliability renders "out-of-the-box" GPT
unsuitable for deployment in education or engineering practice. We introduce an
"N-Plus-1" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering
Problem Statements. Agency first launches N instantiations of Agent Solve to
yield N independent Proposed Problem Solution Realizations; Agency then invokes
Agent Compare to summarize and compare the N Proposed Problem Solution
Realizations and to provide a Recommended Problem Solution. We argue from
Condorcet's Jury Theorem that, for a Problem Statement characterized by
per-Solve success probability greater than 1/2 (and N sufficiently large), the
Predominant (Agent Compare) Proposed Problem Solution will, with high
probability, correspond to a Correct Proposed Problem Solution. Furthermore,
Agent Compare can also incorporate aspects of Secondary (Agent Compare)
Proposed Problem Solutions, in particular when the latter represent alternative
Problem Statement interpretations - different Mathematical Models - or
alternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a
commercial multi-agent model, show similarities in design and performance, but
also important differences in emphasis: our Agency focuses on transparency and
pedagogical value.

</details>


### [142] [Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces](https://arxiv.org/abs/2509.18230)
*Zihan Dong,Xinyu Fan,Zixiang Tang,Yunqing Li*

Main category: cs.AI

TL;DR: 本文提出ComputerAgent，一个轻量级分层强化学习框架，用于桌面应用控制。它通过分层策略和多模态编码器，实现了显著降低模型大小和推理时间，同时在复杂任务上表现优于或媲美现有MLLM基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态大语言模型（MLLMs）的桌面应用控制方法存在推理延迟高、长周期稀疏奖励任务样本效率低以及难以设备端部署等问题。

Method: 引入ComputerAgent，一个轻量级分层强化学习框架。该框架将操作系统控制表述为两级选项过程（管理器和子策略），采用三模态状态编码器（截图、任务ID、数字状态），集成带有早期停止机制的元动作，并使用紧凑视觉骨干网络和小型策略网络（15M参数）实现设备端推理。

Result: 在135个真实世界桌面任务中，ComputerAgent在简单任务（<8步）上达到92.1%的成功率，在困难任务（>=8步）上达到58.8%的成功率。它在简单场景下与200B参数的MLLM基线相当或超越，同时将模型大小减少了四个数量级以上，并将推理时间减半。

Conclusion: 分层强化学习为计算机控制提供了一种实用、可扩展的替代方案，相较于单一的基于MLLM的自动化方法，更具优势。

Abstract: Controlling desktop applications via software remains a fundamental yet
under-served problem. Existing multi-modal large language models (MLLMs) ingest
screenshots and task instructions to generate keystrokes and mouse events, but
they suffer from prohibitive inference latency, poor sample efficiency on
long-horizon sparse-reward tasks, and infeasible on-device deployment. We
introduce a lightweight hierarchical reinforcement learning framework,
ComputerAgent, that formulates OS control as a two-level option process
(manager and subpolicy), employs a triple-modal state encoder (screenshot, task
ID, numeric state) to handle visual and contextual diversity, integrates
meta-actions with an early-stop mechanism to reduce wasted interactions, and
uses a compact vision backbone plus small policy networks for on-device
inference (15M parameters). On a suite of 135 real-world desktop tasks,
ComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on
hard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on
simple scenarios while reducing model size by over four orders of magnitude and
halving inference time. These results demonstrate that hierarchical RL offers a
practical, scalable alternative to monolithic MLLM-based automation for
computer control.

</details>


### [143] [The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks](https://arxiv.org/abs/2509.18234)
*Yu Gu,Jingjing Fu,Xiaodong Liu,Jeya Maria Jose Valanarasu,Noel Codella,Reuben Tan,Qianchu Liu,Ying Jin,Sheng Zhang,Jinyu Wang,Rui Wang,Lei Song,Guanghui Qin,Naoto Usuyama,Cliff Wong,Cheng Hao,Hohin Lee,Praneeth Sanapathi,Sarah Hilado,Bian Jiang,Javier Alvarez-Valle,Mu Wei,Jianfeng Gao,Eric Horvitz,Matt Lungren,Hoifung Poon,Paul Vozila*

Main category: cs.AI

TL;DR: 大型模型在医疗基准测试中表现优异，但应力测试揭示它们存在脆性、投机取巧，且缺乏真正的医学理解，当前的基准分数无法反映实际应用能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型前沿模型在医疗基准测试中取得高分，但作者怀疑这些分数是否真正反映了模型的医学理解能力和实际应用价值，并担忧现有基准可能奖励“应试技巧”而非真实医学能力。

Method: 对六个主流模型在六个广泛使用的医疗基准上进行应力测试，包括移除关键输入、改变提示语，并辅以临床医生指导的评估标准来分析基准测量内容。

Result: 高基准分数掩盖了模型的脆性和“投机取巧”的学习方式；模型在关键输入缺失时仍能猜对，在微小提示语变化下答案翻转，并捏造有缺陷的推理。不同基准衡量内容差异大，且被互换使用，掩盖了模型失效模式。

Conclusion: 医疗基准测试分数不能直接反映模型在现实世界的准备程度。为了让AI在医疗领域赢得信任，需要超越排行榜成绩，并要求系统具备鲁棒性、可靠推理能力，并与实际医疗需求对齐。

Abstract: Large frontier models like GPT-5 now achieve top scores on medical
benchmarks. But our stress tests tell a different story. Leading systems often
guess correctly even when key inputs like images are removed, flip answers
under trivial prompt changes, and fabricate convincing yet flawed reasoning.
These aren't glitches; they expose how today's benchmarks reward test-taking
tricks over medical understanding. We evaluate six flagship models across six
widely used benchmarks and find that high leaderboard scores hide brittleness
and shortcut learning. Through clinician-guided rubric evaluation, we show that
benchmarks vary widely in what they truly measure yet are treated
interchangeably, masking failure modes. We caution that medical benchmark
scores do not directly reflect real-world readiness. If we want AI to earn
trust in healthcare, we must demand more than leaderboard wins and must hold
systems accountable for robustness, sound reasoning, and alignment with real
medical demands.

</details>


### [144] [Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints](https://arxiv.org/abs/2509.18382)
*Adarsha Balaji,Le Chen,Rajeev Thakur,Franck Cappello,Sandeep Madireddy*

Main category: cs.AI

TL;DR: 本研究针对语言模型推理计算成本高的问题，探讨了推理长度限制和模型量化两种计算约束策略，并分析了它们在计算效率与模型安全之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 语言模型通过生成更长的思维链（CoT）序列虽能提升推理性能，但计算成本显著增加。本研究旨在通过探索计算约束策略，降低推理模型的计算需求，并分析这些策略对模型安全性能的影响，同时研究计算效率与模型安全之间的权衡。

Method: 本研究探索两种应用计算约束的方法：1. 使用基于长度控制策略优化（LCPO）的强化学习方法微调推理模型，以满足用户定义的CoT推理长度；2. 应用模型量化，在用户定义的计算约束内最大化CoT序列的生成。这些方法旨在降低推理模型的计算需求并评估其对安全性能的影响。

Result: 本研究旨在通过评估推理长度限制和模型量化两种策略，分析它们如何影响语言模型的计算需求和安全性能，并揭示计算效率与模型安全之间的具体权衡关系。

Conclusion: 本研究旨在为在保证模型安全的前提下，降低推理语言模型的计算成本提供有效策略，并量化计算效率与模型安全间的权衡，从而指导未来高效且安全的语言模型开发。

Abstract: Test-time compute scaling has demonstrated the ability to improve the
performance of reasoning language models by generating longer chain-of-thought
(CoT) sequences. However, this increase in performance comes with a significant
increase in computational cost. In this work, we investigate two compute
constraint strategies: (1) reasoning length constraint and (2) model
quantization, as methods to reduce the compute demand of reasoning models and
study their impact on their safety performance. Specifically, we explore two
approaches to apply compute constraints to reasoning models: (1) fine-tuning
reasoning models using a length controlled policy optimization (LCPO) based
reinforcement learning method to satisfy a user-defined CoT reasoning length,
and (2) applying quantization to maximize the generation of CoT sequences
within a user-defined compute constraint. Furthermore, we study the trade-off
between the computational efficiency and the safety of the model.

</details>


### [145] [Gödel Test: Can Large Language Models Solve Easy Conjectures?](https://arxiv.org/abs/2509.18383)
*Moran Feldman,Amin Karbasi*

Main category: cs.AI

TL;DR: 本研究通过“哥德尔测试”评估GPT-5解决新的、简单数学猜想的能力，发现它在较简单问题上表现良好，甚至能反驳原有猜想并提出有效方案，但在跨论文综合和复杂分析上仍有限制。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在高中和本科数学竞赛中表现出色，但它们能否解决更高级数学领域中新的、简单的猜想仍不明确。

Method: 提出了“哥德尔测试”，将GPT-5应用于组合优化领域的五个未解决的简单猜想。测试过程中，提供了相关源论文，但未告知具体猜想内容，并详细评估模型的推理过程。

Result: GPT-5在三个较简单的猜想上给出了接近正确的解决方案。对于问题2，它导出了一个不同的近似保证，反驳了原有猜想并提供了一个有效解。模型在需要结合两篇论文结果的问题4上失败。在问题5上，GPT-5提出了与研究者相同的算法，但在分析上未能成功。

Conclusion: 尽管样本量小，但结果表明，GPT-5在常规推理方面取得了显著进展，偶尔展现出原创性，但在需要跨论文综合时存在明显局限。这可能是前沿模型最终通过哥德尔测试的早期一步。

Abstract: Recent announcements from frontier AI model labs have highlighted strong
results on high-school and undergraduate math competitions. Yet it remains
unclear whether large language models can solve new, simple conjectures in more
advanced areas of mathematics. We propose the G\"odel Test: evaluating whether
a model can produce correct proofs for very simple, previously unsolved
conjectures. To this end, we study the performance of GPT-5 on five conjectures
in combinatorial optimization. For each problem, we provided one or two source
papers from which the conjecture arose, withheld our own conjecture, and then
assessed the model's reasoning in detail. On the three easier problems, GPT-5
produced nearly correct solutions; for Problem 2 it even derived a different
approximation guarantee that, upon checking, refuted our conjecture while
providing a valid solution. The model failed on Problem 4, which required
combining results from two papers. On Problem 5, a harder case without a
validated conjecture, GPT-5 proposed the same algorithm we had in mind but
failed in the analysis, suggesting the proof is more challenging than expected.
Although our sample is small, the results point to meaningful progress on
routine reasoning, occasional flashes of originality, and clear limitations
when cross-paper synthesis is required. GPT-5 may represent an early step
toward frontier models eventually passing the G\"odel Test.

</details>


### [146] [ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification](https://arxiv.org/abs/2509.18400)
*Pritish Yuvraj,Siva Devarakonda*

Main category: cs.AI

TL;DR: 本文提出首个海关协调关税制度（HTS）代码分类基准，并引入名为Atlas的微调LLM模型，其在10位和6位分类准确率上显著优于现有主流LLM，且成本更低、可自托管，但该任务仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 海关协调关税制度（HTS）下产品分类是全球贸易的关键瓶颈，错误分类可能导致货物停运。然而，机器学习领域对此关注甚少。

Method: 研究从美国海关裁定在线搜索系统（CROSS）中构建了首个HTS代码分类基准。评估了领先的LLM模型，并微调了LLaMA-3.3-70B模型（命名为Atlas）。

Result: Atlas模型在10位完全正确分类中达到40%的准确率，在6位正确分类中达到57.5%的准确率，分别比GPT-5-Thinking高15个百分点，比Gemini-2.5-Pro-Thinking高27.5个百分点。此外，Atlas的成本约为GPT-5-Thinking的五分之一，Gemini-2.5-Pro-Thinking的八分之一，并可自托管以保证数据隐私。

Conclusion: Atlas为HTS分类任务设定了一个强大的基线，但10位分类准确率仅为40%，表明该基准仍极具挑战性。通过发布数据集和模型，研究旨在将HTS分类定位为新的社区基准任务，并邀请未来在检索、推理和对齐方面的研究。

Abstract: Accurate classification of products under the Harmonized Tariff Schedule
(HTS) is a critical bottleneck in global trade, yet it has received little
attention from the machine learning community. Misclassification can halt
shipments entirely, with major postal operators suspending deliveries to the
U.S. due to incomplete customs documentation. We introduce the first benchmark
for HTS code classification, derived from the U.S. Customs Rulings Online
Search System (CROSS). Evaluating leading LLMs, we find that our fine-tuned
Atlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit
classifications and 57.5 percent correct 6-digit classifications, improvements
of 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.
Beyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and
eight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to
guarantee data privacy in high-stakes trade and compliance workflows. While
Atlas sets a strong baseline, the benchmark remains highly challenging, with
only 40 percent 10-digit accuracy. By releasing both dataset and model, we aim
to position HTS classification as a new community benchmark task and invite
future work in retrieval, reasoning, and alignment.

</details>


### [147] [Instruction-Following Evaluation in Function Calling for Large Language Models](https://arxiv.org/abs/2509.18420)
*Nikolai Skripko*

Main category: cs.AI

TL;DR: 本文引入IFEval-FC基准，专门评估大型语言模型在函数调用中遵循嵌入式格式指令的能力，发现即使是顶尖模型也常在此方面出错。


<details>
  <summary>Details</summary>
Motivation: 现有函数调用基准（如BFCL、tau^2-Bench、ACEBench）主要评估参数的正确性，但未能测试模型是否遵循参数描述中嵌入的格式指令（例如使用双引号或ISO日期格式），这在大模型驱动的AI代理中是一个实际限制。

Method: 研究者提出了IFEval-FC基准，灵感来源于IFEval。该基准将可验证的格式要求直接编码到JSON schema描述中（例如指定值不能包含标点）。它包含750个测试用例，每个用例都由一个函数、其某个输入参数的嵌入格式以及相应的用户查询组成。评估过程完全算法化，以确保客观性、可重现性和可扩展性。

Result: 研究结果显示，即使是GPT-5和Claude 4.1 Opus等最先进的专有模型，也频繁未能遵循基本的格式规则。

Conclusion: 这揭示了真实世界代理系统的一个实际局限，强调了模型在精确遵循格式指令方面的不足。

Abstract: Function calling is a core capability of large language models, essential for
AI agents. Existing benchmarks such as the Berkeley Function Calling
Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench
(arXiv:2501.12851) evaluate argument correctness but do not test adherence to
format instructions embedded in parameter descriptions, such as enclosing
values in double quotes or using ISO date formats.
  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)
that assesses precise instruction following in function calling. IFEval-FC
encodes verifiable formats directly within JSON schema descriptions, for
example specifying that a value must not contain punctuation. It includes 750
test cases, each consisting of a function with an embedded format for one of
its input parameters and a corresponding user query. Evaluation is fully
algorithmic, ensuring objectivity, reproducibility, and scalability.
  Our results show that even state-of-the-art proprietary models, including
GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,
highlighting a practical limitation for real-world agent systems. The complete
codebase and data are publicly available at
https://github.com/Skripkon/IFEval-FC.

</details>


### [148] [Memory-QA: Answering Recall Questions Based on Multimodal Memories](https://arxiv.org/abs/2509.18436)
*Hongda Jiang,Xinyuan Zhang,Siddhant Garg,Rishab Arora,Shiun-Zu Kuo,Jiayang Xu,Christopher Brossman,Yue Liu,Aaron Colak,Ahmed Aly,Anuj Kumar,Xin Luna Dong*

Main category: cs.AI

TL;DR: 本文介绍了一个新的真实世界任务Memory-QA，即从多模态记忆中回答关于视觉内容的召回问题。为应对其挑战，我们提出了Pensieve管道，并在新创建的多模态基准上展现出优于现有SOTA方案的性能（问答准确率提高达14%）。


<details>
  <summary>Details</summary>
Motivation: 引入了一个新颖的真实世界任务Memory-QA，即从先前存储的多模态记忆中回答关于视觉内容的召回问题。该任务面临独特挑战，包括创建面向任务的记忆、有效利用记忆中的时间与位置信息，以及利用多个记忆来回答召回问题。

Method: 提出了一个名为Pensieve的综合管道，该管道整合了记忆特定增强、时间感知和位置感知的多信号检索，以及多记忆问答微调。同时，创建了一个多模态基准来展示任务中的各种实际挑战。

Result: 通过创建的多模态基准，展示了Pensieve管道在该任务上的卓越性能，其问答准确率比现有最先进的解决方案高出高达14%。

Conclusion: Memory-QA是一个具有挑战性的新任务，而Pensieve管道作为一种综合解决方案，能够有效应对从多模态记忆中回答召回问题的独特挑战，并在性能上显著超越现有技术。

Abstract: We introduce Memory-QA, a novel real-world task that involves answering
recall questions about visual content from previously stored multimodal
memories. This task poses unique challenges, including the creation of
task-oriented memories, the effective utilization of temporal and location
information within memories, and the ability to draw upon multiple memories to
answer a recall question. To address these challenges, we propose a
comprehensive pipeline, Pensieve, integrating memory-specific augmentation,
time- and location-aware multi-signal retrieval, and multi-memory QA
fine-tuning. We created a multimodal benchmark to illustrate various real
challenges in this task, and show the superior performance of Pensieve over
state-of-the-art solutions (up to 14% on QA accuracy).

</details>


### [149] [FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning](https://arxiv.org/abs/2509.18527)
*Ziwen Chen,Zhong Wang*

Main category: cs.AI

TL;DR: 本文提出了FERA，一个AI击剑裁判助手原型，它结合了姿态识别和规则推理，用于自动化花剑判罚，并在有限数据下取得了优于基线模型的表现。


<details>
  <summary>Details</summary>
Motivation: 击剑运动的裁判工作面临主观判断、人为错误、偏见以及实践环境可用性有限等挑战，促使开发自动化裁判辅助系统。

Method: FERA整合了基于姿态的多标签动作识别和规则推理。具体方法包括：从视频中提取并标准化2D关节位置；计算101维运动学特征；使用Transformer进行多标签动作和剑的分类；并应用一个带有编码优先权规则的蒸馏语言模型来决定得分并提供解释。

Result: 在有限的手工标注数据下，通过5折交叉验证，FERA取得了0.549的平均宏观F1分数，并优于包括TCN、BiLSTM和普通Transformer在内的多种基线模型。

Conclusion: 尽管FERA尚未达到部署阶段，但其结果为花剑运动中的自动化裁判辅助开辟了有前景的道路，并为AI在击剑领域的应用（如教练）带来了新的机遇。

Abstract: The sport of fencing, like many other sports, faces challenges in refereeing:
subjective calls, human errors, bias, and limited availability in practice
environments. We present FERA (Fencing Referee Assistant), a prototype AI
referee for foil fencing which integrates pose-based multi-label action
recognition and rule-based reasoning. FERA extracts 2D joint positions from
video, normalizes them, computes a 101-dimensional kinematic feature set, and
applies a Transformer for multi-label move and blade classification. To
determine priority and scoring, FERA applies a distilled language model with
encoded right-of-way rules, producing both a decision and an explanation for
each exchange. With limited hand-labeled data, a 5-fold cross-validation
achieves an average macro-F1 score of 0.549, outperforming multiple baselines,
including a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla
Transformer. While not ready for deployment, these results demonstrate a
promising path towards automated referee assistance in foil fencing and new
opportunities for AI applications, such as coaching in the field of fencing.

</details>


### [150] [LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs](https://arxiv.org/abs/2509.18557)
*Tom Pawelek,Raj Patel,Charlotte Crowell,Noorbakhsh Amiri,Sudip Mittal,Shahram Rahimi,Andy Perkins*

Main category: cs.AI

TL;DR: 提出LLMZ+，通过提示词白名单机制强化代理型AI安全性，有效抵御越狱攻击并保持合法通信畅通。


<details>
  <summary>Details</summary>
Motivation: 代理型AI因其对数据源和API工具的特权访问以及非确定性行为，成为高价值攻击目标，带来显著操作和信息安全风险。现有基于检测的防御机制（如防止提示注入）面临局限。

Method: 引入LLMZ+方法，采用提示词白名单机制而非传统的检测方法。该方法只允许符合上下文且安全的请求与代理型LLM交互，确保所有外部用户与LLM的交流都符合预定义的用例和操作边界。

Result: LLMZ+对最常见的越狱提示词表现出强大的抵御能力，同时不中断合法的业务通信，授权流量能无缝流动。在实验设置中，误报率和漏报率均可降至0。

Conclusion: LLMZ+通过提示词白名单机制，为代理型LLM提供了一种流线型、长期弹性强且资源需求低的全新安全框架，有效抵御越狱攻击，同时保障正常业务通信不受影响。

Abstract: Compared to traditional models, agentic AI represents a highly valuable
target for potential attackers as they possess privileged access to data
sources and API tools, which are traditionally not incorporated into classical
agents. Unlike a typical software application residing in a Demilitarized Zone
(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI
(only defining a final goal, leaving the path selection to LLM). This
characteristic introduces substantial security risk to both operational
security and information security. Most common existing defense mechanism rely
on detection of malicious intent and preventing it from reaching the LLM agent,
thus protecting against jailbreak attacks such as prompt injection. In this
paper, we present an alternative approach, LLMZ+, which moves beyond
traditional detection-based approaches by implementing prompt whitelisting.
Through this method, only contextually appropriate and safe messages are
permitted to interact with the agentic LLM. By leveraging the specificity of
context, LLMZ+ guarantees that all exchanges between external users and the LLM
conform to predefined use cases and operational boundaries. Our approach
streamlines the security framework, enhances its long-term resilience, and
reduces the resources required for sustaining LLM information security. Our
empirical evaluation demonstrates that LLMZ+ provides strong resilience against
the most common jailbreak prompts. At the same time, legitimate business
communications are not disrupted, and authorized traffic flows seamlessly
between users and the agentic LLM. We measure the effectiveness of approach
using false positive and false negative rates, both of which can be reduced to
0 in our experimental setting.

</details>


### [151] [Solving Math Word Problems Using Estimation Verification and Equation Generation](https://arxiv.org/abs/2509.18565)
*Mitchell Piehl,Dillon Wilson,Ananya Kalita,Jugal Kalita*

Main category: cs.AI

TL;DR: 本研究提出一种结合LLM方程生成、外部求解器计算和LLM估算验证的新方法，显著提升了LLM解决数学应用题（MWPs）的性能，尤其是在数值、代数和三角MWPs上取得了SOTA或满意结果，并引入了新数据集。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在多种任务中表现出色，但在解决数学应用题（MWPs）时仍面临挑战，因为这需要复杂的推理和数学能力。

Method: 本研究提出一种新方法：首先，LLM根据问题分解生成数学方程；其次，使用外部符号方程求解器得出精确答案。为确保答案准确性，LLM会第二次估算答案进行验证；若验证失败，则采用迭代纠正过程，直至找到正确答案。

Result: 该方法在数值和代数MWP数据集上取得了新的SOTA结果，平均提升近2%；首次在三角MWP任务上获得了满意结果；同时，引入了两个新的数据集SVAMPClean和Trig300。

Conclusion: 结合LLM的方程生成、外部求解器的精确计算以及LLM的估算验证与纠正机制，能够显著提升LLM解决复杂数学应用题的能力，并在多项MWP任务上超越现有最佳表现，同时拓展了其解决三角MWP的能力。

Abstract: Large Language Models (LLMs) excel at various tasks, including
problem-solving and question-answering. However, LLMs often find Math Word
Problems (MWPs) challenging because solving them requires a range of reasoning
and mathematical abilities with which LLMs seem to struggle. Recent efforts
have helped LLMs solve more complex MWPs with improved prompts. This study
proposes a novel method that initially prompts an LLM to create equations from
a decomposition of the question, followed by using an external symbolic
equation solver to produce an answer. To ensure the accuracy of the obtained
answer, inspired by an established recommendation of math teachers, the LLM is
instructed to solve the MWP a second time, but this time with the objective of
estimating the correct answer instead of solving it exactly. The estimation is
then compared to the generated answer to verify. If verification fails, an
iterative rectification process is employed to ensure the correct answer is
eventually found. This approach achieves new state-of-the-art results on
datasets used by prior published research on numeric and algebraic MWPs,
improving the previous best results by nearly two percent on average. In
addition, the approach obtains satisfactory results on trigonometric MWPs, a
task not previously attempted to the authors' best knowledge. This study also
introduces two new datasets, SVAMPClean and Trig300, to further advance the
testing of LLMs' reasoning abilities.

</details>


### [152] [Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents](https://arxiv.org/abs/2509.18633)
*Yara Mohajerani*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的地理空间基于代理模型，结合气候灾害数据与经济代理的进化学习，以评估气候风险对经济系统的影响及适应策略。


<details>
  <summary>Details</summary>
Motivation: 气候风险评估需要对空间异质性灾害和适应性经济系统之间的复杂互动进行建模，以理解和量化其影响。

Method: 研究采用了一种新颖的地理空间基于代理模型，该模型将基于Mesa的空间建模与CLIMADA气候影响评估相结合。它引入了适应性学习行为，允许企业通过基于适应度和突变的选择，进化预算分配、定价、工资和风险适应策略。

Result: 框架在RCP8.5情景下模拟了河流洪水到2100年的影响，结果表明，进化适应使企业在经历数十年的气候压力干扰后，能够恢复到基线（无灾害）生产水平。研究还揭示了系统性风险，即使未直接暴露于洪水的代理也会因供应链中断而受影响，到本世纪末，RCP8.5情景下的商品平均价格比基线高5.6%。

Conclusion: 该开源框架为金融机构和公司提供了量化直接和级联气候风险，并评估经济有效适应策略的工具。

Abstract: Climate risk assessment requires modelling complex interactions between
spatially heterogeneous hazards and adaptive economic systems. We present a
novel geospatial agent-based model that integrates climate hazard data with
evolutionary learning for economic agents. Our framework combines Mesa-based
spatial modelling with CLIMADA climate impact assessment, introducing adaptive
learning behaviours that allow firms to evolve strategies for budget
allocation, pricing, wages, and risk adaptation through fitness-based selection
and mutation. We demonstrate the framework using riverine flood projections
under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to
converge with baseline (no hazard) production levels after decades of
disruption due to climate stress. Our results reveal systemic risks where even
agents that are not directly exposed to floods face impacts through supply
chain disruptions, with the end-of-century average price of goods 5.6% higher
under RCP8.5 compared to the baseline. This open-source framework provides
financial institutions and companies with tools to quantify both direct and
cascading climate risks while evaluating cost-effective adaptation strategies.

</details>


### [153] [TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2509.18667)
*Qiao Xiao,Hong Ting Tsang,Jiaxin Bai*

Main category: cs.AI

TL;DR: 现有图基RAG系统因图构建成本高昂而受限。TERAG提出一种低成本构建信息图的方法，通过整合个性化PageRank (PPR)，在保持80%准确率的同时，显著减少LLM token消耗（3%-11%）。


<details>
  <summary>Details</summary>
Motivation: 现有图基RAG系统在图构建过程中LLM token使用成本高昂，阻碍了其大规模应用。

Method: 提出TERAG框架，旨在以显著更低的成本构建信息图。受HippoRAG启发，在检索阶段融入个性化PageRank (PPR)。

Result: TERAG在保持与广泛使用的图基RAG方法至少80%准确率的同时，仅消耗3%-11%的输出token。

Conclusion: TERAG提供了一个简单而有效的框架，显著降低了图基RAG的构建成本，同时保持了高准确性，有利于其大规模应用。

Abstract: Graph-based Retrieval-augmented generation (RAG) has become a widely studied
approach for improving the reasoning, accuracy, and factuality of Large
Language Models. However, many existing graph-based RAG systems overlook the
high cost associated with LLM token usage during graph construction, hindering
large-scale adoption. To address this, we propose TERAG, a simple yet effective
framework designed to build informative graphs at a significantly lower cost.
Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the
retrieval phase, and we achieve at least 80% of the accuracy of widely used
graph-based RAG methods while consuming only 3%-11% of the output tokens.

</details>


### [154] [Implementation of airborne ML models with semantics preservation](https://arxiv.org/abs/2509.18681)
*Nicolas Valot,Louis Fabre,Benjamin Lesage,Ammar Mechouche,Claire Pagetti*

Main category: cs.AI

TL;DR: 简述航空ML系统安全合规挑战，论文提出区分ML模型与ML模型描述(MLMD)，并细化语义保存概念以确保模型精确复制，并应用于工业用例。


<details>
  <summary>Details</summary>
Motivation: 机器学习为航空系统带来新能力，但其必须保证安全运行并符合相关监管指导。现有指南（如EASA和ED-324）仅提供了高级目标，缺乏确保ML模型功能实现和训练性能维持的具体方法。

Method: 论文旨在澄清ML模型与其明确描述（MLMD）之间的区别。在此基础上，它细化了“语义保存”的核心概念，以确保模型的准确复制。

Result: 将所提出的方法应用于多个工业用例，成功构建并比较了多种目标模型。

Conclusion: 通过明确MLMD和语义保存的概念，该研究为ML模型在航空系统中的安全合规复制提供了关键的理论和方法基础。

Abstract: Machine Learning (ML) may offer new capabilities in airborne systems.
However, as any piece of airborne systems, ML-based systems will be required to
guarantee their safe operation. Thus, their development will have to be
demonstrated to be compliant with the adequate guidance. So far, the European
Union Aviation Safety Agency (EASA) has published a concept paper and an
EUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level
objectives to confirm the ML model achieves its intended function and maintains
training performance in the target environment. The paper aims to clarify the
difference between an ML model and its corresponding unambiguous description,
referred to as the Machine Learning Model Description (MLMD). It then refines
the essential notion of semantics preservation to ensure the accurate
replication of the model. We apply our contributions to several industrial use
cases to build and compare several target models.

</details>


### [155] [Advances in Large Language Models for Medicine](https://arxiv.org/abs/2509.18690)
*Zhiyu Kan,Wensheng Gan,Zhenlian Qi,Philip S. Yu*

Main category: cs.AI

TL;DR: 本文系统综述了大型语言模型（LLMs）在医疗领域的最新研究进展，分析了其训练、应用、优缺点，并创新性地分类了医疗LLMs及其评估方法，最后提出了解决方案和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 人工智能（AI）技术，尤其是大型语言模型（LLMs）取得了显著突破，并在各行各业产生影响，其中医疗领域是突出的应用方向。因此，有必要对医疗领域LLMs的最新研究进展进行深入分析和理解。

Method: 通过系统性地综述医疗领域大型语言模型（LLMs）的最新研究进展。

Result: 深入分析了医疗大型模型的训练技术、在医疗环境中的适应性、相关应用、优势与局限性。创新性地将医疗LLMs依据训练方法分为三类，并将其评估方法分为两类。

Conclusion: 旨在强调开发医疗LLMs的必要性，提供对其当前发展状态的深入理解，并为后续研究提供明确指导。同时，提出了现有挑战的解决方案，并规划了未来研究方向。

Abstract: Artificial intelligence (AI) technology has advanced rapidly in recent years,
with large language models (LLMs) emerging as a significant breakthrough. LLMs
are increasingly making an impact across various industries, with the medical
field standing out as the most prominent application area. This paper
systematically reviews the up-to-date research progress of LLMs in the medical
field, providing an in-depth analysis of training techniques for large medical
models, their adaptation in healthcare settings, related applications, as well
as their strengths and limitations. Furthermore, it innovatively categorizes
medical LLMs into three distinct types based on their training methodologies
and classifies their evaluation approaches into two categories. Finally, the
study proposes solutions to existing challenges and outlines future research
directions based on identified issues in the field of medical LLMs. By
systematically reviewing previous and advanced research findings, we aim to
highlight the necessity of developing medical LLMs, provide a deeper
understanding of their current state of development, and offer clear guidance
for subsequent research.

</details>


### [156] [Autonomous Data Agents: A New Opportunity for Smart Data](https://arxiv.org/abs/2509.18710)
*Yanjie Fu,Dongjie Wang,Wangyang Ying,Xiangliang Zhang,Huan Liu,Jian Pei*

Main category: cs.AI

TL;DR: 本文介绍数据智能体（DataAgents），一种集成LLM推理的自主代理，旨在自动化数据准备、转换和分析，将复杂数据转化为可操作的知识，标志着数据到知识系统范式的转变。


<details>
  <summary>Details</summary>
Motivation: 随着数据规模和复杂性增加，数据准备、转换和分析劳动密集、重复且难以扩展。数据与AI的对齐至关重要，但数据通常不适合AI利用。传统数据管理工具缺乏动态规划和适应性。

Method: 引入数据智能体（DataAgents），其通过整合LLM推理、任务分解、行动推理与落地以及工具调用，自主解释数据任务、分解子任务、规划工作流、执行Python代码或工具操作，处理数据的收集、整合、预处理、选择、转换、增强和修复等。

Result: 数据智能体代表了自主数据到知识系统的新范式，能够将复杂非结构化数据转化为连贯且可操作的知识。本文定义了DataAgents并讨论了其架构设计、训练策略和所支持的新能力。

Conclusion: 数据智能体是自主数据到知识系统的重要趋势。未来工作应关注行动工作流优化、开放数据集和基准建立、隐私保护、效率与可伸缩性平衡，以及开发可信赖的DataAgent安全防护措施。

Abstract: As data continues to grow in scale and complexity, preparing, transforming,
and analyzing it remains labor-intensive, repetitive, and difficult to scale.
Since data contains knowledge and AI learns knowledge from it, the alignment
between AI and data is essential. However, data is often not structured in ways
that are optimal for AI utilization. Moreover, an important question arises:
how much knowledge can we pack into data through intensive data operations?
Autonomous data agents (DataAgents), which integrate LLM reasoning with task
decomposition, action reasoning and grounding, and tool calling, can
autonomously interpret data task descriptions, decompose tasks into subtasks,
reason over actions, ground actions into python code or tool calling, and
execute operations. Unlike traditional data management and engineering tools,
DataAgents dynamically plan workflows, call powerful tools, and adapt to
diverse data tasks at scale. This report argues that DataAgents represent a
paradigm shift toward autonomous data-to-knowledge systems. DataAgents are
capable of handling collection, integration, preprocessing, selection,
transformation, reweighing, augmentation, reprogramming, repairs, and
retrieval. Through these capabilities, DataAgents transform complex and
unstructured data into coherent and actionable knowledge. We first examine why
the convergence of agentic AI and data-to-knowledge systems has emerged as a
critical trend. We then define the concept of DataAgents and discuss their
architectural design, training strategies, as well as the new skills and
capabilities they enable. Finally, we call for concerted efforts to advance
action workflow optimization, establish open datasets and benchmark ecosystems,
safeguard privacy, balance efficiency with scalability, and develop trustworthy
DataAgent guardrails to prevent malicious actions.

</details>


### [157] [Experience Scaling: Post-Deployment Evolution For Large Language Models](https://arxiv.org/abs/2509.18771)
*Xingkun Yin,Kaibin Huang,Dong In Kim,Hongyang Du*

Main category: cs.AI

TL;DR: 本文提出“经验缩放”框架，通过LLM部署后的自主交互和经验共享，实现持续进化，从而提高准确性、维持性能，并超越静态人类数据带来的限制。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的性能提升依赖模型规模、训练数据和计算能力，但这些方法正因人类生成文本的耗尽而接近饱和，进一步的性能提升变得困难。

Method: 提出“经验缩放”框架，通过LLM与环境自主交互和共享累积经验，实现部署后的持续进化。该框架捕获原始交互，将其提炼为紧凑、可复用的知识，并定期优化存储内容以保持相关性和效率。通过模拟真实场景（新任务泛化、重复查询、知识库饱和）进行验证。

Result: 在所有测试场景中，经验缩放框架均提高了LLM的准确性，随时间推移维持了性能，并在应用于新情境时保持了增益。

Conclusion: 结构化的部署后学习能够将LLM的能力扩展到静态人类生成数据的限制之外，为持续的智能发展提供了一条可扩展的路径。

Abstract: Scaling model size, training data, and compute power have driven advances in
large language models (LLMs), but these approaches are reaching saturation as
human-generated text is exhausted and further gains diminish. We propose
experience scaling, a framework for continuous post-deployment evolution for
LLMs through autonomous interaction with the environment and collaborative
sharing of accumulated experience. The framework captures raw interactions,
distills them into compact, reusable knowledge, and periodically refines stored
content to preserve relevance and efficiency. We validate the framework in
simulated real-world scenarios involving generalization to previously unseen
but related tasks, repetitive queries, and over-saturated knowledge stores.
Across all settings, experience scaling improves accuracy, sustains performance
over time, and maintains gains when applied to novel situations. These results
demonstrate that structured post-deployment learning can extend LLM
capabilities beyond the limits of static human-generated data, offering a
scalable path for continued intelligence progress.

</details>


### [158] [The AGNTCY Agent Directory Service: Architecture and Implementation](https://arxiv.org/abs/2509.18787)
*Luca Muscariello,Vijoy Pandey,Ramiz Polic*

Main category: cs.AI

TL;DR: ADS是一个分布式目录服务，用于发现AI代理的能力、元数据和来源，支持跨异构多代理系统的高效、可验证和多维度发现。


<details>
  <summary>Details</summary>
Motivation: 需要在异构多代理系统（MAS）中实现AI代理能力、元数据和来源的高效、可验证和多维度发现。

Method: ADS基于Open Agentic Schema Framework (OASF)，利用内容寻址存储、分层分类和加密签名。它通过基于Kademlia的分布式哈希表（DHT）实现两级映射，将能力索引与内容位置解耦。同时，它重用OCI/ORAS基础设施进行工件分发，集成Sigstore用于溯源，并支持通过Schema驱动的扩展性以适应新兴代理模式。

Result: ADS实现了AI代理能力、元数据和来源的分布式、高效、可验证和多维度发现。它通过架构模型、存储和发现层、安全和性能特性以及在代理注册和互操作性领域中的定位得到了形式化描述。

Conclusion: ADS提供了一个健壮且可扩展的分布式目录解决方案，能够解决异构多代理系统中的代理发现、互操作性和溯源挑战，并为未来新兴代理模式提供了支持。

Abstract: The Agent Directory Service (ADS) is a distributed directory for the
discovery of AI agent capabilities, metadata, and provenance. It leverages
content-addressed storage, hierarchical taxonomies, and cryptographic signing
to enable efficient, verifiable, and multi-dimensional discovery across
heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema
Framework (OASF), ADS decouples capability indexing from content location
through a two-level mapping realized over a Kademlia-based Distributed Hash
Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact
distribution, integrates Sigstore for provenance, and supports schema-driven
extensibility for emerging agent modalities (LLM prompt agents, MCP servers,
A2A-enabled components). This paper formalizes the architectural model,
describes storage and discovery layers, explains security and performance
properties, and positions ADS within the broader landscape of emerging agent
registry and interoperability initiatives.

</details>


### [159] [Bounded PCTL Model Checking of Large Language Model Outputs](https://arxiv.org/abs/2509.18836)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb*

Main category: cs.AI

TL;DR: LLMCHECKER是一种基于PCTL模型检查的验证方法，用于形式化验证LLM文本生成过程的属性，特别是通过引入α-k边界文本生成来限制关注的token范围。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对LLM文本生成过程的PCTL（概率计算树逻辑）属性进行形式化验证的能力；同时，经验观察发现LLM在生成时通常只选择有限且非固定的token。

Method: 本文提出了LLMCHECKER，一种基于模型检查的验证方法。该方法首先通过经验洞察，引入了“α-k边界文本生成”，将验证焦点限定在每一步文本生成中前k个token中累积概率达到α最大值的token。LLMCHECKER通过考虑初始字符串和后续的前k个token，并结合多样化的文本量化方法（如文本质量和偏见评估），来形式化验证α-k边界LLM的PCTL属性。

Result: 该方法已成功应用于Llama、Gemma、Mistral、Genstruct和BERT等多个大型语言模型，并验证了其适用性。

Conclusion: LLMCHECKER首次将基于PCTL的模型检查应用于验证LLM文本生成过程的一致性，提供了一种新的形式化验证途径。

Abstract: In this paper, we introduce LLMCHECKER, a model-checking-based verification
method to verify the probabilistic computation tree logic (PCTL) properties of
an LLM text generation process. We empirically show that only a limited number
of tokens are typically chosen during text generation, which are not always the
same. This insight drives the creation of $\alpha$-$k$-bounded text generation,
narrowing the focus to the $\alpha$ maximal cumulative probability on the
top-$k$ tokens at every step of the text generation process. Our verification
method considers an initial string and the subsequent top-$k$ tokens while
accommodating diverse text quantification methods, such as evaluating text
quality and biases. The threshold $\alpha$ further reduces the selected tokens,
only choosing those that exceed or meet it in cumulative probability.
LLMCHECKER then allows us to formally verify the PCTL properties of
$\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in
several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our
knowledge, this is the first time PCTL-based model checking has been used to
check the consistency of the LLM text generation process.

</details>


### [160] [Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning](https://arxiv.org/abs/2509.18846)
*Hong-Jie Dai,Zheng-Hao Li,An-Tai Lu,Bo-Tsz Shain,Ming-Ta Li,Tatheer Hussain Mir,Kuang-Te Wang,Min-I Su,Pei-Kang Liu,Ming-Ju Tsai*

Main category: cs.AI

TL;DR: 提出一个模块化框架，通过原则性模型选择、去冗余数据采样和结构化输入设计，解决大语言模型在ICD-10-CM编码预测中的挑战。


<details>
  <summary>Details</summary>
Motivation: ICD编码耗时且易出错，现有LLM在基模型选择、上下文处理和训练数据冗余方面存在局限性，限制了其自动化编码的有效性。

Method: 该框架包括：1. 基于“LLM作为评判者”协议和Plackett-Luce聚合评估和排名开源LLM；2. 引入基于嵌入的相似性度量和去冗余采样策略来清理数据；3. 利用结构化出院摘要设计输入，并评估不同临床部分的上下文效应。

Result: 经过微调的基模型在内部和外部评估中持续优于基线LLM。纳入更多临床部分能持续提高预测性能。

Conclusion: 本研究提供了一个实用且有原则的ICD-10-CM代码预测方法，该框架是一个可扩展、机构就绪的解决方案，适用于自动化医疗编码系统的实际部署。

Abstract: Accurate International Classification of Diseases (ICD) coding is critical
for clinical documentation, billing, and healthcare analytics, yet it remains a
labour-intensive and error-prone task. Although large language models (LLMs)
show promise in automating ICD coding, their challenges in base model
selection, input contextualization, and training data redundancy limit their
effectiveness. We propose a modular framework for ICD-10 Clinical Modification
(ICD-10-CM) code prediction that addresses these challenges through principled
model selection, redundancy-aware data sampling, and structured input design.
The framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce
aggregation to assess and rank open-source LLMs based on their intrinsic
comprehension of ICD-10-CM code definitions. We introduced embedding-based
similarity measures, a redundancy-aware sampling strategy to remove
semantically duplicated discharge summaries. We leverage structured discharge
summaries from Taiwanese hospitals to evaluate contextual effects and examine
section-wise content inclusion under universal and section-specific modelling
paradigms. Experiments across two institutional datasets demonstrate that the
selected base model after fine-tuning consistently outperforms baseline LLMs in
internal and external evaluations. Incorporating more clinical sections
consistently improves prediction performance. This study uses open-source LLMs
to establish a practical and principled approach to ICD-10-CM code prediction.
The proposed framework provides a scalable, institution-ready solution for
real-world deployment of automated medical coding systems by combining informed
model selection, efficient data refinement, and context-aware prompting.

</details>


### [161] [MAPO: Mixed Advantage Policy Optimization](https://arxiv.org/abs/2509.18849)
*Wenke Huang,Quan Zhang,Yiyang Fang,Jian Liang,Xuankun Rong,Huanjin Yao,Guancheng Wan,Ke Liang,Wenwen He,Mingjun Li,Leszek Rutkowski,Mang Ye,Bo Du,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出混合优势策略优化（MAPO），通过考虑轨迹确定性并动态调整优势函数，解决了GRPO中优势函数分配不合理的问题，从而提高了基础模型在推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管GRPO显著提升了基础模型在推理任务上的性能，但其核心机制——优势函数在分配轨迹重要性时存在“优势反转”和“优势镜像”问题，导致优势分配不合理，亟需改进。

Method: 本文提出MAPO策略，通过揭示轨迹具有不同确定性。具体而言，针对高确定性轨迹样本，提出优势百分比偏差；并根据轨迹确定性动态地重新加权优势函数，以自适应地配置优势函数来适应样本特异性特征。

Result: 通过与现有最先进方法的比较以及对不同优势变体的消融研究，验证了MAPO策略的有效性。

Conclusion: MAPO通过对GRPO中优势函数的创新性改进，成功解决了优势分配难题，从而显著提升了基础模型在推理任务中的表现。

Abstract: Recent advances in reinforcement learning for foundation models, such as
Group Relative Policy Optimization (GRPO), have significantly improved the
performance of foundation models on reasoning tasks. Notably, the advantage
function serves as a central mechanism in GRPO for ranking the trajectory
importance. However, existing explorations encounter both advantage reversion
and advantage mirror problems, which hinder the reasonable advantage allocation
across different query samples. In this work, we propose an easy but effective
GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the
trajectory appears with different certainty and propose the advantage percent
deviation for samples with high-certainty trajectories. Furthermore, we
dynamically reweight the advantage function for samples with varying trajectory
certainty, thereby adaptively configuring the advantage function to account for
sample-specific characteristics. Comparison with related state-of-the-art
methods, along with ablation studies on different advantage variants, validates
the effectiveness of our approach.

</details>


### [162] [Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling](https://arxiv.org/abs/2509.18864)
*Yingxin Li,Jianbo Zhao,Xueyu Ren,Jie Tang,Wangjie You,Xu Chen,Kan Zhou,Chao Feng,Jiao Ran,Yuan Meng,Zhi Wang*

Main category: cs.AI

TL;DR: 针对LLMs在用户画像中缺乏基准和标签的挑战，本文提出ProfileBench基准和一个置信度驱动的两阶段框架Conf-Profile，通过标签合成和强化学习显著提升了用户画像的性能。


<details>
  <summary>Details</summary>
Motivation: 用户画像是理解用户的核心技术，LLMs在此领域具有巨大潜力。然而，现有进展受限于缺乏全面的基准。同时，用户画像任务面临难以收集大规模真实标签、以及异构和噪声用户信息影响LLMs可靠性的难题。

Method: 本文提出ProfileBench，一个源自真实视频平台的工业级基准，包含异构用户数据和结构化的画像分类体系。为实现无标签且可靠的用户画像，提出Conf-Profile框架，采用两阶段范式：首先，利用高级LLM结合置信度提示合成高质量标签，并通过置信度加权投票和校准来提升准确性和平衡分布，然后将结果蒸馏到轻量级LLM中。其次，通过置信度引导的无监督强化学习进一步增强推理能力，利用置信度进行难度过滤、准真实标签投票和奖励加权。

Result: 实验结果表明，Conf-Profile通过其两阶段训练显著提升了性能，在Qwen3-8B模型上F1分数提高了13.97。

Conclusion: Conf-Profile框架通过置信度驱动的标签合成和强化学习，有效解决了LLMs在用户画像任务中缺乏大规模真实标签和信息噪声的问题，实现了性能的显著提升，并为用户画像领域提供了急需的基准ProfileBench。

Abstract: User profiling, as a core technique for user understanding, aims to infer
structural attributes from user information. Large Language Models (LLMs)
provide a promising avenue for user profiling, yet the progress is hindered by
the lack of comprehensive benchmarks. To bridge this gap, we propose
ProfileBench, an industrial benchmark derived from a real-world video platform,
encompassing heterogeneous user data and a well-structured profiling taxonomy.
However, the profiling task remains challenging due to the difficulty of
collecting large-scale ground-truth labels, and the heterogeneous and noisy
user information can compromise the reliability of LLMs. To approach label-free
and reliable user profiling, we propose a Confidence-driven Profile reasoning
framework Conf-Profile, featuring a two-stage paradigm. We first synthesize
high-quality labels by leveraging advanced LLMs with confidence hints, followed
by confidence-weighted voting for accuracy improvement and confidence
calibration for a balanced distribution. The multiple profile results,
rationales, and confidence scores are aggregated and distilled into a
lightweight LLM. We further enhance the reasoning ability via confidence-guided
unsupervised reinforcement learning, which exploits confidence for difficulty
filtering, quasi-ground truth voting, and reward weighting. Experimental
results demonstrate that Conf-Profile delivers substantial performance through
the two-stage training, improving F1 by 13.97 on Qwen3-8B.

</details>


### [163] [Memory in Large Language Models: Mechanisms, Evaluation and Evolution](https://arxiv.org/abs/2509.18868)
*Dianxing Zhang,Wendong Li,Kani Song,Jiaye Lu,Gang Li,Liuchun Yang,Sheng Li*

Main category: cs.AI

TL;DR: 本文提出了一个统一的大语言模型（LLM）记忆操作性定义、四部分分类法、记忆四元组、分层评估协议和可审计的记忆更新与遗忘框架（DMM Gov），旨在为LLM记忆研究与部署提供一个可复现、可比较和可治理的坐标系统。


<details>
  <summary>Details</summary>
Motivation: 为避免在异构设置下对LLM记忆能力进行扭曲比较，并建立一个统一的LLM记忆操作性定义，同时解决LLM记忆的评估、治理、更新和遗忘问题。

Method: 1. 提出了LLM记忆的统一操作性定义。2. 建立了四部分分类法（参数记忆、上下文记忆、外部记忆、程序/情景记忆）和记忆四元组。3. 提出三设置协议（仅参数、离线检索、在线检索）以解耦能力与信息可用性。4. 构建了分层评估（涵盖参数、上下文、外部和程序/情景记忆）。5. 集成了时间治理、泄漏审计和不确定性报告。6. 提出了DMM Gov框架，协调DAPT/TAPT、PEFT、模型编辑（如ROME、MEND、MEMIT、SERAC）和RAG，形成可审计的更新与遗忘闭环。

Result: 本研究成果为一个可复现、可比较和可治理的LLM记忆研究与部署坐标系统。提出了四个可测试的命题：最小可识别性、最小评估卡、可验证遗忘的因果约束编辑、以及何时小窗口重放的检索优于超长上下文阅读。

Conclusion: 该框架提供了一个全面、统一的LLM记忆理解、评估、治理和更新机制，为LLM记忆的理论研究与实际应用奠定了坚实的基础，确保了研究的可复现性、可比较性和可治理性。

Abstract: Under a unified operational definition, we define LLM memory as a persistent
state written during pretraining, finetuning, or inference that can later be
addressed and that stably influences outputs. We propose a four-part taxonomy
(parametric, contextual, external, procedural/episodic) and a memory quadruple
(location, persistence, write/access path, controllability). We link mechanism,
evaluation, and governance via the chain write -> read -> inhibit/update. To
avoid distorted comparisons across heterogeneous setups, we adopt a
three-setting protocol (parametric only, offline retrieval, online retrieval)
that decouples capability from information availability on the same data and
timeline. On this basis we build a layered evaluation: parametric (closed-book
recall, edit differential, memorization/privacy), contextual (position curves
and the mid-sequence drop), external (answer correctness vs snippet
attribution/faithfulness), and procedural/episodic (cross-session consistency
and timeline replay, E MARS+). The framework integrates temporal governance and
leakage auditing (freshness hits, outdated answers, refusal slices) and
uncertainty reporting via inter-rater agreement plus paired tests with
multiple-comparison correction. For updating and forgetting, we present DMM
Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),
and RAG to form an auditable loop covering admission thresholds, rollout,
monitoring, rollback, and change audits, with specs for timeliness, conflict
handling, and long-horizon consistency. Finally, we give four testable
propositions: minimum identifiability; a minimal evaluation card; causally
constrained editing with verifiable forgetting; and when retrieval with
small-window replay outperforms ultra-long-context reading. This yields a
reproducible, comparable, and governable coordinate system for research and
deployment.

</details>


### [164] [LongCat-Flash-Thinking Technical Report](https://arxiv.org/abs/2509.18883)
*Meituan LongCat Team,Anchun Gui,Bei Li,Bingyang Tao,Bole Zhou,Borun Chen,Chao Zhang,Chao Zhang,Chengcheng Han,Chenhui Yang,Chi Zhang,Chong Peng,Chuyu Zhang,Cong Chen,Fengcun Li,Gang Xu,Guoyuan Lin,Hao Jiang,Hao Liang,Haomin Fu,Haoxiang Ma,Hong Liu,Hongyan Hao,Hongyin Tang,Hongyu Zang,Hongzhi Ni,Hui Su,Jiahao Liu,Jiahuan Li,Jialin Liu,Jianfei Zhang,Jianhao Xu,Jianing Wang,Jiaqi Sun,Jiaqi Zhang,Jiarong Shi,Jiawei Yang,Jingang Wang,Jinrui Ding,Jun Kuang,Jun Xu,Ke He,Kefeng Zhang,Keheng Wang,Keqing He,Li Wei,Liang Shi,Lin Qiu,Lingbin Kong,Lingchuan Liu,Linsen Guo,Longfei An,Mai Xia,Meng Zhou,Mengshen Zhu,Peng Pei,Pengcheng Jia,Qi Gu,Qi Guo,Qiong Huang,Quan Chen,Quanchi Weng,Rongxiang Weng,Ruichen Shao,Rumei Li,Shanglin Lei,Shuai Du,Shuaikang Liu,Shuang Zhou,Shuhao Hu,Siyu Xu,Songshan Gong,Tao Liang,Tianhao Hu,Wei He,Wei Shi,Wei Wang,Wei Wu,Wei Zhuo,Weifeng Tang,Wenjie Shi,Wenlong Zhu,Xi Su,Xiangcheng Liu,Xiangyu Xi,Xiangzhou Huang,Xiao Liu,Xiaochen Jiang,Xiaowei Shi,Xiaowen Shi,Xiaoyu Li,Xin Chen,Xinyue Zhao,Xuan Huang,Xuemiao Zhang,Xuezhi Cao,Xunliang Cai,Yajie Zhang,Yang Chen,Yang Liu,Yang Liu,Yang Zheng,Yaoming Wang,Yaqi Huo,Yerui Sun,Yifan Lu,Yiyang Li,Youshao Xiao,Yuanzhe Lei,Yuchen Xie,Yueqing Sun,Yufei Zhang,Yuhuai Wei,Yulei Qian,Yunke Zhao,Yuqing Ding,Yuwei Jiang,Zhaohua Yang,Zhengyu Chen,Zhijian Liu,Zhikang Xia,Zhongda Su,Ziran Li,Ziwen Wang,Ziyuan Zhuang,Zongyu Wang,Zunyuan Yang*

Main category: cs.AI

TL;DR: LongCat-Flash-Thinking是一个5600亿参数的开源MoE推理模型，通过CoT冷启动和大规模RL训练，在复杂推理任务上达到SOTA性能，并在智能体推理中显著提升效率，减少64.5%的token消耗。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个高效、强大的开源推理模型，特别是在复杂推理和智能体AI方面，以推动相关领域的研究进展。

Method: 采用CoT数据冷启动和大规模强化学习（RL）进行训练。核心方法包括：精心设计的冷启动策略以增强推理能力；域并行训练方案，将不同领域（如STEM、代码、智能体）的优化解耦并融合；使用DORA系统，一个异步RL框架，实现训练速度提升三倍以上。

Result: LongCat-Flash-Thinking在复杂推理任务上达到开源模型的SOTA性能。在AIME-25智能体推理任务中，平均token消耗减少64.5%（从19,653降至6,965），同时未降低任务准确率。

Conclusion: 成功开发并发布了LongCat-Flash-Thinking，一个在复杂推理和智能体AI方面表现出最先进性能和卓越效率的开源模型，旨在促进推理系统和智能体AI的进一步发展。

Abstract: We present LongCat-Flash-Thinking, an efficient 560-billion-parameter
open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities
are cultivated through a meticulously crafted training process, beginning with
long Chain-of-Thought (CoT) data cold-start and culminating in large-scale
Reinforcement Learning (RL). We first employ a well-designed cold-start
training strategy, which significantly enhances the reasoning potential and
equips the model with specialized skills in both formal and agentic reasoning.
Then, a core innovation is our domain-parallel training scheme, which decouples
optimization across distinct domains (e.g., STEM, Code, Agentic) and
subsequently fuses the resulting expert models into a single, nearly
Pareto-optimal model. This entire process is powered by our Dynamic
ORchestration for Asynchronous rollout (DORA) system, a large-scale RL
framework that delivers a greater than threefold training speedup over
synchronous methods on tens of thousands of accelerators. As a result,
LongCat-Flash-Thinking achieves state-of-the-art performance among open-source
models on a suite of complex reasoning tasks. The model exhibits exceptional
efficiency in agentic reasoning, reducing average token consumption by 64.5%
(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We
release LongCat-Flash-Thinking to promote further advances in reasoning systems
and agentic AI research.

</details>


### [165] [How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective](https://arxiv.org/abs/2509.18905)
*Songsong Yu,Yuxin Chen,Hao Ju,Lianjie Jia,Fuxi Zhang,Shaofei Huang,Yuhan Wu,Rundi Cui,Binghao Ran,Zaibin Zhang,Zhedong Zheng,Zhipeng Zhang,Yifan Wang,Lin Song,Lijun Wang,Yanwei Li,Ying Shan,Huchuan Lu*

Main category: cs.AI

TL;DR: 本文系统性地调查了视觉空间推理（VSR）在视觉语言模型（VLMs）中的表现，提出了一个三级空间智能分类和SIBench基准，并揭示了VLM在感知与推理之间存在的显著差距。


<details>
  <summary>Details</summary>
Motivation: 视觉空间推理（VSR）是人类核心认知能力，对具身智能和自主系统至关重要。然而，当前的视觉语言模型（VLMs）在复杂的三维空间表示和推理方面仍面临巨大挑战，难以达到人类水平。

Method: 研究方法包括：系统回顾现有VSR方法（涵盖输入模态、模型架构、训练策略和推理机制）；将空间智能分为基本感知、空间理解和空间规划三个能力层级；构建SIBench空间智能基准，整合近20个开源数据集，涵盖23种任务设置。

Result: 实验结果显示，最先进的VLMs在基本感知任务上表现尚可，但在空间理解和规划任务（尤其是在数值估计、多视角推理、时间动态和空间想象方面）上持续表现不佳，揭示了感知与推理之间存在显著差距。

Conclusion: 当前实现空间智能仍面临巨大挑战。本研究为未来该领域的研究提供了一个系统性路线图和全面的基准，以推动空间智能的发展。

Abstract: Visual Spatial Reasoning (VSR) is a core human cognitive ability and a
critical requirement for advancing embodied intelligence and autonomous
systems. Despite recent progress in Vision-Language Models (VLMs), achieving
human-level VSR remains highly challenging due to the complexity of
representing and reasoning over three-dimensional space. In this paper, we
present a systematic investigation of VSR in VLMs, encompassing a review of
existing methodologies across input modalities, model architectures, training
strategies, and reasoning mechanisms. Furthermore, we categorize spatial
intelligence into three levels of capability, ie, basic perception, spatial
understanding, spatial planning, and curate SIBench, a spatial intelligence
benchmark encompassing nearly 20 open-source datasets across 23 task settings.
Experiments with state-of-the-art VLMs reveal a pronounced gap between
perception and reasoning, as models show competence in basic perceptual tasks
but consistently underperform in understanding and planning tasks, particularly
in numerical estimation, multi-view reasoning, temporal dynamics, and spatial
imagination. These findings underscore the substantial challenges that remain
in achieving spatial intelligence, while providing both a systematic roadmap
and a comprehensive benchmark to drive future research in the field. The
related resources of this study are accessible at
https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.

</details>


### [166] [Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning](https://arxiv.org/abs/2509.18942)
*Xiao Han,Zimo Zhao,Wanyu Wang,Maolin Wang,Zitao Liu,Yi Chang,Xiangyu Zhao*

Main category: cs.AI

TL;DR: 本文提出了DEAL框架，结合LoRA和持续微调策略，旨在解决LLM微调中的灾难性遗忘和数据效率低下问题，并在多样化数据集上取得了显著的性能和资源效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）微调方法存在灾难性遗忘和数据效率低下等问题，限制了其在实际场景中的应用，尤其在从头训练计算量不可行时。

Method: 本文提出了一种名为DEAL的新颖框架，该框架将低秩适应（LoRA）与持续微调策略相结合，并集成了知识保留和自适应参数更新模块。

Result: 在15个多样化数据集上的实验表明，DEAL框架持续优于基线方法，在任务准确性和资源效率方面取得了显著提升。

Conclusion: DEAL方法通过提高LLM的任务性能和资源效率，展示了其在推进LLM持续适应性方面的巨大潜力。

Abstract: Recent advancements in Large Language Models (LLMs) have emphasized the
critical role of fine-tuning (FT) techniques in adapting LLMs to specific
tasks, especially when retraining from scratch is computationally infeasible.
Fine-tuning enables LLMs to leverage task- or domain-specific data, producing
models that more effectively meet the requirements of targeted applications.
However, con- ventional FT approaches often suffer from catastrophic forgetting
and suboptimal data efficiency, limiting their real-world applicability. To
address these challenges, this paper proposes DEAL, a novel framework that
integrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.
By incorporating knowledge retention and adaptive parameter update modules, the
framework mitigates the lim- itations of existing FT methods while maintaining
efficiency in privacy-preserving settings. Experiments on 15 diverse datasets
show that DEAL consistently outper- forms baseline methods, yielding
substantial gains in task accuracy and resource efficiency. These findings
demonstrate the potential of our approach to advance continual adaptation in
LLMs by enhancing task performance while improving resource efficiency.

</details>


### [167] [LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions](https://arxiv.org/abs/2509.18970)
*Xixun Lin,Yucheng Ning,Jingwen Zhang,Yan Dong,Yilong Liu,Yongxuan Wu,Xiaohua Qi,Nan Sun,Yanmin Shang,Pengfei Cao,Lixin Zou,Xu Chen,Chuan Zhou,Jia Wu,Shirui Pan,Bin Wang,Yanan Cao,Kai Chen,Songlin Hu,Li Guo*

Main category: cs.AI

TL;DR: 本综述首次全面分析了大型语言模型（LLM）代理的幻觉问题，提出了新的分类法，探讨了触发原因，并总结了缓解与检测方法，旨在促进更可靠的代理系统开发。


<details>
  <summary>Details</summary>
Motivation: LLM代理在多领域广泛应用，但其固有的幻觉问题导致任务执行错误，严重影响系统可靠性。因此，亟需深入理解并系统整合相关进展以解决此关键挑战。

Method: 本文通过分析LLM代理的完整工作流程，提出了一个识别不同阶段幻觉类型的新分类法。此外，深入研究了18种导致代理幻觉的触发原因，并通过大量现有研究总结了幻觉的缓解和检测方法。

Result: 本研究提出了一个针对LLM代理幻觉的新分类法，识别了18种幻觉的触发原因，并系统性地总结了现有缓解和检测幻觉的方法。

Conclusion: 本综述旨在为解决LLM代理的幻觉问题提供全面理解，并希望激发未来的研究努力，最终促进开发更稳健和可靠的LLM代理系统。

Abstract: Driven by the rapid advancements of Large Language Models (LLMs), LLM-based
agents have emerged as powerful intelligent systems capable of human-like
cognition, reasoning, and interaction. These agents are increasingly being
deployed across diverse real-world applications, including student education,
scientific research, and financial analysis. However, despite their remarkable
potential, LLM-based agents remain vulnerable to hallucination issues, which
can result in erroneous task execution and undermine the reliability of the
overall system design. Addressing this critical challenge requires a deep
understanding and a systematic consolidation of recent advances on LLM-based
agents. To this end, we present the first comprehensive survey of
hallucinations in LLM-based agents. By carefully analyzing the complete
workflow of agents, we propose a new taxonomy that identifies different types
of agent hallucinations occurring at different stages. Furthermore, we conduct
an in-depth examination of eighteen triggering causes underlying the emergence
of agent hallucinations. Through a detailed review of a large number of
existing studies, we summarize approaches for hallucination mitigation and
detection, and highlight promising directions for future research. We hope this
survey will inspire further efforts toward addressing hallucinations in
LLM-based agents, ultimately contributing to the development of more robust and
reliable agent systems.

</details>


### [168] [From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system](https://arxiv.org/abs/2509.18980)
*Maxime Manderlier,Fabian Lecron,Olivier Vu Thanh,Nicolas Gillis*

Main category: cs.AI

TL;DR: 本研究探讨了大型语言模型（LLM）能否从数学上可解释的推荐模型生成有效的用户解释。一项326名参与者的用户研究表明，所有解释类型都普遍受到好评，策略间存在中等统计差异。


<details>
  <summary>Details</summary>
Motivation: 研究LLM是否能为可解释的推荐模型生成有效的、面向用户的解释。现有可解释AI工作多依赖自动评估指标，未能捕捉用户真实需求，因此需要采用以用户为中心的方法。

Method: 使用基于约束矩阵分解的推荐模型（具有显式用户类型和可解释的预测分数）。通过精心设计的LLM提示，将模型结构转换为自然语言解释。通过改变LLM的输入信息生成多种解释类型。对326名参与者进行用户研究，评估解释在透明度、有效性、说服力、信任和满意度五个关键维度上的质量以及推荐本身。

Result: 所有解释类型普遍受到用户的好评。不同解释策略之间存在中等的统计差异。用户评论为量化结果提供了补充性见解，进一步强调了参与者对不同类型解释的反应。

Conclusion: 大型语言模型能够有效生成可解释推荐模型的面向用户解释，且这些解释普遍受到用户的好评。用户中心评估方法是有效的，不同解释策略的感知差异值得进一步关注。

Abstract: We investigate whether large language models (LLMs) can generate effective,
user-facing explanations from a mathematically interpretable recommendation
model. The model is based on constrained matrix factorization, where user types
are explicitly represented and predicted item scores share the same scale as
observed ratings, making the model's internal representations and predicted
scores directly interpretable. This structure is translated into natural
language explanations using carefully designed LLM prompts. Many works in
explainable AI rely on automatic evaluation metrics, which often fail to
capture users' actual needs and perceptions. In contrast, we adopt a
user-centered approach: we conduct a study with 326 participants who assessed
the quality of the explanations across five key dimensions-transparency,
effectiveness, persuasion, trust, and satisfaction-as well as the
recommendations themselves.To evaluate how different explanation strategies are
perceived, we generate multiple explanation types from the same underlying
model, varying the input information provided to the LLM. Our analysis reveals
that all explanation types are generally well received, with moderate
statistical differences between strategies. User comments further underscore
how participants react to each type of explanation, offering complementary
insights beyond the quantitative results.

</details>


### [169] [Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)](https://arxiv.org/abs/2509.18986)
*Erik Penther,Michael Grohs,Jana-Rebecca Rehse*

Main category: cs.AI

TL;DR: 本文比较了四种剩余时间预测方法在一个航空物流公司的真实出库流程中的表现。结果显示，深度学习模型准确性最高，但浅层方法如提升技术在保持竞争力准确性的同时，计算资源需求显著更少。


<details>
  <summary>Details</summary>
Motivation: 预测流程执行的剩余时间是流程挖掘中预测性流程监控的一个重要目标，旨在展望进行中流程的未来。

Method: 本文在一个航空业务物流公司的真实出库仓库流程中，比较了四种不同的剩余时间预测方法。研究使用了公司提供的一个包含169,523条轨迹的全新事件日志。

Result: 研究发现，深度学习模型取得了最高的准确性。然而，像传统提升技术这样的浅层方法也取得了具有竞争力的准确性，并且所需的计算资源显著更少。

Conclusion: 尽管深度学习模型在预测准确性方面表现最佳，但考虑到计算资源限制，浅层方法如传统提升技术提供了具有竞争力的性能，是实际应用中值得考虑的选择。

Abstract: Predictive process monitoring is a sub-domain of process mining which aims to
forecast the future of ongoing process executions. One common prediction target
is the remaining time, meaning the time that will elapse until a process
execution is completed. In this paper, we compare four different remaining time
prediction approaches in a real-life outbound warehouse process of a logistics
company in the aviation business. For this process, the company provided us
with a novel and original event log with 169,523 traces, which we can make
publicly available. Unsurprisingly, we find that deep learning models achieve
the highest accuracy, but shallow methods like conventional boosting techniques
achieve competitive accuracy and require significantly fewer computational
resources.

</details>


### [170] [Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action](https://arxiv.org/abs/2509.19030)
*Victoire Hervé,Henrik Warpefelt,Christoph Salge*

Main category: cs.AI

TL;DR: 针对程序化生成内容（PCG）评估与人类体验不符的问题，本文提出了“地标”、“纪念碑”和“信标”等以玩家为中心的概念，以实现PCG的自动化分解和子组件评估，旨在弥合人文与技术游戏研究之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现有程序化生成内容（PCG）的算法评估难以找到与人类体验相符的指标，尤其是对于复合型作品。自动分解作为一种可能的解决方案，需要一系列符合特定属性的概念。

Method: 借鉴游戏研究和游戏AI研究，作者引入了“地标”（Landmarks）、“纪念碑”（Monuments）和“信标”（Beacons）这三个嵌套概念。这些概念以玩家为中心，基于作品的“可感知性”、“唤起性”和“行动召唤”来定义。

Result: 这些新概念实体可以使用现有研究和行业技术进行发现和评估，从而为PCG的完全自动化分解及其突出子组件的评估提供了可行途径。

Conclusion: 该方法旨在连接人文与技术游戏研究，并实现更优的计算PCG评估。尽管强调了混合主动PCG和组合PCG，但该方法预计可应用于更广泛的领域。

Abstract: Algorithmic evaluation of procedurally generated content struggles to find
metrics that align with human experience, particularly for composite artefacts.
Automatic decomposition as a possible solution requires concepts that meet a
range of properties. To this end, drawing on Games Studies and Game AI
research, we introduce the nested concepts of \textit{Landmarks},
\textit{Monuments}, and \textit{Beacons}. These concepts are based on the
artefact's perceivability, evocativeness, and Call to Action, all from a
player-centric perspective. These terms are generic to games and usable across
genres. We argue that these entities can be found and evaluated with techniques
currently used in both research and industry, opening a path towards a fully
automated decomposition of PCG, and evaluation of the salient sub-components.
Although the work presented here emphasises mixed-initiative PCG and
compositional PCG, we believe it applies beyond those domains. With this
approach, we intend to create a connection between humanities and technical
game research and allow for better computational PCG evaluation

</details>


### [171] [Towards Causal Representation Learning with Observable Sources as Auxiliaries](https://arxiv.org/abs/2509.19058)
*Kwonho Kim,Heejeong Nam,Inwoo Hwang,Sanghack Lee*

Main category: cs.AI

TL;DR: 本文提出一种因果表示学习框架，利用可观测源作为内部辅助变量，以识别潜在因子并提高其可恢复性。


<details>
  <summary>Details</summary>
Motivation: 现有因果表示学习方法通常依赖外部辅助变量实现可识别性，但未充分利用可观测到的系统驱动潜在因子。本文旨在通过利用这些内部可观测源来增强潜在因子的识别能力。

Method: 引入了一个将可观测源作为内部辅助变量的框架，并将其作为有效的条件变量。该方法使用体积保持编码器实现潜在变量的识别，并提供一个变量选择方案，以在存在多个已知辅助变量时最大化潜在因子的可恢复性。

Result: 结果表明，该框架可以使用体积保持编码器识别出所有潜在变量（在子空间变换和置换的条件下）。变量选择方案能最大化潜在因子的可恢复性。并在合成图和图像数据上通过实验验证了其有效性。

Conclusion: 该框架通过有效利用内部可观测源，扩展了当前因果表示学习方法的应用范围和识别能力。

Abstract: Causal representation learning seeks to recover latent factors that generate
observational data through a mixing function. Needing assumptions on latent
structures or relationships to achieve identifiability in general, prior works
often build upon conditional independence given known auxiliary variables.
However, prior frameworks limit the scope of auxiliary variables to be external
to the mixing function. Yet, in some cases, system-driving latent factors can
be easily observed or extracted from data, possibly facilitating
identification. In this paper, we introduce a framework of observable sources
being auxiliaries, serving as effective conditioning variables. Our main
results show that one can identify entire latent variables up to subspace-wise
transformations and permutations using volume-preserving encoders. Moreover,
when multiple known auxiliary variables are available, we offer a
variable-selection scheme to choose those that maximize recoverability of the
latent factors given knowledge of the latent causal graph. Finally, we
demonstrate the effectiveness of our framework through experiments on synthetic
graph and image data, thereby extending the boundaries of current approaches.

</details>


### [172] [Code Driven Planning with Domain-Adaptive Critic](https://arxiv.org/abs/2509.19077)
*Zikang Tian,Shaohui Peng,Du Huang,Jiaming Guo,Ruizhi Chen,Rui Zhang,Xishan Zhang,Yuxuan Guo,Zidong Du,Qi Guo,Ling Li,Yewen Pu,Xing Hu,Yunji Chen*

Main category: cs.AI

TL;DR: 针对LLM作为AI代理规划器面临的环境适应性差、查询成本高和无法优化长期奖励的问题，本文提出了CoPiC。该方法利用LLM生成高层规划程序来迭代生成和精炼计划，并使用领域自适应评论器评估长期奖励，显著提高了规划成功率并大幅降低了查询成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM规划器由于通用知识与特定环境需求之间存在差距，导致计划不准确。为纠正计划，现有方法依赖频繁的LLM查询，导致高昂成本，且这种基于短期反馈的修正限制了LLM制定符合长期奖励计划的能力。

Method: CoPiC不依赖频繁查询，而是让LLM生成一套多样化的高层规划程序，这些程序能迭代地生成和精炼候选计划。随后，一个训练过的领域自适应评论器评估这些候选计划，并选择最符合长期奖励的计划进行执行。

Result: 在ALFWorld、NetHack和StarCraft II Unit Building任务中，CoPiC的表现优于AdaPlanner和Reflexion等先进的LLM基线，平均成功率提高了23.33%，查询成本降低了91.27%。

Conclusion: CoPiC通过结合高层规划程序作为规划器和领域自适应评论器作为评估器，有效改善了LLM在序列决策问题中的规划能力，显著降低了查询成本，并实现了对长期奖励的优化。

Abstract: Large Language Models (LLMs) have been widely adopted as task planners for AI
agents in sequential decision-making problems, leveraging their extensive world
knowledge. However, the gap between their general knowledge and
environment-specific requirements often leads to inaccurate plans. To address
this, existing approaches rely on frequent LLM queries to iteratively refine
plans based on immediate environmental feedback, which incurs substantial query
costs. However, this refinement is typically guided by short-term environmental
feedback, limiting LLMs from developing plans aligned with long-term rewards.
We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of
relying on frequent queries, CoPiC employs LLMs to generate a diverse set of
high-level planning programs, which iteratively produce and refine candidate
plans. A trained domain-adaptive critic then evaluates these candidates and
selects the one most aligned with long-term rewards for execution. Using
high-level planning programs as planner and domain-adaptive critic as
estimator, CoPiC improves planning while significantly reducing query costs.
Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC
outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving
an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in
query costs.

</details>


### [173] [AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration](https://arxiv.org/abs/2509.19236)
*Chunhao Tian,Yutong Wang,Xuebo Liu,Zhexuan Wang,Liang Ding,Miao Zhang,Min Zhang*

Main category: cs.AI

TL;DR: AgentInit是一种多智能体系统（MAS）初始化方法，通过优化智能体团队结构来促进协作，其性能优于现有方法并显著降低了token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统初始化方法未能充分考虑生成智能体在后续阶段的协作需求，导致团队组成不佳，影响系统效率和有效性。

Method: 提出了AgentInit方法。它在智能体生成过程中引入多轮交互和反思，结合自然语言到格式（Natural Language to Format）机制以确保一致性和标准化，并采用基于帕累托原则的平衡团队选择策略，综合考虑团队多样性和任务相关性。

Result: 实验表明，AgentInit在不同框架和任务上始终优于现有先进初始化方法和预定义策略，性能提升分别高达1.2倍和1.6倍，并显著降低了token消耗。此外，它显示出强大的任务迁移能力，并验证了其关键组件的有效性。

Conclusion: 鉴于其卓越的性能提升、资源消耗降低以及强大的可迁移性，AgentInit是一种能力强、适应性广且可靠的多智能体系统初始化方法。

Abstract: Proper initialization is crucial for any system, particularly in multi-agent
systems (MAS), where it plays a pivotal role in determining both the system's
efficiency and effectiveness. However, existing MAS initialization methods do
not fully account for the collaborative needs of the generated agents in
subsequent stages. Inspired by the principles of effective team composition, we
propose AgentInit, which aims to optimize the structure of agent teams.
Specifically, in addition to multi-round interactions and reflections between
agents during agent generation, AgentInit incorporates a Natural Language to
Format mechanism to ensure consistency and standardization. Balanced team
selection strategies using Pareto principles are subsequently applied to
jointly consider agent team diversity and task relevance to promote effective
and efficient collaboration and enhance overall system performance. Experiments
show that AgentInit consistently outperforms state-of-the-art initialization
methods and pre-defined strategies across various frameworks and tasks,
achieving an overall performance improvement of up to 1.2 and 1.6,
respectively, while also significantly reducing token consumption. Further
analysis confirms its strong transferability to similar tasks and verifies the
effectiveness of its key components, demonstrating its capability and
adaptability as a reliable MAS initialization method. Source code and models
are available at https://github.com/1737423697/AgentInit.

</details>


### [174] [Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World](https://arxiv.org/abs/2509.19265)
*Saeed Almheiri,Rania Hossam,Mena Attia,Chenxi Wang,Preslav Nakov,Timothy Baldwin,Fajri Koto*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型在阿拉伯世界的跨文化常识推理转移，发现少量文化特定示例或跨文化示例均能有效提升模型性能，证明了高效跨文化对齐的可行性，并为低资源文化环境下的LLM适应提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）普遍存在西方中心偏见，限制了其在多样文化背景下的有效性。尽管已有一些文化对齐研究，但利用一种文化背景的对齐来提升其他文化背景性能的跨文化转移潜力仍未被充分探索。

Method: 研究使用涵盖13个阿拉伯国家的文化常识推理数据集，评估了情境学习和基于演示的强化（DITTO）等轻量级对齐方法，并与监督微调和直接偏好优化等基线方法进行了比较。

Result: 结果显示，在多语言模型中，仅用12个来自一个国家的文化特定示例，即可平均提升其他国家性能10%。此外，来自印度尼西亚和美国等非本文化背景的演示，在多项选择题推理中能与本文化对齐效果持平或超越，凸显了文化常识的跨文化可转移性不仅限于阿拉伯世界。

Conclusion: 研究表明，高效的跨文化对齐是可能实现的，为将大型语言模型适应到资源匮乏的文化环境中提供了一种有前景的方法。

Abstract: Large language models (LLMs) often reflect Western-centric biases, limiting
their effectiveness in diverse cultural contexts. Although some work has
explored cultural alignment, the potential for cross-cultural transfer, using
alignment in one culture to improve performance in others, remains
underexplored. This paper investigates cross-cultural transfer of commonsense
reasoning in the Arab world, where linguistic and historical similarities
coexist with local cultural differences. Using a culturally grounded
commonsense reasoning dataset covering 13 Arab countries, we evaluate
lightweight alignment methods such as in-context learning and
demonstration-based reinforcement (DITTO), alongside baselines like supervised
fine-tuning and direct preference optimization. Our results show that merely 12
culture-specific examples from one country can improve performance in others by
10\% on average, within multilingual models. In addition, we demonstrate that
out-of-culture demonstrations from Indonesia and US contexts can match or
surpass in-culture alignment for MCQ reasoning, highlighting cultural
commonsense transferability beyond the Arab world. These findings demonstrate
that efficient cross-cultural alignment is possible and offer a promising
approach to adapt LLMs to low-resource cultural settings.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [175] [Machine Learnability as a Measure of Order in Aperiodic Sequences](https://arxiv.org/abs/2509.18103)
*Jennifer Dodgson,Michael Joedhitya,Adith Ramdas,Surender Suresh Kumar,Adarsh Singh Chauhan,Akira Rafhael,Wang Mingshu,Nordine Lotfi*

Main category: cs.LG

TL;DR: 本文利用图像机器学习模型分析乌拉姆螺旋中素数的分布规律，发现高阶数区域（约5亿）比低阶数区域（2500万以下）存在更容易学习的规律性，表明机器学习可作为数论的新实验工具。


<details>
  <summary>Details</summary>
Motivation: 素数定义上确定，行为上却呈现统计和随机性。研究旨在利用图像机器学习模型，测量乌拉姆螺旋特定区域素数场的比较规律性，并探索机器学习能否作为数论，特别是加密应用中素数模式研究的新工具。

Method: 使用一个专注于图像的机器学习模型，在乌拉姆螺旋上对素数分布进行分析。具体地，模型在从螺旋的特定区域（例如，接近5亿的区域和低于2500万的区域）提取的图像块上进行训练，并比较其准确度、精确度和召回率，以衡量规律性。

Result: 在纯准确度方面，模型在5亿左右区域训练的表现优于2500万以下区域，表明高阶数区域存在更容易学习的规律。精确度和召回率分析显示，模型在不同区域采用不同分类策略：低阶数时侧重识别素数模式，高阶数时侧重排除合数，这与数论关于高阶素数分布噪声递减的猜想一致。

Conclusion: 机器学习可以作为数论的一种新实验工具，为探索素数分布规律提供了新途径。该方法在高阶数素数中发现的更易学习的秩序与现有数论猜想相符，并显示出在密码学中研究强弱素数模式的潜力。

Abstract: Research on the distribution of prime numbers has revealed a dual character:
deterministic in definition yet exhibiting statistical behavior reminiscent of
random processes. In this paper we show that it is possible to use an
image-focused machine learning model to measure the comparative regularity of
prime number fields at specific regions of an Ulam spiral. Specifically, we
demonstrate that in pure accuracy terms, models trained on blocks extracted
from regions of the spiral in the vicinity of 500m outperform models trained on
blocks extracted from the region representing integers lower than 25m. This
implies existence of more easily learnable order in the former region than in
the latter. Moreover, a detailed breakdown of precision and recall scores seem
to imply that the model is favouring a different approach to classification in
different regions of the spiral, focusing more on identifying prime patterns at
lower numbers and more on eliminating composites at higher numbers. This aligns
with number theory conjectures suggesting that at higher orders of magnitude we
should see diminishing noise in prime number distributions, with averages
(density, AP equidistribution) coming to dominate, while local randomness
regularises after scaling by log x. Taken together, these findings point toward
an interesting possibility: that machine learning can serve as a new
experimental instrument for number theory. Notably, the method shows potential
1 for investigating the patterns in strong and weak primes for cryptographic
purposes.

</details>


### [176] [Data Valuation and Selection in a Federated Model Marketplace](https://arxiv.org/abs/2509.18104)
*Wenqian Li,Youjia Yang,Ruoxi Jia,Yan Pang*

Main category: cs.LG

TL;DR: 为解决联邦学习中异构数据估值与选择的挑战，本文提出一个基于Wasserstein距离的估算框架，能在保护隐私的前提下高效预测模型性能并筛选出高质量数据组合。


<details>
  <summary>Details</summary>
Motivation: 尽管联邦学习（FL）在保护数据隐私的同时实现了协作学习，但在FL设置中，如何有效评估和选择来自异构源的数据，仍是建立可信数据市场面临的关键挑战。

Method: 本文引入了一个以Wasserstein距离估算器为核心的联邦学习框架，该估算器能预测模型性能并揭示数据异构性与FL聚合算法的兼容性。为保护隐私，提出了一种无需访问原始数据的分布式Wasserstein距离近似方法。此外，利用神经网络缩放定律推断模型性能，实现不进行全面训练的高效数据选择。

Result: 通过在标签偏斜、错误标记和无标签源等多样化场景中的广泛实验，结果表明该方法能够持续识别出高性能的数据组合。

Conclusion: 本研究通过高效、隐私保护的数据估值与选择机制，为构建更可靠的联邦学习模型市场铺平了道路。

Abstract: In the era of Artificial Intelligence (AI), marketplaces have become
essential platforms for facilitating the exchange of data products to foster
data sharing. Model transactions provide economic solutions in data
marketplaces that enhance data reusability and ensure the traceability of data
ownership. To establish trustworthy data marketplaces, Federated Learning (FL)
has emerged as a promising paradigm to enable collaborative learning across
siloed datasets while safeguarding data privacy. However, effective data
valuation and selection from heterogeneous sources in the FL setup remain key
challenges. This paper introduces a comprehensive framework centered on a
Wasserstein-based estimator tailored for FL. The estimator not only predicts
model performance across unseen data combinations but also reveals the
compatibility between data heterogeneity and FL aggregation algorithms. To
ensure privacy, we propose a distributed method to approximate Wasserstein
distance without requiring access to raw data. Furthermore, we demonstrate that
model performance can be reliably extrapolated under the neural scaling law,
enabling effective data selection without full-scale training. Extensive
experiments across diverse scenarios, such as label skew, mislabeled, and
unlabeled sources, show that our approach consistently identifies
high-performing data combinations, paving the way for more reliable FL-based
model marketplaces.

</details>


### [177] [BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand](https://arxiv.org/abs/2509.18105)
*Nachiket N. Naik,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 研究发现，在连续时间库存动力学预测中，对于轻尾或时间相关的噪声，包含物理结构的通用微分方程（UDE）表现优于纯神经ODE；而对于重尾或极端事件，纯神经ODE的灵活性更佳。


<details>
  <summary>Details</summary>
Motivation: 经典供应链模型解释了牛鞭效应，但新兴的物理信息和神经微分方程混合方法尚未明确结构性偏差在不同需求情况下是帮助还是阻碍预测。本研究旨在填补这一空白，为混合建模提供指导。

Method: 通过比较完全学习的神经ODE (NODE) 和保留结构并学习残差策略项的物理信息通用微分方程 (UDE)，在单一梯队测试平台上，使用AR(1)、i.i.d.高斯和重尾对数正态三种需求模式进行测试。模型在不同比例的轨迹数据上进行训练，并评估库存、订单率和需求的多步预测。

Result: 在结构化需求（AR(1)、高斯）下，UDE的泛化能力显著优于NODE（例如，AR(1)下库存RMSE从4.92降至0.26）。但在重尾对数正态冲击下，NODE的灵活性表现更好。随着训练数据减少，这些趋势依然存在，NODE在推断时表现出相移漂移，而UDE虽然稳定但对稀有尖峰反应不足。

Conclusion: 研究建议在噪声为轻尾或时间相关时应强制执行结构；当极端事件主导时则应放宽结构。此指导原则也适用于其他科学和工程系统的混合建模，即在守恒律和适度噪声主导时应用已知结构，而在稀有事件驱动动力学时放松结构以捕捉极端情况。

Abstract: We study learning of continuous-time inventory dynamics under stochastic
demand and quantify when structure helps or hurts forecasting of the bullwhip
effect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the
entire right-hand side against a physics-informed Universal Differential
Equation (UDE) that preserves conservation and order-up-to structure while
learning a small residual policy term. Classical supply chain models explain
the bullwhip through control/forecasting choices and information sharing, while
recent physics-informed and neural differential equation methods blend domain
constraints with learned components. It is unclear whether structural bias
helps or hinders forecasting under different demand regimes. We address this by
using a single-echelon testbed with three demand regimes - AR(1)
(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done
on varying fractions of each trajectory, followed by evaluation of multi-step
forecasts for inventory I, order rate O, and demand D. Across the structured
regimes, UDE consistently generalizes better: with 90% of the training horizon,
inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96
to 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the
flexibility of NODE is better. These trends persist as train18 ing data
shrinks, with NODE exhibiting phase drift in extrapolation while UDE remains
stable but underreacts to rare spikes. Our results provide concrete guidance:
enforce structure when noise is light-tailed or temporally correlated; relax
structure when extreme events dominate. Beyond inventory control, the results
offer guidance for hybrid modeling in scientific and engineering systems:
enforce known structure when conservation laws and modest noise dominate, and
relax structure to capture extremes in settings where rare events drive
dynamics.

</details>


### [178] [Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks](https://arxiv.org/abs/2509.18106)
*Elisa Tomassini,Enrique García-Macías,Filippo Ubertini*

Main category: cs.LG

TL;DR: 本研究提出一种基于模型传输学习的神经网络代理模型，实现桥梁结构监测数据的跨结构知识迁移，以提高大型桥梁网络的损伤评估效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着监测系统数据量的增长，大型桥梁网络面临结构评估的可扩展性挑战。需要高效追踪和比较长期行为，并实现相似结构间的知识迁移，以应对多结构管理。

Method: 采用基于模型的传输学习方法，使用神经网络代理模型。在具有相似特征的桥梁之间进行模型训练和适应，捕捉共享损伤机制。将传输后的模型整合到贝叶斯推理框架中，利用监测数据的模态特征进行连续损伤评估。方法已通过两座桥梁的真实数据验证。

Result: 研究结果表明，该方法对损伤的位置、严重程度和范围具有高度敏感性。

Conclusion: 该方法增强了实时监测能力，实现了跨结构知识迁移，促进了智能监测策略和网络层面韧性的提升。

Abstract: The growing use of permanent monitoring systems has increased data
availability, offering new opportunities for structural assessment but also
posing scalability challenges, especially across large bridge networks.
Managing multiple structures requires tracking and comparing long-term
behaviour efficiently. To address this, knowledge transfer between similar
structures becomes essential. This study proposes a model-based transfer
learning approach using neural network surrogate models, enabling a model
trained on one bridge to be adapted to another with similar characteristics.
These models capture shared damage mechanisms, supporting a scalable and
generalizable monitoring framework. The method was validated using real data
from two bridges. The transferred model was integrated into a Bayesian
inference framework for continuous damage assessment based on modal features
from monitoring data. Results showed high sensitivity to damage location,
severity, and extent. This approach enhances real-time monitoring and enables
cross-structure knowledge transfer, promoting smart monitoring strategies and
improved resilience at the network level.

</details>


### [179] [AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting](https://arxiv.org/abs/2509.18107)
*Huanyao Zhang,Jiaye Lin,Wentao Zhang,Haitao Yuan,Guoliang Li*

Main category: cs.LG

TL;DR: 本文提出AdaMixT，一种自适应加权多尺度专家Transformer架构，通过多尺度特征提取和动态权重分配实现自适应融合，有效提升多元时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列预测方法主要依赖预定义的单一尺度补丁或缺乏有效的多尺度特征融合机制，这限制了它们捕捉时间序列复杂模式的能力，导致性能受限和泛化能力不足。

Method: 提出AdaMixT架构，引入多种补丁并结合通用预训练模型(GPM)和领域专用模型(DSM)进行多尺度特征提取。通过一个门控网络动态分配不同专家模型之间的权重，实现自适应的多尺度特征融合，以适应时间特征的异质性。

Result: 在包括Weather、Traffic、Electricity、ILI以及四种ETT数据集在内的八个广泛使用的基准测试上，AdaMixT持续且显著地展示了其在实际场景中的有效性。

Conclusion: AdaMixT通过其新颖的自适应加权多尺度专家融合机制，有效解决了现有方法在多尺度特征捕获和融合方面的局限性，显著提高了多元时间序列预测的性能和泛化能力。

Abstract: Multivariate time series forecasting involves predicting future values based
on historical observations. However, existing approaches primarily rely on
predefined single-scale patches or lack effective mechanisms for multi-scale
feature fusion. These limitations hinder them from fully capturing the complex
patterns inherent in time series, leading to constrained performance and
insufficient generalizability. To address these challenges, we propose a novel
architecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers
(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both
General Pre-trained Models (GPM) and Domain-specific Models (DSM) for
multi-scale feature extraction. To accommodate the heterogeneity of temporal
features, AdaMixT incorporates a gating network that dynamically allocates
weights among different experts, enabling more accurate predictions through
adaptive multi-scale fusion. Comprehensive experiments on eight widely used
benchmarks, including Weather, Traffic, Electricity, ILI, and four ETT
datasets, consistently demonstrate the effectiveness of AdaMixT in real-world
scenarios.

</details>


### [180] [Solve it with EASE](https://arxiv.org/abs/2509.18108)
*Adam Viktorin,Tomas Kadavy,Jozef Kovac,Michal Pluhacek,Roman Senkerik*

Main category: cs.LG

TL;DR: EASE是一个开源、模块化的框架，利用大型语言模型（LLMs）迭代生成算法解决方案，并提供可复现的反馈循环和全面控制。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs生成解决方案过程复杂，缺乏可复现性和精细控制；需要一个平台来简化提示设计和模型管理，使研究人员和实践者能够高效共同设计算法及其他生成式解决方案。

Method: EASE框架将生成、测试、分析和评估集成到可复现的反馈循环中，赋予用户对错误处理、分析和质量评估的完全控制。其架构支持编排多个LLM担任生成器、分析器、评估器等互补角色。

Result: EASE成功提供了一个透明且可扩展的平台，抽象了提示设计和模型管理，从而实现了跨领域算法及其他生成式解决方案的协同设计。

Conclusion: EASE通过提供一个高度模块化、可控且多LLM协调的平台，显著降低了算法和生成式解决方案的设计门槛，赋能研究者和实践者跨领域进行高效创新。

Abstract: This paper presents EASE (Effortless Algorithmic Solution Evolution), an
open-source and fully modular framework for iterative algorithmic solution
generation leveraging large language models (LLMs). EASE integrates generation,
testing, analysis, and evaluation into a reproducible feedback loop, giving
users full control over error handling, analysis, and quality assessment. Its
architecture supports the orchestration of multiple LLMs in complementary
roles-such as generator, analyst, and evaluator. By abstracting the complexity
of prompt design and model management, EASE provides a transparent and
extensible platform for researchers and practitioners to co-design algorithms
and other generative solutions across diverse domains.

</details>


### [181] [Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks](https://arxiv.org/abs/2509.18109)
*Jonatan Katz Nielsen*

Main category: cs.LG

TL;DR: 本研究提出了一种基于AIS数据的机器学习流程，用于海峡尺度船舶类型分类，通过轻量级轨迹特征，随机森林模型实现了92.15%的准确率。


<details>
  <summary>Details</summary>
Motivation: 准确识别船舶类型对于保障航运安全监督和打击非法、未报告和无管制（IUU）活动至关重要。

Method: 利用丹麦海事局2025年1月22日至30日覆盖博恩霍尔姆海峡的8天AIS历史数据。数据经过航行记录填充、异常值移除、按MMSI航迹分割（排除停泊时段）等预处理。提取了31个轨迹级特征，涵盖运动学、时间、地理空间和船体形状属性。采用按MMSI分组的训练/测试分割和分层5折交叉验证，使用SMOTE的随机森林等树形模型进行分类。

Result: 在货船、油轮、客船、高速船、渔船五类船舶中，结合SMOTE的随机森林模型在测试集上实现了92.15%的准确率（宏观F1值为93.27%），调优后的随机森林模型ROC-AUC最高达0.9897。特征重要性分析表明，桥位比和最大对地航速是最具区分度的信号。主要的分类错误发生在货船和油轮之间。该方法还展示了回填缺失船舶类型的实际应用价值。

Conclusion: 研究结果表明，基于AIS轨迹的轻量级特征能够实现海峡尺度的实时船舶类型分类，为未来的性能提升和应用提供了基础。

Abstract: Accurate recognition of vessel types from Automatic Identification System
(AIS) tracks is essential for safety oversight and combating illegal,
unreported, and unregulated (IUU) activity. This paper presents a strait-scale,
machine-learning pipeline that classifies moving vessels using only AIS data.
We analyze eight days of historical AIS from the Danish Maritime Authority
covering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After
forward/backward filling voyage records, removing kinematic and geospatial
outliers, and segmenting per-MMSI tracks while excluding stationary periods
($\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,
SOG statistics), temporal, geospatial (Haversine distances, spans), and
ship-shape attributes computed from AIS A/B/C/D reference points (length,
width, aspect ratio, bridge-position ratio). To avoid leakage, we perform
grouped train/test splits by MMSI and use stratified 5-fold cross-validation.
Across five classes (cargo, tanker, passenger, high-speed craft, fishing;
N=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest
with SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall
92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches
one-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the
bridge-position ratio and maximum SOG as the most discriminative signals;
principal errors occur between cargo and tanker, reflecting similar transit
behavior. We demonstrate operational value by backfilling missing ship types on
unseen data and discuss improvements such as DBSCAN based trip segmentation and
gradient-boosted ensembles to handle frequent-stop ferries and further lift
performance. The results show that lightweight features over AIS trajectories
enable real-time vessel type classification in straits.

</details>


### [182] [Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs](https://arxiv.org/abs/2509.18110)
*Mrigank Dhingra,Romit Maulik,Adil Rasheed,Omer San*

Main category: cs.LG

TL;DR: 为解决偏微分方程神经算子学习中全局PCA的计算开销问题，提出一种基于分块的PCA-Net框架，显著降低计算复杂度并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 在数据驱动的偏微分方程(PDEs)神经算子学习中，将主成分分析(PCA)应用于高维解场会产生显著的计算开销。

Method: 提出一个基于分块的PCA-Net框架，将解场分解为小块，在每个块内应用PCA，并在降维后的PCA空间中训练神经算子。研究了两种分块方法（局部到全局、局部到局部），并为最高效的方法探索了两种细化方案（引入带平滑滤波器的重叠分块，以及使用卷积神经网络(CNN)进行两步细化）。

Result: 基于分块的PCA显著降低了计算复杂度，同时保持了高精度，与全局PCA相比，端到端处理时间缩短了3.7到4倍。

Conclusion: 基于分块的PCA是实现基于PDE系统的神经算子高效学习的一种有前景的技术。

Abstract: Neural operator learning has emerged as a powerful approach for solving
partial differential equations (PDEs) in a data-driven manner. However,
applying principal component analysis (PCA) to high-dimensional solution fields
incurs significant computational overhead. To address this, we propose a
patch-based PCA-Net framework that decomposes the solution fields into smaller
patches, applies PCA within each patch, and trains a neural operator in the
reduced PCA space. We investigate two different patch-based approaches that
balance computational efficiency and reconstruction accuracy: (1)
local-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off
between computational cost and accuracy is analyzed, highlighting the
advantages and limitations of each approach. Furthermore, within each approach,
we explore two refinements for the most computationally efficient method: (i)
introducing overlapping patches with a smoothing filter and (ii) employing a
two-step process with a convolutional neural network (CNN) for refinement. Our
results demonstrate that patch-based PCA significantly reduces computational
complexity while maintaining high accuracy, reducing end-to-end pipeline
processing time by a factor of 3.7 to 4 times compared to global PCA, thefore
making it a promising technique for efficient operator learning in PDE-based
systems.

</details>


### [183] [Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection](https://arxiv.org/abs/2509.18111)
*Faizul Rakib Sayem,Shahana Ibrahim*

Main category: cs.LG

TL;DR: 提出了一种结合子空间表示学习和提示调优的OOD检测框架，通过利用VLM的特征嵌入来提高ID-OOD可分性。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示学习的OOD检测方法仅依赖softmax概率，忽视了大型视觉-语言模型（VLM）学习到的丰富特征嵌入的判别潜力，导致ID-OOD可分性不足。

Method: 提出了一种新颖的基于上下文优化（CoOp）的框架，将子空间表示学习与提示调优相结合。该方法通过将ID特征投影到提示向量张成的子空间中，同时将ID不相关特征投影到正交空空间，以提高ID-OOD可分性。并设计了一个易于处理的端到端学习准则进行训练。

Result: 在真实世界数据集上的实验表明，该方法能有效提高ID-OOD可分性和ID分类准确性。

Conclusion: 所提出的CoOp框架通过有效利用VLM的特征嵌入和子空间表示学习，显著提升了开放世界中AI系统的OOD检测性能。

Abstract: The reliability of artificial intelligence (AI) systems in open-world
settings depends heavily on their ability to flag out-of-distribution (OOD)
inputs unseen during training. Recent advances in large-scale vision-language
models (VLMs) have enabled promising few-shot OOD detection frameworks using
only a handful of in-distribution (ID) samples. However, existing prompt
learning-based OOD methods rely solely on softmax probabilities, overlooking
the rich discriminative potential of the feature embeddings learned by VLMs
trained on millions of samples. To address this limitation, we propose a novel
context optimization (CoOp)-based framework that integrates subspace
representation learning with prompt tuning. Our approach improves ID-OOD
separability by projecting the ID features into a subspace spanned by prompt
vectors, while projecting ID-irrelevant features into an orthogonal null space.
To train such OOD detection framework, we design an easy-to-handle end-to-end
learning criterion that ensures strong OOD detection performance as well as
high ID classification accuracy. Experiments on real-world datasets showcase
the effectiveness of our approach.

</details>


### [184] [Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis](https://arxiv.org/abs/2509.18112)
*Sheng Wong,Ravi Shankar,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: 本研究发现，在产前胎心监护（CTG）自动化分析中，经过微调的大型语言模型（LLMs）表现优于基础模型和领域特定方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型和LLMs在医疗领域展现出巨大潜力，但其在胎儿电子监护（EFM）/胎心监护（CTG）分析这一关键领域仍未被充分探索。产前CTG解读因其复杂的时间序列数据和主观性临床判断而面临独特挑战，常导致诊断准确性变异和延误。

Method: 本研究对自动化产前CTG分析的先进AI方法进行了首次全面比较。系统地评估了时间序列基础模型（FMs）和大型语言模型（LLMs）与现有的CTG特定架构。评估使用了超过500份反映真实临床情况的CTG记录。

Result: 结果表明，经过微调的LLMs在CTG解读方面取得了卓越的性能，优于基础模型和领域特定方法。

Conclusion: 这些发现为胎儿监护应用中不同AI方法的相对优势提供了关键见解，并为未来产前护理中的临床AI发展奠定了基础。

Abstract: Foundation models (FMs) and large language models (LLMs) demonstrate
remarkable capabilities across diverse domains through training on massive
datasets. These models have demonstrated exceptional performance in healthcare
applications, yet their potential for electronic fetal monitoring
(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating
fetal well-being, remains largely underexplored. Antepartum CTG interpretation
presents unique challenges due to the complex nature of fetal heart rate (FHR)
patterns and uterine activity, requiring sophisticated analysis of long
time-series data. The assessment of CTG is heavily based on subjective clinical
interpretation, often leading to variability in diagnostic accuracy and
deviation from timely pregnancy care. This study presents the first
comprehensive comparison of state-of-the-art AI approaches for automated
antepartum CTG analysis. We systematically compare time-series FMs and LLMs
against established CTG-specific architectures. Our evaluation encompasses over
500 CTG recordings of varying durations reflecting real-world clinical
recordings, providing robust performance benchmarks across different modelling
paradigms. Our results demonstrate that fine-tuned LLMs achieve superior
performance compared to both foundation models and domain-specific approaches,
offering a promising alternative pathway for clinical CTG interpretation. These
findings provide critical insights into the relative strengths of different AI
methodologies for fetal monitoring applications and establish a foundation for
future clinical AI development in prenatal care.

</details>


### [185] [A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU](https://arxiv.org/abs/2509.18114)
*Javed I. Khan an Henry Uwabor Moye*

Main category: cs.LG

TL;DR: 针对大型语言模型（LLMs）自回归推理在解码阶段的GPU负载不均导致的效率问题，本研究提出并分析了一个DPU辅助框架，利用BlueField-3数据处理单元实时检测和缓解多节点张量并行推理中的负载不均，以提升运行时效率。


<details>
  <summary>Details</summary>
Motivation: LLMs的自回归推理在解码阶段存在显著的运行时效率挑战，主要表现为GPU分片间的负载不均，这会导致吞吐量下降和延迟激增。本研究旨在识别LLM张量计算中（训练和推理期间）多GPU执行出现的负载不均和病态条件，并评估其对计算性能的影响，以及DPU网络是否能够有效跟踪并潜在地缓解这些问题。

Method: 使用由BlueField-3数据处理单元（DPU）辅助的框架，通过将监控任务卸载到DPU，并分析GPU遥测数据和节点间通信模式，实现对多节点张量并行推理中负载不均的实时检测和缓解。该系统能够向推理控制器和调度器提供可操作的反馈。

Result: 该抽象主要描述了研究目标和所提框架的潜在能力，而非已完成研究的具体结果。所提的DPU辅助框架能够通过卸载监控任务并分析GPU遥测和节点间通信模式，实时检测并缓解负载不均，并向推理控制器和调度器提供可操作的反馈。

Conclusion: 本研究旨在通过DPU网络对多GPU LLM执行中出现的负载不均进行跟踪和潜在缓解，从而解决LLM自回归推理的效率挑战。所提出的DPU辅助框架有望提升LLM的运行时效率和性能，为未来的LLM部署提供有效的优化方案。

Abstract: Autoregressive inference in large transformer-based language models (LLMs)
presents significant challenges for runtime efficiency, particularly during the
decode phase where load imbalance across GPU shards can cause throughput
degradation and latency spikes. A DPU-assisted framework leveraged by
BlueField-3 Data Processing Units can enable real-time detection and mitigation
of load imbalance in multi-node tensor-parallel inference. By offloading
monitoring tasks to the DPU and analyzing GPU telemetry and inter-node
communication patterns, the resulting system can provide actionable feedback to
inference controllers and schedulers. The goal of this study is three-fold i)
identify the reported skews/imbalances/pathological conditions that arise in
muti-GPU execution of a) LLM tensor computing (both during training and
inference), b) identify their impact on computational performance, and c) make
a critical assessment if those can be tracked for potential mitigation from a
DPU network.

</details>


### [186] [Towards Scalable and Structured Spatiotemporal Forecasting](https://arxiv.org/abs/2509.18115)
*Hongyi Chen,Xiucheng Li,Xinyang Chen,Jing Li,Kehai Chen,Liqiang Nie*

Main category: cs.LG

TL;DR: 本文提出一种新颖的空间平衡注意力模块用于时空预测。它通过结合子图内和子图间注意力，平衡局部与全局空间关联，构建了一个可扩展、高效的多尺度模型，在真实世界数据集上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的时空预测方法难以有效平衡空间邻近性（局部关联）与捕获全局空间关联的需求。

Method: 提出一种空间平衡注意力模块。该模块将空间图划分为子图，通过子图内注意力学习局部空间关联；通过聚合节点生成子图表示，并利用子图间注意力实现子图间的消息传递，从而捕获全局空间关联。在此基础上，通过逐步增加子图规模，开发了一个多尺度时空预测模型。

Result: 在真实世界时空数据集上，该模型相对于基线方法实现了高达7.7%的性能提升，且运行成本较低。模型具有可扩展性，能生成结构化的空间关联，并且易于实现。

Conclusion: 所提出的空间平衡注意力模块及其构建的多尺度时空预测模型，能有效平衡局部和全局空间关联，在各种规模的时空数据集上均展现出优越的性能和效率。

Abstract: In this paper, we propose a novel Spatial Balance Attention block for
spatiotemporal forecasting. To strike a balance between obeying spatial
proximity and capturing global correlation, we partition the spatial graph into
a set of subgraphs and instantiate Intra-subgraph Attention to learn local
spatial correlation within each subgraph; to capture the global spatial
correlation, we further aggregate the nodes to produce subgraph representations
and achieve message passing among the subgraphs via Inter-subgraph Attention.
Building on the proposed Spatial Balance Attention block, we develop a
multiscale spatiotemporal forecasting model by progressively increasing the
subgraph scales. The resulting model is both scalable and able to produce
structured spatial correlation, and meanwhile, it is easy to implement. We
evaluate its efficacy and efficiency against the existing models on real-world
spatiotemporal datasets from medium to large sizes. The experimental results
show that it can achieve performance improvements up to 7.7% over the baseline
methods at low running costs.

</details>


### [187] [Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization](https://arxiv.org/abs/2509.18116)
*Nathan Egbuna,Saatvik Gaur,Sunishchal Dev,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: 该研究提出了一种名为“摊销潜在空间引导”（ALS）的方法，通过离线计算一个方向向量来取代昂贵的测试时优化循环，从而在保持性能的同时，显著提高推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时优化方法（如迭代细化、多步验证、潜在空间优化）因需要大量计算（10-100倍于标准解码）而难以大规模应用，即使是潜在空间方法也需要多步反向传播，成本高昂。

Method: Amortized Latent Steering (ALS) 方法的核心是离线计算成功与不成功生成之间隐藏状态的平均差异向量。在推理时，ALS以恒定成本应用这个预计算的向量，当解码偏离成功流形时，它会将激活值推回该流形，从而将迭代优化折叠为单次操作。

Result: 在GSM8K和MATH-500基准测试中，ALS比迭代方法快2-5倍，同时达到或超越了贪婪思维链（CoT）和自洽性基线，在效率-准确性权衡方面最高实现了101%的改进。

Conclusion: 研究结果表明，潜在优化的许多益处可以通过离线计算捕获，这使得复杂的推理技术在生产部署中变得可行和实用。

Abstract: Test-time optimization remains impractical at scale due to prohibitive
inference costs\textemdash techniques like iterative refinement and multi-step
verification can require $10$--$100\times$ more compute per query than standard
decoding. Latent space test-time optimization methods like LatentSeek offer a
more direct approach by steering hidden representations, but still demand
expensive per-query optimization loops with multiple backward passes. We
propose Amortized Latent Steering (ALS), which collapses this iterative
optimization into a single offline-computed vector applied at constant cost
during inference. ALS computes the mean difference between hidden states from
successful versus unsuccessful generations, then uses this direction to
calibrate the model's hidden representations: when decoding drifts away from
the success manifold, ALS nudges activations back toward it. Across GSM8K and
MATH-$500$ benchmarks, ALS achieves $2$--$5\times$ speedup over iterative
methods while matching or surpassing greedy Chain-of-Thought (CoT) and
Self-Consistency baselines, yielding up to 101\% improvement in
efficiency--accuracy trade-off. These results show that much of latent
optimization's benefit can be captured offline, making sophisticated reasoning
techniques viable for production deployment. Code is available
at~\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}

</details>


### [188] [Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs](https://arxiv.org/abs/2509.18117)
*Eric Petit,Denis Chêne*

Main category: cs.LG

TL;DR: 本文提出一种基于在线增量贝叶斯学习的机器学习方法，用于设计能根据用户浏览习惯动态调整的数字界面，旨在提升用户导航和操作体验。


<details>
  <summary>Details</summary>
Motivation: 设计能够动态适应不同用户和使用策略的数字界面，特别是关注用户个体习惯而非群体偏好，以期通过帮助用户更好地导航和操作界面来改善用户体验。

Method: 采用机器学习方法，利用贝叶斯统计建模用户浏览行为和习惯。其特点是：在线增量学习（即使数据量少也能进行可靠预测，并适应环境变化），能生成任务模型（提供当前用户导航的图形化表示和使用统计），并能在学习新任务的同时保留先验知识。

Result: 理论框架得到了描述，并通过仿真实验证明了该方法在平稳和非平稳环境中的有效性。

Conclusion: 这项研究为自适应系统铺平了道路，这些系统通过帮助用户更好地导航和操作界面来改善用户体验。

Abstract: The paper presents a machine learning approach to design digital interfaces
that can dynamically adapt to different users and usage strategies. The
algorithm uses Bayesian statistics to model users' browsing behavior, focusing
on their habits rather than group preferences. It is distinguished by its
online incremental learning, allowing reliable predictions even with little
data and in the case of a changing environment. This inference method generates
a task model, providing a graphical representation of navigation with the usage
statistics of the current user. The algorithm learns new tasks while preserving
prior knowledge. The theoretical framework is described, and simulations show
the effectiveness of the approach in stationary and non-stationary
environments. In conclusion, this research paves the way for adaptive systems
that improve the user experience by helping them to better navigate and act on
their interface.

</details>


### [189] [Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices](https://arxiv.org/abs/2509.18118)
*Marcelo Ribeiro,Diogo Costa,Gonçalo Moreira,Sandro Pinto,Tiago Gomes*

Main category: cs.LG

TL;DR: 本文将轻量级随机梯度下降 (L-SGD) 扩展到RISC-V MCU，并提出其8位量化版本，在显著降低内存消耗和加速训练的同时，保持了可接受的精度。


<details>
  <summary>Details</summary>
Motivation: 现代物联网设备对本地机器学习的需求日益增长，但多数设备缺乏GPU或专用加速器，使得设备上训练困难，通常依赖云服务，从而引发隐私和连接性问题。联邦学习虽能解决这些问题，但需要高效的优化算法。RISC-V作为新兴开放架构，目前尚缺乏对设备上训练的强大支持。

Method: 1. 将适用于Arm Cortex-M MCU的L-SGD算法扩展到RISC-V MCU。2. 使用32位浮点运算在Arm和RISC-V平台上对L-SGD进行评估，以量化RISC-V MCU中FPU缺失带来的性能影响。3. 为减轻RISC-V的硬件局限性，提出L-SGD的8位量化版本。

Result: 1. 32位浮点L-SGD在RISC-V平台上的性能受到FPU缺失的显著影响。2. 提出的8位量化L-SGD版本在RISC-V上实现了内存使用量近4倍的减少。3. 8位量化L-SGD在训练时间上达到了2.2倍的加速。4. 8位量化L-SGD的准确性退化可忽略不计。

Conclusion: 通过引入L-SGD的8位量化版本，可以有效解决RISC-V MCU在设备上训练面临的内存和计算效率挑战，为RISC-V架构上的去中心化、协作式机器学习训练提供了高效且实用的解决方案。

Abstract: Modern IoT devices increasingly rely on machine learning solutions to process
data locally. However, the lack of graphics processing units (GPUs) or
dedicated accelerators on most platforms makes on-device training largely
infeasible, often requiring cloud-based services to perform this task. This
procedure often raises privacy-related concerns, and creates dependency on
reliable and always-on connectivity. Federated Learning (FL) is a new trend
that addresses these issues by enabling decentralized and collaborative
training directly on devices, but it requires highly efficient optimization
algorithms. L-SGD, a lightweight variant of stochastic gradient descent, has
enabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).
This work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture
that still lacks robust support for on-device training. L-SGD was evaluated on
both Arm and RISC-V platforms using 32-bit floating-point arithmetic,
highlighting the performance impact of the absence of Floating-Point Units
(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit
quantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in
memory usage and a 2.2x speedup in training time, with negligible accuracy
degradation.

</details>


### [190] [MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents](https://arxiv.org/abs/2509.18119)
*Yifan Xu,Xiao Liu,Xinghan Liu,Jiaqi Fu,Hanchen Zhang,Bohao Jing,Shudan Zhang,Yuting Wang,Wenyi Zhao,Yuxiao Dong*

Main category: cs.LG

TL;DR: MOBILERL是一个在线智能体强化学习框架，通过难度自适应算法和奖励调整，有效提升了移动GUI智能体在复杂环境中的性能，并取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 开发基于强化学习的移动GUI智能体面临两大挑战：任务难度分布呈重尾效应导致训练不稳定，以及大规模环境采样效率低下。

Method: 提出MOBILERL在线智能体强化学习框架，其核心是难度自适应GRPO (ADAGRPO) 算法。ADAGRPO通过设计难度自适应正向回放和失败课程过滤来适应不同任务难度，并引入最短路径奖励调整策略来处理多轮任务长度，共同提升RL训练的稳定性与采样效率。

Result: 将MOBILERL应用于Qwen2.5-VL-7B和GLM-4.1V-9B模型，其中MOBILERL-9B模型在AndroidWorld (75.8%) 和AndroidLab (46.8%) 基准测试上均达到最先进的成功率。

Conclusion: MOBILERL框架有效克服了移动GUI智能体RL训练的挑战，显著提升了其在复杂移动应用中的性能，并已被产品采用且开源，具有重要的理论与实践价值。

Abstract: Building general-purpose graphical user interface (GUI) agents has become
increasingly promising with the progress in vision language models. However,
developing effective mobile GUI agents with reinforcement learning (RL) remains
challenging due to the heavy-tailed distribution of task difficulty and the
inefficiency of large-scale environment sampling. We present an online agentic
reinforcement learning framework MOBILERL to enhance GUI agents in mobile
environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)
algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and
failure curriculum filtering to adapt the model to different task difficulties.
We introduce the shortest path reward adjustment strategy to reshape rewards
concerning the task length in multi-turn agentic tasks. Those strategies
jointly stabilize RL training, improve sample efficiency, and generate strong
performance across diverse mobile apps and tasks. We apply MOBILERL to two open
models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B
model achieves state-of-the-art results in terms of success rates on both
AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted
in the AutoGLM products, and also open-sourced at
https://github.com/THUDM/MobileRL.

</details>


### [191] [A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning](https://arxiv.org/abs/2509.18120)
*Thanh Linh Nguyen,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: 本文提出CoCoGen框架，结合生成式AI和潜在博弈论，解决跨筒仓联邦学习中统计异质性和经济竞争问题，通过数据生成策略优化社会福利。


<details>
  <summary>Details</summary>
Motivation: 现有跨筒仓联邦学习主要关注统计异质性，但未充分解决机构间的经济竞争（市场竞争者）导致的参与意愿低和潜在收益损失问题。此外，统计异质性与机构间竞争的综合影响及其对机构行为和社会福利的影响尚待探索。

Method: 提出CoCoGen框架，一个合作竞争兼容的数据生成框架。它利用生成式AI（GenAI）和潜在博弈论来建模、分析和优化异质和竞争环境下的协作学习。CoCoGen通过学习性能和基于效用的公式表征竞争和统计异质性，并将每个训练轮次建模为加权潜在博弈。然后推导出基于GenAI的数据生成策略，以最大化社会福利。

Result: 在Fashion-MNIST数据集上的实验结果表明，不同的异质性和竞争水平会影响机构行为。CoCoGen始终优于基线方法。

Conclusion: CoCoGen框架能有效应对联邦学习中统计异质性和经济竞争带来的挑战，通过优化数据生成策略显著提升系统整体社会福利，并展现出优越的性能。

Abstract: Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or
banks) to collaboratively train artificial intelligence (AI) models while
preserving data privacy by keeping data local. While prior work has primarily
addressed statistical heterogeneity across organizations, a critical challenge
arises from economic competition, where organizations may act as market rivals,
making them hesitant to participate in joint training due to potential utility
loss (i.e., reduced net benefit). Furthermore, the combined effects of
statistical heterogeneity and inter-organizational competition on
organizational behavior and system-wide social welfare remain underexplored. In
this paper, we propose CoCoGen, a coopetitive-compatible data generation
framework, leveraging generative AI (GenAI) and potential game theory to model,
analyze, and optimize collaborative learning under heterogeneous and
competitive settings. Specifically, CoCoGen characterizes competition and
statistical heterogeneity through learning performance and utility-based
formulations and models each training round as a weighted potential game. We
then derive GenAI-based data generation strategies that maximize social
welfare. Experimental results on the Fashion-MNIST dataset reveal how varying
heterogeneity and competition levels affect organizational behavior and
demonstrate that CoCoGen consistently outperforms baseline methods.

</details>


### [192] [Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters](https://arxiv.org/abs/2509.18124)
*Edmund Agyemang,Lawrence Agbota,Vincent Agbenyeavu,Peggy Akabuah,Bismark Bimpong,Christopher Attafuah*

Main category: cs.LG

TL;DR: 本研究利用监督机器学习算法，结合用户评论的文本和数值属性，预测咖啡评分。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过数据驱动的方法，基于用户评论预测咖啡评分，以补充传统的专业咖啡杯测评估。

Method: 研究采用了数据预处理（文本清洗、TF-IDF特征提取、SelectKBest特征选择），训练并优化了六种监督学习模型（决策树、K近邻、多层感知机、随机森林、极端随机树和XGBoost），并使用F1分数、Gmean和AUC进行性能评估。

Result: 集成方法（极端随机树、随机森林、XGBoost）和多层感知机在F1分数、Gmean和AUC等评估指标上，持续优于简单的分类器（决策树和K近邻）。

Conclusion: 研究强调了严格的特征选择和超参数调优对于构建稳健的感官产品预测系统的重要性，为传统咖啡杯测提供了一种数据驱动的补充方法。

Abstract: This study explores the application of supervised machine learning algorithms
to predict coffee ratings based on a combination of influential textual and
numerical attributes extracted from user reviews. Through careful data
preprocessing including text cleaning, feature extraction using TF-IDF, and
selection with SelectKBest, the study identifies key factors contributing to
coffee quality assessments. Six models (Decision Tree, KNearest Neighbors,
Multi-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained
and evaluated using optimized hyperparameters. Model performance was assessed
primarily using F1-score, Gmean, and AUC metrics. Results demonstrate that
ensemble methods (Extra Trees, Random Forest, and XGBoost), as well as
Multi-layer Perceptron, consistently outperform simpler classifiers (Decision
Trees and K-Nearest Neighbors) in terms of evaluation metrics such as F1
scores, G-mean and AUC. The findings highlight the essence of rigorous feature
selection and hyperparameter tuning in building robust predictive systems for
sensory product evaluation, offering a data driven approach to complement
traditional coffee cupping by expertise of trained professionals.

</details>


### [193] [NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment](https://arxiv.org/abs/2509.18125)
*Harsha Koduri*

Main category: cs.LG

TL;DR: 提出NurseSchedRL，一个基于强化学习的护士-病人分配框架，用于高效地管理医疗资源，解决多约束环境下的排班难题。


<details>
  <summary>Details</summary>
Motivation: 医疗系统面临有限护理资源、技能差异、病人紧急程度、员工疲劳和护理连续性等复杂因素，传统方法难以有效处理这些动态多约束环境下的护士排班问题。

Method: 开发了NurseSchedRL强化学习框架，该框架集成了结构化状态编码、受约束动作掩码和基于注意力机制的技能、疲劳及地理语境表示。使用近端策略优化（PPO）算法并结合可行性掩码，以确保分配方案符合实际约束，并能动态适应病人到达和护士可用性变化。

Result: 在模拟真实护士和病人数据的环境中，NurseSchedRL相较于基线启发式方法和无约束RL方法，实现了更高的排班效率、更好的技能与病人需求匹配度以及更低的护士疲劳度。

Conclusion: 强化学习在复杂、高风险的医疗劳动力管理决策支持方面具有巨大潜力。

Abstract: Healthcare systems face increasing pressure to allocate limited nursing
resources efficiently while accounting for skill heterogeneity, patient acuity,
staff fatigue, and continuity of care. Traditional optimization and heuristic
scheduling methods struggle to capture these dynamic, multi-constraint
environments. I propose NurseSchedRL, a reinforcement learning framework for
nurse-patient assignment that integrates structured state encoding, constrained
action masking, and attention-based representations of skills, fatigue, and
geographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with
feasibility masks to ensure assignments respect real-world constraints, while
dynamically adapting to patient arrivals and varying nurse availability. In
simulation with realistic nurse and patient data, NurseSchedRL achieves
improved scheduling efficiency, better alignment of skills to patient needs,
and reduced fatigue compared to baseline heuristic and unconstrained RL
approaches. These results highlight the potential of reinforcement learning for
decision support in complex, high-stakes healthcare workforce management.

</details>


### [194] [Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning](https://arxiv.org/abs/2509.18126)
*Bishal K C,Amr Hilal,Pawan Thapa*

Main category: cs.LG

TL;DR: 本文研究联邦学习（FL）在异构电动汽车充电站（EVCS）异常检测中的性能。发现在异构设置下，FedAvgM表现优于FedAvg，能为EVCS提供稳健且保护隐私的安全保障。


<details>
  <summary>Details</summary>
Motivation: 确保物联网电动汽车充电站（EVCS）免受网络威胁至关重要。传统集中式入侵检测系统（IDS）存在隐私担忧，而联邦学习（FL）是一个有前景的替代方案。然而，现有FL-based IDS评估忽略了系统异构性和非独立同分布（non-IID）数据等实际挑战。

Method: 通过实验，评估了联邦学习在系统和数据异构性下对电动汽车充电站异常检测的性能。研究使用了FedAvg和FedAvgM两种优化方法来分析其在异常检测中的有效性。

Result: 在独立同分布（IID）设置下，FedAvg性能优于使用相同神经网络的集中式模型。但在非独立同分布数据和系统异构性下，性能有所下降。FedAvgM在异构设置中始终优于FedAvg，表现出更好的收敛性和更高的异常检测准确性。

Conclusion: 联邦学习能够在物联网电动汽车充电站中处理异构性，且性能损失不显著。FedAvgM是一种有前景的解决方案，可为电动汽车充电站提供鲁棒、保护隐私的安全性。

Abstract: Federated Learning (FL) is a decentralized training framework widely used in
IoT ecosystems that preserves privacy by keeping raw data local, making it
ideal for IoT-enabled cyber-physical systems with sensing and communication
like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric
Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle
infrastructure, securing these IoT-based charging stations against cyber
threats has become critical. Centralized Intrusion Detection Systems (IDS)
raise privacy concerns due to sensitive network and user data, making FL a
promising alternative. However, current FL-based IDS evaluations overlook
practical challenges such as system heterogeneity and non-IID data. To address
these challenges, we conducted experiments to evaluate the performance of
federated learning for anomaly detection in EV charging stations under system
and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization
approaches, to analyze their effectiveness in anomaly detection. Under IID
settings, FedAvg achieves superior performance to centralized models using the
same neural network. However, performance degrades with non-IID data and system
heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous
settings, showing better convergence and higher anomaly detection accuracy. Our
results demonstrate that FL can handle heterogeneity in IoT-based EVCS without
significant performance loss, with FedAvgM as a promising solution for robust,
privacy-preserving EVCS security.

</details>


### [195] [Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework](https://arxiv.org/abs/2509.18127)
*Jiaqi Weng,Han Zheng,Hanyu Zhang,Qinqin He,Jialing Tao,Hui Xue,Zhixuan Chu,Xiting Wang*

Main category: cs.LG

TL;DR: 针对LLM安全风险，本文提出Safe-SAIL框架，通过解释SAE特征，系统识别并高效解释LLM中的安全相关神经元，以增强对安全机制的理解。


<details>
  <summary>Details</summary>
Motivation: LLM大规模部署带来严重安全问题，现有安全研究和SAE应用未能细粒度解释安全相关行为（如生成毒性响应），无法应对广泛、未定义的风险。为进行严格安全分析，需要提取丰富多样的安全相关特征，但面临两大挑战：识别最能生成安全概念特定神经元的SAE，以及高昂的特征解释成本。

Method: 本文提出Safe-SAIL框架，用于解释LLM中的SAE特征，以推进安全领域的机械理解。该方法系统地识别具有最佳概念特定可解释性的SAE，解释安全相关神经元，并引入高效策略以扩大解释过程的规模。

Result: Safe-SAIL框架能够系统识别出最能生成安全概念特定神经元的SAE，并有效地解释这些安全相关的神经元，同时引入高效策略解决了大规模解释的成本问题。研究成果将发布包含SAE检查点和人类可读神经元解释的综合工具包。

Conclusion: Safe-SAIL框架通过提供一种系统性方法来解释LLM中的SAE安全特征，增强了LLM安全领域的机械理解。通过发布工具包，Safe-SAIL将支持对安全风险的实证分析，并促进LLM安全研究。

Abstract: Increasing deployment of large language models (LLMs) in real-world
applications raises significant safety concerns. Most existing safety research
focuses on evaluating LLM outputs or specific safety tasks, limiting their
ability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)
facilitate interpretability research to clarify model behavior by explaining
single-meaning atomic features decomposed from entangled signals. jHowever,
prior applications on SAEs do not interpret features with fine-grained
safety-related con- cepts, thus inadequately addressing safety-critical
behaviors, such as generating toxic responses and violating safety regu-
lations. For rigorous safety analysis, we must extract a rich and diverse set
of safety-relevant features that effectively capture these high-risk behaviors,
yet face two challenges: identifying SAEs with the greatest potential for
generating safety concept-specific neurons, and the prohibitively high cost of
detailed feature explanation. In this paper, we pro- pose Safe-SAIL, a
framework for interpreting SAE features within LLMs to advance mechanistic
understanding in safety domains. Our approach systematically identifies SAE
with best concept-specific interpretability, explains safety-related neurons,
and introduces efficient strategies to scale up the in- terpretation process.
We will release a comprehensive toolkit including SAE checkpoints and
human-readable neuron ex- planations, which supports empirical analysis of
safety risks to promote research on LLM safety.

</details>


### [196] [Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis](https://arxiv.org/abs/2509.18128)
*Amirreza Tootchi,Xiaoping Du*

Main category: cs.LG

TL;DR: 提出一种高斯-埃尔米特求积方法，用于解耦机器学习代理模型中嵌套的随机不确定性和认知不确定性，从而提高可靠性分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 机器学习代理模型在物理可靠性分析中引入的认知不确定性与输入中的随机不确定性耦合，可能损害可靠性预测的准确性。

Method: 采用高斯-埃尔米特求积方法。首先使用一阶/二阶可靠性方法评估随机不确定性下的条件失效概率，然后将这些概率对认知不确定性进行积分。

Result: 该方法在保持计算效率的同时，比忽略模型不确定性的传统方法提供了更可信的预测。

Conclusion: 所提出的方法能有效解耦嵌套不确定性，显著提高基于机器学习代理模型的可靠性分析的准确性和可信度。

Abstract: Machine learning surrogates are increasingly employed to replace expensive
computational models for physics-based reliability analysis. However, their use
introduces epistemic uncertainty from model approximation errors, which couples
with aleatory uncertainty in model inputs, potentially compromising the
accuracy of reliability predictions. This study proposes a Gauss-Hermite
quadrature approach to decouple these nested uncertainties and enable more
accurate reliability analysis. The method evaluates conditional failure
probabilities under aleatory uncertainty using First and Second Order
Reliability Methods and then integrates these probabilities across realizations
of epistemic uncertainty. Three examples demonstrate that the proposed approach
maintains computational efficiency while yielding more trustworthy predictions
than traditional methods that ignore model uncertainty.

</details>


### [197] [Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model](https://arxiv.org/abs/2509.18130)
*Zijie Zhou,Huichen Ma*

Main category: cs.LG

TL;DR: 本文提出一种结合STL分解和GRU的地铁换乘客流预测模型（STL-GRU），显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 在地铁智能交通系统中，准确的换乘客流预测是优化运营计划、提高运输效率的关键。本研究旨在进一步完善地铁内部换乘客流预测理论，为智能运营决策提供更可靠支持。

Method: 该模型首先利用Keras构建和训练GRU模型；接着对地铁刷卡原始数据进行预处理，使用基于图的深度优先搜索算法识别乘客路径并构建换乘客流时间序列；然后采用STL时间序列分解算法将时间序列分解为趋势、周期和残差分量，并使用3σ原则消除和填充残差分量中的异常值，最终完成换乘客流预测。模型以某地铁站的换乘客流数据为样本进行验证。

Result: 研究结果显示，与LSTM、GRU及STL-LSTM模型相比，STL-GRU组合预测模型显著提高了工作日（非周五）、周五和休息日的换乘客流预测精度。预测结果的平均绝对百分比误差（MAPE）分别至少降低了2.3、1.36和6.42个百分点。

Conclusion: STL-GRU组合预测模型在地铁换乘客流预测方面具有显著的有效性和更高的精度，能为地铁智能运营决策提供更可靠的支持。

Abstract: In the metro intelligent transportation system, accurate transfer passenger
flow prediction is a key link in optimizing operation plans and improving
transportation efficiency. To further improve the theory of metro internal
transfer passenger flow prediction and provide more reliable support for
intelligent operation decisions, this paper innovatively proposes a metro
transfer passenger flow prediction model that integrates the Seasonal and Trend
decomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In
practical application, the model first relies on the deep learning library
Keras to complete the construction and training of the GRU model, laying the
foundation for subsequent prediction; then preprocesses the original metro card
swiping data, uses the graph-based depth-first search algorithm to identify
passengers' travel paths, and further constructs the transfer passenger flow
time series; subsequently adopts the STL time series decomposition algorithm to
decompose the constructed transfer passenger flow time series into trend
component, periodic component and residual component, and uses the 3{\sigma}
principle to eliminate and fill the outliers in the residual component, and
finally completes the transfer passenger flow prediction.Taking the transfer
passenger flow data of a certain metro station as the research sample, the
validity of the model is verified. The results show that compared with Long
Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of
STL time series decomposition method and Long Short-Term Memory (STL-LSTM), the
STL-GRU combined prediction model significantly improves the prediction
accuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays
and rest days, with the mean absolute percentage error (MAPE) of the prediction
results reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.

</details>


### [198] [Two ways to knowledge?](https://arxiv.org/abs/2509.18131)
*Jean-Michel Tucny,Abhisek Ganguly,Santosh Ansumali,Sauro Succi*

Main category: cs.LG

TL;DR: Transformer模型在物理应用中，其权重矩阵呈现随机性，与物理结构无直接关联，这表明机器学习与科学方法是两种不同的知识获取路径，且可解释性仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer在解决物理问题时，其内部权重矩阵的特性以及与物理结构的关联，从而理解机器学习作为一种知识获取方式与传统科学方法的异同。

Method: 分析Transformer模型应用于两个代表性物理问题时的权重矩阵特征；将Transformer操作与（广义）路径积分技术进行类比。

Result: Transformer的权重矩阵表现出随机性，与所研究物理问题的物理和数学结构无直接可识别的联系；将Transformer操作与路径积分进行类比可能解释了权重的随机性，但未能解决可解释性问题。

Conclusion: 机器学习与科学方法可能是获取知识的两种不同但互补的途径，然而，网络参数与物理结构直接对应的严格可解释性难以实现；在缺乏深刻洞察力的情况下获取知识存在固有风险。

Abstract: It is shown that the weight matrices of transformer-based machine learning
applications to the solution of two representative physical applications show a
random-like character which bears no directly recognizable link to the physical
and mathematical structure of the physical problem under study. This suggests
that machine learning and the scientific method may represent two distinct and
potentially complementary paths to knowledge, even though a strict notion of
explainability in terms of direct correspondence between network parameters and
physical structures may remain out of reach. It is also observed that drawing a
parallel between transformer operation and (generalized) path-integration
techniques may account for the random-like nature of the weights, but still
does not resolve the tension with explainability. We conclude with some general
comments on the hazards of gleaning knowledge without the benefit of Insight.

</details>


### [199] [Self-Evolving LLMs via Continual Instruction Tuning](https://arxiv.org/abs/2509.18133)
*Le Huang,Jiazheng Kang,Cheng Hou,Zhe Zhao,Zhenxiang Yan,Chuan Shi,Ting Bai*

Main category: cs.LG

TL;DR: MoE-CL是一种参数高效的对抗性MoE框架，用于LLM的持续指令调优，通过双专家设计和对抗学习，有效缓解灾难性遗忘，实现知识保留和跨任务泛化的平衡，并在工业实践中显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 在工业环境中，大型语言模型（LLMs）需要持续学习以适应多样化和不断演进的任务，但现有持续学习方法（如回放和参数隔离）常面临灾难性遗忘问题，即新任务训练会因过拟合和泛化能力减弱而损害旧任务性能。

Method: 提出MoE-CL，一个参数高效的对抗性专家混合框架，用于工业级LLM的自进化持续指令调优。它采用双专家设计：为每个任务分配一个专用LoRA专家以保留任务知识并缓解遗忘；同时使用一个共享LoRA专家以实现跨任务迁移。为防止共享专家传递无关噪声，框架内集成了一个任务感知判别器（GAN的一部分），通过对抗学习确保共享专家仅传递任务相关信息，从而平衡知识保留与跨任务泛化。

Result: 在公共MTL5和工业级Tencent3基准测试上验证了MoE-CL的有效性。在腾讯视频平台内容合规审查的真实A/B测试中，MoE-CL成功将人工审查成本降低了15.3%。

Conclusion: MoE-CL在需要持续适应和稳定迁移的大规模工业部署中具有很高的实用性。

Abstract: In real-world industrial settings, large language models (LLMs) must learn
continually to keep pace with diverse and evolving tasks, requiring
self-evolution to refine knowledge under dynamic data distributions. However,
existing continual learning (CL) approaches, such as replay and parameter
isolation, often suffer from catastrophic forgetting: training on new tasks
degrades performance on earlier ones by overfitting to the new distribution and
weakening generalization.We propose MoE-CL, a parameter-efficient adversarial
mixture-of-experts framework for industrial-scale, self-evolving continual
instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated
LoRA expert per task to preserve task-specific knowledge via parameter
independence, mitigating forgetting; and (2) a shared LoRA expert to enable
cross-task transfer. To prevent transferring task-irrelevant noise through the
shared pathway, we integrate a task-aware discriminator within a GAN. The
discriminator encourages the shared expert to pass only task-aligned
information during sequential training. Through adversarial learning, the
shared expert acquires generalized representations that mimic the
discriminator, while dedicated experts retain task-specific details, balancing
knowledge retention and cross-task generalization and thereby supporting
self-evolution.Extensive experiments on the public MTL5 benchmark and an
industrial Tencent3 benchmark validate the effectiveness of MoE-CL for
continual instruction tuning. In real-world A/B testing for content compliance
review on the Tencent Video platform, MoE-CL reduced manual review costs by
15.3%. These results demonstrate that MoE-CL is practical for large-scale
industrial deployment where continual adaptation and stable transfer are
critical.

</details>


### [200] [A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization](https://arxiv.org/abs/2509.18134)
*Furan Xie,Bing Liu,Li Chai*

Main category: cs.LG

TL;DR: 本文解决了分布式优化中的隐私保护问题，提出一种加权梯度跟踪算法以消除梯度跟踪的隐私泄露风险，并证明其精确收敛性和有效性。


<details>
  <summary>Details</summary>
Motivation: 在分布式优化过程中，保护代理的私有信息免受潜在攻击者窃取至关重要，特别是先进的梯度跟踪技术存在固有的隐私泄露风险。

Method: 首先揭示了梯度跟踪固有的隐私泄露风险。在此基础上，提出了一种加权梯度跟踪分布式隐私保护算法，通过使用衰减权重因子消除隐私泄露风险。随后，表征了算法在时变异构步长下的收敛性，并证明了其在温和假设下能精确收敛到最优解。

Result: 揭示了梯度跟踪的固有隐私泄露风险。所提出的加权梯度跟踪算法成功消除了隐私泄露风险，并在理论上证明了其能精确收敛到最优解。数值模拟（分布式估计问题和分布式卷积神经网络训练）验证了算法的有效性。

Conclusion: 所提出的加权梯度跟踪分布式隐私保护算法有效解决了分布式优化中梯度跟踪的隐私泄露问题，同时保证了算法的精确收敛性，并通过实际应用场景的仿真得到了验证。

Abstract: This paper investigates the privacy-preserving distributed optimization
problem, aiming to protect agents' private information from potential attackers
during the optimization process. Gradient tracking, an advanced technique for
improving the convergence rate in distributed optimization, has been applied to
most first-order algorithms in recent years. We first reveal the inherent
privacy leakage risk associated with gradient tracking. Building upon this
insight, we propose a weighted gradient tracking distributed privacy-preserving
algorithm, eliminating the privacy leakage risk in gradient tracking using
decaying weight factors. Then, we characterize the convergence of the proposed
algorithm under time-varying heterogeneous step sizes. We prove the proposed
algorithm converges precisely to the optimal solution under mild assumptions.
Finally, numerical simulations validate the algorithm's effectiveness through a
classical distributed estimation problem and the distributed training of a
convolutional neural network.

</details>


### [201] [SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.18135)
*Shaoxun Wang,Xingjun Zhang,Qianyang Li,Jiawei Cao,Zhendong Tan*

Main category: cs.LG

TL;DR: 提出SDGF网络，通过静态图和动态图融合来捕捉多元时间序列预测中的多尺度序列间复杂相关性，以提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模多元时间序列预测中的多尺度序列间相关性方面存在局限性，难以捕捉其复杂且动态变化的特性。

Method: 提出Static-Dynamic Graph Fusion (SDGF) 网络。核心是双路径图结构学习：利用静态图（基于先验知识）锚定长期稳定依赖，并结合多级小波分解提取多尺度特征构建自适应学习的动态图，以捕获不同尺度的关联。设计注意力门控模块智能融合这两种互补信息，并使用多核膨胀卷积网络深化对时间模式的理解。

Result: 在多个广泛使用的真实世界基准数据集上的综合实验证明了所提出模型的有效性。

Conclusion: SDGF模型通过有效融合静态和动态图结构，成功解决了多元时间序列预测中多尺度序列间复杂相关性的建模挑战，提高了预测性能。

Abstract: Inter-series correlations are crucial for accurate multivariate time series
forecasting, yet these relationships often exhibit complex dynamics across
different temporal scales. Existing methods are limited in modeling these
multi-scale dependencies and struggle to capture their intricate and evolving
nature. To address this challenge, this paper proposes a novel Static-Dynamic
Graph Fusion network (SDGF), whose core lies in capturing multi-scale
inter-series correlations through a dual-path graph structure learning
approach. Specifically, the model utilizes a static graph based on prior
knowledge to anchor long-term, stable dependencies, while concurrently
employing Multi-level Wavelet Decomposition to extract multi-scale features for
constructing an adaptively learned dynamic graph to capture associations at
different scales. We design an attention-gated module to fuse these two
complementary sources of information intelligently, and a multi-kernel dilated
convolutional network is then used to deepen the understanding of temporal
patterns. Comprehensive experiments on multiple widely used real-world
benchmark datasets demonstrate the effectiveness of our proposed model.

</details>


### [202] [From Parameters to Performance: A Data-Driven Study on LLM Structure and Development](https://arxiv.org/abs/2509.18136)
*Suqing Wang,Zuchao Li,Luohe Shi,Bo Du,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.LG

TL;DR: 尽管LLMs发展迅速，但缺乏系统性、数据驱动的结构配置与性能关系研究。本文构建了一个大型数据集，并进行数据挖掘分析，揭示结构配置如何影响LLM性能，以指导未来模型开发。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）取得了显著成功，但关于其结构配置如何影响性能的系统性、数据驱动研究仍然稀缺。本文旨在填补这一空白，为LLM优化提供数据驱动的见解。

Method: 本文构建了一个包含多样化开源LLM结构及其在多基准测试上性能的大型数据集。利用该数据集，进行系统性的数据挖掘分析，以验证并量化结构配置与性能之间的关系。研究还回顾了LLMs的历史发展、探索未来趋势，并使用机械可解释性技术进一步验证研究发现。

Result: 研究提供了关于LLM优化的数据驱动见解，分析了各种结构选择如何影响模型在基准测试上的性能，并计划发布所构建的数据集。

Conclusion: 本工作旨在通过提供数据驱动的洞察，指导未来模型的定向开发和应用。

Abstract: Large language models (LLMs) have achieved remarkable success across various
domains, driving significant technological advancements and innovations.
Despite the rapid growth in model scale and capability, systematic, data-driven
research on how structural configurations affect performance remains scarce. To
address this gap, we present a large-scale dataset encompassing diverse
open-source LLM structures and their performance across multiple benchmarks.
Leveraging this dataset, we conduct a systematic, data mining-driven analysis
to validate and quantify the relationship between structural configurations and
performance. Our study begins with a review of the historical development of
LLMs and an exploration of potential future trends. We then analyze how various
structural choices impact performance across benchmarks and further corroborate
our findings using mechanistic interpretability techniques. By providing
data-driven insights into LLM optimization, our work aims to guide the targeted
development and application of future models. We will release our dataset at
https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset

</details>


### [203] [LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods](https://arxiv.org/abs/2509.18137)
*Shaoheng Wang,Yao Lu,Yuqi Li,Yaxin Gao,Jiaqi Nie,Shanqing Yu,Yingli Tian,Qi Xuan*

Main category: cs.LG

TL;DR: 为解决现有LoRA-MoE方法缺乏统一标准导致难以公平比较的问题，本研究提出了LoRALib统一基准。该基准包含40个标准化任务的680个LoRA模块。基于LoRALib的大规模实验表明，LoRAMoE表现最佳，且优先选择与目标任务相关的LoRA能进一步提升MoE性能。


<details>
  <summary>Details</summary>
Motivation: 低秩自适应（LoRA）在单任务上表现优异，但其跨任务泛化能力不足。现有LoRA-MoE方法旨在增强LoRA的适应性，但这些方法在模型、数据集、超参数和评估方法上缺乏统一标准，使得公平比较变得困难。

Method: 本研究提出了名为LoRALib的统一基准。具体方法包括：将来自40个下游任务的数据集标准化为统一格式；使用相同的超参数对17种模型架构进行微调，得到680个LoRA模块。基于此LoRA库，使用开源测试工具OpenCompass对3种代表性LoRA-MoE方法和不同的LoRA选择机制进行了大规模实验。

Result: 广泛实验结果显示，LoRAMoE方法性能最佳。此外，优先选择与目标任务相关的LoRA模块可以进一步提高MoE的性能。

Conclusion: 本研究通过建立统一基准LoRALib解决了LoRA-MoE方法比较标准的缺失问题，并揭示了LoRAMoE的优越性能以及任务相关LoRA选择机制的有效性。这些发现有望为未来的相关研究提供启发。

Abstract: As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation
(LoRA) can save significant costs in storage and computing, but its strong
adaptability to a single task is often accompanied by insufficient cross-task
generalization capabilities. To improve this, existing work combines LoRA with
mixture-of-experts (MoE) to enhance the model's adaptability through expert
modules and routing mechanisms. However, existing LoRA-MoE methods lack unified
standards in models, datasets, hyperparameters, and evaluation methods, making
it difficult to conduct fair comparisons between different methods. To this
end, we proposed a unified benchmark named LoRALib. Specifically, we
standardized datasets from $40$ downstream tasks into a unified format,
fine-tuned them using the same hyperparameters and obtained $680$ LoRA modules
across $17$ model architectures. Based on this LoRA library, we conduct
large-scale experiments on $3$ representative LoRA-MoE methods and different
LoRA selection mechanisms using the open-sourced testing tool OpenCompass.
Extensive experiments show that LoRAMoE performs best, and that prioritizing
LoRAs relevant to the target task can further improve the performance of MoE.
We hope these findings will inspire future work. Our datasets and LoRA library
are available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset
and https://huggingface.co/YaoLuzjut/models.

</details>


### [204] [Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts](https://arxiv.org/abs/2509.18138)
*Tiantian Zhang*

Main category: cs.LG

TL;DR: 引入RIPLM算法，其在“沉睡专家”设置中，通过直接在秩诱导Plackett-Luce参数化中更新，实现了秩忠诚和方差自适应。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能同时在“沉睡专家”设置中实现秩忠诚和方差自适应。本文旨在开发一种新的算法来填补这一空白，并利用已建立的秩基准与分布基准之间的结构等价性。

Method: 本文提出了一种名为秩诱导Plackett-Luce镜像下降（RIPLM）的新算法。该算法利用了秩基准和分布基准之间的结构等价性，并直接在秩诱导Plackett-Luce (PL) 参数化中进行更新，与先前操作专家身份的方法不同。这种方法确保了算法的分布在每一轮都保持在秩诱导分布类别内，从而保留了与秩基准的等价性。

Result: RIPLM是首个在“沉睡专家”设置中同时具备秩忠诚性和方差自适应性的算法。其独特更新机制确保了所玩的分布始终保持在秩诱导分布类别内，从而保持了与秩基准的等价性。

Conclusion: RIPLM是一种新颖且高效的算法，通过直接在秩诱导Plackett-Luce参数化中更新，成功地在“沉睡专家”设置中实现了前所未有的秩忠诚性和方差自适应性。

Abstract: We introduce a new algorithm, \emph{Rank-Induced Plackett--Luce Mirror
Descent (RIPLM)}, which leverages the structural equivalence between the
\emph{rank benchmark} and the \emph{distributional benchmark} established in
\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert
identities, RIPLM updates directly in the \emph{rank-induced Plackett--Luce
(PL)} parameterization. This ensures that the algorithm's played distributions
remain within the class of rank-induced distributions at every round,
preserving the equivalence with the rank benchmark. To our knowledge, RIPLM is
the first algorithm that is both (i) \emph{rank-faithful} and (ii)
\emph{variance-adaptive} in the sleeping experts setting.

</details>


### [205] [Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification](https://arxiv.org/abs/2509.18139)
*Akshay Murthy,Shawn Sebastian,Manil Shangle,Huaduo Wang,Sopam Dasgupta,Gopal Gupta*

Main category: cs.LG

TL;DR: 本研究比较了规则基分类器 FOLD-SE 与 FOLD-R++（二元分类）和 XGBoost（多类别分类）。结果显示，FOLD-SE 在二元分类中规则更少，在多类别分类中比 XGBoost 更精确高效，并能生成可解释的规则集，有效弥合了模型可解释性与性能之间的差距。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型对准确性、效率和可解释性需求的增长，传统预测模型在准确性和可解释性之间存在权衡（如神经网络高准确性但缺乏透明度）。因此，需要开发新型规则基算法来提供可解释的预测依据。

Method: 本研究旨在比较规则基分类器 FOLD-SE 和 FOLD-R++ 在二元分类中的表现，并评估 FOLD-SE 在多类别分类中与集成分类器 XGBoost 的性能。研究假设 FOLD-SE 因生成更精简可解释的规则集，其准确性和 F1 分数可能比对照模型低约 3%。研究使用分类数据集，并以准确性、F1 分数和处理时间作为主要性能衡量指标。

Result: 研究结果表明，在二元分类任务中，FOLD-SE 在规则数量上优于 FOLD-R++，同时仅牺牲了极小部分的准确性和处理效率。在多类别分类任务中，FOLD-SE 相比 XGBoost 表现出更高的精确度和显著更高的效率，并且能够生成易于理解的规则集。

Conclusion: FOLD-SE 在二元和多类别分类任务中均表现出色，是更优的选择。这些结果证明了像 FOLD-SE 这样的规则基方法能够成功弥合可解释性和性能之间的差距，表明它们在各种分类任务中是黑盒模型可行的替代方案。

Abstract: Recently, the demand for Machine Learning (ML) models that can balance
accuracy, efficiency, and interpreability has grown significantly.
Traditionally, there has been a tradeoff between accuracy and explainability in
predictive models, with models such as Neural Networks achieving high accuracy
on complex datasets while sacrificing internal transparency. As such, new
rule-based algorithms such as FOLD-SE have been developed that provide tangible
justification for predictions in the form of interpretable rule sets. The
primary objective of this study was to compare FOLD-SE and FOLD-R++, both
rule-based classifiers, in binary classification and evaluate how FOLD-SE
performs against XGBoost, a widely used ensemble classifier, when applied to
multi-category classification. We hypothesized that because FOLD-SE can
generate a condensed rule set in a more explainable manner, it would lose
upwards of an average of 3 percent in accuracy and F1 score when compared with
XGBoost and FOLD-R++ in multiclass and binary classification, respectively. The
research used data collections for classification, with accuracy, F1 scores,
and processing time as the primary performance measures. Outcomes show that
FOLD-SE is superior to FOLD-R++ in terms of binary classification by offering
fewer rules but losing a minor percentage of accuracy and efficiency in
processing time; in tasks that involve multi-category classifications, FOLD-SE
is more precise and far more efficient compared to XGBoost, in addition to
generating a comprehensible rule set. The results point out that FOLD-SE is a
better choice for both binary tasks and classifications with multiple
categories. Therefore, these results demonstrate that rule-based approaches
like FOLD-SE can bridge the gap between explainability and performance,
highlighting their potential as viable alternatives to black-box models in
diverse classification tasks.

</details>


### [206] [A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders](https://arxiv.org/abs/2509.18140)
*Iram Wajahat,Amritpal Singh,Fazel Keshtkar,Syed Ahmad Chan Bukhari*

Main category: cs.LG

TL;DR: 本研究提出一个结合机器学习预测模型和基因非特异性通路图谱的框架，用于识别2型糖尿病高风险个体并发现潜在治疗靶点。通过分析皮马印第安人数据集，该框架实现了较高的预测准确率，并提出了针对性的治疗策略，以推进精准医疗。


<details>
  <summary>Details</summary>
Motivation: 代谢性疾病，特别是2型糖尿病（T2DM），是全球性的重大健康负担，对皮马印第安人等具有遗传易感性的群体影响尤为显著。当前需要创新的解决方案来实现早期检测和靶向干预。

Method: 开发了一个整合预测建模（采用逻辑回归和主成分分析，结合t检验）与基因非特异性通路图谱的机器学习框架。利用皮马印第安人数据集识别2型糖尿病的关键预测因子。构建通路图谱策略，将识别出的预测因子与胰岛素信号、AMPK和PPAR等关键信号网络关联。提出的治疗策略通过通路富集分析进行验证。

Result: 该模型在2型糖尿病风险预测中达到了78.43%的整体准确率。成功识别了关键预测因子，并通过通路图谱提供了机制性见解，将其与胰岛素信号、AMPK和PPAR等通路联系起来。基于此，提出了如双GLP-1/GIP受体激动剂、AMPK激活剂、SIRT1调节剂和植物化学物等潜在治疗策略。

Conclusion: 该框架通过提供可解释、可扩展的早期检测和靶向干预方案，推进了代谢性疾病的精准医疗。其主要贡献包括开发用于T2DM风险预测的ML框架、引入基因非特异性通路图谱以生成机制性见解，以及为高风险人群识别新颖治疗策略。

Abstract: Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent
a significant global health burden, disproportionately impacting genetically
predisposed populations such as the Pima Indians (a Native American tribe from
south central Arizona). This study introduces a novel machine learning (ML)
framework that integrates predictive modeling with gene-agnostic pathway
mapping to identify high-risk individuals and uncover potential therapeutic
targets. Using the Pima Indian dataset, logistic regression and t-tests were
applied to identify key predictors of T2DM, yielding an overall model accuracy
of 78.43%. To bridge predictive analytics with biological relevance, we
developed a pathway mapping strategy that links identified predictors to
critical signaling networks, including insulin signaling, AMPK, and PPAR
pathways. This approach provides mechanistic insights without requiring direct
molecular data. Building upon these connections, we propose therapeutic
strategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1
modulators, and phytochemical, further validated through pathway enrichment
analyses. Overall, this framework advances precision medicine by offering
interpretable and scalable solutions for early detection and targeted
intervention in metabolic disorders. The key contributions of this work are:
(1) development of an ML framework combining logistic regression and principal
component analysis (PCA) for T2DM risk prediction; (2) introduction of a
gene-agnostic pathway mapping approach to generate mechanistic insights; and
(3) identification of novel therapeutic strategies tailored for high-risk
populations.

</details>


### [207] [KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots](https://arxiv.org/abs/2509.18141)
*Yao Zhao,Haoyue Sun,Yantian Ding,Yanxun Xu*

Main category: cs.LG

TL;DR: KM-GPT是一个全自动、AI驱动的工具，用于高准确、高鲁棒性地从Kaplan-Meier图中重建个体患者数据（IPD），解决了现有手动方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 从Kaplan-Meier图重建IPD对临床研究中的证据合成至关重要，但现有方法依赖手动数字化，效率低、易出错且缺乏可扩展性。

Method: KM-GPT是一个结合了先进图像预处理、基于GPT-5的多模态推理和迭代重建算法的全自动AI管道。它采用混合推理架构，将非结构化信息转换为结构化数据，并通过用户友好的网页界面和AI助手提高可访问性。

Result: KM-GPT在合成和真实数据集上都展现出卓越的准确性。它被成功应用于胃癌免疫治疗试验的荟萃分析，以促进证据合成和基于生物标志物的亚组分析。

Conclusion: KM-GPT通过自动化传统手动流程并提供可扩展的网页解决方案，利用重建的IPD赋能更明智的下游分析，从而改变临床研究并支持循证决策。

Abstract: Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots
provides valuable insights for evidence synthesis in clinical research.
However, existing approaches often rely on manual digitization, which is
error-prone and lacks scalability. To address these limitations, we develop
KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD
directly from KM plots with high accuracy, robustness, and reproducibility.
KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered
by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD
without manual input or intervention. Its hybrid reasoning architecture
automates the conversion of unstructured information into structured data flows
and validates data extraction from complex KM plots. To improve accessibility,
KM-GPT is equipped with a user-friendly web interface and an integrated AI
assistant, enabling researchers to reconstruct IPD without requiring
programming expertise. KM-GPT was rigorously evaluated on synthetic and
real-world datasets, consistently demonstrating superior accuracy. To
illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer
immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and
biomarker-based subgroup analyses. By automating traditionally manual processes
and providing a scalable, web-based solution, KM-GPT transforms clinical
research by leveraging reconstructed IPD to enable more informed downstream
analyses, supporting evidence-based decision-making.

</details>


### [208] [AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation](https://arxiv.org/abs/2509.18144)
*Yubo Yang,Yichen Zhu,Bo Jiang*

Main category: cs.LG

TL;DR: 本文提出AdaSTI，一种基于条件扩散模型的时空数据插补方法，通过引入BiS4PI进行预插补和设计Noise-Aware Spatio-Temporal (NAST)网络来捕捉不同扩散步骤下噪声数据中依赖关系的变化，显著提高了插补性能。


<details>
  <summary>Details</summary>
Motivation: 时空数据常因传感器故障等原因存在缺失值。尽管扩散模型在时空数据插补方面表现优异，但现有方法在提取和利用时空依赖作为条件信息时，存在误差累积问题，并忽略了不同扩散步骤下噪声数据中依赖关系的变异性。

Method: 本文提出了AdaSTI (Adaptive Dependency Model in Diffusion-based Spatio-Temporal Imputation) 方法，一个基于条件扩散模型的时空插补框架。该方法包含：1) 一个基于双向S4模型的BiS4PI网络，用于预插补；2) 一个Spatio-Temporal Conditionalizer (STC) 网络，用于从预插补结果中提取条件信息；3) 一个带有门控注意力机制的Noise-Aware Spatio-Temporal (NAST) 网络，用于捕捉跨扩散步骤的变异依赖。

Result: 在三个真实世界数据集上进行的大量实验表明，AdaSTI在所有设置下均优于现有方法，插补误差最多减少了46.4%。

Conclusion: AdaSTI通过自适应地处理时空依赖和噪声变异性，有效地解决了现有扩散模型在时空数据插补中的局限性，显著提升了缺失值插补的准确性。

Abstract: Spatio-temporal data abounds in domain like traffic and environmental
monitoring. However, it often suffers from missing values due to sensor
malfunctions, transmission failures, etc. Recent years have seen continued
efforts to improve spatio-temporal data imputation performance. Recently
diffusion models have outperformed other approaches in various tasks, including
spatio-temporal imputation, showing competitive performance. Extracting and
utilizing spatio-temporal dependencies as conditional information is vital in
diffusion-based methods. However, previous methods introduce error accumulation
in this process and ignore the variability of the dependencies in the noisy
data at different diffusion steps. In this paper, we propose AdaSTI (Adaptive
Dependency Model in Diffusion-based Spatio-Temporal Imputation), a novel
spatio-temporal imputation approach based on conditional diffusion model.
Inside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model
for pre-imputation with the imputed result used to extract conditional
information by our designed Spatio-Temporal Conditionalizer (STC)network. We
also propose a Noise-Aware Spatio-Temporal (NAST) network with a gated
attention mechanism to capture the variant dependencies across diffusion steps.
Extensive experiments on three real-world datasets show that AdaSTI outperforms
existing methods in all the settings, with up to 46.4% reduction in imputation
error.

</details>


### [209] [Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records](https://arxiv.org/abs/2509.18145)
*Syed Ahmad Chan Bukhari,Amritpal Singh,Shifath Hossain,Iram Wajahat*

Main category: cs.LG

TL;DR: 本研究提出了一个多标签分类框架，利用ICU患者前24小时数据，早期预测多种护理升级触发因素（如呼吸衰竭、血流动力学不稳定等），以克服传统预警系统只关注单一结果的局限性。


<details>
  <summary>Details</summary>
Motivation: ICU患者常表现出复杂、重叠的生理恶化迹象，需要及时升级护理。传统的早期预警系统（如SOFA或MEWS）因只关注单一结果而受到限制，无法捕捉临床恶化的多维性质。

Method: 本研究采用多标签分类框架预测护理升级触发因素（CETs），包括呼吸衰竭、血流动力学不稳定、肾脏损害和神经功能恶化。CETs通过对ICU入住后24至72小时的数据应用基于规则的标准来定义（例如，血氧饱和度低于90%）。特征从ICU入住前24小时提取，包括生命体征聚合、实验室值和静态人口统计学数据。使用MIMIC-IV数据库中的85,242例ICU住院数据进行训练和评估。评估指标包括每个标签的精确度、召回率、F1-分数和汉明损失。XGBoost被选为最佳模型。

Result: XGBoost作为表现最佳的模型，在呼吸衰竭、血流动力学、肾脏损害和神经功能恶化方面的F1-分数分别为0.66、0.72、0.76和0.62，优于基线模型。特征分析表明，呼吸频率、血压和肌酐等临床相关参数是最具影响力的预测因子，这与CETs的临床定义一致。

Conclusion: 所提出的框架展示了在无需复杂时间序列建模或自然语言处理的情况下，实现早期、可解释的临床预警的实际潜力。

Abstract: Intensive Care Unit (ICU) patients often present with complex, overlapping
signs of physiological deterioration that require timely escalation of care.
Traditional early warning systems, such as SOFA or MEWS, are limited by their
focus on single outcomes and fail to capture the multi-dimensional nature of
clinical decline. This study proposes a multi-label classification framework to
predict Care Escalation Triggers (CETs), including respiratory failure,
hemodynamic instability, renal compromise, and neurological deterioration,
using the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are
defined through rule-based criteria applied to data from hours 24 to 72 (for
example, oxygen saturation below 90, mean arterial pressure below 65 mmHg,
creatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale
score greater than 2). Features are extracted from the first 24 hours and
include vital sign aggregates, laboratory values, and static demographics. We
train and evaluate multiple classification models on a cohort of 85,242 ICU
stays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation
metrics include per-label precision, recall, F1-score, and Hamming loss.
XGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,
0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,
outperforming baseline models. Feature analysis shows that clinically relevant
parameters such as respiratory rate, blood pressure, and creatinine are the
most influential predictors, consistent with the clinical definitions of the
CETs. The proposed framework demonstrates practical potential for early,
interpretable clinical alerts without requiring complex time-series modeling or
natural language processing.

</details>


### [210] [ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks](https://arxiv.org/abs/2509.18147)
*Xinyu Mu,Hui Dou,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: ConceptFlow是一种概念可解释性框架，通过跟踪概念在CNN层间的演变来模拟模型内部“思考路径”，从而提供更深层次、更忠实、更符合人类认知的解释。


<details>
  <summary>Details</summary>
Motivation: 现有概念可解释性方法忽视了CNN中单个滤波器的语义作用以及概念在层间的动态传播。

Method: 提出ConceptFlow框架，包含两个核心组件：(i) 概念注意力（将每个滤波器与相关高层概念关联），和 (ii) 概念路径（基于概念转换矩阵量化概念在滤波器间的传播和转换）。

Result: 实验结果表明ConceptFlow为模型推理提供了有语义意义的洞察，验证了概念注意力和概念路径在解释决策行为方面的有效性。

Conclusion: ConceptFlow通过建模分层概念路径，提供了对CNN内部逻辑的更深层理解，并支持生成更忠实、更符合人类认知的解释。

Abstract: Concept-based interpretability for Convolutional Neural Networks (CNNs) aims
to align internal model representations with high-level semantic concepts, but
existing approaches largely overlook the semantic roles of individual filters
and the dynamic propagation of concepts across layers. To address these
limitations, we propose ConceptFlow, a concept-based interpretability framework
that simulates the internal "thinking path" of a model by tracing how concepts
emerge and evolve across layers. ConceptFlow comprises two key components: (i)
concept attentions, which associate each filter with relevant high-level
concepts to enable localized semantic interpretation, and (ii) conceptual
pathways, derived from a concept transition matrix that quantifies how concepts
propagate and transform between filters. Together, these components offer a
unified and structured view of internal model reasoning. Experimental results
demonstrate that ConceptFlow yields semantically meaningful insights into model
reasoning, validating the effectiveness of concept attentions and conceptual
pathways in explaining decision behavior. By modeling hierarchical conceptual
pathways, ConceptFlow provides deeper insight into the internal logic of CNNs
and supports the generation of more faithful and human-aligned explanations.

</details>


### [211] [Sparse Training Scheme for Multimodal LLM](https://arxiv.org/abs/2509.18150)
*Kean Shi,Liang Chen,Haozhe Zhao,Baobao Chang*

Main category: cs.LG

TL;DR: 为解决多模态大模型(MLLMs)训练效率低的问题，本文提出了稀疏训练方案(STS)，通过视觉token压缩和动态跳层来提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型(MLLMs)因多模态数据引入的超长输入序列和层间计算利用率低，导致训练效率低下。

Method: 提出稀疏训练方案(STS)，包含两部分：1) 视觉Token压缩器(Visual Token Compressor)，用于压缩视觉token以减少信息负荷；2) 层动态跳过器(Layer Dynamic Skipper)，用于在正向和反向传播中动态跳过语言模型中不必要的层以降低计算开销。

Result: 该方法适用于多种MLLM架构，并在多个基准测试中得到广泛验证，证明了其有效性和效率。

Conclusion: 所提出的STS框架能有效提高多模态大模型的训练效率，并具有广泛适用性。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated outstanding
performance across a variety of domains. However, training MLLMs is often
inefficient due to the significantly longer input sequences introduced by
multimodal data and the low utilization of inter-layer computations. To address
this challenge, we shift the focus to the training process itself and propose a
novel training-efficient framework based on sparse representations, termed the
Sparse Training Scheme (STS). This scheme consists of two key components: the
Visual Token Compressor, which reduces the information load by compressing
visual tokens, and the Layer Dynamic Skipper, which mitigates the computational
overhead by dynamically skipping unnecessary layers in the language model
during both forward and backward passes. Our approach is broadly applicable to
diverse MLLM architectures and has been extensively evaluated on multiple
benchmarks, demonstrating its effectiveness and efficiency.

</details>


### [212] [HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork](https://arxiv.org/abs/2509.18151)
*Jindi Lv,Yuhao Zhou,Yuxin Tian,Qing Ye,Wentao Feng,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperNAS提出一种新型神经预测器范式，通过全局编码和共享超网络增强架构表示学习，以解决神经架构搜索（NAS）中评估耗时和预测器泛化性差的问题。


<details>
  <summary>Details</summary>
Motivation: 神经架构搜索（NAS）中的性能评估耗时，且现有神经预测器（代理模型）因难以捕捉复杂架构关系而泛化性差，限制了新架构的性能预测。

Method: HyperNAS包含：1) 全局编码方案，用于捕捉宏观结构信息；2) 共享超网络，作为辅助任务增强架构间模式学习。为稳定训练，还开发了动态自适应多任务损失，促进帕累托前沿的个性化探索。

Result: 在五个代表性搜索空间（包括ViTs）上的广泛实验证明了HyperNAS的优势，尤其在少样本场景下。例如，在CIFAR-10上达到97.60%的top-1准确率，在ImageNet上达到82.4%的top-1准确率，且所需样本量至少减少5.0倍，取得了新的最先进结果。

Conclusion: HyperNAS通过创新的架构表示学习和稳定的训练机制，显著提高了神经预测器的泛化能力和预测效率，在少样本NAS任务中实现了最先进的性能。

Abstract: Time-intensive performance evaluations significantly impede progress in
Neural Architecture Search (NAS). To address this, neural predictors leverage
surrogate models trained on proxy datasets, allowing for direct performance
predictions for new architectures. However, these predictors often exhibit poor
generalization due to their limited ability to capture intricate relationships
among various architectures. In this paper, we propose HyperNAS, a novel neural
predictor paradigm for enhancing architecture representation learning. HyperNAS
consists of two primary components: a global encoding scheme and a shared
hypernetwork. The global encoding scheme is devised to capture the
comprehensive macro-structure information, while the shared hypernetwork serves
as an auxiliary task to enhance the investigation of inter-architecture
patterns. To ensure training stability, we further develop a dynamic adaptive
multi-task loss to facilitate personalized exploration on the Pareto front.
Extensive experiments across five representative search spaces, including ViTs,
demonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For
instance, HyperNAS strikes new state-of-the-art results, with 97.60\% top-1
accuracy on CIFAR-10 and 82.4\% top-1 accuracy on ImageNet, using at least
5.0$\times$ fewer samples.

</details>


### [213] [WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation](https://arxiv.org/abs/2509.18152)
*Zhenyu Qi,Qing Yu,Jichen Wang,Yun-Bo Zhao,Zerui Li,Wenjun Lv*

Main category: cs.LG

TL;DR: 针对测井解释中的挑战，本文提出了WLFM基础模型，通过多曲线测井数据进行预训练，在孔隙度估计和岩性分类任务上表现出色，并展示了其可扩展、可解释和可迁移的特性。


<details>
  <summary>Details</summary>
Motivation: 井测解释是地下表征的基础，但面临工具响应异构、信号噪声和标签有限等挑战。

Method: 提出了WLFM基础模型，该模型在来自1200口井的多曲线测井数据上进行预训练。方法包括三个阶段：将测井块标记化为地质标记；通过掩码标记建模和地层感知对比学习进行自监督预训练；以及通过少样本微调进行多任务适应。

Result: WLFM在孔隙度估计中达到0.0041 MSE，在岩性分类中达到74.13%的准确率，优于现有基线。WLFM-Finetune进一步提升至0.0038 MSE和78.10%准确率。WLFM展现出层位感知、学习可复用地质词汇和高保真重建掩码曲线的能力，尽管在浅层和超深层存在系统性偏差。聚类分析表明其在边界检测方面具有巨大潜力。

Conclusion: 研究结果表明WLFM是地质AI领域可扩展、可解释和可迁移的骨干模型，对测井、地震和文本等多模态数据整合具有重要意义。

Abstract: Well-log interpretation is fundamental for subsurface characterization but
remains challenged by heterogeneous tool responses, noisy signals, and limited
labels. We propose WLFM, a foundation model pretrained on multi-curve logs from
1200 wells, comprising three stages: tokenization of log patches into
geological tokens, self-supervised pretraining with masked-token modeling and
stratigraphy-aware contrastive learning, and multi-task adaptation with
few-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,
achieving 0.0041 MSE in porosity estimation and 74.13\% accuracy in lithology
classification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\%
accuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,
learns a reusable geological vocabulary, and reconstructs masked curves with
reasonable fidelity, though systematic offsets are observed in shallow and
ultra-deep intervals. Although boundary detection is not explicitly evaluated
here, clustering analyses suggest strong potential for future extension. These
results establish WLFM as a scalable, interpretable, and transferable backbone
for geological AI, with implications for multi-modal integration of logs,
seismic, and textual data.

</details>


### [214] [A deep reinforcement learning platform for antibiotic discovery](https://arxiv.org/abs/2509.18153)
*Hanqun Cao,Marcelo D. T. Torres,Jingjie Zhang,Zijun Gao,Fang Wu,Chunbin Gu,Jure Leskovec,Yejin Choi,Cesar de la Fuente-Nunez,Guangyong Chen,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: 该研究提出了一个名为ApexAmphion的深度学习框架，结合大型蛋白质语言模型和强化学习，旨在从头设计新型肽类抗生素。该框架能快速生成高效、广谱的候选药物，这些药物主要通过靶向细胞膜发挥作用，为应对抗菌素耐药性提供了可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 抗菌素耐药性（AMR）预计到2050年每年将导致1000万人死亡，这凸显了开发新抗生素的紧迫需求。

Method: 开发了ApexAmphion深度学习框架，该框架结合了一个64亿参数的蛋白质语言模型与强化学习。模型首先通过精选的肽数据进行微调，以捕获抗菌序列规律，然后使用近端策略优化（PPO）进行优化，其复合奖励结合了学习到的最小抑菌浓度（MIC）分类器预测和可微分的理化目标。

Result: 体外评估显示，所有100种设计肽均表现出低MIC值（部分达到纳摩尔级别，100%命中率）。其中99种化合物对至少两种临床相关细菌显示出广谱抗菌活性。主要的先导分子通过有效靶向细胞质膜来杀灭细菌。

Conclusion: 该方法通过深度强化学习将生成、评分和多目标优化整合到一个单一流程中，能够快速生成多样化、高效的候选药物，为肽类抗生素的开发提供了一条可扩展的途径，并建立了一个可在数小时内迭代调整效力和可开发性的平台。

Abstract: Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths
annually by 2050, underscoring the urgent need for new antibiotics. Here we
present ApexAmphion, a deep-learning framework for de novo design of
antibiotics that couples a 6.4-billion-parameter protein language model with
reinforcement learning. The model is first fine-tuned on curated peptide data
to capture antimicrobial sequence regularities, then optimised with proximal
policy optimization against a composite reward that combines predictions from a
learned minimum inhibitory concentration (MIC) classifier with differentiable
physicochemical objectives. In vitro evaluation of 100 designed peptides showed
low MIC values (nanomolar range in some cases) for all candidates (100% hit
rate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial
activity against at least two clinically relevant bacteria. The lead molecules
killed bacteria primarily by potently targeting the cytoplasmic membrane. By
unifying generation, scoring and multi-objective optimization with deep
reinforcement learning in a single pipeline, our approach rapidly produces
diverse, potent candidates, offering a scalable route to peptide antibiotics
and a platform for iterative steering toward potency and developability within
hours.

</details>


### [215] [MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe](https://arxiv.org/abs/2509.18154)
*Tianyu Yu,Zefan Wang,Chongyi Wang,Fuwei Huang,Wenshuo Ma,Zhihui He,Tianchi Cai,Weize Chen,Yuxiang Huang,Yuanqian Zhao,Bokai Xu,Junbo Cui,Yingjing Xu,Liqing Ruan,Luoyuan Zhang,Hanyu Liu,Jingkun Tang,Hongyuan Liu,Qining Guo,Wenhao Hu,Bingxiang He,Jie Zhou,Jie Cai,Ji Qi,Zonghao Guo,Chi Chen,Guoyang Zeng,Yuxuan Li,Ganqu Cui,Ning Ding,Xu Han,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: MiniCPM-V 4.5是一个8B参数的多模态大语言模型，通过架构、数据和训练策略的创新，在保持高效率的同时，性能超越了GPT-4o-latest和更大的开源模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）进展迅速，但其训练和推理效率是限制其普及和扩展性的核心瓶颈。

Method: 引入了MiniCPM-V 4.5模型（8B参数），并实现了三项核心改进：统一的3D-Resampler架构用于图像和视频的紧凑编码；统一的学习范式用于文档知识和文本识别，无需繁重的数据工程；混合强化学习策略以提升短、长推理能力。

Result: MiniCPM-V 4.5在OpenCompass评估中超越了GPT-4o-latest等专有模型和Qwen2.5-VL 72B等更大的开源模型。在VideoMME基准测试中，它在30B以下模型中达到了最先进的性能，且GPU内存消耗仅为Qwen2.5-VL 7B的46.7%，推理时间仅为8.7%。

Conclusion: MiniCPM-V 4.5证明了在多模态大语言模型领域，通过模型架构、数据策略和训练方法的创新，可以同时实现卓越的性能和显著的效率提升，有效解决了当前MLLM面临的瓶颈问题。

Abstract: Multimodal Large Language Models (MLLMs) are undergoing rapid progress and
represent the frontier of AI development. However, their training and inference
efficiency have emerged as a core bottleneck in making MLLMs more accessible
and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B
parameter model designed for high efficiency and strong performance. We
introduce three core improvements in model architecture, data strategy and
training method: a unified 3D-Resampler model architecture for highly compact
encoding over images and videos, a unified learning paradigm for document
knowledge and text recognition without heavy data engineering, and a hybrid
reinforcement learning strategy for proficiency in both short and long
reasoning modes. Comprehensive experimental results in OpenCompass evaluation
show that MiniCPM-V 4.5 surpasses widely used proprietary models such as
GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL
72B. Notably, the strong performance is achieved with remarkable efficiency.
For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves
state-of-the-art performance among models under 30B size, using just 46.7\% GPU
memory cost and 8.7\% inference time of Qwen2.5-VL 7B.

</details>


### [216] [Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks](https://arxiv.org/abs/2509.18161)
*William H Patty*

Main category: cs.LG

TL;DR: 本研究通过优化参数化的B样条激活函数形状，探索了神经网络中的双重优化，实现了显著的错误率降低，但增加了模型的复杂性和延迟。


<details>
  <summary>Details</summary>
Motivation: 传统的静态激活函数（如ReLU）限制了神经网络的参数效率和准确性。研究动机是通过优化激活函数的形状，为神经元分配更优的激活函数，从而训练出更参数高效和准确的模型。

Method: 本文提出并比较了9种训练方法，以探索使用参数化线性B样条激活函数的神经网络中的双重优化动态。

Result: 实验表明，与基于ReLU的传统模型相比，优化激活函数的模型在FNN中实现了高达94%的终端模型错误率降低，在CNN中实现了51%的错误率降低。然而，这些性能提升是以增加额外的开发和训练复杂性以及终端模型延迟为代价的。

Conclusion: 优化神经网络的激活函数形状可以显著提高模型性能（降低错误率），但这种性能提升伴随着开发、训练复杂性和模型延迟的增加。

Abstract: Activation functions in neural networks are typically selected from a set of
empirically validated, commonly used static functions such as ReLU, tanh, or
sigmoid. However, by optimizing the shapes of a network's activation functions,
we can train models that are more parameter-efficient and accurate by assigning
more optimal activations to the neurons. In this paper, I present and compare 9
training methodologies to explore dual-optimization dynamics in neural networks
with parameterized linear B-spline activation functions. The experiments
realize up to 94% lower end model error rates in FNNs and 51% lower rates in
CNNs compared to traditional ReLU-based models. These gains come at the cost of
additional development and training complexity as well as end model latency.

</details>


### [217] [A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge](https://arxiv.org/abs/2509.18162)
*Meraryslan Meraliyev,Cemil Turan,Shirali Kadyrov*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study last-mile delivery with one truck and one drone under explicit
battery management: the drone flies at twice the truck speed; each sortie must
satisfy an endurance budget; after every delivery the drone recharges on the
truck before the next launch. We introduce a hybrid reinforcement learning (RL)
solver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a
small pointer/attention policy that schedules drone sorties. The policy decodes
launch--serve--rendezvous triplets with hard feasibility masks for endurance
and post-delivery recharge; a fast, exact timeline simulator enforces
launch/recovery handling and computes the true makespan used by masked
greedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and
$R{=}0.1$, the method achieves an average makespan of \textbf{5.203}$\pm$0.093,
versus \textbf{5.349}$\pm$0.038 for ALNS and \textbf{5.208}$\pm$0.124 for NN --
i.e., \textbf{2.73\%} better than ALNS on average and within \textbf{0.10\%} of
NN. Per-seed, the RL scheduler never underperforms ALNS on the same instance
and ties or beats NN on two of three seeds. A decomposition of the makespan
shows the expected truck--wait trade-off across heuristics; the learned
scheduler balances both to minimize the total completion time. We provide a
config-first implementation with plotting and significance-test utilities to
support replication.

</details>


### [218] [DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns](https://arxiv.org/abs/2509.18164)
*Ranfei Chen,Ming Chen*

Main category: cs.LG

TL;DR: dLLMs在数学和逻辑任务上表现不佳。本文提出DSFT策略，通过调整掩码和损失函数，显著提升了dLLMs在这些任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)在数值敏感的数学和顺序敏感的逻辑任务上学习和理解能力面临巨大挑战，现有训练方法未能全面理解这些模式。

Method: 提出DSFT（Diffusion SFT）策略，通过调整掩码策略和损失函数，引导模型理解数学和逻辑模式。该策略可与预训练、强化学习等其他训练方法灵活结合。

Result: 在LLaDA和Dream系列模型上验证，DSFT在小规模数据上使数学问题性能提升5-10%，逻辑问题性能提升约2%。

Conclusion: DSFT提供了一种有启发性的掩码方法，有助于未来特定模式的学习，易于高效地与其他训练方法结合，并适用于各种dLLMs。

Abstract: Diffusion large language models (dLLMs) have emerged as a new architecture
following auto regressive models. Their denoising process offers a powerful
generative advantage, but they present significant challenges in learning and
understanding numerically sensitive mathematical and order-sensitive logical
tasks. Current training methods, including pre-training, fine-tuning, and
reinforcement learning, focus primarily on improving general knowledge
retention and reasoning abilities, but lack a comprehensive understanding of
mathematical and logical patterns. We propose DSFT, a simple yet effective
Diffusion SFT strategy, by adjusting the masking strategy and loss function,
guiding models to understand mathematical and logical patterns. This strategy
can be flexibly combined with pre-training, reinforcement learning, and other
training methods. Validated on models such as LLaDA and Dream series, we prove
that DSFT on small-scale data can achieve improvements of 5-10% and
approximately 2% on mathematical and logical problems, respectively. This
inspiring masking approach offers insights for future learning of specific
patterns, which can be easily and efficiently combined with other training
methods and applied to various dLLMs. Our code is publicly available at
https://anonymous.4open.science/r/DSFT-0FFB/

</details>


### [219] [MobiGPT: A Foundation Model for Mobile Wireless Networks](https://arxiv.org/abs/2509.18166)
*Xiaoqian Qi,Haoye Chai,Yong Li*

Main category: cs.LG

TL;DR: MobiGPT是一个统一的移动数据预测基础模型，能同时预测基站流量、用户应用使用和信道质量，并通过软提示学习和时间掩码机制，实现了高精度、强泛化和卓越的零/少样本迁移能力，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前移动数据预测方法依赖于针对特定数据类型的定制模型，导致在大规模异构网络中复杂性和部署成本高昂。需要一个统一的预测范式来提高效率和降低成本。

Method: 设计了一个名为MobiGPT的移动数据预测基础模型，采用统一结构来预测基站流量、用户应用使用和信道质量。提出了软提示学习方法以理解不同数据类型特征，并引入时间掩码机制以处理短期预测、长期预测和分布生成等任务。

Result: MobiGPT在真实数据集上实现了准确的多类型预测，相较于现有模型，预测准确性分别提升了27.37%、20.08%和7.27%，展现出强大的泛化能力。此外，MobiGPT在未见场景中表现出卓越的零/少样本性能，提升超过21.51%，验证了其作为基础模型的强大迁移能力。

Conclusion: MobiGPT作为一种统一的移动数据预测基础模型，通过创新方法解决了现有定制化模型的复杂性和成本问题，实现了对多种移动数据类型的高精度预测，并表现出优异的泛化和迁移能力。

Abstract: With the rapid development of mobile communication technologies, future
mobile networks will offer vast services and resources for commuting,
production, daily life, and entertainment. Accurate and efficient forecasting
of mobile data (e.g., cell traffic, user behavior, channel quality) helps
operators monitor network state changes, orchestrate wireless resources, and
schedule infrastructure and users, thereby improving supply efficiency and
service quality. However, current forecasting paradigms rely on customized
designs with tailored models for exclusive data types. Such approaches increase
complexity and deployment costs under large-scale, heterogeneous networks
involving base stations, users, and channels. In this paper, we design a
foundation model for mobile data forecasting, MobiGPT, with a unified structure
capable of forecasting three data types: base station traffic, user app usage,
and channel quality. We propose a soft-prompt learning method to help the model
understand features of different data types, and introduce a temporal masking
mechanism to guide the model through three forecasting tasks: short-term
prediction, long-term prediction, and distribution generation, supporting
diverse optimization scenarios. Evaluations on real-world datasets with over
100,000 samples show that MobiGPT achieves accurate multi-type forecasting.
Compared to existing models, it improves forecasting accuracy by 27.37%,
20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits
superior zero/few-shot performance in unseen scenarios, with over 21.51%
improvement, validating its strong transferability as a foundation model.

</details>


### [220] [PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning](https://arxiv.org/abs/2509.18169)
*Hengbo Xiao,Jingyuan Fan,Xin Tong,Jingzhao Zhang,Chao Lu,Guannan He*

Main category: cs.LG

TL;DR: 本文提出了PiMoE（Physically-isolated Mixture of Experts）架构，通过内生整合计算能力到大型语言模型中，实现了高效、可解释的推理与高精度数值计算的交替进行。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）无法将高精度数值计算作为内在可解释的能力，而主流多代理方法虽能利用外部专家，但存在通信开销、多模态能力低效和可扩展性受限的问题。

Method: PiMoE是一种训练和推理架构，通过单独训练专家、一个文本到计算模块以及一个路由器，将计算能力内生地集成到神经网络中。在推理时，路由器在token级别引导计算和推理的迭代交替进行。

Result: 与直接微调LLMs相比，PiMoE实现了更高的准确性；与主流多代理方法相比，在响应延迟、token使用和GPU能耗方面有显著改进。

Conclusion: PiMoE为下一代科学或工业智能系统提供了一种高效、可解释且可扩展的范式，用于整合计算和推理能力。

Abstract: Complex systems typically rely on high-precision numerical computation to
support decisions, but current large language models (LLMs) cannot yet
incorporate such computations as an intrinsic and interpretable capability with
existing architectures. Mainstream multi-agent approaches can leverage external
experts, but inevitably introduce communication overhead and suffer from
inefficient multimodal emergent capability and limited scalability. To this
end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and
inference architecture for integrating computation and reasoning. Instead of
the workflow paradigm of tool invocation, PiMoE endogenously integrates
computational capabilities into neural networks after separately training
experts, a text-to-computation module, and a router. At inference, the router
directs computation and reasoning at the token level, thereby enabling
iterative alternation within a single chain of thought. We evaluate PiMoE on
two reasoning-computation tasks against LLM finetuning and the multi-agent
system approaches. Results show that the PiMoE architecture achieves not only
higher accuracy than directly finetuning LLMs but also significant improvements
in response latency, token usage, and GPU energy consumption compared with
mainstream multi-agent approaches. PiMoE offers an efficient, interpretable,
and scalable paradigm for next-generation scientific or industrial intelligent
systems.

</details>


### [221] [FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification](https://arxiv.org/abs/2509.18171)
*Zhanting Zhou,KaHou Tam,Zeqin Wu,Pengzhao Sun,Jinbo Wang,Fengli Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Federated Graph Learning (FGL) under domain skew -- as observed on platforms
such as \emph{Twitch Gamers} and multilingual \emph{Wikipedia} networks --
drives client models toward incompatible representations, rendering naive
aggregation both unstable and ineffective. We find that the culprit is not the
weighting scheme but the \emph{noisy gradient signal}: empirical analysis of
baseline methods suggests that a vast majority of gradient dimensions can be
dominated by domain-specific variance. We therefore shift focus from
"aggregation-first" to a \emph{projection-first} strategy that denoises client
updates \emph{before} they are combined. The proposed FedIA framework realises
this \underline{I}mportance-\underline{A}ware idea through a two-stage,
plug-and-play pipeline: (i) a server-side top-$\rho$ mask keeps only the most
informative about 5% of coordinates, and (ii) a lightweight
influence-regularised momentum weight suppresses outlier clients. FedIA adds
\emph{no extra uplink traffic and only negligible server memory}, making it
readily deployable. On both homogeneous (Twitch Gamers) and heterogeneous
(Wikipedia) graphs, it yields smoother, more stable convergence and higher
final accuracy than nine strong baselines. A convergence sketch further shows
that dynamic projection maintains the optimal
$\mathcal{O}(\sigma^{2}/\sqrt{T})$ rate.

</details>


### [222] [SBVR: Summation of BitVector Representation for Efficient LLM Quantization](https://arxiv.org/abs/2509.18172)
*Wonjun Bang,Jongseok Park,Hongseung Yu,Kyungmin Bin,Kyunghan Lee*

Main category: cs.LG

TL;DR: 本文提出SBVR，一种新型LLM量化方法，通过硬件友好的高斯分布编码和定制CUDA核，实现了高效压缩、高精度及快速推理，显著提升了生成速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）参数量巨大导致部署困难，现有后训练量化（PTQ）方法存在局限：基于RTN的方法未考虑LLM权重的高斯分布；基于码本的方法虽考虑分布，但内存访问模式导致推理速度下降。

Method: 提出SBVR（Summation of BitVector Representation）方法，将权重值映射到符合LLM权重实际高斯分布的非均匀表示点，实现更精确压缩。同时，设计定制CUDA核，直接在SBVR格式下进行矩阵-向量乘法，无需解压，实现高性能执行。

Result: SBVR在各种模型上表现出最先进的困惑度和准确性基准性能。在4比特量化下，相较于原生FP16模型，端到端token生成速度提升了2.21倍至3.04倍。

Conclusion: SBVR通过实现高斯分布的代码表示和硬件友好的定制CUDA核，成功克服了现有LLM量化方法的局限性，在实现准确压缩的同时，显著提高了推理速度，展现出卓越的性能。

Abstract: With the advent of large language models (LLMs), numerous Post-Training
Quantization (PTQ) strategies have been proposed to alleviate deployment
barriers created by their enormous parameter counts. Quantization achieves
compression by limiting the number of representable points in the data.
Therefore, the key to achieving efficient quantization is selecting the optimal
combination of representation points, or codes, for the given data. Existing
PTQ solutions adopt two major approaches to this problem: Round-To-Nearest
(RTN)-based methods and codebook-based methods. RTN-based methods map LLM
weights onto uniformly distributed integer grids, failing to account for the
Gaussian-like weight distribution of LLM weights. Codebook-based methods
mitigate this issue by constructing distribution-aware codebooks; however, they
suffer from random and strided memory access patterns, resulting in degraded
inference speed that is exacerbated by the limited size of GPU L1 cache. To
overcome these limitations, we propose a novel LLM quantization method, SBVR
(Summation of BitVector Representation), that enables Gaussian-like code
representation in a hardware-friendly manner for fast inference. SBVR maps
weight values to non-uniform representation points whose distribution follows
the actual distribution of LLM weights, enabling more accurate compression.
Additionally, we design a custom CUDA kernel that allows matrix-vector
multiplication directly in the SBVR format without decompression, thereby
enabling high-performance execution of SBVR-compressed models. Our evaluations
of SBVR on various models demonstrate state-of-the-art perplexity and accuracy
benchmark performance while delivering a 2.21x- 3.04x end-to-end
token-generation speedup over naive FP16 models in the 4-bit quantization
regime.

</details>


### [223] [TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route](https://arxiv.org/abs/2509.18173)
*Hongyi Luo,Qing Cheng,Daniel Matos,Hari Krishna Gadi,Yanfeng Zhang,Lu Liu,Yongliang Wang,Niclas Zeller,Daniel Cremers,Liqiu Meng*

Main category: cs.LG

TL;DR: 本文提出一个大规模基准测试，评估大语言模型（LLMs）的地理空间路径认知能力，发现LLMs在路径反转任务上存在显著局限性，包括无法返回起点、非最优解、低鲁棒性和对错误答案的高置信度。


<details>
  <summary>Details</summary>
Motivation: 人类能通过自然语言解释地理空间信息，但LLMs的地理空间认知能力尚待探索。现有研究受限于非量化指标、有限的评估数据集和不清晰的研究层次。

Method: 我们构建了一个包含来自全球12个大都市的36000条路径的大规模评估数据集。引入了PathBuilder工具，用于自然语言指令与导航路径之间的相互转换。并提出了新的评估框架和指标，严格评估了11个最先进的LLMs在路径反转任务上的表现。

Result: 基准测试显示，LLMs在反转路径方面存在局限性：大多数反转路径既不能返回起点，也不接近最优路径。此外，LLMs在路径生成方面鲁棒性较低，并且对其错误答案表现出高置信度。

Conclusion: LLMs在地理空间路径认知，特别是路径反转任务上，面临显著挑战，亟需改进其地理空间推理和鲁棒性。

Abstract: Humans can interpret geospatial information through natural language, while
the geospatial cognition capabilities of Large Language Models (LLMs) remain
underexplored. Prior research in this domain has been constrained by
non-quantifiable metrics, limited evaluation datasets and unclear research
hierarchies. Therefore, we propose a large-scale benchmark and conduct a
comprehensive evaluation of the geospatial route cognition of LLMs. We create a
large-scale evaluation dataset comprised of 36000 routes from 12 metropolises
worldwide. Then, we introduce PathBuilder, a novel tool for converting natural
language instructions into navigation routes, and vice versa, bridging the gap
between geospatial information and natural language. Finally, we propose a new
evaluation framework and metrics to rigorously assess 11 state-of-the-art
(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs
exhibit limitation to reverse routes: most reverse routes neither return to the
starting point nor are similar to the optimal route. Additionally, LLMs face
challenges such as low robustness in route generation and high confidence for
their incorrect answers. Code\ \&\ Data available here:
\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}

</details>


### [224] [Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought](https://arxiv.org/abs/2509.18200)
*Yu Ti Huang*

Main category: cs.LG

TL;DR: 本文提出多模态思维链（MCoT）框架和对话方位推理（COR）基准，旨在解决对话智能体将自我中心话语（如“在我右边”）转化为绝对方向（如“N/E/S/W”）的挑战。MCoT通过三步推理过程整合语音和地标坐标，在真实世界场景和资源受限模型上实现了高准确率和鲁棒性，尤其在中文及ASR转录环境中。


<details>
  <summary>Details</summary>
Motivation: 对话智能体在GPS信号弱、地图信息缺乏的室内或复杂环境中，需要将用户自我中心（egocentric）的描述转换为绝对方位（allocentric）。现有链式思维（CoT）推理在多模态空间定位中的应用尚不充分，尤其在非英语和ASR转录场景下缺乏研究。

Method: 引入了“对话方位推理（COR）”基准，用于解决基于ASR转录的繁体中文对话导航中的自我中心到绝对方位推理问题。提出了多模态思维链（MCoT）框架，该框架通过一个结构化的三步推理过程（1.提取空间关系；2.将坐标映射到绝对方向；3.推断用户方位）整合ASR转录语音与地标坐标。在台湾大模型Taiwan-LLM-13B-v2.0-Chat上采用课程学习策略进行训练。

Result: MCoT在干净转录文本上达到100%的方位准确率，在ASR转录文本上达到98.1%的准确率，显著优于单模态和非结构化基线。该模型在嘈杂对话条件下（包括ASR识别错误和多语言代码切换）表现出强大的鲁棒性，并在跨领域评估中保持高准确性，对语言变异、领域漂移和指代歧义也具有韧性。

Conclusion: 结构化的多模态思维链（MCoT）空间推理是一种有潜力的路径，可以实现可解释且资源高效的具身导航。

Abstract: Conversational agents must translate egocentric utterances (e.g., "on my
right") into allocentric orientations (N/E/S/W). This challenge is particularly
critical in indoor or complex facilities where GPS signals are weak and
detailed maps are unavailable. While chain-of-thought (CoT) prompting has
advanced reasoning in language and vision tasks, its application to multimodal
spatial orientation remains underexplored. We introduce Conversational
Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese
conversational navigation projected from real-world environments, addressing
egocentric-to-allocentric reasoning in non-English and ASR-transcribed
scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which
integrates ASR-transcribed speech with landmark coordinates through a
structured three-step reasoning process: (1) extracting spatial relations, (2)
mapping coordinates to absolute directions, and (3) inferring user orientation.
A curriculum learning strategy progressively builds these capabilities on
Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of
resource-constrained settings. Experiments show that MCoT achieves 100%
orientation accuracy on clean transcripts and 98.1% with ASR transcripts,
substantially outperforming unimodal and non-structured baselines. Moreover,
MCoT demonstrates robustness under noisy conversational conditions, including
ASR recognition errors and multilingual code-switching. The model also
maintains high accuracy in cross-domain evaluation and resilience to linguistic
variation, domain shift, and referential ambiguity. These findings highlight
the potential of structured MCoT spatial reasoning as a path toward
interpretable and resource-efficient embodied navigation.

</details>


### [225] [Variational Task Vector Composition](https://arxiv.org/abs/2509.18208)
*Boyuan Zhang,Yingjun Du,Xiantong Zhen,Ling Shao*

Main category: cs.LG

TL;DR: 提出变分任务向量组合方法，通过贝叶斯推断和门控采样，选择性地整合任务向量中的关键信息，显著提升多任务知识集成性能。


<details>
  <summary>Details</summary>
Motivation: 任务向量组合存在结构冗余，导致高方差和采样低效。现有方法主要在任务级别操作，缺乏样本特异性，限制了多任务知识整合的效率。

Method: 提出变分任务向量组合，将组合系数视为潜在变量，通过贝叶斯推断框架进行估计。引入Spike-and-Slab先验以促进稀疏性并保留信息量最大的分量。开发门控采样机制，基于不确定性和重要性过滤组合系数，构建可控后验，实现样本级、稳定且可解释的组合。

Result: 实验结果表明，该方法在所有数据集上持续优于现有方法，通过选择性利用任务向量中最可靠和信息量大的分量，实现了更高效和有效的知识集成。

Conclusion: 本研究提出的方法为高效、有效的任务向量组合建立了新标准，具有显著的实际应用价值，提升了模型整合多任务知识的能力。

Abstract: Task vectors capture how a model changes during fine-tuning by recording the
difference between pre-trained and task-specific weights. The composition of
task vectors, a key operator in task arithmetic, enables models to integrate
knowledge from multiple tasks without incurring additional inference costs. In
this paper, we propose variational task vector composition, where composition
coefficients are taken as latent variables and estimated in a Bayesian
inference framework. Unlike previous methods that operate at the task level,
our framework focuses on sample-specific composition. Motivated by the
observation of structural redundancy in task vectors, we introduce a
Spike-and-Slab prior that promotes sparsity and preserves only the most
informative components. To further address the high variance and sampling
inefficiency in sparse, high-dimensional spaces, we develop a gated sampling
mechanism that constructs a controllable posterior by filtering the composition
coefficients based on both uncertainty and importance. This yields a more
stable and interpretable variational framework by deterministically selecting
reliable task components, reducing sampling variance while improving
transparency and generalization. Experimental results demonstrate that our
method consistently outperforms existing approaches across all datasets by
selectively leveraging the most reliable and informative components in task
vectors. These findings highlight the practical value of our approach,
establishing a new standard for efficient and effective task vector
composition.

</details>


### [226] [MolPILE - large-scale, diverse dataset for molecular representation learning](https://arxiv.org/abs/2509.18353)
*Jakub Adamczyk,Jakub Poziemski,Franciszek Job,Mateusz Król,Maciej Makowski*

Main category: cs.LG

TL;DR: 本文提出了MolPILE，一个包含2.22亿化合物的大规模、多样化且精心整理的分子预训练数据集，旨在解决现有数据集的局限性并提升基础模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有小分子预训练数据集的规模、多样性和质量有限，阻碍了分子表示学习的有效性以及基础模型的泛化能力，导致化学信息学领域缺乏类似ImageNet的标准化资源。

Method: 通过自动化整理流程，从6个大型数据库中构建了MolPILE，一个包含2.22亿化合物的大规模、多样化且严格筛选的数据集。同时，对现有预训练数据集进行了全面分析。

Result: 揭示了现有预训练数据集在训练机器学习模型方面存在显著缺陷。实验证明，在MolPILE上重新训练现有模型能够有效提升其泛化性能。

Conclusion: MolPILE为分子化学领域提供了急需的、类似ImageNet的标准化模型训练资源，有效弥补了当前数据集的不足。

Abstract: The size, diversity, and quality of pretraining datasets critically determine
the generalization ability of foundation models. Despite their growing
importance in chemoinformatics, the effectiveness of molecular representation
learning has been hindered by limitations in existing small molecule datasets.
To address this gap, we present MolPILE, large-scale, diverse, and rigorously
curated collection of 222 million compounds, constructed from 6 large-scale
databases using an automated curation pipeline. We present a comprehensive
analysis of current pretraining datasets, highlighting considerable
shortcomings for training ML models, and demonstrate how retraining existing
models on MolPILE yields improvements in generalization performance. This work
provides a standardized resource for model training, addressing the pressing
need for an ImageNet-like dataset in molecular chemistry.

</details>


### [227] [FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction](https://arxiv.org/abs/2509.18362)
*Yuxuan Cai,Xiaozhuan Liang,Xinghua Wang,Jin Ma,Haijin Liang,Jinwen Luo,Xinyu Zuo,Lisheng Duan,Yuyang Yin,Xi Chen*

Main category: cs.LG

TL;DR: FastMTP是一种新方法，通过优化多令牌预测（MTP）训练使其与推理模式对齐，显著加速大型语言模型（LLM）的推理，实现无损输出质量下的平均2.03倍加速。


<details>
  <summary>Details</summary>
Motivation: 自回归生成LLM的顺序性导致吞吐量瓶颈，限制了实际部署。多令牌预测（MTP）虽有训练效率优势，但在推理加速方面的潜力尚未充分开发。

Method: 本文提出FastMTP，通过以下方式提高多步草稿质量和推测解码性能：1) 使用位置共享权重的MTP头在自蒸馏数据上进行微调，以捕获连续未来令牌的依赖关系；2) 将语言感知动态词汇压缩集成到MTP头中，以减少草稿过程中的计算开销。

Result: 在七个不同基准测试中，FastMTP与标准下一令牌预测相比，实现了平均2.03倍的加速，且输出质量无损；比普通MTP性能提升82%。

Conclusion: FastMTP仅需轻量级训练，可无缝集成到现有推理框架中，为加速LLM推理提供了一种实用且可快速部署的解决方案。

Abstract: As large language models (LLMs) become increasingly powerful, the sequential
nature of autoregressive generation creates a fundamental throughput bottleneck
that limits the practical deployment. While Multi-Token Prediction (MTP) has
demonstrated remarkable benefits for model training efficiency and performance,
its inherent potential for inference acceleration remains largely unexplored.
This paper introduces FastMTP, a simple yet effective method that improves
multi-step draft quality by aligning MTP training with its inference pattern,
significantly enhancing speculative decoding performance. Our approach
fine-tunes a single MTP head with position-shared weights on self-distilled
data, enabling it to capture dependencies among consecutive future tokens and
maintain high acceptance rates across multiple recursive draft steps. By
integrating language-aware dynamic vocabulary compression into the MTP head, we
further reduce computational overhead in the drafting process. Experimental
results across seven diverse benchmarks demonstrate that FastMTP achieves an
average of 2.03x speedup compared to standard next token prediction with
lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires
only lightweight training and seamlessly integrates with existing inference
frameworks, offering a practical and rapidly deployable solution for
accelerating LLM inference.

</details>


### [228] [Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data](https://arxiv.org/abs/2509.18367)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.LG

TL;DR: 本文提出M-DSL算法，通过引入non-i.i.d.度量指标和多工作者选择机制，有效解决了分布式蜂群学习(DSL)在非独立同分布(non-i.i.d.)数据下性能下降的问题，并通过理论和实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 分布式蜂群学习(DSL)在边缘物联网中很有前景，但非独立同分布(non-i.i.d.)数据会严重降低学习性能并导致训练行为发散。此外，目前缺乏数据异构性如何影响模型训练准确性的理论指导。

Method: ['研究了DSL框架下非i.i.d.数据集的数据异构性影响。', '提出M-DSL算法，一种新的多工作者选择设计，以有效处理分布式异构数据。', '引入并定义了一个新的non-i.i.d.度量指标，用于量化局部数据集的统计差异，并将数据异构性与DSL性能评估联系起来。', 'M-DSL算法利用该指标指导选择对全局模型更新贡献显著的工作者。', '提供了M-DSL收敛行为的理论分析。', '在不同异构数据集和non-i.i.d.数据设置上进行了广泛实验。']

Result: 数值结果验证了M-DSL算法在性能和网络智能方面的提升，超越了现有基准方法。

Conclusion: M-DSL算法有效解决了分布式蜂群学习中非i.i.d.数据带来的挑战，通过创新的工作者选择机制和异构度量，显著提高了在复杂异构环境下的学习性能和网络智能。

Abstract: Recent advances in distributed swarm learning (DSL) offer a promising
paradigm for edge Internet of Things. Such advancements enhance data privacy,
communication efficiency, energy saving, and model scalability. However, the
presence of non-independent and identically distributed (non-i.i.d.) data pose
a significant challenge for multi-access edge computing, degrading learning
performance and diverging training behavior of vanilla DSL. Further, there
still lacks theoretical guidance on how data heterogeneity affects model
training accuracy, which requires thorough investigation. To fill the gap, this
paper first study the data heterogeneity by measuring the impact of non-i.i.d.
datasets under the DSL framework. This then motivates a new multi-worker
selection design for DSL, termed M-DSL algorithm, which works effectively with
distributed heterogeneous data. A new non-i.i.d. degree metric is introduced
and defined in this work to formulate the statistical difference among local
datasets, which builds a connection between the measure of data heterogeneity
and the evaluation of DSL performance. In this way, our M-DSL guides effective
selection of multiple works who make prominent contributions for global model
updates. We also provide theoretical analysis on the convergence behavior of
our M-DSL, followed by extensive experiments on different heterogeneous
datasets and non-i.i.d. data settings. Numerical results verify performance
improvement and network intelligence enhancement provided by our M-DSL beyond
the benchmarks.

</details>


### [229] [GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability](https://arxiv.org/abs/2509.18376)
*Burouj Armgaan,Eshan Jain,Harsh Pandey,Mahesh Chandran,Sayan Ranu*

Main category: cs.LG

TL;DR: GnnXemplar是一种新的全局GNN解释器，它通过识别代表性节点并利用LLMs生成自然语言规则，解决了现有方法在大型图解释上的不足，显著提升了忠实度、可扩展性和人类可解释性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在节点分类中广泛应用，但其决策过程不透明，限制了信任和采用。尽管局部解释方法已存在，但表征整个类别的全局解释方法仍不成熟，尤其是在大型、真实世界的图中，现有基于motif发现的方法因子图重复稀少、高维属性和复杂结构-属性交互而失效。

Method: 本文提出了GnnXemplar，一个受认知科学中范例理论启发的全局解释器。它在GNN嵌入空间中识别代表性节点（范例），并利用从这些范例邻域导出的自然语言规则来解释预测。范例选择被建模为基于反向k近邻的覆盖最大化问题，并提供高效的贪婪近似。为了生成可解释规则，GnnXemplar采用了一种结合大型语言模型（LLMs）的自精炼提示策略。

Result: 在各种基准测试中的实验表明，GnnXemplar在忠实度、可扩展性和人类可解释性方面显著优于现有方法。一项有60名参与者的用户研究进一步验证了其优越性。

Conclusion: GnnXemplar提供了一种高效、可扩展且高度可解释的全局GNN解释框架，成功克服了现有方法在处理大型复杂图时的局限性，从而显著增强了GNN的可信赖度和应用潜力。

Abstract: Graph Neural Networks (GNNs) are widely used for node classification, yet
their opaque decision-making limits trust and adoption. While local
explanations offer insights into individual predictions, global explanation
methods, those that characterize an entire class, remain underdeveloped.
Existing global explainers rely on motif discovery in small graphs, an approach
that breaks down in large, real-world settings where subgraph repetition is
rare, node attributes are high-dimensional, and predictions arise from complex
structure-attribute interactions. We propose GnnXemplar, a novel global
explainer inspired from Exemplar Theory from cognitive science. GnnXemplar
identifies representative nodes in the GNN embedding space, exemplars, and
explains predictions using natural language rules derived from their
neighborhoods. Exemplar selection is framed as a coverage maximization problem
over reverse k-nearest neighbors, for which we provide an efficient greedy
approximation. To derive interpretable rules, we employ a self-refining prompt
strategy using large language models (LLMs). Experiments across diverse
benchmarks show that GnnXemplar significantly outperforms existing methods in
fidelity, scalability, and human interpretability, as validated by a user study
with 60 participants.

</details>


### [230] [Graph Enhanced Trajectory Anomaly Detection](https://arxiv.org/abs/2509.18386)
*Jonathan Kabala Mbuya,Dieter Pfoser,Antonios Anastasopoulos*

Main category: cs.LG

TL;DR: 该论文提出了GETAD框架，通过整合道路网络拓扑、语义和历史模式，利用图注意力网络和Transformer模型，并引入置信度加权负对数似然损失函数，提高了轨迹异常检测在道路受限环境中的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹异常检测方法仅将轨迹视为采样位置序列，忽略了底层移动网络的约束和连接信息，导致无法有效识别受道路网络限制的复杂运动模式中的异常。

Method: GETAD框架整合了道路网络拓扑、路段语义和历史出行模式。它使用图注意力网络学习融合物理属性和转换行为的道路感知嵌入，并通过基于图的位置编码增强。一个基于Transformer的解码器用于建模序列运动，并结合自回归预测和监督链接预测的多目标损失函数。此外，引入了置信度加权负对数似然（CW NLL）作为异常评分函数以提高鲁棒性。

Result: 在真实世界和合成数据集上的实验表明，GETAD在检测异常方面（尤其是在道路受限环境中检测细微异常时）持续优于现有方法。

Conclusion: 研究结果强调了将图结构和上下文语义融入轨迹建模的益处，这使得异常检测更加精确和情境感知。

Abstract: Trajectory anomaly detection is essential for identifying unusual and
unexpected movement patterns in applications ranging from intelligent
transportation systems to urban safety and fraud prevention.
  Existing methods only consider limited aspects of the trajectory nature and
its movement space by treating trajectories as sequences of sampled locations,
with sampling determined by positioning technology, e.g., GPS, or by high-level
abstractions such as staypoints. Trajectories are analyzed in Euclidean space,
neglecting the constraints and connectivity information of the underlying
movement network, e.g., road or transit networks.
  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework
tightly integrates road network topology, segment semantics, and historical
travel patterns to model trajectory data. GETAD uses a Graph Attention Network
to learn road-aware embeddings that capture both physical attributes and
transition behavior, and augments these with graph-based positional encodings
that reflect the spatial layout of the road network.
  A Transformer-based decoder models sequential movement, while a
multiobjective loss function combining autoregressive prediction and supervised
link prediction ensures realistic and structurally coherent representations.
  To improve the robustness of anomaly detection, we introduce Confidence
Weighted Negative Log Likelihood (CW NLL), an anomaly scoring function that
emphasizes high-confidence deviations.
  Experiments on real-world and synthetic datasets demonstrate that GETAD
achieves consistent improvements over existing methods, particularly in
detecting subtle anomalies in road-constrained environments. These results
highlight the benefits of incorporating graph structure and contextual
semantics into trajectory modeling, enabling more precise and context-aware
anomaly detection.

</details>


### [231] [Towards Provable Emergence of In-Context Reinforcement Learning](https://arxiv.org/abs/2509.18389)
*Jiuqi Wang,Rohan Chandra,Shangtong Zhang*

Main category: cs.LG

TL;DR: 现有RL代理通过上下文解决新任务（ICRL），但预训练过程使用标准RL。本文探讨标准RL预训练为何能产生支持ICRL的参数，并提出假设：ICRL参数是预训练损失的最小化器，并通过案例研究提供初步支持。


<details>
  <summary>Details</summary>
Motivation: 现代RL代理在预训练后，无需参数更新，仅通过上下文输入即可解决新任务，这种现象称为上下文强化学习（ICRL）。然而，许多ICRL工作仍采用标准RL算法进行预训练。因此，该论文旨在回答核心问题：为什么RL预训练算法能够生成支持ICRL的网络参数？

Method: 本文通过案例研究为提出的假设提供初步支持。具体而言，它对一个用于策略评估的Transformer进行预训练，并进行理论证明。

Result: 研究证明，当一个Transformer为策略评估进行预训练时，预训练损失的其中一个全局最小化器能够实现上下文时序差分学习。这为作者的假设提供了初步支持。

Conclusion: 本文初步证明了支持ICRL的参数可以是预训练损失的最小化器，通过一个案例研究表明，针对策略评估预训练的Transformer可以实现上下文时序差分学习。

Abstract: Typically, a modern reinforcement learning (RL) agent solves a task by
updating its neural network parameters to adapt its policy to the task.
Recently, it has been observed that some RL agents can solve a wide range of
new out-of-distribution tasks without parameter updates after pretraining on
some task distribution. When evaluated in a new task, instead of making
parameter updates, the pretrained agent conditions its policy on additional
input called the context, e.g., the agent's interaction history in the new
task. The agent's performance increases as the information in the context
increases, with the agent's parameters fixed. This phenomenon is typically
called in-context RL (ICRL). The pretrained parameters of the agent network
enable the remarkable ICRL phenomenon. However, many ICRL works perform the
pretraining with standard RL algorithms. This raises the central question this
paper aims to address: Why can the RL pretraining algorithm generate network
parameters that enable ICRL? We hypothesize that the parameters capable of ICRL
are minimizers of the pretraining loss. This work provides initial support for
this hypothesis through a case study. In particular, we prove that when a
Transformer is pretrained for policy evaluation, one of the global minimizers
of the pretraining loss can enable in-context temporal difference learning.

</details>


### [232] [Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules](https://arxiv.org/abs/2509.18396)
*Doğay Altınel*

Main category: cs.LG

TL;DR: 本文对深度学习优化器进行了全面综述，涵盖其演变、工作原理、特点及面临的挑战，旨在提供理解现状和指明未来方向的资源。


<details>
  <summary>Details</summary>
Motivation: 深度学习优化器对模型学习效果至关重要且发展迅速，种类繁多，急需一份系统性综述来梳理其发展历程、机制及当前挑战。

Method: 研究采用时间顺序审查方法，从SGD到最新优化器（如Momentum, AdamW, Sophia, Muon）逐一分析，详细阐述其更新规则、相关概念、技术应用、贡献及默认超参数设置，并探讨了深度学习模型优化中的开放挑战。

Result: 本研究提供了一份全面的资源，既有助于理解优化器的当前状态，也能识别未来潜在的发展领域。

Conclusion: 该综述为理解当前深度学习优化器的演进与现状，并为未来研究方向的探索提供了宝贵的综合性参考。

Abstract: Deep learning optimizers are optimization algorithms that enable deep neural
networks to learn. The effectiveness of learning is highly dependent on the
optimizer employed in the training process. Alongside the rapid advancement of
deep learning, a wide range of optimizers with different approaches have been
developed. This study aims to provide a review of various optimizers that have
been proposed and received attention in the literature. From Stochastic
gradient descent to the most recent ones such as Momentum, AdamW, Sophia, and
Muon in chronological order, optimizers are examined individually, and their
distinctive features are highlighted in the study. The update rule of each
optimizer is presented in detail, with an explanation of the associated
concepts and variables. The techniques applied by these optimizers, their
contributions to the optimization process, and their default hyperparameter
settings are also discussed. In addition, insights are offered into the open
challenges encountered in the optimization of deep learning models. Thus, a
comprehensive resource is provided both for understanding the current state of
optimizers and for identifying potential areas of future development.

</details>


### [233] [Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations](https://arxiv.org/abs/2509.18408)
*Sarwan Ali*

Main category: cs.LG

TL;DR: 提出了一种名为Reverse-CGR (R-CGR)的新型信息保存混沌博弈表示法，解决了传统CGR信息丢失的问题，实现了生物序列的完全重建和可解释分析。


<details>
  <summary>Details</summary>
Motivation: 传统混沌博弈表示法（CGR）在几何映射过程中存在序列信息丢失这一根本局限性。

Method: 通过明确的路径编码结合有理算术精度控制，实现完全的序列恢复和从几何轨迹的完美重建。该方法在每一步都维护位置和字符信息，实现可逆性。

Result: 在生物序列分类任务中表现出与传统基于序列方法相当的竞争性性能，并提供了可解释的几何可视化。生成了适用于深度学习的特征丰富图像，同时通过明确编码保持了完整的序列信息。

Conclusion: R-CGR为可解释的生物信息学分析开辟了新途径，在需要准确性和序列恢复的场景中至关重要。

Abstract: We present a novel information-preserving Chaos Game Representation (CGR)
method, also called Reverse-CGR (R-CGR), for biological sequence analysis that
addresses the fundamental limitation of traditional CGR approaches - the loss
of sequence information during geometric mapping. Our method introduces
complete sequence recovery through explicit path encoding combined with
rational arithmetic precision control, enabling perfect sequence reconstruction
from stored geometric traces. Unlike purely geometric approaches, our
reversibility is achieved through comprehensive path storage that maintains
both positional and character information at each step. We demonstrate the
effectiveness of R-CGR on biological sequence classification tasks, achieving
competitive performance compared to traditional sequence-based methods while
providing interpretable geometric visualizations. The approach generates
feature-rich images suitable for deep learning while maintaining complete
sequence information through explicit encoding, opening new avenues for
interpretable bioinformatics analysis where both accuracy and sequence recovery
are essential.

</details>


### [234] [Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors](https://arxiv.org/abs/2509.18433)
*Chang Liu,Ladda Thiamwong,Yanjie Fu,Rui Xie*

Main category: cs.LG

TL;DR: 针对医疗AI中离线强化学习（RL）的挑战（奖励定义难、IRL不准确、策略对齐差），本文提出KANDI（Kolmogorov-Arnold Networks and Diffusion Policies for Offline IRL），结合KAN估计专家奖励函数和扩散策略优化动作，以促进高跌倒风险老年人身体活动，并在临床数据和D4RL上表现出色。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在医疗AI中受关注，但面临挑战：直接奖励难以定义；逆强化学习（IRL）难以从复杂环境中的专家行为推断准确奖励函数；离线RL策略难以与医疗应用中的人类行为对齐。具体需求是通过可穿戴传感器监测活动，促进高跌倒风险老年人身体活动。

Method: 本文引入KANDI（Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse Reinforcement Learning）。KANDI利用Kolmogorov-Arnold Networks（KANs）的灵活函数逼近能力，通过学习低跌倒风险老年人（专家）的自由生活行为来估计奖励函数。同时，将基于扩散的策略集成到Actor-Critic框架中，为离线RL中的动作优化和效率提供生成式方法。

Result: KANDI在PEER研究（一项两臂临床试验）的可穿戴活动监测数据上进行了评估，验证了其在跌倒风险干预计划中促进老年人身体活动的实际应用。此外，KANDI在D4RL基准测试中优于最先进的方法。

Conclusion: KANDI展现出解决医疗应用中离线强化学习关键挑战的潜力，为医疗保健中的活动推广干预策略提供了一个有效的解决方案。

Abstract: Utilizing offline reinforcement learning (RL) with real-world clinical data
is getting increasing attention in AI for healthcare. However, implementation
poses significant challenges. Defining direct rewards is difficult, and inverse
RL (IRL) struggles to infer accurate reward functions from expert behavior in
complex environments. Offline RL also encounters challenges in aligning learned
policies with observed human behavior in healthcare applications. To address
challenges in applying offline RL to physical activity promotion for older
adults at high risk of falls, based on wearable sensor activity monitoring, we
introduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse
Reinforcement Learning (KANDI). By leveraging the flexible function
approximation in Kolmogorov-Arnold Networks, we estimate reward functions by
learning free-living environment behavior from low-fall-risk older adults
(experts), while diffusion-based policies within an Actor-Critic framework
provide a generative approach for action refinement and efficiency in offline
RL. We evaluate KANDI using wearable activity monitoring data in a two-arm
clinical trial from our Physio-feedback Exercise Program (PEER) study,
emphasizing its practical application in a fall-risk intervention program to
promote physical activity among older adults. Additionally, KANDI outperforms
state-of-the-art methods on the D4RL benchmark. These results underscore
KANDI's potential to address key challenges in offline RL for healthcare
applications, offering an effective solution for activity promotion
intervention strategies in healthcare.

</details>


### [235] [MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems](https://arxiv.org/abs/2509.18445)
*Kangzheng Liu,Leixin Ma*

Main category: cs.LG

TL;DR: MeshODENet框架结合GNN与神经ODE，解决传统数值求解器和GNN在复杂物理系统长期预测中的计算昂贵、误差累积和不稳定性问题，实现高精度、高稳定性和大幅加速。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器在复杂物理系统多查询任务中计算成本高昂；现有GNN代理模型在长期预测中存在误差累积和不稳定性问题。

Method: 引入MeshODENet框架，该框架结合了图神经网络（GNN）的空间推理能力和神经常微分方程（Neural ODE）的连续时间建模能力。在包括一维和二维弹性体大非线性变形在内的一系列结构力学问题上进行了验证。

Result: MeshODENet在长期预测精度和稳定性方面显著优于基线模型，并比传统求解器实现了显著的计算加速。

Conclusion: 本研究提出了一种强大且可泛化的数据驱动代理方法，可加速复杂结构系统的分析和建模。

Abstract: The simulation of complex physical systems using a discretized mesh is a
cornerstone of applied mechanics, but traditional numerical solvers are often
computationally prohibitive for many-query tasks. While Graph Neural Networks
(GNNs) have emerged as powerful surrogate models for mesh-based data, their
standard autoregressive application for long-term prediction is often plagued
by error accumulation and instability. To address this, we introduce
MeshODENet, a general framework that synergizes the spatial reasoning of GNNs
with the continuous-time modeling of Neural Ordinary Differential Equations. We
demonstrate the framework's effectiveness and versatility on a series of
challenging structural mechanics problems, including one- and two-dimensional
elastic bodies undergoing large, non-linear deformations. The results
demonstrate that our approach significantly outperforms baseline models in
long-term predictive accuracy and stability, while achieving substantial
computational speed-ups over traditional solvers. This work presents a powerful
and generalizable approach for developing data-driven surrogates to accelerate
the analysis and modeling of complex structural systems.

</details>


### [236] [Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion](https://arxiv.org/abs/2509.18452)
*Anton Lebedev,Won Kyung Lee,Soumyadip Ghosh,Olha I. Yaman,Vassilis Kalantzis,Yingdong Lu,Tomasz Nowicki,Shashanka Ubaru,Lior Horesh,Vassil Alexandrov*

Main category: cs.LG

TL;DR: 提出一个AI驱动框架，通过图神经网络和贝叶斯优化来高效推荐MCMC预处理器参数，显著减少了大型稀疏线性系统的求解迭代次数和参数搜索成本。


<details>
  <summary>Details</summary>
Motivation: Krylov子空间求解器在处理病态大型稀疏线性系统时收敛缓慢，需预处理器。MCMC基矩阵求逆可生成预处理器，但其有效性高度依赖参数，而手动或网格搜索这些参数成本高昂。

Method: 开发了一个AI驱动框架来推荐给定线性系统的MCMC参数。该框架包含一个图神经网络代理模型，用于根据系统矩阵A和MCMC参数预测预处理速度；然后利用贝叶斯采集函数选择最有可能最小化迭代次数的参数集。

Result: 在未经训练的病态系统上，该框架以传统方法50%的搜索预算实现了更好的预处理效果，使收敛迭代次数减少了约10%。

Conclusion: 这些结果表明，该AI驱动框架为将MCMC基预处理器整合到大规模系统中提供了一条有效途径，提高了其实用性。

Abstract: Large, sparse linear systems are pervasive in modern science and engineering,
and Krylov subspace solvers are an established means of solving them. Yet
convergence can be slow for ill-conditioned matrices, so practical deployments
usually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix
inversion can generate such preconditioners and accelerate Krylov iterations,
but its effectiveness depends on parameters whose optima vary across matrices;
manual or grid search is costly. We present an AI-driven framework recommending
MCMC parameters for a given linear system. A graph neural surrogate predicts
preconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition
function then chooses the parameter sets most likely to minimise iterations. On
a previously unseen ill-conditioned system, the framework achieves better
preconditioning with 50\% of the search budget of conventional methods,
yielding about a 10\% reduction in iterations to convergence. These results
suggest a route for incorporating MCMC-based preconditioners into large-scale
systems.

</details>


### [237] [GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting](https://arxiv.org/abs/2509.18457)
*Ebrahim Farahmand,Reza Rahimi Azghan,Nooshin Taheri Chatrudi,Velarie Yaa Ansu-Baidoo,Eric Kim,Gautham Krishna Gudur,Mohit Malu,Owen Krueger,Edison Thomaz,Giulia Pedrielli,Pavan Turaga,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: 该论文提出GluMind，一个基于Transformer的多模态框架，用于连续、长期的血糖预测，通过特殊的注意力机制和知识保留技术实现高精度和持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有血糖预测面临挑战，包括有效整合采样率不同的多模态生理和行为数据、捕捉长期时间依赖性，以及在持续学习新数据时缓解灾难性遗忘，以实现稳健和准确的预测。

Method: 本文提出GluMind，一个基于Transformer的多模态框架。它设计了两种并行的注意力机制：交叉注意力（整合血糖与生理行为信号，解决采样率差异）和多尺度注意力（捕捉长期时间依赖）。此外，GluMind还整合了知识保留技术，以缓解持续学习中的灾难性遗忘，并提升整体预测性能。

Result: 在AIREADI数据集上的评估显示，GluMind持续优于其他SOTA预测模型，在均方根误差（RMSE）上约有15%的改进，在平均绝对误差（MAE）上约有9%的改进。

Conclusion: GluMind通过创新的多模态框架、并行注意力机制和知识保留技术，有效解决了连续、长期血糖预测中的多模态数据整合、长期依赖捕捉及灾难性遗忘问题，实现了卓越的预测性能和持续学习能力。

Abstract: This paper proposes GluMind, a transformer-based multimodal framework
designed for continual and long-term blood glucose forecasting. GluMind devises
two attention mechanisms, including cross-attention and multi-scale attention,
which operate in parallel and deliver accurate predictive performance.
Cross-attention effectively integrates blood glucose data with other
physiological and behavioral signals such as activity, stress, and heart rate,
addressing challenges associated with varying sampling rates and their adverse
impacts on robust prediction. Moreover, the multi-scale attention mechanism
captures long-range temporal dependencies. To mitigate catastrophic forgetting,
GluMind incorporates a knowledge retention technique into the transformer-based
forecasting model. The knowledge retention module not only enhances the model's
ability to retain prior knowledge but also boosts its overall forecasting
performance. We evaluate GluMind on the recently released AIREADI dataset,
which contains behavioral and physiological data collected from healthy people,
individuals with prediabetes, and those with type 2 diabetes. We examine the
performance stability and adaptability of GluMind in learning continuously as
new patient cohorts are introduced. Experimental results show that GluMind
consistently outperforms other state-of-the-art forecasting models, achieving
approximately 15% and 9% improvements in root mean squared error (RMSE) and
mean absolute error (MAE), respectively.

</details>


### [238] [Probabilistic Geometric Principal Component Analysis with application to neural data](https://arxiv.org/abs/2509.18469)
*Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 提出概率几何主成分分析(PGPCA)算法，将PPCA扩展至非线性流形几何，能有效处理围绕非线性流形分布的噪声高维数据，并在模拟和脑数据分析中优于PPCA。


<details>
  <summary>Details</summary>
Motivation: 现有的概率主成分分析(PPCA)及其扩展主要基于线性模型和欧几里得坐标系，无法有效描述神经科学等领域中常见于非线性流形周围的数据分布。

Method: 开发了概率几何主成分分析(PGPCA)算法。该方法显式地将预先从数据中拟合的非线性流形知识纳入模型，并为流形导出了一个几何坐标系来捕捉数据偏离流形和噪声。此外，还推导了一个数据驱动的EM算法来学习PGPCA模型参数。

Result: PGPCA能够有效地建模围绕各种流形的数据分布，在模拟和脑数据分析中均优于PPCA。它还能测试新的几何坐标系是否比欧几里得坐标系能更好地描述数据，并执行降维及学习流形上和流形周围的数据分布。

Conclusion: PGPCA通过引入非线性流形几何，泛化了PPCA，能够更好地描述数据分布。这些能力使其在增强分析那些包含噪声并分布在非线性流形周围的高维数据降维的效率方面具有重要价值。

Abstract: Dimensionality reduction is critical across various domains of science
including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a
prominent dimensionality reduction method that provides a probabilistic
approach unlike the deterministic approach of PCA and serves as a connection
between PCA and Factor Analysis (FA). Despite their power, PPCA and its
extensions are mainly based on linear models and can only describe the data in
a Euclidean coordinate system. However, in many neuroscience applications, data
may be distributed around a nonlinear geometry (i.e., manifold) rather than
lying in the Euclidean space. We develop Probabilistic Geometric Principal
Component Analysis (PGPCA) for such datasets as a new dimensionality reduction
algorithm that can explicitly incorporate knowledge about a given nonlinear
manifold that is first fitted from these data. Further, we show how in addition
to the Euclidean coordinate system, a geometric coordinate system can be
derived for the manifold to capture the deviations of data from the manifold
and noise. We also derive a data-driven EM algorithm for learning the PGPCA
model parameters. As such, PGPCA generalizes PPCA to better describe data
distributions by incorporating a nonlinear manifold geometry. In simulations
and brain data analyses, we show that PGPCA can effectively model the data
distribution around various given manifolds and outperforms PPCA for such data.
Moreover, PGPCA provides the capability to test whether the new geometric
coordinate system better describes the data than the Euclidean one. Finally,
PGPCA can perform dimensionality reduction and learn the data distribution both
around and on the manifold. These capabilities make PGPCA valuable for
enhancing the efficacy of dimensionality reduction for analysis of
high-dimensional data that exhibit noise and are distributed around a nonlinear
manifold.

</details>


### [239] [Discrete-time diffusion-like models for speech synthesis](https://arxiv.org/abs/2509.18470)
*Xiaozhou Tan,Minghui Zhao,Mattias Cross,Anton Ragni*

Main category: cs.LG

TL;DR: 本文探讨并提出了新的离散时间扩散类过程，用于语音生成，旨在解决连续时间扩散模型的局限性，并在实验中证明其在效率和一致性方面更优，同时保持可比的语音质量。


<details>
  <summary>Details</summary>
Motivation: 连续时间扩散模型在语音生成中受到关注，但存在训练受限于加性高斯噪声和训练/推理条件不匹配的问题。离散时间过程有望克服这些限制，实现更少的推理步骤和训练/推理完全一致性。

Method: 研究了扩散类离散时间过程，并提出了新的变体，包括应用加性高斯噪声、乘性高斯噪声、模糊噪声以及模糊噪声与高斯噪声混合的过程。

Result: 实验结果表明，离散时间过程在语音主观和客观质量上与广泛流行的连续时间对应物相当。同时，它们提供了更高效和一致的训练与推理方案。

Conclusion: 离散时间过程是语音生成中连续时间扩散模型的一个有效替代方案，能在保持语音质量的同时，显著提高训练和推理的效率与一致性。

Abstract: Diffusion models have attracted a lot of attention in recent years. These
models view speech generation as a continuous-time process. For efficient
training, this process is typically restricted to additive Gaussian noising,
which is limiting. For inference, the time is typically discretized, leading to
the mismatch between continuous training and discrete sampling conditions.
Recently proposed discrete-time processes, on the other hand, usually do not
have these limitations, may require substantially fewer inference steps, and
are fully consistent between training/inference conditions. This paper explores
some diffusion-like discrete-time processes and proposes some new variants.
These include processes applying additive Gaussian noise, multiplicative
Gaussian noise, blurring noise and a mixture of blurring and Gaussian noises.
The experimental results suggest that discrete-time processes offer comparable
subjective and objective speech quality to their widely popular continuous
counterpart, with more efficient and consistent training and inference schemas.

</details>


### [240] [Individualized non-uniform quantization for vector search](https://arxiv.org/abs/2509.18471)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: 本文提出NVQ（非均匀向量量化）技术，通过高效的非线性压缩解决了高维嵌入向量存储和检索成本高的问题，并在高保真度下提升了向量搜索精度。


<details>
  <summary>Details</summary>
Motivation: 高维嵌入向量因其庞大尺寸，导致现代向量搜索技术中检索成本高昂且占用大量内存/存储空间。

Method: 引入NVQ（非均匀向量量化），一种新的向量压缩技术。其核心在于使用新颖、精简且计算高效的非线性函数来构建非均匀向量量化器，并且这些量化器是为每个索引向量单独学习的。

Result: 实验结果表明，NVQ在计算成本最小的情况下，与现有最先进技术相比展现出更高的精度。

Conclusion: NVQ提供了一种计算和空间上高效的向量压缩解决方案，有效解决了高维向量搜索的成本问题，并提高了搜索准确性。

Abstract: Embedding vectors are widely used for representing unstructured data and
searching through it for semantically similar items. However, the large size of
these vectors, due to their high-dimensionality, creates problems for modern
vector search techniques: retrieving large vectors from memory/storage is
expensive and their footprint is costly. In this work, we present NVQ
(non-uniform vector quantization), a new vector compression technique that is
computationally and spatially efficient in the high-fidelity regime. The core
in NVQ is to use novel parsimonious and computationally efficient
nonlinearities for building non-uniform vector quantizers. Critically, these
quantizers are \emph{individually} learned for each indexed vector. Our
experimental results show that NVQ exhibits improved accuracy compared to the
state of the art with a minimal computational cost.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [241] [LIFY: IoT System for Monitoring Vital Signs of Elderly People](https://arxiv.org/abs/2509.18411)
*Sara Gonzalez,Martin Vasquez,Wilder Castellanos*

Main category: cs.NI

TL;DR: 开发了一种智能设备与云平台结合的系统，用于实时监测养老机构老年人的生理信号（体温、心率、血氧），并通过Telegram发送个性化警报。


<details>
  <summary>Details</summary>
Motivation: 旨在改进养老机构中老年人生理信号的记录方式。

Method: 开发了配备体温、心率、血氧传感器的智能设备，通过互联网将数据传输至云平台存储。云平台提供实时可视化仪表板、用户管理和个性化警报配置功能，警报通过Telegram发送。

Result: 成功实现了一个包含智能传感设备、云存储、实时可视化仪表板及个性化警报功能的生理信号监测技术解决方案。

Conclusion: 该系统为提高养老机构老年人生理信号监测的效率和响应速度提供了一种实用的技术途径。

Abstract: This article describes the implementation of a technological solution aimed
at improving the recording of physiological signals in the elderly population
residing in geriatric facilities. The developed system consists of a smart
device equipped with sensors for body temperature, heart rate, and blood oxygen
levels. This device establishes an Internet connection to transmit data to a
cloud-based platform for storage. Within this platform, a dashboard has been
created to visualize real-time values captured by the sensors, along with
additional functionalities such as user management and the configuration of
personalized alerts, which are transmitted to the solution's users through the
instant messaging system called Telegram.

</details>


### [242] [5GC-Bench: A Framework for Stress-Testing and Benchmarking 5G Core VNFs](https://arxiv.org/abs/2509.18443)
*Ioannis Panitsas,Tolga O. Atalay,Dragoslav Stojadinovic,Angelos Stavrou,Leandros Tassiulas*

Main category: cs.NI

TL;DR: 提出5GC-Bench框架，通过模拟真实负载对5G核心网进行压力测试，以识别瓶颈并优化性能。


<details>
  <summary>Details</summary>
Motivation: 5G核心网的解耦和云原生设计虽提供灵活性和可扩展性，但也带来复杂挑战（控制面VNF交互复杂，用户面流量密集）。现有工具多孤立基准测试、依赖合成负载或缺乏精细资源可见性。

Method: 提出了5GC-Bench，一个模块化框架，用于在真实工作负载下对5G核心网进行压力测试。它联合模拟信令和服务流量，支持VNF性能分析和端到端服务链分析。

Result: 5GC-Bench成功与OpenAirInterface (OAI) 5GC集成，并在真实5G测试平台上部署，揭示了资源限制和跨VNF依赖性，为容量规划和性能优化提供了可操作的洞察。

Conclusion: 5GC-Bench能有效表征5G核心网的瓶颈和资源需求，为性能优化提供实用洞察。项目成果已公开，促进复现和进一步研究。

Abstract: The disaggregated, cloud-native design of the 5G Core (5GC) enables
flexibility and scalability but introduces significant challenges.
Control-plane procedures involve complex interactions across multiple Virtual
Network Functions (VNFs), while the user plane must sustain diverse and
resource-intensive traffic. Existing tools often benchmark these dimensions in
isolation, rely on synthetic workloads, or lack visibility into fine-grained
resource usage. This paper presents 5GC-Bench, a modular framework for
stress-testing the 5GC under realistic workloads. 5GC-Bench jointly emulates
signaling and service traffic, supporting both VNF profiling and end-to-end
service-chain analysis. By characterizing bottlenecks and resource demands, it
provides actionable insights for capacity planning and performance
optimization. We integrated 5GC-Bench with the OpenAirInterface (OAI) 5GC and
deployed it on a real 5G testbed, demonstrating its ability to uncover resource
constraints and expose cross-VNF dependencies under scenarios that mirror
operational 5G deployments. To foster reproducibility and further research, we
release publicly all the artifacts.

</details>


### [243] [Using Age of Information for Throughput Optimal Spectrum Sharing](https://arxiv.org/abs/2509.18465)
*Hongjae Nam,Vishrant Tripathi,David J. Love*

Main category: cs.NI

TL;DR: 本研究提出了一种基于Whittle指数的调度策略，利用信息年龄（AoI）帮助次用户（SU）在多通道频谱共享中最大化吞吐量并最小化与主用户（PU）的冲突。


<details>
  <summary>Details</summary>
Motivation: 次用户（SU）需要在多通道频谱共享环境中，在最大化自身吞吐量的同时，最小化与主用户（PU）的冲突并满足频谱访问约束。

Method: ['将多通道问题解耦为N个单通道问题。', '证明每个单通道问题存在依赖于上次观察到的PU占用情况及其新鲜度的最优阈值策略。', '通过分析最优阈值策略的结构，建立了解耦问题的可索引性（indexability）。', '基于此结构，推导出并使用了一种基于Whittle指数的调度策略，该策略利用访问通道的信息年龄（AoI）来分配SU传输。', '将方法扩展到通道间相关的PU占用模型，并融入未知马尔可夫转移矩阵的学习机制。']

Result: ['证明了单通道问题中存在最优阈值策略。', '建立了问题的可索引性。', '提出了一种新的Whittle指数调度策略，并通过详细的数值模拟验证了其性能增益。']

Conclusion: 本研究提出了一种高效的Whittle指数调度策略，利用信息年龄解决了次用户在多通道频谱共享中的吞吐量最大化与冲突最小化问题，并通过数值模拟证明了其性能优势。

Abstract: We consider a spectrum sharing problem where two users attempt to communicate
over N channels. The Primary User (PU) has prioritized transmissions and its
occupancy on each channel over time can be modeled as a Markov chain. The
Secondary User (SU) needs to determine which channels are free at each
time-slot and attempt opportunistic transmissions. The goal of the SU is to
maximize its own throughput, while simultaneously minimizing collisions with
the PU, and satisfying spectrum access constraints. To solve this problem, we
first decouple the multiple-channel problem into N single-channel problems. For
each decoupled problem, we prove that there exists an optimal threshold policy
that depends on the last observed PU occupancy and the freshness of this
occupancy information. Second, we establish the indexability of the decoupled
problems by analyzing the structure of the optimal threshold policy. Using this
structure, we derive a Whittle index-based scheduling policy that allocates SU
transmissions using the Age of Information (AoI) of accessed channels. We also
extend our insights to PU occupancy models that are correlated across channels
and incorporate learning of unknown Markov transition matrices into our
policies. Finally, we provide detailed numerical simulations that demonstrate
the performance gains of our approach.

</details>


### [244] [Whack-a-Mole: Deterministic Packet Spraying Across Multiple Network Paths](https://arxiv.org/abs/2509.18519)
*Michael Luby,John Byers*

Main category: cs.NI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present Whack-a-Mole, a deterministic packet spraying algorithm for
distributing packets across multiple network paths with provably tight
discrepancy bounds. The algorithm is motivated by large-scale distributed AI/ML
training and inference workloads, where collective completion time (CCT) and
effective training time ratio (ETTR) are highly sensitive to tail latency and
transport imbalance. Whack-a-Mole represents the path profile as a discrete
allocation of $m$ selection units across $n$ paths and uses a bit-reversal
counter to choose a path for each packet. We prove that the discrepancy between
expected and actual packet counts per path is bounded by $O(\log m)$ over any
contiguous packet sequence. The algorithm responds quickly to congestion
feedback by reducing allocations to degraded paths and redistributing load to
healthier ones. This combination of deterministic distribution, low per-packet
overhead, and compatibility with erasure-coded transport makes Whack-a-Mole an
effective building block for multipath transport protocols that aim to minimize
CCT and maximize GPU utilization.

</details>


### [245] [Accelerating Network Slice Placement with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.18545)
*Ioannis Panitsas,Tolga O. Atalay,Dragoslav Stojadinovic,Angelos Stavrou,Leandros Tassiulas*

Main category: cs.NI

TL;DR: 本文提出一个基于解耦多智能体强化学习（MARL）的模块化框架，用于在异构多云环境中自主且近优地放置VNF，以优化网络切片部署成本和QoS，并实现了显著的决策加速。


<details>
  <summary>Details</summary>
Motivation: 随着蜂窝网络向软件化和云原生基础设施演进，在异构多云环境下部署网络切片中的虚拟网络功能（VNF）面临挑战，需要考虑多变的资源能力和切片特定需求，寻找自主且近优的VNF放置解决方案。

Method: 引入了一个模块化框架，采用解耦多智能体强化学习（MARL）方法进行VNF放置。该框架利用真实流量配置文件估算切片资源需求，并使用基于MARL的调度器，在满足QoS约束的同时最小化部署成本。

Result: 在多云测试平台的实验评估显示，该方法与组合优化相比，速度提升19倍，部署成本在最优解的7.8%以内。尽管在高负载下QoS违规增加高达2.42倍，但其提供了显著更快的决策速度和降低的计算复杂度。

Conclusion: 研究结果表明，基于多智能体强化学习（MARL）的方法为异构基础设施中的实时网络切片放置提供了一种可扩展且成本效益高的解决方案。

Abstract: Cellular networks are increasingly realized through software-based entities,
with core functions deployed as Virtual Network Functions (VNFs) on
Commercial-off-the-Shelf (COTS) hardware. Network slicing has emerged as a key
enabler of 5G by providing logically isolated Quality of Service (QoS)
guarantees for diverse applications. With the adoption of cloud-native
infrastructures, the placement of network slices across heterogeneous
multi-cloud environments poses new challenges due to variable resource
capabilities and slice-specific requirements. This paper introduces a modular
framework for autonomous and near-optimal VNF placement based on a
disaggregated Multi-Agent Reinforcement Learning (MARL) approach. The framework
incorporates real traffic profiles to estimate slice resource demands and
employs a MARL-based scheduler to minimize deployment cost while meeting QoS
constraints. Experimental evaluation on a multi-cloud testbed shows a 19x
speed-up compared to combinatorial optimization, with deployment costs within
7.8% of the optimal. While the method incurs up to 2.42x more QoS violations
under high load, the trade-off provides significantly faster decision-making
and reduced computational complexity. These results suggest that MARL-based
approaches offer a scalable and cost-efficient solution for real-time network
slice placement in heterogeneous infrastructures.

</details>


### [246] [Online Learning for Optimizing AoI-Energy Tradeoff under Unknown Channel Statistics](https://arxiv.org/abs/2509.18654)
*Mohamed A. Abd-Elmagid,Ming Shi,Eylem Ekici,Ness B. Shroff*

Main category: cs.NI

TL;DR: 本文提出在线学习算法，在信道统计信息未知的情况下，优化实时监测系统中信息新鲜度（AoI）与传输能耗之间的权衡，并取得了O(1)的阶最优后悔值。


<details>
  <summary>Details</summary>
Motivation: 实时监测系统中，能量受限的源节点需在信息新鲜度（AoI）与传输能耗之间取得平衡。现有文献在此优化问题上依赖于信道统计信息的完整知识，但在实际应用中，信道统计信息通常是未知的，这构成了一个实际挑战。

Method: 本研究首先证明了在信道统计信息已知时，最优调度策略具有基于AoI阈值的结构。基于这一关键洞察，研究者开发了具有有限时间保证的在线学习算法，以在信道统计信息未知的情况下优化AoI与能耗之间的权衡。

Result: 当信道统计信息已知时，最优调度策略被证明是基于AoI阈值的。所提出的在线学习算法，在时间范围长度上实现了阶最优的后悔值（O(1)）。

Conclusion: 本研究通过在线学习算法，有效解决了信道统计信息未知场景下，实时监测系统中信息新鲜度与传输能耗的优化问题，并取得了令人满意的（阶最优）性能。

Abstract: We consider a real-time monitoring system where a source node (with energy
limitations) aims to keep the information status at a destination node as fresh
as possible by scheduling status update transmissions over a set of channels.
The freshness of information at the destination node is measured in terms of
the Age of Information (AoI) metric. In this setting, a natural tradeoff exists
between the transmission cost (or equivalently, energy consumption) of the
source and the achievable AoI performance at the destination. This tradeoff has
been optimized in the existing literature under the assumption of having a
complete knowledge of the channel statistics. In this work, we develop online
learning-based algorithms with finite-time guarantees that optimize this
tradeoff in the practical scenario where the channel statistics are unknown to
the scheduler. In particular, when the channel statistics are known, the
optimal scheduling policy is first proven to have a threshold-based structure
with respect to the value of AoI (i.e., it is optimal to drop updates when the
AoI value is below some threshold). This key insight was then utilized to
develop the proposed learning algorithms that surprisingly achieve an
order-optimal regret (i.e., $O(1)$) with respect to the time horizon length.

</details>


### [247] [Accurate and Efficient Prediction of Wi-Fi Link Quality Based on Machine Learning](https://arxiv.org/abs/2509.18933)
*Gabriele Formis,Gianluca Cena,Lukasz Wisniewski,Stefano Scanzio*

Main category: cs.NI

TL;DR: 本文使用基于指数移动平均线性组合的低复杂度机器学习模型，预测Wi-Fi链路质量，发现在真实环境中信道无关模型表现良好，对工业环境中的Wi-Fi可靠性提升有实用价值。


<details>
  <summary>Details</summary>
Motivation: 无线通信的不可预测性给维持一致的通信质量带来挑战，因此需要开发准确高效的Wi-Fi链路质量预测模型。

Method: 采用机器学习技术，评估基于指数移动平均线性组合的数据驱动模型。这些模型专为低复杂度实现而设计，适用于资源有限的硬件平台。使用真实Wi-Fi测试台的实验数据进行准确性评估，同时考虑了信道相关和信道无关的训练数据。

Result: 信道无关模型（允许设备制造商进行通用训练）展现出具有竞争力的预测性能。

Conclusion: 本研究为在工业环境中实际部署基于机器学习的预测模型以增强Wi-Fi可靠性提供了深刻见解。

Abstract: Wireless communications are characterized by their unpredictability, posing
challenges for maintaining consistent communication quality. This paper
presents a comprehensive analysis of various prediction models, with a focus on
achieving accurate and efficient Wi-Fi link quality forecasts using machine
learning techniques. Specifically, the paper evaluates the performance of
data-driven models based on the linear combination of exponential moving
averages, which are designed for low-complexity implementations and are then
suitable for hardware platforms with limited processing resources. Accuracy of
the proposed approaches was assessed using experimental data from a real-world
Wi-Fi testbed, considering both channel-dependent and channel-independent
training data. Remarkably, channel-independent models, which allow for
generalized training by equipment manufacturers, demonstrated competitive
performance. Overall, this study provides insights into the practical
deployment of machine learning-based prediction models for enhancing Wi-Fi
dependability in industrial environments.

</details>


### [248] [Poster: The Internet Quality Barometer Framework](https://arxiv.org/abs/2509.19034)
*Lai Yi Ohlsen,Pavlos Sermpezis,Melissa Newcomb*

Main category: cs.NI

TL;DR: 本文提出了互联网质量晴雨表（IQB）框架，旨在通过考虑用户用例和网络要求，重新定义超越速度的互联网质量，并计算一个综合得分。


<details>
  <summary>Details</summary>
Motivation: 现有互联网质量定义主要关注“速度”，未能全面反映用户实际体验，因此需要一个更以用户为中心且综合性的评估框架。

Method: IQB框架(i)以流行用例为基础，以用户为中心定义互联网质量；(ii)通过权重和质量阈值将网络要求映射到用例；(iii)利用公开的互联网性能数据集计算IQB分数，作为反映互联网体验质量的综合指标。

Result: 成功提出并实现了一个新的、以用户为中心的互联网质量评估框架——IQB，能够计算出反映互联网体验质量的综合IQB分数。

Conclusion: IQB框架提供了一种更全面、以用户为中心的互联网质量评估方法，超越了单一的速度指标，能更好地反映用户的真实体验。

Abstract: In this paper, we introduce the Internet Quality Barometer (IQB), a framework
aiming to redefine Internet quality beyond ``speed''. IQB (i) defines Internet
quality in a user-centric way by considering popular use cases, (ii) maps
network requirements to use cases through a set of weights and quality
thresholds, and (iii) leverages publicly available Internet performance
datasets, to calculate the IQB score, a composite metric that reflects the
quality of Internet experience.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [249] [Optimal Service Mode Assignment in a Simple Computation Offloading System: Extended Version](https://arxiv.org/abs/2509.18356)
*Darin Jeff,Eytan Modiano*

Main category: eess.SY

TL;DR: 研究计算卸载模型中的作业分配策略，旨在最小化系统延迟。结果表明，当云服务器空闲时直接卸载至云端最优；当云服务器繁忙时，最优策略是基于队列长度的阈值策略，决定是否先本地处理。


<details>
  <summary>Details</summary>
Motivation: 在计算卸载模型中，设计一种基于系统状态的作业分配策略（完全卸载到云端或部分本地处理后再传至云端），以最小化系统延迟。

Method: 通过理论分析推导了在不同云服务器状态（空闲或繁忙）下的最优作业分配策略，并利用仿真验证了所提出策略的结构。

Result: 当云服务器空闲时，最优策略是将下一个作业分配给云端处理。当云服务器繁忙时，在温和假设下，最优策略是阈值类型：如果系统队列超过某个阈值，则将下一个作业发送到本地服务器进行预处理。

Conclusion: 本研究提供了一个能够最小化计算卸载系统延迟的作业分配策略，该策略根据云服务器的忙闲状态和系统队列长度动态调整，并通过仿真得到了验证。

Abstract: We consider a simple computation offloading model where jobs can either be
fully processed in the cloud or be partially processed at a local server before
being sent to the cloud to complete processing. Our goal is to design a policy
for assigning jobs to service modes, i.e., full offloading or partial
offloading, based on the state of the system, in order to minimize delay in the
system. We show that when the cloud server is idle, the optimal policy is to
assign the next job in the system queue to the cloud for processing. However,
when the cloud server is busy, we show that, under mild assumptions, the
optimal policy is of a threshold type, that sends the next job in the system
queue to the local server if the queue exceeds a certain threshold. Finally, we
demonstrate this policy structure through simulations.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [250] [No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS](https://arxiv.org/abs/2509.18531)
*Seungyoun Shin,Dongha Ahn,Jiwoo Kim,Sungwook Jeon*

Main category: eess.AS

TL;DR: 针对GRPO在TTS中导致韵律不自然问题，本文提出迭代DPO方法，利用少量人工偏好数据直接优化韵律，在韩语客服数据集上取得最佳人体偏好和竞争性CER。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法在TTS中虽然能降低错误率，但由于缺乏可验证的韵律奖励，导致生成的语音单调、不自然；同时，增加说话人相似性会进一步破坏训练并降低错误率。

Method: 提出一种迭代的直接偏好优化（DPO）方案。该方案每轮仅使用几百对人工标注的偏好对，直接优化韵律的自然度，并对当前模型进行正则化。

Result: 在KoCC-TTS（一个精选的韩语呼叫中心对话数据集）上，该方法取得了最高的人体偏好（ELO）分数和有竞争力的字符错误率（CER），优于GRPO和强大的商业基线。

Conclusion: 当韵律无法自动奖励时，人工偏好优化为实现自然、鲁棒的TTS提供了一条实用且数据高效的路径。

Abstract: Recent work reports gains in neural text-to-speech (TTS) with Group Relative
Policy Optimization (GRPO). However, in the absence of a verifiable reward for
\textit{prosody}, GRPO trained on transcription-oriented signals (CER/NLL)
lowers error rates yet collapses prosody into monotone, unnatural speech;
adding speaker-similarity further destabilizes training and degrades CER. We
address this with an \textit{iterative Direct Preference Optimization (DPO)}
scheme that uses only a few hundred human-labeled preference pairs per round to
directly optimize prosodic naturalness while regularizing to the current model.
On \textbf{KoCC-TTS}, a curated dataset of authentic Korean call center
interactions capturing task-oriented dialogues, our method attains the highest
human preference (ELO) with competitive CER, outperforming GRPO and strong
commercial baselines. These results suggest that when prosody cannot be
rewarded automatically, \textit{human preference optimization} offers a
practical and data-efficient path to natural and robust TTS. The demo page is
available at \href{https://tts.ch.dev}

</details>


### [251] [HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task Speech Language Modeling](https://arxiv.org/abs/2509.18570)
*Yuke Si,Runyan Yang,Yingying Gao,Junlan Feng,Chao Deng,Shilei Zhang*

Main category: eess.AS

TL;DR: 本文提出HarmoniFuse框架，通过组件选择和提示自适应融合，解决多任务语音语言模型中异构任务（如ASR和SER）的信息冲突问题，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有统一语音语言模型（SLMs）在处理ASR和SER等需要不同类型信息的任务时，缺乏显式建模信息构成差异，导致任务干扰和性能下降，尤其是在数据受限条件下。

Method: 本文提出HarmoniFuse，一个组件选择和提示自适应的多任务语音语言建模框架。它集成了门控语音编码器来提取任务特定的声学特征，以及一个提示自适应动态融合模块来根据任务特性聚合Transformer层。此外，采用批次交错训练策略以利用独立的ASR和SER数据集。

Result: 实验结果表明，HarmoniFuse显著提高了ASR和SER的性能。

Conclusion: HarmoniFuse为在实际数据约束下进行多任务语音理解提供了一个可扩展且稳健的解决方案。

Abstract: Recent advances in large language models have facilitated the development of
unified speech language models (SLMs) capable of supporting multiple speech
tasks within a shared architecture. However, tasks such as automatic speech
recognition (ASR) and speech emotion recognition (SER) rely on distinct types
of information: ASR primarily depends on linguistic content, whereas SER
requires the integration of both linguistic and paralinguistic cues. Existing
multitask SLMs typically adopt naive parameter sharing or prompt-based
conditioning without explicitly modeling the differences in information
composition required by each task. Such designs risk task interference and
performance degradation, especially under limited data conditions. To address
these limitations, we propose HarmoniFuse, a component-selective and
prompt-adaptive framework for multi-task speech language modeling. HarmoniFuse
is designed to harmonize heterogeneous task demands by selecting and fusing
task-relevant components of speech representations. Specifically, it integrates
a gated speech encoder to extract task-specific acoustic features and a
prompt-adaptive dynamic fusion module to aggregate transformer layers based on
task characteristics. In addition, a batch-interleaved training strategy
enables leveraging separate ASR and SER datasets without requiring joint
annotation. Experimental results demonstrate that HarmoniFuse improves both ASR
and SER performance, offering a scalable and robust solution for multitask
speech understanding under realistic data constraints.

</details>
