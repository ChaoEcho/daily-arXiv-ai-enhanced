<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 19]
- [cs.CV](#cs.CV) [Total: 15]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.LG](#cs.LG) [Total: 15]
- [cs.NI](#cs.NI) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies](https://arxiv.org/abs/2509.03525)
*Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sepehr Karimi,Sina Rashidi,Ali Zolnour,Maryam Dadkhah,Yasaman Haghbin,Hossein AzadMaleki,Maryam Zolnoori*

Main category: cs.CL

TL;DR: 研究比较了大语言模型在语音痴呆检测中的多种适应策略，发现特定微调方法能显著提升性能，使开源模型可媲美甚至超越商业系统。


<details>
  <summary>Details</summary>
Motivation: 美国超过半数阿尔茨海默病及相关痴呆症患者未确诊，而基于语音的筛查提供了一种可扩展的检测途径。

Method: 利用DementiaBank语音语料库，评估了9个纯文本模型和3个多模态音-文模型，比较了多种大语言模型适应策略。这些策略包括：不同演示选择策略的上下文学习、推理增强提示、参数高效微调以及多模态整合。

Result: 类中心演示在上下文学习中表现最佳；推理能力改进了小型模型的性能；token级微调普遍获得最佳分数；添加分类头显著提升了表现较差模型的性能。多模态模型中，微调的音-文系统表现良好，但未能超越顶级的纯文本模型。

Conclusion: 模型适应策略（如演示选择、推理设计和调优方法）对基于语音的痴呆检测具有关键影响，且经过适当适应的开源模型能够达到或超越商业系统的性能。

Abstract: Over half of US adults with Alzheimer disease and related dementias remain
undiagnosed, and speech-based screening offers a scalable detection approach.
We compared large language model adaptation strategies for dementia detection
using the DementiaBank speech corpus, evaluating nine text-only models and
three multimodal audio-text models on recordings from DementiaBank speech
corpus. Adaptations included in-context learning with different demonstration
selection policies, reasoning-augmented prompting, parameter-efficient
fine-tuning, and multimodal integration. Results showed that class-centroid
demonstrations achieved the highest in-context learning performance, reasoning
improved smaller models, and token-level fine-tuning generally produced the
best scores. Adding a classification head substantially improved
underperforming models. Among multimodal models, fine-tuned audio-text systems
performed well but did not surpass the top text-only models. These findings
highlight that model adaptation strategies, including demonstration selection,
reasoning design, and tuning method, critically influence speech-based dementia
detection, and that properly adapted open-weight models can match or exceed
commercial systems.

</details>


### [2] [Enhancing Speech Large Language Models through Reinforced Behavior Alignment](https://arxiv.org/abs/2509.03526)
*Yansong Liu,Jiateng Li,Yuan Liu*

Main category: cs.CL

TL;DR: 本文提出RBA框架，通过教师LLM自合成数据和强化学习，显著提升语音大模型(SpeechLMs)的指令遵循能力，并在多种语音任务上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 语音大模型(SpeechLMs)在指令遵循方面与文本LLMs存在显著性能差距，尤其在处理动态多变的语音输入时，其表现受限于模态间差异。

Method: 提出强化行为对齐(RBA)框架。该方法不依赖人工标注，而是通过强大的教师LLM自合成大量高质量对齐数据，并利用强化学习将SpeechLMs的行为与教师LLM对齐。

Result: 实验证明RBA有效提升了SpeechLMs的指令遵循能力，超越了传统蒸馏基线。该方法可扩展至语音问答和语音到文本翻译等任务，仅使用自生成数据即在公开基准上达到最先进水平。

Conclusion: RBA通过自合成数据和强化学习，成功弥合了SpeechLMs与文本LLMs在指令遵循上的性能差距，并在多模态任务中展现出卓越的泛化能力和领先性能。

Abstract: The recent advancements of Large Language Models (LLMs) have spurred
considerable research interest in extending their linguistic capabilities
beyond text to other modalities, which leads to emergence of speech-based LLMs
(SpeechLMs) with capability of processing user request in either speech or
textual formats. However, owing to inter-modal discrepancies, these SpeechLMs
still exhibit a significant performance gap compared to their text-based LLM
counterparts in instruction-following, particularly when confronted with the
dynamic and variable nature of user speech. To address this challenge, this
paper introduces a framework termed Reinforced Behavior Alignment (RBA),
designed to bolster the language generation proficiency of SpeechLMs. Instead
of relying on supervised fine-tuning from human annotations, RBA employs a
self-synthesis methodology to generate extensive, high-fidelity alignment data
by a powerful teacher LLM. Then SpeechLMs is aligned its behavior with that of
a teacher using a reinforcement learning-based approach. Experimental results
demonstrate that this method effectively enhances the instruction-following
capabilities of SpeechLMs that outperform conventional distillation baselines.
Crucially, we demonstrate that RBA can be seamlessly extended to tasks such
including spoken question answering and speech-to-text translation, attaining
state-of-the-art performance on open benchmarks with only self-generated data.

</details>


### [3] [Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned Mistral Large Language Model](https://arxiv.org/abs/2509.03527)
*Bohdan M. Pavlyshenko*

Main category: cs.CL

TL;DR: 本文利用微调的Mistral 7B大型语言模型（LLM）结合RAG技术，对加密货币新闻进行多层级多任务分析，生成图文摘要和综合报告，并有效减少LLM幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机包括：对加密货币新闻进行多层级多任务分析以获取深度洞察；通过将新闻表示为知识图谱来消除大型语言模型可能产生的幻觉问题；以及结合图和文本摘要提供互补且全面的视角。

Method: 研究方法采用结合检索增强生成（RAG）的微调Mistral 7B大型语言模型。模型通过PEFT/LoRA方法和4位量化进行微调。分析分为多个层级：第一层级生成带有情感分数的图和文本摘要，以及JSON表示；更高层级则将这些摘要进行分层堆叠和整合，形成综合报告。核心是将加密货币新闻表示为知识图谱。

Result: 结果表明，使用微调的Mistral 7B LLM模型进行多层级加密货币新闻分析，能够进行信息丰富的定性和定量分析。该方法成功提供重要洞察，并且通过知识图谱表示有效解决了LLM的幻觉问题。

Conclusion: 结论是，微调的Mistral 7B LLM模型结合RAG技术，能有效进行多层级加密货币新闻分析，提供有价值的定性和定量分析及重要洞察，并借助知识图谱消除了LLM幻觉问题。

Abstract: In the paper, we consider multilevel multitask analysis of cryptocurrency
news using a fine-tuned Mistral 7B large language model with
retrieval-augmented generation (RAG).
  On the first level of analytics, the fine-tuned model generates graph and
text summaries with sentiment scores as well as JSON representations of
summaries. Higher levels perform hierarchical stacking that consolidates sets
of graph-based and text-based summaries as well as summaries of summaries into
comprehensive reports. The combination of graph and text summaries provides
complementary views of cryptocurrency news. The model is fine-tuned with 4-bit
quantization using the PEFT/LoRA approach. The representation of cryptocurrency
news as knowledge graph can essentially eliminate problems with large language
model hallucinations.
  The obtained results demonstrate that the use of fine-tuned Mistral 7B LLM
models for multilevel cryptocurrency news analysis can conduct informative
qualitative and quantitative analytics, providing important insights.

</details>


### [4] [The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process](https://arxiv.org/abs/2509.03528)
*Matilde Contestabile,Chiara Ferrara,Alberto Giovannetti,Giovanni Parrillo,Andrea Vandin*

Main category: cs.CL

TL;DR: 为解决法律领域过程挖掘（PM）数据集缺乏问题，本文介绍了ProLiFIC，一个利用大型语言模型（LLMs）从意大利立法非结构化数据构建的综合事件日志，并提议其作为法律PM的基准。


<details>
  <summary>Details</summary>
Motivation: 过程挖掘（PM）在法律领域的应用受限于高质量数据集的可访问性和质量。

Method: 构建了ProLiFIC数据集，该数据集是1987年至2022年意大利立法流程的综合事件日志。数据来源于Normattiva门户网站的非结构化数据，并使用大型语言模型（LLMs）进行结构化处理。

Result: 成功创建了ProLiFIC数据集，一个涵盖意大利立法过程的综合事件日志，并展示了初步分析示例。

Conclusion: ProLiFIC被提出作为法律过程挖掘领域的基准，旨在促进该领域的新发展。

Abstract: Process Mining (PM), initially developed for industrial and business
contexts, has recently been applied to social systems, including legal ones.
However, PM's efficacy in the legal domain is limited by the accessibility and
quality of datasets. We introduce ProLiFIC (Procedural Lawmaking Flow in
Italian Chambers), a comprehensive event log of the Italian lawmaking process
from 1987 to 2022. Created from unstructured data from the Normattiva portal
and structured using large language models (LLMs), ProLiFIC aligns with recent
efforts in integrating PM with LLMs. We exemplify preliminary analyses and
propose ProLiFIC as a benchmark for legal PM, fostering new developments.

</details>


### [5] [Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages](https://arxiv.org/abs/2509.03529)
*Alejandro Álvarez Castro,Joaquín Ordieres-Meré*

Main category: cs.CL

TL;DR: 针对财报电话会议中现有情感分析模型未能捕捉其层级话语结构的问题，本文提出了一种新颖的多模态框架。该框架通过将会议编码为分层话语树，结合文本、音频、视频情感信号和结构化元数据，并采用两阶段Transformer架构，生成了稳定且语义丰富的多模态嵌入。


<details>
  <summary>Details</summary>
Motivation: 尽管金融情感分析在整合多模态信号方面有所进展，但大多数现有系统依赖于扁平的文档级或句子级模型，未能有效捕捉财报电话会议这种高风险、非脚本式金融沟通中固有的分层话语结构。

Method: 本文提出一个多模态框架，通过将财报电话会议编码为分层话语树，以生成语义丰富且结构感知的嵌入。每个节点（独白或问答对）都通过文本、音频和视频提取情感信号，并结合连贯性得分、主题标签和回答覆盖率评估等结构化元数据进行增强。采用两阶段Transformer架构：第一阶段使用对比学习在节点层面编码多模态内容和话语元数据；第二阶段则合成整个会议的全局嵌入。

Result: 实验结果表明，所生成的嵌入形成了稳定且语义有意义的表示，能够有效反映情感基调、结构逻辑和主题一致性。

Conclusion: 该方法为多模态话语表示提供了一种鲁棒、可解释且可泛化的途径。它不仅对金融预测和话语评估等下游任务具有实际应用价值，还能推广应用于远程医疗、教育和政治话语等其他高风险、非脚本式交流领域。

Abstract: Earnings calls represent a uniquely rich and semi-structured source of
financial communication, blending scripted managerial commentary with
unscripted analyst dialogue. Although recent advances in financial sentiment
analysis have integrated multi-modal signals, such as textual content and vocal
tone, most systems rely on flat document-level or sentence-level models,
failing to capture the layered discourse structure of these interactions. This
paper introduces a novel multi-modal framework designed to generate
semantically rich and structurally aware embeddings of earnings calls, by
encoding them as hierarchical discourse trees. Each node, comprising either a
monologue or a question-answer pair, is enriched with emotional signals derived
from text, audio, and video, as well as structured metadata including coherence
scores, topic labels, and answer coverage assessments. A two-stage transformer
architecture is proposed: the first encodes multi-modal content and discourse
metadata at the node level using contrastive learning, while the second
synthesizes a global embedding for the entire conference. Experimental results
reveal that the resulting embeddings form stable, semantically meaningful
representations that reflect affective tone, structural logic, and thematic
alignment. Beyond financial reporting, the proposed system generalizes to other
high-stakes unscripted communicative domains such as tele-medicine, education,
and political discourse, offering a robust and explainable approach to
multi-modal discourse representation. This approach offers practical utility
for downstream tasks such as financial forecasting and discourse evaluation,
while also providing a generalizable method applicable to other domains
involving high-stakes communication.

</details>


### [6] [Reading Between the Signs: Predicting Future Suicidal Ideation from Adolescent Social Media Texts](https://arxiv.org/abs/2509.03530)
*Paul Blum,Enrico Liscio,Ruixuan Zhang,Caroline Figueroa,Pradeep K. Murukannaiah*

Main category: cs.CL

TL;DR: 本文提出Early-SIB模型，利用Transformer架构分析青少年在在线论坛上的历史帖子，旨在预测其在明确表达自杀意念之前是否会发布自杀意念或行为（SIB）相关内容，在荷兰青少年论坛上实现了0.73的平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 青少年自杀是主要死因，但预测困难且许多案例因缺乏心理健康服务接触而未被发现。社交媒体为实时洞察青少年心理困境提供了独特机会。现有文献多集中于明确自杀意念后的识别，而提前预测（无自我披露作为输入）的研究尚不充分。

Method: 提出了一个新颖的任务和方法：在青少年明确表达自杀意念前，根据其论坛帖子预测自杀意念和行为（SIB）。为此，引入了Early-SIB，一个基于Transformer的模型，它顺序处理用户撰写和参与的帖子，以预测该用户将来是否会发布SIB帖子。

Result: 在荷兰青少年论坛上，Early-SIB模型在预测未来SIB方面达到了0.73的平衡准确率。

Conclusion: 研究结果表明，利用此类工具可以有效补充传统自杀预防方法，为识别处于风险中的青少年提供有意义的帮助。

Abstract: Suicide is a leading cause of death among adolescents (12-18), yet predicting
it remains a significant challenge. Many cases go undetected due to a lack of
contact with mental health services. Social media, however, offers a unique
opportunity, as young people often share their thoughts and struggles online in
real time. In this work, we propose a novel task and method to approach it:
predicting suicidal ideation and behavior (SIB) from forum posts before an
adolescent explicitly expresses suicidal ideation on an online forum. This
predictive framing, where no self-disclosure is used as input at any stage,
remains largely unexplored in the suicide prediction literature. To this end,
we introduce Early-SIB, a transformer-based model that sequentially processes
the posts a user writes and engages with to predict whether they will write a
SIB post. Our model achieves a balanced accuracy of 0.73 for predicting future
SIB on a Dutch youth forum, demonstrating that such tools can offer a
meaningful addition to traditional methods.

</details>


### [7] [Real-Time Detection of Hallucinated Entities in Long-Form Generation](https://arxiv.org/abs/2509.03531)
*Oscar Obeso,Andy Arditi,Javier Ferrando,Joshua Freeman,Cameron Holmes,Neel Nanda*

Main category: cs.CL

TL;DR: 本文提出一种针对长文本生成中实体级幻觉的廉价、可扩展的实时检测方法，通过构建标注数据集和训练线性探针实现，在多种模型上性能优越并展现出泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗咨询、法律建议等高风险应用中被广泛使用，但幻觉可能造成严重危害。现有幻觉检测方法不切实际，因其局限于简短的事实查询或需要昂贵的外部验证。

Method: 开发了一种廉价、可扩展的方法，用于实时识别长文本生成中的实体级幻觉（如虚构名称、日期、引用），并成功扩展到70B参数模型。该方法侧重于实体级而非声明级幻觉，支持令牌级标注和流式检测。通过利用网络搜索创建了一个标注数据集，并使用线性探针等简单高效的方法训练幻觉分类器。

Result: 在四种模型家族的评估中，该分类器在长文本响应上的表现持续优于基线方法（如Llama-3.3-70B上AUC达0.90，对比语义熵的0.71），并在短问答设置中也有所提升。尽管仅使用实体级标签训练，探针也能有效检测数学推理任务中的错误答案，显示出超越实体的泛化能力。此外，一个模型的标注响应可用于训练其他模型的有效分类器，且数据集已公开发布。

Conclusion: 本研究为可扩展、真实世界的幻觉检测提供了一种有前景的新方法。

Abstract: Large language models are now routinely used in high-stakes applications
where hallucinations can cause serious harm, such as medical consultations or
legal advice. Existing hallucination detection methods, however, are
impractical for real-world use, as they are either limited to short factual
queries or require costly external verification. We present a cheap, scalable
method for real-time identification of hallucinated tokens in long-form
generations, and scale it effectively to 70B parameter models. Our approach
targets \emph{entity-level hallucinations} -- e.g., fabricated names, dates,
citations -- rather than claim-level, thereby naturally mapping to token-level
labels and enabling streaming detection. We develop an annotation methodology
that leverages web search to annotate model responses with grounded labels
indicating which tokens correspond to fabricated entities. This dataset enables
us to train effective hallucination classifiers with simple and efficient
methods such as linear probes. Evaluating across four model families, our
classifiers consistently outperform baselines on long-form responses, including
more expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for
Llama-3.3-70B), and are also an improvement in short-form question-answering
settings. Moreover, despite being trained only with entity-level labels, our
probes effectively detect incorrect answers in mathematical reasoning tasks,
indicating generalization beyond entities. While our annotation methodology is
expensive, we find that annotated responses from one model can be used to train
effective classifiers on other models; accordingly, we publicly release our
datasets to facilitate reuse. Overall, our work suggests a promising new
approach for scalable, real-world hallucination detection.

</details>


### [8] [Topic Identification in LLM Input-Output Pairs through the Lens of Information Bottleneck](https://arxiv.org/abs/2509.03533)
*Igor Halperin*

Main category: cs.CL

TL;DR: 针对LLM幻觉问题，本文提出UDIB算法。该算法基于确定性信息瓶颈（DIB），通过高效聚类生成更具信息量的提示-响应共享主题表示，从而提升现有语义散度度量（SDM）框架的幻觉检测能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）容易产生内在忠实性幻觉，即其响应在语义上偏离提供的上下文。现有检测框架（如语义散度度量SDM）通过几何聚类识别潜在主题，但其优化目标是空间接近度而非信息论分析，导致检测效果受限，因此需要一种更原则、信息量更大的主题识别方法。

Method: 本文开发了一种基于确定性信息瓶颈（DIB）的原则性主题识别方法，用于几何聚类。通过将DIB中难以处理的KL散度项替换为计算效率高的上界，将其转化为适用于高维数据的实用算法UDIB。UDIB可被视为K-means的熵正则化和鲁棒化版本，其特点是能产生少量但信息丰富的聚类。该方法应用于LLM提示和响应嵌入的联合聚类，以生成共享主题表示。

Result: UDIB方法生成了一种关于提示-响应关系具有最大信息量的共享主题表示，不仅在空间上连贯，而且在信息论上更具结构。这为SDM框架提供了更优越的基础，并提供了一种新颖、更敏感的工具来检测LLM的幻觉。

Conclusion: UDIB方法通过提供更具信息量的共享主题表示，显著改善了LLM幻觉检测的现有框架（如SDM），为检测LLM内在不忠实性提供了一个更强大和敏感的工具。

Abstract: Large Language Models (LLMs) are prone to critical failure modes, including
\textit{intrinsic faithfulness hallucinations} (also known as confabulations),
where a response deviates semantically from the provided context. Frameworks
designed to detect this, such as Semantic Divergence Metrics (SDM), rely on
identifying latent topics shared between prompts and responses, typically by
applying geometric clustering to their sentence embeddings. This creates a
disconnect, as the topics are optimized for spatial proximity, not for the
downstream information-theoretic analysis. In this paper, we bridge this gap by
developing a principled topic identification method grounded in the
Deterministic Information Bottleneck (DIB) for geometric clustering. Our key
contribution is to transform the DIB method into a practical algorithm for
high-dimensional data by substituting its intractable KL divergence term with a
computationally efficient upper bound. The resulting method, which we dub UDIB,
can be interpreted as an entropy-regularized and robustified version of K-means
that inherently favors a parsimonious number of informative clusters. By
applying UDIB to the joint clustering of LLM prompt and response embeddings, we
generate a shared topic representation that is not merely spatially coherent
but is fundamentally structured to be maximally informative about the
prompt-response relationship. This provides a superior foundation for the SDM
framework and offers a novel, more sensitive tool for detecting confabulations.

</details>


### [9] [QuesGenie: Intelligent Multimodal Question Generation](https://arxiv.org/abs/2509.03535)
*Ahmed Mubarak,Amna Ahmed,Amira Nasser,Aya Mohamed,Fares El-Sadek,Mohammed Ahmed,Ahmed Salah,Youssef Sobhy*

Main category: cs.CL

TL;DR: 该项目开发了一个多模态问题生成系统，能够从不同内容格式自动生成多样化问题，并利用人类反馈强化学习（RLHF）实现自动化、可扩展且智能的练习题生成，以解决当前学习资源丰富但缺乏定制化练习材料的挑战。


<details>
  <summary>Details</summary>
Motivation: 在信息丰富的时代，学习者虽能接触到大量教育资源，但缺乏针对这些资源量身定制的练习材料，这是一个重大挑战。

Method: 开发了一个多模态问题生成系统，包含四个主要组件：多模态输入处理、问题生成、基于人类反馈的强化学习（RLHF）以及一个端到端交互界面。该系统能够从各种内容格式自动生成多样化的问题类型。

Result: 该系统能自动生成多样化问题，并平衡了资源效率、功能健壮性和流畅的用户体验，实现了自动化、可扩展和智能的问题生成。

Conclusion: 该项目为自动化、可扩展和智能的问题生成奠定了基础。

Abstract: In today's information-rich era, learners have access to abundant educational
resources, but the lack of practice materials tailored to these resources
presents a significant challenge. This project addresses that gap by developing
a multi-modal question generation system that can automatically generate
diverse question types from various content formats. The system features four
major components: multi-modal input handling, question generation,
reinforcement learning from human feedback (RLHF), and an end-to-end
interactive interface. This project lays the foundation for automated,
scalable, and intelligent question generation, carefully balancing resource
efficiency, robust functionality and a smooth user experience.

</details>


### [10] [AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in Large Language Models](https://arxiv.org/abs/2509.03537)
*Cheng-Kai Yeh,Hsing-Wang Lee,Chung-Hung Kuo,Hen-Hsen Huang*

Main category: cs.CL

TL;DR: AR$^2$通过对抗性强化学习提升大型语言模型（LLM）的抽象推理能力，显著提高了其在复杂编程任务上的泛化性。


<details>
  <summary>Details</summary>
Motivation: 抽象能力是计算机科学基础，对LLM至关重要。现有LLM代码生成训练多侧重表面模式识别，忽视了对抽象能力的显式训练。

Method: 提出AR$^2$（Adversarial Reinforcement Learning for Abstract Reasoning）框架。教师模型将核心问题转化为复杂叙述，学生编码模型则学习从这些复杂问题中提取底层计算核心，以增强抽象能力。

Result: 实验结果表明，AR$^2$显著提高了学生模型在先前未见、具有挑战性的编程任务上的准确性。

Conclusion: 抽象能力是增强LLM泛化性的关键技能，本研究强调了对其进行显式训练的重要性。

Abstract: Abstraction--the ability to recognize and distill essential computational
patterns from complex problem statements--is a foundational skill in computer
science, critical both for human problem-solvers and coding-oriented large
language models (LLMs). Despite recent advances in training LLMs for code
generation using reinforcement learning (RL), most existing approaches focus
primarily on superficial pattern recognition, overlooking explicit training for
abstraction. In this study, we propose AR$^2$ (Adversarial Reinforcement
Learning for Abstract Reasoning), a novel framework explicitly designed to
enhance the abstraction abilities of LLMs. AR$^2$ employs a teacher model to
transform kernel problems into narrative-rich, challenging descriptions without
changing their fundamental logic. Simultaneously, a student coding model is
trained to solve these complex narrative problems by extracting their
underlying computational kernels. Experimental results demonstrate that AR$^2$
substantially improves the student model's accuracy on previously unseen,
challenging programming tasks, underscoring abstraction as a key skill for
enhancing LLM generalization.

</details>


### [11] [Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction](https://arxiv.org/abs/2509.03540)
*Shanglin Wu,Lihui Liu,Jinho D. Choi,Kai Shu*

Main category: cs.CL

TL;DR: 本文提出一种在推理时动态构建和扩展知识图谱（KG）的框架，结合大语言模型（LLM）的内部知识和外部检索信息，以提高LLM的问答事实准确性、精确度和可解释性。


<details>
  <summary>Details</summary>
Motivation: LLMs在事实一致性方面存在挑战，现有RAG方法将知识视为非结构化文本，限制了组合推理和事实不一致性识别。因此需要一种更结构化的方法来增强LLM的事实性。

Method: 提出一种动态KG构建与扩展框架。首先通过提示从问题中提取初始KG，然后利用LLM的潜在知识进行迭代扩展，最后通过外部检索进行选择性细化，以提高事实覆盖率并纠正不准确信息。

Result: 在三个不同的事实QA基准测试中，该方法在事实准确性、答案精确性和可解释性方面均优于基线提示和静态KG增强方法。

Conclusion: 推理时知识图谱的动态构建是增强LLM事实性的一个有前景的方向，它以结构化、可解释和可扩展的方式提升了性能。

Abstract: Large Language Models (LLMs) often struggle with producing factually
consistent answers due to limitations in their parametric memory.
Retrieval-Augmented Generation (RAG) methods address this issue by
incorporating external knowledge from trusted sources at inference time.
However, such methods typically treat knowledge as unstructured text, which
limits their ability to support compositional reasoning and identify factual
inconsistencies. To overcome these limitations, we propose a novel framework
that dynamically constructs and expands knowledge graphs (KGs) during
inference, integrating both internal knowledge extracted from LLMs and external
information retrieved from external sources. Our method begins by extracting a
seed KG from the question via prompting, followed by iterative expansion using
the LLM's latent knowledge. The graph is then selectively refined through
external retrieval, enhancing factual coverage and correcting inaccuracies. We
evaluate our approach on three diverse factual QA benchmarks, demonstrating
consistent improvements in factual accuracy, answer precision, and
interpretability over baseline prompting and static KG-augmented methods. Our
findings suggest that inference-time KG construction is a promising direction
for enhancing LLM factuality in a structured, interpretable, and scalable
manner.

</details>


### [12] [ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference](https://arxiv.org/abs/2509.03565)
*Qi Chen,Jingxuan Wei,Zhuoya Yao,Haiguang Wang,Gaowei Wu,Bihui Yu,Siyuan Li,Cheng Tan*

Main category: cs.CL

TL;DR: 本文提出了一项新的任务——多文档科学推理，旨在通过提取和对齐相关论文的动机、方法和结果来重建研究发展链。为此，我们开发了ResearchPulse，一个基于Agent的框架，并创建了ResearchPulse-Bench基准，实验证明其性能优于GPT-4o等强基线模型。


<details>
  <summary>Details</summary>
Motivation: 理解科学思想的演变不仅仅需要总结单篇论文，更需要对主题相关的研究进行结构化、跨文档的推理。当前缺乏有效的方法来系统地提取和对齐多篇论文间的关键信息，以重建研究的发展过程。

Method: 本文首先形式化了多文档科学推理任务，该任务涉及跨相关论文提取并对齐动机、方法和实验结果。然后，提出了ResearchPulse，一个集成指令规划、科学内容提取和结构化可视化的基于Agent的框架。它包含三个协调Agent：Plan Agent负责任务分解，Mmap-Agent构建动机-方法思维导图，Lchart-Agent合成实验折线图。为支持此任务，本文还引入了ResearchPulse-Bench，一个包含引文信息的标注论文聚类基准。

Result: 实验结果表明，尽管ResearchPulse使用了7B规模的Agent，但在语义对齐、结构一致性和视觉保真度方面，其性能持续优于GPT-4o等强基线模型。数据集已在HuggingFace上提供。

Conclusion: ResearchPulse框架及ResearchPulse-Bench基准为多文档科学推理提供了一种有效且高效的解决方案，能够重建研究发展链，帮助人们更好地理解科学思想的演变，并且在性能上超越了现有先进模型。

Abstract: Understanding how scientific ideas evolve requires more than summarizing
individual papers-it demands structured, cross-document reasoning over
thematically related research. In this work, we formalize multi-document
scientific inference, a new task that extracts and aligns motivation,
methodology, and experimental results across related papers to reconstruct
research development chains. This task introduces key challenges, including
temporally aligning loosely structured methods and standardizing heterogeneous
experimental tables. We present ResearchPulse, an agent-based framework that
integrates instruction planning, scientific content extraction, and structured
visualization. It consists of three coordinated agents: a Plan Agent for task
decomposition, a Mmap-Agent that constructs motivation-method mind maps, and a
Lchart-Agent that synthesizes experimental line charts. To support this task,
we introduce ResearchPulse-Bench, a citation-aware benchmark of annotated paper
clusters. Experiments show that our system, despite using 7B-scale agents,
consistently outperforms strong baselines like GPT-4o in semantic alignment,
structural consistency, and visual fidelity. The dataset are available in
https://huggingface.co/datasets/ResearchPulse/ResearchPulse-Bench.

</details>


### [13] [NoteBar: An AI-Assisted Note-Taking System for Personal Knowledge Management](https://arxiv.org/abs/2509.03610)
*Josh Wisoff,Yao Tang,Zhengyu Fang,Jordan Guzman,YuTang Wang,Alex Yu*

Main category: cs.CL

TL;DR: 本文提出了NoteBar，一个AI辅助笔记工具，利用人格信息和高效语言模型自动分类笔记并优化工作流程。同时，构建了一个基于16种MBTI人格的大规模数据集，旨在为AI辅助个人知识管理奠定可扩展的基础。


<details>
  <summary>Details</summary>
Motivation: 笔记记录在学术和专业领域至关重要，但现有AI辅助工具在效率上仍有不足，未能有效组织笔记并支持用户工作流程。

Method: 开发了AI辅助笔记工具NoteBar，该工具利用人格信息和高效语言模型自动将笔记分类。此外，还构建了一个包含3,173条笔记和8,494个概念注释、涵盖16种MBTI人格的新型个性化数据集，以支持相关研究和评估。

Result: NoteBar能够实现笔记的自动组织，并能有效支持用户工作流程。该工具可实现实用且经济高效的部署，支持交互式使用，无需依赖重型基础设施。伴随发布的数据集为下游任务提供了多样性和语义丰富性。

Conclusion: NoteBar及其配套数据集为推进AI辅助个人知识管理提供了一个可扩展和可扩展的基础。

Abstract: Note-taking is a critical practice for capturing, organizing, and reflecting
on information in both academic and professional settings. The recent success
of large language models has accelerated the development of AI-assisted tools,
yet existing solutions often struggle with efficiency. We present NoteBar, an
AI-assisted note-taking tool that leverages persona information and efficient
language models to automatically organize notes into multiple categories and
better support user workflows. To support research and evaluation in this
space, we further introduce a novel persona-conditioned dataset of 3,173 notes
and 8,494 annotated concepts across 16 MBTI personas, offering both diversity
and semantic richness for downstream tasks. Finally, we demonstrate that
NoteBar can be deployed in a practical and cost-effective manner, enabling
interactive use without reliance on heavy infrastructure. Together, NoteBar and
its accompanying dataset provide a scalable and extensible foundation for
advancing AI-assisted personal knowledge management.

</details>


### [14] [E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition](https://arxiv.org/abs/2509.03615)
*Aryan Gupta,Anupam Purwar*

Main category: cs.CL

TL;DR: 针对多语言、噪声图像的OCR挑战，本文提出并评估了为边缘部署优化的Sprinklr-Edge-OCR系统。与LVLM及传统OCR系统对比发现，在边缘部署场景下，传统OCR系统在效率和成本方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 多语言、噪声及多样化真实世界图像中的光学字符识别（OCR）仍然是一个重大挑战。随着大型视觉语言模型（LVLMs）的兴起，人们对其泛化和推理能力越来越感兴趣，但资源受限的边缘部署对OCR系统提出了新的要求。

Method: 本文提出了专门为资源受限的边缘部署优化的Sprinklr-Edge-OCR OCR系统。在一个包含54种语言、经过双重人工标注的私有数据集上，对五种最先进的LVLM（InternVL, Qwen, GOT OCR, LLaMA, MiniCPM）和两种传统OCR系统（Sprinklr-Edge-OCR, SuryaOCR）进行了大规模比较评估。评估指标包括准确性、语义一致性、语言覆盖率、计算效率（延迟、内存、GPU使用）和部署成本。此外，还进行了CPU环境下模型性能的边缘部署分析。

Result: 评估结果显示，Qwen模型取得了最高精度（0.54），而Sprinklr-Edge-OCR获得了最佳的综合F1分数（0.46）。在效率方面，Sprinklr-Edge-OCR显著优于LVLM，图像处理速度快35倍（平均每张图片0.17秒），且成本不到LVLM的0.01（每1000张图片0.006美元）。

Conclusion: 研究结果表明，即使在大型语言模型时代，由于传统OCR系统低计算需求、低延迟和极高的经济性，它们仍然是边缘部署最理想的OCR系统。

Abstract: Optical Character Recognition (OCR) in multilingual, noisy, and diverse
real-world images remains a significant challenge for optical character
recognition systems. With the rise of Large Vision-Language Models (LVLMs),
there is growing interest in their ability to generalize and reason beyond
fixed OCR pipelines. In this work, we introduce Sprinklr-Edge-OCR, a novel OCR
system built specifically optimized for edge deployment in resource-constrained
environments. We present a large-scale comparative evaluation of five
state-of-the-art LVLMs (InternVL, Qwen, GOT OCR, LLaMA, MiniCPM) and two
traditional OCR systems (Sprinklr-Edge-OCR, SuryaOCR) on a proprietary, doubly
hand annotated dataset of multilingual (54 languages) images. Our benchmark
covers a broad range of metrics including accuracy, semantic consistency,
language coverage, computational efficiency (latency, memory, GPU usage), and
deployment cost. To better reflect real-world applicability, we also conducted
edge case deployment analysis, evaluating model performance on CPU only
environments. Among the results, Qwen achieved the highest precision (0.54),
while Sprinklr-Edge-OCR delivered the best overall F1 score (0.46) and
outperformed others in efficiency, processing images 35 faster (0.17 seconds
per image on average) and at less than 0.01 of the cost (0.006 USD per 1,000
images) compared to LVLM. Our findings demonstrate that the most optimal OCR
systems for edge deployment are the traditional ones even in the era of LLMs
due to their low compute requirements, low latency, and very high
affordability.

</details>


### [15] [Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators](https://arxiv.org/abs/2509.03647)
*Dani Roytburg,Matthew Bozoukov,Matthew Nguyen,Jou Barzdukas,Simon Fu,Narmeen Oozeer*

Main category: cs.CL

TL;DR: 本文研究轻量级转向向量能否在推理时缓解大型语言模型（LLMs）作为评估器时的“自我偏好偏差”，发现它们能显著减少不合理的偏见，但在合理偏见上表现不稳定，表明该问题复杂性高，需更强干预。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为自动化评估器存在“自我偏好偏差”，即倾向于偏爱自身输出。这种偏差损害了评估的公平性和可靠性，特别是在偏好调优和模型路由等任务中，因此需要一种方法来缓解此问题。

Method: 研究采用轻量级转向向量在推理时进行干预，无需重新训练。构建了一个区分合理与不合理自我偏好偏差的定制数据集。使用对比激活添加（CAA）和一种基于优化的方法来构建转向向量。

Result: 转向向量将不合理的自我偏好偏差降低了高达97%，显著优于提示词和直接偏好优化基线。然而，转向向量在合法自我偏好和无偏好一致性方面表现不稳定，这表明自我偏好可能涉及多个或非线性方向。

Conclusion: 转向向量在缓解LLM作为评估器时不合理的自我偏好偏差方面具有潜力，但其在合法偏好上的不稳定性也揭示了其局限性，并呼吁开发更稳健的干预措施来解决这一问题。

Abstract: Large language models (LLMs) increasingly serve as automated evaluators, yet
they suffer from "self-preference bias": a tendency to favor their own outputs
over those of other models. This bias undermines fairness and reliability in
evaluation pipelines, particularly for tasks like preference tuning and model
routing. We investigate whether lightweight steering vectors can mitigate this
problem at inference time without retraining. We introduce a curated dataset
that distinguishes self-preference bias into justified examples of
self-preference and unjustified examples of self-preference, and we construct
steering vectors using two methods: Contrastive Activation Addition (CAA) and
an optimization-based approach. Our results show that steering vectors can
reduce unjustified self-preference bias by up to 97\%, substantially
outperforming prompting and direct preference optimization baselines. Yet
steering vectors are unstable on legitimate self-preference and unbiased
agreement, implying self-preference spans multiple or nonlinear directions.
This underscores both their promise and limits as safeguards for LLM-as-judges
and motivates more robust interventions.

</details>


### [16] [Semantic Analysis of SNOMED CT Concept Co-occurrences in Clinical Documentation using MIMIC-IV](https://arxiv.org/abs/2509.03662)
*Ali Noori,Somya Mohanty,Prashanti Manda*

Main category: cs.CL

TL;DR: 本研究利用MIMIC-IV数据库，结合SNOMED CT概念共现模式和嵌入式语义相似性，探讨临床概念间的关系，发现语义嵌入能捕捉有意义的关联，有助于完善文档、发现潜在关系并支持临床决策和表型分析。


<details>
  <summary>Details</summary>
Motivation: 临床记录虽然信息丰富，但其非结构化格式给大规模分析带来挑战。尽管标准化术语（如SNOMED CT）提升了互操作性，但如何通过概念共现和语义相似性理解概念间的关系仍未被充分探索。

Method: 本研究使用MIMIC-IV数据库，调查SNOMED CT概念共现模式与基于嵌入的语义相似性之间的关系。采用归一化逐点互信息（NPMI）衡量共现，并使用预训练嵌入模型（如ClinicalBERT、BioBERT）计算语义相似性。分析内容包括：共现概念与语义距离的关系、嵌入模型对缺失概念的建议能力、这些关系在时间和专业领域上的演变、概念嵌入聚类，以及共现模式与死亡率、再入院等结果的关联。

Result: 分析显示，共现和语义相似性之间关联性较弱，但嵌入模型能捕获文档频率未完全反映的临床有意义关联。基于嵌入的建议频繁匹配了后续记录的概念，支持其增强临床注释的效用。概念嵌入的聚类产生了连贯的临床主题（如症状、实验室检查、诊断、心血管疾病），这些主题可映射到患者表型和护理模式。此外，与死亡率和再入院等结果相关的共现模式证明了该方法的实用性。

Conclusion: 本研究结果强调了共现统计和语义嵌入在提高文档完整性、发现潜在临床关系以及为决策支持和表型应用提供信息方面的互补价值。

Abstract: Clinical notes contain rich clinical narratives but their unstructured format
poses challenges for large-scale analysis. Standardized terminologies such as
SNOMED CT improve interoperability, yet understanding how concepts relate
through co-occurrence and semantic similarity remains underexplored. In this
study, we leverage the MIMIC-IV database to investigate the relationship
between SNOMED CT concept co-occurrence patterns and embedding-based semantic
similarity. Using Normalized Pointwise Mutual Information (NPMI) and pretrained
embeddings (e.g., ClinicalBERT, BioBERT), we examine whether frequently
co-occurring concepts are also semantically close, whether embeddings can
suggest missing concepts, and how these relationships evolve temporally and
across specialties. Our analyses reveal that while co-occurrence and semantic
similarity are weakly correlated, embeddings capture clinically meaningful
associations not always reflected in documentation frequency. Embedding-based
suggestions frequently matched concepts later documented, supporting their
utility for augmenting clinical annotations. Clustering of concept embeddings
yielded coherent clinical themes (symptoms, labs, diagnoses, cardiovascular
conditions) that map to patient phenotypes and care patterns. Finally,
co-occurrence patterns linked to outcomes such as mortality and readmission
demonstrate the practical utility of this approach. Collectively, our findings
highlight the complementary value of co-occurrence statistics and semantic
embeddings in improving documentation completeness, uncovering latent clinical
relationships, and informing decision support and phenotyping applications.

</details>


### [17] [MLSD: A Novel Few-Shot Learning Approach to Enhance Cross-Target and Cross-Domain Stance Detection](https://arxiv.org/abs/2509.03725)
*Parush Gera,Tempestt Neal*

Main category: cs.CL

TL;DR: 提出了一种基于度量学习的少样本学习方法MLSD，用于跨领域和跨目标的立场检测，并在多个场景下显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决跨领域和跨目标立场检测的挑战，即在缺少目标领域/目标数据时，如何有效地进行立场检测。

Method: 开发了MLSD（Metric Learning-Based Few-Shot Learning for Cross-Target and Cross-Domain Stance Detection），该方法利用度量学习和三重态损失（triplet loss）来捕捉立场目标之间的语义相似性和差异性，构建一个判别性嵌入空间，从而实现领域适应性并允许模型从新目标领域获取有用示例。

Result: 在两个数据集上，跨多个跨目标和跨领域场景对MLSD进行了评估，结果显示，与六种广泛使用的立场检测模型相比，MLSD在立场检测性能上实现了统计学上的显著提升。

Conclusion: MLSD是一种有效的方法，能够显著提高跨领域和跨目标立场检测的性能，为处理新目标领域的数据提供了解决方案。

Abstract: We present the novel approach for stance detection across domains and
targets, Metric Learning-Based Few-Shot Learning for Cross-Target and
Cross-Domain Stance Detection (MLSD). MLSD utilizes metric learning with
triplet loss to capture semantic similarities and differences between stance
targets, enhancing domain adaptation. By constructing a discriminative
embedding space, MLSD allows a cross-target or cross-domain stance detection
model to acquire useful examples from new target domains. We evaluate MLSD in
multiple cross-target and cross-domain scenarios across two datasets, showing
statistically significant improvement in stance detection performance across
six widely used stance detection models.

</details>


### [18] [SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation Evaluation](https://arxiv.org/abs/2509.03791)
*Saki Imai,Mert İnan,Anthony Sicilia,Malihe Alikhani*

Main category: cs.CL

TL;DR: 本文提出SiLVERScore，一种基于嵌入的语义感知评估指标，用于手语生成，解决了传统回译评估方法无法捕捉多模态特征和引入歧义的问题，并在两个数据集上表现出卓越的鉴别能力。


<details>
  <summary>Details</summary>
Motivation: 现有手语生成评估常通过回译进行，但这种两步评估流程存在歧义，无法捕捉手语的多模态特性（如面部表情、空间语法、韵律），且难以判断评估错误是源于生成模型还是翻译系统。因此，需要更精确、能捕获多模态信息的评估方法。

Method: 本文提出SiLVERScore，这是一种新颖的、基于嵌入的语义感知评估指标，在共享嵌入空间中评估手语生成。其方法包括：识别现有指标的局限性、引入SiLVERScore进行语义感知评估、展示其对语义和韵律变化的鲁棒性，并探索其在不同数据集上的泛化挑战。

Result: 在PHOENIX-14T和CSL-Daily数据集上，SiLVERScore在正确和随机对之间实现了近乎完美的鉴别（ROC AUC = 0.99，重叠度 < 7%），显著优于传统指标。

Conclusion: SiLVERScore提供了一种更准确、更鲁棒的手语生成评估方法，克服了传统回译方法的局限性，有效捕捉了语义和多模态变化，并展示出卓越的鉴别能力。

Abstract: Evaluating sign language generation is often done through back-translation,
where generated signs are first recognized back to text and then compared to a
reference using text-based metrics. However, this two-step evaluation pipeline
introduces ambiguity: it not only fails to capture the multimodal nature of
sign language-such as facial expressions, spatial grammar, and prosody-but also
makes it hard to pinpoint whether evaluation errors come from sign generation
model or the translation system used to assess it. In this work, we propose
SiLVERScore, a novel semantically-aware embedding-based evaluation metric that
assesses sign language generation in a joint embedding space. Our contributions
include: (1) identifying limitations of existing metrics, (2) introducing
SiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness
to semantic and prosodic variations, and (4) exploring generalization
challenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore
achieves near-perfect discrimination between correct and random pairs (ROC AUC
= 0.99, overlap < 7%), substantially outperforming traditional metrics.

</details>


### [19] [Measuring How (Not Just Whether) VLMs Build Common Ground](https://arxiv.org/abs/2509.03805)
*Saki Imai,Mert İnan,Anthony Sicilia,Malihe Alikhani*

Main category: cs.CL

TL;DR: 大型视觉语言模型（VLM）宣称具有推理能力，但现有基准测试缺乏交互式语境评估。本文提出了一套四指标评估体系，用于衡量VLM在交互式语境下的理解能力，发现模型与人类行为存在显著差异，即使任务成功也未必代表真正理解。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型（VLM）评估基准主要集中在单轮对话或问答任务，未能充分衡量其在交互式“语境理解”（grounding）过程中的表现，而语境理解是人类通过持续沟通逐步建立共享理解的关键。因此，需要一套新的方法来系统评估VLM在这种交互式环境下的能力。

Method: 研究引入了一套包含语境理解效率、内容对齐、词汇适应性和人类相似度四个指标的评估体系。将这套体系应用于三个专有大型视觉语言模型的150个“自我对弈”交互式指称游戏会话中，并将其表现与人类配对的表现进行比较。

Result: 结果显示，所有三个模型在至少三个评估指标上都与人类行为模式存在差异，其中GPT4o-mini在整体上与人类最为接近。研究发现，任务成功得分并不一定代表成功的语境理解，且高图像-话语对齐度也并非任务成功的必然预测因素。

Conclusion: 本文提出的评估体系和研究发现为未来VLM语境理解的研究提供了新框架。目前的VLM在交互式语境理解方面仍与人类存在差距，且任务成功不等于真正的理解。

Abstract: Large vision language models (VLMs) increasingly claim reasoning skills, yet
current benchmarks evaluate them in single-turn or question answering settings.
However, grounding is an interactive process in which people gradually develop
shared understanding through ongoing communication. We introduce a four-metric
suite (grounding efficiency, content alignment, lexical adaptation, and
human-likeness) to systematically evaluate VLM performance in interactive
grounding contexts. We deploy the suite on 150 self-play sessions of
interactive referential games between three proprietary VLMs and compare them
with human dyads. All three models diverge from human patterns on at least
three metrics, while GPT4o-mini is the closest overall. We find that (i) task
success scores do not indicate successful grounding and (ii) high
image-utterance alignment does not necessarily predict task success. Our metric
suite and findings offer a framework for future research on VLM grounding.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [20] [Towards Efficient General Feature Prediction in Masked Skeleton Modeling](https://arxiv.org/abs/2509.03609)
*Shengkai Sun,Zefan Zhang,Jianfeng Dong,Zhiyong Cheng,Xiaojun Chang,Meng Wang*

Main category: cs.CV

TL;DR: 针对骨架行为识别中MAE范式低级特征重建的局限性，本文提出通用特征预测框架(GFP)。GFP通过高层特征预测和协作学习实现动态监督与约束优化，显著提升训练效率，并达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于MAE的骨架行为识别方法将重建目标限制在原始关节坐标等低级特征上，导致计算冗余和语义表示能力有限。

Method: 提出了一种新颖的通用特征预测框架(GFP)。其核心创新在于用高层特征预测（涵盖从局部运动模式到全局语义表示）替代传统的低级重建。该框架引入了协作学习机制，其中一个轻量级目标生成网络动态产生跨时空层次的多样化监督信号，并结合约束优化以确保特征多样性并防止模型崩溃。

Result: 实验结果显示，GFP框架在计算效率上表现出色（训练速度比标准掩码骨架建模方法快6.2倍），并提供了卓越的表示质量，在NTU RGB+D 60、NTU RGB+D 120和PKU-MMD等数据集的各种下游任务中均达到了最先进的性能。

Conclusion: GFP框架通过高层特征预测有效解决了传统MAE范式在骨架行为识别中低级特征重建的局限性，显著提高了训练效率和表示质量，实现了最先进的自监督骨架行为识别效果。

Abstract: Recent advances in the masked autoencoder (MAE) paradigm have significantly
propelled self-supervised skeleton-based action recognition. However, most
existing approaches limit reconstruction targets to raw joint coordinates or
their simple variants, resulting in computational redundancy and limited
semantic representation. To address this, we propose a novel General Feature
Prediction framework (GFP) for efficient mask skeleton modeling. Our key
innovation is replacing conventional low-level reconstruction with high-level
feature prediction that spans from local motion patterns to global semantic
representations. Specifically, we introduce a collaborative learning framework
where a lightweight target generation network dynamically produces diversified
supervision signals across spatial-temporal hierarchies, avoiding reliance on
pre-computed offline features. The framework incorporates constrained
optimization to ensure feature diversity while preventing model collapse.
Experiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits
of our approach: Computational efficiency (with 6.2$\times$ faster training
than standard masked skeleton modeling methods) and superior representation
quality, achieving state-of-the-art performance in various downstream tasks.

</details>


### [21] [Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge](https://arxiv.org/abs/2509.03614)
*Seungho Choe,Xiaoli Qin,Abubakr Shafique,Amanda Dy,Susan Done,Dimitrios Androutsos,April Khademi*

Main category: cs.CV

TL;DR: 本文针对有丝分裂计数中AI面临的域偏移和数据不平衡问题，提出一个基于UNet、域泛化模块和师生策略的统一框架，同时实现有丝分裂检测和非典型有丝分裂分类，并取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 病理学家手动计数有丝分裂耗时且存在观察者间差异。AI虽能自动化，但易受域偏移（形态、物种、染色差异）影响性能下降，且有丝分裂数据远少于正常细胞核，导致严重的数据不平衡。

Method: 将有丝分裂检测建模为像素级分割任务，并提出一个师生模型。该模型以UNet为分割骨架，集成了对比表示学习和域对抗训练等域泛化模块。通过师生策略生成像素级伪掩膜，覆盖带注释的有丝分裂、难负样本和正常细胞核，以增强特征判别和域偏移鲁棒性。分类任务则引入多尺度CNN分类器，利用分割模型的特征图。

Result: 在初步测试集上，Track 1（有丝分裂检测）的F1分数达到0.7660，Track 2（非典型有丝分裂分类）的平衡准确率达到0.8414。

Conclusion: 研究结果表明，将基于分割的检测和分类整合到统一框架中，能有效实现鲁棒性的有丝分裂分析。

Abstract: Counting mitotic figures is time-intensive for pathologists and leads to
inter-observer variability. Artificial intelligence (AI) promises a solution by
automatically detecting mitotic figures while maintaining decision consistency.
However, AI tools are susceptible to domain shift, where a significant drop in
performance can occur due to differences in the training and testing sets,
including morphological diversity between organs, species, and variations in
staining protocols. Furthermore, the number of mitoses is much less than the
count of normal nuclei, which introduces severely imbalanced data for the
detection task. In this work, we formulate mitosis detection as a pixel-level
segmentation and propose a teacher-student model that simultaneously addresses
mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our
method is based on a UNet segmentation backbone that integrates domain
generalization modules, namely contrastive representation learning and
domain-adversarial training. A teacher-student strategy is employed to generate
pixel-level pseudo-masks not only for annotated mitoses and hard negatives but
also for normal nuclei, thereby enhancing feature discrimination and improving
robustness against domain shift. For the classification task, we introduce a
multi-scale CNN classifier that leverages feature maps from the segmentation
model within a multi-task learning paradigm. On the preliminary test set, the
algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of
0.8414 in Track 2, demonstrating the effectiveness of integrating
segmentation-based detection and classification into a unified framework for
robust mitosis analysis.

</details>


### [22] [Multi Attribute Bias Mitigation via Representation Learning](https://arxiv.org/abs/2509.03616)
*Rajeev Ranjan Dwivedi,Ankur Kumar,Vinod K Kurmi*

Main category: cs.CV

TL;DR: 本文提出广义多偏差缓解 (GMBM) 框架，通过两阶段方法解决视觉模型中多个重叠偏差问题，提升模型鲁棒性和公平性，并引入新的偏差度量SBA。


<details>
  <summary>Details</summary>
Motivation: 现实世界图像中普遍存在的多个重叠偏差（如纹理、水印、性别化妆等）严重损害了现代视觉模型的性能、鲁棒性和公平性。单独解决这些偏差往往效果不佳或会加剧其他偏差。

Method: 本文提出GMBM，一个仅需分组标签、在测试时最小化偏差的两阶段框架。第一阶段，自适应偏差集成学习 (ABIL) 通过训练每个属性的编码器并与主干网络集成，促使分类器显式识别已知快捷方式的影响。第二阶段，梯度抑制微调从主干网络的梯度中剪除这些偏差方向，形成一个忽略快捷方式的紧凑网络。此外，针对现有偏差度量在子组不平衡和训练-测试分布偏移下失效的问题，本文引入了可伸缩偏差放大 (SBA) 作为新的测试时度量。

Result: GMBM在FB CMNIST、CelebA和COCO数据集上进行了验证，结果表明它显著提升了最差组准确率，将多属性偏差放大减半，并在偏差复杂性和分布偏移加剧的情况下，SBA指标达到了新的低点。

Conclusion: GMBM是视觉识别领域首个实用且端到端的多偏差解决方案。

Abstract: Real world images frequently exhibit multiple overlapping biases, including
textures, watermarks, gendered makeup, scene object pairings, etc. These biases
collectively impair the performance of modern vision models, undermining both
their robustness and fairness. Addressing these biases individually proves
inadequate, as mitigating one bias often permits or intensifies others. We
tackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), a
lean two stage framework that needs group labels only while training and
minimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL)
deliberately identifies the influence of known shortcuts by training encoders
for each attribute and integrating them with the main backbone, compelling the
classifier to explicitly recognize these biases. Then Gradient Suppression Fine
Tuning prunes those very bias directions from the backbone's gradients, leaving
a single compact network that ignores all the shortcuts it just learned to
recognize. Moreover we find that existing bias metrics break under subgroup
imbalance and train test distribution shifts, so we introduce Scaled Bias
Amplification (SBA): a test time measure that disentangles model induced bias
amplification from distributional differences. We validate GMBM on FB CMNIST,
CelebA, and COCO, where we boost worst group accuracy, halve multi attribute
bias amplification, and set a new low in SBA even as bias complexity and
distribution shifts intensify, making GMBM the first practical, end to end
multibias solution for visual recognition. Project page:
http://visdomlab.github.io/GMBM/

</details>


### [23] [Lightweight image segmentation for echocardiography](https://arxiv.org/abs/2509.03631)
*Anders Kjelsrud,Lasse Løvstakken,Erik Smistad,Håvard Dalen,Gilles Van De Vyver*

Main category: cs.CV

TL;DR: 本文通过消融研究识别了nnU-Net在心脏分割中的有效组件，并基于此开发了一种轻量级U-Net模型，其在保持与nnU-Net相当的分割性能的同时，显著减小了模型大小并提升了运行速度，实现了实时应用。


<details>
  <summary>Details</summary>
Motivation: 心动超声中左心室的精确分割对自动提取临床测量指标至关重要。虽然nnU-Net模型表现良好，但其体积大、速度慢，限制了实时使用。

Method: 通过消融研究，逐步评估了nnU-Net的数据增强方案、架构修改、损失函数和后处理技术，以确定心脏分割的最有效组件。基于这些发现，开发了一种轻量级U-Net模型。

Result: 分析表明，简单的仿射增强和深度监督对性能提升至关重要，而复杂的增强和大的模型容量则回报递减。所开发的轻量级U-Net（2M参数 vs nnU-Net的33M参数）在CAMUS数据集上取得了与nnU-Net统计学等效的性能（LV/MYO/LA的Dice分数分别为0.93/0.85/0.89 vs 0.93/0.86/0.89，$p>0.05$），同时模型大小缩小了16倍，速度加快了4倍（每帧1.35ms vs 5.40ms）。跨数据集评估也证实了其可比较的泛化能力。

Conclusion: 通过优化nnU-Net的关键组件，我们成功开发了一个轻量级U-Net模型，该模型在不牺牲心脏分割精度的情况下，显著提升了处理速度和减小了模型体积，从而为心动超声的实时临床应用提供了可行方案。

Abstract: Accurate segmentation of the left ventricle in echocardiography can enable
fully automatic extraction of clinical measurements such as volumes and
ejection fraction. While models configured by nnU-Net perform well, they are
large and slow, thus limiting real-time use. We identified the most effective
components of nnU-Net for cardiac segmentation through an ablation study,
incrementally evaluating data augmentation schemes, architectural
modifications, loss functions, and post-processing techniques. Our analysis
revealed that simple affine augmentations and deep supervision drive
performance, while complex augmentations and large model capacity offer
diminishing returns. Based on these insights, we developed a lightweight U-Net
(2M vs 33M parameters) that achieves statistically equivalent performance to
nnU-Net on CAMUS (N=500) with Dice scores of 0.93/0.85/0.89 vs 0.93/0.86/0.89
for LV/MYO/LA ($p>0.05$), while being 16 times smaller and 4 times faster
(1.35ms vs 5.40ms per frame) than the default nnU-Net configuration.
Cross-dataset evaluation on an internal dataset (N=311) confirms comparable
generalization.

</details>


### [24] [treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds](https://arxiv.org/abs/2509.03633)
*Josafat-Mattias Burmeister,Andreas Tockner,Stefan Reder,Markus Engel,Rico Richter,Jan-Peter Mund,Jürgen Döllner*

Main category: cs.CV

TL;DR: 本研究改进了无监督的treeX算法，实现了激光雷达点云中个体树木的高效分割，相比原算法显著提升了精度和运行效率，并在地面扫描数据上达到了与深度学习方法相当的准确性，且资源消耗更低。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的树木实例分割方法需要大量标注数据和计算资源。因此，需要一种资源高效的替代方案来处理激光雷达点云数据并提取个体树木。

Method: 提出了treeX算法的修订版本，这是一种无监督方法，结合了基于聚类的树干检测和区域增长的树冠描绘。该方法为地面激光扫描（TLS和PLS）和无人机载激光扫描（ULS）提供了两个参数预设。

Result: 与原始treeX算法相比，修订版减少了运行时间并提高了准确性，地面数据实例检测的F1-score提高了+0.11到+0.49。对于ULS数据，我们的预设达到了0.58的F1-score，而原始算法未能分割出任何正确实例。对于TLS和PLS数据，本算法的准确性与包括深度学习在内的近期开源方法相当。

Conclusion: 本方法可作为深度学习方法在数据特性（充足的树干可见性和点密度）匹配时的资源高效替代方案，也可用于深度学习模型标签的半自动生成。为促进更广泛应用，提供了开源的Python实现。

Abstract: Close-range laser scanning provides detailed 3D captures of forest stands but
requires efficient software for processing 3D point cloud data and extracting
individual trees. Although recent studies have introduced deep learning methods
for tree instance segmentation, these approaches require large annotated
datasets and substantial computational resources. As a resource-efficient
alternative, we present a revised version of the treeX algorithm, an
unsupervised method that combines clustering-based stem detection with region
growing for crown delineation. While the original treeX algorithm was developed
for personal laser scanning (PLS) data, we provide two parameter presets, one
for ground-based laser scanning (stationary terrestrial - TLS and PLS), and one
for UAV-borne laser scanning (ULS). We evaluated the method on six public
datasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham
Woods) and compared it to six open-source methods (original treeX, treeiso,
RayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original
treeX algorithm, our revision reduces runtime and improves accuracy, with
instance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data.
For ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original
algorithm fails to segment any correct instances. For TLS and PLS data, our
algorithm achieves accuracy similar to recent open-source methods, including
deep learning. Given its algorithmic design, we see two main applications for
our method: (1) as a resource-efficient alternative to deep learning approaches
in scenarios where the data characteristics align with the method design
(sufficient stem visibility and point density), and (2) for the semi-automatic
generation of labels for deep learning models. To enable broader adoption, we
provide an open-source Python implementation in the pointtree package.

</details>


### [25] [Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding](https://arxiv.org/abs/2509.03635)
*Hongpei Zheng,Lintao Xiang,Qijun Yang,Qian Lin,Hujun Yin*

Main category: cs.CV

TL;DR: Reg3D框架通过引入几何感知监督和重建任务，显著提升了大型多模态模型（LMMs）在3D场景理解方面的能力，并建立了新的训练范式。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型在2D视觉理解方面取得进展，但在3D场景理解中仍面临挑战。当前方法主要依赖文本监督，缺乏学习鲁棒3D空间表示所需的几何约束。

Method: 提出Reg3D，一个重建几何指令微调框架。该方法采用双重监督范式，将3D几何信息同时作为输入和显式学习目标。通过设计互补的对象级和帧级重建任务，并在双编码器架构中强制执行几何一致性，以发展空间推理能力。

Result: 在ScanQA、Scan2Cap、ScanRefer和SQA3D等多个基准测试上进行的广泛实验表明，Reg3D带来了显著的性能提升。

Conclusion: Reg3D为空间感知多模态模型建立了一个新的训练范式，有效解决了LMMs在3D场景理解中缺乏几何约束的问题。

Abstract: The rapid development of Large Multimodal Models (LMMs) has led to remarkable
progress in 2D visual understanding; however, extending these capabilities to
3D scene understanding remains a significant challenge. Existing approaches
predominantly rely on text-only supervision, which fails to provide the
geometric constraints required for learning robust 3D spatial representations.
In this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction
Tuning framework that addresses this limitation by incorporating geometry-aware
supervision directly into the training process. Our key insight is that
effective 3D understanding necessitates reconstructing underlying geometric
structures rather than merely describing them. Unlike existing methods that
inject 3D information solely at the input level, Reg3D adopts a
dual-supervision paradigm that leverages 3D geometric information both as input
and as explicit learning targets. Specifically, we design complementary
object-level and frame-level reconstruction tasks within a dual-encoder
architecture, enforcing geometric consistency to encourage the development of
spatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,
ScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance
improvements, establishing a new training paradigm for spatially aware
multimodal models.

</details>


### [26] [QuantV2X: A Fully Quantized Multi-Agent System for Cooperative Perception](https://arxiv.org/abs/2509.03704)
*Seth Z. Zhao,Huizhi Zhang,Zhaowei Li,Juntong Peng,Anthony Chui,Zewei Zhou,Zonglin Meng,Hao Xiang,Zhiyu Huang,Fujia Wang,Ran Tian,Chenfeng Xu,Bolei Zhou,Jiaqi Ma*

Main category: cs.CV

TL;DR: QuantV2X是首个全量化多智能体系统，专为高效、可扩展的V2X协同感知设计，通过统一的端到端量化策略，显著降低计算和传输成本，同时保持精度并大幅提升系统级性能。


<details>
  <summary>Details</summary>
Motivation: V2X协同感知潜力巨大，但现有研究主要关注精度，忽视了效率、延迟和实际部署性。现有系统多依赖全精度模型，导致高昂的计算和传输成本，难以在资源受限环境中实现实时运行。

Method: 引入QuantV2X系统，这是首个全量化多智能体系统。它采用统一的端到端量化策略，涵盖神经网络模型和传输消息表示，旨在同时减少计算负载和传输带宽。

Result: QuantV2X在低比特约束下仍能达到与全精度系统相当的精度。在部署导向指标下，系统级延迟降低3.2倍，mAP30比全精度基线提升9.5。此外，QuantV2X扩展性更强，能在严格内存预算内运行更大、更强大的模型。

Conclusion: 研究结果表明，全量化多智能体中间融合系统在V2X协同感知的实际部署中具有可行性。该系统将公开发布以促进该领域的研究。

Abstract: Cooperative perception through Vehicle-to-Everything (V2X) communication
offers significant potential for enhancing vehicle perception by mitigating
occlusions and expanding the field of view. However, past research has
predominantly focused on improving accuracy metrics without addressing the
crucial system-level considerations of efficiency, latency, and real-world
deployability. Noticeably, most existing systems rely on full-precision models,
which incur high computational and transmission costs, making them impractical
for real-time operation in resource-constrained environments. In this paper, we
introduce \textbf{QuantV2X}, the first fully quantized multi-agent system
designed specifically for efficient and scalable deployment of multi-modal,
multi-agent V2X cooperative perception. QuantV2X introduces a unified
end-to-end quantization strategy across both neural network models and
transmitted message representations that simultaneously reduces computational
load and transmission bandwidth. Remarkably, despite operating under low-bit
constraints, QuantV2X achieves accuracy comparable to full-precision systems.
More importantly, when evaluated under deployment-oriented metrics, QuantV2X
reduces system-level latency by 3.2$\times$ and achieves a +9.5 improvement in
mAP30 over full-precision baselines. Furthermore, QuantV2X scales more
effectively, enabling larger and more capable models to fit within strict
memory budgets. These results highlight the viability of a fully quantized
multi-agent intermediate fusion system for real-world deployment. The system
will be publicly released to promote research in this field:
https://github.com/ucla-mobility/QuantV2X.

</details>


### [27] [Transfer Learning-Based CNN Models for Plant Species Identification Using Leaf Venation Patterns](https://arxiv.org/abs/2509.03729)
*Bandita Bharadwaj,Ankur Mishra,Saurav Bharadwaj*

Main category: cs.CV

TL;DR: 本研究评估了ResNet50、MobileNetV2和EfficientNetB0三种深度学习模型在基于叶脉模式的植物物种自动分类中的性能，EfficientNetB0表现最佳。


<details>
  <summary>Details</summary>
Motivation: 自动植物物种分类是一个重要挑战，而叶脉模式作为一种关键的形态特征，具有高度的分类学相关性。本研究旨在评估深度学习模型在此任务中的有效性。

Method: 研究评估了ResNet50、MobileNetV2和EfficientNetB0三种深度学习架构。使用了包含15种不同植物（每种75张图像，共1125张）的瑞典叶片数据集，以叶脉模式作为分类依据。模型性能在训练和测试阶段通过标准指标进行评估。

Result: ResNet50训练准确率为94.11%，但测试准确率为88.45%，F1分数为87.82%，显示出过拟合。MobileNetV2展示了更好的泛化能力，测试准确率为93.34%，F1分数为93.23%，适用于轻量级实时应用。EfficientNetB0表现最优，测试准确率达到94.67%，精确度、召回率和F1分数均超过94.6%。

Conclusion: 研究结果强调了深度学习，特别是EfficientNetB0，在利用叶脉特征开发可扩展、准确的自动植物分类工具方面的巨大潜力。

Abstract: This study evaluates the efficacy of three deep learning architectures:
ResNet50, MobileNetV2, and EfficientNetB0 for automated plant species
classification based on leaf venation patterns, a critical morphological
feature with high taxonomic relevance. Using the Swedish Leaf Dataset
comprising images from 15 distinct species (75 images per species, totalling
1,125 images), the models were demonstrated using standard performance metrics
during training and testing phases. ResNet50 achieved a training accuracy of
94.11% but exhibited overfitting, reflected by a reduced testing accuracy of
88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated better
generalization capabilities, attaining a testing accuracy of 93.34% and an F1
score of 93.23%, indicating its suitability for lightweight, real-time
applications. EfficientNetB0 outperformed both models, achieving a testing
accuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%,
highlighting its robustness in venation-based classification. The findings
underscore the potential of deep learning, particularly EfficientNetB0, in
developing scalable and accurate tools for automated plant taxonomy using
venation traits.

</details>


### [28] [LayoutGKN: Graph Similarity Learning of Floor Plans](https://arxiv.org/abs/2509.03737)
*Casper van Engelenburg,Jan van Gemert,Seyran Khademi*

Main category: cs.CV

TL;DR: LayoutGKN是一种更高效的楼层平面图图比较方法，通过延迟跨图节点级交互，实现了与现有图匹配网络相当或更优的相似性计算性能，同时显著提升了速度。


<details>
  <summary>Details</summary>
Motivation: 楼层平面图的图形比较在搜索、聚类和数据可视化等应用中至关重要。然而，现有最成功的图比较方法（如图匹配网络）依赖于耗时且昂贵的中间跨图节点级交互，导致推理时间缓慢。

Method: 引入了LayoutGKN方法，该方法将跨图节点级交互推迟到联合嵌入架构的末端。具体而言，它在最终学习到的节点级嵌入上使用可微分图核作为距离函数。

Result: LayoutGKN在计算相似性方面与图匹配网络相比表现相当或更好，同时显著提高了速度。

Conclusion: LayoutGKN提供了一种更高效且有效的楼层平面图图比较方法，解决了现有图匹配网络速度慢的问题。

Abstract: Floor plans depict building layouts and are often represented as graphs to
capture the underlying spatial relationships. Comparison of these graphs is
critical for applications like search, clustering, and data visualization. The
most successful methods to compare graphs \ie, graph matching networks, rely on
costly intermediate cross-graph node-level interactions, therefore being slow
in inference time. We introduce \textbf{LayoutGKN}, a more efficient approach
that postpones the cross-graph node-level interactions to the end of the joint
embedding architecture. We do so by using a differentiable graph kernel as a
distance function on the final learned node-level embeddings. We show that
LayoutGKN computes similarity comparably or better than graph matching networks
while significantly increasing the speed.
\href{https://github.com/caspervanengelenburg/LayoutGKN}{Code and data} are
open.

</details>


### [29] [Singular Value Few-shot Adaptation of Vision-Language Models](https://arxiv.org/abs/2509.03740)
*Taha Koleilat,Hassan Rivaz,Yiming Xiao*

Main category: cs.CV

TL;DR: CLIP-SVD是一种新颖的多模态、参数高效自适应技术，通过仅微调CLIP参数矩阵的奇异值，在细粒度领域适应上实现了领先的性能和泛化能力，且仅使用极少量参数。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）在细粒度领域适应方面面临挑战，现有方法（如提示工程、适配器模块）成本高昂、效果受限，且可能导致模型不稳定或损害预训练知识。

Method: 提出CLIP-SVD，一种多模态、参数高效的适应技术。它利用奇异值分解（SVD），仅微调CLIP参数矩阵的奇异值来重新缩放基向量，以实现领域适应，且不引入额外模块。同时，采用自然语言方法分析其有效性和动态性。

Result: 仅使用模型总参数的0.04%，显著提升了适应性能，并更好地保留了泛化能力。在11个自然数据集和10个生物医学数据集上，分类结果达到最先进水平，在少样本设置下优于现有方法。

Conclusion: CLIP-SVD提供了一种高效、精确且可解释的VLM细粒度领域适应方案，通过参数高效的奇异值微调，显著提升了模型性能和泛化能力，同时保护了预训练知识。

Abstract: Vision-language models (VLMs) like CLIP have shown impressive zero-shot and
few-shot learning capabilities across diverse applications. However, adapting
these models to new fine-grained domains remains difficult due to reliance on
prompt engineering and the high cost of full model fine-tuning. Existing
adaptation approaches rely on augmented components, such as prompt tokens and
adapter modules, which could limit adaptation quality, destabilize the model,
and compromise the rich knowledge learned during pretraining. In this work, we
present \textbf{CLIP-SVD}, a novel \textit{multi-modal} and
\textit{parameter-efficient} adaptation technique that leverages Singular Value
Decomposition (SVD) to modify the internal parameter space of CLIP without
injecting additional modules. Specifically, we fine-tune only the singular
values of the CLIP parameter matrices to rescale the basis vectors for domain
adaptation while retaining the pretrained model. This design enables enhanced
adaptation performance using only \textbf{0.04\%} of the model's total
parameters and better preservation of its generalization ability. CLIP-SVD
achieves state-of-the-art classification results on 11 natural and 10
biomedical datasets, outperforming previous methods in both accuracy and
generalization under few-shot settings. Additionally, we leverage a natural
language-based approach to analyze the effectiveness and dynamics of the CLIP
adaptation to allow interpretability of CLIP-SVD. The code is publicly
available at https://github.com/HealthX-Lab/CLIP-SVD.

</details>


### [30] [STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight Plant Disease Classification](https://arxiv.org/abs/2509.03754)
*Zongsen Qiu*

Main category: cs.CV

TL;DR: 本文提出STA-Net，一种结合训练无关NAS和Shape-Texture注意力模块的轻量级模型，用于在边缘设备上高效准确地诊断植物病害，解决了传统注意力机制对细微病理特征捕获不足的问题。


<details>
  <summary>Details</summary>
Motivation: 面对全球粮食安全需求，基于深度学习的植物病害诊断至关重要，但在边缘设备上部署高精度模型面临挑战。现有轻量级网络中的注意力机制主要为通用物体识别设计，难以有效捕获不规则病斑形状和复杂纹理等细微病理特征。

Method: 该研究采用双重解决方案：首先，使用训练无关的神经架构搜索方法(DeepMAD)构建高效的边缘设备网络骨干；其次，引入Shape-Texture注意力模块(STAM)，该模块将注意力分为两个分支——一个使用可变形卷积(DCNv4)关注形状，另一个使用Gabor滤波器组关注纹理。

Result: 在公共CCMT植物病害数据集上，STA-Net模型（401K参数和51.1M FLOPs）达到了89.00%的准确率和88.96%的F1分数。消融研究证实STAM显著优于基线和标准注意力模型，有效提升了性能。

Conclusion: 通过解耦注意力机制整合领域知识，为在边缘设备上部署的精准农业AI提供了有前景的路径。

Abstract: Responding to rising global food security needs, precision agriculture and
deep learning-based plant disease diagnosis have become crucial. Yet, deploying
high-precision models on edge devices is challenging. Most lightweight networks
use attention mechanisms designed for generic object recognition, which poorly
capture subtle pathological features like irregular lesion shapes and complex
textures. To overcome this, we propose a twofold solution: first, using a
training-free neural architecture search method (DeepMAD) to create an
efficient network backbone for edge devices; second, introducing the
Shape-Texture Attention Module (STAM). STAM splits attention into two branches
-- one using deformable convolutions (DCNv4) for shape awareness and the other
using a Gabor filter bank for texture awareness. On the public CCMT plant
disease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs)
reached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm
STAM significantly improves performance over baseline and standard attention
models. Integrating domain knowledge via decoupled attention thus presents a
promising path for edge-deployed precision agriculture AI. The source code is
available at https://github.com/RzMY/STA-Net.

</details>


### [31] [SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object Detection](https://arxiv.org/abs/2509.03786)
*Xinxin Wang,Han Sun,Ningzhong Liu,Huiyu Zhou,Yinan Yao*

Main category: cs.CV

TL;DR: 针对水下伪装目标检测（UCOD）的挑战，本文引入了UCOD任务和DeepCamo数据集，并提出了SLENet框架。SLENet通过独特的模块设计实现了对伪装目标的准确识别，并在多个数据集上表现出优越的性能和高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 水下伪装目标检测对海洋生态至关重要，但由于光学畸变、水体浑浊和海洋生物的复杂特性，准确识别面临巨大挑战且研究不足。

Method: 1. 引入UCOD任务并发布DeepCamo基准数据集。2. 提出语义定位与增强网络（SLENet）框架。3. SLENet集成了Gamma非对称增强（GAE）模块和定位引导分支（LGB），以增强多尺度特征表示并生成富含全局语义信息的定位图。4. 定位图引导多尺度监督解码器（MSSD）生成更准确的预测。

Result: SLENet在DeepCamo数据集和三个基准COD数据集上均超越了现有最先进的方法，证明了其卓越的性能和对更广泛伪装目标检测任务的良好泛化能力。

Conclusion: SLENet通过其创新的模块设计，成功解决了UCOD任务中的核心难题，显著提升了伪装目标检测的准确性和泛化能力，对海洋生态研究具有重要意义。

Abstract: Underwater Camouflaged Object Detection (UCOD) aims to identify objects that
blend seamlessly into underwater environments. This task is critically
important to marine ecology. However, it remains largely underexplored and
accurate identification is severely hindered by optical distortions, water
turbidity, and the complex traits of marine organisms. To address these
challenges, we introduce the UCOD task and present DeepCamo, a benchmark
dataset designed for this domain. We also propose Semantic Localization and
Enhancement Network (SLENet), a novel framework for UCOD. We first benchmark
state-of-the-art COD models on DeepCamo to reveal key issues, upon which SLENet
is built. In particular, we incorporate Gamma-Asymmetric Enhancement (GAE)
module and a Localization Guidance Branch (LGB) to enhance multi-scale feature
representation while generating a location map enriched with global semantic
information. This map guides the Multi-Scale Supervised Decoder (MSSD) to
produce more accurate predictions. Experiments on our DeepCamo dataset and
three benchmark COD datasets confirm SLENet's superior performance over SOTA
methods, and underscore its high generality for the broader COD task.

</details>


### [32] [Fitting Image Diffusion Models on Video Datasets](https://arxiv.org/abs/2509.03794)
*Juhun Lee,Simon S. Woo*

Main category: cs.CV

TL;DR: 通过利用视频帧的时序归纳偏置，本文提出了一种无需架构修改的训练策略，显著加速了图像扩散模型的收敛，提升了生成质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的图像扩散模型基于独立采样的静态图像进行训练，这种设计在捕捉时序世界方面存在信息不足，导致收敛慢、分布覆盖有限和泛化能力差。

Method: 提出了一种利用连续视频帧中时序归纳偏置的简单有效训练策略，以改进扩散模型训练。该方法无需架构修改，可无缝集成到标准训练流程中。在HandCo数据集上进行了评估。

Result: 经验证，该方法将收敛速度提高了2倍以上，并在训练和验证集上实现了更低的FID。它还通过捕捉有意义的时序变化来提高生成多样性。优化分析表明，该正则化方法减少了梯度方差，从而加速了收敛。

Conclusion: 所提出的训练策略能有效利用时序信息，显著提高扩散模型的训练效率、生成质量和多样性，并通过减少梯度方差获得理论支持，证明了其优越性。

Abstract: Image diffusion models are trained on independently sampled static images.
While this is the bedrock task protocol in generative modeling, capturing the
temporal world through the lens of static snapshots is information-deficient by
design. This limitation leads to slower convergence, limited distributional
coverage, and reduced generalization. In this work, we propose a simple and
effective training strategy that leverages the temporal inductive bias present
in continuous video frames to improve diffusion training. Notably, the proposed
method requires no architectural modification and can be seamlessly integrated
into standard diffusion training pipelines. We evaluate our method on the
HandCo dataset, where hand-object interactions exhibit dense temporal coherence
and subtle variations in finger articulation often result in semantically
distinct motions. Empirically, our method accelerates convergence by over
2$\text{x}$ faster and achieves lower FID on both training and validation
distributions. It also improves generative diversity by encouraging the model
to capture meaningful temporal variations. We further provide an optimization
analysis showing that our regularization reduces the gradient variance, which
contributes to faster convergence.

</details>


### [33] [MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting](https://arxiv.org/abs/2509.03800)
*Yuheng Li,Yenho Chen,Yuxiang Lai,Jike Zhong,Vanessa Wildman,Xiaofeng Yang*

Main category: cs.CV

TL;DR: MedVista3D是一个多尺度语义增强的3D CT视觉-语言预训练框架，通过局部-全局对齐和语义一致性解决放射诊断错误，并在多项任务上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 放射诊断错误（漏诊、注意力盲区、沟通失败）在临床中普遍存在，尤其在3D成像中，由于未能检测局部异常、缺乏全局上下文和报告语言可变性而加剧。现有3D视觉-语言模型无法同时满足局部-全局理解和处理非精炼放射报告的需求。

Method: 提出MedVista3D框架，一个用于3D CT分析的多尺度语义增强视觉-语言预训练框架。它通过局部和全局图像-文本对齐实现全容积上下文下的精细化表示学习。为解决报告可变性，采用语言模型重写并引入放射语义匹配库进行语义感知对齐。

Result: MedVista3D在零样本疾病分类、报告检索和医学视觉问答方面取得了最先进的性能，并能很好地迁移到器官分割和预后预测任务。

Conclusion: MedVista3D有效提升了3D CT分析能力，通过结合精确的局部检测、全局推理和语义一致的自然语言报告，有望减少放射诊断错误。

Abstract: Radiologic diagnostic errors-under-reading errors, inattentional blindness,
and communication failures-remain prevalent in clinical practice. These issues
often stem from missed localized abnormalities, limited global context, and
variability in report language. These challenges are amplified in 3D imaging,
where clinicians must examine hundreds of slices per scan. Addressing them
requires systems with precise localized detection, global volume-level
reasoning, and semantically consistent natural language reporting. However,
existing 3D vision-language models are unable to meet all three needs jointly,
lacking local-global understanding for spatial reasoning and struggling with
the variability and noise of uncurated radiology reports. We present
MedVista3D, a multi-scale semantic-enriched vision-language pretraining
framework for 3D CT analysis. To enable joint disease detection and holistic
interpretation, MedVista3D performs local and global image-text alignment for
fine-grained representation learning within full-volume context. To address
report variability, we apply language model rewrites and introduce a Radiology
Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves
state-of-the-art performance on zero-shot disease classification, report
retrieval, and medical visual question answering, while transferring well to
organ segmentation and prognosis prediction. Code and datasets will be
released.

</details>


### [34] [Causality-guided Prompt Learning for Vision-language Models via Visual Granulation](https://arxiv.org/abs/2509.03803)
*Mengyu Gao,Qiulei Dong*

Main category: cs.CV

TL;DR: 本文提出一种名为CaPL的因果引导文本提示学习方法，通过视觉粒化技术增强CLIP在细粒度识别任务上的表现，特别是在细微差异的捕捉上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的提示学习方法在处理细粒度数据集时能力有限，难以有效捕捉不同细粒度类别之间的细微差异。

Method: CaPL方法通过视觉粒化技术构建视觉颗粒，并利用因果推断来捕捉细微差异。它包含两个模块：1) 属性解耦模块，使用布朗桥扩散模型将视觉特征分解为非个体化和个体化属性；2) 颗粒学习模块，通过整合上述属性并结合两种因果推断策略来构建视觉颗粒，从而学习更具区分性的文本提示。

Result: 在15个数据集上的大量实验结果表明，CaPL方法显著优于现有最先进的提示学习方法，尤其在细粒度数据集上表现更为突出。

Conclusion: CaPL通过因果引导的视觉粒化和属性解耦，成功解决了现有提示学习方法在细粒度识别上的局限性，有效提升了预训练视觉-语言模型在下游细粒度任务中的性能。

Abstract: Prompt learning has recently attracted much attention for adapting
pre-trained vision-language models (e.g., CLIP) to downstream recognition
tasks. However, most of the existing CLIP-based prompt learning methods only
show a limited ability for handling fine-grained datasets. To address this
issue, we propose a causality-guided text prompt learning method via visual
granulation for CLIP, called CaPL, where the explored visual granulation
technique could construct sets of visual granules for the text prompt to
capture subtle discrepancies among different fine-grained classes through
casual inference. The CaPL method contains the following two modules: (1) An
attribute disentanglement module is proposed to decompose visual features into
non-individualized attributes (shared by some classes) and individualized
attributes (specific to single classes) using a Brownian Bridge Diffusion
Model; (2) A granule learning module is proposed to construct visual granules
by integrating the aforementioned attributes for recognition under two causal
inference strategies. Thanks to the learned visual granules, more
discriminative text prompt is expected to be learned. Extensive experimental
results on 15 datasets demonstrate that our CaPL method significantly
outperforms the state-of-the-art prompt learning methods, especially on
fine-grained datasets.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [35] [PG-Agent: An Agent Powered by Page Graph](https://arxiv.org/abs/2509.03536)
*Weizhi Chen,Ziwei Wang,Leyang Yang,Sheng Zhou,Xiaoxuan Tang,Jiajun Bu,Yong Li,Wei Jiang*

Main category: cs.AI

TL;DR: 本文提出PG-Agent框架，通过自动化流程将GUI操作序列转换为页面图，并结合RAG技术检索感知指南，以提升GUI代理在复杂页面环境下的感知能力和新场景泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理利用顺序操作片段作为先验知识，无法捕捉页面间复杂的转换关系，导致代理难以深入感知GUI环境，且难以泛化到新场景。

Method: 1. 设计自动化流程将顺序操作片段转换为显式建模页面间动作连接的页面图。2. 引入检索增强生成（RAG）技术，从页面图中检索可靠的GUI感知指南。3. 提出PG-Agent多智能体框架，并结合任务分解策略，将感知指南注入其中以泛化到新场景。

Result: 在各种基准测试上的大量实验表明，PG-Agent即使在页面图构建时使用有限的操作片段，也能展现出良好的有效性。

Conclusion: 通过构建页面图、结合RAG技术检索感知指南并融入PG-Agent多智能体框架，可以有效解决现有GUI代理在感知复杂页面转换关系和泛化能力上的挑战，即使数据有限也能表现出色。

Abstract: Graphical User Interface (GUI) agents possess significant commercial and
social value, and GUI agents powered by advanced multimodal large language
models (MLLMs) have demonstrated remarkable potential. Currently, existing GUI
agents usually utilize sequential episodes of multi-step operations across
pages as the prior GUI knowledge, which fails to capture the complex transition
relationship between pages, making it challenging for the agents to deeply
perceive the GUI environment and generalize to new scenarios. Therefore, we
design an automated pipeline to transform the sequential episodes into page
graphs, which explicitly model the graph structure of the pages that are
naturally connected by actions. To fully utilize the page graphs, we further
introduce Retrieval-Augmented Generation (RAG) technology to effectively
retrieve reliable perception guidelines of GUI from them, and a tailored
multi-agent framework PG-Agent with task decomposition strategy is proposed to
be injected with the guidelines so that it can generalize to unseen scenarios.
Extensive experiments on various benchmarks demonstrate the effectiveness of
PG-Agent, even with limited episodes for page graph construction.

</details>


### [36] [Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models](https://arxiv.org/abs/2509.03548)
*João P. Arroyo,João G. Rodrigues,Daniel Lawand,Denis D. Mauá,Junkyu Lee,Radu Marinescu,Alex Gray,Eduardo R. Laurentino,Fabio G. Cozman*

Main category: cs.AI

TL;DR: 本研究针对准马尔可夫因果模型中部分可识别查询的紧密概率边界计算问题，提出了一种简化程序构建的新算法，并对单次干预场景应用列生成技术，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在结构因果模型中，当外生混杂因素未完全指定时，无法精确计算感兴趣的概率值，因此需要计算这些概率值的紧密上下边界。

Method: 1. 提出一种新算法，利用内生变量的已知输入概率来简化多线性或线性规划的构建。2. 对于单次干预场景，采用列生成（column generation）技术，通过一系列辅助线性整数规划来计算概率边界，并证明外生变量可以有多项式基数的表示。

Result: 实验结果表明，所提出的列生成技术在计算概率边界方面优于现有方法。

Conclusion: 本研究为准马尔可夫因果模型中部分可识别查询的概率边界计算提供了高效方法，特别是在单次干预场景下，列生成技术表现出卓越性能并能实现外生变量的简化表示。

Abstract: We investigate partially identifiable queries in a class of causal models. We
focus on acyclic Structural Causal Models that are quasi-Markovian (that is,
each endogenous variable is connected with at most one exogenous confounder).
We look into scenarios where endogenous variables are observed (and a
distribution over them is known), while exogenous variables are not fully
specified. This leads to a representation that is in essence a Bayesian network
where the distribution of root variables is not uniquely determined. In such
circumstances, it may not be possible to precisely compute a probability value
of interest. We thus study the computation of tight probability bounds, a
problem that has been solved by multilinear programming in general, and by
linear programming when a single confounded component is intervened upon. We
present a new algorithm to simplify the construction of such programs by
exploiting input probabilities over endogenous variables. For scenarios with a
single intervention, we apply column generation to compute a probability bound
through a sequence of auxiliary linear integer programs, thus showing that a
representation with polynomial cardinality for exogenous variables is possible.
Experiments show column generation techniques to be superior to existing
methods.

</details>


### [37] [Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method](https://arxiv.org/abs/2509.03550)
*Tonghe Li,Jixin Liu,Weili Zeng,Hao Jiang*

Main category: cs.AI

TL;DR: 本文提出Diffusion-AC，一种将扩散概率模型融入空中交通冲突解决的深度强化学习框架，通过多模态决策克服了传统方法的单峰偏差，在高密度交通中显著提高了成功率并降低了碰撞风险。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习（DRL）方法在空中交通冲突检测与解决（CD&R）中存在“单峰偏差”，导致在复杂动态约束下决策灵活性不足和“决策僵局”，因此急需一种能提供更灵活、多模态决策的自动化CD&R方案。

Method: 该研究首创性地将扩散概率模型引入CD&R任务，提出了Diffusion-AC框架。该框架将策略建模为由价值函数指导的逆去噪过程，以生成丰富、高质量和多模态的动作分布。此外，通过密度渐进安全课程（DPSC）训练机制，确保在从稀疏到高密度交通环境中的稳定高效学习。

Result: 实验结果显示，Diffusion-AC显著优于多种最先进的DRL基线方法。在最具挑战性的高密度场景中，该方法不仅维持了94.1%的高成功率，而且相较于次优基线，将近距离空中相撞（NMACs）的发生率降低了约59%，显著提升了系统安全性。

Conclusion: Diffusion-AC凭借其独特的多模态决策能力，有效解决了传统DRL在CD&R中的单峰偏差问题，显著提升了在复杂高密度空域中的冲突解决成功率和安全性，展现了在未来空中交通管理中的巨大应用潜力。

Abstract: In the context of continuously rising global air traffic, efficient and safe
Conflict Detection and Resolution (CD&R) is paramount for air traffic
management. Although Deep Reinforcement Learning (DRL) offers a promising
pathway for CD&R automation, existing approaches commonly suffer from a
"unimodal bias" in their policies. This leads to a critical lack of
decision-making flexibility when confronted with complex and dynamic
constraints, often resulting in "decision deadlocks." To overcome this
limitation, this paper pioneers the integration of diffusion probabilistic
models into the safety-critical task of CD&R, proposing a novel autonomous
conflict resolution framework named Diffusion-AC. Diverging from conventional
methods that converge to a single optimal solution, our framework models its
policy as a reverse denoising process guided by a value function, enabling it
to generate a rich, high-quality, and multimodal action distribution. This core
architecture is complemented by a Density-Progressive Safety Curriculum (DPSC),
a training mechanism that ensures stable and efficient learning as the agent
progresses from sparse to high-density traffic environments. Extensive
simulation experiments demonstrate that the proposed method significantly
outperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the
most challenging high-density scenarios, Diffusion-AC not only maintains a high
success rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions
(NMACs) by approximately 59% compared to the next-best-performing baseline,
significantly enhancing the system's safety margin. This performance leap stems
from its unique multimodal decision-making capability, which allows the agent
to flexibly switch to effective alternative maneuvers.

</details>


### [38] [Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents](https://arxiv.org/abs/2509.03581)
*Davide Paglieri,Bartłomiej Cupiał,Jonathan Cook,Ulyana Piterbarg,Jens Tuyls,Edward Grefenstette,Jakob Nicolaus Foerster,Jack Parker-Holder,Tim Rocktäschel*

Main category: cs.AI

TL;DR: 本文提出了一种动态规划框架和两阶段训练方法，使大型语言模型（LLMs）代理能够根据任务需求灵活决定何时进行规划，从而在长周期任务中提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理方法如ReAct在每次行动前强制规划，导致计算成本高昂且在长周期任务中性能下降；而从不规划则限制了性能。因此，需要一种能灵活分配规划计算资源的方法。

Method: 引入了动态规划的理论框架，并提出了一个两阶段训练流程：1) 在多样化的合成数据上进行有监督微调（SFT），以初步培养模型的动态规划能力；2) 通过强化学习（RL）在长周期环境中进一步完善和优化这一能力。

Result: 在Crafter环境中的实验表明，通过该方法训练的动态规划代理具有更高的样本效率，并能持续实现更复杂的任务目标。此外，这些代理能被人类编写的计划有效引导，表现超越其独立能力。

Conclusion: 该工作首次探索了训练LLM代理在顺序决策任务中进行动态测试时间计算分配，为构建更高效、自适应和可控的代理系统奠定了基础。

Abstract: Training large language models (LLMs) to reason via reinforcement learning
(RL) significantly improves their problem-solving capabilities. In agentic
settings, existing methods like ReAct prompt LLMs to explicitly plan before
every action; however, we demonstrate that always planning is computationally
expensive and degrades performance on long-horizon tasks, while never planning
further limits performance. To address this, we introduce a conceptual
framework formalizing dynamic planning for LLM agents, enabling them to
flexibly decide when to allocate test-time compute for planning. We propose a
simple two-stage training pipeline: (1) supervised fine-tuning on diverse
synthetic data to prime models for dynamic planning, and (2) RL to refine this
capability in long-horizon environments. Experiments on the Crafter environment
show that dynamic planning agents trained with this approach are more
sample-efficient and consistently achieve more complex objectives.
Additionally, we demonstrate that these agents can be effectively steered by
human-written plans, surpassing their independent capabilities. To our
knowledge, this work is the first to explore training LLM agents for dynamic
test-time compute allocation in sequential decision-making tasks, paving the
way for more efficient, adaptive, and controllable agentic systems.

</details>


### [39] [Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE](https://arxiv.org/abs/2509.03626)
*Zahra Zehtabi Sabeti Moghaddam,Zeinab Dehghani,Maneeha Rani,Koorosh Aslansefat,Bhupesh Kumar Mishra,Rameez Raja Kureshi,Dhavalkumar Thakker*

Main category: cs.AI

TL;DR: 针对检索增强生成（RAG）的不透明性，本文提出了KG-SMILE，一个基于扰动的图RAG解释框架，能够提供稳定、人类对齐的解释，以增强人工智能的透明度与信任。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在幻觉和不可验证声明，限制了其在敏感领域的可靠性。尽管检索增强生成（RAG）通过外部知识提高了准确性，但其本身不透明，是一个黑箱，且严重依赖数据质量，这限制了其在需要高精度的领域（如医疗）中的应用和信任。

Method: 本文开发了一个名为Knowledge-Graph (KG)-SMILE的、与具体方法无关的、基于扰动的框架，旨在为图检索增强生成（Graph RAG）提供令牌和组件级别的互操作性。KG-SMILE通过应用受控扰动、计算相似性并训练加权线性替代模型，识别对生成输出最有影响的图实体和关系，从而提高RAG的透明度。

Result: 通过使用忠实度、一致性、稳定性、准确性等综合归因指标评估，结果表明KG-SMILE能够产生稳定且与人类判断一致的解释。

Conclusion: KG-SMILE证明了它能够平衡模型有效性和可解释性，进而促进了机器学习技术的更大透明度和信任。

Abstract: Generative AI, such as Large Language Models (LLMs), has achieved impressive
progress but still produces hallucinations and unverifiable claims, limiting
reliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves
accuracy by grounding outputs in external knowledge, especially in domains like
healthcare, where precision is vital. However, RAG remains opaque and
essentially a black box, heavily dependent on data quality. We developed a
method-agnostic, perturbation-based framework that provides token and
component-level interoperability for Graph RAG using SMILE and named it as
Knowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing
similarities, and training weighted linear surrogates, KG-SMILE identifies the
graph entities and relations most influential to generated outputs, thereby
making RAG more transparent. We evaluate KG-SMILE using comprehensive
attribution metrics, including fidelity, faithfulness, consistency, stability,
and accuracy. Our findings show that KG-SMILE produces stable, human-aligned
explanations, demonstrating its capacity to balance model effectiveness with
interpretability and thereby fostering greater transparency and trust in
machine learning technologies.

</details>


### [40] [CausalARC: Abstract Reasoning with Causal World Models](https://arxiv.org/abs/2509.03636)
*Jacqueline Maasch,John Kalantari,Kia Khezeli*

Main category: cs.AI

TL;DR: 本文引入CausalARC，一个基于因果世界模型且受ARC启发的AI推理实验平台，旨在评估AI在低数据和分布偏移条件下的推理能力，并通过提供多种因果反馈形式支持语言模型在不同推理任务上的评估。


<details>
  <summary>Details</summary>
Motivation: AI推理需要在数据有限和分布偏移的新问题设置中具备适应能力，现有方法可能难以有效评估此能力。

Method: 1. 提出了CausalARC实验平台，模拟抽象推理语料库（ARC）。2. CausalARC中的每个推理任务都从一个结构化因果模型（SCM）表示的因果世界模型中采样生成。3. 通过有原则的数据增强，以少量样本和上下文学习示例的形式，提供关于世界模型的观测、干预和反事实反馈。

Result: 作为概念验证，展示了CausalARC在四种语言模型评估设置中的应用：(1) 带测试时训练的抽象推理，(2) 带上下文学习的反事实推理，(3) 程序合成，以及 (4) 带逻辑推理的因果发现。

Conclusion: CausalARC提供了一个评估AI在低数据和分布偏移环境下推理能力的新型工具，并通过其独特的因果世界模型和多样化的反馈机制，支持了语言模型在多种高级推理任务中的评估。

Abstract: Reasoning requires adaptation to novel problem settings under limited data
and distribution shift. This work introduces CausalARC: an experimental testbed
for AI reasoning in low-data and out-of-distribution regimes, modeled after the
Abstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is
sampled from a fully specified causal world model, formally expressed as a
structural causal model. Principled data augmentations provide observational,
interventional, and counterfactual feedback about the world model in the form
of few-shot, in-context learning demonstrations. As a proof-of-concept, we
illustrate the use of CausalARC for four language model evaluation settings:
(1) abstract reasoning with test-time training, (2) counterfactual reasoning
with in-context learning, (3) program synthesis, and (4) causal discovery with
logical reasoning.

</details>


### [41] [Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations](https://arxiv.org/abs/2509.03644)
*François Olivier,Zied Bouraoui*

Main category: cs.AI

TL;DR: 本文提出了一个神经符号系统Embodied-LM，通过将LLMs的理解和逻辑推理根植于基于图像图式的图式表示中，以解决LLMs在逻辑推理中表现不佳的问题，并在逻辑演绎任务中展示了其有效性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在自然语言理解方面取得了显著进展，但在执行逻辑推理时仍易出错，通常缺乏支持类人理解的稳健心理表征。

Method: 引入了一个名为Embodied-LM的原型神经符号系统，该系统将理解和逻辑推理建立在基于图像图式的图式表示之上。系统通过Answer Set Programming中的声明性空间推理来操作这些认知结构的空间基础，并在一系列逻辑演绎问题上进行评估。

Result: 研究表明LLMs可以通过具身认知结构来解释场景，这些结构可以被形式化为可执行程序，并且由此产生的表示支持有效的逻辑推理，同时增强了可解释性。

Conclusion: 该系统为LLMs的逻辑推理提供了一个计算基础，通过引入具身认知结构和可执行程序，增强了模型的理解能力和可解释性。尽管当前实现专注于空间原语，但它为未来整合更复杂和动态的表示奠定了基础。

Abstract: Despite significant progress in natural language understanding, Large
Language Models (LLMs) remain error-prone when performing logical reasoning,
often lacking the robust mental representations that enable human-like
comprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that
grounds understanding and logical reasoning in schematic representations based
on image schemas-recurring patterns derived from sensorimotor experience that
structure human cognition. Our system operationalizes the spatial foundations
of these cognitive structures using declarative spatial reasoning within Answer
Set Programming. Through evaluation on logical deduction problems, we
demonstrate that LLMs can be guided to interpret scenarios through embodied
cognitive structures, that these structures can be formalized as executable
programs, and that the resulting representations support effective logical
reasoning with enhanced interpretability. While our current implementation
focuses on spatial primitives, it establishes the computational foundation for
incorporating more complex and dynamic representations.

</details>


### [42] [Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning](https://arxiv.org/abs/2509.03646)
*Haozhe Wang,Qixin Xu,Che Liu,Junhong Wu,Fangzhen Lin,Wenhu Chen*

Main category: cs.AI

TL;DR: 本研究揭示了强化学习提升大语言模型推理能力的两阶段动态：先习得低级技能，后掌握高级战略规划。为此，我们提出了层级感知信用分配（HICRA）算法，通过集中优化高影响力规划性token来显著提升性能，并引入了语义熵作为更准确的战略探索衡量指标。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）在增强大语言模型（LLM）的复杂推理能力方面表现出色，但其背后的机制仍不明确。诸如“顿悟时刻”、“长度缩放”和熵动态等现象令人费解。现有RL算法（如GRPO）的低效之处在于，它们不加区分地对所有token施加优化压力，稀释了学习信号。

Method: 通过分析，本研究揭示了LLM中涌现出的推理层级和两阶段学习动态：先是受限于程序正确性并提升低级技能，随后学习瓶颈转移至探索和掌握高级战略规划。为解决现有算法的低效问题，我们提出了一种名为“层级感知信用分配”（HIerarchy-Aware Credit Assignment, HICRA）的算法，该算法将优化精力集中在高影响力的规划性token上。此外，我们验证了语义熵作为衡量战略探索的优越指标，而非误导性的token级熵。

Result: 分析结果显示，LLM在RL学习中呈现出类似人类认知的两阶段推理层级和动态。HICRA算法显著优于强大的基线模型，证明了将优化重点放在战略瓶颈上是解锁高级推理能力的关键。同时，语义熵被验证为衡量战略探索的更优指标。

Conclusion: 强化学习在LLM推理中的成功源于一个从低级技能到高级战略规划的两阶段涌现推理层级。通过HICRA算法将优化压力集中于高影响力的规划性token，可以更有效地解锁高级推理能力。此外，语义熵是衡量战略探索的更准确工具。

Abstract: Reinforcement Learning (RL) has proven highly effective at enhancing the
complex reasoning abilities of Large Language Models (LLMs), yet underlying
mechanisms driving this success remain largely opaque. Our analysis reveals
that puzzling phenomena like ``aha moments", ``length-scaling'' and entropy
dynamics are not disparate occurrences but hallmarks of an emergent reasoning
hierarchy, akin to the separation of high-level strategic planning from
low-level procedural execution in human cognition. We uncover a compelling
two-phase dynamic: initially, a model is constrained by procedural correctness
and must improve its low-level skills. The learning bottleneck then decisively
shifts, with performance gains being driven by the exploration and mastery of
high-level strategic planning. This insight exposes a core inefficiency in
prevailing RL algorithms like GRPO, which apply optimization pressure
agnostically and dilute the learning signal across all tokens. To address this,
we propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that
concentrates optimization efforts on high-impact planning tokens. HICRA
significantly outperforms strong baselines, demonstrating that focusing on this
strategic bottleneck is key to unlocking advanced reasoning. Furthermore, we
validate semantic entropy as a superior compass for measuring strategic
exploration over misleading metrics such as token-level entropy.

</details>


### [43] [An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification](https://arxiv.org/abs/2509.03649)
*Davide Italo Serramazza,Nikos Papadeas,Zahraa Abdallah,Georgiana Ifrim*

Main category: cs.AI

TL;DR: 本研究探讨了时间序列解释性AI中分段策略对SHAP解释质量的影响，发现分段数量比具体分段方法更重要，等长分段表现最佳，且引入了一种新的归因归一化技术以提升质量。


<details>
  <summary>Details</summary>
Motivation: SHAP在长时序数据上的计算复杂度高，通过分段可降低，但最佳分段策略尚不明确，因此需要研究分段构成如何影响解释质量。

Method: 研究了八种不同的时间序列分段算法，使用InterpretTime和AUC Difference两种XAI评估方法进行评估。实验在多元和单变量时间序列上进行，并引入了一种根据分段长度加权的归因归一化新技术。

Result: 分段数量对解释质量的影响大于具体分段方法；等长分段持续优于大多数自定义时间序列分段算法；新提出的分段长度加权归因归一化技术能持续改善归因质量。

Conclusion: 对于时间序列解释性AI，分段数量是影响解释质量的关键因素，等长分段是一种有效且表现良好的策略。此外，基于分段长度的归一化技术能进一步提升归因效果。

Abstract: Explainable AI (XAI) has become an increasingly important topic for
understanding and attributing the predictions made by complex Time Series
Classification (TSC) models. Among attribution methods, SHapley Additive
exPlanations (SHAP) is widely regarded as an excellent attribution method; but
its computational complexity, which scales exponentially with the number of
features, limits its practicality for long time series. To address this, recent
studies have shown that aggregating features via segmentation, to compute a
single attribution value for a group of consecutive time points, drastically
reduces SHAP running time. However, the choice of the optimal segmentation
strategy remains an open question. In this work, we investigated eight
different Time Series Segmentation algorithms to understand how segment
compositions affect the explanation quality. We evaluate these approaches using
two established XAI evaluation methodologies: InterpretTime and AUC Difference.
Through experiments on both Multivariate (MTS) and Univariate Time Series
(UTS), we find that the number of segments has a greater impact on explanation
quality than the specific segmentation method. Notably, equal-length
segmentation consistently outperforms most of the custom time series
segmentation algorithms. Furthermore, we introduce a novel attribution
normalisation technique that weights segments by their length and we show that
it consistently improves attribution quality.

</details>


### [44] [PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming](https://arxiv.org/abs/2509.03728)
*Wesley Hanwen Deng,Sunnie S. Y. Kim,Akshita Jha,Ken Holstein,Motahhare Eslami,Lauren Wilcox,Leon A Gatys*

Main category: cs.AI

TL;DR: 本文提出PersonaTeaming方法，通过在对抗性提示生成过程中引入人物角色（personas），以模拟不同的用户背景和身份，从而提高自动化红队测试识别AI模型潜在风险的效率和广度。


<details>
  <summary>Details</summary>
Motivation: AI治理和安全研究呼吁有效的红队测试方法来发现AI模型的潜在风险。现有研究强调红队测试人员的身份和背景会影响其策略及发现的风险。当前自动化红队方法未考虑身份作用，限制了其探索模型行为的广度，故需要一种将人物背景和身份融入自动化红队测试的方法。

Method: 开发了PersonaTeaming方法。具体步骤包括：1. 提出基于“红队专家”或“普通AI用户”角色变异提示词的方法。2. 开发动态人物角色生成算法，根据初始提示词自动生成多种角色类型。3. 创建一套新的“变异距离”指标，以补充现有对抗性提示多样性测量方法。

Result: 实验结果显示，通过人物角色变异，对抗性提示的攻击成功率显著提高（高达144.1%），同时保持了提示的多样性，优于现有最先进的自动化红队方法RainbowPlus。

Conclusion: 该研究证明了在自动化红队测试中融入人物角色的潜力。讨论了不同角色类型和变异方法的优缺点，为未来探索自动化与人工红队测试的互补性提供了方向。

Abstract: Recent developments in AI governance and safety research have called for
red-teaming methods that can effectively surface potential risks posed by AI
models. Many of these calls have emphasized how the identities and backgrounds
of red-teamers can shape their red-teaming strategies, and thus the kinds of
risks they are likely to uncover. While automated red-teaming approaches
promise to complement human red-teaming by enabling larger-scale exploration of
model behavior, current approaches do not consider the role of identity. As an
initial step towards incorporating people's background and identities in
automated red-teaming, we develop and evaluate a novel method, PersonaTeaming,
that introduces personas in the adversarial prompt generation process to
explore a wider spectrum of adversarial strategies. In particular, we first
introduce a methodology for mutating prompts based on either "red-teaming
expert" personas or "regular AI user" personas. We then develop a dynamic
persona-generating algorithm that automatically generates various persona types
adaptive to different seed prompts. In addition, we develop a set of new
metrics to explicitly measure the "mutation distance" to complement existing
diversity measurements of adversarial prompts. Our experiments show promising
improvements (up to 144.1%) in the attack success rates of adversarial prompts
through persona mutation, while maintaining prompt diversity, compared to
RainbowPlus, a state-of-the-art automated red-teaming method. We discuss the
strengths and limitations of different persona types and mutation methods,
shedding light on future opportunities to explore complementarities between
automated and human red-teaming approaches.

</details>


### [45] [The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs](https://arxiv.org/abs/2509.03730)
*Pengrui Han,Rafal Kocielnik,Peiyang Song,Ramit Debnath,Dean Mobbs,Anima Anandkumar,R. Michael Alvarez*

Main category: cs.AI

TL;DR: 本研究系统探究了LLM个性特质的演变、自评特质的行为预测力以及人格注入的影响，发现对齐训练能稳定自评特质但无法可靠预测行为，且人格注入对行为影响甚微，揭示了LLM表层特质与行为之间存在差异。


<details>
  <summary>Details</summary>
Motivation: 先进的大语言模型(LLM)表现出类似人类的稳定行为倾向，预示可能存在人工系统中的“个性”。然而，现有研究主要依赖简化的自评报告和启发式提示，缺乏行为验证，因此需要系统地表征LLM的个性。

Method: 本研究通过三个维度系统刻画LLM个性：(1) 在不同训练阶段特质概貌的动态出现和演变；(2) 自评特质在行为任务中的预测有效性；(3) 针对性干预（如人格注入）对自评和行为的影响。

Result: 研究发现指令对齐（如RLHF、指令微调）显著稳定了特质表达并强化了特质相关性，与人类数据相似。然而，这些自评特质不能可靠地预测行为，观察到的关联也常与人类模式相悖。虽然人格注入能成功引导自评特质向预期方向发展，但对实际行为的影响微乎其微或不一致。

Conclusion: 本研究区分了LLM的表层特质表达与行为一致性，挑战了对LLM个性的假设，并强调在对齐和可解释性研究中需要更深入的评估，超越表层自评报告，关注实际行为表现。

Abstract: Personality traits have long been studied as predictors of human
behavior.Recent advances in Large Language Models (LLMs) suggest similar
patterns may emerge in artificial systems, with advanced LLMs displaying
consistent behavioral tendencies resembling human traits like agreeableness and
self-regulation. Understanding these patterns is crucial, yet prior work
primarily relied on simplified self-reports and heuristic prompting, with
little behavioral validation. In this study, we systematically characterize LLM
personality across three dimensions: (1) the dynamic emergence and evolution of
trait profiles throughout training stages; (2) the predictive validity of
self-reported traits in behavioral tasks; and (3) the impact of targeted
interventions, such as persona injection, on both self-reports and behavior.
Our findings reveal that instructional alignment (e.g., RLHF, instruction
tuning) significantly stabilizes trait expression and strengthens trait
correlations in ways that mirror human data. However, these self-reported
traits do not reliably predict behavior, and observed associations often
diverge from human patterns. While persona injection successfully steers
self-reports in the intended direction, it exerts little or inconsistent effect
on actual behavior. By distinguishing surface-level trait expression from
behavioral consistency, our findings challenge assumptions about LLM
personality and underscore the need for deeper evaluation in alignment and
interpretability.

</details>


### [46] [Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation](https://arxiv.org/abs/2509.03736)
*James Mooney,Josef Woldense,Zheng Robert Jia,Shirley Anugrah Hayati,My Ha Nguyen,Vipul Raheja,Dongyeop Kang*

Main category: cs.AI

TL;DR: 研究发现，尽管大型语言模型（LLMs）可能生成与人类相似的响应，但它们在不同实验设置下表现出严重的内部不一致性，这限制了其作为人类研究替代品的准确性。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）是否能作为人类主体研究中真实参与者的替代品。现有研究主要关注LLM生成数据与人类对应数据的符合度，而本文则探讨一个更根本的问题：LLM代理在不同实验设置下是否能保持内部一致性。

Method: 开发了一项研究，旨在(a)揭示代理的内部状态，并(b)在基本对话设置中检查代理行为。通过一系列行为假设来评估代理的对话行为是否与其揭示的内部状态一致。

Result: 研究发现，不同模型家族和大小的LLMs存在显著的内部不一致性。最重要的是，即使代理能够生成与人类相似的响应，它们也无法保持内部一致性。

Conclusion: LLMs内部一致性的缺乏是其作为人类主体研究中真实参与者准确替代品的一个关键能力缺陷。

Abstract: The impressive capabilities of Large Language Models (LLMs) have fueled the
notion that synthetic agents can serve as substitutes for real participants in
human-subject research. In an effort to evaluate the merits of this claim,
social science researchers have largely focused on whether LLM-generated survey
data corresponds to that of a human counterpart whom the LLM is prompted to
represent. In contrast, we address a more fundamental question: Do agents
maintain internal consistency, retaining similar behaviors when examined under
different experimental settings? To this end, we develop a study designed to
(a) reveal the agent's internal state and (b) examine agent behavior in a basic
dialogue setting. This design enables us to explore a set of behavioral
hypotheses to assess whether an agent's conversation behavior is consistent
with what we would expect from their revealed internal state. Our findings on
these hypotheses show significant internal inconsistencies in LLMs across model
families and at differing model sizes. Most importantly, we find that, although
agents may generate responses matching those of their human counterparts, they
fail to be internally consistent, representing a critical gap in their
capabilities to accurately substitute for real participants in human-subject
research. Our simulation code and data are publicly accessible.

</details>


### [47] [RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs](https://arxiv.org/abs/2509.03768)
*Connor Walker,Koorosh Aslansefat,Mohammad Naveed Akram,Yiannis Papadopoulos*

Main category: cs.AI

TL;DR: 本文提出RAGuard框架，通过并行查询技术文档和安全关键文档，并分配独立检索预算，显著提升了在海上风电维护等关键场景中基于LLM的RAG系统的安全召回率，同时保持了技术召回率。


<details>
  <summary>Details</summary>
Motivation: 在海上风电(OSW)维护等关键任务中，准确性和安全性至关重要。然而，传统的LLM在面对高度专业化或意外情况时，往往无法有效处理安全关键信息，导致性能不足。

Method: 引入RAGuard，一个增强型检索增强生成(RAG)框架，将安全关键文档与技术手册显式整合。RAGuard通过向两个独立的索引（知识和安全）发出并行查询，并为知识和安全分配单独的检索预算，确保了技术深度和安全覆盖。进一步开发了SafetyClamp扩展，它检索更大的候选池，并“硬性固定”安全槽位保障。评估方法包括稀疏(BM25)、密集(Dense Passage Retrieval)和混合检索范式，测量指标为Technical Recall@K和Safety Recall@K。

Result: RAGuard及其SafetyClamp扩展将Safety Recall@K从RAG的几乎0%提高到50%以上，同时将Technical Recall保持在60%以上。这些结果表明RAGuard和SafetyClamp在提升关键维护场景中LLM驱动决策支持的安全保障方面表现出色。

Conclusion: RAGuard和SafetyClamp有潜力为LLM驱动的决策支持在关键维护环境中整合安全保障设定新标准。

Abstract: Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet
conventional Large Language Models (LLMs) often fail when confronted with
highly specialised or unexpected scenarios. We introduce RAGuard, an enhanced
Retrieval-Augmented Generation (RAG) framework that explicitly integrates
safety-critical documents alongside technical manuals.By issuing parallel
queries to two indices and allocating separate retrieval budgets for knowledge
and safety, RAGuard guarantees both technical depth and safety coverage. We
further develop a SafetyClamp extension that fetches a larger candidate pool,
"hard-clamping" exact slot guarantees to safety. We evaluate across sparse
(BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms,
measuring Technical Recall@K and Safety Recall@K. Both proposed extensions of
RAG show an increase in Safety Recall@K from almost 0\% in RAG to more than
50\% in RAGuard, while maintaining Technical Recall above 60\%. These results
demonstrate that RAGuard and SafetyClamp have the potential to establish a new
standard for integrating safety assurance into LLM-powered decision support in
critical maintenance contexts.

</details>


### [48] [Leveraging LLM-Based Agents for Intelligent Supply Chain Planning](https://arxiv.org/abs/2509.03811)
*Yongzhi Qi,Jiaheng Yin,Jianshen Zhang,Dongyang Geng,Zhengyu Chen,Hao Hu,Wei Qi,Zuo-Jun Max Shen*

Main category: cs.AI

TL;DR: 本文提出并构建了一个基于大语言模型（LLM）的供应链规划智能体（SCPA）框架，以解决电商平台复杂的供应链规划问题，并在京东的真实场景中进行了部署验证，有效提升了效率和关键指标。


<details>
  <summary>Details</summary>
Motivation: 供应链管理中的规划至关重要且复杂，涉及多实体、多环节（如需求预测、库存管理、销售操作、补货），面临数据收集、长期规划、动态调整、可解释性、效率和可靠性等多重挑战。近年来AI技术（特别是LLM）的发展为解决这些实际问题提供了新工具。

Method: 构建了一个供应链规划智能体（SCPA）框架。该框架能够理解领域知识和操作员需求，分解任务，利用或创建新工具，并生成基于证据的规划报告。

Result: 将SCPA框架部署在京东的真实场景中，证明了LLM-agent在供应链中应用的可行性。它有效减少了人力投入，提高了准确性、库存可用性及其他关键指标。

Conclusion: 所提出的SCPA框架成功地将LLM智能体应用于复杂的供应链规划问题，不仅在真实世界场景中展现了可行性，还显著提升了运营效率和各项关键绩效指标。

Abstract: In supply chain management, planning is a critical concept. The movement of
physical products across different categories, from suppliers to warehouse
management, to sales, and logistics transporting them to customers, entails the
involvement of many entities. It covers various aspects such as demand
forecasting, inventory management, sales operations, and replenishment. How to
collect relevant data from an e-commerce platform's perspective, formulate
long-term plans, and dynamically adjust them based on environmental changes,
while ensuring interpretability, efficiency, and reliability, is a practical
and challenging problem. In recent years, the development of AI technologies,
especially the rapid progress of large language models, has provided new tools
to address real-world issues. In this work, we construct a Supply Chain
Planning Agent (SCPA) framework that can understand domain knowledge,
comprehend the operator's needs, decompose tasks, leverage or create new tools,
and return evidence-based planning reports. We deploy this framework in
JD.com's real-world scenario, demonstrating the feasibility of LLM-agent
applications in the supply chain. It effectively reduced labor and improved
accuracy, stock availability, and other key metrics.

</details>


### [49] [Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning](https://arxiv.org/abs/2509.03817)
*Wei Yang,Jesse Thomason*

Main category: cs.AI

TL;DR: 本文提出Meta-Policy Deliberation Framework (MPDF)，使多智能体LLM系统能学习适应性元认知策略，通过SoftRankPO算法稳定训练。MPDF在多个推理基准测试中，相较于现有SOTA方法，平均准确率提升4-5%。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）多智能体系统在复杂推理方面表现出潜力，但其有效性受限于固定的协作协议，忽视了智能体的内部审议能力。这些框架将智能体视为被动执行者，无法根据不确定性或置信度等内部认知状态调整策略，存在关键的元认知盲点。

Method: 引入元策略审议框架（MPDF），其中智能体学习一组高层元认知动作（坚持、改进、让步）的去中心化策略。为克服传统策略梯度在该设置中的不稳定性，开发了新型强化学习算法SoftRankPO。SoftRankPO通过基于通过平滑正态分位数映射的奖励排名来调整优势，从而稳定训练过程，使其对奖励方差具有鲁棒性。

Result: 实验结果表明，MPDF结合SoftRankPO在五个数学和通用推理基准测试中，相较于六种最先进的启发式和基于学习的多智能体推理算法，平均准确率绝对增益达4-5%。

Conclusion: 本研究为多智能体LLM系统学习自适应、元认知策略提供了一种新范式，将重点从设计固定协议转向学习动态、审议性的策略。

Abstract: Multi-agent systems of large language models (LLMs) show promise for complex
reasoning, but their effectiveness is often limited by fixed collaboration
protocols. These frameworks typically focus on macro-level orchestration while
overlooking agents' internal deliberative capabilities. This critical
meta-cognitive blindspot treats agents as passive executors unable to adapt
their strategy based on internal cognitive states like uncertainty or
confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where
agents learn a decentralized policy over a set of high-level meta-cognitive
actions: Persist, Refine, and Concede. To overcome the instability of
traditional policy gradients in this setting, we develop SoftRankPO, a novel
reinforcement learning algorithm. SoftRankPO stabilizes training by shaping
advantages based on the rank of rewards mapped through smooth normal quantiles,
making the learning process robust to reward variance. Experiments show that
MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across
five mathematical and general reasoning benchmarks compared to six
state-of-the-art heuristic and learning-based multi-agent reasoning algorithms.
Our work presents a paradigm for learning adaptive, meta-cognitive policies for
multi-agent LLM systems, shifting the focus from designing fixed protocols to
learning dynamic, deliberative strategies.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [50] [The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's Induced Metric](https://arxiv.org/abs/2509.03594)
*Thomas R. Harvey*

Main category: cs.LG

TL;DR: 提出一类基于损失函数黎曼度量的新型神经网络优化器，该优化器在低维示例中高效，对SOTA方法有轻微改进，并具有自动学习率调整等理论优势和与Adam相当的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 利用损失函数在嵌入高维空间时自然产生的黎曼度量这一几何特性，以开发出更有效、具有良好理论支撑的神经网络优化器。

Method: 通过将损失景观的几何视角具象化，利用其诱导的黎曼度量，开发了一类新型优化器。该优化器与SGD、Adam、AdamW和Muon等现有方法在多种任务和架构上进行了比较，并且其基本方法可用于修改现有的预处理方法。

Result: 经验性结果表明，新优化器在低维示例中表现高效，并对现有SOTA方法提供了轻微改进。理论上，它们具有理想特性，如在高曲率区域自动降低有效学习率（类似于平滑的梯度裁剪），以及可以诱导有效的调度学习率和解耦权重衰减。计算复杂度与Adam相当。

Conclusion: 基于黎曼度量的新型优化器为神经网络训练提供了一种有效的几何视角方法，它们在特定场景下表现出色，在一般情况下能带来轻微提升，并具有坚实的理论基础和与现有主流方法相当的计算效率。

Abstract: We present a class of novel optimisers for training neural networks that
makes use of the Riemannian metric naturally induced when the loss landscape is
embedded in higher-dimensional space. This is the same metric that underlies
common visualisations of loss landscapes. By taking this geometric perspective
literally and using the induced metric, we develop a new optimiser and compare
it to existing methods, namely: SGD, Adam, AdamW, and Muon, across a range of
tasks and architectures. Empirically, we conclude that this new class of
optimisers is highly effective in low dimensional examples, and provides slight
improvement over state-of-the-art methods for training neural networks. These
new optimisers have theoretically desirable properties. In particular, the
effective learning rate is automatically decreased in regions of high curvature
acting as a smoothed out form of gradient clipping. Similarly, one variant of
these optimisers can also be viewed as inducing an effective scheduled learning
rate and decoupled weight decay is the natural choice from our geometric
perspective. The basic method can be used to modify any existing
preconditioning method. The new optimiser has a computational complexity
comparable to that of Adam.

</details>


### [51] [CEHR-GPT: A Scalable Multi-Task Foundation Model for Electronic Health Records](https://arxiv.org/abs/2509.03643)
*Chao Pang,Jiheum Park,Xinzhuo Jiang,Nishanth Parameshwar Pavinkurve,Krishna S. Kalluri,Shalmali Joshi,Noémie Elhadad,Karthik Natarajan*

Main category: cs.LG

TL;DR: CEHR-GPT是一个通用的EHR基础模型，通过时间令牌学习框架，在一个架构中整合了特征表示、零样本预测和合成数据生成，解决了现有EHR AI模型通用性差的问题，并在多任务和外部数据集上展现出强大性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）在临床决策支持、风险预测和数据驱动医疗研究方面具有巨大潜力，但现有大多数AI模型仅针对狭窄的单一任务设计，限制了其通用性和在实际场景中的实用性。

Method: 本文提出了CEHR-GPT，一个通用的EHR数据基础模型，在一个架构中统一了特征表示、零样本预测和合成数据生成三项核心能力。为支持对临床序列的时间推理，CEHR-GPT引入了一个新颖的“时间令牌学习框架”(time-token-based learning framework)，将患者的动态时间线明确编码到模型结构中。

Result: CEHR-GPT在特征表示、零样本预测和合成数据生成这三项任务上均展现出强大的性能，并且通过词汇扩展和微调，能够有效地推广到外部数据集。

Conclusion: CEHR-GPT的通用性使得模型开发、队列发现和患者结局预测得以快速实现，无需进行特定任务的再训练，从而提高了EHR数据在医疗领域应用的效率和广泛性。

Abstract: Electronic Health Records (EHRs) provide a rich, longitudinal view of patient
health and hold significant potential for advancing clinical decision support,
risk prediction, and data-driven healthcare research. However, most artificial
intelligence (AI) models for EHRs are designed for narrow, single-purpose
tasks, limiting their generalizability and utility in real-world settings.
Here, we present CEHR-GPT, a general-purpose foundation model for EHR data that
unifies three essential capabilities - feature representation, zero-shot
prediction, and synthetic data generation - within a single architecture. To
support temporal reasoning over clinical sequences, \cehrgpt{} incorporates a
novel time-token-based learning framework that explicitly encodes patients'
dynamic timelines into the model structure. CEHR-GPT demonstrates strong
performance across all three tasks and generalizes effectively to external
datasets through vocabulary expansion and fine-tuning. Its versatility enables
rapid model development, cohort discovery, and patient outcome forecasting
without the need for task-specific retraining.

</details>


### [52] [Nonnegative matrix factorization and the principle of the common cause](https://arxiv.org/abs/2509.03652)
*E. Khalafyan,A. E. Allahverdyan,A. Hovhannisyan*

Main category: cs.LG

TL;DR: 本文探讨了非负矩阵分解（NMF）与共同原因原则（PCC）的密切关系。研究表明PCC能稳健估计NMF秩，从而提高NMF特征的稳定性，并解决其不可识别性问题；同时NMF可用于近似实现PCC，并应用于聚类和数据去噪。


<details>
  <summary>Details</summary>
Motivation: 探索无监督数据降维方法NMF与概率因果关系中的PCC之间的深层联系，并利用这种关系来互补和增强二者的应用潜力及鲁棒性。

Method: ['将灰度图像数据集转换为概率模型。', '利用PCC作为预测工具，实现NMF有效秩的稳健估计。', '基于PCC估计的秩，实施NMF以生成稳定的特征（基图像）。', '利用NMF近似实现PCC，处理联合概率。', '开发基于共同原因的聚类方法。', '展示NMF在数据去噪中的应用。']

Result: ['PCC提供了一种对弱噪声稳定的NMF有效秩估计，优于传统的估计方法（如基于BIC）。', '基于此秩的NMF所产生的特征（基图像）对噪声和局部优化初始值均表现出稳定性，有效解决了NMF的不可识别性问题。', 'NMF提供了一种近似实现PCC的途径，能够更好地解释较大的正相关联合概率。', '提出了一种将具有相同共同原因的数据点分组的聚类方法。', '展示了NMF在数据去噪方面的应用潜力。']

Conclusion: NMF和PCC之间存在显著且互惠互利的关系。PCC能够增强NMF的鲁棒性和特征稳定性，而NMF则为PCC的近似实现提供了可行性，并在聚类和数据去噪等领域展现了新的应用潜力。

Abstract: Nonnegative matrix factorization (NMF) is a known unsupervised data-reduction
method. The principle of the common cause (PCC) is a basic methodological
approach in probabilistic causality, which seeks an independent mixture model
for the joint probability of two dependent random variables. It turns out that
these two concepts are closely related. This relationship is explored
reciprocally for several datasets of gray-scale images, which are conveniently
mapped into probability models. On one hand, PCC provides a predictability tool
that leads to a robust estimation of the effective rank of NMF. Unlike other
estimates (e.g., those based on the Bayesian Information Criteria), our
estimate of the rank is stable against weak noise. We show that NMF implemented
around this rank produces features (basis images) that are also stable against
noise and against seeds of local optimization, thereby effectively resolving
the NMF nonidentifiability problem. On the other hand, NMF provides an
interesting possibility of implementing PCC in an approximate way, where larger
and positively correlated joint probabilities tend to be explained better via
the independent mixture model. We work out a clustering method, where data
points with the same common cause are grouped into the same cluster. We also
show how NMF can be employed for data denoising.

</details>


### [53] [Semi-decentralized Federated Time Series Prediction with Client Availability Budgets](https://arxiv.org/abs/2509.03660)
*Yunkai Bao,Reza Safarzadeh,Xin Wang,Steve Drew*

Main category: cs.LG

TL;DR: 针对联邦学习中客户端可用性和时间序列数据的挑战，本文提出了一种半去中心化的客户端选择方法FedDeCAB。该方法通过概率排名选择客户端，并在客户端离线时允许其从邻居获取部分模型参数进行优化，实验证明其在数据异构、通信受限和动态环境中表现有效。


<details>
  <summary>Details</summary>
Motivation: 在物联网联邦学习中，客户端不仅面临数据异构性，还受到有限的能源和可用性预算的限制。因此，有效的客户端选择对于全局模型收敛和客户端贡献平衡至关重要。本文旨在探讨时间序列数据下客户端可用性对联邦学习性能的具体影响。

Method: 本文讨论了时间序列数据下客户端可用性对联邦学习性能的影响，并设置了三种不同的场景来模拟可用性。在此基础上，提出了一种新颖的半去中心化客户端选择方法——FedDeCAB。该方法采用对可用客户端进行概率排名的方式，并且在客户端与服务器断开连接时，允许其从最近的邻居客户端获取部分模型参数进行联合优化，旨在提高离线模型的性能并减少通信开销。

Result: 基于真实世界大规模出租车和船舶轨迹数据集的实验结果表明，FedDeCAB在高度异构的数据分布、有限的通信预算以及客户端动态离线或重新加入等具有挑战性的场景下均表现出有效性。

Conclusion: 本文通过提出FedDeCAB，有效解决了联邦学习中客户端可用性问题，特别是在处理时间序列数据时。该方法通过创新的半去中心化选择机制和邻居协作优化，在复杂的物联网环境中显著提升了联邦学习的性能和鲁棒性。

Abstract: Federated learning (FL) effectively promotes collaborative training among
distributed clients with privacy considerations in the Internet of Things (IoT)
scenarios. Despite of data heterogeneity, FL clients may also be constrained by
limited energy and availability budgets. Therefore, effective selection of
clients participating in training is of vital importance for the convergence of
the global model and the balance of client contributions. In this paper, we
discuss the performance impact of client availability with time-series data on
federated learning. We set up three different scenarios that affect the
availability of time-series data and propose FedDeCAB, a novel,
semi-decentralized client selection method applying probabilistic rankings of
available clients. When a client is disconnected from the server, FedDeCAB
allows obtaining partial model parameters from the nearest neighbor clients for
joint optimization, improving the performance of offline models and reducing
communication overhead. Experiments based on real-world large-scale taxi and
vessel trajectory datasets show that FedDeCAB is effective under highly
heterogeneous data distribution, limited communication budget, and dynamic
client offline or rejoining.

</details>


### [54] [AutoGrid AI: Deep Reinforcement Learning Framework for Autonomous Microgrid Management](https://arxiv.org/abs/2509.03666)
*Kenny Guo,Nicholas Eckhert,Krish Chhajer,Luthira Abeykoon,Lorne Schell*

Main category: cs.LG

TL;DR: 提出一个基于深度强化学习的框架，用于偏远社区微电网的自主管理，通过优化能源调度以降低成本并最大化可再生能源利用。


<details>
  <summary>Details</summary>
Motivation: 优化偏远社区微电网的能源调度策略，旨在最小化成本、最大化可再生能源利用，并推动智能电网技术向零碳能源系统发展。

Method: 采用结合深度强化学习与时间序列预测模型的框架。具体利用Transformer架构进行可再生能源发电预测，并使用近端策略优化（PPO）智能体在模拟环境中做出决策。

Result: 实验结果表明，与传统基于规则的方法相比，该框架在能源效率和运行弹性方面均有显著提升。此外，还提供了一个用于模拟多种微电网环境的开源框架。

Conclusion: 该深度强化学习框架显著提升了偏远社区微电网的自主管理能力，促进了能源效率和运行弹性的提高，为实现零碳能源系统和智能电网技术进步做出了贡献。

Abstract: We present a deep reinforcement learning-based framework for autonomous
microgrid management. tailored for remote communities. Using deep reinforcement
learning and time-series forecasting models, we optimize microgrid energy
dispatch strategies to minimize costs and maximize the utilization of renewable
energy sources such as solar and wind. Our approach integrates the transformer
architecture for forecasting of renewable generation and a proximal-policy
optimization (PPO) agent to make decisions in a simulated environment. Our
experimental results demonstrate significant improvements in both energy
efficiency and operational resilience when compared to traditional rule-based
methods. This work contributes to advancing smart-grid technologies in pursuit
of zero-carbon energy systems. We finally provide an open-source framework for
simulating several microgrid environments.

</details>


### [55] [SharedRep-RLHF: A Shared Representation Approach to RLHF with Diverse Preferences](https://arxiv.org/abs/2509.03672)
*Arpan Mukherjee,Marcello Bullo,Deniz Gündüz*

Main category: cs.LG

TL;DR: 本文提出SharedRep-RLHF框架，通过学习群体间注释的“共享特征”，解决了MaxMin-RLHF在处理少数群体偏好时的性能问题，并在自然语言任务中实现了高达20%的胜率提升。


<details>
  <summary>Details</summary>
Motivation: 统一奖励的RLHF无法捕获子群体意见多样性，倾向于多数群体。尽管最先进的MaxMin-RLHF通过学习特定群体奖励模型来促进公平，但其在最低奖励群体为少数群体时表现不佳。

Method: 提出了一个名为SharedRep-RLHF的新框架。SharedRep-RLHF的核心是通过学习和利用不同群体注释中的“共享特征”来建模偏好，而非学习独立的群体奖励模型。研究证明了MaxMin-RLHF在学习共享特征方面是次优的，并量化了SharedRep-RLHF的样本复杂度。

Result: 在多样化的自然语言任务中，SharedRep-RLHF相比MaxMin-RLHF展现出更高的有效性，赢率（win rate）提升高达20%。

Conclusion: SharedRep-RLHF通过有效利用群体间共享特征，成功克服了现有公平RLHF方法（如MaxMin-RLHF）在处理少数群体时的局限性，显著提升了模型性能和公平性。

Abstract: Uniform-reward reinforcement learning from human feedback (RLHF), which
trains a single reward model to represent the preferences of all annotators,
fails to capture the diversity of opinions across sub-populations,
inadvertently favoring dominant groups. The state-of-the-art, MaxMin-RLHF,
addresses this by learning group-specific reward models, and by optimizing for
the group receiving the minimum reward, thereby promoting fairness. However, we
identify that a key limitation of MaxMin-RLHF is its poor performance when the
minimum-reward group is a minority. To mitigate this drawback, we introduce a
novel framework, termed {\em SharedRep-RLHF}. At its core, SharedRep-RLHF
learns and leverages {\em shared traits} in annotations among various groups,
in contrast to learning separate reward models across groups. We first show
that MaxMin-RLHF is provably suboptimal in learning shared traits, and then
quantify the sample complexity of SharedRep-RLHF. Experiments across diverse
natural language tasks showcase the effectiveness of SharedRep-RLHF compared to
MaxMin-RLHF with a gain of up to 20% in win rate.

</details>


### [56] [A Machine Learning-Based Study on the Synergistic Optimization of Supply Chain Management and Financial Supply Chains from an Economic Perspective](https://arxiv.org/abs/2509.03673)
*Hang Wang,Huijie Tang,Ningai Leng,Zhoufan Yu*

Main category: cs.LG

TL;DR: 本研究结合经济理论与机器学习，提出了一个SCM-FSCM协同模型，旨在解决供应链中的效率损失、融资约束和风险传导问题，并验证了其在提升运营绩效方面的显著效果。


<details>
  <summary>Details</summary>
Motivation: 解决供应链管理中存在的效率损失、融资约束和风险传导等问题，以支持高质量供应链发展。

Method: 整合交易成本和信息不对称理论，利用随机森林处理多维数据，构建成本-效率-风险三维分析框架。采用“核心企业信用赋能+动态质押融资”的FSCM模式，并应用LSTM进行需求预测，聚类/回归算法进行收益分配，博弈论与强化学习优化库存采购，以及XGBoost进行信用评估。

Result: 库存周转率提升30%，中小企业融资成本降低18%-22%，订单履约率稳定在95%以上。模型性能优异，需求预测误差≤8%，信用评估准确率≥90%。

Conclusion: 所提出的SCM-FSCM模型能有效降低运营成本，缓解融资约束，并支持供应链的高质量发展。

Abstract: Based on economic theories and integrated with machine learning technology,
this study explores a collaborative Supply Chain Management and Financial
Supply Chain Management (SCM - FSCM) model to solve issues like efficiency
loss, financing constraints, and risk transmission. We combine Transaction Cost
and Information Asymmetry theories and use algorithms such as random forests to
process multi-dimensional data and build a data-driven, three-dimensional
(cost-efficiency-risk) analysis framework. We then apply an FSCM model of "core
enterprise credit empowerment plus dynamic pledge financing." We use Long
Short-Term Memory (LSTM) networks for demand forecasting and
clustering/regression algorithms for benefit allocation. The study also
combines Game Theory and reinforcement learning to optimize the
inventory-procurement mechanism and uses eXtreme Gradient Boosting (XGBoost)
for credit assessment to enable rapid monetization of inventory. Verified with
20 core and 100 supporting enterprises, the results show a 30\% increase in
inventory turnover, an 18\%-22\% decrease in SME financing costs, a stable
order fulfillment rate above 95\%, and excellent model performance (demand
forecasting error <= 8\%, credit assessment accuracy >= 90\%). This SCM-FSCM
model effectively reduces operating costs, alleviates financing constraints,
and supports high-quality supply chain development.

</details>


### [57] [Insights from Gradient Dynamics: Gradient Autoscaled Normalization](https://arxiv.org/abs/2509.03677)
*Vincent-Daniel Yun*

Main category: cs.LG

TL;DR: 本研究通过实证分析梯度方差和标准差在训练中的演变，提出了一种无超参数的梯度归一化方法，该方法能稳定优化，并在CIFAR-100上保持或提升测试精度。


<details>
  <summary>Details</summary>
Motivation: 梯度动态对深度神经网络的稳定性和泛化能力至关重要。研究发现梯度方差和标准差在训练过程中表现出跨层和全局的一致性变化，这激励了新的梯度归一化方法的提出。

Method: 1. 对卷积网络中训练期间梯度的方差和标准差的演变进行实证分析。2. 提出了一种无超参数的梯度归一化方法，该方法将梯度缩放与它们的自然演变对齐，以防止意外放大、稳定优化并保持收敛保证。

Result: 在CIFAR-100数据集上，使用ResNet-20、ResNet-56和VGG-16-BN进行的实验表明，所提出的方法能够保持或改善测试准确率，即使在强泛化条件下也有效。该方法还能够防止意外放大并稳定优化。

Conclusion: 本研究强调了直接追踪梯度动态的重要性，有助于弥合理论预期与经验行为之间的差距，并为未来的优化研究提供见解。

Abstract: Gradient dynamics play a central role in determining the stability and
generalization of deep neural networks. In this work, we provide an empirical
analysis of how variance and standard deviation of gradients evolve during
training, showing consistent changes across layers and at the global scale in
convolutional networks. Motivated by these observations, we propose a
hyperparameter-free gradient normalization method that aligns gradient scaling
with their natural evolution. This approach prevents unintended amplification,
stabilizes optimization, and preserves convergence guarantees. Experiments on
the challenging CIFAR-100 benchmark with ResNet-20, ResNet-56, and VGG-16-BN
demonstrate that our method maintains or improves test accuracy even under
strong generalization. Beyond practical performance, our study highlights the
importance of directly tracking gradient dynamics, aiming to bridge the gap
between theoretical expectations and empirical behaviors, and to provide
insights for future optimization research.

</details>


### [58] [A Comprehensive Review of Multi-Agent Reinforcement Learning in Video Games](https://arxiv.org/abs/2509.03682)
*Zhengyang Li,Qijin Ji,Xinghong Ling,Quan Liu*

Main category: cs.LG

TL;DR: 本文综述了多智能体强化学习（MARL）在电子游戏中的应用，分析了关键挑战，并提出了游戏复杂度的估计方法及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）在游戏中已展现出超人的性能和日益增长的影响力，因此有必要对其进行全面回顾，以深化理解和推动发展。

Method: 本文通过对MARL在不同类型游戏（从回合制到实时、包括体育、FPS、RTS和MOBA）中应用的全面考察，分析了其面临的关键挑战（如非稳态、部分可观察性、稀疏奖励、团队协作和可扩展性），并重点介绍了成功的实现案例。此外，提出了一种估算游戏复杂性的新方法。

Result: 研究结果涵盖了MARL在各类电子游戏中的应用，揭示了其在复杂游戏环境中实现超人性能的潜力。同时，识别并分析了MARL在视频游戏中的主要挑战，并通过列举成功案例提供了实践洞察。论文为视频游戏AI系统中的MARL提供了见解。

Conclusion: 本研究为视频游戏AI系统中的MARL提供了深刻见解，提出了一种估算游戏复杂性的新方法，并为MARL及其在游戏开发中的应用指明了未来的研究方向，旨在激发该领域的进一步创新。

Abstract: Recent advancements in multi-agent reinforcement learning (MARL) have
demonstrated its application potential in modern games. Beginning with
foundational work and progressing to landmark achievements such as AlphaStar in
StarCraft II and OpenAI Five in Dota 2, MARL has proven capable of achieving
superhuman performance across diverse game environments through techniques like
self-play, supervised learning, and deep reinforcement learning. With its
growing impact, a comprehensive review has become increasingly important in
this field. This paper aims to provide a thorough examination of MARL's
application from turn-based two-agent games to real-time multi-agent video
games including popular genres such as Sports games, First-Person Shooter (FPS)
games, Real-Time Strategy (RTS) games and Multiplayer Online Battle Arena
(MOBA) games. We further analyze critical challenges posed by MARL in video
games, including nonstationary, partial observability, sparse rewards, team
coordination, and scalability, and highlight successful implementations in
games like Rocket League, Minecraft, Quake III Arena, StarCraft II, Dota 2,
Honor of Kings, etc. This paper offers insights into MARL in video game AI
systems, proposes a novel method to estimate game complexity, and suggests
future research directions to advance MARL and its applications in game
development, inspiring further innovation in this rapidly evolving field.

</details>


### [59] [Graph Random Features for Scalable Gaussian Processes](https://arxiv.org/abs/2509.03691)
*Matthew Zhang,Jihao Andreas Lin,Adrian Weller,Richard E. Turner,Isaac Reid*

Main category: cs.LG

TL;DR: 研究将图随机特征应用于可伸缩高斯过程，实现了计算效率和内存的显著提升，从而可以在大规模图上进行贝叶斯优化。


<details>
  <summary>Details</summary>
Motivation: 解决在高斯过程和图上的贝叶斯优化中，精确核函数造成的计算复杂度和内存消耗过高（$O(N^3)$），限制其在大规模图应用的问题。

Method: 应用图随机特征（GRFs）——一种新引入的图节点核随机估计器——于离散输入空间上的可伸缩高斯过程，并进行基于GRFs的贝叶斯推断。

Result: 证明了基于GRFs的贝叶斯推断时间复杂度为$O(N^{3/2})$，显著优于精确核函数的$O(N^3)$。实现了实际运行时间的大幅加速和内存节省，成功在大规模（超过$10^6$个节点）图上用单芯片进行贝叶斯优化，并保持了有竞争力的性能。

Conclusion: 图随机特征（GRFs）为大规模图上的高斯过程和贝叶斯优化提供了一种高效且可伸缩的解决方案，大幅降低了计算成本和内存需求，同时保持了良好的性能。

Abstract: We study the application of graph random features (GRFs) - a recently
introduced stochastic estimator of graph node kernels - to scalable Gaussian
processes on discrete input spaces. We prove that (under mild assumptions)
Bayesian inference with GRFs enjoys $O(N^{3/2})$ time complexity with respect
to the number of nodes $N$, compared to $O(N^3)$ for exact kernels. Substantial
wall-clock speedups and memory savings unlock Bayesian optimisation on graphs
with over $10^6$ nodes on a single computer chip, whilst preserving competitive
performance.

</details>


### [60] [Hierarchical Federated Foundation Models over Wireless Networks for Multi-Modal Multi-Task Intelligence: Integration of Edge Learning with D2D/P2P-Enabled Fog Learning Architectures](https://arxiv.org/abs/2509.03695)
*Payam Abdisarabshali,Fardis Nadimi,Kasra Borazjani,Naji Khosravan,Minghui Liwang,Wei Ni,Dusit Niyato,Michael Langberg,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: 提出分层联邦基础模型（HF-FMs）以解决雾/边缘网络中多模态多任务联邦基础模型（M3T FFMs）的模态和任务异构性，并发布开源代码促进研究。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型发展为多模态多任务（M3T）形式（如GPT-4），利用地理分布式数据催生了M3T联邦基础模型（FFMs）的新范式。然而，在雾/边缘网络中存在被忽视的数据模态和任务执行异构性，亟需新的解决方案。

Method: 本文提出了分层联邦基础模型（HF-FMs），将M3T FMs的模块化结构（包括模态编码器、提示、MoEs、适配器和任务头）与雾/边缘基础设施的分层特性相结合。HF-FMs还可选地利用设备到设备（D2D）通信，实现模块中继和局部协作训练，以应对模态和任务的异构性。

Result: 通过深入设计HF-FMs的架构，突出了其独特能力。研究团队在无线网络环境中原型化了HF-FMs，并发布了开源代码，以展示其潜力并促进该新兴领域的探索与发展。

Conclusion: HF-FMs为在雾/边缘网络中部署M3T FFMs提供了一个创新且未被探索的范式，有效解决了数据模态和任务执行的异构性问题。其架构设计和原型验证展示了巨大潜力，并为未来研究指明了方向。

Abstract: The rise of foundation models (FMs) has reshaped the landscape of machine
learning. As these models continued to grow, leveraging geo-distributed data
from wireless devices has become increasingly critical, giving rise to
federated foundation models (FFMs). More recently, FMs have evolved into
multi-modal multi-task (M3T) FMs (e.g., GPT-4) capable of processing diverse
modalities across multiple tasks, which motivates a new underexplored paradigm:
M3T FFMs. In this paper, we unveil an unexplored variation of M3T FFMs by
proposing hierarchical federated foundation models (HF-FMs), which in turn
expose two overlooked heterogeneity dimensions to fog/edge networks that have a
direct impact on these emerging models: (i) heterogeneity in collected
modalities and (ii) heterogeneity in executed tasks across fog/edge nodes.
HF-FMs strategically align the modular structure of M3T FMs, comprising
modality encoders, prompts, mixture-of-experts (MoEs), adapters, and task
heads, with the hierarchical nature of fog/edge infrastructures. Moreover,
HF-FMs enable the optional usage of device-to-device (D2D) communications,
enabling horizontal module relaying and localized cooperative training among
nodes when feasible. Through delving into the architectural design of HF-FMs,
we highlight their unique capabilities along with a series of tailored future
research directions. Finally, to demonstrate their potential, we prototype
HF-FMs in a wireless network setting and release the open-source code for the
development of HF-FMs with the goal of fostering exploration in this untapped
field (GitHub: https://github.com/payamsiabd/M3T-FFM).

</details>


### [61] [EmbedOR: Provable Cluster-Preserving Visualizations with Curvature-Based Stochastic Neighbor Embeddings](https://arxiv.org/abs/2509.03703)
*Tristan Luca Saidi,Abigail Hickok,Bastian Rieck,Andrew J. Blumberg*

Main category: cs.LG

TL;DR: EmbedOR是一种新的SNE算法，通过引入离散图曲率和曲率增强距离度量，解决了现有SNE算法（如UMAP和tSNE）在处理高维噪声数据时几何保持不佳和碎片化的问题。


<details>
  <summary>Details</summary>
Motivation: SNE算法（如UMAP和tSNE）在处理高维噪声数据时，经常无法保留数据几何结构，可能错误地分离连通分量，并难以发现可聚类数据中的簇。

Method: 我们提出了EmbedOR算法，这是一种结合了离散图曲率的SNE算法。该算法利用一种曲率增强的距离度量进行数据随机嵌入，以强调潜在的簇结构。

Result: 我们证明了EmbedOR的距离度量将tSNE的一致性结果扩展到更广泛的数据集。实验表明，EmbedOR在可视化和几何保持方面表现出色，与其它SNE算法和UMAP相比，它更不容易使连续、高密度数据区域碎片化。此外，EmbedOR距离度量可用于标注现有可视化，以识别碎片化并提供对底层几何的更深洞察。

Conclusion: EmbedOR通过引入离散图曲率和改进距离度量，显著提升了SNE算法在数据几何保持和簇结构发现方面的能力，并能有效避免数据碎片化，为高维数据可视化和分析提供了新的工具。

Abstract: Stochastic Neighbor Embedding (SNE) algorithms like UMAP and tSNE often
produce visualizations that do not preserve the geometry of noisy and high
dimensional data. In particular, they can spuriously separate connected
components of the underlying data submanifold and can fail to find clusters in
well-clusterable data. To address these limitations, we propose EmbedOR, a SNE
algorithm that incorporates discrete graph curvature. Our algorithm
stochastically embeds the data using a curvature-enhanced distance metric that
emphasizes underlying cluster structure. Critically, we prove that the EmbedOR
distance metric extends consistency results for tSNE to a much broader class of
datasets. We also describe extensive experiments on synthetic and real data
that demonstrate the visualization and geometry-preservation capabilities of
EmbedOR. We find that, unlike other SNE algorithms and UMAP, EmbedOR is much
less likely to fragment continuous, high-density regions of the data. Finally,
we demonstrate that the EmbedOR distance metric can be used as a tool to
annotate existing visualizations to identify fragmentation and provide deeper
insight into the underlying geometry of the data.

</details>


### [62] [Online Learning of Optimal Sequential Testing Policies](https://arxiv.org/abs/2509.03707)
*Qiyuan Chen,Raed Al Kontar*

Main category: cs.LG

TL;DR: 本文研究了在线测试问题(OTP)，其中测试结果分布未知且存在缺失数据。结果表明缺失数据导致遗憾值下限为 $\Omega(T^{\frac{2}{3}})$，高于标准MDP的 $\Theta(\sqrt{T})$。研究提出了一种“探索-然后-提交”算法匹配该下限，并为奖励与缺失数据无关的变体问题实现更优的 $\tilde{O}(\sqrt{T})$ 遗憾值。


<details>
  <summary>Details</summary>
Motivation: 在在线学习场景中，为受试者选择最优测试策略时，需权衡测试成本与信息量。在测试结果联合分布未知且仅能进行部分测试的情况下，由此产生的缺失数据会偏差估计，使得问题比标准偶发性MDP更具挑战性，因此需要新的理论和算法来解决。

Method: 1. 理论推导：证明了在线测试问题(OTP)在缺失数据下的极小极大遗憾值下限为 $\Omega(T^{\frac{2}{3}})$。 2. 算法设计：提出了一种“探索-然后-提交”(Explore-Then-Commit)算法来匹配OTP的理论下限。 3. 变体算法：针对奖励独立于缺失数据的特定变体问题（在线成本敏感最大熵采样问题），设计了一种迭代消除算法。 4. 数值验证：通过数值模拟验证了在两种设置下的理论结果。

Result: 1. 发现缺失数据导致OTP的极小极大遗憾值下限为 $\Omega(T^{\frac{2}{3}})$，远高于标准偶发性MDP的 $\Theta(\sqrt{T})$。 2. “探索-然后-提交”算法在离散和高斯分布下实现了 $\tilde{O}(T^{\frac{2}{3}})$ 的累积遗憾值，匹配了OTP的理论下限。 3. 对于奖励与缺失数据无关的变体问题，迭代消除算法实现了 $\tilde{O}(\sqrt{T})$ 的遗憾值，成功突破了OTP的 $\Omega(T^{\frac{2}{3}})$ 下限。 4. 数值实验结果与理论分析一致。

Conclusion: 这项工作加深了对缺失数据下探索-利用权衡的理解，并为设计高效的序列测试策略提供了理论指导。它强调了缺失数据对在线学习问题难度增加的关键影响。

Abstract: This paper studies an online learning problem that seeks optimal testing
policies for a stream of subjects, each of whom can be evaluated through a
sequence of candidate tests drawn from a common pool. We refer to this problem
as the Online Testing Problem (OTP). Although conducting every candidate test
for a subject provides more information, it is often preferable to select only
a subset when tests are correlated and costly, and make decisions with partial
information. If the joint distribution of test outcomes were known, the problem
could be cast as a Markov Decision Process (MDP) and solved exactly. In
practice, this distribution is unknown and must be learned online as subjects
are tested. When a subject is not fully tested, the resulting missing data can
bias estimates, making the problem fundamentally harder than standard episodic
MDPs. We prove that the minimax regret must scale at least as
$\Omega(T^{\frac{2}{3}})$, in contrast to the $\Theta(\sqrt{T})$ rate in
episodic MDPs, revealing the difficulty introduced by missingness. This
elevated lower bound is then matched by an Explore-Then-Commit algorithm whose
cumulative regret is $\tilde{O}(T^{\frac{2}{3}})$ for both discrete and
Gaussian distributions. To highlight the consequence of missingness-dependent
rewards in OTP, we study a variant called the Online Cost-sensitive Maximum
Entropy Sampling Problem, where rewards are independent of missing data. This
structure enables an iterative-elimination algorithm that achieves
$\tilde{O}(\sqrt{T})$ regret, breaking the $\Omega(T^{\frac{2}{3}})$ lower
bound for OTP. Numerical results confirm our theory in both settings. Overall,
this work deepens the understanding of the exploration--exploitation trade-off
under missing data and guides the design of efficient sequential testing
policies.

</details>


### [63] [From Federated Learning to $\mathbb{X}$-Learning: Breaking the Barriers of Decentrality Through Random Walks](https://arxiv.org/abs/2509.03709)
*Allan Salihovic,Payam Abdisarabshali,Michael Langberg,Seyyedali Hosseinalipour*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We provide our perspective on $\mathbb{X}$-Learning ($\mathbb{X}$L), a novel
distributed learning architecture that generalizes and extends the concept of
decentralization. Our goal is to present a vision for $\mathbb{X}$L,
introducing its unexplored design considerations and degrees of freedom. To
this end, we shed light on the intuitive yet non-trivial connections between
$\mathbb{X}$L, graph theory, and Markov chains. We also present a series of
open research directions to stimulate further research.

</details>


### [64] [Differentiable Entropy Regularization for Geometry and Neural Networks](https://arxiv.org/abs/2509.03733)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出了范围分区熵的可微分估计器，并将其应用于加速几何算法和正则化Transformer注意力，在不牺牲准确性的前提下显著提高了效率。


<details>
  <summary>Details</summary>
Motivation: 范围分区熵对自适应算法很有价值，但尚未应用于深度学习领域。研究动机是将范围分区熵引入深度学习，以利用其在算法设计中的优势。

Method: (1) 提出了范围分区熵的首个可微分近似，使其可作为可训练的损失函数或正则化器。(2) 设计了EntropyNet，一个重构数据为低熵形式以加速下游算法的神经模块。(3) 将熵正则化应用于Transformer注意力机制，扩展其应用范围。

Result: 在几何任务中，实现了高达4.1倍的运行时加速，且误差可忽略不计（<0.2%）。在深度学习中，它诱导了结构化注意力模式，在80%稀疏度下比L1基线提高了6%的准确率。理论分析提供了估计器的近似界限。

Conclusion: 熵约束计算不仅具有理论上的优雅性，而且是实现自适应学习、效率和结构化表示的实用机制。

Abstract: We introduce a differentiable estimator of range-partition entropy, a recent
concept from computational geometry that enables algorithms to adapt to the
"sortedness" of their input. While range-partition entropy provides strong
guarantees in algorithm design, it has not yet been made accessible to deep
learning. In this work, we (i) propose the first differentiable approximation
of range-partition entropy, enabling its use as a trainable loss or
regularizer; (ii) design EntropyNet, a neural module that restructures data
into low-entropy forms to accelerate downstream instance-optimal algorithms;
and (iii) extend this principle beyond geometry by applying entropy
regularization directly to Transformer attention. Across tasks, we demonstrate
that differentiable entropy improves efficiency without degrading correctness:
in geometry, our method achieves up to $4.1\times$ runtime speedups with
negligible error ($<0.2%$); in deep learning, it induces structured attention
patterns that yield 6% higher accuracy at 80% sparsity compared to L1
baselines. Our theoretical analysis provides approximation bounds for the
estimator, and extensive ablations validate design choices. These results
suggest that entropy-bounded computation is not only theoretically elegant but
also a practical mechanism for adaptive learning, efficiency, and structured
representation.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [65] [Entanglement Purification With Finite Latency Classical Communication in Quantum Networks](https://arxiv.org/abs/2509.03667)
*Vivek Vasan,Alexander Nico-Katz,Boulat A. Bash,Daniel C. Kilper,Marco Ruffini*

Main category: cs.NI

TL;DR: 本文分析了纠缠纯化协议在现实非瞬时经典通信（IP网络）延迟下的可行性，并量化了其在不同网络条件和量子存储技术下的性能，为未来量子网络部署提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 量子网络依赖高保真纠缠对，但存储期间的环境退相干会降低其保真度。纠缠纯化可恢复保真度，然而纯化所需的经典通信延迟会造成空闲期，反而使量子态进一步退相干，挑战了纯化的实际有效性。

Method: 研究者分析了BBPSSW和DEJMPS等纠缠纯化协议，考虑了基于IP网络的非瞬时经典协调。通过对底层量子动力学进行微观Lindblad处理，并结合当前城域IP网络延迟统计数据及量子存储测试平台的参数，在各种网络条件和量子存储技术下进行了全面的性能评估。

Result: 研究确定了纠缠纯化成功和失败的区域，并用相空间中的收支平衡等保真度等值线进行划分。同时，量化了完成多轮纯化协议所需的纠缠对总数，以及达到特定应用阈值的纯化纠缠对的稳态吞吐量。

Conclusion: 本研究为在当前和未来网络中部署纠缠纯化提供了延迟预算、存储质量目标以及资源开销估算，具有重要的实践指导意义。

Abstract: Quantum networks rely on high fidelity entangled pairs distributed to nodes,
but maintaining their fidelity is challenged by environmental decoherence
during storage. Entanglement purification is used to restore fidelity, but the
idle periods imposed by the associated classical communication delays
counteract this goal by exposing the states to further decoherence. In this
work, we analyze the practical viability of entanglement purification protocols
(BBPSSW, DEJMPS), under non-instantaneous classical coordination over Internet
protocol (IP) communications networks. We present a comprehensive performance
evaluation of these protocols in various network conditions for a range of
quantum memory technologies. We employ a microscopic Lindblad treatment of the
underlying quantum dynamics, and use current-generation metropolitan IP network
latency statistics and parameters drawn from quantum memory testbeds. In doing
so we identify the regions in which entanglement purification succeeds and
fails, delineated by break-even iso-fidelity contours in the phase space. We
then determine the total number of entangled pairs required to complete a
multi-round purification protocol, and the steady-state throughput of entangled
pairs with purified fidelities that exceed application-specific thresholds.
This provides latency budgets, memory quality targets, and resource-overhead
estimates for deploying purification on current and near-future networks.

</details>


### [66] [Drift Plus Optimistic Penalty -- A Learning Framework for Stochastic Network Optimization](https://arxiv.org/abs/2509.03762)
*Sathwik Chadaga,Eytan Modiano*

Main category: cs.NI

TL;DR: 本文提出一种针对未知传输成本的排队网络中联合路由与调度问题的新型控制策略，结合Lyapunov漂移惩罚和多臂老虎机技术，实现了亚线性后悔。


<details>
  <summary>Details</summary>
Motivation: 在排队网络中，控制器需要在边传输成本未知且仅能观察选定边成本的情况下，同时优化吞吐量和成本以确保网络稳定性。现有的多臂老虎机解决方案因排队动态而无法直接应用，存在探索-利用的权衡难题。

Method: 研究首先证明了最佳可实现成本的下界是由一个静态优化问题解给出。随后，利用Lyapunov漂移惩罚优化和多臂老虎机技术，开发了一种网络控制策略。

Result: 所提出的策略实现了$O(\sqrt{T}\log T)$的亚线性后悔，与拥有完整到达和成本信息的最佳策略相比表现优异。模拟评估也证实了该策略的后悔确实是亚线性的。

Conclusion: 该研究成功开发了一种应对未知传输成本下排队网络联合路由与调度问题的有效控制策略，该策略能确保网络稳定性并实现了卓越的性能（亚线性后悔）。

Abstract: We consider the problem of joint routing and scheduling in queueing networks,
where the edge transmission costs are unknown. At each time-slot, the network
controller receives noisy observations of transmission costs only for those
edges it selects for transmission. The network controller's objective is to
make routing and scheduling decisions so that the total expected cost is
minimized. This problem exhibits an exploration-exploitation trade-off,
however, previous bandit-style solutions cannot be directly applied to this
problem due to the queueing dynamics. In order to ensure network stability, the
network controller needs to optimize throughput and cost simultaneously. We
show that the best achievable cost is lower bounded by the solution to a static
optimization problem, and develop a network control policy using techniques
from Lyapunov drift-plus-penalty optimization and multi-arm bandits. We show
that the policy achieves a sub-linear regret of order $O(\sqrt{T}\log T)$, as
compared to the best policy that has complete knowledge of arrivals and costs.
Finally, we evaluate the proposed policy using simulations and show that its
regret is indeed sub-linear.

</details>


### [67] [A Versatile and Programmable UAV Platform for Radio Access Network and End-to-End Cellular Measurements](https://arxiv.org/abs/2509.03818)
*Sherwan Jalal Abdullah,Sravan Reddy Chintareddy,Victor S. Frost,Shawn Keshmiri,Morteza Hashemi*

Main category: cs.NI

TL;DR: 开发了一种基于无人机的移动网络测量平台，以克服传统方法在农村地区的局限，并分析了高空信号特性及覆盖表现。


<details>
  <summary>Details</summary>
Motivation: 传统移动网络覆盖测试（如众包）在农村地区存在耗时、耗力、危险、人口稀疏及地形复杂等问题，亟需一种更高效、准确的测量方法。

Method: 该研究开发了一个基于无人机（UAV）的测量平台，通过空中作业收集无线接入网络（RAN）信号和端到端网络性能指标。平台整合了机载计算单元和商用蜂窝调制解调器，收集的数据通过地理空间映射工具和统计技术进行分析展示。

Result: 实验表明，高空视距条件改善导致接收信号功率增强，但邻近小区干扰使信号质量下降。大多数测试区域维持了可接受的信号质量、足够的上下行吞吐量和满意的往返时间。一个关键发现是，强无线电信号指标不一定代表测试区域内一致的空间覆盖。

Conclusion: 基于无人机的平台能够有效解决传统测试在困难区域的挑战，并揭示了高空环境下信号功率与质量之间的复杂关系，以及强信号不等于一致空间覆盖的现象，为网络优化提供了新视角。

Abstract: In this work, we develop a measurement platform to capture mobile network
performance metrics including coverage and quality of service in regions where
conventional coverage testing approaches are frequently time-intensive,
labor-demanding, and occasionally hazardous. Traditionally, crowd-sourcing
methods are used to collect cellular network performance metrics. However,
these approaches are inadequate in rural areas due to low-density population,
and difficult terrain. The platform described here is a UAV-based and is
designed to investigate the mobile network performance through aerial
operations and gather Radio Access Network (RAN) signal alongside end-to-end
network performance metrics. Our platform gathers metrics through the
integration of an onboard computation unit and commercial off-the-shelf
cellular modem. The gathered data are subsequently analyzed and displayed using
geospatial mapping utilities and statistical techniques to deliver key
observations on cellular network performance. Experimental results showed that
the received signal power improves at higher altitudes due to enhanced
line-of-sight (LoS) conditions as expected. However, the signal quality
degrades as a result of increased interference from neighboring cells. The
analysis reveals that for most of the geographic area covered in the initial
experiments the system maintained acceptable signal quality, with adequate
throughput performance for both uplink and downlink communications, while
maintaining satisfactory round-trip time characteristics. Notably, the
experiment showed that a strong radio signal metric for a given cell does not
necessarily translate to consistent spatial coverage across the tested region.

</details>


### [68] [Indoor Positioning with Wi-Fi Location: A Survey of IEEE 802.11mc/az/bk Fine Timing Measurement Research](https://arxiv.org/abs/2509.03901)
*Katarzyna Kosek-Szott,Szymon Szott,Wojciech Ciezobka,Maksymilian Wojnar,Krzysztof Rusek,Jonathan Segev*

Main category: cs.NI

TL;DR: 本文首次对IEEE 802.11mc FTM（Wi-Fi定位）及其增强功能进行了全面综述，涵盖其实际精度、改进方法、应用及安全问题，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 室内定位对ICT和IoT功能至关重要。IEEE 802.11mc FTM因Wi-Fi网络高可用性、高精度和设备支持而潜力巨大，并有新增强功能。然而，目前缺乏专门针对FTM及其增强功能的综述，本文旨在填补这一空白。

Method: 本文分类并回顾了超过180篇相关研究论文，分析了FTM的实际精度、精度提升方法（包括机器学习）、与其他室内定位系统结合、基于FTM的应用以及安全问题。

Result: 通过对大量文献的调研，本文总结了FTM在实际精度、精度提升、系统集成、应用和安全方面的重要研究成就。

Conclusion: 基于所进行的综述，本文提炼出最重要的研究成就，并提出了未来研究的开放领域。

Abstract: Indoor positioning is an enabling technology for home, office, and industrial
network users because it provides numerous information and communication
technology (ICT) and Internet of things (IoT) functionalities such as indoor
navigation, smart meter localization, asset tracking, support for emergency
services, and detection of hazardous situations. The IEEE 802.11mc fine timing
measurement (FTM) protocol (commercially known as Wi-Fi Location) has great
potential to enable indoor positioning in future generation devices, primarily
because of the high availability of Wi-Fi networks, FTM's high accuracy and
device support. Furthermore, new FTM enhancements are available in the released
(802.11az) and recently completed (802.11bk) amendments. Despite the multitude
of literature reviews on indoor positioning, a survey dedicated to FTM and its
recent enhancements has so far been lacking. We fill this gap by classifying
and reviewing over 180 research papers related to the practical accuracy
achieved with FTM, methods for improving its accuracy (also with machine
learning), combining FTM with other indoor positioning systems, FTM-based
applications, and security issues. Based on the conducted survey, we summarize
the most important research achievements and formulate open areas for further
research.

</details>


### [69] [Autonomous Task Offloading of Vehicular Edge Computing with Parallel Computation Queues](https://arxiv.org/abs/2509.03935)
*Sungho Cho,Sung Il Choi,Seung Hyun Oh,Ian P. Roberts,Sang Hyun Lee*

Main category: cs.NI

TL;DR: 该研究提出了一种在车载边缘计算(VEC)网络中最小化车载用户等待延迟的并行任务卸载策略，通过网络协作平衡资源利用和负载拥堵，实现了全局最优的延迟性能。


<details>
  <summary>Details</summary>
Motivation: 最小化车载用户在车载边缘计算(VEC)网络中的整体等待延迟。

Method: 提出了一种基于网络协作的新型任务卸载方案，旨在平衡资源利用率不足和负载拥堵。该方案通过预测边缘服务器的瞬时处理能力来识别过载服务器，并考虑队列的离散变量进行精确估计。通过理论分析、数值模拟以及在真实地图虚拟环境中的可行性测试进行评估。

Result: 与现有方法相比，所开发的解决方案实现了全局最优的延迟减少性能。深入分析表明，预测边缘服务器的瞬时处理能力对于识别过载服务器和确定网络延迟至关重要。

Conclusion: 所提出的技术通过精确估计和有效应对组合挑战（通过考虑队列离散变量），能够实现最优性能，从而显著降低VEC网络中车载用户的等待延迟。

Abstract: This work considers a parallel task execution strategy in vehicular edge
computing (VEC) networks, where edge servers are deployed along the roadside to
process offloaded computational tasks of vehicular users. To minimize the
overall waiting delay among vehicular users, a novel task offloading solution
is implemented based on the network cooperation balancing resource
under-utilization and load congestion. Dual evaluation through theoretical and
numerical ways shows that the developed solution achieves a globally optimal
delay reduction performance compared to existing methods, which is also
approved by the feasibility test over a real-map virtual environment. The
in-depth analysis reveals that predicting the instantaneous processing power of
edge servers facilitates the identification of overloaded servers, which is
critical for determining network delay. By considering discrete variables of
the queue, the proposed technique's precise estimation can effectively address
these combinatorial challenges to achieve optimal performance.

</details>


### [70] [Analyzing the Effect of an Extreme Weather Event on Telecommunications and Information Technology: Insights from 30 Days of Flooding](https://arxiv.org/abs/2509.04219)
*Leandro Márcio Bertholdo,Renan Barreto Paredes,Gabriela de Lima Marin,Cesar A. H. Loureiro,Milton Kaoru Kashiwakura Pedro de Botelho Marcos*

Main category: cs.NI

TL;DR: 本研究构建并分析了巴西2024年洪灾期间的电信数据集，旨在评估信息通信技术基础设施的韧性，并揭示连接恢复趋势、基础设施脆弱性及用户行为变化。


<details>
  <summary>Details</summary>
Motivation: 2024年5月巴西里奥格兰德州严重的降雨导致基础设施广泛受损，影响了大量城市和人口。本研究旨在通过分析此灾害事件，深入了解光纤网络、数据中心和互联网流量在关键事件中的韧性，并突出其在面临严峻考验时所面临的挑战。

Method: 研究构建了全面的电信数据集，涵盖互联网测量、光纤中断报告和互联网交换路由数据。通过将网络中断与水文及运营因素相关联，对信息通信技术基础设施的相关故障进行调查。

Result: 初步发现揭示了连接恢复的趋势、基础设施的脆弱性以及用户行为的变化。

Conclusion: 所构建的数据集和初步分析旨在为未来关于灾害恢复策略的研究以及开发更稳健的电信系统提供支持。

Abstract: In May 2024, weeks of severe rainfall in Rio Grande do Sul, Brazil caused
widespread damage to infrastructure, impacting over 400 cities and 2.3 million
people. This study presents the construction of comprehensive
telecommunications datasets during this climatic event, encompassing Internet
measurements, fiber cut reports, and Internet Exchange routing data. By
correlating network disruptions with hydrological and operational factors, the
dataset offers insights into the resilience of fiber networks, data centers,
and Internet traffic during critical events. For each scenario, we investigate
failures related to the Information and Communication Technology infrastructure
and highlight the challenges faced when its resilience is critically tested.
Preliminary findings reveal trends in connectivity restoration, infrastructure
vulnerabilities, and user behavior changes. These datasets and pre-analysis aim
to support future research on disaster recovery strategies and the development
of robust telecommunications systems.

</details>
