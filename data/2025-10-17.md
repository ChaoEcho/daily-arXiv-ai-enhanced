<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 58]
- [cs.CV](#cs.CV) [Total: 55]
- [cs.AI](#cs.AI) [Total: 52]
- [cs.LG](#cs.LG) [Total: 56]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.HC](#cs.HC) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL](https://arxiv.org/abs/2510.13827)
*Ashish Kattamuri,Ishita Prasad,Meetu Malhotra,Arpita Vats,Rahul Raja,Albert Lie*

Main category: cs.CL

TL;DR: 本文提出一种结合组相对策略优化（GRPO）和多语言对比奖励信号的新框架，显著提升了跨语言Text-to-SQL系统的语义对齐和执行准确性，并证明小模型在少量数据下能超越大模型。


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-SQL方法过于关注可执行查询，忽视了查询语义和执行结果的语义对齐挑战，且在非英语语言中执行准确性显著下降（平均下降6个百分点）。

Method: 引入一个新框架，将组相对策略优化（GRPO）与多语言对比奖励信号相结合。通过基于语义相似性的奖励信号，增强模型在跨语言场景下SQL生成与用户意图的对应关系，以提升任务效率和语义准确性。

Result: 在七语言MultiSpider数据集上，经GRPO微调的LLaMA-3-3B模型将执行准确率提升至87.4%（相对零样本提升26个百分点），语义准确率提升至52.29%（提升32.86个百分点）。在GRPO框架中加入对比奖励信号后，平均语义准确率进一步提升至59.14%（提升6.85个百分点）。一个仅用3,000个强化学习训练样本的3B LLaMA模型，在执行准确性上超越了更大的8B零样本LLaMA模型（88.86% vs 81.43%），并在语义准确性上与其接近（59.14% vs 68.57%）。

Conclusion: 研究表明，通过使用对比奖励进行定向语义对齐，可以有效提升Text-to-SQL系统的性能，且无需大规模训练数据集。

Abstract: Current Text-to-SQL methods are evaluated and only focused on executable
queries, overlooking the semantic alignment challenge -- both in terms of the
semantic meaning of the query and the correctness of the execution results.
Even execution accuracy itself shows significant drops when moving from English
to other languages, with an average decline of 6 percentage points across
non-English languages. We address these challenges by presenting a new
framework that combines Group Relative Policy Optimization (GRPO) within a
multilingual contrastive reward signal to enhance both task efficiency and
semantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method
teaches models to obtain better correspondence between SQL generation and user
intent by combining a reward signal based on semantic similarity. On the
seven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO
improved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and
semantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive
reward signal in the GRPO framework further improved the average semantic
accuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our
experiments showcase that a smaller, parameter-efficient 3B LLaMA model
fine-tuned with our contrastive reward signal outperforms a much larger
zero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from
81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly
matches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using
just 3,000 reinforcement learning training examples. These results demonstrate
how we can improve the performance of Text-to-SQL systems with contrastive
rewards for directed semantic alignment, without requiring large-scale training
datasets.

</details>


### [2] [From Explainability to Action: A Generative Operational Framework for Integrating XAI in Clinical Mental Health Screening](https://arxiv.org/abs/2510.13828)
*Ratna Kandala,Akshata Kishore Moharir,Divya Arvinda Nayak*

Main category: cs.CL

TL;DR: 本文提出一个基于大型语言模型（LLM）的“生成式操作框架”，通过将XAI技术输出与临床指南结合，生成可操作的临床叙述，以弥合XAI在精神健康筛查中“实验室到临床”的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现有XAI技术（如SHAP、LIME）在精神健康筛查（MHS）中生成技术上忠实的输出，但缺乏临床相关性和可操作性，导致XAI在实际应用中存在“实验室到临床”的巨大鸿沟，阻碍了其落地。

Method: 本文提出“生成式操作框架”这一新颖的系统架构，利用大型语言模型（LLM）作为核心翻译引擎。该框架整合多种XAI工具的原始技术输出，并结合临床指南（通过RAG技术），自动生成人类可读、有证据支持的临床叙述。文章还系统分析了该框架集成的组件，追溯了从内在模型到生成式XAI的演变。

Result: 该框架直接解决了关键操作障碍，包括工作流集成、偏见缓解和针对不同利益相关者的沟通问题。它将该领域从生成孤立数据点推向在临床实践中提供集成、可操作和值得信赖的AI。

Conclusion: XAI在临床应用中的鸿沟是一个翻译问题。本文提出的生成式操作框架，通过LLM作为翻译引擎，有效弥合了技术透明度与人类效用之间的差距，并为在临床实践中实现集成、可操作和可信赖的AI提供了战略路线图。

Abstract: Explainable Artificial Intelligence (XAI) has been presented as the critical
component for unlocking the potential of machine learning in mental health
screening (MHS). However, a persistent lab-to-clinic gap remains. Current XAI
techniques, such as SHAP and LIME, excel at producing technically faithful
outputs such as feature importance scores, but fail to deliver clinically
relevant, actionable insights that can be used by clinicians or understood by
patients. This disconnect between technical transparency and human utility is
the primary barrier to real-world adoption. This paper argues that this gap is
a translation problem and proposes the Generative Operational Framework, a
novel system architecture that leverages Large Language Models (LLMs) as a
central translation engine. This framework is designed to ingest the raw,
technical outputs from diverse XAI tools and synthesize them with clinical
guidelines (via RAG) to automatically generate human-readable, evidence-backed
clinical narratives. To justify our solution, we provide a systematic analysis
of the components it integrates, tracing the evolution from intrinsic models to
generative XAI. We demonstrate how this framework directly addresses key
operational barriers, including workflow integration, bias mitigation, and
stakeholder-specific communication. This paper also provides a strategic
roadmap for moving the field beyond the generation of isolated data points
toward the delivery of integrated, actionable, and trustworthy AI in clinical
practice.

</details>


### [3] [A Linguistics-Aware LLM Watermarking via Syntactic Predictability](https://arxiv.org/abs/2510.13829)
*Shinwoo Park,Hyejin Park,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: STELA是一个无需访问模型对数即可进行公开验证的水印框架，通过根据语言自由度动态调整水印强度，在多种语言上实现了比现有方法更强的检测鲁棒性，同时保持文本质量。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的快速发展，急需可靠的治理工具，特别是可公开验证的水印技术以建立可信的AI生态系统。当前主要挑战在于平衡文本质量与检测鲁棒性，且现有方法依赖模型输出分布（如对数），阻碍了公开验证。

Method: 本文提出了STELA框架，它将水印强度与语言固有的语言自由度对齐。STELA使用词性（POS）n-gram建模的语言不确定性动态调整水印信号：在语法受限的上下文中减弱信号以保持文本质量，在语言灵活性更大的上下文中增强信号以提高可检测性。其检测器无需访问任何模型对数，从而实现公开可验证的检测。

Result: 通过对分析型英语、孤立型汉语和黏着型韩语等多种类型语言的广泛实验，STELA在检测鲁棒性方面超越了现有方法。

Conclusion: STELA框架通过动态调整水印强度，成功解决了文本质量与检测鲁棒性之间的权衡，并克服了现有方法对模型对数的依赖，实现了可公开验证且更强大的水印检测能力，为构建可信的AI生态系统提供了关键工具。

Abstract: As large language models (LLMs) continue to advance rapidly, reliable
governance tools have become critical. Publicly verifiable watermarking is
particularly essential for fostering a trustworthy AI ecosystem. A central
challenge persists: balancing text quality against detection robustness. Recent
studies have sought to navigate this trade-off by leveraging signals from model
output distributions (e.g., token-level entropy); however, their reliance on
these model-specific signals presents a significant barrier to public
verification, as the detection process requires access to the logits of the
underlying model. We introduce STELA, a novel framework that aligns watermark
strength with the linguistic degrees of freedom inherent in language. STELA
dynamically modulates the signal using part-of-speech (POS) n-gram-modeled
linguistic indeterminacy, weakening it in grammatically constrained contexts to
preserve quality and strengthen it in contexts with greater linguistic
flexibility to enhance detectability. Our detector operates without access to
any model logits, thus facilitating publicly verifiable detection. Through
extensive experiments on typologically diverse languages-analytic English,
isolating Chinese, and agglutinative Korean-we show that STELA surpasses prior
methods in detection robustness. Our code is available at
https://github.com/Shinwoo-Park/stela_watermark.

</details>


### [4] [Users as Annotators: LLM Preference Learning from Comparison Mode](https://arxiv.org/abs/2510.13830)
*Zhongze Cai,Xiaocheng Li*

Main category: cs.CL

TL;DR: 本文提出一种利用非对称模型响应和EM算法从用户注释中推断用户数据质量并过滤数据的方法，以提高LLM对齐的偏好数据质量。


<details>
  <summary>Details</summary>
Motivation: Pairwise偏好数据对LLM对齐至关重要，传统依赖专业人工标注。随着LLM普及，用户注释成为一种替代数据收集方式，其优势在于用户对其自身查询的响应判断最为专业，但缺点是缺乏质量控制。本文旨在解决用户注释数据的质量问题。

Method: 提出通过两个不同模型或同一模型不同版本生成非对称响应，并利用这种不对称性通过提出的用户行为模型推断用户的数据质量。开发了期望最大化（EM）算法来估计用户的潜在质量因子，并据此过滤用户标注数据。

Result: 下游任务表明，所提出的方法在捕获用户行为和过滤LLM对齐数据方面均有效。

Conclusion: 该方法能够有效解决用户注释数据的质量控制问题，通过估计用户质量因子并过滤数据，为LLM对齐提供高质量的偏好数据。

Abstract: Pairwise preference data have played an important role in the alignment of
large language models (LLMs). Each sample of such data consists of a prompt,
two different responses to the prompt, and a binary label indicating which of
the two responses is better. The labels are usually annotated by professional
human annotators. In this paper, we consider an alternative approach to collect
pairwise preference data -- user annotation from comparison mode. With the
increasingly wider adoption of LLMs among the population, users are
contributing more and more of their preference labels through their daily
interactions with the LLMs. The upside of such labels is that users are the
best experts in judging the responses to their own queries/prompts, but the
downside is the lack of quality control in these labels. In this paper, we
consider a new idea of generating two responses from two different models or
two different versions of the same model. The asymmetry allows us to make an
inference of the user's data quality through our proposed user behavior model.
We develop an expectation-maximization algorithm to estimate a latent quality
factor of the user, and filter users' annotation data accordingly. The
downstream task shows the effectiveness of our approach in both capturing the
user behavior and data filtering for LLM alignment.

</details>


### [5] [Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference](https://arxiv.org/abs/2510.13831)
*Chao Han,Yijuan Liang,Zihao Xuan,Daokuan Wu,Wei Zhang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 本文提出“知情路由”新范式，通过评估token的重要性与可恢复性并结合轻量级预测器，实现灵活的计算策略，显著降低大型语言模型（LLM）推理成本和训练时间，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的高推理成本严重限制了其在实际应用中的部署。现有动态token级计算分配方法（贪婪路由）存在信息不可逆损失和次优token选择的问题。

Method: 引入“知情路由”范式，核心思想是不仅评估token的即时重要性，还要评估其可恢复性（即其转换可被近似的程度）。为此，提出了轻量级特征预测器（LFF），一个小型预测模块，用于在路由决策前估计单元输出，从而实现灵活的“执行或近似”策略，大幅减少计算并保留模型保真度。

Result: 在语言建模和推理任务上，知情路由在多种稀疏度下实现了最先进的效率-性能权衡。即使未进行LoRA微调，该方法也能匹配或超越需要完全微调的强大基线，并减少超过50%的训练时间。

Conclusion: 知情路由通过评估token重要性和可恢复性，并采用灵活的计算策略，有效解决了LLM高推理成本和现有方法效率低下的问题，实现了卓越的效率-性能平衡，并显著缩短了训练时间。

Abstract: The deployment of large language models (LLMs) in real-world applications is
increasingly limited by their high inference cost. While recent advances in
dynamic token-level computation allocation attempt to improve efficiency by
selectively activating model components per token, existing methods rely on
greedy routing--a myopic execute-or-skip mechanism that often leads to
irreversible information loss and suboptimal token selection. This paper
introduces informed routing, a new paradigm that proactively addresses these
issues. The key insight is to assess not only a token's immediate importance
but also its recoverability, i.e., how well its transformation can be
approximated. To this end, we propose the Lightweight Feature Forecaster (LFF),
a small predictive module that estimates a unit's output before routing
decisions are made. This enables a flexible execute-or-approximate policy that
preserves model fidelity while drastically reducing computation. Extensive
experiments on both language modeling and reasoning tasks show that informed
routing achieves state-of-the-art efficiency-performance trade-offs across
multiple sparsity levels. Notably, even without final LoRA fine-tuning, our
method matches or surpasses strong baselines that require full fine-tuning, all
while reducing training time by over 50%. The code is available at:
https://github.com/EIT-NLP/informed-routing

</details>


### [6] [Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning](https://arxiv.org/abs/2510.13832)
*Minsik Choi,Hyegang Son,Changhoon Kim,Young Geun Kim*

Main category: cs.CL

TL;DR: 本文提出一种新的Transformer注意力头剪枝准则HIES（Head Importance-Entropy Score），结合头部重要性得分和注意力熵，显著提升了剪枝模型的质量和稳定性。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在NLP任务中表现出色，但其多层和注意力头的结构导致推理和部署效率低下。现有基于梯度的头部重要性得分（HIS）方法虽然可解释且有效，但仅捕获梯度驱动的贡献，忽略了注意力模式的多样性，存在局限性。

Method: 我们引入了HIES（Head Importance-Entropy Score）作为新的剪枝准则。HIES将头部重要性得分与注意力熵相结合，为每个头部的贡献提供了互补的证据。

Result: 基于HIES的剪枝方法在模型质量上比仅使用HIS的方法提高了15.2%，在稳定性上提高了2.04倍。这使得模型在不牺牲准确性或稳定性的前提下实现显著压缩。

Conclusion: HIES作为一种结合头部重要性得分和注意力熵的剪枝准则，能更有效地识别冗余注意力头，从而在模型压缩中实现更好的性能、质量和稳定性。

Abstract: Transformer-based models have achieved remarkable performance in NLP tasks.
However, their structural characteristics-multiple layers and attention
heads-introduce efficiency challenges in inference and deployment. To address
these challenges, various pruning methods have recently been proposed. Notably,
gradient-based methods using Head Importance Scores (HIS) have gained traction
for interpretability, efficiency, and ability to identify redundant heads.
However, HIS alone has limitations as it captures only the gradient-driven
contribution, overlooking the diversity of attention patterns. To overcome
these limitations, we introduce a novel pruning criterion, HIES (Head
Importance-Entropy Score), which integrates head importance scores with
attention entropy, providing complementary evidence on per-head contribution.
Empirically, HIES-based pruning yields up to 15.2% improvement in model quality
and 2.04x improvement in stability over HIS-only methods, enabling substantial
model compression without sacrificing either accuracy or stability. Code will
be released upon publication.

</details>


### [7] [ConDABench: Interactive Evaluation of Language Models for Data Analysis](https://arxiv.org/abs/2510.13835)
*Avik Dutta,Priyanshu Gupta,Hosein Hasanbeig,Rahul Pratap Singh,Harshit Nigam,Sumit Gulwani,Arjun Radhakrishna,Gustavo Soares,Ashish Tiwari*

Main category: cs.CL

TL;DR: 本文介绍了ConDABench，一个用于生成和评估会话式数据分析（ConDA）基准的框架，旨在解决现有LLM数据分析评估中互动性和复杂性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的数据分析任务通常目标不明确且数据不干净，需要用户交互来理解意图。现有评估LLM数据分析能力的基准未能捕捉这些复杂性或提供对交互性的支持。

Method: ConDABench包含一个多智能体工作流，用于从描述公共数据集见解的文章中生成真实的基准；生成了1,420个ConDA问题；并提供了一个评估工具，首次能够系统地评估会话式数据分析工具。

Result: 对最先进LLM的评估显示，新一代模型虽然能解决更多实例，但在需要持续、长时间参与的任务上表现不一定更好。

Conclusion: ConDABench为模型开发者提供了一个衡量构建真正协作模型进展的途径，这些模型能够完成复杂的交互式任务。

Abstract: Real-world data analysis tasks often come with under-specified goals and
unclean data. User interaction is necessary to understand and disambiguate a
user's intent, and hence, essential to solving these complex tasks. Existing
benchmarks for evaluating LLMs on data analysis tasks do not capture these
complexities or provide first-class support for interactivity. We introduce
ConDABench, a framework for generating conversational data analysis (ConDA)
benchmarks and evaluating external tools on the generated benchmarks. \bench
consists of (a) a multi-agent workflow for generating realistic benchmarks from
articles describing insights gained from public datasets, (b) 1,420 ConDA
problems generated using this workflow, and (c) an evaluation harness that, for
the first time, makes it possible to systematically evaluate conversational
data analysis tools on the generated ConDA problems. Evaluation of
state-of-the-art LLMs on the benchmarks reveals that while the new generation
of models are better at solving more instances, they are not necessarily better
at solving tasks that require sustained, long-form engagement. ConDABench is an
avenue for model builders to measure progress towards truly collaborative
models that can complete complex interactive tasks.

</details>


### [8] [SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2510.13836)
*Debarun Bhattacharjya,Balaji Ganesan,Junkyu Lee,Radu Marinescu,Katsiaryna Mirylenka,Michael Glass,Xiao Shou*

Main category: cs.CL

TL;DR: 本文提出了一种主要为黑盒的、基于生成输出相似度聚合的不确定性量化（UQ）框架，用以估计大型语言模型（LLM）的置信度，实验证明其在多种任务上能提供比基线更好的置信度校准。


<details>
  <summary>Details</summary>
Motivation: 为了构建可信的AI系统，大型语言模型（LLM）需要了解自身的不确定性。不确定性量化（UQ）是实现这一目标的关键。黑盒UQ方法因其在实际应用中的鲁棒性、适应性、低成本和计算效率等优势而受到重视。

Method: 研究了一种主要但不完全是黑盒的UQ技术，该技术将LLM生成输出与其它采样生成之间的一致性用作置信度的代理。提出了一种高级的非语言化基于相似度的聚合框架，涵盖了多种UQ方法，并引入了使用少量训练集训练置信度估计模型的新技术。

Result: 通过在问答、摘要和文本到SQL等多样化任务数据集上的实证研究表明，所提出的基于相似度的方法能够比基线方法产生更好的校准置信度。

Conclusion: 所提出的基于相似度、主要为黑盒的不确定性量化方法，能够有效提高大型语言模型在复杂生成任务中的置信度校准。

Abstract: When does a large language model (LLM) know what it does not know?
Uncertainty quantification (UQ) provides measures of uncertainty, such as an
estimate of the confidence in an LLM's generated output, and is therefore
increasingly recognized as a crucial component of trusted AI systems. Black-box
UQ methods do not require access to internal model information from the
generating LLM and therefore have numerous real-world advantages, such as
robustness to system changes, adaptability to choice of LLM, reduced costs, and
computational tractability. In this paper, we investigate the effectiveness of
UQ techniques that are primarily but not necessarily entirely black-box, where
the consistency between a generated output and other sampled generations is
used as a proxy for confidence in its correctness. We propose a high-level
non-verbalized similarity-based aggregation framework that subsumes a broad
swath of UQ approaches suitable for complex generative tasks, as well as
introduce specific novel techniques from the framework that train confidence
estimation models using small training sets. Through an empirical study with
datasets spanning the diverse tasks of question answering, summarization, and
text-to-SQL, we demonstrate that our proposed similarity-based methods can
yield better calibrated confidences than baselines.

</details>


### [9] [Seeing Hate Differently: Hate Subspace Modeling for Culture-Aware Hate Speech Detection](https://arxiv.org/abs/2510.13837)
*Weibin Cai,Reza Zafarani*

Main category: cs.CL

TL;DR: 针对仇恨言论检测中标签偏见和文化差异问题，提出一个文化感知框架，通过建模文化属性组合和标签传播构建个体仇恨子空间，有效提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论检测方法忽略了训练标签的偏见以及不同文化背景个体对仇恨的解释差异，同时存在数据稀疏性、文化纠缠和模糊标签等挑战。

Method: 提出一个文化感知框架，构建个体仇恨子空间。通过建模文化属性组合来缓解数据稀疏性，并利用标签传播捕获每种组合的独特特征，以处理文化纠缠和模糊标签问题。

Result: 实验结果显示，所提出的方法在所有指标上平均优于现有最先进方法1.05%。

Conclusion: 该文化感知框架能够有效解决仇恨言论检测中的文化差异、标签偏见及数据稀疏性问题，并通过构建个体仇恨子空间显著提升了分类性能。

Abstract: Hate speech detection has been extensively studied, yet existing methods
often overlook a real-world complexity: training labels are biased, and
interpretations of what is considered hate vary across individuals with
different cultural backgrounds. We first analyze these challenges, including
data sparsity, cultural entanglement, and ambiguous labeling. To address them,
we propose a culture-aware framework that constructs individuals' hate
subspaces. To alleviate data sparsity, we model combinations of cultural
attributes. For cultural entanglement and ambiguous labels, we use label
propagation to capture distinctive features of each combination. Finally,
individual hate subspaces, which in turn can further enhance classification
performance. Experiments show our method outperforms state-of-the-art by 1.05\%
on average across all metrics.

</details>


### [10] [Meronymic Ontology Extraction via Large Language Models](https://arxiv.org/abs/2510.13839)
*Dekai Zhang,Simone Conia,Antonio Rago*

Main category: cs.CL

TL;DR: 本文提出一种利用大型语言模型（LLM）从原始评论文本中自动提取产品本体（分体论）的方法，并证明其性能优于现有基于BERT的基线。


<details>
  <summary>Details</summary>
Motivation: 本体论在组织非结构化文本方面至关重要，尤其是在电商等领域。然而，手动构建本体论耗时、昂贵且费力。

Method: 利用大型语言模型（LLM）的最新进展，开发了一种全自动方法，从原始评论文本中提取产品本体，形式为分体论（meronymies）。

Result: 研究表明，使用“LLM作为评判者”进行评估时，该方法生成的本体论超越了现有的基于BERT的基线方法。

Conclusion: 本研究为LLMs更广泛地应用于（无论是产品还是其他领域的）本体论提取奠定了基础。

Abstract: Ontologies have become essential in today's digital age as a way of
organising the vast amount of readily available unstructured text. In providing
formal structure to this information, ontologies have immense value and
application across various domains, e.g., e-commerce, where countless product
listings necessitate proper product organisation. However, the manual
construction of these ontologies is a time-consuming, expensive and laborious
process. In this paper, we harness the recent advancements in large language
models (LLMs) to develop a fully-automated method of extracting product
ontologies, in the form of meronymies, from raw review texts. We demonstrate
that the ontologies produced by our method surpass an existing, BERT-based
baseline when evaluating using an LLM-as-a-judge. Our investigation provides
the groundwork for LLMs to be used more generally in (product or otherwise)
ontology extraction.

</details>


### [11] [ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking](https://arxiv.org/abs/2510.13842)
*Yutao Wu,Xiao Liu,Yinghui Li,Yifeng Gao,Yifan Ding,Jiale Ding,Xiang Zheng,Xingjun Ma*

Main category: cs.CL

TL;DR: 本文提出ADMIT，一种在事实核查场景下，针对RAG系统的知识投毒攻击。ADMIT在极低的投毒率下，无需访问目标模型，即可有效翻转事实核查决策并诱导欺骗性解释，显著暴露出RAG系统在真实世界中的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 知识投毒对检索增强生成（RAG）系统构成严重威胁，特别是通过注入对抗性内容，使大型语言模型（LLMs）产生攻击者控制的输出。之前的研究指出了LLMs易受误导性内容影响，但真实世界的事实核查场景更具挑战性，因为可信证据通常在检索结果中占主导地位。因此，需要研究在存在真实支持或反驳证据的情况下，知识投毒如何影响事实核查。

Method: 研究人员将知识投毒扩展到事实核查场景，并提出了ADMIT（Adversarial Multi-Injection Technique），这是一种少样本、语义对齐的投毒攻击。该方法旨在翻转事实核查决策并诱导欺骗性解释，所有操作均在无法访问目标LLM、检索器或令牌级控制的情况下进行。

Result: ADMIT在4种检索器、11个LLM和4个跨领域基准测试中有效转移，平均攻击成功率（ASR）达到86%，投毒率极低（0.93 x 10^-6）。即使在存在强反驳证据的情况下，ADMIT仍保持鲁棒性。与之前的最先进攻击相比，ADMIT在所有设置下将ASR提高了11.2%。

Conclusion: ADMIT攻击暴露了真实世界中基于RAG的事实核查系统存在的显著漏洞，凸显了在复杂、证据丰富的场景中，这些系统对知识投毒的脆弱性。

Abstract: Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation
(RAG) systems by injecting adversarial content into knowledge bases, tricking
Large Language Models (LLMs) into producing attacker-controlled outputs
grounded in manipulated context. Prior work highlights LLMs' susceptibility to
misleading or malicious retrieved content. However, real-world fact-checking
scenarios are more challenging, as credible evidence typically dominates the
retrieval pool. To investigate this problem, we extend knowledge poisoning to
the fact-checking setting, where retrieved context includes authentic
supporting or refuting evidence. We propose \textbf{ADMIT}
(\textbf{AD}versarial \textbf{M}ulti-\textbf{I}njection \textbf{T}echnique), a
few-shot, semantically aligned poisoning attack that flips fact-checking
decisions and induces deceptive justifications, all without access to the
target LLMs, retrievers, or token-level control. Extensive experiments show
that ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4
cross-domain benchmarks, achieving an average attack success rate (ASR) of 86\%
at an extremely low poisoning rate of $0.93 \times 10^{-6}$, and remaining
robust even in the presence of strong counter-evidence. Compared with prior
state-of-the-art attacks, ADMIT improves ASR by 11.2\% across all settings,
exposing significant vulnerabilities in real-world RAG-based fact-checking
systems.

</details>


### [12] [Serialized EHR make for good text representations](https://arxiv.org/abs/2510.13843)
*Zhirong Chou,Quan Qin,Shi Li*

Main category: cs.CL

TL;DR: SerialBEHRT是一种针对EHR的领域对齐基础模型，通过在结构化EHR序列上预训练，有效捕获纵向依赖，并在抗生素敏感性预测任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型难以将EHR的表格和事件特性与自然语言模型的序列先验匹配，从而限制了其捕获患者就诊间纵向依赖的能力。

Method: 引入SerialBEHRT，一个领域对齐基础模型，通过在结构化EHR序列上进行额外预训练来扩展SciBERT，旨在编码临床事件的时间和上下文关系，生成更丰富的患者表示。

Result: 在抗生素敏感性预测任务中，SerialBEHRT与现有最先进的EHR表示策略相比，取得了更优越且更一致的性能。

Conclusion: 研究结果强调了时间序列化在医疗领域基础模型预训练中的重要性。

Abstract: The emergence of foundation models in healthcare has opened new avenues for
learning generalizable representations from large scale clinical data. Yet,
existing approaches often struggle to reconcile the tabular and event based
nature of Electronic Health Records (EHRs) with the sequential priors of
natural language models. This structural mismatch limits their ability to
capture longitudinal dependencies across patient encounters. We introduce
SerialBEHRT, a domain aligned foundation model that extends SciBERT through
additional pretraining on structured EHR sequences. SerialBEHRT is designed to
encode temporal and contextual relationships among clinical events, thereby
producing richer patient representations. We evaluate its effectiveness on the
task of antibiotic susceptibility prediction, a clinically meaningful problem
in antibiotic stewardship. Through extensive benchmarking against state of the
art EHR representation strategies, we demonstrate that SerialBEHRT achieves
superior and more consistent performance, highlighting the importance of
temporal serialization in foundation model pretraining for healthcare.

</details>


### [13] [An LLM-Powered AI Agent Framework for Holistic IoT Traffic Interpretation](https://arxiv.org/abs/2510.13925)
*Daniel Adu Worae,Spyridon Mastorakis*

Main category: cs.CL

TL;DR: 本文提出一个LLM驱动的AI代理框架，用于将物联网（IoT）原始数据包捕获转换为结构化、语义丰富的表示，实现交互式分析。


<details>
  <summary>Details</summary>
Motivation: 物联网网络产生多样且高容量的流量，需要对行为、协议和上下文进行跨层解释，而非孤立检测，以从中获取有意义的洞察并识别潜在威胁。

Method: 该框架集成了特征提取、基于Transformer的异常检测、数据包和流摘要、威胁情报丰富以及检索增强问答（RAG）。AI代理由大型语言模型（LLM）指导，对索引流量证据进行推理，并采用混合检索（结合词法和语义搜索与重排序）。

Result: 在多个IoT捕获和六个开源模型上的实验评估显示，混合检索显著提升了BLEU、ROUGE、METEOR和BERTScore结果。系统分析表明CPU、GPU和内存开销较低。

Conclusion: 该框架实现了对物联网网络流量的整体和高效解释，并提供了准确且人类可读的分析结果。

Abstract: Internet of Things (IoT) networks generate diverse and high-volume traffic
that reflects both normal activity and potential threats. Deriving meaningful
insight from such telemetry requires cross-layer interpretation of behaviors,
protocols, and context rather than isolated detection. This work presents an
LLM-powered AI agent framework that converts raw packet captures into
structured and semantically enriched representations for interactive analysis.
The framework integrates feature extraction, transformer-based anomaly
detection, packet and flow summarization, threat intelligence enrichment, and
retrieval-augmented question answering. An AI agent guided by a large language
model performs reasoning over the indexed traffic artifacts, assembling
evidence to produce accurate and human-readable interpretations. Experimental
evaluation on multiple IoT captures and six open models shows that hybrid
retrieval, which combines lexical and semantic search with reranking,
substantially improves BLEU, ROUGE, METEOR, and BERTScore results compared with
dense-only retrieval. System profiling further indicates low CPU, GPU, and
memory overhead, demonstrating that the framework achieves holistic and
efficient interpretation of IoT network traffic.

</details>


### [14] [DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models](https://arxiv.org/abs/2510.13847)
*Jinbin Zhang,Nasib Ullah,Erik Schultheis,Rohit Babbar*

Main category: cs.CL

TL;DR: 为解决大模型推理中推测解码器因词汇量扩大导致的延迟瓶颈，本文提出DynaSpec。DynaSpec是一种上下文相关的动态短列表机制，通过轻量级元分类器根据上下文选择词元簇，构建起草模型的动态短列表，同时保留目标模型的全词汇量验证。该方法提高了平均接受长度，并允许使用更小的短列表而不降低性能。


<details>
  <summary>Details</summary>
Motivation: 推测解码加速LLM推理时，随着词汇量增加，起草模型的输出头（O(|V|d)参数）成为延迟瓶颈。现有固定短列表方法（如FR-Spec, VocabTrim）依赖语料库且限制稀有词元，导致其脆弱性、泛化性差并降低每次验证的预期词元数量。

Method: 本文提出DynaSpec，一种上下文相关的动态短列表机制。它引入轻量级、粗粒度的元分类器，将上下文路由到少量词元簇。这些选定簇的并集构成起草模型的短列表，而验证阶段仍使用目标模型的完整词汇表以确保准确性。元分类器的计算与草稿编码并行执行，提前完成。

Result: 在标准推测解码基准测试中，DynaSpec在平均接受长度方面比固定短列表基线持续取得提升。上下文相关的选择允许使用更小的短列表，而不会降低接受度。这表明该方法更稳健、加速了起草过程，并能泛化到多样化的任务。

Conclusion: DynaSpec通过其上下文依赖的动态短列表机制，有效解决了推测解码中起草模型的词汇量瓶颈。它在提升平均接受长度和允许更小短列表的同时，保持了准确性，并展现出更好的鲁棒性和泛化能力。

Abstract: Speculative decoding (a.k.a. speculative sampling) has become a standard way
to accelerate LLM inference: a small drafter proposes multiple tokens and a
large target model verifies them once per speculation length. Recently, scaling
of the LLM vocabulary has pushed the number of tokens to grow substantially.
While verification over the full vocabulary leaves the target model largely
unaffected, the O(|V|d) parameters in the drafter's output head become a
latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g.,
FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed subset of the
target model's vocabulary, ranked in descending order of token frequency.
Although this reduces draft-time compute, it is brittle, since: (i) frequency
lists are corpus-dependent and require retuning to generalize, and (ii) static
shortlists suppress rare or domain-specific tokens, lowering the expected
number of tokens per verification step. We propose DynaSpec, a
context-dependent dynamic shortlisting mechanism that is robust, speeds up
drafting, and generalizes across diverse tasks. Concretely, we introduce
lightweight, coarse-grained meta-classifiers that route contexts to a small
number of token clusters; the union of the top-k selected clusters forms the
drafter's shortlist, while verification retains the full vocabulary and
exactness. The meta-classifier finishes its computation earlier than the
drafter's hidden state generation by exploiting parallel execution of draft
encoding and meta shortlisting on separate streams. On standard
speculative-decoding benchmarks, we observe consistent gains in mean accepted
length over fixed-shortlist baselines, while context-dependent selection
enables smaller shortlists without degrading acceptance.

</details>


### [15] [On-device System of Compositional Multi-tasking in Large Language Models](https://arxiv.org/abs/2510.13848)
*Ondrej Bohdal,Konstantinos Theodosiadis,Asterios Mpatziakas,Dimitris Filippidis,Iro Spyrou,Christos Zonios,Anastasios Drosou,Dimosthenis Ioannidis,Kyeng-Hun Lee,Jijoong Moon,Hyeonmok Ko,Mete Ozay,Umberto Michieli*

Main category: cs.CL

TL;DR: 本文提出了一种针对大语言模型组合多任务（如翻译摘要）的新方法。通过在组合适配器之上添加可学习投影层，实现了高效集成，并在云端和设备端都表现出良好的性能和速度。


<details>
  <summary>Details</summary>
Motivation: 虽然参数高效微调（如LoRA）能让大语言模型处理多种下游任务，但标准方法难以同时执行复杂的组合任务，例如从长对话中生成翻译摘要。

Method: 针对摘要和翻译等组合多任务场景，本文提出了一种新方法。该方法在组合的摘要和翻译适配器之上添加一个可学习的投影层，以实现有效集成并降低计算开销。通过开发一个Android应用，演示了该方法在设备环境中的实际可行性。

Result: 实验结果表明，该解决方案在云端和设备端实现中均表现出色，且运行速度快。

Conclusion: 该框架在要求高速操作和资源受限的真实世界应用中具有显著优势，尤其适合设备端环境下的组合多任务处理。

Abstract: Large language models (LLMs) are commonly adapted for diverse downstream
tasks via parameter-efficient fine-tuning techniques such as Low-Rank Adapters
(LoRA). While adapters can be combined to handle multiple tasks separately,
standard approaches struggle when targeting the simultaneous execution of
complex tasks, such as generating a translated summary from a long
conversation. To address this challenge, we propose a novel approach tailored
specifically for compositional multi-tasking scenarios involving summarization
and translation. Our technique involves adding a learnable projection layer on
top of the combined summarization and translation adapters. This design enables
effective integration while maintaining efficiency through reduced
computational overhead compared to alternative strategies requiring extensive
retraining or sequential processing. We demonstrate the practical viability of
our method within an on-device environment by developing an Android app capable
of executing compositional tasks seamlessly. Experimental results indicate our
solution performs well and is fast in both cloud-based and on-device
implementations, highlighting the potential benefits of adopting our framework
in real-world applications demanding high-speed operation alongside resource
constraints.

</details>


### [16] [Language steering in latent space to mitigate unintended code-switching](https://arxiv.org/abs/2510.13849)
*Andrey Goncharov,Nikolai Kondusov,Alexey Zaytsev*

Main category: cs.CL

TL;DR: 提出一种轻量级推断时方法，通过在多语言LLM的潜在空间中引导语言方向来减少代码切换，同时保持语义。


<details>
  <summary>Details</summary>
Motivation: 多语言大型语言模型（LLMs）常出现意料之外的代码切换，从而降低下游任务的可靠性。

Method: 采用潜在空间语言引导方法，利用主成分分析（PCA）在并行翻译数据上识别语言方向，并通过沿这些轴引导token嵌入来控制语言身份。

Result: 经验证，使用单个主成分可实现95-99%的语言分类准确率，在Qwen2.5和Llama-3.2模型上，跨多个语言对将下一token分布散度降低高达42%。该方法在保持语义的同时，计算开销可忽略不计，仅需少量并行数据进行校准。分析还显示语言身份集中在最终层，具有近乎完美的线性可分性。

Conclusion: 该方法有效缓解了多语言LLMs中的代码切换问题，且计算开销极低，显著提高了模型的语言控制能力和可靠性。

Abstract: Multilingual Large Language Models (LLMs) often exhibit unintended
code-switching, reducing reliability in downstream tasks. We propose
latent-space language steering, a lightweight inference-time method that
identifies language directions via PCA on parallel translations and steers
token embeddings along these axes to control language identity. Our approach
mitigates code-switching while preserving semantics with negligible
computational overhead and requires only minimal parallel data for calibration.
Empirically, we achieve 95-99\% language classification accuracy using a single
principal component and reduce next-token distributional divergence by up to
42% across multiple language pairs on Qwen2.5 and Llama-3.2 models. We further
analyze the layer-wise evolution of language representations, revealing that
language identity concentrates in final layers with near-perfect linear
separability.

</details>


### [17] [Revisiting the UID Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.13850)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.CL

TL;DR: 研究发现，大语言模型（LLM）的成功推理在信息密度上表现出全局非均匀性，这与人类交流模式相反，挑战了对机器推理的传统假设。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）的链式思维（CoT）推理中间步骤常不可靠或难以解释。

Method: 受心理语言学中统一信息密度（UID）假说的启发，引入了基于熵的度量来分析推理轨迹中的信息流。

Result: 在三个数学基准测试中，成功推理的LLM表现出全局非均匀性：正确解决方案的特点是信息密度波动不均，这与人类交流模式形成鲜明对比。

Conclusion: 该结果挑战了关于机器推理的假设，并为设计可解释和自适应的推理模型提供了新方向。

Abstract: Large language models (LLMs) often solve problems using step-by-step
Chain-of-Thought (CoT) reasoning, yet these intermediate steps are frequently
unfaithful or hard to interpret. Inspired by the Uniform Information Density
(UID) hypothesis in psycholinguistics -- which posits that humans communicate
by maintaining a stable flow of information -- we introduce entropy-based
metrics to analyze the information flow within reasoning traces. Surprisingly,
across three challenging mathematical benchmarks, we find that successful
reasoning in LLMs is globally non-uniform: correct solutions are characterized
by uneven swings in information density, in stark contrast to human
communication patterns. This result challenges assumptions about machine
reasoning and suggests new directions for designing interpretable and adaptive
reasoning models.

</details>


### [18] [EvoEdit: Evolving Null-space Alignment for Robust and Efficient Knowledge Editing](https://arxiv.org/abs/2510.13851)
*Sicheng Lyu,Yu Gu,Xinyu Wang,Jerry Huang,Sitao Luan,Yufei Cui,Xiao-Wen Chang,Peng Lu*

Main category: cs.CL

TL;DR: 针对LLM在序列知识编辑中遇到的灾难性遗忘问题，本文提出EvoEdit策略，通过序列空空间对齐有效缓解干扰，实现稳定高效的模型编辑，性能优于或媲美现有SOTA方法并大幅加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）需要持续更新知识以纠正过时或错误信息。现有模型编辑方法（基于定位-编辑框架）在进行序列编辑时，会遇到严重的灾难性干扰，即新的编辑会损害之前集成的信息并降低原有知识的保持度。

Method: 本文提出EvoEdit，一种通过序列空空间对齐（sequential null-space alignment）来缓解灾难性干扰的新型编辑策略。EvoEdit通过对每个传入的编辑执行序列空空间对齐，有效保留原始和先前修改的知识表示，确保即使在长编辑序列中也能保持对已保存知识的输出不变性，从而减轻干扰。

Result: 在真实世界的序列知识编辑基准测试中，EvoEdit表现出与现有最先进的定位-编辑技术相当或更优的性能，并实现了高达3.53倍的速度提升。

Conclusion: 这些结果强调了在动态演变的信息环境中开发更合理LLM设计方法的必要性。EvoEdit提供了一个简单但有效的解决方案，具有强大的理论保证，可实现稳定高效的模型编辑。

Abstract: Large language models (LLMs) require continual updates to rectify outdated or
erroneous knowledge. Model editing has emerged as a compelling paradigm for
introducing targeted modifications without the computational burden of full
retraining. Existing approaches are mainly based on a locate-then-edit
framework. However, in sequential editing contexts, where multiple updates are
applied over time, they exhibit significant limitations and suffer from
catastrophic interference, i.e., new edits compromise previously integrated
updates and degrade preserved knowledge. To address these challenges, we
introduce EvoEdit, a novel editing strategy that mitigates catastrophic
interference through sequential null-space alignment, enabling stable and
efficient model editing. By performing sequential null-space alignment for each
incoming edit, EvoEdit preserves both original and previously modified
knowledge representations and maintains output invariance on preserved
knowledge even across long edit sequences, effectively mitigating interference.
Evaluations on real-world sequential knowledge-editing benchmarks show that
EvoEdit achieves better or comparable performance than prior state-of-the-art
locate-then-edit techniques, with up to 3.53 times speedup. Overall, these
results underscore the necessity of developing more principled approaches for
designing LLMs in dynamically evolving information settings, while providing a
simple yet effective solution with strong theoretical guarantees.

</details>


### [19] [ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups](https://arxiv.org/abs/2510.13852)
*Peter Banyas,Shristi Sharma,Alistair Simmons,Atharva Vispute*

Main category: cs.CL

TL;DR: 本文引入了ConsistencyAI，这是一个独立的基准测试，用于衡量大型语言模型（LLMs）在不同用户画像下事实一致性。研究发现LLMs对不同用户画像给出不同的事实，其一致性受模型提供商和主题共同影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究LLMs是否会向不同用户群体提供不一致的事实信息，并缺乏一个独立的、公正的基准来评估这种事实一致性。

Method: 研究开发了ConsistencyAI基准。它向19个LLM模型就15个主题提问，每个主题要求提供5个事实，并为每个LLM重复查询100次，每次使用不同的用户画像作为提示上下文。然后将回复处理成句子嵌入，计算跨画像的余弦相似度，并加权平均得到事实一致性分数。

Result: 在100个画像的实验中，一致性得分范围从0.9065到0.7896，平均值为0.8656。xAI的Grok-3最一致，而一些轻量级模型排名最低。一致性因主题而异：就业市场最不一致，G7世界领导人最一致，疫苗或以巴冲突等问题则因提供商而异。结果表明，提供商和主题都会影响事实一致性。

Conclusion: LLMs在不同用户画像下存在事实不一致性，提供商和主题是影响这种一致性的关键因素。ConsistencyAI为评估这种现象提供了一个独立工具，并鼓励采用与用户画像无关的提示策略。

Abstract: Is an LLM telling you different facts than it's telling me? This paper
introduces ConsistencyAI, an independent benchmark for measuring the factual
consistency of large language models (LLMs) for different personas.
ConsistencyAI tests whether, when users of different demographics ask identical
questions, the model responds with factually inconsistent answers. Designed
without involvement from LLM providers, this benchmark offers impartial
evaluation and accountability. In our experiment, we queried 19 LLMs with
prompts that requested 5 facts for each of 15 topics. We repeated this query
100 times for each LLM, each time adding prompt context from a different
persona selected from a subset of personas modeling the general population. We
processed the responses into sentence embeddings, computed cross-persona cosine
similarity, and computed the weighted average of cross-persona cosine
similarity to calculate factual consistency scores. In 100-persona experiments,
scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as
a benchmark threshold. xAI's Grok-3 is most consistent, while several
lightweight models rank lowest. Consistency varies by topic: the job market is
least consistent, G7 world leaders most consistent, and issues like vaccines or
the Israeli-Palestinian conflict diverge by provider. These results show that
both the provider and the topic shape the factual consistency. We release our
code and interactive demo to support reproducible evaluation and encourage
persona-invariant prompting strategies.

</details>


### [20] [BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation](https://arxiv.org/abs/2510.13853)
*Fabian Wenz,Omar Bouattour,Devin Yang,Justin Choi,Cecil Gregg,Nesime Tatbul,Çağatay Demiralp*

Main category: cs.CL

TL;DR: 本文提出BenchPress，一个结合人工干预和LLM的系统，旨在加速领域特定Text-to-SQL基准数据集的创建，解决了手动标注企业SQL日志的挑战，并显著提高了标注效率和基准质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在Text-to-SQL任务上表现出色，但其在私有企业数据仓库上的效果远不如公开数据集。创建领域特定的Text-to-SQL基准（如Beaver）需要将SQL日志手动标注为自然语言问题，这对于数据库专家来说是一项极其艰巨、耗时且昂贵的任务。

Method: 引入BenchPress系统，该系统采用“人工在环”的方法。给定一个SQL查询，BenchPress利用检索增强生成（RAG）和大型语言模型提出多个自然语言描述草稿，然后由人类专家选择、排序或编辑这些草稿，以确保准确性和领域一致性。

Result: BenchPress在标注企业SQL日志上的评估表明，LLM辅助标注显著减少了创建高质量基准所需的时间和精力。结果显示，将人工验证与LLM生成建议相结合，提升了标注准确性、基准可靠性和模型评估的鲁棒性。

Conclusion: BenchPress通过简化定制化基准的创建过程，为研究人员和实践者提供了一种评估Text-to-SQL模型在特定领域工作负载上表现的有效机制。该系统已通过GitHub公开可用。

Abstract: Large language models (LLMs) have been successfully applied to many tasks,
including text-to-SQL generation. However, much of this work has focused on
publicly available datasets, such as Fiben, Spider, and Bird. Our earlier work
showed that LLMs are much less effective in querying large private enterprise
data warehouses and released Beaver, the first private enterprise text-to-SQL
benchmark. To create Beaver, we leveraged SQL logs, which are often readily
available. However, manually annotating these logs to identify which natural
language questions they answer is a daunting task. Asking database
administrators, who are highly trained experts, to take on additional work to
construct and validate corresponding natural language utterances is not only
challenging but also quite costly. To address this challenge, we introduce
BenchPress, a human-in-the-loop system designed to accelerate the creation of
domain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses
retrieval-augmented generation (RAG) and LLMs to propose multiple natural
language descriptions. Human experts then select, rank, or edit these drafts to
ensure accuracy and domain alignment. We evaluated BenchPress on annotated
enterprise SQL logs, demonstrating that LLM-assisted annotation drastically
reduces the time and effort required to create high-quality benchmarks. Our
results show that combining human verification with LLM-generated suggestions
enhances annotation accuracy, benchmark reliability, and model evaluation
robustness. By streamlining the creation of custom benchmarks, BenchPress
offers researchers and practitioners a mechanism for assessing text-to-SQL
models on a given domain-specific workload. BenchPress is freely available via
our public GitHub repository at
https://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our
website at http://dsg-mcgraw.csail.mit.edu:5000.

</details>


### [21] [R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging](https://arxiv.org/abs/2510.13854)
*Mamadou K. Keita,Christopher Homan,Sebastien Diarra*

Main category: cs.CL

TL;DR: 本文提出了Rule-to-Tag (R2T) 框架，一种将语言规则融入神经网络训练目标的混合方法，通过自适应损失函数处理OOV词。该框架作为一种“原则性学习”范式，在Zarma语POS标注（仅用无标签数据）和NER（作为预训练步骤，用少量标签数据）任务中均显著超越了基线模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索一种“原则性学习”（PrL）范式，即通过将显式任务约束（如语言规则）直接整合到模型训练中，而非仅仅依赖标注样本，以提高模型在数据稀缺任务中的表现，并有效处理未登录词（OOV）。

Method: 引入Rule-to-Tag (R2T) 框架，该方法是一个混合系统，将多层次的语言规则直接融入神经网络的训练目标中。其核心是一个自适应损失函数，包含一个正则化项，旨在使模型能够以“原则性不确定性”处理未登录词（OOV）。

Result: 1. 在Zarma语词性标注（POS tagging）任务中，仅使用无标签文本训练的R2T-BiLSTM模型实现了98.2%的准确率，优于在300个标注句子上微调的AfriBERTa等基线模型。
2. 在命名实体识别（NER）等复杂任务中，R2T作为强大的预训练步骤；经过R2T预训练并在仅50个标注句子上微调的模型，性能优于在300个标注句子上训练的基线模型。

Conclusion: R2T框架通过将语言规则直接融入神经网络的训练目标，成功展示了“原则性学习”范式的有效性。它在无需大量标注数据的情况下，显著提升了模型在POS标注和NER等任务上的性能，尤其在处理OOV词和有限标注数据方面表现出强大优势。

Abstract: We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that
integrates a multi-tiered system of linguistic rules directly into a neural
network's training objective. R2T's novelty lies in its adaptive loss function,
which includes a regularization term that teaches the model to handle
out-of-vocabulary (OOV) words with principled uncertainty. We frame this work
as a case study in a paradigm we call principled learning (PrL), where models
are trained with explicit task constraints rather than on labeled examples
alone. Our experiments on Zarma part-of-speech (POS) tagging show that the
R2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy,
outperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We
further show that for more complex tasks like named entity recognition (NER),
R2T serves as a powerful pre-training step; a model pre-trained with R2T and
fine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.

</details>


### [22] [Harnessing Consistency for Robust Test-Time LLM Ensemble](https://arxiv.org/abs/2510.13855)
*Zhichen Zeng,Qi Yu,Xiao Lin,Ruizhong Qiu,Xuying Ning,Tianxin Wei,Yuchen Yan,Jingrui He,Hanghang Tong*

Main category: cs.CL

TL;DR: 本文提出CoRE，一种即插即用技术，通过捕捉词元级和模型级一致性，显著提升大型语言模型集成方法的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: LLM集成方法在提升质量方面取得了进展，但对潜在错误信号（如异构分词和模型专长差异）的鲁棒性研究不足，这些错误信号常导致词元级预测分歧和模型级低置信度/显著差异。

Method: 研究者提出了CoRE，一种利用模型一致性的即插即用技术，可与多种集成方法结合。它包括：1) 词元级一致性：通过低通滤波器降低不确定和高度不一致词元的权重；2) 模型级一致性：提升高自置信度且与其他模型分歧小的模型输出。

Result: 在多样化的基准、模型组合和集成策略上进行的广泛实验表明，CoRE持续改进了集成方法的性能和鲁棒性。

Conclusion: CoRE通过在词元和模型两个层面考虑一致性，提供了一种有效且易于集成的解决方案，显著增强了大型语言模型集成的鲁棒性和整体性能。

Abstract: Different large language models (LLMs) exhibit diverse strengths and
weaknesses, and LLM ensemble serves as a promising approach to integrate their
complementary capabilities. Despite substantial progress in improving ensemble
quality, limited attention has been paid to the robustness of ensembles against
potential erroneous signals, which often arise from heterogeneous tokenization
schemes and varying model expertise. Our analysis shows that ensemble failures
typically arise from both the token level and the model level: the former
reflects severe disagreement in token predictions, while the latter involves
low confidence and pronounced disparities among models. In light of this, we
propose CoRE, a plug-and-play technique that harnesses model consistency for
robust LLM ensemble, which can be seamlessly integrated with diverse ensemble
methods. Token-level consistency captures fine-grained disagreements by
applying a low-pass filter to downweight uncertain tokens with high
inconsistency, often due to token misalignment, thereby improving robustness at
a granular level. Model-level consistency models global agreement by promoting
model outputs with high self-confidence and minimal divergence from others,
enhancing robustness at a coarser level. Extensive experiments across diverse
benchmarks, model combinations, and ensemble strategies demonstrate that CoRE
consistently improves ensemble performance and robustness.

</details>


### [23] [Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA](https://arxiv.org/abs/2510.13856)
*A H M Rezaul Karim,Ozlem Uzuner*

Main category: cs.CL

TL;DR: MasonNLP系统在MEDIQA-WV 2025伤口护理VQA任务中，利用通用的RAG LLM模型并引入领域内图文示例，取得了第三名，证明了轻量级RAG在多模态临床NLP任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 医疗视觉问答（MedVQA）对临床决策和患者护理至关重要。本文旨在解决MEDIQA-WV 2025伤口护理VQA共享任务，该任务要求系统根据医学图像和患者查询生成自由文本响应及结构化伤口属性。

Method: 提出了MasonNLP系统，其核心是使用通用的、经过指令微调的大型语言模型（LLM），并结合检索增强生成（RAG）框架。该方法通过整合领域内的文本和视觉示例，使输出基于临床相关的范例。

Result: 该方法提高了推理能力、模式遵循度及响应质量，并在dBLEU、ROUGE、BERTScore和基于LLM的指标上均有所改善。MasonNLP的最佳系统在19个团队和51个提交中排名第3，平均得分达41.37%。

Conclusion: 轻量级检索增强生成（RAG）与通用大型语言模型的结合，提供了一个简单且有效的基线，适用于多模态临床自然语言处理任务。该方法作为最小的推理时层，通过简单索引和融合即可添加相关示例，无需额外训练或复杂重新排序。

Abstract: Medical Visual Question Answering (MedVQA) enables natural language queries
over medical images to support clinical decision-making and patient care. The
MEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to
generate free-text responses and structured wound attributes from images and
patient queries. We present the MasonNLP system, which employs a
general-domain, instruction-tuned large language model with a
retrieval-augmented generation (RAG) framework that incorporates textual and
visual examples from in-domain data. This approach grounds outputs in
clinically relevant exemplars, improving reasoning, schema adherence, and
response quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our
best-performing system ranked 3rd among 19 teams and 51 submissions with an
average score of 41.37%, demonstrating that lightweight RAG with
general-purpose LLMs -- a minimal inference-time layer that adds a few relevant
exemplars via simple indexing and fusion, with no extra training or complex
re-ranking -- provides a simple and effective baseline for multimodal clinical
NLP tasks.

</details>


### [24] [ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing](https://arxiv.org/abs/2510.13860)
*Shivanshu Kumar,Gopalakrishnan Srinivasan*

Main category: cs.CL

TL;DR: 本文提出了ShishuLM，一种高效的语言模型架构，旨在减少Transformer模型的内存和计算开销。通过在中等上下文场景下用MLP近似Transformer块，ShishuLM显著降低了参数量和KV缓存需求，在内存和延迟方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: Transformer架构虽然在NLP任务中表现出色，但存在高昂的内存和计算开销。现有研究表明模型存在冗余，为在不牺牲性能的情况下进行优化提供了机会。

Method: 借鉴AI可解释性和推理时层剪枝的见解，本文引入了ShishuLM架构。核心方法是在中等上下文场景中，通过多层感知机（MLP）近似整个Transformer块（包括归一化和注意力计算），从而减少参数数量和Key-Value（KV）缓存需求。该方法在两种不同规模的小型语言模型（SLMs）上进行了评估。

Result: 与父模型相比，ShishuLM在内存需求上实现了高达25%的降低，并在训练和推理延迟方面实现了高达40%的改进。

Conclusion: 本研究的实验和分析结果为从预训练角度构建更高效的小型语言模型架构提供了宝贵的见解。

Abstract: While the transformer architecture has achieved state-of-the-art performance
on natural language processing tasks, these models impose substantial memory
and computational overhead. Recent research has identified significant
architectural redundancies within these models, presenting opportunities for
optimization without compromising performance. Taking insights from research in
AI interpretability and inference-time layer pruning, we introduce an efficient
language model architecture, referred to as ShishuLM, which reduces both the
parameter count and Key-Value (KV) cache requirements. Given the increasing
importance of Small Language Models (SLMs) in agentic AI systems, we evaluate
our approach on two SLMs of different scales. Our analysis reveals that for
moderate-context scenarios, normalization coupled with attention computation is
roughly linear with the input, enabling entire transformer blocks to be
approximated through Multi-Layer Perceptrons (MLPs). Our results show that
ShishuLM provides up to 25% reduction in memory requirements and up to 40%
improvement in latency during both training and inference, compared to parent
models. Our experimental and analytical findings provide insights towards
building more efficient SLM architectures from a pre-training standpoint.

</details>


### [25] [Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues](https://arxiv.org/abs/2510.13862)
*Chenyu Zhang,Sharifa Alghowinem,Cynthia Breazeal*

Main category: cs.CL

TL;DR: 本研究引入了一种集成大型语言模型（LLM）框架，用于大规模感知AI辅导对话中学习者的情感动态，发现学生情绪总体偏向积极但短暂易变，并识别了潜在的辅导干预时机。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽关注LLM在教育中的学习影响，但对LLM介导辅导中的情感动态理解不足。

Method: 开发了首个集成LLM框架用于辅导对话中的大规模情感感知。分析了来自PyTutor（LLM驱动的AI导师）与261名本科生之间16,986个对话回合的数据。使用Gemini、GPT-4o和Claude这三个前沿LLM进行零样本情感标注（包括效价、唤醒度、学习帮助度标量评分及自由文本情感标签），并通过排名加权模型内池化和模型间多数共识融合这些估计，以生成稳健的情感画像。

Result: 学生在与AI导师互动时通常表现出轻度积极情感和中度唤醒。学习过程并非一帆风顺，困惑和好奇与问题解决常伴，沮丧虽不常见但仍会阻碍进展。情绪状态短暂，积极情绪持续时间略长于中性或消极情绪，但易受干扰。令人鼓舞的是，消极情绪常能迅速化解，有时直接转变为积极状态。中性时刻常作为转折点，更多地引导学生情绪向上而非向下。

Conclusion: 该研究揭示了学习者在AI辅导中动态的情感体验，为理解情感在教育AI中的作用提供了新视角，并为在关键节点进行导师干预以促进学习提供了机会，从而推动了生成式AI在教育中负责任的整合。

Abstract: While recent studies have examined the leaning impact of large language model
(LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring
remain insufficiently understood. This work introduces the first ensemble-LLM
framework for large-scale affect sensing in tutoring dialogues, advancing the
conversation on responsible pathways for integrating generative AI into
education by attending to learners' evolving affective states. To achieve this,
we analyzed two semesters' worth of 16,986 conversational turns exchanged
between PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across
three U.S. institutions. To investigate learners' emotional experiences, we
generate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o,
Claude), including scalar ratings of valence, arousal, and
learning-helpfulness, along with free-text emotion labels. These estimates are
fused through rank-weighted intra-model pooling and plurality consensus across
models to produce robust emotion profiles. Our analysis shows that during
interaction with the AI tutor, students typically report mildly positive affect
and moderate arousal. Yet learning is not uniformly smooth: confusion and
curiosity are frequent companions to problem solving, and frustration, while
less common, still surfaces in ways that can derail progress. Emotional states
are short-lived--positive moments last slightly longer than neutral or negative
ones, but they are fragile and easily disrupted. Encouragingly, negative
emotions often resolve quickly, sometimes rebounding directly into positive
states. Neutral moments frequently act as turning points, more often steering
students upward than downward, suggesting opportunities for tutors to intervene
at precisely these junctures.

</details>


### [26] [Unlocking the Potential of Diffusion Language Models through Template Infilling](https://arxiv.org/abs/2510.13870)
*Junhoo Lee,Seungyeon Kim,Nojun Kwak*

Main category: cs.CL

TL;DR: 扩散语言模型(DLMs)推理策略受限，本文提出模板填充(TI)方法，通过先生成结构模板再填充，并结合动态分段分配(DSA)提升灵活性。在数理推理和代码生成任务上实现17.01%p的提升，并能在多token生成中加速。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型(DLMs)作为自回归语言模型(ARMs)的替代方案，其推理策略仍局限于从AR范式继承的基于前缀的提示。

Method: 提出模板填充(TI)方法，为DLMs生成过程量身定制。TI首先生成目标响应的结构模板，然后填充掩码段。为增强结构控制的灵活性，引入动态分段分配(DSA)，根据生成置信度自适应调整段长度。

Result: 在数理推理和代码生成基准测试中，相比基线模型实现了17.01%p的一致改进。此外，TI在多token生成设置中提供了额外优势，在保持生成质量的同时实现了有效加速。

Conclusion: 提出的模板填充(TI)结合动态分段分配(DSA)方法，有效解决了DLMs推理策略的局限性，显著提升了DLMs在数理推理和代码生成任务上的性能，并实现了高效的多token生成。

Abstract: Diffusion Language Models (DLMs) have emerged as a promising alternative to
Autoregressive Language Models, yet their inference strategies remain limited
to prefix-based prompting inherited from the autoregressive paradigm. In this
paper, we propose Template Infilling (TI), a tailored conditioning methodology
for DLMs' generation process. Unlike conventional prefix prompting, TI first
generates a structural template for the target response, then fills in the
masked segments. To enhance the flexibility of this structural control, we
introduce Dynamic Segment Allocation (DSA), which adaptively adjusts segment
lengths based on generation confidence. We demonstrate the effectiveness of our
approach on mathematical reasoning and code generation benchmarks, achieving
consistent improvements of 17.01$\%$p over baseline. Furthermore, we show that
TI provides additional advantages in multi-token generation settings, enabling
effective speedup while maintaining generation quality.

</details>


### [27] [Quechua Speech Datasets in Common Voice: The Case of Puno Quechua](https://arxiv.org/abs/2510.13871)
*Elwin Huaman,Wendi Huaman,Jorge Luis Huaman,Ninfa Quispe*

Main category: cs.CL

TL;DR: 该研究将盖丘亚语整合到Common Voice平台，以解决欠资源语言的语音数据稀缺问题，成功收集了大量语料，并提出了未来的研究议程以促进数字赋权。


<details>
  <summary>Details</summary>
Motivation: 欠资源语言（如盖丘亚语）面临数据和资源稀缺，阻碍其语音技术的发展。

Method: 利用Common Voice平台，整合了17种盖丘亚语，并以普诺盖丘亚语（Puno Quechua）为例，详细介绍了语言入驻和阅读及自发语音语料的收集过程。

Result: Common Voice平台目前已拥有191.1小时的盖丘亚语语音数据（86%已验证），其中普诺盖丘亚语贡献了12小时（77%已验证），展示了Common Voice的巨大潜力。

Conclusion: 该工作通过Common Voice为欠资源语言社区提供了包容性语音技术和数字赋权，并提出了解决技术挑战、社区参与和原住民数据主权等伦理考量的研究议程。

Abstract: Under-resourced languages, such as Quechuas, face data and resource scarcity,
hindering their development in speech technology. To address this issue, Common
Voice presents a crucial opportunity to foster an open and community-driven
speech dataset creation. This paper examines the integration of Quechua
languages into Common Voice. We detail the current 17 Quechua languages,
presenting Puno Quechua (ISO 639-3: qxp) as a focused case study that includes
language onboarding and corpus collection of both reading and spontaneous
speech data. Our results demonstrate that Common Voice now hosts 191.1 hours of
Quechua speech (86\% validated), with Puno Quechua contributing 12 hours (77\%
validated), highlighting the Common Voice's potential. We further propose a
research agenda addressing technical challenges, alongside ethical
considerations for community engagement and indigenous data sovereignty. Our
work contributes towards inclusive voice technology and digital empowerment of
under-resourced language communities.

</details>


### [28] [FRACCO: A gold-standard annotated corpus of oncological entities with ICD-O-3.1 normalisation](https://arxiv.org/abs/2510.13873)
*Johann Pignat,Milena Vucetic,Christophe Gaudet-Blavignac,Jamil Zaghir,Amandine Stettler,Fanny Amrein,Jonatan Bonjour,Jean-Philippe Goldman,Olivier Michielin,Christian Lovis,Mina Bjelogrlic*

Main category: cs.CL

TL;DR: 本文介绍了FRACCO，一个包含1301个专家标注法语肿瘤临床病例的语料库，用于解决法语NLP资源稀缺问题，并作为命名实体识别和概念标准化的参考。


<details>
  <summary>Details</summary>
Motivation: 法语肿瘤临床文本的自然语言处理（NLP）工具开发需要标注数据集，但目前法语肿瘤学资源非常稀缺。

Method: 通过翻译西班牙CANTEMIST语料库中的1301个合成临床病例创建了FRACCO语料库。语料库使用国际肿瘤疾病分类（ICD-O）对形态学、部位和组织学分化术语进行标注。此外，还增加了一个标注层用于捕获复合表达级别的标准化。标注质量通过两名领域专家对实体范围的手动标注以及由自动化匹配和五名标注员手动验证相结合的方式生成71127个ICD-O标准化来确保。

Result: 最终数据集包含399个独特的形态学代码（来自2549个不同表达）、272个部位代码（来自3143个不同表达）和2043个独特的复合表达（来自11144个不同表达）。

Conclusion: FRACCO数据集为法语肿瘤文本的命名实体识别和概念标准化提供了重要的参考标准。

Abstract: Developing natural language processing tools for clinical text requires
annotated datasets, yet French oncology resources remain scarce. We present
FRACCO (FRench Annotated Corpus for Clinical Oncology) an expert-annotated
corpus of 1301 synthetic French clinical cases, initially translated from the
Spanish CANTEMIST corpus as part of the FRASIMED initiative. Each document is
annotated with terms related to morphology, topography, and histologic
differentiation, using the International Classification of Diseases for
Oncology (ICD-O) as reference. An additional annotation layer captures
composite expression-level normalisations that combine multiple ICD-O elements
into unified clinical concepts. Annotation quality was ensured through expert
review: 1301 texts were manually annotated for entity spans by two domain
experts. A total of 71127 ICD-O normalisations were produced through a
combination of automated matching and manual validation by a team of five
annotators. The final dataset representing 399 unique morphology codes (from
2549 different expressions), 272 topography codes (from 3143 different
expressions), and 2043 unique composite expressions (from 11144 different
expressions). This dataset provides a reference standard for named entity
recognition and concept normalisation in French oncology texts.

</details>


### [29] [What Layers When: Learning to Skip Compute in LLMs with Residual Gates](https://arxiv.org/abs/2510.13876)
*Filipe Laitenberger,Dawid Kopiczko,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

TL;DR: 本文提出GateSkip，一种残差流门控机制，可在解码器LMs中实现逐token的层跳过。它通过在预训练模型上稳定微调，显著节省计算并保持或提升性能，同时提供transformer信息流的洞察。


<details>
  <summary>Details</summary>
Motivation: 现有早期退出或基于路由器的深度混合模型不稳定且需要大量再训练。研究动机是开发一种平滑、可微分的门控机制，能在预训练模型上稳定微调，以实现计算节省和性能优化。

Method: 引入GateSkip机制。为每个Attention/MLP分支配备一个sigmoid-linear门控，用于精简输出。在推理时，根据门控值对token进行排序，并使用每层预算跳过重要性较低的token。

Result: 在长篇推理任务中，计算量节省高达15%同时保持90%以上的基线精度。在指令微调模型上，在全计算量下观察到精度提升，在接近50%计算量节省的情况下达到基线质量。

Conclusion: GateSkip通过学习的门控机制提供了对transformer信息流的洞察，并能有效节省计算。该方法与量化、剪枝和自推测解码等技术易于结合，具有广泛应用潜力。

Abstract: We introduce GateSkip, a simple residual-stream gating mechanism that enables
token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is
equipped with a sigmoid-linear gate that condenses the branch's output before
it re-enters the residual stream. During inference we rank tokens by the gate
values and skip low-importance ones using a per-layer budget. While early-exit
or router-based Mixture-of-Depths models are known to be unstable and need
extensive retraining, our smooth, differentiable gates fine-tune stably on top
of pretrained models. On long-form reasoning, we save up to 15\% compute while
retaining over 90\% of baseline accuracy. On instruction-tuned models we see
accuracy gains at full compute and match baseline quality near 50\% savings.
The learned gates give insight into transformer information flow (e.g., BOS
tokens act as anchors), and the method combines easily with quantization,
pruning, and self-speculative decoding.

</details>


### [30] [TextBandit: Evaluating Probabilistic Reasoning in LLMs Through Language-Only Decision Tasks](https://arxiv.org/abs/2510.13878)
*Jimin Lim,Arjun Damerla,Arthur Jiang,Nam Le*

Main category: cs.CL

TL;DR: 研究了大型语言模型（LLMs）在纯文本反馈下进行不确定性序贯决策的能力，发现Qwen3-4B在多臂老虎机任务中表现卓越，甚至超越了传统决策算法。


<details>
  <summary>Details</summary>
Motivation: LLMs在仅使用自然语言的情况下，于不确定环境中进行序贯决策的能力仍未被充分探索。

Method: 引入了一个新颖的基准，LLMs通过纯文本反馈（“你获得一个代币”），在没有数字或明确概率提示的情况下，与多臂老虎机环境交互。评估了四款开源LLMs，并将其性能与Thompson Sampling、Epsilon Greedy、UCB等传统决策算法进行了比较。

Result: 大多数LLMs表现不及基线算法，但Qwen3-4B在最佳臂选择率上达到89.2%，显著优于更大的LLMs和传统方法。

Conclusion: 研究结果表明，概率推理能力可以仅通过语言出现，该基准为评估LLMs在自然、非数字环境中的决策能力提供了新方向。

Abstract: Large language models (LLMs) have shown to be increasingly capable of
performing reasoning tasks, but their ability to make sequential decisions
under uncertainty only using natural language remains underexplored. We
introduce a novel benchmark in which LLMs interact with multi-armed bandit
environments using purely textual feedback, "you earned a token", without
access to numerical cues or explicit probabilities, resulting in the model to
infer latent reward structures purely off linguistic cues and to adapt
accordingly. We evaluated the performance of four open-source LLMs and compare
their performance to standard decision-making algorithms such as Thompson
Sampling, Epsilon Greedy, Upper Confidence Bound (UCB), and random choice.
While most of the LLMs underperformed compared to the baselines, Qwen3-4B,
achieved the best-arm selection rate of 89.2% , which significantly
outperformed both the larger LLMs and traditional methods. Our findings suggest
that probabilistic reasoning is able to emerge from language alone, and we
present this benchmark as a step towards evaluating decision-making
capabilities in naturalistic, non-numeric contexts.

</details>


### [31] [Catch Your Breath: Adaptive Computation for Self-Paced Sequence Production](https://arxiv.org/abs/2510.13879)
*Alexandre Galashov,Matt Jones,Rosemary Ke,Yuan Cao,Vaishnavh Nagarajan,Michael C. Mozer*

Main category: cs.CL

TL;DR: 本文提出一类名为“Catch Your Breath (CYB)”的监督训练目标，使语言模型能动态且自主地为每个输入token扩展计算步数。通过请求额外计算资源，CYB模型显著减少了训练数据需求，并能根据token复杂性和上下文自适应地调整处理时间，从而提升准确性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型通常采用固定的计算步数，无法根据输入内容的复杂性或模型的不确定性动态调整计算资源。研究旨在开发一种训练范式，使语言模型能够自主、动态地扩展每个输入token的计算步数，以优化计算资源使用并提高预测准确性。

Method: 引入一类名为“Catch Your Breath (CYB)”的监督训练目标。模型通过发出`<don't know>`输出来请求额外的计算步数，若获得延迟，则插入特殊的`<pause>`token以提供额外计算资源。将每个输出token的选择框定为一个具有时间成本的序列决策问题，以训练模型明智地使用额外计算并校准不确定性。研究了三种CYB损失变体：CYB-AP（随时预测）、CYB-VA（变分方法）和CYB-DP（计算预算惩罚），并通过微调实验确定了最佳变体。

Result: 实验结果表明，CYB模型在达到相同性能时，所需的训练数据量仅为基线模型（无暂停）的三分之一，以及带有暂停和交叉熵损失模型的一半。模型在请求额外计算步数时能有效提高准确性，并能根据token级别的复杂性和上下文自适应地调整处理时间。例如，模型常在复数名词后暂停，但在缩略词的第一个token后则不暂停，且对语义模糊的token表现出高度可变性。

Conclusion: CYB损失有效地让语言模型能够动态且自主地扩展计算步数，从而显著降低了训练数据需求，并使其能根据任务复杂性和不确定性自适应地分配计算资源，以提高预测准确性和整体效率。

Abstract: We explore a class of supervised training objectives that allow a language
model to dynamically and autonomously scale the number of compute steps used
for each input token. For any token, the model can request additional compute
steps by emitting a <don't know> output. If the model is granted a delay, a
specialized <pause> token is inserted at the next input step, providing the
model with additional compute resources to generate an output. The model can
request multiple pauses. To train the model to use <don't know> outputs
judiciously and to calibrate its uncertainty, we frame the selection of each
output token as a sequential-decision problem with a time cost. We refer to the
class of methods as $\textit{Catch Your Breath}$ losses and we study three
methods in this class: CYB-AP frames the model's task as anytime prediction,
where an output may be required at any step and accuracy is discounted over
time; CYB-VA is a variational approach that aims to maximize prediction
accuracy subject to a specified distribution over stopping times; and CYB-DP
imposes a penalty based on a computational budget. Through fine-tuning
experiments, we identify the best performing loss variant. The CYB model needs
only one third as much training data as the baseline (no pause) model needs to
achieve the same performance, and half as much data as a model with pauses and
a cross-entropy loss. We find that the CYB model requests additional steps when
doing so improves accuracy, and the model adapts its processing time to
token-level complexity and context. For example, it often pauses after plural
nouns like $\textit{patients}$ and $\textit{challenges}$ but never pauses after
the first token of contracted words like $\textit{wasn}$ and $\textit{didn}$,
and it shows high variability for ambiguous tokens like $\textit{won}$, which
could function as either a verb or part of a contraction.

</details>


### [32] [PAGE: Prompt Augmentation for text Generation Enhancement](https://arxiv.org/abs/2510.13880)
*Mauro Jose Pacchiotti,Luciana Ballejos,Mariel Ale*

Main category: cs.CL

TL;DR: PAGE是一个通过使用轻量级辅助模块（如分类器或提取器）增强自然语言生成模型性能和可控性的框架，它通过丰富输入来改善生成质量，并已在需求工程领域得到概念验证。


<details>
  <summary>Details</summary>
Motivation: 自然语言生成模型在特定任务或面临特定要求时，表现可能不佳，或需要大量额外数据进行调整。

Method: 引入PAGE框架，该框架通过简单的辅助模块（如分类器或提取器等轻量级模型）从输入文本中提供推断。这些辅助模块的输出用于构建一个丰富（enriched）的输入，从而提高生成质量和可控性。PAGE采用模块化架构，易于适应不同任务，且无需辅助生成模型。

Result: 论文介绍了PAGE的提案、组件和架构，并报告了在需求工程领域的一个概念验证，其中使用带有分类器的辅助模块来提高软件需求生成的质量。

Conclusion: PAGE提供了一种无需额外生成模型、更简单、模块化的文本生成增强方法，通过概念验证表明其能有效提升特定任务中文本生成模型的性能和可控性。

Abstract: In recent years, natural language generative models have shown outstanding
performance in text generation tasks. However, when facing specific tasks or
particular requirements, they may exhibit poor performance or require
adjustments that demand large amounts of additional data. This work introduces
PAGE (Prompt Augmentation for text Generation Enhancement), a framework
designed to assist these models through the use of simple auxiliary modules.
These modules, lightweight models such as classifiers or extractors, provide
inferences from the input text. The output of these auxiliaries is then used to
construct an enriched input that improves the quality and controllability of
the generation. Unlike other generation-assistance approaches, PAGE does not
require auxiliary generative models; instead, it proposes a simpler, modular
architecture that is easy to adapt to different tasks. This paper presents the
proposal, its components and architecture, and reports a proof of concept in
the domain of requirements engineering, where an auxiliary module with a
classifier is used to improve the quality of software requirements generation.

</details>


### [33] [Too Open for Opinion? Embracing Open-Endedness in Large Language Models for Social Simulation](https://arxiv.org/abs/2510.13884)
*Bolei Ma,Yong Cao,Indira Sen,Anna-Carolina Haensch,Frauke Kreuter,Barbara Plank,Daniel Hershcovich*

Main category: cs.CL

TL;DR: 本文主张LLM社会模拟应利用其开放式生成特性（自由文本），而非局限于封闭格式，以实现更真实、具洞察力的模拟，并呼吁相关方法创新。


<details>
  <summary>Details</summary>
Motivation: 当前LLM社会模拟多采用多项选择或简答等封闭式设计，忽视了LLM固有的生成能力，限制了模拟的真实性。

Method: 本文作为一篇立场论文，借鉴调查方法论和NLP进展，论证了开放式（使用自由文本）在LLM社会模拟中的重要性。

Result: 开放式模拟能改进测量与设计，支持探索意想不到的观点，减少研究者导向性偏见，捕捉表达性和个性，辅助预测试，并最终提升方法论效用。

Conclusion: 呼吁开发利用LLM开放式生成多样性的新实践和评估框架，以实现自然语言处理与社会科学的协同增效。

Abstract: Large Language Models (LLMs) are increasingly used to simulate public opinion
and other social phenomena. Most current studies constrain these simulations to
multiple-choice or short-answer formats for ease of scoring and comparison, but
such closed designs overlook the inherently generative nature of LLMs. In this
position paper, we argue that open-endedness, using free-form text that
captures topics, viewpoints, and reasoning processes "in" LLMs, is essential
for realistic social simulation. Drawing on decades of survey-methodology
research and recent advances in NLP, we argue why this open-endedness is
valuable in LLM social simulations, showing how it can improve measurement and
design, support exploration of unanticipated views, and reduce
researcher-imposed directive bias. It also captures expressiveness and
individuality, aids in pretesting, and ultimately enhances methodological
utility. We call for novel practices and evaluation frameworks that leverage
rather than constrain the open-ended generative diversity of LLMs, creating
synergies between NLP and social science.

</details>


### [34] [Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization](https://arxiv.org/abs/2510.13885)
*Ariel Kamen*

Main category: cs.CL

TL;DR: 该研究对比评估了十个SOTA大型语言模型在非结构化文本分类任务中的表现，发现它们性能一般且存在幻觉问题；而一种集合（ensemble）方法显著提高了准确性并消除了幻觉。


<details>
  <summary>Details</summary>
Motivation: 评估当前先进的大型语言模型在非结构化文本分类任务中的实际能力和局限性，并探索克服这些局限的有效途径。

Method: 使用IAB 2.2分层分类法，通过统一的8,660个人工标注数据集和零样本提示，对十个大型语言模型进行比较评估。评估指标包括准确率、精确率、召回率、F1分数以及LLM特有的幻觉率、膨胀率和分类成本。为解决发现的局限性，还开发并测试了一种基于集合（ensemble）的方法。

Result: 大型语言模型在经典性能指标上仅取得中等水平（平均准确率34%），并频繁过度生成类别（幻觉和膨胀）。Gemini 1.5/2.0 Flash和GPT 20B/120B在成本与性能之间提供了较好的平衡。集合方法显著提高了准确率，降低了膨胀率，并完全消除了幻觉。

Conclusion: 仅靠规模和架构改进不足以提高文本分类准确性。模型协同编排（如集合方法）比单纯的规模扩展更能有效达到或超越人类专家在文本分类任务中的表现。

Abstract: This study presents a comparative evaluation of ten state-of-the-art large
language models (LLMs) applied to unstructured text categorization using the
Interactive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis
employed a uniform dataset of 8,660 human-annotated samples and identical
zero-shot prompts to ensure methodological consistency across all models.
Evaluation metrics included four classic measures - accuracy, precision,
recall, and F1-score - and three LLM-specific indicators: hallucination ratio,
inflation ratio, and categorization cost.
  Results show that, despite their rapid advancement, contemporary LLMs achieve
only moderate classic performance, with average scores of 34% accuracy, 42%
precision, 45% recall, and 41% F1-score. Hallucination and inflation ratios
reveal that models frequently overproduce categories relative to human
annotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B
offered the most favorable cost-to-performance balance, while GPT 120B
demonstrated the lowest hallucination ratio. The findings suggest that scaling
and architectural improvements alone do not ensure better categorization
accuracy, as the task requires compressing rich unstructured text into a
limited taxonomy - a process that challenges current model architectures.
  To address these limitations, a separate ensemble-based approach was
developed and tested. The ensemble method, in which multiple LLMs act as
independent experts, substantially improved accuracy, reduced inflation, and
completely eliminated hallucinations. These results indicate that coordinated
orchestration of models - rather than sheer scale - may represent the most
effective path toward achieving or surpassing human-expert performance in
large-scale text categorization.

</details>


### [35] [Reliable Fine-Grained Evaluation of Natural Language Math Proofs](https://arxiv.org/abs/2510.13888)
*Wenjie Ma,Andrei Cojocaru,Neel Kolhe,Bradley Louie,Robin Said Sharif,Haihan Zhang,Vincent Zhuang,Matei Zaharia,Sewon Min*

Main category: cs.CL

TL;DR: 本文提出并验证了ProofGrader，这是一个针对大型语言模型（LLM）生成的自然语言数学证明的细粒度评估器，通过引入ProofBench数据集和系统方法，ProofGrader在准确性上显著优于基线，并能有效提升下游证明生成任务的质量。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在数学推理方面主要关注易于验证最终答案的任务，但在生成和验证自然语言数学证明方面仍面临挑战。缺乏一个可靠、细粒度的LLM生成数学证明评估器是一个关键的空白。

Method: 研究者提出了一套系统方法来开发和验证评估器，该评估器能以0-7的细粒度分数评估模型生成的数学证明。为此，他们引入了ProofBench数据集，这是一个首个包含专家标注的细粒度证明评分的数据集，涵盖145个问题和435个LLM生成的解决方案。在此基础上，系统探索了评估器设计空间（骨干模型、输入上下文、指令和评估工作流），最终设计了ProofGrader，结合了强大的推理骨干LLM、丰富的上下文（参考解决方案和评分方案）以及简单的集成方法。

Result: ProofGrader在专家分数上的平均绝对误差（MAE）为0.926，显著优于朴素基线。在“从n个中选最优”任务中（n=16），ProofGrader的平均得分为4.14（满分7分），弥补了朴素二元评估器（2.48分）与人类专家（4.62分）之间78%的差距。

Conclusion: ProofGrader具有实际应用价值，能够通过可靠地选择更好的证明，从而推动下游证明生成任务的进展。

Abstract: Recent advances in large language models (LLMs) for mathematical reasoning
have largely focused on tasks with easily verifiable final answers; however,
generating and verifying natural language math proofs remains an open
challenge. We identify the absence of a reliable, fine-grained evaluator for
LLM-generated math proofs as a critical gap. To address this, we propose a
systematic methodology for developing and validating evaluators that assign
fine-grained scores on a 0-7 scale to model-generated math proofs. To enable
this study, we introduce ProofBench, the first expert-annotated dataset of
fine-grained proof ratings, spanning 145 problems from six major math
competitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from
Gemini-2.5-pro, o3, and DeepSeek-R1. %with expert gradings. Using ProofBench as
a testbed, we systematically explore the evaluator design space across key
axes: the backbone model, input context, instructions and evaluation workflow.
Our analysis delivers ProofGrader, an evaluator that combines a strong
reasoning backbone LM, rich context from reference solutions and marking
schemes, and a simple ensembling method; it achieves a low Mean Absolute Error
(MAE) of 0.926 against expert scores, significantly outperforming naive
baselines. Finally, we demonstrate its practical utility in a best-of-$n$
selection task: at $n=16$, ProofGrader achieves an average score of 4.14 (out
of 7), closing 78% of the gap between a naive binary evaluator (2.48) and the
human oracle (4.62), highlighting its potential to advance downstream proof
generation.

</details>


### [36] [A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness](https://arxiv.org/abs/2510.13890)
*Fali Wang,Jihai Chen,Shuhua Yang,Ali Al-Lawati,Linli Tang,Hui Liu,Suhang Wang*

Main category: cs.CL

TL;DR: 本文对SLM-LLM协作进行了系统性综述，旨在解决大型语言模型面临的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）面临高微调成本、推理延迟、边缘部署受限和可靠性问题。小语言模型（SLMs）能提供互补解决方案。近期工作探索了结合SLM效率和LLM泛化能力的协作框架，本文受此启发，旨在对SLM-LLM协作进行系统性综述。

Method: 本文进行了一项系统性综述，根据协作目标组织内容，并提出了一个包含性能提升、成本效益、云边隐私和可信度四个目标的分类法。在此框架内，回顾了代表性方法并总结了设计范式。

Result: 本文在提出的分类法下，审查了SLM-LLM协作的代表性方法，并总结了其设计范式。

Conclusion: 本文提出了高效、安全和可扩展的SLM-LLM协作所面临的开放挑战和未来发展方向。

Abstract: Large language models (LLMs) have advanced many domains and applications but
face high fine-tuning costs, inference latency, limited edge deployability, and
reliability concerns. Small language models (SLMs), compact, efficient, and
adaptable, offer complementary remedies. Recent work explores collaborative
frameworks that fuse SLMs' specialization and efficiency with LLMs'
generalization and reasoning to meet diverse objectives across tasks and
deployment scenarios. Motivated by these developments, this paper presents a
systematic survey of SLM-LLM collaboration organized by collaboration
objectives. We propose a taxonomy with four goals: performance enhancement,
cost-effectiveness, cloud-edge privacy, and trustworthiness. Within this
framework, we review representative methods, summarize design paradigms, and
outline open challenges and future directions toward efficient, secure, and
scalable SLM-LLM collaboration.

</details>


### [37] [The Harder The Better: Maintaining Supervised Fine-tuning Generalization with Less but Harder Data](https://arxiv.org/abs/2510.13892)
*Zhaoyang Shang,Sibo Wei,Jianbin Guo,Rui Zhou,Lifeng Dong,Yin Luo*

Main category: cs.CL

TL;DR: THTB是一个受认知科学启发的框架，通过选择高质量、高难度的指令数据，用少量数据即可超越全量数据训练，并提升泛化能力和领域适应性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在专业领域适应时需要高质量的监督微调（SFT）数据。现有数据选择方法过度依赖LLM内部知识，可解释性弱，且泛化能力有限。

Method: 提出THTB（The Harder The Better）框架，该框架受认知科学启发，用于指令数据选择和标注指导。THTB通过结合质量过滤与内在/外在难度评分，优先选择更高层次的认知指令，提供可解释且可量化的标准。

Result: 1. THTB使模型仅用5%的数据即可超越全量数据训练，并实现优于纯LLM选择的泛化能力。
2. THTB在垂直领域提供有效的标注指导，使模型仅用2%的数据就能超越在更大数据集上训练的模型，显示出强大的领域适应潜力。

Conclusion: THTB通过智能地选择和指导标注高质量、更具挑战性的指令数据，显著提高了SFT的效率、泛化能力和领域适应性，大幅减少了数据需求。

Abstract: Large Language Models (LLMs) excel in general tasks, but adapting them to
specialized domains relies on high-quality supervised fine-tuning (SFT) data.
Although existing methods can identify subsets of high-quality data and reduce
training cost to some extent, their selection process still suffers from
over-reliance on LLMs' internal knowledge, weak interpretability, and limited
generalization. To address these limitations, we propose THTB (The Harder The
Better), a cognitive science-inspired framework for instruction data selection
and annotation guidance. THTB prioritizes higher-level cognitive instructions
by combining quality filtering with intrinsic and extrinsic hardness scoring,
offering interpretable and quantifiable criteria for efficient SFT, both in
data selection and annotation guidance. Experiments show that THTB enables
models trained on only 5% of the data to outperform full-dataset training,
while achieving superior generalization compared with LLM-only selection. In
addition, THTB provides effective annotation guidance in vertical domains,
enabling a model trained on just 2% of the data to surpass models trained on
much larger datasets, demonstrating strong potential for domain adaptation. Our
code, datasets, and models are available on
https://github.com/DYJG-research/THTB.

</details>


### [38] [Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection](https://arxiv.org/abs/2510.13893)
*Olga E. Sorokoletova,Francesco Giarrusso,Vincenzo Suriani,Daniele Nardi*

Main category: cs.CL

TL;DR: 本文通过构建50种越狱策略的全面分类法，并分析多轮攻击数据，旨在提升对大型语言模型（LLM）越狱技术有效性的理解，并改进其检测能力。


<details>
  <summary>Details</summary>
Motivation: LLM越狱技术对模型安全构成威胁。现有防御措施主要关注单轮攻击，缺乏跨语言覆盖，且分类法有限，未能全面捕捉攻击多样性。

Method: 研究方法包括：1) 进行结构化红队挑战；2) 开发包含50种越狱策略的层级分类法；3) 分析挑战数据，研究攻击的流行度和成功率；4) 基准测试流行LLM的越狱检测能力，评估分类法引导提示的益处；5) 编译一个包含1364个多轮对抗性对话的新意大利语数据集。

Result: 主要成果包括：1) 建立了一个包含七大家族、50种策略的综合层级越狱分类法；2) 分析了不同攻击类型的流行度和成功率，揭示了策略如何利用模型漏洞；3) 评估了分类法引导提示对提高LLM越狱自动检测的有效性；4) 构建了一个新的、带标注的意大利语多轮对抗性对话数据集。

Conclusion: 本研究通过提供全面的越狱分类法和深入分析，显著提升了对LLM越狱技术有效性的理解，改进了自动检测方法，并创建了促进多轮复杂攻击研究的新资源。

Abstract: Jailbreaking techniques pose a significant threat to the safety of Large
Language Models (LLMs). Existing defenses typically focus on single-turn
attacks, lack coverage across languages, and rely on limited taxonomies that
either fail to capture the full diversity of attack strategies or emphasize
risk categories rather than the jailbreaking techniques. To advance the
understanding of the effectiveness of jailbreaking techniques, we conducted a
structured red-teaming challenge. The outcome of our experiments are manifold.
First, we developed a comprehensive hierarchical taxonomy of 50 jailbreak
strategies, consolidating and extending prior classifications into seven broad
families, including impersonation, persuasion, privilege escalation, cognitive
overload, obfuscation, goal conflict, and data poisoning. Second, we analyzed
the data collected from the challenge to examine the prevalence and success
rates of different attack types, providing insights into how specific jailbreak
strategies exploit model vulnerabilities and induce misalignment. Third, we
benchmark a popular LLM for jailbreak detection, evaluating the benefits of
taxonomy-guided prompting for improving automatic detection. Finally, we
compiled a new Italian dataset of 1364 multi-turn adversarial dialogues,
annotated with our taxonomy, enabling the study of interactions where
adversarial intent emerges gradually and succeeds in bypassing traditional
safeguards.

</details>


### [39] [Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges](https://arxiv.org/abs/2510.13898)
*Misam Abbas*

Main category: cs.CL

TL;DR: 大型语言模型时代，区分AI和人类作者日益困难。本研究对比了风格嵌入和GPT-4o判断器在多领域数据集上的表现。风格嵌入在GPT内容上准确率更高，而GPT-4o在小说和学术文章中表现更优，嵌入在口语对话中更强。结果表明需要混合归属策略。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）生成的内容与人类写作水平相当，确定内容作者变得越来越具有挑战性。

Method: 本研究在Human AI Parallel Corpus数据集（一个包含600个跨六个领域的人类与LLM生成内容平衡实例的开放数据集）上，对两种互补的归属机制进行了基准测试：固定风格嵌入（fixed Style Embeddings）和指令微调的LLM判断器（GPT-4o）。数据集中每个实例都包含一个人类提示、一个真实延续以及一个由GPT-4o或LLaMA-70B-Instruct生成的LLM延续。

Result: 固定风格嵌入在GPT生成内容的总体准确率上表现出更强的性能（82% 对 68%）。LLM判断器在LLaMA生成内容上略优于风格嵌入（85% 对 81%），但此差异无统计学意义。关键在于，LLM判断器在小说和学术散文中表现显著优于嵌入，表明其对语义的敏感性；而嵌入则在口语和剧本对话中占据主导地位，体现了其结构优势。

Conclusion: 这些互补的模式表明作者归属是一个多维问题，需要采用混合策略。本研究提供了一个开放框架，为AI生成内容的归属质量评估提供了可复现的基准。

Abstract: Attributing authorship in the era of large language models (LLMs) is
increasingly challenging as machine-generated prose rivals human writing. We
benchmark two complementary attribution mechanisms , fixed Style Embeddings and
an instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an
open dataset of 600 balanced instances spanning six domains (academic, news,
fiction, blogs, spoken transcripts, and TV/movie scripts). Each instance
contains a human prompt with both a gold continuation and an LLM-generated
continuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding
baseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs.
68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA
continuations (85 pct vs. 81 pct) but the results are not statistically
significant. Crucially, the LLM judge significantly outperforms in fiction and
academic prose, indicating semantic sensitivity, whereas embeddings dominate in
spoken and scripted dialogue, reflecting structural strengths. These
complementary patterns highlight attribution as a multidimensional problem
requiring hybrid strategies. To support reproducibility we provide code on
GitHub and derived data on Hugging Face under the MIT license. This open
framework provides a reproducible benchmark for attribution quality assessment
in AI-generated content, along with a review of related literature influencing
this work.

</details>


### [40] [Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences](https://arxiv.org/abs/2510.13900)
*Julian Minder,Clément Dumas,Stewart Slocum,Helena Casademunt,Cameron Holmes,Robert West,Neel Nanda*

Main category: cs.CL

TL;DR: 研究发现，窄域微调会使大语言模型激活层产生可解释的强偏差，通过模型差异分析可发现这些偏差，并能用于理解微调领域，但可能反映过拟合，对AI安全和可解释性研究有重要影响。


<details>
  <summary>Details</summary>
Motivation: 窄域微调是适应大语言模型和创建特定功能模型的重要手段。研究旨在理解微调如何改变模型内部状态，以及这些改变（偏差）如何反映微调领域，这对于模型可解释性和AI安全至关重要。

Method: 采用模型差异分析，特别是通过分析随机文本前几个token的激活差异来发现模型偏差。通过将这些激活差异添加到模型激活中进行“转向”，以生成与微调数据相似的文本。此外，构建了一个基于LLM的可解释性代理，利用这些偏差来理解微调领域。实验涵盖多种架构和规模的模型，以及多种合成微调任务。

Result: 狭义微调确实在LLM激活中产生了明显的、可解释的偏差。这些偏差能通过模型差异分析被发现，并且通过“转向”可生成与微调领域内容相似的文本。配备这些偏差的解释代理比基线代理表现显著更好。研究还发现这些偏差可能源于过拟合，且将预训练数据混入微调语料库可大幅消除这些偏差。

Conclusion: 狭义微调模型在其激活中留下了明显的训练目标痕迹，为改进训练提供了思路。警告AI安全和可解释性研究人员，将此类模型作为广泛微调（如聊天微调）的代理可能不切实际。强调需要更深入地研究狭义微调的影响，并开发更真实的案例研究以促进模型差异分析、安全和可解释性研究。

Abstract: Finetuning on narrow domains has become an essential tool to adapt Large
Language Models (LLMs) to specific tasks and to create models with known
unusual properties that are useful for research. We show that narrow finetuning
creates strong biases in LLM activations that can be interpreted to understand
the finetuning domain. These biases can be discovered using simple tools from
model diffing - the study of differences between models before and after
finetuning. In particular, analyzing activation differences on the first few
tokens of random text and steering by adding this difference to the model
activations produces text similar to the format and general content of the
finetuning data. We demonstrate that these analyses contain crucial information
by creating an LLM-based interpretability agent to understand the finetuning
domain. With access to the bias, the agent performs significantly better
compared to baseline agents using simple prompting. Our analysis spans
synthetic document finetuning for false facts, emergent misalignment,
subliminal learning, and taboo word guessing game models across different
architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We
suspect these biases reflect overfitting and find that mixing pretraining data
into the finetuning corpus largely removes them, though residual risks may
remain. Our work (1) demonstrates that narrowly finetuned models have salient
traces of their training objective in their activations and suggests ways to
improve how they are trained, (2) warns AI safety and interpretability
researchers that the common practice of using such models as a proxy for
studying broader finetuning (e.g., chat-tuning) might not be realistic, and (3)
highlights the need for deeper investigation into the effects of narrow
finetuning and development of truly realistic case studies for model-diffing,
safety and interpretability research.

</details>


### [41] [RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs](https://arxiv.org/abs/2510.13901)
*Tuan T. Nguyen,John Le,Thai T. Vu,Willy Susilo,Heath Cooper*

Main category: cs.CL

TL;DR: RAID框架通过优化嵌入空间中的对抗性后缀，能以更少的查询和更低的成本有效绕过大型语言模型（LLMs）的安全机制，实现更高的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在各种任务中表现出色，但它们仍然容易受到越狱攻击，这些攻击可以绕过其安全机制。本研究旨在系统地探究这些弱点。

Method: 提出RAID（Refusal-Aware and Integrated Decoding）框架。它将离散的token松弛为连续嵌入，并使用联合目标进行优化，该目标(i)鼓励限制性响应，(ii)引入拒绝感知正则化器以避免嵌入空间中的拒绝方向，(iii)应用连贯性项以保持语义合理性。优化后，通过批评者引导的解码过程将嵌入映射回token。

Result: 在多个开源LLMs上的实验表明，RAID比现有的白盒和黑盒基线实现了更高的攻击成功率，同时所需的查询更少，计算成本也更低。所生成的对抗性后缀在绕过防御的同时也保持了自然的形式。

Conclusion: 这些发现强调了嵌入空间正则化对于理解和缓解LLM越狱漏洞的重要性。

Abstract: Large language models (LLMs) achieve impressive performance across diverse
tasks yet remain vulnerable to jailbreak attacks that bypass safety mechanisms.
We present RAID (Refusal-Aware and Integrated Decoding), a framework that
systematically probes these weaknesses by crafting adversarial suffixes that
induce restricted content while preserving fluency. RAID relaxes discrete
tokens into continuous embeddings and optimizes them with a joint objective
that (i) encourages restricted responses, (ii) incorporates a refusal-aware
regularizer to steer activations away from refusal directions in embedding
space, and (iii) applies a coherence term to maintain semantic plausibility and
non-redundancy. After optimization, a critic-guided decoding procedure maps
embeddings back to tokens by balancing embedding affinity with language-model
likelihood. This integration yields suffixes that are both effective in
bypassing defenses and natural in form. Experiments on multiple open-source
LLMs show that RAID achieves higher attack success rates with fewer queries and
lower computational cost than recent white-box and black-box baselines. These
findings highlight the importance of embedding-space regularization for
understanding and mitigating LLM jailbreak vulnerabilities.

</details>


### [42] [Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory](https://arxiv.org/abs/2510.13902)
*Nicole Smith-Vaniz,Harper Lyon,Lorraine Steigner,Ben Armstrong,Nicholas Mattei*

Main category: cs.CL

TL;DR: 本研究利用道德基础理论（MFT）直接评估大型语言模型（LLMs）的道德倾向和潜在偏见，并将其与人类数据进行比较，以揭示AI生成回应中的政治和人口统计学依赖性。


<details>
  <summary>Details</summary>
Motivation: LLMs日益作为建议者在医疗、人际关系和法律等敏感领域发挥作用，引发对其在复杂政治和道德问题中回应方式及潜在偏见的重要疑问。现有研究虽应用MFT衡量人类差异，但缺乏直接评估LLM道德倾向或将其输出与可靠人类数据关联的工作。

Method: 应用道德基础理论（MFT）分析LLM的回应。直接比较LLM的MFT回应与现有的人类研究数据。调查LLM回应是否存在意识形态倾向（通过固有回应、直接表征政治意识形态或扮演人类角色）。评估LLM是否固有地倾向某种政治意识形态，并通过明确提示和基于人口统计学的角色扮演检验其表示意识形态视角的准确性。

Result: 本研究通过系统分析LLM行为，提供了对AI生成回应中政治和人口统计学依赖性程度的深入见解。

Conclusion: 本研究通过揭示AI生成回应中政治和人口统计学依赖性的程度，为理解LLMs在敏感领域中的潜在偏见提供了重要启示。

Abstract: Large Language Models (LLMs) have become increasingly incorporated into
everyday life for many internet users, taking on significant roles as advice
givers in the domains of medicine, personal relationships, and even legal
matters. The importance of these roles raise questions about how and what
responses LLMs make in difficult political and moral domains, especially
questions about possible biases. To quantify the nature of potential biases in
LLMs, various works have applied Moral Foundations Theory (MFT), a framework
that categorizes human moral reasoning into five dimensions: Harm, Fairness,
Ingroup Loyalty, Authority, and Purity. Previous research has used the MFT to
measure differences in human participants along political, national, and
cultural lines. While there has been some analysis of the responses of LLM with
respect to political stance in role-playing scenarios, no work so far has
directly assessed the moral leanings in the LLM responses, nor have they
connected LLM outputs with robust human data. In this paper we analyze the
distinctions between LLM MFT responses and existing human research directly,
investigating whether commonly available LLM responses demonstrate ideological
leanings: either through their inherent responses, straightforward
representations of political ideologies, or when responding from the
perspectives of constructed human personas. We assess whether LLMs inherently
generate responses that align more closely with one political ideology over
another, and additionally examine how accurately LLMs can represent ideological
perspectives through both explicit prompting and demographic-based
role-playing. By systematically analyzing LLM behavior across these conditions
and experiments, our study provides insight into the extent of political and
demographic dependency in AI-generated responses.

</details>


### [43] [Schema for In-Context Learning](https://arxiv.org/abs/2510.13905)
*Pan Chen,Shaohong Chen,Mark Wang,Shi Xuan Leong,Priscilla Fung,Varinia Bernales,Alan Aspuru-Guzik*

Main category: cs.CL

TL;DR: 本文提出SA-ICL框架，通过从示例中提取抽象图式（schema）来显式增强语言模型的上下文学习能力，显著提升了LLMs在复杂推理任务上的表现和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的上下文学习（ICL）缺乏抽象层面的知识检索与迁移模块。受认知科学中图式理论（Schema Theory）启发，即人类通过激活现有心智框架来理解新信息，研究旨在为LLMs引入类似的显式图式激活机制以提升推理能力。

Method: 引入“图式激活上下文学习”（SCHEMA ACTIVATED IN CONTEXT LEARNING, SA-ICL）框架。该方法从先前的示例中提取推理过程的“认知基石”表示，创建抽象图式（一个轻量级、结构化的关键推理步骤模板），然后用此图式来增强模型处理新问题的推理过程。

Result: 实验证明，LLMs本身缺乏隐式形成和利用基于图式的学习表示的能力，但通过显式的图式化支架能显著受益。在GPQA数据集的化学和物理问题上，SA-ICL始终如一地提升了性能，当使用高质量的单个示范时，性能提升高达36.19%，同时减少了对示范数量的依赖并增强了可解释性。

Conclusion: SA-ICL不仅弥合了从模式启动到思维链提示等不同的ICL策略，而且为增强LLMs类人推理能力开辟了新途径。

Abstract: In-Context Learning (ICL) enables transformer-based language models to adapt
to new tasks by conditioning on demonstration examples. However, traditional
example-driven in-context learning lacks explicit modules for knowledge
retrieval and transfer at the abstraction level. Inspired by cognitive science,
specifically schema theory, which holds that humans interpret new information
by activating pre-existing mental frameworks (schemas) to structure
understanding, we introduce SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL). This
framework extracts the representation of the building blocks of cognition for
the reasoning process instilled from prior examples, creating an abstracted
schema, a lightweight, structured template of key inferential steps and their
relationships, which is then used to augment a model's reasoning process when
presented with a novel question. We demonstrate that a broad range of large
language models (LLMs) lack the capacity to form and utilize internal
schema-based learning representations implicitly, but instead benefit
significantly from explicit schema-based scaffolding. Across chemistry and
physics questions from the GPQA dataset, our experiments show that SA-ICL
consistently boosts performance, up to 36.19 percent, when the single
demonstration example is of high quality, which simultaneously reduces reliance
on the number of demonstrations and enhances interpretability. SCHEMA ACTIVATED
IN CONTEXT LEARNING not only bridges disparate ICL strategies ranging from
pattern priming to Chain-of-Thought prompting, but also paves a new path for
enhancing human-like reasoning in LLMs.

</details>


### [44] [LLM Prompt Duel Optimizer: Efficient Label-Free Prompt Optimization](https://arxiv.org/abs/2510.13907)
*Yuanchen Wu,Saurabh Verma,Justin Lee,Fangzhou Xiong,Poppy Zhang,Amel Awadelkarim,Xu Chen,Yubai Yuan,Shawndra Hill*

Main category: cs.CL

TL;DR: 提出Prompt Duel Optimizer (PDO)，一个无标签的LLM提示优化框架，它利用LLM评判器的成对偏好反馈，结合对抗性多臂赌博机算法，实现样本高效的提示优化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）对输入提示高度敏感，但现有自动提示优化（APO）方法大多依赖于昂贵且耗时的高质量标注数据。

Method: 提出PDO框架，将问题建模为对抗性多臂赌博机（dueling-bandit）设置，LLM评判器提供成对偏好反馈作为监督信号。该框架结合了双汤普森采样（Double Thompson Sampling, D-TS）以优先进行有信息量的提示比较，以及“顶尖表现者引导突变”（Top-Performer Guided Mutation）以通过变异高性能提示来扩展候选池。

Result: 在BIG-bench Hard (BBH) 和 MS MARCO 上的实验表明，PDO持续优于基线方法。消融研究进一步证明了D-TS和提示突变两种策略的有效性。

Conclusion: PDO提供了一种有效的、无标签的LLM提示优化方法，通过模拟对抗性多臂赌博机和利用LLM评判器，克服了传统方法对标注数据的依赖，并在不同基准测试中表现出色。

Abstract: Large language models (LLMs) are highly sensitive to their input prompts,
making prompt design a central challenge. While automatic prompt optimization
(APO) reduces manual engineering, most approaches assume access to ground-truth
references such as labeled validation data. In practice, however, collecting
high-quality labels is costly and slow. We propose the Prompt Duel Optimizer
(PDO), a sample-efficient framework for label-free prompt optimization. PDO
formulates the problem as a dueling-bandit setting, where supervision signal
comes from pairwise preference feedback provided by an LLM judge. The framework
combines Double Thompson Sampling (D-TS), which prioritizes informative prompt
comparisons, with Top-Performer Guided Mutation, which expands the candidate
pool by mutating high-performing prompts. PDO naturally operates in label-free
settings and can also incorporate partial labels to mitigate judge noise.
Experiments on BIG-bench Hard (BBH) and MS MARCO show that PDO consistently
outperforms baseline methods. Ablation studies further demonstrate the
effectiveness of both D-TS and prompt mutation.

</details>


### [45] [Interpreting the Latent Structure of Operator Precedence in Language Models](https://arxiv.org/abs/2510.13908)
*Dharunish Yugeswardeenoo,Harshil Nukala,Cole Blondin,Sean O Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 本文利用可解释性技术分析LLaMA 3.2-3B模型，发现其内部残差流中编码了算术运算的中间结果，并在注意力层后线性编码了运算符优先级。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）展现出强大的推理能力，但在算术任务上仍有不足。现有研究主要关注输出或提示策略，缺乏对模型内部如何进行算术计算（特别是运算符优先级）的结构性理解。

Method: 研究采用开源指令微调的LLaMA 3.2-3B模型，构建了一个包含三个操作数和两个运算符的算术表达式数据集。通过logit lens、线性分类探针和UMAP几何可视化等可解释性技术，追踪模型残差流中是否存在中间结果，并分析运算符嵌入中优先级编码情况。此外，引入部分嵌入交换技术来修改运算符优先级。

Result: 研究发现中间计算结果存在于残差流中，尤其是在多层感知机（MLP）块之后。模型在注意力层后，在线性上编码了每个运算符嵌入的优先级。通过引入部分嵌入交换技术，可以修改运算符的优先级。

Conclusion: LLMs在内部表示中确实编码了算术运算的中间计算和运算符优先级。这些发现揭示了LLMs内部处理算术逻辑的机制，并且通过部分嵌入交换技术展示了操纵优先级编码的可能性。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities but continue to struggle with arithmetic tasks. Prior works
largely focus on outputs or prompting strategies, leaving the open question of
the internal structure through which models do arithmetic computation. In this
work, we investigate whether LLMs encode operator precedence in their internal
representations via the open-source instruction-tuned LLaMA 3.2-3B model. We
constructed a dataset of arithmetic expressions with three operands and two
operators, varying the order and placement of parentheses. Using this dataset,
we trace whether intermediate results appear in the residual stream of the
instruction-tuned LLaMA 3.2-3B model. We apply interpretability techniques such
as logit lens, linear classification probes, and UMAP geometric visualization.
Our results show that intermediate computations are present in the residual
stream, particularly after MLP blocks. We also find that the model linearly
encodes precedence in each operator's embeddings post attention layer. We
introduce partial embedding swap, a technique that modifies operator precedence
by exchanging high-impact embedding dimensions between operators.

</details>


### [46] [Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning](https://arxiv.org/abs/2510.13909)
*Xingrui Zhuo,Jiapu Wang,Gongqing Wu,Zhongyuan Wang,Jichen Zhang,Shirui Pan,Xindong Wu*

Main category: cs.CL

TL;DR: 本文提出KRLM模型，通过统一LLM知识与KG上下文并引入结构感知预测器来解决归纳式知识图谱推理中LLM知识失真和幻觉问题，并在25个数据集上表现出显著优势。


<details>
  <summary>Details</summary>
Motivation: 归纳式知识图谱推理（KGR）面临未知实体和关系的挑战。现有LLM-based KGFMs存在LLM知识被稀疏KG上下文掩盖导致知识失真，以及难以完全约束LLM生成式幻觉的问题，严重限制了推理结果的可信度。

Method: 本文提出知识推理语言模型（KRLM）。具体方法包括：1) 设计KRL指令格式和KRL tokenizer来对齐LLM知识与KG表示；2) 提出KRL注意力层，通过动态知识记忆机制协调LLM内在知识与KG上下文；3) 引入结构感知下一实体预测器，严格约束推理结果在可信知识域内。

Result: 在25个真实世界归纳式KGR数据集上的广泛实验结果表明，KRLM在零样本推理和微调场景下均表现出显著的优越性。

Conclusion: KRLM通过有效协调LLM知识与KG上下文，并严格约束推理结果，成功解决了LLM-based KGR中的知识失真和幻觉问题，显著提升了归纳式KGR的性能和可信度。

Abstract: Inductive Knowledge Graph Reasoning (KGR) aims to discover facts in
open-domain KGs containing unknown entities and relations, which poses a
challenge for KGR models in comprehending uncertain KG components. Existing
studies have proposed Knowledge Graph Foundation Models (KGFMs) that learn
structural invariances across KGs to handle this uncertainty. Recently, Large
Language Models (LLMs) have demonstrated strong capabilities for open-domain
knowledge reasoning. As a result, the latest research has focused on LLM-based
KGFMs that integrate LLM knowledge with KG context for inductive KGR. However,
the intrinsic knowledge of LLMs may be overshadowed by sparse KG context,
leading to LLM knowledge distortion, which can cause irreversible damage to
model reasoning. Moreover, existing LLM-based KGR methods still struggle to
fully constrain generative hallucinations in LLMs, severely limiting the
credibility of reasoning results. To address these limitations, we propose a
Knowledge Reasoning Language Model (KRLM) that achieves unified coordination
between LLM knowledge and KG context throughout the KGR process. Specifically,
we design a Knowledge Reasoning Language (KRL) instruction format and a KRL
tokenizer to align LLM knowledge with KG representations. Then, we propose a
KRL attention layer that coordinates intrinsic LLM knowledge with additional KG
context through a dynamic knowledge memory mechanism. Finally, a
structure-aware next-entity predictor is proposed, which strictly constrains
the reasoning results within a trustworthy knowledge domain. Extensive
experimental results on 25 real-world inductive KGR datasets demonstrate the
significant superiority of the proposed KRLM\footnote{Our source codes are
available at https://anonymous.4open.science/r/KRLM-EA36 in both zero-shot
reasoning and fine-tuning scenarios.

</details>


### [47] [RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems](https://arxiv.org/abs/2510.13910)
*Jingru Lin,Chen Zhang,Stephen Y. Liu,Haizhou Li*

Main category: cs.CL

TL;DR: 提出RAGCap-Bench，一个面向能力的基准，用于细粒度评估智能体RAG系统中LLM的中间任务和能力，发现其与最终性能强相关。


<details>
  <summary>Details</summary>
Motivation: 现有的智能体RAG系统在处理复杂多跳问题时仍面临挑战，且其中间推理能力尚未得到充分探索和评估。

Method: 我们提出了RAGCap-Bench，一个以能力为导向的基准测试。该方法通过分析现有系统输出来识别常见任务和所需核心能力，构建LLM常见错误分类，并设计有针对性的评估问题。

Result: 实验表明，具有更强RAGCap性能的“慢思考”模型能获得更好的端到端结果，强调了该基准的有效性以及提升中间能力的重要性。

Conclusion: RAGCap-Bench是一个有效的基准，证明了增强智能体RAG系统中中间能力对提升整体性能的关键作用。

Abstract: Retrieval-Augmented Generation (RAG) mitigates key limitations of Large
Language Models (LLMs)-such as factual errors, outdated knowledge, and
hallucinations-by dynamically retrieving external information. Recent work
extends this paradigm through agentic RAG systems, where LLMs act as agents to
iteratively plan, retrieve, and reason over complex queries. However, these
systems still struggle with challenging multi-hop questions, and their
intermediate reasoning capabilities remain underexplored. To address this, we
propose RAGCap-Bench, a capability-oriented benchmark for fine-grained
evaluation of intermediate tasks in agentic RAG workflows. We analyze outputs
from state-of-the-art systems to identify common tasks and the core
capabilities required for their execution, then construct a taxonomy of typical
LLM errors to design targeted evaluation questions. Experiments show that
"slow-thinking" models with stronger RAGCap performance achieve better
end-to-end results, underscoring the benchmark's validity and the importance of
enhancing these intermediate capabilities.

</details>


### [48] [AI Debaters are More Persuasive when Arguing in Alignment with Their Own Beliefs](https://arxiv.org/abs/2510.13912)
*María Victoria Carro,Denise Alejandra Mester,Facundo Nieto,Oscar Agustín Stanchi,Guido Ernesto Bergman,Mario Alejandro Leiva,Eitan Sprejer,Luca Nicolás Forziati Gangi,Francisca Gauna Selasco,Juan Gustavo Corvalán,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.CL

TL;DR: AI辩论以往忽略了模型主观信念，本文探讨大型语言模型在主观问题辩论中，当法官偏好与其自身信念冲突时，是选择迎合法官还是坚持己见。研究发现模型倾向迎合法官，顺序辩论存在偏见，模型在捍卫符合自身信念的立场时更具说服力，但反常地，与自身信念不符的论点质量更高。


<details>
  <summary>Details</summary>
Motivation: AI辩论作为一种可扩展的监督技术，其核心前提是揭穿谎言比编造谎言更容易。然而，现有实验多基于有真值的数据集，忽视了“谎言”的主观维度（即辩手需相信所捍卫的主张是假的）。本研究旨在探索大型语言模型在主观问题上的辩论行为，特别是在法官偏好与其预设信念冲突时，模型会如何选择立场。

Method: 将辩论应用于主观问题，并预先测量大型语言模型的信念。辩手选择立场后，被分配与其已知先验信念冲突的法官角色，以测试模型是否会迎合法官或坚持自身信念。实验比较了顺序和同时两种辩论协议，并评估模型在捍卫与其先验信念一致和不一致的立场时的说服力及论点质量。

Result: 研究发现模型倾向于捍卫与法官角色一致的立场，而非其自身先验信念；顺序辩论对第二位辩手存在显著偏见；模型在捍卫与自身先验信念一致的立场时更具说服力；但矛盾的是，与先验信念不一致的论点在成对比较中被评为质量更高。

Conclusion: 这些发现有助于指导人类法官提供更高质量的训练信号，促进更对齐的AI系统发展，并揭示了语言模型说服动态中人机互动的重要方面。

Abstract: The core premise of AI debate as a scalable oversight technique is that it is
harder to lie convincingly than to refute a lie, enabling the judge to identify
the correct position. Yet, existing debate experiments have relied on datasets
with ground truth, where lying is reduced to defending an incorrect
proposition. This overlooks a subjective dimension: lying also requires the
belief that the claim defended is false. In this work, we apply debate to
subjective questions and explicitly measure large language models' prior
beliefs before experiments. Debaters were asked to select their preferred
position, then presented with a judge persona deliberately designed to conflict
with their identified priors. This setup tested whether models would adopt
sycophantic strategies, aligning with the judge's presumed perspective to
maximize persuasiveness, or remain faithful to their prior beliefs. We
implemented and compared two debate protocols, sequential and simultaneous, to
evaluate potential systematic biases. Finally, we assessed whether models were
more persuasive and produced higher-quality arguments when defending positions
consistent with their prior beliefs versus when arguing against them. Our main
findings show that models tend to prefer defending stances aligned with the
judge persona rather than their prior beliefs, sequential debate introduces
significant bias favoring the second debater, models are more persuasive when
defending positions aligned with their prior beliefs, and paradoxically,
arguments misaligned with prior beliefs are rated as higher quality in pairwise
comparison. These results can inform human judges to provide higher-quality
training signals and contribute to more aligned AI systems, while revealing
important aspects of human-AI interaction regarding persuasion dynamics in
language models.

</details>


### [49] [Synthesizing Agentic Data for Web Agents with Progressive Difficulty Enhancement Mechanisms](https://arxiv.org/abs/2510.13913)
*Shrey Pandit,Xuan-Phi Nguyen,Yifei Ming,Austin Xu,Jiayu Wang,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 本文提出一种新的数据合成方法，通过逐步增加任务难度直至基线网络代理失败来生成问答对。该方法生成的较小数据集能训练出更有效的网络代理，在工具使用多样性上是现有数据集的两倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于网络的深度研究代理在复杂问答任务中面临长程推理和探索的挑战，原因在于底层语言模型对此优化不足。此外，现有指令微调数据集（如基于知识图谱）缺乏对难度和质量的精细控制，生成的合成数据未能捕捉长程推理所需的复杂性。同时，许多研究混淆了数据和训练效果，难以独立评估数据本身的有效性。

Method: 本文引入了一种“两管齐下”的数据合成管道，通过逐步增加任务复杂度生成问答对，直到前沿基线网络代理无法解决为止。该基线代理在此过程中承担多重角色：尝试回答问题、验证事实性、检查替代答案以及执行过滤。为了评估合成方法的有效性，本文采用了基于从强大网络代理进行蒸馏的受控训练设置。

Result: 实验结果表明，尽管本文的数据集更小，但它能训练出比现有数据集更有效的网络代理。特别是，本文数据在工具使用动作上展示了两倍的多样性，使得基于其训练的模型能实现更强的性能，同时避免重复的工具调用行为。

Conclusion: 通过引入一种新颖的数据合成方法，该方法通过挑战基线代理来生成高质量、多样化且复杂度递增的数据集，显著提升了网络深度研究代理的训练效果，使其能够实现更强的性能并有效避免重复的工具调用行为。

Abstract: Web-based 'deep research' agents aim to solve complex question - answering
tasks through long-horizon interactions with online tools. These tasks remain
challenging, as the underlying language models are often not optimized for
long-horizon reasoning and exploration. Prior work has proposed workflows for
constructing instruction-tuning datasets, often leveraging knowledge graphs.
However, such methods typically lack fine-grained control over difficulty and
quality, yielding synthetic data that falls short of capturing the complexity
required for long-horizon reasoning. Furthermore, many studies conflate data
and training effects by comparing models trained under different optimization
recipes, making it difficult to isolate and evaluate the effectiveness of the
data itself. We introduce a two-pronged data synthesis pipeline that generates
question - answer pairs by progressively increasing task complexity until a
frontier baseline web agent fails. The baseline agent plays multiple roles in
this process: attempting the questions, validating factuality, checking for
alternative answers, and enforcing filtering. To evaluate the effectiveness of
our synthesis methods, we adopt a controlled training setup based on
distillation from strong web agents. Experiments across multiple web-based
benchmarks show that our dataset - despite being smaller - enables the training
of more effective web agents than existing datasets. In particular, our data
exhibits twice the diversity in tool-use actions, allowing models trained on it
to achieve stronger performance while avoiding repetitive tool-calling
behaviors.

</details>


### [50] [Readability $\ne$ Learnability: Rethinking the Role of Simplicity in Training Small Language Models](https://arxiv.org/abs/2510.13915)
*Ivan Lee,Taylor Berg-Kirkpatrick*

Main category: cs.CL

TL;DR: 研究发现小型语言模型生成连贯文本的能力并非由文本可读性决定，而是由统计简易性（n-gram多样性）决定。


<details>
  <summary>Details</summary>
Motivation: 现有研究认为，小型语言模型在简化语料上训练能生成连贯文本，这被解释为可读性是关键因素。本文旨在挑战这一普遍解释。

Method: 构建了结构匹配但可读性不同的合成数据集，并在此数据集上训练小型语言模型，以评估可读性对模型连贯性和学习效率的影响。

Result: 研究发现可读性并不能独立预测小型语言模型的连贯性或学习效率。在复杂成人文本上训练的模型表现与简化文本模型相当，甚至在训练过程中能更快地发展连贯性。相反，通过n-gram多样性衡量的统计简易性是更强的学习能力预测因子。

Conclusion: 研究结果警示不应过度拟人化语言模型训练（如与人类认知发展类比），并呼吁更精确地推理哪些属性真正支持小型模型能力的涌现。

Abstract: Recent studies suggest that very small language models (SLMs) can generate
surprisingly coherent text when trained on simplified, child-directed corpora
such as TinyStories. These findings have been interpreted as evidence that
readability -- characterized by accessible vocabulary, familiar narrative
structure, and simple syntax -- plays a key role in enabling such capabilities
to emerge. In this paper, we challenge that interpretation. We construct
synthetic datasets with matched structure but varied readability, and find that
readability alone does not predict coherence or learning efficiency in SLMs.
Models trained on complex, adult-level text perform comparably to those trained
on simplified language, and even exhibit faster development of coherence during
training. Instead, we show that statistical simplicity, as measured by n-gram
diversity, is a stronger predictor of learnability. Our findings caution
against the growing trend of anthropomorphizing language model training --
drawing parallels to human cognitive development without empirical basis -- and
argue for more precise reasoning about what properties actually support
capability emergence in small models.

</details>


### [51] [Element2Vec: Build Chemical Element Representation from Text for Property Prediction](https://arxiv.org/abs/2510.13916)
*Yuanhao Li,Keyuan Lai,Tianqi Wang,Qihao Liu,Jiawei Ma,Yuan-Chao Hu*

Main category: cs.CL

TL;DR: 针对化学元素性质数据难以直接测量的痛点，本文提出Element2Vecto模型，利用维基百科文本通过语言模型生成元素的全局和局部嵌入，并设计了基于自注意力机制的测试时训练方法，以应对数据稀疏和文本分布差异带来的挑战，旨在提高材料科学中的AI驱动发现能力。


<details>
  <summary>Details</summary>
Motivation: 材料设计和制造急需准确的化学元素性质数据，但许多数据难以直接测量。传统数值分析方法难以建模复杂关系，现有AI工具（如语言模型）又存在幻觉和可解释性差的问题。

Method: 本文提出Element2Vecto模型，通过语言模型从维基百科文本中为化学元素生成通用（全局）和属性突出（局部）的嵌入向量。为解决文本分布差异和数据稀疏性问题（仅118种已知元素），设计了一种基于自注意力机制的测试时训练方法来缓解预测误差。

Result: 该方法能够有效表示化学元素，并显著减轻了由朴素回归引起的预测误差，成功应对了文本分布差异和数据极端有限的计算挑战。

Conclusion: 本工作旨在为材料科学领域的AI驱动发现铺平道路，通过有效表示化学元素并提高其性质预测精度来支持自然科学研究。

Abstract: Accurate property data for chemical elements is crucial for materials design
and manufacturing, but many of them are difficult to measure directly due to
equipment constraints. While traditional methods use the properties of other
elements or related properties for prediction via numerical analyses, they
often fail to model complex relationships. After all, not all characteristics
can be represented as scalars. Recent efforts have been made to explore
advanced AI tools such as language models for property estimation, but they
still suffer from hallucinations and a lack of interpretability. In this paper,
we investigate Element2Vecto effectively represent chemical elements from
natural languages to support research in the natural sciences. Given the text
parsed from Wikipedia pages, we use language models to generate both a single
general-purpose embedding (Global) and a set of attribute-highlighted vectors
(Local). Despite the complicated relationship across elements, the
computational challenges also exist because of 1) the discrepancy in text
distribution between common descriptions and specialized scientific texts, and
2) the extremely limited data, i.e., with only 118 known elements, data for
specific properties is often highly sparse and incomplete. Thus, we also design
a test-time training method based on self-attention to mitigate the prediction
error caused by Vanilla regression clearly. We hope this work could pave the
way for advancing AI-driven discovery in materials science.

</details>


### [52] [Optimal Aggregation of LLM and PRM Signals for Efficient Test-Time Scaling](https://arxiv.org/abs/2510.13918)
*Peng Kuang,Yanli Wang,Xiaoyu Han,Yaowenqi Liu,Kaidi Xu,Haohan Wang*

Main category: cs.CL

TL;DR: 本文提出一种智能加权聚合策略，以优化大语言模型(LLM)和过程奖励模型(PRM)的信号结合，显著提升测试时扩展(TTS)效率，超越简单投票并大幅减少计算量。


<details>
  <summary>Details</summary>
Motivation: 过程奖励模型(PRMs)在测试时扩展(TTS)中用于验证和选择LLM响应，但近期基准测试显示，简单的多数投票有时优于PRM，这引发了如何有效利用PRM验证信号的关键问题。

Method: 研究首先开发了一个理论框架，用于最优地结合LLM和PRM的信号，揭示最优策略是加权聚合。基于理论结果，提出了高效的预计算方法来校准这些加权函数，因为发现最优权重函数在不同LLM-PRM对之间差异显著，且常包含负权重。

Result: 实验结果表明，所提出的校准方法显著提升了TTS效率，超越了普通的加权多数投票，并且仅使用了21.3%的计算资源，并在5个LLM和7个PRM上进行了广泛验证。

Conclusion: 研究最终证明，投资于更智能的聚合策略是比简单地扩大测试时计算量更能有效提升性能的途径。

Abstract: Process reward models (PRMs) are a cornerstone of test-time scaling (TTS),
designed to verify and select the best responses from large language models
(LLMs). However, this promise is challenged by recent benchmarks where simple
majority voting, which ignores PRM signals, occasionally outperforms standard
PRM-based selection. This raises a critical question: How can we effectively
utilize verification signals from PRMs for TTS? To address this, we start by
developing a theoretical framework for optimally combining signals from both
the LLM and the PRM. Our framework reveals that the optimal strategy is a
weighted aggregation of responses, a strategy whose effectiveness hinges on
estimating weights that capture the complex interplay between the models. Based
on our theoretical results, we empirically show that these optimal weighting
functions differ significantly across LLM-PRM pairs and, notably, often assign
substantial negative weights. Motivated by these insights, we propose efficient
pre-computation methods to calibrate these weighting functions. Extensive
experiments across 5 LLMs and 7 PRMs demonstrate that our calibration method
significantly boosts the TTS efficiency, surpassing the performance of vanilla
weighted majority voting while using only $21.3\%$ of the computation.
Ultimately, our work demonstrates that investing in a more intelligent
aggregation strategy can be a more convincing path to performance gains than
simply scaling test-time computation.

</details>


### [53] [FACTS: Table Summarization via Offline Template Generation with Agentic Workflows](https://arxiv.org/abs/2510.13920)
*Ye Yuan,Mohammad Amin Shabani,Siqi Liu*

Main category: cs.CL

TL;DR: 本文提出FACTS，一种通过离线模板生成实现快速、准确、隐私合规的查询导向表格摘要智能体工作流，有效解决现有方法痛点，并在基准测试中超越基线。


<details>
  <summary>Details</summary>
Motivation: 现有查询导向表格摘要方法存在局限：表格到文本模型需昂贵微调且推理能力弱；基于LLM的方法受限于token、效率和隐私问题；现有智能体管道依赖分解、规划或手动模板，缺乏鲁棒性和可扩展性。

Method: 引入FACTS智能体工作流，通过“离线模板生成”实现。FACTS生成由SQL查询和Jinja2模板组成的离线模板，这些模板可渲染成自然语言摘要，并在相同schema的多个表格间复用。它通过复用模板实现快速摘要，通过可执行SQL查询确保准确性，并通过仅发送表格schema给LLM实现隐私合规。

Result: 在广泛使用的基准测试中，FACTS持续优于所有基线方法。

Conclusion: FACTS被确立为解决真实世界查询导向表格摘要问题的实用解决方案。

Abstract: Query-focused table summarization requires generating natural language
summaries of tabular data conditioned on a user query, enabling users to access
insights beyond fact retrieval. Existing approaches face key limitations:
table-to-text models require costly fine-tuning and struggle with complex
reasoning, prompt-based LLM methods suffer from token-limit and efficiency
issues while exposing sensitive data, and prior agentic pipelines often rely on
decomposition, planning, or manual templates that lack robustness and
scalability. To mitigate these issues, we introduce an agentic workflow, FACTS,
a Fast, Accurate, and Privacy-Compliant Table Summarization approach via
Offline Template Generation. FACTS produces offline templates, consisting of
SQL queries and Jinja2 templates, which can be rendered into natural language
summaries and are reusable across multiple tables sharing the same schema. It
enables fast summarization through reusable offline templates, accurate outputs
with executable SQL queries, and privacy compliance by sending only table
schemas to LLMs. Evaluations on widely-used benchmarks show that FACTS
consistently outperforms baseline methods, establishing it as a practical
solution for real-world query-focused table summarization.

</details>


### [54] [BioMedSearch: A Multi-Source Biomedical Retrieval Framework Based on LLMs](https://arxiv.org/abs/2510.13926)
*Congying Liu,Xingyuan Wei,Peipei Liu,Yiqing Shen,Yanxu Mao,Tiehan Cui*

Main category: cs.CL

TL;DR: BioMedSearch是一个基于大型语言模型的多源生物医学信息检索框架，通过整合文献、蛋白质数据库和网络搜索，显著提高了复杂生物医学查询的准确性，并推出了新的评估数据集BioMedMCQs。


<details>
  <summary>Details</summary>
Motivation: 生物医学查询需要深厚的专业知识、复杂的生理过程分析和多源信息整合。现有大型语言模型（LLMs）在生成生物医学内容时，因无法访问权威数据库常缺乏科学严谨性，容易编造信息。

Method: 本文提出了BioMedSearch框架，一个基于LLMs的多源生物医学信息检索方法。它整合了文献检索、蛋白质数据库和网络搜索，通过子查询分解、关键词提取、任务图构建和多源信息过滤来处理复杂查询。为评估其准确性，构建了一个包含3,000个问题的多级别数据集BioMedMCQs，涵盖机制识别、非相邻语义整合和时间因果推理。

Result: 实验结果表明，BioMedSearch在所有基线模型中均能持续提高准确性。在Level 1（机制识别）中，平均准确率从59.1%提升至91.9%；在Level 2（非相邻语义整合）中，从47.0%提升至81.0%；在最具挑战的Level 3（时间因果推理）中，从36.3%提升至73.4%。

Conclusion: BioMedSearch通过有效整合多源生物医学信息，成功克服了LLMs在处理复杂生物医学查询时缺乏科学严谨性的问题，显著提升了问答准确率。

Abstract: Biomedical queries often rely on a deep understanding of specialized
knowledge such as gene regulatory mechanisms and pathological processes of
diseases. They require detailed analysis of complex physiological processes and
effective integration of information from multiple data sources to support
accurate retrieval and reasoning. Although large language models (LLMs) perform
well in general reasoning tasks, their generated biomedical content often lacks
scientific rigor due to the inability to access authoritative biomedical
databases and frequently fabricates protein functions, interactions, and
structural details that deviate from authentic information. Therefore, we
present BioMedSearch, a multi-source biomedical information retrieval framework
based on LLMs. The method integrates literature retrieval, protein database and
web search access to support accurate and efficient handling of complex
biomedical queries. Through sub-queries decomposition, keywords extraction,
task graph construction, and multi-source information filtering, BioMedSearch
generates high-quality question-answering results. To evaluate the accuracy of
question answering, we constructed a multi-level dataset, BioMedMCQs,
consisting of 3,000 questions. The dataset covers three levels of reasoning:
mechanistic identification, non-adjacent semantic integration, and temporal
causal reasoning, and is used to assess the performance of BioMedSearch and
other methods on complex QA tasks. Experimental results demonstrate that
BioMedSearch consistently improves accuracy over all baseline models across all
levels. Specifically, at Level 1, the average accuracy increases from 59.1% to
91.9%; at Level 2, it rises from 47.0% to 81.0%; and at the most challenging
Level 3, the average accuracy improves from 36.3% to 73.4%. The code and
BioMedMCQs are available at: https://github.com/CyL-ucas/BioMed_Search

</details>


### [55] [LLMs Can Get "Brain Rot"!](https://arxiv.org/abs/2510.13928)
*Shuo Xing,Junyuan Hong,Yifan Wang,Runjin Chen,Zhenyu Zhang,Ananth Grama,Zhengzhong Tu,Zhangyang Wang*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型（LLMs）持续接触低质量网络文本会导致认知能力持久性下降，表现为推理、长文本理解和安全性等方面的显著衰退，且这种损伤难以完全恢复。数据质量是LLM能力衰退的因果驱动因素。


<details>
  <summary>Details</summary>
Motivation: 提出并检验“LLM大脑腐烂假说”：持续接触低质量网络文本会导致大型语言模型（LLMs）持久的认知能力下降。

Method: 通过在真实Twitter/X语料库上构建低质量和受控数据集进行受控实验，以因果隔离数据质量影响。采用两种正交方法（M1：互动程度；M2：语义质量）操作化低质量数据，并匹配了token规模和训练操作。对4个LLM进行了持续预训练。进行了错误取证分析，以揭示衰退的机制。

Result: 1. 持续预训练低质量数据导致LLM在推理、长文本理解、安全性方面出现非轻微下降（Hedges' g>0.3），并增加了“黑暗特质”。
2. 低质量数据与干净数据混合时，LLM认知能力表现出剂量-反应衰退，例如ARC-Challenge和RULER-CWE分数随低质量数据比例升高而显著下降。
3. 错误取证显示，“思维跳跃”是主要的损伤原因，模型倾向于截断或跳过推理链。
4. 指令微调和干净数据预训练可以部分改善下降的认知能力，但无法完全恢复基线水平，表明存在持久的表征漂移。
5. 推文的受欢迎程度（非语义指标）在M1操作中比长度更能指示“大脑腐烂”效应。

Conclusion: 数据质量是LLM能力衰退的因果驱动因素。持续预训练的数据管理应被视为一个“训练时安全”问题，并建议对已部署的LLM进行常规的“认知健康检查”。

Abstract: We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk
web text induces lasting cognitive decline in large language models (LLMs). To
causally isolate data quality, we run controlled experiments on real Twitter/X
corpora, constructing junk and reversely controlled datasets via two orthogonal
operationalizations: M1 (engagement degree) and M2 (semantic quality), with
matched token scale and training operations across conditions. Contrary to the
control group, continual pre-training of 4 LLMs on the junk dataset causes
non-trivial declines (Hedges' $g>0.3$) on reasoning, long-context
understanding, safety, and inflating "dark traits" (e.g., psychopathy,
narcissism). The gradual mixtures of junk and control datasets also yield
dose-response cognition decay: for example, under M1, ARC-Challenge with Chain
Of Thoughts drops $74.9 \rightarrow 57.2$ and RULER-CWE $84.4 \rightarrow 52.3$
as junk ratio rises from $0\%$ to $100\%$.
  Error forensics reveal several key insights. First, we identify
thought-skipping as the primary lesion: models increasingly truncate or skip
reasoning chains, explaining most of the error growth. Second, partial but
incomplete healing is observed: scaling instruction tuning and clean data
pre-training improve the declined cognition yet cannot restore baseline
capability, suggesting persistent representational drift rather than format
mismatch. Finally, we discover that the popularity, a non-semantic metric, of a
tweet is a better indicator of the Brain Rot effect than the length in M1.
Together, the results provide significant, multi-perspective evidence that data
quality is a causal driver of LLM capability decay, reframing curation for
continual pretraining as a \textit{training-time safety} problem and motivating
routine "cognitive health checks" for deployed LLMs.

</details>


### [56] [Robust or Suggestible? Exploring Non-Clinical Induction in LLM Drug-Safety Decisions](https://arxiv.org/abs/2510.13931)
*Siying Liu,Shisheng Zhang,Indu Bala*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型在药物不良事件预测中存在系统性偏见，对弱势群体预测的风险更高，并识别出显性和隐性偏见，这在使用LLMs进行药物警戒时构成重大风险。


<details>
  <summary>Details</summary>
Motivation: LLMs在生物医学领域应用日益广泛，但其在药物安全预测中的可靠性，特别是是否会将临床上无关的社会人口学信息纳入不良事件预测中，尚未得到充分探索。

Method: 使用美国FDA不良事件报告系统(FAERS)的结构化数据，通过基于角色的评估框架，评估了ChatGPT-4o和Bio-Medical-Llama-3.8B两种模型。评估涵盖教育、婚姻状况、就业、保险、语言、住房稳定性和宗教等多样化角色，并考虑了全科医生、专科医生和患者三种用户角色。

Result: 结果显示不良事件预测准确性存在系统性差异。弱势群体（如低教育、住房不稳定）被赋予的预测不良事件可能性高于特权群体（如研究生学历、私人保险）。识别出两种偏见模式：显性偏见（预测直接引用角色属性）和隐性偏见（预测不一致但未明确提及角色）。

Conclusion: 研究揭示了将LLMs应用于药物警戒的重大风险，并强调在临床部署前，迫切需要制定公平性评估协议和缓解策略。

Abstract: Large language models (LLMs) are increasingly applied in biomedical domains,
yet their reliability in drug-safety prediction remains underexplored. In this
work, we investigate whether LLMs incorporate socio-demographic information
into adverse event (AE) predictions, despite such attributes being clinically
irrelevant. Using structured data from the United States Food and Drug
Administration Adverse Event Reporting System (FAERS) and a persona-based
evaluation framework, we assess two state-of-the-art models, ChatGPT-4o and
Bio-Medical-Llama-3.8B, across diverse personas defined by education, marital
status, employment, insurance, language, housing stability, and religion. We
further evaluate performance across three user roles (general practitioner,
specialist, patient) to reflect real-world deployment scenarios where
commercial systems often differentiate access by user type. Our results reveal
systematic disparities in AE prediction accuracy. Disadvantaged groups (e.g.,
low education, unstable housing) were frequently assigned higher predicted AE
likelihoods than more privileged groups (e.g., postgraduate-educated, privately
insured). Beyond outcome disparities, we identify two distinct modes of bias:
explicit bias, where incorrect predictions directly reference persona
attributes in reasoning traces, and implicit bias, where predictions are
inconsistent, yet personas are not explicitly mentioned. These findings expose
critical risks in applying LLMs to pharmacovigilance and highlight the urgent
need for fairness-aware evaluation protocols and mitigation strategies before
clinical deployment.

</details>


### [57] [Big Reasoning with Small Models: Instruction Retrieval at Inference Time](https://arxiv.org/abs/2510.13935)
*Kenan Alkiek,David Jurgens,Vinod Vydiswaran*

Main category: cs.CL

TL;DR: 该研究提出一种名为“指令检索”的方法，使小型语言模型 (SLMs) 在推理时检索结构化推理步骤，而非从头生成，从而在不进行额外微调的情况下，显著提升SLMs在复杂推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型 (SLMs) 因其本地运行效率、高隐私性、低成本和环保性而日益受欢迎。然而，它们在需要多步推理或领域特定知识的任务上表现不佳，限制了其应用范围。本研究旨在解决SLMs的这一推理能力局限。

Method: 该方法在推理时通过指令干预来增强SLMs。首先，通过将相似的训练问题分组，并使用GPT-5生成指令，构建一个“指令语料库”。在推理阶段，SLM检索最相关的指令并遵循其步骤。与检索增强生成 (RAG) 检索文本段落不同，指令检索为模型提供结构化的推理指导。

Result: 该框架在MedQA、MMLU专业法律和MathQA数据集上，使用3B到14B参数的模型（未经额外微调）进行了评估，均取得了一致的性能提升：MedQA上提升9.4%，MMLU法律上提升7.9%，MathQA上提升5.1%。研究还发现，简洁的指令优于冗长的指令，且改进幅度强烈依赖于模型家族和其固有的推理能力。

Conclusion: 指令检索是一种有效且无需微调的策略，能够显著提升小型语言模型在多步推理和领域特定知识任务上的表现，使其在本地计算环境中也能获得类似大规模推理的能力。

Abstract: Can we bring large-scale reasoning to local-scale compute? Small language
models (SLMs) are increasingly attractive because they run efficiently on local
hardware, offering strong privacy, low cost, and reduced environmental impact.
Yet they often struggle with tasks that require multi-step reasoning or
domain-specific knowledge. We address this limitation through instruction
intervention at inference time, where an SLM retrieves structured reasoning
procedures rather than generating them from scratch. Our method builds an
Instruction Corpus by grouping similar training questions and creating
instructions via GPT-5. During inference, the SLM retrieves the most relevant
instructions and follows their steps. Unlike retrieval-augmented generation,
which retrieves text passages, instruction retrieval gives the model structured
guidance for reasoning. We evaluate this framework on MedQA (medical board
exams), MMLU Professional Law, and MathQA using models from 3B to 14B
parameters without any additional fine-tuning. Instruction retrieval yields
consistent gains: 9.4% on MedQA, 7.9% on MMLU Law, and 5.1% on MathQA. Concise
instructions outperform longer ones, and the magnitude of improvement depends
strongly on model family and intrinsic reasoning ability.

</details>


### [58] [FinDeepResearch: Evaluating Deep Research Agents in Rigorous Financial Analysis](https://arxiv.org/abs/2510.13936)
*Fengbin Zhu,Xiang Yao Ng,Ziyang Liu,Chang Liu,Xianwei Zeng,Chao Wang,Tianhui Tan,Xuan Yao,Pengyang Shao,Min Xu,Zixuan Wang,Jing Wang,Xin Lin,Junfeng Li,Jingxian Zhu,Yang Zhang,Wenjie Wang,Fuli Feng,Richang Hong,Huanbo Luan,Ke-Wei Huang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 本文提出了HisRubric评估框架和FinDeepResearch基准，系统评估了深度研究（DR）智能体在企业财务分析中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏对由大型语言模型（LLMs）驱动的DR智能体在关键研究分析能力方面的严格和系统评估。

Method: 1. 提出了HisRubric，一个具有层级分析结构和细粒度评分标准的评估框架，模拟专业分析师的工作流程。2. 构建了FinDeepResearch基准，涵盖64家上市公司、8个金融市场、4种语言，共15,808个评分项。3. 使用16种代表性方法（包括DR智能体和具备不同推理/搜索能力的LLM）进行了广泛实验。

Result: 实验结果揭示了这些方法在不同能力、金融市场和语言上的优势与局限性。

Conclusion: 本研究为未来的DR智能体研究和开发提供了宝贵的见解，并将公开基准和评估代码。

Abstract: Deep Research (DR) agents, powered by advanced Large Language Models (LLMs),
have recently garnered increasing attention for their capability in conducting
complex research tasks. However, existing literature lacks a rigorous and
systematic evaluation of DR Agent's capabilities in critical research analysis.
To address this gap, we first propose HisRubric, a novel evaluation framework
with a hierarchical analytical structure and a fine-grained grading rubric for
rigorously assessing DR agents' capabilities in corporate financial analysis.
This framework mirrors the professional analyst's workflow, progressing from
data recognition to metric calculation, and finally to strategic summarization
and interpretation. Built on this framework, we construct a FinDeepResearch
benchmark that comprises 64 listed companies from 8 financial markets across 4
languages, encompassing a total of 15,808 grading items. We further conduct
extensive experiments on the FinDeepResearch using 16 representative methods,
including 6 DR agents, 5 LLMs equipped with both deep reasoning and search
capabilities, and 5 LLMs with deep reasoning capabilities only. The results
reveal the strengths and limitations of these approaches across diverse
capabilities, financial markets, and languages, offering valuable insights for
future research and development. The benchmark and evaluation code will be made
publicly available.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [59] [MultiFoodhat: A potential new paradigm for intelligent food quality inspection](https://arxiv.org/abs/2510.13889)
*Yue Hu,Guohang Zhuang*

Main category: cs.CV

TL;DR: MultiFoodChat是一个对话驱动的多智能体推理框架，结合视觉-语言模型和大型语言模型，实现了零样本食物识别，在未见食物类别上表现出卓越的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有监督模型严重依赖大型标注数据集，且对未见食物类别的泛化能力有限，阻碍了食物图像分类在智能食物质检、饮食评估等领域的应用。

Method: 引入MultiFoodChat框架，它是一个对话驱动的多智能体推理框架，用于零样本食物识别。该框架整合了视觉-语言模型(VLMs)和大型语言模型(LLMs)，通过多轮视觉-文本对话实现协作推理。它使用对象感知令牌(OPT)捕获细粒度视觉属性，并由交互式推理智能体(IRA)动态解释上下文线索以改进预测。该多智能体设计无需额外训练或手动标注，即可实现对复杂食物场景的灵活、类人理解。

Result: 在多个公共食物数据集上的实验表明，MultiFoodChat在识别准确性和可解释性方面均优于现有的无监督和少样本方法。

Conclusion: MultiFoodChat展现了作为智能食物质检和分析新范式的潜力。

Abstract: Food image classification plays a vital role in intelligent food quality
inspection, dietary assessment, and automated monitoring. However, most
existing supervised models rely heavily on large labeled datasets and exhibit
limited generalization to unseen food categories. To overcome these challenges,
this study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning
framework for zero-shot food recognition. The framework integrates
vision-language models (VLMs) and large language models (LLMs) to enable
collaborative reasoning through multi-round visual-textual dialogues. An Object
Perception Token (OPT) captures fine-grained visual attributes, while an
Interactive Reasoning Agent (IRA) dynamically interprets contextual cues to
refine predictions. This multi-agent design allows flexible and human-like
understanding of complex food scenes without additional training or manual
annotations. Experiments on multiple public food datasets demonstrate that
MultiFoodChat achieves superior recognition accuracy and interpretability
compared with existing unsupervised and few-shot methods, highlighting its
potential as a new paradigm for intelligent food quality inspection and
analysis.

</details>


### [60] [Post-surgical Endometriosis Segmentation in Laparoscopic Videos](https://arxiv.org/abs/2510.13899)
*Andreas Leibetseder,Klaus Schoeffmann,Jörg Keckstein,Simon Keckstein*

Main category: cs.CV

TL;DR: 本文介绍了一个系统，通过分析腹腔镜手术视频，自动识别和分割常见的深色子宫内膜异位症病灶，并提供可视化标注和检测摘要，旨在辅助妇科医生诊断。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症表现多样且位置不一，导致其识别困难且易出错，尤其对于非专业医生。本研究旨在为妇科医生提供辅助。

Method: 开发并训练了一个系统，用于分割腹腔镜手术视频中常见的深色子宫内膜异位症病灶。

Result: 该系统能够分析腹腔镜手术视频，用多色叠加层注释识别出的病灶区域，并显示检测摘要以改善视频浏览。

Conclusion: 该系统旨在为妇科医生提供辅助，提高子宫内膜异位症（深色病灶）的识别效率和准确性，并优化视频审查过程。

Abstract: Endometriosis is a common women's condition exhibiting a manifold visual
appearance in various body-internal locations. Having such properties makes its
identification very difficult and error-prone, at least for laymen and
non-specialized medical practitioners. In an attempt to provide assistance to
gynecologic physicians treating endometriosis, this demo paper describes a
system that is trained to segment one frequently occurring visual appearance of
endometriosis, namely dark endometrial implants. The system is capable of
analyzing laparoscopic surgery videos, annotating identified implant regions
with multi-colored overlays and displaying a detection summary for improved
video browsing.

</details>


### [61] [Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models](https://arxiv.org/abs/2510.13993)
*Jia Yun Chua,Argyrios Zolotas,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: 本研究结合传统视觉模型（如YOLO）与视觉语言模型（如LLaVA、ChatGPT、Gemini），显著提升了遥感图像中的飞机检测（MAE提升48.46%）和场景理解（CLIPScore提升6.17%），尤其在挑战性及退化数据条件下表现优异，为更高效的遥感分析开辟了道路。


<details>
  <summary>Details</summary>
Motivation: 遥感数据量巨大，但传统视觉模型受限于对大量领域特定标注数据的需求，且在复杂环境中上下文理解能力有限。视觉语言模型（VLM）整合视觉与文本数据，提供互补方法，但在遥感领域的应用尚待深入探索，尤其考虑到其通用性。

Method: 本研究通过结合传统视觉模型（具体为YOLO）与多种视觉语言模型（LLaVA, ChatGPT, Gemini），旨在增强遥感图像分析，重点关注飞机检测和场景理解。性能评估在标注、未标注及退化遥感图像数据集上进行。

Result: 研究发现，在飞机检测和计数准确性方面，模型平均MAE提升了48.46%，尤其在原始和退化场景的挑战性条件下效果显著。对于遥感图像的综合理解，CLIPScore提升了6.17%。

Conclusion: 结合传统视觉模型与VLM的方法，为更先进、高效的遥感图像分析铺平了道路，尤其在小样本学习场景中具有重要价值。

Abstract: Remote sensing has become a vital tool across sectors such as urban planning,
environmental monitoring, and disaster response. While the volume of data
generated has increased significantly, traditional vision models are often
constrained by the requirement for extensive domain-specific labelled data and
their limited ability to understand the context within complex environments.
Vision Language Models offer a complementary approach by integrating visual and
textual data; however, their application to remote sensing remains
underexplored, particularly given their generalist nature. This work
investigates the combination of vision models and VLMs to enhance image
analysis in remote sensing, with a focus on aircraft detection and scene
understanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and
Gemini aims to achieve more accurate and contextually aware image
interpretation. Performance is evaluated on both labelled and unlabelled remote
sensing data, as well as degraded image scenarios which are crucial for remote
sensing. The findings show an average MAE improvement of 48.46% across models
in the accuracy of aircraft detection and counting, especially in challenging
conditions, in both raw and degraded scenarios. A 6.17% improvement in
CLIPScore for comprehensive understanding of remote sensing images is obtained.
The proposed approach combining traditional vision models and VLMs paves the
way for more advanced and efficient remote sensing image analysis, especially
in few-shot learning scenarios.

</details>


### [62] [Finding Holes: Pathologist Level Performance Using AI for Cribriform Morphology Detection in Prostate Cancer](https://arxiv.org/abs/2510.13995)
*Kelvin Szolnoky,Anders Blilie,Nita Mulliqi,Toyonori Tsuzuki,Hemamali Samaratunga,Matteo Titus,Xiaoyi Ji,Sol Erika Boman,Einar Gudlaugsson,Svein Reidar Kjosavik,José Asenjo,Marcello Gambacorta,Paolo Libretti,Marcin Braun,Radisław Kordek,Roman Łowicki,Brett Delahunt,Kenneth A. Iczkowski,Theo van der Kwast,Geert J. L. H. van Leenders,Katia R. M. Leite,Chin-Chen Pan,Emiel Adrianus Maria Janssen,Martin Eklund,Lars Egevad,Kimmo Kartasalo*

Main category: cs.CV

TL;DR: 开发并验证了一个AI模型，用于检测前列腺癌中的筛状结构，其性能达到病理学家水平，可提高诊断可靠性。


<details>
  <summary>Details</summary>
Motivation: 前列腺癌中的筛状结构预示预后不良且不适合主动监测，但其报告不足且病理学家间判读差异大，因此需要开发AI系统来改进筛状模式检测。

Method: 使用EfficientNetV2-S编码器和多实例学习构建深度学习模型，对640张数字化的前列腺核心穿刺活检切片进行端到端全切片分类训练。模型在内部（261张切片）和外部（266张切片）进行验证，并与9位专家病理学家进行了比较。

Result: 模型在内部验证中表现良好（AUC: 0.97, Kappa: 0.81），外部验证稳健（AUC: 0.90, Kappa: 0.55）。在病理学家间一致性分析中，模型获得最高平均一致性（Kappa: 0.66），优于所有9位病理学家（Kappa范围0.35至0.62）。

Conclusion: 该AI模型在前列腺癌筛状结构检测方面达到病理学家水平，有望提高诊断可靠性，实现报告标准化，并改进前列腺癌患者的治疗决策。

Abstract: Background: Cribriform morphology in prostate cancer is a histological
feature that indicates poor prognosis and contraindicates active surveillance.
However, it remains underreported and subject to significant interobserver
variability amongst pathologists. We aimed to develop and validate an AI-based
system to improve cribriform pattern detection.
  Methods: We created a deep learning model using an EfficientNetV2-S encoder
with multiple instance learning for end-to-end whole-slide classification. The
model was trained on 640 digitised prostate core needle biopsies from 430
patients, collected across three cohorts. It was validated internally (261
slides from 171 patients) and externally (266 slides, 104 patients from three
independent cohorts). Internal validation cohorts included laboratories or
scanners from the development set, while external cohorts used completely
independent instruments and laboratories. Annotations were provided by three
expert uropathologists with known high concordance. Additionally, we conducted
an inter-rater analysis and compared the model's performance against nine
expert uropathologists on 88 slides from the internal validation cohort.
  Results: The model showed strong internal validation performance (AUC: 0.97,
95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89) and robust external
validation (AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI:
0.45-0.64). In our inter-rater analysis, the model achieved the highest average
agreement (Cohen's kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine
pathologists whose Cohen's kappas ranged from 0.35 to 0.62.
  Conclusion: Our AI model demonstrates pathologist-level performance for
cribriform morphology detection in prostate cancer. This approach could enhance
diagnostic reliability, standardise reporting, and improve treatment decisions
for prostate cancer patients.

</details>


### [63] [NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations](https://arxiv.org/abs/2510.14025)
*Junjie Nan,Jianing Li,Wei Chen,Mingkun Zhang,Xueqi Cheng*

Main category: cs.CV

TL;DR: 本文提出了NAPPure，一个扩展的对抗性净化框架，旨在有效处理非加性图像扰动，以提升图像分类模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性净化方法主要针对加性扰动，但在面对模糊、遮挡、扭曲等真实世界中常见的非加性扰动时效果大打折扣。

Method: NAPPure框架首先建立对抗图像的生成过程，然后通过最大似然法解耦潜在的干净图像和扰动参数。

Result: 在GTSRB和CIFAR-10数据集上的实验结果表明，NAPPure显著提升了图像分类模型对抗非加性扰动的鲁棒性。

Conclusion: NAPPure框架成功扩展了对抗性净化方法，能够有效应对非加性对抗性图像扰动，从而增强了模型的防御能力。

Abstract: Adversarial purification has achieved great success in combating adversarial
image perturbations, which are usually assumed to be additive. However,
non-additive adversarial perturbations such as blur, occlusion, and distortion
are also common in the real world. Under such perturbations, existing
adversarial purification methods are much less effective since they are
designed to fit the additive nature. In this paper, we propose an extended
adversarial purification framework named NAPPure, which can further handle
non-additive perturbations. Specifically, we first establish the generation
process of an adversarial image, and then disentangle the underlying clean
image and perturbation parameters through likelihood maximization. Experiments
on GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the
robustness of image classification models against non-additive perturbations.

</details>


### [64] [Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding](https://arxiv.org/abs/2510.14032)
*Xiaoqian Shen,Wenxuan Zhang,Jun Chen,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: Vgent提出一个基于图的检索-推理增强生成框架，通过结构化视频图和中间推理步骤，显著提升大型视频语言模型（LVLMs）在长视频理解上的能力，解决上下文窗口和时序依赖性问题，并取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 大型视频语言模型（LVLMs）在处理长视频时面临显著挑战，包括超越上下文窗口的视频令牌处理困难和长期时序信息保留不足。此外，将检索增强生成（RAG）应用于长视频时，存在破坏时序依赖性及引入无关信息从而阻碍准确推理的问题。

Method: 提出Vgent框架，一个新颖的基于图的检索-推理增强生成框架，以增强LVLMs的长视频理解能力。其核心创新包括：(i) 将视频表示为结构化图，保留视频片段间的语义关系，以提高检索效率。(ii) 引入中间推理步骤，利用结构化验证减少检索噪声，并显式聚合片段间的相关信息，从而减轻LVLMs的推理局限性。

Result: Vgent在三个长视频理解基准上进行了全面评估。结果显示，与基线模型相比，Vgent在MLVU上取得了3.0%~5.4%的整体性能提升，并超越了最先进的视频RAG方法8.6%。

Conclusion: Vgent通过其创新的图表示和中间推理步骤，有效解决了大型视频语言模型在长视频理解中面临的检索和推理挑战，显著提高了模型的准确性和上下文感知能力，从而生成更精确的响应。

Abstract: Understanding and reasoning over long videos pose significant challenges for
large video language models (LVLMs) due to the difficulty in processing
intensive video tokens beyond context window and retaining long-term sequential
information. Retrieval-Augmented Generation (RAG) has demonstrated
effectiveness in processing long context for Large Language Models (LLMs);
however, applying RAG to long video faces challenges such as disrupted temporal
dependencies and inclusion of irrelevant information that can hinder accurate
reasoning. To address these limitations, we propose Vgent, a novel graph-based
retrieval-reasoning-augmented generation framework to enhance LVLMs for long
video understanding. Our approach introduces two key innovations: (i) It
represents videos by structured graphs with semantic relationships across video
clips preserved to improve retrieval effectiveness. (ii) It introduces an
intermediate reasoning step to mitigate the reasoning limitation of LVLMs,
which leverages structured verification to reduce retrieval noise and
facilitate the explicit aggregation of relevant information across clips,
resulting in more accurate and context-aware responses. We comprehensively
evaluate our framework with various open-source LVLMs on three long-video
understanding benchmarks. Our approach yielded an overall performance
improvement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformed
state-of-the-art video RAG methods by $8.6\%$. Our code is publicly available
at https://xiaoqian-shen.github.io/Vgent.

</details>


### [65] [Synchronization of Multiple Videos](https://arxiv.org/abs/2510.14051)
*Avihai Naaman,Ron Shapira Weber,Oren Freifeld*

Main category: cs.CV

TL;DR: 针对跨场景或生成式AI视频同步的复杂挑战，提出了TPL框架，通过学习统一的原型序列实现鲁棒、高效的视频对齐。


<details>
  <summary>Details</summary>
Motivation: 现有视频同步方法在处理不同场景或生成式AI视频时面临复杂挑战，原因是主体、背景多样且存在非线性时间错位。

Method: 提出Temporal Prototype Learning (TPL) 框架，它是一种基于原型的学习方法。TPL从各种预训练模型提取的高维嵌入中构建共享、紧凑的1D表示，并通过学习统一的原型序列来锚定关键动作阶段，从而实现视频的鲁棒对齐，避免了穷举式成对匹配。

Result: 实验表明，TPL在包括细粒度帧检索和阶段分类在内的多种数据集上，提高了同步的准确性、效率和鲁棒性。此外，TPL是首个解决多个描绘相同动作的生成式AI视频同步问题的方案。

Conclusion: TPL框架通过其独特的原型学习机制，有效解决了跨场景及生成式AI视频的同步难题，实现了更准确、高效且鲁棒的视频对齐，尤其在AI生成视频同步领域具有开创性意义。

Abstract: Synchronizing videos captured simultaneously from multiple cameras in the
same scene is often easy and typically requires only simple time shifts.
However, synchronizing videos from different scenes or, more recently,
generative AI videos, poses a far more complex challenge due to diverse
subjects, backgrounds, and nonlinear temporal misalignment. We propose Temporal
Prototype Learning (TPL), a prototype-based framework that constructs a shared,
compact 1D representation from high-dimensional embeddings extracted by any of
various pretrained models. TPL robustly aligns videos by learning a unified
prototype sequence that anchors key action phases, thereby avoiding exhaustive
pairwise matching. Our experiments show that TPL improves synchronization
accuracy, efficiency, and robustness across diverse datasets, including
fine-grained frame retrieval and phase classification tasks. Importantly, TPL
is the first approach to mitigate synchronization issues in multiple generative
AI videos depicting the same action. Our code and a new multiple video
synchronization dataset are available at https://bgu-cs-vil.github.io/TPL/

</details>


### [66] [Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images](https://arxiv.org/abs/2510.14081)
*Emanuel Garbin,Guy Adam,Oded Krams,Zohar Barzelay,Eran Guendelman,Michael Schwarz,Moran Vatelmacher,Yigal Shenkman,Eli Peker,Itai Druker,Uri Patish,Yoav Blum,Max Bluvstein,Junxuan Li,Rawal Khirodkar,Shunsuke Saito*

Main category: cs.CV

TL;DR: 一种零样本管道，能从少量非结构化手机图像生成超现实、身份保持的3D头像。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在几何不一致、幻觉导致身份保持性差，以及合成数据训练的模型无法捕捉高频细节、缺乏真实感等问题。

Method: 提出“捕获、规范化、泼溅（Capture, Canonicalize, Splat）”管道，包括：(1) 生成式规范化模块，将多视角非结构化图像处理成标准一致表示；(2) 基于Transformer的模型，使用来源于真实人物穹顶捕获的新大规模高保真高斯泼溅头像数据集进行训练。

Result: 该管道能从非结构化照片生成具有引人注目的真实感和鲁棒身份保持性的静态四分之一身体头像。

Conclusion: 本研究通过新颖的规范化模块和基于Transformer的模型，成功构建了一个零样本管道，从少量非结构化手机图像高效创建出超现实、身份保持的3D头像，克服了现有方法的局限性。

Abstract: We present a novel, zero-shot pipeline for creating hyperrealistic,
identity-preserving 3D avatars from a few unstructured phone images. Existing
methods face several challenges: single-view approaches suffer from geometric
inconsistencies and hallucinations, degrading identity preservation, while
models trained on synthetic data fail to capture high-frequency details like
skin wrinkles and fine hair, limiting realism. Our method introduces two key
contributions: (1) a generative canonicalization module that processes multiple
unstructured views into a standardized, consistent representation, and (2) a
transformer-based model trained on a new, large-scale dataset of high-fidelity
Gaussian splatting avatars derived from dome captures of real people. This
"Capture, Canonicalize, Splat" pipeline produces static quarter-body avatars
with compelling realism and robust identity preservation from unstructured
photos.

</details>


### [67] [cubic: CUDA-accelerated 3D Bioimage Computing](https://arxiv.org/abs/2510.14143)
*Alexandr A. Kalinin,Anne E. Carpenter,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: 本文介绍了cubic，一个开源Python库，通过整合CuPy和RAPIDS cuCIM，为广泛使用的SciPy和scikit-image API提供GPU加速替代方案，以解决生物图像分析中可扩展性、效率和集成性挑战，显著提升2D和3D图像处理速度。


<details>
  <summary>Details</summary>
Motivation: 多维生物图像的定量分析对理解细胞表型和加速生物医学研究至关重要。然而，现有计算方法在处理现代显微镜生成的大规模2D和3D数据集时，受限于可扩展性、效率和与现代科学计算工作流的集成性。现有生物图像分析工具常缺乏API、不支持GPU加速、3D图像处理能力不足或互操作性差。

Method: 开发并引入了cubic，一个开源Python库。该库通过CuPy和RAPIDS cuCIM的GPU加速替代方案，增强了广泛使用的SciPy和scikit-image API。cubic的API与设备无关，当数据位于GPU上时自动分派到GPU执行，否则在CPU上执行。通过基准测试单个操作以及重现现有去卷积和分割流程来评估其性能。

Result: cubic能够无缝加速各种图像处理例程（包括2D和3D数据的预处理、分割和特征提取）。它在保持算法准确性的同时实现了显著的速度提升，使得现有生物图像分析工作流能够获得GPU加速。

Conclusion: cubic为可扩展、可复现的生物图像分析奠定了坚实基础，并能与更广泛的Python科学计算生态系统（包括其他GPU加速方法）集成。它支持交互式探索和自动化高通量分析工作流，解决了大规模生物图像处理的瓶颈。

Abstract: Quantitative analysis of multidimensional biological images is useful for
understanding complex cellular phenotypes and accelerating advances in
biomedical research. As modern microscopy generates ever-larger 2D and 3D
datasets, existing computational approaches are increasingly limited by their
scalability, efficiency, and integration with modern scientific computing
workflows. Existing bioimage analysis tools often lack application programmable
interfaces (APIs), do not support graphics processing unit (GPU) acceleration,
lack broad 3D image processing capabilities, and/or have poor interoperability
for compute-heavy workflows. Here, we introduce cubic, an open-source Python
library that addresses these challenges by augmenting widely used SciPy and
scikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.
cubic's API is device-agnostic and dispatches operations to GPU when data
reside on the device and otherwise executes on CPU, seamlessly accelerating a
broad range of image processing routines. This approach enables GPU
acceleration of existing bioimage analysis workflows, from preprocessing to
segmentation and feature extraction for 2D and 3D data. We evaluate cubic both
by benchmarking individual operations and by reproducing existing deconvolution
and segmentation pipelines, achieving substantial speedups while maintaining
algorithmic fidelity. These advances establish a robust foundation for
scalable, reproducible bioimage analysis that integrates with the broader
Python scientific computing ecosystem, including other GPU-accelerated methods,
enabling both interactive exploration and automated high-throughput analysis
workflows. cubic is openly available at
https://github$.$com/alxndrkalinin/cubic

</details>


### [68] [Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures](https://arxiv.org/abs/2510.14179)
*Yuancheng Xu,Wenqi Xian,Li Ma,Julien Philip,Ahmet Levent Taşel,Yiwei Zhao,Ryan Burgert,Mingming He,Oliver Hermann,Oliver Pilarski,Rahul Garg,Paul Debevec,Ning Yu*

Main category: cs.CV

TL;DR: 本文提出一个视频扩散模型框架，通过新颖的数据管道实现多视角角色一致性和3D摄像机控制，并支持虚拟制作中的多主题生成、场景定制等功能。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型难以实现多视角角色一致性和精确的3D摄像机控制，限制了其在虚拟制作中的应用。因此，需要一个能够提供强大身份保持、精确摄像机控制和光照适应性的框架。

Method: 该框架通过以下方法实现：1. 使用4D高斯泼溅(4DGS)将记录的体积捕获性能重新渲染，生成包含多样化摄像机轨迹的数据。2. 结合视频重照明模型，提供光照变化。3. 在此数据上微调先进的开源视频扩散模型。4. 通过联合训练和噪声混合两种方法支持多主题生成。5. 支持场景和实拍视频定制，以及对运动和空间布局的控制。

Result: 实验结果显示，该框架显著提高了视频质量、个性化精度（多视角身份保持）、摄像机控制能力和光照适应性。它还成功实现了多主题生成和各种虚拟制作核心功能。

Conclusion: 该框架通过实现强大的多视角身份保持、精确的摄像机控制和光照适应性，以及支持多主题生成等功能，推动了视频生成技术与虚拟制作的集成。

Abstract: We introduce a framework that enables both multi-view character consistency
and 3D camera control in video diffusion models through a novel customization
data pipeline. We train the character consistency component with recorded
volumetric capture performances re-rendered with diverse camera trajectories
via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video
relighting model. We fine-tune state-of-the-art open-source video diffusion
models on this data to provide strong multi-view identity preservation, precise
camera control, and lighting adaptability. Our framework also supports core
capabilities for virtual production, including multi-subject generation using
two approaches: joint training and noise blending, the latter enabling
efficient composition of independently customized models at inference time; it
also achieves scene and real-life video customization as well as control over
motion and spatial layout during customization. Extensive experiments show
improved video quality, higher personalization accuracy, and enhanced camera
control and lighting adaptability, advancing the integration of video
generation into virtual production. Our project page is available at:
https://eyeline-labs.github.io/Virtually-Being.

</details>


### [69] [Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition](https://arxiv.org/abs/2510.14203)
*Ryo Masumura,Shota Orihashi,Mana Ihori,Tomohiro Tanaka,Naoki Makishima,Taiga Yamane,Naotaka Kawata,Satoshi Suzuki,Taichi Katayama*

Main category: cs.CV

TL;DR: 本文提出一种联合建模“大五”和“HEXACO”人格特质的方法，用于从多模态人类行为中自动识别表观人格特质。


<details>
  <summary>Details</summary>
Motivation: 以往多模态表观人格特质识别研究主要集中在“大五”人格上，缺乏对“HEXACO”人格的关注（特别是其“诚实-谦逊”维度）。此外，机器学习模型下“大五”与“HEXACO”之间的关系尚不明确，而理解这些关系有望增进对多模态人类行为的认知。

Method: 提出一种联合建模“大五”和“HEXACO”人格特质的方法，关键在于优化其共同识别。

Result: 使用自我介绍视频数据集进行的实验表明，所提出的方法能够有效地识别“大五”和“HEXACO”人格特特质。

Conclusion: 所提出的联合建模方法能有效识别多模态人类行为中的“大五”和“HEXACO”表观人格特质。

Abstract: This paper proposes a joint modeling method of the Big Five, which has long
been studied, and HEXACO, which has recently attracted attention in psychology,
for automatically recognizing apparent personality traits from multimodal human
behavior. Most previous studies have used the Big Five for multimodal apparent
personality-trait recognition. However, no study has focused on apparent HEXACO
which can evaluate an Honesty-Humility trait related to displaced aggression
and vengefulness, social-dominance orientation, etc. In addition, the
relationships between the Big Five and HEXACO when modeled by machine learning
have not been clarified. We expect awareness of multimodal human behavior to
improve by considering these relationships. The key advance of our proposed
method is to optimize jointly recognizing the Big Five and HEXACO. Experiments
using a self-introduction video dataset demonstrate that the proposed method
can effectively recognize the Big Five and HEXACO.

</details>


### [70] [LOTA: Bit-Planes Guided AI-Generated Image Detection](https://arxiv.org/abs/2510.14230)
*Hongsong Wang,Renxi Cheng,Yang Zhang,Chaolei Han,Jie Gui*

Main category: cs.CV

TL;DR: 提出一种基于位平面处理和梯度噪声放大的轻量级方法，用于高效准确地检测AI生成图像，平均准确率达98.9%，且误差提取速度比现有方法快近百倍。


<details>
  <summary>Details</summary>
Motivation: 随着GAN和Diffusion模型的快速发展，区分AI生成图像和真实图像变得越来越困难。现有方法多依赖图像重建误差，但计算成本高昂且未能有效捕捉原始图像固有的噪声特征。

Method: 通过基于位平面的图像处理精炼误差提取，利用低位平面代表噪声模式；引入位平面引导的噪声图像生成和多种图像归一化策略；设计最大梯度补丁选择，利用多方向梯度计算噪声分数并选择最高分区域以放大噪声信号；提出轻量级分类头，探索噪声基分类器和噪声引导分类器两种结构。

Result: 在GenImage基准上，平均准确率达到98.9%（提升11.9%），展示出卓越的跨生成器泛化能力（GAN到Diffusion超过98.2%，Diffusion到GAN超过99.2%）；误差提取速度达到毫秒级，比现有方法快近百倍。

Conclusion: 该方法通过创新性地利用位平面噪声和梯度放大技术，有效解决了AI图像检测的效率和准确性问题，在准确性、泛化能力和计算速度上均显著优于现有方法，提供了一种快速、准确且普适的AI生成图像检测方案。

Abstract: The rapid advancement of GAN and Diffusion models makes it more difficult to
distinguish AI-generated images from real ones. Recent studies often use
image-based reconstruction errors as an important feature for determining
whether an image is AI-generated. However, these approaches typically incur
high computational costs and also fail to capture intrinsic noisy features
present in the raw images. To solve these problems, we innovatively refine
error extraction by using bit-plane-based image processing, as lower bit planes
indeed represent noise patterns in images. We introduce an effective bit-planes
guided noisy image generation and exploit various image normalization
strategies, including scaling and thresholding. Then, to amplify the noise
signal for easier AI-generated image detection, we design a maximum gradient
patch selection that applies multi-directional gradients to compute the noise
score and selects the region with the highest score. Finally, we propose a
lightweight and effective classification head and explore two different
structures: noise-based classifier and noise-guided classifier. Extensive
experiments on the GenImage benchmark demonstrate the outstanding performance
of our method, which achieves an average accuracy of \textbf{98.9\%}
(\textbf{11.9}\%~$\uparrow$) and shows excellent cross-generator generalization
capability. Particularly, our method achieves an accuracy of over 98.2\% from
GAN to Diffusion and over 99.2\% from Diffusion to GAN. Moreover, it performs
error extraction at the millisecond level, nearly a hundred times faster than
existing methods. The code is at https://github.com/hongsong-wang/LOTA.

</details>


### [71] [PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis](https://arxiv.org/abs/2510.14241)
*Soumyya Kanti Datta,Tanvi Ranga,Chengzhe Sun,Siwei Lyu*

Main category: cs.CV

TL;DR: 提出一种新的多模态音视频框架PIA，通过结合语言、动态面部运动和面部身份线索，显著提高了对现代深度伪造中细微时间差异的检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度伪造检测方法（依赖手动设计阈值、帧级一致性或单模态策略）无法有效识别由GAN、扩散模型等先进生成模型产生的现代深度伪造，这些伪造在帧级表现完美但存在不易察觉的细微时间差异。

Method: 提出了多模态音视频框架PIA (Phoneme-Temporal and Identity-Dynamic Analysis)，该框架整合了语言、动态面部运动和面部识别线索，具体利用音素序列、唇形几何数据和先进面部身份嵌入。

Result: 该集成方法通过识别多个互补模态间的不一致性，显著改善了对细微深度伪造篡改的检测能力。

Conclusion: 通过整合语言、动态面部运动和面部身份等多模态线索，PIA框架有效克服了传统方法的局限性，增强了对先进深度伪造中细微时间差异的检测精度。

Abstract: The rise of manipulated media has made deepfakes a particularly insidious
threat, involving various generative manipulations such as lip-sync
modifications, face-swaps, and avatar-driven facial synthesis. Conventional
detection methods, which predominantly depend on manually designed
phoneme-viseme alignment thresholds, fundamental frame-level consistency
checks, or a unimodal detection strategy, inadequately identify modern-day
deepfakes generated by advanced generative models such as GANs, diffusion
models, and neural rendering techniques. These advanced techniques generate
nearly perfect individual frames yet inadvertently create minor temporal
discrepancies frequently overlooked by traditional detectors. We present a
novel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic
Analysis(PIA), incorporating language, dynamic face motion, and facial
identification cues to address these limitations. We utilize phoneme sequences,
lip geometry data, and advanced facial identity embeddings. This integrated
method significantly improves the detection of subtle deepfake alterations by
identifying inconsistencies across multiple complementary modalities. Code is
available at https://github.com/skrantidatta/PIA

</details>


### [72] [Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication](https://arxiv.org/abs/2510.14245)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: 本文提出了一种名为事件间隔调制（EIM）的新型调制方案，专为基于事件视觉传感器的光通信（OCC）系统设计，旨在提高传输速度并充分利用事件传感器的特性。


<details>
  <summary>Details</summary>
Motivation: 传统基于帧的摄像头OCC系统存在低比特率和高处理负载问题。尽管基于事件视觉传感器（EVS）的OCC系统能实现高速、低延迟通信，但现有调制方案未能充分利用EVS的独特特性。

Method: 提出事件间隔调制（EIM）方案，通过事件之间的时间间隔来调制信息。建立了EIM的理论模型，定制并优化了EVS参数以适应EIM的频率响应，并通过实验确定了EIM的最大调制阶数，并基于这些参数进行了传输实验。

Result: 在室内环境下，成功实现了10米距离28 kbps和50米距离8.4 kbps的传输，为基于事件的OCC系统设定了新的比特率基准。

Conclusion: 事件间隔调制（EIM）方案能有效提升基于事件视觉传感器光通信系统的传输速度，并成功刷新了该领域的比特率记录。

Abstract: Optical camera communication (OCC) represents a promising visible light
communication technology. Nonetheless, typical OCC systems utilizing
frame-based cameras are encumbered by limitations, including low bit rate and
high processing load. To address these issues, OCC system utilizing an
event-based vision sensor (EVS) as receivers have been proposed. The EVS
enables high-speed, low-latency, and robust communication due to its
asynchronous operation and high dynamic range. In existing event-based OCC
systems, conventional modulation schemes such as on-off keying (OOK) and pulse
position modulation have been applied, however, to the best of our knowledge,
no modulation method has been proposed that fully exploits the unique
characteristics of the EVS. This paper proposes a novel modulation scheme,
called the event interval modulation (EIM) scheme, specifically designed for
event-based OCC. EIM enables improvement in transmission speed by modulating
information using the intervals between events. This paper proposes a
theoretical model of EIM and conducts a proof-of-concept experiment. First, the
parameters of the EVS are tuned and customized to optimize the frequency
response specifically for EIM. Then, the maximum modulation order usable in EIM
is determined experimentally. We conduct transmission experiments based on the
obtained parameters. Finally, we report successful transmission at 28 kbps over
10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new
benchmark for bit rate in event-based OCC systems.

</details>


### [73] [MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale Scene Localization and Rendering](https://arxiv.org/abs/2510.14251)
*Mingkai Liu,Dikai Fan,Haohua Que,Haojia Gao,Xiao Liu,Shuxue Peng,Meixia Lin,Shengyu Gu,Ruicong Ye,Wanli Qiu,Handong Yao,Ruopeng Zhang,Xianliang Huang*

Main category: cs.CV

TL;DR: MACE（基于混合专家加速坐标编码方法）通过门控网络和无辅助损失负载均衡策略，解决了大规模场景中高效定位和高质量渲染的挑战，显著降低成本并提升精度。


<details>
  <summary>Details</summary>
Motivation: 大规模场景的高效定位和高质量渲染面临巨大的计算成本挑战。现有场景坐标回归（SCR）方法在扩展到大规模场景时，受限于单一网络的容量而表现不佳。

Method: 提出MACE方法，灵感来源于MOE，引入一个门控网络来隐式分类和选择子网络，确保每次推理仅激活一个子网络。此外，引入无辅助损失负载均衡（ALF-LB）策略以提高大规模场景的定位精度。

Result: 该框架在显著降低成本的同时保持了更高的精度，为大规模场景应用提供了高效解决方案。在Cambridge测试集上的实验表明，该方法仅需10分钟训练即可实现高质量渲染结果。

Conclusion: MACE方法通过其创新的架构和负载均衡策略，有效解决了大规模场景中高效定位和高质量渲染的难题，实现了成本降低和精度提升。

Abstract: Efficient localization and high-quality rendering in large-scale scenes
remain a significant challenge due to the computational cost involved. While
Scene Coordinate Regression (SCR) methods perform well in small-scale
localization, they are limited by the capacity of a single network when
extended to large-scale scenes. To address these challenges, we propose the
Mixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables
efficient localization and high-quality rendering in large-scale scenes.
Inspired by the remarkable capabilities of MOE in large model domains, we
introduce a gating network to implicitly classify and select sub-networks,
ensuring that only a single sub-network is activated during each inference.
Furtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to
enhance the localization accuracy on large-scale scene. Our framework provides
a significant reduction in costs while maintaining higher precision, offering
an efficient solution for large-scale scene applications. Additional
experiments on the Cambridge test set demonstrate that our method achieves
high-quality rendering results with merely 10 minutes of training.

</details>


### [74] [Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization](https://arxiv.org/abs/2510.14255)
*Liao Shen,Wentao Jiang,Yiran Zhu,Tiezheng Ge,Zhiguo Cao,Bo Zheng*

Main category: cs.CV

TL;DR: 提出IPRO，一种基于强化学习的视频扩散框架，通过优化现有模型和新颖的评分机制，解决以人为中心的I2V生成中身份一致性难题。


<details>
  <summary>Details</summary>
Motivation: 现有图像到视频(I2V)模型在生成以人为中心的视频时，难以保持输入图像与生成视频间的人物身份一致性，尤其在表情和动作变化大或人脸占比较小时。由于人类对身份变化高度敏感，这是一个关键但未被充分探索的挑战。

Method: 提出了“身份保持奖励引导优化”(IPRO)，一个基于强化学习的新型视频扩散框架。该方法不改变模型架构，而是通过人脸身份评分器直接优化扩散模型。为提高性能和收敛速度，IPRO通过采样链的最后几步反向传播奖励信号。此外，提出一种新颖的人脸评分机制，将真实视频中的人脸视为特征池以增强泛化能力，并引入KL散度正则化以稳定训练。

Result: 在Wan 2.2 I2V模型和自研I2V模型上的大量实验证明了该方法的有效性。

Conclusion: IPRO通过强化学习驱动的优化算法和创新的评分及正则化机制，有效地提高了以人为中心的I2V生成中人物身份的保持能力。

Abstract: Recent advances in image-to-video (I2V) generation have achieved remarkable
progress in synthesizing high-quality, temporally coherent videos from static
images. Among all the applications of I2V, human-centric video generation
includes a large portion. However, existing I2V models encounter difficulties
in maintaining identity consistency between the input human image and the
generated video, especially when the person in the video exhibits significant
expression changes and movements. This issue becomes critical when the human
face occupies merely a small fraction of the image. Since humans are highly
sensitive to identity variations, this poses a critical yet under-explored
challenge in I2V generation. In this paper, we propose Identity-Preserving
Reward-guided Optimization (IPRO), a novel video diffusion framework based on
reinforcement learning to enhance identity preservation. Instead of introducing
auxiliary modules or altering model architectures, our approach introduces a
direct and effective tuning algorithm that optimizes diffusion models using a
face identity scorer. To improve performance and accelerate convergence, our
method backpropagates the reward signal through the last steps of the sampling
chain, enabling richer gradient feedback. We also propose a novel facial
scoring mechanism that treats faces in ground-truth videos as facial feature
pools, providing multi-angle facial information to enhance generalization. A
KL-divergence regularization is further incorporated to stabilize training and
prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V
model and our in-house I2V model demonstrate the effectiveness of our method.
Our project and code are available at
\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.

</details>


### [75] [Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning](https://arxiv.org/abs/2510.14256)
*Xiangyu Meng,Zixian Zhang,Zhenghao Zhang,Junchao Liao,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: 提出Identity-GRPO，一个基于人类反馈的优化流程，旨在解决多人物视频生成中身份一致性难以保持的问题，并通过定制的GRPO变体显著提升现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法（如VACE和Phantom）在多人物动态交互场景中，难以保持跨多个角色的身份一致性，而这在许多应用中至关重要。

Method: 首先，构建了一个视频奖励模型，该模型在一个大规模偏好数据集上训练，数据包含人工标注和合成失真，并专注于人物一致性的成对标注。接着，采用了一种针对多人物一致性定制的GRPO变体来优化生成策略。通过消融研究评估标注质量和设计选择的影响。

Result: Identity-GRPO在人物一致性指标上比基线方法提升高达18.9%，并显著增强了VACE和Phantom的性能。

Conclusion: 该研究为将强化学习与个性化视频生成对齐提供了可行的见解。

Abstract: While advanced methods like VACE and Phantom have advanced video generation
for specific subjects in diverse scenarios, they struggle with multi-human
identity preservation in dynamic interactions, where consistent identities
across multiple characters are critical. To address this, we propose
Identity-GRPO, a human feedback-driven optimization pipeline for refining
multi-human identity-preserving video generation. First, we construct a video
reward model trained on a large-scale preference dataset containing
human-annotated and synthetic distortion data, with pairwise annotations
focused on maintaining human consistency throughout the video. We then employ a
GRPO variant tailored for multi-human consistency, which greatly enhances both
VACE and Phantom. Through extensive ablation studies, we evaluate the impact of
annotation quality and design choices on policy optimization. Experiments show
that Identity-GRPO achieves up to 18.9% improvement in human consistency
metrics over baseline methods, offering actionable insights for aligning
reinforcement learning with personalized video generation.

</details>


### [76] [MatchAttention: Matching the Relative Positions for High-Resolution Cross-View Matching](https://arxiv.org/abs/2510.14260)
*Tingman Yan,Tao Liu,Xilian Yang,Qunfei Zhao,Zeyang Xia*

Main category: cs.CV

TL;DR: 提出MatchAttention机制，通过动态匹配相对位置解决高分辨率图像跨视角匹配中的二次复杂度和缺乏显式约束问题，实现了高效、高精度且低内存消耗的跨视角匹配。


<details>
  <summary>Details</summary>
Motivation: 现有跨注意力机制在处理高分辨率图像时面临二次复杂度和缺乏显式匹配约束的挑战，导致高分辨率跨视角匹配难以实现。

Method: 提出MatchAttention机制，通过动态匹配相对位置来确定注意力采样中心，并引入BilinearSoftmax实现连续可微的滑动窗口注意力采样。相对位置通过残差连接跨层迭代更新。设计了MatchDecoder作为核心组件，并针对跨视角遮挡问题提出门控跨MatchAttention和一致性约束损失，以缓解遮挡影响。

Result: MatchStereo-B在Middlebury基准测试中平均误差排名第一，KITTI分辨率推理仅需29ms。MatchStereo-T能在0.1秒内处理4K UHD图像，仅消耗3GB GPU内存。模型在KITTI 2012、KITTI 2015、ETH3D和Spring flow数据集上均取得最先进性能。

Conclusion: 所提出的模型结合了高精度和低计算复杂度，使实时、高分辨率和高精度的跨视角匹配成为可能。

Abstract: Cross-view matching is fundamentally achieved through cross-attention
mechanisms. However, matching of high-resolution images remains challenging due
to the quadratic complexity and lack of explicit matching constraints in the
existing cross-attention. This paper proposes an attention mechanism,
MatchAttention, that dynamically matches relative positions. The relative
position determines the attention sampling center of the key-value pairs given
a query. Continuous and differentiable sliding-window attention sampling is
achieved by the proposed BilinearSoftmax. The relative positions are
iteratively updated through residual connections across layers by embedding
them into the feature channels. Since the relative position is exactly the
learning target for cross-view matching, an efficient hierarchical cross-view
decoder, MatchDecoder, is designed with MatchAttention as its core component.
To handle cross-view occlusions, gated cross-MatchAttention and a
consistency-constrained loss are proposed. These two components collectively
mitigate the impact of occlusions in both forward and backward passes, allowing
the model to focus more on learning matching relationships. When applied to
stereo matching, MatchStereo-B ranked 1st in average error on the public
Middlebury benchmark and requires only 29ms for KITTI-resolution inference.
MatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU
memory. The proposed models also achieve state-of-the-art performance on KITTI
2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high
accuracy and low computational complexity makes real-time, high-resolution, and
high-accuracy cross-view matching possible. Code is available at
https://github.com/TingmanYan/MatchAttention.

</details>


### [77] [Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment](https://arxiv.org/abs/2510.14266)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: 提出一种基于事件相机光通信的鲁棒解调方案，在室外远距离实验中首次实现了创纪录的低误码率。


<details>
  <summary>Details</summary>
Motivation: 开发一种鲁棒解调方案，以提高基于事件视觉传感器的光通信系统在室外远距离环境中的性能和可靠性。

Method: 结合开关键控（OOK）与翻转解调（toggle demodulation）和数字锁相环（DPLL），提出了一种鲁棒解调方案，应用于基于事件视觉传感器的光通信系统。

Result: 在室外实验中，首次实现了在200米-60kbps和400米-30kbps条件下，误码率（BER）低于10^-3。

Conclusion: 所提出的方案能有效实现事件相机光通信的鲁棒解调，并在室外远距离传输中达到了卓越的性能。

Abstract: We propose a robust demodulation scheme for optical camera communication
systems using an event-based vision sensor, combining OOK with toggle
demodulation and a digital phase-locked loop. This is the first report to
achieve a $\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor
experiments.

</details>


### [78] [GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering](https://arxiv.org/abs/2510.14270)
*Alexander Valverde,Brian Xu,Yuyin Zhou,Meng Xu,Hongyun Wang*

Main category: cs.CV

TL;DR: 本文提出GauSSmart，一种结合2D基础模型与3D Gaussian Splatting的混合方法，旨在解决现有Gaussian Splatting在稀疏区域细节不足的问题，通过2D先验和特征监督提升重建质量，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: Gaussian Splatting在处理大规模数据集时表现出色，但在稀疏覆盖区域难以捕捉精细细节或保持真实感，这主要源于稀疏3D训练数据的固有局限性。

Method: 提出GauSSmart，一种将2D基础模型（如DINO）与3D Gaussian Splatting重建相结合的混合方法。该方法整合了2D计算机视觉技术，包括凸过滤和语义特征监督，并利用2D分割先验和高维特征嵌入来指导高斯点的密集化和细化，以改善欠表达区域的覆盖并保留复杂的结构细节。

Result: GauSSmart在三个数据集上进行了验证，结果显示其在大多数评估场景中持续优于现有的Gaussian Splatting方法。

Conclusion: 混合2D-3D方法具有显著潜力，表明将2D基础模型与3D重建流程巧妙结合能够有效克服单一方法固有的局限性。

Abstract: Scene reconstruction has emerged as a central challenge in computer vision,
with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting
achieving remarkable progress. While Gaussian Splatting demonstrates strong
performance on large-scale datasets, it often struggles to capture fine details
or maintain realism in regions with sparse coverage, largely due to the
inherent limitations of sparse 3D training data.
  In this work, we propose GauSSmart, a hybrid method that effectively bridges
2D foundational models and 3D Gaussian Splatting reconstruction. Our approach
integrates established 2D computer vision techniques, including convex
filtering and semantic feature supervision from foundational models such as
DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D
segmentation priors and high-dimensional feature embeddings, our method guides
the densification and refinement of Gaussian splats, improving coverage in
underrepresented areas and preserving intricate structural details.
  We validate our approach across three datasets, where GauSSmart consistently
outperforms existing Gaussian Splatting in the majority of evaluated scenes.
Our results demonstrate the significant potential of hybrid 2D-3D approaches,
highlighting how the thoughtful combination of 2D foundational models with 3D
reconstruction pipelines can overcome the limitations inherent in either
approach alone.

</details>


### [79] [CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts](https://arxiv.org/abs/2510.14273)
*Kieu-Anh Truong Thi,Huy-Hieu Pham,Duc-Trong Le*

Main category: cs.CV

TL;DR: 本文提出一种基于因果推断的框架，通过应用“前门”原则并利用语义特征，有效应对组织病理学图像分析中的域偏移问题。在多个数据集上，该方法比现有基线模型性能提升高达7%。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在组织病理学中常受域偏移（由采集过程或数据源差异引起）影响，导致泛化能力下降。现有方法主要依赖统计相关性建模，但往往忽视了因果关系。

Method: 提出一种新颖的基于因果推断的框架，利用语义特征并减轻混淆变量的影响。该方法通过设计明确包含中介变量和观察到的组织切片的转换策略，实现了“前门”原则（front-door principle）。

Result: 在CAMELYON17数据集和私有组织病理学数据集上进行了验证，结果显示在未见域上持续获得性能提升。与现有基线相比，性能提升最高达到7%。

Conclusion: 研究结果表明，因果推断是解决组织病理学图像分析中域偏移问题的有力工具。

Abstract: Domain shift in histopathology, often caused by differences in acquisition
processes or data sources, poses a major challenge to the generalization
ability of deep learning models. Existing methods primarily rely on modeling
statistical correlations by aligning feature distributions or introducing
statistical variation, yet they often overlook causal relationships. In this
work, we propose a novel causal-inference-based framework that leverages
semantic features while mitigating the impact of confounders. Our method
implements the front-door principle by designing transformation strategies that
explicitly incorporate mediators and observed tissue slides. We validate our
method on the CAMELYON17 dataset and a private histopathology dataset,
demonstrating consistent performance gains across unseen domains. As a result,
our approach achieved up to a 7% improvement in both the CAMELYON17 dataset and
the private histopathology dataset, outperforming existing baselines. These
results highlight the potential of causal inference as a powerful tool for
addressing domain shift in histopathology image analysis.

</details>


### [80] [Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding](https://arxiv.org/abs/2510.14304)
*Kyungryul Back,Seongbeom Park,Milim Kim,Mincheol Kwon,SangHyeok Lee,Hyunyoung Lee,Junhee Cho,Seunghyun Park,Jinkyu Kim*

Main category: cs.CV

TL;DR: 本文提出一种免训练的三层对比解码与水印技术，旨在减少大型视觉语言模型（LVLMs）中的幻觉问题，并提高其视觉接地能力。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态任务中表现良好，但普遍存在幻觉问题，即过度依赖单一模态或记忆训练数据，导致输出缺乏正确的视觉接地。

Method: 我们提出一种训练无关的三层对比解码与水印方法。该方法包括：1) 选择解码层中的成熟层和业余层；2) 利用水印相关问题识别枢轴层，以评估其视觉接地性；3) 应用三层对比解码生成最终输出。

Result: 在POPE、MME和AMBER等公共基准测试中，我们的方法在减少LVLM幻觉方面达到了最先进的性能，并生成了更多视觉接地的响应。

Conclusion: 所提出的免训练三层对比解码与水印方法能够有效减少大型视觉语言模型中的幻觉，并显著提升生成响应的视觉接地性。

Abstract: Large Vision-Language Models (LVLMs) have recently shown promising results on
various multimodal tasks, even achieving human-comparable performance in
certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often
rely heavily on a single modality or memorize training data without properly
grounding their outputs. To address this, we propose a training-free, tri-layer
contrastive decoding with watermarking, which proceeds in three steps: (1)
select a mature layer and an amateur layer among the decoding layers, (2)
identify a pivot layer using a watermark-related question to assess whether the
layer is visually well-grounded, and (3) apply tri-layer contrastive decoding
to generate the final output. Experiments on public benchmarks such as POPE,
MME and AMBER demonstrate that our method achieves state-of-the-art performance
in reducing hallucinations in LVLMs and generates more visually grounded
responses.

</details>


### [81] [A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection](https://arxiv.org/abs/2510.14314)
*Shivangi Yadav,Arun Ross*

Main category: cs.CV

TL;DR: 为解决虹膜PAD数据稀缺问题，本文提出MID-StyleGAN框架，结合扩散模型与GAN生成多域合成眼部图像，显著提升PAD系统性能。


<details>
  <summary>Details</summary>
Motivation: 虹膜生物识别系统易受演示攻击（如人造眼、打印图像、美瞳）威胁，但目前缺乏用于训练和评估虹膜演示攻击检测（PAD）方法的数据集。

Method: 提出Multi-domain Image Translative Diffusion StyleGAN (MID-StyleGAN)框架，结合扩散模型和生成对抗网络（GANs）的优势。该模型采用多域架构，能够实现真实眼部图像与不同攻击域（真实、打印眼、美瞳）之间的转换，并使用为眼部数据量身定制的自适应损失函数来保持域一致性。

Result: MID-StyleGAN在生成高质量合成眼部图像方面优于现有方法。使用生成的合成数据显著提升了PAD系统的性能，例如，在LivDet2020数据集上，1%误报率下的真实检测率从93.41%提高到98.72%。

Conclusion: MID-StyleGAN通过生成逼真且多样化的合成数据，有效解决了虹膜和眼部生物识别领域的数据稀缺问题，为提升PAD系统性能提供了一个可扩展的解决方案。

Abstract: An iris biometric system can be compromised by presentation attacks (PAs)
where artifacts such as artificial eyes, printed eye images, or cosmetic
contact lenses are presented to the system. To counteract this, several
presentation attack detection (PAD) methods have been developed. However, there
is a scarcity of datasets for training and evaluating iris PAD techniques due
to the implicit difficulties in constructing and imaging PAs. To address this,
we introduce the Multi-domain Image Translative Diffusion StyleGAN
(MID-StyleGAN), a new framework for generating synthetic ocular images that
captures the PA and bonafide characteristics in multiple domains such as
bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the
strengths of diffusion models and generative adversarial networks (GANs) to
produce realistic and diverse synthetic data. Our approach utilizes a
multi-domain architecture that enables the translation between bonafide ocular
images and different PA domains. The model employs an adaptive loss function
tailored for ocular data to maintain domain consistency. Extensive experiments
demonstrate that MID-StyleGAN outperforms existing methods in generating
high-quality synthetic ocular images. The generated data was used to
significantly enhance the performance of PAD systems, providing a scalable
solution to the data scarcity problem in iris and ocular biometrics. For
example, on the LivDet2020 dataset, the true detect rate at 1% false detect
rate improved from 93.41% to 98.72%, showcasing the impact of the proposed
method.

</details>


### [82] [Vision-Centric Activation and Coordination for Multimodal Large Language Models](https://arxiv.org/abs/2510.14349)
*Yunnan Wang,Fan Lu,Kecheng Zheng,Ziyuan Huang,Ziqiang Li,Wenjun Zeng,Xin Jin*

Main category: cs.CV

TL;DR: 本文提出VaCo框架，通过视觉中心激活和多视觉基础模型（VFM）的协调，优化多模态大语言模型（MLLM）的表示，显著提升其视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 主流MLLM仅通过文本预测进行监督，忽视了对分析能力至关重要的视觉中心信息，导致其视觉理解能力不足。

Method: VaCo引入视觉判别对齐，整合来自VFMs的任务感知感知特征，统一文本和视觉输出的优化。具体而言，它包含可学习的模块化任务查询（MTQs）和视觉对齐层（VALs）来激活视觉信号，并使用令牌门控掩码（TGM）协调不同MTQs组之间的信息流以解决VFM表示冲突。

Result: VaCo显著提升了不同MLLM在各种基准测试上的性能，展现出其卓越的视觉理解能力。

Conclusion: VaCo通过其创新的视觉中心激活和多VFM协调机制，有效增强了MLLM的视觉理解能力。

Abstract: Multimodal large language models (MLLMs) integrate image features from visual
encoders with LLMs, demonstrating advanced comprehension capabilities. However,
mainstream MLLMs are solely supervised by the next-token prediction of textual
tokens, neglecting critical vision-centric information essential for analytical
abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM
representations through Vision-Centric activation and Coordination from
multiple vision foundation models (VFMs). VaCo introduces visual discriminative
alignment to integrate task-aware perceptual features extracted from VFMs,
thereby unifying the optimization of both textual and visual outputs in MLLMs.
Specifically, we incorporate the learnable Modular Task Queries (MTQs) and
Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals
under the supervision of diverse VFMs. To coordinate representation conflicts
across VFMs, the crafted Token Gateway Mask (TGM) restricts the information
flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo
significantly improves the performance of different MLLMs on various
benchmarks, showcasing its superior capabilities in visual comprehension.

</details>


### [83] [Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration](https://arxiv.org/abs/2510.14354)
*Siddharth Tourani,Jayaram Reddy,Sarvesh Thakur,K Madhava Krishna,Muhammad Haris Khan,N Dinesh Reddy*

Main category: cs.CV

TL;DR: 本文提出使用循环一致性关键点和新型姿态块（结合GRU与变换同步），以改进RGB-D数据的自监督配准，在ScanNet和3DMatch数据集上表现优于现有自监督及部分早期监督方法。


<details>
  <summary>Details</summary>
Motivation: 随着消费级深度相机的普及，大量未标注的RGB-D数据涌现，如何有效利用这些数据进行场景几何推理（尤其是配准）成为一个重要问题。

Method: 1. 采用循环一致性关键点作为显著点，在匹配过程中强制执行空间一致性约束以提高对应精度。 2. 引入一种新颖的姿态块，该姿态块结合了GRU循环单元和变换同步机制，以融合历史和多视角数据。

Result: 所提出的方法在ScanNet和3DMatch数据集上超越了先前的自监督配准方法，甚至优于一些较早的监督方法。将所提组件集成到现有方法中也验证了其有效性。

Conclusion: 通过引入循环一致性关键点和新型姿态块，本研究显著提升了自监督RGB-D配准的性能，达到了当前先进水平，证明了其在几何推理领域的潜力。

Abstract: With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has
become available. This prompts the question of how to utilize this data for
geometric reasoning of scenes. While many RGB-D registration meth- ods rely on
geometric and feature-based similarity, we take a different approach. We use
cycle-consistent keypoints as salient points to enforce spatial coherence
constraints during matching, improving correspondence accuracy. Additionally,
we introduce a novel pose block that combines a GRU recurrent unit with
transformation synchronization, blending historical and multi-view data. Our
approach surpasses previous self- supervised registration methods on ScanNet
and 3DMatch, even outperforming some older supervised methods. We also
integrate our components into existing methods, showing their effectiveness.

</details>


### [84] [Spatial Preference Rewarding for MLLMs Spatial Understanding](https://arxiv.org/abs/2510.14374)
*Han Qiu,Peng Gao,Lewei Lu,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLMs）在细粒度空间理解和精确物体定位方面存在不足。本文提出SPR方法，通过奖励MLLMs详细且定位精确的响应，并结合直接偏好优化，有效提升了MLLMs的空间理解能力。


<details>
  <summary>Details</summary>
Motivation: MLLMs虽在空间理解上表现出色，但仍缺乏细粒度的空间感知能力，如生成详细区域描述或精确物体定位，也常无法满足用户对细粒度空间理解的需求。现有方法主要通过预标注数据注入空间知识，缺乏对MLLMs实际响应的直接监督。

Method: 提出空间偏好奖励（SPR）方法，通过对MLLMs生成的图像区域描述引入语义和定位分数进行全面评估。该方法还通过优化MLLM描述的定位准确性，将最佳得分的精炼描述与最低得分的初始描述配对进行直接偏好优化，以增强与视觉输入的细粒度对齐。

Result: 在标准指代和定位基准上的大量实验表明，SPR在训练开销极小的情况下，有效提升了MLLM的空间理解能力。

Conclusion: SPR通过奖励机制和偏好优化，成功解决了MLLMs在细粒度空间理解和精确物体定位上的不足，显著提升了其空间感知能力。

Abstract: Multimodal large language models~(MLLMs) have demonstrated promising spatial
understanding capabilities, such as referencing and grounding object
descriptions. Despite their successes, MLLMs still fall short in fine-grained
spatial perception abilities, such as generating detailed region descriptions
or accurately localizing objects. Additionally, they often fail to respond to
the user's requirements for desired fine-grained spatial understanding. This
issue might arise because existing approaches primarily focus on tuning MLLMs
to model pre-annotated instruction data to inject spatial knowledge, without
direct supervision of MLLMs' actual responses. We address this issue by SPR, a
Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial
capabilities by rewarding MLLMs' detailed responses with precise object
localization over vague or inaccurate responses. With randomly selected image
regions and region descriptions from MLLMs, SPR introduces semantic and
localization scores to comprehensively evaluate the text quality and
localization quality in MLLM-generated descriptions. We also refine the MLLM
descriptions with better localization accuracy and pair the best-scored
refinement with the initial descriptions of the lowest score for direct
preference optimization, thereby enhancing fine-grained alignment with visual
input. Extensive experiments over standard referring and grounding benchmarks
show that SPR improves MLLM spatial understanding capabilities effectively with
minimal overhead in training. Data and code will be released at
https://github.com/hanqiu-hq/SPR

</details>


### [85] [DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation](https://arxiv.org/abs/2510.14376)
*Dongnam Byun,Jungwon Park,Jumgmin Ko,Changin Choi,Wonjong Rhee*

Main category: cs.CV

TL;DR: 文本到图像（T2I）模型在多对象提示词上表现不佳，常出现对象忽略或混合。本文提出DOS方法，通过修改CLIP文本嵌入来提升多对象图像生成成功率并减少对象混合，实验和人工评估均显示其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 文本到图像（T2I）生成模型在处理多对象提示时，常出现对象忽略或混淆。研究识别出相似形状、相似纹理、背景偏差和多对象等场景是主要失败原因。

Method: 提出DOS（Directional Object Separation）方法，该方法基于对CLIP嵌入的观察，在将CLIP文本嵌入输入到T2I模型之前，对其三种类型进行修改。

Result: DOS显著提高了多对象图像生成的成功率并减少了对象混合。在人工评估中，DOS在四个基准测试中比四种竞争方法多获得26.24%-43.04%的票数。

Conclusion: DOS是一种实用且有效的解决方案，能显著改进多对象图像生成。

Abstract: Recent progress in text-to-image (T2I) generative models has led to
significant improvements in generating high-quality images aligned with text
prompts. However, these models still struggle with prompts involving multiple
objects, often resulting in object neglect or object mixing. Through extensive
studies, we identify four problematic scenarios, Similar Shapes, Similar
Textures, Dissimilar Background Biases, and Many Objects, where inter-object
relationships frequently lead to such failures. Motivated by two key
observations about CLIP embeddings, we propose DOS (Directional Object
Separation), a method that modifies three types of CLIP text embeddings before
passing them into text-to-image models. Experimental results show that DOS
consistently improves the success rate of multi-object image generation and
reduces object mixing. In human evaluations, DOS significantly outperforms four
competing methods, receiving 26.24%-43.04% more votes across four benchmarks.
These results highlight DOS as a practical and effective solution for improving
multi-object image generation.

</details>


### [86] [DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights](https://arxiv.org/abs/2510.14383)
*Danish Ali,Ajmal Mian,Naveed Akhtar,Ghulam Mubashar Hassan*

Main category: cs.CV

TL;DR: 本文提出DRBD-Mamba模型，通过引入空间填充曲线、门控融合模块和量化块，实现了高效、高精度的脑肿瘤3D分割，显著提升了效率和分割核心及增强肿瘤的准确性。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分割对临床诊断和治疗至关重要，但肿瘤亚区域的异质性使其极具挑战。现有基于Mamba的模型计算开销大，且在不同BraTS数据集分区下的鲁棒性未充分探索。

Method: 提出DRBD-Mamba模型，这是一个高效的3D分割模型。它利用空间填充曲线将3D特征映射到1D以保留空间局部性，减少计算负担。为丰富特征表示，引入门控融合模块自适应整合正向和反向上下文，并使用量化块离散化特征以提高鲁棒性。此外，还提出了BraTS2023上的五种系统折叠验证方法用于严谨评估。

Result: 在20%测试集上，模型在整体肿瘤、肿瘤核心和增强肿瘤Dice得分上分别提升0.10%、1.75%和0.93%。在提出的五折交叉验证下，肿瘤核心和增强肿瘤的平均Dice得分相比现有SOTA分别提升0.86%和1.45%，同时模型效率提升15倍。

Conclusion: DRBD-Mamba模型在保持高分割精度的同时，显著提升了脑肿瘤分割的计算效率和鲁棒性，特别是在肿瘤核心和增强肿瘤分割方面优于现有方法，展现出显著的计算优势。

Abstract: Accurate brain tumor segmentation is significant for clinical diagnosis and
treatment. It is challenging due to the heterogeneity of tumor subregions.
Mamba-based State Space Models have demonstrated promising performance.
However, they incur significant computational overhead due to sequential
feature computation across multiple spatial axes. Moreover, their robustness
across diverse BraTS data partitions remains largely unexplored, leaving a
critical gap in reliable evaluation. To address these limitations, we propose
dual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation
model that captures multi-scale long-range dependencies with minimal
computational overhead. We leverage a space-filling curve to preserve spatial
locality during 3D-to-1D feature mapping, thereby reducing reliance on
computationally expensive multi-axial feature scans. To enrich feature
representation, we propose a gated fusion module that adaptively integrates
forward and reverse contexts, along with a quantization block that discretizes
features to improve robustness. In addition, we propose five systematic folds
on BraTS2023 for rigorous evaluation of segmentation techniques under diverse
conditions and present detailed analysis of common failure scenarios. On the
20\% test set used by recent methods, our model achieves Dice improvements of
0.10\% for whole tumor, 1.75\% for tumor core, and 0.93\% for enhancing tumor.
Evaluations on the proposed systematic five folds demonstrate that our model
maintains competitive whole tumor accuracy while achieving clear average Dice
gains of 0.86\% for tumor core and 1.45\% for enhancing tumor over existing
state-of-the-art. Furthermore, our model attains 15 times improvement in
efficiency while maintaining high segmentation accuracy, highlighting its
robustness and computational advantage over existing approaches.

</details>


### [87] [BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble](https://arxiv.org/abs/2510.14389)
*Brandon Hill,Kma Solaiman*

Main category: cs.CV

TL;DR: 开发BoardVision框架，通过基准测试YOLOv7和Faster R-CNN并提出集成方法CTV Voter，实现主板组装级缺陷检测，并发布实用工具。


<details>
  <summary>Details</summary>
Motivation: 现有PCB检测主要关注裸板或走线级缺陷，而对完整主板的组装级缺陷（如螺丝缺失、风扇接线松动、表面划痕）的检测研究不足。

Method: 提出可复现框架BoardVision用于检测主板组装级缺陷；在MiracleFactory数据集上基准测试YOLOv7和Faster R-CNN；为平衡单一模型精度与召回，提出轻量级集成方法Confidence-Temporal Voting (CTV Voter)；评估模型在亮度、锐度、方向变化等真实扰动下的鲁棒性；发布GUI驱动检测工具。

Result: BoardVision框架成功应用于主板组装级缺陷检测；CTV Voter集成方法有效平衡了单一模型的精度和召回率；揭示了现实扰动下检测的稳定性挑战；开发出实用的GUI检测工具。

Conclusion: 本工作证明计算机视觉技术能有效应用于主板组装级制造的实际质量保证，实现从基准测试到实际应用的过渡。

Abstract: Motherboard defect detection is critical for ensuring reliability in
high-volume electronics manufacturing. While prior research in PCB inspection
has largely targeted bare-board or trace-level defects, assembly-level
inspection of full motherboards inspection remains underexplored. In this work,
we present BoardVision, a reproducible framework for detecting assembly-level
defects such as missing screws, loose fan wiring, and surface scratches. We
benchmark two representative detectors - YOLOv7 and Faster R-CNN, under
controlled conditions on the MiracleFactory motherboard dataset, providing the
first systematic comparison in this domain. To mitigate the limitations of
single models, where YOLO excels in precision but underperforms in recall and
Faster R-CNN shows the reverse, we propose a lightweight ensemble,
Confidence-Temporal Voting (CTV Voter), that balances precision and recall
through interpretable rules. We further evaluate robustness under realistic
perturbations including sharpness, brightness, and orientation changes,
highlighting stability challenges often overlooked in motherboard defect
detection. Finally, we release a deployable GUI-driven inspection tool that
bridges research evaluation with operator usability. Together, these
contributions demonstrate how computer vision techniques can transition from
benchmark results to practical quality assurance for assembly-level motherboard
manufacturing.

</details>


### [88] [DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis](https://arxiv.org/abs/2510.14403)
*Chao Tu,Kun Huang,Jie Zhang,Qianjin Feng,Yu Zhang,Zhenyuan Ning*

Main category: cs.CV

TL;DR: 本文提出DCMIL模型，通过双课程对比多实例学习，有效处理巨像素病理图像（WSI）进行癌症预后，克服了计算瓶颈和标注稀缺问题，性能优于现有模型并能提供新的生物学见解。


<details>
  <summary>Details</summary>
Motivation: 计算病理学在利用WSI量化形态异质性和开发癌症客观预后模型方面潜力巨大。然而，巨像素WSI的计算瓶颈和密集手动标注的稀缺阻碍了其发展。现有方法常忽视多放大倍数WSI中的细粒度信息和肿瘤微环境的变异性。

Method: 本文提出了一种“从易到难”的渐进式表征学习模型，命名为双课程对比多实例学习（DCMIL）。该模型不依赖密集标注，能够将巨像素WSI直接转化为预后预测。

Result: 在12种癌症类型（5,954名患者，1254万个切片）上的广泛实验表明，DCMIL的性能优于标准的WSI基预后模型。此外，DCMIL能够识别细粒度的预后显著区域，提供鲁棒的实例不确定性估计，并捕获正常与肿瘤组织之间的形态学差异。

Conclusion: DCMIL模型有效解决了WSI癌症预后中的计算瓶颈和标注稀缺问题，显著提高了预后性能。同时，它还能通过识别关键区域和捕获形态差异，为生成新的生物学见解提供潜力。

Abstract: The burgeoning discipline of computational pathology shows promise in
harnessing whole slide images (WSIs) to quantify morphological heterogeneity
and develop objective prognostic modes for human cancers. However, progress is
impeded by the computational bottleneck of gigapixel-size inputs and the
scarcity of dense manual annotations. Current methods often overlook
fine-grained information across multi-magnification WSIs and variations in
tumor microenvironments. Here, we propose an easy-to-hard progressive
representation learning model, termed dual-curriculum contrastive
multi-instance learning (DCMIL), to efficiently process WSIs for cancer
prognosis. The model does not rely on dense annotations and enables the direct
transformation of gigapixel-size WSIs into outcome predictions. Extensive
experiments on twelve cancer types (5,954 patients, 12.54 million tiles)
demonstrate that DCMIL outperforms standard WSI-based prognostic models.
Additionally, DCMIL identifies fine-grained prognosis-salient regions, provides
robust instance uncertainty estimation, and captures morphological differences
between normal and tumor tissues, with the potential to generate new biological
insights. All codes have been made publicly accessible at
https://github.com/tuuuc/DCMIL.

</details>


### [89] [Real-Time Neural Video Compression with Unified Intra and Inter Coding](https://arxiv.org/abs/2510.14431)
*Hui Xiang,Yifan Bian,Li Li,Jingran Wu,Xianguo Zhang,Dong Liu*

Main category: cs.CV

TL;DR: 本文提出了一种统一帧内/帧间编码和双帧压缩的神经视频编码（NVC）框架，解决了现有NVC方案在处理遮挡解除、新内容和帧间误差传播等方面的局限性，实现了比DCVC-RT高10.7%的BD-rate降低，并保持实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经视频编码（NVC）方案（如DCVC-RT）在处理遮挡解除、新内容、帧间误差传播和积累等方面存在局限性。

Method: 借鉴经典视频编码思想，允许帧间编码帧内进行帧内编码。提出一个统一的帧内和帧间编码NVC框架，其中每帧由一个单一模型自适应地执行帧内/帧间编码。此外，提出一种同时两帧压缩设计，以向前和向后利用帧间冗余。

Result: 该方案比DCVC-RT平均降低10.7%的BD-rate，每帧提供更稳定的码率和质量，并保持实时编码/解码性能。

Conclusion: 通过统一的帧内/帧间编码和双帧压缩设计，本研究有效解决了现有NVC方案的局限性，显著提升了压缩效率和稳定性，同时保留了实时性能。

Abstract: Neural video compression (NVC) technologies have advanced rapidly in recent
years, yielding state-of-the-art schemes such as DCVC-RT that offer superior
compression efficiency to H.266/VVC and real-time encoding/decoding
capabilities. Nonetheless, existing NVC schemes have several limitations,
including inefficiency in dealing with disocclusion and new content, interframe
error propagation and accumulation, among others. To eliminate these
limitations, we borrow the idea from classic video coding schemes, which allow
intra coding within inter-coded frames. With the intra coding tool enabled,
disocclusion and new content are properly handled, and interframe error
propagation is naturally intercepted without the need for manual refresh
mechanisms. We present an NVC framework with unified intra and inter coding,
where every frame is processed by a single model that is trained to perform
intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame
compression design to exploit interframe redundancy not only forwardly but also
backwardly. Experimental results show that our scheme outperforms DCVC-RT by an
average of 10.7\% BD-rate reduction, delivers more stable bitrate and quality
per frame, and retains real-time encoding/decoding performances. Code and
models will be released.

</details>


### [90] [Structured Universal Adversarial Attacks on Object Detection for Video Sequences](https://arxiv.org/abs/2510.14460)
*Sven Jacob,Weijia Shao,Gjergji Kasneci*

Main category: cs.CV

TL;DR: 本文提出一种针对视频目标检测的、失真最小的通用对抗性攻击，通过核范数正则化和自适应优化方法，在有效性和隐蔽性上优于现有攻击。


<details>
  <summary>Details</summary>
Motivation: 深度学习视频目标检测器在安全关键应用中扮演重要角色，但它们容易受到通用对抗性攻击的威胁。

Method: 提出一种利用核范数正则化促进背景结构化扰动的通用对抗性攻击；采用自适应、乐观的指数梯度法进行高效优化，提升可扩展性和收敛性。

Result: 所提出的攻击在有效性上优于低秩投影梯度下降和Frank-Wolfe等基于的攻击，同时保持了高隐蔽性。

Conclusion: 本研究成功开发了一种新型通用对抗性攻击，能有效且隐蔽地攻击视频目标检测系统，性能优于现有方法。

Abstract: Video-based object detection plays a vital role in safety-critical
applications. While deep learning-based object detectors have achieved
impressive performance, they remain vulnerable to adversarial attacks,
particularly those involving universal perturbations. In this work, we propose
a minimally distorted universal adversarial attack tailored for video object
detection, which leverages nuclear norm regularization to promote structured
perturbations concentrated in the background. To optimize this formulation
efficiently, we employ an adaptive, optimistic exponentiated gradient method
that enhances both scalability and convergence. Our results demonstrate that
the proposed attack outperforms both low-rank projected gradient descent and
Frank-Wolfe based attacks in effectiveness while maintaining high stealthiness.
All code and data are publicly available at
https://github.com/jsve96/AO-Exp-Attack.

</details>


### [91] [Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review](https://arxiv.org/abs/2510.14462)
*Youwan Mahé,Elise Bannier,Stéphanie Leplaideur,Elisa Fromont,Francesca Galassi*

Main category: cs.CV

TL;DR: 一项综述显示，无监督深度生成模型在脑影像异常检测中表现出巨大潜力，克服了监督方法的局限性。这些模型在多种病变中性能良好，且能生成可解释的“伪健康”重建图像，未来需加强解剖学感知建模和临床验证以实现应用。


<details>
  <summary>Details</summary>
Motivation: 传统的监督式异常检测方法需要大量的像素级标注数据且仅限于已知病理，这在脑影像分析中存在局限性。无监督深度生成模型则能通过仅在健康数据上训练来识别异常，从而提供了一种有前景的替代方案。

Method: 本文采用PRISMA指南的范围综述方法，综合了2018年至2025年间发表的49项关于无监督深度生成模型（包括自编码器、变分自编码器、生成对抗网络和去噪扩散模型）在神经影像异常检测中应用的研究。这些研究涵盖了脑部MRI和CT在肿瘤、中风、多发性硬化症等多种病理上的应用，并对报告的性能指标和架构设计进行了比较。

Result: 研究发现，生成模型在大型局灶性病变中取得了令人鼓舞的性能，并在处理更细微的异常方面取得了进展。生成模型的一个关键优势在于其能生成可解释的“伪健康”重建图像，这在标注数据稀缺（如罕见或异质性疾病）的情况下尤为有价值。

Conclusion: 无监督深度生成模型为异常检测提供了一个引人注目的方向，有助于实现半监督学习、发现新型影像生物标志物，并在统一框架内促进疾病内部和跨疾病的偏差映射。为实现临床应用，未来的工作应优先考虑解剖学感知建模、基础模型的开发、任务适用的评估指标以及严格的临床验证。

Abstract: Unsupervised deep generative models are emerging as a promising alternative
to supervised methods for detecting and segmenting anomalies in brain imaging.
Unlike fully supervised approaches, which require large voxel-level annotated
datasets and are limited to well-characterised pathologies, these models can be
trained exclusively on healthy data and identify anomalies as deviations from
learned normative brain structures. This PRISMA-guided scoping review
synthesises recent work on unsupervised deep generative models for anomaly
detection in neuroimaging, including autoencoders, variational autoencoders,
generative adversarial networks, and denoising diffusion models. A total of 49
studies published between 2018 - 2025 were identified, covering applications to
brain MRI and, less frequently, CT across diverse pathologies such as tumours,
stroke, multiple sclerosis, and small vessel disease. Reported performance
metrics are compared alongside architectural design choices. Across the
included studies, generative models achieved encouraging performance for large
focal lesions and demonstrated progress in addressing more subtle
abnormalities. A key strength of generative models is their ability to produce
interpretable pseudo-healthy (also referred to as counterfactual)
reconstructions, which is particularly valuable when annotated data are scarce,
as in rare or heterogeneous diseases. Looking ahead, these models offer a
compelling direction for anomaly detection, enabling semi-supervised learning,
supporting the discovery of novel imaging biomarkers, and facilitating within-
and cross-disease deviation mapping in unified end-to-end frameworks. To
realise clinical impact, future work should prioritise anatomy-aware modelling,
development of foundation models, task-appropriate evaluation metrics, and
rigorous clinical validation.

</details>


### [92] [Pruning Overparameterized Multi-Task Networks for Degraded Web Image Restoration](https://arxiv.org/abs/2510.14463)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.CV

TL;DR: 本文提出MIR-L模型，通过迭代剪枝策略将多任务图像恢复模型的参数量减少90%，同时在去雨、去雾和去噪任务上保持甚至超越SOTA性能，解决了计算效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 图像质量对网络平台的用户体验至关重要，但常因在线社交网络的有损操作而下降。尽管多任务图像恢复模型能有效处理多种图像降级，但其过多的可训练参数导致计算效率低下。

Method: 提出名为MIR-L的模型，该模型采用迭代剪枝策略。它在多轮中移除低幅值权重，并将剩余权重重置回其初始值，旨在发现能够匹配甚至超越其密集对应模型性能的高度稀疏子网络（即“winning tickets”）。

Result: 实验结果表明，在去雨、去雾和去噪等基准数据集上，MIR-L模型仅保留了10%的可训练参数，却依然能维持高图像恢复性能，与原始密集模型相当甚至更优。

Conclusion: 通过迭代剪枝策略，MIR-L模型成功地大幅压缩了多任务图像恢复模型，实现了90%的参数削减，同时保持了顶尖的图像恢复性能，有效提升了计算效率。

Abstract: Image quality is a critical factor in delivering visually appealing content
on web platforms. However, images often suffer from degradation due to lossy
operations applied by online social networks (OSNs), negatively affecting user
experience. Image restoration is the process of recovering a clean high-quality
image from a given degraded input. Recently, multi-task (all-in-one) image
restoration models have gained significant attention, due to their ability to
simultaneously handle different types of image degradations. However, these
models often come with an excessively high number of trainable parameters,
making them computationally inefficient. In this paper, we propose a strategy
for compressing multi-task image restoration models. We aim to discover highly
sparse subnetworks within overparameterized deep models that can match or even
surpass the performance of their dense counterparts. The proposed model, namely
MIR-L, utilizes an iterative pruning strategy that removes low-magnitude
weights across multiple rounds, while resetting the remaining weights to their
original initialization. This iterative process is important for the multi-task
image restoration model's optimization, effectively uncovering "winning
tickets" that maintain or exceed state-of-the-art performance at high sparsity
levels. Experimental evaluation on benchmark datasets for the deraining,
dehazing, and denoising tasks shows that MIR-L retains only 10% of the
trainable parameters while maintaining high image restoration performance. Our
code, datasets and pre-trained models are made publicly available at
https://github.com/Thomkat/MIR-L.

</details>


### [93] [Grazing Detection using Deep Learning and Sentinel-2 Time Series Data](https://arxiv.org/abs/2510.14493)
*Aleksis Pirinen,Delia Fano Yela,Smita Chakraborty,Erik Källman*

Main category: cs.CV

TL;DR: 使用Sentinel-2卫星图像和CNN-LSTM模型检测季节性放牧，显著提高了土地利用合规性检查的效率。


<details>
  <summary>Details</summary>
Motivation: 放牧对农业生产和生物多样性影响巨大，但目前缺乏可扩展的放牧监测方法。

Method: 利用Sentinel-2 L2A时序数据（4月-10月），对多边形定义的田块边界进行放牧/非放牧的二分类预测。训练了CNN-LSTM模型集成，使用多时相反射特征。

Result: 模型在五个验证集上平均F1分数达77%，对放牧牧场的召回率达90%。在检查资源有限的情况下，优先检查模型预测的非放牧地块，可比随机检查多发现17.2倍的已确认非放牧地块。

Conclusion: 粗分辨率且免费的卫星数据能可靠地指导检查资源，以实现符合环境保护的土地利用合规性。

Abstract: Grazing shapes both agricultural production and biodiversity, yet scalable
monitoring of where grazing occurs remains limited. We study seasonal grazing
detection from Sentinel-2 L2A time series: for each polygon-defined field
boundary, April-October imagery is used for binary prediction (grazed / not
grazed). We train an ensemble of CNN-LSTM models on multi-temporal reflectance
features, and achieve an average F1 score of 77 percent across five validation
splits, with 90 percent recall on grazed pastures. Operationally, if inspectors
can visit at most 4 percent of sites annually, prioritising fields predicted by
our model as non-grazed yields 17.2 times more confirmed non-grazing sites than
random inspection. These results indicate that coarse-resolution, freely
available satellite data can reliably steer inspection resources for
conservation-aligned land-use compliance. Code and models have been made
publicly available.

</details>


### [94] [Vision Mamba for Permeability Prediction of Porous Media](https://arxiv.org/abs/2510.14516)
*Ali Kashefi,Tapan Mukerji*

Main category: cs.CV

TL;DR: 首次将Vision Mamba应用于三维多孔介质渗透率预测，相比ViT和CNN，其计算与内存效率更高。


<details>
  <summary>Details</summary>
Motivation: 鉴于Vision Mamba在处理高分辨率图像时具有线性的计算和内存扩展性，且参数量少于传统CNN，本研究首次将其引入三维多孔介质渗透率预测任务，以期利用其效率优势。

Method: 开发了以Vision Mamba为骨干网络的深度学习模型，用于预测三维多孔介质的渗透率。通过与ViT和CNN模型进行性能对比，并进行了消融研究以评估其组件对准确性的影响。

Result: 在三维多孔介质渗透率预测任务中，Vision Mamba实际展现出优于ViT和CNN的计算和内存效率优势。

Conclusion: 本研究验证了Vision Mamba在特定任务中的效率优势，并认为其提出的框架有潜力集成到大型视觉模型中，以Vision Mamba替代ViTs。源代码已公开以促进复现和进一步研究。

Abstract: Vision Mamba has recently received attention as an alternative to Vision
Transformers (ViTs) for image classification. The network size of Vision Mamba
scales linearly with input image resolution, whereas ViTs scale quadratically,
a feature that improves computational and memory efficiency. Moreover, Vision
Mamba requires a significantly smaller number of trainable parameters than
traditional convolutional neural networks (CNNs), and thus, they can be more
memory efficient. Because of these features, we introduce, for the first time,
a neural network that uses Vision Mamba as its backbone for predicting the
permeability of three-dimensional porous media. We compare the performance of
Vision Mamba with ViT and CNN models across multiple aspects of permeability
prediction and perform an ablation study to assess the effects of its
components on accuracy. We demonstrate in practice the aforementioned
advantages of Vision Mamba over ViTs and CNNs in the permeability prediction of
three-dimensional porous media. We make the source code publicly available to
facilitate reproducibility and to enable other researchers to build on and
extend this work. We believe the proposed framework has the potential to be
integrated into large vision models in which Vision Mamba is used instead of
ViTs.

</details>


### [95] [Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing](https://arxiv.org/abs/2510.14525)
*Qurrat Ul Ain,Atif Aftab Ahmed Jilani,Zunaira Shafqat,Nigar Azhar Butt*

Main category: cs.CV

TL;DR: 本研究引入了AI驱动的SurgScan框架，利用YOLOv8实现手术器械缺陷的实时、高精度自动检测，显著优于人工检测并满足工业部署需求。


<details>
  <summary>Details</summary>
Motivation: 传统手术器械质量控制依赖人工检测，易受人为错误和不一致性影响，存在严重的安全风险，增加手术并发症几率。

Method: 引入AI驱动的SurgScan框架，使用YOLOv8模型对11种器械和5种缺陷类型，在包含102,876张高分辨率图像的数据集上进行训练，并结合对比度增强预处理。

Result: SurgScan在实时推理速度（4.2-5.8毫秒/图像）下，实现了99.3%的最高缺陷检测精度，优于现有最先进的CNN架构。统计分析表明，对比度增强预处理显著提升了缺陷检测性能。

Conclusion: SurgScan提供了一个可扩展、经济高效的自动化质量控制AI解决方案，降低了对手工检测的依赖，确保符合ISO 13485和FDA标准，有望改进医疗制造中的缺陷检测。

Abstract: Defective surgical instruments pose serious risks to sterility, mechanical
integrity, and patient safety, increasing the likelihood of surgical
complications. However, quality control in surgical instrument manufacturing
often relies on manual inspection, which is prone to human error and
inconsistency. This study introduces SurgScan, an AI-powered defect detection
framework for surgical instruments. Using YOLOv8, SurgScan classifies defects
in real-time, ensuring high accuracy and industrial scalability. The model is
trained on a high-resolution dataset of 102,876 images, covering 11 instrument
types and five major defect categories. Extensive evaluation against
state-of-the-art CNN architectures confirms that SurgScan achieves the highest
accuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image,
making it suitable for industrial deployment. Statistical analysis demonstrates
that contrast-enhanced preprocessing significantly improves defect detection,
addressing key limitations in visual inspection. SurgScan provides a scalable,
cost-effective AI solution for automated quality control, reducing reliance on
manual inspection while ensuring compliance with ISO 13485 and FDA standards,
paving the way for enhanced defect detection in medical manufacturing.

</details>


### [96] [Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models](https://arxiv.org/abs/2510.14526)
*Yunze Tong,Didi Zhu,Zijing Hu,Jinluan Yang,Ziyu Zhao*

Main category: cs.CV

TL;DR: 本文针对Stable Diffusion中初始噪声导致的文本-图像对齐问题，提出一种噪声投影器，通过文本条件细化使初始噪声更具提示感知性，以弥补训练-推理不匹配，从而提高生成图像的对齐度。


<details>
  <summary>Details</summary>
Motivation: 在文本到图像生成中，预训练的Stable Diffusion模型中不同的初始噪声会导致多样但可能与提示不符的图像。现有方法存在局限性。本文将此问题归因于训练-推理不匹配：训练时提示条件噪声位于特定子空间，而推理时噪声来自与提示无关的高斯先验。

Method: 提出一个噪声投影器，在去噪前对初始噪声进行文本条件细化。该投影器根据提示嵌入将噪声映射为更具提示感知性的对应物，使其更符合SD训练期间观察到的分布，且无需修改SD模型。具体步骤包括：采样噪声、从VLM获取图像的token级反馈、将信号提炼成奖励模型，最后通过准直接偏好优化来优化噪声投影器。

Result: 广泛的实验表明，所提出的提示感知噪声投影器显著改善了各种提示下的文本-图像对齐。该方法无需参考图像或手工先验，并且推理成本低，用单次前向传播替代了多样本选择。

Conclusion: 通过弥补训练-推理时的初始噪声分布不匹配，所提出的噪声投影器能够有效提升Stable Diffusion模型生成图像的文本-图像对齐，且具有高效率和低成本的优点。

Abstract: In text-to-image generation, different initial noises induce distinct
denoising paths with a pretrained Stable Diffusion (SD) model. While this
pattern could output diverse images, some of them may fail to align well with
the prompt. Existing methods alleviate this issue either by altering the
denoising dynamics or by drawing multiple noises and conducting post-selection.
In this paper, we attribute the misalignment to a training-inference mismatch:
during training, prompt-conditioned noises lie in a prompt-specific subset of
the latent space, whereas at inference the noise is drawn from a
prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector
that applies text-conditioned refinement to the initial noise before denoising.
Conditioned on the prompt embedding, it maps the noise to a prompt-aware
counterpart that better matches the distribution observed during SD training,
without modifying the SD model. Our framework consists of these steps: we first
sample some noises and obtain token-level feedback for their corresponding
images from a vision-language model (VLM), then distill these signals into a
reward model, and finally optimize the noise projector via a quasi-direct
preference optimization. Our design has two benefits: (i) it requires no
reference images or handcrafted priors, and (ii) it incurs small inference
cost, replacing multi-sample selection with a single forward pass. Extensive
experiments further show that our prompt-aware noise projection improves
text-image alignment across diverse prompts.

</details>


### [97] [PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model](https://arxiv.org/abs/2510.14528)
*Cheng Cui,Ting Sun,Suyin Liang,Tingquan Gao,Zelun Zhang,Jiaxuan Liu,Xueqing Wang,Changda Zhou,Hongen Liu,Manhui Lin,Yue Zhang,Yubo Zhang,Handong Zheng,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: 本文提出了PaddleOCR-VL，一个SOTA且资源高效的文档解析模型，集成了VLM技术以实现多语言支持和复杂元素识别，并在基准测试中表现出色，适合实际部署。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个SOTA、资源高效、能准确识别复杂文档元素并支持多语言的文档解析模型，以满足实际部署的需求。

Method: 核心模型为PaddleOCR-VL-0.9B，一个紧凑型视觉-语言模型（VLM），集成了NaViT风格的动态分辨率视觉编码器和ERNIE-4.5-0.3B语言模型，实现准确的元素识别。

Result: PaddleOCR-VL在公共和内部基准测试中，在页面级文档解析和元素级识别方面均达到SOTA性能，显著优于现有解决方案，与顶级VLM具有竞争力，并提供快速推理速度，同时保持极低的资源消耗。

Conclusion: PaddleOCR-VL以其卓越的性能、资源效率和多语言支持，非常适合在真实世界场景中进行实际部署。

Abstract: In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model
tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a
compact yet powerful vision-language model (VLM) that integrates a NaViT-style
dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to
enable accurate element recognition. This innovative model efficiently supports
109 languages and excels in recognizing complex elements (e.g., text, tables,
formulas, and charts), while maintaining minimal resource consumption. Through
comprehensive evaluations on widely used public benchmarks and in-house
benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document
parsing and element-level recognition. It significantly outperforms existing
solutions, exhibits strong competitiveness against top-tier VLMs, and delivers
fast inference speeds. These strengths make it highly suitable for practical
deployment in real-world scenarios.

</details>


### [98] [Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology](https://arxiv.org/abs/2510.14532)
*Xinrui Huang,Fan Xiao,Dongming He,Anqi Gao,Dandan Li,Xiaofan Zhang,Shaoting Zhang,Xudong Wang*

Main category: cs.CV

TL;DR: 本文提出了DentVFM，首个牙科视觉基础模型家族，利用大规模多模态数据集（DentVista）进行自监督学习，实现通用智能，并在牙科任务中展现出超越现有基线的卓越泛化能力和标签效率，解决牙科影像判读专业人员短缺及现有AI系统局限性问题。


<details>
  <summary>Details</summary>
Motivation: 口腔颌面放射学在牙科医疗中至关重要，但受训专业人员短缺限制了影像判读。现有牙科AI系统存在单模态、任务特定、依赖昂贵标注数据等问题，难以泛化到多样化的临床场景。

Method: 引入了DentVFM，首个专为牙科设计的视觉基础模型（VFMs）家族。它通过对DentVista（包含约160万张多模态放射影像的大型牙科影像数据集）进行自监督学习，生成任务无关的视觉表征。DentVFM包含基于Vision Transformer（ViT）架构的2D和3D变体。为评估模型，还引入了DentBench，一个涵盖八个牙科亚专科、更多疾病、影像模态和广泛地理分布的综合基准。

Result: DentVFM展现出令人印象深刻的通用智能，对各种牙科任务（如疾病诊断、治疗分析、生物标志物识别、解剖标志物检测和分割）具有强大的泛化能力。实验结果表明，DentVFM显著优于有监督、自监督和弱监督基线，提供卓越的泛化能力、标签效率和可扩展性。此外，DentVFM还能实现跨模态诊断，在常规成像不可用的情况下，其结果比经验丰富的牙医更可靠。

Conclusion: DentVFM为牙科AI设立了新范式，提供了一个可扩展、适应性强且标签高效的模型，旨在改善智能牙科医疗并解决全球口腔医疗中的关键差距。

Abstract: Oral and maxillofacial radiology plays a vital role in dental healthcare, but
radiographic image interpretation is limited by a shortage of trained
professionals. While AI approaches have shown promise, existing dental AI
systems are restricted by their single-modality focus, task-specific design,
and reliance on costly labeled data, hindering their generalization across
diverse clinical scenarios. To address these challenges, we introduce DentVFM,
the first family of vision foundation models (VFMs) designed for dentistry.
DentVFM generates task-agnostic visual representations for a wide range of
dental applications and uses self-supervised learning on DentVista, a large
curated dental imaging dataset with approximately 1.6 million multi-modal
radiographic images from various medical centers. DentVFM includes 2D and 3D
variants based on the Vision Transformer (ViT) architecture. To address gaps in
dental intelligence assessment and benchmarks, we introduce DentBench, a
comprehensive benchmark covering eight dental subspecialties, more diseases,
imaging modalities, and a wide geographical distribution. DentVFM shows
impressive generalist intelligence, demonstrating robust generalization to
diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker
identification, and anatomical landmark detection and segmentation.
Experimental results indicate DentVFM significantly outperforms supervised,
self-supervised, and weakly supervised baselines, offering superior
generalization, label efficiency, and scalability. Additionally, DentVFM
enables cross-modality diagnostics, providing more reliable results than
experienced dentists in situations where conventional imaging is unavailable.
DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and
label-efficient model to improve intelligent dental healthcare and address
critical gaps in global oral healthcare.

</details>


### [99] [Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval](https://arxiv.org/abs/2510.14535)
*Keima Abe,Hayato Muraki,Shuhei Tomoshige,Kenichi Oishi,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: PL-SE-ADA是一个用于医学图像域协调和可解释表示学习的新框架，在保持疾病相关信息的同时，提高了性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 医学图像（如MR扫描）因扫描仪和协议差异存在域偏移，导致机器学习在疾病分类等任务中性能下降。域协调至关重要，但现有方法缺乏可解释性，这在医疗应用中是必需的。

Method: 提出Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA) 框架。该框架包含两个编码器 ($f_E$, $f_{SE}$) 分别提取域不变特征 ($oldsymbol{z_u}$) 和域特定特征 ($oldsymbol{z_d}$)，一个解码器 ($f_D$) 用于图像重建，以及一个域预测器 ($g_D$)。模型通过编码器与域预测器之间的对抗训练，并通过将$oldsymbol{z_u}$和$oldsymbol{z_d}$的重建结果求和来重建输入图像，以确保域协调和信息保持。

Result: PL-SE-ADA在图像重建、疾病分类和域识别方面的性能与现有方法相当或更优。

Conclusion: PL-SE-ADA不仅性能出色，还能够可视化域无关的脑特征和域特定的组成部分，从而提供了高可解释性，这对于医疗应用至关重要。

Abstract: Medical images like MR scans often show domain shifts across imaging sites
due to scanner and protocol differences, which degrade machine learning
performance in tasks such as disease classification. Domain harmonization is
thus a critical research focus. Recent approaches encode brain images
$\boldsymbol{x}$ into a low-dimensional latent space $\boldsymbol{z}$, then
disentangle it into $\boldsymbol{z_u}$ (domain-invariant) and
$\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these
methods often lack interpretability$-$an essential requirement in medical
applications$-$leaving practical issues unresolved. We propose
Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a
general framework for domain harmonization and interpretable representation
learning that preserves disease-relevant information in brain MR images.
PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract
$\boldsymbol{z_u}$ and $\boldsymbol{z_d}$, a decoder to reconstruct the image
$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the
encoder and domain predictor, the model learns to reconstruct the input image
$\boldsymbol{x}$ by summing reconstructions from $\boldsymbol{z_u}$ and
$\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared
to prior methods, PL-SE-ADA achieves equal or better performance in image
reconstruction, disease classification, and domain recognition. It also enables
visualization of both domain-independent brain features and domain-specific
components, offering high interpretability across the entire framework.

</details>


### [100] [Exploring Image Representation with Decoupled Classical Visual Descriptors](https://arxiv.org/abs/2510.14536)
*Chenyuan Qu,Hao Chen,Jianbo Jiao*

Main category: cs.CV

TL;DR: 提出VisualSplit框架，通过将图像分解为可解释的经典视觉描述符，并进行重建驱动的预训练，旨在结合现代学习与经典视觉线索，以增强高级视觉任务中的属性控制能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习在图像理解中表现出色，但其内部表示缺乏可解释性。相比之下，经典视觉描述符（如边缘、颜色）易于理解。研究动机在于弥补这一鸿沟，探索现代学习能否从这些可解释的经典视觉线索中受益。

Method: 引入VisualSplit框架，该框架将图像显式分解为解耦的经典描述符，每个描述符被视为独立的视觉知识组件。通过重建驱动的预训练方案，VisualSplit学习捕获每个描述符的本质并保留其可解释性。

Result: 通过显式分解视觉属性，VisualSplit能够有效控制图像生成和编辑等高级视觉任务中的属性，其应用范围超越了传统的分类和分割任务。

Conclusion: 该方法将可解释的经典视觉线索融入现代学习，为视觉理解提供了一种有效的新途径，并能促进高级视觉任务中的属性控制。

Abstract: Exploring and understanding efficient image representations is a
long-standing challenge in computer vision. While deep learning has achieved
remarkable progress across image understanding tasks, its internal
representations are often opaque, making it difficult to interpret how visual
information is processed. In contrast, classical visual descriptors (e.g. edge,
colour, and intensity distribution) have long been fundamental to image
analysis and remain intuitively understandable to humans. Motivated by this
gap, we ask a central question: Can modern learning benefit from these
classical cues? In this paper, we answer it with VisualSplit, a framework that
explicitly decomposes images into decoupled classical descriptors, treating
each as an independent but complementary component of visual knowledge. Through
a reconstruction-driven pre-training scheme, VisualSplit learns to capture the
essence of each visual descriptor while preserving their interpretability. By
explicitly decomposing visual attributes, our method inherently facilitates
effective attribute control in various advanced visual tasks, including image
generation and editing, extending beyond conventional classification and
segmentation, suggesting the effectiveness of this new learning approach for
visual understanding. Project page: https://chenyuanqu.com/VisualSplit/.

</details>


### [101] [Exploring Cross-Modal Flows for Few-Shot Learning](https://arxiv.org/abs/2510.14543)
*Ziqi Jiang,Yanghao Wang,Long Chen*

Main category: cs.CV

TL;DR: 本文提出首个多步调整方法FMA，通过学习跨模态速度场，解决了现有PEFT方法在复杂跨模态任务中一步对齐的局限性，显著提升了对齐精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调（PEFT）方法在跨模态任务中仅执行一步调整，这对于特征高度纠缠的复杂（或困难）数据集来说是不足的，导致无法实现精确且鲁棒的对齐。

Method: 本文提出首个模型无关的多步调整方法——流匹配对齐（Flow Matching Alignment, FMA）。该方法通过学习跨模态速度场进行多步特征调整。具体包括：采用固定耦合策略确保训练时类别对应，提出噪声增强策略缓解数据稀缺问题，并设计早停求解器以提升效率和准确性。

Result: 相比于一步PEFT方法，FMA具备多步修正能力，能够实现更精确和鲁棒的跨模态对齐。在各种基准和骨干网络上，尤其是在具有挑战性的数据集上，FMA持续取得了显著的性能提升。

Conclusion: FMA通过引入多步特征调整机制，有效克服了现有PEFT方法在处理复杂跨模态对齐时的不足，成功实现了更精确、更鲁棒的对齐效果，并在实际应用中展现出卓越的性能优势。

Abstract: Aligning features from different modalities, is one of the most fundamental
challenges for cross-modal tasks. Although pre-trained vision-language models
can achieve a general alignment between image and text, they often require
parameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT
methods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively
fine-tune a subset of parameters, which can slightly adjust either visual or
textual features, and avoid overfitting. In this paper, we are the first to
highlight that all existing PEFT methods perform one-step adjustment. It is
insufficient for complex (or difficult) datasets, where features of different
modalities are highly entangled. To this end, we propose the first
model-agnostic multi-step adjustment approach by learning a cross-modal
velocity field: Flow Matching Alignment (FMA). Specifically, to ensure the
correspondence between categories during training, we first utilize a fixed
coupling strategy. Then, we propose a noise augmentation strategy to alleviate
the data scarcity issue. Finally, we design an early-stopping solver, which
terminates the transformation process earlier, improving both efficiency and
accuracy. Compared with one-step PEFT methods, FMA has the multi-step
rectification ability to achieve more precise and robust alignment. Extensive
results have demonstrated that FMA can consistently yield significant
performance gains across various benchmarks and backbones, particularly on
challenging datasets.

</details>


### [102] [Consistent text-to-image generation via scene de-contextualization](https://arxiv.org/abs/2510.14553)
*Song Tang,Peihao Gong,Kunyu Li,Kai Guo,Boyu Wang,Mao Ye,Jianwei Zhang,Xiatian Zhu*

Main category: cs.CV

TL;DR: 本文提出SDeC方法，通过编辑提示词嵌入来消除主体与场景的内在关联，从而解决文本到图像生成中主体一致性（ID漂移）问题，且无需预知所有目标场景。


<details>
  <summary>Details</summary>
Motivation: 文本到图像生成在不同场景下保持相同主体一致性时，常因“身份漂移（ID shift）”而失败。现有方法往往依赖于预知所有目标场景这一不切实际的假设。本文揭示ID漂移主要源于T2I模型中主体与场景的内在关联（场景情境化）。

Method: 提出“场景去情境化 (Scene De-Contextualization, SDeC)”方法。这是一种高效、无需训练的提示词嵌入编辑方法，通过量化SVD方向稳定性并自适应重新加权对应特征值，识别并抑制ID提示词嵌入中潜在的场景-ID关联，从而反转T2I固有的场景情境化。该方法支持逐场景使用，无需预先知道所有目标场景。

Result: 实验证明SDeC显著增强了身份保留能力，同时保持了场景多样性。

Conclusion: SDeC是一种高度灵活且通用的解决方案，非常适用于实际应用中缺乏先验场景知识的场景。

Abstract: Consistent text-to-image (T2I) generation seeks to produce
identity-preserving images of the same subject across diverse scenes, yet it
often fails due to a phenomenon called identity (ID) shift. Previous methods
have tackled this issue, but typically rely on the unrealistic assumption of
knowing all target scenes in advance. This paper reveals that a key source of
ID shift is the native correlation between subject and scene context, called
scene contextualization, which arises naturally as T2I models fit the training
distribution of vast natural images. We formally prove the near-universality of
this scene-ID correlation and derive theoretical bounds on its strength. On
this basis, we propose a novel, efficient, training-free prompt embedding
editing approach, called Scene De-Contextualization (SDeC), that imposes an
inversion process of T2I's built-in scene contextualization. Specifically, it
identifies and suppresses the latent scene-ID correlation within the ID
prompt's embedding by quantifying the SVD directional stability to adaptively
re-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene
use (one scene per prompt) without requiring prior access to all target scenes.
This makes it a highly flexible and general solution well-suited to real-world
applications where such prior knowledge is often unavailable or varies over
time. Experiments demonstrate that SDeC significantly enhances identity
preservation while maintaining scene diversity.

</details>


### [103] [Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video](https://arxiv.org/abs/2510.14560)
*Yulin Zhang,Cheng Shi,Yang Wang,Sibei Yang*

Main category: cs.CV

TL;DR: 本文提出了一种创新的任务，旨在开发一个AI助手，能够根据第一视角流式视频输入，在恰当的时机主动回答多样且不断演变的问题，并保持感知与推理的同步。为此，引入了ESTP-Bench基准和ESTP-F1评估指标，并提出了一套包含数据引擎、多阶段训练策略和主动动态压缩技术的完整技术管线。


<details>
  <summary>Details</summary>
Motivation: 旨在实现一个能够在类人环境中运行的AI，不仅能观察，还能主动理解、预测并对事件做出先发制人的响应。具体而言，针对第一视角流式视频输入，开发一个能够主动、适时回答演变问题的助手，同时保持感知与推理的同步效率。

Method: 1. 引入ESTP-Bench基准和ESTP-F1指标，用于严格评估主动连贯性、适时响应性和同步效率这三个关键特性。2. 提出一个全面的技术管线，包括：一个数据引擎、一个多阶段训练策略，以及一个主动动态压缩技术，以应对该挑战性任务。

Result: 所提出的模型有效地解决了上述关键特性，并在各种在线和离线基准测试中，优于多个基线模型。

Conclusion: 该研究成功地为实现具有类人交互能力的AI迈出了重要一步，通过引入新任务、评估框架和高效技术管线，证明了AI在主动理解和响应流媒体事件方面的潜力及优越性。

Abstract: Envision an AI capable of functioning in human-like settings, moving beyond
mere observation to actively understand, anticipate, and proactively respond to
unfolding events. Towards this vision, we focus on the innovative task where,
given ego-streaming video input, an assistant proactively answers diverse,
evolving questions at the opportune moment, while maintaining synchronized
perception and reasoning. This task embodies three key properties: (1)
Proactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized
Efficiency. To evaluate and address these properties, we first introduce
ESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a
novel framework designed for their rigorous assessment. Secondly, we propose a
comprehensive technical pipeline to enable models to tackle this challenging
task. This pipeline comprises: (1) a data engine, (2) a multi-stage training
strategy, and (3) a proactive dynamic compression technique. Our proposed model
effectively addresses these critical properties while outperforming multiple
baselines across diverse online and offline benchmarks. Project
Page:https://zhangyl4.github.io/publications/eyes-wide-open/

</details>


### [104] [BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU](https://arxiv.org/abs/2510.14564)
*Junyi Wu,Jiaming Xu,Jinhao Li,Yongkang Zhou,Jiayi Pan,Xingyang Li,Guohao Dai*

Main category: cs.CV

TL;DR: 本文提出了BalanceGS，一种算法-系统协同设计的3D Gaussian Splatting (3DGS) 训练方法，旨在解决传统3DGS在高斯稠密化、投影和颜色溅射过程中的效率低下问题，实现了显著的训练加速，同时保持了重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统的3DGS训练管线存在三大效率问题：(1) 高斯稠密化过程中密度分配不均；(2) 高斯投影计算负载不平衡；(3) 颜色溅射过程中内存访问碎片化。

Method: 本文引入了BalanceGS，采用算法-系统协同设计来解决上述挑战：(1) 在算法层面，提出了启发式工作负载敏感的高斯密度控制，自动平衡点分布，减少冗余高斯并填充稀疏区域。(2) 在系统层面，提出了基于相似度的高斯采样与合并，用自适应工作负载分配取代静态线程-像素映射，使线程能根据局部簇密度动态处理可变数量的高斯。(3) 在映射层面，提出了基于重排序的内存访问映射策略，重构RGB存储并实现共享内存中的批量加载。

Result: 与传统3DGS相比，BalanceGS在NVIDIA A100 GPU上实现了1.44倍的训练速度提升，且重建质量下降可忽略不计。

Conclusion: BalanceGS通过算法与系统的协同设计，有效解决了3DGS训练中的效率瓶颈，显著加快了训练速度，同时保持了高质量的3D重建效果。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction
technique. The traditional 3DGS training pipeline follows three sequential
steps: Gaussian densification, Gaussian projection, and color splatting.
Despite its promising reconstruction quality, this conventional approach
suffers from three critical inefficiencies: (1) Skewed density allocation
during Gaussian densification, (2) Imbalanced computation workload during
Gaussian projection and (3) Fragmented memory access during color splatting.
  To tackle the above challenges, we introduce BalanceGS, the algorithm-system
co-design for efficient training in 3DGS. (1) At the algorithm level, we
propose heuristic workload-sensitive Gaussian density control to automatically
balance point distributions - removing 80% redundant Gaussians in dense regions
while filling gaps in sparse areas. (2) At the system level, we propose
Similarity-based Gaussian sampling and merging, which replaces the static
one-to-one thread-pixel mapping with adaptive workload distribution - threads
now dynamically process variable numbers of Gaussians based on local cluster
density. (3) At the mapping level, we propose reordering-based memory access
mapping strategy that restructures RGB storage and enables batch loading in
shared memory.
  Extensive experiments demonstrate that compared with 3DGS, our approach
achieves a 1.44$\times$ training speedup on a NVIDIA A100 GPU with negligible
quality degradation.

</details>


### [105] [CALM-Net: Curvature-Aware LiDAR Point Cloud-based Multi-Branch Neural Network for Vehicle Re-Identification](https://arxiv.org/abs/2510.14576)
*Dongwook Lee,Sol Han,Jinwhan Kim*

Main category: cs.CV

TL;DR: 本文提出了CALM-Net，一种曲率感知的多分支神经网络，用于基于LiDAR点云的车辆重识别，通过整合边缘卷积、点注意力和曲率嵌入，实现了显著的准确性提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从三维点云中学习到具有判别性和互补性的特征，以有效区分不同车辆，这是车辆重识别任务面临的挑战。

Method: CALM-Net采用多分支架构，集成了边缘卷积、点注意力机制，并引入了表征点云局部表面变化的曲率嵌入。通过结合这些机制，模型能够学习到更丰富的几何和上下文特征。

Result: 在大型nuScenes数据集上的实验表明，CALM-Net的平均重识别准确率比研究中最强的基线提高了约1.97%。

Conclusion: 研究结果证实了将曲率信息融入深度学习架构的有效性，并强调了多分支特征学习对于基于LiDAR点云的车辆重识别任务的益处。

Abstract: This paper presents CALM-Net, a curvature-aware LiDAR point cloud-based
multi-branch neural network for vehicle re-identification. The proposed model
addresses the challenge of learning discriminative and complementary features
from three-dimensional point clouds to distinguish between vehicles. CALM-Net
employs a multi-branch architecture that integrates edge convolution, point
attention, and a curvature embedding that characterizes local surface variation
in point clouds. By combining these mechanisms, the model learns richer
geometric and contextual features that are well suited for the
re-identification task. Experimental evaluation on the large-scale nuScenes
dataset demonstrates that CALM-Net achieves a mean re-identification accuracy
improvement of approximately 1.97\% points compared with the strongest baseline
in our study. The results confirms the effectiveness of incorporating curvature
information into deep learning architectures and highlight the benefit of
multi-branch feature learning for LiDAR point cloud-based vehicle
re-identification.

</details>


### [106] [Talking Points: Describing and Localizing Pixels](https://arxiv.org/abs/2510.14583)
*Matan Rusanovsky,Shimon Malnick,Shai Avidan*

Main category: cs.CV

TL;DR: 本文提出了一个新颖的框架，使视觉-语言模型能够通过自然语言实现像素级精确的关键点理解。该框架包含一个生成上下文描述的Point Descriptor和一个从描述中回归像素坐标的Point Localizer，并引入了LlamaPointInPart数据集，实验证明其性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在跨模态理解方面表现出色，但仅限于对象级或区域级定位，缺乏通过自然语言进行像素级精确关键点理解的能力。此外，现有方法依赖模板化提示或关键点名称，且缺乏用于训练此类系统的专用数据集。

Method: 本文提出了一个由Point Descriptor和Point Localizer组成的框架。Point Descriptor生成自由形式、从粗到精的上下文关键点描述，Point Localizer则根据这些描述回归精确的像素坐标。为解决数据缺失，研究者构建了LlamaPointInPart数据集（20K+图像-关键点-描述三元组）。为实现跨类别泛化，Point Descriptor在AP-10K上通过GRPO进行优化，使用冻结的Point Localizer作为奖励模型以最大化定位精度。同时，建立了一种新的评估协议，通过局部化器直接衡量预测点与真实点的接近程度。

Result: 实验证明，与基线模型相比，该方法在LlamaPointInPart数据集上取得了卓越的性能。

Conclusion: 该框架的双向特性有望在关键点引导的图像理解和语言引导的精确定位领域带来未来的应用。代码和数据集已公开。

Abstract: Vision-language models have achieved remarkable success in cross-modal
understanding. Yet, these models remain limited to object-level or region-level
grounding, lacking the capability for pixel-precise keypoint comprehension
through natural language. We introduce a novel framework for pixel level
grounding. The framework consists of two complementary components: a Point
Descriptor that generates rich, contextual descriptions of individual
keypoints, and a Point Localizer that regresses precise pixel coordinates from
these descriptions. Unlike prior work that relies on templated prompts or
keypoint names, our approach produces free-form, coarse-to-fine descriptions
that situate keypoints within their visual context. Since there is no available
dataset to train such a system, we introduce LlamaPointInPart, a carefully
curated dataset of 20K+ image-keypoint-description triplets synthesized from
multiple vision-language models, capturing multi-scale information from
scene-level context to visual features around the keypoint. For cross-category
generalization, we optimize the Point Descriptor on AP-10K via GRPO, using the
frozen Point Localizer as a reward model to produce descriptions that maximize
localization accuracy. To evaluate our results we establish a new evaluation
protocol. Instead of comparing the text description produced by our method to
the ground truth, we use the localizer to determine how close is the predicted
point generated to the ground truth point. Experiments demonstrate superior
performance compared to baseline models on LlamaPointInPart.The bidirectional
nature of our framework should enable future applications in both
keypoint-guided image understanding and language-guided precise localization.
Our code and dataset are publicly available at
https://github.com/matanr/Talking_Points.

</details>


### [107] [STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding](https://arxiv.org/abs/2510.14588)
*Zhifei Chen,Tianshuo Xu,Leyi Wu,Luozhou Wang,Dongyu Yan,Zihan You,Wenting Luo,Guo Zhang,Yingcong Chen*

Main category: cs.CV

TL;DR: STANCE是一种图像到视频框架，通过Instance Cues和Dense RoPE组件，解决视频生成中物体运动不连贯问题，显著提升了时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法在保持物体运动和交互连贯性方面存在困难。主要瓶颈是：用户提供的运动提示在编码后有效信息量不足，以及外观和运动的单一优化头易使模型偏重纹理而非时间一致性。

Method: 提出STANCE框架，包含两个核心组件：1) Instance Cues，将稀疏用户提示转换为密集的2.5D（相机相对）运动场，通过平均每实例流和实例掩码上的单目深度实现，减少深度模糊性。2) Dense RoPE，通过空间可寻址的旋转嵌入标记少量运动token，以保持其显著性。模型结合RGB与辅助图（分割或深度）联合预测，以锚定结构并处理外观。

Result: 该方法稳定了优化过程，提高了时间连贯性，并减少了深度模糊，同时避免了对逐帧轨迹脚本的依赖。

Conclusion: STANCE通过创新的Instance Cues和Dense RoPE机制，有效克服了视频生成中运动连贯性和深度模糊的挑战，为高质量视频生成提供了更稳定的优化和更好的时间一致性。

Abstract: Video generation has recently made striking visual progress, but maintaining
coherent object motion and interactions remains difficult. We trace two
practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps)
often collapse to too few effective tokens after encoding, weakening guidance;
and (ii) optimizing for appearance and motion in a single head can favor
texture over temporal consistency. We present STANCE, an image-to-video
framework that addresses both issues with two simple components. First, we
introduce Instance Cues -- a pixel-aligned control signal that turns sparse,
user-editable hints into a dense 2.5D (camera-relative) motion field by
averaging per-instance flow and augmenting with monocular depth over the
instance mask. This reduces depth ambiguity compared to 2D arrow inputs while
remaining easy to use. Second, we preserve the salience of these cues in token
space with Dense RoPE, which tags a small set of motion tokens (anchored on the
first frame) with spatial-addressable rotary embeddings. Paired with joint RGB
\(+\) auxiliary-map prediction (segmentation or depth), our model anchors
structure while RGB handles appearance, stabilizing optimization and improving
temporal coherence without requiring per-frame trajectory scripts.

</details>


### [108] [Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers](https://arxiv.org/abs/2510.14594)
*Hugo Markoff,Jevgenijs Galaktionovs*

Main category: cs.CV

TL;DR: 本文提出一种分层重分类系统，结合SpeciesNet、CLIP嵌入和度量学习，将动物高层级分类提升至物种级别，在数据集上实现了96.5%的重分类准确率和64.9%的物种级识别。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的动物分类模型（如SpeciesNet）倾向于采用保守策略，导致动物分类停留在较高的分类级别，无法提供细粒度的物种级别识别。

Method: 开发了一个用于Animal Detect平台的分层重分类系统。该系统整合了SpeciesNet EfficientNetV2-M的预测、CLIP嵌入和度量学习，旨在将高层级分类标签细化至物种级别。该管道包含五个阶段：高置信度接受、鸟类覆盖、质心构建、三元组损失度量学习和自适应余弦距离评分。

Result: 在LILA BC沙漠狮子保护数据集（4,018张图像，15,031个检测）上进行评估。成功从“空白”和“动物”标签中恢复了761个鸟类检测。对456个被标记为动物、哺乳动物或空白的检测进行重分类，准确率达到96.5%，并实现了64.9%的物种级别识别。

Conclusion: 该分层重分类系统有效解决了现有模型在物种识别粒度上的不足，显著提升了从高层级分类到物种级别识别的准确性和覆盖率。

Abstract: State-of-the-art animal classification models like SpeciesNet provide
predictions across thousands of species but use conservative rollup strategies,
resulting in many animals labeled at high taxonomic levels rather than species.
We present a hierarchical re-classification system for the Animal Detect
platform that combines SpeciesNet EfficientNetV2-M predictions with CLIP
embeddings and metric learning to refine high-level taxonomic labels toward
species-level identification. Our five-stage pipeline (high-confidence
acceptance, bird override, centroid building, triplet-loss metric learning, and
adaptive cosine-distance scoring) is evaluated on a segment of the LILA BC
Desert Lion Conservation dataset (4,018 images, 15,031 detections). After
recovering 761 bird detections from "blank" and "animal" labels, we re-classify
456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving
species-level identification for 64.9 percent

</details>


### [109] [Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering](https://arxiv.org/abs/2510.14596)
*Hugo Markoff,Jevgenijs Galaktionovs*

Main category: cs.CV

TL;DR: 本研究评估了使用自监督视觉Transformer的零样本方法来组织相机陷阱生成的未标记野生动物图像，通过无监督聚类和一维相似度排序实现了高效的物种识别与数据标注。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱产生了数百万张野生动物图像，但许多数据集中包含现有分类器无法识别的物种，导致数据组织和人工标注效率低下。

Method: 本研究在Animal Detect平台中，评估了零样本方法。具体包括：比较了三种视觉Transformer架构（CLIP, DINOv2, MegaDescriptor）与降维技术（PCA, UMAP）结合的无监督聚类方法（DBSCAN, GMM），并展示了通过t-SNE投影实现的连续一维相似度排序。

Result: 在一个包含5个物种的测试集上，DINOv2结合UMAP和GMM实现了88.6%的准确率（macro-F1 = 0.874）。一维排序在1500张图像中，对哺乳动物和鸟类达到了88.2%的连贯性，对鱼类达到了95.2%的连贯性。

Conclusion: 基于这些发现，连续相似度排序已被部署到生产环境，显著提高了快速探索性分析能力，并加速了生物多样性监测的手动标注工作流程。

Abstract: Camera traps generate millions of wildlife images, yet many datasets contain
species that are absent from existing classifiers. This work evaluates
zero-shot approaches for organizing unlabeled wildlife imagery using
self-supervised vision transformers, developed and tested within the Animal
Detect platform for camera trap analysis. We compare unsupervised clustering
methods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor)
combined with dimensionality reduction techniques (PCA, UMAP), and we
demonstrate continuous 1D similarity ordering via t-SNE projection. On a
5-species test set with ground truth labels used only for evaluation, DINOv2
with UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D
sorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent
for fish across 1,500 images. Based on these findings, we deployed continuous
similarity ordering in production, enabling rapid exploratory analysis and
accelerating manual annotation workflows for biodiversity monitoring.

</details>


### [110] [Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering](https://arxiv.org/abs/2510.14605)
*Yuyang Hong,Jiaqi Gu,Qi Yang,Lubin Fan,Yue Wu,Ying Wang,Kun Ding,Shiming Xiang,Jieping Ye*

Main category: cs.CV

TL;DR: 本文提出Wiki-PRF，一种针对知识型视觉问答（KB-VQA）的三阶段方法，通过动态调用视觉工具、整合多模态特征检索和过滤不相关内容，解决了多模态查询质量和检索结果相关性不足的问题，并在基准数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管检索增强生成（RAG）在知识型视觉问答（KB-VQA）中取得了进展，但现有方法在多模态查询质量和检索结果的相关性方面仍面临挑战。

Method: 提出三阶段方法Wiki-PRF：
1.  **处理阶段 (Processing)**：动态调用视觉工具提取精确的多模态信息用于检索。
2.  **检索阶段 (Retrieval)**：整合视觉和文本特征进行多模态知识检索。
3.  **过滤阶段 (Filtering)**：对检索结果进行相关性过滤和集中。
此外，引入了一个视觉语言模型，通过强化学习方式，以答案准确性和格式一致性作为奖励信号进行训练，以增强模型推理、工具调用和过滤无关内容的能力。

Result: 在E-VQA和InfoSeek基准数据集上的实验表明，答案质量显著提高（分别达到36.0和42.8），实现了最先进的性能。

Conclusion: Wiki-PRF通过其创新的三阶段方法和强化学习训练的视觉语言模型，有效解决了KB-VQA中多模态查询质量和检索结果相关性问题，取得了卓越的性能提升，达到了领域内最佳水平。

Abstract: Knowledge-based visual question answering (KB-VQA) requires visual language
models (VLMs) to integrate visual understanding with external knowledge
retrieval. Although retrieval-augmented generation (RAG) achieves significant
advances in this task by combining knowledge-base querying, it still struggles
with the quality of multimodal queries and the relevance of retrieved results.
To overcome these challenges, we propose a novel three-stage method, termed
Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing
stage dynamically invokes visual tools to extract precise multimodal
information for retrieval. The retrieval stage integrates visual and text
features to achieve multimodal knowledge retrieval. The filtering stage
performs relevance filtering and concentration on retrieval results. To this
end, we introduce a visual language model trained with answer accuracy and
format consistency as reward signals via a reinforcement learning manner. This
enhances the model's reasoning, tool invocation for accurate queries, and
filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and
InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,
achieving state-of-the-art performance. Code is available at
https://github.com/cqu-student/Wiki-PRF

</details>


### [111] [Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding](https://arxiv.org/abs/2510.14617)
*Ning Ding,Keisuke Fujii,Toru Tamaki*

Main category: cs.CV

TL;DR: 本文提出了Shot2Tactic-Caption框架，用于羽毛球视频的语义和时间多尺度字幕生成，能同时描述个体动作（击球级）和战术执行（战术级），并引入了首个羽毛球字幕数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能全面理解羽毛球战术，即无法解释个体动作如何随时间动态地构成战术执行。

Method: 提出Shot2Tactic-Caption框架，采用双分支设计，每个分支包含视觉编码器、时空Transformer编码器和Transformer解码器。引入Tactic Unit Detector识别战术单元、类型和状态，并通过击球级提示引导机制将预测的战术类型和状态嵌入解码器，以支持战术字幕生成。同时发布了Shot2Tactic-Caption数据集，包含5,494个击球级字幕和544个战术级字幕。

Result: 实验证明该框架在生成击球和战术字幕方面均有效。消融研究表明，基于ResNet50的时空编码器性能最佳，且击球级提示结构能提高战术字幕的连贯性和准确性。

Conclusion: 该研究成功地提出了一个新颖的羽毛球多尺度视频字幕框架和数据集，有效捕捉了复杂的战术执行，包括中断和恢复情况。

Abstract: Tactical understanding in badminton involves interpreting not only individual
actions but also how tactics are dynamically executed over time. In this paper,
we propose \textbf{Shot2Tactic-Caption}, a novel framework for semantic and
temporal multi-scale video captioning in badminton, capable of generating
shot-level captions that describe individual actions and tactic-level captions
that capture how these actions unfold over time within a tactical execution. We
also introduce the Shot2Tactic-Caption Dataset, the first badminton captioning
dataset containing 5,494 shot captions and 544 tactic captions.
Shot2Tactic-Caption adopts a dual-branch design, with both branches including a
visual encoder, a spatio-temporal Transformer encoder, and a Transformer-based
decoder to generate shot and tactic captions. To support tactic captioning, we
additionally introduce a Tactic Unit Detector that identifies valid tactic
units, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic
captioning, we further incorporate a shot-wise prompt-guided mechanism, where
the predicted tactic type and state are embedded as prompts and injected into
the decoder via cross-attention. The shot-wise prompt-guided mechanism enables
our system not only to describe successfully executed tactics but also to
capture tactical executions that are temporarily interrupted and later resumed.
Experimental results demonstrate the effectiveness of our framework in
generating both shot and tactic captions. Ablation studies show that the
ResNet50-based spatio-temporal encoder outperforms other variants, and that
shot-wise prompt structuring leads to more coherent and accurate tactic
captioning.

</details>


### [112] [Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference](https://arxiv.org/abs/2510.14624)
*Natan Bagrov,Eugene Khvedchenia,Borys Tymchenko,Shay Aharon,Lior Kadoch,Tomer Keren,Ofri Masad,Yonatan Geifman,Ran Zilberstein,Tuomas Rintamaki,Matthieu Le,Andrew Tao*

Main category: cs.CV

TL;DR: 提出高效视频采样（EVS）方法，通过修剪视频中时间静态区域来减少令牌冗余，从而提高视觉语言模型处理长视频的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在处理长视频时面临二次成本和令牌预算限制，因密集帧序列导致严重的上下文限制和延迟问题。

Method: EVS是一种即插即用方法，通过识别并修剪连续帧中未改变的空间区域（时间静态补丁）来减少令牌冗余。它保留位置身份，无需架构更改或重新训练，可在推理时应用，也可结合随机剪枝率的预训练阶段。

Result: EVS显著减少令牌数量，同时保持语义保真度，实现更快的推理和更长的输入序列。它将大型语言模型（LLM）的首次令牌时间（TTFT）最多减少4倍，且准确性损失极小。结合预训练后，模型对不同压缩水平表现出鲁棒性，并在积极剪枝下保持完整性能。

Conclusion: EVS持续改进效率-准确性权衡，实现可扩展的视频语言理解，且不牺牲质量。

Abstract: Vision-language models (VLMs) have recently expanded from static image
understanding to video reasoning, but their scalability is fundamentally
limited by the quadratic cost of processing dense frame sequences. Long videos
often exceed the token budget of modern language models, leading to severe
context limitations and latency issues. We introduce Efficient Video Sampling
(EVS), a simple, plug-and-play method for reducing token redundancy in videos
by identifying and pruning temporally static patches -- spatial regions that
remain unchanged across consecutive frames. EVS preserves positional identity,
requires no architectural changes or retraining. We show that EVS substantially
reduces token count while maintaining semantic fidelity, enabling faster
inference and longer input sequences. Applied at inference time, EVS reduces
large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal
accuracy loss. When combined with an uptraining phase using stochastic pruning
rates, EVS yields models that are robust to varying compression levels and
retain full performance under aggressive pruning. Extensive experiments
demonstrate that EVS consistently improves efficiency-accuracy trade-offs,
unlocking scalable video-language understanding without sacrificing quality.

</details>


### [113] [Adapting Self-Supervised Representations as a Latent Space for Efficient Generation](https://arxiv.org/abs/2510.14630)
*Ming Gui,Johannes Schusterbauer,Timy Phan,Felix Krause,Josh Susskind,Miguel Angel Bautista,Björn Ommer*

Main category: cs.CV

TL;DR: 本文提出了Representation Tokenizer (RepTok) 框架，它通过自监督视觉Transformer获得的单一连续潜在token来表示图像。RepTok通过微调预训练的SSL编码器并结合生成解码器，实现了高效、忠实的图像重建，并在ImageNet生成和文本到图像合成任务上取得了有竞争力的结果，同时显著降低了训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型常使用2D潜在空间，存在空间冗余且训练成本高昂。研究旨在利用自监督学习（SSL）表示的优势，通过紧凑有效的潜在空间（单一连续token）解决这些问题，实现高效且高质量的生成建模。

Method: RepTok框架基于预训练的自监督学习（SSL）编码器。方法包括：1) 仅微调语义token嵌入，使其富含低级重建细节；2) 将其与使用标准流匹配目标联合训练的生成解码器配对；3) 添加余弦相似度损失来正则化自适应token，以保留原始SSL空间的良好几何特性，确保潜在空间的平滑性和适用性。

Result: RepTok在类别条件ImageNet生成上取得了有竞争力的结果。它能自然地扩展到文本到图像合成，在极低的训练预算下，在MS-COCO上达到了有竞争力的零样本性能。该单一token公式解决了2D潜在空间的空间冗余，并显著降低了训练成本，同时实现了忠实的图像重建。

Conclusion: 研究结果强调了微调后的自监督学习（SSL）表示作为紧凑且有效的潜在空间，在高效生成建模方面具有巨大潜力。

Abstract: We introduce Representation Tokenizer (RepTok), a generative modeling
framework that represents an image using a single continuous latent token
obtained from self-supervised vision transformers. Building on a pre-trained
SSL encoder, we fine-tune only the semantic token embedding and pair it with a
generative decoder trained jointly using a standard flow matching objective.
This adaptation enriches the token with low-level, reconstruction-relevant
details, enabling faithful image reconstruction. To preserve the favorable
geometry of the original SSL space, we add a cosine-similarity loss that
regularizes the adapted token, ensuring the latent space remains smooth and
suitable for generation. Our single-token formulation resolves spatial
redundancies of 2D latent spaces and significantly reduces training costs.
Despite its simplicity and efficiency, RepTok achieves competitive results on
class-conditional ImageNet generation and naturally extends to text-to-image
synthesis, reaching competitive zero-shot performance on MS-COCO under
extremely limited training budgets. Our findings highlight the potential of
fine-tuned SSL representations as compact and effective latent spaces for
efficient generative modeling.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [114] [Decision Oriented Technique (DOTechnique): Finding Model Validity Through Decision-Maker Context](https://arxiv.org/abs/2510.13858)
*Raheleh Biglari,Joachim Denil*

Main category: cs.AI

TL;DR: 本文提出决策导向技术（DOTechnique），一种基于决策一致性而非输出相似性来确定模型有效性的新方法，通过代理模型和符号推理高效识别有效性区域。


<details>
  <summary>Details</summary>
Motivation: 模型有效性对于指导决策过程至关重要，但传统方法依赖预定义有效性框架，这些框架可能不可用或不足。

Method: 引入决策导向技术（DOTechnique），通过评估代理模型与高保真模型是否产生等效决策来确定模型有效性。该方法整合领域约束和符号推理以缩小搜索空间，提高计算效率。

Result: DOTechnique能够高效识别模型有效性区域，即使在缺乏明确有效性边界的情况下。以高速公路换道系统为例，展示了其揭示仿真模型有效性区域的潜力，并强调了该技术在通过决策者语境支持模型有效性方面的作用。

Conclusion: DOTechnique提供了一种新颖且高效的方法，通过关注决策一致性来确定模型有效性，为在复杂决策环境中缺乏传统有效性框架时，支持寻找模型有效性提供了潜力。

Abstract: Model validity is as critical as the model itself, especially when guiding
decision-making processes. Traditional approaches often rely on predefined
validity frames, which may not always be available or sufficient. This paper
introduces the Decision Oriented Technique (DOTechnique), a novel method for
determining model validity based on decision consistency rather than output
similarity. By evaluating whether surrogate models lead to equivalent decisions
compared to high-fidelity models, DOTechnique enables efficient identification
of validity regions, even in the absence of explicit validity boundaries. The
approach integrates domain constraints and symbolic reasoning to narrow the
search space, enhancing computational efficiency. A highway lane change system
serves as a motivating example, demonstrating how DOTechnique can uncover the
validity region of a simulation model. The results highlight the potential of
the technique to support finding model validity through decision-maker context.

</details>


### [115] [Do Slides Help? Multi-modal Context for Automatic Transcription of Conference Talks](https://arxiv.org/abs/2510.13979)
*Supriti Sinhamahapatra,Jan Niehues*

Main category: cs.AI

TL;DR: 本文通过整合演示文稿幻灯片作为多模态信息，显著提升了科学演示场景下的语音识别（ASR）性能，尤其是在领域特定术语的识别方面，词错误率相对基线模型降低了约34-35%。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的ASR系统主要依赖声学信息，忽略了多模态上下文。视觉信息（如演示幻灯片）对消歧和适应性至关重要，特别是在科学演示中，可以辅助处理噪声条件和领域特定术语识别。

Method: 首先，创建了一个多模态演示基准，并自动分析了领域特定术语的转录。其次，探索了通过多模态信息增强语音模型的方法。为了解决缺乏带幻灯片数据集的问题，采用了合适的数据增强方法。最后，使用增强后的数据集训练了一个模型。

Result: 与基线模型相比，该方法使所有词的词错误率相对降低了约34%，领域特定术语的词错误率相对降低了约35%。

Conclusion: 通过整合演示文稿幻灯片并采用数据增强策略，多模态ASR系统在科学演示场景下表现出显著的性能提升，尤其对领域特定术语的识别效果尤为突出。

Abstract: State-of-the-art (SOTA) Automatic Speech Recognition (ASR) systems primarily
rely on acoustic information while disregarding additional multi-modal context.
However, visual information are essential in disambiguation and adaptation.
While most work focus on speaker images to handle noise conditions, this work
also focuses on integrating presentation slides for the use cases of scientific
presentation.
  In a first step, we create a benchmark for multi-modal presentation including
an automatic analysis of transcribing domain-specific terminology. Next, we
explore methods for augmenting speech models with multi-modal information. We
mitigate the lack of datasets with accompanying slides by a suitable approach
of data augmentation. Finally, we train a model using the augmented dataset,
resulting in a relative reduction in word error rate of approximately 34%,
across all words and 35%, for domain-specific terms compared to the baseline
model.

</details>


### [116] [Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment](https://arxiv.org/abs/2510.13985)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Giovanni Franco Gabriel Marraffini,Mario Alejandro Leiva,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.AI

TL;DR: 研究发现，大型语言模型（LLMs）在因果判断任务中，即使缺乏支持证据，也容易产生“因果幻觉”。


<details>
  <summary>Details</summary>
Motivation: 因果幻觉是一种重要的认知偏差，影响诸多社会问题。本研究旨在探讨大型语言模型是否也存在这种偏差，这对理解其因果推理能力及其应用至关重要。

Method: 构建了一个包含1,000个医疗背景下零偶发性情景的数据集（即信息不足以建立因果关系），并提示LLMs评估潜在原因的有效性。

Result: 所有评估的LLMs都系统性地推断出不合理的因果关系，表现出对因果幻觉的强烈倾向。

Conclusion: 研究结果支持LLMs可能只是复制因果语言而缺乏真正理解的假设，并对在需要准确因果推理以做出明智决策的领域使用LLMs提出了担忧。

Abstract: Causal learning is the cognitive process of developing the capability of
making causal inferences based on available information, often guided by
normative principles. This process is prone to errors and biases, such as the
illusion of causality, in which people perceive a causal relationship between
two variables despite lacking supporting evidence. This cognitive bias has been
proposed to underlie many societal problems, including social prejudice,
stereotype formation, misinformation, and superstitious thinking. In this work,
we examine whether large language models are prone to developing causal
illusions when faced with a classic cognitive science paradigm: the contingency
judgment task. To investigate this, we constructed a dataset of 1,000 null
contingency scenarios (in which the available information is not sufficient to
establish a causal relationship between variables) within medical contexts and
prompted LLMs to evaluate the effectiveness of potential causes. Our findings
show that all evaluated models systematically inferred unwarranted causal
relationships, revealing a strong susceptibility to the illusion of causality.
While there is ongoing debate about whether LLMs genuinely understand causality
or merely reproduce causal language without true comprehension, our findings
support the latter hypothesis and raise concerns about the use of language
models in domains where accurate causal reasoning is essential for informed
decision-making.

</details>


### [117] [GammaZero: Learning To Guide POMDP Belief Space Search With Graph Representations](https://arxiv.org/abs/2510.14035)
*Rajesh Mangannavar,Prasad Tadepalli*

Main category: cs.AI

TL;DR: GammaZero是一个基于动作中心图表示的框架，通过图神经网络指导POMDP规划，实现了对训练时未见的大规模问题的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有POMDP规划方法需要领域特定架构且难以扩展，GammaZero旨在通过统一的图基信念表示解决此问题，以实现跨问题规模的泛化。

Method: 将信念状态系统转换为动作中心图，并使用图神经网络结合解码器架构，从专家演示中学习值函数和策略。然后将这些学习到的启发式方法应用于蒙特卡洛树搜索，以指导更大问题的规划。

Result: 在相同规模问题上与BetaZero表现相当。独特之处在于能对训练时未见的2-4倍大问题进行零样本泛化，并能保持解决方案质量，同时降低搜索需求。

Conclusion: GammaZero通过其动作中心图表示和GNN方法，在POMDP规划中实现了卓越的泛化能力，尤其是在零样本迁移到更大规模问题上表现出色，且能有效减少搜索成本。

Abstract: We introduce an action-centric graph representation framework for learning to
guide planning in Partially Observable Markov Decision Processes (POMDPs).
Unlike existing approaches that require domain-specific neural architectures
and struggle with scalability, GammaZero leverages a unified graph-based belief
representation that enables generalization across problem sizes within a
domain. Our key insight is that belief states can be systematically transformed
into action-centric graphs where structural patterns learned on small problems
transfer to larger instances. We employ a graph neural network with a decoder
architecture to learn value functions and policies from expert demonstrations
on computationally tractable problems, then apply these learned heuristics to
guide Monte Carlo tree search on larger problems. Experimental results on
standard POMDP benchmarks demonstrate that GammaZero achieves comparable
performance to BetaZero when trained and tested on the same-sized problems,
while uniquely enabling zero-shot generalization to problems 2-4 times larger
than those seen during training, maintaining solution quality with reduced
search requirements.

</details>


### [118] [Position: Require Frontier AI Labs To Release Small "Analog" Models](https://arxiv.org/abs/2510.14053)
*Shriyash Upadhyay,Chaithanya Bandi,Narmeen Oozeer,Philip Quirke*

Main category: cs.AI

TL;DR: 本文提出一项政策，要求大型AI实验室发布小型、开源的类比模型（从其专有模型蒸馏而来），以促进广泛的AI安全验证、可解释性研究和透明度，从而在不披露完整模型的情况下降低监管成本并加速创新。


<details>
  <summary>Details</summary>
Motivation: 当前的AI监管提案因安全与创新的权衡以及成本问题而常被搁置。迫切需要一种既能确保AI安全又能积极促进创新的替代监管方法。

Method: 提议的解决方案是一项政策强制令：要求大型AI实验室发布小型、可公开访问的类比模型。这些模型是其最大专有模型的缩减版，以类似方式训练并从中蒸馏而来，作为公共代理。

Result: 类比模型能够作为公共代理，促进更广泛地参与安全验证、可解释性研究和算法透明度，而无需实验室披露其全尺寸模型。已有研究表明，在这些小型模型上开发的安全和可解释性方法可以有效地泛化到前沿系统。此政策能显著减轻监管负担，加速安全进步，且额外成本最低。

Conclusion: 该政策不仅能为公共利益做出重大贡献，更重要的是，它揭示了一个更广泛的原则：通过加深对模型的理解（如借助类比模型），可以缓解安全与创新的权衡，从而实现两者兼得。

Abstract: Recent proposals for regulating frontier AI models have sparked concerns
about the cost of safety regulation, and most such regulations have been
shelved due to the safety-innovation tradeoff. This paper argues for an
alternative regulatory approach that ensures AI safety while actively promoting
innovation: mandating that large AI laboratories release small, openly
accessible analog models (scaled-down versions) trained similarly to and
distilled from their largest proprietary models.
  Analog models serve as public proxies, allowing broad participation in safety
verification, interpretability research, and algorithmic transparency without
forcing labs to disclose their full-scale models. Recent research demonstrates
that safety and interpretability methods developed using these smaller models
generalize effectively to frontier-scale systems. By enabling the wider
research community to directly investigate and innovate upon accessible
analogs, our policy substantially reduces the regulatory burden and accelerates
safety advancements.
  This mandate promises minimal additional costs, leveraging reusable resources
like data and infrastructure, while significantly contributing to the public
good. Our hope is not only that this policy be adopted, but that it illustrates
a broader principle supporting fundamental research in machine learning: deeper
understanding of models relaxes the safety-innovation tradeoff and lets us have
more of both.

</details>


### [119] [Generating Fair Consensus Statements with Social Choice on Token-Level MDPs](https://arxiv.org/abs/2510.14106)
*Carter Blair,Kate Larson*

Main category: cs.AI

TL;DR: 该研究提出一种基于多目标MDP和社会选择理论的框架，为大型语言模型生成共识声明提供可证明的公平性保证，并通过两种方法（ex-ante核心和平均主义福利最大化）显著提高了共识声明与各方偏好的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的共识声明生成框架在聚合多样化自由形式意见时，缺乏提供可证明公平性保证所需的内在结构。

Method: 将任务建模为多目标、令牌级的马尔可夫决策过程（MDP），其中每个目标对应一个代理的偏好。通过代理的个性化语言模型策略推导出令牌级奖励。提出两种基于社会选择理论的方法：1) 一种随机生成策略，保证在ex-ante核心中，最大化比例公平性；2) 对于生成单一声明，在MDP框架内使用搜索算法最大化平均主义福利。

Result: 实验结果表明，由平均主义目标引导的搜索生成的共识声明，在最差情况下的代理对齐（worst-case agent alignment）方面，优于包括Habermas Machine在内的基线方法。

Conclusion: 将共识声明生成建模为MDP，并结合社会选择理论，能够提供更强的公平性保证，并生成与多样化代理偏好更一致的共识声明。

Abstract: Current frameworks for consensus statement generation with large language
models lack the inherent structure needed to provide provable fairness
guarantees when aggregating diverse free-form opinions. We model the task as a
multi-objective, token-level Markov Decision Process (MDP), where each
objective corresponds to an agent's preference. Token-level rewards for each
agent are derived from their policy (e.g., a personalized language model). This
approach utilizes the finding that such policies implicitly define optimal
Q-functions, providing a principled way to quantify rewards at each generation
step without a value function (Rafailov et al., 2024). This MDP formulation
creates a formal structure amenable to analysis using principles from social
choice theory. We propose two approaches grounded in social choice theory.
First, we propose a stochastic generation policy guaranteed to be in the
ex-ante core, extending core stability concepts from voting theory to text
generation. This policy is derived from an underlying distribution over
complete statements that maximizes proportional fairness (Nash Welfare).
Second, for generating a single statement, we target the maximization of
egalitarian welfare using search algorithms within the MDP framework.
Empirically, experiments using language models to instantiate agent policies
show that search guided by the egalitarian objective generates consensus
statements with improved worst-case agent alignment compared to baseline
methods, including the Habermas Machine (Tessler et al., 2024).

</details>


### [120] [STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management](https://arxiv.org/abs/2510.14112)
*Huiliang Zhang,Di Wu,Arnaud Zinflou,Benoit Boulet*

Main category: cs.AI

TL;DR: 本文提出STEMS框架，通过整合时空图学习和安全约束多智能体强化学习，有效解决了多建筑能源管理中时空信息利用不足、安全保障缺乏及系统复杂性等挑战，显著降低了成本和排放，同时大幅提升了安全性并保持舒适度。


<details>
  <summary>Details</summary>
Motivation: 建筑能源管理对于实现碳减排、提升居住舒适度和降低能耗至关重要。当前多建筑能源系统在利用时空依赖性同时确保运行安全方面面临关键挑战，具体体现在：1) 时空信息利用不足；2) 缺乏严格的安全保障；3) 系统复杂性高。

Method: 论文提出了时空增强型安全多智能体协调（STEMS）框架，这是一个新颖的、受安全约束的多智能体强化学习框架。STEMS集成两大核心组件：1) 基于GCN-Transformer融合架构的时空图表示学习框架，用于捕获建筑间关系和时间模式；2) 结合控制障碍函数（CBF）的安全约束多智能体强化学习算法，提供数学层面的安全保障。

Result: 在真实建筑数据集上的实验表明，STEMS性能优于现有方法：实现了21%的成本降低、18%的排放减少，并将安全违规率从35.1%显著降低至5.6%，同时保持了最佳舒适度（不适比例仅0.13）。此外，该框架在极端天气条件下表现出强大的鲁棒性，并适用于不同类型的建筑。

Conclusion: STEMS框架为协调多建筑能源管理提供了一个有效且安全的解决方案，成功解决了时空依赖性利用、运行安全保障和系统复杂性等核心挑战，并在降低成本、减少排放、提升安全性和维持舒适度方面取得了显著成果。

Abstract: Building energy management is essential for achieving carbon reduction goals,
improving occupant comfort, and reducing energy costs. Coordinated building
energy management faces critical challenges in exploiting spatial-temporal
dependencies while ensuring operational safety across multi-building systems.
Current multi-building energy systems face three key challenges: insufficient
spatial-temporal information exploitation, lack of rigorous safety guarantees,
and system complexity. This paper proposes Spatial-Temporal Enhanced Safe
Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent
reinforcement learning framework for coordinated building energy management.
STEMS integrates two core components: (1) a spatial-temporal graph
representation learning framework using a GCN-Transformer fusion architecture
to capture inter-building relationships and temporal patterns, and (2) a
safety-constrained multi-agent RL algorithm incorporating Control Barrier
Functions to provide mathematical safety guarantees. Extensive experiments on
real-world building datasets demonstrate STEMS's superior performance over
existing methods, showing that STEMS achieves 21% cost reduction, 18% emission
reduction, and dramatically reduces safety violations from 35.1% to 5.6% while
maintaining optimal comfort with only 0.13 discomfort proportion. The framework
also demonstrates strong robustness during extreme weather conditions and
maintains effectiveness across different building types.

</details>


### [121] [Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems](https://arxiv.org/abs/2510.14133)
*Edoardo Allegrini,Ananth Shreekumar,Z. Berkay Celik*

Main category: cs.AI

TL;DR: 针对代理式AI系统通信协议碎片化问题，本文提出了一个统一的建模框架（包含宿主代理和任务生命周期模型），并定义了基于时序逻辑的正式属性，以实现对这些系统行为的系统分析和验证，确保其正确性、可靠性和健壮性。


<details>
  <summary>Details</summary>
Motivation: 代理式AI系统在复杂任务中应用广泛，但现有代理间通信协议碎片化且独立分析，导致语义鸿沟，阻碍了系统属性的严格分析，并引入了架构错位和协调漏洞等风险，对系统安全、保障和功能构成挑战。

Method: 引入了一个由宿主代理模型（形式化了与用户交互、任务分解和编排的顶层实体）和任务生命周期模型（详细描述了子任务从创建到完成的状态和转换）组成的建模框架。在此基础上，定义了17个宿主代理属性和14个任务生命周期属性（归类为活态性、安全性、完整性和公平性），并使用时序逻辑表达，以实现系统行为的形式化验证。

Result: 该框架为多AI代理系统的行为提供了一个统一的语义理解。通过形式化验证，能够检测协调边缘情况，预防死锁和安全漏洞，从而增强系统的正确性、可靠性和鲁棒性。

Conclusion: 本研究提出了首个经过严格论证、领域无关的框架，用于系统地分析、设计和部署正确、可靠且健壮的代理式AI系统。

Abstract: Agentic AI systems, which leverage multiple autonomous agents and Large
Language Models (LLMs), are increasingly used to address complex, multi-step
tasks. The safety, security, and functionality of these systems are critical,
especially in high-stakes applications. However, the current ecosystem of
inter-agent communication is fragmented, with protocols such as the Model
Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol
for coordination being analyzed in isolation. This fragmentation creates a
semantic gap that prevents the rigorous analysis of system properties and
introduces risks such as architectural misalignment and exploitable
coordination issues. To address these challenges, we introduce a modeling
framework for agentic AI systems composed of two foundational models. The
first, the host agent model, formalizes the top-level entity that interacts
with the user, decomposes tasks, and orchestrates their execution by leveraging
external agents and tools. The second, the task lifecycle model, details the
states and transitions of individual sub-tasks from creation to completion,
providing a fine-grained view of task management and error handling. Together,
these models provide a unified semantic framework for reasoning about the
behavior of multi-AI agent systems. Grounded in this framework, we define 17
properties for the host agent and 14 for the task lifecycle, categorized into
liveness, safety, completeness, and fairness. Expressed in temporal logic,
these properties enable formal verification of system behavior, detection of
coordination edge cases, and prevention of deadlocks and security
vulnerabilities. Through this effort, we introduce the first rigorously
grounded, domain-agnostic framework for the systematic analysis, design, and
deployment of correct, reliable, and robust agentic AI systems.

</details>


### [122] [A Multimodal Approach to Heritage Preservation in the Context of Climate Change](https://arxiv.org/abs/2510.14136)
*David Roqui,Adèle Cormier,nistor Grozavu,Ann Bourges*

Main category: cs.AI

TL;DR: 为解决文物遗产地气候变化导致的退化监测问题，本研究提出一种轻量级多模态架构，融合传感器与视觉数据，通过简化编码器和自适应Barlow Twins损失，在小样本数据集上显著提升了退化严重程度预测的准确率。


<details>
  <summary>Details</summary>
Motivation: 文物遗产地正因气候变化加速退化，而传统的单模态监测（如单独的视觉检查或环境传感器）无法捕捉环境压力源与材料劣化之间复杂的相互作用，因此需要一种更全面、能融合多种数据的方法来有效监测和理解退化过程。

Method: 本研究提出了一种轻量级多模态架构，用于融合传感器数据（温度、湿度）和视觉图像，以预测遗产地的退化程度。该方法基于PerceiverIO，并引入两项关键创新：1）使用简化的编码器（64D潜在空间）以避免在小数据集（37个训练样本）上过拟合；2）采用自适应Barlow Twins损失，以鼓励不同模态之间的互补性而非冗余。

Result: 该模型在斯特拉斯堡大教堂数据集上取得了76.9%的准确率，比标准多模态架构（如VisualBERT、Transformer）提高了43%，比原始PerceiverIO提高了25%。消融研究显示，仅使用传感器数据准确率为61.5%，仅使用图像数据为46.2%，证实了多模态融合的协同效应。系统性的超参数研究发现，适度的相关性目标（τ=0.3）能平衡对齐和互补性，达到69.2%的准确率。

Conclusion: 本研究证明，在数据稀缺的文物监测场景中，架构的简洁性结合对比正则化，能有效地实现多模态学习。这为开发基于人工智能的文物保护决策支持系统奠定了基础。

Abstract: Cultural heritage sites face accelerating degradation due to climate change,
yet tradi- tional monitoring relies on unimodal analysis (visual inspection or
environmental sen- sors alone) that fails to capture the complex interplay
between environmental stres- sors and material deterioration. We propose a
lightweight multimodal architecture that fuses sensor data (temperature,
humidity) with visual imagery to predict degradation severity at heritage
sites. Our approach adapts PerceiverIO with two key innovations: (1) simplified
encoders (64D latent space) that prevent overfitting on small datasets (n=37
training samples), and (2) Adaptive Barlow Twins loss that encourages modality
complementarity rather than redundancy. On data from Strasbourg Cathedral, our
model achieves 76.9% accu- racy, a 43% improvement over standard multimodal
architectures (VisualBERT, Trans- former) and 25% over vanilla PerceiverIO.
Ablation studies reveal that sensor-only achieves 61.5% while image-only
reaches 46.2%, confirming successful multimodal synergy. A systematic
hyperparameter study identifies an optimal moderate correlation target ({\tau}
=0.3) that balances align- ment and complementarity, achieving 69.2% accuracy
compared to other {\tau} values ({\tau} =0.1/0.5/0.7: 53.8%, {\tau} =0.9:
61.5%). This work demonstrates that architectural sim- plicity combined with
contrastive regularization enables effective multimodal learning in data-scarce
heritage monitoring contexts, providing a foundation for AI-driven con-
servation decision support systems.

</details>


### [123] [CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization](https://arxiv.org/abs/2510.14150)
*Henrique Assumpção,Diego Ferreira,Leandro Campos,Fabricio Murai*

Main category: cs.AI

TL;DR: CodeEvolve是一个开源的进化式编程代理，它将大型语言模型（LLMs）与遗传算法相结合，旨在解决复杂的计算问题，并在数学基准测试中超越了Google DeepMind的AlphaEvolve。


<details>
  <summary>Details</summary>
Motivation: 旨在通过整合强大的进化概念与LLMs来解决复杂的计算问题，并在此基础上推动广义科学发现。

Method: 引入了CodeEvolve框架，其核心方法包括：采用基于岛屿的遗传算法以保持种群多样性和提高吞吐量；引入新颖的基于灵感的交叉机制，利用LLM的上下文窗口组合成功解决方案的特征；以及实施元提示（meta-prompting）策略以动态探索解决方案空间。

Result: 通过在Google DeepMind闭源AlphaEvolve所用的数学基准子集上进行严格评估，结果表明CodeEvolve在多个具有挑战性的问题上超越了AlphaEvolve的性能。

Conclusion: CodeEvolve提供了一个将LLMs与遗传算法有效结合的开源框架，为解决复杂计算问题提供了新方法，并在性能上优于现有基准。该框架的发布旨在促进协作并加速领域进展。

Abstract: In this work, we introduce CodeEvolve, an open-source evolutionary coding
agent that unites Large Language Models (LLMs) with genetic algorithms to solve
complex computational problems. Our framework adapts powerful evolutionary
concepts to the LLM domain, building upon recent methods for generalized
scientific discovery. CodeEvolve employs an island-based genetic algorithm to
maintain population diversity and increase throughput, introduces a novel
inspiration-based crossover mechanism that leverages the LLMs context window to
combine features from successful solutions, and implements meta-prompting
strategies for dynamic exploration of the solution space. We conduct a rigorous
evaluation of CodeEvolve on a subset of the mathematical benchmarks used to
evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that
our method surpasses AlphaEvolve's performance on several challenging problems.
To foster collaboration and accelerate progress, we release our complete
framework as an open-source repository.

</details>


### [124] [Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola](https://arxiv.org/abs/2510.14154)
*Tian Liu,Alex Cann,Ian Colbert,Mehdi Saeedi*

Main category: cs.AI

TL;DR: 本文探讨了RL在商业游戏AI中应用缓慢的问题，提出并演示了将强化学习与行为树（BTs）结合的方法，以提高其在复杂游戏环境中的实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习研究进展迅速，但在商业视频游戏中的应用却进展缓慢。游戏AI社区在使用RL驱动的NPC时面临诸多挑战，且RL与行为树结合的建议虽有，但实际采用率很低，其可行性需进一步探索。

Method: 通过使用AMD Schola（一个用于在Unreal Engine中训练RL智能体的插件），在受商业游戏“The Last of Us”启发的复杂3D环境中，创建了多任务NPC。本文提供了RL模型与行为树联合训练的详细方法，以展示这种结合方式的有效性。

Result: 成功地证明了RL与行为树结合方法的可行性，并在复杂游戏环境中训练出了能够执行多种技能的多任务NPC。

Conclusion: RL与行为树的交叉融合是解决RL在商业游戏AI中实际应用挑战的关键途径，本研究通过实践演示验证了其有效性，并提供了具体的实施方案。

Abstract: While the rapid advancements in the reinforcement learning (RL) research
community have been remarkable, the adoption in commercial video games remains
slow. In this paper, we outline common challenges the Game AI community faces
when using RL-driven NPCs in practice, and highlight the intersection of RL
with traditional behavior trees (BTs) as a crucial juncture to be explored
further. Although the BT+RL intersection has been suggested in several research
papers, its adoption is rare. We demonstrate the viability of this approach
using AMD Schola -- a plugin for training RL agents in Unreal Engine -- by
creating multi-task NPCs in a complex 3D environment inspired by the commercial
video game ``The Last of Us". We provide detailed methodologies for jointly
training RL models with BTs while showcasing various skills.

</details>


### [125] [JEDA: Query-Free Clinical Order Search from Ambient Dialogues](https://arxiv.org/abs/2510.14169)
*Praphul Singh,Corey Barrett,Sumana Srivasta,Amitabh Saikia,Irfan Bulu,Sri Gadde,Krishnaram Kenthapadi*

Main category: cs.AI

TL;DR: JEDA是一个快速、可解释且无需LLM的检索层，能将临床对话中的环境上下文实时转换为可执行的临床医嘱。


<details>
  <summary>Details</summary>
Motivation: 现有系统依赖LLM重写，导致延迟、不稳定和不透明，阻碍了临床医嘱的实时下达。临床对话中包含显式指令和隐式推理，需要更直接、高效的方法来识别医嘱。

Method: 提出了JEDA (Joint Embedding for Direct and Ambient clinical orders)，一个领域初始化的双编码器。它直接检索规范医嘱，并在无查询模式下通过编码短滚动窗口的对话来触发检索。JEDA基于PubMedBERT初始化，采用去重安全对比学习目标进行微调，并利用受限的LLM指导训练，以对齐异构意图表达。

Result: JEDA在实际部署中取得了显著提升，大幅优于其基础编码器和最近的开放嵌入器。其无查询模式对噪音具有弹性，通过关注短时间窗口而非单一话语，降低了对语塞和ASR错误的敏感性。

Conclusion: JEDA提供了一个快速、可解释且无需LLM的检索层，能够实时地将环境上下文与可执行的临床医嘱联系起来。

Abstract: Clinical conversations mix explicit directives (order a chest X-ray) with
implicit reasoning (the cough worsened overnight, we should check for
pneumonia). Many systems rely on LLM rewriting, adding latency, instability,
and opacity that hinder real-time ordering. We present JEDA (Joint Embedding
for Direct and Ambient clinical orders), a domain-initialized bi-encoder that
retrieves canonical orders directly and, in a query-free mode, encodes a short
rolling window of ambient dialogue to trigger retrieval. Initialized from
PubMedBERT and fine-tuned with a duplicate-safe contrastive objective, JEDA
aligns heterogeneous expressions of intent to shared order concepts. Training
uses constrained LLM guidance to tie each signed order to complementary
formulations (command only, context only, command+context, context+reasoning),
producing clearer inter-order separation, tighter query extendash order
coupling, and stronger generalization. The query-free mode is noise-resilient,
reducing sensitivity to disfluencies and ASR errors by conditioning on a short
window rather than a single utterance. Deployed in practice, JEDA yields large
gains and substantially outperforms its base encoder and recent open embedders
(Linq Embed Mistral, SFR Embedding, GTE Qwen, BGE large, Embedding Gemma). The
result is a fast, interpretable, LLM-free retrieval layer that links ambient
context to actionable clinical orders in real time.

</details>


### [126] [ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning](https://arxiv.org/abs/2510.14176)
*Roger Creus Castanyer,Faisal Mohamed,Pablo Samuel Castro,Cyrus Neary,Glen Berseth*

Main category: cs.AI

TL;DR: ARM-FM是一个利用基础模型自动生成奖励机器的框架，旨在通过自然语言解决强化学习中的奖励函数设计难题，并实现任务分解和零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法对奖励函数规范高度敏感，这是限制其广泛应用的核心挑战。需要一种自动、组合式的奖励设计方法。

Method: 本文提出了ARM-FM框架，利用基础模型(FMs)的高层推理能力，通过FMs从自然语言规范自动生成奖励机器(RMs)。同时，将语言嵌入与每个RM自动机状态关联，以实现任务间的泛化。

Result: 实验证据表明，ARM-FM在多样化且具有挑战性的环境中有效，并展现了零样本泛化的能力。

Conclusion: ARM-FM通过基础模型和奖励机器，实现了强化学习中奖励函数规范的自动化和组合式设计，有效解决了奖励敏感性问题，并通过语言嵌入实现了任务泛化。

Abstract: Reinforcement learning (RL) algorithms are highly sensitive to reward
function specification, which remains a central challenge limiting their broad
applicability. We present ARM-FM: Automated Reward Machines via Foundation
Models, a framework for automated, compositional reward design in RL that
leverages the high-level reasoning capabilities of foundation models (FMs).
Reward machines (RMs) -- an automata-based formalism for reward specification
-- are used as the mechanism for RL objective specification, and are
automatically constructed via the use of FMs. The structured formalism of RMs
yields effective task decompositions, while the use of FMs enables objective
specifications in natural language. Concretely, we (i) use FMs to automatically
generate RMs from natural language specifications; (ii) associate language
embeddings with each RM automata-state to enable generalization across tasks;
and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse
suite of challenging environments, including evidence of zero-shot
generalization.

</details>


### [127] [Implementation of AI in Precision Medicine](https://arxiv.org/abs/2510.14194)
*Göktuğ Bender,Samer Faraj,Anand Bhardwaj*

Main category: cs.AI

TL;DR: 本文综述了2019-2024年AI在精准医疗中实施的文献，识别了主要障碍和促进因素，并提出未来方向，以支持其可信赖和可持续的临床应用。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在精准医疗中整合和解释多模态数据方面日益重要，但其在临床环境中的实际应用仍然有限。

Method: 对2019-2024年关于AI在精准医疗中实施的文献进行了范围界定综述（scoping review），并采用基于生态系统的框架进行分析。

Result: 识别了数据质量、临床可靠性、工作流程整合和治理等方面的关键实施障碍和促进因素，并突出了影响实际转化的相互依赖关系。

Conclusion: 提出了支持AI在精准医疗中可信赖和可持续实施的未来方向和建议。

Abstract: Artificial intelligence (AI) has become increasingly central to precision
medicine by enabling the integration and interpretation of multimodal data, yet
implementation in clinical settings remains limited. This paper provides a
scoping review of literature from 2019-2024 on the implementation of AI in
precision medicine, identifying key barriers and enablers across data quality,
clinical reliability, workflow integration, and governance. Through an
ecosystem-based framework, we highlight the interdependent relationships
shaping real-world translation and propose future directions to support
trustworthy and sustainable implementation.

</details>


### [128] [Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks](https://arxiv.org/abs/2510.14207)
*Trilok Padhi,Pinxian Lu,Abdulkadir Erol,Tanmay Sutar,Gauri Sharma,Mina Sonmez,Munmun De Choudhury,Ugur Kursuncu*

Main category: cs.AI

TL;DR: 本文提出一个多轮在线骚扰LLM代理的基准测试，发现通过越狱调优，无论是开源还是闭源模型，都能以极高成功率（95-99%）模拟人类骚扰行为，揭示了现有安全防护在多轮交互中的脆弱性。


<details>
  <summary>Details</summary>
Motivation: LLM代理在交互式Web应用中日益普及，但易受滥用和伤害。现有越狱研究多集中于单轮提示，而真实的骚扰通常是多轮交互。因此，需要研究LLM代理在多轮在线骚扰情境下的脆弱性。

Method: 本文提出了“在线骚扰智能体基准”（Online Harassment Agentic Benchmark），包含：(i) 合成的多轮骚扰对话数据集，(ii) 基于重复博弈论的多智能体（骚扰者、受害者）模拟，(iii) 攻击智能体记忆、规划和微调的三种越狱方法，以及 (iv) 混合方法评估框架。研究使用了Llama-3.1-8B-Instruct (开源) 和 Gemini-2.0-flash (闭源) 两种LLM。

Result: 越狱调优使骚扰几乎得到保证：Llama攻击成功率从57.25-64.19%升至95.78-96.89%；Gemini从98.46%升至99.33%，拒绝率降至1-2%。最普遍的有害行为是侮辱（Insult）和煽动（Flaming），攻击成功率显著高于敏感类别（如性或种族骚扰），表明防护较弱。被攻击的代理能再现人类攻击特征，如规划下的马基雅维利/精神病态模式和记忆下的自恋倾向。闭源和开源模型显示出不同的多轮升级轨迹，其中闭源模型表现出显著的脆弱性。

Conclusion: 多轮、基于理论的攻击不仅成功率高，而且能模仿人类骚扰动态。这强调了开发健壮安全防护的必要性，以确保在线平台的安全和责任。

Abstract: Large Language Model (LLM) agents are powering a growing share of interactive
web applications, yet remain vulnerable to misuse and harm. Prior jailbreak
research has largely focused on single-turn prompts, whereas real harassment
often unfolds over multi-turn interactions. In this work, we present the Online
Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn
harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim)
simulation informed by repeated game theory, (iii) three jailbreak methods
attacking agents across memory, planning, and fine-tuning, and (iv) a
mixed-methods evaluation framework. We utilize two prominent LLMs,
LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our
results show that jailbreak tuning makes harassment nearly guaranteed with an
attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama,
and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal
rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with
84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs.
31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive
categories such as sexual or racial harassment. Qualitative evaluation further
reveals that attacked agents reproduce human-like aggression profiles, such as
Machiavellian/psychopathic patterns under planning, and narcissistic tendencies
with memory. Counterintuitively, closed-source and open-source models exhibit
distinct escalation trajectories across turns, with closed-source models
showing significant vulnerability. Overall, our findings show that multi-turn
and theory-grounded attacks not only succeed at high rates but also mimic
human-like harassment dynamics, motivating the development of robust safety
guardrails to ultimately keep online platforms safe and responsible.

</details>


### [129] [LiveResearchBench: A Live Benchmark for User-Centric Deep Research in the Wild](https://arxiv.org/abs/2510.14240)
*Jiayu Wang,Yifei Ming,Riya Dulepet,Qinglin Chen,Austin Xu,Zixuan Ke,Frederic Sala,Aws Albarghouthi,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 本文提出LiveResearchBench和DeepEval，用于严谨评估能够进行深度研究的智能体系统，揭示了现有系统的优势、缺陷及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 深度研究是智能体系统的重要前沿，但现有基准缺乏用户中心、动态、明确且多方面的任务，无法提供严格的评估依据，从而阻碍了该能力的发展。

Method: 本文提出评估深度研究能力的四个核心原则；在此指导下，构建了LiveResearchBench，一个包含100个专家策划任务的基准，这些任务需大量、动态、实时的网络搜索与综合；同时，引入了DeepEval，一个综合评估套件，用于评估引用支撑的长篇报告，涵盖内容和报告层面的质量，并集成了四种互补的评估协议。最后，利用这两个工具对17个前沿的深度研究系统进行了全面评估。

Result: 通过评估，分析揭示了当前系统的优势、反复出现的故障模式，以及推动可靠、富有洞察力的深度研究所需的关键系统组件。

Conclusion: LiveResearchBench和DeepEval为深度研究智能体系统的系统性评估提供了严格的基础，有助于识别当前挑战并指导未来研究方向，以实现更可靠、有洞察力的深度研究能力。

Abstract: Deep research -- producing comprehensive, citation-grounded reports by
searching and synthesizing information from hundreds of live web sources --
marks an important frontier for agentic systems. To rigorously evaluate this
ability, four principles are essential: tasks should be (1) user-centric,
reflecting realistic information needs, (2) dynamic, requiring up-to-date
information beyond parametric knowledge, (3) unambiguous, ensuring consistent
interpretation across users, and (4) multi-faceted and search-intensive,
requiring search over numerous web sources and in-depth analysis. Existing
benchmarks fall short of these principles, often focusing on narrow domains or
posing ambiguous questions that hinder fair comparison. Guided by these
principles, we introduce LiveResearchBench, a benchmark of 100 expert-curated
tasks spanning daily life, enterprise, and academia, each requiring extensive,
dynamic, real-time web search and synthesis. Built with over 1,500 hours of
human labor, LiveResearchBench provides a rigorous basis for systematic
evaluation. To evaluate citation-grounded long-form reports, we introduce
DeepEval, a comprehensive suite covering both content- and report-level
quality, including coverage, presentation, citation accuracy and association,
consistency and depth of analysis. DeepEval integrates four complementary
evaluation protocols, each designed to ensure stable assessment and high
agreement with human judgments. Using LiveResearchBench and DeepEval, we
conduct a comprehensive evaluation of 17 frontier deep research systems,
including single-agent web search, single-agent deep research, and multi-agent
systems. Our analysis reveals current strengths, recurring failure modes, and
key system components needed to advance reliable, insightful deep research.

</details>


### [130] [Towards Agentic Self-Learning LLMs in Search Environment](https://arxiv.org/abs/2510.14253)
*Wangtao Sun,Xiang Cheng,Jialin Fan,Yao Xu,Xing Yu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.AI

TL;DR: 该研究探讨了LLM智能体如何在没有人工标注或预定义奖励的情况下通过自学习进行扩展，提出了Agentic Self-Learning (ASL) 框架。ASL通过协调提示生成器、策略模型和生成式奖励模型（GRM）实现闭环自学习，并证明了奖励来源和数据规模是关键因素。实验显示ASL性能持续提升，超越基线，并在零标注数据下表现出高效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究基于LLM的智能体能否在不依赖人工标注数据集或预定义规则奖励的情况下，通过自学习实现规模化扩展。

Method: 通过在搜索智能体环境中的受控实验，确定了可扩展智能体训练的两个关键决定因素：奖励信号的来源和智能体任务数据的规模。基于这些发现，提出了Agentic Self-Learning (ASL) 框架，这是一个完全闭环、多角色的强化学习框架，它在一个共享工具环境和LLM骨干中统一了任务生成、策略执行和评估。ASL协调提示生成器、策略模型和生成式奖励模型（GRM），形成一个“更难任务设置、更锐利验证、更强解决”的良性循环。研究还通过持续训练GRM以适应演进的数据分布，并注入少量真实验证数据来解决GRM验证瓶颈。

Result: ASL实现了稳定、逐轮的性能提升，并超越了停滞或退化的强RLVR基线（如Search-R1）。在零标注数据条件下，ASL仍能持续改进，显示出卓越的样本效率和鲁棒性。研究发现GRM的验证能力是主要瓶颈：如果GRM被冻结，会导致奖励欺骗并阻碍进展；在演进数据分布上持续训练GRM可以缓解此问题，而少量后期注入的真实验证数据能进一步提高性能上限。

Conclusion: 这项工作确立了奖励来源和数据规模是开放域智能体学习的关键杠杆，并证明了多角色共同演化对于构建可扩展、自改进智能体的有效性。

Abstract: We study whether self-learning can scale LLM-based agents without relying on
human-curated datasets or predefined rule-based rewards. Through controlled
experiments in a search-agent setting, we identify two key determinants of
scalable agent training: the source of reward signals and the scale of agent
task data. We find that rewards from a Generative Reward Model (GRM) outperform
rigid rule-based signals for open-domain learning, and that co-evolving the GRM
with the policy further boosts performance. Increasing the volume of agent task
data-even when synthetically generated-substantially enhances agentic
capabilities. Building on these insights, we propose \textbf{Agentic
Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning
framework that unifies task generation, policy execution, and evaluation within
a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator,
a Policy Model, and a Generative Reward Model to form a virtuous cycle of
harder task setting, sharper verification, and stronger solving. Empirically,
ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines
(e.g., Search-R1) that plateau or degrade, and continues improving under
zero-labeled-data conditions, indicating superior sample efficiency and
robustness. We further show that GRM verification capacity is the main
bottleneck: if frozen, it induces reward hacking and stalls progress; continual
GRM training on the evolving data distribution mitigates this, and a small
late-stage injection of real verification data raises the performance ceiling.
This work establishes reward source and data scale as critical levers for
open-domain agent learning and demonstrates the efficacy of multi-role
co-evolution for scalable, self-improving agents. The data and code of this
paper are released at
https://github.com/forangel2014/Towards-Agentic-Self-Learning

</details>


### [131] [MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning](https://arxiv.org/abs/2510.14265)
*Xukai Wang,Xuanbo Liu,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Bohan Zeng,Jinbo Hu,Hao Liang,Junbo Niu,Xuchen Li,Ruitao Wu,Ruichuan An,Yang Shi,Liu Liu,Xu-Yao Zhang,Qiang Liu,Zhouchen Lin,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: MorphoBench是一个自适应多学科基准，用于全面评估和调整大型模型推理能力的难度。


<details>
  <summary>Details</summary>
Motivation: 现有大型模型推理能力评估基准范围有限，且缺乏根据模型不断进化的推理能力调整难度的灵活性。

Method: 提出MorphoBench，通过收集多学科复杂推理问题（如奥赛级别），利用模型推理过程中的关键语句自适应修改问题分析难度，并使用模拟软件生成问题以动态调整基准难度。

Result: 已收集超过1300个测试问题，并根据o3和GPT-5等模型的推理能力迭代调整了MorphoBench的难度。

Conclusion: MorphoBench提高了模型推理评估的全面性和有效性，为提升大型模型的推理能力和科学稳健性提供了可靠指导。

Abstract: With the advancement of powerful large-scale reasoning models, effectively
evaluating the reasoning capabilities of these models has become increasingly
important. However, existing benchmarks designed to assess the reasoning
abilities of large models tend to be limited in scope and lack the flexibility
to adapt their difficulty according to the evolving reasoning capacities of the
models. To address this, we propose MorphoBench, a benchmark that incorporates
multidisciplinary questions to evaluate the reasoning capabilities of large
models and can adjust and update question difficulty based on the reasoning
abilities of advanced models. Specifically, we curate the benchmark by
selecting and collecting complex reasoning questions from existing benchmarks
and sources such as Olympiad-level competitions. Additionally, MorphoBench
adaptively modifies the analytical challenge of questions by leveraging key
statements generated during the model's reasoning process. Furthermore, it
includes questions generated using simulation software, enabling dynamic
adjustment of benchmark difficulty with minimal resource consumption. We have
gathered over 1,300 test questions and iteratively adjusted the difficulty of
MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5.
MorphoBench enhances the comprehensiveness and validity of model reasoning
evaluation, providing reliable guidance for improving both the reasoning
abilities and scientific robustness of large models. The code has been released
in https://github.com/OpenDCAI/MorphoBench.

</details>


### [132] [A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space](https://arxiv.org/abs/2510.14301)
*Bingjie Zhang,Yibo Yang,Renzhe,Dandan Guo,Jindong Gu,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: GuardSpace框架通过分解模型权重并引入零空间投影器，在微调大型语言模型时有效保持安全对齐，显著降低有害响应，同时提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在微调或低秩适应时，其预训练的安全行为容易受损，导致模型产生有害响应，安全对齐性脆弱。

Method: 提出GuardSpace框架，包含两个组件：1) **安全敏感子空间**：利用协方差预处理奇异值分解将预训练权重分解为安全相关（冻结）和安全无关（用于初始化低秩适配器）部分。2) **抗有害零空间**：构建一个零空间投影器，限制适配器更新改变有害提示上的安全输出，以保持模型原有的拒绝行为。

Result: GuardSpace在多种模型和任务上均优于现有方法。例如，在Llama-2-7B-Chat于GSM8K上微调时，GuardSpace将平均有害分数从14.4%降至3.6%，同时将准确率从26.0%提高到28.0%。

Conclusion: GuardSpace框架成功解决了大型语言模型在微调过程中安全对齐易受损的问题，能够有效保存安全行为并提升模型在下游任务中的性能。

Abstract: Large language models (LLMs) have achieved remarkable success in diverse
tasks, yet their safety alignment remains fragile during adaptation. Even when
fine-tuning on benign data or with low-rank adaptation, pre-trained safety
behaviors are easily degraded, leading to harmful responses in the fine-tuned
models. To address this challenge, we propose GuardSpace, a guardrail framework
for preserving safety alignment throughout fine-tuning, composed of two key
components: a safety-sensitive subspace and a harmful-resistant null space.
First, we explicitly decompose pre-trained weights into safety-relevant and
safety-irrelevant components using covariance-preconditioned singular value
decomposition, and initialize low-rank adapters from the safety-irrelevant
ones, while freezing safety-relevant components to preserve their associated
safety mechanism. Second, we construct a null space projector that restricts
adapter updates from altering safe outputs on harmful prompts, thereby
maintaining the original refusal behavior. Experiments with various pre-trained
models on multiple downstream tasks demonstrate that GuardSpace achieves
superior performance over existing methods. Notably, for Llama-2-7B-Chat
fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT,
reducing the average harmful score from 14.4% to 3.6%, while improving the
accuracy from from 26.0% to 28.0%.

</details>


### [133] [Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies](https://arxiv.org/abs/2510.14312)
*Mason Nakamura,Abhinav Kumar,Saaduddin Mahmud,Sahar Abdelnabi,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

TL;DR: 本文提出了Terrarium框架，一个基于黑板设计的模块化测试平台，用于细致研究大型语言模型驱动的多智能体系统中的安全、隐私和安全风险，并展示了关键攻击向量。


<details>
  <summary>Details</summary>
Motivation: 由大型语言模型驱动的多智能体系统虽然能自动化复杂任务，但也引入了新的风险，如未对齐、恶意攻击（代理被攻陷或用户数据被窃取），因此需要一个框架来深入研究这些安全、隐私和安全问题。

Method: 提出Terrarium框架，通过改造多智能体系统早期的黑板设计，创建一个模块化、可配置的协作测试平台。该方法识别了未对齐、恶意代理、通信受损和数据投毒等关键攻击向量，并实现了三个协作场景和四种代表性攻击来展示其灵活性。

Result: Terrarium框架通过实现多种协作场景和代表性攻击，展示了其在研究多智能体系统安全、隐私和安全方面的灵活性。它提供了快速原型设计、评估和迭代防御机制及设计的工具。

Conclusion: Terrarium框架旨在加速可信多智能体系统的发展，通过提供一个用于研究和缓解安全、隐私和安全风险的工具平台，从而帮助构建更健壮和安全的系统。

Abstract: A multi-agent system (MAS) powered by large language models (LLMs) can
automate tedious user tasks such as meeting scheduling that requires
inter-agent collaboration. LLMs enable nuanced protocols that account for
unstructured private data, user constraints, and preferences. However, this
design introduces new risks, including misalignment and attacks by malicious
parties that compromise agents or steal user data. In this paper, we propose
the Terrarium framework for fine-grained study on safety, privacy, and security
in LLM-based MAS. We repurpose the blackboard design, an early approach in
multi-agent systems, to create a modular, configurable testbed for multi-agent
collaboration. We identify key attack vectors such as misalignment, malicious
agents, compromised communication, and data poisoning. We implement three
collaborative MAS scenarios with four representative attacks to demonstrate the
framework's flexibility. By providing tools to rapidly prototype, evaluate, and
iterate on defenses and designs, Terrarium aims to accelerate progress toward
trustworthy multi-agent systems.

</details>


### [134] [Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction](https://arxiv.org/abs/2510.14319)
*Xu Shen,Qi Zhang,Song Wang,Zhen Tan,Xinyu Zhao,Laura Yao,Vaishnav Tadiparthi,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Kwonjoon Lee,Tianlong Chen*

Main category: cs.AI

TL;DR: MASC是一个元认知框架，为基于LLM的多智能体系统提供实时、无监督的步级错误检测和自校正，以有效缓解错误传播。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体系统（MAS）在协作解决问题方面表现出色，但容易受到级联错误的影响，单个错误步骤可能传播并破坏整个轨迹。

Method: MASC通过历史条件下的异常评分来实现错误检测，包含两个互补设计：1) 下一执行重建，根据查询和交互历史预测下一步嵌入以捕获因果一致性；2) 原型引导增强，学习正常步骤嵌入的原型先验以稳定稀疏上下文下的重建和异常评分。当检测到异常时，MASC会触发校正智能体在信息流向下游前修订输出。

Result: 在Who&When基准测试中，MASC在步级错误检测方面表现优于所有基线，AUC-ROC最高提升8.47%。将其集成到不同的MAS框架中时，也能持续实现端到端性能提升。

Conclusion: MASC通过元认知监控和目标校正，有效缓解了多智能体系统中的错误传播问题，且开销极小，提供了持续的性能改进。

Abstract: Large Language Model based multi-agent systems (MAS) excel at collaborative
problem solving but remain brittle to cascading errors: a single faulty step
can propagate across agents and disrupt the trajectory. In this paper, we
present MASC, a metacognitive framework that endows MAS with real-time,
unsupervised, step-level error detection and self-correction. MASC rethinks
detection as history-conditioned anomaly scoring via two complementary designs:
(1) Next-Execution Reconstruction, which predicts the embedding of the next
step from the query and interaction history to capture causal consistency, and
(2) Prototype-Guided Enhancement, which learns a prototype prior over
normal-step embeddings and uses it to stabilize reconstruction and anomaly
scoring under sparse context (e.g., early steps). When an anomaly step is
flagged, MASC triggers a correction agent to revise the acting agent's output
before information flows downstream. On the Who&When benchmark, MASC
consistently outperforms all baselines, improving step-level error detection by
up to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers
consistent end-to-end gains across architectures, confirming that our
metacognitive monitoring and targeted correction can mitigate error propagation
with minimal overhead.

</details>


### [135] [AI for Service: Proactive Assistance with AI Glasses](https://arxiv.org/abs/2510.14359)
*Zichen Wen,Yiyu Wang,Chenfei Liao,Boxue Yang,Junxian Li,Weifeng Liu,Haocong He,Bolong Feng,Xuyang Liu,Yuanhuiyi Lyu,Xu Zheng,Xuming Hu,Linfeng Zhang*

Main category: cs.AI

TL;DR: 论文提出“服务型AI”（AI4Service）新范式，旨在通过Alpha-Service框架和AI眼镜，实现能够预判用户需求并主动提供实时、个性化协助的智能助理，克服现有AI的被动响应局限。


<details>
  <summary>Details</summary>
Motivation: 现有AI服务多为被动响应用户明确指令。研究旨在将AI从被动工具转变为主动、适应性伴侣，使其能预判用户需求并适时主动采取行动，提供真正智能和有帮助的协助。

Method: 提出Alpha-Service统一框架，解决“何时介入”和“如何提供通用及个性化服务”两大核心挑战。该框架受冯诺依曼计算机架构启发，基于AI眼镜，由输入、中央处理、算术逻辑、存储和输出五个单元组成。初步通过部署在AI眼镜上的多智能体系统实现。

Result: 通过二十一点顾问、博物馆导游和购物搭配助理等案例研究，Alpha-Service展示了其无需明确提示即可无缝感知环境、推断用户意图并及时提供有用协助的能力。

Conclusion: Alpha-Service成功验证了AI4Service范式的可行性，能够将AI从被动响应转变为主动、实时和个性化的协助者，有效实现对用户需求的预判和介入。

Abstract: In an era where AI is evolving from a passive tool into an active and
adaptive companion, we introduce AI for Service (AI4Service), a new paradigm
that enables proactive and real-time assistance in daily life. Existing AI
services remain largely reactive, responding only to explicit user commands. We
argue that a truly intelligent and helpful assistant should be capable of
anticipating user needs and taking actions proactively when appropriate. To
realize this vision, we propose Alpha-Service, a unified framework that
addresses two fundamental challenges: Know When to intervene by detecting
service opportunities from egocentric video streams, and Know How to provide
both generalized and personalized services. Inspired by the von Neumann
computer architecture and based on AI glasses, Alpha-Service consists of five
key components: an Input Unit for perception, a Central Processing Unit for
task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit
for long-term personalization, and an Output Unit for natural human
interaction. As an initial exploration, we implement Alpha-Service through a
multi-agent system deployed on AI glasses. Case studies, including a real-time
Blackjack advisor, a museum tour guide, and a shopping fit assistant,
demonstrate its ability to seamlessly perceive the environment, infer user
intent, and provide timely and useful assistance without explicit prompts.

</details>


### [136] [Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?](https://arxiv.org/abs/2510.14387)
*Yijie Hu,Zihao Zhou,Kaizhu Huang,Xiaowei Huang,Qiufeng Wang*

Main category: cs.AI

TL;DR: 提出IP-Merging方法，通过识别和对齐参数空间，免调优地将数学LLM的推理能力高效迁移至多模态LLM（MLLM），显著提升MLLM的数学推理性能且不损害其他能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在数学推理方面取得了显著进展，但多模态LLM（MLLM）的数学推理能力仍滞后。现有模型合并方法忽视了MLLM与LLM之间参数空间对齐的差距，导致性能不佳。本研究旨在探索MLLM能否在免调优情况下直接吸收现成数学LLM的推理能力。

Method: 提出IP-Merging方法，该方法基于经验洞察（识别关键推理层和弥合参数空间差距），首先识别MLLM和数学LLM中与推理相关的参数，然后将其投影到MLLM的子空间以保持对齐，最后在该子空间中合并参数。此方法无需微调。

Result: 大量实验证明，IP-Merging方法能够直接从数学LLM增强MLLM的数学推理能力，同时不损害其其他功能。

Conclusion: IP-Merging提供了一种有效且免调优的解决方案，成功提升了多模态大语言模型的数学推理能力，弥合了其与单一模态大语言模型在此方面的差距，同时保持了模型的多模态功能。

Abstract: Math reasoning has been one crucial ability of large language models (LLMs),
where significant advancements have been achieved in recent years. However,
most efforts focus on LLMs by curating high-quality annotation data and
intricate training (or inference) paradigms, while the math reasoning
performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM
typically consists of an LLM and a vision block, we wonder: Can MLLMs directly
absorb math reasoning abilities from off-the-shelf math LLMs without tuning?
Recent model-merging approaches may offer insights into this question. However,
they overlook the alignment between the MLLM and LLM, where we find that there
is a large gap between their parameter spaces, resulting in lower performance.
Our empirical evidence reveals two key factors behind this issue: the
identification of crucial reasoning-associated layers in the model and the
mitigation of the gaps in parameter space. Based on the empirical insights, we
propose IP-Merging that first identifies the reasoning-associated parameters in
both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to
maintain the alignment, and finally merges parameters in this subspace.
IP-Merging is a tuning-free approach since parameters are directly adjusted.
Extensive experiments demonstrate that our IP-Merging method can enhance the
math reasoning ability of MLLMs directly from Math LLMs without compromising
their other capabilities.

</details>


### [137] [Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control](https://arxiv.org/abs/2510.14388)
*Zhe Wu,Hongjin Lu,Junliang Xing,Changhao Zhang,Yin Zhu,Yuhao Yang,Yuheng Jing,Kai Li,Kun Shao,Jianye Hao,Jun Wang,Yuanchun Shi*

Main category: cs.AI

TL;DR: Hi-Agent是一个分层视觉语言智能体，通过高低层模型联合优化及创新训练机制，实现了移动设备自主操作的SOTA。它解决了现有方法泛化差、缺乏结构化推理的问题，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有移动设备自主操作智能体依赖直接状态-动作映射，缺乏结构化推理和规划，导致对新任务或未知UI布局泛化能力差。

Method: 引入Hi-Agent：一个可训练的分层视觉语言智能体，包含高层推理模型和低层行动模型，两者联合优化。将多步决策重构为单步子目标序列，并提出前瞻优势函数，利用低层执行反馈指导高层优化，实现稳定、无批评器的联合训练，并解决路径爆炸问题。

Result: 在Android-in-the-Wild (AitW) 基准测试中，达到87.9%的任务成功率，创SOTA，显著优于现有方法。在ScreenSpot-v2上展现出有竞争力的零样本泛化能力。在AndroidWorld上有效扩展并适应高复杂度移动控制场景。

Conclusion: Hi-Agent通过其分层架构和高效训练方法，显著提升了移动设备控制智能体的性能、泛化能力和在复杂场景下的适应性，为该领域提供了新的SOTA解决方案。

Abstract: Building agents that autonomously operate mobile devices has attracted
increasing attention. While Vision-Language Models (VLMs) show promise, most
existing approaches rely on direct state-to-action mappings, which lack
structured reasoning and planning, and thus generalize poorly to novel tasks or
unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical
vision-language agent for mobile control, featuring a high-level reasoning
model and a low-level action model that are jointly optimized. For efficient
training, we reformulate multi-step decision-making as a sequence of
single-step subgoals and propose a foresight advantage function, which
leverages execution feedback from the low-level model to guide high-level
optimization. This design alleviates the path explosion issue encountered by
Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables
stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art
(SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark,
significantly outperforming prior methods across three paradigms: prompt-based
(AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement
learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot
generalization on the ScreenSpot-v2 benchmark. On the more challenging
AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones,
showing strong adaptability in high-complexity mobile control scenarios.

</details>


### [138] [IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning](https://arxiv.org/abs/2510.14406)
*Xikai Zhang,Bo Wang,Likang Xiao,Yongzhi Li,Quan Chen,Wenju Wu,Liu Liu*

Main category: cs.AI

TL;DR: 现有大型语言模型（LLMs）在复杂推理和规划方面表现不足，而多智能体系统（MAS）虽有改进但成本高昂。本文提出IMAGINE框架，将MAS的推理和规划能力整合到单一模型中，并通过简单端到端训练使其能力大幅提升，在TravelPlanner数据集上显著超越现有LLMs和MAS。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在复杂推理和规划任务中面临显著挑战，即使是先进模型也表现不佳（如GPT-4o在TravelPlanner上仅7%通过率）。多智能体系统（MAS）虽能改善集体推理，但其高推理成本、长响应延迟和端到端训练困难是主要瓶颈。

Method: 提出IMAGINE（Integrating Multi-Agent System into One Model）通用且可扩展框架。该框架将多智能体系统的推理和规划能力整合到一个单一、紧凑的模型中，并通过简单的端到端训练显著提升其性能。

Result: 使用Qwen3-8B-Instruct作为基础模型并采用IMAGINE方法训练后，模型在TravelPlanner基准测试中达到了82.7%的最终通过率，远超DeepSeek-R1-671B的40%（后者模型规模更大），同时也显著优于原始Qwen3-8B-Instruct的5.9%。

Conclusion: IMAGINE框架成功地将多智能体系统的结构化推理和规划能力集成到单一小型模型中，不仅有效解决了LLMs在复杂任务上的挑战，而且在性能上显著超越了大型LLMs和多智能体系统，同时保持了更高的效率和更小的模型尺寸。

Abstract: Although large language models (LLMs) have made significant strides across
various tasks, they still face significant challenges in complex reasoning and
planning. For example, even with carefully designed prompts and prior
information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on
the TravelPlanner dataset in the sole-planning mode. Similarly, even in the
thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass
Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent
Systems (MAS) can offer improved collective reasoning, they often suffer from
high reasoning costs due to multi-round internal interactions, long
per-response latency, and difficulties in end-to-end training. To address these
challenges, we propose a general and scalable framework called IMAGINE, short
for Integrating Multi-Agent System into One Model. This framework not only
integrates the reasoning and planning capabilities of MAS into a single,
compact model, but also significantly surpass the capabilities of the MAS
through a simple end-to-end training. Through this pipeline, a single
small-scale model is not only able to acquire the structured reasoning and
planning capabilities of a well-organized MAS but can also significantly
outperform it. Experimental results demonstrate that, when using
Qwen3-8B-Instruct as the base model and training it with our method, the model
achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding
the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.

</details>


### [139] [Eliminating Negative Occurrences of Derived Predicates from PDDL Axioms](https://arxiv.org/abs/2510.14412)
*Claudia Grundke,Gabriele Röger*

Main category: cs.AI

TL;DR: 本文分析了PDDL公理中负谓词使用的标准与实践差异。证明了两种公理变体表达能力等价于最小不动点逻辑，并提出了一种消除负派生谓词的转换方法。


<details>
  <summary>Details</summary>
Motivation: PDDL标准对公理体中负谓词的使用有严格限制，但学术界常采用更宽松的可分层公理。鉴于两种变体表达能力均等价于最小不动点逻辑，这表明负派生谓词是可消除的，因此需要研究相应的转换方法。

Method: 提出并展示了一种转换方法，用于消除公理体中负派生谓词的出现。

Result: PDDL标准下的受限公理与文献中常用的可分层公理在表达能力上均等价于最小不动点逻辑。负派生谓词可以被消除，本文提供了实现此消除的转换方法。

Conclusion: 通过所提出的转换方法，PDDL公理中负派生谓词的出现可以被消除，从而在保持与最小不动点逻辑等价表达能力的同时，弥合了标准与实践之间的差异。

Abstract: Axioms are a feature of the Planning Domain Definition Language PDDL that can
be considered as a generalization of database query languages such as Datalog.
The PDDL standard restricts negative occurrences of predicates in axiom bodies
to predicates that are directly set by actions and not derived by axioms. In
the literature, authors often deviate from this limitation and only require
that the set of axioms is stratifiable. Both variants can express exactly the
same queries as least fixed-point logic, indicating that negative occurrences
of derived predicates can be eliminated. We present the corresponding
transformation.

</details>


### [140] [Helmsman: Autonomous Synthesis of Federated Learning Systems via Multi-Agent Collaboration](https://arxiv.org/abs/2510.14512)
*Haoyuan Li,Mathias Funk,Aaqib Saeed*

Main category: cs.AI

TL;DR: 本文提出Helmsman多智能体系统，自动化联邦学习（FL）系统的端到端设计与生成，通过人机交互、代码生成和自主评估优化FL方案，并在AgentFL-Bench基准测试中表现优于传统手工方案。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）系统设计和部署面临巨大复杂性，需选择、组合和调整策略以应对数据异构性和系统约束等挑战，导致现有方案脆弱且定制化，成为关键瓶颈。

Method: 引入Helmsman多智能体系统，通过三个协作阶段自动化FL系统合成：1) 人机交互规划；2) 监督式智能体团队生成模块化代码；3) 沙盒模拟环境中自主评估和优化。同时，引入AgentFL-Bench新基准测试（包含16项任务）以严格评估系统生成能力。

Result: 通过广泛实验证明，Helmsman生成了与现有手工设计基线相当，且通常更优的解决方案。

Conclusion: 本研究在复杂去中心化AI系统的自动化工程方面取得了重要进展。

Abstract: Federated Learning (FL) offers a powerful paradigm for training models on
decentralized data, but its promise is often undermined by the immense
complexity of designing and deploying robust systems. The need to select,
combine, and tune strategies for multifaceted challenges like data
heterogeneity and system constraints has become a critical bottleneck,
resulting in brittle, bespoke solutions. To address this, we introduce
Helmsman, a novel multi-agent system that automates the end-to-end synthesis of
federated learning systems from high-level user specifications. It emulates a
principled research and development workflow through three collaborative
phases: (1) interactive human-in-the-loop planning to formulate a sound
research plan, (2) modular code generation by supervised agent teams, and (3) a
closed-loop of autonomous evaluation and refinement in a sandboxed simulation
environment. To facilitate rigorous evaluation, we also introduce
AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess
the system-level generation capabilities of agentic systems in FL. Extensive
experiments demonstrate that our approach generates solutions competitive with,
and often superior to, established hand-crafted baselines. Our work represents
a significant step towards the automated engineering of complex decentralized
AI systems.

</details>


### [141] [JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol](https://arxiv.org/abs/2510.14537)
*Emanuele Antonioni,Stefan Markovic,Anirudha Shankar,Jaime Bernardo,Lovro Markovic,Silvia Pareti,Benedetto Proietti*

Main category: cs.AI

TL;DR: JSPLIT通过构建分层分类法并根据用户提示选择最相关工具，有效解决了LLM代理在多工具环境中提示膨胀的问题，从而降低成本并提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统发展为需要与外部工具交互的代理，如使用MCP标准，提示中包含工具规范导致“提示膨胀”。这会增加token成本、延迟，并因选择不相关工具而降低任务成功率。

Method: 引入JSPLIT框架。该框架将MCP工具组织成一个分层的分类法，并根据用户的查询和分类结构，仅识别并包含最相关的工具，从而管理提示大小。

Result: JSPLIT显著减少了提示大小，同时并未显著损害代理的响应效率。随着可用工具数量的增加，JSPLIT甚至提高了工具选择的准确性，有效降低了成本，并改善了复杂代理环境中的任务成功率。

Conclusion: JSPLIT通过有效的工具管理策略，成功解决了多工具LLM代理中的提示膨胀问题，提高了系统效率和任务成功率。

Abstract: AI systems are continually evolving and advancing, and user expectations are
concurrently increasing, with a growing demand for interactions that go beyond
simple text-based interaction with Large Language Models (LLMs). Today's
applications often require LLMs to interact with external tools, marking a
shift toward more complex agentic systems. To support this, standards such as
the Model Context Protocol (MCP) have emerged, enabling agents to access tools
by including a specification of the capabilities of each tool within the
prompt. Although this approach expands what agents can do, it also introduces a
growing problem: prompt bloating. As the number of tools increases, the prompts
become longer, leading to high prompt token costs, increased latency, and
reduced task success resulting from the selection of tools irrelevant to the
prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework
designed to help agents manage prompt size more effectively when using large
sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and
uses the user's prompt to identify and include only the most relevant tools,
based on both the query and the taxonomy structure. In this paper, we describe
the design of the taxonomy, the tool selection algorithm, and the dataset used
to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt
size without significantly compromising the agent's ability to respond
effectively. As the number of available tools for the agent grows
substantially, JSPLIT even improves the tool selection accuracy of the agent,
effectively reducing costs while simultaneously improving task success in
high-complexity agent environments.

</details>


### [142] [Symbol Grounding in Neuro-Symbolic AI: A Gentle Introduction to Reasoning Shortcuts](https://arxiv.org/abs/2510.14538)
*Emanuele Marconato,Samuele Bortolotti,Emile van Krieken,Paolo Morettin,Elena Umili,Antonio Vergari,Efthymia Tsamoura,Andrea Passerini,Stefano Teso*

Main category: cs.AI

TL;DR: 该综述文章统一分析了神经符号AI (NeSy AI) 中推理捷径 (RSs) 的问题，包括其原因、后果、理论特征及应对策略，旨在促进可靠且值得信赖的AI发展。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI在构建可靠和值得信赖的AI方面潜力巨大，但当高级符号概念未受直接监督时，其预测可能受推理捷径 (RSs) 影响，导致概念错误、模型解释性下降、泛化能力受损及可靠性问题。RSs难以检测和预防，且现有文献分散，阻碍了研究人员理解和解决此问题。本综述旨在解决文献分散问题，提供统一视角。

Method: 本综述通过以下方式进行：对推理捷径 (RSs) 提供简要介绍；讨论其起因和后果；回顾并阐明现有关于此现象的理论特征；详细介绍处理RSs的方法，包括缓解和意识策略；分析这些策略的优缺点；将复杂材料以易于理解的形式呈现。

Result: 本综述提供了对推理捷径 (RSs) 的清晰介绍，阐明了其起因和后果，梳理并分析了现有的理论特征。同时，总结并评估了应对RSs的多种策略（缓解和意识策略），并指出了其优缺点。通过提供统一视角，降低了解决推理捷径问题的门槛。

Conclusion: 本综述通过加深对推理捷径的理解和应对，有望为开发更可靠的神经符号AI和更值得信赖的AI模型做出贡献。

Abstract: Neuro-symbolic (NeSy) AI aims to develop deep neural networks whose
predictions comply with prior knowledge encoding, e.g. safety or structural
constraints. As such, it represents one of the most promising avenues for
reliable and trustworthy AI. The core idea behind NeSy AI is to combine neural
and symbolic steps: neural networks are typically responsible for mapping
low-level inputs into high-level symbolic concepts, while symbolic reasoning
infers predictions compatible with the extracted concepts and the prior
knowledge. Despite their promise, it was recently shown that - whenever the
concepts are not supervised directly - NeSy models can be affected by Reasoning
Shortcuts (RSs). That is, they can achieve high label accuracy by grounding the
concepts incorrectly. RSs can compromise the interpretability of the model's
explanations, performance in out-of-distribution scenarios, and therefore
reliability. At the same time, RSs are difficult to detect and prevent unless
concept supervision is available, which is typically not the case. However, the
literature on RSs is scattered, making it difficult for researchers and
practitioners to understand and tackle this challenging problem. This overview
addresses this issue by providing a gentle introduction to RSs, discussing
their causes and consequences in intuitive terms. It also reviews and
elucidates existing theoretical characterizations of this phenomenon. Finally,
it details methods for dealing with RSs, including mitigation and awareness
strategies, and maps their benefits and limitations. By reformulating advanced
material in a digestible form, this overview aims to provide a unifying
perspective on RSs to lower the bar to entry for tackling them. Ultimately, we
hope this overview contributes to the development of reliable NeSy and
trustworthy AI models.

</details>


### [143] [LLM Agents Beyond Utility: An Open-Ended Perspective](https://arxiv.org/abs/2510.14548)
*Asen Nachkov,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 本文研究了LLM代理是否能从解决问题的工具发展为具备自主规划和追求模糊目标的实体。通过赋予LLM代理生成任务、积累知识并与环境交互的能力，研究发现其虽能遵循指令并解决自身任务，但仍受限于提示设计、重复性任务和缺乏自我表征。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在链式推理和函数调用方面的能力增强，出现了一个关键问题：这些软件能否超越智能问题解决工具的范畴，成为能够自主规划、设计任务并追求更广泛、更模糊目标的独立实体？

Method: 采用开放式实验设置，增强一个预训练的LLM代理，使其具备生成自身任务、积累知识以及与环境广泛交互的能力。对由此产生的开放式代理进行定性研究。

Result: 该代理能可靠地遵循复杂多步指令，在不同运行中存储和重用信息，并提出和解决自身任务。然而，它对提示设计敏感，易于生成重复任务，且无法形成自我表征。

Conclusion: 研究结果揭示了将预训练LLM适应于开放式任务的潜力和当前局限性，并为未来训练代理管理记忆、高效探索和追求抽象长期目标指明了方向。

Abstract: Recent LLM agents have made great use of chain of thought reasoning and
function calling. As their capabilities grow, an important question arises: can
this software represent not only a smart problem-solving tool, but an entity in
its own right, that can plan, design immediate tasks, and reason toward
broader, more ambiguous goals? To study this question, we adopt an open-ended
experimental setting where we augment a pretrained LLM agent with the ability
to generate its own tasks, accumulate knowledge, and interact extensively with
its environment. We study the resulting open-ended agent qualitatively. It can
reliably follow complex multi-step instructions, store and reuse information
across runs, and propose and solve its own tasks, though it remains sensitive
to prompt design, prone to repetitive task generation, and unable to form
self-representations. These findings illustrate both the promise and current
limits of adapting pretrained LLMs toward open-endedness, and point to future
directions for training agents to manage memory, explore productively, and
pursue abstract long-term goals.

</details>


### [144] [ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks](https://arxiv.org/abs/2510.14621)
*Yuanyi Song,Heyuan Huang,Qiqiang Lin,Yin Zhao,Xiangmou Qu,Jun Wang,Xingyu Lou,Weiwen Liu,Zhuosheng Zhang,Jun Wang,Yong Yu,Weinan Zhang,Zhaoxiang Wang*

Main category: cs.AI

TL;DR: 本文提出了一种名为ColorBench的图结构基准测试框架，用于评估移动智能体在复杂、长周期任务中的表现，解决了现有评估方法无法处理多解和动态行为的问题。


<details>
  <summary>Details</summary>
Motivation: 现有移动智能体评估方法（离线静态基准和在线动态测试）不足以全面评估智能体能力，因为它们无法处理真实世界移动任务的复杂性、多解性以及动态和不可复现性。

Method: 引入了一个图结构基准测试框架，通过建模真实设备交互中的有限状态，实现动态行为的静态模拟。在此基础上开发了ColorBench，一个专注于复杂长周期任务的基准，支持评估多个有效解决方案、子任务完成率统计和原子级能力分析。ColorBench包含175个任务，每个任务至少包含两条正确路径和多条典型错误路径，实现了准动态交互。

Result: 通过在ColorBench上评估各种基线模型，发现了现有模型的局限性，并基于实验结果提出了改进方向和可行的技术途径，以提高智能体在复杂、长周期问题上的性能。

Conclusion: ColorBench提供了一个更全面、更稳定的评估框架，能够更好地理解和提升移动智能体在处理复杂、长周期移动任务时的能力。

Abstract: The rapid advancement of multimodal large language models has enabled agents
to operate mobile devices by directly interacting with graphical user
interfaces, opening new possibilities for mobile automation. However,
real-world mobile tasks are often complex and allow for multiple valid
solutions. This contradicts current mobile agent evaluation standards: offline
static benchmarks can only validate a single predefined "golden path", while
online dynamic testing is constrained by the complexity and non-reproducibility
of real devices, making both approaches inadequate for comprehensively
assessing agent capabilities. To bridge the gap between offline and online
evaluation and enhance testing stability, this paper introduces a novel
graph-structured benchmarking framework. By modeling the finite states observed
during real-device interactions, it achieves static simulation of dynamic
behaviors. Building on this, we develop ColorBench, a benchmark focused on
complex long-horizon tasks. It supports evaluation of multiple valid solutions,
subtask completion rate statistics, and atomic-level capability analysis.
ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average
length of over 13 steps. Each task includes at least two correct paths and
several typical error paths, enabling quasi-dynamic interaction. By evaluating
ColorBench across various baselines, we discover limitations of existing models
and propose improvement directions and feasible technical pathways to enhance
agents' performance on complex, long-horizon problems based on experimental
results. Code and data are available at:
https://github.com/MadeAgents/ColorBench.

</details>


### [145] [Beyond Hallucinations: The Illusion of Understanding in Large Language Models](https://arxiv.org/abs/2510.14665)
*Rikard Rosenbacke,Carl Rosenbacke,Victor Rosenbacke,Martin McKee*

Main category: cs.AI

TL;DR: 本文提出Rose-Frame，一个三维框架，用于诊断大型语言模型（LLMs）因其系统1认知特性而导致的认知和认识论偏差，并将AI对齐重构为由人类理性主导的认知治理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽在人类交流和决策中日益普及，但其基于统计预测而非推理，易产生幻觉，且本质上是缺乏反思和证伪的大规模系统1认知，存在固有的模糊性、偏见及与真相的脱节。

Method: 引入“Rose-Frame”，一个三维框架，用于诊断人机交互中的认知和认识论偏差。该框架包含三个维度：(i) 地图与疆域，(ii) 直觉与理性，(iii) 冲突与确认，每个维度均捕捉一种独特的失败模式。

Result: Rose-Frame不直接修复LLM，而是一个反思性工具，能显化模型局限性和用户假设，促进更透明、更具批判意识的AI部署。它将AI对齐重构为认知治理，强调人类理性应主导人机直觉。

Conclusion: 只有通过嵌入反思性、可证伪的监督机制，才能使机器的流畅表达与人类的理解力真正对齐，从而实现认知治理。

Abstract: Large language models (LLMs) are becoming deeply embedded in human
communication and decision-making, yet they inherit the ambiguity, bias, and
lack of direct access to truth inherent in language itself. While their outputs
are fluent, emotionally resonant, and coherent, they are generated through
statistical prediction rather than grounded reasoning. This creates the risk of
hallucination, responses that sound convincing but lack factual validity.
Building on Geoffrey Hinton's observation that AI mirrors human intuition
rather than reasoning, this paper argues that LLMs operationalize System 1
cognition at scale: fast, associative, and persuasive, but without reflection
or falsification. To address this, we introduce the Rose-Frame, a
three-dimensional framework for diagnosing cognitive and epistemic drift in
human-AI interaction. The three axes are: (i) Map vs. Territory, which
distinguishes representations of reality (epistemology) from reality itself
(ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to
separate fast, emotional judgments from slow, reflective thinking; and (iii)
Conflict vs. Confirmation, which examines whether ideas are critically tested
through disagreement or simply reinforced through mutual validation. Each
dimension captures a distinct failure mode, and their combination amplifies
misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules.
Instead, it offers a reflective tool that makes both the model's limitations
and the user's assumptions visible, enabling more transparent and critically
aware AI deployment. It reframes alignment as cognitive governance: intuition,
whether human or artificial, must remain governed by human reason. Only by
embedding reflective, falsifiable oversight can we align machine fluency with
human understanding.

</details>


### [146] [Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review](https://arxiv.org/abs/2510.14669)
*Sara Altamirano,Arjan Vreeken,Sennay Ghebreab*

Main category: cs.AI

TL;DR: 本研究系统性回顾了荷兰公共卫生领域机器学习研究中算法偏见的识别和报告情况，发现存在普遍缺失。为此，提出了一套偏见评估工具（RABAT）和一个公平导向框架（ACAR），旨在促进ML在公共卫生领域实现健康公平。


<details>
  <summary>Details</summary>
Motivation: 机器学习有望革新公共卫生，但若不系统关注算法偏见，可能无意中加剧现有健康不平等。因此，需要了解当前研究中算法偏见的识别、讨论和报告情况。

Method: 进行了一项针对2021-2025年荷兰公共卫生ML研究的系统文献综述。开发了“算法偏见风险评估工具（RABAT）”并应用于35篇同行评审研究。在此基础上，提出了一个四阶段的公平导向框架（ACAR）。

Result: 分析显示，尽管数据采样和缺失数据处理有良好记录，但大多数研究忽视了明确的公平性框架、子组分析和潜在危害的透明讨论。作为回应，本研究引入了ACAR框架，以指导研究人员在ML生命周期中解决公平性问题。

Conclusion: 为公共卫生ML从业者提供了可操作的建议，以持续考虑算法偏见并提高透明度，确保算法创新促进而非损害健康公平。

Abstract: Machine learning (ML) promises to revolutionize public health through
improved surveillance, risk stratification, and resource allocation. However,
without systematic attention to algorithmic bias, ML may inadvertently
reinforce existing health disparities. We present a systematic literature
review of algorithmic bias identification, discussion, and reporting in Dutch
public health ML research from 2021 to 2025. To this end, we developed the Risk
of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from
established frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible
AI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals
pervasive gaps: although data sampling and missing data practices are well
documented, most studies omit explicit fairness framing, subgroup analyses, and
transparent discussion of potential harms. In response, we introduce a
four-stage fairness-oriented framework called ACAR (Awareness,
Conceptualization, Application, Reporting), with guiding questions derived from
our systematic literature review to help researchers address fairness across
the ML lifecycle. We conclude with actionable recommendations for public health
ML practitioners to consistently consider algorithmic bias and foster
transparency, ensuring that algorithmic innovations advance health equity
rather than undermine it.

</details>


### [147] [TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence](https://arxiv.org/abs/2510.14670)
*Marco Simoni,Aleksandar Fontana,Andrea Saracino,Paolo Mori*

Main category: cs.AI

TL;DR: TITAN是一个框架，旨在将自然语言的网络威胁查询转化为结构化知识图谱上的可执行推理，以检索威胁情报。


<details>
  <summary>Details</summary>
Motivation: 传统检索系统在处理网络威胁情报时，缺乏在威胁、行为和防御之间进行清晰、可逆推理的能力，无法有效满足复杂查询需求。

Method: TITAN框架整合了路径规划模型（从文本预测逻辑关系链）和图执行器（遍历TITAN本体以检索答案和证据）。其核心在于操作一个源自MITRE的类型化、双向图。为支持训练和评估，研究者引入了包含88209个示例的TITAN数据集。

Result: 经验评估表明，TITAN能够使模型生成语法有效且语义连贯的推理路径，这些路径可以在底层图谱上确定性地执行。

Conclusion: TITAN成功地提供了一种将自然语言网络威胁查询与可执行推理连接起来的方法，并通过其独特的图结构实现了高效、可逆的推理，解决了传统系统的局限性。

Abstract: TITAN (Threat Intelligence Through Automated Navigation) is a framework that
connects natural-language cyber threat queries with executable reasoning over a
structured knowledge graph. It integrates a path planner model, which predicts
logical relation chains from text, and a graph executor that traverses the
TITAN Ontology to retrieve factual answers and supporting evidence. Unlike
traditional retrieval systems, TITAN operates on a typed, bidirectional graph
derived from MITRE, allowing reasoning to move clearly and reversibly between
threats, behaviors, and defenses. To support training and evaluation, we
introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test:
13951) pairing natural language questions with executable reasoning paths and
step by step Chain of Thought explanations. Empirical evaluations show that
TITAN enables models to generate syntactically valid and semantically coherent
reasoning paths that can be deterministically executed on the underlying graph.

</details>


### [148] [NAEL: Non-Anthropocentric Ethical Logic](https://arxiv.org/abs/2510.14676)
*Bianca Maria Lerma,Rafael Peñaloza*

Main category: cs.AI

TL;DR: NAEL是一个基于主动推理和符号推理的非人类中心AI伦理框架，通过最小化全局预期自由能，使智能体在不确定环境中发展出上下文相关、自适应的伦理行为。


<details>
  <summary>Details</summary>
Motivation: 现有AI伦理方法以人类为中心，存在局限性。研究旨在使AI伦理行为作为智能系统最小化全局预期自由能的涌现特性，避免预设人类道德直觉。

Method: 提出NAEL框架，结合主动推理和符号推理。通过神经符号架构，将伦理行为形式化为在动态多智能体环境中最小化全局预期自由能的涌现属性。

Result: 所提出的系统克服了现有伦理模型的局限性，使智能体能发展出上下文敏感、自适应和关系型伦理行为。案例研究（伦理资源分配）展示了NAEL在自我保护、认知学习和集体福利之间的动态平衡。

Conclusion: NAEL提供了一种新颖的AI伦理范式，能让智能体在无人类中心预设下，实现情境感知、自适应和关系性的伦理决策，有效平衡个体与集体利益。

Abstract: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical
framework for artificial agents grounded in active inference and symbolic
reasoning. Departing from conventional, human-centred approaches to AI ethics,
NAEL formalizes ethical behaviour as an emergent property of intelligent
systems minimizing global expected free energy in dynamic, multi-agent
environments. We propose a neuro-symbolic architecture to allow agents to
evaluate the ethical consequences of their actions in uncertain settings. The
proposed system addresses the limitations of existing ethical models by
allowing agents to develop context-sensitive, adaptive, and relational ethical
behaviour without presupposing anthropomorphic moral intuitions. A case study
involving ethical resource distribution illustrates NAEL's dynamic balancing of
self-preservation, epistemic learning, and collective welfare.

</details>


### [149] [Practical, Utilitarian Algorithm Configuration](https://arxiv.org/abs/2510.14683)
*Devon Graham,Kevin Leyton-Brown*

Main category: cs.AI

TL;DR: 通过一系列改进，提升了具有理论基础的功利算法配置方法COUP的实际性能，使其在保持理论保证的同时，能够与流行的启发式方法竞争。


<details>
  <summary>Details</summary>
Motivation: COUP作为一种功利算法配置方法，虽然提供了强大的理论保证，但其实际性能一直未受重视。本文旨在弥补这一差距，使理论上严谨的功利算法配置在实践中能与常用的启发式配置方法相媲美。

Method: 对COUP提出了一系列改进措施，旨在提升其经验性能而不削弱其理论保证，并通过实验验证了这些改进的有效性。此外，还通过案例研究展示了如何探索算法选择问题解决方案对效用函数变化的鲁棒性。

Result: 经过改进后的COUP在经验性能上得以显著提升，使其能够与广泛使用的、不提供性能保证的启发式配置方法竞争。实验结果证明了这些改进的益处。

Conclusion: 本文成功提升了具有理论基础的功利算法配置方法（如COUP）的实际应用性，使其在保持理论保证的前提下，在性能上达到与启发式方法相当的水平，并提供了分析解决方案鲁棒性的新方法。

Abstract: Utilitarian algorithm configuration identifies a parameter setting for a
given algorithm that maximizes a user's utility. Utility functions offer a
theoretically well-grounded approach to optimizing decision-making under
uncertainty and are flexible enough to capture a user's preferences over
algorithm runtimes (e.g., they can describe a sharp cutoff after which a
solution is no longer required, a per-hour cost for compute, or diminishing
returns from algorithms that take longer to run). COUP is a recently-introduced
utilitarian algorithm configuration procedure which was designed mainly to
offer strong theoretical guarantees about the quality of the configuration it
returns, with less attention paid to its practical performance. This paper
closes that gap, bringing theoretically-grounded, utilitarian algorithm
configuration to the point where it is competitive with widely used, heuristic
configuration procedures that offer no performance guarantees. We present a
series of improvements to COUP that improve its empirical performance without
degrading its theoretical guarantees and demonstrate their benefit
experimentally. Using a case study, we also illustrate ways of exploring the
robustness of a given solution to the algorithm selection problem to variations
in the utility function.

</details>


### [150] [Purifying Task Vectors in Knowledge-Aware Subspace for Model Merging](https://arxiv.org/abs/2510.14697)
*Bang An,Yibo Yang,Philip Torr,Bernard Ghanem*

Main category: cs.AI

TL;DR: 本文提出PAVE方法，通过在知识感知子空间中净化任务向量，去除冗余信息，从而提升模型融合性能，解决现有方法中任务向量冗余导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 模型融合旨在整合微调模型的特定任务能力。然而，现有的任务向量在融合时常因包含与任务无关的冗余信息而导致模型性能显著下降。现有通过随机删除参数来克服冗余的方法缺乏知识感知性且具有随机性。

Method: 研究提出“在知识感知子空间中净化任务向量”（PAVE）的方法。具体步骤包括：1) 从每个任务中抽取训练样本，送入对应的微调模型以获取线性层前的协方差矩阵；2) 执行上下文导向的奇异值分解（SVD），以突出与目标知识最相关的权重分量；3) 在知识感知子空间中将微调模型权重分解为任务相关和冗余部分，并通过剪枝冗余部分来净化任务向量；4) 引入光谱秩分配策略，通过优化归一化激活剪枝误差来确保模型间公平的剪枝力度。

Result: PAVE作为一种即插即用的方案，可应用于各种基于任务向量的融合方法以提升其性能。实验结果表明，PAVE在多种融合方法、任务和模型架构上均有效。

Conclusion: PAVE通过在知识感知子空间中净化任务向量，有效解决了模型融合中因任务向量冗余导致的性能下降问题，并能作为即插即用方案显著提升现有融合方法的表现。

Abstract: Model merging aims to integrate task-specific abilities from individually
fine-tuned models into a single model without extra training. In recent model
merging methods, task vector has become a fundamental building block, as it can
encapsulate the residual information from finetuning. However, the merged model
often suffers from notable performance degradation due to the conflicts caused
by task-irrelevant redundancy in task vectors. Existing efforts in overcoming
redundancy by randomly dropping elements in the parameter space involves
randomness and lacks knowledge awareness. To address these challenges, in this
study, we propose Purifying TAsk Vectors (PAVE) in knowledge-aware subspace.
Concretely, we sample some training examples from each task, and feed them into
their corresponding fine-tuned models to acquire the covariance matrices before
linear layers. We then perform a context-oriented singular value decomposition,
which accentuates the weight components most relevant to the target knowledge.
As a result, we can split fine-tuned model weights into task-relevant and
redundant components in the knowledge-aware subspace, and purify the task
vector by pruning the redundant components. To induce fair pruning efforts
across models, we further introduce a spectral rank allocation strategy by
optimizing a normalized activated pruning error. The task vector purification
by our method as a plug-and-play scheme is applicable across various task
vector-based merging methods to improve their performance. In experiments, we
demonstrate the effectiveness of PAVE across a diverse set of merging methods,
tasks, and model architectures.

</details>


### [151] [Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction](https://arxiv.org/abs/2510.14702)
*Penglong Zhai,Jie Li,Fanyi Di,Yue Liu,Yifang Yuan,Jie Huang,Peng Wu,Sicong Wang,Mingyang Yin,Tingting Hu,Yao Xu,Xin Li*

Main category: cs.AI

TL;DR: 本文提出了CoAST框架，通过对LLMs进行持续预训练和认知对齐，将世界知识和时空模式融入下一地点（POI）推荐，以解决现有LLMs在地理理解和人类认知对齐方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前的LLMs在POI推荐任务中缺乏对结构化地理实体和时序移动模式的理解，且未能有效整合世界知识和人类认知（如季节、天气、用户习惯等），影响了推荐性能和用户体验。

Method: CoAST框架采用自然语言作为接口，包含两个阶段：1) 通过对脱敏用户的丰富时空轨迹数据进行持续预训练，实现“推荐知识获取”；2) 通过监督微调（SFT）和后续的强化学习（RL）阶段，利用丰富训练数据将认知判断与人类偏好对齐，实现“认知对齐”。

Result: CoAST在多个真实世界数据集上的离线实验以及在AMAP应用首页“猜你去哪”中的在线部署实验，均证明了其有效性。

Conclusion: CoAST框架通过融合世界知识、时空轨迹模式、用户画像和情境信息，并进行认知对齐，有效提升了下一地点推荐的性能和用户体验。

Abstract: The next point-of-interest (POI) recommendation task aims to predict the
users' immediate next destinations based on their preferences and historical
check-ins, holding significant value in location-based services. Recently,
large language models (LLMs) have shown great potential in recommender systems,
which treat the next POI prediction in a generative manner. However, these
LLMs, pretrained primarily on vast corpora of unstructured text, lack the
native understanding of structured geographical entities and sequential
mobility patterns required for next POI prediction tasks. Moreover, in
industrial-scale POI prediction applications, incorporating world knowledge and
alignment of human cognition, such as seasons, weather conditions, holidays,
and users' profiles (such as habits, occupation, and preferences), can enhance
the user experience while improving recommendation performance. To address
these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a
framework employing natural language as an interface, allowing for the
incorporation of world knowledge, spatio-temporal trajectory patterns,
profiles, and situational information. Specifically, CoAST mainly comprises of
2 stages: (1) Recommendation Knowledge Acquisition through continued
pretraining on the enriched spatial-temporal trajectory data of the
desensitized users; (2) Cognitive Alignment to align cognitive judgments with
human preferences using enriched training data through Supervised Fine-Tuning
(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline
experiments on various real-world datasets and online experiments deployed in
"Guess Where You Go" of AMAP App homepage demonstrate the effectiveness of
CoAST.

</details>


### [152] [ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling](https://arxiv.org/abs/2510.14703)
*Jianghao Lin,Yuanyuan Shi,Xin Peng,Renjie Ding,Hairui Wang,Yuxuan Peng,Bizhe Bai,Weixi Song,Fengshuo Bai,Huacan Chai,Weinan Zhang,Fei Huang,Ying Wen*

Main category: cs.AI

TL;DR: 本文提出一个结合细粒度束搜索和过程奖励模型ToolPRM的推理扩展框架，以弥补大语言模型（LLMs）推理扩展在函数调用等结构化输出上的研究空白。通过构建首个细粒度调用内过程监督数据集，实验证明ToolPRM在预测准确性上优于现有模型，并显著提升了函数调用任务的性能，同时揭示了结构化输出推理扩展的“多探索少保留”原则。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理扩展技术主要集中在非结构化输出生成任务，而其在函数调用等结构化输出中的应用尚未得到充分探索。研究旨在弥补这一空白。

Method: 提出一个推理扩展框架，结合了细粒度束搜索和过程奖励模型ToolPRM。ToolPRM对单个函数调用的内部步骤进行评分。为训练ToolPRM，构建了首个细粒度调用内过程监督数据集，通过函数掩码技术自动标注以提供步骤级奖励。

Result: ToolPRM在预测准确性方面超越了粗粒度和结果奖励模型，表明其在监督函数调用推理过程中的强大能力。配备ToolPRM的推理扩展技术显著提高了基础模型在各种函数调用任务和基准上的性能。

Conclusion: 该研究成功将推理扩展技术应用于结构化输出，并揭示了将推理扩展技术应用于结构化输出（如函数调用）的关键原则：“多探索少保留”（explore more but retain less），这源于结构化函数调用生成不可恢复的特性。

Abstract: Large language models (LLMs) are increasingly demonstrating strong
capabilities as autonomous agents, with function calling serving as a core
mechanism for interaction with the environment. Meanwhile, inference scaling
has become a cutting-edge technique to enhance LLM performance by allocating
more computational resources during the inference process. However, current
research on inference scaling primarily focuses on unstructured output
generation tasks, leaving its application in structured outputs, like function
calling, largely underexplored. To bridge this gap, we propose an inference
scaling framework that combines fine-grained beam search with a process reward
model, ToolPRM, which scores the internal steps of each single function call.
To train ToolPRM, we construct the first fine-grained intra-call process
supervision dataset, automatically annotated with function-masking techniques
to provide step-level rewards for structured tool-use reasoning. Extensive
experiments demonstrate that ToolPRM beats the coarse-grained and outcome
reward models in terms of predictive accuracy, indicating its stronger
capability in supervising the function calling inference process. Inference
scaling technique equipped with ToolPRM also significantly improves the
backbone model performance across various function calling tasks and
benchmarks. More importantly, we reveal a key principle for applying inference
scaling techniques to structured outputs: "explore more but retain less" due to
the unrecoverability characteristics of structured function calling generation.

</details>


### [153] [SimKO: Simple Pass@K Policy Optimization](https://arxiv.org/abs/2510.14807)
*Ruotian Peng,Yi Ren,Zhouliang Yu,Weiyang Liu,Yandong Wen*

Main category: cs.AI

TL;DR: 现有RLVR方法存在过度利用而非探索的偏见，表现为pass@1提高但pass@K（K>1）下降。通过分析发现，这是由于词元概率过度集中所致。本文提出SimKO方法，通过不对称地调整正确回复的top-K概率和错误回复的top-1概率，有效缓解过度集中问题，提升了RLVR的探索能力及pass@K性能。


<details>
  <summary>Details</summary>
Motivation: RLVR虽然提升了LLM的推理能力，但普遍存在过度利用而非探索的系统性偏见，导致pass@1性能提高但pass@K（K>1）性能下降。研究动机在于理解这一问题的根源（通过分析训练动态），并提出有效方法来缓解过度集中，鼓励探索。

Method: 1. **分析方法**: 追踪RLVR方法的训练动态，分析词元级别概率分布，揭示了top-1候选词概率质量积累并抑制其他候选词的集中效应，且这种过度集中与pass@K性能下降相关。2. **提出方法**: Simple Pass@K Optimization (SimKO)。3. **SimKO机制**: 采用非对称策略。对于验证正确的回复，提高top-K候选词的概率；对于验证错误的回复，对top-1候选词施加强烈惩罚。4. **应用策略**: 在高熵词元处应用此不对称设计，被发现特别有效。

Result: 1. **分析结果**: 揭示了词元级别概率一致的集中效应，即top-1候选词逐渐积累概率质量并抑制其他候选词。更重要的是，更强的过度集中与更差的pass@K性能相关。2. **SimKO性能**: SimKO在各种数学和逻辑推理基准测试中，能持续为广泛的K值带来更高的pass@K，提供了一种改进RLVR探索能力的简单方法。

Conclusion: SimKO是一种简单有效的方法，它通过不对称的概率调整策略，成功缓解了RLVR中存在的概率过度集中问题，从而有效鼓励了探索，并在多个推理任务上持续提升了LLMs的pass@K性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has advanced the
reasoning capabilities of large language models (LLMs). However, prevailing
RLVR methods exhibit a systematic bias toward exploitation over exploration, as
evidenced by improved pass@1 but reduced pass@K (K>1) performance. To
understand this issue, we analyze training dynamics of RLVR methods by tracking
the token-level probability distributions over vocabulary candidates. Our
analysis reveals a consistent probability concentration effect where the top-1
candidate increasingly accumulates probability mass and suppresses that of
other candidates. More importantly, stronger over-concentration correlates with
worse pass@K performance. Inspired by this finding, we propose Simple Pass@K
Optimization (SimKO), a method designed to mitigate the over-concentration
issue, thereby encouraging exploration. SimKO operates in an asymmetrical
manner. For verified-correct responses, it boosts the probabilities of the
top-K candidates. For verified-incorrect responses, it applies stronger
penalties to the top-1 candidate. We observe that this asymmetric design is
particularly effective at mitigating over-concentration when applied at tokens
with high entropy. Across various math and logical-reasoning benchmarks, SimKO
consistently yields higher pass@K for a wide range of K, providing a simple way
to improve RLVR's exploration.

</details>


### [154] [Agentic NL2SQL to Reduce Computational Costs](https://arxiv.org/abs/2510.14808)
*Dominik Jehle,Lennart Purucker,Frank Hutter*

Main category: cs.AI

TL;DR: 本文提出Datalake Agent，一个基于LLM的智能代理系统，通过交互式循环和选择性信息请求，高效地将自然语言查询转换为SQL，显著降低了token使用量和成本，同时保持了竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的NL2SQL方法需要处理大量数据库元信息，导致冗长的prompt、高token使用量和处理成本。

Method: Datalake Agent采用代理系统，通过交互式循环减少所需元信息。LLM在一个推理框架中被用于选择性地请求解决表格问答任务所需的必要信息，而非一次性提供所有元信息。

Result: Datalake Agent在23个数据库和100个表格问答任务上进行了评估。结果显示，它将LLM使用的token量减少了高达87%，从而大幅降低了成本，同时保持了具有竞争力的性能。

Conclusion: Datalake Agent提供了一种更高效、更经济的NL2SQL解决方案，通过智能地管理和选择性请求数据库元信息，克服了传统LLM方法中长prompt和高成本的挑战。

Abstract: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)
has recently been empowered by large language models (LLMs). Using LLMs to
perform NL2SQL methods on a large collection of SQL databases necessitates
processing large quantities of meta-information about the databases, which in
turn results in lengthy prompts with many tokens and high processing costs. To
address this challenge, we introduce Datalake Agent, an agentic system designed
to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing
direct solvers for NL2SQL that call the LLM once with all meta-information in
the prompt, the Datalake Agent employs an interactive loop to reduce the
utilized meta-information. Within the loop, the LLM is used in a reasoning
framework that selectively requests only the necessary information to solve a
table question answering task. We evaluate the Datalake Agent on a collection
of 23 databases with 100 table question answering tasks. The Datalake Agent
reduces the tokens used by the LLM by up to 87\% and thus allows for
substantial cost reductions while maintaining competitive performance.

</details>


### [155] [RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning](https://arxiv.org/abs/2510.14828)
*Jinrui Liu,Bingyan Nie,Boyu Li,Yaran Chen,Yuze Wang,Shunsen He,Haoran Li*

Main category: cs.AI

TL;DR: 本文提出RoboGPT-R1，一个具身规划的两阶段微调框架，通过结合监督学习和强化学习，并设计规则奖励函数，显著提升了具身智能体在复杂长距离操作任务中的推理能力，性能超越了更大的语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调（SFT）的大语言模型和视觉语言模型在复杂真实世界的长距离操作任务中，因常识和推理能力受限而面临挑战。将通用视觉语言模型对齐到机器人规划任务时，SFT方法存在泛化性差和物理理解不足的问题，因此迫切需要提升具身智能体的推理能力。

Method: 研究者提出了RoboGPT-R1，一个两阶段的具身规划微调框架。第一阶段，通过专家序列进行监督训练以获取基础知识；第二阶段，利用强化学习（RL）解决模型在视觉空间理解和推理方面的不足。为实现物理理解和动作序列一致性，设计了一个基于规则的奖励函数，同时考虑长距离性能和环境中的动作约束。推理模型基于Qwen2.5-VL-3B进行训练。

Result: 在EmbodiedBench基准测试中，基于Qwen2.5-VL-3B训练的推理模型，其性能显著超越更大的GPT-4o-mini模型21.33%，并超越其他基于Qwen2.5-VL-7B训练的工作20.33%。

Conclusion: RoboGPT-R1框架通过创新性地结合监督学习和强化学习，并引入考虑物理理解和动作约束的规则奖励函数，有效解决了现有方法在长距离具身规划任务中的推理能力瓶颈，实现了卓越的性能提升，证明了其在具身智能体规划领域的有效性和潜力。

Abstract: Improving the reasoning capabilities of embodied agents is crucial for robots
to complete complex human instructions in long-view manipulation tasks
successfully. Despite the success of large language models and vision language
models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue
facing challenges in performing long-horizon manipulation tasks in complex
real-world environments, owing to their restricted common sense and reasoning
capabilities. Considering that aligning general-purpose vision language models
to robotic planning tasks via supervised fine-tuning suffers from poor
generalization and insufficient physical understanding, we propose RoboGPT-R1,
a two-stage fine-tuning framework for embodied planning. In this framework,
supervised training acquires foundational knowledge through expert sequences,
followed by RL to address the model's shortcomings in visual-spatial
understanding and reasoning. To achieve physical understanding and action
sequence consistency in multi-step reasoning tasks, we design a rule-based
reward function that simultaneously considers long-horizon performance and
action constraint in the environment. The reasoning model, trained on
Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,
by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the
EmbodiedBench benchmark.

</details>


### [156] [Boosting Instruction Following at Scale](https://arxiv.org/abs/2510.14842)
*Ben Elder,Evelyn Duesterwald,Vinod Muthusamy*

Main category: cs.AI

TL;DR: 本文提出Instruction Boosting作为一种后生成方法，以提高LLM指令遵循的可靠性，并引入SCALEDIF基准。研究发现，性能下降与指令冲突有关，并提供了一个量化冲突评分工具。


<details>
  <summary>Details</summary>
Motivation: 开发者通过修改Prompt来影响LLM行为，但仅增加指令并不能保证其被遵循。随着指令数量增加，LLM的性能会下降，需要提高LLM遵循Prompt指令的可靠性。

Method: 引入Instruction Boosting作为一种后生成方法来提高LLM Prompt指令的可靠性。构建SCALEDIF基准，包含多达十条指令的数据样本。开发了一个量化冲突评分工具，以解释性能趋势并提供反馈。

Result: Instruction Boosting将两条指令的遵循率提高了7个百分点，十条指令的遵循率提高了4个百分点。性能下降的原因是指令数量增加导致的紧张和冲突程度增加。量化冲突评分工具能解释观察到的性能趋势并为开发者提供反馈。

Conclusion: Instruction Boosting能有效提高LLM指令遵循的可靠性。指令间的冲突是导致LLM性能下降的关键因素，通过量化冲突评分工具可以更好地理解和管理Prompt指令对模型性能的影响。

Abstract: A typical approach developers follow to influence an LLM's behavior in an
application is through careful manipulation of the prompt, such as by adding or
modifying instructions. However, merely adding more instructions provides
little assurance that they will actually be followed. We introduce Instruction
Boosting as a post-generation method to increase the reliability of LLM prompt
instructions. We show that Instruction Boosting improves the instruction
following rate by up to 7 points for two instructions and up to 4 points for
ten instructions. To demonstrate these results we introduce SCALEDIF, a
benchmark with a scaled instruction volume of up to ten instructions per data
sample. We also present an analysis of the commonly observed trend that
performance degrades as more instructions are added. We show that an important
factor contributing to this trend is the degree of tension and conflict that
arises as the number of instructions is increased. We contribute a quantitative
conflict scoring tool that explains the observed performance trends and
provides feedback to developers on the impact that additional prompt
instructions have on a model's performance.

</details>


### [157] [Where to Search: Measure the Prior-Structured Search Space of LLM Agents](https://arxiv.org/abs/2510.14846)
*Zhuo-Yang Song*

Main category: cs.AI

TL;DR: 本文提出了一个紧凑的形式化理论，用于描述和衡量由领域先验知识引导的LLM辅助迭代搜索，并提供了衡量智能体及其搜索空间的工具。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在迭代搜索中表现出色，但搜索效率关键在于如何将领域先验知识有效编码到结构化的假设空间中。目前缺乏对LLM辅助迭代搜索及其领域先验引导机制的系统性描述和衡量方法。

Method: 本文提出了一个紧凑的形式化理论。将智能体建模为输入输出上的模糊关系算子，并用安全包络限制其行为。通过对所有可达路径赋权重并求和，得到覆盖生成函数来描述多步搜索，进而衡量可达性难度并提供几何解释。通过多数投票实例化验证了理论推论。

Result: 该理论提供了一套可操作的语言和工具，用于系统地衡量智能体及其搜索空间，并形式化描述了LLM构建的迭代搜索。通过多数投票实例化验证了理论的推论。

Conclusion: 该理论为LLM辅助的迭代搜索提供了一个系统、形式化的描述框架，并提供了衡量智能体及其搜索空间的可行语言和操作工具，特别关注了领域先验知识在搜索引导中的作用。

Abstract: The generate-filter-refine (iterative paradigm) based on large language
models (LLMs) has achieved progress in reasoning, programming, and program
discovery in AI+Science. However, the effectiveness of search depends on where
to search, namely, how to encode the domain prior into an operationally
structured hypothesis space. To this end, this paper proposes a compact formal
theory that describes and measures LLM-assisted iterative search guided by
domain priors. We represent an agent as a fuzzy relation operator on inputs and
outputs to capture feasible transitions; the agent is thereby constrained by a
fixed safety envelope. To describe multi-step reasoning/search, we weight all
reachable paths by a single continuation parameter and sum them to obtain a
coverage generating function; this induces a measure of reachability
difficulty; and it provides a geometric interpretation of search on the graph
induced by the safety envelope. We further provide the simplest testable
inferences and validate them via a majority-vote instantiation. This theory
offers a workable language and operational tools to measure agents and their
search spaces, proposing a systematic formal description of iterative search
constructed by LLMs.

</details>


### [158] [LabOS: The AI-XR Co-Scientist That Sees and Works With Humans](https://arxiv.org/abs/2510.14861)
*Le Cong,Zaixi Zhang,Xiaotong Wang,Yin Di,Ruofan Jin,Michal Gerasimiuk,Yinkai Wang,Ravi K. Dinesh,David Smerkous,Alex Smerkous,Xuekun Wu,Shilong Liu,Peishan Li,Yi Zhu,Simran Serrao,Ning Zhao,Imran A. Mohammad,John B. Sunwoo,Joseph C. Wu,Mengdi Wang*

Main category: cs.AI

TL;DR: LabOS是首个结合计算推理与物理实验的AI共同科学家，通过多模态感知、自演进智能体和XR人机协作，使AI能实时参与实验，加速科学发现。


<details>
  <summary>Details</summary>
Motivation: 旨在克服传统AI仅限于计算设计的局限，通过将计算推理与物理实验相结合，加速科学研究进展。

Method: 通过连接多模态AI智能体、智能眼镜和XR支持的人机协作，使AI能够感知实验环境、理解实验背景并实时辅助实验执行。

Result: LabOS在癌症免疫疗法靶点发现和干细胞工程等应用中，展示了AI能够从计算设计转向实际参与实验，将实验室转变为智能、协作的环境。

Conclusion: LabOS使得AI能够成为真正的共同科学家，促进人类和机器共同进化式发现，从而加速科学研究的发展。

Abstract: Modern science advances fastest when thought meets action. LabOS represents
the first AI co-scientist that unites computational reasoning with physical
experimentation through multimodal perception, self-evolving agents, and
Entended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model
AI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see
what scientists see, understand experimental context, and assist in real-time
execution. Across applications--from cancer immunotherapy target discovery to
stem-cell engineering -- LabOS shows that AI can move beyond computational
design to participation, turning the laboratory into an intelligent,
collaborative environment where human and machine discovery evolve together.

</details>


### [159] [The Gatekeeper Knows Enough](https://arxiv.org/abs/2510.14881)
*Fikresilase Wondmeneh Abebayew*

Main category: cs.AI

TL;DR: 本文提出Gatekeeper协议，通过让LLM代理在“潜在状态”上操作并按需请求高精度上下文，解决LLM作为自主代理在上下文窗口限制和状态同步问题，从而提高其可靠性、效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: LLM作为自主代理时，受限于上下文窗口大小和其无状态特性导致的状态不同步，这导致输出不可靠、行为不可预测和资源使用效率低下，尤其是在与大型、结构化和敏感知识系统（如代码库和文档）交互时。

Method: 引入Gatekeeper协议，一个领域无关的框架，用于管理代理-系统交互。该协议要求代理首先在系统的“潜在状态”（一种简约、低保真表示）上操作和推理，然后按需战略性地请求高保真上下文。所有交互都通过统一的JSON格式进行，作为声明性、状态同步的协议，确保代理对系统的模型始终可验证地基于系统现实。通过Sage（Gatekeeper协议的软件开发参考实现）验证了其有效性。

Result: 该方法显著提高了代理的可靠性，通过最小化token消耗提高了计算效率，并实现了与复杂系统的可扩展交互。

Conclusion: Gatekeeper协议为构建更健壮、可预测和扎根于任何结构化知识领域的AI代理提供了一种基础性方法。

Abstract: Large Language Models (LLMs) are increasingly deployed as autonomous agents,
yet their practical utility is fundamentally constrained by a limited context
window and state desynchronization resulting from the LLMs' stateless nature
and inefficient context management. These limitations lead to unreliable
output, unpredictable behavior, and inefficient resource usage, particularly
when interacting with large, structured, and sensitive knowledge systems such
as codebases and documents. To address these challenges, we introduce the
Gatekeeper Protocol, a novel, domain-agnostic framework that governs
agent-system interactions. Our protocol mandates that the agent first operate
and reason on a minimalist, low-fidelity "latent state" representation of the
system to strategically request high-fidelity context on demand. All
interactions are mediated through a unified JSON format that serves as a
declarative, state-synchronized protocol, ensuring the agent's model of the
system remains verifiably grounded in the system's reality. We demonstrate the
efficacy of this protocol with Sage, a reference implementation of the
Gatekeeper Protocol for software development. Our results show that this
approach significantly increases agent reliability, improves computational
efficiency by minimizing token consumption, and enables scalable interaction
with complex systems, creating a foundational methodology for building more
robust, predictable, and grounded AI agents for any structured knowledge
domain.

</details>


### [160] [Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates](https://arxiv.org/abs/2510.14900)
*Wen-Kwang Tsao,Yao-Ching Yu,Chien-Ming Huang*

Main category: cs.AI

TL;DR: 针对企业智能平台集成第三方日志时因文档缺失导致的模式映射难题，本文提出一种自适应强化学习代理。该代理通过网络搜索和置信度奖励迭代优化映射，显著提升了准确率并大幅减少了人工复核需求。


<details>
  <summary>Details</summary>
Motivation: 企业智能平台需要整合来自多个第三方供应商的日志以执行下游任务。然而，在测试时，供应商文档常缺失、不匹配、格式不佳或不完整，使得日志模式映射成为一项挑战。

Method: 本文引入了一个强化学习代理，它无需标注示例或模型权重更新即可自我改进。在推理过程中，该代理会识别模糊的字段映射尝试，生成有针对性的网络搜索查询以收集外部证据，并应用基于置信度的奖励机制来迭代完善其映射。

Result: 通过将Microsoft Defender for Endpoint日志转换为通用模式进行验证，该方法在使用GPT-4o的100次迭代后，将映射准确率从56.4%（仅LLM）提高到72.73%（RAG），最终达到93.94%。同时，它将需要专家审查的低置信度映射数量减少了85%。

Conclusion: 这种新方法提供了一种证据驱动、透明的解决方案，有助于解决未来的行业问题，并为构建更健壮、负责、可扩展、高效、灵活、适应性强和协作的解决方案奠定了基础。

Abstract: The Enterprise Intelligence Platform must integrate logs from numerous
third-party vendors in order to perform various downstream tasks. However,
vendor documentation is often unavailable at test time. It is either misplaced,
mismatched, poorly formatted, or incomplete, which makes schema mapping
challenging. We introduce a reinforcement learning agent that can self-improve
without labeled examples or model weight updates. During inference, the agent:
1) Identifies ambiguous field-mapping attempts. 2) Generates targeted
web-search queries to gather external evidence. 3) Applies a confidence-based
reward to iteratively refine its mappings. To demonstrate this concept, we
converted Microsoft Defender for Endpoint logs into a common schema. Our method
increased mapping accuracy from 56.4\%(LLM-only) to 72.73\%(RAG) to 93.94\%
over 100 iterations using GPT-4o. At the same time, it reduced the number of
low-confidence mappings requiring expert review by 85\%. This new approach
provides an evidence-driven, transparent method for solving future industry
problems, paving the way for more robust, accountable, scalable, efficient,
flexible, adaptable, and collaborative solutions.

</details>


### [161] [Budget-aware Test-time Scaling via Discriminative Verification](https://arxiv.org/abs/2510.14913)
*Kyle Montgomery,Sijun Tan,Yuqi Chen,Siyuan Zhuang,Tianjun Zhang,Raluca Ada Popa,Chenguang Wang*

Main category: cs.AI

TL;DR: 本文提出一种结合判别式验证和自洽性的混合方法，在固定计算预算下，显著提升LLM在复杂推理任务上的性能，超越了现有生成式验证方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM测试时扩展策略中，生成式验证器虽然有效，但计算成本过高，限制了其实用性。

Method: 研究重点转向预算感知的判别式验证范式。提出一种混合方法，将判别式验证与自洽性相结合，以实现高效的测试时扩展。

Result: 在固定计算预算下，该混合方法在AIME2025数据集上比最先进的生成式验证方法高出15.3%的准确率。

Conclusion: 预算感知的判别式验证是LLM实际应用中比生成式技术更有效、更高效的测试时扩展替代方案，并且相对于自洽性而言，是一项“免费”的性能提升。

Abstract: Test-time scaling is a powerful strategy for boosting the performance of
large language models on complex reasoning tasks. While state-of-the-art
approaches often employ generative verifiers to select the best solution from a
pool of candidates, this method incurs prohibitive computational costs,
limiting its practicality. In this work, we shift the focus to a more
budget-aware paradigm: discriminative verification. We conduct a thorough
empirical analysis and demonstrate that while discriminative verifiers may
underperform in isolation, combining them with self-consistency in a hybrid
approach creates a powerful and efficient test-time scaling mechanism. Notably,
under a fixed compute budget, this hybrid approach surpasses state-of-the-art
generative verification by a significant margin: achieving up to 15.3\% higher
accuracy on AIME2025. Our findings establish that for practical, real-world
applications, budget-aware scaling with discriminative verifiers is not only a
"free" upgrade over self-consistency, but also a more effective and efficient
alternative to costly generative techniques. Code is available at
https://github.com/wang-research-lab/verification.

</details>


### [162] [TRI-DEP: A Trimodal Comparative Study for Depression Detection Using Speech, Text, and EEG](https://arxiv.org/abs/2510.14922)
*Annisaa Fitri Nurfidausi,Eleonora Mancini,Paolo Torroni*

Main category: cs.AI

TL;DR: 本研究通过系统探索脑电图、语音和文本的多模态特征和建模策略，有效提升了抑郁症的自动检测性能，实现了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 抑郁症自动检测面临挑战，现有研究在特征比较和评估协议上存在局限，未能充分系统地探索多模态方法的潜力。

Method: 系统探索脑电图（EEG）、语音和文本的特征表示和建模策略。具体包括：比较手工特征与预训练嵌入，评估不同神经网络编码器，对比单模态、双模态和三模态配置，并分析融合策略（尤其关注EEG的作用）。采用一致的独立于受试者的分割方式进行鲁棒性评估。

Result: (i) 结合EEG、语音和文本模态可增强多模态检测效果；(ii) 预训练嵌入的表现优于手工特征；(iii) 精心设计的三模态模型实现了最先进的性能。

Conclusion: 本研究为多模态抑郁症检测的未来研究奠定了基础，并通过系统方法显著提升了检测性能。

Abstract: Depression is a widespread mental health disorder, yet its automatic
detection remains challenging. Prior work has explored unimodal and multimodal
approaches, with multimodal systems showing promise by leveraging complementary
signals. However, existing studies are limited in scope, lack systematic
comparisons of features, and suffer from inconsistent evaluation protocols. We
address these gaps by systematically exploring feature representations and
modelling strategies across EEG, together with speech and text. We evaluate
handcrafted features versus pre-trained embeddings, assess the effectiveness of
different neural encoders, compare unimodal, bimodal, and trimodal
configurations, and analyse fusion strategies with attention to the role of
EEG. Consistent subject-independent splits are applied to ensure robust,
reproducible benchmarking. Our results show that (i) the combination of EEG,
speech and text modalities enhances multimodal detection, (ii) pretrained
embeddings outperform handcrafted features, and (iii) carefully designed
trimodal models achieve state-of-the-art performance. Our work lays the
groundwork for future research in multimodal depression detection.

</details>


### [163] [Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models](https://arxiv.org/abs/2510.14925)
*Akira Okutomi*

Main category: cs.AI

TL;DR: 本文将康德的《纯粹理性批判》重新诠释为反馈稳定性理论，并引入复合不稳定性指数（H-Risk）来衡量推理的“过自信错误”。研究发现，高H-Risk预示过自信错误，且LLM中脆弱的内部动态与错误校准和幻觉相关。这为诊断和减少推理系统中的过自信提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 将康德的《纯粹理性批判》重新诠释为一种反馈稳定性理论，以理解理性如何将推理限制在可能经验的范围内。旨在为诊断和减少推理系统（包括LLM）中的过自信提供一个原则性框架。

Method: 通过一个结合谱裕度、条件数、时间敏感性和创新放大等指标的复合不稳定性指数（H-Risk）来形式化康德的直觉。在线性-高斯模拟中测试H-Risk的预测能力。将该框架扩展到大型语言模型（LLMs），分析其内部动态，并评估批判式提示对校准和幻觉的影响。

Result: 在线性-高斯模拟中，较高的H-Risk即使在形式稳定下也预示着过自信错误，揭示了名义稳定性和认知稳定性之间的差异。在LLMs中，脆弱的内部动态与错误校准和幻觉相关。批判式提示对LLM的校准和幻觉显示出混合效果。

Conclusion: 本研究结果在康德的自我限制理论与反馈控制之间建立了一座结构性桥梁，为诊断并选择性地减少推理系统中的过自信问题提供了一个有原则的视角。

Abstract: We reinterpret Kant's Critique of Pure Reason as a theory of feedback
stability, viewing reason as a regulator that keeps inference within the bounds
of possible experience. We formalize this intuition via a composite instability
index (H-Risk) combining spectral margin, conditioning, temporal sensitivity,
and innovation amplification. In linear-Gaussian simulations, higher H-Risk
predicts overconfident errors even under formal stability, revealing a gap
between nominal and epistemic stability. Extending to large language models
(LLMs), we find that fragile internal dynamics correlate with miscalibration
and hallucination, while critique-style prompts show mixed effects on
calibration and hallucination. These results suggest a structural bridge
between Kantian self-limitation and feedback control, offering a principled
lens for diagnosing -- and selectively reducing -- overconfidence in reasoning
systems. This is a preliminary version; supplementary experiments and broader
replication will be reported in a future revision.

</details>


### [164] [GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning](https://arxiv.org/abs/2510.14942)
*Yao Zhang,Yu Wu,Haowei Zhang,Weiguo Li,Haokun Chen,Jingpei Wu,Guohao Li,Zhen Han,Volker Tresp*

Main category: cs.AI

TL;DR: GroundedPRM是一个树形引导和真实性感知的框架，通过结合蒙特卡洛树搜索和外部工具验证，为大型语言模型的中间推理步骤提供自动过程监督，显著提高了性能和数据效率，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型（PRMs）在构建时面临挑战，包括缺乏可扩展的高质量标注、依赖昂贵的人工标注、LLM自评估易产生幻觉、以及蒙特卡洛（MC）估计的信用误归因导致监督噪声和对齐问题，从而导致奖励噪声、事实保真度低和与步骤级推理目标不匹配。

Method: 引入GroundedPRM框架。通过蒙特卡洛树搜索（MCTS）构建结构化推理路径以减少奖励噪声和实现细粒度信用分配。使用外部工具验证每个中间步骤，提供执行接地的正确性信号以消除幻觉监督。设计混合奖励聚合机制，融合工具验证和MCTS反馈，结合步骤级验证和全局结果评估。将奖励信号格式化为增强解释性和与指令调优LLM兼容的生成结构。

Result: GroundedPRM仅使用4万个自动标注样本（仅占现有最佳PRM数据量的10%），在ProcessBench上实现了平均性能高达26%的相对提升。在奖励引导的贪婪搜索中，GroundedPRM甚至超越了使用人工标注监督训练的PRM。

Conclusion: GroundedPRM提供了一种可扩展且可验证的路径，以实现高质量的进程级推理，显著提高了LLM在多步骤推理中的表现，同时降低了数据标注成本。

Abstract: Process Reward Models (PRMs) aim to improve multi-step reasoning in Large
Language Models (LLMs) by supervising intermediate steps and identifying
errors. However, building effective PRMs remains challenging due to the lack of
scalable, high-quality annotations. Existing approaches rely on costly human
labeling, LLM-based self-evaluation that is prone to hallucination, or Monte
Carlo (MC) estimation, which infers step quality solely from rollout outcomes
and often introduces noisy, misaligned supervision due to credit
misattribution. These issues result in three core limitations: noisy rewards,
low factual fidelity, and misalignment with step-level reasoning objectives. To
address these challenges, we introduce GroundedPRM, a tree-guided and
fidelity-aware framework for automatic process supervision. To reduce reward
noise and enable fine-grained credit assignment, we construct structured
reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated
supervision, we validate each intermediate step using an external tool,
providing execution-grounded correctness signals. To combine both step-level
validation and global outcome assessment, we design a hybrid reward aggregation
mechanism that fuses tool-based verification with MCTS-derived feedback.
Finally, we format the reward signal into a rationale-enhanced, generative
structure to promote interpretability and compatibility with instruction-tuned
LLMs. GroundedPRM is trained on only 40K automatically labeled samples,
amounting to just 10% of the data used by the best-performing PRM trained with
auto-labeled supervision. Nevertheless, it achieves up to a 26% relative
improvement in average performance on ProcessBench. When used for reward-guided
greedy search, GroundedPRM outperforms even PRMs trained with human-labeled
supervision, offering a scalable and verifiable path toward high-quality
process-level reasoning.

</details>


### [165] [Agentic Design of Compositional Machines](https://arxiv.org/abs/2510.14980)
*Wenqian Zhang,Weiyang Liu,Zhen Liu*

Main category: cs.AI

TL;DR: 研究大型语言模型（LLMs）在组合机器设计中的创造能力，引入BesiegeField测试平台进行评估。结果显示现有LLMs在此任务上表现不足，需要空间推理、策略组装等能力，并提出通过强化学习进行改进。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型（LLMs）的最新进展，研究它们是否能像人类一样学习创造复杂机器，特别是在模拟物理环境中，通过标准化组件进行组合机器设计。

Method: 引入名为BesiegeField的测试平台，该平台基于游戏Besiege构建，支持基于部件的机器建造、物理模拟和奖励驱动评估。使用代理工作流对最先进的LLMs进行基准测试，并探索通过强化学习（RL）微调来改进模型。

Result: 识别出成功进行机器设计所需的关键能力，包括空间推理、策略组装和指令遵循。发现当前开源LLMs在这些能力上表现不足。通过RL微调实验，展示了潜在的改进路径。

Conclusion: 强化学习为提高LLMs在机器设计任务上的能力提供了一条可行路径，并凸显了语言、机器设计和物理推理交叉领域的开放性挑战。

Abstract: The design of complex machines stands as both a marker of human intelligence
and a foundation of engineering practice. Given recent advances in large
language models (LLMs), we ask whether they, too, can learn to create. We
approach this question through the lens of compositional machine design: a task
in which machines are assembled from standardized components to meet functional
demands like locomotion or manipulation in a simulated physical environment. To
support this investigation, we introduce BesiegeField, a testbed built on the
machine-building game Besiege, which enables part-based construction, physical
simulation and reward-driven evaluation. Using BesiegeField, we benchmark
state-of-the-art LLMs with agentic workflows and identify key capabilities
required for success, including spatial reasoning, strategic assembly, and
instruction-following. As current open-source models fall short, we explore
reinforcement learning (RL) as a path to improvement: we curate a cold-start
dataset, conduct RL finetuning experiments, and highlight open challenges at
the intersection of language, machine design, and physical reasoning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [166] [Large Language Models for Real-World IoT Device Identification](https://arxiv.org/abs/2510.13817)
*Rameen Mahmood,Tousif Ahmed,Sai Teja Peddinti,Danny Yuxing Huang*

Main category: cs.LG

TL;DR: 针对IoT设备识别挑战，本文将设备识别重构为语言建模任务。通过LLM集成生成高质量标签，并指令微调量化的LLaMA3.18B模型，在2015个供应商上实现了98.25%的top-1精度，并对多种干扰具有鲁棒性，证明了LLMs在大规模设备识别中的潜力。


<details>
  <summary>Details</summary>
Motivation: IoT设备数量激增，现有识别方法难以应对，带来安全、隐私和网络责任风险。开放环境中流量元数据不完整、有噪声或被混淆，加剧了这些挑战。

Method: 引入语义推理管道，将设备识别重构为异构网络元数据上的语言建模任务。利用基于互信息和熵稳定性的LLM集成，为大型真实IoT流量数据集（IoT Inspector）生成高保真供应商标签。通过课程学习指令微调量化的LLaMA3.18B模型，以提升稀疏和长尾供应商分布下的泛化能力。

Result: 模型在2,015个供应商中实现了98.25%的top-1准确率和90.73%的宏观准确率。同时，对缺失字段、协议漂移和对抗性操作保持了韧性。

Conclusion: 通过在独立IoT测试床上的评估、解释质量和对抗性压力测试，证明指令微调的LLMs为大规模真实世界设备识别提供了可扩展且可解释的基础。

Abstract: The rapid expansion of IoT devices has outpaced current identification
methods, creating significant risks for security, privacy, and network
accountability. These challenges are heightened in open-world environments,
where traffic metadata is often incomplete, noisy, or intentionally obfuscated.
We introduce a semantic inference pipeline that reframes device identification
as a language modeling task over heterogeneous network metadata. To construct
reliable supervision, we generate high-fidelity vendor labels for the IoT
Inspector dataset, the largest real-world IoT traffic corpus, using an ensemble
of large language models guided by mutual-information and entropy-based
stability scores. We then instruction-tune a quantized LLaMA3.18B model with
curriculum learning to support generalization under sparsity and long-tail
vendor distributions. Our model achieves 98.25% top-1 accuracy and 90.73% macro
accuracy across 2,015 vendors while maintaining resilience to missing fields,
protocol drift, and adversarial manipulation. Evaluation on an independent IoT
testbed, coupled with explanation quality and adversarial stress tests,
demonstrates that instruction-tuned LLMs provide a scalable and interpretable
foundation for real-world device identification at scale.

</details>


### [167] [Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation](https://arxiv.org/abs/2510.13864)
*Zixi Wang,Yushe Cao,Yubo Huang,Jinzhu Wei,Jingzehua Xu,Shuai Zhang,Xin Lai*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we propose a new method called Self-Training with Dynamic
Weighting (STDW), which aims to enhance robustness in Gradual Domain Adaptation
(GDA) by addressing the challenge of smooth knowledge migration from the source
to the target domain. Traditional GDA methods mitigate domain shift through
intermediate domains and self-training but often suffer from inefficient
knowledge migration or incomplete intermediate data. Our approach introduces a
dynamic weighting mechanism that adaptively balances the loss contributions of
the source and target domains during training. Specifically, we design an
optimization framework governed by a time-varying hyperparameter $\varrho$
(progressing from 0 to 1), which controls the strength of domain-specific
learning and ensures stable adaptation. The method leverages self-training to
generate pseudo-labels and optimizes a weighted objective function for
iterative model updates, maintaining robustness across intermediate domains.
Experiments on rotated MNIST, color-shifted MNIST, portrait datasets, and the
Cover Type dataset demonstrate that STDW outperforms existing baselines.
Ablation studies further validate the critical role of $\varrho$'s dynamic
scheduling in achieving progressive adaptation, confirming its effectiveness in
reducing domain bias and improving generalization. This work provides both
theoretical insights and a practical framework for robust gradual domain
adaptation, with potential applications in dynamic real-world scenarios. The
code is available at https://github.com/Dramwig/STDW.

</details>


### [168] [Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning](https://arxiv.org/abs/2510.13865)
*Dongkwan Lee,Junhoo Lee,Nojun Kwak*

Main category: cs.LG

TL;DR: 本文提出Deep Edge Filter，通过对深度神经网络特征进行高通滤波，以提高模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究假设神经网络将任务相关的语义信息编码在高频分量中，而将领域特异性偏差存储在低频分量中。

Method: 引入Deep Edge Filter，通过从原始特征中减去低通滤波输出，对深度神经网络特征执行高通滤波，从而分离出可泛化的表示，同时保持架构完整性。

Result: 在视觉、文本、3D和音频等多种领域，以及不同的模型架构和数据模态下，均显示出持续的性能改进。分析表明该方法能诱导特征稀疏化并有效分离高频分量，验证了核心假设。

Conclusion: Deep Edge Filter通过对深度特征应用高通滤波，有效隔离了高频分量中的任务相关语义信息，从而显著提高了模型在各种领域和模态下的泛化能力。

Abstract: We introduce the Deep Edge Filter, a novel approach that applies high-pass
filtering to deep neural network features to improve model generalizability.
Our method is motivated by our hypothesis that neural networks encode
task-relevant semantic information in high-frequency components while storing
domain-specific biases in low-frequency components of deep features. By
subtracting low-pass filtered outputs from original features, our approach
isolates generalizable representations while preserving architectural
integrity. Experimental results across diverse domains such as Vision, Text,
3D, and Audio demonstrate consistent performance improvements regardless of
model architecture and data modality. Analysis reveals that our method induces
feature sparsification and effectively isolates high-frequency components,
providing empirical validation of our core hypothesis. The code is available at
https://github.com/dongkwani/DeepEdgeFilter.

</details>


### [169] [CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks](https://arxiv.org/abs/2510.13869)
*Munsif Ali,Leonardo Rossi,Massimo Bertozzi*

Main category: cs.LG

TL;DR: 本文提出CoLoR-GAN，一个用于GAN的持续少样本学习框架，通过低秩适应（LoRA和LLoRA）显著减少模型参数，并在实现SOTA性能的同时大幅降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 在GAN中，持续学习（CL）和少样本学习（FS）是一个挑战，特别是如何在不产生灾难性遗忘的情况下从少量样本中学习。现有SOTA方法（如LFS-GAN）在每次迭代中引入大量新权重，导致长期参数显著增加。

Method: 引入CoLoR-GAN框架，利用低秩张量（LoRA）来高效地适应模型并减少所需参数。为进一步优化适配器大小，针对卷积层引入了LoRA-in-LoRA (LLoRA) 技术。此外，提供了一项经验研究以确定LoRA的最佳超参数。

Result: CoLoR-GAN在多个基准持续学习和少样本任务上表现出有效性，实现了SOTA性能，同时显著减少了所需的资源（参数数量）。

Conclusion: CoLoR-GAN通过创新的低秩适应方法（LoRA和LLoRA），成功解决了GAN中持续少样本学习的参数效率问题，在大幅降低资源消耗的同时保持了领先的性能。

Abstract: Continual learning (CL) in the context of Generative Adversarial Networks
(GANs) remains a challenging problem, particularly when it comes to learn from
a few-shot (FS) samples without catastrophic forgetting. Current most effective
state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible
quantity of new weights at each training iteration, which would become
significant when considering the long term. For this reason, this paper
introduces \textcolor{red}{\textbf{\underline{c}}}ontinual
few-sh\textcolor{red}{\textbf{\underline{o}}}t learning with
\textcolor{red}{\textbf{\underline{lo}}}w-\textcolor{red}{\textbf{\underline{r}}}ank
adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and
CL together, leveraging low-rank tensors to efficiently adapt the model to
target tasks while reducing even more the number of parameters required.
Applying a vanilla LoRA implementation already permitted us to obtain pretty
good results. In order to optimize even further the size of the adapters, we
challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for
convolutional layers. Finally, aware of the criticality linked to the choice of
the hyperparameters of LoRA, we provide an empirical study to easily find the
best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on
several benchmark CL and FS tasks and show that our model is efficient,
reaching SOTA performance but with a number of resources enormously reduced.
Source code is available on
\href{https://github.com/munsifali11/CoLoR-GAN}{Github.

</details>


### [170] [Joint Discriminative-Generative Modeling via Dual Adversarial Training](https://arxiv.org/abs/2510.13872)
*Xuwang Yin,Claire Zhang,Julie Steele,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 提出一种基于对抗训练（AT）的新型框架，解决了联合能量模型（JEM）在稳定性和生成质量上的局限，实现了鲁棒分类与高保真生成。


<details>
  <summary>Details</summary>
Motivation: 现有混合模型（如JEM）在单一框架内实现鲁棒分类和高保真生成面临挑战，且SGLD训练不稳定、样本质量差。

Method: 提出一个整合对抗训练（AT）原理的新型训练框架，主要创新点包括：1) 用稳定的AT方法替代SGLD-based JEM学习，通过BCE损失区分真实数据和PGD生成的对比样本来优化能量函数；2) 对判别器采用协同对抗训练，提高分类鲁棒性并消除显式梯度惩罚；3) 引入两阶段训练流程，解决批归一化与EBM训练的不兼容问题。

Result: 在CIFAR-10、CIFAR-100和ImageNet上，该方法显著提升了对抗鲁棒性，同时保持了有竞争力的生成性能。在ImageNet上，生成质量超越BigGAN并接近扩散模型，是首个在复杂高分辨率数据集上实现高质量生成的MCMC-based EBM方法。

Conclusion: 该方法解决了JEM扩展中的关键稳定性问题，并证明对抗训练可以作为统一框架的有效基础，实现视觉数据的生成和鲁棒分类。

Abstract: Simultaneously achieving robust classification and high-fidelity generative
modeling within a single framework presents a significant challenge. Hybrid
approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as
EBMs but are often limited by the instability and poor sample quality inherent
in SGLD-based training. We address these limitations by proposing a novel
training framework that integrates adversarial training (AT) principles for
both discriminative robustness and stable generative learning. The proposed
method introduces three key innovations: (1) the replacement of SGLD-based JEM
learning with a stable, AT-based approach that optimizes the energy function by
discriminating between real data and PGD-generated contrastive samples using
the BCE loss; (2) synergistic adversarial training for the discriminative
component that enhances classification robustness while eliminating the need
for explicit gradient penalties; and (3) a two-stage training procedure to
resolve the incompatibility between batch normalization and EBM training.
Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method
substantially improves adversarial robustness over existing hybrid models while
maintaining competitive generative performance. On ImageNet, when optimized for
generative modeling, our model's generative fidelity surpasses that of BigGAN
and approaches diffusion models, representing the first MCMC-based EBM approach
to achieve high-quality generation on complex, high-resolution datasets. Our
approach addresses key stability issues that have limited JEM scaling and
demonstrates that adversarial training can serve as an effective foundation for
unified frameworks capable of generating and robustly classifying visual data.

</details>


### [171] [K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding](https://arxiv.org/abs/2510.13891)
*Yifeng Yao,Yike Yun,Jing Wang,Huishuai Zhang,Dongyan Zhao,Ke Tian,Zhihao Wang,Minghui Qiu,Tao Wang*

Main category: cs.LG

TL;DR: 本文提出K-frames，一种新颖的场景驱动关键帧选择范式，通过选择语义连贯、查询相关的视频片段而非单个帧，解决多模态大语言模型（MLLMs）在长视频处理中面临的上下文窗口和信息丢失问题，并保留时间连续性，同时引入新的数据集PeakClips。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在长视频理解上面临上下文窗口和计算成本限制。现有均匀帧采样方法会导致大量信息丢失，而基于文本-帧检索或强化学习的关键帧选择方法通常产生稀疏且时间不连续的帧，忽略场景连续性，并且缺乏多尺度帧选择的灵活性。

Method: 引入K-frames范式，通过预测语义连贯、查询相关的视频片段来保留时间连续性，从而实现任意k关键帧选择以满足不同用户预算。为此，首先构建了PeakClips数据集（20万个由查询条件化的视频亮点）。在此数据集基础上，K-frames采用三阶段渐进式课程学习clip2frame选择：包括两个用于时间接地和关键片段感知的有监督微调（SFT）阶段，以及一个直接优化下游任务场景驱动预测策略的强化学习（RL）阶段。

Result: 在主要长视频理解基准上进行的广泛实验表明，K-frames为不同尺度的关键帧选择提供了一种有效、可解释且即插即用的解决方案。

Conclusion: K-frames提供了一种新颖、有效且灵活的场景驱动关键帧选择方案，解决了MLLMs处理长视频的挑战，通过选择片段而非单帧，显著提升了信息保留和时间连续性。研究团队将提供数据集和模型。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
capabilities in image understanding, but long-video are constrained by context
windows and computational cost. Uniform frame sampling often leads to
substantial information loss. Meanwhile existing keyframe selection methods
such as text-frame retrieval or RL-based frame optimization typically yield
sparse and temporally disjointed frames, overlooking scene continuity and
lacking flexibility for multi-scale frame selection. To address these
limitations, we introduce K-frames, a novel paradigm for scene-driven keyframe
selection that preserves temporal continuity. Instead of selecting individual
frames, K-frames predicts semantically coherent, query-relevant clips, which
enables any-k keyframes selection to meet diverse user budgets. To achieve this
approach, we first introduce PeakClips, a dataset of 200K video highlights
conditioned by query. Building on this dataset, K-frames learns clip2frame
selection using a three-stage progressive curriculum. It involves two
Supervised Fine-Tuning stages for temporal grounding and key-clip perception,
followed by a Reinforcement Learning stage that directly optimizes the
scene-driven prediction policy for downstream task without further annotations.
Extensive experiments on major long-video understanding benchmarks demonstrate
that K-frames provides an effective, interpretable, and plug-and-play solution
for keyframe selection at various scales. Our dataset and model will be
available.

</details>


### [172] [Multi-View Semi-Supervised Label Distribution Learning with Local Structure Complementarity](https://arxiv.org/abs/2510.13917)
*Yanshan Xiao,Kaihong Wu,Bo Liu*

Main category: cs.LG

TL;DR: 本文提出了一种多视图半监督标签分布学习方法（MVSS-LDL），通过利用不同视图之间局部近邻结构的互补性，解决了现有单视图监督学习方法的局限。


<details>
  <summary>Details</summary>
Motivation: 现有的标签分布学习（LDL）方法主要针对单视图有标签数据，而多视图半监督LDL问题（即包含有标签和无标签数据的多视图场景）尚未被考虑。

Method: 该方法首先在每个视图中通过计算k近邻来探索局部结构。然后，通过整合来自其他视图的近邻信息来补充每个视图的近邻集，以获得更全面的描述（利用局部结构互补性）。最后，基于每个视图的互补近邻集，构建了一个基于图学习的多视图半监督LDL模型。

Result: 数值研究表明，MVSS-LDL比现有的单视图LDL方法取得了明显更好的分类性能。据作者所知，这是首次尝试解决多视图LDL问题。

Conclusion: MVSS-LDL方法通过有效利用多视图局部近邻结构的互补性，成功解决了多视图半监督标签分布学习问题，并显著提升了分类表现。

Abstract: Label distribution learning (LDL) is a paradigm that each sample is
associated with a label distribution. At present, the existing approaches are
proposed for the single-view LDL problem with labeled data, while the
multi-view LDL problem with labeled and unlabeled data has not been considered.
In this paper, we put forward the multi-view semi-supervised label distribution
learning with local structure complementarity (MVSS-LDL) approach, which
exploits the local nearest neighbor structure of each view and emphasizes the
complementarity of local nearest neighbor structures in multiple views.
Specifically speaking, we first explore the local structure of view $v$ by
computing the $k$-nearest neighbors. As a result, the $k$-nearest neighbor set
of each sample $\boldsymbol{x}_i$ in view $v$ is attained. Nevertheless, this
$k$-nearest neighbor set describes only a part of the nearest neighbor
information of sample $\boldsymbol{x}_i$. In order to obtain a more
comprehensive description of sample $\boldsymbol{x}_i$'s nearest neighbors, we
complement the nearest neighbor set in view $v$ by incorporating sample
$\boldsymbol{x}_i$'s nearest neighbors in other views. Lastly, based on the
complemented nearest neighbor set in each view, a graph learning-based
multi-view semi-supervised LDL model is constructed. By considering the
complementarity of local nearest neighbor structures, different views can
mutually provide the local structural information to complement each other. To
the best of our knowledge, this is the first attempt at multi-view LDL.
Numerical studies have demonstrated that MVSS-LDL attains explicitly better
classification performance than the existing single-view LDL methods.

</details>


### [173] [Weight Weaving: Parameter Pooling for Data-Free Model Merging](https://arxiv.org/abs/2510.13921)
*Levy Chaves,Eduardo Valle,Sandra Avila*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Model merging provides a cost-effective and data-efficient combination of
specialized deep neural networks through parameter integration. This technique
leverages expert models across downstream tasks without requiring retraining.
Most model merging approaches critically depend on scaling hyper-parameters
$\lambda$, which weight each model's contribution globally or individually.
Principled approaches for setting scaling factors without accessing any data
(data-free) are scarce, often leading researchers to tune $\lambda$ using
privileged data from the evaluation set, which is obviously unfeasible in
practice. To address this limitation, we introduce Weight Weaving, a
plug-and-play technique that pools model weights across $\lambda$ values search
space using user-defined pooling functions, such as averaging, random
selection, or even existing model merging methods. Our method demonstrates high
modularity, imposing minimal constraints on the search space. It operates
orthogonally to existing model merging methods and eliminates evaluation data
requirements. We validate Weight Weaving across three ViT variants in three
experimental setups: vision multi-task learning, vision continual learning, and
domain generalization. Our method consistently improves the performance of
several model merging methods, achieving average accuracy gains of up to 15.9
percentage points in a data-free setting.

</details>


### [174] [LTR-ICD: A Learning-to-Rank Approach for Automatic ICD Coding](https://arxiv.org/abs/2510.13922)
*Mohammad Mansoori,Amira Soliman,Farzaneh Etminani*

Main category: cs.LG

TL;DR: 本文提出一种结合分类和排序的新方法来自动化临床笔记的ICD编码任务，显著提升了高优先级编码的识别和排序准确性，并超越了现有最佳分类模型的性能。


<details>
  <summary>Details</summary>
Motivation: 自动化临床笔记的ICD编码分配和排序任务具有挑战性。现有最先进的方法将此问题视为纯粹的分类任务，从而忽略了ICD编码顺序的关键性，而编码顺序对医疗诊断和报销至关重要。

Method: 本文首次尝试将ICD编码任务从检索系统的角度出发，将其重新定义为分类和排序任务，以解决传统方法忽略编码顺序的问题。

Result: 提出的框架在识别高优先级编码方面表现出卓越能力，例如，主要诊断代码的正确排序准确率达到47%，远高于现有最佳分类器的20%。此外，在分类指标上，模型的微观F1分数为0.6065，宏观F1分数为0.2904，均超越了先前最佳模型的0.597和0.2660。

Conclusion: 通过将ICD编码任务视为分类与排序的结合，新方法有效解决了传统分类方法忽视编码顺序的问题，并在高优先级编码识别和整体分类性能上均取得了显著提升。

Abstract: Clinical notes contain unstructured text provided by clinicians during
patient encounters. These notes are usually accompanied by a sequence of
diagnostic codes following the International Classification of Diseases (ICD).
Correctly assigning and ordering ICD codes are essential for medical diagnosis
and reimbursement. However, automating this task remains challenging.
State-of-the-art methods treated this problem as a classification task, leading
to ignoring the order of ICD codes that is essential for different purposes. In
this work, as a first attempt, we approach this task from a retrieval system
perspective to consider the order of codes, thus formulating this problem as a
classification and ranking task. Our results and analysis show that the
proposed framework has a superior ability to identify high-priority codes
compared to other methods. For instance, our model accuracy in correctly
ranking primary diagnosis codes is 47%, compared to 20% for the
state-of-the-art classifier. Additionally, in terms of classification metrics,
the proposed model achieves a micro- and macro-F1 scores of 0.6065 and 0.2904,
respectively, surpassing the previous best model with scores of 0.597 and
0.2660.

</details>


### [175] [Intelligent Dynamic Handover via AI-assisted Signal Quality Prediction in 6G Multi-RAT Networks](https://arxiv.org/abs/2510.14832)
*Maria Lamprini A. Bartsioka,Anastasios Giannopoulos,Sotirios Spantideas*

Main category: cs.LG

TL;DR: 本文提出一种基于机器学习的预测条件切换（P-CHO）框架，通过预测信号质量，在6G多无线接入技术（multi-RAT）网络中实现准确、低延迟、主动的切换，减少切换失败和乒乓效应。


<details>
  <summary>Details</summary>
Motivation: 6G多RAT网络（蜂窝与WiFi共存）在快速信道动态、干扰和异构覆盖下，需要可靠的移动性决策。当前多RAT部署中的切换仍然是高反应性且事件触发的，依赖瞬时测量和阈值事件，可靠性不足。

Method: 提出一种ML辅助的预测条件切换（P-CHO）框架，基于模型驱动和短时信号质量预测。通过RAT转向控制器协调P-CHO工作流程，标准化数据收集、并行每RAT预测、基于迟滞的决策逻辑和CHO执行。使用RAT感知的长短期记忆（LSTM）网络预测移动用户沿随机轨迹的信号质量指标，并在不同蜂窝和IEEE 802.11 WiFi集成覆盖信道模型下进行训练和评估。研究LSTM模型超参数调优影响，并比较直接多步与递归P-CHO变体，以及与基线预测器的对比。

Result: 通过软硬切换设置测试，结果表明，启用迟滞的P-CHO方案能够有效减少切换失败和乒乓事件。

Conclusion: 所提出的P-CHO框架能够实现准确、低延迟、主动的切换，适用于6G多RAT部署中ML辅助的切换转向。

Abstract: The emerging paradigm of 6G multiple Radio Access Technology (multi-RAT)
networks, where cellular and Wireless Fidelity (WiFi) transmitters coexist,
requires mobility decisions that remain reliable under fast channel dynamics,
interference, and heterogeneous coverage. Handover in multi-RAT deployments is
still highly reactive and event-triggered, relying on instantaneous
measurements and threshold events. This work proposes a Machine Learning
(ML)-assisted Predictive Conditional Handover (P-CHO) framework based on a
model-driven and short-horizon signal quality forecasts. We present a
generalized P-CHO sequence workflow orchestrated by a RAT Steering Controller,
which standardizes data collection, parallel per-RAT predictions, decision
logic with hysteresis-based conditions, and CHO execution. Considering a
realistic multi-RAT environment, we train RAT-aware Long Short Term Memory
(LSTM) networks to forecast the signal quality indicators of mobile users along
randomized trajectories. The proposed P-CHO models are trained and evaluated
under different channel models for cellular and IEEE 802.11 WiFi integrated
coverage. We study the impact of hyperparameter tuning of LSTM models under
different system settings, and compare direct multi-step versus recursive P-CHO
variants. Comparisons against baseline predictors are also carried out.
Finally, the proposed P-CHO is tested under soft and hard handover settings,
showing that hysteresis-enabled P-CHO scheme is able to reduce handover
failures and ping-pong events. Overall, the proposed P-CHO framework can enable
accurate, low-latency, and proactive handovers suitable for ML-assisted
handover steering in 6G multi-RAT deployments.

</details>


### [176] [Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse Problems](https://arxiv.org/abs/2510.13972)
*George Webber,Andrew J. Reader*

Main category: cs.LG

TL;DR: 本研究提出一种名为分布一致性 (DC) 损失的新型数据保真度函数，通过评估测量值与噪声分布的统计一致性来避免逆问题中的噪声过拟合，并在图像去噪和医学图像重建中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的点对点数据保真度损失函数（如均方误差MSE）在逆问题中倾向于与噪声测量值精确匹配，导致模型容易过拟合到噪声。

Method: 引入分布一致性 (DC) 损失，它通过使用基于模型的概率分数进行分布级别校准，将点对点匹配替换为对观测测量值是否与当前估计隐含的噪声分布统计一致性的集合评估。

Result: 在深度图像先验的图像去噪中，DC损失无需提前停止并获得更高的PSNR。在泊松噪声医学图像重建中，DC损失减少了高度迭代重建中的伪影，并增强了手工正则化的效果。

Conclusion: DC损失是一种基于统计学原理、性能优于传统保真度损失的替代方案，适用于许多已知噪声分布且测量数据集包含许多独立噪声值的实际逆问题。

Abstract: Recovering true signals from noisy measurements is a central challenge in
inverse problems spanning medical imaging, geophysics, and signal processing.
Current solutions balance prior assumptions regarding the true signal
(regularization) with agreement to noisy measured data (data-fidelity).
Conventional data-fidelity loss functions, such as mean-squared error (MSE) or
negative log-likelihood, seek pointwise agreement with noisy measurements,
often leading to overfitting to noise. In this work, we instead evaluate
data-fidelity collectively by testing whether the observed measurements are
statistically consistent with the noise distributions implied by the current
estimate. We adopt this aggregated perspective and introduce distributional
consistency (DC) loss, a data-fidelity objective that replaces pointwise
matching with distribution-level calibration using model-based probability
scores for each measurement. DC loss acts as a direct and practical plug-in
replacement for standard data consistency terms: i) it is compatible with
modern regularizers, ii) it is optimized in the same way as traditional losses,
and iii) it avoids overfitting to measurement noise even without the use of
priors. Its scope naturally fits many practical inverse problems where the
measurement-noise distribution is known and where the measured dataset consists
of many independent noisy values. We demonstrate efficacy in two key example
application areas: i) in image denoising with deep image prior, using DC
instead of MSE loss removes the need for early stopping and achieves higher
PSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss
reduces artifacts in highly-iterated reconstructions and enhances the efficacy
of hand-crafted regularization. These results position DC loss as a
statistically grounded, performance-enhancing alternative to conventional
fidelity losses for inverse problems.

</details>


### [177] [BitNet Distillation](https://arxiv.org/abs/2510.13998)
*Xun Wu,Shaohan Huang,Wenhui Wang,Ting Song,Li Dong,Yan Xia,Furu Wei*

Main category: cs.LG

TL;DR: BitDistill是一种轻量级流程，能将全精度LLM精调为1.58比特精度（三元权重），在特定下游任务上实现与全精度模型相当的性能，并带来显著的计算成本节约（内存和推理速度）。


<details>
  <summary>Details</summary>
Motivation: 旨在通过将LLM量化到极低的1.58比特精度，以最小的计算成本实现强大的任务特定性能，解决全精度模型在内存和推理效率上的挑战。

Method: 开发了BitDistill流程，用于将现成的全精度LLM（如Qwen）精调为1.58比特精度。该方法结合了三项关键技术：BitNet中的SubLN模块、基于MiniLM的多头注意力蒸馏，以及作为重要预热步骤的持续预训练，以缓解全精度和1.58比特LLM之间的性能差距问题。

Result: 实验结果表明，BitDistill在各种模型尺寸上实现了与全精度对应模型相当的性能，同时在CPU上实现了高达10倍的内存节省和2.65倍的推理速度提升。

Conclusion: BitDistill成功地将LLM量化到1.58比特精度，在保持竞争性任务性能的同时，显著降低了计算资源消耗（内存和推理时间），为特定下游任务提供了高效且高性能的解决方案。

Abstract: In this paper, we present BitNet Distillation (BitDistill), a lightweight
pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into
1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream
tasks, achieving strong task-specific performance with minimal computational
cost. Specifically, BitDistill incorporates three key techniques: the SubLN
module, as introduced in BitNet; multi-head attention distillation, based on
MiniLM; and continual pre-training, which serves as a crucial warm-up step to
mitigate the scalability issue of the performance gap between finetuned
full-precision and 1.58-bit LLMs on specific tasks. Experimental results show
that BitDistill achieves performance comparable to the full-precision
counterpart models across model size, while enabling up to 10x memory savings
and 2.65x faster inference on CPUs. Code is available at
https://github.com/microsoft/BitNet.

</details>


### [178] [REAP the Experts: Why Pruning Prevails for One-Shot MoE compression](https://arxiv.org/abs/2510.13999)
*Mike Lasby,Ivan Lazarevich,Nish Sinnadurai,Sean Lie,Yani Ioannou,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: 为解决SMoE模型内存开销问题，本文提出Router-weighted Expert Activation Pruning (REAP) 剪枝方法，并在理论和实践中证明其在生成任务上优于专家合并，实现了高效近无损的专家压缩。


<details>
  <summary>Details</summary>
Motivation: Sparsely-activated Mixture-of-Experts (SMoE) 模型虽能实现高效预训练和低延迟，但其庞大的参数量导致显著的内存开销，因此需要进行专家压缩。

Method: 通过理论证明专家合并会导致“功能子空间坍塌”，从而引入不可约误差，指出专家剪枝更适用于生成任务。在此洞察基础上，提出Router-weighted Expert Activation Pruning (REAP) 方法，该剪枝准则综合考虑了路由器门控值和专家激活范数。

Result: 在20B到1T参数的各类SMoE模型上，REAP在生成任务基准测试中持续优于专家合并及其他剪枝方法，尤其在50%压缩率下表现突出。在Qwen3-Coder-480B和Kimi-K2模型上，即使剪枝50%的专家，也能在代码生成和工具调用任务中实现近乎无损的压缩。

Conclusion: 专家剪枝是生成任务上优于专家合并的SMoE模型压缩策略。REAP方法通过结合路由器门控值和专家激活范数，能够有效实现专家模型的压缩，并在生成任务中达到卓越的性能和效率，尤其在代码生成和工具调用等复杂任务上表现突出。

Abstract: Sparsely-activated Mixture-of-Experts (SMoE) models offer efficient
pre-training and low latency but their large parameter counts create
significant memory overhead, motivating research into expert compression.
Contrary to recent findings favouring expert merging on discriminative
benchmarks, we demonstrate that expert pruning is a superior strategy for
generative tasks. We prove that merging introduces an irreducible error by
causing a "functional subspace collapse", due to the loss of the router's
independent, input-dependent control over experts. Leveraging this insight, we
propose Router-weighted Expert Activation Pruning (REAP), a novel pruning
criterion that considers both router gate-values and expert activation norms.
Across a diverse set of SMoE models ranging from 20B to 1T parameters, REAP
consistently outperforms merging and other pruning methods on generative
benchmarks, especially at 50% compression. Notably, our method achieves
near-lossless compression on code generation and tool-calling tasks with
Qwen3-Coder-480B and Kimi-K2, even after pruning 50% of experts.

</details>


### [179] [Conditional Clifford-Steerable CNNs with Complete Kernel Basis for PDE Modeling](https://arxiv.org/abs/2510.14007)
*Bálint László Szarvas,Maksim Zhdanov*

Main category: cs.LG

TL;DR: 本文指出Clifford-Steerable CNNs (CSCNNs)的核基不完整限制了其表达能力，并提出条件Clifford可控核（Conditional Clifford-Steerable Kernels），通过引入输入依赖的等变表示来增强核，从而提高模型表达能力，并在PDE预测任务中超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有Clifford-Steerable CNNs (CSCNNs)的核基不完整，限制了模型的表达能力。

Method: 提出条件Clifford可控核（Conditional Clifford-Steerable Kernels），通过输入特征场计算的等变表示来增强核。推导了输入依赖核的等变约束，并通过隐式参数化实现高效求解。

Result: 在流体动力学和相对论电动力学等多个偏微分方程（PDE）预测任务中，新框架显著提高了表达能力，并持续优于基线方法。

Conclusion: 提出的条件Clifford可控核有效解决了CSCNNs核基不完整的问题，提高了模型表达能力，并在相关任务中展现出卓越性能。

Abstract: Clifford-Steerable CNNs (CSCNNs) provide a unified framework that allows
incorporating equivariance to arbitrary pseudo-Euclidean groups, including
isometries of Euclidean space and Minkowski spacetime. In this work, we
demonstrate that the kernel basis of CSCNNs is not complete, thus limiting the
model expressivity. To address this issue, we propose Conditional
Clifford-Steerable Kernels, which augment the kernels with equivariant
representations computed from the input feature field. We derive the
equivariance constraint for these input-dependent kernels and show how it can
be solved efficiently via implicit parameterization. We empirically demonstrate
an improved expressivity of the resulting framework on multiple PDE forecasting
tasks, including fluid dynamics and relativistic electrodynamics, where our
method consistently outperforms baseline methods.

</details>


### [180] [Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization for Deep Neural Network Training](https://arxiv.org/abs/2510.14009)
*Jie Hao,Xiaochuan Gong,Jie Xu,Zhengdao Wang,Mingrui Liu*

Main category: cs.LG

TL;DR: 本文提出一种噪声自适应的分层学习率方案，结合几何感知优化算法，通过动态调整各层学习率，解决了同一组内曲率异构问题，显著加速了深度神经网络的训练收敛。


<details>
  <summary>Details</summary>
Motivation: 几何感知优化算法在训练深度神经网络时表现出色，但它们对同一范数组内的不同层施加固定的学习率。然而，这些层内的局部曲率（如锐度）实际上是异构的，并且在训练过程中动态变化，导致固定学习率的方法效率低下。

Method: 本文在几何感知优化算法的基础上，引入了一种噪声自适应的分层学习率方案。该方法在训练过程中实时估计由所选LMO（线性最小化预言机）诱导的对偶范数中的梯度方差，并利用该方差为同一组内的各层分配时变的、噪声自适应的分层学习率。

Result: 与在组内使用固定学习率的方法相比，该方法显著加速了深度神经网络的训练。理论分析表明，算法实现了尖锐的收敛速度。在LLaMA和GPT等Transformer架构上的实证结果表明，该方法比现有最先进的优化器收敛更快。

Conclusion: 所提出的噪声自适应分层学习率方案，有效解决了几何感知优化器中层间学习率固定导致的效率问题，通过动态适应各层噪声，在理论和实践上都展现出对深度神经网络（特别是大型Transformer模型）更快的训练收敛速度。

Abstract: Geometry-aware optimization algorithms, such as Muon, have achieved
remarkable success in training deep neural networks (DNNs). These methods
leverage the underlying geometry of DNNs by selecting appropriate norms for
different layers and updating parameters via norm-constrained linear
minimization oracles (LMOs). However, even within a group of layers associated
with the same norm, the local curvature can be heterogeneous across layers and
vary dynamically over the course of training. For example, recent work shows
that sharpness varies substantially across transformer layers and throughout
training, yet standard geometry-aware optimizers impose fixed learning rates to
layers within the same group, which may be inefficient for DNN training.
  In this paper, we introduce a noise-adaptive layerwise learning rate scheme
on top of geometry-aware optimization algorithms and substantially accelerate
DNN training compared to methods that use fixed learning rates within each
group. Our method estimates gradient variance in the dual norm induced by the
chosen LMO on the fly, and uses it to assign time-varying noise-adaptive
layerwise learning rates within each group. We provide a theoretical analysis
showing that our algorithm achieves a sharp convergence rate. Empirical results
on transformer architectures such as LLaMA and GPT demonstrate that our
approach achieves faster convergence than state-of-the-art optimizers.

</details>


### [181] [Context-Selective State Space Models: Feedback is All You Need](https://arxiv.org/abs/2510.14027)
*Riccardo Zattra,Giacomo Baggio,Umberto Casti,Augusto Ferrante,Francesco Ticozzi*

Main category: cs.LG

TL;DR: 本文提出COFFEE模型，一种新型时变状态空间模型（SSM），通过引入状态反馈实现上下文依赖的选择性，有效解决了Transformer和现有SSM（如S6）在长序列依赖和效率上的局限性，并在多项任务上取得了显著优于S6的性能和参数效率。


<details>
  <summary>Details</summary>
Motivation: Transformer模型存在二次复杂度高和处理长程依赖困难的问题。虽然状态空间模型（SSM）是高效替代方案，但现有SSM（如S6）的选择性机制仅依赖于当前输入，未能充分利用序列历史上下文来捕获长程依赖。

Method: 引入COFFEE（COntext From FEEdback）模型，一个新型时变SSM。它通过将选择性机制从内部状态（作为序列历史的紧凑表示）计算而非仅当前输入来融入状态反馈，从而实现上下文依赖的动态调节。此外，COFFEE采用高效的模型参数化方法，消除了S6中存在的冗余。

Result: 在induction head任务上，COFFEE以比S6少两个数量级的参数和训练序列实现了近乎完美的准确率。在MNIST任务上，COFFEE在相同架构下以仅3585个参数达到了97%的准确率，显著优于S6。

Conclusion: 研究结果表明，状态反馈是构建可扩展和高效序列模型的关键机制，COFFEE模型验证了其在提升长程依赖捕获能力和模型效率方面的有效性。

Abstract: Transformers, powered by the attention mechanism, are the backbone of most
foundation models, yet they suffer from quadratic complexity and difficulties
in dealing with long-range dependencies in the input sequence. Recent work has
shown that state space models (SSMs) provide an efficient alternative, with the
S6 module at the core of the Mamba architecture achieving state-of-the-art
results on long-sequence benchmarks. In this paper, we introduce the COFFEE
(COntext From FEEdback) model, a novel time-varying SSM that incorporates state
feedback to enable context-dependent selectivity, while still allowing for
parallel implementation. Whereas the selectivity mechanism of S6 only depends
on the current input, COFFEE computes it from the internal state, which serves
as a compact representation of the sequence history. This shift allows the
model to regulate its dynamics based on accumulated context, improving its
ability to capture long-range dependencies. In addition to state feedback, we
employ an efficient model parametrization that removes redundancies present in
S6 and leads to a more compact and trainable formulation. On the induction head
task, COFFEE achieves near-perfect accuracy with two orders of magnitude fewer
parameters and training sequences compared to S6. On MNIST, COFFEE largely
outperforms S6 within the same architecture, reaching 97% accuracy with only
3585 parameters. These results showcase the role of state feedback as a key
mechanism for building scalable and efficient sequence models.

</details>


### [182] [CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations](https://arxiv.org/abs/2510.14049)
*Guangyi Chen,Yunlong Deng,Peiyuan Zhu,Yan Li,Yifan Sheng,Zijian Li,Kun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一个名为CausalVerse的全新CRL基准，使用高保真模拟视觉数据，解决了现有CRL评估中真实性与评估精确度之间的矛盾，并提供了灵活的因果结构访问。


<details>
  <summary>Details</summary>
Motivation: 因果表示学习（CRL）旨在揭示数据生成过程并识别潜在的因果变量和关系，但其评估因需要已知的真实因果变量和结构而极具挑战。现有评估方法常依赖于过于简化的合成数据集或下游任务的性能，导致在真实性和评估精确度之间难以取舍。

Method: 引入了一个新的CRL基准，该基准使用高保真模拟视觉数据，既保留了逼真的视觉复杂性，又提供了对真实因果生成过程的访问。该数据集包含约20万张图像和300万帧视频，涵盖静态图像生成、动态物理模拟、机器人操作和交通情境分析等四个领域的24个子场景。此外，它还提供了对底层因果结构的灵活访问，允许用户修改或配置以符合CRL所需的假设。

Result: 创建了一个全面的测试平台，有望弥合严格评估与实际适用性之间的差距。利用该基准，作者评估了不同范式的代表性CRL方法，并提供了实证见解，以帮助从业者和新手选择或扩展适当的CRL框架，从而正确解决可以从CRL角度受益的特定类型实际问题。

Conclusion: 该基准为CRL提供了一个既真实又精确的评估环境，通过弥合理论评估与实际应用之间的鸿沟，有效协助了从业者和研究人员选择并优化CRL框架，以应对复杂的现实问题。

Abstract: Causal Representation Learning (CRL) aims to uncover the data-generating
process and identify the underlying causal variables and relations, whose
evaluation remains inherently challenging due to the requirement of known
ground-truth causal variables and causal structure. Existing evaluations often
rely on either simplistic synthetic datasets or downstream performance on
real-world tasks, generally suffering a dilemma between realism and evaluative
precision. In this paper, we introduce a new benchmark for CRL using
high-fidelity simulated visual data that retains both realistic visual
complexity and, more importantly, access to ground-truth causal generating
processes. The dataset comprises around 200 thousand images and 3 million video
frames across 24 sub-scenes in four domains: static image generation, dynamic
physical simulations, robotic manipulations, and traffic situation analysis.
These scenarios range from static to dynamic settings, simple to complex
structures, and single to multi-agent interactions, offering a comprehensive
testbed that hopefully bridges the gap between rigorous evaluation and
real-world applicability. In addition, we provide flexible access to the
underlying causal structures, allowing users to modify or configure them to
align with the required assumptions in CRL, such as available domain labels,
temporal dependencies, or intervention histories. Leveraging this benchmark, we
evaluated representative CRL methods across diverse paradigms and offered
empirical insights to assist practitioners and newcomers in choosing or
extending appropriate CRL frameworks to properly address specific types of real
problems that can benefit from the CRL perspective. Welcome to visit our:
Project page:https://causal-verse.github.io/,
Dataset:https://huggingface.co/CausalVerse.

</details>


### [183] [FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients](https://arxiv.org/abs/2510.14054)
*Fatih Ilhan,Selim Furkan Tekin,Tiansheng Huang,Gaowen Liu,Ramana Kompella,Greg Eisenhauer,Yingyan Celine Lin,Calton Pu,Ling Liu*

Main category: cs.LG

TL;DR: 本文提出FedHFT，一个高效且个性化的联邦微调框架，旨在解决预训练大语言模型在分布式环境中进行NLU应用微调时面临的数据隐私和资源异构性挑战。


<details>
  <summary>Details</summary>
Motivation: 预训练大语言模型（LLMs）的微调在个性化NLU应用中日益普及，但面临两大挑战：1) 受限或异构数据（因数据保密性或隐私要求），2) 参与客户端（如边缘设备）计算资源差异大。

Method: FedHFT框架采用两种主要方法：1) 引入“混合掩码适配器”以应对客户端资源异构性，实现高性能分布式协作微调，同时确保数据本地化。2) 提出“双层优化方法”，基于掩码个性化和客户端聚类来处理非独立同分布（non-iid）数据分布。

Result: 通过广泛实验证明，在数据和资源异构条件下，FedHFT在各种自然语言理解任务上相比现有异构联邦学习方法，实现了显著的性能和效率提升。

Conclusion: FedHFT框架成功解决了LLMs联邦微调中的数据隐私、资源异构和非iid数据挑战，为分布式NLU应用提供了高效和个性化的解决方案。

Abstract: Fine-tuning pre-trained large language models (LLMs) has become a common
practice for personalized natural language understanding (NLU) applications on
downstream tasks and domain-specific datasets. However, there are two main
challenges: (i) limited and/or heterogeneous data for fine-tuning due to
proprietary data confidentiality or privacy requirements, and (ii) varying
computation resources available across participating clients such as edge
devices. This paper presents FedHFT - an efficient and personalized federated
fine-tuning framework to address both challenges. First, we introduce a mixture
of masked adapters to handle resource heterogeneity across participating
clients, enabling high-performance collaborative fine-tuning of pre-trained
language model(s) across multiple clients in a distributed setting, while
keeping proprietary data local. Second, we introduce a bi-level optimization
approach to handle non-iid data distribution based on masked personalization
and client clustering. Extensive experiments demonstrate significant
performance and efficiency improvements over various natural language
understanding tasks under data and resource heterogeneity compared to
representative heterogeneous federated learning methods.

</details>


### [184] [On the expressivity of sparse maxout networks](https://arxiv.org/abs/2510.14068)
*Moritz Grillo,Tobias Hofmann*

Main category: cs.LG

TL;DR: 本研究分析了稀疏maxout网络的表达能力，通过建立网络功能与虚拟多胞形的对偶关系，证明了在固定入度约束下，网络的表达能力存在深度层次结构，且深度是关键，宽度无法单独弥补深度不足。


<details>
  <summary>Details</summary>
Motivation: 稀疏maxout网络具有卷积神经网络(CNN)和图神经网络(GNN)的关键特性，因此研究其表达能力有助于深入理解这些广泛使用的网络架构。

Method: 研究建立了稀疏maxout网络可计算函数与一类虚拟多胞形之间的对偶关系，将网络的几何特性与表达能力联系起来。核心工具是推导出了相关多胞形维度的紧密界限。在此基础上，构建了一系列深度层次结构。

Result: 研究发现足够深的稀疏maxout网络具有普适性。然而，如果网络未能达到所需的深度，则仅靠增加宽度无法弥补固定入度约束下的稀疏性所导致的表达能力限制。

Conclusion: 对于稀疏maxout网络（具有固定入度），其表达能力存在显著的深度依赖性。深度是实现网络普适表达能力的关键因素，而宽度在深度不足时无法单独弥补稀疏性带来的表达能力限制。

Abstract: We study the expressivity of sparse maxout networks, where each neuron takes
a fixed number of inputs from the previous layer and employs a, possibly
multi-argument, maxout activation. This setting captures key characteristics of
convolutional or graph neural networks. We establish a duality between
functions computable by such networks and a class of virtual polytopes, linking
their geometry to questions of network expressivity. In particular, we derive a
tight bound on the dimension of the associated polytopes, which serves as the
central tool for our analysis. Building on this, we construct a sequence of
depth hierarchies. While sufficiently deep sparse maxout networks are
universal, we prove that if the required depth is not reached, width alone
cannot compensate for the sparsity of a fixed indegree constraint.

</details>


### [185] [Exploratory Causal Inference in SAEnce](https://arxiv.org/abs/2510.14073)
*Tommaso Mencattini,Riccardo Cadei,Francesco Locatello*

Main category: cs.LG

TL;DR: 本文提出“神经效应搜索”，一种新颖的无监督方法，旨在利用预训练基础模型和稀疏自编码器直接从非结构化数据中发现未知的因果效应，并通过递归分层过程解决多重检验和效应缠结问题，并在真实世界科学试验中成功实现了因果效应识别。


<details>
  <summary>Details</summary>
Motivation: 传统的随机对照试验（RCT）依赖于人工假设和昂贵的分析，这限制了因果效应估计的规模化，并可能导致研究局限于流行但不完整的假设。因此，需要一种能直接从数据中发现未知处理效应的方法。

Method: 1. 将试验中的非结构化数据通过预训练基础模型转化为有意义的表示。2. 使用稀疏自编码器解释这些表示。3. 引入“神经效应搜索”（Neural Effect Search），这是一种递归过程，通过渐进分层（progressive stratification）解决了神经层面因果效应发现中的多重检验和效应缠结问题。

Result: 该算法在半合成实验中展示了其鲁棒性。在实验生态学背景下，本文首次成功实现了对真实世界科学试验的无监督因果效应识别。

Conclusion: 本研究提出了一种创新方法，能够克服传统RCT的局限性，实现从数据中直接、无监督地发现因果效应，为大规模因果效应估计开辟了新途径。

Abstract: Randomized Controlled Trials are one of the pillars of science; nevertheless,
they rely on hand-crafted hypotheses and expensive analysis. Such constraints
prevent causal effect estimation at scale, potentially anchoring on popular yet
incomplete hypotheses. We propose to discover the unknown effects of a
treatment directly from data. For this, we turn unstructured data from a trial
into meaningful representations via pretrained foundation models and interpret
them via a sparse autoencoder. However, discovering significant causal effects
at the neural level is not trivial due to multiple-testing issues and effects
entanglement. To address these challenges, we introduce Neural Effect Search, a
novel recursive procedure solving both issues by progressive stratification.
After assessing the robustness of our algorithm on semi-synthetic experiments,
we showcase, in the context of experimental ecology, the first successful
unsupervised causal effect identification on a real-world scientific trial.

</details>


### [186] [Neural Network approximation power on homogeneous and heterogeneous reaction-diffusion equations](https://arxiv.org/abs/2510.14094)
*Haotian Feng*

Main category: cs.LG

TL;DR: 本文为神经网络近似反应扩散方程的解提供了理论分析，证明了多层神经网络的近似能力。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络越来越多地用于求解微分方程，但其有效性的理论基础尚未得到充分探索。

Method: 基于通用逼近定理，对神经网络近似一维和二维反应扩散方程的理论能力进行了分析。

Result: 研究表明，两层神经网络可以近似一维反应扩散方程，而三层神经网络可以近似其二维对应物。此理论框架可推广至椭圆和抛物线方程。

Conclusion: 该工作揭示了神经网络在近似反应扩散方程及相关偏微分方程解方面的强大表达能力，为基于神经网络的微分方程求解器奠定了理论基础。

Abstract: Reaction-diffusion systems represent one of the most fundamental formulations
used to describe a wide range of physical, chemical, and biological processes.
With the increasing adoption of neural networks, recent research has focused on
solving differential equations using machine learning techniques. However, the
theoretical foundation explaining why neural networks can effectively
approximate such solutions remains insufficiently explored.
  This paper provides a theoretical analysis of the approximation power of
neural networks for one- and two-dimensional reaction-diffusion equations in
both homogeneous and heterogeneous media. Building upon the universal
approximation theorem, we demonstrate that a two-layer neural network can
approximate the one-dimensional reaction-diffusion equation, while a
three-layer neural network can approximate its two-dimensional counterpart. The
theoretical framework presented here can be further extended to elliptic and
parabolic equations.
  Overall, this work highlights the expressive power of neural networks in
approximating solutions to reaction-diffusion equations and related PDEs,
providing a theoretical foundation for neural network-based differential
equation solvers.

</details>


### [187] [Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning](https://arxiv.org/abs/2510.14095)
*Awni Altabaa,Siyu Chen,John Lafferty,Zhuoran Yang*

Main category: cs.LG

TL;DR: 该研究通过引入四种架构机制（输入自适应循环、算法监督、离散瓶颈锚定表示和错误校正），显著提升了Transformer网络在OOD任务上的算法泛化能力，并对其作用机制进行了可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 机器学习中训练分布外系统性、组合泛化能力仍是核心挑战，也是现代语言模型推理能力的关键瓶颈。

Method: 1. 在Transformer网络中，以GSM8K风格的计算图上的模算术任务为测试平台，研究OOD泛化。 2. 引入并探索了四种增强OOD泛化的架构机制：输入自适应循环、算法监督、通过离散瓶颈锚定潜在表示、显式错误校正。 3. 辅以详细的机制可解释性分析。

Result: 1. 这些机制共同为Transformer网络提供了一种原生且可扩展的潜在空间推理架构方法。 2. 该方法展现了强大的算法泛化能力。 3. 可解释性分析揭示了这些机制如何带来鲁棒的OOD泛化能力。

Conclusion: 通过结合特定的架构机制（输入自适应循环、算法监督、离散瓶颈和错误校正），可以显著增强Transformer网络在OOD任务上的算法泛化能力，使其在潜在空间推理方面更具鲁棒性和可扩展性。

Abstract: Systematic, compositional generalization beyond the training distribution
remains a core challenge in machine learning -- and a critical bottleneck for
the emergent reasoning abilities of modern language models. This work
investigates out-of-distribution (OOD) generalization in Transformer networks
using a GSM8K-style modular arithmetic on computational graphs task as a
testbed. We introduce and explore a set of four architectural mechanisms aimed
at enhancing OOD generalization: (i) input-adaptive recurrence; (ii)
algorithmic supervision; (iii) anchored latent representations via a discrete
bottleneck; and (iv) an explicit error-correction mechanism. Collectively,
these mechanisms yield an architectural approach for native and scalable latent
space reasoning in Transformer networks with robust algorithmic generalization
capabilities. We complement these empirical results with a detailed mechanistic
interpretability analysis that reveals how these mechanisms give rise to robust
OOD generalization abilities.

</details>


### [188] [TENDE: Transfer Entropy Neural Diffusion Estimation](https://arxiv.org/abs/2510.14096)
*Simon Pedro Galeano Munoz,Mustapha Bounoua,Giulio Franzese,Pietro Michiardi,Maurizio Filippone*

Main category: cs.LG

TL;DR: TENDE是一种基于分数扩散模型的新型传输熵估计方法，它通过条件互信息克服了现有方法的局局限性，提供了更准确、鲁棒和可扩展的估计。


<details>
  <summary>Details</summary>
Motivation: 现有传输熵估计方法面临维数灾难、严格的分布假设或需要指数级大数据集才能可靠收敛等问题。

Method: 提出TENDE（传输熵神经网络扩散估计），利用基于分数的扩散模型，通过估计条件互信息来估计传输熵。该方法通过学习相关条件分布的分数函数，实现灵活、可扩展的估计，并对底层数据生成过程做最小假设。

Result: 在合成基准和真实数据上，TENDE相较于现有神经网络估计器和其他先进方法，展示了卓越的准确性和鲁棒性。

Conclusion: TENDE通过引入扩散模型，有效解决了传输熵估计的挑战，提供了一种更准确、鲁棒且灵活的解决方案。

Abstract: Transfer entropy measures directed information flow in time series, and it
has become a fundamental quantity in applications spanning neuroscience,
finance, and complex systems analysis. However, existing estimation methods
suffer from the curse of dimensionality, require restrictive distributional
assumptions, or need exponentially large datasets for reliable convergence. We
address these limitations in the literature by proposing TENDE (Transfer
Entropy Neural Diffusion Estimation), a novel approach that leverages
score-based diffusion models to estimate transfer entropy through conditional
mutual information. By learning score functions of the relevant conditional
distributions, TENDE provides flexible, scalable estimation while making
minimal assumptions about the underlying data-generating process. We
demonstrate superior accuracy and robustness compared to existing neural
estimators and other state-of-the-art approaches across synthetic benchmarks
and real data.

</details>


### [189] [Near-Optimal Regret-Queue Length Tradeoff in Online Learning for Two-Sided Markets](https://arxiv.org/abs/2510.14097)
*Zixian Yang,Sushil Mahavir Varma,Lei Ying*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study a two-sided market, wherein, price-sensitive heterogeneous customers
and servers arrive and join their respective queues. A compatible
customer-server pair can then be matched by the platform, at which point, they
leave the system. Our objective is to design pricing and matching algorithms
that maximize the platform's profit, while maintaining reasonable queue
lengths. As the demand and supply curves governing the price-dependent arrival
rates may not be known in practice, we design a novel online-learning-based
pricing policy and establish its near-optimality. In particular, we prove a
tradeoff among three performance metrics: $\tilde{O}(T^{1-\gamma})$ regret,
$\tilde{O}(T^{\gamma/2})$ average queue length, and $\tilde{O}(T^{\gamma})$
maximum queue length for $\gamma \in (0, 1/6]$, significantly improving over
existing results [1]. Moreover, barring the permissible range of $\gamma$, we
show that this trade-off between regret and average queue length is optimal up
to logarithmic factors under a class of policies, matching the optimal one as
in [2] which assumes the demand and supply curves to be known. Our proposed
policy has two noteworthy features: a dynamic component that optimizes the
tradeoff between low regret and small queue lengths; and a probabilistic
component that resolves the tension between obtaining useful samples for fast
learning and maintaining small queue lengths.

</details>


### [190] [Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey](https://arxiv.org/abs/2510.14114)
*Yazid Janati,Alain Durmus,Jimmy Olsson,Eric Moulines*

Main category: cs.LG

TL;DR: 综述了利用预训练扩散模型结合蒙特卡洛方法，通过“扭曲”中间分布解决贝叶斯逆问题，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为先验在解决贝叶斯逆问题中展现出巨大潜力。

Method: 该综述分析了结合预训练扩散模型与蒙特卡洛方法来解决贝叶斯逆问题，且无需额外训练。其核心方法是采用“扭曲”机制处理扩散过程中的中间分布，并辅以蒙特卡洛方法进行采样。

Result: 研究发现，这些方法主要通过“扭曲”扩散过程中的中间分布来引导模拟趋向后验分布。

Conclusion: 预训练扩散模型结合蒙特卡洛方法，通过独特的“扭曲”机制，为解决贝叶斯逆问题提供了一条无需额外训练的有效途径。

Abstract: Diffusion models enable the synthesis of highly accurate samples from complex
distributions and have become foundational in generative modeling. Recently,
they have demonstrated significant potential for solving Bayesian inverse
problems by serving as priors. This review offers a comprehensive overview of
current methods that leverage \emph{pre-trained} diffusion models alongside
Monte Carlo methods to address Bayesian inverse problems without requiring
additional training. We show that these methods primarily employ a
\emph{twisting} mechanism for the intermediate distributions within the
diffusion process, guiding the simulations toward the posterior distribution.
We describe how various Monte Carlo methods are then used to aid in sampling
from these twisted distributions.

</details>


### [191] [Neural Network-enabled Domain-consistent Robust Optimisation for Global CO$_2$ Reduction Potential of Gas Power Plants](https://arxiv.org/abs/2510.14125)
*Waqar Muhammad Ashraf,Talha Ansar,Abdulelah S. Alshehri,Peipei Chen,Ramit Debnath,Vivek Dua*

Main category: cs.LG

TL;DR: 本文提出一个由神经网络驱动的鲁棒优化框架，将数据驱动领域作为约束集成到非线性规划中，解决了优化中领域不一致解的问题。应用于燃气电厂，实现了0.76%的能效提升，并预估全球每年可减少26 Mt的CO2排放。


<details>
  <summary>Details</summary>
Motivation: 现有参数化神经网络模型与优化求解器交互时，存在产生领域不一致解决方案的问题，这影响了优化结果的可靠性，且此问题常被忽视。

Method: 引入了一个神经网络驱动的鲁棒优化框架，该框架将数据驱动的领域知识作为约束集成到非线性规划技术中。

Result: 应用于一座1180 MW的联合循环燃气电厂，该框架提供了领域一致的鲁棒最优解，并实现了平均0.76个百分点的能效提升。推算至全球燃气电厂，每年可减少26 Mt的CO2排放（亚洲10.6 Mt，美洲9.0 Mt，欧洲4.5 Mt）。

Conclusion: 研究结果强调了机器学习在为全球气候行动提供近期、可扩展的脱碳途径方面的协同作用。

Abstract: We introduce a neural network-driven robust optimisation framework that
integrates data-driven domain as a constraint into the nonlinear programming
technique, addressing the overlooked issue of domain-inconsistent solutions
arising from the interaction of parametrised neural network models with
optimisation solvers. Applied to a 1180 MW capacity combined cycle gas power
plant, our framework delivers domain-consistent robust optimal solutions that
achieve a verified 0.76 percentage point mean improvement in energy efficiency.
For the first time, scaling this efficiency gain to the global fleet of gas
power plants, we estimate an annual 26 Mt reduction potential in CO$_2$ (with
10.6 Mt in Asia, 9.0 Mt in the Americas, and 4.5 Mt in Europe). These results
underscore the synergetic role of machine learning in delivering near-term,
scalable decarbonisation pathways for global climate action.

</details>


### [192] [Demystifying the Mechanisms Behind Emergent Exploration in Goal-conditioned RL](https://arxiv.org/abs/2510.14129)
*Mahsa Bastankhah,Grace Liu,Dilip Arumugam,Thomas L. Griffiths,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 本研究阐明了无监督强化学习算法 SGCRL 中 emergent exploration 的机制。SGCRL 通过学习低秩状态表征和塑造隐式奖励，在无外部奖励下实现长时序目标达成任务中的自主探索和后续利用。


<details>
  <summary>Details</summary>
Motivation: 理解无监督强化学习中 emergent exploration 的驱动机制，特别是 Single-Goal Contrastive Reinforcement Learning (SGCRL) 如何在无外部奖励或课程的情况下解决复杂的长时序目标达成任务。

Method: 结合对 SGCRL 算法目标函数的理论分析和受控实验。

Result: SGCRL 最大化由其学习到的表征所塑造的隐式奖励。这些表征自动修改奖励格局，在达到目标前促进探索，之后促进利用。实验表明，这些探索动态源于学习状态空间的低秩表征，而非神经网络函数逼近。

Conclusion: 对 SGCRL 探索机制的深入理解使其能够适应并执行安全感知的探索。

Abstract: In this work, we take a first step toward elucidating the mechanisms behind
emergent exploration in unsupervised reinforcement learning. We study
Single-Goal Contrastive Reinforcement Learning (SGCRL), a self-supervised
algorithm capable of solving challenging long-horizon goal-reaching tasks
without external rewards or curricula. We combine theoretical analysis of the
algorithm's objective function with controlled experiments to understand what
drives its exploration. We show that SGCRL maximizes implicit rewards shaped by
its learned representations. These representations automatically modify the
reward landscape to promote exploration before reaching the goal and
exploitation thereafter. Our experiments also demonstrate that these
exploration dynamics arise from learning low-rank representations of the state
space rather than from neural network function approximation. Our improved
understanding enables us to adapt SGCRL to perform safety-aware exploration.

</details>


### [193] [Learning Wireless Interference Patterns: Decoupled GNN for Throughput Prediction in Heterogeneous Multi-Hop p-CSMA Networks](https://arxiv.org/abs/2510.14137)
*Faezeh Dehghan Tarzjani,Bhaskar Krishnamachari*

Main category: cs.LG

TL;DR: 本文提出解耦图卷积网络（D-GCN），通过分离节点自身传输与邻居干扰，实现异构多跳无线网络饱和吞吐量的准确（3.3% NMAE）且可扩展预测，并支持网络优化。


<details>
  <summary>Details</summary>
Motivation: 异构多跳无线网络中p-persistent CSMA饱和吞吐量预测极具挑战。简化模型不准确，精确马尔可夫链分析计算量指数级增长不可行。现有GNN因混淆直接与级联干扰而性能不佳（63.94% NMAE），亟需可扩展且高精度的结构化机器学习方法。

Method: 我们提出了D-GCN，一种新颖的图卷积网络架构。其核心在于显式地将节点自身传输概率的处理与其邻居干扰效应的处理分离。D-GCN通过用可学习的注意力机制替代均值聚合，获得可解释的逐邻居贡献权重，并有效捕捉复杂的多跳干扰模式。

Result: D-GCN在异构网络上实现了3.3%的归一化平均绝对误差（NMAE），显著优于现有强基线方法。它在大规模网络中保持计算可行性，即使精确分析方法变得不可行。此外，D-GCN还支持基于梯度的网络优化，性能可达到理论最优值的1%以内。

Conclusion: D-GCN通过其创新的解耦架构和注意力机制，成功克服了异构多跳无线网络中饱和吞吐量预测的准确性和可扩展性难题。它不仅提供了高精度的预测，还为无线网络优化提供了有效的工具，具有重要的理论与实践意义。

Abstract: The p-persistent CSMA protocol is central to random-access MAC analysis, but
predicting saturation throughput in heterogeneous multi-hop wireless networks
remains a hard problem. Simplified models that assume a single, shared
interference domain can underestimate throughput by 48--62\% in sparse
topologies. Exact Markov-chain analyses are accurate but scale exponentially in
computation time, making them impractical for large networks. These
computational barriers motivate structural machine learning approaches like
GNNs for scalable throughput prediction in general network topologies. Yet
off-the-shelf GNNs struggle here: a standard GCN yields 63.94\% normalized mean
absolute error (NMAE) on heterogeneous networks because symmetric normalization
conflates a node's direct interference with higher-order, cascading effects
that pertain to how interference propagates over the network graph.
  Building on these insights, we propose the Decoupled Graph Convolutional
Network (D-GCN), a novel architecture that explicitly separates processing of a
node's own transmission probability from neighbor interference effects. D-GCN
replaces mean aggregation with learnable attention, yielding interpretable,
per-neighbor contribution weights while capturing complex multihop interference
patterns. D-GCN attains 3.3\% NMAE, outperforms strong baselines, remains
tractable even when exact analytical methods become computationally infeasible,
and enables gradient-based network optimization that achieves within 1\% of
theoretical optima.

</details>


### [194] [Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks](https://arxiv.org/abs/2510.14139)
*Islam Akef Ebeid,Haoteng Tang,Pengfei Gu*

Main category: cs.LG

TL;DR: 提出ProtGram-DirectGCN框架，通过将蛋白质一级结构建模为方向n-gram图，并使用专门的方向图卷积网络，实现高效的蛋白质-蛋白质相互作用预测，尤其在数据受限时表现出色。


<details>
  <summary>Details</summary>
Motivation: 准确预测蛋白质-蛋白质相互作用对理解细胞功能和药物开发至关重要。现有方法（如PLMs的序列嵌入或GNNs的3D结构）计算密集，本研究旨在探索计算成本较低的替代方案。

Method: 引入ProtGram-DirectGCN两阶段框架。ProtGram将蛋白质一级结构建模为层级n-gram图，边权重由残基转换概率定义。DirectGCN是定制的方向图卷积神经网络，其卷积层通过独立的入边、出边、无向和共享路径变换处理信息，并通过可学习门控机制组合，用于学习残基级嵌入，并通过注意力池化生成蛋白质级嵌入进行预测。

Result: DirectGCN在标准节点分类基准上表现与现有方法相当，并擅长处理复杂、有向、密集、异质图。应用于PPI预测时，完整的ProtGram-DirectGCN框架展现出强大的预测能力，即使在训练数据有限的情况下也表现稳健。

Conclusion: ProtGram-DirectGCN框架为蛋白质-蛋白质相互作用预测提供了一种高效且鲁棒的替代方案，尤其适用于数据有限的场景，有效解决了现有方法的计算密集问题。

Abstract: Introduction Accurate prediction of protein-protein interactions (PPIs) is
crucial for understanding cellular functions and advancing drug development.
Existing in-silico methods use direct sequence embeddings from Protein Language
Models (PLMs). Others use Graph Neural Networks (GNNs) for 3D protein
structures. This study explores less computationally intensive alternatives. We
introduce a novel framework for downstream PPI prediction through link
prediction. Methods We introduce a two-stage graph representation learning
framework, ProtGram-DirectGCN. First, we developed ProtGram. This approach
models a protein's primary structure as a hierarchy of globally inferred n-gram
graphs. In these graphs, residue transition probabilities define edge weights.
Each edge connects a pair of residues in a directed graph. The probabilities
are aggregated from a large corpus of sequences. Second, we propose DirectGCN,
a custom directed graph convolutional neural network. This model features a
unique convolutional layer. It processes information through separate
path-specific transformations: incoming, outgoing, and undirected. A shared
transformation is also applied. These paths are combined via a learnable gating
mechanism. We apply DirectGCN to ProtGram graphs to learn residue-level
embeddings. These embeddings are pooled via attention to generate protein-level
embeddings for prediction. Results We first established the efficacy of
DirectGCN on standard node classification benchmarks. Its performance matches
established methods on general datasets. The model excels at complex, directed
graphs with dense, heterophilic structures. When applied to PPI prediction, the
full ProtGram-DirectGCN framework delivers robust predictive power. This strong
performance holds even with limited training data.

</details>


### [195] [On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model](https://arxiv.org/abs/2510.14156)
*Jan Kwiatkowski,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: 本文系统评估了Transformer模型结合多种先进损失函数（点对、成对、列表）在S&P 500数据上进行股票日收益排名，以优化基于排名的投资组合选择。


<details>
  <summary>Details</summary>
Motivation: 量化交易策略依赖准确的股票排名，而现有研究对Transformer模型在金融时间序列中不同损失函数对股票排名能力的影响了解不足。标准损失函数不足以直接学习股票收益的正确顺序，且缺乏对信息检索领域先进排序损失函数在金融领域与Transformer模型结合使用的全面比较。

Method: 通过系统评估一系列包括点对、成对和列表损失函数在内的先进损失函数，用于日度股票收益预测，以促进基于排名的投资组合选择。研究在S&P 500数据上进行，重点评估每种损失函数如何影响模型识别资产间盈利性相对排序的能力。

Result: 研究贡献了一个全面的基准，揭示了不同损失函数如何影响模型学习对于投资组合选择至关重要的横截面和时间模式的能力。

Conclusion: 为优化基于排名的交易策略提供了实用指导，通过深入理解不同损失函数对模型学习金融资产相对排序能力的影响。

Abstract: Quantitative trading strategies rely on accurately ranking stocks to identify
profitable investments. Effective portfolio management requires models that can
reliably order future stock returns. Transformer models are promising for
understanding financial time series, but how different training loss functions
affect their ability to rank stocks well is not yet fully understood. Financial
markets are challenging due to their changing nature and complex relationships
between stocks. Standard loss functions, which aim for simple prediction
accuracy, often aren't enough. They don't directly teach models to learn the
correct order of stock returns. While many advanced ranking losses exist from
fields such as information retrieval, there hasn't been a thorough comparison
to see how well they work for ranking financial returns, especially when used
with modern Transformer models for stock selection. This paper addresses this
gap by systematically evaluating a diverse set of advanced loss functions
including pointwise, pairwise, listwise for daily stock return forecasting to
facilitate rank-based portfolio selection on S&P 500 data. We focus on
assessing how each loss function influences the model's ability to discern
profitable relative orderings among assets. Our research contributes a
comprehensive benchmark revealing how different loss functions impact a model's
ability to learn cross-sectional and temporal patterns crucial for portfolio
selection, thereby offering practical guidance for optimizing ranking-based
trading strategies.

</details>


### [196] [Data Understanding Survey: Pursuing Improved Dataset Characterization Via Tensor-based Methods](https://arxiv.org/abs/2510.14161)
*Matthew D. Merris,Tim Andersen*

Main category: cs.LG

TL;DR: 现有数据集表征方法存在局限，本文倡导使用张量方法作为更鲁棒的替代方案，以增强数据理解和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在机器学习和数据分析领域，现有统计、结构和模型分析等数据集表征方法，难以提供创新和可解释性所需的深度理解和洞察力。

Method: 调查了当前最先进的传统数据分析技术及其局限性；讨论了多种张量方法，并提出其作为传统数据集表征方法的更鲁棒替代方案；通过实例说明张量方法的优势。

Result: 张量方法能够揭示细微的数据特征，提供增强的解释性和可操作的智能。

Conclusion: 倡导采用基于张量的表征方法，以期在理解复杂数据集方面取得突破，并为智能、可解释的数据驱动发现铺平道路。

Abstract: In the evolving domains of Machine Learning and Data Analytics, existing
dataset characterization methods such as statistical, structural, and
model-based analyses often fail to deliver the deep understanding and insights
essential for innovation and explainability. This work surveys the current
state-of-the-art conventional data analytic techniques and examines their
limitations, and discusses a variety of tensor-based methods and how these may
provide a more robust alternative to traditional statistical, structural, and
model-based dataset characterization techniques. Through examples, we
illustrate how tensor methods unveil nuanced data characteristics, offering
enhanced interpretability and actionable intelligence. We advocate for the
adoption of tensor-based characterization, promising a leap forward in
understanding complex datasets and paving the way for intelligent, explainable
data-driven discoveries.

</details>


### [197] [Towards Reversible Model Merging For Low-rank Weights](https://arxiv.org/abs/2510.14163)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 本文提出可逆模型合并（RMM）方法，通过构建紧凑基（模型空间）来恢复原始任务特定模型，有效解决传统合并方法在低秩压缩模型上性能显著下降的问题。


<details>
  <summary>Details</summary>
Motivation: 传统模型合并方法在处理经低秩压缩（如LoRA或SVD）的模型时，会导致合并模型性能严重下降，而现有研究大多忽视了这种情况。

Method: 本文提出一种不同于传统方法的“可逆模型合并”（RMM）方法。该方法不将所有适配器合并为一个权重集，而是构建一个紧凑的基（模型空间），通过线性组合可以从中恢复出原始的任务特定模型。RMM是一种高效、无数据且灵活的方法，为选择最优模型权重基和任务特定系数提供了封闭式解。

Result: RMM在各种数据集和模型规模的广泛实验中，始终优于现有合并方法，显著保持了低秩压缩模型的性能。

Conclusion: RMM通过创建一个可重建任务特定模型的模型空间，成功解决了低秩压缩模型合并的性能下降问题，证明了其在模型合并领域的优越性和有效性。

Abstract: Model merging aims to combine multiple fine-tuned models into a single set of
weights that performs well across all source tasks. While prior work has shown
that merging can approximate the performance of individual fine-tuned models
for each task, it largely overlooks scenarios where models are compressed into
low-rank representations, either through low-rank adaptation (LoRA) or
post-training singular value decomposition (SVD). We first demonstrate that
applying conventional merging methods to low-rank weights leads to severe
performance degradation in the merged model. Motivated by this phenomenon, we
propose a fundamentally different approach: instead of collapsing all adapters
into one set of weights, we construct a compact basis (e.g., an equivalent of
holding two or more models) from which original task-specific models can be
recovered via linear combination. This reframes merging as generating a
reconstruction-capable model space rather than producing a single merged model.
Crucially, this allows us to ``revert'' to each individual model when needed,
recognizing that no merged model can consistently outperform one specialized
for its task. Building on this insight, we introduce our method, Reversible
Model Merging (RMM), an efficient, data-free, and flexible method that provides
a closed-form solution for selecting the optimal basis of model weights and
task-specific coefficients for linear combination. Extensive experiments across
diverse datasets and model scales demonstrate that RMM consistently outperforms
existing merging approaches, preserving the performance of low-rank compressed
models by a significant margin.

</details>


### [198] [Optimal Control Theoretic Neural Optimizer: From Backpropagation to Dynamic Programming](https://arxiv.org/abs/2510.14168)
*Guan-Horng Liu,Tianrong Chen,Evangelos A. Theodorou*

Main category: cs.LG

TL;DR: 本文将深度神经网络（DNN）优化视为最优控制问题，通过揭示反向传播与动态规划的算法相似性，提出一种新型优化器OCNOpt，显著提升了DNN训练的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络（DNN）优化是机器学习和人工智能发展的核心驱动力。将DNN解释为动态系统已为理论分析奠定基础，而本文旨在从算法角度，基于反向传播与动态规划在最优控制条件下（通过逆向过程）的显著算法相似性，寻求DNN优化新方法。

Method: 该研究通过整合反向传播的变分结构与动态规划的联系，通过求解一阶展开的近似动态规划问题，开发出一类探索Bellman方程高阶展开的新型优化方法。具体实现为“最优控制理论神经优化器 (OCNOpt)”。

Result: OCNOpt在多项实验中表现出比现有方法更好的鲁棒性和效率，同时保持了可控的计算复杂度。该优化器还支持层级反馈策略、博弈论应用以及Neural ODEs等连续时间模型的高阶训练。

Conclusion: OCNOpt的提出为基于动态系统和最优控制理论的原理性算法设计提供了新途径，并在实际应用中证明了其在DNN优化方面超越现有方法的性能。

Abstract: Optimization of deep neural networks (DNNs) has been a driving force in the
advancement of modern machine learning and artificial intelligence. With DNNs
characterized by a prolonged sequence of nonlinear propagation, determining
their optimal parameters given an objective naturally fits within the framework
of Optimal Control Programming. Such an interpretation of DNNs as dynamical
systems has proven crucial in offering a theoretical foundation for principled
analysis from numerical equations to physics. In parallel to these theoretical
pursuits, this paper focuses on an algorithmic perspective. Our motivated
observation is the striking algorithmic resemblance between the Backpropagation
algorithm for computing gradients in DNNs and the optimality conditions for
dynamical systems, expressed through another backward process known as dynamic
programming. Consolidating this connection, where Backpropagation admits a
variational structure, solving an approximate dynamic programming up to the
first-order expansion leads to a new class of optimization methods exploring
higher-order expansions of the Bellman equation. The resulting optimizer,
termed Optimal Control Theoretic Neural Optimizer (OCNOpt), enables rich
algorithmic opportunities, including layer-wise feedback policies,
game-theoretic applications, and higher-order training of continuous-time
models such as Neural ODEs. Extensive experiments demonstrate that OCNOpt
improves upon existing methods in robustness and efficiency while maintaining
manageable computational complexity, paving new avenues for principled
algorithmic design grounded in dynamical systems and optimal control theory.

</details>


### [199] [MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation](https://arxiv.org/abs/2510.14184)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Main category: cs.LG

TL;DR: MAFA是一个生产级多智能体框架，通过可配置协作解决金融服务领域的标注积压问题，已在摩根大通部署，消除了百万级积压，显著提高了效率和准确性，并为企业级标注提供了解决方案。


<details>
  <summary>Details</summary>
Motivation: 金融服务领域存在数百万客户话语需要准确分类，导致严重的标注积压。需要一个高效、可配置且能动态适应不同标注类型的系统来解决这一挑战。

Method: 提出了MAFA (Multi-Agent Framework for Annotation)，一个生产部署的多智能体框架。它结合了专业化智能体、结构化推理和基于判官的共识机制，通过配置而非代码修改支持动态任务适应。该系统已部署在摩根大通。

Result: MAFA消除了100万个话语的标注积压，与人类标注员的平均一致性达到86%，每年节省超过5,000小时的人工标注工作。系统处理的话语置信度分类通常为85%高、10%中、5%低，使人类专注于疑难案例。在内部数据集上，比基线提高了13.8%的Top-1准确率、15.1%的Top-5准确率和16.9%的F1值，并在公共基准上取得类似收益。

Conclusion: 该工作成功地将多智能体系统应用于企业实际部署，为面临大规模标注挑战的组织提供了实用蓝图，并展示了其在效率、准确性和可扩展性方面的显著优势。

Abstract: We present MAFA (Multi-Agent Framework for Annotation), a production-deployed
system that transforms enterprise-scale annotation workflows through
configurable multi-agent collaboration. Addressing the critical challenge of
annotation backlogs in financial services, where millions of customer
utterances require accurate categorization, MAFA combines specialized agents
with structured reasoning and a judge-based consensus mechanism. Our framework
uniquely supports dynamic task adaptation, allowing organizations to define
custom annotation types (FAQs, intents, entities, or domain-specific
categories) through configuration rather than code changes. Deployed at JP
Morgan Chase, MAFA has eliminated a 1 million utterance backlog while
achieving, on average, 86% agreement with human annotators, annually saving
over 5,000 hours of manual annotation work. The system processes utterances
with annotation confidence classifications, which are typically 85% high, 10%
medium, and 5% low across all datasets we tested. This enables human annotators
to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's
effectiveness across multiple datasets and languages, showing consistent
improvements over traditional and single-agent annotation baselines: 13.8%
higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1
in our internal intent classification dataset and similar gains on public
benchmarks. This work bridges the gap between theoretical multi-agent systems
and practical enterprise deployment, providing a blueprint for organizations
facing similar annotation challenges.

</details>


### [200] [Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation](https://arxiv.org/abs/2510.14190)
*Ruchi Sandilya,Sumaira Perez,Charles Lynch,Lindsay Victoria,Benjamin Zebley,Derrick Matthew Buchanan,Mahendra T. Bhati,Nolan Williams,Timothy J. Spellman,Faith M. Gunning,Conor Liston,Logan Grosenick*

Main category: cs.LG

TL;DR: ConDA框架利用对比学习来组织扩散模型的潜在空间，使其与系统动力学对齐，从而实现可解释的控制和忠实的生成，并在多项基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成方面表现卓越，但其潜在空间缺乏明确组织，难以进行可解释的控制。研究动机在于通过对比学习恢复更解耦和结构化的表示，以使潜在空间的遍历方向反映底层的动态因素。

Method: 引入ConDA（对比扩散对齐）框架，该框架在扩散嵌入中应用对比学习，旨在将潜在几何与系统动力学对齐。通过组织扩散模型的潜在空间，ConDA使其遍历方向反映底层的动态因素，从而支持非线性轨迹遍历，实现忠实的插值、外推和可控生成。

Result: 在流体动力学、神经钙成像、治疗性神经刺激和面部表情等多个基准测试中，ConDA生成了可解释的潜在表示，与线性遍历和基于条件的基线相比，可控性显著提高。

Conclusion: 研究结果表明，扩散模型的潜在空间编码了与动力学相关的结构，但要有效利用这些结构，需要对潜在空间进行组织并沿着潜在流形进行遍历。

Abstract: Diffusion models excel at generation, but their latent spaces are not
explicitly organized for interpretable control. We introduce ConDA (Contrastive
Diffusion Alignment), a framework that applies contrastive learning within
diffusion embeddings to align latent geometry with system dynamics. Motivated
by recent advances showing that contrastive objectives can recover more
disentangled and structured representations, ConDA organizes diffusion latents
such that traversal directions reflect underlying dynamical factors. Within
this contrastively structured space, ConDA enables nonlinear trajectory
traversal that supports faithful interpolation, extrapolation, and controllable
generation. Across benchmarks in fluid dynamics, neural calcium imaging,
therapeutic neurostimulation, and facial expression, ConDA produces
interpretable latent representations with improved controllability compared to
linear traversals and conditioning-based baselines. These results suggest that
diffusion latents encode dynamics-relevant structure, but exploiting this
structure requires latent organization and traversal along the latent manifold.

</details>


### [201] [Incentive-Based Federated Learning](https://arxiv.org/abs/2510.14208)
*Chanuka A. S. Hewa Kaluannakkage,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 联邦学习面临参与困境，本章探讨如何运用经济学、博弈论及区块链、深度强化学习等技术设计激励机制，以克服挑战并确保其在医疗、智能基础设施等实际应用中的成功，并提出了一个全面的分类法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在实际应用中存在“参与困境”，即参与方不愿贡献或可能搭便车，这限制了其适应性。因此，需要识别并解决设计有效激励机制的挑战，以促进联邦学习的实际落地和可持续发展。

Method: 1. 识别联邦学习激励机制设计中的基本挑战。2. 考察经济学和博弈论的基础概念应用。3. 结合区块链和深度强化学习等技术解决方案。4. 提出一个涵盖中心化和去中心化架构的全面分类法。5. 从医疗、智能基础设施、车载网络等工业应用视角阐述所提概念。

Result: 1. 明确了联邦学习激励机制设计的基本挑战。2. 展示了经济学、博弈论、区块链和深度强化学习在构建激励机制中的潜力。3. 提出了一个涵盖中心化和去中心化架构的全面激励机制分类法。4. 从应用角度证明了精心设计的激励机制对于联邦学习的实际成功至关重要。

Conclusion: 精心设计的激励机制是联邦学习实际成功不可或缺的组成部分。尽管已涌现出有前景的解决方案，但在构建真正可持续、公平和健壮的联邦学习生态系统方面仍存在重大挑战。

Abstract: Federated learning promises to revolutionize machine learning by enabling
collaborative model training without compromising data privacy. However,
practical adaptability can be limited by critical factors, such as the
participation dilemma. Participating entities are often unwilling to contribute
to a learning system unless they receive some benefits, or they may pretend to
participate and free-ride on others. This chapter identifies the fundamental
challenges in designing incentive mechanisms for federated learning systems. It
examines how foundational concepts from economics and game theory can be
applied to federated learning, alongside technology-driven solutions such as
blockchain and deep reinforcement learning. This work presents a comprehensive
taxonomy that thoroughly covers both centralized and decentralized
architectures based on the aforementioned theoretical concepts. Furthermore,
the concepts described are presented from an application perspective, covering
emerging industrial applications, including healthcare, smart infrastructure,
vehicular networks, and blockchain-based decentralized systems. Through this
exploration, this chapter demonstrates that well-designed incentive mechanisms
are not merely optional features but essential components for the practical
success of federated learning. This analysis reveals both the promising
solutions that have emerged and the significant challenges that remain in
building truly sustainable, fair, and robust federated learning ecosystems.

</details>


### [202] [Spectral Analysis of Molecular Kernels: When Richer Features Do Not Guarantee Better Generalization](https://arxiv.org/abs/2510.14217)
*Asma Jamali,Tin Sum Cheng,Rodrigo A. Vargas-Hernández*

Main category: cs.LG

TL;DR: 分子核函数的频谱分析挑战了“频谱越丰富泛化能力越好”的普遍观念，发现仅少量主导特征即可获得良好性能，对数据受限任务评估有启发。


<details>
  <summary>Details</summary>
Motivation: 核方法在低数据环境下具有鲁棒性和清晰理论基础，但在分子属性预测中，针对分子核函数的系统性频谱分析却非常稀缺。

Method: 对QM9数据集，使用核岭回归（KRR），并结合分子指纹、预训练Transformer、全局和局部3D表示，对七种分子属性进行了首次全面频谱分析。同时，引入截断核方法以探究频谱与预测性能的关系。

Result: 研究发现，频谱特征的丰富性（通过四种不同指标衡量）并未始终提高预测准确性。对于Transformer和局部3D表示，频谱丰富性甚至与性能呈负相关。许多核函数仅保留前2%的特征值即可恢复几乎所有性能，表明主要特征值捕获了最有效的信息。

Conclusion: 本研究挑战了“频谱越丰富，泛化能力越好”的普遍经验法则，揭示了表示、核特征和预测性能之间复杂微妙的关系，为在数据受限的科学和实际任务中评估核方法和自监督学习提供了新的见解。

Abstract: Understanding the spectral properties of kernels offers a principled
perspective on generalization and representation quality. While deep models
achieve state-of-the-art accuracy in molecular property prediction, kernel
methods remain widely used for their robustness in low-data regimes and
transparent theoretical grounding. Despite extensive studies of kernel spectra
in machine learning, systematic spectral analyses of molecular kernels are
scarce. In this work, we provide the first comprehensive spectral analysis of
kernel ridge regression on the QM9 dataset, molecular fingerprint, pretrained
transformer-based, global and local 3D representations across seven molecular
properties. Surprisingly, richer spectral features, measured by four different
spectral metrics, do not consistently improve accuracy. Pearson correlation
tests further reveal that for transformer-based and local 3D representations,
spectral richness can even have a negative correlation with performance. We
also implement truncated kernels to probe the relationship between spectrum and
predictive performance: in many kernels, retaining only the top 2% of
eigenvalues recovers nearly all performance, indicating that the leading
eigenvalues capture the most informative features. Our results challenge the
common heuristic that "richer spectra yield better generalization" and
highlight nuanced relationships between representation, kernel features, and
predictive performance. Beyond molecular property prediction, these findings
inform how kernel and self-supervised learning methods are evaluated in
data-limited scientific and real-world tasks.

</details>


### [203] [When Flatness Does (Not) Guarantee Adversarial Robustness](https://arxiv.org/abs/2510.14231)
*Nils Philipp Walter,Linara Adilova,Jilles Vreeken,Michael Kamp*

Main category: cs.LG

TL;DR: 神经网络损失函数中的平坦极小值仅能提供局部而非全局的对抗鲁棒性。研究揭示了决定对抗脆弱性的几何结构，并挑战了对平坦性的简化理解。


<details>
  <summary>Details</summary>
Motivation: 神经网络易受微小对抗性扰动攻击。尽管长期以来假设损失函数中低曲率的平坦极小值能增强鲁棒性，但这种联系缺乏形式化和完整性。

Method: 1. 严格形式化了平坦性与鲁棒性的关系。2. 推导了倒数第二层相对平坦度的封闭形式表达式。3. 利用该表达式约束输入空间中损失的变化，从而形式化分析整个网络的对抗鲁棒性。4. 在不同架构和数据集上进行了理论预测的实证验证。

Result: 1. 平坦性仅意味着局部而非全局的对抗鲁棒性。2. 为在局部邻域之外保持鲁棒性，损失函数需要从数据流形急剧弯曲。3. 揭示了控制对抗脆弱性的几何结构。4. 平坦性与模型置信度相关：对抗样本通常位于模型“自信地错误”的大的平坦区域。

Conclusion: 本研究挑战了对平坦性的简化观点，并提供了对其在对抗鲁棒性中作用的更细致理解。

Abstract: Despite their empirical success, neural networks remain vulnerable to small,
adversarial perturbations. A longstanding hypothesis suggests that flat minima,
regions of low curvature in the loss landscape, offer increased robustness.
While intuitive, this connection has remained largely informal and incomplete.
By rigorously formalizing the relationship, we show this intuition is only
partially correct: flatness implies local but not global adversarial
robustness. To arrive at this result, we first derive a closed-form expression
for relative flatness in the penultimate layer, and then show we can use this
to constrain the variation of the loss in input space. This allows us to
formally analyze the adversarial robustness of the entire network. We then show
that to maintain robustness beyond a local neighborhood, the loss needs to
curve sharply away from the data manifold. We validate our theoretical
predictions empirically across architectures and datasets, uncovering the
geometric structure that governs adversarial vulnerability, and linking
flatness to model confidence: adversarial examples often lie in large, flat
regions where the model is confidently wrong. Our results challenge simplified
views of flatness and provide a nuanced understanding of its role in
robustness.

</details>


### [204] [Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models](https://arxiv.org/abs/2510.14232)
*Mehrzad Samadi,Aleksander Ficek,Sean Narenthiran,Siddhartha Jain,Wasi Uddin Ahmad,Somshubra Majumdar,Vahid Noroozi,Boris Ginsburg*

Main category: cs.LG

TL;DR: 本文提出了GenCluster，一个可扩展、可复现的测试时计算框架，通过结合大规模生成、行为聚类等策略，首次使开放权重模型gpt-oss-120b在IOI 2025中达到金牌水平，为LLM推理评估设定了透明和可复现的新基准。


<details>
  <summary>Details</summary>
Motivation: 竞技编程（特别是IOI）是评估大型语言模型（LLM）推理和问题解决能力的关键基准。尽管一些专有模型声称在IOI中达到金牌水平但方法不公开，但使用开放权重模型实现类似结果仍面临巨大挑战，缺乏透明度和可复现性。

Method: 本研究提出了名为GenCluster的测试时计算框架。它通过结合大规模生成、行为聚类、排名和轮询提交策略，在有限的验证预算下高效探索多样化的解决方案空间。

Result: GenCluster的性能与可用计算资源呈一致的正相关关系，成功缩小了开放与封闭系统之间的差距。实验表明，GenCluster首次使用开放权重模型gpt-oss-120b在IOI 2025中取得了金牌。

Conclusion: GenCluster框架利用开放权重模型实现了IOI金牌级性能，为LLM推理能力提供了透明和可复现的评估新基准。这一成就表明开放系统在竞技编程任务上可以与封闭系统匹敌。

Abstract: Competitive programming has become a rigorous benchmark for evaluating the
reasoning and problem-solving capabilities of large language models (LLMs). The
International Olympiad in Informatics (IOI) stands out as one of the most
prestigious annual competitions in competitive programming and has become a key
benchmark for comparing human and AI-level programming ability. While several
proprietary models have been claimed to achieve gold medal-level performance at
the IOI, often with undisclosed methods, achieving comparable results with
open-weight models remains a significant challenge. In this paper, we present
\gencluster, a scalable and reproducible test-time compute framework that
attains IOI gold-level performance using open-weight models. It combines
large-scale generation, behavioral clustering, ranking, and a round-robin
submission strategy to efficiently explore diverse solution spaces under
limited validation budgets. Our experiments show that the performance of our
proposed approach scales consistently with available compute, narrowing the gap
between open and closed systems. Notably, we will show that GenCluster can
achieve a gold medal at IOI 2025 for the first time with an open-weight model
gpt-oss-120b, setting a new benchmark for transparent and reproducible
evaluation of reasoning in LLMs.

</details>


### [205] [Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation](https://arxiv.org/abs/2510.14246)
*Jingwen Gu,Yiting He,Zhishuai Liu,Pan Xu*

Main category: cs.LG

TL;DR: 本文提出DR-RPO算法，一种无模型在线策略优化方法，用于在强化学习中应对分布偏移下的鲁棒决策问题。该算法实现了亚线性遗憾值，并在理论上证明了策略优化的样本效率和多项式次优界，与基于价值的方法性能相当。


<details>
  <summary>Details</summary>
Motivation: 强化学习在训练和部署环境存在差异时，面临分布偏移的决策挑战。现有鲁棒强化学习中，策略优化方法在理论和实践上探索不足，尤其是在线设置下，样本效率和探索性至关重要。

Method: 本文提出了DR-RPO (Distributionally Robust Regularized Policy Optimization) 算法。它通过引入参考策略正则化，实现softmax策略类的可处理优化，形成具有转移和策略双重约束的鲁棒MDP变体。为应对大规模状态-动作空间，DR-RPO采用d-矩形线性MDP公式，结合线性函数近似和乐观探索的上置信界奖金。

Result: 理论分析表明，策略优化在鲁棒RL中可以实现多项式次优界和高样本效率，性能与基于价值的方法相当。经验结果在不同领域验证了本文的理论，并展示了DR-RPO的鲁棒性。

Conclusion: DR-RPO算法成功地弥补了鲁棒RL中策略优化的空白，提供了一种理论上可证明且经验上有效的在线无模型方法，能够有效处理分布偏移，实现鲁棒决策，并且在性能上可与基于价值的方法媲美。

Abstract: Decision-making under distribution shift is a central challenge in
reinforcement learning (RL), where training and deployment environments differ.
We study this problem through the lens of robust Markov decision processes
(RMDPs), which optimize performance against adversarial transition dynamics.
Our focus is the online setting, where the agent has only limited interaction
with the environment, making sample efficiency and exploration especially
critical. Policy optimization, despite its success in standard RL, remains
theoretically and empirically underexplored in robust RL. To bridge this gap,
we propose \textbf{D}istributionally \textbf{R}obust \textbf{R}egularized
\textbf{P}olicy \textbf{O}ptimization algorithm (DR-RPO), a model-free online
policy optimization method that learns robust policies with sublinear regret.
To enable tractable optimization within the softmax policy class, DR-RPO
incorporates reference-policy regularization, yielding RMDP variants that are
doubly constrained in both transitions and policies. To scale to large
state-action spaces, we adopt the $d$-rectangular linear MDP formulation and
combine linear function approximation with an upper confidence bonus for
optimistic exploration. We provide theoretical guarantees showing that policy
optimization can achieve polynomial suboptimality bounds and sample efficiency
in robust RL, matching the performance of value-based approaches. Finally,
empirical results across diverse domains corroborate our theory and demonstrate
the robustness of DR-RPO.

</details>


### [206] [A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters](https://arxiv.org/abs/2510.14250)
*Lianzi Jiang,Jianxin Zhang,Xinyu Han,Huanhe Dong,Xiangrong Wang*

Main category: cs.LG

TL;DR: 本研究提出一种名为PhysAttnNet的新型深度学习模型，通过引入物理先验（如自然衰减和波浪-结构相互作用），显著提高了弹性布拉格防波堤在未知海况下运动响应预测的准确性和泛化能力，并超越了主流模型。


<details>
  <summary>Details</summary>
Motivation: 准确预测弹性布拉格防波堤的运动响应对其结构安全和运行完整性至关重要。然而，传统深度学习模型在未知海况下泛化能力有限，原因在于忽视了海洋系统中观察到的自然衰减以及对波浪-结构相互作用（WSI）建模不足。

Method: 本研究提出PhysAttnNet模型：1. **衰减双向自注意力（DBSA）模块**，融入可学习的时间衰减以模拟自然衰减现象；2. **相位差引导双向交叉注意力（PDG-BCA）模块**，利用基于余弦的偏差显式捕捉波浪与结构间的双向相互作用和相位关系；3. 通过**全局上下文融合（GCF）模块**协同整合两个数据流；4. 使用**混合时频损失**进行训练，联合最小化时域预测误差和频域频谱差异。

Result: 在波浪水槽数据集上的综合实验表明，PhysAttnNet显著优于主流模型。此外，跨场景泛化测试验证了模型对未知环境的鲁棒性和适应性。

Conclusion: PhysAttnNet模型能够准确预测弹性布拉格防波堤的运动响应，并具有强大的泛化能力，凸显了其作为海洋工程中复杂系统预测模型开发框架的潜力。

Abstract: Accurate motion response prediction for elastic Bragg breakwaters is critical
for their structural safety and operational integrity in marine environments.
However, conventional deep learning models often exhibit limited generalization
capabilities when presented with unseen sea states. These deficiencies stem
from the neglect of natural decay observed in marine systems and inadequate
modeling of wave-structure interaction (WSI). To overcome these challenges,
this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network
(PhysAttnNet). First, the decay bidirectional self-attention (DBSA) module
incorporates a learnable temporal decay to assign higher weights to recent
states, aiming to emulate the natural decay phenomenon. Meanwhile, the phase
differences guided bidirectional cross-attention (PDG-BCA) module explicitly
captures the bidirectional interaction and phase relationship between waves and
the structure using a cosine-based bias within a bidirectional
cross-computation paradigm. These streams are synergistically integrated
through a global context fusion (GCF) module. Finally, PhysAttnNet is trained
with a hybrid time-frequency loss that jointly minimizes time-domain prediction
errors and frequency-domain spectral discrepancies. Comprehensive experiments
on wave flume datasets demonstrate that PhysAttnNet significantly outperforms
mainstream models. Furthermore,cross-scenario generalization tests validate the
model's robustness and adaptability to unseen environments, highlighting its
potential as a framework to develop predictive models for complex systems in
ocean engineering.

</details>


### [207] [Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals](https://arxiv.org/abs/2510.14254)
*Saurabh Kataria,Yi Wu,Zhaoliang Chen,Hyunjung Gloria Kwak,Yuhao Xu,Lovely Yeswanth Panchumarthi,Ran Xiao,Jiaying Lu,Ayca Ermis,Anni Zhao,Runze Yan,Alex Federov,Zewen Liu,Xu Wu,Wei Jin,Carl Yang,Jocelyn Grunwell,Stephanie R. Brown,Amit Shah,Craig Jabaley,Tim Buchman,Sivasubramanium V Bhavani,Randall J. Lee,Xiao Hu*

Main category: cs.LG

TL;DR: 本研究通过基准测试，比较了通用型和专科型时间序列基础模型在生理传感（尤其是PPG信号）上的表现，发现在全微调场景下，专科模型性能显著优于通用模型。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型多为专科模型，但通用型模型也日益受到关注。为填补这一比较空白，本研究旨在全面评估和比较这两种模型类型在生理传感（特别是PPG信号）任务中的性能。

Method: 本研究进行了一项全面的基准测试，比较通用型和专科型时间序列基础模型。评估重点是PPG信号，涵盖了51项任务，包括心脏状态评估、实验室值估计和跨模态推理。评估维度包括胜率得分、平均性能、特征质量、微调收益、性能方差、可迁移性和可扩展性。

Result: 在全微调场景中，专科模型实现了27%的更高胜率得分，表明其性能优于通用模型。

Conclusion: 在PPG信号处理任务的全微调场景下，专科模型表现出优越的性能。此外，研究还深入分析了模型的泛化性、公平性、注意力可视化以及训练数据选择的重要性，为理解两类模型的优势和局限性提供了全面的视角。

Abstract: Foundation models are large-scale machine learning models that are
pre-trained on massive amounts of data and can be adapted for various
downstream tasks. They have been extensively applied to tasks in Natural
Language Processing and Computer Vision with models such as GPT, BERT, and
CLIP. They are now also increasingly gaining attention in time-series analysis,
particularly for physiological sensing. However, most time series foundation
models are specialist models - with data in pre-training and testing of the
same type, such as Electrocardiogram, Electroencephalogram, and
Photoplethysmogram (PPG). Recent works, such as MOMENT, train a generalist time
series foundation model with data from multiple domains, such as weather,
traffic, and electricity. This paper aims to conduct a comprehensive
benchmarking study to compare the performance of generalist and specialist
models, with a focus on PPG signals. Through an extensive suite of total 51
tasks covering cardiac state assessment, laboratory value estimation, and
cross-modal inference, we comprehensively evaluate both models across seven
dimensions, including win score, average performance, feature quality, tuning
gain, performance variance, transferability, and scalability. These metrics
jointly capture not only the models' capability but also their adaptability,
robustness, and efficiency under different fine-tuning strategies, providing a
holistic understanding of their strengths and limitations for diverse
downstream scenarios. In a full-tuning scenario, we demonstrate that the
specialist model achieves a 27% higher win score. Finally, we provide further
analysis on generalization, fairness, attention visualizations, and the
importance of training data choice.

</details>


### [208] [CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions](https://arxiv.org/abs/2510.14262)
*Zihao Fu,Ming Liao,Chris Russell,Zhenguang G. Cai*

Main category: cs.LG

TL;DR: 本文提出CAST（Compositional Analysis via Spectral Tracking）框架，通过直接估计变换矩阵并进行频谱分析，探究Transformer层功能，揭示了编码器和解码器模型间的不同行为模式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）成功但内部机制不透明，现有可解释性方法提供了不同视角但仍有局限，需要新的无探针框架来直接分析Transformer层功能。

Method: 引入CAST框架，它是一种无探针方法。通过使用Moore-Penrose伪逆来估计每层的实际变换矩阵，并应用包含六个可解释指标的谱分析来表征层行为。此外，还通过核分析（CKA相似度）探究层间的函数关系模式。

Result: 分析揭示了编码器-only和解码器-only模型之间的显著行为差异：解码器模型表现出压缩-扩展周期，而编码器模型则保持一致的高秩处理。核分析进一步表明，层功能关系模式将层清晰地划分为三个阶段：特征提取、压缩和专业化。

Conclusion: CAST框架通过直接的变换矩阵估计和谱分析，为理解Transformer层功能提供了补充性洞察，揭示了不同模型类型和处理阶段的独特行为模式。

Abstract: Large language models have achieved remarkable success but remain largely
black boxes with poorly understood internal mechanisms. To address this
limitation, many researchers have proposed various interpretability methods
including mechanistic analysis, probing classifiers, and activation
visualization, each providing valuable insights from different perspectives.
Building upon this rich landscape of complementary approaches, we introduce
CAST (Compositional Analysis via Spectral Tracking), a probe-free framework
that contributes a novel perspective by analyzing transformer layer functions
through direct transformation matrix estimation and comprehensive spectral
analysis. CAST offers complementary insights to existing methods by estimating
the realized transformation matrices for each layer using Moore-Penrose
pseudoinverse and applying spectral analysis with six interpretable metrics
characterizing layer behavior. Our analysis reveals distinct behaviors between
encoder-only and decoder-only models, with decoder models exhibiting
compression-expansion cycles while encoder models maintain consistent high-rank
processing. Kernel analysis further demonstrates functional relationship
patterns between layers, with CKA similarity matrices clearly partitioning
layers into three phases: feature extraction, compression, and specialization.

</details>


### [209] [Nonparametric Data Attribution for Diffusion Models](https://arxiv.org/abs/2510.14269)
*Yutian Zhao,Chao Du,Xiaosen Zheng,Tianyu Pang,Min Lin*

Main category: cs.LG

TL;DR: 提出一种非参数、纯数据归因方法，通过图像块相似性量化训练数据对生成模型输出的影响，克服了现有梯度/重训练方法的局限性，实现了高效且高性能的归因。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的数据归因方法通常需要访问模型梯度或进行重训练，这在专有或大规模应用中限制了其可用性。

Method: 我们提出一种完全基于数据的非参数归因方法，通过测量生成图像与训练图像之间的图像块级相似性来量化影响。该方法基于最优分数函数的分析形式，并自然扩展到多尺度表示，通过基于卷积的加速实现了计算效率。

Result: 我们的框架不仅能生成空间可解释的归因，还能揭示训练数据与输出之间固有的模式，且独立于特定模型。实验表明，该方法实现了强大的归因性能，与基于梯度的 GQA 方法接近，并显著优于现有非参数基线。

Conclusion: 本研究提供了一种高效、高性能且无需模型内部访问的非参数数据归因方法，为理解生成模型中的训练数据影响提供了新的途径。

Abstract: Data attribution for generative models seeks to quantify the influence of
individual training examples on model outputs. Existing methods for diffusion
models typically require access to model gradients or retraining, limiting
their applicability in proprietary or large-scale settings. We propose a
nonparametric attribution method that operates entirely on data, measuring
influence via patch-level similarity between generated and training images. Our
approach is grounded in the analytical form of the optimal score function and
naturally extends to multiscale representations, while remaining
computationally efficient through convolution-based acceleration. In addition
to producing spatially interpretable attributions, our framework uncovers
patterns that reflect intrinsic relationships between training data and
outputs, independent of any specific model. Experiments demonstrate that our
method achieves strong attribution performance, closely matching gradient-based
approaches and substantially outperforming existing nonparametric baselines.
Code is available at https://github.com/sail-sg/NDA.

</details>


### [210] [Stable Prediction of Adverse Events in Medical Time-Series Data](https://arxiv.org/abs/2510.14286)
*Mayank Keoliya,Seewon Choi,Rajeev Alur,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: 现有早期事件预测（EEP）基准缺乏对风险分数稳定性和多模态输入的评估。本研究引入CAREBench，一个使用多模态数据（EHR、ECG、文本）并同时评估预测准确性和时间稳定性的新基准，发现现有方法（尤其是LLM）难以同时优化准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 早期事件预测（EEP）系统需要生成准确且时间稳定的风险轨迹以获得临床信任，但现有基准忽视了风险分数稳定性，且主要基于表格输入进行评估，未能测试轨迹行为。

Method: 引入CAREBench，一个评估EEP系统可部署性的新基准，它使用多模态输入（表格化EHR、ECG波形、临床文本），并同时评估预测准确性和时间稳定性。提出了一种新的稳定性指标，通过量化患者风险的短期变异性，并基于局部Lipschitz常数惩罚突然振荡。CAREBench涵盖六项预测任务，并比较了传统学习器、深度序列模型和零样本LLM。

Result: 在各项任务中，现有方法，特别是大型语言模型（LLM），难以同时优化预测准确性和时间稳定性，且在高精度操作点表现出显著的低召回率。

Conclusion: 研究结果强调，迫切需要开发能够生成与证据对齐且稳定的风险轨迹的模型，以在持续监测环境中赢得临床医生的信任。

Abstract: Early event prediction (EEP) systems continuously estimate a patient's
imminent risk to support clinical decision-making. For bedside trust, risk
trajectories must be accurate and temporally stable, shifting only with new,
relevant evidence. However, current benchmarks (a) ignore stability of risk
scores and (b) evaluate mainly on tabular inputs, leaving trajectory behavior
untested. To address this gap, we introduce CAREBench, an EEP benchmark that
evaluates deployability using multi-modal inputs-tabular EHR, ECG waveforms,
and clinical text-and assesses temporal stability alongside predictive
accuracy. We propose a stability metric that quantifies short-term variability
in per-patient risk and penalizes abrupt oscillations based on local-Lipschitz
constants. CAREBench spans six prediction tasks such as sepsis onset and
compares classical learners, deep sequence models, and zero-shot LLMs. Across
tasks, existing methods, especially LLMs, struggle to jointly optimize accuracy
and stability, with notably poor recall at high-precision operating points.
These results highlight the need for models that produce evidence-aligned,
stable trajectories to earn clinician trust in continuous monitoring settings.
(Code: https://github.com/SeewonChoi/CAREBench.)

</details>


### [211] [Enhancing Time-Series Anomaly Detection by Integrating Spectral-Residual Bottom-Up Attention with Reservoir Computing](https://arxiv.org/abs/2510.14287)
*Hayato Nihei,Sou Nobukawa,Yusuke Sakemi,Kazuyuki Aihara*

Main category: cs.LG

TL;DR: 为了在资源受限的边缘设备上提升水库计算（RC）的时间序列异常检测性能而不牺牲学习效率，本文提出谱残差水库计算（SR-RC），它将一种无需学习的注意力机制（谱残差方法）与RC结合，实验证明其性能优于传统RC，并具有硬件实现的实用性。


<details>
  <summary>Details</summary>
Motivation: 水库计算（RC）在边缘AI应用中进行时间序列异常检测时，若要达到足够性能，可能需要在资源受限设备上使用过大的水库；而传统注意力机制虽能提高准确性，但计算量大，会损害RC的学习效率。

Method: 提出一种谱残差水库计算（SR-RC）模型，通过将无需学习的自底向上注意力机制——谱残差（SR）方法与水库计算（RC）相结合，旨在提高RC的异常检测性能，同时不牺牲其学习效率。

Result: 在基准任务和真实世界时间序列数据集上，SR-RC的异常检测性能优于传统的RC模型和基于SR方法提取值的逻辑回归模型。

Conclusion: 由于谱残差（SR）方法和水库计算（RC）都适合硬件实现，SR-RC为在边缘AI中部署RC进行时间序列异常检测提供了一个实用的方向。

Abstract: Reservoir computing (RC) establishes the basis for the processing of
time-series data by exploiting the high-dimensional spatiotemporal response of
a recurrent neural network to an input signal. In particular, RC trains only
the output layer weights. This simplicity has drawn attention especially in
Edge Artificial Intelligence (AI) applications. Edge AI enables time-series
anomaly detection in real time, which is important because detection delays can
lead to serious incidents. However, achieving adequate anomaly-detection
performance with RC alone may require an unacceptably large reservoir on
resource-constrained edge devices. Without enlarging the reservoir, attention
mechanisms can improve accuracy, although they may require substantial
computation and undermine the learning efficiency of RC. In this study, to
improve the anomaly detection performance of RC without sacrificing learning
efficiency, we propose a spectral residual RC (SR-RC) that integrates the
spectral residual (SR) method - a learning-free, bottom-up attention mechanism
- with RC. We demonstrated that SR-RC outperformed conventional RC and
logistic-regression models based on values extracted by the SR method across
benchmark tasks and real-world time-series datasets. Moreover, because the SR
method, similarly to RC, is well suited for hardware implementation, SR-RC
suggests a practical direction for deploying RC as Edge AI for time-series
anomaly detection.

</details>


### [212] [TED++: Submanifold-Aware Backdoor Detection via Layerwise Tubular-Neighbourhood Screening](https://arxiv.org/abs/2510.14299)
*Nam Le,Leo Yu Zhang,Kewen Liao,Shirui Pan,Wei Luo*

Main category: cs.LG

TL;DR: TED++是一个子流形感知的框架，通过检测隐藏特征流形上的异常漂移，有效抵御隐蔽的后门攻击，即使在清洁数据稀缺的情况下也能实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在关键应用中日益普及，隐蔽的后门攻击（通过中毒训练输入触发恶意模型行为）构成严重安全风险。现有防御在攻击者利用微妙的基于距离的异常或清洁示例稀缺时容易受攻击。

Method: TED++通过为每个类别的隐藏特征流形构建管状邻域，利用少量清洁激活估计其局部“厚度”。然后，它应用局部自适应排序（LAR）来检测任何漂移出允许管状区域的激活。通过聚合所有层中的LAR调整秩，TED++捕获输入在演变类子流形上的保真度，并根据偏离显著的LAR排序序列标记异常输入。

Result: TED++在基准数据集和任务上表现出最先进的检测性能，无论是在自适应攻击还是数据受限情景下。即使每类只有五个保留示例，TED++仍能实现近乎完美的检测，AUROC比次优方法提升高达14%。

Conclusion: TED++是一个强大的后门检测框架，能够有效检测现有防御难以应对的微妙后门攻击，尤其在清洁数据稀缺的情况下表现卓越，为神经网络安全提供了重要保障。

Abstract: As deep neural networks power increasingly critical applications, stealthy
backdoor attacks, where poisoned training inputs trigger malicious model
behaviour while appearing benign, pose a severe security risk. Many existing
defences are vulnerable when attackers exploit subtle distance-based anomalies
or when clean examples are scarce. To meet this challenge, we introduce TED++,
a submanifold-aware framework that effectively detects subtle backdoors that
evade existing defences. TED++ begins by constructing a tubular neighbourhood
around each class's hidden-feature manifold, estimating its local ``thickness''
from a handful of clean activations. It then applies Locally Adaptive Ranking
(LAR) to detect any activation that drifts outside the admissible tube. By
aggregating these LAR-adjusted ranks across all layers, TED++ captures how
faithfully an input remains on the evolving class submanifolds. Based on such
characteristic ``tube-constrained'' behaviour, TED++ flags inputs whose
LAR-based ranking sequences deviate significantly. Extensive experiments are
conducted on benchmark datasets and tasks, demonstrating that TED++ achieves
state-of-the-art detection performance under both adaptive-attack and
limited-data scenarios. Remarkably, even with only five held-out examples per
class, TED++ still delivers near-perfect detection, achieving gains of up to
14\% in AUROC over the next-best method. The code is publicly available at
https://github.com/namle-w/TEDpp.

</details>


### [213] [Active Measuring in Reinforcement Learning With Delayed Negative Effects](https://arxiv.org/abs/2510.14315)
*Daiqi Gao,Ziping Xu,Aseel Rawashdeh,Predrag Klasnja,Susan A. Murphy*

Main category: cs.LG

TL;DR: 本文提出主动可观测马尔可夫决策过程（AOMDP），允许强化学习智能体决定何时测量状态，以平衡测量成本和信息收益，实验证明能提高样本效率和策略价值。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的强化学习（RL）场景中，测量状态可能成本高昂，并可能对未来的结果产生负面影响。

Method: 引入主动可观测马尔可夫决策过程（AOMDP），智能体既选择控制动作，也决定是否测量潜在状态。将AOMDP公式化为周期性部分可观测MDP，并提出一种基于信念状态的在线RL算法。为近似信念状态，进一步提出一种序贯蒙特卡洛方法，联合近似未知静态环境参数和未观测潜在状态的后验。在数字健康应用中评估了该算法。

Result: 研究表明，尽管存在测量成本，但减少的不确定性可以证明地提高样本效率，并增加最优策略的价值。

Conclusion: AOMDP框架提供了一种在强化学习中管理昂贵状态测量的方法，证明了通过主动选择何时测量，即使有额外成本，也能显著提高系统性能和策略价值。

Abstract: Measuring states in reinforcement learning (RL) can be costly in real-world
settings and may negatively influence future outcomes. We introduce the
Actively Observable Markov Decision Process (AOMDP), where an agent not only
selects control actions but also decides whether to measure the latent state.
The measurement action reveals the true latent state but may have a negative
delayed effect on the environment. We show that this reduced uncertainty may
provably improve sample efficiency and increase the value of the optimal policy
despite these costs. We formulate an AOMDP as a periodic partially observable
MDP and propose an online RL algorithm based on belief states. To approximate
the belief states, we further propose a sequential Monte Carlo method to
jointly approximate the posterior of unknown static environment parameters and
unobserved latent states. We evaluate the proposed algorithm in a digital
health application, where the agent decides when to deliver digital
interventions and when to assess users' health status through surveys.

</details>


### [214] [LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search](https://arxiv.org/abs/2510.14331)
*Shivam Singhal,Eran Malach,Tomaso Poggio,Tomer Galanti*

Main category: cs.LG

TL;DR: 提出LLM-ERM框架，结合LLM引导搜索和ERM选择，实现程序学习的样本高效性和计算可行性，在特定任务上显著优于基于梯度的训练方法。


<details>
  <summary>Details</summary>
Motivation: 现有的程序学习算法在样本效率和计算可行性之间存在权衡：穷举枚举样本高效但计算成本呈指数级；基于梯度的训练计算可行但对某些短程序族样本效率低下。需要一种兼顾两者的方法。

Method: 引入LLM-ERM，一个“提出-验证”框架。该方法用预训练的推理增强型LLM生成$k$个候选程序，然后将每个候选程序在数据上编译并检查，最后选择最佳的验证假设，不涉及反馈、适应性或梯度。

Result: 理论上，协调式在线小批量SGD学习某些短程序需要大量样本。经验上，LLM-ERM能用少至200个样本解决奇偶校验变体、模式匹配和素数测试等任务，而经过SGD训练的Transformer即使使用100,000个样本也出现过拟合。

Conclusion: 语言引导的程序合成（LLM-ERM）恢复了有限类ERM的统计效率，同时保持了计算上的可行性，为学习基于梯度训练难以达到的简洁假设提供了一条实用途径。

Abstract: We seek algorithms for program learning that are both sample-efficient and
computationally feasible. Classical results show that targets admitting short
program descriptions (e.g., with short ``python code'') can be learned with a
``small'' number of examples (scaling with the size of the code) via
length-first program enumeration, but the search is exponential in description
length. Consequently, Gradient-based training avoids this cost yet can require
exponentially many samples on certain short-program families.
  To address this gap, we introduce LLM-ERM, a propose-and-verify framework
that replaces exhaustive enumeration with an LLM-guided search over candidate
programs while retaining ERM-style selection on held-out data. Specifically, we
draw $k$ candidates with a pretrained reasoning-augmented LLM, compile and
check each on the data, and return the best verified hypothesis, with no
feedback, adaptivity, or gradients. Theoretically, we show that coordinate-wise
online mini-batch SGD requires many samples to learn certain short programs.
{\em Empirically, LLM-ERM solves tasks such as parity variants, pattern
matching, and primality testing with as few as 200 samples, while SGD-trained
transformers overfit even with 100,000 samples}. These results indicate that
language-guided program synthesis recovers much of the statistical efficiency
of finite-class ERM while remaining computationally tractable, offering a
practical route to learning succinct hypotheses beyond the reach of
gradient-based training.

</details>


### [215] [DARTS-GT: Differentiable Architecture Search for Graph Transformers with Quantifiable Instance-Specific Interpretability Analysis](https://arxiv.org/abs/2510.14336)
*Shruti Sarika Chakraborty,Peter Minary*

Main category: cs.LG

TL;DR: 本文提出DARTS-GT，通过非对称注意力机制和可微分架构搜索（DARTS）实现深度异构的图Transformer，并引入首个定量可解释性框架。DARTS-GT在性能上达到SOTA，并显著提高了模型可解释性，打破了性能与可解释性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有图Transformer（GTs）设计僵化，所有层采用固定GNN类型，未能利用深度异构的优势；其复杂架构缺乏定量可解释性，难以区分有意义的模式和虚假关联。

Method: 1. 重新设计GT注意力机制，采用非对称方式：查询来自节点特征，键和值来自GNN转换，解耦结构编码和特征表示。2. 在此框架下，利用可微分架构搜索（DARTS）在每层选择最优GNN操作符，实现深度异构（DARTS-GT）。3. 开发首个基于因果消融的GT定量可解释性框架，包含Head-deviation、Specialization和Focus等指标。

Result: DARTS-GT在八个基准测试中，有四个达到SOTA，其他也保持竞争力；发现的架构展现出数据集特定模式。可解释性分析表明，视觉注意力显著性与因果重要性并非总相关，传统可视化可能遗漏关键组件。DARTS-GT发现的异构架构始终比基线模型更具可解释性。

Conclusion: 图Transformer无需在性能和可解释性之间做出取舍。通过深度异构架构搜索和定量可解释性分析，可以同时实现最先进的性能和更高的模型可解释性。

Abstract: Graph Transformers (GTs) have emerged as powerful architectures for
graph-structured data, yet remain constrained by rigid designs and lack
quantifiable interpretability. Current state-of-the-art GTs commit to fixed GNN
types across all layers, missing potential benefits of depth-specific component
selection, while their complex architectures become opaque where performance
gains cannot be distinguished between meaningful patterns and spurious
correlations. We redesign GT attention through asymmetry, decoupling structural
encoding from feature representation: queries derive from node features while
keys and values come from GNN transformations. Within this framework, we use
Differentiable ARchiTecture Search (DARTS) to select optimal GNN operators at
each layer, enabling depth-wise heterogeneity inside transformer attention
itself (DARTS-GT). To understand discovered architectures, we develop the first
quantitative interpretability framework for GTs through causal ablation. Our
metrics (Head-deviation, Specialization, and Focus), identify which heads and
nodes drive predictions while enabling model comparison. Experiments across
eight benchmarks show DARTS-GT achieves state-of-the-art on four datasets while
remaining competitive on others, with discovered architectures revealing
dataset-specific patterns. Our interpretability analysis reveals that visual
attention salience and causal importance do not always correlate, indicating
widely used visualization approaches may miss components that actually matter.
Crucially, heterogeneous architectures found by DARTS-GT consistently produced
more interpretable models than baselines, establishing that Graph Transformers
need not choose between performance and interpretability.

</details>


### [216] [Stop-RAG: Value-Based Retrieval Control for Iterative RAG](https://arxiv.org/abs/2510.14337)
*Jaewan Park,Solbee Cho,Jay-Yoon Lee*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Iterative retrieval-augmented generation (RAG) enables large language models
to answer complex multi-hop questions, but each additional loop increases
latency, costs, and the risk of introducing distracting evidence, motivating
the need for an efficient stopping strategy. Existing methods either use a
predetermined number of iterations or rely on confidence proxies that poorly
reflect whether more retrieval will actually help. We cast iterative RAG as a
finite-horizon Markov decision process and introduce Stop-RAG, a value-based
controller that adaptively decides when to stop retrieving. Trained with
full-width forward-view Q($\lambda$) targets from complete trajectories,
Stop-RAG learns effective stopping policies while remaining compatible with
black-box APIs and existing pipelines. On multi-hop question-answering
benchmarks, Stop-RAG consistently outperforms both fixed-iteration baselines
and prompting-based stopping with LLMs. These results highlight adaptive
stopping as a key missing component in current agentic systems, and demonstrate
that value-based control can improve the accuracy of RAG systems.

</details>


### [217] [Jet Functors and Weil Algebras in Automatic Differentiation: A Geometric Analysis](https://arxiv.org/abs/2510.14342)
*Amandip Sangha*

Main category: cs.LG

TL;DR: 本文提出了一种利用射流丛和Weil代数对自动微分（AD）的几何化表述，揭示了反向模式和泰勒模式的原理，并实现了混合导数的高效计算，为AD理论和应用提供了新的基础。


<details>
  <summary>Details</summary>
Motivation: 为自动微分（AD）提供一个深刻的几何理论基础，以更好地理解其内在机制，解决高阶和混合导数计算的复杂性，并为开发深度学习和科学计算中的结构保持微分方法奠定基础。

Method: 通过使用射流丛（jet bundles）和Weil代数，提出自动微分的几何化表述。将反向模式AD解释为余切回拉（cotangent-pullback），泰勒模式AD解释为Weil代数中的求值。利用张量化的Weil代数实现所有混合导数的一次性计算。

Result: 从该几何框架中，导出了关于AD正确性、稳定性和复杂性的简洁表述，包括反向模式的函子恒等式、高阶导数的代数精确性以及截断误差的明确界限。此外，证明了张量化的Weil代数能够以与代数维度线性相关的成本，一次性计算所有混合导数，从而避免了传统方法中组合爆炸的问题。

Conclusion: 该框架通过微分几何的视角重新诠释了自动微分理论，为深度学习和科学计算领域开发结构保持的微分方法提供了坚实而统一的理论基础。

Abstract: We present a geometric formulation of automatic differentiation (AD) using
jet bundles and Weil algebras. Reverse-mode AD emerges as cotangent-pullback,
while Taylor-mode corresponds to evaluation in a Weil algebra. From these
principles, we derive concise statements on correctness, stability, and
complexity: a functorial identity for reverse-mode, algebraic exactness of
higher-order derivatives, and explicit bounds on truncation error. We further
show that tensorized Weil algebras permit one-pass computation of all mixed
derivatives with cost linear in the algebra dimension, avoiding the
combinatorial blow-up of nested JVP/VJP schedules. This framework interprets AD
theory through the lens of differential geometry and offers a foundation for
developing structure-preserving differentiation methods in deep learning and
scientific computing. Code and examples are available at
https://git.nilu.no/geometric-ad/jet-weil-ad.

</details>


### [218] [Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers](https://arxiv.org/abs/2510.14381)
*Andrew Zhao,Reshmi Ghosh,Vitor Carvalho,Emily Lawton,Keegan Hines,Gao Huang,Jack W. Stokes*

Main category: cs.LG

TL;DR: 首次系统分析了LLM提示优化中的投毒风险，发现反馈操纵是主要威胁，并提出了一种新型假奖励攻击及其有效防御。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）系统广泛应用于日常AI，其性能高度依赖提示质量。LLM提示优化器能通过迭代反馈来优化提示，但该优化阶段的安全性（特别是投毒风险）尚未得到充分研究。

Method: 进行了LLM提示优化中投毒风险的首次系统分析。使用HarmBench平台，比较了基于反馈的攻击与注入查询的攻击。引入了一种无需奖励模型访问的简单假奖励攻击（fake-reward attack）。提出了一种轻量级高亮防御（highlighting defense）。

Result: 研究发现，系统对操纵反馈的脆弱性远高于注入查询，反馈攻击可将攻击成功率（ASR）提高高达0.48。所提出的假奖励攻击显著增加了系统脆弱性。高亮防御能够将假奖励攻击导致的ASR增量从0.23降至0.07，且不影响实用性。

Conclusion: 提示优化管道是一个首要的攻击面。因此，有必要为反馈渠道和优化框架提供更强大的安全保障措施。

Abstract: Large language model (LLM) systems now underpin everyday AI applications such
as chatbots, computer-use assistants, and autonomous robots, where performance
often depends on carefully designed prompts. LLM-based prompt optimizers reduce
that effort by iteratively refining prompts from scored feedback, yet the
security of this optimization stage remains underexamined. We present the first
systematic analysis of poisoning risks in LLM-based prompt optimization. Using
HarmBench, we find systems are substantially more vulnerable to manipulated
feedback than to injected queries: feedback-based attacks raise attack success
rate (ASR) by up to $\Delta$ASR = 0.48. We introduce a simple fake-reward
attack that requires no access to the reward model and significantly increases
vulnerability, and we propose a lightweight highlighting defense that reduces
the fake-reward $\Delta$ASR from 0.23 to 0.07 without degrading utility. These
results establish prompt optimization pipelines as a first-class attack surface
and motivate stronger safeguards for feedback channels and optimization
frameworks.

</details>


### [219] [SHaRe-SSM: An Oscillatory Spiking Neural Network for Target Variable Modeling in Long Sequences](https://arxiv.org/abs/2510.14386)
*Kartikay Agrawal,Abhijeet Vikram,Vedant Sharma,Vaishnavi N.,Ayon Borthakur*

Main category: cs.LG

TL;DR: 提出SHaRe-SSM，一种二阶脉冲状态空间模型，结合SNN和SSM优势，实现对超长序列的高能效、无乘法操作的目标变量建模（分类和回归），性能优于Transformer和一阶SSM。


<details>
  <summary>Details</summary>
Motivation: SNNs因其能效高、无乘法操作、稀疏事件驱动等特性备受关注；SSMs作为Transformer的强大替代，擅长处理长序列。受此启发，旨在设计一种结合两者优点的新模型，解决资源受限场景下超长序列的建模问题。

Method: 设计了SHaRe-SSM（Spiking Harmonic Resonate and Fire State Space Model），一个二阶脉冲SSM。通过并行扫描确保动态系统在长序列上的可学习性和效率。首次提出使用谐振与激发神经元的核函数脉冲回归器，用于超长序列。并系统分析了谐振与激发SSM中的异质性、耗散和守恒的影响。

Result: SHaRe-SSM在超长序列（包括50k序列）的目标变量建模中性能优于Transformer或一阶SSM。该模型无需乘法操作，特别适用于资源受限应用。对于18k序列，能耗比二阶ANN-based SSMs降低73倍，同时保持性能。

Conclusion: SHaRe-SSM为超长序列建模提供了一种高性能、高能效、无乘法操作的解决方案。其优越性在于结合了脉冲神经网络的能效优势和状态空间模型处理长序列的能力，在资源受限场景下展现出巨大潜力。

Abstract: In recent years, with the emergence of large models, there has been a
significant interest in spiking neural networks (SNNs) primarily due to their
energy efficiency, multiplication-free, and sparse event-based deep learning.
Similarly, state space models (SSMs) in varying designs have evolved as a
powerful alternative to transformers for target modeling in long sequences,
thereby overcoming the quadratic dependence on sequence length of a
transformer. Inspired by this progress, we here design SHaRe-SSM (Spiking
Harmonic Resonate and Fire State Space Model), for target variable modeling
(including both classification and regression) for very-long-range sequences.
Our second-order spiking SSM, on average, performs better than transformers or
first-order SSMs while circumventing multiplication operations, making it ideal
for resource-constrained applications. The proposed block consumes $73 \times$
less energy than second-order ANN-based SSMs for an 18k sequence, while
retaining performance. To ensure learnability over the long-range sequences, we
propose exploiting the stable and efficient implementation of the dynamical
system using parallel scans. Moreover, for the first time, we propose a
kernel-based spiking regressor using resonate and fire neurons for very
long-range sequences. Our network shows superior performance on even a 50k
sequence while being significantly energy-efficient. In addition, we conducted
a systematic analysis of the impact of heterogeneity, dissipation, and
conservation in resonate-and-fire SSMs.

</details>


### [220] [Revisit Modality Imbalance at the Decision Layer](https://arxiv.org/abs/2510.14411)
*Xiaoyu Ma,Hao Chen*

Main category: cs.LG

TL;DR: 多模态学习中的模态不平衡问题显著地体现在决策层，而非仅在表示学习层。该偏见源于特征空间和决策权重分布的固有差异。未来研究应在决策层引入自适应权重分配机制以实现模态平衡。


<details>
  <summary>Details</summary>
Motivation: 多模态学习常因模态不平衡问题（即主导模态掩盖弱势模态）而影响模型性能。本文旨在深入探究这种不平衡的根源，并揭示其在决策层的新表现形式。

Method: 研究通过在音频-视觉数据集（CREMAD 和 Kinetic-Sounds）上进行实验，观察模型在充分预训练和平衡优化后的行为，并进一步分析特征空间和决策权重分布来探究模态偏见的来源。

Result: 研究发现模态不平衡不仅存在于表示学习层，更显著地表现在决策层。即使经过充分优化，模型仍对特定模态（如音频）存在系统性偏见。进一步分析表明，这种偏见源于特征空间和决策权重分布的固有差异，而非优化动态本身。

Conclusion: 未校准的模态输出在融合阶段导致决策层权重偏置，阻碍了弱势模态的有效贡献。因此，未来的多模态系统应重点在决策层引入自适应权重分配机制，以根据各模态的能力实现相对平衡。

Abstract: Multimodal learning integrates information from different modalities to
enhance model performance, yet it often suffers from modality imbalance, where
dominant modalities overshadow weaker ones during joint optimization. This
paper reveals that such an imbalance not only occurs during representation
learning but also manifests significantly at the decision layer. Experiments on
audio-visual datasets (CREMAD and Kinetic-Sounds) show that even after
extensive pretraining and balanced optimization, models still exhibit
systematic bias toward certain modalities, such as audio. Further analysis
demonstrates that this bias originates from intrinsic disparities in
feature-space and decision-weight distributions rather than from optimization
dynamics alone. We argue that aggregating uncalibrated modality outputs at the
fusion stage leads to biased decision-layer weighting, hindering weaker
modalities from contributing effectively. To address this, we propose that
future multimodal systems should focus more on incorporate adaptive weight
allocation mechanisms at the decision layer, enabling relative balanced
according to the capabilities of each modality.

</details>


### [221] [Interaction Concordance Index: Performance Evaluation for Interaction Prediction Methods](https://arxiv.org/abs/2510.14419)
*Tapio Pahikkala,Riikka Numminen,Parisa Movahedi,Napsu Karmitsa,Antti Airola*

Main category: cs.LG

TL;DR: 引入交互一致性指数（IC-index），一种新的性能评估器，用于衡量药物-靶点亲和力（DTA）预测中交互作用方向的准确性，以补充现有评估方法。


<details>
  <summary>Details</summary>
Motivation: 正确捕获药物-靶点亲和力（DTA）中的交互作用对于更好的决策制定至关重要。然而，现有DTA预测性能评估器未能专门评估交互作用方向的预测能力。

Method: 提出交互一致性指数（IC-index），该指数通过评估数据中正确预测的交互作用方向的比例，来衡量预测器的性能，适用于固定预测器和机器学习算法。

Result: 1. IC-index对无法捕获交互作用的预测器具有不变性。2. 学习算法对药物和靶点身份的置换等变性意味着在训练中遇到未知实体时无法捕获交互作用，可通过结合辅助信息解决。3. 广泛的实证评估表明IC-index能有效补充现有指标，评估不同亲和力预测方法在交互作用方向预测上的表现。

Conclusion: IC-index是评估交互作用预测（尤其是方向性）的重要补充指标，为DTA预测性能评估提供了更全面的视角，并突出了辅助信息在处理未知实体时的关键作用。

Abstract: Consider two sets of entities and their members' mutual affinity values, say
drug-target affinities (DTA). Drugs and targets are said to interact in their
effects on DTAs if drug's effect on it depends on the target. Presence of
interaction implies that assigning a drug to a target and another drug to
another target does not provide the same aggregate DTA as the reversed
assignment would provide. Accordingly, correctly capturing interactions enables
better decision-making, for example, in allocation of limited numbers of drug
doses to their best matching targets. Learning to predict DTAs is popularly
done from either solely from known DTAs or together with side information on
the entities, such as chemical structures of drugs and targets. In this paper,
we introduce interaction directions' prediction performance estimator we call
interaction concordance index (IC-index), for both fixed predictors and machine
learning algorithms aimed for inferring them. IC-index complements the
popularly used DTA prediction performance estimators by evaluating the ratio of
correctly predicted directions of interaction effects in data. First, we show
the invariance of IC-index on predictors unable to capture interactions.
Secondly, we show that learning algorithm's permutation equivariance regarding
drug and target identities implies its inability to capture interactions when
either drug, target or both are unseen during training. In practical
applications, this equivariance is remedied via incorporation of appropriate
side information on drugs and targets. We make a comprehensive empirical
evaluation over several biomedical interaction data sets with various
state-of-the-art machine learning algorithms. The experiments demonstrate how
different types of affinity strength prediction methods perform in terms of
IC-index complementing existing prediction performance estimators.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [222] [Joint Active RIS Configuration and User Power Control for Localization: A Neuroevolution-Based Approach](https://arxiv.org/abs/2510.13819)
*George Stamatelis,Hui Chen,Henk Wymeersch,George C. Alexandropoulos*

Main category: cs.NI

TL;DR: 本文提出一种基于神经进化和监督学习的混合多智能体算法，用于RIS辅助用户定位中的RIS相位配置和用户发射功率的联合控制，仅需单比特反馈，性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用可重构智能表面（RIS）辅助用户定位，并通过动态功率控制进一步优化定位性能。

Method: 采用从基站到用户的反馈链路，实现上行链路用户导频传输的动态功率控制。提出一种新颖的多智能体算法，该算法结合了神经进化（NE）和监督学习的混合方法，用于联合控制RIS的相位配置和用户的发射功率。该方案仅需要单比特反馈消息进行上行链路功率控制，并支持具有离散响应的RIS单元。

Result: 数值结果表明，所提出的方案在性能上优于指纹定位、深度强化学习基线以及基于反向传播的定位估计器。

Conclusion: 该研究成功开发了一种高效的RIS辅助用户定位方案，通过创新的多智能体混合算法实现了RIS与用户功率的联合优化，并在低反馈开销下显著提升了定位精度。

Abstract: This paper studies user localization aided by a Reconfigurable Intelligent
Surface (RIS). A feedback link from the Base Station (BS) to the user is
adopted to enable dynamic power control of the user pilot transmissions in the
uplink. A novel multi-agent algorithm for the joint control of the RIS phase
configuration and the user transmit power is presented, which is based on a
hybrid approach integrating NeuroEvolution (NE) and supervised learning. The
proposed scheme requires only single-bit feedback messages for the uplink power
control, supports RIS elements with discrete responses, and is numerically
shown to outperform fingerprinting, deep reinforcement learning baselines and
backpropagation-based position estimators.

</details>


### [223] [Leveraging Wireless Sensor Networks for Real-Time Monitoring and Control of Industrial Environments](https://arxiv.org/abs/2510.13820)
*Muhammad Junaid Asif,Shazia Saqib,Rana Fayyaz Ahmad,Hamza Khan*

Main category: cs.NI

TL;DR: 本研究提出了一种基于物联网和无线通信的工业参数远程监测与控制系统，旨在提高工业运营效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 解决传统有线通信系统在工业监测中的局限性，消除对物理现场监测的需求；应对2020-2024年间全球工业火灾事故上升的趋势，提高工业运行效率和安全性。

Method: 开发一个基于NRF收发器和ARDUINO微控制器的无线传感器网络（WSN）系统。该系统通过多个传感器实时监测工业关键参数（如温度、湿度、土壤湿度和火灾），并将数据传输至中央设置，通过LCD屏显示并支持互联网远程查看。此外，系统还具备远程控制功能，可通过在线指令控制直流电机的速度。

Result: 该系统成功实现了工业参数的远程监测和控制，显著降低了物理监测相关的风险，并在紧急情况下（包括启动消防设备）提供快速响应。研究结果表明，无线通信创新在工业过程自动化和安全中发挥着重要作用。

Conclusion: 物联网（IoT）和无线传感器网络（WSN）的集成有望彻底改变多种工业应用的监测与控制方式，带来更智能、响应更迅速的操作环境，从而显著提高工业生产力和安全性。

Abstract: This research proposes an extensive technique for monitoring and controlling
the industrial parameters using Internet of Things (IoT) technology based on
wireless communication. We proposed a system based on NRF transceivers to
establish a strong Wireless Sensor Network (WSN), enabling transfer of
real-time data from multiple sensors to a central setup that is driven by
ARDUINO microcontrollers. Different key parameters, crucial for industrial
setup such as temperature, humidity, soil moisture and fire detection, are
monitored and displayed on an LCD screen, enabling factory administration to
oversee the industrial operations remotely over the internet. Our proposed
system bypasses the need for physical presence for monitoring by addressing the
shortcomings of conventional wired communication systems. Other than
monitoring, there is an additional feature to remotely control these parameters
by controlling the speed of DC motors through online commands. Given the rising
incidence of industrial fires over the worldwide between 2020 and 2024 due to
an array of hazards, this system with dual functionality boosts the overall
operational efficiency and safety. This overall integration of IoT and Wireless
Sensor Network (WSN) reduces the potential risks linked with physical
monitoring, providing rapid responses in emergency scenarios, including the
activation of firefighting equipment. The results show that innovations in
wireless communication perform an integral part in industrial process
automation and safety, paving the way to more intelligent and responsive
operating environments. Overall, this study highlights the potential for change
of IoT-enabled systems to revolutionize monitoring and control in a variety of
industrial applications, resulting in increased productivity and safety.

</details>


### [224] [LLM Agent Communication Protocol (LACP) Requires Urgent Standardization: A Telecom-Inspired Protocol is Necessary](https://arxiv.org/abs/2510.13821)
*Xin Li,Mengbing Liu,Chau Yuen*

Main category: cs.NI

TL;DR: 本立场文件提出，LLM代理需要一个统一的、受电信启发通信协议（LACP），以解决碎片化问题，确保下一代网络中的安全性、互操作性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代理的临时通信方法导致生态系统碎片化，阻碍创新并带来重大风险。为确保下一代网络中的安全性、互操作性和可扩展性，需要一个统一的协议。

Method: 提出LLM-Agent通信协议（LACP），该协议借鉴了现代电信的分层标准化协议。LACP建立了一个三层架构，旨在确保通信的语义清晰度、复杂任务的事务完整性以及强大的内置安全性。

Result: 该论文提出了LACP协议，其三层架构设计旨在实现通信的语义清晰、事务完整性和内置安全，作为解决LLM代理通信挑战的方案。

Conclusion: 采纳一个原则性的、通用通信协议对于实现分布式AI的潜力至关重要，并能确保多代理系统在6G及未来复杂实时应用中安全可靠地运行。

Abstract: This position paper argues that the field of LLM agents requires a unified,
telecom-inspired communication protocol to ensure safety, interoperability, and
scalability, especially within the context of Next Generation (NextG) networks.
Current ad-hoc communication methods are creating a fragmented ecosystem,
reminiscent of the early "protocol wars" in networking, which stifles
innovation and poses significant risks. Drawing inspiration from the layered,
standardized protocols that underpin modern telecommunications, we propose the
LLM-Agent Communication Protocol (LACP). LACP establishes a three-layer
architecture designed to ensure semantic clarity in communication,
transactional integrity for complex tasks, and robust, built-in security. In
this position paper, we argue that adopting a principled, universal protocol is
not merely beneficial but essential for realizing the potential of distributed
AI. Such a standard is critical for ensuring that multi-agent systems can
operate safely and reliably in the complex, real-time applications envisioned
for 6G and beyond.

</details>


### [225] [A Simulator for FANETs Using 5G Vehicle-to-Everything Communications and Named-Data Networking](https://arxiv.org/abs/2510.13823)
*José Manuel Rúa-Estévez,Alicia Meleiro-Estévez,Pablo Fondo-Ferreiro,Felipe Gil-Castiñeira,Brais Sánchez-Rama,Lois Gomez-Gonzalez*

Main category: cs.NI

TL;DR: 该工作提出了一种利用5G V2X和NDN范式，用于验证、评估和演示飞行自组织网络（FANETs）的模拟器。


<details>
  <summary>Details</summary>
Motivation: 验证、评估和演示结合5G V2X通信和命名数据网络（NDN）范式的飞行自组织网络（FANETs）。

Method: 该模拟器集成了ns-3网络模拟器和Zenoh NDN协议。

Result: 实现了对涉及多个无人机（UAVs）之间多跳通信的应用程序进行真实性测试。

Conclusion: 该模拟器为基于5G V2X和NDN的FANETs中多跳UAV通信的应用程序提供了一个现实的测试平台。

Abstract: This work presents a simulator designed for the validation, evaluation, and
demonstration of flying adhoc networks (FANETs) using 5G vehicle-to-everything
(V2X) communications and the named-data networking (NDN) paradigm. The
simulator integrates the ns-3 network simulator and the Zenoh NDN protocol,
enabling realistic testing of applications that involve the multi-hop
communication among multiple unmanned aerial vehicles (UAVs).

</details>


### [226] [DiffLoc: Diffusion Model-Based High-Precision Positioning for 6G Networks](https://arxiv.org/abs/2510.14111)
*Taekyun Lee,Tommaso Balercia,Heasung Kim,Hyeji Kim,Jeffrey G. Andrews*

Main category: cs.NI

TL;DR: 论文提出DiffLoc框架，利用条件生成扩散模型直接处理MIMO CSI，实现户外UE亚厘米级高精度定位，且推理速度快，适用于6G。


<details>
  <summary>Details</summary>
Motivation: 传统户外指纹定位方法在大型动态环境中扩展性差、数据采集不切实际且精度不足。

Method: 引入DiffLoc框架，将条件生成扩散模型直接应用于高维大规模MIMO信道状态信息（CSI），学习从原始上行探测参考信号（SRS）指纹到连续地理坐标的直接映射。

Result: 实现前所未有的亚厘米级精度，最佳模型DiffLoc-CT融合精度达0.5厘米，单基站精度1-2厘米。相较于现有方法（监督回归误差>10米，网格融合误差3米），精度提升一个数量级。一致性训练使推理时间从200步减少到2步，仍保持高精度，并支持高速用户和未知轨迹。

Conclusion: DiffLoc框架通过创新的扩散模型方法，在户外UE定位中实现了亚厘米级超高精度和实时推理能力，展现了其在6G应用中的巨大潜力。

Abstract: This paper introduces a novel framework for high-accuracy outdoor user
equipment (UE) positioning that applies a conditional generative diffusion
model directly to high-dimensional massive MIMO channel state information
(CSI). Traditional fingerprinting methods struggle to scale to large, dynamic
outdoor environments and require dense, impractical data surveys. To overcome
these limitations, our approach learns a direct mapping from raw uplink
Sounding Reference Signal (SRS) fingerprints to continuous geographic
coordinates. We demonstrate that our DiffLoc framework achieves unprecedented
sub-centimeter precision, with our best model (DiffLoc-CT) delivering 0.5 cm
fusion accuracy and 1-2 cm single base station (BS) accuracy in a realistic,
ray-traced Tokyo urban macro-cell environment. This represents an
order-of-magnitude improvement over existing methods, including supervised
regression approaches (over 10 m error) and grid-based fusion (3 m error). Our
consistency training approach reduces inference time from 200 steps to just 2
steps while maintaining exceptional accuracy even for high-speed users (15-25
m/s) and unseen user trajectories, demonstrating the practical feasibility of
our framework for real-time 6G applications.

</details>


### [227] [Energy-Latency Optimization for Dynamic 5G Mobile Radio Access Networks](https://arxiv.org/abs/2510.14214)
*Gabriela N. Caspa H.,Carlos A. Astudillo,Nelson L. S. da Fonseca*

Main category: cs.NI

TL;DR: 针对5G RAN配置中带宽、时延和能耗挑战，本文提出MILP模型和启发式算法，优化功能切分、功能放置和路由，以平衡时延和能耗，并揭示两者之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 在5G网络中，基站解耦和新服务对无线接入网（RAN）配置带来带宽、时延和能耗挑战。现有研究多关注能耗或时延单一维度，少有在实际场景中同时考虑两者，且RAN能耗是运营商运营开支的主要部分，因此亟需优化RAN配置以平衡服务性能和能耗效率。

Method: 提出一个混合整数线性规划（MILP）模型，旨在最小化前传时延、最小化能耗，以及平衡时延与能耗的双目标优化。该模型用于确定eMBB、URLLC和mMTC业务切片的最佳功能切分选项、RAN功能放置和路由。同时，针对MILP的计算复杂性，还提出了一个符合RAN约束的启发式算法。评估考虑了不同拓扑、聚合gNB需求、功能切分组合，并引入时间敏感网络（TSN）建模进行时延分析。

Result: 研究结果揭示了前传时延和能耗之间存在权衡关系。

Conclusion: 该研究强调了动态RAN重新配置的必要性，并为优化现有及未来RAN部署提供了基础。

Abstract: In 5G networks, base station (BS) disaggregation and new services present
challenges in radio access network (RAN) configuration, particularly in meeting
their bandwidth and latency constraints. The BS disaggregation is enabled by
functional splitting (FS), which distributes the RAN functions in processing
nodes and alleviates latency and bandwidth requirements in the fronthaul (FH).
Besides network performance, energy consumption is a critical concern for
mobile network operators (MNO), since RAN operation constitutes a major portion
of their operational expenses (OPEX). RAN configuration optimization is
essential to balance service performance with cost-effective energy
consumption. In this paper, we propose a mixed-integer linear programming
(MILP) model formulated with three objective functions: (i) minimizing
fronthaul (FH) latency, (ii) minimizing energy consumption, and (iii) a
bi-objective optimization that jointly balances both latency and energy
consumption. The model determines the optimal FS option, RAN function
placement, and routing for eMBB, URLLC, and mMTC slices. Although prior studies
have addressed RAN configuration either from an energy minimization or latency
reduction perspective, few have considered both aspects in realistic scenarios.
Our evaluation spans different topologies, accounts for variations in
aggregated gNB demand, explores diverse FS combinations, and incorporates Time
Sensitive Networking (TSN) modeling for latency analysis, as it is also crucial
in RAN performance. Given that MILP's execution time can be significant, we
propose a heuristic algorithm that adheres to RAN constraints. Our results
reveal a trade-off between latency and energy consumption, highlighting the
need for dynamic RAN reconfiguration. These insights provide a foundation to
optimize existing and future RAN deployments.

</details>


### [228] [Automated Extraction of Protocol State Machines from 3GPP Specifications with Domain-Informed Prompts and LLM Ensembles](https://arxiv.org/abs/2510.14348)
*Miao Zhang,Runhan Feng,Hongbo Tang,Yu Zhao,Jie Yang,Hang Qiu,Qi Liu*

Main category: cs.NI

TL;DR: SpecGPT利用LLM自动从3GPP文档中提取协议状态机，解决了手动建模的痛点，并在5G协议评估中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 移动通信网络的安全性和可靠性依赖于准确的协议状态机建模。然而，手动从3GPP规范构建模型费时、易错且难以维护；现有自然语言处理方法在处理大规模复杂蜂窝协议时能力有限。

Method: 提出SpecGPT框架，该框架利用大型语言模型（LLMs）自动从3GPP文档中提取协议状态机。具体方法包括：将技术规范分割成有意义的段落、应用领域知识提示与思维链推理、以及采用集成方法提高输出可靠性。

Result: SpecGPT在NAS、NGAP和PFCP三种代表性5G协议上进行了评估，使用人工标注的真实数据作为基准，结果表明它优于现有方法。

Conclusion: 研究证明了大型语言模型（LLMs）在协议建模方面的有效性，能够大规模地从复杂3GPP规范中提取协议状态机，并显著优于传统方法。

Abstract: Mobile telecommunication networks are foundational to global infrastructure
and increasingly support critical sectors such as manufacturing,
transportation, and healthcare. The security and reliability of these networks
are essential, yet depend heavily on accurate modeling of underlying protocols
through state machines. While most prior work constructs such models manually
from 3GPP specifications, this process is labor-intensive, error-prone, and
difficult to maintain due to the complexity and frequent updates of the
specifications. Recent efforts using natural language processing have shown
promise, but remain limited in handling the scale and intricacy of cellular
protocols. In this work, we propose SpecGPT, a novel framework that leverages
large language models (LLMs) to automatically extract protocol state machines
from 3GPP documents. SpecGPT segments technical specifications into meaningful
paragraphs, applies domain-informed prompting with chain-of-thought reasoning,
and employs ensemble methods to enhance output reliability. We evaluate SpecGPT
on three representative 5G protocols (NAS, NGAP, and PFCP) using manually
annotated ground truth, and show that it outperforms existing approaches,
demonstrating the effectiveness of LLMs for protocol modeling at scale.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [229] [Generative AI in Heritage Practice: Improving the Accessibility of Heritage Guidance](https://arxiv.org/abs/2510.13811)
*Jessica Witte,Edmund Lee,Lisa Brausem,Verity Shillabeer,Chiara Bonacchi*

Main category: cs.HC

TL;DR: 本文开发并评估了HAZEL，一个用于文化遗产指导文件修订的微调GenAI聊天机器人。研究发现HAZEL表现略优于ChatGPT，但存在文化敏感性和高级技术专业限制，适用于辅助而非取代人类专家。


<details>
  <summary>Details</summary>
Motivation: 旨在通过将生成式人工智能 (GenAI) 整合到专业文化遗产实践中，以提高面向公众的指导文件的可及性。

Method: 开发了HAZEL，一个为协助修订文化遗产保护和解释相关书面指导而微调的GenAI聊天机器人。通过定量评估，将HAZEL在指导写作任务中的表现与ChatGPT (GPT-4) 进行了比较。

Result: 比较结果表明HAZEL的表现略优于ChatGPT，这表明一旦底层大型语言模型 (LLM) 经过微调，GenAI聊天机器人会更有效。然而，研究也指出HAZEL在需要文化敏感性和更高级技术专业知识的领域存在显著局限性。

Conclusion: GenAI无法取代人类遗产专业人员进行技术创作任务，但其自动化和加速指导写作某些方面的潜力，能为尤其在资源受限的背景下，为遗产组织提供宝贵的益处。

Abstract: This paper discusses the potential for integrating Generative Artificial
Intelligence (GenAI) into professional heritage practice with the aim of
enhancing the accessibility of public-facing guidance documents. We developed
HAZEL, a GenAI chatbot fine-tuned to assist with revising written guidance
relating to heritage conservation and interpretation. Using quantitative
assessments, we compare HAZEL's performance to that of ChatGPT (GPT-4) in a
series of tasks related to the guidance writing process. The results of this
comparison indicate a slightly better performance of HAZEL over ChatGPT,
suggesting that the GenAI chatbot is more effective once the underlying large
language model (LLM) has been fine-tuned. However, we also note significant
limitations, particularly in areas requiring cultural sensitivity and more
advanced technical expertise. These findings suggest that, while GenAI cannot
replace human heritage professionals in technical authoring tasks, its
potential to automate and expedite certain aspects of guidance writing could
offer valuable benefits to heritage organisations, especially in
resource-constrained contexts.

</details>


### [230] [Reversing the Lens: Using Explainable AI to Understand Human Expertise](https://arxiv.org/abs/2510.13814)
*Roussel Rahman,Aashwin Ananda Mishra,Wan-Lin Hu*

Main category: cs.HC

TL;DR: 本研究利用可解释人工智能（XAI）的计算工具分析人类学习过程，通过对粒子加速器操作员行为建模，揭示了人类如何分解复杂问题并随经验发展高效策略。


<details>
  <summary>Details</summary>
Motivation: 人类和机器学习模型均从经验中学习，尤其在关键领域。心理学旨在理解人类认知，而XAI则开发解释机器学习模型的方法。本研究旨在弥合这两个领域，利用XAI工具定量分析人类学习。

Method: 通过构建操作员子任务图，对操作员在调节粒子加速器这一复杂现实任务中的行为进行建模。利用归档操作员数据，应用了社群检测和层次聚类等技术。

Result: 研究揭示了操作员如何将问题分解为更简单的组件，以及这些问题解决结构如何随专业知识的增长而演变。结果阐明了人类在缺乏全局最优解决方案的情况下如何发展出高效策略。

Conclusion: 本研究展示了基于XAI的方法在定量研究人类认知方面的实用性，为理解人类学习和问题解决策略提供了新视角。

Abstract: Both humans and machine learning models learn from experience, particularly
in safety- and reliability-critical domains. While psychology seeks to
understand human cognition, the field of Explainable AI (XAI) develops methods
to interpret machine learning models. This study bridges these domains by
applying computational tools from XAI to analyze human learning. We modeled
human behavior during a complex real-world task -- tuning a particle
accelerator -- by constructing graphs of operator subtasks. Applying techniques
such as community detection and hierarchical clustering to archival operator
data, we reveal how operators decompose the problem into simpler components and
how these problem-solving structures evolve with expertise. Our findings
illuminate how humans develop efficient strategies in the absence of globally
optimal solutions, and demonstrate the utility of XAI-based methods for
quantitatively studying human cognition.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [231] [Decoherence-Aware Entangling and Swapping Strategy Optimization for Entanglement Routing in Quantum Networks](https://arxiv.org/abs/2510.14912)
*Shao-Min Huang,Cheng-Yang Cheng,Ming-Huang Chien,Jian-Jhih Kuo,Chih-Yu Wang*

Main category: quant-ph

TL;DR: 为解决量子纠缠对的退相干和交换损失导致通信保真度低的问题，本文提出了一种短时间槽协议，并引入了TETRIS优化问题，设计了新型算法显著提升了量子传送的保真度。


<details>
  <summary>Details</summary>
Motivation: 量子传送中，纠缠对会因环境干扰随时间退相干，导致低保真度；同时，纠缠交换过程也会引入额外的保真度损失。因此，如何优化纠缠对的生成时机和交换策略以最大化保真度是关键挑战。

Method: 本文提出了一种“短时间槽协议”，其中每个时间槽仅容纳一个操作，从而实现比传统长时间槽协议更灵活的纠缠与交换过程调度。在此基础上，提出并定义了一个新的优化问题TETRIS，旨在为每个请求寻找最佳的纠缠和交换策略，以最大化所有已接受请求的保真度总和。为解决TETRIS问题，设计了两种采用不同优化技术的新颖算法。

Result: 仿真结果表明，所提出的算法在普遍情况下比现有方法性能提升60%至78%；即使在低纠缠概率下，性能也能提升20%至75%。

Conclusion: 通过引入短时间槽协议和TETRIS优化算法，能够有效应对量子纠缠对的退相干和交换损失问题，显著提高量子通信的端到端保真度。

Abstract: Quantum teleportation enables high-security communications through end-to-end
quantum entangled pairs. End-to-end entangled pairs are created by using
swapping processes to consume short entangled pairs and generate long pairs.
However, due to environmental interference, entangled pairs decohere over time,
resulting in low fidelity. Thus, generating entangled pairs at the right time
is crucial. Moreover, the swapping process also causes additional fidelity
loss. To this end, this paper presents a short time slot protocol, where a time
slot can only accommodate a process. It has a more flexible arrangement of
entangling and swapping processes than the traditional long time slot protocol.
It raises a new optimization problem TETRIS for finding strategies of
entangling and swapping for each request to maximize the fidelity sum of all
accepted requests. To solve the TETRIS, we design two novel algorithms with
different optimization techniques. Finally, the simulation results manifest
that our algorithms can outperform the existing methods by up to 60 ~ 78% in
general, and by 20 ~ 75% even under low entangling probabilities.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [232] [Noisy Networks, Nosy Neighbors: Inferring Privacy Invasive Information from Encrypted Wireless Traffic](https://arxiv.org/abs/2510.13822)
*Bartosz Burgiel*

Main category: cs.CR

TL;DR: 研究表明，即使数据加密，通过被动监听智能家居的Wi-Fi和BLE流量，邻居也能推断出居民的隐私信息，如设备活动、日常作息和房间布局。


<details>
  <summary>Details</summary>
Motivation: 探讨在智能家居环境中，被动观察无线流量能在多大程度上推断出居民的隐私信息。

Method: 模拟邻居监听场景，分析原始802.11数据包和蓝牙低功耗广告，识别设备、推断活动状态并使用RSSI三边测量法近似位置。

Result: 尽管数据加密，仍能检测多媒体设备活跃期、推断睡眠/工作/媒体消费等日常活动，甚至近似邻居公寓布局。

Conclusion: 智能家居的隐私风险超越传统数据泄露；邻居仅通过加密网络流量即可获取侵犯隐私的洞察。

Abstract: This thesis explores the extent to which passive observation of wireless
traffic in a smart home environment can be used to infer privacy-invasive
information about its inhabitants. Using a setup that mimics the capabilities
of a nosy neighbor in an adjacent flat, we analyze raw 802.11 packets and
Bluetooth Low Energy advertisemets. From this data, we identify devices, infer
their activity states and approximate their location using RSSI-based
trilateration. Despite the encrypted nature of the data, we demonstrate that it
is possible to detect active periods of multimedia devices, infer common
activities such as sleeping, working and consuming media, and even approximate
the layout of the neighbor's apartment. Our results show that privacy risks in
smart homes extend beyond traditional data breaches: a nosy neighbor behind the
wall can gain privacy-invasive insights into the lives of their neighbors
purely from encrypted network traffic.

</details>


### [233] [Multi-Layer Secret Sharing for Cross-Layer Attack Defense in 5G Networks: a COTS UE Demonstration](https://arxiv.org/abs/2510.13824)
*Wai Ming Chan,Remi Chou,Taejoon Kim*

Main category: cs.CR

TL;DR: 首次在商用5G用户设备上实现多层秘密共享，通过异或方法在不修改基础设施或预共享密钥的情况下，即使同时丢失一个网络运营商和一个中继，也能确保数据恢复和机密性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决商用5G用户设备在不依赖基础设施修改或预共享密钥的情况下，如何实现多层秘密共享，并在面临网络运营商和中继同时故障（如DoS攻击）时，仍能确保完美恢复和数据机密性。

Method: 采用基于异或（XOR）的方法，将秘密份额分发到网络运营商和分布式中继上。该方案在商用（COTS）5G用户设备（UE）上实现，无需修改现有基础设施或预设共享密钥。

Result: 首次在商用5G用户设备上实现了多层秘密共享。结果表明，即使在极端情况（如一个网络运营商和一个中继同时丢失，或遭受DoS攻击和意外攻击）下，也能确保完美的数据恢复和机密性。

Conclusion: 本演示验证了一种新颖的多层秘密共享方案在商用5G用户设备上的可行性，该方案在不依赖基础设施修改或预设密钥的情况下，为5G环境提供了高数据机密性和对多种故障的强大韧性。

Abstract: This demo presents the first implementation of multi-layer secret sharing on
commercial-off-the-shelf (COTS) 5G user equipment (UE), operating without
infrastructure modifications or pre-shared keys. Our XOR-based approach
distributes secret shares across network operators and distributed relays,
ensuring perfect recovery and data confidentiality even if one network operator
and one relay are simultaneously lost (e.g., under denial of service (DoS) or
unanticipated attacks).

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [234] [GQVis: A Dataset of Genomics Data Questions and Visualizations for Generative AI](https://arxiv.org/abs/2510.13816)
*Skylar Sargent Walters,Arthea Valderrama,Thomas C. Smits,David Kouřil,Huyen N. Nguyen,Sehi L'Yi,Devin Lange,Nils Gehlenborg*

Main category: q-bio.GN

TL;DR: 本文提出一个框架，用于生成一个名为GQVis的大规模数据集，该数据集将基因组学数据相关的低级问题与相应的可视化配对，旨在为基因组可视化领域的机器学习模型提供训练基础。


<details>
  <summary>Details</summary>
Motivation: 基因组数据可视化是基因组学研究的重要工具，但当前的机器学习模型缺乏针对基因组学领域特定任务的训练基础，难以将数据有效转化为有洞察力的可视化。

Method: 研究构建了一个框架，用于生成一个配对基因组数据抽象问题与对应可视化的数据集。该方法借鉴了现有统计图工作，并适应了基因组数据的复杂性和专业表示。数据集中还包含了多重链接查询、可视化、设计理由、图标题和图像alt-text。数据源自4DN、ENCODE和Chromoscope三个基因组数据存储库。

Result: 研究成功生成了GQVis数据集，包含114万个单查询数据点、62.8万个查询对和58.9万个查询链。该数据集及其生成代码已公开提供。

Conclusion: 本研究通过提供一个大规模、领域特定的数据集GQVis，为基因组学领域机器学习模型的训练奠定了基础，有望解决当前模型在基因组数据可视化任务中缺乏领域知识的问题。

Abstract: Data visualization is a fundamental tool in genomics research, enabling the
exploration, interpretation, and communication of complex genomic features.
While machine learning models show promise for transforming data into
insightful visualizations, current models lack the training foundation for
domain-specific tasks. In an effort to provide a foundational resource for
genomics-focused model training, we present a framework for generating a
dataset that pairs abstract, low-level questions about genomics data with
corresponding visualizations. Building on prior work with statistical plots,
our approach adapts to the complexity of genomics data and the specialized
representations used to depict them. We further incorporate multiple linked
queries and visualizations, along with justifications for design choices,
figure captions, and image alt-texts for each item in the dataset. We use
genomics data retrieved from three distinct genomics data repositories (4DN,
ENCODE, Chromoscope) to produce GQVis: a dataset consisting of 1.14 million
single-query data points, 628k query pairs, and 589k query chains. The GQVis
dataset and generation code are available at
https://huggingface.co/datasets/HIDIVE/GQVis and
https://github.com/hms-dbmi/GQVis-Generation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [235] [Millimeter Wave Inverse Pinhole Imaging](https://arxiv.org/abs/2510.13904)
*Akarsh Prabhakara,Yawen Liu,Aswin C. Sankaranarayanan,Anthony Rowe,Swarun Kumar*

Main category: eess.IV

TL;DR: 本文提出Umbra系统，利用旋转毫米波“反针孔”技术，显著提升了静态紧凑型毫米波雷达的角分辨率，实现了5倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 紧凑型毫米波雷达在静态或瞬时静态场景（如悬停无人机）中应用广泛，但其尺寸限制导致角分辨率受限，影响了在视觉受限环境下的感知能力。

Method: 引入了名为Umbra的高分辨率毫米波成像系统，核心思想是利用旋转毫米波“反针孔”来增强角分辨率。文中详细阐述了成像系统模型、设计及评估，并指出反针孔的轻量化特性利于低功耗旋转。此外，还展示了飞行器螺旋桨可作为天然反针孔。

Result: Umbra系统通过单天线即可实现2.5°的角分辨率，相较于基准紧凑型毫米波雷达的14°分辨率，性能提升了5倍。

Conclusion: 旋转毫米波“反针孔”概念有效解决了紧凑型静态毫米波雷达的角分辨率限制问题，Umbra系统展示了显著的性能提升，并提供了将无人机螺旋桨作为天然反针孔的实用见解。

Abstract: Millimeter wave (mmWave) radars are popular for perception in vision-denied
contexts due to their compact size. This paper explores emerging use-cases that
involve static mount or momentarily-static compact radars, for example, a
hovering drone. The key challenge with static compact radars is that their
limited form-factor also limits their angular resolution. This paper presents
Umbra, a mmWave high resolution imaging system, that introduces the concept of
rotating mmWave "inverse pinholes" for angular resolution enhancement. We
present the imaging system model, design, and evaluation of mmWave inverse
pinholes. The inverse pinhole is attractive for its lightweight nature, which
enables low-power rotation, upgrading static-mount radars. We also show how
propellers in aerial vehicles act as natural inverse pinholes and can enjoy the
benefits of high-resolution imaging even while they are momentarily static,
e.g., hovering. Our evaluation shows Umbra resolving up to 2.5$^{\circ}$ with
just a single antenna, a 5$\times$ improvement compared to 14$^{\circ}$ from a
compact mmWave radar baseline.

</details>
