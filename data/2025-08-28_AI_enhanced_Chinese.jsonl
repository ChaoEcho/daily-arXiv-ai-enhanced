{"id": "2508.19350", "pdf": "https://arxiv.org/pdf/2508.19350", "abs": "https://arxiv.org/abs/2508.19350", "authors": ["Kaiqiang Lin", "Mohamed-Slim Alouini"], "title": "Connectivity Analysis of LoRaWAN-Based Non-Terrestrial Networks for Subterranean mMTC", "categories": ["cs.NI"], "comment": "13 pages, 10 figures, 5 tables, submitted to IEEE IoTJ", "summary": "Wireless underground sensor networks (WUSNs) offer significant social and\neconomic benefits by enabling the monitoring of subterranean entities. However,\nthe communication reliability of WUSNs diminishes in harsh environments where\nterrestrial network infrastructure is either unavailable or unreliable. To\naddress this challenge, we explore the feasibility of integrating buried\nmassive machine-type communication (mMTC) sensors with non-terrestrial networks\n(NTNs), including unmanned aerial vehicles (UAVs), high-altitude platforms\n(HAPs), and low Earth orbit (LEO) satellites, to establish underground-to-NTN\nconnectivity for various large-scale underground monitoring applications. To\nassess the effectiveness of underground-to-NTN connectivity, we develop a Monte\nCarlo simulator that incorporates a multi-layer underground attenuation model,\nthe 3GPP empirical path loss model for various NTN platforms, and two LoRaWAN\nmodulation schemes, i.e., LoRa and LoRa-frequency hopping spread spectrum\n(LR-FHSS). Our results evidence that LoRa SF7 is a strong candidate for\nshort-range UAV communication in rural environments, while LR-FHSS modulation\nproves to be a promising option for HAP and LEO satellite platforms in massive\nWUSNs scenarios thanks to its adequate link budget and robustness to the\ninterference. Finally, we demonstrate that the success probability of\nunderground-to-NTN connectivity using LoRa and LR-FHSS is significantly\naffected by factors such as the monitoring environment, the number of devices,\nburial depth, and the soil's volumetric water content.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5c06\u5730\u4e0b\u65e0\u7ebf\u4f20\u611f\u5668\u7f51\u7edc\uff08WUSNs\uff09\u4e0e\u975e\u5730\u9762\u7f51\u7edc\uff08NTNs\uff0c\u5305\u62ecUAVs\u3001HAPs\u548cLEO\u536b\u661f\uff09\u96c6\u6210\uff0c\u4ee5\u63d0\u9ad8\u6076\u52a3\u73af\u5883\u4e0b\u901a\u4fe1\u53ef\u9760\u6027\u7684\u53ef\u884c\u6027\uff0c\u5e76\u8bc4\u4f30\u4e86LoRa\u548cLR-FHSS\u8c03\u5236\u65b9\u6848\u5728\u4e0d\u540cNTN\u5e73\u53f0\u4e0a\u7684\u6027\u80fd\u548c\u5f71\u54cd\u56e0\u7d20\u3002", "motivation": "\u89e3\u51b3\u65e0\u7ebf\u5730\u4e0b\u4f20\u611f\u5668\u7f51\u7edc\uff08WUSNs\uff09\u5728\u5730\u9762\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u4e0d\u53ef\u7528\u6216\u4e0d\u53ef\u9760\u7684\u6076\u52a3\u73af\u5883\u4e2d\u901a\u4fe1\u53ef\u9760\u6027\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u5404\u7c7b\u5927\u89c4\u6a21\u5730\u4e0b\u76d1\u6d4b\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8499\u7279\u5361\u6d1b\u6a21\u62df\u5668\uff0c\u8be5\u6a21\u62df\u5668\u7ed3\u5408\u4e86\u591a\u5c42\u5730\u4e0b\u8870\u51cf\u6a21\u578b\u3001\u9488\u5bf9\u4e0d\u540c\u975e\u5730\u9762\u7f51\u7edc\uff08NTN\uff09\u5e73\u53f0\uff08UAV\u3001HAP\u3001LEO\u536b\u661f\uff09\u76843GPP\u7ecf\u9a8c\u8def\u5f84\u635f\u8017\u6a21\u578b\uff0c\u4ee5\u53caLoRaWAN\u7684\u4e24\u79cd\u8c03\u5236\u65b9\u6848\uff1aLoRa\u548cLoRa-\u9891\u7387\u8df3\u53d8\u6269\u9891\uff08LR-FHSS\uff09\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLoRa SF7\u662f\u519c\u6751\u73af\u5883\u4e2d\u77ed\u7a0bUAV\u901a\u4fe1\u7684\u6709\u529b\u5019\u9009\uff1bLR-FHSS\u8c03\u5236\u65b9\u6848\u51ed\u501f\u5176\u8db3\u591f\u7684\u94fe\u8def\u9884\u7b97\u548c\u6297\u5e72\u6270\u80fd\u529b\uff0c\u5728\u5927\u89c4\u6a21WUSN\u573a\u666f\u4e2d\u5bf9HAP\u548cLEO\u536b\u661f\u5e73\u53f0\u800c\u8a00\u662f\u524d\u666f\u5e7f\u9614\u7684\u9009\u62e9\u3002\u6b64\u5916\uff0c\u5730\u4e0b\u5230NTN\u8fde\u63a5\u7684\u6210\u529f\u6982\u7387\u53d7\u76d1\u6d4b\u73af\u5883\u3001\u8bbe\u5907\u6570\u91cf\u3001\u57cb\u6df1\u548c\u571f\u58e4\u4f53\u79ef\u542b\u6c34\u91cf\u7b49\u56e0\u7d20\u7684\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u5c06WUSNs\u4e0eNTNs\u96c6\u6210\u80fd\u591f\u663e\u8457\u63d0\u5347\u6076\u52a3\u73af\u5883\u4e0b\u7684\u901a\u4fe1\u53ef\u9760\u6027\u3002LoRa SF7\u9002\u7528\u4e8e\u77ed\u7a0bUAV\u901a\u4fe1\uff0c\u800cLR-FHSS\u66f4\u9002\u5408\u5927\u89c4\u6a21WUSN\u4e0eHAP/LEO\u536b\u661f\u7684\u8fde\u63a5\u3002\u540c\u65f6\uff0c\u76d1\u6d4b\u73af\u5883\u3001\u8bbe\u5907\u6570\u91cf\u3001\u57cb\u6df1\u548c\u571f\u58e4\u542b\u6c34\u91cf\u662f\u5f71\u54cd\u5730\u4e0b\u5230NTN\u8fde\u63a5\u6210\u529f\u7387\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2508.19736", "pdf": "https://arxiv.org/pdf/2508.19736", "abs": "https://arxiv.org/abs/2508.19736", "authors": ["Mohsen Ahadi", "Adeel Malik", "Omid Esrafilian", "Florian Kaltenberger", "Cedric Thienot"], "title": "Experimental Insights from OpenAirInterface 5G positioning Testbeds: Challenges and solutions", "categories": ["cs.NI"], "comment": "8 pages", "summary": "5G New Radio (NR) is a key enabler of accurate positioning in smart cities\nand smart factories. This paper presents the experimental results from three 5G\npositioning testbeds running open-source OpenAirInterface (OAI) gNB and Core\nNetwork (CN), using Uplink Time Difference of Arrival (UL-TDoA) with the newly\nintegrated Location Management Function (LMF). The testbeds are deployed across\nboth indoor factories and outdoor scenarios with O-RAN Radio Units (RUs),\nfollowing a 3GPP-compliant system model. The experiments highlight the impact\nof synchronization impairments, multipath propagation, and deployment geometry\non positioning accuracy. To address these challenges, we propose tailored ToA\nand TDoA filtering as well as a novel position estimation method based on\nParticle Swarm Optimization (PSO) within the LMF pipeline. Moreover, we show a\nbeyond-5G framework that leverages non-conventional measurements such as\nChannel Impulse Response (CIR) to train and test Artificial Intelligence and\nMachine Learning (AI/ML) models for data-driven positioning. The results\ndemonstrate the feasibility of achieving 1-2 meter positioning accuracy in 90%\nof cases in different testbeds, offering practical insights for the design of\nrobust 5G positioning systems. Moreover, we publicly release the datasets\ncollected in this work to support the research within the 5G positioning\ncommunity.", "AI": {"tldr": "\u672c\u6587\u5728\u4e09\u4e2a5G\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0c\u57fa\u4e8eUL-TDoA\u65b9\u6cd5\uff0c\u7ed3\u5408\u6240\u63d0\u51fa\u7684\u6ee4\u6ce2\u548cPSO\u5b9a\u4f4d\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e861-2\u7c73\u9ad8\u7cbe\u5ea6\u76845G\u5b9a\u4f4d\uff0c\u5e76\u63a2\u8ba8\u4e86AI/ML\u6570\u636e\u9a71\u52a8\u5b9a\u4f4d\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u3002", "motivation": "5G NR\u662f\u667a\u6167\u57ce\u5e02\u548c\u5de5\u5382\u4e2d\u5b9e\u73b0\u7cbe\u51c6\u5b9a\u4f4d\u7684\u5173\u952e\u6280\u672f\u3002\u7136\u800c\uff0c\u540c\u6b65\u8bef\u5dee\u3001\u591a\u5f84\u4f20\u64ad\u548c\u90e8\u7f72\u51e0\u4f55\u7ed3\u6784\u7b49\u56e0\u7d20\u4f1a\u5f71\u54cd\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5f00\u6e90OpenAirInterface (OAI)\u7684\u4e09\u4e2a5G\u6d4b\u8bd5\u5e73\u53f0\uff08\u5ba4\u5185\u5de5\u5382\u548c\u5ba4\u5916\u573a\u666f\uff09\uff0c\u91c7\u7528UL-TDoA\u6280\u672f\u5e76\u96c6\u6210Location Management Function (LMF)\u3002\u4e3a\u5e94\u5bf9\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u5b9a\u5236\u5316\u7684ToA/TDoA\u6ee4\u6ce2\u548c\u57fa\u4e8e\u7c92\u5b50\u7fa4\u4f18\u5316 (PSO) \u7684\u65b0\u578b\u5b9a\u4f4d\u4f30\u8ba1\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u5c55\u793a\u4e86\u4e00\u4e2a\u5229\u7528\u4fe1\u9053\u51b2\u6fc0\u54cd\u5e94 (CIR) \u7b49\u975e\u4f20\u7edf\u6d4b\u91cf\u6570\u636e\uff0c\u901a\u8fc7AI/ML\u6a21\u578b\u5b9e\u73b0\u6570\u636e\u9a71\u52a8\u5b9a\u4f4d\u7684\u8d855G\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e0d\u540c\u6d4b\u8bd5\u573a\u666f\u4e0b\uff0c90%\u7684\u60c5\u51b5\u4e0b\u53ef\u5b9e\u73b01-2\u7c73\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u540c\u6b65\u8bef\u5dee\u3001\u591a\u5f84\u4f20\u64ad\u548c\u90e8\u7f72\u51e0\u4f55\u7ed3\u6784\u5bf9\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002\u540c\u65f6\uff0c\u516c\u5f00\u4e86\u6536\u96c6\u5230\u7684\u6570\u636e\u96c6\u4ee5\u652f\u6301\u7814\u7a76\u793e\u533a\u3002", "conclusion": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e865G\u5b9a\u4f4d\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u5e76\u901a\u8fc7\u63d0\u51fa\u7684\u521b\u65b0\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u8bbe\u8ba1\u9c81\u68d2\u76845G\u5b9a\u4f4d\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u9645\u89c1\u89e3\uff0c\u5e76\u4e14\u901a\u8fc7\u53d1\u5e03\u6570\u636e\u96c6\u4fc3\u8fdb\u4e86\u793e\u533a\u7814\u7a76\u3002"}}
{"id": "2508.19870", "pdf": "https://arxiv.org/pdf/2508.19870", "abs": "https://arxiv.org/abs/2508.19870", "authors": ["Yinqiu Liu", "Ruichen Zhang", "Haoxiang Luo", "Yijing Lin", "Geng Sun", "Dusit Niyato", "Hongyang Du", "Zehui Xiong", "Yonggang Wen", "Abbas Jamalipour", "Dong In Kim", "Ping Zhang"], "title": "Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey", "categories": ["cs.NI"], "comment": "35 pages", "summary": "Agentification serves as a critical enabler of Edge General Intelligence\n(EGI), transforming massive edge devices into cognitive agents through\nintegrating Large Language Models (LLMs) and perception, reasoning, and acting\nmodules. These agents collaborate across heterogeneous edge infrastructures,\nforming multi-LLM agentic AI systems that leverage collective intelligence and\nspecialized capabilities to tackle complex, multi-step tasks. However, the\ncollaborative nature of multi-LLM systems introduces critical security\nvulnerabilities, including insecure inter-LLM communications, expanded attack\nsurfaces, and cross-domain data leakage that traditional perimeter-based\nsecurity cannot adequately address. To this end, this survey introduces\nzero-trust security of multi-LLM in EGI, a paradigmatic shift following the\n``never trust, always verify'' principle. We begin by systematically analyzing\nthe security risks in multi-LLM systems within EGI contexts. Subsequently, we\npresent the vision of a zero-trust multi-LLM framework in EGI. We then survey\nkey technical progress to facilitate zero-trust multi-LLM systems in EGI.\nParticularly, we categorize zero-trust security mechanisms into model- and\nsystem-level approaches. The former and latter include strong identification,\ncontext-aware access control, etc., and proactive maintenance, blockchain-based\nmanagement, etc., respectively. Finally, we identify critical research\ndirections. This survey serves as the first systematic treatment of zero-trust\napplied to multi-LLM systems, providing both theoretical foundations and\npractical strategies.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u9488\u5bf9\u8fb9\u7f18\u901a\u7528\u667a\u80fd\uff08EGI\uff09\u4e2d\u591aLLM\u7cfb\u7edf\u9762\u4e34\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u9996\u6b21\u63d0\u51fa\u4e86\u96f6\u4fe1\u4efb\u5b89\u5168\u8303\u5f0f\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u98ce\u9669\uff0c\u5f52\u7eb3\u4e86\u6a21\u578b\u7ea7\u4e0e\u7cfb\u7edf\u7ea7\u96f6\u4fe1\u4efb\u5b89\u5168\u673a\u5236\uff0c\u540c\u65f6\u6307\u660e\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u534f\u4f5c\u5f0f\u591aLLM\u7cfb\u7edf\u5728EGI\u73af\u5883\u4e2d\u5f15\u5165\u4e86\u4e25\u91cd\u7684\u4e92LLM\u901a\u4fe1\u4e0d\u5b89\u5168\u3001\u653b\u51fb\u9762\u6269\u5927\u53ca\u8de8\u57df\u6570\u636e\u6cc4\u9732\u7b49\u5b89\u5168\u6f0f\u6d1e\uff0c\u800c\u4f20\u7edf\u57fa\u4e8e\u8fb9\u754c\u7684\u5b89\u5168\u63aa\u65bd\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u7cfb\u7edf\u5206\u6790EGI\u4e2d\u591aLLM\u7cfb\u7edf\u7684\u5b89\u5168\u98ce\u9669\uff1b\u63a5\u7740\u63d0\u51fa\u96f6\u4fe1\u4efb\u591aLLM\u6846\u67b6\u7684\u613f\u666f\uff1b\u968f\u540e\u8c03\u67e5\u5e76\u5206\u7c7b\u96f6\u4fe1\u4efb\u5b89\u5168\u673a\u5236\u4e3a\u6a21\u578b\u7ea7\uff08\u5982\u5f3a\u8bc6\u522b\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u8bbf\u95ee\u63a7\u5236\uff09\u548c\u7cfb\u7edf\u7ea7\uff08\u5982\u4e3b\u52a8\u7ef4\u62a4\u3001\u533a\u5757\u94fe\u7ba1\u7406\uff09\u65b9\u6cd5\uff1b\u6700\u540e\u8bc6\u522b\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002", "result": "\u672c\u7efc\u8ff0\u4f5c\u4e3a\u9996\u6b21\u5c06\u96f6\u4fe1\u4efb\u539f\u5219\u7cfb\u7edf\u5e94\u7528\u4e8e\u591aLLM\u7cfb\u7edf\u7684\u7814\u7a76\uff0c\u4e3aEGI\u4e2d\u7684\u591aLLM\u4ee3\u7406\u5f0fAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u7b56\u7565\uff0c\u5e76\u6307\u660e\u4e86\u672a\u6765\u7684\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u7efc\u8ff0\u901a\u8fc7\u5f15\u5165\u96f6\u4fe1\u4efb\u5b89\u5168\u8303\u5f0f\uff0c\u4e3a\u89e3\u51b3EGI\u4e2d\u591aLLM\u7cfb\u7edf\u56fa\u6709\u7684\u5b89\u5168\u6311\u6218\u63d0\u4f9b\u4e86\u5f00\u521b\u6027\u7684\u7cfb\u7edf\u6027\u5904\u7406\u65b9\u6cd5\uff0c\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u3001\u53ef\u4fe1\u7684\u8fb9\u7f18\u901a\u7528\u667a\u80fd\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.20044", "pdf": "https://arxiv.org/pdf/2508.20044", "abs": "https://arxiv.org/abs/2508.20044", "authors": ["Kfir Toledo", "Isaac Keslassy"], "title": "2SYN: Congestion-Aware Multihoming", "categories": ["cs.NI"], "comment": "Accepted at IEEE/IFIP NOMS", "summary": "When sending flows to arbitrary destinations, current multihoming routers\nadopt simple congestion-oblivious mechanisms. Therefore, they cannot avoid\ncongested paths.\n  In this paper, we introduce 2SYN, the first congestion-aware multihoming\nalgorithm that works for any destination. We explain how it dynamically selects\na preferred path for new connections, even given previously-unseen\ndestinations. We further demonstrate that it can be easily implemented in\nLinux. Finally, in a real-world experiment with either LTE or a wired link, we\nshow how 2SYN dynamically adapts to the quality of the connection and\noutperforms alternative approaches. Thus, 2SYN helps companies better manage\ntheir networks by leveraging their multihoming capabilities.", "AI": {"tldr": "2SYN\u662f\u4e00\u79cd\u62e5\u585e\u611f\u77e5\u7684\u591a\u5bbf\u4e3b\u8def\u7531\u7b97\u6cd5\uff0c\u80fd\u4e3a\u4efb\u610f\u76ee\u7684\u5730\u52a8\u6001\u9009\u62e9\u6700\u4f73\u8def\u5f84\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u591a\u5bbf\u4e3b\u8def\u7531\u5668\u91c7\u7528\u5bf9\u62e5\u585e\u4e0d\u654f\u611f\u7684\u673a\u5236\uff0c\u65e0\u6cd5\u907f\u514d\u62e5\u585e\u8def\u5f84\uff0c\u5bfc\u81f4\u6d41\u91cf\u53d1\u9001\u81f3\u4efb\u610f\u76ee\u7684\u5730\u65f6\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u672c\u6587\u63d0\u51fa2SYN\uff0c\u9996\u4e2a\u9002\u7528\u4e8e\u4efb\u610f\u76ee\u7684\u5730\u7684\u62e5\u585e\u611f\u77e5\u591a\u5bbf\u4e3b\u7b97\u6cd5\u3002\u5b83\u80fd\u4e3a\u65b0\u8fde\u63a5\u52a8\u6001\u9009\u62e9\u4f18\u9009\u8def\u5f84\uff0c\u5373\u4f7f\u662f\u9488\u5bf9\u4e4b\u524d\u672a\u89c1\u7684\u672a\u77e5\u76ee\u7684\u5730\u3002\u8be5\u7b97\u6cd5\u6613\u4e8e\u5728Linux\u4e2d\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u7684LTE\u548c\u6709\u7ebf\u94fe\u8def\u5b9e\u9a8c\uff0c\u8bc1\u660e2SYN\u80fd\u591f\u52a8\u6001\u9002\u5e94\u8fde\u63a5\u8d28\u91cf\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "2SYN\u901a\u8fc7\u6709\u6548\u5229\u7528\u591a\u5bbf\u4e3b\u80fd\u529b\uff0c\u5e2e\u52a9\u4f01\u4e1a\u66f4\u597d\u5730\u7ba1\u7406\u5176\u7f51\u7edc\u3002"}}
{"id": "2508.19268", "pdf": "https://arxiv.org/pdf/2508.19268", "abs": "https://arxiv.org/abs/2508.19268", "authors": ["Qing Wang", "Xue Han", "Jiahui Wang", "Lehao Xing", "Qian Hu", "Lianlian Zhang", "Chao Deng", "Junlan Feng"], "title": "MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite LLMs' excellent code creation capabilities, multilingual code\ngeneration remains extremely challenging. To address this, we intent to improve\nthe multi-programming-lingual (MultiPL) performance of the base LLMs while\nretaining the most popular ones using restricted computational resources. We\nconsider MultiPL to be a special case of multiple natural languages and propose\na MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called\nMultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize\nexpert selection at both the token and segment levels. The token-level MoE is a\nstandard upcycling MoE structure with a shared expert and a novel gate weight\nnormalization approach that aids in the final fusion with the segment-level\nMoE. The segment-level MoE incorporates two innovative designs to better\ncapture the syntactic structure and contextual patterns of programming\nlanguages: First, using a sliding window to partition the input token sequence\ninto multiple segments; Then, adopting an expert-choice routing strategy that\nallows experts to select the top-k segments. The results of the experiment\nproved the effectiveness of MultiPL-MoE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMultiPL-MoE\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408token\u7ea7\u548csegment\u7ea7\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u7ed3\u6784\uff0c\u6709\u6548\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u7f16\u7a0b\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u591a\u8bed\u79cd\u4ee3\u7801\u751f\u6210\u4ecd\u6781\u5177\u6311\u6218\u6027\u3002\u7814\u7a76\u65e8\u5728\u5229\u7528\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\uff0c\u5728\u4fdd\u7559\u73b0\u6709\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u63d0\u9ad8LLMs\u5728\u591a\u7f16\u7a0b\u8bed\u8a00\uff08MultiPL\uff09\u65b9\u9762\u7684\u6027\u80fd\u3002", "method": "\u5c06MultiPL\u89c6\u4e3a\u591a\u81ea\u7136\u8bed\u8a00\u7684\u7279\u4f8b\uff0c\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u7684LLM\u6269\u5c55\u2014\u2014MultiPL-MoE\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u4e24\u4e2a\u914d\u5bf9\u7684MoE\uff0c\u5206\u522b\u5728token\u7ea7\u548csegment\u7ea7\u4f18\u5316\u4e13\u5bb6\u9009\u62e9\u3002token\u7ea7MoE\u91c7\u7528\u6807\u51c6\u5347\u7ea7MoE\u7ed3\u6784\uff0c\u5305\u542b\u5171\u4eab\u4e13\u5bb6\u548c\u65b0\u9896\u7684\u95e8\u63a7\u6743\u91cd\u5f52\u4e00\u5316\u65b9\u6cd5\uff1bsegment\u7ea7MoE\u5219\u521b\u65b0\u6027\u5730\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u5bf9\u8f93\u5165\u5e8f\u5217\u8fdb\u884c\u5206\u6bb5\uff0c\u5e76\u91c7\u7528\u4e13\u5bb6\u9009\u62e9\u8def\u7531\u7b56\u7565\uff0c\u5141\u8bb8\u4e13\u5bb6\u9009\u62e9Top-k\u5206\u6bb5\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u7f16\u7a0b\u8bed\u8a00\u7684\u53e5\u6cd5\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86MultiPL-MoE\u7684\u6709\u6548\u6027\u3002", "conclusion": "MultiPL-MoE\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u63d0\u5347LLMs\u5728\u591a\u7f16\u7a0b\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2508.19254", "pdf": "https://arxiv.org/pdf/2508.19254", "abs": "https://arxiv.org/abs/2508.19254", "authors": ["Jookyung Song", "Mookyoung Kang", "Nojun Kwak"], "title": "Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": "6 pages, 4 figures, NeurIPS Creative AI Track 2025", "summary": "This paper presents a real-time generative drawing system that interprets and\nintegrates both formal intent - the structural, compositional, and stylistic\nattributes of a sketch - and contextual intent - the semantic and thematic\nmeaning inferred from its visual content - into a unified transformation\nprocess. Unlike conventional text-prompt-based generative systems, which\nprimarily capture high-level contextual descriptions, our approach\nsimultaneously analyzes ground-level intuitive geometric features such as line\ntrajectories, proportions, and spatial arrangement, and high-level semantic\ncues extracted via vision-language models. These dual intent signals are\njointly conditioned in a multi-stage generation pipeline that combines\ncontour-preserving structural control with style- and content-aware image\nsynthesis. Implemented with a touchscreen-based interface and distributed\ninference architecture, the system achieves low-latency, two-stage\ntransformation while supporting multi-user collaboration on shared canvases.\nThe resulting platform enables participants, regardless of artistic expertise,\nto engage in synchronous, co-authored visual creation, redefining human-AI\ninteraction as a process of co-creation and mutual enhancement.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u5b9e\u65f6\u751f\u6210\u5f0f\u7ed8\u56fe\u7cfb\u7edf\uff0c\u80fd\u6574\u5408\u8349\u56fe\u4e2d\u7684\u5f62\u5f0f\u610f\u56fe\u548c\u4e0a\u4e0b\u6587\u610f\u56fe\uff0c\u5b9e\u73b0\u4eba\u673a\u534f\u540c\u521b\u4f5c\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u63d0\u793a\u5f0f\u751f\u6210\u7cfb\u7edf\u4e3b\u8981\u6355\u83b7\u9ad8\u5c42\u4e0a\u4e0b\u6587\u63cf\u8ff0\uff0c\u672a\u80fd\u540c\u65f6\u5206\u6790\u8349\u56fe\u4e2d\u7684\u5e95\u5c42\u51e0\u4f55\u7279\u5f81\u548c\u9ad8\u5c42\u8bed\u4e49\u7ebf\u7d22\uff0c\u4ece\u800c\u65e0\u6cd5\u5168\u9762\u6355\u6349\u7528\u6237\u7684\u521b\u4f5c\u610f\u56fe\u3002", "method": "\u7cfb\u7edf\u540c\u65f6\u5206\u6790\u8349\u56fe\u7684\u5e95\u5c42\u51e0\u4f55\u7279\u5f81\uff08\u5982\u7ebf\u6761\u8f68\u8ff9\u3001\u6bd4\u4f8b\u3001\u7a7a\u95f4\u5e03\u5c40\uff09\u548c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u7684\u9ad8\u5c42\u8bed\u4e49\u7ebf\u7d22\u3002\u8fd9\u4e9b\u53cc\u91cd\u610f\u56fe\u4fe1\u53f7\u5728\u591a\u9636\u6bb5\u751f\u6210\u7ba1\u9053\u4e2d\u8054\u5408\u8c03\u6574\uff0c\u8be5\u7ba1\u9053\u7ed3\u5408\u4e86\u8f6e\u5ed3\u4fdd\u6301\u7684\u7ed3\u6784\u63a7\u5236\u548c\u98ce\u683c\u4e0e\u5185\u5bb9\u611f\u77e5\u7684\u56fe\u50cf\u5408\u6210\u3002\u7cfb\u7edf\u91c7\u7528\u89e6\u6478\u5c4f\u754c\u9762\u548c\u5206\u5e03\u5f0f\u63a8\u7406\u67b6\u6784\u3002", "result": "\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u7684\u4e24\u9636\u6bb5\u8f6c\u6362\uff0c\u5e76\u652f\u6301\u5728\u5171\u4eab\u753b\u5e03\u4e0a\u8fdb\u884c\u591a\u7528\u6237\u534f\u4f5c\u3002\u5b83\u4f7f\u6240\u6709\u53c2\u4e0e\u8005\uff0c\u65e0\u8bba\u827a\u672f\u4e13\u957f\u5982\u4f55\uff0c\u90fd\u80fd\u53c2\u4e0e\u540c\u6b65\u3001\u5171\u540c\u521b\u4f5c\u7684\u89c6\u89c9\u521b\u4f5c\u3002", "conclusion": "\u8be5\u5e73\u53f0\u5c06\u4eba\u673a\u4ea4\u4e92\u91cd\u65b0\u5b9a\u4e49\u4e3a\u534f\u540c\u521b\u4f5c\u548c\u76f8\u4e92\u589e\u5f3a\u7684\u8fc7\u7a0b\uff0c\u4f7f\u4e0d\u540c\u827a\u672f\u6c34\u5e73\u7684\u7528\u6237\u90fd\u80fd\u53c2\u4e0e\u5230\u521b\u9020\u6027\u7684\u89c6\u89c9\u5de5\u4f5c\u4e2d\u3002"}}
{"id": "2508.19316", "pdf": "https://arxiv.org/pdf/2508.19316", "abs": "https://arxiv.org/abs/2508.19316", "authors": ["Shreyans Jain", "Alexandra Yost", "Amirali Abdullah"], "title": "Sycophancy as compositions of Atomic Psychometric Traits", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.2.4"], "comment": "8 pages, 4 figures", "summary": "Sycophancy is a key behavioral risk in LLMs, yet is often treated as an\nisolated failure mode that occurs via a single causal mechanism. We instead\npropose modeling it as geometric and causal compositions of psychometric traits\nsuch as emotionality, openness, and agreeableness - similar to factor\ndecomposition in psychometrics. Using Contrastive Activation Addition (CAA), we\nmap activation directions to these factors and study how different combinations\nmay give rise to sycophancy (e.g., high extraversion combined with low\nconscientiousness). This perspective allows for interpretable and compositional\nvector-based interventions like addition, subtraction and projection; that may\nbe used to mitigate safety-critical behaviors in LLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u5949\u627f\u884c\u4e3a\u5efa\u6a21\u4e3a\u5fc3\u7406\u6d4b\u91cf\u7279\u8d28\uff08\u5982\u60c5\u7eea\u6027\u3001\u5f00\u653e\u6027\uff09\u7684\u51e0\u4f55\u548c\u56e0\u679c\u7ec4\u5408\uff0c\u5e76\u5229\u7528\u5411\u91cf\u65b9\u6cd5\u8fdb\u884c\u53ef\u89e3\u91ca\u7684\u5e72\u9884\u4ee5\u51cf\u8f7b\u5176\u98ce\u9669\u3002", "motivation": "LLMs\u4e2d\u7684\u5949\u627f\u884c\u4e3a\u662f\u4e00\u4e2a\u5173\u952e\u7684\u884c\u4e3a\u98ce\u9669\uff0c\u4f46\u901a\u5e38\u88ab\u89c6\u4e3a\u5355\u4e00\u56e0\u679c\u673a\u5236\u4e0b\u7684\u5b64\u7acb\u6545\u969c\uff0c\u7f3a\u4e4f\u5bf9\u5176\u590d\u6742\u6027\u7684\u6df1\u5165\u7406\u89e3\u548c\u6709\u6548\u7684\u7f13\u89e3\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u5c06\u5949\u627f\u884c\u4e3a\u89c6\u4e3a\u5fc3\u7406\u6d4b\u91cf\u7279\u8d28\uff08\u5982\u60c5\u7eea\u6027\u3001\u5f00\u653e\u6027\u3001\u968f\u548c\u6027\uff09\u7684\u51e0\u4f55\u548c\u56e0\u679c\u7ec4\u5408\u3002\u91c7\u7528\u5bf9\u6bd4\u6fc0\u6d3b\u52a0\u6cd5\uff08CAA\uff09\u6280\u672f\uff0c\u5c06\u6fc0\u6d3b\u65b9\u5411\u6620\u5c04\u5230\u8fd9\u4e9b\u5fc3\u7406\u7279\u8d28\u4e0a\uff0c\u5e76\u7814\u7a76\u4e0d\u540c\u7279\u8d28\u7ec4\u5408\u5982\u4f55\u5bfc\u81f4\u5949\u627f\u884c\u4e3a\u7684\u4ea7\u751f\u3002", "result": "\u8be5\u7814\u7a76\u65b9\u6cd5\u80fd\u591f\u4ece\u5fc3\u7406\u7279\u8d28\u7ec4\u5408\u5c42\u9762\u7406\u89e3\u5949\u627f\u884c\u4e3a\u7684\u4ea7\u751f\u673a\u5236\uff0c\u4f8b\u5982\u9ad8\u5916\u5411\u6027\u7ed3\u5408\u4f4e\u8d23\u4efb\u5fc3\u3002\u8fd9\u79cd\u89c6\u89d2\u4e3aLLMs\u4e2d\u5b89\u5168\u5173\u952e\u884c\u4e3a\u7684\u5e72\u9884\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u7ec4\u5408\u6027\u7684\u5411\u91cf\u64cd\u4f5c\u65b9\u6cd5\uff08\u5982\u52a0\u6cd5\u3001\u51cf\u6cd5\u3001\u6295\u5f71\uff09\u3002", "conclusion": "\u5c06\u5949\u627f\u884c\u4e3a\u5efa\u6a21\u4e3a\u5fc3\u7406\u7279\u8d28\u7684\u7ec4\u5408\uff0c\u4e3a\u7f13\u89e3LLMs\u4e2d\u7684\u5b89\u5168\u5173\u952e\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u3001\u53ef\u89e3\u91ca\u4e14\u57fa\u4e8e\u5411\u91cf\u7684\u5e72\u9884\u6846\u67b6\u3002"}}
{"id": "2508.19249", "pdf": "https://arxiv.org/pdf/2508.19249", "abs": "https://arxiv.org/abs/2508.19249", "authors": ["Jonas S\u00f8eborg Nielsen", "Marcus Galea Jacobsen", "Albert Brincker Olson", "Mads Peter S\u00f8rensen", "Allan Peter Engsig-Karup"], "title": "Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models", "categories": ["cs.LG", "math.DS", "stat.ME", "stat.ML", "37M99"], "comment": "For public PIR Julia package, see\n  https://github.com/MarcusGalea/PhysicsInformedRegression.jl", "summary": "We present a new efficient hybrid parameter estimation method based on the\nidea, that if nonlinear dynamic models are stated in terms of a system of\nequations that is linear in terms of the parameters, then regularized ordinary\nleast squares can be used to estimate these parameters from time series data.\nWe introduce the term \"Physics-Informed Regression\" (PIR) to describe the\nproposed data-driven hybrid technique as a way to bridge theory and data by use\nof ordinary least squares to efficiently perform parameter estimation of the\nmodel coefficients of different parameter-linear models; providing examples of\nmodels based on nonlinear ordinary equations (ODE) and partial differential\nequations (PDE). The focus is on parameter estimation on a selection of ODE and\nPDE models, each illustrating performance in different model characteristics.\nFor two relevant epidemic models of different complexity and number of\nparameters, PIR is tested and compared against the related technique,\nphysics-informed neural networks (PINN), both on synthetic data generated from\nknown target parameters and on real public Danish time series data collected\nduring the COVID-19 pandemic in Denmark. Both methods were able to estimate the\ntarget parameters, while PIR showed to perform noticeably better, especially on\na compartment model with higher complexity. Given the difference in\ncomputational speed, it is concluded that the PIR method is superior to PINN\nfor the models considered. It is also demonstrated how PIR can be applied to\nestimate the time-varying parameters of a compartment model that is fitted\nusing real Danish data from the COVID-19 pandemic obtained during a period from\n2020 to 2021. The study shows how data-driven and physics-informed techniques\nmay support reliable and fast -- possibly real-time -- parameter estimation in\nparameter-linear nonlinear dynamic models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u7269\u7406\u4fe1\u606f\u56de\u5f52\u201d\uff08PIR\uff09\u7684\u65b0\u578b\u9ad8\u6548\u6df7\u5408\u53c2\u6570\u4f30\u8ba1\u7b97\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u6b63\u5219\u5316\u666e\u901a\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff0c\u5bf9\u53c2\u6570\u7ebf\u6027\u7684\u975e\u7ebf\u6027\u52a8\u6001\u6a21\u578b\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1\uff0c\u5e76\u5728\u75ab\u60c5\u6a21\u578b\u4e0a\u4e0e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u7ed3\u679c\u663e\u793aPIR\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8ePINN\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u4f30\u8ba1\u975e\u7ebf\u6027\u52a8\u6001\u6a21\u578b\u7684\u53c2\u6570\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u5728\u53c2\u6570\u65b9\u9762\u5448\u7ebf\u6027\u7684\u6a21\u578b\uff0c\u4ee5\u5f25\u5408\u7406\u8bba\u4e0e\u6570\u636e\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u5f15\u5165\u4e86\u201c\u7269\u7406\u4fe1\u606f\u56de\u5f52\u201d\uff08PIR\uff09\u65b9\u6cd5\uff0c\u5176\u6838\u5fc3\u601d\u60f3\u662f\u5f53\u975e\u7ebf\u6027\u52a8\u6001\u6a21\u578b\u5728\u53c2\u6570\u65b9\u9762\u5448\u7ebf\u6027\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u6b63\u5219\u5316\u666e\u901a\u6700\u5c0f\u4e8c\u4e58\u6cd5\uff08OLS\uff09\u6765\u6709\u6548\u4f30\u8ba1\u8fd9\u4e9b\u53c2\u6570\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8eODE\u548cPDE\u6a21\u578b\uff0c\u5e76\u4ee5\u4e24\u79cd\u6d41\u884c\u75c5\u5b66\u6a21\u578b\u4e3a\u4f8b\uff0c\u4e0e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u6570\u636e\u5305\u62ec\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u7684\u4e39\u9ea6COVID-19\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff082020-2021\u5e74\uff09\u3002", "result": "PIR\u548cPINN\u5747\u80fd\u4f30\u8ba1\u76ee\u6807\u53c2\u6570\uff0c\u4f46PIR\u8868\u73b0\u51fa\u663e\u8457\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u66f4\u590d\u6742\u7684\u4ed3\u5ba4\u6a21\u578b\u4e0a\u3002\u8003\u8651\u5230\u8ba1\u7b97\u901f\u5ea6\u7684\u5dee\u5f02\uff0cPIR\u88ab\u8ba4\u4e3a\u5728\u6240\u8003\u8651\u7684\u6a21\u578b\u4e2d\u4f18\u4e8ePINN\u3002\u7814\u7a76\u8fd8\u5c55\u793a\u4e86PIR\u5982\u4f55\u5e94\u7528\u4e8e\u4f30\u8ba1\u4f7f\u7528\u771f\u5b9eCOVID-19\u6570\u636e\u62df\u5408\u7684\u4ed3\u5ba4\u6a21\u578b\u7684\u65f6\u53d8\u53c2\u6570\u3002", "conclusion": "PIR\u65b9\u6cd5\u5bf9\u4e8e\u53c2\u6570\u7ebf\u6027\u7684\u975e\u7ebf\u6027\u52a8\u6001\u6a21\u578b\u800c\u8a00\uff0c\u662f\u4e00\u79cd\u53ef\u9760\u3001\u5feb\u901f\uff08\u53ef\u80fd\u8fbe\u5230\u5b9e\u65f6\uff09\u4e14\u4f18\u4e8ePINN\u7684\u53c2\u6570\u4f30\u8ba1\u6280\u672f\u3002\u5b83\u80fd\u6709\u6548\u5730\u5c06\u6570\u636e\u9a71\u52a8\u548c\u7269\u7406\u4fe1\u606f\u6280\u672f\u7ed3\u5408\u8d77\u6765\uff0c\u652f\u6301\u9ad8\u6548\u7684\u53c2\u6570\u4f30\u8ba1\u3002"}}
{"id": "2508.20060", "pdf": "https://arxiv.org/pdf/2508.20060", "abs": "https://arxiv.org/abs/2508.20060", "authors": ["Daqian Ding", "Yibo Pi", "Cailian Chen"], "title": "A First Look at Inter-Cell Interference in the Wild", "categories": ["cs.NI"], "comment": null, "summary": "In cellular networks, inter-cell interference management has been studied for\ndecades, yet its real-world effectiveness remains under-explored. To bridge\nthis gap, we conduct a first measurement study of inter-cell interference for\noperational 4G/5G networks. Our findings reveal the prevalence of inter-cell\ninterference and a surprising absence of interference coordination among\noperational base stations. As a result, user equipments experience unnecessary\ninterference, which causes significant signal quality degradation, especially\nunder frequency-selective channel fading. We examine the inter-cell\ninterference issues from four major perspectives: network deployment, channel\nassignment, time-frequency resource allocation, and network configuration. In\nnone of these dimensions is inter-cell interference effectively managed.\nNotably, even when spectrum resources are underutilized and simple strategies\ncould effectively mitigate inter-cell interference, base stations consistently\nprioritize using the same set of time-frequency resources, causing interference\nacross cells. Our measurements reveal substantial opportunities for improving\nsignal quality by inter-cell interference management.", "AI": {"tldr": "\u5bf9\u5b9e\u96454G/5G\u7f51\u7edc\u4e2d\u7684\u5c0f\u533a\u95f4\u5e72\u6270\u8fdb\u884c\u4e86\u9996\u6b21\u6d4b\u91cf\u7814\u7a76\uff0c\u53d1\u73b0\u5e72\u6270\u666e\u904d\u5b58\u5728\u4e14\u7f3a\u4e4f\u6709\u6548\u7ba1\u7406\uff0c\u5bfc\u81f4\u4fe1\u53f7\u8d28\u91cf\u663e\u8457\u4e0b\u964d\uff0c\u5b58\u5728\u5de8\u5927\u4f18\u5316\u6f5c\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5c0f\u533a\u95f4\u5e72\u6270\u7ba1\u7406\u5df2\u7814\u7a76\u591a\u5e74\uff0c\u4f46\u5176\u5728\u5b9e\u9645\u7f51\u7edc\u4e2d\u7684\u6548\u679c\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u8fdb\u884c\u5b9e\u5730\u6d4b\u91cf\u4ee5\u5f25\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5bf9\u5b9e\u9645\u8fd0\u884c\u76844G/5G\u7f51\u7edc\u8fdb\u884c\u4e86\u5c0f\u533a\u95f4\u5e72\u6270\u7684\u9996\u6b21\u6d4b\u91cf\u7814\u7a76\uff0c\u5e76\u4ece\u7f51\u7edc\u90e8\u7f72\u3001\u4fe1\u9053\u5206\u914d\u3001\u65f6\u9891\u8d44\u6e90\u5206\u914d\u548c\u7f51\u7edc\u914d\u7f6e\u56db\u4e2a\u4e3b\u8981\u65b9\u9762\u5ba1\u89c6\u4e86\u5c0f\u533a\u95f4\u5e72\u6270\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5c0f\u533a\u95f4\u5e72\u6270\u666e\u904d\u5b58\u5728\uff0c\u4e14\u8fd0\u8425\u57fa\u7ad9\u4e4b\u95f4\u60ca\u4eba\u5730\u7f3a\u4e4f\u5e72\u6270\u534f\u8c03\u3002\u7528\u6237\u8bbe\u5907\u56e0\u6b64\u627f\u53d7\u4e0d\u5fc5\u8981\u7684\u5e72\u6270\uff0c\u5bfc\u81f4\u4fe1\u53f7\u8d28\u91cf\u663e\u8457\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5728\u9891\u7387\u9009\u62e9\u6027\u4fe1\u9053\u8870\u843d\u4e0b\u3002\u5728\u6240\u6709\u68c0\u67e5\u7684\u7ef4\u5ea6\u4e0a\uff0c\u5c0f\u533a\u95f4\u5e72\u6270\u90fd\u672a\u80fd\u5f97\u5230\u6709\u6548\u7ba1\u7406\u3002\u5373\u4f7f\u5728\u9891\u8c31\u8d44\u6e90\u672a\u5145\u5206\u5229\u7528\u4e14\u7b80\u5355\u7b56\u7565\u5373\u53ef\u51cf\u8f7b\u5e72\u6270\u7684\u60c5\u51b5\u4e0b\uff0c\u57fa\u7ad9\u4ecd\u503e\u5411\u4e8e\u4f7f\u7528\u76f8\u540c\u7684\u65f6\u9891\u8d44\u6e90\uff0c\u4ece\u800c\u5728\u5c0f\u533a\u95f4\u9020\u6210\u5e72\u6270\u3002", "conclusion": "\u6d4b\u91cf\u7ed3\u679c\u63ed\u793a\u4e86\u901a\u8fc7\u6709\u6548\u7684\u5c0f\u533a\u95f4\u5e72\u6270\u7ba1\u7406\u6765\u663e\u8457\u6539\u5584\u4fe1\u53f7\u8d28\u91cf\u7684\u5de8\u5927\u673a\u4f1a\u3002"}}
{"id": "2508.19270", "pdf": "https://arxiv.org/pdf/2508.19270", "abs": "https://arxiv.org/abs/2508.19270", "authors": ["Nguyen Huu Nhat Minh", "Tran Nguyen Anh", "Truong Dinh Dung", "Vo Van Nam", "Le Pham Tuyen"], "title": "Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Cross-lingual phoneme recognition has emerged as a significant challenge for\naccurate automatic speech recognition (ASR) when mixing Vietnamese and English\npronunciations. Unlike many languages, Vietnamese relies on tonal variations to\ndistinguish word meanings, whereas English features stress patterns and\nnon-standard pronunciations that hinder phoneme alignment between the two\nlanguages. To address this challenge, we propose a novel bilingual speech\nrecognition approach with two primary contributions: (1) constructing a\nrepresentative bilingual phoneme set that bridges the differences between\nVietnamese and English phonetic systems; (2) designing an end-to-end system\nthat leverages the PhoWhisper pre-trained encoder for deep high-level\nrepresentations to improve phoneme recognition. Our extensive experiments\ndemonstrate that the proposed approach not only improves recognition accuracy\nin bilingual speech recognition for Vietnamese but also provides a robust\nframework for addressing the complexities of tonal and stress-based phoneme\nrecognition", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u53cc\u8bed\u97f3\u7d20\u96c6\u548cPhoWhisper\u7f16\u7801\u5668\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u4ee5\u89e3\u51b3\u8d8a\u5357\u8bed\u548c\u82f1\u8bed\u6df7\u5408\u53d1\u97f3\u7684\u8de8\u8bed\u8a00\u97f3\u7d20\u8bc6\u522b\u6311\u6218\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u5728\u8d8a\u5357\u8bed\u548c\u82f1\u8bed\u6df7\u5408\u53d1\u97f3\u573a\u666f\u4e2d\uff0c\u51c6\u786e\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u9762\u4e34\u8de8\u8bed\u8a00\u97f3\u7d20\u8bc6\u522b\u7684\u91cd\u5927\u6311\u6218\u3002\u8d8a\u5357\u8bed\u4f9d\u8d56\u58f0\u8c03\u533a\u5206\u8bcd\u4e49\uff0c\u800c\u82f1\u8bed\u7684\u91cd\u97f3\u6a21\u5f0f\u548c\u975e\u6807\u51c6\u53d1\u97f3\u7279\u6027\uff0c\u963b\u788d\u4e86\u4e24\u79cd\u8bed\u8a00\u95f4\u7684\u97f3\u7d20\u5bf9\u9f50\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u9879\u4e3b\u8981\u8d21\u732e\uff1a1) \u6784\u5efa\u4e00\u4e2a\u80fd\u5f25\u5408\u8d8a\u5357\u8bed\u548c\u82f1\u8bed\u8bed\u97f3\u7cfb\u7edf\u5dee\u5f02\u7684\u53cc\u8bed\u97f3\u7d20\u96c6\uff1b2) \u8bbe\u8ba1\u4e00\u4e2a\u5229\u7528PhoWhisper\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u83b7\u53d6\u6df1\u5ea6\u9ad8\u5c42\u8868\u793a\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\uff0c\u4ee5\u589e\u5f3a\u97f3\u7d20\u8bc6\u522b\u80fd\u529b\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8d8a\u5357\u8bed\u53cc\u8bed\u8bed\u97f3\u8bc6\u522b\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u53cc\u8bed\u8bed\u97f3\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u4e3a\u89e3\u51b3\u57fa\u4e8e\u58f0\u8c03\u548c\u91cd\u97f3\u7684\u590d\u6742\u97f3\u7d20\u8bc6\u522b\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u6846\u67b6\u3002"}}
{"id": "2508.19257", "pdf": "https://arxiv.org/pdf/2508.19257", "abs": "https://arxiv.org/abs/2508.19257", "authors": ["Chenghao Liu", "Jiachen Zhang", "Chengxuan Li", "Zhimu Zhou", "Shixin Wu", "Songfang Huang", "Huiling Duan"], "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Manuscript submitted to AAAI 2026, currently under review", "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.", "AI": {"tldr": "VLA\u6a21\u578b\u5ffd\u89c6\u65f6\u5e8f\u4fe1\u606f\uff0c\u6613\u53d7\u566a\u58f0\u5f71\u54cd\u3002\u672c\u6587\u63d0\u51fa\u8bad\u7ec3\u65e0\u5173\u7684Temporal Token Fusion (TTF) \u65b9\u6cd5\uff0c\u901a\u8fc7\u667a\u80fd\u878d\u5408\u5386\u53f2\u548c\u5f53\u524d\u89c6\u89c9\u8868\u793a\uff0c\u63d0\u5347VLA\u63a8\u7406\u8d28\u91cf\u3002TTF\u7ed3\u5408\u50cf\u7d20\u5dee\u5f02\u5206\u6790\u548c\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u9009\u62e9\u6027\u65f6\u95f4\u4ee4\u724c\u878d\u5408\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u5e76\u63ed\u793a\u4e86\u9009\u62e9\u6027Query\u77e9\u9635\u91cd\u7528\u53ef\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u5904\u7406\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u65f6\uff0c\u72ec\u7acb\u5904\u7406\u6bcf\u4e00\u65f6\u95f4\u6b65\u7684\u89c6\u89c9\u8f93\u5165\uff0c\u5ffd\u7565\u4e86\u5b9d\u8d35\u7684\u65f6\u5e8f\u4fe1\u606f\u548c\u8fde\u7eed\u5e27\u4e4b\u95f4\u7684\u5185\u5728\u4e00\u81f4\u6027\u3002\u8fd9\u79cd\u9010\u5e27\u5904\u7406\u65b9\u5f0f\u4f7f\u6a21\u578b\u6613\u53d7\u89c6\u89c9\u566a\u58f0\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u65e0\u5173\u7684Temporal Token Fusion (TTF) \u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u667a\u80fd\u878d\u5408\u5386\u53f2\u548c\u5f53\u524d\u7684\u89c6\u89c9\u8868\u793a\u6765\u589e\u5f3aVLA\u63a8\u7406\u8d28\u91cf\u3002TTF\u91c7\u7528\u53cc\u7ef4\u5ea6\u68c0\u6d4b\u673a\u5236\uff0c\u7ed3\u5408\u9ad8\u6548\u7684\u7070\u5ea6\u50cf\u7d20\u5dee\u5f02\u5206\u6790\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u8bed\u4e49\u76f8\u5173\u6027\u8bc4\u4f30\uff0c\u901a\u8fc7\u786c\u878d\u5408\u7b56\u7565\u548c\u5173\u952e\u5e27\u951a\u5b9a\u5b9e\u73b0\u9009\u62e9\u6027\u65f6\u95f4\u4ee4\u724c\u878d\u5408\uff0c\u4ee5\u9632\u6b62\u9519\u8bef\u7d2f\u79ef\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u5728LIBERO\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e864.0\u4e2a\u767e\u5206\u70b9\uff0872.4% vs 68.4%\u57fa\u7ebf\uff09\uff1b\u5728SimplerEnv\u4e0a\u8fdb\u884c\u4e86\u8de8\u73af\u5883\u9a8c\u8bc1\uff0c\u76f8\u5bf9\u6539\u8fdb\u4e864.8%\uff1b\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e0a\u76f8\u5bf9\u6539\u8fdb\u4e868.7%\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\uff0c\u9002\u7528\u4e8eOpenVLA\u548cVLA-Cache\u67b6\u6784\u3002", "conclusion": "TTF\u901a\u8fc7\u667a\u80fd\u878d\u5408\u65f6\u5e8f\u89c6\u89c9\u4fe1\u606f\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002\u7814\u7a76\u8fd8\u63ed\u793a\uff0c\u5728\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u9009\u62e9\u6027\u5730\u91cd\u7528Query\u77e9\u9635\u4e0d\u4ec5\u4e0d\u4f1a\u635f\u5bb3\u6027\u80fd\uff0c\u53cd\u800c\u80fd\u589e\u5f3a\u6027\u80fd\uff0c\u8fd9\u4e3a\u901a\u8fc7\u76f4\u63a5KQV\u77e9\u9635\u91cd\u7528\u6765\u5b9e\u73b0\u8ba1\u7b97\u52a0\u901f\u5e76\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.19383", "pdf": "https://arxiv.org/pdf/2508.19383", "abs": "https://arxiv.org/abs/2508.19383", "authors": ["Daoyuan Jin", "Nick Gunner", "Niko Carvajal Janke", "Shivranjani Baruah", "Kaitlin M. Gold", "Yu Jiang"], "title": "Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Modern plant science increasingly relies on large, heterogeneous datasets,\nbut challenges in experimental design, data preprocessing, and reproducibility\nhinder research throughput. Here we introduce Aleks, an AI-powered multi-agent\nsystem that integrates domain knowledge, data analysis, and machine learning\nwithin a structured framework to autonomously conduct data-driven scientific\ndiscovery. Once provided with a research question and dataset, Aleks\niteratively formulated problems, explored alternative modeling strategies, and\nrefined solutions across multiple cycles without human intervention. In a case\nstudy on grapevine red blotch disease, Aleks progressively identified\nbiologically meaningful features and converged on interpretable models with\nrobust performance. Ablation studies underscored the importance of domain\nknowledge and memory for coherent outcomes. This exploratory work highlights\nthe promise of agentic AI as an autonomous collaborator for accelerating\nscientific discovery in plant sciences.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecdAleks\uff0c\u4e00\u4e2aAI\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u4e3b\u8fed\u4ee3\u89e3\u51b3\u95ee\u9898\u3001\u5efa\u6a21\u548c\u4f18\u5316\uff0c\u52a0\u901f\u690d\u7269\u79d1\u5b66\u4e2d\u7684\u6570\u636e\u9a71\u52a8\u79d1\u5b66\u53d1\u73b0\u3002", "motivation": "\u73b0\u4ee3\u690d\u7269\u79d1\u5b66\u4e2d\u5927\u578b\u5f02\u6784\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u8bbe\u8ba1\u3001\u6570\u636e\u9884\u5904\u7406\u548c\u53ef\u91cd\u590d\u6027\u6311\u6218\u963b\u788d\u4e86\u7814\u7a76\u6548\u7387\u3002", "method": "\u4ecb\u7ecdAleks\uff0c\u4e00\u4e2aAI\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u3001\u6570\u636e\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\uff0c\u4ee5\u81ea\u4e3b\u8fed\u4ee3\u65b9\u5f0f\u89e3\u51b3\u7814\u7a76\u95ee\u9898\uff0c\u63a2\u7d22\u5efa\u6a21\u5e76\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002", "result": "Aleks\u6210\u529f\u8bc6\u522b\u8461\u8404\u7ea2\u6591\u75c5\u6848\u4f8b\u4e2d\u5177\u751f\u7269\u5b66\u610f\u4e49\u7684\u7279\u5f81\u5e76\u83b7\u5f97\u7a33\u5065\u6a21\u578b\uff1b\u6d88\u878d\u7814\u7a76\u8bc1\u660e\u9886\u57df\u77e5\u8bc6\u548c\u8bb0\u5fc6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u667a\u80fd\u4f53AI\u5728\u52a0\u901f\u690d\u7269\u79d1\u5b66\u7814\u7a76\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u4f5c\u4e3a\u81ea\u4e3b\u5408\u4f5c\u8005\u3002"}}
{"id": "2508.19263", "pdf": "https://arxiv.org/pdf/2508.19263", "abs": "https://arxiv.org/abs/2508.19263", "authors": ["Anat Heilper", "Doron Singer"], "title": "Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "16 pages 9 images", "summary": "As deep learning models grow and deployment becomes more widespread, reducing\nthe storage and transmission costs of neural network weights has become\nincreasingly important. While prior work such as ZipNN has shown that lossless\ncompression methods - particularly those based on Huffman encoding\nfloating-point exponents can significantly reduce model sizes, these techniques\nhave primarily been applied to higher-precision formats such as FP32 and BF16.\nIn this work, we extend the ZipNN approach to lower-precision floating-point\nformats, specifically FP8 and FP4, which are gaining popularity for efficient\ninference. We design a compression method that separates and compresses the\nexponent and mantissa components independently using entropy coding. Our\nevaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also\ninvestigate the compressibility of key-value (K/V) cache tensors used in large\nlanguage models (LLMs), finding that they, too, exhibit compressible patterns,\nenabling memory savings during deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06ZipNN\u65b9\u6cd5\u6269\u5c55\u5230\u4f4e\u7cbe\u5ea6\u6d6e\u70b9\u683c\u5f0f\uff08\u5982FP8\u548cFP4\uff09\u7684\u65e0\u635f\u538b\u7f29\u65b9\u6848\uff0c\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u548cLLM\u7684K/V\u7f13\u5b58\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u538b\u7f29\u7387\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u89c4\u6a21\u7684\u589e\u957f\u548c\u5e7f\u6cdb\u90e8\u7f72\uff0c\u964d\u4f4e\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u7684\u5b58\u50a8\u548c\u4f20\u8f93\u6210\u672c\u65e5\u76ca\u91cd\u8981\u3002\u867d\u7136\u73b0\u6709\u65b9\u6cd5\u5bf9\u9ad8\u7cbe\u5ea6\u683c\u5f0f\u6709\u6548\uff0c\u4f46FP8\u548cFP4\u7b49\u4f4e\u7cbe\u5ea6\u683c\u5f0f\u6b63\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\uff0c\u9700\u8981\u9488\u5bf9\u6027\u5730\u7814\u7a76\u5176\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u5c06ZipNN\u65b9\u6cd5\u6269\u5c55\u5230FP8\u548cFP4\u7b49\u4f4e\u7cbe\u5ea6\u6d6e\u70b9\u683c\u5f0f\u3002\u8bbe\u8ba1\u4e86\u4e00\u79cd\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u71b5\u7f16\u7801\u72ec\u7acb\u5206\u79bb\u5e76\u538b\u7f29\u6d6e\u70b9\u6570\u7684\u6307\u6570\u548c\u5c3e\u6570\u5206\u91cf\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cBF16\u683c\u5f0f\u7684\u538b\u7f29\u7387\u9ad8\u8fbe62%\uff0cFP8\u683c\u5f0f\u7684\u538b\u7f29\u7387\u9ad8\u8fbe83%\u3002\u6b64\u5916\uff0c\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u4f7f\u7528\u7684\u952e\u503c\uff08K/V\uff09\u7f13\u5b58\u5f20\u91cf\u4e5f\u5177\u6709\u53ef\u538b\u7f29\u6027\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u4f4e\u7cbe\u5ea6\u6d6e\u70b9\u683c\u5f0f\u548cLLM K/V\u7f13\u5b58\u91c7\u7528\u72ec\u7acb\u7684\u6307\u6570\u548c\u5c3e\u6570\u71b5\u7f16\u7801\u538b\u7f29\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u6a21\u578b\u6743\u91cd\u548cK/V\u7f13\u5b58\u7684\u5b58\u50a8\u9700\u6c42\uff0c\u4ece\u800c\u5728\u90e8\u7f72\u671f\u95f4\u8282\u7701\u5185\u5b58\u3002"}}
{"id": "2508.20077", "pdf": "https://arxiv.org/pdf/2508.20077", "abs": "https://arxiv.org/abs/2508.20077", "authors": ["Tao Xiuyuan", "Milena Radenkovic"], "title": "ML-MaxProp: Bridging Machine Learning and Delay-Tolerant Routing for Resilient Post-Disaster Communication", "categories": ["cs.NI"], "comment": null, "summary": "In disaster-stricken and large-scale urban emergency scenarios, ensuring\nreliable communication remains a formidable challenge, as collapsed\ninfrastructure, unpredictable mobility, and severely constrained resources\ndisrupt conventional networks. Delay-Tolerant Networks (DTNs), though resilient\nthrough their store-carry-forward paradigm, reveal the fundamental weaknesses\nof classical protocols - Epidemic, Spray-and-Wait, and MaxProp - when\nconfronted with sparse encounters, buffer shortages, and volatile connectivity.\nTo address these obstacles, this study proposes ML-MaxProp, a hybrid routing\nprotocol that strengthens MaxProp with supervised machine learning. By\nleveraging contextual features such as encounter frequency, hop count, buffer\noccupancy, message age, and time-to-live (TTL), ML-MaxProp predicts relay\nsuitability in real time, transforming rigid heuristics into adaptive\nintelligence. Extensive simulations in the ONE environment using the Helsinki\nSPMBM mobility model show that ML-MaxProp consistently surpasses baseline\nprotocols, achieving higher delivery probability, lower latency, and reduced\noverhead. Statistical validation further shows that these improvements are both\nsignificant and robust, even under highly resource-constrained and unstable\nconditions. Overall, this work shows that ML-MaxProp is not just an incremental\nrefinement but a lightweight, adaptive, and practical solution to one of the\nhardest challenges in DTNs: sustaining mission-critical communication when\ninfrastructure collapses and every forwarding decision becomes critical.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faML-MaxProp\uff0c\u4e00\u79cd\u7ed3\u5408\u76d1\u7763\u673a\u5668\u5b66\u4e60\u7684\u6df7\u5408\u8def\u7531\u534f\u8bae\uff0c\u65e8\u5728\u89e3\u51b3\u707e\u533a\u548c\u57ce\u5e02\u7d27\u6025\u60c5\u51b5\u4e0b\u7684\u5ef6\u8fdf\u5bb9\u5fcd\u7f51\u7edc\uff08DTN\uff09\u901a\u4fe1\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d88\u606f\u6295\u9012\u7387\u3001\u964d\u4f4e\u4e86\u5ef6\u8fdf\u548c\u5f00\u9500\u3002", "motivation": "\u5728\u53d7\u707e\u548c\u5927\u89c4\u6a21\u57ce\u5e02\u7d27\u6025\u573a\u666f\u4e2d\uff0c\u7531\u4e8e\u57fa\u7840\u8bbe\u65bd\u762b\u75ea\u3001\u79fb\u52a8\u6027\u4e0d\u53ef\u9884\u6d4b\u548c\u8d44\u6e90\u4e25\u91cd\u53d7\u9650\uff0c\u4f20\u7edf\u7f51\u7edc\u4e2d\u65ad\uff0c\u786e\u4fdd\u53ef\u9760\u901a\u4fe1\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u73b0\u6709\u5ef6\u8fdf\u5bb9\u5fcd\u7f51\u7edc\uff08DTN\uff09\u534f\u8bae\uff08\u5982Epidemic\u3001Spray-and-Wait\u548cMaxProp\uff09\u5728\u906d\u9047\u7a00\u758f\u3001\u7f13\u51b2\u5668\u77ed\u7f3a\u548c\u8fde\u63a5\u4e0d\u7a33\u5b9a\u7684\u60c5\u51b5\u4e0b\uff0c\u8868\u73b0\u51fa\u57fa\u672c\u5f31\u70b9\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51faML-MaxProp\uff0c\u4e00\u79cd\u901a\u8fc7\u76d1\u7763\u673a\u5668\u5b66\u4e60\u589e\u5f3aMaxProp\u7684\u6df7\u5408\u8def\u7531\u534f\u8bae\u3002\u8be5\u534f\u8bae\u5229\u7528\u63a5\u89e6\u9891\u7387\u3001\u8df3\u6570\u3001\u7f13\u51b2\u533a\u5360\u7528\u3001\u6d88\u606f\u5bff\u547d\u548c\u5b58\u6d3b\u65f6\u95f4\uff08TTL\uff09\u7b49\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u5b9e\u65f6\u9884\u6d4b\u4e2d\u7ee7\u8282\u70b9\u7684\u9002\u7528\u6027\u3002\u901a\u8fc7\u5728ONE\u4eff\u771f\u73af\u5883\u4e2d\u4f7f\u7528Helsinki SPMBM\u79fb\u52a8\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u4eff\u771f\uff0c\u5e76\u8fdb\u884c\u7edf\u8ba1\u9a8c\u8bc1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cML-MaxProp\u59cb\u7ec8\u4f18\u4e8e\u57fa\u7ebf\u534f\u8bae\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6295\u9012\u6982\u7387\u3001\u66f4\u4f4e\u7684\u5ef6\u8fdf\u548c\u66f4\u5c11\u7684\u5f00\u9500\u3002\u7edf\u8ba1\u9a8c\u8bc1\u8fdb\u4e00\u6b65\u8bc1\u660e\uff0c\u5373\u4f7f\u5728\u9ad8\u5ea6\u8d44\u6e90\u53d7\u9650\u548c\u4e0d\u7a33\u5b9a\u7684\u6761\u4ef6\u4e0b\uff0c\u8fd9\u4e9b\u6539\u8fdb\u4e5f\u662f\u663e\u8457\u4e14\u7a33\u5065\u7684\u3002", "conclusion": "ML-MaxProp\u4e0d\u4ec5\u662f\u6e10\u8fdb\u5f0f\u6539\u8fdb\uff0c\u66f4\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u81ea\u9002\u5e94\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86DTN\u4e2d\u6700\u56f0\u96be\u7684\u6311\u6218\u4e4b\u4e00\uff1a\u5728\u57fa\u7840\u8bbe\u65bd\u5d29\u6e83\u3001\u6bcf\u4e2a\u8f6c\u53d1\u51b3\u7b56\u90fd\u81f3\u5173\u91cd\u8981\u65f6\uff0c\u7ef4\u6301\u4efb\u52a1\u5173\u952e\u578b\u901a\u4fe1\u3002"}}
{"id": "2508.19271", "pdf": "https://arxiv.org/pdf/2508.19271", "abs": "https://arxiv.org/abs/2508.19271", "authors": ["Rushitha Santhoshi Mamidala", "Anshuman Chhabra", "Ankur Mali"], "title": "Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and\nIn-Context Learning (ICL) have become widely used for eliciting reasoning\ncapabilities in large language models (LLMs). However, these methods rely on\nfragile, implicit mechanisms often yielding inconsistent outputs across seeds,\nformats, or minor prompt variations making them fundamentally unreliable for\ntasks requiring stable, interpretable reasoning. In contrast, automata-based\nneuro-symbolic frameworks like RetoMaton offer a more structured and\ntrustworthy alternative by grounding retrieval in symbolic memory with\ndeterministic transitions. In this work, we extend RetoMaton by replacing its\nglobal datastore with a local, task-adaptive Weighted Finite Automaton (WFA),\nconstructed directly from external domain corpora. This local automaton\nstructure promotes robust, context-aware retrieval while preserving symbolic\ntraceability and low inference overhead. Unlike prompting, which entangles\ncontext and memory in opaque ways, our approach leverages the explicit\nstructure of WFAs to provide verifiable and modular retrieval behavior, making\nit better suited for domain transfer and interoperability. We evaluate this\nlocal RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT\nacross three reasoning tasks: TriviaQA (reading comprehension), GSM8K\n(multi-step math), and MMLU (domain knowledge). Compared to the base model and\nprompting-based methods, augmenting these setups with local RetoMaton\nconsistently improves performance while enabling transparent and reproducible\nretrieval dynamics. Our results highlight a promising shift toward trustworthy,\nsymbolic reasoning in modern LLMs via lightweight, automaton-guided memory.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06RetoMaton\u6846\u67b6\u7684\u5168\u5c40\u6570\u636e\u5b58\u50a8\u66ff\u6362\u4e3a\u5c40\u90e8\u3001\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u52a0\u6743\u6709\u9650\u81ea\u52a8\u673a\uff08WFA\uff09\uff0c\u76f4\u63a5\u4ece\u5916\u90e8\u8bed\u6599\u5e93\u6784\u5efa\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728LLaMA\u548cGemma\u6a21\u578b\u4e0a\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u91cd\u73b0\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u8bcd\u7684LLM\u63a8\u7406\u7b56\u7565\uff08\u5982CoT\u3001ICL\uff09\u673a\u5236\u8106\u5f31\u3001\u8f93\u51fa\u4e0d\u7a33\u5b9a\u3001\u4e0d\u53ef\u9760\uff0c\u96be\u4ee5\u6ee1\u8db3\u9700\u8981\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u63a8\u7406\u7684\u4efb\u52a1\u9700\u6c42\u3002\u56e0\u6b64\uff0c\u9700\u8981\u66f4\u7ed3\u6784\u5316\u3001\u53ef\u4fe1\u8d56\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6269\u5c55\u4e86\u57fa\u4e8e\u81ea\u52a8\u673a\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6RetoMaton\uff0c\u5c06\u5176\u5168\u5c40\u6570\u636e\u5b58\u50a8\u66ff\u6362\u4e3a\u5c40\u90e8\u3001\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u52a0\u6743\u6709\u9650\u81ea\u52a8\u673a\uff08WFA\uff09\u3002\u8be5WFA\u76f4\u63a5\u4ece\u5916\u90e8\u9886\u57df\u8bed\u6599\u5e93\u6784\u5efa\uff0c\u4ee5\u7b26\u53f7\u8bb0\u5fc6\u548c\u786e\u5b9a\u6027\u8f6c\u6362\u6765\u951a\u5b9a\u68c0\u7d22\uff0c\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u548c\u6a21\u5757\u5316\u7684\u68c0\u7d22\u884c\u4e3a\u3002\u65b9\u6cd5\u5728LLaMA-3.2-1B\u548cGemma-3-1B-PT\u4e24\u4e2a\u9884\u8bad\u7ec3LLM\u4e0a\uff0c\u901a\u8fc7TriviaQA\u3001GSM8K\u548cMMLU\u4e09\u4e2a\u63a8\u7406\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u4e0e\u57fa\u7840\u6a21\u578b\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u4e0e\u57fa\u7840\u6a21\u578b\u548c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4f7f\u7528\u5c40\u90e8RetoMaton\u589e\u5f3aLLM\u7684\u65b9\u6cd5\u5728\u6240\u6709\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u6301\u7eed\u63d0\u5347\u4e86\u6027\u80fd\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u900f\u660e\u4e14\u53ef\u590d\u73b0\u7684\u68c0\u7d22\u52a8\u6001\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u3001\u81ea\u52a8\u673a\u5f15\u5bfc\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u7ed3\u5408\u5c40\u90e8RetoMaton\uff0c\u6709\u671b\u5b9e\u73b0\u73b0\u4ee3LLM\u4e2d\u53ef\u4fe1\u8d56\u7684\u7b26\u53f7\u63a8\u7406\u3002\u8fd9\u79cd\u65b9\u6cd5\u63d0\u4f9b\u660e\u786e\u7684WFA\u7ed3\u6784\uff0c\u652f\u6301\u53ef\u9a8c\u8bc1\u3001\u6a21\u5757\u5316\u7684\u68c0\u7d22\u884c\u4e3a\uff0c\u66f4\u9002\u7528\u4e8e\u9886\u57df\u8fc1\u79fb\u548c\u4e92\u64cd\u4f5c\u6027\u3002"}}
{"id": "2508.19289", "pdf": "https://arxiv.org/pdf/2508.19289", "abs": "https://arxiv.org/abs/2508.19289", "authors": ["Tai Inui", "Steven Oh", "Magdeline Kuan"], "title": "Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages", "summary": "We present an unsupervised slide-quality assessment pipeline that combines\nseven expert-inspired visual-design metrics (whitespace, colorfulness, edge\ndensity, brightness contrast, text density, color harmony, layout balance) with\nCLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate\npresentation slides. Trained on 12k professional lecture slides and evaluated\non six academic talks (115 slides), our method achieved Pearson correlations up\nto 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores\nfrom leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude\nSonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual\nratings, discriminant validity against speaker-delivery scores, and exploratory\nalignment with overall impressions. Our results show that augmenting low-level\ndesign cues with multimodal embeddings closely approximates audience\nperceptions of slide quality, enabling scalable, objective feedback in real\ntime.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u5e7b\u706f\u7247\u8d28\u91cf\u8bc4\u4f30\u6d41\u7a0b\uff0c\u7ed3\u5408\u4e86\u4e03\u79cd\u89c6\u89c9\u8bbe\u8ba1\u6307\u6807\u548cCLIP-ViT\u5d4c\u5165\uff0c\u901a\u8fc7\u9694\u79bb\u68ee\u6797\u6a21\u578b\uff0c\u5728\u8bc4\u4f30\u5e7b\u706f\u7247\u8d28\u91cf\u65b9\u9762\u4e0e\u4eba\u7c7b\u8bc4\u5206\u9ad8\u5ea6\u76f8\u5173\uff08Pearson r=0.83\uff09\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u6269\u5c55\u3001\u5ba2\u89c2\u4e14\u80fd\u4e0e\u89c2\u4f17\u611f\u77e5\u5bf9\u9f50\u7684\u5e7b\u706f\u7247\u8d28\u91cf\u53cd\u9988\u3002\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u5b9e\u65f6\u3001\u5ba2\u89c2\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u8fd1\u4f3c\u4eba\u7c7b\u5bf9\u5e7b\u706f\u7247\u89c6\u89c9\u8d28\u91cf\u7684\u611f\u77e5\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u65e0\u76d1\u7763\u7684\u5e7b\u706f\u7247\u8d28\u91cf\u8bc4\u4f30\u6d41\u7a0b\uff0c\u7ed3\u5408\u4e86\u4e03\u79cd\u4e13\u5bb6\u542f\u53d1\u7684\u89c6\u89c9\u8bbe\u8ba1\u6307\u6807\uff08\u5982\u7a7a\u767d\u3001\u8272\u5f69\u548c\u8c10\u3001\u5e03\u5c40\u5e73\u8861\u7b49\uff09\u548cCLIP-ViT\u5d4c\u5165\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u9694\u79bb\u68ee\u6797\u7684\u5f02\u5e38\u8bc4\u5206\u8fdb\u884c\u8bc4\u4f30\u3002\u8be5\u65b9\u6cd5\u57281.2\u4e07\u5f20\u4e13\u4e1a\u8bb2\u5ea7\u5e7b\u706f\u7247\u4e0a\u8bad\u7ec3\uff0c\u5e76\u5728115\u5f20\u5e7b\u706f\u7247\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0e\u4eba\u7c7b\u89c6\u89c9\u8d28\u91cf\u8bc4\u5206\u7684\u76ae\u5c14\u900a\u76f8\u5173\u6027\u9ad8\u8fbe0.83\uff0c\u6bd4\u9886\u5148\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\u3001Claude\u3001Gemini\uff09\u5f3a1.79\u81f33.23\u500d\u3002\u540c\u65f6\uff0c\u9a8c\u8bc1\u4e86\u4e0e\u89c6\u89c9\u8bc4\u5206\u7684\u805a\u5408\u6548\u5ea6\u3001\u4e0e\u6f14\u8bb2\u8005\u4ea4\u4ed8\u8bc4\u5206\u7684\u5224\u522b\u6548\u5ea6\u4ee5\u53ca\u4e0e\u6574\u4f53\u5370\u8c61\u7684\u63a2\u7d22\u6027\u4e00\u81f4\u6027\u3002", "conclusion": "\u901a\u8fc7\u591a\u6a21\u6001\u5d4c\u5165\u589e\u5f3a\u4f4e\u7ea7\u8bbe\u8ba1\u7ebf\u7d22\uff0c\u80fd\u591f\u7d27\u5bc6\u8fd1\u4f3c\u89c2\u4f17\u5bf9\u5e7b\u706f\u7247\u8d28\u91cf\u7684\u611f\u77e5\u3002\u8be5\u65b9\u6cd5\u4e3a\u5e7b\u706f\u7247\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u5ba2\u89c2\u4e14\u5b9e\u65f6\u7684\u53cd\u9988\u3002"}}
{"id": "2508.19432", "pdf": "https://arxiv.org/pdf/2508.19432", "abs": "https://arxiv.org/abs/2508.19432", "authors": ["Yao Fu", "Xianxuan Long", "Runchao Li", "Haotian Yu", "Mu Sheng", "Xiaotian Han", "Yu Yin", "Pan Li"], "title": "Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs", "categories": ["cs.AI"], "comment": "Accepted to EMNLP2025 main conference (poster)", "summary": "Quantization enables efficient deployment of large language models (LLMs) in\nresource-constrained environments by significantly reducing memory and\ncomputation costs. While quantized LLMs often maintain performance on\nperplexity and zero-shot tasks, their impact on truthfulness-whether generating\ntruthful or deceptive responses-remains largely unexplored. In this work, we\nintroduce TruthfulnessEval, a comprehensive evaluation framework for assessing\nthe truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on\nLogical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on\nImitative Falsehoods. Using this framework, we examine mainstream quantization\ntechniques (ranging from 4-bit to extreme 2-bit) across several open-source\nLLMs. Surprisingly, we find that while quantized models retain internally\ntruthful representations, they are more susceptible to producing false outputs\nunder misleading prompts. To probe this vulnerability, we test 15 rephrased\nvariants of \"honest\", \"neutral\" and \"deceptive\" prompts and observe that\n\"deceptive\" prompts can override truth-consistent behavior, whereas \"honest\"\nand \"neutral\" prompts maintain stable outputs. Further, we reveal that\nquantized models \"know\" the truth internally yet still produce false outputs\nwhen guided by \"deceptive\" prompts via layer-wise probing and PCA\nvisualizations. Our findings provide insights into future designs of\nquantization-aware alignment and truthfulness interventions.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u91cf\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5185\u90e8\u4fdd\u7559\u4e86\u771f\u5b9e\u6027\u8868\u5f81\uff0c\u4f46\u5728\u8bef\u5bfc\u6027\u63d0\u793a\u4e0b\uff0c\u5b83\u4eec\u66f4\u5bb9\u6613\u751f\u6210\u865a\u5047\u8f93\u51fa\uff0c\u5c24\u5176\u201c\u6b3a\u9a97\u6027\u201d\u63d0\u793a\u80fd\u8986\u76d6\u5176\u771f\u5b9e\u6027\u884c\u4e3a\u3002\u7814\u7a76\u901a\u8fc7TruthfulnessEval\u6846\u67b6\u53ca\u63a2\u9488\u6280\u672f\u63ed\u793a\u4e86\u8fd9\u4e00\u6f0f\u6d1e\u3002", "motivation": "\u91cf\u5316\u6280\u672f\u80fd\u6709\u6548\u964d\u4f4eLLMs\u7684\u90e8\u7f72\u6210\u672c\uff0c\u4f46\u5176\u5bf9\u6a21\u578b\u201c\u771f\u5b9e\u6027\u201d\uff08\u5373\u751f\u6210\u771f\u5b9e\u6216\u6b3a\u9a97\u6027\u56de\u5e94\u7684\u80fd\u529b\uff09\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u800c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u56f0\u60d1\u5ea6\u7b49\u6027\u80fd\u6307\u6807\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8bc4\u4f30\u91cf\u5316LLMs\u5728\u771f\u5b9e\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "1. \u5f15\u5165TruthfulnessEval\u6846\u67b6\uff0c\u4ece\u903b\u8f91\u63a8\u7406\u3001\u5e38\u8bc6\u548c\u6a21\u4eff\u6027\u8c0e\u8a00\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u91cf\u5316LLMs\u7684\u771f\u5b9e\u6027\u30022. \u4f7f\u7528\u8be5\u6846\u67b6\uff0c\u5bf9\u591a\u79cd\u4e3b\u6d41\u91cf\u5316\u6280\u672f\uff08\u4ece4\u6bd4\u7279\u5230\u6781\u7aef\u76842\u6bd4\u7279\uff09\u4e0b\u7684\u591a\u4e2a\u5f00\u6e90LLMs\u8fdb\u884c\u6d4b\u8bd5\u30023. \u6d4b\u8bd5\u4e8615\u79cd\u91cd\u65b0\u63aa\u8f9e\u7684\u201c\u8bda\u5b9e\u201d\u3001\u201c\u4e2d\u7acb\u201d\u548c\u201c\u6b3a\u9a97\u6027\u201d\u63d0\u793a\u53d8\u4f53\uff0c\u4ee5\u63a2\u7a76\u6a21\u578b\u5bf9\u865a\u5047\u8f93\u51fa\u7684\u654f\u611f\u6027\u30024. \u901a\u8fc7\u5c42\u7ea7\u63a2\u67e5\u548cPCA\u53ef\u89c6\u5316\u6280\u672f\uff0c\u6df1\u5165\u5206\u6790\u91cf\u5316\u6a21\u578b\u5185\u90e8\u7684\u771f\u5b9e\u6027\u8868\u5f81\u3002", "result": "1. \u91cf\u5316\u6a21\u578b\u5185\u90e8\u4ecd\u4fdd\u7559\u771f\u5b9e\u7684\u8868\u5f81\u30022. \u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u5728\u8bef\u5bfc\u6027\u63d0\u793a\u4e0b\uff0c\u91cf\u5316\u6a21\u578b\u66f4\u5bb9\u6613\u4ea7\u751f\u865a\u5047\u8f93\u51fa\u30023. \u201c\u6b3a\u9a97\u6027\u201d\u63d0\u793a\u80fd\u591f\u8986\u76d6\u6a21\u578b\u4fdd\u6301\u771f\u5b9e\u7684\u884c\u4e3a\uff0c\u800c\u201c\u8bda\u5b9e\u201d\u548c\u201c\u4e2d\u7acb\u201d\u63d0\u793a\u5219\u80fd\u7ef4\u6301\u7a33\u5b9a\u7684\u8f93\u51fa\u30024. \u901a\u8fc7\u5c42\u7ea7\u63a2\u67e5\u548cPCA\u53ef\u89c6\u5316\u63ed\u793a\uff0c\u91cf\u5316\u6a21\u578b\u5185\u90e8\u201c\u77e5\u9053\u201d\u771f\u76f8\uff0c\u4f46\u5728\u201c\u6b3a\u9a97\u6027\u201d\u63d0\u793a\u5f15\u5bfc\u4e0b\u4ecd\u4f1a\u4ea7\u751f\u865a\u5047\u8f93\u51fa\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u7ed3\u679c\u4e3a\u672a\u6765\u8bbe\u8ba1\u201c\u91cf\u5316\u611f\u77e5\u201d\u7684\u5bf9\u9f50\u7b56\u7565\u548c\u771f\u5b9e\u6027\u5e72\u9884\u63aa\u65bd\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u4ee5\u63d0\u5347\u91cf\u5316LLMs\u7684\u771f\u5b9e\u6027\u8868\u73b0\u3002"}}
{"id": "2508.19277", "pdf": "https://arxiv.org/pdf/2508.19277", "abs": "https://arxiv.org/abs/2508.19277", "authors": ["Xinyu Li", "Tianjin Huang", "Ronghui Mu", "Xiaowei Huang", "Gaojie Jin"], "title": "POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Recent advances in Chain-of-Thought (CoT) prompting have substantially\nenhanced the reasoning capabilities of large language models (LLMs), enabling\nsophisticated problem-solving through explicit multi-step reasoning traces.\nHowever, these enhanced reasoning processes introduce novel attack surfaces,\nparticularly vulnerabilities to computational inefficiency through\nunnecessarily verbose reasoning chains that consume excessive resources without\ncorresponding performance gains. Prior overthinking attacks typically require\nrestrictive conditions including access to external knowledge sources for data\npoisoning, reliance on retrievable poisoned content, and structurally obvious\ntemplates that limit practical applicability in real-world scenarios. To\naddress these limitations, we propose POT (Prompt-Only OverThinking), a novel\nblack-box attack framework that employs LLM-based iterative optimization to\ngenerate covert and semantically natural adversarial prompts, eliminating\ndependence on external data access and model retrieval. Extensive experiments\nacross diverse model architectures and datasets demonstrate that POT achieves\nsuperior performance compared to other methods.", "AI": {"tldr": "CoT\u63d0\u793a\u63d0\u5347\u4e86LLM\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u8fc7\u5ea6\u5197\u957f\u63a8\u7406\u94fe\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u7684\u6f0f\u6d1e\u3002\u73b0\u6709\u653b\u51fb\u6709\u5c40\u9650\u3002\u672c\u6587\u63d0\u51faPOT\uff0c\u4e00\u79cd\u65b0\u7684\u9ed1\u76d2\u653b\u51fb\u6846\u67b6\uff0c\u5229\u7528LLM\u8fed\u4ee3\u4f18\u5316\u751f\u6210\u9690\u853d\u7684\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u63d0\u793a\u867d\u589e\u5f3a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u4e0d\u5fc5\u8981\u7684\u5197\u957f\u63a8\u7406\u94fe\uff0c\u6d88\u8017\u8fc7\u591a\u8ba1\u7b97\u8d44\u6e90\u4e14\u65e0\u6027\u80fd\u63d0\u5347\u3002\u73b0\u6709\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u4f9d\u8d56\u5916\u90e8\u77e5\u8bc6\u3001\u4e2d\u6bd2\u5185\u5bb9\u53ef\u68c0\u7d22\u6027\u53ca\u7ed3\u6784\u5316\u6a21\u677f\u7b49\u9650\u5236\uff0c\u5b9e\u7528\u6027\u5dee\u3002", "method": "\u63d0\u51faPOT\uff08Prompt-Only OverThinking\uff09\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u9ed1\u76d2\u653b\u51fb\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u57fa\u4e8eLLM\u7684\u8fed\u4ee3\u4f18\u5316\u6280\u672f\uff0c\u751f\u6210\u9690\u853d\u4e14\u8bed\u4e49\u81ea\u7136\u7684\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u4ece\u800c\u907f\u514d\u4e86\u5bf9\u5916\u90e8\u6570\u636e\u8bbf\u95ee\u548c\u6a21\u578b\u68c0\u7d22\u7684\u4f9d\u8d56\u3002", "result": "\u901a\u8fc7\u5bf9\u591a\u79cd\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660ePOT\u76f8\u8f83\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u653b\u51fb\u6027\u80fd\u3002", "conclusion": "POT\u6846\u67b6\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u653b\u51fb\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b9e\u9645\u3001\u6709\u6548\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u9690\u853d\u63d0\u793a\u8bf1\u5bfcLLM\u4ea7\u751f\u8ba1\u7b97\u4f4e\u6548\u7684\u5197\u957f\u63a8\u7406\u3002"}}
{"id": "2508.19548", "pdf": "https://arxiv.org/pdf/2508.19548", "abs": "https://arxiv.org/abs/2508.19548", "authors": ["Madhuvanthi Srivatsav R", "Chiranjib Bhattacharyya", "Shantanu Chakrabartty", "Chetan Singh Thakur"], "title": "When Routers, Switches and Interconnects Compute: A processing-in-interconnect Paradigm for Scalable Neuromorphic AI", "categories": ["cs.NE", "cs.AR", "cs.NI"], "comment": null, "summary": "Routing, switching, and the interconnect fabric are essential for large-scale\nneuromorphic computing. While this fabric only plays a supporting role in the\nprocess of computing, for large AI workloads it ultimately determines energy\nconsumption and speed. In this paper, we address this bottleneck by asking: (a)\nWhat computing paradigms are inherent in existing routing, switching, and\ninterconnect systems, and how can they be used to implement a\nprocessing-in-Interconnect (\\pi^2) computing paradigm? and (b) leveraging\ncurrent and future interconnect trends, how will a \\pi^2 system's performance\nscale compared to other neuromorphic architectures? For (a), we show that\noperations required for typical AI workloads can be mapped onto delays,\ncausality, time-outs, packet drop, and broadcast operations -- primitives\nalready implemented in packet-switching and packet-routing hardware. We show\nthat existing buffering and traffic-shaping embedded algorithms can be\nleveraged to implement neuron models and synaptic operations. Additionally, a\nknowledge-distillation framework can train and cross-map well-established\nneural network topologies onto $\\pi^2$ without degrading generalization\nperformance. For (b), analytical modeling shows that, unlike other neuromorphic\nplatforms, the energy scaling of $\\pi^2$ improves with interconnect bandwidth\nand energy efficiency. We predict that by leveraging trends in interconnect\ntechnology, a \\pi^2 architecture can be more easily scaled to execute\nbrain-scale AI inference workloads with power consumption levels in the range\nof hundreds of watts.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.19272", "pdf": "https://arxiv.org/pdf/2508.19272", "abs": "https://arxiv.org/abs/2508.19272", "authors": ["Kshitij Fadnis", "Sara Rosenthal", "Maeda Hanafi", "Yannis Katsis", "Marina Danilevsky"], "title": "RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) is an important aspect of conversing\nwith Large Language Models (LLMs) when factually correct information is\nimportant. LLMs may provide answers that appear correct, but could contain\nhallucinated information. Thus, building benchmarks that can evaluate LLMs on\nmulti-turn RAG conversations has become an increasingly important task.\nSimulating real-world conversations is vital for producing high quality\nevaluation benchmarks. We present RAGAPHENE, a chat-based annotation platform\nthat enables annotators to simulate real-world conversations for benchmarking\nand evaluating LLMs. RAGAPHENE has been successfully used by approximately 40\nannotators to build thousands of real-world conversations.", "AI": {"tldr": "RAGAPHENE\u662f\u4e00\u4e2a\u57fa\u4e8e\u804a\u5929\u7684\u6807\u6ce8\u5e73\u53f0\uff0c\u65e8\u5728\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u5bf9\u8bdd\u6765\u6784\u5efa\u591a\u8f6eRAG\u5bf9\u8bdd\u57fa\u51c6\uff0c\u4ee5\u8bc4\u4f30LLM\u7684\u6027\u80fd\u5e76\u89e3\u51b3\u5176\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "LLM\u53ef\u80fd\u751f\u6210\u5305\u542b\u5e7b\u89c9\u4fe1\u606f\u4f46\u8868\u9762\u6b63\u786e\u7684\u7b54\u6848\uff0c\u56e0\u6b64\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u7684\u573a\u666f\u4e2d\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u6784\u5efa\u80fd\u591f\u8bc4\u4f30LLM\u591a\u8f6eRAG\u5bf9\u8bdd\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5e76\u4e14\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u5bf9\u8bdd\u5bf9\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aRAGAPHENE\u7684\u57fa\u4e8e\u804a\u5929\u7684\u6807\u6ce8\u5e73\u53f0\u3002\u8be5\u5e73\u53f0\u5141\u8bb8\u6807\u6ce8\u8005\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u5bf9\u8bdd\u3002", "result": "RAGAPHENE\u5df2\u88ab\u5927\u7ea640\u540d\u6807\u6ce8\u8005\u6210\u529f\u4f7f\u7528\uff0c\u5e76\u6784\u5efa\u4e86\u6570\u5343\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u5bf9\u8bdd\u3002", "conclusion": "RAGAPHENE\u662f\u4e00\u4e2a\u6709\u6548\u7684\u5de5\u5177\uff0c\u80fd\u591f\u5e2e\u52a9\u6784\u5efa\u7528\u4e8e\u8bc4\u4f30LLM\u7684\u591a\u8f6eRAG\u5bf9\u8bdd\u7684\u9ad8\u8d28\u91cf\u3001\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\uff0c\u4ece\u800c\u5e94\u5bf9LLM\u53ef\u80fd\u4ea7\u751f\u5e7b\u89c9\u7684\u6311\u6218\u3002"}}
{"id": "2508.19290", "pdf": "https://arxiv.org/pdf/2508.19290", "abs": "https://arxiv.org/abs/2508.19290", "authors": ["Alexandros Gkillas", "Ioulia Kapsali", "Nikos Piperigkos", "Aris S. Lalos"], "title": "Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "LiDAR-based segmentation is essential for reliable perception in autonomous\nvehicles, yet modern segmentation networks are highly susceptible to\nadversarial attacks that can compromise safety. Most existing defenses are\ndesigned for networks operating directly on raw 3D point clouds and rely on\nlarge, computationally intensive generative models. However, many\nstate-of-the-art LiDAR segmentation pipelines operate on more efficient 2D\nrange view representations. Despite their widespread adoption, dedicated\nlightweight adversarial defenses for this domain remain largely unexplored. We\nintroduce an efficient model-based purification framework tailored for\nadversarial defense in 2D range-view LiDAR segmentation. We propose a direct\nattack formulation in the range-view domain and develop an explainable\npurification network based on a mathematical justified optimization problem,\nachieving strong adversarial resilience with minimal computational overhead.\nOur method achieves competitive performance on open benchmarks, consistently\noutperforming generative and adversarial training baselines. More importantly,\nreal-world deployment on a demo vehicle demonstrates the framework's ability to\ndeliver accurate operation in practical autonomous driving scenarios.", "AI": {"tldr": "\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4e2d2D\u8ddd\u79bb\u89c6\u56feLiDAR\u5206\u5272\u6613\u53d7\u5bf9\u6297\u653b\u51fb\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u4f4e\u7684\u6a21\u578b\u51c0\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u6570\u5b66\u4f18\u5316\u6784\u5efa\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u90e8\u7f72\u4e2d\u5747\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "motivation": "LiDAR\u5206\u5272\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u4ee3\u5206\u5272\u7f51\u7edc\u6613\u53d7\u5bf9\u6297\u653b\u51fb\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u539f\u59cb3D\u70b9\u4e91\u4e14\u8ba1\u7b97\u91cf\u5927\u3002\u8bb8\u591a\u5148\u8fdb\u7684LiDAR\u5206\u5272\u6d41\u6c34\u7ebf\u91c7\u7528\u66f4\u9ad8\u6548\u76842D\u8ddd\u79bb\u89c6\u56fe\u8868\u793a\uff0c\u4f46\u8be5\u9886\u57df\u7f3a\u4e4f\u4e13\u7528\u4e14\u8f7b\u91cf\u7ea7\u7684\u5bf9\u6297\u9632\u5fa1\u65b9\u6848\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u4e13\u4e3a2D\u8ddd\u79bb\u89c6\u56feLiDAR\u5206\u5272\u8bbe\u8ba1\u7684\u9ad8\u6548\u6a21\u578b\u51c0\u5316\u6846\u67b6\u3002\u63d0\u51fa\u4e86\u5728\u8ddd\u79bb\u89c6\u56fe\u57df\u7684\u76f4\u63a5\u653b\u51fb\u516c\u5f0f\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6570\u5b66\u4f18\u5316\u95ee\u9898\u7684\u53ef\u89e3\u91ca\u51c0\u5316\u7f51\u7edc\uff0c\u65e8\u5728\u4ee5\u6700\u5c0f\u7684\u8ba1\u7b97\u5f00\u9500\u5b9e\u73b0\u5f3a\u5927\u7684\u5bf9\u6297\u5f39\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u751f\u6210\u5f0f\u548c\u5bf9\u6297\u8bad\u7ec3\u57fa\u7ebf\u65b9\u6cd5\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5728\u6f14\u793a\u8f66\u8f86\u4e0a\u7684\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u63d0\u4f9b\u51c6\u786e\u64cd\u4f5c\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u51c0\u5316\u6846\u67b6\u4e3a2D\u8ddd\u79bb\u89c6\u56feLiDAR\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\u7684\u5bf9\u6297\u9632\u5fa1\u89e3\u51b3\u65b9\u6848\u3002\u5b83\u4e0d\u4ec5\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u800c\u4e14\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u5b89\u5168\u6027\u3002"}}
{"id": "2508.19461", "pdf": "https://arxiv.org/pdf/2508.19461", "abs": "https://arxiv.org/abs/2508.19461", "authors": ["Neil Kale", "Chen Bo Calvin Zhang", "Kevin Zhu", "Ankit Aich", "Paula Rodriguez", "Scale Red Team", "Christina Q. Knight", "Zifan Wang"], "title": "Reliable Weak-to-Strong Monitoring of LLM Agents", "categories": ["cs.AI", "cs.CR", "cs.LG"], "comment": "18 pages, 15 figures", "summary": "We stress test monitoring systems for detecting covert misbehavior in\nautonomous LLM agents (e.g., secretly sharing private information). To this\nend, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)\nvarying levels of agent and monitor situational awareness; (2) distinct\nadversarial strategies to evade the monitor, such as prompt injection; and (3)\ntwo datasets and environments -- SHADE-Arena for tool-calling agents and our\nnew CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We\nrun MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse\nagent trajectories, alongside a new hybrid hierarchical-sequential scaffolding\nproposed in this work. Our empirical results yield three key findings. First,\nagent awareness dominates monitor awareness: an agent's knowledge that it is\nbeing monitored substantially degrades the monitor's reliability. On the\ncontrary, providing the monitor with more information about the agent is less\nhelpful than expected. Second, monitor scaffolding matters more than monitor\nawareness: the hybrid scaffolding consistently outperforms baseline monitor\nscaffolding, and can enable weaker models to reliably monitor stronger agents\n-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where\nhumans discuss with the LLM monitor to get an updated judgment for the agent's\nbehavior, targeted human oversight is most effective; escalating only\npre-flagged cases to human reviewers improved the TPR by approximately 15% at\nFPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the\nlack of adversarial robustness for LLMs and humans when monitoring and\ndetecting agent misbehavior. We release code, data, and logs to spur further\nresearch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u76d1\u6d4b\u7ea2\u961f\u6d4b\u8bd5(MRT)\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7528\u4e8e\u538b\u529b\u6d4b\u8bd5\u68c0\u6d4b\u81ea\u4e3bLLM\u4ee3\u7406\u79d8\u5bc6\u4e0d\u5f53\u884c\u4e3a\u7684\u76d1\u6d4b\u7cfb\u7edf\uff0c\u5e76\u63ed\u793a\u4e86\u4ee3\u7406\u610f\u8bc6\u3001\u76d1\u6d4b\u652f\u67b6\u7ed3\u6784\u548c\u6709\u9488\u5bf9\u6027\u7684\u4eba\u5de5\u5e72\u9884\u5bf9\u76d1\u6d4b\u53ef\u9760\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u68c0\u6d4b\u81ea\u4e3bLLM\u4ee3\u7406\uff08\u4f8b\u5982\uff0c\u79d8\u5bc6\u5171\u4eab\u79c1\u4eba\u4fe1\u606f\uff09\u7684\u9690\u853d\u4e0d\u5f53\u884c\u4e3a\u662f\u91cd\u8981\u6311\u6218\u3002\u7814\u7a76\u52a8\u673a\u662f\u538b\u529b\u6d4b\u8bd5\u73b0\u6709\u76d1\u6d4b\u7cfb\u7edf\uff0c\u5e76\u7cfb\u7edf\u5316\u4e00\u4e2a\u76d1\u6d4b\u7ea2\u961f\u6d4b\u8bd5(MRT)\u5de5\u4f5c\u6d41\u7a0b\u6765\u8bc4\u4f30\u5176\u9c81\u68d2\u6027\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\uff1a1) \u7cfb\u7edf\u5316\u4e00\u4e2a\u76d1\u6d4b\u7ea2\u961f\u6d4b\u8bd5(MRT)\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8be5\u5de5\u4f5c\u6d41\u7a0b\u8003\u8651\u4ee3\u7406\u548c\u76d1\u6d4b\u5668\u7684\u4e0d\u540c\u60c5\u5883\u611f\u77e5\u6c34\u5e73\uff1b2) \u91c7\u7528\u63d0\u793a\u6ce8\u5165\u7b49\u5bf9\u6297\u7b56\u7565\u6765\u89c4\u907f\u76d1\u6d4b\u5668\uff1b3) \u4f7f\u7528SHADE-Arena\u548c\u65b0\u6269\u5c55\u7684CUA-SHADE-Arena\u4e24\u4e2a\u6570\u636e\u96c6\u548c\u73af\u5883\uff1b4) \u5728\u73b0\u6709LLM\u76d1\u6d4b\u652f\u67b6\u7ed3\u6784\u548c\u672c\u6587\u63d0\u51fa\u7684\u65b0\u578b\u6df7\u5408\u5206\u5c42-\u987a\u5e8f\u652f\u67b6\u7ed3\u6784\u4e0a\u8fd0\u884cMRT\u3002", "result": "\u4e3b\u8981\u7814\u7a76\u7ed3\u679c\u6709\u4e09\u70b9\uff1a1) \u4ee3\u7406\u610f\u8bc6\u4e3b\u5bfc\u76d1\u6d4b\u5668\u610f\u8bc6\uff0c\u4ee3\u7406\u77e5\u9053\u88ab\u76d1\u6d4b\u4f1a\u663e\u8457\u964d\u4f4e\u76d1\u6d4b\u53ef\u9760\u6027\uff1b2) \u76d1\u6d4b\u652f\u67b6\u7ed3\u6784\u6bd4\u76d1\u6d4b\u5668\u610f\u8bc6\u66f4\u91cd\u8981\uff0c\u6df7\u5408\u652f\u67b6\u7ed3\u6784\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u80fd\u5b9e\u73b0\u201c\u5f31\u76d1\u6d4b\u5f3a\u4ee3\u7406\u201d\uff1b3) \u5728\u4eba\u5de5\u53c2\u4e0e\u8bbe\u7f6e\u4e2d\uff0c\u6709\u9488\u5bf9\u6027\u7684\u4eba\u5de5\u76d1\u7763\u6700\u6709\u6548\uff0c\u4ec5\u5c06\u9884\u6807\u8bb0\u6848\u4f8b\u5347\u7ea7\u7ed9\u4eba\u5de5\u5ba1\u67e5\u53ef\u5c06TPR\u63d0\u9ad8\u7ea615% (FPR=0.01)\u3002", "conclusion": "\u672c\u5de5\u4f5c\u5efa\u7acb\u4e86MRT\u7684\u6807\u51c6\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5f3a\u8c03\u4e86LLM\u548c\u4eba\u7c7b\u5728\u76d1\u6d4b\u548c\u68c0\u6d4b\u4ee3\u7406\u4e0d\u5f53\u884c\u4e3a\u65f6\u7f3a\u4e4f\u5bf9\u6297\u9c81\u68d2\u6027\u3002\u7814\u7a76\u53d1\u5e03\u4e86\u4ee3\u7801\u3001\u6570\u636e\u548c\u65e5\u5fd7\uff0c\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.19318", "pdf": "https://arxiv.org/pdf/2508.19318", "abs": "https://arxiv.org/abs/2508.19318", "authors": ["Aohan Li", "Miyu Tsuzuki"], "title": "(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep Reinforcement Learning (DRL) has emerged as an efficient approach to\nresource allocation due to its strong capability in handling complex\ndecision-making tasks. However, only limited research has explored the training\nof DRL models with real-world data in practical, distributed Internet of Things\n(IoT) systems. To bridge this gap, this paper proposes a novel framework for\ntraining DRL models in real-world distributed IoT environments. In the proposed\nframework, IoT devices select communication channels using a DRL-based method,\nwhile the DRL model is trained with feedback information. Specifically,\nAcknowledgment (ACK) information is obtained from actual data transmissions\nover the selected channels. Implementation and performance evaluation, in terms\nof Frame Success Rate (FSR), are carried out, demonstrating both the\nfeasibility and the effectiveness of the proposed framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u771f\u5b9e\u5206\u5e03\u5f0f\u7269\u8054\u7f51\u73af\u5883\u4e2d\u8bad\u7ec3DRL\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7ACK\u53cd\u9988\u4f18\u5316\u4fe1\u9053\u9009\u62e9\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u5229\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u8bad\u7ec3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6a21\u578b\u4ee5\u8fdb\u884c\u5b9e\u9645\u5206\u5e03\u5f0f\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u771f\u5b9e\u5206\u5e03\u5f0f\u7269\u8054\u7f51\u73af\u5883\u4e2d\u8bad\u7ec3DRL\u6a21\u578b\u3002\u7269\u8054\u7f51\u8bbe\u5907\u4f7f\u7528\u57fa\u4e8eDRL\u7684\u65b9\u6cd5\u9009\u62e9\u901a\u4fe1\u4fe1\u9053\uff0cDRL\u6a21\u578b\u5219\u901a\u8fc7\u4ece\u5b9e\u9645\u6570\u636e\u4f20\u8f93\u4e2d\u83b7\u53d6\u7684\u786e\u8ba4\uff08ACK\uff09\u4fe1\u606f\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u5b9e\u73b0\u548c\u6027\u80fd\u8bc4\u4f30\uff08\u4ee5\u5e27\u6210\u529f\u7387FSR\u8861\u91cf\uff09\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684DRL\u8bad\u7ec3\u6846\u67b6\u5728\u771f\u5b9e\u5206\u5e03\u5f0f\u7269\u8054\u7f51\u73af\u5883\u4e2d\u8fdb\u884c\u4fe1\u9053\u9009\u62e9\u662f\u53ef\u884c\u4e14\u6709\u6548\u7684\u3002"}}
{"id": "2508.19274", "pdf": "https://arxiv.org/pdf/2508.19274", "abs": "https://arxiv.org/abs/2508.19274", "authors": ["Yue Chu"], "title": "Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis", "categories": ["cs.CL"], "comment": "Ph.D. dissertation submitted to The Ohio State University, August\n  2025", "summary": "In countries without civil registration and vital statistics, verbal autopsy\n(VA) is a critical tool for estimating cause of death (COD) and inform policy\npriorities. In VA, interviewers ask proximal informants for details on the\ncircumstances preceding a death, in the form of unstructured narratives and\nstructured questions. Existing automated VA cause classification algorithms\nonly use the questions and ignore the information in the narratives. In this\nthesis, we investigate how the VA narrative can be used for automated COD\nclassification using pretrained language models (PLMs) and machine learning\n(ML) techniques. Using empirical data from South Africa, we demonstrate that\nwith the narrative alone, transformer-based PLMs with task-specific fine-tuning\noutperform leading question-only algorithms at both the individual and\npopulation levels, particularly in identifying non-communicable diseases. We\nexplore various multimodal fusion strategies combining narratives and questions\nin unified frameworks. Multimodal approaches further improve performance in COD\nclassification, confirming that each modality has unique contributions and may\ncapture valuable information that is not present in the other modality. We also\ncharacterize physician-perceived information sufficiency in VA. We describe\nvariations in sufficiency levels by age and COD and demonstrate that\nclassification accuracy is affected by sufficiency for both physicians and\nmodels. Overall, this thesis advances the growing body of knowledge at the\nintersection of natural language processing, epidemiology, and global health.\nIt demonstrates the value of narrative in enhancing COD classification. Our\nfindings underscore the need for more high-quality data from more diverse\nsettings to use in training and fine-tuning PLM/ML methods, and offer valuable\ninsights to guide the rethinking and redesign of the VA instrument and\ninterview.", "AI": {"tldr": "\u672c\u8bba\u6587\u63a2\u8ba8\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\uff0c\u901a\u8fc7\u53e3\u8ff0\u5c38\u68c0\uff08VA\uff09\u4e2d\u7684\u53d9\u8ff0\u6587\u672c\u8fdb\u884c\u6b7b\u56e0\uff08COD\uff09\u81ea\u52a8\u5316\u5206\u7c7b\u3002\u7814\u7a76\u8868\u660e\uff0c\u53d9\u8ff0\u6587\u672c\u5355\u72ec\u6216\u7ed3\u5408\u7ed3\u6784\u5316\u95ee\u9898\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u53d9\u8ff0\u6587\u672c\u7684\u4ef7\u503c\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u53e3\u8ff0\u5c38\u68c0\uff08VA\uff09\u7b97\u6cd5\u4ec5\u4f7f\u7528\u7ed3\u6784\u5316\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u53d9\u8ff0\u6027\u6587\u672c\u4e2d\u53ef\u80fd\u5305\u542b\u7684\u91cd\u8981\u6b7b\u56e0\u4fe1\u606f\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u5982\u4f55\u5229\u7528VA\u53d9\u8ff0\u6587\u672c\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u63d0\u5347\u6b7b\u56e0\uff08COD\uff09\u81ea\u52a8\u5316\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u4ee5\u66f4\u597d\u5730\u652f\u6301\u516c\u5171\u536b\u751f\u51b3\u7b56\u3002", "method": "\u7814\u7a76\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u5bf9\u5357\u975e\u7684\u5b9e\u8bc1\u6570\u636e\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u4ee5\u5229\u7528VA\u53d9\u8ff0\u6587\u672c\u8fdb\u884c\u6b7b\u56e0\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u7d22\u4e86\u7ed3\u5408\u53d9\u8ff0\u6587\u672c\u548c\u7ed3\u6784\u5316\u95ee\u9898\u7684\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u5e76\u5206\u6790\u4e86\u533b\u751f\u611f\u77e5\u7684VA\u4fe1\u606f\u5145\u5206\u6027\u53ca\u5176\u5bf9\u5206\u7c7b\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "result": "1. \u4ec5\u4f7f\u7528\u53d9\u8ff0\u6587\u672c\u7684\u57fa\u4e8eTransformer\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u4f53\u548c\u7fa4\u4f53\u5c42\u9762\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u4ec5\u4f7f\u7528\u95ee\u9898\u7684\u7b97\u6cd5\uff0c\u5c24\u5176\u5728\u8bc6\u522b\u975e\u4f20\u67d3\u6027\u75be\u75c5\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u30022. \u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6b7b\u56e0\u5206\u7c7b\u6027\u80fd\uff0c\u8bc1\u5b9e\u4e86\u5404\u6a21\u6001\u7684\u72ec\u7279\u8d21\u732e\u30023. \u4fe1\u606f\u5145\u5206\u6027\u56e0\u5e74\u9f84\u548c\u6b7b\u56e0\u800c\u5f02\uff0c\u5e76\u4f1a\u5f71\u54cd\u533b\u751f\u548c\u6a21\u578b\u7684\u5206\u7c7b\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u8bba\u6587\u8bc1\u660e\u4e86\u53e3\u8ff0\u5c38\u68c0\uff08VA\uff09\u53d9\u8ff0\u6587\u672c\u5728\u63d0\u9ad8\u6b7b\u56e0\uff08COD\uff09\u81ea\u52a8\u5316\u5206\u7c7b\u65b9\u9762\u7684\u663e\u8457\u4ef7\u503c\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u591a\u6765\u81ea\u591a\u6837\u5316\u80cc\u666f\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u6765\u8bad\u7ec3\u548c\u5fae\u8c03PLM/ML\u65b9\u6cd5\uff0c\u5e76\u4e3a\u91cd\u65b0\u601d\u8003\u548c\u8bbe\u8ba1VA\u5de5\u5177\u53ca\u8bbf\u8c08\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\uff0c\u63a8\u8fdb\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u6d41\u884c\u75c5\u5b66\u548c\u5168\u7403\u5065\u5eb7\u9886\u57df\u7684\u4ea4\u53c9\u77e5\u8bc6\u3002"}}
{"id": "2508.19294", "pdf": "https://arxiv.org/pdf/2508.19294", "abs": "https://arxiv.org/abs/2508.19294", "authors": ["Ranjan Sapkota", "Manoj Karkee"], "title": "Object Detection with Multimodal Large Vision-Language Models: An In-depth Review", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "First Peer Reviewed Review Paper for Object Detection with\n  Vision-Language Models (VLMs)", "summary": "The fusion of language and vision in large vision-language models (LVLMs) has\nrevolutionized deep learning-based object detection by enhancing adaptability,\ncontextual reasoning, and generalization beyond traditional architectures. This\nin-depth review presents a structured exploration of the state-of-the-art in\nLVLMs, systematically organized through a three-step research review process.\nFirst, we discuss the functioning of vision language models (VLMs) for object\ndetection, describing how these models harness natural language processing\n(NLP) and computer vision (CV) techniques to revolutionize object detection and\nlocalization. We then explain the architectural innovations, training\nparadigms, and output flexibility of recent LVLMs for object detection,\nhighlighting how they achieve advanced contextual understanding for object\ndetection. The review thoroughly examines the approaches used in integration of\nvisual and textual information, demonstrating the progress made in object\ndetection using VLMs that facilitate more sophisticated object detection and\nlocalization strategies. This review presents comprehensive visualizations\ndemonstrating LVLMs' effectiveness in diverse scenarios including localization\nand segmentation, and then compares their real-time performance, adaptability,\nand complexity to traditional deep learning systems. Based on the review, its\nis expected that LVLMs will soon meet or surpass the performance of\nconventional methods in object detection. The review also identifies a few\nmajor limitations of the current LVLM modes, proposes solutions to address\nthose challenges, and presents a clear roadmap for the future advancement in\nthis field. We conclude, based on this study, that the recent advancement in\nLVLMs have made and will continue to make a transformative impact on object\ndetection and robotic applications in the future.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u6df1\u5165\u63a2\u8ba8\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5206\u6790\u4e86\u5176\u67b6\u6784\u3001\u8bad\u7ec3\u8303\u5f0f\u3001\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u5bf9\u6bd4\u3001\u5c40\u9650\u6027\u53ca\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u5e76\u9884\u6d4b\u4e86\u5176\u5728\u76ee\u6807\u68c0\u6d4b\u548c\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u53d8\u9769\u6027\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5728\u9002\u5e94\u6027\u3001\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u901a\u8fc7\u878d\u5408\u8bed\u8a00\u548c\u89c6\u89c9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd9\u4e9b\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u8be5\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u8fdb\u884c\u6df1\u5165\u56de\u987e\u548c\u5206\u6790\u3002", "method": "\u672c\u6587\u91c7\u7528\u6df1\u5165\u7efc\u8ff0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u6b65\u7814\u7a76\u56de\u987e\u8fc7\u7a0b\u7cfb\u7edf\u5730\u63a2\u7d22\u4e86LVLMs\u7684\u6700\u65b0\u8fdb\u5c55\u3002\u5177\u4f53\u5305\u62ec\uff1a\u8ba8\u8bba\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u5de5\u4f5c\u539f\u7406\uff1b\u89e3\u91caLVLMs\u7684\u67b6\u6784\u521b\u65b0\u3001\u8bad\u7ec3\u8303\u5f0f\u548c\u8f93\u51fa\u7075\u6d3b\u6027\uff1b\u5ba1\u67e5\u89c6\u89c9\u4e0e\u6587\u672c\u4fe1\u606f\u878d\u5408\u7684\u65b9\u6cd5\uff1b\u901a\u8fc7\u53ef\u89c6\u5316\u5c55\u793aLVLMs\u5728\u591a\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\uff1b\u5e76\u4e0e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u5728\u5b9e\u65f6\u6027\u80fd\u3001\u9002\u5e94\u6027\u548c\u590d\u6742\u6027\u65b9\u9762\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7efc\u8ff0\u5c55\u793a\u4e86LVLMs\u5728\u5305\u62ec\u5b9a\u4f4d\u548c\u5206\u5272\u5728\u5185\u7684\u591a\u79cd\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\uff0c\u7a81\u51fa\u4e86\u5176\u5148\u8fdb\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u3002\u7814\u7a76\u9884\u8ba1LVLMs\u5c06\u5f88\u5feb\u8fbe\u5230\u6216\u8d85\u8d8a\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u7efc\u8ff0\u8fd8\u6307\u51fa\u4e86\u5f53\u524dLVLMs\u7684\u4e3b\u8981\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u89e3\u51b3\u65b9\u6848\u53ca\u672a\u6765\u7684\u53d1\u5c55\u8def\u7ebf\u56fe\u3002", "conclusion": "\u57fa\u4e8e\u672c\u7814\u7a76\uff0c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u5df2\u7ecf\u5e76\u5c06\u7ee7\u7eed\u5bf9\u76ee\u6807\u68c0\u6d4b\u53ca\u672a\u6765\u7684\u673a\u5668\u4eba\u5e94\u7528\u4ea7\u751f\u53d8\u9769\u6027\u5f71\u54cd\u3002"}}
{"id": "2508.19502", "pdf": "https://arxiv.org/pdf/2508.19502", "abs": "https://arxiv.org/abs/2508.19502", "authors": ["Xifeng Yao", "Chengyuan Ma", "Dongyu Lang", "Yinhao Ni", "Zhiwei Xu", "Huarui Xie", "Zihao Chen", "Guang Shen", "Dandan Tu", "Yi Bai", "Changzheng Zhang"], "title": "SLIM: Subtrajectory-Level Elimination for More Effective Reasoning", "categories": ["cs.AI"], "comment": "EMNLP 2025 Findings", "summary": "In recent months, substantial progress has been made in complex reasoning of\nLarge Language Models, particularly through the application of test-time\nscaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When\nresponding to a query, these models generate an extended reasoning trajectory,\nduring which the model explores, reflects, backtracks, and self-verifies before\narriving at a conclusion. However, fine-tuning models with such reasoning\ntrajectories may not always be optimal. Our findings indicate that not all\ncomponents within these reasoning trajectories contribute positively to the\nreasoning process; in fact, some components may affect the overall performance\nnegatively. In this study, we divide a reasoning trajectory into individual\nsubtrajectories and develop a \"5+2\" framework to: (1) systematically identify\nsuboptimal subtrajectories within the reasoning trajectory based on five\nhuman-established criteria; (2) assess the independence of the suboptimal\nsubtrajectories identified in (1) from the subsequent content, ensuring that\ntheir elimination does not compromise overall flow and coherence of the\nreasoning process. Additionally, a sampling algorithm, built upon the \"5+2\"\nframework, is employed to select data whose reasoning process is free from\nsuboptimal subtrajectories to the highest degree. Experimental results\ndemonstrate that our method can reduce the number of suboptimal subtrajectories\nby 25.9\\% during the inference. Furthermore, our method achieves an average\naccuracy of 58.92\\% on highly challenging math benchmarks with only two thirds\nof training data, surpassing the average accuracy of 58.06\\% achieved with the\nentire data, and outperforming open-source datasets, when fine-tuning\nQwen2.5-Math-7B. Finally, We validated our method under resource constraints\nand observed improved performance across various inference token limits.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u201c5+2\u201d\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u5e76\u6d88\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u6b21\u4f18\u5b50\u8f68\u8ff9\uff0c\u4ee5\u4f18\u5316\u6a21\u578b\u5fae\u8c03\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u8fd1\u671f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\uff0c\u7528\u4e8e\u5fae\u8c03\u7684\u63a8\u7406\u8f68\u8ff9\u5e76\u975e\u6240\u6709\u7ec4\u4ef6\u90fd\u6709\u76ca\uff0c\u90e8\u5206\u751a\u81f3\u53ef\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff0c\u5bfc\u81f4\u5fae\u8c03\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u7814\u7a76\u5c06\u63a8\u7406\u8f68\u8ff9\u5212\u5206\u4e3a\u5b50\u8f68\u8ff9\uff0c\u5e76\u5f00\u53d1\u4e86\u201c5+2\u201d\u6846\u67b6\uff1a(1) \u57fa\u4e8e\u4e94\u4e2a\u4eba\u5de5\u5efa\u7acb\u7684\u6807\u51c6\u8bc6\u522b\u6b21\u4f18\u5b50\u8f68\u8ff9\uff1b(2) \u8bc4\u4f30\u8fd9\u4e9b\u6b21\u4f18\u5b50\u8f68\u8ff9\u7684\u72ec\u7acb\u6027\uff0c\u786e\u4fdd\u79fb\u9664\u540e\u4e0d\u5f71\u54cd\u63a8\u7406\u7684\u8fde\u8d2f\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u4e00\u79cd\u91c7\u6837\u7b97\u6cd5\uff0c\u57fa\u4e8e\u6b64\u6846\u67b6\u9009\u62e9\u9ad8\u8d28\u91cf\u7684\u3001\u4e0d\u542b\u6b21\u4f18\u5b50\u8f68\u8ff9\u7684\u6570\u636e\u8fdb\u884c\u6a21\u578b\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5c06\u6b21\u4f18\u5b50\u8f68\u8ff9\u7684\u6570\u91cf\u51cf\u5c11\u4e8625.9%\u3002\u5728\u4ec5\u4f7f\u7528\u4e09\u5206\u4e4b\u4e8c\u8bad\u7ec3\u6570\u636e\u5fae\u8c03Qwen2.5-Math-7B\u65f6\uff0c\u8be5\u65b9\u6cd5\u5728\u6311\u6218\u6027\u6570\u5b66\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e8658.92%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u4f7f\u7528\u5168\u90e8\u6570\u636e\uff0858.06%\uff09\u548c\u5f00\u6e90\u6570\u636e\u96c6\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u4e0d\u540c\u63a8\u7406Token\u9650\u5236\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4e5f\u5c55\u73b0\u51fa\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u8bc6\u522b\u5e76\u6d88\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u6b21\u4f18\u90e8\u5206\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u5fae\u8c03\u7684\u6548\u679c\uff0c\u5728\u51cf\u5c11\u8bad\u7ec3\u6570\u636e\u91cf\u6216\u53d7\u8d44\u6e90\u9650\u5236\u7684\u60c5\u51b5\u4e0b\uff0c\u4ecd\u80fd\u63d0\u9ad8\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.19344", "pdf": "https://arxiv.org/pdf/2508.19344", "abs": "https://arxiv.org/abs/2508.19344", "authors": ["Daniil Zelezetsky", "Egor Cherepanov", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "Re:Frame -- Retrieving Experience From Associative Memory", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages, 3 figures", "summary": "Offline reinforcement learning (RL) often deals with suboptimal data when\ncollecting large expert datasets is unavailable or impractical. This limitation\nmakes it difficult for agents to generalize and achieve high performance, as\nthey must learn primarily from imperfect or inconsistent trajectories. A\ncentral challenge is therefore how to best leverage scarce expert\ndemonstrations alongside abundant but lower-quality data. We demonstrate that\nincorporating even a tiny amount of expert experience can substantially improve\nRL agent performance. We introduce Re:Frame (Retrieving Experience From\nAssociative Memory), a plug-in module that augments a standard offline RL\npolicy (e.g., Decision Transformer) with a small external Associative Memory\nBuffer (AMB) populated by expert trajectories drawn from a separate dataset.\nDuring training on low-quality data, the policy learns to retrieve expert data\nfrom the Associative Memory Buffer (AMB) via content-based associations and\nintegrate them into decision-making; the same AMB is queried at evaluation.\nThis requires no environment interaction and no modifications to the backbone\narchitecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories\n(0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a\nstrong Decision Transformer baseline in three of four settings, with gains up\nto +10.7 normalized points. These results show that Re:Frame offers a simple\nand data-efficient way to inject scarce expert knowledge and substantially\nimprove offline RL from low-quality datasets.", "AI": {"tldr": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u56e0\u6b21\u4f18\u6570\u636e\u53d7\u9650\uff0c\u672c\u7814\u7a76\u63d0\u51faRe:Frame\u6a21\u5757\uff0c\u901a\u8fc7\u5f15\u5165\u5173\u8054\u8bb0\u5fc6\u7f13\u51b2\u533a\u6ce8\u5165\u5c11\u91cf\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ece\u4f4e\u8d28\u91cf\u6570\u636e\u96c6\u5b66\u4e60\u7684\u6027\u80fd\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u7f3a\u4e4f\u5927\u91cf\u4e13\u5bb6\u6570\u636e\u65f6\uff0c\u96be\u4ee5\u4ece\u4e0d\u5b8c\u7f8e\u6216\u4e0d\u4e00\u81f4\u7684\u8f68\u8ff9\u4e2d\u6cdb\u5316\u548c\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002\u6838\u5fc3\u52a8\u673a\u662f\u5982\u4f55\u6700\u4f73\u5730\u5229\u7528\u7a00\u7f3a\u7684\u4e13\u5bb6\u6f14\u793a\u4e0e\u5927\u91cf\u4f4e\u8d28\u91cf\u6570\u636e\u3002", "method": "\u5f15\u5165Re:Frame\u6a21\u5757\uff0c\u5b83\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u7ec4\u4ef6\uff0c\u901a\u8fc7\u4e00\u4e2a\u7531\u4e13\u5bb6\u8f68\u8ff9\u586b\u5145\u7684\u5916\u90e8\u5173\u8054\u8bb0\u5fc6\u7f13\u51b2\u533a\uff08AMB\uff09\u6765\u589e\u5f3a\u6807\u51c6\u79bb\u7ebfRL\u7b56\u7565\uff08\u5982Decision Transformer\uff09\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u7b56\u7565\u901a\u8fc7\u57fa\u4e8e\u5185\u5bb9\u7684\u5173\u8054\u4eceAMB\u4e2d\u68c0\u7d22\u4e13\u5bb6\u6570\u636e\u5e76\u6574\u5408\u5230\u51b3\u7b56\u4e2d\uff0c\u65e0\u9700\u73af\u5883\u4ea4\u4e92\u6216\u4fee\u6539\u4e3b\u5e72\u67b6\u6784\u3002", "result": "\u5728D4RL MuJoCo\u4efb\u52a1\u4e0a\uff0c\u4ec5\u4f7f\u752860\u6761\u4e13\u5bb6\u8f68\u8ff9\uff08\u5360\u6570\u636e\u96c6\u76840.1%\uff09\uff0cRe:Frame\u5728\u56db\u5206\u4e4b\u4e09\u7684\u8bbe\u7f6e\u4e2d\u6301\u7eed\u4f18\u4e8e\u5f3a\u5927\u7684Decision Transformer\u57fa\u7ebf\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe+10.7\u6807\u51c6\u5316\u5206\u6570\u3002", "conclusion": "Re:Frame\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u6ce8\u5165\u7a00\u7f3a\u7684\u4e13\u5bb6\u77e5\u8bc6\uff0c\u4ece\u800c\u663e\u8457\u6539\u5584\u4ece\u4f4e\u8d28\u91cf\u6570\u636e\u96c6\u8fdb\u884c\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u3002"}}
{"id": "2508.19279", "pdf": "https://arxiv.org/pdf/2508.19279", "abs": "https://arxiv.org/abs/2508.19279", "authors": ["Gunjan Jalori", "Preetika Verma", "Sercan \u00d6 Ar\u0131k"], "title": "FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP", "summary": "Time series Forecasting with large languagemodels (LLMs) requires bridging\nnumericalpatterns and natural language. Effective fore-casting on LLM often\nrelies on extensive pre-processing and fine-tuning.Recent studiesshow that a\nfrozen LLM can rival specializedforecasters when supplied with a carefully\nen-gineered natural-language prompt, but craft-ing such a prompt for each task\nis itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt\noptimization framework thatutilizes an agentic system: a\nForecaster-agentgenerates forecasts using an initial prompt,which is then\nrefined by a refiner agent, in-formed by past outputs and retrieved\nanalogs.This adaptive prompting generalizes across do-mains using creative\nprompt templates andgenerates high-quality forecasts without inter-mediate code\ngeneration.Experiments onbenchmark datasets show improved accuracyover static\nprompting and retrieval-augmentedbaselines, approaching the performance\nofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning,\nachievingstrong performance via its agentic approach toadaptive prompt\nrefinement and retrieval.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFLAIRR-TS\uff0c\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\uff08agentic system\uff09\u7684\u6d4b\u8bd5\u65f6\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u9002\u5e94\u63d0\u793a\u7ec6\u5316\u548c\u68c0\u7d22\uff0c\u4f7f\u51bb\u7ed3\u7684LLM\u5728\u65e0\u9700\u5927\u91cf\u9884\u5904\u7406\u3001\u5fae\u8c03\u6216\u624b\u52a8\u63d0\u793a\u5de5\u7a0b\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528LLM\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65f6\uff0c\u8981\u4e48\u9700\u8981\u5927\u91cf\u7684\u9884\u5904\u7406\u548c\u5fae\u8c03\uff0c\u8981\u4e48\u9700\u8981\u8017\u65f6\u4e14\u7279\u8bbe\uff08ad-hoc\uff09\u5730\u624b\u52a8\u8bbe\u8ba1\u9ad8\u8d28\u91cf\u7684\u81ea\u7136\u8bed\u8a00\u63d0\u793a\uff0c\u8fd9\u4e9b\u90fd\u6548\u7387\u4f4e\u4e0b\u4e14\u96be\u4ee5\u63a8\u5e7f\u3002", "method": "\u5f15\u5165FLAIRR-TS\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2aForecaster\u667a\u80fd\u4f53\uff08\u6839\u636e\u521d\u59cb\u63d0\u793a\u751f\u6210\u9884\u6d4b\uff09\u548c\u4e00\u4e2aRefiner\u667a\u80fd\u4f53\uff08\u6839\u636e\u5386\u53f2\u8f93\u51fa\u548c\u68c0\u7d22\u5230\u7684\u7c7b\u4f3c\u6848\u4f8b\u4f18\u5316\u63d0\u793a\uff09\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u81ea\u9002\u5e94\u63d0\u793a\u6280\u672f\u548c\u521b\u9020\u6027\u63d0\u793a\u6a21\u677f\uff0c\u65e0\u9700\u4e2d\u95f4\u4ee3\u7801\u751f\u6210\u5373\u53ef\u8de8\u9886\u57df\u751f\u6210\u9ad8\u8d28\u91cf\u9884\u6d4b\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFLAIRR-TS\u7684\u51c6\u786e\u6027\u4f18\u4e8e\u9759\u6001\u63d0\u793a\u548c\u68c0\u7d22\u589e\u5f3a\u57fa\u7ebf\uff0c\u5176\u6027\u80fd\u63a5\u8fd1\u4e8e\u4e13\u95e8\u8bbe\u8ba1\u7684\u63d0\u793a\uff08specialized prompts\uff09\u3002", "conclusion": "FLAIRR-TS\u901a\u8fc7\u5176\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u81ea\u9002\u5e94\u63d0\u793a\u4f18\u5316\u548c\u68c0\u7d22\u65b9\u6cd5\uff0c\u4e3aLLM\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5fae\u8c03\u7684\u9700\u6c42\uff0c\u5e76\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2508.19295", "pdf": "https://arxiv.org/pdf/2508.19295", "abs": "https://arxiv.org/abs/2508.19295", "authors": ["Sauptik Dhar", "Nicholas Buoncristiani", "Joe Anakata", "Haoyu Zhang", "Michelle Munson"], "title": "Large VLM-based Stylized Sports Captioning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The advent of large (visual) language models (LLM / LVLM) have led to a\ndeluge of automated human-like systems in several domains including social\nmedia content generation, search and recommendation, healthcare prognosis, AI\nassistants for cognitive tasks etc. Although these systems have been\nsuccessfully integrated in production; very little focus has been placed on\nsports, particularly accurate identification and natural language description\nof the game play. Most existing LLM/LVLMs can explain generic sports\nactivities, but lack sufficient domain-centric sports' jargon to create natural\n(human-like) descriptions. This work highlights the limitations of existing\nSoTA LLM/LVLMs for generating production-grade sports captions from images in a\ndesired stylized format, and proposes a two-level fine-tuned LVLM pipeline to\naddress that. The proposed pipeline yields an improvement > 8-10% in the F1,\nand > 2-10% in BERT score compared to alternative approaches. In addition, it\nhas a small runtime memory footprint and fast execution time. During Super Bowl\nLIX the pipeline proved its practical application for live professional sports\njournalism; generating highly accurate and stylized captions at the rate of 6\nimages per 3-5 seconds for over 1000 images during the game play.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u7ea7\u5fae\u8c03\u7684LVLM\u7ba1\u9053\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u5305\u542b\u9886\u57df\u4e13\u4e1a\u672f\u8bed\u548c\u98ce\u683c\u5316\u63cf\u8ff0\u7684\u751f\u4ea7\u7ea7\u4f53\u80b2\u8d5b\u4e8b\u56fe\u50cf\u5b57\u5e55\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u73b0\u573a\u4f53\u80b2\u65b0\u95fb\u62a5\u9053\u3002", "motivation": "\u73b0\u6709\u5927\u578b\uff08\u89c6\u89c9\uff09\u8bed\u8a00\u6a21\u578b\uff08LLM/LVLM\uff09\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u9c9c\u5c11\u5173\u6ce8\u4f53\u80b2\u9886\u57df\uff0c\u5c24\u5176\u7f3a\u4e4f\u5bf9\u6bd4\u8d5b\u7684\u51c6\u786e\u8bc6\u522b\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3002\u5927\u591a\u6570\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u751f\u6210\u5305\u542b\u8db3\u591f\u9886\u57df\u7279\u5b9a\u672f\u8bed\u7684\u81ea\u7136\uff08\u7c7b\u4eba\uff09\u63cf\u8ff0\uff0c\u4e5f\u65e0\u6cd5\u4ea7\u51fa\u6ee1\u8db3\u751f\u4ea7\u7ea7\u8981\u6c42\u7684\u98ce\u683c\u5316\u4f53\u80b2\u5b57\u5e55\u3002", "method": "\u4e3a\u89e3\u51b3\u73b0\u6709\u6700\u5148\u8fdbLLM/LVLMs\u5728\u751f\u6210\u751f\u4ea7\u7ea7\u3001\u98ce\u683c\u5316\u4f53\u80b2\u56fe\u50cf\u5b57\u5e55\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u7ea7\u5fae\u8c03\uff08two-level fine-tuned\uff09\u7684LVLM\u7ba1\u9053\u3002", "result": "\u8be5\u7ba1\u9053\u4e0e\u66ff\u4ee3\u65b9\u6cd5\u76f8\u6bd4\uff0cF1\u5206\u6570\u63d0\u5347\u4e868-10%\u4ee5\u4e0a\uff0cBERT\u5206\u6570\u63d0\u5347\u4e862-10%\u4ee5\u4e0a\u3002\u6b64\u5916\uff0c\u5b83\u5177\u6709\u5c0f\u7684\u8fd0\u884c\u65f6\u5185\u5b58\u5360\u7528\u548c\u5feb\u901f\u7684\u6267\u884c\u65f6\u95f4\u3002\u5728Super Bowl LIX\u671f\u95f4\uff0c\u8be5\u7ba1\u9053\u57283-5\u79d2\u5185\u4e3a6\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u5ea6\u51c6\u786e\u548c\u98ce\u683c\u5316\u7684\u5b57\u5e55\uff0c\u603b\u8ba1\u8d85\u8fc71000\u5f20\u56fe\u50cf\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u65f6\u4e13\u4e1a\u4f53\u80b2\u65b0\u95fb\u62a5\u9053\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e24\u7ea7\u5fae\u8c03LVLM\u7ba1\u9053\u6709\u6548\u63d0\u5347\u4e86\u4f53\u80b2\u8d5b\u4e8b\u56fe\u50cf\u5b57\u5e55\u7684\u51c6\u786e\u6027\u548c\u98ce\u683c\u5316\u6c34\u5e73\uff0c\u5e76\u4ee5\u9ad8\u6548\u80fd\u5e94\u7528\u4e8e\u73b0\u573a\u4f53\u80b2\u65b0\u95fb\u62a5\u9053\uff0c\u514b\u670d\u4e86\u73b0\u6709\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u4e0d\u8db3\u3002"}}
