<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 8]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.NI](#cs.NI) [Total: 5]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents](https://arxiv.org/abs/2505.06416)
*Elias Lumer,Anmol Gulati,Vamse Kumar Subbiah,Pradeep Honaganahalli Basavaraju,James A. Burke*

Main category: cs.CL

TL;DR: 本文提出ScaleMCP，一种新的工具选择方法，通过MCP工具检索器和自动同步工具存储系统，赋予LLM代理动态选择和使用工具的自主权，并引入TDWA嵌入策略优化工具检索。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理的工具选择框架未集成模型上下文协议 (MCP) 服务器，依赖易出错的手动更新本地工具库，导致重复、不一致和低效。此外，当前方法在代理调用前进行工具选择，限制了其自主性和动态重新查询能力。

Method: 引入ScaleMCP方法：1. 动态为LLM代理配备MCP工具检索器，使其能自主将工具添加到内存。2. 通过CRUD操作与MCP服务器（作为唯一真实来源）实现自动同步的工具存储系统。同时提出TDWA（工具文档加权平均）嵌入策略，以在嵌入过程中选择性强调工具文档的关键部分。

Result: 在包含5000个金融指标MCP服务器的自建数据集上，通过10种LLM模型、5种嵌入模型和5种检索器类型进行的全面评估表明，ScaleMCP在工具检索和代理调用性能方面取得了显著改进。

Conclusion: ScaleMCP在可扩展、动态的工具选择和调用方面表现出有效性，显著提升了LLM代理与外部工具交互的性能。

Abstract: Recent advancements in Large Language Models (LLMs) and the introduction of
the Model Context Protocol (MCP) have significantly expanded LLM agents'
capability to interact dynamically with external tools and APIs. However,
existing tool selection frameworks do not integrate MCP servers, instead
relying heavily on error-prone manual updates to monolithic local tool
repositories, leading to duplication, inconsistencies, and inefficiencies.
Additionally, current approaches abstract tool selection before the LLM agent
is invoked, limiting its autonomy and hindering dynamic re-querying
capabilities during multi-turn interactions. To address these issues, we
introduce ScaleMCP, a novel tool selection approach that dynamically equips LLM
agents with a MCP tool retriever, giving agents the autonomy to add tools into
their memory, as well as an auto-synchronizing tool storage system pipeline
through CRUD (create, read, update, delete) operations with MCP servers as the
single source of truth. We also propose a novel embedding strategy, Tool
Document Weighted Average (TDWA), designed to selectively emphasize critical
components of tool documents (e.g. tool name or synthetic questions) during the
embedding process. Comprehensive evaluations conducted on a created dataset of
5,000 financial metric MCP servers, across 10 LLM models, 5 embedding models,
and 5 retriever types, demonstrate substantial improvements in tool retrieval
and agent invocation performance, emphasizing ScaleMCP's effectiveness in
scalable, dynamic tool selection and invocation.

</details>


### [2] [Is your multimodal large language model a good science tutor?](https://arxiv.org/abs/2505.06418)
*Ming Liu,Liwen Wang,Wensheng Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种评估和改进多模态大语言模型 (MLLM) 作为科学辅导教师的方法，重点关注其教学能力而非仅仅答案的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有对 MLLM 的基准测试大多只关注最终答案的准确性，忽略了其他指标，尤其是在教育场景中，模型的教学能力至关重要。

Method: 1. 提出一个评估框架，使用综合教育评估标准和模拟学生模型来判断 MLLM 作为科学辅导教师的教学表现。2. 基于学生判断对候选 MLLM 辅导教师进行评分，区分出强弱辅导教师。3. 利用 ScienceQA 训练集构建强弱辅导教师输出的成对比较数据集。4. 应用多种偏好优化方法对表现欠佳的辅导模型 (Qwen2-VL-2B) 进行微调，以提升其教学效果。

Result: 研究结果表明，强大的问题解决能力并不等同于高质量的辅导能力。通过偏好优化引导的改进可以产生更符合教育需求的辅导模型，有效提升了欠佳模型的教学表现。

Conclusion: 该方法为构建不仅能解决问题，还能成为真正有用的教育助手的 MLLM 开辟了新途径。

Abstract: Multimodal large language models (MLLMs) demonstrate impressive performance
on scientific reasoning tasks (e.g., ScienceQA). However, most existing
benchmarks focus narrowly on the accuracy of the final answer while ignoring
other metrics. In particular, when applying MLLMs to educational contexts, the
goal is not only correctness but also the ability to teach. In this paper, we
propose a framework that evaluates MLLMs as science tutors using a
comprehensive educational rubric and a simulated student model that judges the
teaching performance of the tutors. Given a list of candidate MLLM science
tutors, we use rubric-based student judgments to produce a range of tutor
performance scores, identifying both strong and weak tutors. Using the training
section of the ScienceQA dataset, we then construct a data set of pairwise
comparisons between the outputs of strong and weak tutors. This enables us to
apply multiple preference optimization methods to fine-tune an underperforming
tutor model (Qwen2-VL-2B) into more effective ones. Our results also show that
strong problem-solving skills do not guarantee high-quality tutoring and that
performance optimization-guided refinements can yield more educationally
aligned tutor models. This approach opens avenues for building MLLMs that serve
not only as problem solvers, but as genuinely helpful educational assistants.

</details>


### [3] [xGen-small Technical Report](https://arxiv.org/abs/2505.06496)
*Erik Nijkamp,Bo Pang,Egor Pakhomov,Akash Gokul,Jin Qu,Silvio Savarese,Yingbo Zhou,Caiming Xiong*

Main category: cs.CL

TL;DR: 介绍xGen-small，一个为长上下文应用优化的4B和9B参数Transformer解码器模型系列。


<details>
  <summary>Details</summary>
Motivation: 开发在长上下文应用中表现更优的Transformer模型。

Method: 采用垂直整合的流程，包括：1) 构建4B和9B参数的Transformer解码器模型；2) 领域平衡、频率感知的数据管理；3) 多阶段预训练，含质量退火并将上下文长度扩展至128k token；4) 目标导向的后训练，包括有监督微调、偏好学习和在线强化学习。

Result: xGen-small在多种任务（尤其是在数学和编码领域）上展现出强大的性能，并在长上下文基准测试中表现优异。

Conclusion: xGen-small是一系列成功的长上下文Transformer模型，通过其综合的训练流程，在多个领域（特别是数学和编码）及长上下文处理方面取得了显著成果。

Abstract: We introduce xGen-small, a family of 4B and 9B Transformer decoder models
optimized for long-context applications. Our vertically integrated pipeline
unites domain-balanced, frequency-aware data curation; multi-stage pre-training
with quality annealing and length extension to 128k tokens; and targeted
post-training via supervised fine-tuning, preference learning, and online
reinforcement learning. xGen-small delivers strong performance across various
tasks, especially in math and coding domains, while excelling at long context
benchmarks.

</details>


### [4] [Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model](https://arxiv.org/abs/2505.06538)
*Xinyue Lou,You Li,Jinan Xu,Xiangyu Shi,Chi Chen,Kaiyu Huang*

Main category: cs.CL

TL;DR: 本文评估了多模态大推理模型 (MLRMs) 的安全性，发现其存在安全退化现象，并提出通过构建包含安全导向思维过程的多模态微调数据集来增强模型安全性，实验证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态大推理模型 (MLRMs) 发展迅速且应用潜力巨大，但其安全性和可靠性问题尚未得到系统性探索，是亟待解决的关键问题。

Method: 1. 对11个MLRMs在5个基准上进行全面的安全性评估。 2. 发现模型内在的推理能力（如更长的思考过程）有助于检测不安全意图。 3. 构建了一个包含安全导向思维过程的多模态微调数据集。 4. 使用该数据集对现有MLRMs进行微调。

Result: 多数先进的MLRMs表现出普遍的安全退化现象，尤其在越狱鲁棒性基准上退化显著，而在安全意识基准上退化较轻。在某些场景下，较长的思考过程甚至能提升安全性能。利用新构建的数据集微调MLRMs后，其在越狱鲁棒性和安全意识基准上的安全性均得到有效增强。

Conclusion: 该研究为开发安全的MLRMs提供了一个新视角，即利用模型内在的推理能力来增强其安全性。研究构建的安全导向思维过程微调数据集证明了此方法的可行性。

Abstract: The rapid development of multimodal large reasoning models (MLRMs) has
demonstrated broad application potential, yet their safety and reliability
remain critical concerns that require systematic exploration. To address this
gap, we conduct a comprehensive and systematic safety evaluation of 11 MLRMs
across 5 benchmarks and unveil prevalent safety degradation phenomena in most
advanced models. Moreover, our analysis reveals distinct safety patterns across
different benchmarks: significant safety degradation is observed across
jailbreak robustness benchmarks, whereas safety-awareness benchmarks
demonstrate less pronounced degradation. In particular, a long thought process
in some scenarios even enhances safety performance. Therefore, it is a
potential approach to addressing safety issues in MLRMs by leveraging the
intrinsic reasoning capabilities of the model to detect unsafe intent. To
operationalize this insight, we construct a multimodal tuning dataset that
incorporates a safety-oriented thought process. Experimental results from
fine-tuning existing MLRMs with this dataset effectively enhances the safety on
both jailbreak robustness and safety-awareness benchmarks. This study provides
a new perspective for developing safe MLRMs. Our dataset is available at
https://github.com/xinyuelou/Think-in-Safety.

</details>


### [5] [REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback](https://arxiv.org/abs/2505.06548)
*Aniruddha Roy,Pretam Ray,Abhilash Nandy,Somak Aditya,Pawan Goyal*

Main category: cs.CL

TL;DR: 本文探讨了使用小型开源大语言模型（LLaMA 2, Mistral 7B）结合半自动化框架和强化学习来生成指令微调数据集，以降低成本和人力。


<details>
  <summary>Details</summary>
Motivation: 人工标注指令数据成本高昂、耗时，且在数量和任务多样性上常受限制。以往研究尝试通过模型自身半自动生成指令，但多依赖昂贵且有查询限制的大型API模型（如GPT-3.5）。本研究旨在探索使用小型开源模型降低生成指令数据集的人力、精力和成本。

Method: 研究使用了三种小型开源大语言模型（LLaMA 2-7B, LLaMA 2-13B, Mistral 7B），并采用一个半自动化框架来生成指令数据集。此外，将基于强化学习（RL）的训练算法整合到这个基于大语言模型的框架中。

Result: 对数据集的评估显示，与先前方法相比，这些基于强化学习的框架在63-66%的任务中取得了显著改进。

Conclusion: 使用小型开源大语言模型，结合半自动化框架及强化学习算法，可以有效生成用于微调的指令数据集，减少了人工干预、精力和成本，并在多数任务上优于以往方法。

Abstract: Instruction-based Large Language Models (LLMs) have proven effective in
numerous few-shot or zero-shot Natural Language Processing (NLP) tasks.
However, creating human-annotated instruction data is time-consuming,
expensive, and often limited in quantity and task diversity. Previous research
endeavors have attempted to address this challenge by proposing frameworks
capable of generating instructions in a semi-automated and task-agnostic manner
directly from the model itself. Many of these efforts have relied on large
API-only parameter-based models such as GPT-3.5 (175B), which are expensive,
and subject to limits on a number of queries. This paper explores the
performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B,
and Mistral 7B, using a semi-automated framework, thereby reducing human
intervention, effort, and cost required to generate an instruction dataset for
fine-tuning LLMs. Furthermore, we demonstrate that incorporating a
Reinforcement Learning (RL) based training algorithm into this LLMs-based
framework leads to further enhancements. Our evaluation of the dataset reveals
that these RL-based frameworks achieve a substantial improvements in 63-66% of
the tasks compared to previous approaches.

</details>


### [6] [References Indeed Matter? Reference-Free Preference Optimization for Conversational Query Reformulation](https://arxiv.org/abs/2505.06552)
*Doyoung Kim,Youngjun Lee,Joeun Kim,Jihwan Bang,Hwanjun Song,Susik Yoon,Jae-Gil Lee*

Main category: cs.CL

TL;DR: 本文提出了一种名为DualReform的无参考对话式查询重构（CQR）框架，它通过从仅包含查询和响应的对话数据中生成伪参考段落来优化CQR模型，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的对话式查询重构（CQR）方法通常依赖参考段落进行优化，但在实际场景中获取这些参考段落不切实际，这限制了CQR技术的应用。

Method: 引入了DualReform，一个无需参考段落的偏好优化框架。它通过两个关键创新实现目标：（1）基于响应的推断，利用对话中的响应作为代理来推断伪参考段落；（2）通过CQR的双重角色进行响应优化，即利用CQR模型根据响应优化和CQR之间的共同目标来优化响应，从而生成高质量的伪参考。

Result: 尽管不依赖真实的参考段落，DualReform方法达到了使用参考段落才能实现的检索准确率的96.9%至99.1%，并且其性能超越了当前最先进的方法高达31.6%。

Conclusion: DualReform为CQR提供了一种有效的无参考优化方案，解决了现有方法对参考段落的依赖问题，并显著提升了性能，具有很强的实际应用价值。

Abstract: Conversational query reformulation (CQR) has become indispensable for
improving retrieval in dialogue-based applications. However, existing
approaches typically rely on reference passages for optimization, which are
impractical to acquire in real-world scenarios. To address this limitation, we
introduce a novel reference-free preference optimization framework DualReform
that generates pseudo reference passages from commonly-encountered
conversational datasets containing only queries and responses. DualReform
attains this goal through two key innovations: (1) response-based inference,
where responses serve as proxies to infer pseudo reference passages, and (2)
response refinement via the dual-role of CQR, where a CQR model refines
responses based on the shared objectives between response refinement and CQR.
Despite not relying on reference passages, DualReform achieves 96.9--99.1% of
the retrieval accuracy attainable only with reference passages and surpasses
the state-of-the-art method by up to 31.6%.

</details>


### [7] [MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG](https://arxiv.org/abs/2505.06569)
*Woosang Lim,Zekun Li,Gyuwan Kim,Sungyoung Ji,HyeonJung Kim,Kyuri Choi,Jin Hyuk Lim,Kyungpyo Park,William Yang Wang*

Main category: cs.CL

TL;DR: 提出了一种名为 MacRAG 的多尺度自适应上下文 RAG 框架，通过分层检索和自适应上下文合并，优化长上下文和多跳推理任务。


<details>
  <summary>Details</summary>
Motivation: 现有的 RAG 系统在处理复杂多跳和大型文档任务时，常面临检索不精确、上下文覆盖不完整以及因次优上下文构建导致的信息碎片化问题。

Method: 引入 MacRAG，一个分层检索框架。该框架将文档压缩并划分为从粗到细的粒度，然后通过实时的块级和文档级扩展，从最细粒度的检索开始，逐步合并更高级别和更广泛的上下文，自适应地构建与查询相关的有效长上下文。

Result: 在 HotpotQA、2WikiMultihopQA 和 Musique 的 LongBench 扩展数据集上，使用 Llama-3.1-8B、Gemini-1.5-pro 和 GPT-4o 进行评估，MacRAG 在单步和多步生成任务上均持续优于基线 RAG 流水线。

Conclusion: MacRAG 是一种高效且可扩展的解决方案，适用于真实世界的长上下文、多跳推理任务。

Abstract: Long-context (LC) Large Language Models (LLMs) combined with
Retrieval-Augmented Generation (RAG) hold strong potential for complex
multi-hop and large-document tasks. However, existing RAG systems often suffer
from imprecise retrieval, incomplete context coverage under constrained context
windows, and fragmented information caused by suboptimal context construction.
We introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical
retrieval framework that compresses and partitions documents into
coarse-to-fine granularities, then adaptively merges relevant contexts through
chunk- and document-level expansions in real time. By starting from the
finest-level retrieval and progressively incorporating higher-level and broader
context, MacRAG constructs effective query-specific long contexts, optimizing
both precision and coverage. Evaluations on the challenging LongBench
expansions of HotpotQA, 2WikiMultihopQA, and Musique confirm that MacRAG
consistently surpasses baseline RAG pipelines on single- and multi-step
generation with Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish
MacRAG as an efficient, scalable solution for real-world long-context,
multi-hop reasoning. Our code is available at
https://github.com/Leezekun/MacRAG.

</details>


### [8] [Evaluating LLM-Generated Q&A Test: a Student-Centered Study](https://arxiv.org/abs/2505.06591)
*Anna Wróblewska,Bartosz Grabek,Jakub Świstak,Daniel Dan*

Main category: cs.CL

TL;DR: 本研究利用AI聊天机器人（GPT-4o-mini）自动生成问答测试，并通过评估证明其具有良好的心理测量学特性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 探索使用AI自动生成高质量问答测试的可行性，以期实现可扩展的AI辅助测评开发。

Method: 构建了基于GPT-4o-mini的自动问答测试生成流程，并对生成的自然语言处理课程测试进行了心理测量学（IRT分析、DIF检查）和感知质量（学生与专家评分）评估。

Result: 生成的测试项目展现出良好的区分度和适当难度。学生与专家评分均较高，表明测试整体质量高，在心理测量性能和用户满意度方面可与人工测试媲美。DIF检查识别出少量需审查项目。

Conclusion: 大型语言模型生成的评估在质量和用户满意度上可与人工测试相当，为AI辅助评估开发提供了可扩展的有效途径。

Abstract: This research prepares an automatic pipeline for generating reliable
question-answer (Q&A) tests using AI chatbots. We automatically generated a
GPT-4o-mini-based Q&A test for a Natural Language Processing course and
evaluated its psychometric and perceived-quality metrics with students and
experts. A mixed-format IRT analysis showed that the generated items exhibit
strong discrimination and appropriate difficulty, while student and expert star
ratings reflect high overall quality. A uniform DIF check identified two items
for review. These findings demonstrate that LLM-generated assessments can match
human-authored tests in psychometric performance and user satisfaction,
illustrating a scalable approach to AI-assisted assessment development.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [9] [Understanding and Mitigating Toxicity in Image-Text Pretraining Datasets: A Case Study on LLaVA](https://arxiv.org/abs/2505.06356)
*Karthik Reddy Kanjula,Surya Guthikonda,Nahid Alam,Shayekh Bin Islam*

Main category: cs.CV

TL;DR: 本文研究了LLaVA图文预训练数据集中的毒性内容，分析了其表现形式，提出了缓解策略，并创建了一个移除了7531对有毒图文对的优化数据集。


<details>
  <summary>Details</summary>
Motivation: 多模态模型的预训练数据集来源于网络，常含有固有的偏见和有毒内容，影响模型的健康发展。

Method: 调查LLaVA数据集中毒性的普遍性及其在不同模态的表现，分析常见毒性类别，提出针对性缓解策略，并创建优化数据集，同时提供毒性检测流程指南。

Result: 成功创建了一个毒性缓解的数据集，从LLaVA预训练数据集中移除了7,531对有毒的图文对。该优化数据集已开源。

Conclusion: 研究强调了主动识别和过滤如仇恨言论、露骨图像和定向骚扰等有毒内容的必要性，以构建更负责任和公平的多模态系统。

Abstract: Pretraining datasets are foundational to the development of multimodal
models, yet they often have inherent biases and toxic content from the
web-scale corpora they are sourced from. In this paper, we investigate the
prevalence of toxicity in LLaVA image-text pretraining dataset, examining how
harmful content manifests in different modalities. We present a comprehensive
analysis of common toxicity categories and propose targeted mitigation
strategies, resulting in the creation of a refined toxicity-mitigated dataset.
This dataset removes 7,531 of toxic image-text pairs in the LLaVA pre-training
dataset. We offer guidelines for implementing robust toxicity detection
pipelines. Our findings underscore the need to actively identify and filter
toxic content - such as hate speech, explicit imagery, and targeted harassment
- to build more responsible and equitable multimodal systems. The
toxicity-mitigated dataset is open source and is available for further
research.

</details>


### [10] [Robust & Precise Knowledge Distillation-based Novel Context-Aware Predictor for Disease Detection in Brain and Gastrointestinal](https://arxiv.org/abs/2505.06381)
*Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel*

Main category: cs.CV

TL;DR: 提出了一种结合蚁群优化（ACO）进行模型选择和上下文感知温度缩放的新框架，用于改进医学图像疾病预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像疾病预测方法，特别是知识蒸馏（KD）方法，在处理医学数据的复杂性、不确定性以及适应不同不确定性水平方面存在局限性。

Method: 采用蚁群优化（ACO）算法选择最优的师生模型对，并引入一种上下文感知的预测器方法，根据图像质量、疾病复杂度和教师模型置信度等因素动态调整知识蒸馏中的温度参数。

Result: 该框架在三个公开基准数据集上（Kaggle MRI脑肿瘤、Figshare MRI、GastroNet）的准确率分别达到98.01%、92.81%和96.20%，均显著优于现有技术水平。

Conclusion: 所提出的集成蚁群优化和上下文感知温度缩放的框架，能有效提升医学图像疾病预测的性能，特别是在处理不确定性和进行知识蒸馏方面表现出优越性。

Abstract: Medical disease prediction, particularly through imaging, remains a
challenging task due to the complexity and variability of medical data,
including noise, ambiguity, and differing image quality. Recent deep learning
models, including Knowledge Distillation (KD) methods, have shown promising
results in brain tumor image identification but still face limitations in
handling uncertainty and generalizing across diverse medical conditions.
Traditional KD methods often rely on a context-unaware temperature parameter to
soften teacher model predictions, which does not adapt effectively to varying
uncertainty levels present in medical images. To address this issue, we propose
a novel framework that integrates Ant Colony Optimization (ACO) for optimal
teacher-student model selection and a novel context-aware predictor approach
for temperature scaling. The proposed context-aware framework adjusts the
temperature based on factors such as image quality, disease complexity, and
teacher model confidence, allowing for more robust knowledge transfer.
Additionally, ACO efficiently selects the most appropriate teacher-student
model pair from a set of pre-trained models, outperforming current optimization
methods by exploring a broader solution space and better handling complex,
non-linear relationships within the data. The proposed framework is evaluated
using three publicly available benchmark datasets, each corresponding to a
distinct medical imaging task. The results demonstrate that the proposed
framework significantly outperforms current state-of-the-art methods, achieving
top accuracy rates: 98.01% on the MRI brain tumor (Kaggle) dataset, 92.81% on
the Figshare MRI dataset, and 96.20% on the GastroNet dataset. This enhanced
performance is further evidenced by the improved results, surpassing existing
benchmarks of 97.24% (Kaggle), 91.43% (Figshare), and 95.00% (GastroNet).

</details>


### [11] [Deep Learning-Based Robust Optical Guidance for Hypersonic Platforms](https://arxiv.org/abs/2505.06389)
*Adrien Chan-Hon-Tong,Aurélien Plyer,Baptiste Cadalen,Laurent Serre*

Main category: cs.CV

TL;DR: 提出了一种将图像堆栈编码到深度网络中的方法，用于传感器引导，以克服经典配准的局限性，尤其适用于双模态场景（如雪天/非雪天）。


<details>
  <summary>Details</summary>
Motivation: 远程平台需要基于传感器的引导，而经典的基于参考图像的配准方法存在结构性限制，特别是在场景外观变化大（如双模态）时。

Method: 将场景的一系列图像（图像堆栈）编码到一个深度网络中，用于传感器引导。

Result: 研究表明，在双模态场景（例如，场景中可能存在积雪或无积雪的情况）下，依赖图像堆栈进行编码是有效的。

Conclusion: 通过将图像堆栈编码到深度网络中，可以有效处理双模态场景下的传感器引导问题，并克服了经典配准方法的局限性。

Abstract: Sensor-based guidance is required for long-range platforms. To bypass the
structural limitation of classical registration on reference image framework,
we offer in this paper to encode a stack of images of the scene into a deep
network. Relying on a stack is showed to be relevant on bimodal scene (e.g.
when the scene can or can not be snowy).

</details>


### [12] [Toward Advancing License Plate Super-Resolution in Real-World Scenarios: A Dataset and Benchmark](https://arxiv.org/abs/2505.06393)
*Valfride Nascimento,Gabriel E. Lima,Rafael O. Ribeiro,William Robson Schwartz,Rayson Laroca,David Menotti*

Main category: cs.CV

TL;DR: 该研究引入了一个新的真实场景车牌超分辨率数据集UFPR-SR-Plates，并证明超分辨率技术结合多帧OCR结果融合能显著提升恶劣条件下低分辨率车牌的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有车牌超分辨率研究依赖私有数据集和简化的图像退化模型，难以解决真实世界监控、交通监测和取证应用中低分辨率（LR）和退化图像带来的挑战。

Method: 1. 引入了一个包含10万对真实场景下低分辨率和高分辨率车牌图像的新数据集UFPR-SR-Plates。
2. 使用每辆车多个连续的低分辨率和高分辨率图像（各五个）以及两种先进的车牌超分辨率模型建立基准。
3. 研究了三种融合策略，以评估结合来自领先OCR模型对多个超分辨率车牌的预测如何增强整体性能。

Result: 研究发现超分辨率能显著提升车牌识别（LPR）性能，应用基于多数投票的融合技术后性能进一步提升。具体而言，Layout-Aware and Character-Driven Network (LCDNet) 模型结合字符位置多数投票 (MVCP) 策略取得了最高的识别率，从低分辨率图像的1.7%提升至超分辨率图像的31.1%，在结合五个超分辨率图像的OCR输出后，识别率高达44.7%。

Conclusion: 超分辨率技术和时间信息（多帧融合）对于在真实世界恶劣条件下提高车牌识别准确性至关重要。提出的数据集已公开以支持进一步研究。

Abstract: Recent advancements in super-resolution for License Plate Recognition (LPR)
have sought to address challenges posed by low-resolution (LR) and degraded
images in surveillance, traffic monitoring, and forensic applications. However,
existing studies have relied on private datasets and simplistic degradation
models. To address this gap, we introduce UFPR-SR-Plates, a novel dataset
containing 10,000 tracks with 100,000 paired low and high-resolution license
plate images captured under real-world conditions. We establish a benchmark
using multiple sequential LR and high-resolution (HR) images per vehicle --
five of each -- and two state-of-the-art models for super-resolution of license
plates. We also investigate three fusion strategies to evaluate how combining
predictions from a leading Optical Character Recognition (OCR) model for
multiple super-resolved license plates enhances overall performance. Our
findings demonstrate that super-resolution significantly boosts LPR
performance, with further improvements observed when applying majority
vote-based fusion techniques. Specifically, the Layout-Aware and
Character-Driven Network (LCDNet) model combined with the Majority Vote by
Character Position (MVCP) strategy led to the highest recognition rates,
increasing from 1.7% with low-resolution images to 31.1% with super-resolution,
and up to 44.7% when combining OCR outputs from five super-resolved images.
These findings underscore the critical role of super-resolution and temporal
information in enhancing LPR accuracy under real-world, adverse conditions. The
proposed dataset is publicly available to support further research and can be
accessed at: https://valfride.github.io/nascimento2024toward/

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [13] [BedreFlyt: Improving Patient Flows through Hospital Wards with Digital Twins](https://arxiv.org/abs/2505.06287)
*Riccardo Sieve,Paul Kobialka,Laura Slaughter,Rudolf Schlatte,Einar Broch Johnsen,Silvia Lizeth Tapia Tarifa*

Main category: cs.AI

TL;DR: 该论文介绍了一种利用数字孪生技术优化医院住院病房资源规划的方法，通过结合可执行形式化模型、本体论和SMT求解器来支持短期决策和长期战略规划。


<details>
  <summary>Details</summary>
Motivation: 数字孪生在多个领域（包括医疗保健）的短期决策和长期战略规划中显示出价值，特别是在优化医院资源（如住院病房需求）方面存在提升空间。

Method: 采用可执行形式化模型进行系统探索并将患者流转化为优化问题，利用本体进行知识表示和配置数字孪生，并使用SMT（可满足性模理论）求解器解决约束满足问题，以探索“假设”情景。

Result: 论文提出了一个数字孪生架构，该架构能将到达的患者流转化为一系列优化问题（如每日病房需求），并通过SMT技术解决。知识库用于对数字孪生进行配置，使其能够生成覆盖平均和最坏情况资源需求的场景，并考虑可用资源的变化。通过医院病房床位分配问题展示了该架构。

Conclusion: 所提出的数字孪生解决方案，通过整合可执行形式化模型、本体和SMT求解器，为医院资源规划提供了一种有前景的方法，能够支持短期决策和通过生成不同场景来改进长期战略规划。

Abstract: Digital twins are emerging as a valuable tool for short-term decision-making
as well as for long-term strategic planning across numerous domains, including
process industry, energy, space, transport, and healthcare. This paper reports
on our ongoing work on designing a digital twin to enhance resource planning,
e.g., for the in-patient ward needs in hospitals. By leveraging executable
formal models for system exploration, ontologies for knowledge representation
and an SMT solver for constraint satisfiability, our approach aims to explore
hypothetical "what-if" scenarios to improve strategic planning processes, as
well as to solve concrete, short-term decision-making tasks. Our proposed
solution uses the executable formal model to turn a stream of arriving
patients, that need to be hospitalized, into a stream of optimization problems,
e.g., capturing daily inpatient ward needs, that can be solved by SMT
techniques. The knowledge base, which formalizes domain knowledge, is used to
model the needed configuration in the digital twin, allowing the twin to
support both short-term decision-making and long-term strategic planning by
generating scenarios spanning average-case as well as worst-case resource
needs, depending on the expected treatment of patients, as well as ranging over
variations in available resources, e.g., bed distribution in different rooms.
We illustrate our digital twin architecture by considering the problem of bed
bay allocation in a hospital ward.

</details>


### [14] [A Grounded Memory System For Smart Personal Assistants](https://arxiv.org/abs/2505.06328)
*Felix Ocker,Jörg Deigmöller,Pavel Smirnov,Julian Eggert*

Main category: cs.AI

TL;DR: 论文提出了一种包含三个组件的、基于现实的代理AI鲁棒记忆系统。


<details>
  <summary>Details</summary>
Motivation: 各种代理AI应用（如痴呆症患者的认知助手、机器人技术）需要一个强大的、基于现实的记忆系统。

Method: 该系统包含三个组件：1. 结合视觉语言模型（图像字幕、实体消歧）和大型语言模型（一致性信息提取）进行感知。2. 将提取的信息表示在由向量嵌入增强的知识图谱中，以有效管理关系信息。3. 通过检索增强生成（RAG）结合语义搜索和图查询生成进行问答。

Result: 通过一个真实世界的例子展示了该系统的工作方式和潜力。

Conclusion: 所提出的三组件记忆系统为代理AI提供了强大的、基于现实的记忆能力，并展示了其潜力和有效性。

Abstract: A wide variety of agentic AI applications - ranging from cognitive assistants
for dementia patients to robotics - demand a robust memory system grounded in
reality. In this paper, we propose such a memory system consisting of three
components. First, we combine Vision Language Models for image captioning and
entity disambiguation with Large Language Models for consistent information
extraction during perception. Second, the extracted information is represented
in a memory consisting of a knowledge graph enhanced by vector embeddings to
efficiently manage relational information. Third, we combine semantic search
and graph query generation for question answering via Retrieval Augmented
Generation. We illustrate the system's working and potential using a real-world
example.

</details>


### [15] [Reliable Collaborative Conversational Agent System Based on LLMs and Answer Set Programming](https://arxiv.org/abs/2505.06438)
*Yankai Zeng,Gopal Gupta*

Main category: cs.AI

TL;DR: 提出了一种基于答案集编程（ASP）的管理员-助手双智能体范式，用于构建可靠的任务导向对话系统，并通过一个快餐店得来速管理系统验证了其相较于现有LLM驱动机器人的优越性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）驱动的AI机器人在任务导向对话中存在知识不可靠、任务完成无法保证的问题，且智能体间的协作面临信息传递不明确、不可靠以及易受恶意知识注入的挑战。

Method: 提出了一种管理员-助手双智能体范式。两个基于答案集编程（ASP）的机器人共享同一知识库，独立完成任务，并通过协作规则集（CRS）传递信息。知识和信息被封装且对用户不可见，以确保安全。构建了名为AutoManager的系统用于快餐店得来速管理，其中助手机器人负责点餐，管理员机器人负责管理菜单和食物供应。

Result: 对AutoManager系统进行了评估，并与真实的Taco Bell得来速AI点餐系统进行了比较，结果表明所提出的方法（AutoManager）更为可靠。

Conclusion: 使用ASP驱动的双智能体系统和协作规则集，可以构建更安全、可靠的任务导向对话代理，并使代理间通信更高效、安全。

Abstract: As the Large-Language-Model-driven (LLM-driven) Artificial Intelligence (AI)
bots became popular, people realized their strong potential in Task-Oriented
Dialogue (TOD). However, bots relying wholly on LLMs are unreliable in their
knowledge, and whether they can finally produce a correct result for the task
is not guaranteed. The collaboration among these agents also remains a
challenge, since the necessary information to convey is unclear, and the
information transfer is by prompts -- unreliable, and malicious knowledge is
easy to inject. With the help of logic programming tools such as Answer Set
Programming (ASP), conversational agents can be built safely and reliably, and
communication among the agents made more efficient and secure. We proposed an
Administrator-Assistant Dual-Agent paradigm, where the two ASP-driven bots
share the same knowledge base and complete their tasks independently, while the
information can be passed by a Collaborative Rule Set (CRS). The knowledge and
information conveyed are encapsulated and invisible to the users, ensuring the
security of information transmission. We have constructed AutoManager, a
dual-agent system for managing the drive-through window of a fast-food
restaurant such as Taco Bell in the US. In AutoManager, the assistant bot takes
the customer's order while the administrator bot manages the menu and food
supply. We evaluated our AutoManager and compared it with the real-world Taco
Bell Drive-Thru AI Order Taker, and the results show that our method is more
reliable.

</details>


### [16] [Opening the Scope of Openness in AI](https://arxiv.org/abs/2505.06464)
*Tamara Paris,AJung Moon,Jin Guo*

Main category: cs.AI

TL;DR: 该研究认为当前AI开放性受开源软件启发，但其实践和益处无法完全移植到AI领域。通过分析跨学科的98个开放性概念，构建了一个开放性分类法，以更全面地定义AI开放性，并识别当前讨论中的差距。


<details>
  <summary>Details</summary>
Motivation: 当前AI开放性的概念主要借鉴自开源软件，但这一定义无法完全适应AI自身的挑战及其日益增长的社会影响、风险和能力。因此，需要一个专门为AI量身定制的开放性框架。

Method: 通过主题建模发现了98个开放性概念，并对这些概念进行了定性分析，从而开发出一个开放性分类法。该分类法被用作分析工具，以审视当前关于AI开放性的讨论，识别现有差距，并强调与其他学科的联系。

Result: 研究成功开发了一个开放性分类法。利用该分类法，研究将当前AI开放性的讨论置于更广泛的学科背景下，识别了现有讨论中的不足之处，并突出了AI开放性与其他学科开放性概念之间的联系。

Conclusion: 该工作通过反思超越开源软件的开放性原则和实践，为构建AI开放性框架做出了贡献，并呼吁从行动、系统属性和伦理目标等多个维度对AI开放性采取更全面的视角。

Abstract: The concept of openness in AI has so far been heavily inspired by the
definition and community practice of open source software. This positions
openness in AI as having positive connotations; it introduces assumptions of
certain advantages, such as collaborative innovation and transparency. However,
the practices and benefits of open source software are not fully transferable
to AI, which has its own challenges. Framing a notion of openness tailored to
AI is crucial to addressing its growing societal implications, risks, and
capabilities. We argue that considering the fundamental scope of openness in
different disciplines will broaden discussions, introduce important
perspectives, and reflect on what openness in AI should mean. Toward this goal,
we qualitatively analyze 98 concepts of openness discovered from topic
modeling, through which we develop a taxonomy of openness. Using this taxonomy
as an instrument, we situate the current discussion on AI openness, identify
gaps and highlight links with other disciplines. Our work contributes to the
recent efforts in framing openness in AI by reflecting principles and practices
of openness beyond open source software and calls for a more holistic view of
openness in terms of actions, system properties, and ethical objectives.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [Neural Network Operator-Based Fractal Approximation: Smoothness Preservation and Convergence Analysis](https://arxiv.org/abs/2505.06229)
*Aaqib Ayoub Bhat,Asif Khan,M. Mursaleen*

Main category: cs.LG

TL;DR: 论文提出了一种利用神经网络算子构造α-分形插值函数的新方法，该方法仅需节点处的函数值，能保持原函数的光滑性，并分析了其收敛性和逼近误差。


<details>
  <summary>Details</summary>
Motivation: 克服传统分形插值方法需要依赖整个原始函数的限制，并探索利用神经网络算子生成具有特定性质（如光滑性保持）的分形插值函数。

Method: 1. 采用神经网络算子（包括四层神经网络算子）构造α-分形插值函数。 2. 所构造的分形插值函数仅利用原始函数在节点或分割点处的值。 3. 运用逼近论工具（如连续模和插值算子）分析收敛性和推导一致逼近误差界。

Result: 1. 成功构建了基于神经网络算子的α-分形插值函数。 2. 实现了仅使用节点函数值的分形插值。 3. 在特定约束下，通过四层神经网络算子构建的α-分形能够保持原函数的光滑性（若原函数为Cr类，则分形函数也为Cr类）。 4. 分析并获得了这些α-分形在适当条件下向原始函数收敛的结果以及一致逼近误差界。

Conclusion: 本文提出了一种基于神经网络算子的创新α-分形插值函数构造方法，该方法在减少对原始函数信息依赖、保持光滑性方面具有优势，并提供了收敛性分析和误差界，为分形插值领域提供了新的视角和工具。

Abstract: This paper presents a new approach of constructing $\alpha$-fractal
interpolation functions (FIFs) using neural network operators, integrating
concepts from approximation theory. Initially, we construct $\alpha$-fractals
utilizing neural network-based operators, providing an approach to generating
fractal functions with interpolation properties. Based on the same foundation,
we have developed fractal interpolation functions that utilize only the values
of the original function at the nodes or partition points, unlike traditional
methods that rely on the entire original function.
  Further, we have constructed \(\alpha\)-fractals that preserve the smoothness
of functions under certain constraints by employing a four-layered neural
network operator, ensuring that if \(f \in C^{r}[a,b]\), then the corresponding
fractal \(f^{\alpha} \in C^{r}[a,b]\). Furthermore, we analyze the convergence
of these $\alpha$-fractals to the original function under suitable conditions.
The work uses key approximation theory tools, such as the modulus of continuity
and interpolation operators, to develop convergence results and uniform
approximation error bounds.

</details>


### [18] [Beyond Attention: Toward Machines with Intrinsic Higher Mental States](https://arxiv.org/abs/2505.06257)
*Ahsan Adeel*

Main category: cs.LG

TL;DR: 该研究提出了一种受哺乳动物大脑启发的机制，使 Transformer 等模型能在应用注意力之前预先选择相关信息，从而提高学习效率和降低计算需求。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制中确定相关性是一个核心挑战，通常依赖于反向传播等学习算法，计算成本高。该研究受到新皮层锥体细胞与不同精神状态（如高级感知处理和想象）相关的神经生物学证据的启发。

Method: 模型通过模拟高级感知处理和清醒思维（想象）状态来预选相关信息。这通过问题（Q）、线索（K）和假设（V）之间的三元神经元级调制环路实现，从而在表征层面形成多样化、深度的并行推理链。

Result: 学习速度提升了几个数量级，计算需求显著降低（例如，更少的注意力头、层数和标记），计算成本近似为 O(N)。该方法在强化学习（如高维视觉环境下的赛车游戏）、计算机视觉和自然语言问答等领域均取得了成果。

Conclusion: 通过模拟大脑中预先选择相关信息的过程，可以显著提升机器学习模型（如 Transformer）的学习效率和性能，同时大幅降低计算开销。

Abstract: Attending to what is relevant is fundamental to both the mammalian brain and
modern machine learning models such as Transformers. Yet, determining relevance
remains a core challenge, traditionally offloaded to learning algorithms like
backpropagation. Inspired by recent cellular neurobiological evidence linking
neocortical pyramidal cells to distinct mental states, this work shows how
models (e.g., Transformers) can emulate high-level perceptual processing and
awake thought (imagination) states to pre-select relevant information before
applying attention. Triadic neuronal-level modulation loops among questions
($Q$), clues (keys, $K$), and hypotheses (values, $V$) enable diverse, deep,
parallel reasoning chains at the representation level and allow a rapid shift
from initial biases to refined understanding. This leads to orders-of-magnitude
faster learning with significantly reduced computational demand (e.g., fewer
heads, layers, and tokens), at an approximate cost of $\mathcal{O}(N)$, where
$N$ is the number of input tokens. Results span reinforcement learning (e.g.,
CarRacing in a high-dimensional visual setup), computer vision, and natural
language question answering.

</details>


### [19] [ABE: A Unified Framework for Robust and Faithful Attribution-Based Explainability](https://arxiv.org/abs/2505.06258)
*Zhiyu Zhu,Jiayu Zhang,Zhibo Jin,Fang Chen,Jianlong Zhou*

Main category: cs.LG

TL;DR: 提出了一种名为ABE的统一归因可解释性框架，旨在解决现有框架在可扩展性、耦合性、理论约束和用户友好性方面的局限性，以增强深度学习模型的可解释性和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有的归因方法集成框架（如InterpretDL和OmniXAI）存在可扩展性不足、高耦合、理论约束以及用户友好性差等问题，阻碍了神经网络的透明度和互操作性。

Method: 提出了ABE（Attribution-Based Explainability）统一框架。该框架形式化了基本归因方法，集成了最先进的归因算法，确保符合归因公理，并通过四个可定制模块（鲁棒性、可解释性、验证、数据与模型）增强可解释性。

Result: ABE框架使研究人员能够开发新的归因技术，并为推进基于归因的可解释性研究提供了一个可扩展、可延伸的基础。

Conclusion: ABE框架为推进基于归因的可解释性研究和培育透明的人工智能系统提供了一个可扩展、可延伸的基础。

Abstract: Attribution algorithms are essential for enhancing the interpretability and
trustworthiness of deep learning models by identifying key features driving
model decisions. Existing frameworks, such as InterpretDL and OmniXAI,
integrate multiple attribution methods but suffer from scalability limitations,
high coupling, theoretical constraints, and lack of user-friendly
implementations, hindering neural network transparency and interoperability. To
address these challenges, we propose Attribution-Based Explainability (ABE), a
unified framework that formalizes Fundamental Attribution Methods and
integrates state-of-the-art attribution algorithms while ensuring compliance
with attribution axioms. ABE enables researchers to develop novel attribution
techniques and enhances interpretability through four customizable modules:
Robustness, Interpretability, Validation, and Data & Model. This framework
provides a scalable, extensible foundation for advancing attribution-based
explainability and fostering transparent AI systems. Our code is available at:
https://github.com/LMBTough/ABE-XAI.

</details>


### [20] [Fair Clustering with Clusterlets](https://arxiv.org/abs/2505.06259)
*Mattia Setzu,Riccardo Guidotti*

Main category: cs.LG

TL;DR: 本文提出一种基于簇元（clusterlet）匹配的简单模糊聚类算法，以提升聚类结果的公平性。


<details>
  <summary>Details</summary>
Motivation: 聚类方法的公平性日益受到关注，但现有理论虽证明公平性具有传递性，发现合适的初始公平小簇群却计算昂贵、复杂或随意。

Method: 提出了一系列基于“簇元”（clusterlet）的简单模糊聚类算法。这些算法通过匹配单类簇元，利用簇元距离优化传统聚类目标，同时对公平性进行正则化约束。

Result: 实验结果表明，简单的匹配策略能够实现较高的公平性，并且通过适当的参数调整可以获得高内聚度和低重叠度的聚类效果。

Conclusion: 所提出的基于簇元的简单匹配策略为实现公平聚类提供了一种有效途径，能够在保证聚类质量的同时提升公平性。

Abstract: Given their widespread usage in the real world, the fairness of clustering
methods has become of major interest. Theoretical results on fair clustering
show that fairness enjoys transitivity: given a set of small and fair clusters,
a trivial centroid-based clustering algorithm yields a fair clustering.
Unfortunately, discovering a suitable starting clustering can be
computationally expensive, rather complex or arbitrary.
  In this paper, we propose a set of simple \emph{clusterlet}-based fuzzy
clustering algorithms that match single-class clusters, optimizing fair
clustering. Matching leverages clusterlet distance, optimizing for classic
clustering objectives, while also regularizing for fairness. Empirical results
show that simple matching strategies are able to achieve high fairness, and
that appropriate parameter tuning allows to achieve high cohesion and low
overlap.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [21] [A Comprehensive Data Description for LoRaWAN Path Loss Measurements in an Indoor Office Setting: Effects of Environmental Factors](https://arxiv.org/abs/2505.06375)
*Nahshon Mokua Obiri,Kristof Van Laerhoven*

Main category: cs.NI

TL;DR: 本文提供了一个室内LoRaWAN路径损耗数据集，量化了环境因素对信号传播的影响，并提出了一种改进的路径损耗模型。


<details>
  <summary>Details</summary>
Motivation: 研究室内办公环境中环境因素对LoRaWAN信号传播的具体影响，以优化网络部署、提高可靠性并延长设备电池寿命。

Method: 在德国锡根大学的室内办公环境中，部署了6个LoRaWAN终端设备和1个网关，系统地测量了不同环境条件（温度、湿度、二氧化碳浓度、气压、PM2.5水平）下的接收信号强度指示（RSSI）和信噪比（SNR）。基于此数据集，测试并评估了一种集成了结构障碍（多墙）和环境参数的改进对数距离路径损耗与阴影模型（LDPLSM-MW-EP）。

Result: 实证分析表明，反射、散射、干扰、占用模式（由环境参数变化引起）和家具重新布置等瞬态现象可导致信号衰减变化高达10.58 dB。与仅考虑多墙的基线模型相比，改进后的LDPLSM-MW-EP模型将均方根误差（RMSE）从10.58 dB降低到8.04 dB，决定系数（R²）从0.6917提高到0.8222。

Conclusion: 该数据集和改进模型通过捕捉环境条件和占用动态的额外影响，为优化功耗、延长设备电池寿命和增强室内物联网（IoT）部署的网络可靠性提供了有价值的见解，并为未来室内无线通信研究奠定了坚实基础。

Abstract: This paper presents a comprehensive dataset of LoRaWAN technology path loss
measurements collected in an indoor office environment, focusing on quantifying
the effects of environmental factors on signal propagation. Utilizing a network
of six strategically placed LoRaWAN end devices (EDs) and a single indoor
gateway (GW) at the University of Siegen, City of Siegen, Germany, we
systematically measured signal strength indicators such as the Received Signal
Strength Indicator (RSSI) and the Signal-to-Noise Ratio (SNR) under various
environmental conditions, including temperature, relative humidity, carbon
dioxide (CO$_2$) concentration, barometric pressure, and particulate matter
levels (PM$_{2.5}$). Our empirical analysis confirms that transient phenomena
such as reflections, scattering, interference, occupancy patterns (induced by
environmental parameter variations), and furniture rearrangements can alter
signal attenuation by as much as 10.58 dB, highlighting the dynamic nature of
indoor propagation. As an example of how this dataset can be utilized, we
tested and evaluated a refined Log-Distance Path Loss and Shadowing Model that
integrates both structural obstructions (Multiple Walls) and Environmental
Parameters (LDPLSM-MW-EP). Compared to a baseline model that considers only
Multiple Walls (LDPLSM-MW), the enhanced approach reduced the root mean square
error (RMSE) from 10.58 dB to 8.04 dB and increased the coefficient of
determination (R$^2$) from 0.6917 to 0.8222. By capturing the extra effects of
environmental conditions and occupancy dynamics, this improved model provides
valuable insights for optimizing power usage and prolonging device battery
life, enhancing network reliability in indoor Internet of Things (IoT)
deployments, among other applications. This dataset offers a solid foundation
for future research and development in indoor wireless communication.

</details>


### [22] [Distributionally Robust Contract Theory for Edge AIGC Services in Teleoperation](https://arxiv.org/abs/2505.06678)
*Zijun Zhan,Yaxian Dong,Daniel Mawunyo Doe,Yuqing Hu,Shuai Li,Shaohua Cao,Lei Fan,Zhu Han*

Main category: cs.NI

TL;DR: 该研究提出一种基于分布式鲁棒优化（DRO）的合约理论，为远程操作中的AIGC任务卸载设计鲁棒的激励机制，以应对服务质量不确定性和信息不对称问题。


<details>
  <summary>Details</summary>
Motivation: 先进的AIGC技术为远程操作注入了新动力，边缘AIGC网络满足了其低延迟需求。然而，AIGC服务质量的不确定性以及激励AIGC服务提供商（ASP）的需求，使得设计鲁棒的激励机制至关重要，尤其是在不确定性和信息不对称（远程操作员对ASP剩余资源容量了解有限）的挑战下。

Method: 研究提出了一种基于分布式鲁棒优化（DRO）的合约理论来设计鲁棒的奖励方案。通过将DRO整合到合约理论中来解决不确定性下的合约设计挑战，其中合约理论用于建模信息不对称，DRO用于捕捉AIGC服务质量的不确定性。鉴于原始DRO合约理论问题的复杂性，将其重新表述为一个等效且易于处理的双层优化问题，并开发了一种基于块坐标下降（BCD）的算法来求解。

Result: 在基于Unity的远程操作平台上的仿真结果表明，所提出的方法在不同程度的AIGC服务质量变化下，可将远程操作员效用提高2.7%至10.74%，并将ASP效用比现有技术（基于深度强化学习的合约理论）提高60.02%。

Conclusion: 该研究提出的DRO合约理论能够有效设计AIGC任务卸载的鲁棒奖励方案，在不确定性条件下，其在提升远程操作员和ASP效用方面均优于现有方法。

Abstract: Advanced AI-Generated Content (AIGC) technologies have injected new impetus
into teleoperation, further enhancing its security and efficiency. Edge AIGC
networks have been introduced to meet the stringent low-latency requirements of
teleoperation. However, the inherent uncertainty of AIGC service quality and
the need to incentivize AIGC service providers (ASPs) make the design of a
robust incentive mechanism essential. This design is particularly challenging
due to both uncertainty and information asymmetry, as teleoperators have
limited knowledge of the remaining resource capacities of ASPs. To this end, we
propose a distributionally robust optimization (DRO)-based contract theory to
design robust reward schemes for AIGC task offloading. Notably, our work
extends the contract theory by integrating DRO, addressing the fundamental
challenge of contract design under uncertainty. In this paper, contract theory
is employed to model the information asymmetry, while DRO is utilized to
capture the uncertainty in AIGC service quality. Given the inherent complexity
of the original DRO-based contract theory problem, we reformulate it into an
equivalent, tractable bi-level optimization problem. To efficiently solve this
problem, we develop a Block Coordinate Descent (BCD)-based algorithm to derive
robust reward schemes. Simulation results on our unity-based teleoperation
platform demonstrate that the proposed method improves teleoperator utility by
2.7\% to 10.74\% under varying degrees of AIGC service quality shifts and
increases ASP utility by 60.02\% compared to the SOTA method, i.e., Deep
Reinforcement Learning (DRL)-based contract theory. The code and data are
publicly available at https://github.com/Zijun0819/DRO-Contract-Theory.

</details>


### [23] [Improving 5G/B5G Network Performance with RFID-Enabled Resource Management Systems](https://arxiv.org/abs/2505.06764)
*Stella N. Arinze,Halima I. Kure,Augustine O. Nwajana*

Main category: cs.NI

TL;DR: 研究探索将RFID技术集成到5G/B5G网络中以增强资源管理。


<details>
  <summary>Details</summary>
Motivation: 克服传统资源分配方法在频谱拥塞、高延迟和负载均衡效率低下等方面的挑战。

Method: 在网络组件（用户设备、基站、IoT节点）中嵌入RFID标签收集实时数据，RFID阅读器捕获数据，中央控制器使用定制优化算法动态管理频谱、负载均衡和能耗。通过仿真与传统4G动态资源分配技术进行比较。

Result: 仿真结果显示，基于RFID的模型在关键性能指标上相比传统4G动态资源分配技术有显著改进。

Conclusion: 将RFID技术集成到5G/B5G网络中，是一种有前景的增强资源管理、提升网络性能的方法。

Abstract: In the rapidly evolving landscape of 5G and B5G (beyond 5G) networks,
efficient resource optimization is critical to addressing the escalating
demands for high-speed, low-latency, and energy efficient communication. This
study explores the integration of Radio Frequency Identification (RFID)
technology as a novel approach to enhance resource management in 5G/B5G
networks. The motivation behind this research lies in overcoming persistent
challenges such as spectrum congestion, high latency, and inefficient load
balancing, which impede the performance of traditional resource allocation
methods. To achieve this, RFID tags were embedded in critical network
components, including user devices, base stations, and Internet of Things (IoT)
nodes, enabling the collection of real-time data on device status, location,
and resource utilization. RFID readers strategically placed across the network
continuously captured this data, which was processed by a centralized
controller using a custom-designed optimization algorithm. This algorithm
dynamically managed key network resources, including spectrum allocation, load
balancing, and energy consumption, ensuring efficient operation under varying
network conditions. Simulations were conducted to evaluate the performance of
the RFID-based model against traditional 4G dynamic resource allocation
techniques. The results demonstrated substantial improvements in key
performance metrics.

</details>


### [24] [Towards NWDAF-enabled Analytics and Closed-Loop Automation in 5G Networks](https://arxiv.org/abs/2505.06789)
*Fatemeh Shafiei Ardestani,Niloy Saha,Noura Limam,Raouf Boutaba*

Main category: cs.NI

TL;DR: 本文提出了一种实用的5G网络数据分析功能(NWDAF)实现方案，通过与开源5G核心网集成，实现了闭环自动化管理，并展示了其在安全管理方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 5G网络带来了更高的复杂性，使得闭环自动化至关重要。尽管3GPP引入了NWDAF，但现有研究在标准化数据收集、分析与闭环自动化的集成以及端到端系统评估方面关注不足。

Method: 本文实现了一个NWDAF，并将其与主流开源5G核心网解决方案集成。开发了符合3GPP标准的UPF事件暴露服务用于实时数据收集，以及一个与MLflow集成的ML模型配置服务来支持端到端机器学习生命周期管理。同时，增强了SMF以使用NWDAF的分析结果并做出相应响应。

Result: 评估结果表明，该解决方案在可扩展性、资源效率方面表现良好，并能有效实现5G网络中的闭环安全管理。

Conclusion: 本工作通过提出一个实用的NWDAF实现及其集成方案，填补了现有研究空白，并证明了其在实现5G网络闭环自动化（尤其是在安全管理方面）的可行性和有效性。

Abstract: The fifth generation of cellular technology (5G) delivers faster speeds,
lower latency, and improved network service alongside support for a large
number of users and a diverse range of verticals. This brings increased
complexity to network control and management, making closed-loop automation
essential. In response, the 3rd Generation Partnership Project (3GPP)
introduced the Network Data Analytics Function (NWDAF) to streamline network
monitoring by collecting, analyzing, and providing insights from network data.
While prior research has focused mainly on isolated applications of machine
learning within NWDAF, critical aspects such as standardized data collection,
analytics integration in closed-loop automation, and end-to-end system
evaluation have received limited attention. This work addresses existing gaps
by presenting a practical implementation of NWDAF and its integration with
leading open-source 5G core network solutions. We develop a 3GPP-compliant User
Plane Function (UPF) event exposure service for real-time data collection and
an ML model provisioning service integrated with MLflow to support end-to-end
machine learning lifecycle management. Additionally, we enhance the Session
Management Function (SMF) to consume NWDAF analytics and respond accordingly.
Our evaluation demonstrates the solution's scalability, resource efficiency,
and effectiveness in enabling closed-loop security management in 5G networks.

</details>


### [25] [ContribChain: A Stress-Balanced Blockchain Sharding Protocol with Node Contribution Awareness](https://arxiv.org/abs/2505.06899)
*Xinpeng Huang,Wanqing Jie,Shiwen Zhang,Haofu Yang,Wangjie Qiu,Qinnan Zhang,Huawei Huang,Zehui Xiong,Shaoting Tang,Hongwei Zheng,Zhiming Zheng*

Main category: cs.NI

TL;DR: 提出了一种名为ContribChain的区块链分片协议，通过评估节点贡献并采用新的分配算法（NACV和P-Louvain）来实现分片间的压力平衡，从而提高系统吞吐量并减少跨分片交易。


<details>
  <summary>Details</summary>
Motivation: 现有区块链分片协议主要关注工作负载均衡，但忽略了节点处理能力的差异，这会导致分片间压力不均和交易积压。因此，在动态异构环境中实现分片间的压力平衡是一个重要挑战。

Method: 1. 提出ContribChain协议，根据节点历史行为计算贡献值以评估其性能和安全性。2. 提出节点分配算法NACV和账户分配算法P-Louvain，通过匹配分片性能与工作负载来实现压力平衡。

Result: 实验结果表明，P-Louvain算法将分配执行时间减少了86%，跨分片交易比例降低了7.5%。ContribChain协议将系统吞吐量提高了35.8%，跨分片交易比例降低了16%。

Conclusion: ContribChain通过感知节点贡献并采用优化的分配策略，有效地实现了分片间的压力平衡，显著提升了区块链分片系统的性能。

Abstract: Existing blockchain sharding protocols have focused on eliminating imbalanced
workload distributions. However, even with workload balance, disparities in
processing capabilities can lead to differential stress among shards, resulting
in transaction backlogs in certain shards. Therefore, achieving stress balance
among shards in the dynamic and heterogeneous environment presents a
significant challenge of blockchain sharding. In this paper, we propose
ContribChain, a blockchain sharding protocol that can automatically be aware of
node contributions to achieve stress balance. We calculate node contribution
values based on the historical behavior to evaluate the performance and
security of nodes. Furthermore, we propose node allocation algorithm NACV and
account allocation algorithm P-Louvain, which both match shard performance with
workload to achieve stress balance. Finally, we conduct extensive experiments
to compare our work with state-of-the-art baselines based on real Ethereum
transactions. The evaluation results show that P-Louvain reduces allocation
execution time by 86% and the cross-shard transaction ratio by 7.5%. Meanwhile,
ContribChain improves throughput by 35.8% and reduces the cross-shard
transaction ratio by 16%.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [26] [LMLCC-Net: A Semi-Supervised Deep Learning Model for Lung Nodule Malignancy Prediction from CT Scans using a Novel Hounsfield Unit-Based Intensity Filtering](https://arxiv.org/abs/2505.06370)
*Adhora Madhuri,Nusaiba Sobir,Tasnia Binte Mamun,Taufiq Hasan*

Main category: eess.IV

TL;DR: 提出LMLCC-Net，一种利用3D CNN和亨氏单位(HU)强度滤波对CT图像中的肺结节进行分类的新型深度学习框架，在LUNA16数据集上取得了优异的分类性能。


<details>
  <summary>Details</summary>
Motivation: 肺癌是全球患者死亡的主要原因，早期诊断恶性肺结节对降低死亡率至关重要。现有研究未能充分利用良性和恶性结节在亨氏单位（HU）强度分布上的显著差异。

Method: 提出了LMLCC-Net框架：1. 使用3D卷积神经网络（3D CNN）。2. 引入基于HU的可学习强度滤波阶段，同时考虑结节的强度模式和纹理信息。3. LMLCC-Net从多个分支提取特征，每个分支使用独立的可学习HU强度滤波。4. 提出了用于标记模糊病例的半监督学习方案，并开发了一个轻量级模型。

Result: 在LUNA16数据集上的实验评估显示，所提出的方法达到了91.96%的分类准确率（ACC），92.04%的灵敏度（SEN）和91.87%的曲线下面积（AUC），性能优于现有方法。

Conclusion: LMLCC-Net通过有效利用HU强度信息和3D CNN，显著提高了肺结节的分类性能，有望辅助放射科医生进行诊断，从而改善患者护理。

Abstract: Lung cancer is the leading cause of patient mortality in the world. Early
diagnosis of malignant pulmonary nodules in CT images can have a significant
impact on reducing disease mortality and morbidity. In this work, we propose
LMLCC-Net, a novel deep learning framework for classifying nodules from CT scan
images using a 3D CNN, considering Hounsfield Unit (HU)-based intensity
filtering. Benign and malignant nodules have significant differences in their
intensity profile of HU, which was not exploited in the literature. Our method
considers the intensity pattern as well as the texture for the prediction of
malignancies. LMLCC-Net extracts features from multiple branches that each use
a separate learnable HU-based intensity filtering stage. Various combinations
of branches and learnable ranges of filters were explored to finally produce
the best-performing model. In addition, we propose a semi-supervised learning
scheme for labeling ambiguous cases and also developed a lightweight model to
classify the nodules. The experimental evaluations are carried out on the
LUNA16 dataset. Our proposed method achieves a classification accuracy (ACC) of
91.96%, a sensitivity (SEN) of 92.04%, and an area under the curve (AUC) of
91.87%, showing improved performance compared to existing methods. The proposed
method can have a significant impact in helping radiologists in the
classification of pulmonary nodules and improving patient care.

</details>
