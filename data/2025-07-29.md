<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 20]
- [cs.CV](#cs.CV) [Total: 16]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 16]
- [cs.NI](#cs.NI) [Total: 16]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media](https://arxiv.org/abs/2507.19511)
*Khalid Hasan,Jamil Saquer,Mukulika Ghosh*

Main category: cs.CL

TL;DR: 本研究评估了在Reddit数据上使用Transformer模型和LSTM模型进行心理健康障碍分类的性能。结果显示Transformer模型（特别是RoBERTa）表现出色，且结合BERT嵌入的LSTM模型在资源有限的情况下也具有很强的竞争力，证明了其在心理健康监测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 心理健康障碍日益普遍，亟需开发强大、自动化的工具进行早期检测和监测。自然语言处理（NLP）特别是基于Transformer的架构在文本分析中展现了巨大潜力。

Method: 本研究构建了一个大型Reddit心理健康障碍分类标注数据集，并通过统计判断分析和主题建模验证了其可靠性。随后，全面评估了最先进的Transformer模型（BERT, RoBERTa, DistilBERT, ALBERT, ELECTRA）与基于长短期记忆网络（LSTM）的方法，并结合了不同的文本嵌入技术，用于心理健康障碍分类。

Result: 实验结果表明，Transformer模型优于传统的深度学习方法。RoBERTa实现了最高的分类性能，在内部测试集上F1分数为99.54%，在外部测试集上为96.05%。值得注意的是，增强了BERT嵌入的LSTM模型也表现出高度竞争力，在外部数据集上的F1分数超过94%，同时所需的计算资源显著更少。

Conclusion: 研究结果突出了基于Transformer的模型在实时、可扩展心理健康监测中的有效性。这些发现对于临床应用和数字心理健康干预具有重要意义，并深入探讨了最先进NLP方法在精神障碍检测中的能力和局限性。

Abstract: The rising prevalence of mental health disorders necessitates the development
of robust, automated tools for early detection and monitoring. Recent advances
in Natural Language Processing (NLP), particularly transformer-based
architectures, have demonstrated significant potential in text analysis. This
study provides a comprehensive evaluation of state-of-the-art transformer
models (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term
Memory (LSTM) based approaches using different text embedding techniques for
mental health disorder classification on Reddit. We construct a large annotated
dataset, validating its reliability through statistical judgmental analysis and
topic modeling. Experimental results demonstrate the superior performance of
transformer models over traditional deep-learning approaches. RoBERTa achieved
the highest classification performance, with a 99.54% F1 score on the hold-out
test set and a 96.05% F1 score on the external test set. Notably, LSTM models
augmented with BERT embeddings proved highly competitive, achieving F1 scores
exceeding 94% on the external dataset while requiring significantly fewer
computational resources. These findings highlight the effectiveness of
transformer-based models for real-time, scalable mental health monitoring. We
discuss the implications for clinical applications and digital mental health
interventions, offering insights into the capabilities and limitations of
state-of-the-art NLP methodologies in mental disorder detection.

</details>


### [2] [Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables](https://arxiv.org/abs/2507.19521)
*Vishakh Padmakumar,Joseph Chee Chang,Kyle Lo,Doug Downey,Aakanksha Naik*

Main category: cs.CL

TL;DR: 本文提出新数据集和LLM编辑技术，解决学术论文模式生成中的歧义和缺乏改进方法的问题，显著提升了模式生成性能，并证明小型模型也能表现出色。


<details>
  <summary>Details</summary>
Motivation: 学术文献量增长，需要有效组织和比较文档。大语言模型(LLMs)可用于生成比较模式，但现有方法存在两项不足：1) 基于引用的评估存在歧义；2) 缺乏编辑/改进方法。

Method: 1. 提出一种方法，用合成意图扩充未标注的表格语料库，并创建了一个用于研究在给定信息需求下生成模式的数据集，以减少歧义。2. 提出几种基于LLM的模式编辑技术。3. 全面评估了多种单次模式生成方法（包括LLM提示工作流和微调模型）。

Result: 1. 结合表格意图显著改善了基线模型在重构参考模式方面的性能。2. 证明较小的开源模型经过微调后，可以与最先进的LLM提示方法媲美。3. 提出的编辑技术可以进一步改进这些方法生成的模式。

Conclusion: 本文通过引入新数据集和LLM编辑技术，有效解决了模式生成中的歧义和缺乏改进方法的问题，显著提升了模式生成的性能和可靠性，并证明小型模型也能达到先进水平。

Abstract: The increasing volume of academic literature makes it essential for
researchers to organize, compare, and contrast collections of documents. Large
language models (LLMs) can support this process by generating schemas defining
shared aspects along which to compare papers. However, progress on schema
generation has been slow due to: (i) ambiguity in reference-based evaluations,
and (ii) lack of editing/refinement methods. Our work is the first to address
both issues. First, we present an approach for augmenting unannotated table
corpora with synthesized intents and apply it to create a dataset for studying
schema generation conditioned on a given information need, thus reducing
ambiguity. With this dataset, we show how incorporating table intents
significantly improves baseline performance in reconstructing reference
schemas. Next, we propose several LLM-based schema editing techniques. We start
by comprehensively benchmarking several single-shot schema generation methods,
including prompted LLM workflows and fine-tuned models, showing that smaller,
open-weight models can be fine-tuned to be competitive with state-of-the-art
prompted LLMs. Then we demonstrate that our editing techniques can further
improve schemas generated by these methods.

</details>


### [3] [Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri](https://arxiv.org/abs/2507.19537)
*Felix Kraus,Nicolas Blumenröhr,Danah Tonne,Achim Streit*

Main category: cs.CL

TL;DR: WOKIE是一个开源、模块化的自动化SKOS词库翻译工具，它结合外部翻译服务和LLM精炼，旨在提高数字人文领域知识资源的可访问性、重用性和互操作性。


<details>
  <summary>Details</summary>
Motivation: 数字人文（DH）领域中，语言多样性限制了知识资源（如SKOS词库）的访问、重用和语义互操作性，亟需一种解决方案来克服语言障碍。

Method: 引入WOKIE，一个自动化SKOS词库翻译管道。该方法结合外部翻译服务与大型语言模型（LLMs）的定向精炼，以平衡翻译质量、可扩展性和成本。WOKIE设计为可在日常硬件上运行，易于扩展，且无需机器学习或LLM专业知识。

Result: 通过在15种语言的多个数字人文词库上的评估，结果表明WOKIE能够通过无障碍的自动化翻译和改进的本体匹配性能，有效增强词库的可访问性、重用性和跨语言互操作性。

Conclusion: WOKIE是一款实用的工具，能实现SKOS词库的自动化翻译，显著提升知识资源的可访问性、重用性和跨语言互操作性，从而支持更具包容性和多语言的研究基础设施。

Abstract: We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for
the automated translation of SKOS thesauri. This work addresses a critical need
in the Digital Humanities (DH), where language diversity can limit access,
reuse, and semantic interoperability of knowledge resources. WOKIE combines
external translation services with targeted refinement using Large Language
Models (LLMs), balancing translation quality, scalability, and cost. Designed
to run on everyday hardware and be easily extended, the application requires no
prior expertise in machine translation or LLMs. We evaluate WOKIE across
several DH thesauri in 15 languages with different parameters, translation
services and LLMs, systematically analysing translation quality, performance,
and ontology matching improvements. Our results show that WOKIE is suitable to
enhance the accessibility, reuse, and cross-lingual interoperability of
thesauri by hurdle-free automated translation and improved ontology matching
performance, supporting more inclusive and multilingual research
infrastructures.

</details>


### [4] [Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning](https://arxiv.org/abs/2507.19586)
*Shengyuan Wang,Jie Feng,Tianhui Liu,Dan Pei,Yong Li*

Main category: cs.CL

TL;DR: 本文提出了一个评估和缓解大型语言模型（LLMs）地理空间幻觉的框架，并引入了基于Kahneman-Tversky优化的方法，显著提升了LLMs在地理空间任务中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs拥有广泛的地理空间知识并成功应用于相关任务，但它们常产生不准确的地理空间信息（即地理空间幻觉），这损害了其可靠性。现有研究广泛关注LLMs的一般知识幻觉，但对地理空间幻觉的系统评估和缓解仍缺乏探索。

Method: 研究提出了一个利用结构化地理空间知识图谱的全面评估框架，用于受控评估地理空间幻觉。通过对20个先进LLMs进行广泛评估，揭示了其地理空间知识中的幻觉。在此基础上，引入了一种基于Kahneman-Tversky优化的动态事实对齐方法来缓解地理空间幻觉。

Result: 通过评估，揭示了LLMs在地理空间知识中存在的幻觉。所提出的基于Kahneman-Tversky优化的方法在提出的基准上将性能提高了29.6%以上。

Conclusion: 研究成果表明，所提出的基准和学习算法能有效提高LLMs在地理空间知识和推理任务中的可信度。

Abstract: Large language models (LLMs) possess extensive world knowledge, including
geospatial knowledge, which has been successfully applied to various geospatial
tasks such as mobility prediction and social indicator prediction. However,
LLMs often generate inaccurate geospatial knowledge, leading to geospatial
hallucinations (incorrect or inconsistent representations of geospatial
information) that compromise their reliability. While the phenomenon of general
knowledge hallucination in LLMs has been widely studied, the systematic
evaluation and mitigation of geospatial hallucinations remain largely
unexplored. To address this gap, we propose a comprehensive evaluation
framework for geospatial hallucinations, leveraging structured geospatial
knowledge graphs for controlled assessment. Through extensive evaluation across
20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge.
Building on these insights, we introduce a dynamic factuality aligning method
based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial
hallucinations in LLMs, leading to a performance improvement of over 29.6% on
the proposed benchmark. Extensive experimental results demonstrate the
effectiveness of our benchmark and learning algorithm in enhancing the
trustworthiness of LLMs in geospatial knowledge and reasoning tasks.

</details>


### [5] [Efficient Attention Mechanisms for Large Language Models: A Survey](https://arxiv.org/abs/2507.19595)
*Yutao Sun,Zhenyu Li,Yike Zhang,Tengyu Pan,Bowen Dong,Yuyi Guo,Jianyong Wang*

Main category: cs.CL

TL;DR: 鉴于Transformer自注意力机制的长上下文建模瓶颈，本文全面综述了线性注意力与稀疏注意力两大高效机制及其在大型语言模型中的应用，旨在为未来可扩展模型设计提供参考。


<details>
  <summary>Details</summary>
Motivation: Transformer架构中自注意力机制的二次时间与内存复杂度，已成为高效长上下文建模的根本障碍。

Method: 本文对高效注意力机制（包括线性注意力与稀疏注意力）的最新发展进行了系统而全面的综述，整合了算法创新和硬件层面考量。此外，还分析了高效注意力在大型预训练语言模型中的整合方式，包括纯高效注意力架构和混合设计。

Result: 本综述提供了高效注意力机制的系统性概述，涵盖了算法和硬件层面的创新，并分析了其在大规模预训练语言模型中的整合与应用模式。

Conclusion: 本工作旨在通过结合理论基础与实际部署策略，为推动可扩展和高效语言模型的设计提供一个基础性参考。

Abstract: Transformer-based architectures have become the prevailing backbone of large
language models. However, the quadratic time and memory complexity of
self-attention remains a fundamental obstacle to efficient long-context
modeling. To address this limitation, recent research has introduced two
principal categories of efficient attention mechanisms. Linear attention
methods achieve linear complexity through kernel approximations, recurrent
formulations, or fastweight dynamics, thereby enabling scalable inference with
reduced computational overhead. Sparse attention techniques, in contrast, limit
attention computation to selected subsets of tokens based on fixed patterns,
block-wise routing, or clustering strategies, enhancing efficiency while
preserving contextual coverage. This survey provides a systematic and
comprehensive overview of these developments, integrating both algorithmic
innovations and hardware-level considerations. In addition, we analyze the
incorporation of efficient attention into largescale pre-trained language
models, including both architectures built entirely on efficient attention and
hybrid designs that combine local and global components. By aligning
theoretical foundations with practical deployment strategies, this work aims to
serve as a foundational reference for advancing the design of scalable and
efficient language models.

</details>


### [6] [MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?](https://arxiv.org/abs/2507.19598)
*Muntasir Wahed,Xiaona Zhou,Kiet A. Nguyen,Tianjiao Yu,Nirav Diwan,Gang Wang,Dilek Hakkani-Tür,Ismini Lourentzou*

Main category: cs.CL

TL;DR: 本文提出一种“代码分解攻击”方法来规避LLM的安全过滤器，并引入MOCHA基准测试评估代码LLM在单轮和多轮恶意提示下的鲁棒性。研究发现LLM在此类攻击下普遍脆弱，但通过在MOCHA上微调可显著提升其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在代码生成能力上取得了显著进步，但其在面对对抗性滥用（特别是通过多轮恶意编码提示）时的鲁棒性尚未得到充分探索。

Method: 1. 引入“代码分解攻击”：将恶意编码任务分解为多轮看似无害的子任务，以规避LLM的安全过滤器。2. 构建MOCHA（一个大规模基准测试）：用于系统评估代码LLMs在单轮和多轮恶意提示下的鲁棒性。3. 在开放源和闭源模型上进行实证评估。4. 对模型在MOCHA上进行微调以提升性能。

Result: 1. 开放源和闭源LLM都存在持续的漏洞，尤其是在多轮恶意提示场景下。2. 在MOCHA上对模型进行微调可以提高其对恶意提示的拒绝率，同时不损害代码生成能力。3. 微调后的模型在外部对抗性数据集上表现出增强的鲁棒性，拒绝率最高提升32.4%，且无需额外监督。

Conclusion: LLMs在多轮恶意代码生成提示下存在明显的鲁棒性弱点，但通过本文提出的MOCHA基准和相应的微调方法，可以有效评估并显著提升LLMs的鲁棒性，同时保持其核心功能。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
enhanced their code generation capabilities. However, their robustness against
adversarial misuse, particularly through multi-turn malicious coding prompts,
remains underexplored. In this work, we introduce code decomposition attacks,
where a malicious coding task is broken down into a series of seemingly benign
subtasks across multiple conversational turns to evade safety filters. To
facilitate systematic evaluation, we introduce \benchmarkname{}, a large-scale
benchmark designed to evaluate the robustness of code LLMs against both
single-turn and multi-turn malicious prompts. Empirical results across open-
and closed-source models reveal persistent vulnerabilities, especially under
multi-turn scenarios. Fine-tuning on MOCHA improves rejection rates while
preserving coding ability, and importantly, enhances robustness on external
adversarial datasets with up to 32.4% increase in rejection rates without any
additional supervision.

</details>


### [7] [HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track](https://arxiv.org/abs/2507.19616)
*Xuchen Wei,Yangxin Wu,Yaoyin Zhang,Henglyu Liu,Kehai Chen,Xuefeng Bai,Min Zhang*

Main category: cs.CL

TL;DR: HITSZ为IWSLT 2025印度语赛道提交的论文，提出一个结合Whisper和Krutrim的端到端语音到文本翻译系统，用于低资源英语-印度语及印度语-英语翻译。该系统取得了良好的BLEU分数，并探讨了CoT方法，发现其有提升潜力但存在格式依从性挑战。


<details>
  <summary>Details</summary>
Motivation: 在低资源场景下，提升英语-印度语和印度语-英语语音到文本翻译（ST）的质量。

Method: 提出一个端到端系统，整合预训练的Whisper自动语音识别（ASR）模型与专注于印度语的Krutrim大语言模型（LLM）。此外，还研究了Chain-of-Thought (CoT) 方法。

Result: 端到端系统在英语-印度语方向上平均BLEU得分为28.88，在印度语-英语方向上为27.86。CoT方法在成功解析的输出上显示出显著的翻译质量提升潜力（例如，泰米尔语到英语提升13.84 BLEU），但观察到模型难以持续遵循所需CoT输出格式的挑战。

Conclusion: 结合Whisper和Krutrim的端到端系统在印度语语音到文本翻译任务中表现良好。CoT方法具有提升翻译质量的潜力，但在实际应用中需要解决模型输出格式一致性的问题。

Abstract: This paper presents HITSZ's submission for the IWSLT 2025 Indic track,
focusing on speech-to-text translation (ST) for English-to-Indic and
Indic-to-English language pairs. To enhance translation quality in this
low-resource scenario, we propose an end-to-end system integrating the
pre-trained Whisper automated speech recognition (ASR) model with Krutrim, an
Indic-specialized large language model (LLM). Experimental results demonstrate
that our end-to-end system achieved average BLEU scores of $28.88$ for
English-to-Indic directions and $27.86$ for Indic-to-English directions.
Furthermore, we investigated the Chain-of-Thought (CoT) method. While this
method showed potential for significant translation quality improvements on
successfully parsed outputs (e.g. a $13.84$ BLEU increase for
Tamil-to-English), we observed challenges in ensuring the model consistently
adheres to the required CoT output format.

</details>


### [8] [MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks](https://arxiv.org/abs/2507.19634)
*Sara Papi,Maike Züfle,Marco Gaido,Beatrice Savoldi,Danni Liu,Ioannis Douros,Luisa Bentivogli,Jan Niehues*

Main category: cs.CL

TL;DR: 本文提出了MCIF，一个多语言、多模态、人工标注的基准，用于评估多模态大语言模型（MLLMs）在跨语言、多模态及长短上下文场景下的指令遵循能力，以填补现有评估工具的空白。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型（MLLMs）正在向通用指令遵循模型发展，但现有基准在联合评估其多语言、多模态以及长短上下文理解能力方面存在不足，如仅限于英语、侧重单模态、依赖短上下文或缺乏人工标注，阻碍了对模型性能的全面评估。

Method: 为解决上述问题，研究者引入了MCIF（Multimodal Crosslingual Instruction Following），这是首个基于科学演讲的多语言人工标注基准。MCIF旨在评估MLLMs在跨语言、多模态设置下，对长短输入进行指令遵循的能力，涵盖语音、视觉和文本三种核心模态，以及英语、德语、意大利语和中文四种语言。

Result: 该抽象主要介绍了MCIF基准的创建及其特性，并未直接呈现使用此基准评估模型所获得的具体研究结果。基准的创建本身是本研究的主要产出，其具备了全面评估MLLMs跨语言理解和多模态信息结合能力的功能。

Conclusion: MCIF的推出填补了MLLMs评估领域的空白，提供了一个全面、多语言、多模态且支持长短上下文的指令遵循评估工具。它的发布将促进MLLMs的开放研究与发展。

Abstract: Recent advances in large language models have catalyzed the development of
multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified
frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to
general-purpose instruction-following models, a key frontier lies in evaluating
their multilingual and multimodal capabilities over both long and short
contexts. However, existing benchmarks fall short in evaluating these
dimensions jointly: they are often limited to English, mostly focus on one
single modality at a time, rely on short-form contexts, or lack human
annotations -- hindering comprehensive assessment of model performance across
languages, modalities, and task complexity. To address these gaps, we introduce
MCIF (Multimodal Crosslingual Instruction Following), the first multilingual
human-annotated benchmark based on scientific talks that is designed to
evaluate instruction-following in crosslingual, multimodal settings over both
short- and long-form inputs. MCIF spans three core modalities -- speech,
vision, and text -- and four diverse languages (English, German, Italian, and
Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret
instructions across languages and combine them with multimodal contextual
information. MCIF is released under a CC-BY 4.0 license to encourage open
research and progress in MLLMs development.

</details>


### [9] [RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams](https://arxiv.org/abs/2507.19666)
*Andrei Vlad Man,Răzvan-Alexandru Smădu,Cristian-George Craciun,Dumitru-Clementin Cercel,Florin Pop,Mihaela-Claudia Cercel*

Main category: cs.CL

TL;DR: 本文评估了LLMs和VLMs在罗马尼亚驾驶法律教育中的能力，引入了多模态数据集RoD-TAL并使用RAG等方法。研究表明领域微调和思维链提示能提高准确性，但视觉推理仍具挑战。


<details>
  <summary>Details</summary>
Motivation: 人工智能与法律系统交叉领域对法律教育工具的需求日益增长，尤其是在罗马尼亚语等资源匮乏的语言中。本研究旨在评估大型语言模型（LLMs）和视觉语言模型（VLMs）在理解和推理罗马尼亚驾驶法律方面的能力。

Method: 引入了新型多模态数据集RoD-TAL，包含罗马尼亚驾驶考试的文本和图像问题，以及法律参考和人工解释。实施并评估了检索增强生成（RAG）管道、密集检索器和推理优化模型，涵盖信息检索（IR）、问答（QA）、视觉IR和视觉QA等任务。实验中还使用了领域特定微调和思维链提示。

Result: 领域特定微调显著提升了检索性能；思维链提示和专用推理模型提高了问答准确性，超越了通过驾驶考试所需的最低分数。然而，视觉推理仍具有挑战性。

Conclusion: LLMs和VLMs在法律教育中具有应用潜力，但其在视觉推理方面也存在局限性。

Abstract: The intersection of AI and legal systems presents a growing need for tools
that support legal education, particularly in under-resourced languages such as
Romanian. In this work, we aim to evaluate the capabilities of Large Language
Models (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning
about Romanian driving law through textual and visual question-answering tasks.
To facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising
Romanian driving test questions, text-based and image-based, alongside
annotated legal references and human explanations. We implement and assess
retrieval-augmented generation (RAG) pipelines, dense retrievers, and
reasoning-optimized models across tasks including Information Retrieval (IR),
Question Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate
that domain-specific fine-tuning significantly enhances retrieval performance.
At the same time, chain-of-thought prompting and specialized reasoning models
improve QA accuracy, surpassing the minimum grades required to pass driving
exams. However, visual reasoning remains challenging, highlighting the
potential and the limitations of applying LLMs and VLMs to legal education.

</details>


### [10] [Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks](https://arxiv.org/abs/2507.19699)
*Maitha Alshehhi,Ahmed Sharshar,Mohsen Guizani*

Main category: cs.CL

TL;DR: 该研究评估了多语言和单语言大模型在低资源语言（如卡纳达语、阿拉伯语）中的性能及其压缩策略（剪枝、量化），发现多语言模型表现更优，量化有效，但激进剪枝会损害性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在高资源语言中取得了显著成功，但在低资源语言环境（如卡纳达语和阿拉伯语）中的能力尚未完全理解。研究旨在探究语言多样性和资源可用性如何影响其性能，并评估模型压缩策略（如剪枝和量化）的效果。

Method: 研究对BLOOMZ、AceGPT、Jais、LLaMA-2、XGLM和AraGPT2等多种多语言和单语言大模型在阿拉伯语、英语和印地语系语言上进行了基准测试，并特别关注了剪枝和量化等模型压缩策略对性能的影响。

Result: 研究发现，语言多样性和资源可用性导致模型性能存在显著差异；多语言版本模型普遍优于其特定语言版本，表明具有显著的跨语言迁移优势；4位和8位量化在提高效率的同时能有效保持模型准确性；但激进剪枝会显著损害模型性能，尤其对大型模型影响更大。

Conclusion: 本研究明确了构建可扩展和公平的多语言自然语言处理解决方案的关键策略，并强调在低资源环境下需要采取干预措施来解决模型的幻觉和泛化错误问题。

Abstract: Although LLMs have attained significant success in high-resource languages,
their capacity in low-resource linguistic environments like Kannada and Arabic
is not yet fully understood. This work benchmarking the performance of
multilingual and monolingual Large Language Models (LLMs) across Arabic,
English, and Indic languages, with particular emphasis on the effects of model
compression strategies such as pruning and quantization. Findings shows
significant performance differences driven by linguistic diversity and resource
availability on SOTA LLMS as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2.
We find that multilingual versions of the model outperform their
language-specific counterparts across the board, indicating substantial
cross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in
maintaining model accuracy while promoting efficiency, but aggressive pruning
significantly compromises performance, especially in bigger models. Our
findings pinpoint key strategies to construct scalable and fair multilingual
NLP solutions and underscore the need for interventions to address
hallucination and generalization errors in the low-resource setting.

</details>


### [11] [Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs](https://arxiv.org/abs/2507.19710)
*Ronak Upasham,Tathagata Dey,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本文提出一种新颖的三阶段管道，利用RDF中间表示从表格生成兼具客观性和主观性的文本，其性能可与大型语言模型媲美，但使用更小的T5模型。


<details>
  <summary>Details</summary>
Motivation: 现有表格到文本（T2T）生成主要侧重于提供表格数据的客观描述，但对结合主观解释的文本生成研究不足。

Method: 提出一个三阶段管道：1) 提取资源描述框架（RDF）三元组；2) 将文本聚合成连贯的叙述；3) 注入主观性。该方法利用中间表示（RDF）增强事实准确性，并采用小型微调T5模型。

Result: 该方法在使用较小T5模型的情况下，性能可与GPT-3.5媲美，并在多个指标上优于Mistral-7B和Llama-2。通过定量和定性分析，证明了其在平衡事实准确性和主观解释方面的有效性。

Conclusion: 该研究首次提出一个结构化的T2T生成管道，通过整合中间表示，同时提升了文本的事实正确性和主观性。

Abstract: In Table-to-Text (T2T) generation, existing approaches predominantly focus on
providing objective descriptions of tabular data. However, generating text that
incorporates subjectivity, where subjectivity refers to interpretations beyond
raw numerical data, remains underexplored. To address this, we introduce a
novel pipeline that leverages intermediate representations to generate both
objective and subjective text from tables. Our three-stage pipeline consists
of: 1) extraction of Resource Description Framework (RDF) triples, 2)
aggregation of text into coherent narratives, and 3) infusion of subjectivity
to enrich the generated text. By incorporating RDFs, our approach enhances
factual accuracy while maintaining interpretability. Unlike large language
models (LLMs) such as GPT-3.5, Mistral-7B, and Llama-2, our pipeline employs
smaller, fine-tuned T5 models while achieving comparable performance to GPT-3.5
and outperforming Mistral-7B and Llama-2 in several metrics. We evaluate our
approach through quantitative and qualitative analyses, demonstrating its
effectiveness in balancing factual accuracy with subjective interpretation. To
the best of our knowledge, this is the first work to propose a structured
pipeline for T2T generation that integrates intermediate representations to
enhance both factual correctness and subjectivity.

</details>


### [12] [Basic Reading Distillation](https://arxiv.org/abs/2507.19741)
*Zhi Zhou,Sirui Miao,Xiangyu Duan,Hao Yang,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出基础阅读蒸馏（BRD）方法，通过让小模型模仿大型语言模型（LLMs）的基础阅读行为，使其在多种任务上表现超越或媲美大20倍以上的LLMs。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽能力强大，但计算资源需求高，限制了其在实际部署中的应用。现有蒸馏方法（如知识蒸馏或任务蒸馏）忽视了在与下游任务无关的通用文本上对小模型进行“基础阅读教育”。

Method: 本文提出基础阅读蒸馏（BRD）。该方法旨在教育小型模型模仿大型语言模型（LLMs）的基本阅读行为，例如在每个句子上进行命名实体识别、问题提出和回答。

Result: 经过BRD训练后，小型模型在语言推理基准和BIG-bench任务上表现优异，能超越或媲美体量大20倍以上的LLMs。分析表明，BRD能有效影响小型模型的概率分布，且与现有知识蒸馏或任务蒸馏方法具有正交性。

Conclusion: 基础阅读蒸馏（BRD）是一种有效的方法，能显著提升小型模型的性能，使其在多种任务上达到甚至超越大型LLMs的水平，为LLM的轻量化部署提供了新途径。

Abstract: Large language models (LLMs) have demonstrated remarkable abilities in
various natural language processing areas, but they demand high computation
resources which limits their deployment in real-world. Distillation is one
technique to solve this problem through either knowledge distillation or task
distillation. Both distillation approaches train small models to imitate
specific features of LLMs, but they all neglect basic reading education for
small models on generic texts that are \emph{unrelated} to downstream tasks. In
this paper, we propose basic reading distillation (BRD) which educates a small
model to imitate LLMs basic reading behaviors, such as named entity
recognition, question raising and answering, on each sentence. After such basic
education, we apply the small model on various tasks including language
inference benchmarks and BIG-bench tasks. It shows that the small model can
outperform or perform comparable to over 20x bigger LLMs. Analysis reveals that
BRD effectively influences the probability distribution of the small model, and
has orthogonality to either knowledge distillation or task distillation.

</details>


### [13] [JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2507.19748)
*Yifan Hao,Fangning Chao,Yaqian Hao,Zhaojun Cui,Huan Bai,Haiyu Zhang,Yankai Liu,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 本文介绍了JT-Math-8B，一个通过多阶段优化框架、高质量数据和新颖强化学习（RL）课程训练的开源数学推理模型系列，在同等规模模型中实现了最先进（SOTA）的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管现有大型语言模型（LLMs）展现出潜力，但在处理需要深度理解和多步推导的复杂数学问题时常有不足，而数学推理是通用人工智能（AGI）的基石和评估LLMs能力的关键。

Method: 研究团队开发了JT-Math-8B系列开源模型（包括基础、指令和思维版本），并构建了一个高质量的2100亿token预训练数据集。指令模型通过监督微调（SFT）和基于GRPO的RL优化以提供简洁答案。思维模型则采用长思维链（Long CoT）方法，结合SFT和新颖的多阶段RL课程，逐步增加任务难度和上下文长度至32K token，以解决复杂问题。

Result: JT-Math-8B在同等规模的开源模型中取得了最先进的结果，超越了OpenAI的O1-mini和GPT-4o等知名模型，并在竞赛级数学问题上展现出卓越的性能。

Conclusion: JT-Math-8B通过其创新的训练方法和数据处理，显著提升了开源LLM在复杂数学推理方面的能力，为数学推理领域树立了新的基准。

Abstract: Mathematical reasoning is a cornerstone of artificial general intelligence
and a primary benchmark for evaluating the capabilities of Large Language
Models (LLMs). While state-of-the-art models show promise, they often falter
when faced with complex problems that demand deep conceptual understanding and
intricate, multi-step deliberation. To address this challenge, we introduce
JT-Math-8B, a series of open-source models comprising base, instruct, and
thinking versions, built upon a systematic, multi-stage optimization framework.
Our pre-training corpus is a high-quality, 210B-token dataset curated through a
dedicated data pipeline that uses model-based validation to ensure quality and
diversity. The Instruct Model is optimized for direct, concise answers through
Supervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL)
method. The Thinking Model is trained for complex problem-solving using a Long
Chain-of-Thought (Long CoT) approach, combining SFT with a novel, multi-stage
RL curriculum that progressively increases task difficulty and context length
up to 32K tokens. JT-Math-8B achieves state-of-the-art results among
open-source models of similar size, surpassing prominent models like OpenAI's
O1-mini and GPT-4o , and demonstrating superior performance on
competition-level mathematics.

</details>


### [14] [Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs](https://arxiv.org/abs/2507.19756)
*Rebecca M. M. Hicke,Brian Haggard,Mia Ferrante,Rayhan Khanna,David Mimno*

Main category: cs.CL

TL;DR: 本研究利用计算工具分析基督教小说，揭示了该文学体裁的主题概览及其对“上帝之行”的描绘，并发现《末日迷踪》系列与更广泛的基督教小说以及不同性别作者作品之间的显著差异。


<details>
  <summary>Details</summary>
Motivation: 美国福音派的文化和文学方面，特别是基督教小说，研究不足。现有学术关注主要集中在热门的《末日迷踪》系列，缺乏对整个基督教小说的广泛分析和深入探索。

Method: 首先，与人工标注者合作，制定了“上帝之行”的定义和编码手册。其次，借助大型模型，将人工标注指南适配于轻量级语言模型（LM）。最后，使用该模型对基督教小说进行主题分析，并探讨作者如何描绘神圣行为。

Result: 开发的轻量级语言模型能够精确匹配人类标注，即使面对微妙和复杂的任务也能表现出色。分析结果显示，《末日迷踪》系列与更广泛的基督教小说之间存在显著差异，同时男性和女性作者的作品之间也存在有意义的区别。

Conclusion: 本研究成功运用计算方法对基督教小说进行了深入分析，填补了该领域的研究空白，并超越了以往对特定系列作品的狭窄关注。研究结果揭示了该文体内部的多样性，并验证了轻量级LM在复杂文本标注任务中的有效性。

Abstract: In addition to its more widely studied political activities, the American
Evangelical movement has a well-developed but less externally visible cultural
and literary side. Christian Fiction, however, has been little studied, and
what scholarly attention there is has focused on the explosively popular Left
Behind series. In this work, we use computational tools to provide both a broad
topical overview of Christian Fiction as a genre and a more directed
exploration of how its authors depict divine acts. Working with human
annotators we first developed definitions and a codebook for "acts of God." We
then adapted those instructions designed for human annotators for use by a
recent, lightweight LM with the assistance of a much larger model. The
laptop-scale LM is capable of matching human annotations, even when the task is
subtle and challenging. Using these annotations, we show that significant and
meaningful differences exist between the Left Behind books and Christian
Fiction more broadly and between books by male and female authors.

</details>


### [15] [UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities](https://arxiv.org/abs/2507.19766)
*Dong Du,Shulin Liu,Tao Yang,Shaohua Chen,Yang Li*

Main category: cs.CL

TL;DR: 本文提出UloRL，一种新的强化学习方法，通过分段解码和动态掩蔽技术，显著提升了LLM在处理超长输出序列时的训练效率和推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于可验证奖励的强化学习(RLVR)能增强大型语言模型(LLMs)的推理能力，但传统RL框架在处理超长输出时面临效率低下问题，如长尾序列分布和训练中的熵崩溃。

Method: 提出Ultra-Long Output Reinforcement Learning (UloRL)方法。具体包括：1) 将超长输出解码划分为短段，以提高训练效率并缓解长尾样本导致的延迟。2) 引入动态掩蔽已掌握正向Token (MPTs) 以防止熵崩溃。

Result: 实验证明，在Qwen3-30B-A3B模型上，分段回滚RL训练速度提升2.06倍。使用128k-token输出的RL训练使模型在AIME2025上的性能从70.9%提升至85.1%，在BeyondAIME上从50.7%提升至61.9%，甚至超越了Qwen3-235B-A22B。

Conclusion: 本研究提出的方法在提升LLM超长序列生成及推理能力方面展现巨大潜力。作者将发布代码和模型以供社区进一步使用。

Abstract: Recent advances in large language models (LLMs) have highlighted the
potential of reinforcement learning with verifiable rewards (RLVR) to enhance
reasoning capabilities through extended output sequences. However, traditional
RL frameworks face inefficiencies when handling ultra-long outputs due to
long-tail sequence distributions and entropy collapse during training. To
address these challenges, we propose an Ultra-Long Output Reinforcement
Learning (UloRL) approach for advancing large language models' reasoning
abilities. Specifically, we divide ultra long output decoding into short
segments, enabling efficient training by mitigating delays caused by long-tail
samples. Additionally, we introduce dynamic masking of well-Mastered Positive
Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the
effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment
rollout achieved 2.06x increase in training speed, while RL training with
128k-token outputs improves the model's performance on AIME2025 from 70.9\% to
85.1\% and on BeyondAIME from 50.7\% to 61.9\%, even surpassing Qwen3-235B-A22B
with remarkable gains. These findings underscore the potential of our methods
to advance the reasoning capabilities of LLMs with ultra-long sequence
generation. We will release our code and model for further use by the
community.

</details>


### [16] [Flora: Effortless Context Construction to Arbitrary Length and Scale](https://arxiv.org/abs/2507.19786)
*Tianxiang Chen,Zhentao Tan,Xiaofan Bo,Yue Wu,Tao Gong,Qi Chu,Jieping Ye,Nenghai Yu*

Main category: cs.CL

TL;DR: Flora是一种无需人工/LLM干预的长文本构建策略，显著提升LLM长文本能力并保持短文本性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理长上下文时面临挑战，包括长文本稀缺、计算开销大以及短期上下文能力的显著遗忘。现有长上下文构建方法成本高昂（需LLM或人工干预），且在长度和多样性上受限，同时导致LLMs短期上下文性能明显下降。

Method: 本文提出Flora策略，一种无需人工或LLM干预的长上下文构建方法。Flora通过任意组合基于类别的短指令，并指示LLMs基于长上下文元指令生成响应，从而生成任意长度、规模和丰富多样性的长上下文。

Result: 实验在Llama3-8B-Instruct和QwQ-32B模型上进行，结果显示经Flora增强的LLMs在三个长上下文基准测试中表现出色，同时在短上下文任务中保持了强大性能。

Conclusion: Flora提供了一种简单、有效且高效的长上下文构建策略，能够显著提升LLMs处理长文本的能力，同时仅轻微牺牲短上下文性能，解决了当前LLMs在长上下文处理上的关键痛点。

Abstract: Effectively handling long contexts is challenging for Large Language Models
(LLMs) due to the rarity of long texts, high computational demands, and
substantial forgetting of short-context abilities. Recent approaches have
attempted to construct long contexts for instruction tuning, but these methods
often require LLMs or human interventions, which are both costly and limited in
length and diversity. Also, the drop in short-context performances of present
long-context LLMs remains significant. In this paper, we introduce Flora, an
effortless (human/LLM-free) long-context construction strategy. Flora can
markedly enhance the long-context performance of LLMs by arbitrarily assembling
short instructions based on categories and instructing LLMs to generate
responses based on long-context meta-instructions. This enables Flora to
produce contexts of arbitrary length and scale with rich diversity, while only
slightly compromising short-context performance. Experiments on
Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three
long-context benchmarks while maintaining strong performances in short-context
tasks. Our data-construction code is available at
\href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.

</details>


### [17] [HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs](https://arxiv.org/abs/2507.19823)
*Dongquan Yang,Yifan Yang,Xiaotian Yu,Xianbiao Qi,Rong Xiao*

Main category: cs.CL

TL;DR: HCAttention是一种异构注意力计算框架，通过键量化、值卸载和动态KV逐出，显著压缩大型语言模型在长上下文推理时的KV缓存，同时保持高精度，并在极端内存约束下实现了处理400万tokens的能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在处理长上下文输入时，其键值（KV）缓存需要巨大的内存，构成显著挑战。现有KV缓存压缩方法在内存减少超过85%时会表现出明显的性能下降。此外，利用GPU-CPU协同进行近似注意力的策略仍未得到充分探索。

Method: 本文提出了HCAttention，一个异构注意力计算框架。该方法通过整合键量化、值卸载和动态KV逐出技术，以在极端内存约束下实现高效推理。HCAttention兼容现有Transformer架构，且无需进行模型微调。

Result: 在LongBench基准测试中，HCAttention在将KV缓存内存占用缩小到原始大小的25%时，仍能保持全注意力模型的准确性。即使在仅使用12.5%缓存的情况下，其表现仍具竞争力，并在LLM KV缓存压缩领域达到了新的技术水平。HCAttention首次成功将Llama-3-8B模型扩展到在单张80GB内存的A100 GPU上处理400万个tokens。

Conclusion: HCAttention成功地解决了LLM长上下文推理中的KV缓存内存限制问题，通过创新的异构计算框架，在大幅压缩内存的同时保持了模型性能，显著提升了LLM处理超长上下文的能力，并在KV缓存压缩领域树立了新的标杆。

Abstract: Processing long-context inputs with large language models presents a
significant challenge due to the enormous memory requirements of the Key-Value
(KV) cache during inference. Existing KV cache compression methods exhibit
noticeable performance degradation when memory is reduced by more than 85%.
Additionally, strategies that leverage GPU-CPU collaboration for approximate
attention remain underexplored in this setting. We propose HCAttention, a
heterogeneous attention computation framework that integrates key quantization,
value offloading, and dynamic KV eviction to enable efficient inference under
extreme memory constraints. The method is compatible with existing transformer
architectures and does not require model fine-tuning. Experimental results on
the LongBench benchmark demonstrate that our approach preserves the accuracy of
full-attention model while shrinking the KV cache memory footprint to 25% of
its original size. Remarkably, it stays competitive with only 12.5% of the
cache, setting a new state-of-the-art in LLM KV cache compression. To the best
of our knowledge, HCAttention is the first to extend the Llama-3-8B model to
process 4 million tokens on a single A100 GPU with 80GB memory.

</details>


### [18] [DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments](https://arxiv.org/abs/2507.19867)
*Anshul Chavda,M Jagadeesh,Chintalapalli Raja Kullayappa,B Jayaprakash,Medchalimi Sruthi,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 引入了DiscoDrive，一个合成语料库，旨在解决车载对话AI中现有数据集缺乏自然口语不流畅现象的问题，并在训练和数据增强方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有车载对话AI数据集未能捕获真实驾驶员-AI对话中常见的口语不流畅现象（如犹豫、口误、重复、自我修正），这阻碍了AI在实际车载交互中的有效应用。

Method: 提出了DiscoDrive，一个包含3500个多轮对话的合成语料库，涵盖七个汽车领域。该语料库通过一个两阶段、提示驱动的管道生成，在合成过程中动态整合了口语不流畅现象。

Result: 1. 作为训练资源：使DialoGPT-Medium和T5-Base在MultiWOZ 2.2和Schema-Guided Dialogue (SGD) 相关测试集上匹配或超越KVRET训练的模型，BLEU-4、METEOR、ROUGE-L和BERTScore F1等多项指标均有显著提升。
2. 作为数据增强资源：在低资源场景下，与10%的KVRET数据结合时，能带来额外的性能提升。
3. 人工评估：DiscoDrive的对话在自然度和连贯性方面得分高于KVRET的人工收集对话，并且比其他领先的事后处理方法（如LARD）更具上下文适宜性，同时不损害清晰度。

Conclusion: DiscoDrive填补了现有资源的关键空白，作为一个多功能语料库，可用于训练和增强对话AI，使其能够更稳健地处理现实世界中带有口语不流畅现象的车载交互。

Abstract: In-car conversational AI is becoming increasingly critical as autonomous
vehicles and smart assistants gain widespread adoption. Yet, existing datasets
fail to capture the spontaneous disfluencies such as hesitations, false starts,
repetitions, and self-corrections that characterize real driver-AI dialogs. To
address this, we introduce DiscoDrive, a synthetic corpus of 3500 multi-turn
dialogs across seven automotive domains, generated using a two-stage,
prompt-driven pipeline that dynamically integrates disfluencies during
synthesis. We show that DiscoDrive is effective both as a training resource,
enabling DialoGPT-Medium and T5-Base to match or exceed KVRET-trained models on
the MultiWOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets (BLEU-4
improvements of 0.26 to 0.61; METEOR +2.10; ROUGE-L +3.48; BERTScore F1
improvements of 1.35 to 3.48), and as a data augmentation resource in
low-resource scenarios, delivering additional gains of up to BLEU-4 +0.38,
METEOR +1.95, ROUGE-L +2.87, and BERTScore F1 +4.00 when combined with 10
percent of KVRET. Human evaluations further confirm that dialogs sampled from
DiscoDrive are rated higher than KVRET's human-collected dialogs in naturalness
(3.8 vs 3.6) and coherence (4.1 vs 4.0), and are perceived as more
context-appropriate than leading post-hoc methods (such as LARD), without
compromising clarity. DiscoDrive fills a critical gap in existing resources and
serves as a versatile corpus for both training and augmenting conversational
AI, enabling robust handling of real-world, disfluent in-car interactions.

</details>


### [19] [The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment](https://arxiv.org/abs/2507.19869)
*Danil Fokin,Monika Płużyczka,Grigory Golovin*

Main category: cs.CL

TL;DR: 介绍了一种名为PVST的新型波兰语词汇量测试工具，该工具基于项目反应理论和计算机自适应测试，能高效准确地评估波兰语母语及非母语学习者的词汇量。


<details>
  <summary>Details</summary>
Motivation: 开发一种新颖、高效且准确的工具，用于评估波兰语母语者和非母语学习者的接收性词汇量。

Method: 该测试工具基于项目反应理论（IRT）和计算机自适应测试（CAT）设计，能够根据测试者的熟练程度动态调整题目。通过一项包含1475名参与者的初步研究进行了验证。

Result: 研究显示，波兰语母语者的词汇量显著大于非母语者。此外，对于母语者而言，词汇量与年龄之间存在显著正相关。

Conclusion: 成功开发并初步验证了PVST作为一种评估波兰语词汇量的有效工具。该工具已在线可用，可用于评估波兰语母语和非母语学习者的词汇量，并揭示了与年龄相关的词汇量增长模式。

Abstract: We present the Polish Vocabulary Size Test (PVST), a novel tool for assessing
the receptive vocabulary size of both native and non-native Polish speakers.
Based on Item Response Theory and Computerized Adaptive Testing, PVST
dynamically adjusts to each test-taker's proficiency level, ensuring high
accuracy while keeping the test duration short. To validate the test, a pilot
study was conducted with 1.475 participants. Native Polish speakers
demonstrated significantly larger vocabularies compared to non-native speakers.
For native speakers, vocabulary size showed a strong positive correlation with
age. The PVST is available online at myvocab.info/pl.

</details>


### [20] [Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam](https://arxiv.org/abs/2507.19885)
*Cesar Augusto Madid Truyts,Amanda Gomes Rabelo,Gabriel Mesquita de Souza,Daniel Scaldaferri Lages,Adriano Jose Pereira,Uri Adrian Prync Flato,Eduardo Pontes dos Reis,Joaquim Edson Vieira,Paulo Sergio Panse Silveira,Edson Amaro Junior*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）和多模态大型语言模型（MLLMs）在巴西葡萄牙语医疗考试中的表现，发现部分模型达到人类水平，但在多模态和非英语任务上仍有差距，强调了多语言评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医疗领域潜力巨大，但现有模型评估主要集中于英语，可能导致在不同语言环境下的性能偏差。因此，需要评估这些模型在非英语医疗语境中的能力。

Method: 本研究选取了六个LLM和四个MLLM，使用巴西葡萄牙语的圣保罗大学医学院附属医院（HCFMUSP）医学住院医师入学考试问题进行测试。将模型表现与人类考生进行对比，分析了准确性、处理时间和解释的连贯性。

Result: 结果显示，部分模型（特别是Claude-3.5-Sonnet和Claude-3-Opus）的准确率可与人类考生媲美。但在需要图像解读的多模态问题上仍存在性能差距。研究还突出了语言差异，强调非英语医疗AI应用需进一步微调和数据增强。

Conclusion: 研究结果强调了在不同语言和临床环境下评估生成式AI的重要性，以确保其在医疗领域公平可靠的部署。未来研究应探索改进训练方法、多模态推理能力以及AI驱动医疗辅助的实际临床整合。

Abstract: Artificial intelligence (AI) has shown the potential to revolutionize
healthcare by improving diagnostic accuracy, optimizing workflows, and
personalizing treatment plans. Large Language Models (LLMs) and Multimodal
Large Language Models (MLLMs) have achieved notable advancements in natural
language processing and medical applications. However, the evaluation of these
models has focused predominantly on the English language, leading to potential
biases in their performance across different languages.
  This study investigates the capability of six LLMs (GPT-4.0 Turbo,
LLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and
Command R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet,
and Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese
from the medical residency entrance exam of the Hospital das Cl\'inicas da
Faculdade de Medicina da Universidade de S\~ao Paulo (HCFMUSP) - the largest
health complex in South America. The performance of the models was benchmarked
against human candidates, analyzing accuracy, processing time, and coherence of
the generated explanations.
  The results show that while some models, particularly Claude-3.5-Sonnet and
Claude-3-Opus, achieved accuracy levels comparable to human candidates,
performance gaps persist, particularly in multimodal questions requiring image
interpretation. Furthermore, the study highlights language disparities,
emphasizing the need for further fine-tuning and data set augmentation for
non-English medical AI applications.
  Our findings reinforce the importance of evaluating generative AI in various
linguistic and clinical settings to ensure a fair and reliable deployment in
healthcare. Future research should explore improved training methodologies,
improved multimodal reasoning, and real-world clinical integration of AI-driven
medical assistance.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [21] [Tuning adaptive gamma correction (TAGC) for enhancing images in low ligh](https://arxiv.org/abs/2507.19574)
*Ghufran Abualhail Alhamzawi,Ali Saeed Alfoudi,Ali Hakem Alsaeedi,Suha Mohammed Hadi,Amjed Abbas Ahmed,Md. Riad Hassan,Nurhizam Safie Mohd Satar,Waeel Yahya Yasseen*

Main category: cs.CV

TL;DR: 提出一种自适应伽马校正（TAGC）模型，用于自动提升低光图像质量。


<details>
  <summary>Details</summary>
Motivation: 低光照条件导致图像质量下降，表现为对比度低、噪声大和细节模糊，是计算机视觉领域的重要挑战。

Method: 本文提出了一种名为“调整自适应伽马校正”（TAGC）的模型。该模型通过分析低光图像的色彩亮度并计算平均颜色来确定自适应伽马系数。伽马值能够根据图像的不同光照水平自动、自适应地计算，无需人工干预或手动调整。

Result: 通过定性和定量评估，TAGC模型有效改善了低光图像，同时保持了细节、自然对比度和正确的色彩分布，并提供了自然的视觉质量。

Conclusion: TAGC模型被认为是一种更有效的低光图像处理解决方案，可应用于夜间监控、医学图像质量提升和低光环境摄影等多个领域。

Abstract: Enhancing images in low-light conditions is an important challenge in
computer vision. Insufficient illumination negatively affects the quality of
images, resulting in low contrast, intensive noise, and blurred details. This
paper presents a model for enhancing low-light images called tuning adaptive
gamma correction (TAGC). The model is based on analyzing the color luminance of
the low-light image and calculating the average color to determine the adaptive
gamma coefficient. The gamma value is calculated automatically and adaptively
at different illumination levels suitable for the image without human
intervention or manual adjustment. Based on qualitative and quantitative
evaluation, tuning adaptive gamma correction model has effectively improved
low-light images while maintaining details, natural contrast, and correct color
distribution. It also provides natural visual quality. It can be considered a
more efficient solution for processing low-light images in multiple
applications such as night surveillance, improving the quality of medical
images, and photography in low-light environments.

</details>


### [22] [Is Exchangeability better than I.I.D to handle Data Distribution Shifts while Pooling Data for Data-scarce Medical image segmentation?](https://arxiv.org/abs/2507.19575)
*Ayush Roy,Samin Enam,Jun Xia,Vishnu Suresh Lokhande,Won Hwa Kim*

Main category: cs.CV

TL;DR: 针对医学图像分割中数据稀缺和数据增加引发的分布偏移问题，本研究提出一种基于因果框架的方法，通过控制特征差异提升特征表示，在组织病理学和超声图像上实现了最先进的分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像中数据稀缺是深度学习模型的挑战。虽然数据池化和数据增加能提高模型性能，但可能导致分布偏移，即“数据增加困境”，从而损害模型性能。在多源数据背景下，传统的i.i.d.假设不成立，需要更实用的框架。

Method: 在数据集可交换性假设下，研究医学图像分割。借鉴因果框架的见解，提出一种方法，用于控制深度网络所有层中的前景-背景特征差异，以改善数据增加场景下的特征表示。

Result: 该方法在组织病理学和超声图像上的五个数据集（包括一个新策划的超声数据集）中实现了最先进（SOTA）的分割性能。定性结果显示，与主要基线相比，在三种模型架构下产生了更精细、更准确的分割图。

Conclusion: 本研究提出的方法通过改善特征表示，有效解决了“数据增加困境”，并在数据增加场景下的医学图像分割任务中取得了最先进的性能。

Abstract: Data scarcity is a major challenge in medical imaging, particularly for deep
learning models. While data pooling (combining datasets from multiple sources)
and data addition (adding more data from a new dataset) have been shown to
enhance model performance, they are not without complications. Specifically,
increasing the size of the training dataset through pooling or addition can
induce distributional shifts, negatively affecting downstream model
performance, a phenomenon known as the "Data Addition Dilemma". While the
traditional i.i.d. assumption may not hold in multi-source contexts, assuming
exchangeability across datasets provides a more practical framework for data
pooling. In this work, we investigate medical image segmentation under these
conditions, drawing insights from causal frameworks to propose a method for
controlling foreground-background feature discrepancies across all layers of
deep networks. This approach improves feature representations, which are
crucial in data-addition scenarios. Our method achieves state-of-the-art
segmentation performance on histopathology and ultrasound images across five
datasets, including a novel ultrasound dataset that we have curated and
contributed. Qualitative results demonstrate more refined and accurate
segmentation maps compared to prominent baselines across three model
architectures. The code will be available on Github.

</details>


### [23] [T-MPEDNet: Unveiling the Synergy of Transformer-aware Multiscale Progressive Encoder-Decoder Network with Feature Recalibration for Tumor and Liver Segmentation](https://arxiv.org/abs/2507.19590)
*Chandravardhan Singh Raghaw,Jasmer Singh Sanjotra,Mohammad Zia Ur Rehman,Shubhi Bansal,Shahid Shafi Dar,Nagendra Kumar*

Main category: cs.CV

TL;DR: 本文提出T-MPEDNet，一种基于Transformer和多尺度渐进编解码器的新型网络，用于CT扫描中肝脏及其肿瘤的自动化精确分割，在公开数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 肝脏及肿瘤在CT扫描中的精确自动分割对于肝病诊断和治疗至关重要。然而，肿瘤固有的异质性和肝脏视觉特征的多样性为自动化分割带来了显著挑战。

Method: 本文提出了Transformer-aware Multiscale Progressive Encoder-Decoder Network (T-MPEDNet)。该网络利用渐进式编解码结构和跳跃连接实现深度自适应特征提取，并结合Transformer启发的动态注意力机制捕捉长距离上下文关系。此外，还通过多尺度特征利用来细化局部细节，并采用形态学边界细化来处理模糊边界。

Result: T-MPEDNet在LiTS和3DIRCADb两个公开基准数据集上进行了评估，结果显示其性能优于12种现有方法。在LiTS数据集上，肝脏和肿瘤分割的Dice相似系数（DSC）分别达到97.6%和89.1%。在3DIRCADb数据集上，DSC分别为98.3%和83.3%。

Conclusion: 研究结果证明，T-MPEDNet是一个在CT扫描中实现肝脏及其肿瘤自动分割的有效且可靠的框架。

Abstract: Precise and automated segmentation of the liver and its tumor within CT scans
plays a pivotal role in swift diagnosis and the development of optimal
treatment plans for individuals with liver diseases and malignancies. However,
automated liver and tumor segmentation faces significant hurdles arising from
the inherent heterogeneity of tumors and the diverse visual characteristics of
livers across a broad spectrum of patients. Aiming to address these challenges,
we present a novel Transformer-aware Multiscale Progressive Encoder-Decoder
Network (T-MPEDNet) for automated segmentation of tumor and liver. T-MPEDNet
leverages a deep adaptive features backbone through a progressive
encoder-decoder structure, enhanced by skip connections for recalibrating
channel-wise features while preserving spatial integrity. A
Transformer-inspired dynamic attention mechanism captures long-range contextual
relationships within the spatial domain, further enhanced by multi-scale
feature utilization for refined local details, leading to accurate prediction.
Morphological boundary refinement is then employed to address indistinct
boundaries with neighboring organs, capturing finer details and yielding
precise boundary labels. The efficacy of T-MPEDNet is comprehensively assessed
on two widely utilized public benchmark datasets, LiTS and 3DIRCADb. Extensive
quantitative and qualitative analyses demonstrate the superiority of T-MPEDNet
compared to twelve state-of-the-art methods. On LiTS, T-MPEDNet achieves
outstanding Dice Similarity Coefficients (DSC) of 97.6% and 89.1% for liver and
tumor segmentation, respectively. Similar performance is observed on 3DIRCADb,
with DSCs of 98.3% and 83.3% for liver and tumor segmentation, respectively.
Our findings prove that T-MPEDNet is an efficacious and reliable framework for
automated segmentation of the liver and its tumor in CT scans.

</details>


### [24] [SurgPIS: Surgical-instrument-level Instances and Part-level Semantics for Weakly-supervised Part-aware Instance Segmentation](https://arxiv.org/abs/2507.19592)
*Meng Wei,Charlie Budd,Oluwatosin Alabi,Miaojing Shi,Tom Vercauteren*

Main category: cs.CV

TL;DR: 提出了一种名为SurgPIS的统一部分感知实例分割模型，用于解决机器人辅助手术中器械分割的挑战，并在弱监督下实现了多任务的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的手术器械分割方法将器械级实例分割（IIS）和部件级语义分割（PSS）独立处理，缺乏两者之间的交互。这种一致性分割对于机器人辅助手术的自动化至关重要，而现有方法无法满足此需求。

Method: 作者将手术工具分割问题统一表述为部分感知实例分割（PIS），并引入了SurgPIS模型。该模型采用基于Transformer的掩膜分类方法，引入从器械级对象查询派生出的特定部件查询，明确将部件与其父级器械实例关联。为解决缺乏同时包含实例和部件级别标签的大规模数据集的问题，提出了一种弱监督学习策略，允许SurgPIS从仅标注了IIS或PSS的独立数据集中学习。训练时，PIS预测被聚合成IIS或PSS掩膜，以计算针对部分标记数据集的损失。同时，开发了一种学生-教师方法，以维护部分标记数据中缺失PIS信息的预测一致性。

Result: 在多个数据集上进行的广泛实验验证了SurgPIS的有效性，在PIS以及IIS、PSS和器械级语义分割方面均取得了最先进的性能。

Conclusion: SurgPIS首次将手术器械分割问题统一为部分感知实例分割，并通过创新的模型架构和弱监督学习策略，成功解决了多任务联动和数据稀缺的挑战，显著提升了机器人辅助手术中器械分割的一致性和准确性，为自动化奠定了基础。

Abstract: Consistent surgical instrument segmentation is critical for automation in
robot-assisted surgery. Yet, existing methods only treat instrument-level
instance segmentation (IIS) or part-level semantic segmentation (PSS)
separately, without interaction between these tasks. In this work, we formulate
a surgical tool segmentation as a unified part-aware instance segmentation
(PIS) problem and introduce SurgPIS, the first PIS model for surgical
instruments. Our method adopts a transformer-based mask classification approach
and introduces part-specific queries derived from instrument-level object
queries, explicitly linking parts to their parent instrument instances. In
order to address the lack of large-scale datasets with both instance- and
part-level labels, we propose a weakly-supervised learning strategy for SurgPIS
to learn from disjoint datasets labelled for either IIS or PSS purposes. During
training, we aggregate our PIS predictions into IIS or PSS masks, thereby
allowing us to compute a loss against partially labelled datasets. A
student-teacher approach is developed to maintain prediction consistency for
missing PIS information in the partially labelled data, e.g., parts of the IIS
labelled data. Extensive experiments across multiple datasets validate the
effectiveness of SurgPIS, achieving state-of-the-art performance in PIS as well
as IIS, PSS, and instrument-level semantic segmentation.

</details>


### [25] [Object-centric Video Question Answering with Visual Grounding and Referring](https://arxiv.org/abs/2507.19599)
*Haochen Wang,Qirui Chen,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Yao Hu,Weidi Xie,Stratis Gavves*

Main category: cs.CV

TL;DR: 本文提出一种新的VideoLLM模型，通过引入视觉提示和STOM模块，实现了以对象为中心的多轮视频交互，并在多个基准测试中显著超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有VideoLLMs主要专注于高层次理解且仅支持文本响应，缺乏以对象为中心的多轮交互灵活性。

Method: 1. 引入一个VideoLLM模型，支持在视频推理任务中进行对象引用（输入）和定位（输出），允许用户通过文本和视觉提示与视频交互。2. 提出STOM（Spatial-Temporal Overlay Module），一种新颖的方法，可将任意视觉提示从单个时间戳传播到视频的其余帧。3. 构建VideoInfer，一个人工整理的以对象为中心的视频指令数据集，包含需要推理的问答对。

Result: 在VideoInfer及其他现有视频问答和指代对象分割基准上进行综合实验。结果显示，在6项任务的12个基准上，所提出的模型在视频问答和分割方面均持续优于基线模型，验证了其在多模态、以对象为中心的视频和图像理解方面的鲁棒性。

Conclusion: 该研究通过创新的模型设计和数据构建，显著增强了VideoLLMs在以对象为中心的视频理解和多模态交互方面的能力，有效克服了现有模型的局限性。

Abstract: Video Large Language Models (VideoLLMs) have recently demonstrated remarkable
progress in general video understanding. However, existing models primarily
focus on high-level comprehension and are limited to text-only responses,
restricting the flexibility for object-centric, multiround interactions. In
this paper, we make three contributions: (i) we address these limitations by
introducing a VideoLLM model, capable of performing both object referring for
input and grounding for output in video reasoning tasks, i.e., allowing users
to interact with videos using both textual and visual prompts; (ii) we propose
STOM (Spatial-Temporal Overlay Module), a novel approach that propagates
arbitrary visual prompts input at any single timestamp to the remaining frames
within a video; (iii) we present VideoInfer, a manually curated object-centric
video instruction dataset featuring questionanswering pairs that require
reasoning. We conduct comprehensive experiments on VideoInfer and other
existing benchmarks across video question answering and referring object
segmentation. The results on 12 benchmarks of 6 tasks show that our proposed
model consistently outperforms baselines in both video question answering and
segmentation, underscoring its robustness in multimodal, object-centric video
and image understanding. Project page:
https://qirui-chen.github.io/RGA3-release/.

</details>


### [26] [Exemplar Med-DETR: Toward Generalized and Robust Lesion Detection in Mammogram Images and beyond](https://arxiv.org/abs/2507.19621)
*Sheethal Bhat,Bogdan Georgescu,Adarsh Bhandary Panambur,Mathias Zinnen,Tri-Thien Nguyen,Awais Mansoor,Karim Khalifa Elbarbary,Siming Bayer,Florin-Cristian Ghesu,Sasa Grbic,Andreas Maier*

Main category: cs.CV

TL;DR: 本研究提出Exemplar Med-DETR，一种新颖的多模态对比检测器，通过利用类别特异性范例特征和迭代训练，显著提升了医学图像异常检测的鲁棒性和泛化能力，在多种模态上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像中的异常检测面临独特挑战，如特征表示差异和解剖结构与异常的复杂关系。特别是乳腺X线图像中，致密乳腺组织可能掩盖病灶。现有检测方法难以学习有效的类别特异性特征，限制了其在不同任务和成像模态间的适用性。

Method: 引入Exemplar Med-DETR，这是一种新型的多模态对比检测器，可实现基于特征的检测。它采用交叉注意力机制，结合内在地推导出的、直观的类别特异性范例特征，并通过迭代策略进行训练。

Result: 在三种不同成像模态的四个公共数据集上实现了最先进的性能。在越南致密型乳腺X线图像上，肿块检测mAP达0.7，钙化检测mAP达0.55，绝对提升16个百分点。对来自中国异分布队列的100张乳腺X线图像进行放射科医生支持的评估，病灶检测性能提升两倍。在胸部X线和血管造影中，肿块检测mAP达0.25，狭窄检测mAP达0.37，分别提升4和7个百分点。

Conclusion: 研究结果表明，本方法有望推动医学成像领域开发出更鲁棒和泛化性强的检测系统。

Abstract: Detecting abnormalities in medical images poses unique challenges due to
differences in feature representations and the intricate relationship between
anatomical structures and abnormalities. This is especially evident in
mammography, where dense breast tissue can obscure lesions, complicating
radiological interpretation. Despite leveraging anatomical and semantic
context, existing detection methods struggle to learn effective class-specific
features, limiting their applicability across different tasks and imaging
modalities. In this work, we introduce Exemplar Med-DETR, a novel multi-modal
contrastive detector that enables feature-based detection. It employs
cross-attention with inherently derived, intuitive class-specific exemplar
features and is trained with an iterative strategy. We achieve state-of-the-art
performance across three distinct imaging modalities from four public datasets.
On Vietnamese dense breast mammograms, we attain an mAP of 0.7 for mass
detection and 0.55 for calcifications, yielding an absolute improvement of 16
percentage points. Additionally, a radiologist-supported evaluation of 100
mammograms from an out-of-distribution Chinese cohort demonstrates a twofold
gain in lesion detection performance. For chest X-rays and angiography, we
achieve an mAP of 0.25 for mass and 0.37 for stenosis detection, improving
results by 4 and 7 percentage points, respectively. These results highlight the
potential of our approach to advance robust and generalizable detection systems
for medical imaging.

</details>


### [27] [Pre- and Post-Treatment Glioma Segmentation with the Medical Imaging Segmentation Toolkit](https://arxiv.org/abs/2507.19626)
*Adrian Celaya,Tucker Netherton,Dawid Schellingerhout,Caroline Chung,Beatrice Riviere,David Fuentes*

Main category: cs.CV

TL;DR: MIST是一个灵活的医学图像分割后处理工具包，旨在为BraTS 2025挑战提供标准化、可定制的解决方案，通过模块化策略提高分割质量。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法因缺乏标准化和可定制的工具，导致难以进行严格比较。

Method: 介绍并扩展了医学图像分割工具包（MIST）的后处理框架。该框架支持多种可组合的变换操作（如小对象移除、最大连通分量提取、形态学操作），可组成用户定义策略。通过BraTS排名协议评估了三种此类策略。

Result: MIST促进了快速实验和目标性优化，最终为BraTS 2025挑战赛生成了高质量的分割结果。

Conclusion: MIST是开源且可扩展的，支持医学图像分割领域的可重复和可扩展研究，为解决工具缺乏问题提供了有效途径。

Abstract: Medical image segmentation continues to advance rapidly, yet rigorous
comparison between methods remains challenging due to a lack of standardized
and customizable tooling. In this work, we present the current state of the
Medical Imaging Segmentation Toolkit (MIST), with a particular focus on its
flexible and modular postprocessing framework designed for the BraTS 2025 pre-
and post-treatment glioma segmentation challenge. Since its debut in the 2024
BraTS adult glioma post-treatment segmentation challenge, MIST's postprocessing
module has been significantly extended to support a wide range of transforms,
including removal or replacement of small objects, extraction of the largest
connected components, and morphological operations such as hole filling and
closing. These transforms can be composed into user-defined strategies,
enabling fine-grained control over the final segmentation output. We evaluate
three such strategies - ranging from simple small-object removal to more
complex, class-specific pipelines - and rank their performance using the BraTS
ranking protocol. Our results highlight how MIST facilitates rapid
experimentation and targeted refinement, ultimately producing high-quality
segmentations for the BraTS 2025 challenge. MIST remains open source and
extensible, supporting reproducible and scalable research in medical image
segmentation.

</details>


### [28] [SynPAIN: A Synthetic Dataset of Pain and Non-Pain Facial Expressions](https://arxiv.org/abs/2507.19673)
*Babak Taati,Muhammad Muzammil,Yasamin Zarghami,Abhishek Moturu,Airhossein Kazerouni,Hailey Reimer,Alex Mihailidis,Thomas Hadjistavropoulos*

Main category: cs.CV

TL;DR: SynPAIN是一个大规模、人口统计学多样化的合成数据集，专门用于老年人疼痛检测，旨在解决现有数据集的不足，并能帮助识别及缓解算法偏差，提高疼痛检测模型性能。


<details>
  <summary>Details</summary>
Motivation: 对于难以沟通的患者（如痴呆老年人）进行准确的疼痛评估是一项关键的医疗挑战。虽然自动化疼痛检测系统有帮助，但现有数据集存在种族/民族多样性有限、隐私限制以及老年人样本不足的问题，而老年人正是临床部署的主要目标人群。

Method: 我们提出了SynPAIN，一个包含10,710张面部表情图像（5,355对中性/表现对）的大规模合成数据集。该数据集覆盖了五种民族/种族、两个年龄组（年轻：20-35岁，老年：75岁以上）和两种性别。通过使用商业生成式AI工具，我们创建了人口统计学平衡且具有临床意义的疼痛表情合成身份。

Result: 验证结果表明，合成疼痛表情表现出预期的疼痛模式，使用临床验证的疼痛评估工具，其得分显著高于中性及非疼痛表情。实验证明SynPAIN有助于识别现有疼痛检测模型中的算法偏差，揭示了不同人口特征之间的显著性能差异。此外，年龄匹配的合成数据增强可将真实临床数据的疼痛检测平均精度提高7.0%。

Conclusion: SynPAIN通过提供首个公开可用的、人口统计学多样化的合成数据集，专门针对老年人疼痛检测，解决了疼痛评估研究中的关键空白。同时，它也为测量和缓解算法偏差提供了一个框架。

Abstract: Accurate pain assessment in patients with limited ability to communicate,
such as older adults with dementia, represents a critical healthcare challenge.
Robust automated systems of pain detection may facilitate such assessments.
Existing pain detection datasets, however, suffer from limited ethnic/racial
diversity, privacy constraints, and underrepresentation of older adults who are
the primary target population for clinical deployment. We present SynPAIN, a
large-scale synthetic dataset containing 10,710 facial expression images (5,355
neutral/expressive pairs) across five ethnicities/races, two age groups (young:
20-35, old: 75+), and two genders. Using commercial generative AI tools, we
created demographically balanced synthetic identities with clinically
meaningful pain expressions. Our validation demonstrates that synthetic pain
expressions exhibit expected pain patterns, scoring significantly higher than
neutral and non-pain expressions using clinically validated pain assessment
tools based on facial action unit analysis. We experimentally demonstrate
SynPAIN's utility in identifying algorithmic bias in existing pain detection
models. Through comprehensive bias evaluation, we reveal substantial
performance disparities across demographic characteristics. These performance
disparities were previously undetectable with smaller, less diverse datasets.
Furthermore, we demonstrate that age-matched synthetic data augmentation
improves pain detection performance on real clinical data, achieving a 7.0%
improvement in average precision. SynPAIN addresses critical gaps in pain
assessment research by providing the first publicly available, demographically
diverse synthetic dataset specifically designed for older adult pain detection,
while establishing a framework for measuring and mitigating algorithmic bias.
The dataset is available at https://doi.org/10.5683/SP3/WCXMAP

</details>


### [29] [Efficient Learning for Product Attributes with Compact Multimodal Models](https://arxiv.org/abs/2507.19679)
*Mandar Kulkarni*

Main category: cs.CV

TL;DR: 本研究提出一种标签高效的半监督微调策略，利用无标签产品列表数据和DPO（Direct Preference Optimization）来提升电商图像商品属性预测中紧凑型视觉语言模型（VLM）的性能。


<details>
  <summary>Details</summary>
Motivation: 电商中基于图像的商品属性预测至关重要，但监督式微调VLM面临高昂的手动或API标注成本带来的规模挑战。

Method: 首先，从少量API标注的数据集开始，使用PEFT（Parameter-Efficient Fine-Tuning）训练低秩适配器模块。接着，利用无标签数据，为每个无标签样本生成多条推理-回答链，并基于“自洽性”将其分为偏好和非偏好链。然后，使用DPO损失对模型进行微调，并用更新后的模型进行下一轮迭代。该方法通过结合PEFT和DPO实现高效收敛。

Result: 该方法实现了高效收敛，且计算开销极小。在涵盖12个电商垂直领域的数据集上，仅利用无标签数据的DPO微调显著优于监督模型。此外，实验表明DPO训练的准确性随无标签数据量的增加而提高。

Conclusion: 大量无标签样本可以被有效利用来显著提高电商图像商品属性预测任务的性能，DPO结合PEFT是一种有效的半监督微调方法。

Abstract: Image-based product attribute prediction in e-commerce is a crucial task with
numerous applications. The supervised fine-tuning of Vision Language Models
(VLMs) faces significant scale challenges due to the cost of manual or API
based annotation. In this paper, we investigate label-efficient semi-supervised
fine-tuning strategies for compact VLMs (2B-3B parameters) that leverage
unlabeled product listings through Direct Preference Optimization (DPO).
Beginning with a small, API-based, annotated, and labeled set, we first employ
PEFT to train low-rank adapter modules. To update the adapter weights with
unlabeled data, we generate multiple reasoning-and-answer chains per unlabeled
sample and segregate these chains into preferred and dispreferred based on
self-consistency. We then fine-tune the model with DPO loss and use the updated
model for the next iteration. By using PEFT fine-tuning with DPO, our method
achieves efficient convergence with minimal compute overhead. On a dataset
spanning twelve e-commerce verticals, DPO-based fine-tuning, which utilizes
only unlabeled data, demonstrates a significant improvement over the supervised
model. Moreover, experiments demonstrate that accuracy with DPO training
improves with more unlabeled data, indicating that a large pool of unlabeled
samples can be effectively leveraged to improve performance.

</details>


### [30] [DeepJIVE: Learning Joint and Individual Variation Explained from Multimodal Data Using Deep Learning](https://arxiv.org/abs/2507.19682)
*Matthew Drexler,Benjamin Risk,James J Lah,Suprateek Kundu,Deqiang Qiu*

Main category: cs.CV

TL;DR: DeepJIVE是一种深度学习方法，用于多模态数据分析，旨在解决传统方法在高维和非线性结构处理上的局限性，并成功揭示联合与个体变异。


<details>
  <summary>Details</summary>
Motivation: 传统多模态数据集成方法在处理高维数据和识别非线性结构方面存在局限性。

Method: 本文提出DeepJIVE，一种基于深度学习的联合与个体方差解释（JIVE）方法。该方法进行了数学推导，并在合成和真实的一维、二维、三维数据集上进行了实验验证。同时，探索了实现DeepJIVE恒等和正交约束的不同策略，并导出了三种可行的损失函数。

Result: DeepJIVE成功揭示了多模态数据集的联合和个体变异。将其应用于阿尔茨海默病神经影像学计划（ADNI）数据集，还识别出了淀粉样正电子发射断层扫描（PET）和磁共振（MR）图像之间具有生物学合理性的协变模式。

Conclusion: 所提出的DeepJIVE可以成为多模态数据分析的有用工具。

Abstract: Conventional multimodal data integration methods provide a comprehensive
assessment of the shared or unique structure within each individual data type
but suffer from several limitations such as the inability to handle
high-dimensional data and identify nonlinear structures. In this paper, we
introduce DeepJIVE, a deep-learning approach to performing Joint and Individual
Variance Explained (JIVE). We perform mathematical derivation and experimental
validations using both synthetic and real-world 1D, 2D, and 3D datasets.
Different strategies of achieving the identity and orthogonality constraints
for DeepJIVE were explored, resulting in three viable loss functions. We found
that DeepJIVE can successfully uncover joint and individual variations of
multimodal datasets. Our application of DeepJIVE to the Alzheimer's Disease
Neuroimaging Initiative (ADNI) also identified biologically plausible
covariation patterns between the amyloid positron emission tomography (PET) and
magnetic resonance (MR) images. In conclusion, the proposed DeepJIVE can be a
useful tool for multimodal data analysis.

</details>


### [31] [Co-Win: Joint Object Detection and Instance Segmentation in LiDAR Point Clouds via Collaborative Window Processing](https://arxiv.org/abs/2507.19691)
*Haichuan Li,Tomi Westerlund*

Main category: cs.CV

TL;DR: Co-Win是一个新颖的BEV感知框架，通过结合点云编码和并行窗口特征提取，实现自动驾驶中复杂城市环境的精确感知和细粒度场景理解。


<details>
  <summary>Details</summary>
Motivation: 为确保自动驾驶的安全高效，在复杂的城市环境中实现精确感知和场景理解是一个关键挑战。

Method: 本文提出Co-Win，一个新颖的鸟瞰图（BEV）感知框架。它整合了点云编码与高效并行窗口特征提取，以处理环境多模态性。Co-Win采用分层架构，包括专用编码器、基于窗口的主干网络和基于查询的解码器头部。与以往方法不同，Co-Win引入了变分方法和基于掩码的实例分割，以实现细粒度的场景分解和理解。该架构通过渐进式特征提取阶段处理点云数据。

Result: Co-Win能够有效捕获多样空间特征和物体关系，确保预测的掩码既数据一致又与上下文相关。它能产生可解释且多样的实例预测，并实现细粒度的场景分解和理解。

Conclusion: Co-Win极大地增强了自动驾驶系统中的下游决策和规划能力。

Abstract: Accurate perception and scene understanding in complex urban environments is
a critical challenge for ensuring safe and efficient autonomous navigation. In
this paper, we present Co-Win, a novel bird's eye view (BEV) perception
framework that integrates point cloud encoding with efficient parallel
window-based feature extraction to address the multi-modality inherent in
environmental understanding. Our method employs a hierarchical architecture
comprising a specialized encoder, a window-based backbone, and a query-based
decoder head to effectively capture diverse spatial features and object
relationships. Unlike prior approaches that treat perception as a simple
regression task, our framework incorporates a variational approach with
mask-based instance segmentation, enabling fine-grained scene decomposition and
understanding. The Co-Win architecture processes point cloud data through
progressive feature extraction stages, ensuring that predicted masks are both
data-consistent and contextually relevant. Furthermore, our method produces
interpretable and diverse instance predictions, enabling enhanced downstream
decision-making and planning in autonomous driving systems.

</details>


### [32] [Bias Analysis for Synthetic Face Detection: A Case Study of the Impact of Facial Attribute](https://arxiv.org/abs/2507.19705)
*Asmae Lamsaf,Lucia Cascone,Hugo Proença,João Neves*

Main category: cs.CV

TL;DR: 分析合成人脸检测模型中的偏见及其来源，并提出评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有合成人脸检测模型及其训练数据可能存在偏见，导致对特定人群的检测失败，从而引发社会、法律和伦理问题，但这一点常被忽视。

Method: 引入一个评估框架，该框架利用生成均匀分布属性标签的合成数据来分析合成人脸检测器的偏见。通过对五种SOTA检测器在25种受控面部属性上的案例研究，并结合训练集属性平衡分析和激活图分析来揭示偏见来源。

Result: 研究证实，合成人脸检测器普遍对特定面部属性的存在/缺失存在偏见。此外，研究揭示了观测到的偏见来源于训练数据集中人脸属性的不平衡以及检测器激活模式。

Conclusion: 合成人脸检测器的偏见是一个普遍存在的问题，其根源在于训练数据集中属性的不平衡。这项工作为未来开发更公平、更可靠的检测模型提供了关键见解。

Abstract: Bias analysis for synthetic face detection is bound to become a critical
topic in the coming years. Although many detection models have been developed
and several datasets have been released to reliably identify synthetic content,
one crucial aspect has been largely overlooked: these models and training
datasets can be biased, leading to failures in detection for certain
demographic groups and raising significant social, legal, and ethical issues.
In this work, we introduce an evaluation framework to contribute to the
analysis of bias of synthetic face detectors with respect to several facial
attributes. This framework exploits synthetic data generation, with evenly
distributed attribute labels, for mitigating any skew in the data that could
otherwise influence the outcomes of bias analysis. We build on the proposed
framework to provide an extensive case study of the bias level of five
state-of-the-art detectors in synthetic datasets with 25 controlled facial
attributes. While the results confirm that, in general, synthetic face
detectors are biased towards the presence/absence of specific facial
attributes, our study also sheds light on the origins of the observed bias
through the analysis of the correlations with the balancing of facial
attributes in the training sets of the detectors, and the analysis of detectors
activation maps in image pairs with controlled attribute modifications.

</details>


### [33] [Quaternion-Based Robust PCA for Efficient Moving Target Detection and Background Recovery in Color Videos](https://arxiv.org/abs/2507.19730)
*Liyang Wang,Shiqian Wu,Shun Fang,Qile Zhu,Jiaxin Wu,Sos Again*

Main category: cs.CV

TL;DR: 本文提出了一种名为uQRPCA及其增强版uQRPCA+的框架，通过优化四元数奇异值分解（QSVD）并引入颜色秩1批处理（CR1B）方法，解决了彩色视频中移动目标检测和背景恢复的挑战，并取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 移动目标检测在静态摄像机捕获的复杂彩色视频中是一项具有挑战性的计算机视觉任务。现有基于四元数的鲁棒主成分分析（QRPCA）在处理彩色视频时面临两大问题：四元数奇异值分解（QSVD）的计算成本高昂，以及秩1四元数矩阵无法生成秩1彩色通道。同时提取背景和目标以生成合成数据，对于丰富标注数据集和增强深度模型的泛化能力至关重要，因此需要克服这些技术障碍。

Method: 本研究通过以下方法进行：1. 利用四元数黎曼流形将QSVD的计算复杂度降低至o(1)。2. 提出了通用QRPCA (uQRPCA) 框架，旨在同时平衡彩色视频中的目标分割和背景恢复。3. 进一步引入颜色秩1批处理 (CR1B) 方法，将uQRPCA扩展为uQRPCA+，以处理并获得跨颜色通道的理想低秩背景。

Result: 实验结果表明，与现有开源方法相比，作者提出的uQRPCA+在移动目标检测和背景恢复任务上均达到了最先进（SOTA）的性能。

Conclusion: 本研究成功地通过优化QSVD计算和提出uQRPCA/uQRPCA+框架，解决了彩色视频中移动目标检测和背景恢复的难题。所提出的方法不仅提高了效率，还在性能上超越了现有方案，为生成高质量合成数据和提升深度模型泛化能力提供了有效途径。

Abstract: Moving target detection is a challenging computer vision task aimed at
generating accurate segmentation maps in diverse in-the-wild color videos
captured by static cameras. If backgrounds and targets can be simultaneously
extracted and recombined, such synthetic data can significantly enrich
annotated in-the-wild datasets and enhance the generalization ability of deep
models. Quaternion-based RPCA (QRPCA) is a promising unsupervised paradigm for
color image processing. However, in color video processing, Quaternion Singular
Value Decomposition (QSVD) incurs high computational costs, and rank-1
quaternion matrix fails to yield rank-1 color channels. In this paper, we
reduce the computational complexity of QSVD to o(1) by utilizing a quaternion
Riemannian manifold. Furthermor, we propose the universal QRPCA (uQRPCA)
framework, which achieves a balance in simultaneously segmenting targets and
recovering backgrounds from color videos. Moreover, we expand to uQRPCA+ by
introducing the Color Rank-1 Batch (CR1B) method to further process and obtain
the ideal low-rank background across color channels. Experiments demonstrate
our uQRPCA+ achieves State Of The Art (SOTA) performance on moving target
detection and background recovery tasks compared to existing open-source
methods. Our implementation is publicly available on GitHub at
https://github.com/Ruchtech/uQRPCA

</details>


### [34] [Leveraging Sparse LiDAR for RAFT-Stereo: A Depth Pre-Fill Perspective](https://arxiv.org/abs/2507.19738)
*Jinsu Yoo,Sooyoung Jeon,Zanming Huang,Tai-Yu Pan,Wei-Lun Chao*

Main category: cs.CV

TL;DR: 研究了激光雷达稀疏时RAFT-Stereo性能下降问题，提出通过插值预填充初始视差图和图像特征来解决，形成GRAFT-Stereo，显著提升了稀疏激光雷达下的立体匹配精度。


<details>
  <summary>Details</summary>
Motivation: 旨在通过注入精确的激光雷达深度信息来提高RAFT-Stereo立体匹配的精度，并解决激光雷达点稀疏时引导效果显著下降的问题。

Method: 首先从信号处理角度解释了稀疏激光雷达数据导致引导效果下降的原因；提出通过插值预填充稀疏初始视差图；进一步发现预填充对通过早期融合将激光雷达深度注入图像特征也有效，但需不同方法；最终结合两种预填充方案，提出Guided RAFT-Stereo (GRAFT-Stereo)模型。

Result: 所提出的GRAFT-Stereo在多种数据集和稀疏激光雷达条件下，显著优于现有激光雷达引导方法。

Conclusion: 本研究为稀疏激光雷达引导的立体匹配提供了新颖的见解和简单有效的解决方案，有望启发更多高效的激光雷达引导立体方法。

Abstract: We investigate LiDAR guidance within the RAFT-Stereo framework, aiming to
improve stereo matching accuracy by injecting precise LiDAR depth into the
initial disparity map. We find that the effectiveness of LiDAR guidance
drastically degrades when the LiDAR points become sparse (e.g., a few hundred
points per frame), and we offer a novel explanation from a signal processing
perspective. This insight leads to a surprisingly simple solution that enables
LiDAR-guided RAFT-Stereo to thrive: pre-filling the sparse initial disparity
map with interpolation. Interestingly, we find that pre-filling is also
effective when injecting LiDAR depth into image features via early fusion, but
for a fundamentally different reason, necessitating a distinct pre-filling
approach. By combining both solutions, the proposed Guided RAFT-Stereo
(GRAFT-Stereo) significantly outperforms existing LiDAR-guided methods under
sparse LiDAR conditions across various datasets. We hope this study inspires
more effective LiDAR-guided stereo methods.

</details>


### [35] [Latest Object Memory Management for Temporally Consistent Video Instance Segmentation](https://arxiv.org/abs/2507.19754)
*Seunghun Lee,Jiwan Seo,Minwoo Choi,Kiljoon Han,Jaehoon Jeong,Zane Durante,Ehsan Adeli,Sang Hyun Park,Sunghoon Im*

Main category: cs.CV

TL;DR: 提出LOMM框架，通过创新的内存管理和对象关联策略，显著提升了视频实例分割的长期跟踪能力和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 解决视频实例分割（VIS）中长期实例跟踪的时间一致性问题。

Method: 核心是“最新对象内存（LOM）”，通过显式建模对象在每帧中的存在来鲁棒跟踪并持续更新对象最新状态。此外，引入“解耦对象关联（DOA）”策略，单独处理新出现和已存在对象，利用内存系统精确分配对象索引，确保动态场景下的稳定身份一致性。

Result: 广泛的实验和消融研究表明，该方法优于传统方法，并在VIS领域设立了新基准。LOMM在挑战性的YouTube-VIS 2022数据集上达到了54.0的最新AP分数。

Conclusion: LOMM通过LOM和DOA等创新机制，有效解决了视频实例分割中的长期跟踪和时间一致性问题，取得了最先进的性能，为VIS领域树立了新标杆。

Abstract: In this paper, we present Latest Object Memory Management (LOMM) for
temporally consistent video instance segmentation that significantly improves
long-term instance tracking. At the core of our method is Latest Object Memory
(LOM), which robustly tracks and continuously updates the latest states of
objects by explicitly modeling their presence in each frame. This enables
consistent tracking and accurate identity management across frames, enhancing
both performance and reliability through the VIS process. Moreover, we
introduce Decoupled Object Association (DOA), a strategy that separately
handles newly appearing and already existing objects. By leveraging our memory
system, DOA accurately assigns object indices, improving matching accuracy and
ensuring stable identity consistency, even in dynamic scenes where objects
frequently appear and disappear. Extensive experiments and ablation studies
demonstrate the superiority of our method over traditional approaches, setting
a new benchmark in VIS. Notably, our LOMM achieves state-of-the-art AP score of
54.0 on YouTube-VIS 2022, a dataset known for its challenging long videos.
Project page: https://seung-hun-lee.github.io/projects/LOMM/

</details>


### [36] [MoFRR: Mixture of Diffusion Models for Face Retouching Restoration](https://arxiv.org/abs/2507.19770)
*Jiaxin Liu,Qichao Ying,Zhenxing Qian,Sheng Li,Runqi Zhang,Jian Liu,Xinpeng Zhang*

Main category: cs.CV

TL;DR: 提出一种新任务——面部修图恢复（FRR），旨在还原修图后的原始面部。为此，提出MoFRR模型，一个基于扩散模型的混合专家系统，通过专门和共享专家处理不同修图类型，并在新建数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上广泛使用的面部修图引发了对图像真实性的担忧。现有方法侧重于检测修图，但如何准确地从修图图像中恢复原始面部仍是一个未解决的问题。本文旨在填补这一空白，引入面部修图恢复（FRR）这一新型计算机视觉任务。

Method: 提出MoFRR（Mixture of Diffusion Models for FRR）模型。MoFRR借鉴DeepSeek的专家隔离策略，采用稀疏激活的专业专家来处理不同的修图类型，并结合一个处理通用修图痕迹的共享专家。每个专业专家都采用双分支结构：一个由迭代失真评估模块（IDEM）指导的基于DDIM的低频分支，以及一个用于细节细化的基于交叉注意力的HFCAM高频分支。

Result: 在新建的面部修图数据集RetouchingFFHQ++上进行了广泛实验，结果表明MoFRR在面部修图恢复任务（FRR）上表现出有效性。

Conclusion: 本文成功引入并定义了面部修图恢复（FRR）这一新型任务，并提出了有效的MoFRR模型，其在专门构建的数据集上验证了性能，为从修图图像中恢复原始面部提供了新的解决方案。

Abstract: The widespread use of face retouching on social media platforms raises
concerns about the authenticity of face images. While existing methods focus on
detecting face retouching, how to accurately recover the original faces from
the retouched ones has yet to be answered. This paper introduces Face
Retouching Restoration (FRR), a novel computer vision task aimed at restoring
original faces from their retouched counterparts. FRR differs from traditional
image restoration tasks by addressing the complex retouching operations with
various types and degrees, which focuses more on the restoration of the
low-frequency information of the faces. To tackle this challenge, we propose
MoFRR, Mixture of Diffusion Models for FRR. Inspired by DeepSeek's expert
isolation strategy, the MoFRR uses sparse activation of specialized experts
handling distinct retouching types and the engagement of a shared expert
dealing with universal retouching traces. Each specialized expert follows a
dual-branch structure with a DDIM-based low-frequency branch guided by an
Iterative Distortion Evaluation Module (IDEM) and a Cross-Attention-based
High-Frequency branch (HFCAM) for detail refinement. Extensive experiments on a
newly constructed face retouching dataset, RetouchingFFHQ++, demonstrate the
effectiveness of MoFRR for FRR.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [37] [MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation](https://arxiv.org/abs/2507.19489)
*Simone Bendazzoli,Sanna Persson,Mehdi Astaraki,Sebastian Pettersson,Vitali Grozman,Rodrigo Moreno*

Main category: cs.AI

TL;DR: MAIA是一个基于Kubernetes的开源平台，旨在通过提供协作和自动化工具，加速AI研究在临床医疗中的应用。


<details>
  <summary>Details</summary>
Motivation: 将人工智能（AI）整合到临床工作流中，需要强大的协作平台来弥合技术创新与实际医疗应用之间的鸿沟。

Method: 本文引入了MAIA（Medical Artificial Intelligence Assistant），一个基于Kubernetes构建的开源平台。它提供模块化、可扩展的环境，集成了数据管理、模型开发、标注、部署和临床反馈工具，并支持项目隔离、CI/CD自动化以及与高性能计算基础设施和临床工作流的集成。

Result: MAIA已在学术和临床环境中部署，支持医学影像AI的实际用例，并在KTH皇家理工学院和卡罗林斯卡大学医院的不同项目中展示了其应用。

Conclusion: MAIA旨在通过促进协作和互操作性，加速AI研究向有影响力的临床解决方案的转化，同时提升可重复性、透明度和以用户为中心的设计。

Abstract: The integration of Artificial Intelligence (AI) into clinical workflows
requires robust collaborative platforms that are able to bridge the gap between
technical innovation and practical healthcare applications. This paper
introduces MAIA (Medical Artificial Intelligence Assistant), an open-source
platform designed to facilitate interdisciplinary collaboration among
clinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a
modular, scalable environment with integrated tools for data management, model
development, annotation, deployment, and clinical feedback. Key features
include project isolation, CI/CD automation, integration with high-computing
infrastructures and in clinical workflows. MAIA supports real-world use cases
in medical imaging AI, with deployments in both academic and clinical
environments. By promoting collaborations and interoperability, MAIA aims to
accelerate the translation of AI research into impactful clinical solutions
while promoting reproducibility, transparency, and user-centered design. We
showcase the use of MAIA with different projects, both at KTH Royal Institute
of Technology and Karolinska University Hospital.

</details>


### [38] [Agent WARPP: Workflow Adherence via Runtime Parallel Personalization](https://arxiv.org/abs/2507.19543)
*Maria Emilia Mazzolenis,Ruirui Zhang*

Main category: cs.AI

TL;DR: WARPP是一个无训练框架，通过运行时并行个性化提升LLM在复杂任务型对话中对条件工作流的遵循度，性能优于基线，并降低了token消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在任务型对话（TOD）系统中处理涉及外部工具调用和用户特定信息的长、条件性工作流时，常常难以遵循既定流程。

Method: 本文提出了WARPP（Workflow Adherence via Runtime Parallel Personalization）框架，一个无需训练、模块化的框架。它结合了多智能体协调和运行时个性化，采用并行化架构。一个专用的个性化代理（Personalizer agent）与模块化、领域特定的代理并行工作，根据用户属性动态裁剪条件分支，实时调整执行路径，从而减少推理开销并缩小工具选择范围。

Result: 在银行、航空、医疗三个领域的五种不同复杂度的用户意图中进行了评估。结果表明，WARPP性能优于非个性化方法和ReAct基线，随着意图复杂性的增加，其在参数保真度和工具准确性方面的提升更为显著，同时还降低了平均token使用量，且无需额外训练。

Conclusion: WARPP通过运行时个性化和多智能体协调，有效解决了LLMs在复杂任务型对话工作流中依从性不足的问题，显著提升了性能和准确性，同时降低了资源消耗，且无需额外训练，展现了其在实际应用中的潜力。

Abstract: Large language models (LLMs) are increasingly applied in task-oriented
dialogue (TOD) systems but often struggle with long, conditional workflows that
involve external tool calls and depend on user-specific information. We present
Workflow Adherence via Runtime Parallel Personalization, or WARPP, a
training-free, modular framework that combines multi-agent orchestration with
runtime personalization to improve workflow adherence in LLM-based systems. By
dynamically pruning conditional branches based on user attributes, the
framework reduces reasoning overhead and narrows tool selection at runtime.
WARPP deploys a parallelized architecture where a dedicated Personalizer agent
operates alongside modular, domain-specific agents to dynamically tailor
execution paths in real time. The framework is evaluated across five
representative user intents of varying complexity within three domains:
banking, flights, and healthcare. Our evaluation leverages synthetic datasets
and LLM-powered simulated users to test scenarios with conditional
dependencies. Our results demonstrate that WARPP outperforms both the
non-personalized method and the ReAct baseline, achieving increasingly larger
gains in parameter fidelity and tool accuracy as intent complexity grows, while
also reducing average token usage, without any additional training.

</details>


### [39] [Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems](https://arxiv.org/abs/2507.19593)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

TL;DR: 本研究对超博弈理论在多智能体系统（MAS）中的应用进行了系统综述，分析了其在建模智能体主观感知方面的能力，并指出了当前趋势、研究空白和未来方向。


<details>
  <summary>Details</summary>
Motivation: 经典的博弈论模型假设过于理想化，无法有效处理现实世界多智能体系统中普遍存在的不确定性、感知错位和嵌套信念等问题。超博弈理论被提出以显式建模智能体对战略情景的主观感知，本研究旨在系统审查其在动态、交互式MAS背景下的应用。

Method: 本研究对44项来自网络安全、机器人、社会仿真、通信和一般博弈论建模领域的超博弈理论智能体兼容应用进行了系统回顾。基于对超博弈理论及其分层超博弈和HNF两个主要扩展的正式介绍，研究开发了智能体兼容性标准和基于智能体的分类框架，以评估其整合模式和实际适用性。

Result: 分析揭示了在欺骗性推理中分层和基于图的模型普遍存在，以及复杂理论框架在实际应用中被简化的主流趋势。同时，也识别出结构性缺陷，包括HNF模型的有限采用、缺乏形式化的超博弈语言，以及在建模人-智能体和智能体-智能体错位方面的未探索机会。

Conclusion: 本综述通过综合分析当前趋势、挑战和开放研究方向，为将超博弈理论应用于增强动态多智能体环境中战略建模的真实性和有效性提供了一个新的路线图。

Abstract: Classical game-theoretic models typically assume rational agents, complete
information, and common knowledge of payoffs - assumptions that are often
violated in real-world MAS characterized by uncertainty, misaligned
perceptions, and nested beliefs. To overcome these limitations, researchers
have proposed extensions that incorporate models of cognitive constraints,
subjective beliefs, and heterogeneous reasoning. Among these, hypergame theory
extends the classical paradigm by explicitly modeling agents' subjective
perceptions of the strategic scenario, known as perceptual games, in which
agents may hold divergent beliefs about the structure, payoffs, or available
actions. We present a systematic review of agent-compatible applications of
hypergame theory, examining how its descriptive capabilities have been adapted
to dynamic and interactive MAS contexts. We analyze 44 selected studies from
cybersecurity, robotics, social simulation, communications, and general
game-theoretic modeling. Building on a formal introduction to hypergame theory
and its two major extensions - hierarchical hypergames and HNF - we develop
agent-compatibility criteria and an agent-based classification framework to
assess integration patterns and practical applicability. Our analysis reveals
prevailing tendencies, including the prevalence of hierarchical and graph-based
models in deceptive reasoning and the simplification of extensive theoretical
frameworks in practical applications. We identify structural gaps, including
the limited adoption of HNF-based models, the lack of formal hypergame
languages, and unexplored opportunities for modeling human-agent and
agent-agent misalignment. By synthesizing trends, challenges, and open research
directions, this review provides a new roadmap for applying hypergame theory to
enhance the realism and effectiveness of strategic modeling in dynamic
multi-agent environments.

</details>


### [40] [DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference](https://arxiv.org/abs/2507.19608)
*Jiawen Qi,Chang Gao,Zhaochun Ren,Qinyu Chen*

Main category: cs.AI

TL;DR: DeltaLLM是一个免训练框架，通过利用注意力模式中的时间稀疏性，在资源受限的边缘设备上高效部署大型语言模型（LLMs），显著提高了注意力稀疏性并保持或提升了性能。


<details>
  <summary>Details</summary>
Motivation: 由于计算量随序列长度呈平方增长，在边缘设备上部署大型语言模型（LLMs）面临挑战。现有的动态注意力剪枝方法多为GPU/TPU设计，针对长上下文长度，不适用于资源受限的边缘场景。

Method: DeltaLLM是一个免训练框架，利用注意力模式的时间稀疏性。它引入了：1) 一种兼顾精度和内存的Delta矩阵构建策略，以引入时间稀疏性；2) 一种上下文感知混合注意力机制，结合局部上下文窗口内的全注意力与外部的Delta近似以提高精度。

Result: 在BitNet模型上，预填充阶段注意力稀疏性从0%提升到60%，WG任务精度略有提升；在SQuAD-v2任务上，预填充和解码阶段整体稀疏性从0%提升到57%，F1分数从29.63提升到30.97。在Llama模型上，预填充阶段最高可达60%稀疏性，整体约57%稀疏性，精度下降可忽略不计。

Conclusion: DeltaLLM为LLM在边缘设备上的高效部署提供了一个有前景的解决方案，无需微调即可与现有推理流程无缝集成，显著提升了注意力稀疏性并保持了性能。

Abstract: Deploying Large Language Models (LLMs) on edge devices remains challenging
due to their quadratically increasing computations with the sequence length.
Existing studies for dynamic attention pruning are designed for hardware with
massively parallel computation capabilities, such as GPUs or TPUs, and aim at
long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We
present DeltaLLM, a training-free framework that exploits temporal sparsity in
attention patterns to enable efficient LLM inference across both the prefilling
and decoding stages, on resource-constrained edge devices. DeltaLLM introduces
an accuracy- and memory-aware delta matrix construction strategy that
introduces temporal sparsity, and a context-aware hybrid attention mechanism
that combines full attention in a local context window with delta approximation
outside it to increase accuracy. We evaluate our framework on the
edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model
across diverse language tasks. The results show that on BitNet, our framework
increases the attention sparsity from 0% to 60% during the prefilling stage
with slight accuracy improvement on the WG task, and 0% to 57% across both the
prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97
on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity
during the prefilling stage and around 57% across both stages with negligible
accuracy drop. These results demonstrate that DeltaLLM offers a promising
solution for efficient edge deployment, requiring no fine-tuning and seamlessly
integrating with existing inference pipelines.

</details>


### [41] [Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges](https://arxiv.org/abs/2507.19672)
*Haoran Lu,Luyang Fang,Ruidong Zhang,Xinliang Li,Jiazhang Cai,Huimin Cheng,Lin Tang,Ziyu Liu,Zeliang Sun,Tao Wang,Yingchuan Zhang,Arif Hassan Zidan,Jinwen Xu,Jincheng Yu,Meizhi Yu,Hanqi Jiang,Xilin Gong,Weidi Luo,Bolun Sun,Yongkai Chen,Terry Ma,Shushan Wu,Yifan Zhou,Junhao Chen,Haotian Xiang,Jing Zhang,Afrar Jahin,Wei Ruan,Ke Deng,Yi Pan,Peilong Wang,Jiahui Li,Zhengliang Liu,Lu Zhang,Lin Zhao,Wei Liu,Dajiang Zhu,Xin Xing,Fei Dou,Wei Zhang,Chao Huang,Rongjie Liu,Mengrui Zhang,Yiwen Liu,Xiaoxiao Sun,Qin Lu,Zhen Xiang,Wenxuan Zhong,Tianming Liu,Ping Ma*

Main category: cs.AI

TL;DR: 本综述全面审视了大型语言模型（LLMs）与人类意图对齐的关键挑战、实用技术、评估局限性及未来开放问题。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）能力显著增强且影响日益扩大，它们已深度融入社会多方面，因此确保LLMs与人类价值观和意图对齐已成为一项关键挑战。

Method: 本综述全面概述了LLM对齐的实用技术、训练协议和实证发现。通过分析不同范式下对齐方法的发展，探讨了核心对齐目标之间的权衡。还讨论了先进技术、评估框架和基准数据集，并总结了领先AI实验室的策略。

Result: 分析显示，监督微调能实现基本指令遵循，而基于偏好的方法在对齐细微人类意图方面更灵活。讨论了包括DPO、宪法式AI等在内的前沿技术如何平衡质量与效率。同时，回顾了现有评估框架及其在奖励指定、鲁棒性和可扩展监督等方面的局限性。

Conclusion: 本综述概述了监督、价值多元化、鲁棒性和持续对齐方面的开放问题，旨在为LLM对齐领域的研究人员和实践者提供指导与参考。

Abstract: Due to the remarkable capabilities and growing impact of large language
models (LLMs), they have been deeply integrated into many aspects of society.
Thus, ensuring their alignment with human values and intentions has emerged as
a critical challenge. This survey provides a comprehensive overview of
practical alignment techniques, training protocols, and empirical findings in
LLM alignment. We analyze the development of alignment methods across diverse
paradigms, characterizing the fundamental trade-offs between core alignment
objectives. Our analysis shows that while supervised fine-tuning enables basic
instruction-following, preference-based methods offer more flexibility for
aligning with nuanced human intent. We discuss state-of-the-art techniques,
including Direct Preference Optimization (DPO), Constitutional AI,
brain-inspired methods, and alignment uncertainty quantification (AUQ),
highlighting their approaches to balancing quality and efficiency. We review
existing evaluation frameworks and benchmarking datasets, emphasizing
limitations such as reward misspecification, distributional robustness, and
scalable oversight. We summarize strategies adopted by leading AI labs to
illustrate the current state of practice. We conclude by outlining open
problems in oversight, value pluralism, robustness, and continuous alignment.
This survey aims to inform both researchers and practitioners navigating the
evolving landscape of LLM alignment.

</details>


### [42] [The wall confronting large language models](https://arxiv.org/abs/2507.19703)
*Peter V. Coveney,Sauro Succi*

Main category: cs.AI

TL;DR: 本研究指出大型语言模型（LLMs）的缩放定律严重限制了其预测不确定性的改进，使其难以达到科学可靠性标准。作者认为，LLMs从高斯输入生成非高斯输出的机制可能是错误累积和AI退化的根源。避免未来AI退化需要更深入地理解问题结构。


<details>
  <summary>Details</summary>
Motivation: 分析大型语言模型（LLMs）在提高预测不确定性方面的固有局限性，并探讨为何其可靠性难以满足科学标准。

Method: 通过分析LLMs的缩放定律，并提出其从高斯输入生成非高斯输出的机制是导致错误累积、信息灾难和退化AI行为的潜在原因。此外，还考虑了数据集中随规模增加的虚假相关性（由Calude和Longo指出）的影响。

Result: 研究表明，决定LLMs性能的缩放定律严重限制了它们改进预测不确定性的能力，使其可靠性难以达到科学探究标准。推测其学习能力来源于从高斯输入生成非高斯输出的机制，这可能是导致错误累积和退化AI行为的根本原因。

Conclusion: LLMs的学习能力与准确性之间存在内在矛盾，这可能导致其走向退化的人工智能路径。为避免这种退化，未来的AI研究必须更加重视对所研究问题结构特征的洞察和理解。

Abstract: We show that the scaling laws which determine the performance of large
language models (LLMs) severely limit their ability to improve the uncertainty
of their predictions. As a result, raising their reliability to meet the
standards of scientific inquiry is intractable by any reasonable measure. We
argue that the very mechanism which fuels much of the learning power of LLMs,
namely the ability to generate non-Gaussian output distributions from Gaussian
input ones, might well be at the roots of their propensity to produce error
pileup, ensuing information catastrophes and degenerative AI behaviour. This
tension between learning and accuracy is a likely candidate mechanism
underlying the observed low values of the scaling components. It is
substantially compounded by the deluge of spurious correlations pointed out by
Calude and Longo which rapidly increase in any data set merely as a function of
its size, regardless of its nature. The fact that a degenerative AI pathway is
a very probable feature of the LLM landscape does not mean that it must
inevitably arise in all future AI research. Its avoidance, which we also
discuss in this paper, necessitates putting a much higher premium on insight
and understanding of the structural characteristics of the problems being
investigated.

</details>


### [43] [Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors](https://arxiv.org/abs/2507.19725)
*Leonardo Villalobos-Arias,Grant Forbes,Jianxun Wang,David L Roberts,Arnav Jhala*

Main category: cs.AI

TL;DR: 针对RL稀疏奖励与内生激励(IM)导致的“奖励欺骗”问题，本研究在MiniGrid环境实证评估IM对智能体行为的影响，并发现广义奖励匹配(GRM)可部分缓解“奖励欺骗”。


<details>
  <summary>Details</summary>
Motivation: 强化学习(RL)在稀疏奖励游戏中面临挑战，而内生激励(IM)是解决稀疏奖励的有效方法。然而，IM也带来了“奖励欺骗”问题，即智能体过度优化内生奖励而偏离游戏目标。目前对“奖励欺骗”的程度与具体影响尚不明确。本研究旨在通过实证评估，深入理解IM对RL智能体行为的具体影响。

Method: 本研究在MiniGrid游戏环境中，评估了三种内生激励(IM)技术对智能体行为的影响。同时，将这些IM模型与广义奖励匹配(GRM)方法进行比较，GRM旨在通过理论保证优化性。

Result: 研究结果表明，内生激励(IM)显著改变了智能体的行为，不仅增加了初始奖励，还改变了其决策与游戏策略。此外，广义奖励匹配(GRM)在部分场景下有效缓解了“奖励欺骗”问题。

Conclusion: 内生激励(IM)对强化学习智能体的行为有显著影响，远超仅仅增加奖励。广义奖励匹配(GRM)在缓解“奖励欺骗”方面展现出潜力。本研究为理解和应对“奖励欺骗”现象迈出了重要一步。

Abstract: Games are challenging for Reinforcement Learning~(RL) agents due to their
reward-sparsity, as rewards are only obtainable after long sequences of
deliberate actions. Intrinsic Motivation~(IM) methods -- which introduce
exploration rewards -- are an effective solution to reward-sparsity. However,
IM also causes an issue known as `reward hacking' where the agent optimizes for
the new reward at the expense of properly playing the game. The larger problem
is that reward hacking itself is largely unknown; there is no answer to
whether, and to what extent, IM rewards change the behavior of RL agents. This
study takes a first step by empirically evaluating the impact on behavior of
three IM techniques on the MiniGrid game-like environment. We compare these IM
models with Generalized Reward Matching~(GRM), a method that can be used with
any intrinsic reward function to guarantee optimality. Our results suggest that
IM causes noticeable change by increasing the initial rewards, but also
altering the way the agent plays; and that GRM mitigated reward hacking in some
scenarios.

</details>


### [44] [HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare](https://arxiv.org/abs/2507.19726)
*Yuzhang Xie,Xu Han,Ran Xu,Xiao Hu,Jiaying Lu,Carl Yang*

Main category: cs.AI

TL;DR: 提出HypKG框架，通过超图模型将电子健康记录中的患者信息整合到知识图谱中，以实现更精准的医疗预测。


<details>
  <summary>Details</summary>
Motivation: 通用知识图谱（KGs）缺乏患者特定上下文（如患者状态），而这对于精准医疗至关重要。电子健康记录（EHRs）包含丰富的个人数据，可以为知识图谱提供这种上下文，以提高医疗预测的准确性。

Method: 提出HypKG框架，该框架通过先进的实体链接技术将通用知识图谱与电子健康记录中的患者信息连接起来。接着，利用超图模型将知识与患者信息进行上下文关联。最后，采用由下游预测任务引导的超图Transformer，联合学习知识图谱和患者的语境化表示。

Result: 在大型生物医学知识图谱和两个真实世界电子健康记录数据集上的实验表明，HypKG在多项评估指标下显著提高了医疗预测任务的性能。此外，通过整合外部上下文，HypKG能够调整知识图谱中实体和关系的表示，从而可能提高知识的质量和实际应用价值。

Conclusion: HypKG成功地将电子健康记录中的患者上下文整合到知识图谱中，生成了更准确和实用的知识表示，显著提高了医疗预测的性能，并可能提升知识图谱本身的质量和实用性。

Abstract: Knowledge graphs (KGs) are important products of the semantic web, which are
widely used in various application domains. Healthcare is one of such domains
where KGs are intensively used, due to the high requirement for knowledge
accuracy and interconnected nature of healthcare data. However, KGs storing
general factual information often lack the ability to account for important
contexts of the knowledge such as the status of specific patients, which are
crucial in precision healthcare. Meanwhile, electronic health records (EHRs)
provide rich personal data, including various diagnoses and medications, which
provide natural contexts for general KGs. In this paper, we propose HypKG, a
framework that integrates patient information from EHRs into KGs to generate
contextualized knowledge representations for accurate healthcare predictions.
Using advanced entity-linking techniques, we connect relevant knowledge from
general KGs with patient information from EHRs, and then utilize a hypergraph
model to "contextualize" the knowledge with the patient information. Finally,
we employ hypergraph transformers guided by downstream prediction tasks to
jointly learn proper contextualized representations for both KGs and patients,
fully leveraging existing knowledge in KGs and patient contexts in EHRs. In
experiments using a large biomedical KG and two real-world EHR datasets, HypKG
demonstrates significant improvements in healthcare prediction tasks across
multiple evaluation metrics. Additionally, by integrating external contexts,
HypKG can learn to adjust the representations of entities and relations in KG,
potentially improving the quality and real-world utility of knowledge.

</details>


### [45] [Integrating Activity Predictions in Knowledge Graphs](https://arxiv.org/abs/2507.19733)
*Alec Scully,Cameron Stockton,Forrest Hare*

Main category: cs.AI

TL;DR: 该研究提出利用本体结构化知识图谱结合马尔可夫链来预测未来事件，并修正了传统的概率本体模型，以更好地捕捉现实世界现象的动态性。


<details>
  <summary>Details</summary>
Motivation: 论证本体结构化知识图谱在预测未来事件中的关键作用，并通过批判现有概率模型并提出替代方案，来克服其缺陷。

Method: 利用BFO和CCO等本体框架组织和检索知识图谱中的数据（例如渔船移动），并用查询结果构建马尔可夫链模型以预测未来状态。引入“时空瞬时”概念完善结构语义。批判了将概率与可能性混淆的现有模型，提出将概率视为“过程剖面”的替代观点。最终将基于马尔可夫链的概率计算结果无缝整合回知识图谱。

Result: 成功展示了如何利用本体（BFO和CCO）组织和检索知识图谱中的数据，并基于此数据通过马尔可夫链模型预测未来状态。提出了“时空瞬时”概念以完善语义。批判了现有概率模型，并提出了将概率视为“过程剖面”的新视角。实现了将马尔可夫链计算出的概率无缝整合回知识图谱，以支持进一步分析和决策。

Conclusion: 本体结构化知识图谱在生成未来事件预测中扮演着至关重要的角色。通过结合马尔可夫链和提出的新概率观，可以有效地进行预测分析，并支持决策制定，从而更好地捕捉真实世界的动态现象。

Abstract: We argue that ontology-structured knowledge graphs can play a crucial role in
generating predictions about future events. By leveraging the semantic
framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies
(CCO), we demonstrate how data such as the movements of a fishing vessel can be
organized in and retrieved from a knowledge graph. These query results are then
used to create Markov chain models, allowing us to predict future states based
on the vessel's history. To fully support this process, we introduce the term
`spatiotemporal instant' to complete the necessary structural semantics.
Additionally, we critique the prevailing ontological model of probability,
which conflates probability with likelihood and relies on the problematic
concept of modal measurements: measurements of future entities. We propose an
alternative view, where probabilities are treated as being about process
profiles, which better captures the dynamics of real world phenomena. Finally,
we demonstrate how our Markov chain based probability calculations can be
seamlessly integrated back into the knowledge graph, enabling further analysis
and decision-making. Keywords: predictive analytics, ontology, Markov chains,
probability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.

</details>


### [46] [Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)](https://arxiv.org/abs/2507.19749)
*Lin Ren,Guohui Xiao,Guilin Qi,Yishuai Geng,Haohan Xue*

Main category: cs.AI

TL;DR: 研究引入了ASPBench基准来评估大型语言模型（LLMs）在答案集编程（ASP）方面的能力。结果显示LLMs在ASP的核心任务——答案集计算上表现不佳，揭示了其在符号推理方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在逻辑推理方面展现出潜力，但当前对LLM在答案集编程（ASP）能力上的评估存在局限性。现有工作通常使用过于简化的ASP程序，缺乏对否定、析取或多重答案集的支持，并且缺乏专门为ASP求解设计的基准。

Method: 为弥补现有评估的不足，研究者引入了一个名为ASPBench的综合ASP基准测试，其中包含三个专门的ASP任务：ASP蕴涵、答案集验证和答案集计算。并使用14个最先进的LLM在此基准上进行了广泛评估。

Result: 对LLM在ASPBench上的评估显示，虽然它们在ASP蕴涵和答案集验证这两个相对简单的任务上表现较好，但在作为ASP求解核心的答案集计算任务上表现挣扎。

Conclusion: 研究结果揭示了LLM在ASP求解方面的当前局限性，并强调需要开发更有效地整合符号推理能力的新方法。

Abstract: Answer Set Programming (ASP) is a powerful paradigm for non-monotonic
reasoning. Recently, large language models (LLMs) have demonstrated promising
capabilities in logical reasoning. Despite this potential, current evaluations
of LLM capabilities in ASP are often limited. Existing works normally employ
overly simplified ASP programs, do not support negation, disjunction, or
multiple answer sets. Furthermore, there is a lack of benchmarks that introduce
tasks specifically designed for ASP solving. To bridge this gap, we introduce
ASPBench, a comprehensive ASP benchmark, including three ASP specific tasks:
ASP entailment, answer set verification, and answer set computation. Our
extensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs,
including \emph{deepseek-r1}, \emph{o4-mini}, and
\emph{gemini-2.5-flash-thinking}, perform relatively well on the first two
simpler tasks, they struggle with answer set computation, which is the core of
ASP solving. These findings offer insights into the current limitations of LLMs
in ASP solving. This highlights the need for new approaches that integrate
symbolic reasoning capabilities more effectively. The code and dataset are
available at https://github.com/HomuraT/ASPBench.

</details>


### [47] [Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation](https://arxiv.org/abs/2507.19788)
*Rifny Rachman,Josh Tingey,Richard Allmendinger,Pradyumn Shukla,Wei Pan*

Main category: cs.AI

TL;DR: 本研究开发了一个基于马尔可夫决策过程的广义多目标、多层级供应链优化模型，并使用多目标强化学习(RL)方法进行评估。结果显示，该方法在平衡经济、环境和社会目标方面表现卓越，尤其在复杂场景下，其最优性、多样性和密度优于其他基准方法，并能稳定生产和库存。


<details>
  <summary>Details</summary>
Motivation: 在非平稳市场环境下，需要一个能够同时考虑经济、环境和社会等多重目标，并实现生产和交付数量近最优权衡的通用供应链优化模型。

Method: 1. 基于马尔可夫决策过程构建广义多目标、多层级供应链优化模型，纳入经济、环境、社会考量。
2. 采用多目标强化学习(RL)方法评估模型。
3. 与修改后的单目标RL算法（加权求和）以及基于多目标进化算法(MOEA)的方法进行基准测试。
4. 使用可定制模拟器在不同网络复杂性下进行实验。
5. 通过共享经验缓冲区增强知识转移以提升模型性能。

Result: 1. 该方法在最优性、多样性和密度之间提供了最平衡的权衡。
2. 在复杂设置下，其超体积比基于MOEA的方法高出75%。
3. 生成的解决方案密度比修改后的单目标RL方法高出约11倍，显示出更强的鲁棒性。
4. 有效确保了生产和库存水平的稳定，并最大限度地减少了需求损失。

Conclusion: 本研究提出的多目标强化学习方法在非平稳市场中的多目标、多层级供应链优化问题上表现出色，尤其在复杂场景下，其在实现多目标权衡、帕累托前沿近似以及鲁棒性方面均优于现有基准方法，为供应链管理提供了稳定且高效的解决方案。

Abstract: This study develops a generalised multi-objective, multi-echelon supply chain
optimisation model with non-stationary markets based on a Markov decision
process, incorporating economic, environmental, and social considerations. The
model is evaluated using a multi-objective reinforcement learning (RL) method,
benchmarked against an originally single-objective RL algorithm modified with
weighted sum using predefined weights, and a multi-objective evolutionary
algorithm (MOEA)-based approach. We conduct experiments on varying network
complexities, mimicking typical real-world challenges using a customisable
simulator. The model determines production and delivery quantities across
supply chain routes to achieve near-optimal trade-offs between competing
objectives, approximating Pareto front sets. The results demonstrate that the
primary approach provides the most balanced trade-off between optimality,
diversity, and density, further enhanced with a shared experience buffer that
allows knowledge transfer among policies. In complex settings, it achieves up
to 75\% higher hypervolume than the MOEA-based method and generates solutions
that are approximately eleven times denser, signifying better robustness, than
those produced by the modified single-objective RL method. Moreover, it ensures
stable production and inventory levels while minimising demand loss.

</details>


### [48] [Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation](https://arxiv.org/abs/2507.19882)
*Xinshu Li,Ruoyu Wang,Erdun Gao,Mingming Gong,Lina Yao*

Main category: cs.AI

TL;DR: DiCap是一个基于扩散的反事实提示学习框架，通过理论指导克服了现有提示学习方法在因果不变性和跨类别泛化性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法因缺乏足够的理论基础，难以实现因果不变的提示，从而未能捕获能够有效泛化到不同类别的鲁棒特征。

Method: 提出了DiCap模型，一个基于扩散的反事实提示学习框架。它利用扩散过程从因果模型的边际和条件分布中迭代采样梯度，生成满足最小充分性准则的反事实。该方法有严格的理论推导，确保反事实结果的可识别性并限制估计误差。此外，结合对比学习框架，利用生成的反事实精确提取与数据因果特征对齐的提示。

Result: 在图像分类、图文检索和视觉问答等任务中表现出色，尤其在未见类别中展现出显著优势。

Conclusion: DiCap通过引入理论基础和扩散-反事实-对比学习范式，有效提升了提示学习的因果不变性和跨类别泛化能力，特别是在处理新颖数据方面具有强大优势。

Abstract: Prompt learning has garnered attention for its efficiency over traditional
model training and fine-tuning. However, existing methods, constrained by
inadequate theoretical foundations, encounter difficulties in achieving
causally invariant prompts, ultimately falling short of capturing robust
features that generalize effectively across categories. To address these
challenges, we introduce the $\textit{\textbf{DiCap}}$ model, a theoretically
grounded $\textbf{Di}$ffusion-based $\textbf{C}$ounterf$\textbf{a}$ctual
$\textbf{p}$rompt learning framework, which leverages a diffusion process to
iteratively sample gradients from the marginal and conditional distributions of
the causal model, guiding the generation of counterfactuals that satisfy the
minimal sufficiency criterion. Grounded in rigorous theoretical derivations,
this approach guarantees the identifiability of counterfactual outcomes while
imposing strict bounds on estimation errors. We further employ a contrastive
learning framework that leverages the generated counterfactuals, thereby
enabling the refined extraction of prompts that are precisely aligned with the
causal features of the data. Extensive experimental results demonstrate that
our method performs excellently across tasks such as image classification,
image-text retrieval, and visual question answering, with particularly strong
advantages in unseen categories.

</details>


### [49] [What Does 'Human-Centred AI' Mean?](https://arxiv.org/abs/2507.19960)
*Olivia Guest*

Main category: cs.AI

TL;DR: 该文将AI定义为技术与人类认知劳动之间的关系，并分析其对人类认知劳动的影响类型。作者强调，理解AI与人类认知的内在联系对于真正以人为本的AI发展至关重要。


<details>
  <summary>Details</summary>
Motivation: 作者旨在提出一种新的AI视角，即AI是技术与人类认知劳动之间的关系。其动机在于纠正对AI的误解，强调AI对人类认知的深远影响，并指出忽视这种关联会阻碍以人为本的AI发展。

Method: 文章通过列举具体例子（如算盘对心算、闹钟对叫醒员等）来对比技术与人类认知任务。在此基础上，作者运用新颖的定义和分析方法，对社会技术关系进行分类，评估其对人类认知劳动的不同影响。

Result: 研究将AI对人类认知劳动的影响分为三类：有害的“取代”（displacement）、有益的“增强”（enhancement）和中性的“替换”（replacement）。文章指出，所有AI都不可避免地涉及人类认知。

Conclusion: 混淆AI与认知的关系会导致对AI的误读和批判性思考的缺失，从而限制了以人为本的AI系统设计。作者强调，要“去神秘化”AI，必须正视“人机协作”中的人类角色。

Abstract: While it seems sensible that human-centred artificial intelligence (AI) means
centring "human behaviour and experience," it cannot be any other way. AI, I
argue, is usefully seen as a relationship between technology and humans where
it appears that artifacts can perform, to a greater or lesser extent, human
cognitive labour. This is evinced using examples that juxtapose technology with
cognition, inter alia: abacus versus mental arithmetic; alarm clock versus
knocker-upper; camera versus vision; and sweatshop versus tailor. Using novel
definitions and analyses, sociotechnical relationships can be analysed into
varying types of: displacement (harmful), enhancement (beneficial), and/or
replacement (neutral) of human cognitive labour. Ultimately, all AI implicates
human cognition; no matter what. Obfuscation of cognition in the AI context --
from clocks to artificial neural networks -- results in distortion, in slowing
critical engagement, perverting cognitive science, and indeed in limiting our
ability to truly centre humans and humanity in the engineering of AI systems.
To even begin to de-fetishise AI, we must look the human-in-the-loop in the
eyes.

</details>


### [50] [Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization](https://arxiv.org/abs/2507.19973)
*Ebrahim Rasromani,Stella K. Kang,Yanqi Xu,Beisong Liu,Garvit Luhadia,Wan Fung Chui,Felicia L. Pasadyn,Yu Chih Hung,Julie Y. An,Edwin Mathieu,Zehui Gu,Carlos Fernandez-Granda,Ammar A. Javed,Greg D. Sacks,Tamas Gonda,Chenchan Huang,Yiqiu Shen*

Main category: cs.AI

TL;DR: 研究人员开发并评估了通过思维链（CoT）微调的开源大型语言模型（LLM），以自动从MRI/CT报告中提取胰腺囊性病变（PCL）特征并进行风险分类，其性能与GPT-4o相当。


<details>
  <summary>Details</summary>
Motivation: 从放射学报告中手动提取胰腺囊性病变（PCL）特征耗时费力，限制了大规模研究的开展，从而阻碍了PCL研究的进展。

Method: 研究人员收集了6000份描述PCL的腹部MRI/CT报告作为训练数据集。使用GPT-4o通过思维链（CoT）提示生成PCL特征和主胰管特征的标签。两个开源LLM（LLaMA和DeepSeek）在GPT-4o生成的CoT数据上使用QLoRA进行了微调。根据机构指南和2017 ACR白皮书，将提取的特征映射到风险类别。模型在285份人工标注的报告上进行了评估，并由三名放射科医生独立审查了100个案例的输出。评估指标包括特征提取的精确匹配准确率、风险分类的宏观平均F1分数以及放射科医生与模型之间的一致性（Fleiss' Kappa）。

Result: 思维链微调显著提高了LLaMA（从80%到97%）和DeepSeek（从79%到98%）的特征提取准确率，达到了与GPT-4o（97%）相当的水平。风险分类的F1分数也有所提高（LLaMA：0.95；DeepSeek：0.94），与GPT-4o（0.97）非常接近，且无统计学显著差异。放射科医生间的一致性很高（Fleiss' Kappa = 0.888），加入DeepSeek-FT-CoT（Fleiss' Kappa = 0.893）或GPT-CoT（Fleiss' Kappa = 0.897）后，一致性没有统计学显著差异，表明模型达到与放射科医生一致的水平。

Conclusion: 通过思维链监督微调的开源大型语言模型，能够为大规模PCL研究提供准确、可解释和高效的表型提取能力，其性能可与GPT-4o媲美。

Abstract: Background: Manual extraction of pancreatic cystic lesion (PCL) features from
radiology reports is labor-intensive, limiting large-scale studies needed to
advance PCL research. Purpose: To develop and evaluate large language models
(LLMs) that automatically extract PCL features from MRI/CT reports and assign
risk categories based on guidelines. Materials and Methods: We curated a
training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134
patients that described PCLs. Labels were generated by GPT-4o using
chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct
features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated
CoT data. Features were mapped to risk categories per institutional guideline
based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out
human-annotated reports. Model outputs for 100 cases were independently
reviewed by three radiologists. Feature extraction was evaluated using exact
match accuracy, risk categorization with macro-averaged F1 score, and
radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning
improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%
to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved
(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no
statistically significant differences. Radiologist inter-reader agreement was
high (Fleiss' Kappa = 0.888) and showed no statistically significant difference
with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT
(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels
on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT
supervision enable accurate, interpretable, and efficient phenotyping for
large-scale PCL research, achieving performance comparable to GPT-4o.

</details>


### [51] [Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application](https://arxiv.org/abs/2507.19974)
*Tongjie Li,Jianhua Zhang,Li Yu,Yuxiang Zhang,Yunlong Cai,Fan Xu,Guangyi Liu*

Main category: cs.AI

TL;DR: 为应对6G网络严苛的资源分配需求，本文提出一种基于数字孪生信道（DTC）的在线优化框架。该框架利用环境感知预测信道状态信息（CSI），并通过轻量级博弈论算法进行实时资源分配，显著提升网络吞吐量。


<details>
  <summary>Details</summary>
Motivation: 6G网络中的全息通信、自动驾驶、工业物联网等新兴应用对资源分配的灵活性、低延迟和可靠性提出了严格要求。然而，传统的统计建模方法在动态特定环境中难以实现最优性能，且获取实时信道状态信息（CSI）的导频开销过大。

Method: 本文提出一个基于数字孪生信道（DTC）的在线优化框架。该框架中，DTC通过环境感知预测信道状态信息（CSI），随后轻量级博弈论算法利用预测的CSI进行及时高效的在线资源分配。

Result: 基于真实工业车间数字副本的仿真结果表明，与基于导频的理想CSI方案相比，所提方法实现了高达11.5%的吞吐量提升。

Conclusion: 该方法有效验证了其在未来6G网络中实现可扩展、低开销、环境感知通信的潜力。

Abstract: Emerging applications such as holographic communication, autonomous driving,
and the industrial Internet of Things impose stringent requirements on
flexible, low-latency, and reliable resource allocation in 6G networks.
Conventional methods, which rely on statistical modeling, have proven effective
in general contexts but may fail to achieve optimal performance in specific and
dynamic environments. Furthermore, acquiring real-time channel state
information (CSI) typically requires excessive pilot overhead. To address these
challenges, a digital twin channel (DTC)-enabled online optimization framework
is proposed, in which DTC is employed to predict CSI based on environmental
sensing. The predicted CSI is then utilized by lightweight game-theoretic
algorithms to perform online resource allocation in a timely and efficient
manner. Simulation results based on a digital replica of a realistic industrial
workshop demonstrate that the proposed method achieves throughput improvements
of up to 11.5\% compared with pilot-based ideal CSI schemes, validating its
effectiveness for scalable, low-overhead, and environment-aware communication
in future 6G networks.

</details>


### [52] [Matching Game Preferences Through Dialogical Large Language Models: A Perspective](https://arxiv.org/abs/2507.20000)
*Renaud Fabre,Daniel Egret,Patrice Bellot*

Main category: cs.AI

TL;DR: 本文提出一个概念框架，结合大型语言模型（LLMs）和GRAPHYP系统，旨在通过结构化对话整合用户偏好，创建透明、可解释的个性化AI系统，以增强“对话智能”。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统缺乏透明度、可追溯性和个性化，难以让用户理解其推理过程。研究旨在通过结合LLMs和GRAPHYP来增强AI的“对话智能”和信任度，使人类能理解AI的决策机制。

Method: 提出“对话式大型语言模型（D-LLMs）”的概念框架，该框架将LLMs与GRAPHYP网络系统结合。D-LLM框架包含三个主要组件：1) 分析搜索经验和引导性能的推理过程；2) 识别用户偏好模式的分类系统；3) 帮助人类解决冲突信息的对话方法。

Result: 概念上，D-LLMs系统能够通过结构化对话整合多用户偏好，将个体偏好直接嵌入AI决策中。该框架旨在创建一个可解释的AI系统，用户可以通过GRAPHYP网络审查、理解并整合影响AI响应的不同人类偏好。

Conclusion: 该视角旨在构建不仅提供答案，还能展示推理过程的AI系统，从而提高人工智能的透明度和可信度，更好地支持人类决策。

Abstract: This perspective paper explores the future potential of "conversational
intelligence" by examining how Large Language Models (LLMs) could be combined
with GRAPHYP's network system to better understand human conversations and
preferences. Using recent research and case studies, we propose a conceptual
framework that could make AI rea-soning transparent and traceable, allowing
humans to see and understand how AI reaches its conclusions. We present the
conceptual perspective of "Matching Game Preferences through Dialogical Large
Language Models (D-LLMs)," a proposed system that would allow multiple users to
share their different preferences through structured conversations. This
approach envisions personalizing LLMs by embedding individual user preferences
directly into how the model makes decisions. The proposed D-LLM framework would
require three main components: (1) reasoning processes that could analyze
different search experiences and guide performance, (2) classification systems
that would identify user preference patterns, and (3) dialogue approaches that
could help humans resolve conflicting information. This perspective framework
aims to create an interpretable AI system where users could examine,
understand, and combine the different human preferences that influence AI
responses, detected through GRAPHYP's search experience networks. The goal of
this perspective is to envision AI systems that would not only provide answers
but also show users how those answers were reached, making artificial
intelligence more transparent and trustworthy for human decision-making.

</details>


### [53] [Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems](https://arxiv.org/abs/2507.20010)
*Müge Fidan,Esra Erdem*

Main category: cs.AI

TL;DR: 研究稳定室友问题，提出一种考虑习惯和社交网络的个性化匹配方法，旨在寻找“足够好”的解决方案。


<details>
  <summary>Details</summary>
Motivation: 稳定室友问题在现实世界中有应用需求，但并非总存在稳定解。因此，有必要计算“足够好”的匹配。此外，研究还考虑了代理人的习惯、习惯性偏好及其朋友网络。

Method: 引入一种新方法，通过将代理人的习惯、习惯性偏好以及其偏好朋友网络纳入考虑，以生成稳定室友问题的个性化解决方案。

Result: 通过示例和实证评估，证明了所提出方法的实用性/有效性。

Conclusion: 该方法能够为稳定室友问题生成个性化且“足够好”的匹配方案，尤其是在缺乏完美稳定解的情况下，并通过整合更丰富的代理人偏好信息来增强解决方案。

Abstract: The Stable Roommates problems are characterized by the preferences of agents
over other agents as roommates. A solution is a partition of the agents into
pairs that are acceptable to each other (i.e., they are in the preference lists
of each other), and the matching is stable (i.e., there do not exist any two
agents who prefer each other to their roommates, and thus block the matching).
Motivated by real-world applications, and considering that stable roommates
problems do not always have solutions, we continue our studies to compute
"good-enough" matchings. In addition to the agents' habits and habitual
preferences, we consider their networks of preferred friends, and introduce a
method to generate personalized solutions to stable roommates problems. We
illustrate the usefulness of our method with examples and empirical
evaluations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [54] [Beyond 9-to-5: A Generative Model for Augmenting Mobility Data of Underrepresented Shift Workers](https://arxiv.org/abs/2507.19510)
*Haoxuan Ma,Xishun Liao,Yifan Liu,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: 本研究提出一种基于Transformer的方法，利用零散GPS数据生成倒班工人的完整活动模式，以解决其在传统交通模型中代表性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统交通调查和规划中，占劳动力15-20%的倒班工人出行模式被系统性地低估，导致城市出行建模存在关键空白。研究通过对比GPS和调查数据，揭示了倒班工人与常规朝九晚五模式的显著差异，确认了这种代表性不足的问题。

Method: 提出一种新颖的基于Transformer的方法，利用零散GPS轨迹数据生成完整的、行为有效的非标准工时人员活动模式。该方法采用周期感知的时间嵌入和以转换为主的损失函数，专门捕获倒班工人独特的活动节律并减少传统数据集的偏差。

Result: 评估结果显示，生成的数据与洛杉矶县的GPS数据实现了显著的分布一致性（所有评估指标的平均JSD < 0.02）。

Conclusion: 该方法通过将不完整的GPS轨迹转化为完整的、有代表性的活动模式，为交通规划者提供了一个强大的数据增强工具，以填补理解城市人口24/7出行需求的关键空白，从而实现更精确和包容的交通规划。

Abstract: This paper addresses a critical gap in urban mobility modeling by focusing on
shift workers, a population segment comprising 15-20% of the workforce in
industrialized societies yet systematically underrepresented in traditional
transportation surveys and planning. This underrepresentation is revealed in
this study by a comparative analysis of GPS and survey data, highlighting stark
differences between the bimodal temporal patterns of shift workers and the
conventional 9-to-5 schedules recorded in surveys. To address this bias, we
introduce a novel transformer-based approach that leverages fragmented GPS
trajectory data to generate complete, behaviorally valid activity patterns for
individuals working non-standard hours. Our method employs periodaware temporal
embeddings and a transition-focused loss function specifically designed to
capture the unique activity rhythms of shift workers and mitigate the inherent
biases in conventional transportation datasets. Evaluation shows that the
generated data achieves remarkable distributional alignment with GPS data from
Los Angeles County (Average JSD < 0.02 for all evaluation metrics). By
transforming incomplete GPS traces into complete, representative activity
patterns, our approach provides transportation planners with a powerful data
augmentation tool to fill critical gaps in understanding the 24/7 mobility
needs of urban populations, enabling precise and inclusive transportation
planning.

</details>


### [55] [Enhancing Spatiotemporal Networks with xLSTM: A Scalar LSTM Approach for Cellular Traffic Forecasting](https://arxiv.org/abs/2507.19513)
*Khalid Ali,Zineddine Bettouche,Andreas Kassler,Andreas Fischer*

Main category: cs.LG

TL;DR: 本文提出一种轻量级双路径时空网络，结合Scalar LSTM和Conv3D模块，用于精确的时空交通预测。该网络在真实数据集上表现优于ConvLSTM基线，并具有更强的泛化能力，适用于下一代网络部署。


<details>
  <summary>Details</summary>
Motivation: 在5G及未来网络中，精确的时空交通预测对智能资源管理至关重要。然而，传统AI方法难以捕捉复杂的时空模式，尤其是在用户移动性方面表现不足。

Method: 引入了一种轻量级、双路径的时空网络。该网络利用Scalar LSTM (sLSTM) 进行高效的时间建模，并使用一个三层Conv3D模块进行空间特征提取。一个融合层将这两个数据流整合为统一表示，以实现鲁棒预测。

Result: 在真实数据集上的评估显示，该模型的预测性能优于ConvLSTM基线，并对未见区域表现出强大的泛化能力。实验结果表明，MAE比ConvLSTM减少了23%，模型泛化能力提高了30%。该设计还改善了梯度稳定性和收敛速度，并降低了预测误差。

Conclusion: 所提出的轻量级双路径时空网络能够有效捕捉复杂的时空交通模式，实现高精度预测和优异泛化能力，使其非常适合大规模、下一代网络部署。

Abstract: Accurate spatiotemporal traffic forecasting is vital for intelligent resource
management in 5G and beyond. However, conventional AI approaches often fail to
capture the intricate spatial and temporal patterns that exist, due to e.g.,
the mobility of users. We introduce a lightweight, dual-path Spatiotemporal
Network that leverages a Scalar LSTM (sLSTM) for efficient temporal modeling
and a three-layer Conv3D module for spatial feature extraction. A fusion layer
integrates both streams into a cohesive representation, enabling robust
forecasting. Our design improves gradient stability and convergence speed while
reducing prediction error. Evaluations on real-world datasets show superior
forecast performance over ConvLSTM baselines and strong generalization to
unseen regions, making it well-suited for large-scale, next-generation network
deployments. Experimental evaluation shows a 23% MAE reduction over ConvLSTM,
with a 30% improvement in model generalization.

</details>


### [56] [Wavelet Logic Machines: Learning and Reasoning in the Spectral Domain Without Neural Networks](https://arxiv.org/abs/2507.19514)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: 本文提出一种全谱学习框架，通过在小波域中操作并直接对小波系数应用可学习非线性变换，消除了传统神经网络层。该模型支持可微分小波基选择，在参数量和内存使用量大幅减少的情况下，在去噪和自然语言任务上实现了与Transformer相当的性能，并提供了更高的效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络（尤其是Transformer）面临参数量大、内存占用高、推理成本高（自注意力机制的二次复杂度）的问题。本研究旨在开发一种更紧凑、可解释且高效的替代方案，探索无需过度参数化架构的原理性谱学习方法。

Method: 该框架完全在小波域中操作，不使用空间卷积或注意力机制。它直接对小波系数应用可学习的非线性变换（包括软阈值和增益-相位调制），并包含一个可微分的小波基选择机制（如Haar、Daubechies、Biorthogonal小波）。模型在PyTorch中实现，支持3D数据，并利用线性时间的小波变换和点式非线性操作。

Result: 在合成3D去噪和GLUE基准（SST-2情感分类）任务上，模型达到了89.3%的准确率，接近四层Transformer基线（90.1%）。同时，该模型参数量减少了72%，峰值内存使用减少了58%。由于谱稀疏先验，模型收敛速度更快。与Transformer相比，其推理成本显著降低。

Conclusion: 本研究的成果表明，提出的谱学习方法是神经网络模型的一种紧凑、可解释且高效的替代方案。它验证了原理性谱学习在视觉和语言任务中的可行性，为模型设计提供了新的方向，无需依赖过度参数化的架构。

Abstract: We introduce a fully spectral learning framework that eliminates traditional
neural layers by operating entirely in the wavelet domain. The model applies
learnable nonlinear transformations, including soft-thresholding and gain-phase
modulation, directly to wavelet coefficients. It also includes a differentiable
wavelet basis selection mechanism, enabling adaptive processing using families
such as Haar, Daubechies, and Biorthogonal wavelets.
  Implemented in PyTorch with full 3D support, the model maintains a spectral
pipeline without spatial convolutions or attention. On synthetic 3D denoising
and natural language tasks from the GLUE benchmark, including SST-2 sentiment
classification, the model achieves 89.3 percent accuracy, close to a 4-layer
Transformer baseline (90.1 percent), while using 72 percent fewer parameters
and 58 percent less peak memory. Faster early convergence is observed due to
spectral sparsity priors.
  In contrast to the quadratic complexity of self-attention and large matrix
multiplications in Transformers, our approach uses linear-time wavelet
transforms and pointwise nonlinearities, significantly reducing inference cost.
This yields a compact, interpretable, and efficient alternative to neural
models. Our results support the viability of principled spectral learning in
both vision and language tasks, offering new directions for model design
without overparameterized architectures.

</details>


### [57] [A Comparative Analysis of Traditional and Deep Learning Time Series Architectures for Influenza A Infectious Disease Forecasting](https://arxiv.org/abs/2507.19515)
*Edmund F. Agyemang,Hansapani Rodrigo,Vincent Agbenyeavu*

Main category: cs.LG

TL;DR: 本研究比较了传统模型与深度学习模型在甲型流感爆发预测中的性能，结果显示深度学习模型，特别是Transformer，预测效果显著优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 甲型流感每年导致大量呼吸道死亡，准确预测其爆发对公共卫生至关重要。研究旨在通过比较传统和深度学习模型，探索更高效的预测方法。

Method: 使用2009年1月至2023年12月的历史数据，比较了传统模型（ARIMA、ETS）与六种深度学习模型（Simple RNN、LSTM、GRU、BiLSTM、BiGRU、Transformer）的性能。评估指标为平均测试MSE和MAE。

Result: 所有深度学习模型均表现出优于传统基线模型（ARIMA和ETS）的预测能力。其中，Transformer模型表现最佳，其平均测试MSE和MAE分别为0.0433 ± 0.0020和0.1126 ± 0.0016，能更好地捕捉甲型流感数据的时间复杂性。

Conclusion: 先进的深度学习架构能显著提升传染病预测建模的准确性，预示着深度学习方法在公共卫生预测和干预策略规划中具有广阔应用前景。

Abstract: Influenza A is responsible for 290,000 to 650,000 respiratory deaths a year,
though this estimate is an improvement from years past due to improvements in
sanitation, healthcare practices, and vaccination programs. In this study, we
perform a comparative analysis of traditional and deep learning models to
predict Influenza A outbreaks. Using historical data from January 2009 to
December 2023, we compared the performance of traditional ARIMA and Exponential
Smoothing(ETS) models with six distinct deep learning architectures: Simple
RNN, LSTM, GRU, BiLSTM, BiGRU, and Transformer. The results reveal a clear
superiority of all the deep learning models, especially the state-of-the-art
Transformer with respective average testing MSE and MAE of 0.0433 \pm 0.0020
and 0.1126 \pm 0.0016 for capturing the temporal complexities associated with
Influenza A data, outperforming well known traditional baseline ARIMA and ETS
models. These findings of this study provide evidence that state-of-the-art
deep learning architectures can enhance predictive modeling for infectious
diseases and indicate a more general trend toward using deep learning methods
to enhance public health forecasting and intervention planning strategies.
Future work should focus on how these models can be incorporated into real-time
forecasting and preparedness systems at an epidemic level, and integrated into
existing surveillance systems.

</details>


### [58] [BikeVAE-GNN: A Variational Autoencoder-Augmented Hybrid Graph Neural Network for Sparse Bicycle Volume Estimation](https://arxiv.org/abs/2507.19517)
*Mohit Gupta,Debjit Bhowmick,Ben Beck*

Main category: cs.LG

TL;DR: BikeVAE-GNN是一种结合GNN和VAE的新型双任务框架，旨在解决稀疏数据下自行车交通量估计的挑战。


<details>
  <summary>Details</summary>
Motivation: 准确的自行车交通量估算对城市规划至关重要，但全球城市自行车网络普遍面临计数数据极其稀疏的挑战。

Method: 本文提出了BikeVAE-GNN，一个双任务框架。它将混合图神经网络（结合GCN、GAT和GraphSAGE）与变分自编码器（VAE）相结合。混合GNN用于建模稀疏网络中的复杂空间关系，而VAE则生成合成节点和边以丰富图结构，从而提高估计性能。该模型同时执行自行车交通量回归和交通水平分类任务。

Result: 研究在墨尔本（99%数据稀疏度）的OpenStreetMap和公开自行车计数数据上验证了BikeVAE-GNN。结果显示，BikeVAE-GNN优于机器学习和基线GNN模型，平均绝对误差（MAE）为30.82辆/天，准确率达99%，F1分数达0.99。消融研究进一步证实了Hybrid-GNN和VAE组件的有效性。

Conclusion: 该研究通过新颖且先进的方法推进了稀疏网络中的自行车交通量估计，为可持续自行车基础设施建设提供了见解。

Abstract: Accurate link-level bicycle volume estimation is essential for informed urban
and transport planning but it is challenged by extremely sparse count data in
urban bicycling networks worldwide. We propose BikeVAE-GNN, a novel dual-task
framework augmenting a Hybrid Graph Neural Network (GNN) with Variational
Autoencoder (VAE) to estimate Average Daily Bicycle (ADB) counts, addressing
sparse bicycle networks. The Hybrid-GNN combines Graph Convolutional Networks
(GCN), Graph Attention Networks (GAT), and GraphSAGE to effectively model
intricate spatial relationships in sparse networks while VAE generates
synthetic nodes and edges to enrich the graph structure and enhance the
estimation performance. BikeVAE-GNN simultaneously performs - regression for
bicycling volume estimation and classification for bicycling traffic level
categorization. We demonstrate the effectiveness of BikeVAE-GNN using
OpenStreetMap data and publicly available bicycle count data within the City of
Melbourne - where only 141 of 15,933 road segments have labeled counts
(resulting in 99% count data sparsity). Our experiments show that BikeVAE-GNN
outperforms machine learning and baseline GNN models, achieving a mean absolute
error (MAE) of 30.82 bicycles per day, accuracy of 99% and F1-score of 0.99.
Ablation studies further validate the effective role of Hybrid-GNN and VAE
components. Our research advances bicycling volume estimation in sparse
networks using novel and state-of-the-art approaches, providing insights for
sustainable bicycling infrastructures.

</details>


### [59] [Target Circuit Matching in Large-Scale Netlists using GNN-Based Region Prediction](https://arxiv.org/abs/2507.19518)
*Sangwoo Seo,Jimin Seo,Yoonho Lee,Donghyeon Kim,Hyejin Shin,Banghyun Sung,Chanyoung Park*

Main category: cs.LG

TL;DR: 提出了一种基于GNN的高效子图匹配方法，用于在大规模电路中预测目标电路的高概率区域。


<details>
  <summary>Details</summary>
Motivation: 子图匹配在EDA和电路验证中很重要。传统规则方法泛化能力差；节点对节点匹配计算效率低，尤其对于大型电路。现有深度学习模型未能高效捕获全局子图嵌入或依赖低效匹配矩阵，限制了它们在大电路中的有效性。

Method: 利用图神经网络（GNNs）预测包含目标电路的高概率区域。构建多种负样本使GNNs学习目标电路的存在。开发了一种从整个电路中直接提取子图嵌入的方法，以捕获全局子图信息并解决GNN应用于所有候选子图的低效问题。

Result: 实验证明，该方法在时间效率和目标区域预测方面显著优于现有方法。

Conclusion: 为大规模电路中的子图匹配提供了一个可扩展且有效的解决方案。

Abstract: Subgraph matching plays an important role in electronic design automation
(EDA) and circuit verification. Traditional rule-based methods have limitations
in generalizing to arbitrary target circuits. Furthermore, node-to-node
matching approaches tend to be computationally inefficient, particularly for
large-scale circuits. Deep learning methods have emerged as a potential
solution to address these challenges, but existing models fail to efficiently
capture global subgraph embeddings or rely on inefficient matching matrices,
which limits their effectiveness for large circuits. In this paper, we propose
an efficient graph matching approach that utilizes Graph Neural Networks (GNNs)
to predict regions of high probability for containing the target circuit.
Specifically, we construct various negative samples to enable GNNs to
accurately learn the presence of target circuits and develop an approach to
directly extracting subgraph embeddings from the entire circuit, which captures
global subgraph information and addresses the inefficiency of applying GNNs to
all candidate subgraphs. Extensive experiments demonstrate that our approach
significantly outperforms existing methods in terms of time efficiency and
target region prediction, offering a scalable and effective solution for
subgraph matching in large-scale circuits.

</details>


### [60] [Physics-informed transfer learning for SHM via feature selection](https://arxiv.org/abs/2507.19519)
*J. Poole,P. Gardner,A. J. Hughes,N. Dervilis,R. S. Mills,T. A. Dardeno,K. Worden*

Main category: cs.LG

TL;DR: 针对结构健康监测(SHM)中数据获取困难和模型泛化差的问题，本文提出一种基于物理知识并利用模态保证准则(MAC)来选择适合迁移学习特征的方法，以提高SHM系统在不同结构间的泛化能力，并在数值和实验案例中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测(SHM)系统训练数据昂贵且带标签数据更难获取。群体SHM虽可利用多结构数据，但不同结构间的数据分布差异导致传统机器学习方法难以泛化。迁移学习有望解决此问题，但关键在于在目标域缺乏标签的情况下，如何有效选择合适的源结构和特征，以确保条件分布的关联性。此选择通常依赖领域专业知识，但对于复杂的损伤机制，该任务非易事。

Method: 本文利用物理知识来指导特征选择，具体采用模态保证准则（MAC）来量化健康结构模态间的对应关系。MAC被用作一种指标，以识别那些在结构受损时能跨领域保持行为一致性（即条件分布不变性）的特征。

Result: 研究表明，MAC与衡量联合分布相似度的有监督指标（该指标是分类器跨领域泛化的关键）具有高度一致性。该方法已通过数值模拟和实验案例研究验证了其有效性。

Conclusion: 该方法能有效选择具有条件分布不变性的特征，从而提升迁移学习在结构健康监测领域中的应用效果，提高模型在不同结构间的泛化能力。

Abstract: Data used for training structural health monitoring (SHM) systems are
expensive and often impractical to obtain, particularly labelled data.
Population-based SHM presents a potential solution to this issue by considering
the available data across a population of structures. However, differences
between structures will mean the training and testing distributions will
differ; thus, conventional machine learning methods cannot be expected to
generalise between structures. To address this issue, transfer learning (TL),
can be used to leverage information across related domains. An important
consideration is that the lack of labels in the target domain limits data-based
metrics to quantifying the discrepancy between the marginal distributions.
Thus, a prerequisite for the application of typical unsupervised TL methods is
to identify suitable source structures (domains), and a set of features, for
which the conditional distributions are related to the target structure.
Generally, the selection of domains and features is reliant on domain
expertise; however, for complex mechanisms, such as the influence of damage on
the dynamic response of a structure, this task is not trivial. In this paper,
knowledge of physics is leveraged to select more similar features, the modal
assurance criterion (MAC) is used to quantify the correspondence between the
modes of healthy structures. The MAC is shown to have high correspondence with
a supervised metric that measures joint-distribution similarity, which is the
primary indicator of whether a classifier will generalise between domains. The
MAC is proposed as a measure for selecting a set of features that behave
consistently across domains when subjected to damage, i.e. features with
invariance in the conditional distributions. This approach is demonstrated on
numerical and experimental case studies to verify its effectiveness in various
applications.

</details>


### [61] [Exoplanet Detection Using Machine Learning Models Trained on Synthetic Light Curves](https://arxiv.org/abs/2507.19520)
*Ethan Lo,Dan C. Lo*

Main category: cs.LG

TL;DR: 本研究利用多种机器学习模型（如逻辑回归、K近邻、随机森林）结合数据增强技术，提高了系外行星的发现效率和预测准确性，特别是在召回率和精确率方面。


<details>
  <summary>Details</summary>
Motivation: 系外行星的手动搜索效率低下且耗时，现有大型机构（如NASA）的机器学习模型过于复杂并依赖超级计算机。本研究旨在探索使用更简单、更广为人知的机器学习模型来降低复杂性，提高系外行星发现和验证的效率。

Method: 研究评估了逻辑回归、K近邻和随机森林等机器学习模型，并在NASA开普勒空间望远镜获取的数据集上进行训练和预测。为解决潜在偏差和数据集不平衡问题，研究采用了数据增强技术。

Result: 初步结果显示各模型均表现良好。应用数据增强技术后，系外行星搜索的召回率和精确率得到显著提升，但准确率因模型而异。

Conclusion: 在系外行星搜索中，数据增强技术能显著提高召回率和精确率，尽管不同模型的准确率有所不同。

Abstract: With manual searching processes, the rate at which scientists and astronomers
discover exoplanets is slow because of inefficiencies that require an extensive
time of laborious inspections. In fact, as of now there have been about only
5,000 confirmed exoplanets since the late 1900s. Recently, machine learning
(ML) has proven to be extremely valuable and efficient in various fields,
capable of processing massive amounts of data in addition to increasing its
accuracy by learning. Though ML models for discovering exoplanets owned by
large corporations (e.g. NASA) exist already, they largely depend on complex
algorithms and supercomputers. In an effort to reduce such complexities, in
this paper, we report the results and potential benefits of various, well-known
ML models in the discovery and validation of extrasolar planets. The ML models
that are examined in this study include logistic regression, k-nearest
neighbors, and random forest. The dataset on which the models train and predict
is acquired from NASA's Kepler space telescope. The initial results show
promising scores for each model. However, potential biases and dataset
imbalances necessitate the use of data augmentation techniques to further
ensure fairer predictions and improved generalization. This study concludes
that, in the context of searching for exoplanets, data augmentation techniques
significantly improve the recall and precision, while the accuracy varies for
each model.

</details>


### [62] [Applications and Manipulations of Physics-Informed Neural Networks in Solving Differential Equations](https://arxiv.org/abs/2507.19522)
*Aarush Gupta,Kendric Hsu,Syna Mathod*

Main category: cs.LG

TL;DR: 本文介绍并旨在利用物理信息神经网络（PINN）来解决复杂的微分方程的正向和反向问题。PINN通过将微分方程作为先验知识嵌入损失函数，通过最小化残差来同时优化网络权重和模型参数，即使在数据稀疏的情况下也能有效工作。


<details>
  <summary>Details</summary>
Motivation: 传统的神经网络在解决复杂微分方程及其参数优化（即正向和反向问题）时可能面临挑战，尤其是在数据稀疏或需要泛化到训练集边界之外时。引入物理先验信息（如微分方程）可以显著提升模型性能，克服这些限制。

Method: 核心方法是使用物理信息神经网络（PINN）。具体操作是将微分方程形式的先验分析信息注入到损失函数中，通过最小化微分方程的残差（左右侧之差），从而实现微分方程求解、神经网络权重优化和模型参数发现的同步进行。本文将从线性、二次模型开始，逐步扩展到热方程及其他复杂微分方程的PINN构建与应用。主要使用Python语言和PyTorch库进行研究。

Result: 通过构建不同复杂度的PINN模型，本文旨在证明PINN能够有效解决复杂的微分方程相关的正向和反向问题。预期结果包括PINN在稀疏数据下的高效性、避免过拟合的能力，以及能够通过外推适应数据中的更大趋势，并同时找到神经网络权重和模型参数。

Conclusion: 物理信息神经网络（PINN）提供了一种强大而通用的方法，能够通过结合领域知识（微分方程）来有效解决复杂的正向和反向问题，尤其在数据稀疏场景下表现出色。这项研究有望展示PINN在科学计算和数据建模中的巨大潜力。

Abstract: Mathematical models in neural networks are powerful tools for solving complex
differential equations and optimizing their parameters; that is, solving the
forward and inverse problems, respectively. A forward problem predicts the
output of a network for a given input by optimizing weights and biases. An
inverse problem finds equation parameters or coefficients that effectively
model the data. A Physics-Informed Neural Network (PINN) can solve both
problems. PINNs inject prior analytical information about the data into the
cost function to improve model performance outside the training set boundaries.
This also allows PINNs to efficiently solve problems with sparse data without
overfitting by extrapolating the model to fit larger trends in the data. The
prior information we implement is in the form of differential equations.
Residuals are the differences between the left-hand and right-hand sides of
corresponding differential equations; PINNs minimize these residuals to
effectively solve the differential equation and take advantage of prior
knowledge. In this way, the solution and parameters are embedded into the loss
function and optimized, allowing both the weights of the neural network and the
model parameters to be found simultaneously, solving both the forward and
inverse problems in the process. In this paper, we will create PINNs with
residuals of varying complexity, beginning with linear and quadratic models and
then expanding to fit models for the heat equation and other complex
differential equations. We will mainly use Python as the computing language,
using the PyTorch library to aid us in our research.

</details>


### [63] [Language Models for Controllable DNA Sequence Design](https://arxiv.org/abs/2507.19523)
*Xingyu Su,Xiner Li,Yuchao Lin,Ziqian Xie,Degui Zhi,Shuiwang Ji*

Main category: cs.LG

TL;DR: 针对可控DNA序列设计，本文提出ATGC-Gen，一个基于Transformer的生成模型，利用交叉模态编码整合生物信号，能生成符合特定生物学特性、性能优于现有方法的DNA序列。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在自然语言生成领域成果显著，但在DNA序列生成中的应用仍未充分探索，且缺乏能根据特定生物学特性生成可控DNA序列的方法。

Method: 提出ATGC-Gen模型，一个基于Transformer的可控生成器，采用交叉模态编码整合生物信号。模型支持仅解码器和仅编码器两种Transformer架构，并可灵活选择自回归或掩码恢复训练目标。在启动子、增强子序列设计及基于ChIP-Seq的蛋白质结合特异性建模任务上进行评估。

Result: 实验表明ATGC-Gen能生成流畅、多样且具有生物学相关性的序列，并与所需特性一致。与现有方法相比，模型在可控性和功能相关性方面有显著提升。

Conclusion: 该研究展示了语言模型在推进可编程基因组设计方面的巨大潜力，为可控DNA序列设计提供了有效工具。

Abstract: We consider controllable DNA sequence design, where sequences are generated
by conditioning on specific biological properties. While language models (LMs)
such as GPT and BERT have achieved remarkable success in natural language
generation, their application to DNA sequence generation remains largely
underexplored. In this work, we introduce ATGC-Gen, an Automated Transformer
Generator for Controllable Generation, which leverages cross-modal encoding to
integrate diverse biological signals. ATGC-Gen is instantiated with both
decoder-only and encoder-only transformer architectures, allowing flexible
training and generation under either autoregressive or masked recovery
objectives. We evaluate ATGC-Gen on representative tasks including promoter and
enhancer sequence design, and further introduce a new dataset based on ChIP-Seq
experiments for modeling protein binding specificity. Our experiments
demonstrate that ATGC-Gen can generate fluent, diverse, and biologically
relevant sequences aligned with the desired properties. Compared to prior
methods, our model achieves notable improvements in controllability and
functional relevance, highlighting the potential of language models in
advancing programmable genomic design. The source code is released at
(https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).

</details>


### [64] [Kolmogorov Arnold Network Autoencoder in Medicine](https://arxiv.org/abs/2507.19524)
*Ugo Lomoio,Pierangelo Veltri,Pietro Hiram Guzzi*

Main category: cs.LG

TL;DR: 本研究旨在对比基于Kolmogorov-Arnold Networks (KAN) 的自编码器与传统自编码器在心血管信号处理任务中的性能，涉及重建、生成、去噪、修复和异常检测等。


<details>
  <summary>Details</summary>
Motivation: Kolmogorov Arnold Networks (KAN) 通过在神经网络边上引入可学习激活函数，在多项任务中展现出优越性能。当前研究正致力于优化KAN架构并将其应用于自编码器等领域。本研究的动机是评估KAN架构的自编码器在心脏信号处理方面的潜力，并与参数量相同或更少的传统自编码器进行基准测试。

Method: 本研究将多种经典自编码器（如线性、卷积和变分自编码器）与其对应的Kolmogorov-Arnold自编码器进行对比。输入数据为心脏病学信号（来自听诊器的音频信号）。实验将在医学数据集`AbnormalHeartbeat`上进行，涵盖五种自编码器任务：重建、生成、去噪、图像修复和异常检测。

Result: 摘要中未提供具体研究结果。

Conclusion: 摘要中未得出研究结论。

Abstract: Deep learning neural networks architectures such Multi Layer Perceptrons
(MLP) and Convolutional blocks still play a crucial role in nowadays research
advancements. From a topological point of view, these architecture may be
represented as graphs in which we learn the functions related to the nodes
while fixed edges convey the information from the input to the output. A recent
work introduced a new architecture called Kolmogorov Arnold Networks (KAN) that
reports how putting learnable activation functions on the edges of the neural
network leads to better performances in multiple scenarios. Multiple studies
are focusing on optimizing the KAN architecture by adding important features
such as dropout regularization, Autoencoders (AE), model benchmarking and last,
but not least, the KAN Convolutional Network (KCN) that introduced matrix
convolution with KANs learning. This study aims to benchmark multiple versions
of vanilla AEs (such as Linear, Convolutional and Variational) against their
Kolmogorov-Arnold counterparts that have same or less number of parameters.
Using cardiological signals as model input, a total of five different classic
AE tasks were studied: reconstruction, generation, denoising, inpainting and
anomaly detection. The proposed experiments uses a medical dataset
\textit{AbnormalHeartbeat} that contains audio signals obtained from the
stethoscope.

</details>


### [65] [MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs](https://arxiv.org/abs/2507.19525)
*Chenchen Zhao,Zhengyuan Shi,Xiangyu Wen,Chengjie Liu,Yi Liu,Yunhao Zhou,Yuxiang Zhao,Hefei Feng,Yinan Zhu,Gwok-Waa Wan,Xin Cheng,Weiyu Chen,Yongqi Fu,Chujie Chen,Chenhao Xue,Guangyu Sun,Ying Wang,Yibo Lin,Jun Yang,Ning Xu,Xi Wang,Qiang Xu*

Main category: cs.LG

TL;DR: 为解决现有基准狭窄的问题，本文推出了MMCircuitEval，首个用于全面评估多模态大语言模型（MLLMs）在电子设计自动化（EDA）中性能的多模态基准，并揭示了模型在后端设计和复杂计算方面的显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在电路设计领域的评估基准范围过于狭窄，难以全面衡量其在电子设计自动化（EDA）中的能力。

Method: 引入MMCircuitEval，一个包含3614个精心策划的问题-答案对的多模态基准，涵盖数字和模拟电路的各项EDA阶段。这些问题来自教材、技术题库和实际文档，经过专家评审，并按设计阶段、电路类型、测试能力和难度进行分类。

Result: 对现有LLMs的广泛评估显示出显著的性能差距，特别是在后端设计和复杂计算方面，这突显了对目标训练数据集和建模方法的需求。

Conclusion: MMCircuitEval为推进MLLMs在EDA领域的应用提供了基础资源，有助于其整合到真实的电路设计工作流程中。

Abstract: The emergence of multimodal large language models (MLLMs) presents promising
opportunities for automation and enhancement in Electronic Design Automation
(EDA). However, comprehensively evaluating these models in circuit design
remains challenging due to the narrow scope of existing benchmarks. To bridge
this gap, we introduce MMCircuitEval, the first multimodal benchmark
specifically designed to assess MLLM performance comprehensively across diverse
EDA tasks. MMCircuitEval comprises 3614 meticulously curated question-answer
(QA) pairs spanning digital and analog circuits across critical EDA stages -
ranging from general knowledge and specifications to front-end and back-end
design. Derived from textbooks, technical question banks, datasheets, and
real-world documentation, each QA pair undergoes rigorous expert review for
accuracy and relevance. Our benchmark uniquely categorizes questions by design
stage, circuit type, tested abilities (knowledge, comprehension, reasoning,
computation), and difficulty level, enabling detailed analysis of model
capabilities and limitations. Extensive evaluations reveal significant
performance gaps among existing LLMs, particularly in back-end design and
complex computations, highlighting the critical need for targeted training
datasets and modeling approaches. MMCircuitEval provides a foundational
resource for advancing MLLMs in EDA, facilitating their integration into
real-world circuit design workflows. Our benchmark is available at
https://github.com/cure-lab/MMCircuitEval.

</details>


### [66] [Quantizing Text-attributed Graphs for Semantic-Structural Integration](https://arxiv.org/abs/2507.19526)
*Jianyuan Bo,Hao Wu,Yuan Fang*

Main category: cs.LG

TL;DR: STAG提出一种自监督框架，通过量化图结构为离散token，实现图学习与LLM的无缝结合，支持零样本迁移，并在节点分类任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将图结构信息嵌入到LLM兼容格式时面临挑战：计算成本高、信息丢失、且需要源域的标注数据，限制了适应性。

Method: 提出STAG框架，通过冻结码本将图结构信息直接量化为离散token。采用软分配和KL散度引导量化，以解决图数据缺乏自然token化结构的问题。该框架支持LLM和传统学习方法，实现零样本迁移学习。

Result: STAG在多个节点分类基准测试中表现出最先进的性能，并与不同LLM架构兼容。

Conclusion: STAG为弥合图学习和LLMs之间的鸿沟提供了一个优雅的解决方案，克服了当前方法的局限性，实现了高效且无需标注数据的图数据处理。

Abstract: Text-attributed graphs (TAGs) have emerged as a powerful representation for
modeling complex relationships across diverse domains. With the rise of large
language models (LLMs), there is growing interest in leveraging their
capabilities for graph learning. However, current approaches face significant
challenges in embedding structural information into LLM-compatible formats,
requiring either computationally expensive alignment mechanisms or manual graph
verbalization techniques that often lose critical structural details. Moreover,
these methods typically require labeled data from source domains for effective
transfer learning, significantly constraining their adaptability. We propose
STAG, a novel self-supervised framework that directly quantizes graph
structural information into discrete tokens using a frozen codebook. Unlike
traditional quantization approaches, our method employs soft assignment and KL
divergence guided quantization to address the unique challenges of graph data,
which lacks natural tokenization structures. Our framework enables both
LLM-based and traditional learning approaches, supporting true zero-shot
transfer learning without requiring labeled data even in the source domain.
Extensive experiments demonstrate state-of-the-art performance across multiple
node classification benchmarks while maintaining compatibility with different
LLM architectures, offering an elegant solution to bridging graph learning with
LLMs.

</details>


### [67] [Research on the application of graph data structure and graph neural network in node classification/clustering tasks](https://arxiv.org/abs/2507.19527)
*Yihan Wang,Jianing Zhao*

Main category: cs.LG

TL;DR: 本研究通过理论分析和实验评估，发现图神经网络（GNNs）在节点分类和聚类任务中，相比传统图算法能显著提高43%到70%的准确性，并探索了二者的集成策略。


<details>
  <summary>Details</summary>
Motivation: 图结构数据普遍存在，但其非欧几里得特性对传统机器学习方法构成挑战，因此需要更有效的数据处理和分析方法。

Method: 研究调查了图数据结构、经典图算法和图神经网络（GNNs），进行了全面的理论分析和比较评估。通过比较实验，量化评估了传统算法与GNNs在节点分类和聚类任务中的性能差异。此外，还探索了经典算法与GNN架构的集成策略。

Result: GNNs在准确性上比传统方法有显著提升，达到43%至70%。研究还探索了经典算法与GNN架构的集成策略。

Conclusion: GNNs在处理图结构数据任务上表现出显著优势，并为图表示学习的进一步发展提供了理论指导和集成策略方向。

Abstract: Graph-structured data are pervasive across domains including social networks,
biological networks, and knowledge graphs. Due to their non-Euclidean nature,
such data pose significant challenges to conventional machine learning methods.
This study investigates graph data structures, classical graph algorithms, and
Graph Neural Networks (GNNs), providing comprehensive theoretical analysis and
comparative evaluation. Through comparative experiments, we quantitatively
assess performance differences between traditional algorithms and GNNs in node
classification and clustering tasks. Results show GNNs achieve substantial
accuracy improvements of 43% to 70% over traditional methods. We further
explore integration strategies between classical algorithms and GNN
architectures, providing theoretical guidance for advancing graph
representation learning research.

</details>


### [68] [Machine Learning Risk Intelligence for Green Hydrogen Investment: Insights for Duqm R3 Auction](https://arxiv.org/abs/2507.19529)
*Obumneme Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: 本文提出一个基于人工智能的决策支持系统，利用公开气象数据预测绿氢基础设施的维护压力指数（MPI），以弥补沙漠环境下缺乏运营数据的风险评估空白。


<details>
  <summary>Details</summary>
Motivation: 大规模绿氢设施在沙漠环境下缺乏历史运营和维护数据，导致基础设施规划和拍卖决策面临巨大的风险评估知识空白。鉴于此，需要一个利用可获取环境数据预测维护压力的工具，以实现精确的风险评估。

Method: 本文提出了一个人工智能决策支持系统。该系统利用公开可用的气象数据，开发并预测一个“维护压力指数”（MPI），旨在评估未来绿氢基础设施的风险和维护需求。

Result: 该MPI能够预测绿氢基础设施的风险水平和未来的维护需求。该工具能增强监管预见性和运营决策能力，并通过时间基准测试来评估和验证项目性能，即使在缺乏历史运营基准的情况下也能将时间风险智能纳入拍卖评估标准。

Conclusion: 所提出的人工智能驱动的MPI工具，通过利用环境数据作为可靠代理，有效地解决了沙漠地区绿氢项目规划和评估中因缺乏历史运营数据而导致的风险评估挑战，从而提升了决策质量和监管能力。

Abstract: As green hydrogen emerges as a major component of global decarbonisation,
Oman has positioned itself strategically through national auctions and
international partnerships. Following two successful green hydrogen project
rounds, the country launched its third auction (R3) in the Duqm region. While
this area exhibits relative geospatial homogeneity, it is still vulnerable to
environmental fluctuations that pose inherent risks to productivity. Despite
growing global investment in green hydrogen, operational data remains scarce,
with major projects like Saudi Arabia's NEOM facility not expected to commence
production until 2026, and Oman's ACME Duqm project scheduled for 2028. This
absence of historical maintenance and performance data from large-scale
hydrogen facilities in desert environments creates a major knowledge gap for
accurate risk assessment for infrastructure planning and auction decisions.
Given this data void, environmental conditions emerge as accessible and
reliable proxy for predicting infrastructure maintenance pressures, because
harsh desert conditions such as dust storms, extreme temperatures, and humidity
fluctuations are well-documented drivers of equipment degradation in renewable
energy systems. To address this challenge, this paper proposes an Artificial
Intelligence decision support system that leverages publicly available
meteorological data to develop a predictive Maintenance Pressure Index (MPI),
which predicts risk levels and future maintenance demands on hydrogen
infrastructure. This tool strengthens regulatory foresight and operational
decision-making by enabling temporal benchmarking to assess and validate
performance claims over time. It can be used to incorporate temporal risk
intelligence into auction evaluation criteria despite the absence of historical
operational benchmarks.

</details>


### [69] [Clinical-Grade Blood Pressure Prediction in ICU Settings: An Ensemble Framework with Uncertainty Quantification and Cross-Institutional Validation](https://arxiv.org/abs/2507.19530)
*Md Basit Azam,Sarangthem Ibotombi Singh*

Main category: cs.LG

TL;DR: 本研究提出了一个针对电子健康记录（EHR）血压预测的全面机器学习框架，包含新颖的数据泄露预防、不确定性量化和跨机构验证，旨在解决现有方法的局限性，并为重症监护环境提供更可靠的AI辅助血压监测。


<details>
  <summary>Details</summary>
Motivation: 血压监测在重症监护室（ICU）至关重要，而当前机器学习方法在EHR血压预测方面存在外部验证缺失、缺乏不确定性量化和数据泄露预防不足三大局限性，导致其临床应用受限。

Method: 本研究提出了一个综合框架，包括系统性数据泄露预防、通过分位数回归实现不确定性量化，以及在MIMIC-III和eICU数据库之间进行外部验证。该方法采用包含梯度提升、随机森林和XGBoost的集成框架，利用来自五个生理领域的74个特征。

Result: 内部验证显示出临床可接受的性能（收缩压：R^2=0.86，RMSE=6.03 mmHg；舒张压：R^2=0.49，RMSE=7.13 mmHg），符合AAMI标准。外部验证表现出30%的性能下降，在低血压患者中存在显著局限性。不确定性量化生成了有效的预测区间（收缩压覆盖率为80.3%，舒张压为79.9%），支持风险分层方案。

Conclusion: 该框架为重症监护环境中AI辅助血压监测的跨机构部署提供了更切合实际的预期，并解决了现有机器学习方法的关键局限性。

Abstract: Blood pressure (BP) monitoring is critical in in tensive care units (ICUs)
where hemodynamic instability can
  rapidly progress to cardiovascular collapse. Current machine
  learning (ML) approaches suffer from three limitations: lack of
  external validation, absence of uncertainty quantification, and
  inadequate data leakage prevention. This study presents the
  first comprehensive framework with novel algorithmic leakage
  prevention, uncertainty quantification, and cross-institutional
  validation for electronic health records (EHRs) based BP pre dictions. Our
methodology implemented systematic data leakage
  prevention, uncertainty quantification through quantile regres sion, and
external validation between the MIMIC-III and eICU
  databases. An ensemble framework combines Gradient Boosting,
  Random Forest, and XGBoost with 74 features across five
  physiological domains. Internal validation achieved a clinically
  acceptable performance (for SBP: R^2 = 0.86, RMSE = 6.03
  mmHg; DBP: R^2 = 0.49, RMSE = 7.13 mmHg), meeting AAMI
  standards. External validation showed 30% degradation with
  critical limitations in patients with hypotensive. Uncertainty
  quantification generated valid prediction intervals (80.3% SBP
  and 79.9% DBP coverage), enabling risk-stratified protocols
  with narrow intervals (< 15 mmHg) for standard monitoring
  and wide intervals (> 30 mmHg) for manual verification. This
  framework provides realistic deployment expectations for cross institutional
AI-assisted BP monitoring in critical care settings.
  The source code is publicly available at https://github.com/
  mdbasit897/clinical-bp-prediction-ehr.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [70] [On the Limitations of Ray-Tracing for Learning-Based RF Tasks in Urban Environments](https://arxiv.org/abs/2507.19653)
*Armen Manukyan,Hrant Khachatrian,Edvard Ghukasyan,Theofanis P. Raptis*

Main category: cs.NI

TL;DR: 研究Sionna光线追踪在室外蜂窝链路中的真实性，发现天线参数至关重要，但城市噪声建模仍是高保真模拟的挑战。


<details>
  <summary>Details</summary>
Motivation: 评估Sionna v1.0.2光线追踪模拟器在真实城市环境下（罗马市中心）室外蜂窝链路建模的真实性和准确性。

Method: 利用罗马市中心1664个用户设备和6个基站的真实测量数据。系统性地调整了光线追踪模拟参数，包括路径深度、反射/散射/折射标志、载波频率及天线属性。通过测量功率与模拟功率之间的斯皮尔曼相关性，以及基于RSSI指纹的kNN定位算法来评估模拟器的准确性。对天线位置和方向进行了贪婪优化。

Result: 求解器超参数对结果影响不大，而天线位置和方向则具有决定性作用。通过贪婪优化，斯皮尔曼相关性提升了5%至130%。使用模拟数据作为参考的kNN定位误差在真实样本上降低了三分之一，但仍是纯真实数据误差的两倍。

Conclusion: 精确的几何模型和可信的天线模型是必要但不充分的。忠实地捕捉剩余的城市噪声，对于可迁移的高保真室外射频模拟来说，仍然是一个开放的挑战。

Abstract: We study the realism of Sionna v1.0.2 ray-tracing for outdoor cellular links
in central Rome. We use a real measurement set of 1,664 user-equipments (UEs)
and six nominal base-station (BS) sites. Using these fixed positions we
systematically vary the main simulation parameters, including path depth,
diffuse/specular/refraction flags, carrier frequency, as well as antenna's
properties like its altitude, radiation pattern, and orientation. Simulator
fidelity is scored for each base station via Spearman correlation between
measured and simulated powers, and by a fingerprint-based k-nearest-neighbor
localization algorithm using RSSI-based fingerprints. Across all experiments,
solver hyper-parameters are having immaterial effect on the chosen metrics. On
the contrary, antenna locations and orientations prove decisive. By simple
greedy optimization we improve the Spearman correlation by 5% to 130% for
various base stations, while kNN-based localization error using only simulated
data as reference points is decreased by one-third on real-world samples, while
staying twice higher than the error with purely real data. Precise geometry and
credible antenna models are therefore necessary but not sufficient; faithfully
capturing the residual urban noise remains an open challenge for transferable,
high-fidelity outdoor RF simulation.

</details>


### [71] ["X of Information'' Continuum: A Survey on AI-Driven Multi-dimensional Metrics for Next-Generation Networked Systems](https://arxiv.org/abs/2507.19657)
*Beining Wu,Jun Huang,Shui Yu*

Main category: cs.NI

TL;DR: 下一代网络系统正从基于吞吐量转向注重信息质量的设计。本综述首次提出一个四维框架来量化信息指标，探讨了人工智能在优化这些指标中的作用，并展示其在自动驾驶、数字孪生等六大应用领域的巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 传统网络指标（如延迟和丢包率）不足以量化自动驾驶、数字孪生和元宇宙等现代智能应用对信息质量的精细化要求。

Method: 本文进行了一项综合性调查，首次系统研究了“信息之X”连续体，并引入了一个四维分类框架来构建信息指标，涵盖时间、质量/效用、可靠性/鲁棒性以及网络/通信维度。

Result: 研究揭示了这些维度之间日益增长的相互依赖性；分析表明人工智能技术（如深度强化学习）能够实现对竞争性信息质量目标的自适应、上下文感知优化；通过对六个关键应用领域的广泛研究，展示了多维信息指标在满足多样化操作需求方面的革命性潜力。

Conclusion: 多维信息指标对于满足下一代智能应用的复杂信息质量需求至关重要，人工智能技术是实现这些目标优化的关键推动力，但仍存在显著的实施挑战。

Abstract: The development of next-generation networking systems has inherently shifted
from throughput-based paradigms towards intelligent, information-aware designs
that emphasize the quality, relevance, and utility of transmitted information,
rather than sheer data volume. While classical network metrics, such as latency
and packet loss, remain significant, they are insufficient to quantify the
nuanced information quality requirements of modern intelligent applications,
including autonomous vehicles, digital twins, and metaverse environments. In
this survey, we present the first comprehensive study of the ``X of
Information'' continuum by introducing a systematic four-dimensional taxonomic
framework that structures information metrics along temporal, quality/utility,
reliability/robustness, and network/communication dimensions. We uncover the
increasing interdependencies among these dimensions, whereby temporal freshness
triggers quality evaluation, which in turn helps with reliability appraisal,
ultimately enabling effective network delivery. Our analysis reveals that
artificial intelligence technologies, such as deep reinforcement learning,
multi-agent systems, and neural optimization models, enable adaptive,
context-aware optimization of competing information quality objectives. In our
extensive study of six critical application domains, covering autonomous
transportation, industrial IoT, healthcare digital twins, UAV communications,
LLM ecosystems, and metaverse settings, we illustrate the revolutionary promise
of multi-dimensional information metrics for meeting diverse operational needs.
Our survey identifies prominent implementation challenges, including ...

</details>


### [72] [Predicting Locations of Cell Towers for Network Capacity Expansion](https://arxiv.org/abs/2507.19925)
*Sowmiyan Morri,Joy Bose,L Raghunatha Reddy,Sai Hareesh Anamandra*

Main category: cs.NI

TL;DR: 本文提出一个机器学习框架，结合深度神经网络和空间聚类，智能推荐新的蜂窝基站位置，以克服传统方法在网络扩容中未能考虑真实世界因素和预算限制的不足。


<details>
  <summary>Details</summary>
Motivation: 电信运营商的网络扩容面临挑战，需要策略性地放置新基站。传统方法（如人工路测和静态优化）未能充分考虑用户密度、地形特征和财务限制等关键真实世界因素，导致覆盖和性能不佳。

Method: 本文提出了一个基于机器学习的框架。该框架结合深度神经网络进行信号覆盖预测，并利用空间聚类技术推荐服务不足区域的新塔站位置。系统整合地理空间、人口统计和基础设施数据，并纳入预算约束以优先部署。它在一个迭代规划循环中运行，每次提议安装后细化覆盖估计，实现自适应和成本效益的扩展。

Result: 尽管受限于数据可用性未能进行全面仿真，但所提出的架构具有模块化、对缺失输入鲁棒以及在多样化部署场景中具有通用性。

Conclusion: 该方法通过提供一种可扩展、数据驱动的替代方案，取代传统人工方法，从而推进了无线网络规划。

Abstract: Network capacity expansion is a critical challenge for telecom operators,
requiring strategic placement of new cell sites to ensure optimal coverage and
performance. Traditional approaches, such as manual drive tests and static
optimization, often fail to consider key real-world factors including user
density, terrain features, and financial constraints. In this paper, we propose
a machine learning-based framework that combines deep neural networks for
signal coverage prediction with spatial clustering to recommend new tower
locations in underserved areas. The system integrates geospatial, demographic,
and infrastructural data, and incorporates budget-aware constraints to
prioritize deployments. Operating within an iterative planning loop, the
framework refines coverage estimates after each proposed installation, enabling
adaptive and cost-effective expansion. While full-scale simulation was limited
by data availability, the architecture is modular, robust to missing inputs,
and generalizable across diverse deployment scenarios. This approach advances
radio network planning by offering a scalable, data-driven alternative to
manual methods.

</details>


### [73] [Optimizing Spreading Factor Selection for Mobile LoRa Gateways Using Single-Channel Hardware](https://arxiv.org/abs/2507.19938)
*W. A. Sasindu Wijesuriya*

Main category: cs.NI

TL;DR: 针对低成本单通道移动LoRa网关缺乏动态配置的问题，本文提出一种两阶段算法以静态选择最佳扩展因子（SF），通过规则排除和加权评分模型实现。该方法在模拟中能以超过92%的准确率匹配最佳SF，适用于成本敏感型移动LoRa部署。


<details>
  <summary>Details</summary>
Motivation: 低成本单通道移动LoRa网关缺乏动态配置支持，导致难以维持可靠通信；而传统LoRaWAN的自适应数据速率（ADR）机制通常仅由昂贵的多通道网关支持。

Method: 提出一种两阶段算法来静态选择最优SF。第一阶段通过规则排除（基于距离、数据速率、链路裕度和法规限制）筛选不符合条件的SF；第二阶段使用包含空中时间、能耗、数据速率和链路鲁棒性的加权评分模型评估剩余SF。该算法通过现场测试和NS-3仿真进行验证。

Result: 在672个模拟场景中，所选SF与最佳SF的匹配度超过92%，证明了算法的有效性。

Conclusion: 该方法为动态协议提供了一种可扩展的替代方案，可在农业和农村传感等成本敏感环境中实现可靠的移动LoRa部署。

Abstract: The deployment of mobile LoRa gateways using low-cost single-channel hardware
presents a significant challenge in maintaining reliable communication due to
the lack of dynamic configuration support. In traditional LoRaWAN networks,
Adaptive Data Rate (ADR) mechanisms optimize communication parameters in real
time. However, such features are typically supported only by expensive
multi-channel gateways. This study proposes a cost-effective and
energy-efficient solution by statically selecting the optimal Spreading Factor
(SF) using a two-phase algorithm. The method first applies rule-based exclusion
to eliminate SFs that violate constraints related to distance, data rate, link
margin, and regulatory limits. Remaining candidates are then evaluated using a
weighted scoring model incorporating Time-on-Air, energy consumption, data
rate, and link robustness. The proposed algorithm was validated through
extensive field tests and NS-3 simulations under line-of-sight conditions.
Results demonstrate that the selected SF matched the optimal SF in over 92% of
cases across 672 simulated scenarios, confirming the algorithm's effectiveness.
This approach offers a scalable alternative to dynamic protocols, enabling
reliable mobile LoRa deployments in cost-sensitive environments such as
agriculture and rural sensing applications.

</details>


### [74] [A Scalable Resource Management Layer for FPGA SoCs in 6G Radio Units](https://arxiv.org/abs/2507.19963)
*Nikolaos Bartzoudis,José Rubio Fernández,David López-Bueno,Antonio Román Villarroel*

Main category: cs.NI

TL;DR: 本文提出一种动态资源管理层，旨在解决5G和边缘计算中FPGA SoC设备计算资源利用率低的问题。


<details>
  <summary>Details</summary>
Motivation: 解决5G无线和边缘计算基础设施中FPGA SoC设备计算资源利用率低下。

Method: 开发一个资源管理层，该层能够根据上下文事件动态迁移和扩展FPGA SoC设备内的功能。该层是构建分层、数据驱动的微编排器以管理功能生命周期的基础。

Result: 在本文中，所提出的资源管理层被用于根据计算机视觉边缘应用识别的事件来重新配置功能。

Conclusion: 该工作提出了一种通过动态资源管理层提升FPGA SoC设备资源利用率的视角，并为未来构建功能生命周期微编排器奠定了基础。

Abstract: This work presents a perspective on addressing the underutilization of
computing resources in FPGA SoC devices deployed in 5G radio and edge computing
infrastructure. The initial step in this approach involves developing a
resource management layer capable of dynamically migrating and scaling
functions within these devices in response to contextual events. This layer
serves as the foundation for designing a hierarchical, data-driven
micro-orchestrator responsible for managing the lifecycle of functions in FPGA
SoC devices. In this paper, the proposed resource management layer is utilized
to reconfigure a function based on events identified by a computer vision edge
application.

</details>


### [75] [Towards Next Generation Immersive Applications in 5G Environments](https://arxiv.org/abs/2507.20050)
*Rohail Asim,Ankit Bhardwaj,Lakshmi Suramanian,Yasir Zaki*

Main category: cs.NI

TL;DR: 本文提出Hera框架，一个专为下一代多用户沉浸式应用设计的模块化方案，通过优化流媒体和速率控制，旨在克服无线网络瓶颈，提升低延迟、高画质和公平性。


<details>
  <summary>Details</summary>
Motivation: 多用户沉浸式现实（MIR）应用发展迅速，但当前无线网络（特别是5G）在满足其所需的高带宽和超低延迟方面存在关键瓶颈，影响了下一代MIR体验。

Method: 本文提出Hera框架，包含一个用于AR/VR系统的高级流媒体与同步层，以及一个为动态无线环境优化的低级基于延迟的QoE（体验质量）感知速率控制协议。Hera将应用感知流媒体逻辑与以QoE为中心的速率控制核心相结合，以实现自适应视频质量、多用户公平性和低延迟通信。

Result: 实验证明，Hera在保持可比吞吐量性能的同时，将延迟降低高达66%；视觉质量更高，平均比特率提高50%；并显著改善了多用户公平性，优于现有最先进的速率控制算法。

Conclusion: Hera通过连接应用层响应能力与网络层适应性，为未来更具可伸缩性、鲁棒性和高保真度的多用户沉浸式体验奠定了基础。

Abstract: The Multi-user Immersive Reality (MIR) landscape is evolving rapidly, with
applications spanning virtual collaboration, entertainment, and training.
However, wireless network limitations create a critical bottleneck, struggling
to meet the high-bandwidth and ultra-low latency demands essential for
next-generation MIR experiences. This paper presents Hera, a modular framework
for next-generation immersive applications, comprising a high-level streaming
and synchronization layer for AR/VR systems and a low-level delay-based
QoE-aware rate control protocol optimized for dynamic wireless environments.
The Hera framework integrates application-aware streaming logic with a
QoE-centric rate control core, enabling adaptive video quality, multi-user
fairness, and low-latency communication across challenging 5G network
conditions. We demonstrate that Hera outperforms existing state-of-the-art rate
control algorithms by maintaining up to 66% lower latencies with comparable
throughput performance, higher visual quality with 50% average bitrate
improvements in our analysis, and improved fairness. By bridging the gap
between application-level responsiveness and network-level adaptability, Hera
lays the foundation for more scalable, robust, and high-fidelity multi-user
immersive experiences.

</details>


### [76] [Packet-Level DDoS Data Augmentation Using Dual-Stream Temporal-Field Diffusion](https://arxiv.org/abs/2507.20115)
*Gongli Xi,Ye Tian,Yannan Hu,Yuchao Zhang,Yapeng Niu,Xiangyang Gong*

Main category: cs.NI

TL;DR: 提出DSTF-Diffusion模型，用于生成高质量的合成网络流量数据，以增强基于机器学习的DDoS攻击检测能力。


<details>
  <summary>Details</summary>
Motivation: 机器学习在DDoS检测中依赖高质量标注数据集，但此类数据稀缺。现有合成数据生成方法无法有效捕捉DDoS攻击的复杂时空模式，导致生成数据与真实数据差异大，影响ML检测精度。

Method: 提出DSTF-Diffusion模型，这是一个基于扩散模型的多视图、多流网络流量生成模型。它包含两个主要流：一个“字段流”利用空间映射将网络数据特性与预训练的稳定扩散模型结合；另一个“时间流”（原文spatial stream根据描述应为temporal stream）采用动态时间建模方法，精确捕捉网络流量的内在时间模式。

Result: 该模型生成的数据与原始数据相比，表现出更高的统计相似性，并且在多种下游任务（如DDoS检测）中显著提升了性能。

Conclusion: DSTF-Diffusion模型通过其创新的双流架构，成功解决了合成网络流量数据生成中时空模式捕捉不足的问题，为DDoS检测提供了更真实、更高质量的训练数据，从而提升了检测效率和准确性。

Abstract: In response to Distributed Denial of Service (DDoS) attacks, recent research
efforts increasingly rely on Machine Learning (ML)-based solutions, whose
effectiveness largely depends on the quality of labeled training datasets. To
address the scarcity of such datasets, data augmentation with synthetic traces
is often employed. However, current synthetic trace generation methods struggle
to capture the complex temporal patterns and spatial distributions exhibited in
emerging DDoS attacks. This results in insufficient resemblance to real traces
and unsatisfied detection accuracy when applied to ML tasks. In this paper, we
propose Dual-Stream Temporal-Field Diffusion (DSTF-Diffusion), a multi-view,
multi-stream network traffic generative model based on diffusion models,
featuring two main streams: The field stream utilizes spatial mapping to bridge
network data characteristics with pre-trained realms of stable diffusion
models, effectively translating complex network interactions into formats that
stable diffusion can process, while the spatial stream adopts a dynamic
temporal modeling approach, meticulously capturing the intrinsic temporal
patterns of network traffic. Extensive experiments demonstrate that data
generated by our model exhibits higher statistical similarity to originals
compared to current state-of-the-art solutions, and enhance performances on a
wide range of downstream tasks.

</details>


### [77] [Accelerating Containerized Service Delivery at the Network Edge](https://arxiv.org/abs/2507.20116)
*Yinuo Deng,Hailiang Zhao,Dongjing Wang,Peng Chen,Wenzhuo Qian,Jianwei Yin,Schahram Dustdar,Shuiguang Deng*

Main category: cs.NI

TL;DR: PeerSync是一个去中心化P2P系统，旨在优化边缘网络中的容器镜像分发，通过智能下载引擎和管理机制，显著提高了分发速度并降低了网络流量。


<details>
  <summary>Details</summary>
Motivation: 在资源受限和网络动态变化的边缘环境中，实现高效的容器镜像分发对机器学习推理至关重要，但面临巨大挑战。

Method: 提出PeerSync系统，一个去中心化的P2P容器镜像分发系统。它采用流行度与网络感知的下载引擎（利用滑动窗口机制），并集成了自动化跟踪器选举以快速发现对等节点，以及动态缓存管理以高效利用存储。系统使用Rust语言实现，并在物理边缘设备和Docker仿真环境中进行广泛测试。

Result: 实验结果显示，PeerSync的分发速度相较于Baseline、Dragonfly和Kraken分别提高了2.72倍、1.79倍和1.28倍。在拥塞和变化的组网条件下，峰值跨网络流量显著减少了90.72%。

Conclusion: PeerSync能够有效应对边缘环境下的挑战，显著优化容器镜像分发，实现更高的分发速度和更低的峰值网络流量。

Abstract: Efficient container image distribution is crucial for enabling machine
learning inference at the network edge, where resource limitations and dynamic
network conditions create significant challenges. In this paper, we present
PeerSync, a decentralized P2P-based system designed to optimize image
distribution in edge environments. PeerSync employs a popularity- and
network-aware download engine that dynamically adapts to content popularity and
real-time network conditions using a sliding window mechanism. PeerSync further
integrates automated tracker election for rapid peer discovery and dynamic
cache management for efficient storage utilization. We implement PeerSync with
8000+ lines of Rust code and test its performance extensively on both physical
edge devices and Docker-based emulations. Experimental results show that
PeerSync delivers a remarkable speed increase of 2.72$\times$, 1.79$\times$,
and 1.28$\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,
while significantly reducing peak cross-network traffic by 90.72\% under
congested and varying network conditions.

</details>


### [78] [Democracy for DAOs: An Empirical Study of Decentralized Governance and Dynamic (Case Study Internet Computer SNS Ecosystem)](https://arxiv.org/abs/2507.20234)
*Burak Arda Okutan,Stefan Schmid,Yvonne-Anne Pignolet*

Main category: cs.NI

TL;DR: 对ICP的SNS框架下DAO用户治理行为的实证研究，发现其具有高参与度、高效率和持续参与度。


<details>
  <summary>Details</summary>
Motivation: 研究去中心化自治组织（DAO）在无中心化领导下的治理机制，特别是评估互联网计算机协议（ICP）的SNS框架下用户行为和治理效率。

Method: 采用实证研究方法，分析了来自14个SNS DAO的3000多个提案，时间跨度20个月。衡量指标包括参与率、提案提交和通过率、决策时长以及随时间变化的指标趋势。并与其他区块链平台上的DAO进行对比。

Result: 总体提案通过率较高，SNS DAO表现出略高的共识度、更高的活跃度、更低的成本和更快的决策。与其它框架下参与度随时间下降的趋势不同，SNS DAO展示了持续或增长的参与度。

Conclusion: ICP的SNS治理机制和流程带来了更高的活动性、更低的成本和更快的决策，并且最重要的是，能够维持或增加用户参与度，优于其他一些DAO框架。

Abstract: Decentralized autonomous organizations (DAOs) rely on governance mechanism
without centralized leadership. This paper presents an empirical study of user
behavior in governance for a variety of DAOs, ranging from DeFi to gaming,
using the Internet Computer Protocol DAO framework called SNS (Service Nervous
System). To analyse user engagement, we measure participation rates and
frequency of proposals submission and voter approval rates. We evaluate
decision duration times to determine DAO agility. To investigate dynamic
aspects, we also measure metric shifts in time. We evaluate over 3,000
proposals submitted in a time frame of 20 months from 14 SNS DAOs. The selected
DAO have been existing between 6 and 20 months and cover a wide spectrum of use
cases, treasury sizes, and number of participants. We also compare our results
for SNS DAOs with DAOs from other blockchain platforms. While approval rates
are generally high for all DAOs studied, SNS DAOs show slightly more alignment.
We observe that the SNS governance mechanisms and processes in ICP lead to
higher activity, lower costs and faster decisions. Most importantly, in
contrast to studies which report a decline in participation over time for other
frameworks, SNS DAOs exhibit sustained or increasing engagement levels over
time.

</details>


### [79] [Joint Fiber and Free Space Optical Infrastructure Planning for Hybrid Integrated Access and Backhaul Networks](https://arxiv.org/abs/2507.20367)
*Charitha Madapatha,Piotr Lechowicz,Carlos Natalino,Paolo Monti,Tommy Svensson*

Main category: cs.NI

TL;DR: 本研究针对5G/6G综合接入回传（IAB）网络在光纤部署受成本限制下的规划问题，探索了混合光纤/自由空间光（FSO）部署对网络覆盖、能效和成本的影响，发现其能有效降低成本并提升服务覆盖。


<details>
  <summary>Details</summary>
Motivation: 综合接入回传（IAB）是5G及未来6G网络的关键技术，但其回传链路对速率和可靠性要求高，需要恰当的网络规划来保证性能。特别是在光纤连接受成本限制的情况下，寻找有效的解决方案至关重要。

Method: 本文研究了基础设施规划和优化对IAB网络覆盖的影响。具体地，在节点光纤连接受成本限制的场景下，分析了引入自由空间光（FSO）通信链路后，网络性能增益和能效的变化。

Result: 研究结果表明，混合光纤/FSO部署相比全光纤网络能显著节省成本，并能在战略性链路部署中提供有利的权衡，同时提高了服务覆盖概率。通过适当的网络规划，服务覆盖、能效和成本效率均可得到提升。

Conclusion: 通过适当的网络规划，结合混合光纤/FSO部署，可以显著提升IAB网络的服务覆盖、能效和成本效率，尤其在光纤连接受限的场景下，这提供了一个有益的解决方案。

Abstract: Integrated access and backhaul (IAB) is one of the promising techniques for
5G networks and beyond (6G), in which the same node/hardware is used to provide
both backhaul and cellular services in a multi-hop architecture. Due to the
sensitivity of the backhaul links with high rate/reliability demands, proper
network planning is needed to ensure the IAB network performs with the desired
performance levels. In this paper, we study the effect of infrastructure
planning and optimization on the coverage of IAB networks. We concentrate on
the cases where the fiber connectivity to the nodes is constrained due to cost.
Thereby, we study the performance gains and energy efficiency in the presence
of free-space optical (FSO) communication links. Our results indicate hybrid
fiber/FSO deployments offer substantial cost savings compared to fully fibered
networks, suggesting a beneficial trade-off for strategic link deployment while
improving the service coverage probability. As we show, with proper network
planning, the service coverage, energy efficiency, and cost efficiency can be
improved.

</details>


### [80] [Teleoperating Autonomous Vehicles over Commercial 5G Networks: Are We There Yet?](https://arxiv.org/abs/2507.20438)
*Rostand A. K. Fezeu,Jason Carpenter,Rushikesh Zende,Sree Ganesh Lalitaditya Divakarla,Nitin Varyani,Faaiq Bilal,Steven Sleder,Nanditha Naik,Duncan Joly,Eman Ramadan,Ajay Kumar Gurumadaiah,Zhi-Li Zhang*

Main category: cs.NI

TL;DR: 该研究系统性地评估了商用5G网络支持自动驾驶车辆远程操作的可行性，特别关注上行传感器数据传输，揭示了当前5G的挑战和现有数据流机制的局限性，为未来系统协同设计提供了洞察。


<details>
  <summary>Details</summary>
Motivation: 5G网络旨在支持自动驾驶车辆的远程操作（AVs teleoperation），但其可行性，特别是传感器数据（如摄像头和激光雷达数据）的及时传输性能，需要进行系统性研究。

Method: 研究采用跨层和端到端（E2E）视角，分析物理层（如信道条件、无线资源分配、切换）对E2E延迟的影响，并检查5G网络对上层协议（如RTSP、WebRTC）和E2E应用体验质量（QoE）自适应机制性能的影响，重点关注上行传感器数据传输。

Result: 研究揭示了当前5G网络在AV远程操作方面存在的挑战，以及现有传感器数据流传输机制的局限性。

Conclusion: 研究获得的洞察将有助于未来无线网络、边缘云系统和应用进行协同设计，以克服自动驾驶车辆远程操作中的低延迟障碍。

Abstract: Remote driving, or teleoperating Autonomous Vehicles (AVs), is a key
application that emerging 5G networks aim to support. In this paper, we conduct
a systematic feasibility study of AV teleoperation over commercial 5G networks
from both cross-layer and end-to-end (E2E) perspectives. Given the critical
importance of timely delivery of sensor data, such as camera and LiDAR data,
for AV teleoperation, we focus in particular on the performance of uplink
sensor data delivery. We analyze the impacts of Physical Layer (PHY layer) 5G
radio network factors, including channel conditions, radio resource allocation,
and Handovers (HOs), on E2E latency performance. We also examine the impacts of
5G networks on the performance of upper-layer protocols and E2E application
Quality-of-Experience (QoE) adaptation mechanisms used for real-time sensor
data delivery, such as Real-Time Streaming Protocol (RTSP) and Web Real Time
Communication (WebRTC). Our study reveals the challenges posed by today's 5G
networks and the limitations of existing sensor data streaming mechanisms. The
insights gained will help inform the co-design of future-generation wireless
networks, edge cloud systems, and applications to overcome the low-latency
barriers in AV teleoperation.

</details>


### [81] [DD-JSCC: Dynamic Deep Joint Source-Channel Coding for Semantic Communications](https://arxiv.org/abs/2507.20467)
*Avi Deb Raha,Apurba Adhikary,Mrityunjoy Gain,Yumin Park,Walid Saad,Choong Seon Hong*

Main category: cs.NI

TL;DR: 传统的Deep-JSCC固定结构限制了其适应性。本文提出DD-JSCC，一个动态调整编码器-解码器层结构的模型，以适应不同条件，提升图像重建性能并降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 传统的深度联合信源信道编码（Deep-JSCC）模型采用固定编码器-解码器结构，限制了其在不同设备能力、实时性能优化、功耗限制和信道条件下的适应性。

Method: 提出DD-JSCC（动态深度联合信源信道编码），一种新型的编码器-解码器架构。它通过分层激活机制和序列随机训练的隐式正则化，实现编码器和解码器层结构的实时动态调整，以适应发射器/接收器能力、功耗、压缩比和信道条件。

Result: DD-JSCC在图像重建性能上，PSNR比固定Deep-JSCC架构提升高达2 dB，并且训练成本降低超过40%。该框架消除了对多个专用模型的需求，显著减少了训练复杂度和部署开销。

Conclusion: DD-JSCC通过其动态可调的架构，显著提升了无线图像传输的语义通信性能、训练效率和部署灵活性，为适应多变通信环境提供了一个统一且高效的解决方案。

Abstract: Deep Joint Source-Channel Coding (Deep-JSCC) has emerged as a promising
semantic communication approach for wireless image transmission by jointly
optimizing source and channel coding using deep learning techniques. However,
traditional Deep-JSCC architectures employ fixed encoder-decoder structures,
limiting their adaptability to varying device capabilities, real-time
performance optimization, power constraints and channel conditions. To address
these limitations, we propose DD-JSCC: Dynamic Deep Joint Source-Channel Coding
for Semantic Communications, a novel encoder-decoder architecture designed for
semantic communication systems. Unlike traditional Deep-JSCC models, DD-JSCC is
flexible for dynamically adjusting its layer structures in real-time based on
transmitter and receiver capabilities, power constraints, compression ratios,
and current channel conditions. This adaptability is achieved through a
hierarchical layer activation mechanism combined with implicit regularization
via sequential randomized training, effectively reducing combinatorial
complexity, preventing overfitting, and ensuring consistent feature
representations across varying configurations. Simulation results demonstrate
that DD-JSCC enhances the performance of image reconstruction in semantic
communications, achieving up to 2 dB improvement in Peak Signal-to-Noise Ratio
(PSNR) over fixed Deep-JSCC architectures, while reducing training costs by
over 40%. The proposed unified framework eliminates the need for multiple
specialized models, significantly reducing training complexity and deployment
overhead.

</details>


### [82] [A Lyapunov-Guided Diffusion-Based Reinforcement Learning Approach for UAV-Assisted Vehicular Networks with Delayed CSI Feedback](https://arxiv.org/abs/2507.20524)
*Zhang Liu,Lianfen Huang,Zhibin Gao,Xianbin Wang,Dusit Niyato,Xuemin,Shen*

Main category: cs.NI

TL;DR: 本文提出一种基于深度强化学习（D3PG）的算法，以优化无人机辅助车联网中的信道分配、功率控制和飞行高度，旨在最大化车-无人机通信总速率并满足无人机长期能量约束，同时考虑信道状态信息（CSI）反馈延迟。


<details>
  <summary>Details</summary>
Motivation: 低空无人机有望促进空地一体化智能交通系统和低空经济发展，但面临网络资源和轨迹动态优化、无人机续航有限及信道状态信息不完善等挑战。因此，需要探索符合无人机能效的智能无人机辅助车联网通信策略。

Method: 研究将问题建模为联合信道分配、功率控制和飞行高度调整的优化问题，目标是在考虑CSI反馈延迟的情况下，最大化车-无人机通信总速率并满足无人机长期能量约束。首先，利用Lyapunov优化将长期问题分解为一系列逐时隙的确定性子问题；然后，提出一种创新的基于扩散模型（diffusion model）的深度确定性策略梯度（D3PG）算法来确定最优决策。

Result: 通过使用真实车辆移动轨迹进行大量仿真，结果表明所提出的D3PG算法在性能上优于现有基准解决方案。

Conclusion: 所提出的D3PG算法能有效解决无人机辅助车联网中的资源优化问题，实现了通信速率最大化和能量约束的平衡，为低空经济网络提供了高效的解决方案。

Abstract: Low altitude uncrewed aerial vehicles (UAVs) are expected to facilitate the
development of aerial-ground integrated intelligent transportation systems and
unlocking the potential of the emerging low-altitude economy. However, several
critical challenges persist, including the dynamic optimization of network
resources and UAV trajectories, limited UAV endurance, and imperfect channel
state information (CSI). In this paper, we offer new insights into low-altitude
economy networking by exploring intelligent UAV-assisted vehicle-to-everything
communication strategies aligned with UAV energy efficiency. Particularly, we
formulate an optimization problem of joint channel allocation, power control,
and flight altitude adjustment in UAV-assisted vehicular networks. Taking CSI
feedback delay into account, our objective is to maximize the vehicle-to-UAV
communication sum rate while satisfying the UAV's long-term energy constraint.
To this end, we first leverage Lyapunov optimization to decompose the original
long-term problem into a series of per-slot deterministic subproblems. We then
propose a diffusion-based deep deterministic policy gradient (D3PG) algorithm,
which innovatively integrates diffusion models to determine optimal channel
allocation, power control, and flight altitude adjustment decisions. Through
extensive simulations using real-world vehicle mobility traces, we demonstrate
the superior performance of the proposed D3PG algorithm compared to existing
benchmark solutions.

</details>


### [83] [Collusion Resistant DNS With Private Information Retrieval](https://arxiv.org/abs/2507.20806)
*Yunming Xiao,Peizhi Liu,Ruijie Yu,Chenkai Weng,Matteo Varvello,Aleksandar Kuzmanovic*

Main category: cs.NI

TL;DR: 现有DNS隐私方案依赖信任假设。本文提出PDNS，利用单服务器PIR实现无信任假设的DNS隐私保护，并在性能和隐私间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 互联网用户隐私日益受关注，但作为关键组件的DNS隐私性长期被忽视。现有DNS隐私方案（如DoH、ODoH）依赖于DNS解析器和代理之间不串通的假设，这在实践中难以保证，存在隐私泄露风险。因此，需要一种不依赖信任假设的DNS隐私保护方案。

Method: 提出并设计了PDNS，一个利用单服务器私有信息检索（PIR）的DNS扩展方案，旨在实现加密查询处理，从而避免信任假设。该方法在处理DNS的层级结构挑战时，在性能和隐私之间进行权衡。研究人员实现了PDNS原型，并通过基于流量跟踪的实验，将其性能与现有最先进方案进行了对比。

Result: PDNS在提供强大隐私保证的同时，实现了可接受的性能，比提供相似隐私保证的DoH over Tor快2倍。目前主要的代价是其可伸缩性，但该问题有望在未来通过专用PIR硬件解决。

Conclusion: PDNS通过整合单服务器PIR技术，提供了一种不依赖信任假设的强大DNS隐私保护方案。其在性能和隐私保障方面的表现证明了其可行性，并为未来DNS隐私领域的发展指明了方向，尽管当前面临可伸缩性挑战，但硬件发展有望进一步优化。

Abstract: There has been a growing interest in Internet user privacy, demonstrated by
the popularity of privacy-preserving products such as Telegram and Brave, and
the widespread adoption of HTTPS. The Domain Name System (DNS) is a key
component of Internet-based communication and its privacy has been neglected
for years. Recently, DNS over HTTPS (DoH) has improved the situation by fixing
the issue of in-path middleboxes. Further progress has been made with
proxy-based solutions such as Oblivious DoH (ODoH), which separate a user's
identity from their DNS queries. However, these solutions rely on non-collusion
assumptions between DNS resolvers and proxies -- an assumption difficult to
guarantee in practice. To address this, we explore integrating single-server
Private Information Retrieval (PIR) into DNS to enable encrypted query
processing without relying on trust assumptions. However, applying PIR to DNS
is challenging due to its hierarchical nature -- particularly, interactions
with recursive resolvers can still leak information. Navigating performance and
privacy trade-offs, we propose PDNS, a DNS extension leveraging single-server
PIR to strengthen privacy guarantees. We have implemented a prototype of PDNS
and compared its performance against state-of-the-art solutions via
trace-driven experiments. The results show that PDNS achieves acceptable
performance (2x faster than DoH over Tor with similar privacy guarantees) and
strong privacy guarantees today, mainly at the cost of its scalability, which
specialized hardware for PIR can address in the near future.

</details>


### [84] [\textit{FedABC}: Attention-Based Client Selection for Federated Learning with Long-Term View](https://arxiv.org/abs/2507.20871)
*Wenxuan Ye,Xueli An,Junfan Wang,Xueqiang Yan,Georg Carle*

Main category: cs.NI

TL;DR: 本文提出FedABC，一种基于注意力机制和优化问题的客户端选择算法，旨在解决联邦学习中数据异质性和客户端参与负担，显著提升模型准确性和客户端效率，以支持6G网络中的本地AI。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在6G网络中支持本地AI面临挑战：客户端数据异质性导致收敛慢、模型准确性降低；频繁的客户端参与造成通信和计算负担。

Method: 提出FedABC客户端选择算法。该算法受注意力机制启发，通过评估模型相似性和其对全局模型的独特贡献来选择有信息量的客户端。同时，通过构建优化问题指导训练，并遵循“后期更好”原则自适应调整选择阈值，鼓励后期更多客户端参与。

Result: 在CIFAR-10上的仿真显示，FedABC在模型准确性和客户端参与效率上显著优于现有方法。与经典FedAvg相比，客户端数量减少32%时性能相当；与现有最先进方法相比，客户端减少2%的同时准确率提高3.5%。

Conclusion: 本工作是推动联邦学习在异构、资源受限环境中部署的重要一步，从而有效支持6G网络中的本地AI能力。

Abstract: Native AI support is a key objective in the evolution of 6G networks, with
Federated Learning (FL) emerging as a promising paradigm. FL allows
decentralized clients to collaboratively train an AI model without directly
sharing their data, preserving privacy. Clients train local models on private
data and share model updates, which a central server aggregates to refine the
global model and redistribute it for the next iteration. However, client data
heterogeneity slows convergence and reduces model accuracy, and frequent client
participation imposes communication and computational burdens. To address these
challenges, we propose \textit{FedABC}, an innovative client selection
algorithm designed to take a long-term view in managing data heterogeneity and
optimizing client participation. Inspired by attention mechanisms,
\textit{FedABC} prioritizes informative clients by evaluating both model
similarity and each model's unique contributions to the global model. Moreover,
considering the evolving demands of the global model, we formulate an
optimization problem to guide \textit{FedABC} throughout the training process.
Following the ``later-is-better" principle, \textit{FedABC} adaptively adjusts
the client selection threshold, encouraging greater participation in later
training stages. Extensive simulations on CIFAR-10 demonstrate that
\textit{FedABC} significantly outperforms existing approaches in model accuracy
and client participation efficiency, achieving comparable performance with 32\%
fewer clients than the classical FL algorithm \textit{FedAvg}, and 3.5\% higher
accuracy with 2\% fewer clients than the state-of-the-art. This work marks a
step toward deploying FL in heterogeneous, resource-constrained environments,
thereby supporting native AI capabilities in 6G networks.

</details>


### [85] [Towards a Robust Transport Network With Self-adaptive Network Digital Twin](https://arxiv.org/abs/2507.20971)
*Cláudio Modesto,João Borges,Cleverson Nahum,Lucas Matni,Cristiano Bonato Both,Kleber Cardoso,Glauco Gonçalves,Ilan Correa,Silvia Lins,Andrey Silva,Aldebaro Klautau*

Main category: cs.NI

TL;DR: 本文提出一种自适应网络数字孪生（NDT）架构，通过概念漂移检测和再训练，在流量波动下仍能提供准确的时延预测，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能充分解决网络数字孪生（NDT）在流量多变环境下保持与其物理对应物（PTwin）同步的挑战，特别是如何应对意外流量变化，以维持虚拟模型（VTwin）的最佳性能。提高数据驱动型NDT平台抵御流量波动和改善VTwin与PTwin之间同步的韧性是一个亟待解决的关键问题。

Method: 本文提出一种自适应网络数字孪生架构。该架构利用遥测模块监控入站流量，并采用概念漂移检测技术来指导在必要时更新和重新部署VTwin的再训练决策。通过网络管理用例，在各种仿真网络拓扑和不同流量模式下验证了其有效性。

Result: 在所有测试拓扑中，以归一化均方误差作为评估指标，本文提出的架构在流量概念漂移后，预测性能比没有NDT同步的配置至少提高了56.7%。

Conclusion: 所提出的自适应网络数字孪生架构能够有效地保持可接受的性能并在意外流量变化下预测每流时延，显著提升了数据驱动型NDT平台在动态流量环境下的预测精度和韧性。

Abstract: The ability of the network digital twin (NDT) to remain aware of changes in
its physical counterpart, known as the physical twin (PTwin), is a fundamental
condition to enable timely synchronization, also referred to as twinning. In
this way, considering a transport network, a key requirement is to handle
unexpected traffic variability and dynamically adapt to maintain optimal
performance in the associated virtual model, known as the virtual twin (VTwin).
In this context, we propose a self-adaptive implementation of a novel NDT
architecture designed to provide accurate delay predictions, even under
fluctuating traffic conditions. This architecture addresses an essential
challenge, underexplored in the literature: improving the resilience of
data-driven NDT platforms against traffic variability and improving
synchronization between the VTwin and its physical counterpart. Therefore, the
contributions of this article rely on NDT lifecycle by focusing on the
operational phase, where telemetry modules are used to monitor incoming
traffic, and concept drift detection techniques guide retraining decisions
aimed at updating and redeploying the VTwin when necessary. We validate our
architecture with a network management use case, across various emulated
network topologies, and diverse traffic patterns to demonstrate its
effectiveness in preserving acceptable performance and predicting per-flow
delay under unexpected traffic variation. The results in all tested topologies,
using the normalized mean square error as the evaluation metric, demonstrate
that our proposed architecture, after a traffic concept drift, achieves a
performance improvement in prediction of at least 56.7% compared to a
configuration without NDT synchronization.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [86] [Towards Multi-Agent Economies: Enhancing the A2A Protocol with Ledger-Anchored Identities and x402 Micropayments for AI Agents](https://arxiv.org/abs/2507.19550)
*Awid Vaziry,Sandro Rodriguez Garzon,Axel Küpper*

Main category: cs.MA

TL;DR: 该研究提出一种新颖架构，通过整合DLT和x402标准，解决Agent2Agent (A2A) 协议中代理发现和微支付的限制，赋能安全、可信的多代理经济互动。


<details>
  <summary>Details</summary>
Motivation: 解决新兴Agent2Agent (A2A) 通信协议面临的两个关键限制：去中心化代理的可发现性（decentralized agent discoverability）和代理间的微支付（agent-to-agent micropayments）问题，以赋能多代理经济体。

Method: 1. 整合分布式账本技术 (DLT)，将AgentCards作为智能合约发布到链上，实现防篡改、可验证的代理身份。2. 扩展A2A协议，引入x402开放标准，通过HTTP 402状态码实现基于HTTP的、与区块链无关的微支付功能。

Result: 所提出的架构使自主代理能够跨组织边界无缝地发现、认证并相互支付。通过全面的技术实现和评估，该研究证明了基于DLT的代理发现和微支付的可行性。

Conclusion: 该方法为构建安全、可扩展、经济可行的多代理生态系统奠定了基础，推动了代理AI领域向可信赖、自主的经济互动发展。

Abstract: This research article presents a novel architecture to empower multi-agent
economies by addressing two critical limitations of the emerging Agent2Agent
(A2A) communication protocol: decentralized agent discoverability and
agent-to-agent micropayments. By integrating distributed ledger technology
(DLT), this architecture enables tamper-proof, on-chain publishing of
AgentCards as smart contracts, providing secure and verifiable agent
identities. The architecture further extends A2A with the x402 open standard,
facilitating blockchain-agnostic, HTTP-based micropayments via the HTTP 402
status code. This enables autonomous agents to seamlessly discover,
authenticate, and compensate each other across organizational boundaries. This
work further presents a comprehensive technical implementation and evaluation,
demonstrating the feasibility of DLT-based agent discovery and micropayments.
The proposed approach lays the groundwork for secure, scalable, and
economically viable multi-agent ecosystems, advancing the field of agentic AI
toward trusted, autonomous economic interactions.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [87] [Quantifying the Performance Gap for Simple Versus Optimal Dynamic Server Allocation Policies](https://arxiv.org/abs/2507.19667)
*Niklas Carlsson,Derek Eager*

Main category: cs.DC

TL;DR: 本文分析了云计算中动态服务器分配策略，通过分析模型和半马尔可夫决策模型量化了简单策略与最优策略的性能差距，并研究了多站点系统中状态依赖路由的潜在收益。


<details>
  <summary>Details</summary>
Motivation: 云计算允许动态分配服务器资源，因此需要策略来根据当前负载动态分配（和释放）服务器，以帮助服务提供商平衡云服务成本和延迟。

Method: 描述了几种简单的动态服务器分配策略并开发了分析模型进行分析；设计了半马尔可夫决策模型来确定最优策略的性能，以量化简单策略与最优策略之间的性能差距；将模型应用于研究多站点系统中状态依赖路由在动态服务器分配下的潜在性能优势。

Result: 量化了简单易实现的策略与最优策略之间的性能差距；研究了多站点系统中采用状态依赖路由时的潜在性能优势；研究结果为服务提供商平衡云服务成本和延迟提供了有价值的见解。

Conclusion: 研究结果对希望平衡云服务成本和延迟的服务提供商具有重要价值，尤其是在评估简单策略与最优策略的性能差异以及状态依赖路由的益处方面。

Abstract: Cloud computing enables the dynamic provisioning of server resources. To
exploit this opportunity, a policy is needed for dynamically allocating (and
deallocating) servers in response to the current load conditions. In this paper
we describe several simple policies for dynamic server allocation and develop
analytic models for their analysis. We also design semi-Markov decision models
that enable determination of the performance achieved with optimal policies,
allowing us to quantify the performance gap between simple, easily implemented
policies, and optimal policies. Finally, we apply our models to study the
potential performance benefits of state-dependent routing in multi-site systems
when using dynamic server allocation at each site. Insights from our results
are valuable to service providers wanting to balance cloud service costs and
delays.

</details>


### [88] [Oranits: Mission Assignment and Task Offloading in Open RAN-based ITS using Metaheuristic and Deep Reinforcement Learning](https://arxiv.org/abs/2507.19712)
*Ngoc Hung Nguyen,Nguyen Van Thieu,Quang-Trung Luu,Anh Tuan Nguyen,Senura Wanasekara,Nguyen Cong Luong,Fatemeh Kavehmadavani,Van-Dinh Nguyen*

Main category: cs.DC

TL;DR: 本文提出Oranits系统，旨在Open RAN智能交通系统中解决任务分配和卸载问题，通过考虑任务依赖和卸载成本，并采用CGG-ARO和MA-DDQN两种优化算法，显著提升任务完成数和系统收益。


<details>
  <summary>Details</summary>
Motivation: 现有关于Open RAN智能交通系统中任务分配和卸载的研究，常忽略任务间复杂依赖性及将任务卸载到边缘服务器的成本，导致决策次优。

Method: 提出Oranits系统模型，明确考虑任务依赖和卸载成本，并通过车辆协作优化性能。采用双重优化方法：1. CGG-ARO：一种基于元启发式进化计算算法，作为单时隙优化的基线。2. MA-DDQN：一种增强型基于奖励的深度强化学习框架，融合多智能体协作和多动作选择机制，旨在减少任务分配时间并提高适应性。

Result: 模拟结果显示，CGG-ARO使完成任务数和总体收益分别提高约7.1%和7.7%。MA-DDQN则取得更大提升，完成任务数提高11.0%，总体收益提高12.5%。

Conclusion: 结果表明Oranits系统在动态ITS环境中，能实现更快、更具适应性、更高效的任务处理，有效解决了现有研究的不足。

Abstract: In this paper, we explore mission assignment and task offloading in an Open
Radio Access Network (Open RAN)-based intelligent transportation system (ITS),
where autonomous vehicles leverage mobile edge computing for efficient
processing. Existing studies often overlook the intricate interdependencies
between missions and the costs associated with offloading tasks to edge
servers, leading to suboptimal decision-making. To bridge this gap, we
introduce Oranits, a novel system model that explicitly accounts for mission
dependencies and offloading costs while optimizing performance through vehicle
cooperation. To achieve this, we propose a twofold optimization approach.
First, we develop a metaheuristic-based evolutionary computing algorithm,
namely the Chaotic Gaussian-based Global ARO (CGG-ARO), serving as a baseline
for one-slot optimization. Second, we design an enhanced reward-based deep
reinforcement learning (DRL) framework, referred to as the Multi-agent Double
Deep Q-Network (MA-DDQN), that integrates both multi-agent coordination and
multi-action selection mechanisms, significantly reducing mission assignment
time and improving adaptability over baseline methods. Extensive simulations
reveal that CGG-ARO improves the number of completed missions and overall
benefit by approximately 7.1% and 7.7%, respectively. Meanwhile, MA-DDQN
achieves even greater improvements of 11.0% in terms of mission completions and
12.5% in terms of the overall benefit. These results highlight the
effectiveness of Oranits in enabling faster, more adaptive, and more efficient
task processing in dynamic ITS environments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [89] [ACCESS-AV: Adaptive Communication-Computation Codesign for Sustainable Autonomous Vehicle Localization in Smart Factories](https://arxiv.org/abs/2507.20399)
*Rajat Bhattacharjya,Arnab Sarkar,Ish Kool,Sabur Baidya,Nikil Dutt*

Main category: eess.SY

TL;DR: 该论文提出ACCESS-AV，一个利用现有5G基础设施的V2I定位框架，通过机会性地使用5G同步信号块，实现了自动驾驶车辆在智能工厂中的高能效、低成本定位，并保持亚30厘米的精度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（ADVs）在5G智能工厂中日益普及，其计算密集型定位模块存在巨大的优化空间。传统定位方案可能需要额外的专用路边单元或车载传感器，导致能耗和成本增加。

Method: ACCESS-AV框架利用智能工厂中现有的5G基础设施，机会性地访问周期性广播的5G同步信号块（SSBs）进行定位，从而无需专用路边单元或额外车载传感器。该框架采用基于到达角（AoA）的估计方法，具体使用多重信号分类（MUSIC）算法，并通过自适应通信-计算策略进行优化，根据信噪比（SNR）和车辆速度等环境条件动态平衡能耗与定位精度。

Result: 实验结果表明，与非自适应系统（如香草MUSIC、ESPRIT和Root-MUSIC）相比，ACCESS-AV平均能耗降低了43.09%。它保持了亚30厘米的定位精度，同时显著降低了基础设施和运营成本。

Conclusion: ACCESS-AV框架通过利用现有5G基础设施和自适应策略，为智能工厂环境中的ADVs提供了一种高效、经济且高精度的定位解决方案，验证了其在可持续智能工厂环境中的可行性。

Abstract: Autonomous Delivery Vehicles (ADVs) are increasingly used for transporting
goods in 5G network-enabled smart factories, with the compute-intensive
localization module presenting a significant opportunity for optimization. We
propose ACCESS-AV, an energy-efficient Vehicle-to-Infrastructure (V2I)
localization framework that leverages existing 5G infrastructure in smart
factory environments. By opportunistically accessing the periodically broadcast
5G Synchronization Signal Blocks (SSBs) for localization, ACCESS-AV obviates
the need for dedicated Roadside Units (RSUs) or additional onboard sensors to
achieve energy efficiency as well as cost reduction. We implement an
Angle-of-Arrival (AoA)-based estimation method using the Multiple Signal
Classification (MUSIC) algorithm, optimized for resource-constrained ADV
platforms through an adaptive communication-computation strategy that
dynamically balances energy consumption with localization accuracy based on
environmental conditions such as Signal-to-Noise Ratio (SNR) and vehicle
velocity. Experimental results demonstrate that ACCESS-AV achieves an average
energy reduction of 43.09% compared to non-adaptive systems employing AoA
algorithms such as vanilla MUSIC, ESPRIT, and Root-MUSIC. It maintains sub-30
cm localization accuracy while also delivering substantial reductions in
infrastructure and operational costs, establishing its viability for
sustainable smart factory environments.

</details>
