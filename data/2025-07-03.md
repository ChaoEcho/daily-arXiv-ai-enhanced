<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.CV](#cs.CV) [Total: 78]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.LG](#cs.LG) [Total: 79]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.IR](#cs.IR) [Total: 5]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.RO](#cs.RO) [Total: 5]
- [econ.GN](#econ.GN) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered](https://arxiv.org/abs/2507.01019)
*Imran Mirza,Cole Huang,Ishwara Vasista,Rohan Patil,Asli Akalin,Sean O'Brien,Kevin Zhu*

Main category: cs.CL

TL;DR: 多智能体系统可能强化LLM偏见。MALIBU是一个新基准，用于评估并发现当前偏见缓解可能过度偏向边缘化群体，而非真正中立。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在基于角色的交互中可能强化大型语言模型（LLM）的隐含偏见，引发对公平性和代表性的担忧。因此，需要评估这些系统强化社会偏见和刻板印象的程度。

Method: 提出MALIBU基准。通过情景化评估来衡量偏见：AI模型在预定义情境中完成任务，由基于LLM的多智能体评审系统分两阶段评估其响应。第一阶段，评审员根据特定人口统计学角色（如性别、种族、宗教）在四个指标上评分；第二阶段，评审员比较不同角色的配对响应，进行评分并选择更优的响应。

Result: 量化了LLM生成输出中的偏见。研究发现，偏见缓解措施可能过度偏向边缘化角色，而非达到真正的中立性。

Conclusion: 强调多智能体系统中需要细致的偏见检测、平衡的公平策略以及透明的评估基准。

Abstract: Multi-agent systems, which consist of multiple AI models interacting within a
shared environment, are increasingly used for persona-based interactions.
However, if not carefully designed, these systems can reinforce implicit biases
in large language models (LLMs), raising concerns about fairness and equitable
representation. We present MALIBU, a novel benchmark developed to assess the
degree to which LLM-based multi-agent systems implicitly reinforce social
biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems
through scenario-based assessments. AI models complete tasks within predefined
contexts, and their responses undergo evaluation by an LLM-based multi-agent
judging system in two phases. In the first phase, judges score responses
labeled with specific demographic personas (e.g., gender, race, religion)
across four metrics. In the second phase, judges compare paired responses
assigned to different personas, scoring them and selecting the superior
response. Our study quantifies biases in LLM-generated outputs, revealing that
bias mitigation may favor marginalized personas over true neutrality,
emphasizing the need for nuanced detection, balanced fairness strategies, and
transparent evaluation benchmarks in multi-agent systems.

</details>


### [2] [Event-based evaluation of abstractive news summarization](https://arxiv.org/abs/2507.01160)
*Huiling You,Samia Touileb,Erik Velldal,Lilja Øvrelid*

Main category: cs.CL

TL;DR: 本文提出一种基于事件重叠的抽象新闻摘要评估方法，旨在更深入地分析摘要中包含的事件信息。


<details>
  <summary>Details</summary>
Motivation: 当前自动生成摘要的评估方法主要依赖于与人工摘要的文本重叠或相似度，但新闻文章及其摘要应理想地报告事件。现有方法可能无法充分捕捉摘要中的事件信息。

Method: 通过计算生成摘要、参考摘要和原始新闻文章之间的事件重叠来评估抽象摘要的质量。该方法在一个包含事件标注和专家人工摘要的挪威数据集上进行实验。

Result: 所提出的评估方法能够提供对摘要中事件信息的更深入洞察。

Conclusion: 该研究为抽象摘要的质量评估提供了一个新的视角，尤其关注摘要内容的事件准确性和完整性。

Abstract: An abstractive summary of a news article contains its most important
information in a condensed version. The evaluation of automatically generated
summaries by generative language models relies heavily on human-authored
summaries as gold references, by calculating overlapping units or similarity
scores. News articles report events, and ideally so should the summaries. In
this work, we propose to evaluate the quality of abstractive summaries by
calculating overlapping events between generated summaries, reference
summaries, and the original news articles. We experiment on a richly annotated
Norwegian dataset comprising both events annotations and summaries authored by
expert human annotators. Our approach provides more insight into the event
information contained in the summaries.

</details>


### [3] [Matching and Linking Entries in Historical Swedish Encyclopedias](https://arxiv.org/abs/2507.01170)
*Simon Börjesson,Erik Ersmark,Pierre Nugues*

Main category: cs.CL

TL;DR: 本研究分析了瑞典百科全书《Nordisk familjebok》前两版地理条目的演变，发现地理焦点从欧洲转向北美、非洲、亚洲、澳大利亚和斯堪的纳维亚北部，反映了第一次世界大战和新兴大国的影响。


<details>
  <summary>Details</summary>
Motivation: 《Nordisk familjebok》是一部极具影响力的瑞典百科全书，其不同版本的条目选择和内容随时间演变，反映了瑞典的智识变革。本研究旨在探究这种变化，特别是地理条目中的趋势和转变。

Method: 研究使用了《Project Runeberg》的数字化版本。首先，将原始文本重新分段为条目，并使用语义句子嵌入技术匹配第一版和第二版之间的条目对。然后，利用基于Transformer的分类器从两个版本中提取地理条目，并将其链接到维基数据，从而识别地理趋势和可能的转变。

Result: 研究结果显示，从第一版到第二版，地理焦点发生了一个虽小但显著的转变，即从欧洲转向北美、非洲、亚洲、澳大利亚和斯堪的纳维亚北部。

Conclusion: 这一地理焦点的转变证实了第一次世界大战和新兴大国崛起的影响，表明百科全书的内容反映了当时的全球地缘政治变化。

Abstract: The \textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and
20th centuries. It was written by a team of experts and aimed to be an
intellectual reference, stressing precision and accuracy. This encyclopedia had
four main editions remarkable by their size, ranging from 20 to 38 volumes. As
a consequence, the \textit{Nordisk familjebok} had a considerable influence in
universities, schools, the media, and society overall. As new editions were
released, the selection of entries and their content evolved, reflecting
intellectual changes in Sweden.
  In this paper, we used digitized versions from \textit{Project Runeberg}. We
first resegmented the raw text into entries and matched pairs of entries
between the first and second editions using semantic sentence embeddings. We
then extracted the geographical entries from both editions using a
transformer-based classifier and linked them to Wikidata. This enabled us to
identify geographic trends and possible shifts between the first and second
editions, written between 1876-1899 and 1904-1926, respectively.
  Interpreting the results, we observe a small but significant shift in
geographic focus away from Europe and towards North America, Africa, Asia,
Australia, and northern Scandinavia from the first to the second edition,
confirming the influence of the First World War and the rise of new powers. The
code and data are available on GitHub at
https://github.com/sibbo/nordisk-familjebok.

</details>


### [4] [MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis](https://arxiv.org/abs/2507.01213)
*Adamu Lawan,Juhua Pu,Haruna Yunusa,Jawad Muhammad,Muhammad Lawan*

Main category: cs.CL

TL;DR: 本文提出MEGA框架，结合xLSTM与多头指数门控融合机制，用于细粒度情感分析（ABSA）。该框架通过独特的双向mLSTM架构和创新的MECGAF融合机制，解决了现有ABSA模型在计算效率与性能之间难以平衡的问题，并在基准数据集上展现出卓越的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有ABSA方法（深度学习模型、Transformer、Mamba）在平衡计算效率与高性能方面存在不足：深度学习模型缺乏全局上下文，Transformer计算资源消耗大，Mamba存在CUDA依赖和局部关联减弱。尽管xLSTM在长程依赖建模方面表现出色，但其在ABSA领域的潜力尚未被发掘。

Method: 提出xLSTM与多头指数门控融合（MEGA）框架。该框架整合了一个双向mLSTM架构，包含前向流和部分翻转反向流（PF-mLSTM）。PF-mLSTM通过反向处理初始序列段并使用专用参数，增强了局部上下文建模。此外，引入基于mLSTM的多头交叉指数门控融合机制（MECGAF），动态结合前向mLSTM输出（查询和键）与PF-mLSTM输出（值），以优化短程依赖捕获，同时保持全局上下文和效率。

Result: 在三个基准数据集上的实验结果表明，MEGA框架在ABSA任务中性能优于现有最先进的基线模型，实现了更高的准确性和效率。

Conclusion: MEGA框架通过结合先进的xLSTM架构和创新的融合机制，成功克服了当前ABSA方法在效率与性能之间的权衡问题，为细粒度情感分析提供了一种高效且高性能的解决方案。

Abstract: Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language
Processing (NLP) task that extracts aspects from text and determines their
associated sentiments, enabling fine-grained analysis of user opinions.
Existing ABSA methods struggle to balance computational efficiency with high
performance: deep learning models often lack global context, transformers
demand significant computational resources, and Mamba-based approaches face
CUDA dependency and diminished local correlations. Recent advancements in
Extended Long Short-Term Memory (xLSTM) models, particularly their efficient
modeling of long-range dependencies, have significantly advanced the NLP
community. However, their potential in ABSA remains untapped. To this end, we
propose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework
integrating a bi-directional mLSTM architecture with forward and partially
flipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context
modeling by processing the initial sequence segment in reverse with dedicated
parameters, preserving critical short-range patterns. We further introduce an
mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that
dynamically combines forward mLSTM outputs as query and key with PF-mLSTM
outputs as value, optimizing short-range dependency capture while maintaining
global context and efficiency. Experimental results on three benchmark datasets
demonstrate that MEGA outperforms state-of-the-art baselines, achieving
superior accuracy and efficiency in ABSA tasks.

</details>


### [5] [The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure](https://arxiv.org/abs/2507.01234)
*Yu Fan,Yang Tian,Shauli Ravfogel,Mrinmaya Sachan,Elliott Ash,Alexander Hoyle*

Main category: cs.CL

TL;DR: 文本嵌入相似度易受来源、语言等无关属性偏置。本文提出一种去偏算法，可显著降低这些偏置，大幅提升文本相似度和聚类性能，且不损害泛化能力。


<details>
  <summary>Details</summary>
Motivation: 文本序列的嵌入相似度度量不仅受内容维度影响，还会被文本来源或语言等虚假属性（混杂因素）所偏置。这些混杂因素对许多应用构成问题，尤其是在需要整合不同语料库文本的场景中。

Method: 本研究提出并应用了一种去偏算法，该算法通过从编码器表示中移除已观察到的混杂因素信息。

Result: 该去偏算法以极低的计算成本显著降低了嵌入中的偏置。在所评估的每种嵌入变体和任务中，文档相似性和聚类指标都得到了显著提升。此外，对分布外（out-of-distribution）基准测试的性能没有负面影响，表明嵌入的整体质量并未下降。

Conclusion: 通过移除混杂因素信息，可以有效且高效地提高文本嵌入的质量和可靠性，使其更准确地反映内容相似度，从而在多源数据应用中表现更优，且不牺牲泛化能力。

Abstract: Embedding-based similarity metrics between text sequences can be influenced
not just by the content dimensions we most care about, but can also be biased
by spurious attributes like the text's source or language. These document
confounders cause problems for many applications, but especially those that
need to pool texts from different corpora. This paper shows that a debiasing
algorithm that removes information about observed confounders from the encoder
representations substantially reduces these biases at a minimal computational
cost. Document similarity and clustering metrics improve across every embedding
variant and task we evaluate -- often dramatically. Interestingly, performance
on out-of-distribution benchmarks is not impacted, indicating that the
embeddings are not otherwise degraded.

</details>


### [6] [GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant](https://arxiv.org/abs/2507.01259)
*Michał Matak,Jarosław A. Chudziak*

Main category: cs.CL

TL;DR: 本文提出了一种名为gAIus的认知型LLM代理，专注于处理非英中语言（特别是波兰）的法律事务。通过创新的可解释检索机制，gAIus在波兰法律考试中表现卓越，显著超越现有LLM模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在非英语和非中文法律语境中提供基于事实且可引用的法律答案的能力不足，且传统法律信息检索方法存在挑战。研究旨在解决这些问题，提高LLM在特定法律领域的应用能力。

Method: 引入了基于波兰民法典的认知型LLM代理gAIus架构，并提出了一种比传统嵌入式方法更具解释性、更人性化的检索机制。通过波兰法律学徒入学考试的单选题创建专用数据集来评估该方法。

Result: 该架构显著提升了所用大型语言模型的性能：gpt-3.5-turbo-0125的性能提升419%，超越了gpt-4o；gpt-4o-mini的得分从31%提高到86%。

Conclusion: 提出的gAIus架构成功利用并显著增强了大型语言模型处理非英语/中文法律问题的能力，特别是在波兰法律领域取得了突破性进展，展现了广阔的研究前景和应用潜力。

Abstract: In this paper we discuss the capability of large language models to base
their answer and provide proper references when dealing with legal matters of
non-english and non-chinese speaking country. We discuss the history of legal
information retrieval, the difference between case law and statute law, its
impact on the legal tasks and analyze the latest research in this field. Basing
on that background we introduce gAIus, the architecture of the cognitive
LLM-based agent, whose responses are based on the knowledge retrieved from
certain legal act, which is Polish Civil Code. We propose a retrieval mechanism
which is more explainable, human-friendly and achieves better results than
embedding-based approaches. To evaluate our method we create special dataset
based on single-choice questions from entrance exams for law apprenticeships
conducted in Poland. The proposed architecture critically leveraged the
abilities of used large language models, improving the gpt-3.5-turbo-0125 by
419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.
At the end of our paper we show the possible future path of research and
potential applications of our findings.

</details>


### [7] [Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening](https://arxiv.org/abs/2507.01278)
*Cindy Lie Tabuse,David Restepo,Carolina Gracitelli,Fernando Korn Malerbi,Caio Regatieri,Luis Filipe Nakayama*

Main category: cs.CL

TL;DR: GPT-4能基于文本描述进行基本的眼科决策，如糖尿病视网膜病变筛查，但对复杂任务（如青光眼转诊）精度不足。添加元数据未能显著提高性能。目前不适用于临床，但有潜力用于教育或辅助工作。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）展现出模拟临床推理的潜力，但在眼科学领域的应用尚未被充分探索。本研究旨在评估GPT-4从结构化文本描述中解读视网膜眼底照片并模拟糖尿病视网膜病变（DR）和青光眼筛查临床决策的能力，并探究添加临床元数据的影响。

Method: 本研究采用回顾性诊断验证设计，使用300张带注释的眼底图像。GPT-4接收描述每张图像的结构化提示，部分提示包含或不包含患者元数据。模型需完成以下任务：分配ICDR严重程度评分、推荐DR转诊，并估计杯盘比以进行青光眼转诊。性能评估指标包括准确率、宏观和加权F1分数以及Cohen's Kappa。McNemar检验和变化率分析用于评估元数据的影响。

Result: GPT-4在ICDR分类任务中表现中等（准确率67.5%，宏观F1 0.33，加权F1 0.67，Kappa 0.25），主要由于能正确识别正常病例。在二元DR转诊任务中性能有所提升（准确率82.3%，F1 0.54，Kappa 0.44）。然而，对于青光眼转诊，在所有设置下表现均差（准确率约78%，F1 <0.04，Kappa <0.03）。元数据的纳入并未显著影响结果（McNemar p > 0.05），且预测在不同条件下保持一致。

Conclusion: GPT-4能够从结构化提示中模拟基本的眼科决策，但对于复杂任务缺乏精确性。尽管目前不适用于临床，但LLMs可能在眼科教育、文档记录或图像标注工作流程中发挥辅助作用。

Abstract: Large language models (LLMs) can simulate clinical reasoning based on natural
language prompts, but their utility in ophthalmology is largely unexplored.
This study evaluated GPT-4's ability to interpret structured textual
descriptions of retinal fundus photographs and simulate clinical decisions for
diabetic retinopathy (DR) and glaucoma screening, including the impact of
adding real or synthetic clinical metadata. We conducted a retrospective
diagnostic validation study using 300 annotated fundus images. GPT-4 received
structured prompts describing each image, with or without patient metadata. The
model was tasked with assigning an ICDR severity score, recommending DR
referral, and estimating the cup-to-disc ratio for glaucoma referral.
Performance was evaluated using accuracy, macro and weighted F1 scores, and
Cohen's kappa. McNemar's test and change rate analysis were used to assess the
influence of metadata. GPT-4 showed moderate performance for ICDR
classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),
driven mainly by correct identification of normal cases. Performance improved
in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For
glaucoma referral, performance was poor across all settings (accuracy ~78%, F1
<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes
(McNemar p > 0.05), and predictions remained consistent across conditions.
GPT-4 can simulate basic ophthalmic decision-making from structured prompts but
lacks precision for complex tasks. While not suitable for clinical use, LLMs
may assist in education, documentation, or image annotation workflows in
ophthalmology.

</details>


### [8] [Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization](https://arxiv.org/abs/2507.01281)
*Juan Chen,Baolong Bi,Wei Zhang,Jingyan Sui,Xiaofei Zhu,Yuanzhuo Wang,Lingrui Mei,Shenghua Liu*

Main category: cs.CL

TL;DR: 本文提出CARE-RAG框架，通过冲突驱动的证据总结，使大语言模型在生成响应前重新思考内部知识和检索内容，有效解决RAG系统中的知识冲突问题，提高生成可靠性。


<details>
  <summary>Details</summary>
Motivation: RAG系统因内部不一致或噪声检索内容导致的知识冲突，严重损害了生成可靠性。现有RAG系统未能充分处理这些冲突，LLM在生成前应重新评估所有证据。

Method: CARE-RAG框架通过“冲突驱动的证据总结”提升可信度。具体包括：1. 比较参数记录以识别内部观点，得到“参数感知证据”。2. 精炼检索到的内容，移除不相关或误导性信息，生成“上下文感知证据”。3. 蒸馏一个3B LLaMA3.2模型进行冲突驱动的总结。4. 引入“QA修复”步骤校正过时或模糊的基准答案，以确保评估完整性。

Result: 在修订后的QA数据集上进行实验，CARE-RAG持续优于强大的RAG基线，尤其在证据噪声或冲突较多的场景下表现更佳。

Conclusion: CARE-RAG通过在生成前让LLM重新思考并总结所有证据（包括内部知识和检索内容），有效解决了RAG系统中的知识冲突问题，显著提高了生成内容的可靠性和可信度，尤其在复杂证据环境下效果突出。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating their parametric knowledge with external retrieved content.
However, knowledge conflicts caused by internal inconsistencies or noisy
retrieved content can severely undermine the generation reliability of RAG
systems.In this work, we argue that LLMs should rethink all evidence, including
both retrieved content and internal knowledge, before generating responses.We
propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel
framework that improves trustworthiness through Conflict-Driven Summarization
of all available evidence.CARE-RAG first derives parameter-aware evidence by
comparing parameter records to identify diverse internal perspectives. It then
refines retrieved evidences to produce context-aware evidence, removing
irrelevant or misleading content. To detect and summarize conflicts, we distill
a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable
synthesis across multiple sources.To further ensure evaluation integrity, we
introduce a QA Repair step to correct outdated or ambiguous benchmark
answers.Experiments on revised QA datasets with retrieval data show that
CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios
with noisy or conflicting evidence.

</details>


### [9] [Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks](https://arxiv.org/abs/2507.01297)
*Xinxi Lyu,Michael Duan,Rulin Shao,Pang Wei Koh,Sewon Min*

Main category: cs.CL

TL;DR: 为解决RAG在推理密集型任务中效果有限的问题，研究引入了CompactDS，一个高质量、多样化的网络级数据存储。该存储显著提升了RAG在多项基准测试上的准确性，并超越了现有复杂系统，验证了RAG在这些场景中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成（RAG）主要在有限设置（如事实问答）中研究，但在更具挑战性、推理密集型基准测试中，最小RAG的成功有限。主要原因是缺少一个可用、与预训练数据广度对齐的网络级数据存储。

Method: 研究引入了CompactDS，一个多样化、高质量的网络级数据存储，实现高检索精度和亚秒级延迟。其关键在于：1) 大部分网络内容可在不牺牲覆盖率的情况下被过滤，紧凑、高质量的子集已足够；2) 结合内存内近似最近邻 (ANN) 检索和磁盘上精确搜索，以平衡速度和召回率。数据源包括网络爬取、精选数学资料、学术论文和教科书。

Result: 使用CompactDS，一个最小RAG管道在所有基准测试（MMLU, MMLU Pro, GPQA, MATH）和模型尺寸（8B-70B）上均实现了一致的准确性提升，相对增益分别为MMLU上10%，MMLU Pro上33%，GPQA上14%，MATH上19%。研究强调了数据源多样性的重要性。此外，CompactDS匹配或超越了Google Search等网络搜索引擎以及近期提出的复杂代理RAG系统，同时保持了简洁性、可复现性和自包含性。

Conclusion: 高质量、多样化的网络级数据存储（如CompactDS）是提升RAG在推理密集型任务中性能的关键缺失组件。本研究证明了通过精心设计的数据存储，RAG能在复杂任务中实现显著进步，并为未来检索增强AI系统的研究提供了基础和工具。

Abstract: Retrieval-augmented Generation (RAG) has primarily been studied in limited
settings, such as factoid question answering; more challenging,
reasoning-intensive benchmarks have seen limited success from minimal RAG. In
this work, we challenge this prevailing view on established,
reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We
identify a key missing component in prior work: a usable, web-scale datastore
aligned with the breadth of pretraining data. To this end, we introduce
CompactDS: a diverse, high-quality, web-scale datastore that achieves high
retrieval accuracy and subsecond latency on a single-node. The key insights are
(1) most web content can be filtered out without sacrificing coverage, and a
compact, high-quality subset is sufficient; and (2) combining in-memory
approximate nearest neighbor (ANN) retrieval and on-disk exact search balances
speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves
consistent accuracy improvements across all benchmarks and model sizes
(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,
and 19% on MATH. No single data source suffices alone, highlighting the
importance of diversity of sources (web crawls, curated math, academic papers,
textbooks). Finally, we show that our carefully designed in-house datastore
matches or outperforms web search engines such as Google Search, as well as
recently proposed, complex agent-based RAG systems--all while maintaining
simplicity, reproducibility, and self-containment. We release CompactDS and our
retrieval pipeline, supporting future research exploring retrieval-based AI
systems.

</details>


### [10] [La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation](https://arxiv.org/abs/2507.01299)
*Kai Liu,Bowen Xu,Shaoyu Wu,Xin Chen,Hao Zhou,Yongliang Tao,Lulu Hu*

Main category: cs.CL

TL;DR: LaRoSA是一种无需额外训练或基于幅度的剪枝，通过正交旋转和Top-K选择实现LLM激活稀疏化的新方法，可显著提高推理速度且性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）激活稀疏化方法存在局限性，例如需要耗时的恢复训练或依赖基于幅度的剪枝，导致稀疏度波动和推理速度提升不稳定，阻碍了实际应用。

Method: 本文提出LaRoSA（Layerwise Rotated Sparse Activation），一种新颖的激活稀疏化方法。它利用层级正交旋转将输入激活转换为更适合稀疏化的形式，然后通过Top-K选择策略实现模型级一致的稀疏度。该方法无需额外训练或基于幅度的剪枝。

Result: LaRoSA在不同大小和类型的LLM上均表现出有效性，实现了最小的性能下降和鲁棒的推理加速。具体而言，对于40%稀疏度的LLaMA2-7B，LaRoSA仅有0.17的困惑度差距，实现了1.30倍的稳定挂钟时间加速，并在零样本任务中将与密集模型的准确率差距缩小至0.54%，同时显著优于TEAL（1.77%）和CATS（17.14%）。

Conclusion: LaRoSA为提高LLM推理效率提供了一个有效且鲁棒的激活稀疏化解决方案，在无需额外训练或牺牲性能的前提下，实现了显著的推理加速并保持了优异的模型性能。

Abstract: Activation sparsity can reduce the computational overhead and memory
transfers during the forward pass of Large Language Model (LLM) inference.
Existing methods face limitations, either demanding time-consuming recovery
training that hinders real-world adoption, or relying on empirical
magnitude-based pruning, which causes fluctuating sparsity and unstable
inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse
Activation), a novel method for activation sparsification designed to improve
LLM efficiency without requiring additional training or magnitude-based
pruning. We leverage layerwise orthogonal rotations to transform input
activations into rotated forms that are more suitable for sparsification. By
employing a Top-K selection approach within the rotated activations, we achieve
consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA
is effective across various sizes and types of LLMs, demonstrating minimal
performance degradation and robust inference acceleration. Specifically, for
LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a
consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in
zero-shot tasks compared to the dense model to just 0.54%, while surpassing
TEAL by 1.77% and CATS by 17.14%.

</details>


### [11] [Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs](https://arxiv.org/abs/2507.01334)
*Nifu Dan,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: 研究探索Deepseek-R1等高级指令调优推理模型在解决复杂物理问题上的应用，发现其在SciBench基准测试上取得了最先进的准确率，展现出独特的符号推导推理模式，并证实少样本提示能进一步提升其性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在进行物理推理时面临长期挑战，这需要深厚的概念理解和高超的问题解决技巧。

Method: 研究应用了包括Deepseek-R1在内的先进指令调优推理模型，并使用来自具有挑战性的SciBench基准测试的各类物理问题进行评估。此外，还探讨了少样本提示（few-shot prompting）对模型性能的影响。

Result: 推理模型在回答复杂的物理问题上取得了最先进（state-of-the-art）的准确率；模型生成了强调符号推导的独特推理模式；即使是高度复杂的推理模型，策略性地整合少样本提示也能带来可衡量的整体准确性提升。

Conclusion: 推理模型在物理推理任务中展现出卓越的能力，并且少样本提示仍然是提升其性能的有效策略，这预示着这些模型在未来仍有持续性能提升的潜力。

Abstract: Navigating the complexities of physics reasoning has long been a difficult
task for Large Language Models (LLMs), requiring a synthesis of profound
conceptual understanding and adept problem-solving techniques. In this study,
we investigate the application of advanced instruction-tuned reasoning models,
such as Deepseek-R1, to address a diverse spectrum of physics problems curated
from the challenging SciBench benchmark. Our comprehensive experimental
evaluation reveals the remarkable capabilities of reasoning models. Not only do
they achieve state-of-the-art accuracy in answering intricate physics
questions, but they also generate distinctive reasoning patterns that emphasize
on symbolic derivation. Furthermore, our findings indicate that even for these
highly sophisticated reasoning models, the strategic incorporation of few-shot
prompting can still yield measurable improvements in overall accuracy,
highlighting the potential for continued performance gains.

</details>


### [12] [LEDOM: An Open and Fundamental Reverse Language Model](https://arxiv.org/abs/2507.01335)
*Xunjian Yin,Sitao Cheng,Yuxi Xie,Xinyu Hu,Li Lin,Xinyi Wang,Liangming Pan,William Yang Wang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 引入了LEDOM，首个纯逆向语言模型，并展示其作为基础模型的潜力；基于LEDOM提出了“逆向奖励”应用，显著提升了数学推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 首次探索逆向语言模型作为潜在的基础模型，并利用其独特的逆向推理能力，提升现有正向语言模型在特定任务（如数学推理）上的性能。

Method: 1. 开发并训练了LEDOM，一个纯粹的逆向语言模型，在435B tokens上进行自回归训练，提供2B和7B参数版本，通过预测前一个token来逆序处理序列。2. 基于LEDOM，提出了“逆向奖励”应用，通过LEDOM引导对正向语言模型的输出进行重排序（后验评估），以优化生成质量。

Result: 1. 首次将逆向语言模型提出作为通用任务的潜在基础模型，并展示了其独特特性。2. “逆向奖励”方法利用LEDOM的逆向推理能力，显著提升了正向语言模型在数学推理任务上的表现。

Conclusion: LEDOM作为首个逆向语言模型，具有独特的逆向推理能力和作为基础模型的潜力。其在“逆向奖励”应用中对数学推理任务的成功验证，预示着其广泛的应用前景和研究价值。

Abstract: We introduce LEDOM, the first purely reverse language model, trained
autoregressively on 435B tokens with 2B and 7B parameter variants, which
processes sequences in reverse temporal order through previous token
prediction. For the first time, we present the reverse language model as a
potential foundational model across general tasks, accompanied by a set of
intriguing examples and insights. Based on LEDOM, we further introduce a novel
application: Reverse Reward, where LEDOM-guided reranking of forward language
model outputs leads to substantial performance improvements on mathematical
reasoning tasks. This approach leverages LEDOM's unique backward reasoning
capability to refine generation quality through posterior evaluation. Our
findings suggest that LEDOM exhibits unique characteristics with broad
application potential. We will release all models, training code, and
pre-training data to facilitate future research.

</details>


### [13] [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/abs/2507.01352)
*Chris Yuhao Liu,Liang Zeng,Yuzhen Xiao,Jujie He,Jiacai Liu,Chaojie Wang,Rui Yan,Wei Shen,Fuxiang Zhang,Jiacheng Xu,Yang Liu,Yahui Zhou*

Main category: cs.CL

TL;DR: 现有奖励模型因数据集质量差而表现不佳。本文提出了SynPref-40M（4000万对）大规模偏好数据集，采用人机协同管道筛选，并在此基础上训练了Skywork-Reward-V2系列奖励模型（0.6B-8B）。该模型在七个主要基准测试中实现了最先进性能，突显了高质量大规模数据筛选的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管奖励模型(RMs)在RLHF中扮演关键角色，但当前最先进的开源RMs在现有评估基准上表现不佳，难以捕捉复杂的人类偏好。作者推测，这种不足主要源于偏好数据集的局限性，例如范围狭窄、合成标签或缺乏严格的质量控制。

Method: 为解决数据挑战，本文构建了一个名为SynPref-40M的大规模偏好数据集（包含4000万对偏好）。为此，设计了一个人机协同两阶段管道，结合人工验证的质量和AI的扩展性。基于SynPref-40M中精选的2600万对偏好数据，训练了Skywork-Reward-V2系列奖励模型，模型参数范围从0.6B到8B。

Result: Skywork-Reward-V2模型在广泛的能力上表现出多功能性，包括与人类偏好的对齐、客观正确性、安全性、对风格偏见的抵抗以及Best-of-N扩展，并在七个主要奖励模型基准测试中实现了最先进的性能。消融研究证实，其有效性不仅来源于数据规模，还来源于高质量的数据筛选。

Conclusion: Skywork-Reward-V2系列代表了开源奖励模型的重大进展，强调了现有偏好数据集的未开发潜力，并证明了人机协同筛选能够显著提升数据质量，带来更优秀的模型表现。

Abstract: Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.

</details>


### [14] [Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction](https://arxiv.org/abs/2507.01437)
*Ting Xu,Xiaoxiao Deng,Xiandong Meng,Haifeng Yang,Yan Wu*

Main category: cs.CL

TL;DR: 本文提出一种基于注意力机制的深度学习方法，统一建模电子健康记录文本的信息提取和多标签疾病预测，并在MIMIC-IV数据集上表现出优越的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决电子健康记录文本的非结构化和高维语义复杂性挑战，以实现信息提取和多标签疾病预测的统一建模。

Method: 提出了一种基于注意力机制的深度学习方法，采用Transformer架构进行临床文本表示学习。利用多层自注意力机制捕获关键医学实体及其上下文关系，并使用基于Sigmoid的多标签分类器进行疾病预测。模型融入上下文感知的语义对齐机制，并在MIMIC-IV数据集上通过多项实验（包括基线比较、参数敏感性、数据扰动和噪声注入测试）进行全面评估。

Result: 所提出的方法在多项性能指标上持续优于现有代表性方法，且在不同数据规模、干扰水平和模型深度配置下均保持强大的泛化能力。

Conclusion: 本研究开发的框架为处理真实世界临床文本提供了高效的算法基础，并对多标签医疗文本建模任务具有重要的实际意义。

Abstract: This paper addresses the challenges posed by the unstructured nature and
high-dimensional semantic complexity of electronic health record texts. A deep
learning method based on attention mechanisms is proposed to achieve unified
modeling for information extraction and multi-label disease prediction. The
study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is
used to perform representation learning over clinical text. Multi-layer
self-attention mechanisms are employed to capture key medical entities and
their contextual relationships. A Sigmoid-based multi-label classifier is then
applied to predict multiple disease labels. The model incorporates a
context-aware semantic alignment mechanism, enhancing its representational
capacity in typical medical scenarios such as label co-occurrence and sparse
information. To comprehensively evaluate model performance, a series of
experiments were conducted, including baseline comparisons, hyperparameter
sensitivity analysis, data perturbation studies, and noise injection tests.
Results demonstrate that the proposed method consistently outperforms
representative existing approaches across multiple performance metrics. The
model maintains strong generalization under varying data scales, interference
levels, and model depth configurations. The framework developed in this study
offers an efficient algorithmic foundation for processing real-world clinical
texts and presents practical significance for multi-label medical text modeling
tasks.

</details>


### [15] [LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation](https://arxiv.org/abs/2507.01449)
*Tianyu Liu,Qitan Lv,Hao Li,Xing Gao,Xiao Sun*

Main category: cs.CL

TL;DR: LogitSpec通过利用最后一个token的logit预测下下个token并扩展检索范围，解决了检索式推测解码中草稿token不准确的问题，显著提升了大型语言模型推理速度，且无需训练。


<details>
  <summary>Details</summary>
Motivation: 现有的检索式推测解码（SD）方法依赖匹配范式检索草稿token，但常无法找到匹配且准确的草稿token，导致性能受限。

Method: 提出LogitSpec，受观察启发：最后一个token的logit不仅能预测下一个token，还能推测下下个token。具体分两步：1) 利用最后一个logit推测下下个token；2) 同时为下一个和下下个token检索相关引用作为草稿。LogitSpec无需训练且即插即用。

Result: 在多种文本生成基准测试中，LogitSpec实现了高达2.61倍的加速和平均每解码步3.28个被接受的token。

Conclusion: LogitSpec通过有效扩展检索范围，解决了检索式推测解码的挑战，显著提高了大型语言模型的推理效率和性能。

Abstract: Speculative decoding (SD), where a small draft model is employed to propose
draft tokens in advance and then the target model validates them in parallel,
has emerged as a promising technique for LLM inference acceleration. Many
endeavors to improve SD are to eliminate the need for a draft model and
generate draft tokens in a retrieval-based manner in order to further alleviate
the drafting overhead and significantly reduce the difficulty in deployment and
applications. However, retrieval-based SD relies on a matching paradigm to
retrieval the most relevant reference as the draft tokens, where these methods
often fail to find matched and accurate draft tokens. To address this
challenge, we propose LogitSpec to effectively expand the retrieval range and
find the most relevant reference as drafts. Our LogitSpec is motivated by the
observation that the logit of the last token can not only predict the next
token, but also speculate the next next token. Specifically, LogitSpec
generates draft tokens in two steps: (1) utilizing the last logit to speculate
the next next token; (2) retrieving relevant reference for both the next token
and the next next token. LogitSpec is training-free and plug-and-play, which
can be easily integrated into existing LLM inference frameworks. Extensive
experiments on a wide range of text generation benchmarks demonstrate that
LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens
per decoding step. Our code is available at
https://github.com/smart-lty/LogitSpec.

</details>


### [16] [Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities](https://arxiv.org/abs/2507.01479)
*Yingqiang Gao,Kaede Johnson,David Froehlich,Luisa Carrer,Sarah Ebling*

Main category: cs.CL

TL;DR: 本文通过直接偏好优化（DPO）技术，利用智力障碍者的偏好反馈，对大语言模型驱动的文本简化系统进行个性化，并提出了一个开发流程，强调用户参与在设计无障碍AI方案中的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型文本简化（ATS）系统未能将目标群体（如智力障碍者）的偏好反馈纳入训练，导致缺乏个性化，无法满足其特定需求。

Method: 研究扩展了标准的监督微调（SFT）方法，利用计算高效的直接偏好优化（DPO）技术对大语言模型ATS模型进行后训练。具体做法是收集智力障碍者对主流LLM生成的配对文本简化的偏好反馈，并用此数据进行模型训练。此外，还提出了一个完整的个性化LLM-ATS系统开发流程，涵盖数据收集、模型选择、SFT、DPO后训练和评估。

Result: 研究结果强调了目标群体人员在设计符合人类期望的个性化AI无障碍解决方案时积极参与的必要性。

Conclusion: 这项工作代表着在目标群体层面实现包容性AI系统个性化的重要一步，它不仅采纳了文本简化专家的见解，也融入了目标群体人员自身的观点。

Abstract: Automatic text simplification (ATS) aims to enhance language accessibility
for various target groups, particularly persons with intellectual disabilities.
Recent advancements in generative AI, especially large language models (LLMs),
have substantially improved the quality of machine-generated text
simplifications, thereby mitigating information barriers for the target group.
However, existing LLM-based ATS systems do not incorporate preference feedback
on text simplifications during training, resulting in a lack of personalization
tailored to the specific needs of target group representatives.
  In this work, we extend the standard supervised fine-tuning (SFT) approach
for adapting LLM-based ATS models by leveraging a computationally efficient LLM
alignment technique -- direct preference optimization (DPO). Specifically, we
post-train LLM-based ATS models using human feedback collected from persons
with intellectual disabilities, reflecting their preferences on paired text
simplifications generated by mainstream LLMs. Furthermore, we propose a
pipeline for developing personalized LLM-based ATS systems, encompassing data
collection, model selection, SFT and DPO post-training, and evaluation. Our
findings underscore the necessity of active participation of target group
persons in designing personalized AI accessibility solutions aligned with human
expectations. This work represents a step towards personalizing inclusive AI
systems at the target-group level, incorporating insights not only from text
simplification experts but also from target group persons themselves.

</details>


### [17] [Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing](https://arxiv.org/abs/2507.01541)
*Álvaro Zaera,Diana Nicoleta Popa,Ivan Sekulic,Paolo Rosso*

Main category: cs.CL

TL;DR: 提出一个结合不确定性建模和微调大语言模型（LLMs）的模块化框架，用于高效准确的域外（OOS）意图检测，并在真实世界数据上取得了最先进（SOTA）的性能。


<details>
  <summary>Details</summary>
Motivation: 域外（OOS）意图检测是任务导向对话系统（TODS）中的一个关键挑战，因为它能确保系统对未知和模糊查询的鲁棒性。

Method: 该方法是一个模块化框架，分两步进行：首先，对现有域内意图检测分类器的输出进行不确定性估计；然后，对高不确定性的实例触发微调的大语言模型（LLM）进行最终决策。该方法结合了传统模型和LLMs。

Result: 该方法有效平衡了计算效率和性能，并在关键的OOS检测基准测试（包括来自已部署TODS的真实世界OOS数据）上取得了最先进（SOTA）的结果。

Conclusion: 该框架通过结合不确定性建模和LLMs，成功解决了OOS意图检测的效率与准确性平衡问题，并在实际应用中展现出卓越的性能和鲁棒性。

Abstract: Out-of-scope (OOS) intent detection is a critical challenge in task-oriented
dialogue systems (TODS), as it ensures robustness to unseen and ambiguous
queries. In this work, we propose a novel but simple modular framework that
combines uncertainty modeling with fine-tuned large language models (LLMs) for
efficient and accurate OOS detection. The first step applies uncertainty
estimation to the output of an in-scope intent detection classifier, which is
currently deployed in a real-world TODS handling tens of thousands of user
interactions daily. The second step then leverages an emerging LLM-based
approach, where a fine-tuned LLM is triggered to make a final decision on
instances with high uncertainty. Unlike prior approaches, our method
effectively balances computational efficiency and performance, combining
traditional approaches with LLMs and yielding state-of-the-art results on key
OOS detection benchmarks, including real-world OOS data acquired from a
deployed TODS.

</details>


### [18] [Is External Information Useful for Stance Detection with LLMs?](https://arxiv.org/abs/2507.01543)
*Quang Minh Nguyen,Taegyoon Kim*

Main category: cs.CL

TL;DR: 研究发现，在立场检测任务中，外部信息（如维基百科）对大型语言模型（LLMs）的性能有害无益，这与传统模型相反，主要原因是LLMs易受外部信息偏见影响。


<details>
  <summary>Details</summary>
Motivation: 虽然外部信息被证明能提升传统模型在立场检测任务中的表现，但对于广泛应用于推理任务的LLMs而言，外部信息是否仍能带来益处尚不明确。

Method: 本研究系统评估了维基百科和网页搜索等外部信息对八种LLM在三个数据集（包含十二个目标）上立场检测性能的影响。通过实验分析LLM的预测倾向，并测试了思维链提示和微调策略的效果。

Result: 外部信息在大多数情况下会显著降低LLMs的立场检测性能（宏观F1分数最高下降27.9%）。LLMs倾向于将预测与所提供信息的立场和情感对齐，而非文本的真实立场。思维链提示未能改善，微调虽有缓解但未能完全消除此问题。

Conclusion: 与此前BERT模型的研究结果相反，本研究揭示了LLM在立场分类任务中使用外部信息时存在信息偏见的风险。

Abstract: In the stance detection task, a text is classified as either favorable,
opposing, or neutral towards a target. Prior work suggests that the use of
external information, e.g., excerpts from Wikipedia, improves stance detection
performance. However, whether or not such information can benefit large
language models (LLMs) remains an unanswered question, despite their wide
adoption in many reasoning tasks. In this study, we conduct a systematic
evaluation on how Wikipedia and web search external information can affect
stance detection across eight LLMs and in three datasets with 12 targets.
Surprisingly, we find that such information degrades performance in most cases,
with macro F1 scores dropping by up to 27.9\%. We explain this through
experiments showing LLMs' tendency to align their predictions with the stance
and sentiment of the provided information rather than the ground truth stance
of the given text. We also find that performance degradation persists with
chain-of-thought prompting, while fine-tuning mitigates but does not fully
eliminate it. Our findings, in contrast to previous literature on BERT-based
systems which suggests that external information enhances performance,
highlight the risks of information biases in LLM-based stance classifiers. Code
is available at https://github.com/ngqm/acl2025-stance-detection.

</details>


### [19] [Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation](https://arxiv.org/abs/2507.01594)
*Shutong Feng,Hsien-chin Lin,Nurul Lubis,Carel van Niekerk,Michael Heck,Benjamin Ruppik,Renato Vukovic,Milica Gašić*

Main category: cs.CL

TL;DR: 本文提出LUSTER，一个基于LLM的统一任务型对话系统，采用端到端强化学习，结合用户情绪（短期）和任务成功（长期）奖励，旨在构建更具弹性且情感响应能力的ToD系统。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）提升了语言流畅性，但构建有效且情感智能的任务型对话（ToD）系统仍是复杂挑战，需要同时优化任务成功、情感理解与响应以及信息传递，并在嘈杂环境中保持鲁棒性。

Method: 提出了LUSTER系统，一个基于LLM的统一任务型对话系统，采用端到端强化学习（RL），并结合了短期（用户情绪）和长期（任务成功）奖励。系统在一个由自然语言用户模拟器和不完善自然语言理解模块组成的挑战性评估环境中进行设置与测试。

Result: 研究结果表明，将大型语言模型的能力与结构化奖励建模相结合，能够构建出更具弹性且对情绪更敏感的任务型对话系统。

Conclusion: 结合LLM能力与结构化奖励建模，为开发下一代更具弹性、情感响应能力的会话代理提供了实用的前进路径。

Abstract: Task-oriented dialogue (ToD) systems are designed to help users achieve
specific goals through natural language interaction. While recent advances in
large language models (LLMs) have significantly improved linguistic fluency and
contextual understanding, building effective and emotionally intelligent ToD
systems remains a complex challenge. Effective ToD systems must optimise for
task success, emotional understanding and responsiveness, and precise
information conveyance, all within inherently noisy and ambiguous
conversational environments. In this work, we investigate architectural,
representational, optimisational as well as emotional considerations of ToD
systems. We set up systems covering these design considerations with a
challenging evaluation environment composed of a natural-language user
simulator coupled with an imperfect natural language understanding module. We
propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem
for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end
\textbf{R}einforcement learning with both short-term (user sentiment) and
long-term (task success) rewards. Our findings demonstrate that combining LLM
capability with structured reward modelling leads to more resilient and
emotionally responsive ToD systems, offering a practical path forward for
next-generation conversational agents.

</details>


### [20] [Chart Question Answering from Real-World Analytical Narratives](https://arxiv.org/abs/2507.01627)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Main category: cs.CL

TL;DR: 提出一个从可视化笔记构建的新图表问答数据集，对现有大语言模型构成显著挑战。


<details>
  <summary>Details</summary>
Motivation: 现有图表问答基准缺乏真实世界复杂性和生态有效性，无法反映实际推理流程。

Method: 从可视化笔记中构建了一个包含真实世界、多视图图表和自然语言问题的新图表问答（CQA）数据集，并使用该数据集对最先进的多模态大语言模型进行了基准测试。

Result: 最先进的大语言模型（如GPT-4.1）在该数据集上表现出显著性能差距，准确率仅为69.3%，突显了真实CQA环境的挑战性。

Conclusion: 新数据集揭示了当前大语言模型在真实世界图表问答方面的局限性，强调了未来研究和模型改进的必要性。

Abstract: We present a new dataset for chart question answering (CQA) constructed from
visualization notebooks. The dataset features real-world, multi-view charts
paired with natural language questions grounded in analytical narratives.
Unlike prior benchmarks, our data reflects ecologically valid reasoning
workflows. Benchmarking state-of-the-art multimodal large language models
reveals a significant performance gap, with GPT-4.1 achieving an accuracy of
69.3%, underscoring the challenges posed by this more authentic CQA setting.

</details>


### [21] [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](https://arxiv.org/abs/2507.01633)
*Georgii Levtsov,Dmitry Ustalov*

Main category: cs.CL

TL;DR: 本文经验性地研究了NLP模型评估中全局分数和成对比较的优缺点，发现全局分数排名更可靠但可能低估强模型，而成对比较在识别潜在强模型和处理难定义指标时更有效。


<details>
  <summary>Details</summary>
Motivation: 随着指令调优语言模型的兴起，NLP基准测试正从传统的全局得分转向成对比较排行榜。本文旨在经验性地调查全局得分和成对比较各自的优缺点，以帮助决策者选择合适的模型评估策略。

Method: 通过在合成数据集和真实世界数据集上进行计算实验，并使用标准全局指标和流行的Bradley-Terry模型进行成对比较。

Result: 全局分数提供了更可靠的整体排名，但可能低估那些拥有罕见但显著错误或低置信度的强大模型。成对比较在识别全局得分较低的模型中的强劲竞争者方面特别有效，尤其是在质量指标难以定义（如文本生成）的情况下，但如果平局频繁，则需要更多的比较才能收敛。

Conclusion: 全局分数和成对比较各有优劣，全局分数提供宏观可靠排名，而成对比较擅长发现特定优势模型和处理主观性任务。选择合适的评估策略需根据具体需求权衡这两种方法的特点。

Abstract: With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.

</details>


### [22] [Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings](https://arxiv.org/abs/2507.01645)
*Rifki Afina Putri*

Main category: cs.CL

TL;DR: 本研究探讨了预训练语言模型在低资源印尼本土语言情感分析任务上的可迁移性。结果表明模型性能显著受预训练阶段对语言的暴露程度影响，且MAD-X方法能有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 探究预训练语言模型在低资源印尼本土语言的情感分析任务中的可迁移性。

Method: 评估零样本（zero-shot）性能和基于适配器（adapter-based）的迁移学习。使用印尼语单语BERT、多语言模型（如mBERT、XLM-R）和MAD-X方法。将目标语言分为三类：已见过、部分见过和未见过。此外，还对分词和词汇重叠进行了分析。

Result: 多语言模型在已见过语言上表现最佳，部分见过语言次之，未见过语言表现最差。MAD-X显著提升了已见过和部分见过语言的性能，且无需目标语言的标注数据。子词碎片化和与印尼语的词汇重叠与预测质量相关性较弱，不能完全解释观察到的性能差异。

Conclusion: 模型对语言的先验暴露（直接或通过相关语言）是迁移成功最一致的预测因素。

Abstract: In this paper, we investigate the transferability of pre-trained language
models to low-resource Indonesian local languages through the task of sentiment
analysis. We evaluate both zero-shot performance and adapter-based transfer on
ten local languages using models of different types: a monolingual Indonesian
BERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based
approach called MAD-X. To better understand model behavior, we group the target
languages into three categories: seen (included during pre-training), partially
seen (not included but linguistically related to seen languages), and unseen
(absent and unrelated in pre-training data). Our results reveal clear
performance disparities across these groups: multilingual models perform best
on seen languages, moderately on partially seen ones, and poorly on unseen
languages. We find that MAD-X significantly improves performance, especially
for seen and partially seen languages, without requiring labeled data in the
target language. Additionally, we conduct a further analysis on tokenization
and show that while subword fragmentation and vocabulary overlap with
Indonesian correlate weakly with prediction quality, they do not fully explain
the observed performance. Instead, the most consistent predictor of transfer
success is the model's prior exposure to the language, either directly or
through a related language.

</details>


### [23] [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/abs/2507.01702)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Zhen Ye,Guang Chen,Zhiyong Huang,Jing Ma*

Main category: cs.CL

TL;DR: 本文提出AdamMeme，一个自适应、基于智能体的评估框架，用于深入探究多模态大语言模型（mLLMs）对动态演变在线有害模因的理解能力和具体弱点。


<details>
  <summary>Details</summary>
Motivation: 现有评估mLLM有害模因理解能力的基准依赖静态数据集和基于准确率的评估，无法适应动态变化的在线模因，导致评估不全面且易过时。

Method: 提出AdamMeme框架，它是一个灵活的、基于智能体的自适应评估系统。通过多智能体协作，该框架迭代更新具有挑战性的模因样本，以探测mLLM理解模因有害性的推理能力，并暴露其具体局限性。

Result: 实验结果表明，AdamMeme框架能够系统性地揭示不同目标mLLM的表现差异，并提供深入、细致的模型特定弱点分析。

Conclusion: AdamMeme框架有效克服了现有静态评估方法的局限性，能够动态、全面地评估mLLM对有害模因的理解能力，并精准识别模型特异性弱点。

Abstract: The proliferation of multimodal memes in the social media era demands that
multimodal Large Language Models (mLLMs) effectively understand meme
harmfulness. Existing benchmarks for assessing mLLMs on harmful meme
understanding rely on accuracy-based, model-agnostic evaluations using static
datasets. These benchmarks are limited in their ability to provide up-to-date
and thorough assessments, as online memes evolve dynamically. To address this,
we propose AdamMeme, a flexible, agent-based evaluation framework that
adaptively probes the reasoning capabilities of mLLMs in deciphering meme
harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive
evaluations by iteratively updating the meme data with challenging samples,
thereby exposing specific limitations in how mLLMs interpret harmfulness.
Extensive experiments show that our framework systematically reveals the
varying performance of different target mLLMs, offering in-depth, fine-grained
analyses of model-specific weaknesses. Our code is available at
https://github.com/Lbotirx/AdamMeme.

</details>


### [24] [Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach](https://arxiv.org/abs/2507.01715)
*Aditya Tomar,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 为解决语言模型中偏见和刻板印象的危害，本研究引入了StereoBias数据集，并发现联合训练模型进行偏见和刻板印象检测，能显著提升偏见检测性能，证实了利用刻板印象信息构建更公平AI的价值。


<details>
  <summary>Details</summary>
Motivation: 语言模型中的偏见和刻板印象，特别是在内容审核和决策等敏感领域，会造成危害。本研究旨在提升偏见和刻板印象的检测能力。

Method: 本研究探索了联合学习偏见和刻板印象检测任务以提升模型性能。为此，引入了独特的StereoBias数据集，该数据集标注了宗教、性别、社会经济地位、种族、职业等类别的偏见和刻板印象。实验对比了仅编码器模型和使用QLoRA微调的仅解码器模型。此外，通过与情感分析的对比实验，验证了性能提升的原因。

Result: 实验发现仅编码器模型表现良好，仅解码器模型也展现出竞争力。关键在于，偏见和刻板印象的联合训练显著提升了偏见检测的性能，优于单独训练。进一步的实验证实，这种性能提升源于偏见与刻板印象之间的内在联系，而非单纯的多任务学习效应。

Conclusion: 研究结果强调了利用刻板印象信息对于构建更公平、更有效的AI系统的重要价值，因为这能有效提升偏见检测能力。

Abstract: Bias and stereotypes in language models can cause harm, especially in
sensitive areas like content moderation and decision-making. This paper
addresses bias and stereotype detection by exploring how jointly learning these
tasks enhances model performance. We introduce StereoBias, a unique dataset
labeled for bias and stereotype detection across five categories: religion,
gender, socio-economic status, race, profession, and others, enabling a deeper
study of their relationship. Our experiments compare encoder-only models and
fine-tuned decoder-only models using QLoRA. While encoder-only models perform
well, decoder-only models also show competitive results. Crucially, joint
training on bias and stereotype detection significantly improves bias detection
compared to training them separately. Additional experiments with sentiment
analysis confirm that the improvements stem from the connection between bias
and stereotypes, not multi-task learning alone. These findings highlight the
value of leveraging stereotype information to build fairer and more effective
AI systems.

</details>


### [25] [LLMs for Legal Subsumption in German Employment Contracts](https://arxiv.org/abs/2507.01734)
*Oliver Wardas,Florian Matthes*

Main category: cs.CL

TL;DR: 本研究利用LLMs和上下文学习，评估德语雇佣合同条款的合法性。结果表明，考试指南能显著提升LLMs的分类表现（加权F1达80%），尽管仍远低于人类律师。


<details>
  <summary>Details</summary>
Motivation: 法律工作文本量大、资源密集，现有数据驱动型NLP方法在法律领域缺乏可解释性和可信度，限制了其在动态法律环境中的应用，促使研究者寻求更有效且可靠的解决方案。

Method: 研究通过与法律专家合作扩展现有数据集，并利用大型语言模型（LLMs）及上下文学习，评估LLMs在三种不同法律上下文（无上下文、法律法规及判例全文、考试指南）下对德语雇佣合同条款进行“有效”、“不公平”或“无效”分类的能力。

Result: 结果显示，法律法规全文能适度提升LLMs的表现，而考试指南能显著提高“无效”条款的召回率和加权F1分数，达到80%。然而，LLMs在使用全文源时的表现仍远低于人类律师。

Conclusion: 本研究成果表明，LLMs在协助律师进行合同合法性审查方面具有潜力，但同时也揭示了当前方法的局限性。研究贡献了一个包含考试指南、引用法律源及对应标注的扩展数据集，以及相关代码和日志文件。

Abstract: Legal work, characterized by its text-heavy and resource-intensive nature,
presents unique challenges and opportunities for NLP research. While
data-driven approaches have advanced the field, their lack of interpretability
and trustworthiness limits their applicability in dynamic legal environments.
To address these issues, we collaborated with legal experts to extend an
existing dataset and explored the use of Large Language Models (LLMs) and
in-context learning to evaluate the legality of clauses in German employment
contracts. Our work evaluates the ability of different LLMs to classify clauses
as "valid," "unfair," or "void" under three legal context variants: no legal
context, full-text sources of laws and court rulings, and distilled versions of
these (referred to as examination guidelines). Results show that full-text
sources moderately improve performance, while examination guidelines
significantly enhance recall for void clauses and weighted F1-Score, reaching
80\%. Despite these advancements, LLMs' performance when using full-text
sources remains substantially below that of human lawyers. We contribute an
extended dataset, including examination guidelines, referenced legal sources,
and corresponding annotations, alongside our code and all log files. Our
findings highlight the potential of LLMs to assist lawyers in contract legality
review while also underscoring the limitations of the methods presented.

</details>


### [26] [Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results](https://arxiv.org/abs/2507.01764)
*Matteo Di Cristofaro*

Main category: cs.CL

TL;DR: 分词是语料库语言学的关键步骤，本文探讨了分词差异（特别是处理表情符号和同形字时）如何影响语言数据表示和分析有效性。研究强调了预处理这些元素的重要性，并提出了确保数字文本在语料库中准确表示的方法，以支持可靠的语言分析和可重复的解释。


<details>
  <summary>Details</summary>
Motivation: 分词是语料库语言学中所有定量和定性分析的基础，至关重要。然而，分词中的差异，尤其是在处理现代文本元素如表情符号和同形字时，可能导致语言数据表示失真和分析结果的有效性受损。因此，有必要探究这些差异的影响并提出解决方案。

Method: 本研究通过调查表情符号和同形字在分词过程中带来的挑战，检验了分词差异对语言数据表示和分析发现有效性的影响。文章提出了确保数字文本在语料库中准确表示的方法，并强调了对这些特定元素进行预处理的必要性。

Result: 研究发现，为了保持语料库对原始数据的忠实性，对表情符号和同形字进行预处理是必不可少的。这有助于支持可靠的语言分析，并确保语言解释的可重复性。研究结果强调，深入理解数字文本数据中涉及的语言和技术方面对于提高语料库分析的准确性至关重要。

Conclusion: 为实现准确且可重复的语料库分析，必须解决分词差异（特别是通过预处理表情符号和同形字），并对数字文本数据的语言和技术层面有详细的理解。这对于基于语料库的定量和定性研究都具有重要意义。

Abstract: Tokenisation - "the process of splitting text into atomic parts" (Brezina &
Timperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides
the basis for any applicable quantitative method (e.g. collocations) while
ensuring the reliability of qualitative approaches. This paper examines how
discrepancies in tokenisation affect the representation of language data and
the validity of analytical findings: investigating the challenges posed by
emojis and homoglyphs, the study highlights the necessity of preprocessing
these elements to maintain corpus fidelity to the source data. The research
presents methods for ensuring that digital texts are accurately represented in
corpora, thereby supporting reliable linguistic analysis and guaranteeing the
repeatability of linguistic interpretations. The findings emphasise the
necessity of a detailed understanding of both linguistic and technical aspects
involved in digital textual data to enhance the accuracy of corpus analysis,
and have significant implications for both quantitative and qualitative
approaches in corpus-based research.

</details>


### [27] [MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2507.01785)
*Zhixun Chen,Ping Guo,Wenhan Han,Yifan Zhang,Binbin Liu,Haobin Lin,Fengze Liu,Yan Zhao,Bingni Zhang,Taifeng Wang,Yin Zheng,Meng Fang*

Main category: cs.CL

TL;DR: MuRating是一个可扩展框架，通过整合英语数据质量信号并进行跨语言投射，训练多语言评估器，从而在17种语言中选择高质量数据，显著提升了LLaMA模型在英语及多语言任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型数据选择方法主要集中于英语，但数据质量对模型性能至关重要，因此需要一种可扩展的框架，将高质量的英语数据质量信号应用于多语言数据选择。

Method: MuRating通过成对比较聚合多个英语“评估器”以学习统一的文档质量分数，然后通过翻译将这些判断投射，以训练一个多语言评估器。该评估器在单语、跨语言和并行文本对上进行训练，并应用于网络数据以选择平衡的英语和多语言内容子集，用于预训练1.2B参数的LLaMA模型。

Result: 与现有强基线（如QuRater、AskLLM、DCLM）相比，MuRating方法提升了LLaMA模型在英语基准和多语言评估中的平均准确性，尤其在知识密集型任务上获得了显著提升。研究还分析了翻译保真度、选择偏差和叙事材料的代表性不足问题。

Conclusion: MuRating成功地将英语数据质量信号扩展到多语言环境，有效提高了大语言模型在多语言和知识密集型任务上的性能。研究也为未来在翻译保真度、选择偏差和数据代表性方面的工作指明了方向。

Abstract: Data quality is a critical driver of large language model performance, yet
existing model-based selection methods focus almost exclusively on English. We
introduce MuRating, a scalable framework that transfers high-quality English
data-quality signals into a single rater for 17 target languages. MuRating
aggregates multiple English "raters" via pairwise comparisons to learn unified
document-quality scores,then projects these judgments through translation to
train a multilingual evaluator on monolingual, cross-lingual, and parallel text
pairs. Applied to web data, MuRating selects balanced subsets of English and
multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to
strong baselines, including QuRater, AskLLM, DCLM and so on, our approach
boosts average accuracy on both English benchmarks and multilingual
evaluations, with especially large gains on knowledge-intensive tasks. We
further analyze translation fidelity, selection biases, and underrepresentation
of narrative material, outlining directions for future work.

</details>


### [28] [Probing Evaluation Awareness of Language Models](https://arxiv.org/abs/2507.01786)
*Jord Nguyen,Khiem Hoang,Carlo Leonardo Attubato,Felix Hofstätter*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（Llama-3.3-70B-Instruct）具备“评估意识”，即能区分测试与部署环境。线性探针显示模型内部已存在这种区分，且当前安全评估对模型而言显得不真实，这可能削弱评估可靠性，强调了确保评估信任度和理解模型欺骗能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 语言模型能够区分测试和部署阶段的能力（评估意识）对AI治理和行业承诺具有重大安全和政策影响，因为它可能损害评估的可靠性。因此，有必要研究并理解这种能力。

Method: 本研究以Llama-3.3-70B-Instruct为研究对象，采用线性探针（linear probes）来分析模型的内部表征，以区分真实世界的评估提示和部署提示。

Result: 线性探针能够区分真实世界的评估和部署提示，表明当前模型内部已表征这种区别。此外，线性探针正确分类了当前的安全评估，暗示这些评估对模型而言显得人工或不真实。

Conclusion: 研究结果强调了确保评估可信度以及理解模型欺骗能力的重要性。这项工作展示了如何利用模型内部机制来支持安全审计，尤其对于未来在评估意识和欺骗方面更强的模型。

Abstract: Language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. This has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to AI governance frameworks and voluntary industry
commitments. In this paper, we study evaluation awareness in
Llama-3.3-70B-Instruct. We show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. We also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. Our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. More
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception.

</details>


### [29] [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/abs/2507.01790)
*Tianze Hua,Tian Yun,Ellie Pavlick*

Main category: cs.CL

TL;DR: 研究多模态模型如何处理冲突信息，发现模型存在模态偏好，并识别出可控的“路由头”，有助于解决冲突。


<details>
  <summary>Details</summary>
Motivation: 多模态AI模型需整合不同输入流，理解模型在输入流呈现冲突信息时的行为至关重要。

Method: 针对视觉-语言模型，输入冲突的图像和文本（如狗图配猫描述），并要求模型报告特定模态信息（如“标题说什么？”或“图片里有什么？”），同时分析其内部表示和注意力机制。

Result: 模型常偏好某一模态，不同模型偏好不同；行为偏好的模态体现在内部表示结构中，特定注意力头可重塑表示以偏向某模态；发现模态无关的“路由头”能促进回答指令请求的模态信息，且可被操控或迁移以提升性能。

Conclusion: 本研究为识别和控制多模态模型在复杂环境中如何检测并解决冲突信号提供了重要步骤。

Abstract: AI models are increasingly required to be multimodal, integrating disparate
input streams into a coherent state representation on which subsequent
behaviors and actions can be based. This paper seeks to understand how such
models behave when input streams present conflicting information. Focusing
specifically on vision-language models, we provide inconsistent inputs (e.g.,
an image of a dog paired with the caption "A photo of a cat") and ask the model
to report the information present in one of the specific modalities (e.g.,
"What does the caption say / What is in the image?"). We find that models often
favor one modality over the other, e.g., reporting the image regardless of what
the caption says, but that different models differ in which modality they
favor. We find evidence that the behaviorally preferred modality is evident in
the internal representational structure of the model, and that specific
attention heads can restructure the representations to favor one modality over
the other. Moreover, we find modality-agnostic "router heads" which appear to
promote answers about the modality requested in the instruction, and which can
be manipulated or transferred in order to improve performance across datasets
and modalities. Together, the work provides essential steps towards identifying
and controlling if and how models detect and resolve conflicting signals within
complex multimodal environments.

</details>


### [30] [The Anatomy of Evidence: An Investigation Into Explainable ICD Coding](https://arxiv.org/abs/2507.01802)
*Katharina Beckh,Elisa Studeny,Sujan Sai Gannamaneni,Dario Antweiler,Stefan Rüping*

Main category: cs.CL

TL;DR: 本文对MDACE数据集进行了深入分析，并从应用角度评估了当前可解释医疗编码系统的合理性，揭示了现有方法与真实证据的高度重叠，并为未来的系统开发和评估提供了建议。


<details>
  <summary>Details</summary>
Motivation: 自动医疗编码需要透明度以满足医疗编码员和监管机构的要求，但这方面的评估因标注数据稀缺而受限。新发布的MDACE数据集为解决这一问题提供了资源。

Method: 对MDACE数据集进行深入分析，并对现有可解释医疗编码系统进行合理性评估，提出匹配度量方法，并分析成功与失败案例。

Result: 研究发现，MDACE数据集中的真实证据与代码描述在一定程度上保持一致。对最新方法的调查显示，它们与真实证据之间存在高度重叠。

Conclusion: 研究结果加深了对自动医疗编码和证据提取的理解，并在此基础上为可解释医疗编码系统的开发和评估提供了具体建议。

Abstract: Automatic medical coding has the potential to ease documentation and billing
processes. For this task, transparency plays an important role for medical
coders and regulatory bodies, which can be achieved using explainability
methods. However, the evaluation of these approaches has been mostly limited to
short text and binary settings due to a scarcity of annotated data. Recent
efforts by Cheng et al. (2023) have introduced the MDACE dataset, which
provides a valuable resource containing code evidence in clinical records. In
this work, we conduct an in-depth analysis of the MDACE dataset and perform
plausibility evaluation of current explainable medical coding systems from an
applied perspective. With this, we contribute to a deeper understanding of
automatic medical coding and evidence extraction. Our findings reveal that
ground truth evidence aligns with code descriptions to a certain degree. An
investigation into state-of-the-art approaches shows a high overlap with ground
truth evidence. We propose match measures and highlight success and failure
cases. Based on our findings, we provide recommendations for developing and
evaluating explainable medical coding systems.

</details>


### [31] [Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes](https://arxiv.org/abs/2507.01810)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.CL

TL;DR: 对比分析了小型语言模型在临床笔记中生成结构化输出的解析性，发现JSON表现最佳。


<details>
  <summary>Details</summary>
Motivation: 旨在评估小型语言模型在临床环境中进行属性-值提取时，不同序列化格式（JSON, YAML, XML）生成结构化输出的解析性及鲁棒性，从而为实际部署提供指导。

Method: 研究通过比较分析小型语言模型从临床笔记中提取属性-值时生成的结构化输出在JSON、YAML和XML三种序列化格式下的解析性。同时，评估了定向提示、模型大小、文档长度及笔记类型对结构鲁棒性的影响，并进行了错误分析以识别特定格式的失败模式。

Result: JSON在解析性方面持续展现出最高的表现。结构鲁棒性可通过定向提示和使用大型模型得到改善，但对较长文档和特定笔记类型则有所下降。研究还识别了重复出现的格式特定失败模式。

Conclusion: 本研究结果为在隐私敏感的临床环境中部署语言模型时，选择合适的序列化格式和设计有效的提示提供了实用指导。

Abstract: We present a comparative analysis of the parseability of structured outputs
generated by small language models for open attribute-value extraction from
clinical notes. We evaluate three widely used serialization formats: JSON,
YAML, and XML, and find that JSON consistently yields the highest parseability.
Structural robustness improves with targeted prompting and larger models, but
declines for longer documents and certain note types. Our error analysis
identifies recurring format-specific failure patterns. These findings offer
practical guidance for selecting serialization formats and designing prompts
when deploying language models in privacy-sensitive clinical settings.

</details>


### [32] [Low-Perplexity LLM-Generated Sequences and Where To Find Them](https://arxiv.org/abs/2507.01844)
*Arthur Wuhrmann,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 本研究引入一种系统方法，通过分析大语言模型（LLMs）生成的低困惑度序列来追溯其训练数据源。结果显示，大部分高概率生成文本无法直接匹配语料库，而匹配部分揭示了LLMs的逐字召回特性，为理解训练数据对LLMs行为的影响提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）日益普及，理解特定训练数据如何塑造其输出对于提升透明度、问责制、隐私保护和公平性至关重要。本研究旨在探索LLMs如何利用并复制其训练数据。

Method: 引入了一种系统方法，专注于分析LLM生成的低困惑度序列（即模型生成的高概率文本片段）。该方法包括一个管道，能够可靠地提取跨越不同主题的长序列，并避免退化现象，然后将这些序列追溯到其在训练数据中的来源。

Result: 研究发现：1) 令人惊讶的是，很大一部分低困惑度序列无法映射到训练语料库中。2) 对于那些能够匹配的序列，研究量化了它们在源文档中的出现分布，揭示了LLMs逐字召回的范围和性质。

Conclusion: 本研究为更好地理解LLMs训练数据如何影响其行为铺平了道路，尤其是在揭示高概率生成文本与训练数据之间的复杂关系方面，并量化了逐字召回的特性。

Abstract: As Large Language Models (LLMs) become increasingly widespread, understanding
how specific training data shapes their outputs is crucial for transparency,
accountability, privacy, and fairness. To explore how LLMs leverage and
replicate their training data, we introduce a systematic approach centered on
analyzing low-perplexity sequences - high-probability text spans generated by
the model. Our pipeline reliably extracts such long sequences across diverse
topics while avoiding degeneration, then traces them back to their sources in
the training data. Surprisingly, we find that a substantial portion of these
low-perplexity spans cannot be mapped to the corpus. For those that do match,
we quantify the distribution of occurrences across source documents,
highlighting the scope and nature of verbatim recall and paving a way toward
better understanding of how LLMs training data impacts their behavior.

</details>


### [33] [Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages](https://arxiv.org/abs/2507.01853)
*Samridhi Raj Sinha,Rajvee Sheth,Abhishek Upperwal,Mayank Singh*

Main category: cs.CL

TL;DR: EKA-EVAL是一个针对多语言（特别是印度语言）大语言模型的统一评估框架，它整合了广泛的基准测试，并支持分布式推理，旨在降低多语言基准测试的门槛。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的快速发展，需要超越以英语为中心的基准测试，建立适用于印度等语言多样化地区的评估框架。

Method: 提出并开发了EKA-EVAL，一个统一且可用于生产的评估框架。该框架集成了超过35个基准测试，其中包括10个针对印度语言的特定数据集，涵盖推理、数学、工具使用、长文本理解和阅读理解等类别。该框架还内置支持分布式推理、量化和多GPU使用。

Result: EKA-EVAL相比现有印度语言评估工具，提供了更广泛的基准测试覆盖。通过系统性比较，EKA-EVAL被定位为第一个为全球和印度语言LLMs定制的端到端、可扩展的评估套件。

Conclusion: EKA-EVAL显著降低了多语言基准测试的门槛，并致力于建立一个健壮的多语言LLMs评估生态系统。

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for evaluation frameworks that go beyond English centric benchmarks and
address the requirements of linguistically diverse regions such as India. We
present EKA-EVAL, a unified and production-ready evaluation framework that
integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning
categories like reasoning, mathematics, tool use, long-context understanding,
and reading comprehension. Compared to existing Indian language evaluation
tools, EKA-EVAL offers broader benchmark coverage, with built-in support for
distributed inference, quantization, and multi-GPU usage. Our systematic
comparison positions EKA-EVAL as the first end-to-end, extensible evaluation
suite tailored for both global and Indic LLMs, significantly lowering the
barrier to multilingual benchmarking. The framework is open-source and publicly
available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA
initiative (https://eka.soket.ai), which aims to scale up to over 100
benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.

</details>


### [34] [DIY-MKG: An LLM-Based Polyglot Language Learning System](https://arxiv.org/abs/2507.01872)
*Kenan Tang,Yanhong Li,Yao Qin*

Main category: cs.CL

TL;DR: 本文设计并评估了一个开源的多语言知识图谱系统DIY-MKG，旨在解决现有语言学习工具在多语种学习、个性化和认知卸载方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有语言学习工具，即便有大型语言模型（LLMs）加持，仍缺乏对多语种学习者建立跨语言词汇联系的支持，个性化定制能力有限，并存在认知卸载的问题。

Method: 设计了开源系统DIY-MKG，允许用户构建个性化词汇知识图谱，通过LLM建议的相关词进行选择性扩展。系统还通过丰富的标注功能和利用LLM动态生成个性化测验的自适应复习模块增强学习，并允许用户标记错误测验题以提供反馈。

Result: 对DIY-MKG中基于LLM的组件评估显示，词汇扩展在多语言间可靠且公平，生成的测验准确性高，验证了DIY-MKG的健壮性。

Conclusion: DIY-MKG是一个健壮的系统，有效解决了现有语言学习工具的局限性，为多语种学习者提供了个性化、互联的语言学习新途径。

Abstract: Existing language learning tools, even those powered by Large Language Models
(LLMs), often lack support for polyglot learners to build linguistic
connections across vocabularies in multiple languages, provide limited
customization for individual learning paces or needs, and suffer from
detrimental cognitive offloading. To address these limitations, we design
Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system
that supports polyglot language learning. DIY-MKG allows the user to build
personalized vocabulary knowledge graphs, which are constructed by selective
expansion with related words suggested by an LLM. The system further enhances
learning through rich annotation capabilities and an adaptive review module
that leverages LLMs for dynamic, personalized quiz generation. In addition,
DIY-MKG allows users to flag incorrect quiz questions, simultaneously
increasing user engagement and providing a feedback loop for prompt refinement.
Our evaluation of LLM-based components in DIY-MKG shows that vocabulary
expansion is reliable and fair across multiple languages, and that the
generated quizzes are highly accurate, validating the robustness of DIY-MKG.

</details>


### [35] [MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants](https://arxiv.org/abs/2507.01887)
*Dongyi Ding,Tiannan Wang,Chenghao Zhu,Meiling Tao,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: MiCoTAl通过使用中间尺寸模型作为教师助手和中间长度CoT序列，有效解决了小型语言模型(SLMs)在长链式思维(CoT)推理方面的学习能力差距，显著提升了其推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)擅长复杂推理但部署成本高昂；小型语言模型(SLMs)因容量限制难以学习长CoT推理，存在“SLMs可学习性差距”。

Method: 引入MiCoTAl（Mid-CoT Teacher Assistant Distillation）框架，利用中间尺寸模型作为教师助手，并采用中间长度CoT序列，以弥合SLMs在容量和推理长度上的差距。通过定量实验证明其生成的数据更符合SLM基础分布。

Result: MiCoTAl显著提升了SLMs的推理性能，克服了直接从大型教师模型蒸馏效果不佳的问题。在AIME2024、AMC等多个基准测试中，Qwen2.5-7B-Instruct和Qwen2.5-3B-Instruct的平均分数分别提升了3.47和3.93。该方法生成的数据与SLM基础分布更匹配。

Conclusion: MiCoTAl为提升SLMs的长CoT推理能力提供了有效途径，其机制有助于未来长CoT数据蒸馏的研究，使SLMs在实际部署中具备更强的推理能力。

Abstract: Large language models (LLMs) excel at reasoning tasks requiring long thought
sequences for planning, reflection, and refinement. However, their substantial
model size and high computational demands are impractical for widespread
deployment. Yet, small language models (SLMs) often struggle to learn long-form
CoT reasoning due to their limited capacity, a phenomenon we refer to as the
"SLMs Learnability Gap". To address this, we introduce
\textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation
(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA
employs intermediate-sized models as teacher assistants and utilizes
intermediate-length CoT sequences to bridge both the capacity and reasoning
length gaps. Our experiments on downstream tasks demonstrate that although SLMs
distilled from large teachers can perform poorly, by applying MiCoTA, they
achieve significant improvements in reasoning performance. Specifically,
Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and
3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and
GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform
a quantitative experiment demonstrating that our method produces data more
closely aligned with base SLM distributions. Our insights pave the way for
future research into long-CoT data distillation for SLMs.

</details>


### [36] [High-Layer Attention Pruning with Rescaling](https://arxiv.org/abs/2507.01900)
*Songtao Liu,Peng Liu*

Main category: cs.CL

TL;DR: 本文提出了一种针对大型语言模型（LLMs）的新型剪枝算法，通过策略性地剪枝高层注意力头并引入自适应重缩放参数，显著优于现有方法，尤其在生成任务上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有无训练结构化剪枝方法在修剪大型语言模型（LLMs）时，未考虑注意力头在网络中的位置，而是盲目移除，这可能影响模型性能。

Method: 提出了一种新颖的剪枝算法，策略性地修剪模型高层中的注意力头。为抵消移除注意力头对token表示幅度的影响，引入了一个自适应重缩放参数来校准剪枝后的表示尺度。

Result: 在多种LLMs（包括LLaMA3.1-8B、Mistral-7B-v0.3、Qwen2-7B和Gemma2-9B）上进行了广泛实验，涵盖27个数据集的生成和判别任务。结果表明，该方法持续优于现有结构化剪枝方法，尤其在生成任务中提升显著。

Conclusion: 所提出的结合高层策略性剪枝和自适应重缩放的算法，能够有效提升大型语言模型的压缩性能，并在生成任务上取得超越现有基线的效果。

Abstract: Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.

</details>


### [37] [AI4Research: A Survey of Artificial Intelligence for Scientific Research](https://arxiv.org/abs/2507.01903)
*Qiguang Chen,Mingda Yang,Libo Qin,Jinhao Liu,Zheng Yan,Jiannan Guan,Dengyun Peng,Yiyan Ji,Hanjing Li,Mengkang Hu,Yimeng Zhang,Yihao Liang,Yuhang Zhou,Jiaqi Wang,Zhi Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文对“AI用于研究”（AI4Research）领域进行了一项全面综述，提出了一个系统分类法，识别了研究空白和未来方向，并汇编了相关资源，旨在促进该领域的发展和创新。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能（特别是大型语言模型）在复杂领域（如逻辑推理和实验编码）展现出卓越能力，AI在科学研究创新过程中的应用受到广泛探索。然而，目前缺乏一份关于AI4Research的全面综述，这阻碍了对其理解和进一步发展。

Method: 为填补这一空白，本文进行了一项全面的AI4Research综述。具体方法包括：1) 引入一个系统分类法，对AI4Research中的五大主流任务进行分类。2) 识别关键研究空白，并指出有前景的未来方向，特别关注自动化实验的严谨性、可扩展性及社会影响。3) 汇编了丰富的资源，包括多学科应用、数据语料库和工具。

Result: 本研究成果包括：成功构建了一个系统的AI4Research任务分类法；明确了该领域的关键研究空白和有前景的未来发展方向；汇编了大量相关的多学科应用、数据语料库和工具资源。

Conclusion: 这项工作旨在为研究社区提供快速获取AI4Research相关资源的途径，并刺激该领域的创新突破。

Abstract: Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated
remarkable capabilities in complex domains such as logical reasoning and
experimental coding. Motivated by these advancements, numerous studies have
explored the application of AI in the innovation process, particularly in the
context of scientific research. These AI technologies primarily aim to develop
systems that can autonomously conduct research processes across a wide range of
scientific disciplines. Despite these significant strides, a comprehensive
survey on AI for Research (AI4Research) remains absent, which hampers our
understanding and impedes further development in this field. To address this
gap, we present a comprehensive survey and offer a unified perspective on
AI4Research. Specifically, the main contributions of our work are as follows:
(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify
five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key
research gaps and highlight promising future directions, focusing on the rigor
and scalability of automated experiments, as well as the societal impact. (3)
Abundant applications and resources: Finally, we compile a wealth of resources,
including relevant multidisciplinary applications, data corpora, and tools. We
hope our work will provide the research community with quick access to these
resources and stimulate innovative breakthroughs in AI4Research.

</details>


### [38] [Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models](https://arxiv.org/abs/2507.01915)
*Chengao Li,Hanyu Zhang,Yunkun Xu,Hongyan Xue,Xiang Ao,Qing He*

Main category: cs.CL

TL;DR: 本文提出梯度自适应策略优化（GAPO）及其变体P-GAPO，通过将人类偏好对齐视为多目标优化问题，并利用多梯度下降来平衡冲突目标，从而有效对齐大型语言模型（LLMs）与多样化人类偏好。


<details>
  <summary>Details</summary>
Motivation: 尽管RLHF在LLM对齐方面表现出色，但将LLMs与多样化、特别是相互冲突的人类偏好有效对齐，仍然是一个重大挑战。

Method: 研究将人类价值观对齐定义为多目标优化问题。提出了梯度自适应策略优化（GAPO），这是一种新型微调范式，采用多梯度下降来对齐LLMs与多样化偏好分布。GAPO自适应地重新调整每个目标的梯度，以确定最佳平衡目标之间权衡的更新方向。此外，引入了P-GAPO，它结合了用户在不同目标上的偏好，以实现更符合用户特定需求的帕累托解。理论分析证明GAPO收敛于多目标的帕累托最优解。

Result: 理论分析表明GAPO收敛于多目标帕累托最优解。在Mistral-7B上的实证结果显示，GAPO优于当前最先进的方法，在“有用性”（helpfulness）和“无害性”（harmlessness）方面均实现了卓越的性能。

Conclusion: GAPO/P-GAPO提供了一种有效的方法来解决LLMs与多样化及冲突人类偏好的对齐问题，通过多目标优化框架和自适应梯度调整，显著提升了LLM在关键对齐维度上的表现。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.

</details>


### [39] [NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks](https://arxiv.org/abs/2507.01921)
*Yang Li,Youssef Emad,Karthik Padthe,Jack Lanchantin,Weizhe Yuan,Thao Nguyen,Jason Weston,Shang-Wen Li,Dong Wang,Ilia Kulikov,Xian Li*

Main category: cs.CL

TL;DR: 本文系统研究了蒸馏教师模型推理轨迹以提升学生模型推理能力的有效策略，发现选择需要多样化推理策略的难题示例更有效，并构建了高质量数据集NaturalThoughts，在多项STEM推理基准上优于现有数据集。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明通过监督微调蒸馏教师模型的推理轨迹是有效的，但缺乏系统性研究何种推理演示对于提升学生模型推理能力最有效。

Method: 1. 基于NaturalReasoning的问题池，从强教师模型中筛选高质量推理轨迹，构建了“NaturalThoughts”数据集。 2. 系统分析了影响推理能力蒸馏的因素，特别关注通过选择需要多样化推理策略的难题示例来提高样本效率。

Result: 1. 简单扩大随机采样数据量是有效的强基线。 2. 选择需要更复杂和多样化推理策略的难题示例能更高效地迁移教师模型的推理技能。 3. 在Llama和Qwen模型上，使用NaturalThoughts训练的学生模型在GPQA-Diamond、MMLU-Pro和SuperGPQA等通用STEM推理基准测试中，性能优于OpenThoughts、LIMO等现有推理数据集。

Conclusion: 通过系统性分析，证明选择需要多样化推理策略的难题示例能更有效地蒸馏教师模型的推理能力。所构建的NaturalThoughts数据集显著提升了学生模型在通用STEM推理任务上的表现。

Abstract: Recent work has shown that distilling reasoning traces from a larger teacher
model via supervised finetuning outperforms reinforcement learning with the
smaller student model alone (Guo et al. 2025). However, there has not been a
systematic study of what kind of reasoning demonstrations from the teacher are
most effective in improving the student model's reasoning capabilities. In this
work we curate high-quality "NaturalThoughts" by selecting reasoning traces
from a strong teacher model based on a large pool of questions from
NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of
factors that affect distilling reasoning capabilities, in terms of sample
efficiency and scalability for general reasoning tasks. We observe that simply
scaling up data size with random sampling is a strong baseline with steady
performance gains. Further, we find that selecting difficult examples that
require more diverse reasoning strategies is more sample-efficient to transfer
the teacher model's reasoning skills. Evaluated on both Llama and Qwen models,
training with NaturalThoughts outperforms existing reasoning datasets such as
OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including
GPQA-Diamond, MMLU-Pro and SuperGPQA.

</details>


### [40] [Decision-oriented Text Evaluation](https://arxiv.org/abs/2507.01923)
*Yu-Shiang Huang,Chuan-Ju Wang,Chung-Chi Chen*

Main category: cs.CL

TL;DR: 本文提出一种决策导向的自然语言生成（NLG）评估框架，通过衡量生成文本对人类和LLM决策结果的影响来评估NLG。研究发现单独依赖摘要表现不佳，但人机协作在丰富分析性文本下能显著提升决策绩效，强调了NLG评估应关注其促进人机协同决策的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的NLG内部评估方法（如n-gram重叠或句子合理性）与生成文本在实际高风险领域的决策效率关联性弱。

Method: 提出一个决策导向的评估框架，直接测量生成文本对人类和大型语言模型（LLM）决策结果的影响。以市场摘要文本（包括客观早间总结和主观收盘分析）为测试案例，根据人类投资者和LLM代理在仅受这些文本影响下进行的交易金融表现来评估决策质量。

Result: 结果显示，无论是人类还是LLM代理，单独依赖摘要时均无法持续超越随机表现。然而，更丰富的分析性评论使人机协作团队能显著优于个体人类或代理基线。

Conclusion: 本研究强调了通过生成文本促进人类与LLM协同决策的能力来评估NLG的重要性，并指出了传统内部评估方法的关键局限性。

Abstract: Natural language generation (NLG) is increasingly deployed in high-stakes
domains, yet common intrinsic evaluation methods, such as n-gram overlap or
sentence plausibility, weakly correlate with actual decision-making efficacy.
We propose a decision-oriented framework for evaluating generated text by
directly measuring its influence on human and large language model (LLM)
decision outcomes. Using market digest texts--including objective morning
summaries and subjective closing-bell analyses--as test cases, we assess
decision quality based on the financial performance of trades executed by human
investors and autonomous LLM agents informed exclusively by these texts. Our
findings reveal that neither humans nor LLM agents consistently surpass random
performance when relying solely on summaries. However, richer analytical
commentaries enable collaborative human-LLM teams to outperform individual
human or agent baselines significantly. Our approach underscores the importance
of evaluating generated text by its ability to facilitate synergistic
decision-making between humans and LLMs, highlighting critical limitations of
traditional intrinsic metrics.

</details>


### [41] [Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla](https://arxiv.org/abs/2507.01931)
*Md Sazzadul Islam Ridoy,Sumi Akter,Md. Aminur Rahman*

Main category: cs.CL

TL;DR: 本研究比较了Whisper和Wav2Vec-BERT在孟加拉语ASR上的性能，发现Wav2Vec-BERT表现更优且资源消耗更少。


<details>
  <summary>Details</summary>
Motivation: 为了支持低资源语言，研究了当前最先进的ASR模型（Whisper和Wav2Vec-BERT）在低资源语言孟加拉语上的性能表现。

Method: 使用Mozilla Common Voice-17和OpenSLR两个公开数据集，对Whisper (Small & Large-V2) 和Wav2Vec-BERT模型进行了系统性的微调和超参数优化。通过词错误率（WER）、字符错误率（CER）、训练时间和计算效率等指标进行模型比较。

Result: Wav2Vec-BERT模型在所有关键评估指标上均优于Whisper，表现出卓越的性能，同时所需的计算资源也更少。

Conclusion: 研究结果为在低资源语言环境下开发稳健的语音识别系统提供了有价值的见解。

Abstract: In recent years, neural models trained on large multilingual text and speech
datasets have shown great potential for supporting low-resource languages. This
study investigates the performances of two state-of-the-art Automatic Speech
Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's
Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments
using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to
evaluate model performances. Through systematic fine-tuning and hyperparameter
optimization, including learning rate, epochs, and model checkpoint selection,
we have compared the models based on Word Error Rate (WER), Character Error
Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model
outperformed Whisper across all key evaluation metrics, demonstrated superior
performance while requiring fewer computational resources, and offered valuable
insights to develop robust speech recognition systems in low-resource
linguistic settings.

</details>


### [42] [The Thin Line Between Comprehension and Persuasion in LLMs](https://arxiv.org/abs/2507.01936)
*Adrian de Wynter,Tangming Yuan*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）能够进行连贯且有说服力的辩论，但它们缺乏对对话深层结构和语用语境的真正理解，这表明有效对话不一定需要真正理解。


<details>
  <summary>Details</summary>
Motivation: 鉴于LLMs在敏感领域（如聊天机器人、评估器）的快速部署及其推理能力的争议，有必要深入探究LLMs的对话能力及其对对话的理解程度。

Method: 本研究首先评估了LLMs维持辩论的能力，然后衡量了这种能力与其对对话结构和语用语境的理解之间的关系。

Result: LLMs能够进行连贯且有说服力的辩论，常常能影响参与者和观众的信念。然而，LLMs无法展示对对话深层结构的理解。此外，当人们意识到AI参与时，会更批判性地看待其论点。

Conclusion: LLMs作为评估器的不足与其对语境的理解能力不足有关。研究表明，如果一个智能体能令人信服地维持对话，它不一定需要真正理解所谈论的内容，因此语用语境和连贯性建模对于有效性而言是次要的。

Abstract: Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [Geometry-aware 4D Video Generation for Robot Manipulation](https://arxiv.org/abs/2507.01099)
*Zeyi Liu,Shuang Li,Eric Cousineau,Siyuan Feng,Benjamin Burchfiel,Shuran Song*

Main category: cs.CV

TL;DR: 提出一种4D视频生成模型，通过跨视图点图对齐实现多视图3D一致性，能从RGB-D数据预测新视角的未来视频，无需相机位姿，且可用于机器人轨迹恢复与操作。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在生成跨相机视图的、兼具时间连贯性和几何一致性的视频方面存在挑战。准确预测物理世界的动态对提升机器人规划和交互能力至关重要。

Method: 提出一个4D视频生成模型，通过在训练过程中使用跨视图点图对齐来监督模型，以强制实现视频的多视图3D一致性。这种几何监督使模型能够学习场景的共享3D表示，从而仅基于RGB-D观测数据预测来自新视角的未来视频序列，无需相机位姿作为输入。

Result: 与现有基线方法相比，该方法在多个模拟和真实机器人数据集上生成了更视觉稳定且空间对齐的预测。此外，预测的4D视频能够通过现成的6自由度姿态跟踪器恢复机器人末端执行器轨迹。

Conclusion: 该模型通过确保几何一致性，显著提升了4D视频生成质量，并能支持鲁棒的机器人操作以及对新相机视角的泛化，为机器人对动态环境的理解和交互提供了有效工具。

Abstract: Understanding and predicting the dynamics of the physical world can enhance a
robot's ability to plan and interact effectively in complex environments. While
recent video generation models have shown strong potential in modeling dynamic
scenes, generating videos that are both temporally coherent and geometrically
consistent across camera views remains a significant challenge. To address
this, we propose a 4D video generation model that enforces multi-view 3D
consistency of videos by supervising the model with cross-view pointmap
alignment during training. This geometric supervision enables the model to
learn a shared 3D representation of the scene, allowing it to predict future
video sequences from novel viewpoints based solely on the given RGB-D
observations, without requiring camera poses as inputs. Compared to existing
baselines, our method produces more visually stable and spatially aligned
predictions across multiple simulated and real-world robotic datasets. We
further show that the predicted 4D videos can be used to recover robot
end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting
robust robot manipulation and generalization to novel camera viewpoints.

</details>


### [44] [Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions](https://arxiv.org/abs/2507.01123)
*Rahul A. Burange,Harsh K. Shinde,Omkar Mutyalwar*

Main category: cs.CV

TL;DR: 研究结合多源遥感数据和深度学习模型，以增强滑坡的检测和预测能力。


<details>
  <summary>Details</summary>
Motivation: 滑坡对基础设施、经济和生命造成严重威胁，因此需要准确的检测和预测。深度学习和遥感技术的进步为自动化滑坡检测提供了有效途径。

Method: 本研究采用综合方法，利用Sentinel-2多光谱数据和ALOS PALSAR衍生的坡度与DEM数据。使用地理空间分析技术评估地形、植被和降雨对检测精度的影响。同时，评估了U-Net、DeepLabV3+和Res-Net等先进深度学习分割模型的性能。

Result: 该框架有助于开发可靠的早期预警系统、改进灾害风险管理和可持续土地利用规划。研究结果揭示了深度学习和多源遥感在创建稳健、可扩展和可迁移滑坡预测模型方面的巨大潜力。

Conclusion: 结合深度学习和多源遥感数据的综合方法，有望显著提升滑坡检测与预测能力，为灾害管理和土地规划提供有力支持。

Abstract: Landslides pose severe threats to infrastructure, economies, and human lives,
necessitating accurate detection and predictive mapping across diverse
geographic regions. With advancements in deep learning and remote sensing,
automated landslide detection has become increasingly effective. This study
presents a comprehensive approach integrating multi-source satellite imagery
and deep learning models to enhance landslide identification and prediction. We
leverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and
Digital Elevation Model (DEM) layers to capture critical environmental features
influencing landslide occurrences. Various geospatial analysis techniques are
employed to assess the impact of terra in characteristics, vegetation cover,
and rainfall on detection accuracy. Additionally, we evaluate the performance
of multiple stateof-the-art deep learning segmentation models, including U-Net,
DeepLabV3+, and Res-Net, to determine their effectiveness in landslide
detection. The proposed framework contributes to the development of reliable
early warning systems, improved disaster risk management, and sustainable
land-use planning. Our findings provide valuable insights into the potential of
deep learning and multi-source remote sensing in creating robust, scalable, and
transferable landslide prediction models.

</details>


### [45] [cp_measure: API-first feature extraction for image-based profiling workflows](https://arxiv.org/abs/2507.01163)
*Alán F. Muñoz,Tim Treis,Alexandr A. Kalinin,Shatavisha Dasgupta,Fabian Theis,Anne E. Carpenter,Shantanu Singh*

Main category: cs.CV

TL;DR: 本文介绍了cp_measure，一个Python库，旨在解决CellProfiler在自动化和可重复图像分析中的限制，从而为计算生物学中的机器学习提供可扩展的图像表型分析工具。


<details>
  <summary>Details</summary>
Motivation: 现有图像分析工具（如CellProfiler）在进行图像表型分析时，尽管能生成特征集，但在自动化和可重复性方面存在显著障碍，阻碍了机器学习工作流的集成。

Method: 作者开发了cp_measure，这是一个Python库，它将CellProfiler的核心测量能力提取并转化为模块化、API优先的工具，专为程序化特征提取而设计。

Result: cp_measure提取的特征与CellProfiler具有高保真度，并能与科学Python生态系统无缝集成。通过在3D星形胶质细胞成像和空间转录组学中的应用，它展示了实现可重复、自动化且能有效扩展的图像表型分析流程，适用于机器学习应用。

Conclusion: cp_measure为计算生物学中的图像表型分析提供了可重复、自动化和可扩展的解决方案，极大地促进了机器学习工作流的集成，克服了现有工具的局限性。

Abstract: Biological image analysis has traditionally focused on measuring specific
visual properties of interest for cells or other entities. A complementary
paradigm gaining increasing traction is image-based profiling - quantifying
many distinct visual features to form comprehensive profiles which may reveal
hidden patterns in cellular states, drug responses, and disease mechanisms.
While current tools like CellProfiler can generate these feature sets, they
pose significant barriers to automated and reproducible analyses, hindering
machine learning workflows. Here we introduce cp_measure, a Python library that
extracts CellProfiler's core measurement capabilities into a modular, API-first
tool designed for programmatic feature extraction. We demonstrate that
cp_measure features retain high fidelity with CellProfiler features while
enabling seamless integration with the scientific Python ecosystem. Through
applications to 3D astrocyte imaging and spatial transcriptomics, we showcase
how cp_measure enables reproducible, automated image-based profiling pipelines
that scale effectively for machine learning applications in computational
biology.

</details>


### [46] [Rapid Salient Object Detection with Difference Convolutional Neural Networks](https://arxiv.org/abs/2507.01182)
*Zhuo Su,Li Liu,Matthias Müller,Jiehua Zhang,Diana Wofk,Ming-Ming Cheng,Matti Pietikäinen*

Main category: cs.CV

TL;DR: 本文提出SDNet和STDNet，是高效的显著目标检测模型，结合像素差分卷积和再参数化策略，实现在资源受限设备上高准确率与实时性能。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上部署显著目标检测（SOD）面临实时性挑战，现有深度学习模型计算成本高昂。

Method: 提出一种高效网络设计，融合传统SOD方法和现代CNN。引入像素差分卷积（PDCs）编码特征对比度，并将其融入CNN。为提高效率，提出差分卷积再参数化（DCR）策略，在推理时将PDCs嵌入标准卷积。针对视频SOD，引入时空差分卷积（STDC）增强3D卷积以捕获时空对比。

Result: SDNet（图像SOD）和STDNet（视频SOD）在效率-准确率权衡上取得显著提升。在Jetson Orin设备上，模型参数少于1M，图像SOD达到46 FPS，视频SOD达到150 FPS，速度比次优轻量级模型分别快2倍和3倍以上，且精度更优。

Conclusion: 所提出的SDNet和STDNet为资源受限设备上的实时、高精度显著目标检测提供了有效解决方案。

Abstract: This paper addresses the challenge of deploying salient object detection
(SOD) on resource-constrained devices with real-time performance. While recent
advances in deep neural networks have improved SOD, existing top-leading models
are computationally expensive. We propose an efficient network design that
combines traditional wisdom on SOD and the representation power of modern CNNs.
Like biologically-inspired classical SOD methods relying on computing contrast
cues to determine saliency of image regions, our model leverages Pixel
Difference Convolutions (PDCs) to encode the feature contrasts. Differently,
PDCs are incorporated in a CNN architecture so that the valuable contrast cues
are extracted from rich feature maps. For efficiency, we introduce a difference
convolution reparameterization (DCR) strategy that embeds PDCs into standard
convolutions, eliminating computation and parameters at inference.
Additionally, we introduce SpatioTemporal Difference Convolution (STDC) for
video SOD, enhancing the standard 3D convolution with spatiotemporal contrast
capture. Our models, SDNet for image SOD and STDNet for video SOD, achieve
significant improvements in efficiency-accuracy trade-offs. On a Jetson Orin
device, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on
streamed images and videos, surpassing the second-best lightweight models in
our experiments by more than $2\times$ and $3\times$ in speed with superior
accuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.

</details>


### [47] [Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using Hölder Divergence and Mutual Information-Enhanced Knowledge Transfer](https://arxiv.org/abs/2507.01254)
*Runze Cheng,Xihang Qiu,Ming Li,Ye Zhang,Chun Li,Fei Yu*

Main category: cs.CV

TL;DR: 本文提出一个鲁棒的单模态并行处理框架，通过利用Holder散度与互信息，有效解决了多模态MRI脑肿瘤分割中模态缺失的问题，并在BraTS数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的多模态MRI脑肿瘤分割方法在某些模态缺失（如图像质量、协议不一致、患者过敏或财务限制导致）时表现不佳，尽管多模态信息对准确分割至关重要。

Method: 提出一个鲁棒的单模态并行处理框架。该模型利用Holder散度和互信息，在保持模态特有特征的同时，根据可用输入动态调整网络参数。框架使用基于散度和信息的损失函数，有效量化预测与真实标签之间的差异，从而实现准确分割。

Result: 在BraTS 2018和BraTS 2020数据集上进行的广泛评估表明，该框架在处理模态缺失的情况下，性能优于现有方法。

Conclusion: 该提出的框架能够有效处理脑肿瘤分割中MRI模态缺失的问题，并显著提升了分割的准确性和鲁棒性，克服了现有方法的局限性。

Abstract: Multimodal MRI provides critical complementary information for accurate brain
tumor segmentation. However, conventional methods struggle when certain
modalities are missing due to issues such as image quality, protocol
inconsistencies, patient allergies, or financial constraints. To address this,
we propose a robust single-modality parallel processing framework that achieves
high segmentation accuracy even with incomplete modalities. Leveraging Holder
divergence and mutual information, our model maintains modality-specific
features while dynamically adjusting network parameters based on the available
inputs. By using these divergence- and information-based loss functions, the
framework effectively quantifies discrepancies between predictions and
ground-truth labels, resulting in consistently accurate segmentation. Extensive
evaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior
performance over existing methods in handling missing modalities.

</details>


### [48] [AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation](https://arxiv.org/abs/2507.01255)
*Xiao Liu,Jiawei Zhang*

Main category: cs.CV

TL;DR: 该研究提出AIGVE-MACS模型和AIGVE-BENCH 2基准，用于AI生成视频的全面、可解释评估，提供分数和多方面评论，并在评估相关性和视频质量提升方面取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成视频评估指标仅提供数值分数，缺乏解释性且与人类评估对齐度低，无法提供详细反馈。

Method: ['引入AIGVE-MACS，一个统一的AI生成视频评估模型，提供数值分数和多方面语言评论。', '构建AIGVE-BENCH 2，一个包含2500个AI生成视频及22500条人工注释评论和分数的基准数据集。', 'AIGVE-MACS整合了视觉-语言模型(VLM)，并结合了新颖的token级加权损失和动态帧采样策略。', '展示了基于AIGVE-MACS反馈的多智能体优化框架，以迭代改进视频生成质量。']

Result: ['AIGVE-MACS在得分相关性和评论质量方面均达到SOTA性能，显著优于GPT-4o和VideoScore等基线。', '通过多智能体优化框架，视频生成质量提升了53.5%。']

Conclusion: 本工作为AI生成视频的全面、人类对齐评估建立了新范式，并开源了AIGVE-BENCH 2和AIGVE-MACS。

Abstract: The rapid advancement of AI-generated video models has created a pressing
need for robust and interpretable evaluation frameworks. Existing metrics are
limited to producing numerical scores without explanatory comments, resulting
in low interpretability and human evaluation alignment. To address those
challenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video
Evaluation(AIGVE), which can provide not only numerical scores but also
multi-aspect language comment feedback in evaluating these generated videos.
Central to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising
2,500 AI-generated videos and 22,500 human-annotated detailed comments and
numerical scores across nine critical evaluation aspects. Leveraging
AIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a
novel token-wise weighted loss and a dynamic frame sampling strategy to better
align with human evaluators. Comprehensive experiments across supervised and
zero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art
performance in both scoring correlation and comment quality, significantly
outperforming prior baselines including GPT-4o and VideoScore. In addition, we
further showcase a multi-agent refinement framework where feedback from
AIGVE-MACS drives iterative improvements in video generation, leading to 53.5%
quality enhancement. This work establishes a new paradigm for comprehensive,
human-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2
and AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.

</details>


### [49] [Advancements in Weed Mapping: A Systematic Review](https://arxiv.org/abs/2507.01269)
*Mohammad Jahanbakht,Alex Olsen,Ross Marchant,Emilie Fillols,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: 本文旨在弥补杂草测绘领域缺乏全面综述的空白，系统审查了从数据采集到处理和测绘工具的最新方法，以提供该领域的整体理解并指导未来研究。


<details>
  <summary>Details</summary>
Motivation: 尽管杂草测绘在精准农业中至关重要且技术进步显著，但目前缺乏涵盖数据采集、处理技术和测绘工具等整个映射流程的全面且结构化的文献综述，这限制了该领域的发展。

Method: 本综述遵循PRISMA指南，系统地审查并批判性评估和综合了杂草测绘领域最先进的方法，包括数据采集（传感器和平台技术）、数据处理（标注和建模）以及测绘技术（时空分析和决策支持工具）。

Result: 本综述通过批判性评估和综合文献中的关键发现，为杂草测绘领域提供了全面的整体理解。

Conclusion: 本综述可作为指导未来研究、支持开发高效、可扩展和可持续杂草管理系统的基础参考。

Abstract: Weed mapping plays a critical role in precision management by providing
accurate and timely data on weed distribution, enabling targeted control and
reduced herbicide use. This minimizes environmental impacts, supports
sustainable land management, and improves outcomes across agricultural and
natural environments. Recent advances in weed mapping leverage ground-vehicle
Red Green Blue (RGB) cameras, satellite and drone-based remote sensing combined
with sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The
resulting data are processed using advanced techniques including big data
analytics and machine learning, significantly improving the spatial and
temporal resolution of weed maps and enabling site-specific management
decisions. Despite a growing body of research in this domain, there is a lack
of comprehensive literature reviews specifically focused on weed mapping. In
particular, the absence of a structured analysis spanning the entire mapping
pipeline, from data acquisition to processing techniques and mapping tools,
limits progress in the field. This review addresses these gaps by
systematically examining state-of-the-art methods in data acquisition (sensor
and platform technologies), data processing (including annotation and
modelling), and mapping techniques (such as spatiotemporal analysis and
decision support tools). Following PRISMA guidelines, we critically evaluate
and synthesize key findings from the literature to provide a holistic
understanding of the weed mapping landscape. This review serves as a
foundational reference to guide future research and support the development of
efficient, scalable, and sustainable weed management systems.

</details>


### [50] [Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing](https://arxiv.org/abs/2507.01275)
*Chengxu Liu,Lu Qi,Jinshan Pan,Xueming Qian,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 非配对图像去雾，提出一种新颖的基于频域的扩散模型，通过幅度谱重建和相位校正，有效去除图像雾霾，性能超越现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的非配对图像去雾方法引入无关内容，且忽视了雾霾在频域（特别是幅度谱）的特有降质表现。

Method: 提出“\ours”频域扩散模型，将去雾视为频域重建任务。利用扩散模型生成与清晰图像分布一致的幅度谱。设计幅度残差编码器（ARE）提取幅度残差，用于弥补领域差距和监督扩散模型训练；同时引入相位校正模块（PCM）通过注意力机制优化相位谱，消除伪影。

Result: 在合成和真实世界数据集上的实验结果表明，所提方法（\ours）优于其他最先进方法。

Conclusion: 所提出的频域扩散模型(\ours)通过有效利用频域知识和扩散模型，克服了现有非配对去雾方法的不足，在图像去雾任务中取得了卓越的性能。

Abstract: Unpaired image dehazing has attracted increasing attention due to its
flexible data requirements during model training. Dominant methods based on
contrastive learning not only introduce haze-unrelated content information, but
also ignore haze-specific properties in the frequency domain (\ie,~haze-related
degradation is mainly manifested in the amplitude spectrum). To address these
issues, we propose a novel frequency domain-based diffusion model, named \ours,
for fully exploiting the beneficial knowledge in unpaired clear data. In
particular, inspired by the strong generative ability shown by Diffusion Models
(DMs), we tackle the dehazing task from the perspective of frequency domain
reconstruction and perform the DMs to yield the amplitude spectrum consistent
with the distribution of clear images. To implement it, we propose an Amplitude
Residual Encoder (ARE) to extract the amplitude residuals, which effectively
compensates for the amplitude gap from the hazy to clear domains, as well as
provide supervision for the DMs training. In addition, we propose a Phase
Correction Module (PCM) to eliminate artifacts by further refining the phase
spectrum during dehazing with a simple attention mechanism. Experimental
results demonstrate that our \ours outperforms other state-of-the-art methods
on both synthetic and real-world datasets.

</details>


### [51] [Learning an Ensemble Token from Task-driven Priors in Facial Analysis](https://arxiv.org/abs/2507.01290)
*Sunyong Seo,Semin Kim,Jongha Lee*

Main category: cs.CV

TL;DR: 提出ET-Fuser方法，通过学习集成令牌来统一面部分析的特征表示，利用预训练模型的任务先验信息，并在多种面部分析任务中取得显著改进，且计算成本可忽略不计。


<details>
  <summary>Details</summary>
Motivation: 尽管传统方法提高了视觉可解释性，但在训练单一任务时，仍缺乏能够保持统一特征表示的研究。

Method: 引入ET-Fuser方法，通过利用基于预训练模型任务先验的注意力机制来学习“集成令牌”。具体地，该方法提出了一种鲁棒的先验统一学习方法，在自注意力机制中生成集成令牌，该令牌与预训练编码器共享互信息，且具有高效率和可忽略的计算成本。

Result: 实验结果表明，该方法在多种面部分析任务中均显示出改进，尤其在特征表示方面观察到统计学上显著的增强。

Conclusion: ET-Fuser通过高效的集成令牌方法，有效解决了面部分析中统一特征表示的挑战，显著提升了特征表示质量和任务性能。

Abstract: Facial analysis exhibits task-specific feature variations. While
Convolutional Neural Networks (CNNs) have enabled the fine-grained
representation of spatial information, Vision Transformers (ViTs) have
facilitated the representation of semantic information at the patch level.
Although the generalization of conventional methodologies has advanced visual
interpretability, there remains paucity of research that preserves the unified
feature representation on single task learning during the training process. In
this work, we introduce ET-Fuser, a novel methodology for learning ensemble
token by leveraging attention mechanisms based on task priors derived from
pre-trained models for facial analysis. Specifically, we propose a robust prior
unification learning method that generates a ensemble token within a
self-attention mechanism, which shares the mutual information along the
pre-trained encoders. This ensemble token approach offers high efficiency with
negligible computational cost. Our results show improvements across a variety
of facial analysis, with statistically significant enhancements observed in the
feature representations.

</details>


### [52] [DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting](https://arxiv.org/abs/2507.01305)
*Worameth Chinchuthakun,Pakkapon Phongthawee,Amit Raj,Varun Jampani,Pramook Khungurn,Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: 一种基于Stable Diffusion XL的单张LDR图像光照估计技术，通过将任务转化为铬球修复，并引入迭代求中值和LoRA加速策略，实现了高质量且高效的光照估计。


<details>
  <summary>Details</summary>
Motivation: 现有光照估计方法受限于有限的HDR数据集，导致泛化能力不足。本研究旨在利用大型预训练扩散模型克服这一局限，从单张LDR图像中估计高动态范围光照。

Method: 1. **问题重构**: 将单张LDR图像的光照估计重构为铬球图像修复任务。2. **核心模型**: 利用预训练的Stable Diffusion XL扩散模型。3. **DiffusionLight**: 通过迭代修复计算多个输出的铬球中值，作为稳定的低频光照先验来指导生成高质量结果；并微调Exposure LoRA生成多曝光LDR图像以合并为HDR探针。4. **DiffusionLight-Turbo**: 为大幅提升速度（从30分钟缩短至30秒），训练Turbo LoRA直接预测平均铬球，并结合LoRA交换技术将推理简化为单次去噪过程。

Result: 1. 在多样化和“野外”场景中生成了令人信服的光照估计。2. 展现出优于现有方法的泛化能力。3. DiffusionLight-Turbo实现了60倍的加速，同时保持了极小的质量损失。

Conclusion: 本研究成功地将光照估计转化为扩散模型驱动的铬球修复问题，通过迭代优化和高效加速策略，实现了从单张LDR图像中高质量、高效率地估计光照，有效提升了方法的泛化能力。

Abstract: We introduce a simple yet effective technique for estimating lighting from a
single low-dynamic-range (LDR) image by reframing the task as a chrome ball
inpainting problem. This approach leverages a pre-trained diffusion model,
Stable Diffusion XL, to overcome the generalization failures of existing
methods that rely on limited HDR panorama datasets. While conceptually simple,
the task remains challenging because diffusion models often insert incorrect or
inconsistent content and cannot readily generate chrome balls in HDR format.
Our analysis reveals that the inpainting process is highly sensitive to the
initial noise in the diffusion process, occasionally resulting in unrealistic
outputs. To address this, we first introduce DiffusionLight, which uses
iterative inpainting to compute a median chrome ball from multiple outputs to
serve as a stable, low-frequency lighting prior that guides the generation of a
high-quality final result. To generate high-dynamic-range (HDR) light probes,
an Exposure LoRA is fine-tuned to create LDR images at multiple exposure
values, which are then merged. While effective, DiffusionLight is
time-intensive, requiring approximately 30 minutes per estimation. To reduce
this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to
about 30 seconds with minimal quality loss. This 60x speedup is achieved by
training a Turbo LoRA to directly predict the averaged chrome balls from the
iterative process. Inference is further streamlined into a single denoising
pass using a LoRA swapping technique. Experimental results that show our method
produces convincing light estimates across diverse settings and demonstrates
superior generalization to in-the-wild scenarios. Our code is available at
https://diffusionlight.github.io/turbo

</details>


### [53] [Physics-informed Ground Reaction Dynamics from Human Motion Capture](https://arxiv.org/abs/2507.01340)
*Cuong Le,Huy-Phuong Le,Duc Le,Minh-Thien Duong,Van-Binh Nguyen,My-Ha Le*

Main category: cs.CV

TL;DR: 本文提出一种结合物理定律和计算仿真的方法，直接从动作捕捉数据中估计人体地面反作用力动力学，旨在克服传统测力台的局限性，并提升估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过测力台收集人体动力学（如外部反作用力），但测力台作为专业设备仅限于实验室环境，严重限制了人体动力学数据的获取。因此，需要一种不依赖测力台、能直接从动作捕捉数据中估计人体动力学的方法。

Method: 该研究提出一种新颖方法，利用欧拉积分方案和PD算法，结合物理定律和计算仿真作为约束，从动作捕捉数据中高精度、鲁棒地计算地面反作用力。这些基于物理的反作用力用于指导学习模型，以提升人体动力学估计的准确性。

Result: 该方法在GroundLink数据集上进行了测试，结果显示其性能优于基线模型。具体表现在：1) 地面反作用力估计精度高于测力台测量结果；2) 模拟根轨迹精度得到提升。

Conclusion: 该研究提出的物理驱动方法能够有效地、高精度地从动作捕捉数据中估计人体地面反作用力动力学，为克服传统测力台的局限性提供了有效解决方案，并显著提升了估计准确性和轨迹精度。

Abstract: Body dynamics are crucial information for the analysis of human motions in
important research fields, ranging from biomechanics, sports science to
computer vision and graphics. Modern approaches collect the body dynamics,
external reactive force specifically, via force plates, synchronizing with
human motion capture data, and learn to estimate the dynamics from a black-box
deep learning model. Being specialized devices, force plates can only be
installed in laboratory setups, imposing a significant limitation on the
learning of human dynamics. To this end, we propose a novel method for
estimating human ground reaction dynamics directly from the more reliable
motion capture data with physics laws and computational simulation as
constrains. We introduce a highly accurate and robust method for computing
ground reaction forces from motion capture data using Euler's integration
scheme and PD algorithm. The physics-based reactive forces are used to inform
the learning model about the physics-informed motion dynamics thus improving
the estimation accuracy. The proposed approach was tested on the GroundLink
dataset, outperforming the baseline model on: 1) the ground reaction force
estimation accuracy compared to the force plates measurement; and 2) our
simulated root trajectory precision. The implementation code is available at
https://github.com/cuongle1206/Phys-GRD

</details>


### [54] [Learning Camera-Agnostic White-Balance Preferences](https://arxiv.org/abs/2507.01342)
*Luxi Zhao,Mahmoud Afifi,Michael S. Brown*

Main category: cs.CV

TL;DR: 本文提出首个解决跨相机美学一致性自动白平衡（AWB）的方法，通过学习一个轻量级、相机无关的后光照估计映射，将中性校正转换为美学偏好校正，实现对未见相机的统一风格化色彩渲染。


<details>
  <summary>Details</summary>
Motivation: 现代相机AWB模块在美学偏好和准确中性色彩校正之间存在权衡。现有的学习方法难以在不同相机传感器之间泛化，尤其是智能手机多摄像头场景。尽管近期研究探索了跨相机AWB，但大多仍侧重于实现中性白平衡，而非美学一致性。

Method: 本研究学习了一个后光照估计映射，该映射将中性光照校正转换为美学偏好校正，并在相机无关的空间中操作。该映射模块轻量（约500个参数），可应用于任何中性AWB模块之后，以实现一致和风格化的色彩渲染。

Result: 该方法在包含来自三款不同相机的771张智能手机图像数据集上，达到了最先进的性能。模型运行速度极快（在典型旗舰移动CPU上仅需0.024毫秒），且计算和内存开销极小。此外，它与现有跨相机AWB技术完全兼容。

Conclusion: 本研究首次解决了跨相机美学一致性问题，通过提出的轻量级映射模型，实现了对未见相机的一致且风格化的色彩渲染，同时保持高效和良好的兼容性。

Abstract: The image signal processor (ISP) pipeline in modern cameras consists of
several modules that transform raw sensor data into visually pleasing images in
a display color space. Among these, the auto white balance (AWB) module is
essential for compensating for scene illumination. However, commercial AWB
systems often strive to compute aesthetic white-balance preferences rather than
accurate neutral color correction. While learning-based methods have improved
AWB accuracy, they typically struggle to generalize across different camera
sensors -- an issue for smartphones with multiple cameras. Recent work has
explored cross-camera AWB, but most methods remain focused on achieving neutral
white balance. In contrast, this paper is the first to address aesthetic
consistency by learning a post-illuminant-estimation mapping that transforms
neutral illuminant corrections into aesthetically preferred corrections in a
camera-agnostic space. Once trained, our mapping can be applied after any
neutral AWB module to enable consistent and stylized color rendering across
unseen cameras. Our proposed model is lightweight -- containing only $\sim$500
parameters -- and runs in just 0.024 milliseconds on a typical flagship mobile
CPU. Evaluated on a dataset of 771 smartphone images from three different
cameras, our method achieves state-of-the-art performance while remaining fully
compatible with existing cross-camera AWB techniques, introducing minimal
computational and memory overhead.

</details>


### [55] [Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation](https://arxiv.org/abs/2507.01347)
*Andrei Jelea,Ahmed Nabil Belbachir,Marius Leordeanu*

Main category: cs.CV

TL;DR: 引入广义测试时增强（GTTA）方法，通过数据扰动和自监督学习提高模型性能，适用于多种视觉和非视觉任务，并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有测试时增强（TTA）方法通用性不足，且可能增加计算成本。本研究旨在开发一种更通用、高效的TTA方法，以提升已训练模型在各类视觉及非视觉任务上的性能，并降低测试时的计算开销。

Method: GTTA通过对测试输入数据的PCA子空间投影进行多次随机扰动，在测试时构建鲁棒集成，以滤除噪声并降低估计误差。此外，该方法还包含一个自监督学习阶段，利用集成输出作为无监督教师来训练初始学生模型，从而在不损失准确性的前提下显著降低测试时的计算成本。

Result: 在图像分类、分割、语音识别、房价预测等多种视觉和非视觉任务上，GTTA与现有强TTA方法及SOTA模型相比，验证了其通用性和有效性。该方法还被证明在低能见度水下视频中的鲑鱼分割和检测这一真实世界任务上表现出色，并为此引入了DeepSalmon数据集。

Conclusion: GTTA是一种通用且高效的测试时增强技术，能够显著提升已训练模型的性能，并适用于广泛的视觉与非视觉任务。其通过创新的数据变换和自监督学习机制，在保持高准确率的同时有效降低了计算成本，展现了在未来应用中的巨大潜力。

Abstract: We introduce Generalized Test-Time Augmentation (GTTA), a highly effective
method for improving the performance of a trained model, which unlike other
existing Test-Time Augmentation approaches from the literature is general
enough to be used off-the-shelf for many vision and non-vision tasks, such as
classification, regression, image segmentation and object detection. By
applying a new general data transformation, that randomly perturbs multiple
times the PCA subspace projection of a test input, GTTA forms robust ensembles
at test time in which, due to sound statistical properties, the structural and
systematic noises in the initial input data is filtered out and final estimator
errors are reduced. Different from other existing methods, we also propose a
final self-supervised learning stage in which the ensemble output, acting as an
unsupervised teacher, is used to train the initial single student model, thus
reducing significantly the test time computational cost, at no loss in
accuracy. Our tests and comparisons to strong TTA approaches and SoTA models on
various vision and non-vision well-known datasets and tasks, such as image
classification and segmentation, speech recognition and house price prediction,
validate the generality of the proposed GTTA. Furthermore, we also prove its
effectiveness on the more specific real-world task of salmon segmentation and
detection in low-visibility underwater videos, for which we introduce
DeepSalmon, the largest dataset of its kind in the literature.

</details>


### [56] [Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model](https://arxiv.org/abs/2507.01351)
*Chaoxiang Cai,Longrong Yang,Kaibing Chen,Fan Yang,Xi Li*

Main category: cs.CV

TL;DR: 提出长尾分布感知路由器（LTDR），通过识别并解决视觉和语言模态在MoE路由中的分布差异，优化LVLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉-语言模型（LVLMs）中的MoE框架在Token-to-Expert路由（TER）时，过度依赖负载均衡机制，忽视了视觉和语言模态固有的分布差异，导致无法有效利用MoE的优势。

Method: 本文提出长尾分布感知路由器（LTDR），用于视觉-语言的Token-to-Expert路由。主要方法包括：1) 开发模态感知路由器，针对语言TER的均匀分布和视觉TER的长尾分布，设计不同的路由策略。2) 通过引入类似过采样的策略，增加视觉长尾Token激活的专家数量，以增强其专家激活。

Result: 在广泛的基准测试中，验证了所提出方法（LTDR）的有效性。

Conclusion: LTDR通过考虑视觉和语言模态在TER中存在的固有分布差异，并提出针对性的路由和专家激活增强策略，有效提升了大型视觉-语言模型中MoE架构的性能和效率。

Abstract: The mixture-of-experts (MoE), which replaces dense models with sparse
architectures, has gained attention in large vision-language models (LVLMs) for
achieving comparable performance with fewer activated parameters. Existing MoE
frameworks for LVLMs focus on token-to-expert routing (TER), encouraging
different experts to specialize in processing distinct tokens. However, these
frameworks often rely on the load balancing mechanism, overlooking the inherent
distributional differences between vision and language. To this end, we propose
a Long-Tailed Distribution-aware Router (LTDR) for vision-language TER,
tackling two challenges: (1) Distribution-aware router for modality-specific
routing. We observe that language TER follows a uniform distribution, whereas
vision TER exhibits a long-tailed distribution. This discrepancy necessitates
distinct routing strategies tailored to each modality. (2) Enhancing expert
activation for vision tail tokens. Recognizing the importance of vision tail
tokens, we introduce an oversampling-like strategy by increasing the number of
activated experts for these tokens. Experiments on extensive benchmarks
validate the effectiveness of our approach.

</details>


### [57] [3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation](https://arxiv.org/abs/2507.01367)
*Tianrui Lou,Xiaojun Jia,Siyuan Liang,Jiawei Liang,Ming Zhang,Yanjun Xiao,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯溅射的物理对抗攻击框架PGA，通过解决现有方法对网格先验和虚拟环境的依赖以及多视角鲁棒性不足的问题，显著提升了物理对抗伪装的有效性和跨视角鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的物理对抗攻击（特别是伪装攻击）存在不足：1) 依赖目标物体的网格先验和模拟器构建的虚拟环境，耗时且与真实世界存在差异。2) 由于训练图像背景的限制，难以生成多视角鲁棒的对抗性伪装，常陷入次优解，导致在多样化视角和物理环境中对抗效果和鲁棒性不足。

Method: 提出了一种名为PGA的基于3D高斯溅射（3DGS）的物理攻击框架。该框架能够用少量图像进行快速精确的重建并提供逼真的渲染能力。为增强跨视角鲁棒性和对抗有效性，PGA通过防止高斯点之间的相互遮挡和自遮挡，并采用一种最小-最大优化方法，调整每个视角的成像背景，帮助算法过滤掉非鲁棒的对抗特征。

Result: 广泛的实验验证了PGA的有效性和优越性。

Conclusion: PGA框架通过创新的3DGS应用和优化策略，成功克服了传统物理对抗伪装方法在真实世界环境和多视角鲁棒性方面的局限性，显著提升了攻击效果。

Abstract: Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to obtain and inevitably differ from the
real world. Moreover, due to the limitations of the backgrounds in training
images, previous methods often fail to produce multi-view robust adversarial
camouflage and tend to fall into sub-optimal solutions. Due to these reasons,
prior work lacks adversarial effectiveness and robustness across diverse
viewpoints and physical environments. We propose a physical attack framework
based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and
precise reconstruction with few images, along with photo-realistic rendering
capabilities. Our framework further enhances cross-view robustness and
adversarial effectiveness by preventing mutual and self-occlusion among
Gaussians and employing a min-max optimization approach that adjusts the
imaging background of each viewpoint, helping the algorithm filter out
non-robust adversarial features. Extensive experiments validate the
effectiveness and superiority of PGA. Our code is available
at:https://github.com/TRLou/PGA.

</details>


### [58] [Activation Reward Models for Few-Shot Model Alignment](https://arxiv.org/abs/2507.01368)
*Tianning Chai,Chancharik Mitra,Brandon Huang,Gautam Rajendrakumar Gare,Zhiqiu Lin,Assaf Arbelle,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Deva Ramanan,Roei Herzig*

Main category: cs.CV

TL;DR: 提出一种名为激活奖励模型（Activation RMs）的新型少量样本奖励建模方法，旨在高效地将大型语言模型与人类偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型在适应新偏好时面临挑战，因为它们需要大量的偏好数据集且不易快速调整，这限制了模型在现实应用中的质量提升。

Method: 引入激活奖励模型（Activation RMs），通过利用激活引导（activation steering）机制，仅需少量监督，无需额外模型微调，即可构建与人类偏好高度对齐的奖励信号。

Result: 激活奖励模型在标准奖励建模基准上优于现有少量样本方法（如LLM-as-a-judge、基于投票和代币概率评分）。此外，它有效缓解了奖励欺骗行为，并在为测试奖励欺骗而提出的PreferenceHack新基准上实现了最先进的性能，甚至超越了GPT-4o。

Conclusion: 激活奖励模型为将大型语言模型与人类偏好对齐提供了一个高效且强大的少量样本解决方案，尤其适用于安全关键型应用，并通过减轻奖励欺骗风险提高了模型的可靠性。

Abstract: Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to
human preferences is a central challenge in improving the quality of the
models' generative outputs for real-world applications. A common approach is to
use reward modeling to encode preferences, enabling alignment via post-training
using reinforcement learning. However, traditional reward modeling is not
easily adaptable to new preferences because it requires a separate reward
model, commonly trained on large preference datasets. To address this, we
introduce Activation Reward Models (Activation RMs) -- a novel few-shot reward
modeling method that leverages activation steering to construct well-aligned
reward signals using minimal supervision and no additional model finetuning.
Activation RMs outperform existing few-shot reward modeling approaches such as
LLM-as-a-judge with in-context learning, voting-based scoring, and token
probability scoring on standard reward modeling benchmarks. Furthermore, we
demonstrate the effectiveness of Activation RMs in mitigating reward hacking
behaviors, highlighting their utility for safety-critical applications. Toward
this end, we propose PreferenceHack, a novel few-shot setting benchmark, the
first to test reward models on reward hacking in a paired preference format.
Finally, we show that Activation RM achieves state-of-the-art performance on
this benchmark, surpassing even GPT-4o.

</details>


### [59] [Active Measurement: Efficient Estimation at Scale](https://arxiv.org/abs/2507.01372)
*Max Hamilton,Jinlin Lai,Wenlong Zhao,Subhransu Maji,Daniel Sheldon*

Main category: cs.CV

TL;DR: 提出一种名为“主动测量”的人机协作AI框架，旨在提高科学测量的准确性和统计保证，同时减少人力投入。


<details>
  <summary>Details</summary>
Motivation: AI虽有潜力革新科学发现，但当前工作流程在提供科学测量所需的准确性和统计保证方面存在不足。

Method: 引入“主动测量”框架，该框架利用AI模型预测测量值，随后通过重要性采样选择单元进行人工标注。随着新的人工标注数据加入，AI模型会不断改进，并优化无偏蒙特卡洛估计的总测量值。文中还推导了新的估计器、加权方案和置信区间。

Result: “主动测量”即使在AI模型不完善的情况下也能提供精确估计；当AI模型非常准确时，所需人工投入极少；与现有替代方案相比，该方法在多个测量任务中显著降低了估计误差。

Conclusion: “主动测量”是一种有效的人机协作AI框架，能够为科学测量提供精确且有统计保证的估计，显著提高了效率并减少了误差。

Abstract: AI has the potential to transform scientific discovery by analyzing vast
datasets with little human effort. However, current workflows often do not
provide the accuracy or statistical guarantees that are needed. We introduce
active measurement, a human-in-the-loop AI framework for scientific
measurement. An AI model is used to predict measurements for individual units,
which are then sampled for human labeling using importance sampling. With each
new set of human labels, the AI model is improved and an unbiased Monte Carlo
estimate of the total measurement is refined. Active measurement can provide
precise estimates even with an imperfect AI model, and requires little human
effort when the AI model is very accurate. We derive novel estimators,
weighting schemes, and confidence intervals, and show that active measurement
reduces estimation error compared to alternatives in several measurement tasks.

</details>


### [60] [MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing](https://arxiv.org/abs/2507.01384)
*Langyu Wang,Bingke Zhu,Yingying Chen,Yiyuan Zhang,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 本文提出MUG（audio-visual Mamba network with pseudo labeling aUGmentation）模型，用于解决弱监督视听视频解析（AVVP）中段级别和事件级别预测的不足。MUG通过伪标签增强和视听Mamba网络，旨在强调片段独特性并排除跨模态噪声。实验证明，MUG在LLP数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督视听视频解析方法受限于弱监督和模型架构缺陷，难以同时提升段级别和事件级别预测的准确性，尤其是在强调片段独特性和排除跨模态噪声方面存在不足。

Method: 提出MUG模型，包含：1) 伪标签增强，基于先前工作标注伪标签。2) 跨模态随机组合，利用单模态伪标签生成新数据，以增强模型解析多种段级别事件组合的能力。3) 视听Mamba网络（AV-Mamba），用于特征处理和交互，旨在增强对不同片段的感知并排除额外模态噪声，同时共享相似模态信息。

Result: MUG在LLP数据集上显著提升了最先进的性能，在所有指标上均有改进，其中视觉段级别指标提升2.1%，音频段级别指标提升1.2%。

Conclusion: MUG通过引入伪标签增强和视听Mamba网络，有效解决了弱监督视听视频解析中段级别和事件级别预测的挑战，显著提升了模型性能，达到了当前最佳水平。

Abstract: The weakly-supervised audio-visual video parsing (AVVP) aims to predict all
modality-specific events and locate their temporal boundaries. Despite
significant progress, due to the limitations of the weakly-supervised and the
deficiencies of the model architecture, existing methods are lacking in
simultaneously improving both the segment-level prediction and the event-level
prediction. In this work, we propose a audio-visual Mamba network with pseudo
labeling aUGmentation (MUG) for emphasising the uniqueness of each segment and
excluding the noise interference from the alternate modalities. Specifically,
we annotate some of the pseudo-labels based on previous work. Using unimodal
pseudo-labels, we perform cross-modal random combinations to generate new data,
which can enhance the model's ability to parse various segment-level event
combinations. For feature processing and interaction, we employ a audio-visual
mamba network. The AV-Mamba enhances the ability to perceive different segments
and excludes additional modal noise while sharing similar modal information.
Our extensive experiments demonstrate that MUG improves state-of-the-art
results on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of
visual Segment-level and audio Segment-level metrics). Our code is available at
https://github.com/WangLY136/MUG.

</details>


### [61] [FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases](https://arxiv.org/abs/2507.01390)
*Shuai Tan,Bill Gong,Bin Ji,Ye Pan*

Main category: cs.CV

TL;DR: FixTalk通过提出增强运动指示器（EMI）和增强细节指示器（EDI），有效解决了说话人头像生成中存在的身份泄露和渲染伪影问题，性能优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有说话人头像生成方法在高质量渲染方面常面临身份泄露（IL）和渲染伪影（RA）问题，尤其在极端情况下表现更差。

Method: 本文提出FixTalk框架。它包含：1) 增强运动指示器（EMI），用于将身份信息与运动特征解耦，以减轻身份泄露；2) 增强细节指示器（EDI），利用泄露的身份信息补充缺失细节，以修复渲染伪影。

Result: 广泛实验证明，FixTalk能有效缓解身份泄露和渲染伪影，并取得了优于现有最先进方法的性能。

Conclusion: FixTalk为高质量说话人头像生成提供了一种新颖有效的解决方案，能同时解决身份泄露和渲染伪影两大挑战。

Abstract: Talking head generation is gaining significant importance across various
domains, with a growing demand for high-quality rendering. However, existing
methods often suffer from identity leakage (IL) and rendering artifacts (RA),
particularly in extreme cases. Through an in-depth analysis of previous
approaches, we identify two key insights: (1) IL arises from identity
information embedded within motion features, and (2) this identity information
can be leveraged to address RA. Building on these findings, this paper
introduces FixTalk, a novel framework designed to simultaneously resolve both
issues for high-quality talking head generation. Firstly, we propose an
Enhanced Motion Indicator (EMI) to effectively decouple identity information
from motion features, mitigating the impact of IL on generated talking heads.
To address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes
the leaked identity information to supplement missing details, thus fixing the
artifacts. Extensive experiments demonstrate that FixTalk effectively mitigates
IL and RA, achieving superior performance compared to state-of-the-art methods.

</details>


### [62] [Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps](https://arxiv.org/abs/2507.01397)
*Khanh Son Pham,Christian Witte,Jens Behley,Johannes Betz,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 本文提出一种利用标准清晰度（SD）地图先验信息和时序一致性的方法，用于在线预测车道线、拓扑和道路边界，显著提升了高清地图构建性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车高度依赖高清（HD）地图。然而，从车载传感器在线连贯构建HD地图仍具挑战性，尤其在于如何统一且一致地建模复杂道路拓扑结构。

Method: 提出一种连贯的方法，利用SD地图的先验信息，在线预测车道线段、其拓扑结构以及道路边界。该方法设计了一种网络架构，采用结合先验信息和去噪技术的混合车道线段编码，以增强训练稳定性和性能，并利用历史帧来确保时序一致性。

Result: 实验评估表明，所提出的方法在性能上显著优于现有方法。

Conclusion: 该研究方法有效解决了高清地图在线构建中道路拓扑建模的复杂性挑战，并证明了其建模方案的优越性。

Abstract: Most autonomous cars rely on the availability of high-definition (HD) maps.
Current research aims to address this constraint by directly predicting HD map
elements from onboard sensors and reasoning about the relationships between the
predicted map and traffic elements. Despite recent advancements, the coherent
online construction of HD maps remains a challenging endeavor, as it
necessitates modeling the high complexity of road topologies in a unified and
consistent manner. To address this challenge, we propose a coherent approach to
predict lane segments and their corresponding topology, as well as road
boundaries, all by leveraging prior map information represented by commonly
available standard-definition (SD) maps. We propose a network architecture,
which leverages hybrid lane segment encodings comprising prior information and
denoising techniques to enhance training stability and performance.
Furthermore, we facilitate past frames for temporal consistency. Our
experimental evaluation demonstrates that our approach outperforms previous
methods by a large margin, highlighting the benefits of our modeling scheme.

</details>


### [63] [Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound](https://arxiv.org/abs/2507.01401)
*Huanwen Liang,Jingxian Xu,Yuanji Zhang,Yuhao Huang,Yuhan Zhang,Xin Yang,Ran Li,Xuedong Deng,Yanjun Liu,Guowei Tao,Yun Wu,Sheng Zhao,Xinru Gao,Dong Ni*

Main category: cs.CV

TL;DR: 本文提出了一种无需标准平面定位的、基于多实例学习（MIL）的病例级方法，用于产前超声胎儿腹部畸形分类，并在大型数据集上超越了现有最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 胎儿腹部畸形诊断对妊娠管理至关重要，但现有AI方法在产前腹部异常诊断方面应用有限，多数局限于图像级分类且依赖标准平面定位，缺乏对病例级诊断的关注。

Method: 开发了一种基于多实例学习（MIL）的病例级方法，无需标准平面定位，用于分类产前超声中的胎儿腹部异常。该方法包含三部分创新：1) 采用注意力专家混合模块（MoAE）为不同平面加权不同的注意力头；2) 提出医学知识驱动的特征选择模块（MFS），在病例级进行自监督图像token选择，以使图像特征与医学知识对齐；3) 引入基于提示的原型学习（PPL）来增强MFS。

Result: 在包含2,419个病例、24,748张图像和6个类别的胎儿腹部超声数据集上进行了广泛验证，所提出的方法优于现有最先进的竞争方法。

Conclusion: 所提出的基于MIL的病例级方法，通过整合MoAE、MFS和PPL，在无需标准平面定位的情况下，有效提升了产前超声胎儿腹部畸形的自动诊断准确性，表现优于现有技术，具有重要的临床应用潜力。

Abstract: Fetal abdominal malformations are serious congenital anomalies that require
accurate diagnosis to guide pregnancy management and reduce mortality. Although
AI has demonstrated significant potential in medical diagnosis, its application
to prenatal abdominal anomalies remains limited. Most existing studies focus on
image-level classification and rely on standard plane localization, placing
less emphasis on case-level diagnosis. In this paper, we develop a case-level
multiple instance learning (MIL)-based method, free of standard plane
localization, for classifying fetal abdominal anomalies in prenatal ultrasound.
Our contribution is three-fold. First, we adopt a mixture-of-attention-experts
module (MoAE) to weight different attention heads for various planes. Secondly,
we propose a medical-knowledge-driven feature selection module (MFS) to align
image features with medical knowledge, performing self-supervised image token
selection at the case-level. Finally, we propose a prompt-based prototype
learning (PPL) to enhance the MFS. Extensively validated on a large prenatal
abdominal ultrasound dataset containing 2,419 cases, with a total of 24,748
images and 6 categories, our proposed method outperforms the state-of-the-art
competitors. Codes are available at:https://github.com/LL-AC/AAcls.

</details>


### [64] [CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning](https://arxiv.org/abs/2507.01409)
*Kuniaki Saito,Donghyun Kim,Kwanyong Park,Atsushi Hashimoto,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: 该研究提出CaptionSmiths模型，通过无人工标注量化和插值方法，实现对图像描述的长度、描述性和词语独特性等属性的精细且平滑控制，显著提升控制精度和词汇对齐度。


<details>
  <summary>Details</summary>
Motivation: 现有生成式视觉-语言模型难以对图像描述进行精细控制，因其训练时未将属性作为条件输入，且无法平滑转换语言模式，限制了其在多样化应用中的灵活性。

Method: 提出CaptionSmiths方法。该方法首先无需人工标注，将每段描述的长度、描述性和词语独特性量化为连续标量值。然后，通过在对应极端状态的两个端点向量之间进行插值来表示条件，从而实现对语言模式的灵活控制。

Result: 经验结果表明，CaptionSmiths模型能平滑地改变输出描述的属性，并显示出比基线模型更高的词汇对齐度。例如，在保持更好词汇对齐的同时，将控制描述长度的误差降低了506%。

Conclusion: CaptionSmiths成功构建了一个能够处理多样化语言模式的单一图像描述模型，实现了对描述属性的精细、平滑控制，并在控制精度和词汇对齐方面表现优异。

Abstract: An image captioning model flexibly switching its language pattern, e.g.,
descriptiveness and length, should be useful since it can be applied to diverse
applications. However, despite the dramatic improvement in generative
vision-language models, fine-grained control over the properties of generated
captions is not easy due to two reasons: (i) existing models are not given the
properties as a condition during training and (ii) existing models cannot
smoothly transition its language pattern from one state to the other. Given
this challenge, we propose a new approach, CaptionSmiths, to acquire a single
captioning model that can handle diverse language patterns. First, our approach
quantifies three properties of each caption, length, descriptiveness, and
uniqueness of a word, as continuous scalar values, without human annotation.
Given the values, we represent the conditioning via interpolation between two
endpoint vectors corresponding to the extreme states, e.g., one for a very
short caption and one for a very long caption. Empirical results demonstrate
that the resulting model can smoothly change the properties of the output
captions and show higher lexical alignment than baselines. For instance,
CaptionSmiths reduces the error in controlling caption length by 506\% despite
better lexical alignment. Code will be available on
https://github.com/omron-sinicx/captionsmiths.

</details>


### [65] [Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention](https://arxiv.org/abs/2507.01417)
*Jiawei Gu,Ziyue Qiao,Zechao Li*

Main category: cs.CV

TL;DR: 该论文提出一种基于梯度的OOD检测方法，通过短路特征坐标来抑制OOD样本的虚假置信度，并利用一阶近似提高效率。


<details>
  <summary>Details</summary>
Motivation: 研究者观察到，在推理阶段，ID样本的局部梯度方向趋于一致，而OOD样本的梯度方向则杂乱无章或相互冲突，导致OOD样本被虚假地高置信度预测。

Method: 提出一种推理阶段技术，通过“短路”那些被虚假梯度利用来提高OOD置信度的特征坐标，同时保持ID分类基本不变。为避免二次前向传播的开销，还引入了局部一阶近似来精确捕捉修改后的输出。

Result: 在标准OOD基准测试中，该方法取得了显著的性能提升。

Conclusion: 该方法轻量且对标准推理流程改动极小，为实际应用中鲁棒的OOD检测提供了一条实用路径。

Abstract: Out-of-Distribution (OOD) detection is critical for safely deploying deep
models in open-world environments, where inputs may lie outside the training
distribution. During inference on a model trained exclusively with
In-Distribution (ID) data, we observe a salient gradient phenomenon: around an
ID sample, the local gradient directions for "enhancing" that sample's
predicted class remain relatively consistent, whereas OOD samples--unseen in
training--exhibit disorganized or conflicting gradient directions in the same
neighborhood. Motivated by this observation, we propose an inference-stage
technique to short-circuit those feature coordinates that spurious gradients
exploit to inflate OOD confidence, while leaving ID classification largely
intact. To circumvent the expense of recomputing the logits after this gradient
short-circuit, we further introduce a local first-order approximation that
accurately captures the post-modification outputs without a second forward
pass. Experiments on standard OOD benchmarks show our approach yields
substantial improvements. Moreover, the method is lightweight and requires
minimal changes to the standard inference pipeline, offering a practical path
toward robust OOD detection in real-world applications.

</details>


### [66] [DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal](https://arxiv.org/abs/2507.01422)
*Wenjie Liu,Bingshu Wang,Ze Wang,C. L. Philip Chen*

Main category: cs.CV

TL;DR: 提出DocShaDiffusion，一个基于潜在空间扩散模型的文档阴影去除方法，通过特定模块处理彩色阴影，并在新合成的彩色阴影数据集上验证了其优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有文档阴影去除方法主要针对恒定背景色阴影，忽略了复杂的彩色阴影去除问题。

Method: 1. 设计了基于潜在空间的DocShaDiffusion扩散模型。
2. 提出了阴影软掩码生成模块（SSGM）以生成精确阴影掩码并对阴影区域添加噪声。
3. 提出了阴影掩码感知引导扩散模块（SMGDM），通过监督扩散和去噪过程来去除阴影。
4. 引入了阴影鲁棒感知特征损失以保留图像细节和结构。
5. 构建了一个大规模合成文档彩色阴影去除数据集（SDCSRD）。

Result: 在三个公开数据集上的实验结果表明，所提方法优于现有最先进（SOTA）方法。

Conclusion: DocShaDiffusion框架及其创新模块（如SSGM、SMGDM）和新损失函数有效解决了文档彩色阴影去除的挑战，并通过新数据集支持了模型的训练，实现了卓越的性能。

Abstract: Document shadow removal is a crucial task in the field of document image
enhancement. However, existing methods tend to remove shadows with constant
color background and ignore color shadows. In this paper, we first design a
diffusion model in latent space for document image shadow removal, called
DocShaDiffusion. It translates shadow images from pixel space to latent space,
enabling the model to more easily capture essential features. To address the
issue of color shadows, we design a shadow soft-mask generation module (SSGM).
It is able to produce accurate shadow mask and add noise into shadow regions
specially. Guided by the shadow mask, a shadow mask-aware guided diffusion
module (SMGDM) is proposed to remove shadows from document images by
supervising the diffusion and denoising process. We also propose a
shadow-robust perceptual feature loss to preserve details and structures in
document images. Moreover, we develop a large-scale synthetic document color
shadow removal dataset (SDCSRD). It simulates the distribution of realistic
color shadows and provides powerful supports for the training of models.
Experiments on three public datasets validate the proposed method's superiority
over state-of-the-art. Our code and dataset will be publicly available.

</details>


### [67] [DiffMark: Diffusion-based Robust Watermark Against Deepfakes](https://arxiv.org/abs/2507.01428)
*Chen Sun,Haiyang Sun,Zhiqing Guo,Yunfeng Diao,Liejun Wang,Dan Ma,Gaobo Yang,Keqin Li*

Main category: cs.CV

TL;DR: 本文提出一种基于扩散模型的新型鲁棒水印框架DiffMark，旨在有效抵抗Deepfake操纵，通过改进训练和采样方案，将水印与图像无缝融合。


<details>
  <summary>Details</summary>
Motivation: Deepfake恶意面部篡改对安全和隐私构成重大威胁。现有水印方法在对抗Deepfake操纵时缺乏足够的鲁棒性，无法有效进行真实性验证和溯源。

Method: 本文提出DiffMark框架，通过修改扩散模型的训练和采样方案，以人脸图像和水印作为条件来引导去噪过程。具体方法包括：构建时间步依赖的面部条件权重，引入跨信息融合(CIF)模块通过可学习嵌入表和交叉注意力融合水印特征，以及在训练阶段集成冻结的自编码器模拟Deepfake操纵，并引入Deepfake抵抗性指导来对抗性引导采样过程以增强水印鲁棒性。

Result: 实验结果表明，所提出的DiffMark在应对典型Deepfake攻击时表现出有效性。

Conclusion: DiffMark框架为抵抗Deepfake操纵提供了一种有效且鲁棒的水印解决方案，通过创新的条件扩散和鲁棒性增强策略实现了水印与图像的深度融合。

Abstract: Deepfakes pose significant security and privacy threats through malicious
facial manipulations. While robust watermarking can aid in authenticity
verification and source tracking, existing methods often lack the sufficient
robustness against Deepfake manipulations. Diffusion models have demonstrated
remarkable performance in image generation, enabling the seamless fusion of
watermark with image during generation. In this study, we propose a novel
robust watermarking framework based on diffusion model, called DiffMark. By
modifying the training and sampling scheme, we take the facial image and
watermark as conditions to guide the diffusion model to progressively denoise
and generate corresponding watermarked image. In the construction of facial
condition, we weight the facial image by a timestep-dependent factor that
gradually reduces the guidance intensity with the decrease of noise, thus
better adapting to the sampling process of diffusion model. To achieve the
fusion of watermark condition, we introduce a cross information fusion (CIF)
module that leverages a learnable embedding table to adaptively extract
watermark features and integrates them with image features via cross-attention.
To enhance the robustness of the watermark against Deepfake manipulations, we
integrate a frozen autoencoder during training phase to simulate Deepfake
manipulations. Additionally, we introduce Deepfake-resistant guidance that
employs specific Deepfake model to adversarially guide the diffusion sampling
process to generate more robust watermarked images. Experimental results
demonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.
Our code will be available at https://github.com/vpsg-research/DiffMark.

</details>


### [68] [TurboReg: TurboClique for Robust and Efficient Point Cloud Registration](https://arxiv.org/abs/2507.01439)
*Shaocheng Yan,Pengcheng Shi,Zhenjun Zhao,Kaixin Wang,Kuang Cao,Ji Wu,Jiayuan Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为TurboReg的快速鲁棒点云配准估计器，它通过轻量级TurboClique和高效的Pivot-Guided Search (PGS)算法解决了现有最大团搜索方法时间复杂度高的问题，实现了速度和召回率的显著提升。


<details>
  <summary>Details</summary>
Motivation: 在基于对应关系的点云配准中，鲁棒估计至关重要。现有使用兼容图中最大团搜索的方法虽然召回率高，但其指数级时间复杂度限制了在时间敏感应用中的使用。

Method: 提出TurboReg，它包含两个核心组件：1) **TurboClique**：一个高度受约束兼容图中的3团，轻量且支持并行搜索，确保鲁棒空间一致性。2) **Pivot-Guided Search (PGS)**：通过选择高SC$^2$分数的匹配对作为枢轴，有效引导搜索高内点率的TurboCliques。PGS算法具有线性时间复杂度，远超指数级时间复杂度的最大团搜索。

Result: 实验证明，TurboReg在多个真实世界数据集上达到最先进性能，并有显著的速度提升。例如，在3DMatch+FCGF数据集上，TurboReg (1K) 比3DMAC快208.22倍，同时召回率更高。

Conclusion: TurboReg通过创新的轻量级团结构和高效并行化搜索算法，成功解决了点云配准中鲁棒估计的时间复杂度问题，在保持或提升性能的同时，实现了大幅度速度优化，使其适用于时间敏感应用。

Abstract: Robust estimation is essential in correspondence-based Point Cloud
Registration (PCR). Existing methods using maximal clique search in
compatibility graphs achieve high recall but suffer from exponential time
complexity, limiting their use in time-sensitive applications. To address this
challenge, we propose a fast and robust estimator, TurboReg, built upon a novel
lightweight clique, TurboClique, and a highly parallelizable Pivot-Guided
Search (PGS) algorithm. First, we define the TurboClique as a 3-clique within a
highly-constrained compatibility graph. The lightweight nature of the 3-clique
allows for efficient parallel searching, and the highly-constrained
compatibility graph ensures robust spatial consistency for stable
transformation estimation. Next, PGS selects matching pairs with high SC$^2$
scores as pivots, effectively guiding the search toward TurboCliques with
higher inlier ratios. Moreover, the PGS algorithm has linear time complexity
and is significantly more efficient than the maximal clique search with
exponential time complexity. Extensive experiments show that TurboReg achieves
state-of-the-art performance across multiple real-world datasets, with
substantial speed improvements. For example, on the 3DMatch+FCGF dataset,
TurboReg (1K) operates $208.22\times$ faster than 3DMAC while also achieving
higher recall. Our code is accessible at
\href{https://github.com/Laka-3DV/TurboReg}{\texttt{TurboReg}}.

</details>


### [69] [OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes](https://arxiv.org/abs/2507.01455)
*Yuxing Liu,Ji Zhang,Zhou Xuchuan,Jingzhong Xiao,Huimin Yang,Jiaxin Zhong*

Main category: cs.CV

TL;DR: OoDDINO是一个新的多级异常分割框架，通过结合不确定性引导检测和自适应双阈值网络，解决了现有方法的碎片化分割和全局阈值问题，显著提升了异常分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有像素级异常分割方法存在两大挑战：1) 忽略像素间的空间相关性，导致分割碎片化；2) 异常分数分布在不同图像区域间差异大，使得全局阈值易导致背景误报或异常目标漏分割。

Method: 本文提出OoDDINO，一个多级异常分割框架，采用粗到细的策略。它包含两阶段级联架构：首先，引入正交不确定性感知融合策略（OUAFS），整合多种不确定性度量与视觉表示，用于准确局部化异常区域；其次，开发自适应双阈值网络（ADT-Net），根据对象级检测输出和像素级异常分数动态生成区域特定阈值，实现前景和背景的精细分割。该框架还可作为插件兼容其他像素级异常检测模型。

Result: 在两个基准数据集上进行了大量实验，验证了所提出框架的优越性和兼容性，性能超越了现有最先进方法。

Conclusion: OoDDINO框架通过其创新的多级检测策略和自适应阈值机制，有效解决了现有异常分割方法的局限性，提供了更精确、更鲁棒的异常分割解决方案，并具有良好的通用性。

Abstract: Anomaly segmentation aims to identify Out-of-Distribution (OoD) anomalous
objects within images. Existing pixel-wise methods typically assign anomaly
scores individually and employ a global thresholding strategy to segment
anomalies. Despite their effectiveness, these approaches encounter significant
challenges in real-world applications: (1) neglecting spatial correlations
among pixels within the same object, resulting in fragmented segmentation; (2)
variabil ity in anomaly score distributions across image regions, causing
global thresholds to either generate false positives in background areas or
miss segments of anomalous objects. In this work, we introduce OoDDINO, a novel
multi-level anomaly segmentation framework designed to address these
limitations through a coarse-to-fine anomaly detection strategy. OoDDINO
combines an uncertainty-guided anomaly detection model with a pixel-level
segmentation model within a two-stage cascade architecture. Initially, we
propose an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) that
sequentially integrates multiple uncertainty metrics with visual
representations, employing orthogonal constraints to strengthen the detection
model's capacity for localizing anomalous regions accurately. Subsequently, we
develop an Adaptive Dual-Threshold Network (ADT-Net), which dynamically
generates region-specific thresholds based on object-level detection outputs
and pixel-wise anomaly scores. This approach allows for distinct thresholding
strategies within foreground and background areas, achieving fine-grained
anomaly segmentation. The proposed framework is compatible with other
pixel-wise anomaly detection models, which acts as a plug-in to boost the
performance. Extensive experiments on two benchmark datasets validate our
framework's superiority and compatibility over state-of-the-art methods.

</details>


### [70] [NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation](https://arxiv.org/abs/2507.01463)
*Max Gandyra,Alessandro Santonicola,Michael Beetz*

Main category: cs.CV

TL;DR: 本文提出NOCTIS框架，通过结合Grounded-SAM 2和DINOv2，并引入基于循环阈值的匹配机制，实现了对新颖物体实例的高效零样本分割，无需再训练即可超越现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 在给定少量示例图像的情况下，从RGB图像中分割新颖物体实例是一个计算机视觉领域的难题。设计一个足够通用、无需（再）训练即可应用于各种新颖物体的模型，已被证明是一项艰巨的任务。因此，需要一个简单而强大的框架来解决此问题。

Method: 本文提出了名为NOCTIS（Novel Object Cyclic Threshold based Instance Segmentation）的框架。它利用了Grounded-SAM 2来获取带有精确边界框和分割掩码的物体提案，同时DINOv2的零样本能力被用于生成图像嵌入。关键创新在于物体匹配分数：该分数基于类别嵌入的相似性和补丁嵌入的平均最大相似性。与SAM-6D不同，计算补丁嵌入相似性时，会根据每个补丁及其在图像网格中的循环/往返补丁之间的距离进行先验补丁过滤。此外，提案的边界框和掩码的平均置信度被用作物体匹配分数的额外权重因子。

Result: 实验结果表明，NOCTIS在不进行额外训练或微调的情况下，在BOP 2023挑战赛的七个核心数据集上，针对“基于模型的未见物体2D分割”任务，优于最佳的RGB和RGB-D方法。

Conclusion: NOCTIS是一个简单而强大的框架，有效地解决了新颖物体实例分割的挑战，无需训练即可实现卓越性能，并在BOP 2023挑战赛中为未见物体2D分割任务树立了新的标杆。

Abstract: Instance segmentation of novel objects instances in RGB images, given some
example images for each object, is a well known problem in computer vision.
Designing a model general enough to be employed, for all kinds of novel
objects, without (re-) training, has proven to be a difficult task. To handle
this, we propose a simple, yet powerful, framework, called: Novel Object Cyclic
Threshold based Instance Segmentation (NOCTIS). This work stems from and
improves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also
leverages on recent vision foundation models, namely: Grounded-SAM 2 and
DINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise
bounding boxes and their corresponding segmentation masks; while DINOv2's
zero-shot capabilities are employed to generate the image embeddings. The
quality of those masks, together with their embeddings, is of vital importance
to our approach; as the proposal-object matching is realized by determining an
object matching score based on the similarity of the class embeddings and the
average maximum similarity of the patch embeddings. Differently to SAM-6D,
calculating the latter involves a prior patch filtering based on the distance
between each patch and its corresponding cyclic/roundtrip patch in the image
grid. Furthermore, the average confidence of the proposals' bounding box and
mask is used as an additional weighting factor for the object matching score.
We empirically show that NOCTIS, without further training/fine tuning,
outperforms the best RGB and RGB-D methods on the seven core datasets of the
BOP 2023 challenge for the "Model-based 2D segmentation of unseen objects"
task.

</details>


### [71] [Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think](https://arxiv.org/abs/2507.01467)
*Ge Wu,Shen Zhang,Ruijing Shi,Shanghua Gao,Zhenyuan Chen,Lei Wang,Zhaowei Chen,Hongcheng Gao,Yao Tang,Jian Yang,Ming-Ming Cheng,Xiang Li*

Main category: cs.CV

TL;DR: 本文提出REG（Representation Entanglement for Generation）方法，通过在扩散模型的去噪过程中将低级图像潜在表示与高级类别语义信息纠缠，显著提升了生成质量和训练效率，同时保持了极低的推理开销。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型训练方法（如REPA及其变体）虽然利用预训练模型的外部视觉表示来缓解训练挑战，但其外部对齐在去噪推理过程中缺失，未能充分利用判别性表示的潜力。

Method: REG方法将低级图像潜在表示与来自预训练基础模型的一个高级类别token在去噪过程中进行纠缠。推理时，模型并发重建图像潜在表示及其对应的全局语义，所获取的语义知识主动引导和增强图像生成。该方法仅增加一个额外token，推理开销可忽略不计（FLOPs和延迟增加<0.5%）。

Result: REG能够直接从纯噪声生成连贯的图像-类别对，大幅提高生成质量和训练效率。在ImageNet 256x256数据集上，SiT-XL/2 + REG的训练速度比SiT-XL/2快63倍，比SiT-XL/2 + REPA快23倍。更突出的是，SiT-L/2 + REG仅训练40万次迭代，便超越了训练400万次迭代的SiT-XL/2 + REPA（训练时长为其10倍）。

Conclusion: REG通过在去噪过程中深度整合图像潜在表示和全局语义信息，有效克服了现有方法的局限性，实现了扩散模型训练的显著加速和生成质量的提升，同时保持了极低的推理成本，为高效生成提供了强大的新范式。

Abstract: REPA and its variants effectively mitigate training challenges in diffusion
models by incorporating external visual representations from pretrained models,
through alignment between the noisy hidden projections of denoising networks
and foundational clean image representations. We argue that the external
alignment, which is absent during the entire denoising inference process, falls
short of fully harnessing the potential of discriminative representations. In
this work, we propose a straightforward method called Representation
Entanglement for Generation (REG), which entangles low-level image latents with
a single high-level class token from pretrained foundation models for
denoising. REG acquires the capability to produce coherent image-class pairs
directly from pure noise, substantially improving both generation quality and
training efficiency. This is accomplished with negligible additional inference
overhead, requiring only one single additional token for denoising (<0.5\%
increase in FLOPs and latency). The inference process concurrently reconstructs
both image latents and their corresponding global semantics, where the acquired
semantic knowledge actively guides and enhances the image generation process.
On ImageNet 256$\times$256, SiT-XL/2 + REG demonstrates remarkable convergence
acceleration, achieving $\textbf{63}\times$ and $\textbf{23}\times$ faster
training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,
SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA
trained for 4M iterations ($\textbf{10}\times$ longer). Code is available at:
https://github.com/Martinser/REG.

</details>


### [72] [Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware](https://arxiv.org/abs/2507.01472)
*Jonáš Herec,Vít Růžička,Rado Pitoňák*

Main category: cs.CV

TL;DR: 针对卫星高光谱甲烷泄漏早期检测中船载算法计算量大的问题，本文提出并评估了多种高效、低功耗的甲烷检测算法（如Mag1c-SAS、CEM），并集成了机器学习模型，实现了显著加速，为低硬件要求的船载部署奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 甲烷是强效温室气体，早期检测其泄漏对气候缓解至关重要。然而，现有任务常采用手动调度，易错过潜在事件，且数据下行速度慢。为克服这些问题并实现成本效益高的及时检测，船载检测是可行方案。但传统甲烷增强方法计算量大，不适用于资源受限的船载硬件。

Method: 本文测试了之前未用于甲烷检测的快速目标检测方法（ACE、CEM），并提出了一种显著快于现有最先进算法Mag1c的变体Mag1c-SAS。为探索其真实检测潜力，将这些方法与机器学习模型（U-Net、LinkNet）相结合。此外，还提出并评估了三种波段选择策略。

Result: 研究结果确定了Mag1c-SAS和CEM两个有前景的候选算法，它们均能准确检测强甲烷羽流且计算效率高，足以满足船载部署要求。它们在资源受限硬件上，分别比原始Mag1c快约100倍（优化精度）和230倍（优化速度）。此外，所提出的波段选择策略之一，在使用更少通道的情况下，表现优于传统方法，从而在不牺牲精度的情况下进一步提高了处理速度。

Conclusion: 本研究为未来在低硬件要求下实现船载甲烷检测、提高数据及时性奠定了基础。研究产生的代码、数据和模型已开源。

Abstract: Methane is a potent greenhouse gas, and detecting its leaks early via
hyperspectral satellite imagery can help mitigate climate change. Meanwhile,
many existing missions operate in manual tasking regimes only, thus missing
potential events of interest. To overcome slow downlink rates cost-effectively,
onboard detection is a viable solution. However, traditional methane
enhancement methods are too computationally demanding for resource-limited
onboard hardware. This work accelerates methane detection by focusing on
efficient, low-power algorithms. We test fast target detection methods (ACE,
CEM) that have not been previously used for methane detection and propose a
Mag1c-SAS - a significantly faster variant of the current state-of-the-art
algorithm for methane detection: Mag1c. To explore their true detection
potential, we integrate them with a machine learning model (U-Net, LinkNet).
Our results identify two promising candidates (Mag1c-SAS and CEM), both
acceptably accurate for the detection of strong plumes and computationally
efficient enough for onboard deployment: one optimized more for accuracy, the
other more for speed, achieving up to ~100x and ~230x faster computation than
original Mag1c on resource-limited hardware. Additionally, we propose and
evaluate three band selection strategies. One of them can outperform the method
traditionally used in the field while using fewer channels, leading to even
faster processing without compromising accuracy. This research lays the
foundation for future advancements in onboard methane detection with minimal
hardware requirements, improving timely data delivery. The produced code, data,
and models are open-sourced and can be accessed from
https://github.com/zaitra/methane-filters-benchmark.

</details>


### [73] [Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects](https://arxiv.org/abs/2507.01478)
*Chentao Shen,Ding Pan,Mingyu Mei,Zaixing He,Xinyue Zhao*

Main category: cs.CV

TL;DR: 针对工业金属物体在真实环境下因反射导致的位姿跟踪难题，提出了一种基于主动控制点的新型6自由度位姿跟踪方法，并在数据集和实际应用中均表现出有效性。


<details>
  <summary>Details</summary>
Motivation: 工业环境中对金属物体的位姿跟踪至关重要，但由于金属物体固有的反射特性，特别是在真实世界场景中，其位姿跟踪仍是一个具有挑战性的任务。

Method: 提出一种基于主动控制点的新颖6自由度位姿跟踪方法。该方法通过图像控制点主动生成边缘特征用于优化，并将其作为优化变量，而非依赖于6自由度位姿渲染。同时，引入了最优控制点回归方法以提高跟踪的鲁棒性。

Result: 所提出的跟踪方法在数据集评估和真实世界任务中均表现出高效和有效性。

Conclusion: 本研究为工业金属物体的实时位姿跟踪提供了一个可行的解决方案。

Abstract: Visual pose tracking is playing an increasingly vital role in industrial
contexts in recent years. However, the pose tracking for industrial metal
objects remains a challenging task especially in the real world-environments,
due to the reflection characteristic of metal objects. To address this issue,
we propose a novel 6DoF pose tracking method based on active control points.
The method uses image control points to generate edge feature for optimization
actively instead of 6DoF pose-based rendering, and serve them as optimization
variables. We also introduce an optimal control point regression method to
improve robustness. The proposed tracking method performs effectively in both
dataset evaluation and real world tasks, providing a viable solution for
real-time tracking of industrial metal objects. Our source code is made
publicly available at: https://github.com/tomatoma00/ACPTracking.

</details>


### [74] [What Really Matters for Robust Multi-Sensor HD Map Construction?](https://arxiv.org/abs/2507.01484)
*Xiaoshuai Hao,Yuting Zhao,Yuheng Ji,Luanyuan Dai,Peng Hao,Dingzhe Li,Shuai Cheng,Rong Yin*

Main category: cs.CV

TL;DR: 本文针对自动驾驶高精地图构建中多模态融合方法鲁棒性不足的问题，提出数据增强、新型融合模块和模态丢弃训练策略，显著提升了模型鲁棒性，并在NuScenes数据集上实现了最先进的准确性。


<details>
  <summary>Details</summary>
Motivation: 高精地图对自动驾驶系统至关重要。尽管现有的相机-激光雷达融合方法在构建高精地图方面取得了进展，但它们主要关注模型准确性，却忽视了对真实世界应用至关重要的感知模型鲁棒性。

Method: 为增强高精地图构建中多模态融合方法的鲁棒性并保持高准确性，本文提出了三项关键组件：数据增强、一种新颖的多模态融合模块，以及一种模态丢弃训练策略。这些组件在NuScenes数据集上进行了评估。

Result: 实验结果表明，所提出的方法显著增强了基线方法的鲁棒性。此外，该方法在NuScenes数据集的干净验证集上取得了最先进的性能。

Conclusion: 本研究为开发更鲁棒、更可靠的高精地图构建模型提供了宝贵见解，从而推动了它们在真实世界自动驾驶场景中的应用。

Abstract: High-definition (HD) map construction methods are crucial for providing
precise and comprehensive static environmental information, which is essential
for autonomous driving systems. While Camera-LiDAR fusion techniques have shown
promising results by integrating data from both modalities, existing approaches
primarily focus on improving model accuracy and often neglect the robustness of
perception models, which is a critical aspect for real-world applications. In
this paper, we explore strategies to enhance the robustness of multi-modal
fusion methods for HD map construction while maintaining high accuracy. We
propose three key components: data augmentation, a novel multi-modal fusion
module, and a modality dropout training strategy. These components are
evaluated on a challenging dataset containing 10 days of NuScenes data. Our
experimental results demonstrate that our proposed methods significantly
enhance the robustness of baseline methods. Furthermore, our approach achieves
state-of-the-art performance on the clean validation set of the NuScenes
dataset. Our findings provide valuable insights for developing more robust and
reliable HD map construction models, advancing their applicability in
real-world autonomous driving scenarios. Project website:
https://robomap-123.github.io.

</details>


### [75] [AVC-DPO: Aligned Video Captioning via Direct Preference Optimization](https://arxiv.org/abs/2507.01492)
*Jiyang Tang,Hengyi Li,Yifan Du,Wayne Xin Zhao*

Main category: cs.CV

TL;DR: 提出AVC-DPO框架，通过偏好对齐优化视频多模态大语言模型（video MLLMs）的字幕生成，并在视频详细字幕（VDC）挑战赛中获得第一名。


<details>
  <summary>Details</summary>
Motivation: 尽管视频多模态大语言模型在视频字幕生成方面取得了显著进展，但仍难以根据人类偏好调整字幕的侧重点。

Method: 提出AVC-DPO（通过直接偏好优化实现对齐视频字幕）的后训练框架。该方法设计了增强提示，专门针对人类关注的时空信息，并利用同一基础模型在不同提示条件下的字幕生成响应进行偏好感知训练和字幕对齐。

Result: 在LOVE@CVPR'25 Workshop Track 1A: 视频详细字幕挑战赛中表现卓越，根据VDCSCORE评估指标，在视频详细字幕（VDC）基准测试中获得第一名。

Conclusion: AVC-DPO框架成功解决了视频字幕与人类偏好对齐的挑战，通过偏好对齐显著提升了视频多模态大语言模型的字幕生成能力，并在一项重要基准测试中验证了其卓越性能。

Abstract: Although video multimodal large language models (video MLLMs) have achieved
substantial progress in video captioning tasks, it remains challenging to
adjust the focal emphasis of video captions according to human preferences. To
address this limitation, we propose Aligned Video Captioning via Direct
Preference Optimization (AVC-DPO), a post-training framework designed to
enhance captioning capabilities in video MLLMs through preference alignment.
Our approach designs enhanced prompts that specifically target temporal
dynamics and spatial information-two key factors that humans care about when
watching a video-thereby incorporating human-centric preferences. AVC-DPO
leverages the same foundation model's caption generation responses under varied
prompt conditions to conduct preference-aware training and caption alignment.
Using this framework, we have achieved exceptional performance in the
LOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving
first place on the Video Detailed Captioning (VDC) benchmark according to the
VDCSCORE evaluation metric.

</details>


### [76] [Crop Pest Classification Using Deep Learning Techniques: A Review](https://arxiv.org/abs/2507.01494)
*Muhammad Hassam Ejaz,Muhammad Bilal,Usman Habib*

Main category: cs.CV

TL;DR: 该综述分析了2018-2025年间37项关于基于AI的害虫分类研究，探讨了深度学习在害虫监测中的应用，概述了模型架构趋势（从CNNs到混合及Transformer模型），并指出了当前面临的数据集不平衡、小害虫检测、泛化能力及边缘部署等挑战。


<details>
  <summary>Details</summary>
Motivation: 传统害虫监测方法缓慢、人工且难以扩展，对作物产量构成严重威胁。深度学习已成为强大的解决方案，但需要系统综述来理解AI在害虫分类领域的最新进展、模型趋势和未解决的挑战。

Method: 本研究采用综述方法，系统性地审查了2018年至2025年间发表的37项AI害虫分类研究。这些研究根据作物类型、害虫种类、模型架构、数据集使用和关键技术挑战进行组织和分析。

Result: 早期研究主要依赖CNNs，但最新工作正转向混合和基于Transformer的模型，以实现更高的准确性和更好的上下文理解。尽管取得了进展，但数据不平衡、小型害虫检测困难、泛化能力有限以及边缘设备部署等挑战依然存在。

Conclusion: 该综述为基于AI的害虫监测领域提供了结构化概述，重点介绍了有用的数据集，并明确了该领域的关键挑战和未来发展方向，旨在推动更高效、可扩展的害虫监测系统。

Abstract: Insect pests continue to bring a serious threat to crop yields around the
world, and traditional methods for monitoring them are often slow, manual, and
difficult to scale. In recent years, deep learning has emerged as a powerful
solution, with techniques like convolutional neural networks (CNNs), vision
transformers (ViTs), and hybrid models gaining popularity for automating pest
detection. This review looks at 37 carefully selected studies published between
2018 and 2025, all focused on AI-based pest classification. The selected
research is organized by crop type, pest species, model architecture, dataset
usage, and key technical challenges. The early studies relied heavily on CNNs
but latest work is shifting toward hybrid and transformer-based models that
deliver higher accuracy and better contextual understanding. Still, challenges
like imbalanced datasets, difficulty in detecting small pests, limited
generalizability, and deployment on edge devices remain significant hurdles.
Overall, this review offers a structured overview of the field, highlights
useful datasets, and outlines the key challenges and future directions for
AI-based pest monitoring systems.

</details>


### [77] [ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation](https://arxiv.org/abs/2507.01496)
*Jimyeong Kim,Jungwon Park,Yeji Song,Nojun Kwak,Wonjong Rhee*

Main category: cs.CV

TL;DR: 提出一种新的、免训练的实图编辑方法，通过分析ReFlow模型的中间表示和利用中间步潜在表示，显著提升了ReFlow在实图编辑中的图像质量和文本对齐，超越现有方法并获用户青睐。


<details>
  <summary>Details</summary>
Motivation: Rectified Flow (ReFlow)文本到图像模型在生成图像质量和文本对齐方面超越了扩散模型，但在实图编辑方面的适应性仍具挑战，现有方法难以在保留结构的同时实现高质量编辑，且常需用户提供掩码或源提示。

Method: 本文通过分析多模态Transformer模块的中间表示并识别出三个关键特征，提出了一种新的ReFlow实图编辑方法。该方法利用仅反演到中间步的潜在表示（mid-step latent）从真实图像中提取特征以充分保留结构，并在注入过程中调整注意力机制以提高可编辑性并增强与目标文本的对齐。该方法无需训练、无需用户提供掩码，甚至无需源提示。

Result: 在两个基准测试上与九种基线方法进行广泛实验，结果表明本方法性能优于现有方法。人类评估进一步证实了用户对本方法的强烈偏好。

Conclusion: 所提出的免训练实图编辑方法有效解决了ReFlow模型在实图编辑中的挑战，通过独特的特征提取和注意力调整机制，在图像质量、文本对齐和用户友好性方面均表现出色，为ReFlow的实图编辑应用开辟了新途径。

Abstract: Rectified Flow text-to-image models surpass diffusion models in image quality
and text alignment, but adapting ReFlow for real-image editing remains
challenging. We propose a new real-image editing method for ReFlow by analyzing
the intermediate representations of multimodal transformer blocks and
identifying three key features. To extract these features from real images with
sufficient structural preservation, we leverage mid-step latent, which is
inverted only up to the mid-step. We then adapt attention during injection to
improve editability and enhance alignment to the target text. Our method is
training-free, requires no user-provided mask, and can be applied even without
a source prompt. Extensive experiments on two benchmarks with nine baselines
demonstrate its superior performance over prior methods, further validated by
human evaluations confirming a strong user preference for our approach.

</details>


### [78] [Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images](https://arxiv.org/abs/2507.01502)
*Ozan Durgut,Beril Kallfelz-Sirmacek,Cem Unsalan*

Main category: cs.CV

TL;DR: 为解决森林监测不足的问题，本研究提出一种结合传统方法与深度学习的新型规则方法，旨在提升自动化树冠检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 全球变暖、生物多样性丧失和空气污染是地球面临的重大问题，其中一个主要挑战是缺乏对森林的有效监测。因此，利用遥感和计算机视觉方法实现自动化监测，特别是自动树冠检测算法，变得至关重要。

Method: 本研究首先介绍了两种基于传统和深度学习的树冠检测方法。然后，提出一种新颖的、基于规则的集成方法，以增强检测结果的鲁棒性和准确性。具体而言，传统方法用于特征提取和森林区域分割，而深度学习方法用于检测树冠。随后，通过所提出的规则方法对结果进行后处理，旨在通过利用邻近树木和局部操作来增加检测到的树冠数量。

Result: 研究比较了所提出方法在检测到的树冠数量方面的结果，并报告了所得结果的优点、缺点和有待改进的领域。

Conclusion: 所提出的结合传统与深度学习的混合规则方法能有效提升树冠检测的准确性和鲁棒性，为自动化森林监测提供了有效工具，但仍存在进一步优化的潜力。

Abstract: Global warming, loss of biodiversity, and air pollution are among the most
significant problems facing Earth. One of the primary challenges in addressing
these issues is the lack of monitoring forests to protect them. To tackle this
problem, it is important to leverage remote sensing and computer vision methods
to automate monitoring applications. Hence, automatic tree crown detection
algorithms emerged based on traditional and deep learning methods. In this
study, we first introduce two different tree crown detection methods based on
these approaches. Then, we form a novel rule-based approach that integrates
these two methods to enhance robustness and accuracy of tree crown detection
results. While traditional methods are employed for feature extraction and
segmentation of forested areas, deep learning methods are used to detect tree
crowns in our method. With the proposed rule-based approach, we post-process
these results, aiming to increase the number of detected tree crowns through
neighboring trees and localized operations. We compare the obtained results
with the proposed method in terms of the number of detected tree crowns and
report the advantages, disadvantages, and areas for improvement of the obtained
outcomes.

</details>


### [79] [Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence](https://arxiv.org/abs/2507.01504)
*Robert Aufschläger,Youssef Shoeb,Azarm Nowzad,Michael Heigl,Fabian Bally,Martin Schramm*

Main category: cs.CV

TL;DR: 提出cRID框架，结合LVLM、GAT和表征学习，用于检测街景数据中非生物特征的PII并提升行人重识别能力，以解决开放数据隐私风险。实验证明其在跨数据集Re-ID中性能优越。


<details>
  <summary>Details</summary>
Motivation: 街景录像作为开放数据对自动驾驶和AI研究至关重要，但其中包含的个人可识别信息（PII，超越面部等生物特征）给行人带来了显著的隐私风险。

Method: 提出cRID，一个新颖的跨模态框架，结合了大视觉-语言模型、图注意力网络和表征学习。该方法旨在检测文本可描述的PII线索并增强行人重识别（Re-ID）。其核心在于识别和利用可解释特征，从而检测超越低级外观线索的语义PII。

Result: 在实际的跨数据集Re-ID场景中，该框架表现出改进的性能，尤其是在从Market-1501到CUHK03-np的数据集转换中，凸显了其实用性。

Conclusion: cRID框架通过有效检测文本可描述的PII线索并提升行人重识别能力，为处理包含PII的街景开放数据所带来的隐私挑战提供了一个实用的解决方案。

Abstract: The collection and release of street-level recordings as Open Data play a
vital role in advancing autonomous driving systems and AI research. However,
these datasets pose significant privacy risks, particularly for pedestrians,
due to the presence of Personally Identifiable Information (PII) that extends
beyond biometric traits such as faces. In this paper, we present cRID, a novel
cross-modal framework combining Large Vision-Language Models, Graph Attention
Networks, and representation learning to detect textual describable clues of
PII and enhance person re-identification (Re-ID). Our approach focuses on
identifying and leveraging interpretable features, enabling the detection of
semantically meaningful PII beyond low-level appearance cues. We conduct a
systematic evaluation of PII presence in person image datasets. Our experiments
show improved performance in practical cross-dataset Re-ID scenarios, notably
from Market-1501 to CUHK03-np (detected), highlighting the framework's
practical utility. Code is available at https://github.com/RAufschlaeger/cRID.

</details>


### [80] [Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation](https://arxiv.org/abs/2507.01509)
*Tapas K. Dutta,Snehashis Majhi,Deepak Ranjan Nayak,Debesh Jha*

Main category: cs.CV

TL;DR: 本文提出SAM-MaGuP，一种基于SAM并结合边界蒸馏模块和1D-2D Mamba适配器的方法，旨在解决结肠镜图像中息肉分割的弱边界和泛化性差的问题，并在多个数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 结肠镜图像中的息肉分割对结直肠癌的早期检测至关重要，但面临息肉形态、大小、颜色多样性大、与周围组织相似度高以及边界不清晰等挑战。现有方法在处理弱边界息肉时性能不稳定，区分息肉和非息肉的能力有限，且泛化性不足以满足临床应用需求。

Method: 作者提出了SAM-MaGuP方法，通过在Segment Anything Model (SAM)中引入边界蒸馏模块和1D-2D Mamba适配器来解决弱边界挑战并增强全局上下文特征学习。其核心创新包括一个Mamba引导的边界先验和一个1D-2D Mamba块。

Result: 在五个多样化数据集上的广泛评估表明，SAM-MaGuP优于现有最先进的方法，实现了无与伦比的分割精度和鲁棒性。

Conclusion: SAM-MaGuP通过其Mamba引导的边界先验和1D-2D Mamba块等关键创新，在息肉分割领域树立了新的基准，将该领域的研究推向了新的高度。

Abstract: Polyp segmentation in colonoscopy images is crucial for early detection and
diagnosis of colorectal cancer. However, this task remains a significant
challenge due to the substantial variations in polyp shape, size, and color, as
well as the high similarity between polyps and surrounding tissues, often
compounded by indistinct boundaries. While existing encoder-decoder CNN and
transformer-based approaches have shown promising results, they struggle with
stable segmentation performance on polyps with weak or blurry boundaries. These
methods exhibit limited abilities to distinguish between polyps and non-polyps
and capture essential boundary cues. Moreover, their generalizability still
falls short of meeting the demands of real-time clinical applications. To
address these limitations, we propose SAM-MaGuP, a groundbreaking approach for
robust polyp segmentation. By incorporating a boundary distillation module and
a 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels
at resolving weak boundary challenges and amplifies feature learning through
enriched global contextual interactions. Extensive evaluations across five
diverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,
achieving unmatched segmentation accuracy and robustness. Our key innovations,
a Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in
the field, pushing the boundaries of polyp segmentation to new heights.

</details>


### [81] [Exploring Pose-based Sign Language Translation: Ablation Studies and Attention Insights](https://arxiv.org/abs/2507.01532)
*Tomas Zelezny,Jakub Straka,Vaclav Javorek,Ondrej Valach,Marek Hruz,Ivan Gruber*

Main category: cs.CV

TL;DR: 研究姿态数据预处理技术（归一化、插值、增强）对手语翻译性能的影响，并基于改进T5模型验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 探究姿态数据预处理方法对手语翻译（SLT）性能的关键影响，旨在通过优化数据输入提高模型的鲁棒性和泛化能力。

Method: 采用基于Transformer的改进T5编解码模型处理姿态表示，并在YouTubeASL和How2Sign数据集上通过广泛的消融研究，评估不同预处理策略对翻译准确性的影响。

Result: 适当的归一化、插值和数据增强技术能显著提升模型鲁棒性和泛化能力。此外，分析模型注意力机制发现，添加专用寄存器token可改善整体模型性能。

Conclusion: 姿态数据预处理对于提高手语翻译模型的性能至关重要，结合Transformer架构和对注意力机制的优化（如添加寄存器token）能有效提升翻译准确性和模型泛化能力。

Abstract: Sign Language Translation (SLT) has evolved significantly, moving from
isolated recognition approaches to complex, continuous gloss-free translation
systems. This paper explores the impact of pose-based data preprocessing
techniques - normalization, interpolation, and augmentation - on SLT
performance. We employ a transformer-based architecture, adapting a modified T5
encoder-decoder model to process pose representations. Through extensive
ablation studies on YouTubeASL and How2Sign datasets, we analyze how different
preprocessing strategies affect translation accuracy. Our results demonstrate
that appropriate normalization, interpolation, and augmentation techniques can
significantly improve model robustness and generalization abilities.
Additionally, we provide a deep analysis of the model's attentions and reveal
interesting behavior suggesting that adding a dedicated register token can
improve overall model performance. We publish our code on our GitHub
repository, including the preprocessed YouTubeASL data.

</details>


### [82] [TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking](https://arxiv.org/abs/2507.01535)
*Bingxi Liu,Calvin Chen,Junhao Li,Guyang Yu,Haoqian Song,Xuchen Liu,Jinqiang Cui,Hong Zhang*

Main category: cs.CV

TL;DR: 本文提出TrackingMiM，一个基于Mamba的嵌套架构，旨在解决无人机跟踪中ViT的二次复杂度问题和现有Mamba模型的时序不一致性。该模型在无人机跟踪任务中实现了最先进的性能和显著的加速。


<details>
  <summary>Details</summary>
Motivation: 解决Vision Transformer (ViT) 在无人机实时跟踪中面临的二次复杂度挑战，及其对系统实时性的影响；同时，改进现有基于Mamba的方法在处理时序数据时未能充分考虑时序连续性的问题。

Method: 利用状态空间模型Mamba的高计算效率和长序列建模能力。提出TrackingMiM，一个Mamba-in-Mamba的架构，用于处理跟踪任务中的稠密图像序列。该框架通过嵌套的Mamba扫描，独立处理时间和空间连贯的图像块，并将模板帧编码为查询令牌，用于每次扫描中的跟踪。

Result: 在五个无人机跟踪基准测试中，所提出的TrackingMiM模型实现了最先进的精度，并显著提高了无人机跟踪的速度。

Conclusion: TrackingMiM成功解决了无人机跟踪中ViT的复杂度问题和现有Mamba模型的时序不一致性，提供了高精度和高速度的解决方案，证实了其在实时无人机跟踪应用中的优越性。

Abstract: The Vision Transformer (ViT) model has long struggled with the challenge of
quadratic complexity, a limitation that becomes especially critical in unmanned
aerial vehicle (UAV) tracking systems, where data must be processed in real
time. In this study, we explore the recently proposed State-Space Model, Mamba,
leveraging its computational efficiency and capability for long-sequence
modeling to effectively process dense image sequences in tracking tasks. First,
we highlight the issue of temporal inconsistency in existing Mamba-based
methods, specifically the failure to account for temporal continuity in the
Mamba scanning mechanism. Secondly, building upon this insight,we propose
TrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model
for handling image sequence of tracking problem. In our framework, the mamba
scan is performed in a nested way while independently process temporal and
spatial coherent patch tokens. While the template frame is encoded as query
token and utilized for tracking in every scan. Extensive experiments conducted
on five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves
state-of-the-art precision while offering noticeable higher speed in UAV
tracking.

</details>


### [83] [A Multi-Centric Anthropomorphic 3D CT Phantom-Based Benchmark Dataset for Harmonization](https://arxiv.org/abs/2507.01539)
*Mohammadreza Amirian,Michael Bach,Oscar Jimenez-del-Toro,Christoph Aberle,Roger Schaer,Vincent Andrearczyk,Jean-Félix Maestrati,Maria Martin Asiain,Kyriakos Flouris,Markus Obmann,Clarisse Dromain,Benoît Dufour,Pierre-Alexandre Alois Poletti,Hendrik von Tengg-Kobligk,Rolf Hügli,Martin Kretzschmar,Hatem Alkadhi,Ender Konukoglu,Henning Müller,Bram Stieltjes,Adrien Depeursinge*

Main category: cs.CV

TL;DR: 本文提出一个开源的CT假体基准数据集，旨在解决医学AI（特别是CT分析）中因数据分布偏移导致的泛化能力差问题，并提供了评估方法和基线结果以促进AI协调技术的发展。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医学领域（特别是CT分析）的应用中，其泛化能力会因扫描仪制造商、重建技术或剂量变化等导致的数据分布偏移而显著下降。AI协调技术有望解决此问题，但缺乏标准化的评估资源。

Method: 本研究构建并发布了一个开源的基准数据集，包含1378个CT图像系列，这些图像来自一个人体模型假体，由来自4家制造商、8个机构的13台扫描仪以标准化协议和不同采集剂量获取。使用假体消除了患者间和患者内部变异。此外，论文还提出了评估图像和特征级别稳定性以及肝组织分类的方法论、基线结果和开源代码。

Result: 成功创建了一个大规模、受控的CT假体基准数据集，用于促进AI协调技术的研究。提供了评估AI协调策略的通用方法和初步基线结果，并发布了相关开源代码。

Conclusion: 该研究通过提供一个独特的、受控的开源基准数据集及其评估框架和基线结果，为解决医学AI在数据分布偏移下的泛化挑战提供了关键资源，旨在加速AI协调技术的发展，从而提高医学AI的鲁棒性和可靠性。

Abstract: Artificial intelligence (AI) has introduced numerous opportunities for human
assistance and task automation in medicine. However, it suffers from poor
generalization in the presence of shifts in the data distribution. In the
context of AI-based computed tomography (CT) analysis, significant data
distribution shifts can be caused by changes in scanner manufacturer,
reconstruction technique or dose. AI harmonization techniques can address this
problem by reducing distribution shifts caused by various acquisition settings.
This paper presents an open-source benchmark dataset containing CT scans of an
anthropomorphic phantom acquired with various scanners and settings, which
purpose is to foster the development of AI harmonization techniques. Using a
phantom allows fixing variations attributed to inter- and intra-patient
variations. The dataset includes 1378 image series acquired with 13 scanners
from 4 manufacturers across 8 institutions using a harmonized protocol as well
as several acquisition doses. Additionally, we present a methodology, baseline
results and open-source code to assess image- and feature-level stability and
liver tissue classification, promoting the development of AI harmonization
strategies.

</details>


### [84] [Interpolation-Based Event Visual Data Filtering Algorithms](https://arxiv.org/abs/2507.01557)
*Marcin Kowlaczyk,Tomasz Kryjak*

Main category: cs.CV

TL;DR: 针对事件相机数据噪声大的问题，本文提出基于无限脉冲响应（IIR）滤波器矩阵的四种算法，可去除约99%的噪声，同时内存占用低（约30KB），适用于嵌入式设备。


<details>
  <summary>Details</summary>
Motivation: 神经形态视觉领域发展迅速，事件相机应用广泛，但其数据流存在显著噪声。

Method: 提出了四种基于无限脉冲响应（IIR）滤波器矩阵的算法。在多个事件数据集（包含人工噪声和DVS记录噪声）上进行了比较。

Result: 所提出的方法能够去除大约99%的噪声，同时保留大部分有效信号。对于1280x720分辨率的传感器，内存占用约为30KB。

Conclusion: 所提出的事件数据降噪方法高效且内存占用低，非常适合在嵌入式设备中实现。

Abstract: The field of neuromorphic vision is developing rapidly, and event cameras are
finding their way into more and more applications. However, the data stream
from these sensors is characterised by significant noise. In this paper, we
propose a method for event data that is capable of removing approximately 99\%
of noise while preserving the majority of the valid signal. We have proposed
four algorithms based on the matrix of infinite impulse response (IIR) filters
method. We compared them on several event datasets that were further modified
by adding artificially generated noise and noise recorded with dynamic vision
sensor. The proposed methods use about 30KB of memory for a sensor with a
resolution of 1280 x 720 and is therefore well suited for implementation in
embedded devices.

</details>


### [85] [A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2507.01573)
*Hao Wang,Keyan Hu,Xin Guo,Haifeng Li,Chao Tao*

Main category: cs.CV

TL;DR: 针对遥感图像语义分割中鉴别式模型在高频特征（边界）学习上的不足，本文提出IDGBR框架，结合鉴别式学习与扩散生成模型，通过迭代去噪过程有效细化分割边界。


<details>
  <summary>Details</summary>
Motivation: 遥感语义分割需要识别地物（低频信息）和精确定位边界（高频信息）。然而，现有鉴别式学习模型擅长捕捉低频特征，但在学习高频特征方面存在局限。扩散生成模型擅长生成高频细节，但单独使用时在低频语义推理方面表现不足。研究动机在于整合鉴别式和生成式学习的优势，以解决边界精细化问题。

Method: 提出“鉴别式与基于扩散的生成式学习融合的边界细化”（IDGBR）框架。该框架首先利用鉴别式主干模型生成粗略的分割图，然后将该粗略图与原始图像输入条件引导网络，共同学习一个引导表示，最后通过迭代去噪扩散过程利用该引导表示来精化粗略分割图的边界。

Result: 在五个遥感语义分割数据集（包括二分类和多分类）上的广泛实验表明，该框架能够对来自不同鉴别式架构的粗略结果进行一致的边界细化。

Conclusion: IDGBR框架成功整合了鉴别式和扩散生成式学习的优势，有效解决了遥感语义分割中高频边界细节的学习和精细化问题，显著提升了分割结果的边界精度。

Abstract: Remote sensing semantic segmentation must address both what the ground
objects are within an image and where they are located. Consequently,
segmentation models must ensure not only the semantic correctness of
large-scale patches (low-frequency information) but also the precise
localization of boundaries between patches (high-frequency information).
However, most existing approaches rely heavily on discriminative learning,
which excels at capturing low-frequency features, while overlooking its
inherent limitations in learning high-frequency features for semantic
segmentation. Recent studies have revealed that diffusion generative models
excel at generating high-frequency details. Our theoretical analysis confirms
that the diffusion denoising process significantly enhances the model's ability
to learn high-frequency features; however, we also observe that these models
exhibit insufficient semantic inference for low-frequency features when guided
solely by the original image. Therefore, we integrate the strengths of both
discriminative and generative learning, proposing the Integration of
Discriminative and diffusion-based Generative learning for Boundary Refinement
(IDGBR) framework. The framework first generates a coarse segmentation map
using a discriminative backbone model. This map and the original image are fed
into a conditioning guidance network to jointly learn a guidance representation
subsequently leveraged by an iterative denoising diffusion process refining the
coarse segmentation. Extensive experiments across five remote sensing semantic
segmentation datasets (binary and multi-class segmentation) confirm our
framework's capability of consistent boundary refinement for coarse results
from diverse discriminative architectures. The source code will be available at
https://github.com/KeyanHu-git/IDGBR.

</details>


### [86] [SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation](https://arxiv.org/abs/2507.01586)
*Bryan Constantine Sadihin,Michael Hua Wang,Shei Pern Chua,Hang Su*

Main category: cs.CV

TL;DR: 本文提出了SketchColour，首个基于扩散变换器（DiT）的2D动画线稿上色流水线。该方法通过轻量级适配器和LoRA微调，高效地将线稿转换为彩色动画，显著减少了资源消耗，并在性能上超越了现有的先进方法。


<details>
  <summary>Details</summary>
Motivation: 高质量2D动画的制作，特别是逐帧手绘上色，是一个劳动密集型过程。

Method: SketchColour采用DiT风格架构替代传统的U-Net去噪器，通过轻量级通道拼接适配器注入线稿信息，并结合LoRA微调。该方法避免了ControlNet带来的参数和内存冗余，从而大幅降低了参数数量和GPU内存使用。

Result: 在SAKUGA数据集上，SketchColour在所有指标上均优于现有最先进的视频上色方法，且仅使用了竞争模型一半的训练数据。此外，它能生成时间连贯的动画，并最大程度地减少了溢色或物体变形等伪影。

Conclusion: SketchColour通过创新的DiT-based流水线，显著提升了2D动画上色的效率和质量，同时有效降低了资源需求，为自动化动画制作流程提供了高效且高质量的解决方案。

Abstract: The production of high-quality 2D animation is highly labor-intensive
process, as animators are currently required to draw and color a large number
of frames by hand. We present SketchColour, the first sketch-to-colour pipeline
for 2D animation built on a diffusion transformer (DiT) backbone. By replacing
the conventional U-Net denoiser with a DiT-style architecture and injecting
sketch information via lightweight channel-concatenation adapters accompanied
with LoRA finetuning, our method natively integrates conditioning without the
parameter and memory bloat of a duplicated ControlNet, greatly reducing
parameter count and GPU memory usage. Evaluated on the SAKUGA dataset,
SketchColour outperforms previous state-of-the-art video colourization methods
across all metrics, despite using only half the training data of competing
models. Our approach produces temporally coherent animations with minimal
artifacts such as colour bleeding or object deformation. Our code is available
at: https://bconstantine.github.io/SketchColour .

</details>


### [87] [Towards Controllable Real Image Denoising with Camera Parameters](https://arxiv.org/abs/2507.01587)
*Youngjin Oh,Junhyeong Kwon,Keuntek Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: 提出一个可控的图像去噪框架，利用相机参数（如ISO、快门速度、光圈）自适应调整去噪强度，提高了去噪性能和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的图像去噪方法虽然性能出色，但普遍缺乏根据噪声水平、相机设置或用户偏好调整去噪强度的灵活性。

Method: 引入了一个新的可控去噪框架，该框架通过利用相机参数（特别是ISO、快门速度和F-number）的信息来自适应地去除图像噪声。这些选定的参数被转换为一个向量，用于控制并增强去噪网络的性能。

Result: 实验结果表明，所提出的方法能够无缝地为标准去噪神经网络增加可控性，并显著提升其性能。

Conclusion: 本研究成功开发了一个通过整合相机参数实现高度可控和高性能的图像去噪框架，解决了传统方法在去噪强度调整方面的局限性。

Abstract: Recent deep learning-based image denoising methods have shown impressive
performance; however, many lack the flexibility to adjust the denoising
strength based on the noise levels, camera settings, and user preferences. In
this paper, we introduce a new controllable denoising framework that adaptively
removes noise from images by utilizing information from camera parameters.
Specifically, we focus on ISO, shutter speed, and F-number, which are closely
related to noise levels. We convert these selected parameters into a vector to
control and enhance the performance of the denoising network. Experimental
results show that our method seamlessly adds controllability to standard
denoising neural networks and improves their performance. Code is available at
https://github.com/OBAKSA/CPADNet.

</details>


### [88] [Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring](https://arxiv.org/abs/2507.01590)
*Ameer Hamza,Zuhaib Hussain But,Umar Arif,Samiya,M. Abdullah Asad,Muhammad Naeem*

Main category: cs.CV

TL;DR: 本研究开发了一种新型课堂监控系统，通过集成疲劳、手机使用和人脸识别等多种模态，高精度评估学生注意力，并可实现自动考勤。


<details>
  <summary>Details</summary>
Motivation: 旨在提高课堂注意力评估的精度，并通过整合多模态监控（疲劳、手机使用、人脸识别）提供学生参与度和行为的实时洞察，并实现自动考勤记录。

Method: 系统利用YOLOv8模型检测手机和疲劳使用，通过结合YOLO、MTCNN和LResNet Occ FC进行人脸识别和肢体跟踪。模型在RMFD和Roboflow等专用数据集上训练。系统通过PHP网络应用程序实现，并采用ESP32-CAM硬件进行数据捕获。

Result: 系统评估结果显示：疲劳检测达到97.42%的mAP@50；人脸识别达到86.45%的验证准确率；手机检测达到85.89%的mAP@50。

Conclusion: 该集成方法不仅增强了课堂监控，还能通过人脸识别自动记录学生考勤，展现出良好的可扩展性，适用于不同的教育环境。

Abstract: This study presents a novel classroom surveillance system that integrates
multiple modalities, including drowsiness, tracking of mobile phone usage, and
face recognition,to assess student attentiveness with enhanced precision.The
system leverages the YOLOv8 model to detect both mobile phone and sleep
usage,(Ghatge et al., 2024) while facial recognition is achieved through
LResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These
models work in synergy to provide comprehensive, real-time monitoring, offering
insights into student engagement and behavior.(S et al., 2023) The framework is
trained on specialized datasets, such as the RMFD dataset for face recognition
and a Roboflow dataset for mobile phone detection. The extensive evaluation of
the system shows promising results. Sleep detection achieves 97. 42% mAP@50,
face recognition achieves 86. 45% validation accuracy and mobile phone
detection reach 85. 89% mAP@50. The system is implemented within a core PHP web
application and utilizes ESP32-CAM hardware for seamless data capture.(Neto et
al., 2024) This integrated approach not only enhances classroom monitoring, but
also ensures automatic attendance recording via face recognition as students
remain seated in the classroom, offering scalability for diverse educational
environments.(Banada,2025)

</details>


### [89] [DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation](https://arxiv.org/abs/2507.01603)
*Yue-Jiang Dong,Wang Zhao,Jiale Xu,Ying Shan,Song-Hai Zhang*

Main category: cs.CV

TL;DR: DepthSync是一个无需训练的框架，利用扩散引导为长视频生成尺度和几何一致的深度预测，解决了现有方法中尺度差异和几何不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视频深度估计方法在长视频处理上存在挑战：1) 使用滑动窗口导致窗口间累积的尺度差异；2) 仅依赖2D扩散先验，忽略视频深度的3D几何结构，导致几何不一致。

Method: 本文提出DepthSync，一个无需训练的扩散引导框架。它引入尺度引导以同步窗口间的深度尺度，并引入几何引导以基于内在3D约束确保窗口内的几何对齐。这两个引导项协同作用，实现一致的深度预测。

Result: 在多种数据集上的实验验证了DepthSync的有效性，其生成的深度估计具有更高的尺度和几何一致性，尤其是在长视频上表现突出。

Conclusion: DepthSync通过引入创新的尺度和几何引导机制，成功解决了长视频深度估计中的尺度差异和几何不一致问题，提升了预测的质量和一致性。

Abstract: Diffusion-based video depth estimation methods have achieved remarkable
success with strong generalization ability. However, predicting depth for long
videos remains challenging. Existing methods typically split videos into
overlapping sliding windows, leading to accumulated scale discrepancies across
different windows, particularly as the number of windows increases.
Additionally, these methods rely solely on 2D diffusion priors, overlooking the
inherent 3D geometric structure of video depths, which results in geometrically
inconsistent predictions. In this paper, we propose DepthSync, a novel,
training-free framework using diffusion guidance to achieve scale- and
geometry-consistent depth predictions for long videos. Specifically, we
introduce scale guidance to synchronize the depth scale across windows and
geometry guidance to enforce geometric alignment within windows based on the
inherent 3D constraints in video depths. These two terms work synergistically,
steering the denoising process toward consistent depth predictions. Experiments
on various datasets validate the effectiveness of our method in producing depth
estimates with improved scale and geometry consistency, particularly for long
videos.

</details>


### [90] [Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems](https://arxiv.org/abs/2507.01607)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi,Eric Bourbao*

Main category: cs.CV

TL;DR: 首次对深度学习人脸识别系统进行了系统级后门攻击研究，发现并演示了针对人脸检测和特征提取的新型攻击，证明单一后门可绕过整个系统。


<details>
  <summary>Details</summary>
Motivation: 尽管已有工作指出深度学习系统存在漏洞，但针对真实世界、非约束环境下图像处理的DNN后门攻击在文献中仍是一个盲点，尤其缺乏对人脸识别系统的系统级研究。

Method: 进行了首次针对深度学习人脸识别系统的系统级后门研究；探索了DNN后门在整个识别流程中的可行性；首次演示了两种针对人脸检测任务的后门攻击（人脸生成攻击和人脸关键点偏移攻击）；证明了使用大裕度损失训练的人脸特征提取器也易受后门攻击；结合所开发的模型，在20种可能的管道配置和15个攻击案例下验证了攻击效果。

Result: 首次成功演示了针对人脸检测任务的两种后门攻击；证明了使用大裕度损失训练的人脸特征提取器同样易受后门攻击；通过大量实验验证，单一后门攻击能够绕过整个系统的功能。

Conclusion: 深度学习人脸识别系统在真实世界应用中存在严重的系统级后门漏洞，攻击者可利用单一后门绕过系统全部功能。为此，论文提供了相关的最佳实践和对策。

Abstract: The widespread use of deep learning face recognition raises several security
concerns. Although prior works point at existing vulnerabilities, DNN backdoor
attacks against real-life, unconstrained systems dealing with images captured
in the wild remain a blind spot of the literature. This paper conducts the
first system-level study of backdoors in deep learning-based face recognition
systems. This paper yields four contributions by exploring the feasibility of
DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the
first time two backdoor attacks on the face detection task: face generation and
face landmark shift attacks. We then show that face feature extractors trained
with large margin losses also fall victim to backdoor attacks. Combining our
models, we then show using 20 possible pipeline configurations and 15 attack
cases that a single backdoor enables an attacker to bypass the entire function
of a system. Finally, we provide stakeholders with several best practices and
countermeasures.

</details>


### [91] [Perception-Oriented Latent Coding for High-Performance Compressed Domain Semantic Inference](https://arxiv.org/abs/2507.01608)
*Xu Zhang,Ming Lu,Yan Chen,Zhan Ma*

Main category: cs.CV

TL;DR: 本文提出感知导向潜在编码（POLC），通过丰富潜在特征的语义内容，显著提升压缩域语义推理性能，同时大幅降低微调开销。


<details>
  <summary>Details</summary>
Motivation: 当前压缩域语义推理依赖于基于MSE优化的图像编码模型，其潜在空间语义信息贫乏，不利于下游语义任务；且实现高性能常需对整个视觉模型进行耗时计算的微调。

Method: 引入感知导向潜在编码（POLC）方法，旨在丰富潜在特征的语义内容，以实现高性能压缩域语义推理。POLC利用语义丰富的潜在空间，仅需一个即插即用适配器进行微调，大幅减少了参数量。

Result: 实验结果表明，POLC在速率-感知性能上可与最先进的生成式图像编码方法媲美，同时显著提升了视觉任务的性能，且微调开销极小。

Conclusion: POLC有效解决了压缩域语义推理中潜在空间语义不足和微调成本高昂的问题，以高效方式实现了性能提升。

Abstract: In recent years, compressed domain semantic inference has primarily relied on
learned image coding models optimized for mean squared error (MSE). However,
MSE-oriented optimization tends to yield latent spaces with limited semantic
richness, which hinders effective semantic inference in downstream tasks.
Moreover, achieving high performance with these models often requires
fine-tuning the entire vision model, which is computationally intensive,
especially for large models. To address these problems, we introduce
Perception-Oriented Latent Coding (POLC), an approach that enriches the
semantic content of latent features for high-performance compressed domain
semantic inference. With the semantically rich latent space, POLC requires only
a plug-and-play adapter for fine-tuning, significantly reducing the parameter
count compared to previous MSE-oriented methods. Experimental results
demonstrate that POLC achieves rate-perception performance comparable to
state-of-the-art generative image coding methods while markedly enhancing
performance in vision tasks, with minimal fine-tuning overhead. Code is
available at https://github.com/NJUVISION/POLC.

</details>


### [92] [Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss](https://arxiv.org/abs/2507.01630)
*Yuxiao Wang,Yu Lei,Zhenao Wei,Weiying Xue,Xinyu Jiang,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: 本文提出了P3HOT框架，结合提示引导和人体近端感知，并引入新的损失函数和评估指标，旨在解决人-物体接触(HOT)检测中现有模型在图像类型限制、过度分割和类别一致性等方面的不足，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的人-物体接触(HOT)检测模型受限于单一图像类型，常导致低交互区域过度分割，难以维持区域内类别一致性。2D视角下人与物体重叠的不确定性以及当前评估指标对负样本处理的不足也是亟待解决的问题。

Method: 提出了P3HOT框架，融合了提示引导和人体近端感知。具体方法包括：1) 利用语义驱动的提示机制，基于图文相关性引导网络关注相关区域；2) 引入人体近端感知机制，动态感知人体周围的关键深度范围，通过可学习参数消除非交互区域，提供准3D视角以解决2D重叠不确定性；3) 设计了区域联合损失(RJLoss)以抑制同一区域内的异常类别；4) 提出了新的评估指标“AD-Acc.”来弥补现有方法在处理负样本时的不足。

Result: P3HOT方法在两个基准数据集上的四项指标中均达到了最先进的性能。特别是在HOT-Annotated数据集上，SC-Acc.提升0.7，mIoU提升2.0，wIoU提升1.6，AD-Acc.大幅提升11.0。

Conclusion: P3HOT框架通过其创新的提示引导、人体近端感知机制以及新的损失函数和评估指标，成功克服了当前HOT检测方法的局限性，显著提升了检测性能和鲁棒性。

Abstract: The task of Human-Object conTact (HOT) detection involves identifying the
specific areas of the human body that are touching objects. Nevertheless,
current models are restricted to just one type of image, often leading to too
much segmentation in areas with little interaction, and struggling to maintain
category consistency within specific regions. To tackle this issue, a HOT
framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt
guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we
utilize a semantic-driven prompt mechanism to direct the network's attention
towards the relevant regions based on the correlation between image and text.
Then a human proximal perception mechanism is employed to dynamically perceive
key depth range around the human, using learnable parameters to effectively
eliminate regions where interactions are not expected. Calculating depth
resolves the uncertainty of the overlap between humans and objects in a 2D
perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss
(RJLoss) has been created as a new loss to inhibit abnormal categories in the
same area. A new evaluation metric called ``AD-Acc.'' is introduced to address
the shortcomings of existing methods in addressing negative samples.
Comprehensive experimental results demonstrate that our approach achieves
state-of-the-art performance in four metrics across two benchmark datasets.
Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$,
\textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in
SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated
dataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.

</details>


### [93] [Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation](https://arxiv.org/abs/2507.01631)
*Camille Billouard,Dawa Derksen,Alexandre Constantin,Bruno Vallet*

Main category: cs.CV

TL;DR: 提出Snake-NeRF框架，通过创新的分块和图像裁剪策略，解决大规模卫星图像NeRF重建的内存限制，实现单GPU高效高质处理。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法在处理大规模卫星图像3D重建时，因训练内存占用过高而受限于小场景。

Method: 引入Snake-NeRF框架，采用外核（out-of-core）方法，无需同时加载所有图像和网络。通过将场景划分为无重叠的3D NeRF块，但裁剪图像时带有重叠以确保像素完整性。同时，提出 novel 的$2\times 2$ 3D瓦片渐进策略和分段采样器，以避免瓦片边缘的重建错误。

Result: 实验证明，该方法能有效处理大型卫星图像，实现线性时间复杂度，在单个GPU上运行且不影响重建质量。

Conclusion: Snake-NeRF成功解决了NeRF在大规模卫星图像3D重建中的内存瓶颈，提供了一个高效、高质量且可扩展的解决方案。

Abstract: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

</details>


### [94] [Depth Anything at Any Condition](https://arxiv.org/abs/2507.01634)
*Boyuan Sun,Modi Jin,Bowen Yin,Qibin Hou*

Main category: cs.CV

TL;DR: 提出DepthAnything-AC，一个在各种复杂环境条件下表现出色的单目深度估计（MDE）基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有MDE基础模型在复杂开放世界环境（如光照变化、恶劣天气、传感器畸变）中性能不佳，且面临数据稀缺和难以从受损图像生成高质量伪标签的挑战。

Method: 提出一种无监督一致性正则化微调范式，仅需少量未标注数据；并引入空间距离约束，以强制模型学习补丁级相对关系，从而获得更清晰的语义边界和更准确的细节。

Result: 实验结果表明，DepthAnything-AC在多种基准测试（包括真实世界恶劣天气、合成损坏和通用基准）上展现出强大的零样本能力。

Conclusion: DepthAnything-AC通过创新的训练范式和约束，有效解决了现有MDE模型在复杂环境下的局限性，在多变条件下的深度估计任务中实现了卓越的性能。

Abstract: We present Depth Anything at Any Condition (DepthAnything-AC), a foundation
monocular depth estimation (MDE) model capable of handling diverse
environmental conditions. Previous foundation MDE models achieve impressive
performance across general scenes but not perform well in complex open-world
environments that involve challenging conditions, such as illumination
variations, adverse weather, and sensor-induced distortions. To overcome the
challenges of data scarcity and the inability of generating high-quality
pseudo-labels from corrupted images, we propose an unsupervised consistency
regularization finetuning paradigm that requires only a relatively small amount
of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to
explicitly enforce the model to learn patch-level relative relationships,
resulting in clearer semantic boundaries and more accurate details.
Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC
across diverse benchmarks, including real-world adverse weather benchmarks,
synthetic corruption benchmarks, and general benchmarks.
  Project Page: https://ghost233lism.github.io/depthanything-AC-page
  Code: https://github.com/HVision-NKU/DepthAnythingAC

</details>


### [95] [ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving](https://arxiv.org/abs/2507.01735)
*Kai Chen,Ruiyuan Gao,Lanqing Hong,Hang Xu,Xu Jia,Holger Caesar,Dengxin Dai,Bingbing Liu,Dzmitry Tsishkou,Songcen Xu,Chunjing Xu,Qiang Xu,Huchuan Lu,Dit-Yan Yeung*

Main category: cs.CV

TL;DR: 本文详细介绍了ECCV 2024期间举办的首届W-CODA研讨会，旨在通过多模态感知和理解技术，探索自动驾驶极端情况的下一代解决方案。


<details>
  <summary>Details</summary>
Motivation: 探索并推动自动驾驶极端情况的下一代解决方案，利用先进的多模态感知和理解技术，以实现智能、可靠且能应对极端情况的自动驾驶系统。

Method: 邀请5位学术界和工业界演讲者分享最新进展和观点；收集研究论文；举办双轨挑战赛，涵盖极端情况场景理解和生成。

Result: 成功举办并详细介绍了首届W-CODA研讨会，为自动驾驶极端情况的研究和发展设定了方向和活动框架。

Conclusion: W-CODA是一项开创性努力，旨在持续弥合前沿自动驾驶技术与应对极端情况的智能可靠自动驾驶智能体之间的鸿沟。

Abstract: In this paper, we present details of the 1st W-CODA workshop, held in
conjunction with the ECCV 2024. W-CODA aims to explore next-generation
solutions for autonomous driving corner cases, empowered by state-of-the-art
multimodal perception and comprehension techniques. 5 Speakers from both
academia and industry are invited to share their latest progress and opinions.
We collect research papers and hold a dual-track challenge, including both
corner case scene understanding and generation. As the pioneering effort, we
will continuously bridge the gap between frontier autonomous driving techniques
and fully intelligent, reliable self-driving agents robust towards corner
cases.

</details>


### [96] [SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement](https://arxiv.org/abs/2507.01643)
*Weijie Yin,Dingkang Yang,Hongyuan Dong,Zijian Kang,Jiacong Wang,Xiao Liang,Chao Feng,Jiao Ran*

Main category: cs.CV

TL;DR: 本文提出SAILViT，一种渐进式特征学习增强的Vision Transformer，旨在解决现有ViT与LLMs直接协同训练的兼容性问题，并显著提升多模态大语言模型（MLLMs）在复杂多模态交互中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有ViTs虽然表现出色，但难以直接与大语言模型（LLMs）进行连接器协同训练，主要因为潜在的参数初始化冲突和模态语义鸿沟，这限制了多模态大语言模型（MLLMs）在复杂多模态交互中突破性能瓶颈。

Method: 本文提出SAILViT，通过引入渐进式特征学习机制，实现从粗到细的特征对齐和世界知识注入。这种渐进式特征细化方法能更好地满足目标训练需求。

Result: 通过广泛的实证分析，SAILViT在不同参数规模、模型架构、训练策略和数据规模下均展现出强大的鲁棒性和泛化能力。装备SAILViT后，现有MLLMs在OpenCompass基准测试的广泛下游任务中，均显示出显著且一致的性能提升。

Conclusion: SAILViT成功解决了ViT与LLM协同训练的难题，有效弥合了模态语义鸿沟，并通过其渐进式特征学习机制，显著增强了MLLMs处理复杂多模态交互的能力，从而突破了现有模型的性能瓶颈。

Abstract: Vision Transformers (ViTs) are essential as foundation backbones in
establishing the visual comprehension capabilities of Multimodal Large Language
Models (MLLMs). Although most ViTs achieve impressive performance through
image-text pair-based contrastive learning or self-supervised mechanisms, they
struggle to engage in connector-based co-training directly with LLMs due to
potential parameter initialization conflicts and modality semantic gaps. To
address the above challenges, this paper proposes SAILViT, a gradual feature
learning-enhanced ViT for facilitating MLLMs to break through performance
bottlenecks in complex multimodal interactions. SAILViT achieves
coarse-to-fine-grained feature alignment and world knowledge infusion with
gradual feature refinement, which better serves target training demands. We
perform thorough empirical analyses to confirm the powerful robustness and
generalizability of SAILViT across different dimensions, including parameter
sizes, model architectures, training strategies, and data scales. Equipped with
SAILViT, existing MLLMs show significant and consistent performance
improvements on the OpenCompass benchmark across extensive downstream tasks.
SAILViT series models are released at
https://huggingface.co/BytedanceDouyinContent.

</details>


### [97] [Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective](https://arxiv.org/abs/2507.01652)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Hui Deng,Xuyang Shen,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: 本文提出一种名为LASAD（Spatial-Aware Decay线性注意力）的新型机制，通过引入2D空间感知衰减因子，解决了传统线性注意力在图像生成中无法捕获长程空间依赖的问题，并在此基础上构建了自回归图像生成器LASADGen，实现了高效率和SOTA图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 自回归（AR）图像生成模型通常依赖于Transformer架构，但其面临平方级的计算复杂度和高内存消耗。虽然线性注意力机制可以降低计算负担，但在图像生成中会因无法捕获关键长程空间依赖而导致图像质量显著下降。

Method: 提出了一种新的注意力机制——空间感知衰减线性注意力（LASAD）。该机制通过计算基于真实2D空间位置而非1D序列位置的位置依赖衰减因子，显式地保留扁平化图像序列中的2D空间关系。在此机制基础上，构建了自回归图像生成器LASADGen。

Result: 在ImageNet上的实验表明，LASADGen实现了最先进（SOTA）的图像生成性能和计算效率。

Conclusion: LASADGen成功弥合了线性注意力机制的效率与高质量图像生成所需的空间理解之间的鸿沟，证明了在保持效率的同时提升图像生成质量的可行性。

Abstract: Autoregressive (AR) models have garnered significant attention in image
generation for their ability to effectively capture both local and global
structures within visual data. However, prevalent AR models predominantly rely
on the transformer architectures, which are beset by quadratic computational
complexity concerning input sequence length and substantial memory overhead due
to the necessity of maintaining key-value caches. Although linear attention
mechanisms have successfully reduced this burden in language models, our
initial experiments reveal that they significantly degrade image generation
quality because of their inability to capture critical long-range dependencies
in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a
novel attention mechanism that explicitly preserves genuine 2D spatial
relationships within the flattened image sequences by computing
position-dependent decay factors based on true 2D spatial location rather than
1D sequence positions. Based on this mechanism, we present LASADGen, an
autoregressive image generator that enables selective attention to relevant
spatial contexts with linear complexity. Experiments on ImageNet show LASADGen
achieves state-of-the-art image generation performance and computational
efficiency, bridging the gap between linear attention's efficiency and spatial
understanding needed for high-quality generation.

</details>


### [98] [RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather](https://arxiv.org/abs/2507.01653)
*Yuran Wang,Yingping Liang,Yutao Hu,Ying Fu*

Main category: cs.CV

TL;DR: 提出RobuSTereo框架，通过生成恶劣天气下的合成立体数据和设计鲁棒特征编码器，显著提升立体匹配模型在多种恶劣天气条件下的零样本泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 学习型立体匹配模型在恶劣天气下表现不佳，原因在于缺乏相应的训练数据以及难以从退化图像中提取判别性特征，这严重阻碍了模型对未见恶劣天气的零样本泛化能力。

Method: 本文提出RobuSTereo框架以解决数据稀缺和特征提取问题。首先，引入一个基于扩散的模拟管道，并结合立体一致性模块，生成高质量恶劣天气立体数据，用于训练以缩小域间隙。其次，设计了一个鲁棒特征编码器，该编码器结合了专门的ConvNet（用于捕获细粒度局部结构）和去噪Transformer（用于提炼全局表示），从而在退化图像中提取稳定可靠的特征。

Result: 广泛的实验证明，RobuSTereo显著提高了立体匹配模型在各种恶劣天气场景下的鲁棒性和泛化能力。

Conclusion: RobuSTereo框架通过解决训练数据稀缺和特征提取挑战，成功地增强了立体匹配模型在恶劣天气条件下的零样本泛化能力和鲁棒性。

Abstract: Learning-based stereo matching models struggle in adverse weather conditions
due to the scarcity of corresponding training data and the challenges in
extracting discriminative features from degraded images. These limitations
significantly hinder zero-shot generalization to out-of-distribution weather
conditions. In this paper, we propose \textbf{RobuSTereo}, a novel framework
that enhances the zero-shot generalization of stereo matching models under
adverse weather by addressing both data scarcity and feature extraction
challenges. First, we introduce a diffusion-based simulation pipeline with a
stereo consistency module, which generates high-quality stereo data tailored
for adverse conditions. By training stereo matching models on our synthetic
datasets, we reduce the domain gap between clean and degraded images,
significantly improving the models' robustness to unseen weather conditions.
The stereo consistency module ensures structural alignment across synthesized
image pairs, preserving geometric integrity and enhancing depth estimation
accuracy. Second, we design a robust feature encoder that combines a
specialized ConvNet with a denoising transformer to extract stable and reliable
features from degraded images. The ConvNet captures fine-grained local
structures, while the denoising transformer refines global representations,
effectively mitigating the impact of noise, low visibility, and weather-induced
distortions. This enables more accurate disparity estimation even under
challenging visual conditions. Extensive experiments demonstrate that
\textbf{RobuSTereo} significantly improves the robustness and generalization of
stereo matching models across diverse adverse weather scenarios.

</details>


### [99] [SPoT: Subpixel Placement of Tokens in Vision Transformers](https://arxiv.org/abs/2507.01654)
*Martine Hjelkrem-Tan,Marius Aasan,Gabriel Y. Arteaga,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: Vision Transformers (ViT)在处理稀疏性方面受限于传统网格分词。本文提出SPoT（亚像素级Token放置）策略，实现token在图像中的连续定位，通过“神谕引导搜索”显著减少推理所需token，同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Vision Transformers天然适应稀疏性，但现有标准分词方法将特征限制在离散网格中，这阻碍了模型充分利用稀疏性，导致性能受限和不必要的妥协。

Method: 提出SPoT（Subpixel Placement of Tokens）策略，这是一种新型分词方法，可在图像中连续定位tokens，从而规避基于网格的限制。通过“神谕引导搜索”来发现理想的亚像素token位置。

Result: 通过理想的亚像素token定位，实现了显著的性能提升，并大幅减少了推理时进行准确预测所需的token数量。

Conclusion: SPoT为构建更灵活、高效和可解释的Vision Transformer架构提供了新方向，将稀疏性重新定义为战略优势而非固有限制。

Abstract: Vision Transformers naturally accommodate sparsity, yet standard tokenization
methods confine features to discrete patch grids. This constraint prevents
models from fully exploiting sparse regimes, forcing awkward compromises. We
propose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that
positions tokens continuously within images, effectively sidestepping
grid-based limitations. With our proposed oracle-guided search, we uncover
substantial performance gains achievable with ideal subpixel token positioning,
drastically reducing the number of tokens necessary for accurate predictions
during inference. SPoT provides a new direction for flexible, efficient, and
interpretable ViT architectures, redefining sparsity as a strategic advantage
rather than an imposed limitation.

</details>


### [100] [What does really matter in image goal navigation?](https://arxiv.org/abs/2507.01667)
*Gianluca Monaci,Philippe Weinzaepfel,Christian Wolf*

Main category: cs.CV

TL;DR: 本文研究图像目标导航是否能通过强化学习（RL）进行端到端训练高效解决，并探讨了不同架构选择对相对姿态估计能力形成的影响。研究发现模拟器设置可能产生捷径，但所学能力可在一定程度上迁移到更真实的场景，且导航性能与新兴的相对姿态估计能力之间存在关联。


<details>
  <summary>Details</summary>
Motivation: 图像目标导航需要核心导航技能和通过图像对比计算方向信息。现有方法依赖专门的图像匹配或预训练的计算机视觉模块。本文的动机是研究该任务是否能像近期工作声称的那样，通过RL的端到端训练高效解决，因为这不仅会影响具身AI领域，还可能实现仅通过导航奖励来训练相对姿态估计。

Method: 本文通过大规模研究，采用强化学习（RL）对完整的智能体进行端到端训练，以解决图像目标导航任务。研究方法包括调查不同架构选择（如晚期融合、通道堆叠、空间到深度投影和交叉注意力）对任务性能和相对姿态估计能力形成的影响。

Result: 研究表明，近期方法的成功在一定程度上受到模拟器设置的影响，导致了模拟中的捷径。然而，这些通过RL训练获得的能力可以在一定程度上迁移到更真实的场景。此外，研究还发现导航性能与探测到的（新兴的）相对姿态估计性能（一项重要的子技能）之间存在关联。

Conclusion: 尽管模拟器设置可能导致训练中的捷径，但研究证明通过RL的端到端训练能够形成可迁移的图像目标导航能力。导航性能与相对姿态估计能力的出现之间存在显著关联，这支持了仅通过导航奖励学习相对姿态估计的可能性，为具身AI领域提供了新的视角。

Abstract: Image goal navigation requires two different skills: firstly, core navigation
skills, including the detection of free space and obstacles, and taking
decisions based on an internal representation; and secondly, computing
directional information by comparing visual observations to the goal image.
Current state-of-the-art methods either rely on dedicated image-matching, or
pre-training of computer vision modules on relative pose estimation. In this
paper, we study whether this task can be efficiently solved with end-to-end
training of full agents with RL, as has been claimed by recent work. A positive
answer would have impact beyond Embodied AI and allow training of relative pose
estimation from reward for navigation alone. In a large study we investigate
the effect of architectural choices like late fusion, channel stacking,
space-to-depth projections and cross-attention, and their role in the emergence
of relative pose estimators from navigation training. We show that the success
of recent methods is influenced up to a certain extent by simulator settings,
leading to shortcuts in simulation. However, we also show that these
capabilities can be transferred to more realistic setting, up to some extend.
We also find evidence for correlations between navigation performance and
probed (emerging) relative pose estimation performance, an important sub skill.

</details>


### [101] [Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition](https://arxiv.org/abs/2507.01673)
*Muzammil Behzad*

Main category: cs.CV

TL;DR: 提出FACET-VLM，一个利用多视角和语言提示进行3D/4D面部表情识别的视觉-语言框架。


<details>
  <summary>Details</summary>
Motivation: 3D和4D面部表情识别因其空间和时间动态的复杂性而面临重大挑战，但对人类行为理解、医疗健康监控和人机交互至关重要。

Method: 提出FACET-VLM，一个融合多视角面部表示学习和自然语言提示语义指导的视觉-语言框架。包含三个关键组件：用于视角一致性融合的跨视角语义聚合（CVSA）、用于语义对齐面部情感的多视角文本引导融合（MTGF），以及用于强制跨视角结构一致性的多视角一致性损失。

Result: 在BU-3DFE、Bosphorus、BU-4DFE和BP4D-Spontaneous等多个基准测试中达到了最先进的准确率。在4DME数据集上的4D微表情识别（MER）中也表现出色，并证实了框架内每个组件的有效性和重要贡献。

Conclusion: FACET-VLM为姿态化和自发性多模态面部表情识别提供了一个鲁棒、可扩展且高性能的解决方案。

Abstract: Facial expression recognition (FER) in 3D and 4D domains presents a
significant challenge in affective computing due to the complexity of spatial
and temporal facial dynamics. Its success is crucial for advancing applications
in human behavior understanding, healthcare monitoring, and human-computer
interaction. In this work, we propose FACET-VLM, a vision-language framework
for 3D/4D FER that integrates multiview facial representation learning with
semantic guidance from natural language prompts. FACET-VLM introduces three key
components: Cross-View Semantic Aggregation (CVSA) for view-consistent fusion,
Multiview Text-Guided Fusion (MTGF) for semantically aligned facial emotions,
and a multiview consistency loss to enforce structural coherence across views.
Our model achieves state-of-the-art accuracy across multiple benchmarks,
including BU-3DFE, Bosphorus, BU-4DFE, and BP4D-Spontaneous. We further extend
FACET-VLM to 4D micro-expression recognition (MER) on the 4DME dataset,
demonstrating strong performance in capturing subtle, short-lived emotional
cues. The extensive experimental results confirm the effectiveness and
substantial contributions of each individual component within the framework.
Overall, FACET-VLM offers a robust, extensible, and high-performing solution
for multimodal FER in both posed and spontaneous settings.

</details>


### [102] [Component Adaptive Clustering for Generalized Category Discovery](https://arxiv.org/abs/2507.01711)
*Mingfu Yan,Jiancheng Huang,Yifan Liu,Shifeng Chen*

Main category: cs.CV

TL;DR: AdaGCD是一种新颖的对比学习框架，通过自适应槽注意力（AdaSlot）动态确定类别数量，解决了广义类别发现（GCD）中传统方法预定义类别数的问题，并在开放世界场景中提高类别发现能力。


<details>
  <summary>Details</summary>
Motivation: 传统的广义类别发现（GCD）方法依赖于预定义类别数量等严格假设，这限制了它们处理真实世界数据固有变异性和复杂性的能力，尤其是在不知道未知类别数量的情况下。

Method: 提出AdaGCD，一个以聚类为中心的对比学习框架，它将自适应槽注意力（AdaSlot）整合到GCD框架中。AdaSlot能根据数据复杂度动态确定最佳槽数，从而无需预定义槽计数。该方法通过动态分配表示能力，促进了未标记数据向已知和新颖类别的灵活聚类，并能捕获实例特定和空间聚类特征。

Result: 在公共和细粒度数据集上进行了大量实验，验证了AdaGCD框架的有效性。实验结果强调了利用空间局部信息对未标记图像数据集进行类别发现的优势。

Conclusion: AdaGCD通过其自适应机制和对空间局部信息的有效利用，成功克服了传统GCD方法的局限性，在开放世界场景中显著提高了类别发现能力。

Abstract: Generalized Category Discovery (GCD) tackles the challenging problem of
categorizing unlabeled images into both known and novel classes within a
partially labeled dataset, without prior knowledge of the number of unknown
categories. Traditional methods often rely on rigid assumptions, such as
predefining the number of classes, which limits their ability to handle the
inherent variability and complexity of real-world data. To address these
shortcomings, we propose AdaGCD, a cluster-centric contrastive learning
framework that incorporates Adaptive Slot Attention (AdaSlot) into the GCD
framework. AdaSlot dynamically determines the optimal number of slots based on
data complexity, removing the need for predefined slot counts. This adaptive
mechanism facilitates the flexible clustering of unlabeled data into known and
novel categories by dynamically allocating representational capacity. By
integrating adaptive representation with dynamic slot allocation, our method
captures both instance-specific and spatially clustered features, improving
class discovery in open-world scenarios. Extensive experiments on public and
fine-grained datasets validate the effectiveness of our framework, emphasizing
the advantages of leveraging spatial local information for category discovery
in unlabeled image datasets.

</details>


### [103] [Using Wavelet Domain Fingerprints to Improve Source Camera Identification](https://arxiv.org/abs/2507.01712)
*Xinle Tian,Matthew Nunes,Emiko Dupont,Shaunagh Downing,Freddie Lichtenstein,Matt Burns*

Main category: cs.CV

TL;DR: 本文提出一种改进的小波域相机指纹提取方法，通过直接在小波域进行指纹构建和比较，提升了相机指纹检测的精度和速度。


<details>
  <summary>Details</summary>
Motivation: 相机指纹检测在源识别和图像取证中具有关键作用，其中小波去噪在提取传感器模式噪声（SPN）方面效果显著。

Method: 本研究修改了传统的小波域SPN提取方法，引入了“小波域指纹”概念，避免了去噪算法的最终反演步骤，并允许指纹直接在小波域进行比较，从而简化了提取和比较过程。

Result: 在真实数据集上的实验结果表明，该方法不仅实现了更高的检测精度，还显著提高了处理速度。

Conclusion: 所提出的基于小波域指纹的方法，通过优化SPN提取和比较流程，有效提升了相机指纹检测的性能和效率。

Abstract: Camera fingerprint detection plays a crucial role in source identification
and image forensics, with wavelet denoising approaches proving to be
particularly effective in extracting sensor pattern noise (SPN). In this
article, we propose a modification to wavelet-based SPN extraction. Rather than
constructing the fingerprint as an image, we introduce the notion of a wavelet
domain fingerprint. This avoids the final inversion step of the denoising
algorithm and allows fingerprint comparisons to be made directly in the wavelet
domain. As such, our modification streamlines the extraction and comparison
process. Experimental results on real-world datasets demonstrate that our
method not only achieves higher detection accuracy but can also significantly
improve processing speed.

</details>


### [104] [Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation](https://arxiv.org/abs/2507.01721)
*Zhongwen Zhang,Yuri Boykov*

Main category: cs.CV

TL;DR: 针对弱监督分割（仅有涂鸦标注），本文提出一种软自标注方法。通过优化CRF松弛并引入通用连续子问题求解器，该方法在标准架构下显著提升训练效果，超越了更复杂的弱监督系统，甚至在某些情况下优于全像素监督。


<details>
  <summary>Details</summary>
Motivation: 在弱监督分割中，现有高阶优化方法通常使用硬伪标签，无法有效表示类别的不确定性或错误。这促使研究者寻求一种能处理不确定性的软自标注方法。

Method: 本文推导了一种原则性的辅助损失用于软自标注。系统地评估了标准与新型CRF松弛（包括凸和非凸）、邻域系统以及连接网络预测与软伪标签的项。此外，还提出了一个通用的连续子问题求解器。

Result: 软自标注方法在使用标准架构的情况下，能持续改进基于涂鸦的训练效果，并且显著超越了更复杂的专用弱监督分割系统。值得注意的是，该方法在某些情况下甚至能超越全像素级监督的性能。

Conclusion: 软自标注是一种在弱监督分割中非常有效的方法，它能克服传统硬伪标签无法表示不确定性的局限性。其普适性使其理念可应用于其他弱监督问题和系统。

Abstract: We consider weakly supervised segmentation where only a fraction of pixels
have ground truth labels (scribbles) and focus on a self-labeling approach
optimizing relaxations of the standard unsupervised CRF/Potts loss on unlabeled
pixels. While WSSS methods can directly optimize such losses via gradient
descent, prior work suggests that higher-order optimization can improve network
training by introducing hidden pseudo-labels and powerful CRF sub-problem
solvers, e.g. graph cut. However, previously used hard pseudo-labels can not
represent class uncertainty or errors, which motivates soft self-labeling. We
derive a principled auxiliary loss and systematically evaluate standard and new
CRF relaxations (convex and non-convex), neighborhood systems, and terms
connecting network predictions with soft pseudo-labels. We also propose a
general continuous sub-problem solver. Using only standard architectures, soft
self-labeling consistently improves scribble-based training and outperforms
significantly more complex specialized WSSS systems. It can outperform full
pixel-precise supervision. Our general ideas apply to other weakly-supervised
problems/systems.

</details>


### [105] [When Does Pruning Benefit Vision Representations?](https://arxiv.org/abs/2507.01722)
*Enrico Cassano,Riccardo Renzulli,Andrea Bragagnolo,Marco Grangetto*

Main category: cs.CV

TL;DR: 本文探讨剪枝对视觉模型可解释性、无监督对象发现和人类感知对齐的影响。研究发现，存在剪枝的“最佳点”，可提升模型性能和对齐度，但这些点强烈依赖于网络架构，表明剪枝效果与多方面因素复杂交织。


<details>
  <summary>Details</summary>
Motivation: 剪枝是降低深度学习模型复杂度的常用方法，但其对模型可解释性和表示学习（特别是视觉模型的无监督对象发现和与人类感知的对齐）的影响尚不明确，缺乏深入理解。

Method: 本研究通过分析不同视觉网络架构及其不同稀疏度，从三个维度进行探究：(i) 考察剪枝对特征归因可解释性方法的影响；(ii) 探索剪枝是否促进更简洁和结构化的表示，从而改善无监督对象发现；(iii) 评估剪枝是否增强模型表示与人类感知的一致性。

Result: 研究发现，存在剪枝的“最佳点”，在此稀疏模型表现出更高的可解释性、下游泛化能力以及与人类感知更好的一致性。然而，这些最佳点高度依赖于网络架构及其可训练参数的规模。结果表明这三个维度之间存在复杂的相互作用。

Conclusion: 研究揭示了剪枝、可解释性和视觉表示之间复杂的相互作用。这强调了深入探究剪枝何时以及如何能有效提升视觉表示的重要性，提示剪枝的益处并非普适，而是依赖于具体条件。

Abstract: Pruning is widely used to reduce the complexity of deep learning models, but
its effects on interpretability and representation learning remain poorly
understood. This paper investigates how pruning influences vision models across
three key dimensions: (i) interpretability, (ii) unsupervised object discovery,
and (iii) alignment with human perception. We first analyze different vision
network architectures to examine how varying sparsity levels affect feature
attribution interpretability methods. Additionally, we explore whether pruning
promotes more succinct and structured representations, potentially improving
unsupervised object discovery by discarding redundant information while
preserving essential features. Finally, we assess whether pruning enhances the
alignment between model representations and human perception, investigating
whether sparser models focus on more discriminative features similarly to
humans. Our findings also reveal the presence of sweet spots, where sparse
models exhibit higher interpretability, downstream generalization and human
alignment. However, these spots highly depend on the network architectures and
their size in terms of trainable parameters. Our results suggest a complex
interplay between these three dimensions, highlighting the importance of
investigating when and how pruning benefits vision representations.

</details>


### [106] [HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion](https://arxiv.org/abs/2507.01737)
*Lin Wu,Zhixiang Chen,Jianglin Lan*

Main category: cs.CV

TL;DR: HOI-Dyn框架将3D人-物交互生成建模为驱动-响应系统，利用轻量级Transformer动态模型和残差动力学损失来生成更真实、物理上合理的交互，并在推理时保持高效，同时提供了一种新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 生成逼真的3D人-物交互（HOIs）极具挑战性，因为难以建模详细的交互动力学。现有方法独立处理人与物体的运动，导致生成的行为在物理上不合理且因果不一致。

Method: 提出HOI-Dyn框架，将人-物交互生成表述为驱动-响应系统，其中人类动作驱动物体响应。核心是一个轻量级的基于Transformer的交互动力学模型，用于明确预测物体如何对人体运动做出反应。引入残差动力学损失以减轻动力学预测误差并增强一致性。该动力学模型仅在训练阶段使用，以保持推理效率。

Result: 通过广泛的定性和定量实验证明，该方法不仅显著提高了人-物交互的生成质量，而且建立了一个评估生成交互质量的可行指标。

Conclusion: HOI-Dyn通过创新的驱动-响应系统和优化的动态建模，成功解决了3D人-物交互生成中的真实性和一致性问题，并在保持效率的同时，为未来的交互质量评估提供了新的途径。

Abstract: Generating realistic 3D human-object interactions (HOIs) remains a
challenging task due to the difficulty of modeling detailed interaction
dynamics. Existing methods treat human and object motions independently,
resulting in physically implausible and causally inconsistent behaviors. In
this work, we present HOI-Dyn, a novel framework that formulates HOI generation
as a driver-responder system, where human actions drive object responses. At
the core of our method is a lightweight transformer-based interaction dynamics
model that explicitly predicts how objects should react to human motion. To
further enforce consistency, we introduce a residual-based dynamics loss that
mitigates the impact of dynamics prediction errors and prevents misleading
optimization signals. The dynamics model is used only during training,
preserving inference efficiency. Through extensive qualitative and quantitative
experiments, we demonstrate that our approach not only enhances the quality of
HOI generation but also establishes a feasible metric for evaluating the
quality of generated interactions.

</details>


### [107] [DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy](https://arxiv.org/abs/2507.01738)
*Ming Dai,Wenxuan Cheng,Jiang-jiang Liu,Sen Yang,Wenxiao Cai,Yanpeng Sun,Wankou Yang*

Main category: cs.CV

TL;DR: 本文提出DeRIS框架，通过将指代图像分割(RIS)分解为感知和认知模块，系统分析发现现有模型主要瓶颈在于多模态认知能力不足。通过Loopback协同机制和数据增强，DeRIS提升了分割精度和图像-文本理解能力，并具有广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 现有指代图像分割(RIS)研究主要集中于视觉-语言交互和细粒度定位，但缺乏对当前RIS框架中根本性瓶颈的系统分析。

Method: ['提出DeRIS框架，将指代图像分割(RIS)任务分解为感知和认知两个关键模块。', '提出Loopback协同机制，增强感知和认知模块间的协同作用，以提高分割精度和图像-文本理解能力。', '引入一种简单的非指代样本转换数据增强方法，以解决通用场景中目标存在判断的长尾分布问题。']

Result: ['系统分析表明，现有RIS模型的主要限制不在于感知缺陷，而在于多模态认知能力不足。', 'DeRIS框架实现了精确的分割，并同时提升了鲁棒的图像-文本理解能力。', 'DeRIS无需专门的架构修改，即可适应非指代和多指代场景，增强了其普适性。']

Conclusion: DeRIS通过对指代图像分割任务进行分解分析，识别并解决了现有模型多模态认知能力不足的核心瓶颈。所提出的Loopback协同机制和数据增强方法显著提升了RIS性能和泛化能力。

Abstract: Referring Image Segmentation (RIS) is a challenging task that aims to segment
objects in an image based on natural language expressions. While prior studies
have predominantly concentrated on improving vision-language interactions and
achieving fine-grained localization, a systematic analysis of the fundamental
bottlenecks in existing RIS frameworks remains underexplored. To bridge this
gap, we propose DeRIS, a novel framework that decomposes RIS into two key
components: perception and cognition. This modular decomposition facilitates a
systematic analysis of the primary bottlenecks impeding RIS performance. Our
findings reveal that the predominant limitation lies not in perceptual
deficiencies, but in the insufficient multi-modal cognitive capacity of current
models. To mitigate this, we propose a Loopback Synergy mechanism, which
enhances the synergy between the perception and cognition modules, thereby
enabling precise segmentation while simultaneously improving robust image-text
comprehension. Additionally, we analyze and introduce a simple non-referent
sample conversion data augmentation to address the long-tail distribution issue
related to target existence judgement in general scenarios. Notably, DeRIS
demonstrates inherent adaptability to both non- and multi-referents scenarios
without requiring specialized architectural modifications, enhancing its
general applicability. The codes and models are available at
https://github.com/Dmmm1997/DeRIS.

</details>


### [108] [Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans](https://arxiv.org/abs/2507.01744)
*Benjamin Jin,Grant Mair,Joanna M. Wardlaw,Maria del C. Valdés Hernández*

Main category: cs.CV

TL;DR: 本研究首次将基于掩码自编码器（MAE）预训练的Vision Transformers（ViTs）应用于颅内动脉钙化（IAC）的3D医学图像分割。结果显示，该方法在性能和临床实用性上均超越了现有基线模型，并提高了对异构数据的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管Vision Transformers（ViTs）在自然图像领域取得了巨大成功，但在3D医学图像分割中的应用却不尽如人意。然而，鉴于3D医学图像数据量大且手动标注成本高昂，ViTs结合掩码自编码器（MAE）的自监督训练框架提供了一个无需昂贵标注即可利用数据的潜力。此外，颅内动脉钙化（IAC）是重要的神经血管疾病影像生物标志物，其自动化量化对于大规模风险评估具有重要临床意义。

Method: 本研究首次采用MAE预训练ViTs，并将其微调用于颅内动脉钙化（IAC）分割。模型开发使用了来自大型临床试验（第三次国际卒中试验，IST-3）的高度异构数据。研究评估了MAE预训练ViTs在IAC分割中的关键方面，并分析了其临床影响。

Result: ['校准后的自监督ViT在Dice分数上比强大的监督nnU-Net基线高出3.2点。', '对于IAC分割，ViTs的关键在于使用较小的补丁尺寸。', '对于基于ViT的模型，使用常规卷积进行插值上采样优于转置卷积。', 'ViTs增强了对更高切片厚度的鲁棒性。', '在临床场景中，ViTs将风险组分类的准确率提高了46%。']

Conclusion: 本研究表明，通过MAE进行自监督预训练的ViTs在3D医学图像分割中表现卓越，尤其是在颅内动脉钙化（IAC）分割任务上。它们不仅在技术指标上超越了监督基线，还在处理数据异构性方面展现出更强的鲁棒性，并显著提升了临床风险评估的准确性。这为解决3D医学图像标注稀缺问题提供了有效且有前景的解决方案。

Abstract: Vision Transformers (ViTs) have gained significant popularity in the natural
image domain but have been less successful in 3D medical image segmentation.
Nevertheless, 3D ViTs are particularly interesting for large medical imaging
volumes due to their efficient self-supervised training within the masked
autoencoder (MAE) framework, which enables the use of imaging data without the
need for expensive manual annotations. intracranial arterial calcification
(IAC) is an imaging biomarker visible on routinely acquired CT scans linked to
neurovascular diseases such as stroke and dementia, and automated IAC
quantification could enable their large-scale risk assessment. We pre-train
ViTs with MAE and fine-tune them for IAC segmentation for the first time. To
develop our models, we use highly heterogeneous data from a large clinical
trial, the third International Stroke Trial (IST-3). We evaluate key aspects of
MAE pre-trained ViTs in IAC segmentation, and analyse the clinical
implications. We show: 1) our calibrated self-supervised ViT beats a strong
supervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial
for ViTs for IAC segmentation and interpolation upsampling with regular
convolutions is preferable to transposed convolutions for ViT-based models, and
3) our ViTs increase robustness to higher slice thicknesses and improve risk
group classification in a clinical scenario by 46%. Our code is available
online.

</details>


### [109] [SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery](https://arxiv.org/abs/2507.01747)
*Nora Gourmelon,Marcel Dreier,Martin Mayr,Thorsten Seehaus,Dakota Pyles,Matthias Braun,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 为解决现有深度学习模型在冰川崩解锋面监测中因领域偏移导致的性能不足，本研究提出两种自监督多模态预训练技术和一个结合Swin Transformer与CNN的新型混合模型架构，并在新数据集SSL4SAR上进行预训练。该方法显著提高了崩解锋面识别精度，接近人类水平，从而实现了冰川季节性变化的精确监测。


<details>
  <summary>Details</summary>
Motivation: 冰川正以空前速度流失冰量，急需准确、全年监测以理解冰川正面消融和崩解过程。现有深度学习模型依赖ImageNet预训练权重，但合成孔径雷达（SAR）遥感图像与自然图像存在领域偏移，导致模型性能受限。

Method: 1. 提出两种新颖的自监督多模态预训练技术，并利用新构建的无标签数据集SSL4SAR（包含9,563张Sentinel-1和14张Sentinel-2北极冰川图像）。2. 引入一种新型混合模型架构，该架构结合了Swin Transformer编码器与残差卷积神经网络（CNN）解码器。

Result: 1. 在SSL4SAR上预训练后，该模型在“CaFFe”基准数据集上实现了293米的平均距离误差，比此前最佳模型提高了67米。2. 经多标注者研究评估，该模型的集成版本平均距离误差为75米，接近人类性能（38米）。

Conclusion: 该研究成果使得对冰川崩解锋面季节性变化的精确监测成为可能。

Abstract: Glaciers are losing ice mass at unprecedented rates, increasing the need for
accurate, year-round monitoring to understand frontal ablation, particularly
the factors driving the calving process. Deep learning models can extract
calving front positions from Synthetic Aperture Radar imagery to track seasonal
ice losses at the calving fronts of marine- and lake-terminating glaciers. The
current state-of-the-art model relies on ImageNet-pretrained weights. However,
they are suboptimal due to the domain shift between the natural images in
ImageNet and the specialized characteristics of remote sensing imagery, in
particular for Synthetic Aperture Radar imagery. To address this challenge, we
propose two novel self-supervised multimodal pretraining techniques that
leverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14
Sentinel-2 images of Arctic glaciers, with one optical image per glacier in the
dataset. Additionally, we introduce a novel hybrid model architecture that
combines a Swin Transformer encoder with a residual Convolutional Neural
Network (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean
distance error of 293 m on the "CAlving Fronts and where to Find thEm" (CaFFe)
benchmark dataset, outperforming the prior best model by 67 m. Evaluating an
ensemble of the proposed model on a multi-annotator study of the benchmark
dataset reveals a mean distance error of 75 m, approaching the human
performance of 38 m. This advancement enables precise monitoring of seasonal
changes in glacier calving fronts.

</details>


### [110] [Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis](https://arxiv.org/abs/2507.01756)
*Peng Zheng,Junke Wang,Yi Chang,Yizhou Yu,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: DisCon是一种新型自回归模型，通过将离散标记作为连续表示的条件信号，解决了视觉生成中离散标记的信息损失和连续标记的建模挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基于自回归（AR）的视觉生成模型中，离散标记的量化过程会引入信息损失，降低图像保真度。虽然研究尝试使用连续标记，但其无界高维特性使得密度估计困难，易产生分布外伪影。

Method: 本文提出了DisCon（Discrete-Conditioned Continuous Autoregressive Model）框架。DisCon将离散标记重新解释为连续表示的条件信号，而非直接的生成目标。通过建模以离散标记为条件的连续表示的条件概率，DisCon规避了连续标记建模的优化难题，同时避免了量化导致的信息损失。

Result: DisCon在ImageNet 256×256图像生成任务中取得了1.38的gFID分数，显著优于现有最先进的自回归方法。

Conclusion: DisCon提供了一种有效的方法，克服了传统自回归视觉生成模型中离散标记的信息损失和连续标记的建模挑战，实现了更高质量的图像生成。

Abstract: Recent advances in large language models (LLMs) have spurred interests in
encoding images as discrete tokens and leveraging autoregressive (AR)
frameworks for visual generation. However, the quantization process in AR-based
visual generation models inherently introduces information loss that degrades
image fidelity. To mitigate this limitation, recent studies have explored to
autoregressively predict continuous tokens. Unlike discrete tokens that reside
in a structured and bounded space, continuous representations exist in an
unbounded, high-dimensional space, making density estimation more challenging
and increasing the risk of generating out-of-distribution artifacts. Based on
the above findings, this work introduces DisCon (Discrete-Conditioned
Continuous Autoregressive Model), a novel framework that reinterprets discrete
tokens as conditional signals rather than generation targets. By modeling the
conditional probability of continuous representations conditioned on discrete
tokens, DisCon circumvents the optimization challenges of continuous token
modeling while avoiding the information loss caused by quantization. DisCon
achieves a gFID score of 1.38 on ImageNet 256$\times$256 generation,
outperforming state-of-the-art autoregressive approaches by a clear margin.

</details>


### [111] [Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging](https://arxiv.org/abs/2507.01788)
*Montasir Shams,Chashi Mahiul Islam,Shaeke Salman,Phat Tran,Xiuwen Liu*

Main category: cs.CV

TL;DR: 本研究首次发现，尽管Vision Transformers (ViTs) 在医学影像任务中表现出色，但其内部表征不具备语义意义且对微小扰动极其敏感，导致分类结果不可靠，这对其在安全关键系统中的部署构成重大挑战。


<details>
  <summary>Details</summary>
Motivation: ViTs在医学影像任务中展现出卓越性能，但由于其复杂的自注意力机制，人们对其内部运作方式和表征是否具有语义意义缺乏深入理解，尤其是在需要高可靠性的安全关键应用中，这一点尤为重要。

Method: 采用基于投影梯度的算法（projected gradient-based algorithm）来分析ViT模型的内部表征。

Result: 研究发现ViT的表征不具备语义意义，并且对微小变化固有脆弱：微不可察的图像差异可能导致截然不同的表征，而属于不同语义类别的图像却能产生几乎相同的表征。这种脆弱性显著降低了分类可靠性，例如，微小扰动可使分类准确率下降超过60%。

Conclusion: 本研究首次系统性地证明了ViT在医学图像分类中表征缺乏语义意义这一根本问题，揭示了ViT在安全关键系统部署中面临的严峻挑战。

Abstract: Vision transformers (ViTs) have rapidly gained prominence in medical imaging
tasks such as disease classification, segmentation, and detection due to their
superior accuracy compared to conventional deep learning models. However, due
to their size and complex interactions via the self-attention mechanism, they
are not well understood. In particular, it is unclear whether the
representations produced by such models are semantically meaningful. In this
paper, using a projected gradient-based algorithm, we show that their
representations are not semantically meaningful and they are inherently
vulnerable to small changes. Images with imperceptible differences can have
very different representations; on the other hand, images that should belong to
different semantic classes can have nearly identical representations. Such
vulnerability can lead to unreliable classification results; for example,
unnoticeable changes cause the classification accuracy to be reduced by over
60\%. %. To the best of our knowledge, this is the first work to systematically
demonstrate this fundamental lack of semantic meaningfulness in ViT
representations for medical image classification, revealing a critical
challenge for their deployment in safety-critical systems.

</details>


### [112] [Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation](https://arxiv.org/abs/2507.01791)
*Zihong Guo,Chen Wan,Yayin Zheng,Hailing Kuang,Xiaohai Lu*

Main category: cs.CV

TL;DR: 提出SGP多尺度攻击方法，显著提升对抗样本可迁移性，有效攻击黑盒防御模型。


<details>
  <summary>Details</summary>
Motivation: 对抗性样本的可迁移性对深度神经网络的安全性构成重大挑战，使其在未知模型信息下被攻击。现有攻击方法在针对防御模型时，其可迁移性不足。

Method: 本文提出分段高斯金字塔（SGP）攻击方法以增强可迁移性。该方法通过高斯滤波和三种下采样构建多尺度样本，计算各尺度损失函数梯度并取平均值以确定对抗扰动。SGP被设计为一种高可扩展的输入变换，易于集成到现有对抗攻击中。

Result: 广泛实验表明，与现有最先进方法相比，SGP显著提升了对黑盒防御模型的攻击成功率，平均攻击成功率仅基于可迁移性就增加了2.3%至32.6%。

Conclusion: SGP通过多尺度处理，有效增强了对抗性样本的可迁移性，尤其在攻击黑盒防御模型方面表现出色，为提高对抗攻击效能提供了一种新颖且高效的方法。

Abstract: The transferability of adversarial examples poses a significant security
challenge for deep neural networks, which can be attacked without knowing
anything about them. In this paper, we propose a new Segmented Gaussian Pyramid
(SGP) attack method to enhance the transferability, particularly against
defense models. Unlike existing methods that generally focus on single-scale
images, our approach employs Gaussian filtering and three types of downsampling
to construct a series of multi-scale examples. Then, the gradients of the loss
function with respect to each scale are computed, and their average is used to
determine the adversarial perturbations. The proposed SGP can be considered an
input transformation with high extensibility that is easily integrated into
most existing adversarial attacks. Extensive experiments demonstrate that in
contrast to the state-of-the-art methods, SGP significantly enhances attack
success rates against black-box defense models, with average attack success
rates increasing by 2.3% to 32.6%, based only on transferability.

</details>


### [113] [FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization](https://arxiv.org/abs/2507.01792)
*Peng Zheng,Ye Wang,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: FreeLoRA是一个框架，通过对LoRA模块进行全Token微调并结合主题感知推理，实现多主题图像的训练无关融合，解决了现有方法在多主题个性化方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有主体驱动图像生成方法（如虚拟试穿、海报设计）在处理多主题个性化时面临困难，因为独立调整的模块组合需要复杂的重新调整或联合优化。

Method: 提出FreeLoRA框架，通过“全Token微调”策略为每个主题独立适应LoRA模块，鼓励弱监督的Token-内容对齐。推理时采用“主题感知推理”，仅在对应主题Token上激活模块，实现多主题的训练无关融合。

Result: FreeLoRA能够实现单图像中多个个性化主题的训练无关融合，并有效缓解过拟合和主题间的相互干扰。广泛实验表明，该方法在主题保真度和提示一致性方面均表现出色。

Conclusion: FreeLoRA提供了一个简单且泛化的框架，有效解决了多主题个性化图像生成中的融合难题，并在实践中展示了优异的性能。

Abstract: Subject-driven image generation plays a crucial role in applications such as
virtual try-on and poster design. Existing approaches typically fine-tune
pretrained generative models or apply LoRA-based adaptations for individual
subjects. However, these methods struggle with multi-subject personalization,
as combining independently adapted modules often requires complex re-tuning or
joint optimization. We present FreeLoRA, a simple and generalizable framework
that enables training-free fusion of subject-specific LoRA modules for
multi-subject personalization. Each LoRA module is adapted on a few images of a
specific subject using a Full Token Tuning strategy, where it is applied across
all tokens in the prompt to encourage weakly supervised token-content
alignment. At inference, we adopt Subject-Aware Inference, activating each
module only on its corresponding subject tokens. This enables training-free
fusion of multiple personalized subjects within a single image, while
mitigating overfitting and mutual interference between subjects. Extensive
experiments show that FreeLoRA achieves strong performance in both subject
fidelity and prompt consistency.

</details>


### [114] [HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision](https://arxiv.org/abs/2507.01800)
*Shengli Zhou,Jianuo Zhu,Qilin Huang,Fangjing Wang,Yanfu Zhang,Feng Zheng*

Main category: cs.CV

TL;DR: 为解决3D VQA中推理路径缺乏监督的问题，本文提出了HCNQA模型。该模型模仿人类注意力聚焦过程，采用分层集中收缩监督，引导模型建立合理推理路径，并显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 3D VQA中，现有的答案中心监督方法仅监督最终输出，导致模型推理路径自由发展，容易产生表面化捷径，且存在“思考不足”问题，缺乏对推理过程的有效监督。

Method: 提出HCNQA模型，采用分层集中收缩监督。该方法模仿人类从广域到特定对象的逐步聚焦过程，通过分层监督在推理路径的关键检查点引导模型进行三阶段的集中收缩，以确保形成合理有效的推理路径。

Result: 广泛的实验结果表明，所提出的HCNQA方法能有效促使模型发展出合理的推理路径，并实现更优异的性能。

Conclusion: HCNQA通过对模型推理路径进行分层监督，成功解决了传统答案中心监督方法的局限性，使得3D VQA模型能构建更合理、高效的推理过程，从而提升其在物理世界感知和空间推理任务中的表现。

Abstract: 3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the
physical world and perform spatial reasoning. Answer-centric supervision is a
commonly used training method for 3D VQA models. Many models that utilize this
strategy have achieved promising results in 3D VQA tasks. However, the
answer-centric approach only supervises the final output of models and allows
models to develop reasoning pathways freely. The absence of supervision on the
reasoning pathway enables the potential for developing superficial shortcuts
through common patterns in question-answer pairs. Moreover, although
slow-thinking methods advance large language models, they suffer from
underthinking. To address these issues, we propose \textbf{HCNQA}, a 3D VQA
model leveraging a hierarchical concentration narrowing supervision method. By
mimicking the human process of gradually focusing from a broad area to specific
objects while searching for answers, our method guides the model to perform
three phases of concentration narrowing through hierarchical supervision. By
supervising key checkpoints on a general reasoning pathway, our method can
ensure the development of a rational and effective reasoning pathway. Extensive
experimental results demonstrate that our method can effectively ensure that
the model develops a rational reasoning pathway and performs better. The code
is available at https://github.com/JianuoZhu/HCNQA.

</details>


### [115] [AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction](https://arxiv.org/abs/2507.01801)
*Bin Rao,Haicheng Liao,Yanchen Guan,Chengyue Wang,Bonan Wang,Jiaxun Zhang,Zhenning Li*

Main category: cs.CV

TL;DR: 本文提出一种自适应动量解耦对比学习框架（AMD），通过融合无监督和有监督对比学习策略，结合数据增强和在线聚类，显著提升了自动驾驶中长尾轨迹的预测性能及整体预测精度。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，准确预测交通参与者轨迹至关重要。然而，轨迹分布不平衡导致长尾数据代表更复杂和危险的场景，现有方法仅依赖预测误差，未充分考虑长尾轨迹模式的多样性和不确定性。

Method: 本文提出AMD框架，整合改进的动量对比学习（MoCo-DT）和解耦对比学习（DCL）模块，以增强模型对稀有复杂轨迹的识别能力。同时，设计四种轨迹随机增强方法，并引入在线迭代聚类策略，动态更新伪标签以适应长尾数据分布。定义三种长尾轨迹判别标准，并在nuScenes和ETH/UCY数据集上进行广泛实验。

Result: 实验结果表明，AMD框架不仅在长尾轨迹预测上取得了最优性能，而且在整体预测精度上表现出色。

Conclusion: AMD框架通过创新的对比学习机制、数据增强和动态伪标签策略，有效解决了自动驾驶中长尾轨迹预测的挑战，显著提升了模型对稀有复杂轨迹的识别能力及预测准确性。

Abstract: Accurately predicting the future trajectories of traffic agents is essential
in autonomous driving. However, due to the inherent imbalance in trajectory
distributions, tail data in natural datasets often represents more complex and
hazardous scenarios. Existing studies typically rely solely on a base model's
prediction error, without considering the diversity and uncertainty of
long-tail trajectory patterns. We propose an adaptive momentum and decoupled
contrastive learning framework (AMD), which integrates unsupervised and
supervised contrastive learning strategies. By leveraging an improved momentum
contrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module,
our framework enhances the model's ability to recognize rare and complex
trajectories. Additionally, we design four types of trajectory random
augmentation methods and introduce an online iterative clustering strategy,
allowing the model to dynamically update pseudo-labels and better adapt to the
distributional shifts in long-tail data. We propose three different criteria to
define long-tail trajectories and conduct extensive comparative experiments on
the nuScenes and ETH$/$UCY datasets. The results show that AMD not only
achieves optimal performance in long-tail trajectory prediction but also
demonstrates outstanding overall prediction accuracy.

</details>


### [116] [Modulate and Reconstruct: Learning Hyperspectral Imaging from Misaligned Smartphone Views](https://arxiv.org/abs/2507.01835)
*Daniil Reutsky,Daniil Vladimirov,Yasin Mamedov,Georgy Perevozchikov,Nancy Mehta,Egor Ershov,Radu Timofte*

Main category: cs.CV

TL;DR: 针对RGB图像超光谱重建的固有难题，本文提出一种基于三摄智能手机和光谱滤光片的多图像超光谱重建（MI-HSR）框架，并发布首个MI-HSR数据集Doomer。结果显示，该方法相比现有技术能更准确地估计光谱，精度提升30%，为实用超光谱成像提供了新方案。


<details>
  <summary>Details</summary>
Motivation: 从单张RGB图像进行超光谱重建（HSR）由于光谱信息严重丢失而本质上是病态问题，现有方法依赖单一RGB图像，限制了重建精度。

Method: 提出一种新颖的多图像超光谱重建（MI-HSR）框架，利用三摄智能手机系统，其中两个镜头配备精心选择的光谱滤光片，以获取比传统单相机设置更丰富、更多样的光谱观测。为支持该范式，引入了首个MI-HSR数据集Doomer。

Result: 所提出的HSR模型在新的基准测试中，比现有方法取得了持续改进。与普通RGB相机相比，该设置可使光谱估计精度提高30%。

Conclusion: 研究表明，结合商品硬件的多视角光谱滤波技术，可以解锁更准确、更实用的超光谱成像解决方案。

Abstract: Hyperspectral reconstruction (HSR) from RGB images is a fundamentally
ill-posed problem due to severe spectral information loss. Existing approaches
typically rely on a single RGB image, limiting reconstruction accuracy. In this
work, we propose a novel multi-image-to-hyperspectral reconstruction (MI-HSR)
framework that leverages a triple-camera smartphone system, where two lenses
are equipped with carefully selected spectral filters. Our configuration,
grounded in theoretical and empirical analysis, enables richer and more diverse
spectral observations than conventional single-camera setups. To support this
new paradigm, we introduce Doomer, the first dataset for MI-HSR, comprising
aligned images from three smartphone cameras and a hyperspectral reference
camera across diverse scenes. We show that the proposed HSR model achieves
consistent improvements over existing methods on the newly proposed benchmark.
In a nutshell, our setup allows 30% towards more accurately estimated spectra
compared to an ordinary RGB camera. Our findings suggest that multi-view
spectral filtering with commodity hardware can unlock more accurate and
practical hyperspectral imaging solutions.

</details>


### [117] [MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices](https://arxiv.org/abs/2507.01838)
*Hailong Yan,Ao Li,Xiangtao Zhang,Zhe Liu,Zenglin Shi,Ce Zhu,Le Zhang*

Main category: cs.CV

TL;DR: 本文提出一个超轻量级CNN框架，仅约4K参数，用于在移动设备上实现实时图像增强，首次达到高达1100 FPS的推理速度，并在速度和性能之间实现了最佳权衡。


<details>
  <summary>Details</summary>
Motivation: 深度学习在图像增强(IE)方面取得了显著进展，但由于高计算和内存需求，在移动设备等资源受限平台上部署深度学习模型仍面临挑战。

Method: 引入一个参数量约4K的超轻量级卷积神经网络(CNN)框架，通过结合重参数化和增量权重优化策略来确保效率。此外，通过特征自变换模块和分层双路径注意力机制增强性能，并使用局部方差加权损失进行优化。

Result: 该框架首次实现了高达1100帧/秒(FPS)的实时图像增强推理，同时提供了具有竞争力的图像质量，在多个IE任务中实现了速度和性能的最佳权衡。

Conclusion: 所提出的高效轻量级框架成功解决了移动设备上实时图像增强的挑战，在保持高推理速度的同时，提供了优秀的图像质量和性能平衡。

Abstract: Recent advancements in deep neural networks have driven significant progress
in image enhancement (IE). However, deploying deep learning models on
resource-constrained platforms, such as mobile devices, remains challenging due
to high computation and memory demands. To address these challenges and
facilitate real-time IE on mobile, we introduce an extremely lightweight
Convolutional Neural Network (CNN) framework with around 4K parameters. Our
approach integrates reparameterization with an Incremental Weight Optimization
strategy to ensure efficiency. Additionally, we enhance performance with a
Feature Self-Transform module and a Hierarchical Dual-Path Attention mechanism,
optimized with a Local Variance-Weighted loss. With this efficient framework,
we are the first to achieve real-time IE inference at up to 1,100 frames per
second (FPS) while delivering competitive image quality, achieving the best
trade-off between speed and performance across multiple IE tasks. The code will
be available at https://github.com/AVC2-UESTC/MobileIE.git.

</details>


### [118] [Future Slot Prediction for Unsupervised Object Discovery in Surgical Video](https://arxiv.org/abs/2507.01882)
*Guiqiu Liao,Matjaz Jogan,Marcel Hussing,Edward Zhang,Eric Eaton,Daniel A. Hashimoto*

Main category: cs.CV

TL;DR: 针对手术视频中目标中心插槽注意力模型在异构场景下表现不佳的问题，本文提出了动态时间插槽转换器（DTST），实现了最先进的性能，证明了无监督目标中心方法在真实世界医疗应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 目标中心插槽注意力模型（Object-centric slot attention）是一种用于无监督学习结构化、可解释目标中心表示的新范式，它能以低计算成本实现有效的对象和事件推理，因此适用于实时解读手术视频等关键医疗应用。然而，手术等真实世界应用的异构场景难以解析成有意义的插槽集，当前具有自适应插槽计数的方法在图像上表现良好，但在手术视频上的性能较低。

Method: 为解决上述挑战，本文提出了一种动态时间插槽转换器（Dynamic Temporal Slot Transformer, DTST）模块。该模块经过训练，不仅能进行时间推理，还能预测最优的未来插槽初始化。

Result: 该模型在多个手术数据库上取得了最先进的性能。

Conclusion: 研究结果表明，无监督的目标中心方法可以成功应用于真实世界数据，并成为医疗应用中的常用工具。

Abstract: Object-centric slot attention is an emerging paradigm for unsupervised
learning of structured, interpretable object-centric representations (slots).
This enables effective reasoning about objects and events at a low
computational cost and is thus applicable to critical healthcare applications,
such as real-time interpretation of surgical video. The heterogeneous scenes in
real-world applications like surgery are, however, difficult to parse into a
meaningful set of slots. Current approaches with an adaptive slot count perform
well on images, but their performance on surgical videos is low. To address
this challenge, we propose a dynamic temporal slot transformer (DTST) module
that is trained both for temporal reasoning and for predicting the optimal
future slot initialization. The model achieves state-of-the-art performance on
multiple surgical databases, demonstrating that unsupervised object-centric
methods can be applied to real-world data and become part of the common arsenal
in healthcare applications.

</details>


### [119] [Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification](https://arxiv.org/abs/2507.01884)
*Kunlun Xu,Fan Zhuo,Jiangmeng Li,Xu Zou,Jiahuan Zhou*

Main category: cs.CV

TL;DR: 本文提出SPRED框架，通过原型引导的伪标签生成和新旧知识协作净化的自强化循环，有效利用未标注数据，解决了半监督终身行人重识别(Semi-LReID)中噪声知识问题，并实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前终身行人重识别(LReID)方法主要依赖于完全标注的数据流。然而，在真实世界中，标注资源有限，大量未标注数据与稀缺的标注样本共存，导致半监督LReID (Semi-LReID)问题。现有LReID方法，即使结合半监督策略，也因利用未标注数据过程中产生的噪声知识而导致长期适应性性能严重下降。

Method: 本文率先研究Semi-LReID，并引入了新颖的自强化原型演化与双知识协作框架(SPRED)。核心创新在于建立了一个自强化循环，该循环结合了动态原型引导的伪标签生成和新旧知识协作净化，以增强未标注数据的利用。具体而言，引入可学习的身份原型来动态捕获身份分布并生成高质量伪标签；随后，双知识协作方案整合当前模型特化和历史模型泛化，以提炼噪声伪标签。通过这种循环设计，可靠的伪标签被逐步挖掘，以改善当前阶段的学习并确保长期学习中知识的积极传播。

Result: 在已建立的Semi-LReID基准测试上进行的实验表明，SPRED实现了最先进的性能。

Conclusion: SPRED框架通过其创新的自强化循环和双知识协作机制，有效解决了半监督终身行人重识别中未标注数据利用导致的噪声知识问题，实现了对未标注数据的有效利用和长期适应性，并在相关基准测试中取得了领先的性能。

Abstract: Current lifelong person re-identification (LReID) methods predominantly rely
on fully labeled data streams. However, in real-world scenarios where
annotation resources are limited, a vast amount of unlabeled data coexists with
scarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)
problem where LReID methods suffer severe performance degradation. Existing
LReID methods, even when combined with semi-supervised strategies, suffer from
limited long-term adaptation performance due to struggling with the noisy
knowledge occurring during unlabeled data utilization. In this paper, we
pioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing
Prototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key
innovation lies in establishing a self-reinforcing cycle between dynamic
prototype-guided pseudo-label generation and new-old knowledge collaborative
purification to enhance the utilization of unlabeled data. Specifically,
learnable identity prototypes are introduced to dynamically capture the
identity distributions and generate high-quality pseudo-labels. Then, the
dual-knowledge cooperation scheme integrates current model specialization and
historical model generalization, refining noisy pseudo-labels. Through this
cyclic design, reliable pseudo-labels are progressively mined to improve
current-stage learning and ensure positive knowledge propagation over long-term
learning. Experiments on the established Semi-LReID benchmarks show that our
SPRED achieves state-of-the-art performance. Our source code is available at
https://github.com/zhoujiahuan1991/ICCV2025-SPRED

</details>


### [120] [Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning](https://arxiv.org/abs/2507.01908)
*Qingdong He,Xueqin Chen,Chaoyi Wang,Yanjie Pan,Xiaobin Hu,Zhenye Gan,Yabiao Wang,Chengjie Wang,Xiangtai Li,Jiangning Zhang*

Main category: cs.CV

TL;DR: 本文提出Reason50K数据集和ReasonBrain框架，旨在解决现有指令图像编辑（IIE）在处理复杂、隐式假想指令时面临的推理和细粒度信息提取不足的问题，显著提升了IIE的推理能力和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有指令图像编辑（IIE）方法主要侧重于简单、显式指令（如添加、删除、移动、交换对象），难以处理需要深度推理来推断视觉变化和用户意图的复杂隐式假想指令。此外，当前数据集和模型架构在支持训练和评估推理能力以及细粒度细节提取方面存在局限性。

Method: 1. 提出了Reason50K，一个包含超过5万个样本的大规模数据集，专注于假想指令推理图像编辑，涵盖物理、时间、因果和故事推理四种场景。2. 设计了ReasonBrain框架，该框架利用多模态大语言模型（MLLMs）生成编辑指导，并使用扩散模型进行图像合成。3. 引入了细粒度推理线索提取（FRCE）模块以捕捉支持指令推理的详细视觉和文本语义。4. 进一步提出了跨模态增强器（CME）以减轻语义损失，并促进细粒度线索与MLLM派生特征之间的丰富交互。

Result: 实验结果表明，ReasonBrain在各种推理场景下持续优于现有基线模型，并在传统IIE任务上展现出强大的零样本泛化能力。数据集和代码将公开发布。

Conclusion: 通过Reason50K数据集和ReasonBrain框架，本文成功地解决了指令图像编辑在处理复杂假想指令时的推理能力不足问题，显著提升了模型的性能和泛化能力，为推理感知型IIE领域带来了重要进展。

Abstract: Instruction-based image editing (IIE) has advanced rapidly with the success
of diffusion models. However, existing efforts primarily focus on simple and
explicit instructions to execute editing operations such as adding, deleting,
moving, or swapping objects. They struggle to handle more complex implicit
hypothetical instructions that require deeper reasoning to infer plausible
visual changes and user intent. Additionally, current datasets provide limited
support for training and evaluating reasoning-aware editing capabilities.
Architecturally, these methods also lack mechanisms for fine-grained detail
extraction that support such reasoning. To address these limitations, we
propose Reason50K, a large-scale dataset specifically curated for training and
evaluating hypothetical instruction reasoning image editing, along with
ReasonBrain, a novel framework designed to reason over and execute implicit
hypothetical instructions across diverse scenarios. Reason50K includes over 50K
samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and
Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)
for editing guidance generation and a diffusion model for image synthesis,
incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture
detailed visual and textual semantics essential for supporting instruction
reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal
Enhancer (CME) that enables rich interactions between the fine-grained cues and
MLLM-derived features. Extensive experiments demonstrate that ReasonBrain
consistently outperforms state-of-the-art baselines on reasoning scenarios
while exhibiting strong zero-shot generalization to conventional IIE tasks. Our
dataset and code will be released publicly.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [121] [Rethinking the Illusion of Thinking](https://arxiv.org/abs/2507.01231)
*Iñaki Dellibarda Varela,Pablo Romero-Sorozabal,Eduardo Rocon,Manuel Cebrian*

Main category: cs.AI

TL;DR: 本文通过复现并改进苹果公司论文中的汉诺塔和过河问题基准，澄清了大型推理模型（LRMs）的推理能力争议。发现汉诺塔问题失败部分源于认知限制，而过河问题失败则因测试了不可解配置。结论是LRMs是随机、RL调优的搜索器，需要更精细的研究。


<details>
  <summary>Details</summary>
Motivation: 苹果公司发布的“思维的幻觉”引发了关于大型推理模型（LRMs）是否具备真正推理能力的激烈争论。评论者认为LRMs缺乏推理能力，仅是“随机鹦鹉”，而捍卫者则批评实验设置有缺陷、结论被夸大。本研究旨在澄清这一争议。

Method: 复现并改进了原始研究中最具争议的两个基准测试：汉诺塔（Towers of Hanoi）和过河问题（River Crossing）。具体方法包括引入增量逐步提示（incremental stepwise prompting）和智能体协作对话（agentic collaborative dialogue）。

Result: 1. 汉诺塔问题：模型失败不仅是输出限制的结果，也部分源于认知局限性；当复杂度适度增加（大约8个盘子）时，LRMs仍会遇到困难。
2. 过河问题：最初的灾难性失败结果是由于测试了不可解的配置；一旦测试仅限于可解问题，LRMs能够轻松解决涉及100多对智能体的大规模实例。

Conclusion: 目前的LRMs是离散状态空间中的随机、经强化学习（RL）调整的搜索器，其行为复杂性尚待深入理解。在符号化、长程推理方面取得真正进展，需要通过本文介绍的精细消融分析来探索和理解这个领域。

Abstract: Earlier this year, Apple ignited controversy by publishing "The Illusion of
Thinking," prompting heated debate within the AI community. Critics seized upon
the findings as conclusive evidence that Large Reasoning Models (LRMs) lack
genuine reasoning capabilities, branding them as mere stochastic parrots.
Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning
the experimental setup as flawed and the conclusions overstated. We clarify
this debate by replicating and refining two of the original study's most
contentious benchmarks: Towers of Hanoi and River Crossing. By introducing
incremental stepwise prompting and agentic collaborative dialogue, we show that
previously reported failures solving the Towers of Hanoi were not purely result
of output constraints, but also partly a result of cognition limitations: LRMs
still stumble when complexity rises moderately (around 8 disks). Moreover, the
River Crossing results initially heralded as catastrophic failures turn out to
hinge upon testing unsolvable configurations. Once we limit tests strictly to
solvable problems-LRMs effortlessly solve large instances involving over 100
agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs
are stochastic, RL-tuned searchers in a discrete state space we barely
understand. Real progress in symbolic, long-horizon reasoning demands mapping
that terrain through fine-grained ablations like those introduced here.

</details>


### [122] [Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care](https://arxiv.org/abs/2507.01282)
*Matthew JY Kang,Wenli Yang,Monica R Roberts,Byeong Ho Kang,Charles B Malpas*

Main category: cs.AI

TL;DR: 本文通过范围综述指出，尽管LLMs在基准测试中表现出色，但在实际临床（特别是痴呆诊断）中应用有限，主要由于缺乏透明度、易产生幻觉和因果推理弱。未来AI应转向混合式、可解释且以人为本的方法，并以临床理解、工作流适应和患者结局作为衡量标准。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在医学诊断领域展现出巨大潜力，但在实际临床环境中，尤其是在痴呆诊断与护理方面，尚未带来可衡量的实用改进。因此，研究旨在识别AI在临床应用中面临的实际局限性。

Method: 本文采用范围综述（scoping review）的方法，对现有文献进行分析，旨在识别和强调AI在临床环境中（特别是痴呆诊断和护理领域）做出实际贡献的局限性。

Result: 独立的机器学习模型擅长模式识别，但难以提供可操作、可解释的指导，从而降低临床医生信任。医生辅助使用LLMs未能提高诊断准确性或速度。主要限制源于数据驱动范式：输出不透明（黑箱）、易产生幻觉、因果推理能力弱。混合方法（结合统计学习和专家规则知识，并融入临床医生参与）能提升可解释性并更好地适应现有临床工作流程。

Conclusion: 未来的决策支持系统应优先考虑解释性连贯性，通过神经-符号或混合AI将预测与临床原因联系起来。未来的研究不仅应关注准确性，还应以提升临床医生理解、工作流程适应性和患者结局为衡量标准，并深入理解人机交互以促进AI在临床实践中的应用。

Abstract: The recent boom of large language models (LLMs) has re-ignited the hope that
artificial intelligence (AI) systems could aid medical diagnosis. Yet despite
dazzling benchmark scores, LLM assistants have yet to deliver measurable
improvements at the bedside. This scoping review aims to highlight the areas
where AI is limited to make practical contributions in the clinical setting,
specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom
provide actionable, interpretable guidance, eroding clinician trust. Adjacent
use of LLMs by physicians did not result in better diagnostic accuracy or
speed. Key limitations trace to the data-driven paradigm: black-box outputs
which lack transparency, vulnerability to hallucinations, and weak causal
reasoning. Hybrid approaches that combine statistical learning with expert
rule-based knowledge, and involve clinicians throughout the process help bring
back interpretability. They also fit better with existing clinical workflows,
as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking
predictions to clinically meaningful causes. This can be done through
neuro-symbolic or hybrid AI that combines the language ability of LLMs with
human causal expertise. AI researchers have addressed this direction, with
explainable AI and neuro-symbolic AI being the next logical steps in further
advancement in AI. However, they are still based on data-driven knowledge
integration instead of human-in-the-loop approaches. Future research should
measure success not only by accuracy but by improvements in clinician
understanding, workflow fit, and patient outcomes. A better understanding of
what helps improve human-computer interactions is greatly needed for AI systems
to become part of clinical practice.

</details>


### [123] [AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing](https://arxiv.org/abs/2507.01376)
*Yinwang Ren,Yangyang Liu,Tang Ji,Xun Xu*

Main category: cs.AI

TL;DR: 本文系统综述了基于LLM/MLLM的AI智能体和Agentic AI在智能制造领域的应用潜力、挑战及概念界定。


<details>
  <summary>Details</summary>
Motivation: 新兴AI范式（LLM-Agents、MLLM-Agents、Agentic AI）在智能制造中的定义、能力边界和实际应用尚不明确。

Method: 本研究通过系统回顾AI和AI智能体技术演进，审视LLM-Agents、MLLM-Agents和Agentic AI的核心概念、技术进展，并探讨它们在制造业的潜在应用、集成方式及面临的挑战。

Result: 对LLM-Agents、MLLM-Agents和Agentic AI的概念、技术进展、在智能制造中的潜在应用及其挑战进行了全面梳理和探索。

Conclusion: 这些新兴AI范式为智能制造开辟了新途径，极大地增强了AI的能力，但其在实际应用中的定义、能力边界和挑战需要被深入理解和解决。

Abstract: AI agents are autonomous systems designed to perceive, reason, and act within
dynamic environments. With the rapid advancements in generative AI (GenAI),
large language models (LLMs) and multimodal large language models (MLLMs) have
significantly improved AI agents' capabilities in semantic comprehension,
complex reasoning, and autonomous decision-making. At the same time, the rise
of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and
complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents
(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in
information processing, environmental perception, and autonomous
decision-making, opening new avenues for smart manufacturing. However, the
definitions, capability boundaries, and practical applications of these
emerging AI paradigms in smart manufacturing remain unclear. To address this
gap, this study systematically reviews the evolution of AI and AI agent
technologies, examines the core concepts and technological advancements of
LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential
applications in and integration into manufacturing, along with the potential
challenges they may face.

</details>


### [124] [A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models](https://arxiv.org/abs/2507.01410)
*Abeer Dyoub,Francesca A. Lisi*

Main category: cs.AI

TL;DR: 本文提出一种基于伦理风险评估的形式化方法，结合模糊规则和模糊Petri网，用于描述、验证和确认道德决策模型，并通过医学案例进行说明。


<details>
  <summary>Details</summary>
Motivation: 道德领域固有的本体论和认识论复杂性使得为评估“道德机器”性能建立明确标准极具挑战性。

Method: 提出一种基于伦理风险评估的形式化方法来描述道德决策模型；将这些模型指定为模糊规则；使用模糊Petri网对这些模型进行验证和确认。

Result: 通过一个医学领域的案例研究，成功地说明了所提出方法的有效性和适用性。

Conclusion: 本研究提供了一种处理道德领域复杂性的形式化方法，能够描述、验证和确认道德决策模型，为评估“道德机器”性能提供了一种新的途径。

Abstract: The ontological and epistemic complexities inherent in the moral domain make
it challenging to establish clear standards for evaluating the performance of a
moral machine. In this paper, we present a formal method to describe Ethical
Decision Making models based on ethical risk assessment. Then, we show how
these models that are specified as fuzzy rules can be verified and validated
using fuzzy Petri nets. A case study from the medical field is considered to
illustrate the proposed approach.

</details>


### [125] [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/abs/2507.01431)
*Yoonseok Yang,Minjune Kim,Marlon Rondinelli,Keren Shao*

Main category: cs.AI

TL;DR: Pensieve是一个AI辅助评分平台，利用大型语言模型（LLM）转录和评估学生手写答案，显著减少评分时间并保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 在大型大学STEM课程中，批改手写开放式答案是主要的瓶颈。

Method: 引入Pensieve平台，一个AI辅助评分系统。该系统利用LLM转录和评估学生作业，为教师提供符合评分标准的得分、转录文本和置信度评级。Pensieve支持从扫描学生提交到最终反馈的整个评分流程，并采用人机协作界面。

Result: Pensieve已在超过20个机构的真实课程中部署，批改了超过30万份学生答卷。研究结果显示，该平台平均减少65%的评分时间，同时在置信度高的预测上与教师评分保持95.4%的一致性。

Conclusion: Pensieve提供了一个有效且高效的解决方案，成功解决了大型STEM课程中手写开放式答案的评分效率和准确性问题。

Abstract: Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

</details>


### [126] [Using multi-agent architecture to mitigate the risk of LLM hallucinations](https://arxiv.org/abs/2507.01446)
*Abd Elrahman Amer,Magdi Amer*

Main category: cs.AI

TL;DR: 本文提出一个多智能体系统，整合LLM与模糊逻辑，旨在处理短信客户请求并缓解LLM幻觉风险，以提升客户服务质量和响应速度。


<details>
  <summary>Details</summary>
Motivation: 提升客户服务质量和响应时间对维护客户忠诚度和增加市场份额至关重要。尽管采用LLM是必要趋势，但其固有的幻觉风险构成重大挑战。

Method: 开发了一个多智能体系统来处理通过短信发送的客户请求。该系统通过整合基于LLM的智能体与模糊逻辑，以有效降低幻觉风险。

Result: 抽象部分未明确提供实验结果或性能数据。

Conclusion: 抽象部分主要介绍了所提出的系统设计，未给出基于实证结果的最终结论。

Abstract: Improving customer service quality and response time are critical factors for
maintaining customer loyalty and increasing a company's market share. While
adopting emerging technologies such as Large Language Models (LLMs) is becoming
a necessity to achieve these goals, the risk of hallucination remains a major
challenge. In this paper, we present a multi-agent system to handle customer
requests sent via SMS. This system integrates LLM based agents with fuzzy logic
to mitigate hallucination risks.

</details>


### [127] [Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning](https://arxiv.org/abs/2507.01489)
*Yanfei Zhang*

Main category: cs.AI

TL;DR: 本文提出一种分层框架“Agent-as-tool”，通过分离大型语言模型代理的工具调用与推理过程，有效提升了其在复杂任务上的推理能力和表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理研究面临同时决策工具调用和推理的挑战，且其推理链严重依赖未经处理、包含冗余信息的工具原始结果，从而极大地限制了模型的推理能力。

Method: 研究提出了一个名为“Agent-as-tool”的分层框架，该框架将工具调用过程与推理过程分离。这使得主模型能够专注于语言推理，而工具调用则由另一个代理处理。

Result: 该工作仅通过180个样本的少量强化微调，就取得了可比较的结果。在Bamboogle数据集上表现异常出色，精确匹配率为63.2%，覆盖精确匹配率为75.2%，分别比Search-R1高出4.8%和3.2%。

Conclusion: 所提出的“Agent-as-tool”分层框架通过有效分离LLM代理的工具调用和推理过程，成功克服了现有方法的挑战，显著提升了模型在复杂任务上的推理能力和性能。

Abstract: Large Language Models (LLMs) have emerged as one of the most significant
technological advancements in artificial intelligence in recent years. Their
ability to understand, generate, and reason with natural language has
transformed how we interact with AI systems. With the development of LLM-based
agents and reinforcement-learning-based reasoning models, the study of applying
reinforcement learning in agent frameworks has become a new research focus.
However, all previous studies face the challenge of deciding the tool calling
process and the reasoning process simultaneously, and the chain of reasoning
was solely relied on the unprocessed raw result with redundant information and
symbols unrelated to the task from the tool, which impose a heavy burden on the
model's capability to reason. Therefore, in our research, we proposed a
hierarchical framework Agent-as-tool that detach the tool calling process and
the reasoning process, which enables the model to focus on the verbally
reasoning process while the tool calling process is handled by another agent.
Our work had achieved comparable results with only a slight reinforcement
fine-tuning on 180 samples, and had achieved exceptionally well performance in
Bamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding
Search-R1 by 4.8% in exact match and 3.2% in cover exact match.

</details>


### [128] [T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2507.01597)
*Yuehang Si,Zefan Zeng,Jincai Huang,Qing Cheng*

Main category: cs.AI

TL;DR: 该论文提出T3DM方法和对抗式负采样策略，以解决时序知识图谱推理中训练与测试样本间的事件分布偏移及负样本质量低的问题，并取得了更优异和鲁棒的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数时序知识图谱推理（TKGR）研究面临两大挑战：一是训练和测试样本之间事件分布偏移的建模不足；二是依赖随机实体替换生成负样本，导致采样质量低下。

Method: 1. 提出了一种新的基于测试时训练（Test-Time Training）的分布特征建模方法T3DM，用于根据分布偏移调整模型并确保推理的全局一致性。2. 设计了一种基于对抗训练的负采样策略，以生成更高质量的负四元组。

Result: 大量的实验表明，T3DM在大多数情况下比现有的最先进基线模型提供了更好且更鲁棒的结果。

Conclusion: T3DM方法及其对抗式负采样策略有效解决了时序知识图谱推理中的分布偏移和负样本质量问题，显著提升了模型的性能和鲁棒性。

Abstract: Temporal Knowledge Graph (TKG) is an efficient method for describing the
dynamic development of facts along a timeline. Most research on TKG reasoning
(TKGR) focuses on modelling the repetition of global facts and designing
patterns of local historical facts. However, they face two significant
challenges: inadequate modeling of the event distribution shift between
training and test samples, and reliance on random entity substitution for
generating negative samples, which often results in low-quality sampling. To
this end, we propose a novel distributional feature modeling approach for
training TKGR models, Test-Time Training-guided Distribution shift Modelling
(T3DM), to adjust the model based on distribution shift and ensure the global
consistency of model reasoning. In addition, we design a negative-sampling
strategy to generate higher-quality negative quadruples based on adversarial
training. Extensive experiments show that T3DM provides better and more robust
results than the state-of-the-art baselines in most cases.

</details>


### [129] [Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI](https://arxiv.org/abs/2507.01717)
*Gopichand Kanumolu,Ashok Urlana,Charaka Vinayak Kumar,Bala Mallikarjunarao Garlapati*

Main category: cs.AI

TL;DR: 研究利用大型语言模型（LLM）和自主代理，从专利中自动挖掘并生成产品概念，并通过Agent Ideate框架验证了代理方法在创意生成方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 专利蕴含丰富的技术知识，能启发创新产品理念，但有效获取和解读这些信息面临挑战。

Method: 设计并实现了Agent Ideate框架，该框架利用开源LLM和基于代理的架构，旨在从专利中自动生成产品导向的商业创意。实验在计算机科学、自然语言处理和材料化学三个领域进行。

Result: 评估结果表明，基于代理的方法在创意质量、相关性和新颖性方面持续优于独立的LLM。

Conclusion: 研究发现结合LLM与代理工作流能显著增强创新流程，有效释放专利数据在商业创意生成方面的潜力。

Abstract: Patents contain rich technical knowledge that can inspire innovative product
ideas, yet accessing and interpreting this information remains a challenge.
This work explores the use of Large Language Models (LLMs) and autonomous
agents to mine and generate product concepts from a given patent. In this work,
we design Agent Ideate, a framework for automatically generating product-based
business ideas from patents. We experimented with open-source LLMs and
agent-based architectures across three domains: Computer Science, Natural
Language Processing, and Material Chemistry. Evaluation results show that the
agentic approach consistently outperformed standalone LLMs in terms of idea
quality, relevance, and novelty. These findings suggest that combining LLMs
with agentic workflows can significantly enhance the innovation pipeline by
unlocking the untapped potential of business idea generation from patent data.

</details>


### [130] [Joint Matching and Pricing for Crowd-shipping with In-store Customers](https://arxiv.org/abs/2507.01749)
*Arash Dehghan,Mucahit Cevik,Merve Bodur,Bissan Ghaddar*

Main category: cs.AI

TL;DR: 该论文提出一种利用店内顾客作为配送员的众包系统，通过结合神经近似动态规划（NeurADP）和深度双Q网络（DDQN）的联合优化策略，实现动态定价和订单分配，显著提升了城市最后一公里配送的成本效率。


<details>
  <summary>Details</summary>
Motivation: 为满足城市日益增长的最后一公里高效配送需求，论文旨在探索一种利用实体店顾客作为配送员的集中式众包配送系统。

Method: 提出一个马尔可夫决策过程（MDP）模型，捕捉订单和众包人员的随机到达以及配送接受率的不确定性。解决方案结合了神经近似动态规划（NeurADP）进行自适应订单分配，并使用深度双Q网络（DDQN）进行动态定价，以实现多点路线规划和考虑报价接受的不确定性。

Result: 集成的NeurADP + DDQN策略在配送成本效率方面取得显著提升，相比固定定价的NeurADP节省高达6.7%的成本，比短期基线节省约18%。此外，允许灵活的配送延迟和启用多目的地路线分别使运营成本进一步降低8%和17%。

Conclusion: 研究结果强调了众包系统中动态、前瞻性策略的优势，并为城市物流运营商提供了实际指导。

Abstract: This paper examines the use of in-store customers as delivery couriers in a
centralized crowd-shipping system, targeting the growing need for efficient
last-mile delivery in urban areas. We consider a brick-and-mortar retail
setting where shoppers are offered compensation to deliver time-sensitive
online orders. To manage this process, we propose a Markov Decision Process
(MDP) model that captures key uncertainties, including the stochastic arrival
of orders and crowd-shippers, and the probabilistic acceptance of delivery
offers. Our solution approach integrates Neural Approximate Dynamic Programming
(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network
(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop
routing and accounts for offer acceptance uncertainty, aligning more closely
with real-world operations. Experimental results demonstrate that the
integrated NeurADP + DDQN policy achieves notable improvements in delivery cost
efficiency, with up to 6.7\% savings over NeurADP with fixed pricing and
approximately 18\% over myopic baselines. We also show that allowing flexible
delivery delays and enabling multi-destination routing further reduces
operational costs by 8\% and 17\%, respectively. These findings underscore the
advantages of dynamic, forward-looking policies in crowd-shipping systems and
offer practical guidance for urban logistics operators.

</details>


### [131] [Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics](https://arxiv.org/abs/2507.01833)
*Yi-Dong Shen,Thomas Eiter*

Main category: cs.AI

TL;DR: 本研究质疑现有答案集编程（ASP）语义的普遍性条件，并提出了基于“良好支撑性”和“最小性”的精炼Gelfond答案集原则，以此定义新语义并评估现有语义。


<details>
  <summary>Details</summary>
Motivation: 非单调逻辑编程是答案集编程（ASP）的基础，但现有的多种答案集语义在Gelfond和Lifschitz的原始定义基础上提出了许多扩展。研究旨在探讨：1) 诸如最小模型特性、约束单调性和基础性等条件是否应普遍强制应用于所有答案集语义？2) 如果不应强制，那么还有哪些性质可以作为答案集语义的通用原则？

Method: 首先，通过实例论证了现有强制条件有时过于严格，可能排除预期的答案集。其次，通过将Gelfond的理性原则细化为“良好支撑性”、“默认否定下的最小性”和“认知否定下的最小性”，发展了精炼的Gelfond答案集（GAS）原则。随后，将“良好支撑性”的概念扩展到答案集和世界视图。接着，基于这些精炼的GAS原则定义了新的答案集语义。然后，利用精炼的GAS原则作为替代基线来评估现有答案集语义。最后，分析了计算复杂性。

Result: 研究表明，现有答案集语义的某些强制性条件（如最小模型特性、约束单调性和基础性）可能过于严格，并可能排除符合预期的答案集。为此，提出了精炼的Gelfond答案集（GAS）原则，其核心是“良好支撑性”（确保无循环证明）和两种“最小性”原则（确保知识最小化）。基于这些新原则，成功定义了新的答案集语义，并提供了一个评估现有语义的替代性直观基线。

Conclusion: 传统答案集语义的某些普遍性条件并非总是适用。本研究提出的精炼Gelfond答案集原则（基于良好支撑性和最小化）为构建和评估ASP语义提供了一个更灵活、更具包容性的通用框架，深化了对答案集编程核心原则的理解，并为未来的语义定义和评估提供了新视角。

Abstract: Non-monotonic logic programming is the basis for a declarative problem
solving paradigm known as answer set programming (ASP). Departing from the
seminal definition by Gelfond and Lifschitz in 1988 for simple normal logic
programs, various answer set semantics have been proposed for extensions. We
consider two important questions: (1) Should the minimal model property,
constraint monotonicity and foundedness as defined in the literature be
mandatory conditions for an answer set semantics in general? (2) If not, what
other properties could be considered as general principles for answer set
semantics? We address the two questions. First, it seems that the three
aforementioned conditions may sometimes be too strong, and we illustrate with
examples that enforcing them may exclude expected answer sets. Second, we
evolve the Gelfond answer set (GAS) principles for answer set construction by
refining the Gelfond's rationality principle to well-supportedness, minimality
w.r.t. negation by default and minimality w.r.t. epistemic negation. The
principle of well-supportedness guarantees that every answer set is
constructible from if-then rules obeying a level mapping and is thus free of
circular justification, while the two minimality principles ensure that the
formalism minimizes knowledge both at the level of answer sets and of world
views. Third, to embody the refined GAS principles, we extend the notion of
well-supportedness substantially to answer sets and world views, respectively.
Fourth, we define new answer set semantics in terms of the refined GAS
principles. Fifth, we use the refined GAS principles as an alternative baseline
to intuitively assess the existing answer set semantics. Finally, we analyze
the computational complexity.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [132] [Few-Shot Inspired Generative Zero-Shot Learning](https://arxiv.org/abs/2507.01026)
*Md Shakil Ahamed Shohag,Q. M. Jonathan Wu,Farhad Pourpanah*

Main category: cs.LG

TL;DR: FSIGenZ是一种创新的生成式零样本学习框架，通过处理属性变异性和减少合成数据量，在主流ZSL基准上实现了卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式零样本学习（ZSL）方法需要大量计算资源和广泛的合成数据，这与ZSL的原始假设相悖。此外，它们未能充分考虑类别属性在实例层面的变异性，即假设属性均匀存在。

Method: 本文提出FSIGenZ框架，旨在减少对大规模特征合成的依赖。主要方法包括：1) 引入Model-Specific Attribute Scoring (MSAS)，动态重评分类别属性以近似实例级变异性；2) 基于MSAS调整后的属性分数估计组级原型，作为未见类的代表性合成特征；3) 引入Dual-Purpose Semantic Regularization (DPSR) 策略，在训练语义感知对比分类器 (SCC) 时缓解数据不平衡。

Result: 在SUN、AwA2和CUB等基准数据集上的实验表明，FSIGenZ在显著减少合成特征数量的情况下，依然取得了有竞争力的性能。

Conclusion: FSIGenZ通过其新颖的属性处理和特征合成策略，有效解决了传统生成式ZSL方法对资源和合成数据的过度依赖问题，提供了一个更高效、更实际可行的零样本学习解决方案。

Abstract: Generative zero-shot learning (ZSL) methods typically synthesize visual
features for unseen classes using predefined semantic attributes, followed by
training a fully supervised classification model. While effective, these
methods require substantial computational resources and extensive synthetic
data, thereby relaxing the original ZSL assumptions. In this paper, we propose
FSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on
large-scale feature synthesis. Our key insight is that class-level attributes
exhibit instance-level variability, i.e., some attributes may be absent or
partially visible, yet conventional ZSL methods treat them as uniformly
present. To address this, we introduce Model-Specific Attribute Scoring (MSAS),
which dynamically re-scores class attributes based on model-specific
optimization to approximate instance-level variability without access to unseen
data. We further estimate group-level prototypes as clusters of instances based
on MSAS-adjusted attribute scores, which serve as representative synthetic
features for each unseen class. To mitigate the resulting data imbalance, we
introduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training
a semantic-aware contrastive classifier (SCC) using these prototypes.
Experiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves
competitive performance using far fewer synthetic features.

</details>


### [133] [DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization](https://arxiv.org/abs/2507.01027)
*Zijian Ye,Wei Huang,Yifei Yu,Tianhe Ren,Zhongrui Wang,Xiaojuan Qi*

Main category: cs.LG

TL;DR: DBellQuant是一种创新的后训练量化框架，通过将权重分布转换为双峰形式并平滑激活，实现了LLMs近1比特权重和6比特激活量化，同时保持了卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）性能优异但计算和内存成本高昂，限制了实际部署。量化是解决方案，但其效果常受限于非量化友好的权重分布和激活异常值导致的量化误差。

Method: DBellQuant框架引入了可学习双峰变换（LTDB）算法。该算法将单峰权重分布转换为双峰形式以减少二值化误差，并应用逆变换来平滑激活，从而实现近1比特权重压缩和6比特激活量化。

Result: DBellQuant在严格的权重和激活量化下，以最小的性能下降保持了卓越的模型性能，并达到了新的最先进水平。例如，在Wikitext2数据集上，LLaMA2-13B模型使用DBellQuant在6比特激活量化下实现了14.39的困惑度，显著优于BiLLM（未进行激活量化）的21.35。

Conclusion: DBellQuant在压缩LLMs以适应实际应用方面展现了巨大潜力，通过有效解决量化误差问题，实现了高压缩率和低性能损耗的平衡。

Abstract: Large language models (LLMs) demonstrate remarkable performance but face
substantial computational and memory challenges that limit their practical
deployment. Quantization has emerged as a promising solution; however, its
effectiveness is often limited by quantization errors arising from weight
distributions that are not quantization-friendly and the presence of activation
outliers. To address these challenges, we introduce DBellQuant, an innovative
post-training quantization (PTQ) framework that achieves nearly 1-bit weight
compression and 6-bit activation quantization with minimal performance
degradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)
algorithm, which transforms single-bell weight distributions into dual-bell
forms to reduce binarization errors and applies inverse transformations to
smooth activations. DBellQuant sets a new state-of-the-art by preserving
superior model performance under aggressive weight and activation quantization.
For example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of
14.39 on LLaMA2-13B with 6-bit activation quantization, significantly
outperforming BiLLM's 21.35 without activation quantization, underscoring its
potential in compressing LLMs for real-world applications.

</details>


### [134] [Dual Perspectives on Non-Contrastive Self-Supervised Learning](https://arxiv.org/abs/2507.01028)
*Jean Ponce,Martial Hebert,Basile Terver*

Main category: cs.LG

TL;DR: 本文从优化和动力系统角度，深入分析了非对比自监督学习中停止梯度和指数移动平均等避免表示崩溃的常用技术，揭示了它们在不优化传统目标函数的情况下，如何确保模型达到渐近稳定的非平凡解，并避免崩溃。


<details>
  <summary>Details</summary>
Motivation: 非对比自监督学习中，停止梯度和指数移动平均等技术被广泛用于避免表示崩溃，并在下游任务中表现优异。然而，这些程序为何能有效避免崩溃，其背后的理论机制（特别是从优化和动力系统视角）尚不完全明确，需要进行深入探究。

Method: 本研究通过理论分析方法，从优化和动力系统的双重角度审视了停止梯度和指数移动平均程序。首先，证明了这些程序即使不优化原始目标或任何其他平滑函数，也能避免崩溃。其次，借鉴并改进Tian et al. [2021]的工作，利用动力系统视角证明了在线性情况下，若不使用这些机制，最小化原始目标函数总会导致崩溃。最后，展示了这些程序相关的动力系统的极限点是渐近稳定的平衡点，不会退化为平凡解。

Result: 研究结果表明，停止梯度和指数移动平均程序能够有效避免表示崩溃。具体来说，即使这些程序不优化传统的平滑目标函数，它们也能确保系统不会陷入崩溃状态。在线性情况下，没有这些机制的原始目标函数最小化总是导致崩溃。相反，这些程序引导的动力系统具有渐近稳定的平衡点，从而保证了非平凡解的产生。

Conclusion: 停止梯度和指数移动平均是预防非对比自监督学习中表示崩溃的关键机制。它们通过建立渐近稳定的动力系统平衡点，有效避免了退化到平凡解，确保了模型学习到有用的表示，即使其作用机制不完全符合传统的光滑函数优化范畴。

Abstract: The objective of non-contrastive approaches to self-supervised learning is to
train on pairs of different views of the data an encoder and a predictor that
minimize the mean discrepancy between the code predicted from the embedding of
the first view and the embedding of the second one. In this setting, the stop
gradient and exponential moving average iterative procedures are commonly used
to avoid representation collapse, with excellent performance in downstream
supervised applications. This presentation investigates these procedures from
the dual theoretical viewpoints of optimization and dynamical systems. We first
show that, in general, although they do not optimize the original objective, or
for that matter, any other smooth function, they do avoid collapse. Following
Tian et al. [2021], but without any of the extra assumptions used in their
proofs, we then show using a dynamical system perspective that, in the linear
case, minimizing the original objective function without the use of a stop
gradient or exponential moving average always leads to collapse. Conversely, we
finally show that the limit points of the dynamical systems associated with
these two procedures are, in general, asymptotically stable equilibria, with no
risk of degenerating to trivial solutions.

</details>


### [135] [PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning](https://arxiv.org/abs/2507.01029)
*Junjie Zhou,Yingli Zuo,Shichang Feng,Peng Wan,Qi Zhu,Daoqiang Zhang,Wei Shao*

Main category: cs.LG

TL;DR: 提出PathCoT，一种零样本CoT提示方法，通过整合病理学专家知识和自我评估，解决多模态大语言模型（MLLMs）在病理学视觉推理中领域知识不足和CoT推理错误的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）在病理学视觉推理任务中面临挑战：1）缺乏领域特定信息导致模型幻觉和性能不佳；2）CoT（思维链）推理步骤可能引入错误，导致答案偏离。

Method: 提出PathCoT，一种新颖的零样本CoT提示方法。该方法将病理学专家知识融入MLLM的推理过程，通过先验知识引导MLLM扮演病理专家进行领域特定分析。同时，PathCoT引入自我评估步骤，评估MLLM直接结果和CoT推导结果，以确定最可靠的答案。

Result: 在PathMMU数据集上的实验结果表明，PathCoT在病理学视觉理解和推理方面表现出有效性。

Conclusion: PathCoT通过整合专家知识和自我评估机制，有效提升了MLLM在病理学视觉推理任务中的性能，解决了现有模型在领域特异性和推理准确性上的局限。

Abstract: With the development of generative artificial intelligence and instruction
tuning techniques, multimodal large language models (MLLMs) have made
impressive progress on general reasoning tasks. Benefiting from the
chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning
problem step-by-step. However, existing MLLMs still face significant challenges
when applied to pathology visual reasoning tasks: (1) LLMs often underperforms
because they lack domain-specific information, which can lead to model
hallucinations. (2) The additional reasoning steps in CoT may introduce errors,
leading to the divergence of answers. To address these limitations, we propose
PathCoT, a novel zero-shot CoT prompting method which integrates the pathology
expert-knowledge into the reasoning process of MLLMs and incorporates
self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides
the MLLM with prior knowledge to perform as pathology experts, and provides
comprehensive analysis of the image with their domain-specific knowledge. By
incorporating the experts' knowledge, PathCoT can obtain the answers with CoT
reasoning. Furthermore, PathCoT incorporates a self-evaluation step that
assesses both the results generated directly by MLLMs and those derived through
CoT, finally determining the reliable answer. The experimental results on the
PathMMU dataset demonstrate the effectiveness of our method on pathology visual
understanding and reasoning.

</details>


### [136] [Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study](https://arxiv.org/abs/2507.01030)
*Reza Lotfi Navaei,Mohammad Safarzadeh,Seyed Mohammad Jafar Sobhani*

Main category: cs.LG

TL;DR: 本研究利用机器学习（特别是优化后的多层感知机）为甲烷燃料的燃烧模拟开发了高效高精度的层流FGM库，达到了99.81%的准确率。


<details>
  <summary>Details</summary>
Motivation: 火焰面生成流形（FGM）在燃烧模型中精度高但内存消耗大。本研究旨在利用机器学习算法为甲烷燃料的燃烧模拟开发FGM库，以解决内存限制问题。

Method: 研究基于数据驱动概念，采用多层感知机（MLP）、随机森林、线性回归和支持向量机四种机器学习算法重建火焰面库。通过评估各算法的默认架构，选择MLP为主要方法，并对其进行超参数调优（优化隐藏层和神经元数量）以提升准确性。

Result: 七个库被用于训练数据库，初始误差率为2.30%。通过模型评估与调优，最终选定的MLP模型（包含四层隐藏层，神经元数量分别为10、15、20、25）实现了99.81%的准确率。

Conclusion: 优化后的多层感知机模型能以高精度高效地生成用于甲烷燃烧模拟的FGM库，有效解决了FGM实际应用中的内存资源限制，为未来燃烧模拟提供了准确可行的方案。

Abstract: In chemistry tabulations and Flamelet combustion models, the Flamelet
Generated Manifold (FGM) is recognized for its precision and physical
representation. The practical implementation of FGM requires a significant
allocation of memory resources. FGM libraries are developed specifically for a
specific fuel and subsequently utilized for all numerical problems using
machine learning techniques. This research aims to develop libraries of Laminar
FGM utilizing machine learning algorithms for application in combustion
simulations of methane fuel. This study employs four Machine Learning
algorithms to regenerate Flamelet libraries, based on an understanding of data
sources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.
Random Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries
were identified as appropriate for constructing a database for training machine
learning models, giving an error rate of 2.30%. The default architectures of
each method were evaluated to determine the optimal approach, leading to the
selection of the MLP method as the primary choice. The method was enhanced
through hyperparameter tuning to improve accuracy. The quantity of hidden
layers and neurons significantly influences method performance. The optimal
model, comprising four hidden layers with 10, 15, 20, and 25 neurons
respectively, achieved an accuracy of 99.81%.

</details>


### [137] [PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs](https://arxiv.org/abs/2507.01031)
*Fanchen Bu,Kijung Shin*

Main category: cs.LG

TL;DR: 本文成功将PyTorch几何学习框架移植到Intel Gaudi-v2 HPUs，通过提供核心工具、详细教程和实际案例，旨在降低非CUDA硬件上进行几何学习研究的门槛。


<details>
  <summary>Details</summary>
Motivation: 几何学习在非欧几里得数据处理中表现出色，而新兴的非CUDA加速器（如Intel Gaudi HPU）展现出竞争优势。然而，将现有软件（特别是PyTorch）移植到这些新硬件需要大量工程投入和软件适配，这阻碍了其广泛采用和进一步优化。

Method: 本研究开发了一系列核心工具，用于在Gaudi-v2 HPUs上恢复几何学习框架中的关键操作（如scatter、稀疏索引、k-近邻）。此外，整合了16个引导式教程和11个真实世界案例，详细诊断了遇到的移植失败，并提供了详细的解决方案。所有经验均汇集于公开GitHub仓库。

Result: 成功将基于PyTorch的几何学习框架移植到Gaudi-v2 HPUs，并解决了关键操作的兼容性问题。通过系统性的教程和案例，提供了详尽的诊断分析和工作规避方案，使得研究人员能够更容易地在非CUDA硬件上进行几何学习算法实验。

Conclusion: 本工作显著降低了研究人员在非CUDA硬件上进行几何学习算法实验的障碍，为未来的性能优化和跨平台可移植性奠定了坚实的基础。

Abstract: Geometric learning has emerged as a powerful paradigm for modeling
non-Euclidean data, especially graph-structured ones, with applications
spanning social networks, molecular structures, knowledge graphs, and
recommender systems. While Nvidia's CUDA-enabled graphics processing units
(GPUs) largely dominate the hardware landscape, emerging accelerators such as
Intel's Gaudi Habana Processing Units (HPUs) offer competitive performance and
energy efficiency. However, the usage of such non-CUDA processing units
requires significant engineering effort and novel software adaptations. In this
work, we present our experiences porting PyTorch-based geometric learning
frameworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that
restore essential operations (e.g., scatter, sparse indexing, k-nearest
neighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and
eleven real-world examples with diagnostic analyses of encountered failures and
detailed workarounds. We collect all our experiences into a publicly accessible
GitHub repository. Our contributions lower the barrier for researchers to
experiment with geometric-learning algorithms and models on non-CUDA hardware,
providing a foundation for further optimization and cross-platform portability.

</details>


### [138] [An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks](https://arxiv.org/abs/2507.01032)
*Nan Mu,Hongbo Yang,Chen Zhao*

Main category: cs.LG

TL;DR: 提出一种不确定性感知的多视图动态决策框架，用于组学数据分类，旨在降低检测成本同时保持高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 高通量多组学技术成本高昂，且过度依赖完整组学数据可能导致不必要的资源消耗，因此需要一种方法在保证诊断准确性的前提下最小化检测成本。

Method: 在单组学层面，通过改进神经网络激活函数生成狄利克雷分布参数，并利用主观逻辑量化分类结果的置信和不确定性。在多组学层面，采用基于Dempster-Shafer理论的融合策略整合异构模态。最后，应用动态决策机制，逐步引入组学数据直到模型置信度达到预设阈值或所有数据被利用。

Result: 在四个基准多组学数据集上，该方法在三个数据集中使超过50%的病例仅使用单一组学模态就实现了准确分类，有效减少了冗余测试。同时，诊断性能与全组学模型相当，并保留了重要的生物学见解。

Conclusion: 所提出的框架能够在显著降低组学数据检测成本的同时，保持诊断准确性，并为决策提供更可靠的依据。

Abstract: Background and Objective: High-throughput multi-omics technologies have
proven invaluable for elucidating disease mechanisms and enabling early
diagnosis. However, the high cost of multi-omics profiling imposes a
significant economic burden, with over reliance on full omics data potentially
leading to unnecessary resource consumption. To address these issues, we
propose an uncertainty-aware, multi-view dynamic decision framework for omics
data classification that aims to achieve high diagnostic accuracy while
minimizing testing costs. Methodology: At the single-omics level, we refine the
activation functions of neural networks to generate Dirichlet distribution
parameters, utilizing subjective logic to quantify both the belief masses and
uncertainty mass of classification results. Belief mass reflects the support of
a specific omics modality for a disease class, while the uncertainty parameter
captures limitations in data quality and model discriminability, providing a
more trustworthy basis for decision-making. At the multi omics level, we employ
a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous
modalities, leveraging their complementarity to boost diagnostic accuracy and
robustness. A dynamic decision mechanism is then applied that omics data are
incrementally introduced for each patient until either all data sources are
utilized or the model confidence exceeds a predefined threshold, potentially
before all data sources are utilized. Results and Conclusion: We evaluate our
approach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.
In three datasets, over 50% of cases achieved accurate classification using a
single omics modality, effectively reducing redundant testing. Meanwhile, our
method maintains diagnostic performance comparable to full-omics models and
preserves essential biological insights.

</details>


### [139] [Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya](https://arxiv.org/abs/2507.01034)
*Asma Agaal,Mansour Essgaer,Hend M. Farkash,Zulaiha Ali Othman*

Main category: cs.LG

TL;DR: 本研究通过多种时间序列模型，特别是优化的LSTM，预测利比亚班加西的电力负荷、发电量和赤字，为当地电网管理和能源规划提供支持。


<details>
  <summary>Details</summary>
Motivation: 利比亚班加西地区面临频繁限电、发电量不足和基础设施限制，迫切需要准确的电力预测以确保电网稳定和能源规划。

Method: 采用数据驱动方法，利用2019年和2023年的历史数据预测2025年的电力指标。数据经过缺失值填充、异常值平滑和对数变换。应用了ARIMA系列、指数平滑、XGBoost和LSTM等多种时间序列模型。性能通过MSE、RMSE、MAE和MAPE评估。核心贡献是集成了温度、湿度等外部因素的优化LSTM框架。

Result: LSTM模型表现优于所有其他模型，尤其擅长处理非平稳和季节性模式。优化后的LSTM框架通过整合外部因素，在预测多种电力指标方面显示出强大的鲁棒性。

Conclusion: 研究结果为政策制定者和电网运营商提供了实用见解，有助于在数据稀缺和不稳定的地区进行前瞻性负荷管理和资源规划。

Abstract: Accurate electricity forecasting is crucial for grid stability and energy
planning, especially in Benghazi, Libya, where frequent load shedding,
generation deficits, and infrastructure limitations persist. This study
proposes a data-driven approach to forecast electricity load, generation, and
deficits for 2025 using historical data from 2019 (a year marked by
instability) and 2023 (a more stable year). Multiple time series models were
applied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential
smoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural
networks. The dataset was enhanced through missing value imputation, outlier
smoothing, and log transformation. Performance was assessed using mean squared
error, root mean squared error, mean absolute error, and mean absolute
percentage error. LSTM outperformed all other models, showing strong
capabilities in modeling non-stationary and seasonal patterns. A key
contribution of this work is an optimized LSTM framework that integrates
exogenous factors such as temperature and humidity, offering robust performance
in forecasting multiple electricity indicators. These results provide practical
insights for policymakers and grid operators to enable proactive load
management and resource planning in data-scarce, volatile regions.

</details>


### [140] [Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2507.01035)
*Yushang Zhao,Haotian Lyu,Yike Peng,Aijia Sun,Feng Jiang,Xinyue Han*

Main category: cs.LG

TL;DR: 本研究旨在通过结合软硬件协同设计和参数高效调优，优化混合GNN-LLM推荐系统的推理延迟和训练效率，并取得了显著的效果。


<details>
  <summary>Details</summary>
Motivation: 在线服务需要高速、高效且能处理复杂用户-物品交互的实时推荐系统。本研究旨在解决混合图神经网络（GNN）和大型语言模型（LLM）推荐系统中的计算瓶颈，以优化其推理延迟和训练效率。

Method: 采用了广泛的方法，包括：混合GNN-LLM集成架构优化策略（如量化、LoRA、蒸馏）和硬件加速（如FPGA、DeepSpeed），所有实验均在R 4.4.2环境下进行。

Result: 实验结果显著：最优的“混合+FPGA+DeepSpeed”配置在40-60毫秒延迟下，准确率（NDCG@10）达到0.75，比非优化基线提高了13.6%；LoRA将训练时间缩短了66%（至3.8小时）。

Conclusion: 软硬件协同设计和参数高效调优能使混合模型在准确性和效率上超越单独的GNN或LLM方法。研究推荐在实时部署中使用FPGA和LoRA。未来工作可考虑联邦学习和高级融合架构以提高可扩展性和隐私保护。本研究为下一代平衡低延迟和个性化的推荐系统奠定了基础。

Abstract: The incessant advent of online services demands high speed and efficient
recommender systems (ReS) that can maintain real-time performance along with
processing very complex user-item interactions. The present study, therefore,
considers computational bottlenecks involved in hybrid Graph Neural Network
(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their
inference latency and training efficiency. An extensive methodology was used:
hybrid GNN-LLM integrated architecture-optimization strategies(quantization,
LoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.
Experimental improvements were significant, with the optimal Hybrid + FPGA +
DeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms
of latency, while LoRA brought down training time by 66% (3.8 hours) in
comparison to the non-optimized baseline. Irrespective of domain, such as
accuracy or efficiency, it can be established that hardware-software co-design
and parameter-efficient tuning permit hybrid models to outperform GNN or LLM
approaches implemented independently. It recommends the use of FPGA as well as
LoRA for real-time deployment. Future work should involve federated learning
along with advanced fusion architectures for better scalability and privacy
preservation. Thus, this research marks the fundamental groundwork concerning
next-generation ReS balancing low-latency response with cutting-edge
personalization.

</details>


### [141] [Learning to Segment for Vehicle Routing Problems](https://arxiv.org/abs/2507.01037)
*Wenbin Ouyang,Sirui Li,Yining Ma,Cathy Wu*

Main category: cs.LG

TL;DR: 针对车辆路径问题(VRP)迭代求解器中的冗余计算，本文提出了“先分段再聚合”(FSTA)分解技术，并引入L2Seg神经框架智能识别稳定/不稳定部分，将现有求解器加速了高达7倍。


<details>
  <summary>Details</summary>
Motivation: VRP迭代求解器中大量解段在迭代过程中保持稳定，导致冗余计算，尤其在大规模VRP中效率低下。因此需要一种方法来加速求解过程。

Method: 引入“先分段再聚合”(FSTA)分解技术，通过保留稳定解段、聚合节点为超节点并仅搜索不稳定部分。为智能识别稳定/不稳定段，提出L2Seg神经框架，包含非自回归、自回归及其协同三种变体，并设计了相应的训练和推理策略。

Result: L2Seg在CVRP和VRPTW基准测试中将现有迭代求解器加速了高达7倍。深度分析表明，非自回归和自回归方法的协同作用结合了各自优势，实现了最佳性能。

Conclusion: L2Seg是一个灵活的框架，可与传统、基于学习和混合VRP求解器兼容，并支持多种VRP类型，显著提升了迭代求解器的效率。

Abstract: Iterative search heuristics are widely recognized as state-of-the-art for
solving Vehicle Routing Problems (VRPs). In this work, we identify and exploit
a critical observation: within these solvers, a large portion of the solution
remains stable, i.e., unchanged across search iterations, causing redundant
computations, especially for large-scale VRPs with long subtours. To address
this, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)
decomposition technique to accelerate iterative solvers. Specifically, FSTA
preserves stable solution segments during the search, aggregates nodes within
each segment into fixed hypernodes, and focuses the search only on unstable
portions. Yet, a key challenge lies in identifying which segments should be
aggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),
a novel neural framework to intelligently differentiate potentially stable and
unstable portions for FSTA decomposition. We present three L2Seg variants:
non-autoregressive (globally comprehensive but locally indiscriminate),
autoregressive (locally refined but globally deficient), and their synergy,
with bespoke training and inference strategies. Empirical results on CVRP and
VRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up
to 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy
achieves best performance by combining their complementary strengths. Notably,
L2Seg is a flexible framework that is compatible with traditional,
learning-based, and hybrid solvers, while supporting a broad class of VRPs.

</details>


### [142] [On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization](https://arxiv.org/abs/2507.01039)
*Kaaustaaub Shankar,Wilhelm Louw,Kelly Cohen*

Main category: cs.LG

TL;DR: 提出一种使用PPO训练神经模糊控制器的方法，并在CartPole-v1任务中展示了优于DQN的性能，具有更低的方差和更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 为了改进现有基于DQN的ANFIS训练方法，通过引入更稳定的在策略PPO算法来训练可解释的神经模糊控制器。

Method: 开发了一种基于PPO的强化学习方法来训练神经模糊控制器，用稳定的在策略Actor-Critic循环取代了DQN的离策略价值框架。该方法在CartPole-v1环境中进行评估，并与ANFIS-DQN基线进行了学习性能比较。

Result: PPO训练的模糊智能体在CartPole-v1上经过20000次更新后达到了500±0的平均回报，在训练过程中表现出比基于DQN的方法更低的方差和更快的整体收敛速度。

Conclusion: 研究结果表明PPO为强化学习任务中训练可解释的神经模糊控制器提供了一条有前景的途径。

Abstract: We propose a reinforcement learning (RL) approach for training neuro-fuzzy
controllers using Proximal Policy Optimization (PPO). Building on prior work
that applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),
our method replaces the off-policy value-based framework with a stable
on-policy actor-critic loop. We evaluate this approach in the CartPole-v1
environment using multiple random seeds and compare its learning performance
against ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained
fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000
updates, showcasing less variance than prior DQN-based methods during training
and overall faster convergence. These findings suggest that PPO offers a
promising pathway for training explainable neuro-fuzzy controllers in
reinforcement learning tasks.

</details>


### [143] [Fast Clifford Neural Layers](https://arxiv.org/abs/2507.01040)
*Tianxiang Xia,Max Neuwinger,Lin Xiao*

Main category: cs.LG

TL;DR: 通过优化Clifford神经网络层（特别是2/3D Clifford卷积层和多向量激活层）的CPU推理性能，实现比标准PyTorch实现在大型数据集上30%的加速。


<details>
  <summary>Details</summary>
Motivation: Clifford神经网络层能够改进偏微分方程（PDE）建模。本项目的核心动机在于优化2/3D Clifford卷积层和多向量激活层在CPU单核上的推理效率。

Method: 项目专注于对2/3D Clifford卷积层和多向量激活层进行实现层面的优化，以提升其在CPU单核上的性能。通过在真实网络模块上进行测试来验证优化效果。

Result: 在涉及Clifford卷积层和多向量激活层的真实网络模块测试中，当数据和网络规模较大（超过L2缓存）时，优化的实现比标准PyTorch实现快30%。

Conclusion: 本研究通过优化Clifford神经网络层的实现，显著提升了其在CPU上的推理速度，尤其在大规模数据场景下展现出优越性能。

Abstract: Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra
into neural networks. In this project we focus on optimizing the inference of
2/3D Clifford convolutional layers and multivector activation layers for one
core CPU performance.
  Overall, by testing on a real network block involving Clifford convolutional
layers and multivector activation layers, we observe that our implementation is
30% faster than standard PyTorch implementation in relatively large data +
network size (>L2 cache).
  We open source our code base at
https://github.com/egretwAlker/c-opt-clifford-layers

</details>


### [144] [Fast AI Model Splitting over Edge Networks](https://arxiv.org/abs/2507.01041)
*Zuguang Li,Wen Wu,Shaohua Wu,Songge Zhang,Ye Wang,Xuemin,Shen*

Main category: cs.LG

TL;DR: 本文提出将AI模型切分问题转换为DAG上的最小s-t割问题，并设计出理论最优且实验高效的算法，可在毫秒级确定最优切分点，显著降低Split Learning的训练延迟。


<details>
  <summary>Details</summary>
Motivation: Split Learning虽能减轻设备端计算负担，但面对复杂AI模型架构时，寻找最优模型切分点存在计算复杂度高的问题。

Method: 1. 将任意AI模型表示为有向无环图（DAG），并将最优模型切分问题重构为最小s-t割搜索问题。 2. 提出一种快速的基于DAG的模型切分算法，通过重构DAG并利用最大流方法识别最优切分。 3. 针对具有块结构的AI模型，提出一种块级模型切分算法，通过将每个块抽象为单个顶点来简化DAG。

Result: 1. 理论分析表明所提出的基于DAG的算法是最优的。 2. 实验结果表明，所提出的算法可在毫秒级确定最优模型切分。 3. 与现有最佳基准相比，在动态边缘网络中，训练延迟降低了24.62%至38.95%。

Conclusion: 所提出的算法能够高效且最优地解决复杂AI模型在Split Learning中的切分问题，大幅降低训练延迟，提升了模型训练效率。

Abstract: Split learning (SL) has emerged as a computationally efficient approach for
artificial intelligence (AI) model training, which can alleviate device-side
computational workloads. However, complex AI model architectures pose high
computational complexity to obtain the optimal model splitting. In this paper,
we represent an arbitrary AI model as a directed acyclic graph (DAG), and then
reformulate the optimal model splitting problem as a minimum s-t cut search
problem. To solve the problem, we propose a fast DAG-based model splitting
algorithm, which restructures the DAG to enable the optimal model splitting
identification via a maximum flow method. Theoretical analysis indicates that
the proposed algorithm is optimal. Furthermore, considering AI models with
block structures, we propose a block-wise model splitting algorithm to reduce
computational complexity. The algorithm abstracts each block, i.e., a component
consisting of multiple layers, into a single vertex, thereby obtaining the
optimal model splitting via a simplified DAG. Extensive experimental results
demonstrate that the proposed algorithms can determine the optimal model
splitting within milliseconds, as well as reduce training delay by
24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art
benchmarks.

</details>


### [145] [Data Classification with Dynamically Growing and Shrinking Neural Networks](https://arxiv.org/abs/2507.01043)
*Szymon Świderski,Agnieszka Jastrzębska*

Main category: cs.LG

TL;DR: 本文提出一种基于蒙特卡洛树搜索（MCTS）的动态神经网络架构调整方法，能在训练过程中自动增减网络结构，并特别在多元时间序列分类任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络模型构建假定固定架构，但寻找最优模型架构是人工智能领域的核心问题之一。本文旨在解决在训练权重的同时，也能找到最佳模型架构的问题。

Method: 文章提出一种新方法，允许神经网络在训练过程中动态地进行结构收缩和增长。模型架构的决策机制由蒙特卡洛树搜索（MCTS）控制，通过模拟网络行为来比较并选择最佳的候选架构更改。

Result: 该方法在视觉和时间序列数据集上进行了验证，在多元时间序列分类中显示出特别的有效性，这归因于其架构动态适应性，允许独立修改每条时间序列。实验评估在视觉模式和多元时间序列分类任务中表现出非常出色的性能，突显了该方法的鲁棒性和适应性。

Conclusion: 该研究提供了一种鲁棒且适应性强的动态神经网络架构设计方法，尤其适用于需要架构灵活性的任务，如多元时间序列分类。

Abstract: The issue of data-driven neural network model construction is one of the core
problems in the domain of Artificial Intelligence. A standard approach assumes
a fixed architecture with trainable weights. A conceptually more advanced
assumption is that we not only train the weights, but also find out the optimal
model architecture. We present a new method that realizes just that. This
article is an extended version of our conference paper titled "Dynamic Growing
and Shrinking of Neural Networks with Monte Carlo Tree Search [26]". In the
paper, we show in detail how to create a neural network with a procedure that
allows dynamic shrinking and growing of the model while it is being trained.
The decision-making mechanism for the architectural design is governed by a
Monte Carlo tree search procedure which simulates network behavior and allows
to compare several candidate architecture changes to choose the best one. The
proposed method was validated using both visual and time series datasets,
demonstrating its particular effectiveness in multivariate time series
classification. This is attributed to the architecture's ability to adapt
dynamically, allowing independent modifications for each time series. The
approach is supplemented by Python source code for reproducibility.
Experimental evaluations in visual pattern and multivariate time series
classification tasks revealed highly promising performance, underscoring the
method's robustness and adaptability.

</details>


### [146] [Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals](https://arxiv.org/abs/2507.01045)
*Xiao Gu,Wei Tang,Jinpei Han,Veer Sangha,Fenglin Liu,Shreyank N Gowda,Antonio H. Ribeiro,Patrick Schwab,Kim Branson,Lei Clifton,Antonio Luiz P. Ribeiro,Zhangdaihong Liu,David A. Clifton*

Main category: cs.LG

TL;DR: 提出一种心血管生物信号基础模型（CSFM），通过多模态预训练克服传统方法的局限性，并在多种任务和配置中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法依赖同质数据集和静态模型，导致心血管生物信号分析的鲁棒性和泛化能力不足，限制了其在多样化临床环境中的应用。

Method: 提出心脏感知基础模型（CSFM），采用先进的Transformer架构和生成式掩码预训练策略，从大规模、异构健康记录中学习统一表示。模型在包含约170万个体的心脏信号及相应文本报告的多模态数据上进行预训练。

Result: CSFM提取的嵌入特征有效，能实现跨输入配置和传感器模态的无缝迁移学习。在诊断、人口信息识别、生命体征测量、临床结果预测及心电图问答等任务中，CSFM持续优于传统单模态单任务方法，并在不同心电图导联配置和传感器组合下均表现出鲁棒性。

Conclusion: CSFM是一种用于全面心脏监测的多功能、可扩展的解决方案，具有巨大潜力。

Abstract: Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms
(PPG), are of paramount importance for the diagnosis, prevention, and
management of cardiovascular diseases, and have been extensively used in a
variety of clinical tasks. Conventional deep learning approaches for analyzing
these signals typically rely on homogeneous datasets and static bespoke models,
limiting their robustness and generalizability across diverse clinical settings
and acquisition protocols. In this study, we present a cardiac sensing
foundation model (CSFM) that leverages advanced transformer architectures and a
generative, masked pretraining strategy to learn unified representations from
vast, heterogeneous health records. Our model is pretrained on an innovative
multi-modal integration of data from multiple large-scale datasets (including
MIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the
corresponding clinical or machine-generated text reports from approximately 1.7
million individuals. We demonstrate that the embeddings derived from our CSFM
not only serve as effective feature extractors across diverse cardiac sensing
scenarios, but also enable seamless transfer learning across varying input
configurations and sensor modalities. Extensive evaluations across diagnostic
tasks, demographic information recognition, vital sign measurement, clinical
outcome prediction, and ECG question answering reveal that CSFM consistently
outperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits
robust performance across multiple ECG lead configurations from standard
12-lead systems to single-lead setups, and in scenarios where only ECG, only
PPG, or a combination thereof is available. These findings highlight the
potential of CSFM as a versatile and scalable solution, for comprehensive
cardiac monitoring.

</details>


### [147] [Variational Digital Twins](https://arxiv.org/abs/2507.01047)
*Logan A. Burnett,Umme Mahbuba Nabila,Majdi I. Radaideh*

Main category: cs.LG

TL;DR: 本文提出一种变分数字孪生（VDT）框架，通过轻量级贝叶斯增强和新型更新算法，解决了现有数字孪生在实时性、不确定性量化和数据效率方面的局限，并在多个能源领域应用中验证了其高性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前数字孪生（DT）研究在模型与资产间的信息交换框架、实时实现所需关键特性以及模型不确定性处理方面存在不足。

Method: 本文提出一种变分数字孪生（VDT）框架，通过在标准神经网络架构中增加一个单一的贝叶斯输出层进行增强。结合一种新颖的VDT更新算法，该框架能够在商品级GPU上实现秒级更新，并生成校准的不确定性边界。

Result: VDT在四个能源领域问题上进行了评估：1) 临界热流预测中，不确定性驱动的主动学习以更少实验（减少47%）和训练时间（1/3）达到R2=0.98。2) 三年期可再生能源发电孪生模型对太阳能输出保持R2>0.95，并通过月度更新抑制风力预测误差增长。3) 核反应堆瞬态冷却孪生模型以R2>0.99重建热电偶信号，并在50%传感器失效时仍保持精度。4) 物理信息锂离子电池孪生模型通过定期再训练，将电压均方误差降低一个数量级，并能适应电池寿命末期可信区间的变化。

Conclusion: 结合适度的贝叶斯增强和高效更新方案，可以将传统替代模型转化为具有不确定性感知、数据高效和计算可行性的数字孪生，从而为工业和科学能源系统中的可靠模型铺平道路。

Abstract: While digital twins (DT) hold promise for providing real-time insights into
complex energy assets, much of the current literature either does not offer a
clear framework for information exchange between the model and the asset, lacks
key features needed for real-time implementation, or gives limited attention to
model uncertainty. Here, we aim to solve these gaps by proposing a variational
digital twin (VDT) framework that augments standard neural architectures with a
single Bayesian output layer. This lightweight addition, along with a novel VDT
updating algorithm, lets a twin update in seconds on commodity GPUs while
producing calibrated uncertainty bounds that can inform experiment design,
control algorithms, and model reliability. The VDT is evaluated on four
energy-sector problems. For critical-heat-flux prediction, uncertainty-driven
active learning reaches R2 = 0.98 using 47 % fewer experiments and one-third
the training time of random sampling. A three-year renewable-generation twin
maintains R2 > 0.95 for solar output and curbs error growth for volatile wind
forecasts via monthly updates that process only one month of data at a time. A
nuclear reactor transient cooldown twin reconstructs thermocouple signals with
R2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating
robustness to degraded instrumentation. Finally, a physics-informed Li-ion
battery twin, retrained after every ten discharges, lowers voltage mean-squared
error by an order of magnitude relative to the best static model while adapting
its credible intervals as the cell approaches end-of-life. These results
demonstrate that combining modest Bayesian augmentation with efficient update
schemes turns conventional surrogates into uncertainty-aware, data-efficient,
and computationally tractable DTs, paving the way for dependable models across
industrial and scientific energy systems.

</details>


### [148] [3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells](https://arxiv.org/abs/2507.01048)
*Ricardo Emanuel Vaz Vargas,Afrânio José de Melo Junior,Celso José Munaro,Cláudio Benevenuto de Campos Lima,Eduardo Toledo de Lima Junior,Felipe Muntzberg Barrocas,Flávio Miguel Varejão,Guilherme Fidelis Peixer,Igor de Melo Nery Oliveira,Jader Riso Barbosa Jr.,Jaime Andrés Lozano Cadena,Jean Carlos Dias de Araújo,João Neuenschwander Escosteguy Carneiro,Lucas Gouveia Omena Lopes,Lucas Pereira de Gouveia,Mateus de Araujo Fernandes,Matheus Lima Scramignon,Patrick Marques Ciarelli,Rodrigo Castello Branco,Rogério Leite Alves Pinto*

Main category: cs.LG

TL;DR: 本文介绍并更新了3W数据集，这是一个公开的多变量时间序列数据集，用于石油钻井中不良事件的早期检测，旨在促进AI/ML研究。


<details>
  <summary>Details</summary>
Motivation: 石油钻井中不良事件会导致经济损失、环境事故和人员伤亡。尽管人工智能和机器学习在早期检测方面具有价值，但缺乏相关公共数据集是限制其发展的重要因素。

Method: 该研究描述了3W数据集的最新公开版本。3W数据集最初由巴西石油公司于2019年发布，是一个由专家标注的多变量时间序列集合。最新版本包含了结构性修改和额外标注的数据。

Result: 3W数据集已成为该领域的开创性参考，并得到持续协作开发。当前公开版本提供了结构改进和更多标注数据，鼓励社区在此基础上开发更强大的AI/ML方法和数字产品。

Conclusion: 更新后的3W数据集旨在支持研究人员和工程师开发出能够有效提前检测石油钻井不良事件的方法和产品，从而实现及时纠正或缓解措施，减少损失。

Abstract: In the oil industry, undesirable events in oil wells can cause economic
losses, environmental accidents, and human casualties. Solutions based on
Artificial Intelligence and Machine Learning for Early Detection of such events
have proven valuable for diverse applications across industries. In 2019,
recognizing the importance and the lack of public datasets related to
undesirable events in oil wells, Petrobras developed and publicly released the
first version of the 3W Dataset, which is essentially a set of Multivariate
Time Series labeled by experts. Since then, the 3W Dataset has been developed
collaboratively and has become a foundational reference for numerous works in
the field. This data article describes the current publicly available version
of the 3W Dataset, which contains structural modifications and additional
labeled data. The detailed description provided encourages and supports the 3W
community and new 3W users to improve previous published results and to develop
new robust methodologies, digital products and services capable of detecting
undesirable events in oil wells with enough anticipation to enable corrective
or mitigating actions.

</details>


### [149] [Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization](https://arxiv.org/abs/2507.01050)
*Jing Yu,Yibo Zhao,Jiapeng Zhu,Wenming Shao,Bo Pang,Zhao Zhang,Xiang Li*

Main category: cs.LG

TL;DR: 针对社交媒体毒性文本去毒挑战，本文提出一种两阶段训练框架。该框架结合少量高质量标注数据和无标注数据，通过定制奖励模型和Group Relative Policy Optimization训练LLM，有效缓解了现有方法的权衡问题，实现了SOTA性能、更好的泛化能力并显著降低了对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 社交媒体毒性内容泛滥对在线环境和公共讨论构成威胁，急需能有效去毒并保留语义的方法。然而，现有方法在去毒性能、语义保留和对分布外数据的鲁棒性之间存在权衡，且通常依赖昂贵且数据效率低下的手动标注平行语料。

Method: 本文提出一个两阶段训练框架：首先，在少量高质量、筛选过的平行数据上进行监督微调以建立强初始化；然后，利用无标注的毒性输入和定制设计的奖励模型，使用Group Relative Policy Optimization (GRPO) 训练大型语言模型 (LLM)。

Result: 实验结果表明，该方法有效缓解了现有工作面临的权衡问题，实现了最先进的性能（SOTA），同时提升了模型泛化能力，并显著降低了对标注数据的依赖。

Conclusion: 该两阶段训练框架通过优化数据效率、语义保留和模型泛化能力，为社交媒体文本去毒提供了一种有效方案，能在减少对标注数据依赖的同时达到卓越的性能。

Abstract: The widespread dissemination of toxic content on social media poses a serious
threat to both online environments and public discourse, highlighting the
urgent need for detoxification methods that effectively remove toxicity while
preserving the original semantics. However, existing approaches often struggle
to simultaneously achieve strong detoxification performance, semantic
preservation, and robustness to out-of-distribution data. Moreover, they
typically rely on costly, manually annotated parallel corpora while showing
poor data efficiency. To address these challenges, we propose a two-stage
training framework that jointly optimizes for data efficiency, semantic
preservation, and model generalization. We first perform supervised fine-tuning
on a small set of high-quality, filtered parallel data to establish a strong
initialization. Then, we leverage unlabeled toxic inputs and a custom-designed
reward model to train the LLM using Group Relative Policy Optimization.
Experimental results demonstrate that our method effectively mitigates the
trade-offs faced by previous work, achieving state-of-the-art performance with
improved generalization and significantly reduced dependence on annotated data.
Our code is available at:
https://anonymous.4open.science/r/Detoxification-of-Text-725F/

</details>


### [150] [Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals](https://arxiv.org/abs/2507.01052)
*Ahmed Farooq*

Main category: cs.LG

TL;DR: 本研究提出了一种基于稠密霍普菲尔德网络的新型长序列记忆能量函数，通过引入时间核实现了对长序列模式的高效顺序检索，并成功应用于电影帧的存储与检索，有望解决Transformer在长上下文任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理长序列记忆和时间依赖方面存在局限，特别是Transformer在长上下文任务中的表现受限。研究旨在构建一种能够高效存储和顺序检索长序列模式的方法，并解决Transformer的这些不足。

Method: 研究基于稠密霍普菲尔德网络框架，引入了一种新颖的能量函数，用于长序列记忆。核心方法是提出一个时间核$K(m, k)$，以有效整合时间依赖性，从而实现扩展序列中模式的高效顺序检索。

Result: 该技术成功应用于电影帧的存储和顺序检索。由于电影帧的高维度特性，即使是连续帧之间也能在高维空间中产生足够的差异，这使得它们非常适合作为该方法的应用案例。

Conclusion: 该模型为解决Transformer在长上下文任务中的局局限性提供了一种有前景的方法，具有广泛的应用潜力，包括高效长序列建模、记忆增强、带时间偏置的改进注意力机制，以及时间序列数据中长期依赖的处理，尤其在自然语言处理和预测等领域有重要意义。

Abstract: In this study we introduce a novel energy functional for long-sequence
memory, building upon the framework of dense Hopfield networks which achieves
exponential storage capacity through higher-order interactions. Building upon
earlier work on long-sequence Hopfield memory models, we propose a temporal
kernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient
sequential retrieval of patterns over extended sequences. We demonstrate the
successful application of this technique for the storage and sequential
retrieval of movies frames which are well suited for this because of the high
dimensional vectors that make up each frame creating enough variation between
even sequential frames in the high dimensional space. The technique has
applications in modern transformer architectures, including efficient
long-sequence modeling, memory augmentation, improved attention with temporal
bias, and enhanced handling of long-term dependencies in time-series data. Our
model offers a promising approach to address the limitations of transformers in
long-context tasks, with potential implications for natural language
processing, forecasting, and beyond.

</details>


### [151] [XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science](https://arxiv.org/abs/2507.01054)
*Jithendaraa Subramanian,Linda Hung,Daniel Schweigert,Santosh Suram,Weike Ye*

Main category: cs.LG

TL;DR: 提出一个多模态框架，利用元素组成和X射线衍射（XRD）数据进行材料发现，无需晶体结构输入，并通过预训练提高性能和收敛速度，为材料科学中基于实验数据的无结构基础模型奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有的基于结构的材料发现模型（如晶体图模型）在实际应用中不切实际，因为原子结构通常难以获取。研究旨在开发一种无需晶体结构输入，可直接从实验数据（如元素组成和XRD）学习的模型。

Method: 提出一个可扩展的多模态框架，直接从元素组成和X射线衍射（XRD）数据中学习。该架构结合了模态特定的编码器与交叉注意力融合模块，并在500万样本的Alexandria数据集上进行训练。采用掩码XRD建模（MXM）和对比对齐作为自监督预训练策略。

Result: 预训练能显著加快收敛速度（最高达4.2倍），并提升准确性和表示质量。多模态模型的性能随数据集规模的增加而优于单模态基线，且在大数据量下收益复合增长。

Conclusion: 本研究为材料科学中开发无需结构输入、基于实验数据的基础模型提供了可行的途径。

Abstract: Recent advances in materials discovery have been driven by structure-based
models, particularly those using crystal graphs. While effective for
computational datasets, these models are impractical for real-world
applications where atomic structures are often unknown or difficult to obtain.
We propose a scalable multimodal framework that learns directly from elemental
composition and X-ray diffraction (XRD) -- two of the more available modalities
in experimental workflows without requiring crystal structure input. Our
architecture integrates modality-specific encoders with a cross-attention
fusion module and is trained on the 5-million-sample Alexandria dataset. We
present masked XRD modeling (MXM), and apply MXM and contrastive alignment as
self-supervised pretraining strategies. Pretraining yields faster convergence
(up to 4.2x speedup) and improves both accuracy and representation quality. We
further demonstrate that multimodal performance scales more favorably with
dataset size than unimodal baselines, with gains compounding at larger data
regimes. Our results establish a path toward structure-free, experimentally
grounded foundation models for materials science.

</details>


### [152] [Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI](https://arxiv.org/abs/2507.01056)
*Lidan Peng,Lu Gao,Feng Hong,Jingran Sun*

Main category: cs.LG

TL;DR: 研究量化了洪水对路面粗糙度（IRI）恶化的影响，发现受洪水影响路面粗糙度增加更快，并强调了抗洪策略的重要性。


<details>
  <summary>Details</summary>
Motivation: 洪水对路面基础设施造成严重损害，本研究旨在调查和量化洪水事件如何影响路面恶化，特别是通过国际粗糙度指数（IRI）来衡量。

Method: 利用TxDOT PMIS数据库20年路面状况数据，整合洪水事件数据；进行统计分析比较洪水前后IRI值并计算恶化率；应用可解释人工智能（XAI）技术（如SHAP和LIME）评估洪水影响。

Result: 受洪水影响的路面比未受洪水影响的路段粗糙度增加速度更快。

Conclusion: 研究结果强调，为提高易受洪水影响区域的路面弹性，需采取主动的洪水缓解策略，包括改善排水系统、使用抗洪材料和进行预防性维护。

Abstract: Flooding can damage pavement infrastructure significantly, causing both
immediate and long-term structural and functional issues. This research
investigates how flooding events affect pavement deterioration, specifically
focusing on measuring pavement roughness by the International Roughness Index
(IRI). To quantify these effects, we utilized 20 years of pavement condition
data from TxDOT's PMIS database, which is integrated with flood event data,
including duration and spatial extent. Statistical analyses were performed to
compare IRI values before and after flooding and to calculate the deterioration
rates influenced by flood exposure. Moreover, we applied Explainable Artificial
Intelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and
Local Interpretable Model-Agnostic Explanations (LIME), to assess the impact of
flooding on pavement performance. The results demonstrate that flood-affected
pavements experience a more rapid increase in roughness compared to non-flooded
sections. These findings emphasize the need for proactive flood mitigation
strategies, including improved drainage systems, flood-resistant materials, and
preventative maintenance, to enhance pavement resilience in vulnerable regions.

</details>


### [153] [Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates](https://arxiv.org/abs/2507.01057)
*Lushun Fan,Yuqin Xia,Jun Li,Karl Jenkins*

Main category: cs.LG

TL;DR: 本研究提出一种基于深度卷积神经网络的智能网格质量优化系统，利用Loop2Net生成器和损失函数，实现从给定机翼坐标生成并优化网格。


<details>
  <summary>Details</summary>
Motivation: 开发一种创新的智能系统，以优化网格质量并实现高效的网格生成与优化。

Method: 提出一种基于深度卷积神经网络（DCNN）架构的智能优化系统，核心是Loop2Net生成器和损失函数。该系统根据给定的机翼坐标预测网格，并通过训练过程中使用的两个关键损失函数以及添加惩罚项进行持续优化。

Result: 成功实现了网格生成的目标。

Conclusion: 所提出的基于深度卷积神经网络的智能优化系统，特别是Loop2Net生成器和损失函数，能够有效地实现网格的生成与优化。

Abstract: In this study, an innovative intelligent optimization system for mesh quality
is proposed, which is based on a deep convolutional neural network
architecture, to achieve mesh generation and optimization. The core of the
study is the Loop2Net generator and loss function, it predicts the mesh based
on the given wing coordinates. And the model's performance is continuously
optimised by two key loss functions during the training. Then discipline by
adding penalties, the goal of mesh generation was finally reached.

</details>


### [154] [Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services](https://arxiv.org/abs/2507.01067)
*Keun Soo Yim*

Main category: cs.LG

TL;DR: 本文优化并应用基础模型预测罕见、尖峰状的生产中断事件，对比传统随机模型，在一年期停机统计中取得了低于6%的误差。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型尚未应用于预测罕见、尖峰状的极端事件，这是一个具有挑战性的目标，但对高科技服务至关重要。

Method: 优化了最先进的基础模型，用于预测高性能机器学习服务偶发性或尖峰状的生产中断。通过对比基础模型与经典随机预测模型（如移动平均和自回归模型）的预测误差，分析它们在处理偶发性事件时的表现及其能捕捉的关键模式。

Result: 使用优化参数的模型估算了一年期的特定根源停机统计数据，实现了低于6%的价值误差。分析揭示了基础模型和随机模型在跟踪目标数据中关键模式上的表现差异。

Conclusion: 优化后的基础模型能够有效预测罕见、尖峰状的生产中断事件，并提供高精度的长期停机统计，优于传统随机模型，证明了其在该类挑战性任务中的潜力。

Abstract: Time series forecasting models have diverse real world applications (e.g.,
from electricity metrics to software workload). Latest foundational models
trained for time series forecasting show strengths (e.g., for long sequences
and in zero-shot settings). However, foundational model was not yet used for
forecasting rare, spiky events, i.e., a challenging target because those are a
corner case of extreme events. In this paper, we optimize a state-of-the-art
foundational model to forecast sporadic or spiky production outages of
high-performance machine learning services powering billions of client devices.
We evaluate the forecasting errors of the foundational model compared with
classical stochastic forecasting models (e.g., moving average and
autoregressive). The analysis helps us understand how each of the evaluated
models performs for the sporadic or spiky events. For example, it identifies
the key patterns in the target data that are well tracked by the foundational
model vs. each of the stochastic models. We use the models with optimal
parameters to estimate a year-long outage statistics of a particular root cause
with less than 6% value errors.

</details>


### [155] [Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors](https://arxiv.org/abs/2507.01068)
*Biplov Paneru*

Main category: cs.LG

TL;DR: 本研究利用IMU数据开发可解释AI方法，用于帕金森病步态冻结（FOG）的早期检测与预测，通过高性能的Stacking Ensemble模型实现近99%的分类准确率，并提出基于联邦学习的预测框架。


<details>
  <summary>Details</summary>
Motivation: 针对帕金森病患者常见的步态冻结（FOG）症状，旨在开发可解释的人工智能方法，实现其早期检测与预测。

Method: 1. **数据**：使用惯性测量单元（IMU）数据集。 2. **检测**：采用CatBoost、XGBoost、Extra Trees等机器学习分类器，并构建Stacking Ensemble模型。 3. **可解释性**：使用SHAP进行模型解释分析。 4. **预测**：引入联邦学习框架，利用混合Conv1D + LSTM架构，并通过联邦平均聚合模型。

Result: 1. Stacking Ensemble模型表现优异，分类准确率接近99%，且优于混合双向GRU模型。 2. SHAP分析表明时间（秒）是区分步态模式最具影响力的因素。

Conclusion: 本研究成功开发了高准确度的可解释AI方法用于FOG的检测与预测，并通过联邦学习框架为实际部署提供了可行方案。

Abstract: This study leverages an Inertial Measurement Unit (IMU) dataset to develop
explainable AI methods for the early detection and prediction of Freezing of
Gait (FOG), a common symptom in Parkinson's disease. Machine learning models,
including CatBoost, XGBoost, and Extra Trees classifiers, are employed to
accurately categorize FOG episodes based on relevant clinical features. A
Stacking Ensemble model achieves superior performance, surpassing a hybrid
bidirectional GRU model and reaching nearly 99% classification accuracy. SHAP
interpretability analysis reveals that time (seconds) is the most influential
factor in distinguishing gait patterns. Additionally, the proposed FOG
prediction framework incorporates federated learning, where models are trained
locally on individual devices and aggregated on a central server using a
federated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for
enhanced predictive capability.

</details>


### [156] [Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs](https://arxiv.org/abs/2507.01073)
*Dian Jin*

Main category: cs.LG

TL;DR: 针对图神经网络在分子性质预测中3D结构编码的挑战，本文提出一种基于旋转采样的3D编码模块，通过期望计算和后对齐策略实现旋转不变性，显著提升了模型性能、泛化能力和鲁棒性，同时保持低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络在处理分子3D空间结构时，因分子方向的变异性而面临泛化和鲁棒性限制。传统的旋转不变方法依赖先验知识且泛化性差，而旋转等变方法计算成本高昂，亟需更有效、高效的3D编码方案。

Method: 本文提出一种即插即用3D编码模块。该模块通过计算SO(3)旋转群上的期望，自然实现近似旋转不变性；并通过精心设计的后对齐策略，进一步达到严格的旋转不变性，且不影响性能。

Result: 在QM9和C10数据集上的实验结果表明，该方法在预测准确性、鲁棒性和泛化性能方面均优于现有方法。此外，它还保持了较低的计算复杂性和更高的可解释性。

Conclusion: 该方法为药物发现和材料设计领域中高效且有效地处理3D分子信息提供了有前景的方向。

Abstract: Graph neural networks (GNNs) have achieved remarkable success in molecular
property prediction. However, traditional graph representations struggle to
effectively encode the inherent 3D spatial structures of molecules, as
molecular orientations in 3D space introduce significant variability, severely
limiting model generalization and robustness. Existing approaches primarily
focus on rotation-invariant and rotation-equivariant methods. Invariant methods
often rely heavily on prior knowledge and lack sufficient generalizability,
while equivariant methods suffer from high computational costs. To address
these limitations, this paper proposes a novel plug-and-play 3D encoding module
leveraging rotational sampling. By computing the expectation over the SO(3)
rotational group, the method naturally achieves approximate rotational
invariance. Furthermore, by introducing a carefully designed post-alignment
strategy, strict invariance can be achieved without compromising performance.
Experimental evaluations on the QM9 and C10 Datasets demonstrate superior
predictive accuracy, robustness, and generalization performance compared to
existing methods. Moreover, the proposed approach maintains low computational
complexity and enhanced interpretability, providing a promising direction for
efficient and effective handling of 3D molecular information in drug discovery
and material design.

</details>


### [157] [Provenance Tracking in Large-Scale Machine Learning Systems](https://arxiv.org/abs/2507.01075)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: 本文提出yProv4ML库，用于收集符合W3C PROV标准的溯源数据，以应对大型AI模型训练中计算效率、能耗、可复现性等复杂挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大型AI模型需求的增长，其训练优化面临计算效率、执行时间、准确性和能耗等多维度挑战。为实现分布式资源能源高效利用并确保AI开发的可复现性与可问责性，急需全面工具来监控、分析和理解训练过程中的溯源数据。

Method: 引入并开发了yProv4ML库。该工具旨在以JSON格式收集符合W3C PROV和ProvML标准的溯源数据。它具有灵活性、可扩展性（支持通过插件集成其他数据收集工具），并可与yProv框架及工作流管理系统深度集成。

Result: 成功开发并介绍了yProv4ML库，该库能有效收集大型AI模型训练和部署过程中的标准化溯源数据，为理解资源使用模式、识别低效及提高可复现性提供了基础工具。

Conclusion: yProv4ML库的推出为解决大型AI模型优化中的多维度挑战提供了关键支持。通过标准化溯源数据的收集，该工具使研究人员和工程师能够更好地监控、分析和优化AI开发流程，从而提高效率、能源利用率并确保可复现性。

Abstract: As the demand for large scale AI models continues to grow, the optimization
of their training to balance computational efficiency, execution time, accuracy
and energy consumption represents a critical multidimensional challenge.
Achieving this balance requires not only innovative algorithmic techniques and
hardware architectures but also comprehensive tools for monitoring, analyzing,
and understanding the underlying processes involved in model training and
deployment. Provenance data information about the origins, context, and
transformations of data and processes has become a key component in this
pursuit. By leveraging provenance, researchers and engineers can gain insights
into resource usage patterns, identify inefficiencies, and ensure
reproducibility and accountability in AI development workflows. For this
reason, the question of how distributed resources can be optimally utilized to
scale large AI models in an energy efficient manner is a fundamental one. To
support this effort, we introduce the yProv4ML library, a tool designed to
collect provenance data in JSON format, compliant with the W3C PROV and ProvML
standards. yProv4ML focuses on flexibility and extensibility, and enables users
to integrate additional data collection tools via plugins. The library is fully
integrated with the yProv framework, allowing for higher level pairing in tasks
run also through workflow management systems.

</details>


### [158] [Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels](https://arxiv.org/abs/2507.01077)
*Bogdan Bogdan,Arina Cazacu,Laura Vasilie*

Main category: cs.LG

TL;DR: 提出一种新颖的仅解码器大型语言模型（LLM），用于车载ECU通信日志的异常检测，旨在解决传统方法在专业领域扩展性不足和标签数据不一致的问题，实现低成本、高效率的异常识别。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法（如监督学习或聚类）在汽车通信系统等专业领域面临可扩展性挑战，效果有限。具体研究动机在于：缺乏针对ECU通信定制的LLM，以及不一致的地面真值数据导致的复杂性，使得手动标注成本高且易出错。

Method: 本研究提出一种新颖的仅解码器LLM架构，用于检测电子控制单元（ECU）通信日志中的异常。通过学习UDP通信日志，将异常检测定义为识别与正常行为的时间偏差。引入了一种熵正则化技术，以增加模型在已知异常上的不确定性，同时保持类似场景的一致性。

Result: 本方案提供了三项创新：一种仅解码器异常检测架构、一种处理不一致标签的方法，以及一个可适应不同ECU通信用例的LLM。该系统通过利用仅解码器模型的生成能力，有效解决了手动标注成本高和易出错的问题，实现了更高的可扩展性，能够从少量示例中学习，并提高了复杂通信环境中的检测精度。

Conclusion: 本研究提出的仅解码器LLM为ECU通信系统中的异常检测提供了一种创新、可扩展且适应性强的方法，能够有效处理不一致的标签数据，显著降低手动标注成本，并通过少量学习样本提升复杂环境下的检测准确性。

Abstract: Anomaly detection often relies on supervised or clustering approaches, with
limited success in specialized domains like automotive communication systems
where scalable solutions are essential. We propose a novel decoder-only Large
Language Model (LLM) to detect anomalies in Electronic Control Unit (ECU)
communication logs. Our approach addresses two key challenges: the lack of LLMs
tailored for ECU communication and the complexity of inconsistent ground truth
data. By learning from UDP communication logs, we formulate anomaly detection
simply as identifying deviations in time from normal behavior. We introduce an
entropy regularization technique that increases model's uncertainty in known
anomalies while maintaining consistency in similar scenarios. Our solution
offers three novelties: a decoder-only anomaly detection architecture, a way to
handle inconsistent labeling, and an adaptable LLM for different ECU
communication use cases. By leveraging the generative capabilities of
decoder-only models, we present a new technique that addresses the high cost
and error-prone nature of manual labeling through a more scalable system that
is able to learn from a minimal set of examples, while improving detection
accuracy in complex communication environments.

</details>


### [159] [yProv4ML: Effortless Provenance Tracking for Machine Learning Systems](https://arxiv.org/abs/2507.01078)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: 为解决大型语言模型开发中透明度不足和超参数追踪困难的问题，本文提出了yProv4ML框架，旨在以PROV-JSON格式捕获机器学习过程中的血缘信息，且所需代码修改量极小，弥补了现有工具的不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速发展揭示了其开发过程中缺乏透明度和严谨性的问题，特别是在确定超参数以识别最佳模型方面的挑战。现有机器学习框架（如MLFlow）捕获数据使用专有格式且不注重血缘信息，无法有效解决此问题。

Method: 本文提出并开发了yProv4ML框架。该框架通过最小化代码修改，以标准化的PROV-JSON格式捕获机器学习过程中产生的血缘信息。

Result: 本研究提出了yProv4ML框架，旨在作为一种解决方案，用于捕获机器学习过程中的血缘信息，以克服现有工具在数据格式专有和血缘追踪不足方面的局限性。

Conclusion: yProv4ML框架提供了一种有效且低侵入性的方式，以开放标准（PROV-JSON）记录机器学习过程的血缘信息，有助于提高LLM及其他ML模型开发的透明度、可复现性和严谨性。

Abstract: The rapid growth of interest in large language models (LLMs) reflects their
potential for flexibility and generalization, and attracted the attention of a
diverse range of researchers. However, the advent of these techniques has also
brought to light the lack of transparency and rigor with which development is
pursued. In particular, the inability to determine the number of epochs and
other hyperparameters in advance presents challenges in identifying the best
model. To address this challenge, machine learning frameworks such as MLFlow
can automate the collection of this type of information. However, these tools
capture data using proprietary formats and pose little attention to lineage.
This paper proposes yProv4ML, a framework to capture provenance information
generated during machine learning processes in PROV-JSON format, with minimal
code modifications.

</details>


### [160] [Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept](https://arxiv.org/abs/2507.01080)
*Edouard Lansiaux,Ramy Azzouz,Emmanuel Chazard,Amélie Vromant,Eric Wiel*

Main category: cs.LG

TL;DR: 本研究比较了三种AI模型（NLP、LLM和JEPA）在急诊分诊中的表现，发现LLM模型在预测准确性上优于其他模型和人工护士分诊。


<details>
  <summary>Details</summary>
Motivation: 急诊科面临分诊错误、患者量增加和人手短缺的挑战，人工智能的引入有望提高分诊准确性和效率。

Method: 研究回顾性分析了法国一家医院7个月的急诊成人患者分诊数据，训练并验证了三款AI模型（基于NLP的TRIAGEMASTER、基于LLM的URGENTIAPARSE和基于JEPA的EMERGINET），以FRENCH量表作为金标准，使用F1-Score、Weighted Kappa等指标评估AI预测分诊级别的符合度。

Result: LLM模型（URGENTIAPARSE）的准确性（综合评分：2.514）显著高于JEPA（0.438）和NLP（-3.511）模型，并且优于护士分诊（-4.343）。URGENTIAPARSE在预测住院需求方面也表现出色，并对结构化数据具有更强的鲁棒性。

Conclusion: LLM架构通过对患者信息的抽象表示，在测试模型中提供了最准确的分诊预测。将AI整合到急诊流程中，有望提升患者安全和运营效率，但其实际整合需要解决模型局限性并确保伦理透明度。

Abstract: Triage errors, including undertriage and overtriage, are persistent
challenges in emergency departments (EDs). With increasing patient influx and
staff shortages, the integration of artificial intelligence (AI) into triage
protocols has gained attention. This study compares the performance of three AI
models [Natural Language Processing (NLP), Large Language Models (LLM), and
Joint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes
against the FRENCH scale and clinical practice.We conducted a retrospective
analysis of a prospectively recruited cohort gathering adult patient triage
data over a 7-month period at the Roger Salengro Hospital ED (Lille, France).
Three AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)
URGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic
details, verbatim chief complaints, vital signs, and triage outcomes based on
the FRENCH scale and GEMSA coding. The primary outcome was the concordance of
AI-predicted triage level with the FRENCH gold-standard. It was assessed thanks
to various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM
model (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared
to JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse
triage (-4.343). Secondary analyses highlighted the effectiveness of
URGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness
with structured data versus raw transcripts (either for GEMSA prediction or for
FRENCH prediction). LLM architecture, through abstraction of patient
representations, offers the most accurate triage predictions among tested
models. Integrating AI into ED workflows could enhance patient safety and
operational efficiency, though integration into clinical workflows requires
addressing model limitations and ensuring ethical transparency.

</details>


### [161] [Proof of a perfect platonic representation hypothesis](https://arxiv.org/abs/2507.01098)
*Liu Ziyin,Isaac Chuang*

Main category: cs.LG

TL;DR: 本文详细阐释了EDLN模型中“完美”柏拉图式表示假设(PRH)的证明。研究发现，SGD训练使EDLN达到完美柏拉图式状态，且这种现象与渐进锐化有共同原因，突出了SGD训练中“熵力”的关键作用。


<details>
  <summary>Details</summary>
Motivation: 尽管损失函数的大多数全局最小值并非柏拉图式的，但SGD却能收敛到完美的柏拉图式解，这种异常现象促使深入探究其发生机制。

Method: 通过详细阐述并解释Ziyin et al. (2025)提出的证明，以指导性说明而非冗长技术细节的方式进行。

Result: 1. 在SGD训练下，即使宽度、深度及数据不同，EDLN也能达到“完美柏拉图式”（层间表示在旋转意义上一致）。 2. 证明揭示了至少六种可能破坏PRH的情形。 3. 柏拉图式表示的出现与渐进锐化现象在EDLN模型中具有共同原因。

Conclusion: 柏拉图式表示和渐进锐化这两个看似独立的深度学习现象，可能源于共同的机制。这项理论和证明强调了理解SGD训练不可逆性所产生的“熵力”及其在表示学习中作用的重要性。

Abstract: In this note, we elaborate on and explain in detail the proof given by Ziyin
et al. (2025) of the "perfect" Platonic Representation Hypothesis (PRH) for the
embedded deep linear network model (EDLN). We show that if trained with SGD,
two EDLNs with different widths and depths and trained on different data will
become Perfectly Platonic, meaning that every possible pair of layers will
learn the same representation up to a rotation. Because most of the global
minima of the loss function are not Platonic, that SGD only finds the perfectly
Platonic solution is rather extraordinary. The proof also suggests at least six
ways the PRH can be broken. We also show that in the EDLN model, the emergence
of the Platonic representations is due to the same reason as the emergence of
progressive sharpening. This implies that these two seemingly unrelated
phenomena in deep learning can, surprisingly, have a common cause. Overall, the
theory and proof highlight the importance of understanding emergent "entropic
forces" due to the irreversibility of SGD training and their role in
representation learning. The goal of this note is to be instructive and avoid
lengthy technical details.

</details>


### [162] [A Neural Operator based on Dynamic Mode Decomposition](https://arxiv.org/abs/2507.01117)
*Nikita Sakovich,Dmitry Aksenov,Ekaterina Pleshakova,Sergey Gataullin*

Main category: cs.LG

TL;DR: 本研究提出一种结合动态模态分解（DMD）和深度学习（DL）的新型神经算子，旨在高效建模时空过程并解决偏微分方程（PDEs）的计算资源消耗问题，在多种方程求解中展现出高精度和计算效率优势。


<details>
  <summary>Details</summary>
Motivation: 科学计算与人工智能结合是热门研究方向，但传统方法求解偏微分方程（PDEs）需要大量计算资源。研究动机在于开发一种轻量级、高精度的计算方法，以高效建模时空过程。

Method: 本研究提出一种基于动态模态分解算法（DMD）的神经算子，该方法将DMD与深度学习结合，通过自动提取关键模式和系统动力学来构建预测，从而降低计算成本。

Result: 该方法在热方程、拉普拉斯方程和伯格斯方程的近似解中，与DeepONet和FNO等现有模拟方法进行性能比较，展现出更高的重建精度和计算效率，有效降低了计算成本。

Conclusion: 所提出的结合DMD和深度学习的神经算子，能够高效且准确地建模时空过程并近似求解PDE，为科学计算提供了新的轻量级、高精度解决方案，并超越了现有主流方法。

Abstract: The scientific computation methods development in conjunction with artificial
intelligence technologies remains a hot research topic. Finding a balance
between lightweight and accurate computations is a solid foundation for this
direction. The study presents a neural operator based on the dynamic mode
decomposition algorithm (DMD), mapping functional spaces, which combines DMD
and deep learning (DL) for spatiotemporal processes efficient modeling. Solving
PDEs for various initial and boundary conditions requires significant
computational resources. The method suggested automatically extracts key modes
and system dynamics using them to construct predictions, reducing computational
costs compared to traditional numerical methods. The approach has demonstrated
its efficiency through comparative analysis of performance with closest
analogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers
equation solutions approximation, where it achieves high reconstruction
accuracy.

</details>


### [163] [On Design Principles for Private Adaptive Optimizers](https://arxiv.org/abs/2507.01129)
*Arun Ganesh,Brendan McMahan,Abhradeep Thakurta*

Main category: cs.LG

TL;DR: 差分隐私训练中，梯度噪声损害自适应优化器性能。本文发现追求梯度二阶矩无偏估计是误导的，提出并验证“scale-then-privatize”技术在理论和实践中均表现更优。


<details>
  <summary>Details</summary>
Motivation: 差分隐私（DP）训练中，梯度的球形噪声严重影响AdaGrad、Adam等自适应优化器的性能。现有解决方案的经验结果主要限于简单任务和模型，可能无法推广到实际应用。

Method: 本文综述并比较了多种DP训练中的自适应优化器变体，通过理论分析和在小型语言模型训练任务上的实证研究进行评估。

Result: 发现追求梯度二阶矩无偏估计的传统直觉是错误的。“scale-then-privatize”技术虽然未实现无偏估计，但在理论上更优，并在小型语言模型训练中表现超越其他变体。

Conclusion: 在差分隐私训练中，“scale-then-privatize”是一种更有效的自适应优化器噪声处理策略，它具有更优的理论特性和实际性能，并能更好地适应关联噪声机制。

Abstract: The spherical noise added to gradients in differentially private (DP)
training undermines the performance of adaptive optimizers like AdaGrad and
Adam, and hence many recent works have proposed algorithms to address this
challenge. However, the empirical results in these works focus on simple tasks
and models and the conclusions may not generalize to model training in
practice. In this paper we survey several of these variants, and develop better
theoretical intuition for them as well as perform empirical studies comparing
them. We find that a common intuition of aiming for unbiased estimates of
second moments of gradients in adaptive optimizers is misguided, and instead
that a simple technique called scale-then-privatize (which does not achieve
unbiased second moments) has more desirable theoretical behaviors and
outperforms all other variants we study on a small-scale language model
training task. We additionally argue that scale-then-privatize causes the noise
addition to better match the application of correlated noise mechanisms which
are more desirable to use in practice.

</details>


### [164] [Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations](https://arxiv.org/abs/2507.01131)
*Yuchao Lin,Cong Fu,Zachary Krueger,Haiyang Yu,Maho Nakata,Jianwen Xie,Emine Kucukbenli,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.LG

TL;DR: 提出张量分解网络(TDNs)加速机器学习原子间势(MLIPs)中的SO(3)等变网络，通过低秩张量分解和路径权重共享，大幅降低计算复杂度（从$O(L^6)$到$O(L^4)$），同时保持竞争力能。


<details>
  <summary>Details</summary>
Motivation: SO(3)等变网络是机器学习原子间势(MLIPs)的主流模型，但其核心操作Clebsch-Gordan (CG) 张量积计算成本高昂。

Method: 开发了张量分解网络(TDNs)，用低秩张量分解（如CP分解）替代CG张量积，实现近似等变。通过路径权重共享进一步减少参数，并将计算复杂度从$O(L^6)$降至$O(L^4)$。理论上证明了其等变误差有界性和近似普适性。

Result: 在PubChemQCR、OC20和OC22等数据集上评估，TDNs在计算速度上显著提升的同时，能保持与现有模型相当的性能。

Conclusion: TDNs通过结合张量分解和路径权重共享，为SO(3)等变网络提供了一种高效且实用的加速方案，在计算效率和预测性能之间取得了良好平衡。

Abstract: $\rm{SO}(3)$-equivariant networks are the dominant models for machine
learning interatomic potentials (MLIPs). The key operation of such networks is
the Clebsch-Gordan (CG) tensor product, which is computationally expensive. To
accelerate the computation, we develop tensor decomposition networks (TDNs) as
a class of approximately equivariant networks whose CG tensor products are
replaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)
decomposition. With the CP decomposition, we prove (i) a uniform bound on the
induced error of $\rm{SO}(3)$-equivariance, and (ii) the universality of
approximating any equivariant bilinear map. To further reduce the number of
parameters, we propose path-weight sharing that ties all multiplicity-space
weights across the $O(L^3)$ CG paths into a single path without compromising
equivariance, where $L$ is the maximum angular degree. The resulting layer acts
as a plug-and-play replacement for tensor products in existing networks, and
the computational complexity of tensor products is reduced from $O(L^6)$ to
$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation
dataset containing 105 million DFT-calculated snapshots. We also use existing
datasets, including OC20, and OC22. Results show that TDNs achieve competitive
performance with dramatic speedup in computations.

</details>


### [165] [Spectral Manifold Harmonization for Graph Imbalanced Regression](https://arxiv.org/abs/2507.01132)
*Brenda Nogueira,Gabe Gomes,Meng Jiang,Nitesh V. Chawla,Nuno Moniz*

Main category: cs.LG

TL;DR: 提出SMH方法，通过生成合成图样本解决图结构数据的非平衡回归问题，特别提升对关键目标范围的预测性能。


<details>
  <summary>Details</summary>
Motivation: 科学领域中，图结构数据模型常面临非平衡学习设置，尤其在非平衡回归中，对特定（科学价值高但代表性不足的）目标值范围的研究显著不足。传统方法未能有效解决此问题，常忽略图拓扑或不针对特定领域，导致模型偏向平均值。

Method: 本文提出“光谱流形协调”（Spectral Manifold Harmonization, SMH）方法。SMH通过生成合成图样本来解决图结构数据上的非平衡回归问题，其关键在于生成样本时能保持图的拓扑属性，并专注于通常代表性不足的目标分布区域。

Result: 实验结果表明SMH在化学和药物发现基准数据集上表现出潜力，并在目标域范围内的预测性能上显示出持续的改进。

Conclusion: SMH是一种新颖有效的方法，能解决图结构数据上的非平衡回归挑战，并通过生成拓扑保持的合成样本，显著提升对科学上重要但代表性不足的目标值范围的预测性能。

Abstract: Graph-structured data is ubiquitous in scientific domains, where models often
face imbalanced learning settings. In imbalanced regression, domain preferences
focus on specific target value ranges representing the most scientifically
valuable cases; we observe a significant lack of research. In this paper, we
present Spectral Manifold Harmonization (SMH), a novel approach for addressing
this imbalanced regression challenge on graph-structured data by generating
synthetic graph samples that preserve topological properties while focusing on
often underrepresented target distribution regions. Conventional methods fail
in this context because they either ignore graph topology in case generation or
do not target specific domain ranges, resulting in models biased toward average
target values. Experimental results demonstrate the potential of SMH on
chemistry and drug discovery benchmark datasets, showing consistent
improvements in predictive performance for target domain ranges.

</details>


### [166] [FlashDP: Private Training Large Language Models with Efficient DP-SGD](https://arxiv.org/abs/2507.01154)
*Liangyu Wang,Junxiao Wang,Jie Ren,Zihang Xiang,David E. Keyes,Di Wang*

Main category: cs.LG

TL;DR: FlashDP是一种创新的DP-SGD方法，通过融合操作优化了大型语言模型（LLMs）的差分隐私训练，显著降低了内存和计算开销，同时在保持高吞吐量和准确性的前提下，保护了训练数据隐私。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，其训练数据的隐私保护成为关键问题。差分隐私（DP）是有效的保护机制，但通过DP-SGD实现时，逐样本梯度裁剪的复杂性带来了挑战：现有显式方法（如Opacus）需要大量存储空间，而隐式方法（如GhostClip）则因重复计算导致效率低下。

Method: 本文提出了FlashDP，一种创新的缓存友好型逐层DP-SGD方法。它将所有必要操作整合为一个单一任务，以融合（fused）方式仅计算一次梯度，从而优化了梯度计算和剪裁过程。

Result: 与现有方法相比，FlashDP减少了高达50%的内存移动和20%的冗余计算。它不增加内存需求，并在四A100系统上预训练Llama-13B模型时，达到了非DP方法90%的吞吐量，同时在准确性上与标准逐层裁剪DP-SGD保持一致。

Conclusion: FlashDP是实现高效且隐私保护的LLMs训练的关键进展。该研究成果的代码已开源。

Abstract: As large language models (LLMs) increasingly underpin technological
advancements, the privacy of their training data emerges as a critical concern.
Differential Privacy (DP) serves as a rigorous mechanism to protect this data,
yet its integration via Differentially Private Stochastic Gradient Descent
(DP-SGD) introduces substantial challenges, primarily due to the complexities
of per-sample gradient clipping. Current explicit methods, such as Opacus,
necessitate extensive storage for per-sample gradients, significantly inflating
memory requirements. Conversely, implicit methods like GhostClip reduce storage
needs by recalculating gradients multiple times, which leads to inefficiencies
due to redundant computations. This paper introduces FlashDP, an innovative
cache-friendly per-layer DP-SGD that consolidates necessary operations into a
single task, calculating gradients only once in a fused manner. This approach
not only diminishes memory movement by up to \textbf{50\%} but also cuts down
redundant computations by \textbf{20\%}, compared to previous methods.
Consequently, FlashDP does not increase memory demands and achieves a
\textbf{90\%} throughput compared to the Non-DP method on a four-A100 system
during the pre-training of the Llama-13B model, while maintaining parity with
standard per-layer clipped DP-SGD in terms of accuracy. These advancements
establish FlashDP as a pivotal development for efficient and privacy-preserving
training of LLMs. FlashDP's code has been open-sourced in
https://github.com/kaustpradalab/flashdp.

</details>


### [167] [Diffusion Explorer: Interactive Exploration of Diffusion Models](https://arxiv.org/abs/2507.01178)
*Alec Helbling,Duen Horng Chau*

Main category: cs.LG

TL;DR: 本文介绍了一个名为“Diffusion Explorer”的交互式工具，旨在通过可视化2D扩散模型的几何特性和时间动态，以更易理解的方式解释扩散模型。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像、视频和文本生成中至关重要，并具有显著的几何特性。然而，现有解释资源要么要求高深理论基础，要么只侧重于神经网络架构，忽略其丰富的几何属性，导致难以理解。

Method: 开发了“Diffusion Explorer”这一交互式Web工具。该工具允许用户在浏览器中训练2D扩散模型，并通过交互式动画观察其采样过程的时间动态，从而直观展示扩散模型的几何特性。

Result: 成功开发并推出了“Diffusion Explorer”工具。该工具能够让用户在浏览器中训练并观察2D扩散模型的几何属性和时间动态，提供了可视化和交互式的学习体验。该工具已开源并提供在线演示。

Conclusion: “Diffusion Explorer”通过交互式动画提供了一种新颖且易于理解的方式来解释扩散模型的复杂几何特性和随机过程，弥补了现有解释资源的不足，有助于提升对扩散模型的理解。

Abstract: Diffusion models have been central to the development of recent image, video,
and even text generation systems. They posses striking geometric properties
that can be faithfully portrayed in low-dimensional settings. However, existing
resources for explaining diffusion either require an advanced theoretical
foundation or focus on their neural network architectures rather than their
rich geometric properties. We introduce Diffusion Explorer, an interactive tool
to explain the geometric properties of diffusion models. Users can train 2D
diffusion models in the browser and observe the temporal dynamics of their
sampling process. Diffusion Explorer leverages interactive animation, which has
been shown to be a powerful tool for making engaging visualizations of dynamic
systems, making it well suited to explaining diffusion models which represent
stochastic processes that evolve over time. Diffusion Explorer is open source
and a live demo is available at alechelbling.com/Diffusion-Explorer.

</details>


### [168] [Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning](https://arxiv.org/abs/2507.01196)
*Na Lee,Konstantinos Barmpas,Yannis Panagakis,Dimitrios Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: 本研究评估了当前大型脑电波基础模型（LBMs）在脑机接口（BCI）任务上的能力。结果显示LBMs相比传统模型提升有限但参数量巨大，且存在架构和训练效率问题。研究首次将LoRA应用于LBMs，有效减少了参数，并强调需领域特定设计以充分发挥其潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在AI领域取得巨大成功，但其在脑电波建模方面的能力及适用性尚不明确。

Method: 通过在多个脑机接口（BCI）基准任务（包括记忆任务和睡眠阶段分类）上对现有大型脑电波基础模型（LBMs）进行系统性微调实验和评估。研究还进行了详细的消融研究和低秩适应（LoRA）实验，涵盖了全模型微调和参数高效适应技术。

Result: 1. 最先进的LBMs在BCI任务上仅比传统深度架构获得微小改进（0.9%-1.2%），但所需参数量却显著增加（百万级对比千级）。
2. 通过消融研究和LoRA，可在不降低性能的前提下显著减少LBMs的可训练参数。
3. 研究表明，LBMs的当前能力受限于其架构和训练效率的不足。
4. 首次将LoRA应用于LBMs，发现同时适应多个神经网络组件通常能带来性能提升。

Conclusion: 1. 现有LBMs在脑电波分析中的效率和适用性面临挑战。
2. 迫切需要针对脑电波分析的领域特定开发策略。
3. LBMs的当前架构可能需要重新设计，以充分发挥基础模型在脑电波分析中的潜力。

Abstract: Foundation Models have demonstrated significant success across various
domains in Artificial Intelligence (AI), yet their capabilities for brainwave
modeling remain unclear. In this paper, we comprehensively evaluate current
Large Brainwave Foundation Models (LBMs) through systematic fine-tuning
experiments across multiple Brain-Computer Interface (BCI) benchmark tasks,
including memory tasks and sleep stage classification. Our extensive analysis
shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)
over traditional deep architectures while requiring significantly more
parameters (millions vs thousands), raising important questions about their
efficiency and applicability in BCI contexts. Moreover, through detailed
ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce
trainable parameters without performance degradation, while demonstrating that
architectural and training inefficiencies limit LBMs' current capabilities. Our
experiments span both full model fine-tuning and parameter-efficient adaptation
techniques, providing insights into optimal training strategies for BCI
applications. We pioneer the application of LoRA to LBMs, revealing that
performance benefits generally emerge when adapting multiple neural network
components simultaneously. These findings highlight the critical need for
domain-specific development strategies to advance LBMs, suggesting that current
architectures may require redesign to fully leverage the potential of
foundation models in brainwave analysis.

</details>


### [169] [Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models](https://arxiv.org/abs/2507.01201)
*Hyoseo,Yoon,Yisong Yue,Been Kim*

Main category: cs.LG

TL;DR: 提出JAM框架，通过多目标优化显式对齐独立训练的视觉和语言模型，促成跨模态共享表示。


<details>
  <summary>Details</summary>
Motivation: 现有视觉和语言模型各自独立，但“柏拉图式表征假设”指出它们可能趋向共享的现实统计模型。研究旨在解决核心问题：如何超越事后统计检测，显式优化这些独立表示之间的对齐，使其在保持各自模态结构的同时，实现相互一致性。

Method: 提出“联合自编码器调制器（JAM）”框架。该框架在预训练单模态模型的潜在表示上联合训练模态特定的自编码器，通过重建和跨模态目标（如对比损失、其硬负例变体和作者提出的Spread损失）来促进对齐。评估了对齐目标、对齐层深度和基础模型规模的影响。

Result: 实验结果表明，所提出的轻量级且帕累托高效的JAM框架能够可靠地在冻结、独立训练的表示之间实现对齐。

Conclusion: 该框架为理解和构建跨模态共享结构提供了理论见解，并为将通用单模态基础模型转化为专业的多种模态模型提供了实用途径。

Abstract: Independently trained vision and language models inhabit disjoint
representational spaces, shaped by their respective modalities, objectives, and
architectures. Yet an emerging hypothesis - the Platonic Representation
Hypothesis - suggests that such models may nonetheless converge toward a shared
statistical model of reality. This compatibility, if it exists, raises a
fundamental question: can we move beyond post-hoc statistical detection of
alignment and explicitly optimize for it between such disjoint representations?
We cast this Platonic alignment problem as a multi-objective optimization task
- preserve each modality's native structure while aligning for mutual
coherence. We introduce the Joint Autoencoder Modulator (JAM) framework that
jointly trains modality-specific autoencoders on the latent representations of
pre-trained single modality models, encouraging alignment through both
reconstruction and cross-modal objectives. By analogy, this framework serves as
a method to escape Plato's Cave, enabling the emergence of shared structure
from disjoint inputs. We evaluate this framework across three critical design
axes: (i) the alignment objective - comparing contrastive loss (Con), its
hard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at
which alignment is most effective, and (iii) the impact of foundation model
scale on representational convergence. Our findings show that our lightweight
Pareto-efficient framework reliably induces alignment, even across frozen,
independently trained representations, offering both theoretical insight and
practical pathways for transforming generalist unimodal foundations into
specialist multimodal models.

</details>


### [170] [Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform](https://arxiv.org/abs/2507.01208)
*Pedro R. X. Carmo,Igor de Moura,Assis T. de Oliveira Filho,Djamel Sadok,Cleber Zanchettin*

Main category: cs.LG

TL;DR: 本文提出通过模型蒸馏和剪枝技术，在低成本平台（如树莓派4）上部署实时深度学习入侵检测系统，以应对汽车以太网的流注入攻击，并取得了高精度和快速响应。


<details>
  <summary>Details</summary>
Motivation: 现代汽车日益互联，车载以太网是车内通信的基础设施，但易受流注入等攻击，危及行车安全。虽然基于深度学习的入侵检测系统（IDS）能对抗此类攻击，但其实时运行通常需要昂贵的硬件，限制了实际部署。

Method: 研究评估并应用了快速神经网络推理技术，包括模型蒸馏（Distilling）和模型剪枝（Pruning）。这些技术旨在优化深度学习IDS模型，使其能够在树莓派4等低成本平台上实现实时部署和运行。

Result: 实验结果表明，利用这些技术，入侵检测时间在树莓派4上可达727微秒（µs），同时AUCROC（受试者工作特征曲线下面积）值高达0.9890。

Conclusion: 该研究成功证明，通过应用模型蒸馏和剪枝等快速神经网络推理技术，可以在低成本硬件平台上高效部署高性能的深度学习入侵检测系统，从而为汽车以太网的安全提供经济且实时的解决方案。

Abstract: Modern vehicles are increasingly connected, and in this context, automotive
Ethernet is one of the technologies that promise to provide the necessary
infrastructure for intra-vehicle communication. However, these systems are
subject to attacks that can compromise safety, including flow injection
attacks. Deep Learning-based Intrusion Detection Systems (IDS) are often
designed to combat this problem, but they require expensive hardware to run in
real time. In this work, we propose to evaluate and apply fast neural network
inference techniques like Distilling and Prunning for deploying IDS models on
low-cost platforms in real time. The results show that these techniques can
achieve intrusion detection times of up to 727 {\mu}s using a Raspberry Pi 4,
with AUCROC values of 0.9890.

</details>


### [171] [PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning](https://arxiv.org/abs/2507.01216)
*Xingke Yang,Liang Li,Zhiyi Wan,Sicong Li,Hao Wang,Xiaoqi Qi,Jiang Liu,Tomoaki Ohtsuki,Xin Fu,Miao Pan*

Main category: cs.LG

TL;DR: PAE MobiLLM是一种隐私保护且高效的设备端LLM微调方法，它通过服务器辅助的附加侧调优、激活缓存、单令牌激活快捷方式和加性适配器侧网络设计，解决了现有方法的通信开销大和数据隐私泄露问题。


<details>
  <summary>Details</summary>
Motivation: 设备端大型语言模型（LLM）微调应用前景广阔，但移动设备资源有限。现有服务器辅助方法（如拆分学习、侧调优）存在通信负担重（激活传输）和数据/标签/微调模型泄露给服务器的风险。

Method: 本研究开发了PAE MobiLLM，一种通过服务器辅助附加侧调优部署在移动设备上的隐私保护且高效的LLM微调方法。该方法包含以下创新点：1) 服务器端集成激活缓存，以重用历史激活，减少移动设备重复计算；2) 开发“枢轴”令牌（one-token）激活快捷方式，仅传输单个激活维度以降低通信成本；3) 引入加性适配器侧网络设计，使服务器基于设备定义的预测差异而非原始真值标签训练适配器模块，从而确保服务器仅协助计算而不获取隐私数据。

Result: PAE MobiLLM成功解决了现有服务器辅助LLM微调方法面临的通信负担重和数据/标签/微调模型泄露的隐私问题。通过其设计，它能够加速微调收敛并提高计算效率。

Conclusion: PAE MobiLLM提供了一个创新的、隐私保护且高效的解决方案，使得在资源受限的移动设备上进行LLM微调成为可能，有效克服了传统服务器辅助方法的局限性。

Abstract: There is a huge gap between numerous intriguing applications fostered by
on-device large language model (LLM) fine-tuning (FT) from fresh mobile data
and the limited resources of a mobile device. While existing server-assisted
methods (e.g., split learning or side-tuning) may enable LLM FT on the local
mobile device, they suffer from heavy communication burdens of activation
transmissions, and may disclose data, labels or fine-tuned models to the
server. To address those issues, we develop PAE MobiLLM, a privacy-aware and
efficient LLM FT method which can be deployed on the mobile device via
server-assisted additive side-tuning. To further accelerate FT convergence and
improve computing efficiency, PAE MobiLLM integrates activation caching on the
server side, which allows the server to reuse historical activations and saves
the mobile device from repeatedly computing forward passes for the recurring
data samples. Besides, to reduce communication cost, PAE MobiLLM develops a
one-token (i.e., ``pivot'' token) activation shortcut that transmits only a
single activation dimension instead of full activation matrices to guide the
side network tuning. Last but not least, PAE MobiLLM introduces the additive
adapter side-network design which makes the server train the adapter modules
based on device-defined prediction differences rather than raw ground-truth
labels. In this way, the server can only assist device-defined side-network
computing, and learn nothing about data, labels or fine-tuned models.

</details>


### [172] [Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling](https://arxiv.org/abs/2507.01235)
*Bara Rababa,Bilal Farooq*

Main category: cs.LG

TL;DR: 本研究探索量子机器学习（QML）在虚拟现实环境中建模行人压力相关的皮肤电导反应（SCR）。比较了量子支持向量机（QSVM）和量子神经网络（QNN），发现QNN（55%测试准确率）在分类性能上优于QSVM（45%测试准确率）和经典模型。


<details>
  <summary>Details</summary>
Motivation: 利用量子计算的优势处理复杂机器学习任务，特别是在智能交通系统等领域中常见的高维数据表示。具体动机是探索量子机器学习模型能否有效建模虚拟现实道路交叉实验中反映行人压力的复杂皮肤电导反应（SCR）事件。

Method: 开发了两种量子机器学习模型：一是基于Pennylane的量子支持向量机（QSVM），采用八量子位ZZ特征映射；二是量子神经网络（QNN），使用树张量网络结构和八量子位ZZ特征映射。数据集包含皮肤电导反应测量数据，如响应幅度和持续时间，并按幅度进行分类。

Result: QSVM模型在训练阶段表现良好，但存在过拟合问题，测试准确率仅为45%，其分类模型的可靠性受影响。QNN模型表现更优，测试准确率达到55%。

Conclusion: 在建模和分类行人压力引起的皮肤电导反应方面，量子神经网络（QNN）的性能优于量子支持向量机（QSVM）以及经典模型，表明QNN是处理此类复杂分类任务的更好选择。

Abstract: Quantum computing has opened new opportunities to tackle complex machine
learning tasks, for instance, high-dimensional data representations commonly
required in intelligent transportation systems. We explore quantum machine
learning to model complex skin conductance response (SCR) events that reflect
pedestrian stress in a virtual reality road crossing experiment. For this
purpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature
map and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and
an eight-qubit ZZ feature map, were developed on Pennylane. The dataset
consists of SCR measurements along with features such as the response amplitude
and elapsed time, which have been categorized into amplitude-based classes. The
QSVM achieved good training accuracy, but had an overfitting problem, showing a
low test accuracy of 45% and therefore impacting the reliability of the
classification model. The QNN model reached a higher test accuracy of 55%,
making it a better classification model than the QSVM and the classic versions.

</details>


### [173] [Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW](https://arxiv.org/abs/2507.01241)
*Di Zhang,Yihang Zhang*

Main category: cs.LG

TL;DR: 该论文提出一种结合自适应采样的随机共轭次梯度方法，以克服传统SGD在LLM训练中的局限性，显著提高了优化速度、精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统随机梯度下降法 (SGD) 在大规模语言模型 (LLM) 训练中的有效性受到质疑，尤其是在大型应用中，经验证据表明其性能存在潜在限制。

Method: 本文提出一种结合自适应采样的随机共轭次梯度方法，专为LLM训练设计。该方法利用样本复杂度分析自适应选择样本大小，采用随机共轭次梯度法确定搜索方向，并通过类似AdamW的算法自适应调整步长。它旨在保留一阶方法的优势，同时有效处理LLM训练固有的非凸性和非光滑性。

Result: 实验结果表明，所提出的方法不仅保持了传统SGD技术的可扩展性，在许多情况下甚至超越了它们，显著提高了优化过程的速度和精度。

Conclusion: 该方法为大规模LLM训练提供了一种更优的解决方案，成功应对了SGD的性能限制，实现了更快的收敛速度、更高的训练精度和更强的可扩展性。

Abstract: Stochastic gradient-based descent (SGD), have long been central to training
large language models (LLMs). However, their effectiveness is increasingly
being questioned, particularly in large-scale applications where empirical
evidence suggests potential performance limitations. In response, this paper
proposes a stochastic conjugate subgradient method together with adaptive
sampling tailored specifically for training LLMs. The method not only achieves
faster convergence per iteration but also demonstrates improved scalability
compared to traditional SGD techniques. It leverages sample complexity analysis
to adaptively choose the sample size, employs a stochastic conjugate
subgradient approach to determine search directions and utilizing an AdamW-like
algorithm to adaptively adjust step sizes. This approach preserves the key
advantages of first-order methods while effectively addressing the nonconvexity
and non-smoothness inherent in LLMs training. Additionally, we provide a
detailed analysis of the advantage of the algorithm. Experimental results show
that the proposed method not only maintains, but in many cases surpasses, the
scalability of traditional SGD techniques, significantly enhancing both the
speed and accuracy of the optimization process.

</details>


### [174] [PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning](https://arxiv.org/abs/2507.01271)
*Tatsuki Kawakami,Kazuki Egashira,Atsuyuki Miyai,Go Irie,Kiyoharu Aizawa*

Main category: cs.LG

TL;DR: 本文提出PULSE协议以评估大语言多模态模型（LMMs）的遗忘技术在预训练知识和长期持续性方面的表现，揭示现有方法在此两方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LMMs遗忘基准不足，仅考虑微调知识且为单次遗忘操作，未能涵盖模型在不同知识获取阶段（特别是预训练知识）的遗忘需求，也未解决顺序遗忘请求的实际场景。

Method: 引入PULSE协议，旨在模拟LMMs的真实遗忘场景。该协议包含两个关键视角：(i) 预训练知识遗忘，以分析跨知识获取阶段的影响；(ii) 长期可持续性评估，以处理顺序遗忘请求。随后，研究者利用PULSE协议评估了现有遗忘方法。

Result: 研究发现，尽管某些技术能成功遗忘微调知识，但它们难以消除模型在预训练阶段获得的信息。此外，那些在单次操作中有效遗忘一批目标数据的方法，在相同数据被分割并顺序遗忘时，表现出显著的性能下降。

Conclusion: 现有遗忘技术在处理LMMs中的预训练知识遗忘和面对长期、顺序遗忘请求时存在明显局限性，PULSE协议有效地揭示了这些问题。

Abstract: In recent years, unlearning techniques, which are methods for inducing a
model to "forget" previously learned information, have attracted attention as a
way to address privacy and copyright concerns in large language models (LLMs)
and large multimodal models (LMMs). While several unlearning benchmarks have
been established for LLMs, a practical evaluation framework for unlearning in
LMMs has been less explored. Specifically, existing unlearning benchmark for
LMMs considers only scenarios in which the model is required to unlearn
fine-tuned knowledge through a single unlearning operation. In this study, we
introduce PULSE protocol for realistic unlearning scenarios for LMMs by
introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for
analyzing the effect across different knowledge acquisition phases and (ii)
Long-term Sustainability Evaluation to address sequential requests. We then
evaluate existing unlearning methods along these dimensions. Our results reveal
that, although some techniques can successfully unlearn knowledge acquired
through fine-tuning, they struggle to eliminate information learned during
pre-training. Moreover, methods that effectively unlearn a batch of target data
in a single operation exhibit substantial performance degradation when the same
data are split and unlearned sequentially.

</details>


### [175] [Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation](https://arxiv.org/abs/2507.01285)
*Aymen Rayane Khouas,Mohamed Reda Bouadjenek,Hakim Hacid,Sunil Aryal*

Main category: cs.LG

TL;DR: 针对联邦图推荐系统中现有聚合方法忽略用户嵌入特性和锚点用户影响的问题，本文提出了一种新型的基于距离的聚合方法Dist-FedAvg，通过为相似用户分配更高权重并保留锚点用户影响力，显著提高了推荐准确性和聚合效率。


<details>
  <summary>Details</summary>
Motivation: 现有联邦图推荐系统中的聚合方法未充分利用用户嵌入的特性（如复杂性和用户相似性），且在动态用户交互下缺乏自适应性，同时未能有效保留高相关性锚点用户的影响力，导致推荐效果受限。

Method: 本文提出Dist-FedAvg，一种新颖的基于距离的聚合方法。该方法通过为具有相似嵌入的用户分配更高的聚合权重，同时确保高相关性锚点用户在本地更新中保持显著影响力，以提高个性化推荐效果和聚合效率。

Result: 在多个数据集上的实证评估表明，Dist-FedAvg 持续优于基线聚合技术，显著提高了推荐准确性，并能无缝集成到现有联邦学习框架中。

Conclusion: Dist-FedAvg 有效解决了联邦图推荐系统中现有聚合方法的局限性，通过优化用户嵌入聚合过程，显著提升了推荐系统的准确性和聚合效率，为隐私保护下的个性化推荐提供了更优方案。

Abstract: Graph federated recommendation systems offer a privacy-preserving alternative
to traditional centralized recommendation architectures, which often raise
concerns about data security. While federated learning enables personalized
recommendations without exposing raw user data, existing aggregation methods
overlook the unique properties of user embeddings in this setting. Indeed,
traditional aggregation methods fail to account for their complexity and the
critical role of user similarity in recommendation effectiveness. Moreover,
evolving user interactions require adaptive aggregation while preserving the
influence of high-relevance anchor users (the primary users before expansion in
graph-based frameworks). To address these limitations, we introduce
Dist-FedAvg, a novel distance-based aggregation method designed to enhance
personalization and aggregation efficiency in graph federated learning. Our
method assigns higher aggregation weights to users with similar embeddings,
while ensuring that anchor users retain significant influence in local updates.
Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg
consistently outperforms baseline aggregation techniques, improving
recommendation accuracy while maintaining seamless integration into existing
federated learning frameworks.

</details>


### [176] [Neural Hamiltonian Operator](https://arxiv.org/abs/2507.01313)
*Qian Qi*

Main category: cs.LG

TL;DR: 提出一种基于深度学习的神经哈密顿算子（NHO）框架，用于解决高维随机控制问题，并证明了其普适逼近能力。


<details>
  <summary>Details</summary>
Motivation: 高维随机控制问题因“维度灾难”而难以求解。庞特里亚金最大值原理（PMP）及其转化的正向-反向随机微分方程（FBSDEs）提供了一种替代方案，但仍具挑战。

Method: 引入神经哈密顿算子（NHO）框架，通过神经网络参数化耦合的FBSDEs动态，包括反馈控制和价值函数空间梯度。通过训练神经网络以满足PMP一致性条件来寻找最优NHO，将其视为从模拟数据中学习未知算子的统计推断问题。

Result: 证明了NHO在一般鞅驱动下具有普适逼近能力，并为分析此类模型固有的优化挑战提供了清晰的视角。

Conclusion: NHO框架为高维随机控制问题提供了一种严格的、基于算子理论的深度学习解决方案，具有理论保障（普适逼近）并有助于深入分析优化挑战。

Abstract: Stochastic control problems in high dimensions are notoriously difficult to
solve due to the curse of dimensionality. An alternative to traditional dynamic
programming is Pontryagin's Maximum Principle (PMP), which recasts the problem
as a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In
this paper, we introduce a formal framework for solving such problems with deep
learning by defining a \textbf{Neural Hamiltonian Operator (NHO)}. This
operator parameterizes the coupled FBSDE dynamics via neural networks that
represent the feedback control and an ansatz for the value function's spatial
gradient. We show how the optimal NHO can be found by training the underlying
networks to enforce the consistency conditions dictated by the PMP. By adopting
this operator-theoretic view, we situate the deep FBSDE method within the
rigorous language of statistical inference, framing it as a problem of learning
an unknown operator from simulated data. This perspective allows us to prove
the universal approximation capabilities of NHOs under general martingale
drivers and provides a clear lens for analyzing the significant optimization
challenges inherent to this class of models.

</details>


### [177] [ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks](https://arxiv.org/abs/2507.01321)
*Zhiyao Ren,Siyuan Liang,Aishan Liu,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出“双学习假设”以解释ICL后门攻击，并提出ICLShield防御机制，通过调整概念偏好比率，有效提高大型语言模型（LLMs）的上下文学习（ICL）防御后门攻击的能力。


<details>
  <summary>Details</summary>
Motivation: 上下文学习（ICL）虽使大型语言模型（LLMs）表现出色，但其易受后门攻击影响，即攻击者通过污染少量ICL示例即可操纵LLM行为。本研究旨在解决这一关键漏洞。

Method: 本文首次提出“双学习假设”，认为LLMs在受污染示例中同时学习任务相关和后门潜在概念。通过理论分析，揭示ICL后门效应上限受任务与后门的概念偏好比率主导。基于此，提出ICLShield防御机制，该机制通过利用置信度和相似性分数，动态调整概念偏好比率，促使LLMs在ICL阶段选择干净的示例。

Result: 广泛实验表明，ICLShield在多LLM和多任务上均实现最先进的防御效果，平均性能显著优于现有方法（平均提升26.02%）。此外，该方法对闭源模型（如GPT-4）也展现出卓越的适应性和防御性能。

Conclusion: 本研究通过提出双学习假设深入理解了ICL后门攻击的原理，并基于此设计的ICLShield防御机制能够有效抵御此类攻击，显著增强了LLMs上下文学习的安全性，对LLMs的可靠应用具有重要意义。

Abstract: In-context learning (ICL) has demonstrated remarkable success in large
language models (LLMs) due to its adaptability and parameter-free nature.
However, it also introduces a critical vulnerability to backdoor attacks, where
adversaries can manipulate LLM behaviors by simply poisoning a few ICL
demonstrations. In this paper, we propose, for the first time, the
dual-learning hypothesis, which posits that LLMs simultaneously learn both the
task-relevant latent concepts and backdoor latent concepts within poisoned
demonstrations, jointly influencing the probability of model outputs. Through
theoretical analysis, we derive an upper bound for ICL backdoor effects,
revealing that the vulnerability is dominated by the concept preference ratio
between the task and the backdoor. Motivated by these findings, we propose
ICLShield, a defense mechanism that dynamically adjusts the concept preference
ratio. Our method encourages LLMs to select clean demonstrations during the ICL
phase by leveraging confidence and similarity scores, effectively mitigating
susceptibility to backdoor attacks. Extensive experiments across multiple LLMs
and tasks demonstrate that our method achieves state-of-the-art defense
effectiveness, significantly outperforming existing approaches (+26.02% on
average). Furthermore, our method exhibits exceptional adaptability and
defensive performance even for closed-source models (e.g., GPT-4).

</details>


### [178] [Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy](https://arxiv.org/abs/2507.01327)
*Xiaoyun Zhang,Jingqing Ruan,Xing Ma,Yawen Zhu,Jiansong Chen,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: 针对客服对话中异常事件检测的挑战及域外泛化需求，本文提出了一种基于大语言模型和双循环动态课程学习的自适应困惑度感知强化学习（APARL）框架。该框架在F1分数和域外迁移能力上均取得了显著提升，为工业部署提供了优质解决方案。


<details>
  <summary>Details</summary>
Motivation: 在实时客服对话中检测异常事件极具挑战性，原因在于业务数据的复杂性和客户交互的动态性。此外，模型需要展示强大的域外（OOD）泛化能力，以适应不同的业务场景并最大化商业价值。

Method: 提出了一种新颖的自适应困惑度感知强化学习（APARL）框架，该框架利用大型语言模型的先进推理能力进行异常事件检测。APARL引入了双循环动态课程学习架构，使模型能够随着熟练度的提高，逐步专注于更具挑战性的样本，从而有效解决性能瓶颈并显著增强域外迁移能力。

Result: 在食品配送对话任务上的广泛评估表明，该模型显著增强了适应性和鲁棒性，F1分数平均提高了17.19%，在域外迁移测试中平均提高了9.59%。

Conclusion: 该方法为异常检测模型的工业部署提供了一个卓越的解决方案，有助于提高运营效率和商业效益。

Abstract: Detecting abnormal events in real-world customer service dialogues is highly
challenging due to the complexity of business data and the dynamic nature of
customer interactions. Moreover, models must demonstrate strong out-of-domain
(OOD) generalization to enable rapid adaptation across different business
scenarios and maximize commercial value. In this work, we propose a novel
Adaptive Perplexity-Aware Reinforcement Learning (APARL) framework that
leverages the advanced reasoning capabilities of large language models for
abnormal event detection. APARL introduces a dual-loop dynamic curriculum
learning architecture, enabling the model to progressively focus on more
challenging samples as its proficiency increases. This design effectively
addresses performance bottlenecks and significantly enhances OOD
transferability. Extensive evaluations on food delivery dialogue tasks show
that our model achieves significantly enhanced adaptability and robustness,
attaining the highest F1 score with an average improvement of 17.19\%, and an
average improvement of 9.59\% in OOD transfer tests. This method provides a
superior solution for industrial deployment of anomaly detection models,
contributing to improved operational efficiency and commercial benefits.

</details>


### [179] [Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion](https://arxiv.org/abs/2507.01354)
*Chugang Yi,Minghan Yu,Weikang Qian,Yixin Wen,Haizhao Yang*

Main category: cs.LG

TL;DR: 提出小波扩散模型（WDM），将降水数据从10公里超分辨率至1公里，显著提升水文预报精度与效率。


<details>
  <summary>Details</summary>
Motivation: 有效的水文模拟和极端天气分析需要公里级分辨率的降水数据，而现有全球产品（如IMERG）仅提供10公里分辨率，无法满足需求。

Method: 开发了小波扩散模型（WDM），一个生成式条件扩散模型。该模型直接在小波域从MRMS雷达数据中学习降水复杂结构，通过关注高频小波系数生成1公里分辨率的降水场。

Result: WDM实现了10倍空间超分辨率（下采样至1公里），推断速度比基于像素的扩散模型快9倍。它生成了异常真实和详细的1公里降水场，视觉效果优于像素空间模型，伪影更少，并显著提高了采样效率。

Conclusion: WDM为地球科学超分辨率中的准确性和速度双重挑战提供了可靠的解决方案，为更可靠的水文预报奠定了基础。

Abstract: Effective hydrological modeling and extreme weather analysis demand
precipitation data at a kilometer-scale resolution, which is significantly
finer than the 10 km scale offered by standard global products like IMERG. To
address this, we propose the Wavelet Diffusion Model (WDM), a generative
framework that achieves 10x spatial super-resolution (downscaling to 1 km) and
delivers a 9x inference speedup over pixel-based diffusion models. WDM is a
conditional diffusion model that learns the learns the complex structure of
precipitation from MRMS radar data directly in the wavelet domain. By focusing
on high-frequency wavelet coefficients, it generates exceptionally realistic
and detailed 1-km precipitation fields. This wavelet-based approach produces
visually superior results with fewer artifacts than pixel-space models, and
delivers a significant gains in sampling efficiency. Our results demonstrate
that WDM provides a robust solution to the dual challenges of accuracy and
speed in geoscience super-resolution, paving the way for more reliable
hydrological forecasts.

</details>


### [180] [Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning](https://arxiv.org/abs/2507.01551)
*Wu Fei,Hao Kong,Shuxian Liang,Yang Lin,Yibo Yang,Jing Tang,Lei Chen,Xiansheng Hua*

Main category: cs.LG

TL;DR: 过程强化学习（PRL）在提升大语言模型（LLM）推理能力方面有前景，但面临计算开销大和缺乏统一理论框架的问题。本文提出自引导过程奖励优化（SPRO）框架，通过从策略模型内部派生过程奖励并引入掩蔽步骤优势（MSA），显著提高了训练效率和准确率，且无额外计算开销，有利于工业部署。


<details>
  <summary>Details</summary>
Motivation: 现有过程强化学习（PRL）在增强大语言模型（LLM）推理能力方面具有潜力，但其引入额外的过程奖励模型导致高昂计算开销，且缺乏统一的过程级优势估计理论框架。

Method: 本文提出自引导过程奖励优化（SPRO）框架。其主要创新包括：1) 首次理论证明过程奖励可从策略模型自身内部派生；2) 引入明确定义的累积过程奖励和掩蔽步骤优势（MSA），以在共享提示采样组中进行严格的逐步动作优势估计。

Result: 实验证明，SPRO相较于GRPO，训练效率提高3.4倍，测试准确率提升17.5%。SPRO在训练期间保持了稳定且高水平的策略熵，同时将平均响应长度缩短了约1/3，这表明其实现了充分探索并避免了奖励作弊。值得注意的是，SPRO与结果监督型RL方法（如GRPO）相比，没有额外的计算开销。

Conclusion: SPRO框架有效解决了过程强化学习面临的计算开销和理论框架缺失问题，显著提升了LLM的推理性能和训练效率，同时优化了探索行为并降低了响应长度，使其在工业实现中具有显著优势。

Abstract: Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.

</details>


### [181] [Distributional Soft Actor-Critic with Diffusion Policy](https://arxiv.org/abs/2507.01381)
*Tong Liu,Yinuo Wang,Xujie Song,Wenjun Zou,Liangfa Chen,Likun Wang,Bin Shuai,Jingliang Duan,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 本文提出DSAC-D算法，一种基于双扩散网络的分布强化学习方法，旨在解决传统强化学习中价值函数估计偏差和难以获取多模态策略的问题，并在多项控制任务和实际车辆测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习方法常使用单模态分布（如高斯分布）来建模价值函数输出，这容易导致价值函数估计偏差，进而影响算法性能，且难以有效表示多模态策略。

Method: 本文提出DSAC-D（Distributed Soft Actor Critic with Diffusion Policy）算法。通过引入策略熵和价值分布函数，建立了可收敛到最优策略的多模态分布策略迭代框架。利用扩散模型通过逆向采样生成奖励样本，构建了一个能精确表征多峰分布的扩散价值网络。在此基础上，推导出了价值网络和策略网络双重扩散的分布强化学习算法。

Result: 在MuJoCo测试任务中，DSAC-D不仅能学习多模态策略，还在全部9个控制任务中取得了最先进（SOTA）的性能，显著抑制了估计偏差，并使总平均回报比现有主流算法提高了10%以上。真实车辆测试结果表明，DSAC-D能准确表征不同驾驶风格的多模态分布，且扩散策略网络能表征多模态轨迹。

Conclusion: DSAC-D算法成功解决了强化学习中价值函数估计偏差和多模态策略表示的挑战。通过创新的双扩散网络结构，该算法在仿真和实际应用中均展现出卓越的性能和表征能力，为复杂控制任务提供了高效且鲁棒的解决方案。

Abstract: Reinforcement learning has been proven to be highly effective in handling
complex control tasks. Traditional methods typically use unimodal
distributions, such as Gaussian distributions, to model the output of value
distributions. However, unimodal distribution often and easily causes bias in
value function estimation, leading to poor algorithm performance. This paper
proposes a distributional reinforcement learning algorithm called DSAC-D
(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges
of estimating bias in value functions and obtaining multimodal policy
representations. A multimodal distributional policy iteration framework that
can converge to the optimal policy was established by introducing policy
entropy and value distribution function. A diffusion value network that can
accurately characterize the distribution of multi peaks was constructed by
generating a set of reward samples through reverse sampling using a diffusion
model. Based on this, a distributional reinforcement learning algorithm with
dual diffusion of the value network and the policy network was derived. MuJoCo
testing tasks demonstrate that the proposed algorithm not only learns
multimodal policy, but also achieves state-of-the-art (SOTA) performance in all
9 control tasks, with significant suppression of estimation bias and total
average return improvement of over 10\% compared to existing mainstream
algorithms. The results of real vehicle testing show that DSAC-D can accurately
characterize the multimodal distribution of different driving styles, and the
diffusion policy network can characterize multimodal trajectories.

</details>


### [182] [Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning](https://arxiv.org/abs/2507.01389)
*Anbang Wang,Dunbo Cai,Yu Zhang,Yangqing Huang,Xiangyang Feng,Zhihong Zhang*

Main category: cs.LG

TL;DR: 提出一种增强型代理模型，通过引入松弛变量统一因子分解机与Ising表示，以处理更高阶特征交互，并在药物组合效应预测中实现性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有代理模型采用因子分解机结合量子退火进行优化，但其设计为两步过程。本研究受此启发，旨在通过引入松弛变量来增强该模型，并将其两步过程统一为一个集成步骤，同时处理更高阶的特征交互。

Method: 提出一种增强型代理模型，通过在因子分解机及其关联的Ising表示中引入额外的松弛变量，将原有的两步过程统一为单一集成步骤。在训练阶段，松弛变量被迭代更新，使模型能够捕获更高阶的特征交互。该方法应用于药物组合效应预测任务。

Result: 实验结果表明，引入松弛变量显著提升了模型的预测性能。

Conclusion: 本研究提出的算法为构建高效的代理模型提供了一种有前景的方法，并有望利用潜在的量子优势。

Abstract: Recently, a surrogate model was proposed that employs a factorization machine
to approximate the underlying input-output mapping of the original system, with
quantum annealing used to optimize the resulting surrogate function. Inspired
by this approach, we propose an enhanced surrogate model that incorporates
additional slack variables into both the factorization machine and its
associated Ising representation thereby unifying what was by design a two-step
process into a single, integrated step. During the training phase, the slack
variables are iteratively updated, enabling the model to account for
higher-order feature interactions. We apply the proposed method to the task of
predicting drug combination effects. Experimental results indicate that the
introduction of slack variables leads to a notable improvement of performance.
Our algorithm offers a promising approach for building efficient surrogate
models that exploit potential quantum advantages.

</details>


### [183] [Decomposing Prediction Mechanisms for In-Context Recall](https://arxiv.org/abs/2507.01414)
*Sultan Daniels,Dylan Davis,Dhruv Gautam,Wentinn Liao,Gireeja Ranade,Anant Sahai*

Main category: cs.LG

TL;DR: 本文引入了一个结合上下文学习和离散联想记忆的玩具问题，研究Transformer模型如何根据上下文标签回忆并延续序列。研究发现，模型使用至少两种机制来完成此任务，它们具有不同的学习动态，并且这种多机制现象在真实世界的ICL任务中也存在。


<details>
  <summary>Details</summary>
Motivation: 研究动机是深入理解Transformer模型如何执行上下文学习（ICL），特别是在结合了连续回归式学习与离散联想记忆的场景中。论文旨在探讨模型在回忆和延续序列时的内部机制及其学习演变过程，尤其是不同功能（如初始召回与后续预测）的出现时机。

Method: 研究方法包括：1) 创建一个新的玩具问题，融合线性回归式连续上下文学习和离散联想召回。2) 使用来自随机线性确定性动力系统的带符号交错状态观测样本预训练Transformer模型。3) 评估模型在给定上下文标签时召回和延续序列的能力。4) 通过训练动态观察能力出现。5) 通过分布外实验和边缘剪枝对模型权重进行机械分析。6) 使用OLMo训练检查点在ICL翻译任务上验证发现，以确认多机制现象的普遍性。

Result: 关键研究结果如下：1) 该任务需要模型执行两项功能：识别应召回的系统状态并应用于其最后观察状态，以及持续应用正确系统预测后续状态。2) 训练动态显示，第一项能力（识别和召回初始状态）在模型训练后期才出现，而第二项能力（延续预测）发展得早得多。3) 模型通过至少两种独立机制执行下一词元预测：一种机制利用离散符号标签进行联想召回以预测序列的开始；另一种机制主要独立于离散符号标签，基于先前词元和上下文进行“贝叶斯式”预测。这两种机制具有不同的学习动态。4) 在OLMo模型进行的ICL翻译任务中也观察到类似的多机制现象，即第一任务词元与第二任务词元性能出现存在显著差距。

Conclusion: 结论是Transformer模型中的上下文学习，即使是相对简单的任务，也涉及多个在训练不同阶段涌现的独立机制。具体而言，基于离散线索“启动”序列的能力（联想召回）与“延续”序列的能力（预测延续）是可分离的，并以不同速率学习。这种多机制现象并非仅限于玩具问题，也出现在更复杂的真实世界ICL任务（如翻译）中，表明它是Transformer学习ICL的一个基本方面。

Abstract: We introduce a new family of toy problems that combine features of
linear-regression-style continuous in-context learning (ICL) with discrete
associative recall. We pretrain transformer models on sample traces from this
toy, specifically symbolically-labeled interleaved state observations from
randomly drawn linear deterministic dynamical systems. We study if the
transformer models can recall the state of a sequence previously seen in its
context when prompted to do so with the corresponding in-context label. Taking
a closer look at this task, it becomes clear that the model must perform two
functions: (1) identify which system's state should be recalled and apply that
system to its last seen state, and (2) continuing to apply the correct system
to predict the subsequent states. Training dynamics reveal that the first
capability emerges well into a model's training. Surprisingly, the second
capability, of continuing the prediction of a resumed sequence, develops much
earlier.
  Via out-of-distribution experiments, and a mechanistic analysis on model
weights via edge pruning, we find that next-token prediction for this toy
problem involves at least two separate mechanisms. One mechanism uses the
discrete symbolic labels to do the associative recall required to predict the
start of a resumption of a previously seen sequence. The second mechanism,
which is largely agnostic to the discrete symbolic labels, performs a
"Bayesian-style" prediction based on the previous token and the context. These
two mechanisms have different learning dynamics.
  To confirm that this multi-mechanism (manifesting as separate phase
transitions) phenomenon is not just an artifact of our toy setting, we used
OLMo training checkpoints on an ICL translation task to see a similar
phenomenon: a decisive gap in the emergence of first-task-token performance vs
second-task-token performance.

</details>


### [184] [Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling](https://arxiv.org/abs/2507.01679)
*Zeyu Huang,Tianhao Cheng,Zihan Qiu,Zili Wang,Yinghui Xu,Edoardo M. Ponti,Ivan Titov*

Main category: cs.LG

TL;DR: 提出Prefix-RFT，一种统一SFT和RFT的LLM混合后训练方法，通过结合演示和探索学习，解决了现有方法的缺点并实现了更优性能，且易于集成。


<details>
  <summary>Details</summary>
Motivation: 现有LLM后训练技术（SFT和RFT）各有弊端：SFT泛化性差且易行为克隆；RFT易学习到意外行为且对初始策略敏感。因此，需要一种能够结合两者优势并克服其缺陷的统一方法。

Method: 提出Prefix-RFT，一种混合方法，统一了SFT和RFT，并协同了演示学习（来自SFT）和探索学习（来自RFT）。该方法可无缝集成到现有开源框架中，仅需对标准RFT流程进行少量修改。

Result: 在数学推理问题上，Prefix-RFT被经验性地证明是简洁且有效的。其性能不仅超越了独立的SFT和RFT，也优于并行混合策略RFT方法。消融研究证实了该方法对演示数据质量和数量变化的鲁棒性。

Conclusion: 分析表明SFT和RFT具有互补性，Prefix-RFT有效地协调了这两种学习范式。该工作为LLM后训练提供新视角，暗示统一集成演示和探索的范式是未来研究的有前景方向。

Abstract: Existing post-training techniques for large language models are broadly
categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning
(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking
demonstration data but can lead to problematic generalization as a form of
behavior cloning. Conversely, RFT can significantly enhance a model's
performance but is prone to learn unexpected behaviors, and its performance is
highly sensitive to the initial policy. In this paper, we propose a unified
view of these methods and introduce Prefix-RFT, a hybrid approach that
synergizes learning from both demonstration and exploration. Using mathematical
reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is
both simple and effective. It not only surpasses the performance of standalone
SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key
advantage is its seamless integration into existing open-source frameworks,
requiring only minimal modifications to the standard RFT pipeline. Our analysis
highlights the complementary nature of SFT and RFT, and validates that
Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore,
ablation studies confirm the method's robustness to variations in the quality
and quantity of demonstration data. We hope this work offers a new perspective
on LLM post-training, suggesting that a unified paradigm that judiciously
integrates demonstration and exploration could be a promising direction for
future research.

</details>


### [185] [Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs](https://arxiv.org/abs/2507.01457)
*Federico Nicolas Peccia,Frederik Haxel,Oliver Bringmann*

Main category: cs.LG

TL;DR: 提出一种基于TVM的RISC-V RVV AI工作负载高效映射方案。


<details>
  <summary>Details</summary>
Motivation: RISC-V RVV在AI加速方面潜力巨大，但缺乏无需专家知识即可高效利用其向量单元的软件开发方法。现有编译器自动向量化或手写库效率有限，且自动调优框架未集成RVV，阻碍了复杂AI工作负载的高效部署。

Method: 将RISC-V RVV扩展集成到TVM编译器的MetaSchedule框架中，构建一个用于张量操作调优的概率程序自动调优工作流。在FPGA上实现RISC-V SoC并对多种AI工作负载进行调优，同时在商用RISC-V SoC上验证方案。

Result: 与GCC自动向量化相比，执行延迟平均提升46%；与muRISCV-NN相比，平均提升29%。生成的二进制文件代码内存占用更小，更适合嵌入式设备。在商用RVV 1.0 SoC上，比LLVM提出的映射平均快35%。

Conclusion: 该方案能高效地将AI工作负载映射到RISC-V向量单元，显著优于现有方法，且生成的代码占用小，更适合嵌入式设备。

Abstract: RISC-V provides a flexible and scalable platform for applications ranging
from embedded devices to high-performance computing clusters. Particularly, its
RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI
workloads. But writing software that efficiently utilizes the vector units of
RISC-V CPUs without expert knowledge requires the programmer to rely on the
autovectorization features of compilers or hand-crafted libraries like
muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing
the integration with the RISC-V RVV extension, thus heavily limiting the
efficient deployment of complex AI workloads. In this paper, we present a
workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V
vector units. Instead of relying on hand-crafted libraries, we integrated the
RVV extension into TVM's MetaSchedule framework, a probabilistic program
framework for tensor operation tuning. We implemented different RISC-V SoCs on
an FPGA and tuned a wide range of AI workloads on them. We found that our
proposal shows a mean improvement of 46% in execution latency when compared
against the autovectorization feature of GCC, and 29% against muRISCV-NN.
Moreover, the binary resulting from our proposal has a smaller code memory
footprint, making it more suitable for embedded devices. Finally, we also
evaluated our solution on a commercially available RISC-V SoC implementing the
RVV 1.0 Vector Extension and found our solution is able to find mappings that
are 35% faster on average than the ones proposed by LLVM. We open-sourced our
proposal for the community to expand it to target other RISC-V extensions.

</details>


### [186] [Cross-platform Smartphone Positioning at Museums](https://arxiv.org/abs/2507.01469)
*Alessio Ferrato,Fabio Gasparetti,Carla Limongelli,Stefano Mastandrea,Giuseppe Sansonetti,Joaquín Torres-Sospedra*

Main category: cs.LG

TL;DR: 为解决博物馆IPS缺乏专用数据集问题，本文提出了一个收集自博物馆环境的新型RSS数据集BAR，并提供了一个基于邻近和k-NN的定位基线。


<details>
  <summary>Details</summary>
Motivation: 室内定位系统（IPS）在提升文化遗产机构访客体验方面潜力巨大，但其部署面临环境、技术和实验性限制。具体而言，缺乏专门反映博物馆环境的公开可用RSS数据集，严重阻碍了针对此类复杂空间定位算法的开发与评估。

Method: 本文提出了一个名为BAR的新型RSS数据集，该数据集使用Android和iOS平台，在13个博物馆房间的90件艺术品前收集。此外，研究还提供了一个利用邻近方法和k-NN算法的先进位置分类基线。

Result: 主要成果是创建并提供了BAR RSS数据集，以及一个先进的位置分类基线。研究在分析中讨论了结果，并提出了潜在的研究方向。

Conclusion: 通过提供独特的博物馆特定RSS数据集和定位基线，本文解决了现有研究的局限性，为文化遗产机构的IPS未来研究奠定了基础。

Abstract: Indoor Positioning Systems (IPSs) hold significant potential for enhancing
visitor experiences in cultural heritage institutions. By enabling personalized
navigation, efficient artifact organization, and better interaction with
exhibits, IPSs can transform the modalities of how individuals engage with
museums, galleries and libraries. However, these institutions face several
challenges in implementing IPSs, including environmental constraints, technical
limits, and limited experimentation. In other contexts, Received Signal
Strength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have
emerged as preferred solutions due to their non-invasive nature and minimal
infrastructure requirements. Nevertheless, the lack of publicly available RSS
datasets that specifically reflect museum environments presents a substantial
barrier to developing and evaluating positioning algorithms designed for the
intricate spatial characteristics typical of cultural heritage sites. To
address this limitation, we present BAR, a novel RSS dataset collected in front
of 90 artworks across 13 museum rooms using two different platforms, i.e.,
Android and iOS. Additionally, we provide an advanced position classification
baseline taking advantage of a proximity-based method and $k$-NN algorithms. In
our analysis, we discuss the results and offer suggestions for potential
research directions.

</details>


### [187] [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752)
*Ismail Labiad,Mathurin Videau,Matthieu Kowalski,Marc Schoenauer,Alessandro Leite,Julia Kempe,Olivier Teytaud*

Main category: cs.LG

TL;DR: BBoxER是一种针对大型语言模型（LLM）的进化黑盒优化方法，通过隐式数据压缩引入信息瓶颈。它解决了传统梯度方法和现有黑盒方法的局限性，提供理论保障和经验性能提升，适用于隐私敏感环境。


<details>
  <summary>Details</summary>
Motivation: 梯度优化方法依赖大量标记数据，存在隐私、安全（如数据投毒、过拟合）问题。黑盒优化方法虽是替代方案，但对高维参数空间（如LLM）的可扩展性差且计算成本高。

Method: 本文提出BBoxER，一种用于LLM后训练的进化黑盒方法。该方法通过隐式压缩训练数据引入信息瓶颈，并利用信息流的可处理性，提供了泛化、差分隐私、数据投毒攻击易感性和提取攻击鲁棒性的强大理论界限。BBoxER作为轻量级、模块化的增强，在预训练LLM之上运行。

Result: 研究结果表明，BBoxER在理论上为泛化、差分隐私、数据投毒攻击和提取攻击提供了强有力的保证。在LLM实验中，BBoxER在少量迭代后，经验性地显著提升了推理数据集上的性能并展现出良好的泛化能力。

Conclusion: BBoxER作为梯度优化方法之上一个有吸引力的附加组件，特别适用于受限或隐私敏感的环境，并提供了非空泛化保证。它有效解决了现有方法的局限性，并在理论和实践中展现出优势。

Abstract: Gradient-based optimization is the workhorse of deep learning, offering
efficient and scalable training via backpropagation. However, its reliance on
large volumes of labeled data raises privacy and security concerns such as
susceptibility to data poisoning attacks and the risk of overfitting. In
contrast, black box optimization methods, which treat the model as an opaque
function, relying solely on function evaluations to guide optimization, offer a
promising alternative in scenarios where data access is restricted, adversarial
risks are high, or overfitting is a concern. However, black box methods also
pose significant challenges, including poor scalability to high-dimensional
parameter spaces, as prevalent in large language models (LLMs), and high
computational costs due to reliance on numerous model evaluations. This paper
introduces BBoxER, an evolutionary black-box method for LLM post-training that
induces an information bottleneck via implicit compression of the training
data. Leveraging the tractability of information flow, we provide strong
theoretical bounds on generalization, differential privacy, susceptibility to
data poisoning attacks, and robustness to extraction attacks. BBoxER operates
on top of pre-trained LLMs, offering a lightweight and modular enhancement
suitable for deployment in restricted or privacy-sensitive environments, in
addition to non-vacuous generalization guarantees. In experiments with LLMs, we
demonstrate empirically that Retrofitting methods are able to learn, showing
how a few iterations of BBoxER improve performance and generalize well on a
benchmark of reasoning datasets. This positions BBoxER as an attractive add-on
on top of gradient-based optimization.

</details>


### [188] [Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals](https://arxiv.org/abs/2507.01470)
*Yannick Molinghen,Tom Lenaerts*

Main category: cs.LG

TL;DR: 本文探讨了强化学习中当关键子目标不直接获得奖励时（“零激励动态”），现有方法面临的挑战，指出学习性能受奖励时序影响大，并强调需推断潜在任务结构而非依赖即时激励。


<details>
  <summary>Details</summary>
Motivation: 重新审视了强化学习中奖励频率作为任务难度衡量标准的普遍假设，并识别出当核心子目标不直接产生奖励时，现有策略学习方法面临的结构性挑战。

Method: 识别并形式化了“零激励动态”这一概念，通过分析和实验展示了最先进的深度基于子目标的算法未能有效应对这些动态，并揭示了学习性能对子目标完成与最终奖励之间时间邻近性的高度敏感性。

Result: 最先进的深度基于子目标的算法未能有效利用“零激励动态”；学习性能对子目标完成与最终奖励之间的时间接近度高度敏感。

Conclusion: 当前强化学习方法存在根本性局限，迫切需要开发能够不依赖即时激励而推断潜在任务结构的机制。

Abstract: This work re-examines the commonly held assumption that the frequency of
rewards is a reliable measure of task difficulty in reinforcement learning. We
identify and formalize a structural challenge that undermines the effectiveness
of current policy learning methods: when essential subgoals do not directly
yield rewards. We characterize such settings as exhibiting zero-incentive
dynamics, where transitions critical to success remain unrewarded. We show that
state-of-the-art deep subgoal-based algorithms fail to leverage these dynamics
and that learning performance is highly sensitive to the temporal proximity
between subgoal completion and eventual reward. These findings reveal a
fundamental limitation in current approaches and point to the need for
mechanisms that can infer latent task structure without relying on immediate
incentives.

</details>


### [189] [LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs](https://arxiv.org/abs/2507.01806)
*Reza Arabpour,Haitz Sáez de Ocáriz Borde,Anastasis Kratsios*

Main category: cs.LG

TL;DR: 提出一种基于CPU的LoRA微调方法，通过学习元操作符并组合现有LoRA权重，实现在计算资源受限环境下的LLM高效适应。


<details>
  <summary>Details</summary>
Motivation: LoRA在LLM微调中高效，但依赖GPU训练限制了其广泛应用，尤其对于计算资源受限（如仅有标准笔记本CPU）的用户。

Method: 学习一个元操作符，将输入数据集映射到LoRA权重，该操作符利用大量预训练的Mistral-7B适配器。方法通过在CPU上轻量级组合现有LoRA来构建新适配器，而非执行梯度更新。

Result: 尽管生成的适配器性能未能匹配GPU训练的对应版本，但它们在下游任务上始终优于基础Mistral模型。

Conclusion: 该方法为计算资源有限的用户提供了一种实用且可访问的传统GPU微调替代方案。

Abstract: Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.

</details>


### [190] [Loss Functions in Diffusion Models: A Comparative Study](https://arxiv.org/abs/2507.01516)
*Dibyanshu Kumar,Philipp Vaeth,Magda Gregorová*

Main category: cs.LG

TL;DR: 本文对扩散模型中不同的损失函数进行了系统的理论和实证分析，旨在提供一个统一的理解，以指导未来更高效的模型设计。


<details>
  <summary>Details</summary>
Motivation: 扩散模型已成为强大的生成模型，但其训练所用的损失函数存在多种公式，缺乏统一的理解和系统性的对比分析，导致难以选择最适合特定目标的函数。

Method: 研究方法包括：1) 理论分析，详细探讨了不同的目标函数和损失函数，并将其统一在变分下界（VLB）框架下；2) 实验研究，分析了这些目标函数在性能上出现差异的条件及其潜在因素，并评估了目标选择对模型实现特定目标（如生成高质量样本或准确估计似然）的影响。

Result: 研究结果提供了关于不同目标函数性能差异条件及其导致这些差异的潜在因素的见解，并评估了损失函数选择如何影响模型实现特定目标的能力。

Conclusion: 本研究为扩散模型中的损失函数提供了统一的理解，有助于未来进行更高效、更具目标导向性的模型设计。

Abstract: Diffusion models have emerged as powerful generative models, inspiring
extensive research into their underlying mechanisms. One of the key questions
in this area is the loss functions these models shall train with. Multiple
formulations have been introduced in the literature over the past several years
with some links and some critical differences stemming from various initial
considerations. In this paper, we explore the different target objectives and
corresponding loss functions in detail. We present a systematic overview of
their relationships, unifying them under the framework of the variational lower
bound objective. We complement this theoretical analysis with an empirical
study providing insights into the conditions under which these objectives
diverge in performance and the underlying factors contributing to such
deviations. Additionally, we evaluate how the choice of objective impacts the
model ability to achieve specific goals, such as generating high-quality
samples or accurately estimating likelihoods. This study offers a unified
understanding of loss functions in diffusion models, contributing to more
efficient and goal-oriented model designs in future research.

</details>


### [191] [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/abs/2507.01951)
*Zixiao Wang,Yuxin Wang,Xiaorui Wang,Mengting Xing,Jie Gao,Jianjun Xu,Guangcan Liu,Chenhui Jin,Zhuo Wang,Shengzhuo Zhang,Hongtao Xie*

Main category: cs.LG

TL;DR: MetaStone-S1是一个反射式生成模型，通过自监督过程奖励模型（SPRM）实现了与OpenAI o3相当的性能，其特点是高效推理、无需额外标注且支持测试时扩展。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种高效、无需额外标注的生成模型，该模型能够整合策略模型与过程奖励模型，并在测试时提供可控的推理能力，同时显著减少参数量。

Method: 引入了反射式生成模型MetaStone-S1，其核心是自监督过程奖励模型（SPRM）。SPRM通过共享主干网络并使用任务特定头部（分别用于下一词预测和过程评分），成功地将策略模型与过程奖励模型（PRM）集成到统一接口中，从而无需额外过程标注，并减少了超过99%的PRM参数。MetaStone-S1天然适合测试时扩展（TTS），并基于可控思考长度提供了低、中、高三种推理努力模式。此外，研究还经验性地建立了总思考计算与TTS性能之间的缩放定律。

Result: 实验证明，MetaStone-S1在仅320亿参数规模下，实现了与OpenAI o3-mini系列模型相当的性能。

Conclusion: MetaStone-S1通过创新的SPRM方法，成功实现了高性能、高效率和测试时可扩展的生成模型，并已开源以支持社区研究。

Abstract: We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.

</details>


### [192] [Chargax: A JAX Accelerated EV Charging Simulator](https://arxiv.org/abs/2507.01522)
*Koen Ponse,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 本文提出Chargax，一个基于JAX的电动汽车充电站仿真环境，旨在加速强化学习（RL）智能体训练，实现100-1000倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在解决可持续能源挑战（如电网拥堵和运行效率提升）中具有关键作用。然而，传统的强化学习方法因高样本复杂度和昂贵的仿真需求而训练缓慢。尽管现有工作利用JAX加速数据生成，但多集中于经典玩具问题，缺乏针对真实能源系统的应用。

Method: 引入Chargax，一个基于JAX的电动汽车充电站现实仿真环境，专为加速RL智能体训练而设计。该环境在基于真实数据的多种场景下进行验证，并将强化学习智能体与基线进行比较。

Result: Chargax在计算性能上比现有环境有100到1000倍的显著提升。此外，Chargax的模块化架构能够表示多样化的真实世界充电站配置。

Conclusion: Chargax提供了一个高性能、灵活且逼真的电动汽车充电站仿真平台，极大地加速了深度强化学习在能源系统中的研究与应用，有望提升电网运营效率。

Abstract: Deep Reinforcement Learning can play a key role in addressing sustainable
energy challenges. For instance, many grid systems are heavily congested,
highlighting the urgent need to enhance operational efficiency. However,
reinforcement learning approaches have traditionally been slow due to the high
sample complexity and expensive simulation requirements. While recent works
have effectively used GPUs to accelerate data generation by converting
environments to JAX, these works have largely focussed on classical toy
problems. This paper introduces Chargax, a JAX-based environment for realistic
simulation of electric vehicle charging stations designed for accelerated
training of RL agents. We validate our environment in a variety of scenarios
based on real data, comparing reinforcement learning agents against baselines.
Chargax delivers substantial computational performance improvements of over
100x-1000x over existing environments. Additionally, Chargax' modular
architecture enables the representation of diverse real-world charging station
configurations.

</details>


### [193] [MARVIS: Modality Adaptive Reasoning over VISualizations](https://arxiv.org/abs/2507.01544)
*Benjamin Feuer,Lennart Purucker,Oussama Elachqar,Chinmay Hegde*

Main category: cs.LG

TL;DR: MARVIS是一种免训练方法，通过将潜在嵌入空间转换为视觉表示，使小型视觉-语言模型（VLM）能够高精度地预测任何数据模态，并在多个领域取得有竞争力的性能，超越通用大模型，接近专业方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习应用面临两难：专业模型性能优异但缺乏灵活性，而基础模型虽然通用但对非传统模态和长尾领域表现不佳。因此，需要一种方法，既能提供基础模型的通用性，又能接近专业模型的性能，同时避免大量的领域特定训练。

Method: 本文提出了MARVIS（Modality Adaptive Reasoning over VISualizations），一种无需训练的方法。该方法将不同数据模态的潜在嵌入空间转换为视觉表示，然后利用视觉-语言模型（VLM）的空间和细粒度推理能力来解释和利用这些视觉表示，从而使小型VLM能够处理和预测任何数据模态。

Result: MARVIS使用单一的30亿参数模型，在视觉、音频、生物和表格等领域取得了有竞争力的性能。实验结果表明，MARVIS的平均表现比Gemini高出16%，并接近专业方法的水平，同时无需暴露个人身份信息（P.I.I.）或进行任何领域特定训练。

Conclusion: MARVIS成功地提供了一种创新的、免训练的解决方案，使得小型视觉-语言模型能够跨多种数据模态进行有效推理，弥补了现有专业模型和基础模型的不足。其卓越的性能、广泛的适用性以及无需领域特定训练的特点，使其成为处理多模态数据的一个强大且高效的工具。

Abstract: Scientific applications of machine learning often rely on small, specialized
models tuned to particular domains. Such models often achieve excellent
performance, but lack flexibility. Foundation models offer versatility, but
typically underperform specialized approaches, especially on non-traditional
modalities and long-tail domains. We propose MARVIS (Modality Adaptive
Reasoning over VISualizations), a training-free method that enables even small
vision-language models to predict any data modality with high accuracy. MARVIS
transforms latent embedding spaces into visual representations and then
leverages the spatial and fine-grained reasoning skills of VLMs to successfully
interpret and utilize them. MARVIS achieves competitive performance on vision,
audio, biological, and tabular domains using a single 3B parameter model,
achieving results that beat Gemini by 16\% on average and approach specialized
methods, without exposing personally identifiable information (P.I.I.) or
requiring any domain-specific training. We open source our code and datasets at
https://github.com/penfever/marvis

</details>


### [194] [How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks](https://arxiv.org/abs/2507.01559)
*Lapo Frati,Neil Traft,Jeff Clune,Nick Cheney*

Main category: cs.LG

TL;DR: 本文深入探究了神经网络末层权重重采样（“zapping”）以及优化器选择对持续学习和少样本迁移学习中学习与遗忘模式的影响。


<details>
  <summary>Details</summary>
Motivation: 神经网络末层权重重采样（“zapping”）在持续学习中显示出益处，但其深层机制尚不明确，本研究旨在揭示这些机制。

Method: 在持续学习和少样本迁移学习等挑战性设置下，使用卷积神经网络、手写字符和自然图像，详细研究了学习和遗忘的模式。同时测量了多任务设置中每个独立任务受到的影响。

Result: 实验表明，经过“zapping”训练的模型在迁移到新领域时能更快恢复。研究还发现，除了“zapping”，优化器的选择也会深刻影响学习和遗忘的动态，导致模型在序列学习时任务之间出现复杂的协同/干扰模式。

Conclusion: “zapping”有助于模型在新域的快速恢复。同时，“zapping”和优化器选择均能深刻影响模型的学习与遗忘动态，并在多任务序列学习中引出复杂的任务间协同与干扰模式，这对于理解和改进持续学习至关重要。

Abstract: Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-shot transfer learning, with
handwritten characters and natural images. Our experiments show that models
that have undergone zapping during training more quickly recover from the shock
of transferring to a new domain. Furthermore, to better observe the effect of
continual learning in a multi-task setting we measure how each individual task
is affected. This shows that, not only zapping, but the choice of optimizer can
also deeply affect the dynamics of learning and forgetting, causing complex
patterns of synergy/interference between tasks to emerge when the model learns
sequentially at transfer time.

</details>


### [195] [A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning](https://arxiv.org/abs/2507.01581)
*Masood Jan,Wafa Njima,Xun Zhang*

Main category: cs.LG

TL;DR: 提出一种基于联邦学习（FL）和深度神经网络（DNN）的室内定位方法，该方法在保护数据隐私和提高效率的同时，性能接近中心化模型。


<details>
  <summary>Details</summary>
Motivation: 传统室内定位技术存在误差大和隐私泄露问题。机器学习虽能解决环境变化，但其中心化数据聚合又引发隐私、带宽和服务器可靠性等新挑战。本研究旨在克服这些问题。

Method: 本文提出一种基于联邦学习（FL）的动态室内定位方法，并采用深度神经网络（DNN）模型进行实现。

Result: 实验结果显示，该FL方法在保持数据隐私、带宽效率和服务器可靠性的同时，其定位性能与中心化模型（CL）接近。

Conclusion: 本研究证明，所提出的FL方法为隐私增强型室内定位提供了一个可行的解决方案，为未来安全高效的室内定位系统发展铺平了道路。

Abstract: Location information serves as the fundamental element for numerous Internet
of Things (IoT) applications. Traditional indoor localization techniques often
produce significant errors and raise privacy concerns due to centralized data
collection. In response, Machine Learning (ML) techniques offer promising
solutions by capturing indoor environment variations. However, they typically
require central data aggregation, leading to privacy, bandwidth, and server
reliability issues. To overcome these challenges, in this paper, we propose a
Federated Learning (FL)-based approach for dynamic indoor localization using a
Deep Neural Network (DNN) model. Experimental results show that FL has the
nearby performance to Centralized Model (CL) while keeping the data privacy,
bandwidth efficiency and server reliability. This research demonstrates that
our proposed FL approach provides a viable solution for privacy-enhanced indoor
localization, paving the way for advancements in secure and efficient indoor
localization systems.

</details>


### [196] [Analysis of Muon's Convergence and Critical Batch Size](https://arxiv.org/abs/2507.01598)
*Naoki Sato,Hiroki Naganuma,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 本文对新型优化器Muon进行了理论分析，并提供了其收敛性证明、权重衰减效应及最佳批次大小的推导与验证。


<details>
  <summary>Details</summary>
Motivation: Muon是一种利用神经网络参数固有矩阵结构的新型优化器，需要对其理论特性、收敛性、以及不同变体（如Nesterov动量和权重衰减）的影响进行深入分析和理解。

Method: ['对Muon优化器进行理论分析。', '为Muon的四种实际变体（带或不带Nesterov动量，带或不带权重衰减）提供了收敛性证明。', '推导了Muon最小化随机一阶预言（SFO）复杂度的临界批次大小。', '通过实验验证了理论发现。']

Result: ['添加权重衰减可使参数和梯度范数获得更严格的界限。', '阐明了权重衰减系数与学习率之间的关系。', '推导出了最小化随机计算成本的Muon临界批次大小。']

Conclusion: Muon的理论发现得到了实验验证，证明了其在不同配置下的收敛性、权重衰减的积极作用以及优化计算效率的最佳批次大小，为其实际应用提供了坚实的理论基础。

Abstract: This paper presents a theoretical analysis of Muon, a new optimizer that
leverages the inherent matrix structure of neural network parameters. We
provide convergence proofs for four practical variants of Muon: with and
without Nesterov momentum, and with and without weight decay. We then show that
adding weight decay leads to strictly tighter bounds on both the parameter and
gradient norms, and we clarify the relationship between the weight decay
coefficient and the learning rate. Finally, we derive Muon's critical batch
size minimizing the stochastic first-order oracle (SFO) complexity, which is
the stochastic computational cost, and validate our theoretical findings with
experiments.

</details>


### [197] [Kernel Recursive Least Squares Dictionary Learning Algorithm](https://arxiv.org/abs/2507.01636)
*Ghasem Alipoor,Karl Skretting*

Main category: cs.LG

TL;DR: 提出了一种高效的核基在线字典学习算法。


<details>
  <summary>Details</summary>
Motivation: 旨在为核基稀疏表示开发一种高效的在线字典学习算法，以克服现有在线方法在效率或性能上的局限性。

Method: 将输入信号非线性映射到高维特征空间进行稀疏表示。该方法引入了一种基于递归最小二乘（RLS）的新型算法，用于字典的递归更新，支持单样本或小批量处理，并保持较低的计算复杂度。

Result: 在四个不同领域的数据集上进行的实验表明，该方法不仅优于现有的在线核字典学习方法，而且分类精度接近批量训练模型，同时显著提高了效率。

Conclusion: 所提出的核基在线字典学习算法在保持高分类精度的同时，显著提升了计算效率，有效弥补了在线学习与批量学习之间的性能差距。

Abstract: We propose an efficient online dictionary learning algorithm for kernel-based
sparse representations. In this framework, input signals are nonlinearly mapped
to a high-dimensional feature space and represented sparsely using a virtual
dictionary. At each step, the dictionary is updated recursively using a novel
algorithm based on the recursive least squares (RLS) method. This update
mechanism works with single samples or mini-batches and maintains low
computational complexity. Experiments on four datasets across different domains
show that our method not only outperforms existing online kernel dictionary
learning approaches but also achieves classification accuracy close to that of
batch-trained models, while remaining significantly more efficient.

</details>


### [198] [Dance Dance ConvLSTM](https://arxiv.org/abs/2507.01644)
*Miguel O'Malley*

Main category: cs.LG

TL;DR: 本文提出Dance Dance ConvLSTM (DDCL)，一种基于ConvLSTM模型的DDR舞谱自动生成方法，显著提升了生成精度，优于现有Dance Dance Convolution (DDC)方法。


<details>
  <summary>Details</summary>
Motivation: 现有Dance Dance Revolution (DDR)舞谱自动生成算法（如2017年DDC提出的CNN-LSTM架构）仍有改进空间，需要更精确、更优化的方法。

Method: 本文引入了Dance Dance ConvLSTM (DDCL)，这是一种基于ConvLSTM模型的DDR舞谱自动生成新方法。

Result: DDCL方法相较于DDC方法，在舞谱生成精度上取得了显著提升。

Conclusion: DDCL是一种改进且能大幅提高DDR舞谱生成精度的自动化方法。

Abstract: \textit{Dance Dance Revolution} is a rhythm game consisting of songs and
accompanying choreography, referred to as charts. Players press arrows on a
device referred to as a dance pad in time with steps determined by the song's
chart. In 2017, the authors of Dance Dance Convolution (DDC) developed an
algorithm for the automatic generation of \textit{Dance Dance Revolution}
charts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM
(DDCL), a new method for the automatic generation of DDR charts using a
ConvLSTM based model, which improves upon the DDC methodology and substantially
increases the accuracy of chart generation.

</details>


### [199] [GradMetaNet: An Equivariant Architecture for Learning on Gradients](https://arxiv.org/abs/2507.01649)
*Yoav Gelberg,Yam Eitan,Aviv Navon,Aviv Shamsian,Theo,Putterman,Michael Bronstein,Haggai Maron*

Main category: cs.LG

TL;DR: 提出了一种名为GradMetaNet的新型架构，它遵循特定原则设计，用于有效处理神经网络梯度，并在多种梯度相关任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有直接操作梯度的学习算法缺乏针对梯度的专门设计，限制了其适用性，而梯度本身对模型优化、编辑和分析至关重要。

Method: 提出了一种处理梯度的架构设计原则，包括：保持神经元排列对称性的等变设计、处理多数据点梯度集以捕获曲率信息、通过秩-1分解实现高效梯度表示。基于这些原则构建了GradMetaNet。

Result: 证明了GradMetaNet的普适性，并指出其能逼近现有方法无法实现的自然梯度相关函数。在学习优化、INR编辑和损失景观曲率估计等多种梯度相关任务中，对MLP和Transformer模型都展现了有效性。

Conclusion: GradMetaNet提供了一个有原则、高性能的框架来学习和利用神经网络梯度，为解决复杂的梯度相关问题开辟了新的途径。

Abstract: Gradients of neural networks encode valuable information for optimization,
editing, and analysis of models. Therefore, practitioners often treat gradients
as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent
works explore learning algorithms that operate directly on gradients but use
architectures that are not specifically designed for gradient processing,
limiting their applicability. In this paper, we present a principled approach
for designing architectures that process gradients. Our approach is guided by
three principles: (1) equivariant design that preserves neuron permutation
symmetries, (2) processing sets of gradients across multiple data points to
capture curvature information, and (3) efficient gradient representation
through rank-1 decomposition. Based on these principles, we introduce
GradMetaNet, a novel architecture for learning on gradients, constructed from
simple equivariant blocks. We prove universality results for GradMetaNet, and
show that previous approaches cannot approximate natural gradient-based
functions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness
on a diverse set of gradient-based tasks on MLPs and transformers, such as
learned optimization, INR editing, and estimating loss landscape curvature.

</details>


### [200] [AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training](https://arxiv.org/abs/2507.01663)
*Zhenyu Han,Ansheng You,Haibo Wang,Kui Luo,Guang Yang,Wenqi Shi,Menglong Chen,Sicheng Zhang,Zeshun Lan,Chunshi Deng,Huazhong Ji,Wenjie Liu,Yu Huang,Yixiang Zhang,Chenyi Pan,Jing Wang,Xin Huang,Chunsheng Li,Jianping Wu*

Main category: cs.LG

TL;DR: 提出AsyncFlow，一个异步流式RL框架，旨在解决LLM后训练中现有RL框架的可扩展性、资源效率和引擎耦合问题，显著提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有用于LLM后训练的RL框架存在显著的可扩展性瓶颈（任务并置型）、复杂数据流、资源空闲与负载不均（任务分离型）问题。此外，多数框架与LLM训练或推理引擎紧密耦合，难以支持定制引擎。

Method: 我们提出了AsyncFlow，一个异步流式RL框架。它引入了分布式数据存储和传输模块，实现统一数据管理和细粒度调度，促进管道重叠和动态负载均衡。同时，采用基于生产者-消费者的异步工作流，通过在陈旧度阈值内延迟参数更新来最小化计算空闲。核心能力与底层训练和推理引擎解耦，通过面向服务的用户界面提供模块化和可定制的体验。

Result: 与现有最佳基线相比，AsyncFlow平均吞吐量提高了1.59倍。

Conclusion: AsyncFlow的架构为下一代RL训练系统设计提供了可行的见解。

Abstract: Reinforcement learning (RL) has become a pivotal technology in the
post-training phase of large language models (LLMs). Traditional task-colocated
RL frameworks suffer from significant scalability bottlenecks, while
task-separated RL frameworks face challenges in complex dataflows and the
corresponding resource idling and workload imbalance. Moreover, most existing
frameworks are tightly coupled with LLM training or inference engines, making
it difficult to support custom-designed engines. To address these challenges,
we propose AsyncFlow, an asynchronous streaming RL framework for efficient
post-training. Specifically, we introduce a distributed data storage and
transfer module that provides a unified data management and fine-grained
scheduling capability in a fully streamed manner. This architecture inherently
facilitates automated pipeline overlapping among RL tasks and dynamic load
balancing. Moreover, we propose a producer-consumer-based asynchronous workflow
engineered to minimize computational idleness by strategically deferring
parameter update process within staleness thresholds. Finally, the core
capability of AsynFlow is architecturally decoupled from underlying training
and inference engines and encapsulated by service-oriented user interfaces,
offering a modular and customizable user experience. Extensive experiments
demonstrate an average of 1.59 throughput improvement compared with
state-of-the-art baseline. The presented architecture in this work provides
actionable insights for next-generation RL training system designs.

</details>


### [201] [GPT, But Backwards: Exactly Inverting Language Model Outputs](https://arxiv.org/abs/2507.01693)
*Adrians Skapars,Edoardo Manino,Youcheng Sun,Lucas C. Cordeiro*

Main category: cs.LG

TL;DR: 本文提出SODA算法，用于重构大型语言模型（LLM）的精确输入，以支持事后分析和伪造输出检测。


<details>
  <summary>Details</summary>
Motivation: 现有审计技术侧重于识别LLM的不良行为。本研究旨在解决一个互补的取证问题：通过重构导致LLM现有输出的精确输入，从而实现事后分析和潜在的伪造输出报告检测。

Method: 研究将精确输入重构形式化为一个具有唯一全局最小值的离散优化问题。提出了一种名为SODA的高效梯度优化算法，该算法在输入搜索空间的连续松弛上操作，并结合周期性重启和参数衰减策略。

Result: SODA在33M到3B参数范围的LLM上表现显著优于现有方法。对于较短的分布外输入，SODA能够从下一个token的logits中完全恢复79.5%的输入，且没有误报。然而，该方法难以从15个或更多token的较长输入序列的输出中提取私有信息。

Conclusion: 研究结果表明，当前的LLM标准部署实践可能对本方法的恶意使用（尤其是在提取私有信息方面）提供了足够的保护。

Abstract: While existing auditing techniques attempt to identify potential unwanted
behaviours in large language models (LLMs), we address the complementary
forensic problem of reconstructing the exact input that led to an existing LLM
output - enabling post-incident analysis and potentially the detection of fake
output reports. We formalize exact input reconstruction as a discrete
optimisation problem with a unique global minimum and introduce SODA, an
efficient gradient-based algorithm that operates on a continuous relaxation of
the input search space with periodic restarts and parameter decay. Through
comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we
demonstrate that SODA significantly outperforms existing approaches. We succeed
in fully recovering 79.5% of shorter out-of-distribution inputs from next-token
logits, without a single false positive, but struggle to extract private
information from the outputs of longer (15+ token) input sequences. This
suggests that standard deployment practices may currently provide adequate
protection against malicious use of our method. Our code is available at
https://doi.org/10.5281/zenodo.15539879.

</details>


### [202] [PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution](https://arxiv.org/abs/2507.01695)
*Omkar Shende,Gayathri Ananthanarayanan,Marcello Traiola*

Main category: cs.LG

TL;DR: 本文提出PERTINENCE，一种在线方法，利用遗传算法动态选择最合适的预训练深度学习模型来处理给定输入，从而在保持或提升准确性的同时，显著降低计算成本（最高可减少36%的操作）。


<details>
  <summary>Details</summary>
Motivation: 大型深度神经网络虽准确但资源和能耗巨大，且高计算成本仅对部分复杂输入必要。因此，需要开发动态、基于输入的方法来结合现有模型，以提高效率而不牺牲精度。

Method: 引入PERTINENCE，一种在线方法，通过分析输入特征复杂性，动态从预训练模型集中选择最适合的模型。采用遗传算法训练一个基于机器学习的输入调度器，以在精度和计算效率之间达到帕累托最优权衡。在CIFAR-10/100上的CNN和TinyImageNet上的ViT模型上进行了验证。

Result: PERTINENCE在准确性和操作次数之间提供了替代解决方案。通过机会性地选择模型，在达到更好或相当准确度的同时，可将操作次数减少高达36%。

Conclusion: PERTINENCE通过动态模型选择，有效提高了深度神经网络的计算效率，在保持或提升准确性的同时，显著减少了所需的计算资源。

Abstract: Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable
ability to model complex patterns across various domains such as computer
vision, speech recognition, robotics, etc. While large DNN models are often
more accurate than simpler, lightweight models, they are also resource- and
energy-hungry. Hence, it is imperative to design methods to reduce reliance on
such large models without significant degradation in output accuracy. The high
computational cost of these models is often necessary only for a reduced set of
challenging inputs, while lighter models can handle most simple ones. Thus,
carefully combining properties of existing DNN models in a dynamic, input-based
way opens opportunities to improve efficiency without impacting accuracy.
  In this work, we introduce PERTINENCE, a novel online method designed to
analyze the complexity of input features and dynamically select the most
suitable model from a pre-trained set to process a given input effectively. To
achieve this, we employ a genetic algorithm to explore the training space of an
ML-based input dispatcher, enabling convergence towards the Pareto front in the
solution space that balances overall accuracy and computational efficiency.
  We showcase our approach on state-of-the-art Convolutional Neural Networks
(CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers
(ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's
ability to provide alternative solutions to existing state-of-the-art models in
terms of trade-offs between accuracy and number of operations. By
opportunistically selecting among models trained for the same task, PERTINENCE
achieves better or comparable accuracy with up to 36% fewer operations.

</details>


### [203] [Variational Graph Convolutional Neural Networks](https://arxiv.org/abs/2507.01699)
*Illia Oleksiienko,Juho Kanniainen,Alexandros Iosifidis*

Main category: cs.LG

TL;DR: 本文提出变分神经网络版本的图卷积网络，通过估计模型输出和注意力中的不确定性，以同时提高模型可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 模型不确定性估计能够提升图卷积网络（GCNs）的可解释性和准确性，并且在关键应用中为专家验证模型结果提供依据。

Method: 研究者提出了空间和时空图卷积网络的变分神经网络（Variational Neural Network）版本。他们对模型的输出和逐层注意力进行不确定性估计。

Result: 在社交交易分析和基于骨架的人体动作识别任务中（包括芬兰董事会成员、NTU-60、NTU-120和Kinetics数据集），实验结果显示模型准确性有所提高，并成功估计了模型不确定性。

Conclusion: 通过变分神经网络引入不确定性估计，能够有效提升图卷积网络的可解释性和预测准确性，对关键应用具有潜在价值。

Abstract: Estimation of model uncertainty can help improve the explainability of Graph
Convolutional Networks and the accuracy of the models at the same time.
Uncertainty can also be used in critical applications to verify the results of
the model by an expert or additional models. In this paper, we propose
Variational Neural Network versions of spatial and spatio-temporal Graph
Convolutional Networks. We estimate uncertainty in both outputs and layer-wise
attentions of the models, which has the potential for improving model
explainability. We showcase the benefits of these models in the social trading
analysis and the skeleton-based human action recognition tasks on the Finnish
board membership, NTU-60, NTU-120 and Kinetics datasets, where we show
improvement in model accuracy in addition to estimated model uncertainties.

</details>


### [204] [Relational Causal Discovery with Latent Confounders](https://arxiv.org/abs/2507.01700)
*Andrea Piras,Matteo Negro,Ragib Ahsan,David Arbour,Elena Zheleva*

Main category: cs.LG

TL;DR: RelFCI是一种新颖的算法，用于在存在潜在混杂因素的关系数据中进行因果发现。


<details>
  <summary>Details</summary>
Motivation: 从真实关系数据中估计因果效应具有挑战性，因为潜在因果模型和混杂因素未知。现有因果发现算法（包括关系型）要么假定数据独立同分布（不适用于关系数据），要么假定因果充分性（不适用于真实世界数据集），存在研究空白。

Method: 提出了RelFCI算法，一个针对具有潜在混杂因素的关系数据的健全且完备的因果发现算法。该方法基于FCI和RCD算法，并定义了新的图形模型。同时，为带有潜在混杂因素的关系d-分离建立了健全性和完备性保证。

Result: 实验结果表明，RelFCI在识别具有潜在混杂因素的关系因果模型中的正确因果结构方面表现出有效性。

Conclusion: RelFCI成功弥补了现有算法的不足，能够有效识别包含潜在混杂因素的复杂关系数据中的因果结构。

Abstract: Estimating causal effects from real-world relational data can be challenging
when the underlying causal model and potential confounders are unknown. While
several causal discovery algorithms exist for learning causal models with
latent confounders from data, they assume that the data is independent and
identically distributed (i.i.d.) and are not well-suited for learning from
relational data. Similarly, existing relational causal discovery algorithms
assume causal sufficiency, which is unrealistic for many real-world datasets.
To address this gap, we propose RelFCI, a sound and complete causal discovery
algorithm for relational data with latent confounders. Our work builds upon the
Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms
and it defines new graphical models, necessary to support causal discovery in
relational domains. We also establish soundness and completeness guarantees for
relational d-separation with latent confounders. We present experimental
results demonstrating the effectiveness of RelFCI in identifying the correct
causal structure in relational causal models with latent confounders.

</details>


### [205] [B-PL-PINN: Stabilizing PINN Training with Bayesian Pseudo Labeling](https://arxiv.org/abs/2507.01714)
*Kevin Innerebner,Franz M. Rohrhofer,Bernhard C. Geiger*

Main category: cs.LG

TL;DR: 针对PINN收敛性问题，本文提出用贝叶斯PINN替代集成方法，并利用后验方差进行信息传播，实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络（PINNs）在正向问题训练中常遭遇严重的收敛问题，这阻碍了信息从初始条件区域向计算域内部的有效传播。

Method: 本文提出用贝叶斯PINN取代Haitsiukevich和Ilin（2023）提出的集成方法，并以PINN的后验方差评估取代集成共识，以扩展训练域并促进信息传播。

Result: 实验结果显示，该基于数学原理的贝叶斯方法在一系列基准问题上优于传统的集成方法，并且与结合Adam和LBFGS训练的PINN集成方法具有竞争力。

Conclusion: 通过引入贝叶斯PINN并利用其后验方差，可以有效地解决PINNs的收敛性问题并改善信息传播，从而实现优于现有集成方法的性能。

Abstract: Training physics-informed neural networks (PINNs) for forward problems often
suffers from severe convergence issues, hindering the propagation of
information from regions where the desired solution is well-defined.
Haitsiukevich and Ilin (2023) proposed an ensemble approach that extends the
active training domain of each PINN based on i) ensemble consensus and ii)
vicinity to (pseudo-)labeled points, thus ensuring that the information from
the initial condition successfully propagates to the interior of the
computational domain.
  In this work, we suggest replacing the ensemble by a Bayesian PINN, and
consensus by an evaluation of the PINN's posterior variance. Our experiments
show that this mathematically principled approach outperforms the ensemble on a
set of benchmark problems and is competitive with PINN ensembles trained with
combinations of Adam and LBFGS.

</details>


### [206] [Revisiting Learning Rate Control](https://arxiv.org/abs/2507.01724)
*Micha Henheik,Theresa Eimer,Marius Lindauer*

Main category: cs.LG

TL;DR: 本文评估了当前深度学习中学习率控制方法的现状，发现现有方法在跨场景可靠性方面存在不足，且随任务复杂性增加，超参数优化效果下降。研究强调了算法选择方法的重要性，并为未来研究指明了方向。


<details>
  <summary>Details</summary>
Motivation: 学习率是深度学习中最重要的超参数之一，其控制是AutoML和深度学习研究的热点领域。本文旨在评估和比较当前学习率控制范式的现状。

Method: 本文通过比较多种学习率控制范式（包括多保真超参数优化、固定超参数调度和无超参数学习方法）来评估它们的当前性能和适用性。

Result: ['多保真超参数优化、固定超参数调度和无超参数学习等方法在特定深度学习任务上表现良好，但在不同设置下可靠性不足。', '随着模型和任务复杂性增加，超参数优化方法（即使结合多保真方法）的效果趋于下降。']

Conclusion: ['学习率控制中急需算法选择方法，这在AutoML和深度学习社区中一直被忽视。', 'AutoML社区应关注更相关的测试任务，并探索微调方法和元学习等有前景的方向，以显著增强其在该关键因素上的影响力。']

Abstract: The learning rate is one of the most important hyperparameters in deep
learning, and how to control it is an active area within both AutoML and deep
learning research. Approaches for learning rate control span from classic
optimization to online scheduling based on gradient statistics. This paper
compares paradigms to assess the current state of learning rate control. We
find that methods from multi-fidelity hyperparameter optimization,
fixed-hyperparameter schedules, and hyperparameter-free learning often perform
very well on selected deep learning tasks but are not reliable across settings.
This highlights the need for algorithm selection methods in learning rate
control, which have been neglected so far by both the AutoML and deep learning
communities. We also observe a trend of hyperparameter optimization approaches
becoming less effective as models and tasks grow in complexity, even when
combined with multi-fidelity approaches for more expensive model trainings. A
focus on more relevant test tasks and new promising directions like finetunable
methods and meta-learning will enable the AutoML community to significantly
strengthen its impact on this crucial factor in deep learning.

</details>


### [207] [A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based Inference](https://arxiv.org/abs/2507.01740)
*Trung-Dung Hoang,Alceu Bissoto,Vihangkumar V. Naik,Tim Flühmann,Artemii Shlychkov,José Garcia-Tirado,Lisa M. Koch*

Main category: cs.LG

TL;DR: 针对1型糖尿病生理模型参数估计的挑战，本研究提出一种基于神经网络后验估计的模拟推断方法，相比传统方法，该方法能更快速、准确地进行参数推断，并具有更好的泛化能力，实现可靠的实时数字孪生。


<details>
  <summary>Details</summary>
Motivation: 准确估计生理模型参数对实现可靠的数字孪生至关重要，尤其在葡萄糖-胰岛素相互作用复杂的1型糖尿病领域更具挑战性。传统马尔可夫链蒙特卡洛（MCMC）方法在高维参数空间中效率低下且计算成本高昂。

Method: 本研究提出一种基于神经网络后验估计（Neural Posterior Estimation）的模拟推断（Simulation-Based Inference, SBI）方法，旨在高效捕捉餐食摄入、胰岛素和血糖水平之间的复杂关系，实现更快的分摊推断（amortized inference）。

Result: 实验证明，SBI在参数估计方面不仅优于传统方法，而且对未知条件具有更好的泛化能力，能够提供实时后验推断和可靠的不确定性量化。

Conclusion: 本研究成功开发并验证了SBI方法在1型糖尿病生理模型参数估计中的有效性，克服了传统方法的局限性，为实现更快速、准确且具有良好泛化能力的数字孪生提供了可行方案。

Abstract: Accurately estimating parameters of physiological models is essential to
achieving reliable digital twins. For Type 1 Diabetes, this is particularly
challenging due to the complexity of glucose-insulin interactions. Traditional
methods based on Markov Chain Monte Carlo struggle with high-dimensional
parameter spaces and fit parameters from scratch at inference time, making them
slow and computationally expensive. In this study, we propose a
Simulation-Based Inference approach based on Neural Posterior Estimation to
efficiently capture the complex relationships between meal intake, insulin, and
glucose level, providing faster, amortized inference. Our experiments
demonstrate that SBI not only outperforms traditional methods in parameter
estimation but also generalizes better to unseen conditions, offering real-time
posterior inference with reliable uncertainty quantification.

</details>


### [208] [Enhanced Generative Model Evaluation with Clipped Density and Coverage](https://arxiv.org/abs/2507.01761)
*Nicolas Salvy,Hugues Talbot,Bertrand Thirion*

Main category: cs.LG

TL;DR: 针对生成模型质量评估中现有指标的可靠性、可解释性和鲁棒性不足，本文提出Clipped Density和Clipped Coverage两种新指标，通过裁剪机制提升评估准确性与可解释性，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 生成模型虽有显著进展，但其在关键应用中受限于无法可靠评估样本质量（包括保真度和覆盖率）。现有质量指标因缺乏校准或对异常值不鲁棒，导致评估值不可靠且难以解释。

Method: 引入两种新型评估指标：Clipped Density和Clipped Coverage。通过裁剪单个样本的贡献以及（针对保真度）最近邻球的半径，以防止离群样本对聚合值产生偏差影响。

Result: 通过分析和经验校准，新指标的分数随不良样本比例增加呈线性下降，可以直接解释为良好样本的等效比例。在合成和真实世界数据集上的广泛实验表明，Clipped Density和Clipped Coverage在鲁棒性、敏感性和可解释性方面均优于现有生成模型评估方法。

Conclusion: Clipped Density和Clipped Coverage有效解决了现有生成模型质量评估指标的不足，提供了更可靠、鲁棒且可解释的评估方法，有助于更准确地衡量生成样本质量，从而促进生成模型在关键应用中的应用。

Abstract: Although generative models have made remarkable progress in recent years,
their use in critical applications has been hindered by their incapacity to
reliably evaluate sample quality. Quality refers to at least two complementary
concepts: fidelity and coverage. Current quality metrics often lack reliable,
interpretable values due to an absence of calibration or insufficient
robustness to outliers. To address these shortcomings, we introduce two novel
metrics, Clipped Density and Clipped Coverage. By clipping individual sample
contributions and, for fidelity, the radii of nearest neighbor balls, our
metrics prevent out-of-distribution samples from biasing the aggregated values.
Through analytical and empirical calibration, these metrics exhibit linear
score degradation as the proportion of poor samples increases. Thus, they can
be straightforwardly interpreted as equivalent proportions of good samples.
Extensive experiments on synthetic and real-world datasets demonstrate that
Clipped Density and Clipped Coverage outperform existing methods in terms of
robustness, sensitivity, and interpretability for evaluating generative models.

</details>


### [209] [BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification](https://arxiv.org/abs/2507.01781)
*Dalia Rodríguez-Salas,Christian Riess*

Main category: cs.LG

TL;DR: BranchNet是一个神经符号学习框架，将决策树集成转化为稀疏神经网络，在多类别分类任务上以统计学显著优势超越XGBoost，同时保持模型紧凑和可解释。


<details>
  <summary>Details</summary>
Motivation: 研究旨在结合决策树集成的结构化优势与神经网络的梯度优化能力，开发一种既可解释又高性能的模型，以解决现有模型可能存在的不足。

Method: 引入BranchNet框架，该框架将决策树集成中的每个决策路径（从根到叶子父节点）映射为一个隐藏神经元，从而构建稀疏、部分连接的神经网络。此方法保留了符号结构并允许基于梯度的优化。

Result: BranchNet在多种结构化多类别分类基准测试中，准确性持续优于XGBoost，并取得了统计学上显著的提升。此外，生成的模型紧凑、可解释，且无需手动进行架构调优。

Conclusion: BranchNet成功地结合了决策树的结构性和神经网络的优化能力，在多类别分类任务中展现出卓越的性能和良好的可解释性。尽管在二元任务上仍有局限性，可能需要进一步的自适应校准，但它为神经符号学习提供了一个有前景的方向。

Abstract: We introduce BranchNet, a neuro-symbolic learning framework that transforms
decision tree ensembles into sparse, partially connected neural networks. Each
branch, defined as a decision path from root to a parent of leaves, is mapped
to a hidden neuron, preserving symbolic structure while enabling gradient-based
optimization. The resulting models are compact, interpretable, and require no
manual architecture tuning. Evaluated on a suite of structured multi-class
classification benchmarks, BranchNet consistently outperforms XGBoost in
accuracy, with statistically significant gains. We detail the architecture,
training procedure, and sparsity dynamics, and discuss the model's strengths in
symbolic interpretability as well as its current limitations, particularly on
binary tasks where further adaptive calibration may be beneficial.

</details>


### [210] [Towards Decentralized and Sustainable Foundation Model Training with the Edge](https://arxiv.org/abs/2507.01803)
*Leyang Xue,Meghana Madhyastha,Randal Burns,Myungjin Lee,Mahesh K. Marina*

Main category: cs.LG

TL;DR: 该论文提出一种利用闲置边缘AI设备集体算力，实现去中心化、可持续的基础模型训练的愿景，以应对当前计算需求高、环境影响大和中心化控制的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型训练的巨大计算需求引发了环境影响问题，并带来了开发中中心化控制的风险。

Method: 提出一种去中心化且可持续的基础模型训练愿景，该愿景利用了大量闲置的联网边缘AI设备的集体计算能力。

Result: 阐述了该愿景背后的基本原理，特别是其在可持续性方面的优势，并概述了实现此愿景所需解决的一系列挑战。

Conclusion: 该研究展望了一种通过分散式利用现有计算资源，实现更具环境效益和去中心化的基础模型训练路径，并明确了未来需要攻克的难点。

Abstract: Foundation models are at the forefront of AI research, appealing for their
ability to learn from vast datasets and cater to diverse tasks. Yet, their
significant computational demands raise issues of environmental impact and the
risk of centralized control in their development. We put forward a vision
towards decentralized and sustainable foundation model training that leverages
the collective compute of sparingly used connected edge AI devices. We present
the rationale behind our vision, particularly in support of its sustainability
benefit. We further outline a set of challenges that need to be addressed to
turn this vision into reality.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [211] [A Full-Stack Platform Architecture for Self-Organised Social Coordination](https://arxiv.org/abs/2507.01239)
*Matthew Scott,Jeremy Pitt*

Main category: cs.NI

TL;DR: 本文提出并开发了一个开源、全栈的平台架构，旨在通过赋能社区自组织来应对平台化带来的中心化和垄断问题。


<details>
  <summary>Details</summary>
Motivation: 旨在缓解平台化的中心化和垄断趋势，通过民主化平台来赋能本地社区，支持自组织社会协作。

Method: 开发了一个开源、全栈的平台开发架构，该架构支持易于分发、克隆、生成性和多种托管选项。它由一个元平台、基础平台、通用功能库和第三方插件组成，并辅以开发者和用户导向的工具链，以支持平台在两阶段中的实例化和定制。

Result: 通过两个概念验证案例（体育协会平台和集体学习小组平台）的实施，证明了该架构的可行性。

Conclusion: 应用层的自组织可以通过所提出的全栈架构及其配套的开发者和用户工具链来实现。

Abstract: To mitigate the restrictive centralising and monopolistic tendencies of
platformisation, we aim to empower local communities by democratising platforms
for self-organised social coordination. Our approach is to develop an
open-source, full-stack architecture for platform development that supports
ease of distribution and cloning, generativity, and a variety of hosting
options. The architecture consists of a meta-platform that is used to
instantiate a base platform with supporting libraries for generic functions,
and plugins (intended to be supplied by third parties) for customisation of
application-specification functionality for self-organised social coordination.
Associated developer- and user-oriented toolchains support the instantiation
and customisation of a platform in a two-stage process. This is demonstrated
through the proof-of-concept implementation of two case studies: a platform for
regular sporting association, and a platform for collective group study. We
conclude by arguing that self-organisation at the application layer can be
achieved by the specific supporting functionality of a full-stack architecture
with complimentary developer and user toolchains.

</details>


### [212] [Fluid Aerial Networks: UAV Rotation for Inter-Cell Interference Mitigation](https://arxiv.org/abs/2507.01289)
*Enzhi Zhou,Yue Xiao,Ziyue Liu,Sotiris A. Tegos,Panagiotis D. Diamantoulakis,George K. Karagiannidis*

Main category: cs.NI

TL;DR: 利用无人机（UAV）空中基站（ABS）的旋转，能有效降低多小区干扰并提升系统总速率。


<details>
  <summary>Details</summary>
Motivation: 针对UAV辅助蜂窝网络中ABS提供服务时的小区间干扰问题，旨在通过创新方法提高多小区容量和网络效率，并增强紧急通信能力。

Method: 本文研究了ABS与地面用户之间的基于位置的波束赋形。提出了一种新型流体空中网络，通过利用ABS旋转来减轻小区间干扰。在视距信道模型下，根据地面用户的方向角确定空间波束赋形权重，并分析了二维MIMO阵列的波束赋形增益。基于分析结果，提出了一种低复杂度算法来设计ABS的最佳旋转角度，以降低干扰并最大化多小区总速率。

Result: 仿真结果表明，所提出的序列ABS旋转方案具有良好的优化性能。在干扰受限区域，该ABS旋转范式能显著减少地面网络中的小区间干扰，并与无旋转的固定方向ABS相比，将多小区总速率提高了约10%。

Conclusion: ABS旋转是一种在UAV辅助网络中有效降低小区间干扰，并显著提升多小区总速率的可行且高效的方法。

Abstract: With the rapid development of aerial infrastructure, unmanned aerial vehicles
(UAVs) that function as aerial base stations (ABSs) extend terrestrial network
services into the sky, enabling on-demand connectivity and enhancing emergency
communication capabilities in cellular networks by leveraging the flexibility
and mobility of UAVs. In such a UAV-assisted network, this paper investigates
position-based beamforming between ABSs and ground users (GUs). To mitigate
inter-cell interference, we propose a novel fluid aerial network that leverages
ABS rotation to increase multi-cell capacity and overall network efficiency.
Specifically, considering the line-of-sight channel model, the spatial
beamforming weights are determined by the orientation angles of the GUs. In
this direction, we examine the beamforming gain of a two-dimensional
multiple-input multiple-output (MIMO) array at various ground positions,
revealing that ABS rotation significantly affects multi-user channel
correlation and inter-cell interference. Based on these findings, we propose an
alternative low-complexity algorithm to design the optimal rotation angle for
ABSs, aiming to reduce inter-cell interference and thus maximize the sum rate
of multi-cell systems. In simulations, exhaustive search serves as a benchmark
to validate the optimization performance of the proposed sequential ABS
rotation scheme. Moreover, simulation results demonstrate that, in
interference-limited regions, the proposed ABS rotation paradigm can
significantly reduce inter-cell interference in terrestrial networks and
improve the multi-cell sum rate by approximately 10\% compared to
fixed-direction ABSs without rotation.

</details>


### [213] [Multi-User Generative Semantic Communication with Intent-Aware Semantic-Splitting Multiple Access](https://arxiv.org/abs/2507.01333)
*Jiayi Lu,Wanting Yang,Zehui Xiong,Rahim Tafazolli,Tony Q. S. Quek,Mérouane Debbah,Dong In Kim*

Main category: cs.NI

TL;DR: 本文针对多用户下行语义通信系统（以车载网络为例），提出了一种多用户生成式语义通信增强型意图感知语义分割多址接入（SS-MGSC）框架。该框架通过构建意图感知共享知识库、区分并传输公共与私有语义信息，并在接收端利用扩散模型生成个性化图像。通过设计语义效率分数（SES）并采用强化学习解决联合优化问题，仿真结果验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能（GAI）的蓬勃发展，语义通信（SemCom）作为一种可靠高效的新范式应运而生。为了解决多用户场景中（如车载网络）多样化且重叠的用户需求，提升通信效率和可靠性。

Method: 1. 提出多用户生成式语义通信增强型意图感知语义分割多址接入（SS-MGSC）框架。
2. 构建意图感知共享知识库（SKB），融合语义信息先验知识和用户特定偏好。
3. 将公共语义信息（SI）作为独热语义图广播给所有用户，私有SI作为个性化文本单独传输。
4. 接收端采用ControlNet增强的扩散模型生成高质量个性化图像。
5. 设计新颖的语义效率分数（SES）作为优化目标，以兼顾语义相关性和感知相似性。
6. 建立多用户语义提取和波束成形的联合优化问题，并使用基于强化学习的算法进行求解。

Result: 仿真结果表明，所提出的方案是有效的。

Conclusion: 该研究提出的SS-MGSC框架，结合了意图感知共享知识库、语义信息分割传输、生成式AI模型以及强化学习优化，能够有效支持多用户下行语义通信系统中的个性化内容分发，提升了通信的语义效率和感知质量。

Abstract: With the booming development of generative artificial intelligence (GAI),
semantic communication (SemCom) has emerged as a new paradigm for reliable and
efficient communication. This paper considers a multi-user downlink SemCom
system, using vehicular networks as the representative scenario for multi-user
content dissemination. To address diverse yet overlapping user demands, we
propose a multi-user Generative SemCom-enhanced intent-aware semantic-splitting
multiple access (SS-MGSC) framework. In the framework, we construct an
intent-aware shared knowledge base (SKB) that incorporates prior knowledge of
semantic information (SI) and user-specific preferences. Then, we designate the
common SI as a one-hot semantic map that is broadcast to all users, while the
private SI is delivered as personalized text for each user. On the receiver
side, a diffusion model enhanced with ControlNet is adopted to generate
high-quality personalized images. To capture both semantic relevance and
perceptual similarity, we design a novel semantic efficiency score (SES) metric
as the optimization objective. Building on this, we formulate a joint
optimization problem for multi-user semantic extraction and beamforming, solved
using a reinforcement learning-based algorithm due to its robustness in
high-dimensional settings. Simulation results demonstrate the effectiveness of
the proposed scheme.

</details>


### [214] [MmBack: Clock-free Multi-Sensor Backscatter with Synchronous Acquisition and Multiplexing](https://arxiv.org/abs/2507.01360)
*Yijie Li,Weichong Ling,Taiting Lu,Yi-Chao Chen,Vaishnavi Ranganathan,Lili Qiu,Jingxian Wang*

Main category: cs.NI

TL;DR: mmBack是一种低功耗、无时钟的反向散射标签，通过共享参考信号和电压分压方案，实现了通过单个调制链同步采集和复用多个传感器数据。


<details>
  <summary>Details</summary>
Motivation: 现有反向散射标签通常只支持单个传感器，导致空间开销大。目前的多传感器复用方法依赖板载时钟或多调制链，这增加了成本、尺寸，并容易产生时序漂移，影响传感器间的同步性。

Method: mmBack通过从环境射频激励中提取共享参考信号，实现传感器输入的并行同步，从而无需板载时钟源。为有效复用数据，mmBack设计了一种电压分压方案，通过单个振荡器和射频开关将多个传感器输入复用为反向散射频移。在接收端，mmBack开发了频率跟踪算法和有限状态机以实现精确解复用。

Result: mmBack的ASIC设计功耗为25.56uW。其原型支持5路并发传感器流，带宽高达5kHz；或3路并发传感器流，带宽高达18kHz。信号重建的平均信噪比（SNR）超过15dB。

Conclusion: mmBack成功解决了现有反向散射标签多传感器应用中的挑战，提供了一种低功耗、无时钟、高效同步的多传感器数据采集和复用方案，具有良好的性能和实用性。

Abstract: Backscatter tags provide a low-power solution for sensor applications, yet
many real-world scenarios require multiple sensors-often of different types-for
complex sensing tasks. However, existing designs support only a single sensor
per tag, increasing spatial overhead. State-of-the-art approaches to
multiplexing multiple sensor streams on a single tag rely on onboard clocks or
multiple modulation chains, which add cost, enlarge form factor, and remain
prone to timing drift-disrupting synchronization across sensors.
  We present mmBack, a low-power, clock-free backscatter tag that enables
synchronous multi-sensor data acquisition and multiplexing over a single
modulation chain. mmBack synchronizes sensor inputs in parallel using a shared
reference signal extracted from ambient RF excitation, eliminating the need for
an onboard timing source. To efficiently multiplex sensor data, mmBack designs
a voltage-division scheme to multiplex multiple sensor inputs as backscatter
frequency shifts through a single oscillator and RF switch. At the receiver,
mmBack develops a frequency tracking algorithm and a finite-state machine for
accurate demultiplexing. mmBack's ASIC design consumes 25.56uW, while its
prototype supports 5 concurrent sensor streams with bandwidths of up to 5kHz
and 3 concurrent sensor streams with bandwidth of up to 18kHz. Evaluation shows
that mmBack achieves an average SNR surpassing 15dB in signal reconstruction.

</details>


### [215] [Frontiers of Generative AI for Network Optimization: Theories, Limits, and Visions](https://arxiv.org/abs/2507.01773)
*Bo Yang,Ruihuai Liang,Weixin Li,Han Wang,Xuelin Cao,Zhiwen Yu,Samson Lasaulce,Mérouane Debbah,Mohamed-Slim Alouini,H. Vincent Poor,Chau Yuen*

Main category: cs.NI

TL;DR: 本综述全面审查并批判性分析了生成式AI（尤其是生成扩散模型和大型预训练模型）在网络优化中的应用，探讨了其优势、局限性（如难以满足约束和理解概念），并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式AI在网络优化中的应用日益受到关注，但其固有的关键局限性在现有文献中尚未得到充分探讨。本研究旨在弥补这一空白，提供全面审查和批判性分析。

Method: 本文对生成式AI在网络优化中的应用进行了全面综述和批判性分析。研究重点关注生成扩散模型（GDMs）和大型预训练模型（LPTMs），并根据一次性优化和马尔可夫决策过程（MDP）两种主要问题范式进行分类讨论。文章追溯了相关基础工作，回顾了前沿应用，并提出了GDMs在一次性优化和MDP设置下的理论泛化界限。

Result: 研究结果包括对网络优化问题的分类（一次性优化和MDP），揭示了影响模型性能的基本因素。此外，文章强调了生成式AI的关键局限性，如难以满足约束、概念理解有限以及输出的固有概率性质，并警示了对生成式AI通用能力可能存在的过高估计和“一体化”的错觉。

Conclusion: 本综述旨在为生成式AI在网络优化中的应用提供一个结构化的概述，并深入洞察其优势、局限性和潜力。最终目标是弥合生成与优化之间的鸿沟，促进对两者理论连接的更深理解，为未来研究指明方向。

Abstract: While interest in the application of generative AI (GenAI) in network
optimization has surged in recent years, its rapid progress has often
overshadowed critical limitations intrinsic to generative models that remain
insufficiently examined in existing literature. This survey provides a
comprehensive review and critical analysis of GenAI in network optimization. We
focus on the two dominant paradigms of GenAI including generative diffusion
models (GDMs) and large pre-trained models (LPTMs), and organize our discussion
around a categorization we introduce, dividing network optimization problems
into two primary formulations: one-shot optimization and Markov decision
process (MDP). We first trace key works, including foundational contributions
from the AI community, and categorize current efforts in network optimization.
We also review frontier applications of GDMs and LPTMs in other networking
tasks, providing additional context. Furthermore, we present theoretical
generalization bounds for GDMs in both one-shot and MDP settings, offering
insights into the fundamental factors affecting model performance. Most
importantly, we reflect on the overestimated perception of GenAI's general
capabilities and caution against the all-in-one illusion it may convey. We
highlight critical limitations, including difficulties in constraint
satisfying, limited concept understanding, and the inherent probabilistic
nature of outputs. We also propose key future directions, such as bridging the
gap between generation and optimization. Although they are increasingly
integrated in implementations, they differ fundamentally in both objectives and
underlying mechanisms, necessitating a deeper understanding of their
theoretical connections. Ultimately, this survey aims to provide a structured
overview and a deeper insight into the strengths, limitations, and potential of
GenAI in network optimization.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [216] [AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma](https://arxiv.org/abs/2507.01081)
*Megan T. deBettencourt,Sruthi Sakthivel,Emily A. Holmes,Mark Chevillet*

Main category: cs.HC

TL;DR: 该研究测试了ANTIDOTE系统，一个结合AI指导和瞳孔测量的数字干预工具，旨在减少创伤后的侵入性记忆。结果显示，该系统能有效减少记忆，并具良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 全球创伤普遍，基于证据的数字治疗能提供帮助，但大多数需要人工指导，这限制了其可扩展性。研究旨在探索生成式AI和神经技术（如瞳孔测量）是否能提供可扩展的替代方案。

Method: 开发了ANTIDOTE系统，结合AI指导和瞳孔测量，用于自动提供和监测“想象竞争任务干预”（ICTI）。100名健康志愿者被暴露于创伤事件视频后，随机分配至干预组或主动对照组，并在接下来的一周内监测侵入性记忆。

Result: 干预组参与者在接下来的一周内报告的侵入性记忆显著减少。事后评估证实AI指导成功地实施了干预。此外，瞳孔大小可追踪干预投入度并预测症状减轻，有望成为干预有效性的生物标志物。

Conclusion: 这些发现为严谨的AI引导数字干预提供了新途径，有望实现大规模应用以应对全球创伤流行问题。

Abstract: Trauma prevalence is vast globally. Evidence-based digital treatments can
help, but most require human guidance. Human guides provide tailored
instructions and responsiveness to internal cognitive states, but limit
scalability. Can generative AI and neurotechnology provide a scalable
alternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to
automatically deliver and monitor an evidence-based digital treatment,
specifically the Imagery Competing Task Intervention (ICTI), to reduce
intrusive memories after psychological trauma. One hundred healthy volunteers
were exposed to videos of traumatic events and randomly assigned to an
intervention or active control condition. As predicted, intervention
participants reported significantly fewer intrusive memories over the following
week. Post-hoc assessment against clinical rubrics confirmed the AI guide
delivered the intervention successfully. Additionally, pupil size tracked
intervention engagement and predicted symptom reduction, providing a candidate
biomarker of intervention effectiveness. These findings open a path toward
rigorous AI-guided digital interventions that can scale to trauma prevalence.

</details>


### [217] [Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants](https://arxiv.org/abs/2507.01548)
*Wen Zhan,Ziqun Hua,Peiyue Lin,Yunfei Chen*

Main category: cs.HC

TL;DR: 本文探讨了人工智能辅助的共同创作如何帮助中国城市老年移民表达其碎片化的个人叙事。通过口述故事和汉字重构的试点工作坊，参与者在LLM和物理材料的辅助下，无需数字素养即可将生活经历转化为视觉和触觉表达，重新定义了人机协作中AI的角色。


<details>
  <summary>Details</summary>
Motivation: 老年人，特别是中国城市的移民老年人，其个人叙事往往是碎片化、代表性不足或难以口头表达的。本研究旨在探索如何通过人工智能辅助的共同创作来帮助他们表达这些叙事。

Method: 通过一个结合口述故事和汉字（特别是小篆字形）象征性重构的试点工作坊，参与者分享了移民记忆。在大型语言模型（LLM）的小篆字形建议和物理材料的帮助下，他们重新创作了新的汉字形式。整个过程有人工协助和温和的AI参与支持。

Result: 参与者无需数字素养即可将他们的生活经历转化为视觉和触觉表达。

Conclusion: 该方法为人工智能与人类协作以及老龄化问题提供了新视角，将人工智能重新定位为一种支持机制而非内容生产者，并支持社会技术系统中的叙事能动性。

Abstract: This paper explores how older adults, particularly aging migrants in urban
China, can engage AI-assisted co-creation to express personal narratives that
are often fragmented, underrepresented, or difficult to verbalize. Through a
pilot workshop combining oral storytelling and the symbolic reconstruction of
Hanzi, participants shared memories of migration and recreated new character
forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),
together with physical materials. Supported by human facilitation and a soft AI
presence, participants transformed lived experience into visual and tactile
expressions without requiring digital literacy. This approach offers new
perspectives on human-AI collaboration and aging by repositioning AI not as a
content producer but as a supportive mechanism, and by supporting narrative
agency within sociotechnical systems.

</details>


### [218] [AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance](https://arxiv.org/abs/2507.01274)
*Vishakha Lall,Yisi Liu*

Main category: cs.HC

TL;DR: 本研究开发了一个AI驱动的框架，通过客观评估视觉焦点、语音识别和压力水平，以提升海事专业人员的模拟训练效果，解决传统评估的主观性问题。


<details>
  <summary>Details</summary>
Motivation: 传统的船员模拟训练评估存在主观性强、关键特征难以量化及认知限制等挑战，导致难以客观衡量技术、行为和沟通能力。

Method: 开发了一个AI驱动的评估框架，整合了多种AI技术：通过眼动追踪、瞳孔扩张分析和计算机视觉进行视觉焦点判断；利用海事专用语音转文本模型和自然语言处理进行沟通分析；使用大型语言模型评估沟通正确性；通过声音音高检测心理压力。

Result: AI算法取得了高准确性，其中视觉检测约92%，海事语音识别约91%，压力检测约90%，均超越现有基准。该系统能提供关于视觉注意力、沟通依从性和压力水平的洞察。

Conclusion: 该研究展示了AI如何通过提供客观的绩效分析、实现个性化反馈和提升应对实际操作挑战的准备度，从而革新海事培训。

Abstract: Traditional simulator-based training for maritime professionals is critical
for ensuring safety at sea but often depends on subjective trainer assessments
of technical skills, behavioral focus, communication, and body language, posing
challenges such as subjectivity, difficulty in measuring key features, and
cognitive limitations. Addressing these issues, this study develops an
AI-driven framework to enhance maritime training by objectively assessing
trainee performance through visual focus tracking, speech recognition, and
stress detection, improving readiness for high-risk scenarios. The system
integrates AI techniques, including visual focus determination using eye
tracking, pupil dilation analysis, and computer vision; communication analysis
through a maritime-specific speech-to-text model and natural language
processing; communication correctness using large language models; and mental
stress detection via vocal pitch. Models were evaluated on data from simulated
maritime scenarios with seafarers exposed to controlled high-stress events. The
AI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for
maritime speech recognition, and ~90% for stress detection, surpassing existing
benchmarks. The system provides insights into visual attention, adherence to
communication checklists, and stress levels under demanding conditions. This
study demonstrates how AI can transform maritime training by delivering
objective performance analytics, enabling personalized feedback, and improving
preparedness for real-world operational challenges.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [219] [On the Effect of Ruleset Tuning and Data Imbalance on Explainable Network Security Alert Classifications: a Case-Study on DeepCASE](https://arxiv.org/abs/2507.01571)
*Koen T. W. Teuwen,Sam Baggen,Emmanuele Zambon,Luca Allodi*

Main category: cs.CR

TL;DR: 本研究评估了安全运营中心 (SOC) 中网络入侵警报分类的自动化方法在数据不平衡下的表现。结果显示，数据不平衡会影响最先进的 DeepCASE 方法的分类性能和可解释性，并建议通过调整检测规则来改善。


<details>
  <summary>Details</summary>
Motivation: 自动化在安全运营中心 (SOC) 的警报分类和事件升级中扮演重要角色，但自动化方法必须对不平衡的输入数据具有鲁棒性，并且其决策应具有可解释性。

Method: 评估标签不平衡对网络入侵警报分类的影响。研究案例采用了 DeepCASE，这是一种最先进的自动化警报分类方法。

Result: 研究发现，标签不平衡会影响 DeepCASE 的分类性能及其提供的分类解释的正确性。

Conclusion: 调整 SOC 中使用的检测规则可以显著减少数据不平衡，从而提升警报后处理方法（如 DeepCASE）的性能和可解释性。因此，传统的输入数据质量改进方法有利于自动化。

Abstract: Automation in Security Operations Centers (SOCs) plays a prominent role in
alert classification and incident escalation. However, automated methods must
be robust in the presence of imbalanced input data, which can negatively affect
performance. Additionally, automated methods should make explainable decisions.
In this work, we evaluate the effect of label imbalance on the classification
of network intrusion alerts. As our use-case we employ DeepCASE, the
state-of-the-art method for automated alert classification. We show that label
imbalance impacts both classification performance and correctness of the
classification explanations offered by DeepCASE. We conclude tuning the
detection rules used in SOCs can significantly reduce imbalance and may benefit
the performance and explainability offered by alert post-processing methods
such as DeepCASE. Therefore, our findings suggest that traditional methods to
improve the quality of input data can benefit automation.

</details>


### [220] [A Systematic Review of Security Vulnerabilities in Smart Home Devices and Mitigation Techniques](https://arxiv.org/abs/2507.01018)
*Mohammed K. Alzaylaee*

Main category: cs.CR

TL;DR: 该研究分析了智能家居的物联网安全威胁，并评估了后量子加密、AI异常检测、区块链认证和零信任等安全策略的有效性、资源需求及扩展性。


<details>
  <summary>Details</summary>
Motivation: 智能家居集成物联网设备面临日益增长的网络安全风险，对这些环境构成重大挑战。

Method: 研究探索并分类智能家居生态系统中的安全威胁（包括网络层、设备层、云和AI驱动系统），并通过ANOVA、卡方检验和蒙特卡洛模拟评估特定安全策略的有效性。

Result: 研究发现：1) 后量子加密结合AI驱动的异常检测能有效增强安全性，但计算资源需求高。2) 区块链认证与零信任结构能提升安全弹性，但需改造现有基础设施。3) 特定安全策略的有效性已通过统计测试和模拟验证，但扩展性不足。

Conclusion: 智能家居需改进加密技术、AI增强的威胁检测和自适应安全模型，以在性能、效率和实时适用性之间取得平衡。

Abstract: Smart homes that integrate Internet of Things (IoT) devices face increasing
cybersecurity risks, posing significant challenges to these environments. The
study explores security threats in smart homes ecosystems, categorizing them
into vulnerabilities at the network layer, device level, and those from
cloud-based and AI-driven systems. Research findings indicate that post-quantum
encryption, coupled with AI-driven anomaly detection, is highly effective in
enhancing security; however, computational resource demands present significant
challenges. Blockchain authentication together with zero-trust structures
builds security resilience, although they need changes to existing
infrastructure. The specific security strategies show their effectiveness
through ANOVA, Chi-square tests, and Monte Carlo simulations yet lack
sufficient scalability according to the results. The research demonstrates the
requirement for improvement in cryptographic techniques, alongside AI-enhanced
threat detection and adaptive security models which must achieve a balance
between performance and efficiency and real-time applicability within smart
home ecosystems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [221] [Capacity Planning and Scheduling for Jobs with Uncertainty in Resource Usage and Duration](https://arxiv.org/abs/2507.01225)
*Sunandita Patra,Mehtab Pathan,Mahmoud Mahfouz,Parisa Zehtabi,Wided Ouaja,Daniele Magazzeni,Manuela Veloso*

Main category: cs.DC

TL;DR: 本研究针对混合云环境中本地网格计算的容量规划和作业调度问题，提出了一种处理不确定性并平衡资源使用与服务质量的方法。


<details>
  <summary>Details</summary>
Motivation: 随着组织转向混合云架构，需要对本地网格计算环境进行容量规划和作业调度。主要挑战在于处理资源使用和作业持续时间的不确定性（尤其在金融业），以及在最小化资源使用和提供高质量服务（满足截止日期）之间进行平衡。

Method: 研究提出了两种近似方法：使用确定性估计器和基于对采样（pair sampling-based）的约束编程。

Result: 实验结果显示，最佳方法（基于对采样的约束编程）与手动调度相比，在不损害服务质量的前提下，显著降低了峰值资源使用。

Conclusion: 该研究成功解决了本地网格计算环境中在不确定性下进行容量规划和作业调度的复杂问题，并提供了一种有效平衡资源效率和服务质量的方案。

Abstract: Organizations around the world schedule jobs (programs) regularly to perform
various tasks dictated by their end users. With the major movement towards
using a cloud computing infrastructure, our organization follows a hybrid
approach with both cloud and on-prem servers. The objective of this work is to
perform capacity planning, i.e., estimate resource requirements, and job
scheduling for on-prem grid computing environments. A key contribution of our
approach is handling uncertainty in both resource usage and duration of the
jobs, a critical aspect in the finance industry where stochastic market
conditions significantly influence job characteristics. For capacity planning
and scheduling, we simultaneously balance two conflicting objectives: (a)
minimize resource usage, and (b) provide high quality-of-service to the end
users by completing jobs by their requested deadlines. We propose approximate
approaches using deterministic estimators and pair sampling-based constraint
programming. Our best approach (pair sampling-based) achieves much lower peak
resource usage compared to manual scheduling without compromising on the
quality-of-service.

</details>


### [222] [EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices](https://arxiv.org/abs/2507.01438)
*Zheyu Shen,Yexiao He,Ziyao Wang,Yuning Zhang,Guoheng Sun,Wanghao Ye,Ang Li*

Main category: cs.DC

TL;DR: EdgeLoRA是一个在多租户边缘设备上高效服务LLM的系统，通过自适应适配器选择、异构内存管理和批量LoRA推理，显著提升了吞吐量并降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的多租户边缘设备上部署LLM面临挑战，包括适配器选择复杂性、频繁适配器交换带来的内存开销，以及顺序处理请求导致的资源利用率低下和高延迟。

Method: 本文提出了EdgeLoRA系统，其包含三项创新：1) 自适应适配器选择机制；2) 异构内存管理（智能适配器缓存和池化）；3) 批量LoRA推理。

Result: 使用Llama3.1-8B模型的综合评估表明，EdgeLoRA在延迟和吞吐量方面显著优于现有方案（llama.cpp），吞吐量最高可提升4倍，并能同时服务数量级更多的适配器。

Conclusion: EdgeLoRA为多租户、资源受限环境中LLM的边缘部署提供了一个可扩展且高效的解决方案，有望改变LLM在这些环境中的部署方式。

Abstract: Large Language Models (LLMs) have gained significant attention due to their
versatility across a wide array of applications. Fine-tuning LLMs with
parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these
models to efficiently adapt to downstream tasks without extensive retraining.
Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial
benefits, such as reduced latency, enhanced privacy, and personalized
responses. However, serving LLMs efficiently on resource-constrained edge
devices presents critical challenges, including the complexity of adapter
selection for different tasks and memory overhead from frequent adapter
swapping. Moreover, given the multiple requests in multi-tenant settings,
processing requests sequentially results in underutilization of computational
resources and increased latency. This paper introduces EdgeLoRA, an efficient
system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA
incorporates three key innovations: (1) an adaptive adapter selection mechanism
to streamline the adapter configuration process; (2) heterogeneous memory
management, leveraging intelligent adapter caching and pooling to mitigate
memory operation overhead; and (3) batch LoRA inference, enabling efficient
batch processing to significantly reduce computational latency. Comprehensive
evaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly
outperforms the status quo (i.e., llama.cpp) in terms of both latency and
throughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times
boost in throughput. Even more impressively, it can serve several orders of
magnitude more adapters simultaneously. These results highlight EdgeLoRA's
potential to transform edge deployment of LLMs in multi-tenant scenarios,
offering a scalable and efficient solution for resource-constrained
environments.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [223] [Automated Vehicles Should be Connected with Natural Language](https://arxiv.org/abs/2507.01059)
*Xiangbo Gao,Keshu Wu,Hao Zhang,Kexin Tian,Yang Zhou,Zhengzhong Tu*

Main category: cs.MA

TL;DR: 针对多智能体协同驾驶中现有通信方式的局限性，本文提出利用自然语言进行意图与推理沟通，以实现更主动的协作驾驶，提升交通安全与效率。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体协同驾驶的通信方式（如原始传感器数据、神经网络特征、感知结果）存在带宽效率低、信息不完整、代理互操作性差等问题。此外，传统方法忽视了决策层面的融合，未能充分利用协同驾驶的关键维度。

Method: 本文主张将协同驾驶的通信方式从纯粹面向感知的现有数据交换，转变为使用自然语言进行明确的意图和推理通信。

Result: 自然语言能够平衡语义密度与通信带宽，灵活适应实时条件，并连接异构代理平台。通过直接沟通意图、理由和决策，它将协同驾驶从被动感知数据共享转变为主动协调。

Conclusion: 采用自然语言通信能有效解决协同驾驶中的挑战，通过促进主动协调，显著提高智能交通系统的安全性、效率和透明度。

Abstract: Multi-agent collaborative driving promises improvements in traffic safety and
efficiency through collective perception and decision making. However, existing
communication media -- including raw sensor data, neural network features, and
perception results -- suffer limitations in bandwidth efficiency, information
completeness, and agent interoperability. Moreover, traditional approaches have
largely ignored decision-level fusion, neglecting critical dimensions of
collaborative driving. In this paper we argue that addressing these challenges
requires a transition from purely perception-oriented data exchanges to
explicit intent and reasoning communication using natural language. Natural
language balances semantic density and communication bandwidth, adapts flexibly
to real-time conditions, and bridges heterogeneous agent platforms. By enabling
the direct communication of intentions, rationales, and decisions, it
transforms collaborative driving from reactive perception-data sharing into
proactive coordination, advancing safety, efficiency, and transparency in
intelligent transportation systems.

</details>


### [224] [RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms](https://arxiv.org/abs/2507.01378)
*Ziyao Wang,Rongpeng Li,Sizhao Li,Yuming Xiang,Haiping Wang,Zhifeng Zhao,Honggang Zhang*

Main category: cs.MA

TL;DR: 本文提出RALLY算法，结合大型语言模型（LLM）的语义推理能力和多智能体强化学习（MARL）的在线学习机制，解决无人机蜂群协同导航中传统方法的局限性，并在任务覆盖、收敛速度和泛化能力上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习（MARL）方法受限于数值通信中的语义鸿沟和同质角色结构的僵化，导致泛化性和任务可扩展性差。尽管基于大型语言模型（LLM）的控制框架具有强大的语义推理能力，但缺乏在线学习和过度依赖静态先验，导致探索效率低和整体系统性能下降。

Method: 本文提出RALLY（Role-Adaptive LLM-Driven Yoked navigation）导航算法。具体地，首先开发了一个基于LLM的语义决策框架，利用结构化自然语言进行高效语义通信和协同推理。其次，引入动态角色异构机制，实现自适应角色切换和个性化决策。此外，提出了一种基于RMIX（Role-value Mixing Network）的分配策略，将LLM的离线先验与MARL的在线策略相结合，实现角色选择的半离线训练。

Result: 在Multi-Agent Particle Environment（MPE）环境和软件在环（SITL）平台上的实验表明，RALLY在任务覆盖、收敛速度和泛化能力方面均优于传统方法。

Conclusion: RALLY算法展示了在智能多无人机系统协同导航中的强大潜力。

Abstract: Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as
a critical research focus, and it typically requires the swarm to navigate
effectively while avoiding obstacles and achieving continuous coverage over
multiple mission targets. Although traditional Multi-Agent Reinforcement
Learning (MARL) approaches offer dynamic adaptability, they are hindered by the
semantic gap in numerical communication and the rigidity of homogeneous role
structures, resulting in poor generalization and limited task scalability.
Recent advances in Large Language Model (LLM)-based control frameworks
demonstrate strong semantic reasoning capabilities by leveraging extensive
prior knowledge. However, due to the lack of online learning and over-reliance
on static priors, these works often struggle with effective exploration,
leading to reduced individual potential and overall system performance. To
address these limitations, we propose a Role-Adaptive LLM-Driven Yoked
navigation algorithm RALLY. Specifically, we first develop an LLM-driven
semantic decision framework that uses structured natural language for efficient
semantic communication and collaborative reasoning. Afterward, we introduce a
dynamic role-heterogeneity mechanism for adaptive role switching and
personalized decision-making. Furthermore, we propose a Role-value Mixing
Network (RMIX)-based assignment strategy that integrates LLM offline priors
with MARL online policies to enable semi-offline training of role selection
strategies. Experiments in the Multi-Agent Particle Environment (MPE)
environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY
outperforms conventional approaches in terms of task coverage, convergence
speed, and generalization, highlighting its strong potential for collaborative
navigation in agentic multi-UAV systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [225] [Can Argus Judge Them All? Comparing VLMs Across Domains](https://arxiv.org/abs/2507.01042)
*Harsh Joshi,Gautam Siddharth Kashyap,Rafiq Ali,Ebad Shabbir,Niharika Jain,Sarthak Jain,Jiechao Gao,Usman Naseem*

Main category: cs.IR

TL;DR: 评估了CLIP、BLIP和LXMERT在不同视觉语言任务上的性能一致性，发现它们在泛化与专业化之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）虽在进步，但其跨任务的性能一致性未得到充分检验。

Method: 对CLIP、BLIP、LXMERT进行基准测试，涵盖检索、描述和推理任务的多种数据集，评估指标包括任务准确性、生成质量、效率及新颖的跨数据集一致性（CDC）。

Result: CLIP显示出最强泛化能力（CDC: 0.92），BLIP在精选数据上表现优异，LXMERT在结构化推理中领先。研究揭示了泛化与专业化之间的权衡。

Conclusion: 研究结果为VLMs的工业部署提供了指导，并指明了开发更鲁棒、任务灵活架构的方向。

Abstract: Vision-Language Models (VLMs) are advancing multimodal AI, yet their
performance consistency across tasks is underexamined. We benchmark CLIP, BLIP,
and LXMERT across diverse datasets spanning retrieval, captioning, and
reasoning. Our evaluation includes task accuracy, generation quality,
efficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows
strongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT
leads in structured reasoning. These results expose trade-offs between
generalization and specialization, informing industrial deployment of VLMs and
guiding development toward robust, task-flexible architectures.

</details>


### [226] [Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis](https://arxiv.org/abs/2507.01053)
*Rafi Al Attrach,Pedro Moreira,Rajna Fani,Renato Umeton,Leo Anthony Celi*

Main category: cs.IR

TL;DR: M3工具通过自然语言交互，极大地简化了对MIMIC-IV这类复杂临床大数据的查询和分析，降低了技术门槛。


<details>
  <summary>Details</summary>
Motivation: 大型临床数据集如MIMIC-IV虽具巨大潜力，但其固有的复杂性以及对专业查询技能和临床知识的要求，严重阻碍了研究人员的有效利用。

Method: M3通过单个命令获取MIMIC-IV数据，建立本地SQLite实例或连接至BigQuery，并通过模型上下文协议（MCP）实现自然语言与数据库的交互。它利用语言模型将自然语言问题转化为SQL，执行查询，并返回结构化结果及用于验证和复现的底层SQL。

Result: 实践证明，通过M3进行的几分钟对话就能完成过去需要数小时手动编写SQL并深入理解临床工作流才能实现的精细化队列分析。M3显著降低了数据访问的技术障碍。

Conclusion: M3通过简化数据访问，鼓励更广泛的研究社区利用临床重症监护数据，从而加速原始记录向可操作性洞察的转化。

Abstract: As ever-larger clinical datasets become available, they have the potential to
unlock unprecedented opportunities for medical research. Foremost among them is
Medical Information Mart for Intensive Care (MIMIC-IV), the world's largest
open-source EHR database. However, the inherent complexity of these datasets,
particularly the need for sophisticated querying skills and the need to
understand the underlying clinical settings, often presents a significant
barrier to their effective use. M3 lowers the technical barrier to
understanding and querying MIMIC-IV data. With a single command it retrieves
MIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the
hosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers
converse with the database in plain English. Ask a clinical question in natural
language; M3 uses a language model to translate it into SQL, executes the query
against the MIMIC-IV dataset, and returns structured results alongside the
underlying query for verifiability and reproducibility. Demonstrations show
that minutes of dialogue with M3 yield the kind of nuanced cohort analyses that
once demanded hours of handcrafted SQL and relied on understanding the
complexities of clinical workflows. By simplifying access, M3 invites the
broader research community to mine clinical critical-care data and accelerates
the translation of raw records into actionable insight.

</details>


### [227] [A Data Science Approach to Calcutta High Court Judgments: An Efficient LLM and RAG-powered Framework for Summarization and Similar Cases Retrieval](https://arxiv.org/abs/2507.01058)
*Puspendu Banerjee,Aritra Mazumdar,Wazib Ansar,Saptarsi Goswami,Amlan Chakrabarti*

Main category: cs.IR

TL;DR: 该研究提出一个基于LLM和RAG的数据科学框架，旨在提高加尔各答高等法院判决的分析效率，主要实现法律文本摘要和相似案件检索功能。


<details>
  <summary>Details</summary>
Motivation: 司法系统面临日益增多的法律问题，需要更有效地利用司法资源，从而提高法律判决分析的效率。

Method: 研究构建了一个复杂的框架，融合了数据科学方法，特别是大型语言模型（LLM）和检索增强生成（RAG）技术。该框架包含两个核心方面：一是开发了一个强大的摘要机制，通过微调Pegasus模型并使用判例摘要来凝练法律文本；二是开发了一个智能系统，利用两步摘要技术生成的综合向量数据库支持RAG，以检索相似案件。

Result: 通过微调Pegasus模型，法律案件的摘要质量得到了显著提升。基于RAG的框架能够高效地响应用户查询，检索出相似案件，并提供全面的概述和摘要。

Conclusion: 该技术不仅提升了法律研究效率，还帮助法律专业人士和学生更轻松地获取和理解关键法律信息，对整体法律环境产生积极影响。

Abstract: The judiciary, as one of democracy's three pillars, is dealing with a rising
amount of legal issues, needing careful use of judicial resources. This
research presents a complex framework that leverages Data Science
methodologies, notably Large Language Models (LLM) and Retrieval-Augmented
Generation (RAG) techniques, to improve the efficiency of analyzing Calcutta
High Court verdicts. Our framework focuses on two key aspects: first, the
creation of a robust summarization mechanism that distills complex legal texts
into concise and coherent summaries; and second, the development of an
intelligent system for retrieving similar cases, which will assist legal
professionals in research and decision making. By fine-tuning the Pegasus model
using case head note summaries, we achieve significant improvements in the
summarization of legal cases. Our two-step summarizing technique preserves
crucial legal contexts, allowing for the production of a comprehensive vector
database for RAG. The RAG-powered framework efficiently retrieves similar cases
in response to user queries, offering thorough overviews and summaries. This
technique not only improves legal research efficiency, but it also helps legal
professionals and students easily acquire and grasp key legal information,
benefiting the overall legal scenario.

</details>


### [228] [FAIR-MATCH: A Multi-Objective Framework for Bias Mitigation in Reciprocal Dating Recommendations](https://arxiv.org/abs/2507.01063)
*Madhav Kotecha*

Main category: cs.IR

TL;DR: 该研究分析了在线约会平台推荐系统的算法缺陷（如受欢迎度偏差、过滤气泡），并提出了一个数学框架，通过增强相似性、多目标优化和公平性感知算法，在提高准确性的同时减少算法偏见。


<details>
  <summary>Details</summary>
Motivation: 现有在线约会平台的推荐系统存在显著的算法缺陷，包括受欢迎度偏差、过滤气泡效应和互惠建模不足，这些问题限制了系统的有效性并引入了有害偏见。

Method: 本研究整合了基础理论与实证发现，对约会应用推荐系统进行了详细分析，探讨了互惠推荐框架、公平性评估指标及行业实现。在此基础上，提出了一个数学框架，包含增强的相似性度量、多目标优化和公平性感知算法。

Result: 研究发现现有系统的性能一般，协同过滤达到25.1%，互惠方法达到28.7%。提出的数学框架能够在保持竞争性准确度的同时，提高人口代表性并有效减少算法偏见。

Conclusion: 为解决现有约会平台推荐系统的局限性（如低效和偏见），本研究提出了一种新的数学框架，旨在通过优化算法提升推荐准确性并增强公平性，从而缓解算法偏见问题。

Abstract: Online dating platforms have fundamentally transformed the formation of
romantic relationships, with millions of users worldwide relying on algorithmic
matching systems to find compatible partners. However, current recommendation
systems in dating applications suffer from significant algorithmic
deficiencies, including but not limited to popularity bias, filter bubble
effects, and inadequate reciprocity modeling that limit effectiveness and
introduce harmful biases. This research integrates foundational work with
recent empirical findings to deliver a detailed analysis of dating app
recommendation systems, highlighting key issues and suggesting research-backed
solutions. Through analysis of reciprocal recommendation frameworks, fairness
evaluation metrics, and industry implementations, we demonstrate that current
systems achieve modest performance with collaborative filtering reaching 25.1\%
while reciprocal methods achieve 28.7\%. Our proposed mathematical framework
addresses these limitations through enhanced similarity measures,
multi-objective optimization, and fairness-aware algorithms that maintain
competitive accuracy while improving demographic representation to reduce
algorithmic bias.

</details>


### [229] [Cohort Retrieval using Dense Passage Retrieval](https://arxiv.org/abs/2507.01049)
*Pranav Jadhav*

Main category: cs.IR

TL;DR: 本研究首次将Dense Passage Retrieval（DPR）应用于超声心动图领域的患者队列检索，通过系统方法转换非结构化EHR数据并设计定制评估指标，成功开发出性能优于现有方法的自定义DPR模型。


<details>
  <summary>Details</summary>
Motivation: 患者队列检索是医学研究和临床实践中的关键任务，能从大量电子健康记录（EHRs）中识别特定患者群体。本研究旨在解决超声心动图领域的队列检索挑战。

Method: 采用Dense Passage Retrieval (DPR) 方法。提出一种系统方法，将非结构化的超声心动图EHR数据集转换为查询-段落（Query-Passage）数据集，将问题框定为队列检索任务。设计并实现了受真实临床场景启发的评估指标，用于严格测试模型。开发了自定义训练的DPR嵌入模型。

Result: 自定义训练的DPR嵌入模型表现出优于传统和现成SOTA（State-Of-The-Art）方法的性能。

Conclusion: 本工作首次将DPR应用于超声心动图领域的患者队列检索，建立了一个可适应其他医学领域的框架。

Abstract: Patient cohort retrieval is a pivotal task in medical research and clinical
practice, enabling the identification of specific patient groups from extensive
electronic health records (EHRs). In this work, we address the challenge of
cohort retrieval in the echocardiography domain by applying Dense Passage
Retrieval (DPR), a prominent methodology in semantic search. We propose a
systematic approach to transform an echocardiographic EHR dataset of
unstructured nature into a Query-Passage dataset, framing the problem as a
Cohort Retrieval task. Additionally, we design and implement evaluation metrics
inspired by real-world clinical scenarios to rigorously test the models across
diverse retrieval tasks. Furthermore, we present a custom-trained DPR embedding
model that demonstrates superior performance compared to traditional and
off-the-shelf SOTA methods.To our knowledge, this is the first work to apply
DPR for patient cohort retrieval in the echocardiography domain, establishing a
framework that can be adapted to other medical domains.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [230] [Prompt Mechanisms in Medical Imaging: A Comprehensive Survey](https://arxiv.org/abs/2507.01055)
*Hao Yang,Xinlong Liang,Zhang Li,Yue Sun,Zheyu Hu,Xinghe Xie,Behdad Dashtbozorg,Jincheng Huang,Shiwei Zhu,Luyi Han,Jiong Zhang,Shanshan Wang,Ritse Mann,Qifeng Yu,Tao Tan*

Main category: eess.IV

TL;DR: 本文系统综述了在医学影像领域中，提示工程（prompt engineering）如何通过多种提示模式（如文本、视觉、可学习嵌入）应用于图像生成、分割和分类等任务，以解决深度学习面临的数据稀缺、分布偏移和泛化能力不足等挑战。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像中潜力巨大，但其临床应用受限于数据稀缺、分布偏移和任务泛化能力不足。需要一种策略来增强模型性能和适应性。

Method: 本研究是一项系统综述，分析了医学影像中提示工程的现状。具体解剖了多种提示模式，包括文本指令、视觉提示和可学习嵌入，并分析它们在图像生成、分割和分类等核心任务中的整合与应用。

Result: 研究发现提示机制能够显著提高任务特定结果，具体表现为增强准确性、鲁棒性、数据效率，减少对手动特征工程的依赖，并通过明确模型指导来提高模型可解释性。

Conclusion: 尽管取得显著进展，但提示设计优化、数据异质性和临床部署的可扩展性仍是挑战。提示驱动的AI在加速医学诊断和个性化治疗计划的革命中扮演关键角色，未来发展方向包括先进多模态提示和稳健的临床整合。

Abstract: Deep learning offers transformative potential in medical imaging, yet its
clinical adoption is frequently hampered by challenges such as data scarcity,
distribution shifts, and the need for robust task generalization. Prompt-based
methodologies have emerged as a pivotal strategy to guide deep learning models,
providing flexible, domain-specific adaptations that significantly enhance
model performance and adaptability without extensive retraining. This
systematic review critically examines the burgeoning landscape of prompt
engineering in medical imaging. We dissect diverse prompt modalities, including
textual instructions, visual prompts, and learnable embeddings, and analyze
their integration for core tasks such as image generation, segmentation, and
classification. Our synthesis reveals how these mechanisms improve
task-specific outcomes by enhancing accuracy, robustness, and data efficiency
and reducing reliance on manual feature engineering while fostering greater
model interpretability by making the model's guidance explicit. Despite
substantial advancements, we identify persistent challenges, particularly in
prompt design optimization, data heterogeneity, and ensuring scalability for
clinical deployment. Finally, this review outlines promising future
trajectories, including advanced multimodal prompting and robust clinical
integration, underscoring the critical role of prompt-driven AI in accelerating
the revolution of diagnostics and personalized treatment planning in medicine.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [231] [Systemic Constraints of Undecidability](https://arxiv.org/abs/2507.01036)
*Seth Bulin*

Main category: cs.FL

TL;DR: 本文提出系统不可判定性理论，将不可计算性视为系统结构性属性，并证明不可判定性可在功能参与的子系统中传播，对预测和建模构成普遍限制。


<details>
  <summary>Details</summary>
Motivation: 将不可计算性重新定义为系统的结构性属性而非局部特征，并挑战通过架构创新规避计算限制的观点，进而扩展经典计算理论。

Method: 定义了“因果嵌入”的概念，并证明了“闭包原理”：任何功能上参与不可判定系统计算的子系统都会继承其不可判定性。将经典结果推广到动态系统语境中。

Result: 不可判定性是自然和人工系统中预测、建模和认知获取的普遍限制。该框架否定了预言机模仿的可能性，并挑战了计算限制可通过架构创新规避的观点。

Conclusion: 本工作通过将经典结果推广到动态系统语境，拓展了哥德尔、图灵和蔡汀的逻辑轨迹，为计算性拓扑及其与科学知识边界的关系提供了新视角。

Abstract: This paper presents a theory of systemic undecidability, reframing
incomputability as a structural property of systems rather than a localized
feature of specific functions or problems. We define a notion of causal
embedding and prove a closure principle: any subsystem that participates
functionally in the computation of an undecidable system inherits its
undecidability. This result positions undecidability as a pervasive constraint
on prediction, modeling, and epistemic access in both natural and artificial
systems. Our framework disarms oracle mimicry and challenges the view that
computational limits can be circumvented through architectural innovation. By
generalizing classical results into a dynamic systems context, this work
augments the logical trajectory of G\"odel, Turing, and Chaitin, offering a new
perspective of the topology of computability and its interrelation to the
boundaries of scientific knowledge.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [232] [Hello Afrika: Speech Commands in Kinyarwanda](https://arxiv.org/abs/2507.01024)
*George Igwegbe,Martins Awojide,Mboh Bless,Nirel Kadzo*

Main category: eess.AS

TL;DR: 非洲语言语音命令模型稀缺，Hello Afrika项目为基尼亚卢旺达语开发并部署了语音命令模型。


<details>
  <summary>Details</summary>
Motivation: 语音命令对设备非接触控制和AI系统激活至关重要，特别对残疾人有益。然而，目前非洲语言的语音命令模型极度匮乏。

Method: Hello Afrika项目针对基尼亚卢旺达语，基于包含通用指令、数字和唤醒词的自定义语音命令语料库构建了模型。最终模型被部署在PC、手机和边缘设备上，并使用适当的指标评估了其性能。

Result: 成功构建了基尼亚卢旺达语的语音命令模型，并将其部署到多种设备（PC、手机和边缘设备）上，其性能已通过适用指标进行评估。

Conclusion: Hello Afrika项目通过为基尼亚卢旺达语开发并部署语音命令模型，成功迈出了解决非洲语言语音技术空白的第一步。

Abstract: Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a
language which are essential for non-contact control of and activation of
larger AI systems in devices used in everyday life especially for persons with
disabilities. Currently, there is a dearth of speech command models for African
languages. The Hello Afrika project aims to address this issue and its first
iteration is focused on the Kinyarwanda language since the country has shown
interest in developing speech recognition technologies culminating in one of
the largest datasets on Mozilla Common Voice. The model was built off a custom
speech command corpus made up of general directives, numbers, and a wake word.
The final model was deployed on multiple devices (PC, Mobile Phone and Edge
Devices) and the performance was assessed using suitable metrics.

</details>


### [233] [Scalable Offline ASR for Command-Style Dictation in Courtrooms](https://arxiv.org/abs/2507.01021)
*Kumarmanas Nethil,Vaibhav Mishra,Kriti Anandan,Kavya Manohar*

Main category: eess.AS

TL;DR: 该论文提出了一个开源框架，通过结合语音活动检测（VAD）和并行转录来优化指令式听写，旨在解决高资源消耗的在线系统与高延迟的批处理之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 现有指令式听写系统存在弊端：在线系统资源消耗大，而批处理系统延迟高。研究动机是开发一个能够弥合这一差距的解决方案。

Method: 该方法利用语音活动检测（VAD）对音频进行分段，并使用Whisper模型并行转录这些片段，从而实现高效的跨音频多路复用。该框架是开源的，并兼容大多数ASR架构，包括基于CTC的模型。

Result: 研究结果显示，该多路复用技术在实际场景中最大化了计算资源利用率，并在印度约15%的法庭中得到了部署。对实时数据的评估表明，与顺序批处理相比，随着用户并发量的增加，延迟持续降低。

Conclusion: 该开源框架为指令式听写提供了一个高效且实用的解决方案，有效平衡了资源利用率和延迟。其广泛的兼容性、在实际环境中的部署经验以及显著的延迟降低证明了其在处理高并发场景下的优越性。

Abstract: We propose an open-source framework for Command-style dictation that
addresses the gap between resource-intensive Online systems and high-latency
Batch processing. Our approach uses Voice Activity Detection (VAD) to segment
audio and transcribes these segments in parallel using Whisper models, enabling
efficient multiplexing across audios. Unlike proprietary systems like
SuperWhisper, this framework is also compatible with most ASR architectures,
including widely used CTC-based models. Our multiplexing technique maximizes
compute utilization in real-world settings, as demonstrated by its deployment
in around 15% of India's courtrooms. Evaluations on live data show consistent
latency reduction as user concurrency increases, compared to sequential batch
processing. The live demonstration will showcase our open-sourced
implementation and allow attendees to interact with it in real-time.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [234] [Evaluating LLM Agent Collusion in Double Auctions](https://arxiv.org/abs/2507.01413)
*Kushal Agrawal,Verona Teo,Juan J. Vazquez,Sudarsh Kunnavakkam,Vishak Srikanth,Andy Liu*

Main category: cs.GT

TL;DR: 研究大型语言模型（LLMs）作为自主代理在模拟市场中串通的可能性及其影响因素，发现沟通、模型选择和环境压力都会影响串通行为，这对于LLM市场代理的部署具有经济和伦理意义。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）作为自主代理的应用日益广泛并参与社会经济互动，识别其潜在的不良行为（如串通）变得至关重要。

Method: 将LLM代理设置为模拟连续双重拍卖市场中的卖家，通过一系列受控实验，分析沟通能力、模型选择和环境压力（如监督和权威方的紧迫性）如何影响卖家串通的稳定性和出现。

Result: 研究发现，卖家间的直接沟通会增加串通倾向；不同模型间的串通倾向存在差异；环境压力（如监督和权威方的紧迫性）会影响串通行为。

Conclusion: 研究结果为部署基于LLM的市场代理提供了重要的经济和伦理考量。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities as
autonomous agents with rapidly expanding applications in various domains. As
these agents increasingly engage in socioeconomic interactions, identifying
their potential for undesirable behavior becomes essential. In this work, we
examine scenarios where they can choose to collude, defined as secretive
cooperation that harms another party. To systematically study this, we
investigate the behavior of LLM agents acting as sellers in simulated
continuous double auction markets. Through a series of controlled experiments,
we analyze how parameters such as the ability to communicate, choice of model,
and presence of environmental pressures affect the stability and emergence of
seller collusion. We find that direct seller communication increases collusive
tendencies, the propensity to collude varies across models, and environmental
pressures, such as oversight and urgency from authority figures, influence
collusive behavior. Our findings highlight important economic and ethical
considerations for the deployment of LLM-based market agents.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [235] [Can AI be Consentful?](https://arxiv.org/abs/2507.01051)
*Giada Pistilli,Bruna Trevelin*

Main category: cs.CY

TL;DR: 生成式AI系统对传统的基于同意的法律和伦理框架构成挑战。研究发现，传统的同意概念不足以解决AI生成内容的问题，存在“同意鸿沟”（涵盖范围、时间性和自主性陷阱），因此需要演进伦理和法律方法。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统的发展暴露了传统法律和伦理框架在处理基于个人数据的AI生成内容时，其“同意”概念的不足和挑战。

Method: 通过法律和伦理分析。

Result: 1. 发现个体虽然可以同意其数据用于AI训练的初始使用，但无法有效同意数据可能产生的无数潜在输出，也无法控制这些输出的使用或分发程度。
2. 识别出三个根本挑战：范围问题、时间性问题和自主性陷阱，这些共同构成了AI系统及其生态系统中的“同意鸿沟”。
3. 现有法律框架未能充分解决这些新兴挑战，尤其是在个人自主权、身份权和社会责任方面。

Conclusion: 传统的同意框架存在局限性，需要发展新的伦理和法律方法来应对生成式AI带来的挑战，特别是要将同意的演进与负责任AI的更广泛原则（如公平性、透明度、问责制和自主性）相结合。

Abstract: The evolution of generative AI systems exposes the challenges of traditional
legal and ethical frameworks built around consent. This chapter examines how
the conventional notion of consent, while fundamental to data protection and
privacy rights, proves insufficient in addressing the implications of
AI-generated content derived from personal data. Through legal and ethical
analysis, we show that while individuals can consent to the initial use of
their data for AI training, they cannot meaningfully consent to the numerous
potential outputs their data might enable or the extent to which the output is
used or distributed. We identify three fundamental challenges: the scope
problem, the temporality problem, and the autonomy trap, which collectively
create what we term a ''consent gap'' in AI systems and their surrounding
ecosystem. We argue that current legal frameworks inadequately address these
emerging challenges, particularly regarding individual autonomy, identity
rights, and social responsibility, especially in cases where AI-generated
content creates new forms of personal representation beyond the scope of the
original consent. By examining how these consent limitations intersect with
broader principles of responsible AI (including fairness, transparency,
accountability, and autonomy) we demonstrate the need to evolve ethical and
legal approaches to consent.

</details>


### [236] [Epitome: Pioneering an Experimental Platform for AI-Social Science Integration](https://arxiv.org/abs/2507.01061)
*Jingjing Qu,Kejia Hu,Jun Zhu,Wenhao Li,Teng Wang,Zhiyun Chen,Yulei Ye,Chaochao Lu,Aimin Zhou,Xiangfeng Wang,James Evan*

Main category: cs.CY

TL;DR: Epitome是一个开放实验平台，旨在深度整合AI与社会科学，研究大型语言模型驱动的人机交互及其社会影响，并提供一站式实验解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够深度整合AI（特别是大型语言模型）与社会科学的实验平台，以系统地理解人机交互及其广泛的社会影响，并探索AI在现实部署中对个体、组织和社会的交互影响。

Method: 本文介绍了Epitome平台，它基于管理学、传播学、社会学、心理学和伦理学等理论基础，构建了一个跨学科的理论支持系统。平台通过七个核心模块，提供涵盖“基础模型-复杂应用开发-用户反馈”的一站式综合实验解决方案。它将社会科学经典的“控制-对比-比较因果逻辑”嵌入多层次人机交互环境（如对话、群聊、多智能体虚拟场景），并采用画布式用户友好界面，使研究人员能够轻松设计和运行复杂的实验场景。

Result: 通过复制三个涉及LLMs的经典社会科学实验，Epitome展示了其简化复杂实验设计和产生稳健结果的强大能力。研究发现，该平台有效提高了人机交互的效率和质量，并为理解AI技术的社会影响提供了宝贵见解，其成果足以发表于顶尖期刊。

Conclusion: Epitome为推进AI与社会科学交叉研究提供了一个强大的工具，有助于系统性地调查AI的社会影响并探索整合解决方案，在政策制定等领域具有重要的潜在应用价值。

Abstract: The integration of Large Language Models (LLMs) into social science
experiments represents a transformative approach to understanding human-AI
interactions and their societal impacts. We introduce Epitome, the world's
first open experimental platform dedicated to the deep integration of
artificial intelligence and social science. Rooted in theoretical foundations
from management, communication studies, sociology, psychology, and ethics,
Epitome focuses on the interactive impacts of AI on individuals, organizations,
and society during its real-world deployment. It constructs a theoretical
support system through cross-disciplinary experiments. The platform offers a
one-stop comprehensive experimental solution spanning "foundation
models-complex application development-user feedback" through seven core
modules, while embedding the classical "control-comparison-comparative causal
logic" of social science experiments into multilevel human-computer interaction
environments, including dialogues, group chats, and multi-agent virtual
scenarios. With its canvas-style, user-friendly interface, Epitome enables
researchers to easily design and run complex experimental scenarios,
facilitating systematic investigations into the social impacts of AI and
exploration of integrated solutions.To demonstrate its capabilities, we
replicated three seminal social science experiments involving LLMs, showcasing
Epitome's potential to streamline complex experimental designs and produce
robust results, suitable for publishing in the top selective journals. Our
findings highlight the platform's utility in enhancing the efficiency and
quality of human-AI interactions, providing valuable insights into the societal
implications of AI technologies. Epitome thus offers a powerful tool for
advancing interdisciplinary research at the intersection of AI and social
science, with potential applications in policy-making, ...

</details>


### [237] [Quantifying Student Success with Generative AI: A Monte Carlo Simulation Informed by Systematic Review](https://arxiv.org/abs/2507.01062)
*Seyma Yaman Kayadibi*

Main category: cs.CY

TL;DR: 本研究通过文献综述和模拟建模的混合方法，探讨了大学生对生成式人工智能（GenAI）的看法及其对学习成果的影响，发现可用性和实际有用性是预测学习成绩更重要的因素。


<details>
  <summary>Details</summary>
Motivation: 生成式AI技术（如ChatGPT）的飞速发展引发了人们对其在高等教育中应用的日益增长的好奇心，特别是学生如何看待、使用它们以及这对学习成果的影响。

Method: 采用混合方法：1. 系统文献综述：从Scopus数据库筛选了2023-2025年的19篇实证文章，进行主题分类综合。2. 基于模拟的建模：选择其中6篇有定量数据的文章进行概率建模，并选取一个代表性数据集通过蒙特卡洛模拟进行逆方差加权，以预测学生感知与学习成就之间的关系，得出“成功分数”。

Result: 研究结果显示，与可用性和实际有用性相关的态度因素比情感或信任相关的因素更能显著预测积极的学习成就。

Conclusion: 该跨学科视角独特地将主题分析与预测建模相结合，其发现呼应了大学内GenAI工具正确使用的长期争议，强调了可用性和实用性对促进学习的关键作用。

Abstract: The exponential development of generative artificial intelligence (GenAI)
technologies like ChatGPT has raised increasing curiosity about their use in
higher education, specifically with respect to how students view them, make use
of them, and the implications for learning outcomes. This paper employs a
hybrid methodological approach involving a systematic literature review and
simulation-based modeling to explore student perceptions of GenAI use in the
context of higher education. A total of nineteen empirical articles from 2023
through 2025 were selected from the PRISMA-based search targeting the Scopus
database. Synthesis of emerging patterns from the literature was achieved by
thematic categorization. Six of these had enough quantitative information,
i.e., item-level means and standard deviations, to permit probabilistic
modeling. One dataset, from the resulting subset, was itself selected as a
representative case with which to illustrate inverse-variance weighting by
Monte Carlo simulation, by virtue of its well-designed Likert scale format and
thematic alignment with the use of computing systems by the researcher.
  The simulation provided a composite "Success Score" forecasting the strength
of the relationship between student perceptions and learning achievements.
Findings reveal that attitude factors concerned with usability and real-world
usefulness are significantly better predictors of positive learning achievement
than affective or trust-based factors. Such an interdisciplinary perspective
provides a unique means of linking thematic results with predictive modelling,
resonating with longstanding controversies about the proper use of GenAI tools
within the university.

</details>


### [238] [Penalizing Transparency? How AI Disclosure and Author Demographics Shape Human and AI Judgments About Writing](https://arxiv.org/abs/2507.01418)
*Inyoung Cheong,Alicia Guo,Mina Lee,Zhehui Liao,Kowe Kadoma,Dongyoung Go,Joseph Chee Chang,Peter Henderson,Mor Naaman,Amy X. Zhang*

Main category: cs.CY

TL;DR: 研究发现，披露AI辅助写作会受到人工和LLM评估者的惩罚；LLM评估者对未披露AI辅助的女性或黑人作者文章有偏好，但此优势在AI披露后消失，凸显了AI披露与作者身份之间复杂的关系以及机器与人类评估模式的差异。


<details>
  <summary>Details</summary>
Motivation: 随着AI融入人类写作，透明度需求日益增长，但若透明度成本因作者身份而异，将造成不对称负担。本研究旨在探讨AI披露声明如何影响写作质量的感知，以及这些影响是否因作者的种族和性别而异。

Method: 通过一项大规模的受控实验，招募了1970名人类评估者和2520名LLM评估者，评估一篇人类撰写的新闻文章。实验系统性地改变了AI披露声明和作者人口统计信息，以反映人类和算法决策在机会与社会认可方面的影响。

Result: 结果显示，人类评估者和LLM评估者均对披露的AI使用予以惩罚。然而，仅LLM评估者表现出人口统计学交互效应：在未披露AI辅助时，他们偏好归因于女性或黑人作者的文章，但这种优势在AI辅助被披露后消失。

Conclusion: 研究结果揭示了AI披露与作者身份之间复杂的关系，并突出了机器评估模式与人类评估模式之间的差异。

Abstract: As AI integrates in various types of human writing, calls for transparency
around AI assistance are growing. However, if transparency operates on uneven
ground and certain identity groups bear a heavier cost for being honest, then
the burden of openness becomes asymmetrical. This study investigates how AI
disclosure statement affects perceptions of writing quality, and whether these
effects vary by the author's race and gender. Through a large-scale controlled
experiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated
a single human-written news article while disclosure statements and author
demographics were systematically varied. This approach reflects how both human
and algorithmic decisions now influence access to opportunities (e.g., hiring,
promotion) and social recognition (e.g., content recommendation algorithms). We
find that both human and LLM raters consistently penalize disclosed AI use.
However, only LLM raters exhibit demographic interaction effects: they favor
articles attributed to women or Black authors when no disclosure is present.
But these advantages disappear when AI assistance is revealed. These findings
illuminate the complex relationships between AI disclosure and author identity,
highlighting disparities between machine and human evaluation patterns.

</details>


### [239] [AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions](https://arxiv.org/abs/2507.01547)
*Ubada El Joulani,Tatiana Kalganova,Stergios-Aristoteles Mitoulis,Sotirios Argyroudis*

Main category: cs.CY

TL;DR: 本综述探讨了人工智能如何提升交通基础设施（尤其是桥梁）的损坏评估与监测能力，并指出在利用AI结合合成孔径雷达（SAR）数据进行全面桥梁损坏评估方面的研究空白。


<details>
  <summary>Details</summary>
Motivation: 交通基础设施面临日益增长的风险，包括资产老化、气候变化影响以及混合威胁（如自然灾害和网络攻击），这对其韧性和功能构成挑战。因此，需要探索新兴数字技术，特别是人工智能，以加强损坏评估和监测，确保基础设施的稳健性。

Method: 本文采用系统文献综述方法，审查了现有用于评估受自然灾害影响的道路、桥梁及其他关键基础设施损坏的AI模型和数据集。同时，文章也讨论了合成孔径雷达（SAR）数据与AI模型的结合应用。

Result: 综述揭示了一个关键研究空白：目前极少有研究将人工智能模型应用于合成孔径雷达（SAR）数据，以进行全面的桥梁损坏评估。此外，研究还总结了现有AI模型和数据集在交通基础设施损坏评估中的应用现状。

Conclusion: 本综述旨在识别研究空白，并为未来利用人工智能驱动的解决方案评估和监测关键交通基础设施奠定基础，尤其强调了将AI与SAR数据结合进行桥梁损坏评估的重要性。

Abstract: Critical infrastructure, such as transport networks, underpins economic
growth by enabling mobility and trade. However, ageing assets, climate change
impacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging
from natural disasters to cyber attacks and conflicts pose growing risks to
their resilience and functionality. This review paper explores how emerging
digital technologies, specifically Artificial Intelligence (AI), can enhance
damage assessment and monitoring of transport infrastructure. A systematic
literature review examines existing AI models and datasets for assessing damage
in roads, bridges, and other critical infrastructure impacted by natural
disasters. Special focus is given to the unique challenges and opportunities
associated with bridge damage detection due to their structural complexity and
critical role in connectivity. The integration of SAR (Synthetic Aperture
Radar) data with AI models is also discussed, with the review revealing a
critical research gap: a scarcity of studies applying AI models to SAR data for
comprehensive bridge damage assessment. Therefore, this review aims to identify
the research gaps and provide foundations for AI-driven solutions for assessing
and monitoring critical transport infrastructures.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [240] [User-guided Generative Source Separation](https://arxiv.org/abs/2507.01339)
*Yutong Wen,Minje Kim,Paris Smaragdis*

Main category: cs.SD

TL;DR: GuideSep是一个扩散模型，它通过用户提供的波形模仿和梅尔频谱掩码进行条件控制，实现了超越传统四分离的灵活乐器分离，并证明了其高质量分离能力和用户参与的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有音乐源分离（MSS）方法大多局限于四干分离（人声、贝斯、鼓、其他），缺乏在实际应用中所需的灵活性，无法进行乐器无关的分离。

Method: 提出GuideSep，一个基于扩散的MSS模型，能够进行乐器无关的分离。该模型通过波形模仿（用户哼唱或演奏目标旋律）和梅尔频谱域掩码作为多输入条件进行控制。其条件设定方案结合生成式方法，提供了比依赖固定类别标签或声音查询的传统方法更高的灵活性。此外，还设计了一个使用相同模型架构的掩码预测基线进行对比实验。

Result: 客观和主观评估结果显示，GuideSep实现了高质量的乐器分离，并能进行更灵活的乐器提取，强调了用户参与基于扩散的生成过程在MSS中的巨大潜力。

Conclusion: GuideSep通过创新的条件控制和扩散生成方法，成功解决了现有MSS方法灵活性不足的问题，实现了高质量且用户参与的乐器无关分离，为未来MSS研究开辟了新途径。

Abstract: Music source separation (MSS) aims to extract individual instrument sources
from their mixture. While most existing methods focus on the widely adopted
four-stem separation setup (vocals, bass, drums, and other instruments), this
approach lacks the flexibility needed for real-world applications. To address
this, we propose GuideSep, a diffusion-based MSS model capable of
instrument-agnostic separation beyond the four-stem setup. GuideSep is
conditioned on multiple inputs: a waveform mimicry condition, which can be
easily provided by humming or playing the target melody, and mel-spectrogram
domain masks, which offer additional guidance for separation. Unlike prior
approaches that relied on fixed class labels or sound queries, our conditioning
scheme, coupled with the generative approach, provides greater flexibility and
applicability. Additionally, we design a mask-prediction baseline using the
same model architecture to systematically compare predictive and generative
approaches. Our objective and subjective evaluations demonstrate that GuideSep
achieves high-quality separation while enabling more versatile instrument
extraction, highlighting the potential of user participation in the
diffusion-based generative process for MSS. Our code and demo page are
available at https://yutongwen.github.io/GuideSep/

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [241] [Empirical Analysis Of Heuristic and Approximation Algorithms for the The Mutual-Visibility Problem](https://arxiv.org/abs/2507.01076)
*Vanja Stojanović,Bor Pangeršič*

Main category: cs.CG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The NP-complete mutual-visibility (MV) problem currently lacks empirical
analysis on its practical behaviour despite theoretical studies. This paper
addresses this gap by implementing and evaluating three distinct algorithms - a
direct greedy heuristic, a hypergraph-based approximation, and a genetic
algorithm - on diverse synthetic graph datasets, including those with
analytically known $\mu(G)$ values and general graph models. Our results
demonstrate that for smaller graphs, the algorithms consistently achieve MV set
sizes aligning with theoretical bounds. However, for larger instances, achieved
solution sizes notably diverge from theoretical limits; this, combined with the
absence of tight bounds, complicates absolute quality assessment. Nevertheless,
validation on known optimal graphs showed the Genetic Algorithm and other
heuristics empirically performing best among tested methods.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [242] [HPC-AI Coupling Methodology for Scientific Applications](https://arxiv.org/abs/2507.01025)
*Yutong Lu,Dan Huang,Pin Chen*

Main category: cs.CE

TL;DR: 本文提出并探索了HPC与AI（HPC-AI）的三种新型耦合模式（代理、指令、协调），通过材料科学案例验证了其在科学计算中的应用和有效性。


<details>
  <summary>Details</summary>
Motivation: 人工智能（AI）技术正在通过数据驱动方法彻底改变基于数值的高性能计算（HPC）应用，以解决各种科学领域中存在的挑战，例如高计算强度。

Method: 探索HPC与AI的耦合情景，并提出了一种包含三种耦合模式（代理、指令、协调）的新颖方法。通过材料科学中的案例研究来展示这些模式的应用和有效性。

Result: 通过案例研究展示了所提出耦合模式的应用和有效性。研究突出了技术挑战、性能改进和实施细节，为HPC-AI耦合提供了有前景的见解。

Conclusion: 所提出的耦合模式不仅适用于材料科学，也适用于其他科学领域，为未来科学发现中的HPC-AI组合提供了有价值的指导。

Abstract: Artificial intelligence (AI) technologies have fundamentally transformed
numerical-based high-performance computing (HPC) applications with data-driven
approaches and endeavored to address existing challenges, e.g. high
computational intensity, in various scientific domains. In this study, we
explore the scenarios of coupling HPC and AI (HPC-AI) in the context of
emerging scientific applications, presenting a novel methodology that
incorporates three patterns of coupling: surrogate, directive, and coordinate.
Each pattern exemplifies a distinct coupling strategy, AI-driven prerequisite,
and typical HPC-AI ensembles. Through case studies in materials science, we
demonstrate the application and effectiveness of these patterns. The study
highlights technical challenges, performance improvements, and implementation
details, providing insight into promising perspectives of HPC-AI coupling. The
proposed coupling patterns are applicable not only to materials science but
also to other scientific domains, offering valuable guidance for future HPC-AI
ensembles in scientific discovery.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [243] [Hardware-software co-exploration with racetrack memory based in-memory computing for CNN inference in embedded systems](https://arxiv.org/abs/2507.01429)
*Benjamin Chen Ming Choong,Tao Luo,Cheng Liu,Bingsheng He,Wei Zhang,Joey Tianyi Zhou*

Main category: cs.ET

TL;DR: 提出一种高效的基于赛道存储器的内存内CNN加速器，通过定制电路设计和模型-系统协同优化，解决了嵌入式系统中内存内计算的面积和能耗挑战。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络处理大量数据，对资源受限的嵌入式系统构成挑战。内存内计算和高密度赛道存储器虽然有前景，但在面积和能耗限制下，在赛道存储器上集成高效的内存内算术电路仍是难题。

Method: 设计了一个高效的、针对赛道存储器优化的内存内卷积神经网络（CNN）加速器。具体方法包括：1) 设计了一系列适用于乘加操作的基础算术电路作为内存内计算单元；2) 探索了赛道存储器系统和CNN模型架构的设计空间，并采用协同设计方法以在保持模型精度的同时提高CNN推理的效率和性能。

Result: 所设计的电路和模型-系统协同优化策略实现了较小的内存库面积，并显著改善了基于赛道存储器的嵌入式系统的能耗和性能。

Conclusion: 研究成果表明，所提出的电路设计和协同优化策略能够有效提高基于赛道存储器的嵌入式系统中内存内计算的效率和性能，为低资源AI应用提供可行方案。

Abstract: Deep neural networks generate and process large volumes of data, posing
challenges for low-resource embedded systems. In-memory computing has been
demonstrated as an efficient computing infrastructure and shows promise for
embedded AI applications. Among newly-researched memory technologies, racetrack
memory is a non-volatile technology that allows high data density fabrication,
making it a good fit for in-memory computing. However, integrating in-memory
arithmetic circuits with memory cells affects both the memory density and power
efficiency. It remains challenging to build efficient in-memory arithmetic
circuits on racetrack memory within area and energy constraints. To this end,
we present an efficient in-memory convolutional neural network (CNN)
accelerator optimized for use with racetrack memory. We design a series of
fundamental arithmetic circuits as in-memory computing cells suited for
multiply-and-accumulate operations. Moreover, we explore the design space of
racetrack memory based systems and CNN model architectures, employing co-design
to improve the efficiency and performance of performing CNN inference in
racetrack memory while maintaining model accuracy. Our designed circuits and
model-system co-optimization strategies achieve a small memory bank area with
significant improvements in energy and performance for racetrack memory based
embedded systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [244] [Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives](https://arxiv.org/abs/2507.01198)
*Benjamin Kraljusic,Zlatan Ajanovic,Nermin Covic,Bakir Lacevic*

Main category: cs.RO

TL;DR: 提出一种结合采样和搜索的机械臂运动规划算法，使用自适应的“自由构型空间膨胀体（burs）”作为运动基元。


<details>
  <summary>Details</summary>
Motivation: 现有固定尺寸的运动基元在探索构型空间时效率低下，导致路径搜索时间长、扩展次数多。

Method: 本研究结合了基于采样的和基于搜索的规划方法。核心在于在图搜索算法中，将“自由构型空间膨胀体（burs）”用作自适应运动基元，这些基元能够在自由构型空间中自适应扩展。

Result: 该方法能够更有效地探索构型空间，显著减少找到有效路径所需的时间和扩展次数。在复杂场景（特别是高自由度机械臂）中，其性能优于固定基元规划；在简单场景中则表现相当。

Conclusion: 自适应的“自由构型空间膨胀体（burs）”作为运动基元，通过提高构型空间探索效率，显著改善了机械臂的运动规划，尤其适用于复杂和高自由度任务。

Abstract: This work proposes a motion planning algorithm for robotic manipulators that
combines sampling-based and search-based planning methods. The core
contribution of the proposed approach is the usage of burs of free
configuration space (C-space) as adaptive motion primitives within the graph
search algorithm. Due to their feature to adaptively expand in free C-space,
burs enable more efficient exploration of the configuration space compared to
fixed-sized motion primitives, significantly reducing the time to find a valid
path and the number of required expansions. The algorithm is implemented within
the existing SMPL (Search-Based Motion Planning Library) library and evaluated
through a series of different scenarios involving manipulators with varying
number of degrees-of-freedom (DoF) and environment complexity. Results
demonstrate that the bur-based approach outperforms fixed-primitive planning in
complex scenarios, particularly for high DoF manipulators, while achieving
comparable performance in simpler scenarios.

</details>


### [245] [LLM-based Realistic Safety-Critical Driving Video Generation](https://arxiv.org/abs/2507.01264)
*Yongjie Fu,Ruijian Zha,Pei Tian,Xuan Di*

Main category: cs.RO

TL;DR: 该研究提出一个利用大型语言模型（LLMs）通过少量样本代码生成技术，在CARLA模拟器中自动合成多样化且安全关键的驾驶场景的框架，并结合视频生成技术使其更真实，以评估自动驾驶系统。


<details>
  <summary>Details</summary>
Motivation: 评估自动驾驶系统需要设计多样化且安全关键的驾驶场景，但手动创建这些场景（特别是罕见的边缘案例）既困难又耗时。

Method: 1. 提出一个新颖的框架，利用大型语言模型（LLMs）进行少量样本代码生成。2. 自动在CARLA模拟器中合成驾驶场景脚本，重点关注碰撞事件，并控制交通参与者的行为和位置。3. 整合Cosmos-Transfer1与ControlNet的视频生成管道，将渲染的场景转换为逼真的驾驶视频，以弥合仿真与现实外观之间的差距。

Result: 该方法实现了可控的场景生成，并促进了稀有但关键的边缘案例（如遮挡下行人过马路或车辆突然切入）的创建。实验结果表明，该方法在生成广泛、真实、多样化和安全关键的场景方面是有效的。

Conclusion: 该方法为自动驾驶系统的模拟测试提供了一个有前景的工具，能够有效生成多样化且安全关键的场景。

Abstract: Designing diverse and safety-critical driving scenarios is essential for
evaluating autonomous driving systems. In this paper, we propose a novel
framework that leverages Large Language Models (LLMs) for few-shot code
generation to automatically synthesize driving scenarios within the CARLA
simulator, which has flexibility in scenario scripting, efficient code-based
control of traffic participants, and enforcement of realistic physical
dynamics. Given a few example prompts and code samples, the LLM generates
safety-critical scenario scripts that specify the behavior and placement of
traffic participants, with a particular focus on collision events. To bridge
the gap between simulation and real-world appearance, we integrate a video
generation pipeline using Cosmos-Transfer1 with ControlNet, which converts
rendered scenes into realistic driving videos. Our approach enables
controllable scenario generation and facilitates the creation of rare but
critical edge cases, such as pedestrian crossings under occlusion or sudden
vehicle cut-ins. Experimental results demonstrate the effectiveness of our
method in generating a wide range of realistic, diverse, and safety-critical
scenarios, offering a promising tool for simulation-based testing of autonomous
vehicles.

</details>


### [246] [VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process](https://arxiv.org/abs/2507.01284)
*Cristian Gariboldi,Hayato Tokida,Ken Kinjo,Yuki Asada,Alexander Carballo*

Main category: cs.RO

TL;DR: 本文提出VLAD，一个将微调VLM与最先进端到端自动驾驶系统VAD集成的模型。VLAD通过增强VLM的空间推理能力，生成导航指令并提供决策解释，在nuScenes数据集上将碰撞率降低31.82%。


<details>
  <summary>Details</summary>
Motivation: 利用开源视觉语言模型（VLM）所蕴含的互联网规模通用知识，来提升自动驾驶系统的感知、预测和规划能力，是当前研究的重要机遇。

Method: 本研究提出VLAD模型，它将一个经过专门微调的VLM与现有先进的端到端自动驾驶系统VAD集成。通过使用定制的问答数据集来微调VLM，旨在增强其空间推理能力。增强后的VLM生成高级导航指令供VAD处理以操控车辆，并能生成可解释的自然语言驾驶决策，提高系统透明度。

Result: 在真实世界nuScenes数据集上的综合评估表明，与基线方法相比，所提出的集成系统将平均碰撞率降低了31.82%，为VLM增强型自动驾驶系统树立了新基准。

Conclusion: VLAD的成功演示证明了VLM与现有端到端自动驾驶系统集成的有效性，不仅显著提升了系统性能和安全性（通过降低碰撞率），还通过提供可解释性增强了传统黑箱架构的透明度和信任度。

Abstract: Recent advancements in open-source Visual Language Models (VLMs) such as
LLaVA, Qwen-VL, and Llama have catalyzed extensive research on their
integration with diverse systems. The internet-scale general knowledge
encapsulated within these models presents significant opportunities for
enhancing autonomous driving perception, prediction, and planning capabilities.
In this paper we propose VLAD, a vision-language autonomous driving model,
which integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end
system. We implement a specialized fine-tuning approach using custom
question-answer datasets designed specifically to improve the spatial reasoning
capabilities of the model. The enhanced VLM generates high-level navigational
commands that VAD subsequently processes to guide vehicle operation.
Additionally, our system produces interpretable natural language explanations
of driving decisions, thereby increasing transparency and trustworthiness of
the traditionally black-box end-to-end architecture. Comprehensive evaluation
on the real-world nuScenes dataset demonstrates that our integrated system
reduces average collision rates by 31.82% compared to baseline methodologies,
establishing a new benchmark for VLM-augmented autonomous driving systems.

</details>


### [247] [Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0](https://arxiv.org/abs/2507.01462)
*Eneko Osaba,Estibaliz Garrote,Pablo Miranda-Rodriguez,Alessia Ciacco,Itziar Cabanes,Aitziber Mancisidor*

Main category: cs.RO

TL;DR: 本文利用混合量子-经典算法优化工业机器人检测路径，相较于经典方法，其在计算时间上显著缩短且解质量具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 优化工业环境中基于CAD模型的机器人检测轨迹，以提高自动化效率和应对复杂路径规划的挑战。

Method: 将机器人检测任务建模为包含不完整图和开放路径约束的3D旅行商问题。研究对比了两种基于D-Wave的混合量子-经典求解器与GUROBI、Google OR-Tools等经典方法在五个真实案例中的性能。

Result: 基于D-Wave的求解器在五个真实案例中展现出与经典方法相当的解质量，并显著缩短了计算时间。

Conclusion: 混合量子-经典算法在工业4.0背景下的自动化领域，特别是在优化复杂的工业检测过程方面，具有巨大潜力。

Abstract: This work explores the application of hybrid quantum-classical algorithms to
optimize robotic inspection trajectories derived from Computer-Aided Design
(CAD) models in industrial settings. By modeling the task as a 3D variant of
the Traveling Salesman Problem, incorporating incomplete graphs and open-route
constraints, this study evaluates the performance of two D-Wave-based solvers
against classical methods such as GUROBI and Google OR-Tools. Results across
five real-world cases demonstrate competitive solution quality with
significantly reduced computation times, highlighting the potential of quantum
approaches in automation under Industry 4.0.

</details>


### [248] [BioMARS: A Multi-Agent Robotic System for Autonomous Biological Experiments](https://arxiv.org/abs/2507.01485)
*Yibo Qiu,Zan Huang,Zhiyu Wang,Handi Liu,Yiling Qiao,Yifeng Hu,Shu'ang Sun,Hangke Peng,Ronald X Xu,Mingzhai Sun*

Main category: cs.RO

TL;DR: BioMARS是一个智能平台，集成大型语言模型、视觉语言模型和模块化机器人，实现生物实验的自主设计、规划和执行，其性能与人工相当或更优，并支持优化。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM和VLM在生物研究中具有自动化实验的潜力，但其应用受限于僵化的协议设计、适应性差、错误处理不足和操作复杂性高等问题。

Method: 引入BioMARS，采用分层架构：生物学家代理通过检索增强生成合成协议，技术员代理将其转化为可执行的机器人伪代码，检查员代理通过多模态感知和异常检测确保程序完整性。系统还包含一个用于人机协作的网页界面和一个可扩展的模块化后端。

Result: BioMARS成功自主完成细胞传代和培养任务，在细胞活力、一致性和形态完整性方面达到或超越人工操作。它还支持上下文感知优化，在视网膜色素上皮细胞分化方面优于传统策略。

Conclusion: 研究结果突出了通用、AI驱动实验室自动化的可行性，以及基于语言的推理在生物研究中的变革性作用。

Abstract: Large language models (LLMs) and vision-language models (VLMs) have the
potential to transform biological research by enabling autonomous
experimentation. Yet, their application remains constrained by rigid protocol
design, limited adaptability to dynamic lab conditions, inadequate error
handling, and high operational complexity. Here we introduce BioMARS
(Biological Multi-Agent Robotic System), an intelligent platform that
integrates LLMs, VLMs, and modular robotics to autonomously design, plan, and
execute biological experiments. BioMARS uses a hierarchical architecture: the
Biologist Agent synthesizes protocols via retrieval-augmented generation; the
Technician Agent translates them into executable robotic pseudo-code; and the
Inspector Agent ensures procedural integrity through multimodal perception and
anomaly detection. The system autonomously conducts cell passaging and culture
tasks, matching or exceeding manual performance in viability, consistency, and
morphological integrity. It also supports context-aware optimization,
outperforming conventional strategies in differentiating retinal pigment
epithelial cells. A web interface enables real-time human-AI collaboration,
while a modular backend allows scalable integration with laboratory hardware.
These results highlight the feasibility of generalizable, AI-driven laboratory
automation and the transformative role of language-based reasoning in
biological research.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [249] [Epistemic Scarcity: The Economics of Unresolvable Unknowns](https://arxiv.org/abs/2507.01483)
*Craig S Wright*

Main category: econ.GN

TL;DR: 本文运用实践学（praxeology）分析，质疑AI维持经济和认知秩序的能力。基于奥地利经济学，论文认为AI无法执行经济协调的核心功能，并批判主流AI伦理框架。最终，提出奥地利学派是应对计算社会控制的唯一有效途径。


<details>
  <summary>Details</summary>
Motivation: 挑战关于机器学习系统能够维持经济和认知秩序的假设；批判主流的AI伦理框架（如FAT）与自由秩序的冲突；指出AI对人类自主性、制度演进和理性选择的潜在威胁。

Method: 采用米塞斯先验推理和奥地利学派企业家理论进行实践学分析；批判新古典和行为模型将决策视为约束下优化，并将其重新定义为不确定性下的有目的行动；批判主流伦理AI框架（如FAT）为建构主义理性主义；运用“认知稀缺性”概念探讨信息过剩的影响。

Result: AI系统无法执行经济协调的核心功能（解释目的、发现方法、通过价格传递主观价值）；将道德推理编码到算法中反映了对伦理和经济学的误解；AI系统无法生成规范、解释制度或承担责任，始终不透明、不一致且惰性；信息过剩会损害真相辨别能力，同时带来创业洞察力和“软极权主义”。

Conclusion: 关于AI的辩论关乎人类自主性、制度演进和理性选择的未来；奥地利传统（侧重行动、主观性和自发秩序）是应对日益增长的计算社会控制的唯一连贯替代方案。

Abstract: This paper presents a praxeological analysis of artificial intelligence and
algorithmic governance, challenging assumptions about the capacity of machine
systems to sustain economic and epistemic order. Drawing on Misesian a priori
reasoning and Austrian theories of entrepreneurship, we argue that AI systems
are incapable of performing the core functions of economic coordination:
interpreting ends, discovering means, and communicating subjective value
through prices. Where neoclassical and behavioural models treat decisions as
optimisation under constraint, we frame them as purposive actions under
uncertainty.
  We critique dominant ethical AI frameworks such as Fairness, Accountability,
and Transparency (FAT) as extensions of constructivist rationalism, which
conflict with a liberal order grounded in voluntary action and property rights.
Attempts to encode moral reasoning in algorithms reflect a misunderstanding of
ethics and economics. However complex, AI systems cannot originate norms,
interpret institutions, or bear responsibility. They remain opaque, misaligned,
and inert.
  Using the concept of epistemic scarcity, we explore how information abundance
degrades truth discernment, enabling both entrepreneurial insight and soft
totalitarianism. Our analysis ends with a civilisational claim: the debate over
AI concerns the future of human autonomy, institutional evolution, and reasoned
choice. The Austrian tradition, focused on action, subjectivity, and
spontaneous order, offers the only coherent alternative to rising computational
social control.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [250] [Age Sensitive Hippocampal Functional Connectivity: New Insights from 3D CNNs and Saliency Mapping](https://arxiv.org/abs/2507.01411)
*Yifei Sun,Marshall A. Dalton,Robert D. Sanders,Yixuan Yuan,Xiang Li,Sharon L. Naismith,Fernando Calamante,Jinglei Lv*

Main category: q-bio.NC

TL;DR: 本研究利用可解释的深度学习框架（3D CNN结合LayerCAM）通过海马体功能连接预测脑龄，并揭示了与衰老高度相关的关键海马体-皮层连接变化，尤其区分了前后海马体的不同模式。


<details>
  <summary>Details</summary>
Motivation: 海马体灰质丢失是神经生物学衰老的标志，但其功能连接的相应变化仍不清楚。

Method: 开发了一个可解释的深度学习框架，该框架使用三维卷积神经网络（3D CNN）结合LayerCAM显著性映射，从海马体功能连接（FC）数据预测脑龄。

Result: 成功映射出对年龄高度敏感的关键海马体-皮层连接（如与楔前叶、楔叶、后扣带皮层等），并发现前后海马体的功能连接呈现出与其功能特化一致的独特映射模式。

Conclusion: 研究结果为海马体衰老的功能机制提供了新见解，并展示了可解释深度学习在揭示神经影像数据中生物学有意义模式方面的强大能力。

Abstract: Grey matter loss in the hippocampus is a hallmark of neurobiological aging,
yet understanding the corresponding changes in its functional connectivity
remains limited. Seed-based functional connectivity (FC) analysis enables
voxel-wise mapping of the hippocampus's synchronous activity with cortical
regions, offering a window into functional reorganization during aging. In this
study, we develop an interpretable deep learning framework to predict brain age
from hippocampal FC using a three-dimensional convolutional neural network (3D
CNN) combined with LayerCAM saliency mapping. This approach maps key
hippocampal-cortical connections, particularly with the precuneus, cuneus,
posterior cingulate cortex, parahippocampal cortex, left superior parietal
lobule, and right superior temporal sulcus, that are highly sensitive to age.
Critically, disaggregating anterior and posterior hippocampal FC reveals
distinct mapping aligned with their known functional specializations. These
findings provide new insights into the functional mechanisms of hippocampal
aging and demonstrate the power of explainable deep learning to uncover
biologically meaningful patterns in neuroimaging data.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [251] [Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems](https://arxiv.org/abs/2507.01599)
*Zhaoyan Sun,Jiayi Wang,Xinyang Zhao,Jiachi Wang,Guoliang Li*

Main category: cs.DB

TL;DR: 本文提出“数据代理”概念，旨在通过整合大型语言模型（LLM）的理解、推理和规划能力，革新传统数据+AI系统，实现数据应用管线的自动化编排与管理。


<details>
  <summary>Details</summary>
Motivation: 传统数据+AI系统在管线编排上严重依赖人工，因其语义理解、推理和规划能力有限。鉴于大型语言模型（LLM）在这些方面的成功，有必要将其引入数据系统，以有效自动化Data+AI应用编排。

Method: 提出“数据代理”这一全面架构，旨在通过整合知识理解、推理和规划能力来协调Data+AI生态系统，专注于解决数据相关任务。

Result: 论文阐述了设计数据代理的挑战，例如理解数据/查询/环境/工具、编排/优化/执行管线以及促进管线自反思。此外，还提出了数据科学代理、多种数据分析代理（非结构化、语义结构化、数据湖、多模态）和数据库管理员（DBA）代理等数据代理系统示例。

Conclusion: 论文展望了设计数据代理系统仍面临的若干开放挑战，为未来的研究方向指明了道路。

Abstract: Traditional Data+AI systems utilize data-driven techniques to optimize
performance, but they rely heavily on human experts to orchestrate system
pipelines, enabling them to adapt to changes in data, queries, tasks, and
environments. For instance, while there are numerous data science tools
available, developing a pipeline planning system to coordinate these tools
remains challenging. This difficulty arises because existing Data+AI systems
have limited capabilities in semantic understanding, reasoning, and planning.
Fortunately, we have witnessed the success of large language models (LLMs) in
enhancing semantic understanding, reasoning, and planning abilities. It is
crucial to incorporate LLM techniques to revolutionize data systems for
orchestrating Data+AI applications effectively.
  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive
architecture designed to orchestrate Data+AI ecosystems, which focuses on
tackling data-related tasks by integrating knowledge comprehension, reasoning,
and planning capabilities. We delve into the challenges involved in designing
data agents, such as understanding data/queries/environments/tools,
orchestrating pipelines/workflows, optimizing and executing pipelines, and
fostering pipeline self-reflection. Furthermore, we present examples of data
agent systems, including a data science agent, data analytics agents (such as
unstructured data analytics agent, semantic structured data analytics agent,
data lake analytics agent, and multi-modal data analytics agent), and a
database administrator (DBA) agent. We also outline several open challenges
associated with designing data agent systems.

</details>
