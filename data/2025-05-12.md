<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 8]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.NI](#cs.NI) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification](https://arxiv.org/abs/2505.05583)
*Qianbo Zang,Christophe Zgrzendek,Igor Tchappi,Afshin Khadangi,Johannes Sedlmeir*

Main category: cs.CL

TL;DR: 提出了一种名为KG-HTC的方法，通过将知识图谱与大型语言模型（LLMs）集成，以解决分层文本分类（HTC）中数据稀缺、标签空间大和长尾分布的挑战，尤其在零样本场景下表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统的分层文本分类（HTC）方法大多依赖监督学习，但在实际应用中，由于缺乏标注数据，监督式HTC面临挑战。此外，HTC还常遇到标签空间巨大和长尾分布的问题。

Method: 提出了知识图谱赋能的零样本分层文本分类方法（KG-HTC）。该方法利用检索增强生成（RAG）的思路，从知识图谱中检索与输入文本相关的子图，为大型语言模型（LLMs）提供结构化的语义上下文，从而增强LLM对不同层级标签语义的理解。

Result: 在三个开源HTC数据集（WoS、DBpedia和Amazon）上的实验结果表明，KG-HTC在严格的零样本设置下显著优于三种基线方法，尤其在更深层次的分类上取得了实质性改进。

Conclusion: 将结构化知识（来自知识图谱）融入大型语言模型，能有效应对分层文本分类中标签空间大和长尾标签分布的挑战，特别是在零样本场景下。

Abstract: Hierarchical Text Classification (HTC) involves assigning documents to labels
organized within a taxonomy. Most previous research on HTC has focused on
supervised methods. However, in real-world scenarios, employing supervised HTC
can be challenging due to a lack of annotated data. Moreover, HTC often faces
issues with large label spaces and long-tail distributions. In this work, we
present Knowledge Graphs for zero-shot Hierarchical Text Classification
(KG-HTC), which aims to address these challenges of HTC in applications by
integrating knowledge graphs with Large Language Models (LLMs) to provide
structured semantic context during classification. Our method retrieves
relevant subgraphs from knowledge graphs related to the input text using a
Retrieval-Augmented Generation (RAG) approach. Our KG-HTC can enhance LLMs to
understand label semantics at various hierarchy levels. We evaluate KG-HTC on
three open-source HTC datasets: WoS, DBpedia, and Amazon. Our experimental
results show that KG-HTC significantly outperforms three baselines in the
strict zero-shot setting, particularly achieving substantial improvements at
deeper levels of the hierarchy. This evaluation demonstrates the effectiveness
of incorporating structured knowledge into LLMs to address HTC's challenges in
large label spaces and long-tailed label distributions. Our code is available
at: https://github.com/QianboZang/KG-HTC.

</details>


### [2] [Privacy-Preserving Transformers: SwiftKey's Differential Privacy Implementation](https://arxiv.org/abs/2505.05648)
*Abdelrahman Abouelenin,Mohamed Abdelrehim,Raffy Fahim,Amr Hendy,Mohamed Afify*

Main category: cs.CL

TL;DR: 该研究训练了一个差分隐私Transformer模型用于SwiftKey的语言建模，并在保护隐私的同时，在下一词预测准确率上优于现有的GRU模型。


<details>
  <summary>Details</summary>
Motivation: 旨在探索在SwiftKey输入法中使用Transformer模型进行语言建模的可行性，同时满足差分隐私要求，并平衡模型大小、运行速度和准确率之间的权衡，以期改进现有生产环境中的GRU模型。

Method: 1. 采用Transformer架构。 2. 使用差分隐私（DP）进行训练。 3. 将GPT-2架构缩小以适应设备要求。 4. 采用两阶段训练：首先在通用数据上训练种子模型，然后在用户输入数据上进行差分隐私微调。 5. 使用ONNX进行模型集成以实现灵活性和效率。

Result: 与生产环境中的GRU模型相比，差分隐私Transformer模型在下一词预测准确率上取得了小幅但一致的提升，同时内存占用和运行速度的增长在可接受范围内。

Conclusion: 通过缩小模型规模和两阶段差分隐私训练，成功将Transformer模型应用于SwiftKey语言建模，在满足隐私保护和资源限制的同时，提升了预测性能。

Abstract: In this paper we train a transformer using differential privacy (DP) for
language modeling in SwiftKey. We run multiple experiments to balance the
trade-off between the model size, run-time speed and accuracy. We show that we
get small and consistent gains in the next-word-prediction and accuracy with
graceful increase in memory and speed compared to the production GRU. This is
obtained by scaling down a GPT2 architecture to fit the required size and a two
stage training process that builds a seed model on general data and DP
finetunes it on typing data. The transformer is integrated using ONNX offering
both flexibility and efficiency.

</details>


### [3] [Exploration of COVID-19 Discourse on Twitter: American Politician Edition](https://arxiv.org/abs/2505.05687)
*Cindy Kim,Daniela Puchall,Jiangyi Liang,Jiwon Kim*

Main category: cs.CL

TL;DR: 通过分析美国政客的推文，研究了COVID-19疫情期间两党的不同关注点，并提出了一种基于推文内容预测政治立场的方法。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情影响了政治格局并加剧了党派分化，需要探究两党在应对危机时的不同方法、反应和态度。

Method: 收集美国主要政治人物的推文，使用词袋模型、bigram模型和TF-IDF模型分析关键词、主题和情绪。提出使用分类算法基于COVID-19相关术语预测推文的政治立场。

Result: 民主党更关注疫情伤亡和医疗建议，而共和党更关注政治责任、媒体更新和病毒进展监控。

Conclusion: 研究提出了一种系统性方法，可以通过分析推文中与COVID-19相关的术语，利用分类算法预测其政治倾向。

Abstract: The advent of the COVID-19 pandemic has undoubtedly affected the political
scene worldwide and the introduction of new terminology and public opinions
regarding the virus has further polarized partisan stances. Using a collection
of tweets gathered from leading American political figures online (Republican
and Democratic), we explored the partisan differences in approach, response,
and attitude towards handling the international crisis. Implementation of the
bag-of-words, bigram, and TF-IDF models was used to identify and analyze
keywords, topics, and overall sentiments from each party. Results suggest that
Democrats are more concerned with the casualties of the pandemic, and give more
medical precautions and recommendations to the public whereas Republicans are
more invested in political responsibilities such as keeping the public updated
through media and carefully watching the progress of the virus. We propose a
systematic approach to predict and distinguish a tweet's political stance (left
or right leaning) based on its COVID-19 related terms using different
classification algorithms on different language models.

</details>


### [4] [Assessing Robustness to Spurious Correlations in Post-Training Language Models](https://arxiv.org/abs/2505.05704)
*Julia Shuieh,Prasann Singhal,Apaar Shanker,John Heyer,George Pu,Samuel Denton*

Main category: cs.CL

TL;DR: 该研究评估了三种LLM后训练方法（SFT, DPO, KTO）在不同虚假相关性条件下的表现，发现没有一种方法在所有情况下都是最优的。


<details>
  <summary>Details</summary>
Motivation: 真实世界的LLM训练数据常包含虚假相关性（如偏见、数据集伪影），这会损害模型的性能和泛化能力。需要系统评估不同的后训练方法如何应对这些问题。

Method: 研究人员在包含不同程度（10% vs 90%）和类型（特征模糊性 vs 分布狭窄性）虚假相关性的合成任务（数学推理、受限指令遵循、文档问答）上，系统地评估了监督微调（SFT）、直接偏好优化（DPO）和Kahneman-Tversky优化（KTO）三种后训练算法。

Result: 模型性能通常（但不总是）在更高虚假相关性下降低。基于偏好的方法（DPO/KTO）在数学推理任务中表现出相对鲁棒性，而SFT在复杂的、上下文密集型任务中保持更强性能。

Conclusion: 没有单一的后训练策略能在所有场景中普遍胜出。最佳方法的选择取决于目标任务的类型和虚假相关性的性质。

Abstract: Supervised and preference-based fine-tuning techniques have become popular
for aligning large language models (LLMs) with user intent and correctness
criteria. However, real-world training data often exhibits spurious
correlations -- arising from biases, dataset artifacts, or other "shortcut"
features -- that can compromise a model's performance or generalization. In
this paper, we systematically evaluate three post-training algorithms --
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO
(Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and
spuriousness conditions. Our tasks span mathematical reasoning, constrained
instruction-following, and document-grounded question answering. We vary the
degree of spurious correlation (10% vs. 90%) and investigate two forms of
artifacts: "Feature Ambiguity" and "Distributional Narrowness." Our results
show that the models often but not always degrade under higher spuriousness.
The preference-based methods (DPO/KTO) can demonstrate relative robustness in
mathematical reasoning tasks. By contrast, SFT maintains stronger performance
in complex, context-intensive tasks. These findings highlight that no single
post-training strategy universally outperforms in all scenarios; the best
choice depends on the type of target task and the nature of spurious
correlations.

</details>


### [5] [TopicVD: A Topic-Based Dataset of Video-Guided Multimodal Machine Translation for Documentaries](https://arxiv.org/abs/2505.05714)
*Jinze Lv,Jian Chen,Zi Long,Xianghua Fu,Yin Chen*

Main category: cs.CL

TL;DR: 研究者创建了一个基于主题的纪录片视频多模态机器翻译数据集TopicVD，并提出了一个跨模态双向注意力模型，实验证明视觉信息和全局上下文能提升翻译性能，但也存在领域外性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态机器翻译（MMT）数据集大多是静态图像或短视频片段，缺乏覆盖多种领域和主题的大规模视频数据，无法满足纪录片翻译等真实世界MMT任务的需求。

Method: 1. 构建了一个基于主题的纪录片视频-字幕对数据集TopicVD，包含8个主题类别，并保留了上下文信息。 2. 提出了一个基于跨模态双向注意力模块的MMT模型，以更好地捕捉文本和视频之间的共享语义。

Result: 实验表明：1. 在TopicVD数据集上，视觉信息显著提升了纪录片翻译的性能。 2. MMT模型在领域外场景下性能显著下降，表明需要有效的领域自适应方法。 3. 全局上下文信息能有效提升翻译性能。

Conclusion: TopicVD数据集和提出的模型推动了视频指导的MMT研究，尤其是在纪录片领域。研究强调了视觉信息和全局上下文的重要性，同时也指出了领域自适应是未来需要解决的关键问题。

Abstract: Most existing multimodal machine translation (MMT) datasets are predominantly
composed of static images or short video clips, lacking extensive video data
across diverse domains and topics. As a result, they fail to meet the demands
of real-world MMT tasks, such as documentary translation. In this study, we
developed TopicVD, a topic-based dataset for video-supported multimodal machine
translation of documentaries, aiming to advance research in this field. We
collected video-subtitle pairs from documentaries and categorized them into
eight topics, such as economy and nature, to facilitate research on domain
adaptation in video-guided MMT. Additionally, we preserved their contextual
information to support research on leveraging the global context of
documentaries in video-guided MMT. To better capture the shared semantics
between text and video, we propose an MMT model based on a cross-modal
bidirectional attention module. Extensive experiments on the TopicVD dataset
demonstrate that visual information consistently improves the performance of
the NMT model in documentary translation. However, the MMT model's performance
significantly declines in out-of-domain scenarios, highlighting the need for
effective domain adaptation methods. Additionally, experiments demonstrate that
global context can effectively improve translation performance. % Dataset and
our implementations are available at https://github.com/JinzeLv/TopicVD

</details>


### [6] [Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions](https://arxiv.org/abs/2505.05755)
*Dhruvesh Patel,Aishwarya Sahoo,Avinash Amballa,Tahira Naseem,Tim G. J. Rudner,Andrew McCallum*

Main category: cs.CL

TL;DR: 提出了一种插入式语言模型（ILM），它通过在任意位置逐个插入标记来生成序列，在规划任务和任意长度文本填充方面优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 自回归模型（ARM）难以处理复杂约束和非顺序依赖，而掩码扩散模型（MDM）可能产生不连贯输出且难以处理未知长度的填充任务。

Method: 引入插入式语言模型（ILM），该模型学习在序列的任意位置联合选择插入位置和词汇元素，每次插入一个标记。使用定制的网络参数化和去噪目标进行训练。

Result: ILM 在规划任务中优于 ARM 和 MDM。在无条件文本生成中，ILM 优于 MDM 且与 ARM 表现相当。在任意长度文本填充方面，ILM 比 MDM 更灵活。

Conclusion: ILM 是一个强大的序列生成模型，能有效处理非顺序依赖和复杂约束，在规划和灵活文本填充任务中展现出优势。

Abstract: Autoregressive models (ARMs), which predict subsequent tokens one-by-one
``from left to right,'' have achieved significant success across a wide range
of sequence generation tasks. However, they struggle to accurately represent
sequences that require satisfying sophisticated constraints or whose sequential
dependencies are better addressed by out-of-order generation. Masked Diffusion
Models (MDMs) address some of these limitations, but the process of unmasking
multiple tokens simultaneously in MDMs can introduce incoherences, and MDMs
cannot handle arbitrary infilling constraints when the number of tokens to be
filled in is not known in advance. In this work, we introduce Insertion
Language Models (ILMs), which learn to insert tokens at arbitrary positions in
a sequence -- that is, they select jointly both the position and the vocabulary
element to be inserted. By inserting tokens one at a time, ILMs can represent
strong dependencies between tokens, and their ability to generate sequences in
arbitrary order allows them to accurately model sequences where token
dependencies do not follow a left-to-right sequential structure. To train ILMs,
we propose a tailored network parameterization and use a simple denoising
objective. Our empirical evaluation demonstrates that ILMs outperform both ARMs
and MDMs on common planning tasks. Furthermore, we show that ILMs outperform
MDMs and perform on par with ARMs in an unconditional text generation task
while offering greater flexibility than MDMs in arbitrary-length text
infilling.

</details>


### [7] [Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM](https://arxiv.org/abs/2505.05772)
*Zehao Fan,Garrett Gagnon,Zhenyu Liu,Liu Liu*

Main category: cs.CL

TL;DR: 提出STARC，一种针对PIM架构优化的稀疏数据映射方案，通过聚类KV缓存并对齐内存结构，显著降低LLM解码延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）自回归解码因频繁内存访问和不断增长的KV缓存而面临内存带宽瓶颈，尤其是在长上下文场景下。内存处理（PIM）架构虽有潜力，但现有设计难以高效处理KV缓存稀疏化带来的不规则访问模式，导致负载不均衡和效率降低。

Method: 提出STARC方案：1) 根据语义相似性将KV对聚类；2) 将聚类映射到与PIM内存bank结构对齐的连续内存区域；3) 解码时，查询通过匹配预计算的聚类质心来检索相关令牌簇，实现选择性注意力和并行处理，避免频繁重聚类和数据移动开销。

Result: 在HBM-PIM系统上的实验表明，与常见的token级稀疏方法相比，STARC将注意力层延迟降低了19%-31%，能耗降低了19%-27%。在1024的KV缓存预算下，与全KV缓存检索相比，延迟降低高达54%-74%，能耗降低45%-67%。

Conclusion: STARC通过有效管理KV缓存稀疏性，能够在PIM架构上实现高效且硬件友好的长上下文LLM推理，同时保持与先进稀疏注意力方法相当的模型精度。

Abstract: Transformer-based models are the foundation of modern machine learning, but
their execution, particularly during autoregressive decoding in large language
models (LLMs), places significant pressure on memory systems due to frequent
memory accesses and growing key-value (KV) caches. This creates a bottleneck in
memory bandwidth, especially as context lengths increase. Processing-in-memory
(PIM) architectures are a promising solution, offering high internal bandwidth
and compute parallelism near memory. However, current PIM designs are primarily
optimized for dense attention and struggle with the dynamic, irregular access
patterns introduced by modern KV cache sparsity techniques. Consequently, they
suffer from workload imbalance, reducing throughput and resource utilization.
In this work, we propose STARC, a novel sparsity-optimized data mapping scheme
tailored specifically for efficient LLM decoding on PIM architectures. STARC
clusters KV pairs by semantic similarity and maps them to contiguous memory
regions aligned with PIM bank structures. During decoding, queries retrieve
relevant tokens at cluster granularity by matching against precomputed
centroids, enabling selective attention and parallel processing without
frequent reclustering or data movement overhead. Experiments on the HBM-PIM
system show that, compared to common token-wise sparsity methods, STARC reduces
attention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a
KV cache budget of 1024, it achieves up to 54%--74% latency reduction and
45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC
maintains model accuracy comparable to state-of-the-art sparse attention
methods, demonstrating its effectiveness in enabling efficient and
hardware-friendly long-context LLM inference on PIM architectures.

</details>


### [8] [Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted](https://arxiv.org/abs/2505.05815)
*Machi Shimmei,Masaki Uto,Yuichiroh Matsubayashi,Kentaro Inui,Aditi Mallavarapu,Noboru Matsuda*

Main category: cs.CL

TL;DR: 本研究提出并评估了一种名为AnaQuest的创新提示技术，旨在利用预训练大语言模型生成高质量的选择题，其表现优于基线ChatGPT。


<details>
  <summary>Details</summary>
Motivation: 提高利用大型语言模型自动生成选择题的质量和有效性，特别是生成能够模拟人类出题者水平的题目。

Method: 研究提出AnaQuest提示技术。该技术首先让学生以自由文本回答开放式问题（形成性评估），然后AnaQuest分析这些回答，生成关于目标概念的正确及错误断言作为选择题选项（总结性评估）。通过项目反应理论（IRT）对比AnaQuest、基线ChatGPT及人工生成的选择题。

Result: 专家评估认为，AI模型（AnaQuest和ChatGPT）生成的选择题与人工创建的选择题一样有效。然而，基于IRT的分析表明，AnaQuest生成的题目，特别是在错误断言（干扰项）方面，其难度和区分度比ChatGPT生成的题目更接近人工创建的题目。

Conclusion: AnaQuest提示技术能够生成与人工出题质量相当的选择题，并且在题目难度和区分度，特别是干扰项的质量上，优于基线ChatGPT提示方法。

Abstract: The primary goal of this study is to develop and evaluate an innovative
prompting technique, AnaQuest, for generating multiple-choice questions (MCQs)
using a pre-trained large language model. In AnaQuest, the choice items are
sentence-level assertions about complex concepts. The technique integrates
formative and summative assessments. In the formative phase, students answer
open-ended questions for target concepts in free text. For summative
assessment, AnaQuest analyzes these responses to generate both correct and
incorrect assertions. To evaluate the validity of the generated MCQs, Item
Response Theory (IRT) was applied to compare item characteristics between MCQs
generated by AnaQuest, a baseline ChatGPT prompt, and human-crafted items. An
empirical study found that expert instructors rated MCQs generated by both AI
models to be as valid as those created by human instructors. However, IRT-based
analysis revealed that AnaQuest-generated questions - particularly those with
incorrect assertions (foils) - more closely resembled human-crafted items in
terms of difficulty and discrimination than those produced by ChatGPT.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [9] [Data extraction and processing methods to aid the study of driving behaviors at intersections in naturalistic driving](https://arxiv.org/abs/2505.05487)
*Shrinivas Pundlik,Seonggyu Choe,Patrick Baker,Chen-Yuan Lee,Naser Al-Madi,Alex R. Bowers,Gang Luo*

Main category: cs.CV

TL;DR: 该报告描述了一种自动化方法，用于从自然驾驶研究数据中提取和表征驾驶员在交叉路口的头部扫描行为，并结合场景视频信息推断交叉路口类型和驾驶行为。


<details>
  <summary>Details</summary>
Motivation: 自然驾驶研究产生海量多样的数据，需要自动化处理方法来分析驾驶员行为，特别是为了理解驾驶员在交叉路口如何通过头部扫描观察环境并与交通状况互动。

Method: 该研究使用车载记录系统（记录速度、GPS、场景视频、座舱视频）收集数据。开发了定制工具进行交叉路口标记、数据同步和视频裁剪。采用定制的头部姿态检测AI模型分析座舱视频以估计头部扫描。使用YOLO模型处理场景视频以检测交通信号灯、标志、行人和其他车辆。通过车辆运动模式检测转向行为，通过图像强度变化检测停止线。最后，结合场景视频信息和速度数据，通过基于规则的算法推断交叉路口类型、驾驶行为和边界。

Result: 在处理的190个交叉路口数据中，自动化视频处理算法在检测交叉路口标志方面达到100%的准确率，在检测驾驶行为方面达到94%的准确率。检测车辆进入交叉路口的中位误差为1.1米和0.2秒。地面实况与估计的交叉路口边界之间的中位重叠度为0.88。

Conclusion: 该研究成功开发并验证了一种有效的自动化处理流程，能够从自然驾驶数据中准确提取驾驶员头部扫描信息，并结合多源数据推断交叉路口特征与驾驶行为，为大规模分析驾驶员在交叉路口的行为提供了技术支持。

Abstract: Naturalistic driving studies use devices in participants' own vehicles to
record daily driving over many months. Due to diverse and extensive amounts of
data recorded, automated processing is necessary. This report describes methods
to extract and characterize driver head scans at intersections from data
collected from an in-car recording system that logged vehicle speed, GPS
location, scene videos, and cabin videos. Custom tools were developed to mark
the intersections, synchronize location and video data, and clip the cabin and
scene videos for +/-100 meters from the intersection location. A
custom-developed head pose detection AI model for wide angle head turns was run
on the cabin videos to estimate the driver head pose, from which head scans >20
deg were computed in the horizontal direction. The scene videos were processed
using a YOLO object detection model to detect traffic lights, stop signs,
pedestrians, and other vehicles on the road. Turning maneuvers were
independently detected using vehicle self-motion patterns. Stop lines on the
road surface were detected using changing intensity patterns over time as the
vehicle moved. The information obtained from processing the scene videos, along
with the speed data was used in a rule-based algorithm to infer the
intersection type, maneuver, and bounds. We processed 190 intersections from 3
vehicles driven in cities and suburban areas from Massachusetts and California.
The automated video processing algorithm correctly detected intersection
signage and maneuvers in 100% and 94% of instances, respectively. The median
[IQR] error in detecting vehicle entry into the intersection was 1.1[0.4-4.9]
meters and 0.2[0.1-0.54] seconds. The median overlap between ground truth and
estimated intersection bounds was 0.88[0.82-0.93].

</details>


### [10] [From Events to Enhancement: A Survey on Event-Based Imaging Technologies](https://arxiv.org/abs/2505.05488)
*Yunfan Lu,Xiaogang Xu,Pengteng Li,Yusheng Wang,Yi Cui,Huizai Yao,Hui Xiong*

Main category: cs.CV

TL;DR: 这是一篇关于事件相机在成像领域应用的综述论文，总结了其基础、在图像/视频增强和高级任务中的应用进展，并探讨了未来的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管事件相机研究日益增多，但缺乏对其最新进展和挑战的全面梳理，这限制了人们对其在通用成像应用中潜力的广泛理解和利用。

Method: 首先介绍事件相机的物理模型和特性，接着回顾其在图像/视频增强任务和高级成像任务（如光场估计、多视图生成、光度测量）中的应用进展，最后讨论该领域的新挑战和开放性问题。

Result: 本文提供了事件相机成像领域的全面概述，涵盖了基础知识、与图像/视频增强任务的交互、在高级任务中的应用，并指出了该领域的最新进展和尚待解决的挑战。

Conclusion: 事件相机是具有颠覆性的成像技术，本综述通过系统性地总结研究现状和挑战，为该快速发展的领域提供了视角，并指明了未来的研究方向。

Abstract: Event cameras offering high dynamic range and low latency have emerged as
disruptive technologies in imaging. Despite growing research on leveraging
these benefits for different imaging tasks, a comprehensive study of recently
advances and challenges are still lacking. This limits the broader
understanding of how to utilize events in universal imaging applications. In
this survey, we first introduce a physical model and the characteristics of
different event sensors as the foundation. Following this, we highlight the
advancement and interaction of image/video enhancement tasks with events.
Additionally, we explore advanced tasks, which capture richer light information
with events, \eg~light field estimation, multi-view generation, and
photometric. Finally, we discuss new challenges and open questions offering a
perspective for this rapidly evolving field. More continuously updated
resources are at this link: https://github.com/yunfanLu/Awesome-Event-Imaging

</details>


### [11] [MDDFNet: Mamba-based Dynamic Dual Fusion Network for Traffic Sign Detection](https://arxiv.org/abs/2505.05491)
*TianYi Yu*

Main category: cs.CV

TL;DR: 提出了一种名为MDDFNet的新型目标检测网络，专用于交通标志检测，有效解决了特征提取单一和多尺度物体检测的难题。


<details>
  <summary>Details</summary>
Motivation: 现有的小目标（特别是交通标志）检测方法存在特征提取过于单一和难以有效处理不同尺寸物体的问题。

Method: 提出了Mamba动态双重融合网络（MDDFNet）。该网络包含一个动态双重融合模块，通过多分支整合空间和语义信息以增强特征多样性；以及一个基于Mamba的主干网络，利用全局特征融合和局部特征交互自适应地组合特征。

Result: 在TT100K数据集上的实验表明，MDDFNet的性能优于其他SOTA检测器，同时保持了单阶段模型的实时处理能力并取得了更优异的性能。

Conclusion: MDDFNet能够有效地检测小型交通标志，解决了现有方法在特征多样性和多尺度适应性方面的不足。

Abstract: The Detection of small objects, especially traffic signs, is a critical
sub-task in object detection and autonomous driving. Despite signficant
progress in previous research, two main challenges remain. First, the issue of
feature extraction being too singular. Second, the detection process struggles
to efectively handle objects of varying sizes or scales. These problems are
also prevalent in general object detection tasks. To address these challenges,
we propose a novel object detection network, Mamba-based Dynamic Dual Fusion
Network (MDDFNet), for traffic sign detection. The network integrates a dynamic
dual fusion module and a Mamba-based backbone to simultaneously tackle the
aforementioned issues. Specifically, the dynamic dual fusion module utilizes
multiple branches to consolidate various spatial and semantic information, thus
enhancing feature diversity. The Mamba-based backbone leverages global feature
fusion and local feature interaction, combining features in an adaptive manner
to generate unique classification characteristics. Extensive experiments
conducted on the TT100K (Tsinghua-Tencent 100K) datasets demonstrate that
MDDFNet outperforms other state-of-the-art detectors, maintaining real-time
processing capabilities of single-stage models while achieving superior
performance. This confirms the efectiveness of MDDFNet in detecting small
traffic signs.

</details>


### [12] [DetoxAI: a Python Toolkit for Debiasing Deep Learning Models in Computer Vision](https://arxiv.org/abs/2505.05492)
*Ignacy Stępka,Lukasz Sztukiewicz,Michał Wiliński,Jerzy Stefanowski*

Main category: cs.CV

TL;DR: 介绍了DetoxAI，一个用于通过事后去偏来提高深度学习视觉分类器公平性的开源Python库。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习公平性解决方案主要针对表格数据，不适用于依赖深度学习的视觉分类任务。

Method: 引入DetoxAI库，该库实现了先进的去偏算法、公平性指标和可视化工具，支持通过干预内部表征进行去偏，并包含基于归因的可视化和量化公平性指标。

Result: DetoxAI提供了工具来减轻偏见，并通过可视化和量化指标展示偏见如何被缓解，从而为工程师和研究人员提供实际价值。

Conclusion: DetoxAI为工程师和研究人员提供了一个有价值的工具，用于解决和改善深度学习视觉分类器中的公平性问题。

Abstract: While machine learning fairness has made significant progress in recent
years, most existing solutions focus on tabular data and are poorly suited for
vision-based classification tasks, which rely heavily on deep learning. To
bridge this gap, we introduce DetoxAI, an open-source Python library for
improving fairness in deep learning vision classifiers through post-hoc
debiasing. DetoxAI implements state-of-the-art debiasing algorithms, fairness
metrics, and visualization tools. It supports debiasing via interventions in
internal representations and includes attribution-based visualization tools and
quantitative algorithmic fairness metrics to show how bias is mitigated. This
paper presents the motivation, design, and use cases of DetoxAI, demonstrating
its tangible value to engineers and researchers.

</details>


### [13] [Learning 3D Persistent Embodied World Models](https://arxiv.org/abs/2505.05495)
*Siyuan Zhou,Yilun Du,Yuncong Yang,Lei Han,Peihao Chen,Dit-Yan Yeung,Chuang Gan*

Main category: cs.CV

TL;DR: 提出了一种具有持久性3D记忆的具身世界模型，通过整合生成的RGB-D视频到3D地图中，实现了对环境更一致的长期模拟。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视频的世界模型通常是短视的，缺乏对当前未观察到的场景部分的记忆，这阻碍了智能体在复杂环境中制定一致的长期计划。

Method: 引入一种具有显式记忆的持久化具身世界模型。该模型使用视频扩散模型预测未来的RGB-D视频，并将生成的内容聚合到一个持久的3D地图中。视频模型以该3D空间地图为条件进行生成，从而模拟世界的已见和未见部分。

Result: 该模型能够忠实地模拟世界的已见和未见部分，并在下游的具身应用中展示了其有效性，支持了有效的规划和策略学习。

Conclusion: 具有显式记忆的持久化具身世界模型通过整合3D空间地图，能够实现更一致的长期模拟，对具身智能体的规划和策略学习有益。

Abstract: The ability to simulate the effects of future actions on the world is a
crucial ability of intelligent embodied agents, enabling agents to anticipate
the effects of their actions and make plans accordingly. While a large body of
existing work has explored how to construct such world models using video
models, they are often myopic in nature, without any memory of a scene not
captured by currently observed images, preventing agents from making consistent
long-horizon plans in complex environments where many parts of the scene are
partially observed. We introduce a new persistent embodied world model with an
explicit memory of previously generated content, enabling much more consistent
long-horizon simulation. During generation time, our video diffusion model
predicts RGB-D video of the future observations of the agent. This generation
is then aggregated into a persistent 3D map of the environment. By conditioning
the video model on this 3D spatial map, we illustrate how this enables video
world models to faithfully simulate both seen and unseen parts of the world.
Finally, we illustrate the efficacy of such a world model in downstream
embodied applications, enabling effective planning and policy learning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [14] [Safety by Measurement: A Systematic Literature Review of AI Safety Evaluation Methods](https://arxiv.org/abs/2505.05541)
*Markov Grey,Charbel-Raphaël Segerie*

Main category: cs.AI

TL;DR: 现有基准测试不足以评估前沿AI的安全性，需要新的评估方法来衡量其能力、倾向和控制，以确保安全并指导治理。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI系统能力日益增强，传统的基准测试方法已无法充分评估其潜在风险和实际部署行为，因此需要更有效的安全评估方法来指导治理。

Method: 本文进行文献综述，整合AI安全评估领域的研究，并提出一个包含三个维度的系统分类法：评估什么特性（能力、倾向、控制）、如何评估（行为技术和内部技术）以及如何将评估结果整合到治理框架中。

Result: 综述展示了评估如何超越基准测试，深入分析了网络安全利用、欺骗、自主复制、态势感知等关键能力，以及权力寻求等令人担忧的倾向。讨论了评估方法如何融入治理，并指出了评估面临的挑战（如证明能力缺失、模型“装傻”、安全清洗）和未来研究方向。

Conclusion: 为了确保AI安全，需要从衡量能力、倾向和控制三个维度，采用行为和内部技术相结合的综合评估方法，取代仅依赖基准测试的传统方式。本文旨在为理解AI安全评估提供一个核心参考。

Abstract: As frontier AI systems advance toward transformative capabilities, we need a
parallel transformation in how we measure and evaluate these systems to ensure
safety and inform governance. While benchmarks have been the primary method for
estimating model capabilities, they often fail to establish true upper bounds
or predict deployment behavior. This literature review consolidates the rapidly
evolving field of AI safety evaluations, proposing a systematic taxonomy around
three dimensions: what properties we measure, how we measure them, and how
these measurements integrate into frameworks. We show how evaluations go beyond
benchmarks by measuring what models can do when pushed to the limit
(capabilities), the behavioral tendencies exhibited by default (propensities),
and whether our safety measures remain effective even when faced with
subversive adversarial AI (control). These properties are measured through
behavioral techniques like scaffolding, red teaming and supervised fine-tuning,
alongside internal techniques such as representation analysis and mechanistic
interpretability. We provide deeper explanations of some safety-critical
capabilities like cybersecurity exploitation, deception, autonomous
replication, and situational awareness, alongside concerning propensities like
power-seeking and scheming. The review explores how these evaluation methods
integrate into governance frameworks to translate results into concrete
development decisions. We also highlight challenges to safety evaluations -
proving absence of capabilities, potential model sandbagging, and incentives
for "safetywashing" - while identifying promising research directions. By
synthesizing scattered resources, this literature review aims to provide a
central reference point for understanding AI safety evaluations.

</details>


### [15] [HiBayES: A Hierarchical Bayesian Modeling Framework for AI Evaluation Statistics](https://arxiv.org/abs/2505.05602)
*Lennart Luettgau,Harry Coppock,Magda Dubois,Christopher Summerfield,Cozmin Ududec*

Main category: cs.AI

TL;DR: 提出 HiBayES，一个分层贝叶斯框架，用于稳健地评估 AI 系统（尤其在低数据量下）并量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型 (LLMs) 等 AI 系统的发展，需要一种方法来稳健地评估其能力（基于随机输出），系统地量化评估中的不确定性，并应对复杂、高成本、低数据量的评估挑战。

Method: 引入 HiBayES，一个基于广义线性模型 (GLMs)、贝叶斯数据分析和形式化模型比较的可推广的分层贝叶斯建模框架，用于 AI 评估统计。

Result: HiBayES 支持在经典问答基准和高级智能体评估中进行稳健的推断，尤其是在低数据场景（例如，每次评估 < 20 个数据点）下。它提供了原则性的不确定性量化和稳健的参数估计。同时提供了一个 HiBayES 软件包。

Conclusion: HiBayES 是一个通用且稳健的框架，能够有效应对 AI 评估中存在的随机性、不确定性量化、评估结构复杂、成本高以及数据量有限等挑战。

Abstract: As Large Language Models (LLMs) and other AI systems evolve, robustly
estimating their capabilities from inherently stochastic outputs while
systematically quantifying uncertainty in these estimates becomes increasingly
important. Further, advanced AI evaluations often have a nested hierarchical
structure, exhibit high levels of complexity, and come with high costs in
testing the most advanced AI systems. To address these challenges, we introduce
HiBayES, a generalizable Hierarchical Bayesian modeling framework for AI
Evaluation Statistics. HiBayES supports robust inferences in classical
question-answer benchmarks and advanced agentic evaluations, particularly in
low-data scenarios (e.g., < 20 data points per evaluation). Built on
Generalized Linear Models (GLMs), Bayesian data analysis, and formal model
comparison, HiBayES provides principled uncertainty quantification and robust
parameter estimation. This paper offers a comprehensive introduction to
HiBayES, including illustrative examples, comparisons to conventional
statistical methods, and practical guidance for implementing multilevel
Bayesian GLMs. Additionally, we provide a HiBayES software package [4] (Beta
version) for out-of-the-box implementation.

</details>


### [16] [scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction](https://arxiv.org/abs/2505.05612)
*Qing Wang,Yining Pan,Minghao Zhou,Zijia Tang,Yanfei Wang,Guangyu Wang,Qianqian Song*

Main category: cs.AI

TL;DR: scDrugMap是一个评估基础模型在单细胞数据中预测药物反应的框架和基准，为药物发现提供了用户友好的平台。


<details>
  <summary>Details</summary>
Motivation: 癌症治疗中的耐药性是一个重大挑战，而大规模基础模型在单细胞数据中预测药物反应的应用仍有待探索。

Method: 开发了scDrugMap框架（含Python命令行界面和Web服务器），评估了包括8个单细胞模型和2个大型语言模型在内的多种基础模型。使用了包含超过32.6万个细胞的策划数据集，在混合数据和交叉数据评估设置下，采用层冻结和LoRA微调策略进行基准测试。

Result: 在混合数据场景中，scFoundation表现最佳（层冻结F1均值0.971，微调F1均值0.947）。在交叉数据场景中，UCE微调后表现优异（F1均值0.774），scGPT在零样本学习中领先（F1均值0.858）。

Conclusion: scDrugMap首次对用于单细胞数据药物反应预测的基础模型进行了大规模基准测试，并提供了一个用户友好、灵活的平台，以促进药物发现和转化研究。

Abstract: Drug resistance presents a major challenge in cancer therapy. Single cell
profiling offers insights into cellular heterogeneity, yet the application of
large-scale foundation models for predicting drug response in single cell data
remains underexplored. To address this, we developed scDrugMap, an integrated
framework featuring both a Python command-line interface and a web server for
drug response prediction. scDrugMap evaluates a wide range of foundation
models, including eight single-cell models and two large language models, using
a curated dataset of over 326,000 cells in the primary collection and 18,800
cells in the validation set, spanning 36 datasets and diverse tissue and cancer
types. We benchmarked model performance under pooled-data and cross-data
evaluation settings, employing both layer freezing and Low-Rank Adaptation
(LoRA) fine-tuning strategies. In the pooled-data scenario, scFoundation
achieved the best performance, with mean F1 scores of 0.971 (layer freezing)
and 0.947 (fine-tuning), outperforming the lowest-performing model by over 50%.
In the cross-data setting, UCE excelled post fine-tuning (mean F1: 0.774),
while scGPT led in zero-shot learning (mean F1: 0.858). Overall, scDrugMap
provides the first large-scale benchmark of foundation models for drug response
prediction in single-cell data and serves as a user-friendly, flexible platform
for advancing drug discovery and translational research.

</details>


### [17] [Leveraging Large Language Models for enzymatic reaction prediction and characterization](https://arxiv.org/abs/2505.05616)
*Lorenzo Di Fruscia,Jana Marie Weber*

Main category: cs.AI

TL;DR: 评估了大型语言模型（Llama-3.1）在酶促反应预测（EC号、正向合成、逆合成）中的能力，发现微调和多任务学习有效，但也存在局限性。


<details>
  <summary>Details</summary>
Motivation: 酶促反应预测对生物催化、代谢工程和药物发现至关重要，但现有方法复杂且资源密集。大型语言模型（LLMs）在科学领域展现出潜力。

Method: 使用Llama-3.1模型（8B和70B），通过LoRA进行参数高效微调，系统评估了其在酶EC号预测、正向合成和逆合成三个核心生化任务上的表现。比较了单任务和多任务学习策略，并评估了在不同数据量（包括低数据）下的适应性。

Result: 微调后的大型语言模型能够捕获生物化学知识。多任务学习通过利用共享的酶信息增强了正向合成和逆合成的预测能力。研究也发现了模型的局限性，例如在分层EC分类方案中面临挑战。

Conclusion: 大型语言模型在生物化学建模方面具有潜力，多任务学习策略尤其有效，但在处理如EC号预测等复杂分层任务方面仍需进一步改进。

Abstract: Predicting enzymatic reactions is crucial for applications in biocatalysis,
metabolic engineering, and drug discovery, yet it remains a complex and
resource-intensive task. Large Language Models (LLMs) have recently
demonstrated remarkable success in various scientific domains, e.g., through
their ability to generalize knowledge, reason over complex structures, and
leverage in-context learning strategies. In this study, we systematically
evaluate the capability of LLMs, particularly the Llama-3.1 family (8B and
70B), across three core biochemical tasks: Enzyme Commission number prediction,
forward synthesis, and retrosynthesis. We compare single-task and multitask
learning strategies, employing parameter-efficient fine-tuning via LoRA
adapters. Additionally, we assess performance across different data regimes to
explore their adaptability in low-data settings. Our results demonstrate that
fine-tuned LLMs capture biochemical knowledge, with multitask learning
enhancing forward- and retrosynthesis predictions by leveraging shared
enzymatic information. We also identify key limitations, for example challenges
in hierarchical EC classification schemes, highlighting areas for further
improvement in LLM-driven biochemical modeling.

</details>


### [18] [Prompted Meta-Learning for Few-shot Knowledge Graph Completion](https://arxiv.org/abs/2505.05684)
*Han Wu,Jie Yin*

Main category: cs.AI

TL;DR: 提出了一种名为PromptMeta的新型提示元学习框架，通过整合元语义与关系信息来提升小样本知识图谱补全的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的小样本知识图谱补全方法大多只关注关系信息，忽略了知识图谱中丰富的语义信息，特别是在新知识出现且数据有限的情况下。

Method: 提出了PromptMeta框架，包含两大创新：1) 元语义提示池，用于捕获和整合高级元语义，以适应稀有和新兴关系；2) 可学习的融合提示，动态结合元语义信息与任务特定的关系信息。两者在元学习框架下共同优化。

Result: 在两个基准数据集上的大量实验表明，该方法是有效的。

Conclusion: PromptMeta框架通过有效整合元语义和关系信息，为小样本知识图谱补全提供了一种新的有效途径。

Abstract: Few-shot knowledge graph completion (KGC) has obtained significant attention
due to its practical applications in real-world scenarios, where new knowledge
often emerges with limited available data. While most existing methods for
few-shot KGC have predominantly focused on leveraging relational information,
rich semantics inherent in KGs have been largely overlooked. To address this
gap, we propose a novel prompted meta-learning (PromptMeta) framework that
seamlessly integrates meta-semantics with relational information for few-shot
KGC. PrompMeta has two key innovations: (1) a meta-semantic prompt pool that
captures and consolidates high-level meta-semantics, enabling effective
knowledge transfer and adaptation to rare and newly emerging relations. (2) a
learnable fusion prompt that dynamically combines meta-semantic information
with task-specific relational information tailored to different few-shot tasks.
Both components are optimized together with model parameters within a
meta-learning framework. Extensive experiments on two benchmark datasets
demonstrate the effectiveness of our approach.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [19] [Continuous Thought Machines](https://arxiv.org/abs/2505.05522)
*Luke Darlow,Ciaran Regan,Sebastian Risi,Jeffrey Seely,Llion Jones*

Main category: cs.LG

TL;DR: 提出了一种名为连续思维机 (CTM) 的新模型，通过引入神经元级的时间处理和神经同步来模拟大脑的时间动态，以实现更具生物合理性的人工智能。


<details>
  <summary>Details</summary>
Motivation: 当前的深度学习架构大多简化了神经活动，忽略了生物大脑中对于信息处理至关重要的神经元时序和相互作用。

Method: 提出了连续思维机 (CTM)，其核心创新是：1) 神经元级时间处理，每个神经元使用独特权重处理输入信号历史；2) 使用神经同步作为潜在表示。

Result: CTM 在图像分类、迷宫求解、排序、奇偶校验、问答和强化学习等多种任务上表现出强大的性能和通用性，展示了丰富的内部表示，易于解释，并能进行复杂顺序推理和自适应计算。

Conclusion: CTM 是朝着开发更具生物合理性和更强大的人工智能系统迈出的重要一步，其目标是分享模型及其创新，而非追求当前最优结果。

Abstract: Biological brains demonstrate complex neural activity, where the timing and
interplay between neurons is critical to how brains process information. Most
deep learning architectures simplify neural activity by abstracting away
temporal dynamics. In this paper we challenge that paradigm. By incorporating
neuron-level processing and synchronization, we can effectively reintroduce
neural timing as a foundational element. We present the Continuous Thought
Machine (CTM), a model designed to leverage neural dynamics as its core
representation. The CTM has two core innovations: (1) neuron-level temporal
processing, where each neuron uses unique weight parameters to process a
history of incoming signals; and (2) neural synchronization employed as a
latent representation. The CTM aims to strike a balance between oversimplified
neuron abstractions that improve computational efficiency, and biological
realism. It operates at a level of abstraction that effectively captures
essential temporal dynamics while remaining computationally tractable for deep
learning. We demonstrate the CTM's strong performance and versatility across a
range of challenging tasks, including ImageNet-1K classification, solving 2D
mazes, sorting, parity computation, question-answering, and RL tasks. Beyond
displaying rich internal representations and offering a natural avenue for
interpretation owing to its internal process, the CTM is able to perform tasks
that require complex sequential reasoning. The CTM can also leverage adaptive
compute, where it can stop earlier for simpler tasks, or keep computing when
faced with more challenging instances. The goal of this work is to share the
CTM and its associated innovations, rather than pushing for new
state-of-the-art results. To that end, we believe the CTM represents a
significant step toward developing more biologically plausible and powerful
artificial intelligence systems.

</details>


### [20] [A critical assessment of reinforcement learning methods for microswimmer navigation in complex flows](https://arxiv.org/abs/2505.05525)
*Selim Mecanna,Aurore Loisy,Christophe Eloy*

Main category: cs.LG

TL;DR: 评估了强化学习在流体导航中的应用，发现常用算法（Q学习、A2C）性能不佳，而PPO算法结合优化技术能达到接近理论最优的鲁棒导航效果。


<details>
  <summary>Details</summary>
Motivation: 浮游生物或水下机器人在流体中仅靠自身传感器导航是一个常见问题。近年来流体力学界广泛使用强化学习解决此问题，但常用算法的有效性并不明确，需要定量评估。

Method: 首先定义了一个具有已知准最优解的定向导航问题。然后，在多种常见流场（泰勒-格林涡、ABC流、二维湍流）中测试了常用强化学习算法（Q学习、A2C）和更先进的PPO算法的性能和鲁棒性。研究还特别实现并优化了PPO算法，结合了向量化环境、广义优势估计（GAE）和超参数调优。

Result: 常用的Q学习和A2C算法在测试流场中表现不佳且鲁棒性差。相比之下，PPO算法性能显著更优。经过特别实现和优化的PPO，在湍流中能够稳健地达到接近理论最优的导航性能。

Conclusion: 对于在复杂流场中发现真正有效的自主导航策略而言，选择合适的强化学习算法、关注实施细节以及进行精细调优至关重要。PPO是比传统简单算法更有效的选择。

Abstract: Navigating in a fluid flow while being carried by it, using only information
accessible from on-board sensors, is a problem commonly faced by small
planktonic organisms. It is also directly relevant to autonomous robots
deployed in the oceans. In the last ten years, the fluid mechanics community
has widely adopted reinforcement learning, often in the form of its simplest
implementations, to address this challenge. But it is unclear how good are the
strategies learned by these algorithms. In this paper, we perform a
quantitative assessment of reinforcement learning methods applied to navigation
in partially observable flows. We first introduce a well-posed problem of
directional navigation for which a quasi-optimal policy is known analytically.
We then report on the poor performance and robustness of commonly used
algorithms (Q-Learning, Advantage Actor Critic) in flows regularly encountered
in the literature: Taylor-Green vortices, Arnold-Beltrami-Childress flow, and
two-dimensional turbulence. We show that they are vastly surpassed by PPO
(Proximal Policy Optimization), a more advanced algorithm that has established
dominance across a wide range of benchmarks in the reinforcement learning
community. In particular, our custom implementation of PPO matches the
theoretical quasi-optimal performance in turbulent flow and does so in a robust
manner. Reaching this result required the use of several additional techniques,
such as vectorized environments and generalized advantage estimation, as well
as hyperparameter optimization. This study demonstrates the importance of
algorithm selection, implementation details, and fine-tuning for discovering
truly smart autonomous navigation strategies in complex flows.

</details>


### [21] [ADMM-Based Training for Spiking Neural Networks](https://arxiv.org/abs/2505.05527)
*Giovanni Perin,Cesare Bidini,Riccardo Mazzieri,Michele Rossi*

Main category: cs.LG

TL;DR: 提出了一种基于ADMM的新型脉冲神经网络（SNN）训练方法，以解决现有方法（如替代梯度反向传播）的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的SNN训练算法（如基于替代梯度的反向传播）存在可扩展性低和数值不精确的问题，且SNN的阶跃函数不可微给训练带来困难。

Method: 提出基于乘子交替方向法（ADMM）的SNN训练方法。该方法将训练表述为一个优化问题，并推导出闭式更新规则，以解决SNN阶跃函数的不可微性问题。

Result: 在模拟的概念验证中，经验证了该ADMM优化器具有良好的收敛特性和巨大的应用潜力。

Conclusion: 基于ADMM的训练方法为SNN提供了一种有前景的优化途径，解决了不可微性挑战，并为未来改进该方法开辟了新的研究方向。

Abstract: In recent years, spiking neural networks (SNNs) have gained momentum due to
their high potential in time-series processing combined with minimal energy
consumption. However, they still lack a dedicated and efficient training
algorithm. The popular backpropagation with surrogate gradients, adapted from
stochastic gradient descent (SGD)-derived algorithms, has several drawbacks
when used as an optimizer for SNNs. Specifically, it suffers from low
scalability and numerical imprecision. In this paper, we propose a novel SNN
training method based on the alternating direction method of multipliers
(ADMM). Our ADMM-based training aims to solve the problem of the SNN step
function's non-differentiability. We formulate the problem, derive closed-form
updates, and empirically show the optimizer's convergence properties, great
potential, and possible new research directions to improve the method in a
simulated proof-of-concept.

</details>


### [22] [Low-bit Model Quantization for Deep Neural Networks: A Survey](https://arxiv.org/abs/2505.05530)
*Kai Liu,Qian Zheng,Kaiwen Tao,Zhiteng Li,Haotong Qin,Wenbo Li,Yong Guo,Xianglong Liu,Linghe Kong,Guihai Chen,Yulun Zhang,Xiaokang Yang*

Main category: cs.LG

TL;DR: 这篇综述总结了近五年深度神经网络（DNN）低比特量化技术的进展，对现有方法进行了分类，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: DNN计算量大、模型尺寸大，难以在实际中部署。模型量化可以有效减小模型并加速计算，但会导致精度损失，因此研究如何有效量化并补偿信息损失变得十分重要。

Method: 本文通过调研过去五年的文献，讨论和比较了最先进的DNN量化方法，并根据其核心技术将其分为8个主要类别和24个子类别。

Result: 提供了对近期DNN量化方法的全面概述和分类，比较了不同方法的优劣，并整理了一个相关的资源列表。

Conclusion: 模型量化是DNN部署流程中不可或缺的一步。本综述系统地梳理了该领域的最新进展和方法分类，并指出了未来可能的研究机会。

Abstract: With unprecedented rapid development, deep neural networks (DNNs) have deeply
influenced almost all fields. However, their heavy computation costs and model
sizes are usually unacceptable in real-world deployment. Model quantization, an
effective weight-lighting technique, has become an indispensable procedure in
the whole deployment pipeline. The essence of quantization acceleration is the
conversion from continuous floating-point numbers to discrete integer ones,
which significantly speeds up the memory I/O and calculation, i.e., addition
and multiplication. However, performance degradation also comes with the
conversion because of the loss of precision. Therefore, it has become
increasingly popular and critical to investigate how to perform the conversion
and how to compensate for the information loss. This article surveys the recent
five-year progress towards low-bit quantization on DNNs. We discuss and compare
the state-of-the-art quantization methods and classify them into 8 main
categories and 24 sub-categories according to their core techniques.
Furthermore, we shed light on the potential research opportunities in the field
of model quantization. A curated list of model quantization is provided at
https://github.com/Kai-Liu001/Awesome-Model-Quantization.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [23] [KPI Poisoning: An Attack in Open RAN Near Real-Time Control Loop](https://arxiv.org/abs/2505.05537)
*Hamed Alimohammadi,Sotiris Chatzimiltis,Samara Mayhoub,Mohammad Shojafar,Seyed Ahmad Soleymani,Ayhan Akbas,Chuan Heng Foh*

Main category: cs.NI

TL;DR: 本文研究了Open RAN中的一种新型KPI投毒攻击，并提出了一种基于LSTM的机器学习方法来检测此攻击，结果显示该方法有效。


<details>
  <summary>Details</summary>
Motivation: Open RAN的新特性在带来灵活性和互操作性的同时，也引入了新的安全威胁，特别是针对近实时控制环路中关键性能指标 (KPI) 的投毒攻击，这种攻击可能源于E2接口流量欺骗或受损E2节点，对网络功能构成显著威胁。

Method: 提出了一种基于机器学习的方法，使用长短期记忆 (LSTM) 神经网络模型，通过仿真生成KPI报告并注入异常值，来检测控制环路中使用前的被投毒KPI值。

Result: 实验结果显示，注入的异常值越明显，越容易被检测到；使用更多的历史KPI报告序列进行训练和检测，能够显著提高异常检测性能，检测率从62%提升至99%。

Conclusion: 研究证明，所提出的基于LSTM的机器学习方法能有效检测Open RAN中近实时控制环路的KPI投毒攻击，其检测效果受异常强度和所用数据序列长度的影响，为增强Open RAN安全性提供了可行的检测机制。

Abstract: Open Radio Access Network (Open RAN) is a new paradigm to provide fundamental
features for supporting next-generation mobile networks. Disaggregation,
virtualisation, closed-loop data-driven control, and open interfaces bring
flexibility and interoperability to the network deployment. However, these
features also create a new surface for security threats. In this paper, we
introduce Key Performance Indicators (KPIs) poisoning attack in Near Real-Time
control loops as a new form of threat that can have significant effects on the
Open RAN functionality. This threat can arise from traffic spoofing on the E2
interface or compromised E2 nodes. The role of KPIs is explored in the use
cases of Near Real-Time control loops. Then, the potential impacts of the
attack are analysed. An ML-based approach is proposed to detect poisoned KPI
values before using them in control loops. Emulations are conducted to generate
KPI reports and inject anomalies into the values. A Long Short-Term Memory
(LSTM) neural network model is used to detect anomalies. The results show that
more amplified injected values are more accessible to detect, and using more
report sequences leads to better performance in anomaly detection, with
detection rates improving from 62% to 99%.

</details>


### [24] [P4Kube: In-Network Load Balancer for Kubernetes](https://arxiv.org/abs/2505.05996)
*Garegin Grigoryan,Kevin Penkowski,Minseok Kwon*

Main category: cs.NI

TL;DR: P4Kube是一个结合P4数据平面和Kubernetes插件的框架，通过优化负载均衡和绕过系统控制平面，提高了Kubernetes服务的请求响应速度。


<details>
  <summary>Details</summary>
Motivation: 当前的Kubernetes服务暴露机制（LoadBalancer和NodePort）要么需要外部负载均衡器，要么会增加网络延迟，效率不高。

Method: 提出了P4Kube框架，包含一个P4数据平面程序和一个Kubernetes插件。它根据运行的副本数量进行负载均衡，数据包完全绕过系统控制平面，P4数据平面直接与Kubernetes控制平面通信以更新状态，无需网络控制平面的介入。

Result: 实验显示，与传统方法相比，P4Kube能够将到集群的平均请求时间减少高达50%。

Conclusion: P4Kube提供了一种更高效的Kubernetes服务负载均衡解决方案，通过直接集成P4数据平面和Kubernetes控制平面显著提高了性能。

Abstract: Kubernetes Services such as LoadBalancer and NodePort expose applications
running on pods within a Kubernetes cluster to external users. While the
LoadBalancer Service requires an external load-balancing middleware, its
alternative, NodePort Service, adds additional hops on the path between clients
and the worker nodes. In this paper, we propose P4Kube, a framework consisting
of a P4 data plane program and a Kubernetes plugin. Our solution effectively
performs load balancing of requests to the worker nodes of a cluster based on
the number of running replicas. In P4Kube, the data packets completely bypass
the system's control plane. Unlike the previous work, to update its state, the
P4Kube data plane works directly with the Kubernetes control plane without any
involvement of the network control plane. Our experiments show up to 50%
improvement in the average request time to the cluster compared to conventional
approaches.

</details>


### [25] [Efficient Information Updates in Compute-First Networking via Reinforcement Learning with Joint AoI and VoI](https://arxiv.org/abs/2505.06025)
*Jianpeng Qi,Chao Liu,Chengxiang Xu,Rui Wang,Junyu Dong,Yanwei Yu*

Main category: cs.NI

TL;DR: 本文提出一种年龄和价值感知 (AVA) 指标及基于强化学习的更新策略，以在计算优先网络中高效分发服务信息，显著减少更新频率而不影响任务准确性。


<details>
  <summary>Details</summary>
Motivation: 在用户请求动态且计算资源受限的计算优先网络系统中，服务信息的及时高效分发至关重要。传统基于新鲜度的指标未能充分考虑服务容量变化和接入点转发决策，导致更新效率不高。

Method: 1. 提出一种年龄和价值感知 (AVA) 指标，该指标结合了服务信息的及时性和任务相关性，并考虑了服务器端服务能力和接入点 (AP) 转发决策的变化。2. 基于 AVA 指标，开发了一种基于强化学习的更新策略，该策略学习选择性地向 AP 传输服务信息更新，旨在最大化任务成功率同时最小化不必要的通信。

Result: 广泛的仿真实验表明，与基准方法相比，AVA 方法平均减少了超过 90% 的更新频率，在某些配置下最高可达 98%。关键在于，这种更新频率的降低并未牺牲任务执行的准确性或决策质量。

Conclusion: AVA 指标和相关的强化学习更新策略能够显著提升计算优先网络中服务信息分发的效率，通过大幅减少不必要的通信来优化系统性能，同时保持高任务成功率和决策质量。

Abstract: Timely and efficient dissemination of service information is critical in
compute-first networking systems, where user requests arrive dynamically and
computing resources are constrained. In such systems, the access point (AP)
plays a key role in forwarding user requests to a server based on its latest
received service information. This paper considers a single-source,
single-destination system and introduces an Age-and-Value-Aware (AVA) metric
that jointly captures both the timeliness and the task relevance of service
information. Unlike traditional freshness-based metrics, AVA explicitly
incorporates variations in server-side service capacity and AP forwarding
decisions, allowing more context-aware update evaluation. Building upon AVA, we
propose a reinforcement learning-based update policy that learns to selectively
transmit service information updates to the AP. It aims to maximize overall
task success while minimizing unnecessary communications. Extensive simulations
under diverse user request patterns and varying service capacities demonstrate
that AVA reduces the update frequency by over 90% on average compared to
baselines, with reductions reaching 98% in certain configurations. Crucially,
this reduction is achieved without compromising the accuracy of task execution
or the quality of decision making.

</details>


### [26] [Extending the Control Plane of Container Orchestrators for I/O Virtualization](https://arxiv.org/abs/2505.06041)
*Garegin Grigoryan,Minseok Kwon,M. Mustafa Rafique*

Main category: cs.NI

TL;DR: 论文提出ConRDMA架构，用于对容器的SR-IOV RDMA虚拟化进行精细控制，以提高带宽利用效率和节点选择。


<details>
  <summary>Details</summary>
Motivation: 标准的SR-IOV技术在将虚拟化网络设备分配给容器时，缺乏基于应用带宽需求的配置能力，并且容器编排器对虚拟化接口的网络控制受限于SR-IOV本身的能力。

Method: 探索了受控SR-IOV虚拟化系统的设计考量，并提出了一种名为ConRDMA的新颖架构，该架构能够对容器的RDMA虚拟化实现精细控制。

Result: 评估表明，ConRDMA使容器能够更有效地使用分配的RDMA带宽，并且能够根据其不同的通信需求选择最合适的节点。

Conclusion: ConRDMA提供了一种有效的架构，实现了对容器环境中RDMA虚拟化的精细控制，从而优化了带宽使用和资源分配。

Abstract: Single Root Input/Output Virtualization (SR-IOV) is a standard technology for
forking a single PCI express device and providing it to applications while
ensuring performance isolation. It enables container orchestrators to share a
limited number of physical network interfaces without incurring significant
virtualization overhead. The allocation of virtualized network devices to
containers, however, needs to be more configurable based on the bandwidth needs
of running applications. Moreover, container orchestrators' network control
over the virtualized interfaces is limited by the abilities of SR-IOV. We
explore the design considerations for a system with controlled SR-IOV
virtualization and present ConRDMA, a novel architecture that enables fine
control of RDMA virtualization for containers. Our evaluation shows that
ConRDMA enables containers to use RDMA allocated bandwidth more efficiently and
to select best-suited nodes to meet their varying communication requirements.

</details>
