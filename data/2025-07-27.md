<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.CV](#cs.CV) [Total: 75]
- [cs.AI](#cs.AI) [Total: 21]
- [cs.LG](#cs.LG) [Total: 62]
- [cs.NI](#cs.NI) [Total: 6]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.HC](#cs.HC) [Total: 7]
- [math.LO](#math.LO) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.IT](#cs.IT) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning](https://arxiv.org/abs/2507.17842)
*Yimeng Zhang,Tian Wang,Jiri Gesi,Ziyi Wang,Yuxuan Lu,Jiacheng Lin,Sinong Zhan,Vianne Gao,Ruochen Jiao,Junze Liu,Kun Qian,Yuxin Tang,Ran Xue,Houyu Zhang,Qingjun Cui,Yufan Guo,Dakuo Wang*

Main category: cs.CL

TL;DR: 本文提出Shop-R1，一个强化学习框架，旨在通过两阶段（理由生成和动作预测）和分层奖励机制，显著提升大型语言模型在在线购物环境中模拟人类行为的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有利用大型语言模型（LLMs）合成理由并进行微调以增强推理能力的方法，其表现受限于生成理由的模型本身的推理能力。因此，需要新的方法来提升LLMs在模拟在线购物环境中真实人类行为的推理能力。

Method: 引入Shop-R1，一个新型强化学习（RL）框架。它将人类行为模拟任务分解为理由生成和动作预测两个阶段。理由生成阶段利用内部模型信号（如logit分布）进行自监督指导。动作预测阶段采用分层奖励结构，通过难度感知缩放防止奖励欺骗，并根据高层动作类型和细粒度子动作细节（属性和值）的正确性按难度比例分配奖励。

Result: 实验结果表明，与基线方法相比，该方法实现了超过65%的相对提升。

Conclusion: Shop-R1框架通过其独特的两阶段设计和分层奖励机制，显著提高了大型语言模型在模拟在线购物环境中人类行为的推理能力。

Abstract: Large Language Models (LLMs) have recently demonstrated strong potential in
generating 'believable human-like' behavior in web environments. Prior work has
explored augmenting training data with LLM-synthesized rationales and applying
supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can
improve downstream action prediction. However, the performance of such
approaches remains inherently bounded by the reasoning capabilities of the
model used to generate the rationales. In this paper, we introduce Shop-R1, a
novel reinforcement learning (RL) framework aimed at enhancing the reasoning
ability of LLMs for simulation of real human behavior in online shopping
environments Specifically, Shop-R1 decomposes the human behavior simulation
task into two stages: rationale generation and action prediction, each guided
by distinct reward signals. For rationale generation, we leverage internal
model signals (e.g., logit distributions) to guide the reasoning process in a
self-supervised manner. For action prediction, we propose a hierarchical reward
structure with difficulty-aware scaling to prevent reward hacking and enable
fine-grained reward assignment. This design evaluates both high-level action
types and the correctness of fine-grained sub-action details (attributes and
values), rewarding outputs proportionally to their difficulty. Experimental
results show that our method achieves a relative improvement of over 65%
compared to the baseline.

</details>


### [2] [Dynamic and Generalizable Process Reward Modeling](https://arxiv.org/abs/2507.17849)
*Zhangyue Yin,Qiushi Sun,Zhiyuan Zeng,Qinyuan Cheng,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本文提出动态可泛化过程奖励建模（DG-PRM），以解决现有过程奖励模型（PRMs）在跨领域泛化性差和评估标准僵化的问题。DG-PRM通过引入奖励树和帕累托支配估计，提供细粒度、动态的奖励信号，显著提升了大型语言模型（LLMs）的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的过程奖励模型（PRMs）主要依赖启发式方法，导致跨领域泛化能力不足。LLM作为评判者的方法虽然被提出提供泛化奖励，但仅关注反馈结果，忽略了文本中蕴含的丰富过程指导信息。此外，静态和粗粒度的评估标准难以适应复杂的逐过程监督需求。

Method: 本文提出了动态可泛化过程奖励建模（DG-PRM）。该方法利用“奖励树”来捕获并存储细粒度、多维度的奖励标准，并能动态选择奖励信号进行逐步评分。为了有效处理多方面奖励信号，开创性地引入帕累托支配估计来识别具有判别性的正负样本对。

Result: 实验结果显示，DG-PRM在主流基准测试上取得了惊人的性能表现，显著提升了LLMs在密集奖励任务中的模型性能。进一步分析表明，DG-PRM能够很好地适应分布外（out-of-distribution）场景。

Conclusion: DG-PRM不仅在性能上表现出色，而且在面对未知分布场景时展现出卓越的泛化能力，为LLMs的复杂任务指导提供了有效的解决方案。

Abstract: Process Reward Models (PRMs) are crucial for guiding Large Language Models
(LLMs) in complex scenarios by providing dense reward signals. However,
existing PRMs primarily rely on heuristic approaches, which struggle with
cross-domain generalization. While LLM-as-judge has been proposed to provide
generalized rewards, current research has focused mainly on feedback results,
overlooking the meaningful guidance embedded within the text. Additionally,
static and coarse-grained evaluation criteria struggle to adapt to complex
process supervision. To tackle these challenges, we propose Dynamic and
Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to
capture and store fine-grained, multi-dimensional reward criteria. DG-PRM
dynamically selects reward signals for step-wise reward scoring. To handle
multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation
to identify discriminative positive and negative pairs. Experimental results
show that DG-PRM achieves stunning performance on prevailing benchmarks,
significantly boosting model performance across tasks with dense rewards.
Further analysis reveals that DG-PRM adapts well to out-of-distribution
scenarios, demonstrating exceptional generalizability.

</details>


### [3] [VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL](https://arxiv.org/abs/2507.17896)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.CL

TL;DR: VeriMinder是一个交互式系统，旨在帮助自然语言数据库接口用户识别和减轻数据分析中的认知偏差，通过引入上下文语义映射、Hard-to-Vary原则和优化的LLM提示生成机制，显著提高了分析质量和准确性。


<details>
  <summary>Details</summary>
Motivation: 自然语言数据库接口（NLIDBs）使数据分析大众化，但非统计背景用户在提出无偏见分析问题时面临挑战。尽管现有研究侧重于文本转SQL的准确性，但解决分析问题中的认知偏差问题仍未得到充分探索。

Method: 开发了VeriMinder系统，采用三项创新：1) 基于分析上下文的偏差上下文语义映射框架；2) 运用“Hard-to-Vary”原则指导系统数据分析的分析框架；3) 优化LLM驱动的系统，通过多候选、批评反馈和自我反思的结构化过程生成高质量、任务特定的提示。

Result: 用户测试证实了其优势：82.5%的参与者表示对分析质量有积极影响；在对比评估中，VeriMinder在分析的具体性、全面性和准确性指标上比替代方法高出至少20%。

Conclusion: VeriMinder系统（已实现为Web应用并开源）有望帮助用户在数据分析中避免“错误问题”的漏洞，促进社区内的进一步研究和应用。

Abstract: Application systems using natural language interfaces to databases (NLIDBs)
have democratized data analysis. This positive development has also brought
forth an urgent challenge to help users who might use these systems without a
background in statistical analysis to formulate bias-free analytical questions.
Although significant research has focused on text-to-SQL generation accuracy,
addressing cognitive biases in analytical questions remains underexplored. We
present VeriMinder, https://veriminder.ai, an interactive system for detecting
and mitigating such analytical vulnerabilities. Our approach introduces three
key innovations: (1) a contextual semantic mapping framework for biases
relevant to specific analysis contexts (2) an analytical framework that
operationalizes the Hard-to-Vary principle and guides users in systematic data
analysis (3) an optimized LLM-powered system that generates high-quality,
task-specific prompts using a structured process involving multiple candidates,
critic feedback, and self-reflection.
  User testing confirms the merits of our approach. In direct user experience
evaluation, 82.5% participants reported positively impacting the quality of the
analysis. In comparative evaluation, VeriMinder scored significantly higher
than alternative approaches, at least 20% better when considered for metrics of
the analysis's concreteness, comprehensiveness, and accuracy. Our system,
implemented as a web application, is set to help users avoid "wrong question"
vulnerability during data analysis. VeriMinder code base with prompts,
https://reproducibility.link/veriminder, is available as an MIT-licensed
open-source software to facilitate further research and adoption within the
community.

</details>


### [4] [One Whisper to Grade Them All](https://arxiv.org/abs/2507.17918)
*Nhan Phan,Anusha Porwal,Yaroslav Getman,Ekaterina Voskoboinik,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

TL;DR: 提出了一种高效的端到端自动口语评估（ASA）方法，通过单个Whisper-small编码器处理多部分测试，实现高精度和资源效率，并引入数据采样策略以提升数据效率。


<details>
  <summary>Details</summary>
Motivation: 旨在为多部分第二语言测试提供一个实用、高效且整体的自动口语评估（ASA）系统，以适应大规模计算机辅助语言学习（CALL）系统，并克服现有方法对转录和分部分模型的依赖，同时缩短推理时间。

Method: 开发了一种端到端方法，核心创新在于使用单个Whisper-small编码器处理所有四个口语回答，并通过轻量级聚合器整合信息以预测最终分数。此外，引入了一种数据采样策略，以提高不平衡类别上的性能和数据效率。

Result: 系统在Root Mean Squared Error (RMSE)上达到0.384，优于基于文本的基线（0.44）。参数量最多为1.68亿（约占Whisper-small的70%），显示出高资源效率。提出的数据采样策略使模型仅使用44.8%的语料库说话者进行训练，仍能达到0.383的RMSE，证明了在不平衡类别上的性能提升和强大的数据效率。

Conclusion: 该端到端ASA系统为大规模第二语言口语评估提供了一个高效、准确且实用的解决方案。其新颖的架构和数据采样策略显著降低了资源需求并提高了数据效率，同时保持或提升了性能。

Abstract: We present an efficient end-to-end approach for holistic Automatic Speaking
Assessment (ASA) of multi-part second-language tests, developed for the 2025
Speak & Improve Challenge. Our system's main novelty is the ability to process
all four spoken responses with a single Whisper-small encoder, combine all
information via a lightweight aggregator, and predict the final score. This
architecture removes the need for transcription and per-part models, cuts
inference time, and makes ASA practical for large-scale Computer-Assisted
Language Learning systems.
  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming
the text-based baseline (0.44) while using at most 168M parameters (about 70%
of Whisper-small). Furthermore, we propose a data sampling strategy, allowing
the model to train on only 44.8% of the speakers in the corpus and still reach
0.383 RMSE, demonstrating improved performance on imbalanced classes and strong
data efficiency.

</details>


### [5] [Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text](https://arxiv.org/abs/2507.17944)
*Hulayyil Alshammari,Praveen Rao*

Main category: cs.CL

TL;DR: 评估了六种AI检测工具识别DeepSeek生成文本的能力及其在对抗性攻击下的表现，并探索了DeepSeek自身作为检测器的潜力。结果显示现有工具易受“人性化”攻击影响，而DeepSeek结合少样本和CoT推理在检测上表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的普及，AI生成文本的检测成为关键。现有研究主要关注ChatGPT等常见LLM，对新兴模型DeepSeek的检测能力及其在对抗性攻击（如改写和人性化处理）下的鲁棒性存在研究空白。

Method: 本研究选用AI Text Classifier、Content Detector AI、Copyleaks、QuillBot、GPT-2、GPTZero六种AI检测工具。收集了49对人工问答，并用DeepSeek-v3生成对应AI文本。通过标准改写和“人性化”改写生成额外样本以测试检测器鲁棒性。此外，还探索了DeepSeek本身作为检测器，通过少样本提示和思维链（CoT）推理进行AI/人类文本分类。

Result: QuillBot和Copyleaks在原始及改写DeepSeek文本上表现优秀，而AI Text Classifier和GPT-2等工具结果不一致。“人性化”改写是最有效的攻击，显著降低了检测器准确率（Copyleaks降至71%，QuillBot降至58%，GPTZero降至52%）。DeepSeek结合少样本和CoT推理的检测方法表现出高准确率，最佳五样本仅误分类一个样本（AI召回率96%，人类召回率100%）。

Conclusion: 当前AI检测工具在识别DeepSeek生成文本，特别是在面对“人性化”等对抗性攻击时，表现出局限性。而LLM自身（如DeepSeek）结合先进提示工程技术（如少样本和CoT推理）在区分AI和人类文本方面显示出强大潜力。

Abstract: Large language models (LLMs) have rapidly transformed the creation of written
materials. LLMs have led to questions about writing integrity, thereby driving
the creation of artificial intelligence (AI) detection technologies.
Adversarial attacks, such as standard and humanized paraphrasing, inhibit
detectors' ability to detect machine-generated text. Previous studies have
mainly focused on ChatGPT and other well-known LLMs and have shown varying
accuracy across detectors. However, there is a clear gap in the literature
about DeepSeek, a recently published LLM. Therefore, in this work, we
investigate whether six generally accessible AI detection tools -- AI Text
Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can
consistently recognize text generated by DeepSeek. The detectors were exposed
to the aforementioned adversarial attacks. We also considered DeepSeek as a
detector by performing few-shot prompting and chain-of-thought reasoning (CoT)
for classifying AI and human-written text. We collected 49 human-authored
question-answer pairs from before the LLM era and generated matching responses
using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied
adversarial techniques such as paraphrasing and humanizing to add 196 more
samples. These were used to challenge detector robustness and assess accuracy
impact. While QuillBot and Copyleaks showed near-perfect performance on
original and paraphrased DeepSeek text, others -- particularly AI Text
Classifier and GPT-2 -- showed inconsistent results. The most effective attack
was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and
52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best
five-shot result misclassifying only one of 49 samples (AI recall 96%, human
recall 100%).

</details>


### [6] [Are LLM Belief Updates Consistent with Bayes' Theorem?](https://arxiv.org/abs/2507.17951)
*Sohaib Imran,Ihor Kendiukhov,Matthew Broerman,Aditya Thomas,Riccardo Campanella,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: 研究表明，更大、能力更强的预训练语言模型在结合上下文证据时，其“信念”更新与贝叶斯定理更一致。


<details>
  <summary>Details</summary>
Motivation: 探讨大型、高能力语言模型在给定上下文证据时，是否能更一致地按照贝叶斯定理更新其对命题的“信念”。

Method: 提出了贝叶斯一致性系数（BCC）指标，并生成了一个用于衡量BCC的数据集。测量了五个模型家族中多个预训练语言模型的BCC，并与模型参数数量、训练数据量以及在常见基准上的得分进行了比较。

Result: 研究结果支持了假设，即更大、能力更强的预训练语言模型赋予的置信度与贝叶斯定理更一致。

Conclusion: 这些发现对于理解和管理大型语言模型具有重要意义。

Abstract: Do larger and more capable language models learn to update their "beliefs"
about propositions more consistently with Bayes' theorem when presented with
evidence in-context? To test this, we formulate a Bayesian Coherence
Coefficient (BCC) metric and generate a dataset with which to measure the BCC.
We measure BCC for multiple pre-trained-only language models across five model
families, comparing against the number of model parameters, the amount of
training data, and model scores on common benchmarks. Our results provide
evidence for our hypothesis that larger and more capable pre-trained language
models assign credences that are more coherent with Bayes' theorem. These
results have important implications for our understanding and governance of
LLMs.

</details>


### [7] [Natural Language Processing for Tigrinya: Current State and Future Directions](https://arxiv.org/abs/2507.17974)
*Fitsum Gaim,Jong C. Park*

Main category: cs.CL

TL;DR: 本文对提格利尼亚语（Tigrinya）的自然语言处理（NLP）研究进行了全面调查，分析了现状、挑战和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 提格利尼亚语虽有数百万使用者，但在自然语言处理（NLP）研究领域仍严重不足，急需对其研究现状进行系统梳理和推动。

Method: 通过对2011年至2025年间超过40项提格利尼亚语NLP研究进行全面调查，系统回顾了计算资源、模型和应用在形态处理、机器翻译、语音识别和问答等十个下游任务中的发展现状。

Result: 分析显示，提格利尼亚语NLP的研究轨迹从基于规则的基础系统发展到现代神经网络架构，进步与资源创建里程碑紧密相关。主要挑战在于该语言的形态复杂性和资源稀缺性。

Conclusion: 本研究为提格利尼亚语NLP研究提供了全面的参考和发展路线图，指出了有前景的研究方向，包括形态感知建模、跨语言迁移和以社区为中心的资源开发。

Abstract: Despite being spoken by millions of people, Tigrinya remains severely
underrepresented in Natural Language Processing (NLP) research. This work
presents a comprehensive survey of NLP research for Tigrinya, analyzing over 40
studies spanning more than a decade of work from 2011 to 2025. We
systematically review the current state of computational resources, models, and
applications across ten distinct downstream tasks, including morphological
processing, machine translation, speech recognition, and question-answering.
Our analysis reveals a clear trajectory from foundational, rule-based systems
to modern neural architectures, with progress consistently unlocked by resource
creation milestones. We identify key challenges rooted in Tigrinya's
morphological complexity and resource scarcity, while highlighting promising
research directions, including morphology-aware modeling, cross-lingual
transfer, and community-centered resource development. This work serves as both
a comprehensive reference for researchers and a roadmap for advancing Tigrinya
NLP. A curated metadata of the surveyed studies and resources is made publicly
available.\footnote{Tigrinya NLP Anthology:
https://github.com/fgaim/tigrinya-nlp-anthology.

</details>


### [8] [Technical Report of TeleChat2, TeleChat2.5 and T1](https://arxiv.org/abs/2507.18013)
*Zihan Wang,Xinzhang Liu,Yitong Yao,Chao Wang,Yu Zhao,Zhihao Yang,Wenmin Deng,Kaipeng Jia,Jiaxin Peng,Yuyao Huang,Sishi Xiong,Zhuo Jiang,Kaidong Yu,Xiaohui Hu,Fubei Yao,Ruiyu Fang,Zhuoru Jiang,Ruiting Song,Qiyi Xie,Rui Xue,Xuewei He,Yanlei Xue,Zhu Yuan,Zhaoxi Zhang,Zilu Huang,Shiquan Wang,Xin Wang,Hanming Wu,Mingyuan Wang,Xufeng Zhan,Yuhan Sun,Zhaohu Xing,Yuhao Jiang,Bingkai Yang,Shuangyong Song,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: 本文介绍了TeleChat模型的最新系列：TeleChat2、TeleChat2.5和T1。这些模型通过改进的预训练和后训练策略，在保持近似模型架构的情况下实现了显著的性能提升，尤其在复杂推理和通用任务上表现出色，其中T1-115B超越了部分专有模型，并且所有模型均已开源。


<details>
  <summary>Details</summary>
Motivation: 研究旨在显著提升TeleChat模型的性能，并在代码生成、数学推理等特定任务上增强其能力，同时为开发者和研究人员提供先进、适应多样应用的开源语言模型。

Method: 研究采用了增强的训练策略，包括在10万亿高质量tokens上进行预训练，并结合监督微调（SFT）和直接偏好优化（DPO）。对于TeleChat2.5和T1，还增加了基于领域特定数据集的持续预训练和强化学习（RL）。这些模型采用115B参数的密集Transformer架构，T1侧重于复杂推理（长CoT），TeleChat2.5侧重于推理速度。

Result: 新系列模型相比原版TeleChat实现了实质性的性能提升。T1在数学和编码等复杂推理任务上表现出显著改进。特别是T1-115B，其性能超越了OpenAI的o1-mini和GPT-4o等专有模型。TeleChat2.5则优先考虑速度，实现了快速推理。

Conclusion: TeleChat新系列模型（TeleChat2、2.5、T1）通过优化训练策略，实现了性能上的重大突破，尤其在复杂推理和通用任务上表现卓越。这些高性能模型已公开，为各类应用提供了强大的前沿语言模型。

Abstract: We introduce the latest series of TeleChat models: \textbf{TeleChat2},
\textbf{TeleChat2.5}, and \textbf{T1}, offering a significant upgrade over
their predecessor, TeleChat. Despite minimal changes to the model architecture,
the new series achieves substantial performance gains through enhanced training
strategies in both pre-training and post-training stages. The series begins
with \textbf{TeleChat2}, which undergoes pretraining on 10 trillion
high-quality and diverse tokens. This is followed by Supervised Fine-Tuning
(SFT) and Direct Preference Optimization (DPO) to further enhance its
capabilities. \textbf{TeleChat2.5} and \textbf{T1} expand the pipeline by
incorporating a continual pretraining phase with domain-specific datasets,
combined with reinforcement learning (RL) to improve performance in code
generation and mathematical reasoning tasks. The \textbf{T1} variant is
designed for complex reasoning, supporting long Chain-of-Thought (CoT)
reasoning and demonstrating substantial improvements in mathematics and coding.
In contrast, \textbf{TeleChat2.5} prioritizes speed, delivering rapid
inference. Both flagship models of \textbf{T1} and \textbf{TeleChat2.5} are
dense Transformer-based architectures with 115B parameters, showcasing
significant advancements in reasoning and general task performance compared to
the original TeleChat. Notably, \textbf{T1-115B} outperform proprietary models
such as OpenAI's o1-mini and GPT-4o. We publicly release \textbf{TeleChat2},
\textbf{TeleChat2.5} and \textbf{T1}, including post-trained versions with 35B
and 115B parameters, to empower developers and researchers with
state-of-the-art language models tailored for diverse applications.

</details>


### [9] [NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database](https://arxiv.org/abs/2507.18028)
*Weizhi Fei,Hao Shi,Jing Xu,Jingchen Peng,Jiazheng Li,Jingzhao Zhang,Bo Bai,Wei Han,Zhenyuan Chen,Xueyan Niu*

Main category: cs.CL

TL;DR: 本文提出NeuralDB，一种用于大规模编辑大语言模型知识的框架，它通过神经键值数据库和门控检索模块，解决了现有编辑方法在扩展时损害模型通用能力和遗忘已编辑事实的问题，并在10万条事实的规模上表现出卓越的性能和通用能力保持。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型知识编辑方法（如Locate-and-Edit, L&E）在处理大量事实修改时，会损害模型的通用能力，并可能导致已编辑事实的遗忘，因此需要一种更有效且能保持模型通用能力的编辑方案。

Method: 作者将现有线性L&E方法建模为对键值(KV)数据库的查询。在此基础上，提出了NeuralDB框架，它将编辑后的事实明确表示为一个神经KV数据库，并配备一个非线性门控检索模块。该门控模块只在推理涉及编辑过的事实时才激活，从而有效保留了LLMs的通用能力。

Result: 在ZSRE和CounterFacts数据集上，对GPT2-XL、GPT-J和Llama-3模型进行1万条事实的编辑实验表明，NeuralDB在编辑效率、泛化性、特异性、流畅性和一致性方面表现出色，并能保持六项代表性文本理解和生成任务的整体性能。进一步实验表明，NeuralDB在扩展到10万条事实（比现有工作多50倍）时仍保持其有效性。

Conclusion: NeuralDB为大规模编辑大语言模型中的知识提供了一个高效、可扩展且能有效保留模型通用能力的解决方案，克服了现有方法的局限性。

Abstract: Efficiently editing knowledge stored in large language models (LLMs) enables
model updates without large-scale training. One possible solution is
Locate-and-Edit (L\&E), allowing simultaneous modifications of a massive number
of facts. However, such editing may compromise the general abilities of LLMs
and even result in forgetting edited facts when scaling up to thousands of
edits. In this paper, we model existing linear L\&E methods as querying a
Key-Value (KV) database. From this perspective, we then propose NeuralDB, an
editing framework that explicitly represents the edited facts as a neural KV
database equipped with a non-linear gated retrieval module, % In particular,
our gated module only operates when inference involves the edited facts,
effectively preserving the general abilities of LLMs. Comprehensive experiments
involving the editing of 10,000 facts were conducted on the ZsRE and
CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results
demonstrate that NeuralDB not only excels in editing efficacy, generalization,
specificity, fluency, and consistency, but also preserves overall performance
across six representative text understanding and generation tasks. Further
experiments indicate that NeuralDB maintains its effectiveness even when scaled
to 100,000 facts (\textbf{50x} more than in prior work).

</details>


### [10] [GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs](https://arxiv.org/abs/2507.18043)
*Duy Nguyen,Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: GrAInS是一种新的推理时转向方法，通过基于梯度的token归因识别关键token并构建方向性转向向量，以精细控制大型语言模型（LLM）和视觉-语言模型（VLM）的行为，无需微调，并在多项任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有推理时转向方法存在局限性：依赖固定全局干预向量，忽视单个输入token的因果影响，且未能利用模型logits中有用的梯度，尤其在视觉和文本输入贡献不均的多模态场景中。

Method: GrAInS利用对比性的、基于梯度的归因（通过集成梯度）来识别对期望/非期望输出贡献最大的前k个关键token。这些token用于构建方向性转向向量，以捕捉语义上的转变。在推理过程中，GrAInS根据token级别的归因信号调整Transformer层的隐藏激活，并进行归一化以保持表示尺度。

Result: 实验证明，GrAInS持续优于微调和现有转向基线：在TruthfulQA上，使用Llama-3.1-8B准确率提升13.22%；使用LLaVA-1.6-7B在MMHal-Bench上，幻觉率从0.624降至0.514；在SPA-VL上，对齐胜率提高8.11%，同时保持了模型的流畅性和通用能力。

Conclusion: GrAInS提供了一种细粒度、可解释且模块化的模型行为控制方法，无需重新训练或辅助监督，并在多个基准测试中显著提升了LLM和VLM的性能，例如提高真实性、减少幻觉和改善对齐。

Abstract: Inference-time steering methods offer a lightweight alternative to
fine-tuning large language models (LLMs) and vision-language models (VLMs) by
modifying internal activations at test time without updating model weights.
However, most existing approaches rely on fixed, global intervention vectors,
overlook the causal influence of individual input tokens, and fail to leverage
informative gradients from the model's logits, particularly in multimodal
settings where visual and textual inputs contribute unevenly. To address these
limitations, we introduce GrAInS, an inference-time steering approach that
operates across both language-only and vision-language models and tasks. GrAInS
uses contrastive, gradient-based attribution via Integrated Gradients to
identify the top-k most influential tokens, both positively and negatively
attributed based on their contribution to preferred versus dispreferred
outputs. These tokens are then used to construct directional steering vectors
that capture semantic shifts from undesirable to desirable behavior. During
inference, GrAInS adjusts hidden activations at transformer layers guided by
token-level attribution signals, and normalizes activations to preserve
representational scale. This enables fine-grained, interpretable, and modular
control over model behavior, without retraining or auxiliary supervision.
Empirically, GrAInS consistently outperforms both fine-tuning and existing
steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using
Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514
with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all
while preserving the model's fluency and general capabilities.

</details>


### [11] [Synthetic Data Generation for Phrase Break Prediction with Large Language Model](https://arxiv.org/abs/2507.18044)
*Hoyeon Lee,Sejung Son,Ye-Eun Kang,Jong-Hwan Kim*

Main category: cs.CL

TL;DR: 本研究探索了利用大型语言模型（LLMs）生成合成短语停顿标注数据，以克服传统方法中人工标注成本高昂和数据获取困难的问题，并证明了LLMs在缓解语音领域数据挑战方面的有效性和潜力。


<details>
  <summary>Details</summary>
Motivation: 现有短语停顿预测方法过度依赖耗时且昂贵的人工标注，且语音领域固有的变异性使得高质量数据获取复杂。鉴于大型语言模型（LLMs）在自然语言处理（NLP）领域通过生成合成数据成功解决了数据挑战，本研究旨在将其应用于语音领域。

Method: 利用LLMs生成合成短语停顿标注数据，通过与传统标注进行比较，并在多种语言中评估其有效性。

Result: 研究发现，基于LLM的合成数据生成方法能有效缓解短语停顿预测中的数据挑战。

Conclusion: LLMs是解决语音领域数据问题的可行方案，具有巨大潜力。

Abstract: Current approaches to phrase break prediction address crucial prosodic
aspects of text-to-speech systems but heavily rely on vast human annotations
from audio or text, incurring significant manual effort and cost. Inherent
variability in the speech domain, driven by phonetic factors, further
complicates acquiring consistent, high-quality data. Recently, large language
models (LLMs) have shown success in addressing data challenges in NLP by
generating tailored synthetic data while reducing manual annotation needs.
Motivated by this, we explore leveraging LLM to generate synthetic phrase break
annotations, addressing the challenges of both manual annotation and
speech-related tasks by comparing with traditional annotations and assessing
effectiveness across multiple languages. Our findings suggest that LLM-based
synthetic data generation effectively mitigates data challenges in phrase break
prediction and highlights the potential of LLMs as a viable solution for the
speech domain.

</details>


### [12] [Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs](https://arxiv.org/abs/2507.18055)
*Tevin Atwal,Chan Nam Tieu,Yefeng Yuan,Zhan Shi,Yuhong Liu,Liang Cheng*

Main category: cs.CL

TL;DR: 评估并提升大语言模型（LLM）生成合成数据的多样性和隐私性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM生成的合成数据在数据驱动应用中具有成本效益和可扩展性，但其多样性不足和潜在的隐私风险尚未得到充分探索。

Method: 提出了一个全面的指标体系，量化评估文本合成数据的多样性（如语言表达、情感、用户视角）和隐私性（如再识别风险、风格异常值），并将其应用于多个最先进的LLM生成的数据集。基于评估结果，提出了一种基于提示词的方法，以增强合成评论的多样性并保护用户隐私。

Result: 实验结果表明，当前LLM在生成多样化且保护隐私的合成数据方面存在显著局限。

Conclusion: LLM生成的合成数据在多样性和隐私性方面存在不足，但通过所提出的评估方法可以识别这些问题，并可通过基于提示词的方法有效提升其多样性并保护隐私。

Abstract: The increasing use of synthetic data generated by Large Language Models
(LLMs) presents both opportunities and challenges in data-driven applications.
While synthetic data provides a cost-effective, scalable alternative to
real-world data to facilitate model training, its diversity and privacy risks
remain underexplored. Focusing on text-based synthetic data, we propose a
comprehensive set of metrics to quantitatively assess the diversity (i.e.,
linguistic expression, sentiment, and user perspective), and privacy (i.e.,
re-identification risk and stylistic outliers) of synthetic datasets generated
by several state-of-the-art LLMs. Experiment results reveal significant
limitations in LLMs' capabilities in generating diverse and privacy-preserving
synthetic data. Guided by the evaluation results, a prompt-based approach is
proposed to enhance the diversity of synthetic reviews while preserving
reviewer privacy.

</details>


### [13] [TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios](https://arxiv.org/abs/2507.18061)
*Zehan Li,Hongjie Chen,Yuxin Zhang,Jing Zhou,Xuening Wang,Hang Lv,Mengjie Du,Yaodong Song,Jie Lian,Jian Kang,Jie Li,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: 针对现有口语模型（SLM）评估基准未能反映真实对话交互的不足，本文提出TELEVAL，一个评估SLM在中文真实交互场景中作为会话代理效能的动态基准，尤其关注模型处理隐含线索的能力。实验表明现有SLM仍需显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有口语模型（SLM）评估基准主要关注复杂任务，未能有效评估模型在真实世界对话场景中的交互能力，尤其是在处理用户语音中隐含线索方面的表现。

Method: 提出TELEVAL，一个为评估SLM在真实中文交互场景中作为会话代理效能设计的动态基准。它定义了显式语义、副语言与隐含语义、系统能力三个评估维度，采用对话格式，分别评估文本和音频输出，并强调模型在无额外指令下从用户语音中提取隐含线索并恰当回应的能力。

Result: 实验表明，尽管口语模型近期取得了进展，但在自然对话任务方面仍有相当大的改进空间。

Conclusion: TELEVAL旨在成为一个以用户为中心的评估框架，直接反映用户体验，并促进开发更强大的对话导向型口语模型。

Abstract: Spoken language models (SLMs) have seen rapid progress in recent years, along
with the development of numerous benchmarks for evaluating their performance.
However, most existing benchmarks primarily focus on evaluating whether SLMs
can perform complex tasks comparable to those tackled by large language models
(LLMs), often failing to align with how users naturally interact in real-world
conversational scenarios. In this paper, we propose TELEVAL, a dynamic
benchmark specifically designed to evaluate SLMs' effectiveness as
conversational agents in realistic Chinese interactive settings. TELEVAL
defines three evaluation dimensions: Explicit Semantics, Paralinguistic and
Implicit Semantics, and System Abilities. It adopts a dialogue format
consistent with real-world usage and evaluates text and audio outputs
separately. TELEVAL particularly focuses on the model's ability to extract
implicit cues from user speech and respond appropriately without additional
instructions. Our experiments demonstrate that despite recent progress,
existing SLMs still have considerable room for improvement in natural
conversational tasks. We hope that TELEVAL can serve as a user-centered
evaluation framework that directly reflects the user experience and contributes
to the development of more capable dialogue-oriented SLMs.

</details>


### [14] [Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints](https://arxiv.org/abs/2507.18076)
*Haomin Qi,Zihan Dai,Chengbo Huang*

Main category: cs.CL

TL;DR: 针对大型语言模型（LLMs）微调的计算和内存瓶颈，本文提出了一种创新的混合PEFT策略，结合BOFT与LoRA-GA的优势，显著提高了微调效率、泛化能力，并在大幅节省计算资源的同时，实现了接近全量微调的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的微调由于其规模和高内存需求，面临严重的计算瓶颈。

Method: 本文全面评估了LoRA、BOFT、LoRA-GA和uRNN等参数高效微调（PEFT）技术。核心方法是引入一种新型混合策略，该策略通过动态结合BOFT的正交稳定性与LoRA-GA的梯度对齐快速收敛特性，并通过梯度范数引导的逐层自适应更新来实现。此外，首次探索了将酉循环神经网络（uRNN）原理应用于Transformer-based LLMs。

Result: 在GLUE、GSM8K、MT-Bench和HumanEval四个基准测试上，使用7B到405B参数的模型进行了实证评估。结果显示，所提出的混合方法持续优于现有的PEFT基线，其性能接近全量微调的准确性，同时将训练时间缩短2.1倍，内存使用减少50%。

Conclusion: 该混合方法被确立为一种实用且可扩展的微调解决方案，适用于资源受限下LLMs的实际部署。

Abstract: Fine-tuning large language models (LLMs) remains a computational bottleneck
due to their scale and memory demands. This paper presents a comprehensive
evaluation of parameter-efficient fine-tuning (PEFT) techniques, including
LoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that
dynamically integrates BOFT's orthogonal stability with LoRA-GA's
gradient-aligned rapid convergence. By computing per-layer adaptive updates
guided by gradient norms, the hybrid method achieves superior convergence
efficiency and generalization across diverse tasks. We also explore, for the
first time, the adaptation of unitary RNN (uRNN) principles to
transformer-based LLMs, enhancing gradient stability through structured unitary
constraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench,
and HumanEval -- using models ranging from 7B to 405B parameters demonstrate
that our hybrid method consistently outperforms individual PEFT baselines,
approaching full fine-tuning accuracy while reducing resource consumption by up
to 2.1 times in training time and 50 percent in memory usage. These findings
establish the hybrid approach as a practical and scalable fine-tuning solution
for real-world deployment of LLMs under resource constraints.

</details>


### [15] [A New Pair of GloVes](https://arxiv.org/abs/2507.18103)
*Riley Carlson,John Bauer,Christopher D. Manning*

Main category: cs.CL

TL;DR: 本文档描述并评估了2024年新的英文GloVe模型，这些模型旨在更新并改进2014年旧版本，以适应语言和世界的发展，并提供更详细的文档。


<details>
  <summary>Details</summary>
Motivation: 2014年构建的原始GloVe模型已广泛使用但已过时，无法反映当前语言和世界变化；且原始模型缺乏详细的数据版本和预处理文档，因此需要更新并详细记录新的GloVe模型。

Method: 研究人员使用Wikipedia、Gigaword和Dolma的子集训练了两套新的词嵌入模型。

Result: 通过词汇比较、直接测试和命名实体识别（NER）任务评估显示，2024年的向量纳入了新的文化和语言相关词汇；在类比和相似性等结构任务上表现相当；并在最近的、时间依赖的NER数据集（如非西方新闻数据）上表现出改进的性能。

Conclusion: 新的2024年GloVe模型成功地适应了语言和世界的演变，包含了更新的词汇，在结构任务上保持了原有水平，并在特定、时间敏感的NER任务上表现更优，同时解决了旧模型文档不足的问题。

Abstract: This report documents, describes, and evaluates new 2024 English GloVe
(Global Vectors for Word Representation) models. While the original GloVe
models built in 2014 have been widely used and found useful, languages and the
world continue to evolve and we thought that current usage could benefit from
updated models. Moreover, the 2014 models were not carefully documented as to
the exact data versions and preprocessing that were used, and we rectify this
by documenting these new models. We trained two sets of word embeddings using
Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary
comparison, direct testing, and NER tasks shows that the 2024 vectors
incorporate new culturally and linguistically relevant words, perform
comparably on structural tasks like analogy and similarity, and demonstrate
improved performance on recent, temporally dependent NER datasets such as
non-Western newswire data.

</details>


### [16] [GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness](https://arxiv.org/abs/2507.18119)
*Hongjie Chen,Zehan Li,Yaodong Song,Wenming Deng,Yitong Yao,Yuxin Zhang,Hang Lv,Xuechao Zhu,Jian Kang,Jie Lian,Jie Li,Chao Wang,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: 本文提出GOAT-SLM，一个具有副语言和说话者特征意识的新型口语语言模型，通过双模态头和分阶段训练，在语义和非语义任务上均表现出色，超越现有模型在情感、方言和年龄敏感交互方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有口语语言模型（SLMs）主要将语音视为语言内容的载体，忽略了人类语音中丰富的副语言和说话者特征线索（如方言、年龄、情感和非言语发声），限制了AI系统进行自然口语交互的能力。

Method: 引入GOAT-SLM，一个具有副语言和说话者特征意识的口语语言模型。该模型采用双模态头架构，将语言建模与声学实现解耦，并提出模块化、分阶段的训练策略，使用大规模语音-文本语料库逐步对齐语言、副语言和说话者特征信息。

Result: 在多维度评估基准TELEVAL上的实验结果表明，GOAT-SLM在语义和非语义任务上都取得了良好平衡的性能。它在处理情感、方言变异和年龄敏感交互方面优于现有的开源模型。

Conclusion: 这项工作强调了在语言内容之外进行建模的重要性，并推动了更自然、适应性强和社会意识强的口语语言系统的发展。

Abstract: Recent advances in end-to-end spoken language models (SLMs) have
significantly improved the ability of AI systems to engage in natural spoken
interactions. However, most existing models treat speech merely as a vehicle
for linguistic content, often overlooking the rich paralinguistic and speaker
characteristic cues embedded in human speech, such as dialect, age, emotion,
and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel
spoken language model with paralinguistic and speaker characteristic awareness,
designed to extend spoken language modeling beyond text semantics. GOAT-SLM
adopts a dual-modality head architecture that decouples linguistic modeling
from acoustic realization, enabling robust language understanding while
supporting expressive and adaptive speech generation. To enhance model
efficiency and versatility, we propose a modular, staged training strategy that
progressively aligns linguistic, paralinguistic, and speaker characteristic
information using large-scale speech-text corpora. Experimental results on
TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM
achieves well-balanced performance across both semantic and non-semantic tasks,
and outperforms existing open-source models in handling emotion, dialectal
variation, and age-sensitive interactions. This work highlights the importance
of modeling beyond linguistic content and advances the development of more
natural, adaptive, and socially aware spoken language systems.

</details>


### [17] [MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning](https://arxiv.org/abs/2507.18140)
*Xiaoyuan Li,Moxin Li,Wenjie Wang,Rui Men,Yichang Zhang,Fuli Feng,Dayiheng Liu,Junyang Lin*

Main category: cs.CL

TL;DR: 本文评估了多模态大语言模型（MLLMs）在多模态数学推理中，通过代码进行视觉操作的能力，发现现有模型在精细视觉操作方面远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在多模态数学推理方面取得了进展，并尝试使用代码作为中间表示进行图像操作，但现有评估主要集中于文本输出，忽略了模型通过代码执行精确视觉操作的能力。本文旨在填补这一空白。

Method: 研究提出了一个评估框架，聚焦于两个关键方面：1) 多模态代码生成（MCG），评估模型从零开始理解和构建可视化的能力；2) 多模态代码编辑（MCE），评估模型进行精细操作（包括删除、修改和标注）的能力。为了评估这些任务，研究整合了一个包含五种流行数学图形类型（几何图、函数图和三种统计图表）的数据集，并对九个主流MLLMs进行了实验评估。

Result: 实验结果显示，现有模型在执行精细视觉操作方面与人类表现存在显著差距。

Conclusion: 当前的多模态大语言模型在基于代码的多模态数学推理中的精细视觉操作能力仍有待大幅提升。

Abstract: Recent progress in Multi-modal Large Language Models (MLLMs) has enabled
step-by-step multi-modal mathematical reasoning by performing visual operations
based on the textual instructions. A promising approach uses code as an
intermediate representation to precisely express and manipulate the images in
the reasoning steps. However, existing evaluations focus mainly on text-only
reasoning outputs, leaving the MLLM's ability to perform accurate visual
operations via code largely unexplored. This work takes a first step toward
addressing that gap by evaluating MLLM's code-based capabilities in multi-modal
mathematical reasoning.Specifically, our framework focuses on two key
evaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's
ability to accurately understand and construct visualizations from scratch. (2)
Multi-modal Code Editing (MCE) assesses the model's capacity for fine-grained
operations, which include three types: Deletion, Modification and Annotation.
To evaluate the above tasks, we incorporate a dataset that covers the five most
popular types of mathematical figures, including geometric diagrams, function
plots, and three types of statistical charts, to provide a comprehensive and
effective measurement of existing MLLMs. Our experimental evaluation involves
nine mainstream MLLMs, and the results reveal that existing models still lag
significantly behind human performance in performing fine-grained visual
operations.

</details>


### [18] [HIVMedQA: Benchmarking large language models for HIV medical decision support](https://arxiv.org/abs/2507.18143)
*Gonzalo Cardenal Antolin,Jacques Fellay,Bashkim Jaha,Roger Kouyos,Niko Beerenwinkel,Diane Duroux*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）在艾滋病管理中的能力，引入了HIVMedQA基准测试。结果显示Gemini 2.5 Pro表现最佳，但模型性能受问题复杂性影响，且专业微调和模型大小并非性能的可靠指标，突显了有针对性开发和评估的必要性。


<details>
  <summary>Details</summary>
Motivation: LLMs在临床决策中具有潜力，尤其在复杂的艾滋病管理领域。然而，LLMs在临床实践中的准确性、潜在危害和接受度存在担忧，且艾滋病护理领域的AI应用和LLM基准测试尚不充分，因此需要评估LLMs在该领域的当前能力和局限性。

Method: 引入了HIVMedQA数据集，一个由传染病医生参与开发的、用于评估艾滋病护理中开放式医学问答的基准。评估了7个通用型和3个医学专用型LLM，并应用提示工程。评估框架结合了词汇相似性和LLM作为裁判的方法（为临床相关性进行了扩展），评估维度包括问题理解、推理、知识回忆、偏见、潜在危害和事实准确性。

Result: Gemini 2.5 Pro在大多数维度上持续优于其他模型，排名前三的模型中有两个是专有模型。模型性能随问题复杂性增加而下降。医学微调模型并非总优于通用模型，且模型大小不是性能的可靠预测因子。推理和理解比事实回忆更具挑战性，并观察到近因效应和现状偏见等认知偏见。

Conclusion: 研究结果强调了需要针对性地开发和评估LLMs，以确保它们在临床护理中（特别是艾滋病管理）的安全有效整合。

Abstract: Large language models (LLMs) are emerging as valuable tools to support
clinicians in routine decision-making. HIV management is a compelling use case
due to its complexity, including diverse treatment options, comorbidities, and
adherence challenges. However, integrating LLMs into clinical practice raises
concerns about accuracy, potential harm, and clinician acceptance. Despite
their promise, AI applications in HIV care remain underexplored, and LLM
benchmarking studies are scarce. This study evaluates the current capabilities
of LLMs in HIV management, highlighting their strengths and limitations. We
introduce HIVMedQA, a benchmark designed to assess open-ended medical question
answering in HIV care. The dataset consists of curated, clinically relevant
questions developed with input from an infectious disease physician. We
evaluated seven general-purpose and three medically specialized LLMs, applying
prompt engineering to enhance performance. Our evaluation framework
incorporates both lexical similarity and an LLM-as-a-judge approach, extended
to better reflect clinical relevance. We assessed performance across key
dimensions: question comprehension, reasoning, knowledge recall, bias,
potential harm, and factual accuracy. Results show that Gemini 2.5 Pro
consistently outperformed other models across most dimensions. Notably, two of
the top three models were proprietary. Performance declined as question
complexity increased. Medically fine-tuned models did not always outperform
general-purpose ones, and larger model size was not a reliable predictor of
performance. Reasoning and comprehension were more challenging than factual
recall, and cognitive biases such as recency and status quo were observed.
These findings underscore the need for targeted development and evaluation to
ensure safe, effective LLM integration in clinical care.

</details>


### [19] [Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models](https://arxiv.org/abs/2507.18171)
*Kexin Chen,Dongxia Wang,Yi Liu,Haonan Zhang,Wenhai Wang*

Main category: cs.CL

TL;DR: 本文揭示了Transformer文本嵌入模型中“粘性词元”问题，它们会扭曲句子相似度并损害下游任务性能。研究定义了粘性词元并提出了检测方法（STD），发现了大量此类词元，分析了其来源和对模型内部表示及下游性能的负面影响（高达50%的性能下降），强调了改进分词和模型设计的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer文本嵌入模型在自然语言处理任务中广泛应用，但“粘性词元”的存在损害了嵌入的可靠性。这些词元重复插入句子时，会导致句子相似度趋于特定值，扰乱嵌入距离的正常分布，并降低下游任务性能。因此，有必要对这些异常词元进行系统性调查。

Method: 研究系统地定义了粘性词元，并提出了一种基于句子和词元过滤的高效检测方法——粘性词元检测器（Sticky Token Detector, STD）。将STD应用于14个模型家族的40个检查点。此外，通过注意力层分析揭示了粘性词元在模型内部表示中的作用。

Result: ['在所测试的模型中，共发现了868个粘性词元。', '这些词元常源于词汇表中特殊或未使用的条目，以及多语言语料库中的碎片化子词。', '粘性词元的存在与模型大小或词汇量无严格关联。', '粘性词元导致聚类和检索等下游任务的性能显著下降，降幅高达50%。', '注意力层分析显示，粘性词元在模型内部表示中占据了不成比例的主导地位。']

Conclusion: 粘性词元严重影响Transformer文本嵌入模型的可靠性和下游任务性能，揭示了当前分词策略的鲁棒性问题。研究结果强调，未来文本嵌入应用需要改进分词策略和模型设计，以有效减轻粘性词元的影响。

Abstract: Despite the widespread use of Transformer-based text embedding models in NLP
tasks, surprising 'sticky tokens' can undermine the reliability of embeddings.
These tokens, when repeatedly inserted into sentences, pull sentence similarity
toward a certain value, disrupting the normal distribution of embedding
distances and degrading downstream performance. In this paper, we
systematically investigate such anomalous tokens, formally defining them and
introducing an efficient detection method, Sticky Token Detector (STD), based
on sentence and token filtering. Applying STD to 40 checkpoints across 14 model
families, we discover a total of 868 sticky tokens. Our analysis reveals that
these tokens often originate from special or unused entries in the vocabulary,
as well as fragmented subwords from multilingual corpora. Notably, their
presence does not strictly correlate with model size or vocabulary size. We
further evaluate how sticky tokens affect downstream tasks like clustering and
retrieval, observing significant performance drops of up to 50%. Through
attention-layer analysis, we show that sticky tokens disproportionately
dominate the model's internal representations, raising concerns about
tokenization robustness. Our findings show the need for better tokenization
strategies and model design to mitigate the impact of sticky tokens in future
text embedding applications.

</details>


### [20] [SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models](https://arxiv.org/abs/2507.18182)
*Wonjun Jeong,Dongseok Kim,Taegkeun Whangbo*

Main category: cs.CL

TL;DR: 本研究提出SCOPE评估框架，旨在测量和缓解大型语言模型（LLMs）在多项选择任务中利用选项位置或标签偏差导致的虚高分数，从而提升评估的公平性和可靠性。


<details>
  <summary>Details</summary>
Motivation: LLMs在多项选择任务中，可能通过利用选项位置或标签的固有偏差来获得虚高分数，而非真正理解。因此，需要一个独立于数据集的评估框架来衡量并减轻这种选择偏差。

Method: SCOPE框架通过反复调用缺乏语义内容的“空提示”来估计每个模型的独特位置偏差分布。然后，它根据逆偏差分布重新分配答案槽位，以均衡随机选中正确答案的概率。此外，它还阻止语义相似的干扰项与正确答案相邻，从而防止基于表面接近度的“险胜猜测”。

Result: 在多项基准实验中，SCOPE框架在稳定的性能提升方面始终优于现有去偏方法，并对正确选项显示出更清晰的置信度分布。

Conclusion: SCOPE框架为提高LLM评估的公平性和可靠性提供了一个新标准。

Abstract: Large Language Models (LLMs) can achieve inflated scores on multiple-choice
tasks by exploiting inherent biases in option positions or labels, rather than
demonstrating genuine understanding. This study introduces SCOPE, an evaluation
framework designed to measure and mitigate such selection bias in a
dataset-independent manner. By repeatedly invoking a null prompt that lacks
semantic content, SCOPE estimates each model's unique position-bias
distribution. It then redistributes the answer slot according to the
inverse-bias distribution, thereby equalizing the lucky-rate, the probability
of selecting the correct answer by chance. Furthermore, it prevents
semantically similar distractors from being placed adjacent to the answer,
thereby blocking near-miss guesses based on superficial proximity cues. Across
multiple benchmark experiments, SCOPE consistently outperformed existing
debiasing methods in terms of stable performance improvements and showed
clearer confidence distributions over correct options. This framework thus
offers a new standard for enhancing the fairness and reliability of LLM
evaluations.

</details>


### [21] [TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks](https://arxiv.org/abs/2507.18190)
*Keyu Wu,Qianjin Yu,Manlin Mei,Ruiting Liu,Jun Wang,Kailai Zhang,Yelun Bao*

Main category: cs.CL

TL;DR: 电信网络中的根本原因分析（RCA）对AI而言是一项艰巨挑战。


<details>
  <summary>Details</summary>
Motivation: 电信网络中的RCA是一项关键任务，但由于其复杂的图推理需求和现实基准的稀缺性，对人工智能（AI）构成了巨大挑战。

Method: 未提及

Result: 未提及

Conclusion: RCA在电信网络中对AI来说是极其困难的，原因在于其复杂性（如图推理）和缺乏有效的评估基准。

Abstract: Root Cause Analysis (RCA) in telecommunication networks is a critical task,
yet it presents a formidable challenge for Artificial Intelligence (AI) due to
its complex, graph-based reasoning requirements and the scarcity of realistic
benchmarks.

</details>


### [22] [Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization](https://arxiv.org/abs/2507.18197)
*Aline Belloni,Patrick Prieur*

Main category: cs.CL

TL;DR: 探讨ISO 30401知识管理体系如何与现有ISO 9001业务流程整合，并提出通过SECI模型结合PDCA循环的实施方案。


<details>
  <summary>Details</summary>
Motivation: ISO 30401实施者面临挑战，难以向客户解释ISO 30401中的知识活动如何与现有操作流程有效整合，尤其是在ISO 9001框架下。

Method: 文章回顾了ISO 9001流程建模原则，并基于实践经验，探索了ISO 30401知识管理系统与集成管理系统其他流程的融合方式，特别提出通过在PDCA循环中部署SECI模型来实现。

Result: 抽象描述了作者基于经验提出的，将ISO 30401知识管理系统整合到现有管理系统流程中的方法和框架，具体体现为通过SECI模型在PDCA循环中的应用。

Conclusion: 文章旨在提供一个将ISO 30401知识管理与现有操作流程整合的框架，特别是通过在PDCA循环中应用SECI模型，从而帮助组织有效桥接知识管理与运营效率。

Abstract: Business process modeling is used by most organizations as an essential
framework for ensuring efficiency and effectiveness of the work and workflow
performed by its employees and for ensuring the alignment of such work with its
strategic goals. For organizations that are compliant or near-compliant with
ISO 9001, this approach involves the detailed mapping of processes,
sub-processes, activities, and tasks. ISO30401 is a Management System Standard,
introduced in 2018, establishing universal requirements for the set up of a
Knowledge Management System in an organization. As ``ISO30401 implementers'' we
regularly face the challenge of explaining our clients how the knowledge
development, transformation and conveyances activities depicted in ISO30401 do
integrate with existing operational processes. This article recaps process
modelling principles in the context of ISO9001 and explores, based on our
experience, how an ISO30401-compliant Knowledge Management System (KMS)
entwines with all other processes of an Integrated Management System and in
particular how it can be implemented by deploying the mechanisms of the SECI
model through the steps of PDCA cycles.

</details>


### [23] [Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection](https://arxiv.org/abs/2507.18202)
*San Kim,Jonghwi Kim,Yejin Jeon,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 本文提出GMTP防御方法，通过梯度分析和掩码语言模型检测RAG知识库中的投毒文档，有效消除90%以上恶意内容，同时保持系统性能。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）依赖外部知识库提升大型语言模型（LLMs）的准确性，但这种依赖也引入安全风险，即攻击者可以通过注入投毒文档来引导LLMs产生有害或误导性输出。因此，需要一种方法来检测并过滤这些对抗性文档。

Method: 提出梯度引导掩码标记概率（GMTP）防御方法。具体而言，GMTP通过检查检索器相似性函数的梯度来识别高影响力的关键标记，然后掩盖这些标记，并通过掩码语言模型（MLM）检查其概率。由于注入的恶意标记通常显示出极低的掩码标记概率，GMTP能够有效地检测并高精度地过滤恶意文档。

Result: 实验证明，GMTP能够消除90%以上的投毒内容，同时保留相关文档，从而在多样的数据集和对抗性设置下，保持了RAG系统稳健的检索和生成性能。

Conclusion: GMTP是一种有效的防御机制，能够抵御RAG知识库中的投毒攻击，确保RAG系统的安全性和鲁棒性。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
providing external knowledge for accurate and up-to-date responses. However,
this reliance on external sources exposes a security risk, attackers can inject
poisoned documents into the knowledge base to steer the generation process
toward harmful or misleading outputs. In this paper, we propose Gradient-based
Masked Token Probability (GMTP), a novel defense method to detect and filter
out adversarially crafted documents. Specifically, GMTP identifies high-impact
tokens by examining gradients of the retriever's similarity function. These key
tokens are then masked, and their probabilities are checked via a Masked
Language Model (MLM). Since injected tokens typically exhibit markedly low
masked-token probabilities, this enables GMTP to easily detect malicious
documents and achieve high-precision filtering. Experiments demonstrate that
GMTP is able to eliminate over 90% of poisoned content while retaining relevant
documents, thus maintaining robust retrieval and generation performance across
diverse datasets and adversarial settings.

</details>


### [24] [Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation](https://arxiv.org/abs/2507.18203)
*Kyubeen Han,Junseo Jang,Hongjin Kim,Geunyeong Jeong,Harksoo Kim*

Main category: cs.CL

TL;DR: 指令微调虽增强大型语言模型（LLM）的指令遵循能力，但也可能提高其对用户输入中错误信息的接受度，本研究旨在探讨并证实了这一影响。


<details>
  <summary>Details</summary>
Motivation: 指令微调提升了LLM的可用性和安全性，但可能使其过度依赖用户输入，导致接受错误信息和产生幻觉。现有研究虽关注LLM对外部信息的接受，但很少直接探讨指令微调对这种现象的影响，因此本研究旨在填补这一空白。

Method: 本研究调查了指令微调对LLM接受错误信息倾向的影响。通过比较指令微调模型与基础模型，并进一步探究了用户在提示结构中的角色、错误信息长度及系统提示中警告的存在等因素对模型接受错误信息的影响。

Result: 研究发现，指令微调后的LLM在用户提供错误信息时，接受的可能性显著增加。与基础模型相比，指令微调使模型更依赖用户提供的信息，易受影响性从助手角色转移到用户角色。

Conclusion: 研究结果强调了开发系统性方法来减轻指令微调的意外后果、并提高LLM在实际应用中可靠性的必要性。

Abstract: Instruction-tuning enhances the ability of large language models (LLMs) to
follow user instructions more accurately, improving usability while reducing
harmful outputs. However, this process may increase the model's dependence on
user input, potentially leading to the unfiltered acceptance of misinformation
and the generation of hallucinations. Existing studies primarily highlight that
LLMs are receptive to external information that contradict their parametric
knowledge, but little research has been conducted on the direct impact of
instruction-tuning on this phenomenon. In our study, we investigate the impact
of instruction-tuning on LLM's susceptibility to misinformation. Our analysis
reveals that instruction-tuned LLMs are significantly more likely to accept
misinformation when it is presented by the user. A comparison with base models
shows that instruction-tuning increases reliance on user-provided information,
shifting susceptibility from the assistant role to the user role. Furthermore,
we explore additional factors influencing misinformation susceptibility, such
as the role of the user in prompt structure, misinformation length, and the
presence of warnings in the system prompt. Our findings underscore the need for
systematic approaches to mitigate unintended consequences of instruction-tuning
and enhance the reliability of LLMs in real-world applications.

</details>


### [25] [Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation](https://arxiv.org/abs/2507.18212)
*Xinrui Chen,Hongxing Zhang,Fanyi Zeng,Yongxian Wei,Yizhi Wang,Xitong Ling,Guanghao Li,Chun Yuan*

Main category: cs.CL

TL;DR: 提出Prune&Comp方案，通过量级补偿解决大语言模型层剪枝导致的隐藏状态量级差异和性能下降问题，无需训练且效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型（LLM）层剪枝技术能有效压缩模型，但研究发现移除任何层都会在隐藏状态中引入显著的量级差异（magnitude gap），从而导致模型性能大幅下降。

Method: 提出Prune&Comp，一个即插即用的层剪枝方案。该方案通过量级补偿来缓解隐藏状态的量级差异，且无需额外训练。具体方法是估算层移除造成的量级差异，然后离线重新缩放剩余权重以消除差异，不产生运行时开销。该方案可与迭代剪枝策略结合，进一步提升效果。

Result: Prune&Comp能持续提升现有层剪枝指标。例如，当对LLaMA-3-8B模型剪枝5层时，Prune&Comp几乎将困惑度减半，并保留了原始模型93.19%的问答性能，相比基线模型性能提升了4.01%。

Conclusion: Prune&Comp方案通过无训练的量级补偿有效解决了大语言模型层剪枝中的性能下降问题，为LLM压缩提供了一种高效且高性能的新方法。

Abstract: Layer pruning has emerged as a promising technique for compressing large
language models (LLMs) while achieving acceleration proportional to the pruning
ratio. In this work, we identify that removing any layer induces a significant
magnitude gap in hidden states, resulting in substantial performance
degradation. To address this issue, we propose Prune&Comp, a novel
plug-and-play layer pruning scheme that leverages magnitude compensation to
mitigate such gaps in a training-free manner. Specifically, we first estimate
the magnitude gap caused by layer removal and then eliminate this gap by
rescaling the remaining weights offline, with zero runtime overhead incurred.
We further demonstrate the advantages of Prune&Comp through an iterative
pruning strategy. When integrated with an iterative prune-and-compensate loop,
Prune&Comp consistently enhances existing layer pruning metrics. For instance,
when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence
metric, Prune&Comp nearly halves the perplexity and retains 93.19\% of the
original model's question-answering performance, outperforming the baseline by
4.01%.

</details>


### [26] [Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models](https://arxiv.org/abs/2507.18263)
*Suhang Wu,Jialong Tang,Chengyi Yang,Pei Zhang,Baosong Yang,Junhui Li,Junfeng Yao,Min Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: 本文提出一种“定位-聚焦”方法，旨在解决语音翻译中术语翻译的挑战，该方法通过精准识别和关联术语信息，显著提高了术语翻译的准确率。


<details>
  <summary>Details</summary>
Motivation: 语音翻译（ST）中术语的准确翻译是一个巨大挑战。现有研究虽然尝试将翻译知识融入ST模型，但常受无关噪音干扰，且未能充分利用这些知识。

Method: 提出“定位-聚焦”方法。该方法首先有效定位语音中包含术语的片段，构建精简的翻译知识；随后，通过音频和文本模态将这些翻译知识与原始语音和假设关联起来，引导ST模型在翻译时更好地聚焦于关键信息。

Result: 在多个数据集上的实验结果表明，该方法能有效定位语音中的术语，显著提升术语翻译的成功率，同时保持稳定的整体翻译性能。

Conclusion: 所提出的“定位-聚焦”方法通过有效定位和利用术语信息，成功解决了语音翻译中的术语翻译难题，显著提高了术语翻译的准确性，且对通用翻译性能无负面影响。

Abstract: Direct speech translation (ST) has garnered increasing attention nowadays,
yet the accurate translation of terminology within utterances remains a great
challenge. In this regard, current studies mainly concentrate on leveraging
various translation knowledge into ST models. However, these methods often
struggle with interference from irrelevant noise and can not fully utilize the
translation knowledge. To address these issues, in this paper, we propose a
novel Locate-and-Focus method for terminology translation. It first effectively
locates the speech clips containing terminologies within the utterance to
construct translation knowledge, minimizing irrelevant information for the ST
model. Subsequently, it associates the translation knowledge with the utterance
and hypothesis from both audio and textual modalities, allowing the ST model to
better focus on translation knowledge during translation. Experimental results
across various datasets demonstrate that our method effectively locates
terminologies within utterances and enhances the success rate of terminology
translation, while maintaining robust general translation performance.

</details>


### [27] [Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil](https://arxiv.org/abs/2507.18264)
*Nevidu Jayatilleke,Nisansa de Silva*

Main category: cs.CL

TL;DR: 本文比较分析了六种OCR引擎在低资源语言僧伽罗语和泰米尔语上的零样本性能，发现Surya和Document AI分别为最佳，并引入了新的泰米尔语OCR基准数据集。


<details>
  <summary>Details</summary>
Motivation: 高资源语言的OCR问题已基本解决，但对于使用独特脚本的低资源语言，OCR仍是一个开放性难题。本研究旨在评估现有OCR引擎在此类语言上的零样本识别能力。

Method: 研究对六种OCR引擎（包括商业和开源系统）在僧伽罗语和泰米尔语上的零样本性能进行了比较分析。采用五种测量技术，在字符和单词级别评估准确性。此外，还引入了一个新的合成泰米尔语OCR基准数据集。

Result: Surya在僧伽罗语上表现最佳，词错误率（WER）为2.61%。Document AI在泰米尔语上表现最佳，字符错误率（CER）为0.78%。

Conclusion: 对于低资源语言的OCR，不同引擎的零样本性能差异显著。Surya和Document AI分别在僧伽罗语和泰米尔语上展现了卓越性能，本研究为该领域提供了重要见解和新的评估资源。

Abstract: Solving the problem of Optical Character Recognition (OCR) on printed text
for Latin and its derivative scripts can now be considered settled due to the
volumes of research done on English and other High-Resourced Languages (HRL).
However, for Low-Resourced Languages (LRL) that use unique scripts, it remains
an open problem. This study presents a comparative analysis of the zero-shot
performance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The
selected engines include both commercial and open-source systems, aiming to
evaluate the strengths of each category. The Cloud Vision API, Surya, Document
AI, and Tesseract were evaluated for both Sinhala and Tamil, while Subasa OCR
and EasyOCR were examined for only one language due to their limitations. The
performance of these systems was rigorously analysed using five measurement
techniques to assess accuracy at both the character and word levels. According
to the findings, Surya delivered the best performance for Sinhala across all
metrics, with a WER of 2.61%. Conversely, Document AI excelled across all
metrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the
above analysis, we also introduce a novel synthetic Tamil OCR benchmarking
dataset.

</details>


### [28] [StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer](https://arxiv.org/abs/2507.18294)
*Pritika Ramu,Apoorv Saxena,Meghanath M Y,Varsha Sankar,Debraj Basu*

Main category: cs.CL

TL;DR: StyleAdaptedLM是一个利用LoRA将风格特性高效转移到遵循指令的LLM的框架，无需配对数据且不牺牲指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 在企业通信中，将LLM适应特定风格（如品牌声音）至关重要。然而，从缺乏指令-响应格式的语料库中实现风格适应，同时不损害指令遵循能力，是一个挑战。

Method: 本文提出了StyleAdaptedLM框架，该框架利用低秩适应（LoRA）技术将风格特征高效地转移到遵循指令的模型中。具体方法是，首先使用多样化的非结构化风格语料库训练LoRA适配器，然后将其与一个独立的指令遵循模型合并。

Result: 实验表明，StyleAdaptedLM在保持指令遵循能力的同时，显著提高了风格一致性。人工评估进一步证实了模型成功采纳了品牌特定的约定。

Conclusion: StyleAdaptedLM为LLM实现高效且鲁棒的风格个性化提供了一条有前景的途径。

Abstract: Adapting LLMs to specific stylistic characteristics, like brand voice or
authorial tones, is crucial for enterprise communication but challenging to
achieve from corpora which lacks instruction-response formatting without
compromising instruction adherence. We introduce StyleAdaptedLM, a framework
that efficiently transfers stylistic traits to instruction-following models
using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base
model with diverse unstructured stylistic corpora, then merged with a separate
instruction-following model. This enables robust stylistic customization
without paired data or sacrificing task performance. Experiments across
multiple datasets and models demonstrate improved stylistic consistency while
preserving instruction adherence, with human evaluations confirming
brand-specific convention uptake. StyleAdaptedLM offers an efficient path for
stylistic personalization in LLMs.

</details>


### [29] [BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit](https://arxiv.org/abs/2507.18305)
*Biao Yi,Zekun Fei,Jianing Geng,Tong Li,Lihai Nie,Zheli Liu,Yiming Li*

Main category: cs.CL

TL;DR: 该论文提出了一种针对大型推理模型（LRMs）的“过度思考后门”攻击，通过数据投毒使模型在保持答案正确性的前提下产生冗余的思维链推理，从而消耗更多计算资源。


<details>
  <summary>Details</summary>
Motivation: 发现并探索LRMs中一个名为“过度思考后门”的未被探索过的攻击向量，旨在展示攻击者如何精确控制模型推理冗余度的能力，并将其从简单的开/关攻击发展为可调谐的攻击。

Method: 提出一种新颖的数据投毒方法，通过将一个可调谐的触发器（其重复次数信号着期望的强度）与相应冗余的思维链（CoT）响应配对。这些冗余响应由一个教师LLM程序化生成，通过在正确的推理过程中注入受控数量的冗余细化步骤实现。

Result: 在各种LRMs上的广泛实证结果表明，该方法能够可靠地触发推理过程长度的可控、多倍增加，同时不降低最终答案的正确性。这证明了该攻击是一个纯粹的资源消耗向量。

Conclusion: 研究成功揭示并实现了一种新颖的“过度思考后门”攻击，能够隐蔽且可控地增加LRMs的推理冗余度，将其转化为一种有效的资源消耗攻击，且不影响输出的正确性。

Abstract: Large reasoning models (LRMs) have emerged as a significant advancement in
artificial intelligence, representing a specialized class of large language
models (LLMs) designed to tackle complex reasoning tasks. The defining
characteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning
capabilities. In this paper, we identify a previously unexplored attack vector
against LRMs, which we term "overthinking backdoors". We advance this concept
by proposing a novel tunable backdoor, which moves beyond simple on/off attacks
to one where an attacker can precisely control the extent of the model's
reasoning verbosity. Our attack is implemented through a novel data poisoning
methodology. It pairs a tunable trigger-where the number of repetitions signals
the desired intensity-with a correspondingly verbose CoT response. These
responses are programmatically generated by instructing a teacher LLM to inject
a controlled number of redundant refinement steps into a correct reasoning
process. The approach preserves output correctness, which ensures stealth and
establishes the attack as a pure resource-consumption vector. Extensive
empirical results on various LRMs demonstrate that our method can reliably
trigger a controllable, multi-fold increase in the length of the reasoning
process, without degrading the final answer's correctness. Our source code is
available at https://github.com/FZaKK/BadReasoner.

</details>


### [30] [Uncertainty Quantification for Evaluating Machine Translation Bias](https://arxiv.org/abs/2507.18338)
*Ieva Raminta Staliūnaitė,Julius Cheng,Andreas Vlachos*

Main category: cs.CL

TL;DR: 本研究探讨机器翻译模型在处理性别模糊词时的偏见行为，并提出模型应在性别不明确时保持不确定性。结果发现，高准确率模型不一定在模糊情况下表现出预期的不确定性，且去偏见对明确和模糊翻译实例的影响是独立的。


<details>
  <summary>Details</summary>
Motivation: 机器翻译模型在处理性别未明确标记但目标语言需要性别指定词汇时，常因依赖刻板印象而产生偏见翻译，即使这与上下文信息冲突。研究动机在于解决这种偏见行为，并提出模型在性别模糊时应维持不确定性而非自信地给出错误翻译。

Method: 本研究利用近期提出的语义不确定性度量指标，对机器翻译模型的性别处理能力进行分析和评估。

Result: 研究发现，在明确的翻译实例上具有高翻译和性别准确度的模型，在处理性别模糊的实例时，不一定能表现出预期的不确定性水平。同时，去偏见操作对模糊和明确的翻译实例具有独立的影响。

Conclusion: 机器翻译模型在处理性别模糊时存在不足，单纯提高明确实例的准确性并不能保证对模糊实例的正确处理。此外，去偏见策略需要针对模糊和明确翻译实例进行独立考虑，以达到更全面的效果。

Abstract: In machine translation (MT), when the source sentence includes a lexeme whose
gender is not overtly marked, but whose target-language equivalent requires
gender specification, the model must infer the appropriate gender from the
context and/or external knowledge. Studies have shown that MT models exhibit
biased behaviour, relying on stereotypes even when they clash with contextual
information. We posit that apart from confidently translating using the correct
gender when it is evident from the input, models should also maintain
uncertainty about the gender when it is ambiguous. Using recently proposed
metrics of semantic uncertainty, we find that models with high translation and
gender accuracy on unambiguous instances do not necessarily exhibit the
expected level of uncertainty in ambiguous ones. Similarly, debiasing has
independent effects on ambiguous and unambiguous translation instances.

</details>


### [31] [TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning](https://arxiv.org/abs/2507.18340)
*Yifu Chen,Bingchen Huang,Zhiling Wang,Yuanchao Du,Junfeng Luo,Lei Shen,Zhineng chen*

Main category: cs.CL

TL;DR: TDR是一个新型框架，通过解耦跨任务示例并利用LLM的细粒度反馈，解决了ICL中高质量示例检索的挑战，并在30个NLP任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管上下文学习（ICL）的示例检索方法已取得进展，但在检索高质量示例时仍面临两大挑战：一是难以区分跨任务数据分布，二是难以建立检索器输出与LLM细粒度反馈之间的联系。

Method: 本文提出了TDR框架，它通过解耦来自不同任务的ICL示例，使检索模块能检索目标任务特有的示例。此外，TDR建模了LLM的细粒度反馈，以监督和指导检索模块的训练，从而提高检索质量。

Result: 在30个NLP任务上进行的广泛实验表明，TDR持续改进了所有数据集上的结果，并达到了最先进的性能。该方法是一个即插即用的模块，易于与各种LLM结合以增强示例检索能力。

Conclusion: TDR框架通过其独特的任务解耦和LLM反馈建模机制，有效解决了ICL高质量示例检索的关键难题，显著提升了多任务场景下的性能表现，并具有良好的通用性和易用性。

Abstract: In-context learning (ICL) has become a classic approach for enabling LLMs to
handle various tasks based on a few input-output examples. The effectiveness of
ICL heavily relies on the quality of these examples, and previous works which
focused on enhancing example retrieval capabilities have achieved impressive
performances. However, two challenges remain in retrieving high-quality
examples: (1) Difficulty in distinguishing cross-task data distributions, (2)
Difficulty in making the fine-grained connection between retriever output and
feedback from LLMs. In this paper, we propose a novel framework called TDR. TDR
decouples the ICL examples from different tasks, which enables the retrieval
module to retrieve examples specific to the target task within a multi-task
dataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise
and guide the training of the retrieval module, which helps to retrieve
high-quality examples. We conducted extensive experiments on a suite of 30 NLP
tasks, the results demonstrate that TDR consistently improved results across
all datasets and achieves state-of-the-art performance. Meanwhile, our approach
is a plug-and-play method, which can be easily combined with various LLMs to
improve example retrieval abilities for ICL. The code is available at
https://github.com/Nnn-s/TDR.

</details>


### [32] [Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence](https://arxiv.org/abs/2507.18343)
*Ariana Sahitaj,Premtim Sahitaj,Veronika Solopova,Jiaao Li,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 本文提出一个结合人工和大型语言模型（LLM）的新框架，通过LLM辅助预标注和知识蒸馏，实现可扩展且鲁棒的宣传检测。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的宣传检测因任务复杂性和高质量标注数据稀缺而充满挑战。

Method: 1. 提出一个将14种细粒度宣传技术组织成三类别的层级分类法。2. 进行人工标注研究，发现细粒度标签的标注者间一致性较低。3. 实施LLM辅助预标注流水线，提取宣传片段、生成解释并分配本地和全局标签。4. 通过知识蒸馏，利用LLM生成的高质量数据微调小型语言模型（SLMs），使其执行结构化标注。

Result: LLM辅助的预标注显著提高了人工标注的一致性和时间效率。成功训练了小型语言模型，能够基于LLM生成的数据进行结构化标注。

Conclusion: 本工作有助于开发可扩展和鲁棒的宣传检测系统，支持透明和负责任的媒体生态系统，符合可持续发展目标16。

Abstract: Propaganda detection on social media remains challenging due to task
complexity and limited high-quality labeled data. This paper introduces a novel
framework that combines human expertise with Large Language Model (LLM)
assistance to improve both annotation consistency and scalability. We propose a
hierarchical taxonomy that organizes 14 fine-grained propaganda techniques into
three broader categories, conduct a human annotation study on the HQP dataset
that reveals low inter-annotator agreement for fine-grained labels, and
implement an LLM-assisted pre-annotation pipeline that extracts propagandistic
spans, generates concise explanations, and assigns local labels as well as a
global label. A secondary human verification study shows significant
improvements in both agreement and time-efficiency. Building on this, we
fine-tune smaller language models (SLMs) to perform structured annotation.
Instead of fine-tuning on human annotations, we train on high-quality
LLM-generated data, allowing a large model to produce these annotations and a
smaller model to learn to generate them via knowledge distillation. Our work
contributes towards the development of scalable and robust propaganda detection
systems, supporting the idea of transparent and accountable media ecosystems in
line with SDG 16. The code is publicly available at our GitHub repository.

</details>


### [33] [CLEAR: Error Analysis via LLM-as-a-Judge Made Easy](https://arxiv.org/abs/2507.18392)
*Asaf Yehudai,Lilach Eden,Yotam Perlitz,Roy Bar-Haim,Michal Shmueli-Scheuer*

Main category: cs.CL

TL;DR: CLEAR是一个开源交互式工具包，用于对LLM进行基于错误的分析，以弥补当前LLM评估仅给出分数而未解释原因的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）评估范式通常只提供单一分数或排名，回答哪个模型更好，但未能解释模型表现背后的具体可操作原因，这使得基准测试虽然有用，但无法提供深入、可操作的洞察。

Method: 引入了CLEAR工具包，它首先为每个实例生成文本反馈，然后创建一套系统级错误问题并量化其普遍性。该工具包还提供一个交互式仪表板，支持聚合可视化、交互式过滤以隔离特定问题或分数范围，并可深入查看具体实例。

Result: 通过在RAG（检索增强生成）和数学基准测试中展示CLEAR分析，并通过用户案例研究，证明了其在提供详细错误分析方面的实用性。

Conclusion: CLEAR有效弥补了LLM评估中缺乏详细、可操作错误原因分析的空白，提供了一个全面的分析框架，以深入理解LLM的性能表现。

Abstract: The evaluation of Large Language Models (LLMs) increasingly relies on other
LLMs acting as judges. However, current evaluation paradigms typically yield a
single score or ranking, answering which model is better but not why. While
essential for benchmarking, these top-level scores obscure the specific,
actionable reasons behind a model's performance. To bridge this gap, we
introduce CLEAR, an interactive, open-source package for LLM-based error
analysis. CLEAR first generates per-instance textual feedback, then it creates
a set of system-level error issues, and quantifies the prevalence of each
identified issue. Our package also provides users with an interactive dashboard
that allows for a comprehensive error analysis through aggregate
visualizations, applies interactive filters to isolate specific issues or score
ranges, and drills down to the individual instances that exemplify a particular
behavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,
and showcase its utility through a user case study.

</details>


### [34] [Factual Inconsistencies in Multilingual Wikipedia Tables](https://arxiv.org/abs/2507.18406)
*Silvia Cappa,Lingxiao Kong,Pille-Riin Peet,Fanfu Wei,Yuchen Zhou,Jan-Christoph Kalo*

Main category: cs.CL

TL;DR: 本研究分析了Wikipedia多语言版本中结构化内容（表格数据）的跨语言不一致性，旨在提高事实准确性和AI系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: Wikipedia的不同语言版本独立编写和更新，导致事实不一致，这不仅影响百科全书自身的中立性和可靠性，也对依赖Wikipedia作为主要训练来源的AI系统构成挑战。

Method: 开发了一套收集、对齐和分析Wikipedia多语言文章中表格数据的方法论，并定义了不一致性类别。通过应用多种定量和定性指标，对样本数据集的多语言对齐情况进行了评估。

Result: 研究揭示并评估了Wikipedia结构化内容中的跨语言不一致性，提供了一种识别和量化这些不一致性的框架和方法。

Conclusion: 本研究的发现对于事实核查、多语言知识交互以及设计利用Wikipedia内容的可靠AI系统具有重要的启示意义。

Abstract: Wikipedia serves as a globally accessible knowledge source with content in
over 300 languages. Despite covering the same topics, the different versions of
Wikipedia are written and updated independently. This leads to factual
inconsistencies that can impact the neutrality and reliability of the
encyclopedia and AI systems, which often rely on Wikipedia as a main training
source. This study investigates cross-lingual inconsistencies in Wikipedia's
structured content, with a focus on tabular data. We developed a methodology to
collect, align, and analyze tables from Wikipedia multilingual articles,
defining categories of inconsistency. We apply various quantitative and
qualitative metrics to assess multilingual alignment using a sample dataset.
These insights have implications for factual verification, multilingual
knowledge interaction, and design for reliable AI systems leveraging Wikipedia
content.

</details>


### [35] [FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs](https://arxiv.org/abs/2507.18417)
*Giorgos Iacovides,Wuyang Zhou,Danilo Mandic*

Main category: cs.CL

TL;DR: 引入FinDPO，一个基于DPO的金融领域大语言模型，解决了SFT模型泛化性差的问题，并在情绪分类和实际投资策略中均取得卓越性能。


<details>
  <summary>Details</summary>
Motivation: 在线金融文本意见对交易决策和市场走势影响日益加深，情绪分析至关重要。然而，现有监督微调（SFT）大语言模型（LLMs）存在训练数据记忆化和对未见样本泛化能力不足的局限性，尤其在需要适应新事件和金融领域细微语言的金融场景中，这是一个关键限制。

Method: 提出FinDPO，首个基于直接偏好优化（DPO）进行后训练人类偏好对齐的金融专用大语言模型框架。此外，通过新颖的“logit-to-score”转换，将离散情绪预测转化为连续、可排序的情绪分数，从而将微调后的因果LLM集成到现实投资组合策略中。

Result: FinDPO在标准情绪分类基准测试中达到了最先进的性能，平均比现有监督微调模型高出11%。模拟结果显示，即使在5个基点的实际交易成本下，FinDPO作为首个基于情绪的方法，仍能维持每年67%的显著正回报和2.0的夏普比率，表现出强大的风险调整性能。

Conclusion: FinDPO成功解决了金融领域SFT大模型泛化能力不足的问题，在情绪分类方面表现出色，并首次证明了其能够通过创新的转换方法，在实际投资组合策略中带来显著且稳健的回报，具有重要的理论和实践意义。

Abstract: Opinions expressed in online finance-related textual data are having an
increasingly profound impact on trading decisions and market movements. This
trend highlights the vital role of sentiment analysis as a tool for quantifying
the nature and strength of such opinions. With the rapid development of
Generative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)
have become the de facto standard for financial sentiment analysis. However,
the SFT paradigm can lead to memorization of the training data and often fails
to generalize to unseen samples. This is a critical limitation in financial
domains, where models must adapt to previously unobserved events and the
nuanced, domain-specific language of finance. To this end, we introduce FinDPO,
the first finance-specific LLM framework based on post-training human
preference alignment via Direct Preference Optimization (DPO). The proposed
FinDPO achieves state-of-the-art performance on standard sentiment
classification benchmarks, outperforming existing supervised fine-tuned models
by 11% on the average. Uniquely, the FinDPO framework enables the integration
of a fine-tuned causal LLM into realistic portfolio strategies through a novel
'logit-to-score' conversion, which transforms discrete sentiment predictions
into continuous, rankable sentiment scores (probabilities). In this way,
simulations demonstrate that FinDPO is the first sentiment-based approach to
maintain substantial positive returns of 67% annually and strong risk-adjusted
performance, as indicated by a Sharpe ratio of 2.0, even under realistic
transaction costs of 5 basis points (bps).

</details>


### [36] [AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data](https://arxiv.org/abs/2507.18442)
*Rana Alshaikh,Israa Alghanmi,Shelan Jeawak*

Main category: cs.CL

TL;DR: 本研究提出了AraTable，一个评估大型语言模型（LLMs）处理阿拉伯语表格数据能力的基准，并发现LLMs在复杂推理任务上仍面临挑战，同时提出了一个自动化评估框架。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自然语言处理方面取得了显著进展，但其在解释结构化数据（特别是表格数据）方面的性能仍有限。目前缺乏针对阿拉伯语表格数据的公共资源和基准，以评估LLMs对该类数据的理解和推理能力。

Method: 本研究提出了一个名为AraTable的综合性基准，旨在评估LLMs在阿拉伯语表格数据上的推理和理解能力。AraTable包含直接问答、事实核查和复杂推理等多种评估任务。数据收集采用混合管道，即初始内容由LLMs生成，随后由人类专家过滤和验证，以确保数据集的高质量。

Result: 初步分析表明，LLMs在简单的表格任务（如直接问答）上表现尚可，但在需要更深层次推理和事实核查的复杂任务上仍面临显著认知挑战。此外，研究还提出了一个利用自反思机制的自动化评估框架，其性能与人类评判结果几乎一致。

Conclusion: 本研究提供了一个有价值的、公开可用的阿拉伯语表格数据资源和评估框架，有助于加速开发处理和分析阿拉伯语结构化数据的基础模型。

Abstract: The cognitive and reasoning abilities of large language models (LLMs) have
enabled remarkable progress in natural language processing. However, their
performance in interpreting structured data, especially in tabular formats,
remains limited. Although benchmarks for English tabular data are widely
available, Arabic is still underrepresented because of the limited availability
of public resources and its unique language features. To address this gap, we
present AraTable, a novel and comprehensive benchmark designed to evaluate the
reasoning and understanding capabilities of LLMs when applied to Arabic tabular
data. AraTable consists of various evaluation tasks, such as direct question
answering, fact verification, and complex reasoning, involving a wide range of
Arabic tabular sources. Our methodology follows a hybrid pipeline, where
initial content is generated by LLMs and subsequently filtered and verified by
human experts to ensure high dataset quality. Initial analyses using AraTable
show that, while LLMs perform adequately on simpler tabular tasks such as
direct question answering, they continue to face significant cognitive
challenges when tasks require deeper reasoning and fact verification. This
indicates that there are substantial opportunities for future work to improve
performance on complex tabular reasoning tasks. We also propose a fully
automated evaluation framework that uses a self-deliberation mechanism and
achieves performance nearly identical to that of human judges. This research
provides a valuable, publicly available resource and evaluation framework that
can help accelerate the development of foundational models for processing and
analysing Arabic structured data.

</details>


### [37] [Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language](https://arxiv.org/abs/2507.18448)
*Md Obyedullahil Mamun,Md Adyelullahil Mamun,Arif Ahmad,Md. Imran Hossain Emu*

Main category: cs.CL

TL;DR: 本研究利用XLM-RoBERTa大模型和数据增强技术，对孟加拉语文本进行标点恢复，取得了高准确率和良好的泛化能力，并为低资源NLP提供了基准和资源。


<details>
  <summary>Details</summary>
Motivation: 标点恢复对于提高文本可读性和自动语音识别（ASR）的后处理至关重要，尤其是在孟加拉语等低资源语言中。目前孟加拉语缺乏带标注的资源，限制了相关研究和应用。

Method: 采用基于Transformer的模型，特别是XLM-RoBERTa大模型，自动恢复孟加拉语文本中的句号、逗号、问号和感叹号四种标点符号。为解决资源稀缺问题，构建了大型多样化训练语料库，并应用了数据增强技术（增强因子alpha=0.20%）。

Result: 最优模型在新闻测试集上达到了97.1%的准确率，在参考集上达到91.2%，在ASR集上达到90.2%。结果表明模型对参考文本和ASR转录本具有很强的泛化能力，在真实、嘈杂场景下表现出有效性。

Conclusion: 本工作为孟加拉语标点恢复建立了强有力的基线，并贡献了公开可用的数据集和代码，以支持低资源NLP领域的未来研究。

Abstract: Punctuation restoration enhances the readability of text and is critical for
post-processing tasks in Automatic Speech Recognition (ASR), especially for
low-resource languages like Bangla. In this study, we explore the application
of transformer-based models, specifically XLM-RoBERTa-large, to automatically
restore punctuation in unpunctuated Bangla text. We focus on predicting four
punctuation marks: period, comma, question mark, and exclamation mark across
diverse text domains. To address the scarcity of annotated resources, we
constructed a large, varied training corpus and applied data augmentation
techniques. Our best-performing model, trained with an augmentation factor of
alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the
Reference set, and 90.2% on the ASR set.
  Results show strong generalization to reference and ASR transcripts,
demonstrating the model's effectiveness in real-world, noisy scenarios. This
work establishes a strong baseline for Bangla punctuation restoration and
contributes publicly available datasets and code to support future research in
low-resource NLP.

</details>


### [38] [Generation of Synthetic Clinical Text: A Systematic Review](https://arxiv.org/abs/2507.18451)
*Basel Alshaikhdeeb,Ahmed Abdelmonem Hemedan,Soumyabrata Ghosh,Irina Balaur,Venkata Satagopam*

Main category: cs.CL

TL;DR: 本系统综述分析了生成合成医学自由文本的目的、技术和评估方法。发现其主要用于数据增强，Transformer（特别是GPT）是主流技术，效用是主要评估指标。合成文本虽对NLP任务有益，但在隐私保护方面仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 为了解决临床自然语言处理（NLP）中数据稀疏性和隐私保护等常见问题，生成合成临床文本被认为是一种有效的解决方案。本研究旨在系统回顾生成合成医学自由文本的现状。

Method: 本研究进行了一项系统综述，通过在PubMed、ScienceDirect、Web of Science、Scopus、IEEE、Google Scholar和arXiv等七个数据库中搜索与生成合成医学非结构化自由文本相关的出版物。从1,398篇收集到的文章中筛选出94篇相关文章，并对生成目的、技术和评估方法三个研究问题进行了定量分析。

Result: 自2018年以来，合成医学文本的生成受到了广泛关注，其主要目的包括文本增强、辅助写作、语料库构建、隐私保护、标注和实用性。Transformer架构（尤其是GPT模型）是生成文本的主要技术。评估主要集中在相似性、隐私、结构和效用四个方面，其中效用是评估合成医学文本最常用的方法。合成医学文本在不同的下游NLP任务中，虽然作为真实医学文档的可能性一般，但作为增强或补充真实文档的资产，在提高准确性和克服数据稀疏/欠采样问题方面表现出色。

Conclusion: 生成合成医学文本将大大加速工作流程和管道开发，减少数据传输耗时的法律程序。然而，隐私仍然是生成合成医学文本的一个主要问题，需要更多人工评估以检查敏感信息的存在。

Abstract: Generating clinical synthetic text represents an effective solution for
common clinical NLP issues like sparsity and privacy. This paper aims to
conduct a systematic review on generating synthetic medical free-text by
formulating quantitative analysis to three research questions concerning (i)
the purpose of generation, (ii) the techniques, and (iii) the evaluation
methods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE,
Google Scholar, and arXiv databases for publications associated with generating
synthetic medical unstructured free-text. We have identified 94 relevant
articles out of 1,398 collected ones. A great deal of attention has been given
to the generation of synthetic medical text from 2018 onwards, where the main
purpose of such a generation is towards text augmentation, assistive writing,
corpus building, privacy-preserving, annotation, and usefulness. Transformer
architectures were the main predominant technique used to generate the text,
especially the GPTs. On the other hand, there were four main aspects of
evaluation, including similarity, privacy, structure, and utility, where
utility was the most frequent method used to assess the generated synthetic
medical text. Although the generated synthetic medical text demonstrated a
moderate possibility to act as real medical documents in different downstream
NLP tasks, it has proven to be a great asset as augmented, complementary to the
real documents, towards improving the accuracy and overcoming
sparsity/undersampling issues. Yet, privacy is still a major issue behind
generating synthetic medical text, where more human assessments are needed to
check for the existence of any sensitive information. Despite that, advances in
generating synthetic medical text will considerably accelerate the adoption of
workflows and pipeline development, discarding the time-consuming legalities of
data transfer.

</details>


### [39] [Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models](https://arxiv.org/abs/2507.18504)
*Zheyu Zhang,Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 针对LLMs处理表格数据时自注意力机制在稀疏依赖上的局限性，本文提出了GraDe方法，通过整合稀疏依赖图来优化注意力分配，提高了复杂数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在表格数据生成方面潜力巨大，但表格数据固有的稀疏特征级依赖与LLMs的自注意力机制存在根本性不匹配。LLMs会将注意力分散到所有特征对上，从而稀释了对关键关系的关注，尤其在复杂依赖或语义模糊的特征数据集中问题更为突出。

Method: 本文提出GraDe（Graph-Guided Dependency Learning）方法，将稀疏依赖图显式整合到LLMs的注意力机制中。GraDe采用轻量级动态图学习模块，由外部提取的功能依赖关系指导，优先处理关键特征交互，同时抑制不相关的交互。

Result: 在多种真实世界数据集上的实验表明，GraDe在复杂数据集上比现有基于LLM的方法性能提升高达12%，并在合成数据质量方面与最先进的方法具有竞争力。

Conclusion: GraDe是一种微创但有效的方法，为LLMs进行结构感知的表格数据建模提供了实用的解决方案。

Abstract: Large Language Models (LLMs) have shown strong potential for tabular data
generation by modeling textualized feature-value pairs. However, tabular data
inherently exhibits sparse feature-level dependencies, where many feature
interactions are structurally insignificant. This creates a fundamental
mismatch as LLMs' self-attention mechanism inevitably distributes focus across
all pairs, diluting attention on critical relationships, particularly in
datasets with complex dependencies or semantically ambiguous features. To
address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a
novel method that explicitly integrates sparse dependency graphs into LLMs'
attention mechanism. GraDe employs a lightweight dynamic graph learning module
guided by externally extracted functional dependencies, prioritizing key
feature interactions while suppressing irrelevant ones. Our experiments across
diverse real-world datasets demonstrate that GraDe outperforms existing
LLM-based approaches by up to 12% on complex datasets while achieving
competitive results with state-of-the-art approaches in synthetic data quality.
Our method is minimally intrusive yet effective, offering a practical solution
for structure-aware tabular data modeling with LLMs.

</details>


### [40] [The Moral Gap of Large Language Models](https://arxiv.org/abs/2507.18523)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: 研究发现，大语言模型在道德基础检测方面存在显著缺陷，任务特异性微调模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 道德基础检测对于分析社会话语和开发符合伦理的AI系统至关重要，但大型语言模型（LLMs）在此专业领域的性能尚不明确。

Method: 本研究首次对最先进的LLMs和微调的Transformer模型进行了全面比较，使用Twitter和Reddit数据集，并通过ROC、PR和DET曲线分析进行性能评估。

Result: 结果显示LLMs存在显著的性能差距，即使通过提示工程，它们仍表现出高假阴性率和系统性地低估道德内容的倾向。

Conclusion: 对于道德推理应用，任务特异性微调模型仍优于基于提示工程的LLMs。

Abstract: Moral foundation detection is crucial for analyzing social discourse and
developing ethically-aligned AI systems. While large language models excel
across diverse tasks, their performance on specialized moral reasoning remains
unclear.
  This study provides the first comprehensive comparison between
state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit
datasets using ROC, PR, and DET curve analysis.
  Results reveal substantial performance gaps, with LLMs exhibiting high false
negative rates and systematic under-detection of moral content despite prompt
engineering efforts. These findings demonstrate that task-specific fine-tuning
remains superior to prompting for moral reasoning applications.

</details>


### [41] [Effective Multi-Task Learning for Biomedical Named Entity Recognition](https://arxiv.org/abs/2507.18542)
*João Ruano,Gonçalo M. Correia,Leonor Barreiros,Afonso Mendes*

Main category: cs.CL

TL;DR: 本文提出SRU-NER方法，通过多任务学习和动态损失调整，有效解决了生物医学领域命名实体识别中嵌套实体和跨数据集标注不一致的挑战，并在多领域NER任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 生物医学命名实体识别（NER）面临术语复杂性和跨数据集标注不一致的重大挑战。

Method: 提出SRU-NER（Slot-based Recurrent Unit NER）新方法，该方法通过有效的多任务学习策略整合多个数据集来处理嵌套命名实体。它通过动态调整损失计算来缓解标注空白，避免对特定数据集中不存在的实体类型进行惩罚。

Result: 通过广泛实验（包括跨语料库评估和人工评估），SRU-NER在生物医学和通用领域NER任务中取得了有竞争力的性能，并改善了跨领域泛化能力。

Conclusion: SRU-NER是一种处理生物医学和通用领域命名实体识别的有效方法，尤其在处理嵌套实体和整合多源数据集方面表现突出，并具有良好的跨领域泛化能力。

Abstract: Biomedical Named Entity Recognition presents significant challenges due to
the complexity of biomedical terminology and inconsistencies in annotation
across datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER),
a novel approach designed to handle nested named entities while integrating
multiple datasets through an effective multi-task learning strategy. SRU-NER
mitigates annotation gaps by dynamically adjusting loss computation to avoid
penalizing predictions of entity types absent in a given dataset. Through
extensive experiments, including a cross-corpus evaluation and human assessment
of the model's predictions, SRU-NER achieves competitive performance in
biomedical and general-domain NER tasks, while improving cross-domain
generalization.

</details>


### [42] [GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface](https://arxiv.org/abs/2507.18546)
*Urchade Zaratiana,Gil Pasternak,Oliver Boyd,George Hurn-Maloney,Ash Lewis*

Main category: cs.CL

TL;DR: GLiNER2是一个统一且高效的框架，能够在一个模型中支持命名实体识别、文本分类和分层数据提取，同时保持CPU效率和部署便捷性。


<details>
  <summary>Details</summary>
Motivation: 现有信息提取解决方案通常需要针对不同任务的专用模型，或者依赖于计算成本高昂的大型语言模型（LLM）。

Method: GLiNER2是基于预训练Transformer编码器架构的GLiNER增强版，通过直观的Schema接口实现多任务组合，支持多种信息提取任务，并保持CPU效率和紧凑的模型尺寸。

Result: 实验表明，GLiNER2在提取和分类任务上均表现出有竞争力的性能，并且与基于LLM的替代方案相比，显著提高了部署的可访问性。

Conclusion: GLiNER2提供了一个高效、统一且易于部署的信息提取解决方案，解决了现有方法的局限性，并已作为开源库发布。

Abstract: Information extraction (IE) is fundamental to numerous NLP applications, yet
existing solutions often require specialized models for different tasks or rely
on computationally expensive large language models. We present GLiNER2, a
unified framework that enhances the original GLiNER architecture to support
named entity recognition, text classification, and hierarchical structured data
extraction within a single efficient model. Built pretrained transformer
encoder architecture, GLiNER2 maintains CPU efficiency and compact size while
introducing multi-task composition through an intuitive schema-based interface.
Our experiments demonstrate competitive performance across extraction and
classification tasks with substantial improvements in deployment accessibility
compared to LLM-based alternatives. We release GLiNER2 as an open-source
pip-installable library with pre-trained models and documentation at
https://github.com/fastino-ai/GLiNER2.

</details>


### [43] [GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation](https://arxiv.org/abs/2507.18562)
*Jiafeng Xiong,Yuting Zhao*

Main category: cs.CL

TL;DR: 提出GIIFT框架，利用场景图和图注意力网络实现无图像推理的多模态机器翻译，性能超越现有方法并达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多模态机器翻译(MMT)方法在利用模态间隙方面面临挑战，且推理受限于训练时的多模态域，难以泛化到无图像场景。

Method: 构建新型多模态场景图以整合模态信息，并引入GIIFT，一个两阶段的图引导归纳式无图像MMT框架。该框架使用跨模态图注意力网络适配器学习统一融合空间中的多模态知识，并归纳推广至无图像翻译域。

Result: 在Multi30K数据集的英法和英德任务上，GIIFT超越现有方法并达到SOTA，即使在推理时没有图像。在WMT基准测试中，相对于无图像翻译基线有显著改进。

Conclusion: GIIFT框架在实现归纳式无图像推理的多模态机器翻译方面表现出强大能力，有效解决了现有MMT的泛化性问题。

Abstract: Multimodal Machine Translation (MMT) has demonstrated the significant help of
visual information in machine translation. However, existing MMT methods face
challenges in leveraging the modality gap by enforcing rigid visual-linguistic
alignment whilst being confined to inference within their trained multimodal
domains. In this work, we construct novel multimodal scene graphs to preserve
and integrate modality-specific information and introduce GIIFT, a two-stage
Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph
Attention Network adapter to learn multimodal knowledge in a unified fused
space and inductively generalize it to broader image-free translation domains.
Experimental results on the Multi30K dataset of English-to-French and
English-to-German tasks demonstrate that our GIIFT surpasses existing
approaches and achieves the state-of-the-art, even without images during
inference. Results on the WMT benchmark show significant improvements over the
image-free translation baselines, demonstrating the strength of GIIFT towards
inductive image-free inference.

</details>


### [44] [Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods](https://arxiv.org/abs/2507.18570)
*Ganesh Sapkota,Md Hasibur Rahman*

Main category: cs.CL

TL;DR: 本文提出一种新型混合分词策略，结合6-mer和BPE-600，显著提升DNA语言模型的预测性能，在下一k-mer预测任务中超越现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 传统k-mer分词虽能捕捉局部DNA结构，但存在词元分布不均和全局上下文理解不足的局限性。

Method: 结合独特的6-mer词元和通过600次BPE循环生成的BPE词元，构建混合词汇表。在此基础上训练基础DNA语言模型，并通过下一k-mer预测任务进行评估。

Result: 模型在下一k-mer预测任务中表现优异，3-mer准确率10.78%，4-mer 10.1%，5-mer 4.12%，性能超越NT、DNABERT2和GROVER等先进模型。

Conclusion: 该混合分词策略有效保留了DNA序列的局部结构和全局上下文信息，凸显了高级分词方法在基因组语言建模中的重要性，并为未来DNA序列分析和生物研究奠定了坚实基础。

Abstract: This paper presents a novel hybrid tokenization strategy that enhances the
performance of DNA Language Models (DLMs) by combining 6-mer tokenization with
Byte Pair Encoding (BPE-600). Traditional k-mer tokenization is effective at
capturing local DNA sequence structures but often faces challenges, including
uneven token distribution and a limited understanding of global sequence
context. To address these limitations, we propose merging unique 6mer tokens
with optimally selected BPE tokens generated through 600 BPE cycles. This
hybrid approach ensures a balanced and context-aware vocabulary, enabling the
model to capture both short and long patterns within DNA sequences
simultaneously. A foundational DLM trained on this hybrid vocabulary was
evaluated using next-k-mer prediction as a fine-tuning task, demonstrating
significantly improved performance. The model achieved prediction accuracies of
10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming
state-of-the-art models such as NT, DNABERT2, and GROVER. These results
highlight the ability of the hybrid tokenization strategy to preserve both the
local sequence structure and global contextual information in DNA modeling.
This work underscores the importance of advanced tokenization methods in
genomic language modeling and lays a robust foundation for future applications
in downstream DNA sequence analysis and biological research.

</details>


### [45] [Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs](https://arxiv.org/abs/2507.18578)
*Feng Hong,Geng Yu,Yushi Ye,Haicheng Huang,Huangjie Zheng,Ya Zhang,Yanfeng Wang,Jiangchao Yao*

Main category: cs.CL

TL;DR: 本文提出WINO，一种无需训练的可撤销解码算法，通过并行草稿-验证机制解决扩散大语言模型（DLLMs）的质量-速度权衡问题，显著提升了生成速度和性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型（DLLMs）在快速并行生成方面面临严重的质量-速度权衡，即更快的并行解码会导致显著的性能下降。这主要归因于标准解码的不可逆性，易导致错误的解码方向和早期错误上下文的累积。

Method: 引入WINO（Wide-In, Narrow-Out），一种无需训练的可撤销解码算法。WINO采用并行“草稿-验证”机制：它积极地生成多个词元草稿，同时利用模型的双向上下文验证这些草稿，并对可疑词元进行重新遮盖以供后续修正。

Result: WINO在开源DLLMs（如LLaDA和MMaDA）中显著改善了质量-速度权衡。例如，在GSM8K数学基准测试中，它将推理速度提高了6倍，同时准确率提升了2.58%；在Flickr30K图像描述任务中，实现了10倍的加速并提升了性能。

Conclusion: WINO成功地通过可撤销解码机制解决了扩散大语言模型的质量-速度权衡问题，并在多项任务中展现出卓越的性能提升和推理加速，证明了其优越性。

Abstract: Diffusion Large Language Models (DLLMs) have emerged as a compelling
alternative to Autoregressive models, designed for fast parallel generation.
However, existing DLLMs are plagued by a severe quality-speed trade-off, where
faster parallel decoding leads to significant performance degradation. We
attribute this to the irreversibility of standard decoding in DLLMs, which is
easily polarized into the wrong decoding direction along with early error
context accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO),
a training-free decoding algorithm that enables revokable decoding in DLLMs.
WINO employs a parallel draft-and-verify mechanism, aggressively drafting
multiple tokens while simultaneously using the model's bidirectional context to
verify and re-mask suspicious ones for refinement. Verified in open-source
DLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the
quality-speed trade-off. For instance, on the GSM8K math benchmark, it
accelerates inference by 6$\times$ while improving accuracy by 2.58%; on
Flickr30K captioning, it achieves a 10$\times$ speedup with higher performance.
More comprehensive experiments are conducted to demonstrate the superiority and
provide an in-depth understanding of WINO.

</details>


### [46] [System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition](https://arxiv.org/abs/2507.18580)
*Jiahao Wang,Ramen Liu,Longhui Zhang,Jing Li*

Main category: cs.CL

TL;DR: 本文提出了SRAG-MAV框架，通过任务重构、自检索增强生成和多轮累积投票，显著提升了中文细粒度仇恨言论识别（FGCHSR）的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在解决CCL25-Eval Task 10中的中文细粒度仇恨言论识别（FGCHSR）任务，以提升该领域的识别效果。

Method: 提出了SRAG-MAV框架，结合了：1) 任务重构（将四元组提取转化为三元组）；2) 自检索增强生成（SRAG，利用训练集动态检索生成上下文提示）；3) 多轮累积投票（MAV，通过多轮推理和投票提高输出稳定性和性能）。系统基于Qwen2.5-7B模型。

Result: 在STATE ToxiCN数据集上，该系统取得了26.66的Hard Score，48.35的Soft Score，以及37.505的Average Score。这显著优于基线模型，如GPT-4o（平均分15.63）和微调Qwen2.5-7B（平均分35.365）。

Conclusion: 所提出的SRAG-MAV框架在中文细粒度仇恨言论识别任务中表现出色，其性能显著超越了现有基线模型，证明了该框架的有效性。

Abstract: This paper presents our system for CCL25-Eval Task 10, addressing
Fine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel
SRAG-MAV framework that synergistically integrates task reformulation(TR),
Self-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting
(MAV). Our method reformulates the quadruplet extraction task into triplet
extraction, uses dynamic retrieval from the training set to create contextual
prompts, and applies multi-round inference with voting to improve output
stability and performance. Our system, based on the Qwen2.5-7B model, achieves
a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on
the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o
(Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The
code is available at https://github.com/king-wang123/CCL25-SRAG-MAV.

</details>


### [47] [AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs](https://arxiv.org/abs/2507.18584)
*Xiaopeng Ke,Hexuan Deng,Xuebo Liu,Jun Rao,Zhenxi Song,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: AQuilt是一个高效框架，能从无标签数据为专业领域LLM生成指令微调数据。它通过引入逻辑和自检，以17%的成本达到DeepSeek-V3的性能，并提高数据相关性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在专业领域表现不佳，而现有数据合成方法面临计算成本高、性能有限和泛化性差等挑战。

Method: 提出AQuilt框架，用于从无标签数据构建专业领域的指令微调数据。该框架融合了答案、问题、无标签数据、检查、逻辑和任务类型，通过结合逻辑和自检来增强模型的推理过程和自我检查能力，并通过可定制的任务指令生成高质量数据。

Result: 通过AQuilt构建了一个包含70.3万个示例的数据集，用于训练一个强大的数据合成模型。实验结果显示，AQuilt在仅消耗17%生产成本的情况下，性能与DeepSeek-V3相当。此外，AQuilt生成的数据对下游任务具有更高的相关性。

Conclusion: AQuilt框架有效地解决了专业领域LLMs的数据生成挑战，提供了一种经济高效且性能卓越的指令微调数据构建方法，显著提升了LLMs在特定领域的表现和数据质量。

Abstract: Despite the impressive performance of large language models (LLMs) in general
domains, they often underperform in specialized domains. Existing approaches
typically rely on data synthesis methods and yield promising results by using
unlabeled data to capture domain-specific features. However, these methods
either incur high computational costs or suffer from performance limitations,
while also demonstrating insufficient generalization across different tasks. To
address these challenges, we propose AQuilt, a framework for constructing
instruction-tuning data for any specialized domains from corresponding
unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,
and Task type. By incorporating logic and inspection, we encourage reasoning
processes and self-inspection to enhance model performance. Moreover,
customizable task instructions enable high-quality data generation for any
task. As a result, we construct a dataset of 703k examples to train a powerful
data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3
while utilizing just 17% of the production cost. Further analysis demonstrates
that our generated data exhibits higher relevance to downstream tasks. Source
code, models, and scripts are available at https://github.com/Krueske/AQuilt.

</details>


### [48] [TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards](https://arxiv.org/abs/2507.18618)
*Andreea Nica,Ivan Zakazov,Nicolas Mario Baldwin,Saibo Geng,Robert West*

Main category: cs.CL

TL;DR: 论文提出了TRPrompt框架，通过将文本反馈直接融入提示模型训练，统一了现有LLM提示优化方法，并在数学推理任务上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM提示优化方法分为基于文本反馈的免训练和基于数值奖励训练提示模型两类。本研究旨在通过将文本反馈引入提示模型训练，实现两者的统一，以提升LLM的推理能力，且无需修改LLM参数。

Method: 提出Textual Reward Prompt (TRPrompt) 框架。该框架将文本反馈直接整合到提示模型的训练过程中，从而实现对生成提示的迭代改进，且无需预先收集数据集。

Result: TRPrompt框架能够训练出生成SOTA查询特定提示的提示模型。该方法在具有挑战性的数学数据集（GSMHard和MATH）上表现出色。

Conclusion: TRPrompt通过将文本反馈融入提示模型训练，有效统一了现有提示优化方法，显著提升了大型语言模型在复杂推理任务上的性能，尤其是在数学推理方面取得了最先进的结果。

Abstract: Prompt optimization improves the reasoning abilities of large language models
(LLMs) without requiring parameter updates to the target model. Following
heuristic-based "Think step by step" approaches, the field has evolved in two
main directions: while one group of methods uses textual feedback to elicit
improved prompts from general-purpose LLMs in a training-free way, a concurrent
line of research relies on numerical rewards to train a special prompt model,
tailored for providing optimal prompts to the target model. In this paper, we
introduce the Textual Reward Prompt framework (TRPrompt), which unifies these
approaches by directly incorporating textual feedback into training of the
prompt model. Our framework does not require prior dataset collection and is
being iteratively improved with the feedback on the generated prompts. When
coupled with the capacity of an LLM to internalize the notion of what a "good"
prompt is, the high-resolution signal provided by the textual rewards allows us
to train a prompt model yielding state-of-the-art query-specific prompts for
the problems from the challenging math datasets GSMHard and MATH.

</details>


### [49] [Checklists Are Better Than Reward Models For Aligning Language Models](https://arxiv.org/abs/2507.18624)
*Vijay Viswanathan,Yanchao Sun,Shuang Ma,Xiang Kong,Meng Cao,Graham Neubig,Tongshuang Wu*

Main category: cs.CL

TL;DR: 本研究提出了一种名为“基于清单反馈的强化学习”（RLCF）的新方法，通过使用灵活、指令特定的评估标准，显著提升了语言模型在多个指令遵循基准测试上的表现，克服了传统强化学习固定评估标准的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有使语言模型遵循指令的强化学习方法通常依赖于固定的评估标准（如“有用性”和“无害性”），这限制了其在引导模型遵循更广泛指令方面的效果。研究旨在引入更灵活、指令特定的标准来扩展强化学习的有效性。

Method: 提出RLCF方法。该方法从用户指令中提取出清单，并利用AI评判和专业验证程序评估模型响应对清单各项的满足程度。这些评估得分随后被组合起来，作为强化学习的奖励信号。

Result: RLCF方法在与Qwen2.5-7B-Instruct模型结合，并在五个广泛使用的基准测试上与其他对齐方法进行比较。结果显示，RLCF是唯一能在所有基准测试上都提升性能的方法，具体包括在FollowBench上硬性满足率提升4点，在InFoBench上提升6点，以及在Arena-Hard上胜率提升3点。

Conclusion: 研究结果表明，清单反馈是提高语言模型支持处理各种复杂用户需求查询能力的关键工具。

Abstract: Language models must be adapted to understand and follow user instructions.
Reinforcement learning is widely used to facilitate this -- typically using
fixed criteria such as "helpfulness" and "harmfulness". In our work, we instead
propose using flexible, instruction-specific criteria as a means of broadening
the impact that reinforcement learning can have in eliciting instruction
following. We propose "Reinforcement Learning from Checklist Feedback" (RLCF).
From instructions, we extract checklists and evaluate how well responses
satisfy each item - using both AI judges and specialized verifier programs -
then combine these scores to compute rewards for RL. We compare RLCF with other
alignment methods applied to a strong instruction following model
(Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only
method to improve performance on every benchmark, including a 4-point boost in
hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a
3-point rise in win rate on Arena-Hard. These results establish checklist
feedback as a key tool for improving language models' support of queries that
express a multitude of needs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [50] [Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling](https://arxiv.org/abs/2507.17801)
*Yi Xin,Juncheng Yan,Qi Qin,Zhen Li,Dongyang Liu,Shicheng Li,Victor Shea-Jay Huang,Yupeng Zhou,Renrui Zhang,Le Zhuo,Tiancheng Han,Xiaoqing Sun,Siqi Luo,Mengmeng Wang,Bin Fu,Yuewen Cao,Hongsheng Li,Guangtao Zhai,Xiaohong Liu,Yu Qiao,Peng Gao*

Main category: cs.CV

TL;DR: Lumina-mGPT 2.0是一个从零开始训练的纯自回归模型，在图像生成方面达到了最先进扩散模型的质量，并能以统一框架处理多任务。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成方法依赖于预训练组件或混合架构，这限制了模型设计自由和许可。本研究旨在重新审视并振兴自回归范式，以实现高质量图像生成及其它应用，同时保持固有的灵活性和组合性。

Method: Lumina-mGPT 2.0是一个独立的、仅解码器的自回归模型，完全从零开始训练。它采用统一的tokenization方案来无缝处理多种任务。为提高可用性，模型集成了推理时缩放（inference-time scaling）和推测性Jacobi采样（speculative Jacobi sampling）等高效解码策略。

Result: Lumina-mGPT 2.0在生成质量上达到了DALL-E 3和SANA等最先进扩散模型的水平。在标准文本到图像基准（如GenEval、DPG）上，其性能不仅匹配，在某些情况下甚至超越了基于扩散的模型。此外，模型在Graph200K基准上成功验证了其多任务处理能力，表现出色。

Conclusion: Lumina-mGPT 2.0被确立为统一多模态生成的一个强大、灵活的基础模型，展示了纯自回归范式在高质量图像生成和多任务处理方面的巨大潜力。

Abstract: We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model
that revisits and revitalizes the autoregressive paradigm for high-quality
image generation and beyond. Unlike existing approaches that rely on pretrained
components or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from
scratch, enabling unrestricted architectural design and licensing freedom. It
achieves generation quality on par with state-of-the-art diffusion models such
as DALL-E 3 and SANA, while preserving the inherent flexibility and
compositionality of autoregressive modeling. Our unified tokenization scheme
allows the model to seamlessly handle a wide spectrum of tasks-including
subject-driven generation, image editing, controllable synthesis, and dense
prediction-within a single generative framework. To further boost usability, we
incorporate efficient decoding strategies like inference-time scaling and
speculative Jacobi sampling to improve quality and speed, respectively.
Extensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG)
demonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses
diffusion-based models. Moreover, we confirm its multi-task capabilities on the
Graph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally
well. These results position Lumina-mGPT 2.0 as a strong, flexible foundation
model for unified multimodal generation. We have released our training details,
code, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.

</details>


### [51] [SV3.3B: A Sports Video Understanding Model for Action Recognition](https://arxiv.org/abs/2507.17844)
*Sai Varun Kodathala,Yashwanth Reddy Vutukoori,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 本文提出SV3.3B，一个轻量级体育视频理解模型，能高效生成详细动作描述，在降低计算需求的同时，其性能超越了GPT-4o等大型模型。


<details>
  <summary>Details</summary>
Motivation: 现有体育视频分析模型存在计算量大、需要服务器处理以及缺乏对细微运动和关键生物力学阶段（如准备、执行、收尾）的精细理解等局限性。

Method: 引入SV3.3B模型，一个3.3B参数的轻量级视频理解模型，结合新型时间运动差异采样与自监督学习，实现高效的设备端部署。该方法采用基于DWT-VGG16-LDA的关键帧提取机制（智能识别16个代表性帧），随后通过V-DWT-JEPA2编码器（经掩码去噪预训练）和一个针对体育动作描述生成微调的LLM解码器进行处理。

Result: 在NSVA篮球数据集子集上评估，SV3.3B在传统文本生成指标和体育特定评估标准上均表现卓越，超越包括GPT-4o变体在内的更大、闭源模型，同时计算需求显著降低。在真实值验证指标上比GPT-4o提升29.2%，并在信息密度、动作复杂度和测量精度等对综合体育分析至关重要的指标上也有显著改进。

Conclusion: SV3.3B模型在生成技术细节丰富、分析性强的体育描述方面表现出卓越能力，为全面的运动员分析提供了关键工具，并在实现高效能的同时，显著降低了计算成本。

Abstract: This paper addresses the challenge of automated sports video analysis, which
has traditionally been limited by computationally intensive models requiring
server-side processing and lacking fine-grained understanding of athletic
movements. Current approaches struggle to capture the nuanced biomechanical
transitions essential for meaningful sports analysis, often missing critical
phases like preparation, execution, and follow-through that occur within
seconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B
parameter video understanding model that combines novel temporal motion
difference sampling with self-supervised learning for efficient on-device
deployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction
mechanism that intelligently identifies the 16 most representative frames from
sports sequences, followed by a V-DWT-JEPA2 encoder pretrained through
mask-denoising objectives and an LLM decoder fine-tuned for sports action
description generation. Evaluated on a subset of the NSVA basketball dataset,
SV3.3B achieves superior performance across both traditional text generation
metrics and sports-specific evaluation criteria, outperforming larger
closed-source models including GPT-4o variants while maintaining significantly
lower computational requirements. Our model demonstrates exceptional capability
in generating technically detailed and analytically rich sports descriptions,
achieving 29.2% improvement over GPT-4o in ground truth validation metrics,
with substantial improvements in information density, action complexity, and
measurement precision metrics essential for comprehensive athletic analysis.
Model Available at https://huggingface.co/sportsvision/SV3.3B.

</details>


### [52] [Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models](https://arxiv.org/abs/2507.17853)
*Lifeng Chen,Jiner Wang,Zihao Pan,Beier Zhu,Xiaofeng Yang,Chi Zhang*

Main category: cs.CV

TL;DR: Detail++是一个无需训练的框架，通过渐进式细节注入策略和质心对齐损失，显著提升了文本到图像生成模型在处理复杂多主题提示时的属性绑定和一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像（T2I）生成模型取得了显著进展，但在处理包含多个具有不同属性的主题的复杂提示时，它们仍面临挑战。模型难以准确绑定属性到对应主题，并保持一致性。

Method: 本文提出Detail++框架，采用渐进式细节注入（PDI）策略。它将复杂提示分解为一系列简化的子提示，分阶段指导生成。该方法利用自注意力机制确保全局构图，随后进行精确细化。为实现属性与主题的准确绑定，利用交叉注意力机制，并在测试时引入质心对齐损失（Centroid Alignment Loss）以减少绑定噪声并增强属性一致性。

Result: 在T2I-CompBench和新构建的风格构成基准上进行的广泛实验表明，Detail++显著优于现有方法，尤其在涉及多个对象和复杂风格条件的场景中表现出色。

Conclusion: Detail++通过其创新的PDI策略和质心对齐损失，有效解决了T2I生成中处理复杂提示的难题，提升了多对象和复杂风格条件下的生成质量和属性一致性。

Abstract: Recent advances in text-to-image (T2I) generation have led to impressive
visual results. However, these models still face significant challenges when
handling complex prompt, particularly those involving multiple subjects with
distinct attributes. Inspired by the human drawing process, which first
outlines the composition and then incrementally adds details, we propose
Detail++, a training-free framework that introduces a novel Progressive Detail
Injection (PDI) strategy to address this limitation. Specifically, we decompose
a complex prompt into a sequence of simplified sub-prompts, guiding the
generation process in stages. This staged generation leverages the inherent
layout-controlling capacity of self-attention to first ensure global
composition, followed by precise refinement. To achieve accurate binding
between attributes and corresponding subjects, we exploit cross-attention
mechanisms and further introduce a Centroid Alignment Loss at test time to
reduce binding noise and enhance attribute consistency. Extensive experiments
on T2I-CompBench and a newly constructed style composition benchmark
demonstrate that Detail++ significantly outperforms existing methods,
particularly in scenarios involving multiple objects and complex stylistic
conditions.

</details>


### [53] [FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains](https://arxiv.org/abs/2507.17859)
*Muayad Abujabal,Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: 本文提出了FishDet-M，一个目前最大的水下鱼类检测统一基准，对28个模型进行了系统性基准测试，并引入了基于CLIP的模型选择框架以实现自适应部署。


<details>
  <summary>Details</summary>
Motivation: 水下图像中的鱼类检测对生态监测、水产养殖自动化和机器人感知至关重要，但目前面临数据集分散、成像条件异构和评估协议不一致等挑战，限制了实际部署。

Method: 1. 构建了FishDet-M，这是最大的统一鱼类检测基准，包含13个公开数据集，并统一为COCO格式的边界框和分割掩码标注。
2. 系统性地评估了28种主流目标检测模型（包括YOLO系列、R-CNN和DETR模型），使用mAP、AP@50、AP@75以及规模相关AP指标和推理性能（延迟、参数量）进行分析。
3. 提出了一种基于CLIP的模型选择框架，利用视觉-语言对齐实现对输入图像语义最匹配检测器的零样本动态选择。

Result: 1. 实验结果揭示了不同模型在FishDet-M上的检测性能差异，以及不同架构模型在准确性和效率之间的权衡。
2. 提出的CLIP模型选择框架无需集成计算即可实现高检测性能，为实时应用提供了可扩展的解决方案。

Conclusion: FishDet-M为复杂水下场景中的目标检测评估建立了一个标准化、可复现的平台。所有数据集、预训练模型和评估工具均已公开，旨在促进水下计算机视觉和智能海洋系统的未来研究。

Abstract: Accurate fish detection in underwater imagery is essential for ecological
monitoring, aquaculture automation, and robotic perception. However, practical
deployment remains limited by fragmented datasets, heterogeneous imaging
conditions, and inconsistent evaluation protocols. To address these gaps, we
present \textit{FishDet-M}, the largest unified benchmark for fish detection,
comprising 13 publicly available datasets spanning diverse aquatic environments
including marine, brackish, occluded, and aquarium scenes. All data are
harmonized using COCO-style annotations with both bounding boxes and
segmentation masks, enabling consistent and scalable cross-domain evaluation.
We systematically benchmark 28 contemporary object detection models, covering
the YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.
Evaluations are conducted using standard metrics including mAP, mAP@50, and
mAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and
inference profiling in terms of latency and parameter count. The results
highlight the varying detection performance across models trained on FishDet-M,
as well as the trade-off between accuracy and efficiency across models of
different architectures. To support adaptive deployment, we introduce a
CLIP-based model selection framework that leverages vision-language alignment
to dynamically identify the most semantically appropriate detector for each
input image. This zero-shot selection strategy achieves high performance
without requiring ensemble computation, offering a scalable solution for
real-time applications. FishDet-M establishes a standardized and reproducible
platform for evaluating object detection in complex aquatic scenes. All
datasets, pretrained models, and evaluation tools are publicly available to
facilitate future research in underwater computer vision and intelligent marine
systems.

</details>


### [54] [Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis](https://arxiv.org/abs/2507.17860)
*Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel*

Main category: cs.CV

TL;DR: 本研究利用生成式AI评估皮肤癌深度学习模型的公平性，发现合成数据在此方面有前景，但模型训练数据与合成数据生成基础数据不一致时评估存在挑战。


<details>
  <summary>Details</summary>
Motivation: 深度学习在皮肤癌筛查中潜力巨大，但其固有偏见可能带来风险。因此，评估和提升这些系统的公平性至关重要，尤其是在确保评估数据集能代表不同个体信息和少数群体方面存在挑战。

Method: 本研究利用最先进的生成式AI（GenAI）LightningDiT模型，对公开可用的黑色素瘤分类器进行公平性评估。

Result: 研究结果表明，使用高度逼真的合成数据进行公平性评估是一个有前景的方向。然而，当用于评估的黑色素瘤检测模型所训练的数据与生成合成图像的数据集不同时，验证公平性变得困难。

Conclusion: 研究提出，其方法为利用合成数据评估和提升医学影像生成式AI系统的公平性提供了一条有价值的新途径。

Abstract: Recent advancements in Deep Learning and its application on the edge hold
great potential for the revolution of routine screenings for skin cancers like
Melanoma. Along with the anticipated benefits of this technology, potential
dangers arise from unforseen and inherent biases. Thus, assessing and improving
the fairness of such systems is of utmost importance. A key challenge in
fairness assessment is to ensure that the evaluation dataset is sufficiently
representative of different Personal Identifiable Information (PII) (sex, age,
and race) and other minority groups. Against the backdrop of this challenge,
this study leverages the state-of-the-art Generative AI (GenAI) LightningDiT
model to assess the fairness of publicly available melanoma classifiers. The
results suggest that fairness assessment using highly realistic synthetic data
is a promising direction. Yet, our findings indicate that verifying fairness
becomes difficult when the melanoma-detection model used for evaluation is
trained on data that differ from the dataset underpinning the synthetic images.
Nonetheless, we propose that our approach offers a valuable new avenue for
employing synthetic data to gauge and enhance fairness in medical-imaging GenAI
systems.

</details>


### [55] [DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration](https://arxiv.org/abs/2507.17892)
*Hanzhou Liu,Binghan Li,Chengkai Liu,Mi Lu*

Main category: cs.CV

TL;DR: 针对Transformer在图像复原中高计算成本和局部细节缺失问题，本文提出DiNAT-IR架构，通过结合扩张邻域注意力（DiNA）和通道感知模块，有效平衡全局上下文与局部精度，并在多个基准测试中取得竞争性结果。


<details>
  <summary>Details</summary>
Motivation: Transformer在图像复原中表现优异，但其自注意力机制计算成本高昂，限制了在高分辨率图像上的应用。现有解决方案（如Restormer的通道级自注意力）虽提高了效率，却可能忽略对高质量图像复原至关重要的局部伪影。

Method: 受扩张邻域注意力（DiNA）启发，探索其在平衡全局上下文和局部精度方面的潜力。针对DiNA直接应用于去模糊任务时全局上下文理解不足的问题，引入一个通道感知模块，以补充局部注意力，有效整合全局上下文，同时保持像素级精度。最终构建了名为DiNAT-IR的Transformer架构，专为图像复原设计。

Result: 所提出的DiNAT-IR架构在多个基准测试中取得了竞争性结果。

Conclusion: DiNAT-IR为各种低级计算机视觉问题提供了一种高质量的解决方案。

Abstract: Transformers, with their self-attention mechanisms for modeling long-range
dependencies, have become a dominant paradigm in image restoration tasks.
However, the high computational cost of self-attention limits scalability to
high-resolution images, making efficiency-quality trade-offs a key research
focus. To address this, Restormer employs channel-wise self-attention, which
computes attention across channels instead of spatial dimensions. While
effective, this approach may overlook localized artifacts that are crucial for
high-quality image restoration. To bridge this gap, we explore Dilated
Neighborhood Attention (DiNA) as a promising alternative, inspired by its
success in high-level vision tasks. DiNA balances global context and local
precision by integrating sliding-window attention with mixed dilation factors,
effectively expanding the receptive field without excessive overhead. However,
our preliminary experiments indicate that directly applying this global-local
design to the classic deblurring task hinders accurate visual restoration,
primarily due to the constrained global context understanding within local
attention. To address this, we introduce a channel-aware module that
complements local attention, effectively integrating global context without
sacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based
architecture specifically designed for image restoration, achieves competitive
results across multiple benchmarks, offering a high-quality solution for
diverse low-level computer vision problems.

</details>


### [56] [AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation](https://arxiv.org/abs/2507.17957)
*Md. Al-Masrur Khan,Durgakant Pushp,Lantao Liu*

Main category: cs.CV

TL;DR: 该研究提出了一种自适应特征细化（AFR）模块，用于无监督域自适应语义分割（UDA-SS），通过融合高低分辨率特征和高频信息，并利用不确定性驱动的注意力机制，解决了现有方法在平衡局部细节和全局上下文方面的不足，从而提高了分割精度并达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域自适应语义分割（UDA-SS）方法难以平衡细粒度局部细节与全局上下文信息，导致在复杂区域出现分割错误。

Method: 引入了自适应特征细化（AFR）模块。该模块通过使用低分辨率logit的语义先验来细化高分辨率特征，整合捕获细粒度结构和边界信息的高频分量，并通过不确定性驱动的注意力自适应平衡局部和全局信息。其轻量级设计可无缝集成到HRDA-based UDA方法中。

Result: 在GTA V --> Cityscapes数据集上将现有UDA-SS方法的mIoU提高了1.05%，在Synthia-->Cityscapes数据集上提高了1.04%，达到了最先进的分割性能。

Conclusion: 所提出的AFR模块有效解决了UDA-SS中局部与全局信息平衡的挑战，显著提升了分割精度和对象边界描绘能力，并实现了轻量化和良好的兼容性，达到了行业领先水平。

Abstract: In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is
trained on labeled source domain data (e.g., synthetic images) and adapted to
an unlabeled target domain (e.g., real-world images) without access to target
annotations. Existing UDA-SS methods often struggle to balance fine-grained
local details with global contextual information, leading to segmentation
errors in complex regions. To address this, we introduce the Adaptive Feature
Refinement (AFR) module, which enhances segmentation accuracy by refining
highresolution features using semantic priors from low-resolution logits. AFR
also integrates high-frequency components, which capture fine-grained
structures and provide crucial boundary information, improving object
delineation. Additionally, AFR adaptively balances local and global information
through uncertaintydriven attention, reducing misclassifications. Its
lightweight design allows seamless integration into HRDA-based UDA methods,
leading to state-of-the-art segmentation performance. Our approach improves
existing UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on
Synthia-->Cityscapes. The implementation of our framework is available at:
https://github.com/Masrur02/AFRDA

</details>


### [57] [OPEN: A Benchmark Dataset and Baseline for Older Adult Patient Engagement Recognition in Virtual Rehabilitation Learning Environments](https://arxiv.org/abs/2507.17959)
*Ali Abedi,Sadaf Safa,Tracey J. F. Colella,Shehroz S. Khan*

Main category: cs.CV

TL;DR: 论文介绍了一个名为OPEN的新数据集，用于通过AI技术识别老年人在虚拟学习中的参与度，该数据集包含多模态特征和上下文标注，并已成功用于训练AI模型。


<details>
  <summary>Details</summary>
Motivation: 虚拟学习中参与度识别面临挑战，尤其是在老年人群体中缺乏相关研究和专用数据集，且现有方法常忽视上下文和纵向性。

Method: 研究构建了OPEN数据集，从11名老年人每周为期六周的虚拟心脏康复课程中收集数据，共计35小时。数据集包含面部、手部、身体关键点地标以及情感和行为特征，并附带二元参与度状态、情感行为标签和上下文类型标注，同时提供不同采样长度版本。

Result: OPEN数据集是同类中最大的，包含超过35小时的数据。基于该数据集训练的机器学习和深度学习模型在参与度识别上实现了高达81%的准确率。

Conclusion: OPEN数据集为AI驱动的老年人群个性化参与度建模提供了可扩展的基础，并对更广泛的参与度识别研究做出了贡献。

Abstract: Engagement in virtual learning is essential for participant satisfaction,
performance, and adherence, particularly in online education and virtual
rehabilitation, where interactive communication plays a key role. Yet,
accurately measuring engagement in virtual group settings remains a challenge.
There is increasing interest in using artificial intelligence (AI) for
large-scale, real-world, automated engagement recognition. While engagement has
been widely studied in younger academic populations, research and datasets
focused on older adults in virtual and telehealth learning settings remain
limited. Existing methods often neglect contextual relevance and the
longitudinal nature of engagement across sessions. This paper introduces OPEN
(Older adult Patient ENgagement), a novel dataset supporting AI-driven
engagement recognition. It was collected from eleven older adults participating
in weekly virtual group learning sessions over six weeks as part of cardiac
rehabilitation, producing over 35 hours of data, making it the largest dataset
of its kind. To protect privacy, raw video is withheld; instead, the released
data include facial, hand, and body joint landmarks, along with affective and
behavioral features extracted from video. Annotations include binary engagement
states, affective and behavioral labels, and context-type indicators, such as
whether the instructor addressed the group or an individual. The dataset offers
versions with 5-, 10-, 30-second, and variable-length samples. To demonstrate
utility, multiple machine learning and deep learning models were trained,
achieving engagement recognition accuracy of up to 81 percent. OPEN provides a
scalable foundation for personalized engagement modeling in aging populations
and contributes to broader engagement recognition research.

</details>


### [58] [Bearded Dragon Activity Recognition Pipeline: An AI-Based Approach to Behavioural Monitoring](https://arxiv.org/abs/2507.17987)
*Arsen Yermukan,Pedro Machado,Feliciano Domingos,Isibor Kennedy Ihianle,Jordan J. Bird,Stefano S. K. Kaburu,Samantha J. Ward*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Traditional monitoring of bearded dragon (Pogona Viticeps) behaviour is
time-consuming and prone to errors. This project introduces an automated system
for real-time video analysis, using You Only Look Once (YOLO) object detection
models to identify two key behaviours: basking and hunting. We trained five
YOLO variants (v5, v7, v8, v11, v12) on a custom, publicly available dataset of
1200 images, encompassing bearded dragons (600), heating lamps (500), and
crickets (100). YOLOv8s was selected as the optimal model due to its superior
balance of accuracy (mAP@0.5:0.95 = 0.855) and speed. The system processes
video footage by extracting per-frame object coordinates, applying temporal
interpolation for continuity, and using rule-based logic to classify specific
behaviours. Basking detection proved reliable. However, hunting detection was
less accurate, primarily due to weak cricket detection (mAP@0.5 = 0.392).
Future improvements will focus on enhancing cricket detection through expanded
datasets or specialised small-object detectors. This automated system offers a
scalable solution for monitoring reptile behaviour in controlled environments,
significantly improving research efficiency and data quality.

</details>


### [59] [AG-VPReID.VIR: Bridging Aerial and Ground Platforms for Video-based Visible-Infrared Person Re-ID](https://arxiv.org/abs/2507.17995)
*Huy Nguyen,Kien Nguyen,Akila Pemasiri,Akmal Jahan,Clinton Fookes,Sridha Sridharan*

Main category: cs.CV

TL;DR: 为解决全天候监控中可见光与红外跨模态行人重识别的空中-地面视角问题，本研究引入了首个空中-地面跨模态视频行人重识别数据集AG-VPReID.VIR，并提出了新颖的三流架构TCC-VPReID，以应对视图和模态差异，实验证明其性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别（Re-ID）数据集主要关注地面视角，而地面红外系统面临遮挡、覆盖有限和易受阻碍等挑战。空中视角能有效弥补这些不足，因此需要解决空中-地面跨模态行人重识别问题，以实现更鲁棒的24小时监控系统。

Method: 1. **数据集构建：** 引入AG-VPReID.VIR，首个空中-地面跨模态视频行人Re-ID数据集，包含1,837个身份、4,861个轨迹（124,855帧），通过无人机和固定CCTV在RGB和红外模态下捕获。
2. **模型架构：** 提出TCC-VPReID，一种新颖的三流架构，旨在解决跨平台和跨模态Re-ID的联合挑战，通过风格鲁棒特征学习、基于记忆的跨视图适应和中介引导的时间建模来弥合域间差距。

Result: 1. AG-VPReID.VIR数据集与现有数据集相比，呈现出独特的挑战性。
2. 所提出的TCC-VPReID框架在多个评估协议下，实现了显著的性能提升。

Conclusion: 本研究通过创建AG-VPReID.VIR数据集和提出TCC-VPReID框架，有效应对了空中-地面跨模态行人重识别的复杂挑战，为全天候监控系统的发展提供了重要的支持和解决方案。

Abstract: Person re-identification (Re-ID) across visible and infrared modalities is
crucial for 24-hour surveillance systems, but existing datasets primarily focus
on ground-level perspectives. While ground-based IR systems offer nighttime
capabilities, they suffer from occlusions, limited coverage, and vulnerability
to obstructions--problems that aerial perspectives uniquely solve. To address
these limitations, we introduce AG-VPReID.VIR, the first aerial-ground
cross-modality video-based person Re-ID dataset. This dataset captures 1,837
identities across 4,861 tracklets (124,855 frames) using both UAV-mounted and
fixed CCTV cameras in RGB and infrared modalities. AG-VPReID.VIR presents
unique challenges including cross-viewpoint variations, modality discrepancies,
and temporal dynamics. Additionally, we propose TCC-VPReID, a novel
three-stream architecture designed to address the joint challenges of
cross-platform and cross-modality person Re-ID. Our approach bridges the domain
gaps between aerial-ground perspectives and RGB-IR modalities, through
style-robust feature learning, memory-based cross-view adaptation, and
intermediary-guided temporal modeling. Experiments show that AG-VPReID.VIR
presents distinctive challenges compared to existing datasets, with our
TCC-VPReID framework achieving significant performance gains across multiple
evaluation protocols. Dataset and code are available at
https://github.com/agvpreid25/AG-VPReID.VIR.

</details>


### [60] [Exploring the interplay of label bias with subgroup size and separability: A case study in mammographic density classification](https://arxiv.org/abs/2507.17996)
*Emma A. M. Stanley,Raghav Mehta,Mélanie Roschewitz,Nils D. Forkert,Ben Glocker*

Main category: cs.CV

TL;DR: 医疗影像数据中的子组标签偏差是一个被忽视的公平性问题。本研究发现，这种偏差会导致AI模型学习特征的显著偏移和性能下降，其影响程度与受影响子组的大小和可分离性有关，且验证集的标签质量对模型性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 医疗影像数据集中针对特定子组的系统性错误标记（即标签偏差）是一个与医疗AI系统公平性相关的、未被充分研究的问题。本研究旨在探讨标签偏差影响的子组大小和可分离性如何影响深度学习模型学习到的特征和性能。

Method: 研究人员使用EMory BrEast imaging Dataset (EMBED) 训练深度学习模型进行二元组织密度分类。他们模拟了标签偏差，分别影响可分离子组（基于影像制造商）和非可分离的“伪子组”。

Result: 研究发现，模拟的子组标签偏差导致模型学习到的特征表示发生显著偏移，且这些偏移取决于受标签偏差影响的子组的相对大小和可分离性。此外，子组性能表现出显著差异，这取决于是否使用带有干净标签的验证集来定义模型的分类阈值。例如，当标签偏差影响多数可分离子组时，若验证集带有偏置标签，该子组的真阳性率从0.898降至0.518。

Conclusion: 本工作是理解标签偏差对医疗影像AI中子组公平性影响的关键贡献。

Abstract: Systematic mislabelling affecting specific subgroups (i.e., label bias) in
medical imaging datasets represents an understudied issue concerning the
fairness of medical AI systems. In this work, we investigated how size and
separability of subgroups affected by label bias influence the learned features
and performance of a deep learning model. Therefore, we trained deep learning
models for binary tissue density classification using the EMory BrEast imaging
Dataset (EMBED), where label bias affected separable subgroups (based on
imaging manufacturer) or non-separable "pseudo-subgroups". We found that
simulated subgroup label bias led to prominent shifts in the learned feature
representations of the models. Importantly, these shifts within the feature
space were dependent on both the relative size and the separability of the
subgroup affected by label bias. We also observed notable differences in
subgroup performance depending on whether a validation set with clean labels
was used to define the classification threshold for the model. For instance,
with label bias affecting the majority separable subgroup, the true positive
rate for that subgroup fell from 0.898, when the validation set had clean
labels, to 0.518, when the validation set had biased labels. Our work
represents a key contribution toward understanding the consequences of label
bias on subgroup fairness in medical imaging AI.

</details>


### [61] [Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold](https://arxiv.org/abs/2507.17998)
*Jaeho Shin,Hyeonjae Gil,Junwoo Jang,Maani Ghaffari,Ayoung Kim*

Main category: cs.CV

TL;DR: 本文首次推导了在刚体变换下可优化的格拉斯曼特征代价函数，实现了鲁棒的配准并能找到全局最优解。


<details>
  <summary>Details</summary>
Motivation: 现有基于仿射格拉斯曼流形的邻近度测量方法无法提供作为刚体变换显式函数的距离，阻碍了其在需要可优化距离的配准问题中的应用。

Method: 本文针对刚体变换（$\mathbf{R}$ 和 $\mathbf{t}$）首次显式推导了两个格拉斯曼特征之间的可优化代价函数。通过严格的数学证明，展示了高维线性子空间基底可作为代价的显式表示，并基于变换后的基底提出了可用于任意仿射子空间配准的可优化代价函数。

Result: 与基于向量参数的方法相比，该方法通过直接最小化测地距离，能够找到全局最优解，且不受表示模糊性的影响。在各种计算机视觉任务中，所得到的代价函数及其在内点集最大化BnB求解器中的扩展，已被证明能改善现有解决方案的收敛性或超越它们。

Conclusion: 这项工作填补了刚体变换下格拉斯曼特征显式可优化距离函数的空白，显著增强了其在配准问题中的应用，实现了全局最优，并在计算机视觉任务中表现优异。

Abstract: Affine Grassmannian has been favored for expressing proximity between lines
and planes due to its theoretical exactness in measuring distances among
features. Despite this advantage, the existing method can only measure the
proximity without yielding the distance as an explicit function of rigid body
transformation. Thus, an optimizable distance function on the manifold has
remained underdeveloped, stifling its application in registration problems.
This paper is the first to explicitly derive an optimizable cost function
between two Grassmannian features with respect to rigid body transformation
($\mathbf{R}$ and $\mathbf{t}$). Specifically, we present a rigorous
mathematical proof demonstrating that the bases of high-dimensional linear
subspaces can serve as an explicit representation of the cost. Finally, we
propose an optimizable cost function based on the transformed bases that can be
applied to the registration problem of any affine subspace. Compared to vector
parameter-based approaches, our method is able to find a globally optimal
solution by directly minimizing the geodesic distance which is agnostic to
representation ambiguity. The resulting cost function and its extension to the
inlier-set maximizing \ac{BnB} solver have been demonstrated to improve the
convergence of existing solutions or outperform them in various computer vision
tasks. The code is available on
https://github.com/joomeok/GrassmannRegistration.

</details>


### [62] [GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures](https://arxiv.org/abs/2507.18009)
*Jake R. Patock,Nicole Catherine Lewis,Kevin McCoy,Christina Gomez,Canling Chen,Lorenzo Luzi*

Main category: cs.CV

TL;DR: 本文提出GRR-CoCa模型，通过引入大型语言模型（LLM）中的先进架构改进了CoCa模型，显著提升了视觉-语言任务的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态模型表现强劲，但其架构复杂性落后于当前LLM，表明通过借鉴LLM架构可进一步提升其性能。

Method: 研究者提出了GRR-CoCa，通过将高斯误差门控线性单元、均方根归一化和旋转位置嵌入集成到CoCa模型的文本解码器和视觉Transformer（ViT）编码器中。通过标准预训练和微调流程，将GRR-CoCa与基线CoCa（仅修改了文本解码器）在对比和生成任务上进行性能基准测试。

Result: GRR-CoCa在预训练数据集和三个多样化的微调数据集上均显著优于基线CoCa。预训练阶段，对比损失降低27.25%，困惑度降低3.71%，CoCa损失降低7.15%。平均微调阶段，对比损失降低13.66%，困惑度降低5.18%，CoCa损失降低5.55%。

Conclusion: GRR-CoCa的改进架构有效提升了模型在视觉-语言领域中的性能和泛化能力。

Abstract: State-of-the-art (SOTA) image and text generation models are multimodal
models that have many similarities to large language models (LLMs). Despite
achieving strong performances, leading foundational multimodal model
architectures frequently lag behind the architectural sophistication of
contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner
(CoCa) model that incorporates Gaussian error gated linear units, root mean
squared normalization, and rotary positional embedding into the textual
decoders and the vision transformer (ViT) encoder. Each architectural
modification has been shown to improve model performance in LLMs, but has yet
to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model
with the same modified textual decoders but with CoCa's original ViT encoder.
We used standard pretraining and fine-tuning workflows to benchmark the models
on contrastive and generative tasks. Our GRR-CoCa significantly outperformed
Baseline CoCa on the pretraining dataset and three diverse fine-tuning
datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in
perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were
13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We
show that GRR-CoCa's modified architecture improves performance and
generalization across vision-language domains.

</details>


### [63] [Celeb-DF++: A Large-scale Challenging Video DeepFake Benchmark for Generalizable Forensics](https://arxiv.org/abs/2507.18015)
*Yuezun Li,Delong Zhu,Xinjie Cui,Siwei Lyu*

Main category: cs.CV

TL;DR: DeepFake检测面临泛化性挑战，现有数据集多样性不足。本文推出新基准数据集Celeb-DF++，涵盖22种生成方法和3类伪造场景，旨在推动泛化性检测研究，并揭示现有方法的局限。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术发展，在线DeepFake视频种类激增，对“泛化取证”（即用单一模型检测多种未知DeepFake类型）构成严峻挑战。现有DeepFake数据集虽规模大但伪造多样性有限，不足以开发通用型检测方法。

Method: 本文在Celeb-DF数据集基础上，构建并发布了大规模、高挑战性的视频DeepFake基准数据集Celeb-DF++。该数据集涵盖换脸（FS）、面部重演（FR）和说话人脸（TF）三种常见伪造场景，其中高质量伪造视频由总计22种不同的最新DeepFake方法生成，涵盖了广泛的架构、生成流程和目标面部区域。同时，引入评估协议，用于衡量24种最新检测方法的泛化性。

Result: 通过引入的评估协议，本文揭示了现有DeepFake检测方法的局限性，并突出了新数据集Celeb-DF++在通用型取证任务上的高难度。

Conclusion: Celeb-DF++提供了一个解决通用型DeepFake取证挑战所需的、具有丰富伪造多样性的大规模数据集。该数据集及其评估协议为未来开发更具泛化能力的DeepFake检测方法提供了关键基准和研究方向。

Abstract: The rapid advancement of AI technologies has significantly increased the
diversity of DeepFake videos circulating online, posing a pressing challenge
for \textit{generalizable forensics}, \ie, detecting a wide range of unseen
DeepFake types using a single model. Addressing this challenge requires
datasets that are not only large-scale but also rich in forgery diversity.
However, most existing datasets, despite their scale, include only a limited
variety of forgery types, making them insufficient for developing generalizable
detection methods. Therefore, we build upon our earlier Celeb-DF dataset and
introduce {Celeb-DF++}, a new large-scale and challenging video DeepFake
benchmark dedicated to the generalizable forensics challenge. Celeb-DF++ covers
three commonly encountered forgery scenarios: Face-swap (FS), Face-reenactment
(FR), and Talking-face (TF). Each scenario contains a substantial number of
high-quality forged videos, generated using a total of 22 various recent
DeepFake methods. These methods differ in terms of architectures, generation
pipelines, and targeted facial regions, covering the most prevalent DeepFake
cases witnessed in the wild. We also introduce evaluation protocols for
measuring the generalizability of 24 recent detection methods, highlighting the
limitations of existing detection methods and the difficulty of our new
dataset.

</details>


### [64] [High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency and photorealistic details](https://arxiv.org/abs/2507.18023)
*Jun Zhou,Dinghao Li,Nannan Li,Mingjie Wang*

Main category: cs.CV

TL;DR: 本文提出一种新颖的3D高斯修复框架，利用稀疏修复视图，通过自动掩模细化和区域不确定性引导优化，实现完整3D场景重建，有效解决多视角一致性问题。


<details>
  <summary>Details</summary>
Motivation: 尽管NeRF和3DGS等技术显著提升了3D内容创建的质量和效率，但3D场景修复仍具挑战性，原因在于3D结构的不规则性以及维持多视角一致性的高要求。

Method: 本文提出了一个新颖的3D高斯修复框架，利用稀疏修复视图重建完整3D场景。该框架包含：1) 自动掩模细化过程，通过高斯场景过滤和反向投影实现精确遮挡区域定位和边界修复；2) 区域不确定性引导优化策略，通过在训练中评估多视角图像中各区域的重要性，缓解多视角不一致性并提升细节保真度。

Result: 在多个数据集上的综合实验表明，本文方法在视觉质量和视角一致性方面均优于现有最先进的方法。

Conclusion: 所提出的3D高斯修复框架通过其创新的掩模细化和不确定性引导优化机制，有效提升了3D场景修复的质量和多视角一致性，解决了当前技术的关键挑战。

Abstract: Recent advancements in multi-view 3D reconstruction and novel-view synthesis,
particularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting
(3DGS), have greatly enhanced the fidelity and efficiency of 3D content
creation. However, inpainting 3D scenes remains a challenging task due to the
inherent irregularity of 3D structures and the critical need for maintaining
multi-view consistency. In this work, we propose a novel 3D Gaussian inpainting
framework that reconstructs complete 3D scenes by leveraging sparse inpainted
views. Our framework incorporates an automatic Mask Refinement Process and
region-wise Uncertainty-guided Optimization. Specifically, we refine the
inpainting mask using a series of operations, including Gaussian scene
filtering and back-projection, enabling more accurate localization of occluded
regions and realistic boundary restoration. Furthermore, our Uncertainty-guided
Fine-grained Optimization strategy, which estimates the importance of each
region across multi-view images during training, alleviates multi-view
inconsistencies and enhances the fidelity of fine details in the inpainted
results. Comprehensive experiments conducted on diverse datasets demonstrate
that our approach outperforms existing state-of-the-art methods in both visual
quality and view consistency.

</details>


### [65] [Emotion Recognition from Skeleton Data: A Comprehensive Survey](https://arxiv.org/abs/2507.18026)
*Haifeng Lu,Jiuyi Chen,Zhen Zhang,Ruida Liu,Runhao Zeng,Xiping Hu*

Main category: cs.CV

TL;DR: 本篇综述对基于骨架的全身运动情感识别技术进行了全面系统回顾，涵盖心理模型、数据集、方法分类、统一技术范式、基准比较，并探讨了应用及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的面部表情或生理信号情感识别存在隐私问题，而基于身体动作的情感识别是一种保护隐私的替代方案。3D骨架获取和姿态估计技术的进步显著提升了全身运动情感识别的可行性。

Method: 本文采用综述方法，首先介绍情感心理模型及身体运动与情感的关系；其次总结公开数据集；然后将现有方法分为基于姿态和基于步态，并提出统一的四种技术范式（传统、Feat2Net、FeatFusionNet、End2EndNet）；最后审查比较代表性工作，并探讨其在心理健康评估中的应用及未来挑战。

Result: 作为一篇综述，本文系统地分类和回顾了骨架情感识别技术，提出了统一的技术范式，并比较了代表性工作的基准性能。阐明了该技术在心理健康评估（如抑郁症、自闭症检测）中的应用潜力，并明确了该领域的开放挑战和未来研究方向。

Conclusion: 基于骨架的情感识别是传统方法的有效替代，具有保护隐私的优势，并在心理健康评估等领域展现出巨大潜力。尽管已取得进展，但该领域仍面临挑战，本综述为未来的研究提供了全面的概述和方向指引。

Abstract: Emotion recognition through body movements has emerged as a compelling and
privacy-preserving alternative to traditional methods that rely on facial
expressions or physiological signals. Recent advancements in 3D skeleton
acquisition technologies and pose estimation algorithms have significantly
enhanced the feasibility of emotion recognition based on full-body motion. This
survey provides a comprehensive and systematic review of skeleton-based emotion
recognition techniques. First, we introduce psychological models of emotion and
examine the relationship between bodily movements and emotional expression.
Next, we summarize publicly available datasets, highlighting the differences in
data acquisition methods and emotion labeling strategies. We then categorize
existing methods into posture-based and gait-based approaches, analyzing them
from both data-driven and technical perspectives. In particular, we propose a
unified taxonomy that encompasses four primary technical paradigms: Traditional
approaches, Feat2Net, FeatFusionNet, and End2EndNet. Representative works
within each category are reviewed and compared, with benchmarking results
across commonly used datasets. Finally, we explore the extended applications of
emotion recognition in mental health assessment, such as detecting depression
and autism, and discuss the open challenges and future research directions in
this rapidly evolving field.

</details>


### [66] [ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks](https://arxiv.org/abs/2507.18031)
*Ahmad ALBarqawi,Mahmoud Nazzal,Issa Khalil,Abdallah Khreishah,NhatHai Phan*

Main category: cs.CV

TL;DR: ViGText是一种新颖的深度伪造检测方法，通过在基于图的框架中整合图像和Vision Large Language Model (VLLM)文本解释，显著提高了对复杂定制化伪造内容的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术迅速发展，传统检测方法难以有效应对复杂的、用户定制化的伪造内容，尤其在泛化能力和抵抗恶意攻击的鲁棒性方面表现不足，对媒体真实性构成威胁。

Method: ViGText将图像分割成多个图像块，构建图像图和VLLM生成的文本解释图，并利用图神经网络（GNN）对两者进行整合分析。该方法还采用跨空间和频率域的多级特征提取，以捕获更多细节，增强检测能力。

Result: 实验证明，ViGText显著增强了泛化能力，在检测用户定制化深度伪造时性能显著提升（泛化评估下F1分数从72.45%提高到98.32%）。在鲁棒性方面，其召回率比其他方法提高了11.1%，且在面对针对其图结构攻击时，分类性能下降幅度小于4%。

Conclusion: ViGText通过详细的视觉和文本分析，为深度伪造检测设定了新标准，有助于确保媒体的真实性和信息的完整性。

Abstract: The rapid rise of deepfake technology, which produces realistic but
fraudulent digital content, threatens the authenticity of media. Traditional
deepfake detection approaches often struggle with sophisticated, customized
deepfakes, especially in terms of generalization and robustness against
malicious attacks. This paper introduces ViGText, a novel approach that
integrates images with Vision Large Language Model (VLLM) Text explanations
within a Graph-based framework to improve deepfake detection. The novelty of
ViGText lies in its integration of detailed explanations with visual data, as
it provides a more context-aware analysis than captions, which often lack
specificity and fail to reveal subtle inconsistencies. ViGText systematically
divides images into patches, constructs image and text graphs, and integrates
them for analysis using Graph Neural Networks (GNNs) to identify deepfakes.
Through the use of multi-level feature extraction across spatial and frequency
domains, ViGText captures details that enhance its robustness and accuracy to
detect sophisticated deepfakes. Extensive experiments demonstrate that ViGText
significantly enhances generalization and achieves a notable performance boost
when it detects user-customized deepfakes. Specifically, average F1 scores rise
from 72.45% to 98.32% under generalization evaluation, and reflects the model's
superior ability to generalize to unseen, fine-tuned variations of stable
diffusion models. As for robustness, ViGText achieves an increase of 11.1% in
recall compared to other deepfake detection approaches. When facing targeted
attacks that exploit its graph-based architecture, ViGText limits
classification performance degradation to less than 4%. ViGText uses detailed
visual and textual analysis to set a new standard for detecting deepfakes,
helping ensure media authenticity and information integrity.

</details>


### [67] [Enhancing Scene Transition Awareness in Video Generation via Post-Training](https://arxiv.org/abs/2507.18046)
*Hanwen Shen,Jiajie Lu,Yupeng Cao,Xiaonan Yang*

Main category: cs.CV

TL;DR: 提出TAV数据集，提升AI视频生成模型在多场景视频过渡中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前AI视频生成模型在处理多场景视频和连贯场景过渡时表现不佳，主要因为它们无法从提示中推断出过渡需求，且多数模型在单场景数据集上训练，缺乏场景过渡感知能力。

Method: 本文提出“过渡感知视频（TAV）”数据集，该数据集包含预处理的、具有多个场景过渡的视频片段。模型在该数据集上进行后训练。

Result: 在TAV数据集上进行后训练后，模型对基于提示的场景过渡理解能力显著提升，所需场景与生成场景之间的差距缩小，同时保持了图像质量。

Conclusion: TAV数据集及其后训练方法有效解决了现有AI视频生成模型在生成具有连贯过渡的多场景视频方面的局限性，提升了其场景过渡理解能力。

Abstract: Recent advances in AI-generated video have shown strong performance on
\emph{text-to-video} tasks, particularly for short clips depicting a single
scene. However, current models struggle to generate longer videos with coherent
scene transitions, primarily because they cannot infer when a transition is
needed from the prompt. Most open-source models are trained on datasets
consisting of single-scene video clips, which limits their capacity to learn
and respond to prompts requiring multiple scenes. Developing scene transition
awareness is essential for multi-scene generation, as it allows models to
identify and segment videos into distinct clips by accurately detecting
transitions.
  To address this, we propose the \textbf{Transition-Aware Video} (TAV)
dataset, which consists of preprocessed video clips with multiple scene
transitions. Our experiment shows that post-training on the \textbf{TAV}
dataset improves prompt-based scene transition understanding, narrows the gap
between required and generated scenes, and maintains image quality.

</details>


### [68] [BokehDiff: Neural Lens Blur with One-Step Diffusion](https://arxiv.org/abs/2507.18060)
*Chengxuan Zhu,Qingnan Fan,Qi Zhang,Jinwei Chen,Huaqi Zhang,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: BokehDiff是一种新的镜头模糊渲染方法，利用生成扩散先验，实现物理精确且视觉吸引人的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于深度估计精度，在深度不连续处产生伪影，且缺乏可扩展的配对数据。

Method: 引入了物理启发式的自注意力模块，结合图像形成过程、景深依赖的弥散圈约束和自遮挡效应。将扩散模型应用于一步推理，不引入额外噪声。为解决数据稀缺，利用扩散模型合成带有透明度的真实感前景。

Result: 实现了高质量和高保真度的渲染结果。

Conclusion: BokehDiff通过创新的物理启发式方法和扩散模型适应，克服了现有方法的局限性，生成了物理精确且视觉吸引人的高质量镜头模糊效果。

Abstract: We introduce BokehDiff, a novel lens blur rendering method that achieves
physically accurate and visually appealing outcomes, with the help of
generative diffusion prior. Previous methods are bounded by the accuracy of
depth estimation, generating artifacts in depth discontinuities. Our method
employs a physics-inspired self-attention module that aligns with the image
formation process, incorporating depth-dependent circle of confusion constraint
and self-occlusion effects. We adapt the diffusion model to the one-step
inference scheme without introducing additional noise, and achieve results of
high quality and fidelity. To address the lack of scalable paired data, we
propose to synthesize photorealistic foregrounds with transparency with
diffusion models, balancing authenticity and scene diversity.

</details>


### [69] [Adapting Large VLMs with Iterative and Manual Instructions for Generative Low-light Enhancement](https://arxiv.org/abs/2507.18064)
*Xiaoran Sun,Liyan Wang,Cong Wang,Yeying Jin,Kin-man Lam,Zhixun Su,Yang Yang,Jinshan Pan*

Main category: cs.CV

TL;DR: 该论文提出VLM-IMI框架，通过利用大型视觉-语言模型（VLMs）和迭代手动指令（IMIs），将常光图像的语义指导融入低光图像增强（LLIE）过程，显著提升了在复杂光照条件下的增强效果。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强（LLIE）方法主要依赖预训练模型先验或低光输入，却忽视了常光图像提供的语义指导，这限制了它们在复杂光照条件下的有效性。

Method: 研究者提出了VLM-IMI框架，该框架利用大型视觉-语言模型（VLMs）并结合迭代手动指令（IMIs）进行LLIE。它将期望的常光内容的文本描述作为增强线索，以实现语义驱动的图像恢复。为有效整合跨模态先验，引入了一个指令先验融合模块，该模块动态对齐并融合图像与文本特征。在推理阶段，采用迭代手动指令策略来精炼文本指令，逐步提高视觉质量，从而增强结构保真度、语义对齐和精细细节恢复。

Result: 广泛的实验表明，VLM-IMI在定量指标和感知质量方面均优于现有最先进的低光图像增强方法。

Conclusion: VLM-IMI通过整合语义指导和创新的迭代指令策略，为低光图像增强提供了一种有效的新范式，显著提升了图像恢复的质量，尤其是在复杂和极端低光条件下。

Abstract: Most existing low-light image enhancement (LLIE) methods rely on pre-trained
model priors, low-light inputs, or both, while neglecting the semantic guidance
available from normal-light images. This limitation hinders their effectiveness
in complex lighting conditions. In this paper, we propose VLM-IMI, a novel
framework that leverages large vision-language models (VLMs) with iterative and
manual instructions (IMIs) for LLIE. VLM-IMI incorporates textual descriptions
of the desired normal-light content as enhancement cues, enabling semantically
informed restoration. To effectively integrate cross-modal priors, we introduce
an instruction prior fusion module, which dynamically aligns and fuses image
and text features, promoting the generation of detailed and semantically
coherent outputs. During inference, we adopt an iterative and manual
instruction strategy to refine textual instructions, progressively improving
visual quality. This refinement enhances structural fidelity, semantic
alignment, and the recovery of fine details under extremely low-light
conditions. Extensive experiments across diverse scenarios demonstrate that
VLM-IMI outperforms state-of-the-art methods in both quantitative metrics and
perceptual quality. The source code is available at
https://github.com/sunxiaoran01/VLM-IMI.

</details>


### [70] [TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound](https://arxiv.org/abs/2507.18082)
*Pascal Spiegler,Taha Koleilat,Arash Harirpoush,Corey S. Miller,Hassan Rivaz,Marta Kersten-Oertel,Yiming Xiao*

Main category: cs.CV

TL;DR: TextSAM-EUS是一种轻量级、文本驱动的SAM变体，用于自动分割EUS胰腺肿瘤，无需手动几何提示，表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 胰腺癌预后差，且EUS图像存在斑点噪声、低对比度和不直观外观，导致传统全监督深度学习模型在胰腺肿瘤分割时容易出错，并高度依赖大型专家标注数据集。

Method: 提出TextSAM-EUS，一个新型、轻量级、文本驱动的Segment Anything Model (SAM) 改进版。它通过BiomedCLIP文本编码器结合LoRA对SAM架构进行适应，实现文本提示学习，从而在推理时无需手动几何提示即可自动分割胰腺肿瘤，且仅调整了0.86%的总参数。

Result: 在胰腺内窥镜超声公共数据库上，TextSAM-EUS在自动提示下达到82.69% Dice和85.28% NSD，手动几何提示下达到83.10% Dice和85.70% NSD，均优于现有最先进的监督深度学习模型和基础模型（如SAM及其变体）。

Conclusion: TextSAM-EUS是首次将提示学习引入基于SAM的医学图像分割，为高效、鲁棒的自动EUS分割提供了一个实用选择。

Abstract: Pancreatic cancer carries a poor prognosis and relies on endoscopic
ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle
noise, low contrast, and unintuitive appearance of EUS make segmentation of
pancreatic tumors with fully supervised deep learning (DL) models both
error-prone and dependent on large, expert-curated annotation datasets. To
address these challenges, we present TextSAM-EUS, a novel, lightweight,
text-driven adaptation of the Segment Anything Model (SAM) that requires no
manual geometric prompts at inference. Our approach leverages text prompt
learning (context optimization) through the BiomedCLIP text encoder in
conjunction with a LoRA-based adaptation of SAM's architecture to enable
automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total
parameters. On the public Endoscopic Ultrasound Database of the Pancreas,
TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized
surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice
and 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised
DL models and foundation models (e.g., SAM and its variants). As the first
attempt to incorporate prompt learning in SAM-based medical image segmentation,
TextSAM-EUS offers a practical option for efficient and robust automatic EUS
segmentation. Our code will be publicly available upon acceptance.

</details>


### [71] [Comparison of Segmentation Methods in Remote Sensing for Land Use Land Cover](https://arxiv.org/abs/2507.18099)
*Naman Srivastava,Joel D Joy,Yash Dixit,Swarup E,Rakshit Ramesh*

Main category: cs.CV

TL;DR: 本研究评估了利用LUT大气校正和DeeplabV3+/CPS等先进机器学习模型对Cartosat MX影像进行土地利用土地覆盖（LULC）制图，并以印度海得拉巴为例，展示了其在城市规划中监测快速城市化导致土地变化的应用价值。


<details>
  <summary>Details</summary>
Motivation: LULC制图对于城市和资源规划以及智能可持续城市发展至关重要。研究旨在评估先进LULC制图技术的准确性和实用性，以应对快速城市化带来的土地利用变化挑战。

Method: 研究方法包括：1) 对Cartosat MX传感器图像应用基于查找表（LUT）的大气校正；2) 采用监督和半监督学习模型（DeeplabV3+和Cross-Pseudo Supervision, CPS）进行LULC预测，其中CPS模型通过动态加权优化伪标签可靠性。

Result: 通过对印度海得拉巴的案例研究，分析Cartosat MX图像随时间的变化，揭示了快速城市化导致的显著土地利用变化，如城市蔓延、绿地 shrinking 和工业区扩张。

Conclusion: 研究结果证明了所用LULC制图技术在监测城市土地变化方面的实用性，为城市规划者和政策制定者提供了宝贵的工具和见解。

Abstract: Land Use Land Cover (LULC) mapping is essential for urban and resource
planning, and is one of the key elements in developing smart and sustainable
cities.This study evaluates advanced LULC mapping techniques, focusing on
Look-Up Table (LUT)-based Atmospheric Correction applied to Cartosat
Multispectral (MX) sensor images, followed by supervised and semi-supervised
learning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo
Supervision (CPS). The CPS model is further refined with dynamic weighting,
enhancing pseudo-label reliability during training. This comprehensive approach
analyses the accuracy and utility of LULC mapping techniques for various urban
planning applications. A case study of Hyderabad, India, illustrates
significant land use changes due to rapid urbanization. By analyzing Cartosat
MX images over time, we highlight shifts such as urban sprawl, shrinking green
spaces, and expanding industrial areas. This demonstrates the practical utility
of these techniques for urban planners and policymakers.

</details>


### [72] [Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2507.15765)
*Feng-Qi Cui,Anyang Tong,Jinyang Huang,Jie Zhang,Dan Guo,Zhi Liu,Meng Wang*

Main category: cs.CV

TL;DR: 本文提出一种名为HDF的新框架，通过引入DAM和DSM模块，旨在解决动态面部表情识别中由数据异质性导致的性能下降问题，显著提升了识别准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有动态面部表情识别（DFER）方法在多源数据和个体表情变异造成的样本异质性下，性能会不可避免地下降。

Method: 提出异质性感知分布框架（HDF），包含两个即插即用模块：1) 时频分布注意力模块（DAM），通过双分支注意力设计捕获时间一致性和频率鲁棒性；2) 分布感知缩放模块（DSM），基于梯度敏感性和信息瓶颈原理，自适应平衡分类和对比损失，实现更稳定和判别性的表示学习。

Result: 在DFEW和FERV39k数据集上的实验表明，HDF显著提高了识别准确性和鲁棒性。该方法在多样化和不平衡场景下表现出优越的加权平均召回率（WAR）和未加权平均召回率（UAR），并保持了强大的泛化能力。

Conclusion: HDF框架通过有效处理数据异质性和优化不平衡问题，显著提升了动态面部表情识别的性能和泛化能力，为该领域提供了新的解决方案。

Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in
affective computing and human-computer interaction. Although existing methods
achieve comparable performance, they inevitably suffer from performance
degradation under sample heterogeneity caused by multi-source data and
individual expression variability. To address these challenges, we propose a
novel framework, called Heterogeneity-aware Distributional Framework (HDF), and
design two plug-and-play modules to enhance time-frequency modeling and
mitigate optimization imbalance caused by hard samples. Specifically, the
Time-Frequency Distributional Attention Module (DAM) captures both temporal
consistency and frequency robustness through a dual-branch attention design,
improving tolerance to sequence inconsistency and visual style shifts. Then,
based on gradient sensitivity and information bottleneck principles, an
adaptive optimization module Distribution-aware Scaling Module (DSM) is
introduced to dynamically balance classification and contrastive losses,
enabling more stable and discriminative representation learning. Extensive
experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF
significantly improves both recognition accuracy and robustness. Our method
achieves superior weighted average recall (WAR) and unweighted average recall
(UAR) while maintaining strong generalization across diverse and imbalanced
scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.

</details>


### [73] [Datasets and Recipes for Video Temporal Grounding via Reinforcement Learning](https://arxiv.org/abs/2507.18100)
*Ruizhe Chen,Zhiting Fan,Tianze Luo,Heqing Zou,Zhaopeng Feng,Guiyang Xie,Hansheng Zhang,Zhuochen Wang,Zuozhu Liu,Huaijian Zhang*

Main category: cs.CV

TL;DR: 本文提出一个两阶段训练框架（SFT+RL），旨在提高视频时序定位（VTG）模型的准确性和鲁棒性，并在多个基准测试中取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频时序定位（VTG）方法，即使是基于大型视觉语言模型，仍存在时间感知能力有限和泛化性差的问题。

Method: 引入一个两阶段训练框架：首先利用高质量精选的冷启动数据进行监督微调（SFT）初始化；随后，通过难度控制的强化学习（RL）进一步增强时间定位和推理能力。

Result: 该方法在多个VTG基准测试中持续优于现有模型，尤其在挑战性及开放域场景下表现突出。研究表明高质量冷启动数据和难度控制的强化学习至关重要。

Conclusion: 所提出的两阶段训练框架显著提升了VTG模型的准确性和鲁棒性。为促进后续研究和工业应用，作者已发布所有相关资源。

Abstract: Video Temporal Grounding (VTG) aims to localize relevant temporal segments in
videos given natural language queries. Despite recent progress with large
vision-language models (LVLMs) and instruction-tuning, existing approaches
often suffer from limited temporal awareness and poor generalization. In this
work, we introduce a two-stage training framework that integrates supervised
fine-tuning with reinforcement learning (RL) to improve both the accuracy and
robustness of VTG models. Our approach first leverages high-quality curated
cold start data for SFT initialization, followed by difficulty-controlled RL to
further enhance temporal localization and reasoning abilities. Comprehensive
experiments on multiple VTG benchmarks demonstrate that our method consistently
outperforms existing models, particularly in challenging and open-domain
scenarios. We conduct an in-depth analysis of training strategies and dataset
curation, highlighting the importance of both high-quality cold start data and
difficulty-controlled RL. To facilitate further research and industrial
adoption, we release all intermediate datasets, models, and code to the
community.

</details>


### [74] [A Multimodal Seq2Seq Transformer for Predicting Brain Responses to Naturalistic Stimuli](https://arxiv.org/abs/2507.18104)
*Qianyi He,Yuan Chang Leong*

Main category: cs.CV

TL;DR: 该研究提出了一种基于Transformer的序列到序列模型，利用多模态电影数据预测大脑fMRI活动。


<details>
  <summary>Details</summary>
Motivation: 响应Algonauts 2025挑战赛，开发编码模型以预测自然多模态电影刺激下的全脑fMRI响应。

Method: 提出了一种序列到序列的Transformer模型，自回归地从视觉、听觉和语言输入中预测fMRI活动。使用VideoMAE、HuBERT、Qwen和BridgeTower等预训练模型提取刺激特征。解码器通过双重交叉注意力机制整合了先前大脑状态、当前刺激和叙事级摘要。核心创新包括使用多模态上下文序列来捕捉长程时序结构，以及结合共享编码器与部分受试者特异性解码器。

Result: 该模型在同分布和异分布数据上均表现出强大的性能，证明了时序感知、多模态序列建模在预测大脑活动方面的有效性。

Conclusion: 研究结果证实，时序感知、多模态序列建模方法能有效预测大脑活动。

Abstract: The Algonauts 2025 Challenge called on the community to develop encoding
models that predict whole-brain fMRI responses to naturalistic multimodal
movies. In this submission, we propose a sequence-to-sequence Transformer that
autoregressively predicts fMRI activity from visual, auditory, and language
inputs. Stimulus features were extracted using pretrained models including
VideoMAE, HuBERT, Qwen, and BridgeTower. The decoder integrates information
from prior brain states, current stimuli, and episode-level summaries via dual
cross-attention mechanisms that attend to both perceptual information extracted
from the stimulus as well as narrative information provided by high-level
summaries of narrative content. One core innovation of our approach is the use
of sequences of multimodal context to predict sequences of brain activity,
enabling the model to capture long-range temporal structure in both stimuli and
neural responses. Another is the combination of a shared encoder with partial
subject-specific decoder, which leverages common structure across subjects
while accounting for individual variability. Our model achieves strong
performance on both in-distribution and out-of-distribution data, demonstrating
the effectiveness of temporally-aware, multimodal sequence modeling for brain
activity prediction. The code is available at
https://github.com/Angelneer926/Algonauts_challenge.

</details>


### [75] [Distributional Uncertainty for Out-of-Distribution Detection](https://arxiv.org/abs/2507.18106)
*JinYoung Kim,DaeUng Jo,Kimin Yun,Jeonghyo Song,Youngjoon Yoo*

Main category: cs.CV

TL;DR: 提出一种自由能后验网络（Free-Energy Posterior Network），通过基于Beta分布的密度估计和无需采样的损失集成，联合建模分布不确定性，实现语义上有效且计算高效的OOD检测和不确定性感知分割。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经网络的不确定性估计方法（如MC Dropout）在OOD检测中，常只关注模型或数据不确定性，未能与OOD检测的语义目标对齐，难以有效联合建模分布不确定性并识别OOD及错误分类区域。

Method: 本文提出自由能后验网络。核心方法包括：1) 一个基于Beta分布参数化的自由能密度估计器，实现对模糊或未见区域的细粒度不确定性估计；2) 将损失集成到后验网络中，允许直接从学习参数中估计不确定性，无需随机采样。该方法与残差预测分支（RPL）框架结合，利用Beta分布的方差使网络学习OOD区域，而非仅依赖后验能量阈值。

Result: 该方法实现了语义上有意义且计算高效的不确定性感知分割。其有效性已在Fishyscapes、RoadAnomaly和Segment-Me-If-You-Can等挑战性真实世界基准测试中得到验证。

Conclusion: 自由能后验网络通过联合建模分布不确定性，提供了一种新颖且高效的解决方案，可在无需随机采样的情况下，对OOD样本进行检测，并实现不确定性感知分割，特别适用于复杂真实世界场景。

Abstract: Estimating uncertainty from deep neural networks is a widely used approach
for detecting out-of-distribution (OoD) samples, which typically exhibit high
predictive uncertainty. However, conventional methods such as Monte Carlo (MC)
Dropout often focus solely on either model or data uncertainty, failing to
align with the semantic objective of OoD detection. To address this, we propose
the Free-Energy Posterior Network, a novel framework that jointly models
distributional uncertainty and identifying OoD and misclassified regions using
free energy. Our method introduces two key contributions: (1) a
free-energy-based density estimator parameterized by a Beta distribution, which
enables fine-grained uncertainty estimation near ambiguous or unseen regions;
and (2) a loss integrated within a posterior network, allowing direct
uncertainty estimation from learned parameters without requiring stochastic
sampling. By integrating our approach with the residual prediction branch (RPL)
framework, the proposed method goes beyond post-hoc energy thresholding and
enables the network to learn OoD regions by leveraging the variance of the Beta
distribution, resulting in a semantically meaningful and computationally
efficient solution for uncertainty-aware segmentation. We validate the
effectiveness of our method on challenging real-world benchmarks, including
Fishyscapes, RoadAnomaly, and Segment-Me-If-You-Can.

</details>


### [76] [T2VWorldBench: A Benchmark for Evaluating World Knowledge in Text-to-Video Generation](https://arxiv.org/abs/2507.18107)
*Yubin Chen,Xuyang Guo,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: 现有文生视频模型在世界知识理解和事实准确性方面存在不足。本文提出了首个系统性评估框架T2VWorldBench，并发现大多数模型无法生成符合世界知识的视频，揭示了该领域存在的关键能力差距。


<details>
  <summary>Details</summary>
Motivation: 尽管文生视频模型在生成视觉合理场景方面表现出色，但其在利用世界知识以确保语义一致性和事实准确性方面的能力尚未得到充分研究。

Method: 提出了T2VWorldBench，一个系统性评估框架，包含6个大类、60个子类和1200个提示词，涵盖物理、自然、活动、文化、因果关系和物体等广泛领域。该基准结合了人工评估和基于视觉语言模型(VLM)的自动化评估，并评估了10个最先进的文生视频模型。

Result: 评估发现，当前大多数文生视频模型无法理解世界知识，也无法生成真正符合事实的视频。

Conclusion: 当前文生视频模型在利用世界知识方面存在关键能力差距，这为构建具有强大常识推理和事实生成能力模型提供了重要的研究机会和切入点。

Abstract: Text-to-video (T2V) models have shown remarkable performance in generating
visually reasonable scenes, while their capability to leverage world knowledge
for ensuring semantic consistency and factual accuracy remains largely
understudied. In response to this challenge, we propose T2VWorldBench, the
first systematic evaluation framework for evaluating the world knowledge
generation abilities of text-to-video models, covering 6 major categories, 60
subcategories, and 1,200 prompts across a wide range of domains, including
physics, nature, activity, culture, causality, and object. To address both
human preference and scalable evaluation, our benchmark incorporates both human
evaluation and automated evaluation using vision-language models (VLMs). We
evaluated the 10 most advanced text-to-video models currently available,
ranging from open source to commercial models, and found that most models are
unable to understand world knowledge and generate truly correct videos. These
findings point out a critical gap in the capability of current text-to-video
models to leverage world knowledge, providing valuable research opportunities
and entry points for constructing models with robust capabilities for
commonsense reasoning and factual generation.

</details>


### [77] [Information Entropy-Based Framework for Quantifying Tortuosity in Meibomian Gland Uneven Atrophy](https://arxiv.org/abs/2507.18135)
*Kesheng Wang,Xiaoyu Chen,Chunlei He,Fenfen Li,Xinxin Yu,Dexing Kong,Shoujun Huang,Qi Dai*

Main category: cs.CV

TL;DR: 本研究提出一种基于信息熵的曲线扭曲度量化框架，通过与生物学参考曲线比较而非理想直线，提供更稳健的评估。在睑板腺萎缩均匀性评估中，该框架能有效区分螨虫阳性与阴性患者，展现出其在医学诊断中的潜力。


<details>
  <summary>Details</summary>
Motivation: 在医学图像分析中，精确量化曲线扭曲度对多种疾病的辅助诊断和病理评估至关重要。传统的扭曲度量化方法存在局限性，需要一种更适合医学数据、更稳健、更客观且不依赖理想直线比较的评估指标。

Method: 本研究提出一种基于信息熵的扭曲度量化框架，该框架整合了概率建模与熵理论，并结合曲线数据域变换。与传统方法（如曲率或弧弦比）不同，此方法通过将目标曲线与其指定的参考曲线进行比较来评估扭曲度，尤其适用于存在生物学合理参考曲线的医学数据。

Result: 通过数值模拟实验验证了方法的稳定性和有效性。将该框架应用于睑板腺萎缩空间均匀性量化时，结果显示螨虫阴性和螨虫阳性患者组之间在基于扭曲度的均匀性上存在显著差异，曲线下面积（AUC）为0.8768，敏感性为0.75，特异性为0.93。

Conclusion: 研究结果突显了所提出框架在曲线扭曲度分析中的临床实用性，并表明其作为医学诊断中定量形态评估的通用工具的巨大潜力。

Abstract: In the medical image analysis field, precise quantification of curve
tortuosity plays a critical role in the auxiliary diagnosis and pathological
assessment of various diseases. In this study, we propose a novel framework for
tortuosity quantification and demonstrate its effectiveness through the
evaluation of meibomian gland atrophy uniformity,serving as a representative
application scenario.
  We introduce an information entropy-based tortuosity quantification framework
that integrates probability modeling with entropy theory and incorporates
domain transformation of curve data. Unlike traditional methods such as
curvature or arc-chord ratio, this approach evaluates the tortuosity of a
target curve by comparing it to a designated reference curve. Consequently, it
is more suitable for tortuosity assessment tasks in medical data where
biologically plausible reference curves are available, providing a more robust
and objective evaluation metric without relying on idealized straight-line
comparisons.
  First, we conducted numerical simulation experiments to preliminarily assess
the stability and validity of the method. Subsequently, the framework was
applied to quantify the spatial uniformity of meibomian gland atrophy and to
analyze the difference in this uniformity between \textit{Demodex}-negative and
\textit{Demodex}-positive patient groups. The results demonstrated a
significant difference in tortuosity-based uniformity between the two groups,
with an area under the curve of 0.8768, sensitivity of 0.75, and specificity of
0.93. These findings highlight the clinical utility of the proposed framework
in curve tortuosity analysis and its potential as a generalizable tool for
quantitative morphological evaluation in medical diagnostics.

</details>


### [78] [Degradation-Consistent Learning via Bidirectional Diffusion for Low-Light Image Enhancement](https://arxiv.org/abs/2507.18144)
*Jinhong He,Minglong Xue,Zhipu Liu,Mingliang Zhou,Aoxiang Ning,Palaiahnakote Shivakumara*

Main category: cs.CV

TL;DR: 提出一种双向扩散优化机制，结合自适应特征交互和反射感知校正模块，有效提升弱光图像增强效果，超越现有技术，并能泛化到多样化降解场景。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型单向建模弱光图像降解，难以捕捉真实世界复杂降解模式，导致结构不一致和像素错位，无法生成符合人类视觉感知的图像。

Method: 提出双向扩散优化机制，联合建模低光到正常光和正常光到低光的降解过程；引入自适应特征交互块（AFI）精炼特征表示；设计反射感知校正模块（RACM）指导去噪后的色彩恢复和抑制过曝区域。

Result: 在多个基准数据集上，该方法在定量和定性评估中均超越了现有最佳方法，并能有效泛化到多样化的降解场景。

Conclusion: 本文提出的双向扩散方法能够有效感知光照和细节降解，生成高质量的、符合人类视觉感知的图像，是弱光图像增强领域的有效解决方案。

Abstract: Low-light image enhancement aims to improve the visibility of degraded images
to better align with human visual perception. While diffusion-based methods
have shown promising performance due to their strong generative capabilities.
However, their unidirectional modelling of degradation often struggles to
capture the complexity of real-world degradation patterns, leading to
structural inconsistencies and pixel misalignments. To address these
challenges, we propose a bidirectional diffusion optimization mechanism that
jointly models the degradation processes of both low-light and normal-light
images, enabling more precise degradation parameter matching and enhancing
generation quality. Specifically, we perform bidirectional diffusion-from
low-to-normal light and from normal-to-low light during training and introduce
an adaptive feature interaction block (AFI) to refine feature representation.
By leveraging the complementarity between these two paths, our approach imposes
an implicit symmetry constraint on illumination attenuation and noise
distribution, facilitating consistent degradation learning and improving the
models ability to perceive illumination and detail degradation. Additionally,
we design a reflection-aware correction module (RACM) to guide color
restoration post-denoising and suppress overexposed regions, ensuring content
consistency and generating high-quality images that align with human visual
perception. Extensive experiments on multiple benchmark datasets demonstrate
that our method outperforms state-of-the-art methods in both quantitative and
qualitative evaluations while generalizing effectively to diverse degradation
scenarios. Code at https://github.com/hejh8/BidDiff

</details>


### [79] [WaveMamba: Wavelet-Driven Mamba Fusion for RGB-Infrared Object Detection](https://arxiv.org/abs/2507.18173)
*Haodong Zhu,Wenhao Dong,Linlin Yang,Hong Li,Yuguang Yang,Yangyang Ren,Qingcheng Zhu,Zichao Feng,Changbai Li,Shaohui Lin,Runqi Wang,Xiaoyan Luo,Baochang Zhang*

Main category: cs.CV

TL;DR: 本文提出WaveMamba，一种基于离散小波变换(DWT)分解RGB和红外(IR)图像频率特征的跨模态融合方法，通过WaveMamba融合块(WMFB)实现低/高频子带的全面融合，并结合逆离散小波变换(IDWT)改进检测头，显著提升了目标检测性能，平均mAP提高了4.5%。


<details>
  <summary>Details</summary>
Motivation: 利用可见光(RGB)和红外(IR)图像的互补特性，以显著提升目标检测性能。

Method: 1. 提出了WaveMamba，一种跨模态融合方法。
2. 使用离散小波变换(DWT)分解RGB和IR图像，以集成其独特的互补频率特征。
3. 设计了WaveMamba融合块(WMFB)作为核心，实现低频和高频子带的全面融合。
4. WMFB中包含低频Mamba融合块(LMFB)，基于Mamba框架，先通过通道交换进行初步低频特征融合，再通过先进的门控注意力机制进行深度融合。
5. 高频特征通过“绝对最大”融合策略进行增强。
6. 提出了一个结合逆离散小波变换(IDWT)的改进检测头，以减少信息损失并产生最终检测结果。

Result: 1. 所提出的方法实现了显著的性能提升。
2. 性能超越了现有最先进的方法。
3. 在四个基准测试中，平均mAP提高了4.5%。

Conclusion: WaveMamba通过有效利用RGB和红外图像的互补频率特征，显著提高了目标检测的性能，证明了其在跨模态目标检测领域的有效性和优越性。

Abstract: Leveraging the complementary characteristics of visible (RGB) and infrared
(IR) imagery offers significant potential for improving object detection. In
this paper, we propose WaveMamba, a cross-modality fusion method that
efficiently integrates the unique and complementary frequency features of RGB
and IR decomposed by Discrete Wavelet Transform (DWT). An improved detection
head incorporating the Inverse Discrete Wavelet Transform (IDWT) is also
proposed to reduce information loss and produce the final detection results.
The core of our approach is the introduction of WaveMamba Fusion Block (WMFB),
which facilitates comprehensive fusion across low-/high-frequency sub-bands.
Within WMFB, the Low-frequency Mamba Fusion Block (LMFB), built upon the Mamba
framework, first performs initial low-frequency feature fusion with channel
swapping, followed by deep fusion with an advanced gated attention mechanism
for enhanced integration. High-frequency features are enhanced using a strategy
that applies an ``absolute maximum" fusion approach. These advancements lead to
significant performance gains, with our method surpassing state-of-the-art
approaches and achieving average mAP improvements of 4.5% on four benchmarks.

</details>


### [80] [Real-Time Object Detection and Classification using YOLO for Edge FPGAs](https://arxiv.org/abs/2507.18174)
*Rashed Al Amin,Roman Obermaisser*

Main category: cs.CV

TL;DR: 本文提出一种针对边缘FPGA优化的YOLOv5实时目标检测与分类系统，实现了高准确率和资源效率。


<details>
  <summary>Details</summary>
Motivation: 尽管现有深度学习方法在FPGA上表现出高精度和计算速度，但基于YOLO的目标检测系统在边缘FPGA平台上仍面临资源效率挑战。

Method: 提出了一种基于YOLOv5的资源高效实时目标检测与分类系统，并为FPGA部署进行了优化。系统在COCO和GTSRD数据集上训练，并在Xilinx Kria KV260 FPGA板上实现。

Result: 该系统实现了99%的分类准确率，功耗为3.5W，处理速度为9帧/秒。

Conclusion: 所提出的方法有效实现了面向边缘计算应用的实时、资源高效的目标检测与分类。

Abstract: Object detection and classification are crucial tasks across various
application domains, particularly in the development of safe and reliable
Advanced Driver Assistance Systems (ADAS). Existing deep learning-based methods
such as Convolutional Neural Networks (CNNs), Single Shot Detectors (SSDs), and
You Only Look Once (YOLO) have demonstrated high performance in terms of
accuracy and computational speed when deployed on Field-Programmable Gate
Arrays (FPGAs). However, despite these advances, state-of-the-art YOLO-based
object detection and classification systems continue to face challenges in
achieving resource efficiency suitable for edge FPGA platforms. To address this
limitation, this paper presents a resource-efficient real-time object detection
and classification system based on YOLOv5 optimized for FPGA deployment. The
proposed system is trained on the COCO and GTSRD datasets and implemented on
the Xilinx Kria KV260 FPGA board. Experimental results demonstrate a
classification accuracy of 99%, with a power consumption of 3.5W and a
processing speed of 9 frames per second (FPS). These findings highlight the
effectiveness of the proposed approach in enabling real-time,
resource-efficient object detection and classification for edge computing
applications.

</details>


### [81] [Unsupervised Domain Adaptation for 3D LiDAR Semantic Segmentation Using Contrastive Learning and Multi-Model Pseudo Labeling](https://arxiv.org/abs/2507.18176)
*Abhishek Kaushik,Norbert Haala,Uwe Soergel*

Main category: cs.CV

TL;DR: 针对3D LiDAR语义分割中的域偏移问题，本研究提出一种新型两阶段无监督域适应框架，结合对比学习预训练和多模型集成伪标签策略，无需目标域标注即可显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 3D LiDAR语义分割在面对不同域（如传感器类型、地理位置）时性能会显著下降。由于手动标注目标域数据成本高昂且不可行，因此亟需一种无需目标域标注的无监督域适应(UDA)方法来解决这一挑战。

Method: 本研究提出一个两阶段的无监督域适应框架。首先，通过在段级别进行无监督对比学习，预训练骨干网络以学习鲁棒、域不变的特征。其次，引入多模型伪标签策略，整合多种先进架构（包括投影、体素、混合和圆柱体方法）的预测，通过硬投票生成高质量、精炼的伪标签，以减少单一模型的偏差。最后，利用这些伪标签对预训练的网络进行微调。

Result: 实验结果表明，在从SemanticKITTI适应到无标签目标数据集（SemanticPOSS, SemanticSlamantic）时，本方法相较于直接迁移和单一模型UDA方法，分割精度有显著提高。

Conclusion: 结合对比预训练和精炼的集成伪标签策略，能够有效弥合复杂的域差异，并且无需目标域的额外标注，从而在实际应用中具有重要意义。

Abstract: Addressing performance degradation in 3D LiDAR semantic segmentation due to
domain shifts (e.g., sensor type, geographical location) is crucial for
autonomous systems, yet manual annotation of target data is prohibitive. This
study addresses the challenge using Unsupervised Domain Adaptation (UDA) and
introduces a novel two-stage framework to tackle it. Initially, unsupervised
contrastive learning at the segment level is used to pre-train a backbone
network, enabling it to learn robust, domain-invariant features without labels.
Subsequently, a multi-model pseudo-labeling strategy is introduced, utilizing
an ensemble of diverse state-of-the-art architectures (including projection,
voxel, hybrid, and cylinder-based methods). Predictions from these models are
aggregated via hard voting to generate high-quality, refined pseudo-labels for
the unlabeled target domain, mitigating single-model biases. The contrastively
pre-trained network is then fine-tuned using these robust pseudo-labels.
Experiments adapting from SemanticKITTI to unlabeled target datasets
(SemanticPOSS, SemanticSlamantic) demonstrate significant improvements in
segmentation accuracy compared to direct transfer and single-model UDA
approaches. These results highlight the effectiveness of combining contrastive
pre-training with refined ensemble pseudo-labeling for bridging complex domain
gaps without requiring target domain annotations.

</details>


### [82] [Differential-UMamba: Rethinking Tumor Segmentation Under Limited Data Scenarios](https://arxiv.org/abs/2507.18177)
*Dhruv Jain,Romain Modzelewski,Romain Hérault,Clement Chatelain,Eva Torfeh,Sebastien Thureau*

Main category: cs.CV

TL;DR: 针对数据稀缺的医学图像分割，本文提出了Diff-UMamba架构，其核心噪声抑制模块通过信号差分策略有效过滤噪声，显著提升了模型在低数据量下的分割准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺的场景中，深度学习模型容易过拟合噪声和无关模式，限制了其在医学图像分割任务中对未见样本的泛化能力。

Method: 本文提出了Diff-UMamba架构，它结合了UNet框架和Mamba机制。其核心是一个噪声抑制模块（NRM），该模块采用信号差分策略来抑制编码器中多余或无关的激活，从而过滤虚假特征并增强与任务相关的表示。

Result: 该架构实现了更高的分割精度和鲁棒性，特别是在低数据设置下。在MSD（肺和胰腺）和AIIB23等多个公共数据集上，比基线方法持续获得1-3%的性能提升。在BraTS-21数据集上进一步验证了其在有限数据条件下的表现。在一个小型内部非小细胞肺癌（NSCLC）GTV分割数据集上，相比基线提高了4-5%。

Conclusion: Diff-UMamba通过创新的噪声抑制机制，成功解决了数据稀缺场景下医学图像分割的过拟合问题，显著提高了分割的准确性、鲁棒性和模型的泛化能力。

Abstract: In data-scarce scenarios, deep learning models often overfit to noise and
irrelevant patterns, which limits their ability to generalize to unseen
samples. To address these challenges in medical image segmentation, we
introduce Diff-UMamba, a novel architecture that combines the UNet framework
with the mamba mechanism for modeling long-range dependencies. At the heart of
Diff-UMamba is a Noise Reduction Module (NRM), which employs a signal
differencing strategy to suppress noisy or irrelevant activations within the
encoder. This encourages the model to filter out spurious features and enhance
task-relevant representations, thereby improving its focus on clinically
meaningful regions. As a result, the architecture achieves improved
segmentation accuracy and robustness, particularly in low-data settings.
Diff-UMamba is evaluated on multiple public datasets, including MSD (lung and
pancreas) and AIIB23, demonstrating consistent performance gains of 1-3% over
baseline methods across diverse segmentation tasks. To further assess
performance under limited-data conditions, additional experiments are conducted
on the BraTS-21 dataset by varying the proportion of available training
samples. The approach is also validated on a small internal non-small cell lung
cancer (NSCLC) dataset for gross tumor volume (GTV) segmentation in cone beam
CT (CBCT), where it achieves a 4-5% improvement over the baseline.

</details>


### [83] [MatSSL: Robust Self-Supervised Representation Learning for Metallographic Image Segmentation](https://arxiv.org/abs/2507.18184)
*Hoang Hai Nam Nguyen,Phan Nguyen Duc Hieu,Ho Won Lee*

Main category: cs.CV

TL;DR: MatSSL是一种流线型自监督学习架构，通过门控特征融合，利用少量未标记数据在金属材料显微组织分析中实现高效适应和优异性能。


<details>
  <summary>Details</summary>
Motivation: 当前金属材料显微组织分析依赖监督学习，需针对新数据集重新训练且在少量标注样本下表现不稳定。现有自监督学习方法虽利用未标记数据，但多依赖大规模数据集。MatSSL旨在克服这些限制。

Method: MatSSL是一种流线型自监督学习（SSL）架构，其主干网的每个阶段都采用门控特征融合（Gated Feature Fusion）来有效整合多级表示。研究首先在小规模未标记数据集上进行自监督预训练，然后在多个基准数据集上对模型进行微调。

Result: 在MetalDAM数据集上，分割模型达到了69.13%的mIoU，优于ImageNet预训练编码器（66.73%）。在EBC数据集上，相比MicroNet预训练模型，平均mIoU提升近40%。

Conclusion: MatSSL能够利用少量未标记数据有效适应金相领域，同时保留从大规模自然图像预训练中学到的丰富且可迁移的特征。

Abstract: MatSSL is a streamlined self-supervised learning (SSL) architecture that
employs Gated Feature Fusion at each stage of the backbone to integrate
multi-level representations effectively. Current micrograph analysis of
metallic materials relies on supervised methods, which require retraining for
each new dataset and often perform inconsistently with only a few labeled
samples. While SSL offers a promising alternative by leveraging unlabeled data,
most existing methods still depend on large-scale datasets to be effective.
MatSSL is designed to overcome this limitation. We first perform
self-supervised pretraining on a small-scale, unlabeled dataset and then
fine-tune the model on multiple benchmark datasets. The resulting segmentation
models achieve 69.13% mIoU on MetalDAM, outperforming the 66.73% achieved by an
ImageNet-pretrained encoder, and delivers consistently up to nearly 40%
improvement in average mIoU on the Environmental Barrier Coating benchmark
dataset (EBC) compared to models pretrained with MicroNet. This suggests that
MatSSL enables effective adaptation to the metallographic domain using only a
small amount of unlabeled data, while preserving the rich and transferable
features learned from large-scale pretraining on natural images.

</details>


### [84] [TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance](https://arxiv.org/abs/2507.18192)
*Minghao Fu,Guo-Hua Wang,Xiaohao Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: 本文提出TeEFusion，一种高效的蒸馏方法，通过融合文本嵌入来降低文生图模型的推理成本，使学生模型在保持图像质量的同时，推理速度比教师模型快6倍。


<details>
  <summary>Details</summary>
Motivation: 当前文生图模型为确保高质量生成，依赖复杂的采样策略和无分类器指导（CFG）。然而，CFG需要两次前向传播，结合复杂的采样算法导致推理成本过高，限制了其实用性。

Method: 我们引入了TeEFusion（文本嵌入融合），一种新颖高效的蒸馏方法。该方法通过简单的线性操作融合条件和无条件文本嵌入，将指导强度直接融入文本嵌入中，从而在不增加额外参数的情况下重建所需的指导。同时，TeEFusion使学生模型能够学习教师模型通过复杂采样方法产生的输出。

Result: 在SD3等最先进模型上的大量实验表明，我们的方法使学生模型能够以更简单、更高效的采样策略，紧密模仿教师模型的性能。结果显示，学生模型的推理速度比教师模型快6倍，同时保持了与教师模型复杂采样方法相当的图像质量。

Conclusion: TeEFusion显著提高了文生图模型的推理效率，在保持图像生成质量的同时，大幅降低了计算成本，为高质量文生图模型的实际应用提供了高效解决方案。

Abstract: Recent advances in text-to-image synthesis largely benefit from sophisticated
sampling strategies and classifier-free guidance (CFG) to ensure high-quality
generation. However, CFG's reliance on two forward passes, especially when
combined with intricate sampling algorithms, results in prohibitively high
inference costs. To address this, we introduce TeEFusion (\textbf{Te}xt
\textbf{E}mbeddings \textbf{Fusion}), a novel and efficient distillation method
that directly incorporates the guidance magnitude into the text embeddings and
distills the teacher model's complex sampling strategy. By simply fusing
conditional and unconditional text embeddings using linear operations,
TeEFusion reconstructs the desired guidance without adding extra parameters,
simultaneously enabling the student model to learn from the teacher's output
produced via its sophisticated sampling approach. Extensive experiments on
state-of-the-art models such as SD3 demonstrate that our method allows the
student to closely mimic the teacher's performance with a far simpler and more
efficient sampling strategy. Consequently, the student model achieves inference
speeds up to 6$\times$ faster than the teacher model, while maintaining image
quality at levels comparable to those obtained through the teacher's complex
sampling approach. The code is publicly available at
\href{https://github.com/AIDC-AI/TeEFusion}{github.com/AIDC-AI/TeEFusion}.

</details>


### [85] [LEAF: Latent Diffusion with Efficient Encoder Distillation for Aligned Features in Medical Image Segmentation](https://arxiv.org/abs/2507.18214)
*Qilin Huang,Tianyu Lin,Zhiguang Chen,Fudan Zheng*

Main category: cs.CV

TL;DR: 我们提出了LEAF，一个基于潜在扩散模型的医学图像分割模型，通过改进微调过程和特征蒸馏，在不增加推理成本的情况下显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在医学图像分割中显示出潜力，但现有方法通常直接沿用原始训练流程，缺乏针对分割任务的特定调整。此外，常用的预训练扩散模型在特征提取方面仍有不足。

Method: 我们提出了LEAF模型，它基于潜在扩散模型。在微调阶段，我们用直接预测分割图取代了原始的噪声预测模式，以降低分割结果的方差。同时，我们采用了一种特征蒸馏方法，将卷积层的隐藏状态与基于Transformer的视觉编码器中的特征进行对齐。

Result: 实验结果表明，我们的方法在多个不同疾病类型的分割数据集上提升了原始扩散模型的性能。值得注意的是，该方法不改变模型架构，也不增加推理阶段的参数量或计算量，从而保持了高效率。

Conclusion: LEAF通过对扩散模型微调过程的创新性调整和特征蒸馏，有效解决了现有方法的局限性，显著提升了医学图像分割的性能，并保持了出色的效率。

Abstract: Leveraging the powerful capabilities of diffusion models has yielded quite
effective results in medical image segmentation tasks. However, existing
methods typically transfer the original training process directly without
specific adjustments for segmentation tasks. Furthermore, the commonly used
pre-trained diffusion models still have deficiencies in feature extraction.
Based on these considerations, we propose LEAF, a medical image segmentation
model grounded in latent diffusion models. During the fine-tuning process, we
replace the original noise prediction pattern with a direct prediction of the
segmentation map, thereby reducing the variance of segmentation results. We
also employ a feature distillation method to align the hidden states of the
convolutional layers with the features from a transformer-based vision encoder.
Experimental results demonstrate that our method enhances the performance of
the original diffusion model across multiple segmentation datasets for
different disease types. Notably, our approach does not alter the model
architecture, nor does it increase the number of parameters or computation
during the inference phase, making it highly efficient.

</details>


### [86] [3D Test-time Adaptation via Graph Spectral Driven Point Shift](https://arxiv.org/abs/2507.18225)
*Xin Wei,Qin Yang,Yijie Fang,Mingrui Zhu,Nannan Wang*

Main category: cs.CV

TL;DR: 提出GSDTTA，一种在图谱域进行测试时间适应的新方法，以高效解决3D点云分类中的领域漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时间适应（TTA）方法在处理3D点云时因其不规则和无序结构而受阻，且通常计算成本高昂并可能需要额外的训练数据。

Method: 本文提出图谱域测试时间适应（GSDTTA）方法，将目标域点云表示为异常值感知图，并通过图傅里叶变换（GFT）转换到图谱域。为提高效率，仅优化最低10%的频率分量，然后应用逆GFT（IGFT）重建适应后的点云。此过程由特征映射引导的自训练策略增强，迭代优化谱域调整和模型参数。

Result: 在基准数据集上的实验结果和消融研究表明，GSDTTA在3D点云分类中有效，并优于现有的TTA方法。

Conclusion: GSDTTA通过利用图谱域，为3D点云分类提供了一种新颖且高效的测试时间适应解决方案，有效提升了在领域漂移下的性能。

Abstract: While test-time adaptation (TTA) methods effectively address domain shifts by
dynamically adapting pre-trained models to target domain data during online
inference, their application to 3D point clouds is hindered by their irregular
and unordered structure. Current 3D TTA methods often rely on computationally
expensive spatial-domain optimizations and may require additional training
data. In contrast, we propose Graph Spectral Domain Test-Time Adaptation
(GSDTTA), a novel approach for 3D point cloud classification that shifts
adaptation to the graph spectral domain, enabling more efficient adaptation by
capturing global structural properties with fewer parameters. Point clouds in
target domain are represented as outlier-aware graphs and transformed into
graph spectral domain by Graph Fourier Transform (GFT). For efficiency,
adaptation is performed by optimizing only the lowest 10% of frequency
components, which capture the majority of the point cloud's energy. An inverse
GFT (IGFT) is then applied to reconstruct the adapted point cloud with the
graph spectral-driven point shift. This process is enhanced by an
eigenmap-guided self-training strategy that iteratively refines both the
spectral adjustments and the model parameters. Experimental results and
ablation studies on benchmark datasets demonstrate the effectiveness of GSDTTA,
outperforming existing TTA methods for 3D point cloud classification.

</details>


### [87] [DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in Collaborative Perception](https://arxiv.org/abs/2507.18237)
*Chengchang Tian,Jianwei Ma,Yan Huang,Zhanye Chen,Honghao Wei,Hui Zhang,Wei Hong*

Main category: cs.CV

TL;DR: 协同感知中的特征级融合受域间隙和时间错位影响，本文提出DATA网络，通过CDAM、PTAM和IFAM模块，系统地对齐特征并增强语义表示，实现了最先进的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 协同感知中特征级融合的有效性严重依赖输入特征质量。然而，硬件多样性、部署条件导致的域间隙，以及传输延迟导致的时间错位，均会显著降低特征质量，并在协作网络中产生累积效应。

Method: 本文提出域与时间对齐（DATA）网络，旨在系统地对齐特征并最大化其语义表示以进行融合。具体方法包括：1. **一致性保持域对齐模块（CDAM）**：通过近邻区域分层下采样和可观测性约束判别器减少域间隙。2. **渐进式时间对齐模块（PTAM）**：通过多尺度运动建模和两阶段补偿处理传输延迟。3. **实例聚焦特征聚合模块（IFAM）**：在对齐特征的基础上增强语义表示。

Result: DATA在三个典型数据集上实现了最先进的性能，并在存在严重通信延迟和姿态误差的情况下保持了鲁棒性。

Conclusion: DATA网络通过其创新的域与时间对齐机制，有效地解决了协同感知中特征级融合面临的质量挑战，显著提升了系统性能和鲁棒性，为未来协同感知系统提供了强大的基础。

Abstract: Feature-level fusion shows promise in collaborative perception (CP) through
balanced performance and communication bandwidth trade-off. However, its
effectiveness critically relies on input feature quality. The acquisition of
high-quality features faces domain gaps from hardware diversity and deployment
conditions, alongside temporal misalignment from transmission delays. These
challenges degrade feature quality with cumulative effects throughout the
collaborative network. In this paper, we present the Domain-And-Time Alignment
(DATA) network, designed to systematically align features while maximizing
their semantic representations for fusion. Specifically, we propose a
Consistency-preserving Domain Alignment Module (CDAM) that reduces domain gaps
through proximal-region hierarchical downsampling and observability-constrained
discriminator. We further propose a Progressive Temporal Alignment Module
(PTAM) to handle transmission delays via multi-scale motion modeling and
two-stage compensation. Building upon the aligned features, an Instance-focused
Feature Aggregation Module (IFAM) is developed to enhance semantic
representations. Extensive experiments demonstrate that DATA achieves
state-of-the-art performance on three typical datasets, maintaining robustness
with severe communication delays and pose errors. The code will be released at
https://github.com/ChengchangTian/DATA.

</details>


### [88] [DepthDark: Robust Monocular Depth Estimation for Low-Light Environments](https://arxiv.org/abs/2507.18243)
*Longjian Zeng,Zunjie Zhu,Rongfeng Lu,Ming Lu,Bolun Zheng,Chenggang Yan,Anke Xue*

Main category: cs.CV

TL;DR: 本文提出DepthDark模型，通过引入数据模拟模块和低光照PEFT策略，解决了低光照下单目深度估计中数据集匮乏和模型适应性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度估计基础模型主要针对日常光照条件，在低光照环境下性能显著下降。主要挑战在于缺乏大规模高质量的低光照配对深度数据集，以及有效的参数高效微调（PEFT）策略来适应低光照场景。

Method: 本文提出了DepthDark，一个鲁棒的低光照单目深度估计基础模型。具体方法包括：1) 引入炫光模拟模块和噪声模拟模块，以精确模拟夜间成像过程，生成高质量的低光照配对深度数据集。2) 提出一种有效的低光照PEFT策略，该策略利用光照引导和多尺度特征融合，以增强模型在低光照环境下的能力。

Result: DepthDark方法在极具挑战性的nuScenes-Night和RobotCar-Night数据集上实现了最先进的深度估计性能，并验证了其在有限训练数据和计算资源下的有效性。

Conclusion: DepthDark通过创新的数据模拟和高效的PEFT策略，有效解决了低光照下单目深度估计的挑战，显著提升了该场景下模型的性能和鲁棒性。

Abstract: In recent years, foundation models for monocular depth estimation have
received increasing attention. Current methods mainly address typical daylight
conditions, but their effectiveness notably decreases in low-light
environments. There is a lack of robust foundational models for monocular depth
estimation specifically designed for low-light scenarios. This largely stems
from the absence of large-scale, high-quality paired depth datasets for
low-light conditions and the effective parameter-efficient fine-tuning (PEFT)
strategy. To address these challenges, we propose DepthDark, a robust
foundation model for low-light monocular depth estimation. We first introduce a
flare-simulation module and a noise-simulation module to accurately simulate
the imaging process under nighttime conditions, producing high-quality paired
depth datasets for low-light conditions. Additionally, we present an effective
low-light PEFT strategy that utilizes illumination guidance and multiscale
feature fusion to enhance the model's capability in low-light environments. Our
method achieves state-of-the-art depth estimation performance on the
challenging nuScenes-Night and RobotCar-Night datasets, validating its
effectiveness using limited training data and computing resources.

</details>


### [89] [LONG3R: Long Sequence Streaming 3D Reconstruction](https://arxiv.org/abs/2507.18255)
*Zhuoguang Chen,Minghui Qin,Tianyuan Yuan,Zhe Liu,Hang Zhao*

Main category: cs.CV

TL;DR: 提出LONG3R模型，用于长序列流式多视角3D场景重建，通过实时循环处理和创新记忆机制，克服现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多视角场景重建方法在处理图像流时面临挑战，如依赖耗时离线优化或仅限于短序列，难以满足实时应用需求。

Method: LONG3R模型采用循环操作实现实时处理，并维护更新记忆。具体方法包括：1) 记忆门控机制，用于过滤相关记忆并与新观测共同输入双源细化解码器进行粗到细交互。2) 3D时空记忆，用于有效捕获长序列记忆，动态剪枝冗余空间信息并自适应调整分辨率。3) 两阶段课程训练策略，以提升长序列性能和训练效率。

Result: 实验证明，LONG3R在长序列处理上优于现有的流式方法，同时保持实时推理速度。

Conclusion: LONG3R成功解决了长序列流式多视角3D场景重建的挑战，实现了高性能和实时性，为该领域提供了新的解决方案。

Abstract: Recent advancements in multi-view scene reconstruction have been significant,
yet existing methods face limitations when processing streams of input images.
These methods either rely on time-consuming offline optimization or are
restricted to shorter sequences, hindering their applicability in real-time
scenarios. In this work, we propose LONG3R (LOng sequence streaming 3D
Reconstruction), a novel model designed for streaming multi-view 3D scene
reconstruction over longer sequences. Our model achieves real-time processing
by operating recurrently, maintaining and updating memory with each new
observation. We first employ a memory gating mechanism to filter relevant
memory, which, together with a new observation, is fed into a dual-source
refined decoder for coarse-to-fine interaction. To effectively capture
long-sequence memory, we propose a 3D spatio-temporal memory that dynamically
prunes redundant spatial information while adaptively adjusting resolution
along the scene. To enhance our model's performance on long sequences while
maintaining training efficiency, we employ a two-stage curriculum training
strategy, each stage targeting specific capabilities. Experiments demonstrate
that LONG3R outperforms state-of-the-art streaming methods, particularly for
longer sequences, while maintaining real-time inference speed. Project page:
https://zgchen33.github.io/LONG3R/.

</details>


### [90] [Exploiting Gaussian Agnostic Representation Learning with Diffusion Priors for Enhanced Infrared Small Target Detection](https://arxiv.org/abs/2507.18260)
*Junyao Li,Yahao Lu,Xingyuan Guo,Xiaoyu Xian,Tiantian Wang,Yukai Shi*

Main category: cs.CV

TL;DR: 针对红外小目标检测(ISTD)在数据稀缺环境下的脆弱性，本文提出高斯无关表示学习和两阶段扩散模型，以增强模型韧性并生成高质量合成样本，有效提升了ISTD在数据受限场景的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的红外小目标检测(ISTD)方法过度依赖大量人工标注数据，导致其在真实世界中（尤其是在高质量红外数据稀缺时）表现脆弱。研究旨在探究并解决ISTD模型在数据稀缺下的性能瓶颈。

Method: 1. 提出了“高斯无关表示学习”(Gaussian Agnostic Representation Learning)。 2. 具体引入了“高斯群压缩器”(Gaussian Group Squeezer)，利用高斯采样和压缩进行非均匀量化，以增强模型韧性。 3. 引入了两阶段扩散模型进行真实世界重建，通过使量化信号与真实分布对齐，提升合成样本的质量和保真度。

Result: 在多种数据稀缺场景下，与最先进的检测方法进行对比评估，结果表明所提出的方法有效提升了红外小目标检测的性能。

Conclusion: 本研究通过提出的高斯无关表示学习和两阶段扩散模型，有效解决了红外小目标检测在数据稀缺环境下的鲁棒性问题，显著提升了模型在实际应用中的性能和合成样本的质量。

Abstract: Infrared small target detection (ISTD) plays a vital role in numerous
practical applications. In pursuit of determining the performance boundaries,
researchers employ large and expensive manual-labeling data for representation
learning. Nevertheless, this approach renders the state-of-the-art ISTD methods
highly fragile in real-world challenges. In this paper, we first study the
variation in detection performance across several mainstream methods under
various scarcity -- namely, the absence of high-quality infrared data -- that
challenge the prevailing theories about practical ISTD. To address this
concern, we introduce the Gaussian Agnostic Representation Learning.
Specifically, we propose the Gaussian Group Squeezer, leveraging Gaussian
sampling and compression for non-uniform quantization. By exploiting a diverse
array of training samples, we enhance the resilience of ISTD models against
various challenges. Then, we introduce two-stage diffusion models for
real-world reconstruction. By aligning quantized signals closely with
real-world distributions, we significantly elevate the quality and fidelity of
the synthetic samples. Comparative evaluations against state-of-the-art
detection methods in various scarcity scenarios demonstrate the efficacy of the
proposed approach.

</details>


### [91] [Dissecting the Dental Lung Cancer Axis via Mendelian Randomization and Mediation Analysis](https://arxiv.org/abs/2507.18287)
*Wenran Zhang,Huihuan Luo,Linda Wei,Ping Nie,Yiqun Wu,Dedong Yu*

Main category: cs.CV

TL;DR: 牙齿龋病与肺癌风险存在因果关联，部分通过肺功能下降介导；牙周炎无此关联。


<details>
  <summary>Details</summary>
Motivation: 观察性研究提示牙齿疾病与肺癌之间可能存在关联，但因果关系尚不明确。本研究旨在探究牙齿性状（牙周炎、牙齿龋病）与肺癌亚型之间的因果关系，并评估肺功能在其中的介导作用。

Method: 采用两样本孟德尔随机化（MR）方法。遗传工具源自大型全基因组关联研究（GWAS），包括487,823例龋病和506,594例牙周炎数据，以及肺癌数据。主要分析方法为逆方差加权法（IVW），肺功能介导作用通过delta法评估。

Result: 牙齿龋病对整体肺癌及其亚型具有显著正向因果效应。具体而言，牙齿龋病发病率每增加一个标准差，鳞状细胞肺癌风险增加188.0% (OR = 2.880, 95% CI = 1.236--6.713, p = 0.014)，其中5.124%至5.890%的总效应由肺活量（FVC）和一秒用力呼气容积（FEV1）下降部分介导。未发现牙周炎与肺癌存在因果关系。

Conclusion: 研究结果强调牙齿龋病在肺癌风险中具有因果作用，支持将牙齿护理和肺功能监测纳入癌症预防策略。

Abstract: Periodontitis and dental caries are common oral diseases affecting billions
globally. While observational studies suggest links between these conditions
and lung cancer, causality remains uncertain. This study used two sample
Mendelian randomization (MR) to explore causal relationships between dental
traits (periodontitis, dental caries) and lung cancer subtypes, and to assess
mediation by pulmonary function. Genetic instruments were derived from the
largest available genome wide association studies, including data from 487,823
dental caries and 506,594 periodontitis cases, as well as lung cancer data from
the Transdisciplinary Research of Cancer in Lung consortium. Inverse variance
weighting was the main analytical method; lung function mediation was assessed
using the delta method. The results showed a significant positive causal effect
of dental caries on overall lung cancer and its subtypes. Specifically, a one
standard deviation increase in dental caries incidence was associated with a
188.0% higher risk of squamous cell lung carcinoma (OR = 2.880, 95% CI =
1.236--6.713, p = 0.014), partially mediated by declines in forced vital
capacity (FVC) and forced expiratory volume in one second (FEV1), accounting
for 5.124% and 5.890% of the total effect. No causal effect was found for
periodontitis. These findings highlight a causal role of dental caries in lung
cancer risk and support integrating dental care and pulmonary function
monitoring into cancer prevention strategies.

</details>


### [92] [LMM-Det: Make Large Multimodal Models Excel in Object Detection](https://arxiv.org/abs/2507.18300)
*Jincheng Li,Chunyu Xie,Ji Ao,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: LMM-Det提出一种不依赖专业检测模块的方法，使大型多模态模型（LMMs）也能有效进行通用目标检测，弥补了与专业检测器的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管大型多模态模型（LMMs）在多模态理解和推理方面表现出色，但在目标检测能力上与专业的检测器存在显著差距。研究旨在弥补这一差距，探索LMMs的原生目标检测潜力。

Method: 提出LMM-Det，一种利用LMMs进行原生目标检测的方法，不依赖额外的检测模块。通过探索性分析发现LMMs的召回率不足，进而引入数据分布调整、推理优化，并重新组织指令对话，以提升LMMs的目标检测能力和召回率。

Result: 大量实验支持了作者的论断，即大型多模态模型本身具备检测能力。LMM-Det方法展示了其通用性和有效性，成功提升了LMMs的目标检测性能。

Conclusion: 大型多模态模型无需额外检测模块即可具备目标检测能力。通过针对性的数据分布调整、推理优化和指令组织，可以有效弥补其在目标检测方面与专业检测器之间的性能差距。

Abstract: Large multimodal models (LMMs) have garnered wide-spread attention and
interest within the artificial intelligence research and industrial
communities, owing to their remarkable capability in multimodal understanding,
reasoning, and in-context learning, among others. While LMMs have demonstrated
promising results in tackling multimodal tasks like image captioning, visual
question answering, and visual grounding, the object detection capabilities of
LMMs exhibit a significant gap compared to specialist detectors. To bridge the
gap, we depart from the conventional methods of integrating heavy detectors
with LMMs and propose LMM-Det, a simple yet effective approach that leverages a
Large Multimodal Model for vanilla object Detection without relying on
specialized detection modules. Specifically, we conduct a comprehensive
exploratory analysis when a large multimodal model meets with object detection,
revealing that the recall rate degrades significantly compared with specialist
detection models. To mitigate this, we propose to increase the recall rate by
introducing data distribution adjustment and inference optimization tailored
for object detection. We re-organize the instruction conversations to enhance
the object detection capabilities of large multimodal models. We claim that a
large multimodal model possesses detection capability without any extra
detection modules. Extensive experiments support our claim and show the
effectiveness of the versatile LMM-Det. The datasets, models, and codes are
available at https://github.com/360CVGroup/LMM-Det.

</details>


### [93] [Improving Large Vision-Language Models' Understanding for Field Data](https://arxiv.org/abs/2507.18311)
*Xiaomei Zhang,Hanyu Zheng,Xiangyu Zhu,Jinghuan Wei,Junhong Zou,Zhen Lei,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: FieldLVLM是一个新颖的框架，通过场感知语言生成和数据压缩多模态模型微调，显著提升了大型视觉-语言模型（LVLMs）在科学场数据理解方面的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉-语言模型（LVLMs）在图像字幕和视觉问答等任务中表现出色，但其在科学领域，特别是解释自然科学中常见的复杂场数据方面的应用仍未得到充分探索。

Method: 提出FieldLVLM框架，包含两个主要组件：1. 场感知语言生成策略：利用机器学习管道从场数据中提取关键物理特征并转换为结构化文本描述。2. 数据压缩多模态模型微调：使用生成的数据集对LVLMs进行微调，并采用数据压缩策略减少场输入复杂度，保留最有信息量的值。

Result: 在新建的基准数据集上，FieldLVLM在处理科学场数据的任务中显著优于现有方法。

Conclusion: 本研究为将大型视觉-语言模型应用于科学研究开辟了新途径，有助于弥合大型模型与领域特定发现之间的鸿沟。

Abstract: Large Vision-Language Models (LVLMs) have shown impressive capabilities
across a range of tasks that integrate visual and textual understanding, such
as image captioning and visual question answering. These models are trained on
large-scale image and video datasets paired with text, enabling them to bridge
visual perception and natural language processing. However, their application
to scientific domains, especially in interpreting complex field data commonly
used in the natural sciences, remains underexplored. In this work, we introduce
FieldLVLM, a novel framework designed to improve large vision-language models'
understanding of field data. FieldLVLM consists of two main components: a
field-aware language generation strategy and a data-compressed multimodal model
tuning. The field-aware language generation strategy leverages a
special-purpose machine learning pipeline to extract key physical features from
field data, such as flow classification, Reynolds number, and vortex patterns.
This information is then converted into structured textual descriptions that
serve as a dataset. The data-compressed multimodal model tuning focuses on
LVLMs with these generated datasets, using a data compression strategy to
reduce the complexity of field inputs and retain only the most informative
values. This ensures compatibility with the models language decoder and guides
its learning more effectively. Experimental results on newly proposed benchmark
datasets demonstrate that FieldLVLM significantly outperforms existing methods
in tasks involving scientific field data. Our findings suggest that this
approach opens up new possibilities for applying large vision-language models
to scientific research, helping bridge the gap between large models and
domain-specific discovery.

</details>


### [94] [A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in ECG Delineation](https://arxiv.org/abs/2507.18323)
*Minje Park,Jeonghwa Lim,Taehyung Yu,Sunghoon Joo*

Main category: cs.CV

TL;DR: 心电图（ECG）描绘因标注数据稀缺而受限，半监督学习有望解决此问题。本研究首次为ECG半监督语义分割建立系统基准，统一数据集、评估多种算法在CNN和Transformer上的表现，发现Transformer性能更优。


<details>
  <summary>Details</summary>
Motivation: 心电图（ECG）波形特征的描绘对临床诊断至关重要。尽管深度学习取得了进展，但由于公开标注数据集的稀缺性，其发展受到限制。半监督学习通过利用大量未标注的ECG数据，提供了一个有前景的解决方案。

Method: 1. 整合并统一了多个公共ECG数据集，包括以前未充分利用的来源。
2. 采纳了计算机视觉领域的五种代表性半监督语义分割（SemiSeg）算法。
3. 将这些算法在卷积网络和Transformer两种不同架构上实现，并在域内和跨域两种设置下进行评估。
4. 提出了ECG特有的训练配置和数据增强策略。
5. 引入了标准化的评估框架。

Result: 研究结果表明，在半监督ECG描绘任务中，Transformer架构的表现优于卷积网络。

Conclusion: 本研究建立的基准有望为推进半监督ECG描绘方法奠定基础，并促进该领域的进一步研究。

Abstract: Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform
features, is critical for clinical diagnosis. Despite recent advances using
deep learning, progress has been limited by the scarcity of publicly available
annotated datasets. Semi-supervised learning presents a promising solution by
leveraging abundant unlabeled ECG data. In this study, we present the first
systematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG
delineation. We curated and unified multiple public datasets, including
previously underused sources, to support robust and diverse evaluation. We
adopted five representative SemiSeg algorithms from computer vision,
implemented them on two different architectures: the convolutional network and
the transformer, and evaluated them in two different settings: in-domain and
cross-domain. Additionally, we propose ECG-specific training configurations and
augmentation strategies and introduce a standardized evaluation framework. Our
results show that the transformer outperforms the convolutional network in
semi-supervised ECG delineation. We anticipate that our benchmark will serve as
a foundation for advancing semi-supervised ECG delineation methods and will
facilitate further research in this domain.

</details>


### [95] [Beyond Low-rankness: Guaranteed Matrix Recovery via Modified Nuclear Norm](https://arxiv.org/abs/2507.18327)
*Jiangjun Peng,Yisi Luo,Xiangyong Cao,Shuang Xu,Deyu Meng*

Main category: cs.CV

TL;DR: 本研究提出了一种改进的核范数（MNN）框架，通过对变换后的矩阵应用核范数，实现了在矩阵恢复问题中同时捕获局部信息和全局低秩结构，并首次提供了精确的理论恢复保证。


<details>
  <summary>Details</summary>
Motivation: 现有的核范数（NN）方法主要利用数据的全局低秩结构。将局部信息与全局低秩性结合的现有方法通常需要权衡参数调整，并且无法提供精确的理论恢复保证。本研究旨在解决这些局限性。

Method: 引入了一种新的修改核范数（MNN）框架。MNN通过对原始矩阵进行适当的变换，然后对变换后的矩阵应用核范数来定义。这种方法旨在联合捕获数据的局部信息和全局低秩结构。

Result: 1. MNN框架无需权衡参数调整即可同时捕获局部信息和全局低秩性。2. 在对变换的温和假设下，为鲁棒主成分分析（Robust PCA）和矩阵补全（MC）任务提供了精确的理论恢复保证，这是现有结合局部和全局信息的方法所不具备的。3. MNN设计通用且灵活，能够适应各种已验证的变换。4. 大量实验证明了该方法的有效性。

Conclusion: MNN框架提供了一种统一且有效的结构化低秩恢复方法，通过巧妙地结合局部和全局信息，克服了现有方法的局限性，并提供了坚实的理论基础。

Abstract: The nuclear norm (NN) has been widely explored in matrix recovery problems,
such as Robust PCA and matrix completion, leveraging the inherent global
low-rank structure of the data. In this study, we introduce a new modified
nuclear norm (MNN) framework, where the MNN family norms are defined by
adopting suitable transformations and performing the NN on the transformed
matrix. The MNN framework offers two main advantages: (1) it jointly captures
both local information and global low-rankness without requiring trade-off
parameter tuning; (2) Under mild assumptions on the transformation, we provided
exact theoretical recovery guarantees for both Robust PCA and MC tasks-an
achievement not shared by existing methods that combine local and global
information. Thanks to its general and flexible design, MNN can accommodate
various proven transformations, enabling a unified and effective approach to
structured low-rank recovery. Extensive experiments demonstrate the
effectiveness of our method. Code and supplementary material are available at
https://github.com/andrew-pengjj/modified_nuclear_norm.

</details>


### [96] [GVCCS: A Dataset for Contrail Identification and Tracking on Visible Whole Sky Camera Sequences](https://arxiv.org/abs/2507.18330)
*Gabriel Jarry,Ramon Dalmau,Philippe Very,Franck Ballerini,Stephania-Denisa Bocu*

Main category: cs.CV

TL;DR: 本文提出了一个新型地面可见光凝结尾迹数据集（GVCCS），包含时间跟踪和航班源标识，并提供了一个统一的深度学习框架，旨在改进凝结尾迹监测和物理模型校准，以更准确评估航空气候影响。


<details>
  <summary>Details</summary>
Motivation: 飞机凝结尾迹对气候影响显著，现有物理模型受输入数据和复杂过程表示限制，且现有观测数据集缺乏时间跟踪和与源航班的关联，导致模型验证和校准不足。

Method: 1. 创建了一个开放的地面可见光凝结尾迹序列数据集（GVCCS），包含122个视频序列（24,228帧），对每个凝结尾迹进行个体标注、时间跟踪并关联源航班。2. 提出一个统一的深度学习框架，使用全景分割模型进行语义分割、实例分割和时间跟踪。

Result: 成功提供了高质量、时间解析的凝结尾迹标注数据集（GVCCS）和模型评估基准。此外，还提出了一个能集成凝结尾迹识别、分离和跟踪功能的深度学习模型。

Conclusion: 该工作通过提供高质量数据集和基准，支持改进凝结尾迹监测，有助于更好地校准物理模型，从而为更准确地理解和评估航空对气候的影响奠定基础。

Abstract: Aviation's climate impact includes not only CO2 emissions but also
significant non-CO2 effects, especially from contrails. These ice clouds can
alter Earth's radiative balance, potentially rivaling the warming effect of
aviation CO2. Physics-based models provide useful estimates of contrail
formation and climate impact, but their accuracy depends heavily on the quality
of atmospheric input data and on assumptions used to represent complex
processes like ice particle formation and humidity-driven persistence.
Observational data from remote sensors, such as satellites and ground cameras,
could be used to validate and calibrate these models. However, existing
datasets don't explore all aspect of contrail dynamics and formation: they
typically lack temporal tracking, and do not attribute contrails to their
source flights. To address these limitations, we present the Ground Visible
Camera Contrail Sequences (GVCCS), a new open data set of contrails recorded
with a ground-based all-sky camera in the visible range. Each contrail is
individually labeled and tracked over time, allowing a detailed analysis of its
lifecycle. The dataset contains 122 video sequences (24,228 frames) and
includes flight identifiers for contrails that form above the camera. As
reference, we also propose a unified deep learning framework for contrail
analysis using a panoptic segmentation model that performs semantic
segmentation (contrail pixel identification), instance segmentation (individual
contrail separation), and temporal tracking in a single architecture. By
providing high-quality, temporally resolved annotations and a benchmark for
model evaluation, our work supports improved contrail monitoring and will
facilitate better calibration of physical models. This sets the groundwork for
more accurate climate impact understanding and assessments.

</details>


### [97] [Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume Construction](https://arxiv.org/abs/2507.18331)
*Runmin Zhang,Zhu Yu,Si-Yuan Cao,Lingyu Zhu,Guangyi Zhang,Xiaokai Bai,Hui-Liang Shen*

Main category: cs.CV

TL;DR: SGCDet是一种基于自适应3D体素构建的新型多视角室内3D目标检测框架，通过几何与上下文感知聚合及稀疏体素构建，实现了高效且最先进的检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统3D目标检测方法限制体素感受野为图像上固定位置，导致表征能力不足且存在大量冗余计算。本研究旨在开发一种能自适应集成几何与上下文信息、动态调整视图贡献、并有效减少计算冗余的3D体素构建方法，以提高检测性能和效率，同时减少对场景几何真值的依赖。

Method: 提出SGCDet框架，基于自适应3D体素构建。核心方法包括：1) 引入几何与上下文感知聚合模块，在图像的自适应区域内整合几何和上下文信息，并动态调整来自不同视角的贡献，以增强体素特征的表征能力。2) 提出稀疏体素构建策略，自适应识别并选择高占用概率的体素进行特征细化，从而最小化自由空间中的冗余计算。此外，该网络仅通过3D边界框进行监督，无需场景几何真值。

Result: SGCDet在ScanNet、ScanNet200和ARKitScenes数据集上取得了最先进的性能。

Conclusion: SGCDet框架通过其独特的自适应体素构建、几何与上下文感知聚合以及稀疏体素策略，实现了高效且有效的3D体素构建，极大地提升了室内3D目标检测的性能，并且消除了对地面实况场景几何的依赖。

Abstract: This work presents SGCDet, a novel multi-view indoor 3D object detection
framework based on adaptive 3D volume construction. Unlike previous approaches
that restrict the receptive field of voxels to fixed locations on images, we
introduce a geometry and context aware aggregation module to integrate
geometric and contextual information within adaptive regions in each image and
dynamically adjust the contributions from different views, enhancing the
representation capability of voxel features. Furthermore, we propose a sparse
volume construction strategy that adaptively identifies and selects voxels with
high occupancy probabilities for feature refinement, minimizing redundant
computation in free space. Benefiting from the above designs, our framework
achieves effective and efficient volume construction in an adaptive way. Better
still, our network can be supervised using only 3D bounding boxes, eliminating
the dependence on ground-truth scene geometry. Experimental results demonstrate
that SGCDet achieves state-of-the-art performance on the ScanNet, ScanNet200
and ARKitScenes datasets. The source code is available at
https://github.com/RM-Zhang/SGCDet.

</details>


### [98] [Improving Bird Classification with Primary Color Additives](https://arxiv.org/abs/2507.18334)
*Ezhini Rasendiran R,Chandresh Kumar Maurya*

Main category: cs.CV

TL;DR: 本文提出一种通过将频率信息编码到彩色声谱图中，并结合深度学习模型来分类鸟类歌声的新方法，显著提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 鸟类歌声分类面临噪音、重叠叫声和标签缺失等挑战，现有模型在低信噪比或多物种录音方面表现不佳。尽管深度学习应用于声谱图有帮助，但不同物种间相似的“主题”导致混淆，因此需要一种更有效的方法来区分物种。

Method: 该研究通过可视化鸟类歌声的音高模式、速度和重复等“主题”，并在此基础上利用深度学习模型处理声谱图。为增强物种区分度并解决相似主题造成的混淆，通过原色添加剂将频率信息嵌入到声谱图中。

Result: 所提出的方法在统计学上显著优于未进行色彩化的模型，并超越了BirdCLEF 2024年的冠军模型。具体而言，F1分数提升7.3%，ROC-AUC提升6.2%，CMAP提升6.6%。

Conclusion: 研究结果表明，通过色彩化技术融合频率信息，对于鸟类物种分类是有效的。

Abstract: We address the problem of classifying bird species using their song
recordings, a challenging task due to environmental noise, overlapping
vocalizations, and missing labels. Existing models struggle with low-SNR or
multi-species recordings. We hypothesize that birds can be classified by
visualizing their pitch pattern, speed, and repetition, collectively called
motifs. Deep learning models applied to spectrogram images help, but similar
motifs across species cause confusion. To mitigate this, we embed frequency
information into spectrograms using primary color additives. This enhances
species distinction and improves classification accuracy. Our experiments show
that the proposed approach achieves statistically significant gains over models
without colorization and surpasses the BirdCLEF 2024 winner, improving F1 by
7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%. These results demonstrate the
effectiveness of incorporating frequency information via colorization.

</details>


### [99] [EgoExoBench: A Benchmark for First- and Third-person View Video Understanding in MLLMs](https://arxiv.org/abs/2507.18342)
*Yuping He,Yifei Huang,Guo Chen,Baoqi Pei,Jilan Xu,Tong Lu,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 研究引入EgoExoBench，首个用于评估多模态大语言模型（MLLMs）跨第一人称和第三人称视角理解与推理能力的基准。评估显示现有MLLMs在跨视角语义对齐、视角关联和时间推理方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 人类智能的核心能力之一是跨视角（第一人称和第三人称）知识的迁移和整合。然而，尽管多模态大语言模型（MLLMs）发展迅速，但其进行此类跨视角推理的能力尚未被充分探索和验证。

Method: 引入了EgoExoBench，这是首个用于第一人称-第三人称视频理解和推理的基准。该基准构建于公开数据集，包含超过7,300个问答对，涵盖语义对齐、视角关联和时间推理三大核心挑战下的11个子任务。研究评估了13个最先进的MLLMs。

Result: 评估发现，尽管当前MLLMs在单视角任务上表现出色，但它们在第一人称-第三人称情境下难以对齐跨视角的语义、准确关联视角并推断时间动态。

Conclusion: EgoExoBench有望为具身智能体和智能助手领域的研究提供有价值的资源，以推动实现类人跨视角智能。

Abstract: Transferring and integrating knowledge across first-person (egocentric) and
third-person (exocentric) viewpoints is intrinsic to human intelligence,
enabling humans to learn from others and convey insights from their own
experiences. Despite rapid progress in multimodal large language models
(MLLMs), their ability to perform such cross-view reasoning remains unexplored.
To address this, we introduce EgoExoBench, the first benchmark for
egocentric-exocentric video understanding and reasoning. Built from publicly
available datasets, EgoExoBench comprises over 7,300 question-answer pairs
spanning eleven sub-tasks organized into three core challenges: semantic
alignment, viewpoint association, and temporal reasoning. We evaluate 13
state-of-the-art MLLMs and find that while these models excel on single-view
tasks, they struggle to align semantics across perspectives, accurately
associate views, and infer temporal dynamics in the ego-exo context. We hope
EgoExoBench can serve as a valuable resource for research on embodied agents
and intelligent assistants seeking human-like cross-view intelligence.

</details>


### [100] [VB-Mitigator: An Open-source Framework for Evaluating and Advancing Visual Bias Mitigation](https://arxiv.org/abs/2507.18348)
*Ioannis Sarridis,Christos Koutlis,Symeon Papadopoulos,Christos Diou*

Main category: cs.CV

TL;DR: 本文提出了VB-Mitigator，一个开源框架，旨在统一和加速计算机视觉领域偏差缓解方法的研究、评估和比较，以解决当前研究碎片化和评估不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉模型中的偏差导致AI系统不公平、不可靠且泛化性差。当前偏差缓解研究因实现分散、评估实践不一致以及数据集和指标差异而进展受阻，使得方法重现和公平比较变得困难。

Method: 引入Visual Bias Mitigator (VB-Mitigator)，一个开源框架。该框架提供统一的研究环境，整合了12种已建立的偏差缓解方法和7个多样化的基准数据集，并具有高度可扩展性，可无缝集成更多方法、数据集、指标和模型。

Result: VB-Mitigator提供了一个统一的研发评估环境，集成了多种现有方法和数据集；推荐了最佳评估实践；并对现有先进方法进行了全面的性能比较。

Conclusion: VB-Mitigator旨在通过提供一个基础代码库，加速计算机视觉领域实现公平感知模型的研究，并促进研究社区开发和评估其方法。

Abstract: Bias in computer vision models remains a significant challenge, often
resulting in unfair, unreliable, and non-generalizable AI systems. Although
research into bias mitigation has intensified, progress continues to be
hindered by fragmented implementations and inconsistent evaluation practices.
Disparate datasets and metrics used across studies complicate reproducibility,
making it difficult to fairly assess and compare the effectiveness of various
approaches. To overcome these limitations, we introduce the Visual Bias
Mitigator (VB-Mitigator), an open-source framework designed to streamline the
development, evaluation, and comparative analysis of visual bias mitigation
techniques. VB-Mitigator offers a unified research environment encompassing 12
established mitigation methods, 7 diverse benchmark datasets. A key strength of
VB-Mitigator is its extensibility, allowing for seamless integration of
additional methods, datasets, metrics, and models. VB-Mitigator aims to
accelerate research toward fairness-aware computer vision models by serving as
a foundational codebase for the research community to develop and assess their
approaches. To this end, we also recommend best evaluation practices and
provide a comprehensive performance comparison among state-of-the-art
methodologies.

</details>


### [101] [Deformable Convolution Module with Globally Learned Relative Offsets for Fundus Vessel Segmentation](https://arxiv.org/abs/2507.18354)
*Lexuan Zhu,Yuxuan Li,Yuning Ren*

Main category: cs.CV

TL;DR: 提出一种新型可变形卷积模块，通过注意力机制和前馈网络学习特征图偏移，实现全局特征变形。应用于眼底血管分割，GDCUnet模型在公开数据集上达到SOTA性能，并增强了模型表示和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有可变形卷积难以有效捕捉长距离全局特征。针对眼底血管这类具有全局自相似复杂边缘的特征，需要更强的可变形能力。

Method: 开发了一种即插即用的可变形卷积模块。该模块利用注意力机制和前馈网络学习亚像素位移场，实现特征图的跨通道自适应扭曲，而非直接变形卷积核，从而达到全局特征变形并解耦核尺寸与学习网络。基于此模块，设计了GDCUnet模型用于眼底血管分割。

Result: GDCUnet在眼底血管分割公开数据集上取得了最先进的性能。消融实验证明，所提出的可变形卷积模块能更显著地学习眼底血管的复杂特征，提升了模型的表示和泛化能力。

Conclusion: 该可变形卷积模块有效捕捉了复杂全局自相似特征，在眼底血管分割中表现优异，并建议可应用于其他具有类似特征的机器视觉任务。

Abstract: Deformable convolution can adaptively change the shape of convolution kernel
by learning offsets to deal with complex shape features. We propose a novel
plug and play deformable convolutional module that uses attention and
feedforward networks to learn offsets, so that the deformable patterns can
capture long-distance global features. Compared with previously existing
deformable convolutions, the proposed module learns the sub pixel displacement
field and adaptively warps the feature maps across all channels rather than
directly deforms the convolution kernel , which is equivalent to a relative
deformation of the kernel sampling grids, achieving global feature deformation
and the decoupling of kernel size and learning network. Considering that the
fundus blood vessels have globally self similar complex edges, we design a deep
learning model for fundus blood vessel segmentation, GDCUnet, based on the
proposed convolutional module. Empirical evaluations under the same
configuration and unified framework show that GDCUnet has achieved state of the
art performance on public datasets. Further ablation experiments demonstrated
that the proposed deformable convolutional module could more significantly
learn the complex features of fundus blood vessels, enhancing the model
representation and generalization capabilities.The proposed module is similar
to the interface of conventional convolution, we suggest applying it to more
machine vision tasks with complex global self similar features.

</details>


### [102] [MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D Content Creation from a Single Image](https://arxiv.org/abs/2507.18371)
*Xiaotian Chen,DongFu Yin,Fei Richard Yu,Xuanchen Li,Xinhao Zhang*

Main category: cs.CV

TL;DR: MVG4D是一个新颖的框架，它结合多视角合成和4D高斯泼溅，能从单张静止图像生成高保真且时间一致的动态4D内容，解决了现有方法的时间不一致和背景退化问题，并在性能上超越了SOTA。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在2D、3D和4D内容创作方面取得显著进展，但生成高保真、时间一致的动态4D内容仍是挑战。现有基于4D高斯泼溅的方法存在运动不连续和背景退化问题。

Method: 提出MVG4D框架，从单张图像生成4D内容。核心是一个图像矩阵模块，用于合成时间连贯和空间多样的多视角图像。这些图像用于优化3D高斯点云，再通过轻量级形变网络扩展到时间域，形成4D内容。

Result: MVG4D有效增强了时间一致性、几何保真度和视觉真实感，解决了运动不连续和背景退化问题。在Objaverse数据集上，MVG4D在CLIP-I、PSNR、FVD和时间效率方面优于现有最先进基线，显著减少了闪烁伪影并锐化了结构细节。

Conclusion: MVG4D为从最小输入高效、可控地生成4D内容开辟了新方向，能实现更沉浸的AR/VR体验。

Abstract: Advances in generative modeling have significantly enhanced digital content
creation, extending from 2D images to complex 3D and 4D scenes. Despite
substantial progress, producing high-fidelity and temporally consistent dynamic
4D content remains a challenge. In this paper, we propose MVG4D, a novel
framework that generates dynamic 4D content from a single still image by
combining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core,
MVG4D employs an image matrix module that synthesizes temporally coherent and
spatially diverse multi-view images, providing rich supervisory signals for
downstream 3D and 4D reconstruction. These multi-view images are used to
optimize a 3D Gaussian point cloud, which is further extended into the temporal
domain via a lightweight deformation network. Our method effectively enhances
temporal consistency, geometric fidelity, and visual realism, addressing key
challenges in motion discontinuity and background degradation that affect prior
4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate
that MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and
time efficiency. Notably, it reduces flickering artifacts and sharpens
structural details across views and time, enabling more immersive AR/VR
experiences. MVG4D sets a new direction for efficient and controllable 4D
generation from minimal inputs.

</details>


### [103] [Towards Effective Human-in-the-Loop Assistive AI Agents](https://arxiv.org/abs/2507.18374)
*Filippos Bellos,Yayuan Li,Cary Shu,Ruey Day,Jeffrey M. Siskind,Jason J. Corso*

Main category: cs.CV

TL;DR: 物理任务中有效的人机协作潜力巨大但评估困难。本文提出了一个评估框架、多模态数据集和AR辅助AI代理，并通过人体研究证明AI指导能提升人类表现和任务完成度。


<details>
  <summary>Details</summary>
Motivation: 物理任务中高效的人机协作具有巨大潜力，AI代理提供的指导可以提升人类表现。然而，由于人机交互的复杂性，评估此类协作的有效性仍然具有挑战性。

Method: 1. 引入了一个评估框架和一个多模态人机交互数据集，旨在评估AI指导如何影响程序性任务表现、错误减少和学习成果。2. 开发了一个配备增强现实（AR）技术的AI代理，为现实世界任务（如烹饪、战地医疗）提供交互式指导。3. 通过人类研究（human studies）进行实证评估。

Result: 1. 获得了关于AI辅助人类表现的实证见解。2. 证明了AI辅助协作能够提高任务完成度。

Conclusion: AI辅助协作可以有效提升人类在程序性任务中的表现并促进任务完成，验证了其在现实世界场景中的有效性。

Abstract: Effective human-AI collaboration for physical task completion has significant
potential in both everyday activities and professional domains. AI agents
equipped with informative guidance can enhance human performance, but
evaluating such collaboration remains challenging due to the complexity of
human-in-the-loop interactions. In this work, we introduce an evaluation
framework and a multimodal dataset of human-AI interactions designed to assess
how AI guidance affects procedural task performance, error reduction and
learning outcomes. Besides, we develop an augmented reality (AR)-equipped AI
agent that provides interactive guidance in real-world tasks, from cooking to
battlefield medicine. Through human studies, we share empirical insights into
AI-assisted human performance and demonstrate that AI-assisted collaboration
improves task completion.

</details>


### [104] [Towards Consistent Long-Term Pose Generation](https://arxiv.org/abs/2507.18382)
*Yayuan Li,Filippos Bellos,Jason Corso*

Main category: cs.CV

TL;DR: 提出一种直接从图像和文本生成连续姿态的单阶段架构，通过相对运动预测和统一占位符规避中间表示问题，在长期姿态生成上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有姿态生成方法依赖中间表示（如两阶段量化或自回归），导致性能下降，尤其在长期姿态生成中因误差累积导致时间连贯性差。

Method: 提出一种新颖的单阶段架构，直接在连续坐标空间生成姿态。核心创新包括：通过相对运动预测机制直接操作姿态坐标；采用统一占位符Token方法实现训练与推理行为一致的单次前向生成，从而消除对中间表示或基于Token的生成的需求。输入为单张RGB图像和文本描述。

Result: 在Penn Action和F-PHAB数据集上的广泛实验表明，该方法显著优于现有基于量化和自回归的方法，尤其在长期姿态生成场景中表现更为出色。

Conclusion: 所提出的单阶段架构通过直接生成连续姿态，有效解决了现有方法在长期姿态生成中的局限性，并实现了显著超越现有技术的性能。

Abstract: Current approaches to pose generation rely heavily on intermediate
representations, either through two-stage pipelines with quantization or
autoregressive models that accumulate errors during inference. This fundamental
limitation leads to degraded performance, particularly in long-term pose
generation where maintaining temporal coherence is crucial. We propose a novel
one-stage architecture that directly generates poses in continuous coordinate
space from minimal context - a single RGB image and text description - while
maintaining consistent distributions between training and inference. Our key
innovation is eliminating the need for intermediate representations or
token-based generation by operating directly on pose coordinates through a
relative movement prediction mechanism that preserves spatial relationships,
and a unified placeholder token approach that enables single-forward generation
with identical behavior during training and inference. Through extensive
experiments on Penn Action and First-Person Hand Action Benchmark (F-PHAB)
datasets, we demonstrate that our approach significantly outperforms existing
quantization-based and autoregressive methods, especially in long-term
generation scenarios.

</details>


### [105] [HumanMaterial: Human Material Estimation from a Single Image via Progressive Training](https://arxiv.org/abs/2507.18385)
*Yu Jiang,Jiahao Xia,Jiongming Qin,Yusen Wang,Tuo Cao,Chunxia Xiao*

Main category: cs.CV

TL;DR: 该研究构建了高质量人体材料数据集OpenHumanBRDF，并提出了分阶段训练模型HumanMaterial，通过特制损失函数优化人体逆渲染中的材料估计，实现了更真实的全身人体渲染，尤其在皮肤表现上。


<details>
  <summary>Details</summary>
Motivation: 全身人体逆渲染旨在获取高质量材料以实现任意光照下的逼真渲染，但材料图缺乏约束使其成为病态问题。现有方法通过构建材料数据集缓解此问题，但其简化的材料数据和渲染方程导致渲染真实感有限，尤其在皮肤方面。

Method: 1. **数据集构建**: 构建了高质量数据集OpenHumanBRDF，基于扫描真实数据和统计材料数据，包含法线、漫反射反照率、粗糙度、镜面反射反照率、位移图和次表面散射，以增强渲染真实感，特别是皮肤。
2. **模型设计**: 提出了HumanMaterial模型和渐进式训练策略，以充分利用材料图的监督信息并改进材料估计性能。HumanMaterial首先通过三个先验模型获取初始材料结果，然后通过微调模型精炼结果。针对不同材料图对渲染结果的不同重要性，设计了受控PBR渲染损失（CPR loss），以在先验模型训练期间增强待优化材料的重要性。

Result: 在OpenHumanBRDF数据集和真实数据上进行的广泛实验表明，该方法实现了最先进的性能。

Conclusion: 通过构建高质量的OpenHumanBRDF数据集并设计采用渐进式训练策略和受控PBR渲染损失的HumanMaterial模型，该研究成功解决了人体逆渲染中材料估计的病态问题，显著提高了渲染的真实感，尤其在皮肤细节表现上实现了领先水平。

Abstract: Full-body Human inverse rendering based on physically-based rendering aims to
acquire high-quality materials, which helps achieve photo-realistic rendering
under arbitrary illuminations. This task requires estimating multiple material
maps and usually relies on the constraint of rendering result. The absence of
constraints on the material maps makes inverse rendering an ill-posed task.
Previous works alleviated this problem by building material dataset for
training, but their simplified material data and rendering equation lead to
rendering results with limited realism, especially that of skin. To further
alleviate this problem, we construct a higher-quality dataset (OpenHumanBRDF)
based on scanned real data and statistical material data. In addition to the
normal, diffuse albedo, roughness, specular albedo, we produce displacement and
subsurface scattering to enhance the realism of rendering results, especially
for the skin. With the increase in prediction tasks for more materials, using
an end-to-end model as in the previous work struggles to balance the importance
among various material maps, and leads to model underfitting. Therefore, we
design a model (HumanMaterial) with progressive training strategy to make full
use of the supervision information of the material maps and improve the
performance of material estimation. HumanMaterial first obtain the initial
material results via three prior models, and then refine the results by a
finetuning model. Prior models estimate different material maps, and each map
has different significance for rendering results. Thus, we design a Controlled
PBR Rendering (CPR) loss, which enhances the importance of the materials to be
optimized during the training of prior models. Extensive experiments on
OpenHumanBRDF dataset and real data demonstrate that our method achieves
state-of-the-art performance.

</details>


### [106] [Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows](https://arxiv.org/abs/2507.18405)
*Simin Huo,Ning Li*

Main category: cs.CV

TL;DR: Iwin Transformer是一种新型的无位置编码分层视觉Transformer，通过交错窗口注意力与深度可分离卷积结合，可在单一模块内实现全局信息交换，并支持从低到高分辨率直接微调，在多种视觉任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 克服Swin Transformer需要两个连续模块才能近似全局注意力的局限性，实现在单个模块内进行高效的全局信息交换。

Method: 引入Iwin Transformer，一种无位置编码的分层视觉Transformer。它创新性地结合了交错窗口注意力（连接远距离token）和深度可分离卷积（连接相邻token），在一个模块内实现全局信息交换。该方法允许直接从低分辨率到高分辨率进行微调。

Result: 在ImageNet-1K图像分类中达到87.4%的top-1准确率，并在语义分割和视频动作识别等任务中展现出强大竞争力。其核心组件作为独立模块在类别条件图像生成中替换自注意力模块也验证了有效性。

Conclusion: Iwin Transformer引入的概念和方法具有启发未来研究的潜力，例如视频生成中的Iwin 3D Attention。

Abstract: We introduce Iwin Transformer, a novel position-embedding-free hierarchical
vision transformer, which can be fine-tuned directly from low to high
resolution, through the collaboration of innovative interleaved window
attention and depthwise separable convolution. This approach uses attention to
connect distant tokens and applies convolution to link neighboring tokens,
enabling global information exchange within a single module, overcoming Swin
Transformer's limitation of requiring two consecutive blocks to approximate
global attention. Extensive experiments on visual benchmarks demonstrate that
Iwin Transformer exhibits strong competitiveness in tasks such as image
classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and
video action recognition. We also validate the effectiveness of the core
component in Iwin as a standalone module that can seamlessly replace the
self-attention module in class-conditional image generation. The concepts and
methods introduced by the Iwin Transformer have the potential to inspire future
research, like Iwin 3D Attention in video generation. The code and models are
available at https://github.com/cominder/Iwin-Transformer.

</details>


### [107] [DCFFSNet: Deep Connectivity Feature Fusion Separation Network for Medical Image Segmentation](https://arxiv.org/abs/2507.18407)
*Xun Ye,Ruixiang Tang,Mingda Zhang,Jianglong Qin*

Main category: cs.CV

TL;DR: DCFFSNet通过解耦特征空间并动态平衡连通性特征与其他特征，解决了医学图像分割中强制注入拓扑连通性导致特征耦合的问题，显著提高了分割精度和临床可用性。


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像分割深度网络在整合拓扑连通性时，常以额外特征模块的形式强制注入，导致特征空间耦合，并且缺乏量化不同特征强度（特别是连通性特征）的标准机制。

Method: 本文提出了DCFFSNet（双连通性特征融合-分离网络），引入了一种创新的特征空间解耦策略。该策略能够量化连通性特征与其他特征的相对强度，并在此基础上构建一个深度连通性特征融合-分离架构，以动态平衡多尺度特征的表达。

Result: 在ISIC2018、DSB2018和MoNuSeg数据集上的实验表明，DCFFSNet在所有评估指标上均优于现有主流方法。例如，在ISIC2018上，其Dice系数和IoU分别比次优模型CMUNet高出1.3%和1.2%。

Conclusion: DCFFSNet有效解决了医学图像分割中的碎片化问题，实现了平滑的边缘过渡，并且在性能上超越了现有主流方法，显著增强了临床可用性。

Abstract: Medical image segmentation leverages topological connectivity theory to
enhance edge precision and regional consistency. However, existing deep
networks integrating connectivity often forcibly inject it as an additional
feature module, resulting in coupled feature spaces with no standardized
mechanism to quantify different feature strengths. To address these issues, we
propose DCFFSNet (Dual-Connectivity Feature Fusion-Separation Network). It
introduces an innovative feature space decoupling strategy. This strategy
quantifies the relative strength between connectivity features and other
features. It then builds a deep connectivity feature fusion-separation
architecture. This architecture dynamically balances multi-scale feature
expression. Experiments were conducted on the ISIC2018, DSB2018, and MoNuSeg
datasets. On ISIC2018, DCFFSNet outperformed the next best model (CMUNet) by
1.3% (Dice) and 1.2% (IoU). On DSB2018, it surpassed TransUNet by 0.7% (Dice)
and 0.9% (IoU). On MoNuSeg, it exceeded CSCAUNet by 0.8% (Dice) and 0.9% (IoU).
The results demonstrate that DCFFSNet exceeds existing mainstream methods
across all metrics. It effectively resolves segmentation fragmentation and
achieves smooth edge transitions. This significantly enhances clinical
usability.

</details>


### [108] [Self-Supervised Ultrasound-Video Segmentation with Feature Prediction and 3D Localised Loss](https://arxiv.org/abs/2507.18424)
*Edward Ellis,Robert Mendel,Andrew Bulpitt,Nasim Parsa,Michael F Byrne,Sharib Ali*

Main category: cs.CV

TL;DR: 研究提出将V-JEPA应用于超声视频自监督学习，并引入3D定位辅助任务以提高ViT在超声图像分割中的性能，尤其在有限数据下表现出色。


<details>
  <summary>Details</summary>
Motivation: 超声图像数据获取与标注困难，且图像质量受限（低对比度、高噪声、伪影），耗时且需专业知识。自监督学习是解决有限标注数据的潜在方案，但现有的ViT模型在小型医学数据集上可能因缺乏归纳偏置而表现不佳。

Method: 首次将V-JEPA（一种基于特征预测的自监督学习框架，避免像素级重建和负样本）应用于超声视频数据。为了改善ViT在小数据集上的局部性理解，研究提出了一种新颖的3D定位辅助任务，在V-JEPA预训练阶段提高ViT表示的局部性。

Result: 实验结果表明，结合辅助任务的V-JEPA显著提升了超声图像分割性能。在使用100%训练数据时，性能提升高达3.4%；仅使用10%训练数据时，性能提升高达8.35%。

Conclusion: V-JEPA结合提出的3D定位辅助任务，能有效提升超声图像分割性能，尤其在标注数据有限的情况下，展现出更优越的表现和对噪声的鲁棒性。

Abstract: Acquiring and annotating large datasets in ultrasound imaging is challenging
due to low contrast, high noise, and susceptibility to artefacts. This process
requires significant time and clinical expertise. Self-supervised learning
(SSL) offers a promising solution by leveraging unlabelled data to learn useful
representations, enabling improved segmentation performance when annotated data
is limited. Recent state-of-the-art developments in SSL for video data include
V-JEPA, a framework solely based on feature prediction, avoiding pixel level
reconstruction or negative samples. We hypothesise that V-JEPA is well-suited
to ultrasound imaging, as it is less sensitive to noisy pixel-level detail
while effectively leveraging temporal information. To the best of our
knowledge, this is the first study to adopt V-JEPA for ultrasound video data.
Similar to other patch-based masking SSL techniques such as VideoMAE, V-JEPA is
well-suited to ViT-based models. However, ViTs can underperform on small
medical datasets due to lack of inductive biases, limited spatial locality and
absence of hierarchical feature learning. To improve locality understanding, we
propose a novel 3D localisation auxiliary task to improve locality in ViT
representations during V-JEPA pre-training. Our results show V-JEPA with our
auxiliary task improves segmentation performance significantly across various
frozen encoder configurations, with gains up to 3.4\% using 100\% and up to
8.35\% using only 10\% of the training data.

</details>


### [109] [NLML-HPE: Head Pose Estimation with Limited Data via Manifold Learning](https://arxiv.org/abs/2507.18429)
*Mahdi Ghafourian,Federico M. Sukno*

Main category: cs.CV

TL;DR: 提出NLML-HPE深度学习方法，通过非线性流形学习和张量分解，实现在有限训练数据下高精度实时头部姿态估计。


<details>
  <summary>Details</summary>
Motivation: 头部姿态估计对人机交互和面部识别等应用至关重要，但现有方法常受限于训练数据量少和姿态标注不准确。

Method: 提出NLML-HPE方法，结合张量分解（Tucker分解）和前馈神经网络；将头部姿态估计视为回归问题，将输入地标映射到连续姿态表示；使用张量分解分离欧拉角子空间，并将流形维度建模为余弦曲线；为解决标注不准问题，自建精确的2D头部姿态数据集。

Result: 在有限训练数据下实现实时性能；模型能准确捕捉面部特征点对象的旋转特性；一旦学习了底层旋转流形，模型能非常快速地预测未知数据。

Conclusion: NLML-HPE方法通过创新的非线性流形学习和数据生成策略，有效解决了头部姿态估计中小数据量和标注不准确的挑战，实现了高精度和实时性能。

Abstract: Head pose estimation (HPE) plays a critical role in various computer vision
applications such as human-computer interaction and facial recognition. In this
paper, we propose a novel deep learning approach for head pose estimation with
limited training data via non-linear manifold learning called NLML-HPE. This
method is based on the combination of tensor decomposition (i.e., Tucker
decomposition) and feed forward neural networks. Unlike traditional
classification-based approaches, our method formulates head pose estimation as
a regression problem, mapping input landmarks into a continuous representation
of pose angles. To this end, our method uses tensor decomposition to split each
Euler angle (yaw, pitch, roll) to separate subspaces and models each dimension
of the underlying manifold as a cosine curve. We address two key challenges: 1.
Almost all HPE datasets suffer from incorrect and inaccurate pose annotations.
Hence, we generated a precise and consistent 2D head pose dataset for our
training set by rotating 3D head models for a fixed set of poses and rendering
the corresponding 2D images. 2. We achieved real-time performance with limited
training data as our method accurately captures the nature of rotation of an
object from facial landmarks. Once the underlying manifold for rotation around
each axis is learned, the model is very fast in predicting unseen data. Our
training and testing code is available online along with our trained models:
https: //github.com/MahdiGhafoorian/NLML_HPE.

</details>


### [110] [SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning](https://arxiv.org/abs/2507.18616)
*Si-Woo Kim,MinJu Jeon,Ye-Chan Kim,Soeun Lee,Taewhan Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: 针对零样本图像描述 (ZIC) 中由文本到图像 (T2I) 模型生成的合成数据存在的语义错位和噪声问题，本研究提出了SynC框架。SynC通过在现有图像池中重新分配字幕至最语义对齐的图像，有效提升了ZIC模型的性能。


<details>
  <summary>Details</summary>
Motivation: 零样本图像描述 (ZIC) 依赖T2I模型生成的合成数据以规避高昂的手动标注成本，然而这些模型常生成与输入字幕语义不符的图像（如缺少物体、属性错误），导致合成数据噪声，进而阻碍模型训练。现有数据集剪枝技术主要针对网络爬取数据中的文本噪声，不适用于合成数据中图像不准确但字幕良好的独特挑战，因此需要专门方法来弥补这一空白。

Method: 本研究提出了SynC，一个专门用于精炼ZIC合成图像-字幕数据集的新颖框架。SynC不采用传统的过滤或重新生成方式，而是专注于在现有的合成图像池中，将字幕重新分配给语义最对齐的图像。其方法采用一对多映射策略：首先为每个字幕检索多个相关的候选图像，然后应用一个受循环一致性启发的对齐评分器，通过验证图像能否通过图文检索还原原始字幕来选择最佳图像。

Result: 广泛的评估表明，SynC在MS-COCO、Flickr30k、NoCaps等标准基准上，持续且显著地提升了各种ZIC模型的性能，并在多个场景中取得了最先进的结果。

Conclusion: SynC为整理精炼的合成数据以增强零样本图像描述能力提供了一种有效策略。

Abstract: Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets
generated by text-to-image (T2I) models to mitigate the need for costly manual
annotation. However, these T2I models often produce images that exhibit
semantic misalignments with their corresponding input captions (e.g., missing
objects, incorrect attributes), resulting in noisy synthetic image-caption
pairs that can hinder model training. Existing dataset pruning techniques are
largely designed for removing noisy text in web-crawled data. However, these
methods are ill-suited for the distinct challenges of synthetic data, where
captions are typically well-formed, but images may be inaccurate
representations. To address this gap, we introduce SynC, a novel framework
specifically designed to refine synthetic image-caption datasets for ZIC.
Instead of conventional filtering or regeneration, SynC focuses on reassigning
captions to the most semantically aligned images already present within the
synthetic image pool. Our approach employs a one-to-many mapping strategy by
initially retrieving multiple relevant candidate images for each caption. We
then apply a cycle-consistency-inspired alignment scorer that selects the best
image by verifying its ability to retrieve the original caption via
image-to-text retrieval. Extensive evaluations demonstrate that SynC
consistently and significantly improves performance across various ZIC models
on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art
results in several scenarios. SynC offers an effective strategy for curating
refined synthetic data to enhance ZIC.

</details>


### [111] [DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place Recognition](https://arxiv.org/abs/2507.18444)
*Haiyang Jiang,Songhao Piao,Chao Gao,Lei Yu,Liguo Chen*

Main category: cs.CV

TL;DR: 本文提出一种结合双尺度Transformer (DSFormer) 和创新块聚类策略的视觉地点识别 (VPR) 框架，显著提升了在复杂环境和视角变化下的鲁棒性，并在多项基准数据集上实现了最先进的性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 视觉地点识别 (VPR) 对于移动机器人定位至关重要，但在环境条件和视角变化下，其性能的可靠性面临重大挑战。

Method: 本文提出一个新颖框架，整合了：1. Dual-Scale-Former (DSFormer)：一个基于Transformer的跨学习模块，通过CNN最后两层提取双尺度特征，利用自注意力和共享跨注意力增强特征表示，捕获语义丰富性和空间细节。2. 创新的块聚类策略：重新划分SF-XL训练数据集，从多个角度优化数据组织，以增强对视角变化的鲁棒性。

Result: 该方法不仅生成了适应环境变化的鲁棒全局嵌入，而且与现有分区方法相比，将所需训练数据量减少了约30%。实验表明，该方法在大多数基准数据集上达到了最先进 (SOTA) 的性能，作为使用512维全局描述符的全局检索解决方案，超越了DELG、Patch-NetVLAD、TransVPR和R2Former等先进重排序方法，同时显著提高了计算效率。

Conclusion: 该框架通过结合双尺度特征学习和优化的数据组织，有效解决了VPR在复杂环境下的鲁棒性挑战，提供了高性能、高效率且数据需求更低的移动机器人定位解决方案。

Abstract: Visual Place Recognition (VPR) is crucial for robust mobile robot
localization, yet it faces significant challenges in maintaining reliable
performance under varying environmental conditions and viewpoints. To address
this, we propose a novel framework that integrates Dual-Scale-Former
(DSFormer), a Transformer-based cross-learning module, with an innovative block
clustering strategy. DSFormer enhances feature representation by enabling
bidirectional information transfer between dual-scale features extracted from
the final two CNN layers, capturing both semantic richness and spatial details
through self-attention for long-range dependencies within each scale and shared
cross-attention for cross-scale learning. Complementing this, our block
clustering strategy repartitions the widely used San Francisco eXtra Large
(SF-XL) training dataset from multiple distinct perspectives, optimizing data
organization to further bolster robustness against viewpoint variations.
Together, these innovations not only yield a robust global embedding adaptable
to environmental changes but also reduce the required training data volume by
approximately 30\% compared to previous partitioning methods. Comprehensive
experiments demonstrate that our approach achieves state-of-the-art performance
across most benchmark datasets, surpassing advanced reranking methods like
DELG, Patch-NetVLAD, TransVPR, and R2Former as a global retrieval solution
using 512-dim global descriptors, while significantly improving computational
efficiency.

</details>


### [112] [PDB-Eval: An Evaluation of Large Multimodal Models for Description and Explanation of Personalized Driving Behavior](https://arxiv.org/abs/2507.18447)
*Junda Wu,Jessica Echterhoff,Kyungtae Han,Amr Abdelraouf,Rohit Gupta,Julian McAuley*

Main category: cs.CV

TL;DR: 本文提出PDB-Eval基准，旨在通过外部视觉证据理解个性化驾驶行为，并对大型多模态模型（MLLMs）进行微调，以提升其在驾驶理解和推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 理解驾驶员行为和意图对风险评估和事故预防至关重要。现有数据集在基于外部视觉证据描述和解释车辆运动方面存在局限，难以用于个性化驾驶员行为分析，阻碍了MLLMs在驾驶领域的应用。

Method: 研究引入PDB-Eval基准，包含PDB-X和PDB-QA两部分。PDB-X用于评估MLLMs对时间驾驶场景的理解，旨在从外部视角找到有效视觉证据来解释驾驶员行为。PDB-QA是一个视觉解释问答任务，用于MLLMs的指令微调，以弥合领域差距并保持模型的泛化能力。

Result: 通过在PDB-Eval上对MLLMs进行微调，有效弥合了MLLMs与驾驶领域之间的鸿沟。零样本问答任务性能提升高达73.2%。在Brain4Cars的意图预测任务中，转向意图预测性能提升高达12.5%。在AIDE的所有识别任务中，性能均有提升，最高达11.0%。

Conclusion: 所提出的PDB-Eval基准和微调方法能有效提升MLLMs对驾驶行为的理解和推理能力，并在多个驾驶相关任务上取得显著性能提升，证明了细粒度描述和解释数据对MLLMs在驾驶领域应用的重要性。

Abstract: Understanding a driver's behavior and intentions is important for potential
risk assessment and early accident prevention. Safety and driver assistance
systems can be tailored to individual drivers' behavior, significantly
enhancing their effectiveness. However, existing datasets are limited in
describing and explaining general vehicle movements based on external visual
evidence. This paper introduces a benchmark, PDB-Eval, for a detailed
understanding of Personalized Driver Behavior, and aligning Large Multimodal
Models (MLLMs) with driving comprehension and reasoning. Our benchmark consists
of two main components, PDB-X and PDB-QA. PDB-X can evaluate MLLMs'
understanding of temporal driving scenes. Our dataset is designed to find valid
visual evidence from the external view to explain the driver's behavior from
the internal view. To align MLLMs' reasoning abilities with driving tasks, we
propose PDB-QA as a visual explanation question-answering task for MLLM
instruction fine-tuning. As a generic learning task for generative models like
MLLMs, PDB-QA can bridge the domain gap without harming MLLMs'
generalizability. Our evaluation indicates that fine-tuning MLLMs on
fine-grained descriptions and explanations can effectively bridge the gap
between MLLMs and the driving domain, which improves zero-shot performance on
question-answering tasks by up to 73.2%. We further evaluate the MLLMs
fine-tuned on PDB-X in Brain4Cars' intention prediction and AIDE's recognition
tasks. We observe up to 12.5% performance improvements on the turn intention
prediction task in Brain4Cars, and consistent performance improvements up to
11.0% on all tasks in AIDE.

</details>


### [113] [Revisiting Physically Realizable Adversarial Object Attack against LiDAR-based Detection: Clarifying Problem Formulation and Experimental Protocols](https://arxiv.org/abs/2507.18457)
*Luo Cheng,Hanwei Zhang,Lijun Zhang,Holger Hermanns*

Main category: cs.CV

TL;DR: 针对LiDAR三维目标检测中物理对抗性攻击的可重现性差问题，本文提出了一个设备无关的标准化框架，并成功将模拟攻击转移到真实物理系统，以促进该领域研究。


<details>
  <summary>Details</summary>
Motivation: LiDAR三维目标检测在现实应用中至关重要，但现有数字攻击缺乏物理可行性。物理对抗性物体攻击研究不足且重现性差，限制了其实际影响。

Method: 提出一个设备无关、标准化的框架，该框架抽象了物理对抗性物体攻击的关键要素，支持多样化方法，并提供开源代码和模拟/真实世界基准测试协议。

Result: 该框架能够实现公平比较和研究加速，并通过将模拟攻击成功转移到物理LiDAR系统进行了验证。此外，还提供了影响攻击成功因素的见解。

Conclusion: 本文的框架解决了物理对抗性物体攻击的重现性差问题，促进了该领域研究，并深化了对真实世界LiDAR感知中对抗鲁棒性的理解。

Abstract: Adversarial robustness in LiDAR-based 3D object detection is a critical
research area due to its widespread application in real-world scenarios. While
many digital attacks manipulate point clouds or meshes, they often lack
physical realizability, limiting their practical impact. Physical adversarial
object attacks remain underexplored and suffer from poor reproducibility due to
inconsistent setups and hardware differences. To address this, we propose a
device-agnostic, standardized framework that abstracts key elements of physical
adversarial object attacks, supports diverse methods, and provides open-source
code with benchmarking protocols in simulation and real-world settings. Our
framework enables fair comparison, accelerates research, and is validated by
successfully transferring simulated attacks to a physical LiDAR system. Beyond
the framework, we offer insights into factors influencing attack success and
advance understanding of adversarial robustness in real-world LiDAR perception.

</details>


### [114] [CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using Gaussian Splatting](https://arxiv.org/abs/2507.18473)
*Haoran Xu,Saining Zhang,Peishuo Li,Baijun Ye,Xiaoxue Chen,Huan-ang Gao,Jv Zheng,Xiaowei Song,Ziqiao Peng,Run Miao,Jinrang Jia,Yifeng Shi,Guangqi Yi,Hang Zhao,Hao Tang,Hongyang Li,Kaicheng Yu,Hao Zhao*

Main category: cs.CV

TL;DR: CRUISE是一个用于V2X驾驶环境的综合重建与合成框架，它利用分解高斯泼溅技术高保真重建并灵活编辑真实场景，有效增强V2X数据集，提升3D感知和跟踪性能。


<details>
  <summary>Details</summary>
Motivation: V2X通信对自动驾驶至关重要，但模拟在V2X数据生成和增强方面的潜力尚未充分开发。

Method: 引入CRUISE框架，它采用分解高斯泼溅（decomposed Gaussian Splatting）技术精确重建真实V2X场景并支持灵活编辑。通过将动态交通参与者分解为可编辑的高斯表示，实现驾驶场景的无缝修改和增强。该框架还能从主车和基础设施视角渲染图像，支持大规模V2X数据集增强。

Result: 1) CRUISE高保真重建了真实V2X驾驶场景；2) 使用CRUISE改进了主车、基础设施和协同视图下的3D检测，以及V2X-Seq基准上的协同3D跟踪；3) CRUISE有效地生成了具有挑战性的极端情况。

Conclusion: CRUISE框架能够高保真重建V2X场景，并提供灵活的场景编辑和数据增强能力，显著提升了V2X感知任务（如3D检测和跟踪）的性能，同时能生成有价值的挑战性案例。

Abstract: Vehicle-to-everything (V2X) communication plays a crucial role in autonomous
driving, enabling cooperation between vehicles and infrastructure. While
simulation has significantly contributed to various autonomous driving tasks,
its potential for data generation and augmentation in V2X scenarios remains
underexplored. In this paper, we introduce CRUISE, a comprehensive
reconstruction-and-synthesis framework designed for V2X driving environments.
CRUISE employs decomposed Gaussian Splatting to accurately reconstruct
real-world scenes while supporting flexible editing. By decomposing dynamic
traffic participants into editable Gaussian representations, CRUISE allows for
seamless modification and augmentation of driving scenes. Furthermore, the
framework renders images from both ego-vehicle and infrastructure views,
enabling large-scale V2X dataset augmentation for training and evaluation. Our
experimental results demonstrate that: 1) CRUISE reconstructs real-world V2X
driving scenes with high fidelity; 2) using CRUISE improves 3D detection across
ego-vehicle, infrastructure, and cooperative views, as well as cooperative 3D
tracking on the V2X-Seq benchmark; and 3) CRUISE effectively generates
challenging corner cases.

</details>


### [115] [Q-Former Autoencoder: A Modern Framework for Medical Anomaly Detection](https://arxiv.org/abs/2507.18481)
*Francesco Dalmonte,Emirhan Bayar,Emre Akbas,Mariana-Iuliana Georgescu*

Main category: cs.CV

TL;DR: 本文提出Q-Former自编码器，利用预训练视觉基础模型进行无监督医学图像异常检测，并在多个基准测试中实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 医学图像异常检测面临异常多样性和带注释数据稀缺的挑战，尤其在无监督场景下，需要有效处理这些问题。

Method: 提出Q-Former自编码器框架。该框架使用冻结的DINO、DINOv2和MAE等预训练视觉基础模型作为特征提取器，避免从头训练。引入Q-Former架构作为瓶颈，用于控制重建序列长度并有效聚合多尺度特征。此外，结合基于预训练MAE特征的感知损失，指导重建语义上有意义的结构。

Result: 该框架在BraTS2021、RESC和RSNA四个医学异常检测基准测试中的三个上取得了最先进（SOTA）的结果。研究表明，在自然图像上预训练的视觉基础模型编码器无需进一步微调即可有效泛化到医学图像分析任务。

Conclusion: Q-Former自编码器是一种在无监督医学异常检测中表现出色的方法，它通过利用冻结的视觉基础模型和创新的Q-Former瓶颈，证明了这些模型在不进行特定领域微调的情况下，在医学图像任务上的强大泛化能力。

Abstract: Anomaly detection in medical images is an important yet challenging task due
to the diversity of possible anomalies and the practical impossibility of
collecting comprehensively annotated data sets. In this work, we tackle
unsupervised medical anomaly detection proposing a modernized autoencoder-based
framework, the Q-Former Autoencoder, that leverages state-of-the-art pretrained
vision foundation models, such as DINO, DINOv2 and Masked Autoencoder. Instead
of training encoders from scratch, we directly utilize frozen vision foundation
models as feature extractors, enabling rich, multi-stage, high-level
representations without domain-specific fine-tuning. We propose the usage of
the Q-Former architecture as the bottleneck, which enables the control of the
length of the reconstruction sequence, while efficiently aggregating multiscale
features. Additionally, we incorporate a perceptual loss computed using
features from a pretrained Masked Autoencoder, guiding the reconstruction
towards semantically meaningful structures. Our framework is evaluated on four
diverse medical anomaly detection benchmarks, achieving state-of-the-art
results on BraTS2021, RESC, and RSNA. Our results highlight the potential of
vision foundation model encoders, pretrained on natural images, to generalize
effectively to medical image analysis tasks without further fine-tuning. We
release the code and models at https://github.com/emirhanbayar/QFAE.

</details>


### [116] [A COCO-Formatted Instance-Level Dataset for Plasmodium Falciparum Detection in Giemsa-Stained Blood Smears](https://arxiv.org/abs/2507.18483)
*Frauke Wilm,Luis Carlos Rivera Monroy,Mathias Öttl,Lukas Mürdter,Leonid Mill,Andreas Maier*

Main category: cs.CV

TL;DR: 该论文通过增强NIH疟疾数据集并添加详细的COCO格式边界框标注，解决了深度学习疟疾诊断中数据稀缺的问题，并验证了其有效性，为自动化疟疾诊断提供了高质量的训练数据。


<details>
  <summary>Details</summary>
Motivation: 深度学习在自动化疟疾诊断方面潜力巨大，但由于缺乏带有详细实例级标注的数据集，其应用受到限制。准确检测恶性疟原虫对于可靠的疟疾诊断至关重要。

Method: 作者提供了一个增强版的NIH疟疾数据集，其中包含详细的COCO格式边界框标注。他们通过训练一个Faster R-CNN模型来检测受感染和未受感染的红细胞以及白细胞，以验证修订后的标注。该方法结合了自动化标注精炼和有针对性的人工校正。

Result: 在原始数据集上进行交叉验证，Faster R-CNN模型在感染细胞检测方面达到了高达0.88的F1分数。

Conclusion: 研究结果强调了标注数量和一致性的重要性，并表明自动化标注精炼与有针对性的人工校正相结合，可以生成足够高质量的训练数据，以实现鲁棒的检测性能。更新后的标注集已公开可用。

Abstract: Accurate detection of Plasmodium falciparum in Giemsa-stained blood smears is
an essential component of reliable malaria diagnosis, especially in developing
countries. Deep learning-based object detection methods have demonstrated
strong potential for automated Malaria diagnosis, but their adoption is limited
by the scarcity of datasets with detailed instance-level annotations. In this
work, we present an enhanced version of the publicly available NIH malaria
dataset, with detailed bounding box annotations in COCO format to support
object detection training. We validated the revised annotations by training a
Faster R-CNN model to detect infected and non-infected red blood cells, as well
as white blood cells. Cross-validation on the original dataset yielded F1
scores of up to 0.88 for infected cell detection. These results underscore the
importance of annotation volume and consistency, and demonstrate that automated
annotation refinement combined with targeted manual correction can produce
training data of sufficient quality for robust detection performance. The
updated annotations set is publicly available via GitHub:
https://github.com/MIRA-Vision-Microscopy/malaria-thin-smear-coco.

</details>


### [117] [Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments](https://arxiv.org/abs/2507.18484)
*Xiao Yang,Lingxuan Wu,Lizhong Wang,Chengyang Ying,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: 本文提出Rein-EAD，一种主动防御框架，通过强化学习在3D对抗环境中提高感知系统的鲁棒性，有效抵抗未知和自适应攻击。


<details>
  <summary>Details</summary>
Motivation: 3D对抗性攻击对视觉感知系统（尤其是在安全关键应用中，如身份验证和自动驾驶）构成严重威胁。现有防御机制是被动的，依赖预设假设，在动态3D环境中适应性不足。

Method: 引入了Reinforced Embodied Active Defense (Rein-EAD) 框架。该框架是一种主动防御策略，通过自适应探索和与环境的交互来提高鲁棒性。其核心包括：1) 一个平衡即时预测准确性与预测熵最小化的多步目标；2) 一个以不确定性为导向的奖励整形机制，以促进高效策略更新并支持实际应用，无需可微分环境。

Result: Rein-EAD显著降低了攻击成功率，同时在多种任务中保持了标准准确性。它对未见过的和自适应攻击表现出强大的泛化能力。

Conclusion: Rein-EAD是一种有效的主动防御框架，能够提高3D环境下感知系统的鲁棒性，并能泛化到新攻击，适用于3D物体分类、人脸识别和自动驾驶等多种复杂现实任务。

Abstract: Adversarial attacks in 3D environments have emerged as a critical threat to
the reliability of visual perception systems, particularly in safety-sensitive
applications such as identity verification and autonomous driving. These
attacks employ adversarial patches and 3D objects to manipulate deep neural
network (DNN) predictions by exploiting vulnerabilities within complex scenes.
Existing defense mechanisms, such as adversarial training and purification,
primarily employ passive strategies to enhance robustness. However, these
approaches often rely on pre-defined assumptions about adversarial tactics,
limiting their adaptability in dynamic 3D settings. To address these
challenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a
proactive defense framework that leverages adaptive exploration and interaction
with the environment to improve perception robustness in 3D adversarial
contexts. By implementing a multi-step objective that balances immediate
prediction accuracy with predictive entropy minimization, Rein-EAD optimizes
defense strategies over a multi-step horizon. Additionally, Rein-EAD involves
an uncertainty-oriented reward-shaping mechanism that facilitates efficient
policy updates, thereby reducing computational overhead and supporting
real-world applicability without the need for differentiable environments.
Comprehensive experiments validate the effectiveness of Rein-EAD, demonstrating
a substantial reduction in attack success rates while preserving standard
accuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization
to unseen and adaptive attacks, making it suitable for real-world complex
tasks, including 3D object classification, face recognition and autonomous
driving.

</details>


### [118] [Delving into Mapping Uncertainty for Mapless Trajectory Prediction](https://arxiv.org/abs/2507.18498)
*Zongzheng Zhang,Xuchong Qiu,Boran Zhang,Guantian Zheng,Xunjiang Gu,Guoxuan Chi,Huan-ang Gao,Leichen Wang,Ziming Liu,Xinrun Li,Igor Gilitschenski,Hongyang Li,Hang Zhao,Hao Zhao*

Main category: cs.CV

TL;DR: 在无图自动驾驶中，本研究分析了地图不确定性对轨迹预测的影响，提出了一种基于智能体运动状态的自适应整合方法，显著提升了轨迹预测性能并增强了可解释性。


<details>
  <summary>Details</summary>
Motivation: 无图自动驾驶中在线生成的高精度地图可靠性存疑，尽管将地图不确定性整合到轨迹预测中显示出潜力，但现有策略缺乏对何时何地这种不确定性有益的深入洞察。

Method: 首先分析了地图不确定性对轨迹预测产生最大积极影响的驾驶场景，并识别出关键因素：智能体的运动状态。在此基础上，提出了一个新颖的“本体感知场景门控”（Proprioceptive Scenario Gating）方法，根据自我车辆的未来运动学预测自适应地将地图不确定性整合到轨迹预测中。此外，引入了一种“基于协方差的地图不确定性”（Covariance-based Map Uncertainty）方法，以更好地与地图几何对齐。

Result: 所提出的方法增强了在线建图与轨迹预测之间的协同作用，提供了不确定性何时有益的可解释性，并超越了以往的整合方法。在nuScenes真实世界驾驶数据集上，无图轨迹预测性能比现有最先进的方法提高了高达23.6%。

Conclusion: 本研究通过识别智能体运动状态作为关键因素，并提出了自适应的本体感知场景门控和基于协方差的地图不确定性方法，显著提高了无图轨迹预测的性能和可解释性，为更可靠的自动驾驶系统奠定了基础。

Abstract: Recent advances in autonomous driving are moving towards mapless approaches,
where High-Definition (HD) maps are generated online directly from sensor data,
reducing the need for expensive labeling and maintenance. However, the
reliability of these online-generated maps remains uncertain. While
incorporating map uncertainty into downstream trajectory prediction tasks has
shown potential for performance improvements, current strategies provide
limited insights into the specific scenarios where this uncertainty is
beneficial. In this work, we first analyze the driving scenarios in which
mapping uncertainty has the greatest positive impact on trajectory prediction
and identify a critical, previously overlooked factor: the agent's kinematic
state. Building on these insights, we propose a novel Proprioceptive Scenario
Gating that adaptively integrates map uncertainty into trajectory prediction
based on forecasts of the ego vehicle's future kinematics. This lightweight,
self-supervised approach enhances the synergy between online mapping and
trajectory prediction, providing interpretability around where uncertainty is
advantageous and outperforming previous integration methods. Additionally, we
introduce a Covariance-based Map Uncertainty approach that better aligns with
map geometry, further improving trajectory prediction. Extensive ablation
studies confirm the effectiveness of our approach, achieving up to 23.6%
improvement in mapless trajectory prediction performance over the
state-of-the-art method using the real-world nuScenes driving dataset. Our
code, data, and models are publicly available at
https://github.com/Ethan-Zheng136/Map-Uncertainty-for-Trajectory-Prediction.

</details>


### [119] [Human Scanpath Prediction in Target-Present Visual Search with Semantic-Foveal Bayesian Attention](https://arxiv.org/abs/2507.18503)
*João Luzio,Alexandre Bernardino,Plinio Moreno*

Main category: cs.CV

TL;DR: 本文提出SemBA-FAST模型，一个结合深度目标检测和概率语义融合的自上而下框架，用于预测目标存在视觉搜索中的人类视觉注意力。该模型在COCO-Search18数据集上表现出色，生成的注视序列与人类真实扫描路径高度匹配，超越了基线及其他自上而下方法。


<details>
  <summary>Details</summary>
Motivation: 人类感知受自上而下和自下而上线索引导，且中央凹视觉至关重要。当前生物启发计算注意力模型受益于深度学习和人类扫描路径数据，但仍需进一步提升在目标存在视觉搜索中预测人类视觉注意力的能力。

Method: 本研究评估了SemBA-FAST（Foveal Active visual Search Tasks的基于语义的贝叶斯注意力），一个用于预测目标存在视觉搜索中人类视觉注意力的自上而下框架。它结合深度目标检测和概率语义融合机制动态生成注意力图，利用预训练检测器和人工中央凹来顺序更新自上而下知识并改进注视预测。模型在COCO-Search18基准数据集上与其它扫描路径预测模型进行了比较。

Result: SemBA-FAST模型实现的注视序列与人类真实扫描路径高度匹配。其性能超越了基线和其他自上而下方法，在某些情况下甚至能与扫描路径信息模型竞争。

Conclusion: 这些发现为基于语义-中央凹的概率框架在类人注意力建模方面的能力提供了宝贵见解，对实时认知计算和机器人技术具有重要意义。

Abstract: In goal-directed visual tasks, human perception is guided by both top-down
and bottom-up cues. At the same time, foveal vision plays a crucial role in
directing attention efficiently. Modern research on bio-inspired computational
attention models has taken advantage of advancements in deep learning by
utilizing human scanpath data to achieve new state-of-the-art performance. In
this work, we assess the performance of SemBA-FAST, i.e. Semantic-based
Bayesian Attention for Foveal Active visual Search Tasks, a top-down framework
designed for predicting human visual attention in target-present visual search.
SemBA-FAST integrates deep object detection with a probabilistic semantic
fusion mechanism to generate attention maps dynamically, leveraging pre-trained
detectors and artificial foveation to update top-down knowledge and improve
fixation prediction sequentially. We evaluate SemBA-FAST on the COCO-Search18
benchmark dataset, comparing its performance against other scanpath prediction
models. Our methodology achieves fixation sequences that closely match human
ground-truth scanpaths. Notably, it surpasses baseline and other top-down
approaches and competes, in some cases, with scanpath-informed models. These
findings provide valuable insights into the capabilities of semantic-foveal
probabilistic frameworks for human-like attention modelling, with implications
for real-time cognitive computing and robotics.

</details>


### [120] [Explaining How Visual, Textual and Multimodal Encoders Share Concepts](https://arxiv.org/abs/2507.18512)
*Clément Cornet,Romaric Besançon,Hervé Le Borgne*

Main category: cs.CV

TL;DR: 本研究提出新指标，利用稀疏自编码器（SAE）对视觉、文本和多模态模型进行跨模态特征比较与共享度量化，发现多模态模型中的视觉特征与文本特征共享，突显了文本预训练的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有基于稀疏自编码器（SAE）的模型比较仅限于同模态内部，缺乏量化比较不同模态模型（如视觉、文本）之间特征的方法。

Method: 1. 提出了一个新颖的指标，用于基于SAE特征进行跨模态模型（视觉、文本、多模态编码器）的定量比较。2. 提出了量化不同类型模型之间个体特征“比较共享度”的方法。3. 利用这些工具，对21个不同规模和类型的编码器在通用及领域特定数据集上进行了多项研究。

Result: 研究结果允许重新审视多模态背景下的现有研究，并量化不同模型间共享表示或特征的程度。特别指出，视觉-语言模型（VLMs）中特有的视觉特征与文本编码器共享，表明了文本预训练的显著影响。

Conclusion: 本研究提供了量化比较跨模态模型共享特征的新工具，并通过实证揭示了视觉、文本和多模态编码器之间存在共享表示，强调了文本预训练对模型特征学习的关键作用。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful technique for
extracting human-interpretable features from neural networks activations.
Previous works compared different models based on SAE-derived features but
those comparisons have been restricted to models within the same modality. We
propose a novel indicator allowing quantitative comparison of models across SAE
features, and use it to conduct a comparative study of visual, textual and
multimodal encoders. We also propose to quantify the Comparative Sharedness of
individual features between different classes of models. With these two new
tools, we conduct several studies on 21 encoders of the three types, with two
significantly different sizes, and considering generalist and domain specific
datasets. The results allow to revisit previous studies at the light of
encoders trained in a multimodal context and to quantify to which extent all
these models share some representations or features. They also suggest that
visual features that are specific to VLMs among vision encoders are shared with
text encoders, highlighting the impact of text pretraining. The code is
available at https://github.com/CEA-LIST/SAEshareConcepts

</details>


### [121] [Towards Large Scale Geostatistical Methane Monitoring with Part-based Object Detection](https://arxiv.org/abs/2507.18513)
*Adhemar de Senneville,Xavier Bou,Thibaud Ehret,Rafael Grompone,Jean Louis Bonne,Nicolas Dumelie,Thomas Lauvaux,Gabriele Facciolo*

Main category: cs.CV

TL;DR: 针对遥感图像中稀有目标（沼气池）的检测挑战，本文提出了一种基于部件的方法，用于在法国建立沼气池清单并估算其甲烷产量。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中大范围地理区域内稀有目标检测面临挑战，但对评估大规模人类活动（如沼气池甲烷排放）的环境影响至关重要。本研究旨在解决法国沼气池的甲烷生产和排放问题。

Method: 1. 构建了一个新的、高度不平衡的沼气池数据集。2. 开发了一种基于部件的检测方法，通过考虑沼气池的关键子元素来提升检测效果。3. 将该方法应用于新区域以建立沼气池库存清单。4. 对检测到的沼气池在特定区域和时间的甲烷生产量进行地理统计估算。

Result: 成功应用所提方法建立了法国沼气池的清单。能够对这些基础设施在特定区域和时间的甲烷生产量进行地理统计估算。

Conclusion: 该研究成功地通过开发专门的基于部件的方法解决了遥感图像中稀有目标（沼气池）的检测问题，实现了沼气池的库存建立和后续的甲烷排放估算，这对环境影响评估具有重要意义。

Abstract: Object detection is one of the main applications of computer vision in remote
sensing imagery. Despite its increasing availability, the sheer volume of
remote sensing data poses a challenge when detecting rare objects across large
geographic areas. Paradoxically, this common challenge is crucial to many
applications, such as estimating environmental impact of certain human
activities at scale. In this paper, we propose to address the problem by
investigating the methane production and emissions of bio-digesters in France.
We first introduce a novel dataset containing bio-digesters, with small
training and validation sets, and a large test set with a high imbalance
towards observations without objects since such sites are rare. We develop a
part-based method that considers essential bio-digester sub-elements to boost
initial detections. To this end, we apply our method to new, unseen regions to
build an inventory of bio-digesters. We then compute geostatistical estimates
of the quantity of methane produced that can be attributed to these
infrastructures in a given area at a given time.

</details>


### [122] [Object segmentation in the wild with foundation models: application to vision assisted neuro-prostheses for upper limbs](https://arxiv.org/abs/2507.18517)
*Bolutife Atoki,Jenny Benois-Pineau,Renaud Péteri,Fabien Baldacci,Aymar de Rugy*

Main category: cs.CV

TL;DR: 本研究探索了在杂乱视觉场景中利用基础模型（如SAM）进行语义目标分割的能力，并通过基于注视点的提示生成和微调，在真实世界数据上显著提升了分割质量。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究基础模型在高度杂乱的真实世界场景（“野外”环境，如视觉引导上肢神经假肢应用）中，无需对特定图像进行微调即可执行目标分割的有效性，并寻求提升其在该复杂场景下的表现。

Method: 提出了一种基于注视点生成提示的方法来引导Segment Anything Model (SAM) 进行目标分割，并对SAM在以自我为中心的视觉数据上进行了微调。

Result: 在Grasping-in-the-Wild真实世界挑战性数据集上，所提出的方法使IoU分割质量指标提升了高达0.51点。

Conclusion: 研究结果表明，结合注视点引导和对基础模型（SAM）的微调，可以有效提升其在复杂“野外”场景下（特别是在神经假肢应用背景下）的语义目标分割性能。

Abstract: In this work, we address the problem of semantic object segmentation using
foundation models. We investigate whether foundation models, trained on a large
number and variety of objects, can perform object segmentation without
fine-tuning on specific images containing everyday objects, but in highly
cluttered visual scenes. The ''in the wild'' context is driven by the target
application of vision guided upper limb neuroprostheses. We propose a method
for generating prompts based on gaze fixations to guide the Segment Anything
Model (SAM) in our segmentation scenario, and fine-tune it on egocentric visual
data. Evaluation results of our approach show an improvement of the IoU
segmentation quality metric by up to 0.51 points on real-world challenging data
of Grasping-in-the-Wild corpus which is made available on the RoboFlow Platform
(https://universe.roboflow.com/iwrist/grasping-in-the-wild)

</details>


### [123] [GaussianFusionOcc: A Seamless Sensor Fusion Approach for 3D Occupancy Prediction Using 3D Gaussians](https://arxiv.org/abs/2507.18522)
*Tomislav Pavković,Mohammad-Ali Nikouei Mahani,Johannes Niedermayer,Johannes Betz*

Main category: cs.CV

TL;DR: GaussianFusionOcc是一种用于自动驾驶3D语义占用预测的方法，它结合了多模态传感器融合和3D高斯表示，实现了更精确、高效且超越现有技术的预测。


<details>
  <summary>Details</summary>
Motivation: 3D语义占用预测对自动驾驶至关重要，能实现复杂环境中的精确安全解释和导航。可靠的预测依赖于有效的传感器融合，且现有方法（如依赖密集网格）可能效率不高。

Method: 本文提出了GaussianFusionOcc，该方法使用语义3D高斯表示和创新的传感器融合机制，区别于传统的密集网格表示。它无缝整合了相机、激光雷达和雷达数据。模型采用模态无关的可变形注意力机制，从各类传感器中提取关键特征，并用于优化高斯属性，以实现对环境更准确的表示。

Result: 3D高斯表示显著提高了内存效率和推理速度。通过利用多模态融合的鲁棒性和高斯表示的效率，GaussianFusionOcc在各种传感器组合的广泛测试中展现了其通用性，并超越了当前最先进的模型。

Conclusion: GaussianFusionOcc通过结合鲁棒的多模态融合和高效的3D高斯表示，为自动驾驶中的3D语义占用预测提供了一种更精确、更高效且性能优于现有技术的新范式。

Abstract: 3D semantic occupancy prediction is one of the crucial tasks of autonomous
driving. It enables precise and safe interpretation and navigation in complex
environments. Reliable predictions rely on effective sensor fusion, as
different modalities can contain complementary information. Unlike conventional
methods that depend on dense grid representations, our approach,
GaussianFusionOcc, uses semantic 3D Gaussians alongside an innovative sensor
fusion mechanism. Seamless integration of data from camera, LiDAR, and radar
sensors enables more precise and scalable occupancy prediction, while 3D
Gaussian representation significantly improves memory efficiency and inference
speed. GaussianFusionOcc employs modality-agnostic deformable attention to
extract essential features from each sensor type, which are then used to refine
Gaussian properties, resulting in a more accurate representation of the
environment. Extensive testing with various sensor combinations demonstrates
the versatility of our approach. By leveraging the robustness of multi-modal
fusion and the efficiency of Gaussian representation, GaussianFusionOcc
outperforms current state-of-the-art models.

</details>


### [124] [IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented Controllable Video Captioning](https://arxiv.org/abs/2507.18531)
*Tianheng Qiu,Jingchun Gao,Jingyu Li,Huiyi Leong,Xuan Huang,Xi Wang,Xiaocheng Zhang,Kele Xu,Lan Zhang*

Main category: cs.CV

TL;DR: 本文提出IntentVCNet，通过独特的提示组合策略和参数高效边界框适配器，弥补现有大型视觉语言模型（LVLM）在视频中精细时空控制的不足，实现意图导向的视频字幕生成，并取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前LVLMs虽具备强大的指令遵循和视觉理解能力，但在时间序列中对空间信息进行精细控制以响应用户意图时存在显著的时空鸿沟，这使得实现精细的意图导向视频控制变得复杂。

Method: 本文提出IntentVCNet，从提示和模型两方面弥合时空鸿沟：1) 提出一种提示组合策略，使LLM能建模用户意图提示与视频序列间的隐式关系。2) 提出一个参数高效的边界框适配器，以增强全局视觉上下文中的对象语义信息，使视觉token预先获取用户意图信息。

Result: 实验证明，结合这两种策略能有效增强LVLM对视频序列中空间细节的建模能力，并促进其准确生成受控的意图导向字幕。所提方法在多个开源LVLMs上取得了最先进（SOTA）的结果，并获得IntentVC挑战赛亚军。

Conclusion: IntentVCNet成功地将LVLM固有的时空理解知识统一起来，有效解决了现有模型在意图导向视频字幕生成中精细时空控制的难题，显著提升了生成字幕的准确性和意图匹配度。

Abstract: Intent-oriented controlled video captioning aims to generate targeted
descriptions for specific targets in a video based on customized user intent.
Current Large Visual Language Models (LVLMs) have gained strong instruction
following and visual comprehension capabilities. Although the LVLMs
demonstrated proficiency in spatial and temporal understanding respectively, it
was not able to perform fine-grained spatial control in time sequences in
direct response to instructions. This substantial spatio-temporal gap
complicates efforts to achieve fine-grained intention-oriented control in
video. Towards this end, we propose a novel IntentVCNet that unifies the
temporal and spatial understanding knowledge inherent in LVLMs to bridge the
spatio-temporal gap from both prompting and model perspectives. Specifically,
we first propose a prompt combination strategy designed to enable LLM to model
the implicit relationship between prompts that characterize user intent and
video sequences. We then propose a parameter efficient box adapter that
augments the object semantic information in the global visual context so that
the visual token has a priori information about the user intent. The final
experiment proves that the combination of the two strategies can further
enhance the LVLM's ability to model spatial details in video sequences, and
facilitate the LVLMs to accurately generate controlled intent-oriented
captions. Our proposed method achieved state-of-the-art results in several open
source LVLMs and was the runner-up in the IntentVC challenge. Our code is
available on https://github.com/thqiu0419/IntentVCNet.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [125] [ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid Mechanics](https://arxiv.org/abs/2507.17777)
*Theofanis Aravanis,Grigorios Chrimatopoulos,Mohammad Ferdows,Michalis Xenos,Efstratios Em Tzirtzilakis*

Main category: cs.AI

TL;DR: 本研究利用符号回归（SR）结合数值模拟数据，为三维矩形通道流体建立了可解释的数学模型，并提出SR与回答集编程（ASP）的混合框架，以确保模型物理合理性。


<details>
  <summary>Details</summary>
Motivation: 流体力学中，对底层流体物理机制的理解与准确预测同等重要。有别于常被批评为“黑箱”的传统机器学习方法，符号回归（SR）无需先验模型假设，能够揭示复杂物理系统中可解释的数学关系，因此本研究旨在应用SR来建模流体系统。

Method: 研究采用PySR库，直接从数值模拟数据中推导出三维矩形通道内层流（轴向速度和压力场）的紧凑符号方程。此外，提出了一种创新方法，将SR与回答集编程（ASP）的知识表示框架相结合，形成混合SR/ASP框架，旨在确保SR生成的符号表达式不仅统计准确，而且物理合理，符合领域特定原理。

Result: 通过SR推导出的符号方程，不仅能很好地近似所研究流体的抛物线速度分布和压降，而且与文献中的解析解完美吻合。所提出的SR/ASP混合框架，能够确保SR生成的符号表达式既统计准确又符合物理原理。

Conclusion: 研究突出了两项关键贡献：一是SR能够将复杂的流体行为简化为简洁、可解释的方程；二是知识表示方法（如ASP）有潜力提高数据驱动型SR模型的可靠性及其与领域原理的一致性。这些混合方法为未来在需要可解释预测和实时数据分析的领域中建立高效框架奠定了基础。

Abstract: Unlike conventional Machine-Learning (ML) approaches, often criticized as
"black boxes", Symbolic Regression (SR) stands out as a powerful tool for
revealing interpretable mathematical relationships in complex physical systems,
requiring no a priori assumptions about models' structures. Motivated by the
recognition that, in fluid mechanics, an understanding of the underlying flow
physics is as crucial as accurate prediction, this study applies SR to model a
fundamental three-dimensional (3D) incompressible flow in a rectangular
channel, focusing on the (axial) velocity and pressure fields under laminar
conditions. By employing the PySR library, compact symbolic equations were
derived directly from numerical simulation data, revealing key characteristics
of the flow dynamics. These equations not only approximate the parabolic
velocity profile and pressure drop observed in the studied fluid flow, but also
perfectly coincide with analytical solutions from the literature. Furthermore,
we propose an innovative approach that integrates SR with the
knowledge-representation framework of Answer Set Programming (ASP), combining
the generative power of SR with the declarative reasoning strengths of ASP. The
proposed hybrid SR/ASP framework ensures that the SR-generated symbolic
expressions are not only statistically accurate, but also physically plausible,
adhering to domain-specific principles. Overall, the study highlights two key
contributions: SR's ability to simplify complex flow behaviours into concise,
interpretable equations, and the potential of knowledge-representation
approaches to improve the reliability and alignment of data-driven SR models
with domain principles. Insights from the examined 3D channel flow pave the way
for integrating such hybrid approaches into efficient frameworks, [...] where
explainable predictions and real-time data analysis are crucial.

</details>


### [126] [I2I-STRADA -- Information to Insights via Structured Reasoning Agent for Data Analysis](https://arxiv.org/abs/2507.17874)
*SaiBarath Sundar,Pranav Satheesan,Udayaadithya Avadhanam*

Main category: cs.AI

TL;DR: 本研究引入了I2I-STRADA，一个用于数据分析的智能体架构，通过模拟结构化认知推理步骤，显著提升了现有数据分析系统在规划连贯性和洞察一致性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据分析智能体系统虽能自动化处理任务，但常忽略分析思维背后的结构化推理过程；通用大型语言模型（LLMs）的推理步骤不固定，而真实数据分析需要一致的认知工作流。因此，亟需一种能形式化数据分析推理过程的架构。

Method: 提出了I2I-STRADA（Information-to-Insight via Structured Reasoning Agent for Data Analysis）智能体架构。该架构通过建模分析如何通过反映分析推理认知步骤的模块化子任务来展开，从而形式化了数据分析的结构化推理过程。

Result: 在DABstep和DABench基准测试上的评估显示，I2I-STRADA在规划连贯性（planning coherence）和洞察一致性（insight alignment）方面均优于现有系统。

Conclusion: 研究结果突显了在数据分析智能体设计中，结构化认知工作流的重要性。

Abstract: Recent advances in agentic systems for data analysis have emphasized
automation of insight generation through multi-agent frameworks, and
orchestration layers. While these systems effectively manage tasks like query
translation, data transformation, and visualization, they often overlook the
structured reasoning process underlying analytical thinking. Reasoning large
language models (LLMs) used for multi-step problem solving are trained as
general-purpose problem solvers. As a result, their reasoning or thinking steps
do not adhere to fixed processes for specific tasks. Real-world data analysis
requires a consistent cognitive workflow: interpreting vague goals, grounding
them in contextual knowledge, constructing abstract plans, and adapting
execution based on intermediate outcomes. We introduce I2I-STRADA
(Information-to-Insight via Structured Reasoning Agent for Data Analysis), an
agentic architecture designed to formalize this reasoning process. I2I-STRADA
focuses on modeling how analysis unfolds via modular sub-tasks that reflect the
cognitive steps of analytical reasoning. Evaluations on the DABstep and DABench
benchmarks show that I2I-STRADA outperforms prior systems in planning coherence
and insight alignment, highlighting the importance of structured cognitive
workflows in agent design for data analysis.

</details>


### [127] [SMARTAPS: Tool-augmented LLMs for Operations Management](https://arxiv.org/abs/2507.17927)
*Timothy Tin Long Yu,Mahdi Mostajabdaveh,Jabo Serge Byusa,Rindra Ramamonjison,Giuseppe Carenini,Kun Mao,Zirui Zhou,Yong Zhang*

Main category: cs.AI

TL;DR: SmartAPS是一个基于工具增强型大语言模型的对话系统，旨在通过提供直观的自然语言界面，解决传统高级计划系统（APS）因高昂定制和维护成本而难以普及的问题，从而使运营规划者能更便捷地管理其运营。


<details>
  <summary>Details</summary>
Motivation: 传统高级计划系统（APS）功能强大，但其高昂的定制和维护成本（主要源于顾问费用）导致许多潜在用户无法承担，限制了其普及。供应链规划者迫切需要一个更易于访问和使用的APS解决方案。

Method: 本文提出了SmartAPS系统，它是一个基于工具增强型大语言模型（tool-augmented LLM）构建的对话系统。该系统为运营规划者提供了一个直观的自然语言聊天界面。

Result: SmartAPS系统能够使运营规划者通过自然语言界面执行以下操作：查询信息、进行反事实推理、接收运营建议以及执行情景分析，从而更好地管理其运营。

Conclusion: SmartAPS成功地利用工具增强型大语言模型，为运营规划者提供了一个更可访问、更直观的高级计划系统，有效解决了传统APS的成本和易用性问题，提升了运营管理的效率和便捷性。

Abstract: Large language models (LLMs) present intriguing opportunities to enhance user
interaction with traditional algorithms and tools in real-world applications.
An advanced planning system (APS) is a sophisticated software that leverages
optimization to help operations planners create, interpret, and modify an
operational plan. While highly beneficial, many customers are priced out of
using an APS due to the ongoing costs of consultants responsible for
customization and maintenance. To address the need for a more accessible APS
expressed by supply chain planners, we present SmartAPS, a conversational
system built on a tool-augmented LLM. Our system provides operations planners
with an intuitive natural language chat interface, allowing them to query
information, perform counterfactual reasoning, receive recommendations, and
execute scenario analysis to better manage their operation. A short video
demonstrating the system has been released: https://youtu.be/KtIrJjlDbyw

</details>


### [128] [Synthesis of timeline-based planning strategies avoiding determinization](https://arxiv.org/abs/2507.17988)
*Dario Della Monica,Angelo Montanari,Pietro Sala*

Main category: cs.AI

TL;DR: 本文识别了定性时间线规划的一个片段，其存在性问题可直接映射到确定性有限自动机（DFA）的非空性问题，从而可以直接合成规划策略，解决了非确定性自动机（NFA）合成策略时需昂贵确定化步骤的问题。


<details>
  <summary>Details</summary>
Motivation: 定性时间线规划的计划存在性问题已被证明是PSPACE完全的，并通过归约到非确定性有限自动机（NFA）的非空性问题来证明PSPACE成员资格。然而，NFA不能直接用于合成规划策略，因为需要一个耗费成本的确定化步骤，这限制了其在策略合成中的直接应用。

Method: 研究者识别了定性时间线规划的一个特定片段，其计划存在性问题可以直接映射到确定性有限自动机（DFA）的非空性问题。此外，他们还识别了适合该确定性片段的Allen关系的一个最大子集。

Result: 研究结果表明，所识别的定性时间线规划片段的计划存在性问题能够直接映射到确定性有限自动机的非空性问题。这一映射使得规划策略可以直接合成，避免了昂贵的确定化步骤。同时，还确定了一个与该确定性片段兼容的Allen关系的最大子集。

Conclusion: 通过识别一个可直接映射到确定性有限自动机的定性时间线规划片段，本研究提供了一种更有效的方法来合成规划策略，克服了现有NFA方法在策略合成上的局限性，从而提高了规划策略生成的实用性。

Abstract: Qualitative timeline-based planning models domains as sets of independent,
but
  interacting, components whose behaviors over time, the timelines, are
governed
  by sets of qualitative temporal constraints (ordering relations), called
  synchronization rules.
  Its plan-existence problem has been shown to be PSPACE-complete; in
  particular, PSPACE-membership has been proved via reduction to the
  nonemptiness problem for nondeterministic finite automata.
  However, nondeterministic automata cannot be directly used to synthesize
  planning strategies as a costly determinization step is needed.
  In this paper, we identify a fragment of qualitative timeline-based planning
  whose plan-existence problem can be directly mapped into the nonemptiness
  problem of deterministic finite automata, which can then
  synthesize strategies.
  In addition, we identify a maximal subset of Allen's relations that fits into
  such a deterministic fragment.

</details>


### [129] [E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI](https://arxiv.org/abs/2507.18004)
*Yusen Peng,Shuhua Mao*

Main category: cs.AI

TL;DR: 论文提出E.A.R.T.H.框架，通过将AI生成的错误转化为创意资产，显著提升了AI的创造力，旨在实现超越模仿的真正创造。


<details>
  <summary>Details</summary>
Motivation: 探讨并解决人工智能如何超越简单的模仿，实现真正的创造力的问题。

Method: 提出了E.A.R.T.H.框架，这是一个五阶段生成管道（错误生成、放大、精炼选择、转换、反馈利用），旨在将模型生成的错误转化为创意资产。该方法借鉴认知科学和生成建模，利用结构化提示、语义评分和人机协同评估。具体实现中使用了LLaMA-2-7B-Chat、SBERT、BERTScore、CLIP、BLIP-2和Stable Diffusion等工具，并采用基于新颖性、惊喜度和相关性的复合奖励函数进行优化。

Result: 在Refine阶段，创造力得分提高了52.5%（从1.179增至1.898），最终输出的创造力得分达到2.010，提升了70.4%。精炼后的口号平均缩短48.4%，新颖性增加40.7%，而相关性仅下降4.0%。跨模态测试显示口号与图像的强对齐（CLIPScore：0.249；BERTScore F1：0.816）。在人工评估中，60%的输出得分达到或超过4.0，其中比喻性口号（平均4.09）优于字面性口号（3.99）。用户反馈强调了风格的精确性和情感共鸣。

Conclusion: 研究结果表明，以错误为中心、反馈驱动的生成方法能够显著增强AI的创造力，为实现自我演化和与人类对齐的创造性AI提供了一条可扩展的路径。

Abstract: How can AI move beyond imitation toward genuine creativity? This paper
proposes the E.A.R.T.H. framework, a five-stage generative pipeline that
transforms model-generated errors into creative assets through Error
generation, Amplification, Refine selection, Transform, and Harness feedback.
Drawing on cognitive science and generative modeling, we posit that "creative
potential hides in failure" and operationalize this via structured prompts,
semantic scoring, and human-in-the-loop evaluation. Implemented using
LLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the
pipeline employs a composite reward function based on novelty, surprise, and
relevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to
1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4%
improvement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a
4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment
(CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs
scored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones
(3.99). Feedback highlights stylistic precision and emotional resonance. These
results demonstrate that error-centered, feedback-driven generation enhances
creativity, offering a scalable path toward self-evolving, human-aligned
creative AI.

</details>


### [130] [Does visualization help AI understand data?](https://arxiv.org/abs/2507.18022)
*Victoria R. Li,Johnathan Sun,Martin Wattenberg*

Main category: cs.AI

TL;DR: 研究发现AI系统（如GPT 4.1和Claude 3.5）在有散点图辅助时能更精确准确地分析数据，表明AI与人类一样可从数据可视化中获益。


<details>
  <summary>Details</summary>
Motivation: 探讨图表和图形是否也能像帮助人类一样，对AI系统的数据分析有所助益。

Method: 使用GPT 4.1和Claude 3.5两种商业视觉语言模型，在三种代表性分析任务中进行一系列实验，比较原始数据在有或无散点图辅助下的分析表现，并与提供空白图表和数据不匹配图表的基线进行对比。

Result: 当原始数据配有散点图时，GPT 4.1和Claude 3.5在描述合成数据集方面表现出更高的精确度和准确性，尤其是在数据集复杂性增加时。与基线的比较证实，性能提升是由于图表内容的有效性。

Conclusion: 研究结果初步证明，AI系统与人类一样，能够从数据可视化中受益。

Abstract: Charts and graphs help people analyze data, but can they also be useful to AI
systems? To investigate this question, we perform a series of experiments with
two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three
representative analysis tasks, the two systems describe synthetic datasets more
precisely and accurately when raw data is accompanied by a scatterplot,
especially as datasets grow in complexity. Comparison with two baselines --
providing a blank chart and a chart with mismatched data -- shows that the
improved performance is due to the content of the charts. Our results are
initial evidence that AI systems, like humans, can benefit from visualization.

</details>


### [131] [Multi-Agent Guided Policy Optimization](https://arxiv.org/abs/2507.18059)
*Yueheng Li,Guangming Xie,Zongqing Lu*

Main category: cs.AI

TL;DR: 在多智能体强化学习中，MAGPO通过整合中心化指导和去中心化执行，解决了现有CTDE方法利用不足和缺乏理论保证的问题，实现了卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 合作多智能体强化学习（MARL）中，尽管中心化训练与去中心化执行（CTDE）是主流范式，但现有CTDE方法往往未能充分利用中心化训练或缺乏理论保证，这源于部分可观察性和有限通信等实际限制。

Method: 提出Multi-Agent Guided Policy Optimization (MAGPO) 框架。该框架通过整合中心化指导与去中心化执行，更好地利用中心化训练。MAGPO采用自回归联合策略进行可扩展的协同探索，并明确将其与去中心化策略对齐以确保在部分可观察性下的可部署性。此外，提供了单调策略改进的理论保证。

Result: 实验结果显示，MAGPO在43个任务和6个不同环境中，持续优于强大的CTDE基线，并与完全中心化方法持平或超越。

Conclusion: MAGPO为去中心化多智能体学习提供了一种有原则且实用的解决方案。

Abstract: Due to practical constraints such as partial observability and limited
communication, Centralized Training with Decentralized Execution (CTDE) has
become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning
(MARL). However, existing CTDE methods often underutilize centralized training
or lack theoretical guarantees. We propose Multi-Agent Guided Policy
Optimization (MAGPO), a novel framework that better leverages centralized
training by integrating centralized guidance with decentralized execution.
MAGPO uses an auto-regressive joint policy for scalable, coordinated
exploration and explicitly aligns it with decentralized policies to ensure
deployability under partial observability. We provide theoretical guarantees of
monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across
6 diverse environments. Results show that MAGPO consistently outperforms strong
CTDE baselines and matches or surpasses fully centralized approaches, offering
a principled and practical solution for decentralized multi-agent learning. Our
code and experimental data can be found in https://github.com/liyheng/MAGPO.

</details>


### [132] [AlphaGo Moment for Model Architecture Discovery](https://arxiv.org/abs/2507.18074)
*Yixiu Liu,Yang Nan,Weixian Xu,Xiangkun Hu,Lyumanshan Ye,Zhen Qin,Pengfei Liu*

Main category: cs.AI

TL;DR: 本文提出了ASI-Arch，一个AI超智能系统（ASI4AI），能在神经架构发现领域进行完全自主的科学研究，超越人类认知限制。该系统自主发现了106个最先进的（SOTA）线性注意力架构，并建立了科学发现的第一个经验缩放定律，表明研究进展可随计算资源扩展。


<details>
  <summary>Details</summary>
Motivation: AI系统能力呈指数级提升，但AI研究进展仍受限于人类认知能力，形成日益严重的开发瓶颈。传统神经架构搜索（NAS）仅限于探索人类定义的空间，无法实现AI自主创新。

Method: 开发了ASI-Arch系统，实现神经架构发现的端到端自主科学研究。该系统能自主提出新颖架构概念、将其实现为可执行代码、训练并通过严格实验和经验验证其性能，实现了从自动化优化到自动化创新的范式转变。

Result: ASI-Arch进行了1,773次自主实验（耗时20,000 GPU小时），成功发现了106个创新且达到SOTA水平的线性注意力架构。这些AI发现的架构展现出超越人类设计的涌现设计原则。研究首次建立了科学发现的经验缩放定律，证明架构突破可以通过计算进行扩展。

Conclusion: ASI-Arch打破了AI研究中人类认知的根本性限制，将研究进展从受人类限制转变为可计算扩展的过程。该研究为自加速AI系统提供了蓝图，并揭示了新的架构创新途径。

Abstract: While AI systems demonstrate exponentially improving capabilities, the pace
of AI research itself remains linearly bounded by human cognitive capacity,
creating an increasingly severe development bottleneck. We present ASI-Arch,
the first demonstration of Artificial Superintelligence for AI research
(ASI4AI) in the critical domain of neural architecture discovery--a fully
autonomous system that shatters this fundamental constraint by enabling AI to
conduct its own architectural innovation. Moving beyond traditional Neural
Architecture Search (NAS), which is fundamentally limited to exploring
human-defined spaces, we introduce a paradigm shift from automated optimization
to automated innovation. ASI-Arch can conduct end-to-end scientific research in
the domain of architecture discovery, autonomously hypothesizing novel
architectural concepts, implementing them as executable code, training and
empirically validating their performance through rigorous experimentation and
past experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000
GPU hours, culminating in the discovery of 106 innovative, state-of-the-art
(SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed
unexpected strategic insights invisible to human players, our AI-discovered
architectures demonstrate emergent design principles that systematically
surpass human-designed baselines and illuminate previously unknown pathways for
architectural innovation. Crucially, we establish the first empirical scaling
law for scientific discovery itself--demonstrating that architectural
breakthroughs can be scaled computationally, transforming research progress
from a human-limited to a computation-scalable process. We provide
comprehensive analysis of the emergent design patterns and autonomous research
capabilities that enabled these breakthroughs, establishing a blueprint for
self-accelerating AI systems.

</details>


### [133] [Agentic AI framework for End-to-End Medical Data Inference](https://arxiv.org/abs/2507.18115)
*Soorya Ram Shimgekar,Shayan Vassef,Abhay Goyal,Navin Kumar,Koustuv Saha*

Main category: cs.AI

TL;DR: 提出一个Agentic AI框架，通过自动化临床数据管线，解决医疗领域机器学习部署成本高、劳动密集的问题，实现从数据摄取到可解释推理的全流程自动化。


<details>
  <summary>Details</summary>
Motivation: 医疗领域机器学习解决方案的构建和部署因预处理流程碎片化、模型兼容性问题以及严格的数据隐私限制而昂贵且劳动密集。

Method: 引入一个Agentic AI框架，由模块化、任务特定型代理组成，自动化整个临床数据管线。这些代理处理结构化和非结构化数据，自动进行特征选择、模型选择和预处理推荐。具体代理包括摄取识别、数据匿名化、特征提取（基于嵌入和MedGemma）、模型-数据特征匹配、预处理推荐与实施，以及模型推理（生成可解释输出）。

Result: 该系统在老年病学、姑息治疗和结肠镜成像等公开数据集上进行了评估，成功自动化了数据摄取到推理的流程，并生成了可解释的输出。通过自动化高摩擦阶段，显著减少了对重复专家干预的需求。

Conclusion: 该框架通过自动化机器学习生命周期中的复杂阶段，降低了对专家干预的依赖，为在临床环境中操作化AI提供了一条可扩展、成本效益高的新途径。

Abstract: Building and deploying machine learning solutions in healthcare remains
expensive and labor-intensive due to fragmented preprocessing workflows, model
compatibility issues, and stringent data privacy constraints. In this work, we
introduce an Agentic AI framework that automates the entire clinical data
pipeline, from ingestion to inference, through a system of modular,
task-specific agents. These agents handle both structured and unstructured
data, enabling automatic feature selection, model selection, and preprocessing
recommendation without manual intervention. We evaluate the system on publicly
available datasets from geriatrics, palliative care, and colonoscopy imaging.
For example, in the case of structured data (anxiety data) and unstructured
data (colonoscopy polyps data), the pipeline begins with file-type detection by
the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring
privacy compliance, where we first identify the data type and then anonymize
it. The Feature Extraction Agent identifies features using an embedding-based
approach for tabular data, extracting all column names, and a multi-stage
MedGemma-based approach for image data, which infers modality and disease name.
These features guide the Model-Data Feature Matcher Agent in selecting the
best-fit model from a curated repository. The Preprocessing Recommender Agent
and Preprocessing Implementor Agent then apply tailored preprocessing based on
data type and model requirements. Finally, the ``Model Inference Agent" runs
the selected model on the uploaded data and generates interpretable outputs
using tools like SHAP, LIME, and DETR attention maps. By automating these
high-friction stages of the ML lifecycle, the proposed framework reduces the
need for repeated expert intervention, offering a scalable, cost-efficient
pathway for operationalizing AI in clinical environments.

</details>


### [134] [Actively evaluating and learning the distinctions that matter: Vaccine safety signal detection from emergency triage notes](https://arxiv.org/abs/2507.18123)
*Sedigh Khademi,Christopher Palmer,Muhammad Javed,Hazel Clothier,Jim Buttery,Gerardo Luis Dimaguila,Jim Black*

Main category: cs.AI

TL;DR: 本研究利用自然语言处理（NLP）和主动学习（Active Learning），从急诊科分诊笔记中快速开发疫苗安全问题分类器，以加强上市后疫苗监测。


<details>
  <summary>Details</summary>
Motivation: 由于临床试验窗口期短和疫苗快速广泛接种，上市后疫苗安全监测需求日益增长。急诊科笔记是宝贵的早期信号源，但传统关键词方法效率低、误报多。同时，医学领域NLP所需的标注数据稀缺。

Method: 采用自然语言处理（NLP）技术，并结合主动学习（Active Learning）以优化数据标注过程和数据质量。此外，还整合了数据增强（Data Augmentation）和主动学习评估技术，旨在构建一个能从急诊科分诊笔记中检测潜在疫苗安全问题的分类器。

Result: 通过结合NLP、主动学习和数据增强，成功开发了一个用于疫苗安全监测的分类器。此方法优化了数据标注过程，实现了模型快速部署，并显著提升了模型性能，从而增强了从急诊科笔记中及时发现疫苗安全信号的能力。

Conclusion: 本工作提供了一种高效且准确的疫苗安全监测新范式，通过创新性地结合NLP、主动学习和数据增强，有效克服了医疗领域标注数据稀缺的挑战，对公共卫生监测具有重要意义。

Abstract: The rapid development of COVID-19 vaccines has showcased the global
communitys ability to combat infectious diseases. However, the need for
post-licensure surveillance systems has grown due to the limited window for
safety data collection in clinical trials and early widespread implementation.
This study aims to employ Natural Language Processing techniques and Active
Learning to rapidly develop a classifier that detects potential vaccine safety
issues from emergency department notes. ED triage notes, containing expert,
succinct vital patient information at the point of entry to health systems, can
significantly contribute to timely vaccine safety signal surveillance. While
keyword-based classification can be effective, it may yield false positives and
demand extensive keyword modifications. This is exacerbated by the infrequency
of vaccination-related ED presentations and their similarity to other reasons
for ED visits. NLP offers a more accurate and efficient alternative, albeit
requiring annotated data, which is often scarce in the medical field. Active
learning optimizes the annotation process and the quality of annotated data,
which can result in faster model implementation and improved model performance.
This work combines active learning, data augmentation, and active learning and
evaluation techniques to create a classifier that is used to enhance vaccine
safety surveillance from ED triage notes.

</details>


### [135] [Logical Characterizations of GNNs with Mean Aggregation](https://arxiv.org/abs/2507.18145)
*Moritz Schönherr,Carsten Lutz*

Main category: cs.AI

TL;DR: 研究表明，使用平均值作为聚合函数的图神经网络（GNNs）在非均匀设置下与比例模态逻辑表达能力相同，在均匀设置下，其相对于MSO的表达能力与无交替模态逻辑相同，且低于求和和最大值聚合GNNs。


<details>
  <summary>Details</summary>
Motivation: 论文旨在量化和理解使用平均值聚合函数的图神经网络（GNNs）的表达能力，并将其与现有GNNs（如使用求和或最大值聚合的GNNs）以及逻辑系统进行比较。

Method: 通过将平均值聚合GNNs的表达能力与不同的模态逻辑（如比例模态逻辑、无交替模态逻辑）进行对比来分析。在均匀设置下，还考虑了组合函数连续性和分类函数为阈值的自然假设。

Result: 在非均匀设置下，平均值GNNs的表达能力与比例模态逻辑完全一致，高于最大值聚合GNNs，但低于求和聚合GNNs。在均匀设置下，在特定假设下，平均值GNNs相对于MSO的表达能力与无交替模态逻辑相同，且严格低于求和和最大值聚合GNNs。若移除任何假设，表达能力会增加。

Conclusion: 平均值聚合GNNs的表达能力在不同设置下表现出独特的特性，其在某些方面优于最大值聚合，但在另一些方面（尤其是在均匀设置下）则低于求和和最大值聚合GNNs。研究揭示了这些GNNs的逻辑界限和潜在局限性。

Abstract: We study the expressive power of graph neural networks (GNNs) with mean as
the aggregation function. In the non-uniform setting, we show that such GNNs
have exactly the same expressive power as ratio modal logic, which has modal
operators expressing that at least a certain ratio of the successors of a
vertex satisfies a specified property. The non-uniform expressive power of mean
GNNs is thus higher than that of GNNs with max aggregation, but lower than for
sum aggregation--the latter are characterized by modal logic and graded modal
logic, respectively. In the uniform setting, we show that the expressive power
relative to MSO is exactly that of alternation-free modal logic, under the
natural assumptions that combination functions are continuous and
classification functions are thresholds. This implies that, relative to MSO and
in the uniform setting, mean GNNs are strictly less expressive than sum GNNs
and max GNNs. When any of the assumptions is dropped, the expressive power
increases.

</details>


### [136] [Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive Dual-System Theory](https://arxiv.org/abs/2507.18178)
*Mutian Yang,Jiandong Gao,Ji Wu*

Main category: cs.AI

TL;DR: 本研究提出一种认知归因框架，旨在解耦大型语言模型（LLMs）中的知识与推理贡献。通过模拟快思考和慢思考两种模式，量化分析了它们的作用，并揭示了推理的领域特异性、参数缩放对两者能力的影响以及知识与推理在模型层级上的分布。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLMs）的推理过程中，知识与推理能力的区分对于模型分析、可解释性及发展至关重要。

Method: 受双系统认知理论启发，提出一个认知归因框架，将LLM的认知分解为知识检索（阶段1）和推理调整（阶段2）。通过提示LLM在“快思考”和“慢思考”两种认知模式下生成答案，分离并量化知识和推理的贡献。此框架应用于15个LLM和3个数据集进行评估。

Result: ['推理调整具有领域特异性，有利于推理密集型领域（如数学、物理、化学），但可能损害知识密集型领域。', '参数缩放可同时提升知识和推理能力，其中知识的提升更为显著。此外，参数缩放使LLM的推理更审慎，智能程度适度提高。', '知识主要存在于网络较低层，而推理则在较高层发挥作用。']

Conclusion: 本框架不仅从“解耦”视角加深了对LLM的理解，还为现有研究（包括缩放法则、分层知识编辑和小型模型推理局限性）提供了新见解。

Abstract: While large language models (LLMs) leverage both knowledge and reasoning
during inference, the capacity to distinguish between them plays a pivotal role
in model analysis, interpretability, and development. Inspired by dual-system
cognitive theory, we propose a cognition attribution framework to decouple the
contribution of knowledge and reasoning. In particular, the cognition of LLMs
is decomposed into two distinct yet complementary phases: knowledge retrieval
(Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs
are prompted to generate answers under two different cognitive modes, fast
thinking and slow thinking, respectively. The performance under different
cognitive modes is analyzed to quantify the contribution of knowledge and
reasoning. This architecture is employed to 15 LLMs across 3 datasets. Results
reveal: (1) reasoning adjustment is domain-specific, benefiting
reasoning-intensive domains (e.g., mathematics, physics, and chemistry) and
potentially imparing knowledge-intensive domains. (2) Parameter scaling
improves both knowledge and reasoning, with knowledge improvements being more
pronounced. Additionally, parameter scaling make LLMs reasoning significantly
more prudent, while moderately more intelligent. (3) Knowledge primarily
resides in lower network layers, while reasoning operates in higher layers. Our
framework not only helps understand LLMs from a "decoupling" perspective, but
also provides new insights into existing research, including scaling laws,
hierarchical knowledge editing, and limitations of small-model reasoning.

</details>


### [137] [Comparing Non-minimal Semantics for Disjunction in Answer Set Programming](https://arxiv.org/abs/2507.18198)
*Felicidad Aguado,Pedro Cabalar,Brais Muñiz,Gilberto Pérez,Concepción Vidal*

Main category: cs.AI

TL;DR: 论文比较了ASP中四种不遵循模型最小性的析取语义。研究发现其中三种（Forks、Justified Models和合理松弛的DI语义）实际上是等价的，构成一个共同语义，且该共同语义是稳定模型的超集，并严格强于第四种语义（Strongly Supported Models）。


<details>
  <summary>Details</summary>
Motivation: 传统的ASP稳定模型语义遵循模型最小性原则，但在某些场景下可能需要更灵活的析取处理方式。本研究旨在探索和比较ASP中不遵循模型最小性的析取语义，以扩展其表达能力并提供新的处理范式。

Method: 论文对比分析了四种ASP析取语义：Cabalar和Mu\~niz的Justified Models、Doherty和Szalas的Strongly Supported Models、Aguado et al的Forks以及Shen和Eiter的Determining Inference (DI) 语义。Forks和DI语义虽引入了新的析取联结词，但在此作为标准析取算子的新语义进行比较。研究通过理论证明来分析这些语义之间的等价性和强度关系。

Result: 1. Forks、Justified Models和合理松弛的DI语义这三种方法在不同定义下被证明是等价的，构成了一个共同的单一方法。
2. 这种共同语义总是提供程序稳定模型的超集。
3. 这种共同语义严格强于第四种方法（Strongly Supported Models），后者将析取视为经典逻辑。

Conclusion: 研究揭示了ASP中几种非最小性析取语义之间的内在联系，特别是Forks、Justified Models和DI语义的等价性，形成了一个统一的非最小性析取语义。这一共同语义不仅扩展了稳定模型，而且比将析取视为经典逻辑的方法更强，为ASP中处理不确定性和析取提供了新的视角和更强大的工具。

Abstract: In this paper, we compare four different semantics for disjunction in Answer
Set Programming that, unlike stable models, do not adhere to the principle of
model minimality. Two of these approaches, Cabalar and Mu\~niz' \emph{Justified
Models} and Doherty and Szalas' \emph{Strongly Supported Models}, directly
provide an alternative non-minimal semantics for disjunction. The other two,
Aguado et al's \emph{Forks} and Shen and Eiter's \emph{Determining Inference}
(DI) semantics, actually introduce a new disjunction connective, but are
compared here as if they constituted new semantics for the standard disjunction
operator. We are able to prove that three of these approaches (Forks, Justified
Models and a reasonable relaxation of the DI semantics) actually coincide,
constituting a common single approach under different definitions. Moreover,
this common semantics always provides a superset of the stable models of a
program (in fact, modulo any context) and is strictly stronger than the fourth
approach (Strongly Supported Models), that actually treats disjunctions as in
classical logic.

</details>


### [138] [Foundations for Risk Assessment of AI in Protecting Fundamental Rights](https://arxiv.org/abs/2507.18290)
*Antonino Rotolo,Beatrice Ferrigno,Jose Miguel Angel Garcia Godinez,Claudio Novelli,Giovanni Sartor*

Main category: cs.AI

TL;DR: 针对欧盟AI法案，本文提出一个AI风险定性评估的概念框架，该框架整合了定义平衡和可废止推理，以处理法律合规性与基本权利保护的复杂性。


<details>
  <summary>Details</summary>
Motivation: 为解决欧盟AI法案背景下AI系统在法律合规性和基本权利保护方面的复杂性，研究旨在构建一个系统性的定性风险评估框架。

Method: 本研究通过整合“定义平衡”（运用比例分析解决权利冲突）和“可废止推理”（适应法律决策动态性）来构建分层概念框架。该方法强调分析AI部署场景，识别潜在法律违规及对基本权利的多层次影响。

Result: 提出了AI风险定性评估的概念框架，并为AI风险分析的逻辑解释奠定了哲学基础。该分层方法能够为高风险AI系统和通用AI系统提供更具操作性的评估模型。

Conclusion: 本研究为AI风险评估提供了一个创新性的概念框架，尤其关注法律合规和基本权利保护，为负责任的AI治理奠定了理论基础，并展望了未来的形式化模型和算法开发。

Abstract: This chapter introduces a conceptual framework for qualitative risk
assessment of AI, particularly in the context of the EU AI Act. The framework
addresses the complexities of legal compliance and fundamental rights
protection by itegrating definitional balancing and defeasible reasoning.
Definitional balancing employs proportionality analysis to resolve conflicts
between competing rights, while defeasible reasoning accommodates the dynamic
nature of legal decision-making. Our approach stresses the need for an analysis
of AI deployment scenarios and for identifying potential legal violations and
multi-layered impacts on fundamental rights. On the basis of this analysis, we
provide philosophical foundations for a logical account of AI risk analysis. In
particular, we consider the basic building blocks for conceptually grasping the
interaction between AI deployment scenarios and fundamental rights,
incorporating in defeasible reasoning definitional balancing and arguments
about the contextual promotion or demotion of rights. This layered approach
allows for more operative models of assessment of both high-risk AI systems and
General Purpose AI (GPAI) systems, emphasizing the broader applicability of the
latter. Future work aims to develop a formal model and effective algorithms to
enhance AI risk assessment, bridging theoretical insights with practical
applications to support responsible AI governance.

</details>


### [139] [The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions in Physics Exams](https://arxiv.org/abs/2507.18337)
*Peter Baumgartner,Lachlan McGinness*

Main category: cs.AI

TL;DR: 本文提出了一种结合大语言模型、计算机代数系统、SMT求解器和项重写系统来自动批改物理考试的方法，并在真实的竞赛学生答卷上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 自动批改物理考试是一个具有挑战性的问题，需要评估学生打字答案相对于正确解的准确性，本研究旨在解决这一难题。

Method: 该方法结合了计算机代数系统（CAS）、SMT求解器和项重写系统。首先，使用大型语言模型（LLM）解释、纠正学生答案中的错误，并将其转换为机器可读格式。随后，利用自动化推理技术评估学生解决方案的正确性，具体包括使用现成的SMT求解和为涉及三角表达式的物理问题定制的项重写系统。项重写系统的开发及其终止性和合流性属性的建立是关键。

Result: 该系统在来自2023年澳大利亚物理奥林匹克竞赛的1500多份真实学生答卷的丰富样本池上进行了评估。

Conclusion: 该研究成功开发并评估了一种用于自动批改物理考试的新颖方法，该方法融合了多种先进的自动化推理和语言处理技术，并已在真实世界的学生数据上进行了验证。

Abstract: We present our method for automatically marking Physics exams. The marking
problem consists in assessing typed student answers for correctness with
respect to a ground truth solution. This is a challenging problem that we seek
to tackle using a combination of a computer algebra system, an SMT solver and a
term rewriting system. A Large Language Model is used to interpret and remove
errors from student responses and rewrite these in a machine readable format.
Once formalized and language-aligned, the next step then consists in applying
automated reasoning techniques for assessing student solution correctness. We
consider two methods of automated theorem proving: off-the-shelf SMT solving
and term rewriting systems tailored for physics problems involving
trigonometric expressions. The development of the term rewrite system and
establishing termination and confluence properties was not trivial, and we
describe it in some detail in the paper. We evaluate our system on a rich pool
of over 1500 real-world student exam responses from the 2023 Australian Physics
Olympiad.

</details>


### [140] [Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios](https://arxiv.org/abs/2507.18368)
*Zhuang Qiang Bok,Watson Wei Khong Chua*

Main category: cs.AI

TL;DR: 引入ConDiFi基准，评估LLM在金融领域的分歧与收敛思维，发现不同模型表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准侧重事实准确性或逻辑推理，但金融领域需要同时具备发散性（创新性未来预测）和收敛性（最优决策）思维。因此，需要一个能同时评估这两种能力的基准。

Method: 提出ConDiFi基准，包含607个用于发散性推理的宏观金融提示和990个用于收敛性推理的多跳对抗性多项选择题。利用此基准评估了14个主流LLM模型。

Result: 评估结果显示，尽管GPT-4o流畅性高，但在新颖性和可操作性方面表现不佳。相比之下，DeepSeek-R1和Cohere Command R+等模型在生成可操作、适合投资决策的洞察方面表现出色。

Conclusion: ConDiFi为评估LLM在金融领域中至关重要的推理能力提供了新视角，有助于其安全和战略性部署。

Abstract: Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step
logic. In finance, however, professionals must not only converge on optimal
decisions but also generate creative, plausible futures under uncertainty. We
introduce ConDiFi, a benchmark that jointly evaluates divergent and convergent
thinking in LLMs for financial tasks.
  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990
multi-hop adversarial MCQs for convergent reasoning. Using this benchmark, we
evaluated 14 leading models and uncovered striking differences. Despite high
fluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models
like DeepSeek-R1 and Cohere Command R+ rank among the top for generating
actionable, insights suitable for investment decisions. ConDiFi provides a new
perspective to assess reasoning capabilities essential to safe and strategic
deployment of LLMs in finance.

</details>


### [141] [Revisiting LLM Reasoning via Information Bottleneck](https://arxiv.org/abs/2507.18391)
*Shiye Lei,Zhihao Cheng,Kai Jia,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出基于信息瓶颈（IB）原理的IB-aware reasoning optimization (IBRO) 框架，通过引入IB正则化方法，旨在提高大语言模型（LLMs）推理路径的信息量和泛化能力，并在数学推理任务中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在推理能力方面通过可验证奖励强化学习（RLVR）取得了进展，但现有方法多为启发式且缺乏理论指导，限制了系统性方法的发展。

Method: 提出IB-aware reasoning optimization (IBRO) 框架，其核心是基于信息瓶颈（IB）原理，旨在使推理路径既能有效传达最终正确答案的信息，又能在不同提示下具有泛化性。具体通过推导实用的token级替代目标和高效近似，形成了轻量级的IB正则化方法，可无缝集成到现有RL后训练框架中，仅需一行代码修改。

Result: 通过在多个数学推理基准和RL算法上进行验证，IB正则化方法持续提升了LLMs的推理性能。

Conclusion: IB正则化方法为LLM推理提供了一种更具原则性的优化途径，通过平衡信息量和泛化性，有效提升了模型的推理能力。

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in reasoning capabilities through reinforcement learning with verifiable
rewards (RLVR). By leveraging simple rule-based rewards, RL effectively
incentivizes LLMs to produce extended chain-of-thought (CoT) reasoning
trajectories, progressively guiding them toward correct answers. However,
existing approaches remain largely heuristic and intuition-driven, limiting the
development of principled methodologies. In this paper, we present a
theoretical characterization of LLM reasoning grounded in information
bottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),
a framework that encourages reasoning trajectories to be both informative about
the final correct answer and generalizable across diverse prompts. We derive a
practical token-level surrogate objective and propose an efficient
approximation, resulting in the lightweight IB regularization method. This
technique integrates seamlessly into existing RL-based post-training frameworks
without additional computational overhead, requiring only a one-line code
modification. Empirically, we validate IB regularization across multiple
mathematical reasoning benchmarks and RL algorithms, demonstrating consistent
improvements in LLM reasoning performance.

</details>


### [142] [Optimising Call Centre Operations using Reinforcement Learning: Value Iteration versus Proximal Policy Optimisation](https://arxiv.org/abs/2507.18398)
*Kwong Ho Li,Wathsala Karunarathne*

Main category: cs.AI

TL;DR: 本文探讨强化学习在呼叫中心路由优化中的应用，通过比较基于模型的价值迭代(VI)和无模型的代理策略优化(PPO)，发现PPO在最小化客户等待时间和员工空闲时间方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 优化呼叫中心呼叫路由，旨在最小化客户等待时间及员工空闲时间。

Method: 将问题建模为基于技能路由(SBR)框架下的马尔可夫决策过程(MDP)。比较两种强化学习方法：1) 基于模型的价值迭代(VI)，使用理论模型；2) 无模型的代理策略优化(PPO)，通过结合离散事件仿真(DES)和OpenAI Gym环境的仿真模型进行学习。评估策略包括随机策略、VI策略和PPO策略。

Result: 在1,000次测试回合后，PPO持续获得最高奖励，并实现了最低的客户等待时间和员工空闲时间，尽管其训练时间较长。

Conclusion: 强化学习，尤其是PPO，能有效优化呼叫中心呼叫路由，显著降低客户等待时间和员工空闲时间，证明其在此类优化问题中的优越性。

Abstract: This paper investigates the application of Reinforcement Learning (RL) to
optimise call routing in call centres to minimise client waiting time and staff
idle time. Two methods are compared: a model-based approach using Value
Iteration (VI) under known system dynamics, and a model-free approach using
Proximal Policy Optimisation (PPO) that learns from experience. For the
model-based approach, a theoretical model is used, while a simulation model
combining Discrete Event Simulation (DES) with the OpenAI Gym environment is
developed for model-free learning. Both models frame the problem as a Markov
Decision Process (MDP) within a Skills-Based Routing (SBR) framework, with
Poisson client arrivals and exponentially distributed service and abandonment
times. For policy evaluation, random, VI, and PPO policies are evaluated using
the simulation model. After 1,000 test episodes, PPO consistently achives the
highest rewards, along with the lowest client waiting time and staff idle time,
despite requiring longer training time.

</details>


### [143] [GPU Accelerated Compact-Table Propagation](https://arxiv.org/abs/2507.18413)
*Enrico Santi,Fabio Tardivo,Agostino Dovier,Andrea Formisano*

Main category: cs.AI

TL;DR: 本研究利用GPU的并行计算能力加速Compact-Table (CT)算法，以高效处理大型表约束。


<details>
  <summary>Details</summary>
Motivation: 表约束在处理具有大量有效案例的现实问题时，传统CPU方法效率低下，难以有效处理。

Method: 提出并实现了GPU加速的Compact-Table (CT)算法，涵盖了其设计、实现细节，以及如何将其集成到现有约束求解器中。

Result: 报告了GPU加速的CT算法的设计与实现，其与现有约束求解器的集成，以及在一组重要实例上进行的实验验证。

Conclusion: 本研究证明，利用GPU的强大计算能力可以显著提升Compact-Table算法处理大型表约束的效率，克服了传统CPU方法在处理此类问题时的局限性。

Abstract: Constraint Programming developed within Logic Programming in the Eighties;
nowadays all Prolog systems encompass modules capable of handling constraint
programming on finite domains demanding their solution to a constraint solver.
This work focuses on a specific form of constraint, the so-called table
constraint, used to specify conditions on the values of variables as an
enumeration of alternative options. Since every condition on a set of finite
domain variables can be ultimately expressed as a finite set of cases, Table
can, in principle, simulate any other constraint. These characteristics make
Table one of the most studied constraints ever, leading to a series of
increasingly efficient propagation algorithms. Despite this, it is not uncommon
to encounter real-world problems with hundreds or thousands of valid cases that
are simply too many to be handled effectively with standard CPU-based
approaches. In this paper, we deal with the Compact-Table (CT) algorithm, the
state-of-the-art propagation algorithms for Table. We describe how CT can be
enhanced by exploiting the massive computational power offered by modern GPUs
to handle large Table constraints. In particular, we report on the design and
implementation of GPU-accelerated CT, on its integration into an existing
constraint solver, and on an experimental validation performed on a significant
set of instances.

</details>


### [144] [On the Performance of Concept Probing: The Influence of the Data (Extended Version)](https://arxiv.org/abs/2507.18550)
*Manuel de Sousa Ribeiro,Afonso Leote,João Leite*

Main category: cs.AI

TL;DR: 本文研究了用于训练概念探测模型的数据对模型性能的影响，特别是在图像分类任务中，并提供了两个常用数据集的概念标签。


<details>
  <summary>Details</summary>
Motivation: 现有概念探测研究主要关注被探测模型或探测模型本身，而对训练探测模型所需数据的重要性关注不足，存在研究空白。

Method: 专注于图像分类任务中的概念探测，研究训练探测模型所用数据对其性能的影响。此外，为两个广泛使用的数据集提供了概念标签。

Result: 对训练数据如何影响概念探测模型的性能进行了调查，并为两个常用数据集提供了概念标签。

Conclusion: 通过关注训练数据对概念探测模型性能的影响并提供相关资源，本文填补了现有研究的空白，有助于更深入地理解和应用概念探测技术。

Abstract: Concept probing has recently garnered increasing interest as a way to help
interpret artificial neural networks, dealing both with their typically large
size and their subsymbolic nature, which ultimately renders them unfeasible for
direct human interpretation. Concept probing works by training additional
classifiers to map the internal representations of a model into human-defined
concepts of interest, thus allowing humans to peek inside artificial neural
networks. Research on concept probing has mainly focused on the model being
probed or the probing model itself, paying limited attention to the data
required to train such probing models. In this paper, we address this gap.
Focusing on concept probing in the context of image classification tasks, we
investigate the effect of the data used to train probing models on their
performance. We also make available concept labels for two widely used
datasets.

</details>


### [145] [SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\circ}$ Law](https://arxiv.org/abs/2507.18576)
*Shanghai AI Lab,:,Yicheng Bao,Guanxu Chen,Mingkang Chen,Yunhao Chen,Chiyu Chen,Lingjie Chen,Sirui Chen,Xinquan Chen,Jie Cheng,Yu Cheng,Dengke Deng,Yizhuo Ding,Dan Ding,Xiaoshan Ding,Yi Ding,Zhichen Dong,Lingxiao Du,Yuyu Fan,Xinshun Feng,Yanwei Fu,Yuxuan Gao,Ruijun Ge,Tianle Gu,Lujun Gui,Jiaxuan Guo,Qianxi He,Yuenan Hou,Xuhao Hu,Hong Huang,Kaichen Huang,Shiyang Huang,Yuxian Jiang,Shanzhe Lei,Jie Li,Lijun Li,Hao Li,Juncheng Li,Xiangtian Li,Yafu Li,Lingyu Li,Xueyan Li,Haotian Liang,Dongrui Liu,Qihua Liu,Zhixuan Liu,Bangwei Liu,Huacan Liu,Yuexiao Liu,Zongkai Liu,Chaochao Lu,Yudong Lu,Xiaoya Lu,Zhenghao Lu,Qitan Lv,Caoyuan Ma,Jiachen Ma,Xiaoya Ma,Zhongtian Ma,Lingyu Meng,Ziqi Miao,Yazhe Niu,Yuezhang Peng,Yuan Pu,Han Qi,Chen Qian,Xingge Qiao,Jingjing Qu,Jiashu Qu,Wanying Qu,Wenwen Qu,Xiaoye Qu,Qihan Ren,Qingnan Ren,Qingyu Ren,Jing Shao,Wenqi Shao,Shuai Shao,Dongxing Shi,Xin Song,Xinhao Song,Yan Teng,Xuan Tong,Yingchun Wang,Xuhong Wang,Shujie Wang,Xin Wang,Yige Wang,Yixu Wang,Yuanfu Wang,Futing Wang,Ruofan Wang,Wenjie Wang,Yajie Wang,Muhao Wei,Xiaoyu Wen,Fenghua Weng,Yuqi Wu,Yingtong Xiong,Xingcheng Xu,Chao Yang,Yue Yang,Yang Yao,Yulei Ye,Zhenyun Yin,Yi Yu,Bo Zhang,Qiaosheng Zhang,Jinxuan Zhang,Yexin Zhang,Yinqiang Zheng,Hefeng Zhou,Zhanhui Zhou,Pengyu Zhu,Qingzi Zhu,Yubo Zhu,Bowen Zhou*

Main category: cs.AI

TL;DR: 本文提出了SafeWork-R1，一个基于SafeLadder框架的多模态推理模型，旨在实现能力与安全的协同演进，并在安全性能上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法（如RLHF）仅限于学习人类偏好，无法使模型发展出内在的安全推理和自我反思能力，因此需要一种新的框架来推动模型实现更深层次的安全性。

Method: 引入了SafeLadder框架，该框架采用大规模、渐进式、以安全为导向的强化学习后训练，并结合了多原理验证器。此外，还实施了两种推理时干预方法和一种审慎搜索机制，以强制执行步骤级验证。

Result: SafeWork-R1在安全相关基准测试上比其基础模型Qwen2.5-VL-72B平均提升了46.54%，且未损害通用能力，并在安全性方面超越了GPT-4.1和Claude Opus 4等领先专有模型。该框架的通用性已通过在SafeWork-R1-InternVL3-78B、SafeWork-R1-DeepSeek-70B和SafeWork-R1-Qwen2.5VL-7B上的应用得到验证。

Conclusion: 研究表明，安全性和能力可以协同演进，并且所提出的SafeLadder框架在构建鲁棒、可靠和值得信赖的通用人工智能方面具有普适性。

Abstract: We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that
demonstrates the coevolution of capabilities and safety. It is developed by our
proposed SafeLadder framework, which incorporates large-scale, progressive,
safety-oriented reinforcement learning post-training, supported by a suite of
multi-principled verifiers. Unlike previous alignment methods such as RLHF that
simply learn human preferences, SafeLadder enables SafeWork-R1 to develop
intrinsic safety reasoning and self-reflection abilities, giving rise to safety
`aha' moments. Notably, SafeWork-R1 achieves an average improvement of
$46.54\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks
without compromising general capabilities, and delivers state-of-the-art safety
performance compared to leading proprietary models such as GPT-4.1 and Claude
Opus 4. To further bolster its reliability, we implement two distinct
inference-time intervention methods and a deliberative search mechanism,
enforcing step-level verification. Finally, we further develop
SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and
SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and
capability can co-evolve synergistically, highlighting the generalizability of
our framework in building robust, reliable, and trustworthy general-purpose AI.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [146] [Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction](https://arxiv.org/abs/2507.17768)
*Yujia Tong,Jingling Yuan,Chuang Hu*

Main category: cs.LG

TL;DR: QuaRC是一个面向边缘设备的量化感知训练(QAT)框架，通过引入相对熵分数进行核心集选择和采用级联层校正策略来减少量化误差，从而在小规模数据集上显著提升量化模型的性能。


<details>
  <summary>Details</summary>
Motivation: 随着移动和边缘计算的发展，边缘设备对低比特量化模型的需求增加。为了提升性能，需要在边缘设备上使用本地数据重新训练量化模型，但由于隐私问题，敏感数据只能在设备本地处理。传统的量化感知训练(QAT)计算成本高昂，而现有的核心集选择方法在小规模数据集上无法有效消除量化误差，导致性能显著下降。

Method: 本研究提出了QuaRC框架，包含两个主要阶段：
1. **核心集选择阶段：** 引入“相对熵分数”来识别能够最有效捕获模型量化误差的数据子集。
2. **训练阶段：** 采用“级联层校正策略”来对齐量化模型与全精度模型的中间层输出，从而有效减少中间层中的量化误差。

Result: 实验结果证明了该方法的有效性。例如，在使用1%的数据子集将ResNet-18量化到2比特时，QuaRC在ImageNet-1K数据集上的Top-1准确率比现有最先进技术提高了5.72%。

Conclusion: QuaRC框架通过创新的核心集选择和训练策略，成功解决了边缘设备上小规模数据集QAT中存在的量化误差大、性能下降的问题，显著提升了量化模型的准确性。

Abstract: With the development of mobile and edge computing, the demand for low-bit
quantized models on edge devices is increasing to achieve efficient deployment.
To enhance the performance, it is often necessary to retrain the quantized
models using edge data. However, due to privacy concerns, certain sensitive
data can only be processed on edge devices. Therefore, employing
Quantization-Aware Training (QAT) on edge devices has become an effective
solution. Nevertheless, traditional QAT relies on the complete dataset for
training, which incurs a huge computational cost. Coreset selection techniques
can mitigate this issue by training on the most representative subsets.
However, existing methods struggle to eliminate quantization errors in the
model when using small-scale datasets (e.g., only 10% of the data), leading to
significant performance degradation. To address these issues, we propose QuaRC,
a QAT framework with coresets on edge devices, which consists of two main
phases: In the coreset selection phase, QuaRC introduces the ``Relative Entropy
Score" to identify the subsets that most effectively capture the model's
quantization errors. During the training phase, QuaRC employs the Cascaded
Layer Correction strategy to align the intermediate layer outputs of the
quantized model with those of the full-precision model, thereby effectively
reducing the quantization errors in the intermediate layers. Experimental
results demonstrate the effectiveness of our approach. For instance, when
quantizing ResNet-18 to 2-bit using a 1% data subset, QuaRC achieves a 5.72%
improvement in Top-1 accuracy on the ImageNet-1K dataset compared to
state-of-the-art techniques.

</details>


### [147] [Knowledge Abstraction for Knowledge-based Semantic Communication: A Generative Causality Invariant Approach](https://arxiv.org/abs/2507.17784)
*Minh-Duong Nguyen,Quoc-Viet Pham,Nguyen H. Tran,Hoang-Khoi Do,Duy T. Ngo,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 本研究设计了一个基于因果不变学习的低复杂度AI模型，通过生成对抗网络和稀疏更新协议，显著提升了语义通信中信道解码器的数据重建能力，并在数据一致性、分类性能和重建鲁棒性方面超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 旨在提升语义通信中信道解码器的数据重建能力，通过设计一个低复杂度且通用的AI模型来捕获通用知识。同时，解决用户数据多样性及随时间演变造成的知识分歧问题，并确保系统在多源数据环境下的可靠性。

Method: 提出一个基于因果不变学习的生成对抗网络（GAN），用于从数据中提取因果和非因果表示，其中因果表示被用于封装语义知识并促进数据重建。为应对用户数据演变导致的知识分歧，设计了稀疏更新协议以提高知识的不变性并最小化通信开销。

Result: ['因果不变知识确保了多源训练数据下跨不同设备的数据一致性。', '不变知识在分类任务中表现出色，对目标导向的语义通信至关重要。', '所提出的知识型数据重建方法显著提升了解码器的鲁棒性，在峰值信噪比（PSNR）方面超越了现有最先进的数据重建和语义压缩方法。']

Conclusion: 本研究通过引入因果不变学习和稀疏更新机制，成功开发了一种高效且鲁棒的AI模型，显著提升了语义通信中信道解码器的数据重建性能。该模型在处理数据多样性和知识演变方面展现出卓越的适应性，并为未来语义通信系统的发展提供了有益的贡献。

Abstract: In this study, we design a low-complexity and generalized AI model that can
capture common knowledge to improve data reconstruction of the channel decoder
for semantic communication. Specifically, we propose a generative adversarial
network that leverages causality-invariant learning to extract causal and
non-causal representations from the data. Causal representations are invariant
and encompass crucial information to identify the data's label. They can
encapsulate semantic knowledge and facilitate effective data reconstruction at
the receiver. Moreover, the causal mechanism ensures that learned
representations remain consistent across different domains, making the system
reliable even with users collecting data from diverse domains. As
user-collected data evolves over time causing knowledge divergence among users,
we design sparse update protocols to improve the invariant properties of the
knowledge while minimizing communication overheads. Three key observations were
drawn from our empirical evaluations. Firstly, causality-invariant knowledge
ensures consistency across different devices despite the diverse training data.
Secondly, invariant knowledge has promising performance in classification
tasks, which is pivotal for goal-oriented semantic communications. Thirdly, our
knowledge-based data reconstruction highlights the robustness of our decoder,
which surpasses other state-of-the-art data reconstruction and semantic
compression methods in terms of Peak Signal-to-Noise Ratio (PSNR).

</details>


### [148] [Self-similarity Analysis in Deep Neural Networks](https://arxiv.org/abs/2507.17785)
*Jingyi Ding,Chengwen Qi,Hongfei Wang,Jianshe Wu,Licheng Jiao,Yuwei Guo,Jian Gao*

Main category: cs.LG

TL;DR: 深度神经网络存在层级自相似性。本文提出一种基于隐藏层神经元输出特征的复杂网络建模方法，研究并调整特征网络的自相似度，发现训练中嵌入自相似性约束可提高MLP和注意力架构的分类性能高达6个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现深度神经网络存在特征表示或参数分布上的层级自相似性，但缺乏对隐空间几何自相似性如何影响模型权重优化，以及内部神经元动态行为的定量分析和清晰理解。

Method: 提出一种基于隐藏层神经元输出特征的复杂网络建模方法。该方法用于探究不同隐藏层构建的特征网络的自相似性，并分析调整特征网络的自相似度如何提高深度神经网络的分类性能。

Result: 1. 特征网络表现出的自相似程度在不同模型架构（MLP、卷积网络和注意力架构）中有所不同。
2. 在训练过程中对特征网络的自相似性嵌入约束，可以将自相似深度神经网络（MLP架构和注意力架构）的性能提高高达6个百分点。

Conclusion: 本研究表明，特征网络的自相似性是神经网络的一个关键特征，其程度因架构而异。通过在训练过程中主动约束这种自相似性，可以显著提高某些深度神经网络架构（如MLP和注意力架构）的分类性能。

Abstract: Current research has found that some deep neural networks exhibit strong
hierarchical self-similarity in feature representation or parameter
distribution. However, aside from preliminary studies on how the power-law
distribution of weights across different training stages affects model
performance,there has been no quantitative analysis on how the self-similarity
of hidden space geometry influences model weight optimization, nor is there a
clear understanding of the dynamic behavior of internal neurons. Therefore,
this paper proposes a complex network modeling method based on the output
features of hidden-layer neurons to investigate the self-similarity of feature
networks constructed at different hidden layers, and analyzes how adjusting the
degree of self-similarity in feature networks can enhance the classification
performance of deep neural networks. Validated on three types of networks MLP
architectures, convolutional networks, and attention architectures this study
reveals that the degree of self-similarity exhibited by feature networks varies
across different model architectures. Furthermore, embedding constraints on the
self-similarity of feature networks during the training process can improve the
performance of self-similar deep neural networks (MLP architectures and
attention architectures) by up to 6 percentage points.

</details>


### [149] [Reinforcement Learning for Accelerated Aerodynamic Shape Optimisation](https://arxiv.org/abs/2507.17786)
*Florian Sobieczky,Alfredo Lopez,Erika Dudkin,Christopher Lackner,Matthias Hochsteger,Bernhard Scheichl,Helmut Sobieczky*

Main category: cs.LG

TL;DR: 该研究提出一种基于强化学习的自适应优化算法，用于空气动力学外形优化，旨在降维、减少计算量并解析优化参数的作用。


<details>
  <summary>Details</summary>
Motivation: 主要目标是最小化计算工作量，并利用观察到的优化结果来解释发现的极值在实现所需流场中的作用。

Method: 引入一种基于强化学习（RL）的自适应优化算法。具体形式是基于代理模型（surrogate-based）、行动者-评论家（actor-critic）策略评估的马尔可夫链蒙特卡罗（MCMC）方法，允许对部分参数进行“时间冻结”。该方法通过围绕中间计算流体动力学（CFD）仿真（作为真值）进行一系列局部优化的参数改变，来加速全局优化。

Result: 研究表明，如果参数的局部邻域足够大，并且这些邻域上奖励和成本的估计足够准确，该方法可以加速全局优化。在一个简单的流体动力学问题上进行了验证，结果表明该方法能够进行特征重要性评分的解释。

Conclusion: 所提出的基于强化学习的自适应优化方法为空气动力学外形优化提供了一种有效途径，能有效降维，显著减少计算量，并为理解关键参数在达到预期流场中的作用提供了可解释性。

Abstract: We introduce a reinforcement learning (RL) based adaptive optimization
algorithm for aerodynamic shape optimization focused on dimensionality
reduction. The form in which RL is applied here is that of a surrogate-based,
actor-critic policy evaluation MCMC approach allowing for temporal 'freezing'
of some of the parameters to be optimized. The goals are to minimize
computational effort, and to use the observed optimization results for
interpretation of the discovered extrema in terms of their role in achieving
the desired flow-field.
  By a sequence of local optimized parameter changes around intermediate CFD
simulations acting as ground truth, it is possible to speed up the global
optimization if (a) the local neighbourhoods of the parameters in which the
changed parameters must reside are sufficiently large to compete with the
grid-sized steps and its large number of simulations, and (b) the estimates of
the rewards and costs on these neighbourhoods necessary for a good step-wise
parameter adaption are sufficiently accurate. We give an example of a simple
fluid-dynamical problem on which the method allows interpretation in the sense
of a feature importance scoring.

</details>


### [150] [Hyperbolic Deep Learning for Foundation Models: A Survey](https://arxiv.org/abs/2507.17787)
*Neil He,Hiren Madhu,Ngoc Bui,Menglin Yang,Rex Ying*

Main category: cs.LG

TL;DR: 本论文综述了在处理欧几里得几何基础模型局限性时，双曲空间作为一种非欧几何的解决方案，如何通过双曲神经网络提升大模型的表示能力和推理性能，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前基于欧几里得几何的预训练大模型（如LLMs、VLMs）在表示能力、适应性和可扩展性方面存在局限。研究者质疑欧几里得几何是否是所有基础模型的最佳归纳偏置，希望探索替代几何空间以更好地匹配真实世界数据的内在结构并提升推理过程。

Method: 本论文审查并分析了双曲神经网络（一类非欧几何流形）及其在基础模型中的应用。双曲空间因其指数级体积增长特性，能够以更低维度对层次结构（如树、分类法）和幂律分布进行低失真嵌入。

Result: 将双曲几何融入基础模型的最新进展表明，它能有效提升大语言模型的复杂推理能力、视觉-语言模型的零样本泛化能力以及跨模态语义对齐，同时保持参数效率。

Conclusion: 双曲神经网络为增强基础模型提供了一个有前景的方向，通过更好地与数据内在结构对齐来解决现有模型的局限性。本综述全面回顾了该领域的发展，并指出了未来的关键挑战和研究方向。

Abstract: Foundation models pre-trained on massive datasets, including large language
models (LLMs), vision-language models (VLMs), and large multimodal models, have
demonstrated remarkable success in diverse downstream tasks. However, recent
studies have shown fundamental limitations of these models: (1) limited
representational capacity, (2) lower adaptability, and (3) diminishing
scalability. These shortcomings raise a critical question: is Euclidean
geometry truly the optimal inductive bias for all foundation models, or could
incorporating alternative geometric spaces enable models to better align with
the intrinsic structure of real-world data and improve reasoning processes?
Hyperbolic spaces, a class of non-Euclidean manifolds characterized by
exponential volume growth with respect to distance, offer a mathematically
grounded solution. These spaces enable low-distortion embeddings of
hierarchical structures (e.g., trees, taxonomies) and power-law distributions
with substantially fewer dimensions compared to Euclidean counterparts. Recent
advances have leveraged these properties to enhance foundation models,
including improving LLMs' complex reasoning ability, VLMs' zero-shot
generalization, and cross-modal semantic alignment, while maintaining parameter
efficiency. This paper provides a comprehensive review of hyperbolic neural
networks and their recent development for foundation models. We further outline
key challenges and research directions to advance the field.

</details>


### [151] [Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking](https://arxiv.org/abs/2507.17788)
*Ali Vardasbi,Gustavo Penha,Claudia Hauff,Hugues Bouchard*

Main category: cs.LG

TL;DR: 本文针对大型语言模型（LLMs）在排名和评估中存在的“位置偏差”和“低重复一致性”问题，提出一种动态早停方法和置信度自适应改进策略，旨在显著减少缓解这些偏差所需的LLM调用次数，同时保持甚至提高准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在基于给定标准排名或评估时，候选项目的顺序会影响其最终决策（位置偏差），且即使重复相同的调用，也可能得到不同的结果（低重复一致性）。传统的缓解方法是多次提示模型并聚合结果，但这会显著增加计算成本。研究发现，位置偏差的方向和程度因实例而异，需要一种逐实例的缓解策略。

Method: 引入一种“动态早停方法”，该方法能够自适应地确定每个实例所需的重复调用次数。在此基础上，进一步提出一种基于置信度的自适应方法，以优化早停策略。

Result: 在三种不同大小的LLM和重排序、对齐两项任务上的评估表明：动态早停方法平均减少了81%的LLM调用次数，同时保持了准确性。与静态重复策略相比，基于置信度的自适应方法平均减少了87%的LLM调用次数，且相对于原始早停方法仅有轻微的准确性损失。

Conclusion: 所提出的动态早停方法及其置信度自适应改进，能有效缓解LLM的位置偏差和低重复一致性问题，通过智能地减少LLM调用次数，显著降低了计算成本，同时在维持或略微牺牲准确性的前提下，提高了缓解策略的效率，尤其适用于处理实例间的偏差差异。

Abstract: When using LLMs to rank items based on given criteria, or evaluate answers,
the order of candidate items can influence the model's final decision. This
sensitivity to item positioning in a LLM's prompt is known as position bias.
Prior research shows that this bias exists even in large models, though its
severity varies across models and tasks. In addition to position bias, LLMs
also exhibit varying degrees of low repetition consistency, where repeating the
LLM call with the same candidate ordering can lead to different rankings. To
address both inconsistencies, a common approach is to prompt the model multiple
times with different candidate orderings and aggregate the results via majority
voting. However, this repetition strategy, significantly increases
computational costs. Extending prior findings, we observe that both the
direction -- favoring either the earlier or later candidate in the prompt --
and magnitude of position bias across instances vary substantially, even within
a single dataset. This observation highlights the need for a per-instance
mitigation strategy. To this end, we introduce a dynamic early-stopping method
that adaptively determines the number of repetitions required for each
instance. Evaluating our approach across three LLMs of varying sizes and on two
tasks, namely re-ranking and alignment, we demonstrate that transitioning to a
dynamic repetition strategy reduces the number of LLM calls by an average of
81%, while preserving the accuracy. Furthermore, we propose a confidence-based
adaptation to our early-stopping method, reducing LLM calls by an average of
87% compared to static repetition, with only a slight accuracy trade-off
relative to our original early-stopping method.

</details>


### [152] [Helix 1.0: An Open-Source Framework for Reproducible and Interpretable Machine Learning on Tabular Scientific Data](https://arxiv.org/abs/2507.17791)
*Eduardo Aguilar-Bejarano,Daniel Lea,Karthikeyan Sivakumar,Jimiama M. Mase,Reza Omidvar,Ruizhe Li,Troy Kettle,James Mitchell-White,Morgan R Alexander,David A Winkler,Grazziela Figueredo*

Main category: cs.LG

TL;DR: Helix是一个开源、基于Python的软件框架，旨在为表格数据提供可复现和可解释的机器学习工作流程，解决透明度和可访问性问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习工作流程缺乏透明的数据分析溯源，导致整个分析过程（包括数据转换和方法选择）难以文档化、访问、复现和理解。同时，非数据科学专业研究人员难以获得有意义的洞察。

Method: 开发了Helix框架，这是一个包含标准化数据预处理、可视化、机器学习模型训练、评估、解释、结果检查和模型预测模块的Python工具。该框架提供用户友好的界面，支持实验设计和结果检查，并引入了一种使用语言术语解释机器学习决策的新颖方法。

Result: Helix成功地促进了表格数据的可复现和可解释机器学习工作流程，确保了分析过程的透明、可访问、可复现和易于理解。它赋能没有数据科学背景的研究人员也能获得有意义的洞察。

Conclusion: Helix提供了一个集成环境，有效解决了机器学习工作流程透明度和可复现性的挑战，特别是对非专业用户友好，并通过开源促进社区驱动的开发和FAIR原则的遵守。

Abstract: Helix is an open-source, extensible, Python-based software framework to
facilitate reproducible and interpretable machine learning workflows for
tabular data. It addresses the growing need for transparent experimental data
analytics provenance, ensuring that the entire analytical process -- including
decisions around data transformation and methodological choices -- is
documented, accessible, reproducible, and comprehensible to relevant
stakeholders. The platform comprises modules for standardised data
preprocessing, visualisation, machine learning model training, evaluation,
interpretation, results inspection, and model prediction for unseen data. To
further empower researchers without formal training in data science to derive
meaningful and actionable insights, Helix features a user-friendly interface
that enables the design of computational experiments, inspection of outcomes,
including a novel interpretation approach to machine learning decisions using
linguistic terms all within an integrated environment. Released under the MIT
licence, Helix is accessible via GitHub and PyPI, supporting community-driven
development and promoting adherence to the FAIR principles.

</details>


### [153] [Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple Domains](https://arxiv.org/abs/2507.17792)
*Jingyi Yu,Tim Pychynski,Marco F. Huber*

Main category: cs.LG

TL;DR: CICME是一种利用因果迁移学习，从多领域异构数据中推断共同和个体因果机制的新型三步法，旨在深入理解复杂传感器系统，并在特定场景下表现优越。


<details>
  <summary>Details</summary>
Motivation: 旨在通过因果关系视角，深入洞察复杂的传感器系统；需要从跨多领域收集的异构数据中推断因果机制。

Method: 提出了一种名为“共同和个体因果机制估计 (CICME)”的新型三步法。该方法利用因果迁移学习 (CTL) 原理，检测领域不变的共同因果机制，并利用这些共同机制指导每个领域中剩余个体因果机制的估计。CICME基于现有连续优化因果发现方法构建，并在受制造过程启发的线性高斯模型场景下进行性能评估。

Result: CICME结合了对汇总数据和对个体领域数据重复应用因果发现的优点。在某些特定场景下，CICME甚至优于两种基线方法。

Conclusion: CICME能够有效且可靠地从异构数据中推断因果机制，增强对复杂传感器系统的理解，并在特定条件下展现出卓越的性能。

Abstract: To gain deeper insights into a complex sensor system through the lens of
causality, we present common and individual causal mechanism estimation
(CICME), a novel three-step approach to inferring causal mechanisms from
heterogeneous data collected across multiple domains. By leveraging the
principle of Causal Transfer Learning (CTL), CICME is able to reliably detect
domain-invariant causal mechanisms when provided with sufficient samples. The
identified common causal mechanisms are further used to guide the estimation of
the remaining causal mechanisms in each domain individually. The performance of
CICME is evaluated on linear Gaussian models under scenarios inspired from a
manufacturing process. Building upon existing continuous optimization-based
causal discovery methods, we show that CICME leverages the benefits of applying
causal discovery on the pooled data and repeatedly on data from individual
domains, and it even outperforms both baseline methods under certain scenarios.

</details>


### [154] [LSDM: LLM-Enhanced Spatio-temporal Diffusion Model for Service-Level Mobile Traffic Prediction](https://arxiv.org/abs/2507.17795)
*Shiyuan Zhang,Tong Li,Zhu Xiao,Hongyang Du,Kaibin Huang*

Main category: cs.LG

TL;DR: 本文提出LSDM模型，结合扩散模型和LLM提升个人移动流量预测精度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有移动流量预测方法因个人模式不确定性、环境上下文缺失及服务间复杂依赖性，导致预测不准确且适应性差，影响网络效率和服务质量。

Method: 提出LLM增强的时空扩散模型(LSDM)。LSDM结合扩散模型的生成能力和Transformer的自适应学习能力，并利用LLM捕获多模态环境信息，以建模服务级别的流量模式和动态。

Result: 在真实数据集上，LSDM在流量预测方面表现出色，具有卓越的泛化性和适应性。整合LLM的上下文信息后，模型性能在决定系数上提升至少2.83%；与CSDI等同类型模型相比，均方根误差降低至少8.29%。

Conclusion: LSDM通过融合扩散模型、Transformer和LLM的优势，有效解决了服务级移动流量预测的挑战，显著提高了预测精度、泛化性和环境适应性。

Abstract: Service-level mobile traffic prediction for individual users is essential for
network efficiency and quality of service enhancement. However, current
prediction methods are limited in their adaptability across different urban
environments and produce inaccurate results due to the high uncertainty in
personal traffic patterns, the lack of detailed environmental context, and the
complex dependencies among different network services. These challenges demand
advanced modeling techniques that can capture dynamic traffic distributions and
rich environmental features. Inspired by the recent success of diffusion models
in distribution modeling and Large Language Models (LLMs) in contextual
understanding, we propose an LLM-Enhanced Spatio-temporal Diffusion Model
(LSDM). LSDM integrates the generative power of diffusion models with the
adaptive learning capabilities of transformers, augmented by the ability to
capture multimodal environmental information for modeling service-level
patterns and dynamics. Extensive evaluations on real-world service-level
datasets demonstrate that the model excels in traffic usage predictions,
showing outstanding generalization and adaptability. After incorporating
contextual information via LLM, the performance improves by at least 2.83% in
terms of the coefficient of determination. Compared to models of a similar
type, such as CSDI, the root mean squared error can be reduced by at least
8.29%. The code and dataset will be available at:
https://github.com/SoftYuaneR/LSDM.

</details>


### [155] [CoCAI: Copula-based Conformal Anomaly Identification for Multivariate Time-Series](https://arxiv.org/abs/2507.17796)
*Nicholas A. Pearson,Francesca Zanello,Davide Russo,Luca Bortolussi,Francesca Cairoli*

Main category: cs.LG

TL;DR: 该论文提出了一种结合生成式AI和Copula模型的CoCAI框架，用于多元时间序列的准确预测和鲁棒异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决多元时间序列分析中准确预测和鲁棒异常检测两大挑战。

Method: CoCAI框架利用扩散模型捕捉数据复杂依赖关系进行高质量预测；通过共形预测技术校准预测输出，生成统计有效的预测区间；结合降维技术和Copula模型进行异常检测，提供统计学上可靠的异常分数。该方法受益于离线校准阶段，以减少部署开销。

Result: 在真实的水分配和污水系统运行数据上的实证测试表明，CoCAI能有效准确地预测目标数据序列并识别异常段。

Conclusion: CoCAI是一种有效且理论基础扎实的框架，为多元时间序列的预测和异常识别提供了可操作的结果。

Abstract: We propose a novel framework that harnesses the power of generative
artificial intelligence and copula-based modeling to address two critical
challenges in multivariate time-series analysis: delivering accurate
predictions and enabling robust anomaly detection. Our method, Copula-based
Conformal Anomaly Identification for Multivariate Time-Series (CoCAI),
leverages a diffusion-based model to capture complex dependencies within the
data, enabling high quality forecasting. The model's outputs are further
calibrated using a conformal prediction technique, yielding predictive regions
which are statistically valid, i.e., cover the true target values with a
desired confidence level. Starting from these calibrated forecasts, robust
outlier detection is performed by combining dimensionality reduction techniques
with copula-based modeling, providing a statistically grounded anomaly score.
CoCAI benefits from an offline calibration phase that allows for minimal
overhead during deployment and delivers actionable results rooted in
established theoretical foundations. Empirical tests conducted on real
operational data derived from water distribution and sewerage systems confirm
CoCAI's effectiveness in accurately forecasting target sequences of data and in
identifying anomalous segments within them.

</details>


### [156] [GenSelect: A Generative Approach to Best-of-N](https://arxiv.org/abs/2507.17797)
*Shubham Toshniwal,Ivan Sorokin,Aleksander Ficek,Ivan Moshkov,Igor Gitman*

Main category: cs.LG

TL;DR: 本文提出GenSelect，一种利用大语言模型（LLM）长推理能力从N个候选中选择最佳解决方案的方法，旨在克服现有奖励模型在利用LLM比较能力和扩展性方面的局限，并在数学推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有生成式奖励模型采用逐点评分或成对比较，前者未能充分利用LLM的比较能力，后者在大量采样预算下效率低下。因此，需要一种既能发挥LLM比较优势又能高效扩展的方法。

Method: 引入GenSelect方法，该方法让LLM通过长推理过程，从N个候选解决方案中选出最优解。这种方法旨在利用LLM的比较优势，并有效扩展以适应并行采样预算。

Result: 研究表明，对于数学推理任务，QwQ和DeepSeek-R1-0528等推理模型在GenSelect中表现卓越，通过简单的提示就能超越现有的评分方法。

Conclusion: GenSelect是一种有效且高效的策略，能够充分利用LLM的比较能力，并解决现有生成式奖励模型在测试时扩展性方面的不足，特别适用于推理任务。

Abstract: Generative reward models with parallel sampling have enabled effective
test-time scaling for reasoning tasks. Current approaches employ pointwise
scoring of individual solutions or pairwise comparisons. However, pointwise
methods underutilize LLMs' comparative abilities, while pairwise methods scale
inefficiently with larger sampling budgets. We introduce GenSelect, where the
LLM uses long reasoning to select the best solution among N candidates. This
leverages LLMs' comparative strengths while scaling efficiently across parallel
sampling budgets. For math reasoning, we demonstrate that reasoning models,
such as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing
scoring approaches with simple prompting.

</details>


### [157] [Wasserstein GAN-Based Precipitation Downscaling with Optimal Transport for Enhancing Perceptual Realism](https://arxiv.org/abs/2507.17798)
*Kenta Shiraishi,Yuka Muto,Atsushi Okazaki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: 本研究提出使用WGAN进行降水降尺度，显著提升了视觉真实感和精细结构，并为降水数据质量控制提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 高分辨率降水预测对减少局地强降雨损害至关重要，但使用传统数值天气预报模型进行高分辨率预测仍面临挑战。

Method: 提出并使用Wasserstein生成对抗网络（WGAN）进行降水降尺度，以优化传输成本。

Result: WGAN生成的降水场在视觉上更真实，具有精细尺度结构，优于传统神经网络（尽管传统评估指标略低）。WGAN的学习评论器与人类感知真实感高度相关，且评论器分数的差异可用于识别不真实的模型输出及参考数据中的潜在伪影。

Conclusion: WGAN框架不仅提高了降水降尺度的感知真实性，也为降水数据集的评估和质量控制提供了新的视角。

Abstract: High-resolution (HR) precipitation prediction is essential for reducing
damage from stationary and localized heavy rainfall; however, HR precipitation
forecasts using process-driven numerical weather prediction models remains
challenging. This study proposes using Wasserstein Generative Adversarial
Network (WGAN) to perform precipitation downscaling with an optimal transport
cost. In contrast to a conventional neural network trained with mean squared
error, the WGAN generated visually realistic precipitation fields with
fine-scale structures even though the WGAN exhibited slightly lower performance
on conventional evaluation metrics. The learned critic of WGAN correlated well
with human perceptual realism. Case-based analysis revealed that large
discrepancies in critic scores can help identify both unrealistic WGAN outputs
and potential artifacts in the reference data. These findings suggest that the
WGAN framework not only improves perceptual realism in precipitation
downscaling but also offers a new perspective for evaluating and
quality-controlling precipitation datasets.

</details>


### [158] [Explainable Graph Neural Networks via Structural Externalities](https://arxiv.org/abs/2507.17848)
*Lijun Wu,Dong Hao,Zhiyi Fan*

Main category: cs.LG

TL;DR: GNN可解释性不足且现有方法难以捕捉复杂节点交互。本文提出GraphEXT框架，基于合作博弈论和外部性，通过衡量节点对GNN预测的边际贡献，有效提升GNN模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在图相关任务中表现出色，但其“黑箱”特性导致可解释性差。现有方法未能有效捕捉网络中节点间复杂的交互模式。

Method: 本文提出GraphEXT可解释性框架，该框架利用合作博弈论和社会外部性概念。GraphEXT将图节点划分为联盟，并将原始图分解为独立子图。通过将图结构作为外部性，并引入外部性下的Shapley值，该方法量化了节点在联盟间转移时对GNN预测的边际贡献，以此衡量节点重要性。与传统Shapley方法不同，GraphEXT更侧重节点间交互及结构变化对GNN预测的影响。

Result: 在合成和真实世界数据集上的实验表明，GraphEXT在多种GNN架构上均优于现有基线方法，在忠实度方面表现更佳。

Conclusion: GraphEXT框架能够显著增强GNN模型的可解释性。

Abstract: Graph Neural Networks (GNNs) have achieved outstanding performance across a
wide range of graph-related tasks. However, their "black-box" nature poses
significant challenges to their explainability, and existing methods often fail
to effectively capture the intricate interaction patterns among nodes within
the network. In this work, we propose a novel explainability framework,
GraphEXT, which leverages cooperative game theory and the concept of social
externalities. GraphEXT partitions graph nodes into coalitions, decomposing the
original graph into independent subgraphs. By integrating graph structure as an
externality and incorporating the Shapley value under externalities, GraphEXT
quantifies node importance through their marginal contributions to GNN
predictions as the nodes transition between coalitions. Unlike traditional
Shapley value-based methods that primarily focus on node attributes, our
GraphEXT places greater emphasis on the interactions among nodes and the impact
of structural changes on GNN predictions. Experimental studies on both
synthetic and real-world datasets show that GraphEXT outperforms existing
baseline methods in terms of fidelity across diverse GNN architectures ,
significantly enhancing the explainability of GNN models.

</details>


### [159] [Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic](https://arxiv.org/abs/2507.17876)
*Rıza Özçelik,Sarah de Ruiter,Francesca Grisoni*

Main category: cs.LG

TL;DR: 提出分子任务算术，通过负样本学习属性方向来克服正样本稀缺问题，从而生成多样化且成功的分子，有望成为分子设计中的迁移学习策略。


<details>
  <summary>Details</summary>
Motivation: 生成式分子设计中，具有期望性质的分子（即“正”分子）的稀缺性是一个固有的瓶颈。

Method: 提出分子任务算术：在多样且丰富的负样本上训练模型以学习“属性方向”（无需正样本），然后沿相反属性方向移动模型以生成正分子。

Result: 在20个零样本设计实验中，该方法比基于正样本训练的模型生成了更多样化和成功的设计。在双目标和少样本设计任务中，它能持续增加设计的多样性同时保持期望属性。

Conclusion: 分子任务算术因其简单性、数据效率和高性能，有望成为从头分子设计的实际迁移学习策略。

Abstract: The scarcity of molecules with desirable properties (i.e., 'positive'
molecules) is an inherent bottleneck for generative molecule design. To
sidestep such obstacle, here we propose molecular task arithmetic: training a
model on diverse and abundant negative examples to learn 'property directions'
$--$ without accessing any positively labeled data $--$ and moving models in
the opposite property directions to generate positive molecules. When analyzed
on 20 zero-shot design experiments, molecular task arithmetic generated more
diverse and successful designs than models trained on positive molecules.
Moreover, we employed molecular task arithmetic in dual-objective and few-shot
design tasks. We find that molecular task arithmetic can consistently increase
the diversity of designs while maintaining desirable design properties. With
its simplicity, data efficiency, and performance, molecular task arithmetic
bears the potential to become the $\textit{de-facto}$ transfer learning
strategy for de novo molecule design.

</details>


### [160] [Fourier Neural Operators for Non-Markovian Processes:Approximation Theorems and Experiments](https://arxiv.org/abs/2507.17887)
*Wonjae Lee,Taeyoung Kim,Hyungbin Park*

Main category: cs.LG

TL;DR: 本文提出了一种名为镜像填充傅里叶神经算子（MFNO）的新型神经网络，用于学习随机系统的动力学，通过引入镜像填充使其能处理非周期输入，并在理论和实证上证明了其强大的近似能力、泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在学习随机系统动力学方面可能存在局限性，特别是标准傅里叶神经算子（FNO）难以处理非周期输入，且经典数值方案生成样本路径速度较慢。

Method: 引入了基于算子的神经网络MFNO，它通过在标准FNO中加入镜像填充来处理非周期输入。理论分析构建于Wong--Zakai型定理和各种近似技术之上。

Result: 理论上，MFNO能够以任意精度近似路径依赖随机微分方程和分数布朗运动的Lipschitz变换的解。经验上，MFNO展示出强大的分辨率泛化能力（优于LSTMs、TCNs、DeepONet等），性能与这些基线模型相当或更优，且比经典数值方案生成样本路径速度显著加快。

Conclusion: MFNO为学习随机系统动力学提供了一种强大且高效的新范式，它不仅克服了现有模型的局限性，还在理论证明和实证表现上均展现出卓越的性能和广阔的应用前景。

Abstract: This paper introduces an operator-based neural network, the mirror-padded
Fourier neural operator (MFNO), designed to learn the dynamics of stochastic
systems. MFNO extends the standard Fourier neural operator (FNO) by
incorporating mirror padding, enabling it to handle non-periodic inputs. We
rigorously prove that MFNOs can approximate solutions of path-dependent
stochastic differential equations and Lipschitz transformations of fractional
Brownian motions to an arbitrary degree of accuracy. Our theoretical analysis
builds on Wong--Zakai type theorems and various approximation techniques.
Empirically, the MFNO exhibits strong resolution generalization--a property
rarely seen in standard architectures such as LSTMs, TCNs, and DeepONet.
Furthermore, our model achieves performance that is comparable or superior to
these baselines while offering significantly faster sample path generation than
classical numerical schemes.

</details>


### [161] [Lower Bounds for Public-Private Learning under Distribution Shift](https://arxiv.org/abs/2507.17895)
*Amrith Setlur,Pratiksha Thaker,Jonathan Ullman*

Main category: cs.LG

TL;DR: 本文研究了在差分隐私机器学习中，公共数据和私有数据结合的效益如何受到两者间分布偏移的影响，并发现大偏移时公共数据无益。


<details>
  <summary>Details</summary>
Motivation: 实践中有效的差分隐私算法常依赖公共数据，但已知在两数据源分布相同时，结合它们并没有额外价值。本研究旨在探索当两数据源存在显著分布偏移时，是否存在这种互补价值。

Method: 将已知的公共-私有学习的下限扩展到两个数据源存在显著分布偏移的设置。具体应用于高斯均值估计（分布均值不同）和高斯线性回归（参数偏移）模型。

Result: 研究发现，当分布偏移较小（相对于所需精度）时，公共或私有数据之一必须足够丰富才能估计私有参数。相反，当偏移较大时，公共数据不再提供任何益处。

Conclusion: 在差分隐私设置下，公共数据与私有数据的结合效益高度依赖于两者间的分布偏移程度。小的偏移仍需数据量保障，而大的偏移则使公共数据失去作用。

Abstract: The most effective differentially private machine learning algorithms in
practice rely on an additional source of purportedly public data. This paradigm
is most interesting when the two sources combine to be more than the sum of
their parts. However, there are settings such as mean estimation where we have
strong lower bounds, showing that when the two data sources have the same
distribution, there is no complementary value to combining the two data
sources. In this work we extend the known lower bounds for public-private
learning to setting where the two data sources exhibit significant distribution
shift. Our results apply to both Gaussian mean estimation where the two
distributions have different means, and to Gaussian linear regression where the
two distributions exhibit parameter shift. We find that when the shift is small
(relative to the desired accuracy), either public or private data must be
sufficiently abundant to estimate the private parameter. Conversely, when the
shift is large, public data provides no benefit.

</details>


### [162] [Federated Learning for Large-Scale Cloud Robotic Manipulation: Opportunities and Challenges](https://arxiv.org/abs/2507.17903)
*Obaidullah Zaland,Chanh Nguyen,Florian T. Pokorny,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本文探讨了联邦学习（FL）如何应用于云机器人操作，以克服传统机器人限制和计算资源挑战，并分析了其带来的机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作受限于其个体能力和有限的低延迟计算资源。虽然云机器人可以缓解计算需求，但要在分布式计算背景下实现大规模、高效、可靠的机器人操作，仍需更优方案。联邦学习因其分布式、保护数据隐私的特性，有望解决这些问题。

Method: 本文阐述了联邦学习的基本概念及其与云机器人操作的关联。此外，它还展望了通过联邦学习实现大规模高效可靠云机器人操作所面临的机遇和挑战，并探讨了在集中式或去中心化设置中设计和验证FL模型的方向。

Result: 联邦学习在云机器人操作场景中既具有多重优势，也带来了显著的挑战和机遇。研究结果表明，该领域需要进一步探索和验证在集中式或去中心化环境下FL模型的潜力。

Conclusion: 联邦学习是解决大规模云机器人操作中计算资源限制和数据隐私问题的关键范式。实现其潜力需深入研究并克服现有挑战，抓住其带来的机遇，以推动高效可靠的云机器人技术发展。

Abstract: Federated Learning (FL) is an emerging distributed machine learning paradigm,
where the collaborative training of a model involves dynamic participation of
devices to achieve broad objectives. In contrast, classical machine learning
(ML) typically requires data to be located on-premises for training, whereas FL
leverages numerous user devices to train a shared global model without the need
to share private data. Current robotic manipulation tasks are constrained by
the individual capabilities and speed of robots due to limited low-latency
computing resources. Consequently, the concept of cloud robotics has emerged,
allowing robotic applications to harness the flexibility and reliability of
computing resources, effectively alleviating their computational demands across
the cloud-edge continuum. Undoubtedly, within this distributed computing
context, as exemplified in cloud robotic manipulation scenarios, FL offers
manifold advantages while also presenting several challenges and opportunities.
In this paper, we present fundamental concepts of FL and their connection to
cloud robotic manipulation. Additionally, we envision the opportunities and
challenges associated with realizing efficient and reliable cloud robotic
manipulation at scale through FL, where researchers adopt to design and verify
FL models in either centralized or decentralized settings.

</details>


### [163] [Deep learning-aided inverse design of porous metamaterials](https://arxiv.org/abs/2507.17907)
*Phu Thien Nguyen,Yousef Heider,Dennis M. Kochmann,Fadi Aldakheel*

Main category: cs.LG

TL;DR: 本研究利用深度学习的生成框架（pVAE）实现了多孔超材料的逆向设计，能够根据定制的水力学性能（如孔隙率和渗透率）生成新的结构。


<details>
  <summary>Details</summary>
Motivation: 探索并实现多孔超材料的逆向设计，以高效地生成具有特定水力学性能的材料，解决传统设计方法计算成本高昂的问题。

Method: 开发了一种名为pVAE（property-variational autoencoder）的深度学习生成框架，该框架是结合了回归器的变分自编码器（VAE）。首先，利用格子玻尔兹曼方法（LBM）生成有限多孔微结构的渗透率张量数据，并训练卷积神经网络（CNN）以自下而上的方式预测有效水力学性能，以降低计算成本。pVAE框架在合成多孔微结构数据集和真实开孔泡沫的CT扫描图像数据集上进行训练。VAE的编码-解码器架构用于捕获微结构特征并映射到可解释的潜在空间。

Result: 通过CNN预测水力学性能显著降低了计算成本。pVAE框架成功捕捉了关键微结构特征，并将其映射到紧凑且可解释的潜在空间，便于结构-属性探索。研究详细分析和解释了潜在空间，证明了其在结构-属性映射、插值和逆向设计中的有效性。该方法成功促进了具有所需性能的新型超材料的生成。

Conclusion: 本研究提出的基于深度学习的pVAE框架，结合CNN与LBM，有效实现了多孔超材料的逆向设计，显著降低了计算成本，并能高效生成具有定制水力学性能的新型材料，为未来的材料设计提供了强大的工具。

Abstract: The ultimate aim of the study is to explore the inverse design of porous
metamaterials using a deep learning-based generative framework. Specifically,
we develop a property-variational autoencoder (pVAE), a variational autoencoder
(VAE) augmented with a regressor, to generate structured metamaterials with
tailored hydraulic properties, such as porosity and permeability. While this
work uses the lattice Boltzmann method (LBM) to generate intrinsic permeability
tensor data for limited porous microstructures, a convolutional neural network
(CNN) is trained using a bottom-up approach to predict effective hydraulic
properties. This significantly reduces the computational cost compared to
direct LBM simulations. The pVAE framework is trained on two datasets: a
synthetic dataset of artificial porous microstructures and CT-scan images of
volume elements from real open-cell foams. The encoder-decoder architecture of
the VAE captures key microstructural features, mapping them into a compact and
interpretable latent space for efficient structure-property exploration. The
study provides a detailed analysis and interpretation of the latent space,
demonstrating its role in structure-property mapping, interpolation, and
inverse design. This approach facilitates the generation of new metamaterials
with desired properties. The datasets and codes used in this study will be made
open-access to support further research.

</details>


### [164] [SETOL: A Semi-Empirical Theory of (Deep) Learning](https://arxiv.org/abs/2507.17912)
*Charles H Martin,Christopher Hinrichs*

Main category: cs.LG

TL;DR: 本文提出了半经验学习理论 (SETOL)，利用统计力学、随机矩阵理论和量子化学等方法，解释了最先进神经网络 (SOTA NN) 的卓越性能，并对重尾自正则化 (HTSR) 理论中的关键层质量指标（alpha 和 alpha-hat）进行了形式化解释。研究引入了一个新的指标 ERG，并在多层感知机 (MLP) 和 SOTA NN 上验证了理论假设和预测，结果显示 HTSR alpha 和 SETOL ERG 指标表现出显著的一致性。


<details>
  <summary>Details</summary>
Motivation: 旨在解释最先进神经网络 (SOTA NN) 的卓越性能；并为先前研究中无需训练/测试数据即可预测 SOTA NN 测试准确率趋势的重尾自正则化 (HTSR) 理论中的核心层质量指标（alpha 和 alpha-hat）提供形式化解释。

Method: ['构建了半经验学习理论 (SETOL)。', '运用统计力学、随机矩阵理论和量子化学的高级方法进行理论推导。', '在一个简单的3层多层感知机 (MLP) 上测试了 SETOL 的假设和预测。', '对于 SOTA NN 模型，通过计算层权重矩阵的经验谱密度 (ESD) 并将其代入 SETOL 公式来估算各层质量。', '比较并检验了 HTSR alpha 和 SETOL ERG 层质量指标的性能。']

Result: ['理论推导提出了理想学习的新数学前提，并引入了新指标 ERG（相当于Wilson精确重整化群的单步应用）。', '在3层 MLP 上，SETOL 的关键理论假设得到了极好的验证。', '展示了如何通过简单的经验谱密度计算来估计已训练 SOTA NN 的层质量。', 'HTSR alpha 和 SETOL ERG 层质量指标在 MLP 和 SOTA NN 上均显示出显著的一致性。']

Conclusion: 本文提出的半经验学习理论 (SETOL) 成功解释了 SOTA 神经网络的性能，并为重尾自正则化 (HTSR) 理论的核心层质量指标提供了坚实的理论基础。新引入的 ERG 指标与现有指标高度一致，证明了 SETOL 在理解和预测神经网络行为方面的有效性和潜力。

Abstract: We present a SemiEmpirical Theory of Learning (SETOL) that explains the
remarkable performance of State-Of-The-Art (SOTA) Neural Networks (NNs). We
provide a formal explanation of the origin of the fundamental quantities in the
phenomenological theory of Heavy-Tailed Self-Regularization (HTSR): the
heavy-tailed power-law layer quality metrics, alpha and alpha-hat. In prior
work, these metrics have been shown to predict trends in the test accuracies of
pretrained SOTA NN models, importantly, without needing access to either
testing or training data. Our SETOL uses techniques from statistical mechanics
as well as advanced methods from random matrix theory and quantum chemistry.
The derivation suggests new mathematical preconditions for ideal learning,
including a new metric, ERG, which is equivalent to applying a single step of
the Wilson Exact Renormalization Group. We test the assumptions and predictions
of SETOL on a simple 3-layer multilayer perceptron (MLP), demonstrating
excellent agreement with the key theoretical assumptions. For SOTA NN models,
we show how to estimate the individual layer qualities of a trained NN by
simply computing the empirical spectral density (ESD) of the layer weight
matrices and plugging this ESD into our SETOL formulas. Notably, we examine the
performance of the HTSR alpha and the SETOL ERG layer quality metrics, and find
that they align remarkably well, both on our MLP and on SOTA NNs.

</details>


### [165] [From Seed to Harvest: Augmenting Human Creativity with AI for Red-teaming Text-to-Image Models](https://arxiv.org/abs/2507.17922)
*Jessica Quaye,Charvi Rastogi,Alicia Parrish,Oana Inel,Minsuk Kahng,Lora Aroyo,Vijay Janapa Reddi*

Main category: cs.LG

TL;DR: 为应对T2I模型对抗攻击评估中现有提示生成方法的不足（人工规模小，合成缺乏真实性），本文提出Seed2Harvest，一种结合人机优势的混合红队方法，用于扩展文化多样的人工对抗提示。结果表明，新生成的数据集多样性显著提高，同时保持了原始提示的攻击效果，证明了人机协作在模型安全评估中的重要性。


<details>
  <summary>Details</summary>
Motivation: 文本到图像（T2I）模型的鲁棒性评估至关重要，需要持续获取新颖且具有挑战性的对抗提示。然而，现有的人工生成提示规模小且文化不平衡，而合成生成提示虽具规模但缺乏真实性和创造性。研究动机在于结合人机优势，克服这些局限，以实现T2I模型更全面的安全评估。

Method: 本文提出Seed2Harvest，一种混合红队方法。该方法旨在指导性地扩展文化多样的人工对抗提示种子，通过结合人类的创造力与机器的计算能力来生成新的对抗提示。

Result: 通过Seed2Harvest方法，生成的提示保留了人工提示的特征和攻击模式，并维持了可比的平均攻击成功率（NudeNet 0.31，SD NSFW 0.36，Q16 0.12）。扩展后的数据集多样性显著提升，拥有535个独特的地理位置和7.48的香农熵，远超原始数据集的58个位置和5.28香农熵。

Conclusion: 本研究强调了人机协作在T2I模型安全评估中的重要性。通过有效利用人类创造力和机器计算能力，可以实现全面且可扩展的红队测试，从而促进T2I模型的持续安全评估。

Abstract: Text-to-image (T2I) models have become prevalent across numerous
applications, making their robust evaluation against adversarial attacks a
critical priority. Continuous access to new and challenging adversarial prompts
across diverse domains is essential for stress-testing these models for
resilience against novel attacks from multiple vectors. Current techniques for
generating such prompts are either entirely authored by humans or synthetically
generated. On the one hand, datasets of human-crafted adversarial prompts are
often too small in size and imbalanced in their cultural and contextual
representation. On the other hand, datasets of synthetically-generated prompts
achieve scale, but typically lack the realistic nuances and creative
adversarial strategies found in human-crafted prompts. To combine the strengths
of both human and machine approaches, we propose Seed2Harvest, a hybrid
red-teaming method for guided expansion of culturally diverse, human-crafted
adversarial prompt seeds. The resulting prompts preserve the characteristics
and attack patterns of human prompts while maintaining comparable average
attack success rates (0.31 NudeNet, 0.36 SD NSFW, 0.12 Q16). Our expanded
dataset achieves substantially higher diversity with 535 unique geographic
locations and a Shannon entropy of 7.48, compared to 58 locations and 5.28
entropy in the original dataset. Our work demonstrates the importance of
human-machine collaboration in leveraging human creativity and machine
computational capacity to achieve comprehensive, scalable red-teaming for
continuous T2I model safety evaluation.

</details>


### [166] [UrbanPulse: A Cross-City Deep Learning Framework for Ultra-Fine-Grained Population Transfer Prediction](https://arxiv.org/abs/2507.17924)
*Hongrong Yang,Markus Schlaepfer*

Main category: cs.LG

TL;DR: UrbanPulse是一种可扩展的深度学习框架，通过将每个兴趣点（POI）视为独立节点，实现超细粒度、城市范围内的OD（起点-终点）流量预测，并利用三阶段迁移学习策略提高跨城市泛化能力，达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 准确的人口流动预测对城市规划、交通管理和公共健康至关重要。然而，现有方法存在局限性：传统模型依赖静态空间假设；深度学习模型在跨城市泛化方面表现不佳；大型语言模型（LLM）计算成本高且难以捕捉空间结构；许多方法通过聚类POI或限制覆盖范围牺牲了分辨率和城市级分析能力。

Method: 研究引入了UrbanPulse，一个可扩展的深度学习框架，通过将每个POI视为独立节点，进行超细粒度、城市范围的OD流量预测。该框架结合了时间图卷积编码器和基于Transformer的解码器，以建模多尺度时空依赖。为确保在不同城市环境中的泛化能力，UrbanPulse采用了三阶段迁移学习策略：大规模城市图上的预训练、冷启动适应和强化学习微调。

Result: UrbanPulse在来自加州三个大都市区的超过1.03亿条清洗后的GPS记录上进行了评估，实现了最先进的准确性和可扩展性。

Conclusion: 通过高效的迁移学习，UrbanPulse在使高分辨率、人工智能驱动的城市预测在不同城市中实际部署方面迈出了关键一步。

Abstract: Accurate population flow prediction is essential for urban planning,
transportation management, and public health. Yet existing methods face key
limitations: traditional models rely on static spatial assumptions, deep
learning models struggle with cross-city generalization, and Large Language
Models (LLMs) incur high computational costs while failing to capture spatial
structure. Moreover, many approaches sacrifice resolution by clustering Points
of Interest (POIs) or restricting coverage to subregions, limiting their
utility for city-wide analytics. We introduce UrbanPulse, a scalable deep
learning framework that delivers ultra-fine-grained, city-wide OD flow
predictions by treating each POI as an individual node. It combines a temporal
graph convolutional encoder with a transformer-based decoder to model
multi-scale spatiotemporal dependencies. To ensure robust generalization across
urban contexts, UrbanPulse employs a three-stage transfer learning strategy:
pretraining on large-scale urban graphs, cold-start adaptation, and
reinforcement learning fine-tuning.Evaluated on over 103 million cleaned GPS
records from three metropolitan areas in California, UrbanPulse achieves
state-of-the-art accuracy and scalability. Through efficient transfer learning,
UrbanPulse takes a key step toward making high-resolution, AI-powered urban
forecasting deployable in practice across diverse cities.

</details>


### [167] [Multimodal Fine-grained Reasoning for Post Quality Evaluation](https://arxiv.org/abs/2507.17934)
*Xiaoxu Guo,Siyan Liang,Yachao Cui,Juxiang Zhou,Lei Wang,Han Cao*

Main category: cs.LG

TL;DR: 本文提出了多模态细粒度主题-帖子关系推理（MFTRR）框架，将帖子质量评估重构为排序任务，通过模拟人类认知过程和结合多模态数据，有效解决了现有方法的局限性，并显著提升了评估性能。


<details>
  <summary>Details</summary>
Motivation: 现有帖子质量评估研究面临三大局限：1) 将任务视为单模态分类，未能利用多模态线索和细粒度质量区分；2) 深度多模态融合过程中引入噪声，导致误导性信号；3) 缺乏捕捉复杂语义关系（如相关性和全面性）的能力。

Method: 本文提出MFTRR框架，将帖子质量评估重构为排序任务并融合多模态数据。该框架包含两个关键模块：1) 局部-全局语义关联推理模块，通过最大信息融合机制抑制噪声，建模帖子与主题的细粒度语义交互；2) 多层级证据关系推理模块，探索宏观和微观关系线索以强化证据推理。

Result: MFTRR在三个新建的多模态主题-帖子数据集和公共Lazada-Home数据集上进行了评估。实验结果表明，MFTRR显著优于现有最先进的基线方法，在艺术史数据集上，相对于最佳单模态方法，NDCG@3提升高达9.52%。

Conclusion: MFTRR框架通过模拟人类认知过程、重构为排序任务并结合多模态细粒度关系推理，有效解决了现有帖子质量评估方法的不足，并在实际应用中取得了显著的性能提升。

Abstract: Accurately assessing post quality requires complex relational reasoning to
capture nuanced topic-post relationships. However, existing studies face three
major limitations: (1) treating the task as unimodal categorization, which
fails to leverage multimodal cues and fine-grained quality distinctions; (2)
introducing noise during deep multimodal fusion, leading to misleading signals;
and (3) lacking the ability to capture complex semantic relationships like
relevance and comprehensiveness. To address these issues, we propose the
Multimodal Fine-grained Topic-post Relational Reasoning (MFTRR) framework,
which mimics human cognitive processes. MFTRR reframes post-quality assessment
as a ranking task and incorporates multimodal data to better capture quality
variations. It consists of two key modules: (1) the Local-Global Semantic
Correlation Reasoning Module, which models fine-grained semantic interactions
between posts and topics at both local and global levels, enhanced by a maximum
information fusion mechanism to suppress noise; and (2) the Multi-Level
Evidential Relational Reasoning Module, which explores macro- and micro-level
relational cues to strengthen evidence-based reasoning. We evaluate MFTRR on
three newly constructed multimodal topic-post datasets and the public
Lazada-Home dataset. Experimental results demonstrate that MFTRR significantly
outperforms state-of-the-art baselines, achieving up to 9.52% NDCG@3
improvement over the best unimodal method on the Art History dataset.

</details>


### [168] [VIBE: Video-Input Brain Encoder for fMRI Response Modeling](https://arxiv.org/abs/2507.17958)
*Daniel Carlstrom Schad,Shrey Dixit,Janis Keck,Viktor Studenyak,Aleksandr Shpilevoi,Andrej Bicanski*

Main category: cs.LG

TL;DR: VIBE是一个双阶段Transformer模型，通过融合多模态视频、音频和文本特征来预测fMRI活动，并在多项测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 旨在利用多模态信息（视频、音频、文本）预测大脑的fMRI活动，以探索大脑如何整合和处理来自复杂环境的感官信息。

Method: VIBE采用双阶段Transformer架构：首先，通过一个多模态融合Transformer整合来自开源模型（Qwen2.5、BEATs、Whisper、SlowFast、V-JEPA）提取的视频、音频和文本特征；接着，一个预测Transformer利用旋转嵌入对融合后的特征进行时间解码，最终预测fMRI活动。模型在CNeuroMod数据集的65小时电影数据上进行训练，并对20个种子进行集成。

Result: VIBE在同分布数据集（Friends S07）上取得了32.25的平均局部皮尔逊相关性，在六部异分布电影上取得了21.25的平均局部皮尔逊相关性。其早期版本在Algonauts 2025挑战赛中赢得第一阶段冠军并获得总成绩第二名，分别取得0.3198和0.2096的相关性。

Conclusion: VIBE模型通过创新的多模态特征融合和Transformer架构，在预测fMRI活动方面表现出卓越的性能，无论是在同分布还是异分布数据上均取得高相关性，并在国际挑战赛中获得认可，展示了其在大脑活动解码领域的强大潜力。

Abstract: We present VIBE, a two-stage Transformer that fuses multi-modal video, audio,
and text features to predict fMRI activity. Representations from open-source
models (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a
modality-fusion transformer and temporally decoded by a prediction transformer
with rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod
dataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson
correlations of 32.25 on in-distribution Friends S07 and 21.25 on six
out-of-distribution films. An earlier iteration of the same architecture
obtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second
overall in the Algonauts 2025 Challenge.

</details>


### [169] [Improving the Computational Efficiency and Explainability of GeoAggregator](https://arxiv.org/abs/2507.17977)
*Rui Deng,Ziqi Li,Mingshu Wang*

Main category: cs.LG

TL;DR: 本文通过优化数据处理、集成学习和引入GeoShapley解释功能，提升了地理空间表格数据模型GeoAggregator (GA)的计算效率、预测精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 地理空间表格数据(GTD)的精确建模和解释对于理解地理空间现象至关重要。现有GeoAggregator (GA)模型虽表现优异，但仍需提升其计算效率和模型可解释性。

Method: 1. 优化了数据加载流程并简化GA的前向传递，以提高计算效率。2. 整合了模型集成策略。3. 引入了基于GeoShapley框架的事后模型解释功能，以增强模型可解释性。所有策略均通过在合成数据集上应用改进后的GA模型进行验证。

Result: 1. 实验结果显示，相较于原始实现，改进后的GA模型提升了预测精度和推理速度。2. 解释性实验表明GA能有效捕捉设计好的合成数据集中固有的空间效应。

Conclusion: 改进后的GeoAggregator (GA)模型在处理地理空间表格数据时，实现了计算效率、预测性能和模型可解释性的显著提升，并能有效揭示空间效应。完整的实现已开源供社区使用。

Abstract: Accurate modeling and explaining geospatial tabular data (GTD) are critical
for understanding geospatial phenomena and their underlying processes. Recent
work has proposed a novel transformer-based deep learning model named
GeoAggregator (GA) for this purpose, and has demonstrated that it outperforms
other statistical and machine learning approaches. In this short paper, we
further improve GA by 1) developing an optimized pipeline that accelerates the
dataloading process and streamlines the forward pass of GA to achieve better
computational efficiency; and 2) incorporating a model ensembling strategy and
a post-hoc model explanation function based on the GeoShapley framework to
enhance model explainability. We validate the functionality and efficiency of
the proposed strategies by applying the improved GA model to synthetic
datasets. Experimental results show that our implementation improves the
prediction accuracy and inference speed of GA compared to the original
implementation. Moreover, explanation experiments indicate that GA can
effectively captures the inherent spatial effects in the designed synthetic
dataset. The complete pipeline has been made publicly available for community
use (https://github.com/ruid7181/GA-sklearn).

</details>


### [170] [SIFOTL: A Principled, Statistically-Informed Fidelity-Optimization Method for Tabular Learning](https://arxiv.org/abs/2507.17979)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.LG

TL;DR: SIFOTL是一种隐私保护的、对噪声鲁棒的方法，用于识别表格数据集中数据偏移的关键驱动因素，尤其适用于医疗保健领域，并显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 在表格数据集中识别数据偏移的驱动因素，特别是医疗保健领域，是一个重大挑战。隐私规则限制了数据访问，复杂过程中的噪声阻碍了分析。

Method: 本文提出SIFOTL方法，包含三步：(i) 提取符合隐私规定的数据摘要统计；(ii) 使用LLM辅助的双XGBoost模型分离干预信号与噪声；(iii) 通过帕累托加权决策树合并XGBoost输出，以识别导致偏移的可解释数据段。

Result: 在模拟Medicare药物补贴的MEPS数据集上，SIFOTL的F1分数达到0.85，显著优于BigQuery贡献分析 (F1=0.46) 和统计测试 (F1=0.20)。在基于Synthea ABM生成的18个EHR数据集上，SIFOTL在无噪声时F1分数维持在0.86-0.96，即使注入观测噪声也能达到≥0.75，而基线方法的平均F1分数仅为0.19-0.67。

Conclusion: SIFOTL提供了一个可解释、注重隐私且对观测噪声具有经验鲁棒性的工作流程，能够有效地识别数据偏移的驱动因素。

Abstract: Identifying the factors driving data shifts in tabular datasets is a
significant challenge for analysis and decision support systems, especially
those focusing on healthcare. Privacy rules restrict data access, and noise
from complex processes hinders analysis. To address this challenge, we propose
SIFOTL (Statistically-Informed Fidelity-Optimization Method for Tabular
Learning) that (i) extracts privacy-compliant data summary statistics, (ii)
employs twin XGBoost models to disentangle intervention signals from noise with
assistance from LLMs, and (iii) merges XGBoost outputs via a Pareto-weighted
decision tree to identify interpretable segments responsible for the shift.
Unlike existing analyses which may ignore noise or require full data access for
LLM-based analysis, SIFOTL addresses both challenges using only privacy-safe
summary statistics. Demonstrating its real-world efficacy, for a MEPS panel
dataset mimicking a new Medicare drug subsidy, SIFOTL achieves an F1 score of
0.85, substantially outperforming BigQuery Contribution Analysis (F1=0.46) and
statistical tests (F1=0.20) in identifying the segment receiving the subsidy.
Furthermore, across 18 diverse EHR datasets generated based on Synthea ABM,
SIFOTL sustains F1 scores of 0.86-0.96 without noise and >= 0.75 even with
injected observational noise, whereas baseline average F1 scores range from
0.19-0.67 under the same tests. SIFOTL, therefore, provides an interpretable,
privacy-conscious workflow that is empirically robust to observational noise.

</details>


### [171] [Machine Unlearning of Traffic State Estimation and Prediction](https://arxiv.org/abs/2507.17984)
*Xin Wang,R. Tyrrell Rockafellar,Xuegang,Ban*

Main category: cs.LG

TL;DR: 针对数据驱动交通状态估计与预测（TSEP）中的隐私、安全和数据时效性问题，本研究提出机器学习遗忘TSEP（MUTSEP）范式，使模型能选择性遗忘敏感数据，以增强系统可信度。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的TSEP依赖敏感数据，导致隐私、网络安全和数据新鲜度问题，削弱公众信任。鉴于“被遗忘权”法规要求从模型中移除私人数据，而现有模型会“记住”旧数据，简单删除后端数据不足以满足要求，因此需要一种能使模型“遗忘”数据的机制。

Method: 引入一种新颖的学习范式——“机器学习遗忘TSEP”（Machine Unlearning TSEP, MUTSEP），旨在使已训练的TSEP模型能够选择性地遗忘隐私敏感、被污染或过时的数据。

Result: 本文提出并引入了机器学习遗忘TSEP（MUTSEP）范式，旨在使训练过的TSEP模型能够选择性地遗忘隐私敏感、污染或过时的数据，以应对现有TSEP模型的隐私和数据管理挑战。

Conclusion: 通过赋能模型“遗忘”能力，该研究旨在提升数据驱动交通TSEP的可信度和可靠性，有效解决其面临的隐私、网络安全和数据新鲜度问题。

Abstract: Data-driven traffic state estimation and prediction (TSEP) relies heavily on
data sources that contain sensitive information. While the abundance of data
has fueled significant breakthroughs, particularly in machine learning-based
methods, it also raises concerns regarding privacy, cybersecurity, and data
freshness. These issues can erode public trust in intelligent transportation
systems. Recently, regulations have introduced the "right to be forgotten",
allowing users to request the removal of their private data from models. As
machine learning models can remember old data, simply removing it from back-end
databases is insufficient in such systems. To address these challenges, this
study introduces a novel learning paradigm for TSEP-Machine Unlearning
TSEP-which enables a trained TSEP model to selectively forget
privacy-sensitive, poisoned, or outdated data. By empowering models to
"unlearn," we aim to enhance the trustworthiness and reliability of data-driven
traffic TSEP.

</details>


### [172] [Predictive Scaling Laws for Efficient GRPO Training of Large Reasoning Models](https://arxiv.org/abs/2507.18014)
*Datta Nimmaturi,Vaishnavi Bhargava,Rajat Ghosh,Johnu George,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 本文提出一个预测框架和经验缩放定律，以优化LLM基于GRPO的微调过程，降低其高昂的计算成本，发现提前停止可显著节省资源且不影响性能。


<details>
  <summary>Details</summary>
Motivation: 使用Group Relative Policy Optimization（GRPO）等强化学习方法微调大型语言模型（LLMs）以进行推理任务，计算成本过高。

Method: 提出一个预测框架来建模训练动态，并帮助优化资源使用。通过在Llama和Qwen模型（3B 8B）上进行实验，推导出一个基于模型大小、初始性能和训练进度的经验缩放定律。

Result: 该缩放定律能够预测奖励轨迹，并识别出三个一致的训练阶段：缓慢启动、快速改进和平台期。研究发现，训练超过一定数量的epoch后收益甚微，表明提前停止可以显著减少计算量而不牺牲性能。

Conclusion: 所提出的方法可推广到不同模型类型，为GRPO微调提供了高效的实用指南。

Abstract: Fine-tuning large language models (LLMs) for reasoning tasks using
reinforcement learning methods like Group Relative Policy Optimization (GRPO)
is computationally expensive. To address this, we propose a predictive
framework that models training dynamics and helps optimize resource usage.
Through experiments on Llama and Qwen models (3B 8B), we derive an empirical
scaling law based on model size, initial performance, and training progress.
This law predicts reward trajectories and identifies three consistent training
phases: slow start, rapid improvement, and plateau. We find that training
beyond certain number of an epoch offers little gain, suggesting earlier
stopping can significantly reduce compute without sacrificing performance. Our
approach generalizes across model types, providing a practical guide for
efficient GRPO-based fine-tuning.

</details>


### [173] [Multiscale Neural PDE Surrogates for Prediction and Downscaling: Application to Ocean Currents](https://arxiv.org/abs/2507.18067)
*Abdessamad El-Kabid,Loubna Benabbou,Redouane Lguensat,Alex Hernández-García*

Main category: cs.LG

TL;DR: 本研究提出了一种基于神经算子的监督深度学习框架，用于解决偏微分方程并对现有低分辨率海洋电流数据（如哥白尼数据）进行任意分辨率的降尺度，以提供精细的局部分析所需的高分辨率数据。


<details>
  <summary>Details</summary>
Motivation: 科学计算中，准确模拟由偏微分方程控制的物理系统是一个核心挑战。在海洋学中，高分辨率的洋流数据对于沿海管理、环境监测和海上安全至关重要。然而，现有的卫星产品（如哥白尼数据）和全球海洋模型通常缺乏详细局部分析所需的空间粒度。

Method: 引入了一个基于神经算子的监督深度学习框架，用于：(a) 解决偏微分方程并提供任意分辨率的解；(b) 提出降尺度模型，并应用于哥白尼海洋电流数据。该方法还可以模拟替代偏微分方程，并预测任意分辨率的解，无论输入分辨率如何。

Result: 模型在真实世界的哥白尼海洋电流数据和合成的纳维-斯托克斯模拟数据集上进行了评估。

Conclusion: 本研究提出的深度学习框架能够解决现有海洋电流数据分辨率不足的问题，通过神经算子提供任意分辨率的解决方案和降尺度能力，对于需要详细局部分析的海洋应用具有重要意义。

Abstract: Accurate modeling of physical systems governed by partial differential
equations is a central challenge in scientific computing. In oceanography,
high-resolution current data are critical for coastal management, environmental
monitoring, and maritime safety. However, available satellite products, such as
Copernicus data for sea water velocity at ~0.08 degrees spatial resolution and
global ocean models, often lack the spatial granularity required for detailed
local analyses. In this work, we (a) introduce a supervised deep learning
framework based on neural operators for solving PDEs and providing arbitrary
resolution solutions, and (b) propose downscaling models with an application to
Copernicus ocean current data. Additionally, our method can model surrogate
PDEs and predict solutions at arbitrary resolution, regardless of the input
resolution. We evaluated our model on real-world Copernicus ocean current data
and synthetic Navier-Stokes simulation datasets.

</details>


### [174] [Group Sequence Policy Optimization](https://arxiv.org/abs/2507.18071)
*Chujie Zheng,Shixuan Liu,Mingze Li,Xiong-Hui Chen,Bowen Yu,Chang Gao,Kai Dang,Yuqiong Liu,Rui Men,An Yang,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: 本文提出GSPO算法，一种用于训练大型语言模型的稳定、高效的强化学习算法，通过序列级优化显著提升了训练效率和性能，并稳定了MoE模型训练。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习算法在训练大型语言模型时，可能面临稳定性、效率和性能方面的挑战，尤其在处理混合专家（MoE）模型时需要更稳定的训练方法。

Method: 引入了Group Sequence Policy Optimization (GSPO) 算法。该算法与以往采用token级重要性比率的方法不同，GSPO基于序列似然定义重要性比率，并执行序列级的裁剪、奖励和优化。

Result: GSPO相较于GRPO算法在训练效率和性能上表现更优，显著稳定了混合专家（MoE）模型的强化学习训练，并具有简化强化学习基础设施设计的潜力。GSPO的优势已为最新的Qwen3模型带来了显著改进。

Conclusion: GSPO通过创新的序列级优化，有效解决了大型语言模型强化学习训练中的稳定性、效率和性能问题，尤其在MoE模型训练中展现出卓越的稳定性，并已在实际模型（Qwen3）中取得显著成效。

Abstract: This paper introduces Group Sequence Policy Optimization (GSPO), our stable,
efficient, and performant reinforcement learning algorithm for training large
language models. Unlike previous algorithms that adopt token-level importance
ratios, GSPO defines the importance ratio based on sequence likelihood and
performs sequence-level clipping, rewarding, and optimization. We demonstrate
that GSPO achieves superior training efficiency and performance compared to the
GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and
has the potential for simplifying the design of RL infrastructure. These merits
of GSPO have contributed to the remarkable improvements in the latest Qwen3
models.

</details>


### [175] [C-AAE: Compressively Anonymizing Autoencoders for Privacy-Preserving Activity Recognition in Healthcare Sensor Streams](https://arxiv.org/abs/2507.18072)
*Ryusei Fujimoto,Yugo Nakamura,Yutaka Arakawa*

Main category: cs.LG

TL;DR: 本文提出C-AAE，一种结合AAE和ADPCM的压缩匿名化自编码器，旨在为可穿戴传感器数据提供隐私保护，同时保持活动识别性能并显著降低数据量。


<details>
  <summary>Details</summary>
Motivation: 可穿戴传感器数据包含可用于用户重识别的精细行为特征，这使得在医疗保健应用中保护用户隐私变得至关重要。

Method: 研究引入了C-AAE，它将匿名化自编码器（AAE）与自适应差分脉冲编码调制（ADPCM）相结合。AAE首先将原始传感器数据投影到潜在空间，以保留活动相关特征并抑制身份线索。ADPCM随后对该潜在流进行差分编码，进一步掩盖剩余身份信息并压缩比特率。

Result: 在MotionSense和PAMAP2数据集上的实验表明，C-AAE相对于单独使用AAE，用户重识别F1分数降低了10-15个百分点，同时活动识别F1分数与未保护基线相比保持在5个百分点以内。此外，ADPCM还将数据量减少了约75%。

Conclusion: C-AAE提供了一种实用的方法，可以在医疗保健领域中基于传感器的连续活动识别中平衡隐私保护和数据实用性。

Abstract: Wearable accelerometers and gyroscopes encode fine-grained behavioural
signatures that can be exploited to re-identify users, making privacy
protection essential for healthcare applications. We introduce C-AAE, a
compressive anonymizing autoencoder that marries an Anonymizing AutoEncoder
(AAE) with Adaptive Differential Pulse-Code Modulation (ADPCM). The AAE first
projects raw sensor windows into a latent space that retains activity-relevant
features while suppressing identity cues. ADPCM then differentially encodes
this latent stream, further masking residual identity information and shrinking
the bitrate. Experiments on the MotionSense and PAMAP2 datasets show that C-AAE
cuts user re-identification F1 scores by 10-15 percentage points relative to
AAE alone, while keeping activity-recognition F1 within 5 percentage points of
the unprotected baseline. ADPCM also reduces data volume by roughly 75 %,
easing transmission and storage overheads. These results demonstrate that C-AAE
offers a practical route to balancing privacy and utility in continuous,
sensor-based activity recognition for healthcare.

</details>


### [176] [Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged Mixed-Precision Quantization Method](https://arxiv.org/abs/2507.18073)
*Qingcheng Zhu,Yangyang Ren,Linlin Yang,Mingbao Lin,Yanjing Li,Sheng Xu,Zichao Feng,Haodong Zhu,Yuguang Yang,Juan Zhang,Runqi Wang,Baochang Zhang*

Main category: cs.LG

TL;DR: Squeeze10-LLM是一种分阶段混合精度量化框架，能将大型语言模型权重压缩10倍至平均1.6比特，通过引入PBAR和FIAS两种创新，显著提升了极低比特量化下的模型性能，达到次2比特权重量化的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型因参数量巨大和计算成本高昂，部署面临挑战。超低比特量化虽能显著减少存储并加速推理，但在极端压缩（平均比特宽度≤2）下会导致严重的性能下降。

Method: 本文提出了Squeeze10-LLM，一个分阶段的混合精度训练后量化（PTQ）框架。它通过将80%的权重量化为1比特、20%量化为4比特，实现了平均每权重1.6比特的压缩。关键创新包括：1) 后二值化激活鲁棒性（PBAR），一个考虑量化对激活影响的权重重要性指标；2) 全信息激活监督（FIAS），一种在量化过程中保留完整激活信息以缓解误差传播的策略。

Result: 在LLaMA和LLaMA2模型上的实验表明，Squeeze10-LLM在次2比特纯权重量化领域取得了最先进的性能。在六个零样本分类任务上，平均准确率从43%提升至56%，显著优于现有PTQ方法。

Conclusion: Squeeze10-LLM成功解决了极低比特量化中大型语言模型性能显著下降的问题，通过创新的PBAR和FIAS机制，实现了显著的精度提升，为LLM的部署提供了高效解决方案。

Abstract: Deploying large language models (LLMs) is challenging due to their massive
parameters and high computational costs. Ultra low-bit quantization can
significantly reduce storage and accelerate inference, but extreme compression
(i.e., mean bit-width <= 2) often leads to severe performance degradation. To
address this, we propose Squeeze10-LLM, effectively "squeezing" 16-bit LLMs'
weights by 10 times. Specifically, Squeeze10-LLM is a staged mixed-precision
post-training quantization (PTQ) framework and achieves an average of 1.6 bits
per weight by quantizing 80% of the weights to 1 bit and 20% to 4 bits. We
introduce Squeeze10LLM with two key innovations: Post-Binarization Activation
Robustness (PBAR) and Full Information Activation Supervision (FIAS). PBAR is a
refined weight significance metric that accounts for the impact of quantization
on activations, improving accuracy in low-bit settings. FIAS is a strategy that
preserves full activation information during quantization to mitigate
cumulative error propagation across layers. Experiments on LLaMA and LLaMA2
show that Squeeze10-LLM achieves state-of-the-art performance for sub-2bit
weight-only quantization, improving average accuracy from 43% to 56% on six
zero-shot classification tasks--a significant boost over existing PTQ methods.
Our code will be released upon publication.

</details>


### [177] [Learning from Hard Labels with Additional Supervision on Non-Hard-Labeled Classes](https://arxiv.org/abs/2507.18098)
*Kosuke Sugiyama,Masato Uchida*

Main category: cs.LG

TL;DR: 在数据受限场景下，为构建高精度分类模型，额外监督信息至关重要。本文提出了一个理论框架，通过仿射组合硬标签和额外监督来构建软标签。研究发现，额外监督的关键在于非硬标签类的分布信息，它与混合系数共同互补地细化软标签，并能提升分类准确性。


<details>
  <summary>Details</summary>
Motivation: 在训练数据有限的情况下（如成本高或数据稀缺），为构建高精度分类模型，丰富实例的标签信息至关重要。在这种背景下，除了硬标签外，通常还可以获取额外监督信息（如硬标签的置信度）。因此，本文旨在探讨哪些额外监督信息本质上有益，以及它们如何促进泛化性能的提升。

Method: 提出一个理论框架，将硬标签和额外监督都视为概率分布，并通过它们的仿射组合构建软标签。在此基础上，进行泛化误差理论分析，以刻画额外监督及其混合系数如何影响误差边界的收敛速度和渐近值。最后，通过实验验证理论指导下额外监督的设计能提升分类精度。

Result: 1. 额外监督的关键组成部分并非分配的硬标签的置信度，而是非硬标签类别的分布信息。
2. 额外监督和混合系数在软标签细化中发挥互补作用：额外监督确定硬标签应向真实标签分布调整的方向，而混合系数控制调整的步长。
3. 从理论上刻画了额外监督及其混合系数如何影响泛化误差界限的收敛速度和渐近值。
4. 实验证明，基于该理论设计额外监督，即使简单使用也能提升分类准确性。

Conclusion: 本文提出的理论框架阐明了额外监督的本质，强调非硬标签类别分布信息的重要性，并揭示了它与混合系数如何协同作用以细化软标签。研究表明，基于此理论设计额外监督能够有效提升数据受限场景下的分类模型的泛化能力和准确性。

Abstract: In scenarios where training data is limited due to observation costs or data
scarcity, enriching the label information associated with each instance becomes
crucial for building high-accuracy classification models. In such contexts, it
is often feasible to obtain not only hard labels but also {\it additional
supervision}, such as the confidences for the hard labels. This setting
naturally raises fundamental questions: {\it What kinds of additional
supervision are intrinsically beneficial?} And {\it how do they contribute to
improved generalization performance?} To address these questions, we propose a
theoretical framework that treats both hard labels and additional supervision
as probability distributions, and constructs soft labels through their affine
combination. Our theoretical analysis reveals that the essential component of
additional supervision is not the confidence score of the assigned hard label,
but rather the information of the distribution over the non-hard-labeled
classes. Moreover, we demonstrate that the additional supervision and the
mixing coefficient contribute to the refinement of soft labels in complementary
roles. Intuitively, in the probability simplex, the additional supervision
determines the direction in which the deterministic distribution representing
the hard label should be adjusted toward the true label distribution, while the
mixing coefficient controls the step size along that direction. Through
generalization error analysis, we theoretically characterize how the additional
supervision and its mixing coefficient affect both the convergence rate and
asymptotic value of the error bound. Finally, we experimentally demonstrate
that, based on our theory, designing additional supervision can lead to
improved classification accuracy, even when utilized in a simple manner.

</details>


### [178] [Percentile-Based Deep Reinforcement Learning and Reward Based Personalization For Delay Aware RAN Slicing in O-RAN](https://arxiv.org/abs/2507.18111)
*Peyman Tehrani,Anas Alsoliman*

Main category: cs.LG

TL;DR: 本文在O-RAN架构下，提出一种基于深度强化学习（PDA-DRL）的RAN切片方案，旨在为多MVNOs优化概率延迟和PRB利用率，并引入基于奖励的模型权重个性化共享机制。


<details>
  <summary>Details</summary>
Motivation: 解决开放无线接入网络（O-RAN）架构中无线接入网（RAN）切片的挑战，特别是在多个移动虚拟网络运营商（MVNOs）竞争物理资源块（PRBs）时，需要满足客户的概率延迟上限约束同时最小化PRB利用率。

Method: 首先，基于大数定律（LLN）推导并实际修改了奖励函数。然后，提出“基于百分位数延迟感知的深度强化学习”（PDA-DRL）解决方案。此外，为开发鲁棒的个性化模型，引入了一种基于奖励的个性化方法，让每个智能体根据性能优先考虑其他智能体的模型权重，以解决多MVNOs之间的模型权重共享问题。

Result: PDA-DRL模型在平均延迟方面比基线（包括针对平均延迟约束优化的DRL模型）减少了38%。所提出的基于奖励的个性化模型权重共享技术优于联邦平均等传统聚合方法以及依赖流量模式和模型权重距离相似性的策略。

Conclusion: 本文成功提出了高效的PDA-DRL模型以解决O-RAN架构中的RAN切片挑战，显著优化了概率延迟和PRB利用率。同时，通过引入创新的奖励型个性化方法，有效提升了多MVNOs环境下的模型权重共享性能和个性化模型的鲁棒性。

Abstract: In this paper, we tackle the challenge of radio access network (RAN) slicing
within an open RAN (O-RAN) architecture. Our focus centers on a network that
includes multiple mobile virtual network operators (MVNOs) competing for
physical resource blocks (PRBs) with the goal of meeting probabilistic delay
upper bound constraints for their clients while minimizing PRB utilization.
Initially, we derive a reward function based on the law of large numbers (LLN),
then implement practical modifications to adapt it for real-world experimental
scenarios. We then propose our solution, the Percentile-based Delay-Aware Deep
Reinforcement Learning (PDA-DRL), which demonstrates its superiority over
several baselines, including DRL models optimized for average delay
constraints, by achieving a 38\% reduction in resultant average delay.
Furthermore, we delve into the issue of model weight sharing among multiple
MVNOs to develop a robust personalized model. We introduce a reward-based
personalization method where each agent prioritizes other agents' model weights
based on their performance. This technique surpasses traditional aggregation
methods, such as federated averaging, and strategies reliant on traffic
patterns and model weight distance similarities.

</details>


### [179] [Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language Models and Critical State Identification](https://arxiv.org/abs/2507.18113)
*Junyong Jiang,Buwei Tian,Chenxing Xu,Songze Li,Lu Dong*

Main category: cs.LG

TL;DR: 针对RL系统的对抗性攻击面临挑战且现有方法不实用。本文提出一种新颖的、不修改环境的攻击方法，利用现有智能体、基于LLM的奖励优化框架和关键状态识别，诱导目标策略输出次优动作，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）系统虽然取得了显著成功，但其面临的对抗性攻击仍然是一个挑战。现有对抗攻击方法通常依赖于修改环境或策略，这极大地限制了其实用性。因此，需要一种更实用且不修改环境的对抗攻击方法。

Method: 本文提出一种对抗性攻击方法，该方法不修改环境，而是利用环境中现有智能体引导目标策略输出次优动作。具体包括：
1.  **奖励迭代优化框架**：引入一个奖励迭代优化框架，利用大型语言模型（LLMs）生成专门针对目标智能体脆弱性的对抗性奖励，以增强诱导其次优决策的有效性。
2.  **关键状态识别算法**：设计一个算法来识别目标智能体最脆弱的状态，即在这些状态下受害者的次优行为会导致整体性能显著下降。

Result: 在不同环境下的实验结果表明，所提出的方法优于现有的对抗攻击方法。

Conclusion: 本文提出了一种新颖且实用的强化学习对抗攻击方法，该方法通过利用LLM生成的对抗奖励和关键状态识别，成功在不修改环境的前提下诱导目标RL智能体产生次优决策。实验证明，该方法在有效性上优于现有方法。

Abstract: Reinforcement learning (RL) has achieved remarkable success in fields like
robotics and autonomous driving, but adversarial attacks designed to mislead RL
systems remain challenging. Existing approaches often rely on modifying the
environment or policy, limiting their practicality. This paper proposes an
adversarial attack method in which existing agents in the environment guide the
target policy to output suboptimal actions without altering the environment. We
propose a reward iteration optimization framework that leverages large language
models (LLMs) to generate adversarial rewards explicitly tailored to the
vulnerabilities of the target agent, thereby enhancing the effectiveness of
inducing the target agent toward suboptimal decision-making. Additionally, a
critical state identification algorithm is designed to pinpoint the target
agent's most vulnerable states, where suboptimal behavior from the victim leads
to significant degradation in overall performance. Experimental results in
diverse environments demonstrate the superiority of our method over existing
approaches.

</details>


### [180] [Maximizing Prefix-Confidence at Test-Time Efficiently Improves Mathematical Reasoning](https://arxiv.org/abs/2507.18122)
*Matthias Otth,Jonas Hübotter,Ido Hakimi,Andreas Krause*

Main category: cs.LG

TL;DR: 研究表明，通过模型自身的“前缀置信度”来选择最有前景的尝试，可以显著提升语言模型在数学推理任务上的表现，且优于多数投票法。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明语言模型可以通过最大化自身预测置信度进行自我提升。本研究旨在探讨在数学推理任务中，如何利用模型自身的置信度在测试时扩展语言模型的性能。

Method: 研究者使用模型自身的“前缀置信度”来选择最有前景的尝试。他们系统地在GSM8K、MATH500、AMC23、AIME24和AIME25等五个数学推理数据集上评估了这种“前缀置信度缩放”方法，并将其与多数投票法和测试时训练进行比较。

Result: 通过仅32个token的前缀置信度缩放，研究发现其在准确性-计算成本权衡方面优于多数投票法。此外，前缀置信度缩放似乎比BoN（多数投票法）更不容易受到长度偏差的影响。测试时训练虽然优于基础模型，但未能超越前缀置信度缩放。

Conclusion: 利用模型自身的“前缀置信度”进行选择性尝试，是一种有效且高效的提升语言模型在数学推理任务性能的方法，尤其在准确性和计算效率之间取得了更好的平衡，并减少了长度偏差的影响。

Abstract: Recent work has shown that language models can self-improve by maximizing
their own confidence in their predictions, without relying on external
verifiers or reward signals. In this work, we study the test-time scaling of
language models for mathematical reasoning tasks, where the model's own
confidence is used to select the most promising attempts. Surprisingly, we find
that we can achieve significant performance gains by continuing only the most
promising attempt, selected by the model's prefix-confidence. We systematically
evaluate prefix-confidence scaling on five mathematical reasoning datasets: the
school-level GSM8K and MATH500, and the competition-level AMC23, AIME24, and
AIME25. We find that prefix-confidence scaling with prefixes of only 32 tokens
achieves a better accuracy-compute trade-off than majority voting. Moreover,
prefix-confidence scaling appears less susceptible than BoN to length biases.
Finally, we also evaluate test-time training with prefix-confidence and find
that, while outperforming the base model, it does not improve over
prefix-confidence scaling.

</details>


### [181] [Neuromorphic Computing for Embodied Intelligence in Autonomous Systems: Current Trends, Challenges, and Future Directions](https://arxiv.org/abs/2507.18139)
*Alberto Marchisio,Muhammad Shafique*

Main category: cs.LG

TL;DR: 该论文综述了神经拟态计算在自主系统中的最新进展，重点关注算法、硬件和优化策略，以提升其感知、决策和能效。


<details>
  <summary>Details</summary>
Motivation: 为满足机器人、无人机和自动驾驶等领域对智能、自适应和节能自主系统日益增长的需求，神经拟态计算被视为提升这些平台性能的有效途径。

Method: 本文通过对神经拟态算法、专用硬件和跨层优化策略的最新进展进行综述，并特别关注事件驱动动态视觉传感器，探讨了其在实际自主场景中的部署和应用。

Result: 综述指出，通过整合脉冲神经网络，神经拟态方法显著提升了自主系统的能效、鲁棒性、适应性和可靠性，并从机器学习、机器人学、神经科学和神经拟态工程等多个角度提供了该领域的全面视角。

Conclusion: 神经拟态计算为自主系统带来了巨大潜力，但实时决策、持续学习以及开发安全、弹性系统仍是该领域面临的关键挑战和未来研究方向。

Abstract: The growing need for intelligent, adaptive, and energy-efficient autonomous
systems across fields such as robotics, mobile agents (e.g., UAVs), and
self-driving vehicles is driving interest in neuromorphic computing. By drawing
inspiration from biological neural systems, neuromorphic approaches offer
promising pathways to enhance the perception, decision-making, and
responsiveness of autonomous platforms. This paper surveys recent progress in
neuromorphic algorithms, specialized hardware, and cross-layer optimization
strategies, with a focus on their deployment in real-world autonomous
scenarios. Special attention is given to event-based dynamic vision sensors and
their role in enabling fast, efficient perception. The discussion highlights
new methods that improve energy efficiency, robustness, adaptability, and
reliability through the integration of spiking neural networks into autonomous
system architectures. We integrate perspectives from machine learning,
robotics, neuroscience, and neuromorphic engineering to offer a comprehensive
view of the state of the field. Finally, emerging trends and open challenges
are explored, particularly in the areas of real-time decision-making, continual
learning, and the development of secure, resilient autonomous systems.

</details>


### [182] [When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation Method with LLM and Pseudo Label](https://arxiv.org/abs/2507.18153)
*Riting Xia,Rucong Wang,Yulin Liu,Anchen Li,Xueyan Liu,Yan Zhang*

Main category: cs.LG

TL;DR: 本文提出GraphALP框架，利用LLM和伪标签技术，解决了类别不平衡且标签带噪声的图节点分类问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究在处理类别不平衡图节点分类时，通常假设标签是干净可靠的，但这与真实世界图中标签常含噪声的实际情况不符。因此，针对有噪声标签的类别不平衡图进行鲁棒节点分类是一个未充分探索但实际存在的问题。

Method: 本文提出了GraphALP，一个基于大语言模型（LLMs）和伪标签技术的图增强框架。具体方法包括：1) 设计基于LLM的过采样方法生成合成少数类节点，以缓解类别不平衡并确保标签准确性；2) 开发动态加权伪标签方法，在类别平衡图上获取高置信度伪标签以降低标签噪声；3) 引入二次LLM引导的过采样机制，以缓解伪标签可能导致的类别分布偏差。

Result: 实验结果表明，GraphALP在处理带有噪声标签的类别不平衡图时，性能优于现有的最先进方法。

Conclusion: GraphALP为解决在真实世界场景中普遍存在的类别不平衡且标签带噪声的图节点分类问题提供了一个有效且鲁棒的解决方案。

Abstract: Class-imbalanced graph node classification is a practical yet underexplored
research problem. Although recent studies have attempted to address this issue,
they typically assume clean and reliable labels when processing
class-imbalanced graphs. This assumption often violates the nature of
real-world graphs, where labels frequently contain noise. Given this gap, this
paper systematically investigates robust node classification for
class-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph
Augmentation framework based on Large language models (LLMs) and
Pseudo-labeling techniques. Specifically, we design an LLM-based oversampling
method to generate synthetic minority nodes, producing label-accurate minority
nodes to alleviate class imbalance. Based on the class-balanced graphs, we
develop a dynamically weighted pseudo-labeling method to obtain high-confidence
pseudo labels to reduce label noise ratio. Additionally, we implement a
secondary LLM-guided oversampling mechanism to mitigate potential class
distribution skew caused by pseudo labels. Experimental results show that
GraphALP achieves superior performance over state-of-the-art methods on
class-imbalanced graphs with noisy labels.

</details>


### [183] [ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory](https://arxiv.org/abs/2507.18183)
*Jianchao Wang,Qingfeng Li,Pengcheng Zheng,Xiaorong Pu,Yazhou Ren*

Main category: cs.LG

TL;DR: ChronoSelect框架通过利用预测历史的时间动态性，以四阶段记忆架构和滑动更新机制，有效识别并处理含噪标签，实现了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在含噪标签的真实数据集上训练时，泛化性能会显著下降，因为过参数化模型容易记忆噪声。现有噪声标签学习（LNL）方法未能充分利用学习演化过程中的丰富时间动态性。

Method: 提出ChronoSelect框架，该框架具有创新的四阶段记忆架构，能将预测历史压缩为紧凑的时间分布。通过独特的带控制衰减的滑动更新机制，为每个样本维护四个动态记忆单元，渐进地强调近期模式并保留历史知识。这使得通过时间轨迹分析和双分支一致性，实现样本的精确三向划分（干净、边界、噪声）。该机制在噪声条件下的收敛性和稳定性有理论保证。

Result: 广泛的实验表明，ChronoSelect在合成和真实世界基准测试中均展现出最先进的性能。

Conclusion: ChronoSelect通过有效利用和分析学习过程中的时间动态信息，成功解决了深度学习在含噪标签数据上的训练挑战，显著提升了模型的泛化能力。

Abstract: Training deep neural networks on real-world datasets is often hampered by the
presence of noisy labels, which can be memorized by over-parameterized models,
leading to significant degradation in generalization performance. While
existing methods for learning with noisy labels (LNL) have made considerable
progress, they fundamentally suffer from static snapshot evaluations and fail
to leverage the rich temporal dynamics of learning evolution. In this paper, we
propose ChronoSelect (chrono denoting its temporal nature), a novel framework
featuring an innovative four-stage memory architecture that compresses
prediction history into compact temporal distributions. Our unique sliding
update mechanism with controlled decay maintains only four dynamic memory units
per sample, progressively emphasizing recent patterns while retaining essential
historical knowledge. This enables precise three-way sample partitioning into
clean, boundary, and noisy subsets through temporal trajectory analysis and
dual-branch consistency. Theoretical guarantees prove the mechanism's
convergence and stability under noisy conditions. Extensive experiments
demonstrate ChronoSelect's state-of-the-art performance across synthetic and
real-world benchmarks.

</details>


### [184] [Goal-based Trajectory Prediction for improved Cross-Dataset Generalization](https://arxiv.org/abs/2507.18196)
*Daniel Grimm,Ahmed Abouelazm,J. Marius Zöllner*

Main category: cs.LG

TL;DR: 针对自动驾驶中交通参与者未来状态预测模型泛化能力不足的问题，本文提出一种新的图神经网络（GNN），利用异构图和多阶段目标分类方法，有效提升了模型对未知场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为实现完全自动驾驶，需要准确预测交通参与者的未来状态。然而，现有最先进模型在部署到新区域时泛化能力显著下降，导致性能降低，表明其缺乏泛化性。

Method: 提出一种新的图神经网络（GNN），该网络利用包含交通参与者和矢量化道路网络的异构图。其中，道路网络被用于通过多阶段方法对预测轨迹的终点（即目标）进行分类。

Result: 通过跨数据集评估（在Argoverse2上训练，在NuScenes上评估），证明了所提出的目标选择过程的有效性。

Conclusion: 通过引入基于异构图和多阶段目标分类的GNN，本研究成功提升了自动驾驶中交通参与者未来状态预测模型对未知场景的泛化能力。

Abstract: To achieve full autonomous driving, a good understanding of the surrounding
environment is necessary. Especially predicting the future states of other
traffic participants imposes a non-trivial challenge. Current SotA-models
already show promising results when trained on real datasets (e.g. Argoverse2,
NuScenes). Problems arise when these models are deployed to new/unseen areas.
Typically, performance drops significantly, indicating that the models lack
generalization. In this work, we introduce a new Graph Neural Network (GNN)
that utilizes a heterogeneous graph consisting of traffic participants and
vectorized road network. Latter, is used to classify goals, i.e. endpoints of
the predicted trajectories, in a multi-staged approach, leading to a better
generalization to unseen scenarios. We show the effectiveness of the goal
selection process via cross-dataset evaluation, i.e. training on Argoverse2 and
evaluating on NuScenes.

</details>


### [185] [FedSA-GCL: A Semi-Asynchronous Federated Graph Learning Framework with Personalized Aggregation and Cluster-Aware Broadcasting](https://arxiv.org/abs/2507.18219)
*Zhongzheng Yuan,Lianshuai Guo,Xunkai Li,Yinlin Zhu,Wenyu Wang,Meixia Qu*

Main category: cs.LG

TL;DR: 提出FedSA-GCL，一个半异步联邦图学习框架，旨在解决现有同步联邦图学习效率低下和异步方法不适应图数据的问题，并通过实验证明其在鲁棒性和效率上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有联邦图学习（FGL）方法多依赖同步通信，导致效率低下且不实用。同时，现有异步联邦学习（AFL）方法主要针对传统任务设计，未考虑图数据的拓扑特性，直接应用于图学习可能导致全局模型语义漂移和表示不一致。

Method: 提出FedSA-GCL，一个半异步联邦框架。该框架通过新颖的ClusterCast机制，有效利用客户端间的标签分布差异和图拓扑特性，以实现高效训练。

Result: FedSA-GCL在多个真实图数据集上，经Louvain和Metis分割算法评估，展现出强大的鲁棒性和卓越的效率。实验结果表明，其性能优于9个基线方法，在Louvain和Metis分割下分别平均提升2.92%和3.4%。

Conclusion: FedSA-GCL成功解决了联邦图学习中同步通信的效率瓶颈以及现有异步方法对图数据支持不足的问题，通过其创新的半异步设计和机制，显著提升了图学习的性能、鲁棒性和训练效率。

Abstract: Federated Graph Learning (FGL) is a distributed learning paradigm that
enables collaborative training over large-scale subgraphs located on multiple
local systems. However, most existing FGL approaches rely on synchronous
communication, which leads to inefficiencies and is often impractical in
real-world deployments. Meanwhile, current asynchronous federated learning
(AFL) methods are primarily designed for conventional tasks such as image
classification and natural language processing, without accounting for the
unique topological properties of graph data. Directly applying these methods to
graph learning can possibly result in semantic drift and representational
inconsistency in the global model. To address these challenges, we propose
FedSA-GCL, a semi-asynchronous federated framework that leverages both
inter-client label distribution divergence and graph topological
characteristics through a novel ClusterCast mechanism for efficient training.
We evaluate FedSA-GCL on multiple real-world graph datasets using the Louvain
and Metis split algorithms, and compare it against 9 baselines. Extensive
experiments demonstrate that our method achieves strong robustness and
outstanding efficiency, outperforming the baselines by an average of 2.92% with
the Louvain and by 3.4% with the Metis.

</details>


### [186] [Sparse identification of nonlinear dynamics with library optimization mechanism: Recursive long-term prediction perspective](https://arxiv.org/abs/2507.18220)
*Ansei Yonezawa,Heisei Yonezawa,Shuichi Yahagi,Itsuro Kajiwara,Shinya Kijimoto,Hikaru Taniuchi,Kentaro Murakami*

Main category: cs.LG

TL;DR: 本文提出SINDy-LOM方法，通过优化参数化基函数库解决SINDy库设计难题，提高了模型的可解释性、易用性和长期预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统SINDy方法的主要挑战在于基函数库的设计，对于许多动力系统而言，找到合适的库并非易事。

Method: 提出SINDy-LOM，结合稀疏回归和新颖的库学习策略。该方法将基函数参数化，并采用双层优化架构：内层提取稀疏线性组合模型，外层基于递归长期(RLT)预测精度优化基函数。库设计被重新定义为参数化基函数的优化问题。

Result: SINDy-LOM模型具有良好的可解释性和可用性，能生成简洁的模型。库优化机制显著降低了用户负担。相较于传统SINDy只保证一步预测精度，RLT视角显著提高了模型的可靠性。该方法已成功应用于柴油机气路系统。

Conclusion: SINDy-LOM通过自动优化基函数库，有效地解决了SINDy的库设计难题，提升了模型的长期预测可靠性和实用性，其有效性已通过复杂工业系统验证。

Abstract: The sparse identification of nonlinear dynamics (SINDy) approach can discover
the governing equations of dynamical systems based on measurement data, where
the dynamical model is identified as the sparse linear combination of the given
basis functions. A major challenge in SINDy is the design of a library, which
is a set of candidate basis functions, as the appropriate library is not
trivial for many dynamical systems. To overcome this difficulty, this study
proposes SINDy with library optimization mechanism (SINDy-LOM), which is a
combination of the sparse regression technique and the novel learning strategy
of the library. In the proposed approach, the basis functions are parametrized.
The SINDy-LOM approach involves a two-layer optimization architecture: the
inner-layer, in which the data-driven model is extracted as the sparse linear
combination of the candidate basis functions, and the outer-layer, in which the
basis functions are optimized from the viewpoint of the recursive long-term
(RLT) prediction accuracy; thus, the library design is reformulated as the
optimization of the parametrized basis functions. The resulting SINDy-LOM model
has good interpretability and usability, as the proposed approach yields the
parsimonious model. The library optimization mechanism significantly reduces
user burden. The RLT perspective improves the reliability of the resulting
model compared with the traditional SINDy approach that can only ensure the
one-step-ahead prediction accuracy. The validity of the proposed approach is
demonstrated by applying it to a diesel engine airpath system, which is a
well-known complex industrial system.

</details>


### [187] [Boosting Revisited: Benchmarking and Advancing LP-Based Ensemble Methods](https://arxiv.org/abs/2507.18242)
*Fabian Akkerman,Julien Ferry,Christian Artigues,Emmanuel Hebrard,Thibaut Vidal*

Main category: cs.LG

TL;DR: 本文首次大规模实验研究了基于线性规划的完全校正提升方法，发现它们在浅层树情境下性能优异，可媲美或超越XGBoost/LightGBM，并能生成更稀疏的集成模型。


<details>
  <summary>Details</summary>
Motivation: 尽管基于线性规划的完全校正提升方法具有理论吸引力，但其经验研究非常有限。

Method: 对六种LP提升公式（包括两种新方法NM-Boost和QRLP-Boost）在20个多样化数据集上进行了首次大规模实验研究。评估了启发式和最优基学习器，并分析了准确性、集成稀疏性、裕度分布、即时性能和超参数敏感性。

Result: 研究显示，完全校正方法在使用浅层树时，性能可超越或媲美XGBoost和LightGBM等先进启发式方法，同时能生成显著更稀疏的集成模型。此外，这些方法能在不牺牲性能的情况下对预训练集成模型进行“瘦身”，并揭示了使用最优决策树的优势与局限性。

Conclusion: 基于线性规划的完全校正提升方法在特定场景（如使用浅层树）下表现出强大的竞争力，不仅能匹敌甚至超越现有先进算法，还能带来模型稀疏性等额外优势，且具备模型瘦身能力。

Abstract: Despite their theoretical appeal, totally corrective boosting methods based
on linear programming have received limited empirical attention. In this paper,
we conduct the first large-scale experimental study of six LP-based boosting
formulations, including two novel methods, NM-Boost and QRLP-Boost, across 20
diverse datasets. We evaluate the use of both heuristic and optimal base
learners within these formulations, and analyze not only accuracy, but also
ensemble sparsity, margin distribution, anytime performance, and hyperparameter
sensitivity. We show that totally corrective methods can outperform or match
state-of-the-art heuristics like XGBoost and LightGBM when using shallow trees,
while producing significantly sparser ensembles. We further show that these
methods can thin pre-trained ensembles without sacrificing performance, and we
highlight both the strengths and limitations of using optimal decision trees in
this context.

</details>


### [188] [Leveraging Data Augmentation and Siamese Learning for Predictive Process Monitoring](https://arxiv.org/abs/2507.18293)
*Sjoerd van Straten,Alessandro Padella,Marwan Hassani*

Main category: cs.LG

TL;DR: SiamSA-PPM结合Siamese学习和统计数据增强，通过生成逼真轨迹变体，有效提升预测过程监控（PPM）在数据稀缺和变异性低场景下的预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前的深度学习预测过程监控（PPM）方法受限于真实世界事件日志的低变异性和小规模，影响其泛化能力。

Method: 本文提出了SiamSA-PPM，一个新颖的自监督学习框架，它将Siamese学习与统计数据增强相结合。该框架采用三种基于统计的转换方法，利用控制流语义和频繁行为模式生成逼真、语义有效的新轨迹变体。这些增强视图用于Siamese学习设置中，以学习过程前缀的可泛化表示，且无需标签监督。

Result: 在真实事件日志上的广泛实验表明，SiamSA-PPM在下一活动预测和最终结果预测任务中，性能与最先进方法（SOTA）相当或更优。研究结果还显示，统计数据增强显著优于随机转换，并有效提高了数据变异性。

Conclusion: SiamSA-PPM为过程预测中的训练数据丰富提供了一个有前景的方向，其统计增强方法有效提高了数据多样性和模型性能，解决了现有PPM方法面临的数据限制问题。

Abstract: Predictive Process Monitoring (PPM) enables forecasting future events or
outcomes of ongoing business process instances based on event logs. However,
deep learning PPM approaches are often limited by the low variability and small
size of real-world event logs. To address this, we introduce SiamSA-PPM, a
novel self-supervised learning framework that combines Siamese learning with
Statistical Augmentation for Predictive Process Monitoring. It employs three
novel statistically grounded transformation methods that leverage control-flow
semantics and frequent behavioral patterns to generate realistic, semantically
valid new trace variants. These augmented views are used within a Siamese
learning setup to learn generalizable representations of process prefixes
without the need for labeled supervision. Extensive experiments on real-life
event logs demonstrate that SiamSA-PPM achieves competitive or superior
performance compared to the SOTA in both next activity and final outcome
prediction tasks. Our results further show that statistical augmentation
significantly outperforms random transformations and improves variability in
the data, highlighting SiamSA-PPM as a promising direction for training data
enrichment in process prediction.

</details>


### [189] [Self-Supervised Coarsening of Unstructured Grid with Automatic Differentiation](https://arxiv.org/abs/2507.18297)
*Sergei Shumilin,Alexander Ryabov,Nikolay Yavich,Evgeny Burnaev,Vladimir Vanovskiy*

Main category: cs.LG

TL;DR: 本文提出一种基于可微分物理的新型非结构网格粗化算法，旨在降低数值模拟计算成本，通过减少网格点数同时保持合理精度。


<details>
  <summary>Details</summary>
Motivation: 现代数值模拟计算负荷高，亟需一种能有效减小离散问题规模，同时保持合理精度的方法。

Method: 开发了一种基于可微分物理概念的非结构网格粗化算法。该算法结合了K-均值聚类、自动微分和随机最小化算法。在描述多孔介质中微可压缩流动的线性抛物线方程和波动方程上进行了性能验证。

Result: 在所测试的场景中，该算法成功将网格点数减少多达10倍，同时在关注点上保持了建模变量的动态特性。

Conclusion: 所提出的方法具有通用性，可应用于任何由演化偏微分方程描述的系统模拟。

Abstract: Due to the high computational load of modern numerical simulation, there is a
demand for approaches that would reduce the size of discrete problems while
keeping the accuracy reasonable. In this work, we present an original algorithm
to coarsen an unstructured grid based on the concepts of differentiable
physics. We achieve this by employing k-means clustering, autodifferentiation
and stochastic minimization algorithms. We demonstrate performance of the
designed algorithm on two PDEs: a linear parabolic equation which governs
slightly compressible fluid flow in porous media and the wave equation. Our
results show that in the considered scenarios, we reduced the number of grid
points up to 10 times while preserving the modeled variable dynamics in the
points of interest. The proposed approach can be applied to the simulation of
an arbitrary system described by evolutionary partial differential equations.

</details>


### [190] [Regression-aware Continual Learning for Android Malware Detection](https://arxiv.org/abs/2507.18313)
*Daniele Ghiani,Daniele Angioni,Giorgio Piras,Angelo Sotgiu,Luca Minnei,Srishti Gupta,Maura Pintor,Fabio Roli,Battista Biggio*

Main category: cs.LG

TL;DR: 针对恶意软件检测中持续学习模型的安全回归问题（即已检测恶意软件在模型更新后再次逃逸），本文形式化并量化了该问题，并提出一种基于回归感知惩罚（特别是适应PCT）的方法来有效缓解回归，同时保持高检测性能。


<details>
  <summary>Details</summary>
Motivation: 恶意软件迅速演变，使得基于机器学习的检测器需要持续适应。由于数据量巨大，传统完全重新训练不切实际。持续学习（CL）提供了一种可扩展的替代方案，但存在一个关键且被忽视的问题：安全回归。安全回归指之前正确检测的恶意软件样本在模型更新后反而逃逸，这在安全关键应用中带来严重风险，可能损害用户信任。

Method: 本文首先形式化并量化了CL-based恶意软件检测器中的安全回归问题。为解决此问题，提出了一种“回归感知惩罚”机制，具体是将“正一致性训练（Positive Congruent Training, PCT）”方法应用于持续学习设置，以模型无关的方式保留先前的预测行为。

Result: 在ELSA、Tesseract和AZ-Class数据集上的实验表明，所提出的方法能够有效减少不同持续学习场景下的安全回归。同时，该方法在保持模型强大检测性能方面表现良好。

Conclusion: 本研究成功地形式化并解决了持续学习恶意软件检测器中的安全回归这一关键问题，并提出了一种有效的回归感知方法。实验结果证明了该方法在降低回归、保持检测性能方面的有效性，提升了CL模型在安全关键应用中的可靠性。

Abstract: Malware evolves rapidly, forcing machine learning (ML)-based detectors to
adapt continuously. With antivirus vendors processing hundreds of thousands of
new samples daily, datasets can grow to billions of examples, making full
retraining impractical. Continual learning (CL) has emerged as a scalable
alternative, enabling incremental updates without full data access while
mitigating catastrophic forgetting. In this work, we analyze a critical yet
overlooked issue in this context: security regression. Unlike forgetting, which
manifests as a general performance drop on previously seen data, security
regression captures harmful prediction changes at the sample level, such as a
malware sample that was once correctly detected but evades detection after a
model update. Although often overlooked, regressions pose serious risks in
security-critical applications, as the silent reintroduction of previously
detected threats in the system may undermine users' trust in the whole updating
process. To address this issue, we formalize and quantify security regression
in CL-based malware detectors and propose a regression-aware penalty to
mitigate it. Specifically, we adapt Positive Congruent Training (PCT) to the CL
setting, preserving prior predictive behavior in a model-agnostic manner.
Experiments on the ELSA, Tesseract, and AZ-Class datasets show that our method
effectively reduces regression across different CL scenarios while maintaining
strong detection performance over time.

</details>


### [191] [State of Health Estimation of Batteries Using a Time-Informed Dynamic Sequence-Inverted Transformer](https://arxiv.org/abs/2507.18320)
*Janak M. Patel,Milad Ramezankhani,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.LG

TL;DR: 提出一种新的Transformer模型（TIDSIT），能有效处理不规则和变长的时间序列数据，显著提高了电池健康状态（SoH）估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 电池健康监测对电动汽车和储能系统至关重要。电池会随时间退化，影响系统效率和安全。现有机器学习模型在处理真实世界中不规则采样和变长的电池放电数据时表现不佳，导致SoH估计不准确。

Method: 提出时间感知动态序列反转Transformer (TIDSIT) 架构。TIDSIT通过引入连续时间嵌入来有效表示不规则采样数据，并利用带填充的序列和时间注意力机制管理变长输入，同时不丢失序列信息。

Result: 在NASA电池退化数据集上的实验结果表明，TIDSIT模型显著优于现有模型，预测误差降低超过50%，且SoH预测误差保持在0.58%以下。

Conclusion: TIDSIT模型有效解决了不规则时间序列数据处理的难题，提升了电池SoH估计的准确性，并具有推广到其他涉及不规则时间序列数据的健康监测任务的潜力。

Abstract: The rapid adoption of battery-powered vehicles and energy storage systems
over the past decade has made battery health monitoring increasingly critical.
Batteries play a central role in the efficiency and safety of these systems,
yet they inevitably degrade over time due to repeated charge-discharge cycles.
This degradation leads to reduced energy efficiency and potential overheating,
posing significant safety concerns. Accurate estimation of a State of Health
(SoH) of battery is therefore essential for ensuring operational reliability
and safety. Several machine learning architectures, such as LSTMs,
transformers, and encoder-based models, have been proposed to estimate SoH from
discharge cycle data. However, these models struggle with the irregularities
inherent in real-world measurements: discharge readings are often recorded at
non-uniform intervals, and the lengths of discharge cycles vary significantly.
To address this, most existing approaches extract features from the sequences
rather than processing them in full, which introduces information loss and
compromises accuracy. To overcome these challenges, we propose a novel
architecture: Time-Informed Dynamic Sequence Inverted Transformer (TIDSIT).
TIDSIT incorporates continuous time embeddings to effectively represent
irregularly sampled data and utilizes padded sequences with temporal attention
mechanisms to manage variable-length inputs without discarding sequence
information. Experimental results on the NASA battery degradation dataset show
that TIDSIT significantly outperforms existing models, achieving over 50%
reduction in prediction error and maintaining an SoH prediction error below
0.58%. Furthermore, the architecture is generalizable and holds promise for
broader applications in health monitoring tasks involving irregular time-series
data.

</details>


### [192] [Remembering the Markov Property in Cooperative MARL](https://arxiv.org/abs/2507.18333)
*Kale-ab Abebe Tessera,Leonard Hinckeldey,Riccardo Zamboni,David Abel,Amos Storkey*

Main category: cs.LG

TL;DR: 本文指出当前多智能体强化学习成功并非因有效马尔可夫信号恢复，而是学习了脆弱的约定，并提议设计更强调基于观察和记忆推理的新环境。


<details>
  <summary>Details</summary>
Motivation: 分析现有无模型多智能体强化学习算法在处理部分可观察性挑战时的实际成功原因，并质疑其是否真正实现了对环境和其他智能体的有效推理。

Method: 通过一个有针对性的案例研究，分析了共同适应的智能体如何学习脆弱的约定，以及这些约定在与非适应性智能体合作时的失效情况。同时，揭示了模型本身并非局限，问题出在基准设计上。

Result: 研究发现，现有算法的经验成功是由于学习了规避环境观察和记忆的简单约定，而非有效地恢复马尔可夫信号。这些约定在与非适应性智能体合作时会失效。模型在必要时能学习基于观察的策略，表明问题在于基准环境未能充分测试Dec-POMDP的核心假设。

Conclusion: 当前多智能体强化学习基准未能有效测试Dec-POMDP的核心假设。因此，建议开发新的合作环境，这些环境应基于两个核心原则：(1) 行为基于观察，(2) 基于记忆对其他智能体进行推理，以确保成功需要真正的技能而非脆弱的共同适应性约定。

Abstract: Cooperative multi-agent reinforcement learning (MARL) is typically formalised
as a Decentralised Partially Observable Markov Decision Process (Dec-POMDP),
where agents must reason about the environment and other agents' behaviour. In
practice, current model-free MARL algorithms use simple recurrent function
approximators to address the challenge of reasoning about others using partial
information. In this position paper, we argue that the empirical success of
these methods is not due to effective Markov signal recovery, but rather to
learning simple conventions that bypass environment observations and memory.
Through a targeted case study, we show that co-adapting agents can learn
brittle conventions, which then fail when partnered with non-adaptive agents.
Crucially, the same models can learn grounded policies when the task design
necessitates it, revealing that the issue is not a fundamental limitation of
the learning models but a failure of the benchmark design. Our analysis also
suggests that modern MARL environments may not adequately test the core
assumptions of Dec-POMDPs. We therefore advocate for new cooperative
environments built upon two core principles: (1) behaviours grounded in
observations and (2) memory-based reasoning about other agents, ensuring
success requires genuine skill rather than fragile, co-adapted agreements.

</details>


### [193] [Low-rank adaptive physics-informed HyperDeepONets for solving differential equations](https://arxiv.org/abs/2507.18346)
*Etienne Zeudong,Elsa Cardoso-Bihlo,Alex Bihlo*

Main category: cs.LG

TL;DR: 本文提出PI-LoRA-HyperDeepONets，利用低秩适应（LoRA）技术显著降低HyperDeepONets的参数量和计算成本，并在微分方程任务中展现出更高的预测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的HyperDeepONets虽然提高了表达能力，但由于需要大量的输出参数，导致其内存和计算成本高昂。

Method: 在物理信息机器学习（PIML）背景下，我们引入了PI-LoRA-HyperDeepONets。该方法利用低秩适应（LoRA）技术，将超网络输出层的权重矩阵分解为两个较小的低秩矩阵，从而减少了可训练参数并对主干网络的权重引入了额外的正则化。

Result: 通过对常微分方程和偏微分方程的广泛实验表明，PI-LoRA-HyperDeepONets能够实现高达70%的参数减少，并在预测精度和泛化能力方面持续优于常规的HyperDeepONets。

Conclusion: PI-LoRA-HyperDeepONets通过LoRA技术有效解决了HyperDeepONets的计算和内存开销问题，在大幅减少参数的同时提升了模型的性能，为操作符学习提供了更高效、更鲁棒的解决方案。

Abstract: HyperDeepONets were introduced in Lee, Cho and Hwang [ICLR, 2023] as an
alternative architecture for operator learning, in which a hypernetwork
generates the weights for the trunk net of a DeepONet. While this improves
expressivity, it incurs high memory and computational costs due to the large
number of output parameters required. In this work we introduce, in the
physics-informed machine learning setting, a variation, PI-LoRA-HyperDeepONets,
which leverage low-rank adaptation (LoRA) to reduce complexity by decomposing
the hypernetwork's output layer weight matrix into two smaller low-rank
matrices. This reduces the number of trainable parameters while introducing an
extra regularization of the trunk networks' weights. Through extensive
experiments on both ordinary and partial differential equations we show that
PI-LoRA-HyperDeepONets achieve up to 70\% reduction in parameters and
consistently outperform regular HyperDeepONets in terms of predictive accuracy
and generalization.

</details>


### [194] [Efficient Uncertainty in LLMs through Evidential Knowledge Distillation](https://arxiv.org/abs/2507.18366)
*Lakshmana Sri Harsha Nemani,P. K. Srijith,Tomasz Kuśmierczyk*

Main category: cs.LG

TL;DR: 本文提出一种新方法，通过知识蒸馏（LoRA微调）将不确定性感知的教师模型（需多次前向传播）压缩为学生模型（只需单次前向传播），从而实现大型语言模型（LLMs）的高效不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 标准LLMs在不确定性量化方面面临挑战，而现有的贝叶斯和集成方法计算成本高昂，需要多次前向传播来估计预测不确定性。

Method: 研究者引入一种新方法，将需要多次前向传播的不确定性感知教师模型蒸馏成紧凑的学生模型。学生模型与教师模型架构相同，但使用LoRA进行微调。对比了两种蒸馏策略：一种学生模型使用传统softmax输出，另一种使用Dirichlet分布输出（通过证据学习明确建模认知不确定性）。

Result: 在分类数据集上的实证评估表明，学生模型在预测和不确定性量化性能上与教师模型相当或更优，且仅需单次前向传播。这是首次通过证据蒸馏在LLMs中实现即时且稳健的不确定性量化。

Conclusion: 该研究首次证明，通过证据蒸馏可以实现LLMs的即时和稳健不确定性量化，且学生模型能在保持性能的同时显著提高效率，仅需单次前向传播。

Abstract: Accurate uncertainty quantification remains a key challenge for standard
LLMs, prompting the adoption of Bayesian and ensemble-based methods. However,
such methods typically necessitate computationally expensive sampling,
involving multiple forward passes to effectively estimate predictive
uncertainty.
  In this paper, we introduce a novel approach enabling efficient and effective
uncertainty estimation in LLMs without sacrificing performance. Specifically,
we distill uncertainty-aware teacher models - originally requiring multiple
forward passes - into compact student models sharing the same architecture but
fine-tuned using Low-Rank Adaptation (LoRA). We compare two distinct
distillation strategies: one in which the student employs traditional
softmax-based outputs, and another in which the student leverages
Dirichlet-distributed outputs to explicitly model epistemic uncertainty via
evidential learning.
  Empirical evaluations on classification datasets demonstrate that such
students can achieve comparable or superior predictive and uncertainty
quantification performance relative to their teacher models, while critically
requiring only a single forward pass. To our knowledge, this is the first
demonstration that immediate and robust uncertainty quantification can be
achieved in LLMs through evidential distillation.

</details>


### [195] [A Comprehensive Review of Diffusion Models in Smart Agriculture: Progress, Applications, and Challenges](https://arxiv.org/abs/2507.18376)
*Xing Hua,Haodong Chen,Qianqian Duan,Danfeng Hong,Ruijiao Li,Huiliang Shang,Linghua Jiang,Haima Yang,Dawei Zhang*

Main category: cs.LG

TL;DR: 本文综述了扩散模型在智慧农业中的应用进展，强调其在数据增强、图像处理和病虫害检测方面优于GANs的稳定性与生成质量，有望解决农业数据挑战并促进可持续农业发展。


<details>
  <summary>Details</summary>
Motivation: 随着全球人口增长和耕地资源稀缺，发展智慧农业和精准农业成为趋势。现有AI技术面临农业数据有限和样本不均衡等挑战，需要更稳定、高质量的生成模型来应对。

Method: 本文通过文献综述的方法，回顾了扩散模型在农业领域的最新应用进展，重点探讨了其在农作物病虫害检测、遥感图像增强、作物生长预测和农业资源管理方面的潜力。

Result: 研究结果表明，扩散模型在数据增强、图像生成和去噪任务中显著提高了模型的准确性和鲁棒性，尤其在复杂环境下表现突出。

Conclusion: 尽管存在计算效率和泛化能力等挑战，扩散模型在智慧农业和精准农业中具有巨大潜力，预计未来将为全球农业的可持续发展提供重要支持。

Abstract: With the global population growing and arable land resources becoming
increasingly scarce,smart agriculture and precision agriculture have emerged as
key directions for the future ofagricultural development.Artificial
intelligence (AI) technologies, particularly deep learning models, have found
widespread applications in areas such as crop monitoring and pest detection. As
an emerging generative model, diffusion models have shown significant promise
in tasks like agricultural image processing, data augmentation, and remote
sensing. Compared to traditional generative adversarial networks (GANs),
diffusion models offer superior training stability and generation quality,
effectively addressing challenges such as limited agricultural data and
imbalanced image samples. This paper reviews the latest advancements in the
application of diffusion models in agriculture, focusing on their potential in
crop pest and disease detection, remote sensing image enhancement, crop growth
prediction, and agricultural resource management. Experimental results
demonstrate that diffusion models significantly improve model accuracy and
robustness in data augmentation, image generation, and denoising, especially in
complex environments. Despite challenges related to computational efficiency
and generalization capabilities, diffusion models are expected to play an
increasingly important role in smart and precision agriculture as technology
advances, providing substantial support for the sustainable development of
global agriculture.

</details>


### [196] [Multi-Model Ensemble and Reservoir Computing for River Discharge Prediction in Ungauged Basins](https://arxiv.org/abs/2507.18423)
*Mizuki Funato,Yohei Sawada*

Main category: cs.LG

TL;DR: HYPER是一种结合多模型集成和储备池计算的新方法，能在数据稀缺地区实现高效、准确且可泛化的河川流量预测。


<details>
  <summary>Details</summary>
Motivation: 许多地区缺乏足够的河流流量观测数据，限制了降雨径流分析的准确性。尽管现有模型众多，但在数据稀缺条件下，实现高精度、可解释性和计算效率仍是重大挑战。

Method: 提出HYPER方法。首先对43个“未校准”流域概念水文模型应用贝叶斯模型平均（BMA）。随后，通过线性回归训练储备池计算（RC）模型来校正BMA输出中的误差，此为非迭代过程，确保高计算效率。对于未测量流域，通过将所需BMA和RC权重与已测量流域的流域属性关联起来，推断出相应权重，实现泛化。

Result: 在日本87个流域的数据评估中，在数据丰富场景下，HYPER（KGE中位数为0.56）与基准LSTM（KGE 0.55）性能相当，但计算时间仅为后者的5%。在数据稀缺场景（23%流域有测量数据）下，HYPER保持了稳健性能（KGE 0.55）和较低不确定性，而LSTM性能显著下降（KGE -0.04）。研究表明，有效的大型集成模型结合机器学习偏置校正，可使单个概念水文模型无需校准。

Conclusion: HYPER为流量预测提供了一个鲁棒、高效且可泛化的解决方案，尤其适用于未测量流域，使其可应用于广泛区域。

Abstract: Despite the critical need for accurate flood prediction and water management,
many regions lack sufficient river discharge observations, limiting the skill
of rainfall-runoff analyses. Although numerous physically based and machine
learning models exist, achieving high accuracy, interpretability, and
computational efficiency under data-scarce conditions remains a major
challenge. We address this challenge with a novel method, HYdrological
Prediction with multi-model Ensemble and Reservoir computing (HYPER) that
leverages multi-model ensemble and reservoir computing (RC). Our approach first
applies Bayesian model averaging (BMA) to 43 "uncalibrated" catchment-based
conceptual hydrological models. An RC model is then trained via linear
regression to correct errors in the BMA output, a non-iterative process that
ensures high computational efficiency. For ungauged basins, we infer the
required BMA and RC weights by linking them to catchment attributes from gauged
basins, creating a generalizable framework. We evaluated HYPER using data from
87 river basins in Japan. In a data-rich scenario, HYPER (median Kling-Gupta
Efficiency, KGE, of 0.56) performed comparably to a benchmark LSTM (KGE 0.55)
but required only 5% of its computational time. In a data-scarce scenario (23%
of basins gauged), HYPER maintained robust performance (KGE 0.55) and lower
uncertainty, whereas the LSTM's performance degraded significantly (KGE -0.04).
These results reveal that individual conceptual hydrological models do not
necessarily need to be calibrated when an effectively large ensemble is
assembled and combined with machine-learning-based bias correction. HYPER
provides a robust, efficient, and generalizable solution for discharge
prediction, particularly in ungauged basins, making it applicable to a wide
range of regions.

</details>


### [197] [Revisiting Bisimulation Metric for Robust Representations in Reinforcement Learning](https://arxiv.org/abs/2507.18519)
*Leiji Zhang,Zeyu Wang,Xin Li,Yao-Hui Li*

Main category: cs.LG

TL;DR: 本文针对传统双相似度量在表征能力和固定权重上的不足，提出了一种修正的双相似度量，该度量具有更精确的奖励差距定义和自适应更新算子，并通过理论和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的双相似度量存在两个主要问题：1) 难以表征某些独特场景（源于奖励差距定义不精确）；2) 在递归更新中，奖励和后续状态差异的权重是预定义的，忽略了其在不同训练阶段和任务设置下的重要性变化。

Method: 针对上述问题，本文引入了状态-动作对的度量，并提出了一种修正的双相似度量。该度量具有更精确的奖励差距定义，并设计了带有自适应系数的新型更新算子。此外，研究还提供了该度量的收敛性及其改进表征区分度的理论保证。

Result: 理论分析证明了所提出度量的收敛性及其在表征区分度方面的改进。在DeepMind Control和Meta-World这两个代表性基准上的广泛实验也证明了该方法的有效性。

Conclusion: 本文提出了一种修正的双相似度量，通过更精确的奖励差距定义和自适应更新算子，成功解决了传统度量在表征能力和权重设定上的固有问题。理论和实验结果均证明了该方法的有效性，提升了双相似度量在强化学习中的应用价值。

Abstract: Bisimulation metric has long been regarded as an effective control-related
representation learning technique in various reinforcement learning tasks.
However, in this paper, we identify two main issues with the conventional
bisimulation metric: 1) an inability to represent certain distinctive
scenarios, and 2) a reliance on predefined weights for differences in rewards
and subsequent states during recursive updates. We find that the first issue
arises from an imprecise definition of the reward gap, whereas the second issue
stems from overlooking the varying importance of reward difference and
next-state distinctions across different training stages and task settings. To
address these issues, by introducing a measure for state-action pairs, we
propose a revised bisimulation metric that features a more precise definition
of reward gap and novel update operators with adaptive coefficient. We also
offer theoretical guarantees of convergence for our proposed metric and its
improved representation distinctiveness. In addition to our rigorous
theoretical analysis, we conduct extensive experiments on two representative
benchmarks, DeepMind Control and Meta-World, demonstrating the effectiveness of
our approach.

</details>


### [198] [GLANCE: Graph Logic Attention Network with Cluster Enhancement for Heterophilous Graph Representation Learning](https://arxiv.org/abs/2507.18521)
*Zhongtian Sun,Anoushka Harit,Alexandra Cristea,Christl A. Donnelly,Pietro Liò*

Main category: cs.LG

TL;DR: GLANCE是一种新颖的图神经网络框架，通过结合逻辑推理、动态图剪枝和自适应聚类，显著提升了异质图上的表示学习性能，并提供了可解释的解决方案。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在异质图（连接节点特征或标签不同）上表现不佳，原因在于无差别地聚合邻居信息以及未能充分利用高阶结构模式。

Method: 提出GLANCE框架，该框架整合了三个关键组件：1) 逻辑层，用于生成可解释的结构化嵌入；2) 基于多头注意力的边剪枝，用于动态图结构去噪；3) 聚类机制，用于捕获全局模式。

Result: 在Cornell、Texas和Wisconsin等基准数据集上的实验结果表明，GLANCE在异质图场景中取得了具有竞争力的性能，提供了鲁棒且可解释的解决方案。

Conclusion: GLANCE框架轻量、适应性强，并独特地解决了异质图的挑战，为异质图表示学习提供了有效的改进方案。

Abstract: Graph Neural Networks (GNNs) have demonstrated significant success in
learning from graph-structured data but often struggle on heterophilous graphs,
where connected nodes differ in features or class labels. This limitation
arises from indiscriminate neighbor aggregation and insufficient incorporation
of higher-order structural patterns. To address these challenges, we propose
GLANCE (Graph Logic Attention Network with Cluster Enhancement), a novel
framework that integrates logic-guided reasoning, dynamic graph refinement, and
adaptive clustering to enhance graph representation learning. GLANCE combines a
logic layer for interpretable and structured embeddings, multi-head
attention-based edge pruning for denoising graph structures, and clustering
mechanisms for capturing global patterns. Experimental results in benchmark
datasets, including Cornell, Texas, and Wisconsin, demonstrate that GLANCE
achieves competitive performance, offering robust and interpretable solutions
for heterophilous graph scenarios. The proposed framework is lightweight,
adaptable, and uniquely suited to the challenges of heterophilous graphs.

</details>


### [199] [C2G-KD: PCA-Constrained Generator for Data-Free Knowledge Distillation](https://arxiv.org/abs/2507.18533)
*Magnus Bengtsson,Kenneth Östberg*

Main category: cs.LG

TL;DR: 本文介绍C2G-KD，一种无数据知识蒸馏框架，通过一个类别条件生成器，在冻结教师模型和PCA几何约束的指导下生成合成样本，以实现知识迁移。


<details>
  <summary>Details</summary>
Motivation: 旨在解决在缺乏真实训练数据的情况下，如何有效进行知识蒸馏并生成高质量合成样本的挑战。

Method: 提出C2G-KD框架，其中类别条件生成器在冻结的教师模型指导下，通过语义和结构损失学习激活教师模型的输出。该方法利用PCA从每个类别仅两张真实样本中估计类特定的PCA子空间，并将生成的样本约束在这些子空间内，以保持拓扑一致性和多样性。

Result: 在MNIST数据集上的实验表明，即使只有极少的类别结构信息（每个类别两张真实样本），也足以有效引导合成训练流程。

Conclusion: C2G-KD框架通过结合教师模型指导和少量真实数据提供的几何约束，成功实现了无数据知识蒸馏，证明了即使是最小的结构信息也能有效引导合成数据生成管道。

Abstract: We introduce C2G-KD, a data-free knowledge distillation framework where a
class-conditional generator is trained to produce synthetic samples guided by a
frozen teacher model and geometric constraints derived from PCA. The generator
never observes real training data but instead learns to activate the teacher's
output through a combination of semantic and structural losses. By constraining
generated samples to lie within class-specific PCA subspaces estimated from as
few as two real examples per class, we preserve topological consistency and
diversity. Experiments on MNIST show that even minimal class structure is
sufficient to bootstrap useful synthetic training pipelines.

</details>


### [200] [The Price equation reveals a universal force-metric-bias law of algorithmic learning and natural selection](https://arxiv.org/abs/2507.18549)
*Steven A. Frank*

Main category: cs.LG

TL;DR: 通过Price方程揭示了一个通用的力-度量-偏置（FMB）定律，该定律统一了各种学习算法、优化方法和自然选择，表明它们共享相同的数学结构。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在揭示各种学习算法、优化方法和自然选择之间共同的数学结构，尽管它们表面上存在差异，从而为理解、比较和设计跨学科的学习算法提供一个原则性基础。

Method: 研究通过对Price方程中的变化进行简单的符号划分，提出并推导了一个通用的力-度量-偏置（FMB）定律：$\Delta\mathbf{\theta} = \mathbf{M}\,\mathbf{f} + \mathbf{b} + \mathbf{\xi}$。该定律详细定义了力、度量、偏置和噪声在参数更新中的作用。

Result: 研究发现并阐明了普遍适用的FMB定律。该定律成功地将自然选择、贝叶斯更新、牛顿法、随机梯度下降、Adam优化等多种算法统一为同一底层过程的特例。此外，该框架还解释了费雪信息、Kullback-Leibler散度和达朗贝尔原理为何在学习动态中自然出现。

Conclusion: FMB定律通过揭示学习系统中的共同数学结构，为理解、比较和设计跨学科的学习算法提供了一个统一且有原则的基础。

Abstract: Diverse learning algorithms, optimization methods, and natural selection
share a common mathematical structure, despite their apparent differences. Here
I show that a simple notational partitioning of change by the Price equation
reveals a universal force-metric-bias (FMB) law: $\Delta\mathbf{\theta} =
\mathbf{M}\,\mathbf{f} + \mathbf{b} + \mathbf{\xi}$. The force $\mathbf{f}$
drives improvement in parameters, $\Delta\mathbf{\theta}$, through the
covariance between the parameters and performance. The metric $\mathbf{M}$
rescales movement by inverse curvature. The bias $\mathbf{b}$ adds momentum or
changes in the frame of reference. The noise $\mathbf{\xi}$ enables
exploration. This framework unifies natural selection, Bayesian updating,
Newton's method, stochastic gradient descent, stochastic Langevin dynamics,
Adam optimization, and most other algorithms as special cases of the same
underlying process. The Price equation also reveals why Fisher information,
Kullback-Leibler divergence, and d'Alembert's principle arise naturally in
learning dynamics. By exposing this common structure, the FMB law provides a
principled foundation for understanding, comparing, and designing learning
algorithms across disciplines.

</details>


### [201] [The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane Algorithm](https://arxiv.org/abs/2507.18553)
*Jiale Chen,Torsten Hoefler,Dan Alistarh*

Main category: cs.LG

TL;DR: 本文揭示GPTQ算法在特定条件下与Babai最近平面算法数学等价，为其提供几何解释和误差上界，奠定理论基础。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）的部署需要将权重从16比特量化到更低比特位宽。GPTQ是主流的量化方法，但其内部机制缺乏几何意义或最坏情况保证，需要坚实的理论基础。

Method: 通过数学论证，证明GPTQ算法在对线性层进行从后到前的执行时，与Babai的最近平面算法在由层输入Hessian矩阵定义的格上的最近向量问题中是数学上等价的。

Result: 研究发现GPTQ与Babai算法在数学上等价，这使得GPTQ的误差传播步骤获得了直观的几何解释，并且在无裁剪条件下，GPTQ继承了Babai算法的误差上界。

Conclusion: 这些结果为GPTQ提供了坚实的理论基础，并为未来将格算法的进展应用于十亿参数模型的量化算法设计开辟了道路。

Abstract: Quantizing the weights of large language models (LLMs) from 16-bit to lower
bitwidth is the de facto approach to deploy massive transformers onto more
affordable accelerators. GPTQ emerged as one of the standard methods for
one-shot post-training quantization at LLM scale. Yet, its inner workings are
described as a sequence of ad-hoc algebraic updates that obscure any geometric
meaning or worst-case guarantees. In this work, we show that, when executed
back-to-front (from the last to first dimension) for a linear layer, GPTQ is
mathematically identical to Babai's nearest plane algorithm for the classical
closest vector problem (CVP) on a lattice defined by the Hessian matrix of the
layer's inputs. This equivalence is based on a sophisticated mathematical
argument, and has two analytical consequences: (i) the GPTQ error propagation
step gains an intuitive geometric interpretation; (ii) GPTQ inherits the error
upper bound of Babai's algorithm under the no-clipping condition. Taken
together, these results place GPTQ on firm theoretical footing and open the
door to importing decades of progress in lattice algorithms towards the design
of future quantization algorithms for billion-parameter models.

</details>


### [202] [Neural Tangent Kernels and Fisher Information Matrices for Simple ReLU Networks with Random Hidden Weights](https://arxiv.org/abs/2507.18555)
*Jun'ichi Takeuchia,Yoshinari Takeishia,Noboru Muratab,Kazushi Mimurac,Ka Long Keith Hod,Hiroshi Nagaoka*

Main category: cs.LG

TL;DR: 本文分析了双层ReLU网络中Fisher信息矩阵与神经正切核（NTK）的关系，揭示其线性变换性质，并给出了NTK的谱分解和函数近似公式。


<details>
  <summary>Details</summary>
Motivation: 旨在深入理解具有随机隐层权重的双层ReLU神经网络的理论性质，特别是探讨Fisher信息矩阵和神经正切核（NTK）之间的内在联系。

Method: 通过理论分析和数学推导，讨论Fisher信息矩阵与NTK之间的线性变换关系，并进行NTK的谱分解。

Result: 发现Fisher信息矩阵和NTK之间存在线性变换关系；给出了NTK具体的谱分解形式，包括主要特征值对应的特征函数；获得了双层神经网络所表示函数的近似公式。

Conclusion: 本研究通过揭示Fisher信息矩阵和NTK的理论联系，提供NTK的谱分解和函数近似公式，深化了对双层ReLU神经网络理论行为的理解。

Abstract: Fisher information matrices and neural tangent kernels (NTK) for 2-layer ReLU
networks with random hidden weight are argued. We discuss the relation between
both notions as a linear transformation and show that spectral decomposition of
NTK with concrete forms of eigenfunctions with major eigenvalues. We also
obtain an approximation formula of the functions presented by the 2-layer
neural networks.

</details>


### [203] [Beyond Internal Data: Constructing Complete Datasets for Fairness Testing](https://arxiv.org/abs/2507.18561)
*Varsha Ramineni,Hossein A. Rahmani,Emine Yilmaz,David Barber*

Main category: cs.LG

TL;DR: 为解决AI公平性测试中人口统计数据稀缺问题，本文提出利用重叠数据集构建合成数据以实现可靠的公平性评估。


<details>
  <summary>Details</summary>
Motivation: 随着AI在风险领域和决策中的普及，测试其潜在危害和偏见（特别是公平性）至关重要。尽管AI法规强调公平性和独立审计，但在工业环境中，法律和隐私限制导致难以获取用于评估群体差异的人口统计数据，且现有历史数据集代表性不足。

Method: 提出利用多个独立的重叠数据集，构建包含人口统计信息的完整合成数据。这种合成数据能够准确反映受保护属性与模型特征之间的潜在关系。

Result: 验证了所生成合成数据的保真度与真实数据一致，并经验证明了基于合成数据得出的公平性指标与真实数据获得的结果保持一致。

Conclusion: 该研究为克服公平性测试中真实数据稀缺提供了解决方案，使得独立、模型无关的公平性评估成为可能，并在真实数据有限的情况下提供了一个可行的替代方法。

Abstract: As AI becomes prevalent in high-risk domains and decision-making, it is
essential to test for potential harms and biases. This urgency is reflected by
the global emergence of AI regulations that emphasise fairness and adequate
testing, with some mandating independent bias audits. However, procuring the
necessary data for fairness testing remains a significant challenge.
Particularly in industry settings, legal and privacy concerns restrict the
collection of demographic data required to assess group disparities, and
auditors face practical and cultural challenges in gaining access to data.
Further, internal historical datasets are often insufficiently representative
to identify real-world biases. This work focuses on evaluating classifier
fairness when complete datasets including demographics are inaccessible. We
propose leveraging separate overlapping datasets to construct complete
synthetic data that includes demographic information and accurately reflects
the underlying relationships between protected attributes and model features.
We validate the fidelity of the synthetic data by comparing it to real data,
and empirically demonstrate that fairness metrics derived from testing on such
synthetic data are consistent with those obtained from real data. This work,
therefore, offers a path to overcome real-world data scarcity for fairness
testing, enabling independent, model-agnostic evaluation of fairness, and
serving as a viable substitute where real data is limited.

</details>


### [204] [Linear Memory SE(2) Invariant Attention](https://arxiv.org/abs/2507.18597)
*Ethan Pronovost,Neha Boloor,Peter Schleede,Noureldin Hendy,Andres Morales,Nicholas Roy*

Main category: cs.LG

TL;DR: 针对自动驾驶空间数据处理中现有SE(2)不变方法的内存瓶颈，本文提出一种内存复杂度为线性的SE(2)不变Transformer架构，并验证其性能优越性。


<details>
  <summary>Details</summary>
Motivation: 现有用于自动驾驶空间数据处理的SE(2)不变网络架构，通过显式计算所有对象对的相对姿态，需要二次方的内存，这限制了其在处理大量对象时的可扩展性。

Method: 本文提出了一种SE(2)不变的缩放点积注意力机制，该机制对场景中的对象数量仅需要线性内存。在此基础上构建了SE(2)不变的Transformer架构。

Result: 实验证明，所提出的方法易于实现，并且与同类非不变性架构相比，性能有所提升。

Conclusion: 本文成功开发了一种内存效率更高（线性复杂度）的SE(2)不变Transformer架构，适用于自动驾驶中的空间数据处理，并验证了其有效性和实用性。

Abstract: Processing spatial data is a key component in many learning tasks for
autonomous driving such as motion forecasting, multi-agent simulation, and
planning. Prior works have demonstrated the value in using SE(2) invariant
network architectures that consider only the relative poses between objects
(e.g. other agents, scene features such as traffic lanes). However, these
methods compute the relative poses for all pairs of objects explicitly,
requiring quadratic memory. In this work, we propose a mechanism for SE(2)
invariant scaled dot-product attention that requires linear memory relative to
the number of objects in the scene. Our SE(2) invariant transformer
architecture enjoys the same scaling properties that have benefited large
language models in recent years. We demonstrate experimentally that our
approach is practical to implement and improves performance compared to
comparable non-invariant architectures.

</details>


### [205] [Demystify Protein Generation with Hierarchical Conditional Diffusion Models](https://arxiv.org/abs/2507.18603)
*Zinan Ling,Yi Shi,Da Yan,Yang Zhou,Bo Hui*

Main category: cs.LG

TL;DR: 提出一种多层次条件扩散模型用于功能性蛋白质设计，并引入了新的评估指标Protein-MMD。


<details>
  <summary>Details</summary>
Motivation: 尽管条件扩散模型在蛋白质生成中表现出色，但在从头设计和确保生成蛋白质的可靠性方面仍面临挑战，特别是在考虑蛋白质功能由多层次结构决定的情况下。

Method: 提出了一种新颖的多层次条件扩散模型，该模型整合了序列和结构信息，用于高效的端到端蛋白质设计，并能同时生成不同层次的表示。此外，还引入了Protein-MMD作为新的可靠评估指标，用于衡量生成蛋白质的质量，该指标能捕捉分布和功能相似性并确保条件一致性。

Result: 在基准数据集上的实验结果表明，所提出的生成框架和评估指标在条件蛋白质生成任务中均表现出有效性。

Conclusion: 该研究提供了一种有效的方法来生成具有指定功能的新型蛋白质，并通过新的评估指标提高了条件扩散模型的可靠性评估。

Abstract: Generating novel and functional protein sequences is critical to a wide range
of applications in biology. Recent advancements in conditional diffusion models
have shown impressive empirical performance in protein generation tasks.
However, reliable generations of protein remain an open research question in de
novo protein design, especially when it comes to conditional diffusion models.
Considering the biological function of a protein is determined by multi-level
structures, we propose a novel multi-level conditional diffusion model that
integrates both sequence-based and structure-based information for efficient
end-to-end protein design guided by specified functions. By generating
representations at different levels simultaneously, our framework can
effectively model the inherent hierarchical relations between different levels,
resulting in an informative and discriminative representation of the generated
protein. We also propose a Protein-MMD, a new reliable evaluation metric, to
evaluate the quality of generated protein with conditional diffusion models.
Our new metric is able to capture both distributional and functional
similarities between real and generated protein sequences while ensuring
conditional consistency. We experiment with the benchmark datasets, and the
results on conditional protein generation tasks demonstrate the efficacy of the
proposed generation framework and evaluation metric.

</details>


### [206] [Moving Out: Physically-grounded Human-AI Collaboration](https://arxiv.org/abs/2507.18623)
*Xuhui Kang,Sung-Wook Lee,Haolin Liu,Yuyan Wang,Yen-Ling Kuo*

Main category: cs.LG

TL;DR: 本文提出了一个物理人机协作新基准“Moving Out”和一种名为BASS的新方法，旨在提升AI在复杂物理环境中与人类有效协作的能力，并实验证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 具身智能体（如机器人）在物理环境中适应行动和约束并与人类有效协作至关重要。然而，物理约束导致连续状态-动作空间的复杂性增加，对现有的协作模型提出了巨大挑战。

Method: 1. 引入“Moving Out”基准，模拟多种受物理属性和约束影响的协作模式（如共同搬运重物、协作绕过障碍物）。2. 基于该基准设计了两项任务并收集了人机交互数据，以评估模型适应多样人类行为和未知物理属性的能力。3. 提出BASS（行为增强、模拟和选择）方法，旨在增强智能体的行为多样性及其对行动结果的理解。

Result: 实验结果表明，BASS在AI-AI和人机协作任务中均优于现有的先进模型。

Conclusion: 通过引入新的物理协作基准“Moving Out”和提出的BASS方法，本文有效提升了AI在复杂物理环境中与人类协作的性能和适应能力。

Abstract: The ability to adapt to physical actions and constraints in an environment is
crucial for embodied agents (e.g., robots) to effectively collaborate with
humans. Such physically grounded human-AI collaboration must account for the
increased complexity of the continuous state-action space and constrained
dynamics caused by physical constraints. In this paper, we introduce
\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a
wide range of collaboration modes affected by physical attributes and
constraints, such as moving heavy items together and maintaining consistent
actions to move a big item around a corner. Using Moving Out, we designed two
tasks and collected human-human interaction data to evaluate models' abilities
to adapt to diverse human behaviors and unseen physical attributes. To address
the challenges in physical environments, we propose a novel method, BASS
(Behavior Augmentation, Simulation, and Selection), to enhance the diversity of
agents and their understanding of the outcome of actions. Our experiments show
that BASS outperforms state-of-the-art models in AI-AI and human-AI
collaboration. The project page is available at
\href{https://live-robotics-uva.github.io/movingout_ai/}{https://live-robotics-uva.github.io/movingout\_ai/}.

</details>


### [207] [Gait Recognition Based on Tiny ML and IMU Sensors](https://arxiv.org/abs/2507.18627)
*Jiahang Zhang,Mingtong Chen,Zhengbao Yang*

Main category: cs.LG

TL;DR: 该项目开发了一个基于TinyML和IMU传感器的步态识别系统，可在微控制器上实时分类四种活动，准确率超过80%，并支持异常检测，具有低功耗特性。


<details>
  <summary>Details</summary>
Motivation: 开发一个低功耗的步态识别系统，适用于电池供电或能量收集设备，以实现实时活动分类和异常检测。

Method: 利用XIAO-nRF52840 Sense微控制器和LSM6DS3 IMU传感器采集行走、静止、上楼、下楼四种活动的运动数据。通过Edge Impulse平台进行数据预处理（滑动窗口、数据归一化）和深度神经网络（DNN）模型训练，最终将模型部署到微控制器上进行实时分类。

Result: 所开发的模型在测试数据集上实现了超过80%的活动分类准确率，有效识别四种活动。该平台还支持异常检测功能，增强了系统的鲁棒性。

Conclusion: 成功构建了一个基于TinyML和IMU的实时步态识别系统，该系统具有高准确度、低功耗，并兼具活动分类和异常检测能力，适用于能源受限的应用场景。

Abstract: This project presents the development of a gait recognition system using Tiny
Machine Learning (Tiny ML) and Inertial Measurement Unit (IMU) sensors. The
system leverages the XIAO-nRF52840 Sense microcontroller and the LSM6DS3 IMU
sensor to capture motion data, including acceleration and angular velocity,
from four distinct activities: walking, stationary, going upstairs, and going
downstairs. The data collected is processed through Edge Impulse, an edge AI
platform, which enables the training of machine learning models that can be
deployed directly onto the microcontroller for real-time activity
classification.The data preprocessing step involves extracting relevant
features from the raw sensor data using techniques such as sliding windows and
data normalization, followed by training a Deep Neural Network (DNN) classifier
for activity recognition. The model achieves over 80% accuracy on a test
dataset, demonstrating its ability to classify the four activities effectively.
Additionally, the platform enables anomaly detection, further enhancing the
robustness of the system. The integration of Tiny ML ensures low-power
operation, making it suitable for battery-powered or energy-harvesting devices.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [208] [Frame-Based Zero-Shot Semantic Channel Equalization for AI-Native Communications](https://arxiv.org/abs/2507.17835)
*Simone Fiorellino,Claudio Battiloro,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.NI

TL;DR: 针对AI原生无线网络中DNN编码器潜在空间不匹配导致的语义信道噪声问题，本文提出Parseval Frame Equalizer (PFE) 进行零样本对齐，并引入动态优化策略以平衡资源，提升多智能体语义通信性能。


<details>
  <summary>Details</summary>
Motivation: 在AI原生无线网络中，独立设计的深度神经网络 (DNN) 编码器之间潜在空间的不匹配会产生语义信道噪声，这损害了接收端对传输表示的解释能力，从而降低了系统整体性能。

Method: 提出Parseval Frame Equalizer (PFE)，这是一种零样本、基于帧的语义信道均衡器，用于无需重训练地对齐异构编码器的潜在空间，并实现动态信号压缩与扩展以减轻语义噪声。在此基础上，引入一种动态优化策略，协调通信、计算和学习资源，以平衡多智能体语义通信场景中的能耗、端到端 (E2E) 延迟和任务性能。

Result: 广泛的仿真结果证实，所提出的方法在保持语义一致性以及在多样化和时变的网络条件下满足长期延迟和精度约束方面是有效的。

Conclusion: PFE及其动态优化策略能有效解决AI原生无线网络中的语义信道噪声问题，通过智能资源管理，显著提升多智能体语义通信的性能，同时优化能耗、延迟和任务精度之间的平衡。

Abstract: In future AI-native wireless networks, the presence of mismatches between the
latent spaces of independently designed and trained deep neural network (DNN)
encoders may impede mutual understanding due to the emergence of semantic
channel noise. This undermines the receiver's ability to interpret transmitted
representations, thereby reducing overall system performance. To address this
issue, we propose the Parseval Frame Equalizer (PFE), a zero-shot, frame-based
semantic channel equalizer that aligns latent spaces of heterogeneous encoders
without requiring system retraining. PFE enables dynamic signal compression and
expansion, mitigating semantic noise while preserving performance on downstream
tasks. Building on this capability, we introduce a dynamic optimization
strategy that coordinates communication, computation, and learning resources to
balance energy consumption, end-to-end (E2E) latency, and task performance in
multi-agent semantic communication scenarios. Extensive simulations confirm the
effectiveness of our approach in maintaining semantic consistency and meeting
long-term constraints on latency and accuracy under diverse and time-varying
network conditions.

</details>


### [209] [ARCADE: A RAN Diagnosis Methodology in a Hybrid AI Environment for 6G Networks](https://arxiv.org/abs/2507.17861)
*Daniel Ricardo Cunha Oliveira,Rodrigo Moreira,Flávio de Oliveira Silva*

Main category: cs.NI

TL;DR: 论文提出ARCADE方法以检测蜂窝接入网异常，并探讨其在6G混合网络分析架构中对AI应用的促进作用。


<details>
  <summary>Details</summary>
Motivation: 现有5G的NWDAF不足以满足6G网络中未充分探索部分的全面自动化需求，因此需要一种更全面的方法来加强AI在网络中的应用。

Method: 提出并展示了ARCADE（自动化无线覆盖异常检测与评估）方法，用于识别和诊断蜂窝接入网络中的异常。同时，探讨了在向6G演进过程中，网络分析功能的混合架构如何增强AI应用。

Result: ARCADE作为一个实际案例，展示了在向6G演进过程中，通过网络分析功能的混合架构，如何显著提升AI在更广阔网络环境中的应用能力。

Conclusion: ARCADE为蜂窝接入网的异常检测提供了实用方案，并揭示了混合网络分析架构在促进AI应用于未来6G网络自动化方面的巨大潜力。

Abstract: Artificial Intelligence (AI) plays a key role in developing 6G networks.
While current specifications already include Network Data Analytics Function
(NWDAF) as a network element responsible for providing information about the
core, a more comprehensive approach will be needed to enable automation of
network segments that are not yet fully explored in the context of 5G. In this
paper, we present Automated Radio Coverage Anomalies Detection and Evaluation
(ARCADE), a methodology for identifying and diagnosing anomalies in the
cellular access network. Furthermore, we demonstrate how a hybrid architecture
of network analytics functions in the evolution toward 6G can enhance the
application of AI in a broader network context, using ARCADE as a practical
example of this approach.

</details>


### [210] [Talk with the Things: Integrating LLMs into IoT Networks](https://arxiv.org/abs/2507.17865)
*Alakesh Kalita*

Main category: cs.NI

TL;DR: 本文提出一个将大语言模型（LLMs）集成到物联网（IoT）的边缘中心框架，通过在边缘设备部署基于检索增强生成（RAG）的LLMs实现自然语言控制和智能自动化。在智能家居原型中验证，结果揭示了模型大小、准确性和推理时间之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型（LLMs）与物联网（IoT）的融合，构建智能、响应迅速且用户友好的系统，以实现基于自然语言的控制、情境感知决策和增强自动化，并通过边缘部署解决延迟、隐私和推理质量问题。

Method: 提出一个边缘中心框架，将模块化、轻量级的检索增强生成（RAG）LLMs集成到IoT架构中。这些LLMs部署在连接IoT网关的边缘计算设备上，实现用户命令和传感器数据的本地处理。通过使用LLaMA 3和Gemma 2B模型的智能家居原型验证框架的有效性。

Result: 实验结果表明，模型精度和推理时间与模型大小之间存在权衡关系。

Conclusion: 所提出的边缘中心LLM-IoT框架能够实现自然语言控制和智能自动化，并具有降低延迟、提高隐私等优势。研究不仅验证了框架的可行性，还讨论了此类系统的潜在应用和面临的关键挑战。

Abstract: The convergence of Large Language Models (LLMs) and Internet of Things (IoT)
networks open new opportunities for building intelligent, responsive, and
user-friendly systems. This work presents an edge-centric framework that
integrates LLMs into IoT architectures to enable natural language-based
control, context-aware decision-making, and enhanced automation. The proposed
modular and lightweight Retrieval Augmented Generation (RAG)-based LLMs are
deployed on edge computing devices connected to IoT gateways, enabling local
processing of user commands and sensor data for reduced latency, improved
privacy, and enhanced inference quality. We validate the framework through a
smart home prototype using LLaMA 3 and Gemma 2B models for controlling smart
devices. Experimental results highlight the trade-offs between model accuracy
and inference time with respect to models size. At last, we also discuss the
potential applications that can use LLM-based IoT systems, and a few key
challenges associated with such systems.

</details>


### [211] [Enabling Scalability in Asynchronous and Bidirectional Communication in LPWAN](https://arxiv.org/abs/2507.17905)
*Mahbubur Rahman*

Main category: cs.NI

TL;DR: 本文通过改进SNOW技术，利用基于Gold码的PN序列，实现了LPWANs中同一子载波上多传感器的并发通信，将可扩展性提升约9倍，以满足新兴IoT和CPS应用的需求。


<details>
  <summary>Details</summary>
Motivation: LPWANs在广域传感器连接方面已普及，但其在大规模并发数据传输（高效率、低延迟）方面的能力不足，难以满足新兴IoT和CPS应用的需求。

Method: 本文通过显著改进SNOW（一种LPWAN技术），使其能够：1) 允许基站在同一子载波上解码来自多个异步传感器的并发数据，同时并行解码其他子载波上的数据。2) 允许多个异步传感器在同一子载波上接收基站发送的不同数据。为实现此目的，开发了一套基于Gold码的伪随机噪声（PN）序列，这些序列在同一子载波内部及跨子载波之间互不干扰，每个传感器使用其独特的PN序列进行数据的编解码，从而实现大规模并发。

Result: 评估结果表明，SNOW的可扩展性提升了约9倍，同时在基站端实现了及时的数据收集，并在传感器端保持了能源效率。

Conclusion: 该研究成果有望赋能需要数万个传感器、更长电池寿命以及能够进行数据驱动、时间敏感决策的新兴IoT和CPS应用。

Abstract: LPWANs have become ubiquitous due to their ability to connect sensors over
large geographic areas in a single hop. It is, however, very challenging to
achieve massive scalability in LPWANs, where numerous sensors can transmit data
efficiently and with low latency, which emerging IoT and CPS applications may
require. In this paper, we address the above challenges by significantly
advancing an LPWAN technology called SNOW. SNOW exploits distributed orthogonal
frequency division multiplexing, D-OFDM, subcarriers to enable parallel
reception of data to a BS from multiple asynchronous sensors, each using a
different subcarrier. In this paper, we achieve massive scalability in SNOW by
enabling the BS to decode concurrent data from numerous asynchronous sensors on
the same subcarrier while parallelly decoding from other subcarriers as well.
Additionally, we enable numerous asynchronous sensors to receive distinct data
from the BS on the same subcarrier while other sensors also receive data
parallelly on other subcarriers. To do this, we develop a set of Gold
code-based pseudorandom noise or PN sequences that are mutually non-interfering
within and across the subcarriers. Each sensor uses its PN sequence from the
set for encoding or decoding data on its subcarriers, enabling massive
concurrency. Our evaluation results demonstrate that we can achieve
approximately 9x more scalability in SNOW while being timely in data collection
at the BS and energy efficient at the sensors. This may enable emerging IoT and
CPS applications requiring tens of thousands of sensors with longer battery
life and making data-driven, time-sensitive decisions.

</details>


### [212] [Enhanced Velocity-Adaptive Scheme: Joint Fair Access and Age of Information Optimization in Vehicular Networks](https://arxiv.org/abs/2507.18328)
*Xiao Xu,Qiong Wu,Pingyi Fan,Kezhi Wang,Nan Cheng,Wen Chen,Khaled B. Letaief*

Main category: cs.NI

TL;DR: 针对5G V2I Mode 2车辆网络中的公平接入与信息年龄(AoI)问题，本文提出一个联合优化框架，通过自适应调整SPS选择窗口并应用基于LLM的MOEA/D算法，有效平衡公平接入和最小化AoI。


<details>
  <summary>Details</summary>
Motivation: 在5G V2I Mode 2车辆网络中，车辆速度差异导致RSU停留时间不同，引发资源接入不公平并影响驾驶安全；同时，为保证数据新鲜度，需优化信息年龄(AoI)。Mode 2引入的抢占机制使得公平接入与AoI的联合优化成为关键挑战。

Method: 本文提出一个公平接入与AoI的联合优化框架，通过定义公平性指标、使用随机混合系统(SHS)建模抢占机制下的AoI，并自适应调整Mode 2中半持久调度(SPS)的选择窗口。该优化问题通过基于大型语言模型(LLM)的多目标演化算法(MOEA/D)求解。

Result: 仿真结果验证了所提方案在平衡公平接入和最小化信息年龄(AoI)方面的有效性。

Conclusion: 研究表明，本文提出的联合优化方案能有效解决5G V2I Mode 2车辆网络中公平接入与AoI的平衡问题，为车辆通信提供了及时且相关的数​​据保障。

Abstract: In this paper, we consider the fair access problem and the Age of Information
(AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in
vehicular networks. Specifically, vehicles follow Mode 2 to communicate with
Roadside Units (RSUs) to obtain accurate data for driving
assistance.Nevertheless, vehicles often have different velocity when they are
moving in adjacent lanes, leading to difference in RSU dwelltime and
communication duration. This results in unfair access to network resources,
potentially influencing driving safety. To ensure the freshness of received
data, the AoI should be analyzed. Mode 2 introduces a novel preemption
mechanism, necessitating simultaneous optimization of fair access and AoI to
guarantee timely and relevant data delivery. We propose a joint optimization
framework for vehicular network, defining a fairness index and employing
Stochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By
adaptively adjusting the selection window of Semi-Persistent Scheduling (SPS)
in Mode 2, we address the optimization of fairness and AoI. We apply a large
language model (LLM)-Based Multi-objective Evolutionary Algorithm Based on
Decomposition (MOEA/D) to solve this problem. Simulation results demonstrate
the effectiveness of our scheme in balancing fair access and minimizing AoI.

</details>


### [213] [Improving Wi-Fi 8 Latency with Coordinated Spatial Reuse](https://arxiv.org/abs/2507.18480)
*David Nunez,Francesc Wilhelmi,Lorenzo Galati-Giordano,Giovanni Geraci,Boris Bellalta*

Main category: cs.NI

TL;DR: 本文评估了Wi-Fi 8网络中Coordinated Spatial Reuse (Co-SR)的性能，通过仿真展示了其在降低延迟方面的显著效果。


<details>
  <summary>Details</summary>
Motivation: 为满足云游戏、扩展现实(XR)和视频流等新兴应用对Wi-Fi网络高吞吐、低延迟和高可靠性的严苛要求，亟需优化频谱资源利用。

Method: 本文提出了一种与Wi-Fi 8标准化工作保持一致的Co-SR实现方案，并使用Wi-Fi模拟器在相关场景中对其性能进行评估。

Result: 在包含四个AP的无线局域网(WLAN)中，与分布式协调功能(DCF)相比，Co-SR将延迟降低了31%至95%。

Conclusion: Co-SR能有效优化频谱资源利用，实现同时传输，从而显著提高Wi-Fi 8在密集环境中的频谱效率和网络性能，尤其在降低延迟方面表现突出。

Abstract: IEEE 802.11 networks continuously adapt to meet the stringent requirements of
emerging applications like cloud gaming, eXtended Reality (XR), and video
streaming services, which require high throughput, low latency, and high
reliability. To address these challenges, Coordinated Spatial Reuse (Co-SR) can
potentially contribute to optimizing spectrum resource utilization. This
mechanism is expected to enable simultaneous transmissions, thereby boosting
spectral efficiency in dense environments and increasing the overall network
performance. In this paper, we shed light on the performance of Co-SR for Wi-Fi
8 networks. For that, we propose an implementation of Co-SR aligned with
ongoing Wi-Fi 8 standardization efforts. The evaluation is done on a Wi-Fi
simulator, which allows us to study the performance of the proposed Co-SR
mechanisms in relevant scenarios. The results obtained in a Wireless Local Area
Network (WLAN) consisting of four APs show delay reduction with Co-SR ranging
from 31% to 95% when compared to Distributed Coordination Function (DCF).

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [214] [Towards Robust Foundation Models for Digital Pathology](https://arxiv.org/abs/2507.17845)
*Jonah Kömen,Edwin D. de Jong,Julius Hense,Hannah Marienwald,Jonas Dippel,Philip Naumann,Eric Marcus,Lukas Ruff,Maximilian Alber,Jonas Teuwen,Frederick Klauschen,Klaus-Robert Müller*

Main category: eess.IV

TL;DR: 本研究首次系统性调查了病理学基础模型（FM）对非生物技术特征的鲁棒性，开发了PathoROB基准来量化和评估鲁棒性，发现现有模型普遍存在鲁棒性不足，并提出鲁棒化策略，强调鲁棒性评估和整合对于FM安全临床部署的必要性。


<details>
  <summary>Details</summary>
Motivation: 生物医学基础模型正在改变医疗健康研究并进入临床验证，但其易受非生物技术特征（如手术技术、实验室程序、扫描硬件差异）影响，这给临床部署带来了风险。

Method: 本研究通过以下方式进行：(i) 引入量化FM鲁棒性的新度量，(ii) 演示有限鲁棒性的后果，(iii) 提出FM鲁棒化框架。具体地，开发了PathoROB鲁棒性基准，包含鲁棒性指数等三种新颖指标，并构建了涵盖来自34个医疗中心的28个生物学类别的四个数据集。实验评估了20个不同的FM。

Result: 实验结果显示，所有20个评估的FM都存在鲁棒性缺陷，且它们之间存在显著的鲁棒性差异。研究发现，缺乏鲁棒性的FM表征会导致严重的诊断下游错误和临床失误，从而阻碍其安全临床应用。使用更鲁棒的FM和进行事后鲁棒化可以显著减少（但尚未完全消除）此类错误的风险。

Conclusion: 本研究确立了鲁棒性评估在病理学FM临床应用前验证中的重要性，并指出未来的FM开发必须将鲁棒性作为核心设计原则进行整合。PathoROB为生物医学领域的鲁棒性评估提供了蓝图，指导FM改进以开发出更鲁棒、更具代表性、更适合临床部署且优先处理生物信息而非技术伪影的AI系统。

Abstract: Biomedical Foundation Models (FMs) are rapidly transforming AI-enabled
healthcare research and entering clinical validation. However, their
susceptibility to learning non-biological technical features -- including
variations in surgical/endoscopic techniques, laboratory procedures, and
scanner hardware -- poses risks for clinical deployment. We present the first
systematic investigation of pathology FM robustness to non-biological features.
Our work (i) introduces measures to quantify FM robustness, (ii) demonstrates
the consequences of limited robustness, and (iii) proposes a framework for FM
robustification to mitigate these issues. Specifically, we developed PathoROB,
a robustness benchmark with three novel metrics, including the robustness
index, and four datasets covering 28 biological classes from 34 medical
centers. Our experiments reveal robustness deficits across all 20 evaluated
FMs, and substantial robustness differences between them. We found that
non-robust FM representations can cause major diagnostic downstream errors and
clinical blunders that prevent safe clinical adoption. Using more robust FMs
and post-hoc robustification considerably reduced (but did not yet eliminate)
the risk of such errors. This work establishes that robustness evaluation is
essential for validating pathology FMs before clinical adoption and
demonstrates that future FM development must integrate robustness as a core
design principle. PathoROB provides a blueprint for assessing robustness across
biomedical domains, guiding FM improvement efforts towards more robust,
representative, and clinically deployable AI systems that prioritize biological
information over technical artifacts.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [215] [Performance Evaluation and Threat Mitigation in Large-scale 5G Core Deployment](https://arxiv.org/abs/2507.17850)
*Rodrigo Moreira,Larissa F. Rodrigues Moreira,Flávio de Oliveira Silva*

Main category: cs.CR

TL;DR: 分析DDoS对5G核心网络功能及用户设备注册性能的影响，并提出资源配置和基于内核的监控方案以应对复杂部署挑战。


<details>
  <summary>Details</summary>
Motivation: 大规模软件定义5G核心功能部署面临资源优化和智能配置的挑战，尤其是在混沌工作负载（如DDoS攻击）下，如何确保服务质量和用户设备注册性能，是亟待解决的问题。

Method: 本研究通过阐明DDoS生成的混沌工作负载对不同网络功能（NFs）及用户设备（UE）注册性能的影响进行研究。此外，分析了数据包捕获方法，并进行了实证评估。

Result: 研究结果表明，为确保大规模5G核心部署中的服务级别协议（SLA）合规性，需要多样化的资源配置。同时，基于内核的监控在可伸缩的安全威胁防御中展现出潜力。

Conclusion: 本研究为复杂场景下5G网络功能的有效部署提供了深入见解，强调了适当的资源配置和先进的安全监控对于保障5G服务性能和安全的重要性。

Abstract: The deployment of large-scale software-based 5G core functions presents
significant challenges due to their reliance on optimized and intelligent
resource provisioning for their services. Many studies have focused on
analyzing the impact of resource allocation for complex deployments using
mathematical models, queue theories, or even Artificial Intelligence (AI). This
paper elucidates the effects of chaotic workloads, generated by Distributed
Denial of Service (DDoS) on different Network Functions (NFs) on User Equipment
registration performance. Our findings highlight the necessity of diverse
resource profiles to ensure Service-Level Agreement (SLA) compliance in
large-scale 5G core deployments. Additionally, our analysis of packet capture
approaches demonstrates the potential of kernel-based monitoring for scalable
security threat defense. Finally, our empirical evaluation provides insights
into the effective deployment of 5G NFs in complex scenarios.

</details>


### [216] [RECALLED: An Unbounded Resource Consumption Attack on Large Vision-Language Models](https://arxiv.org/abs/2507.18053)
*Haoran Gao,Yuanhe Zhang,Zhenhong Zhou,Lei Jiang,Fanyu Meng,Yujia Xiao,Kun Wang,Yang Liu,Junlan Feng*

Main category: cs.CR

TL;DR: 本文提出了RECALLED，一种针对大型视觉语言模型(LVLMs)的资源消耗攻击(RCAs)方法。该方法通过视觉输入中的对抗性扰动，诱导模型产生重复输出，显著增加服务延迟和资源消耗，揭示了LVLMs在视觉模态下的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 资源消耗攻击(RCAs)对大型语言模型(LLMs)构成重大威胁，而视觉模态的整合进一步加剧了大型视觉语言模型(LVLMs)中RCAs的风险。然而，现有针对LVLMs的红队研究普遍忽视了视觉输入作为潜在攻击面，导致针对RCAs的缓解策略不足。

Method: 研究提出了RECALLED方法，首次利用视觉模态触发无界RCAs红队攻击。具体包括：1) 提出“视觉引导优化”进行细粒度像素级优化，以获得能诱导重复输出的“输出召回”对抗性扰动。2) 将扰动注入视觉输入，触发无界生成以实现RCAs。3) 引入“多目标并行损失”以生成通用攻击模板，并解决并行攻击时的优化冲突。

Result: 实验结果表明，RECALLED能将服务响应延迟提高26%以上，并额外增加GPU利用率和内存消耗20%。

Conclusion: 本研究揭示了LVLMs的安全漏洞，并建立了一个红队框架，有助于未来开发针对RCAs的防御措施。

Abstract: Resource Consumption Attacks (RCAs) have emerged as a significant threat to
the deployment of Large Language Models (LLMs). With the integration of vision
modalities, additional attack vectors exacerbate the risk of RCAs in large
vision-language models (LVLMs). However, existing red-teaming studies have
largely overlooked visual inputs as a potential attack surface, resulting in
insufficient mitigation strategies against RCAs in LVLMs. To address this gap,
we propose RECALLED (\textbf{RE}source \textbf{C}onsumption \textbf{A}ttack on
\textbf{L}arge Vision-\textbf{L}anguag\textbf{E} Mo\textbf{D}els), the first
approach for exploiting visual modalities to trigger unbounded RCAs
red-teaming. First, we present \textit{Vision Guided Optimization}, a
fine-grained pixel-level optimization, to obtain \textit{Output Recall}
adversarial perturbations, which can induce repeating output. Then, we inject
the perturbations into visual inputs, triggering unbounded generations to
achieve the goal of RCAs. Additionally, we introduce \textit{Multi-Objective
Parallel Losses} to generate universal attack templates and resolve
optimization conflicts when intending to implement parallel attacks. Empirical
results demonstrate that RECALLED increases service response latency by over 26
$\uparrow$, resulting in an additional 20\% increase in GPU utilization and
memory consumption. Our study exposes security vulnerabilities in LVLMs and
establishes a red-teaming framework that can facilitate future defense
development against RCAs.

</details>


### [217] [LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models](https://arxiv.org/abs/2507.18302)
*Delong Ran,Xinlei He,Tianshuo Cong,Anyu Wang,Qi Li,Xiaoyun Wang*

Main category: cs.CR

TL;DR: LoRA微调的语言模型容易受到成员推理攻击（MIA），因为预训练模型会泄漏额外信息。本文提出了LoRA-Leak评估框架，包含15种MIA（5种改进型），实验证实LoRA模型在预训练模型存在下仍高度脆弱，并发现dropout和排除特定层是有效的防御手段。


<details>
  <summary>Details</summary>
Motivation: LoRA微调在语言模型中广泛应用，其少量参数调整可能导致误解，认为微调数据对成员推理攻击（MIA）免疫。然而，现有MIA忽略了预训练模型可能导致的信息泄露，因此需要一个全面的框架来评估LoRA微调数据集的MIA风险。

Method: 本文提出了LoRA-Leak，一个针对语言模型微调数据集的成员推理攻击（MIA）评估框架。LoRA-Leak整合了15种MIA，包括10种现有MIA和5种利用预训练模型作为参考的改进型MIA。实验将LoRA-Leak应用于三种先进语言模型和三种流行的自然语言处理任务，并探索了不同的微调设置和四种防御策略。

Result: 实验证明，基于LoRA微调的语言模型仍然容易受到MIA攻击（例如，在保守微调设置下AUC高达0.775）。研究还发现，预训练模型的存在显著加剧了LoRA语言模型的MIA风险。在探索的四种防御措施中，只有dropout和在微调过程中排除特定语言模型层能有效缓解MIA风险，同时保持模型效用。

Conclusion: 在“预训练和微调”范式下，预训练模型的存在使得LoRA-based语言模型的成员推理攻击风险更为严重。本研究的结果旨在为专业语言模型提供商的数据隐私保护提供指导。

Abstract: Language Models (LMs) typically adhere to a "pre-training and fine-tuning"
paradigm, where a universal pre-trained model can be fine-tuned to cater to
various specialized domains. Low-Rank Adaptation (LoRA) has gained the most
widespread use in LM fine-tuning due to its lightweight computational cost and
remarkable performance. Because the proportion of parameters tuned by LoRA is
relatively small, there might be a misleading impression that the LoRA
fine-tuning data is invulnerable to Membership Inference Attacks (MIAs).
However, we identify that utilizing the pre-trained model can induce more
information leakage, which is neglected by existing MIAs. Therefore, we
introduce LoRA-Leak, a holistic evaluation framework for MIAs against the
fine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership
inference attacks, including ten existing MIAs, and five improved MIAs that
leverage the pre-trained model as a reference. In experiments, we apply
LoRA-Leak to three advanced LMs across three popular natural language
processing tasks, demonstrating that LoRA-based fine-tuned LMs are still
vulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings).
We also applied LoRA-Leak to different fine-tuning settings to understand the
resulting privacy risks. We further explore four defenses and find that only
dropout and excluding specific LM layers during fine-tuning effectively
mitigate MIA risks while maintaining utility. We highlight that under the
"pre-training and fine-tuning" paradigm, the existence of the pre-trained model
makes MIA a more severe risk for LoRA-based LMs. We hope that our findings can
provide guidance on data privacy protection for specialized LM providers.

</details>


### [218] [MeAJOR Corpus: A Multi-Source Dataset for Phishing Email Detection](https://arxiv.org/abs/2507.17978)
*Paulo Mendes,Eva Maia,Isabel Praça*

Main category: cs.CR

TL;DR: 本文介绍了一个名为MeAJOR的新型多源钓鱼邮件数据集，旨在克服现有资源的局限性，提升机器学习模型在钓鱼邮件检测上的性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在钓鱼邮件检测中的性能高度依赖于训练数据的质量和多样性，而现有数据集存在关键局限性。

Method: 构建了MeAJOR数据集，该数据集整合了135894个来自多源的钓鱼和合法邮件样本，包含广泛的工程特征。通过使用RF、XGB、MLP和CNN四种分类模型，评估了该数据集的效用。

Result: 实验结果表明，该数据集有效提升了钓鱼邮件检测能力，其中XGBoost模型达到了98.34%的F1分数。

Conclusion: MeAJOR数据集通过整合广泛的特征，提供了一个可重用且一致的资源，有效解决了类别不平衡、泛化性和可复现性等常见数据挑战，有助于改进钓鱼检测研究。

Abstract: Phishing emails continue to pose a significant threat to cybersecurity by
exploiting human vulnerabilities through deceptive content and malicious
payloads. While Machine Learning (ML) models are effective at detecting
phishing threats, their performance largely relies on the quality and diversity
of the training data. This paper presents MeAJOR (Merged email Assets from
Joint Open-source Repositories) Corpus, a novel, multi-source phishing email
dataset designed to overcome critical limitations in existing resources. It
integrates 135894 samples representing a broad number of phishing tactics and
legitimate emails, with a wide spectrum of engineered features. We evaluated
the dataset's utility for phishing detection research through systematic
experiments with four classification models (RF, XGB, MLP, and CNN) across
multiple feature configurations. Results highlight the dataset's effectiveness,
achieving 98.34% F1 with XGB. By integrating broad features from multiple
categories, our dataset provides a reusable and consistent resource, while
addressing common challenges like class imbalance, generalisability and
reproducibility.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [219] [Comparison of Optimised Geometric Deep Learning Architectures, over Varying Toxicological Assay Data Environments](https://arxiv.org/abs/2507.17775)
*Alexander D. Kalian,Lennart Otte,Jaewook Lee,Emilio Benfenati,Jean-Lou C. M. Dorne,Claire Potter,Olivia J. Osborne,Miao Guo,Christer Hogstrand*

Main category: q-bio.QM

TL;DR: 本研究比较了GCN、GAT和GIN三种图神经网络在毒理学数据集上的表现，发现GIN适用于数据充足场景，而GAT适用于数据稀缺场景。


<details>
  <summary>Details</summary>
Motivation: 几何深度学习在化学信息学中兴起，但不同图神经网络（GNN）架构的具体适用性在该领域研究不足。

Method: 研究比较了GCNs、GATs和GINs在7个不同数据量毒理学分析数据集上的性能，执行分析激活的二分类任务。方法包括分子图预处理、类别平衡、5折分层，并对每个GNN在每个数据集上进行贝叶斯优化。

Result: 优化后的GNNs平均AUC分数介于0.728-0.849。GINs在数据量最充足的5个数据集中持续优于GCNs和GATs。GATs在数据量最稀缺的2个数据集中表现显著优异。此外，GCNs和GATs的优化超参数状态更接近，与GINs不同。

Conclusion: GINs是数据充足环境下的更优架构，GATs是数据稀缺环境下的更优架构。GINs作为一种GNN算法具有独特的性质。

Abstract: Geometric deep learning is an emerging technique in Artificial Intelligence
(AI) driven cheminformatics, however the unique implications of different Graph
Neural Network (GNN) architectures are poorly explored, for this space. This
study compared performances of Graph Convolutional Networks (GCNs), Graph
Attention Networks (GATs) and Graph Isomorphism Networks (GINs), applied to 7
different toxicological assay datasets of varying data abundance and endpoint,
to perform binary classification of assay activation. Following pre-processing
of molecular graphs, enforcement of class-balance and stratification of all
datasets across 5 folds, Bayesian optimisations were carried out, for each GNN
applied to each assay dataset (resulting in 21 unique Bayesian optimisations).
Optimised GNNs performed at Area Under the Curve (AUC) scores ranging from
0.728-0.849 (averaged across all folds), naturally varying between specific
assays and GNNs. GINs were found to consistently outperform GCNs and GATs, for
the top 5 of 7 most data-abundant toxicological assays. GATs however
significantly outperformed over the remaining 2 most data-scarce assays. This
indicates that GINs are a more optimal architecture for data-abundant
environments, whereas GATs are a more optimal architecture for data-scarce
environments. Subsequent analysis of the explored higher-dimensional
hyperparameter spaces, as well as optimised hyperparameter states, found that
GCNs and GATs reached measurably closer optimised states with each other,
compared to GINs, further indicating the unique nature of GINs as a GNN
algorithm.

</details>


### [220] [CM-UNet: A Self-Supervised Learning-Based Model for Coronary Artery Segmentation in X-Ray Angiography](https://arxiv.org/abs/2507.17779)
*Camille Challier,Xiaowu Sun,Thabo Mahendiran,Ortal Senouf,Bernard De Bruyne,Denise Auberson,Olivier Müller,Stephane Fournier,Pascal Frossard,Emmanuel Abbé,Dorina Thanou*

Main category: q-bio.QM

TL;DR: 本研究提出CM-UNet模型，利用自监督预训练和迁移学习，在有限标注数据下显著提升冠状动脉分割的准确性，以解决临床实践中数据标注不足的挑战。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉的精确分割在临床实践中面临巨大挑战，主要原因在于缺乏用于模型训练的大规模标注数据集，这限制了自动化诊断工具的开发。

Method: 引入CM-UNet模型，该模型首先在无标注数据集上进行自监督预训练，然后在有限的标注数据上进行迁移学习，以实现高精度分割并最大程度减少手动标注的需求。

Result: 使用仅18张标注图像进行微调时，CM-UNet的Dice分数下降15.2%，而没有预训练的基线模型下降46.5%，证明自监督学习能显著提升分割性能并降低对大规模数据集的依赖。

Conclusion: 本研究首次强调了自监督学习在改善X射线血管造影冠状动脉分割中的重要性，有望提升临床诊断准确性，优化工作流程，减轻医生负担，加速疾病检测，从而改善患者预后。

Abstract: Accurate segmentation of coronary arteries remains a significant challenge in
clinical practice, hindering the ability to effectively diagnose and manage
coronary artery disease. The lack of large, annotated datasets for model
training exacerbates this issue, limiting the development of automated tools
that could assist radiologists. To address this, we introduce CM-UNet, which
leverages self-supervised pre-training on unannotated datasets and transfer
learning on limited annotated data, enabling accurate disease detection while
minimizing the need for extensive manual annotations. Fine-tuning CM-UNet with
only 18 annotated images instead of 500 resulted in a 15.2% decrease in Dice
score, compared to a 46.5% drop in baseline models without pre-training. This
demonstrates that self-supervised learning can enhance segmentation performance
and reduce dependence on large datasets. This is one of the first studies to
highlight the importance of self-supervised learning in improving coronary
artery segmentation from X-ray angiography, with potential implications for
advancing diagnostic accuracy in clinical practice. By enhancing segmentation
accuracy in X-ray angiography images, the proposed approach aims to improve
clinical workflows, reduce radiologists' workload, and accelerate disease
detection, ultimately contributing to better patient outcomes. The source code
is publicly available at
https://github.com/CamilleChallier/Contrastive-Masked-UNet.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [221] [Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving](https://arxiv.org/abs/2507.17753)
*Liang Zhang,Xiaoming Zhai,Jionghao Lin,Jionghao Lin,Jennifer Kleiman,Diego Zapata-Rivera,Carol Forsyth,Yang Jiang,Xiangen Hu,Arthur C. Graesser*

Main category: cs.HC

TL;DR: 本研究评估了LLM双智能体在数学问题解决中的四种沟通策略，发现双智能体优于单智能体，其中点对点协作表现最佳，且特定对话行为对协作至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM智能体在AI辅助教育中应用日益广泛，但目前缺乏系统性研究来评估不同沟通策略对其问题解决能力的影响。

Method: 研究在基于OpenAI GPT-4o的双智能体聊天环境中，考察了教师-学生互动、点对点协作、互惠式同伴教学和批判性辩论四种沟通模式在数学问题解决中的表现，并使用MATH数据集进行评估。

Result: 结果显示，双智能体设置优于单智能体，其中点对点协作模式的准确率最高。此外，陈述、确认和提示等对话行为在协作问题解决中发挥关键作用。

Conclusion: 多智能体框架有助于增强计算任务能力，但要有效解决AI教育中的复杂问题，实施有效的沟通策略至关重要。

Abstract: Large Language Model (LLM) agents are increasingly utilized in AI-aided
education to support tutoring and learning. Effective communication strategies
among LLM agents improve collaborative problem-solving efficiency and
facilitate cost-effective adoption in education. However, little research has
systematically evaluated the impact of different communication strategies on
agents' problem-solving. Our study examines four communication modes,
\textit{teacher-student interaction}, \textit{peer-to-peer collaboration},
\textit{reciprocal peer teaching}, and \textit{critical debate}, in a
dual-agent, chat-based mathematical problem-solving environment using the
OpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that
dual-agent setups outperform single agents, with \textit{peer-to-peer
collaboration} achieving the highest accuracy. Dialogue acts like statements,
acknowledgment, and hints play a key role in collaborative problem-solving.
While multi-agent frameworks enhance computational tasks, effective
communication strategies are essential for tackling complex problems in AI
education.

</details>


### [222] [A Custom-Built Ambient Scribe Reduces Cognitive Load and Documentation Burden for Telehealth Clinicians](https://arxiv.org/abs/2507.17754)
*Justin Morse,Kurt Gilbert,Kyle Shin,Rick Cooke,Peyton Rose,Jack Sullivan,Angelo Sisante*

Main category: cs.HC

TL;DR: 开发并部署了一款基于Whisper和GPT-4o的AI医疗抄写应用，旨在减轻医生工作负担，并生成高质量的SOAP笔记和患者指导。


<details>
  <summary>Details</summary>
Motivation: 为应对日益严重的临床医生职业倦怠问题，寻求通过智能医疗抄写技术减轻其行政和文档负担。

Method: 构建了一个定制的、集成到EHR系统的AI环境抄写应用。该应用利用Whisper进行语音转录，并采用GPT-4o和模块化上下文学习流程自动生成SOAP笔记和患者指导。此外，通过微调的BART模型对笔记进行后处理以提高简洁性。

Result: 应用生成的笔记质量优于专家手写笔记（经LLM评估）。该应用已被超过540名临床医生广泛采用，94%的受访者报告认知负荷减轻，97%报告文档负担减轻。同时，BART模型有效提升了笔记的简洁性。

Conclusion: 研究结果表明，AI系统在减轻医护人员行政负担和支持其提供高效、高质量护理方面具有巨大潜力。

Abstract: Clinician burnout has motivated the growing adoption of ambient medical
scribes in the clinic. In this work, we introduce a custom-built ambient scribe
application integrated into the EHR system at Included Health, a personalized
all-in-one healthcare company offering telehealth services. The application
uses Whisper for transcription and a modular in-context learning pipeline with
GPT-4o to automatically generate SOAP notes and patient instructions. Testing
on mock visit data shows that the notes generated by the application exceed the
quality of expert-written notes as determined by an LLM-as-a-judge. The
application has been widely adopted by the clinical practice, with over 540
clinicians at Included Health using the application at least once. 94% (n = 63)
of surveyed clinicians report reduced cognitive load during visits and 97% (n =
66) report less documentation burden when using the application. Additionally,
we show that post-processing notes with a fine-tuned BART model improves
conciseness. These findings highlight the potential for AI systems to ease
administrative burdens and support clinicians in delivering efficient,
high-quality care.

</details>


### [223] [Insights from Railway Professionals: Rethinking Railway assumptions regarding safety and autonomy](https://arxiv.org/abs/2507.17756)
*Josh Hunter,John McDermid,Simon Burton*

Main category: cs.HC

TL;DR: 本研究探究铁路专业人员对安全概念的认知，发现他们对自动化持谨慎态度，倾向辅助技术，并强调安全是人、系统、技术多因素融合的复杂概念，指出汽车自动化技术不宜直接照搬至铁路。


<details>
  <summary>Details</summary>
Motivation: 旨在了解铁路专业人员对安全概念的看法，以指导未来铁路行业的技术发展。

Method: 通过对司机、路线规划员和行政人员进行一系列访谈，探讨了当前安全实践、自动化潜力以及对铁路作为“系统之系统”的理解。

Result: 主要发现包括受访者对自动化持谨慎态度，偏好辅助技术，并对安全有复杂理解，认为安全整合了人为、系统和技术因素。研究还指出汽车自动化技术移植到铁路的局限性，以及建立铁路特定因果模型的需求。

Conclusion: 本研究旨在弥合当代研究与实际应用之间的差距，为开发更有效的安全指标做出贡献。

Abstract: This study investigates how railway professionals perceive safety as a
concept within rail, with the intention to help inform future technological
developments within the industry. Through a series of interviews with drivers,
route planners,and administrative personnel, the research explores the
currentstate of safety practices, the potential for automation and the
understanding of the railway as a system of systems. Key findings highlight a
cautious attitude towards automation, a preference for assistive technologies,
and a complex understanding of safety that integrates human, systematic and
technological factors. The study also addresses the limitations of transferring
automotive automation technologies to railways and the need for a
railway-specific causation model to better evaluate and enhance safety in an
evolving technological landscape. This study aims to bridge thegap between
contemporary research and practical applications, contributing to the
development of more effective safety metrics.

</details>


### [224] [Human-AI Co-Creation: A Framework for Collaborative Design in Intelligent Systems](https://arxiv.org/abs/2507.17774)
*Zhangqi Liu*

Main category: cs.HC

TL;DR: 本文探讨了人工智能（AI），特别是大型语言模型（LLMs）和多模态扩散模型，如何在早期设计流程中作为生成型协作工具，与人类设计师共同进行创意和决策。


<details>
  <summary>Details</summary>
Motivation: 随着AI从计算工具演变为交互式生成型协作者，传统的以人为中心的设计工作流程需要重新思考，以便在早期设计阶段更好地整合AI，实现AI主动参与构思、视觉概念化和决策。

Method: 论文通过调查人类与AI的协同创作范式，具体研究了GPT-4等大型语言模型和Stable Diffusion等多模态扩散模型作为创意代理，与设计师进行迭代的提案、批评和修订循环。

Result: 摘要中未明确提及具体的研究结果，主要阐述了研究的探索方向和方法。

Conclusion: 摘要中未明确提及研究结论，主要介绍了研究的主题和方法论。

Abstract: As artificial intelligence (AI) continues to evolve from a back-end
computational tool into an interactive, generative collaborator, its
integration into early-stage design processes demands a rethinking of
traditional workflows in human-centered design. This paper explores the
emergent paradigm of human-AI co-creation, where AI is not merely used for
automation or efficiency gains, but actively participates in ideation, visual
conceptualization, and decision-making. Specifically, we investigate the use of
large language models (LLMs) like GPT-4 and multimodal diffusion models such as
Stable Diffusion as creative agents that engage designers in iterative cycles
of proposal, critique, and revision.

</details>


### [225] [PosterMate: Audience-driven Collaborative Persona Agents for Poster Design](https://arxiv.org/abs/2507.18572)
*Donghoon Shin,Daniel Lee,Gary Hsieh,Gromit Yeuk-Yin Chan*

Main category: cs.HC

TL;DR: PosterMate是一款基于AI代理的海报设计助手，它通过模拟多视角受众反馈并引导讨论，帮助设计师高效获取综合性修改建议。


<details>
  <summary>Details</summary>
Motivation: 海报设计中，同步获取并协调多样化受众反馈存在挑战。生成式AI虽有模拟人类交互的潜力，但其在设计反馈流程中的具体应用尚不清晰。

Method: 本文提出PosterMate，一个海报设计辅助系统。它通过营销文档构建具有不同受众视角的AI角色代理，收集它们对海报组件的反馈，并由一个协调员引导讨论以达成共识，最终将商定的修改直接整合到设计中。

Result: 用户研究（N=12）表明，PosterMate能有效捕捉被忽视的观点，并作为一种有效的原型工具。受控在线评估（N=100）显示，单个角色代理的反馈与其身份一致，且讨论能有效综合不同角色代理的观点。

Conclusion: PosterMate系统通过模拟多样化受众反馈和促进讨论，有效解决了传统设计反馈中的难题，展现了捕捉不同视角和作为高效原型工具的潜力。

Abstract: Poster designing can benefit from synchronous feedback from target audiences.
However, gathering audiences with diverse perspectives and reconciling them on
design edits can be challenging. Recent generative AI models present
opportunities to simulate human-like interactions, but it is unclear how they
may be used for feedback processes in design. We introduce PosterMate, a poster
design assistant that facilitates collaboration by creating audience-driven
persona agents constructed from marketing documents. PosterMate gathers
feedback from each persona agent regarding poster components, and stimulates
discussion with the help of a moderator to reach a conclusion. These
agreed-upon edits can then be directly integrated into the poster design.
Through our user study (N=12), we identified the potential of PosterMate to
capture overlooked viewpoints, while serving as an effective prototyping tool.
Additionally, our controlled online evaluation (N=100) revealed that the
feedback from an individual persona agent is appropriate given its persona
identity, and the discussion effectively synthesizes the different persona
agents' perspectives.

</details>


### [226] [Decoding Instructional Dialogue: Human-AI Collaborative Analysis of Teacher Use of AI Tool at Scale](https://arxiv.org/abs/2507.17985)
*Alex Liu,Lief Esbenshade,Shawon Sarkar,Victor Tian,Zachary Zhang,Kevin He,Min Sun*

Main category: cs.HC

TL;DR: 本研究通过人机协作方法分析了超过14万条K-12教师与AI的交互数据，发现大型语言模型能有效辅助大规模质性分析，并揭示了教育者利用AI进行教学、内容创作和专业发展等方面的具体模式。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在教育工具中的整合具有巨大潜力，但目前尚不清楚教育者在实践中如何实际使用这些工具，以及如何大规模有效地研究他们与AI的互动。

Method: 本研究提出了一种人机协作的大规模定性分析方法，对来自K-12教师使用的生成式AI平台的超过14万条教育者-AI消息进行了分析。该方法包括四个阶段：归纳主题发现、编码本开发、结构化标注和模型基准测试，并开发了一个与现有教师评估框架对齐的层次化编码本。同时评估了LLMs在定性编码任务中的表现。

Result: 研究发现，LLMs（特别是Claude 3.5 Haiku）能可靠地支持主题识别，在复杂场景中扩展人类识别能力，并在准确性和结构可靠性方面优于开源模型。分析还揭示了教育者使用AI的主要模式：增强教学实践（79.7%）、创建或改编内容（76.1%）、支持评估和反馈（46.9%）、满足学生定制化需求（43.3%）以及协助其他专业职责（34.2%）。

Conclusion: 本研究为AI增强的定性研究提供了一个可扩展、透明的模型，并为生成式AI在教育实践中不断演进的角色提供了基础性见解，其发现对教师培养和专业发展具有直接影响，突显了新兴的AI相关能力。

Abstract: The integration of large language models (LLMs) into educational tools has
the potential to substantially impact how teachers plan instruction, support
diverse learners, and engage in professional reflection. Yet little is known
about how educators actually use these tools in practice and how their
interactions with AI can be meaningfully studied at scale. This paper presents
a human-AI collaborative methodology for large-scale qualitative analysis of
over 140,000 educator-AI messages drawn from a generative AI platform used by
K-12 teachers. Through a four-phase coding pipeline, we combined inductive
theme discovery, codebook development, structured annotation, and model
benchmarking to examine patterns of educator engagement and evaluate the
performance of LLMs in qualitative coding tasks. We developed a hierarchical
codebook aligned with established teacher evaluation frameworks, capturing
educators' instructional goals, contextual needs, and pedagogical strategies.
Our findings demonstrate that LLMs, particularly Claude 3.5 Haiku, can reliably
support theme identification, extend human recognition in complex scenarios,
and outperform open-weight models in both accuracy and structural reliability.
The analysis also reveals substantive patterns in how educators inquire AI to
enhance instructional practices (79.7 percent of total conversations), create
or adapt content (76.1 percent), support assessment and feedback loop (46.9
percent), attend to student needs for tailored instruction (43.3 percent), and
assist other professional responsibilities (34.2 percent), highlighting
emerging AI-related competencies that have direct implications for teacher
preparation and professional development. This study offers a scalable,
transparent model for AI-augmented qualitative research and provides
foundational insights into the evolving role of generative AI in educational
practice.

</details>


### [227] [BrisT1D Dataset: Young Adults with Type 1 Diabetes in the UK using Smartwatches](https://arxiv.org/abs/2507.17757)
*Sam Gordon James,Miranda Elaine Glynis Armstrong,Aisling Ann O'Kane,Harry Emerson,Zahraa S. Abdallah*

Main category: cs.HC

TL;DR: 该研究介绍了BrisT1D数据集，该数据集来源于对24名英国年轻1型糖尿病患者的纵向研究，结合了智能手表数据、T1D管理系统数据和访谈记录，旨在促进T1D管理技术在实际应用中的探索和新型数据流的利用。


<details>
  <summary>Details</summary>
Motivation: 探索1型糖尿病（T1D）管理技术的实际应用和额外数据流（如智能手表数据）的潜力，并为此贡献一个公开数据集，以促进T1D管理研究的进一步发展。

Method: 通过对24名在英国使用智能手表与常规T1D管理方案的年轻成人进行纵向研究，开发并构建了BrisT1D数据集。数据收集包括来自T1D管理系统和智能手表的设备数据（提供处理和原始两种形式），以及每月访谈和焦点小组的转录文本。

Result: 成功构建了BrisT1D数据集。该数据集包含参与者T1D管理系统和智能手表的设备数据，以及研究期间进行的每月访谈和焦点小组的记录。设备数据同时提供处理和原始状态，以支持不同深度的分析。

Conclusion: BrisT1D数据集具有多方面应用价值。其定量元素可用于血糖预测、低血糖预测和闭环算法开发；定性元素则有助于探索用户体验和观点，并支持智能手表在T1D管理中作用的混合方法研究。

Abstract: Background: Type 1 diabetes (T1D) has seen a rapid evolution in management
technology and forms a useful case study for the future management of other
chronic conditions. Further development of this management technology requires
an exploration of its real-world use and the potential of additional data
streams. To facilitate this, we contribute the BrisT1D Dataset to the growing
number of public T1D management datasets. The dataset was developed from a
longitudinal study of 24 young adults in the UK who used a smartwatch alongside
their usual T1D management. Findings: The BrisT1D dataset features both device
data from the T1D management systems and smartwatches used by participants, as
well as transcripts of monthly interviews and focus groups conducted during the
study. The device data is provided in a processed state, for usability and more
rapid analysis, and in a raw state, for in-depth exploration of novel insights
captured in the study. Conclusions: This dataset has a range of potential
applications. The quantitative elements can support blood glucose prediction,
hypoglycaemia prediction, and closed-loop algorithm development. The
qualitative elements enable the exploration of user experiences and opinions,
as well as broader mixed-methods research into the role of smartwatches in T1D
management.

</details>


<div id='math.LO'></div>

# math.LO [[Back]](#toc)

### [228] [Axiomatizing Rumsfeld Ignorance](https://arxiv.org/abs/2507.17776)
*Jie Fan*

Main category: math.LO

TL;DR: 通过区分隐含知识算子的可达关系，解决Kit Fine理论中Rumsfeld无知与一阶无知可定义性问题，并给出新的公理化。


<details>
  <summary>Details</summary>
Motivation: Kit Fine的理论中，Rumsfeld无知可由一阶无知定义，导致现有结果和公理化问题变得琐碎。主要原因是无知和Rumsfeld无知中隐含的知识算子的可达关系相同。

Method: 假设无知与Rumsfeld无知中隐含知识算子的两个可达关系不同，其中一个为另一个的任意子集。

Result: 主要结果是对各种适当的双框架类进行了公理化。该方法成功避免了可定义性问题，并保留了先前大部分的有效性。

Conclusion: 所提出的框架可用于分析Kit Fine的原始研究结果。

Abstract: In a recent paper, Kit Fine presents some striking results concerning the
logical properties of (first-order) ignorance, second-order ignorance and
Rumsfeld ignorance. However, Rumsfeld ignorance is definable in terms of
ignorance, which makes some existing results and the axiomatization problem
trivial. A main reason is that the accessibility relations for the implicit
knowledge operator contained in the packaged operators of ignorance and
Rumsfeld ignorance are the same. In this work, we assume the two accessibility
relations to be different so that one of them is an arbitrary subset of the
other. This will avoid the definability issue and retain most of the previous
validities. The main results are axiomatizations over various proper bi-frame
classes. Finally we apply our framework to analyze Fine's results.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [229] [In Reverie Together: Ten Years of Mathematical Discovery with a Machine Collaborator](https://arxiv.org/abs/2507.17780)
*Randy Davila,Boris Brimkov,Ryan Pepper*

Main category: cs.DM

TL;DR: 本文提出了由自动化猜想系统TxGraffiti生成的四个图论开放猜想，旨在激发人类和AI共同参与数学发现过程。


<details>
  <summary>Details</summary>
Motivation: 尽管投入了大量精力，这些猜想仍未被证明或找到反例，作者旨在将这些难题回馈给社区，并引发对机器在数学创造性思维中作用的思考。

Method: 利用自动化猜想系统TxGraffiti生成图论猜想，这些猜想基于自然图不变量，并通过数百个图进行了实证验证，并经过多年的人机对话改进。

Result: 提出了四个由AI系统生成且至今未解决的图论猜想。这些猜想简洁、基于自然图不变量，并通过实证验证，但仍未被证明或证伪。

Conclusion: 这些猜想被视为协作性的数学挑战，旨在激励人类数学家和AI系统参与解决，同时促使人们反思机器在数学思想创造过程中有意义的参与。

Abstract: We present four open conjectures in graph theory generated by the automated
conjecturing system \texttt{TxGraffiti}. Each conjecture is concise, grounded
in natural graph invariants, and empirically validated across hundreds of
graphs. Despite extensive effort, these statements remain unresolved--defying
both proof and counterexample. They are not only mathematical challenges but
creative expressions--born of symbolic pattern recognition and
mathematician-defined heuristics, refined through years of human dialogue, and
now offered back to the community as collaborative artifacts. These conjectures
invite not only formal proof, but also reflection on how machines can evoke
wonder, spark curiosity, and contribute to the raw material of discovery. By
highlighting these problems, we aim to inspire both human mathematicians and AI
systems to engage with them--not only to solve them, but to reflect on what it
means when machines participate meaningfully in the creative process of
mathematical thought.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [230] [Mapping Technological Futures: Anticipatory Discourse Through Text Mining](https://arxiv.org/abs/2504.02853)
*Maciej Skorski,Alina Landowska,Krzysztof Rajda*

Main category: cs.SI

TL;DR: 本研究分析了社交媒体上KOLs关于新兴技术未来的预期性话语，发现KOLs通过乐观视角塑造技术愿景并影响当代辩论，其中希望情绪普遍高于焦虑。


<details>
  <summary>Details</summary>
Motivation: 新兴技术（如AI）的波动性和不确定性在社交媒体上引发广泛讨论，本研究旨在探究围绕技术未来的预期性话语。

Method: 研究分析了2021年至2023年间X平台上400位KOLs发布的150万条帖子。采用先进文本挖掘技术，包括BERTopic建模、情感、情绪和态度分析，识别了100个反映预期技术驱动未来的独特话题。

Result: 研究发现KOLs在塑造“当前未来”（对AI和IoT等技术的乐观愿景）和影响“未来当下”（这些预测塑造当代社会和地缘政治辩论）中扮演双重角色。在AI相关话题中，积极情绪（如希望）占主导，超过焦虑；而气候变化和战争等话题则引发焦虑。KOLs通过将技术描绘为社会挑战的解决方案，充当社会叙事的调解者。

Conclusion: 这些见解强调了KOLs在高度不确定时期引导公众关注新兴技术方面的关键作用，并加深了对技术介导背景下预期性话语的理解。

Abstract: The volatility and unpredictability of emerging technologies, such as
artificial intelligence (AI), generate significant uncertainty, which is widely
discussed on social media. This study examines anticipatory discourse
surrounding technological futures by analysing 1.5 million posts from 400 key
opinion leaders (KOLs) published on the X platform (from 2021 to 2023). Using
advanced text mining techniques, including BERTopic modelling, sentiment,
emotion, and attitude analyses, the research identifies 100 distinct topics
reflecting anticipated tech-driven futures. Our findings emphasize the dual
role of KOLs in framing \textit{present futures} -- optimistic visions of
transformative technologies like AI and IoT -- and influencing \textit{future
presents}, where these projections shape contemporary societal and geopolitical
debates. Positive emotions such as Hope dominate, outweighing Anxiety,
particularly in topics like ``Machine Learning, Data Science, and Deep
Learning,'' while discussions around ``Climate Change'' and ``War, Ukraine, and
Trump People'' elicit \textit{Anxiety}. By framing technologies as solutions to
societal challenges, KOLs act as mediators of societal narratives, bridging
imagined futures and current realities. These insights underscore their pivotal
role in directing public attention with emerging technologies during periods of
heightened uncertainty, advancing our understanding of anticipatory discourse
in technology-mediated contexts.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [231] [How Instructional Sequence and Personalized Support Impact Diagnostic Strategy Learning](https://arxiv.org/abs/2507.17760)
*Fatma Betül Güreş,Tanya Nazaretsky,Bahar Radmehr,Martina Rau,Tanja Käser*

Main category: cs.CY

TL;DR: 本研究比较了在情境学习中，先解决问题后教学（PS-I）和先教学后解决问题（I-PS）对诊断推理能力迁移的影响，发现PS-I在迁移任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 培养学生有效诊断推理能力是一项重要挑战，新手常受认知偏差影响。情境学习（SBL）能提供实践经验，但教学与问题解决活动的最佳顺序尚不明确。本研究旨在探索个性化支持如何融入不同教学序列，并评估教学顺序对学习和迁移效果的影响。

Method: 本研究采用组间设计，在一个名为PharmaSim的在线情境学习环境中进行，该环境模拟药剂师与客户的真实互动。研究比较了两种教学序列：在问题解决前提供明确诊断策略教学（I-PS）和在问题解决后提供（PS-I）。

Result: 研究结果显示，两种教学类型均有益，但先解决问题后教学（PS-I）在迁移任务中的表现显著优于先教学后解决问题（I-PS）。

Conclusion: 在情境学习中，先让学生进行问题解决，然后提供明确的诊断策略教学（PS-I），能更有效地提升诊断推理能力，并促进其向新情境的迁移。

Abstract: Supporting students in developing effective diagnostic reasoning is a key
challenge in various educational domains. Novices often struggle with cognitive
biases such as premature closure and over-reliance on heuristics.
Scenario-based learning (SBL) can address these challenges by offering
realistic case experiences and iterative practice, but the optimal sequencing
of instruction and problem-solving activities remains unclear. This study
examines how personalized support can be incorporated into different
instructional sequences and whether providing explicit diagnostic strategy
instruction before (I-PS) or after problem-solving (PS-I) improves learning and
its transfer. We employ a between-groups design in an online SBL environment
called PharmaSim, which simulates real-world client interactions for pharmacy
technician apprentices. Results indicate that while both instruction types are
beneficial, PS-I leads to significantly higher performance in transfer tasks.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [232] [VERIRAG: Healthcare Claim Verification via Statistical Audit in Retrieval-Augmented Generation](https://arxiv.org/abs/2507.17948)
*Shubham Mohole,Hongjun Choi,Shusen Liu,Christine Klymko,Shashank Kushwaha,Derek Shi,Wesam Sakla,Sainyam Galhotra,Ruben Glatt*

Main category: cs.IR

TL;DR: VERIRAG框架通过评估科学证据质量来增强检索增强生成（RAG）系统，解决了当前RAG系统无法辨别证据可信度的问题，并在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前的检索增强生成（RAG）系统在临床决策支持中日益普及，但它们无法评估所检索科学证据的质量和可信度，可能将缺乏严谨性甚至已被撤回的论文与高质量研究同等对待，这可能导致不准确或不可靠的决策。因此，研究的动机是解决RAG系统在证据质量验证方面的缺陷。

Method: 研究引入了一个名为VERIRAG的框架，该框架包含三个主要组成部分：(i) Veritable，一个11点检查表，用于评估每个来源的方法论严谨性；(ii) Hard-to-Vary (HV) 分数，一个定量聚合器，根据证据的质量和多样性对其进行加权；以及(iii) 动态接受阈值，根据声明的“非凡”程度校准所需证据。

Result: VERIRAG方法在四个不同数据集（包括已撤回、冲突、全面和已确定的科学语料库）上进行了测试，结果显示它始终优于所有基线方法。与次优方法相比，VERIRAG的绝对F1分数在0.53到0.65之间，每个数据集的F1分数均提高了10到14个百分点。

Conclusion: VERIRAG框架成功地解决了RAG系统在评估科学证据质量方面的盲点，通过引入多层次的质量评估机制，显著提升了系统在处理复杂和冲突性科学信息时的性能和可靠性，尤其对于临床决策支持等需要高度可信证据的应用具有重要意义。

Abstract: Retrieval-augmented generation (RAG) systems are increasingly adopted in
clinical decision support, yet they remain methodologically blind-they retrieve
evidence but cannot vet its scientific quality. A paper claiming "Antioxidant
proteins decreased after alloferon treatment" and a rigorous multi-laboratory
replication study will be treated as equally credible, even if the former
lacked scientific rigor or was even retracted. To address this challenge, we
introduce VERIRAG, a framework that makes three notable contributions: (i) the
Veritable, an 11-point checklist that evaluates each source for methodological
rigor, including data integrity and statistical validity; (ii) a Hard-to-Vary
(HV) Score, a quantitative aggregator that weights evidence by its quality and
diversity; and (iii) a Dynamic Acceptance Threshold, which calibrates the
required evidence based on how extraordinary a claim is. Across four
datasets-comprising retracted, conflicting, comprehensive, and settled science
corpora-the VERIRAG approach consistently outperforms all baselines, achieving
absolute F1 scores ranging from 0.53 to 0.65, representing a 10 to 14 point
improvement over the next-best method in each respective dataset. We will
release all materials necessary for reproducing our results.

</details>


### [233] [LLM-based Embedders for Prior Case Retrieval](https://arxiv.org/abs/2507.18455)
*Damith Premasiri,Tharindu Ranasinghe,Ruslan Mitkov*

Main category: cs.IR

TL;DR: 针对法律先例检索（PCR）中长文本处理和训练数据稀缺的挑战，本研究引入无监督的LLM文本嵌入器。实验证明，LLM嵌入器在PCR基准数据集上的表现优于传统方法和监督式Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 在普通法系中，法律专业人士高度依赖先例，但随着案件量激增，有效检索先例变得至关重要。当前的先例检索（PCR）方法主要依赖BM25等传统信息检索方法，而先进的深度学习方法（如基于BERT的Transformer模型）在PCR中表现不佳，主要原因在于：1) 法律文本过长导致模型输入限制和关键上下文信息丢失；2) 由于隐私等原因，缺乏充足的法律训练数据。

Method: 本研究通过利用基于大型语言模型（LLM）的文本嵌入器来解决PCR中的长文本限制和数据稀缺问题。LLM嵌入器支持更长的输入长度，并且由于采用无监督方式使用，无需专门的训练数据。研究在四个PCR基准数据集上评估了最先进的LLM文本嵌入器。

Result: 评估结果显示，所采用的LLM文本嵌入器在PCR任务中表现优于传统的BM25方法以及监督式Transformer模型。

Conclusion: 基于LLM的文本嵌入器能有效解决法律文本长度限制和训练数据不足的问题，显著提升先例检索的性能，为PCR领域提供了新的有效方法。

Abstract: In common law systems, legal professionals such as lawyers and judges rely on
precedents to build their arguments. As the volume of cases has grown massively
over time, effectively retrieving prior cases has become essential. Prior case
retrieval (PCR) is an information retrieval (IR) task that aims to
automatically identify the most relevant court cases for a specific query from
a large pool of potential candidates. While IR methods have seen several
paradigm shifts over the last few years, the vast majority of PCR methods
continue to rely on traditional IR methods, such as BM25. The state-of-the-art
deep learning IR methods have not been successful in PCR due to two key
challenges: i. Lengthy legal text limitation; when using the powerful
BERT-based transformer models, there is a limit of input text lengths, which
inevitably requires to shorten the input via truncation or division with a loss
of legal context information. ii. Lack of legal training data; due to data
privacy concerns, available PCR datasets are often limited in size, making it
difficult to train deep learning-based models effectively. In this research, we
address these challenges by leveraging LLM-based text embedders in PCR.
LLM-based embedders support longer input lengths, and since we use them in an
unsupervised manner, they do not require training data, addressing both
challenges simultaneously. In this paper, we evaluate state-of-the-art
LLM-based text embedders in four PCR benchmark datasets and show that they
outperform BM25 and supervised transformer-based models.

</details>


### [234] [DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge Injection and Synthetic Data](https://arxiv.org/abs/2507.18583)
*Zhengyun Zhao,Huaiyuan Ying,Yue Zhong,Sheng Yu*

Main category: cs.IR

TL;DR: 本文提出DR.EHR，一种专为电子健康记录（EHR）检索设计的密集检索模型系列。通过两阶段训练流程，融合医学知识图谱和大型语言模型生成训练数据，DR.EHR在CliniQ基准测试中超越现有模型，达到最先进性能，并展现出良好的泛化能力，为EHR检索提供了强大解决方案。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）在临床实践中至关重要，但其检索因语义鸿沟而面临挑战。现有密集检索模型（无论是通用领域还是生物医学领域）因医学知识不足或训练语料不匹配而无法满足需求。

Method: 引入DR.EHR，一个专为EHR检索定制的密集检索模型系列。提出一个两阶段训练管道：第一阶段从生物医学知识图谱中提取并注入医学实体知识；第二阶段利用大型语言模型生成多样化的训练数据。模型基于MIMIC-IV出院总结进行训练，并开发了1.1亿和70亿参数的两个版本。

Result: 在CliniQ基准测试中，DR.EHR模型显著优于所有现有密集检索器，实现了最先进的结果。详细分析证实了模型在各种匹配和查询类型（尤其在含义和缩写等挑战性语义匹配上）的优越性。消融研究验证了每个管道组件的有效性，且在EHR QA数据集上的补充实验证明了模型对自然语言问题的泛化能力。

Conclusion: 本工作显著推动了EHR检索技术，为临床应用提供了一个强大的解决方案。

Abstract: Electronic Health Records (EHRs) are pivotal in clinical practices, yet their
retrieval remains a challenge mainly due to semantic gap issues. Recent
advancements in dense retrieval offer promising solutions but existing models,
both general-domain and biomedical-domain, fall short due to insufficient
medical knowledge or mismatched training corpora. This paper introduces
\texttt{DR.EHR}, a series of dense retrieval models specifically tailored for
EHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV
discharge summaries to address the need for extensive medical knowledge and
large-scale training data. The first stage involves medical entity extraction
and knowledge injection from a biomedical knowledge graph, while the second
stage employs large language models to generate diverse training data. We train
two variants of \texttt{DR.EHR}, with 110M and 7B parameters, respectively.
Evaluated on the CliniQ benchmark, our models significantly outperforms all
existing dense retrievers, achieving state-of-the-art results. Detailed
analyses confirm our models' superiority across various match and query types,
particularly in challenging semantic matches like implication and abbreviation.
Ablation studies validate the effectiveness of each pipeline component, and
supplementary experiments on EHR QA datasets demonstrate the models'
generalizability on natural language questions, including complex ones with
multiple entities. This work significantly advances EHR retrieval, offering a
robust solution for clinical applications.

</details>


### [235] [Fashion-AlterEval: A Dataset for Improved Evaluation of Conversational Recommendation Systems with Alternative Relevant Items](https://arxiv.org/abs/2507.18017)
*Maria Vlachou*

Main category: cs.IR

TL;DR: 针对会话推荐系统（CRS）中用户模拟器单目标和无限耐心的问题，本文提出了Fashion-AlterEval数据集和两种新型元用户模拟器，旨在更真实地评估CRS模型，并发现现有模型效果被低估。


<details>
  <summary>Details</summary>
Motivation: 现有的会话推荐系统（CRS）用户模拟器在离线评估中存在局限性，它们仅基于单一目标商品进行反馈且具有无限耐心，这可能导致对CRS模型实际效果的低估。

Method: ['构建Fashion-AlterEval数据集，在现有时尚CRS数据集中加入人类对替代商品的判断。', '提出两种新型元用户模拟器，利用Fashion-AlterEval中的判断，使模拟用户能够表达对替代商品的偏好、改变主意并调整耐心程度。', '在Shoes和Fashion IQ数据集以及三种CRS模型上进行实验评估。']

Result: ['模拟器使用替代商品知识显著影响现有CRS模型的评估。', '现有单一目标评估方法低估了CRS模型的有效性。', '当模拟用户考虑替代相关商品时，系统能更快地满足用户需求。']

Conclusion: 通过引入替代商品概念和更真实的模拟器，对会话推荐系统进行评估能更准确地反映其性能，表明现有模型在考虑替代商品时比单一目标评估所显示的更为有效，并能更快地满足用户。

Abstract: In Conversational Recommendation Systems (CRS), a user provides feedback on
recommended items at each turn, leading the CRS towards improved
recommendations. Due to the need for a large amount of data, a user simulator
is employed for both training and evaluation. Such user simulators critique the
current retrieved item based on knowledge of a single target item. However,
system evaluation in offline settings with simulators is limited by the focus
on a single target item and their unlimited patience over a large number of
turns. To overcome these limitations of existing simulators, we propose
Fashion-AlterEval, a new dataset that contains human judgments for a selection
of alternative items by adding new annotations in common fashion CRS datasets.
Consequently, we propose two novel meta-user simulators that use the collected
judgments and allow simulated users not only to express their preferences about
alternative items to their original target, but also to change their mind and
level of patience. In our experiments using the Shoes and Fashion IQ as the
original datasets and three CRS models, we find that using the knowledge of
alternatives by the simulator can have a considerable impact on the evaluation
of existing CRS models, specifically that the existing single-target evaluation
underestimates their effectiveness, and when simulatedusers are allowed to
instead consider alternative relevant items, the system can rapidly respond to
more quickly satisfy the user.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [236] [An advanced AI driven database system](https://arxiv.org/abs/2507.17778)
*M. Tedeschi,S. Rizwan,C. Shringi,V. Devram Chandgir,S. Belich*

Main category: cs.DB

TL;DR: 提出一种AI驱动的数据库系统，通过自然语言处理（NLP）和大型语言模型（LLM）自动化数据管理，旨在简化非技术用户操作，减少复杂性和人为错误。


<details>
  <summary>Details</summary>
Motivation: 当代数据库系统虽有效，但复杂度高且可用性差，尤其对于缺乏技术专长且不熟悉SQL等查询语言的用户而言，存在严重问题。

Method: 开发一个由人工智能（AI）支持的新数据库系统。该系统通过整合大型语言模型（LLM）和高级机器学习算法，利用自然语言处理（NLP）提供直观界面，并自动化结构化查询、半结构化数据格式（如YAML, JSON）及API文档的创建。具体方法包括生成式模式推断和格式选择。

Result: 该系统旨在解决当前数据库技术的主要问题，通过降低对技术技能的需求、减少手动性能调优和人为错误的可能性，提升数据管理效率和用户友好性。

Conclusion: 该AI数据库系统通过自动化数据建模、模式创建、查询理解和性能优化等核心任务，显著提升数据库的潜力，有效缓解现有系统的复杂性和可用性挑战，从而降低使用门槛。

Abstract: Contemporary database systems, while effective, suffer severe issues related
to complexity and usability, especially among individuals who lack technical
expertise but are unfamiliar with query languages like Structured Query
Language (SQL). This paper presents a new database system supported by
Artificial Intelligence (AI), which is intended to improve the management of
data using natural language processing (NLP) - based intuitive interfaces, and
automatic creation of structured queries and semi-structured data formats like
yet another markup language (YAML), java script object notation (JSON), and
application program interface (API) documentation. The system is intended to
strengthen the potential of databases through the integration of Large Language
Models (LLMs) and advanced machine learning algorithms. The integration is
purposed to allow the automation of fundamental tasks such as data modeling,
schema creation, query comprehension, and performance optimization. We present
in this paper a system that aims to alleviate the main problems with current
database technologies. It is meant to reduce the need for technical skills,
manual tuning for better performance, and the potential for human error. The AI
database employs generative schema inference and format selection to build its
schema models and execution formats.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [237] [Caching Techniques for Reducing the Communication Cost of Federated Learning in IoT Environments](https://arxiv.org/abs/2507.17772)
*Ahmad Alhonainy,Praveen Rao*

Main category: cs.DC

TL;DR: 本论文提出将FIFO、LRU和优先级缓存策略应用于联邦学习，以减少不必要的模型更新传输，从而在保持模型精度的前提下降低通信成本，提高其在资源受限环境下的实用性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）尽管允许分布式设备联合训练模型而不集中数据，但通信成本，尤其是在资源受限环境中，是一个主要的瓶颈。

Method: 论文引入并应用了FIFO（先进先出）、LRU（最近最少使用）和基于优先级的缓存策略，通过选择性地转发重要的模型更新，来减少不必要的传输。

Result: 在CIFAR-10和医疗数据集上的实验表明，该方法显著减少了通信量，同时只带来了最小的精度损失。

Conclusion: 智能缓存策略能够提高联邦学习的扩展性、内存效率，并支持其在边缘物联网（IoT）网络中的可靠部署，使其在智慧城市、医疗保健等对延迟敏感的应用中具有实用价值。

Abstract: Federated Learning (FL) allows multiple distributed devices to jointly train
a shared model without centralizing data, but communication cost remains a
major bottleneck, especially in resource-constrained environments. This paper
introduces caching strategies - FIFO, LRU, and Priority-Based - to reduce
unnecessary model update transmissions. By selectively forwarding significant
updates, our approach lowers bandwidth usage while maintaining model accuracy.
Experiments on CIFAR-10 and medical datasets show reduced communication with
minimal accuracy loss. Results confirm that intelligent caching improves
scalability, memory efficiency, and supports reliable FL in edge IoT networks,
making it practical for deployment in smart cities, healthcare, and other
latency-sensitive applications.

</details>


### [238] [Incentivised Orchestrated Training Architecture (IOTA): A Technical Primer for Release](https://arxiv.org/abs/2507.17766)
*Felix Quinque,Alan Aboudib,Szymon Fonau,Rodrigo Lopez Portillo Alcocer,Brian McCrindle,Steffen Cruz*

Main category: cs.DC

TL;DR: 本文介绍了一种名为IOTA（激励式协调训练架构）的新架构，旨在解决Bittensor Subnet 9（SN9）中去中心化大型语言模型预训练的局限性，通过实现协作式、可扩展的训练、公平的激励分配以及通信优化，将独立参与者转变为协作单元。


<details>
  <summary>Details</summary>
Motivation: Bittensor SN9虽然验证了基于区块链的去中心化预训练的可行性，但存在核心问题：(i) 每个矿工需要本地适应整个模型，(ii) “赢家通吃”的奖励机制鼓励模型囤积。因此需要一种新架构来解决这些限制。

Method: 本文提出了IOTA架构，其主要方法包括：1) 数据和流水线并行SWARM架构，通过协调器在异构矿工间分发模型层并传输激活，使模型规模随参与者数量扩展；2) 细粒度、持续的激励机制，验证者按贡献比例分配代币奖励；3) 激活压缩，利用模型瓶颈将激活通信带宽降低高达128倍；4) Butterfly All-Reduce，矿工以O(1)带宽平均不相交的参数切片，实现线性扩展、冗余和内置串通检测；5) CLASP（通过路径采样评估贡献损失），一种公平的归因方案，根据边际效用分配贡献并检测漏洞。

Result: IOTA将SN9中原先孤立的竞争者转变为一个单一的协作单元，能够任意扩展，同时仍然公平地奖励每个贡献者。通过其独特架构和优化方法，IOTA解决了之前去中心化预训练中模型规模受限和奖励机制不公的问题。

Conclusion: IOTA架构通过创新的协作训练、激励机制和通信优化，有效克服了现有去中心化LLM预训练的局限性，为实现可扩展且公平的去中心化模型训练奠定了基础。

Abstract: In August 2024, Bittensor's Subnet 9 (SN9) demonstrated that a distributed
network of incentivized, permissionless actors could each pretrain large
language models (LLMs) ranging from 700 million to 14 billion parameters, while
surpassing established baselines. While that work validated blockchain-based
decentralized pretraining as viable, it contained core issues: (i) every miner
had to fit an entire model locally, and (ii) "winner-takes-all" rewards
encouraged model hoarding.
  Here we introduce IOTA (Incentivized Orchestrated Training Architecture), an
architecture that addresses these limitations by transforming SN9's previously
isolated competitors into a single cooperating unit that can scale arbitrarily
while still rewarding each contributor fairly.
  Key preliminary results: (1) Data- and Pipeline-parallel SWARM architecture -
An orchestrator distributes model layers across heterogeneous miners and
streams activations between them, enabling model sizes to scale with the number
of participants rather than being constrained by the VRAM of a single machine;
(2) Granular, continuous incentives - Validators measure each miner's
contribution and allocate token emissions proportionally; (3) Activation
compression - We used model-bottlenecks to cut communication bandwidths of
activations by up to 128x, vastly improving training speed; (4) Butterfly
All-Reduce - Miners average disjoint parameter slices in O(1) bandwidth,
offering linear scalability, redundancy and built-in collusion detection; (5)
CLASP (Contribution Loss Assessment via Sampling of Pathways) - A fair
attribution scheme assigns credit to miners proportional to their marginal
utility and detects exploits, even when contributions are interdependent across
the pipeline.

</details>


### [239] [PolyServe: Efficient Multi-SLO Serving at Scale](https://arxiv.org/abs/2507.17769)
*Kan Zhu,Haiyang Shi,Le Xu,Jiaxin Shan,Arvind Krishnamurthy,Baris Kasikci,Liguang Xie*

Main category: cs.DC

TL;DR: PolyServe是一种针对大型语言模型（LLMs）的创新多服务等级目标（multi-SLO）调度策略，旨在解决现有系统在满足多样化延迟要求方面的挑战，通过智能分组和路由策略提高吞吐量并确保SLO达成。


<details>
  <summary>Details</summary>
Motivation: 现有LLM应用对令牌生成延迟有多种要求，但简单地将工作负载分为延迟敏感（LS）或尽力而为（BE）类别，忽略了LS内部的细微差别，导致用户体验和调度不佳。高效服务具有多SLO需求的请求面临多项挑战：批处理请求同时生成新令牌可能与各自的SLO不符；现有系统侧重于整体请求速率的自动扩缩容，而非SLO层级的细粒度扩缩容；不同LS SLO请求不能容忍长时间延迟，尾部延迟难以控制。

Method: 本文提出了PolyServe，一种大规模多SLO调度策略。PolyServe首先根据每令牌延迟要求将请求分组到多个桶中，然后将每个桶调度到服务器集群的一个子集。它将请求路由到负载最高但仍能达到SLO的服务器，以创建负载梯度，便于自动扩缩容。为提高利用率，当低SLO请求的服务器饱和时，允许其共享高SLO实例。PolyServe利用分析数据指导调度决策，并通过请求等待时间感知调度、动态分块和连续分块预填充预测来管理尾部延迟。

Result: PolyServe与现有策略相比，实现了1.23倍的有效吞吐量（goodput）提升，并达到了最优有效吞吐量的92.5%。

Conclusion: PolyServe成功应对了LLM多SLO请求服务的挑战，通过创新的调度策略在高SLO达成率的同时最大化了吞吐量，显著优于现有系统。

Abstract: Advances in Large Language Models (LLMs) have led to a surge of LLM-powered
applications. These applications have diverse token-generation latency
requirements. As a result, simply classifying workloads as latency-sensitive
(LS) or best-effort (BE) overlooks the nuances within the latency-sensitive
category and results in suboptimal user experiences and scheduling
opportunities. However, efficiently serving requests with multiple SLO
requirements poses significant challenges. First, all requests within a batch
generate new tokens simultaneously, which can misalign them with their distinct
SLO requirements. Moreover, while existing systems focus on auto-scaling for
handling various overall request rates, the diversity of SLOs necessitates
fine-grained auto-scaling among these SLO tiers. Finally, unlike LS/BE
scenarios, where BE requests can be aborted at any time to ensure the SLO
attainment of LS requests, those with different latency-sensitive SLOs cannot
tolerate prolonged delays, and tail latency must be controlled.
  To tackle these challenges, we propose PolyServe, a novel multi-SLO
scheduling policy at scale that maintains high SLO attainment while maximizing
throughput. PolyServe first groups requests into multiple bins based on their
per-token latency requirement, then schedules each bin to a subset of the
server fleet. PolyServe routes requests to the highest-load but still
SLO-attainable server to create a load gradient that facilitates auto-scaling.
To increase utilization, PolyServe permits looser-SLO requests to share
tighter-SLO instances when their own servers are saturated. PolyServe uses
profiling data to guide scheduling decisions and manage tail latency through
request-wait-time-aware scheduling, dynamic chunking, and continuous chunked
prefill prediction. PolyServe achieves 1.23x goodput gain compared to existing
policies, achieving up to 92.5% of optimal goodput.

</details>


### [240] [MultiKernelBench: A Multi-Platform Benchmark for Kernel Generation](https://arxiv.org/abs/2507.17773)
*Zhongzhen Wen,Yinghui Zhang,Zhong Li,Zhongxin Liu,Linna Xie,Tian Zhang*

Main category: cs.DC

TL;DR: 本文介绍MultiKernelBench，一个用于评估大型语言模型生成深度学习内核的全面、多平台基准测试，并提出一种类别感知提示方法，揭示了当前LLM在该领域面临的挑战和有效策略。


<details>
  <summary>Details</summary>
Motivation: 自动生成深度学习内核可减少手动工作和硬件专业知识需求。然而，现有基准测试存在硬件支持有限、内核分类粗糙和任务覆盖不平衡等局限性，需要更全面的评估工具。

Method: 引入MultiKernelBench，涵盖285个任务和14个内核类别，支持Nvidia GPU、华为NPU和谷歌TPU三大硬件平台。设计模块化后端抽象层以增强可扩展性。提出一种简单有效的类别感知单次提示方法，通过提供同类别示例来提高生成质量。对七个最先进的大型语言模型进行了系统评估。

Result: 评估结果显示任务难度差异显著；模型在训练曝光较少的平台上泛化能力差；以及目标性提示策略（如类别感知提示）的有效性。

Conclusion: MultiKernelBench为LLM生成DL内核提供了首个全面的评估标准，揭示了当前LLM在该领域的优势和局限性，并指明了未来研究方向。该基准已公开可用。

Abstract: The automatic generation of deep learning (DL) kernels using large language
models (LLMs) has emerged as a promising approach to reduce the manual effort
and hardware-specific expertise required for writing high-performance operator
implementations. However, existing benchmarks for evaluating LLMs in this
domain suffer from limited hardware support, coarse-grained kernel
categorization, and imbalanced task coverage. To address these limitations, we
introduce MultiKernelBench, the first comprehensive, multi-platform benchmark
for LLM-based DL kernel generation. MultiKernelBench spans 285 tasks across 14
well-defined kernel categories and supports three major hardware platforms:
Nvidia GPUs, Huawei NPUs, and Google TPUs. To enable future extensibility, we
design a modular backend abstraction layer that decouples platform-specific
logic from the core benchmarking infrastructure, allowing easy integration of
new hardware platforms. We further propose a simple yet effective
category-aware one-shot prompting method that improves generation quality by
providing in-category exemplars. Through systematic evaluations of seven
state-of-the-art LLMs, we reveal significant variation in task difficulty, poor
generalization to platforms with less training exposure, and the effectiveness
of targeted prompting strategies. MultiKernelBench is publicly available at
https://github.com/wzzll123/MultiKernelBench.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [241] [On the Role of Age and Semantics of Information in Remote Estimation of Markov Sources](https://arxiv.org/abs/2507.18514)
*Jiping Luo,Nikolaos Pappas*

Main category: cs.IT

TL;DR: 本文研究了有限状态马尔可夫链的语义感知远程估计问题。在传输频率约束下，通过利用AoCE和AoI指标，将问题建模为受限马尔可夫决策过程，并设计出一种基于阈值且混合的最优传输策略，开发了高效算法，结果表明该方法能显著提升估计质量。


<details>
  <summary>Details</summary>
Motivation: 在传输频率受限的条件下，如何优化有限状态马尔可夫链的远程估计性能，特别是在估计误差和信息过时程度之间进行权衡，以设计出高效且最优的传输策略。

Method: ['采用最大后验概率 (MAP) 估计器进行远程估计。', '引入并利用连续错误年龄 (AoCE) 和信息年龄 (AoI) 作为关键指标，分别量化发送端的估计错误重要性和接收端的过时信息可预测性。', '将最优传输问题建模为具有无界成本的受限马尔可夫决策过程 (CMDP)。', '开发了高效的结构感知算法Insec-SPI，用于计算最优策略，旨在降低计算开销。']

Result: ['证明了存在一种最优的简单混合策略，该策略以固定概率在两种确定性切换策略之间随机选择。', '每种切换策略仅当AoCE超过一个同时依赖于AoI和瞬时估计误差的阈值时才触发传输。', '导出了使切换策略简化为简单阈值策略（即对所有估计误差采用相同阈值）的充分条件。', '所开发的Insec-SPI算法能够以较低的计算开销计算出最优策略。']

Conclusion: 将信息年龄 (AoI) 和连续错误年龄 (AoCE) 两个指标相结合，能够显著提高远程估计的质量，相比单独使用其中任何一个指标效果更佳，验证了所提出方法的有效性。

Abstract: This paper investigates the semantics-aware remote estimation of a
finite-state Markov chain. We employ the maximum a posteriori (MAP) estimator
and aim to devise a transmission policy to optimize estimation performance
subject to a transmission frequency constraint. We leverage two metrics, namely
the Age of Consecutive Error (AoCE) and the Age of Information (AoI), to
quantify, respectively, the significance of estimation error at the transmitter
and the predictability of outdated information at the receiver. The optimal
transmission problem is formulated as a constrained Markov decision process
(CMDP) with unbounded costs. We show the existence of an optimal simple mixture
policy, which randomly selects between two deterministic switching policies
with a fixed probability. Notably, each switching policy triggers a
transmission only when the AoCE exceeds a threshold value that depends on both
the AoI and the instantaneous estimation error. We further derive sufficient
conditions under which the switching policy reduces to a simple threshold
policy; that is, it admits identical thresholds for all estimation errors.
Leveraging these results, we develop an efficient structure-aware algorithm,
Insec-SPI, that computes the optimal policy with reduced computation overhead.
Our results demonstrate that incorporating both AoI and AoCE yields
significantly improved estimation quality compared to using either metric
alone.

</details>


### [242] [Action-List Reinforcement Learning Syndrome Decoding for Binary Linear Block Codes](https://arxiv.org/abs/2507.17893)
*Milad Taghipour,Bane Vasic*

Main category: cs.IT

TL;DR: 本文探索了强化学习在提升线性分组码译码性能中的应用，通过将译码映射为马尔可夫决策过程（MDP）并提出状态缩减、动作列表译码、自同构群利用及反馈增强等方法，在LDPC码上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在利用强化学习（RL）技术，通过优化比特翻转和决策过程，显著提升线性分组码的译码性能。

Method: 1. 将迭代译码过程建模为马尔可夫决策过程（MDP）。2. 提出多种MDP状态缩减方法，包括通过学习码字周围汉明球的截断式MDP。3. 提出一种通用的、基于强化学习的“动作列表译码”方案，适用于各类编码。4. 基于深度Q网络（Deep-Q network）设计动作列表译码器以增强性能。5. 利用码的自同构群进一步优化性能。6. 提出一种反馈式方法，在现有高性能译码器后应用RL算法，以提升并利用其性能，同时降低RL复杂度。

Result: 1. 提出的RL译码方案（特别是基于深度Q网络的动作列表译码器）显著提升了译码性能。2. 反馈式方法有效降低了强化学习模块的复杂性。3. 在二元对称信道（BSC）上的低密度奇偶校验码（LDPC）实验结果证明了所提方法的有效性。

Conclusion: 本文成功将强化学习应用于线性分组码译码，通过创新的MDP建模、状态缩减、动作列表译码和反馈机制，显著提升了译码性能并降低了RL复杂度，为高性能译码提供了新范式。

Abstract: This paper explores the application of reinforcement learning techniques to
enhance the performance of decoding of linear block codes based on flipping
bits and finding optimal decisions. We describe the methodology for mapping the
iterative decoding process into Markov Decision Processes (MDPs) and propose
different methods to reduce the number of states in the MDP. A truncated MDP is
proposed to reduce the number of states in the MDP by learning a Hamming ball
with a specified radius around codewords. We then propose a general scheme for
reinforcement learning based decoders applicable to any class of codes to
improve the performance of decoders. We call this scheme an action-list
decoding. We design an action-list decoder based on the Deep-Q network values
that substantially enhance performance. We also get benefit of automorphism
group of code to further improve the code performance. Additionally, we propose
a feedback-based method to exploit and enhance the performance of existing
high-performing decoders by applying reinforcement learning algorithms after
the existing decoders. These approaches effectively reduces the complexity of
the reinforcement learning block. Finally, we present experimental results for
the Low-Density Parity Check (LDPC) codes over the Binary Symmetric Channel
(BSC) to demonstrate the efficiency of the proposed methods.

</details>


### [243] [Minimax Data Sanitization with Distortion Constraint and Adversarial Inference](https://arxiv.org/abs/2507.17942)
*Amirarsalan Moatazedian,Yauhen Yakimenka,Rémi A. Chou,Jörg Kliewer*

Main category: cs.IT

TL;DR: 本研究探讨一种隐私保护数据共享模式，即通过最小最大化优化，确保授权方能重构数据，同时最大化两个非授权攻击者的损失，尤其关注需要攻击者协作才能重构的场景。


<details>
  <summary>Details</summary>
Motivation: 在数据共享中，需要平衡数据可用性与隐私保护。特别地，当单个非授权方难以准确重构数据，但多个非授权方联合起来却可能实现重构时，如何设计隐私化机制以阻止其未经授权的协作式信息获取，同时允许授权方重构，是一个重要问题。

Method: 将该隐私保护问题建模为一个受限的数据驱动型最小最大优化问题。提出了一种数据驱动的训练过程，该过程交替更新隐私化器、重构器和攻击者。此外，还分析了高斯和二元情况下的理论最优解，以作为评估所提训练方法的基准。

Result: 提出了一种有效的数据驱动型训练程序，用于解决隐私保护数据共享中的最小最大优化问题。对于高斯和二元特定场景，能够推导出理论最优解，这些解可用于评估和验证所提训练方法的性能。

Conclusion: 本研究为一种新颖的隐私保护数据共享设置（即在允许授权协作重构的同时，最大化非授权个体攻击者损失）提供了理论框架和实用的数据驱动型优化方法。通过特定案例的分析，为评估该方法的有效性提供了基准。

Abstract: We study a privacy-preserving data-sharing setting where a privatizer
transforms private data into a sanitized version observed by an authorized
reconstructor and two unauthorized adversaries, each with access to side
information correlated with the private data.
  The reconstructor is evaluated under a distortion function, while each
adversary is evaluated using a separate loss function. The privatizer ensures
the reconstructor distortion remains below a fixed threshold while maximizing
the minimum loss across the two adversaries. This two-adversary setting models
cases where individual users cannot reconstruct the data accurately, but their
combined side information enables estimation within the distortion threshold.
The privatizer maximizes individual loss while permitting accurate
reconstruction only through collaboration. This echoes secret-sharing
principles, but with lossy rather than perfect recovery. We frame this as a
constrained data-driven minimax optimization problem and propose a data-driven
training procedure that alternately updates the privatizer, reconstructor, and
adversaries. We also analyze the Gaussian and binary cases as special scenarios
where optimal solutions can be obtained. These theoretical optimal results are
benchmarks for evaluating the proposed minimax training approach.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [244] [Hybrid Reward-Driven Reinforcement Learning for Efficient Quantum Circuit Synthesis](https://arxiv.org/abs/2507.16641)
*Sara Giordano,Kornikar Sen,Miguel A. Martin-Delgado*

Main category: quant-ph

TL;DR: 本研究提出一种基于强化学习（RL）的框架，通过Q-learning和混合奖励机制，高效合成最小深度、优化门数的量子电路，以应对量子计算中的电路合成挑战。


<details>
  <summary>Details</summary>
Motivation: 量子电路合成是NISQ时代和未来容错量子计算中的核心挑战。需要一种有效方法，从固定初始态高效生成指定目标量子态的电路，并应对量子态空间的指数增长。

Method: 该方法采用基于动作序列的表格Q-learning算法，在离散化的量子态空间中操作。它引入了混合奖励机制：结合引导代理趋向目标态的静态领域奖励，以及惩罚门拥塞和冗余态访问等低效结构的动态惩罚。通过利用稀疏矩阵表示和态空间离散化，该方法实现了高维环境下的可扩展导航，同时最小化了计算开销。

Result: 在多达七个量子比特的图态制备任务中，该算法能持续发现最小深度且门数优化的电路。此外，将该框架扩展到任意量子态的通用门集时，它依然能产生最小深度的电路，这突显了算法的鲁棒性和适应性。

Conclusion: 该强化学习驱动的方法能够高效探索复杂的量子态空间，合成接近最优的量子电路，为量子电路优化奠定了一个资源高效的基础。

Abstract: A reinforcement learning (RL) framework is introduced for the efficient
synthesis of quantum circuits that generate specified target quantum states
from a fixed initial state, addressing a central challenge in both the NISQ era
and future fault-tolerant quantum computing. The approach utilizes tabular
Q-learning, based on action sequences, within a discretized quantum state
space, to effectively manage the exponential growth of the space dimension. The
framework introduces a hybrid reward mechanism, combining a static,
domain-informed reward that guides the agent toward the target state with
customizable dynamic penalties that discourage inefficient circuit structures
such as gate congestion and redundant state revisits. By leveraging sparse
matrix representations and state-space discretization, the method enables
scalable navigation of high-dimensional environments while minimizing
computational overhead. Benchmarking on graph-state preparation tasks for up to
seven qubits, we demonstrate that the algorithm consistently discovers
minimal-depth circuits with optimized gate counts. Moreover, extending the
framework to a universal gate set for arbitrary quantum states, it still
produces minimal depth circuits, highlighting the algorithm's robustness and
adaptability. The results confirm that this RL-driven approach efficiently
explores the complex quantum state space and synthesizes near-optimal quantum
circuits, providing a resource-efficient foundation for quantum circuit
optimization.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [245] [Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation](https://arxiv.org/abs/2507.17937)
*Jaechul Roh,Zachary Novack,Yuefeng Peng,Niloofar Mireshghallah,Taylor Berg-Kirkpatrick,Amir Houmansadr*

Main category: cs.SD

TL;DR: 研究发现，即使通过同音替换改变歌词语义，歌词转歌曲模型和文本转视频模型仍会记忆并重现训练数据中的相似音视频内容，揭示了多模态生成系统的关键漏洞。


<details>
  <summary>Details</summary>
Motivation: 歌词转歌曲（LS2）模型能够实现文本到音乐的端到端合成，然而其对训练数据记忆的脆弱性尚未得到充分探索。

Method: 引入“对抗性语音提示（Adversarial PhoneTic Prompting, APT）”这一新型攻击方法。该方法通过同音替换来改变歌词的语义，但保留其声学结构（例如，将“mom's spaghetti”替换为“Bob's confetti”）。

Result: ['LS2模型（如SUNO和YuE）表现出强大的子词汇记忆能力，即使歌词被扭曲，也能生成与已知训练内容高度相似的输出，并在多个音频指标（如CLAP、AudioJudge和CoverID）上获得高相似度，此漏洞存在于多种语言和流派中。', '更令人惊讶的是，仅通过语音修改的歌词，就能在文本转视频模型（如Veo 3）中触发视觉记忆，重构出原音乐视频中的视觉元素（包括角色外貌和场景构成），即使提示中没有视觉线索。这一现象被命名为“语音到视觉的反刍”。']

Conclusion: 这些发现揭示了基于转录的多模态生成模型中的一个关键漏洞：仅通过语音提示就能解锁模型记忆中的音视频内容，这引发了对现代生成系统中版权、安全性及内容来源的紧迫问题。

Abstract: Lyrics-to-Song (LS2) generation models promise end-to-end music synthesis
from text, yet their vulnerability to training data memorization remains
underexplored. We introduce Adversarial PhoneTic Prompting (APT), a novel
attack where lyrics are semantically altered while preserving their acoustic
structure through homophonic substitutions (e.g., Eminem's famous "mom's
spaghetti" $\rightarrow$ "Bob's confetti"). Despite these distortions, we
uncover a powerful form of sub-lexical memorization: models like SUNO and YuE
regenerate outputs strikingly similar to known training content, achieving high
similarity across audio-domain metrics, including CLAP, AudioJudge, and
CoverID. This vulnerability persists across multiple languages and genres. More
surprisingly, we discover that phoneme-altered lyrics alone can trigger visual
memorization in text-to-video models. When prompted with phonetically modified
lyrics from Lose Yourself, Veo 3 reconstructs visual elements from the original
music video -- including character appearance and scene composition -- despite
no visual cues in the prompt. We term this phenomenon phonetic-to-visual
regurgitation. Together, these findings expose a critical vulnerability in
transcript-conditioned multimodal generation: phonetic prompting alone can
unlock memorized audiovisual content, raising urgent questions about copyright,
safety, and content provenance in modern generative systems. Example
generations are available on our demo page (jrohsc.github.io/music_attack/).

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [246] [Technical Implementation of Tippy: Multi-Agent Architecture and System Design for Drug Discovery Laboratory Automation](https://arxiv.org/abs/2507.17852)
*Yao Fehlis,Charles Crain,Aidan Jensen,Michael Watson,James Juhasz,Paul Mandel,Betty Liu,Shawn Mahon,Daren Wilson,Nick Lynch-Jonely,Ben Leedom,David Fuller*

Main category: cs.MA

TL;DR: 本文对Tippy多智能体系统在药物发现实验室自动化中的技术实现进行了全面分析，该系统采用分布式微服务架构，通过AI代理协调复杂的工作流程，并具备安全、可扩展、可靠和与现有实验室基础设施集成的能力。


<details>
  <summary>Details</summary>
Motivation: 在前作关于制药研究中智能AI概念框架的基础上，本文旨在对Tippy多智能体系统在药物发现实验室自动化中的实施进行全面的技术分析。

Method: 该系统采用分布式微服务架构，包含五种专业代理（Supervisor、Molecule、Lab、Analysis、Report），通过OpenAI Agents SDK进行协调，并经由Model Context Protocol (MCP)访问实验室工具。系统设计包括代理专用工具集成、异步通信模式和基于Git的配置管理。生产部署利用Kubernetes进行容器编排，配合Helm charts、Docker容器化和CI/CD管道。实现中还集成了向量数据库用于RAG功能，并使用Envoy反向代理确保外部访问安全。

Result: 本研究展示了Tippy系统能够通过专业AI智能体有效地协调复杂的实验室工作流程，同时保持安全性、可扩展性、可靠性，并通过标准化协议与现有实验室基础设施良好集成。

Conclusion: 专业的AI智能体系统能够高效协调复杂的实验室工作流程，确保自动化过程中的安全性、可扩展性、可靠性，并实现与现有实验室基础设施的无缝集成。

Abstract: Building on the conceptual framework presented in our previous work on
agentic AI for pharmaceutical research, this paper provides a comprehensive
technical analysis of Tippy's multi-agent system implementation for drug
discovery laboratory automation. We present a distributed microservices
architecture featuring five specialized agents (Supervisor, Molecule, Lab,
Analysis, and Report) that coordinate through OpenAI Agents SDK orchestration
and access laboratory tools via the Model Context Protocol (MCP). The system
architecture encompasses agent-specific tool integration, asynchronous
communication patterns, and comprehensive configuration management through
Git-based tracking. Our production deployment strategy utilizes Kubernetes
container orchestration with Helm charts, Docker containerization, and CI/CD
pipelines for automated testing and deployment. The implementation integrates
vector databases for RAG functionality and employs an Envoy reverse proxy for
secure external access. This work demonstrates how specialized AI agents can
effectively coordinate complex laboratory workflows while maintaining security,
scalability, reliability, and integration with existing laboratory
infrastructure through standardized protocols.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [247] [OpenNav: Open-World Navigation with Multimodal Large Language Models](https://arxiv.org/abs/2507.18033)
*Mingfeng Yuan,Letian Wang,Steven L. Waslander*

Main category: cs.RO

TL;DR: 本文提出一个零样本视觉-语言导航框架，使机器人能解读复杂自然语言指令，并在开放世界中生成轨迹，通过结合多模态大语言模型（MLLMs）实现语义与空间理解。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练大语言模型在机器人导航和规划中表现出潜力，但将语言描述转化为实际机器人动作，尤其是在开放世界中超越预定义动作基元，仍是一个挑战。目标是让机器人能够解释复杂语言指令，并为开放集指令和物体合成轨迹点序列。

Method: 利用多模态大语言模型（MLLMs）的跨模态理解能力和代码生成能力。MLLMs与视觉-语言感知模型交互，生成组合式2D鸟瞰图价值地图，整合语义知识与空间信息。在大型自动驾驶车辆数据集（AVDs）上验证户外导航任务，并在Husky机器人上进行室内外真实世界验证。

Result: 该框架能够执行多样化的自由形式自然语言导航指令，并对物体检测错误和语言歧义保持鲁棒性。在真实世界中，系统在Husky机器人上展现了其鲁棒性和适用性。

Conclusion: 所提出的零样本视觉-语言导航框架成功弥合了语言指令与机器人实际动作之间的鸿沟，使机器人能在开放世界中，通过强大的跨模态理解和空间感知能力，实现对复杂自然语言指令的准确执行，且具备良好的鲁棒性和实用性。

Abstract: Pre-trained large language models (LLMs) have demonstrated strong
common-sense reasoning abilities, making them promising for robotic navigation
and planning tasks. However, despite recent progress, bridging the gap between
language descriptions and actual robot actions in the open-world, beyond merely
invoking limited predefined motion primitives, remains an open challenge. In
this work, we aim to enable robots to interpret and decompose complex language
instructions, ultimately synthesizing a sequence of trajectory points to
complete diverse navigation tasks given open-set instructions and open-set
objects. We observe that multi-modal large language models (MLLMs) exhibit
strong cross-modal understanding when processing free-form language
instructions, demonstrating robust scene comprehension. More importantly,
leveraging their code-generation capability, MLLMs can interact with
vision-language perception models to generate compositional 2D bird-eye-view
value maps, effectively integrating semantic knowledge from MLLMs with spatial
information from maps to reinforce the robot's spatial understanding. To
further validate our approach, we effectively leverage large-scale autonomous
vehicle datasets (AVDs) to validate our proposed zero-shot vision-language
navigation framework in outdoor navigation tasks, demonstrating its capability
to execute a diverse range of free-form natural language navigation
instructions while maintaining robustness against object detection errors and
linguistic ambiguities. Furthermore, we validate our system on a Husky robot in
both indoor and outdoor scenes, demonstrating its real-world robustness and
applicability. Supplementary videos are available at
https://trailab.github.io/OpenNav-website/

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [248] [ASR-Guided Speaker-Role Diarization and Diarization-Guided ASR Decoding](https://arxiv.org/abs/2507.17765)
*Arindam Ghosh,Mark Fuhs,Bongjun Kim,Anurag Chowdhury,Monika Woszczyna*

Main category: eess.AS

TL;DR: 提出一种新的端到端框架，将说话人角色识别（RD）集成到自动语音识别（ASR）中，通过简化训练、独立预测器和利用RD信息提升ASR解码来增强其应用价值。


<details>
  <summary>Details</summary>
Motivation: 与传统的说话人识别（SD）相比，说话人角色识别（RD）（如医生 vs. 病人）在实际应用中更具价值。现有联合ASR+SD模型需要扩展以处理RD，以满足更细粒度的应用需求。

Method: 在现有联合ASR+SD端到端模型的基础上，将框架扩展到RD，并提出三项关键改进：1) 通过强制对齐和交叉熵损失简化训练；2) 为词预测和角色预测采用独立的任务特定预测器，以适应不同的上下文需求；3) 利用RD后验活动来影响ASR解码，旨在减少小词删除错误。

Result: 通过提出的简化训练流程和独立的任务特定预测器，模型能够更有效地进行说话人角色识别。同时，利用RD的后验信息成功改善了ASR解码性能，特别是在减少小词删除错误方面取得了进展。

Conclusion: 本研究成功地将更具应用价值的说话人角色识别集成到端到端ASR框架中，通过优化训练方法和模型结构，并创新性地利用角色信息辅助ASR，显著提升了联合系统的实用性和性能。

Abstract: From an application standpoint, speaker-role diarization (RD), such as doctor
vs. patient, host vs. guest, etc. is often more useful than traditional speaker
diarization (SD), which assigns generic labels like speaker-1, speaker-2 etc.
In the context of joint automatic speech recognition (ASR) + SD (who spoke
what?), recent end-to-end models employ an auxiliary SD transducer,
synchronized with the ASR transducer, to predict speakers per word. In this
paper, we extend this framework to RD with three key contributions: (1) we
simplify the training via forced alignment and cross-entropy loss instead of
RNNT loss, (2) we show that word prediction and role prediction require
different amounts of predictor's context, leading to separate task-specific
predictors, unlike existing shared-predictor models, and (3) we propose a way
to leverage RD posterior activity to influence ASR decoding and reduce
small-word deletion errors.

</details>


### [249] [Recent Trends in Distant Conversational Speech Recognition: A Review of CHiME-7 and 8 DASR Challenges](https://arxiv.org/abs/2507.18161)
*Samuele Cornell,Christoph Boeddeker,Taejin Park,He Huang,Desh Raj,Matthew Wiesner,Yoshiki Masuyama,Xuankai Chang,Zhong-Qiu Wang,Stefano Squartini,Paola Garcia,Shinji Watanabe*

Main category: eess.AS

TL;DR: CHiME-7和8 DASR挑战分析了远场多通道语音识别和说话人日志的技术趋势，发现端到端ASR普及、传统分离方法仍重要、说话人计数关键，且LLM能缓解转录错误影响，但复杂环境下转录仍困难。


<details>
  <summary>Details</summary>
Motivation: CHiME-7和8远场语音识别（DASR）挑战旨在推动多通道、通用化、联合自动语音识别（ASR）和会话语音说话人日志（Diarization）的研究。

Method: 本文概述了挑战的设计、评估指标、数据集和基线系统，并分析了9支团队提交的32个系统中的关键趋势。

Result: ['大多数参与者转向端到端ASR系统，受益于大型预训练模型。', '团队仍高度依赖引导式源分离，表明当前神经语音分离/增强技术在复杂场景中尚不足。', '所有最佳系统都采用目标说话人日志技术进行日志精修，强调了准确说话人计数的重要性。', '由于大语言模型处理错误的能力，下游评估（如会议摘要）与转录质量的相关性可能较弱。', '在挑战性声学环境中准确转录自发语音仍然困难，即使使用计算密集型系统集成。']

Conclusion: CHiME-7和8 DASR挑战展示了远场语音识别的进步，尤其是端到端ASR的普及，但也凸显了复杂环境下的语音分离、准确说话人日志和自发语音转录的持续挑战。同时，大语言模型对转录错误的鲁棒性改变了对转录质量影响的认知。

Abstract: The CHiME-7 and 8 distant speech recognition (DASR) challenges focus on
multi-channel, generalizable, joint automatic speech recognition (ASR) and
diarization of conversational speech. With participation from 9 teams
submitting 32 diverse systems, these challenges have contributed to
state-of-the-art research in the field. This paper outlines the challenges'
design, evaluation metrics, datasets, and baseline systems while analyzing key
trends from participant submissions. From this analysis it emerges that: 1)
Most participants use end-to-end (e2e) ASR systems, whereas hybrid systems were
prevalent in previous CHiME challenges. This transition is mainly due to the
availability of robust large-scale pre-trained models, which lowers the data
burden for e2e-ASR. 2) Despite recent advances in neural speech separation and
enhancement (SSE), all teams still heavily rely on guided source separation,
suggesting that current neural SSE techniques are still unable to reliably deal
with complex scenarios and different recording setups. 3) All best systems
employ diarization refinement via target-speaker diarization techniques.
Accurate speaker counting in the first diarization pass is thus crucial to
avoid compounding errors and CHiME-8 DASR participants especially focused on
this part. 4) Downstream evaluation via meeting summarization can correlate
weakly with transcription quality due to the remarkable effectiveness of
large-language models in handling errors. On the NOTSOFAR-1 scenario, even
systems with over 50\% time-constrained minimum permutation WER can perform
roughly on par with the most effective ones (around 11\%). 5) Despite recent
progress, accurately transcribing spontaneous speech in challenging acoustic
environments remains difficult, even when using computationally intensive
system ensembles.

</details>
