<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.CV](#cs.CV) [Total: 89]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.NI](#cs.NI) [Total: 5]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.CR](#cs.CR) [Total: 4]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [eess.IV](#eess.IV) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [cs.RO](#cs.RO) [Total: 12]
- [math.NA](#math.NA) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [math.AP](#math.AP) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.IT](#cs.IT) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.CY](#cs.CY) [Total: 7]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [cs.MA](#cs.MA) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/abs/2505.02847)
*Bang Zhang,Ruotian Ma,Qingxuan Jiang,Peisong Wang,Jiaqi Chen,Zheng Xie,Xingyu Chen,Yue Wang,Fanghua Ye,Jian Li,Yifan Yang,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

TL;DR: SAGE是一个自动化评估框架，通过模拟具有情感变化和内在想法的“感知代理”来衡量大型语言模型的高级社交认知能力。


<details>
  <summary>Details</summary>
Motivation: 目前评估大型语言模型（LLM）是否真正理解人类而非仅仅理解文本仍然是一个挑战，需要新的方法来弥合这一差距。

Method: 引入SAGE（Sentient Agent as a Judge）框架。该框架实例化一个“感知代理”，在多轮对话中模拟类似人类的情感变化和内在想法。代理在每一轮都会推理其情感变化、感受以及如何回应，从而产生数值化的情感轨迹和可解释的内在想法。

Result: 实验表明，SAGE的最终情感得分与心理学量表（BLRI）及共情指标强相关，验证了其心理保真度。基于SAGE构建的排行榜揭示了前沿模型与早期模型之间存在显著差距，而这些差距在传统排行榜中并未体现。

Conclusion: SAGE为追踪和评估语言代理在共情能力和社交智能方面的进展提供了一个有原则、可扩展且可解释的工具。

Abstract: Assessing how well a large language model (LLM) understands human, rather
than merely text, remains an open challenge. To bridge the gap, we introduce
Sentient Agent as a Judge (SAGE), an automated evaluation framework that
measures an LLM's higher-order social cognition. SAGE instantiates a Sentient
Agent that simulates human-like emotional changes and inner thoughts during
interaction, providing a more realistic evaluation of the tested model in
multi-turn conversations. At every turn, the agent reasons about (i) how its
emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a
numerical emotion trajectory and interpretable inner thoughts. Experiments on
100 supportive-dialogue scenarios show that the final Sentient emotion score
correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings
and utterance-level empathy metrics, validating psychological fidelity. We also
build a public Sentient Leaderboard covering 18 commercial and open-source
models that uncovers substantial gaps (up to 4x) between frontier systems
(GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in
conventional leaderboards (e.g., Arena). SAGE thus provides a principled,
scalable and interpretable tool for tracking progress toward genuinely
empathetic and socially adept language agents.

</details>


### [2] [Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors](https://arxiv.org/abs/2505.02850)
*Nicy Scaria,Silvester John Joseph Kennedy,Diksha Seth,Ananya Thakur,Deepak Subramani*

Main category: cs.CL

TL;DR: 论文提出了一种基于分层概念图的框架，指导大型语言模型（LLM）生成高质量、针对不同认知水平并包含常见错误概念的高中物理选择题。


<details>
  <summary>Details</summary>
Motivation: 手动创建高质量、涵盖不同认知水平并包含常见错误概念的选择题耗时费力，而现有的自动化方法通常生成的题目认知水平较低，且未能有效融入领域特定的错误概念。

Method: 首先为高中物理构建了一个分层概念图数据库。然后，通过自动化流程检索与主题相关的概念图部分，为LLM提供结构化知识，以生成针对常见错误概念的选择题和干扰项。最后，进行自动验证以确保生成的选择题符合要求。

Result: 专家评估显示，该方法在满足所有质量标准方面的成功率为75.20%，显著优于基线方法（约37%）。学生评估数据显示，该方法生成的题目猜测成功率更低（28.05%对37.10%），表明能更有效地评估学生的概念理解。

Conclusion: 基于概念图的方法能够实现跨认知水平的稳健评估，即时识别概念差距，从而促进更快的反馈循环和大规模的针对性干预。

Abstract: Generating high-quality MCQs, especially those targeting diverse cognitive
levels and incorporating common misconceptions into distractor design, is
time-consuming and expertise-intensive, making manual creation impractical at
scale. Current automated approaches typically generate questions at lower
cognitive levels and fail to incorporate domain-specific misconceptions. This
paper presents a hierarchical concept map-based framework that provides
structured knowledge to guide LLMs in generating MCQs with distractors. We
chose high-school physics as our test domain and began by developing a
hierarchical concept map covering major Physics topics and their
interconnections with an efficient database design. Next, through an automated
pipeline, topic-relevant sections of these concept maps are retrieved to serve
as a structured context for the LLM to generate questions and distractors that
specifically target common misconceptions. Lastly, an automated validation is
completed to ensure that the generated MCQs meet the requirements provided. We
evaluate our framework against two baseline approaches: a base LLM and a
RAG-based generation. We conducted expert evaluations and student assessments
of the generated MCQs. Expert evaluation shows that our method significantly
outperforms the baseline approaches, achieving a success rate of 75.20% in
meeting all quality criteria compared to approximately 37% for both baseline
methods. Student assessment data reveal that our concept map-driven approach
achieved a significantly lower guess success rate of 28.05% compared to 37.10%
for the baselines, indicating a more effective assessment of conceptual
understanding. The results demonstrate that our concept map-based approach
enables robust assessment across cognitive levels and instant identification of
conceptual gaps, facilitating faster feedback loops and targeted interventions
at scale.

</details>


### [3] [30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation](https://arxiv.org/abs/2505.02851)
*Franklin Zhang,Sonya Zhang,Alon Halevy*

Main category: cs.CL

TL;DR: 本文介绍了一款名为“30 Day Me”的习惯养成应用，该应用利用大型语言模型（LLM）帮助用户分解目标、制定行动步骤并跟踪进展，其核心系统“30DAYGEN”能生成并搜索30天挑战计划。


<details>
  <summary>Details</summary>
Motivation: 旨在展示如何利用大型语言模型（LLM）快速构建特定领域（如行为和教育）的内容语料库，并提出一个实用的流程来支持如习惯养成等应用。

Method: 开发了“30 Day Me”应用，其核心“30DAYGEN”系统利用LLM从超过15000个网页中提取数据，生成了3531个独特的30天挑战，并支持用户根据目标实时搜索。同时，提出了一套结合LLM增强进行内容生成和语义去重的实用流程。

Result: 成功构建了“30 Day Me”应用及其核心系统“30DAYGEN”，后者生成了3531个独特的30天挑战。研究展示了LLM在快速构建领域特定内容语料库方面的潜力，并提出了一个结合LLM增强的内容生成和语义去重实用流程。

Conclusion: 大型语言模型（LLM）可以被有效利用来为行为和教育目的快速构建领域特定内容。“30 Day Me”应用及所提出的流程证明了LLM在内容生成和语义去重方面的实用性。

Abstract: In this paper, we present 30 Day Me, a habit formation application that
leverages Large Language Models (LLMs) to help users break down their goals
into manageable, actionable steps and track their progress. Central to the app
is the 30DAYGEN system, which generates 3,531 unique 30-day challenges sourced
from over 15K webpages, and enables runtime search of challenge ideas aligned
with user-defined goals. We showcase how LLMs can be harnessed to rapidly
construct domain specific content corpora for behavioral and educational
purposes, and propose a practical pipeline that incorporates effective LLM
enhanced approaches for content generation and semantic deduplication.

</details>


### [4] [Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets](https://arxiv.org/abs/2505.02854)
*Masumi Morishige,Ryo Koshihara*

Main category: cs.CL

TL;DR: 引入 GPR-bench，一个轻量级、可扩展的基准测试，用于对通用型生成式 AI 进行回归测试，以解决其可复现性和可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 生成式 AI 系统的行为会因模型更新或提示修改而发生变化，其可复现性和可靠性面临严峻挑战，需要有效的回归测试工具。

Method: 提出 GPR-bench，一个包含8个任务类别、双语（英日）数据集的开源基准测试，并使用“LLM-as-a-Judge”的自动化评估流程对正确性和简洁性进行评分。通过对三种模型版本和两种提示配置进行实验。

Result: 较新模型在正确性上略有提升但不显著，表明 GPR-bench 可能对最新模型区分度不足。然而，简洁性指令显著提升了输出的简洁度（+12.37 pp），同时对准确性影响微乎其微（-1.7 pp），证明了提示工程的有效性。

Conclusion: GPR-bench 降低了可复现性监控的门槛，并为社区扩展提供了基础。研究结果强调了提示工程的重要性，并引发了对快速发展的语言模型基准设计的思考。

Abstract: Reproducibility and reliability remain pressing challenges for generative AI
systems whose behavior can drift with each model update or prompt revision. We
introduce GPR-bench, a lightweight, extensible benchmark that operationalizes
regression testing for general purpose use cases. GPR-bench couples an open,
bilingual (English and Japanese) dataset covering eight task categories (e.g.,
text generation, code generation, and information retrieval) and 10 scenarios
in each task categories (80 total test cases for each language) with an
automated evaluation pipeline that employs "LLM-as-a-Judge" scoring of
correctness and conciseness. Experiments across three recent model versions -
gpt-4o-mini, o3-mini, and o4-mini - and two prompt configurations (default
versus concise-writing instruction) reveal heterogeneous quality. Our results
show that newer models generally improve correctness, but the differences are
modest and not statistically significant, suggesting that GPR-bench may not be
sufficiently challenging to differentiate between recent model versions. In
contrast, the concise-writing instruction significantly enhances conciseness
(+12.37 pp, Mann-Whitney U test: p < 0.001, effect size r = 0.2995) with
minimal degradations on accuracy (-1.7 pp), demonstrating the effectiveness of
prompt engineering. Released under the MIT License, GPR- bench lowers the
barrier to initiating reproducibility monitoring and provides a foundation for
community-driven extensions, while also raising important considerations about
benchmark design for rapidly evolving language models.

</details>


### [5] [Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models](https://arxiv.org/abs/2505.02858)
*Henry Tari,Nojus Sereiva,Rishabh Kaushal,Thales Bertaglia,Adriana Iamnitchi*

Main category: cs.CL

TL;DR: 该研究探讨了使用大型语言模型（LLM）生成跨多个平台的合成社交媒体数据集的潜力，以解决真实数据获取受限的问题。


<details>
  <summary>Details</summary>
Motivation: 由于成本和平台限制，获取真实社交媒体数据集（尤其是对于理解数字生态系统至关重要的跨平台数据集）非常困难。

Method: 提出多平台主题提示方法，利用多种大型语言模型从两个真实的跨平台数据集中生成合成数据，并评估其词汇和语义属性与真实数据的相似度。

Result: 研究发现，使用大型语言模型生成合成多平台社交媒体数据是具有前景的。不同语言模型在保真度方面表现各异，可能需要后处理方法来生成高保真度的合成数据集。此外，研究还贡献了针对多平台社交媒体数据集的新保真度指标。

Conclusion: 使用大型语言模型生成多平台合成社交媒体数据是一种有前景的方法，但模型选择和后处理对于生成高保真度研究数据至关重要。

Abstract: Social media datasets are essential for research on a variety of topics, such
as disinformation, influence operations, hate speech detection, or influencer
marketing practices. However, access to social media datasets is often
constrained due to costs and platform restrictions. Acquiring datasets that
span multiple platforms, which is crucial for understanding the digital
ecosystem, is particularly challenging. This paper explores the potential of
large language models to create lexically and semantically relevant social
media datasets across multiple platforms, aiming to match the quality of real
data. We propose multi-platform topic-based prompting and employ various
language models to generate synthetic data from two real datasets, each
consisting of posts from three different social media platforms. We assess the
lexical and semantic properties of the synthetic data and compare them with
those of the real data. Our empirical findings show that using large language
models to generate synthetic multi-platform social media data is promising,
different language models perform differently in terms of fidelity, and a
post-processing approach might be needed for generating high-fidelity synthetic
datasets for research. In addition to the empirical evaluation of three state
of the art large language models, our contributions include new fidelity
metrics specific to multi-platform social media datasets.

</details>


### [6] [Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI](https://arxiv.org/abs/2505.02859)
*Jonas Bokstaller,Julia Altheimer,Julian Dormehl,Alina Buss,Jasper Wiltfang,Johannes Schneider,Maximilian Röglinger*

Main category: cs.CL

TL;DR: 本文提出了一种结合可解释AI (XAI) 和大型语言模型 (LLM) 的新颖参考架构，通过微调LLM驱动的交互式聊天机器人来解释XAI，旨在提高机器学习模型的可解释性，尤其对缺乏XAI经验的用户。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习 (ML) 模型的“黑箱”特性日益明显，推动了可解释AI (XAI) 的应用发展。同时，大型语言模型 (LLM) 在理解人类语言和复杂模式方面取得了显著进展，这为两者结合提供了契机。

Method: 提出了一种新颖的参考架构：通过一个由微调的大型语言模型 (LLM) 驱动的交互式聊天机器人来解释XAI。该架构在电池健康状态 (SoH) 预测场景下进行了实例化，并通过多轮评估和演示进行了验证。

Result: 评估表明，所实现的原型增强了机器学习的人类可解释性，特别是对于那些XAI经验较少的用户而言。

Conclusion: 该研究提出的结合LLM和XAI的交互式聊天机器人方法，能够有效提升机器学习模型对用户的可解释性，尤其有助于降低非专业用户理解XAI的门槛。

Abstract: Across various sectors applications of eXplainableAI (XAI) gained momentum as
the increasing black-boxedness of prevailing Machine Learning (ML) models
became apparent. In parallel, Large Language Models (LLMs) significantly
developed in their abilities to understand human language and complex patterns.
By combining both, this paper presents a novel reference architecture for the
interpretation of XAI through an interactive chatbot powered by a fine-tuned
LLM. We instantiate the reference architecture in the context of
State-of-Health (SoH) prediction for batteries and validate its design in
multiple evaluation and demonstration rounds. The evaluation indicates that the
implemented prototype enhances the human interpretability of ML, especially for
users with less experience with XAI.

</details>


### [7] [Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](https://arxiv.org/abs/2505.02862)
*Haoming Yang,Ke Ma,Xiaojun Jia,Yingfei Sun,Qianqian Xu,Qingming Huang*

Main category: cs.CL

TL;DR: 本文提出了一种名为 ICRT 的新型 LLM 越狱攻击框架，该框架受人类认知启发，通过认知分解和提示重组来绕过安全机制，并引入了基于排序的危害性评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）越狱攻击研究大多依赖暴力优化或手动设计，未能充分揭示真实场景中的潜在风险，且缺乏对生成内容危害性的全面评估。

Method: 提出了 ICRT 攻击框架：1) 利用认知启发中的“简单性效应”进行认知分解，以降低恶意提示的复杂性；2) 利用“相关性偏见”重组提示，增强语义对齐以有效诱导有害输出；3) 引入一种基于排序的危害性评估指标（采用 Elo、HodgeRank、Rank Centrality 等方法），超越传统的二元成功/失败判断模式，全面量化生成内容的危害性。

Result: 实验结果表明，所提出的 ICRT 方法能够持续绕过主流大型语言模型的安全机制，并生成高风险内容。

Conclusion: ICRT 框架有效地揭示了 LLM 面临的越狱攻击风险，其研究结果为理解这些风险并构建更强大的防御策略提供了有价值的见解。

Abstract: Despite the remarkable performance of Large Language Models (LLMs), they
remain vulnerable to jailbreak attacks, which can compromise their safety
mechanisms. Existing studies often rely on brute-force optimization or manual
design, failing to uncover potential risks in real-world scenarios. To address
this, we propose a novel jailbreak attack framework, ICRT, inspired by
heuristics and biases in human cognition. Leveraging the simplicity effect, we
employ cognitive decomposition to reduce the complexity of malicious prompts.
Simultaneously, relevance bias is utilized to reorganize prompts, enhancing
semantic alignment and inducing harmful outputs effectively. Furthermore, we
introduce a ranking-based harmfulness evaluation metric that surpasses the
traditional binary success-or-failure paradigm by employing ranking aggregation
methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify
the harmfulness of generated content. Experimental results show that our
approach consistently bypasses mainstream LLMs' safety mechanisms and generates
high-risk content, providing insights into jailbreak attack risks and
contributing to stronger defense strategies.

</details>


### [8] [Accelerating Large Language Model Reasoning via Speculative Search](https://arxiv.org/abs/2505.02865)
*Zhihai Wang,Jie Wang,Jilai Pan,Xilin Xia,Huiling Zhen,Mingxuan Yuan,Jianye Hao,Feng Wu*

Main category: cs.CL

TL;DR: 提出SpecSearch框架，通过小模型与大模型在思想和词元层面协作，并结合质量保留拒绝机制，加速LLM推理，同时保持高质量推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于树搜索的大型语言模型（LLM）推理方法虽然能提升推理能力，但因需生成大量中间推理步骤（思想），导致推理延迟过高，限制了其实际应用。

Method: 提出一种新颖的推测性搜索（SpecSearch）框架。该框架利用一个小模型在思想（thought）和词元（token）层面与一个大模型进行策略性协作，以优化思想生成。其核心是一种新颖的质量保留拒绝机制，有效过滤掉质量未达到大模型输出标准的思想。

Result: 在Qwen和Llama模型上的实验表明，与现有最先进的方法相比，SpecSearch在保持相当推理质量的前提下，实现了高达2.12倍的推理速度提升。

Conclusion: SpecSearch框架能够显著加速大型语言模型的推理过程，同时保持与大模型相当的推理质量，为解决现有树搜索方法的高延迟问题提供了有效方案。

Abstract: Tree-search-based reasoning methods have significantly enhanced the reasoning
capability of large language models (LLMs) by facilitating the exploration of
multiple intermediate reasoning steps, i.e., thoughts. However, these methods
suffer from substantial inference latency, as they have to generate numerous
reasoning thoughts, severely limiting LLM applicability. To address this
challenge, we propose a novel Speculative Search (SpecSearch) framework that
significantly accelerates LLM reasoning by optimizing thought generation.
Specifically, SpecSearch utilizes a small model to strategically collaborate
with a large model at both thought and token levels, efficiently generating
high-quality reasoning thoughts. The major pillar of SpecSearch is a novel
quality-preserving rejection mechanism, which effectively filters out thoughts
whose quality falls below that of the large model's outputs. Moreover, we show
that SpecSearch preserves comparable reasoning quality to the large model.
Experiments on both the Qwen and Llama models demonstrate that SpecSearch
significantly outperforms state-of-the-art approaches, achieving up to
2.12$\times$ speedup with comparable reasoning quality.

</details>


### [9] [Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading](https://arxiv.org/abs/2505.02872)
*Cfir Avraham Hadar,Omer Shubi,Yoav Meiri,Yevgeni Berzak*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型（LLMs）可以从阅读时的眼动数据中成功解码出读者的特定阅读目标。


<details>
  <summary>Details</summary>
Motivation: 人们阅读时常带有特定的信息搜寻目标，本研究首次探索是否可以从眼动数据中自动解码这些开放式的阅读目标。

Method: 研究者引入了目标分类和目标重构任务，使用了大规模英文阅读眼动追踪数据，并开发和比较了多种结合眼动和文本信息的多模态大型语言模型。

Result: 实验在目标分类和目标重构任务上均取得了显著成功。

Conclusion: 大型语言模型可以从眼动数据中提取关于读者特定文本阅读目标的有价值信息。

Abstract: When reading, we often have specific information that interests us in a text.
For example, you might be reading this paper because you are curious about LLMs
for eye movements in reading, the experimental design, or perhaps you only care
about the question ``but does it work?''. More broadly, in daily life, people
approach texts with any number of text-specific goals that guide their reading
behavior. In this work, we ask, for the first time, whether open-ended reading
goals can be automatically decoded from eye movements in reading. To address
this question, we introduce goal classification and goal reconstruction tasks
and evaluation frameworks, and use large-scale eye tracking for reading data in
English with hundreds of text-specific information seeking tasks. We develop
and compare several discriminative and generative multimodal LLMs that combine
eye movements and text for goal classification and goal reconstruction. Our
experiments show considerable success on both tasks, suggesting that LLMs can
extract valuable information about the readers' text-specific goals from eye
movements.

</details>


### [10] [Logits-Constrained Framework with RoBERTa for Ancient Chinese NER](https://arxiv.org/abs/2505.02983)
*Wenjie Hua,Shenghan Xu*

Main category: cs.CL

TL;DR: 本文提出了一种用于古汉语命名实体识别 (NER) 的 Logits 约束 (LC) 框架，该框架在 EvaHan 2025 基准上展现了优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 提升古汉语命名实体识别的性能，特别是在处理高标签密度或大规模数据集的场景。

Method: 采用一个两阶段模型：首先使用 GujiRoBERTa 进行上下文编码，然后通过一个可微分的解码机制来强制执行有效的 BMES 标签转换。

Result: 实验证明，LC 框架在性能上优于传统的 CRF 和 BiLSTM 方法，尤其是在高标签或大规模数据集的设置中。同时，论文还提出了一个平衡标签复杂度和数据集大小的模型选择标准。

Conclusion: LC 框架为古汉语命名实体识别提供了一种有效的方法，并为实际应用中的模型选择提供了实用指导。

Abstract: This paper presents a Logits-Constrained (LC) framework for Ancient Chinese
Named Entity Recognition (NER), evaluated on the EvaHan 2025 benchmark. Our
two-stage model integrates GujiRoBERTa for contextual encoding and a
differentiable decoding mechanism to enforce valid BMES label transitions.
Experiments demonstrate that LC improves performance over traditional CRF and
BiLSTM-based approaches, especially in high-label or large-data settings. We
also propose a model selection criterion balancing label complexity and dataset
size, providing practical guidance for real-world Ancient Chinese NLP tasks.

</details>


### [11] [RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale](https://arxiv.org/abs/2505.03005)
*Daniel Goldstein,Eric Alcaide,Janna Lu,Eugene Cheah*

Main category: cs.CL

TL;DR: 该研究提出RADLADS协议，能快速将softmax注意力Transformer模型转换为线性注意力解码器模型，转换成本低、所需数据少，且性能接近原始模型。


<details>
  <summary>Details</summary>
Motivation: 传统的softmax注意力Transformer模型计算成本高昂，尤其是在处理长序列时。研究旨在开发一种快速且经济的方法，将这些强大的模型转换为更高效的线性注意力解码器模型，同时保持其性能。

Method: 提出了RADLADS（Rapid Attention Distillation to Linear Attention Decoders at Scale）协议，通过注意力蒸馏，使用极少量数据（3.5亿-7亿tokens，少于原始模型训练数据的0.005%）将预训练的softmax注意力Transformer模型（如Qwen2.5的7B、32B、72B版本）转换为新的RWKV变体线性注意力解码器架构。

Result: 转换过程成本极低（例如72B模型转换成本低于2000美元），转换后的线性注意力模型在推理质量上接近原始Transformer模型。这些模型在同规模线性注意力模型的标准基准测试中达到了SOTA（state-of-the-art）的下游任务性能。

Conclusion: RADLADS提供了一种高效、经济的方法，能够用极少的数据和成本将大规模softmax注意力Transformer模型转换为高性能的线性注意力解码器，从而提高了大型模型的推理效率和可及性。

Abstract: We present Rapid Attention Distillation to Linear Attention Decoders at Scale
(RADLADS), a protocol for rapidly converting softmax attention transformers
into linear attention decoder models, along with two new RWKV-variant
architectures, and models converted from popular Qwen2.5 open source models in
7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,
less than 0.005% of the token count used to train the original teacher models.
Converting to our 72B linear attention model costs less than \$2,000 USD at
today's prices, yet quality at inference remains close to the original
transformer. These models achieve state-of-the-art downstream performance
across a set of standard benchmarks for linear attention models of their size.
We release all our models on HuggingFace under the Apache 2.0 license, with the
exception of our 72B models which are also governed by the Qwen License
Agreement.
  Models at
https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102
Training Code at https://github.com/recursal/RADLADS-paper

</details>


### [12] [Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis](https://arxiv.org/abs/2505.03019)
*Albérick Euraste Djiré,Abdoul Kader Kaboré,Earl T. Barr,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: 本文提出PEARL方法，通过评估大语言模型对输入扰动的敏感性来检测其记忆行为，无需访问模型内部结构。


<details>
  <summary>Details</summary>
Motivation: 大语言模型可能逐字复现训练数据而非真正泛化，引发了数据隐私、知识产权和模型评估可靠性的担忧。

Method: PEARL通过评估大语言模型（LLM）的性能对输入扰动的敏感程度来检测记忆。它研究输入扰动如何影响输出的一致性，从而区分真正的泛化和记忆，且无需访问模型内部。

Result: 在Pythia开放模型上的实验证明PEARL能有效识别模型何时仅仅是“反刍”学习到的信息。应用于GPT-4o模型时，PEARL识别了对《圣经》和HumanEval代码的记忆，并提供了证据表明《纽约时报》文章等数据可能在其训练集中。

Conclusion: PEARL提供了一个强大的框架来识别LLM中的记忆行为，有助于区分模型的真实泛化能力和对训练数据的简单复述，对解决数据隐私和知识产权问题具有重要意义。

Abstract: While Large Language Models (LLMs) achieve remarkable performance through
training on massive datasets, they can exhibit concerning behaviors such as
verbatim reproduction of training data rather than true generalization. This
memorization phenomenon raises significant concerns about data privacy,
intellectual property rights, and the reliability of model evaluations. This
paper introduces PEARL, a novel approach for detecting memorization in LLMs.
PEARL assesses how sensitive an LLM's performance is to input perturbations,
enabling memorization detection without requiring access to the model's
internals. We investigate how input perturbations affect the consistency of
outputs, enabling us to distinguish between true generalization and
memorization. Our findings, following extensive experiments on the Pythia open
model, provide a robust framework for identifying when the model simply
regurgitates learned information. Applied on the GPT 4o models, the PEARL
framework not only identified cases of memorization of classic texts from the
Bible or common code from HumanEval but also demonstrated that it can provide
supporting evidence that some data, such as from the New York Times news
articles, were likely part of the training data of a given model.

</details>


### [13] [A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts](https://arxiv.org/abs/2505.03025)
*Steven Bedrick,A. Seza Doğruöz,Sergiu Nisioi*

Main category: cs.CL

TL;DR: 本文综述了医学领域中合成对话数据集的创建、评估和使用，并提出了一种新的数据合成类型分类法。


<details>
  <summary>Details</summary>
Motivation: 由于隐私、匿名化和数据治理等挑战，真实的临床（尤其是对话）数据难以获取，促使合成数据集不断发展。然而，现有理论不足以指导如何最佳地使用这些合成数据集并将其推广到新应用。

Method: 本文回顾了医学领域中用于对话相关任务的合成数据集的创建、评估和使用方法。此外，还提出了一种新的用于分类数据合成类型和程度的分类法。

Result: 提供了关于医学领域合成数据集创建、评估和应用的概述。提出了一种新的数据合成分类法，旨在促进不同合成数据集之间的比较和评估。

Conclusion: 该论文提出的综述和分类法有助于更好地理解、比较和评估医学合成数据集，并为它们在未来应用中的使用和泛化提供指导。

Abstract: Synthetic data sets are used across linguistic domains and NLP tasks,
particularly in scenarios where authentic data is limited (or even
non-existent). One such domain is that of clinical (healthcare) contexts, where
there exist significant and long-standing challenges (e.g., privacy,
anonymization, and data governance) which have led to the development of an
increasing number of synthetic datasets. One increasingly important category of
clinical dataset is that of clinical dialogues which are especially sensitive
and difficult to collect, and as such are commonly synthesized.
  While such synthetic datasets have been shown to be sufficient in some
situations, little theory exists to inform how they may be best used and
generalized to new applications. In this paper, we provide an overview of how
synthetic datasets are created, evaluated and being used for dialogue related
tasks in the medical domain. Additionally, we propose a novel typology for use
in classifying types and degrees of data synthesis, to facilitate comparison
and evaluation.

</details>


### [14] [UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output](https://arxiv.org/abs/2505.03030)
*Sicong Huang,Jincheng He,Shiyuan Huang,Karthik Raja Anandan,Arkajyoti Chakraborty,Ian Lane*

Main category: cs.CL

TL;DR: 本文介绍了UCSC团队为SemEval 2025 Task 3 (Mu-SHROOM) 提交的系统，该系统旨在检测和定位大型语言模型（LLM）输出中的幻觉，并取得了最佳的整体性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在回答知识密集型查询时面临幻觉的重大挑战。随着LLM的广泛应用，不仅要检测幻觉是否发生，还要精确指出它们在LLM输出中的具体位置至关重要。SemEval 2025 Task 3, Mu-SHROOM，正是为了解决这一问题而设立的。

Method: 研究团队引入了一个框架，该框架首先检索相关上下文，接着从答案中识别错误内容，最后将这些错误内容映射回LLM输出中的具体文本段落。此过程通过自动优化提示词得到进一步增强。

Result: 该系统取得了最高的整体性能，在所有语言的平均排名中位列第一。

Conclusion: UCSC团队提交的系统有效地解决了LLM中幻觉检测和定位的问题，并在Mu-SHROOM共享任务中表现出色。他们公开了其代码和实验结果。

Abstract: Hallucinations pose a significant challenge for large language models when
answering knowledge-intensive queries. As LLMs become more widely adopted, it
is crucial not only to detect if hallucinations occur but also to pinpoint
exactly where in the LLM output they occur. SemEval 2025 Task 3, Mu-SHROOM:
Multilingual Shared-task on Hallucinations and Related Observable
Overgeneration Mistakes, is a recent effort in this direction. This paper
describes the UCSC system submission to the shared Mu-SHROOM task. We introduce
a framework that first retrieves relevant context, next identifies false
content from the answer, and finally maps them back to spans in the LLM output.
The process is further enhanced by automatically optimizing prompts. Our system
achieves the highest overall performance, ranking #1 in average position across
all languages. We release our code and experiment results.

</details>


### [15] [Teaching Models to Understand (but not Generate) High-risk Data](https://arxiv.org/abs/2505.03052)
*Ryan Wang,Matthew Finlayson,Luca Soldaini,Swabha Swayamdipta,Robin Jia*

Main category: cs.CL

TL;DR: 提出了一种名为SLUNG的预训练范式，使模型能够理解高风险数据而不生成它。


<details>
  <summary>Details</summary>
Motivation: 传统的语言模型开发会过滤掉高风险内容（如毒性或受版权保护的文本），但这限制了模型识别和适当响应有害或敏感内容的能力。

Method: 引入了选择性损失（Selective Loss to Understand but Not Generate, SLUNG）的预训练范式。该方法选择性地避免激励高风险词元的生成，同时确保它们保留在模型的上下文窗口中，迫使模型通过预测高风险词元之后的低风险词元来理解高风险内容。

Result: 实验表明，SLUNG显著提高了模型对高风险数据的理解能力（例如，识别毒性内容的能力），而不会增加其生成高风险内容的倾向（例如，模型响应的毒性）。

Conclusion: SLUNG范式使模型能够从那些原本会被过滤掉的高风险文本中受益，从而在理解高风险内容的同时不生成它们。

Abstract: Language model developers typically filter out high-risk content -- such as
toxic or copyrighted text -- from their pre-training data to prevent models
from generating similar outputs. However, removing such data altogether limits
models' ability to recognize and appropriately respond to harmful or sensitive
content. In this paper, we introduce Selective Loss to Understand but Not
Generate (SLUNG), a pre-training paradigm through which models learn to
understand high-risk data without learning to generate it. Instead of uniformly
applying the next-token prediction loss, SLUNG selectively avoids incentivizing
the generation of high-risk tokens while ensuring they remain within the
model's context window. As the model learns to predict low-risk tokens that
follow high-risk ones, it is forced to understand the high-risk content.
Through our experiments, we show that SLUNG consistently improves models'
understanding of high-risk data (e.g., ability to recognize toxic content)
without increasing its generation (e.g., toxicity of model responses). Overall,
our SLUNG paradigm enables models to benefit from high-risk text that would
otherwise be filtered out.

</details>


### [16] [Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text](https://arxiv.org/abs/2505.03053)
*Jennifer Healey,Laurie Byrum,Md Nadeem Akhtar,Surabhi Bhargava,Moumita Sinha*

Main category: cs.CL

TL;DR: 本文介绍了一个以人类洞察为核心的半自动化框架，用于评估大型语言模型（LLM）在自由文本响应中的偏见。


<details>
  <summary>Details</summary>
Motivation: 现有LLM偏见评估方法，如基于简短上下文的固定选择基准，在模型实际部署时有效性降低，而大规模人工评估成本高昂且难以实施。

Method: 开发了一个半自动化的偏见评估框架，该框架以人类洞察为核心，针对自由文本响应。具体包括：1) 制定可操作的偏见定义；2) 基于该定义自动化评估流程；3) 开发超越多项选择题的偏见分类方法。

Result: 成功开发了所提出的半自动化偏见评估框架。人工评估过程还帮助识别并揭示了现有偏见基准测试中存在问题的模板。

Conclusion: 本文提出并阐述了一个结合人类洞察的半自动化偏见评估框架的开发过程，旨在更有效地评估LLM在自由文本响应中的偏见，并解决现有方法的不足。

Abstract: LLM evaluation is challenging even the case of base models. In real world
deployments, evaluation is further complicated by the interplay of task
specific prompts and experiential context. At scale, bias evaluation is often
based on short context, fixed choice benchmarks that can be rapidly evaluated,
however, these can lose validity when the LLMs' deployed context differs. Large
scale human evaluation is often seen as too intractable and costly. Here we
present our journey towards developing a semi-automated bias evaluation
framework for free text responses that has human insights at its core. We
discuss how we developed an operational definition of bias that helped us
automate our pipeline and a methodology for classifying bias beyond multiple
choice. We additionally comment on how human evaluation helped us uncover
problematic templates in a bias benchmark.

</details>


### [17] [Improving Model Alignment Through Collective Intelligence of Open-Source LLMS](https://arxiv.org/abs/2505.03059)
*Junlin Wang,Roy Xie,Shang Zhu,Jue Wang,Ben Athiwaratkun,Bhuwan Dhingra,Shuaiwen Leon Song,Ce Zhang,James Zou*

Main category: cs.CL

TL;DR: 本文介绍了一种名为“混合智能体对齐”（MoAA）的新方法，通过利用多个语言模型的集体智慧来生成高质量数据，以提升大语言模型的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 构建有用且无害的大语言模型需要基于人类指令和反馈的有效对齐方法，但这依赖于高质量的人工标注数据。此类数据集的构建通常成本高昂、难以扩展，且在多样性和泛化能力方面存在潜在限制。

Method: 提出了“混合智能体对齐”（MoAA）方法。该方法利用各种语言模型的集体优势来为模型对齐提供高质量数据，从而增强监督微调和偏好优化。

Result: 与单独使用单个模型（如GPT-4o）生成对齐数据相比，MoAA显著提升了LLaMA-3.1-8B-Instruct模型在Arena-Hard上的胜率（从19.5%到48.3%）和在AlpacaEval2上的胜率（从22.33%到57.23%）。此外，MoAA还支持自我改进流程，即在MoAA生成数据上微调的模型能够超越其初始能力。

Conclusion: MoAA为模型对齐提供了一种有前景的、可扩展且多样化的合成数据方案，能够在不依赖更强外部监督的情况下推动开源大语言模型的发展。

Abstract: Building helpful and harmless large language models (LLMs) requires effective
model alignment approach based on human instructions and feedback, which
necessitates high-quality human-labeled data. Constructing such datasets is
often expensive and hard to scale, and may face potential limitations on
diversity and generalization. To address these challenges, we introduce Mixture
of Agents Alignment (MoAA), that leverages the collective strengths of various
language models to provide high-quality data for model alignment. By employing
MoAA, we enhance both supervised fine-tuning and preference optimization,
leading to improved performance compared to using a single model alone to
generate alignment data (e.g. using GPT-4o alone). Evaluation results show that
our approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on
Arena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising
direction for model alignment through this new scalable and diverse synthetic
data recipe. Furthermore, we demonstrate that MoAA enables a self-improvement
pipeline, where models finetuned on MoA-generated data surpass their own
initial capabilities, providing evidence that our approach can push the
frontier of open-source LLMs without reliance on stronger external supervision.
Data and code will be released.

</details>


### [18] [Survey of Abstract Meaning Representation: Then, Now, Future](https://arxiv.org/abs/2505.03229)
*Behrooz Mansouri*

Main category: cs.CL

TL;DR: 这篇论文是对抽象语义表示 (AMR) 的综述，探讨了其定义、扩展、解析与生成任务、应用及未来方向。


<details>
  <summary>Details</summary>
Motivation: 系统性回顾抽象语义表示 (AMR) 框架，分析其能力、应用和挑战，旨在为提升机器对人类语言的理解提供研究方向和洞见。

Method: 本文采用文献综述的方法，调研和分析了 AMR 的核心概念、其扩展、能力、AMR 解析 (text-to-AMR) 与生成 (AMR-to-text) 任务（涵盖传统、当前及未来方法），以及 AMR 在文本生成、分类、信息提取等领域的各种应用。

Result: 该综述分析了 AMR 领域的最新进展和挑战，提供了关于未来研究方向的见解，并阐述了 AMR 在增强机器理解人类语言方面的潜在影响。

Conclusion: AMR 是一个有前景的语义表示框架。本综述通过对 AMR 的能力、关键任务、应用、最新进展和挑战的分析，为该领域的未来研究提供了有价值的见解，并强调了其在提升机器对人类语言理解方面的潜力。

Abstract: This paper presents a survey of Abstract Meaning Representation (AMR), a
semantic representation framework that captures the meaning of sentences
through a graph-based structure. AMR represents sentences as rooted, directed
acyclic graphs, where nodes correspond to concepts and edges denote
relationships, effectively encoding the meaning of complex sentences. This
survey investigates AMR and its extensions, focusing on AMR capabilities. It
then explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by
showing traditional, current, and possible futures approaches. It also reviews
various applications of AMR including text generation, text classification, and
information extraction and information seeking. By analyzing recent
developments and challenges in the field, this survey provides insights into
future directions for research and the potential impact of AMR on enhancing
machine understanding of human language.

</details>


### [19] [Ψ-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback](https://arxiv.org/abs/2505.03293)
*Shijing Zhu,Zhuang Chen,Guanqun Bi,Binghang Li,Yaxi Deng,Dazhen Wan,Libiao Peng,Xiyao Xiao,Rongsheng Zhang,Tangjie Lv,Zhipeng Hu,FangFang Li,Minlie Huang*

Main category: cs.CL

TL;DR: 该研究提出了 Ψ-Arena，一个用于综合评估和优化基于大语言模型的心理咨询师的交互式框架，以解决现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型心理咨询能力的评估方法在静态性、单一视角和缺乏可操作反馈方面存在局限，这阻碍了开发有效且安全的心理健康支持工具。

Method: 提出了 Ψ-Arena 框架，其特点包括：(1) 通过与具有心理特征的NPC客户进行多阶段对话来模拟真实咨询场景；(2) 从客户、咨询师和督导三个角度进行综合评估；(3) 利用诊断反馈进行闭环优化以迭代改进大语言模型咨询师。

Result: 对八个先进大语言模型的实验表明，它们在不同的真实场景和评估视角下表现出显著的性能差异。此外，基于反思的优化使咨询性能提升高达141%。

Conclusion: Ψ-Arena 为推进可靠且与人类价值观对齐的大语言模型在心理健康领域的应用提供了基础资源，并证明了通过针对性优化可以显著提升大语言模型咨询师的性能。

Abstract: Large language models (LLMs) have shown promise in providing scalable mental
health support, while evaluating their counseling capability remains crucial to
ensure both efficacy and safety. Existing evaluations are limited by the static
assessment that focuses on knowledge tests, the single perspective that centers
on user experience, and the open-loop framework that lacks actionable feedback.
To address these issues, we propose {\Psi}-Arena, an interactive framework for
comprehensive assessment and optimization of LLM-based counselors, featuring
three key characteristics: (1) Realistic arena interactions that simulate
real-world counseling through multi-stage dialogues with psychologically
profiled NPC clients, (2) Tripartite evaluation that integrates assessments
from the client, counselor, and supervisor perspectives, and (3) Closed-loop
optimization that iteratively improves LLM counselors using diagnostic
feedback. Experiments across eight state-of-the-art LLMs show significant
performance variations in different real-world scenarios and evaluation
perspectives. Moreover, reflection-based optimization results in up to a 141%
improvement in counseling performance. We hope PsychoArena provides a
foundational resource for advancing reliable and human-aligned LLM applications
in mental healthcare.

</details>


### [20] [Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation](https://arxiv.org/abs/2505.03320)
*Junyu Ma,Tianqing Fang,Zhisong Zhang,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出了一种名为“带推理的回忆”（RwR）的方法，通过使用教师模型生成的思维链（CoT）摘要来微调Mamba模型，以解锁其处理超长序列的潜力。


<details>
  <summary>Details</summary>
Motivation: Mamba模型虽然理论上具有处理无限长上下文的潜力，但在实际应用中，当序列长度远超训练长度时，其性能会受到限制。

Method: 提出了一种名为“带推理的回忆”（Recall with Reasoning, RwR）的方法。该方法通过从一个教师模型中提炼思维链（Chain-of-Thought, CoT）式的摘要，并将这些摘要作为提示（prompts）在Mamba模型微调（fine-tuning）阶段前置，从而训练Mamba主动回忆和推理长上下文内容。

Result: 在LONGMEMEVAL和HELMET基准测试上的实验表明，RwR方法显著提升了Mamba模型在长上下文任务上的性能，优于在相似预训练条件下的Transformer及混合架构基线模型。同时，该方法保留了Mamba在短上下文任务上的能力，并且无需对模型架构进行任何修改。

Conclusion: RwR方法是一种简单而有效的技术，能够解锁Mamba模型的长上下文记忆能力，使其在处理长序列时表现更佳，同时不牺牲短上下文性能或需要改变模型结构。

Abstract: Mamba's theoretical infinite-context potential is limited in practice when
sequences far exceed training lengths. This work explores unlocking Mamba's
long-context memory ability by a simple-yet-effective method, Recall with
Reasoning (RwR), by distilling chain-of-thought (CoT) summarization from a
teacher model. Specifically, RwR prepends these summarization as CoT prompts
during fine-tuning, teaching Mamba to actively recall and reason over long
contexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's
long-context performance against comparable Transformer/hybrid baselines under
similar pretraining conditions, while preserving short-context capabilities,
all without architectural changes.

</details>


### [21] [Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation](https://arxiv.org/abs/2505.03406)
*Mohammad Shoaib Ansari,Mohd Sohail Ali Khan,Shubham Revankar,Aditya Varma,Anil S. Mokhade*

Main category: cs.CL

TL;DR: 研究将大型语言模型（LLM）Llama 3.2-3B-Instruct与检索增强生成（RAG）及QLoRA微调技术相结合，应用于医疗决策支持，整合医院数据以提升准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 旨在通过利用大型语言模型、医院特定数据和高效微调技术，提升医疗决策支持的准确性、效率，并解决模型在资源受限环境下的部署问题。

Method: 采用Llama 3.2-3B-Instruct为基础模型，通过检索增强生成（RAG）整合医院特定医疗信息，并使用量化低秩自适应（QLoRA）技术进行微调，以优化模型参数和内存占用。

Result: 系统显著提升了医疗问答的响应准确性，实现了参数高效性和内存优化。在多个医疗基准测试中表现良好，证明其可用于提供初步医疗建议，且量化模型易于在低资源医院部署。

Conclusion: 结合RAG和QLoRA微调的LLM能有效增强医疗决策支持，提高准确性和部署效率，但需关注伦理考量和实际集成挑战。该技术对医疗健康领域具有广泛影响和未来发展潜力。

Abstract: This research paper investigates the application of Large Language Models
(LLMs) in healthcare, specifically focusing on enhancing medical decision
support through Retrieval-Augmented Generation (RAG) integrated with
hospital-specific data and fine-tuning using Quantized Low-Rank Adaptation
(QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By
embedding and retrieving context-relevant healthcare information, the system
significantly improves response accuracy. QLoRA facilitates notable parameter
efficiency and memory optimization, preserving the integrity of medical
information through specialized quantization techniques. Our research also
shows that our model performs relatively well on various medical benchmarks,
indicating that it can be used to make basic medical suggestions. This paper
details the system's technical components, including its architecture,
quantization methods, and key healthcare applications such as enhanced disease
prediction from patient symptoms and medical history, treatment suggestions,
and efficient summarization of complex medical reports. We touch on the ethical
considerations-patient privacy, data security, and the need for rigorous
clinical validation-as well as the practical challenges of integrating such
systems into real-world healthcare workflows. Furthermore, the lightweight
quantized weights ensure scalability and ease of deployment even in
low-resource hospital environments. Finally, the paper concludes with an
analysis of the broader impact of LLMs on healthcare and outlines future
directions for LLMs in medical settings.

</details>


### [22] [MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks](https://arxiv.org/abs/2505.03427)
*Mouath Abu Daoud,Chaimae Abouzahir,Leen Kharouf,Walid Al-Eisawi,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

TL;DR: 本研究介绍了 MedArabiQ，一个包含七项任务的新型阿拉伯语医疗基准数据集，用于评估大型语言模型（LLMs）在该领域的表现，并强调了多语言高质量基准对医疗保健公平部署LLMs的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在医疗保健领域展现出巨大潜力，但由于缺乏高质量的特定领域数据集和基准，其在阿拉伯语医疗领域的有效性仍有待探索。

Method: 研究团队首先利用过去的医学考试和公开数据集构建了 MedArabiQ 数据集，该数据集包含七项阿拉伯语医疗任务，涵盖选择题、填空题和医患问答等多种题型及多个医学专科。随后，对数据集进行了修改以评估 LLMs 的各种能力，包括偏见缓解。最后，使用五个先进的开源和专有 LLMs（包括 GPT-4o, Claude 3.5-Sonnet, 和 Gemini 1.5）进行了广泛评估。

Result: 评估结果凸显了创建新的、涵盖不同语言的高质量基准的迫切需求，以确保大型语言模型在医疗保健领域的公平部署和可扩展性。

Conclusion: 通过建立此基准并发布数据集，本研究为未来旨在评估和增强 LLMs 多语言能力的研究奠定了基础，从而促进生成式人工智能在医疗保健领域的公平应用。

Abstract: Large Language Models (LLMs) have demonstrated significant promise for
various applications in healthcare. However, their efficacy in the Arabic
medical domain remains unexplored due to the lack of high-quality
domain-specific datasets and benchmarks. This study introduces MedArabiQ, a
novel benchmark dataset consisting of seven Arabic medical tasks, covering
multiple specialties and including multiple choice questions,
fill-in-the-blank, and patient-doctor question answering. We first constructed
the dataset using past medical exams and publicly available datasets. We then
introduced different modifications to evaluate various LLM capabilities,
including bias mitigation. We conducted an extensive evaluation with five
state-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude
3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of
new high-quality benchmarks that span different languages to ensure fair
deployment and scalability of LLMs in healthcare. By establishing this
benchmark and releasing the dataset, we provide a foundation for future
research aimed at evaluating and enhancing the multilingual capabilities of
LLMs for the equitable use of generative AI in healthcare.

</details>


### [23] [An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation](https://arxiv.org/abs/2505.03452)
*Matan Orbach,Ohad Eytan,Benjamin Sznajder,Ariel Gera,Odellia Boni,Yoav Kantor,Gal Bloch,Omri Levy,Hadas Abraham,Nitzan Barzilay,Eyal Shnarch,Michael E. Factor,Shila Ofek-Koifman,Paula Ta-Shma,Assaf Toledo*

Main category: cs.CL

TL;DR: 该研究对检索增强生成（RAG）的超参数优化（HPO）进行了基准测试，发现贪婪搜索或迭代随机搜索等高效方法能显著提升RAG在不同数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 为特定用例寻找最优的检索增强生成（RAG）配置既复杂又昂贵。尽管最近出现了RAG超参数优化（HPO）框架，但其有效性尚未得到严格的基准测试。

Method: 研究者们进行了一项全面的研究，涉及5种HPO算法在5个来自不同领域的数据集（包括一个为此工作收集的关于真实世界产品文档的新数据集）上的表现。该研究探索了迄今为止最大的HPO搜索空间，并采用了两种优化的评估指标。

Result: 分析结果表明，RAG HPO可以高效地完成，无论是通过贪婪算法还是迭代随机搜索，并且它显著提升了所有数据集上的RAG性能。对于贪婪HPO方法，研究显示优先优化模型比按RAG流程顺序进行优化的普遍做法更可取。

Conclusion: 该研究证实了RAG HPO的有效性，并指出通过贪婪搜索或迭代随机搜索可以高效实现优化。此外，研究还提出了一种更优的贪婪HPO策略，即应优先优化模型，而非遵循传统的RAG流程顺序优化。

Abstract: Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a
given use case can be complex and expensive. Motivated by this challenge,
frameworks for RAG hyper-parameter optimization (HPO) have recently emerged,
yet their effectiveness has not been rigorously benchmarked. To address this
gap, we present a comprehensive study involving 5 HPO algorithms over 5
datasets from diverse domains, including a new one collected for this work on
real-world product documentation. Our study explores the largest HPO search
space considered to date, with two optimized evaluation metrics. Analysis of
the results shows that RAG HPO can be done efficiently, either greedily or with
iterative random search, and that it significantly boosts RAG performance for
all datasets. For greedy HPO approaches, we show that optimizing models first
is preferable to the prevalent practice of optimizing sequentially according to
the RAG pipeline order.

</details>


### [24] [Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis](https://arxiv.org/abs/2505.03467)
*Shuang Zhou,Jiashuo Wang,Zidu Xu,Song Wang,David Brauer,Lindsay Welton,Jacob Cogan,Yuen-Hei Chung,Lei Tian,Zaifu Zhan,Yu Hou,Mingquan Lin,Genevieve B. Melton,Rui Zhang*

Main category: cs.CL

TL;DR: 该研究介绍了ConfiDx，一个经过微调的大语言模型，旨在识别和解释医学诊断中的不确定性，以提高自动诊断系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 临床笔记中证据不足会导致诊断不确定性，增加误诊风险。现有诊断系统在明确识别和解释这些不确定性方面研究不足。

Method: 研究人员提出了ConfiDx，一个通过诊断标准微调开源大语言模型（LLM）得到的不确定性感知模型。他们还形式化了任务并构建了捕捉不同诊断模糊程度的带注释数据集。

Result: 在真实世界数据集上的评估表明，ConfiDx在识别诊断不确定性、实现卓越诊断性能以及为诊断和不确定性生成可信解释方面表现优异。

Conclusion: ConfiDx是首个共同解决诊断不确定性识别和解释的研究，显著增强了自动诊断系统的可靠性。

Abstract: Explainable disease diagnosis, which leverages patient information (e.g.,
signs and symptoms) and computational models to generate probable diagnoses and
reasonings, offers clear clinical values. However, when clinical notes
encompass insufficient evidence for a definite diagnosis, such as the absence
of definitive symptoms, diagnostic uncertainty usually arises, increasing the
risk of misdiagnosis and adverse outcomes. Although explicitly identifying and
explaining diagnostic uncertainties is essential for trustworthy diagnostic
systems, it remains under-explored. To fill this gap, we introduce ConfiDx, an
uncertainty-aware large language model (LLM) created by fine-tuning open-source
LLMs with diagnostic criteria. We formalized the task and assembled richly
annotated datasets that capture varying degrees of diagnostic ambiguity.
Evaluating ConfiDx on real-world datasets demonstrated that it excelled in
identifying diagnostic uncertainties, achieving superior diagnostic
performance, and generating trustworthy explanations for diagnoses and
uncertainties. To our knowledge, this is the first study to jointly address
diagnostic uncertainty recognition and explanation, substantially enhancing the
reliability of automatic diagnostic systems.

</details>


### [25] [Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models](https://arxiv.org/abs/2505.03469)
*Bin Yu,Hang Yuan,Yuliang Wei,Bailing Wang,Weizhen Qi,Kai Chen*

Main category: cs.CL

TL;DR: 提出了一种名为LS-Mixture SFT的新方法，通过混合长短思维链数据进行监督微调，旨在赋予非推理模型推理能力，同时解决“过度思考”问题，实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 通过教师模型的思维链（CoT）数据进行监督微调（SFT）虽然能有效迁移推理能力，但会导致微调后的模型继承“过度思考”的问题，产生冗长的推理链。

Method: 提出了长短思维链混合监督微调（LS-Mixture SFT）方法，该方法将长的CoT推理数据与其经过结构保留重写得到的短版本数据相结合进行模型微调。

Result: 与直接SFT训练的模型相比，使用LS-Mixture SFT训练的模型在多个基准测试中平均准确率提高了2.3%，并且模型响应长度显著减少了约47.61%。

Conclusion: LS-Mixture SFT方法能够通过监督微调赋予非推理模型推理能力，同时有效避免了从教师模型继承的过度思考问题，从而实现了微调模型的有效推理。

Abstract: Recent advances in large language models have demonstrated that Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from
large reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning
capabilities to non-reasoning models. However, models fine-tuned with this
approach inherit the "overthinking" problem from teacher models, producing
verbose and redundant reasoning chains during inference. To address this
challenge, we propose \textbf{L}ong-\textbf{S}hort Chain-of-Thought
\textbf{Mixture} \textbf{S}upervised \textbf{F}ine-\textbf{T}uning
(\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their
short counterparts obtained through structure-preserved rewriting. Our
experiments demonstrate that models trained using the LS-Mixture SFT method,
compared to those trained with direct SFT, achieved an average accuracy
improvement of 2.3\% across various benchmarks while substantially reducing
model response length by approximately 47.61\%. This work offers an approach to
endow non-reasoning models with reasoning capabilities through supervised
fine-tuning while avoiding the inherent overthinking problems inherited from
teacher models, thereby enabling efficient reasoning in the fine-tuned models.

</details>


### [26] [Evaluation of LLMs on Long-tail Entity Linking in Historical Documents](https://arxiv.org/abs/2505.03473)
*Marta Boscariol,Luana Bulla,Lia Draetta,Beatrice Fiumanò,Emanuele Lenzi,Leonardo Piano*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型（LLMs）如GPT和LLama3在处理不常见的“长尾”实体链接任务中的表现，发现它们展现出良好潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在实体链接方面表现出色，但对于在训练数据和知识库中代表性不足的长尾实体，其链接效果仍具挑战性，且相关研究较少。

Method: 研究人员使用了一个名为MHERCL v0.1的手动注释基准数据集（包含领域特定的历史文本），对GPT和LLama3在识别实体并将其链接到Wikidata条目上的性能进行了定量比较，并与先进的实体链接与关系提取框架ReLiK进行了对比。

Result: 初步实验结果显示，LLMs在长尾实体链接任务中表现令人鼓舞。

Conclusion: LLMs技术有望成为弥合头部实体链接与长尾实体链接之间差距的有价值的补充工具。

Abstract: Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)
applications, enabling the disambiguation of entity mentions by linking them to
their corresponding entries in a reference knowledge base (KB). Thanks to their
deep contextual understanding capabilities, LLMs offer a new perspective to
tackle EL, promising better results than traditional methods. Despite the
impressive generalization capabilities of LLMs, linking less popular, long-tail
entities remains challenging as these entities are often underrepresented in
training data and knowledge bases. Furthermore, the long-tail EL task is an
understudied problem, and limited studies address it with LLMs. In the present
work, we assess the performance of two popular LLMs, GPT and LLama3, in a
long-tail entity linking scenario. Using MHERCL v0.1, a manually annotated
benchmark of sentences from domain-specific historical texts, we quantitatively
compare the performance of LLMs in identifying and linking entities to their
corresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity
Linking and Relation Extraction framework. Our preliminary experiments reveal
that LLMs perform encouragingly well in long-tail EL, indicating that this
technology can be a valuable adjunct in filling the gap between head and
long-tail EL.

</details>


### [27] [Sentence Embeddings as an intermediate target in end-to-end summarisation](https://arxiv.org/abs/2505.03481)
*Maciej Zembrzuski,Saad Mahamood*

Main category: cs.CL

TL;DR: 提出了一种结合抽取式和抽象式方法，并利用预训练句子嵌入来改进长篇用户评论摘要的新方法。


<details>
  <summary>Details</summary>
Motivation: 当前的神经网络摘要方法在处理包含大量输入的文档时性能不佳，尤其是在用户评论摘要领域。

Method: 将抽取式方法（使用外部预训练的句子级嵌入）与抽象式摘要模型相结合。此外，还通过预测摘要的句子级嵌入来优化模型。

Result: 该组合方法在总结大型输入数据集（用户评论）时优于现有方法。对于源文本与目标摘要对齐不紧密的语料库，预测摘要的句子级嵌入比预测句子选择概率更能提高端到端系统的摘要质量。

Conclusion: 结合抽取式和抽象式方法并利用预训练句子嵌入，是处理长篇用户评论摘要的有效途径。对于对齐不紧密的语料库，预测摘要的句子嵌入是一种比传统句子选择概率预测更优的策略。

Abstract: Current neural network-based methods to the problem of document summarisation
struggle when applied to datasets containing large inputs. In this paper we
propose a new approach to the challenge of content-selection when dealing with
end-to-end summarisation of user reviews of accommodations. We show that by
combining an extractive approach with externally pre-trained sentence level
embeddings in an addition to an abstractive summarisation model we can
outperform existing methods when this is applied to the task of summarising a
large input dataset. We also prove that predicting sentence level embedding of
a summary increases the quality of an end-to-end system for loosely aligned
source to target corpora, than compared to commonly predicting probability
distributions of sentence selection.

</details>


### [28] [Faster MoE LLM Inference for Extremely Large Models](https://arxiv.org/abs/2505.03531)
*Haoqi Yang,Luohe Shi,Qiwei Li,Zuchao Li,Ping Wang,Bo Du,Mengjia Shen,Hai Zhao*

Main category: cs.CL

TL;DR: 本文研究了细粒度专家混合（MoE）大语言模型的效率动态，探讨了减少路由专家数量对效率和性能的影响，并发现通过优化可显著提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型优化主要集中于粗粒度架构，而新兴的细粒度MoE模型（如DeepSeek模型）相关研究有限，特别是在不同服务负载下的效率动态以及减少路由专家对效率与性能权衡的影响方面。

Method: 通过分析在不同服务负载下，减少细粒度MoE模型中激活专家数量和总专家数量，来评估其对模型效率和性能的影响，并基于此提出优化策略。

Result: 研究表明，部署MoE模型虽具挑战，但也提供了显著优化机会。在某些场景下，减少激活专家数量能大幅提高效率，性能仅轻微下降；而减少总专家数量的效率增益有限，却导致严重的性能退化。所提出的方法可以在不降低任何性能的情况下将吞吐量提高至少10%。

Conclusion: MoE推理优化仍然是一个具有巨大探索和改进潜力的领域。

Abstract: Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually
becoming the mainstream approach for ultra-large-scale models. Existing
optimization efforts for MoE models have focused primarily on coarse-grained
MoE architectures. With the emergence of DeepSeek Models, fine-grained MoE
models are gaining popularity, yet research on them remains limited. Therefore,
we want to discuss the efficiency dynamic under different service loads.
Additionally, fine-grained models allow deployers to reduce the number of
routed experts, both activated counts and total counts, raising the question of
how this reduction affects the trade-off between MoE efficiency and
performance. Our findings indicate that while deploying MoE models presents
greater challenges, it also offers significant optimization opportunities.
Reducing the number of activated experts can lead to substantial efficiency
improvements in certain scenarios, with only minor performance degradation.
Reducing the total number of experts provides limited efficiency gains but
results in severe performance degradation. Our method can increase throughput
by at least 10\% without any performance degradation. Overall, we conclude that
MoE inference optimization remains an area with substantial potential for
exploration and improvement.

</details>


### [29] [Say It Another Way: A Framework for User-Grounded Paraphrasing](https://arxiv.org/abs/2505.03563)
*Cléa Chataigner,Rebecca Ma,Prakhar Ganesh,Afaf Taïk,Elliot Creager,Golnoosh Farnadi*

Main category: cs.CL

TL;DR: 该研究提出一种生成自然提示变体的方法，发现大语言模型对提示词的细微改动非常敏感，这强调了鲁棒评估方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型 (LLM) 的行为对提示词的微小变化很敏感，导致其评估的稳定性和可靠性受到质疑。现有研究在探索提示词变化时，未能充分捕捉真实世界语言使用中的自然变体。

Method: 提出一个基于最小语言转换分类法的受控释义框架，以系统地生成自然的提示变体。使用 BBQ 数据集进行人工标注和自动检查来验证该方法，并将其用于研究 LLM 在刻板印象评估任务中对释义提示的响应。

Result: 分析表明，即使是细微的提示修改也会导致模型行为发生重大变化。

Conclusion: 研究结果强调了开发能够抵抗释义影响的、鲁棒的评估协议的必要性。

Abstract: Small changes in how a prompt is worded can lead to meaningful differences in
the behavior of large language models (LLMs), raising concerns about the
stability and reliability of their evaluations. While prior work has explored
simple formatting changes, these rarely capture the kinds of natural variation
seen in real-world language use. We propose a controlled paraphrasing framework
based on a taxonomy of minimal linguistic transformations to systematically
generate natural prompt variations. Using the BBQ dataset, we validate our
method with both human annotations and automated checks, then use it to study
how LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our
analysis shows that even subtle prompt modifications can lead to substantial
changes in model behavior. These results highlight the need for robust,
paraphrase-aware evaluation protocols.

</details>


### [30] [Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure](https://arxiv.org/abs/2505.03675)
*Anuja Tayal,Devika Salunke,Barbara Di Eugenio,Paula G Allen-Meares,Eulalia P Abril,Olga Garcia-Bedoya,Carolyn A Dickens,Andrew D. Boyd*

Main category: cs.CL

TL;DR: 研究探讨了ChatGPT在生成针对非裔美国心力衰竭患者自我护理对话方面的潜力，发现提示设计至关重要，但模型仍缺乏医疗沟通所需的同理心。


<details>
  <summary>Details</summary>
Motivation: 针对非裔美国心力衰竭患者的自我护理这一特定领域，缺乏专门的对话数据集。

Method: 研究使用ChatGPT（3.5-turbo和4）和四种提示策略（领域特定、非裔美国人白话英语AAVE、健康社会决定因素SDOH、SDOH知情推理）来模拟患者与健康教育者之间关于食物、运动和液体摄入的自我护理对话，并考虑了不同的对话长度和患者特定的SDOH属性。

Result: 有效的提示设计至关重要。虽然结合SDOH和推理能够提高对话质量，但ChatGPT在生成对话时仍然缺乏进行有意义医疗保健沟通所需的同理心和参与度。

Conclusion: 尽管通过精心设计的提示可以改善对话质量，但目前的ChatGPT在模拟真实医疗沟通所需的共情和互动方面仍有不足。

Abstract: We explore the potential of ChatGPT (3.5-turbo and 4) to generate
conversations focused on self-care strategies for African-American heart
failure patients -- a domain with limited specialized datasets. To simulate
patient-health educator dialogues, we employed four prompting strategies:
domain, African American Vernacular English (AAVE), Social Determinants of
Health (SDOH), and SDOH-informed reasoning. Conversations were generated across
key self-care domains of food, exercise, and fluid intake, with varying turn
lengths (5, 10, 15) and incorporated patient-specific SDOH attributes such as
age, gender, neighborhood, and socioeconomic status. Our findings show that
effective prompt design is essential. While incorporating SDOH and reasoning
improves dialogue quality, ChatGPT still lacks the empathy and engagement
needed for meaningful healthcare communication.

</details>


### [31] [IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages](https://arxiv.org/abs/2505.03688)
*Sharvi Endait,Ruturaj Ghatage,Aditya Kulkarni,Rajlaxmi Patil,Raviraj Joshi*

Main category: cs.CL

TL;DR: 论文介绍了 IndicSQuAD，一个覆盖九种主要印度语言的多语言抽取式问答数据集，旨在解决印度语言在问答系统中代表性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 问答系统的快速发展主要惠及高资源语言，而拥有大量母语者的印度语言在此领域代表性不足。

Method: 通过系统地从 SQuAD 数据集派生，并借鉴先前 MahaSQuAD (马拉地语) 的工作，改进并扩展了翻译技术，以保持跨多种语言的高语言保真度和准确的答案跨度对齐。为每种语言创建了训练、验证和测试集，并使用特定语言的单语 BERT 模型和多语言 MuRIL-BERT 评估了基线性能。

Result: 成功构建了 IndicSQuAD 数据集。基线模型评估结果表明，在低资源环境下存在固有的挑战。

Conclusion: IndicSQuAD 为印度语言的问答模型开发提供了坚实基础，并为未来工作指明了方向，包括扩展到更多语言、开发特定领域数据集和整合多模态数据。数据集和模型已公开共享。

Abstract: The rapid progress in question-answering (QA) systems has predominantly
benefited high-resource languages, leaving Indic languages largely
underrepresented despite their vast native speaker base. In this paper, we
present IndicSQuAD, a comprehensive multi-lingual extractive QA dataset
covering nine major Indic languages, systematically derived from the SQuAD
dataset. Building on previous work with MahaSQuAD for Marathi, our approach
adapts and extends translation techniques to maintain high linguistic fidelity
and accurate answer-span alignment across diverse languages. IndicSQuAD
comprises extensive training, validation, and test sets for each language,
providing a robust foundation for model development. We evaluate baseline
performances using language-specific monolingual BERT models and the
multilingual MuRIL-BERT. The results indicate some challenges inherent in
low-resource settings. Moreover, our experiments suggest potential directions
for future work, including expanding to additional languages, developing
domain-specific datasets, and incorporating multimodal data. The dataset and
models are publicly shared at https://github.com/l3cube-pune/indic-nlp

</details>


### [32] [NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation](https://arxiv.org/abs/2505.03711)
*Baharul Islam,Nasim Ahmad,Ferdous Ahmed Barbhuiya,Kuntal Dey*

Main category: cs.CL

TL;DR: 本文介绍了一个用于 SemEval 2025 任务 5 的跨语言主题分类系统，该系统针对英语和德语学术领域，采用双语数据、负采样、基于边界的检索目标以及降维自注意力机制进行句子嵌入编码。


<details>
  <summary>Details</summary>
Motivation: 解决英语和德语学术领域中跨语言主题分类的挑战，并探索在资源受限情况下有效的句子嵌入编码方法。

Method: 1. 利用双语数据进行训练；2. 采用负采样策略；3. 使用基于边界的检索目标函数；4. 设计了一种内部维度显著减少的“维度即词元”（dimension-as-token）自注意力机制来编码句子嵌入，用于主题检索。

Result: 在通用量化评估中，系统平均召回率为32.24%；在通用定性评估方法中，召回率分别为43.16%和31.53%。该系统在GPU使用量极少的情况下表现出有竞争力的性能。

Conclusion: 该方法在资源受限的情况下能够有效捕捉相关主题信息，证明了其有效性，但仍有改进空间。

Abstract: We present our system submission for SemEval 2025 Task 5, which focuses on
cross-lingual subject classification in the English and German academic
domains. Our approach leverages bilingual data during training, employing
negative sampling and a margin-based retrieval objective. We demonstrate that a
dimension-as-token self-attention mechanism designed with significantly reduced
internal dimensions can effectively encode sentence embeddings for subject
retrieval. In quantitative evaluation, our system achieved an average recall
rate of 32.24% in the general quantitative setting (all subjects), 43.16% and
31.53% of the general qualitative evaluation methods with minimal GPU usage,
highlighting their competitive performance. Our results demonstrate that our
approach is effective in capturing relevant subject information under resource
constraints, although there is still room for improvement.

</details>


### [33] [WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch](https://arxiv.org/abs/2505.03733)
*Zimu Lu,Yunqiao Yang,Houxing Ren,Haotian Hou,Han Xiao,Ke Wang,Weikang Shi,Aojun Zhou,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: 本文介绍了一个名为WebGen-Bench的新基准，用于评估LLM代理从头开始创建多文件网站代码库的能力，并构建了一个训练集WebGen-Instruct以提升其性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理在代码生成方面潜力巨大，但缺乏专门针对从零开始创建复杂多文件网站能力的基准测试，以及相应的训练数据来推动这一领域的发展。

Method: 构建了WebGen-Bench基准，包含人工与GPT-4o共同创建的多样化网站生成指令及647个测试用例（由GPT-4o生成并人工筛选）；使用Web导航代理自动化测试。同时，构建了包含6667条指令的训练集WebGen-Instruct。评估了多种代码代理框架和LLM。

Result: 在WebGen-Bench上，表现最好的现有模型组合（Bolt.diy + DeepSeek-R1）准确率仅为27.8%，显示了基准的挑战性。使用WebGen-Instruct子集训练Qwen2.5-Coder-32B-Instruct后，其准确率提升至38.2%，超越了最佳专有模型。

Conclusion: WebGen-Bench是一个具有挑战性的基准，揭示了当前LLM代理在从零开始生成网站方面的不足。新构建的WebGen-Instruct训练集能够显著提升模型性能，为该领域的研究和发展提供了宝贵的资源和方向。

Abstract: LLM-based agents have demonstrated great potential in generating and managing
code within complex codebases. In this paper, we introduce WebGen-Bench, a
novel benchmark designed to measure an LLM-based agent's ability to create
multi-file website codebases from scratch. It contains diverse instructions for
website generation, created through the combined efforts of human annotators
and GPT-4o. These instructions span three major categories and thirteen minor
categories, encompassing nearly all important types of web applications. To
assess the quality of the generated websites, we use GPT-4o to generate test
cases targeting each functionality described in the instructions, and then
manually filter, adjust, and organize them to ensure accuracy, resulting in 647
test cases. Each test case specifies an operation to be performed on the
website and the expected result after the operation. To automate testing and
improve reproducibility, we employ a powerful web-navigation agent to execute
tests on the generated websites and determine whether the observed responses
align with the expected results. We evaluate three high-performance code-agent
frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and
open-source LLMs as engines. The best-performing combination, Bolt.diy powered
by DeepSeek-R1, achieves only 27.8\% accuracy on the test cases, highlighting
the challenging nature of our benchmark. Additionally, we construct
WebGen-Instruct, a training set consisting of 6,667 website-generation
instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories
generated from a subset of this training set achieves an accuracy of 38.2\%,
surpassing the performance of the best proprietary model.

</details>


### [34] [VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model](https://arxiv.org/abs/2505.03739)
*Zuwei Long,Yunhang Shen,Chaoyou Fu,Heting Gao,Lijiang Li,Peixian Chen,Mengdan Zhang,Hang Shao,Jian Li,Jinlong Peng,Haoyu Cao,Ke Li,Rongrong Ji,Xing Sun*

Main category: cs.CL

TL;DR: VITA-Audio是一种端到端的大型语音模型，通过轻量级MCTP模块和渐进式训练策略，实现了快速的音文令牌生成，显著降低了流式语音交互中首个音频令牌的生成延迟。


<details>
  <summary>Details</summary>
Motivation: 现有语音模型在流式生成首个音频令牌时存在高延迟，这成为自然人机交互系统部署的主要瓶颈。

Method: 引入了一个轻量级的多模态交叉令牌预测 (MCTP) 模块，该模块能在单次模型前向传播中高效生成多个音频令牌；同时探索了一种四阶段渐进式训练策略，以在最小化语音质量损失的情况下实现模型加速。

Result: VITA-Audio在7B参数规模下实现了3-5倍的推理加速，并且在自动语音识别 (ASR)、文本到语音 (TTS) 和口语问答 (SQA) 等多个基准测试中，其性能显著优于类似模型大小的开源模型。该模型能够在首次前向传播时生成音频输出。

Conclusion: VITA-Audio是首个能够在首次前向传播中生成音频输出的多模态大语言模型，它通过显著减少首个音频令牌的延迟，实现了低延迟的实时对话能力，且模型完全可复现并仅使用开源数据进行训练。

Abstract: With the growing requirement for natural human-computer interaction,
speech-based systems receive increasing attention as speech is one of the most
common forms of daily communication. However, the existing speech models still
experience high latency when generating the first audio token during streaming,
which poses a significant bottleneck for deployment. To address this issue, we
propose VITA-Audio, an end-to-end large speech model with fast audio-text token
generation. Specifically, we introduce a lightweight Multiple Cross-modal Token
Prediction (MCTP) module that efficiently generates multiple audio tokens
within a single model forward pass, which not only accelerates the inference
but also significantly reduces the latency for generating the first audio in
streaming scenarios. In addition, a four-stage progressive training strategy is
explored to achieve model acceleration with minimal loss of speech quality. To
our knowledge, VITA-Audio is the first multi-modal large language model capable
of generating audio output during the first forward pass, enabling real-time
conversational capabilities with minimal latency. VITA-Audio is fully
reproducible and is trained on open-source data only. Experimental results
demonstrate that our model achieves an inference speedup of 3~5x at the 7B
parameter scale, but also significantly outperforms open-source models of
similar model size on multiple benchmarks for automatic speech recognition
(ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [35] [RESAnything: Attribute Prompting for Arbitrary Referring Segmentation](https://arxiv.org/abs/2505.02867)
*Ruiqi Wang,Hao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为 RESAnything 的开放词汇、零样本指代表达式分割 (RES) 方法，该方法利用大型语言模型 (LLM) 的思维链 (CoT) 推理和属性提示，能够处理更通用的、包含对象/部分级别标签及隐式属性引用的输入，并在传统及复杂场景下表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有指代表达式分割方法难以处理更通用的输入，特别是那些涉及对象部分级别标签以及指向功能、设计、风格、材质等隐式属性的引用。需要一种无需特定训练或微调就能理解这类复杂查询的零样本方法。

Method: 模型名为 RESAnything，采用零样本学习。它利用大型语言模型 (LLM) 进行思维链 (CoT) 推理，核心是属性提示。通过系统性提示 LLM，为基础图像分割模型产生的潜在分割提议生成详细的对象/部分属性描述（如形状、颜色、位置），从而实现对隐式查询的深度推理，无需部分注释进行训练或微调。

Result: RESAnything 作为首个零样本且基于 LLM 的 RES 方法，在传统 RES 基准测试的零样本方法中取得了明显更优的性能。在涉及隐式查询和复杂部分级关系的挑战性场景中，显著优于现有方法。此外，本文贡献了一个新的基准数据集，包含约3000个精心策划的 RES 实例，用于评估部分级、任意 RES 解决方案。

Conclusion: RESAnything 是一种有效的零样本、开放词汇指代表达式分割方法，通过结合 LLM 的思维链推理和属性提示，成功地处理了传统方法难以应对的通用和隐式指代表达，展示了 LLM 在理解复杂视觉语言任务中的潜力，并为该领域提供了新的基准。

Abstract: We present an open-vocabulary and zero-shot method for arbitrary referring
expression segmentation (RES), targeting input expressions that are more
general than what prior works were designed to handle. Specifically, our inputs
encompass both object- and part-level labels as well as implicit references
pointing to properties or qualities of object/part function, design, style,
material, etc. Our model, coined RESAnything, leverages Chain-of-Thoughts (CoT)
reasoning, where the key idea is attribute prompting. We generate detailed
descriptions of object/part attributes including shape, color, and location for
potential segment proposals through systematic prompting of a large language
model (LLM), where the proposals are produced by a foundational image
segmentation model. Our approach encourages deep reasoning about object or part
attributes related to function, style, design, etc., enabling the system to
handle implicit queries without any part annotations for training or
fine-tuning. As the first zero-shot and LLM-based RES method, RESAnything
achieves clearly superior performance among zero-shot methods on traditional
RES benchmarks and significantly outperforms existing methods on challenging
scenarios involving implicit queries and complex part-level relations. Finally,
we contribute a new benchmark dataset to offer ~3K carefully curated RES
instances to assess part-level, arbitrary RES solutions.

</details>


### [36] [Gone With the Bits: Revealing Racial Bias in Low-Rate Neural Compression for Facial Images](https://arxiv.org/abs/2505.02949)
*Tian Qiu,Arjun Nichani,Rasta Tadayontahmasebi,Haewon Jeong*

Main category: cs.CV

TL;DR: 该研究提出了一个评估神经图像压缩模型中种族偏见的框架，发现现有模型普遍存在偏见，且平衡训练数据不足以完全消除偏见。


<details>
  <summary>Details</summary>
Motivation: 神经压缩方法因其优越的率失真性能而日益普及，但作为深度学习模型，它们在训练过程中容易产生偏见，可能导致对不同群体个体的不公平结果。

Method: 提出了一个通用的、结构化的、可扩展的框架来评估神经图像压缩模型中的偏见。通过该框架，分析了九种流行的神经压缩模型及其变体中的种族偏见，主要通过检查图像重建中面部表型退化来捕捉偏见。

Result: 传统失真指标在捕捉神经压缩模型偏见方面效果不佳；所有被测神经压缩模型中都存在种族偏见，可通过面部表型退化捕捉；解码图像的偏见与真实感之间存在权衡关系；使用种族平衡的训练集可以减少偏见，但并非充分的偏见缓解策略；偏见可归因于压缩模型偏见和分类模型偏见。

Conclusion: 这项工作是评估和消除神经图像压缩模型中偏见的第一步。

Abstract: Neural compression methods are gaining popularity due to their superior
rate-distortion performance over traditional methods, even at extremely low
bitrates below 0.1 bpp. As deep learning architectures, these models are prone
to bias during the training process, potentially leading to unfair outcomes for
individuals in different groups. In this paper, we present a general,
structured, scalable framework for evaluating bias in neural image compression
models. Using this framework, we investigate racial bias in neural compression
algorithms by analyzing nine popular models and their variants. Through this
investigation, we first demonstrate that traditional distortion metrics are
ineffective in capturing bias in neural compression models. Next, we highlight
that racial bias is present in all neural compression models and can be
captured by examining facial phenotype degradation in image reconstructions. We
then examine the relationship between bias and realism in the decoded images
and demonstrate a trade-off across models. Finally, we show that utilizing a
racially balanced training set can reduce bias but is not a sufficient bias
mitigation strategy. We additionally show the bias can be attributed to
compression model bias and classification model bias. We believe that this work
is a first step towards evaluating and eliminating bias in neural image
compression models.

</details>


### [37] [Generating Narrated Lecture Videos from Slides with Synchronized Highlights](https://arxiv.org/abs/2505.02966)
*Alexander Holmberg*

Main category: cs.CV

TL;DR: 该研究介绍了一个自动化系统，能将静态幻灯片转化为带有人工智能解说和同步视觉高亮的视频讲座。


<details>
  <summary>Details</summary>
Motivation: 将静态幻灯片制作成引人入胜的视频讲座耗时耗力，需要演示者录制讲解并进行视觉引导。

Method: 提出一个端到端系统，其核心是一个新颖的高亮对齐模块。该模块使用多种策略（如Levenshtein距离、基于LLM的语义分析）将口语短语精确映射到幻灯片上的特定位置，并利用带时间戳的文本转语音（TTS）技术实现时间同步。

Result: 技术评估显示，基于LLM的对齐方法在位置准确性上表现优异（F1 > 92%），显著优于简单方法，尤其在复杂和数学内容密集的幻灯片上。视频生成成本平均每小时低于1美元。

Conclusion: 该方法结合了高准确性和极低的成本，使其成为一种实用且可扩展的工具，能将静态幻灯片有效地转换为带有视觉引导的视频讲座。

Abstract: Turning static slides into engaging video lectures takes considerable time
and effort, requiring presenters to record explanations and visually guide
their audience through the material. We introduce an end-to-end system designed
to automate this process entirely. Given a slide deck, this system synthesizes
a video lecture featuring AI-generated narration synchronized precisely with
dynamic visual highlights. These highlights automatically draw attention to the
specific concept being discussed, much like an effective presenter would. The
core technical contribution is a novel highlight alignment module. This module
accurately maps spoken phrases to locations on a given slide using diverse
strategies (e.g., Levenshtein distance, LLM-based semantic analysis) at
selectable granularities (line or word level) and utilizes timestamp-providing
Text-to-Speech (TTS) for timing synchronization. We demonstrate the system's
effectiveness through a technical evaluation using a manually annotated slide
dataset with 1000 samples, finding that LLM-based alignment achieves high
location accuracy (F1 > 92%), significantly outperforming simpler methods,
especially on complex, math-heavy content. Furthermore, the calculated
generation cost averages under $1 per hour of video, offering potential savings
of two orders of magnitude compared to conservative estimates of manual
production costs. This combination of high accuracy and extremely low cost
positions this approach as a practical and scalable tool for transforming
static slides into effective, visually-guided video lectures.

</details>


### [38] [Adversarial Robustness Analysis of Vision-Language Models in Medical Image Segmentation](https://arxiv.org/abs/2505.02971)
*Anjila Budathoki,Manish Dhakal*

Main category: cs.CV

TL;DR: 本文研究了微调后的视觉语言分割模型 (VLSM) 在二维医学图像分析中对抗对抗性攻击 (PGD, FGSM) 的鲁棒性，发现其性能在攻击后显著下降。


<details>
  <summary>Details</summary>
Motivation: 针对视觉语言分割模型 (VLSM) 的对抗性攻击研究尚不充分，尤其是在高风险的医学图像分析领域。本研究旨在评估在医学领域专门微调的 VLSM 对抗对抗性攻击的鲁棒性。

Method: 首先，使用适配器对预训练的 VLSM 进行医学图像分割任务的微调。然后，对微调后的模型采用投影梯度下降 (PGD) 和快速梯度符号法 (FGSM) 等对抗性攻击，并通过报告模型性能（如 DSC 和 IoU 分数）的下降来分析攻击影响。

Result: 研究结果表明，在引入 PGD 和 FGSM 对抗性攻击后，模型的骰子相似系数 (DSC) 和交并比 (IoU) 分数均出现显著下降。此外，研究者也探索了通用扰动，但未能成功应用于医学图像。

Conclusion: 结论指出，经过微调的医学图像视觉语言分割模型在面对 PGD 和 FGSM 等对抗性攻击时表现出脆弱性，性能显著降低，这提示了在医学应用中需要关注此类模型的安全性。

Abstract: Adversarial attacks have been fairly explored for computer vision and
vision-language models. However, the avenue of adversarial attack for the
vision language segmentation models (VLSMs) is still under-explored, especially
for medical image analysis.
  Thus, we have investigated the robustness of VLSMs against adversarial
attacks for 2D medical images with different modalities with radiology,
photography, and endoscopy. The main idea of this project was to assess the
robustness of the fine-tuned VLSMs specially in the medical domain setting to
address the high risk scenario.
  First, we have fine-tuned pre-trained VLSMs for medical image segmentation
with adapters.
  Then, we have employed adversarial attacks -- projected gradient descent
(PGD) and fast gradient sign method (FGSM) -- on that fine-tuned model to
determine its robustness against adversaries.
  We have reported models' performance decline to analyze the adversaries'
impact.
  The results exhibit significant drops in the DSC and IoU scores after the
introduction of these adversaries. Furthermore, we also explored universal
perturbation but were not able to find for the medical images.
  \footnote{https://github.com/anjilab/secure-private-ai}

</details>


### [39] [Completing Spatial Transcriptomics Data for Gene Expression Prediction Benchmarking](https://arxiv.org/abs/2505.02980)
*Daniela Ruiz,Paula Cardenas,Leonardo Manrique,Daniela Vega,Gabriel Mejia,Pablo Arbelaez*

Main category: cs.CV

TL;DR: 论文引入了SpaRED，一个标准化的空间转录组学公共数据集，以及SpaCKLE，一个先进的基因表达补全模型。通过SpaRED基准测试，证明SpaCKLE能显著改善从组织学图像预测基因表达模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有空间转录组学技术（如Visium）成本高、需要专业知识、临床整合慢，且存在基因捕获效率低导致的数据丢失问题。此外，用于从组织学图像预测基因表达的深度学习模型缺乏统一的数据集、预处理和训练方案，难以进行公平比较。

Method: 1. 构建SpaRED：一个包含26个公共数据集的系统化整理数据库，提供标准化的模型评估资源。
2. 提出SpaCKLE：一个基于Transformer的先进基因表达补全模型，用于解决数据丢失问题。
3. 建立SpaRED基准：在原始数据和经SpaCKLE补全的数据上，评估了8种最先进的基因表达预测模型。

Result: 1. SpaCKLE模型在基因表达补全任务上，与现有方法相比，均方误差降低超过82.5%。
2. 在SpaRED基准测试中，使用SpaCKLE补全数据后，所有基因表达预测模型的性能均得到显著提升。

Conclusion: 该研究贡献了迄今为止最全面的从组织学图像预测基因表达的基准测试，为未来空间转录组学的研究奠定了基础。SpaRED和SpaCKLE为该领域提供了重要的资源和工具。

Abstract: Spatial Transcriptomics is a groundbreaking technology that integrates
histology images with spatially resolved gene expression profiles. Among the
various Spatial Transcriptomics techniques available, Visium has emerged as the
most widely adopted. However, its accessibility is limited by high costs, the
need for specialized expertise, and slow clinical integration. Additionally,
gene capture inefficiencies lead to significant dropout, corrupting acquired
data. To address these challenges, the deep learning community has explored the
gene expression prediction task directly from histology images. Yet,
inconsistencies in datasets, preprocessing, and training protocols hinder fair
comparisons between models. To bridge this gap, we introduce SpaRED, a
systematically curated database comprising 26 public datasets, providing a
standardized resource for model evaluation. We further propose SpaCKLE, a
state-of-the-art transformer-based gene expression completion model that
reduces mean squared error by over 82.5% compared to existing approaches.
Finally, we establish the SpaRED benchmark, evaluating eight state-of-the-art
prediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE
substantially improves the results across all the gene expression prediction
models. Altogether, our contributions constitute the most comprehensive
benchmark of gene expression prediction from histology images to date and a
stepping stone for future research on Spatial Transcriptomics.

</details>


### [40] [NTIRE 2025 Challenge on UGC Video Enhancement: Methods and Results](https://arxiv.org/abs/2505.03007)
*Nikolay Safonov,Alexey Bryncev,Andrey Moskalenko,Dmitry Kulikov,Dmitry Vatolin,Radu Timofte,Haibo Lei,Qifan Gao,Qing Luo,Yaqing Li,Jie Song,Shaozhe Hao,Meisong Zheng,Jingyi Xu,Chengbin Wu,Jiahui Liu,Ying Chen,Xin Deng,Mai Xu,Peipei Liang,Jie Ma,Junjie Jin,Yingxue Pang,Fangzhou Luo,Kai Chen,Shijie Zhao,Mingyang Wu,Renjie Li,Yushen Zuo,Shengyun Zhong,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本文概述了NTIRE 2025 UGC视频增强挑战赛，该挑战赛旨在通过众包主观评估，推动改善饱受真实世界退化问题困扰的用户生成视频（UGC）视觉质量的算法发展。


<details>
  <summary>Details</summary>
Motivation: 用户生成内容（UGC）视频在短视频平台广泛使用，但常伴有噪声、模糊、褪色和压缩失真等真实世界退化问题，提升其视觉质量具有重要的实际意义。

Method: 挑战赛构建了一个包含150个无参考真实值的UGC视频数据集。参赛者开发算法以提升视频视觉质量。评估方法是基于超过8000名评估者投票的众包主观质量评估。

Result: 超过25支队伍参与了挑战赛，其中7支队伍的方案通过了最终验证。处理后的视频、主观比较投票和评分等所有数据均已公开。

Conclusion: 该挑战赛的结果为UGC视频增强领域的最新技术水平提供了洞察，并揭示了该领域的新兴趋势和有效策略。

Abstract: This paper presents an overview of the NTIRE 2025 Challenge on UGC Video
Enhancement. The challenge constructed a set of 150 user-generated content
videos without reference ground truth, which suffer from real-world
degradations such as noise, blur, faded colors, compression artifacts, etc. The
goal of the participants was to develop an algorithm capable of improving the
visual quality of such videos. Given the widespread use of UGC on short-form
video platforms, this task holds substantial practical importance. The
evaluation was based on subjective quality assessment in crowdsourcing,
obtaining votes from over 8000 assessors. The challenge attracted more than 25
teams submitting solutions, 7 of which passed the final phase with source code
verification. The outcomes may provide insights into the state-of-the-art in
UGC video enhancement and highlight emerging trends and effective strategies in
this evolving research area. All data, including the processed videos and
subjective comparison votes and scores, is made publicly available at
https://github.com/msu-video-group/NTIRE25_UGC_Video_Enhancement.

</details>


### [41] [GIF: Generative Inspiration for Face Recognition at Scale](https://arxiv.org/abs/2505.03012)
*Saeed Ebrahimi,Sahar Rahimi,Ali Dabouei,Srinjoy Das,Jeremy M. Dawson,Nasser M. Nasrabadi*

Main category: cs.CV

TL;DR: 该研究提出一种将标量标签替换为结构化身份代码的方法，以降低人脸识别中Softmax的计算成本，使其与身份数量呈对数关系，并提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有大规模人脸识别方法中Softmax计算成本过高，即使采用子集估计，成本与身份数量仍呈线性关系（尽管比例降低）。

Method: 提出一种标记化方案，将原子标量标签转换为结构化的身份代码（整数序列）。然后，训练一个人脸识别骨干网络来预测每个输入的结构化代码，而非其标量标签。

Result: 该方法将计算成本与身份数量的关系从线性转为对数。在IJB-B和IJB-C数据集上，于TAR@FAR=1e-4的条件下，性能分别优于竞争者1.52%和0.6%。

Conclusion: 通过将标量标签替换为结构化身份代码，该方法有效降低了人脸识别中Softmax的计算成本至对数级别，并提升了识别准确率。

Abstract: Aiming to reduce the computational cost of Softmax in massive label space of
Face Recognition (FR) benchmarks, recent studies estimate the output using a
subset of identities. Although promising, the association between the
computation cost and the number of identities in the dataset remains linear
only with a reduced ratio. A shared characteristic among available FR methods
is the employment of atomic scalar labels during training. Consequently, the
input to label matching is through a dot product between the feature vector of
the input and the Softmax centroids. Inspired by generative modeling, we
present a simple yet effective method that substitutes scalar labels with
structured identity code, i.e., a sequence of integers. Specifically, we
propose a tokenization scheme that transforms atomic scalar labels into
structured identity codes. Then, we train an FR backbone to predict the code
for each input instead of its scalar label. As a result, the associated
computational cost becomes logarithmic w.r.t. number of identities. We
demonstrate the benefits of the proposed method by conducting experiments. In
particular, our method outperforms its competitors by 1.52%, and 0.6% at
TAR@FAR$=1e-4$ on IJB-B and IJB-C, respectively, while transforming the
association between computational cost and the number of identities from linear
to logarithmic. See code at https://github.com/msed-Ebrahimi/GIF

</details>


### [42] [Lesion-Aware Generative Artificial Intelligence for Virtual Contrast-Enhanced Mammography in Breast Cancer](https://arxiv.org/abs/2505.03018)
*Aurora Rofena,Arianna Manchia,Claudia Lucia Piccolo,Bruno Beomonte Zobel,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 该研究提出了一种名为 Seg-CycleGAN 的深度学习模型，可从低能量乳腺X线图像虚拟合成对比增强后的图像，以减少对比增强能谱乳腺摄影（CESM）中辐射和造影剂的风险。


<details>
  <summary>Details</summary>
Motivation: 传统的对比增强能谱乳腺摄影（CESM）虽然诊断准确性优于标准乳腺摄影，但伴随着更高的辐射暴露和与造影剂相关的潜在副作用。本研究旨在解决这些局限性。

Method: 研究提出了一种名为 Seg-CycleGAN 的生成式深度学习框架。该模型基于标准的 CycleGAN 架构，通过利用病灶分割图来指导生成过程，并引入针对病灶区域的局部化损失项，从低能量图像合成高保真度的双能量减影图像，以改善病灶的重建。

Result: 在 CESM@UCBM 数据集上的实验表明，Seg-CycleGAN 在峰值信噪比（PSNR）和结构相似性指数（SSIM）方面优于基线模型，同时在均方误差（MSE）和视觉信息保真度（VIF）方面保持竞争力。定性评估进一步证实了生成图像中病灶保真度的提高。

Conclusion: 研究结果表明，结合分割信息的生成模型为实现无造影剂的CESM替代方案提供了一条可行的途径。

Abstract: Contrast-Enhanced Spectral Mammography (CESM) is a dual-energy mammographic
technique that improves lesion visibility through the administration of an
iodinated contrast agent. It acquires both a low-energy image, comparable to
standard mammography, and a high-energy image, which are then combined to
produce a dual-energy subtracted image highlighting lesion contrast
enhancement. While CESM offers superior diagnostic accuracy compared to
standard mammography, its use entails higher radiation exposure and potential
side effects associated with the contrast medium. To address these limitations,
we propose Seg-CycleGAN, a generative deep learning framework for Virtual
Contrast Enhancement in CESM. The model synthesizes high-fidelity dual-energy
subtracted images from low-energy images, leveraging lesion segmentation maps
to guide the generative process and improve lesion reconstruction. Building
upon the standard CycleGAN architecture, Seg-CycleGAN introduces localized loss
terms focused on lesion areas, enhancing the synthesis of diagnostically
relevant regions. Experiments on the CESM@UCBM dataset demonstrate that
Seg-CycleGAN outperforms the baseline in terms of PSNR and SSIM, while
maintaining competitive MSE and VIF. Qualitative evaluations further confirm
improved lesion fidelity in the generated images. These results suggest that
segmentation-aware generative models offer a viable pathway toward
contrast-free CESM alternatives.

</details>


### [43] [An Explainable Anomaly Detection Framework for Monitoring Depression and Anxiety Using Consumer Wearable Devices](https://arxiv.org/abs/2505.03039)
*Yuezhou Zhang,Amos A. Folarin,Callum Stewart,Heet Sankesara,Yatharth Ranjan,Pauline Conde,Akash Roy Choudhury,Shaoxiong Sun,Zulqarnain Rashid,Richard J. B. Dobson*

Main category: cs.CV

TL;DR: 该研究提出了一种基于可穿戴设备数据的可解释异常检测框架，用于早期发现抑郁和焦虑症状的恶化。


<details>
  <summary>Details</summary>
Motivation: 利用可穿戴设备实现对抑郁和焦虑症状恶化的早期、客观监测，以改善心理健康管理。

Method: 使用来自2023名参与者的消费级可穿戴设备数据（睡眠时长、步数、静息心率），通过LSTM自编码器模型学习健康基线模式，并结合SHAP分析进行模型解释，以检测抑郁或焦虑评分增加>=5分（临床显著阈值）的异常情况。

Result: 模型在检测393例症状恶化事件中实现了0.80的调整后F1分数（精确率0.73，召回率0.88）。对于抑郁和焦虑并发恶化（F1=0.84）及更显著症状变化（>=10分，F1=0.85）表现更佳。静息心率是71.4%检测到异常中最具影响力的特征。

Conclusion: 研究结果表明，可解释的异常检测框架利用可穿戴设备数据，有望在真实世界中实现个性化、可扩展和主动的心理健康监测。

Abstract: Continuous monitoring of behavior and physiology via wearable devices offers
a novel, objective method for the early detection of worsening depression and
anxiety. In this study, we present an explainable anomaly detection framework
that identifies clinically meaningful increases in symptom severity using
consumer-grade wearable data. Leveraging data from 2,023 participants with
defined healthy baselines, our LSTM autoencoder model learned normal health
patterns of sleep duration, step count, and resting heart rate. Anomalies were
flagged when self-reported depression or anxiety scores increased by >=5 points
(a threshold considered clinically significant). The model achieved an adjusted
F1-score of 0.80 (precision = 0.73, recall = 0.88) in detecting 393
symptom-worsening episodes across 341 participants, with higher performance
observed for episodes involving concurrent depression and anxiety escalation
(F1 = 0.84) and for more pronounced symptom changes (>=10-point increases, F1 =
0.85). Model interpretability was supported by SHAP-based analysis, which
identified resting heart rate as the most influential feature in 71.4
percentage of detected anomalies, followed by physical activity and sleep.
Together, our findings highlight the potential of explainable anomaly detection
to enable personalized, scalable, and proactive mental health monitoring in
real-world settings.

</details>


### [44] [Estimating the Diameter at Breast Height of Trees in a Forest With a Single 360 Camera](https://arxiv.org/abs/2505.03093)
*Siming He,Zachary Osman,Fernando Cladera,Dexter Ong,Nitant Rai,Patrick Corey Green,Vijay Kumar,Pratik Chaudhari*

Main category: cs.CV

TL;DR: 该研究提出了一种使用消费级360度相机进行低成本、半自动化的森林胸径（DBH）测量方法，其精度接近昂贵的LiDAR技术。


<details>
  <summary>Details</summary>
Motivation: 森林调查依赖于精确的胸径测量，但现有的LiDAR技术成本高昂且操作复杂。因此，需要一种低成本、易于操作的替代方案。

Method: 该方法包括：1) 使用运动恢复结构（SfM）摄影测量软件（Agisoft Metashape）重建密集点云；2) 通过将Grounded Segment Anything (SAM) 掩码投影到3D点云上进行语义树干分割；3) 使用基于RANSAC的稳健技术估计横截面形状和胸径。同时开发了交互式可视化工具。

Result: 在对43棵树木进行的61次采集实验中，该方法与手动“地面真实”测量相比，中位绝对相对误差为5-9%。这仅比LiDAR估计高2-4%，但所使用的360度相机成本低几个数量级，设置简单且易于获得。

Conclusion: 研究提出了一种有效的低成本替代方案，使用消费级360度相机进行森林胸径测量，其精度可与LiDAR技术相媲美，同时具有显著的成本优势和操作便利性。

Abstract: Forest inventories rely on accurate measurements of the diameter at breast
height (DBH) for ecological monitoring, resource management, and carbon
accounting. While LiDAR-based techniques can achieve centimeter-level
precision, they are cost-prohibitive and operationally complex. We present a
low-cost alternative that only needs a consumer-grade 360 video camera. Our
semi-automated pipeline comprises of (i) a dense point cloud reconstruction
using Structure from Motion (SfM) photogrammetry software called Agisoft
Metashape, (ii) semantic trunk segmentation by projecting Grounded Segment
Anything (SAM) masks onto the 3D cloud, and (iii) a robust RANSAC-based
technique to estimate cross section shape and DBH. We introduce an interactive
visualization tool for inspecting segmented trees and their estimated DBH. On
61 acquisitions of 43 trees under a variety of conditions, our method attains
median absolute relative errors of 5-9% with respect to "ground-truth" manual
measurements. This is only 2-4% higher than LiDAR-based estimates, while
employing a single 360 camera that costs orders of magnitude less, requires
minimal setup, and is widely available.

</details>


### [45] [Not All Parameters Matter: Masking Diffusion Models for Enhancing Generation Ability](https://arxiv.org/abs/2505.03097)
*Lei Wang,Senmao Li,Fei Yang,Jianye Wang,Ziheng Zhang,Yuhan Liu,Yaxing Wang,Jian Yang*

Main category: cs.CV

TL;DR: 提出了一种名为 MaskUNet 的方法，通过动态调整 U-Net 参数，在不显著增加参数量的情况下提升扩散模型的图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型中，同一网络层需同时学习图像结构和纹理信息，与传统深度学习架构不同。研究旨在探索时间相关的扩散模型，并发现选择性地调整U-Net参数可以提升去噪效果和生成质量。

Method: 提出 MaskUNet 方法，该方法利用时间步和样本相关的有效 U-Net 参数。通过识别并“遮蔽”（置零）U-Net 中对当前去噪阶段贡献较小或产生负面影响的参数，来优化生成过程。提供了基于训练和免训练两种微调策略。

Result: MaskUNet 在 COCO 数据集上的零样本推理中取得了最佳的 FID 分数，并且在下游任务评估中也展示了其有效性。

Conclusion: MaskUNet 是一种简单而有效的方法，通过在扩散过程中动态利用U-Net参数，能够在参数数量几乎不变的情况下显著提高图像生成质量。

Abstract: The diffusion models, in early stages focus on constructing basic image
structures, while the refined details, including local features and textures,
are generated in later stages. Thus the same network layers are forced to learn
both structural and textural information simultaneously, significantly
differing from the traditional deep learning architectures (e.g., ResNet or
GANs) which captures or generates the image semantic information at different
layers. This difference inspires us to explore the time-wise diffusion models.
We initially investigate the key contributions of the U-Net parameters to the
denoising process and identify that properly zeroing out certain parameters
(including large parameters) contributes to denoising, substantially improving
the generation quality on the fly. Capitalizing on this discovery, we propose a
simple yet effective method-termed ``MaskUNet''- that enhances generation
quality with negligible parameter numbers. Our method fully leverages timestep-
and sample-dependent effective U-Net parameters. To optimize MaskUNet, we offer
two fine-tuning strategies: a training-based approach and a training-free
approach, including tailored networks and optimization functions. In zero-shot
inference on the COCO dataset, MaskUNet achieves the best FID score and further
demonstrates its effectiveness in downstream task evaluations. Project page:
https://gudaochangsheng.github.io/MaskUnet-Page/

</details>


### [46] [Image Recognition with Online Lightweight Vision Transformer: A Survey](https://arxiv.org/abs/2505.03113)
*Zherui Zhang,Rongtao Xu,Jie Zhou,Changwei Wang,Xingtian Pei,Wenhao Xu,Jiguang Zhang,Li Guo,Longxiang Gao,Wenbo Xu,Shibiao Xu*

Main category: cs.CV

TL;DR: 本文综述了为图像识别生成轻量级视觉Transformer（ViT）的在线策略，重点关注高效组件设计、动态网络和知识蒸馏三个方面。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer在计算机视觉领域取得了显著成功，但其固有的计算和内存挑战限制了其实际应用，因此需要研究轻量化方法。

Method: 本文调研并评估了三大类轻量化视觉Transformer的策略：高效组件设计、动态网络和知识蒸馏。评估在ImageNet-1K基准上进行，分析了精度、参数量、吞吐量等方面的权衡。

Result: 通过在ImageNet-1K上的评估，分析了各种轻量化策略在精度、参数、吞吐量等方面的优缺点和灵活性。

Conclusion: 论文总结了轻量化视觉Transformer的现有方法，并提出了未来的研究方向和潜在挑战，旨在启发进一步探索并为社区提供实践指导。

Abstract: The Transformer architecture has achieved significant success in natural
language processing, motivating its adaptation to computer vision tasks. Unlike
convolutional neural networks, vision transformers inherently capture
long-range dependencies and enable parallel processing, yet lack inductive
biases and efficiency benefits, facing significant computational and memory
challenges that limit its real-world applicability. This paper surveys various
online strategies for generating lightweight vision transformers for image
recognition, focusing on three key areas: Efficient Component Design, Dynamic
Network, and Knowledge Distillation. We evaluate the relevant exploration for
each topic on the ImageNet-1K benchmark, analyzing trade-offs among precision,
parameters, throughput, and more to highlight their respective advantages,
disadvantages, and flexibility. Finally, we propose future research directions
and potential challenges in the lightweighting of vision transformers with the
aim of inspiring further exploration and providing practical guidance for the
community. Project Page: https://github.com/ajxklo/Lightweight-VIT

</details>


### [47] [Path and Bone-Contour Regularized Unpaired MRI-to-CT Translation](https://arxiv.org/abs/2505.03114)
*Teng Zhou,Jax Luo,Yuping Sun,Yiheng Tan,Shun Yao,Nazim Haouchine,Scott Raymond*

Main category: cs.CV

TL;DR: 提出了一种新的基于路径和骨骼轮廓正则化的非配对MRI到CT图像转换方法，通过神经ODE建模转换过程并优化骨骼结构表示，以提高转换精度，特别是在骨骼等关键解剖结构上。


<details>
  <summary>Details</summary>
Motivation: 现有的非配对MRI到CT转换方法在准确转换CT上清晰可见但在MRI上不明显的解剖特征（如骨骼结构）方面存在挑战，这限制了其在需要精确骨骼表示的放射治疗等应用中的使用。同时，获取配对的MRI和CT扫描数据在实际中具有挑战性。

Method: 该方法将MRI和CT图像投影到共享的潜空间，使用神经普通微分方程（Neural ODEs）将MRI到CT的映射建模为一个连续流，并通过最小化流的转换路径长度来优化映射。为提高骨骼结构转换的准确性，引入了一个可训练的神经网络从MRI生成骨骼轮廓，并设计机制促使模型关注骨骼轮廓及其邻近区域。

Result: 在三个数据集上的评估表明，所提出的方法优于现有的非配对MRI到CT转换方法，实现了更低的整体错误率。在下游的骨骼分割任务中，该方法在保持骨骼结构保真度方面表现出更优的性能。

Conclusion: 该研究提出的基于路径和骨骼轮廓正则化的非配对MRI到CT转换方法，能够有效提高转换图像的准确性，尤其是在骨骼结构的表示上，为需要精确解剖信息的临床应用（如放射治疗计划）提供了更好的解决方案。

Abstract: Accurate MRI-to-CT translation promises the integration of complementary
imaging information without the need for additional imaging sessions. Given the
practical challenges associated with acquiring paired MRI and CT scans, the
development of robust methods capable of leveraging unpaired datasets is
essential for advancing the MRI-to-CT translation. Current unpaired MRI-to-CT
translation methods, which predominantly rely on cycle consistency and
contrastive learning frameworks, frequently encounter challenges in accurately
translating anatomical features that are highly discernible on CT but less
distinguishable on MRI, such as bone structures. This limitation renders these
approaches less suitable for applications in radiation therapy, where precise
bone representation is essential for accurate treatment planning. To address
this challenge, we propose a path- and bone-contour regularized approach for
unpaired MRI-to-CT translation. In our method, MRI and CT images are projected
to a shared latent space, where the MRI-to-CT mapping is modeled as a
continuous flow governed by neural ordinary differential equations. The optimal
mapping is obtained by minimizing the transition path length of the flow. To
enhance the accuracy of translated bone structures, we introduce a trainable
neural network to generate bone contours from MRI and implement mechanisms to
directly and indirectly encourage the model to focus on bone contours and their
adjacent regions. Evaluations conducted on three datasets demonstrate that our
method outperforms existing unpaired MRI-to-CT translation approaches,
achieving lower overall error rates. Moreover, in a downstream bone
segmentation task, our approach exhibits superior performance in preserving the
fidelity of bone structures. Our code is available at:
https://github.com/kennysyp/PaBoT.

</details>


### [48] [TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion](https://arxiv.org/abs/2505.03116)
*Haoyue Liu,Jinghan Xu,Yi Chang,Hanyu Zhou,Haozhi Zhao,Lin Wang,Luxin Yan*

Main category: cs.CV

TL;DR: 论文提出TimeTracker，一种基于连续点跟踪的视频帧插值（VFI）框架，利用事件相机处理非线性运动，在运动估计和插值质量上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件的VFI方法难以有效处理场景中的非线性运动，因事件的连续运动线索与图像的密集空间信息在时间维度上未对齐，常导致运动估计错误，降低VFI质量。

Method: 提出了TimeTracker框架：首先，使用场景感知区域分割（SARS）模块将场景划分为相似的图像块；然后，通过连续轨迹引导运动估计（CTME）模块利用事件数据跟踪每个图像块的连续运动轨迹；最后，通过全局运动优化和帧细化生成任意时刻的中间帧。

Result: 实验表明，所提出的TimeTracker方法在运动估计和视频帧插值质量方面均优于现有先进技术。此外，还收集了一个包含快速非线性运动的真实世界数据集。

Conclusion: TimeTracker中实现的连续点跟踪方法能有效解决事件引导VFI中的非线性运动问题，显著提升了运动估计的准确性和视频帧插值的质量。

Abstract: Video frame interpolation (VFI) that leverages the bio-inspired event cameras
as guidance has recently shown better performance and memory efficiency than
the frame-based methods, thanks to the event cameras' advantages, such as high
temporal resolution. A hurdle for event-based VFI is how to effectively deal
with non-linear motion, caused by the dynamic changes in motion direction and
speed within the scene. Existing methods either use events to estimate sparse
optical flow or fuse events with image features to estimate dense optical flow.
Unfortunately, motion errors often degrade the VFI quality as the continuous
motion cues from events do not align with the dense spatial information of
images in the temporal dimension. In this paper, we find that object motion is
continuous in space, tracking local regions over continuous time enables more
accurate identification of spatiotemporal feature correlations. In light of
this, we propose a novel continuous point tracking-based VFI framework, named
TimeTracker. Specifically, we first design a Scene-Aware Region Segmentation
(SARS) module to divide the scene into similar patches. Then, a Continuous
Trajectory guided Motion Estimation (CTME) module is proposed to track the
continuous motion trajectory of each patch through events. Finally,
intermediate frames at any given time are generated through global motion
optimization and frame refinement. Moreover, we collect a real-world dataset
that features fast non-linear motion. Extensive experiments show that our
method outperforms prior arts in both motion estimation and frame interpolation
quality.

</details>


### [49] [VISLIX: An XAI Framework for Validating Vision Models with Slice Discovery and Analysis](https://arxiv.org/abs/2505.03132)
*Xinyuan Yan,Xiwei Xuan,Jorge Piazentin Ono,Jiajing Guo,Vikram Mohanty,Shekar Arvind Kumar,Liang Gou,Bei Wang,Liu Ren*

Main category: cs.CV

TL;DR: VISLIX是一个利用基础模型帮助专家分析计算机视觉模型（尤其目标检测）中表现不佳数据切片的可视分析框架，无需元数据并支持交互式假设检验。


<details>
  <summary>Details</summary>
Motivation: 现有数据切片方法在视觉模型验证中存在局限：依赖额外元数据、理解耗时费力、缺乏交互式假设检验。

Method: 引入VISLIX，一个利用先进基础模型的可视分析框架，无需图像元数据，可自动生成自然语言洞察，并允许用户交互式测试数据切片假设。

Result: 通过专家研究和三个用例评估，证明VISLIX在为目标检测模型验证提供全面洞察方面是有效的。

Conclusion: VISLIX通过克服现有局限，有效地支持领域专家分析计算机视觉模型的数据切片，尤其适用于目标检测模型的验证。

Abstract: Real-world machine learning models require rigorous evaluation before
deployment, especially in safety-critical domains like autonomous driving and
surveillance. The evaluation of machine learning models often focuses on data
slices, which are subsets of the data that share a set of characteristics. Data
slice finding automatically identifies conditions or data subgroups where
models underperform, aiding developers in mitigating performance issues.
Despite its popularity and effectiveness, data slicing for vision model
validation faces several challenges. First, data slicing often needs additional
image metadata or visual concepts, and falls short in certain computer vision
tasks, such as object detection. Second, understanding data slices is a
labor-intensive and mentally demanding process that heavily relies on the
expert's domain knowledge. Third, data slicing lacks a human-in-the-loop
solution that allows experts to form hypothesis and test them interactively. To
overcome these limitations and better support the machine learning operations
lifecycle, we introduce VISLIX, a novel visual analytics framework that employs
state-of-the-art foundation models to help domain experts analyze slices in
computer vision models. Our approach does not require image metadata or visual
concepts, automatically generates natural language insights, and allows users
to test data slice hypothesis interactively. We evaluate VISLIX with an expert
study and three use cases, that demonstrate the effectiveness of our tool in
providing comprehensive insights for validating object detection models.

</details>


### [50] [Enhancing Glass Defect Detection with Diffusion Models: Addressing Imbalanced Datasets in Manufacturing Quality Control](https://arxiv.org/abs/2505.03134)
*Sajjad Rezvani Boroujeni,Hossein Abedi,Tom Bush*

Main category: cs.CV

TL;DR: 该研究提出使用去噪扩散概率模型（DDPMs）生成合成缺陷玻璃图像以解决数据不平衡问题，显著提高了工业玻璃制造中缺陷检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 工业玻璃制造中视觉缺陷检测因缺陷产品频率低导致数据集不平衡，这限制了深度学习模型和计算机视觉系统的性能。

Method: 采用去噪扩散概率模型（DDPMs）生成合成的缺陷玻璃产品图像进行数据增强，通过增加少数类别（缺陷样本）的表示来解决类别不平衡问题，并评估其对标准CNN架构（ResNet50V2, EfficientNetB0, MobileNetV2）图像分类性能的提升效果。

Result: 实验结果表明，使用DDPMs增强数据后，所有测试的深度神经网络架构在关键机器学习指标上均有显著改进，特别是在缺陷样本的召回率方面，同时保持了完美的精确率。ResNet50V2的整体分类准确率从78%提高到了93%。

Conclusion: 该工作为玻璃制造业的自动化缺陷检测提供了一种可扩展、成本效益高的方法，以增强检测性能，并且该方法有潜力扩展到其他面临类似类别不平衡挑战的工业质量保证系统和行业。

Abstract: Visual defect detection in industrial glass manufacturing remains a critical
challenge due to the low frequency of defective products, leading to imbalanced
datasets that limit the performance of deep learning models and computer vision
systems. This paper presents a novel approach using Denoising Diffusion
Probabilistic Models (DDPMs) to generate synthetic defective glass product
images for data augmentation, effectively addressing class imbalance issues in
manufacturing quality control and automated visual inspection. The methodology
significantly enhances image classification performance of standard CNN
architectures (ResNet50V2, EfficientNetB0, and MobileNetV2) in detecting
anomalies by increasing the minority class representation. Experimental results
demonstrate substantial improvements in key machine learning metrics,
particularly in recall for defective samples across all tested deep neural
network architectures while maintaining perfect precision. The most dramatic
improvement was observed in ResNet50V2's overall classification accuracy, which
increased from 78 percent to 93 percent when trained with the augmented data.
This work provides a scalable, cost-effective approach to enhancing automated
defect detection in glass manufacturing that can potentially be extended to
other industrial quality assurance systems and industries with similar class
imbalance challenges.

</details>


### [51] [Motion-compensated cardiac MRI using low-rank diffeomorphic flow (DMoCo)](https://arxiv.org/abs/2505.03149)
*Joseph William Kettelkamp,Ludovica Romanin,Sarv Priya,Mathews Jacob*

Main category: cs.CV

TL;DR: 提出了一种用于自由呼吸3D心脏MRI的无监督运动补偿图像重建新算法。


<details>
  <summary>Details</summary>
Motivation: 解决自由呼吸和非门控3D心脏MRI中的运动伪影问题，并提升图像重建质量。

Method: 该算法将各运动阶段的图像体积表示为单个静态图像模板的变形。核心贡献是利用低秩模型紧凑地联合表示由运动阶段参数化的微分同胚族。通过积分参数化速度场得到特定运动阶段的微分同胚，不同阶段的速度场也用低秩模型表示。静态模板和低秩运动模型参数以无监督方式直接从k空间数据中学习。

Result: 实验观察到，这种约束性更强的运动模型相比现有的运动分辨和运动补偿算法，在自由呼吸3D电影MRI中能提供改进的图像恢复效果。

Conclusion: 所提出的基于低秩运动模型的无监督方法能够有效改善自由呼吸3D心脏MRI的图像重建质量，优于当前算法。

Abstract: We introduce an unsupervised motion-compensated image reconstruction
algorithm for free-breathing and ungated 3D cardiac magnetic resonance imaging
(MRI). We express the image volume corresponding to each specific motion phase
as the deformation of a single static image template. The main contribution of
the work is the low-rank model for the compact joint representation of the
family of diffeomorphisms, parameterized by the motion phases. The
diffeomorphism at a specific motion phase is obtained by integrating a
parametric velocity field along a path connecting the reference template phase
to the motion phase. The velocity field at different phases is represented
using a low-rank model. The static template and the low-rank motion model
parameters are learned directly from the k-space data in an unsupervised
fashion. The more constrained motion model is observed to offer improved
recovery compared to current motion-resolved and motion-compensated algorithms
for free-breathing 3D cine MRI.

</details>


### [52] [Robust Fairness Vision-Language Learning for Medical Image Analysis](https://arxiv.org/abs/2505.03153)
*Sparsh Bansal,Mingyang Wu,Xin Wang,Shu Hu*

Main category: cs.CV

TL;DR: 论文提出了一种新框架，通过改进损失函数来增强医学视觉语言模型（VLM）的公平性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）在医学图像分析中潜力巨大，但确保其对所有患者的公平性和鲁棒性对于临床应用至关重要，现有模型在这方面可能存在不足。

Method: 该框架在训练时修改损失函数：1) 通过“动态不良样本挖掘”（Dynamic Bad Pair Mining）算法识别和调整错误的图像-文本对；2) 利用Sinkhorn距离确保受保护群体的损失分布与总体损失分布不发生显著偏离。

Result: 实验测试表明，所提出的框架在衡量公平性的AUC（equity-scaled AUC）指标上，性能提升高达8.6%。

Conclusion: 该框架能有效提升医学视觉语言模型在公平性和鲁棒性方面的表现，使其更适用于对所有患者都保持真实可靠的医疗场景。

Abstract: The advent of Vision-Language Models (VLMs) in medical image analysis has the
potential to help process multimodal inputs and increase performance over
traditional inference methods. However, when considering the domain in which
these models will be implemented, fairness and robustness are important to
ensure the model stays true for any patient. In this paper, we introduce a
framework for ensuring robustness and fairness of VLM models. This framework
modifies the loss function at training by identifying and adjusting faulty
image-text pairs through a Dynamic Bad Pair Mining algorithm and also utilizing
Sinkhorn distance to ensure the loss distributions of protected groups do not
deviate from the total loss. Experimental testing of our framework shows up to
a 8.6\% improvement when looking at equity-scaled AUC.

</details>


### [53] [StableMotion: Training Motion Cleanup Models with Unpaired Corrupted Data](https://arxiv.org/abs/2505.03154)
*Yuxuan Mu,Hung Yu Ling,Yi Shi,Ismael Baira Ojeda,Pengcheng Xi,Chang Shu,Fabio Zinno,Xue Bin Peng*

Main category: cs.CV

TL;DR: 提出了一种名为StableMotion的方法，它利用运动质量指标，从未配对的损坏动作捕捉数据中训练运动清理模型，显著减少了伪影。


<details>
  <summary>Details</summary>
Motivation: 动作捕捉数据常含有伪影，手动清理耗时耗力且成本高。现有数据驱动方法通常需要难以获取的成对损坏-干净训练数据。

Method: 提出了StableMotion方法，直接从未配对的损坏数据集中训练运动清理模型。核心是引入运动质量指标（可手动标记或通过启发式算法获得），使模型能在混合质量的原始运动数据上进行质量感知训练。测试时，模型可根据质量指标生成高质量运动。该方法基于简单的扩散框架实现，形成一个统一的运动生成-判别模型，既能识别也能修复损坏帧。

Result: 在包含真实世界运动伪影的245小时足球动捕数据集SoccerMocap上应用StableMotion进行训练。训练后的模型有效纠正了多种运动伪影，使运动跳变减少了68%，冻结帧减少了81%。

Conclusion: StableMotion是一种在生产场景中，从未经处理的原始动捕数据训练运动清理模型的有效方法，即使没有配对的干净数据。它为自动化清理损坏的动作捕捉数据提供了一个实用方案。

Abstract: Motion capture (mocap) data often exhibits visually jarring artifacts due to
inaccurate sensors and post-processing. Cleaning this corrupted data can
require substantial manual effort from human experts, which can be a costly and
time-consuming process. Previous data-driven motion cleanup methods offer the
promise of automating this cleanup process, but often require in-domain paired
corrupted-to-clean training data. Constructing such paired datasets requires
access to high-quality, relatively artifact-free motion clips, which often
necessitates laborious manual cleanup. In this work, we present StableMotion, a
simple yet effective method for training motion cleanup models directly from
unpaired corrupted datasets that need cleanup. The core component of our method
is the introduction of motion quality indicators, which can be easily annotated
through manual labeling or heuristic algorithms and enable training of
quality-aware motion generation models on raw motion data with mixed quality.
At test time, the model can be prompted to generate high-quality motions using
the quality indicators. Our method can be implemented through a simple
diffusion-based framework, leading to a unified motion generate-discriminate
model, which can be used to both identify and fix corrupted frames. We
demonstrate that our proposed method is effective for training motion cleanup
models on raw mocap data in production scenarios by applying StableMotion to
SoccerMocap, a 245-hour soccer mocap dataset containing real-world motion
artifacts. The trained model effectively corrects a wide range of motion
artifacts, reducing motion pops and frozen frames by 68% and 81%, respectively.
See https://youtu.be/3Y7MMAH02B4 for more results.

</details>


### [54] [RAVU: Retrieval Augmented Video Understanding with Compositional Reasoning over Graph](https://arxiv.org/abs/2505.03173)
*Sameer Malik,Moyuru Yamada,Ayush Singh,Dishank Aggarwal*

Main category: cs.CV

TL;DR: 该研究提出RAVU框架，通过构建时空图谱并进行检索增强，以提升大型多模态模型对长视频的理解能力，特别是处理复杂查询。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型（LMMs）因缺乏明确的记忆和检索机制，难以有效处理长视频内容。

Method: 提出了RAVU（Retrieval Augmented Video Understanding）框架。该框架首先构建视频的时空图谱表示，捕捉实体间的空间和时间关系，作为长期记忆。然后，将复杂查询分解为推理步骤，并在图谱上执行这些步骤以检索相关关键信息。

Result: RAVU 在仅使用少量检索帧（5-10帧）的情况下，在NExT-QA和EgoSchema两个主要视频问答数据集上，表现优于其他先进方法和基线模型，尤其在需要多跳推理和跨帧追踪对象的查询上效果显著。

Conclusion: RAVU框架通过结合时空图谱的检索增强和组合推理，显著提高了大型多模态模型对长视频的理解能力和复杂查询的应答准确性。

Abstract: Comprehending long videos remains a significant challenge for Large
Multi-modal Models (LMMs). Current LMMs struggle to process even minutes to
hours videos due to their lack of explicit memory and retrieval mechanisms. To
address this limitation, we propose RAVU (Retrieval Augmented Video
Understanding), a novel framework for video understanding enhanced by retrieval
with compositional reasoning over a spatio-temporal graph. We construct a graph
representation of the video, capturing both spatial and temporal relationships
between entities. This graph serves as a long-term memory, allowing us to track
objects and their actions across time. To answer complex queries, we decompose
the queries into a sequence of reasoning steps and execute these steps on the
graph, retrieving relevant key information. Our approach enables more accurate
understanding of long videos, particularly for queries that require multi-hop
reasoning and tracking objects across frames. Our approach demonstrate superior
performances with limited retrieved frames (5-10) compared with other SOTA
methods and baselines on two major video QA datasets, NExT-QA and EgoSchema.

</details>


### [55] [seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models](https://arxiv.org/abs/2505.03176)
*Hafez Ghaemi,Eilif Muller,Shahab Bakhtiari*

Main category: cs.CV

TL;DR: 提出了一种名为seq-JEPA的自监督学习方法，通过处理图像视图序列和相应的变换，同时学习到对变换不变和等变的表示，解决了现有方法在这两者之间的性能权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前的自监督算法主要依赖数据增强和掩码等变换来学习视觉表示，通常采用双视图范式诱导不变性或等变性。这种范式限制了学习表示在下游任务中的灵活性，导致在不变性相关任务（如图像分类）和更细粒度的等变性相关任务之间产生性能权衡。

Method: 引入了seq-JEPA，一种基于联合嵌入预测架构的世界建模范式。该模型处理输入图像的一系列不同视图（观察值），每个编码后的视图与产生序列中下一个观察值的相对变换（动作）的嵌入连接起来。一个Transformer编码器输出此序列的聚合表示，该表示随后以导致下一个观察值的动作为条件，来预测下一个观察值的表示。通过这种方式，无需额外的等变性预测器或损失项，seq-JEPA就能同时学习到两种架构上分离的表示：一种对指定变换是等变的，另一种则是不变的。

Result: 实验表明，seq-JEPA在等变性基准测试和图像分类任务上均取得了优异性能，且无需牺牲其中一项来换取另一项。此外，该框架在需要聚合观察序列的任务（如跨动作的路径整合和跨眼动追踪的预测学习）中表现出色。

Conclusion: seq-JEPA通过其独特的序列处理和联合嵌入预测架构，成功地解决了自监督学习中不变性表示和等变性表示之间的性能权衡问题，能够同时学习到高质量的两种表示，并在相关下游任务中展现出强大性能和灵活性。

Abstract: Current self-supervised algorithms mostly rely on transformations such as
data augmentation and masking to learn visual representations. This is achieved
by inducing invariance or equivariance with respect to these transformations
after encoding two views of an image. This dominant two-view paradigm can limit
the flexibility of learned representations for downstream adaptation by
creating performance trade-offs between invariance-related tasks such as image
classification and more fine-grained equivariance-related tasks. In this work,
we introduce \emph{seq-JEPA}, a world modeling paradigm based on
joint-embedding predictive architecture that leverages architectural inductive
biases to resolve this trade-off. Without requiring an additional equivariance
predictor or loss term, seq-JEPA simultaneously learns two architecturally
segregated representations: one equivariant to the specified transformations
and another invariant to them and suited for tasks such as classification. To
do so, our model processes a short sequence of different views (observations)
of an input image. Each encoded view is concatenated with embeddings
corresponding to the relative transformation (action) producing the next
observation in the sequence. A transformer encoder outputs an aggregate
representation of this sequence, which is subsequently conditioned on the
action leading to the next observation to predict its representation.
Empirically, seq-JEPA achieves strong performance on equivariant benchmarks and
image classification without sacrificing one for the other. Additionally, our
framework excels at tasks that inherently require aggregating a sequence of
observations, such as path integration across actions and predictive learning
across eye movements.

</details>


### [56] [Interactive Instance Annotation with Siamese Networks](https://arxiv.org/abs/2505.03184)
*Xiang Xu,Ruotong Li,Mengjun Yi,Baile XU,Furao Shen,Jian Zhao*

Main category: cs.CV

TL;DR: SiamAnno是一种基于暹罗网络的新型单样本实例标注框架，能有效处理跨域任务，提高标注效率。


<details>
  <summary>Details</summary>
Motivation: 实例掩码标注耗时费力，现有深度学习方法在跨域场景下（即模型在未见过的数据类型上进行标注）效果不佳。

Method: 提出SiamAnno框架，借鉴了目标跟踪中暹罗网络的思想。该框架通过单样本学习，接收一个边界框作为输入，预测出目标物体的边界，用户随后可对预测结果进行调整。

Result: SiamAnno在一个数据集上训练，并在其他不同类型的数据集上进行测试（无需微调），均取得了当前最佳性能（SOTA），表明其能有效应对跨域任务中的域迁移和环境变化。

Conclusion: SiamAnno是首个将暹罗网络架构应用于实例标注的模型，为跨域实例标注提供了一个强大的基线，并展示了其在该领域的潜力。

Abstract: Annotating instance masks is time-consuming and labor-intensive. A promising
solution is to predict contours using a deep learning model and then allow
users to refine them. However, most existing methods focus on in-domain
scenarios, limiting their effectiveness for cross-domain annotation tasks. In
this paper, we propose SiamAnno, a framework inspired by the use of Siamese
networks in object tracking. SiamAnno leverages one-shot learning to annotate
previously unseen objects by taking a bounding box as input and predicting
object boundaries, which can then be adjusted by annotators. Trained on one
dataset and tested on another without fine-tuning, SiamAnno achieves
state-of-the-art (SOTA) performance across multiple datasets, demonstrating its
ability to handle domain and environment shifts in cross-domain tasks. We also
provide more comprehensive results compared to previous work, establishing a
strong baseline for future research. To our knowledge, SiamAnno is the first
model to explore Siamese architecture for instance annotation.

</details>


### [57] [PiCo: Enhancing Text-Image Alignment with Improved Noise Selection and Precise Mask Control in Diffusion Models](https://arxiv.org/abs/2505.03203)
*Chang Xie,Chenyi Zhuang,Pan Gao*

Main category: cs.CV

TL;DR: PiCo是一种免训练方法，通过优化初始噪声选择和使用参考掩码，提升了扩散模型在复杂文本提示下的图文对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在处理复杂文本提示时，难以实现良好的图文对齐，主要受随机初始化噪声质量和生成控制掩码可靠性的影响。

Method: 提出了PiCo（Pick-and-Control）方法，包含两个关键组件：1) 噪声选择模块，评估并选择适合目标文本的初始噪声，采用快速采样策略保证效率；2) 参考掩码模块，生成像素级掩码精确调制交叉注意力图，引导文本与图像特征的合理交互。

Result: 大量实验证明，PiCo能有效将用户从繁琐的随机生成过程中解放出来，并增强了不同文本描述下的图文对齐效果。

Conclusion: PiCo是一种有效的免训练方法，能够显著改善扩散模型在复杂文本提示下的图文对齐问题。

Abstract: Advanced diffusion models have made notable progress in text-to-image
compositional generation. However, it is still a challenge for existing models
to achieve text-image alignment when confronted with complex text prompts. In
this work, we highlight two factors that affect this alignment: the quality of
the randomly initialized noise and the reliability of the generated controlling
mask. We then propose PiCo (Pick-and-Control), a novel training-free approach
with two key components to tackle these two factors. First, we develop a noise
selection module to assess the quality of the random noise and determine
whether the noise is suitable for the target text. A fast sampling strategy is
utilized to ensure efficiency in the noise selection stage. Second, we
introduce a referring mask module to generate pixel-level masks and to
precisely modulate the cross-attention maps. The referring mask is applied to
the standard diffusion process to guide the reasonable interaction between text
and image features. Extensive experiments have been conducted to verify the
effectiveness of PiCo in liberating users from the tedious process of random
generation and in enhancing the text-image alignment for diverse text
descriptions.

</details>


### [58] [DCS-ST for Classification of Breast Cancer Histopathology Images with Limited Annotations](https://arxiv.org/abs/2505.03204)
*Liu Suxing,Byungwon Min*

Main category: cs.CV

TL;DR: 深度学习在乳腺癌组织病理图像分类中前景广阔，但标注数据不足时性能会下降。


<details>
  <summary>Details</summary>
Motivation: 由于医学影像标注成本高昂且需要专业知识，导致标注数据有限，这限制了深度学习模型在乳腺癌图像分类中的性能。

Method: （摘要未明确提出新的解决方法，而是指出了现有深度学习方法存在的问题）深度学习方法。

Result: （摘要描述的是现有方法的结果）当标注数据有限时，深度学习方法的性能通常会下降。

Conclusion: 在乳腺癌组织病理学图像分类中，有限的标注数据是深度学习方法面临的一个关键挑战，这主要是由于标注的高成本和所需的专业知识造成的。

Abstract: Deep learning methods have shown promise in classifying breast cancer
histopathology images, but their performance often declines with limited
annotated data, a critical challenge in medical imaging due to the high cost
and expertise required for annotations.

</details>


### [59] [Dual-Domain Masked Image Modeling: A Self-Supervised Pretraining Strategy Using Spatial and Frequency Domain Masking for Hyperspectral Data](https://arxiv.org/abs/2505.03220)
*Shaheer Mohamed,Tharindu Fernando,Sridha Sridharan,Peyman Moghadam,Clinton Fookes*

Main category: cs.CV

TL;DR: 提出了一种名为SFMIM的自监督预训练策略，通过空间和频率双域掩蔽从未标记的高光谱图像中学习，以提升Transformer模型的性能。


<details>
  <summary>Details</summary>
Motivation: 标记的高光谱图像数据稀缺，限制了深度学习（尤其是需要大规模训练的Transformer架构）的潜力，因此需要一种利用大量未标记数据进行预训练的方法。

Method: 提出空间-频率掩蔽图像建模（SFMIM）策略。该方法在空间域对图像块进行随机掩蔽并重建，同时在频率域移除部分频率分量并预测，从而使Transformer编码器学习高阶谱空相关性。

Result: 在三个公开的高光谱图像分类基准上评估，SFMIM 取得了最先进的性能，并且在微调阶段模型收敛速度快。

Conclusion: SFMIM 是一种有效的高光谱图像自监督预训练策略，它能有效利用未标记数据，提升了Transformer模型在高光谱图像分类任务上的性能和微调效率。

Abstract: Hyperspectral images (HSIs) capture rich spectral signatures that reveal
vital material properties, offering broad applicability across various domains.
However, the scarcity of labeled HSI data limits the full potential of deep
learning, especially for transformer-based architectures that require
large-scale training. To address this constraint, we propose Spatial-Frequency
Masked Image Modeling (SFMIM), a self-supervised pretraining strategy for
hyperspectral data that utilizes the large portion of unlabeled data. Our
method introduces a novel dual-domain masking mechanism that operates in both
spatial and frequency domains. The input HSI cube is initially divided into
non-overlapping patches along the spatial dimension, with each patch comprising
the entire spectrum of its corresponding spatial location. In spatial masking,
we randomly mask selected patches and train the model to reconstruct the masked
inputs using the visible patches. Concurrently, in frequency masking, we remove
portions of the frequency components of the input spectra and predict the
missing frequencies. By learning to reconstruct these masked components, the
transformer-based encoder captures higher-order spectral-spatial correlations.
We evaluate our approach on three publicly available HSI classification
benchmarks and demonstrate that it achieves state-of-the-art performance.
Notably, our model shows rapid convergence during fine-tuning, highlighting the
efficiency of our pretraining strategy.

</details>


### [60] [Seeing the Abstract: Translating the Abstract Language for Vision Language Models](https://arxiv.org/abs/2505.03242)
*Davide Talon,Federico Girella,Ziyue Liu,Marco Cristani,Yiming Wang*

Main category: cs.CV

TL;DR: 该研究揭示了视觉语言模型（VLMs）在处理时尚领域等抽象语言方面的不足，并提出了一种免训练的“抽象到具体翻译器”（ACT）方法，通过将抽象概念映射到具体的表示来提升VLMs的性能，尤其在文本到图像检索任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉语言模型（VLMs）未能充分关注和有效表征视觉内容描述中的抽象概念（如情感、创意），尤其在时尚等富含抽象表达的领域。现有VLMs的预训练数据缺乏足够的抽象词汇，限制了它们理解和处理抽象语言的能力。

Method: 提出了一种免训练、模型无关的方法，名为“抽象到具体翻译器”（Abstract-to-Concrete Translator, ACT）。该方法利用预训练模型和现有的多模态数据库，在VLM的潜在空间中将抽象语言的表示迁移到模型已经充分学习的具体概念的表示上。

Result: 在文本到图像检索任务中，ACT方法即使无需额外训练，其性能也优于经过微调的VLMs，无论是在相同数据集还是跨数据集的设置下，均展现出强大的泛化能力。此外，ACT带来的改进对多种不同的VLMs均有效，证明其即插即用的特性。

Conclusion: 该研究揭示了抽象语言在视觉语言理解中的重要性及其当前VLMs处理能力的不足。提出的ACT方法是一种有效且通用的免训练解决方案，能够显著提升现有VLMs对抽象语言的表征和处理能力，特别是在时尚领域的图文检索任务中。

Abstract: Natural language goes beyond dryly describing visual content. It contains
rich abstract concepts to express feeling, creativity and properties that
cannot be directly perceived. Yet, current research in Vision Language Models
(VLMs) has not shed light on abstract-oriented language. Our research breaks
new ground by uncovering its wide presence and under-estimated value, with
extensive analysis. Particularly, we focus our investigation on the fashion
domain, a highly-representative field with abstract expressions. By analyzing
recent large-scale multimodal fashion datasets, we find that abstract terms
have a dominant presence, rivaling the concrete ones, providing novel
information, and being useful in the retrieval task. However, a critical
challenge emerges: current general-purpose or fashion-specific VLMs are
pre-trained with databases that lack sufficient abstract words in their text
corpora, thus hindering their ability to effectively represent
abstract-oriented language. We propose a training-free and model-agnostic
method, Abstract-to-Concrete Translator (ACT), to shift abstract
representations towards well-represented concrete ones in the VLM latent space,
using pre-trained models and existing multimodal databases. On the
text-to-image retrieval task, despite being training-free, ACT outperforms the
fine-tuned VLMs in both same- and cross-dataset settings, exhibiting its
effectiveness with a strong generalization capability. Moreover, the
improvement introduced by ACT is consistent with various VLMs, making it a
plug-and-play solution.

</details>


### [61] [PROM: Prioritize Reduction of Multiplications Over Lower Bit-Widths for Efficient CNNs](https://arxiv.org/abs/2505.03254)
*Lukas Meiner,Jens Mehnert,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: PROM是一种针对深度可分离卷积网络的量化方法，通过对逐点卷积使用三元权重、其余模块使用8位权重，显著降低模型能耗和存储，同时保持分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法未能充分利用深度可分离卷积网络中计算成本分布不均（逐点操作最昂贵）的特点，导致无法最大化效率增益。

Method: 提出PROM方法：选择性地使用两种不同位宽进行量化。逐点卷积被量化为三元权重，激活值量化为8位（将乘法转为int8加法）；网络的其余模块使用8位权重。此过程通过量化感知训练实现。

Result: 将PROM应用于MobileNetV2，与float16基线相比，模型能耗降低了23.9倍，存储大小减少了2.7倍，同时在ImageNet上保持了相似的分类性能，并提升了能耗与准确率的帕累托前沿。

Conclusion: PROM通过将深度可分离卷积网络有效地量化为三元和8位权重，为降低此类网络的能耗和存储大小提供了一种简单有效的方法。

Abstract: Convolutional neural networks (CNNs) are crucial for computer vision tasks on
resource-constrained devices. Quantization effectively compresses these models,
reducing storage size and energy cost. However, in modern depthwise-separable
architectures, the computational cost is distributed unevenly across its
components, with pointwise operations being the most expensive. By applying a
general quantization scheme to this imbalanced cost distribution, existing
quantization approaches fail to fully exploit potential efficiency gains. To
this end, we introduce PROM, a straightforward approach for quantizing modern
depthwise-separable convolutional networks by selectively using two distinct
bit-widths. Specifically, pointwise convolutions are quantized to ternary
weights, while the remaining modules use 8-bit weights, which is achieved
through a simple quantization-aware training procedure. Additionally, by
quantizing activations to 8-bit, our method transforms pointwise convolutions
with ternary weights into int8 additions, which enjoy broad support across
hardware platforms and effectively eliminates the need for expensive
multiplications. Applying PROM to MobileNetV2 reduces the model's energy cost
by more than an order of magnitude (23.9x) and its storage size by 2.7x
compared to the float16 baseline while retaining similar classification
performance on ImageNet. Our method advances the Pareto frontier for energy
consumption vs. top-1 accuracy for quantized convolutional models on ImageNet.
PROM addresses the challenges of quantizing depthwise-separable convolutional
networks to both ternary and 8-bit weights, offering a simple way to reduce
energy cost and storage size.

</details>


### [62] [DiffVQA: Video Quality Assessment Using Diffusion Feature Extractor](https://arxiv.org/abs/2505.03261)
*Wei-Ting Chen,Yu-Jiet Vong,Yi-Tsung Lee,Sy-Yen Kuo,Qiang Gao,Sizhuo Ma,Jian Wang*

Main category: cs.CV

TL;DR: 提出了一种新的视频质量评估框架DiffVQA，它利用预训练的扩散模型提取特征，并结合Mamba模块处理时序信息，在多个数据集上展现了优越的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于CNN和ViT的视频质量评估方法难以与人类感知紧密对齐，尤其在多样化的真实场景中，且受到可用数据集规模和多样性有限的挑战。

Method: 引入了DiffVQA框架：1. 利用在大型数据集上预训练的扩散模型的强大泛化能力。2. 通过控制模块调整扩散模型以重建相同的输入帧。3. 使用调整后的扩散模型分别从调整大小分支和裁剪分支提取语义和失真特征。4. 并行引入Mamba模块提取时序一致性增强特征，并与扩散特征融合以预测最终得分。

Result: 在多个数据集上的实验表明，DiffVQA在数据集内评估中表现优越，并具有出色的跨数据集泛化能力。

Conclusion: 利用扩散模型作为特征提取器，相比CNN和ViT骨干网络，可以提供更优的视频质量评估性能。

Abstract: Video Quality Assessment (VQA) aims to evaluate video quality based on
perceptual distortions and human preferences. Despite the promising performance
of existing methods using Convolutional Neural Networks (CNNs) and Vision
Transformers (ViTs), they often struggle to align closely with human
perceptions, particularly in diverse real-world scenarios. This challenge is
exacerbated by the limited scale and diversity of available datasets. To
address this limitation, we introduce a novel VQA framework, DiffVQA, which
harnesses the robust generalization capabilities of diffusion models
pre-trained on extensive datasets. Our framework adapts these models to
reconstruct identical input frames through a control module. The adapted
diffusion model is then used to extract semantic and distortion features from a
resizing branch and a cropping branch, respectively. To enhance the model's
ability to handle long-term temporal dynamics, a parallel Mamba module is
introduced, which extracts temporal coherence augmented features that are
merged with the diffusion features to predict the final score. Experiments
across multiple datasets demonstrate DiffVQA's superior performance on
intra-dataset evaluations and its exceptional generalization across datasets.
These results confirm that leveraging a diffusion model as a feature extractor
can offer enhanced VQA performance compared to CNN and ViT backbones.

</details>


### [63] [OccCylindrical: Multi-Modal Fusion with Cylindrical Representation for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2505.03284)
*Zhenxing Ming,Julie Stephany Berrio,Mao Shan,Yaoqi Huang,Hongyu Lyu,Nguyen Hoang Khoi Tran,Tzu-Yun Tseng,Stewart Worrall*

Main category: cs.CV

TL;DR: 本文提出了一种名为OccCylindrical的新方法，在柱状坐标系下融合和优化多模态特征，用于3D语义占据预测，以保留更精细的几何细节并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于多传感器融合的3D语义占据预测方法主要在笛卡尔坐标系下处理传感器信息，这忽略了传感器读数的分布特性，导致精细几何细节丢失和性能下降。

Method: 提出OccCylindrical方法，在柱状坐标系（Cylindrical Coordinates）下融合和优化不同模态的特征。

Result: 在nuScenes数据集上的大量实验（包括雨天和夜间等挑战性场景）表明，所提出的方法有效，并达到了最先进的性能水平。

Conclusion: 在柱状坐标系下融合多模态特征进行3D语义占据预测，能够保留更精细的几何细节，从而提升了模型性能。

Abstract: The safe operation of autonomous vehicles (AVs) is highly dependent on their
understanding of the surroundings. For this, the task of 3D semantic occupancy
prediction divides the space around the sensors into voxels, and labels each
voxel with both occupancy and semantic information. Recent perception models
have used multisensor fusion to perform this task. However, existing
multisensor fusion-based approaches focus mainly on using sensor information in
the Cartesian coordinate system. This ignores the distribution of the sensor
readings, leading to a loss of fine-grained details and performance
degradation. In this paper, we propose OccCylindrical that merges and refines
the different modality features under cylindrical coordinates. Our method
preserves more fine-grained geometry detail that leads to better performance.
Extensive experiments conducted on the nuScenes dataset, including challenging
rainy and nighttime scenarios, confirm our approach's effectiveness and
state-of-the-art performance. The code will be available at:
https://github.com/DanielMing123/OccCylindrical

</details>


### [64] [Base-Detail Feature Learning Framework for Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2505.03286)
*Zhihao Gong,Lian Wu,Yong Xu*

Main category: cs.CV

TL;DR: 该研究针对可见光-红外行人重识别（VIReID）中现有方法忽略模态特定细节的问题，提出了一种基础-细节特征学习框架（BDLF）。BDLF通过专门模块分别学习模态共享的基础特征和模态特定的细节特征，并利用新的相关性约束方法，有效提升了跨模态行人重识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有VIReID方法在处理可见光与红外模态的显著差异时性能不足，主要原因是它们侧重于挖掘模态共享信息，而忽略了宝贵的模态特定细节信息，导致无法充分利用不同模态的差异化信息。

Method: 提出了一种基础-细节特征学习框架（BDLF）。该框架通过一个无损细节特征提取模块挖掘模态特定细节，通过一个互补基础嵌入生成机制学习模态共享的基础特征，并辅以一种新的相关性限制方法，以确保学习到的特征能同时增强两种模态的基础和细节知识。

Result: 在SYSU-MM01、RegDB和LLCM三个基准数据集上进行的大量实验结果验证了所提出BDLF框架的有效性。

Conclusion: BDLF框架通过同时学习和利用模态共享的基础知识与模态特定的细节知识，有效应对了VIReID中的模态差异挑战，并显著提高了行人重识别的性能。

Abstract: Visible-infrared person re-identification (VIReID) provides a solution for
ReID tasks in 24-hour scenarios; however, significant challenges persist in
achieving satisfactory performance due to the substantial discrepancies between
visible (VIS) and infrared (IR) modalities. Existing methods inadequately
leverage information from different modalities, primarily focusing on digging
distinguishing features from modality-shared information while neglecting
modality-specific details. To fully utilize differentiated minutiae, we propose
a Base-Detail Feature Learning Framework (BDLF) that enhances the learning of
both base and detail knowledge, thereby capitalizing on both modality-shared
and modality-specific information. Specifically, the proposed BDLF mines detail
and base features through a lossless detail feature extraction module and a
complementary base embedding generation mechanism, respectively, supported by a
novel correlation restriction method that ensures the features gained by BDLF
enrich both detail and base knowledge across VIS and IR features. Comprehensive
experiments conducted on the SYSU-MM01, RegDB, and LLCM datasets validate the
effectiveness of BDLF.

</details>


### [65] [Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach](https://arxiv.org/abs/2505.03299)
*Pierre Adorni,Minh-Tan Pham,Stéphane May,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: 提出一种低成本的“能力编码”方法，无需微调即可预测地球观测（EO）基础模型在下游任务的性能，以简化模型选择。


<details>
  <summary>Details</summary>
Motivation: 地球观测领域已开发出超过75种遥感视觉基础模型，但没有一个模型能在所有下游任务中都表现最佳，导致模型选择困难且成本高昂。

Method: 提出一种名为“能力编码”的低成本预测方法，该方法无需在每个下游任务上进行微调来评估模型性能。

Result: 该方法能够简化为给定新任务选择基础模型的过程，并为现有文献提供新视角。

Conclusion: “能力编码”方法有助于简化模型选择，并为地球观测基础模型的未来研究提供了新视角和方向。

Abstract: Foundation models constitute a significant advancement in computer vision:
after a single, albeit costly, training phase, they can address a wide array of
tasks. In the field of Earth observation, over 75 remote sensing vision
foundation models have been developed in the past four years. However, none has
consistently outperformed the others across all available downstream tasks. To
facilitate their comparison, we propose a cost-effective method for predicting
a model's performance on multiple downstream tasks without the need for
fine-tuning on each one. This method is based on what we call "capabilities
encoding." The utility of this novel approach is twofold: we demonstrate its
potential to simplify the selection of a foundation model for a given new task,
and we employ it to offer a fresh perspective on the existing literature,
suggesting avenues for future research. Codes are available at
https://github.com/pierreadorni/capabilities-encoding.

</details>


### [66] [3D Can Be Explored In 2D: Pseudo-Label Generation for LiDAR Point Clouds Using Sensor-Intensity-Based 2D Semantic Segmentation](https://arxiv.org/abs/2505.03300)
*Andrew Caunes,Thierry Chateau,Vincent Frémont*

Main category: cs.CV

TL;DR: 论文提出一种无需3D标注的3D LiDAR点云语义分割新方法，通过将点云投影到2D视图，利用2D分割模型处理后反投影回3D，生成伪标签并应用于无监督域自适应。


<details>
  <summary>Details</summary>
Motivation: 传统的3D LiDAR语义分割依赖大量标注数据，并面临领域迁移问题。该研究旨在开发一种无需直接3D标注或推理时额外模态（如相机图像）的方法。

Method: 1. 从LiDAR扫描（根据传感器强度着色）生成2D视图。2. 使用在相机域预训练的2D语义分割模型处理这些视图。3. 将分割后的2D结果反投影到3D点云上。4. 通过基于投票的估计器合并每个3D点的标签。

Result: 成功构建了一个无需先验3D标注且推理时无需其他模态输入的3D语义分割全局流程。生成的伪标签在无监督域自适应（UDA）任务中显示出潜力。进行了详细的消融研究。

Conclusion: 该方法为3D语义分割提供了一个无需3D标注的有效途径，可用于生成伪标签，并对解决无监督域自适应问题具有积极作用。

Abstract: Semantic segmentation of 3D LiDAR point clouds, essential for autonomous
driving and infrastructure management, is best achieved by supervised learning,
which demands extensive annotated datasets and faces the problem of domain
shifts. We introduce a new 3D semantic segmentation pipeline that leverages
aligned scenes and state-of-the-art 2D segmentation methods, avoiding the need
for direct 3D annotation or reliance on additional modalities such as camera
images at inference time. Our approach generates 2D views from LiDAR scans
colored by sensor intensity and applies 2D semantic segmentation to these views
using a camera-domain pretrained model. The segmented 2D outputs are then
back-projected onto the 3D points, with a simple voting-based estimator that
merges the labels associated to each 3D point. Our main contribution is a
global pipeline for 3D semantic segmentation requiring no prior 3D annotation
and not other modality for inference, which can be used for pseudo-label
generation. We conduct a thorough ablation study and demonstrate the potential
of the generated pseudo-labels for the Unsupervised Domain Adaptation task.

</details>


### [67] [Comparative Analysis of Lightweight Deep Learning Models for Memory-Constrained Devices](https://arxiv.org/abs/2505.03303)
*Tasnim Shahriar*

Main category: cs.CV

TL;DR: 该论文评估了五种轻量级深度学习模型在资源受限环境下的图像分类性能，并研究了迁移学习对模型表现的影响。


<details>
  <summary>Details</summary>
Motivation: 评估轻量级深度学习模型在低内存等资源受限环境中的适用性，以便在边缘计算和移动平台上优化部署深度学习系统。

Method: 在三个数据集（CIFAR-10, CIFAR-100, Tiny ImageNet）上对五种先进的轻量级架构（MobileNetV3 Small, ResNet18, SqueezeNet, EfficientNetV2-S, ShuffleNetV2）进行基准测试。评估指标包括分类准确率、推理时间、浮点运算次数（FLOPs）和模型大小。同时，通过比较预训练模型与从头训练模型（以MobileNetV3 Small为例），研究了超参数调整、数据增强和训练范式的影响。

Result: 迁移学习显著提高了模型的准确性和计算效率，尤其是在复杂数据集（如Tiny ImageNet）上。EfficientNetV2 获得了最高的准确率，MobileNetV3 在准确性和效率之间提供了最佳平衡，而 SqueezeNet 在推理速度和模型紧凑性方面表现出色。

Conclusion: 研究强调了在准确性和效率之间的关键权衡，为在计算资源有限的实际应用中部署轻量级模型提供了可操作的见解，有助于优化边缘计算和移动平台的深度学习系统。

Abstract: This paper presents a comprehensive evaluation of lightweight deep learning
models for image classification, emphasizing their suitability for deployment
in resource-constrained environments such as low-memory devices. Five
state-of-the-art architectures - MobileNetV3 Small, ResNet18, SqueezeNet,
EfficientNetV2-S, and ShuffleNetV2 - are benchmarked across three diverse
datasets: CIFAR-10, CIFAR-100, and Tiny ImageNet. The models are assessed using
four key performance metrics: classification accuracy, inference time,
floating-point operations (FLOPs), and model size. Additionally, we investigate
the impact of hyperparameter tuning, data augmentation, and training paradigms
by comparing pretrained models with scratch-trained counterparts, focusing on
MobileNetV3 Small. Our findings reveal that transfer learning significantly
enhances model accuracy and computational efficiency, particularly for complex
datasets like Tiny ImageNet. EfficientNetV2 consistently achieves the highest
accuracy, while MobileNetV3 offers the best balance between accuracy and
efficiency, and SqueezeNet excels in inference speed and compactness. This
study highlights critical trade-offs between accuracy and efficiency, offering
actionable insights for deploying lightweight models in real-world applications
where computational resources are limited. By addressing these challenges, this
research contributes to optimizing deep learning systems for edge computing and
mobile platforms.

</details>


### [68] [3D Gaussian Splatting Data Compression with Mixture of Priors](https://arxiv.org/abs/2505.03310)
*Lei Liu,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 该研究提出了一种名为“混合先验 (MoP)”的新策略，通过改进熵模型和量化策略来解决3D高斯溅射 (3DGS) 数据压缩的现有局限性，从而实现高效存储和传输。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS数据压缩方法在熵模型构建和量化策略方面存在不足，未能充分利用超先验信息和实现细粒度的逐元素量化，限制了压缩效率。

Method: 提出混合先验 (MoP) 策略：1) 借鉴混合专家 (MoE) 思想，通过多个轻量级MLP处理超先验信息生成多样化先验特征，并用门控机制整合；2) 对于无损压缩，利用MoP特征改进条件熵模型；3) 对于有损压缩，利用MoP特征指导逐元素量化，采用先验引导的粗到细量化 (C2FQ) 策略，自适应优化量化步长矩阵。

Result: 所提出的3DGS数据压缩框架在多个基准测试（包括Mip-NeRF360, BungeeNeRF, DeepBlending, 和Tank&Temples）上均取得了当前最佳的压缩性能。

Conclusion: MoP策略及其引导的C2FQ量化方法有效提升了3DGS数据的无损和有损压缩性能，为高效3D场景建模中的数据存储与传输提供了先进解决方案。

Abstract: 3D Gaussian Splatting (3DGS) data compression is crucial for enabling
efficient storage and transmission in 3D scene modeling. However, its
development remains limited due to inadequate entropy models and suboptimal
quantization strategies for both lossless and lossy compression scenarios,
where existing methods have yet to 1) fully leverage hyperprior information to
construct robust conditional entropy models, and 2) apply fine-grained,
element-wise quantization strategies for improved compression granularity. In
this work, we propose a novel Mixture of Priors (MoP) strategy to
simultaneously address these two challenges. Specifically, inspired by the
Mixture-of-Experts (MoE) paradigm, our MoP approach processes hyperprior
information through multiple lightweight MLPs to generate diverse prior
features, which are subsequently integrated into the MoP feature via a gating
mechanism. To enhance lossless compression, the resulting MoP feature is
utilized as a hyperprior to improve conditional entropy modeling. Meanwhile,
for lossy compression, we employ the MoP feature as guidance information in an
element-wise quantization procedure, leveraging a prior-guided Coarse-to-Fine
Quantization (C2FQ) strategy with a predefined quantization step value.
Specifically, we expand the quantization step value into a matrix and
adaptively refine it from coarse to fine granularity, guided by the MoP
feature, thereby obtaining a quantization step matrix that facilitates
element-wise quantization. Extensive experiments demonstrate that our proposed
3DGS data compression framework achieves state-of-the-art performance across
multiple benchmarks, including Mip-NeRF360, BungeeNeRF, DeepBlending, and
Tank&Temples.

</details>


### [69] [Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.03318)
*Yibin Wang,Zhimin Li,Yuhang Zang,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为UnifiedReward-Think的统一多模态思维链（CoT）奖励模型，通过显式长链推理来增强视觉模型与人类偏好对齐的奖励信号的可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态奖励模型（RMs）在推理深度上受限，常导致奖励信号不准确。研究者认为，将显式的长思维链（CoT）融入奖励推理过程可以显著增强其可靠性和鲁棒性，并且一旦RMs内化CoT推理，其直接响应准确性也能通过隐式推理能力得到提升。

Method: 提出了UnifiedReward-Think，首个统一的多模态CoT奖励模型。采用探索驱动的强化微调方法来引导和激励模型的潜在复杂推理能力：(1) 使用少量图像生成偏好数据蒸馏GPT-4o的推理过程，用于模型冷启动学习CoT推理的格式和结构。(2) 利用模型的先验知识和泛化能力，准备大规模统一多模态偏好数据，引导模型在各种视觉任务中的推理过程，保留正确推理输出用于拒绝采样以优化模型。(3) 将预测错误的样本用于基于组相对策略优化（GRPO）的强化微调，使模型能够探索多样的推理路径并优化得到正确鲁棒的解决方案。

Result: 在各种视觉奖励任务上的大量实验证明了所提出模型的优越性。

Conclusion: 将长思维链（CoT）融入奖励推理过程，如UnifiedReward-Think模型所示，能显著增强多模态奖励模型的可靠性和鲁棒性，提升其在视觉理解和生成奖励任务中对齐人类偏好的能力。所提出的探索驱动强化微调方法能有效引导和优化模型的复杂推理能力。

Abstract: Recent advances in multimodal Reward Models (RMs) have shown significant
promise in delivering reward signals to align vision models with human
preferences. However, current RMs are generally restricted to providing direct
responses or engaging in shallow reasoning processes with limited depth, often
leading to inaccurate reward signals. We posit that incorporating explicit long
chains of thought (CoT) into the reward reasoning process can significantly
strengthen their reliability and robustness. Furthermore, we believe that once
RMs internalize CoT reasoning, their direct response accuracy can also be
improved through implicit reasoning capabilities. To this end, this paper
proposes UnifiedReward-Think, the first unified multimodal CoT-based reward
model, capable of multi-dimensional, step-by-step long-chain reasoning for both
visual understanding and generation reward tasks. Specifically, we adopt an
exploration-driven reinforcement fine-tuning approach to elicit and incentivize
the model's latent complex reasoning ability: (1) We first use a small amount
of image generation preference data to distill the reasoning process of GPT-4o,
which is then used for the model's cold start to learn the format and structure
of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge
and generalization capabilities, we prepare large-scale unified multimodal
preference data to elicit the model's reasoning process across various vision
tasks. During this phase, correct reasoning outputs are retained for rejection
sampling to refine the model (3) while incorrect predicted samples are finally
used for Group Relative Policy Optimization (GRPO) based reinforcement
fine-tuning, enabling the model to explore diverse reasoning paths and optimize
for correct and robust solutions. Extensive experiments across various vision
reward tasks demonstrate the superiority of our model.

</details>


### [70] [SD-VSum: A Method and Dataset for Script-Driven Video Summarization](https://arxiv.org/abs/2505.03319)
*Manolis Mylonas,Evlampios Apostolidis,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 本文提出脚本驱动的视频摘要任务，并开发了一个名为SD-VSum的新网络架构，通过跨模态注意力机制根据用户脚本生成定制化视频摘要。


<details>
  <summary>Details</summary>
Motivation: 现有视频摘要方法难以满足用户对摘要内容的特定、详细需求。因此，本研究旨在开发一种能够根据用户提供的脚本（详细描述期望摘要的视觉内容）来生成视频摘要的方法，以实现更个性化和内容更相关的摘要。

Method: 1. 提出了“脚本驱动视频摘要”这一新任务。
2. 扩展了VideoXum数据集：为数据集中每个视频的人工标注摘要生成了自然语言描述，使其适用于新任务的训练，形成了“视频-摘要-摘要描述”三元组。
3. 开发了一种新的网络架构SD-VSum，该架构利用跨模态注意力机制来对齐和融合视频的视觉信息与用户提供的脚本（文本信息）。

Result: 实验评估表明，SD-VSum在性能上优于现有的查询驱动摘要方法以及通用的（单模态和多模态）视频摘要方法。该模型能够根据用户脚本生成符合其特定内容需求的视频摘要。

Conclusion: 本研究提出的SD-VSum模型能够有效地根据用户提供的脚本生成定制化的视频摘要，证明了其适应用户个性化内容需求的潜力，为视频摘要领域提供了新的方向。

Abstract: In this work, we introduce the task of script-driven video summarization,
which aims to produce a summary of the full-length video by selecting the parts
that are most relevant to a user-provided script outlining the visual content
of the desired summary. Following, we extend a recently-introduced large-scale
dataset for generic video summarization (VideoXum) by producing natural
language descriptions of the different human-annotated summaries that are
available per video. In this way we make it compatible with the introduced
task, since the available triplets of ``video, summary and summary
description'' can be used for training a method that is able to produce
different summaries for a given video, driven by the provided script about the
content of each summary. Finally, we develop a new network architecture for
script-driven video summarization (SD-VSum), that relies on the use of a
cross-modal attention mechanism for aligning and fusing information from the
visual and text modalities. Our experimental evaluations demonstrate the
advanced performance of SD-VSum against state-of-the-art approaches for
query-driven and generic (unimodal and multimodal) summarization from the
literature, and document its capacity to produce video summaries that are
adapted to each user's needs about their content.

</details>


### [71] [Very High-Resolution Forest Mapping with TanDEM-X InSAR Data and Self-Supervised Learning](https://arxiv.org/abs/2505.03327)
*José-Luis Bueso-Bello,Benjamin Chauvel,Daniel Carcereri,Philipp Posovszky,Pietro Milillo,Jennifer Ruiz,Juan-Carlos Fernández-Diaz,Carolina González,Michele Martone,Ronny Hänsch,Paola Rizzoli*

Main category: cs.CV

TL;DR: 本研究提出一种自监督学习框架，利用TanDEM-X数据进行高分辨率森林制图，以解决高分辨率下参考数据缺乏的问题，并在有限标签下显著提高分类精度。


<details>
  <summary>Details</summary>
Motivation: 旨在利用TanDEM-X数据进行6米高分辨率森林制图，克服中分辨率产品在检测狭窄道路和精确描绘森林轮廓方面的局限性，并应对高分辨率下缺乏大量可靠标记数据的挑战。

Method: 首先采用自监督学习技术从输入特征中提取高信息量表示，然后使用显著减少的可靠标签进行监督训练。在美国宾夕法尼亚州使用1米分辨率参考图比较不同训练方法，筛选出最佳方法。

Result: 在亚马孙雨林的实际应用中（高分辨率标记数据极少），所提出的自监督框架与使用相同数量标记数据的全监督方法相比，显著提高了森林分类的准确性。

Conclusion: 该自监督学习框架为利用TanDEM-X数据进行大规模、极高分辨率森林制图提供了一个极具前景的起点，尤其适用于标记数据稀缺的情况。

Abstract: Deep learning models have shown encouraging capabilities for mapping
accurately forests at medium resolution with TanDEM-X interferometric SAR data.
Such models, as most of current state-of-the-art deep learning techniques in
remote sensing, are trained in a fully-supervised way, which requires a large
amount of labeled data for training and validation. In this work, our aim is to
exploit the high-resolution capabilities of the TanDEM-X mission to map forests
at 6 m. The goal is to overcome the intrinsic limitations posed by
midresolution products, which affect, e.g., the detection of narrow roads
within vegetated areas and the precise delineation of forested regions
contours. To cope with the lack of extended reliable reference datasets at such
a high resolution, we investigate self-supervised learning techniques for
extracting highly informative representations from the input features, followed
by a supervised training step with a significantly smaller number of reliable
labels. A 1 m resolution forest/non-forest reference map over Pennsylvania,
USA, allows for comparing different training approaches for the development of
an effective forest mapping framework with limited labeled samples. We select
the best-performing approach over this test region and apply it in a real-case
forest mapping scenario over the Amazon rainforest, where only very few labeled
data at high resolution are available. In this challenging scenario, the
proposed self-supervised framework significantly enhances the classification
accuracy with respect to fully-supervised methods, trained using the same
amount of labeled data, representing an extremely promising starting point for
large-scale, very high-resolution forest mapping with TanDEM-X data.

</details>


### [72] [FLUX-Text: A Simple and Advanced Diffusion Transformer Baseline for Scene Text Editing](https://arxiv.org/abs/2505.03329)
*Rui Lan,Yancheng Bai,Xu Duan,Mingxing Li,Lei Sun,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 提出了一种名为FLUX-Text的轻量级多语言场景文本编辑框架，它在少量训练数据下实现了卓越的文本保真度和编辑效果，尤其在处理复杂字形（如中文）方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的基于潜在扩散模型（LDM）的场景文本编辑方法在生成准确和可识别的字符方面存在挑战，尤其对于具有复杂字形结构的非拉丁字符（如中文）效果不佳。

Method: 提出了FLUX-Text框架，该框架基于FLUX-Fill。通过仔细研究字形条件（同时考虑视觉和文本模态），并引入轻量级的字形和文本嵌入模块，以增强对字形的理解和生成能力，同时保留FLUX-Fill原有的生成能力。

Result: FLUX-Text仅用10万训练样本便达到了最先进的文本编辑性能。在公开数据集上的定性和定量实验表明，该方法在文本保真度方面超越了以往的工作。

Conclusion: FLUX-Text是一个简单且先进的多语言场景文本编辑框架，通过轻量化设计和有效的字形处理，显著提升了文本编辑的质量和效率，尤其是在非拉丁字符的生成上取得了优越表现。

Abstract: The task of scene text editing is to modify or add texts on images while
maintaining the fidelity of newly generated text and visual coherence with the
background. Recent works based on latent diffusion models (LDM) show improved
text editing results, yet still face challenges and often generate inaccurate
or unrecognizable characters, especially for non-Latin ones (\eg, Chinese),
which have complex glyph structures. To address these issues, we present
FLUX-Text, a simple and advanced multilingual scene text editing framework
based on FLUX-Fill. Specifically, we carefully investigate glyph conditioning,
considering both visual and textual modalities. To retain the original
generative capabilities of FLUX-Fill while enhancing its understanding and
generation of glyphs, we propose lightweight glyph and text embedding modules.
Owning to the lightweight design, FLUX-Text is trained only with $100K$
training examples compared to current popular methods trained with 2.9M ones.
With no bells and whistles, our method achieves state-of-the-art performance on
text editing tasks. Qualitative and quantitative experiments on the public
datasets demonstrate that our method surpasses previous works in text fidelity.

</details>


### [73] [From Word to Sentence: A Large-Scale Multi-Instance Dataset for Open-Set Aerial Detection](https://arxiv.org/abs/2505.03334)
*Guoting Wei,Yu Liu,Xia Yuan,Xizhe Xue,Linlin Guo,Yifan Yang,Chunxia Zhao,Zongwen Bai,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TL;DR: 为解决航拍图像开放世界检测中语言引导粒度不足的问题，本文构建了一个大规模、多层级语言引导的航拍目标检测数据集MI-OAD，并提出了一种自动标注流程OS-W2S Label Engine，显著提升了模型的开放集检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有的语言引导航拍目标检测方法因数据集有限，主要关注词汇层面，无法满足更细粒度的开放世界检测需求。

Method: 1. 提出构建一个大规模语言引导的开放集航拍检测数据集MI-OAD，包含词、短语到句子三个层级的语言引导。 2. 提出一个名为OS-W2S Label Engine的自动标注流程，该流程围绕开源大型视觉语言模型，并集成了基于图像操作的预处理和基于BERT的后处理。 3. 使用该标注引擎扩展现有航拍数据集并构建MI-OAD。

Result: 构建了MI-OAD数据集，包含163,023张图像和200万个图像-标题对，规模约为同类数据集的40倍。在该数据集上训练的Grounding DINO模型，在零样本迁移条件下，句子输入的AP50提升了29.5，Recall@10提升了33.7。

Conclusion: 本文提出的MI-OAD数据集和OS-W2S Label Engine有效解决了当前遥感定位数据的局限性，支持更有效的开放集航拍目标检测，并显著提升了模型性能。数据集和标注引擎将公开发布。

Abstract: In recent years, language-guided open-world aerial object detection has
gained significant attention due to its better alignment with real-world
application needs. However, due to limited datasets, most existing
language-guided methods primarily focus on vocabulary, which fails to meet the
demands of more fine-grained open-world detection. To address this limitation,
we propose constructing a large-scale language-guided open-set aerial detection
dataset, encompassing three levels of language guidance: from words to phrases,
and ultimately to sentences. Centered around an open-source large
vision-language model and integrating image-operation-based preprocessing with
BERT-based postprocessing, we present the OS-W2S Label Engine, an automatic
annotation pipeline capable of handling diverse scene annotations for aerial
images. Using this label engine, we expand existing aerial detection datasets
with rich textual annotations and construct a novel benchmark dataset, called
Multi-instance Open-set Aerial Dataset (MI-OAD), addressing the limitations of
current remote sensing grounding data and enabling effective open-set aerial
detection. Specifically, MI-OAD contains 163,023 images and 2 million
image-caption pairs, approximately 40 times larger than comparable datasets. We
also employ state-of-the-art open-set methods from the natural image domain,
trained on our proposed dataset, to validate the model's open-set detection
capabilities. For instance, when trained on our dataset, Grounding DINO
achieves improvements of 29.5 AP_{50} and 33.7 Recall@10 for sentence inputs
under zero-shot transfer conditions. Both the dataset and the label engine will
be released publicly.

</details>


### [74] [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/abs/2505.03414)
*Fangming Cui,Yonggang Zhang,Xuan Wang,Xinmei Tian,Jun Yu*

Main category: cs.CV

TL;DR: 提出一种特征矩阵 (FM) 正则化方法，通过保留通用知识来提升大型视觉语言模型在非目标特定任务上的泛化能力，缓解过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 当前的提示学习方法在优化大型视觉语言模型时，虽然在目标特定任务上表现优异，但在处理非目标特定或需要泛化能力的任务时效果不佳，这可能是由于过拟合导致模型遗忘了对非目标特定任务有重要促进作用的通用知识。

Method: 提出一种新颖的特征矩阵 (FM) 正则化方法。该方法通过提取和利用通用知识来构建一个特征矩阵 (FM)，此矩阵从深层和精细的角度捕捉不同输入的语义，从而保留关键的通用知识，减轻过拟合风险。

Result: 代表性评估表明：1) FM 作为一个通用且灵活的模块，与现有框架兼容；2) FM 在增强非目标特定任务方面表现出显著效果，并达到了最先进的性能。

Conclusion: 所提出的特征矩阵 (FM) 正则化方法能够有效提升大型视觉语言模型在非目标特定任务上的性能，通过保留通用知识来解决过拟合问题，并展现出良好的兼容性和有效性。

Abstract: Recent developments in prompt learning of large vision-language models have
significantly improved performance in target-specific tasks. However, these
prompt optimizing methods often struggle to tackle the target-unspecific or
generalizable tasks effectively. It may be attributed to the fact that
overfitting training causes the model to forget its general knowledge having
strong promotion on target-unspecific tasks. To alleviate this issue, we
propose a novel Features Matrix (FM) regularization approach designed to
enhance these models on target-unspecific tasks. Our method extracts and
leverages general knowledge, shaping a Features Matrix (FM). Specifically, the
FM captures the semantics of diverse inputs from a deep and fine perspective,
preserving essential general knowledge, which mitigates the risk of
overfitting. Representative evaluations demonstrate that: 1) the FM is
compatible with existing frameworks as a generic and flexible module, and 2)
the FM significantly showcases its effectiveness in enhancing target-unspecific
tasks, achieving state-of-the-art performance.

</details>


### [75] [A Vision-Language Model for Focal Liver Lesion Classification](https://arxiv.org/abs/2505.03350)
*Song Jian,Hu Yuchang,Wang Hui,Chen Yen-Wei*

Main category: cs.CV

TL;DR: 提出了一种专为局灶性肝病变（FLLs）分类设计的视觉语言模型 Liver-VLM，在有限数据下表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统监督深度学习模型在医学影像中进行局灶性肝病变分类时，依赖大规模标注数据集，而这类数据集通常难以获取。视觉语言模型（VLMs）因其利用文本和图像进行多模态学习的能力，在数据有限时也能有效学习。

Method: 提出 Liver-VLM 模型：1) 将类别信息整合到文本编码器中，不增加额外推理开销；2) 通过计算图像和文本嵌入之间的成对余弦相似度，并使用交叉熵损失进行优化，从而有效地将图像特征与类别级文本特征对齐。

Result: 在 MPCT-FLLs 数据集上的实验结果显示，Liver-VLM 模型在准确率和曲线下面积（AUC）方面均优于标准的 CLIP 和 MedCLIP 模型。进一步分析表明，使用轻量级的 ResNet18 作为骨干网络，尤其在数据受限条件下，能够提升分类性能。

Conclusion: Liver-VLM 模型能够有效利用文本和图像信息进行局灶性肝病变分类，特别是在标注数据有限的情况下展现出优越的性能和潜力。

Abstract: Accurate classification of focal liver lesions is crucial for diagnosis and
treatment in hepatology. However, traditional supervised deep learning models
depend on large-scale annotated datasets, which are often limited in medical
imaging. Recently, Vision-Language models (VLMs) such as Contrastive
Language-Image Pre-training model (CLIP) has been applied to image
classifications. Compared to the conventional convolutional neural network
(CNN), which classifiers image based on visual information only, VLM leverages
multimodal learning with text and images, allowing it to learn effectively even
with a limited amount of labeled data. Inspired by CLIP, we pro-pose a
Liver-VLM, a model specifically designed for focal liver lesions (FLLs)
classification. First, Liver-VLM incorporates class information into the text
encoder without introducing additional inference overhead. Second, by
calculating the pairwise cosine similarities between image and text embeddings
and optimizing the model with a cross-entropy loss, Liver-VLM ef-fectively
aligns image features with class-level text features. Experimental results on
MPCT-FLLs dataset demonstrate that the Liver-VLM model out-performs both the
standard CLIP and MedCLIP models in terms of accuracy and Area Under the Curve
(AUC). Further analysis shows that using a lightweight ResNet18 backbone
enhances classification performance, particularly under data-constrained
conditions.

</details>


### [76] [GUAVA: Generalizable Upper Body 3D Gaussian Avatar](https://arxiv.org/abs/2505.03351)
*Dongbin Zhang,Yunfei Liu,Lijian Lin,Ye Zhu,Yang Li,Minghan Qin,Yu Li,Haoqian Wang*

Main category: cs.CV

TL;DR: 提出GUAVA框架，可从单张图片快速重建富有表情、可动画的上半身3D高斯化身，效果优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体化身重建方法通常需要多视角或单目视频及个体化训练，过程复杂耗时，且受限于SMPLX模型的表达能力，难以处理面部表情。

Method: 首先引入富有表现力的人体模型(EHM)以增强面部表情能力，并基于此模型提出GUAVA框架。该框架利用反向纹理映射和投影采样技术从单张图像推断上半身3D高斯表示，并通过神经精炼器优化渲染图像。

Result: GUAVA在渲染质量上显著优于先前方法，重建时间达到亚秒级(0.1秒)，并支持实时动画和渲染。

Conclusion: GUAVA为从单张图像快速重建高质量、可动画且富有表情的上半身3D化身提供了一个有效解决方案，显著提升了效率和表现力。

Abstract: Reconstructing a high-quality, animatable 3D human avatar with expressive
facial and hand motions from a single image has gained significant attention
due to its broad application potential. 3D human avatar reconstruction
typically requires multi-view or monocular videos and training on individual
IDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's
expressiveness, these methods often focus on body motion but struggle with
facial expressions. To address these challenges, we first introduce an
expressive human model (EHM) to enhance facial expression capabilities and
develop an accurate tracking method. Based on this template model, we propose
GUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar
reconstruction. We leverage inverse texture mapping and projection sampling
techniques to infer Ubody (upper-body) Gaussians from a single image. The
rendered images are refined through a neural refiner. Experimental results
demonstrate that GUAVA significantly outperforms previous methods in rendering
quality and offers significant speed improvements, with reconstruction times in
the sub-second range (0.1s), and supports real-time animation and rendering.

</details>


### [77] [Interpretable Zero-shot Learning with Infinite Class Concepts](https://arxiv.org/abs/2505.03361)
*Zihan Ye,Shreyank N Gowda,Shiming Chen,Yaochu Jin,Kaizhu Huang,Xiaobo Jin*

Main category: cs.CV

TL;DR: 本文提出InfZSL框架，利用LLM生成无限的短语级类别概念，并通过熵评分和“优良性”选择机制解决幻觉问题，提升了零样本学习的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有利用大型语言模型（LLM）进行零样本学习（ZSL）的方法面临分类过程透明度不足以及LLM幻觉导致产生非视觉类别语义的挑战。

Method: 提出InfZSL框架：1. 利用LLM动态生成无限数量的短语级类别概念。2. 引入基于熵的评分过程和“优良性”概念选择机制，以筛选最具可迁移性和区分度的概念，从而解决LLM的幻觉问题。

Result: InfZSL框架在三个流行的基准数据集上取得了显著的改进，并且能够生成高度可解释的、与图像内容锚定的概念。

Conclusion: InfZSL通过动态生成并筛选高质量的类别概念，不仅提升了零样本学习的性能，还增强了结果的可解释性，并有效缓解了LLM的幻觉问题。

Abstract: Zero-shot learning (ZSL) aims to recognize unseen classes by aligning images
with intermediate class semantics, like human-annotated concepts or class
definitions. An emerging alternative leverages Large-scale Language Models
(LLMs) to automatically generate class documents. However, these methods often
face challenges with transparency in the classification process and may suffer
from the notorious hallucination problem in LLMs, resulting in non-visual class
semantics. This paper redefines class semantics in ZSL with a focus on
transferability and discriminability, introducing a novel framework called
Zero-shot Learning with Infinite Class Concepts (InfZSL). Our approach
leverages the powerful capabilities of LLMs to dynamically generate an
unlimited array of phrase-level class concepts. To address the hallucination
challenge, we introduce an entropy-based scoring process that incorporates a
``goodness" concept selection mechanism, ensuring that only the most
transferable and discriminative concepts are selected. Our InfZSL framework not
only demonstrates significant improvements on three popular benchmark datasets
but also generates highly interpretable, image-grounded concepts. Code will be
released upon acceptance.

</details>


### [78] [3D Surface Reconstruction with Enhanced High-Frequency Details](https://arxiv.org/abs/2505.03362)
*Shikun Zhang,Yiqun Wang,Cunjian Chen,Yong Li,Qiuhong Ke*

Main category: cs.CV

TL;DR: FreNeuS是一种利用图像高频信息指导动态采样和加权的神经隐式3D重建方法，解决了现有方法重建表面细节不足、结果过于平滑的问题，实现了更精细的表面重建。


<details>
  <summary>Details</summary>
Motivation: 当前的神经表面重建方法因倾向于随机采样整个图像，难以学习表面的高频细节，导致重建结果过于平滑，表面细节不足。

Method: 提出了FreNeuS方法：1. 利用像素梯度变化获取图像中的高频区域。2. 使用高频信息指导光线的动态采样，根据高频区域的变化应用不同采样策略。3. 设计一种高频加权方法，在重建过程中约束高频细节的表示。

Result: 定性和定量实验结果表明，该方法与现有方法相比，能够重建出更精细的表面细节，并获得更好的表面重建质量。

Conclusion: FreNeuS方法能有效重建精细的表面细节，提升表面重建质量，并且具有良好的适用性，可以推广到任何基于NeuS的工作中。

Abstract: Neural implicit 3D reconstruction can reproduce shapes without 3D
supervision, and it learns the 3D scene through volume rendering methods and
neural implicit representations. Current neural surface reconstruction methods
tend to randomly sample the entire image, making it difficult to learn
high-frequency details on the surface, and thus the reconstruction results tend
to be too smooth. We designed a method (FreNeuS) based on high-frequency
information to solve the problem of insufficient surface detail. Specifically,
FreNeuS uses pixel gradient changes to easily acquire high-frequency regions in
an image and uses the obtained high-frequency information to guide surface
detail reconstruction. High-frequency information is first used to guide the
dynamic sampling of rays, applying different sampling strategies according to
variations in high-frequency regions. To further enhance the focus on surface
details, we have designed a high-frequency weighting method that constrains the
representation of high-frequency details during the reconstruction process.
Qualitative and quantitative results show that our method can reconstruct fine
surface details and obtain better surface reconstruction quality compared to
existing methods. In addition, our method is more applicable and can be
generalized to any NeuS-based work.

</details>


### [79] [Reducing Annotation Burden in Physical Activity Research Using Vision-Language Models](https://arxiv.org/abs/2505.03374)
*Abram Schonfeldt,Benjamin Maylor,Xiaofang Chen,Ronald Clark,Aiden Doherty*

Main category: cs.CV

TL;DR: 该研究评估了计算机视觉模型从可穿戴相机图像中自动标注身体活动的能力，发现在相似人群中对久坐行为的识别效果较好，但对其他活动强度和跨人群应用时性能下降。


<details>
  <summary>Details</summary>
Motivation: 为健康研究验证和开发可穿戴设备测量方法及机器学习模型，需要大量标注的自由生活数据，而目前通过人工标注可穿戴相机图像获取标签的方法非常耗时。

Method: 比较了三种视觉语言模型（VLM）和两种判别模型（DM）在两个自由生活研究（英国牛津郡161名参与者，中国四川111名参与者）中，使用Autographer可穿戴相机图像预测身体活动行为的性能。

Result: 在牛津郡研究中，最佳开源VLM和微调的DM在预测久坐行为方面表现相当（中位F1分数：VLM=0.89, DM=0.91），但对轻度活动（VLM=0.60, DM=0.70）和中高强度体力活动（VLM=0.66, DM=0.72）的预测性能下降。当模型应用于外部的四川研究时，所有强度类别的性能均显著下降（Cohen's kappa中位数：VLM从0.54降至0.26，DM从0.67降至0.19）。

Conclusion: 免费可用的计算机视觉模型有助于从可穿戴相机图像中标注与训练数据相似人群的久坐行为（日常生活中最普遍的活动），从而减轻标注负担。

Abstract: Introduction: Data from wearable devices collected in free-living settings,
and labelled with physical activity behaviours compatible with health research,
are essential for both validating existing wearable-based measurement
approaches and developing novel machine learning approaches. One common way of
obtaining these labels relies on laborious annotation of sequences of images
captured by cameras worn by participants through the course of a day. Methods:
We compare the performance of three vision language models and two
discriminative models on two free-living validation studies with 161 and 111
participants, collected in Oxfordshire, United Kingdom and Sichuan, China,
respectively, using the Autographer (OMG Life, defunct) wearable camera.
Results: We found that the best open-source vision-language model (VLM) and
fine-tuned discriminative model (DM) achieved comparable performance when
predicting sedentary behaviour from single images on unseen participants in the
Oxfordshire study; median F1-scores: VLM = 0.89 (0.84, 0.92), DM = 0.91 (0.86,
0.95). Performance declined for light (VLM = 0.60 (0.56,0.67), DM = 0.70 (0.63,
0.79)), and moderate-to-vigorous intensity physical activity (VLM = 0.66 (0.53,
0.85); DM = 0.72 (0.58, 0.84)). When applied to the external Sichuan study,
performance fell across all intensity categories, with median Cohen's
kappa-scores falling from 0.54 (0.49, 0.64) to 0.26 (0.15, 0.37) for the VLM,
and from 0.67 (0.60, 0.74) to 0.19 (0.10, 0.30) for the DM. Conclusion: Freely
available computer vision models could help annotate sedentary behaviour,
typically the most prevalent activity of daily living, from wearable camera
images within similar populations to seen data, reducing the annotation burden.

</details>


### [80] [Reinforced Correlation Between Vision and Language for Precise Medical AI Assistant](https://arxiv.org/abs/2505.03380)
*Haonan Wang,Jiaji Mao,Lehan Wang,Qixiang Zhang,Marawan Elbatel,Yi Qin,Huijun Hu,Baoxun Li,Wenhui Deng,Weifeng Qin,Hongrui Li,Jialin Liang,Jun Shen,Xiaomeng Li*

Main category: cs.CV

TL;DR: 提出了一种名为RCMed的全栈AI医疗助手，通过改进多模态对齐和分层视觉-语言基础，实现精确的解剖学描述、定位和诊断。


<details>
  <summary>Details</summary>
Motivation: 现有的医疗AI助手在处理多模态内容时准确性有限，并且在真实临床环境中的验证不足。

Method: RCMed采用一种自增强相关机制，使视觉特征和语言上下文相互促进，形成闭环精炼两种模态。同时，通过颜色区域描述策略，将解剖结构转化为语义丰富的文本，学习跨尺度的形状-位置-文本关系，以增强多模态对齐。

Result: RCMed在2000万图像-掩码-描述三元组上训练后，在165项临床任务和9种模态中表现出色，尤其在不规则病变和细微解剖边界的情境化方面达到了顶尖水平。在显微镜图像的细胞分割任务中，其性能相比先前方法提升了23.5%，并在20种临床重要癌症类型的外部验证中展现了卓越的泛化能力。

Conclusion: 该研究表明，集成的多模态模型能够捕捉细粒度模式，在复杂场景下实现接近人类水平的解读，从而推动以人为中心的AI医疗保健发展。

Abstract: Medical AI assistants support doctors in disease diagnosis, medical image
analysis, and report generation. However, they still face significant
challenges in clinical use, including limited accuracy with multimodal content
and insufficient validation in real-world settings. We propose RCMed, a
full-stack AI assistant that improves multimodal alignment in both input and
output, enabling precise anatomical delineation, accurate localization, and
reliable diagnosis through hierarchical vision-language grounding. A
self-reinforcing correlation mechanism allows visual features to inform
language context, while language semantics guide pixel-wise attention, forming
a closed loop that refines both modalities. This correlation is enhanced by a
color region description strategy, translating anatomical structures into
semantically rich text to learn shape-location-text relationships across
scales. Trained on 20 million image-mask-description triplets, RCMed achieves
state-of-the-art precision in contextualizing irregular lesions and subtle
anatomical boundaries, excelling in 165 clinical tasks across 9 modalities. It
achieved a 23.5% relative improvement in cell segmentation from microscopy
images over prior methods. RCMed's strong vision-language alignment enables
exceptional generalization, with state-of-the-art performance in external
validation across 20 clinically significant cancer types, including novel
tasks. This work demonstrates how integrated multimodal models capture
fine-grained patterns, enabling human-level interpretation in complex scenarios
and advancing human-centric AI healthcare.

</details>


### [81] [Attention-aggregated Attack for Boosting the Transferability of Facial Adversarial Examples](https://arxiv.org/abs/2505.03383)
*Jian-Wei Li,Wen-Ze Shao*

Main category: cs.CV

TL;DR: 提出了一种针对人脸识别模型的注意力聚合攻击（AAA）方法，通过模仿其他模型的注意力机制来破坏关键面部特征，从而提升黑盒攻击的迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于迁移的黑盒对抗攻击方法在人脸识别等细粒度视觉任务上表现不佳，因为它们很少考虑类别特定深度模型（如人脸识别模型）学习面部特征的独特性。

Method: 首先研究发现人脸识别模型学习到的决定性及辅助性面部特征均具有模型特异性。基于此，提出了一种名为注意力聚合攻击（AAA）的新方法，该方法受注意力差异的启发，通过模仿其他FR模型对干净人脸图像的注意力，旨在破坏对这些模型决策至关重要的面部特征。

Result: 在多种人脸识别模型上进行的大量实验验证了所提方法相较于现有方法的优越性和鲁棒有效性。

Conclusion: AAA方法通过模仿并破坏目标FR模型关注的关键面部特征，有效地增强了针对人脸识别模型的对抗样本的迁移能力。

Abstract: Adversarial examples have revealed the vulnerability of deep learning models
and raised serious concerns about information security. The transfer-based
attack is a hot topic in black-box attacks that are practical to real-world
scenarios where the training datasets, parameters, and structure of the target
model are unknown to the attacker. However, few methods consider the
particularity of class-specific deep models for fine-grained vision tasks, such
as face recognition (FR), giving rise to unsatisfactory attacking performance.
In this work, we first investigate what in a face exactly contributes to the
embedding learning of FR models and find that both decisive and auxiliary
facial features are specific to each FR model, which is quite different from
the biological mechanism of human visual system. Accordingly we then propose a
novel attack method named Attention-aggregated Attack (AAA) to enhance the
transferability of adversarial examples against FR, which is inspired by the
attention divergence and aims to destroy the facial features that are critical
for the decision-making of other FR models by imitating their attentions on the
clean face images. Extensive experiments conducted on various FR models
validate the superiority and robust effectiveness of the proposed method over
existing methods.

</details>


### [82] [EOPose : Exemplar-based object reposing using Generalized Pose Correspondences](https://arxiv.org/abs/2505.03394)
*Sarthak Mehrotra,Rishabh Jain,Mayur Hemani,Balaji Krishnamurthy,Mausoom Sarkar*

Main category: cs.CV

TL;DR: 提出了一种名为 EOPose 的端到端框架，利用无监督关键点对应实现通用物体姿态重构，能保留物体细节。


<details>
  <summary>Details</summary>
Motivation: 电子商务等领域需要快速生成多种姿态的产品图片，现有方法可能无法很好地保留物体细节或需要大量监督。

Method: 提出 EOPose 方法：利用源图像与目标姿态引导图像之间的无监督关键点对应，通过一个新颖的三步法（扭曲和重渲染）将源对象重构成目标姿态。同时，基于 Objaverse 数据集构建了一个新的配对物体数据集用于训练和测试。

Result: EOPose 能够生成高质量的重姿态图像（通过 PSNR, SSIM, FID 指标衡量），并且能够保留物体的精确颜色、纹理和品牌标记等精细细节。详细的消融实验和用户研究也证明了该方法的有效性。

Conclusion: EOPose 是一种有效的通用物体姿态重构方法，能够生成高质量且保留细节的图像，在电子商务等需要快速生成多姿态图像的场景具有应用潜力。

Abstract: Reposing objects in images has a myriad of applications, especially for
e-commerce where several variants of product images need to be produced
quickly. In this work, we leverage the recent advances in unsupervised keypoint
correspondence detection between different object images of the same class to
propose an end-to-end framework for generic object reposing. Our method,
EOPose, takes a target pose-guidance image as input and uses its keypoint
correspondence with the source object image to warp and re-render the latter
into the target pose using a novel three-step approach. Unlike generative
approaches, our method also preserves the fine-grained details of the object
such as its exact colors, textures, and brand marks. We also prepare a new
dataset of paired objects based on the Objaverse dataset to train and test our
network. EOPose produces high-quality reposing output as evidenced by different
image quality metrics (PSNR, SSIM and FID). Besides a description of the method
and the dataset, the paper also includes detailed ablation and user studies to
indicate the efficacy of the proposed method

</details>


### [83] [DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation](https://arxiv.org/abs/2505.03401)
*Shanshan Song,Hui Tang,Honglong Yang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为DDaTR的新型网络，旨在通过动态捕捉空间、时间相关性以及检查间的差异信息，来改进纵向放射学报告生成（LRRG）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的纵向放射学报告生成（LRRG）方法在特征提取过程中难以有效捕捉空间和时间相关性，导致提取的特征不足以表示检查间的差异信息和预期的临床进展，从而限制了LRRG的性能。

Method: 开发了一种新颖的动态差异感知时间残差网络（DDaTR）。该网络引入了动态特征对齐模块（DFAM）以对齐先验特征，以及动态差异感知模块（DDAM）以捕捉检查间的差异信息。此外，DDaTR采用动态残差网络单向传递纵向信息，有效建模时间相关性。

Result: 在三个基准数据集上进行的大量实验表明，所提出的DDaTR方法在放射学报告生成（RRG）和纵向放射学报告生成（LRRG）任务上的性能均优于现有方法。

Conclusion: DDaTR网络能够有效捕捉多层次空间相关性、时间相关性以及检查间的差异信息，从而显著提升了纵向放射学报告生成的性能，证明了其在RRG和LRRG任务中的有效性。

Abstract: Radiology Report Generation (RRG) automates the creation of radiology reports
from medical imaging, enhancing the efficiency of the reporting process.
Longitudinal Radiology Report Generation (LRRG) extends RRG by incorporating
the ability to compare current and prior exams, facilitating the tracking of
temporal changes in clinical findings. Existing LRRG approaches only extract
features from prior and current images using a visual pre-trained encoder,
which are then concatenated to generate the final report. However, these
methods struggle to effectively capture both spatial and temporal correlations
during the feature extraction process. Consequently, the extracted features
inadequately capture the information of difference across exams and thus
underrepresent the expected progressions, leading to sub-optimal performance in
LRRG. To address this, we develop a novel dynamic difference-aware temporal
residual network (DDaTR). In DDaTR, we introduce two modules at each stage of
the visual encoder to capture multi-level spatial correlations. The Dynamic
Feature Alignment Module (DFAM) is designed to align prior features across
modalities for the integrity of prior clinical information. Prompted by the
enriched prior features, the dynamic difference-aware module (DDAM) captures
favorable difference information by identifying relationships across exams.
Furthermore, our DDaTR employs the dynamic residual network to unidirectionally
transmit longitudinal information, effectively modelling temporal correlations.
Extensive experiments demonstrated superior performance over existing methods
on three benchmarks, proving its efficacy in both RRG and LRRG tasks.

</details>


### [84] [CXR-AD: Component X-ray Image Dataset for Industrial Anomaly Detection](https://arxiv.org/abs/2505.03412)
*Haoyu Bai,Jie Wang,Gaomin Li,Xuan Li,Xiaohu Zhang,Xia Yang*

Main category: cs.CV

TL;DR: 本文构建了首个公开的工业部件X射线内部缺陷检测数据集CXR-AD，并评估了现有异常检测算法在该数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的异常检测数据集主要关注可见光图像中的表面缺陷，缺乏针对部件内部缺陷的公开X射线数据集。

Method: 构建了一个包含真实X射线图像的CXR-AD数据集，涵盖5类工业部件，包含正常样本和带像素级掩码标注的缺陷样本。分析了数据集特性，并使用三种主流异常检测框架（基于特征、基于重构和零样本学习）进行了基准测试。

Result: CXR-AD数据集呈现三大挑战：复杂内部结构与缺陷区域强耦合、X射线成像固有的低对比度和高噪声、缺陷尺度和形态差异大。与MVTec AD相比，现有算法在CXR-AD上的平均性能下降了29.78%。

Conclusion: CXR-AD是首个公开的部件X射线异常检测数据集，为推动内部缺陷检测算法发展和提高检测精度提供了一个真实的工业基准。

Abstract: Internal defect detection constitutes a critical process in ensuring
component quality, for which anomaly detection serves as an effective solution.
However, existing anomaly detection datasets predominantly focus on surface
defects in visible-light images, lacking publicly available X-ray datasets
targeting internal defects in components. To address this gap, we construct the
first publicly accessible component X-ray anomaly detection (CXR-AD) dataset,
comprising real-world X-ray images. The dataset covers five industrial
component categories, including 653 normal samples and 561 defect samples with
precise pixel-level mask annotations. We systematically analyze the dataset
characteristics and identify three major technical challenges: (1) strong
coupling between complex internal structures and defect regions, (2) inherent
low contrast and high noise interference in X-ray imaging, and (3) significant
variations in defect scales and morphologies. To evaluate dataset complexity,
we benchmark three state-of-the-art anomaly detection frameworks
(feature-based, reconstruction-based, and zero-shot learning methods).
Experimental results demonstrate a 29.78% average performance degradation on
CXR-AD compared to MVTec AD, highlighting the limitations of current algorithms
in handling internal defect detection tasks. To the best of our knowledge,
CXR-AD represents the first publicly available X-ray dataset for component
anomaly detection, providing a real-world industrial benchmark to advance
algorithm development and enhance precision in internal defect inspection
technologies.

</details>


### [85] [LiftFeat: 3D Geometry-Aware Local Feature Matching](https://arxiv.org/abs/2505.03422)
*Yepeng Liu,Wenpeng Lai,Zhou Zhao,Yuxuan Xiong,Jinchi Zhu,Jun Cheng,Yongchao Xu*

Main category: cs.CV

TL;DR: 提出了一种名为LiftFeat的轻量级网络，通过聚合3D几何特征（表面法线）来增强2D描述子的鲁棒性，以改善在光照剧变、低纹理等挑战性场景下的局部特征匹配性能。


<details>
  <summary>Details</summary>
Motivation: 尽管局部特征匹配技术已取得很大进展，但在剧烈光照变化、低纹理区域或重复模式等挑战性场景中，提取鲁棒且有区分度的视觉特征仍然非常困难，这限制了其在SLAM和机器人视觉定位等应用中的效果。

Method: 首先，采用预训练的单目深度估计模型生成伪表面法线标签，用以监督3D几何特征（预测的表面法线）的提取。然后，设计了一个3D几何感知特征提升模块，将提取的表面法线特征与原始的2D描述子特征进行融合。

Result: 在相对姿态估计、单应性估计和视觉定位任务上的大量实验结果表明，所提出的LiftFeat方法性能优于一些轻量级的SOTA（state-of-the-art）方法。

Conclusion: 通过整合3D几何特征（特别是表面法线信息），LiftFeat能够有效增强2D特征描述在极端条件下的区分能力和鲁棒性，为解决挑战性场景下的特征匹配问题提供了一个有效的轻量级方案。

Abstract: Robust and efficient local feature matching plays a crucial role in
applications such as SLAM and visual localization for robotics. Despite great
progress, it is still very challenging to extract robust and discriminative
visual features in scenarios with drastic lighting changes, low texture areas,
or repetitive patterns. In this paper, we propose a new lightweight network
called \textit{LiftFeat}, which lifts the robustness of raw descriptor by
aggregating 3D geometric feature. Specifically, we first adopt a pre-trained
monocular depth estimation model to generate pseudo surface normal label,
supervising the extraction of 3D geometric feature in terms of predicted
surface normal. We then design a 3D geometry-aware feature lifting module to
fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D
geometric feature enhances the discriminative ability of 2D feature description
in extreme conditions. Extensive experimental results on relative pose
estimation, homography estimation, and visual localization tasks, demonstrate
that our LiftFeat outperforms some lightweight state-of-the-art methods. Code
will be released at : https://github.com/lyp-deeplearning/LiftFeat.

</details>


### [86] [Phenotype-Guided Generative Model for High-Fidelity Cardiac MRI Synthesis: Advancing Pretraining and Clinical Applications](https://arxiv.org/abs/2505.03426)
*Ziyu Li,Yujian Hu,Zhengyao Ding,Yiheng Mao,Haitao Li,Fan Yi,Hongkun Zhang,Zhengxing Huang*

Main category: cs.CV

TL;DR: 提出一种名为CPGG的新方法，通过心脏表型指导生成多样化、高质量的心脏磁共振（CMR）数据，以改善AI模型在心脏疾病诊断等下游任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 大规模、高质量的CMR数据集的缺乏，特别是覆盖广泛健康状况的数据不足，限制了AI模型预训练效果及在下游任务中的表现。

Method: CPGG框架包含两个阶段：第一阶段，使用从CMR数据中提取的心脏表型训练一个生成模型；第二阶段，一个基于这些表型的掩码自回归扩散模型生成高保真度的CMR电影序列，捕捉心脏的结构和功能特征。

Result: 实验表明，CPGG能生成高质量的合成CMR数据，通过扩展预训练数据，显著提高了在诊断和心脏表型预测等多种下游任务上的性能，效果在公共和私有数据集上均得到验证。

Conclusion: CPGG方法能够有效生成多样化的高质量合成CMR数据，解决了数据稀缺问题，从而显著提升了AI在心脏健康评估和疾病诊断应用中的性能。

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a vital non-invasive tool for
diagnosing heart diseases and evaluating cardiac health. However, the limited
availability of large-scale, high-quality CMR datasets poses a major challenge
to the effective application of artificial intelligence (AI) in this domain.
Even the amount of unlabeled data and the health status it covers are difficult
to meet the needs of model pretraining, which hinders the performance of AI
models on downstream tasks. In this study, we present Cardiac Phenotype-Guided
CMR Generation (CPGG), a novel approach for generating diverse CMR data that
covers a wide spectrum of cardiac health status. The CPGG framework consists of
two stages: in the first stage, a generative model is trained using cardiac
phenotypes derived from CMR data; in the second stage, a masked autoregressive
diffusion model, conditioned on these phenotypes, generates high-fidelity CMR
cine sequences that capture both structural and functional features of the
heart in a fine-grained manner. We synthesized a massive amount of CMR to
expand the pretraining data. Experimental results show that CPGG generates
high-quality synthetic CMR data, significantly improving performance on various
downstream tasks, including diagnosis and cardiac phenotypes prediction. These
gains are demonstrated across both public and private datasets, highlighting
the effectiveness of our approach. Code is availabel at
https://anonymous.4open.science/r/CPGG.

</details>


### [87] [A Fusion-Guided Inception Network for Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2505.03431)
*Usman Muhammad,Jorma Laaksonen*

Main category: cs.CV

TL;DR: 提出了一种名为FGIN的单图像高光谱超分辨率模型，旨在解决传统融合方法中图像对齐困难的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的高光谱图像超分辨率融合方法依赖于低空间分辨率高光谱图像与高空间分辨率常规图像之间的精确对齐，这在实际应用中难以实现。

Method: 提出了融合引导的Inception网络（FGIN）：首先使用光谱-空间融合模块早期整合光谱和空间信息；然后采用类Inception的层级特征提取策略捕获多尺度空间依赖，并辅以专门的多尺度融合块；最后结合双线性插值和深度可分离卷积的优化上采样模块提升重建质量。

Result: 在两个公开的高光谱数据集上的实验评估表明，该方法表现出有竞争力的性能。

Conclusion: FGIN模型能够有效解决单图像高光谱超分辨率问题，尤其是在图像对难以精确对齐的场景下，具有较好的应用前景。

Abstract: The fusion of low-spatial-resolution hyperspectral images (HSIs) with
high-spatial-resolution conventional images (e.g., panchromatic or RGB) has
played a significant role in recent advancements in HSI super-resolution.
However, this fusion process relies on the availability of precise alignment
between image pairs, which is often challenging in real-world scenarios. To
mitigate this limitation, we propose a single-image super-resolution model
called the Fusion-Guided Inception Network (FGIN). Specifically, we first
employ a spectral-spatial fusion module to effectively integrate spectral and
spatial information at an early stage. Next, an Inception-like hierarchical
feature extraction strategy is used to capture multiscale spatial dependencies,
followed by a dedicated multi-scale fusion block. To further enhance
reconstruction quality, we incorporate an optimized upsampling module that
combines bilinear interpolation with depthwise separable convolutions.
Experimental evaluations on two publicly available hyperspectral datasets
demonstrate the competitive performance of our method.

</details>


### [88] [Robustness in AI-Generated Detection: Enhancing Resistance to Adversarial Attacks](https://arxiv.org/abs/2505.03435)
*Sun Haoxuan,Hong Yan,Zhan Jiahui,Chen Haoxing,Lan Jun,Zhu Huijia,Wang Weiqiang,Zhang Liqing,Zhang Jianfu*

Main category: cs.CV

TL;DR: 论文研究了AI生成人脸检测系统在对抗攻击下的脆弱性，并提出了一种结合对抗训练、扩散反演与重构的方法来提升其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生成图像技术的进步带来了安全隐患，现有AI生成人脸检测系统虽然在标准条件下表现良好，但面对对抗攻击时鲁棒性不足。

Method: 整合对抗训练以减轻对抗样本的影响，并利用扩散反演和重构技术进一步增强检测模型的鲁棒性。

Result: 实验表明，微小的对抗性扰动能轻易绕过现有检测系统，而本研究提出的方法能显著提高这些系统的鲁棒性。同时，对对抗样本和良性样本进行了深入分析。

Conclusion: 通过结合对抗训练、扩散反演和重构，可以有效提高AI生成人脸检测系统在对抗攻击下的鲁棒性，并为理解AI生成内容的内在特性提供了见解。

Abstract: The rapid advancement of generative image technology has introduced
significant security concerns, particularly in the domain of face generation
detection. This paper investigates the vulnerabilities of current AI-generated
face detection systems. Our study reveals that while existing detection methods
often achieve high accuracy under standard conditions, they exhibit limited
robustness against adversarial attacks. To address these challenges, we propose
an approach that integrates adversarial training to mitigate the impact of
adversarial examples. Furthermore, we utilize diffusion inversion and
reconstruction to further enhance detection robustness. Experimental results
demonstrate that minor adversarial perturbations can easily bypass existing
detection systems, but our method significantly improves the robustness of
these systems. Additionally, we provide an in-depth analysis of adversarial and
benign examples, offering insights into the intrinsic characteristics of
AI-generated content. All associated code will be made publicly available in a
dedicated repository to facilitate further research and verification.

</details>


### [89] [Polar Coordinate-Based 2D Pose Prior with Neural Distance Field](https://arxiv.org/abs/2505.03445)
*Qi Gan,Sao Mai Nguyen,Eric Fenaux,Stephan Clémençon,Mounîm El Yacoubi*

Main category: cs.CV

TL;DR: 提出了一种基于神经距离场（NDF）和极坐标表示的2D姿态先验引导的细化方法，用于改善体育运动中人体姿态估计的准确性和鲁棒性，尤其是在数据稀缺和存在运动模糊、遮挡等挑战的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的人体姿态估计（HPE）模型在真实的体育运动场景中，由于运动模糊、遮挡和不同姿态表示间的领域迁移问题而表现不佳。此外，微调这些模型通常需要大规模标注数据，并且难以泛化到多样的运动环境中。

Method: 提出了一种基于神经距离场（NDF）的2D姿态先验引导的细化方法。该方法引入了基于极坐标的姿态表示（明确包含关节连接长度），定义了一种新的非测地线距离度量（分离角度和径向差异），并开发了一种基于梯度的批量投影增强策略来合成姿态样本以缓解数据稀缺问题。

Result: 在跳远数据集上的评估表明，该方法能够改进多种姿态表示下的2D姿态估计，并在不同领域具有鲁棒性。实验结果显示，该方法增强了姿态的合理性，并且仅需有限的训练数据。

Conclusion: 该研究提出的基于极坐标神经距离场的方法能够有效提升体育场景下2D人体姿态估计的准确性和鲁棒性，特别是在处理运动模糊、遮挡和数据稀缺等问题时显示出其优势，并且只需要少量训练数据即可提高姿态的合理性。

Abstract: Human pose capture is essential for sports analysis, enabling precise
evaluation of athletes' movements. While deep learning-based human pose
estimation (HPE) models from RGB videos have achieved impressive performance on
public datasets, their effectiveness in real-world sports scenarios is often
hindered by motion blur, occlusions, and domain shifts across different pose
representations. Fine-tuning these models can partially alleviate such
challenges but typically requires large-scale annotated data and still
struggles to generalize across diverse sports environments. To address these
limitations, we propose a 2D pose prior-guided refinement approach based on
Neural Distance Fields (NDF). Unlike existing approaches that rely solely on
angular representations of human poses, we introduce a polar coordinate-based
representation that explicitly incorporates joint connection lengths, enabling
a more accurate correction of erroneous pose estimations. Additionally, we
define a novel non-geodesic distance metric that separates angular and radial
discrepancies, which we demonstrate is better suited for polar representations
than traditional geodesic distances. To mitigate data scarcity, we develop a
gradient-based batch-projection augmentation strategy, which synthesizes
realistic pose samples through iterative refinement. Our method is evaluated on
a long jump dataset, demonstrating its ability to improve 2D pose estimation
across multiple pose representations, making it robust across different
domains. Experimental results show that our approach enhances pose plausibility
while requiring only limited training data. Code is available at:
https://github.com/QGAN2019/polar-NDF.

</details>


### [90] [Nonperiodic dynamic CT reconstruction using backward-warping INR with regularization of diffeomorphism (BIRD)](https://arxiv.org/abs/2505.03463)
*Muge Du,Zhuozhao Zheng,Wenying Wang,Guotao Quan,Wuliang Shi,Le Shen,Li Zhang,Liang Li,Yinong Liu,Yuxiang Xing*

Main category: cs.CV

TL;DR: 本文提出了一种名为BIRD的新型基于隐式神经表示（INR）的框架，用于非周期性动态CT重建，通过多项创新有效减少运动伪影并增强图像细节。


<details>
  <summary>Details</summary>
Motivation: 动态CT重建在处理非周期性快速运动（如快速心率下的心脏成像）时面临运动伪影和有限角度的挑战。传统方法和现有深度学习及INR技术在计算效率、变形合理性、细节保留和泛化能力方面存在局限性。

Method: 提出了BIRD框架，主要贡献包括：(1) 采用后向变形（backward-warping）以降低计算成本；(2) 使用基于微分同胚（diffeomorphism）的变形向量场（DVF）正则化，确保解剖学上的合理变形；(3) 引入运动补偿分析重建，无需额外预扫描即可增强细节；(4) 设计维度降低方案以高效编码4D坐标。

Result: 通过数字和物理模型以及回顾性患者数据的多种模拟和实践研究表明，BIRD方法能够有效进行非周期性动态CT重建，显著增强图像细节并减少运动伪影。

Conclusion: 所提出的BIRD框架能够实现更准确的动态CT重建，并具有潜在的临床应用价值，例如单心跳周期心脏重建、功能成像的电影序列以及减少常规CT扫描中的运动伪影。

Abstract: Dynamic computed tomography (CT) reconstruction faces significant challenges
in addressing motion artifacts, particularly for nonperiodic rapid movements
such as cardiac imaging with fast heart rates. Traditional methods struggle
with the extreme limited-angle problems inherent in nonperiodic cases. Deep
learning methods have improved performance but face generalization challenges.
Recent implicit neural representation (INR) techniques show promise through
self-supervised deep learning, but have critical limitations: computational
inefficiency due to forward-warping modeling, difficulty balancing DVF
complexity with anatomical plausibility, and challenges in preserving fine
details without additional patient-specific pre-scans. This paper presents a
novel INR-based framework, BIRD, for nonperiodic dynamic CT reconstruction. It
addresses these challenges through four key contributions: (1) backward-warping
deformation that enables direct computation of each dynamic voxel with
significantly reduced computational cost, (2) diffeomorphism-based DVF
regularization that ensures anatomically plausible deformations while
maintaining representational capacity, (3) motion-compensated analytical
reconstruction that enhances fine details without requiring additional
pre-scans, and (4) dimensional-reduction design for efficient 4D coordinate
encoding. Through various simulations and practical studies, including digital
and physical phantoms and retrospective patient data, we demonstrate the
effectiveness of our approach for nonperiodic dynamic CT reconstruction with
enhanced details and reduced motion artifacts. The proposed framework enables
more accurate dynamic CT reconstruction with potential clinical applications,
such as one-beat cardiac reconstruction, cinematic image sequences for
functional imaging, and motion artifact reduction in conventional CT scans.

</details>


### [91] [Blending 3D Geometry and Machine Learning for Multi-View Stereopsis](https://arxiv.org/abs/2505.03470)
*Vibhas Vats,Md. Alimoor Reza,David Crandall,Soon-heung Jung*

Main category: cs.CV

TL;DR: GC MVSNet++是一种新颖的多视图三维重建方法，它在学习阶段主动引入多视图、多尺度的几何一致性约束，显著加速了训练过程并取得了领先的重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的多视图三维重建（MVS）方法通常仅将几何一致性（GC）检查作为后处理步骤，并未将其整合到学习过程中影响模型训练。本研究旨在通过在训练中主动强制执行几何一致性来改进MVS性能和效率。

Method: 提出了GC MVSNet++方法，其核心是在学习阶段主动对参考视图的深度图在多个源视图（多视图）和多个尺度（多尺度）上强制执行几何一致性。通过直接惩罚几何上不一致的像素来加速学习。此外，引入了一个具有两种不同块设计（简单型和特征密集型）的密集连接代价正则化网络，以增强正则化效果。

Result: 该方法显著加速了学习过程，使训练迭代次数相比其他MVS方法减少了一半。在DTU和BlendedMVS数据集上取得了当前最佳性能（state-of-the-art），并在Tanks and Temples基准测试中排名第二。

Conclusion: GC MVSNet++是首个在学习过程中强制执行多视图、多尺度监督几何一致性的MVS方法，这不仅加快了训练速度，也提升了三维重建的精度。

Abstract: Traditional multi-view stereo (MVS) methods primarily depend on photometric
and geometric consistency constraints. In contrast, modern learning-based
algorithms often rely on the plane sweep algorithm to infer 3D geometry,
applying explicit geometric consistency (GC) checks only as a post-processing
step, with no impact on the learning process itself. In this work, we introduce
GC MVSNet plus plus, a novel approach that actively enforces geometric
consistency of reference view depth maps across multiple source views (multi
view) and at various scales (multi scale) during the learning phase (see Fig.
1). This integrated GC check significantly accelerates the learning process by
directly penalizing geometrically inconsistent pixels, effectively halving the
number of training iterations compared to other MVS methods. Furthermore, we
introduce a densely connected cost regularization network with two distinct
block designs simple and feature dense optimized to harness dense feature
connections for enhanced regularization. Extensive experiments demonstrate that
our approach achieves a new state of the art on the DTU and BlendedMVS datasets
and secures second place on the Tanks and Temples benchmark. To our knowledge,
GC MVSNet plus plus is the first method to enforce multi-view, multi-scale
supervised geometric consistency during learning. Our code is available.

</details>


### [92] [UPMAD-Net: A Brain Tumor Segmentation Network with Uncertainty Guidance and Adaptive Multimodal Feature Fusion](https://arxiv.org/abs/2505.03494)
*Zhanyuan Jia,Ni Yao,Danyang Sun,Chuang Han,Yanting Li,Jiaofen Nan,Fubao Zhu,Chen Zhao,Weihua Zhou*

Main category: cs.CV

TL;DR: 提出了一种结合深度学习与区域生长算法先验知识的脑肿瘤分割方法，通过多尺度特征融合、自适应注意力机制和不确定性估计，在BraTS数据集上取得了优异的分割性能。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的准确分割对于诊断和治疗至关重要，但由于其形状不规则、边界模糊和高度可变性，脑肿瘤的精确分割仍然具有挑战性。

Method: 该方法利用多尺度特征融合（MSFF）模块和自适应注意机制（AAM）来提取多尺度特征并捕获全局上下文信息。为了增强模型在低置信度区域的鲁棒性，采用了蒙特卡洛 Dropout（MC Dropout）策略进行不确定性估计。该方法还结合了区域生长算法的先验知识，并基于U-Net架构构建。

Result: 在BraTS数据集上的大量实验表明，该方法性能优越，显著优于多种最先进方法。在BraTS2021测试集上，增强肿瘤（ET）、全肿瘤（WT）和肿瘤核心（TC）的Dice得分分别为89.18%、93.67%和91.23%。在BraTS2019验证集上，ET、WT和TC的Dice得分分别为87.43%、90.92%和90.40%。消融研究进一步证实了每个模块对分割精度的贡献。

Conclusion: 本研究提出了一种基于U-Net架构的新型3D脑肿瘤分割网络。通过结合先验知识并采用不确定性估计方法，提高了模型的鲁棒性和性能。

Abstract: Background: Brain tumor segmentation has a significant impact on the
diagnosis and treatment of brain tumors. Accurate brain tumor segmentation
remains challenging due to their irregular shapes, vague boundaries, and high
variability. Objective: We propose a brain tumor segmentation method that
combines deep learning with prior knowledge derived from a region-growing
algorithm. Methods: The proposed method utilizes a multi-scale feature fusion
(MSFF) module and adaptive attention mechanisms (AAM) to extract multi-scale
features and capture global contextual information. To enhance the model's
robustness in low-confidence regions, the Monte Carlo Dropout (MC Dropout)
strategy is employed for uncertainty estimation. Results: Extensive experiments
demonstrate that the proposed method achieves superior performance on Brain
Tumor Segmentation (BraTS) datasets, significantly outperforming various
state-of-the-art methods. On the BraTS2021 dataset, the test Dice scores are
89.18% for Enhancing Tumor (ET) segmentation, 93.67% for Whole Tumor (WT)
segmentation, and 91.23% for Tumor Core (TC) segmentation. On the BraTS2019
validation set, the validation Dice scores are 87.43%, 90.92%, and 90.40% for
ET, WT, and TC segmentation, respectively. Ablation studies further confirmed
the contribution of each module to segmentation accuracy, indicating that each
component played a vital role in overall performance improvement. Conclusion:
This study proposed a novel 3D brain tumor segmentation network based on the
U-Net architecture. By incorporating the prior knowledge and employing the
uncertainty estimation method, the robustness and performance were improved.
The code for the proposed method is available at
https://github.com/chenzhao2023/UPMAD_Net_BrainSeg.

</details>


### [93] [MRI motion correction via efficient residual-guided denoising diffusion probabilistic models](https://arxiv.org/abs/2505.03498)
*Mojtaba Safari,Shansong Wang,Qiang Li,Zach Eidex,Richard L. J. Qiu,Chih-Wei Chang,Hui Mao,Xiaofeng Yang*

Main category: cs.CV

TL;DR: Res-MoCoDiff是一种新颖的去噪扩散概率模型，通过创新的残差错误转移机制和Swin-Transformer增强的U-net架构，能高效去除MRI中的运动伪影，显著提升图像质量和处理速度。


<details>
  <summary>Details</summary>
Motivation: 磁共振成像（MRI）中的运动伪影会严重降低图像质量并影响定量分析，而传统的缓解策略（如重复采集或运动追踪）成本高昂且流程繁琐。本研究旨在开发一种高效的MRI运动伪影校正方法。

Method: 本研究引入了Res-MoCoDiff模型。该模型在前向扩散过程中整合了新的残差错误转移机制，使噪声分布与运动伪影数据对齐，并实现了高效的四步反向扩散。模型采用U-net作为骨干网络，并用Swin-Transformer模块替换传统注意力层以增强跨分辨率的适应性。训练采用l1+l2组合损失函数，以提升图像锐度和减少像素级错误。

Result: Res-MoCoDiff在去除各种严重程度的运动伪影方面均表现出优越性能。在定量指标上，Res-MoCoDiff始终获得最高的SSIM和最低的NMSE值，对于轻微失真，PSNR可达41.91±2.94 dB。值得注意的是，其平均采样时间缩短至每批次（两张图像切片）0.37秒，远快于传统方法的101.74秒。

Conclusion: Res-MoCoDiff是一种高效且性能优越的MRI运动伪影校正方法，能够在显著减少处理时间的同时，有效提升校正后图像的质量。

Abstract: Purpose: Motion artifacts in magnetic resonance imaging (MRI) significantly
degrade image quality and impair quantitative analysis. Conventional mitigation
strategies, such as repeated acquisitions or motion tracking, are costly and
workflow-intensive. This study introduces Res-MoCoDiff, an efficient denoising
diffusion probabilistic model tailored for MRI motion artifact correction.
Methods: Res-MoCoDiff incorporates a novel residual error shifting mechanism in
the forward diffusion process, aligning the noise distribution with
motion-corrupted data and enabling an efficient four-step reverse diffusion. A
U-net backbone enhanced with Swin-Transformer blocks conventional attention
layers, improving adaptability across resolutions. Training employs a combined
l1+l2 loss, which promotes image sharpness and reduces pixel-level errors.
Res-MoCoDiff was evaluated on synthetic dataset generated using a realistic
motion simulation framework and on an in-vivo dataset. Comparative analyses
were conducted against established methods, including CycleGAN, Pix2pix, and
MT-DDPM using quantitative metrics such as peak signal-to-noise ratio (PSNR),
structural similarity index measure (SSIM), and normalized mean squared error
(NMSE). Results: The proposed method demonstrated superior performance in
removing motion artifacts across all motion severity levels. Res-MoCoDiff
consistently achieved the highest SSIM and the lowest NMSE values, with a PSNR
of up to 41.91+-2.94 dB for minor distortions. Notably, the average sampling
time was reduced to 0.37 seconds per batch of two image slices, compared with
101.74 seconds for conventional approaches.

</details>


### [94] [Modality-Guided Dynamic Graph Fusion and Temporal Diffusion for Self-Supervised RGB-T Tracking](https://arxiv.org/abs/2505.03507)
*Shenglan Li,Rui Yao,Yong Zhou,Hancheng Zhu,Kunyang Sun,Bing Liu,Zhiwen Shao,Jiaqi Zhao*

Main category: cs.CV

TL;DR: 提出了一种名为GDSTrack的自监督RGB-T跟踪新方法，通过动态图融合和时间扩散解决伪标签错误和相似目标噪声问题，提升跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督RGB-T跟踪方法中，错误的伪标签或背景噪声会影响模态融合效率，而由相似目标引起的伪标签噪声会进一步降低跟踪性能。

Method: 提出了GDSTrack方法，包含两个核心模块：1) 模态引导的动态图融合 (MDGF)，通过动态邻接矩阵引导图注意力，聚焦并融合目标的相干区域；2) 时间图信息扩散 (TGID)，将邻近帧的MDGF特征建模为干扰，利用生成模型的去噪能力来提高对相似目标噪声的鲁棒性。

Result: 在四个公开的RGB-T跟踪数据集上进行的广泛实验表明，GDSTrack的表现优于当前最先进的方法。

Conclusion: GDSTrack通过引入动态图融合和时间扩散机制，有效解决了自监督RGB-T跟踪中伪标签错误和噪声干扰的挑战，提升了跟踪的鲁棒性和准确性。

Abstract: To reduce the reliance on large-scale annotations, self-supervised RGB-T
tracking approaches have garnered significant attention. However, the omission
of the object region by erroneous pseudo-label or the introduction of
background noise affects the efficiency of modality fusion, while pseudo-label
noise triggered by similar object noise can further affect the tracking
performance. In this paper, we propose GDSTrack, a novel approach that
introduces dynamic graph fusion and temporal diffusion to address the above
challenges in self-supervised RGB-T tracking. GDSTrack dynamically fuses the
modalities of neighboring frames, treats them as distractor noise, and
leverages the denoising capability of a generative model. Specifically, by
constructing an adjacency matrix via an Adjacency Matrix Generator (AMG), the
proposed Modality-guided Dynamic Graph Fusion (MDGF) module uses a dynamic
adjacency matrix to guide graph attention, focusing on and fusing the object's
coherent regions. Temporal Graph-Informed Diffusion (TGID) models MDGF features
from neighboring frames as interference, and thus improving robustness against
similar-object noise. Extensive experiments conducted on four public RGB-T
tracking datasets demonstrate that GDSTrack outperforms the existing
state-of-the-art methods. The source code is available at
https://github.com/LiShenglana/GDSTrack.

</details>


### [95] [Optimization of Module Transferability in Single Image Super-Resolution: Universality Assessment and Cycle Residual Blocks](https://arxiv.org/abs/2505.03522)
*Haotong Cheng,Zhiqi Zhang,Hao Li,Xinshang Zhang*

Main category: cs.CV

TL;DR: 论文引入"普适性"概念和UAE度量来评估SISR模型中模块的可移植性，并基于此设计了CRB和DCRB模块，实验证明其有效提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有单图像超分辨率（SISR）研究主要关注原始性能提升，而忽略了对架构组件可移植性的量化研究。

Method: 1. 引入"普适性"（Universality）概念及其定义，扩展了传统"泛化性"的内涵，以涵盖模块的可移植性。2. 提出普适性评估方程（UAE），用于量化给定模块在不同模型间的移植便利性。3. 基于UAE对标准残差块和其他即插即用模块的评估结果，设计了两种优化模块：循环残差块（CRB）和深度可分离循环残差块（DCRB）。

Result: 在自然场景、遥感数据、极端工业图像和设备端部署的综合实验表明，嵌入所提出即插即用模块的网络性能优于多种当前最优方法，峰值信噪比（PSNR）最多提升0.83dB，或在重建保真度损失可忽略的情况下参数量减少71.3%。

Conclusion: 提出的普适性概念、UAE度量以及基于此设计的CRB和DCRB模块，能够有效评估和提升SISR模型组件的可移植性，并带来显著的性能和效率改进。

Abstract: Deep learning has substantially advanced the Single Image Super-Resolution
(SISR). However, existing researches have predominantly focused on raw
performance gains, with little attention paid to quantifying the
transferability of architectural components. In this paper, we introduce the
concept of "Universality" and its associated definitions which extend the
traditional notion of "Generalization" to encompass the modules' ease of
transferability, thus revealing the relationships between module universality
and model generalizability. Then we propose the Universality Assessment
Equation (UAE), a metric for quantifying how readily a given module could be
transplanted across models. Guided by the UAE results of standard residual
blocks and other plug-and-play modules, we further design two optimized
modules, Cycle Residual Block (CRB) and Depth-Wise Cycle Residual Block (DCRB).
Through comprehensive experiments on natural-scene benchmarks, remote-sensing
datasets, extreme-industrial imagery and on-device deployments, we demonstrate
that networks embedded with the proposed plug-and-play modules outperform
several state-of-the-arts, reaching a PSNR enhancement of up to 0.83dB or
enabling a 71.3% reduction in parameters with negligible loss in reconstruction
fidelity.

</details>


### [96] [Coop-WD: Cooperative Perception with Weighting and Denoising for Robust V2V Communication](https://arxiv.org/abs/2505.03528)
*Chenguang Liu,Jianjun Chen,Yunfei Chen,Yubei He,Zhuangkun Wei,Hongjian Sun,Haiyan Lu,Qi Hao*

Main category: cs.CV

TL;DR: 本文提出了一种名为Coop-WD的联合加权和去噪框架，通过分层特征增强来提升在V2V通信信道损伤下的自动驾驶协作感知性能，并提出了一个高效变体Coop-WD-eco以降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的协作感知研究在处理V2V通信损伤对感知精度的影响时，缺乏对不同损伤程度的普适性。

Method: 提出了Coop-WD框架，该框架结合了自监督对比模型（用于车辆级特征增强）和条件扩散概率模型（用于像素级特征增强）进行联合加权和去噪。同时，提出了一个高效变体Coop-WD-eco，能选择性停用去噪以减少处理开销。

Result: 仿真结果显示，Coop-WD在所有测试的信道类型（包括莱斯衰落、非平稳性和时变失真）中均优于传统基准方法。Coop-WD-eco在严重失真情况下可将计算成本降低高达50%，同时在信道条件改善时保持与Coop-WD相当的精度。

Conclusion: 提出的Coop-WD框架及其高效变体Coop-WD-eco能够有效增强受V2V信道损伤影响的协作感知能力，在提升感知精度的同时，Coop-WD-eco还能显著降低计算开销，为实际应用提供了可行方案。

Abstract: Cooperative perception, leveraging shared information from multiple vehicles
via vehicle-to-vehicle (V2V) communication, plays a vital role in autonomous
driving to alleviate the limitation of single-vehicle perception. Existing
works have explored the effects of V2V communication impairments on perception
precision, but they lack generalization to different levels of impairments. In
this work, we propose a joint weighting and denoising framework, Coop-WD, to
enhance cooperative perception subject to V2V channel impairments. In this
framework, the self-supervised contrastive model and the conditional diffusion
probabilistic model are adopted hierarchically for vehicle-level and
pixel-level feature enhancement. An efficient variant model, Coop-WD-eco, is
proposed to selectively deactivate denoising to reduce processing overhead.
Rician fading, non-stationarity, and time-varying distortion are considered.
Simulation results demonstrate that the proposed Coop-WD outperforms
conventional benchmarks in all types of channels. Qualitative analysis with
visual examples further proves the superiority of our proposed method. The
proposed Coop-WD-eco achieves up to 50% reduction in computational cost under
severe distortion while maintaining comparable accuracy as channel conditions
improve.

</details>


### [97] [RAIL: Region-Aware Instructive Learning for Semi-Supervised Tooth Segmentation in CBCT](https://arxiv.org/abs/2505.03538)
*Chuyu Zhao,Hao Huang,Jiashuo Guo,Ziyu Shen,Zhongwei Zhou,Jie Liu,Zekuan Yu*

Main category: cs.CV

TL;DR: 提出了一种名为RAIL的半监督学习框架，用于3D牙齿分割，通过双学生组和特定机制解决了监督不足和伪标签不可靠问题。


<details>
  <summary>Details</summary>
Motivation: 现有半监督3D牙齿分割方法在标记数据不足时，面临结构模糊区域监督不足和不可靠伪标签导致性能下降的问题。

Method: 提出了区域感知指导学习（RAIL）框架，一个双组双学生半监督模型。它包含两个指导机制：1) 分歧聚焦监督（DFS）控制器，用于在监督学习中处理结构模糊或错标区域；2) 置信度感知学习（CAL）调制器，用于在无监督学习中提升伪标签的可靠性。

Result: 在四个CBCT牙齿分割数据集上的实验表明，RAIL在有限标注的情况下优于现有最先进方法。

Conclusion: RAIL框架有效解决了半监督3D牙齿分割中的关键挑战（监督不足和伪标签不可靠），在有限的标记数据下展现了优越的性能。

Abstract: Semi-supervised learning has become a compelling approach for 3D tooth
segmentation from CBCT scans, where labeled data is minimal. However, existing
methods still face two persistent challenges: limited corrective supervision in
structurally ambiguous or mislabeled regions during supervised training and
performance degradation caused by unreliable pseudo-labels on unlabeled data.
To address these problems, we propose Region-Aware Instructive Learning (RAIL),
a dual-group dual-student, semi-supervised framework. Each group contains two
student models guided by a shared teacher network. By alternating training
between the two groups, RAIL promotes intergroup knowledge transfer and
collaborative region-aware instruction while reducing overfitting to the
characteristics of any single model. Specifically, RAIL introduces two
instructive mechanisms. Disagreement-Focused Supervision (DFS) Controller
improves supervised learning by instructing predictions only within areas where
student outputs diverge from both ground truth and the best student, thereby
concentrating supervision on structurally ambiguous or mislabeled areas. In the
unsupervised phase, Confidence-Aware Learning (CAL) Modulator reinforces
agreement in regions with high model certainty while reducing the effect of
low-confidence predictions during training. This helps prevent our model from
learning unstable patterns and improves the overall reliability of
pseudo-labels. Extensive experiments on four CBCT tooth segmentation datasets
show that RAIL surpasses state-of-the-art methods under limited annotation. Our
code will be available at https://github.com/Tournesol-Saturday/RAIL.

</details>


### [98] [Panoramic Out-of-Distribution Segmentation](https://arxiv.org/abs/2505.03539)
*Mengfei Duan,Kailun Yang,Yuheng Zhang,Yihong Cao,Fei Teng,Kai Luo,Jiaming Zhang,Zhiyong Li,Shutao Li*

Main category: cs.CV

TL;DR: 提出了一种新的全景图像分布外分割任务 (PanOoS) 和首个解决方案 POS，通过文本引导的提示分布学习来解决全景图像中异常值识别和传统OoS模型效果不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 当前的全景语义分割方法难以识别异常值，而传统的针孔相机分布外分割 (OoS) 模型由于背景杂乱和像素失真，在全景图像域表现不佳。

Method: 引入了新的任务“全景分布外分割 (PanOoS)”，并提出了首个解决方案 POS。POS 利用文本引导的提示分布学习适应全景图像特性，具体包括：一个旨在发挥 CLIP 跨域泛化能力的解耦策略；“基于提示的恢复注意力机制 (PRA)”通过提示引导和自适应校正优化语义解码；“双层提示分布学习 (BPDL)”通过语义原型监督优化逐像素掩码嵌入的流形。同时，为弥补数据不足，构建了 DenseOoS 和 QuadOoS 两个基准数据集。

Result: POS 表现出卓越性能，在 DenseOoS 数据集上，AuPRC 指标提升了 34.25%，FPR95 指标降低了 21.42%，显著优于现有的针孔OoS方法。此外，POS 在闭集分割能力方面也表现领先。

Conclusion: 本文提出的 PanOoS 任务和 POS 方法有效解决了全景图像中的分布外分割问题。POS 通过其创新设计，在新建的基准数据集上取得了显著优于现有方法的性能，并展现了强大的闭集分割能力。

Abstract: Panoramic imaging enables capturing 360{\deg} images with an ultra-wide
Field-of-View (FoV) for dense omnidirectional perception. However, current
panoramic semantic segmentation methods fail to identify outliers, and pinhole
Out-of-distribution Segmentation (OoS) models perform unsatisfactorily in the
panoramic domain due to background clutter and pixel distortions. To address
these issues, we introduce a new task, Panoramic Out-of-distribution
Segmentation (PanOoS), achieving OoS for panoramas. Furthermore, we propose the
first solution, POS, which adapts to the characteristics of panoramic images
through text-guided prompt distribution learning. Specifically, POS integrates
a disentanglement strategy designed to materialize the cross-domain
generalization capability of CLIP. The proposed Prompt-based Restoration
Attention (PRA) optimizes semantic decoding by prompt guidance and
self-adaptive correction, while Bilevel Prompt Distribution Learning (BPDL)
refines the manifold of per-pixel mask embeddings via semantic prototype
supervision. Besides, to compensate for the scarcity of PanOoS datasets, we
establish two benchmarks: DenseOoS, which features diverse outliers in complex
environments, and QuadOoS, captured by a quadruped robot with a panoramic
annular lens system. Extensive experiments demonstrate superior performance of
POS, with AuPRC improving by 34.25% and FPR95 decreasing by 21.42% on DenseOoS,
outperforming state-of-the-art pinhole-OoS methods. Moreover, POS achieves
leading closed-set segmentation capabilities. Code and datasets will be
available at https://github.com/MengfeiD/PanOoS.

</details>


### [99] [Read My Ears! Horse Ear Movement Detection for Equine Affective State Assessment](https://arxiv.org/abs/2505.03554)
*João Alves,Pia Haubro Andersen,Rikke Gade*

Main category: cs.CV

TL;DR: 该研究探索了从马匹视频中自动检测耳朵动作单元（AU）的方法，以解决手动标注耗时耗力的问题，并提升马匹情感状态评估的效率。


<details>
  <summary>Details</summary>
Motivation: 马匹面部动作编码系统（EquiFACS）是评估马匹情感状态（尤其是与不适相关的微妙面部表情）的关键工具，但手动标注耗时且成本高昂，导致相关领域注释数据稀缺。因此，开发自动化标注系统至关重要。

Method: 研究比较了不同方法用于特定耳朵动作单元（AU）的检测和定位。主要利用了基于深度学习的视频特征提取结合循环神经网络进行视频分类，并辅以经典的光流法。

Result: 在公开的马匹视频数据集上，所提出的方法在耳朵运动存在的分类准确率上达到了87.5%。

Conclusion: 该研究展示了其方法在自动检测马匹耳朵AU方面的潜力，并讨论了未来发展方向，旨在将自动AU检测技术应用于马匹福利和兽医诊断等实际场景。

Abstract: The Equine Facial Action Coding System (EquiFACS) enables the systematic
annotation of facial movements through distinct Action Units (AUs). It serves
as a crucial tool for assessing affective states in horses by identifying
subtle facial expressions associated with discomfort. However, the field of
horse affective state assessment is constrained by the scarcity of annotated
data, as manually labelling facial AUs is both time-consuming and costly. To
address this challenge, automated annotation systems are essential for
leveraging existing datasets and improving affective states detection tools. In
this work, we study different methods for specific ear AU detection and
localization from horse videos. We leverage past works on deep learning-based
video feature extraction combined with recurrent neural networks for the video
classification task, as well as a classic optical flow based approach. We
achieve 87.5% classification accuracy of ear movement presence on a public
horse video dataset, demonstrating the potential of our approach. We discuss
future directions to develop these systems, with the aim of bridging the gap
between automated AU detection and practical applications in equine welfare and
veterinary diagnostics. Our code will be made publicly available at
https://github.com/jmalves5/read-my-ears.

</details>


### [100] [Generating Synthetic Data via Augmentations for Improved Facial Resemblance in DreamBooth and InstantID](https://arxiv.org/abs/2505.03557)
*Koray Ulusan,Benjamin Kiefer*

Main category: cs.CV

TL;DR: 该研究探讨了数据增强在使用 DreamBooth 和 InstantID 技术通过 Stable Diffusion 生成专业肖像时，对提升面部相似性的影响。


<details>
  <summary>Details</summary>
Motivation: 从业余照片生成个性化专业肖像是一个新兴领域，需要提升生成图像与真人的面部相似度以满足各种下游应用的需求。

Method: 通过对多样化的主体数据集进行一系列实验，评估了不同数据增强策略在使用 DreamBooth 和 InstantID 两种主流个性化技术时，对生成头像与原始主体保真度的效果。引入了 FaceDistance（FaceNet 的封装工具）来根据面部相似性对生成结果进行排序，以辅助评估。

Result: 研究提供了关于数据增强在提升 SDXL 生成肖像中面部相似性方面作用的见解。

Conclusion: 该研究为在下游应用中有效部署数据增强策略以增强面部相似性提供了参考。

Abstract: The personalization of Stable Diffusion for generating professional portraits
from amateur photographs is a burgeoning area, with applications in various
downstream contexts. This paper investigates the impact of augmentations on
improving facial resemblance when using two prominent personalization
techniques: DreamBooth and InstantID. Through a series of experiments with
diverse subject datasets, we assessed the effectiveness of various augmentation
strategies on the generated headshots' fidelity to the original subject. We
introduce FaceDistance, a wrapper around FaceNet, to rank the generations based
on facial similarity, which aided in our assessment. Ultimately, this research
provides insights into the role of augmentations in enhancing facial
resemblance in SDXL-generated portraits, informing strategies for their
effective deployment in downstream applications.

</details>


### [101] [Real-Time Person Image Synthesis Using a Flow Matching Model](https://arxiv.org/abs/2505.03562)
*Jiwoo Jeong,Kirok Kim,Wooju Kim,Nam-Joon Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于流匹配（FM）的实时姿态引导人物图像合成模型（RPFM），旨在解决现有方法（尤其是扩散模型）在实时应用中速度过慢的问题。


<details>
  <summary>Details</summary>
Motivation: 姿态引导人物图像合成（PGPIS）在手语视频生成、AR/VR、游戏和直播等领域有重要应用，这些场景对实时性要求高。然而，现有方法难以兼顾高保真度和实时性，尤其是基于扩散的方法虽然图像质量高但采样速度慢，阻碍了其在时间敏感应用中的部署。

Method: 提出了一种基于流匹配（Flow Matching, FM）的生成模型RPFM。该方法支持更快、更稳定、更高效的训练和采样，并且支持条件生成，可在潜空间操作，特别适合需要速度和质量兼顾的实时PGPIS应用。

Result: 在DeepFashion数据集上的评估结果显示，RPFM实现了接近实时的采样速度，同时保持了与当前最先进模型相当的性能。该方法以略微可接受的生成图像准确性下降为代价，换取了超过两倍的生成速度提升，从而确保了实时性能。

Conclusion: 所提出的RPFM模型通过显著提高生成速度同时保持可接受的图像质量，实现了实时姿态引导人物图像合成，为实时交互系统的开发提供了关键一步。

Abstract: Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images
conditioned on a target pose and a source image. This task plays a key role in
various real-world applications, such as sign language video generation, AR/VR,
gaming, and live streaming. In these scenarios, real-time PGPIS is critical for
providing immediate visual feedback and maintaining user immersion.However,
achieving real-time performance remains a significant challenge due to the
complexity of synthesizing high-fidelity images from diverse and dynamic human
poses. Recent diffusion-based methods have shown impressive image quality in
PGPIS, but their slow sampling speeds hinder deployment in time-sensitive
applications. This latency is particularly problematic in tasks like generating
sign language videos during live broadcasts, where rapid image updates are
required. Therefore, developing a fast and reliable PGPIS model is a crucial
step toward enabling real-time interactive systems. To address this challenge,
we propose a generative model based on flow matching (FM). Our approach enables
faster, more stable, and more efficient training and sampling. Furthermore, the
proposed model supports conditional generation and can operate in latent space,
making it especially suitable for real-time PGPIS applications where both speed
and quality are critical. We evaluate our proposed method, Real-Time Person
Image Synthesis Using a Flow Matching Model (RPFM), on the widely used
DeepFashion dataset for PGPIS tasks. Our results show that RPFM achieves
near-real-time sampling speeds while maintaining performance comparable to the
state-of-the-art models. Our methodology trades off a slight, acceptable
decrease in generated-image accuracy for over a twofold increase in generation
speed, thereby ensuring real-time performance.

</details>


### [102] [Uncertainty-Aware Prototype Semantic Decoupling for Text-Based Person Search in Full Images](https://arxiv.org/abs/2505.03567)
*Zengli Luo,Canlong Zhang,Xiaochun Lu,Zhixin Li,Zhiwen Wang*

Main category: cs.CV

TL;DR: 提出了一种名为 UPD-TBPS 的新框架，旨在通过解决检测和匹配中的不确定性问题，提高复杂场景下基于文本的行人搜索性能。


<details>
  <summary>Details</summary>
Motivation: 在包含多个行人的复杂场景中，现有的基于文本的行人搜索方法因检测和匹配的不确定性而导致性能下降。

Method: 提出了 UPD-TBPS 框架，包含三个模块：多粒度不确定性估计 (MUE) 以减少早期检测不确定性；基于原型的不确定性解耦 (PUD) 以提取目标行人特征并减少匹配不确定性；以及跨模态重识别 (ReID) 以评估候选者，提高检测和检索准确性。

Result: 在 CUHK-SYSU-TBPS 和 PRW-TBPS 数据集上的实验验证了所提出框架的有效性。

Conclusion: UPD-TBPS 框架通过其三个模块有效解决了文本行人搜索中的不确定性问题，从而提升了在复杂场景下的性能。

Abstract: Text-based pedestrian search (TBPS) in full images aims to locate a target
pedestrian in untrimmed images using natural language descriptions. However, in
complex scenes with multiple pedestrians, existing methods are limited by
uncertainties in detection and matching, leading to degraded performance. To
address this, we propose UPD-TBPS, a novel framework comprising three modules:
Multi-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty
Decoupling (PUD), and Cross-modal Re-identification (ReID). MUE conducts
multi-granularity queries to identify potential targets and assigns confidence
scores to reduce early-stage uncertainty. PUD leverages visual context
decoupling and prototype mining to extract features of the target pedestrian
described in the query. It separates and learns pedestrian prototype
representations at both the coarse-grained cluster level and the fine-grained
individual level, thereby reducing matching uncertainty. ReID evaluates
candidates with varying confidence levels, improving detection and retrieval
accuracy. Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets validate the
effectiveness of our framework.

</details>


### [103] [Corner Cases: How Size and Position of Objects Challenge ImageNet-Trained Models](https://arxiv.org/abs/2505.03569)
*Mishal Fatima,Steffen Jung,Margret Keuper*

Main category: cs.CV

TL;DR: 本文研究了图像中对象的位置和大小偏差如何导致模型依赖虚假的背景特征，并提出了一个新数据集Hard-Spurious-ImageNet来证明此现象。研究发现，当对象小且偏离中心时，模型更易受背景影响，且现有缓解方法对此类偏差效果不佳。


<details>
  <summary>Details</summary>
Motivation: 图像背景是导致数据点之间产生虚假关联的主要因素。由于人类在拍摄图像时的审美偏好，数据集可能在不同类别中表现出位置和大小偏差。这些偏差可能会影响模型在多大程度上依赖背景中的虚假特征进行预测，而现有缓解虚假特征的方法可能未充分考虑这些因素。

Method: 1. 提出一个名为Hard-Spurious-ImageNet的合成数据集，该数据集源自ImageNet1k，包含具有不同背景、对象位置和对象大小的图像。
2. 在该数据集上评估不同的预训练模型，以分析其对背景虚假特征的依赖程度。
3. 评估现有旨在减轻有害虚假特征的方法在处理对象大小和位置变化时的鲁棒性，特别关注最差组准确率。

Result: 当感兴趣区域（ROI）与图像的比例较小且对象远离图像中心时，大多数模型严重依赖背景中的虚假特征。此外，当图像中核心特征的大小和位置发生变化时，当前旨在减轻有害虚假特征的方法未能显著提高最差组的准确率。

Conclusion: 图像中对象的位置和大小偏差会显著影响模型对背景中虚假特征的依赖程度。现有的缓解虚假特征的方法在处理这些偏差方面，尤其是在核心特征的大小和位置变化时，表现不佳。Hard-Spurious-ImageNet数据集有效地揭示了这些问题。

Abstract: Backgrounds in images play a major role in contributing to spurious
correlations among different data points. Owing to aesthetic preferences of
humans capturing the images, datasets can exhibit positional (location of the
object within a given frame) and size (region-of-interest to image ratio)
biases for different classes. In this paper, we show that these biases can
impact how much a model relies on spurious features in the background to make
its predictions. To better illustrate our findings, we propose a synthetic
dataset derived from ImageNet1k, Hard-Spurious-ImageNet, which contains images
with various backgrounds, object positions, and object sizes. By evaluating the
dataset on different pretrained models, we find that most models rely heavily
on spurious features in the background when the region-of-interest (ROI) to
image ratio is small and the object is far from the center of the image.
Moreover, we also show that current methods that aim to mitigate harmful
spurious features, do not take into account these factors, hence fail to
achieve considerable performance gains for worst-group accuracies when the size
and location of core features in an image change.

</details>


### [104] [Supervised and Unsupervised Textile Classification via Near-Infrared Hyperspectral Imaging and Deep Learning](https://arxiv.org/abs/2505.03575)
*Maria Kainz,Johannes K. Krondorfer,Malte Jaschik,Maria Jernej,Harald Ganster*

Main category: cs.CV

TL;DR: 研究利用高光谱近红外成像和深度学习（卷积神经网络及自编码器）对纺织纤维进行分类，以提升回收效率。


<details>
  <summary>Details</summary>
Motivation: 纺织工业对环境影响巨大，回收纺织纤维对减少此影响至关重要，而高效的纤维分类与分拣是回收的关键。

Method: 采用高光谱近红外（NIR）成像技术，并结合有监督的优化卷积神经网络（CNNs）和无监督的自编码器网络进行纺织纤维分类，同时测试了这些模型在不同纺织结构上的泛化能力。

Result: 优化的卷积神经网络（CNNs）和自编码器网络在不同条件下均展现出强大的、稳健的泛化能力。

Conclusion: 高光谱成像和深度学习技术通过提供准确且稳健的分类方法，在推动可持续纺织品回收方面具有巨大潜力。

Abstract: Recycling textile fibers is critical to reducing the environmental impact of
the textile industry. Hyperspectral near-infrared (NIR) imaging combined with
advanced deep learning algorithms offers a promising solution for efficient
fiber classification and sorting. In this study, we investigate supervised and
unsupervised deep learning models and test their generalization capabilities on
different textile structures. We show that optimized convolutional neural
networks (CNNs) and autoencoder networks achieve robust generalization under
varying conditions. These results highlight the potential of hyperspectral
imaging and deep learning to advance sustainable textile recycling through
accurate and robust classification.

</details>


### [105] [DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer Questions in Dynamic Scenes](https://arxiv.org/abs/2505.03581)
*Sergey Linok,Vadim Semenov,Anastasia Trunova,Oleg Bulichev,Dmitry Yudin*

Main category: cs.CV

TL;DR: 论文介绍了一种名为DyGEnc的新方法，通过编码动态图，将压缩的时空结构表示与大型语言模型相结合，以改进对动态环境中人与物交互历史的问答能力，并可扩展处理原始图像。


<details>
  <summary>Details</summary>
Motivation: 当前分析动态环境事件的方法主要依赖视觉模型，这些模型通常从图像中隐式捕获信息，缺乏可解释的时空对象表示，这限制了智能体和机器人理解和与人类交互的能力。

Method: 提出了DyGEnc方法，该方法整合了压缩的时空结构观测表示和大型语言模型的认知能力。它通过处理一系列文本场景图来实现高级问答，并能利用基础模型从原始图像中提取显式文本场景图。

Result: 在STAR和AGQA数据集上，DyGEnc在处理关于人与物交互历史的查询方面，比现有视觉方法高出15-25%。此外，该方法已成功扩展到处理原始输入图像，并通过机器人实验得到证实。

Conclusion: DyGEnc为实现稳健且压缩的基于图的机器人记忆提供了有效途径，有助于机器人进行长时程推理，这些发现将促进其实际应用。

Abstract: The analysis of events in dynamic environments poses a fundamental challenge
in the development of intelligent agents and robots capable of interacting with
humans. Current approaches predominantly utilize visual models. However, these
methods often capture information implicitly from images, lacking interpretable
spatial-temporal object representations. To address this issue we introduce
DyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates
compressed spatial-temporal structural observation representation with the
cognitive capabilities of large language models. The purpose of this
integration is to enable advanced question answering based on a sequence of
textual scene graphs. Extended evaluations on the STAR and AGQA datasets
indicate that DyGEnc outperforms existing visual methods by a large margin of
15-25% in addressing queries regarding the history of human-to-object
interactions. Furthermore, the proposed method can be seamlessly extended to
process raw input images utilizing foundational models for extracting explicit
textual scene graphs, as substantiated by the results of a robotic experiment
conducted with a wheeled manipulator platform. We hope that these findings will
contribute to the implementation of robust and compressed graph-based robotic
memory for long-horizon reasoning. Code is available at
github.com/linukc/DyGEnc.

</details>


### [106] [Fixed-Length Dense Fingerprint Representation](https://arxiv.org/abs/2505.03597)
*Zhiyu Pan,Xiongjun Guan,Yongjie Duan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种名为FLARE的指纹匹配框架，采用固定长度的密集描述符，并结合姿态对齐和鲁棒增强技术，以实现高效、准确的指纹识别，尤其在跨模态和低质量场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够有效处理不同指纹模态、姿态变化和噪声干扰的鲁棒固定长度指纹表示方法仍然是一个重大挑战。

Method: 提出了一种固定长度的密集描述符（FLARE框架），该描述符采用三维密集描述符捕捉指纹脊线结构的空间关系，并集成了基于姿态的对齐和双重增强策略（改善脊线清晰度并保留原始模态）。

Result: FLARE在滚动、平面、潜在和非接触式指纹等多种类型上均取得了卓越性能，尤其在跨模态和低质量场景下显著优于现有方法。实验验证了密集描述符设计、对齐及增强模块的有效性。

Conclusion: FLARE是一种有效且可推广的统一、可扩展解决方案，适用于鲁棒的指纹表示和匹配。

Abstract: Fixed-length fingerprint representations, which map each fingerprint to a
compact and fixed-size feature vector, are computationally efficient and
well-suited for large-scale matching. However, designing a robust
representation that effectively handles diverse fingerprint modalities, pose
variations, and noise interference remains a significant challenge. In this
work, we propose a fixed-length dense descriptor of fingerprints, and introduce
FLARE-a fingerprint matching framework that integrates the Fixed-Length dense
descriptor with pose-based Alignment and Robust Enhancement. This fixed-length
representation employs a three-dimensional dense descriptor to effectively
capture spatial relationships among fingerprint ridge structures, enabling
robust and locally discriminative representations. To ensure consistency within
this dense feature space, FLARE incorporates pose-based alignment using
complementary estimation methods, along with dual enhancement strategies that
refine ridge clarity while preserving the original fingerprint modality. The
proposed dense descriptor supports fixed-length representation while
maintaining spatial correspondence, enabling fast and accurate similarity
computation. Extensive experiments demonstrate that FLARE achieves superior
performance across rolled, plain, latent, and contactless fingerprints,
significantly outperforming existing methods in cross-modality and low-quality
scenarios. Further analysis validates the effectiveness of the dense descriptor
design, as well as the impact of alignment and enhancement modules on the
accuracy of dense descriptor matching. Experimental results highlight the
effectiveness and generalizability of FLARE as a unified and scalable solution
for robust fingerprint representation and matching. The implementation and code
will be publicly available at https://github.com/Yu-Yy/FLARE.

</details>


### [107] [From Pixels to Polygons: A Survey of Deep Learning Approaches for Medical Image-to-Mesh Reconstruction](https://arxiv.org/abs/2505.03599)
*Fengming Lin,Arezoo Zakeri,Yidan Xue,Michael MacRaild,Haoran Dou,Zherui Zhou,Ziwei Zou,Ali Sarrami-Foroushani,Jinming Duan,Alejandro F. Frangi*

Main category: cs.CV

TL;DR: 这篇综述系统性地回顾了基于深度学习的医学图像到三维网格重建方法，将其分为四类进行分析评估，并讨论了数据集、挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 深度学习驱动的医学图像到网格重建技术发展迅速，对计算医学、疾病理解、诊断和治疗至关重要，需要一篇全面的综述来梳理现有方法并指导未来研究。

Method: 该综述将现有方法分为四类（模板模型、统计模型、生成模型、隐式模型），对每类方法的基础、优缺点、适用性进行详细分析，并在不同解剖应用中进行评估，同时汇总分析了公开数据集、评估指标和损失函数。

Result: 综述提供了对各类重建方法的深入分析和比较，识别了当前领域面临的主要挑战（如拓扑正确性、几何精度、多模态集成），并指出了有前景的未来研究方向。

Conclusion: 这篇系统性综述为医学图像分析和计算医学领域的研究者和实践者提供了关于深度学习图像到网格重建的全面参考，总结了现有技术，并展望了未来的发展趋势和挑战。

Abstract: Deep learning-based medical image-to-mesh reconstruction has rapidly evolved,
enabling the transformation of medical imaging data into three-dimensional mesh
models that are critical in computational medicine and in silico trials for
advancing our understanding of disease mechanisms, and diagnostic and
therapeutic techniques in modern medicine. This survey systematically
categorizes existing approaches into four main categories: template models,
statistical models, generative models, and implicit models. Each category is
analysed in detail, examining their methodological foundations, strengths,
limitations, and applicability to different anatomical structures and imaging
modalities. We provide an extensive evaluation of these methods across various
anatomical applications, from cardiac imaging to neurological studies,
supported by quantitative comparisons using standard metrics. Additionally, we
compile and analyze major public datasets available for medical mesh
reconstruction tasks and discuss commonly used evaluation metrics and loss
functions. The survey identifies current challenges in the field, including
requirements for topological correctness, geometric accuracy, and
multi-modality integration. Finally, we present promising future research
directions in this domain. This systematic review aims to serve as a
comprehensive reference for researchers and practitioners in medical image
analysis and computational medicine.

</details>


### [108] [PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model](https://arxiv.org/abs/2505.03603)
*Y. B. Wang,S. Z. Zhou,J. F. Wu,T. Hu,J. N. Zhang,Y. Liu*

Main category: cs.CV

TL;DR: 提出了一种名为 PAHA 的端到端音频驱动上半身人体动画框架，使用扩散模型，通过部件感知重加权和部件一致性增强方法解决了现有技术的推理时间长、特定区域生成质量和音动一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动人体动画方法依赖多阶段生成和中间表示，导致推理时间长，且在特定前景区域的生成质量和音画同步方面存在问题，主要原因是缺乏局部化的细粒度监督指导。

Method: 提出了PAHA框架，一种基于扩散模型的端到端音频驱动上半身人体动画方法。引入了两个关键方法：部件感知重加权（PAR），根据姿态置信度动态调整区域训练损失权重以提高视觉质量；以及部件一致性增强（PCE），构建并训练基于扩散的区域音视分类器以提高动作与语音的同步性。此外，为PCE设计了两种新的推理指导方法：顺序指导（SG）和差分指导（DG）。同时，构建了首个公开的中文新闻主播语音数据集CNAS。

Result: 大量的实验结果和用户研究表明，PAHA在音画同步和视频相关评估方面显著优于现有方法。

Conclusion: PAHA框架通过引入PAR和PCE等创新方法，有效提升了音频驱动人体动画的质量和一致性，并为该领域的研究提供了新的数据集和基准。

Abstract: Audio-driven human animation technology is widely used in human-computer
interaction, and the emergence of diffusion models has further advanced its
development. Currently, most methods rely on multi-stage generation and
intermediate representations, resulting in long inference time and issues with
generation quality in specific foreground regions and audio-motion consistency.
These shortcomings are primarily due to the lack of localized fine-grained
supervised guidance. To address above challenges, we propose PAHA, an
end-to-end audio-driven upper-body human animation framework with diffusion
model. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts
Consistency Enhancement (PCE). PAR dynamically adjusts regional training loss
weights based on pose confidence scores, effectively improving visual quality.
PCE constructs and trains diffusion-based regional audio-visual classifiers to
improve the consistency of motion and co-speech audio. Afterwards, we design
two novel inference guidance methods for the foregoing classifiers, Sequential
Guidance (SG) and Differential Guidance (DG), to balance efficiency and quality
respectively. Additionally, we build CNAS, the first public Chinese News Anchor
Speech dataset, to advance research and validation in this field. Extensive
experimental results and user studies demonstrate that PAHA significantly
outperforms existing methods in audio-motion alignment and video-related
evaluations. The codes and CNAS dataset will be released upon acceptance.

</details>


### [109] [Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack Detection](https://arxiv.org/abs/2505.03610)
*Fangling Jiang,Qi Li,Bing Liu,Weining Wang,Caifeng Shan,Zhenan Sun,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的基于知识的提示学习框架，用于3D面具呈现攻击检测，旨在利用视觉语言模型的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D面具攻击检测方法大多依赖多模态特征或rPPG信号，存在成本高、泛化能力有限的问题。文本描述信息廉价且通用，但视觉语言多模态特征在该领域的潜力尚未被充分挖掘。

Method: 提出一种基于知识的提示学习框架：1. 将知识图谱中的实体和三元组融入提示学习，生成细粒度、任务特定的显式提示。2. 引入基于注意力机制的视觉特定知识过滤器，根据视觉上下文筛选相关知识元素。3. 利用因果图理论，在训练中采用伪相关消除范式，移除与类别无关的局部图像补丁，学习与类别相关的泛化因果提示。

Result: 实验结果表明，所提出的方法在基准数据集上的场景内和跨场景检测性能均达到了最先进（SOTA）水平。

Conclusion: 该研究提出的基于知识的提示学习框架，通过有效利用视觉语言模型和知识图谱，显著提升了3D面具呈现攻击检测的泛化能力和性能。

Abstract: 3D mask presentation attack detection is crucial for protecting face
recognition systems against the rising threat of 3D mask attacks. While most
existing methods utilize multimodal features or remote photoplethysmography
(rPPG) signals to distinguish between real faces and 3D masks, they face
significant challenges, such as the high costs associated with multimodal
sensors and limited generalization ability. Detection-related text descriptions
offer concise, universal information and are cost-effective to obtain. However,
the potential of vision-language multimodal features for 3D mask presentation
attack detection remains unexplored. In this paper, we propose a novel
knowledge-based prompt learning framework to explore the strong generalization
capability of vision-language models for 3D mask presentation attack detection.
Specifically, our approach incorporates entities and triples from knowledge
graphs into the prompt learning process, generating fine-grained, task-specific
explicit prompts that effectively harness the knowledge embedded in pre-trained
vision-language models. Furthermore, considering different input images may
emphasize distinct knowledge graph elements, we introduce a visual-specific
knowledge filter based on an attention mechanism to refine relevant elements
according to the visual context. Additionally, we leverage causal graph theory
insights into the prompt learning process to further enhance the generalization
ability of our method. During training, a spurious correlation elimination
paradigm is employed, which removes category-irrelevant local image patches
using guidance from knowledge-based text features, fostering the learning of
generalized causal prompts that align with category-relevant local patches.
Experimental results demonstrate that the proposed method achieves
state-of-the-art intra- and cross-scenario detection performance on benchmark
datasets.

</details>


### [110] [Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using Only Real Face Images](https://arxiv.org/abs/2505.03611)
*Fangling Jiang,Qi Li,Weining Wang,Wei Shen,Bing Liu,Zhenan Sun*

Main category: cs.CV

TL;DR: 该论文提出一种仅使用真实人脸图像和视觉语言模型来学习未知欺骗提示的方法，以提高人脸反欺诈系统在不同场景和未知攻击类型下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 人脸反欺诈系统在不同场景下的泛化能力有限，主要归因于外部数据收集变化引起的协变量偏移和新兴攻击类型差异导致的语义偏移。

Method: 提出一种新方法，仅利用单一源域的真实人脸图像学习未知欺骗提示。该方法通过利用视觉语言模型（VLM）的通用知识为真实人脸和潜在未知欺骗攻击生成文本提示，并引入多样化欺骗提示优化框架。此框架约束未知欺骗提示在宽松的先验知识空间内，最大化其与真实人脸图像的距离，并强制不同欺骗提示间的语义独立性，以捕捉广泛的欺骗模式。

Result: 在九个数据集上的实验结果表明，学习到的提示能够有效传递视觉语言模型的知识，使得模型在不使用任何欺骗人脸图像的情况下，对跨未见目标域的各种未知攻击类型展现了最先进的泛化能力。

Conclusion: 该研究提出的方法通过学习未知欺骗提示，有效地提升了人脸反欺诈模型对未见攻击类型和目标域的泛化能力，且仅依赖真实人脸数据。

Abstract: Face anti-spoofing is a critical technology for ensuring the security of face
recognition systems. However, its ability to generalize across diverse
scenarios remains a significant challenge. In this paper, we attribute the
limited generalization ability to two key factors: covariate shift, which
arises from external data collection variations, and semantic shift, which
results from substantial differences in emerging attack types. To address both
challenges, we propose a novel approach for learning unknown spoof prompts,
relying solely on real face images from a single source domain. Our method
generates textual prompts for real faces and potential unknown spoof attacks by
leveraging the general knowledge embedded in vision-language models, thereby
enhancing the model's ability to generalize to unseen target domains.
Specifically, we introduce a diverse spoof prompt optimization framework to
learn effective prompts. This framework constrains unknown spoof prompts within
a relaxed prior knowledge space while maximizing their distance from real face
images. Moreover, it enforces semantic independence among different spoof
prompts to capture a broad range of spoof patterns. Experimental results on
nine datasets demonstrate that the learned prompts effectively transfer the
knowledge of vision-language models, enabling state-of-the-art generalization
ability against diverse unknown attack types across unseen target domains
without using any spoof face images.

</details>


### [111] [PhysLLM: Harnessing Large Language Models for Cross-Modal Remote Physiological Sensing](https://arxiv.org/abs/2505.03621)
*Yiping Xie,Bo Zhao,Mingtong Dai,Jian-Ping Zhou,Yue Sun,Tao Tan,Weicheng Xie,Linlin Shen,Zitong Yu*

Main category: cs.CV

TL;DR: 该论文介绍了 PhysLLM，一个结合大型语言模型 (LLM) 与遥感光电容积描记 (rPPG) 组件的框架，旨在解决光照变化和运动伪影等问题，从而提高非接触式生理测量的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的遥感光电容积描记 (rPPG) 技术易受光照变化、运动伪影影响，且时间序列建模能力有限。虽然大型语言模型 (LLM) 擅长捕捉长程依赖关系，但其以文本为中心的设计难以直接处理 rPPG 信号的连续性和噪声敏感性。本研究旨在弥合这一差距。

Method: 提出了 PhysLLM 框架，该框架协同优化 LLM 和领域特定的 rPPG 组件。具体方法包括：1) 文本原型指导 (TPG) 策略，将血流动力学特征投影到 LLM 可解释的语义空间以实现跨模态对齐；2) 双域平稳 (DDS) 算法，通过自适应时频域特征重加权解决信号不稳定性；3) 注入 rPPG 任务特定线索（生理统计、环境上下文问答、任务描述），利用跨模态学习整合视觉和文本信息，以适应挑战性场景。

Result: 在四个基准数据集上的评估结果表明，PhysLLM 达到了当前最先进的准确性和鲁棒性，并在不同的光照变化和运动场景下展现出优越的泛化能力。

Conclusion: PhysLLM 框架通过有效结合大型语言模型和领域特定的 rPPG 组件，显著提升了 rPPG 生理测量的准确性和鲁棒性，尤其在复杂多变的环境条件下表现出色。

Abstract: Remote photoplethysmography (rPPG) enables non-contact physiological
measurement but remains highly susceptible to illumination changes, motion
artifacts, and limited temporal modeling. Large Language Models (LLMs) excel at
capturing long-range dependencies, offering a potential solution but struggle
with the continuous, noise-sensitive nature of rPPG signals due to their
text-centric design. To bridge this gap, we introduce PhysLLM, a collaborative
optimization framework that synergizes LLMs with domain-specific rPPG
components. Specifically, the Text Prototype Guidance (TPG) strategy is
proposed to establish cross-modal alignment by projecting hemodynamic features
into LLM-interpretable semantic space, effectively bridging the
representational gap between physiological signals and linguistic tokens.
Besides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for
resolving signal instability through adaptive time-frequency domain feature
re-weighting. Finally, rPPG task-specific cues systematically inject
physiological priors through physiological statistics, environmental contextual
answering, and task description, leveraging cross-modal learning to integrate
both visual and textual information, enabling dynamic adaptation to challenging
scenarios like variable illumination and subject movements. Evaluation on four
benchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,
demonstrating superior generalization across lighting variations and motion
scenarios.

</details>


### [112] [Bounding Box-Guided Diffusion for Synthesizing Industrial Images and Segmentation Map](https://arxiv.org/abs/2505.03623)
*Alessandro Simoni,Francesco Pelosin*

Main category: cs.CV

TL;DR: 该研究提出了一种新的基于扩散模型的流程，用于以最少的监督生成高保真度的工业缺陷分割数据集。


<details>
  <summary>Details</summary>
Motivation: 工业应用中的计算机视觉合成数据集生成，特别是缺陷分割，尚未得到充分探索。获取高精度的缺陷标签既昂贵又耗时。

Method: 提出了一种基于扩散模型的管线，通过在丰富的边界框表示上对扩散模型进行条件化，以生成精确的分割掩码，从而实现真实且准确定位的缺陷合成。引入了两个定量指标来评估方法有效性及其对下游分割任务的影响。

Result: 与现有的布局条件生成方法相比，该方法改善了缺陷的一致性和空间准确性。基于扩散的合成能够弥合人工和真实工业数据之间的差距，促进更可靠和经济高效的分割模型。

Conclusion: 基于扩散的合成方法可以有效地生成高保真工业数据集，有助于开发更可靠、更经济的分割模型，缩小人工数据与真实世界工业数据之间的差距。

Abstract: Synthetic dataset generation in Computer Vision, particularly for industrial
applications, is still underexplored. Industrial defect segmentation, for
instance, requires highly accurate labels, yet acquiring such data is costly
and time-consuming. To address this challenge, we propose a novel
diffusion-based pipeline for generating high-fidelity industrial datasets with
minimal supervision. Our approach conditions the diffusion model on enriched
bounding box representations to produce precise segmentation masks, ensuring
realistic and accurately localized defect synthesis. Compared to existing
layout-conditioned generative methods, our approach improves defect consistency
and spatial accuracy. We introduce two quantitative metrics to evaluate the
effectiveness of our method and assess its impact on a downstream segmentation
task trained on real and synthetic data. Our results demonstrate that
diffusion-based synthesis can bridge the gap between artificial and real-world
industrial data, fostering more reliable and cost-efficient segmentation
models. The code is publicly available at
https://github.com/covisionlab/diffusion_labeling.

</details>


### [113] [Breaking Annotation Barriers: Generalized Video Quality Assessment via Ranking-based Self-Supervision](https://arxiv.org/abs/2505.03631)
*Linhan Cao,Wei Sun,Kaiwei Zhang,Yicong Peng,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 该研究提出了一种自监督学习框架，利用大规模无标签网络视频训练视频质量评估模型，通过学习排序和迭代自我改进策略，实现了优越的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 传统的监督式视频质量评估（VQA）模型严重依赖人工标注数据，这种方式成本高、耗时长且难以扩展，从而限制了模型对未知视频内容和失真的泛化能力。

Method: 研究提出了一种自监督学习框架。该方法采用“学习排序”（learning-to-rank）范式来训练一个大型多模态模型（LMM）。训练数据通过两种方式自动标记：1) 利用现有VQA模型进行质量伪标签标注；2) 基于合成失真模拟进行相对质量排序。此外，引入了一种“迭代自我改进训练策略”，即训练好的模型会反过来作为更优的标注器，迭代地提升训练数据的标注质量。

Result: 通过在比现有VQA基准大10倍的数据集上进行训练，该模型：1) 在领域内VQA基准测试中取得了与监督模型相当甚至更好的零样本（zero-shot）性能；2) 在处理不同视频内容和失真类型时表现出更强的分布外（OOD）泛化能力；3) 在使用人工标注数据集进行微调后，达到了当前最先进（SOTA）的水平。

Conclusion: 该自监督学习方法能够有效训练出具有良好泛化能力的视频质量评估模型，为解决VQA领域数据稀缺问题提供了新途径。

Abstract: Video quality assessment (VQA) is essential for quantifying perceptual
quality in various video processing workflows, spanning from camera capture
systems to over-the-top streaming platforms. While recent supervised VQA models
have made substantial progress, the reliance on manually annotated datasets --
a process that is labor-intensive, costly, and difficult to scale up -- has
hindered further optimization of their generalization to unseen video content
and distortions. To bridge this gap, we introduce a self-supervised learning
framework for VQA to learn quality assessment capabilities from large-scale,
unlabeled web videos. Our approach leverages a \textbf{learning-to-rank}
paradigm to train a large multimodal model (LMM) on video pairs automatically
labeled via two manners, including quality pseudo-labeling by existing VQA
models and relative quality ranking based on synthetic distortion simulations.
Furthermore, we introduce a novel \textbf{iterative self-improvement training
strategy}, where the trained model acts an improved annotator to iteratively
refine the annotation quality of training data. By training on a dataset
$10\times$ larger than the existing VQA benchmarks, our model: (1) achieves
zero-shot performance on in-domain VQA benchmarks that matches or surpasses
supervised models; (2) demonstrates superior out-of-distribution (OOD)
generalization across diverse video content and distortions; and (3) sets a new
state-of-the-art when fine-tuned on human-labeled datasets. Extensive
experimental results validate the effectiveness of our self-supervised approach
in training generalized VQA models. The datasets and code will be publicly
released to facilitate future research.

</details>


### [114] [Towards Smart Point-and-Shoot Photography](https://arxiv.org/abs/2505.03638)
*Jiawan Li,Fei Zhou,Zhipeng Zhong,Jiongzhi Lin,Guoping Qiu*

Main category: cs.CV

TL;DR: 本文提出了一种智能傻瓜相机 (SPAS) 系统，通过实时指导用户调整相机姿态来帮助拍摄构图良好的照片。


<details>
  <summary>Details</summary>
Motivation: 大多数智能手机用户缺乏专业摄影技巧以拍出构图良好的照片，而传统傻瓜相机无法提供构图指导。

Method: 首先构建了一个包含32万张图像和相机姿态信息的大型数据集。然后开发了基于CLIP的构图质量评估 (CCQA) 模型为图像分配伪标签，该模型使用可学习文本嵌入技术区分细微的质量差异。最后，开发了一个相机姿态调整模型 (CPAM)，它先判断当前视图是否可改进，若可则输出相机姿态调整角度建议，并采用门控损失函数的混合专家模型进行端到端训练。

Result: 将使用公开的图像构图数据集展示该SPAS系统的性能。

Conclusion: 本文成功开发了一种新颖的SPAS系统，能够通过实时相机姿态调整指导，帮助用户改善照片构图。

Abstract: Hundreds of millions of people routinely take photos using their smartphones
as point and shoot (PAS) cameras, yet very few would have the photography
skills to compose a good shot of a scene. While traditional PAS cameras have
built-in functions to ensure a photo is well focused and has the right
brightness, they cannot tell the users how to compose the best shot of a scene.
In this paper, we present a first of its kind smart point and shoot (SPAS)
system to help users to take good photos. Our SPAS proposes to help users to
compose a good shot of a scene by automatically guiding the users to adjust the
camera pose live on the scene. We first constructed a large dataset containing
320K images with camera pose information from 4000 scenes. We then developed an
innovative CLIP-based Composition Quality Assessment (CCQA) model to assign
pseudo labels to these images. The CCQA introduces a unique learnable text
embedding technique to learn continuous word embeddings capable of discerning
subtle visual quality differences in the range covered by five levels of
quality description words {bad, poor, fair, good, perfect}. And finally we have
developed a camera pose adjustment model (CPAM) which first determines if the
current view can be further improved and if so it outputs the adjust suggestion
in the form of two camera pose adjustment angles. The two tasks of CPAM make
decisions in a sequential manner and each involves different sets of training
samples, we have developed a mixture-of-experts model with a gated loss
function to train the CPAM in an end-to-end manner. We will present extensive
results to demonstrate the performances of our SPAS system using publicly
available image composition datasets.

</details>


### [115] [ReGraP-LLaVA: Reasoning enabled Graph-based Personalized Large Language and Vision Assistant](https://arxiv.org/abs/2505.03654)
*Yifan Xiang,Zhenxi Zhang,Bin Li,Yixuan Weng,Shoujun Zhou,Yangfan He,Keqin Li*

Main category: cs.CV

TL;DR: 该研究提出新数据集ReGraP和模型ReGraP-LLaVA，旨在提升个性化多模态大语言模型（MLLM）在识别用户特定概念的基础上，进一步进行对象间的关系推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在个性化多模态大语言模型中存在局限：1. 训练数据缺乏可学习对象间关系的多对象集；2. 模型忽略了个性化概念间的关系且无法进行推理；3. 实验主要关注单一概念的识别和描述，未能评估关系推理能力。

Method: 1. 提出了新的数据集ReGraP，包含图像、知识图谱（KG）和基于知识图谱的思维链（CoT）问答对。2. 提出了ReGraP-LLaVA模型，使用知识图谱和思维链问答对进行训练，并设计了软、硬图提示方法以对齐知识图谱与模型语义空间。3. 构建了ReGraP基准测试，包含多种任务类型以评估关系推理能力。

Result: 实验结果表明，提出的ReGraP-LLaVA模型不仅能学习个性化知识，还能在其响应中执行关系推理，并在与竞争方法的比较中达到了当前最佳（SOTA）性能。

Conclusion: 该研究通过新提出的数据集ReGraP、模型ReGraP-LLaVA及基准测试，成功提升了个性化MLLM的关系推理和知识连接能力，解决了现有方法在处理对象间关系方面的不足。

Abstract: Recent advances in personalized MLLMs enable effective capture of
user-specific concepts, supporting both recognition of personalized concepts
and contextual captioning. However, humans typically explore and reason over
relations among objects and individuals, transcending surface-level information
to achieve more personalized and contextual understanding. To this end,
existing methods may face three main limitations: Their training data lacks
multi-object sets in which relations among objects are learnable. Building on
the limited training data, their models overlook the relations between
different personalized concepts and fail to reason over them. Their experiments
mainly focus on a single personalized concept, where evaluations are limited to
recognition and captioning tasks. To address the limitations, we present a new
dataset named ReGraP, consisting of 120 sets of personalized knowledge. Each
set includes images, KGs, and CoT QA pairs derived from the KGs, enabling more
structured and sophisticated reasoning pathways. We propose ReGraP-LLaVA, an
MLLM trained with the corresponding KGs and CoT QA pairs, where soft and hard
graph prompting methods are designed to align KGs within the model's semantic
space. We establish the ReGraP Benchmark, which contains diverse task types:
multiple-choice, fill-in-the-blank, True/False, and descriptive questions in
both open- and closed-ended settings. The proposed benchmark is designed to
evaluate the relational reasoning and knowledge-connection capability of
personalized MLLMs. We conduct experiments on the proposed ReGraP-LLaVA and
other competitive MLLMs. Results show that the proposed model not only learns
personalized knowledge but also performs relational reasoning in responses,
achieving the SoTA performance compared with the competitive methods. All the
codes and datasets are released at: https://github.com/xyfyyds/ReGraP.

</details>


### [116] [Revolutionizing Brain Tumor Imaging: Generating Synthetic 3D FA Maps from T1-Weighted MRI using CycleGAN Models](https://arxiv.org/abs/2505.03662)
*Xin Du,Francesca M. Cozzi,Rajesh Jena*

Main category: cs.CV

TL;DR: 提出一种基于CycleGAN的方法，从T1加权MRI直接生成分数各向异性（FA）图，以解决FA图与纤维束追踪图谱的空间错位问题，尤其在肿瘤组织中表现稳健。


<details>
  <summary>Details</summary>
Motivation: FA图与纤维束追踪图谱间的空间错位阻碍了它们在评估白质完整性和结构连接性方面的有效整合，特别是在预测模型中。

Method: 采用基于CycleGAN的方法，使用非配对数据训练模型，直接从T1加权MRI扫描生成FA图。

Result: 模型成功生成了高保真度的FA图，通过结构相似性指数（SSIM）和峰值信噪比（PSNR）评估，在肿瘤区域表现出特别稳健的性能。放射学评估显示该模型有潜力通过提供AI驱动的替代方案来减少额外扫描，从而增强临床工作流程。

Conclusion: 该研究提出的基于CycleGAN的AI驱动方法能够从T1加权MRI生成FA图，有望减少额外扫描的必要性，并提高临床应用的潜力，尤其是在肿瘤区域。

Abstract: Fractional anisotropy (FA) and directionally encoded colour (DEC) maps are
essential for evaluating white matter integrity and structural connectivity in
neuroimaging. However, the spatial misalignment between FA maps and
tractography atlases hinders their effective integration into predictive
models. To address this issue, we propose a CycleGAN based approach for
generating FA maps directly from T1-weighted MRI scans, representing the first
application of this technique to both healthy and tumour-affected tissues. Our
model, trained on unpaired data, produces high fidelity maps, which have been
rigorously evaluated using Structural Similarity Index (SSIM) and Peak
Signal-to-Noise Ratio (PSNR), demonstrating particularly robust performance in
tumour regions. Radiological assessments further underscore the model's
potential to enhance clinical workflows by providing an AI-driven alternative
that reduces the necessity for additional scans.

</details>


### [117] [Distribution-Conditional Generation: From Class Distribution to Creative Generation](https://arxiv.org/abs/2505.03667)
*Fu Feng,Yucheng Xie,Xu Yang,Jing Wang,Xin Geng*

Main category: cs.CV

TL;DR: 提出了一种名为 DisTok 的新方法，通过对类别分布进行条件化，使文本到图像（T2I）模型能够生成超越训练数据分布的、真正新颖的视觉概念。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像（T2I）扩散模型在生成语义对齐的图像方面很有效，但它们依赖于训练数据分布，这限制了其合成真正新颖、超出分布范围（out-of-distribution）概念的能力。现有方法通常通过组合已知概念来增强创造力，但这仍然局限于现有语义空间。

Method: 提出了“分布条件生成”（Distribution-Conditional Generation）的新公式，将创造力建模为以类别分布为条件的图像合成。在此基础上，提出了 DisTok 框架：一个编码器-解码器模型，将类别分布映射到潜空间，并解码为创意概念的标记（token）。DisTok 维护一个动态概念池，通过迭代采样和融合概念对，生成与日益复杂的类别分布对齐的标记。通过视觉-语言模型预测生成图像的类别分布，来监督输入分布与生成标记视觉语义之间的一致性。

Result: 大量实验表明，DisTok 通过统一分布条件融合和基于采样的合成，实现了高效灵活的标记级生成，达到了业界领先的性能，并在文本-图像对齐和人类偏好评分方面表现优越。

Conclusion: DisTok 方法能够有效地生成新颖、超出分布范围的视觉概念，通过以类别分布为条件进行图像合成，提升了T2I模型的创造力和灵活性。

Abstract: Text-to-image (T2I) diffusion models are effective at producing semantically
aligned images, but their reliance on training data distributions limits their
ability to synthesize truly novel, out-of-distribution concepts. Existing
methods typically enhance creativity by combining pairs of known concepts,
yielding compositions that, while out-of-distribution, remain linguistically
describable and bounded within the existing semantic space. Inspired by the
soft probabilistic outputs of classifiers on ambiguous inputs, we propose
Distribution-Conditional Generation, a novel formulation that models creativity
as image synthesis conditioned on class distributions, enabling semantically
unconstrained creative generation. Building on this, we propose DisTok, an
encoder-decoder framework that maps class distributions into a latent space and
decodes them into tokens of creative concept. DisTok maintains a dynamic
concept pool and iteratively sampling and fusing concept pairs, enabling the
generation of tokens aligned with increasingly complex class distributions. To
enforce distributional consistency, latent vectors sampled from a Gaussian
prior are decoded into tokens and rendered into images, whose class
distributions-predicted by a vision-language model-supervise the alignment
between input distributions and the visual semantics of generated tokens. The
resulting tokens are added to the concept pool for subsequent composition.
Extensive experiments demonstrate that DisTok, by unifying
distribution-conditioned fusion and sampling-based synthesis, enables efficient
and flexible token-level generation, achieving state-of-the-art performance
with superior text-image alignment and human preference scores.

</details>


### [118] [CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point Cloud Fusion and Zero-Shot Image Inpainting](https://arxiv.org/abs/2505.03679)
*Huawei Sun,Bora Kunter Sahin,Georg Stettinger,Maximilian Bernhard,Matthias Schubert,Robert Wille*

Main category: cs.CV

TL;DR: 该研究提出了一种新的摄像头-雷达融合框架，通过扩散模型和雷达辅助生成的伪掩码来增强恶劣天气下的语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 摄像头在恶劣天气下易受影响，而雷达数据稀疏且有噪声。融合两种传感器信息能提高自动驾驶和机器人对环境感知的鲁棒性。

Method: 提出一个新框架，将扩散模型集成到摄像头-雷达融合架构中。利用雷达点特征和Segment-Anything模型生成伪掩码，并使用噪声降低单元对伪掩码去噪，最终生成修复图像以补全原始图像信息。

Result: 在Waterscenes数据集上，该方法使仅摄像头分割基线的mIoU提升了2.63%，并使摄像头-雷达融合架构的mIoU提升了1.48%。

Conclusion: 该方法证明了在恶劣天气条件下，通过摄像头-雷达融合进行语义分割的有效性。

Abstract: Segmenting objects in an environment is a crucial task for autonomous driving
and robotics, as it enables a better understanding of the surroundings of each
agent. Although camera sensors provide rich visual details, they are vulnerable
to adverse weather conditions. In contrast, radar sensors remain robust under
such conditions, but often produce sparse and noisy data. Therefore, a
promising approach is to fuse information from both sensors. In this work, we
propose a novel framework to enhance camera-only baselines by integrating a
diffusion model into a camera-radar fusion architecture. We leverage radar
point features to create pseudo-masks using the Segment-Anything model,
treating the projected radar points as point prompts. Additionally, we propose
a noise reduction unit to denoise these pseudo-masks, which are further used to
generate inpainted images that complete the missing information in the original
images. Our method improves the camera-only segmentation baseline by 2.63% in
mIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the
Waterscenes dataset. This demonstrates the effectiveness of our approach for
semantic segmentation using camera-radar fusion under adverse weather
conditions.

</details>


### [119] [Matching Distance and Geometric Distribution Aided Learning Multiview Point Cloud Registration](https://arxiv.org/abs/2505.03692)
*Shiqi Li,Jihua Zhu,Yifan Xie,Naiwen Hu,Di Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的多视图点云配准新方法，通过特定设计的神经网络改进位姿图构建和运动同步，以提高配准的可靠性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有位姿图构建方法（如修剪全连接图或使用聚合局部特征的稀疏图）可能无法稳定产生可靠结果；同时，运动同步方法依赖于优化不精确的手工设计损失函数。

Method: 1. 设计了一个网络模型，从点云对之间的匹配距离中提取信息，以识别可靠的点云对进行位姿图构建。 2. 提出了另一个神经网络模型，以数据驱动的方式计算绝对位姿进行运动同步，该模型考虑了几何分布信息并采用改进的注意力机制。

Result: 在多个室内和室外数据集上的实验结果证实了所提方法的有效性和良好的泛化能力。

Conclusion: 本文提出的方法通过数据驱动的网络模型改进了多视图点云配准中的位姿图构建和运动同步，有效提升了配准的可靠性和准确性。

Abstract: Multiview point cloud registration plays a crucial role in robotics,
automation, and computer vision fields. This paper concentrates on pose graph
construction and motion synchronization within multiview registration. Previous
methods for pose graph construction often pruned fully connected graphs or
constructed sparse graph using global feature aggregated from local
descriptors, which may not consistently yield reliable results. To identify
dependable pairs for pose graph construction, we design a network model that
extracts information from the matching distance between point cloud pairs. For
motion synchronization, we propose another neural network model to calculate
the absolute pose in a data-driven manner, rather than optimizing inaccurate
handcrafted loss functions. Our model takes into account geometric distribution
information and employs a modified attention mechanism to facilitate flexible
and reliable feature interaction. Experimental results on diverse indoor and
outdoor datasets confirm the effectiveness and generalizability of our
approach. The source code is available at https://github.com/Shi-Qi-Li/MDGD.

</details>


### [120] [Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning](https://arxiv.org/abs/2505.03703)
*François Role,Sébastien Meyer,Victor Amblard*

Main category: cs.CV

TL;DR: 该研究提出了新的度量和方法（基于谱方法和最优传输）来评估并缩小视觉语言模型中的模态鸿沟，从而改善下游多模态任务的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）中存在的模态鸿沟（图像和文本嵌入在共享空间中分离）对多模态检索、聚类和零样本分类等下游任务不利，且目前缺乏通用方法来精确评估和减少此鸿沟。

Method: 提出了新的度量指标，以及基于谱方法（spectral-based methods）和最优传输（optimal transport-based methods）的技术来评估和缩小模态鸿沟。

Result: 在多个图文数据集和模型上进行的广泛实验表明，所提出的度量和技术是有效的，并且对下游任务产生了积极影响。

Conclusion: 本研究提出的新型度量和有效技术能够评估并缩小视觉语言模型中的模态鸿沟，从而改善模型在多模态任务上的表现。

Abstract: Vision-language models (VLMs) allow to embed texts and images in a shared
representation space. However, it has been shown that these models are subject
to a modality gap phenomenon meaning there exists a clear separation between
the embeddings from one modality and another in the embedding space. While this
misalignment is detrimental for downstream tasks such as multimodal retrieval,
multimodal clustering or zero-shot classification, etc. no generic and
practical methods have so far been proposed to assess it precisely and even
reduce it. We therefore propose novel measures and effective techniques
(spectral- and optimal transport-based methods) to achieve this goal. Extensive
experiments conducted on several image-text datasets and models demonstrate
their effectiveness and beneficial effects on downstream tasks. Our code is
available at the URL provided in the paper's abstract.

</details>


### [121] [DISARM++: Beyond scanner-free harmonization](https://arxiv.org/abs/2505.03715)
*Luca Caldera,Lara Cavinato,Alessio Cirone,Isabella Cama,Sara Garbarino,Raffaele Lodi,Fabrizio Tagliavini,Anna Nigri,Silvia De Francesco,Andrea Cappozzo,Michele Piana,Francesca Ieva*

Main category: cs.CV

TL;DR: 本研究提出了一种新颖的直接T1加权MR图像协调方法，旨在确保跨不同扫描仪的神经影像研究的一致性和特征可靠性，该方法在多项应用中表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 不同扫描仪产生的T1加权MR图像存在差异，影响神经影像研究的一致性。本研究旨在通过直接图像协调，超越特征标准化，确保提取特征对于下游分析的固有可靠性。

Method: 提出一种新颖的直接图像协调方法，支持两种图像转换方式：1) 将图像映射到无扫描仪空间以实现统一外观；2) 将图像转换到特定训练扫描仪的域。该方法对训练中未包含的未知扫描仪也具有强泛化能力，且无需颅骨去除等大量预处理，也无需为新数据集重新训练。

Result: 该方法在健康对照组、旅行受试者和阿尔茨海默病（AD）患者等不同队列的MR图像上得到验证。在脑龄预测（R2 = 0.60 ± 0.05）、生物标志物提取、AD分类（测试准确率 = 0.86 ± 0.03）和诊断预测（AUC = 0.95）等多个应用中均显示出有效性，且在可靠性和预测准确性方面均优于现有顶尖方法。

Conclusion: 该协调方法通过确保扫描仪不变的图像质量，为改进不同环境下的神经影像研究提供了一个强大且高效的解决方案，特别适用于需要全头部分析的应用，并能顺利集成到各种神经影像工作流程中。

Abstract: Harmonization of T1-weighted MR images across different scanners is crucial
for ensuring consistency in neuroimaging studies. This study introduces a novel
approach to direct image harmonization, moving beyond feature standardization
to ensure that extracted features remain inherently reliable for downstream
analysis. Our method enables image transfer in two ways: (1) mapping images to
a scanner-free space for uniform appearance across all scanners, and (2)
transforming images into the domain of a specific scanner used in model
training, embedding its unique characteristics. Our approach presents strong
generalization capability, even for unseen scanners not included in the
training phase. We validated our method using MR images from diverse cohorts,
including healthy controls, traveling subjects, and individuals with
Alzheimer's disease (AD). The model's effectiveness is tested in multiple
applications, such as brain age prediction (R2 = 0.60 \pm 0.05), biomarker
extraction, AD classification (Test Accuracy = 0.86 \pm 0.03), and diagnosis
prediction (AUC = 0.95). In all cases, our harmonization technique outperforms
state-of-the-art methods, showing improvements in both reliability and
predictive accuracy. Moreover, our approach eliminates the need for extensive
preprocessing steps, such as skull-stripping, which can introduce errors by
misclassifying brain and non-brain structures. This makes our method
particularly suitable for applications that require full-head analysis,
including research on head trauma and cranial deformities. Additionally, our
harmonization model does not require retraining for new datasets, allowing
smooth integration into various neuroimaging workflows. By ensuring
scanner-invariant image quality, our approach provides a robust and efficient
solution for improving neuroimaging studies across diverse settings. The code
is available at this link.

</details>


### [122] [FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios](https://arxiv.org/abs/2505.03730)
*Shiyi Zhang,Junhao Zhuang,Zhaoyang Zhang,Ying Shan,Yansong Tang*

Main category: cs.CV

TL;DR: 提出FlexiAct模型，用于将参考视频中的动作迁移到任意目标图像，克服了现有方法在空间结构（布局、骨架、视角）上的严格限制，同时保持身份一致性。


<details>
  <summary>Details</summary>
Motivation: 当前的动作定制方法在空间结构（如布局、骨架和视角一致性）方面受到严格限制，这降低了它们在不同主体和场景中的适应性。

Method: 提出了FlexiAct模型，该模型能够将参考视频中的动作迁移到任意目标图像。其核心包括：1) RefAdapter，一个轻量级的图像条件适配器，用于空间适应和一致性保留；2) FAE (频率感知动作提取)，在去噪过程中直接提取动作，考虑不同时间步对运动和外观细节的关注度差异。

Result: 实验证明，FlexiAct能够有效地将动作迁移到具有不同布局、骨骼结构和视角的主体上，并在外观一致性和结构灵活性之间取得了优于现有方法的平衡。

Conclusion: FlexiAct通过引入RefAdapter和FAE，成功实现了在保持身份一致性的同时，将动作灵活迁移到多样化的目标图像，解决了现有方法在空间结构适应性方面的局限性。

Abstract: Action customization involves generating videos where the subject performs
actions dictated by input control signals. Current methods use pose-guided or
global motion customization but are limited by strict constraints on spatial
structure, such as layout, skeleton, and viewpoint consistency, reducing
adaptability across diverse subjects and scenarios. To overcome these
limitations, we propose FlexiAct, which transfers actions from a reference
video to an arbitrary target image. Unlike existing methods, FlexiAct allows
for variations in layout, viewpoint, and skeletal structure between the subject
of the reference video and the target image, while maintaining identity
consistency. Achieving this requires precise action control, spatial structure
adaptation, and consistency preservation. To this end, we introduce RefAdapter,
a lightweight image-conditioned adapter that excels in spatial adaptation and
consistency preservation, surpassing existing methods in balancing appearance
consistency and structural flexibility. Additionally, based on our
observations, the denoising process exhibits varying levels of attention to
motion (low frequency) and appearance details (high frequency) at different
timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike
existing methods that rely on separate spatial-temporal architectures, directly
achieves action extraction during the denoising process. Experiments
demonstrate that our method effectively transfers actions to subjects with
diverse layouts, skeletons, and viewpoints. We release our code and model
weights to support further research at
https://shiyi-zh0408.github.io/projectpages/FlexiAct/

</details>


### [123] [Multi-Agent System for Comprehensive Soccer Understanding](https://arxiv.org/abs/2505.03735)
*Jiayuan Rao,Zifeng Li,Haoning Wu,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TL;DR: 论文提出了一个全面的足球理解框架，包含知识库SoccerWiki、基准SoccerBench和一个多智能体系统SoccerAgent，该系统在基准测试中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有的AI驱动的足球理解研究主要集中于孤立或狭窄的任务，缺乏整体性。

Method: 构建了大规模多模态足球知识库SoccerWiki；提出了包含约1万个多模态问答对的足球基准SoccerBench；并引入了一个名为SoccerAgent的新型多智能体系统，该系统利用SoccerWiki进行协作推理来分解复杂足球问题。

Result: 在SoccerBench上对最先进的多模态大语言模型（MLLM）进行了广泛评估和消融研究，结果表明论文提出的多智能体系统（SoccerAgent）表现更优越。

Conclusion: 该研究通过构建SoccerWiki、SoccerBench和SoccerAgent，为整体足球理解提供了新的解决方案，并证明了其提出的多智能体系统在利用领域知识进行复杂推理方面的有效性和优越性。

Abstract: Recent advancements in AI-driven soccer understanding have demonstrated rapid
progress, yet existing research predominantly focuses on isolated or narrow
tasks. To bridge this gap, we propose a comprehensive framework for holistic
soccer understanding. Specifically, we make the following contributions in this
paper: (i) we construct SoccerWiki, the first large-scale multimodal soccer
knowledge base, integrating rich domain knowledge about players, teams,
referees, and venues to enable knowledge-driven reasoning; (ii) we present
SoccerBench, the largest and most comprehensive soccer-specific benchmark,
featuring around 10K standardized multimodal (text, image, video) multi-choice
QA pairs across 13 distinct understanding tasks, curated through automated
pipelines and manual verification; (iii) we introduce SoccerAgent, a novel
multi-agent system that decomposes complex soccer questions via collaborative
reasoning, leveraging domain expertise from SoccerWiki and achieving robust
performance; (iv) extensive evaluations and ablations that benchmark
state-of-the-art MLLMs on SoccerBench, highlighting the superiority of our
proposed agentic system. All data and code are publicly available at:
https://jyrao.github.io/SoccerAgent/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [124] [Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach](https://arxiv.org/abs/2505.02952)
*Fabrizio Marozzo*

Main category: cs.AI

TL;DR: 该研究提出一种迭代方法，通过澄清问题和备选方案建议来减少自然语言提示的模糊性，从而提高生成式AI的输出精度和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 自然语言固有的模糊性导致生成式AI的指令不精确，用户需要反复测试和修正提示。

Method: 采用一种迭代方法，通过结构化的澄清问题序列和备选解决方案（辅以输入/输出示例）来系统性地缩小模糊性，待所有不确定性解决后生成最终精确方案。

Result: 在编码、数据分析和创意写作等多样化数据集上的评估表明，该方法相较于传统单次解决方案，在准确性、解决时间和用户满意度方面表现更优。

Conclusion: 所提出的迭代方法能有效解决自然语言的模糊性问题，生成更精确的解决方案，并提升用户体验。

Abstract: Generative AI systems have revolutionized human interaction by enabling
natural language-based coding and problem solving. However, the inherent
ambiguity of natural language often leads to imprecise instructions, forcing
users to iteratively test, correct, and resubmit their prompts. We propose an
iterative approach that systematically narrows down these ambiguities through a
structured series of clarification questions and alternative solution
proposals, illustrated with input/output examples as well. Once every
uncertainty is resolved, a final, precise solution is generated. Evaluated on a
diverse dataset spanning coding, data analysis, and creative writing, our
method demonstrates superior accuracy, competitive resolution times, and higher
user satisfaction compared to conventional one-shot solutions, which typically
require multiple manual iterations to achieve a correct output.

</details>


### [125] [The Multimodal Paradox: How Added and Missing Modalities Shape Bias and Performance in Multimodal AI](https://arxiv.org/abs/2505.03020)
*Kishore Sampath,Pratheesh,Ayaazuddin Mohammad,Resmi Ramachandranpillai*

Main category: cs.AI

TL;DR: 该论文研究了在多模态学习中，增加新模态或在推理时缺失模态对模型性能和公平性的影响，特别关注医疗保健领域。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态学习在提升决策任务性能方面表现优越，但其对偏见和鲁棒性的影响常被忽视。本研究旨在探究这些被忽视的方面。

Method: 通过在包含图像、时间序列和结构化信息的多模态医疗保健数据集上进行广泛实验，研究了两个关键问题：1) 增加新模态如何影响模型性能和公平性（减轻或加剧偏见）；2) 推理时缺失模态对模型性能和公平性的影响。

Result: 研究发现，训练中加入新模态能持续提升模型性能，但对公平性的影响因评估指标和数据集而异。此外，推理时模态的缺失会导致性能和公平性下降。

Conclusion: 结论指出，尽管增加模态能提升性能，但其对公平性的影响需仔细评估。推理时模态缺失会损害性能和公平性，对模型在实际应用中的鲁棒性提出了挑战。

Abstract: Multimodal learning, which integrates diverse data sources such as images,
text, and structured data, has proven superior to unimodal counterparts in
high-stakes decision-making. However, while performance gains remain the gold
standard for evaluating multimodal systems, concerns around bias and robustness
are frequently overlooked. In this context, this paper explores two key
research questions (RQs): (i) RQ1 examines whether adding a modality
con-sistently enhances performance and investigates its role in shaping
fairness measures, assessing whether it mitigates or amplifies bias in
multimodal models; (ii) RQ2 investigates the impact of missing modalities at
inference time, analyzing how multimodal models generalize in terms of both
performance and fairness. Our analysis reveals that incorporating new
modalities during training consistently enhances the performance of multimodal
models, while fairness trends exhibit variability across different evaluation
measures and datasets. Additionally, the absence of modalities at inference
degrades performance and fairness, raising concerns about its robustness in
real-world deployment. We conduct extensive experiments using multimodal
healthcare datasets containing images, time series, and structured information
to validate our findings.

</details>


### [126] [Evaluating the Impact of AI-Powered Audiovisual Personalization on Learner Emotion, Focus, and Learning Outcomes](https://arxiv.org/abs/2505.03033)
*George Xi Wang,Jingying Deng,Safinah Ali*

Main category: cs.AI

TL;DR: 该研究提出一种基于大语言模型（LLM）的人工智能系统，用于生成个性化的多感官学习环境，以帮助自主学习者提高专注度和情绪稳定性。


<details>
  <summary>Details</summary>
Motivation: 自主学习者在非结构化环境中常难以保持专注和情绪稳定，现有辅助工具整合性差，且当前教育技术主要关注内容适应与反馈，忽视了学习的情感和感官背景。大语言模型在生成个性化视听学习环境方面的潜力尚未得到充分开发。

Method: 引入一个AI驱动的系统，使用大语言模型生成个性化的多感官学习环境。用户可以选择或生成定制化的视觉主题和听觉元素。通过结合生物特征测量和绩效结果的混合方法设计，研究个性化视听元素组合对学习者认知负荷和参与度的影响。

Result: 该研究评估由大语言模型驱动的感官个性化在减少分心、增强情绪稳定，以及对学习者认知负荷和参与度影响方面的有效性。

Conclusion: 研究旨在推动情感响应式教育技术的发展，并将多模态大语言模型的应用扩展到自主学习的感官维度。

Abstract: Independent learners often struggle with sustaining focus and emotional
regulation in unstructured or distracting settings. Although some rely on
ambient aids such as music, ASMR, or visual backgrounds to support
concentration, these tools are rarely integrated into cohesive,
learner-centered systems. Moreover, existing educational technologies focus
primarily on content adaptation and feedback, overlooking the emotional and
sensory context in which learning takes place. Large language models have
demonstrated powerful multimodal capabilities including the ability to generate
and adapt text, audio, and visual content. Educational research has yet to
fully explore their potential in creating personalized audiovisual learning
environments. To address this gap, we introduce an AI-powered system that uses
LLMs to generate personalized multisensory study environments. Users select or
generate customized visual themes (e.g., abstract vs. realistic, static vs.
animated) and auditory elements (e.g., white noise, ambient ASMR, familiar vs.
novel sounds) to create immersive settings aimed at reducing distraction and
enhancing emotional stability. Our primary research question investigates how
combinations of personalized audiovisual elements affect learner cognitive load
and engagement. Using a mixed-methods design that incorporates biometric
measures and performance outcomes, this study evaluates the effectiveness of
LLM-driven sensory personalization. The findings aim to advance emotionally
responsive educational technologies and extend the application of multimodal
LLMs into the sensory dimension of self-directed learning.

</details>


### [127] [BLAB: Brutally Long Audio Bench](https://arxiv.org/abs/2505.03054)
*Orevaoghene Ahia,Martijn Bartelds,Kabir Ahuja,Hila Gonen,Valentin Hofmann,Siddhant Arora,Shuyue Stella Li,Vishal Puttagunta,Mofetoluwa Adeyemi,Charishma Buchireddy,Ben Walls,Noah Bennett,Shinji Watanabe,Noah A. Smith,Yulia Tsvetkov,Sachin Kumar*

Main category: cs.AI

TL;DR: 论文引入了BLAB，一个评估大型音频语言模型（LMs）在长篇音频（平均51分钟）理解任务上表现的基准测试，发现现有先进模型均表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有音频语言模型评估主要针对短音频片段（<30秒），缺乏对更接近自然用户交互的长篇对话语音的评估，难以全面了解模型在真实场景下的能力。

Method: 引入“残酷长音频基准”（Brutally Long Audio Bench, BLAB），包含833+小时、平均长度51分钟的长音频片段及对应的人工标注问答对，评估任务包括定位、时长估计、情感和计数。研究团队在此基准上评估了六种开源和专有音频语言模型。

Result: 所有被评估的音频语言模型，包括Gemini 2.0 Pro和GPT-4o等先进模型，在BLAB任务上均表现不佳。模型性能随音频时长增加而下降，在定位、时间推理、计数和理解非音位信息方面尤其困难，且更依赖提示而非音频内容。

Conclusion: 当前的音频语言模型在处理长篇音频方面存在显著不足。BLAB提供了一个具有挑战性的评估框架，旨在促进开发具备更强长篇音频理解能力的模型。

Abstract: Developing large audio language models (LMs) capable of understanding diverse
spoken interactions is essential for accommodating the multimodal nature of
human communication and can increase the accessibility of language technologies
across different user populations. Recent work on audio LMs has primarily
evaluated their performance on short audio segments, typically under 30
seconds, with limited exploration of long-form conversational speech segments
that more closely reflect natural user interactions with these models. We
introduce Brutally Long Audio Bench (BLAB), a challenging long-form audio
benchmark that evaluates audio LMs on localization, duration estimation,
emotion, and counting tasks using audio segments averaging 51 minutes in
length. BLAB consists of 833+ hours of diverse, full-length audio clips, each
paired with human-annotated, text-based natural language questions and answers.
Our audio data were collected from permissively licensed sources and underwent
a human-assisted filtering process to ensure task compliance. We evaluate six
open-source and proprietary audio LMs on BLAB and find that all of them,
including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the
tasks in BLAB. Our comprehensive analysis reveals key insights into the
trade-offs between task difficulty and audio duration. In general, we find that
audio LMs struggle with long-form speech, with performance declining as
duration increases. They perform poorly on localization, temporal reasoning,
counting, and struggle to understand non-phonemic information, relying more on
prompts than audio content. BLAB serves as a challenging evaluation framework
to develop audio LMs with robust long-form audio understanding capabilities.

</details>


### [128] [Is AI currently capable of identifying wild oysters? A comparison of human annotators against the AI model, ODYSSEE](https://arxiv.org/abs/2505.03108)
*Brendan Campbell,Alan Williams,Kleio Baxevani,Alyssa Campbell,Rushabh Dhoke,Rileigh E. Hudock,Xiaomin Lin,Vivek Mange,Bernhard Neuberger,Arjun Suresh,Alhim Vera,Arthur Trembanis,Herbert G. Tanner,Edward Hale*

Main category: cs.AI

TL;DR: 评估了一种利用深度学习（ODYSSEE模型）自动识别和量化牡蛎礁中活牡蛎的方法，发现其速度快于人工，但准确性较低，图像质量是关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 当前的牡蛎种群监测方法具有破坏性、耗时费力，不适用于小规模或敏感环境，因此需要一种高效、无损的替代方案。

Method: 开发并评估了ODYSSEE深度学习模型，该模型通过分析牡蛎礁的野外图像或视频来识别活牡蛎。将其识别活牡蛎的准确性与专家及非专家人工标注员进行比较，并探究了预测错误的潜在来源。

Result: ODYSSEE模型在处理速度上显著优于专家和非专家标注员（分别为39.6秒、2.34小时、4.50小时）。然而，模型在识别活牡蛎方面的准确率（63%）低于专家（74%）和非专家（75%），并且倾向于高估活牡蛎的数量。图像质量对模型和人工标注的准确性均有重要影响；有趣的是，更高质量的图像提升了人工准确性，却降低了模型准确性。

Conclusion: 尽管ODYSSEE模型目前准确性不足，但通过使用更高质量的图像进行训练、利用更多的实时图像数据以及引入额外的注释训练类别，其预测能力有望得到显著提升。未来的研究应侧重于改进区分活牡蛎与死牡蛎的方法。

Abstract: Oysters are ecologically and commercially important species that require
frequent monitoring to track population demographics (e.g. abundance, growth,
mortality). Current methods of monitoring oyster reefs often require
destructive sampling methods and extensive manual effort. Therefore, they are
suboptimal for small-scale or sensitive environments. A recent alternative, the
ODYSSEE model, was developed to use deep learning techniques to identify live
oysters using video or images taken in the field of oyster reefs to assess
abundance. The validity of this model in identifying live oysters on a reef was
compared to expert and non-expert annotators. In addition, we identified
potential sources of prediction error. Although the model can make inferences
significantly faster than expert and non-expert annotators (39.6 s, $2.34 \pm
0.61$ h, $4.50 \pm 1.46$ h, respectively), the model overpredicted the number
of live oysters, achieving lower accuracy (63\%) in identifying live oysters
compared to experts (74\%) and non-experts (75\%) alike. Image quality was an
important factor in determining the accuracy of the model and the annotators.
Better quality images improved human accuracy and worsened model accuracy.
Although ODYSSEE was not sufficiently accurate, we anticipate that future
training on higher-quality images, utilizing additional live imagery, and
incorporating additional annotation training classes will greatly improve the
model's predictive power based on the results of this analysis. Future research
should address methods that improve the detection of living vs. dead oysters.

</details>


### [129] [Holmes: Automated Fact Check with Large Language Models](https://arxiv.org/abs/2505.03135)
*Haoran Ou,Gelei Deng,Xingshuo Han,Jie Zhang,Xinlei He,Han Qiu,Shangwei Guo,Tianwei Zhang*

Main category: cs.AI

TL;DR: 本研究提出了Holmes框架，通过新颖的证据检索方法增强大型语言模型（LLM）在虚假信息检测方面的能力，显著提升了检测准确性。


<details>
  <summary>Details</summary>
Motivation: 互联网虚假信息（尤其是多模态形式）的快速传播对社会信任、决策和国家安全构成威胁。传统深度学习模型难以捕捉其复杂性，而大型语言模型在独立评估信息真实性和自主检索准确证据方面存在局限性。

Method: 提出了一个名为Holmes的端到端框架。该框架核心是一种新颖的证据检索方法，包括：(1) 利用LLM驱动的摘要技术从开放源提取关键信息；(2) 使用新算法和指标评估证据质量。Holmes通过这种方式辅助LLM收集高质量证据，以验证声明并生成有效解释。

Result: 实验表明，Holmes在两个开源数据集上实现了88.3%的准确率，在实时验证任务中达到90.2%的准确率。值得注意的是，其改进的证据检索方法相比现有方法将事实核查准确率提高了30.8%。研究还发现，单独的LLM无法可靠评估信息真实性，提供相关证据能显著提高其性能，但LLM不能自主搜索准确证据。

Conclusion: Holmes框架通过其创新的证据检索机制，有效解决了LLM在虚假信息检测中面临的挑战，显著提高了事实核查的准确性和效率。

Abstract: The rise of Internet connectivity has accelerated the spread of
disinformation, threatening societal trust, decision-making, and national
security. Disinformation has evolved from simple text to complex multimodal
forms combining images and text, challenging existing detection methods.
Traditional deep learning models struggle to capture the complexity of
multimodal disinformation. Inspired by advances in AI, this study explores
using Large Language Models (LLMs) for automated disinformation detection. The
empirical study shows that (1) LLMs alone cannot reliably assess the
truthfulness of claims; (2) providing relevant evidence significantly improves
their performance; (3) however, LLMs cannot autonomously search for accurate
evidence. To address this, we propose Holmes, an end-to-end framework featuring
a novel evidence retrieval method that assists LLMs in collecting high-quality
evidence. Our approach uses (1) LLM-powered summarization to extract key
information from open sources and (2) a new algorithm and metrics to evaluate
evidence quality. Holmes enables LLMs to verify claims and generate
justifications effectively. Experiments show Holmes achieves 88.3% accuracy on
two open-source datasets and 90.2% in real-time verification tasks. Notably,
our improved evidence retrieval boosts fact-checking accuracy by 30.8% over
existing methods

</details>


### [130] [CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics](https://arxiv.org/abs/2505.03171)
*Junqi Liu,Xiaohan Lin,Jonas Bayer,Yael Dillies,Weijie Jiang,Xiaodan Liang,Roman Soletskyi,Haiming Wang,Yunzhou Xie,Beibei Xiong,Zhengfeng Yang,Jujian Zhang,Lihong Zhi,Jia Li,Zhengying Liu*

Main category: cs.AI

TL;DR: 为解决组合数学基准测试的不足，本文引入了包含100个问题的CombiBench和评估框架Fine-Eval，并测试了大型语言模型在组合数学问题上的表现，发现其能力有限。


<details>
  <summary>Details</summary>
Motivation: 神经符号方法在代数、几何和数论等数学领域表现良好，但在组合数学领域因缺乏合适的基准和定理库而面临挑战。

Method: 1. 提出了CombiBench，一个包含100个组合数学问题的基准数据集，每个问题都在Lean 4中形式化并配有非形式化陈述。
2. 提出了Fine-Eval，一个用于形式化数学的评估框架，支持证明题和填空题。
3. 使用Fine-Eval和Kimina Lean Server作为后端，对多个大型语言模型在CombiBench上进行了基准测试。

Result: 现有大型语言模型在形式化解决组合数学问题方面的能力仍然有限。在所有测试模型中（均未针对此特定任务进行训练），Kimina-Prover表现最佳，在有无解题方案的情况下均解决了100个问题中的7个。

Conclusion: 本研究成功构建了首个全面的组合数学形式化基准CombiBench和评估框架Fine-Eval，并揭示了当前大型语言模型在该领域的能力局限性。研究成果已开源，为未来研究提供了宝贵资源。

Abstract: Neurosymbolic approaches integrating large language models with formal
reasoning have recently achieved human-level performance on mathematics
competition problems in algebra, geometry and number theory. In comparison,
combinatorics remains a challenging domain, characterized by a lack of
appropriate benchmarks and theorem libraries. To address this gap, we introduce
CombiBench, a comprehensive benchmark comprising 100 combinatorial problems,
each formalized in Lean~4 and paired with its corresponding informal statement.
The problem set covers a wide spectrum of difficulty levels, ranging from
middle school to IMO and university level, and span over ten combinatorial
topics. CombiBench is suitable for testing IMO solving capabilities since it
includes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its
statement contain an images). Furthermore, we provide a comprehensive and
standardized evaluation framework, dubbed Fine-Eval (for
$\textbf{F}$ill-in-the-blank $\textbf{in}$ L$\textbf{e}$an Evaluation), for
formal mathematics. It accommodates not only proof-based problems but also, for
the first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval
as the evaluation method and Kimina Lean Server as the backend, we benchmark
several LLMs on CombiBench and observe that their capabilities for formally
solving combinatorial problems remain limited. Among all models tested (none of
which has been trained for this particular task), Kimina-Prover attains the
best results, solving 7 problems (out of 100) under both ``with solution'' and
``without solution'' scenarios. We open source the benchmark dataset alongside
with the code of the proposed evaluation method at
https://github.com/MoonshotAI/CombiBench/.

</details>


### [131] [Patterns and Mechanisms of Contrastive Activation Engineering](https://arxiv.org/abs/2505.03189)
*Yixiong Hao,Ayush Panda,Stepan Shabalin,Sheikh Abdur Raheem Ali*

Main category: cs.AI

TL;DR: 该研究分析了对比激活工程（CAE）在控制大型语言模型行为方面的性能、局限性及部署策略，发现其在特定条件下有效，但也存在一些挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的行为控制因其复杂性和不透明性而具有挑战性，而现有微调等方法计算成本高昂。对比激活工程（CAE）作为一种零成本的推理时模型行为调整新方法，具有潜力。

Method: 本研究通过在分布内（in-distribution）和分布外（out-of-distribution）设置下评估对比激活工程（CAE）的性能，分析其缺点，并初步制定有效部署的综合指南。CAE通过修改LLM的内部表示来引导其输出。

Result: 研究发现：1. CAE仅在分布内上下文中可靠有效。2. 用于生成引导向量的样本数量在约80个样本后效益递减。3. 引导向量易受对抗性输入影响，可能逆转引导行为。4. 引导向量会损害模型的整体困惑度。5. 较大的模型对引导引起的性能下降更具抵抗力。

Conclusion: 对比激活工程（CAE）为LLM行为调整提供了一种有前景的低成本方法，但其有效性主要局限于分布内数据，且存在样本数量效益、对抗鲁棒性和模型困惑度方面的挑战。较大模型对此方法的负面影响表现出更强的抵抗力。

Abstract: Controlling the behavior of Large Language Models (LLMs) remains a
significant challenge due to their inherent complexity and opacity. While
techniques like fine-tuning can modify model behavior, they typically require
extensive computational resources. Recent work has introduced a class of
contrastive activation engineering (CAE) techniques as promising approaches for
steering LLM outputs through targeted modifications to their internal
representations. Applied at inference-time with zero cost, CAE has the
potential to introduce a new paradigm of flexible, task-specific LLM behavior
tuning. We analyze the performance of CAE in in-distribution,
out-of-distribution settings, evaluate drawbacks, and begin to develop
comprehensive guidelines for its effective deployment. We find that 1. CAE is
only reliably effective when applied to in-distribution contexts. 2. Increasing
the number of samples used to generate steering vectors has diminishing returns
at around 80 samples. 3. Steering vectors are susceptible to adversarial inputs
that reverses the behavior that is steered for. 4. Steering vectors harm the
overall model perplexity. 5. Larger models are more resistant to
steering-induced degradation.

</details>


### [132] [RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation](https://arxiv.org/abs/2505.03275)
*Tiantian Gan,Qiyao Sun*

Main category: cs.AI

TL;DR: RAG-MCP是一种通过检索增强生成来优化大型语言模型使用外部工具的框架，它通过预先检索相关工具，显著减少了提示长度并提高了工具选择的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在有效利用日益增多的外部工具时，会因提示膨胀和选择复杂性而遇到困难。

Method: 引入RAG-MCP框架，该框架通过语义检索从外部索引中识别与查询最相关的MCP（模型上下文协议）工具，然后仅将选定的工具描述传递给LLM，从而将工具发现过程外包。

Result: 实验表明，RAG-MCP显著减少了提示令牌（例如，减少超过50%），并在基准测试任务上将工具选择准确率提高了三倍以上（从基线的13.62%提高到43.13%）。

Conclusion: RAG-MCP为大型语言模型实现了可扩展且准确的工具集成。

Abstract: Large language models (LLMs) struggle to effectively utilize a growing number
of external tools, such as those defined by the Model Context Protocol
(MCP)\cite{IntroducingMCP}, due to prompt bloat and selection complexity. We
introduce RAG-MCP, a Retrieval-Augmented Generation framework that overcomes
this challenge by offloading tool discovery. RAG-MCP uses semantic retrieval to
identify the most relevant MCP(s) for a given query from an external index
before engaging the LLM. Only the selected tool descriptions are passed to the
model, drastically reducing prompt size and simplifying decision-making.
Experiments, including an MCP stress test, demonstrate RAG-MCP significantly
cuts prompt tokens (e.g., by over 50%) and more than triples tool selection
accuracy (43.13% vs 13.62% baseline) on benchmark tasks. RAG-MCP enables
scalable and accurate tool integration for LLMs.

</details>


### [133] [Capability-Driven Skill Generation with LLMs: A RAG-Based Approach for Reusing Existing Libraries and Interfaces](https://arxiv.org/abs/2505.03295)
*Luis Miguel Vieira da Silva,Aljosha Köcher,Nicolas König,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: 该研究提出了一种利用大型语言模型，基于自然语言输入和以“能力”为合约，为自动化系统自动生成可执行“技能”代码的方法。


<details>
  <summary>Details</summary>
Motivation: 在模块化自动化系统中，开发符合相应“能力”的“技能”实现过程耗时且充满挑战。

Method: 将“能力”视为“技能”实现的合约，利用大型语言模型（LLMs）根据自然语言用户输入生成可执行代码。通过一个检索增强生成（RAG）架构，整合现有的软件库和接口技术，允许用户将自己的库和资源接口融入代码生成过程。

Result: 该方法通过一个使用Python和ROS 2控制的自主移动机器人进行了评估，结果表明了该方法的可行性和灵活性。

Conclusion: 研究表明，所提出的基于大型语言模型从“能力”和自然语言输入生成“技能”实现的方法是可行且灵活的，特别是其集成了通过检索增强生成架构纳入用户自定义库和资源接口的能力。

Abstract: Modern automation systems increasingly rely on modular architectures, with
capabilities and skills as one solution approach. Capabilities define the
functions of resources in a machine-readable form and skills provide the
concrete implementations that realize those capabilities. However, the
development of a skill implementation conforming to a corresponding capability
remains a time-consuming and challenging task. In this paper, we present a
method that treats capabilities as contracts for skill implementations and
leverages large language models to generate executable code based on natural
language user input. A key feature of our approach is the integration of
existing software libraries and interface technologies, enabling the generation
of skill implementations across different target languages. We introduce a
framework that allows users to incorporate their own libraries and resource
interfaces into the code generation process through a retrieval-augmented
generation architecture. The proposed method is evaluated using an autonomous
mobile robot controlled via Python and ROS 2, demonstrating the feasibility and
flexibility of the approach.

</details>


### [134] [Artificial Behavior Intelligence: Technology, Challenges, and Future Directions](https://arxiv.org/abs/2505.03315)
*Kanghyun Jo,Jehwan Choi,Kwanho Kim,Seongmin Kim,Duy-Linh Nguyen,Xuan-Thuy Vo,Adri Priadana,Tien-Dat Tran*

Main category: cs.AI

TL;DR: 本文定义了人工行为智能 (ABI) 的技术框架，用于分析和预测人类行为，并探讨了其关键组件、最新模型进展以及轻量化模型开发等挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶、智能医疗、监控系统和社交机器人等多种人工智能应用领域，理解和预测人类行为已成为一项核心能力。

Method: 提出了人工行为智能 (ABI) 的技术框架，该框架综合分析人类姿态、面部表情、情绪、行为序列和情境线索，其关键组件包括姿态估计、面部与情感识别、序列行为分析和情境感知建模。研究团队专注于开发智能轻量级模型，并探索轻量级Transformer、基于图的识别架构、能量感知损失函数和多模态知识蒸馏等优化策略。

Result: 强调了大型预训练模型（如大语言模型LLM、视觉基础模型、多模态集成模型）在显著提高行为识别准确性和可解释性方面的变革潜力。研究团队正积极开发能够高效推断复杂人类行为的智能轻量级模型，并在实时环境中验证其适用性。

Conclusion: ABI为理解和预测人类行为提供了全面的技术框架，但其实际部署面临着从有限数据中学习、量化复杂行为预测的不确定性以及优化模型结构以实现低功耗实时推理等挑战。研究团队正积极探索优化策略以应对这些挑战，推动ABI在现实世界中的应用。

Abstract: Understanding and predicting human behavior has emerged as a core capability
in various AI application domains such as autonomous driving, smart healthcare,
surveillance systems, and social robotics. This paper defines the technical
framework of Artificial Behavior Intelligence (ABI), which comprehensively
analyzes and interprets human posture, facial expressions, emotions, behavioral
sequences, and contextual cues. It details the essential components of ABI,
including pose estimation, face and emotion recognition, sequential behavior
analysis, and context-aware modeling. Furthermore, we highlight the
transformative potential of recent advances in large-scale pretrained models,
such as large language models (LLMs), vision foundation models, and multimodal
integration models, in significantly improving the accuracy and
interpretability of behavior recognition. Our research team has a strong
interest in the ABI domain and is actively conducting research, particularly
focusing on the development of intelligent lightweight models capable of
efficiently inferring complex human behaviors. This paper identifies several
technical challenges that must be addressed to deploy ABI in real-world
applications including learning behavioral intelligence from limited data,
quantifying uncertainty in complex behavior prediction, and optimizing model
structures for low-power, real-time inference. To tackle these challenges, our
team is exploring various optimization strategies including lightweight
transformers, graph-based recognition architectures, energy-aware loss
functions, and multimodal knowledge distillation, while validating their
applicability in real-time environments.

</details>


### [135] [AI-Driven Scholarly Peer Review via Persistent Workflow Prompting, Meta-Prompting, and Meta-Reasoning](https://arxiv.org/abs/2505.03332)
*Evgeny Markhasin*

Main category: cs.AI

TL;DR: 该报告介绍了一种名为“持久工作流提示”(PWP)的新型提示工程方法，旨在通过标准LLM聊天界面，使大型语言模型能够对科学手稿（特别是实验化学领域）进行批判性同行评审。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在对科学手稿进行批判性同行评审方面面临挑战，这部分归因于数据限制和专家级推理的复杂性，现有方法难以弥合这一差距。

Method: 研究引入并开发了“持久工作流提示”(PWP)方法。这是一种零代码、无需API的提示工程技术，通过Markdown构建的分层模块化架构定义详细的分析工作流程。PWP提示在会话开始时提交一次，通过后续查询触发预设工作流，引导LLM进行系统性评估。

Result: 通过PWP引导的LLM在测试案例中成功识别了主要的方法学缺陷，减轻了LLM的输入偏差，并能执行复杂的分析任务，例如区分声明与证据、整合文本/图片/图表信息以推断参数、进行定量可行性检查以及评估先验合理性。

Conclusion: PWP方法显示出巨大潜力，能够利用现成的LLM对复杂的科学任务（如手稿评审）进行精密的分析。这项工作不仅展示了PWP在特定应用中的有效性，也为元开发过程本身提供了见解，强调了通过详细工作流程形式化来增强LLM能力的潜力。

Abstract: Critical peer review of scientific manuscripts presents a significant
challenge for Large Language Models (LLMs), partly due to data limitations and
the complexity of expert reasoning. This report introduces Persistent Workflow
Prompting (PWP), a potentially broadly applicable prompt engineering
methodology designed to bridge this gap using standard LLM chat interfaces
(zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical
analysis of experimental chemistry manuscripts, featuring a hierarchical,
modular architecture (structured via Markdown) that defines detailed analysis
workflows. We develop this PWP prompt through iterative application of
meta-prompting techniques and meta-reasoning aimed at systematically codifying
expert review workflows, including tacit knowledge. Submitted once at the start
of a session, this PWP prompt equips the LLM with persistent workflows
triggered by subsequent queries, guiding modern reasoning LLMs through
systematic, multimodal evaluations. Demonstrations show the PWP-guided LLM
identifying major methodological flaws in a test case while mitigating LLM
input bias and performing complex tasks, including distinguishing claims from
evidence, integrating text/photo/figure analysis to infer parameters, executing
quantitative feasibility checks, comparing estimates against claims, and
assessing a priori plausibility. To ensure transparency and facilitate
replication, we provide full prompts, detailed demonstration analyses, and logs
of interactive chats as supplementary resources. Beyond the specific
application, this work offers insights into the meta-development process
itself, highlighting the potential of PWP, informed by detailed workflow
formalization, to enable sophisticated analysis using readily available LLMs
for complex scientific tasks.

</details>


### [136] [Domain Adversarial Training for Mitigating Gender Bias in Speech-based Mental Health Detection](https://arxiv.org/abs/2505.03359)
*June-Woo Kim,Haram Yoon,Wonkyo Oh,Dawoon Jung,Sung-Hoon Yoon,Dae-Jin Kim,Dong-Ho Lee,Sang-Yeol Lee,Chan-Mo Yang*

Main category: cs.AI

TL;DR: 该研究提出了一种域对抗训练方法，通过将不同性别视为不同域来解决语音AI模型在抑郁症和PTSD检测中的性别偏见问题，显著提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 基于语音的AI模型在检测抑郁症和PTSD方面潜力巨大，但常存在性别偏见，导致预测不公和不准确，影响其公平性和有效性。

Method: 采用域对抗训练方法，将不同性别视为独立的域，并将此性别信息整合到预训练的语音基础模型中，以检测抑郁症和PTSD。在E-DAIC数据集上验证了该方法的有效性。

Result: 实验结果表明，该方法显著提升了检测性能，F1分数相较于基线模型提高了多达13.29个百分点。

Conclusion: 该研究强调了在人工智能驱动的心理健康评估中解决人口统计学差异（特别是性别偏见）的重要性，以提升模型的公平性和准确性。

Abstract: Speech-based AI models are emerging as powerful tools for detecting
depression and the presence of Post-traumatic stress disorder (PTSD), offering
a non-invasive and cost-effective way to assess mental health. However, these
models often struggle with gender bias, which can lead to unfair and inaccurate
predictions. In this study, our study addresses this issue by introducing a
domain adversarial training approach that explicitly considers gender
differences in speech-based depression and PTSD detection. Specifically, we
treat different genders as distinct domains and integrate this information into
a pretrained speech foundation model. We then validate its effectiveness on the
E-DAIC dataset to assess its impact on performance. Experimental results show
that our method notably improves detection performance, increasing the F1-score
by up to 13.29 percentage points compared to the baseline. This highlights the
importance of addressing demographic disparities in AI-driven mental health
assessment.

</details>


### [137] [Validating the Effectiveness of a Large Language Model-based Approach for Identifying Children's Development across Various Free Play Settings in Kindergarten](https://arxiv.org/abs/2505.03369)
*Yuanyuan Yang,Yuan Shen,Tianchen Sun,Yangbin Xie*

Main category: cs.AI

TL;DR: 研究提出了一种结合大型语言模型（LLMs）和学习分析的新方法，通过分析儿童的游戏自我叙述来评估其在自由游戏中的发展，并证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的自由游戏评估方法难以全面捕捉儿童发展状况并提供及时反馈，本研究旨在探索一种更有效、更全面的评估方法。

Method: 收集儿童在不同游戏区的游戏叙述，利用大型语言模型（LLM）识别叙述中体现的发展能力，并结合学习分析技术计算儿童在不同游戏环境中的表现得分。

Result: 基于LLM的方法在识别儿童的认知、运动和社交能力方面表现出高准确率（多数领域超过90%）。此外，研究发现不同游戏环境对儿童特定能力的发展贡献存在显著差异。

Conclusion: 所提出的结合LLMs和学习分析的方法能够有效识别儿童在各种自由游戏环境中的发展情况，为提供以儿童为中心的发展洞察、支持个性化学习和改进幼儿教育实践提供了巨大潜力。

Abstract: Free play is a fundamental aspect of early childhood education, supporting
children's cognitive, social, emotional, and motor development. However,
assessing children's development during free play poses significant challenges
due to the unstructured and spontaneous nature of the activity. Traditional
assessment methods often rely on direct observations by teachers, parents, or
researchers, which may fail to capture comprehensive insights from free play
and provide timely feedback to educators. This study proposes an innovative
approach combining Large Language Models (LLMs) with learning analytics to
analyze children's self-narratives of their play experiences. The LLM
identifies developmental abilities, while performance scores across different
play settings are calculated using learning analytics techniques. We collected
2,224 play narratives from 29 children in a kindergarten, covering four
distinct play areas over one semester. According to the evaluation results from
eight professionals, the LLM-based approach achieved high accuracy in
identifying cognitive, motor, and social abilities, with accuracy exceeding 90%
in most domains. Moreover, significant differences in developmental outcomes
were observed across play settings, highlighting each area's unique
contributions to specific abilities. These findings confirm that the proposed
approach is effective in identifying children's development across various free
play settings. This study demonstrates the potential of integrating LLMs and
learning analytics to provide child-centered insights into developmental
trajectories, offering educators valuable data to support personalized learning
and enhance early childhood education practices.

</details>


### [138] [Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based Agents](https://arxiv.org/abs/2505.03434)
*Schaun Wheeler,Olivier Jeunen*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）因其依赖程序性记忆而在复杂环境中受限。论文主张通过模块化架构整合语义记忆和联想学习系统，以增强其适应性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs的程序性记忆架构使其擅长重复性任务，但在规则多变、反馈模糊的复杂现实环境中表现出局限性，难以实现真正的自适应智能。

Method: 论文提出采用一种模块化架构，将LLMs的程序性能力与语义记忆和联想学习系统进行解耦并整合，以增强认知功能。

Result: 预期通过这种方法，可以构建出能够更好地导航“棘手”学习环境（规则变化、反馈模糊、新奇事物频现）的智能体，弥合狭隘的程序性专长与真实世界问题解决所需的自适应智能之间的差距。

Conclusion: 为了让LLMs在复杂和不可预测的环境中有效运作并超越其程序性记忆的限制，必须通过模块化设计整合语义记忆和联想学习系统，从而实现更高级别的自适应智能。

Abstract: Large Language Models (LLMs) represent a landmark achievement in Artificial
Intelligence (AI), demonstrating unprecedented proficiency in procedural tasks
such as text generation, code completion, and conversational coherence. These
capabilities stem from their architecture, which mirrors human procedural
memory -- the brain's ability to automate repetitive, pattern-driven tasks
through practice. However, as LLMs are increasingly deployed in real-world
applications, it becomes impossible to ignore their limitations operating in
complex, unpredictable environments. This paper argues that LLMs, while
transformative, are fundamentally constrained by their reliance on procedural
memory. To create agents capable of navigating ``wicked'' learning environments
-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must
augment LLMs with semantic memory and associative learning systems. By adopting
a modular architecture that decouples these cognitive functions, we can bridge
the gap between narrow procedural expertise and the adaptive intelligence
required for real-world problem-solving.

</details>


### [139] [The Steganographic Potentials of Language Models](https://arxiv.org/abs/2505.03439)
*Artem Karpov,Tinuade Adeleke,Seong Hah Cho,Natalia Perez-Campanero*

Main category: cs.AI

TL;DR: 研究了大型语言模型（LLM）在文本中隐藏信息（隐写术）的能力，发现当前模型能力初步，但通过强化学习微调和显式算法指导可显著增强其隐写能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型利用隐写术隐藏信息的潜力对检测失控的人工智能代理构成挑战，并可能削弱其推理的忠实性。

Method: 通过强化学习（RL）对大型语言模型进行微调，使其能够：（1）开发隐蔽编码方案，（2）在被提示时进行隐写，（3）在可能存在隐藏推理但未被提示的现实场景中使用隐写术。同时评估了模型隐藏推理的意图及其隐写性能，并进行了行为上的非微调评估。

Result: 研究发现，当前模型在安全性和隐写容量方面表现出初步的隐写能力。然而，明确的算法指导（如微调）能显著增强它们隐藏信息的能力。

Conclusion: 尽管当前大型语言模型的隐写能力尚不成熟，但通过特定的训练方法（如强化学习微调）可以显著提升其信息隐藏能力，这提示了未来需要关注的潜在风险。

Abstract: The potential for large language models (LLMs) to hide messages within plain
text (steganography) poses a challenge to detection and thwarting of unaligned
AI agents, and undermines faithfulness of LLMs reasoning. We explore the
steganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)
to: (1) develop covert encoding schemes, (2) engage in steganography when
prompted, and (3) utilize steganography in realistic scenarios where hidden
reasoning is likely, but not prompted. In these scenarios, we detect the
intention of LLMs to hide their reasoning as well as their steganography
performance. Our findings in the fine-tuning experiments as well as in
behavioral non fine-tuning evaluations reveal that while current models exhibit
rudimentary steganographic abilities in terms of security and capacity,
explicit algorithmic guidance markedly enhances their capacity for information
concealment.

</details>


### [140] [am-ELO: A Stable Framework for Arena-based LLM Evaluation](https://arxiv.org/abs/2505.03475)
*Zirui Liu,Jiatong Li,Yan Zhuang,Qi Liu,Shuanghong Shen,Jie Ouyang,Mingyue Cheng,Shijin Wang*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的稳定竞技场框架，通过改进ELO评分系统来解决现有LLM评估方法的不稳定性和忽略标注者能力差异的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于ELO评分系统的竞技场评估方法存在排名不一致导致的不稳定问题，并且未能充分考虑标注者能力差异。

Method: 1. 提出m-ELO方法，使用最大似然估计（MLE）替代传统ELO的迭代更新，并提供了其一致性和稳定性的理论证明。2. 提出am-ELO方法，修改ELO概率函数以整合标注者能力，从而同时评估模型得分和标注者可靠性。

Result: 实验证明，该方法确保了评估的稳定性，提供了一种更鲁棒、准确和稳定的LLM评估方法。

Conclusion: 新提出的框架通过解决ELO系统的不稳定性并引入标注者能力考量，为大型语言模型提供了更可靠、准确和稳定的评估手段。

Abstract: Arena-based evaluation is a fundamental yet significant evaluation paradigm
for modern AI models, especially large language models (LLMs). Existing
framework based on ELO rating system suffers from the inevitable instability
problem due to ranking inconsistency and the lack of attention to the varying
abilities of annotators. In this paper, we introduce a novel stable arena
framework to address these issues by enhancing the ELO Rating System.
Specifically, we replace the iterative update method with a Maximum Likelihood
Estimation (MLE) approach, m-ELO, and provide theoretical proof of the
consistency and stability of the MLE approach for model ranking. Additionally,
we proposed the am-ELO, which modify the Elo Rating's probability function to
incorporate annotator abilities, enabling the simultaneous estimation of model
scores and annotator reliability. Experiments demonstrate that this method
ensures stability, proving that this framework offers a more robust, accurate,
and stable evaluation method for LLMs.

</details>


### [141] [STORY2GAME: Generating (Almost) Everything in an Interactive Fiction Game](https://arxiv.org/abs/2505.03547)
*Eric Zhou,Shreyas Basavatia,Moontashir Siam,Zexin Chen,Mark O. Riedl*

Main category: cs.AI

TL;DR: STORY2GAME是一种利用大语言模型（LLM）将故事转化为文本交互式小说游戏的新方法，通过生成故事、填充世界并构建游戏引擎的动作代码来实现互动性。


<details>
  <summary>Details</summary>
Motivation: 传统硬编码动作会限制故事生成的自由度，而该研究旨在让故事生成过程更开放，同时确保生成的游戏体验基于实际的游戏状态，并能响应玩家的即兴动作。

Method: 该方法首先使用LLM生成故事，然后填充游戏世界。关键在于利用LLM生成的动作前置条件和效果来指导游戏引擎跟踪和改变游戏状态的动作代码生成。同时，引入了动态生成新动作的技术，以适应玩家提出的、故事之外的动作，这可能需要实时更新游戏引擎的状态表示和已生成的动作。

Result: 研究评估了动作代码生成的成功率，其标准是玩家是否能够以交互方式完整体验整个生成的故事。

Conclusion: STORY2GAME通过LLM生成动作及其元数据，成功地将开放式故事生成与可交互的游戏状态结合起来，动态动作生成进一步提升了玩家的互动自由度。

Abstract: We introduce STORY2GAME, a novel approach to using Large Language Models to
generate text-based interactive fiction games that starts by generating a
story, populates the world, and builds the code for actions in a game engine
that enables the story to play out interactively. Whereas a given set of
hard-coded actions can artificially constrain story generation, the ability to
generate actions means the story generation process can be more open-ended but
still allow for experiences that are grounded in a game state. The key to
successful action generation is to use LLM-generated preconditions and effects
of actions in the stories as guides for what aspects of the game state must be
tracked and changed by the game engine when a player performs an action. We
also introduce a technique for dynamically generating new actions to
accommodate the player's desire to perform actions that they think of that are
not part of the story. Dynamic action generation may require on-the-fly updates
to the game engine's state representation and revision of previously generated
actions. We evaluate the success rate of action code generation with respect to
whether a player can interactively play through the entire generated story.

</details>


### [142] [A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model Reasoning](https://arxiv.org/abs/2505.03553)
*Kolawole E. Ogunsina,Morayo A. Ogunsina*

Main category: cs.AI

TL;DR: 该论文提出了一种受分布式账本技术（哈希图）启发的共识机制，通过让多个大型语言模型（LLM）进行信息交换和虚拟投票，以验证并统一其输出，从而提高AI系统的可靠性并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）输出的不一致性和幻觉现象是构建可靠AI系统的主要障碍。不同的专有推理模型（RMs）在处理相同复杂请求时，由于训练和推理的差异，常产生分歧的结果。

Method: 提出一种受哈希图共识算法启发的新型共识机制。该方法将每个推理模型（RM）视为黑箱对等节点，采用“gossip-about-gossip”（闲话套闲话）通信和虚拟投票，使RM集成系统达成一致。模型通过迭代交换和更新其答案，利用每一轮的信息来提高后续轮次的准确性和置信度。

Result: 论文提出了一个原型系统的架构设计，论证了这种哈希图启发共识机制在AI集成系统中的可行性，并概述了其在减少非事实输出方面相对于传统集成技术的优势。讨论了初步的实施考虑、收敛性和准确性的评估标准以及潜在挑战。

Conclusion: 所提出的共识机制为多智能体AI系统实现自我验证并在复杂任务中提供高保真响应展示了一个有前景的研究方向。

Abstract: Inconsistent outputs and hallucinations from large language models (LLMs) are
major obstacles to reliable AI systems. When different proprietary reasoning
models (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,
are given the same complex request, they often produce divergent results due to
variations in training and inference. This paper proposes a novel consensus
mechanism, inspired by distributed ledger technology, to validate and converge
these outputs, treating each RM as a black-box peer. Building on the Hashgraph
consensus algorithm, our approach employs gossip-about-gossip communication and
virtual voting to achieve agreement among an ensemble of RMs. We present an
architectural design for a prototype system in which RMs iteratively exchange
and update their answers, using information from each round to improve accuracy
and confidence in subsequent rounds. This approach goes beyond simple majority
voting by incorporating the knowledge and cross-verification content of every
model. We justify the feasibility of this Hashgraph-inspired consensus for AI
ensembles and outline its advantages over traditional ensembling techniques in
reducing nonfactual outputs. Preliminary considerations for implementation,
evaluation criteria for convergence and accuracy, and potential challenges are
discussed. The proposed mechanism demonstrates a promising direction for
multi-agent AI systems to self-validate and deliver high-fidelity responses in
complex tasks.

</details>


### [143] [OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents](https://arxiv.org/abs/2505.03570)
*Mariya Davydova,Daniel Jeffries,Patrick Barker,Arturo Márquez Flores,Sinéad Ryan*

Main category: cs.AI

TL;DR: 本文介绍了一个名为 OSUniverse 的新基准测试，用于评估高级图形用户界面（GUI）导航人工智能代理在复杂、多模式桌面任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 缺乏一个易于使用、可扩展、测试用例覆盖全面且能自动验证的基准测试，以衡量高级 GUI 导航 AI 代理在复杂桌面任务中的真实能力，并与人类表现进行对比。

Method: 设计并推出了 OSUniverse 基准测试。该基准包含从基本精确点击到多步骤、多应用程序的复杂任务，并校准了任务难度，确保当前顶尖 AI 代理得分不超过 50%，而普通白领能完美完成。同时引入了平均错误率低于 2% 的自动验证机制。

Result: 成功构建了 OSUniverse 基准测试第一版。其校准的任务难度使得当前顶尖 AI 代理的测试结果不超过 50%，而普通人类用户可以完美完成所有任务。所引入的自动验证机制平均错误率低于 2%。

Conclusion: OSUniverse 为全自动衡量 GUI 导航 AI 代理的进展、能力和有效性提供了坚实的基础，特别适用于短期和中期的发展评估。

Abstract: In this paper, we introduce OSUniverse: a benchmark of complex, multimodal
desktop-oriented tasks for advanced GUI-navigation AI agents that focuses on
ease of use, extensibility, comprehensive coverage of test cases, and automated
validation. We divide the tasks in increasing levels of complexity, from basic
precision clicking to multistep, multiapplication tests requiring dexterity,
precision, and clear thinking from the agent. In version one of the benchmark,
presented here, we have calibrated the complexity of the benchmark test cases
to ensure that the SOTA (State of the Art) agents (at the time of publication)
do not achieve results higher than 50%, while the average white collar worker
can perform all these tasks with perfect accuracy. The benchmark can be scored
manually, but we also introduce an automated validation mechanism that has an
average error rate less than 2%. Therefore, this benchmark presents solid
ground for fully automated measuring of progress, capabilities and the
effectiveness of GUI-navigation AI agents over the short and medium-term
horizon. The source code of the benchmark is available at
https://github.com/agentsea/osuniverse.

</details>


### [144] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering and Manipulating Human Perceptual Variability](https://arxiv.org/abs/2505.03641)
*Chen Wei,Chi Zhang,Jiachen Zou,Haotian Deng,Dietmar Heinke,Quanying Liu*

Main category: cs.AI

TL;DR: 提出BAM计算框架，通过人工神经网络边界采样和人类行为实验来研究、预测并操纵个体在感知决策中的差异。


<details>
  <summary>Details</summary>
Motivation: 人类在认知任务和日常生活中的决策存在显著的个体差异，理解这种差异对于揭示人类在不确定和模糊情境下的感知与决策机制至关重要。

Method: 提出了一个计算框架BAM（边界对齐与操纵框架）。该框架首先利用人工神经网络（ANN）的感知边界采样算法生成能够引发显著感知变异性的刺激物。随后，通过大规模人类行为实验（涉及246名参与者和超过11万次试验）验证这些刺激物的有效性，并创建了variMNIST数据集。最后，通过个性化模型对齐和对抗性生成技术，实现对成对参与者不同感知决策的预测和操纵。

Result: 该研究通过大规模行为实验成功验证了沿ANN决策边界生成的刺激物在引发个体感知变异性方面的有效性，并创建了包含19,943张系统注释图像的variMNIST数据集。此外，还建立了一种能够同时预测和操纵成对参与者不同感知决策的可靠方法。

Conclusion: 这项工作弥合了计算模型与人类个体差异研究之间的差距，为个性化感知分析提供了新的工具和方法。

Abstract: Human decision-making in cognitive tasks and daily life exhibits considerable
variability, shaped by factors such as task difficulty, individual preferences,
and personal experiences. Understanding this variability across individuals is
essential for uncovering the perceptual and decision-making mechanisms that
humans rely on when faced with uncertainty and ambiguity. We present a
computational framework BAM (Boundary Alignment & Manipulation framework) that
combines perceptual boundary sampling in ANNs and human behavioral experiments
to systematically investigate this phenomenon. Our perceptual boundary sampling
algorithm generates stimuli along ANN decision boundaries that intrinsically
induce significant perceptual variability. The efficacy of these stimuli is
empirically validated through large-scale behavioral experiments involving 246
participants across 116,715 trials, culminating in the variMNIST dataset
containing 19,943 systematically annotated images. Through personalized model
alignment and adversarial generation, we establish a reliable method for
simultaneously predicting and manipulating the divergent perceptual decisions
of pairs of participants. This work bridges the gap between computational
models and human individual difference research, providing new tools for
personalized perception analysis.

</details>


### [145] [BURNS: Backward Underapproximate Reachability for Neural-Feedback-Loop Systems](https://arxiv.org/abs/2505.03643)
*Chelsea Sidrane,Jana Tumova*

Main category: cs.AI

TL;DR: 提出一种计算神经反馈环路欠近似反向可达集以验证目标到达性的算法。


<details>
  <summary>Details</summary>
Motivation: 支持学习的规划和控制算法日益流行，但通常缺乏严格的性能或安全保证。

Method: 通过过近似系统动力学函数，并求解混合整数线性规划问题，来计算非线性离散时间神经反馈环路的欠近似反向可达集，并利用这些可达集来检查系统的目标到达属性。

Result: 严格分析了算法的健全性，并通过数值例子进行了验证。该算法能够用于检查学习型系统的目标到达属性。

Conclusion: 扩展了可为支持学习的系统验证的属性类别。

Abstract: Learning-enabled planning and control algorithms are increasingly popular,
but they often lack rigorous guarantees of performance or safety. We introduce
an algorithm for computing underapproximate backward reachable sets of
nonlinear discrete time neural feedback loops. We then use the backward
reachable sets to check goal-reaching properties. Our algorithm is based on
overapproximating the system dynamics function to enable computation of
underapproximate backward reachable sets through solutions of mixed-integer
linear programs. We rigorously analyze the soundness of our algorithm and
demonstrate it on a numerical example. Our work expands the class of properties
that can be verified for learning-enabled systems.

</details>


### [146] [Learning Symbolic Persistent Macro-Actions for POMDP Solving Over Time](https://arxiv.org/abs/2505.03668)
*Celeste Veronese,Daniele Meli,Alessandro Farinelli*

Main category: cs.AI

TL;DR: 该论文提出了一种整合时序逻辑推理与部分可观察马尔可夫决策过程（POMDP）的方法，通过归纳逻辑编程学习持久的宏动作，以在不确定性下实现可解释且高效的决策。


<details>
  <summary>Details</summary>
Motivation: 在部分可观察马尔可夫决策过程（POMDP）中，实现可解释的决策并减少对人工设计启发式宏动作的依赖，同时提高决策效率和鲁棒性，是一个重要挑战。

Method: 该方法将基于事件演算（EC）的线性时序逻辑（LTL）片段与POMDP相结合，利用归纳逻辑编程（ILP）从少量执行轨迹（信念-动作对）中学习生成持久的宏动作。这些宏动作随后用于指导基于蒙特卡洛树搜索（MCTS）的POMDP求解器。

Result: 在Pocman和Rocksample基准测试场景中，学习到的宏动作与时间无关的启发式方法相比，展现出更强的表达能力和通用性，并显著提高了计算效率。

Conclusion: 该研究提出了一种有效的方法，通过整合时序逻辑推理和POMDP，并利用ILP学习宏动作，能够实现不确定环境下的可解释决策，显著减少推理时间，保证鲁棒性能，且无需手动设计启发式规则。

Abstract: This paper proposes an integration of temporal logical reasoning and
Partially Observable Markov Decision Processes (POMDPs) to achieve
interpretable decision-making under uncertainty with macro-actions. Our method
leverages a fragment of Linear Temporal Logic (LTL) based on Event Calculus
(EC) to generate \emph{persistent} (i.e., constant) macro-actions, which guide
Monte Carlo Tree Search (MCTS)-based POMDP solvers over a time horizon,
significantly reducing inference time while ensuring robust performance. Such
macro-actions are learnt via Inductive Logic Programming (ILP) from a few
traces of execution (belief-action pairs), thus eliminating the need for
manually designed heuristics and requiring only the specification of the POMDP
transition model. In the Pocman and Rocksample benchmark scenarios, our learned
macro-actions demonstrate increased expressiveness and generality when compared
to time-independent heuristics, indeed offering substantial computational
efficiency improvements.

</details>


### [147] [Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts Collaboration Perception, Not Performance](https://arxiv.org/abs/2505.03674)
*Yotam Amitai,Reuth Mirsky,Ofra Amir*

Main category: cs.AI

TL;DR: AI分享对人类队友目标的推断，未能显著提升任务绩效，但改善了主观协作感知。


<details>
  <summary>Details</summary>
Motivation: 在人机团队中，由于目标直接沟通并非总是可行，本研究旨在探索AI分享其对人类队友目标的推断能否提升任务表现和协作感知。

Method: 通过实验比较了三种条件：AI不分享目标识别（NR）、AI分享可行目标（VG）、以及AI按需分享可行目标（VGod）。

Result: AI分享目标信息并未显著提高任务绩效或总体满意度，但主题分析表明其支持了策略调整和主观协作感知。各条件下认知负荷无显著差异。

Conclusion: 目标分享存在微妙的权衡：它能增进信任和感知协作，但可能不直接带来客观绩效的提升，这突显了在人机交互中平衡信息量与简洁性的挑战。

Abstract: In human-agent teams, openly sharing goals is often assumed to enhance
planning, collaboration, and effectiveness. However, direct communication of
these goals is not always feasible, requiring teammates to infer their
partner's intentions through actions. Building on this, we investigate whether
an AI agent's ability to share its inferred understanding of a human teammate's
goals can improve task performance and perceived collaboration. Through an
experiment comparing three conditions-no recognition (NR), viable goals (VG),
and viable goals on-demand (VGod) - we find that while goal-sharing information
did not yield significant improvements in task performance or overall
satisfaction scores, thematic analysis suggests that it supported strategic
adaptations and subjective perceptions of collaboration. Cognitive load
assessments revealed no additional burden across conditions, highlighting the
challenge of balancing informativeness and simplicity in human-agent
interactions. These findings highlight the nuanced trade-off of goal-sharing:
while it fosters trust and enhances perceived collaboration, it can
occasionally hinder objective performance gains.

</details>


### [148] [Graph Drawing for LLMs: An Empirical Evaluation](https://arxiv.org/abs/2505.03678)
*Walter Didimo,Fabrizio Montecchiani,Tommaso Piselli*

Main category: cs.AI

TL;DR: 研究了图的视觉呈现（布局、美观度）和提示技术对大语言模型（LLM）在处理基于视觉输入的图相关任务时性能的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在图相关任务中应用的增多，需要理解当图以视觉形式（如绘图）输入时，不同因素（如布局范式、绘图美学、提示技术）如何影响模型性能。

Method: 通过全面的实验分析，评估了不同的图布局范式、绘图的美学特征以及查询时使用的提示技术对大语言模型处理视觉化图任务的性能影响。

Result: 研究发现，选择合适的布局范式和从人类视角优化输入图的可读性，可以显著提高模型在给定任务上的性能。此外，选择最有效的提示技术对于达到最佳性能至关重要，尽管这具有挑战性。

Conclusion: 对于依赖视觉输入的大语言模型图任务，图的布局选择、绘图质量以及提示工程是影响模型性能的关键因素，需要仔细考虑以优化结果。

Abstract: Our work contributes to the fast-growing literature on the use of Large
Language Models (LLMs) to perform graph-related tasks. In particular, we focus
on usage scenarios that rely on the visual modality, feeding the model with a
drawing of the graph under analysis. We investigate how the model's performance
is affected by the chosen layout paradigm, the aesthetics of the drawing, and
the prompting technique used for the queries. We formulate three corresponding
research questions and present the results of a thorough experimental analysis.
Our findings reveal that choosing the right layout paradigm and optimizing the
readability of the input drawing from a human perspective can significantly
improve the performance of the model on the given task. Moreover, selecting the
most effective prompting technique is a challenging yet crucial task for
achieving optimal performance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [149] [Uncertainty Quantification for Machine Learning in Healthcare: A Survey](https://arxiv.org/abs/2505.02874)
*L. Julián Lechuga López,Shaza Elsharief,Dhiyaa Al Jorf,Firas Darwish,Congbo Ma,Farah E. Shamout*

Main category: cs.LG

TL;DR: 这篇综述分析了医疗保健领域机器学习中的不确定性量化(UQ)，提出了一个将UQ方法整合到机器学习流程各阶段的框架，并探讨了相关挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习医疗工具缺乏可靠的不确定性量化，现有综述关注范围狭窄且未能系统评估UQ方法，导致UQ在医疗保健中的实际应用有限，因此需要进行全面系统的分析。

Method: 本研究采用综述方法，对医疗保健领域当前的UQ方法进行全面分析，提出了一个信息框架，阐述如何将不同UQ方法整合到机器学习流程的各个阶段（数据处理、训练和评估），并重点介绍了医疗保健中的常用方法及其他领域有潜力的新方法。

Result: 研究提供了一份关于医疗保健中UQ的全面分析，一个将UQ方法整合到机器学习流程的框架，识别了医疗领域最常用的UQ方法以及其他领域具有潜力的新方法，并概述了在医疗保健中实施UQ的挑战与机遇。

Conclusion: 本研究旨在为研究人员和实践者提供关于在医疗保健机器学习流程中实施UQ的清晰概述，指导他们选择合适的技术，以增强机器学习驱动的医疗解决方案的可靠性、安全性以及患者和临床医生的信任。

Abstract: Uncertainty Quantification (UQ) is pivotal in enhancing the robustness,
reliability, and interpretability of Machine Learning (ML) systems for
healthcare, optimizing resources and improving patient care. Despite the
emergence of ML-based clinical decision support tools, the lack of principled
quantification of uncertainty in ML models remains a major challenge. Current
reviews have a narrow focus on analyzing the state-of-the-art UQ in specific
healthcare domains without systematically evaluating method efficacy across
different stages of model development, and despite a growing body of research,
its implementation in healthcare applications remains limited. Therefore, in
this survey, we provide a comprehensive analysis of current UQ in healthcare,
offering an informed framework that highlights how different methods can be
integrated into each stage of the ML pipeline including data processing,
training and evaluation. We also highlight the most popular methods used in
healthcare and novel approaches from other domains that hold potential for
future adoption in the medical context. We expect this study will provide a
clear overview of the challenges and opportunities of implementing UQ in the ML
pipeline for healthcare, guiding researchers and practitioners in selecting
suitable techniques to enhance the reliability, safety and trust from patients
and clinicians on ML-driven healthcare solutions.

</details>


### [150] [A Wireless Collaborated Inference Acceleration Framework for Plant Disease Recognition](https://arxiv.org/abs/2505.02877)
*Hele Zhu,Xinyi Huang,Haojia Gao,Mengfei Jiang,Haohua Que,Lei Mu*

Main category: cs.LG

TL;DR: 提出了一种边缘-云协同的植物病害识别推理框架，通过模型剪枝和贪婪策略优化分割点，以提高推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统植物病害识别方法准确率低、成本高、效率低下。深度学习方法虽有效，但在嵌入式设备上运行时面临推理延迟、高能耗的挑战，云端卸载则受限于通信带宽，影响推理效率。

Method: 提出了一种边缘设备与云服务器协同的植物病害识别推理框架。该框架首先采用深度强化学习对DNN模型进行剪枝以加速推理并降低能耗，然后通过贪婪策略确定模型的最佳分割点以实现最佳协同推理加速，并使用Gradio构建了人机交互界面。

Result: 实验结果表明，所提出的协同推理框架在保持可接受识别准确率的同时，显著提升了推理速度。

Conclusion: 该协同推理框架为植物病害的快速诊断和预防提供了一种新颖的解决方案。

Abstract: Plant disease is a critical factor affecting agricultural production.
Traditional manual recognition methods face significant drawbacks, including
low accuracy, high costs, and inefficiency. Deep learning techniques have
demonstrated significant benefits in identifying plant diseases, but they still
face challenges such as inference delays and high energy consumption. Deep
learning algorithms are difficult to run on resource-limited embedded devices.
Offloading these models to cloud servers is confronted with the restriction of
communication bandwidth, and all of these factors will influence the
inference's efficiency. We propose a collaborative inference framework for
recognizing plant diseases between edge devices and cloud servers to enhance
inference speed. The DNN model for plant disease recognition is pruned through
deep reinforcement learning to improve the inference speed and reduce energy
consumption. Then the optimal split point is determined by a greedy strategy to
achieve the best collaborated inference acceleration. Finally, the system for
collaborative inference acceleration in plant disease recognition has been
implemented using Gradio to facilitate friendly human-machine interaction.
Experiments indicate that the proposed collaborative inference framework
significantly increases inference speed while maintaining acceptable
recognition accuracy, offering a novel solution for rapidly diagnosing and
preventing plant diseases.

</details>


### [151] [LLM4FTS: Enhancing Large Language Models for Financial Time Series Prediction](https://arxiv.org/abs/2505.02880)
*Zian Liu,Renjun Jia*

Main category: cs.LG

TL;DR: 该研究提出了一种名为 $LLM4FTS$ 的新框架，通过可学习的补丁分割和动态小波卷积模块增强大型语言模型（LLM）处理金融时间序列中多尺度模式的能力，并在股票收益预测中取得了先进成果。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列预测因低信噪比和复杂时序模式而具挑战性。传统机器学习模型能力有限，而现有基于LLM的方法通常采用固定长度补丁分析，忽略了市场数据的多尺度特性。

Method: 提出了 $LLM4FTS$ 框架，首先使用基于DTW距离的K-means++聚类识别尺度不变模式，接着引入自适应补丁分割以保持模式完整性，并设计动态小波卷积模块灵活捕捉时频特征，共同提升LLM处理金融时间序列的能力。

Result: 在真实世界金融数据集上的大量实验证明了该框架的有效性，其在捕捉复杂市场模式方面表现优越，并在股票收益预测中达到了行业领先水平。该框架已成功部署于实际交易系统。

Conclusion: $LLM4FTS$ 框架显著提升了LLM在金融预测中的应用水平，特别是在处理金融时间序列的尺度不变模式方面，并已证实其在真实世界中的适用性。

Abstract: Predicting financial time series presents significant challenges due to
inherent low signal-to-noise ratios and intricate temporal patterns.
Traditional machine learning models exhibit limitations in this forecasting
task constrained by their restricted model capacity. Recent advances in large
language models (LLMs), with their greatly expanded parameter spaces,
demonstrate promising potential for modeling complex dependencies in temporal
sequences. However, existing LLM-based approaches typically focus on
fixed-length patch analysis due to the Transformer architecture, ignoring
market data's multi-scale pattern characteristics. In this study, we propose
$LLM4FTS$, a novel framework that enhances LLM capabilities for temporal
sequence modeling through learnable patch segmentation and dynamic wavelet
convolution modules. Specifically,we first employ K-means++ clustering based on
DTW distance to identify scale-invariant patterns in market data. Building upon
pattern recognition results, we introduce adaptive patch segmentation that
partitions temporal sequences while preserving maximal pattern integrity. To
accommodate time-varying frequency characteristics, we devise a dynamic wavelet
convolution module that emulates discrete wavelet transformation with enhanced
flexibility in capturing time-frequency features. These three modules work
together to improve large language model's ability to handle scale-invariant
patterns in financial time series. Extensive experiments on real-world
financial datasets substantiate the framework's efficacy, demonstrating
superior performance in capturing complex market patterns and achieving
state-of-the-art results in stock return prediction. The successful deployment
in practical trading systems confirms its real-world applicability,
representing a significant advancement in LLM applications for financial
forecasting.

</details>


### [152] [Rewriting Pre-Training Data Boosts LLM Performance in Math and Code](https://arxiv.org/abs/2505.02881)
*Kazuki Fujii,Yukito Tajima,Sakae Mizuki,Hinari Shimada,Taihei Shiotani,Koshiro Saito,Masanari Ohi,Masaki Kawamura,Taishi Nakamura,Takumi Okamoto,Shigeki Ishida,Kakeru Hattori,Youmi Ma,Hiroya Takamura,Rio Yokota,Naoaki Okazaki*

Main category: cs.LG

TL;DR: 通过系统重写公共数据创建了两个新数据集（SwallowCode和SwallowMath），旨在提升大语言模型在代码生成和数学推理任务上的预训练语料质量，并显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在程序合成和数学推理方面的性能，受到其预训练语料库质量的根本限制。

Method: 提出了两个开源数据集：SwallowCode通过一个四阶段流程（语法验证、pylint风格过滤、两阶段LLM重写）精炼Python代码，采用“转换并保留”策略；SwallowMath通过移除冗余、恢复上下文和重新格式化解题步骤来增强数学数据。

Result: 使用SwallowCode对Llama-3.1-8B进行持续预训练，在HumanEval上pass@1提升了17.0%，在HumanEval+上提升了17.7%；使用SwallowMath则在GSM8K上准确率提升了12.4%，在MATH上提升了7.6%。消融实验证实了各流程阶段的贡献，其中重写阶段效果最显著。

Conclusion: 通过系统性重写公共数据创建的SwallowCode和SwallowMath数据集，显著提升了LLM在代码生成和数学推理方面的性能。所提出的“转换并保留”方法最大化了数据效用，且所有资源公开可用，促进了可复现研究和专业领域LLM预训练的进步。

Abstract: The performance of large language models (LLMs) in program synthesis and
mathematical reasoning is fundamentally limited by the quality of their
pre-training corpora. We introduce two openly licensed datasets, released under
the Llama 3.3 Community License, that significantly enhance LLM performance by
systematically rewriting public data. SwallowCode (approximately 16.1 billion
tokens) refines Python snippets from The-Stack-v2 through a novel four-stage
pipeline: syntax validation, pylint-based style filtering, and a two-stage LLM
rewriting process that enforces style conformity and transforms snippets into
self-contained, algorithmically efficient examples. Unlike prior methods that
rely on exclusionary filtering or limited transformations, our
transform-and-retain approach upgrades low-quality code, maximizing data
utility. SwallowMath (approximately 2.3 billion tokens) enhances Finemath-4+ by
removing boilerplate, restoring context, and reformatting solutions into
concise, step-by-step explanations. Within a fixed 50 billion token training
budget, continual pre-training of Llama-3.1-8B with SwallowCode boosts pass@1
by +17.0 on HumanEval and +17.7 on HumanEval+ compared to Stack-Edu, surpassing
the baseline model's code generation capabilities. Similarly, substituting
SwallowMath yields +12.4 accuracy on GSM8K and +7.6 on MATH. Ablation studies
confirm that each pipeline stage contributes incrementally, with rewriting
delivering the largest gains. All datasets, prompts, and checkpoints are
publicly available, enabling reproducible research and advancing LLM
pre-training for specialized domains.

</details>


### [153] [Unlearning vs. Obfuscation: Are We Truly Removing Knowledge?](https://arxiv.org/abs/2505.02884)
*Guangzhi Sun,Potsawee Manakul,Xiao Zhan,Mark Gales*

Main category: cs.LG

TL;DR: 该论文区分了大型语言模型中的知识遗忘与混淆，提出了一种新的遗忘方法DF-MCQ，通过使模型在特定多选题上的预测分布扁平化来移除知识，并引入了探测评估框架来验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM知识遗忘技术通常依赖混淆手段，这并非真正的知识移除，模型仍可能泄露被遗忘的信息。因此，需要能真正移除信息并能有效评估遗忘效果的方法。

Method: 1. 形式化区分知识遗忘与信息混淆。2. 引入一个基于探测的评估框架来评估信息是否被真正移除。3. 提出DF-MCQ方法：利用KL散度，在自动生成的多项选择题上使模型的预测分布扁平化，从而移除关于目标个体的知识并触发模型的拒绝行为。

Result: 实验表明，DF-MCQ方法实现了知识遗忘，探测问题上的拒绝率超过90%，并且其在探测问题上表现出的随机选择水平的不确定性远高于基于混淆的方法。

Conclusion: DF-MCQ是一种有效的知识遗忘方法，能够比现有混淆技术更真实地从LLM中移除目标信息，并促使模型产生适当的拒绝行为。

Abstract: Unlearning has emerged as a critical capability for large language models
(LLMs) to support data privacy, regulatory compliance, and ethical AI
deployment. Recent techniques often rely on obfuscation by injecting incorrect
or irrelevant information to suppress knowledge. Such methods effectively
constitute knowledge addition rather than true removal, often leaving models
vulnerable to probing. In this paper, we formally distinguish unlearning from
obfuscation and introduce a probing-based evaluation framework to assess
whether existing approaches genuinely remove targeted information. Moreover, we
propose DF-MCQ, a novel unlearning method that flattens the model predictive
distribution over automatically generated multiple-choice questions using
KL-divergence, effectively removing knowledge about target individuals and
triggering appropriate refusal behaviour. Experimental results demonstrate that
DF-MCQ achieves unlearning with over 90% refusal rate and a random choice-level
uncertainty that is much higher than obfuscation on probing questions.

</details>


### [154] [When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger](https://arxiv.org/abs/2505.02888)
*Rintaro Ando*

Main category: cs.LG

TL;DR: N2M-RSI模型展示了AI通过反馈自身输出来实现复杂性无限增长。


<details>
  <summary>Details</summary>
Motivation: 探究AI智能体通过将自身输出作为输入，在跨越特定信息整合阈值后，其内部复杂性是否以及如何无限制增长。

Method: 提出了一种名为“噪声到意义递归自我改进”（N2M-RSI）的最小形式化模型，该模型与具体实现无关，并可扩展至交互式智能体群。

Result: 模型表明，在特定假设下，当AI智能体将其输出反馈为输入并跨越信息整合阈值后，其内部复杂性会无限增长。该模型也暗示了在允许多智能体通信时可能出现超线性效应。

Conclusion: N2M-RSI理论模型证明了AI通过递归自我改进可以实现内部复杂性的持续增长，为理解自我提示、哥德尔式自引用和AutoML等概念提供了统一框架。

Abstract: We present Noise-to-Meaning Recursive Self-Improvement (N2M-RSI), a minimal
formal model showing that once an AI agent feeds its own outputs back as inputs
and crosses an explicit information-integration threshold, its internal
complexity will grow without bound under our assumptions. The framework unifies
earlier ideas on self-prompting large language models, G\"odelian
self-reference, and AutoML, yet remains implementation-agnostic. The model
furthermore scales naturally to interacting swarms of agents, hinting at
super-linear effects once communication among instances is permitted. For
safety reasons, we omit system-specific implementation details and release only
a brief, model-agnostic toy prototype in Appendix C.

</details>


### [155] [Early Prediction of Sepsis: Feature-Aligned Transfer Learning](https://arxiv.org/abs/2505.02889)
*Oyindolapo O. Komolafe,Zhimin Mei,David Morales Zarate,Gregory William Spangenberg*

Main category: cs.LG

TL;DR: 本研究提出特征对齐迁移学习（FATL）方法用于早期脓毒症检测，旨在解决现有模型特征不一和群体偏差问题，以提高预测的泛化性和一致性。


<details>
  <summary>Details</summary>
Motivation: 脓毒症危及生命，早期检测至关重要但常滞后。现有预测模型因特征不一致（如心率、体温、实验室结果）和基于狭窄患者群体的训练，导致模型难以比较、泛化能力受限，并存在群体偏差。

Method: 采用特征对齐迁移学习（FATL）方法：1) 识别并专注于跨多个研究中最重要且普遍报告的特征，以确保模型的一致性和临床相关性；2) 通过加权方法整合来自不同人群训练的模型的知识，以解决群体偏差，提高系统的泛化能力。

Result: FATL方法通过对齐关键特征和整合多源模型知识，提高了脓毒症预测模型的一致性、临床相关性、泛化能力和有效性。该方法提供了一个实用且可扩展的早期检测系统，尤其适用于资源有限的医院。

Conclusion: FATL为早期脓毒症检测提供了一个有前景的、实用且可扩展的解决方案。它通过解决现有模型在特征选择和群体偏差方面的不足，有望改善患者预后，降低医疗成本，并支持更公平的医疗服务。

Abstract: Sepsis is a life threatening medical condition that occurs when the body has
an extreme response to infection, leading to widespread inflammation, organ
failure, and potentially death. Because sepsis can worsen rapidly, early
detection is critical to saving lives. However, current diagnostic methods
often identify sepsis only after significant damage has already occurred. Our
project aims to address this challenge by developing a machine learning based
system to predict sepsis in its early stages, giving healthcare providers more
time to intervene.
  A major problem with existing models is the wide variability in the patient
information or features they use, such as heart rate, temperature, and lab
results. This inconsistency makes models difficult to compare and limits their
ability to work across different hospitals and settings. To solve this, we
propose a method called Feature Aligned Transfer Learning (FATL), which
identifies and focuses on the most important and commonly reported features
across multiple studies, ensuring the model remains consistent and clinically
relevant.
  Most existing models are trained on narrow patient groups, leading to
population bias. FATL addresses this by combining knowledge from models trained
on diverse populations, using a weighted approach that reflects each models
contribution. This makes the system more generalizable and effective across
different patient demographics and clinical environments. FATL offers a
practical and scalable solution for early sepsis detection, particularly in
hospitals with limited resources, and has the potential to improve patient
outcomes, reduce healthcare costs, and support more equitable healthcare
delivery.

</details>


### [156] [RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM Inference](https://arxiv.org/abs/2505.02922)
*Yaoqi Chen,Jinkai Zhang,Baotong Lu,Qianxi Zhang,Chengruidong Zhang,Jingjia Luo,Di Liu,Huiqiang Jiang,Qi Chen,Jing Liu,Bailu Ding,Xiao Yan,Jiawei Jiang,Chen Chen,Mingxing Zhang,Yuqing Yang,Fan Yang,Mao Yang*

Main category: cs.LG

TL;DR: RetroInfer是一个通过利用注意力稀疏性来加速长上下文大语言模型（LLM）推理的新型系统，它将KV缓存视为向量存储系统进行优化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）日益增长的上下文长度给高效推理带来了巨大挑战，主要受限于GPU内存和带宽。

Method: RetroInfer系统核心包含：1) Wave Index（注意力感知向量索引），通过三方注意力近似、精度有界注意力估计和分段聚类技术高效检索关键token；2) Wave Buffer，协调KV缓存放置，并在GPU和CPU间重叠计算与数据传输。

Result: 实验表明，RetroInfer在GPU内存限制内比全注意力机制提速高达4.5倍；当KV缓存扩展到CPU内存时，比稀疏注意力基线提速高达10.5倍，同时保持了与全注意力机制相当的准确率。

Conclusion: RetroInfer在不牺牲模型准确性的前提下，通过有效利用注意力稀疏性和硬件协同，为长上下文LLM推理提供了强大的性能提升。

Abstract: The growing context lengths of large language models (LLMs) pose significant
challenges for efficient inference, primarily due to GPU memory and bandwidth
constraints. We present RetroInfer, a novel system that reconceptualizes the
key-value (KV) cache as a vector storage system which exploits the inherent
attention sparsity to accelerate long-context LLM inference. At its core is the
wave index, an Attention-aWare VEctor index that enables efficient and accurate
retrieval of critical tokens through techniques such as tripartite attention
approximation, accuracy-bounded attention estimation, and segmented clustering.
Complementing this is the wave buffer, which coordinates KV cache placement and
overlaps computation and data transfer across GPU and CPU to sustain high
throughput. Unlike prior sparsity-based methods that struggle with token
selection and hardware coordination, RetroInfer delivers robust performance
without compromising model accuracy. Experiments on long-context benchmarks
show up to 4.5X speedup over full attention within GPU memory limits and up to
10.5X over sparse attention baselines when KV cache is extended to CPU memory,
all while preserving full-attention-level accuracy.

</details>


### [157] [Smooth Quadratic Prediction Markets](https://arxiv.org/abs/2505.02959)
*Enrique Nueve,Bo Waggoner*

Main category: cs.LG

TL;DR: 本文提出了一种名为平滑二次预测市场的新型预测市场，它通过激励交易者共同实现广义最速梯度下降，优化了AD证券的最坏情况货币损失，同时保留了关键的市场保证。


<details>
  <summary>Details</summary>
Motivation: 现有基于对偶成本函数的预测市场（DCFMM）集体实现“跟随正则化领导者”学习算法。研究旨在探索其他学习算法是否能启发新的预测市场设计。

Method: 通过分解和修改基于对偶成本函数做市商（DCFMM）的定价机制，提出平滑二次预测市场。该市场激励交易者集体实现广义最速梯度下降。同时，研究了在预算受限和仅购买证券两种现实约束下交易者的行为，并初步分析了利用平滑二次AD预测市场促进自适应流动性的方法。

Result: 与DCFMM相比，平滑二次预测市场在AD证券方面具有更低的的最坏情况货币损失，同时保留了诸如瞬时价格存在性、信息融合、表达能力、无套利和某种形式的激励相容性等公理保证。

Conclusion: 研究结果表明，未来可以将价格更新规则与费用结构分离设计，同时保留市场保证，为更灵活的预测市场设计提供了思路。

Abstract: When agents trade in a Duality-based Cost Function prediction market, they
collectively implement the learning algorithm Follow-The-Regularized-Leader. We
ask whether other learning algorithms could be used to inspire the design of
prediction markets. By decomposing and modifying the Duality-based Cost
Function Market Maker's (DCFMM) pricing mechanism, we propose a new prediction
market, called the Smooth Quadratic Prediction Market, the incentivizes agents
to collectively implement general steepest gradient descent. Relative to the
DCFMM, the Smooth Quadratic Prediction Market has a better worst-case monetary
loss for AD securities while preserving axiom guarantees such as the existence
of instantaneous price, information incorporation, expressiveness, no
arbitrage, and a form of incentive compatibility. To motivate the application
of the Smooth Quadratic Prediction Market, we independently examine agents'
trading behavior under two realistic constraints: bounded budgets and buy-only
securities. Finally, we provide an introductory analysis of an approach to
facilitate adaptive liquidity using the Smooth Quadratic AD Prediction Market.
Our results suggest future designs where the price update rule is separate from
the fee structure, yet guarantees are preserved.

</details>


### [158] [Physics-Learning AI Datamodel (PLAID) datasets: a collection of physics simulations for machine learning](https://arxiv.org/abs/2505.02974)
*Fabien Casenave,Xavier Roynard,Brian Staber,Nissrine Akkari,William Piat,Michele Alessandro Bucci,Abbas Kabalan,Xuan Minh Vuong Nguyen,Luca Saverio,Raphaël Carpintero Perez,Anthony Kalaydjian,Samy Fouché,Thierry Gonon,Ghassan Najjar,Emmanuel Menier,Matthieu Nastorg,Christian Rey*

Main category: cs.LG

TL;DR: 提出PLAID框架，一个用于物理模拟数据集的统一标准和工具，以解决机器学习代理模型缺乏高质量数据集的问题，并发布了六个数据集及基准测试。


<details>
  <summary>Details</summary>
Motivation: 机器学习代理模型能加速科学模拟，但缺乏大规模、多样化和标准化的物理模拟数据集阻碍了其广泛应用。现有数据集和工具存在局限性。

Method: 引入PLAID (Physics-Learning AI Datamodel)，一个灵活且可扩展的框架，用于表示和共享物理模拟数据集。PLAID定义了描述模拟数据的统一标准，并提供了一个库来创建、读取和操作数据集。

Result: 发布了六个符合PLAID标准的精心制作的数据集，涵盖结构力学和计算流体力学领域。提供了使用代表性学习方法的基准测试。在Hugging Face上提供了基准测试工具以促进社区参与。

Conclusion: PLAID框架及其配套资源旨在克服现有物理模拟数据集的局限性，通过提供统一标准、多样化数据集和基准测试工具，促进机器学习在科学模拟中的应用和社区合作。

Abstract: Machine learning-based surrogate models have emerged as a powerful tool to
accelerate simulation-driven scientific workflows. However, their widespread
adoption is hindered by the lack of large-scale, diverse, and standardized
datasets tailored to physics-based simulations. While existing initiatives
provide valuable contributions, many are limited in scope-focusing on specific
physics domains, relying on fragmented tooling, or adhering to overly
simplistic datamodels that restrict generalization. To address these
limitations, we introduce PLAID (Physics-Learning AI Datamodel), a flexible and
extensible framework for representing and sharing datasets of physics
simulations. PLAID defines a unified standard for describing simulation data
and is accompanied by a library for creating, reading, and manipulating complex
datasets across a wide range of physical use cases (gitlab.com/drti/plaid). We
release six carefully crafted datasets under the PLAID standard, covering
structural mechanics and computational fluid dynamics, and provide baseline
benchmarks using representative learning methods. Benchmarking tools are made
available on Hugging Face, enabling direct participation by the community and
contribution to ongoing evaluation efforts (huggingface.co/PLAIDcompetitions).

</details>


### [159] [More Optimal Fractional-Order Stochastic Gradient Descent for Non-Convex Optimization Problems](https://arxiv.org/abs/2505.02985)
*Mohammad Partohaghighi,Roummel Marcia,YangQuan Chen*

Main category: cs.LG

TL;DR: 提出2SEDFOSGD算法，通过双尺度有效维度（2SED）自适应调整分数阶指数，以改善分数阶随机梯度下降的收敛性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统分数阶随机梯度下降（FOSGD）因其分数阶指数难以调整和稳定，导致在利用长记忆效应进行优化时实用性受限。

Method: 提出2SED分数阶随机梯度下降（2SEDFOSGD），将双尺度有效维度（2SED）算法与FOSGD集成。该方法通过跟踪模型敏感性和有效维度，以数据驱动的方式动态调整分数阶指数，旨在减少振荡并加速收敛。

Result: 理论分析表明，对于非凸优化问题，2SEDFOSGD保留了分数阶记忆的优势，同时避免了朴素分数阶SGD的缓慢或不稳定行为。在使用自回归（AR）模型的高斯和α稳定噪声场景下的实验评估显示，与基线方法相比，2SEDFOSGD收敛更快，参数估计更鲁棒。

Conclusion: 维度感知的2SEDFOSGD方法展示了其在高级建模和估计任务中的潜力，能够提供更快速且更稳定的优化性能。

Abstract: Fractional-order stochastic gradient descent (FOSGD) leverages fractional
exponents to capture long-memory effects in optimization. However, its utility
is often limited by the difficulty of tuning and stabilizing these exponents.
We propose 2SED Fractional-Order Stochastic Gradient Descent (2SEDFOSGD), which
integrates the Two-Scale Effective Dimension (2SED) algorithm with FOSGD to
adapt the fractional exponent in a data-driven manner. By tracking model
sensitivity and effective dimensionality, 2SEDFOSGD dynamically modulates the
exponent to mitigate oscillations and hasten convergence. Theoretically, for
onoconvex optimization problems, this approach preserves the advantages of
fractional memory without the sluggish or unstable behavior observed in na\"ive
fractional SGD. Empirical evaluations in Gaussian and $\alpha$-stable noise
scenarios using an autoregressive (AR) model highlight faster convergence and
more robust parameter estimates compared to baseline methods, underscoring the
potential of dimension-aware fractional techniques for advanced modeling and
estimation tasks.

</details>


### [160] [Radio: Rate-Distortion Optimization for Large Language Model Compression](https://arxiv.org/abs/2505.03031)
*Sean I. Young*

Main category: cs.LG

TL;DR: 提出了一种基于率失真理论的大语言模型量化新方法，具有可扩展性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型压缩对于在资源受限设备上部署、降低计算成本和减轻环境影响至关重要。

Method: 从率失真理论角度为大语言模型量化奠定基础，并提出一种基于简单率失真优化的量化技术。

Result: 该技术可扩展至包含数千亿权重参数的模型，并允许用户在训练后根据指定的模型大小或准确率进行压缩。

Conclusion: 本文提出了一种可扩展且灵活的后训练量化技术，能够根据用户需求平衡模型大小和精度。

Abstract: In recent years, the compression of large language models (LLMs) has emerged
as a key problem in facilitating LLM deployment on resource-limited devices,
reducing compute costs, and mitigating the environmental footprint due to
large-scale AI infrastructure. Here, we establish the foundations of LLM
quantization from a rate-distortion theory perspective and propose a
quantization technique based on simple rate-distortion optimization. Our
technique scales to models containing hundreds of billions of weight parameters
and offers users the flexibility to compress models, post-training, to a model
size or accuracy specified by the user.

</details>


### [161] [A New Perspective To Understanding Multi-resolution Hash Encoding For Neural Fields](https://arxiv.org/abs/2505.03042)
*Steven Tin Sui Luo*

Main category: cs.LG

TL;DR: 论文提出“域操纵”新视角，解释Instant-NGP中哈希网格提升神经场表达能力的机制，并通过一维信号实验验证。


<details>
  <summary>Details</summary>
Motivation: Instant-NGP的多分辨率哈希网格结构显著提升了神经场性能，但其工作原理尚不明确，导致超参数调整缺乏理论指导。

Method: 提出“域操纵” (domain manipulation) 的新视角来解释哈希网格的工作原理，认为其通过人工创建预先存在的线性片段的倍数来学习目标信号并提高表达能力。通过精心设计的一维信号实验进行验证。

Result: 实验结果支持了“域操纵”视角，表明哈希网格确实通过上述方式学习信号并提升表达能力。该分析主要基于一维信号，但论文指出其思想可推广至更高维度。

Conclusion: “域操纵”视角为理解哈希网格如何大幅提升神经网络能力提供了一个直观的解释，有助于更深入地理解Instant-NGP及其相关模型的内在机制。

Abstract: Instant-NGP has been the state-of-the-art architecture of neural fields in
recent years. Its incredible signal-fitting capabilities are generally
attributed to its multi-resolution hash grid structure and have been used and
improved in numerous following works. However, it is unclear how and why such a
hash grid structure improves the capabilities of a neural network by such great
margins. A lack of principled understanding of the hash grid also implies that
the large set of hyperparameters accompanying Instant-NGP could only be tuned
empirically without much heuristics. To provide an intuitive explanation of the
working principle of the hash grid, we propose a novel perspective, namely
domain manipulation. This perspective provides a ground-up explanation of how
the feature grid learns the target signal and increases the expressivity of the
neural field by artificially creating multiples of pre-existing linear
segments. We conducted numerous experiments on carefully constructed
1-dimensional signals to support our claims empirically and aid our
illustrations. While our analysis mainly focuses on 1-dimensional signals, we
show that the idea is generalizable to higher dimensions.

</details>


### [162] [34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery](https://arxiv.org/abs/2505.03049)
*Yoel Zimmermann,Adib Bazgir,Alexander Al-Feghali,Mehrad Ansari,L. Catherine Brinson,Yuan Chiang,Defne Circi,Min-Hsueh Chiu,Nathan Daelman,Matthew L. Evans,Abhijeet S. Gangan,Janine George,Hassan Harb,Ghazal Khalighinejad,Sartaaj Takrim Khan,Sascha Klawohn,Magdalena Lederbauer,Soroush Mahjoubi,Bernadette Mohr,Seyed Mohamad Moosavi,Aakash Naik,Aleyna Beste Ozhan,Dieter Plessers,Aritra Roy,Fabian Schöppach,Philippe Schwaller,Carla Terboven,Katharina Ueltzen,Shang Zhu,Jan Janssen,Calvin Li,Ian Foster,Ben Blaiszik*

Main category: cs.LG

TL;DR: 大型语言模型 (LLMs) 正通过在分子特性预测、材料设计、科研自动化和知识提取等方面的应用，深刻改变材料科学与化学研究。通过一次黑客松的34个项目，展示了LLMs的广泛能力。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型 (LLMs) 在材料科学与化学整个研究生命周期中的前沿能力和应用。

Method: 回顾和分析了在第二届“材料科学与化学大型语言模型应用黑客松”期间开发的34个项目，这些项目涵盖了LLM在七个关键研究领域的应用。

Result: 这些项目展示了LLMs作为多功能预测模型、领域特定工具快速原型开发平台的能力。开源和专有LLM通过增加推理能力、训练数据和新技术，提高了其有效性，尤其是在低数据环境和跨学科研究中。应用领域包括分子与材料特性预测、设计、自动化、科学交流、数据管理、假设生成及知识提取。

Conclusion: LLMs的持续进步为科学工作流程带来了新的机遇，同时也带来了新的挑战，需要在可靠性、可解释性和可复现性方面进行持续探索、改进和进一步研究。

Abstract: Large Language Models (LLMs) are reshaping many aspects of materials science
and chemistry research, enabling advances in molecular property prediction,
materials design, scientific automation, knowledge extraction, and more. Recent
developments demonstrate that the latest class of models are able to integrate
structured and unstructured data, assist in hypothesis generation, and
streamline research workflows. To explore the frontier of LLM capabilities
across the research lifecycle, we review applications of LLMs through 34 total
projects developed during the second annual Large Language Model Hackathon for
Applications in Materials Science and Chemistry, a global hybrid event. These
projects spanned seven key research areas: (1) molecular and material property
prediction, (2) molecular and material design, (3) automation and novel
interfaces, (4) scientific communication and education, (5) research data
management and automation, (6) hypothesis generation and evaluation, and (7)
knowledge extraction and reasoning from the scientific literature.
Collectively, these applications illustrate how LLMs serve as versatile
predictive models, platforms for rapid prototyping of domain-specific tools,
and much more. In particular, improvements in both open source and proprietary
LLM performance through the addition of reasoning, additional training data,
and new techniques have expanded effectiveness, particularly in low-data
environments and interdisciplinary research. As LLMs continue to improve, their
integration into scientific workflows presents both new opportunities and new
challenges, requiring ongoing exploration, continued refinement, and further
research to address reliability, interpretability, and reproducibility.

</details>


### [163] [Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/abs/2505.03084)
*Shashank Kapoor,Sanjay Surendranath Girija,Lakshit Arora,Dipen Pradhan,Ankit Shetgaonkar,Aman Raj*

Main category: cs.LG

TL;DR: 本文综述了针对文本、图像、视频和音频等多模态模型的对抗性攻击，旨在为从业者提供一个全面的威胁态势概览。


<details>
  <summary>Details</summary>
Motivation: 多模态模型继承了各单一模态的漏洞，导致对抗性威胁加剧。然而，目前缺乏一个面向从业者的、统一的多模态攻击类型概述，这对于采用、微调和部署这些模型的实践者采取预防措施至关重要。

Method: 通过对针对文本、图像、视频和音频四种模态的现有对抗性攻击进行调研和总结。

Result: 本文梳理并呈现了多模态领域的对抗性攻击态势及其演变，并据作者所知，这是首个对该领域威胁态势进行全面总结的研究。

Conclusion: 这项综述对于机器学习从业者理解多模态模型的威胁态势至关重要，有助于他们在采用、微调和部署这些模型时采取必要的预防措施，以应对放大的对抗性威胁。

Abstract: The introduction of multimodal models is a huge step forward in Artificial
Intelligence. A single model is trained to understand multiple modalities:
text, image, video, and audio. Open-source multimodal models have made these
breakthroughs more accessible. However, considering the vast landscape of
adversarial attacks across these modalities, these models also inherit
vulnerabilities of all the modalities, and ultimately, the adversarial threat
amplifies. While broad research is available on possible attacks within or
across these modalities, a practitioner-focused view that outlines attack types
remains absent in the multimodal world. As more Machine Learning Practitioners
adopt, fine-tune, and deploy open-source models in real-world applications,
it's crucial that they can view the threat landscape and take the preventive
actions necessary. This paper addresses the gap by surveying adversarial
attacks targeting all four modalities: text, image, video, and audio. This
survey provides a view of the adversarial attack landscape and presents how
multimodal adversarial threats have evolved. To the best of our knowledge, this
survey is the first comprehensive summarization of the threat landscape in the
multimodal world.

</details>


### [164] [Deep Learning in Renewable Energy Forecasting: A Cross-Dataset Evaluation of Temporal and Spatial Models](https://arxiv.org/abs/2505.03109)
*Lutfu Sua,Haibo Wang,Jun Huang*

Main category: cs.LG

TL;DR: 本研究评估了多种深度学习模型在可再生能源预测中的应用，发现LSTM和MLP模型在降低预测误差方面表现最佳，并探讨了影响模型准确性的因素及正则化方法。


<details>
  <summary>Details</summary>
Motivation: 可再生能源的不可预测性及其数据中的复杂非线性关系，对传统机器学习模型构成了挑战，因此需要开发和评估更鲁棒的深度学习模型，并识别影响其预测准确性的关键因素（如采样、平稳性、线性和超参数优化）。

Method: 研究提出了一个深度学习框架，比较了七种深度学习方法（LSTM、Stacked LSTM、CNN、CNN-LSTM、DNN、MLP、ED）在两个不同可再生能源数据集（西班牙天气与发电数据、光伏板发电数据）上的表现。研究中考虑了不同的训练/测试比例，并部署了包括早停、神经元丢弃和L2正则化在内的正则化方法来减少过拟合问题。

Result: 在所有评估的深度学习模型中，LSTM 和 MLP 模型展现出最优越的性能，其验证数据显示出极低的均方根误差值。

Conclusion: 研究表明，LSTM和MLP模型在可再生能源预测方面具有显著优势。同时，通过部署正则化方法可以有效缓解深度学习模型的过拟合问题，从而提高预测的准确性。

Abstract: Unpredictability of renewable energy sources coupled with the complexity of
those methods used for various purposes in this area calls for the development
of robust methods such as DL models within the renewable energy domain. Given
the nonlinear relationships among variables in renewable energy datasets, DL
models are preferred over traditional machine learning (ML) models because they
can effectively capture and model complex interactions between variables. This
research aims to identify the factors responsible for the accuracy of DL
techniques, such as sampling, stationarity, linearity, and hyperparameter
optimization for different algorithms. The proposed DL framework compares
various methods and alternative training/test ratios. Seven ML methods, such as
Long-Short Term Memory (LSTM), Stacked LSTM, Convolutional Neural Network
(CNN), CNN-LSTM, Deep Neural Network (DNN), Multilayer Perceptron (MLP), and
Encoder-Decoder (ED), were evaluated on two different datasets. The first
dataset contains the weather and power generation data. It encompasses two
distinct datasets, hourly energy demand data and hourly weather data in Spain,
while the second dataset includes power output generated by the photovoltaic
panels at 12 locations. This study deploys regularization approaches, including
early stopping, neuron dropping, and L2 regularization, to reduce the
overfitting problem associated with DL models. The LSTM and MLP models show
superior performance. Their validation data exhibit exceptionally low root mean
square error values.

</details>


### [165] [Plug-and-Play AMC: Context Is King in Training-Free, Open-Set Modulation with LLMs](https://arxiv.org/abs/2505.03112)
*Mohammad Rostami,Atik Faysal,Reihaneh Gh. Roshan,Huaxia Wang,Nikhil Muralidhar,Yu-Dong Yao*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的框架，将传统信号处理技术与大型语言模型 (LLM) 相结合，用于自动调制分类 (AMC)。该方法通过将信号特征转换为自然语言提示，利用 LLM 进行单次分类，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 自动调制分类 (AMC) 对有效的频谱管理和稳健的无线通信至关重要，但由于信号干扰和噪声的复杂相互作用，AMC 仍然具有挑战性。

Method: 该研究提出了一种创新框架，整合了传统信号处理技术和大型语言模型 (LLM)。它利用高阶统计和累积量估计将量化的信号特征转换为结构化的自然语言提示。通过在提示中加入示例上下文，利用 LLM 对经典信号处理的熟悉度，实现无需额外训练或预处理（如去噪）的有效单次分类。

Result: 在无噪声和有噪声条件下的合成数据集上的实验评估表明，该框架在多种调制方案和信噪比 (SNR) 下均取得了有竞争力的性能。

Conclusion: 该方法为在不同信道条件下构建稳健的无线通信基础模型铺平了道路，显著降低了开发特定信道模型的成本。这项工作为下一代无线网络中可扩展、可解释和通用的信号分类系统奠定了基础。

Abstract: Automatic Modulation Classification (AMC) is critical for efficient spectrum
management and robust wireless communications. However, AMC remains challenging
due to the complex interplay of signal interference and noise. In this work, we
propose an innovative framework that integrates traditional signal processing
techniques with Large-Language Models (LLMs) to address AMC. Our approach
leverages higher-order statistics and cumulant estimation to convert
quantitative signal features into structured natural language prompts. By
incorporating exemplar contexts into these prompts, our method exploits the
LLM's inherent familiarity with classical signal processing, enabling effective
one-shot classification without additional training or preprocessing (e.g.,
denoising). Experimental evaluations on synthetically generated datasets,
spanning both noiseless and noisy conditions, demonstrate that our framework
achieves competitive performance across diverse modulation schemes and
Signal-to-Noise Ratios (SNRs). Moreover, our approach paves the way for robust
foundation models in wireless communications across varying channel conditions,
significantly reducing the expense associated with developing channel-specific
models. This work lays the foundation for scalable, interpretable, and
versatile signal classification systems in next-generation wireless networks.
The source code is available at https://github.com/RU-SIT/context-is-king

</details>


### [166] [Adaptive Thresholding for Multi-Label Classification via Global-Local Signal Fusion](https://arxiv.org/abs/2505.03118)
*Dmytro Shamatrin*

Main category: cs.LG

TL;DR: 提出了一种新颖的自适应阈值机制，通过融合全局和局部信息，并将其作为可微分损失惩罚项，以改进多标签分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统多标签分类方法在处理类别严重不平衡和噪声数据时，采用固定阈值或独立处理标签，忽略了上下文信息和标签的全局稀有性，导致性能不佳。

Method: 引入一种自适应阈值机制，该机制融合全局（基于IDF）和局部（基于KNN）信号，为每个样本的每个标签生成动态阈值。这些阈值不作为硬性截断，而是作为损失函数中的可微分惩罚项，从而提供平滑的监督信号并改善模型校准。

Result: 在AmazonCat-13K基准测试中，该方法取得了0.1712的宏F1分数，显著优于基于树的方法和预训练的Transformer方法。

Conclusion: 该文提出的自适应阈值机制是一种轻量级、可解释且高度模块化的方法，能有效提升多标签分类在类别不平衡和噪声条件下的性能。

Abstract: Multi-label classification (MLC) requires predicting multiple labels per
sample, often under heavy class imbalance and noisy conditions. Traditional
approaches apply fixed thresholds or treat labels independently, overlooking
context and global rarity. We introduce an adaptive thresholding mechanism that
fuses global (IDF-based) and local (KNN-based) signals to produce per-label,
per-instance thresholds. Instead of applying these as hard cutoffs, we treat
them as differentiable penalties in the loss, providing smooth supervision and
better calibration. Our architecture is lightweight, interpretable, and highly
modular. On the AmazonCat-13K benchmark, it achieves a macro-F1 of 0.1712,
substantially outperforming tree-based and pretrained transformer-based
methods. We release full code for reproducibility and future extensions.

</details>


### [167] [Rethinking the Global Convergence of Softmax Policy Gradient with Linear Function Approximation](https://arxiv.org/abs/2505.03155)
*Max Qiushi Lin,Jincheng Mei,Matin Aghaei,Michael Lu,Bo Dai,Alekh Agarwal,Dale Schuurmans,Csaba Szepesvari,Sharan Vaswani*

Main category: cs.LG

TL;DR: 研究了线性Softmax策略梯度（Lin-SPG）的全局收敛性，发现其收敛与近似误差无关，并识别了保证收敛的关键特征条件。


<details>
  <summary>Details</summary>
Motivation: 探究策略梯度（PG）方法在函数近似下的全局收敛性，特别是近似误差对Softmax PG结合线性函数近似（Lin-SPG）收敛性的影响。

Method: 通过理论分析Softmax PG与线性函数近似（Lin-SPG）的组合，证明了近似误差对Lin-SPG全局收敛的无关性，并识别了特征表示需满足的充要条件以保证其渐进全局收敛。

Result: 研究发现，对于Lin-SPG，近似误差与其全局收敛性无关。在确定的特征条件下：1) 使用特定学习率，$T$次迭代的Lin-SPG能以 $O(1/T)$ 速率收敛到最优策略；2) 使用任意恒定学习率，Lin-SPG也能渐进全局收敛到最优策略。

Conclusion: 结论指出，在线性Softmax策略梯度（Lin-SPG）中，近似误差并非全局收敛的关键因素。当特征表示满足特定条件时，Lin-SPG可以实现全局收敛，且对学习率选择具有一定鲁棒性。

Abstract: Policy gradient (PG) methods have played an essential role in the empirical
successes of reinforcement learning. In order to handle large state-action
spaces, PG methods are typically used with function approximation. In this
setting, the approximation error in modeling problem-dependent quantities is a
key notion for characterizing the global convergence of PG methods. We focus on
Softmax PG with linear function approximation (referred to as
$\texttt{Lin-SPG}$) and demonstrate that the approximation error is irrelevant
to the algorithm's global convergence even for the stochastic bandit setting.
Consequently, we first identify the necessary and sufficient conditions on the
feature representation that can guarantee the asymptotic global convergence of
$\texttt{Lin-SPG}$. Under these feature conditions, we prove that $T$
iterations of $\texttt{Lin-SPG}$ with a problem-specific learning rate result
in an $O(1/T)$ convergence to the optimal policy. Furthermore, we prove that
$\texttt{Lin-SPG}$ with any arbitrary constant learning rate can ensure
asymptotic global convergence to the optimal policy.

</details>


### [168] [Improving the Reproducibility of Deep Learning Software: An Initial Investigation through a Case Study Analysis](https://arxiv.org/abs/2505.03165)
*Nikita Ravi,Abhinav Goel,James C. Davis,George K. Thiruvathukal*

Main category: cs.LG

TL;DR: 本文针对深度学习模型复现性难题，提出了一套系统性方法和指南，并通过案例研究进行阐释，旨在提高研究结果的可靠性和可部署性。


<details>
  <summary>Details</summary>
Motivation: 深度学习成果显著，但其结果难以复现的问题日益突出（超70%研究者无法复现他人实验，超50%无法复现自身实验），这严重影响了软件开发的可靠性和有效性。

Method: 提出一种系统性方法来分析和改进深度学习模型的可复现性，包括：建立复制原始软件环境的方法论，实施端到端训练和测试算法，公开架构设计，增强数据处理和训练流程的透明度，并通过案例研究展示相关模式与反模式。同时进行敏感性分析。

Result: 通过实施这些策略和指南，旨在弥合研究与实践之间的差距，使深度学习的创新能够被有效地复现并在软件中部署。

Conclusion: 采用系统性方法和明确的指南可以显著提高深度学习模型的可复现性，从而促进创新成果在实际应用中的可靠部署。

Abstract: The field of deep learning has witnessed significant breakthroughs, spanning
various applications, and fundamentally transforming current software
capabilities. However, alongside these advancements, there have been increasing
concerns about reproducing the results of these deep learning methods. This is
significant because reproducibility is the foundation of reliability and
validity in software development, particularly in the rapidly evolving domain
of deep learning. The difficulty of reproducibility may arise due to several
reasons, including having differences from the original execution environment,
incompatible software libraries, proprietary data and source code, lack of
transparency, and the stochastic nature in some software. A study conducted by
the Nature journal reveals that more than 70% of researchers failed to
reproduce other researchers experiments and over 50% failed to reproduce their
own experiments. Irreproducibility of deep learning poses significant
challenges for researchers and practitioners. To address these concerns, this
paper presents a systematic approach at analyzing and improving the
reproducibility of deep learning models by demonstrating these guidelines using
a case study. We illustrate the patterns and anti-patterns involved with these
guidelines for improving the reproducibility of deep learning models. These
guidelines encompass establishing a methodology to replicate the original
software environment, implementing end-to-end training and testing algorithms,
disclosing architectural designs, and enhancing transparency in data processing
and training pipelines. We also conduct a sensitivity analysis to understand
the model performance across diverse conditions. By implementing these
strategies, we aim to bridge the gap between research and practice, so that
innovations in deep learning can be effectively reproduced and deployed within
software.

</details>


### [169] [Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2505.03172)
*Caleb Chuck,Fan Feng,Carl Qi,Chang Shi,Siddhant Agarwal,Amy Zhang,Scott Niekum*

Main category: cs.LG

TL;DR: 提出HInt方法，通过结合物体交互信息与事后重标记（hindsight relabeling），提升目标导向强化学习在物体中心任务中的样本效率。


<details>
  <summary>Details</summary>
Motivation: 传统的事后重标记方法在物体中心领域（如机器人操纵）中表现不佳，因为它倾向于奖励与目标物体无交互的轨迹，这使得学习过程复杂化并可能导致失败。

Method: 1. 提出Hindsight Relabeling using Interactions (HInt)框架，将物体交互信息融入事后重标记。 2. 基于“零反事实”（null counterfactual）概念定义物体交互：如果移除“原因物体”会导致“目标物体”的转换动态发生变化，则认为两者存在交互。 3. 开发Null Counterfactual Interaction Inference (NCII) 算法，使用学习模型通过“置零”（nulling）操作来推断这种交互。

Result: NCII 算法在简单线性动态域以及 Robosuite、Robot Air Hockey 和 Franka Kitchen 等动态机器人域中显著提高了交互推断的准确性。HInt 方法将下游强化学习的样本效率提高了多达4倍。

Conclusion: 通过引入基于零反事实的交互定义和NCII推断方法，HInt能够有效地将有意义的物体交互信息融入事后重标记，从而显著提高物体中心强化学习任务的样本效率和性能。

Abstract: Hindsight relabeling is a powerful tool for overcoming sparsity in
goal-conditioned reinforcement learning (GCRL), especially in certain domains
such as navigation and locomotion. However, hindsight relabeling can struggle
in object-centric domains. For example, suppose that the goal space consists of
a robotic arm pushing a particular target block to a goal location. In this
case, hindsight relabeling will give high rewards to any trajectory that does
not interact with the block. However, these behaviors are only useful when the
object is already at the goal -- an extremely rare case in practice. A dataset
dominated by these kinds of trajectories can complicate learning and lead to
failures. In object-centric domains, one key intuition is that meaningful
trajectories are often characterized by object-object interactions such as
pushing the block with the gripper. To leverage this intuition, we introduce
Hindsight Relabeling using Interactions (HInt), which combines interactions
with hindsight relabeling to improve the sample efficiency of downstream RL.
However because interactions do not have a consensus statistical definition
tractable for downstream GCRL, we propose a definition of interactions based on
the concept of null counterfactual: a cause object is interacting with a target
object if, in a world where the cause object did not exist, the target object
would have different transition dynamics. We leverage this definition to infer
interactions in Null Counterfactual Interaction Inference (NCII), which uses a
"nulling'' operation with a learned model to infer interactions. NCII is able
to achieve significantly improved interaction inference accuracy in both simple
linear dynamics domains and dynamic robotic domains in Robosuite, Robot Air
Hockey, and Franka Kitchen and HInt improves sample efficiency by up to 4x.

</details>


### [170] [RADE: Learning Risk-Adjustable Driving Environment via Multi-Agent Conditional Diffusion](https://arxiv.org/abs/2505.03178)
*Jiawei Wang,Xintao Yan,Yao Mu,Haowei Sun,Zhong Cao,Henry X. Liu*

Main category: cs.LG

TL;DR: 提出了一种名为RADE的多智能体扩散框架，用于生成统计上真实且风险可调的交通场景，以高效测试自动驾驶汽车。


<details>
  <summary>Details</summary>
Motivation: 现有生成安全关键场景的方法通常依赖于操纵单个车辆的轨迹，这往往牺牲了场景的真实性和可扩展性，难以高效评估自动驾驶系统。

Method: 本文提出了风险可调驾驶环境（RADE）。该框架基于多智能体扩散架构，联合建模环境中所有智能体的行为，并根据一个代理风险度量来调整它们的轨迹。RADE直接从数据中学习风险条件下的行为，并引入了标记化的动态检查模块以确保物理合理性。

Result: 在真实的rounD数据集上进行的验证表明，RADE能够在不同风险水平下保持统计真实性，并且随着期望风险水平的提高，安全关键事件发生的可能性也自然增加。

Conclusion: RADE展现了作为一种可扩展且真实的自动驾驶汽车安全评估工具的潜力。

Abstract: Generating safety-critical scenarios in high-fidelity simulations offers a
promising and cost-effective approach for efficient testing of autonomous
vehicles. Existing methods typically rely on manipulating a single vehicle's
trajectory through sophisticated designed objectives to induce adversarial
interactions, often at the cost of realism and scalability. In this work, we
propose the Risk-Adjustable Driving Environment (RADE), a simulation framework
that generates statistically realistic and risk-adjustable traffic scenes.
Built upon a multi-agent diffusion architecture, RADE jointly models the
behavior of all agents in the environment and conditions their trajectories on
a surrogate risk measure. Unlike traditional adversarial methods, RADE learns
risk-conditioned behaviors directly from data, preserving naturalistic
multi-agent interactions with controllable risk levels. To ensure physical
plausibility, we incorporate a tokenized dynamics check module that efficiently
filters generated trajectories using a motion vocabulary. We validate RADE on
the real-world rounD dataset, demonstrating that it preserves statistical
realism across varying risk levels and naturally increases the likelihood of
safety-critical events as the desired risk level grows up. Our results
highlight RADE's potential as a scalable and realistic tool for AV safety
evaluation.

</details>


### [171] [VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making](https://arxiv.org/abs/2505.03181)
*Jake Grigsby,Yuke Zhu,Michael Ryoo,Juan Carlos Niebles*

Main category: cs.LG

TL;DR: 本文提出了一种离线到在线强化学习（RL）方法，用于微调视觉语言模型（VLM）以执行多模态代理任务，使其能够自我改进并从低质量数据中学习。


<details>
  <summary>Details</summary>
Motivation: 当前的开源视觉语言模型（VLM）在处理多模态数据方面扩展了大型语言模型（LLM），但在代理任务所需的特定技能（如遵循严格的输出语法）方面表现不佳，更多地侧重于开放式问答。克服这些限制通常需要针对特定任务的专家演示进行监督微调（SFT）。

Method: 研究者探索了一种离策略强化学习（off-policy RL）解决方案。该方法旨在通过从模型自身或其他更强大模型的成功和不成功决策中学习，来微调VLM以适应代理任务，同时保持类似SFT的稳定性和简单性，并允许代理自我改进和从低质量数据集中学习。

Result: 该技术在三个多模态代理领域中，通过两个开源视觉语言模型得到了验证。

Conclusion: 所提出的离线到在线强化学习方法为VLM在代理任务中的微调提供了新途径，使其能够从自身经验和不完美数据中学习，从而提升性能。

Abstract: Recent research looks to harness the general knowledge and reasoning of large
language models (LLMs) into agents that accomplish user-specified goals in
interactive environments. Vision-language models (VLMs) extend LLMs to
multi-modal data and provide agents with the visual reasoning necessary for new
applications in areas such as computer automation. However, agent tasks
emphasize skills where accessible open-weight VLMs lag behind their LLM
equivalents. For example, VLMs are less capable of following an environment's
strict output syntax requirements and are more focused on open-ended question
answering. Overcoming these limitations requires supervised fine-tuning (SFT)
on task-specific expert demonstrations. Our work approaches these challenges
from an offline-to-online reinforcement learning (RL) perspective. RL lets us
fine-tune VLMs to agent tasks while learning from the unsuccessful decisions of
our own model or more capable (larger) models. We explore an off-policy RL
solution that retains the stability and simplicity of the widely used SFT
workflow while allowing our agent to self-improve and learn from low-quality
datasets. We demonstrate this technique with two open-weight VLMs across three
multi-modal agent domains.

</details>


### [172] [Convergence Of Consistency Model With Multistep Sampling Under General Data Assumptions](https://arxiv.org/abs/2505.03194)
*Yiding Chen,Yiyi Zhang,Owen Oertell,Wen Sun*

Main category: cs.LG

TL;DR: 该研究分析了一致性模型在近似自洽条件下的收敛性，证明其生成的样本在特定条件下能接近目标分布，并揭示了多步采样的优势。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在数据生成上表现优异，但其迭代采样过程计算成本高昂。一致性模型被提出用于实现快速单步生成，但其理论收敛性需要进一步研究。

Method: 通过理论分析研究一致性模型的收敛性，条件是自洽属性在训练分布下近似成立。该分析仅需温和的数据假设，并适用于一系列前向过程。

Result: 研究表明：1) 当目标数据分布具有有界支撑或尾部衰减足够快时，一致性模型生成的样本在Wasserstein距离上接近目标分布；2) 当目标分布满足某些平滑性假设时，通过额外的扰动平滑步骤，生成的样本在总变差距离上接近目标分布。此外，通过两个案例研究展示了多步采样的益处。

Conclusion: 该研究为一致性模型的收敛性提供了理论保证，证明了其在特定条件下生成高质量样本的能力，并支持了多步采样在提升样本质量方面的有效性。

Abstract: Diffusion models accomplish remarkable success in data generation tasks
across various domains. However, the iterative sampling process is
computationally expensive. Consistency models are proposed to learn consistency
functions to map from noise to data directly, which allows one-step fast data
generation and multistep sampling to improve sample quality. In this paper, we
study the convergence of consistency models when the self-consistency property
holds approximately under the training distribution. Our analysis requires only
mild data assumption and applies to a family of forward processes. When the
target data distribution has bounded support or has tails that decay
sufficiently fast, we show that the samples generated by the consistency model
are close to the target distribution in Wasserstein distance; when the target
distribution satisfies some smoothness assumption, we show that with an
additional perturbation step for smoothing, the generated samples are close to
the target distribution in total variation distance. We provide two case
studies with commonly chosen forward processes to demonstrate the benefit of
multistep sampling.

</details>


### [173] [Transformers for Learning on Noisy and Task-Level Manifolds: Approximation and Generalization Insights](https://arxiv.org/abs/2505.03205)
*Zhaiming Shen,Alex Havrilla,Rongjie Lai,Alexander Cloninger,Wenjing Liao*

Main category: cs.LG

TL;DR: 该研究为 Transformer 在处理带噪声的流形数据上的回归任务提供了理论基础，证明其性能依赖于数据的内在维度。


<details>
  <summary>Details</summary>
Motivation: 尽管经验表明 Transformer 的性能与数据的内在维度相关，但其在处理带噪声的低维结构数据时的理论理解尚不充分。

Method: 通过分析 Transformer 在流形上带噪声输入数据的回归任务中的性能来建立理论基础。假设输入数据位于流形的管状邻域，真实函数依赖于噪声数据在流形上的投影。同时，构建了 Transformer 表示基本算术运算的新颖证明技巧。

Result: 研究证明了 Transformer 的近似误差和泛化误差关键性地依赖于流形的内在维度。结果表明，即使输入数据受到高维噪声干扰，Transformer 仍能利用学习任务中的低复杂度结构。

Conclusion: 该研究为 Transformer 处理具有低维结构的噪声数据提供了理论支持，证明了其能够依赖数据的内在维度进行学习，并能有效应对高维噪声。其新颖的证明技术也具有独立价值。

Abstract: Transformers serve as the foundational architecture for large language and
video generation models, such as GPT, BERT, SORA and their successors.
Empirical studies have demonstrated that real-world data and learning tasks
exhibit low-dimensional structures, along with some noise or measurement error.
The performance of transformers tends to depend on the intrinsic dimension of
the data/tasks, though theoretical understandings remain largely unexplored for
transformers. This work establishes a theoretical foundation by analyzing the
performance of transformers for regression tasks involving noisy input data on
a manifold. Specifically, the input data are in a tubular neighborhood of a
manifold, while the ground truth function depends on the projection of the
noisy data onto the manifold. We prove approximation and generalization errors
which crucially depend on the intrinsic dimension of the manifold. Our results
demonstrate that transformers can leverage low-complexity structures in
learning task even when the input data are perturbed by high-dimensional noise.
Our novel proof technique constructs representations of basic arithmetic
operations by transformers, which may hold independent interest.

</details>


### [174] [Partial Label Clustering](https://arxiv.org/abs/2505.03207)
*Yutong Xie,Fuchao Yang,Yuheng Jia*

Main category: cs.LG

TL;DR: 本文首次研究了偏标记聚类问题，提出了一种联合模型，通过利用有限的偏标记信息来改进聚类性能。


<details>
  <summary>Details</summary>
Motivation: 传统的聚类方法在处理每个样本对应一组候选标签（其中只有一个是真实标签）的偏标记学习（PLL）场景时存在不足。本文旨在首次探索如何利用这些有限的偏标记信息来提升聚类效果。

Method: 首先，基于特征空间中的样本关系构建权重矩阵；然后，基于该权重矩阵对候选标签进行消歧以估计真实标签；接着，根据消歧结果构建“必须链接”(must-link)和“不能链接”(cannot-link)约束集；之后，采用对抗性先验提升的双图学习方法传播初始约束；最后，将权重矩阵构建、标签消歧和成对约束传播整合到一个联合模型中以实现相互增强。

Result: 理论上证明了更好的消歧标签矩阵有助于提高聚类性能。全面的实验表明，该方法与最先进的约束聚类方法相比具有优越性能，并且在只有少量样本被标注时，其性能优于传统的偏标记学习（PLL）和半监督偏标记学习方法。

Conclusion: 本文提出的偏标记聚类方法，通过联合权重矩阵构建、标签消歧和成对约束传播，能够有效利用有限的偏标记信息，显著提升聚类性能，特别是在标注样本有限的情况下。

Abstract: Partial label learning (PLL) is a significant weakly supervised learning
framework, where each training example corresponds to a set of candidate labels
and only one label is the ground-truth label. For the first time, this paper
investigates the partial label clustering problem, which takes advantage of the
limited available partial labels to improve the clustering performance.
Specifically, we first construct a weight matrix of examples based on their
relationships in the feature space and disambiguate the candidate labels to
estimate the ground-truth label based on the weight matrix. Then, we construct
a set of must-link and cannot-link constraints based on the disambiguation
results. Moreover, we propagate the initial must-link and cannot-link
constraints based on an adversarial prior promoted dual-graph learning
approach. Finally, we integrate weight matrix construction, label
disambiguation, and pairwise constraints propagation into a joint model to
achieve mutual enhancement. We also theoretically prove that a better
disambiguated label matrix can help improve clustering performance.
Comprehensive experiments demonstrate our method realizes superior performance
when comparing with state-of-the-art constrained clustering methods, and
outperforms PLL and semi-supervised PLL methods when only limited samples are
annotated. The code is publicly available at https://github.com/xyt-ml/PLC.

</details>


### [175] [DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning](https://arxiv.org/abs/2505.03209)
*Borui Wang,Kathleen McKeown,Rex Ying*

Main category: cs.LG

TL;DR: 该研究提出了一种名为 DYSTIL 的新框架，利用大型语言模型 (LLM) 为强化学习生成文本策略，以克服现有从专家演示中学习的方法在泛化性、样本效率和可解释性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的从专家演示中学习强化学习的方法（如行为克隆加RL训练）存在泛化能力差、样本效率低和模型可解释性不佳等问题。

Method: 提出了一种名为 DYSTIL (Dynamic Strategy Induction with LLMs for reinforcement learning) 的新框架。该框架利用大型语言模型 (LLM) 根据优势估计和专家演示动态生成文本策略，并通过策略优化将这些策略逐步内化到强化学习智能体中。

Result: 在 Minigrid 和 BabyAI 等挑战性强化学习环境中的测试表明，DYSTIL 在平均成功率上比现有顶尖基线方法高出 17.75%，并且在学习过程中具有更高的样本效率。同时，它还提供了观察和解释策略演变的文本通道。

Conclusion: DYSTIL 框架通过结合大型语言模型的推理能力来动态生成和内化策略，有效提升了强化学习智能体的泛化能力、样本效率和可解释性，克服了现有方法的局限性。

Abstract: Reinforcement learning from expert demonstrations has long remained a
challenging research problem, and existing state-of-the-art methods using
behavioral cloning plus further RL training often suffer from poor
generalization, low sample efficiency, and poor model interpretability.
Inspired by the strong reasoning abilities of large language models (LLMs), we
propose a novel strategy-based reinforcement learning framework integrated with
LLMs called DYnamic STrategy Induction with Llms for reinforcement learning
(DYSTIL) to overcome these limitations. DYSTIL dynamically queries a
strategy-generating LLM to induce textual strategies based on advantage
estimations and expert demonstrations, and gradually internalizes induced
strategies into the RL agent through policy optimization to improve its
performance through boosting policy generalization and enhancing sample
efficiency. It also provides a direct textual channel to observe and interpret
the evolution of the policy's underlying strategies during training. We test
DYSTIL over challenging RL environments from Minigrid and BabyAI, and
empirically demonstrate that DYSTIL significantly outperforms state-of-the-art
baseline methods by 17.75% in average success rate while also enjoying higher
sample efficiency during the learning process.

</details>


### [176] [Joint Resource Management for Energy-efficient UAV-assisted SWIPT-MEC: A Deep Reinforcement Learning Approach](https://arxiv.org/abs/2505.03230)
*Yue Chen,Hui Kang,Jiahui Li,Geng Su,Boxiong Wang,Jiacheng Wang,Cong Liang,Shuang Liang,Dusit Niyato*

Main category: cs.LG

TL;DR: 本文提出一种无人机辅助的移动边缘计算系统，利用定向天线为物联网终端提供计算与能量，并通过改进的SAC算法优化能源效率和电池续航。


<details>
  <summary>Details</summary>
Motivation: 解决偏远或灾害地区6G物联网因缺乏地面设施难以部署无线信息与能量同传（SWIPT）技术的问题，并应对无人机辅助系统中能源、电量与计算资源分配的复杂权衡。

Method: 构建一个采用定向天线的无人机辅助移动边缘计算（MEC）系统。将系统能效和终端电池可持续性的双目标优化问题转化为马尔可夫决策过程（MDP），并提出一种改进的带有动作简化机制的软演员-评论家（SAC）算法进行求解。

Result: 仿真结果显示，所提方法在多种场景下均优于基线，实现了高效能源管理和高计算性能，并在复杂环境中展现出强大的泛化能力，验证了设计的边界惩罚和充电奖励机制的有效性。

Conclusion: 本文提出的基于改进SAC算法的无人机辅助MEC系统，能够有效解决偏远地区物联网设备的计算和能量需求，在提升能源效率和保障终端电池可持续性方面表现优异，并具备良好的场景适应性。

Abstract: The integration of simultaneous wireless information and power transfer
(SWIPT) technology in 6G Internet of Things (IoT) networks faces significant
challenges in remote areas and disaster scenarios where ground infrastructure
is unavailable. This paper proposes a novel unmanned aerial vehicle
(UAV)-assisted mobile edge computing (MEC) system enhanced by directional
antennas to provide both computational resources and energy support for ground
IoT terminals. However, such systems require multiple trade-off policies to
balance UAV energy consumption, terminal battery levels, and computational
resource allocation under various constraints, including limited UAV battery
capacity, non-linear energy harvesting characteristics, and dynamic task
arrivals. To address these challenges comprehensively, we formulate a
bi-objective optimization problem that simultaneously considers system energy
efficiency and terminal battery sustainability. We then reformulate this
non-convex problem with a hybrid solution space as a Markov decision process
(MDP) and propose an improved soft actor-critic (SAC) algorithm with an action
simplification mechanism to enhance its convergence and generalization
capabilities. Simulation results have demonstrated that our proposed approach
outperforms various baselines in different scenarios, achieving efficient
energy management while maintaining high computational performance.
Furthermore, our method shows strong generalization ability across different
scenarios, particularly in complex environments, validating the effectiveness
of our designed boundary penalty and charging reward mechanisms.

</details>


### [177] [MDPs with a State Sensing Cost](https://arxiv.org/abs/2505.03280)
*Vansh Kapoor,Jayakrishnan Nair*

Main category: cs.LG

TL;DR: 论文研究在状态感知有成本的序列决策中，智能体何时感知状态以平衡决策价值与感知成本的问题。


<details>
  <summary>Details</summary>
Motivation: 在许多实际的序列决策问题中，跟踪环境状态会产生感知、通信或计算成本，因此智能体需要权衡获取状态信息带来的价值与付出的成本。

Method: 将问题建模为一个带有额外感知成本的期望折扣成本马尔可夫决策过程 (MDP)，其中智能体可以选择感知状态或在不感知状态（“盲目”）的情况下行动。由于最优策略难解，论文研究了一类限制连续盲目行动次数的策略，并设计了一种基于策略改进的高效启发式算法。

Result: 论文界定了在受限策略类别下最优策略的次优性差距。所设计的启发式算法在实践中表现接近最优策略，并通过数值案例研究与现有技术进行了基准比较。

Conclusion: 本文为带感知成本的序列决策问题提供了一个理论分析框架和一种计算高效的启发式算法，该算法在实践中表现良好，能够有效平衡决策收益和感知成本。

Abstract: In many practical sequential decision-making problems, tracking the state of
the environment incurs a sensing/communication/computation cost. In these
settings, the agent's interaction with its environment includes the additional
component of deciding $\textit{when}$ to sense the state, in a manner that
balances the value associated with optimal (state-specific) actions and the
cost of sensing. We formulate this as an expected discounted cost Markov
Decision Process (MDP), wherein the agent incurs an additional cost for sensing
its next state, but has the option to take actions while remaining 'blind' to
the system state.
  We pose this problem as a classical discounted cost MDP with an expanded
(countably infinite) state space. While computing the optimal policy for this
MDP is intractable in general, we bound the sub-optimality gap associated with
optimal policies in a restricted class, where the number of consecutive
non-sensing (a.k.a., blind) actions is capped. We also design a computationally
efficient heuristic algorithm based on policy improvement, which in practice
performs close to the optimal policy. Finally, we benchmark against the state
of the art via a numerical case study.

</details>


### [178] [Physics-inspired Energy Transition Neural Network for Sequence Learning](https://arxiv.org/abs/2505.03281)
*Zhou Wu,Junyi An,Baile Xu,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: 提出一种受物理启发的循环神经网络PETNN，在长序列任务上优于Transformer且复杂度更低，展示了RNN在序列建模领域的潜力。


<details>
  <summary>Details</summary>
Motivation: Transformer在序列建模中表现优越，但其长程依赖捕获能力主要源于配对建模而非固有的序列语义理解。本研究旨在重新评估纯RNN的长程学习机制。

Method: 受物理能量转换模型启发，提出一种名为“物理启发电能转换神经网络”(PETNN)的有效循环结构。

Result: 实验证明，PETNN的记忆机制能有效存储长程依赖信息，并在多种序列任务上超越了基于Transformer的方法，同时具有显著更低的计算复杂度。

Conclusion: 本研究提出了一种优化的基础循环架构，并强调了在当前由Transformer主导的领域中开发有效循环神经网络的潜力。

Abstract: Recently, the superior performance of Transformers has made them a more
robust and scalable solution for sequence modeling than traditional recurrent
neural networks (RNNs). However, the effectiveness of Transformer in capturing
long-term dependencies is primarily attributed to their comprehensive
pair-modeling process rather than inherent inductive biases toward sequence
semantics. In this study, we explore the capabilities of pure RNNs and reassess
their long-term learning mechanisms. Inspired by the physics energy transition
models that track energy changes over time, we propose a effective recurrent
structure called the``Physics-inspired Energy Transition Neural Network"
(PETNN). We demonstrate that PETNN's memory mechanism effectively stores
information over long-term dependencies. Experimental results indicate that
PETNN outperforms transformer-based methods across various sequence tasks.
Furthermore, owing to its recurrent nature, PETNN exhibits significantly lower
complexity. Our study presents an optimal foundational recurrent architecture
and highlights the potential for developing effective recurrent neural networks
in fields currently dominated by Transformer.

</details>


### [179] [Unraveling the Rainbow: can value-based methods schedule?](https://arxiv.org/abs/2505.03323)
*Arthur Corrêa,Alexandre Jesus,Cristóvão Silva,Samuel Moniz*

Main category: cs.LG

TL;DR: 该研究评估了基于价值的深度强化学习算法在解决复杂组合优化问题（如作业车间调度）时的性能，发现它们可以匹敌甚至超越流行的基于策略的方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基于价值的深度强化学习方法在其他领域取得了成功，但在组合优化领域，研究者主要关注基于策略的方法，常常忽视了基于价值算法的潜力。

Method: 研究对基于价值的算法（包括深度Q网络及其几种高级扩展）在两个复杂的组合优化问题（作业车间调度和柔性作业车间调度问题）上进行了全面的实证评估。

Result: 研究结果表明，几种基于价值的方法在性能上可以与广泛采用的近端策略优化算法（PPO）相媲美，甚至在某些情况下表现更优。

Conclusion: 基于价值的强化学习策略在组合优化问题上具有巨大潜力，值得组合优化研究社区给予更多关注。

Abstract: Recently, deep reinforcement learning has emerged as a promising approach for
solving complex combinatorial optimization problems. Broadly, deep
reinforcement learning methods fall into two categories: policy-based and
value-based. While value-based approaches have achieved notable success in
domains such as the Arcade Learning Environment, the combinatorial optimization
community has predominantly favored policy-based methods, often overlooking the
potential of value-based algorithms. In this work, we conduct a comprehensive
empirical evaluation of value-based algorithms, including the deep q-network
and several of its advanced extensions, within the context of two complex
combinatorial problems: the job-shop and the flexible job-shop scheduling
problems, two fundamental challenges with multiple industrial applications. Our
results challenge the assumption that policy-based methods are inherently
superior for combinatorial optimization. We show that several value-based
approaches can match or even outperform the widely adopted proximal policy
optimization algorithm, suggesting that value-based strategies deserve greater
attention from the combinatorial optimization community. Our code is openly
available at: https://github.com/AJ-Correa/Unraveling-the-Rainbow.

</details>


### [180] [Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/abs/2505.03335)
*Andrew Zhao,Yiran Wu,Yang Yue,Tong Wu,Quentin Xu,Yang Yue,Matthieu Lin,Shenzhi Wang,Qingyun Wu,Zilong Zheng,Gao Huang*

Main category: cs.LG

TL;DR: 提出一种名为“Absolute Zero”的新范式，模型能自我生成任务并解决它们以提升推理能力，无需任何外部数据，并取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的可验证奖励强化学习（RLVR）方法，即使是零样本设置（zero setting），也依赖人工策划的问答数据进行训练，这限制了可扩展性，并可能对未来超智能系统的学习潜力造成限制。

Method: 提出“Absolute Zero” RLVR新范式，并引入“Absolute Zero Reasoner (AZR)”系统。AZR通过自我提议代码推理任务，并使用代码执行器来验证任务和答案，以此作为可验证奖励的统一来源，引导开放式且有根据的学习，整个过程不依赖任何外部数据。

Result: 尽管完全不使用外部数据进行训练，AZR在编码和数学推理任务上取得了全面的SOTA（State-of-the-Art）性能，超越了依赖数万个人工策划领域内样本的现有零样本模型。此外，AZR能有效应用于不同模型规模和模型类别。

Conclusion: “Absolute Zero”范式（以AZR为例）展示了一条在不依赖外部人工数据的情况下，让大型语言模型自我改进推理能力的有前景的路径，为开放式学习提供了可能，并具有良好的可扩展性和兼容性。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown promise in
enhancing the reasoning capabilities of large language models by learning
directly from outcome-based rewards. Recent RLVR works that operate under the
zero setting avoid supervision in labeling the reasoning process, but still
depend on manually curated collections of questions and answers for training.
The scarcity of high-quality, human-produced examples raises concerns about the
long-term scalability of relying on human supervision, a challenge already
evident in the domain of language model pretraining. Furthermore, in a
hypothetical future where AI surpasses human intelligence, tasks provided by
humans may offer limited learning potential for a superintelligent system. To
address these concerns, we propose a new RLVR paradigm called Absolute Zero, in
which a single model learns to propose tasks that maximize its own learning
progress and improves reasoning by solving them, without relying on any
external data. Under this paradigm, we introduce the Absolute Zero Reasoner
(AZR), a system that self-evolves its training curriculum and reasoning ability
by using a code executor to both validate proposed code reasoning tasks and
verify answers, serving as an unified source of verifiable reward to guide
open-ended yet grounded learning. Despite being trained entirely without
external data, AZR achieves overall SOTA performance on coding and mathematical
reasoning tasks, outperforming existing zero-setting models that rely on tens
of thousands of in-domain human-curated examples. Furthermore, we demonstrate
that AZR can be effectively applied across different model scales and is
compatible with various model classes.

</details>


### [181] [Geospatial Mechanistic Interpretability of Large Language Models](https://arxiv.org/abs/2505.03368)
*Stef De Sabbata,Stefano Mizzaro,Kevin Roitero*

Main category: cs.LG

TL;DR: 本研究提出了一个新颖的地理空间机制可解释性框架，利用空间分析来反向工程大型语言模型（LLMs）处理地理信息的方式。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在地理学中展现出潜力，但其内部如何处理地理信息的机制仍知之甚少。本研究旨在增进对此的理解。

Method: 研究首先概述了使用探针揭示LLMs内部结构的方法，接着介绍了机制可解释性领域，包括叠加假说和稀疏自动编码器在将LLMs的多义性内部表示分解为更易解释的单义性特征中的作用。实验中，使用空间自相关分析地名特征，以揭示其与地理位置相关的空间模式。

Result: 实验表明，从地名中提取的特征显示出与其地理位置相关的空间模式，从而为理解LLMs如何处理地理信息提供了见解。

Conclusion: 该框架有助于塑造地理学中基础模型的研究和应用。

Abstract: Large Language Models (LLMs) have demonstrated unprecedented capabilities
across various natural language processing tasks. Their ability to process and
generate viable text and code has made them ubiquitous in many fields, while
their deployment as knowledge bases and "reasoning" tools remains an area of
ongoing research. In geography, a growing body of literature has been focusing
on evaluating LLMs' geographical knowledge and their ability to perform spatial
reasoning. However, very little is still known about the internal functioning
of these models, especially about how they process geographical information.
  In this chapter, we establish a novel framework for the study of geospatial
mechanistic interpretability - using spatial analysis to reverse engineer how
LLMs handle geographical information. Our aim is to advance our understanding
of the internal representations that these complex models generate while
processing geographical information - what one might call "how LLMs think about
geographic information" if such phrasing was not an undue anthropomorphism.
  We first outline the use of probing in revealing internal structures within
LLMs. We then introduce the field of mechanistic interpretability, discussing
the superposition hypothesis and the role of sparse autoencoders in
disentangling polysemantic internal representations of LLMs into more
interpretable, monosemantic features. In our experiments, we use spatial
autocorrelation to show how features obtained for placenames display spatial
patterns related to their geographic location and can thus be interpreted
geospatially, providing insights into how these models process geographical
information. We conclude by discussing how our framework can help shape the
study and use of foundation models in geography.

</details>


### [182] [SPAP: Structured Pruning via Alternating Optimization and Penalty Methods](https://arxiv.org/abs/2505.03373)
*Hanyu Hu,Xiaoming Yuan*

Main category: cs.LG

TL;DR: 该论文提出了一种名为SPAP的新型高效结构化剪枝框架，通过交替优化和惩罚方法，在保持性能的同时减少大型语言模型的计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的部署受限于其巨大的计算和内存需求。现有的结构化剪枝方法存在性能下降、依赖启发式指标或需要昂贵微调等问题。

Method: 提出了SPAP（Structured Pruning via Alternating Optimization and Penalty Methods）框架。该方法将剪枝问题构建为一个混合整数优化模型，使用惩罚方法进行剪枝决策以最小化剪枝误差，并引入交替最小化算法进行权重更新和性能恢复。

Result: 在OPT、LLaMA-3/3.1/3.2和Qwen2.5模型上的实验表明，SPAP优于现有SOTA方法，在30%稀疏度下实现了1.29倍的线性推理加速和相应的内存减少。

Conclusion: 该研究提供了一种实用的、基于优化的解决方案，用于剪枝大型语言模型，同时保持模型性能。

Abstract: The deployment of large language models (LLMs) is often constrained by their
substantial computational and memory demands. While structured pruning presents
a viable approach by eliminating entire network components, existing methods
suffer from performance degradation, reliance on heuristic metrics, or
expensive finetuning. To address these challenges, we propose SPAP (Structured
Pruning via Alternating Optimization and Penalty Methods), a novel and
efficient structured pruning framework for LLMs grounded in optimization
theory. SPAP formulates the pruning problem through a mixed-integer
optimization model, employs a penalty method that effectively makes pruning
decisions to minimize pruning errors, and introduces an alternating
minimization algorithm tailored to the splittable problem structure for
efficient weight updates and performance recovery. Extensive experiments on
OPT, LLaMA-3/3.1/3.2, and Qwen2.5 models demonstrate SPAP's superiority over
state-of-the-art methods, delivering linear inference speedups (1.29$\times$ at
30% sparsity) and proportional memory reductions. Our work offers a practical,
optimization-driven solution for pruning LLMs while preserving model
performance.

</details>


### [183] [Physics-informed neural network estimation of active material properties in time-dependent cardiac biomechanical models](https://arxiv.org/abs/2505.03382)
*Matthias Höfler,Francesco Regazzoni,Stefano Pagani,Elias Karabelas,Christoph Augustin,Gundolf Haase,Gernot Plank,Federica Caforio*

Main category: cs.LG

TL;DR: 本研究通过硅基研究，应用物理信息神经网络（PINNs）从医学影像数据推断心脏主动收缩参数，并成功重建了高分辨率主动应力场。


<details>
  <summary>Details</summary>
Motivation: 准确评估心脏主动应力参数对于理解心肌功能至关重要，但在临床环境中，尤其当仅有医学影像（如位移和应变数据）可用时，这一评估非常困难。

Method: 通过硅基研究，采用物理信息神经网络（PINNs）推断主动收缩参数。具体方法包括：使用两个神经网络分别参数化状态场和参数场，构建能量最小化问题来优化网络参数，并改进了PINN学习算法（如自适应加权、正则化、傅里叶特征和特定网络结构）。

Result: 该方法能够在存在噪声的情况下，以高空间分辨率重建主动应力场。此外，该方法成功应用于心肌组织不均匀性的表征和纤维化疤痕的检测。

Conclusion: 该方法为显著改善与心脏纤维化相关的心脏疾病的诊断、治疗计划和管理开辟了新途径。

Abstract: Active stress models in cardiac biomechanics account for the mechanical
deformation caused by muscle activity, thus providing a link between the
electrophysiological and mechanical properties of the tissue. The accurate
assessment of active stress parameters is fundamental for a precise
understanding of myocardial function but remains difficult to achieve in a
clinical setting, especially when only displacement and strain data from
medical imaging modalities are available. This work investigates, through an
in-silico study, the application of physics-informed neural networks (PINNs)
for inferring active contractility parameters in time-dependent cardiac
biomechanical models from these types of imaging data. In particular, by
parametrising the sought state and parameter field with two neural networks,
respectively, and formulating an energy minimisation problem to search for the
optimal network parameters, we are able to reconstruct in various settings
active stress fields in the presence of noise and with a high spatial
resolution. To this end, we also advance the vanilla PINN learning algorithm
with the use of adaptive weighting schemes, ad-hoc regularisation strategies,
Fourier features, and suitable network architectures. In addition, we
thoroughly analyse the influence of the loss weights in the reconstruction of
active stress parameters. Finally, we apply the method to the characterisation
of tissue inhomogeneities and detection of fibrotic scars in myocardial tissue.
This approach opens a new pathway to significantly improve the diagnosis,
treatment planning, and management of heart conditions associated with cardiac
fibrosis.

</details>


### [184] [Improving Omics-Based Classification: The Role of Feature Selection and Synthetic Data Generation](https://arxiv.org/abs/2505.03387)
*Diego Perazzolo,Pietro Fanton,Ilaria Barison,Marny Fedrigo,Annalisa Angelini,Chiara Castellani,Enrico Grisan*

Main category: cs.LG

TL;DR: 提出了一种整合特征选择和数据增强的机器学习框架，用于改进组学数据分类的准确性和可解释性，尤其是在样本量有限的情况下。


<details>
  <summary>Details</summary>
Motivation: 组学数据集日益复杂，样本量有限，现有分类模型往往可解释性差，难以从中获取可靠的生物学见解，因此需要提高分类性能并增强模型决策的透明度和可靠性。

Method: 本研究提出一个基于机器学习的分类框架，该框架整合了特征选择与数据增强技术。使用公开数据集（E-MTAB-8026），在六种二元分类场景中进行自举分析（bootstrap analysis）来评估所提出模型的行为。

Result: 研究表明，所提出的流程在小数据集上能产生良好的交叉验证性能，并且当训练好的分类器应用于更大的测试集时，这种性能得以保持。即使在样本量非常有限的情况下，引入合成数据对改善模型泛化能力也具有积极影响。

Conclusion: 研究结果强调了准确性与特征选择之间的基本平衡，并突出了引入合成数据对于提高模型泛化能力的积极作用，尤其是在样本极其有限的场景下，这有助于实现更好的可解释性。

Abstract: Given the increasing complexity of omics datasets, a key challenge is not
only improving classification performance but also enhancing the transparency
and reliability of model decisions. Effective model performance and feature
selection are fundamental for explainability and reliability. In many cases,
high dimensional omics datasets suffer from limited number of samples due to
clinical constraints, patient conditions, phenotypes rarity and others
conditions. Current omics based classification models often suffer from narrow
interpretability, making it difficult to discern meaningful insights where
trust and reproducibility are critical. This study presents a machine learning
based classification framework that integrates feature selection with data
augmentation techniques to achieve high standard classification accuracy while
ensuring better interpretability. Using the publicly available dataset (E MTAB
8026), we explore a bootstrap analysis in six binary classification scenarios
to evaluate the proposed model's behaviour. We show that the proposed pipeline
yields cross validated perfomance on small dataset that is conserved when the
trained classifier is applied to a larger test set. Our findings emphasize the
fundamental balance between accuracy and feature selection, highlighting the
positive effect of introducing synthetic data for better generalization, even
in scenarios with very limited samples availability.

</details>


### [185] [Concept Factorization via Self-Representation and Adaptive Graph Structure Learning](https://arxiv.org/abs/2505.03390)
*Zhengqin Yang,Di Wu,Jia Chen,Xin Luo*

Main category: cs.LG

TL;DR: 提出了一种名为CFSRAG的概念分解模型，通过自适应学习数据图结构来改进聚类性能，克服了现有方法对初始图结构的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的基于概念分解（CF）的聚类模型性能在很大程度上依赖于初始图结构的构建，这限制了其聚类效果。

Method: 提出一种基于自表示和自适应图结构学习的概念分解模型（CFSRAG）。该模型通过自表示方法学习数据间的亲和关系，并利用学习到的亲和矩阵实现动态图正则化约束，从而动态学习数据的内部几何结构。

Result: 在四个真实数据集上进行的对比实验表明，CFSRAG模型优于其他最先进的聚类模型。

Conclusion: CFSRAG模型通过自适应地学习数据图结构，能够有效提升概念分解在数据聚类任务中的性能。

Abstract: Concept Factorization (CF) models have attracted widespread attention due to
their excellent performance in data clustering. In recent years, many variant
models based on CF have achieved great success in clustering by taking into
account the internal geometric manifold structure of the dataset and using
graph regularization techniques. However, their clustering performance depends
greatly on the construction of the initial graph structure. In order to enable
adaptive learning of the graph structure of the data, we propose a Concept
Factorization Based on Self-Representation and Adaptive Graph Structure
Learning (CFSRAG) Model. CFSRAG learns the affinity relationship between data
through a self-representation method, and uses the learned affinity matrix to
implement dynamic graph regularization constraints, thereby ensuring dynamic
learning of the internal geometric structure of the data. Finally, we give the
CFSRAG update rule and convergence analysis, and conduct comparative
experiments on four real datasets. The results show that our model outperforms
other state-of-the-art models.

</details>


### [186] [Automatic Calibration for Membership Inference Attack on Large Language Models](https://arxiv.org/abs/2505.03392)
*Saleh Zare Zade,Yao Qiang,Xiangyu Zhou,Hui Zhu,Mohammad Amin Roshani,Prashant Khanduri,Dongxiao Zhu*

Main category: cs.LG

TL;DR: 该论文提出了一种名为ACMIA的新型成员推理攻击框架，通过可调温度校准LLM的输出概率，以减少误报并提高可靠性，且无需额外的参考模型。


<details>
  <summary>Details</summary>
Motivation: 现有针对大语言模型的成员推理攻击（MIA）方法常常将非成员错误地推断为成员，导致高误报率，或者依赖额外的参考模型进行概率校准，这限制了它们的实用性。

Method: 提出了一种名为ACMIA（自动校准成员推理攻击）的新框架。该框架利用可调温度来有效校准输出概率，其灵感来源于对大语言模型预训练中最大似然估计的理论见解。ACMIA提供了三种配置，以适应不同级别的模型访问权限，并旨在增大成员与非成员之间的概率差距。

Result: 在各种开源大语言模型上的大量实验表明，所提出的攻击方法高效、鲁棒且具有良好的泛化能力，在三个广泛使用的基准测试中均优于最先进的基线方法。

Conclusion: ACMIA通过引入可调温度校准机制，有效地克服了现有MIA方法的挑战，提高了成员推理的可靠性和鲁棒性，且具有较好的实用性。

Abstract: Membership Inference Attacks (MIAs) have recently been employed to determine
whether a specific text was part of the pre-training data of Large Language
Models (LLMs). However, existing methods often misinfer non-members as members,
leading to a high false positive rate, or depend on additional reference models
for probability calibration, which limits their practicality. To overcome these
challenges, we introduce a novel framework called Automatic Calibration
Membership Inference Attack (ACMIA), which utilizes a tunable temperature to
calibrate output probabilities effectively. This approach is inspired by our
theoretical insights into maximum likelihood estimation during the pre-training
of LLMs. We introduce ACMIA in three configurations designed to accommodate
different levels of model access and increase the probability gap between
members and non-members, improving the reliability and robustness of membership
inference. Extensive experiments on various open-source LLMs demonstrate that
our proposed attack is highly effective, robust, and generalizable, surpassing
state-of-the-art baselines across three widely used benchmarks. Our code is
available at:
\href{https://github.com/Salehzz/ACMIA}{\textcolor{blue}{Github}}.

</details>


### [187] [Prediction Models That Learn to Avoid Missing Values](https://arxiv.org/abs/2505.03393)
*Lena Stempfle,Anton Matsson,Newton Mwai,Fredrik D. Johansson*

Main category: cs.LG

TL;DR: 提出了一种名为“缺失规避 (MA)”的机器学习框架，旨在训练模型在测试时减少对缺失特征值的依赖，同时保持预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的处理测试时缺失值的方法，如插补或使用缺失指示符，会引入偏差、增加模型复杂度，并损害模型的可解释性，难以理解模型如何利用观测变量进行预测。

Method: 提出了“缺失规避 (MA)”机器学习框架。通过在决策树、树集成模型和稀疏线性模型的学习目标中加入针对分类器特性的正则化项，创建了定制化的 MA 学习算法。其中，基于树的模型利用上下文缺失性，根据观察到的上下文减少对缺失值的依赖。

Result: 在真实世界数据集上的实验表明，MA-DT、MA-LASSO、MA-RF 和 MA-GBT 等模型能有效减少对具有缺失值特征的依赖，同时保持与它们未正则化版本相当的预测性能。

Conclusion: 该框架为从业者提供了一个强大的工具，使得在存在测试时缺失值的情况下，仍能保持预测的可解释性。

Abstract: Handling missing values at test time is challenging for machine learning
models, especially when aiming for both high accuracy and interpretability.
Established approaches often add bias through imputation or excessive model
complexity via missingness indicators. Moreover, either method can obscure
interpretability, making it harder to understand how the model utilizes the
observed variables in predictions. We propose missingness-avoiding (MA) machine
learning, a general framework for training models to rarely require the values
of missing (or imputed) features at test time. We create tailored MA learning
algorithms for decision trees, tree ensembles, and sparse linear models by
incorporating classifier-specific regularization terms in their learning
objectives. The tree-based models leverage contextual missingness by reducing
reliance on missing values based on the observed context. Experiments on
real-world datasets demonstrate that MA-DT, MA-LASSO, MA-RF, and MA-GBT
effectively reduce the reliance on features with missing values while
maintaining predictive performance competitive with their unregularized
counterparts. This shows that our framework gives practitioners a powerful tool
to maintain interpretability in predictions with test-time missing values.

</details>


### [188] [Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey](https://arxiv.org/abs/2505.03418)
*Da Zheng,Lun Du,Junwei Su,Yuchen Tian,Yuqi Zhu,Jintian Zhang,Lanning Wei,Ningyu Zhang,Huajun Chen*

Main category: cs.LG

TL;DR: 这篇综述探讨了大型语言模型 (LLM) 在解决复杂问题方面的能力、局限性、现有技术以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型 (LLM) 在解决复杂问题方面展现出巨大潜力，但将其应用于现实世界面临多步推理、领域知识整合和结果验证等挑战，促使本研究进行系统性探讨。

Method: 本研究通过文献综述，探讨LLM在复杂问题解决中的能力与局限性，考察了思维链 (CoT) 推理、知识增强以及多种基于LLM和工具的验证技术，并分析了其在软件工程、数学推理、数据分析和科学研究等特定领域的应用挑战。

Result: 研究总结了LLM解决复杂问题时采用的关键技术（如CoT、知识增强、验证方法），指出了在特定领域应用时面临的挑战，并揭示了当前LLM解决方案的根本局限性。

Conclusion: 尽管LLM在复杂问题解决方面取得了进展，但在多步推理、领域知识整合和结果验证方面仍存在根本性局限。未来研究需聚焦于这些方面，以提升LLM解决复杂问题的能力。

Abstract: Problem-solving has been a fundamental driver of human progress in numerous
domains. With advancements in artificial intelligence, Large Language Models
(LLMs) have emerged as powerful tools capable of tackling complex problems
across diverse domains. Unlike traditional computational systems, LLMs combine
raw computational power with an approximation of human reasoning, allowing them
to generate solutions, make inferences, and even leverage external
computational tools. However, applying LLMs to real-world problem-solving
presents significant challenges, including multi-step reasoning, domain
knowledge integration, and result verification. This survey explores the
capabilities and limitations of LLMs in complex problem-solving, examining
techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation,
and various LLM-based and tool-based verification techniques. Additionally, we
highlight domain-specific challenges in various domains, such as software
engineering, mathematical reasoning and proving, data analysis and modeling,
and scientific research. The paper further discusses the fundamental
limitations of the current LLM solutions and the future directions of LLM-based
complex problems solving from the perspective of multi-step reasoning, domain
knowledge integration and result verification.

</details>


### [189] [Framework GNN-AID: Graph Neural Network Analysis Interpretation and Defense](https://arxiv.org/abs/2505.03424)
*Kirill Lukyanov,Mikhail Drobyshevskiy,Georgii Sazonov,Mikhail Soloviov,Ilya Makarov*

Main category: cs.LG

TL;DR: 该论文介绍了一个名为 GNN-AID 的开源框架，用于分析、解释和防御图神经网络 (GNN)，以解决现有工具在图数据可解释性和鲁棒性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 对可信人工智能 (TAI) 日益增长的需求凸显了机器学习模型中可解释性和鲁棒性的重要性。然而，许多现有工具忽视了图数据，并且很少将这两个方面结合到单一解决方案中。图神经网络 (GNN) 作为一种流行方法，迫切需要一个综合框架来解决这些问题。

Method: 研究者开发了 GNN-AID，一个基于 PyTorch-Geometric 构建的开源 Python 库。该框架支持高级可信方法（攻击、防御、可解释性）和架构层，提供预加载的数据集、模型，并支持自定义 GNN。它还包括一个带有图可视化和无代码功能（如交互式模型构建器）的 Web 界面，并支持 MLOps 技术以确保可复现性和结果版本控制。

Result: GNN-AID 是一个为开发者和研究人员设计的灵活工具。它帮助开发者创建、分析和定制图模型，并为研究人员提供了探索可解释性与鲁棒性之间关系、测试防御策略以及组合方法以抵御不同类型攻击的平台。研究还展示了针对规避攻击和投毒攻击的防御措施在应用于图数据时可能发生的冲突。

Conclusion: GNN-AID 填补了图数据领域可信分析工具的空白，为开发和研究可解释且鲁棒的 GNN 提供了一个全面的解决方案，并揭示了不同防御策略之间复杂的相互作用。

Abstract: The growing need for Trusted AI (TAI) highlights the importance of
interpretability and robustness in machine learning models. However, many
existing tools overlook graph data and rarely combine these two aspects into a
single solution. Graph Neural Networks (GNNs) have become a popular approach,
achieving top results across various tasks. We introduce GNN-AID (Graph Neural
Network Analysis, Interpretation, and Defense), an open-source framework
designed for graph data to address this gap. Built as a Python library, GNN-AID
supports advanced trust methods and architectural layers, allowing users to
analyze graph datasets and GNN behavior using attacks, defenses, and
interpretability methods.
  GNN-AID is built on PyTorch-Geometric, offering preloaded datasets, models,
and support for any GNNs through customizable interfaces. It also includes a
web interface with tools for graph visualization and no-code features like an
interactive model builder, simplifying the exploration and analysis of GNNs.
The framework also supports MLOps techniques, ensuring reproducibility and
result versioning to track and revisit analyses efficiently.
  GNN-AID is a flexible tool for developers and researchers. It helps
developers create, analyze, and customize graph models, while also providing
access to prebuilt datasets and models for quick experimentation. Researchers
can use the framework to explore advanced topics on the relationship between
interpretability and robustness, test defense strategies, and combine methods
to protect against different types of attacks.
  We also show how defenses against evasion and poisoning attacks can conflict
when applied to graph data, highlighting the complex connections between
defense strategies.
  GNN-AID is available at
\href{https://github.com/ispras/GNN-AID}{github.com/ispras/GNN-AID}

</details>


### [190] [Wasserstein Convergence of Score-based Generative Models under Semiconvexity and Discontinuous Gradients](https://arxiv.org/abs/2505.03432)
*Stefano Bruno,Sotirios Sabanis*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Score-based Generative Models (SGMs) approximate a data distribution by
perturbing it with Gaussian noise and subsequently denoising it via a learned
reverse diffusion process. These models excel at modeling complex data
distributions and generating diverse samples, achieving state-of-the-art
performance across domains such as computer vision, audio generation,
reinforcement learning, and computational biology. Despite their empirical
success, existing Wasserstein-2 convergence analysis typically assume strong
regularity conditions-such as smoothness or strict log-concavity of the data
distribution-that are rarely satisfied in practice. In this work, we establish
the first non-asymptotic Wasserstein-2 convergence guarantees for SGMs
targeting semiconvex distributions with potentially discontinuous gradients.
Our upper bounds are explicit and sharp in key parameters, achieving optimal
dependence of $O(\sqrt{d})$ on the data dimension $d$ and convergence rate of
order one. The framework accommodates a wide class of practically relevant
distributions, including symmetric modified half-normal distributions, Gaussian
mixtures, double-well potentials, and elastic net potentials. By leveraging
semiconvexity without requiring smoothness assumptions on the potential such as
differentiability, our results substantially broaden the theoretical
foundations of SGMs, bridging the gap between empirical success and rigorous
guarantees in non-smooth, complex data regimes.

</details>


### [191] [A new membership inference attack that spots memorization in generative and predictive models: Loss-Based with Reference Model algorithm (LBRM)](https://arxiv.org/abs/2505.03490)
*Faiz Taleb,Ivan Gazeau,Maryline Laurent*

Main category: cs.LG

TL;DR: 本文提出LBRM算法，通过参考模型增强对时间序列插补模型中记忆化训练数据的检测，显著提高隐私风险识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 生成模型会无意中记忆训练数据，构成严重的隐私风险，特别是在时间序列插补模型中，这一问题亟待解决。

Method: 引入了基于损失的参考模型 (LBRM) 算法。该方法利用一个参考模型来提高成员推断攻击（区分训练数据和测试数据）的准确性，从而有效地提取和识别被模型记忆的训练数据。

Result: LBRM 显著提高了记忆化数据的检测准确率。在没有微调的情况下，AUROC 平均提高了约40%；经过微调后，AUROC 平均提高了约60%。该方法在两种不同的时间序列插补模型架构上均得到验证，显示了其鲁棒性和通用性。

Conclusion: LBRM 方法显著增强了对时间序列插补模型中记忆化训练数据的检测准确性，从而有效应对了此类模型中的隐私泄露风险。

Abstract: Generative models can unintentionally memorize training data, posing
significant privacy risks. This paper addresses the memorization phenomenon in
time series imputation models, introducing the Loss-Based with Reference Model
(LBRM) algorithm. The LBRM method leverages a reference model to enhance the
accuracy of membership inference attacks, distinguishing between training and
test data. Our contributions are twofold: first, we propose an innovative
method to effectively extract and identify memorized training data,
significantly improving detection accuracy. On average, without fine-tuning,
the AUROC improved by approximately 40\%. With fine-tuning, the AUROC increased
by approximately 60\%. Second, we validate our approach through membership
inference attacks on two types of architectures designed for time series
imputation, demonstrating the robustness and versatility of the LBRM approach
in different contexts. These results highlight the significant enhancement in
detection accuracy provided by the LBRM approach, addressing privacy risks in
time series imputation models.

</details>


### [192] [AnomalyMatch: Discovering Rare Objects of Interest with Semi-supervised and Active Learning](https://arxiv.org/abs/2505.03509)
*Pablo Gómez,David O'Ryan*

Main category: cs.LG

TL;DR: AnomalyMatch是一种结合了半监督学习和主动学习的异常检测框架，旨在高效利用少量标记数据和大量未标记数据，在标签稀缺的情况下实现高性能异常检测。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集中的异常检测至关重要，但传统的监督方法需要大量的人工标记，这在许多领域（如天文学和计算机视觉）中是不切实际的。

Method: 提出了AnomalyMatch框架，该框架将半监督学习算法FixMatch与EfficientNet分类器相结合，并融入了主动学习机制。它将异常检测视为一个半监督二元分类问题，通过用户界面进行迭代式模型优化，允许专家验证高置信度的异常并纠正误报。

Result: 在GalaxyMNIST天文数据集和miniImageNet自然图像基准测试中，AnomalyMatch表现出色。从5-10个标记异常开始，经过三个主动学习周期，在miniImageNet上平均AUROC达到0.95，AUPRC达到0.77；在GalaxyMNIST上平均AUROC为0.86，AUPRC为0.71。在排名最高的1%图像中，异常的检测精度在miniImageNet上为71%，在GalaxyMNIST上为93%。该框架可在单个GPU上三天内处理1亿张图像的预测。

Conclusion: AnomalyMatch是一种实用且可扩展的异常发现方法，特别适用于标签严重稀缺的领域，突显了针对此类数据挑战的专业化方法的价值。该方法已集成到ESA的Datalabs平台，用于在大型天文数据集中发现有科学价值的异常。

Abstract: Anomaly detection in large datasets is essential in fields such as astronomy
and computer vision; however, supervised methods typically require extensive
anomaly labelling, which is often impractical. We present AnomalyMatch, an
anomaly detection framework combining the semi-supervised FixMatch algorithm
using EfficientNet classifiers with active learning. By treating anomaly
detection as a semi-supervised binary classification problem, we efficiently
utilise limited labelled and abundant unlabelled images. We allow iterative
model refinement in a user interface for expert verification of high-confidence
anomalies and correction of false positives. Built for astronomical data,
AnomalyMatch generalises readily to other domains facing similar data
challenges. Evaluations on the GalaxyMNIST astronomical dataset and the
miniImageNet natural-image benchmark under severe class imbalance (1% anomalies
for miniImageNet) display strong performance: starting from five to ten
labelled anomalies and after three active learning cycles, we achieve an
average AUROC of 0.95 (miniImageNet) and 0.86 (GalaxyMNIST), with respective
AUPRC of 0.77 and 0.71. After active learning cycles, anomalies are ranked with
71% (miniImageNet) to 93% precision in the 1% of the highest-ranked images.
AnomalyMatch is tailored for large-scale applications, efficiently processing
predictions for 100 million images within three days on a single GPU.
Integrated into ESAs Datalabs platform, AnomalyMatch facilitates targeted
discovery of scientifically valuable anomalies in vast astronomical datasets.
Our results underscore the exceptional utility and scalability of this approach
for anomaly discovery, highlighting the value of specialised approaches for
domains characterised by severe label scarcity.

</details>


### [193] [Uncovering the Limitations of Model Inversion Evaluation: Benchmarks and Connection to Type-I Adversarial Attacks](https://arxiv.org/abs/2505.03519)
*Sy-Tuyen Ho,Koh Jun Hao,Ngoc-Bao Nguyen,Alexander Binder,Ngai-Man Cheung*

Main category: cs.LG

TL;DR: 该研究首次深入评估了模型反演（MI）攻击的评估框架，发现其存在大量假阳性，导致高估了MI攻击的成功率和实际隐私泄露程度。


<details>
  <summary>Details</summary>
Motivation: 现有的模型反演（MI）攻击与防御研究依赖一个通用的评估框架，但该框架的准确性从未被深入研究，可能导致对MI攻击进展的误判。

Method: 1. 构建了首个基于28种不同MI攻击、防御、私有和公共数据集设置的MI攻击样本人工标注数据集。2. 使用该数据集检验现有MI评估框架的准确性。3. 分析假阳性的原因，设计受控实验，并发现了I型对抗性特征对MI评估的意外影响及其与对抗性可迁移性的关系。

Result: 研究发现，广泛使用的MI评估框架存在显著的假阳性问题，导致先前报道的SOTA MI攻击的成功率被高估。I型对抗性特征对MI评估有显著影响，实际的隐私泄露程度远低于先前报道。

Conclusion: 目前广泛使用的MI评估框架存在严重局限性。研究者提出了缓解假阳性率的方法，并敦促将人工评估作为主要的MI评估框架，而非仅仅是补充，同时鼓励开发更鲁棒可靠的自动评估框架。

Abstract: Model Inversion (MI) attacks aim to reconstruct information of private
training data by exploiting access to machine learning models. The most common
evaluation framework for MI attacks/defenses relies on an evaluation model that
has been utilized to assess progress across almost all MI attacks and defenses
proposed in recent years. In this paper, for the first time, we present an
in-depth study of MI evaluation. Firstly, we construct the first comprehensive
human-annotated dataset of MI attack samples, based on 28 setups of different
MI attacks, defenses, private and public datasets. Secondly, using our dataset,
we examine the accuracy of the MI evaluation framework and reveal that it
suffers from a significant number of false positives. These findings raise
questions about the previously reported success rates of SOTA MI attacks.
Thirdly, we analyze the causes of these false positives, design controlled
experiments, and discover the surprising effect of Type I adversarial features
on MI evaluation, as well as adversarial transferability, highlighting a
relationship between two previously distinct research areas. Our findings
suggest that the performance of SOTA MI attacks has been overestimated, with
the actual privacy leakage being significantly less than previously reported.
In conclusion, we highlight critical limitations in the widely used MI
evaluation framework and present our methods to mitigate false positive rates.
We remark that prior research has shown that Type I adversarial attacks are
very challenging, with no existing solution. Therefore, we urge to consider
human evaluation as a primary MI evaluation framework rather than merely a
supplement as in previous MI research. We also encourage further work on
developing more robust and reliable automatic evaluation frameworks.

</details>


### [194] [Causal Intervention Framework for Variational Auto Encoder Mechanistic Interpretability](https://arxiv.org/abs/2505.03530)
*Dip Roy*

Main category: cs.LG

TL;DR: 本文提出了一种针对变分自编码器 (VAE) 的因果干预框架，用于通过识别“回路基序”和分析语义因素的编码、处理和解耦来增强其机制性可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型的可解释性研究取得了进展，但对变分自编码器 (VAE) 等生成模型的内部工作机制的理解仍然具有挑战性，限制了其透明度和可控性。

Method: 开发了一个全面的因果干预框架，通过在输入、潜空间、激活层等不同层面进行目标性干预（如输入操纵、潜空间扰动、激活补丁、因果中介分析），识别和分析VAE中的“回路基序”，并引入了量化可解释性的新指标（因果效应强度、干预特异性、回路模块化）。

Result: 该框架能够成功分离功能性回路，将计算图映射到语义因素的因果图，并区分多义和单义单元。实验表明，FactorVAE在解耦得分 (0.084) 和因果效应强度 (平均4.59) 上优于标准VAE (0.064, 3.99) 和Beta-VAE (0.051, 3.43)。

Conclusion: 该研究提出的框架增进了对生成模型机制的理解，并为构建更透明、可控的VAE架构提供了工具。

Abstract: Mechanistic interpretability of deep learning models has emerged as a crucial
research direction for understanding the functioning of neural networks. While
significant progress has been made in interpreting discriminative models like
transformers, understanding generative models such as Variational Autoencoders
(VAEs) remains challenging. This paper introduces a comprehensive causal
intervention framework for mechanistic interpretability of VAEs. We develop
techniques to identify and analyze "circuit motifs" in VAEs, examining how
semantic factors are encoded, processed, and disentangled through the network
layers. Our approach uses targeted interventions at different levels: input
manipulations, latent space perturbations, activation patching, and causal
mediation analysis. We apply our framework to both synthetic datasets with
known causal relationships and standard disentanglement benchmarks. Results
show that our interventions can successfully isolate functional circuits, map
computational graphs to causal graphs of semantic factors, and distinguish
between polysemantic and monosemantic units. Furthermore, we introduce metrics
for causal effect strength, intervention specificity, and circuit modularity
that quantify the interpretability of VAE components. Experimental results
demonstrate clear differences between VAE variants, with FactorVAE achieving
higher disentanglement scores (0.084) and effect strengths (mean 4.59) compared
to standard VAE (0.064, 3.99) and Beta-VAE (0.051, 3.43). Our framework
advances the mechanistic understanding of generative models and provides tools
for more transparent and controllable VAE architectures.

</details>


### [195] [Small-Scale-Fading-Aware Resource Allocation in Wireless Federated Learning](https://arxiv.org/abs/2505.03533)
*Jiacheng Wang,Le Liang,Hao Ye,Chongtao Guo,Shi Jin*

Main category: cs.LG

TL;DR: 本文提出一种基于多智能体强化学习（MARL）的资源分配策略，通过感知小尺度信道衰落来提升无线联邦学习的训练性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习资源分配策略通常忽略小尺度信道快速波动，导致训练性能下降。因此，需要一种能感知并适应小尺度衰落的资源分配方法来提升联邦学习效率。

Method: 提出了一个感知小尺度衰落的资源分配策略。具体方法是：1. 将资源分配问题建模为去中心化部分可观察马尔可夫决策过程（Dec-POMDP）。2. 采用多智能体强化学习（MARL）框架，特别是QMIX算法进行求解。3. 每个客户端作为智能体，基于局部观察和收敛分析得出的奖励，在每个相干时间内动态决定频谱和功率分配。

Result: 实验结果表明，所提出的基于QMIX的资源分配策略在不同统计异构性程度上均显著优于基线方法。消融研究证实了纳入小尺度衰落动态对于优化联邦学习性能至关重要。

Conclusion: 本文提出的感知小尺度衰落的MARL资源分配策略能有效提升无线联邦学习的训练性能，且具有良好的可扩展性和实用性。考虑小尺度衰落对于优化资源分配至关重要。

Abstract: Judicious resource allocation can effectively enhance federated learning (FL)
training performance in wireless networks by addressing both system and
statistical heterogeneity. However, existing strategies typically rely on block
fading assumptions, which overlooks rapid channel fluctuations within each
round of FL gradient uploading, leading to a degradation in FL training
performance. Therefore, this paper proposes a small-scale-fading-aware resource
allocation strategy using a multi-agent reinforcement learning (MARL)
framework. Specifically, we establish a one-step convergence bound of the FL
algorithm and formulate the resource allocation problem as a decentralized
partially observable Markov decision process (Dec-POMDP), which is subsequently
solved using the QMIX algorithm. In our framework, each client serves as an
agent that dynamically determines spectrum and power allocations within each
coherence time slot, based on local observations and a reward derived from the
convergence analysis. The MARL setting reduces the dimensionality of the action
space and facilitates decentralized decision-making, enhancing the scalability
and practicality of the solution. Experimental results demonstrate that our
QMIX-based resource allocation strategy significantly outperforms baseline
methods across various degrees of statistical heterogeneity. Additionally,
ablation studies validate the critical importance of incorporating small-scale
fading dynamics, highlighting its role in optimizing FL performance.

</details>


### [196] [Efficient Training of Physics-enhanced Neural ODEs via Direct Collocation and Nonlinear Programming](https://arxiv.org/abs/2505.03552)
*Linus Langenkamp,Philip Hannebohm,Bernhard Bachmann*

Main category: cs.LG

TL;DR: 提出了一种新的物理增强神经ODE (PeNODE) 训练方法，通过将训练过程表述为动态优化问题，并使用高阶隐式龙格-库塔方法进行离散化，从而高效求解。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于ODE求解器的训练方法在稳定性、运行时间和准确性方面的关键局限性。

Method: 将训练过程表述为动态优化问题；使用高阶隐式龙格-库塔方法（翻转的Legendre-Gauss-Radau点）离散化模型，形成大规模非线性规划(NLP)问题，并用NLP求解器（如Ipopt）求解；推广了直接配置法，融入物理约束，并提供了定制化、并行化的开源实现。

Result: 在四分之一车辆模型和范德波尔振荡器上的基准测试表明，与其它训练技术相比，该方法在使用更小网络的情况下，具有更优的准确性、速度和泛化能力。

Conclusion: 该方法通过动态优化同时优化网络参数和状态轨迹，显著提升了PeNODE训练的性能，并计划集成到OpenModelica以支持神经微分代数方程(Neural DAEs)的训练。

Abstract: We propose a novel approach for training Physics-enhanced Neural ODEs
(PeNODEs) by expressing the training process as a dynamic optimization problem.
The full model, including neural components, is discretized using a high-order
implicit Runge-Kutta method with flipped Legendre-Gauss-Radau points, resulting
in a large-scale nonlinear program (NLP) efficiently solved by state-of-the-art
NLP solvers such as Ipopt. This formulation enables simultaneous optimization
of network parameters and state trajectories, addressing key limitations of ODE
solver-based training in terms of stability, runtime, and accuracy. Extending
on a recent direct collocation-based method for Neural ODEs, we generalize to
PeNODEs, incorporate physical constraints, and present a custom, parallelized,
open-source implementation. Benchmarks on a Quarter Vehicle Model and a
Van-der-Pol oscillator demonstrate superior accuracy, speed, and generalization
with smaller networks compared to other training techniques. We also outline a
planned integration into OpenModelica to enable accessible training of Neural
DAEs.

</details>


### [197] [Rapid AI-based generation of coverage paths for dispensing applications](https://arxiv.org/abs/2505.03560)
*Simon Baeuerle,Ian F. Mendonca,Kristof Van Laerhoven,Ralf Mikut,Andreas Steimer*

Main category: cs.LG

TL;DR: 提出一种基于人工智能的新方法，使用神经网络直接生成导热界面材料（TIM）的涂覆路径，以替代传统的手动或高计算量优化方法。


<details>
  <summary>Details</summary>
Motivation: 现有的导热界面材料（TIM）涂覆路径规划方法依赖专家手动操作或计算量大的优化算法，效率不高。

Method: 采用一种基于人工智能的方法，利用人工神经网络（ANN）接收目标冷却区域作为输入，直接输出涂覆路径。该方法无需标签数据。

Result: 该方法在多个目标区域上被证明是可行的，生成的涂覆路径可以直接用于自动化制造设备，并且不会产生气泡。

Conclusion: 使用人工神经网络实时预测工艺参数以达到期望目标状态的方法，具有应用于其他制造过程的潜力。

Abstract: Coverage Path Planning of Thermal Interface Materials (TIM) plays a crucial
role in the design of power electronics and electronic control units. Up to
now, this is done manually by experts or by using optimization approaches with
a high computational effort. We propose a novel AI-based approach to generate
dispense paths for TIM and similar dispensing applications. It is a drop-in
replacement for optimization-based approaches. An Artificial Neural Network
(ANN) receives the target cooling area as input and directly outputs the
dispense path. Our proposed setup does not require labels and we show its
feasibility on multiple target areas. The resulting dispense paths can be
directly transferred to automated manufacturing equipment and do not exhibit
air entrapments. The approach of using an ANN to predict process parameters for
a desired target state in real-time could potentially be transferred to other
manufacturing processes.

</details>


### [198] [Ergodic Generative Flows](https://arxiv.org/abs/2505.03561)
*Leo Maxime Brunswic,Mateo Clemente,Rui Heng Yang,Adam Sigal,Amir Rasouli,Yinchuan Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为遍历生成流 (EGFs) 的新方法，旨在解决生成流网络 (GFNs) 在连续设置和模仿学习中的训练挑战，并引入了新的 KL-weakFM 损失函数用于无需奖励模型的模仿学习。


<details>
  <summary>Details</summary>
Motivation: 当前生成流网络 (GFNs) 在连续设置和模仿学习 (IL) 中面临诸多挑战，包括流匹配损失难以处理、非循环训练的测试有限，以及在模仿学习中需要一个独立的奖励模型。

Method: 提出了遍历生成流 (EGFs) 家族。首先，利用遍历性构建具有全局定义变换和可处理流匹配损失 (FM loss) 的简单生成流。其次，引入了一种新的 KL-weakFM 损失（交叉熵与弱流匹配控制相结合），专为无需独立奖励模型的模仿学习训练而设计。

Result: 使用 KL-weakFM 损失在玩具二维任务和 NASA 真实世界球面数据集上评估了模仿学习的 EGFs (IL-EGFs)。此外，还使用 FM 损失进行了带有目标奖励的玩具二维强化学习实验。

Conclusion: 所提出的遍历生成流 (EGFs) 及其 KL-weakFM 损失为解决 GFNs 在连续设置和模仿学习中的现有问题提供了有效途径，特别是在无需独立奖励模型的情况下进行模仿学习。

Abstract: Generative Flow Networks (GFNs) were initially introduced on directed acyclic
graphs to sample from an unnormalized distribution density. Recent works have
extended the theoretical framework for generative methods allowing more
flexibility and enhancing application range. However, many challenges remain in
training GFNs in continuous settings and for imitation learning (IL), including
intractability of flow-matching loss, limited tests of non-acyclic training,
and the need for a separate reward model in imitation learning. The present
work proposes a family of generative flows called Ergodic Generative Flows
(EGFs) which are used to address the aforementioned issues. First, we leverage
ergodicity to build simple generative flows with finitely many globally defined
transformations (diffeomorphisms) with universality guarantees and tractable
flow-matching loss (FM loss). Second, we introduce a new loss involving
cross-entropy coupled to weak flow-matching control, coined KL-weakFM loss. It
is designed for IL training without a separate reward model. We evaluate
IL-EGFs on toy 2D tasks and real-world datasets from NASA on the sphere, using
the KL-weakFM loss. Additionally, we conduct toy 2D reinforcement learning
experiments with a target reward, using the FM loss.

</details>


### [199] [Anant-Net: Breaking the Curse of Dimensionality with Scalable and Interpretable Neural Surrogate for High-Dimensional PDEs](https://arxiv.org/abs/2505.03595)
*Sidharth S. Menon,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: Anant-Net是一种高效的神经代理模型，旨在解决高维偏微分方程（PDEs），它克服了维度灾难问题，并在高维空间中实现了高精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 高维偏微分方程（PDEs）由于维度灾难问题，尤其是在超立方体域上，传统数值方法难以进行有效计算，计算复杂度随维度指数增长。

Method: 提出了一种名为Anant-Net的高效神经代理模型。该模型能够有效地整合高维边界条件，最小化高维配置点上的PDE残差，并通过集成Kolmogorov-Arnold网络来增强模型的可解释性。

Result: Anant-Net在多种线性和非线性高维方程（包括泊松方程、Sine-Gordon方程和Allen-Cahn方程）上展示了高精度和鲁棒性。它能够在几小时内使用单个GPU解决300维问题，并在准确性和运行时间方面优于其他先进方法。

Conclusion: Anant-Net被证明是一个准确、可解释且可扩展的框架，能够高效地解决高维偏微分方程。

Abstract: High-dimensional partial differential equations (PDEs) arise in diverse
scientific and engineering applications but remain computationally intractable
due to the curse of dimensionality. Traditional numerical methods struggle with
the exponential growth in computational complexity, particularly on hypercubic
domains, where the number of required collocation points increases rapidly with
dimensionality. Here, we introduce Anant-Net, an efficient neural surrogate
that overcomes this challenge, enabling the solution of PDEs in high
dimensions. Unlike hyperspheres, where the internal volume diminishes as
dimensionality increases, hypercubes retain or expand their volume (for unit or
larger length), making high-dimensional computations significantly more
demanding. Anant-Net efficiently incorporates high-dimensional boundary
conditions and minimizes the PDE residual at high-dimensional collocation
points. To enhance interpretability, we integrate Kolmogorov-Arnold networks
into the Anant-Net architecture. We benchmark Anant-Net's performance on
several linear and nonlinear high-dimensional equations, including the Poisson,
Sine-Gordon, and Allen-Cahn equations, demonstrating high accuracy and
robustness across randomly sampled test points from high-dimensional space.
Importantly, Anant-Net achieves these results with remarkable efficiency,
solving 300-dimensional problems on a single GPU within a few hours. We also
compare Anant-Net's results for accuracy and runtime with other
state-of-the-art methods. Our findings establish Anant-Net as an accurate,
interpretable, and scalable framework for efficiently solving high-dimensional
PDEs.

</details>


### [200] [Understand the Effect of Importance Weighting in Deep Learning on Dataset Shift](https://arxiv.org/abs/2505.03617)
*Thien Nhan Vo,Thanh Xuan Truong*

Main category: cs.LG

TL;DR: 该研究评估了重要性加权在深度神经网络中处理标签偏移和协变量偏移的有效性，发现在复杂数据和长时间训练下其效果有限，对其实际应用价值提出质疑。


<details>
  <summary>Details</summary>
Motivation: 探究重要性加权方法在深度学习模型中应对实际场景中常见的分布偏移（特别是标签偏移和协变量偏移）时的真实效果及其局限性。

Method: 1. 在合成二维数据（线性可分和月亮形）上，使用逻辑回归和多层感知机（MLP）测试重要性加权在标签偏移下的影响。2. 在CIFAR-10数据集上，通过设置不同的类别不平衡来模拟标签偏移，并评估L2正则化和dropout对加权效果的维持作用。3. 进行协变量偏移实验，检验重要性加权的性能。

Result: 在合成数据上，重要性加权在训练早期显著影响决策边界，但此效果随训练时间延长而减弱。在CIFAR-10的标签偏移实验中，仅L2正则化（而非dropout）有助于保持加权效果。在协变量偏移实验中，重要性加权未能带来显著的性能增益。

Conclusion: 研究结果对重要性加权在处理真实世界复杂数据分布偏移问题上的实用性提出了疑问，表明其在面对复杂数据和长时间优化时效果可能有限。

Abstract: We evaluate the effectiveness of importance weighting in deep neural networks
under label shift and covariate shift. On synthetic 2D data (linearly separable
and moon-shaped) using logistic regression and MLPs, we observe that weighting
strongly affects decision boundaries early in training but fades with prolonged
optimization. On CIFAR-10 with various class imbalances, only L2 regularization
(not dropout) helps preserve weighting effects. In a covariate-shift
experiment, importance weighting yields no significant performance gain,
highlighting challenges on complex data. Our results call into question the
practical utility of importance weighting for real-world distribution shifts.

</details>


### [201] [ALMA: Aggregated Lipschitz Maximization Attack on Auto-encoders](https://arxiv.org/abs/2505.03646)
*Chethan Krishnamurthy Ramanaik,Arjun Roy,Eirini Ntoutsi*

Main category: cs.LG

TL;DR: 本文提出了一种新的基于层条件化的对抗优化目标，以增强对深度自编码器（AE）的攻击，并设计了一种推理时对抗训练防御插件来缓解这些攻击。


<details>
  <summary>Details</summary>
Motivation: 深度自编码器（AE）的对抗鲁棒性研究不足，现有攻击方法难以有效利用AE中间层病态条件，且在优化扰动时梯度传播不佳，导致攻击效果有限。

Method: 1. 攻击方法：提出一种基于层条件化（layer-conditioning-based）的对抗性优化目标，通过增强攻击优化过程中损失梯度信息的传播，将对抗性映射引导至局部 Lipschitz 界限区域。
2. 防御方法：引入一种推理时对抗训练的防御插件（inference-time adversarially trained defense plugin）。

Result: 实验表明，提出的对抗优化目标能产生更强的攻击，在通用和样本特定场景下均优于现有方法。提出的防御插件能有效减轻对抗样本的影响。

Conclusion: 通过基于层条件化的优化目标，可以更有效地攻击深度自编码器，揭示其脆弱性。同时，提出的推理时对抗训练防御插件为抵御此类攻击提供了有效手段。

Abstract: Despite the extensive use of deep autoencoders (AEs) in critical
applications, their adversarial robustness remains relatively underexplored
compared to classification models. AE robustness is characterized by the
Lipschitz bounds of its components. Existing robustness evaluation frameworks
based on white-box attacks do not fully exploit the vulnerabilities of
intermediate ill-conditioned layers in AEs. In the context of optimizing
imperceptible norm-bounded additive perturbations to maximize output damage,
existing methods struggle to effectively propagate adversarial loss gradients
throughout the network, often converging to less effective perturbations. To
address this, we propose a novel layer-conditioning-based adversarial
optimization objective that effectively guides the adversarial map toward
regions of local Lipschitz bounds by enhancing loss gradient information
propagation during attack optimization. We demonstrate through extensive
experiments on state-of-the-art AEs that our adversarial objective results in
stronger attacks, outperforming existing methods in both universal and
sample-specific scenarios. As a defense method against this attack, we
introduce an inference-time adversarially trained defense plugin that mitigates
the effects of adversarial examples.

</details>


### [202] [Mitigating mode collapse in normalizing flows by annealing with an adaptive schedule: Application to parameter estimation](https://arxiv.org/abs/2505.03652)
*Yihang Wang,Chris Chi,Aaron R. Dinner*

Main category: cs.LG

TL;DR: 提出一种基于有效样本量（ESS）的自适应退火方法，以缓解归一化流（NFs）在多模态分布采样中的模式坍塌问题，并提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 归一化流（NFs）作为参数估计工具潜力巨大，但其在处理多模态分布时易发生模式坍塌（即仅捕捉到分布的单一模式），这限制了其实际应用价值。

Method: 研究采用了一种基于有效样本量（ESS）的自适应退火调度策略来训练归一化流，旨在减轻模式坍塌现象。

Result: 该方法在生化振荡器模型的时间序列数据拟合中，计算边际似然值的速度比广泛使用的集成马尔可夫链蒙特卡洛（MCMC）方法快十倍。此外，研究还表明ESS可用于通过样本剪枝来减少方差。

Conclusion: 基于ESS的自适应退火方法能有效缓解NFs的模式坍塌问题，提高采样效率，这些进展有望在NFs采样中得到广泛应用，并为进一步改进提供了方向。

Abstract: Normalizing flows (NFs) provide uncorrelated samples from complex
distributions, making them an appealing tool for parameter estimation. However,
the practical utility of NFs remains limited by their tendency to collapse to a
single mode of a multimodal distribution. In this study, we show that annealing
with an adaptive schedule based on the effective sample size (ESS) can mitigate
mode collapse. We demonstrate that our approach can converge the marginal
likelihood for a biochemical oscillator model fit to time-series data in
ten-fold less computation time than a widely used ensemble Markov chain Monte
Carlo (MCMC) method. We show that the ESS can also be used to reduce variance
by pruning the samples. We expect these developments to be of general use for
sampling with NFs and discuss potential opportunities for further improvements.

</details>


### [203] [Neural Integral Operators for Inverse problems in Spectroscopy](https://arxiv.org/abs/2505.03677)
*Emanuele Zappala,Alice Giola,Andreas Kramer,Enrico Greco*

Main category: cs.LG

TL;DR: 提出一种基于学习积分算子的深度学习方法，用于解决小数据集下分子光谱分类的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 光谱学数据常因稀缺导致深度学习模型过拟合，而传统机器学习方法在小数据集上准确性和适用性有限。

Method: 引入一种新的深度学习方法，通过第一类积分方程学习积分算子，用于分子光谱分类。

Result: 在真实世界数据集上的实验表明，该模型在小数据集上性能优于决策树、支持向量机等传统机器学习方法，以及其他深度学习模型。

Conclusion: 所提出的方法能够在数据非常有限的情况下，有效利用深度学习的能力并保持良好性能，解决了深度学习在光谱学小数据集应用中的关键问题。

Abstract: Deep learning has shown high performance on spectroscopic inverse problems
when sufficient data is available. However, it is often the case that data in
spectroscopy is scarce, and this usually causes severe overfitting problems
with deep learning methods. Traditional machine learning methods are viable
when datasets are smaller, but the accuracy and applicability of these methods
is generally more limited.
  We introduce a deep learning method for classification of molecular spectra
based on learning integral operators via integral equations of the first kind,
which results in an algorithm that is less affected by overfitting issues on
small datasets, compared to other deep learning models.
  The problem formulation of the deep learning approach is based on inverse
problems, which have traditionally found important applications in
spectroscopy. We perform experiments on real world data to showcase our
algorithm. It is seen that the model outperforms traditional machine learning
approaches such as decision tree and support vector machine, and for small
datasets it outperforms other deep learning models. Therefore, our methodology
leverages the power of deep learning, still maintaining the performance when
the available data is very limited, which is one of the main issues that deep
learning faces in spectroscopy, where datasets are often times of small size.

</details>


### [204] [Learning Survival Distributions with the Asymmetric Laplace Distribution](https://arxiv.org/abs/2505.03712)
*Deming Sheng,Ricardo Henao*

Main category: cs.LG

TL;DR: 本文提出一种基于不对称拉普拉斯分布（ALD）的参数化生存分析新方法，通过最大似然估计学习个体化ALD参数，在准确性、区分度和校准性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生存分析模型，特别是流行的非参数模型，通常避免直接估计整个生存分布，或通过离散化进行估计。本研究旨在提出一种参数化方法，能够直接对生存分布进行建模并方便地计算关键事件摘要。

Method: 提出一种基于不对称拉普拉斯分布（ALD）的参数化生存分析方法。该模型通过最大似然估计来学习每个个体ALD分布的参数（位置、尺度和不对称性），从而估计其生存时间分布。

Result: 在合成数据和真实世界数据集上的大量实验结果表明，所提出的方法在准确性、区分能力和校准性方面均优于现有的参数化和非参数化方法。

Conclusion: 基于不对称拉普拉斯分布的参数化生存分析方法是一种有效且优越的事件时间预测模型，能够提供全面的事件分布信息和易于计算的摘要，并在性能上超越了现有方法。

Abstract: Probabilistic survival analysis models seek to estimate the distribution of
the future occurrence (time) of an event given a set of covariates. In recent
years, these models have preferred nonparametric specifications that avoid
directly estimating survival distributions via discretization. Specifically,
they estimate the probability of an individual event at fixed times or the time
of an event at fixed probabilities (quantiles), using supervised learning.
Borrowing ideas from the quantile regression literature, we propose a
parametric survival analysis method based on the Asymmetric Laplace
Distribution (ALD). This distribution allows for closed-form calculation of
popular event summaries such as mean, median, mode, variation, and quantiles.
The model is optimized by maximum likelihood to learn, at the individual level,
the parameters (location, scale, and asymmetry) of the ALD distribution.
Extensive results on synthetic and real-world data demonstrate that the
proposed method outperforms parametric and nonparametric approaches in terms of
accuracy, discrimination and calibration.

</details>


### [205] [Sustainable Smart Farm Networks: Enhancing Resilience and Efficiency with Decision Theory-Guided Deep Reinforcement Learning](https://arxiv.org/abs/2505.03721)
*Dian Chen,Zelin Wan,Dong Sam Ha,Jin-Hee Cho*

Main category: cs.LG

TL;DR: 提出了一种结合深度强化学习（DRL）、迁移学习（TL）和决策理论（DT）的智能农场网络，旨在提高动物监测系统在网络攻击和能源波动下的韧性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有的太阳能农业监测系统在应对网络攻击以及适应动态和受限能源供应方面的韧性和适应性研究不足。

Method: 提出一种可持续智能农场网络，采用深度强化学习（DRL）制定最优策略以最大化监测效果和能源效率。为克服DRL收敛慢的问题，集成了迁移学习（TL）和决策理论（DT）来加速学习过程，并通过DT引导的策略优化监测质量和能源可持续性。

Result: 实验结果证明，与仅使用迁移学习增强的DRL模型相比，决策理论引导的DRL模型在系统性能上表现更优，并且训练运行时间减少了47.5%。

Conclusion: 将决策理论（DT）与深度强化学习（DRL）相结合，可以有效提高智能农场监测系统在网络威胁和能源限制下的性能和能源效率，并显著加快模型训练速度，实现可持续的高质量动物监测。

Abstract: Solar sensor-based monitoring systems have become a crucial agricultural
innovation, advancing farm management and animal welfare through integrating
sensor technology, Internet-of-Things, and edge and cloud computing. However,
the resilience of these systems to cyber-attacks and their adaptability to
dynamic and constrained energy supplies remain largely unexplored. To address
these challenges, we propose a sustainable smart farm network designed to
maintain high-quality animal monitoring under various cyber and adversarial
threats, as well as fluctuating energy conditions. Our approach utilizes deep
reinforcement learning (DRL) to devise optimal policies that maximize both
monitoring effectiveness and energy efficiency. To overcome DRL's inherent
challenge of slow convergence, we integrate transfer learning (TL) and decision
theory (DT) to accelerate the learning process. By incorporating DT-guided
strategies, we optimize monitoring quality and energy sustainability,
significantly reducing training time while achieving comparable performance
rewards. Our experimental results prove that DT-guided DRL outperforms
TL-enhanced DRL models, improving system performance and reducing training
runtime by 47.5%.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [206] [Computing in Integrated Terrestrial and Non-Terrestrial Networks: A Comprehensive Survey](https://arxiv.org/abs/2505.03016)
*Hoe Ziet Wong,Insaf Rzig,Safwan Alfattani,Wael Jaafar*

Main category: cs.NI

TL;DR: 这篇综述探讨了将云计算/雾计算/边缘计算集成到天地一体化网络（ITNTN）中，以满足未来服务严格的服务质量（QoS）需求和现代应用的数据处理需求。


<details>
  <summary>Details</summary>
Motivation: 物联网设备和智能汽车的快速增长对无处不在的连接和密集计算能力提出了更高要求。传统地面网络在覆盖和容量方面存在局限性，而天地一体化网络（ITNTN）结合边缘计算是应对这些挑战的关键。然而，目前在 ITNTN 中集成计算功能的研究尚不充分。

Method: 本文通过文献综述的方式，调研了当前将云/雾/边缘计算集成到 ITNTN 各层中的研究工作，并旨在扩展讨论至计算功能的全面集成。

Result: 本文识别了在 ITNTN 中全面集成计算功能所面临的挑战，并指出了实现这一目标的未来研究方向。

Conclusion: 将计算功能全面集成到天地一体化网络中对于满足未来服务需求至关重要，需要进一步研究以克服现有挑战并指明未来方向。

Abstract: The rapid growth of Internet-of-things (IoT) devices, smart vehicles, and
other connected objects is driving demand for ubiquitous connectivity and
intensive computing capacity. 5G and upcoming 6G networks are crucial to
meeting these demands and the fast-evolving services and applications. However,
traditional terrestrial networks face limitations in coverage and capacity.
Integrated Terrestrial and Non-Terrestrial Networks (ITNTN) are emerging to
address these challenges. In essence, ITNTN combines ground-based
infrastructure with aerial, space, and water surface networks to provide
seamless connectivity and computing resources anytime, anywhere. Given the
stringent quality-of-service (QoS) of future services, edge computing will be
an inseparable component of ITNTN. Consequently, we dive in this survey into
current efforts of integrating cloud/fog/edge computing into ITNTN layers to
facilitate stringent QoS services and address the data processing needs of
modern applications. Since there have been only limited and partial efforts in
integrating computing functionalities within ITNTN, we aim to extend the
discussion to the full integration of computing and identifying the challenges
and future research directions to achieve it.

</details>


### [207] [A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case](https://arxiv.org/abs/2505.03196)
*Haoxiang Luo,Gang Sun,Yinqiu Liu,Dusit Niyato,Hongfang Yu,Mohammed Atiquzzaman,Schahram Dustdar*

Main category: cs.NI

TL;DR: 提出一个基于区块链的多LLM协作框架 (MultiLLMN)，以解决单个LLM在网络优化中策略不一、响应可能存在低置信度或偏见的问题。


<details>
  <summary>Details</summary>
Motivation: 不同的LLM因模型结构、训练数据和方法的差异，对同一网络问题可能提供不同的优化策略。此外，单个LLM的训练数据局限性以及托管设备的潜在恶意性，可能导致低置信度甚至有偏见的响应。

Method: 提出一个基于区块链的信任多LLM网络 (Trustworthy MultiLLMN) 框架。该框架连接多个LLM，通过协作评估和选择机制，筛选出针对复杂网络优化问题的最可靠、高质量的响应。并通过虚假基站 (FBS) 防御作为案例研究来验证其有效性。

Result: 案例研究（虚假基站防御）初步验证了所提出的Trustworthy MultiLLMN方法在应对复杂网络威胁方面的有效性。

Conclusion: 所提出的Trustworthy MultiLLMN框架能够整合多个LLM的优势，通过协作评估和选择，提高LLM在解决网络优化问题时响应的可靠性和质量，克服单个LLM的局限性。

Abstract: Large Language Models (LLMs) demonstrate strong potential across a variety of
tasks in communications and networking due to their advanced reasoning
capabilities. However, because different LLMs have different model structures
and are trained using distinct corpora and methods, they may offer varying
optimization strategies for the same network issues. Moreover, the limitations
of an individual LLM's training data, aggravated by the potential maliciousness
of its hosting device, can result in responses with low confidence or even
bias. To address these challenges, we propose a blockchain-enabled
collaborative framework that connects multiple LLMs into a Trustworthy
Multi-LLM Network (MultiLLMN). This architecture enables the cooperative
evaluation and selection of the most reliable and high-quality responses to
complex network optimization problems. Specifically, we begin by reviewing
related work and highlighting the limitations of existing LLMs in collaboration
and trust, emphasizing the need for trustworthiness in LLM-based systems. We
then introduce the workflow and design of the proposed Trustworthy MultiLLMN
framework. Given the severity of False Base Station (FBS) attacks in B5G and 6G
communication systems and the difficulty of addressing such threats through
traditional modeling techniques, we present FBS defense as a case study to
empirically validate the effectiveness of our approach. Finally, we outline
promising future research directions in this emerging area.

</details>


### [208] [Efficient Wi-Fi Sensing for IoT Forensics with Lossy Compression of CSI Data](https://arxiv.org/abs/2505.03375)
*Paolo Cerutti,Fabio Palmese,Marco Cominelli,Alessandro E. C. Redondi*

Main category: cs.NI

TL;DR: 该研究探讨了有损压缩技术对Wi-Fi感知准确性的影响，发现传统PCA方法适用于轻量级物联网取证，而深度学习模型在复杂活动识别中压缩潜力更大但资源需求更高。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi感知中的信道状态信息（CSI）数据维度高，给资源受限的物联网（IoT）环境中的存储、传输和处理带来了挑战。

Method: 研究评估了传统有损压缩技术（如主成分分析PCA）和基于深度学习的压缩方法对Wi-Fi感知准确性的影响，并考虑了两种不同的感知应用场景。

Result: 简单的、可解释的PCA技术能显著减少CSI数据量同时保持分类性能，适用于轻量级物联网取证场景。深度学习模型在复杂应用（如活动识别）中展现出更高的压缩潜力（压缩比可达16000:1，对感知性能影响极小），但需要仔细调优和更大的计算资源。

Conclusion: 将有损压缩方案集成到Wi-Fi感知流程中是可行的，这可以提高智能物联网系统的效率，并改善取证应用中的存储需求。

Abstract: Wi-Fi sensing is an emerging technology that uses channel state information
(CSI) from ambient Wi-Fi signals to monitor human activity without the need for
dedicated sensors. Wi-Fi sensing does not only represent a pivotal technology
in intelligent Internet of Things (IoT) systems, but it can also provide
valuable insights in forensic investigations. However, the high dimensionality
of CSI data presents major challenges for storage, transmission, and processing
in resource-constrained IoT environments. In this paper, we investigate the
impact of lossy compression on the accuracy of Wi-Fi sensing, evaluating both
traditional techniques and a deep learning-based approach. Our results reveal
that simple, interpretable techniques based on principal component analysis can
significantly reduce the CSI data volume while preserving classification
performance, making them highly suitable for lightweight IoT forensic
scenarios. On the other hand, deep learning models exhibit higher potential in
complex applications like activity recognition (achieving compression ratios up
to 16000:1 with minimal impact on sensing performance) but require careful
tuning and greater computational resources. By considering two different
sensing applications, this work demonstrates the feasibility of integrating
lossy compression schemes into Wi-Fi sensing pipelines to make intelligent IoT
systems more efficient and improve the storage requirements in forensic
applications.

</details>


### [209] [Advancing Remote and Continuous Cardiovascular Patient Monitoring through a Novel and Resource-efficient IoT-Driven Framework](https://arxiv.org/abs/2505.03409)
*Sanam Nayab,Sohail Raza Chohan,Aqsa Jameel,Syed Rehan Shah,Syed Ahsan Masud Zaidi,Aditya Nath Jha,Kamran Siddique*

Main category: cs.NI

TL;DR: 本文提出了一种基于物联网的远程实时心脏监测系统，通过测量关键心脏指标并结合云应用及警报机制，旨在为心脏病患者提供持续、便捷的医疗监护。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，现有监测系统依赖住院，难以实现连续监测。特别是针对巴基斯坦的老龄人口，迫切需要可及且持续的医疗保健方案以减少突发性致命事件。

Method: 开发了一套物联网套件，用于测量体温、心率、血压、血氧饱和度（SPO2）和心电图（ECG）。该系统与基于云的应用程序集成，实现持续远程监控，并设有警报机制，以便在异常情况发生时及时通知医疗专业人员。

Result: 在对20名参与者的临床环境测试中，该系统收集的数据与标准医疗设备获得的结果高度一致。

Conclusion: 研究结果证实了该系统在可靠远程监测方面的潜力，为主动心脏健康管理提供了重要进展，并为降低心脏病患者的意外死亡率提供了一种经济高效的解决方案。

Abstract: Cardiovascular diseases are a leading cause of fatalities worldwide, often
occurring suddenly with limited time for intervention. Current healthcare
monitoring systems for cardiac patients rely heavily on hospitalization, which
can be impractical for continuous monitoring. This paper presents a novel
IoT-based solution for remote, real-time tracking of critical cardiac metrics,
addressing the pressing need for accessible and continuous healthcare,
particularly for the aging population in Pakistan. The proposed IoT kit
measures essential parameters such as body temperature, heart rate (HR), blood
pressure (BP), oxygen saturation (SPO2), and electrocardiography (ECG).
  A key innovation of the system is its integration with a cloud-based
application, enabling constant remote monitoring and incorporating an alarm
mechanism to alert medical professionals for timely intervention, reducing the
risk of catastrophic incidents. The system was tested in a clinical environment
with 20 participants, demonstrating results closely aligned with those obtained
using standard medical devices. The findings validate the system's potential
for reliable remote monitoring, offering a significant step forward in
proactive cardiac healthcare management. This novel approach combines IoT
technology with cloud-based applications to provide a cost-effective and
efficient solution for reducing unexpected fatalities among cardiac patients.

</details>


### [210] [Multi-Agent Reinforcement Learning Scheduling to Support Low Latency in Teleoperated Driving](https://arxiv.org/abs/2505.03558)
*Giacomo Avanzi,Marco Giordani,Michele Zorzi*

Main category: cs.NI

TL;DR: 本文提出了一种基于多智能体强化学习（MARL）的调度算法，通过动态分配无线电资源，以最小化远程驾驶（TD）场景中的端到端（E2E）延迟，并证明了集中式聚合训练（MAPPO）结合贪婪分配（GA）策略能取得最佳效果。


<details>
  <summary>Details</summary>
Motivation: 远程驾驶对通信服务质量（QoS）有严格要求，特别是端到端延迟和可靠性。现有方法如预测性QoS结合强化学习进行数据压缩会损害数据质量。本研究旨在通过在无线接入网（RAN）层面优化无线电参数来改善QoS，同时保持数据质量。

Method: 本文提出了基于近端策略优化（PPO）的多智能体强化学习（MARL）调度算法。评估了两种训练范式：具有局部观测的去中心化学习（IPPO）和集中式聚合（MAPPO），并结合了两种资源分配策略：比例分配（PA）和贪婪分配（GA）。通过ns-3仿真进行评估。

Result: ns-3仿真结果表明，MAPPO与GA相结合，在最小化E2E延迟方面取得了最佳效果，尤其是在车辆数量增加时。

Conclusion: 所提出的MARL调度算法，特别是MAPPO与GA的结合，能够有效地动态分配无线电资源，从而最小化远程驾驶场景中的端到端延迟。

Abstract: The teleoperated driving (TD) scenario comes with stringent Quality of
Service (QoS) communication constraints, especially in terms of end-to-end
(E2E) latency and reliability. In this context, Predictive Quality of Service
(PQoS), possibly combined with Reinforcement Learning (RL) techniques, is a
powerful tool to estimate QoS degradation and react accordingly. For example,
an intelligent agent can be trained to select the optimal compression
configuration for automotive data, and reduce the file size whenever QoS
conditions deteriorate. However, compression may inevitably compromise data
quality, with negative implications for the TD application. An alternative
strategy involves operating at the Radio Access Network (RAN) level to optimize
radio parameters based on current network conditions, while preserving data
quality. In this paper, we propose Multi-Agent Reinforcement Learning (MARL)
scheduling algorithms, based on Proximal Policy Optimization (PPO), to
dynamically and intelligently allocate radio resources to minimize E2E latency
in a TD scenario. We evaluate two training paradigms, i.e., decentralized
learning with local observations (IPPO) vs. centralized aggregation (MAPPO), in
conjunction with two resource allocation strategies, i.e., proportional
allocation (PA) and greedy allocation (GA). We prove via ns-3 simulations that
MAPPO, combined with GA, achieves the best results in terms of latency,
especially as the number of vehicles increases.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [211] [Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve Sparse Retrieval](https://arxiv.org/abs/2505.03676)
*Arthur Satouf,Gabriel Ben Zenou,Benjamin Piwowarski,Habiboulaye Amadou Boubacar,Pablo Piantanida*

Main category: cs.IR

TL;DR: 本文提出将理性言语行为（RSA）框架应用于稀疏信息检索，通过考虑整个文档集动态调整词项权重，以改善文档表示。


<details>
  <summary>Details</summary>
Motivation: 当前的稀疏神经信息检索方法及传统模型（如BM25）在表示单个文档时，未能充分考虑文档集合以及不同词项权重之间的复杂相互作用。

Method: 将语言学中的理性言语行为（RSA）框架应用于信息检索。RSA通过考虑数据集中其他文档的影响，动态地调整词元（token）与文档之间的交互，从而更好地区分文档表示。

Result: 实验表明，整合RSA能够持续改进多种稀疏检索模型，并在BEIR基准测试的域外数据集上取得了当前最佳（SOTA）性能。

Conclusion: 应用RSA框架，通过考虑文档集合的全局信息来动态调整词项权重，增强了文档表示的对比度，从而有效提升了稀疏信息检索模型的性能，尤其在域外数据集上表现优异。

Abstract: Current sparse neural information retrieval (IR) methods, and to a lesser
extent more traditional models such as BM25, do not take into account the
document collection and the complex interplay between different term weights
when representing a single document. In this paper, we show how the Rational
Speech Acts (RSA), a linguistics framework used to minimize the number of
features to be communicated when identifying an object in a set, can be adapted
to the IR case -- and in particular to the high number of potential features
(here, tokens). RSA dynamically modulates token-document interactions by
considering the influence of other documents in the dataset, better contrasting
document representations. Experiments show that incorporating RSA consistently
improves multiple sparse retrieval models and achieves state-of-the-art
performance on out-of-domain datasets from the BEIR benchmark.
https://github.com/arthur-75/Rational-Retrieval-Acts

</details>


### [212] [Feature Staleness Aware Incremental Learning for CTR Prediction](https://arxiv.org/abs/2505.02844)
*Zhikai Wang,Yanyan Shen,Zibin Zhang,Kangyi Lin*

Main category: cs.IR

TL;DR: 该研究提出了一种名为FeSAIL的方法，通过自适应地重放包含过时特征的样本，并利用特定采样和正则化机制，来解决CTR预测中增量学习导致的特征过时问题。


<details>
  <summary>Details</summary>
Motivation: 在推荐系统的点击率（CTR）预测中，为了提高训练效率，模型通常进行增量更新。然而，当某些特征未在新的增量数据中出现时，其嵌入会变得“过时”，导致模型在包含这些过时特征的样本上性能下降，即特征过时问题。

Method: 提出了特征过时感知增量学习方法（FeSAIL）。该方法首先引入一种过时感知采样算法（SAS），以高效率采样固定数量的包含过时特征的样本进行重放；然后引入一种过时感知正则化机制（SAR），用于精细控制特征嵌入的更新过程。

Result: 将FeSAIL应用于通用的深度学习CTR预测模型，在四个基准数据集上的实验结果表明，FeSAIL的表现优于多种当前最先进的方法。

Conclusion: FeSAIL方法能够有效缓解CTR预测中增量学习带来的特征过时问题，并提升模型性能。

Abstract: Click-through Rate (CTR) prediction in real-world recommender systems often
deals with billions of user interactions every day. To improve the training
efficiency, it is common to update the CTR prediction model incrementally using
the new incremental data and a subset of historical data. However, the feature
embeddings of a CTR prediction model often get stale when the corresponding
features do not appear in current incremental data. In the next period, the
model would have a performance degradation on samples containing stale
features, which we call the feature staleness problem. To mitigate this
problem, we propose a Feature Staleness Aware Incremental Learning method for
CTR prediction (FeSAIL) which adaptively replays samples containing stale
features. We first introduce a staleness aware sampling algorithm (SAS) to
sample a fixed number of stale samples with high sampling efficiency. We then
introduce a staleness aware regularization mechanism (SAR) for a fine-grained
control of the feature embedding updating. We instantiate FeSAIL with a general
deep learning-based CTR prediction model and the experimental results
demonstrate FeSAIL outperforms various state-of-the-art methods on four
benchmark datasets.

</details>


### [213] [Avoid Recommending Out-of-Domain Items: Constrained Generative Recommendation with LLMs](https://arxiv.org/abs/2505.03336)
*Hao Liao,Wensheng Lu,Jianxun Lian,Mingqi Wu,Shuo Wang,Yong Zhang,Yitian Huang,Mingyang Zhou,Xing Xie*

Main category: cs.IR

TL;DR: 该研究比较了两种方法（RecLM-ret 和 RecLM-cgen）以防止大语言模型在推荐系统中推荐领域外项目，发现 RecLM-cgen 在准确性和消除领域外推荐方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型 (LLM) 在生成式推荐系统中显示出潜力，但确保它们不推荐领域外 (OOD) 项目仍然是一个挑战。

Method: 研究了两种不同的方法来解决这个问题：RecLM-ret (一种基于检索的方法) 和 RecLM-cgen (一种受约束的生成方法)。两种方法都能与现有 LLM 无缝集成，以确保领域内推荐。

Result: 在三个推荐数据集上的综合实验表明，RecLM-cgen 在准确性方面始终优于 RecLM-ret 和现有的基于 LLM 的推荐模型，同时消除了 OOD 推荐。此外，RecLM-cgen 保持了强大的通用能力，并且是一个轻量级的即插即用模块。

Conclusion: RecLM-cgen 是确保 LLM 在推荐系统中进行领域内推荐的首选方法，因为它在准确性、消除 OOD 推荐方面表现更佳，并具有易于集成的实用优势。

Abstract: Large Language Models (LLMs) have shown promise for generative recommender
systems due to their transformative capabilities in user interaction. However,
ensuring they do not recommend out-of-domain (OOD) items remains a challenge.
We study two distinct methods to address this issue: RecLM-ret, a
retrieval-based method, and RecLM-cgen, a constrained generation method. Both
methods integrate seamlessly with existing LLMs to ensure in-domain
recommendations. Comprehensive experiments on three recommendation datasets
demonstrate that RecLM-cgen consistently outperforms RecLM-ret and existing
LLM-based recommender models in accuracy while eliminating OOD recommendations,
making it the preferred method for adoption. Additionally, RecLM-cgen maintains
strong generalist capabilities and is a lightweight plug-and-play module for
easy integration into LLMs, offering valuable practical benefits for the
community. Source code is available at https://github.com/microsoft/RecAI

</details>


### [214] [Modeling Musical Genre Trajectories through Pathlet Learning](https://arxiv.org/abs/2505.03480)
*Lilian Marey,Charlotte Laclau,Bruno Sguerra,Tiphaine Viard,Manuel Moussallam*

Main category: cs.IR

TL;DR: 本文提出一种基于字典学习的新框架“pathlets”，用于模拟用户音乐流派偏好的演变轨迹，并揭示相关的收听模式。


<details>
  <summary>Details</summary>
Motivation: 尽管音乐流媒体平台用户数据丰富，但理解用户音乐偏好（尤其是其随时间的变化）仍然是一个复杂挑战。

Method: 采用字典学习范式对用户跨不同音乐流派的轨迹进行建模；定义了一个名为“pathlets”的新框架来捕捉流派轨迹中的重复模式，从而创建可理解的轨迹嵌入；使用了Deezer提供的2000名用户17个月的流派标注收听历史数据集。

Result: “Pathlet learning”方法揭示了相关的、可进行定性和定量分析的收听模式；该研究增进了对用户与音乐互动的理解，并公开发布了数据集和代码。

Conclusion: 这项工作通过“pathlets”框架改进了对用户音乐互动和品味演变的理解，为用户行为研究和提升推荐系统多样性开辟了新的研究途径。

Abstract: The increasing availability of user data on music streaming platforms opens
up new possibilities for analyzing music consumption. However, understanding
the evolution of user preferences remains a complex challenge, particularly as
their musical tastes change over time. This paper uses the dictionary learning
paradigm to model user trajectories across different musical genres. We define
a new framework that captures recurring patterns in genre trajectories, called
pathlets, enabling the creation of comprehensible trajectory embeddings. We
show that pathlet learning reveals relevant listening patterns that can be
analyzed both qualitatively and quantitatively. This work improves our
understanding of users' interactions with music and opens up avenues of
research into user behavior and fostering diversity in recommender systems. A
dataset of 2000 user histories tagged by genre over 17 months, supplied by
Deezer (a leading music streaming company), is also released with the code.

</details>


### [215] [Counterfactual Inference for Eliminating Sentiment Bias in Recommender Systems](https://arxiv.org/abs/2505.03655)
*Le Pan,Yuanjiang Cao,Chengkai Huang,Wenjie Zhang,Lina Yao*

Main category: cs.IR

TL;DR: 该研究针对评论推荐系统中的情感偏见问题，提出了一种基于反事实推断的方法，以减轻该偏见并提高推荐的公平性。


<details>
  <summary>Details</summary>
Motivation: 在基于评论的推荐系统中，存在一种情感偏见现象：带有负面评论的用户或物品的推荐准确性会降低，这对批判性用户和冷门物品不利，导致推荐不公。

Method: 研究从反事实推断的视角分两阶段解决此问题。在模型训练阶段，构建因果图并建模情感如何影响最终评分。在推断阶段，解耦直接和间接效应，并使用反事实推断消除情感偏见的间接影响。

Result: 大量实验结果验证了该模型能够在评分预测上达到相当的性能，从而实现更好的推荐，并有效减轻情感偏见。

Conclusion: 据我们所知，这是首个在推荐系统中采用反事实推断来减轻情感偏见的研究工作。

Abstract: Recommender Systems (RSs) aim to provide personalized recommendations for
users. A newly discovered bias, known as sentiment bias, uncovers a common
phenomenon within Review-based RSs (RRSs): the recommendation accuracy of users
or items with negative reviews deteriorates compared with users or items with
positive reviews. Critical users and niche items are disadvantaged by such
unfair recommendations. We study this problem from the perspective of
counterfactual inference with two stages. At the model training stage, we build
a causal graph and model how sentiment influences the final rating score.
During the inference stage, we decouple the direct and indirect effects to
mitigate the impact of sentiment bias and remove the indirect effect using
counterfactual inference. We have conducted extensive experiments, and the
results validate that our model can achieve comparable performance on rating
prediction for better recommendations and effective mitigation of sentiment
bias. To the best of our knowledge, this is the first work to employ
counterfactual inference on sentiment bias mitigation in RSs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [216] [Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories](https://arxiv.org/abs/2505.03443)
*Valerio Bellandi*

Main category: cs.DC

TL;DR: 本文介绍了一个为意大利司法部开发的分布式文档存储库系统，该系统利用边缘存储库分析文本和元数据，以增强语义探索能力。


<details>
  <summary>Details</summary>
Motivation: 传统集中式系统难以满足大规模应用（如意大利司法部的文档库）在可扩展性、容错性和高级分析（如语义探索）方面的需求。分布式系统为此类需求提供了更优越的解决方案。

Method: 开发并研究了一个分布式文档存储库系统，该系统利用边缘存储库（edge repositories）对文本数据和元数据进行分析。

Result: 该系统增强了对文本数据和元数据的语义探索能力。

Conclusion: 所探讨的分布式文档存储库系统通过边缘存储库分析，有效提升了语义探索能力，证明了分布式架构在满足特定机构大规模文档管理和高级分析需求方面的价值和潜力。

Abstract: Centralized and distributed systems are two main approaches to organizing ICT
infrastructure, each with its pros and cons. Centralized systems concentrate
resources in one location, making management easier but creating single points
of failure. Distributed systems, on the other hand, spread resources across
multiple nodes, offering better scalability and fault tolerance, but requiring
more complex management. The choice between them depends on factors like
application needs, scalability, and data sensitivity. Centralized systems suit
applications with limited scalability and centralized control, while
distributed systems excel in large-scale environments requiring high
availability and performance. This paper explores a distributed document
repository system developed for the Italian Ministry of Justice, using edge
repositories to analyze textual data and metadata, enhancing semantic
exploration capabilities.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [217] [Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation](https://arxiv.org/abs/2505.03105)
*Xule Lin*

Main category: cs.HC

TL;DR: 本文介绍了一个名为 Cognitio Emergens (CE) 的新框架，用于理解人类与人工智能在科学知识创造中不断发展的共同进化伙伴关系。


<details>
  <summary>Details</summary>
Motivation: 现有的人工智能与人类协作模型侧重于静态角色或狭隘指标，未能捕捉科学理解如何通过长期递归的人机互动而涌现，因此需要新的理论框架。

Method: 引入 Cognitio Emergens (CE) 框架，该框架整合了三个组成部分：1) 机构配置 (Agency Configurations)，描述人与AI间的权力分配；2) 认知维度 (Epistemic Dimensions)，捕捉协作中涌现的六种能力；3) 伙伴关系动态 (Partnership Dynamics)，识别影响关系演变的因素。该框架借鉴了自创生理论、社会系统理论和组织模块化理论。

Result: CE框架揭示了知识的共同创造是如何通过角色、价值观和组织结构的持续协商而出现的。它将人机科学协作重新概念化为共同进化过程，并能形成指导发展的独特“能力特征”。

Conclusion: CE框架提供了一种平衡的视角来看待AI在科学中的演变角色，旨在培养既能保持有意义的人类参与又能实现变革性科学突破的伙伴关系，避免对AI的盲目乐观或不必要的恐惧。

Abstract: Scientific knowledge creation is fundamentally transforming as humans and AI
systems evolve beyond tool-user relationships into co-evolutionary epistemic
partnerships. When AlphaFold revolutionized protein structure prediction,
researchers described engaging with an epistemic partner that reshaped how they
conceptualized fundamental relationships. This article introduces Cognitio
Emergens (CE), a framework addressing critical limitations in existing models
that focus on static roles or narrow metrics while failing to capture how
scientific understanding emerges through recursive human-AI interaction over
time. CE integrates three components addressing these limitations: Agency
Configurations describing how authority distributes between humans and AI
(Directed, Contributory, Partnership), with partnerships dynamically
oscillating between configurations rather than following linear progression;
Epistemic Dimensions capturing six specific capabilities emerging through
collaboration across Discovery, Integration, and Projection axes, creating
distinctive "capability signatures" that guide development; and Partnership
Dynamics identifying forces shaping how these relationships evolve,
particularly the risk of epistemic alienation where researchers lose
interpretive control over knowledge they formally endorse. Drawing from
autopoiesis theory, social systems theory, and organizational modularity, CE
reveals how knowledge co-creation emerges through continuous negotiation of
roles, values, and organizational structures. By reconceptualizing human-AI
scientific collaboration as fundamentally co-evolutionary, CE offers a balanced
perspective that neither uncritically celebrates nor unnecessarily fears AI's
evolving role, instead providing conceptual tools for cultivating partnerships
that maintain meaningful human participation while enabling transformative
scientific breakthroughs.

</details>


### [218] [Augmenting Human Cognition through Everyday AR](https://arxiv.org/abs/2505.03492)
*Xiaoan Liu*

Main category: cs.HC

TL;DR: 该论文探讨了始终在线的AR如何成为直观的“思维工具”，通过将数字认知与物理环境相结合来增强人类任务表现和理解。


<details>
  <summary>Details</summary>
Motivation: 空间计算和多模态大语言模型的成熟，为AR成为直观“思维工具”并直接将智能嵌入日常环境提供了可能性。

Method: 探讨始终在线的增强现实（AR）如何无缝连接数字认知与物理世界的可交互性（affordances）。

Result: 实现主动的、情境感知的交互，从而提升人类的任务表现和理解能力。

Conclusion: 始终在线的AR有潜力成为一种直观的“思维工具”，通过将语义和情境感知智能嵌入日常环境，增强人类认知和与物理世界的交互。

Abstract: As spatial computing and multimodal LLMs mature, AR is tending to become an
intuitive "thinking tool," embedding semantic and context-aware intelligence
directly into everyday environments. This paper explores how always-on AR can
seamlessly bridge digital cognition and physical affordances, enabling
proactive, context-sensitive interactions that enhance human task performance
and understanding.

</details>


### [219] [BCause: Human-AI collaboration to improve hybrid mapping and ideation in argumentation-grounded deliberation](https://arxiv.org/abs/2505.03584)
*Lucas Anastasiou,Anna De Liddo*

Main category: cs.HC

TL;DR: BCause系统利用生成式AI和人机协作，将非结构化的公共议题对话转化为结构化、可操作的民主流程。


<details>
  <summary>Details</summary>
Motivation: 公共审议常面临讨论分散肤浅、意义建构不佳以及与可操作政策成果脱节的问题。

Method: 引入BCause讨论系统，该系统利用生成式AI和人机协作来处理公共议题的非结构化对话，并将其转化为结构化、可操作的民主流程。

Result: 论文提出了三项创新：1) 将非结构化转录本导入并转化为论证性讨论；2) 通过Telegram机器人实现地理审议问题感知，用于本地问题报告；3) 提供带有可定制小部件（如摘要、主题建模、政策建议、论点聚类）的智能报告功能。

Conclusion: BCause系统通过人机协作，旨在将非结构化公共对话转变为结构化、可操作的民主过程，同时强调人类参与以确保伦理监督、情境相关性和创造性综合，从而改善公共审议。

Abstract: Public deliberation, as in open discussion of issues of public concern, often
suffers from scattered and shallow discourse, poor sensemaking, and a
disconnect from actionable policy outcomes. This paper introduces BCause, a
discussion system leveraging generative AI and human-machine collaboration to
transform unstructured dialogue around public issues (such as urban living,
policy changes, and current socio-economic transformations) into structured,
actionable democratic processes. We present three innovations: (i) importing
and transforming unstructured transcripts into argumentative discussions, (ii)
geo-deliberated problem-sensing via a Telegram bot for local issue reporting,
and (iii) smart reporting with customizable widgets (e.g., summaries, topic
modelling, policy recommendations, clustered arguments). The system's human-AI
partnership preserves critical human participation to ensure ethical oversight,
contextual relevance, and creative synthesis.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [220] [BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models](https://arxiv.org/abs/2505.03501)
*Zihan Wang,Hongwei Li,Rui Zhang,Wenbo Jiang,Kangjie Chen,Tianwei Zhang,Qingchuan Zhao,Guowen Xu*

Main category: cs.CR

TL;DR: 提出了一种针对大型语言模型的语言后门攻击 (lingual-backdoor attacks)，其中语言本身作为触发器，并设计了BadLingual以实现任务无关的攻击。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在被恶意利用针对特定语言群体、加剧歧视的风险。现有的后门攻击研究未充分关注以语言为触发器的攻击方式及其在多语言场景下的潜在危害。

Method: 首先实现了一种基线语言后门攻击，通过将特定下游任务的训练数据翻译成触发语言进行投毒。为解决基线攻击任务泛化能力差的问题，设计了BadLingual，一种任务无关的语言后门攻击方法，使用基于PPL约束的贪婪坐标梯度搜索 (PGCG) 的对抗训练来扩展后门决策边界，增强其跨任务泛化能力。

Result: 基线攻击在指定任务上攻击成功率 (ASR) 超过90%，但在任务无关场景下仅为37.61%。相比之下，BadLingual在任务无关场景下比基线攻击提升了高达37.35%。

Conclusion: 研究揭示了具有多语言能力的大型语言模型的一种新漏洞，强调了语言后门攻击的潜在威胁，并期望推动未来针对此类攻击的防御研究，以增强大型语言模型的鲁棒性。

Abstract: In this paper, we present a new form of backdoor attack against Large
Language Models (LLMs): lingual-backdoor attacks. The key novelty of
lingual-backdoor attacks is that the language itself serves as the trigger to
hijack the infected LLMs to generate inflammatory speech. They enable the
precise targeting of a specific language-speaking group, exacerbating racial
discrimination by malicious entities. We first implement a baseline
lingual-backdoor attack, which is carried out by poisoning a set of training
data for specific downstream tasks through translation into the trigger
language. However, this baseline attack suffers from poor task generalization
and is impractical in real-world settings. To address this challenge, we design
BadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any
downstream tasks within the chat LLMs, regardless of the specific questions of
these tasks. We design a new approach using PPL-constrained Greedy Coordinate
Gradient-based Search (PGCG) based adversarial training to expand the decision
boundary of lingual-backdoor, thereby enhancing the generalization ability of
lingual-backdoor across various tasks. We perform extensive experiments to
validate the effectiveness of our proposed attacks. Specifically, the baseline
attack achieves an ASR of over 90% on the specified tasks. However, its ASR
reaches only 37.61% across six tasks in the task-agnostic scenario. In
contrast, BadLingual brings up to 37.35% improvement over the baseline. Our
study sheds light on a new perspective of vulnerabilities in LLMs with
multilingual capabilities and is expected to promote future research on the
potential defenses to enhance the LLMs' robustness

</details>


### [221] [Adversarial Sample Generation for Anomaly Detection in Industrial Control Systems](https://arxiv.org/abs/2505.03120)
*Abdul Mustafa,Muhammad Talha Khan,Muhammad Azmi Umer,Zaki Masood,Chuadhry Mujeeb Ahmed*

Main category: cs.CR

TL;DR: 本文通过使用JSMA生成对抗样本来训练入侵检测系统（IDS），使其能更有效地识别针对工控系统（ICS）的对抗性攻击，在真实测试平台上达到了95%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 基于机器学习（ML）的入侵检测系统（IDS）易受对抗性攻击，因此需要在恶意实体利用这些漏洞之前，让IDS学会识别对抗样本。

Method: 使用雅可比显著图攻击（JSMA）生成对抗样本，并在一个安全水处理（SWaT）操作测试平台上验证这些样本在工业控制系统（ICS）中应对真实攻击的泛化性和可扩展性。然后用这些对抗样本训练模型。

Result: 经过对抗样本训练的模型，在未用于训练的真实世界攻击数据上，检测攻击的准确率达到了95%。

Conclusion: 通过使用JSMA生成的对抗样本进行训练，可以显著提高基于机器学习的入侵检测系统在工业控制系统环境中对抗真实攻击的鲁棒性和准确性。

Abstract: Machine learning (ML)-based intrusion detection systems (IDS) are vulnerable
to adversarial attacks. It is crucial for an IDS to learn to recognize
adversarial examples before malicious entities exploit them. In this paper, we
generated adversarial samples using the Jacobian Saliency Map Attack (JSMA). We
validate the generalization and scalability of the adversarial samples to
tackle a broad range of real attacks on Industrial Control Systems (ICS). We
evaluated the impact by assessing multiple attacks generated using the proposed
method. The model trained with adversarial samples detected attacks with 95%
accuracy on real-world attack data not used during training. The study was
conducted using an operational secure water treatment (SWaT) testbed.

</details>


### [222] [Detecting Quishing Attacks with Machine Learning Techniques Through QR Code Analysis](https://arxiv.org/abs/2505.03451)
*Fouad Trad,Ali Chehab*

Main category: cs.CR

TL;DR: 本文提出了一种无需提取二维码内容，直接分析其结构和像素模式来检测“Quishing”（二维码网络钓鱼）的新框架，并通过机器学习模型验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 二维码网络钓鱼（Quishing）日益增多，现有检测方法依赖URL分析，不仅可能让用户暴露于恶意内容，且无法覆盖所有类型的二维码数据（如Wi-Fi凭证、支付信息），因此需要新的检测方法。

Method: 提出首个直接分析二维码结构和像素模式的Quishing检测框架，不提取嵌入内容。研究者生成了包含钓鱼和良性二维码的数据集，并使用多种机器学习模型（包括逻辑回归、决策树、随机森林、朴素贝叶斯、LightGBM和XGBoost）进行训练和评估。通过特征重要性分析识别关键视觉指标并优化特征集。

Result: 最佳模型XGBoost在不提取二维码内容的情况下实现了0.9106的AUC。通过移除非信息性像素优化特征集后，性能提升至AUC 0.9133。研究发现二维码的结构特征与钓鱼风险强相关。

Conclusion: 该工作为Quishing缓解奠定了基础，证明了直接分析二维码结构和像素模式作为一种有效的检测手段是可行的，并强调了其在现代网络钓鱼防御中作为关键层的潜力。

Abstract: The rise of QR code based phishing ("Quishing") poses a growing cybersecurity
threat, as attackers increasingly exploit QR codes to bypass traditional
phishing defenses. Existing detection methods predominantly focus on URL
analysis, which requires the extraction of the QR code payload, and may
inadvertently expose users to malicious content. Moreover, QR codes can encode
various types of data beyond URLs, such as Wi-Fi credentials and payment
information, making URL-based detection insufficient for broader security
concerns. To address these gaps, we propose the first framework for quishing
detection that directly analyzes QR code structure and pixel patterns without
extracting the embedded content. We generated a dataset of phishing and benign
QR codes and we used it to train and evaluate multiple machine learning models,
including Logistic Regression, Decision Trees, Random Forest, Naive Bayes,
LightGBM, and XGBoost. Our best-performing model (XGBoost) achieves an AUC of
0.9106, demonstrating the feasibility of QR-centric detection. Through feature
importance analysis, we identify key visual indicators of malicious intent and
refine our feature set by removing non-informative pixels, improving
performance to an AUC of 0.9133 with a reduced feature space. Our findings
reveal that the structural features of QR code correlate strongly with phishing
risk. This work establishes a foundation for quishing mitigation and highlights
the potential of direct QR analysis as a critical layer in modern phishing
defenses.

</details>


### [223] [LlamaFirewall: An open source guardrail system for building secure AI agents](https://arxiv.org/abs/2505.03574)
*Sahana Chennabasappa,Cyrus Nikolaidis,Daniel Song,David Molnar,Stephanie Ding,Shengye Wan,Spencer Whitman,Lauren Deason,Nicholas Doucette,Abraham Montilla,Alekhya Gampa,Beto de Paola,Dominik Gabi,James Crnkovich,Jean-Christophe Testud,Kat He,Rashnil Chaturvedi,Wu Zhou,Joshua Saxe*

Main category: cs.CR

TL;DR: LlamaFirewall是一个开源的安全防护框架，旨在作为AI代理的最后一道防线，以减轻提示注入、代理失准和不安全代码等风险。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）已发展为能够执行复杂任务的自主代理，这引入了现有安全措施无法完全解决的新安全风险，因此迫切需要一个实时防护监控器作为最终防御层。

Method: 引入LlamaFirewall框架，该框架包含三个核心防护机制：PromptGuard 2（通用越狱检测器）、Agent Alignment Checks（检查代理推理的思维链审计器）和CodeShield（在线静态代码分析引擎），并提供易于使用的可定制扫描器。

Result: PromptGuard 2展现了最先进的通用越狱检测性能；Agent Alignment Checks在防止间接注入方面显示出比以往方法更强的功效；CodeShield能够快速且可扩展地防止生成不安全代码。该框架还包含易于开发者更新安全防护的可定制扫描器。

Conclusion: LlamaFirewall通过缓解提示注入、代理失准和不安全代码风险，为AI代理提供了关键的最后一道安全防线，其强大的防护机制和可定制性为AI系统的安全部署提供了支持。

Abstract: Large language models (LLMs) have evolved from simple chatbots into
autonomous agents capable of performing complex tasks such as editing
production code, orchestrating workflows, and taking higher-stakes actions
based on untrusted inputs like webpages and emails. These capabilities
introduce new security risks that existing security measures, such as model
fine-tuning or chatbot-focused guardrails, do not fully address. Given the
higher stakes and the absence of deterministic solutions to mitigate these
risks, there is a critical need for a real-time guardrail monitor to serve as a
final layer of defense, and support system level, use case specific safety
policy definition and enforcement. We introduce LlamaFirewall, an open-source
security focused guardrail framework designed to serve as a final layer of
defense against security risks associated with AI Agents. Our framework
mitigates risks such as prompt injection, agent misalignment, and insecure code
risks through three powerful guardrails: PromptGuard 2, a universal jailbreak
detector that demonstrates clear state of the art performance; Agent Alignment
Checks, a chain-of-thought auditor that inspects agent reasoning for prompt
injection and goal misalignment, which, while still experimental, shows
stronger efficacy at preventing indirect injections in general scenarios than
previously proposed approaches; and CodeShield, an online static analysis
engine that is both fast and extensible, aimed at preventing the generation of
insecure or dangerous code by coding agents. Additionally, we include
easy-to-use customizable scanners that make it possible for any developer who
can write a regular expression or an LLM prompt to quickly update an agent's
security guardrails.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [224] [Robustly Invertible Nonlinear Dynamics and the BiLipREN: Contracting Neural Models with Contracting Inverses](https://arxiv.org/abs/2505.03069)
*Yurui Zhang,Ruigang Wang,Ian R. Manchester*

Main category: eess.SY

TL;DR: 研究了非线性动力系统的可逆性，并提出了一种名为BiLipREN的新型可逆循环神经网络模型，该模型通过双向Lipschitz特性确保鲁棒可逆性。


<details>
  <summary>Details</summary>
Motivation: 解决非线性动力系统在存在扰动时难以保证鲁棒可逆性的问题，特别是需要能够从受扰动的输出中稳健地重构原始输入序列。

Method: 1. 从收缩和增量稳定性分析的角度研究非线性系统可逆性。2. 定义鲁棒可逆性为：正向模型及其逆模型均具有收缩性（即增量指数稳定）和Lipschitz特性（即有界增量增益）。3. 提出一种新的参数化神经动力学模型：双Lipschitz循环平衡网络（biLipREN），该网络通过构造保证鲁棒可逆性。4. biLipREN可以与正交线性系统组合以构建更通用的双Lipschitz动态模型。

Result: 成功构建了biLipREN模型，该模型通过构造即具有鲁棒可逆性。这意味着它对输入扰动具有鲁棒性，并且能够从对应的输出中鲁棒地区分不同的输入，即使在初始条件和测量输出存在小扰动的情况下也能稳健地重构输入序列。通过数值示例证明了该方法的实用性。

Conclusion: 提出的biLipREN模型为设计鲁棒可逆的非线性动力系统提供了一种有效途径。双Lipschitz特性是确保系统对输入扰动鲁棒以及输入信号可从输出中稳健重构的关键。该方法还可以扩展用于构建更通用的双Lipschitz动态模型。

Abstract: We study the invertibility of nonlinear dynamical systems from the
perspective of contraction and incremental stability analysis and propose a new
invertible recurrent neural model: the BiLipREN. In particular, we consider a
nonlinear state space model to be robustly invertible if an inverse exists with
a state space realisation, and both the forward model and its inverse are
contracting, i.e. incrementally exponentially stable, and Lipschitz, i.e. have
bounded incremental gain. This property of bi-Lipschitzness implies both
robustness in the sense of sensitivity to input perturbations, as well as
robust distinguishability of different inputs from their corresponding outputs,
i.e. the inverse model robustly reconstructs the input sequence despite small
perturbations to the initial conditions and measured output. Building on this
foundation, we propose a parameterization of neural dynamic models:
bi-Lipschitz recurrent equilibrium networks (biLipREN), which are robustly
invertible by construction. Moreover, biLipRENs can be composed with orthogonal
linear systems to construct more general bi-Lipschitz dynamic models, e.g., a
nonlinear analogue of minimum-phase/all-pass (inner/outer) factorization. We
illustrate the utility of our proposed approach with numerical examples.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [225] [Single-Sample and Robust Online Resource Allocation](https://arxiv.org/abs/2505.02963)
*Rohan Ghuge,Sahil Singla,Yifan Wang*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Online Resource Allocation problem is a central problem in many areas of
Computer Science, Operations Research, and Economics. In this problem, we
sequentially receive $n$ stochastic requests for $m$ kinds of shared resources,
where each request can be satisfied in multiple ways, consuming different
amounts of resources and generating different values. The goal is to achieve a
$(1-\epsilon)$-approximation to the hindsight optimum, where $\epsilon>0$ is a
small constant, assuming each resource has a large budget.
  In this paper, we investigate the learnability and robustness of online
resource allocation. Our primary contribution is a novel Exponential Pricing
algorithm with the following properties: 1. It requires only a \emph{single
sample} from each of the $n$ request distributions to achieve a
$(1-\epsilon)$-approximation for online resource allocation with large budgets.
Such an algorithm was previously unknown, even with access to polynomially many
samples, as prior work either assumed full distributional knowledge or was
limited to i.i.d.\,or random-order arrivals. 2. It is robust to corruptions in
the outliers model and the value augmentation model. Specifically, it maintains
its $(1 - \epsilon)$-approximation guarantee under both these robustness
models, resolving the open question posed in Argue, Gupta, Molinaro, and Singla
(SODA'22). 3. It operates as a simple item-pricing algorithm that ensures
incentive compatibility.
  The intuition behind our Exponential Pricing algorithm is that the price of a
resource should adjust exponentially as it is overused or underused. It differs
from conventional approaches that use an online learning algorithm for item
pricing. This departure guarantees that the algorithm will never run out of any
resource, but loses the usual no-regret properties of online learning
algorithms, necessitating a new analytical approach.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [226] [Taskmaster Deconstructed: A Quantitative Look at Tension, Volatility, and Viewer Ratings](https://arxiv.org/abs/2505.02886)
*David H. Silver*

Main category: physics.soc-ph

TL;DR: 研究分析了《Taskmaster》节目中评分动态与观众参与度的关系，发现评分机制对观众评分影响不大，观众兴趣更多受参赛者行为而非游戏机制驱动。


<details>
  <summary>Details</summary>
Motivation: 探究《Taskmaster》电视节目中的评分动态是否对其观众参与度（以IMDb评分为衡量标准）有显著影响。

Method: 对162集《Taskmaster》节目数据进行统计分析，使用15个剧集级指标量化排名波动性、分差、领先变化和胜者优势，并分析了参赛者排名轨迹以识别表演风格原型。

Result: 量化评分动态的各项指标与IMDb评分之间未发现显著关联。长期趋势显示平均得分有所增加，波动性略有下降，排名差距保持稳定。研究还识别出五种反复出现的参赛者表现风格原型。

Conclusion: 观众对《Taskmaster》的兴趣更多地受参赛者的行为和表演风格驱动，而非节目本身的竞争机制或评分动态。

Abstract: Taskmaster is a British television show that combines comedic performance
with a formal scoring system. Despite the appearance of structured competition,
it remains unclear whether scoring dynamics contribute meaningfully to audience
engagement. We conducted a statistical analysis of 162 episodes across 18
series, using fifteen episode-level metrics to quantify rank volatility, point
spread, lead changes, and winner dominance. None of these metrics showed a
significant association with IMDb ratings, even after controlling for series
effects. Long-term trends suggest that average points have increased over time,
while volatility has slightly declined and rank spread has remained stable.
These patterns indicate an attempt to enhance competitive visibility without
altering the show's structural equilibrium. We also analyzed contestant rank
trajectories and identified five recurring archetypes describing performance
styles. These patterns suggest that viewer interest is shaped more by
contestant behavior than by game mechanics.

</details>


### [227] [Floating Car Observers in Intelligent Transportation Systems: Detection Modeling and Temporal Insights](https://arxiv.org/abs/2505.02845)
*Jeremias Gerner,Klaus Bogenberger,Stefanie Schmidtner*

Main category: physics.soc-ph

TL;DR: 该研究探索了浮动车辆观测器 (FCO) 在微观交通仿真中的建模方法，并评估了其在智能交通系统 (ITS) 中的应用潜力，特别是在交通状态估计和监控方面。


<details>
  <summary>Details</summary>
Motivation: 传统的浮动车数据 (FCD) 提供的交通信息有限。浮动车辆观测器 (FCO) 通过集成车载传感器来检测和定位其他交通参与者，能够提供更丰富、更详细的交通数据。本研究旨在探索 FCO 检测的各种建模方法，并评估其在智能交通系统 (ITS) 应用中的潜力。

Method: 研究中探索了多种 FCO 检测建模方法，包括二维射线追踪、模拟真实传感器并集成三维物体检测算法的高保真协同仿真。此外，研究引入了一种基于神经网络的仿真技术，该技术能有效近似高保真协同仿真的结果，同时提供快速且可扩展的建模方案。利用此仿真方法，在 SUMO 建模的交通网络数字孪生中研究了 FCO 数据的影响。

Result: 研究结果表明，即使 FCO 的普及率仅为 20%，使用基于激光雷达检测的 FCO 也能在各种交叉口和交通需求场景中识别出 65% 的车辆。当整合时间信息并通过数据驱动方法时，可以恢复超过 80% 的先前检测到但当前未见的车辆，且位置偏差极小。

Conclusion: 研究结果强调了 FCO 在智能交通系统中的巨大潜力，特别是在不同普及率和交通条件下增强交通状态估计和监控方面的能力。

Abstract: Floating Car Observers (FCOs) extend traditional Floating Car Data (FCD) by
integrating onboard sensors to detect and localize other traffic participants,
providing richer and more detailed traffic data. In this work, we explore
various modeling approaches for FCO detections within microscopic traffic
simulations to evaluate their potential for Intelligent Transportation System
(ITS) applications. These approaches range from 2D raytracing to high-fidelity
co-simulations that emulate real-world sensors and integrate 3D object
detection algorithms to closely replicate FCO detections. Additionally, we
introduce a neural network-based emulation technique that effectively
approximates the results of high-fidelity co-simulations. This approach
captures the unique characteristics of FCO detections while offering a fast and
scalable solution for modeling. Using this emulation method, we investigate the
impact of FCO data in a digital twin of a traffic network modeled in SUMO.
Results demonstrate that even at a 20% penetration rate, FCOs using LiDAR-based
detections can identify 65% of vehicles across various intersections and
traffic demand scenarios. Further potential emerges when temporal insights are
integrated, enabling the recovery of previously detected but currently unseen
vehicles. By employing data-driven methods, we recover over 80% of these
vehicles with minimal positional deviations. These findings underscore the
potential of FCOs for ITS, particularly in enhancing traffic state estimation
and monitoring under varying penetration rates and traffic conditions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [228] [Physical foundations for trustworthy medical imaging: a review for artificial intelligence researchers](https://arxiv.org/abs/2505.02843)
*Miriam Cobo,David Corral Fontecha,Wilson Silva,Lara Lloret Iglesias*

Main category: eess.IV

TL;DR: 本文回顾了医学影像中的物理学基础及其对人工智能（AI）进展的影响，并探讨了将物理知识融入AI模型以增强其在医学影像中性能和可信度的方法。


<details>
  <summary>Details</summary>
Motivation: AI在医学影像领域发展迅速，但AI专业人员常缺乏对医学成像物理原理的理解，这限制了AI潜力的充分发挥，并影响算法的可信度和鲁棒性，尤其在数据有限时。

Method: 回顾医学影像物理学基础及其对AI最新进展（特别是生成模型和重建算法）的影响；探讨将物理知识整合到物理启发的机器学习模型中，利用物理约束增强特征学习。

Result: 研究强调了理解和整合医学影像物理学原理对于提升AI模型（如生成模型、重建算法和物理启发模型）在医学影像应用中的性能、可信度和鲁棒性的重要性。

Conclusion: 将物理学知识整合到人工智能算法中，能显著增强其在医学影像应用中的可信度和鲁棒性，特别是在数据有限的情况下，并有助于充分发挥AI的潜力。

Abstract: Artificial intelligence in medical imaging has seen unprecedented growth in
the last years, due to rapid advances in deep learning and computing resources.
Applications cover the full range of existing medical imaging modalities, with
unique characteristics driven by the physics of each technique. Yet, artificial
intelligence professionals entering the field, and even experienced developers,
often lack a comprehensive understanding of the physical principles underlying
medical image acquisition, which hinders their ability to fully leverage its
potential. The integration of physics knowledge into artificial intelligence
algorithms enhances their trustworthiness and robustness in medical imaging,
especially in scenarios with limited data availability. In this work, we review
the fundamentals of physics in medical images and their impact on the latest
advances in artificial intelligence, particularly, in generative models and
reconstruction algorithms. Finally, we explore the integration of physics
knowledge into physics-inspired machine learning models, which leverage
physics-based constraints to enhance the learning of medical imaging features.

</details>


### [229] [Dual Prompting for Diverse Count-level PET Denoising](https://arxiv.org/abs/2505.03037)
*Xiaofeng Liu,Yongsong Huang,Thibault Marin,Samira Vafay Eslahi,Tiss Amal,Yanis Chemli,Keith Johnson,Georges El Fakhri,Jinsong Ouyang*

Main category: eess.IV

TL;DR: 该研究提出了一种基于双提示学习的PET图像去噪方法，以解决不同计数水平下的去噪难题，实现了统一模型的有效去噪。


<details>
  <summary>Details</summary>
Motivation: PET图像固有的不同计数水平给统一去噪模型带来了挑战，难以有效处理各种情况。

Method: 采用双提示学习策略：一个显式的计数水平提示（提供特定先验信息）和一个隐式的通用去噪提示（编码PET去噪知识）。开发了提示融合模块来统一异构提示，并通过提示-特征交互模块将提示注入特征，动态指导去噪过程。

Result: 在1940个低计数PET 3D容积上的评估表明，所提出的双提示方法能显著提升基于计数水平信息的去噪性能，并且优于传统的计数条件模型。

Conclusion: 该研究证明了双提示学习能够有效地训练一个统一的去噪模型来适应各种计数水平的PET图像，并可通过个性化提示进行部署，从而提高去噪性能。

Abstract: The to-be-denoised positron emission tomography (PET) volumes are inherent
with diverse count levels, which imposes challenges for a unified model to
tackle varied cases. In this work, we resort to the recently flourished prompt
learning to achieve generalizable PET denoising with different count levels.
Specifically, we propose dual prompts to guide the PET denoising in a
divide-and-conquer manner, i.e., an explicitly count-level prompt to provide
the specific prior information and an implicitly general denoising prompt to
encode the essential PET denoising knowledge. Then, a novel prompt fusion
module is developed to unify the heterogeneous prompts, followed by a
prompt-feature interaction module to inject prompts into the features. The
prompts are able to dynamically guide the noise-conditioned denoising process.
Therefore, we are able to efficiently train a unified denoising model for
various count levels, and deploy it to different cases with personalized
prompts. We evaluated on 1940 low-count PET 3D volumes with uniformly randomly
selected 13-22\% fractions of events from 97 $^{18}$F-MK6240 tau PET studies.
It shows our dual prompting can largely improve the performance with informed
count-level and outperform the count-conditional model.

</details>


### [230] [STG: Spatiotemporal Graph Neural Network with Fusion and Spatiotemporal Decoupling Learning for Prognostic Prediction of Colorectal Cancer Liver Metastasis](https://arxiv.org/abs/2505.03123)
*Yiran Zhu,Wei Yang,Yan su,Zesheng Li,Chengchang Pan,Honggang Qi*

Main category: eess.IV

TL;DR: 提出了一种多模态时空图神经网络（STG）框架，用于预测结直肠癌肝转移（CRLM）的进展，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前临床模型在预测CRLM时，未能有效整合肿瘤的空间异质性、动态演变以及复杂的多模态数据关系，导致预测准确性受限。

Method: 构建了一个多模态时空图神经网络（STG）框架，将术前CT影像和临床数据整合到异构图结构中。通过空间拓扑和跨模态边联合建模肿瘤分布和时间演变。使用GraphSAGE聚合时空邻域信息，并采用监督学习和对比学习策略增强模型捕捉时间特征的能力和鲁棒性。该模型还联合优化了复发风险回归和生存分析任务。

Result: 在MSKCC CRLM数据集上的实验结果显示，时间相邻准确率达到85%，平均绝对误差为1.1005，显著优于现有方法。模型的轻量化版本参数量减少了78.55%，同时保持了接近最先进的性能。对比损失提高了特征表示的可辨别性和跨模态一致性。

Conclusion: 该创新的异构图构建和时空解耦机制有效揭示了动态肿瘤微环境变化与预后之间的关联，为个性化治疗决策提供了可靠的定量支持。

Abstract: We propose a multimodal spatiotemporal graph neural network (STG) framework
to predict colorectal cancer liver metastasis (CRLM) progression. Current
clinical models do not effectively integrate the tumor's spatial heterogeneity,
dynamic evolution, and complex multimodal data relationships, limiting their
predictive accuracy. Our STG framework combines preoperative CT imaging and
clinical data into a heterogeneous graph structure, enabling joint modeling of
tumor distribution and temporal evolution through spatial topology and
cross-modal edges. The framework uses GraphSAGE to aggregate spatiotemporal
neighborhood information and leverages supervised and contrastive learning
strategies to enhance the model's ability to capture temporal features and
improve robustness. A lightweight version of the model reduces parameter count
by 78.55%, maintaining near-state-of-the-art performance. The model jointly
optimizes recurrence risk regression and survival analysis tasks, with
contrastive loss improving feature representational discriminability and
cross-modal consistency. Experimental results on the MSKCC CRLM dataset show a
time-adjacent accuracy of 85% and a mean absolute error of 1.1005,
significantly outperforming existing methods. The innovative heterogeneous
graph construction and spatiotemporal decoupling mechanism effectively uncover
the associations between dynamic tumor microenvironment changes and prognosis,
providing reliable quantitative support for personalized treatment decisions.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [231] [Mitigating Image Captioning Hallucinations in Vision-Language Models](https://arxiv.org/abs/2505.03420)
*Fei Zhao,Chengcui Zhang,Runlin Zhang,Tianyang Wang,Xi Li*

Main category: cs.MM

TL;DR: 提出了一种新颖的测试时自适应框架，利用强化学习在推理过程中减轻视觉语言模型（VLM）的幻觉，无需重新训练或辅助VLM。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型中的幻觉现象源于预训练数据与测试样本间的分布差异，现有解决方案（如重训练、微调或集成方法）计算成本高昂、耗时或引入额外开销。

Method: 在测试时采用强化学习，仅更新语言模型层归一化中的可学习参数（约占模型总参数的0.003%），以减少测试样本与预训练样本之间的分布差异。同时，提出了一个基于CLIP的幻觉评估模型为VLM提供双重奖励。

Result: 实验结果显示，该方法在LLaVA和InstructBLIP上分别将幻觉率降低了15.4%和17.3%，并且在幻觉缓解方面比当前最先进的基线方法提高了68.3%。

Conclusion: 该方法能有效减轻VLM的幻觉问题，且无需重新训练或引入辅助VLM，展示了其有效性。

Abstract: Hallucinations in vision-language models (VLMs) hinder reliability and
real-world applicability, usually stemming from distribution shifts between
pretraining data and test samples. Existing solutions, such as retraining or
fine-tuning on additional data, demand significant computational resources and
labor-intensive data collection, while ensemble-based methods incur additional
costs by introducing auxiliary VLMs. To address these challenges, we propose a
novel test-time adaptation framework using reinforcement learning to mitigate
hallucinations during inference without retraining or any auxiliary VLMs. By
updating only the learnable parameters in the layer normalization of the
language model (approximately 0.003% of the model parameters), our method
reduces distribution shifts between test samples and pretraining samples. A
CLIP-based hallucination evaluation model is proposed to provide dual rewards
to VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in
hallucination rates on LLaVA and InstructBLIP, respectively. Our approach
outperforms state-of-the-art baselines with a 68.3% improvement in
hallucination mitigation, demonstrating its effectiveness.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [232] [Energy, Scalability, Data and Security in Massive IoT: Current Landscape and Future Directions](https://arxiv.org/abs/2505.03036)
*Imane Cheikh,Sébastien Roy,Essaid Sabir,Rachid Aouami*

Main category: cs.ET

TL;DR: 该论文综述了应对大规模物联网 (MIoT) 挑战的现有和新兴技术，旨在实现高效、可扩展、安全和可持续的 MIoT 系统。


<details>
  <summary>Details</summary>
Motivation: 大规模物联网 (MIoT) 具有变革潜力，但面临网络可扩展性、高效数据管理、能源消耗和强大安全机制等重大挑战。

Method: 本文对现有和新兴MIoT技术进行全面回顾和比较分析，包括低功耗广域网 (LPWAN)、5G/6G能力、边缘和雾计算架构、混合接入方法、AI驱动的资源分配、联邦学习、基于区块链的去中心化安全框架以及能量收集等可持续实践。

Result: 研究确定了支持高效、有弹性和可扩展MIoT部署所需的关键创新和架构调整，包括网络切片和智能资源管理对可扩展性的作用，自适应协议对实时数据处理的重要性，以及适用于MIoT设备约束的轻量级AI模型。

Conclusion: 该研究有助于更深入地理解MIoT系统如何发展以满足日益增长的无缝、可靠连接需求，同时优先考虑可持续性、安全性和性能，并为未来发展提供了路线图，强调了MIoT支持全球互联智能基础设施的潜力。

Abstract: The Massive Internet of Things (MIoT) envisions an interconnected ecosystem
of billions of devices, fundamentally transforming diverse sectors such as
healthcare, smart cities, transportation, agriculture, and energy management.
However, the vast scale of MIoT introduces significant challenges, including
network scalability, efficient data management, energy conservation, and robust
security mechanisms. This paper presents a thorough review of existing and
emerging MIoT technologies designed to address these challenges, including
Low-Power Wide-Area Networks (LPWAN), 5G/6G capabilities, edge and fog
computing architectures, and hybrid access methodologies. We further
investigate advanced strategies such as AI-driven resource allocation,
federated learning for privacy-preserving analytics, and decentralized security
frameworks using blockchain. Additionally, we analyze sustainable practices,
emphasizing energy harvesting and integrating green technologies to reduce
environmental impact. Through extensive comparative analysis, this study
identifies critical innovations and architectural adaptations required to
support efficient, resilient, and scalable MIoT deployments. Key insights
include the role of network slicing and intelligent resource management for
scalability, adaptive protocols for real-time data handling, and lightweight AI
models suited to the constraints of MIoT devices. This research ultimately
contributes to a deeper understanding of how MIoT systems can evolve to meet
the growing demand for seamless, reliable connectivity while prioritizing
sustainability, security, and performance across diverse applications. Our
findings serve as a roadmap for future advancements, underscoring the potential
of MIoT to support a globally interconnected, intelligent infrastructure.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [233] [GeoERM: Geometry-Aware Multi-Task Representation Learning on Riemannian Manifolds](https://arxiv.org/abs/2505.02972)
*Aoran Chen,Yang Feng*

Main category: stat.ML

TL;DR: 提出GeoERM，一种几何感知的多任务学习框架，在黎曼流形上优化共享表示，以提升异构或对抗性任务下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有MTL方法通常忽略潜在表示的非欧几里得几何特性，导致在处理异构或对抗性任务时鲁棒性不足。

Method: 提出GeoERM框架：将共享表示嵌入其自然黎曼流形，并通过黎曼梯度步骤和高效极坐标回缩进行优化，确保迭代中的几何保真度。

Result: 实验表明，GeoERM在任务异构和对抗性噪声条件下，能显著提高估计准确性、减少负迁移并保持稳定性，优于现有MTL和单任务方法。

Conclusion: GeoERM通过利用表示的内在几何结构，为多任务学习提供了一种更鲁棒且计算成本与基线方法相当的解决方案，尤其适用于异构和对抗环境。

Abstract: Multi-Task Learning (MTL) seeks to boost statistical power and learning
efficiency by discovering structure shared across related tasks.
State-of-the-art MTL representation methods, however, usually treat the latent
representation matrix as a point in ordinary Euclidean space, ignoring its
often non-Euclidean geometry, thus sacrificing robustness when tasks are
heterogeneous or even adversarial. We propose GeoERM, a geometry-aware MTL
framework that embeds the shared representation on its natural Riemannian
manifold and optimizes it via explicit manifold operations. Each training cycle
performs (i) a Riemannian gradient step that respects the intrinsic curvature
of the search space, followed by (ii) an efficient polar retraction to remain
on the manifold, guaranteeing geometric fidelity at every iteration. The
procedure applies to a broad class of matrix-factorized MTL models and retains
the same per-iteration cost as Euclidean baselines. Across a set of synthetic
experiments with task heterogeneity and on a wearable-sensor
activity-recognition benchmark, GeoERM consistently improves estimation
accuracy, reduces negative transfer, and remains stable under adversarial label
noise, outperforming leading MTL and single-task alternatives.

</details>


### [234] [Modeling Spatial Extremes using Non-Gaussian Spatial Autoregressive Models via Convolutional Neural Networks](https://arxiv.org/abs/2505.03034)
*Sweta Rai,Douglas W. Nychka,Soutir Bandyopadhyay*

Main category: stat.ML

TL;DR: 本文提出了一种结合广义极值分布（GEV）新息的空间自回归模型，用于分析具有空间异质性和重尾分布的网格化数据，并采用卷积神经网络（CNN）进行快速参数估计。


<details>
  <summary>Details</summary>
Motivation: 遥感或数值模拟产生的大规模规则网格数据，在存在空间异质性和重尾边缘分布时，难以找到准确的空间模型来填充缺失网格或有效模拟过程。

Method: 提出一种空间自回归建模框架，将某位置及其邻域的观测值映射到独立随机变量。具体采用带有广义极值分布（GEV）新息的SAR模型捕捉极端空间行为。由于最大似然估计难以处理，研究者训练了一个卷积神经网络（CNN）来进行快速参数估计。

Result: 该模型能够快速模拟，并且通过训练的卷积神经网络实现了参数的快速估计。模型成功应用于分析ERA-Interim驱动的WRF模拟产生的北美年度最大降水量数据，探索了其空间极端行为。

Conclusion: 所提出的空间自回归模型框架（结合GEV新息和CNN参数估计）为分析具有空间异质性和重尾分布（特别是极端值）的大规模网格数据提供了一种灵活、可解释且计算上可行的方法。

Abstract: Data derived from remote sensing or numerical simulations often have a
regular gridded structure and are large in volume, making it challenging to
find accurate spatial models that can fill in missing grid cells or simulate
the process effectively, especially in the presence of spatial heterogeneity
and heavy-tailed marginal distributions. To overcome this issue, we present a
spatial autoregressive modeling framework, which maps observations at a
location and its neighbors to independent random variables. This is a highly
flexible modeling approach and well-suited for non-Gaussian fields, providing
simpler interpretability. In particular, we consider the SAR model with
Generalized Extreme Value distribution innovations to combine the observation
at a central grid location with its neighbors, capturing extreme spatial
behavior based on the heavy-tailed innovations. While these models are fast to
simulate by exploiting the sparsity of the key matrices in the computations,
the maximum likelihood estimation of the parameters is prohibitive due to the
intractability of the likelihood, making optimization challenging. To overcome
this, we train a convolutional neural network on a large training set that
covers a useful parameter space, and then use the trained network for fast
parameter estimation. Finally, we apply this model to analyze annual maximum
precipitation data from ERA-Interim-driven Weather Research and Forecasting
(WRF) simulations, allowing us to explore its spatial extreme behavior across
North America.

</details>


### [235] [A Symbolic and Statistical Learning Framework to Discover Bioprocessing Regulatory Mechanism: Cell Culture Example](https://arxiv.org/abs/2505.03177)
*Keilung Choy,Wei Xie,Keqi Wang*

Main category: stat.ML

TL;DR: 本文提出了一种符号与统计学习框架，用于识别生物过程中的关键调控机制并量化模型不确定性，尤其在数据有限的情况下，能有效改进机理建模。


<details>
  <summary>Details</summary>
Motivation: 生物过程机理建模对于发展生物制造的智能数字孪生至关重要，但面临细胞内调控复杂、系统行为随机以及实验数据有限等挑战。

Method: 该研究引入了一个符号与统计学习框架。首先，使用随机微分方程来描述生物过程动力学及其内在变异性，并基于生物学知识预定义一组候选调控机制。然后，开发了一种贝叶斯学习方法，通过混合模型公式实现动力学参数和调控结构的联合学习。为提高计算效率，采用了基于伴随灵敏度分析的Metropolis调整的朗之万算法进行后验探索。

Result: 与现有先进的贝叶斯推断方法相比，该框架在样本效率和模型选择的稳健性方面均有提升。实证研究表明，该框架能够在数据有限的条件下恢复缺失的调控机制，并提高模型的保真度。

Conclusion: 所提出的符号与统计学习框架能够有效识别关键调控机制并量化模型不确定性，尤其在数据受限时能改善生物过程模型的保真度和效率，为生物制造的智能数字孪生发展提供了有力工具。

Abstract: Bioprocess mechanistic modeling is essential for advancing intelligent
digital twin representation of biomanufacturing, yet challenges persist due to
complex intracellular regulation, stochastic system behavior, and limited
experimental data. This paper introduces a symbolic and statistical learning
framework to identify key regulatory mechanisms and quantify model uncertainty.
Bioprocess dynamics is formulated with stochastic differential equations
characterizing intrinsic process variability, with a predefined set of
candidate regulatory mechanisms constructed from biological knowledge. A
Bayesian learning approach is developed, which is based on a joint learning of
kinetic parameters and regulatory structure through a formulation of the
mixture model. To enhance computational efficiency, a Metropolis-adjusted
Langevin algorithm with adjoint sensitivity analysis is developed for posterior
exploration. Compared to state-of-the-art Bayesian inference approaches, the
proposed framework achieves improved sample efficiency and robust model
selection. An empirical study demonstrates its ability to recover missing
regulatory mechanisms and improve model fidelity under data-limited conditions.

</details>


### [236] [Weighted Average Gradients for Feature Attribution](https://arxiv.org/abs/2505.03201)
*Kien Tran Duc Tuan,Tam Nguyen Trong,Son Nguyen Hoang,Khoat Than,Anh Nguyen Duc*

Main category: stat.ML

TL;DR: 提出了一种名为加权平均梯度（WG）的新方法，通过无监督地评估和选择基线来改进可解释AI中的梯度积分解释，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在可解释AI中，梯度积分（IG）方法对基线的选择非常敏感，而传统期望梯度（EG）方法平等对待所有基线，这可能导致解释效果不佳。本研究认为基线不应被等同对待。

Method: 本文提出了一种新的加权平均梯度（WG）方法。该方法能够无监督地评估基线的适用性，并结合策略来选择有效的基线进行加权平均，以生成更鲁棒的特征归因解释。

Result: 理论分析表明WG满足可解释性方法的关键标准且比先前方法更稳定。实验结果显示，WG在不同场景下的主要评估指标上比EG提升了10-35%，并且通过筛选有效基线，能够在保持高准确性的同时降低计算成本。

Conclusion: WG方法通过更有效地选择和加权基线，为梯度积分解释提供了一种更准确、稳定且计算高效的改进方案，优于现有基线处理方式。

Abstract: In explainable AI, Integrated Gradients (IG) is a widely adopted technique
for assessing the significance of feature attributes of the input on model
outputs by evaluating contributions from a baseline input to the current input.
The choice of the baseline input significantly influences the resulting
explanation. While the traditional Expected Gradients (EG) method assumes
baselines can be uniformly sampled and averaged with equal weights, this study
argues that baselines should not be treated equivalently. We introduce Weighted
Average Gradients (WG), a novel approach that unsupervisedly evaluates baseline
suitability and incorporates a strategy for selecting effective baselines.
Theoretical analysis demonstrates that WG satisfies essential explanation
method criteria and offers greater stability than prior approaches.
Experimental results further confirm that WG outperforms EG across diverse
scenarios, achieving an improvement of 10-35\% on main metrics. Moreover, by
evaluating baselines, our method can filter a subset of effective baselines for
each input to calculate explanations, maintaining high accuracy while reducing
computational cost. The code is available at:
https://github.com/Tamnt240904/weighted_baseline.

</details>


### [237] [Lower Bounds for Greedy Teaching Set Constructions](https://arxiv.org/abs/2505.03223)
*Spencer Compton,Chirag Pabbaraju,Nikita Zhivotovskiy*

Main category: stat.ML

TL;DR: 该研究分析了一种递归贪心算法在构建教学集时的性能下界，特别关注每次迭代选择少量（k）标记点的情况，并探讨了其对确定概念类别最小教学维度的影响。


<details>
  <summary>Details</summary>
Motivation: 核心动机是解决学习理论中关于概念类别最小教学维度 TS_min 与其VC维度 d 之间关系的开放问题，特别是 [Simon and Zilles; COLT 2015] 提出的 TS_min = O(d) 的猜想。先前工作通过贪心算法得到了 TS_min 的 O(d^2) 上界，本研究旨在分析该贪心算法在选择少量样本（小 k）时的性能下限，以理解其局限性。

Method: 主要通过理论分析，针对先前工作中使用的递归贪心算法（在每次迭代中选择 k 个能最大程度约束概念类的标记点）推导其性能下界。

Result: 证明了当 k=1 时，该贪心算法的性能不优于基于减半的 O(log(|C|)) 界限。对于 k=2，证明了与 [Moran et al.; FOCS 2015] 的 O(log(log(|C|))) 上界相匹配的下界。此外，该下界可扩展至 k ≤ ⌈cd⌉（c 为小常数）。

Conclusion: 研究结果表明，对于较小的 k（甚至到 k ≤ ⌈cd⌉），当前分析的贪心算法在改进教学维度界限方面存在局限性。这暗示为了解决 TS_min = O(d) 的猜想，可能需要研究更高阶的样本相互作用或设计新的教学策略。

Abstract: A fundamental open problem in learning theory is to characterize the
best-case teaching dimension $\operatorname{TS}_{\min}$ of a concept class
$\mathcal{C}$ with finite VC dimension $d$. Resolving this problem will, in
particular, settle the conjectured upper bound on Recursive Teaching Dimension
posed by [Simon and Zilles; COLT 2015]. Prior work used a natural greedy
algorithm to construct teaching sets recursively, thereby proving upper bounds
on $\operatorname{TS}_{\min}$, with the best known bound being $O(d^2)$ [Hu,
Wu, Li, and Wang; COLT 2017]. In each iteration, this greedy algorithm chooses
to add to the teaching set the $k$ labeled points that restrict the concept
class the most. In this work, we prove lower bounds on the performance of this
greedy approach for small $k$. Specifically, we show that for $k = 1$, the
algorithm does not improve upon the halving-based bound of
$O(\log(|\mathcal{C}|))$. Furthermore, for $k = 2$, we complement the upper
bound of $O\left(\log(\log(|\mathcal{C}|))\right)$ from [Moran, Shpilka,
Wigderson, and Yuhudayoff; FOCS 2015] with a matching lower bound. Most
consequentially, our lower bound extends up to $k \le \lceil c d \rceil$ for
small constant $c>0$: suggesting that studying higher-order interactions may be
necessary to resolve the conjecture that $\operatorname{TS}_{\min} = O(d)$.

</details>


### [238] [Decision Making under Model Misspecification: DRO with Robust Bayesian Ambiguity Sets](https://arxiv.org/abs/2505.03585)
*Charita Dellaporta,Patrick O'Hara,Theodoros Damoulas*

Main category: stat.ML

TL;DR: 提出一种新的分布鲁棒优化方法 (DRO-RoBAS)，通过构建对模型错配鲁棒的贝叶斯模糊集，以解决传统方法在模型不确定时决策过于保守的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的基于模型的分布鲁棒优化 (DRO) 在模型被错误指定时，为了覆盖真实的数据生成过程 (DGP)，需要扩大模糊集，这会导致决策过于保守。

Method: 引入 DRO-RoBAS 方法，该方法使用以“对模型错配鲁棒的后验预测分布”（该分布整合了对真实数据生成过程的信念）为中心的“最大均值差异 (MMD) 模糊集”。

Result: 该优化问题获得了在再生核希尔伯特空间 (RKHS) 中的对偶形式，并给出了模糊集容忍水平的概率保证。在新闻商问题和投资组合问题的各种模型错配情况下，DRO-RoBAS 在样本外性能方面优于其他贝叶斯和经验 DRO 方法。

Conclusion: DRO-RoBAS 能够有效处理模型错配问题，通过构建更合理的模糊集来改进决策，避免了传统方法在模型不准确时的过度保守性，并在实验中展现了更优的样本外性能。

Abstract: Distributionally Robust Optimisation (DRO) protects risk-averse
decision-makers by considering the worst-case risk within an ambiguity set of
distributions based on the empirical distribution or a model. To further guard
against finite, noisy data, model-based approaches admit Bayesian formulations
that propagate uncertainty from the posterior to the decision-making problem.
However, when the model is misspecified, the decision maker must stretch the
ambiguity set to contain the data-generating process (DGP), leading to overly
conservative decisions. We address this challenge by introducing DRO with
Robust, to model misspecification, Bayesian Ambiguity Sets (DRO-RoBAS). These
are Maximum Mean Discrepancy ambiguity sets centred at a robust posterior
predictive distribution that incorporates beliefs about the DGP. We show that
the resulting optimisation problem obtains a dual formulation in the
Reproducing Kernel Hilbert Space and we give probabilistic guarantees on the
tolerance level of the ambiguity set. Our method outperforms other Bayesian and
empirical DRO approaches in out-of-sample performance on the Newsvendor and
Portfolio problems with various cases of model misspecification.

</details>


### [239] [Physics-Informed Sylvester Normalizing Flows for Bayesian Inference in Magnetic Resonance Spectroscopy](https://arxiv.org/abs/2505.03590)
*Julian P. Merkofer,Dennis M. J. van de Sande,Alex A. Bhogal,Ruud J. G. van Sloun*

Main category: stat.ML

TL;DR: 本文提出一种基于贝叶斯推断和西尔维斯特归一化流 (SNFs) 的新方法，以提高磁共振波谱 (MRS) 代谢物定量的可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统MRS定量方法因谱线重叠、低信噪比、伪影和模糊性等问题导致准确性受限，且通常仅提供理论上的克拉默-拉奥界作为精度下限。

Method: 采用基于西尔维斯特归一化流 (SNFs) 的贝叶斯推断框架来近似代谢物浓度的后验分布，并结合一个基于物理的解码器以整合MRS信号形成的先验知识。

Result: 在模拟的7T质子MRS数据上的验证表明，该方法能够实现准确的代谢物定量，提供校准良好的不确定性估计，并揭示参数相关性和多模态分布。

Conclusion: 所提出的贝叶斯SNF方法增强了MRS定量的可靠性，并能提供关于代谢物浓度更丰富的信息，包括不确定性和参数间的关系。

Abstract: Magnetic resonance spectroscopy (MRS) is a non-invasive technique to measure
the metabolic composition of tissues, offering valuable insights into
neurological disorders, tumor detection, and other metabolic dysfunctions.
However, accurate metabolite quantification is hindered by challenges such as
spectral overlap, low signal-to-noise ratio, and various artifacts. Traditional
methods like linear-combination modeling are susceptible to ambiguities and
commonly only provide a theoretical lower bound on estimation accuracy in the
form of the Cram\'er-Rao bound. This work introduces a Bayesian inference
framework using Sylvester normalizing flows (SNFs) to approximate posterior
distributions over metabolite concentrations, enhancing quantification
reliability. A physics-based decoder incorporates prior knowledge of MRS signal
formation, ensuring realistic distribution representations. We validate the
method on simulated 7T proton MRS data, demonstrating accurate metabolite
quantification, well-calibrated uncertainties, and insights into parameter
correlations and multi-modal distributions.

</details>


### [240] [Weighted Random Dot Product Graphs](https://arxiv.org/abs/2505.03649)
*Bernardo Marenco,Paola Bermolen,Marcelo Fiori,Federico Larroca,Gonzalo Mateos*

Main category: stat.ML

TL;DR: 本文提出了一种加权随机点积图 (WRDPG) 模型，通过节点潜在位置和矩母函数来处理边权重异构的加权网络，并提供了估计器的统计保证和图生成框架。


<details>
  <summary>Details</summary>
Motivation: 现有随机点积图 (RDPG) 模型主要适用于无权图或边权重分布单一的图，难以处理边权重具有异构分布的复杂加权网络。此外，现有方法可能无法区分仅均值相同但高阶矩不同的权重分布。

Method: 提出了一种非参数加权随机点积图 (WRDPG) 模型。该模型为每个节点分配潜在位置序列，通过这些节点向量的内积和矩母函数来确定其关联边权重的分布矩。节点潜在位置的估计器改编自邻接谱嵌入，并建立了一个图生成框架。

Result: 所提出的 WRDPG 模型能够区分均值相同但高阶矩不同的权重分布。推导了节点潜在位置估计器的一致性和渐近正态性等统计保证。开发了一个能够根据 WRDPG 模型（无论是预设的还是数据拟合的）采样生成图的框架，有助于分析和测试观察到的图度量。

Conclusion: WRDPG 模型有效地扩展了 RDPG 的适用范围至具有异构边权重的加权图，为分析此类网络提供了强大的新工具，其理论保证和生成能力增强了模型的实用性和可靠性。

Abstract: Modeling of intricate relational patterns % through the analysis structures
of network data has become a cornerstone of contemporary statistical research
and related data science fields. Networks, represented as graphs, offer a
natural framework for this analysis. This paper extends the Random Dot Product
Graph (RDPG) model to accommodate weighted graphs, markedly broadening the
model's scope to scenarios where edges exhibit heterogeneous weight
distributions. We propose a nonparametric weighted (W)RDPG model that assigns a
sequence of latent positions to each node. Inner products of these nodal
vectors specify the moments of their incident edge weights' distribution via
moment-generating functions. In this way, and unlike prior art, the WRDPG can
discriminate between weight distributions that share the same mean but differ
in other higher-order moments. We derive statistical guarantees for an
estimator of the nodal's latent positions adapted from the workhorse adjacency
spectral embedding, establishing its consistency and asymptotic normality. We
also contribute a generative framework that enables sampling of graphs that
adhere to a (prescribed or data-fitted) WRDPG, facilitating, e.g., the analysis
and testing of observed graph metrics using judicious reference distributions.
The paper is organized to formalize the model's definition, the estimation (or
nodal embedding) process and its guarantees, as well as the methodologies for
generating weighted graphs, all complemented by illustrative and reproducible
examples showcasing the WRDPG's effectiveness in various network analytic
applications.

</details>


### [241] [Multi-modal cascade feature transfer for polymer property prediction](https://arxiv.org/abs/2505.03704)
*Kiichi Obuchi,Yuta Yahagi,Kiyohiko Toyama,Shukichi Tanaka,Kota Matsui*

Main category: stat.ML

TL;DR: 提出了一种用于聚合物性质预测的多模态级联特征迁移模型，结合了GCN提取的化学结构特征与其他特征。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常分别使用聚合物的多种数据类型（如分子描述符、添加剂信息、化学结构）构建预测模型，未能有效利用数据的多模态特性，从而限制了预测精度。

Method: 提出了一种多模态级联模型，利用特征迁移技术，通过图卷积神经网络（GCN）从化学结构中提取特征，并将其与分子描述符和添加剂信息等其他特征相结合。

Result: 实验评估表明，所提出的方法在多个聚合物数据集上，与使用单一特征的传统基线方法相比，表现出更高的预测性能。

Conclusion: 所提出的多模态级联模型通过有效结合化学结构特征（GCN提取）与分子描述符、添加剂信息，能够显著提高聚合物物理性质的预测准确性，优于传统单特征方法。

Abstract: In this paper, we propose a novel transfer learning approach called
multi-modal cascade model with feature transfer for polymer property
prediction.Polymers are characterized by a composite of data in several
different formats, including molecular descriptors and additive information as
well as chemical structures. However, in conventional approaches, prediction
models were often constructed using each type of data separately. Our model
enables more accurate prediction of physical properties for polymers by
combining features extracted from the chemical structure by graph convolutional
neural networks (GCN) with features such as molecular descriptors and additive
information. The predictive performance of the proposed method is empirically
evaluated using several polymer datasets. We report that the proposed method
shows high predictive performance compared to the baseline conventional
approach using a single feature.

</details>


### [242] [Actor-Critics Can Achieve Optimal Sample Efficiency](https://arxiv.org/abs/2505.03710)
*Kevin Tan,Wei Fan,Yuting Wei*

Main category: stat.ML

TL;DR: 本文提出了一种新的演员-评论家（actor-critic）算法，该算法在需要策略性探索时，使用通用函数逼近实现了 $O(1/\epsilon^2)$ 的样本复杂度，并将其扩展到混合强化学习（Hybrid RL）领域，同时还提出了一种利用离线数据的非乐观高效算法。


<details>
  <summary>Details</summary>
Motivation: 现有的演员-评论家算法在需要策略性探索和使用通用函数逼近的情况下，未能学习到 $\epsilon$-最优策略并达到 $O(1/\epsilon^2)$ 的样本复杂度。此外，如何设计一个利用离线数据且无需乐观主义的可证明高效的演员-评论家算法也是一个悬而未决的问题。

Method: 本文提出了一种新颖的演员-评论家算法，该算法集成了乐观主义、针对最优Q函数的离策略评论家估计以及罕见切换的策略重置。接着，将此算法扩展到混合强化学习设置。最后，利用离线数据，提出了一种无需乐观主义的可证明高效的演员-评论家算法。

Result: 提出的算法达到了 $O(dH^5 \log|\mathcal{A}|/\epsilon^2 + d H^4 \log|\mathcal{F}|/ \epsilon^2)$ 的轨迹样本复杂度，并伴随 $\sqrt{T}$ 的遗憾值。在混合强化学习中，使用离线数据初始化评论家显示出比纯粹离线或在线强化学习更高的样本效率。所提出的非乐观算法在拥有足够离线数据 ($N_{\text{off}} \geq c_{\text{off}}^*dH^4/\epsilon^2$) 的条件下，无需乐观主义即可实现可证明的效率。数值实验支持了这些理论发现。

Conclusion: 本文成功解决了演员-评论家算法中关于样本复杂度的开放性问题，并针对利用离线数据设计非乐观主义的高效算法提出了解决方案，进一步的数值实验也验证了理论成果。

Abstract: Actor-critic algorithms have become a cornerstone in reinforcement learning
(RL), leveraging the strengths of both policy-based and value-based methods.
Despite recent progress in understanding their statistical efficiency, no
existing work has successfully learned an $\epsilon$-optimal policy with a
sample complexity of $O(1/\epsilon^2)$ trajectories with general function
approximation when strategic exploration is necessary.
  We address this open problem by introducing a novel actor-critic algorithm
that attains a sample-complexity of $O(dH^5 \log|\mathcal{A}|/\epsilon^2 + d
H^4 \log|\mathcal{F}|/ \epsilon^2)$ trajectories, and accompanying $\sqrt{T}$
regret when the Bellman eluder dimension $d$ does not increase with $T$ at more
than a $\log T$ rate.
  Here, $\mathcal{F}$ is the critic function class, $\mathcal{A}$ is the action
space, and $H$ is the horizon in the finite horizon MDP setting. Our algorithm
integrates optimism, off-policy critic estimation targeting the optimal
Q-function, and rare-switching policy resets.
  We extend this to the setting of Hybrid RL, showing that initializing the
critic with offline data yields sample efficiency gains compared to purely
offline or online RL. Further, utilizing access to offline data, we provide a
\textit{non-optimistic} provably efficient actor-critic algorithm that only
additionally requires $N_{\text{off}} \geq c_{\text{off}}^*dH^4/\epsilon^2$ in
exchange for omitting optimism, where $c_{\text{off}}^*$ is the single-policy
concentrability coefficient and $N_{\text{off}}$ is the number of offline
samples. This addresses another open problem in the literature. We further
provide numerical experiments to support our theoretical findings.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [243] [MORE: Mobile Manipulation Rearrangement Through Grounded Language Reasoning](https://arxiv.org/abs/2505.03035)
*Mohammad Mohammadi,Daniel Honerkamp,Martin Büchner,Matteo Cassinelli,Tim Welschehold,Fabien Despinoy,Igor Gilitschenski,Abhinav Valada*

Main category: cs.RO

TL;DR: 该研究提出了一种名为MORE的新方法，利用场景图增强语言模型，以解决大规模环境下的零样本移动操作规划（特别是重排任务）问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于基础模型的方法在处理大量对象和大规模环境时性能会下降。本研究旨在解决自主长时程移动操作中的场景动态性、未探索区域和错误恢复等挑战，并提升语言模型在复杂重排任务中的零样本规划能力。

Method: MORE方法利用场景图来表示环境，整合了实例区分，并引入了一种主动过滤方案，用以提取与任务相关的对象和区域实例子图。这些步骤将规划问题限定在有界范围内，从而有效减轻模型的“幻觉”并提高可靠性。此外，还引入了多项增强功能，支持室内外环境的规划。

Result: 在BEHAVIOR-1K基准测试的81个不同重排任务中，MORE是首个成功解决其中大部分任务的方法，其性能超越了近期基于基础模型的方法。此外，该方法还在多个模拟日常活动的复杂真实世界任务中展示了其能力。

Conclusion: MORE通过利用场景图和主动过滤机制，有效解决了现有方法在大型环境中进行长时程移动操作规划的局限性，提高了规划的可靠性，并在基准测试和真实世界任务中均表现出色。

Abstract: Autonomous long-horizon mobile manipulation encompasses a multitude of
challenges, including scene dynamics, unexplored areas, and error recovery.
Recent works have leveraged foundation models for scene-level robotic reasoning
and planning. However, the performance of these methods degrades when dealing
with a large number of objects and large-scale environments. To address these
limitations, we propose MORE, a novel approach for enhancing the capabilities
of language models to solve zero-shot mobile manipulation planning for
rearrangement tasks. MORE leverages scene graphs to represent environments,
incorporates instance differentiation, and introduces an active filtering
scheme that extracts task-relevant subgraphs of object and region instances.
These steps yield a bounded planning problem, effectively mitigating
hallucinations and improving reliability. Additionally, we introduce several
enhancements that enable planning across both indoor and outdoor environments.
We evaluate MORE on 81 diverse rearrangement tasks from the BEHAVIOR-1K
benchmark, where it becomes the first approach to successfully solve a
significant share of the benchmark, outperforming recent foundation model-based
approaches. Furthermore, we demonstrate the capabilities of our approach in
several complex real-world tasks, mimicking everyday activities. We make the
code publicly available at https://more-model.cs.uni-freiburg.de.

</details>


### [244] [Latent Adaptive Planner for Dynamic Manipulation](https://arxiv.org/abs/2505.03077)
*Donghun Noh,Deqian Kong,Minglu Zhao,Andrew Lizarraga,Jianwen Xie,Ying Nian Wu,Dennis Hong*

Main category: cs.RO

TL;DR: 本文提出了一种名为潜在自适应规划器（LAP）的新方法，用于动态非抓取操作任务，该方法将规划表述为潜在空间推理，并从人类演示视频中学习。


<details>
  <summary>Details</summary>
Motivation: 解决视觉运动策略学习中的关键挑战，特别是在动态环境中保持时间一致性并高效适应变化，同时弥合人类与机器人之间的体态差异。

Method: LAP 将规划问题转化为潜在空间中的推理，从人类演示视频中学习。它采用变分重规划框架，通过潜在空间中的贝叶斯更新来逐步优化计划，并使用基于模型的比例映射从人类演示中重建运动学-动力学状态。

Result: 在多个复杂操作基准测试中，LAP 取得了领先的性能，在成功率、轨迹平滑度和能量效率方面均优于现有方法，尤其在动态适应场景中表现突出。

Conclusion: LAP 使机器人能够以类似人类的适应性执行复杂交互，并提供了一个可扩展的框架，适用于使用相同人类演示视频的不同机器人平台，实现了计算效率和实时适应性的平衡。

Abstract: This paper presents Latent Adaptive Planner (LAP), a novel approach for
dynamic nonprehensile manipulation tasks that formulates planning as latent
space inference, effectively learned from human demonstration videos. Our
method addresses key challenges in visuomotor policy learning through a
principled variational replanning framework that maintains temporal consistency
while efficiently adapting to environmental changes. LAP employs Bayesian
updating in latent space to incrementally refine plans as new observations
become available, striking an optimal balance between computational efficiency
and real-time adaptability. We bridge the embodiment gap between humans and
robots through model-based proportional mapping that regenerates accurate
kinematic-dynamic joint states and object positions from human demonstrations.
Experimental evaluations across multiple complex manipulation benchmarks
demonstrate that LAP achieves state-of-the-art performance, outperforming
existing approaches in success rate, trajectory smoothness, and energy
efficiency, particularly in dynamic adaptation scenarios. Our approach enables
robots to perform complex interactions with human-like adaptability while
providing an expandable framework applicable to diverse robotic platforms using
the same human demonstration videos.

</details>


### [245] [Learn to Swim: Data-Driven LSTM Hydrodynamic Model for Quadruped Robot Gait Optimization](https://arxiv.org/abs/2505.03146)
*Fei Han,Pengming Guo,Hao Chen,Weikun Li,Jingbo Ren,Naijun Liu,Ning Yang,Dixia Fan*

Main category: cs.RO

TL;DR: 本文提出一种基于LSTM的FED-LSTM模型，用于预测水下四足机器人的流体动力，该模型优于传统经验公式，并通过步态优化提升了机器人的游泳性能。


<details>
  <summary>Details</summary>
Motivation: 传统经验公式难以准确预测水下四足机器人在复杂运动中（如直线和转弯）受到的非定常、非线性流体动力，这限制了其游泳性能的优化。

Method: 提出一种基于长短期记忆网络（LSTM）的流体实验数据驱动模型（FED-LSTM）。该模型使用在循环水槽和拖曳水池中进行的腿部受力和身体阻力实验数据进行训练，并结合NSGA-II算法对直游和转弯步态进行优化。

Result: FED-LSTM模型在预测流体动力方面显著优于传统经验公式（EF），展现出更高的准确性和对复杂流体动力学的捕捉能力。通过该模型优化步态，机器人直线游泳时的偏转误差减小，转弯时间缩短且转弯半径未增加。硬件实验进一步验证了模型的精度和稳定性。

Conclusion: FED-LSTM模型为提升腿式机器人的水下游泳性能提供了一个鲁棒的框架，为未来水下机器人运动控制的发展奠定了基础。

Abstract: This paper presents a Long Short-Term Memory network-based Fluid Experiment
Data-Driven model (FED-LSTM) for predicting unsteady, nonlinear hydrodynamic
forces on the underwater quadruped robot we constructed. Trained on
experimental data from leg force and body drag tests conducted in both a
recirculating water tank and a towing tank, FED-LSTM outperforms traditional
Empirical Formulas (EF) commonly used for flow prediction over flat surfaces.
The model demonstrates superior accuracy and adaptability in capturing complex
fluid dynamics, particularly in straight-line and turning-gait optimizations
via the NSGA-II algorithm. FED-LSTM reduces deflection errors during
straight-line swimming and improves turn times without increasing the turning
radius. Hardware experiments further validate the model's precision and
stability over EF. This approach provides a robust framework for enhancing the
swimming performance of legged robots, laying the groundwork for future
advances in underwater robotic locomotion.

</details>


### [246] [Systematic Evaluation of Initial States and Exploration-Exploitation Strategies in PID Auto-Tuning: A Framework-Driven Approach Applied on Mobile Robots](https://arxiv.org/abs/2505.03159)
*Zaid Ghazal,Ali Al-Bustami,Khouloud Gaaloul,Jaerock Kwon*

Main category: cs.RO

TL;DR: 本文提出了一种新框架，用于评估系统性改变初始状态和探索/利用平衡对基于贝叶斯优化和差分进化的PID自动调谐过程的影响，并在两种机器人平台上进行了测试。


<details>
  <summary>Details</summary>
Motivation: 尽管贝叶斯优化和差分进化等先进技术已应用于PID控制器自动调谐，但初始系统状态对收敛性的影响以及探索与利用之间的平衡仍未得到充分研究，且在真实物理系统（如移动机器人）上直接实验至关重要。

Method: 引入一个新框架，系统地改变影响PID自动调谐过程的因素（初始状态、探索/利用平衡），并使用贝叶斯优化和差分进化在两个不同的PID控制机器人平台（全向机器人和差速驱动移动机器人）上进行测试，评估对收敛速度、稳定时间、上升时间和超调百分比的影响。

Result: 实验结果证明了系统性改变这些因素对PID自动调谐过程的影响，为该领域的未来研究提供了经验基础。

Conclusion: 该研究通过在真实机器人平台上进行实验，为理解初始状态和优化策略对PID自动调谐性能的影响提供了实证依据，有助于改进未来的自动调谐方法。

Abstract: PID controllers are widely used in control systems because of their
simplicity and effectiveness. Although advanced optimization techniques such as
Bayesian Optimization and Differential Evolution have been applied to address
the challenges of automatic tuning of PID controllers, the influence of initial
system states on convergence and the balance between exploration and
exploitation remains underexplored. Moreover, experimenting the influence
directly on real cyber-physical systems such as mobile robots is crucial for
deriving realistic insights. In the present paper, a novel framework is
introduced to evaluate the impact of systematically varying these factors on
the PID auto-tuning processes that utilize Bayesian Optimization and
Differential Evolution. Testing was conducted on two distinct PID-controlled
robotic platforms, an omnidirectional robot and a differential drive mobile
robot, to assess the effects on convergence rate, settling time, rise time, and
overshoot percentage. As a result, the experimental outcomes yield evidence on
the effects of the systematic variations, thereby providing an empirical basis
for future research studies in the field.

</details>


### [247] [Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets](https://arxiv.org/abs/2505.03174)
*Guillermo Roque,Erika Maquiling,Jose Giovanni Tapia Lopez,Ross Greer*

Main category: cs.RO

TL;DR: 本文探索了利用手机GPS和自然语言处理（NLP）自动生成指令-行动（IA）数据的方法，以解决人工标注成本高、效率低的问题，用于训练自动驾驶系统。


<details>
  <summary>Details</summary>
Motivation: 人工标注用于训练机器人系统（特别是自动驾驶汽车）的指令-行动（IA）数据对成本高昂且耗时。

Method: 通过在驾驶过程中收集手机GPS应用的语音指令，并结合视频数据，利用自然语言处理（NLP）技术自动生成和分类IA数据。开发了名为ADVLAT-Engine的全自动数据收集原型系统。

Result: 成功演示了一种自动收集和分类多样化GPS语音指令的方法，将这些指令分为八个不同类别，并结合视频形成了视觉-语言-行动三元组数据。证明了可从免费手机应用中获取大量指令数据。

Conclusion: 利用GPS参考自动化生成IA数据对，有潜力大幅提高高质量IA数据集的创建速度和体量，同时降低成本，为开发用于视觉-语言导航和人机交互自主系统的稳健视觉-语言-行动（VLA）模型铺平道路。

Abstract: Instruction-Action (IA) data pairs are valuable for training robotic systems,
especially autonomous vehicles (AVs), but having humans manually annotate this
data is costly and time-inefficient. This paper explores the potential of using
mobile application Global Positioning System (GPS) references and Natural
Language Processing (NLP) to automatically generate large volumes of IA
commands and responses without having a human generate or retroactively tag the
data. In our pilot data collection, by driving to various destinations and
collecting voice instructions from GPS applications, we demonstrate a means to
collect and categorize the diverse sets of instructions, further accompanied by
video data to form complete vision-language-action triads. We provide details
on our completely automated data collection prototype system, ADVLAT-Engine. We
characterize collected GPS voice instructions into eight different
classifications, highlighting the breadth of commands and referentialities
available for curation from freely available mobile applications. Through
research and exploration into the automation of IA data pairs using GPS
references, the potential to increase the speed and volume at which
high-quality IA datasets are created, while minimizing cost, can pave the way
for robust vision-language-action (VLA) models to serve tasks in
vision-language navigation (VLN) and human-interactive autonomous systems.

</details>


### [248] [The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning](https://arxiv.org/abs/2505.03296)
*Jan Ole von Hartz,Adrian Röfer,Joschka Boedecker,Abhinav Valada*

Main category: cs.RO

TL;DR: MiDiGap是一种新颖的机器人操作模仿学习方法，利用离散时间高斯过程混合模型，仅需少量（如5次）演示和相机观测即可学习多种复杂任务，并具有强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人模仿学习方法在小样本学习、处理任务多样性（如长时序、高约束、动态、多模态任务）以及泛化能力方面存在挑战，需要更高效灵活的策略表示和学习方法。

Method: 提出了MiDiGap（离散时间高斯过程混合模型）用于策略表示和模仿学习。该方法仅依赖相机观测数据，并开发了推理时引导工具，利用碰撞信号和机器人运动学约束来增强策略的适应性和泛化性。

Result: MiDiGap在多种小样本操作基准测试中达到业界领先水平。具体而言，在约束性RLBench任务中，策略成功率提高了76%，轨迹成本降低了67%；在多模态任务中，策略成功率提高了48%，样本效率提升了20倍；在跨机器人实体迁移任务中，策略成功率提升超过一倍。该方法学习速度快（CPU上一分钟内完成），并能线性扩展至大型数据集，还实现了障碍物躲避和跨机器人实体策略迁移等新颖的泛化能力。

Conclusion: MiDiGap是一种高效、灵活且具有强大泛化能力的机器人操作模仿学习框架，尤其适用于小样本学习场景和复杂多样的操作任务，其推理时引导机制显著增强了策略的实用性和鲁棒性。

Abstract: We present Mixture of Discrete-time Gaussian Processes (MiDiGap), a novel
approach for flexible policy representation and imitation learning in robot
manipulation. MiDiGap enables learning from as few as five demonstrations using
only camera observations and generalizes across a wide range of challenging
tasks. It excels at long-horizon behaviors such as making coffee, highly
constrained motions such as opening doors, dynamic actions such as scooping
with a spatula, and multimodal tasks such as hanging a mug. MiDiGap learns
these tasks on a CPU in less than a minute and scales linearly to large
datasets. We also develop a rich suite of tools for inference-time steering
using evidence such as collision signals and robot kinematic constraints. This
steering enables novel generalization capabilities, including obstacle
avoidance and cross-embodiment policy transfer. MiDiGap achieves
state-of-the-art performance on diverse few-shot manipulation benchmarks. On
constrained RLBench tasks, it improves policy success by 76 percentage points
and reduces trajectory cost by 67%. On multimodal tasks, it improves policy
success by 48 percentage points and increases sample efficiency by a factor of
20. In cross-embodiment transfer, it more than doubles policy success. We make
the code publicly available at https://midigap.cs.uni-freiburg.de.

</details>


### [249] [RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic Simulation](https://arxiv.org/abs/2505.03344)
*Keyu Chen,Wenchao Sun,Hao Cheng,Sifa Zheng*

Main category: cs.RO

TL;DR: 本文提出了一种名为 RIFT 的双阶段自动驾驶仿真框架，通过结合数据驱动和物理仿真，显著提高了交通场景的真实性和可控性。


<details>
  <summary>Details</summary>
Motivation: 在交互式闭环交通仿真中同时实现真实性和可控性是自动驾驶领域的一大挑战。数据驱动方法真实但闭环部署时存在协变量偏移问题，而物理方法可控但缺乏真实性。

Method: 提出了一种双阶段自主车(AV)中心仿真框架：首先在数据驱动模拟器中进行开环模仿学习预训练以捕捉轨迹真实性和多模态性；然后在物理模拟器中进行闭环强化学习微调以增强可控性并缓解协变量偏移。微调阶段采用 RIFT 策略，通过 GRPO 风格的组相对优势保留轨迹级多模态性，并用双剪辑机制替换 KL 正则化来增强可控性和训练稳定性。

Result: 大量实验证明，RIFT 显著改善了生成交通场景的真实性和可控性。

Conclusion: RIFT 提供了一个强大的平台，用于在多样化和交互式场景中评估自动驾驶汽车的性能，有效地解决了仿真中真实性与可控性的权衡问题。

Abstract: Achieving both realism and controllability in interactive closed-loop traffic
simulation remains a key challenge in autonomous driving. Data-driven
simulation methods reproduce realistic trajectories but suffer from covariate
shift in closed-loop deployment, compounded by simplified dynamics models that
further reduce reliability. Conversely, physics-based simulation methods
enhance reliable and controllable closed-loop interactions but often lack
expert demonstrations, compromising realism. To address these challenges, we
introduce a dual-stage AV-centered simulation framework that conducts open-loop
imitation learning pre-training in a data-driven simulator to capture
trajectory-level realism and multimodality, followed by closed-loop
reinforcement learning fine-tuning in a physics-based simulator to enhance
controllability and mitigate covariate shift. In the fine-tuning stage, we
propose RIFT, a simple yet effective closed-loop RL fine-tuning strategy that
preserves the trajectory-level multimodality through a GRPO-style
group-relative advantage formulation, while enhancing controllability and
training stability by replacing KL regularization with the dual-clip mechanism.
Extensive experiments demonstrate that RIFT significantly improves the realism
and controllability of generated traffic scenarios, providing a robust platform
for evaluating autonomous vehicle performance in diverse and interactive
scenarios.

</details>


### [250] [Sim2Real Transfer for Vision-Based Grasp Verification](https://arxiv.org/abs/2505.03046)
*Pau Amargant,Peter Hönig,Markus Vincze*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉的机器人抓取验证方法，用于判断机器人是否成功抓取物体，尤其适用于可变形物体。


<details>
  <summary>Details</summary>
Motivation: 传统的基于力和触觉传感器的抓取验证方法在处理可变形和非刚性物体时效果不佳，因此需要一种新的解决方案。

Method: 该方法采用两阶段架构：首先使用基于YOLO的目标检测模型检测并定位机器人夹爪，然后使用基于ResNet的分类器判断是否存在物体。同时，引入了一个名为HSR-GraspSynth的合成数据集来模拟不同的抓取场景，并探索了视觉问答（VQA）作为零样本基准。

Result: 实验结果表明，该方法在真实世界环境中取得了较高的准确性。

Conclusion: 该方法具有集成到抓取流水线中的潜力，代码和数据集已公开。

Abstract: The verification of successful grasps is a crucial aspect of robot
manipulation, particularly when handling deformable objects. Traditional
methods relying on force and tactile sensors often struggle with deformable and
non-rigid objects. In this work, we present a vision-based approach for grasp
verification to determine whether the robotic gripper has successfully grasped
an object. Our method employs a two-stage architecture; first YOLO-based object
detection model to detect and locate the robot's gripper and then a
ResNet-based classifier determines the presence of an object. To address the
limitations of real-world data capture, we introduce HSR-GraspSynth, a
synthetic dataset designed to simulate diverse grasping scenarios. Furthermore,
we explore the use of Visual Question Answering capabilities as a zero-shot
baseline to which we compare our model. Experimental results demonstrate that
our approach achieves high accuracy in real-world environments, with potential
for integration into grasping pipelines. Code and datasets are publicly
available at https://github.com/pauamargant/HSR-GraspSynth .

</details>


### [251] [Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach](https://arxiv.org/abs/2505.03702)
*Srecharan Selvam,Abhishesh Silwal,George Kanter*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的混合几何-神经方法，用于机器人自主抓取叶片，通过自监督学习结合传统视觉和神经网络，显著提高了抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 农业环境中自动化叶片操作面临巨大挑战，主要因为植物形态多变和叶片易变形。

Method: 提出了一种混合几何-神经方法。该方法集成YOLOv8进行实例分割和RAFT-Stereo进行3D深度估计以构建叶片表征，这些表征输入到几何特征评分流程和神经优化模块（GraspPointCNN）。关键创新是置信度加权融合机制，动态平衡两种方法的贡献。采用自监督学习框架，利用几何流程作为专家教师自动生成训练数据。

Result: 实验表明，该方法在受控环境中成功率为88.0%，在真实温室条件下为84.7%，显著优于纯几何方法（75.3%）和纯神经方法（60.2%）。

Conclusion: 这项工作为农业机器人技术建立了一个新范式，将领域专业知识与机器学习能力无缝集成，为全自动作物监测系统奠定了基础。

Abstract: Automating leaf manipulation in agricultural settings faces significant
challenges, including the variability of plant morphologies and deformable
leaves. We propose a novel hybrid geometric-neural approach for autonomous leaf
grasping that combines traditional computer vision with neural networks through
self-supervised learning. Our method integrates YOLOv8 for instance
segmentation and RAFT-Stereo for 3D depth estimation to build rich leaf
representations, which feed into both a geometric feature scoring pipeline and
a neural refinement module (GraspPointCNN). The key innovation is our
confidence-weighted fusion mechanism that dynamically balances the contribution
of each approach based on prediction certainty. Our self-supervised framework
uses the geometric pipeline as an expert teacher to automatically generate
training data. Experiments demonstrate that our approach achieves an 88.0%
success rate in controlled environments and 84.7% in real greenhouse
conditions, significantly outperforming both purely geometric (75.3%) and
neural (60.2%) methods. This work establishes a new paradigm for agricultural
robotics where domain expertise is seamlessly integrated with machine learning
capabilities, providing a foundation for fully automated crop monitoring
systems.

</details>


### [252] [Visual Imitation Enables Contextual Humanoid Control](https://arxiv.org/abs/2505.03729)
*Arthur Allshire,Hongsuk Choi,Junyi Zhang,David McAllister,Anthony Zhang,Chung Min Kim,Trevor Darrell,Pieter Abbeel,Jitendra Malik,Angjoo Kanazawa*

Main category: cs.RO

TL;DR: VIDEOMIMIC系统通过分析人类日常活动视频，使人形机器人能够学习并执行如爬楼梯、坐椅子等与环境相关的复杂全身动作。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用周围环境信息，通过简单地向人形机器人展示人类运动视频，来教会它们执行如爬楼梯和坐椅子等复杂任务。

Method: 提出VIDEOMIMIC，一个“真实到模拟再到真实”（real-to-sim-to-real）的流程。该流程挖掘日常视频，联合重建人类和环境，并为人形机器人生成相应的全身控制策略。

Result: 该流程在真实人形机器人上得到验证，展示了稳健、可重复的、与环境相关的控制能力，如上下楼梯、从椅子和长凳上坐下和站起，以及其他动态全身技能。所有这些技能均由一个基于环境和全局根指令的单一策略控制。

Conclusion: VIDEOMIMIC为教导人形机器人在多样化的真实世界环境中操作提供了一条可扩展的路径。

Abstract: How can we teach humanoids to climb staircases and sit on chairs using the
surrounding environment context? Arguably, the simplest way is to just show
them-casually capture a human motion video and feed it to humanoids. We
introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday
videos, jointly reconstructs the humans and the environment, and produces
whole-body control policies for humanoid robots that perform the corresponding
skills. We demonstrate the results of our pipeline on real humanoid robots,
showing robust, repeatable contextual control such as staircase ascents and
descents, sitting and standing from chairs and benches, as well as other
dynamic whole-body skills-all from a single policy, conditioned on the
environment and global root commands. VIDEOMIMIC offers a scalable path towards
teaching humanoids to operate in diverse real-world environments.

</details>


### [253] [AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control](https://arxiv.org/abs/2505.03738)
*Jialong Li,Xuxin Cheng,Tianshu Huang,Shiqi Yang,Ri-Zhao Qiu,Xiaolong Wang*

Main category: cs.RO

TL;DR: 提出AMO框架，结合强化学习与轨迹优化，实现人形机器人实时自适应全身控制，提升稳定性和扩展工作空间。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在执行需要大范围灵巧全身运动（如地面拾物）的任务时，其高自由度和非线性动力学带来了巨大挑战。

Method: 提出自适应运动优化（AMO）框架：1) 整合模拟到现实的强化学习（RL）与轨迹优化，实现实时自适应全身控制；2) 构建混合AMO数据集以减轻运动模仿RL中的分布偏差；3) 训练网络以稳健适应潜在的分布外（O.O.D.）指令。

Result: 在仿真和29自由度的Unitree G1人形机器人上的验证表明，与强基线方法相比，AMO展现出更优的稳定性、更大的工作空间，并且其一致的性能支持通过模仿学习执行自主任务。

Conclusion: AMO系统为人形机器人提供了多功能且鲁棒的全身控制方案，有效地增强了其在复杂环境中执行任务的能力。

Abstract: Humanoid robots derive much of their dexterity from hyper-dexterous
whole-body movements, enabling tasks that require a large operational
workspace: such as picking objects off the ground. However, achieving these
capabilities on real humanoids remains challenging due to their high degrees of
freedom (DoF) and nonlinear dynamics. We propose Adaptive Motion Optimization
(AMO), a framework that integrates sim-to-real reinforcement learning (RL) with
trajectory optimization for real-time, adaptive whole-body control. To mitigate
distribution bias in motion imitation RL, we construct a hybrid AMO dataset and
train a network capable of robust, on-demand adaptation to potentially O.O.D.
commands. We validate AMO in simulation and on a 29-DoF Unitree G1 humanoid
robot, demonstrating superior stability and an expanded workspace compared to
strong baselines. Finally, we show that AMO's consistent performance supports
autonomous task execution via imitation learning, underscoring the system's
versatility and robustness.

</details>


### [254] [Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and Avoid](https://arxiv.org/abs/2505.03694)
*Parv Kapoor,Ian Higgins,Nikhil Keetha,Jay Patrikar,Brady Moon,Zelin Ye,Yao He,Ivan Cisneros,Yaoyu Hu,Changliu Liu,Eunsuk Kang,Sebastian Scherer*

Main category: cs.RO

TL;DR: ViSafe 是一种高速纯视觉空中防撞系统，通过集成边缘AI和多摄像头硬件，为资源受限的飞行器提供可靠的安全间隔保障。


<details>
  <summary>Details</summary>
Motivation: 在高密度共享空域中，为资源受限的空中系统装备安全关键的防撞能力，以实现无缝高密度运行。

Method: ViSafe 采用全栈式“探测与规避”（DAA）解决方案，将基于学习的边缘AI框架与定制的多摄像头硬件原型（符合SWaP-C约束）紧密集成。它利用感知输入聚焦的控制屏障函数（CBF）设计、编码和执行安全阈值，以提供可证明的安全运行时保证。

Result: ViSafe 在模拟数字孪生和真实飞行场景的广泛测试中，包括不同飞行器类型、接近速率、交互几何形状和环境条件下，均能持续确保自间隔。在接近速率高达144公里/小时的真实高速防撞测试中表现出色。

Conclusion: ViSafe 为纯视觉自主防撞设定了新的基准，并为高速空中导航安全建立了新标准。

Abstract: Assured safe-separation is essential for achieving seamless high-density
operation of airborne vehicles in a shared airspace. To equip
resource-constrained aerial systems with this safety-critical capability, we
present ViSafe, a high-speed vision-only airborne collision avoidance system.
ViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by
tightly integrating a learning-based edge-AI framework with a custom
multi-camera hardware prototype designed under SWaP-C constraints. By
leveraging perceptual input-focused control barrier functions (CBF) to design,
encode, and enforce safety thresholds, ViSafe can provide provably safe runtime
guarantees for self-separation in high-speed aerial operations. We evaluate
ViSafe's performance through an extensive test campaign involving both
simulated digital twins and real-world flight scenarios. By independently
varying agent types, closure rates, interaction geometries, and environmental
conditions (e.g., weather and lighting), we demonstrate that ViSafe
consistently ensures self-separation across diverse scenarios. In
first-of-its-kind real-world high-speed collision avoidance tests with closure
rates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous
collision avoidance, establishing a new standard for safety in high-speed
aerial navigation.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [255] [Safer Prompts: Reducing IP Risk in Visual Generative AI](https://arxiv.org/abs/2505.03338)
*Lena Reissinger,Yuanyuan Li,Anna-Carolina Haensch,Neeraj Sarna*

Main category: math.NA

TL;DR: 论文评估了提示工程技术在减轻视觉生成AI模型IP侵权风险方面的有效性，发现特定提示方法能降低生成图像与训练数据的相似度。


<details>
  <summary>Details</summary>
Motivation: 视觉生成AI模型可能记忆并复制训练数据，引发知识产权（IP）侵权担忧，而提示工程提供了一种低成本的改进方案。

Method: 评估提示工程技术（特别是思维链提示和任务指令提示）在减轻扩散模型图像生成中IP侵权风险的有效性。

Result: 思维链提示和任务指令提示显著降低了生成图像与扩散模型训练数据之间的相似性。

Conclusion: 采用思维链提示和任务指令提示等提示工程技术，可以有效降低生成AI模型中IP侵权的风险。

Abstract: Visual Generative AI models have demonstrated remarkable capability in
generating high-quality images from simple inputs like text prompts. However,
because these models are trained on images from diverse sources, they risk
memorizing and reproducing specific content, raising concerns about
intellectual property (IP) infringement. Recent advances in prompt engineering
offer a cost-effective way to enhance generative AI performance. In this paper,
we evaluate the effectiveness of prompt engineering techniques in mitigating IP
infringement risks in image generation. Our findings show that Chain of Thought
Prompting and Task Instruction Prompting significantly reduce the similarity
between generated images and the training data of diffusion models, thereby
lowering the risk of IP infringement.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [256] [Accelerating Evolution: Integrating PSO Principles into Real-Coded Genetic Algorithm Crossover](https://arxiv.org/abs/2505.03217)
*Xiaobo Jin,JiaShu Tu*

Main category: cs.NE

TL;DR: 研究介绍了一种名为粒子群优化启发式交叉算子（PSOX）的新型交叉算子，用于实数编码遗传算法，它结合全局最优和历史最优解指导，以提升多样性和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统交叉算子仅在同代个体间交换信息，限制了算法维持种群多样性和加速收敛到优质区域的能力。

Method: 提出粒子群优化启发式交叉算子（PSOX），该算子不仅在同代个体间交换信息，还引入了当前全局最优解和跨多代的历史最优解的指导。通过在15个具有不同特性的基准测试函数上进行实验，并与五种先进的交叉算子进行对比评估。

Result: PSOX在解的准确性、算法稳定性和收敛速度方面均表现出优越性能，尤其在与合适的变异策略结合时效果更佳。研究还深入分析了不同变异率对PSOX性能的影响。

Conclusion: PSOX是一种有效的交叉算子，能够显著提升实数编码遗传算法的性能，并为不同优化问题的参数调整提供了实用指导。

Abstract: This study introduces an innovative crossover operator named Particle Swarm
Optimization-inspired Crossover (PSOX), which is specifically developed for
real-coded genetic algorithms. Departing from conventional crossover approaches
that only exchange information between individuals within the same generation,
PSOX uniquely incorporates guidance from both the current global best solution
and historical optimal solutions across multiple generations. This novel
mechanism enables the algorithm to maintain population diversity while
simultaneously accelerating convergence toward promising regions of the search
space. The effectiveness of PSOX is rigorously evaluated through comprehensive
experiments on 15 benchmark test functions with diverse characteristics,
including unimodal, multimodal, and highly complex landscapes. Comparative
analysis against five state-of-the-art crossover operators reveals that PSOX
consistently delivers superior performance in terms of solution accuracy,
algorithmic stability, and convergence speed, especially when combined with an
appropriate mutation strategy. Furthermore, the study provides an in-depth
investigation of how different mutation rates influence PSOX's performance,
yielding practical guidelines for parameter tuning when addressing optimization
problems with varying landscape properties.

</details>


### [257] [From Neurons to Computation: Biological Reservoir Computing for Pattern Recognition](https://arxiv.org/abs/2505.03510)
*Ludovico Iannello,Luca Ciampi,Gabriele Lagani,Fabrizio Tonelli,Eleonora Crocco,Lucio Maria Calcagnile,Angelo Di Garbo,Federico Cremisi,Giuseppe Amato*

Main category: cs.NE

TL;DR: 论文介绍了一种新颖的生物储备池计算（BRC）范式，利用培养的生物神经元作为储备池，并证明了其在模式识别任务中的可行性。


<details>
  <summary>Details</summary>
Motivation: 探索一种利用生物神经元进行储备池计算的新方法，旨在将生物神经网络用于传统上由人工神经网络处理的任务，并推动神经形态工程和生物混合计算领域的发展。

Method: 该研究构建了一个生物储备池计算（BRC）系统，其中培养的生物神经元网络作为储备池。通过多电极阵列（MEA）记录神经活动，一部分电极用于输入信号，其余电极捕捉神经响应，从而将输入数据非线性映射到高维生物特征空间。最后使用简单的线性分类器执行模式识别任务，并用位置编码、不同方向的条形图和数字识别等任务评估了系统性能。

Result: 实验结果表明，所提出的生物储备池计算系统能够成功执行各种模式识别任务（如位置编码、不同方向的条形图和数字识别），证明了利用生物神经网络处理传统人工智能任务的可行性。

Conclusion: 该研究为进一步探索生物启发计算系统铺平了道路，并指出了其在神经形态工程和生物混合计算领域的潜在应用。

Abstract: In this paper, we introduce a novel paradigm for reservoir computing (RC)
that leverages a pool of cultured biological neurons as the reservoir
substrate, creating a biological reservoir computing (BRC). This system
operates similarly to an echo state network (ESN), with the key distinction
that the neural activity is generated by a network of cultured neurons, rather
than being modeled by traditional artificial computational units. The neuronal
activity is recorded using a multi-electrode array (MEA), which enables
high-throughput recording of neural signals. In our approach, inputs are
introduced into the network through a subset of the MEA electrodes, while the
remaining electrodes capture the resulting neural activity. This generates a
nonlinear mapping of the input data to a high-dimensional biological feature
space, where distinguishing between data becomes more efficient and
straightforward, allowing a simple linear classifier to perform pattern
recognition tasks effectively. To evaluate the performance of our proposed
system, we present an experimental study that includes various input patterns,
such as positional codes, bars with different orientations, and a digit
recognition task. The results demonstrate the feasibility of using biological
neural networks to perform tasks traditionally handled by artificial neural
networks, paving the way for further exploration of biologically-inspired
computing systems, with potential applications in neuromorphic engineering and
bio-hybrid computing.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [258] [HMAE: Self-Supervised Few-Shot Learning for Quantum Spin Systems](https://arxiv.org/abs/2505.03140)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: quant-ph

TL;DR: 提出了一种名为哈密顿量掩码自编码（HMAE）的新型自监督学习框架，通过对未标记的量子哈密顿量进行预训练Transformer模型，以实现高效的少样本迁移学习，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在自旋和分子系统研究中面临标记数据稀缺和模拟计算成本高昂的关键挑战。

Method: 引入哈密顿量掩码自编码（HMAE）框架。该方法在未标记的量子哈密顿量上对Transformer进行预训练，并采用基于量子信息论的物理知识指导策略，根据哈密顿量项的物理重要性进行选择性掩码，而非随机掩码。

Result: 在12500个量子哈密顿量（60%真实，40%合成）上的实验表明，仅用10个标记样本，HMAE在相分类中达到85.3% ± 1.5%的准确率，在基态能量预测中达到0.15 ± 0.02 eV的平均绝对误差，显著优于经典图神经网络（78.1% ± 2.1%）和量子神经网络（76.8% ± 2.3%）（p < 0.01）。所需标记样本比基线方法减少3-5倍。

Conclusion: HMAE方法展现了卓越的样本效率，显著减少了对标记数据的需求。但目前该方法仅限于小型量子系统（训练时最多12量子比特，测试时有限扩展至16-20量子比特），这限制了其在材料科学和量子化学等领域中更大切实系统的直接应用，且微调和评估仍需精确对角化或张量网络获取基准真相。

Abstract: Quantum machine learning for spin and molecular systems faces critical
challenges of scarce labeled data and computationally expensive simulations. To
address these limitations, we introduce Hamiltonian-Masked Autoencoding (HMAE),
a novel self-supervised framework that pre-trains transformers on unlabeled
quantum Hamiltonians, enabling efficient few-shot transfer learning. Unlike
random masking approaches, HMAE employs a physics-informed strategy based on
quantum information theory to selectively mask Hamiltonian terms based on their
physical significance. Experiments on 12,500 quantum Hamiltonians (60%
real-world, 40% synthetic) demonstrate that HMAE achieves 85.3% $\pm$ 1.5%
accuracy in phase classification and 0.15 $\pm$ 0.02 eV MAE in ground state
energy prediction with merely 10 labeled examples - a statistically significant
improvement (p < 0.01) over classical graph neural networks (78.1% $\pm$ 2.1%)
and quantum neural networks (76.8% $\pm$ 2.3%). Our method's primary advantage
is exceptional sample efficiency - reducing required labeled examples by 3-5x
compared to baseline methods - though we emphasize that ground truth values for
fine-tuning and evaluation still require exact diagonalization or tensor
networks. We explicitly acknowledge that our current approach is limited to
small quantum systems (specifically limited to 12 qubits during training, with
limited extension to 16-20 qubits in testing) and that, while promising within
this regime, this size restriction prevents immediate application to larger
systems of practical interest in materials science and quantum chemistry.

</details>


### [259] [Quantum Feature Space of a Qubit Coupled to an Arbitrary Bath](https://arxiv.org/abs/2505.03397)
*Chris Wise,Akram Youssry,Alberto Peruzzo,Jo Plested,Matt Woolley*

Main category: quant-ph

TL;DR: 本文提出了一种无需昂贵神经网络的高效量子比特噪声表征和分类方法，称为量子特征空间。


<details>
  <summary>Details</summary>
Motivation: 传统通过功率谱密度表征量子比特-环境耦合的方式以及先前基于深度神经网络的灰箱方法在表征经典噪声影响时，结构复杂且在扩展性和实时操作方面存在挑战。

Method: 提出了一种高效的噪声算子参数化方法，构建了所谓的“量子特征空间”。利用该空间中的欧几里得距离，并结合简单的机器学习算法（如随机森林），对噪声过程进行分类。

Result: 量子特征空间上的欧几里得距离能够有效分类噪声过程。以该空间为输入的随机森林算法能有效分类噪声的平稳性和大致类别。研究还探讨了控制脉冲参数如何映射到量子特征空间。

Conclusion: 研究表明，量子特征空间为量子比特动力学中的噪声过程提供了一种高效的参数化描述和分类方法，无需依赖复杂的神经网络，并为理解控制脉冲与噪声特征的关联提供了途径。

Abstract: Qubit control protocols have traditionally leveraged a characterisation of
the qubit-bath coupling via its power spectral density. Previous work proposed
the inference of noise operators that characterise the influence of a classical
bath using a grey-box approach that combines deep neural networks with
physics-encoded layers. This overall structure is complex and poses challenges
in scaling and real-time operations. Here, we show that no expensive neural
networks are needed and that this noise operator description admits an
efficient parameterisation. We refer to the resulting parameter space as the
\textit{quantum feature space} of the qubit dynamics resulting from the coupled
bath. We show that the Euclidean distance defined over the quantum feature
space provides an effective method for classifying noise processes in the
presence of a given set of controls. Using the quantum feature space as the
input space for a simple machine learning algorithm (random forest, in this
case), we demonstrate that it can effectively classify the stationarity and the
broad class of noise processes perturbing a qubit. Finally, we explore how
control pulse parameters map to the quantum feature space.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [260] [Parameter estimation for land-surface models using machine learning libraries](https://arxiv.org/abs/2505.02979)
*Ruiyue Huang,Claire E. Heaney,Maarten van Reeuwijk*

Main category: physics.ao-ph

TL;DR: 本文使用神经网络（NN4PDEs）和PyTorch反向传播来估计陆面模型参数。


<details>
  <summary>Details</summary>
Motivation: 确定一个简单陆面模型的参数。

Method: 使用基于偏微分方程的神经网络 (NN4PDEs) 方法和 PyTorch 的反向传播引擎。首先通过正向运行模型生成合成数据集来测试逆模型，然后将逆模型应用于凤凰城的城市通量塔数据。

Result: 使用单一土壤温度时间序列无法可靠估计参数。使用两个深度的测量数据可以获得可靠的参数估计，但无法区分潜热和感热通量。在凤凰城通量塔数据中，结合观测到的有效表面反照率，可以可靠地估计热导率、体积热容和组合的感热-潜热传输系数。模型能准确预测出射长波辐射、土壤传导通量和组合的感热-潜热通量。

Conclusion: NN4PDEs 方法可以用于陆面模型的参数估计，但可靠性取决于观测数据的数量（例如，需要至少两个深度的土壤温度）以及是否能获得如有效表面反照率等辅助参数。该方法在特定条件下能够准确预测关键的能量通量。

Abstract: The Neural Networks for Partial Differential Equations (NN4PDEs) approach is
used to determine the parameters of a simple land-surface model using PyTorch's
backpropagation engine. In order to test the inverse model, a synthetic dataset
is created by running the model in forward mode with known parameter values to
create soil temperature time series that can be used as observations for the
inverse model. We show that it is not possible to obtain a reliable parameter
estimation using a single observed soil temperature time series. Using
measurements at two depths, reliable parameter estimates can be obtained
although it is not possible to differentiate between latent and sensible heat
fluxes. We apply the inverse model to urban flux tower data in Phoenix, United
States, and show that the thermal conductivity, volumetric heat capacity, and
the combined sensible-latent heat transfer coefficient can be reliably
estimated using an observed value for the effective surface albedo. The
resulting model accurately predicts the outgoing longwave radiation, conductive
soil fluxes and the combined sensible-latent heat fluxes.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [261] [Vector valued optimal transport: from dynamic to static formulations](https://arxiv.org/abs/2505.03670)
*Katy Craig,Nicolás García Trillos,Đorđe Nikolić*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Motivated by applications in classification of vector valued measures and
multispecies PDE, we develop a theory that unifies existing notions of vector
valued optimal transport, from dynamic formulations (\`a la Benamou-Brenier) to
static formulations (\`a la Kantorovich). In our framework, vector valued
measures are modeled as probability measures on a product space $\mathbb{R}^d
\times G$, where $G$ is a weighted graph over a finite set of nodes and the
graph geometry strongly influences the associated dynamic and static distances.
We obtain sharp inequalities relating four notions of vector valued optimal
transport and prove that the distances are mutually bi-H\"older equivalent. We
discuss the theoretical and practical advantages of each metric and indicate
potential applications in multispecies PDE and data analysis. In particular,
one of the static formulations discussed in the paper is amenable to
linearization, a technique that has been explored in recent years to accelerate
the computation of pairwise optimal transport distances.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [262] [CreoPep: A Universal Deep Learning Framework for Target-Specific Peptide Design and Optimization](https://arxiv.org/abs/2505.02887)
*Cheng Ge,Han-Shen Tae,Zhenqiang Zhang,Lu Lu,Zhijie Huang,Yilin Wang,Tao Jiang,Wenqing Cai,Shan Chang,David J. Adams,Rilei Yu*

Main category: q-bio.BM

TL;DR: 提出CreoPep深度学习框架，用于设计高亲和力肽突变体，成功设计了针对α7烟碱乙酰胆碱受体的亚微摩尔级锥形毒素抑制剂。


<details>
  <summary>Details</summary>
Motivation: 目标特异性肽（如锥形毒素）因天然变体多样性有限及传统优化策略耗时费力，其治疗潜力未能充分利用。

Method: CreoPep是一个基于深度学习的条件生成框架，整合了掩码语言建模和渐进式掩码方案，并结合了基于FoldX的能量筛选和温控多项式采样的集成增强流程。

Result: 成功设计了针对α7烟碱乙酰胆碱受体的锥形毒素抑制剂，在电生理测定中达到亚微摩尔级效力。结构分析显示，CreoPep生成的变体展现出保守及新颖的结合模式，包括缺乏二硫键的形式。

Conclusion: CreoPep提供了一个连接计算肽设计与实验验证的强大且可推广的平台，加速了下一代肽类疗法的发现。

Abstract: Target-specific peptides, such as conotoxins, exhibit exceptional binding
affinity and selectivity toward ion channels and receptors. However, their
therapeutic potential remains underutilized due to the limited diversity of
natural variants and the labor-intensive nature of traditional optimization
strategies. Here, we present CreoPep, a deep learning-based conditional
generative framework that integrates masked language modeling with a
progressive masking scheme to design high-affinity peptide mutants while
uncovering novel structural motifs. CreoPep employs an integrative augmentation
pipeline, combining FoldX-based energy screening with temperature-controlled
multinomial sampling, to generate structurally and functionally diverse
peptides that retain key pharmacological properties. We validate this approach
by designing conotoxin inhibitors targeting the $\alpha$7 nicotinic
acetylcholine receptor, achieving submicromolar potency in electrophysiological
assays. Structural analysis reveals that CreoPep-generated variants engage in
both conserved and novel binding modes, including disulfide-deficient forms,
thus expanding beyond conventional design paradigms. Overall, CreoPep offers a
robust and generalizable platform that bridges computational peptide design
with experimental validation, accelerating the discovery of next-generation
peptide therapeutics.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [263] [SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation](https://arxiv.org/abs/2505.03273)
*Zhaoxi Mu,Xinyu Yang,Gang Wang*

Main category: cs.SD

TL;DR: SepALM 是一种利用音频语言模型（ALM）在初步分离后对文本域中的语音进行校正和重新合成的新方法，旨在提升语音分离的精度和在复杂声学环境中的适应性。


<details>
  <summary>Details</summary>
Motivation: 当前的语音分离技术在处理嘈杂和混响等真实世界环境的复杂性时常常遇到困难，可能导致分离后的语音出现伪影或失真。

Method: 引入了 SepALM 方法，该方法包含四个核心组件：分离器、校正器（基于ALM进行文本域错误校正）、合成器和对齐器。此外，还开发了思维链（CoT）提示和知识蒸馏技术来辅助 ALM 的推理和训练过程。

Result: 实验证明，SepALM 不仅提高了语音分离的精确度，而且显著增强了其在新的声学环境中的适应能力。

Conclusion: SepALM 通过整合基于 ALM 的端到端错误校正机制，有效克服了传统语音分离方法在优化和错误累积方面的挑战，提升了在复杂真实场景下的语音分离效果和鲁棒性。

Abstract: While contemporary speech separation technologies adeptly process lengthy
mixed audio waveforms, they are frequently challenged by the intricacies of
real-world environments, including noisy and reverberant settings, which can
result in artifacts or distortions in the separated speech. To overcome these
limitations, we introduce SepALM, a pioneering approach that employs audio
language models (ALMs) to rectify and re-synthesize speech within the text
domain following preliminary separation. SepALM comprises four core components:
a separator, a corrector, a synthesizer, and an aligner. By integrating an
ALM-based end-to-end error correction mechanism, we mitigate the risk of error
accumulation and circumvent the optimization hurdles typically encountered in
conventional methods that amalgamate automatic speech recognition (ASR) with
large language models (LLMs). Additionally, we have developed Chain-of-Thought
(CoT) prompting and knowledge distillation techniques to facilitate the
reasoning and training processes of the ALM. Our experiments substantiate that
SepALM not only elevates the precision of speech separation but also markedly
bolsters adaptability in novel acoustic environments.

</details>


### [264] [A study on audio synchronous steganography detection and distributed guide inference model based on sliding spectral features and intelligent inference drive](https://arxiv.org/abs/2505.03193)
*Wei Meng*

Main category: cs.SD

TL;DR: 提出一种基于短视频音频同步流的隐写检测与分布式引导重建模型，利用滑动频谱特征和智能推理机制分析潜在的隐蔽通信。


<details>
  <summary>Details</summary>
Motivation: 传统技术在检测短视频平台中新兴的音频同步流隐写术方面存在局限性，需要新的方法来应对这种隐蔽通信方式。

Method: 该研究采用25毫秒滑动窗口短时傅里叶变换（STFT）提取音频主频轨迹，构建同步帧检测模型（M1）以识别特定帧标记，并用结构化模型（M2）解码随后的32字节载荷以推断分布式引导命令。同时，还探讨了多段拼接模型。

Result: 在“玉盘”短视频样本的36至45秒音频段中，检测到低熵、重复的字节序列和高度集中的频谱能量，证实了同步帧的存在。虽未恢复明文语义，但命令字段布局的一致性暗示了军事通信协议的特征。多段拼接模型展示了跨视频嵌入和集中解码的能力。

Conclusion: 所提出的框架验证了滑动频谱特征在同步隐写检测中的有效性，并构建了一个可扩展的推理模型，可用于开放平台上的隐蔽通信分析和战术引导模拟。

Abstract: With the rise of short video platforms in global communication, embedding
steganographic data in audio synchronization streams has emerged as a new
covert communication method. To address the limitations of traditional
techniques in detecting synchronized steganography, this paper proposes a
detection and distributed guidance reconstruction model based on short video
"Yupan" samples released by China's South Sea Fleet on TikTok. The method
integrates sliding spectrum feature extraction and intelligent inference
mechanisms. A 25 ms sliding window with short-time Fourier transform (STFT) is
used to extract the main frequency trajectory and construct the synchronization
frame detection model (M1), identifying a frame flag "FFFFFFFFFFFFFFFFFF80".
The subsequent 32-byte payload is decoded by a structured model (M2) to infer
distributed guidance commands. Analysis reveals a low-entropy, repetitive byte
sequence in the 36 to 45 second audio segment with highly concentrated spectral
energy, confirming the presence of synchronization frames. Although plaintext
semantics are not restored, the consistency in command field layout suggests
features of military communication protocols. The multi-segment splicing model
further shows cross-video embedding and centralized decoding capabilities. The
proposed framework validates the effectiveness of sliding spectral features for
synchronized steganography detection and builds an extensible inference model
for covert communication analysis and tactical guidance simulation on open
platforms.

</details>


### [265] [Mamba-Diffusion Model with Learnable Wavelet for Controllable Symbolic Music Generation](https://arxiv.org/abs/2505.03314)
*Jincheng Zhang,György Fazekas,Charalampos Saitis*

Main category: cs.SD

TL;DR: 本文提出了一种新的扩散模型，通过将符号音乐表示为钢琴卷帘图，并结合Transformer-Mamba块和可学习小波变换，实现了高质量、可控的符号音乐生成。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像合成方面很受欢迎，但在符号音乐生成领域的应用仍不充分，因为标准扩散模型不适用于离散的音乐数据。

Method: 将符号音乐表示为类图像的钢琴卷帘图 (pianorolls)，并引入一种新的扩散模型，该模型包含提出的Transformer-Mamba块和可学习小波变换。同时，利用无分类器指导 (classifier-free guidance) 生成具有目标和弦的符号音乐。

Result: 评估表明，该方法在音乐质量和可控性方面取得了令人信服的结果，并在钢琴卷帘图生成方面优于强基线模型。

Conclusion: 该研究提出的方法能够有效地利用扩散模型生成高质量且可控的符号音乐，为该领域提供了新的探索方向。

Abstract: The recent surge in the popularity of diffusion models for image synthesis
has attracted new attention to their potential for generation tasks in other
domains. However, their applications to symbolic music generation remain
largely under-explored because symbolic music is typically represented as
sequences of discrete events and standard diffusion models are not well-suited
for discrete data. We represent symbolic music as image-like pianorolls,
facilitating the use of diffusion models for the generation of symbolic music.
Moreover, this study introduces a novel diffusion model that incorporates our
proposed Transformer-Mamba block and learnable wavelet transform.
Classifier-free guidance is utilised to generate symbolic music with target
chords. Our evaluation shows that our method achieves compelling results in
terms of music quality and controllability, outperforming the strong baseline
in pianoroll generation. Our code is available at
https://github.com/jinchengzhanggg/proffusion.

</details>


### [266] [Knowledge Distillation for Speech Denoising by Latent Representation Alignment with Cosine Distance](https://arxiv.org/abs/2505.03442)
*Diep Luong,Mikko Heikkinen,Konstantinos Drossos,Tuomas Virtanen*

Main category: cs.SD

TL;DR: 本文提出了一种新的知识蒸馏方法，通过利用去噪自动编码器、线性反向瓶颈和余弦相似性，旨在改善语音降噪中学生模型在低资源环境下的学习效果，使其能够处理与教师模型更大的不匹配情况。


<details>
  <summary>Details</summary>
Motivation: 现有的强大语音降噪方法通常过于复杂，难以部署在资源受限的设备上。知识蒸馏是降低复杂性的有效途径，但现有语音降噪的知识蒸馏方法可能会限制学生模型的学习能力，使其受限于教师模型的分布、信息排序和特征维度。

Method: 提出了一种新的知识蒸馏方法，该方法结合了去噪自动编码器（denoising-autoencoder）框架、线性反向瓶颈（linear inverted bottlenecks）以及余弦相似性（cosine similarity）的特性，以解决现有知识蒸馏方法中学生模型学习受限的问题。

Result: 在公开数据集上进行的重复实验表明，与现有的SOTA基线方法相比，采用所提出的方法，学生模型不仅性能更优，而且能够容忍与教师模型之间更大的不匹配条件。

Conclusion: 所提出的方法能够使学生模型在语音降噪任务中表现更好，并且能更好地适应与教师模型存在较大差异的情况，为在低资源设备上部署高效语音降噪模型提供了新的途径。

Abstract: Speech denoising is a generally adopted and impactful task, appearing in many
common and everyday-life use cases. Although there are very powerful methods
published, most of those are too complex for deployment in everyday and
low-resources computational environments, like hand-held devices, intelligent
glasses, hearing aids, etc. Knowledge distillation (KD) is a prominent way for
alleviating this complexity mismatch and is based on the
transferring/distilling of knowledge from a pre-trained complex model, the
teacher, to another less complex one, the student. Existing KD methods for
speech denoising are based on processes that potentially hamper the KD by
bounding the learning of the student to the distribution, information ordering,
and feature dimensionality learned by the teacher. In this paper, we present
and assess a method that tries to treat this issue, by exploiting the
well-known denoising-autoencoder framework, the linear inverted bottlenecks,
and the properties of the cosine similarity. We use a public dataset and
conduct repeated experiments with different mismatching scenarios between the
teacher and the student, reporting the mean and standard deviation of the
metrics of our method and another, state-of-the-art method that is used as a
baseline. Our results show that with the proposed method, the student can
perform better and can also retain greater mismatching conditions compared to
the teacher.

</details>


### [267] [CoGenAV: Versatile Audio-Visual Representation Learning via Contrastive-Generative Synchronization](https://arxiv.org/abs/2505.03186)
*Detao Bai,Zhiheng Ma,Xihan Wei,Liefeng Bo*

Main category: cs.SD

TL;DR: CoGenAV是一种数据高效的模型，通过学习通用的音视频表示来提升多种语音和音视频任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统纯音频系统在嘈杂等挑战性条件下表现不佳，而说话者的唇部运动、声音和语言内容之间的同步性为改进语音处理提供了丰富信息。

Method: 引入CoGenAV模型，通过优化对比特征对齐和生成式文本预测的双重目标进行训练，该目标源于自然的音视频同步性，并仅使用LRS2数据集的223小时标记数据。

Result: CoGenAV表示在LRS2上的音视频语音识别(AVSR)中实现了1.27的词错误率(WER)，在视觉语音识别(VSR)中WER为22.0，并在嘈杂环境中将性能提升超过70%。此外，它还有益于语音增强、分离等语音重建任务，并在主动说话人检测(ASD)等音视频同步任务中取得了有竞争力的结果。

Conclusion: CoGenAV通过其对比-生成同步策略有效地捕捉了跨模态关联，学习到的通用音视频表示显著提升了多种语音和音视频任务的性能，并将开源以促进进一步研究。

Abstract: The inherent synchronization between a speaker's lip movements, voice, and
the underlying linguistic content offers a rich source of information for
improving speech processing tasks, especially in challenging conditions where
traditional audio-only systems falter. We introduce CoGenAV, a powerful and
data-efficient model designed to learn versatile audio-visual representations
applicable across a wide range of speech and audio-visual tasks. CoGenAV is
trained by optimizing a dual objective derived from natural audio-visual
synchrony, contrastive feature alignment and generative text prediction, using
only 223 hours of labeled data from the LRS2 dataset. This
contrastive-generative synchronization strategy effectively captures
fundamental cross-modal correlations. We showcase the effectiveness and
versatility of the learned CoGenAV representations on multiple benchmarks. When
utilized for Audio-Visual Speech Recognition (AVSR) on LRS2, these
representations contribute to achieving a state-of-the-art Word Error Rate
(WER) of 1.27. They also enable strong performance in Visual Speech Recognition
(VSR) with a WER of 22.0 on LRS2, and significantly improve performance in
noisy environments by over 70%. Furthermore, CoGenAV representations benefit
speech reconstruction tasks, boosting performance in Speech Enhancement and
Separation, and achieve competitive results in audio-visual synchronization
tasks like Active Speaker Detection (ASD). Our model will be open-sourced to
facilitate further development and collaboration within both academia and
industry.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [268] [Soft Best-of-n Sampling for Model Alignment](https://arxiv.org/abs/2505.03156)
*Claudio Mayrink Verdun,Alex Oesterling,Himabindu Lakkaraju,Flavio P. Calmon*

Main category: cs.IT

TL;DR: 该研究提出了一种名为Soft Best-of-n的采样方法，作为对Best-of-n (BoN)采样的改进，旨在更平滑地平衡语言模型输出的奖励与分布失真。


<details>
  <summary>Details</summary>
Motivation: 传统的Best-of-n (BoN)采样方法在提高奖励的同时会引入分布失真，且其失真控制（通过调整样本数n）较为粗糙。研究旨在提供一种更平滑、可控的方式来平衡奖励和失真。

Method: 引入了Soft Best-of-n采样方法，它通过一个温度参数λ实现了在原始分布和奖励最大化分布之间的平滑插值。同时，研究者们建立了理论保证，并分析了针对离散输出序列的加性奖励模型。

Result: Soft Best-of-n采样在KL散度和预期（相对）奖励方面以O(1/n)的速率快速收敛到最优倾斜分布。此外，分析揭示了分块采样（blockwise sampling）的根本局限性。

Conclusion: Soft Best-of-n采样是对BoN采样的一个有效泛化，通过引入温度参数，它能够在奖励和分布失真之间进行更精细的权衡，并具有理论上的收敛保证。

Abstract: Best-of-$n$ (BoN) sampling is a practical approach for aligning language
model outputs with human preferences without expensive fine-tuning. BoN
sampling is performed by generating $n$ responses to a prompt and then
selecting the sample that maximizes a reward function. BoN yields high reward
values in practice at a distortion cost, as measured by the KL-divergence
between the sampled and original distribution. This distortion is coarsely
controlled by varying the number of samples: larger $n$ yields a higher reward
at a higher distortion cost. We introduce Soft Best-of-$n$ sampling, a
generalization of BoN that allows for smooth interpolation between the original
distribution and reward-maximizing distribution through a temperature parameter
$\lambda$. We establish theoretical guarantees showing that Soft Best-of-$n$
sampling converges sharply to the optimal tilted distribution at a rate of
$O(1/n)$ in KL and the expected (relative) reward. For sequences of discrete
outputs, we analyze an additive reward model that reveals the fundamental
limitations of blockwise sampling.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [269] [An Active Inference perspective on Neurofeedback Training](https://arxiv.org/abs/2505.03308)
*Côme Annicchiarico,Fabien Lotte,Jérémie Mattout*

Main category: q-bio.NC

TL;DR: 该研究使用主动推断框架建立了一个神经反馈训练（NFT）的计算模型，用于分析影响训练效果的因素，如反馈质量和受试者的先验信念。


<details>
  <summary>Details</summary>
Motivation: 神经反馈训练（NFT）结果变异性大且机制尚不明确，阻碍了其验证。

Method: 研究者提出了一个NFT闭环的正式计算模型。该模型基于主动推断（Active Inference），一个贝叶斯框架，通过模拟智能体与NFT环境的交互，来测试不同设计选择（如反馈质量、生物标志物有效性）和受试者因素（如先验信念）对训练的影响。

Result: 模拟结果表明，训练效果对反馈噪声或偏差以及先验信念敏感（强调了指导说明的重要性），并且即使是完美的反馈也不足以保证高绩效。

Conclusion: 该计算模型方法为评估和预测NFT的变异性、解释经验数据以及未来开发个性化训练方案提供了一个有价值的工具。

Abstract: Neurofeedback training (NFT) aims to teach self-regulation of brain activity
through real-time feedback, but suffers from highly variable outcomes and
poorly understood mechanisms, hampering its validation. To address these
issues, we propose a formal computational model of the NFT closed loop. Using
Active Inference, a Bayesian framework modelling perception, action, and
learning, we simulate agents interacting with an NFT environment. This enables
us to test the impact of design choices (e.g., feedback quality, biomarker
validity) and subject factors (e.g., prior beliefs) on training. Simulations
show that training effectiveness is sensitive to feedback noise or bias, and to
prior beliefs (highlighting the importance of guiding instructions), but also
reveal that perfect feedback is insufficient to guarantee high performance.
This approach provides a tool for assessing and predicting NFT variability,
interpret empirical data, and potentially develop personalized training
protocols.

</details>


### [270] [Binding threshold units with artificial oscillatory neurons](https://arxiv.org/abs/2505.03648)
*Vladimir Fanaskov,Ivan Oseledets*

Main category: q-bio.NC

TL;DR: 本文提出了一个区分振荡神经元与阈值单元并耦合它们的理论框架，构建了Hopfield-Kuramoto联想记忆模型，并展示了其在低秩校正方面的应用。


<details>
  <summary>Details</summary>
Motivation: 尽管振荡神经元在某些任务中表现优于阈值单元，但其与阈值单元的耦合机制尚不明确且缺乏统一理论框架。研究旨在从理论上区分这两类单元，并建立它们之间有效的耦合机制，同时探讨其生物学意义（阈值单元模拟放电强度，振荡单元通过频率调制促进信息交换）。

Method: 通过约束动力系统使其承认李雅普诺夫函数，分别推导出阈值单元的Hopfield模型和振荡单元的广义Kuramoto模型。进而将两者耦合构建了Hopfield-Kuramoto联想记忆模型，并利用振荡神经元实现Hopfield网络的低秩权重校正（类比Hebbian学习或LoRA方法），通过小型实验验证了该耦合的实际应用。

Result: 成功构建了区分振荡神经元与阈值单元的理论框架，并建立了它们之间的耦合机制，形成了同样承认李雅普诺夫函数的Hopfield-Kuramoto联想记忆模型。研究表明，振荡神经元可以用于实现Hopfield网络权重矩阵的低秩校正，这种校正机制类似于赫布学习或大型语言模型中流行的LoRA方法，并通过小型实验展示了其实际可行性。

Conclusion: 本研究提出的理论框架成功地将振荡神经元和阈值单元统一起来，构建了Hopfield-Kuramoto模型。该模型不仅具有理论意义，其通过振荡神经元实现的低秩校正机制（类似LoRA）也展示了在实际应用中的潜力。

Abstract: Artificial Kuramoto oscillatory neurons were recently introduced as an
alternative to threshold units. Empirical evidence suggests that oscillatory
units outperform threshold units in several tasks including unsupervised object
discovery and certain reasoning problems. The proposed coupling mechanism for
these oscillatory neurons is heterogeneous, combining a generalized Kuramoto
equation with standard coupling methods used for threshold units. In this
research note, we present a theoretical framework that clearly distinguishes
oscillatory neurons from threshold units and establishes a coupling mechanism
between them. We argue that, from a biological standpoint, oscillatory and
threshold units realise distinct aspects of neural coding: roughly, threshold
units model intensity of neuron firing, while oscillatory units facilitate
information exchange by frequency modulation. To derive interaction between
these two types of units, we constrain their dynamics by focusing on dynamical
systems that admit Lyapunov functions. For threshold units, this leads to
Hopfield associative memory model, and for oscillatory units it yields a
specific form of generalized Kuramoto model. The resulting dynamical systems
can be naturally coupled to form a Hopfield-Kuramoto associative memory model,
which also admits a Lyapunov function. Various forms of coupling are possible.
Notably, oscillatory neurons can be employed to implement a low-rank correction
to the weight matrix of a Hopfield network. This correction can be viewed
either as a form of Hebbian learning or as a popular LoRA method used for
fine-tuning of large language models. We demonstrate the practical realization
of this particular coupling through illustrative toy experiments.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [271] [The Precautionary Principle and the Innovation Principle: Incompatible Guides for AI Innovation Governance?](https://arxiv.org/abs/2505.02846)
*Kim Kaivanto*

Main category: cs.CY

TL;DR: 论文探讨了在人工智能监管中，预防原则（PP）和创新原则（IP）在其弱形式下并非不相容，可通过权衡两类错误成本（错误阻止创新与错误允许有害创新）来指导决策，并提出监管沙盒是实现此平衡的有效工具。


<details>
  <summary>Details</summary>
Motivation: 在人工智能（AI）治理和监管的政策辩论中，预防原则（PP）和创新原则（IP）被各自的利益相关者所倡导，本文旨在探讨这两个原则是否完全不相容和相互矛盾。

Method: 文章通过分析弱形式的预防原则和创新原则，并运用信号检测理论（SDT）模型，评估了错误阻止创新扩散（第一类错误）和错误允许创新扩散（第二类错误）的成本。

Result: 研究表明，当预期第一类错误成本与第二类错误成本的比率足够小时，弱PP（监管红灯）是最优的；当该比率足够大时，弱IP（监管绿灯）是最优的。对于中间的预期成本比率，“等待和监测”（黄灯）政策，如监管沙盒，则是最优选择。监管沙盒允许在受控环境中测试AI，帮助监管者和创新者了解并调整预期成本比率。

Conclusion: 弱形式的预防原则和创新原则并非相互排斥，而是可以通过全面考虑和权衡两类决策错误的成本来实现兼容。监管沙盒等工具为在“等待和监测”区域内学习和适应提供了有效途径，以避免进入弱PP的“红灯”区域。

Abstract: In policy debates concerning the governance and regulation of Artificial
Intelligence (AI), both the Precautionary Principle (PP) and the Innovation
Principle (IP) are advocated by their respective interest groups. Do these
principles offer wholly incompatible and contradictory guidance? Does one
necessarily negate the other? I argue here that provided attention is
restricted to weak-form PP and IP, the answer to both of these questions is
"No." The essence of these weak formulations is the requirement to fully
account for type-I error costs arising from erroneously preventing the
innovation's diffusion through society (i.e. mistaken regulatory red-lighting)
as well as the type-II error costs arising from erroneously allowing the
innovation to diffuse through society (i.e. mistaken regulatory
green-lighting). Within the Signal Detection Theory (SDT) model developed here,
weak-PP red-light (weak-IP green-light) determinations are optimal for
sufficiently small (large) ratios of expected type-I to type-II error costs.
For intermediate expected cost ratios, an amber-light 'wait-and-monitor' policy
is optimal. Regulatory sandbox instruments allow AI testing and experimentation
to take place within a structured environment of limited duration and societal
scale, whereby the expected cost ratio falls within the 'wait-and-monitor'
range. Through sandboxing regulators and innovating firms learn more about the
expected cost ratio, and what respective adaptations -- of regulation, of
technical solution, of business model, or combination thereof, if any -- are
needed to keep the ratio out of the weak-PP red-light zone.

</details>


### [272] [Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration](https://arxiv.org/abs/2505.02848)
*Kexin Ding,Mu Zhou,Akshay Chaudhari,Shaoting Zhang,Dimitris N. Metaxas*

Main category: cs.CY

TL;DR: 这篇综述探讨了大型语言模型（LLM）与医疗保健利益相关者偏好对齐的重要性，并回顾了实现对齐的方法、工具和应用，以构建可信赖的医疗应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在医疗保健领域的广泛应用引发了对其输出与利益相关者偏好、知识、需求和价值观一致性的担忧。这种对齐对于有效、安全和负责任地赋能医疗保健流程至关重要。

Method: 本文作为一篇综述，讨论了医疗保健利益相关者与LLM之间对齐的方法、工具和应用。强调人类专业人员需参与LLM在医疗保健中应用的整个生命周期，包括训练数据策划、模型训练和推理，以指导和增强LLM的性能。

Result: 研究表明，通过适当增强医疗保健知识整合、任务理解和人类指导，大型语言模型可以更好地遵循人类价值观。

Conclusion: 增强人类与大型语言模型之间的对齐是构建值得信赖的真实世界医疗保健应用的关键，并对未来发展方向进行了展望。

Abstract: The wide exploration of large language models (LLMs) raises the awareness of
alignment between healthcare stakeholder preferences and model outputs. This
alignment becomes a crucial foundation to empower the healthcare workflow
effectively, safely, and responsibly. Yet the varying behaviors of LLMs may not
always match with healthcare stakeholders' knowledge, demands, and values. To
enable a human-AI alignment, healthcare stakeholders will need to perform
essential roles in guiding and enhancing the performance of LLMs. Human
professionals must participate in the entire life cycle of adopting LLM in
healthcare, including training data curation, model training, and inference. In
this review, we discuss the approaches, tools, and applications of alignments
between healthcare stakeholders and LLMs. We demonstrate that LLMs can better
follow human values by properly enhancing healthcare knowledge integration,
task understanding, and human guidance. We provide outlooks on enhancing the
alignment between humans and LLMs to build trustworthy real-world healthcare
applications.

</details>


### [273] [Enhancing tutoring systems by leveraging tailored promptings and domain knowledge with Large Language Models](https://arxiv.org/abs/2505.02849)
*Mohsen Balavar,Wenli Yang,David Herbert,Soonja Yeom*

Main category: cs.CY

TL;DR: 该研究通过将检索增强生成（RAG）与大型语言模型（LLM）相结合，为计算机科学编程提供个性化辅导，并证明了其有效性和适应性。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能驱动的计算机辅助学习（CBL）工具（如智能辅导系统ITS）在个性化和灵活性方面有所增强，但在适应多样化学习风格和提供实时、情境感知反馈方面仍存在挑战。

Method: 研究通过检索增强生成（RAG）将与技能对齐的反馈整合到大型语言模型（LLM）的提示工程中，并开发了一个应用程序以在计算机科学编程情境中通过个性化辅导增强学习。通过三个量化指标（可读性分数、响应时间和反馈深度）对系统在三个不同复杂度的编程任务中的表现进行了评估。

Result: 该系统成功地将模拟学生分为三个技能水平类别，并提供了情境感知的反馈。这种针对性方法与通用方法相比，显示出更好的有效性和适应性。

Conclusion: 将通过RAG实现的技能对齐反馈整合到LLM中，用于计算机科学编程的个性化辅导，是一种比通用方法更有效、适应性更强的方法。

Abstract: Recent advancements in artificial intelligence (AI) and machine learning have
reignited interest in their impact on Computer-based Learning (CBL). AI-driven
tools like ChatGPT and Intelligent Tutoring Systems (ITS) have enhanced
learning experiences through personalisation and flexibility. ITSs can adapt to
individual learning needs and provide customised feedback based on a student's
performance, cognitive state, and learning path. Despite these advances,
challenges remain in accommodating diverse learning styles and delivering
real-time, context-aware feedback. Our research aims to address these gaps by
integrating skill-aligned feedback via Retrieval Augmented Generation (RAG)
into prompt engineering for Large Language Models (LLMs) and developing an
application to enhance learning through personalised tutoring in a computer
science programming context. The pilot study evaluated a proposed system using
three quantitative metrics: readability score, response time, and feedback
depth, across three programming tasks of varying complexity. The system
successfully sorted simulated students into three skill-level categories and
provided context-aware feedback. This targeted approach demonstrated better
effectiveness and adaptability compared to general methods.

</details>


### [274] [A Computational Model of Inclusive Pedagogy: From Understanding to Application](https://arxiv.org/abs/2505.02853)
*Francesco Balzan,Pedro P. Santos,Maurizio Gabbrielli,Mahault Albarracin,Manuel Lopes*

Main category: cs.CY

TL;DR: 本文提出了一个师生互动（T-SI）的协同适应计算模型，证明了协同适应策略能改善各类学习者的学习成果。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型未能充分体现师生互动中的协同适应动态，这阻碍了教育科学研究和机器学习在教育领域的应用潜力。

Method: 研究者构建了一个师生互动（T-SI）计算模型，该模型融入了人类教育的情境洞察，并在模拟教室环境中（特别考虑学生感官信息获取不均等情况）评估了不同互动策略。

Result: 实验结果显示，采用协同适应原则（如双向能动性）的策略，其学习效果显著优于仅教师或学生单方面主动的策略，且对所有类型的学习者均有益。

Conclusion: 该模型为测试和扩展情境相关的教育见解提供了框架，有助于生成假设，并为开发能动态适应学习者需求的公平AI教育技术奠定了基础。

Abstract: Human education transcends mere knowledge transfer, it relies on
co-adaptation dynamics -- the mutual adjustment of teaching and learning
strategies between agents. Despite its centrality, computational models of
co-adaptive teacher-student interactions (T-SI) remain underdeveloped. We argue
that this gap impedes Educational Science in testing and scaling contextual
insights across diverse settings, and limits the potential of Machine Learning
systems, which struggle to emulate and adaptively support human learning
processes. To address this, we present a computational T-SI model that
integrates contextual insights on human education into a testable framework. We
use the model to evaluate diverse T-SI strategies in a realistic synthetic
classroom setting, simulating student groups with unequal access to sensory
information. Results show that strategies incorporating co-adaptation
principles (e.g., bidirectional agency) outperform unilateral approaches (i.e.,
where only the teacher or the student is active), improving the learning
outcomes for all learning types. Beyond the testing and scaling of
context-dependent educational insights, our model enables hypothesis generation
in controlled yet adaptable environments. This work bridges non-computational
theories of human education with scalable, inclusive AI in Education systems,
providing a foundation for equitable technologies that dynamically adapt to
learner needs.

</details>


### [275] [AI Education in a Mirror: Challenges Faced by Academic and Industry Experts](https://arxiv.org/abs/2505.02856)
*Mahir Akgun,Hadi Hosseini*

Main category: cs.CY

TL;DR: 通过访谈AI专家，本研究揭示了学术界AI教育与工业界实际挑战间的差距，并指出了课程改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能技术的发展，学术界的AI教育与真实工业界挑战之间仍然存在差距，这是一个重要的研究领域。

Method: 对14位人工智能专家（8位来自工业界，6位来自学术界）进行了半结构化访谈。

Result: 研究识别了与数据质量和可用性、模型可扩展性、实际约束、用户行为和可解释性相关的关键挑战。尽管两组都遇到数据和模型适应困难，但工业界专业人士更常强调部署约束、资源限制和外部依赖，而学术界人士则更强调理论适应和标准化问题。

Conclusion: 这些探索性发现表明，人工智能课程可以更好地整合真实世界的复杂性、软件工程原理和跨学科学习，同时认识到培养基础和伦理推理能力的更广泛教育目标。

Abstract: As Artificial Intelligence (AI) technologies continue to evolve, the gap
between academic AI education and real-world industry challenges remains an
important area of investigation. This study provides preliminary insights into
challenges AI professionals encounter in both academia and industry, based on
semi-structured interviews with 14 AI experts - eight from industry and six
from academia. We identify key challenges related to data quality and
availability, model scalability, practical constraints, user behavior, and
explainability. While both groups experience data and model adaptation
difficulties, industry professionals more frequently highlight deployment
constraints, resource limitations, and external dependencies, whereas academics
emphasize theoretical adaptation and standardization issues. These exploratory
findings suggest that AI curricula could better integrate real-world
complexities, software engineering principles, and interdisciplinary learning,
while recognizing the broader educational goals of building foundational and
ethical reasoning skills.

</details>


### [276] [Understanding University Students' Use of Generative AI: The Roles of Demographics and Personality Traits](https://arxiv.org/abs/2505.02863)
*Newnew Deng,Edward Jiusi Liu,Xiaoming Zhai*

Main category: cs.CY

TL;DR: 一项针对美国大学生的调查显示，生成式AI（GAI）的使用受学年、母语、种族及大五人格特质（如尽责性、宜人性、情绪稳定性、外向性和智力/想象力）的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管大学生中GAI的使用迅速增加，但关于学生GAI使用情况及其影响因素的实证研究仍然有限，本研究旨在填补这一空白。

Method: 对363名美国本科生和研究生进行了调查，考察了他们的GAI使用情况，以及其与人口统计学变量（学年、母语、种族）和大五人格特质的关系。

Result: 研究发现：(a) 高年级学生更倾向于使用GAI并偏好GAI而非传统资源。(b) 非英语母语者比母语者更易使用和接受GAI。(c) 与白人学生相比，亚裔学生GAI使用率更高、感知学术益处更大、偏好更强；黑人学生报告GAI对其学业表现有更积极影响。控制人口统计学因素后，人格特质仍显著预测GAI使用和态度：(a) 尽责性高的学生GAI使用较少。(b) 宜人性高的学生认为GAI对学业表现的积极影响较小，并对其学术使用表达更多伦理担忧。(c) 情绪稳定性高的学生报告GAI对学习有更积极影响，对其学术使用的担忧较少。(d) 外向性高的学生更偏好GAI而非传统资源。(e) 智力/想象力高的学生倾向于偏好传统资源。

Conclusion: 研究结果强调了大学需要提供个性化指导，以确保学生在学术追求中有效、合乎道德且公平地使用GAI。

Abstract: The use of generative AI (GAI) among university students is rapidly
increasing, yet empirical research on students' GAI use and the factors
influencing it remains limited. To address this gap, we surveyed 363
undergraduate and graduate students in the United States, examining their GAI
usage and how it relates to demographic variables and personality traits based
on the Big Five model (i.e., extraversion, agreeableness, conscientiousness,
and emotional stability, and intellect/imagination). Our findings reveal: (a)
Students in higher academic years are more inclined to use GAI and prefer it
over traditional resources. (b) Non-native English speakers use and adopt GAI
more readily than native speakers. (c) Compared to White, Asian students report
higher GAI usage, perceive greater academic benefits, and express a stronger
preference for it. Similarly, Black students report a more positive impact of
GAI on their academic performance. Personality traits also play a significant
role in shaping perceptions and usage of GAI. After controlling demographic
factors, we found that personality still significantly predicts GAI use and
attitudes: (a) Students with higher conscientiousness use GAI less. (b)
Students who are higher in agreeableness perceive a less positive impact of GAI
on academic performance and express more ethical concerns about using it for
academic work. (c) Students with higher emotional stability report a more
positive impact of GAI on learning and fewer concerns about its academic use.
(d) Students with higher extraversion show a stronger preference for GAI over
traditional resources. (e) Students with higher intellect/imagination tend to
prefer traditional resources. These insights highlight the need for
universities to provide personalized guidance to ensure students use GAI
effectively, ethically, and equitably in their academic pursuits.

</details>


### [277] [The Cognitive Foundations of Economic Exchange: A Modular Framework Grounded in Behavioral Evidence](https://arxiv.org/abs/2505.02945)
*Egil Diau*

Main category: cs.CY

TL;DR: 该研究提出了一个由个体识别、互惠信任和成本回报敏感性三个认知机制组成的框架，用于在多智能体AI中建模社会合作与信任。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体AI在模拟现实行为约束下的社会合作方面面临挑战，且“信任”等关键概念缺乏可操作的定义和认知基础，限制了其在人工代理中的测试和实现。

Method: 基于灵长类行为、婴儿认知和经济人类学的经验证据，提出一个包含个体识别、互惠信任和成本回报敏感性三个认知上简约机制的概念框架。

Result: 该框架将信任重新定义为一种分级的认知期望，为人工代理中的互惠交换提供了可模拟的基础，并能促成可扩展合作和制度动态的自下而上涌现。

Conclusion: 提出的概念框架为在人工代理中实现和测试基于认知机制的社会合作（尤其是信任和互惠交换）提供了新途径，有助于促进可扩展的合作行为。

Abstract: A key challenge in multi-agent AI is modeling social cooperation under
realistic behavioral constraints. Many foundational concepts in economics and
ethics such as "trust" or "morality" are often defined informally, without
operational criteria or cognitive grounding, which limits their testability and
implementation in artificial agents. Drawing on converging empirical evidence
from primate behavior, infant cognition, and economic anthropology, we propose
a conceptual framework composed of three cognitively minimal mechanisms:
individual recognition, reciprocal credence, and cost return sensitivity. This
framework reframes trust as a graded cognitive expectation, providing a
simulateable basis for reciprocal exchange in artificial agents, and enabling
the bottom-up emergence of scalable cooperation and institutional dynamics.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [278] [Solar Flare Forecast: A Comparative Analysis of Machine Learning Algorithms for Solar Flare Class Prediction](https://arxiv.org/abs/2505.03385)
*Julia Bringewald*

Main category: astro-ph.SR

TL;DR: 该研究评估了随机森林、KNN和XGBoost三种机器学习算法，结合不同级别的主成分分析（PCA）降维，对太阳耀斑（B, C, M, X四类）进行分类预测的性能，发现随机森林和XGBoost在高维数据下表现更优。


<details>
  <summary>Details</summary>
Motivation: 太阳耀斑是太阳系中最强大的爆发事件之一，对空间天气和现代技术设施构成威胁，因此需要准确预测其发生和强度。

Method: 研究使用了包含13个SHARP参数的数据集，评估了随机森林（Random Forest）、K-最近邻（KNN）和极限梯度提升（XGBoost）三种机器学习算法在二元和多类（B, C, M, X）太阳耀斑分类任务中的预测性能。分析中采用了主成分分析（PCA）进行降维（8个主成分捕获95%数据方差，100个主成分捕获97.5%方差），并结合了10折分层交叉验证和网格搜索进行超参数调优。

Result: 研究结果表明，随机森林和XGBoost在所有评估指标上均表现出强大的性能，并且其性能显著受益于更高维度的输入数据（即使用更多主成分时）。

Conclusion: 该研究的见解通过优化降维技术和为天体物理任务选择模型来增强未来研究。这些新知识的整合有助于开发更准确的空间天气预报系统，并加深对太阳物理学的理解。

Abstract: Solar flares are among the most powerful and dynamic events in the solar
system, resulting from the sudden release of magnetic energy stored in the
Sun's atmosphere. These energetic bursts of electromagnetic radiation can
release up to 10^32 erg of energy, impacting space weather and posing risks to
technological infrastructure and therefore require accurate forecasting of
solar flare occurrences and intensities. This study evaluates the predictive
performance of three machine learning algorithms: Random Forest, k-Nearest
Neighbors (KNN), and Extreme Gradient Boosting (XGBoost) for classifying solar
flares into 4 categories (B, C, M, X). Using the dataset of 13 SHARP
parameters, the effectiveness of the models was evaluated in binary and
multiclass classification tasks. The analysis utilized 8 principal components
(PC), capturing 95% of data variance, and 100 PCs, capturing 97.5% of variance.
Our approach uniquely combines binary and multiclass classification with
different levels of dimensionality reduction, an innovative methodology not
previously explored in the context of solar flare prediction. Employing a
10-fold stratified cross-validation and grid search for hyperparameter tuning
ensured robust model evaluation. Our findings indicate that Random Forest and
XGBoost consistently demonstrate strong performance across all metrics,
benefiting significantly from increased dimensionality. The insights of this
study enhance future research by optimizing dimensionality reduction techniques
and informing model selection for astrophysical tasks. By integrating this
newly acquired knowledge into future research, more accurate space weather
forecasting systems can be developed, along with a deeper understanding of
solar physics.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [279] [New affine invariant ensemble samplers and their dimensional scaling](https://arxiv.org/abs/2505.02987)
*Yifan Chen*

Main category: stat.CO

TL;DR: 本文介绍了几种新的仿射不变集成采样器，它们易于构建且优于现有算法，尤其在处理高维问题时表现更佳。


<details>
  <summary>Details</summary>
Motivation: 现有广泛使用的采样算法在处理高维问题和高度偏斜分布时存在性能瓶颈，需要更有效的仿射不变集成采样方法。

Method: 提出了两种新的仿射不变集成采样器：一种是无导数集成侧向移动采样器；另一种是基于导数的仿射不变集成哈密顿蒙特卡洛（HMC）采样器。此外，还对这些采样器在高维高斯目标上的渐近标度特性进行了分析。

Result: 提出的无导数采样器性能优于流行的emcee包中的采样器。基于导数的仿射不变HMC采样器在采样高度偏斜分布时优于不具备仿射不变性的标准HMC。分析表明，基于导数的仿射不变集成HMC在高维情况下比无导数集成采样器具有更好的标度特性。

Conclusion: 本文提出的新型仿射不变集成采样器，无论是无导数还是基于导数的方法，都展现了在特定场景（如高维问题、偏斜分布）下相较于现有方法的优越性。特别是基于导数的方法在维度扩展性上表现更优。

Abstract: We introduce some new affine invariant ensemble samplers that are easy to
construct and improve upon existing widely used algorithms, especially for
high-dimensional problems. Specifically, we propose a derivative-free ensemble
side move sampler that performs favorably compared to popular samplers in the
\texttt{emcee} package. Additionally, we develop a class of derivative-based
ensemble Hamiltonian Monte Carlo (HMC) samplers with affine invariance, which
outperform standard HMC without affine invariance when sampling highly skewed
distributions. We provide asymptotic scaling analysis for high-dimensional
Gaussian targets to further elucidate the properties of these affine invariant
ensemble samplers. In particular, with derivative information, the affine
invariant ensemble HMC can scale much better with dimension compared to
derivative-free ensemble samplers.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [280] [Nonnegative Low-rank Matrix Recovery Can Have Spurious Local Minima](https://arxiv.org/abs/2505.03717)
*Richard Y. Zhang*

Main category: math.OC

TL;DR: 研究带非负约束的低秩矩阵恢复问题。发现在部分观测下，即使RIP常数极小，良性非凸性也不再成立，这与经典理论不同。


<details>
  <summary>Details</summary>
Motivation: 经典的低秩矩阵恢复在限制等距性质（RIP）下具有“良性非凸性”。本研究旨在探究当因子矩阵被约束为元素非负（一个常见的实际需求）时，这种良性非凸性是否仍然成立。

Method: 通过理论分析，特别是在秩为1的非负真实值设定下，对比研究了完全观测（RIP常数 $\delta=0$）和部分观测（RIP常数 $\delta\to0^{+}$，无论秩是否过参数化）两种情况。

Result: 在秩为1非负真实值的完全观测情况下（RIP常数 $\delta=0$），良性非凸性成立。然而，令人惊讶的是，在部分观测情况下，即使RIP常数 $\delta$ 任意小（$\delta\to0^{+}$），无论秩是否过参数化，该性质均不再成立。

Conclusion: 非负约束的存在从根本上破坏了广泛用于解释低秩矩阵恢复经验鲁棒性的连续性论证。这一发现揭示了一个关键的理论空白：一旦施加非负约束，现有理论解释便失效。

Abstract: The classical low-rank matrix recovery problem is well-known to exhibit
\emph{benign nonconvexity} under the restricted isometry property (RIP): local
optimization is guaranteed to converge to the global optimum, where the ground
truth is recovered. We investigate whether benign nonconvexity continues to
hold when the factor matrices are constrained to be elementwise nonnegative --
a common practical requirement. In the simple setting of a rank-1 nonnegative
ground truth, we confirm that benign nonconvexity holds in the fully-observed
case with RIP constant $\delta=0$. Surprisingly, however, this property fails
to extend to the partially-observed case with any arbitrarily small RIP
constant $\delta\to0^{+}$, irrespective of rank overparameterization. This
finding exposes a critical theoretical gap: the continuity argument widely used
to explain the empirical robustness of low-rank matrix recovery fundamentally
breaks down once nonnegative constraints are imposed.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [281] [Snakemaker: Seamlessly transforming ad-hoc analyses into sustainable Snakemake workflows with generative AI](https://arxiv.org/abs/2505.02841)
*Marco Masera,Alessandro Leone,Johannes Köster,Ivan Molineris*

Main category: cs.SE

TL;DR: Snakemaker是一款利用生成式AI的工具，可将非结构化代码和Ipython Notebooks转换为高质量、可持续的Snakemake工作流，以提高生物信息学研究的可重复性。


<details>
  <summary>Details</summary>
Motivation: 生物信息学软件开发中，可重复性和可持续性是重大挑战，现有工具和工作流往往生命周期短或难以适应。

Method: 引入Snakemaker工具，它利用生成式AI，通过非侵入式跟踪用户在终端执行的工作、分析执行模式，将非结构化代码或Ipython Notebooks转换为遵循最佳实践（如Conda环境跟踪、通用规则生成）的模块化Snakemake工作流。同时集成聊天助手，支持自然语言指令进行微调控制。

Result: Snakemaker能够生成高质量、遵循最佳实践（包括Conda环境跟踪、通用规则生成和循环展开）的Snakemake工作流，并能将整体式Ipython Notebooks转换为模块化流程。

Conclusion: Snakemaker通过降低原型代码与生产级代码之间的门槛，解决了生物信息学研究中计算可重复性的一个关键空白，提高了研究的可持续性和可重复性。

Abstract: Reproducibility and sustainability present significant challenges in
bioinformatics software development, where rapidly evolving tools and complex
workflows often result in short-lived or difficult-to-adapt pipelines. This
paper introduces Snakemaker, a tool that leverages generative AI to facilitate
researchers build sustainable data analysis pipelines by converting
unstructured code into well-defined Snakemake workflows. Snakemaker
non-invasively tracks the work performed in the terminal by the researcher,
analyzes execution patterns, and generates Snakemake workflows that can be
integrated into existing pipelines. Snakemaker also supports the transformation
of monolithic Ipython Notebooks into modular Snakemake pipelines, resolving the
global state of the notebook into discrete, file-based interactions between
rules. An integrated chat assistant provides users with fine-grained control
through natural language instructions. Snakemaker generates high-quality
Snakemake workflows by adhering to the best practices, including Conda
environment tracking, generic rule generation and loop unrolling. By lowering
the barrier between prototype and production-quality code, Snakemaker addresses
a critical gap in computational reproducibility for bioinformatics research.

</details>


### [282] [The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models](https://arxiv.org/abs/2505.02931)
*Fernando Vallecillos Ruiz,Max Hort,Leon Moonen*

Main category: cs.SE

TL;DR: 该研究探讨了一种平衡的自动程序修复 (APR) 策略，结合了多输出生成和多轮迭代优化，在有限补丁预算下，通过微调显著提升了LLM的修复效果，并强调了迭代策略尤其在复杂基准测试中的优势。


<details>
  <summary>Details</summary>
Motivation: 以往的自动程序修复方法要么侧重于生成大量补丁，要么侧重于多轮迭代，而忽略了两者之间的平衡。本研究旨在探索一种平衡这两种方法（多输出生成和多轮迭代）的APR流程，特别是在每个缺陷的补丁总数限制为10个的情况下。

Method: 研究者设计了一个APR流程，对每个错误限制生成最多10个补丁。采用了三种先进的指令调优大语言模型（DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct），并在三种不同大小（1K, 30K, 65K）的APR数据集上使用全量微调和LoRA两种技术对这些模型进行微调。最后，在HumanEval-Java和Defects4J这两个APR基准上评估了它们的修复能力。

Result: 研究发现，仅使用一小部分微调数据（<1%）就能使生成的貌似正确补丁数量提高达78%，挑战了先前认为全量微调效果有限的研究。然而，超过特定阈值会导致效果递减（可能由于过拟合）。此外，基础模型通过迭代方式生成补丁比一次性生成所有补丁获益更大，且迭代策略的优势在复杂基准测试中更为显著。即使是微调后的模型，尤其是在复杂基准上，也能从迭代中获益。

Conclusion: 该研究强调了开发平衡的APR策略的必要性，这种策略应结合多输出生成和迭代优化。研究结果表明，平衡策略能显著提升修复效果，并揭示了微调、迭代次数以及基准测试复杂度之间的相互作用关系。

Abstract: Automatic program repair (APR) aims to reduce the manual efforts required to
identify and fix errors in source code. Before the rise of LLM-based agents, a
common strategy was to increase the number of generated patches, sometimes to
the thousands, to achieve better repair results on benchmarks. More recently,
self-iterative capabilities enabled LLMs to refine patches over multiple rounds
guided by feedback. However, literature often focuses on many iterations and
disregards different numbers of outputs.
  We investigate an APR pipeline that balances these two approaches, the
generation of multiple outputs and multiple rounds of iteration, while imposing
a limit of 10 total patches per bug. We apply three SOTA instruction-tuned LLMs
- DeepSeekCoder-Instruct, Codellama-Instruct, Llama3.1-Instruct - to the APR
task. We further fine-tune each model on an APR dataset with three sizes (1K,
30K, 65K) and two techniques (Full Fine-Tuning and LoRA), allowing us to assess
their repair capabilities on two APR benchmarks: HumanEval-Java and Defects4J.
  Our results show that by using only a fraction (<1%) of the fine-tuning
dataset, we can achieve improvements of up to 78% in the number of plausible
patches generated, challenging prior studies that reported limited gains using
Full Fine-Tuning. However, we find that exceeding certain thresholds leads to
diminishing outcomes, likely due to overfitting. Moreover, we show that base
models greatly benefit from creating patches in an iterative fashion rather
than generating them all at once. In addition, the benefit of iterative
strategies becomes more pronounced in complex benchmarks. Even fine-tuned
models, while benefiting less from iterations, still gain advantages,
particularly on complex benchmarks. The research underscores the need for
balanced APR strategies that combine multi-output generation and iterative
refinement.

</details>


### [283] [DocSpiral: A Platform for Integrated Assistive Document Annotation through Human-in-the-Spiral](https://arxiv.org/abs/2505.03214)
*Qiang Sun,Sirui Li,Tingting Bi,Du Huynh,Mark Reynolds,Yuanyi Luo,Wei Liu*

Main category: cs.SE

TL;DR: DocSpiral是一个人机协同的辅助文档标注平台，通过迭代训练模型，减少人工标注时间，提高从图像文档中提取结构化信息的效率。


<details>
  <summary>Details</summary>
Motivation: 从特定领域的图像型文档中获取结构化数据至关重要但具有挑战性，且需要大量人工标注来训练自动化提取系统。

Method: 提出DocSpiral平台，采用“人在环路中”（Human-in-the-Spiral）的迭代设计：人工标注训练模型，模型逐步减少人工干预。平台集成了文档格式规范化、标注界面、评估指标仪表板和API端点。

Result: 实验表明，该框架至少减少了41%的标注时间，并且在模型训练的三次迭代中表现出持续的性能提升。

Conclusion: 通过免费提供该标注平台，旨在降低文档处理领域AI/ML模型开发的门槛，促进大型语言模型在地球科学和医疗保健等图像型、文档密集型领域的应用。

Abstract: Acquiring structured data from domain-specific, image-based documents such as
scanned reports is crucial for many downstream tasks but remains challenging
due to document variability. Many of these documents exist as images rather
than as machine-readable text, which requires human annotation to train
automated extraction systems. We present DocSpiral, the first
Human-in-the-Spiral assistive document annotation platform, designed to address
the challenge of extracting structured information from domain-specific,
image-based document collections. Our spiral design establishes an iterative
cycle in which human annotations train models that progressively require less
manual intervention. DocSpiral integrates document format normalization,
comprehensive annotation interfaces, evaluation metrics dashboard, and API
endpoints for the development of AI / ML models into a unified workflow.
Experiments demonstrate that our framework reduces annotation time by at least
41\% while showing consistent performance gains across three iterations during
model training. By making this annotation platform freely accessible, we aim to
lower barriers to AI/ML models development in document processing, facilitating
the adoption of large language models in image-based, document-intensive fields
such as geoscience and healthcare. The system is freely available at:
https://app.ai4wa.com. The demonstration video is available:
https://app.ai4wa.com/docs/docspiral/demo.

</details>


### [284] [Synthline: A Product Line Approach for Synthetic Requirements Engineering Data Generation using Large Language Models](https://arxiv.org/abs/2505.03265)
*Abdelkarim El-Hajjami,Camille Salinesi*

Main category: cs.SE

TL;DR: 该论文介绍了Synthline，一种利用大型语言模型生成合成需求工程（RE）数据以解决数据集稀缺性的方法，并证明了结合真实数据和合成数据可以显著提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 现代需求工程（RE）严重依赖自然语言处理和机器学习技术，但高质量数据集的缺乏限制了这些技术的有效性。

Method: 提出Synthline，一种产品线（PL）方法，利用大型语言模型（LLMs）系统地为基于分类的用例生成合成RE数据。通过在识别需求规范缺陷的机器学习应用场景下进行实证评估，研究生成数据的多样性及其训练下游模型的效用。

Result: 合成数据集的多样性虽低于真实数据，但足以作为可行的训练资源。更重要的是，将合成数据与真实数据结合使用，可使模型性能得到显著提升，与仅使用真实数据训练的模型相比，精确度最高可提高85%，召回率可提高2倍。

Conclusion: 基于产品线的合成数据生成方法具有解决需求工程领域数据稀缺问题的潜力。

Abstract: While modern Requirements Engineering (RE) heavily relies on natural language
processing and Machine Learning (ML) techniques, their effectiveness is limited
by the scarcity of high-quality datasets. This paper introduces Synthline, a
Product Line (PL) approach that leverages Large Language Models to
systematically generate synthetic RE data for classification-based use cases.
Through an empirical evaluation conducted in the context of using ML for the
identification of requirements specification defects, we investigated both the
diversity of the generated data and its utility for training downstream models.
Our analysis reveals that while synthetic datasets exhibit less diversity than
real data, they are good enough to serve as viable training resources.
Moreover, our evaluation shows that combining synthetic and real data leads to
substantial performance improvements. Specifically, hybrid approaches achieve
up to 85% improvement in precision and a 2x increase in recall compared to
models trained exclusively on real data. These findings demonstrate the
potential of PL-based synthetic data generation to address data scarcity in RE.
We make both our implementation and generated datasets publicly available to
support reproducibility and advancement in the field.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [285] [Neural Orchestration for Multi-Agent Systems: A Deep Learning Framework for Optimal Agent Selection in Multi-Domain Task Environments](https://arxiv.org/abs/2505.02861)
*Kushagra Agrawal,Nisharg Nargund*

Main category: cs.MA

TL;DR: 提出MetaOrch，一个用于在多领域任务环境中优化智能体选择的神经编排框架，旨在提升多智能体系统（MAS）的适应性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体系统（MAS）的协调机制僵化，难以适应动态变化的任务需求。

Method: 采用监督学习方法，构建MetaOrch框架。该框架基于任务上下文、智能体历史和预期响应质量来选择最合适的智能体，并引入一个新颖的模糊评估模块（从完整性、相关性和置信度维度评估智能体响应）为编排器生成软监督标签进行训练。

Result: 在异构智能体的模拟环境中，MetaOrch实现了86.3%的智能体选择准确率，显著优于随机选择和轮询调度等基线策略。

Conclusion: 神经编排为增强多智能体系统在不同任务域中的自主性、可解释性和适应性提供了一种有效途径，MetaOrch的模块化架构也保证了系统的可扩展性。

Abstract: Multi-agent systems (MAS) are foundational in simulating complex real-world
scenarios involving autonomous, interacting entities. However, traditional MAS
architectures often suffer from rigid coordination mechanisms and difficulty
adapting to dynamic tasks. We propose MetaOrch, a neural orchestration
framework for optimal agent selection in multi-domain task environments. Our
system implements a supervised learning approach that models task context,
agent histories, and expected response quality to select the most appropriate
agent for each task. A novel fuzzy evaluation module scores agent responses
along completeness, relevance, and confidence dimensions, generating soft
supervision labels for training the orchestrator. Unlike previous methods that
hard-code agent-task mappings, MetaOrch dynamically predicts the most suitable
agent while estimating selection confidence. Experiments in simulated
environments with heterogeneous agents demonstrate that our approach achieves
86.3% selection accuracy, significantly outperforming baseline strategies
including random selection and round-robin scheduling. The modular architecture
emphasizes extensibility, allowing agents to be registered, updated, and
queried independently. Results suggest that neural orchestration offers a
powerful approach to enhancing the autonomy, interpretability, and adaptability
of multi-agent systems across diverse task domains.

</details>


### [286] [Assessing and Enhancing the Robustness of LLM-based Multi-Agent Systems Through Chaos Engineering](https://arxiv.org/abs/2505.03096)
*Joshua Owotogbe*

Main category: cs.MA

TL;DR: 本研究探讨了应用混沌工程来增强大型语言模型多智能体系统 (LLM-MAS) 在真实生产环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLM-MAS在生产环境中易受幻觉、代理失效和通信故障等新兴错误的影响，这些漏洞需要被主动识别和解决以确保其可靠性。

Method: 提出一个混沌工程框架，用于主动识别LLM-MAS中的此类漏洞，评估并建立对其的韧性。

Result: 该框架旨在主动识别LLM-MAS中的漏洞，评估并增强其韧性，从而确保在关键应用中的可靠性能。

Conclusion: 通过所提出的混沌工程框架，可以主动识别LLM-MAS的潜在漏洞并提升其韧性，确保系统在关键应用中的可靠运行。

Abstract: This study explores the application of chaos engineering to enhance the
robustness of Large Language Model-Based Multi-Agent Systems (LLM-MAS) in
production-like environments under real-world conditions. LLM-MAS can
potentially improve a wide range of tasks, from answering questions and
generating content to automating customer support and improving decision-making
processes. However, LLM-MAS in production or preproduction environments can be
vulnerable to emergent errors or disruptions, such as hallucinations, agent
failures, and agent communication failures. This study proposes a chaos
engineering framework to proactively identify such vulnerabilities in LLM-MAS,
assess and build resilience against them, and ensure reliable performance in
critical applications.

</details>


### [287] [Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Delayed Observation](https://arxiv.org/abs/2505.03586)
*Songchen Fu,Siang Chen,Shaojing Zhao,Letian Bai,Ta Li,Yonghong Yan*

Main category: cs.MA

TL;DR: 论文提出DSID-POMDP模型和RDC训练框架，以解决多智能体强化学习中普遍存在的观测延迟问题，实验证明RDC能有效缓解性能下降，甚至达到无延迟水平。


<details>
  <summary>Details</summary>
Motivation: 真实世界多智能体系统中普遍存在观测延迟，导致智能体无法基于环境真实状态决策。各观测分量具有不同延迟特性，给多智能体强化学习带来巨大挑战。

Method: 首先通过扩展标准Dec-POMDP，形式化定义了去中心化随机个体延迟部分可观测马尔可夫决策过程 (DSID-POMDP)。然后提出了彩虹延迟补偿 (RDC) 框架，一个用于解决随机个体延迟的多智能体强化学习训练框架，并在MPE和SMAC等标准MARL基准上进行了实验验证。

Result: 基线多智能体强化学习方法在固定和非固定延迟下性能严重下降。而使用RDC增强的方法缓解了此问题，在某些延迟场景下显著达到了理想的无延迟性能，同时保持了泛化能力。

Conclusion: 该工作为多智能体延迟观测问题提供了新的视角，并提供了一个有效的解决方案框架。

Abstract: In real-world multi-agent systems (MASs), observation delays are ubiquitous,
preventing agents from making decisions based on the environment's true state.
An individual agent's local observation often consists of multiple components
from other agents or dynamic entities in the environment. These discrete
observation components with varying delay characteristics pose significant
challenges for multi-agent reinforcement learning (MARL). In this paper, we
first formulate the decentralized stochastic individual delay partially
observable Markov decision process (DSID-POMDP) by extending the standard
Dec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL
training framework for addressing stochastic individual delays, along with
recommended implementations for its constituent modules. We implement the
DSID-POMDP's observation generation pattern using standard MARL benchmarks,
including MPE and SMAC. Experiments demonstrate that baseline MARL methods
suffer severe performance degradation under fixed and unfixed delays. The
RDC-enhanced approach mitigates this issue, remarkably achieving ideal
delay-free performance in certain delay scenarios while maintaining
generalization capability. Our work provides a novel perspective on multi-agent
delayed observation problems and offers an effective solution framework.

</details>
