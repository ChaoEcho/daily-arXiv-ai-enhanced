{"id": "2509.04625", "pdf": "https://arxiv.org/pdf/2509.04625", "abs": "https://arxiv.org/abs/2509.04625", "authors": ["Zhenzhou Qi", "Chung-Hsuan Tung", "Zhihui Gao", "Tingjun Chen"], "title": "NEXUS: Efficient and Scalable Multi-Cell mmWave Baseband Processing with Heterogeneous Compute", "categories": ["cs.NI"], "comment": null, "summary": "The rapid adoption of 5G New Radio (NR), particularly in the millimeter-wave\n(mmWave) spectrum, imposes stringent demands on the flexibility, scalability,\nand efficiency of baseband processing. While virtualized Radio Access Networks\n(vRANs) enable dynamic spectrum sharing across cells, compute resource\nallocation for baseband processing, especially in multi-cell deployments with\nheterogeneous workloads, remains underexplored. In this paper, we present\nNEXUS, the first system to realize real-time, virtualized multi-cell mmWave\nbaseband processing on a single server with heterogeneous compute resources.\nNEXUS integrates software-based digital signal processing pipelines with\nhardware-accelerated LDPC decoding, and introduces a novel framework for\nsharing Intel's ACC100 eASIC across multiple CPU cores via virtual functions\n(VFs). For single-cell operation, NEXUS employs a random forest (RAF)-based\nmodel that predicts the most energy-efficient resource allocation for the given\ncell configuration with microsecond-level inference latency and high accuracy.\nFor multi-cell scenarios, NEXUS introduces a power-aware scheduler that\nincorporates a lightweight contention model to adjust resource allocation\nstrategies under concurrent execution. Through extensive evaluation across\nvarious Frequency Range 2 (FR2) cell configurations, we show that NEXUS\nsupports up to 16 concurrent cells under full load, achieving 5.37Gbps\naggregate throughput, while reducing the multi-cell scheduling search space by\norders of magnitude. These results demonstrate that virtualized, resource-aware\nbaseband processing is both practical and efficient for next-generation vRAN\nsystems.", "AI": {"tldr": "NEXUS\u662f\u9996\u4e2a\u5728\u5355\u670d\u52a1\u5668\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u3001\u865a\u62df\u5316\u591a\u5c0f\u533a\u6beb\u7c73\u6ce2\u57fa\u5e26\u5904\u7406\u7684\u7cfb\u7edf\uff0c\u5b83\u901a\u8fc7\u5f02\u6784\u8ba1\u7b97\u8d44\u6e90\u548c\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u541e\u5410\u91cf\u548c\u8d44\u6e90\u6548\u7387\u3002", "motivation": "5G NR\uff08\u5c24\u5176\u662f\u6beb\u7c73\u6ce2\u9891\u6bb5\uff09\u5bf9\u57fa\u5e26\u5904\u7406\u7684\u7075\u6d3b\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u63d0\u51fa\u4e86\u4e25\u683c\u8981\u6c42\u3002\u5c3d\u7ba1vRAN\u652f\u6301\u52a8\u6001\u9891\u8c31\u5171\u4eab\uff0c\u4f46\u5728\u591a\u5c0f\u533a\u3001\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u573a\u666f\u4e0b\uff0c\u57fa\u5e26\u5904\u7406\u7684\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86NEXUS\u7cfb\u7edf\u3002\u5b83\u5c06\u8f6f\u4ef6\u5b9a\u4e49\u4fe1\u53f7\u5904\u7406\u4e0e\u786c\u4ef6\u52a0\u901f\uff08LDPC\u89e3\u7801\uff09\u76f8\u7ed3\u5408\uff0c\u5e76\u5f15\u5165\u4e86\u901a\u8fc7\u865a\u62df\u529f\u80fd\uff08VF\uff09\u5728\u591a\u4e2aCPU\u6838\u5fc3\u95f4\u5171\u4eabIntel ACC100 eASIC\u7684\u65b0\u6846\u67b6\u3002\u5bf9\u4e8e\u5355\u5c0f\u533a\u64cd\u4f5c\uff0cNEXUS\u4f7f\u7528\u57fa\u4e8e\u968f\u673a\u68ee\u6797\uff08RAF\uff09\u7684\u6a21\u578b\u9884\u6d4b\u6700\u8282\u80fd\u7684\u8d44\u6e90\u5206\u914d\u3002\u5bf9\u4e8e\u591a\u5c0f\u533a\u573a\u666f\uff0cNEXUS\u5f15\u5165\u4e86\u4e00\u4e2a\u529f\u8017\u611f\u77e5\u8c03\u5ea6\u5668\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u7ade\u4e89\u6a21\u578b\u6765\u8c03\u6574\u5e76\u53d1\u6267\u884c\u4e0b\u7684\u8d44\u6e90\u5206\u914d\u7b56\u7565\u3002", "result": "NEXUS\u5728\u5404\u79cdFR2\u5c0f\u533a\u914d\u7f6e\u4e0b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u5b83\u5728\u6ee1\u8d1f\u8377\u4e0b\u652f\u6301\u591a\u8fbe16\u4e2a\u5e76\u53d1\u5c0f\u533a\uff0c\u5b9e\u73b0\u4e865.37Gbps\u7684\u603b\u541e\u5410\u91cf\uff0c\u5e76\u80fd\u5c06\u591a\u5c0f\u533a\u8c03\u5ea6\u7684\u641c\u7d22\u7a7a\u95f4\u51cf\u5c11\u6570\u4e2a\u6570\u91cf\u7ea7\u3002\u5355\u5c0f\u533a\u6a21\u578b\u5b9e\u73b0\u4e86\u5fae\u79d2\u7ea7\u7684\u63a8\u7406\u5ef6\u8fdf\u548c\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u865a\u62df\u5316\u3001\u8d44\u6e90\u611f\u77e5\u7684\u57fa\u5e26\u5904\u7406\u5bf9\u4e8e\u4e0b\u4e00\u4ee3vRAN\u7cfb\u7edf\u662f\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u3002"}}
{"id": "2509.04695", "pdf": "https://arxiv.org/pdf/2509.04695", "abs": "https://arxiv.org/abs/2509.04695", "authors": ["Lars Herschbach", "Damien Rossi", "Sina Keshvadi"], "title": "Path Dynamics in a Deployed Path-Aware Network: A Measurement Study of SCIONLab", "categories": ["cs.NI", "C.2.1; C.2"], "comment": "19 pages, 8 figures. Submitted to the Computer Communications journal", "summary": "Path-aware networks promise enhanced performance and resilience through\nmultipath transport, but a lack of empirical data on their real-world dynamics\nhinders the design of effective protocols. This paper presents a longitudinal\nmeasurement study of the SCION architecture on the global SCIONLab testbed,\ncharacterizing the path stability, diversity, and performance crucial for\nprotocols like Multipath QUIC (MPQUIC). Our measurements reveal a dynamic\nenvironment, with significant control-plane churn and short path lifetimes in\nparts of the testbed. We identify and characterize path discrepancy, a\nphenomenon where routing policies create asymmetric path availability between\nendpoints. Furthermore, we observe a performance trade-off where concurrent\nmultipath transmissions can improve aggregate throughput but may degrade the\nlatency and reliability of individual paths. These findings demonstrate that\nprotocols such as MPQUIC should explicitly account for high churn and path\nasymmetry, challenging common assumptions in multipath protocol design.", "AI": {"tldr": "\u5bf9SCIONLab\u6d4b\u8bd5\u5e8a\u7684\u957f\u671f\u6d4b\u91cf\u7814\u7a76\u63ed\u793a\u4e86\u8def\u5f84\u611f\u77e5\u7f51\u7edc\u4e2d\u8def\u5f84\u7684\u52a8\u6001\u6027\u3001\u9ad8\u6d41\u5931\u7387\u3001\u4e0d\u5bf9\u79f0\u6027\u53ca\u6027\u80fd\u6743\u8861\uff0c\u6311\u6218\u4e86\u591a\u8def\u5f84\u534f\u8bae\u8bbe\u8ba1\u7684\u73b0\u6709\u5047\u8bbe\u3002", "motivation": "\u8def\u5f84\u611f\u77e5\u7f51\u7edc\u6709\u671b\u901a\u8fc7\u591a\u8def\u5f84\u4f20\u8f93\u63d0\u5347\u6027\u80fd\u548c\u5f39\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u7684\u5b9e\u8bc1\u6570\u636e\uff0c\u963b\u788d\u4e86\u6709\u6548\u534f\u8bae\uff08\u5982Multipath QUIC\uff09\u7684\u8bbe\u8ba1\u3002", "method": "\u672c\u6587\u5bf9\u5168\u7403SCIONLab\u6d4b\u8bd5\u5e8a\u4e0a\u7684SCION\u67b6\u6784\u8fdb\u884c\u4e86\u7eb5\u5411\u6d4b\u91cf\u7814\u7a76\uff0c\u4ee5\u8868\u5f81\u5bf9\u591a\u8def\u5f84\u534f\u8bae\u81f3\u5173\u91cd\u8981\u7684\u8def\u5f84\u7a33\u5b9a\u6027\u3001\u591a\u6837\u6027\u548c\u6027\u80fd\u3002", "result": "\u6d4b\u91cf\u63ed\u793a\u4e86\u52a8\u6001\u73af\u5883\uff0c\u90e8\u5206\u6d4b\u8bd5\u5e8a\u5b58\u5728\u663e\u8457\u7684\u63a7\u5236\u5e73\u9762\u6d41\u5931\u548c\u77ed\u6682\u7684\u8def\u5f84\u751f\u547d\u5468\u671f\u3002\u7814\u7a76\u8bc6\u522b\u5e76\u8868\u5f81\u4e86\u8def\u5f84\u5dee\u5f02\uff0c\u5373\u8def\u7531\u7b56\u7565\u5bfc\u81f4\u7aef\u70b9\u95f4\u8def\u5f84\u53ef\u7528\u6027\u4e0d\u5bf9\u79f0\u3002\u6b64\u5916\uff0c\u89c2\u5bdf\u5230\u4e00\u79cd\u6027\u80fd\u6743\u8861\uff1a\u5e76\u53d1\u591a\u8def\u5f84\u4f20\u8f93\u53ef\u63d0\u9ad8\u603b\u541e\u5410\u91cf\uff0c\u4f46\u53ef\u80fd\u964d\u4f4e\u5355\u4e2a\u8def\u5f84\u7684\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u5982Multipath QUIC\u7b49\u534f\u8bae\u5728\u8bbe\u8ba1\u65f6\u5e94\u660e\u786e\u8003\u8651\u9ad8\u6d41\u5931\u7387\u548c\u8def\u5f84\u4e0d\u5bf9\u79f0\u6027\uff0c\u8fd9\u6311\u6218\u4e86\u591a\u8def\u5f84\u534f\u8bae\u8bbe\u8ba1\u7684\u5e38\u89c1\u5047\u8bbe\u3002"}}
{"id": "2509.04792", "pdf": "https://arxiv.org/pdf/2509.04792", "abs": "https://arxiv.org/abs/2509.04792", "authors": ["Erik Rye", "Dave Levin", "Robert Beverly"], "title": "Where Have All the Firewalls Gone? Security Consequences of Residential IPv6 Transition", "categories": ["cs.NI", "cs.CR"], "comment": null, "summary": "IPv4 NAT has limited the spread of IoT botnets considerably by\ndefault-denying bots' incoming connection requests to in-home devices unless\nthe owner has explicitly allowed them. As the Internet transitions to majority\nIPv6, however, residential connections no longer require the use of NAT. This\npaper therefore asks: has the transition from IPv4 to IPv6 ultimately made\nresidential networks more vulnerable to attack, thereby empowering the next\ngeneration of IPv6-based IoT botnets? To answer this question, we introduce a\nlarge-scale IPv6 scanning methodology that, unlike those that rely on AI, can\nbe run on low-resource devices common in IoT botnets. We use this methodology\nto perform the largest-scale measurement of IPv6 residential networks to date,\nand compare which devices are publicly accessible to comparable IPv4 networks.\nWe were able to receive responses from 14.0M distinct IPv6 addresses inside of\nresidential networks (i.e., not the external-facing gateway), in 2,436 ASes\nacross 118 countries. These responses come from protocols commonly exploited by\nIoT botnets (including telnet and FTP), as well as protocols typically\nassociated with end-user devices (including iPhone-Sync and IPP). Comparing to\nIPv4, we show that we are able to reach more printers, iPhones, and smart\nlights over IPv6 than full IPv4-wide scans could. Collectively, our results\nshow that NAT has indeed acted as the de facto firewall of the Internet, and\nthe v4-to-v6 transition of residential networks is opening up new devices to\nattack.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u968f\u7740\u4f4f\u5b85\u7f51\u7edc\u4eceIPv4\u8f6c\u5411IPv6\u5e76\u79fb\u9664NAT\uff0c\u4e4b\u524d\u4f5c\u4e3a\u4e8b\u5b9e\u9632\u706b\u5899\u7684NAT\u529f\u80fd\u6d88\u5931\uff0c\u5bfc\u81f4\u66f4\u591a\u5bb6\u5ead\u8bbe\u5907\uff08\u5982\u6253\u5370\u673a\u3001iPhone\u3001\u667a\u80fd\u706f\uff09\u76f4\u63a5\u66b4\u9732\u5728\u4e92\u8054\u7f51\u4e0a\uff0c\u5927\u5927\u589e\u52a0\u4e86\u4e0b\u4e00\u4ee3IPv6\u7269\u8054\u7f51\u50f5\u5c38\u7f51\u7edc\u7684\u653b\u51fb\u9762\u3002", "motivation": "\u63a2\u7a76\u4eceIPv4\u5230IPv6\u7684\u8fc7\u6e21\u662f\u5426\u4f1a\u589e\u52a0\u4f4f\u5b85\u7f51\u7edc\u88ab\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u5e76\u52a9\u957f\u4e0b\u4e00\u4ee3\u57fa\u4e8eIPv6\u7684\u7269\u8054\u7f51\u50f5\u5c38\u7f51\u7edc\uff0c\u56e0\u4e3aIPv4\u7684NAT\u9ed8\u8ba4\u62d2\u7edd\u5165\u7ad9\u8fde\u63a5\u7684\u80fd\u529b\u5c06\u4e0d\u590d\u5b58\u5728\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u53ef\u5728\u4f4e\u8d44\u6e90\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u5927\u89c4\u6a21IPv6\u626b\u63cf\u65b9\u6cd5\uff0c\u5e76\u7528\u6b64\u65b9\u6cd5\u5bf9\u4f4f\u5b85IPv6\u7f51\u7edc\u8fdb\u884c\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u89c4\u6a21\u7684\u6d4b\u91cf\uff0c\u6bd4\u8f83\u4e86\u4e0e\u53ef\u6bd4IPv4\u7f51\u7edc\u76f8\u6bd4\uff0c\u54ea\u4e9b\u8bbe\u5907\u662f\u516c\u5f00\u53ef\u8bbf\u95ee\u7684\u3002", "result": "\u4ece118\u4e2a\u56fd\u5bb6\u76842,436\u4e2aAS\u4e2d\u76841400\u4e07\u4e2a\u72ec\u7acb\u7684\u4f4f\u5b85IPv6\u5730\u5740\uff08\u975e\u5916\u90e8\u7f51\u5173\uff09\u6536\u5230\u4e86\u54cd\u5e94\u3002\u8fd9\u4e9b\u54cd\u5e94\u6765\u81ea\u7269\u8054\u7f51\u50f5\u5c38\u7f51\u7edc\u5e38\u5229\u7528\u7684\u534f\u8bae\u4ee5\u53ca\u7ec8\u7aef\u7528\u6237\u8bbe\u5907\u534f\u8bae\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e0eIPv4\u5168\u7f51\u626b\u63cf\u76f8\u6bd4\uff0c\u901a\u8fc7IPv6\u53ef\u4ee5\u8bbf\u95ee\u5230\u66f4\u591a\u7684\u6253\u5370\u673a\u3001iPhone\u548c\u667a\u80fd\u706f\u3002", "conclusion": "NAT\u786e\u5b9e\u5145\u5f53\u4e86\u4e92\u8054\u7f51\u7684\u5b9e\u9645\u9632\u706b\u5899\u3002\u4f4f\u5b85\u7f51\u7edc\u4eceIPv4\u5411IPv6\u7684\u8fc7\u6e21\u6b63\u5728\u4f7f\u65b0\u8bbe\u5907\u66b4\u9732\u4e8e\u653b\u51fb\u4e4b\u4e0b\uff0c\u4e3a\u672a\u6765\u7684\u7269\u8054\u7f51\u50f5\u5c38\u7f51\u7edc\u63d0\u4f9b\u4e86\u66f4\u591a\u76ee\u6807\u3002"}}
{"id": "2509.04455", "pdf": "https://arxiv.org/pdf/2509.04455", "abs": "https://arxiv.org/abs/2509.04455", "authors": ["Shisong Chen", "Qian Zhu", "Wenyan Yang", "Chengyi Yang", "Zhong Wang", "Ping Wang", "Xuan Lin", "Bo Xu", "Daqian Li", "Chao Yuan", "Licai Qi", "Wanqing Xu", "sun zhenxing", "Xin Lu", "Shiqiang Xiong", "Chao Chen", "Haixiang Hu", "Yanghua Xiao"], "title": "INSEva: A Comprehensive Chinese Benchmark for Large Language Models in Insurance", "categories": ["cs.CL"], "comment": "Under review", "summary": "Insurance, as a critical component of the global financial system, demands\nhigh standards of accuracy and reliability in AI applications. While existing\nbenchmarks evaluate AI capabilities across various domains, they often fail to\ncapture the unique characteristics and requirements of the insurance domain. To\naddress this gap, we present INSEva, a comprehensive Chinese benchmark\nspecifically designed for evaluating AI systems' knowledge and capabilities in\ninsurance. INSEva features a multi-dimensional evaluation taxonomy covering\nbusiness areas, task formats, difficulty levels, and cognitive-knowledge\ndimension, comprising 38,704 high-quality evaluation examples sourced from\nauthoritative materials. Our benchmark implements tailored evaluation methods\nfor assessing both faithfulness and completeness in open-ended responses.\nThrough extensive evaluation of 8 state-of-the-art Large Language Models\n(LLMs), we identify significant performance variations across different\ndimensions. While general LLMs demonstrate basic insurance domain competency\nwith average scores above 80, substantial gaps remain in handling complex,\nreal-world insurance scenarios. The benchmark will be public soon.", "AI": {"tldr": "INSEva\u662f\u4e00\u4e2a\u4e13\u4e3a\u8bc4\u4f30AI\u7cfb\u7edf\u5728\u4fdd\u9669\u9886\u57df\u77e5\u8bc6\u548c\u80fd\u529b\u800c\u8bbe\u8ba1\u7684\u7efc\u5408\u6027\u4e2d\u6587\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLM\u5728\u8be5\u9886\u57df\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4fdd\u9669\u4f5c\u4e3a\u5168\u7403\u91d1\u878d\u7cfb\u7edf\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5bf9AI\u5e94\u7528\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u8981\u6c42\u6781\u9ad8\u3002\u7136\u800c\uff0c\u73b0\u6709AI\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u5145\u5206\u6355\u6349\u4fdd\u9669\u9886\u57df\u7684\u72ec\u7279\u6027\u548c\u7279\u5b9a\u8981\u6c42\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86INSEva\uff0c\u4e00\u4e2a\u591a\u7ef4\u5ea6\uff08\u6db5\u76d6\u4e1a\u52a1\u9886\u57df\u3001\u4efb\u52a1\u683c\u5f0f\u3001\u96be\u5ea6\u7ea7\u522b\u548c\u8ba4\u77e5\u77e5\u8bc6\uff09\u7684\u4e2d\u6587\u4fdd\u9669\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b38,704\u4e2a\u9ad8\u8d28\u91cf\u8bc4\u4f30\u793a\u4f8b\uff0c\u5e76\u91c7\u7528\u5b9a\u5236\u5316\u65b9\u6cd5\u8bc4\u4f30\u5f00\u653e\u5f0f\u56de\u7b54\u7684\u5fe0\u5b9e\u6027\u548c\u5b8c\u6574\u6027\u3002\u4f7f\u7528INSEva\u8bc4\u4f30\u4e868\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u7ef4\u5ea6\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\u3002\u901a\u7528LLM\u5c55\u73b0\u4e86\u57fa\u7840\u7684\u4fdd\u9669\u9886\u57df\u80fd\u529b\uff08\u5e73\u5747\u5f97\u5206\u8d85\u8fc780\u5206\uff09\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u3001\u771f\u5b9e\u4e16\u754c\u7684\u4fdd\u9669\u573a\u666f\u65f6\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "INSEva\u63ed\u793a\u4e86\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u4fdd\u9669\u573a\u666f\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u4e3a\u672a\u6765AI\u5728\u4fdd\u9669\u9886\u57df\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u660e\u786e\u65b9\u5411\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u5c06\u5f88\u5feb\u516c\u5f00\u3002"}}
{"id": "2509.04490", "pdf": "https://arxiv.org/pdf/2509.04490", "abs": "https://arxiv.org/abs/2509.04490", "authors": ["Abel van Elburg", "Konstantinos Gkentsidis", "Mathieu Sarrazin", "Sarah Barendswaard", "Varun Kotian", "Riender Happee"], "title": "Facial Emotion Recognition does not detect feeling unsafe in automated driving", "categories": ["cs.CV", "J.4; I.2.10"], "comment": null, "summary": "Trust and perceived safety play a crucial role in the public acceptance of\nautomated vehicles. To understand perceived risk, an experiment was conducted\nusing a driving simulator under two automated driving styles and optionally\nintroducing a crossing pedestrian. Data was collected from 32 participants,\nconsisting of continuous subjective comfort ratings, motion, webcam footage for\nfacial expression, skin conductance, heart rate, and eye tracking. The\ncontinuous subjective perceived risk ratings showed significant discomfort\nassociated with perceived risk during cornering and braking followed by relief\nor even positive comfort on continuing the ride. The dynamic driving style\ninduced a stronger discomfort as compared to the calm driving style. The\ncrossing pedestrian did not affect discomfort with the calm driving style but\ndoubled the comfort decrement with the dynamic driving style. This illustrates\nthe importance of consequences of critical interactions in risk perception.\nFacial expression was successfully analyzed for 24 participants but most\n(15/24) did not show any detectable facial reaction to the critical event.\nAmong the 9 participants who did, 8 showed a Happy expression, and only 4\nshowed a Surprise expression. Fear was never dominant. This indicates that\nfacial expression recognition is not a reliable method for assessing perceived\nrisk in automated vehicles. To predict perceived risk a neural network model\nwas implemented using vehicle motion and skin conductance. The model correlated\nwell with reported perceived risk, demonstrating its potential for objective\nperceived risk assessment in automated vehicles, reducing subjective bias and\nhighlighting areas for future research.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u9a7e\u9a76\u6a21\u62df\u5668\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u4e2d\u611f\u77e5\u98ce\u9669\u7684\u5f71\u54cd\u56e0\u7d20\uff08\u9a7e\u9a76\u98ce\u683c\u3001\u884c\u4eba\uff09\uff0c\u53d1\u73b0\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4e0d\u53ef\u9760\uff0c\u4f46\u57fa\u4e8e\u8f66\u8f86\u8fd0\u52a8\u548c\u76ae\u80a4\u7535\u53cd\u5e94\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u53ef\u6709\u6548\u5ba2\u89c2\u8bc4\u4f30\u611f\u77e5\u98ce\u9669\u3002", "motivation": "\u516c\u4f17\u5bf9\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u63a5\u53d7\u5ea6\u5173\u952e\u5728\u4e8e\u4fe1\u4efb\u548c\u611f\u77e5\u5b89\u5168\u3002\u4e3a\u6df1\u5165\u7406\u89e3\u7528\u6237\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u611f\u77e5\u98ce\u9669\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5176\u5f71\u54cd\u56e0\u7d20\u5e76\u5bfb\u627e\u5ba2\u89c2\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5b9e\u9a8c\u5728\u9a7e\u9a76\u6a21\u62df\u5668\u4e2d\u8fdb\u884c\uff0c32\u540d\u53c2\u4e0e\u8005\u5728\u4e24\u79cd\u81ea\u52a8\u9a7e\u9a76\u98ce\u683c\u4e0b\u4f53\u9a8c\uff0c\u5e76\u53ef\u9009\u5f15\u5165\u6a2a\u7a7f\u884c\u4eba\u3002\u6570\u636e\u91c7\u96c6\u5305\u62ec\u8fde\u7eed\u4e3b\u89c2\u8212\u9002\u5ea6\u8bc4\u5206\u3001\u8f66\u8f86\u8fd0\u52a8\u3001\u7f51\u7edc\u6444\u50cf\u5934\uff08\u9762\u90e8\u8868\u60c5\uff09\u3001\u76ae\u80a4\u7535\u53cd\u5e94\u3001\u5fc3\u7387\u548c\u773c\u52a8\u8ffd\u8e2a\u3002\u968f\u540e\uff0c\u5229\u7528\u8f66\u8f86\u8fd0\u52a8\u548c\u76ae\u80a4\u7535\u53cd\u5e94\u6784\u5efa\u4e86\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u6765\u9884\u6d4b\u611f\u77e5\u98ce\u9669\u3002", "result": "\u5728\u8f6c\u5f2f\u548c\u5236\u52a8\u65f6\u611f\u77e5\u98ce\u9669\u5f15\u8d77\u7684\u663e\u8457\u4e0d\u9002\uff0c\u4e4b\u540e\u5219\u611f\u5230\u653e\u677e\u6216\u8212\u9002\u3002\u52a8\u6001\u9a7e\u9a76\u98ce\u683c\u6bd4\u5e73\u7a33\u9a7e\u9a76\u98ce\u683c\u5f15\u8d77\u66f4\u5f3a\u7684\u4e0d\u9002\u3002\u6a2a\u7a7f\u884c\u4eba\u5e76\u672a\u5f71\u54cd\u5e73\u7a33\u9a7e\u9a76\u98ce\u683c\u4e0b\u7684\u4e0d\u9002\uff0c\u4f46\u5728\u52a8\u6001\u9a7e\u9a76\u98ce\u683c\u4e0b\u4f7f\u4e0d\u9002\u611f\u52a0\u500d\u3002\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7ed3\u679c\u663e\u793a\uff0c\u5927\u591a\u6570\u53c2\u4e0e\u8005\u65e0\u660e\u663e\u9762\u90e8\u53cd\u5e94\uff0c\u4e14\u5c11\u6570\u53ef\u89c1\u53cd\u5e94\u4e2d\u201c\u5feb\u4e50\u201d\u8868\u60c5\u5360\u4e3b\u5bfc\uff0c\u800c\u975e\u201c\u6050\u60e7\u201d\uff0c\u8868\u660e\u5176\u8bc4\u4f30\u98ce\u9669\u4e0d\u53ef\u9760\u3002\u57fa\u4e8e\u8f66\u8f86\u8fd0\u52a8\u548c\u76ae\u80a4\u7535\u53cd\u5e94\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e0e\u62a5\u544a\u7684\u611f\u77e5\u98ce\u9669\u6709\u826f\u597d\u76f8\u5173\u6027\u3002", "conclusion": "\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4e0d\u662f\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u4e2d\u611f\u77e5\u98ce\u9669\u7684\u53ef\u9760\u65b9\u6cd5\u3002\u5229\u7528\u8f66\u8f86\u8fd0\u52a8\u548c\u76ae\u80a4\u7535\u53cd\u5e94\u6784\u5efa\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u4e2d\u5ba2\u89c2\u8bc4\u4f30\u611f\u77e5\u98ce\u9669\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u51cf\u5c11\u4e3b\u89c2\u504f\u89c1\u5e76\u6307\u660e\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2509.04505", "pdf": "https://arxiv.org/pdf/2509.04505", "abs": "https://arxiv.org/abs/2509.04505", "authors": ["Somtochukwu Azie", "Yiping Meng"], "title": "The Ethical Compass of the Machine: Evaluating Large Language Models for Decision Support in Construction Project Management", "categories": ["cs.AI", "cs.CY"], "comment": "16 Pages", "summary": "The integration of Artificial Intelligence (AI) into construction project\nmanagement (CPM) is accelerating, with Large Language Models (LLMs) emerging as\naccessible decision-support tools. This study aims to critically evaluate the\nethical viability and reliability of LLMs when applied to the ethically\nsensitive, high-risk decision-making contexts inherent in CPM. A mixed-methods\nresearch design was employed, involving the quantitative performance testing of\ntwo leading LLMs against twelve real-world ethical scenarios using a novel\nEthical Decision Support Assessment Checklist (EDSAC), and qualitative analysis\nof semi-structured interviews with 12 industry experts to capture professional\nperceptions. The findings reveal that while LLMs demonstrate adequate\nperformance in structured domains such as legal compliance, they exhibit\nsignificant deficiencies in handling contextual nuance, ensuring\naccountability, and providing transparent reasoning. Stakeholders expressed\nconsiderable reservations regarding the autonomous use of AI for ethical\njudgments, strongly advocating for robust human-in-the-loop oversight. To our\nknowledge, this is one of the first studies to empirically test the ethical\nreasoning of LLMs within the construction domain. It introduces the EDSAC\nframework as a replicable methodology and provides actionable recommendations,\nemphasising that LLMs are currently best positioned as decision-support aids\nrather than autonomous ethical agents.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5efa\u7b51\u9879\u76ee\u7ba1\u7406\uff08CPM\uff09\u4f26\u7406\u51b3\u7b56\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u5904\u7406\u60c5\u5883\u7ec6\u5fae\u4e4b\u5904\u548c\u95ee\u8d23\u5236\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u5f3a\u8c03\u9700\u8981\u4eba\u7c7b\u76d1\u7763\u800c\u975e\u81ea\u4e3b\u5e94\u7528\u3002", "motivation": "\u6279\u5224\u6027\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5efa\u7b51\u9879\u76ee\u7ba1\u7406\uff08CPM\uff09\u4e2d\u4f26\u7406\u654f\u611f\u3001\u9ad8\u98ce\u9669\u51b3\u7b56\u60c5\u5883\u4e0b\u7684\u4f26\u7406\u53ef\u884c\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\u8bbe\u8ba1\u3002\u5b9a\u91cf\u65b9\u9762\uff0c\u4f7f\u7528\u65b0\u578b\u4f26\u7406\u51b3\u7b56\u652f\u6301\u8bc4\u4f30\u6e05\u5355\uff08EDSAC\uff09\u5bf9\u4e24\u4e2a\u4e3b\u6d41LLM\u572812\u4e2a\u771f\u5b9e\u4e16\u754c\u4f26\u7406\u573a\u666f\u4e2d\u8fdb\u884c\u6027\u80fd\u6d4b\u8bd5\u3002\u5b9a\u6027\u65b9\u9762\uff0c\u5bf912\u4f4d\u884c\u4e1a\u4e13\u5bb6\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u4ee5\u83b7\u53d6\u4e13\u4e1a\u89c1\u89e3\u3002", "result": "LLMs\u5728\u6cd5\u5f8b\u5408\u89c4\u7b49\u7ed3\u6784\u5316\u9886\u57df\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u5904\u7406\u60c5\u5883\u7ec6\u5fae\u6027\u3001\u786e\u4fdd\u95ee\u8d23\u5236\u548c\u63d0\u4f9b\u900f\u660e\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002\u5229\u76ca\u76f8\u5173\u8005\u5bf9AI\u81ea\u4e3b\u8fdb\u884c\u4f26\u7406\u5224\u65ad\u8868\u793a\u9ad8\u5ea6\u4fdd\u7559\uff0c\u5f3a\u70c8\u4e3b\u5f20\u5b9e\u884c\u5f3a\u5927\u7684\u4eba\u5de5\u76d1\u7763\u3002", "conclusion": "\u9274\u4e8eLLMs\u5728\u4f26\u7406\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5b83\u4eec\u76ee\u524d\u6700\u9002\u5408\u4f5c\u4e3a\u5efa\u7b51\u9879\u76ee\u7ba1\u7406\u4e2d\u7684\u51b3\u7b56\u652f\u6301\u8f85\u52a9\u5de5\u5177\uff0c\u800c\u975e\u81ea\u4e3b\u4f26\u7406\u4ee3\u7406\uff0c\u5f3a\u8c03\u4e86\u4eba\u5de5\u76d1\u7763\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2509.04536", "pdf": "https://arxiv.org/pdf/2509.04536", "abs": "https://arxiv.org/abs/2509.04536", "authors": ["Oliver Dunn", "Koorosh Aslansefat", "Yiannis Papadopoulos"], "title": "Q-SafeML: Safety Assessment of Quantum Machine Learning via Quantum Distance Metrics", "categories": ["cs.LG", "math.QA", "math.ST", "stat.TH"], "comment": null, "summary": "The rise of machine learning in safety-critical systems has paralleled\nadvancements in quantum computing, leading to the emerging field of Quantum\nMachine Learning (QML). While safety monitoring has progressed in classical ML,\nexisting methods are not directly applicable to QML due to fundamental\ndifferences in quantum computation. Given the novelty of QML, dedicated safety\nmechanisms remain underdeveloped. This paper introduces Q-SafeML, a safety\nmonitoring approach for QML. The method builds on SafeML, a recent method that\nutilizes statistical distance measures to assess model accuracy and provide\nconfidence in the reasoning of an algorithm. An adapted version of Q-SafeML\nincorporates quantum-centric distance measures, aligning with the probabilistic\nnature of QML outputs. This shift to a model-dependent, post-classification\nevaluation represents a key departure from classical SafeML, which is\ndataset-driven and classifier-agnostic. The distinction is motivated by the\nunique representational constraints of quantum systems, requiring distance\nmetrics defined over quantum state spaces. Q-SafeML detects distances between\noperational and training data addressing the concept drifts in the context of\nQML. Experiments on QCNN and VQC Models show that this enables informed human\noversight, enhancing system transparency and safety.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faQ-SafeML\uff0c\u4e00\u79cd\u4e13\u4e3a\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u8bbe\u8ba1\u7684\u5b89\u5168\u76d1\u63a7\u65b9\u6cd5\uff0c\u901a\u8fc7\u9002\u5e94\u91cf\u5b50\u7279\u6027\u6765\u589e\u5f3a\u7cfb\u7edf\u900f\u660e\u5ea6\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u7684\u5174\u8d77\uff0c\u73b0\u6709\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u7684\u5b89\u5168\u76d1\u63a7\u65b9\u6cd5\u56e0\u91cf\u5b50\u8ba1\u7b97\u7684\u6839\u672c\u5dee\u5f02\u800c\u4e0d\u518d\u9002\u7528\u3002\u76ee\u524dQML\u7f3a\u4e4f\u4e13\u95e8\u7684\u5b89\u5168\u673a\u5236\uff0c\u8fd9\u4e00\u9886\u57df\u5c1a\u4e0d\u6210\u719f\u3002", "method": "\u5f15\u5165Q-SafeML\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8eSafeML\uff0c\u4f46\u4e3aQML\u8fdb\u884c\u4e86\u9002\u914d\u3002\u5b83\u5229\u7528\u4ee5\u91cf\u5b50\u4e3a\u4e2d\u5fc3\u7684\u8ddd\u79bb\u5ea6\u91cf\uff0c\u5bf9QML\u8f93\u51fa\u7684\u6982\u7387\u6027\u8d28\u8fdb\u884c\u6a21\u578b\u4f9d\u8d56\u7684\u540e\u5206\u7c7b\u8bc4\u4f30\u3002\u901a\u8fc7\u5728\u91cf\u5b50\u6001\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u8ddd\u79bb\u6307\u6807\uff0cQ-SafeML\u80fd\u591f\u68c0\u6d4b\u8fd0\u884c\u6570\u636e\u4e0e\u8bad\u7ec3\u6570\u636e\u4e4b\u95f4\u7684\u6982\u5ff5\u6f02\u79fb\u3002", "result": "\u5728QCNN\u548cVQC\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cQ-SafeML\u80fd\u591f\u5b9e\u73b0\u77e5\u60c5\u7684\u4eba\u5de5\u76d1\u7763\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u5b89\u5168\u6027\u3002", "conclusion": "Q-SafeML\u4e3aQML\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b89\u5168\u76d1\u63a7\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e0d\u9002\u7528\u7684\u95ee\u9898\uff0c\u589e\u5f3a\u4e86QML\u7cfb\u7edf\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.04456", "pdf": "https://arxiv.org/pdf/2509.04456", "abs": "https://arxiv.org/abs/2509.04456", "authors": ["Anandi Dutta", "Shivani Mruthyunjaya", "Jessica Saddington", "Kazi Sifatul Islam"], "title": "Mentalic Net: Development of RAG-based Conversational AI and Evaluation Framework for Mental Health Support", "categories": ["cs.CL"], "comment": "Preprint Version, Accepted in ISEMV 2025", "summary": "The emergence of large language models (LLMs) has unlocked boundless\npossibilities, along with significant challenges. In response, we developed a\nmental health support chatbot designed to augment professional healthcare, with\na strong emphasis on safe and meaningful application. Our approach involved\nrigorous evaluation, covering accuracy, empathy, trustworthiness, privacy, and\nbias. We employed a retrieval-augmented generation (RAG) framework, integrated\nprompt engineering, and fine-tuned a pre-trained model on novel datasets. The\nresulting system, Mentalic Net Conversational AI, achieved a BERT Score of\n0.898, with other evaluation metrics falling within satisfactory ranges. We\nadvocate for a human-in-the-loop approach and a long-term, responsible strategy\nin developing such transformative technologies, recognizing both their\npotential to change lives and the risks they may pose if not carefully managed.", "AI": {"tldr": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u7ed3\u5408RAG\u548c\u63d0\u793a\u5de5\u7a0b\u7684\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u804a\u5929\u673a\u5668\u4eba\uff0c\u65e8\u5728\u5b89\u5168\u6709\u6548\u5730\u8f85\u52a9\u4e13\u4e1a\u533b\u7597\u3002", "motivation": "\u9274\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e26\u6765\u7684\u673a\u9047\u4e0e\u6311\u6218\uff0c\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u5b89\u5168\u3001\u6709\u610f\u4e49\u4e14\u80fd\u8f85\u52a9\u4e13\u4e1a\u533b\u7597\u7684\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u804a\u5929\u673a\u5668\u4eba\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6846\u67b6\uff0c\u96c6\u6210\u63d0\u793a\u5de5\u7a0b\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\uff1b\u540c\u65f6\u8fdb\u884c\u591a\u7ef4\u5ea6\uff08\u51c6\u786e\u6027\u3001\u540c\u7406\u5fc3\u3001\u53ef\u4fe1\u5ea6\u3001\u9690\u79c1\u3001\u504f\u89c1\uff09\u7684\u4e25\u683c\u8bc4\u4f30\u3002", "result": "\u6240\u5f00\u53d1\u7684\u201cMentalic Net Conversational AI\u201d\u7cfb\u7edfBERT Score\u8fbe\u52300.898\uff0c\u5176\u4ed6\u8bc4\u4f30\u6307\u6807\u4e5f\u5904\u4e8e\u6ee1\u610f\u8303\u56f4\u3002", "conclusion": "\u5efa\u8bae\u5728\u5f00\u53d1\u6b64\u7c7bAI\u6280\u672f\u65f6\u91c7\u53d6\u201c\u4eba\u7c7b\u5728\u73af\u201d\u548c\u957f\u671f\u8d1f\u8d23\u4efb\u7684\u7b56\u7565\uff0c\u4ee5\u5e73\u8861\u5176\u5de8\u5927\u6f5c\u529b\u4e0e\u6f5c\u5728\u98ce\u9669\u3002"}}
{"id": "2509.04545", "pdf": "https://arxiv.org/pdf/2509.04545", "abs": "https://arxiv.org/abs/2509.04545", "authors": ["Linqing Wang", "Ximing Xing", "Yiji Cheng", "Zhiyuan Zhao", "Jiale Tao", "Qixun Wang", "Ruihuang Li", "Xin Li", "Mingrui Wu", "Xinchi Deng", "Chunyu Wang", "Qinglin Lu"], "title": "PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting", "categories": ["cs.CV"], "comment": "technical report", "summary": "Recent advancements in text-to-image (T2I) diffusion models have demonstrated\nremarkable capabilities in generating high-fidelity images. However, these\nmodels often struggle to faithfully render complex user prompts, particularly\nin aspects like attribute binding, negation, and compositional relationships.\nThis leads to a significant mismatch between user intent and the generated\noutput. To address this challenge, we introduce PromptEnhancer, a novel and\nuniversal prompt rewriting framework that enhances any pretrained T2I model\nwithout requiring modifications to its weights. Unlike prior methods that rely\non model-specific fine-tuning or implicit reward signals like image-reward\nscores, our framework decouples the rewriter from the generator. We achieve\nthis by training a Chain-of-Thought (CoT) rewriter through reinforcement\nlearning, guided by a dedicated reward model we term the AlignEvaluator. The\nAlignEvaluator is trained to provide explicit and fine-grained feedback based\non a systematic taxonomy of 24 key points, which are derived from a\ncomprehensive analysis of common T2I failure modes. By optimizing the CoT\nrewriter to maximize the reward from our AlignEvaluator, our framework learns\nto generate prompts that are more precisely interpreted by T2I models.\nExtensive experiments on the HunyuanImage 2.1 model demonstrate that\nPromptEnhancer significantly improves image-text alignment across a wide range\nof semantic and compositional challenges. Furthermore, we introduce a new,\nhigh-quality human preference benchmark to facilitate future research in this\ndirection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPromptEnhancer\uff0c\u4e00\u4e2a\u901a\u7528\u7684\u63d0\u793a\u91cd\u5199\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3CoT\u91cd\u5199\u5668\uff0c\u5e76\u7531\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u53cd\u9988\u7684AlignEvaluator\u5956\u52b1\u6a21\u578b\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u751f\u56fe(T2I)\u6a21\u578b\u5bf9\u590d\u6742\u63d0\u793a\u7684\u7406\u89e3\u548c\u56fe\u6587\u5bf9\u9f50\u80fd\u529b\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u3002", "motivation": "\u73b0\u6709\u6587\u751f\u56fe(T2I)\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u7528\u6237\u63d0\u793a\uff08\u5982\u5c5e\u6027\u7ed1\u5b9a\u3001\u5426\u5b9a\u3001\u7ec4\u5408\u5173\u7cfb\uff09\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5bfc\u81f4\u751f\u6210\u56fe\u50cf\u4e0e\u7528\u6237\u610f\u56fe\u4e0d\u7b26\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u6a21\u578b\u7279\u5b9a\u5fae\u8c03\u6216\u9690\u5f0f\u5956\u52b1\u4fe1\u53f7\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "\u5f15\u5165PromptEnhancer\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u91cd\u5199\u5668\u4e0e\u751f\u6210\u5668\u89e3\u8026\uff0c\u4e0d\u4fee\u6539\u9884\u8bad\u7ec3T2I\u6a21\u578b\u6743\u91cd\u3002\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e00\u4e2aChain-of-Thought (CoT) \u91cd\u5199\u5668\uff0c\u5e76\u7531\u4e13\u95e8\u7684AlignEvaluator\u5956\u52b1\u6a21\u578b\u63d0\u4f9b\u6307\u5bfc\u3002AlignEvaluator\u57fa\u4e8e\u5bf9T2I\u6a21\u578b\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\u5206\u6790\u5f97\u51fa\u768424\u4e2a\u5173\u952e\u70b9\uff0c\u63d0\u4f9b\u663e\u5f0f\u4e14\u7ec6\u7c92\u5ea6\u7684\u53cd\u9988\u3002\u91cd\u5199\u5668\u901a\u8fc7\u6700\u5927\u5316AlignEvaluator\u7684\u5956\u52b1\u6765\u5b66\u4e60\u751f\u6210\u66f4\u7cbe\u786e\u7684\u63d0\u793a\u3002", "result": "\u5728HunyuanImage 2.1\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0cPromptEnhancer\u663e\u8457\u6539\u5584\u4e86\u5404\u79cd\u8bed\u4e49\u548c\u7ec4\u5408\u6311\u6218\u4e0b\u7684\u56fe\u6587\u5bf9\u9f50\u6548\u679c\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u9ad8\u8d28\u91cf\u4eba\u7c7b\u504f\u597d\u57fa\u51c6\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u7684\u7814\u7a76\u3002", "conclusion": "PromptEnhancer\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63d0\u793a\u91cd\u5199\u65b9\u6cd5\uff0c\u663e\u8457\u589e\u5f3a\u4e86T2I\u6a21\u578b\u89e3\u91ca\u590d\u6742\u63d0\u793a\u7684\u80fd\u529b\u548c\u56fe\u6587\u5bf9\u9f50\u8868\u73b0\u3002\u65b0\u5f15\u5165\u7684\u57fa\u51c6\u5c06\u6709\u52a9\u4e8e\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2509.04642", "pdf": "https://arxiv.org/pdf/2509.04642", "abs": "https://arxiv.org/abs/2509.04642", "authors": ["Wenxiao Wang", "Priyatham Kattakinda", "Soheil Feizi"], "title": "Maestro: Joint Graph & Config Optimization for Reliable AI Agents", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": "Technical Report by RELAI.ai", "summary": "Building reliable LLM agents requires decisions at two levels: the graph\n(which modules exist and how information flows) and the configuration of each\nnode (models, prompts, tools, control knobs). Most existing optimizers tune\nconfigurations while holding the graph fixed, leaving structural failure modes\nunaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for\nLLM agents that jointly searches over graphs and configurations to maximize\nagent quality, subject to explicit rollout/token budgets. Beyond numeric\nmetrics, Maestro leverages reflective textual feedback from traces to\nprioritize edits, improving sample efficiency and targeting specific failure\nmodes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses\nleading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%,\n4.9%, and 4.86%, respectively; even when restricted to prompt-only\noptimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these\nresults with far fewer rollouts than GEPA. We further show large gains on two\napplications (interviewer & RAG agents), highlighting that joint graph &\nconfiguration search addresses structural failure modes that prompt tuning\nalone cannot fix.", "AI": {"tldr": "Maestro\u662f\u4e00\u79cd\u9488\u5bf9LLM\u667a\u80fd\u4f53\u7684\u5168\u9762\u4f18\u5316\u5668\uff0c\u5b83\u901a\u8fc7\u8054\u5408\u641c\u7d22\u667a\u80fd\u4f53\u7684\u56fe\u7ed3\u6784\u548c\u914d\u7f6e\u6765\u63d0\u9ad8\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4f18\u5316\u5668\u65e0\u6cd5\u5904\u7406\u7684\u7ed3\u6784\u6027\u6545\u969c\u6a21\u5f0f\uff0c\u5e76\u663e\u8457\u8d85\u8d8a\u4e86\u9886\u5148\u7684\u63d0\u793a\u4f18\u5316\u5668\u3002", "motivation": "\u6784\u5efa\u53ef\u9760\u7684LLM\u667a\u80fd\u4f53\u9700\u8981\u5728\u56fe\u7ed3\u6784\uff08\u6a21\u5757\u53ca\u4fe1\u606f\u6d41\uff09\u548c\u8282\u70b9\u914d\u7f6e\uff08\u6a21\u578b\u3001\u63d0\u793a\u3001\u5de5\u5177\u7b49\uff09\u4e24\u4e2a\u5c42\u9762\u8fdb\u884c\u51b3\u7b56\u3002\u73b0\u6709\u4f18\u5316\u5668\u5927\u591a\u56fa\u5b9a\u56fe\u7ed3\u6784\uff0c\u53ea\u8c03\u6574\u914d\u7f6e\uff0c\u5bfc\u81f4\u65e0\u6cd5\u89e3\u51b3\u7ed3\u6784\u6027\u6545\u969c\u6a21\u5f0f\u3002", "method": "Maestro\u662f\u4e00\u4e2a\u4e0e\u6846\u67b6\u65e0\u5173\u7684\u6574\u4f53\u4f18\u5316\u5668\uff0c\u5b83\u5728\u660e\u786e\u7684\u9884\u7b97\u9650\u5236\u4e0b\uff0c\u540c\u65f6\u641c\u7d22\u667a\u80fd\u4f53\u7684\u56fe\u7ed3\u6784\u548c\u914d\u7f6e\u4ee5\u6700\u5927\u5316\u667a\u80fd\u4f53\u8d28\u91cf\u3002\u5b83\u5229\u7528\u8f68\u8ff9\u4e2d\u7684\u53cd\u5c04\u6027\u6587\u672c\u53cd\u9988\u6765\u4f18\u5148\u7f16\u8f91\uff0c\u4ece\u800c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u5e76\u9488\u5bf9\u7279\u5b9a\u6545\u969c\u6a21\u5f0f\u3002", "result": "\u5728IFBench\u548cHotpotQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMaestro\u5e73\u5747\u8d85\u8d8a\u4e86MIPROv2\u3001GEPA\u548cGEPA+Merge\u7b49\u9886\u5148\u7684\u63d0\u793a\u4f18\u5316\u5668\uff0c\u5206\u522b\u63d0\u9ad8\u4e8612%\u30014.9%\u548c4.86%\u3002\u5373\u4f7f\u4ec5\u9650\u4e8e\u63d0\u793a\u4f18\u5316\uff0c\u5b83\u4ecd\u5206\u522b\u9886\u51489.65%\u30012.37%\u548c2.41%\u3002Maestro\u4ee5\u8fdc\u5c11\u4e8eGEPA\u7684\u6267\u884c\u6b21\u6570\u8fbe\u5230\u8fd9\u4e9b\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u5728\u9762\u8bd5\u5b98\u548cRAG\u667a\u80fd\u4f53\u5e94\u7528\u4e2d\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\uff0c\u8868\u660e\u8054\u5408\u56fe\u7ed3\u6784\u548c\u914d\u7f6e\u641c\u7d22\u80fd\u89e3\u51b3\u4ec5\u51ed\u63d0\u793a\u8c03\u4f18\u65e0\u6cd5\u4fee\u590d\u7684\u7ed3\u6784\u6027\u6545\u969c\u6a21\u5f0f\u3002", "conclusion": "Maestro\u901a\u8fc7\u8054\u5408\u641c\u7d22LLM\u667a\u80fd\u4f53\u7684\u56fe\u7ed3\u6784\u548c\u914d\u7f6e\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u63d0\u793a\u4f18\u5316\u5668\u7684\u5c40\u9650\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ed3\u6784\u6027\u6545\u969c\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u667a\u80fd\u4f53\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684LLM\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002"}}
{"id": "2509.04541", "pdf": "https://arxiv.org/pdf/2509.04541", "abs": "https://arxiv.org/abs/2509.04541", "authors": ["Kasymkhan Khubiev", "Mikhail Semenov", "Irina Podlipnova"], "title": "Finance-Grounded Optimization For Algorithmic Trading", "categories": ["cs.LG", "q-fin.ST"], "comment": "12 pages, 8 figures, 5 tables", "summary": "Deep Learning is evolving fast and integrates into various domains. Finance\nis a challenging field for deep learning, especially in the case of\ninterpretable artificial intelligence (AI). Although classical approaches\nperform very well with natural language processing, computer vision, and\nforecasting, they are not perfect for the financial world, in which specialists\nuse different metrics to evaluate model performance.\n  We first introduce financially grounded loss functions derived from key\nquantitative finance metrics, including the Sharpe ratio, Profit-and-Loss\n(PnL), and Maximum Draw down. Additionally, we propose turnover regularization,\na method that inherently constrains the turnover of generated positions within\npredefined limits.\n  Our findings demonstrate that the proposed loss functions, in conjunction\nwith turnover regularization, outperform the traditional mean squared error\nloss for return prediction tasks when evaluated using algorithmic trading\nmetrics. The study shows that financially grounded metrics enhance predictive\nperformance in trading strategies and portfolio optimization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9488\u5bf9\u91d1\u878d\u6df1\u5ea6\u5b66\u4e60\u7684\u6311\u6218\uff0c\u5f15\u5165\u57fa\u4e8e\u590f\u666e\u6bd4\u7387\u3001\u76c8\u4e8f\u7b49\u91d1\u878d\u6307\u6807\u7684\u635f\u5931\u51fd\u6570\u53ca\u6362\u624b\u7387\u6b63\u5219\u5316\u3002\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56de\u62a5\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u7b97\u6cd5\u4ea4\u6613\u6307\u6807\u8bc4\u4f30\u65f6\uff0c\u4f18\u4e8e\u4f20\u7edf\u5747\u65b9\u8bef\u5dee\u635f\u5931\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4ea4\u6613\u7b56\u7565\u548c\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u91d1\u878d\u9886\u57df\u8868\u73b0\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u53ef\u89e3\u91ca\u6027AI\u65b9\u9762\u3002\u91d1\u878d\u4e13\u5bb6\u91c7\u7528\u72ec\u7279\u7684\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u4e0e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u9886\u57df\u4e0d\u540c\uff0c\u8fd9\u4f7f\u5f97\u7ecf\u5178\u65b9\u6cd5\u4e0d\u5b8c\u5168\u9002\u7528\u4e8e\u91d1\u878d\u4e16\u754c\u3002", "method": "\u5f15\u5165\u4e86\u57fa\u4e8e\u590f\u666e\u6bd4\u7387\u3001\u76c8\u4e8f\uff08PnL\uff09\u548c\u6700\u5927\u56de\u64a4\u7b49\u5173\u952e\u91cf\u5316\u91d1\u878d\u6307\u6807\u7684\u91d1\u878d\u5316\u635f\u5931\u51fd\u6570\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u6362\u624b\u7387\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u4ee5\u5c06\u751f\u6210\u5934\u5bf8\u7684\u6362\u624b\u7387\u9650\u5236\u5728\u9884\u5b9a\u4e49\u8303\u56f4\u5185\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570\u4e0e\u6362\u624b\u7387\u6b63\u5219\u5316\u7ed3\u5408\u540e\uff0c\u5728\u56de\u62a5\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u7b97\u6cd5\u4ea4\u6613\u6307\u6807\u8bc4\u4f30\u65f6\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u7684\u5747\u65b9\u8bef\u5dee\u635f\u5931\u3002", "conclusion": "\u5c06\u91d1\u878d\u5316\u6307\u6807\u878d\u5165\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u4ea4\u6613\u7b56\u7565\u548c\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2509.04457", "pdf": "https://arxiv.org/pdf/2509.04457", "abs": "https://arxiv.org/abs/2509.04457", "authors": ["Xiao Zhang", "Dongyuan Li", "Liuyu Xiang", "Yao Zhang", "Cheng Zhong", "Zhaofeng He"], "title": "Do MLLMs Really Understand the Charts?", "categories": ["cs.CL"], "comment": "19 pages,15 figures", "summary": "Although Multimodal Large Language Models (MLLMs) have demonstrated\nincreasingly impressive performance in chart understanding, most of them\nexhibit alarming hallucinations and significant performance degradation when\nhandling non-annotated charts. Therefore, a question arises: Do MLLMs really\nunderstand the charts? Since a human is capable of understanding charts and\nestimating the values by visual reasoning, we first carefully establish a\ncomprehensive Chart Reasoning Benchmark CRBench to rigorously evaluate the\nvisual reasoning abilities of MLLMs on non-annotated charts. We argue that\nMLLMs are primarily relying on recognition rather than reasoning to interpret\nthe charts. To steer MLLMs to reasonable chart understanding, we propose\nChartReasoner that mimics human behavior by grounding their estimation in chart\nunderstanding. Extensive results on the proposed CRBench show that\nChartReasnoner-3B/7B achieves superior performance in chart reasoning, even\ncompared to GPT-4o and Gemini-2.5-Flash. More importantly, ChartReasnoner also\ndemonstrates the visual reasoning abilities in general chart comprehension on\npublic benchmarks, leading to significant performance gains and enabling MLLMs\nto rationally understand the charts. The code and dataset will be publicly\navailable upon publication.", "AI": {"tldr": "\u73b0\u6709MLLM\u5728\u65e0\u6807\u6ce8\u56fe\u8868\u4e0a\u8868\u73b0\u51fa\u5e7b\u89c9\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u672c\u7814\u7a76\u521b\u5efa\u4e86CRBench\u57fa\u51c6\u6765\u8bc4\u4f30\u5176\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86ChartReasoner\u6a21\u578b\u3002ChartReasoner\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u5347\u4e86MLLM\u7684\u56fe\u8868\u7406\u89e3\u80fd\u529b\uff0c\u5728CRBench\u548c\u516c\u5171\u57fa\u51c6\u4e0a\u5747\u8d85\u8d8a\u4e86GPT-4o\u7b49\u9886\u5148\u6a21\u578b\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u56fe\u8868\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u5e7b\u89c9\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u65e0\u6807\u6ce8\u56fe\u8868\u65f6\u3002\u7814\u7a76\u8005\u8d28\u7591MLLMs\u662f\u5426\u771f\u6b63\u7406\u89e3\u56fe\u8868\uff0c\u5e76\u8ba4\u4e3a\u5b83\u4eec\u4e3b\u8981\u4f9d\u8d56\u8bc6\u522b\u800c\u975e\u63a8\u7406\u3002", "method": ["\u5efa\u7acb\u4e86\u7efc\u5408\u56fe\u8868\u63a8\u7406\u57fa\u51c6CRBench\uff0c\u7528\u4e8e\u4e25\u683c\u8bc4\u4f30MLLMs\u5728\u65e0\u6807\u6ce8\u56fe\u8868\u4e0a\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002", "\u63d0\u51fa\u4e86ChartReasoner\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\uff0c\u5c06\u56fe\u8868\u7406\u89e3\u4e2d\u7684\u4f30\u8ba1\u4e0e\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u4ee5\u5f15\u5bfcMLLMs\u8fdb\u884c\u5408\u7406\u7684\u56fe\u8868\u7406\u89e3\u3002"], "result": ["\u5728\u6240\u63d0\u51fa\u7684CRBench\u4e0a\uff0cChartReasoner-3B/7B\u5728\u56fe\u8868\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u5353\u8d8a\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86GPT-4o\u548cGemini-2.5-Flash\u3002", "ChartReasoner\u5728\u516c\u5171\u57fa\u51c6\u7684\u901a\u7528\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u4e5f\u5c55\u793a\u4e86\u51fa\u8272\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f7fMLLMs\u80fd\u591f\u66f4\u5408\u7406\u5730\u7406\u89e3\u56fe\u8868\u3002"], "conclusion": "ChartReasoner\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u89c6\u89c9\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86MLLMs\u5728\u65e0\u6807\u6ce8\u56fe\u8868\u4e0a\u7684\u7406\u89e3\u4e0d\u8db3\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u4f7f\u5176\u80fd\u591f\u5b9e\u73b0\u7406\u6027\u7684\u56fe\u8868\u7406\u89e3\uff0c\u5e76\u5728\u56fe\u8868\u63a8\u7406\u4efb\u52a1\u4e2d\u8fbe\u5230\u9886\u5148\u6027\u80fd\u3002"}}
{"id": "2509.04548", "pdf": "https://arxiv.org/pdf/2509.04548", "abs": "https://arxiv.org/abs/2509.04548", "authors": ["Hongyang Wei", "Baixin Xu", "Hongbo Liu", "Cyrus Wu", "Jie Liu", "Yi Peng", "Peiyu Wang", "Zexiang Liu", "Jingwen He", "Yidan Xietian", "Chuanxin Tang", "Zidong Wang", "Yichen Wei", "Liang Hu", "Boyi Jiang", "William Li", "Ying He", "Yang Liu", "Xuchen Song", "Eric Li", "Yahui Zhou"], "title": "Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in multimodal models have demonstrated impressive\ncapabilities in unified image generation and editing. However, many prominent\nopen-source models prioritize scaling model parameters over optimizing training\nstrategies, limiting their efficiency and performance. In this work, we present\nUniPic2-SD3.5M-Kontext, a 2B-parameter DiT model based on SD3.5-Medium, which\nachieves state-of-the-art image generation and editing while extending\nseamlessly into a unified multimodal framework. Our approach begins with\narchitectural modifications to SD3.5-Medium and large-scale pre-training on\nhigh-quality data, enabling joint text-to-image generation and editing\ncapabilities. To enhance instruction following and editing consistency, we\npropose a novel Progressive Dual-Task Reinforcement strategy (PDTR), which\neffectively strengthens both tasks in a staged manner. We empirically validate\nthat the reinforcement phases for different tasks are mutually beneficial and\ndo not induce negative interference. After pre-training and reinforcement\nstrategies, UniPic2-SD3.5M-Kontext demonstrates stronger image generation and\nediting capabilities than models with significantly larger generation\nparameters-including BAGEL (7B) and Flux-Kontext (12B). Furthermore, following\nthe MetaQuery, we connect the UniPic2-SD3.5M-Kontext and Qwen2.5-VL-7B via a\nconnector and perform joint training to launch a unified multimodal model\nUniPic2-Metaquery. UniPic2-Metaquery integrates understanding, generation, and\nediting, achieving top-tier performance across diverse tasks with a simple and\nscalable training paradigm. This consistently validates the effectiveness and\ngeneralizability of our proposed training paradigm, which we formalize as\nSkywork UniPic 2.0.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a2B\u53c2\u6570\u7684DiT\u6a21\u578bUniPic2-SD3.5M-Kontext\uff0c\u901a\u8fc7\u67b6\u6784\u4fee\u6539\u548c\u65b0\u9896\u7684\u6e10\u8fdb\u5f0f\u53cc\u4efb\u52a1\u5f3a\u5316\u7b56\u7565\uff08PDTR\uff09\uff0c\u5728\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u65b9\u9762\u8d85\u8d8a\u4e86\u53c2\u6570\u66f4\u5927\u7684\u6a21\u578b\u3002\u8be5\u6a21\u578b\u8fdb\u4e00\u6b65\u6269\u5c55\u4e3a\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578bUniPic2-Metaquery\uff0c\u6574\u5408\u4e86\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86Skywork UniPic 2.0\u8bad\u7ec3\u8303\u5f0f\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8bb8\u591a\u73b0\u6709\u7684\u5f00\u6e90\u591a\u6a21\u6001\u6a21\u578b\u8fc7\u5ea6\u4fa7\u91cd\u4e8e\u6269\u5927\u6a21\u578b\u53c2\u6570\u800c\u975e\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "method": ["\u5728SD3.5-Medium\u57fa\u7840\u4e0a\uff0c\u5bf92B\u53c2\u6570\u7684DiT\u6a21\u578bUniPic2-SD3.5M-Kontext\u8fdb\u884c\u67b6\u6784\u4fee\u6539\uff0c\u5e76\u5229\u7528\u9ad8\u8d28\u91cf\u6570\u636e\u8fdb\u884c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u4f7f\u5176\u5177\u5907\u8054\u5408\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u80fd\u529b\u3002", "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6e10\u8fdb\u5f0f\u53cc\u4efb\u52a1\u5f3a\u5316\u7b56\u7565\uff08PDTR\uff09\uff0c\u5206\u9636\u6bb5\u6709\u6548\u589e\u5f3a\u6307\u4ee4\u9075\u5faa\u548c\u7f16\u8f91\u4e00\u81f4\u6027\u3002", "\u9075\u5faaMetaQuery\u8303\u5f0f\uff0c\u901a\u8fc7\u8fde\u63a5\u5668\u5c06UniPic2-SD3.5M-Kontext\u4e0eQwen2.5-VL-7B\u8054\u5408\u8bad\u7ec3\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578bUniPic2-Metaquery\u3002"], "result": ["\u5b9e\u9a8c\u9a8c\u8bc1\u4e86PDTR\u7b56\u7565\u4e2d\u4e0d\u540c\u4efb\u52a1\u7684\u5f3a\u5316\u9636\u6bb5\u4e92\u60e0\u4e92\u5229\uff0c\u4e14\u4e0d\u4f1a\u4ea7\u751f\u8d1f\u9762\u5e72\u6270\u3002", "UniPic2-SD3.5M-Kontext\u5728\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8eBAGEL (7B)\u548cFlux-Kontext (12B)\u7b49\u53c2\u6570\u66f4\u5927\u7684\u6a21\u578b\u3002", "UniPic2-Metaquery\u901a\u8fc7\u6574\u5408\u7406\u89e3\u3001\u751f\u6210\u548c\u7f16\u8f91\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u4ee5\u7b80\u5355\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u8303\u5f0f\u5b9e\u73b0\u4e86\u9876\u5c16\u6027\u80fd\u3002"], "conclusion": "\u6240\u63d0\u51fa\u7684Skywork UniPic 2.0\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\u548c\u6a21\u578b\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5373\u4f7f\u5728\u8f83\u5c0f\u7684\u53c2\u6570\u89c4\u6a21\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u5353\u8d8a\u7684\u56fe\u50cf\u751f\u6210\u3001\u7f16\u8f91\u548c\u7edf\u4e00\u591a\u6a21\u6001\u529f\u80fd\u3002"}}
{"id": "2509.04646", "pdf": "https://arxiv.org/pdf/2509.04646", "abs": "https://arxiv.org/abs/2509.04646", "authors": ["Philippe J. Giabbanelli", "Ameeta Agrawal"], "title": "Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization", "categories": ["cs.AI", "cs.ET"], "comment": "Accepted at the AAAI 2025 Fall Symposium Series. November 6-8, 2025,\n  Arlington, VA, USA", "summary": "Modeling & Simulation (M&S) approaches such as agent-based models hold\nsignificant potential to support decision-making activities in health, with\nrecent examples including the adoption of vaccines, and a vast literature on\nhealthy eating behaviors and physical activity behaviors. These models are\npotentially usable by different stakeholder groups, as they support\npolicy-makers to estimate the consequences of potential interventions and they\ncan guide individuals in making healthy choices in complex environments.\nHowever, this potential may not be fully realized because of the models'\ncomplexity, which makes them inaccessible to the stakeholders who could benefit\nthe most. While Large Language Models (LLMs) can translate simulation outputs\nand the design of models into text, current approaches typically rely on\none-size-fits-all summaries that fail to reflect the varied informational needs\nand stylistic preferences of clinicians, policymakers, patients, caregivers,\nand health advocates. This limitation stems from a fundamental gap: we lack a\nsystematic understanding of what these stakeholders need from explanations and\nhow to tailor them accordingly. To address this gap, we present a step-by-step\nframework to identify stakeholder needs and guide LLMs in generating tailored\nexplanations of health simulations. Our procedure uses a mixed-methods design\nby first eliciting the explanation needs and stylistic preferences of diverse\nhealth stakeholders, then optimizing the ability of LLMs to generate tailored\noutputs (e.g., via controllable attribute tuning), and then evaluating through\na comprehensive range of metrics to further improve the tailored generation of\nsummaries.", "AI": {"tldr": "M&S\u6a21\u578b\u5728\u5065\u5eb7\u51b3\u7b56\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u56e0\u590d\u6742\u6027\u96be\u4ee5\u88ab\u5229\u76ca\u76f8\u5173\u8005\u7406\u89e3\u3002\u73b0\u6709LLM\u6458\u8981\u65e0\u6cd5\u6ee1\u8db3\u4e2a\u6027\u5316\u9700\u6c42\u3002\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u5206\u6b65\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u9700\u6c42\u5e76\u4f18\u5316LLM\uff0c\u4ee5\u751f\u6210\u5b9a\u5236\u5316\u7684\u5065\u5eb7\u6a21\u62df\u89e3\u91ca\u3002", "motivation": "\u5065\u5eb7\u9886\u57df\u7684\u5efa\u6a21\u4e0e\u6a21\u62df\uff08M&S\uff09\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u6a21\u578b\uff09\u5728\u652f\u6301\u51b3\u7b56\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5176\u590d\u6742\u6027\u4f7f\u5f97\u76f8\u5173\u5229\u76ca\u76f8\u5173\u8005\u96be\u4ee5\u5229\u7528\u3002\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u867d\u7136\u80fd\u5c06\u6a21\u62df\u8f93\u51fa\u7ffb\u8bd1\u6210\u6587\u672c\uff0c\u4f46\u5176\u201c\u4e00\u5200\u5207\u201d\u7684\u6458\u8981\u672a\u80fd\u6ee1\u8db3\u4e34\u5e8a\u533b\u751f\u3001\u653f\u7b56\u5236\u5b9a\u8005\u3001\u60a3\u8005\u7b49\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u591a\u6837\u5316\u7684\u4fe1\u606f\u9700\u6c42\u548c\u98ce\u683c\u504f\u597d\u3002\u8fd9\u79cd\u5c40\u9650\u6027\u6e90\u4e8e\u7f3a\u4e4f\u5bf9\u5229\u76ca\u76f8\u5173\u8005\u89e3\u91ca\u9700\u6c42\u7684\u7cfb\u7edf\u6027\u7406\u89e3\u4ee5\u53ca\u5982\u4f55\u8fdb\u884c\u5b9a\u5236\u5316\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9010\u6b65\u6846\u67b6\u6765\u8bc6\u522b\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\uff0c\u5e76\u6307\u5bfcLLM\u751f\u6210\u5065\u5eb7\u6a21\u62df\u7684\u5b9a\u5236\u5316\u89e3\u91ca\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u6df7\u5408\u8bbe\u8ba1\uff0c\u9996\u5148\u83b7\u53d6\u4e0d\u540c\u5065\u5eb7\u5229\u76ca\u76f8\u5173\u8005\u7684\u89e3\u91ca\u9700\u6c42\u548c\u98ce\u683c\u504f\u597d\uff0c\u7136\u540e\u4f18\u5316LLM\u751f\u6210\u5b9a\u5236\u5316\u8f93\u51fa\u7684\u80fd\u529b\uff08\u4f8b\u5982\uff0c\u901a\u8fc7\u53ef\u63a7\u5c5e\u6027\u8c03\u6574\uff09\uff0c\u6700\u540e\u901a\u8fc7\u5168\u9762\u7684\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff0c\u4ee5\u8fdb\u4e00\u6b65\u6539\u8fdb\u6458\u8981\u7684\u5b9a\u5236\u5316\u751f\u6210\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9010\u6b65\u6846\u67b6\uff0c\u65e8\u5728\u8bc6\u522b\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\u5e76\u6307\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5065\u5eb7\u6a21\u62df\u7684\u5b9a\u5236\u5316\u89e3\u91ca\u3002\u8be5\u6846\u67b6\u5305\u62ec\u9700\u6c42\u8bc6\u522b\u3001LLM\u4f18\u5316\u548c\u8bc4\u4f30\u7b49\u5173\u952e\u6b65\u9aa4\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e8\u5728\u89e3\u51b3\u5065\u5eb7M&S\u6a21\u578b\u56e0\u590d\u6742\u6027\u5bfc\u81f4\u7684\u4e0d\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u514b\u670dLLM\u751f\u6210\u901a\u7528\u6458\u8981\u7684\u5c40\u9650\u6027\uff0c\u4f7f\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u80fd\u83b7\u5f97\u5b9a\u5236\u5316\u3001\u6613\u4e8e\u7406\u89e3\u7684\u5065\u5eb7\u6a21\u62df\u89e3\u91ca\uff0c\u4ece\u800c\u5145\u5206\u53d1\u6325M&S\u65b9\u6cd5\u7684\u51b3\u7b56\u652f\u6301\u6f5c\u529b\u3002"}}
{"id": "2509.04544", "pdf": "https://arxiv.org/pdf/2509.04544", "abs": "https://arxiv.org/abs/2509.04544", "authors": ["Ashutosh Kumar Sinha", "Ayush Patel", "Mitul Dudhat", "Pritam Anand", "Rahul Mishra"], "title": "i-Mask: An Intelligent Mask for Breath-Driven Activity Recognition", "categories": ["cs.LG", "cs.AI"], "comment": "18 Pages, 10 Figures", "summary": "The patterns of inhalation and exhalation contain important physiological\nsignals that can be used to anticipate human behavior, health trends, and vital\nparameters. Human activity recognition (HAR) is fundamentally connected to\nthese vital signs, providing deeper insights into well-being and enabling\nreal-time health monitoring. This work presents i-Mask, a novel HAR approach\nthat leverages exhaled breath patterns captured using a custom-developed mask\nequipped with integrated sensors. Data collected from volunteers wearing the\nmask undergoes noise filtering, time-series decomposition, and labeling to\ntrain predictive models. Our experimental results validate the effectiveness of\nthe approach, achieving over 95\\% accuracy and highlighting its potential in\nhealthcare and fitness applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fai-Mask\uff0c\u4e00\u79cd\u5229\u7528\u914d\u5907\u4f20\u611f\u5668\u53e3\u7f69\u6355\u83b7\u547c\u51fa\u6c14\u4f53\u6a21\u5f0f\u8fdb\u884c\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u7684\u65b0\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\uff0c\u5728\u533b\u7597\u5065\u5eb7\u548c\u5065\u8eab\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "motivation": "\u5438\u6c14\u548c\u547c\u6c14\u6a21\u5f0f\u5305\u542b\u91cd\u8981\u7684\u751f\u7406\u4fe1\u53f7\uff0c\u53ef\u7528\u4e8e\u9884\u6d4b\u4eba\u7c7b\u884c\u4e3a\u3001\u5065\u5eb7\u8d8b\u52bf\u548c\u751f\u547d\u4f53\u5f81\u3002\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u4e0e\u8fd9\u4e9b\u751f\u547d\u4f53\u5f81\u7d27\u5bc6\u76f8\u5173\uff0c\u80fd\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u5065\u5eb7\u6d1e\u5bdf\u5e76\u5b9e\u73b0\u5b9e\u65f6\u5065\u5eb7\u76d1\u6d4b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3ai-Mask\u7684\u65b0\u578bHAR\u65b9\u6cd5\uff0c\u5229\u7528\u5b9a\u5236\u5f00\u53d1\u7684\u3001\u914d\u5907\u96c6\u6210\u4f20\u611f\u5668\u7684\u53e3\u7f69\u6355\u83b7\u547c\u51fa\u6c14\u4f53\u6a21\u5f0f\u3002\u4ece\u5fd7\u613f\u8005\u6536\u96c6\u7684\u6570\u636e\u7ecf\u8fc7\u566a\u58f0\u8fc7\u6ee4\u3001\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u548c\u6807\u6ce8\u540e\uff0c\u7528\u4e8e\u8bad\u7ec3\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "i-Mask\u65b9\u6cd5\u5728\u57fa\u4e8e\u547c\u51fa\u6c14\u4f53\u6a21\u5f0f\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u5e76\u5728\u533b\u7597\u4fdd\u5065\u548c\u5065\u8eab\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.04458", "pdf": "https://arxiv.org/pdf/2509.04458", "abs": "https://arxiv.org/abs/2509.04458", "authors": ["Daniel B. Hier", "Steven Keith Platt", "Tayo Obafemi-Ajayi"], "title": "Predicting Failures of LLMs to Link Biomedical Ontology Terms to Identifiers Evidence Across Models and Ontologies", "categories": ["cs.CL", "I.2"], "comment": "Accepted for Presentation, IEEE-EMBS International Conference on\n  Biomedical and Health Informatics (BHI 25), Atlanta GA USA, October 26-29,\n  2025", "summary": "Large language models often perform well on biomedical NLP tasks but may fail\nto link ontology terms to their correct identifiers. We investigate why these\nfailures occur by analyzing predictions across two major ontologies, Human\nPhenotype Ontology and Gene Ontology, and two high-performing models, GPT-4o\nand LLaMa 3.1 405B. We evaluate nine candidate features related to term\nfamiliarity, identifier usage, morphology, and ontology structure. Univariate\nand multivariate analyses show that exposure to ontology identifiers is the\nstrongest predictor of linking success.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66NLP\u4e2d\u94fe\u63a5\u672c\u4f53\u672f\u8bed\u5230\u6807\u8bc6\u7b26\u65f6\u53ef\u80fd\u5931\u8d25\u3002\u672c\u7814\u7a76\u5206\u6790\u5931\u8d25\u539f\u56e0\uff0c\u53d1\u73b0\u5bf9\u672c\u4f53\u6807\u8bc6\u7b26\u7684\u66b4\u9732\u662f\u94fe\u63a5\u6210\u529f\u7684\u6700\u5f3a\u9884\u6d4b\u56e0\u5b50\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u7269\u533b\u5b66\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5e38\u5e38\u65e0\u6cd5\u5c06\u672c\u4f53\u672f\u8bed\u6b63\u786e\u94fe\u63a5\u5230\u5176\u5bf9\u5e94\u7684\u6807\u8bc6\u7b26\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u8fd9\u4e9b\u5931\u8d25\u53d1\u751f\u7684\u539f\u56e0\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5206\u6790\u4e86GPT-4o\u548cLLaMa 3.1 405B\u8fd9\u4e24\u4e2a\u9ad8\u6027\u80fd\u6a21\u578b\u5728Human Phenotype Ontology\u548cGene Ontology\u8fd9\u4e24\u4e2a\u4e3b\u8981\u672c\u4f53\u4e0a\u7684\u9884\u6d4b\u7ed3\u679c\u3002\u4ed6\u4eec\u8bc4\u4f30\u4e86\u4e0e\u672f\u8bed\u719f\u6089\u5ea6\u3001\u6807\u8bc6\u7b26\u4f7f\u7528\u3001\u5f62\u6001\u5b66\u548c\u672c\u4f53\u7ed3\u6784\u76f8\u5173\u7684\u4e5d\u4e2a\u5019\u9009\u7279\u5f81\uff0c\u5e76\u8fdb\u884c\u4e86\u5355\u53d8\u91cf\u548c\u591a\u53d8\u91cf\u5206\u6790\u3002", "result": "\u5355\u53d8\u91cf\u548c\u591a\u53d8\u91cf\u5206\u6790\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u672c\u4f53\u6807\u8bc6\u7b26\u7684\u66b4\u9732\u7a0b\u5ea6\u662f\u5176\u6210\u529f\u94fe\u63a5\u672f\u8bed\u7684\u6700\u5f3a\u9884\u6d4b\u56e0\u5b50\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u6210\u529f\u5c06\u672c\u4f53\u672f\u8bed\u94fe\u63a5\u5230\u5176\u6b63\u786e\u6807\u8bc6\u7b26\uff0c\u4e3b\u8981\u53d6\u51b3\u4e8e\u6a21\u578b\u5bf9\u8fd9\u4e9b\u672c\u4f53\u6807\u8bc6\u7b26\u7684\u719f\u6089\u7a0b\u5ea6\u6216\u66b4\u9732\u7a0b\u5ea6\u3002"}}
{"id": "2509.04582", "pdf": "https://arxiv.org/pdf/2509.04582", "abs": "https://arxiv.org/abs/2509.04582", "authors": ["Jingyi Lu", "Kai Han"], "title": "Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping", "categories": ["cs.CV", "I.3.6; I.3.3"], "comment": "Accepted to ICCV 2025. Project page:\n  https://visual-ai.github.io/inpaint4drag/", "summary": "Drag-based image editing has emerged as a powerful paradigm for intuitive\nimage manipulation. However, existing approaches predominantly rely on\nmanipulating the latent space of generative models, leading to limited\nprecision, delayed feedback, and model-specific constraints. Accordingly, we\npresent Inpaint4Drag, a novel framework that decomposes drag-based editing into\npixel-space bidirectional warping and image inpainting. Inspired by elastic\nobject deformation in the physical world, we treat image regions as deformable\nmaterials that maintain natural shape under user manipulation. Our method\nachieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at\n512x512 resolution, significantly improving the interaction experience compared\nto existing methods that require minutes per edit. By transforming drag inputs\ndirectly into standard inpainting formats, our approach serves as a universal\nadapter for any inpainting model without architecture modification,\nautomatically inheriting all future improvements in inpainting technology.\nExtensive experiments demonstrate that our method achieves superior visual\nquality and precise control while maintaining real-time performance. Project\npage: https://visual-ai.github.io/inpaint4drag/", "AI": {"tldr": "Inpaint4Drag\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u62d6\u62fd\u5f0f\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7f16\u8f91\u5206\u89e3\u4e3a\u50cf\u7d20\u7a7a\u95f4\u5f62\u53d8\u548c\u56fe\u50cf\u4fee\u590d\uff0c\u514b\u670d\u4e86\u73b0\u6709\u6f5c\u5728\u7a7a\u95f4\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u8d28\u91cf\u7684\u7f16\u8f91\uff0c\u5e76\u53ef\u4f5c\u4e3a\u4efb\u4f55\u4fee\u590d\u6a21\u578b\u7684\u901a\u7528\u9002\u914d\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u62d6\u62fd\u5f0f\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u751f\u6210\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\uff0c\u5bfc\u81f4\u7cbe\u5ea6\u6709\u9650\u3001\u53cd\u9988\u5ef6\u8fdf\u4e14\u5b58\u5728\u6a21\u578b\u7279\u5f02\u6027\u7ea6\u675f\uff0c\u5f71\u54cd\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u7f16\u8f91\u6548\u7387\u3002", "method": "\u63d0\u51faInpaint4Drag\u6846\u67b6\uff0c\u5c06\u62d6\u62fd\u7f16\u8f91\u5206\u89e3\u4e3a\u50cf\u7d20\u7a7a\u95f4\u7684\u53cc\u5411\u5f62\u53d8\uff08bidirectional warping\uff09\u548c\u56fe\u50cf\u4fee\u590d\uff08image inpainting\uff09\u3002\u8be5\u65b9\u6cd5\u53d7\u7269\u7406\u4e16\u754c\u5f39\u6027\u7269\u4f53\u5f62\u53d8\u542f\u53d1\uff0c\u5c06\u56fe\u50cf\u533a\u57df\u89c6\u4e3a\u53ef\u53d8\u5f62\u6750\u6599\u3002\u901a\u8fc7\u5c06\u62d6\u62fd\u8f93\u5165\u76f4\u63a5\u8f6c\u6362\u4e3a\u6807\u51c6\u4fee\u590d\u683c\u5f0f\uff0c\u4f7f\u5176\u80fd\u4f5c\u4e3a\u4efb\u4f55\u56fe\u50cf\u4fee\u590d\u6a21\u578b\u7684\u901a\u7528\u9002\u914d\u5668\uff0c\u65e0\u9700\u4fee\u6539\u67b6\u6784\u3002", "result": "\u8be5\u65b9\u6cd5\u5728512x512\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u4e86\u5b9e\u65f6\u7684\u5f62\u53d8\u9884\u89c8\uff080.01\u79d2\uff09\u548c\u9ad8\u6548\u7684\u56fe\u50cf\u4fee\u590d\uff080.3\u79d2\uff09\uff0c\u663e\u8457\u6539\u5584\u4e86\u4ea4\u4e92\u4f53\u9a8c\uff08\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u9700\u6570\u5206\u949f\uff09\u3002\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u7cbe\u786e\u63a7\u5236\u3002", "conclusion": "Inpaint4Drag\u901a\u8fc7\u5176\u50cf\u7d20\u7a7a\u95f4\u5904\u7406\u548c\u901a\u7528\u9002\u914d\u5668\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u62d6\u62fd\u7f16\u8f91\u65b9\u6cd5\u7684\u5c40\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u65f6\u3001\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u8d28\u91cf\u4e14\u672a\u6765\u53ef\u6269\u5c55\u7684\u56fe\u50cf\u7f16\u8f91\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.04676", "pdf": "https://arxiv.org/pdf/2509.04676", "abs": "https://arxiv.org/abs/2509.04676", "authors": ["Sasha Mitts"], "title": "An Approach to Grounding AI Model Evaluations in Human-derived Criteria", "categories": ["cs.AI", "cs.HC"], "comment": "4 figures, 6 pages, presented at CHI 2025 Workshop on Human-AI\n  Interaction for Augmented Reasoning", "summary": "In the rapidly evolving field of artificial intelligence (AI), traditional\nbenchmarks can fall short in attempting to capture the nuanced capabilities of\nAI models. We focus on the case of physical world modeling and propose a novel\napproach to augment existing benchmarks with human-derived evaluation criteria,\naiming to enhance the interpretability and applicability of model behaviors.\nGrounding our study in the Perception Test and OpenEQA benchmarks, we conducted\nin-depth interviews and large-scale surveys to identify key cognitive skills,\nsuch as Prioritization, Memorizing, Discerning, and Contextualizing, that are\ncritical for both AI and human reasoning. Our findings reveal that participants\nperceive AI as lacking in interpretive and empathetic skills yet hold high\nexpectations for AI performance. By integrating insights from our findings into\nbenchmark design, we offer a framework for developing more human-aligned means\nof defining and measuring progress. This work underscores the importance of\nuser-centered evaluation in AI development, providing actionable guidelines for\nresearchers and practitioners aiming to align AI capabilities with human\ncognitive processes. Our approach both enhances current benchmarking practices\nand sets the stage for future advancements in AI model evaluation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u4eba\u7c7b\u8ba4\u77e5\u6280\u80fd\u6765\u589e\u5f3a\u73b0\u6709AI\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u63d0\u9ad8\u7269\u7406\u4e16\u754c\u5efa\u6a21AI\u7684\u89e3\u91ca\u6027\u548c\u9002\u7528\u6027\u3002", "motivation": "\u5728\u5feb\u901f\u53d1\u5c55\u7684AI\u9886\u57df\uff0c\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u96be\u4ee5\u6355\u6349AI\u6a21\u578b\u7684\u7ec6\u5fae\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u7269\u7406\u4e16\u754c\u5efa\u6a21\u65b9\u9762\u3002\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u589e\u5f3a\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u884c\u4e3a\u7684\u89e3\u91ca\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u7814\u7a76\u4ee5Perception Test\u548cOpenEQA\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u57fa\u7840\uff0c\u901a\u8fc7\u6df1\u5ea6\u8bbf\u8c08\u548c\u5927\u89c4\u6a21\u8c03\u67e5\uff0c\u8bc6\u522b\u51fa\u4f18\u5148\u7ea7\u3001\u8bb0\u5fc6\u3001\u8fa8\u522b\u548c\u60c5\u5883\u5316\u7b49\u5bf9AI\u548c\u4eba\u7c7b\u63a8\u7406\u90fd\u81f3\u5173\u91cd\u8981\u7684\u8ba4\u77e5\u6280\u80fd\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u57fa\u51c6\u8bbe\u8ba1\u4e2d\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u53c2\u4e0e\u8005\u8ba4\u4e3aAI\u7f3a\u4e4f\u89e3\u91ca\u6027\u548c\u540c\u7406\u5fc3\u6280\u80fd\uff0c\u4f46\u5bf9\u5176\u6027\u80fd\u62b1\u6709\u5f88\u9ad8\u671f\u671b\u3002\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u4eba\u7c7b\u5728\u8bc4\u4f30AI\u65f6\u5173\u6ce8\u7684\u5173\u952e\u8ba4\u77e5\u7ef4\u5ea6\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u53d1\u66f4\u7b26\u5408\u4eba\u7c7b\u6807\u51c6\u7684AI\u8fdb\u5c55\u5b9a\u4e49\u548c\u8861\u91cf\u65b9\u5f0f\u7684\u6846\u67b6\uff0c\u5f3a\u8c03\u4e86AI\u5f00\u53d1\u4e2d\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\uff0c\u4ee5\u4f7fAI\u80fd\u529b\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u5bf9\u9f50\u3002"}}
{"id": "2509.04575", "pdf": "https://arxiv.org/pdf/2509.04575", "abs": "https://arxiv.org/abs/2509.04575", "authors": ["Minqi Jiang", "Andrei Lupu", "Yoram Bachrach"], "title": "Bootstrapping Task Spaces for Self-Improvement", "categories": ["cs.LG"], "comment": null, "summary": "Progress in many task domains emerges from repeated revisions to previous\nsolution attempts. Training agents that can reliably self-improve over such\nsequences at inference-time is a natural target for reinforcement learning\n(RL), yet the naive approach assumes a fixed maximum iteration depth, which can\nbe both costly and arbitrary. We present Exploratory Iteration (ExIt), a family\nof autocurriculum RL methods that directly exploits the recurrent structure of\nself-improvement tasks to train LLMs to perform multi-step self-improvement at\ninference-time while only training on the most informative single-step\niterations. ExIt grows a task space by selectively sampling the most\ninformative intermediate, partial histories encountered during an episode for\ncontinued iteration, treating these starting points as new self-iteration task\ninstances to train a self-improvement policy. ExIt can further pair with\nexplicit exploration mechanisms to sustain greater task diversity. Across\nseveral domains, encompassing competition math, multi-turn tool-use, and\nmachine learning engineering, we demonstrate that ExIt strategies, starting\nfrom either a single or many task instances, can produce policies exhibiting\nstrong inference-time self-improvement on held-out task instances, and the\nability to iterate towards higher performance over a step budget extending\nbeyond the average iteration depth encountered during training.", "AI": {"tldr": "ExIt\u662f\u4e00\u79cd\u81ea\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u4fe1\u606f\u91cf\u5927\u7684\u5355\u6b65\u8fed\u4ee3\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u65f6\u80fd\u8fdb\u884c\u591a\u6b65\u81ea\u6211\u6539\u8fdb\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u5c55\u73b0\u51fa\u8d85\u8d8a\u8bad\u7ec3\u6df1\u5ea6\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u5728\u8bad\u7ec3\u667a\u80fd\u4f53\u8fdb\u884c\u63a8\u7406\u65f6\u591a\u6b65\u81ea\u6211\u6539\u8fdb\u65f6\uff0c\u901a\u5e38\u5047\u8bbe\u4e00\u4e2a\u56fa\u5b9a\u7684\u6700\u5927\u8fed\u4ee3\u6df1\u5ea6\uff0c\u8fd9\u79cd\u65b9\u6cd5\u65e2\u6602\u8d35\u53c8\u6b66\u65ad\u3002", "method": "\u672c\u6587\u63d0\u51fa\u63a2\u7d22\u6027\u8fed\u4ee3\uff08ExIt\uff09\uff0c\u4e00\u79cd\u81ea\u8bfe\u7a0bRL\u65b9\u6cd5\u3002ExIt\u5229\u7528\u81ea\u6211\u6539\u8fdb\u4efb\u52a1\u7684\u5faa\u73af\u7ed3\u6784\uff0c\u901a\u8fc7\u53ea\u8bad\u7ec3\u6700\u6709\u4fe1\u606f\u91cf\u7684\u5355\u6b65\u8fed\u4ee3\u6765\u8bad\u7ec3LLMs\u6267\u884c\u591a\u6b65\u81ea\u6211\u6539\u8fdb\u3002ExIt\u901a\u8fc7\u9009\u62e9\u6027\u91c7\u6837\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u6700\u5177\u4fe1\u606f\u91cf\u7684\u4e2d\u95f4\u3001\u90e8\u5206\u5386\u53f2\u4f5c\u4e3a\u65b0\u7684\u81ea\u6211\u8fed\u4ee3\u4efb\u52a1\u5b9e\u4f8b\u6765\u6269\u5c55\u4efb\u52a1\u7a7a\u95f4\uff0c\u5e76\u53ef\u8fdb\u4e00\u6b65\u7ed3\u5408\u663e\u5f0f\u63a2\u7d22\u673a\u5236\u4ee5\u7ef4\u6301\u66f4\u5927\u7684\u4efb\u52a1\u591a\u6837\u6027\u3002", "result": "\u5728\u7ade\u4e89\u6570\u5b66\u3001\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u548c\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u7b49\u591a\u4e2a\u9886\u57df\uff0cExIt\u7b56\u7565\uff08\u65e0\u8bba\u662f\u4ece\u5355\u4e2a\u8fd8\u662f\u591a\u4e2a\u4efb\u52a1\u5b9e\u4f8b\u5f00\u59cb\uff09\u90fd\u80fd\u5728\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u5b9e\u4f8b\u4e0a\u4ea7\u751f\u5f3a\u5927\u7684\u63a8\u7406\u65f6\u81ea\u6211\u6539\u8fdb\u7b56\u7565\uff0c\u5e76\u80fd\u591f\u5728\u8d85\u51fa\u8bad\u7ec3\u671f\u95f4\u5e73\u5747\u8fed\u4ee3\u6df1\u5ea6\u7684\u6b65\u957f\u9884\u7b97\u5185\uff0c\u6301\u7eed\u8fed\u4ee3\u4ee5\u83b7\u5f97\u66f4\u9ad8\u6027\u80fd\u3002", "conclusion": "ExIt\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u4f7fLLMs\u5728\u591a\u4efb\u52a1\u9886\u57df\u5b9e\u73b0\u9ad8\u6548\u3001\u7a33\u5065\u7684\u591a\u6b65\u81ea\u6211\u6539\u8fdb\uff0c\u5e76\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u8fed\u4ee3\u6df1\u5ea6\u4e0a\u7684\u9650\u5236\u3002"}}
{"id": "2509.04459", "pdf": "https://arxiv.org/pdf/2509.04459", "abs": "https://arxiv.org/abs/2509.04459", "authors": ["Shiqin Han", "Manning Gao", "Menghua Jiang", "Yuncheng Jiang", "Haifeng Hu", "Sijie Mai"], "title": "Uncertainty-Aware Collaborative System of Large and Small Models for Multimodal Sentiment Analysis", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The advent of Multimodal Large Language Models (MLLMs) has significantly\nadvanced the state-of-the-art in multimodal machine learning, yet their\nsubstantial computational demands present a critical barrier to real-world\ndeployment. Conversely, smaller, specialized models offer high efficiency but\noften at the cost of performance. To reconcile this performance-efficiency\ntrade-off, we propose a novel Uncertainty-Aware Collaborative System (U-ACS)\nthat synergistically orchestrates a powerful MLLM (e.g., HumanOmni) and a\nlightweight baseline model for multimodal sentiment analysis. The core of our\nsystem is an uncertainty-driven cascade mechanism, where the efficient small\nmodel first acts as a rapid filter for all input samples. Only those samples\nyielding high predictive uncertainty, thereby indicating greater difficulty,\nare selectively escalated to the MLLM for more sophisticated analysis.\nFurthermore, our system introduces advanced strategies to handle ambiguous or\nconflicting predictions, including weighted averaging for predictions of\nsimilar polarity and a prompt-based cross-verification to resolve conflicting\npredictions when both models exhibit high uncertainty. This\nsample-difficulty-aware approach allows for a dynamic allocation of\ncomputational resources, drastically reducing inference costs while retaining\nthe high accuracy of MLLM. Extensive experiments on benchmark datasets\ndemonstrate that our proposed method achieves state-of-the-art performance,\nwhile requiring only a fraction of the computational resources compared to\nusing a standalone MLLM.", "AI": {"tldr": "\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u534f\u4f5c\u7cfb\u7edfU-ACS\uff0c\u901a\u8fc7\u7ea7\u8054\u673a\u5236\u5c06\u96be\u6837\u672c\u5206\u914d\u7ed9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u4ee5\u5728\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u517c\u987eMLLM\u7684\u9ad8\u6027\u80fd\u548c\u5c0f\u578b\u6a21\u578b\u7684\u9ad8\u6548\u7387\uff0c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u963b\u788d\u5b9e\u9645\u90e8\u7f72\uff1b\u5c0f\u578b\u4e13\u4e1a\u6a21\u578b\u6548\u7387\u9ad8\u4f46\u6027\u80fd\u5dee\u3002\u9700\u8981\u89e3\u51b3\u6027\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u534f\u4f5c\u7cfb\u7edf\uff08U-ACS\uff09\u3002\u6838\u5fc3\u662f\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u7ea7\u8054\u673a\u5236\uff1a\u9ad8\u6548\u5c0f\u578b\u6a21\u578b\u9996\u5148\u5feb\u901f\u7b5b\u9009\u6240\u6709\u8f93\u5165\u6837\u672c\uff0c\u4ec5\u5c06\u9ad8\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff08\u5373\u96be\u5ea6\u8f83\u5927\uff09\u7684\u6837\u672c\u9009\u62e9\u6027\u5730\u8f6c\u4ea4\u7ed9\u5f3a\u5927\u7684MLLM\u8fdb\u884c\u66f4\u590d\u6742\u7684\u5206\u6790\u3002\u6b64\u5916\uff0c\u7cfb\u7edf\u5f15\u5165\u9ad8\u7ea7\u7b56\u7565\u5904\u7406\u6a21\u7cca\u6216\u51b2\u7a81\u9884\u6d4b\uff0c\u5305\u62ec\u540c\u6781\u6027\u9884\u6d4b\u7684\u52a0\u6743\u5e73\u5747\u548c\u9ad8\u4e0d\u786e\u5b9a\u6027\u51b2\u7a81\u9884\u6d4b\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u4ea4\u53c9\u9a8c\u8bc1\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4e0e\u5355\u72ec\u4f7f\u7528MLLM\u76f8\u6bd4\uff0c\u4ec5\u9700\u4e00\u5c0f\u90e8\u5206\u8ba1\u7b97\u8d44\u6e90\uff0c\u540c\u65f6\u4fdd\u6301\u4e86MLLM\u7684\u9ad8\u51c6\u786e\u6027\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u57fa\u4e8e\u6837\u672c\u96be\u5ea6\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd-\u6548\u7387\u6743\u8861\u95ee\u9898\uff0c\u5728\u5b9e\u73b0\u9ad8\u51c6\u786e\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2509.04597", "pdf": "https://arxiv.org/pdf/2509.04597", "abs": "https://arxiv.org/abs/2509.04597", "authors": ["Jin Ma", "Mohammed Aldeen", "Christopher Salas", "Feng Luo", "Mashrur Chowdhury", "Mert Pes\u00e9", "Long Cheng"], "title": "DisPatch: Disarming Adversarial Patches in Object Detection with Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Object detection is fundamental to various real-world applications, such as\nsecurity monitoring and surveillance video analysis. Despite their\nadvancements, state-of-theart object detectors are still vulnerable to\nadversarial patch attacks, which can be easily applied to real-world objects to\neither conceal actual items or create non-existent ones, leading to severe\nconsequences. Given the current diversity of adversarial patch attacks and\npotential unknown threats, an ideal defense method should be effective,\ngeneralizable, and robust against adaptive attacks. In this work, we introduce\nDISPATCH, the first diffusion-based defense framework for object detection.\nUnlike previous works that aim to \"detect and remove\" adversarial patches,\nDISPATCH adopts a \"regenerate and rectify\" strategy, leveraging generative\nmodels to disarm attack effects while preserving the integrity of the input\nimage. Specifically, we utilize the in-distribution generative power of\ndiffusion models to regenerate the entire image, aligning it with benign data.\nA rectification process is then employed to identify and replace adversarial\nregions with their regenerated benign counterparts. DISPATCH is attack-agnostic\nand requires no prior knowledge of the existing patches. Extensive experiments\nacross multiple detectors and attacks demonstrate that DISPATCH consistently\noutperforms state-of-the-art defenses on both hiding attacks and creating\nattacks, achieving the best overall mAP.5 score of 89.3% on hiding attacks, and\nlowering the attack success rate to 24.8% on untargeted creating attacks.\nMoreover, it maintains strong robustness against adaptive attacks, making it a\npractical and reliable defense for object detection systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDISPATCH\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7269\u4f53\u68c0\u6d4b\u9632\u5fa1\u6846\u67b6\uff0c\u91c7\u7528\u201c\u518d\u751f\u548c\u4fee\u6b63\u201d\u7b56\u7565\u5bf9\u6297\u5bf9\u6297\u8865\u4e01\u653b\u51fb\uff0c\u5728\u591a\u79cd\u653b\u51fb\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\uff0c\u4e14\u5bf9\u81ea\u9002\u5e94\u653b\u51fb\u8868\u73b0\u51fa\u5f3a\u5927\u9c81\u68d2\u6027\u3002", "motivation": "\u7269\u4f53\u68c0\u6d4b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u5bf9\u6297\u8865\u4e01\u653b\u51fb\u7684\u5a01\u80c1\uff0c\u8fd9\u4e9b\u653b\u51fb\u53ef\u80fd\u9690\u85cf\u7269\u4f53\u6216\u5236\u9020\u865a\u5047\u7269\u4f53\uff0c\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u591a\u6837\u5316\u3001\u672a\u77e5\u4e14\u6f5c\u5728\u7684\u653b\u51fb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u3001\u901a\u7528\u4e14\u9c81\u68d2\u7684\u9632\u5fa1\u65b9\u6848\u3002", "method": "\u672c\u6587\u5f15\u5165DISPATCH\uff0c\u9996\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7269\u4f53\u68c0\u6d4b\u9632\u5fa1\u6846\u67b6\u3002\u5b83\u6452\u5f03\u4e86\u4f20\u7edf\u7684\u201c\u68c0\u6d4b\u5e76\u79fb\u9664\u201d\u7b56\u7565\uff0c\u8f6c\u800c\u91c7\u7528\u201c\u518d\u751f\u548c\u4fee\u6b63\u201d\u7b56\u7565\u3002\u5177\u4f53\u800c\u8a00\uff0cDISPATCH\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u540c\u5206\u5e03\u751f\u6210\u80fd\u529b\uff0c\u91cd\u65b0\u751f\u6210\u6574\u4e2a\u56fe\u50cf\u4ee5\u6062\u590d\u826f\u6027\u6570\u636e\u72b6\u6001\uff0c\u7136\u540e\u901a\u8fc7\u4e00\u4e2a\u4fee\u6b63\u8fc7\u7a0b\u8bc6\u522b\u5e76\u66ff\u6362\u56fe\u50cf\u4e2d\u7684\u5bf9\u6297\u533a\u57df\u4e3a\u5176\u5bf9\u5e94\u7684\u826f\u6027\u518d\u751f\u7248\u672c\u3002\u8be5\u65b9\u6cd5\u4e0e\u653b\u51fb\u7c7b\u578b\u65e0\u5173\uff0c\u65e0\u9700\u9884\u77e5\u8865\u4e01\u4fe1\u606f\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0cDISPATCH\u5728\u9690\u85cf\u653b\u51fb\u548c\u521b\u5efa\u653b\u51fb\u4e0a\u5747\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u9632\u5fa1\u65b9\u6cd5\u3002\u5728\u9690\u85cf\u653b\u51fb\u4e0a\uff0c\u5176\u6574\u4f53mAP.5\u5f97\u5206\u8fbe\u523089.3%\uff1b\u5728\u975e\u5b9a\u5411\u521b\u5efa\u653b\u51fb\u4e0a\uff0c\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e\u81f324.8%\u3002\u6b64\u5916\uff0cDISPATCH\u5bf9\u81ea\u9002\u5e94\u653b\u51fb\u4e5f\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "DISPATCH\u901a\u8fc7\u521b\u65b0\u7684\u201c\u518d\u751f\u548c\u4fee\u6b63\u201d\u7b56\u7565\uff0c\u4e3a\u7269\u4f53\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u9760\u7684\u9632\u5fa1\u65b9\u6cd5\u3002\u5b83\u80fd\u591f\u6709\u6548\u62b5\u5fa1\u591a\u6837\u5316\u7684\u5bf9\u6297\u8865\u4e01\u653b\u51fb\uff0c\u5e76\u5728\u6027\u80fd\u548c\u5bf9\u81ea\u9002\u5e94\u653b\u51fb\u7684\u9c81\u68d2\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2509.04731", "pdf": "https://arxiv.org/pdf/2509.04731", "abs": "https://arxiv.org/abs/2509.04731", "authors": ["Brennen Hill"], "title": "Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO", "68T05, 90C40, 91A26, 68T42, 93E35", "I.2.11; I.2.6; I.2.8; I.2.9; I.2.7"], "comment": null, "summary": "The convergence of Language models, Agent models, and World models represents\na critical frontier for artificial intelligence. While recent progress has\nfocused on scaling Language and Agent models, the development of sophisticated,\nexplicit World Models remains a key bottleneck, particularly for complex,\nlong-horizon multi-agent tasks. In domains such as robotic soccer, agents\ntrained via standard reinforcement learning in high-fidelity but\nstructurally-flat simulators often fail due to intractable exploration spaces\nand sparse rewards. This position paper argues that the next frontier in\ndeveloping capable agents lies in creating environments that possess an\nexplicit, hierarchical World Model. We contend that this is best achieved\nthrough hierarchical scaffolding, where complex goals are decomposed into\nstructured, manageable subgoals. Drawing evidence from a systematic review of\n2024 research in multi-agent soccer, we identify a clear and decisive trend\ntowards integrating symbolic and hierarchical methods with multi-agent\nreinforcement learning (MARL). These approaches implicitly or explicitly\nconstruct a task-based world model to guide agent learning. We then propose a\nparadigm shift: leveraging Large Language Models to dynamically generate this\nhierarchical scaffold, effectively using language to structure the World Model\non the fly. This language-driven world model provides an intrinsic curriculum,\ndense and meaningful learning signals, and a framework for compositional\nlearning, enabling Agent Models to acquire sophisticated, strategic behaviors\nwith far greater sample efficiency. By building environments with explicit,\nlanguage-configurable task layers, we can bridge the gap between low-level\nreactive behaviors and high-level strategic team play, creating a powerful and\ngeneralizable framework for training the next generation of intelligent agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u52a8\u6001\u751f\u6210\u5206\u5c42\u652f\u67b6\uff0c\u6784\u5efa\u5177\u6709\u663e\u5f0f\u3001\u5206\u5c42\u4e16\u754c\u6a21\u578b\u7684\u73af\u5883\uff0c\u4ee5\u514b\u670d\u590d\u6742\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u63a2\u7d22\u7a7a\u95f4\u5927\u548c\u5956\u52b1\u7a00\u758f\u7684\u6311\u6218\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u667a\u80fd\u4f53\u5b66\u4e60\u6548\u7387\u548c\u6218\u7565\u884c\u4e3a\u80fd\u529b\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u3001\u667a\u80fd\u4f53\u6a21\u578b\u548c\u4e16\u754c\u6a21\u578b\u7684\u878d\u5408\u662f\u4eba\u5de5\u667a\u80fd\u7684\u5173\u952e\u524d\u6cbf\uff0c\u4f46\u590d\u6742\u3001\u663e\u5f0f\u7684\u4e16\u754c\u6a21\u578b\u4ecd\u662f\u74f6\u9888\uff0c\u5c24\u5176\u5728\u590d\u6742\u3001\u957f\u671f\u7684\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\u3002\u5728\u5982\u673a\u5668\u4eba\u8db3\u7403\u7b49\u9886\u57df\uff0c\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u4e2d\u7684\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u56e0\u63a2\u7d22\u7a7a\u95f4\u96be\u4ee5\u5904\u7406\u548c\u5956\u52b1\u7a00\u758f\u800c\u5931\u8d25\u3002", "method": "\u672c\u6587\u4e3b\u5f20\u901a\u8fc7\u5206\u5c42\u652f\u67b6\uff08\u5c06\u590d\u6742\u76ee\u6807\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u7684\u5b50\u76ee\u6807\uff09\u521b\u5efa\u5177\u6709\u663e\u5f0f\u3001\u5206\u5c42\u4e16\u754c\u6a21\u578b\u7684\u73af\u5883\u3002\u5e76\u63d0\u51fa\u4e00\u4e2a\u8303\u5f0f\u8f6c\u53d8\uff1a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u52a8\u6001\u751f\u6210\u8fd9\u79cd\u5206\u5c42\u652f\u67b6\uff0c\u5373\u65f6\u5730\u7528\u8bed\u8a00\u6765\u6784\u5efa\u4e16\u754c\u6a21\u578b\u3002\u8fd9\u79cd\u8bed\u8a00\u9a71\u52a8\u7684\u4e16\u754c\u6a21\u578b\u63d0\u4f9b\u4e86\u5185\u5728\u8bfe\u7a0b\u3001\u5bc6\u96c6\u4e14\u6709\u610f\u4e49\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u4ee5\u53ca\u7ec4\u5408\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5bf92024\u5e74\u591a\u667a\u80fd\u4f53\u8db3\u7403\u7814\u7a76\u7684\u7cfb\u7edf\u56de\u987e\u663e\u793a\uff0c\u5c06\u7b26\u53f7\u548c\u5206\u5c42\u65b9\u6cd5\u4e0e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u96c6\u6210\u4ee5\u9690\u5f0f\u6216\u663e\u5f0f\u6784\u5efa\u57fa\u4e8e\u4efb\u52a1\u7684\u4e16\u754c\u6a21\u578b\uff0c\u5df2\u6210\u4e3a\u660e\u786e\u8d8b\u52bf\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7f\u667a\u80fd\u4f53\u6a21\u578b\u80fd\u591f\u4ee5\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u83b7\u5f97\u590d\u6742\u7684\u6218\u7565\u884c\u4e3a\uff0c\u5e76\u5f25\u5408\u4e86\u4f4e\u7ea7\u53cd\u5e94\u884c\u4e3a\u548c\u9ad8\u7ea7\u6218\u7565\u56e2\u961f\u534f\u4f5c\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u5177\u6709\u663e\u5f0f\u3001\u8bed\u8a00\u53ef\u914d\u7f6e\u4efb\u52a1\u5c42\u7684\u73af\u5883\uff0c\u80fd\u591f\u4e3a\u8bad\u7ec3\u4e0b\u4e00\u4ee3\u667a\u80fd\u4f53\u63d0\u4f9b\u4e00\u4e2a\u5f3a\u5927\u4e14\u53ef\u6cdb\u5316\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u5f53\u524d\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u7684\u6311\u6218\uff0c\u5e76\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u66f4\u590d\u6742\u7684\u667a\u80fd\u884c\u4e3a\u5b66\u4e60\u3002"}}
{"id": "2509.04583", "pdf": "https://arxiv.org/pdf/2509.04583", "abs": "https://arxiv.org/abs/2509.04583", "authors": ["Jiequn Han", "Kui Ren", "Nathan Soedjak"], "title": "Instance-Wise Adaptive Sampling for Dataset Construction in Approximating Inverse Problem Solutions", "categories": ["cs.LG", "cs.NA", "math.NA", "physics.comp-ph"], "comment": null, "summary": "We propose an instance-wise adaptive sampling framework for constructing\ncompact and informative training datasets for supervised learning of inverse\nproblem solutions. Typical learning-based approaches aim to learn a\ngeneral-purpose inverse map from datasets drawn from a prior distribution, with\nthe training process independent of the specific test instance. When the prior\nhas a high intrinsic dimension or when high accuracy of the learned solution is\nrequired, a large number of training samples may be needed, resulting in\nsubstantial data collection costs. In contrast, our method dynamically\nallocates sampling effort based on the specific test instance, enabling\nsignificant gains in sample efficiency. By iteratively refining the training\ndataset conditioned on the latest prediction, the proposed strategy tailors the\ndataset to the geometry of the inverse map around each test instance. We\ndemonstrate the effectiveness of our approach in the inverse scattering problem\nunder two types of structured priors. Our results show that the advantage of\nthe adaptive method becomes more pronounced in settings with more complex\npriors or higher accuracy requirements. While our experiments focus on a\nparticular inverse problem, the adaptive sampling strategy is broadly\napplicable and readily extends to other inverse problems, offering a scalable\nand practical alternative to conventional fixed-dataset training regimes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5b9e\u4f8b\u81ea\u9002\u5e94\u91c7\u6837\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u9006\u95ee\u9898\u6c42\u89e3\u6784\u5efa\u7d27\u51d1\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4ee5\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5b66\u4e60\u7684\u9006\u95ee\u9898\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u5148\u9a8c\u6216\u9700\u8981\u9ad8\u7cbe\u5ea6\u65f6\uff0c\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6837\u672c\uff0c\u5bfc\u81f4\u9ad8\u6602\u7684\u6570\u636e\u6536\u96c6\u6210\u672c\u3002", "method": "\u8be5\u65b9\u6cd5\u6839\u636e\u7279\u5b9a\u7684\u6d4b\u8bd5\u5b9e\u4f8b\u52a8\u6001\u5206\u914d\u91c7\u6837\u5de5\u4f5c\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u6700\u65b0\u9884\u6d4b\u8fed\u4ee3\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u4f7f\u5176\u9002\u5e94\u6bcf\u4e2a\u6d4b\u8bd5\u5b9e\u4f8b\u5468\u56f4\u9006\u6620\u5c04\u7684\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5728\u9006\u6563\u5c04\u95ee\u9898\u4e2d\uff0c\u8be5\u65b9\u6cd5\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\uff0c\u4e14\u5728\u5148\u9a8c\u66f4\u590d\u6742\u6216\u7cbe\u5ea6\u8981\u6c42\u66f4\u9ad8\u7684\u573a\u666f\u4e2d\uff0c\u81ea\u9002\u5e94\u65b9\u6cd5\u7684\u4f18\u52bf\u66f4\u52a0\u660e\u663e\u3002", "conclusion": "\u8be5\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u53ef\u6269\u5c55\u5230\u5176\u4ed6\u9006\u95ee\u9898\uff0c\u4e3a\u4f20\u7edf\u7684\u56fa\u5b9a\u6570\u636e\u96c6\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.04460", "pdf": "https://arxiv.org/pdf/2509.04460", "abs": "https://arxiv.org/abs/2509.04460", "authors": ["Yihan Chen", "Jiawei Chen", "Guozhao Mo", "Xuanang Chen", "Ben He", "Xianpei Han", "Le Sun"], "title": "CoCoNUTS: Concentrating on Content while Neglecting Uninformative Textual Styles for AI-Generated Peer Review Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The growing integration of large language models (LLMs) into the peer review\nprocess presents potential risks to the fairness and reliability of scholarly\nevaluation. While LLMs offer valuable assistance for reviewers with language\nrefinement, there is growing concern over their use to generate substantive\nreview content. Existing general AI-generated text detectors are vulnerable to\nparaphrasing attacks and struggle to distinguish between surface language\nrefinement and substantial content generation, suggesting that they primarily\nrely on stylistic cues. When applied to peer review, this limitation can result\nin unfairly suspecting reviews with permissible AI-assisted language\nenhancement, while failing to catch deceptively humanized AI-generated reviews.\nTo address this, we propose a paradigm shift from style-based to content-based\ndetection. Specifically, we introduce CoCoNUTS, a content-oriented benchmark\nbuilt upon a fine-grained dataset of AI-generated peer reviews, covering six\ndistinct modes of human-AI collaboration. Furthermore, we develop CoCoDet, an\nAI review detector via a multi-task learning framework, designed to achieve\nmore accurate and robust detection of AI involvement in review content. Our\nwork offers a practical foundation for evaluating the use of LLMs in peer\nreview, and contributes to the development of more precise, equitable, and\nreliable detection methods for real-world scholarly applications. Our code and\ndata will be publicly available at https://github.com/Y1hanChen/COCONUTS.", "AI": {"tldr": "\u4e3a\u89e3\u51b3LLMs\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u751f\u6210\u5b9e\u8d28\u5185\u5bb9\u5e26\u6765\u7684\u516c\u5e73\u6027\u98ce\u9669\uff0c\u672c\u6587\u63d0\u51faCoCoNUTS\u57fa\u51c6\u548c\u57fa\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u7684CoCoDet\u68c0\u6d4b\u5668\uff0c\u65e8\u5728\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u57fa\u4e8e\u5185\u5bb9\u7684AI\u8bc4\u5ba1\u68c0\u6d4b\uff0c\u800c\u975e\u4f9d\u8d56\u6613\u53d7\u653b\u51fb\u7684\u98ce\u683c\u7279\u5f81\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65e5\u76ca\u878d\u5165\u540c\u884c\u8bc4\u5ba1\u8fc7\u7a0b\uff0c\u867d\u6709\u52a9\u4e8e\u8bed\u8a00\u6da6\u8272\uff0c\u4f46\u5176\u751f\u6210\u5b9e\u8d28\u8bc4\u5ba1\u5185\u5bb9\u7684\u6f5c\u5728\u6ee5\u7528\u5f15\u53d1\u62c5\u5fe7\uff0c\u53ef\u80fd\u635f\u5bb3\u5b66\u672f\u8bc4\u4f30\u7684\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027\u3002\u73b0\u6709AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u5668\u4f9d\u8d56\u98ce\u683c\u7279\u5f81\uff0c\u6613\u53d7\u89c4\u907f\u653b\u51fb\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u533a\u5206\u5141\u8bb8\u7684\u8bed\u8a00\u6da6\u8272\u4e0e\u5b9e\u8d28\u6027\u5185\u5bb9\u751f\u6210\uff0c\u5bfc\u81f4\u68c0\u6d4b\u4e0d\u51c6\u786e\u548c\u8bef\u5224\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4ece\u57fa\u4e8e\u98ce\u683c\u5230\u57fa\u4e8e\u5185\u5bb9\u7684\u68c0\u6d4b\u8303\u5f0f\u8f6c\u53d8\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6784\u5efa\u4e86CoCoNUTS\uff0c\u4e00\u4e2a\u5185\u5bb9\u5bfc\u5411\u7684\u57fa\u51c6\uff0c\u5176\u5efa\u7acb\u5728\u5305\u542b\u516d\u79cd\u4eba\u673a\u534f\u4f5c\u6a21\u5f0f\u7684\u7ec6\u7c92\u5ea6AI\u751f\u6210\u540c\u884c\u8bc4\u5ba1\u6570\u636e\u96c6\u4e4b\u4e0a\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86CoCoDet\uff0c\u4e00\u4e2a\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u7684AI\u8bc4\u5ba1\u68c0\u6d4b\u5668\uff0c\u65e8\u5728\u66f4\u51c6\u786e\u3001\u9c81\u68d2\u5730\u68c0\u6d4bAI\u5728\u8bc4\u5ba1\u5185\u5bb9\u4e2d\u7684\u53c2\u4e0e\u3002", "result": "\u672c\u5de5\u4f5c\u63d0\u4f9b\u4e86CoCoNUTS\u57fa\u51c6\u548cCoCoDet\u68c0\u6d4b\u5668\uff0c\u4e3a\u8bc4\u4f30LLMs\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u4f7f\u7528\u5960\u5b9a\u4e86\u5b9e\u8df5\u57fa\u7840\uff0c\u5e76\u4fc3\u8fdb\u4e86\u5f00\u53d1\u66f4\u7cbe\u786e\u3001\u516c\u5e73\u3001\u53ef\u9760\u7684AI\u8bc4\u5ba1\u5185\u5bb9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5b66\u672f\u5e94\u7528\u9700\u6c42\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u540c\u884c\u8bc4\u5ba1\u4e2dLLMs\u7684\u5e94\u7528\u8bc4\u4f30\u5960\u5b9a\u4e86\u5b9e\u8df5\u57fa\u7840\uff0c\u5e76\u6709\u52a9\u4e8e\u63a8\u52a8\u9762\u5411\u771f\u5b9e\u5b66\u672f\u5e94\u7528\u7684\u66f4\u7cbe\u786e\u3001\u516c\u5e73\u3001\u53ef\u9760\u7684\u68c0\u6d4b\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u4ece\u800c\u63d0\u5347\u5b66\u672f\u8bc4\u4f30\u7684\u516c\u6b63\u6027\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2509.04600", "pdf": "https://arxiv.org/pdf/2509.04600", "abs": "https://arxiv.org/abs/2509.04600", "authors": ["Qijun Ying", "Zhongyuan Hu", "Rui Zhang", "Ronghui Li", "Yu Lu", "Zijiao Zeng"], "title": "WATCH: World-aware Allied Trajectory and pose reconstruction for Camera and Human", "categories": ["cs.CV"], "comment": null, "summary": "Global human motion reconstruction from in-the-wild monocular videos is\nincreasingly demanded across VR, graphics, and robotics applications, yet\nrequires accurate mapping of human poses from camera to world coordinates-a\ntask challenged by depth ambiguity, motion ambiguity, and the entanglement\nbetween camera and human movements. While human-motion-centric approaches excel\nin preserving motion details and physical plausibility, they suffer from two\ncritical limitations: insufficient exploitation of camera orientation\ninformation and ineffective integration of camera translation cues. We present\nWATCH (World-aware Allied Trajectory and pose reconstruction for Camera and\nHuman), a unified framework addressing both challenges. Our approach introduces\nan analytical heading angle decomposition technique that offers superior\nefficiency and extensibility compared to existing geometric methods.\nAdditionally, we design a camera trajectory integration mechanism inspired by\nworld models, providing an effective pathway for leveraging camera translation\ninformation beyond naive hard-decoding approaches. Through experiments on\nin-the-wild benchmarks, WATCH achieves state-of-the-art performance in\nend-to-end trajectory reconstruction. Our work demonstrates the effectiveness\nof jointly modeling camera-human motion relationships and offers new insights\nfor addressing the long-standing challenge of camera translation integration in\nglobal human motion reconstruction. The code will be available publicly.", "AI": {"tldr": "WATCH\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u76f8\u673a\u671d\u5411\u5206\u89e3\u548c\u8f68\u8ff9\u96c6\u6210\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u9ad8\u7cbe\u5ea6\u5168\u5c40\u4eba\u4f53\u8fd0\u52a8\u91cd\u5efa\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4ece\u91ce\u5916\u5355\u76ee\u89c6\u9891\u4e2d\u8fdb\u884c\u5168\u5c40\u4eba\u4f53\u8fd0\u52a8\u91cd\u5efa\u5728VR\u3001\u56fe\u5f62\u548c\u673a\u5668\u4eba\u9886\u57df\u9700\u6c42\u65fa\u76db\uff0c\u4f46\u9762\u4e34\u6df1\u5ea6\u6a21\u7cca\u3001\u8fd0\u52a8\u6a21\u7cca\u4ee5\u53ca\u76f8\u673a\u4e0e\u4eba\u4f53\u8fd0\u52a8\u8026\u5408\u7684\u6311\u6218\u3002\u73b0\u6709\u4ee5\u4eba\u4f53\u8fd0\u52a8\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\u5728\u5229\u7528\u76f8\u673a\u671d\u5411\u4fe1\u606f\u548c\u6709\u6548\u96c6\u6210\u76f8\u673a\u5e73\u79fb\u7ebf\u7d22\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86WATCH\uff08World-aware Allied Trajectory and pose reconstruction for Camera and Human\uff09\u7edf\u4e00\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u5206\u6790\u6027\u822a\u5411\u89d2\u5206\u89e3\u6280\u672f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53d7\u4e16\u754c\u6a21\u578b\u542f\u53d1\u7684\u76f8\u673a\u8f68\u8ff9\u96c6\u6210\u673a\u5236\uff0c\u4ee5\u6709\u6548\u5229\u7528\u76f8\u673a\u5e73\u79fb\u4fe1\u606f\u3002", "result": "\u5728\u91ce\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cWATCH\u5728\u7aef\u5230\u7aef\u8f68\u8ff9\u91cd\u5efa\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u8bc1\u660e\u4e86\u8054\u5408\u5efa\u6a21\u76f8\u673a\u4e0e\u4eba\u4f53\u8fd0\u52a8\u5173\u7cfb\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u89e3\u51b3\u5168\u5c40\u4eba\u4f53\u8fd0\u52a8\u91cd\u5efa\u4e2d\u76f8\u673a\u5e73\u79fb\u96c6\u6210\u8fd9\u4e00\u957f\u671f\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2509.04791", "pdf": "https://arxiv.org/pdf/2509.04791", "abs": "https://arxiv.org/abs/2509.04791", "authors": ["Yuan Sui", "Yanming Zhang", "Yi Liao", "Yu Gu", "Guohua Tang", "Zhongqian Sun", "Wei Yang", "Bryan Hooi"], "title": "What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking", "categories": ["cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2508.21365", "summary": "Large language models (LLMs) excel at processing information reactively but\nlack the ability to systemically explore hypothetical futures. They cannot ask,\n\"what if we take this action? how will it affect the final outcome\" and\nforecast its potential consequences before acting. This critical gap limits\ntheir utility in dynamic, high-stakes scenarios like strategic planning, risk\nassessment, and real-time decision making. To bridge this gap, we propose\nWiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities.\nOur approach integrates What-If Analysis (WIA), a systematic approach for\nevaluating hypothetical scenarios by changing input variables. By leveraging\nenvironmental feedback via reinforcement learning, WiA-LLM moves beyond\nreactive thinking. It dynamically simulates the outcomes of each potential\naction, enabling the model to anticipate future states rather than merely react\nto the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a\ncomplex multiplayer game environment characterized by rapid state changes and\nintricate interactions. The game's real-time state changes require precise\nmulti-step consequence prediction, making it an ideal testbed for our approach.\nExperimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy\nin forecasting game-state changes (up to two times gain over baselines). The\nmodel shows particularly significant gains in high-difficulty scenarios where\naccurate foresight is critical. To our knowledge, this is the first work to\nformally explore and integrate what-if analysis capabilities within LLMs.\nWiA-LLM represents a fundamental advance toward proactive reasoning in LLMs,\nproviding a scalable framework for robust decision-making in dynamic\nenvironments with broad implications for strategic applications.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7f3a\u4e4f\u9884\u6d4b\u672a\u6765\u884c\u52a8\u540e\u679c\u7684\u80fd\u529b\uff0c\u672c\u6587\u63d0\u51fa\u4e86WiA-LLM\u8303\u5f0f\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u96c6\u6210\u5047\u8bbe\u5206\u6790(WIA)\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7fLLMs\u5177\u5907\u4e3b\u52a8\u9884\u6d4b\u672a\u6765\u72b6\u6001\u7684\u80fd\u529b\uff0c\u5e76\u5728\u300a\u738b\u8005\u8363\u8000\u300b\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6e38\u620f\u72b6\u6001\u9884\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "LLMs\u64c5\u957f\u88ab\u52a8\u5904\u7406\u4fe1\u606f\uff0c\u4f46\u65e0\u6cd5\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u5047\u8bbe\u6027\u672a\u6765\uff0c\u5373\u65e0\u6cd5\u5728\u884c\u52a8\u524d\u9884\u6d4b\u5176\u6f5c\u5728\u540e\u679c\u3002\u8fd9\u4e00\u5173\u952e\u7f3a\u9677\u9650\u5236\u4e86LLMs\u5728\u6218\u7565\u89c4\u5212\u3001\u98ce\u9669\u8bc4\u4f30\u548c\u5b9e\u65f6\u51b3\u7b56\u7b49\u52a8\u6001\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86WiA-LLM\uff0c\u4e00\u79cd\u8d4b\u4e88LLMs\u4e3b\u52a8\u601d\u8003\u80fd\u529b\u7684\u65b0\u8303\u5f0f\u3002\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86\u5047\u8bbe\u5206\u6790\uff08What-If Analysis, WIA\uff09\uff0c\u901a\u8fc7\u6539\u53d8\u8f93\u5165\u53d8\u91cf\u7cfb\u7edf\u8bc4\u4f30\u5047\u8bbe\u573a\u666f\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u83b7\u53d6\u73af\u5883\u53cd\u9988\u3002WiA-LLM\u80fd\u591f\u52a8\u6001\u6a21\u62df\u6bcf\u4e2a\u6f5c\u5728\u884c\u52a8\u7684\u7ed3\u679c\uff0c\u4ece\u800c\u9884\u6d4b\u672a\u6765\u72b6\u6001\u3002\u6a21\u578b\u5728\u300a\u738b\u8005\u8363\u8000\u300b\u8fd9\u4e00\u590d\u6742\u591a\u4eba\u6e38\u620f\u73af\u5883\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cWiA-LLM\u5728\u9884\u6d4b\u6e38\u620f\u72b6\u6001\u53d8\u5316\u65b9\u9762\u8fbe\u5230\u4e8674.2%\u7684\u51c6\u786e\u7387\uff0c\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u9ad8\u4e86\u4e24\u500d\u3002\u8be5\u6a21\u578b\u5728\u9700\u8981\u51c6\u786e\u9884\u5224\u7684\u9ad8\u96be\u5ea6\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5c24\u5176\u663e\u8457\u7684\u63d0\u5347\u3002", "conclusion": "WiA-LLM\u662f\u9996\u6b21\u6b63\u5f0f\u63a2\u7d22\u5e76\u6574\u5408LLMs\u5047\u8bbe\u5206\u6790\u80fd\u529b\u7684\u5de5\u4f5c\uff0c\u4ee3\u8868\u4e86LLMs\u5728\u4e3b\u52a8\u63a8\u7406\u65b9\u9762\u7684\u4e00\u9879\u57fa\u7840\u6027\u8fdb\u5c55\u3002\u5b83\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7a33\u5065\u51b3\u7b56\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u5bf9\u6218\u7565\u5e94\u7528\u5177\u6709\u5e7f\u6cdb\u7684\u610f\u4e49\u3002"}}
{"id": "2509.04588", "pdf": "https://arxiv.org/pdf/2509.04588", "abs": "https://arxiv.org/abs/2509.04588", "authors": ["Siyu Zhang", "Kenneth Mcmillan"], "title": "Toward Faithfulness-guided Ensemble Interpretation of Neural Network", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Interpretable and faithful explanations for specific neural inferences are\ncrucial for understanding and evaluating model behavior. Our work introduces\n\\textbf{F}aithfulness-guided \\textbf{E}nsemble \\textbf{I}nterpretation\n(\\textbf{FEI}), an innovative framework that enhances the breadth and\neffectiveness of faithfulness, advancing interpretability by providing superior\nvisualization. Through an analysis of existing evaluation benchmarks,\n\\textbf{FEI} employs a smooth approximation to elevate quantitative\nfaithfulness scores. Diverse variations of \\textbf{FEI} target enhanced\nfaithfulness in hidden layer encodings, expanding interpretability.\nAdditionally, we propose a novel qualitative metric that assesses hidden layer\nfaithfulness. In extensive experiments, \\textbf{FEI} surpasses existing\nmethods, demonstrating substantial advances in qualitative visualization and\nquantitative faithfulness scores. Our research establishes a comprehensive\nframework for elevating faithfulness in neural network explanations,\nemphasizing both breadth and precision", "AI": {"tldr": "\u63d0\u51faFEI\u6846\u67b6\uff0c\u901a\u8fc7\u5e73\u6ed1\u8fd1\u4f3c\u548c\u65b0\u5b9a\u6027\u6307\u6807\uff0c\u5168\u9762\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u89e3\u91ca\u7684\u5fe0\u5b9e\u5ea6\u53ca\u5176\u53ef\u89c6\u5316\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u548c\u8bc4\u4f30\u6a21\u578b\u884c\u4e3a\uff0c\u5bf9\u795e\u7ecf\u7f51\u7edc\u7684\u7279\u5b9a\u63a8\u65ad\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u5fe0\u5b9e\u7684\u89e3\u91ca\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u201c\u5fe0\u5b9e\u5ea6\u5f15\u5bfc\u7684\u96c6\u6210\u89e3\u91ca\uff08FEI\uff09\u201d\u6846\u67b6\u3002FEI\u901a\u8fc7\u5bf9\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u7684\u5206\u6790\uff0c\u91c7\u7528\u5e73\u6ed1\u8fd1\u4f3c\u6765\u63d0\u5347\u91cf\u5316\u5fe0\u5b9e\u5ea6\u5206\u6570\u3002\u8be5\u6846\u67b6\u8fd8\u5305\u542bFEI\u7684\u53d8\u4f53\uff0c\u65e8\u5728\u589e\u5f3a\u9690\u85cf\u5c42\u7f16\u7801\u7684\u5fe0\u5b9e\u5ea6\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u9690\u85cf\u5c42\u5fe0\u5b9e\u5ea6\u7684\u65b0\u9896\u5b9a\u6027\u6307\u6807\u3002", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cFEI\u5728\u5b9a\u6027\u53ef\u89c6\u5316\u548c\u91cf\u5316\u5fe0\u5b9e\u5ea6\u5206\u6570\u4e0a\u5747\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u89e3\u91ca\u5fe0\u5b9e\u5ea6\u7684\u5168\u9762\u6846\u67b6\uff0c\u5f3a\u8c03\u5176\u5e7f\u5ea6\u548c\u7cbe\u786e\u6027\u3002"}}
{"id": "2509.04461", "pdf": "https://arxiv.org/pdf/2509.04461", "abs": "https://arxiv.org/abs/2509.04461", "authors": ["Tian Ma", "Kaiyu Feng", "Yu Rong", "Kangfei Zhao"], "title": "From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Personality prediction from social media posts is a critical task that\nimplies diverse applications in psychology and sociology. The Myers Briggs Type\nIndicator (MBTI), a popular personality inventory, has been traditionally\npredicted by machine learning (ML) and deep learning (DL) techniques. Recently,\nthe success of Large Language Models (LLMs) has revealed their huge potential\nin understanding and inferring personality traits from social media content.\nHowever, directly exploiting LLMs for MBTI prediction faces two key challenges:\nthe hallucination problem inherent in LLMs and the naturally imbalanced\ndistribution of MBTI types in the population. In this paper, we propose\nPostToPersonality (PtoP), a novel LLM based framework for MBTI prediction from\nsocial media posts of individuals. Specifically, PtoP leverages Retrieval\nAugmented Generation with in context learning to mitigate hallucination in\nLLMs. Furthermore, we fine tune a pretrained LLM to improve model specification\nin MBTI understanding with synthetic minority oversampling, which balances the\nclass imbalance by generating synthetic samples. Experiments conducted on a\nreal world social media dataset demonstrate that PtoP achieves state of the art\nperformance compared with 10 ML and DL baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPtoP\u6846\u67b6\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u7684MBTI\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7RAG\u7f13\u89e3LLM\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u5408\u6210\u5c11\u6570\u7c7b\u8fc7\u91c7\u6837\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e94\u7528\u4e8eMBTI\u9884\u6d4b\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1aLLMs\u56fa\u6709\u7684\u5e7b\u89c9\u95ee\u9898\u4ee5\u53caMBTI\u7c7b\u578b\u5728\u4eba\u7fa4\u4e2d\u81ea\u7136\u5b58\u5728\u7684\u4e0d\u5e73\u8861\u5206\u5e03\u3002", "method": "\u63d0\u51faPostToPersonality (PtoP) \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u6765\u7f13\u89e3LLMs\u7684\u5e7b\u89c9\uff1b\u6b64\u5916\uff0c\u901a\u8fc7\u5408\u6210\u5c11\u6570\u7c7b\u8fc7\u91c7\u6837\u6280\u672f\u5fae\u8c03\u9884\u8bad\u7ec3LLM\uff0c\u4ee5\u5e73\u8861\u7c7b\u522b\u4e0d\u5e73\u8861\u5e76\u63d0\u9ad8\u6a21\u578b\u5728MBTI\u7406\u89e3\u4e0a\u7684\u7279\u5f02\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPtoP\u6846\u67b6\u4e0e10\u4e2a\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\uff08state-of-the-art\uff09\u6027\u80fd\u3002", "conclusion": "PtoP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728MBTI\u9884\u6d4b\u4e2d\u9762\u4e34\u7684\u5e7b\u89c9\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u4eba\u683c\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.04602", "pdf": "https://arxiv.org/pdf/2509.04602", "abs": "https://arxiv.org/abs/2509.04602", "authors": ["MinJu Jeon", "Si-Woo Kim", "Ye-Chan Kim", "HyunGee Kim", "Dong-Jin Kim"], "title": "Sali4Vid: Saliency-Aware Video Reweighting and Adaptive Caption Retrieval for Dense Video Captioning", "categories": ["cs.CV"], "comment": "Accepted in EMNLP 2025", "summary": "Dense video captioning aims to temporally localize events in video and\ngenerate captions for each event. While recent works propose end-to-end models,\nthey suffer from two limitations: (1) applying timestamp supervision only to\ntext while treating all video frames equally, and (2) retrieving captions from\nfixed-size video chunks, overlooking scene transitions. To address these, we\npropose Sali4Vid, a simple yet effective saliency-aware framework. We introduce\nSaliency-aware Video Reweighting, which converts timestamp annotations into\nsigmoid-based frame importance weights, and Semantic-based Adaptive Caption\nRetrieval, which segments videos by frame similarity to capture scene\ntransitions and improve caption retrieval. Sali4Vid achieves state-of-the-art\nresults on YouCook2 and ViTT, demonstrating the benefit of jointly improving\nvideo weighting and retrieval for dense video captioning", "AI": {"tldr": "\u9488\u5bf9\u5bc6\u96c6\u89c6\u9891\u5b57\u5e55\u4e2d\u5e27\u52a0\u6743\u4e0d\u5747\u548c\u56fa\u5b9a\u5757\u68c0\u7d22\u7684\u95ee\u9898\uff0c\u63d0\u51faSali4Vid\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u8457\u6027\u611f\u77e5\u89c6\u9891\u91cd\u52a0\u6743\u548c\u81ea\u9002\u5e94\u5b57\u5e55\u68c0\u7d22\uff0c\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u5bc6\u96c6\u89c6\u9891\u5b57\u5e55\u6a21\u578b\u5b58\u5728\u4e24\u70b9\u5c40\u9650\uff1a1) \u65f6\u95f4\u6233\u76d1\u7763\u4ec5\u7528\u4e8e\u6587\u672c\uff0c\u5bf9\u6240\u6709\u89c6\u9891\u5e27\u4e00\u89c6\u540c\u4ec1\uff1b2) \u4ece\u56fa\u5b9a\u5927\u5c0f\u7684\u89c6\u9891\u5757\u4e2d\u68c0\u7d22\u5b57\u5e55\uff0c\u5ffd\u7565\u4e86\u573a\u666f\u8fc7\u6e21\u3002", "method": "\u63d0\u51faSali4Vid\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u663e\u8457\u6027\u611f\u77e5\u89c6\u9891\u91cd\u52a0\u6743\uff0c\u5c06\u65f6\u95f4\u6233\u6807\u6ce8\u8f6c\u6362\u4e3a\u57fa\u4e8eSigmoid\u7684\u5e27\u91cd\u8981\u6027\u6743\u91cd\uff1b2) \u57fa\u4e8e\u8bed\u4e49\u7684\u81ea\u9002\u5e94\u5b57\u5e55\u68c0\u7d22\uff0c\u901a\u8fc7\u5e27\u76f8\u4f3c\u6027\u5206\u5272\u89c6\u9891\u4ee5\u6355\u83b7\u573a\u666f\u8fc7\u6e21\uff0c\u6539\u8fdb\u5b57\u5e55\u68c0\u7d22\u3002", "result": "Sali4Vid\u5728YouCook2\u548cViTT\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u8054\u5408\u6539\u8fdb\u89c6\u9891\u52a0\u6743\u548c\u68c0\u7d22\u5bf9\u5bc6\u96c6\u89c6\u9891\u5b57\u5e55\u4efb\u52a1\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2509.04809", "pdf": "https://arxiv.org/pdf/2509.04809", "abs": "https://arxiv.org/abs/2509.04809", "authors": ["Haechang Kim", "Hao Chen", "Can Li", "Jong Min Lee"], "title": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models", "categories": ["cs.AI", "cs.HC"], "comment": "31 pages total", "summary": "Explainable Reinforcement Learning (XRL) has emerged as a promising approach\nin improving the transparency of Reinforcement Learning (RL) agents. However,\nthere remains a gap between complex RL policies and domain experts, due to the\nlimited comprehensibility of XRL results and isolated coverage of current XRL\napproaches that leave users uncertain about which tools to employ. To address\nthese challenges, we introduce TalkToAgent, a multi-agent Large Language Models\n(LLM) framework that delivers interactive, natural language explanations for RL\npolicies. The architecture with five specialized LLM agents (Coordinator,\nExplainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically\nmap user queries to relevant XRL tools and clarify an agent's actions in terms\nof either key state variables, expected outcomes, or counterfactual\nexplanations. Moreover, our approach extends previous counterfactual\nexplanations by deriving alternative scenarios from qualitative behavioral\ndescriptions, or even new rule-based policies. We validated TalkToAgent on\nquadruple-tank process control problem, a well-known nonlinear control\nbenchmark. Results demonstrated that TalkToAgent successfully mapped user\nqueries into XRL tasks with high accuracy, and coder-debugger interactions\nminimized failures in counterfactual generation. Furthermore, qualitative\nevaluation confirmed that TalkToAgent effectively interpreted agent's actions\nand contextualized their meaning within the problem domain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TalkToAgent\uff0c\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53LLM\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u5f25\u5408\u590d\u6742RL\u7b56\u7565\u4e0e\u9886\u57df\u4e13\u5bb6\u4e4b\u95f4\u7684\u7406\u89e3\u9e3f\u6c9f\uff0c\u5e76\u6269\u5c55\u4e86\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\uff08XRL\uff09\u7ed3\u679c\u7406\u89e3\u6709\u9650\u4e14\u5de5\u5177\u8986\u76d6\u5206\u6563\uff0c\u5bfc\u81f4\u9886\u57df\u4e13\u5bb6\u96be\u4ee5\u7406\u89e3\u590d\u6742\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u7528\u6237\u4e5f\u96be\u4ee5\u9009\u62e9\u5408\u9002\u7684\u5de5\u5177\u3002", "method": "\u5f15\u5165TalkToAgent\uff0c\u4e00\u4e2a\u5305\u542b\u4e94\u4e2a\u4e13\u95e8LLM\u667a\u80fd\u4f53\uff08\u534f\u8c03\u5668\u3001\u89e3\u91ca\u5668\u3001\u7f16\u7801\u5668\u3001\u8bc4\u4f30\u5668\u3001\u8c03\u8bd5\u5668\uff09\u7684\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\u3002\u5b83\u80fd\u81ea\u52a8\u5c06\u7528\u6237\u67e5\u8be2\u6620\u5c04\u5230\u76f8\u5173XRL\u5de5\u5177\uff0c\u5e76\u4ee5\u5173\u952e\u72b6\u6001\u53d8\u91cf\u3001\u9884\u671f\u7ed3\u679c\u6216\u53cd\u4e8b\u5b9e\u89e3\u91ca\u7684\u5f62\u5f0f\u6f84\u6e05\u667a\u80fd\u4f53\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u4ece\u5b9a\u6027\u884c\u4e3a\u63cf\u8ff0\u6216\u65b0\u89c4\u5219\u7b56\u7565\u4e2d\u63a8\u5bfc\u66ff\u4ee3\u573a\u666f\uff0c\u6269\u5c55\u4e86\u53cd\u4e8b\u5b9e\u89e3\u91ca\u3002", "result": "\u5728\u56db\u7f50\u8fc7\u7a0b\u63a7\u5236\u95ee\u9898\u4e0a\u9a8c\u8bc1\uff0cTalkToAgent\u9ad8\u7cbe\u5ea6\u5730\u5c06\u7528\u6237\u67e5\u8be2\u6620\u5c04\u5230XRL\u4efb\u52a1\uff0c\u7f16\u7801\u5668-\u8c03\u8bd5\u5668\u4ea4\u4e92\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u53cd\u4e8b\u5b9e\u751f\u6210\u5931\u8d25\u3002\u5b9a\u6027\u8bc4\u4f30\u8bc1\u5b9e\u5176\u6709\u6548\u89e3\u91ca\u4e86\u667a\u80fd\u4f53\u884c\u4e3a\u5e76\u5728\u95ee\u9898\u9886\u57df\u4e2d\u5bf9\u5176\u8fdb\u884c\u8bed\u5883\u5316\u3002", "conclusion": "TalkToAgent\u901a\u8fc7\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u3001\u81ea\u7136\u8bed\u8a00\u7684\u89e3\u91ca\uff0c\u6210\u529f\u63d0\u5347\u4e86RL\u667a\u80fd\u4f53\u7684\u900f\u660e\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86XRL\u7ed3\u679c\u53ef\u7406\u89e3\u6027\u6709\u9650\u548c\u73b0\u6709\u65b9\u6cd5\u8986\u76d6\u5206\u6563\u7684\u6311\u6218\u3002"}}
{"id": "2509.04601", "pdf": "https://arxiv.org/pdf/2509.04601", "abs": "https://arxiv.org/abs/2509.04601", "authors": ["Han Zhang", "Fengji Ma", "Jiamin Su", "Xinyue Yang", "Lei Wang", "Wen-Cai Ye", "Li Liu"], "title": "Quantum-Enhanced Multi-Task Learning with Learnable Weighting for Pharmacokinetic and Toxicity Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Prediction for ADMET (Absorption, Distribution, Metabolism, Excretion, and\nToxicity) plays a crucial role in drug discovery and development, accelerating\nthe screening and optimization of new drugs. Existing methods primarily rely on\nsingle-task learning (STL), which often fails to fully exploit the\ncomplementarities between tasks. Besides, it requires more computational\nresources while training and inference of each task independently. To address\nthese issues, we propose a new unified Quantum-enhanced and task-Weighted\nMulti-Task Learning (QW-MTL) framework, specifically designed for ADMET\nclassification tasks. Built upon the Chemprop-RDKit backbone, QW-MTL adopts\nquantum chemical descriptors to enrich molecular representations with\nadditional information about the electronic structure and interactions.\nMeanwhile, it introduces a novel exponential task weighting scheme that\ncombines dataset-scale priors with learnable parameters to achieve dynamic loss\nbalancing across tasks. To the best of our knowledge, this is the first work to\nsystematically conduct joint multi-task training across all 13 Therapeutics\nData Commons (TDC) classification benchmarks, using leaderboard-style data\nsplits to ensure a standardized and realistic evaluation setting. Extensive\nexperimental results show that QW-MTL significantly outperforms single-task\nbaselines on 12 out of 13 tasks, achieving high predictive performance with\nminimal model complexity and fast inference, demonstrating the effectiveness\nand efficiency of multi-task molecular learning enhanced by quantum-informed\nfeatures and adaptive task weighting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQW-MTL\u7684\u7edf\u4e00\u91cf\u5b50\u589e\u5f3a\u548c\u4efb\u52a1\u52a0\u6743\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8eADMET\u5206\u7c7b\u4efb\u52a1\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u91cf\u5b50\u5316\u5b66\u63cf\u8ff0\u7b26\u548c\u52a8\u6001\u4efb\u52a1\u52a0\u6743\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u4efb\u52a1\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u7684ADMET\u9884\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5355\u4efb\u52a1\u5b66\u4e60\uff08STL\uff09\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4efb\u52a1\u95f4\u7684\u4e92\u8865\u6027\uff0c\u5e76\u4e14\u72ec\u7acb\u8bad\u7ec3\u548c\u63a8\u65ad\u6bcf\u4e2a\u4efb\u52a1\u9700\u8981\u66f4\u591a\u7684\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86QW-MTL\u6846\u67b6\uff0c\u5b83\u57fa\u4e8eChemprop-RDKit\u9aa8\u5e72\u7f51\u7edc\uff0c\u901a\u8fc7\u5f15\u5165\u91cf\u5b50\u5316\u5b66\u63cf\u8ff0\u7b26\u6765\u4e30\u5bcc\u5206\u5b50\u8868\u793a\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u6307\u6570\u4efb\u52a1\u52a0\u6743\u65b9\u6848\uff0c\u7ed3\u5408\u6570\u636e\u96c6\u89c4\u6a21\u5148\u9a8c\u548c\u53ef\u5b66\u4e60\u53c2\u6570\u5b9e\u73b0\u4efb\u52a1\u95f4\u52a8\u6001\u635f\u5931\u5e73\u8861\u3002\u8fd9\u662f\u9996\u6b21\u5728\u6240\u670913\u4e2aTDC\u5206\u7c7b\u57fa\u51c6\u4e0a\u7cfb\u7edf\u6027\u5730\u8fdb\u884c\u8054\u5408\u591a\u4efb\u52a1\u8bad\u7ec3\uff0c\u5e76\u91c7\u7528\u6392\u884c\u699c\u5f0f\u6570\u636e\u5212\u5206\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cQW-MTL\u572813\u4e2a\u4efb\u52a1\u4e2d\u768412\u4e2a\u4e0a\u663e\u8457\u4f18\u4e8e\u5355\u4efb\u52a1\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u9884\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6700\u5c0f\u7684\u6a21\u578b\u590d\u6742\u5ea6\u548c\u5feb\u901f\u63a8\u7406\u3002", "conclusion": "\u91cf\u5b50\u4fe1\u606f\u7279\u5f81\u548c\u81ea\u9002\u5e94\u4efb\u52a1\u52a0\u6743\u589e\u5f3a\u7684\u591a\u4efb\u52a1\u5206\u5b50\u5b66\u4e60\u5728ADMET\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u9ad8\u6548\u7387\u3002"}}
{"id": "2509.04462", "pdf": "https://arxiv.org/pdf/2509.04462", "abs": "https://arxiv.org/abs/2509.04462", "authors": ["Yu Hou", "Zaifu Zhan", "Rui Zhang"], "title": "Benchmarking GPT-5 for biomedical natural language processing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid expansion of biomedical literature has heightened the need for\nscalable natural language processing (NLP) solutions. While GPT-4 substantially\nnarrowed the gap with task-specific systems, especially in question answering,\nits performance across other domains remained uneven. We updated a standardized\nBioNLP benchmark to evaluate GPT-5 and GPT-4o under zero-, one-, and five-shot\nprompting across 12 datasets spanning six task families: named entity\nrecognition, relation extraction, multi-label document classification, question\nanswering, text summarization, and text simplification. Using fixed prompt\ntemplates, identical decoding parameters, and batch inference, we report\nprimary metrics per dataset and include prior results for GPT-4, GPT-3.5, and\nLLaMA-2-13B for comparison. GPT-5 achieved the strongest overall benchmark\nperformance, with macro-average scores rising to 0.557 under five-shot\nprompting versus 0.506 for GPT-4 and 0.508 for GPT-4o. On MedQA, GPT-5 reached\n94.1% accuracy, exceeding the previous supervised state of the art by over\nfifty points, and attained parity with supervised systems on PubMedQA (0.734).\nIn extraction tasks, GPT-5 delivered major gains in chemical NER (0.886 F1) and\nChemProt relation extraction (0.616 F1), outperforming GPT-4 and GPT-4o, though\nsummarization and disease NER still lagged behind domain-specific baselines.\nThese results establish GPT-5 as a general-purpose model now offering\ndeployment-ready performance for reasoning-oriented biomedical QA, while\nprecision-critical extraction and evidence-dense summarization continue to\nfavor fine-tuned or hybrid approaches. The benchmark delineates where simple\nprompting suffices and where retrieval-augmented or planning-based scaffolds\nare likely required, providing actionable guidance for BioNLP system design as\nfrontier models advance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86GPT-5\u548cGPT-4o\u5728\u751f\u7269\u533b\u5b66NLP\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0GPT-5\u5728\u6574\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u5728\u95ee\u7b54\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709SOTA\uff0c\u4f46\u5728\u67d0\u4e9b\u7279\u5b9a\u62bd\u53d6\u548c\u6458\u8981\u4efb\u52a1\u4e0a\u4ecd\u4e0d\u53ca\u9886\u57df\u4e13\u7528\u6a21\u578b\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u6587\u732e\u7684\u5feb\u901f\u589e\u957f\u5bf9\u53ef\u6269\u5c55\u7684NLP\u89e3\u51b3\u65b9\u6848\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\u3002\u5c3d\u7ba1GPT-4\u5728\u95ee\u7b54\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5176\u4ed6\u751f\u7269\u533b\u5b66\u9886\u57df\u6027\u80fd\u4ecd\u4e0d\u5747\u8861\uff0c\u9700\u8981\u8bc4\u4f30\u65b0\u4e00\u4ee3\u6a21\u578b\uff08\u5982GPT-5\u548cGPT-4o\uff09\u7684\u8fdb\u5c55\u3002", "method": "\u7814\u7a76\u4eba\u5458\u66f4\u65b0\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u7684BioNLP\u57fa\u51c6\uff0c\u572812\u4e2a\u6570\u636e\u96c6\u3001\u6db5\u76d6\u516d\u4e2a\u4efb\u52a1\u5bb6\u65cf\uff08\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u5173\u7cfb\u62bd\u53d6\u3001\u591a\u6807\u7b7e\u6587\u6863\u5206\u7c7b\u3001\u95ee\u7b54\u3001\u6587\u672c\u6458\u8981\u3001\u6587\u672c\u7b80\u5316\uff09\u4e0a\uff0c\u4ee5\u96f6\u3001\u4e00\u548c\u4e94\u6b21\u5c11\u6837\u672c\u63d0\u793a\u65b9\u5f0f\u8bc4\u4f30\u4e86GPT-5\u548cGPT-4o\u3002\u8bc4\u4f30\u91c7\u7528\u4e86\u56fa\u5b9a\u7684\u63d0\u793a\u6a21\u677f\u3001\u76f8\u540c\u7684\u89e3\u7801\u53c2\u6570\u548c\u6279\u5904\u7406\u63a8\u7406\uff0c\u5e76\u4e0eGPT-4\u3001GPT-3.5\u548cLLaMA-2-13B\u7684\u7ed3\u679c\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "GPT-5\u53d6\u5f97\u4e86\u6700\u5f3a\u7684\u6574\u4f53\u57fa\u51c6\u6027\u80fd\uff0c\u5728\u4e94\u6b21\u5c11\u6837\u672c\u63d0\u793a\u4e0b\u5b8f\u89c2\u5e73\u5747\u5206\u6570\u5347\u81f30.557\uff08GPT-4\u4e3a0.506\uff0cGPT-4o\u4e3a0.508\uff09\u3002\u5728MedQA\u4e0a\uff0cGPT-5\u8fbe\u523094.1%\u7684\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u76d1\u7763SOTA\u4e94\u5341\u591a\u4e2a\u767e\u5206\u70b9\uff0c\u5e76\u5728PubMedQA\u4e0a\u4e0e\u76d1\u7763\u7cfb\u7edf\u6301\u5e73\uff080.734\uff09\u3002\u5728\u62bd\u53d6\u4efb\u52a1\u4e2d\uff0cGPT-5\u5728\u5316\u5b66NER\uff080.886 F1\uff09\u548cChemProt\u5173\u7cfb\u62bd\u53d6\uff080.616 F1\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002\u7136\u800c\uff0c\u6458\u8981\u548c\u75be\u75c5NER\u4efb\u52a1\u7684\u8868\u73b0\u4ecd\u843d\u540e\u4e8e\u9886\u57df\u4e13\u7528\u57fa\u7ebf\u3002", "conclusion": "GPT-5\u5df2\u6210\u4e3a\u4e00\u4e2a\u901a\u7528\u6a21\u578b\uff0c\u5728\u9762\u5411\u63a8\u7406\u7684\u751f\u7269\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u4e0a\u63d0\u4f9b\u4e86\u53ef\u90e8\u7f72\u7684\u6027\u80fd\u3002\u4f46\u5bf9\u4e8e\u7cbe\u5ea6\u5173\u952e\u7684\u62bd\u53d6\u548c\u8bc1\u636e\u5bc6\u96c6\u578b\u6458\u8981\u4efb\u52a1\uff0c\u7cbe\u8c03\u6216\u6df7\u5408\u65b9\u6cd5\u4ecd\u7136\u66f4\u5177\u4f18\u52bf\u3002\u8be5\u57fa\u51c6\u4e3aBioNLP\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u660e\u786e\u4e86\u7b80\u5355\u63d0\u793a\u8db3\u4ee5\u5e94\u5bf9\u7684\u573a\u666f\u4ee5\u53ca\u53ef\u80fd\u9700\u8981\u68c0\u7d22\u589e\u5f3a\u6216\u89c4\u5212\u578b\u652f\u67b6\u7684\u573a\u666f\u3002"}}
{"id": "2509.04624", "pdf": "https://arxiv.org/pdf/2509.04624", "abs": "https://arxiv.org/abs/2509.04624", "authors": ["Ali Khanpour", "Tianyi Wang", "Afra Vahidi-Shams", "Wim Ectors", "Farzam Nakhaie", "Amirhossein Taheri", "Christian Claudel"], "title": "UAV-Based Intelligent Traffic Surveillance System: Real-Time Vehicle Detection, Classification, Tracking, and Behavioral Analysis", "categories": ["cs.CV", "cs.ET", "cs.RO", "cs.SY", "eess.IV", "eess.SY"], "comment": "15 pages, 8 figures, 2 tables", "summary": "Traffic congestion and violations pose significant challenges for urban\nmobility and road safety. Traditional traffic monitoring systems, such as fixed\ncameras and sensor-based methods, are often constrained by limited coverage,\nlow adaptability, and poor scalability. To address these challenges, this paper\nintroduces an advanced unmanned aerial vehicle (UAV)-based traffic surveillance\nsystem capable of accurate vehicle detection, classification, tracking, and\nbehavioral analysis in real-world, unconstrained urban environments. The system\nleverages multi-scale and multi-angle template matching, Kalman filtering, and\nhomography-based calibration to process aerial video data collected from\naltitudes of approximately 200 meters. A case study in urban area demonstrates\nrobust performance, achieving a detection precision of 91.8%, an F1-score of\n90.5%, and tracking metrics (MOTA/MOTP) of 92.1% and 93.7%, respectively.\nBeyond precise detection, the system classifies five vehicle types and\nautomatically detects critical traffic violations, including unsafe lane\nchanges, illegal double parking, and crosswalk obstructions, through the fusion\nof geofencing, motion filtering, and trajectory deviation analysis. The\nintegrated analytics module supports origin-destination tracking, vehicle count\nvisualization, inter-class correlation analysis, and heatmap-based congestion\nmodeling. Additionally, the system enables entry-exit trajectory profiling,\nvehicle density estimation across road segments, and movement direction\nlogging, supporting comprehensive multi-scale urban mobility analytics.\nExperimental results confirms the system's scalability, accuracy, and practical\nrelevance, highlighting its potential as an enforcement-aware,\ninfrastructure-independent traffic monitoring solution for next-generation\nsmart cities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5148\u8fdb\u7684\u65e0\u4eba\u673a\uff08UAV\uff09\u4ea4\u901a\u76d1\u63a7\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u8f66\u8f86\u68c0\u6d4b\u3001\u5206\u7c7b\u3001\u8ddf\u8e2a\u3001\u884c\u4e3a\u5206\u6790\u4ee5\u53ca\u4ea4\u901a\u8fdd\u89c4\u68c0\u6d4b\u3002\u8be5\u7cfb\u7edf\u5229\u7528\u591a\u5c3a\u5ea6\u6a21\u677f\u5339\u914d\u3001\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u5355\u5e94\u6027\u6821\u51c6\u7b49\u6280\u672f\u5904\u7406\u822a\u7a7a\u89c6\u9891\u6570\u636e\uff0c\u5e76\u5728\u5b9e\u9645\u57ce\u5e02\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u9ad8\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u667a\u6167\u57ce\u5e02\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u7840\u8bbe\u65bd\u72ec\u7acb\u7684\u4ea4\u901a\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4ea4\u901a\u62e5\u5835\u548c\u8fdd\u89c4\u5bf9\u57ce\u5e02\u4ea4\u901a\u548c\u9053\u8def\u5b89\u5168\u6784\u6210\u91cd\u5927\u6311\u6218\u3002\u4f20\u7edf\u4ea4\u901a\u76d1\u63a7\u7cfb\u7edf\uff08\u5982\u56fa\u5b9a\u6444\u50cf\u5934\u548c\u57fa\u4e8e\u4f20\u611f\u5668\u7684\u65b9\u6cd5\uff09\u5e38\u53d7\u9650\u4e8e\u8986\u76d6\u8303\u56f4\u6709\u9650\u3001\u9002\u5e94\u6027\u5dee\u548c\u53ef\u6269\u5c55\u6027\u4f4e\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u5148\u8fdb\u7cfb\u7edf\u6765\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u4ea4\u901a\u76d1\u63a7\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u5229\u7528\u591a\u5c3a\u5ea6\u548c\u591a\u89d2\u5ea6\u6a21\u677f\u5339\u914d\u3001\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u57fa\u4e8e\u5355\u5e94\u6027\u7684\u6821\u51c6\u6280\u672f\u6765\u5904\u7406\u4ece\u7ea6200\u7c73\u9ad8\u7a7a\u6536\u96c6\u7684\u822a\u7a7a\u89c6\u9891\u6570\u636e\uff0c\u4ee5\u5b9e\u73b0\u8f66\u8f86\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u8ddf\u8e2a\u3002\u4e3a\u68c0\u6d4b\u4ea4\u901a\u8fdd\u89c4\uff0c\u7cfb\u7edf\u878d\u5408\u4e86\u5730\u7406\u56f4\u680f\u3001\u8fd0\u52a8\u6ee4\u6ce2\u548c\u8f68\u8ff9\u504f\u5dee\u5206\u6790\u3002\u6b64\u5916\uff0c\u7cfb\u7edf\u96c6\u6210\u4e86\u5206\u6790\u6a21\u5757\uff0c\u652f\u6301\u8d77\u70b9-\u7ec8\u70b9\u8ddf\u8e2a\u3001\u8f66\u8f86\u8ba1\u6570\u53ef\u89c6\u5316\u3001\u7c7b\u95f4\u5173\u8054\u5206\u6790\u3001\u57fa\u4e8e\u70ed\u56fe\u7684\u62e5\u5835\u5efa\u6a21\u3001\u51fa\u5165\u53e3\u8f68\u8ff9\u5206\u6790\u3001\u8def\u6bb5\u8f66\u8f86\u5bc6\u5ea6\u4f30\u8ba1\u548c\u79fb\u52a8\u65b9\u5411\u8bb0\u5f55\u3002", "result": "\u8be5\u7cfb\u7edf\u5728\u57ce\u5e02\u533a\u57df\u6848\u4f8b\u7814\u7a76\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u8fbe\u523091.8%\uff0cF1-score\u4e3a90.5%\uff0c\u8ddf\u8e2a\u6307\u6807\uff08MOTA/MOTP\uff09\u5206\u522b\u4e3a92.1%\u548c93.7%\u3002\u7cfb\u7edf\u80fd\u591f\u5206\u7c7b\u4e94\u79cd\u8f66\u8f86\u7c7b\u578b\uff0c\u5e76\u901a\u8fc7\u878d\u5408\u5730\u7406\u56f4\u680f\u3001\u8fd0\u52a8\u6ee4\u6ce2\u548c\u8f68\u8ff9\u504f\u5dee\u5206\u6790\uff0c\u81ea\u52a8\u68c0\u6d4b\u4e0d\u5b89\u5168\u53d8\u9053\u3001\u975e\u6cd5\u53cc\u91cd\u505c\u8f66\u548c\u4eba\u884c\u6a2a\u9053\u963b\u585e\u7b49\u5173\u952e\u4ea4\u901a\u8fdd\u89c4\u884c\u4e3a\u3002\u96c6\u6210\u7684\u5206\u6790\u6a21\u5757\u652f\u6301\u5168\u9762\u7684\u591a\u5c3a\u5ea6\u57ce\u5e02\u4ea4\u901a\u5206\u6790\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u8be5\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3001\u51c6\u786e\u6027\u548c\u5b9e\u9645\u76f8\u5173\u6027\u3002\u5b83\u4f5c\u4e3a\u4e00\u79cd\u6267\u6cd5\u611f\u77e5\u3001\u57fa\u7840\u8bbe\u65bd\u72ec\u7acb\u7684\u4ea4\u901a\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4e0b\u4e00\u4ee3\u667a\u6167\u57ce\u5e02\u4e2d\u5177\u6709\u5de8\u5927\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.04847", "pdf": "https://arxiv.org/pdf/2509.04847", "abs": "https://arxiv.org/abs/2509.04847", "authors": ["Mukul Singh", "Arjun Radhakrishna", "Sumit Gulwani"], "title": "Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory", "categories": ["cs.AI"], "comment": "9 pages", "summary": "Language models are increasingly deployed in interactive online environments,\nfrom personal chat assistants to domain-specific agents, raising questions\nabout their cooperative and competitive behavior in multi-party settings. While\nprior work has examined language model decision-making in isolated or\nshort-term game-theoretic contexts, these studies often neglect long-horizon\ninteractions, human-model collaboration, and the evolution of behavioral\npatterns over time. In this paper, we investigate the dynamics of language\nmodel behavior in the iterated prisoner's dilemma (IPD), a classical framework\nfor studying cooperation and conflict. We pit model-based agents against a\nsuite of 240 well-established classical strategies in an Axelrod-style\ntournament and find that language models achieve performance on par with, and\nin some cases exceeding, the best-known classical strategies. Behavioral\nanalysis reveals that language models exhibit key properties associated with\nstrong cooperative strategies - niceness, provocability, and generosity while\nalso demonstrating rapid adaptability to changes in opponent strategy mid-game.\nIn controlled \"strategy switch\" experiments, language models detect and respond\nto shifts within only a few rounds, rivaling or surpassing human adaptability.\nThese results provide the first systematic characterization of long-term\ncooperative behaviors in language model agents, offering a foundation for\nfuture research into their role in more complex, mixed human-AI social\nenvironments.", "AI": {"tldr": "\u672c\u7814\u7a76\u5728\u8fed\u4ee3\u56da\u5f92\u56f0\u5883(IPD)\u4e2d\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u8bed\u8a00\u6a21\u578b(LM)\u7684\u957f\u671f\u5408\u4f5c\u884c\u4e3a\uff0c\u53d1\u73b0LM\u5728\u8868\u73b0\u4e0a\u4e0e\u6700\u4f73\u7ecf\u5178\u7b56\u7565\u76f8\u5f53\u751a\u81f3\u8d85\u8d8a\uff0c\u5e76\u5c55\u73b0\u51fa\u5feb\u901f\u9002\u5e94\u5bf9\u624b\u7b56\u7565\u53d8\u5316\u7684\u5f3a\u5927\u80fd\u529b\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u4ea4\u4e92\u5f0f\u5728\u7ebf\u73af\u5883\u4e2d\u90e8\u7f72\u65e5\u76ca\u589e\u591a\uff0c\u5f15\u53d1\u4e86\u5bf9\u5176\u5728\u591a\u65b9\u8bbe\u7f6e\u4e2d\u5408\u4f5c\u4e0e\u7ade\u4e89\u884c\u4e3a\u7684\u7591\u95ee\u3002\u73b0\u6709\u7814\u7a76\u5e38\u5ffd\u7565\u957f\u671f\u4e92\u52a8\u3001\u4eba\u673a\u534f\u4f5c\u53ca\u884c\u4e3a\u6a21\u5f0f\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u95ee\u9898\u3002", "method": "\u5c06\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u4e0e240\u79cd\u6210\u719f\u7684\u7ecf\u5178\u7b56\u7565\u8fdb\u884cAxelrod\u5f0f\u9526\u6807\u8d5b\u5bf9\u6297\uff0c\u4ee5\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u8fed\u4ee3\u56da\u5f92\u56f0\u5883(IPD)\u4e2d\u7684\u52a8\u6001\u884c\u4e3a\u3002\u901a\u8fc7\u884c\u4e3a\u5206\u6790\u548c\u53d7\u63a7\u7684\u201c\u7b56\u7565\u8f6c\u6362\u201d\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u5176\u5408\u4f5c\u7279\u6027\u548c\u9002\u5e94\u6027\u3002", "result": "\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u5728\u8868\u73b0\u4e0a\u4e0e\u6700\u4f73\u7ecf\u5178\u7b56\u7565\u76f8\u5f53\uff0c\u751a\u81f3\u8d85\u8d8a\u3002\u884c\u4e3a\u5206\u6790\u663e\u793a\uff0c\u8bed\u8a00\u6a21\u578b\u5177\u5907\u5408\u4f5c\u7b56\u7565\u7684\u5173\u952e\u7279\u6027\uff1a\u53cb\u5584\u6027\u3001\u53ef\u6fc0\u6012\u6027\u53ca\u6177\u6168\u6027\uff0c\u5e76\u80fd\u5728\u5bf9\u5c40\u4e2d\u5feb\u901f\u9002\u5e94\u5bf9\u624b\u7b56\u7565\u53d8\u5316\uff0c\u5728\u51e0\u8f6e\u5185\u68c0\u6d4b\u5e76\u54cd\u5e94\u8f6c\u6362\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u5730\u63cf\u7ed8\u4e86\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u7684\u957f\u671f\u5408\u4f5c\u884c\u4e3a\uff0c\u4e3a\u672a\u6765\u5728\u66f4\u590d\u6742\u7684\u4eba\u673a\u6df7\u5408\u793e\u4f1a\u73af\u5883\u4e2d\u7814\u7a76\u5176\u4f5c\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.04622", "pdf": "https://arxiv.org/pdf/2509.04622", "abs": "https://arxiv.org/abs/2509.04622", "authors": ["Jialin Wu", "Shreya Saha", "Yiqing Bo", "Meenakshi Khosla"], "title": "Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "Representational similarity metrics are fundamental tools in neuroscience and\nAI, yet we lack systematic comparisons of their discriminative power across\nmodel families. We introduce a quantitative framework to evaluate\nrepresentational similarity measures based on their ability to separate model\nfamilies-across architectures (CNNs, Vision Transformers, Swin Transformers,\nConvNeXt) and training regimes (supervised vs. self-supervised). Using three\ncomplementary separability measures-dprime from signal detection theory,\nsilhouette coefficients and ROC-AUC, we systematically assess the\ndiscriminative capacity of commonly used metrics including RSA, linear\npredictivity, Procrustes, and soft matching. We show that separability\nsystematically increases as metrics impose more stringent alignment\nconstraints. Among mapping-based approaches, soft-matching achieves the highest\nseparability, followed by Procrustes alignment and linear predictivity.\nNon-fitting methods such as RSA also yield strong separability across families.\nThese results provide the first systematic comparison of similarity metrics\nthrough a separability lens, clarifying their relative sensitivity and guiding\nmetric choice for large-scale model and brain comparisons.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6027\u6bd4\u8f83\u4e86\u4e0d\u540c\u8868\u5f81\u76f8\u4f3c\u6027\u5ea6\u91cf\u5728\u533a\u5206\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5bb6\u65cf\u65b9\u9762\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5ea6\u91cf\u65bd\u52a0\u7684\u5bf9\u9f50\u7ea6\u675f\u8d8a\u4e25\u683c\uff0c\u533a\u5206\u80fd\u529b\u8d8a\u5f3a\uff0c\u5176\u4e2d\u8f6f\u5339\u914d\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u9009\u62e9\u5408\u9002\u7684\u5ea6\u91cf\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "motivation": "\u8868\u5f81\u76f8\u4f3c\u6027\u5ea6\u91cf\u662f\u795e\u7ecf\u79d1\u5b66\u548c\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u5173\u952e\u5de5\u5177\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u5176\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\uff08\u6db5\u76d6\u591a\u79cd\u67b6\u6784\u548c\u8bad\u7ec3\u8303\u5f0f\uff09\u95f4\u533a\u5206\u80fd\u529b\u8fdb\u884c\u7cfb\u7edf\u6027\u6bd4\u8f83\u7684\u7814\u7a76\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u5b9a\u91cf\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30\u8868\u5f81\u76f8\u4f3c\u6027\u5ea6\u91cf\u533a\u5206\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\uff08\u5305\u62ecCNNs, Vision Transformers, Swin Transformers, ConvNeXt\u7b49\u67b6\u6784\uff0c\u4ee5\u53ca\u76d1\u7763\u5b66\u4e60\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u7b49\u8bad\u7ec3\u65b9\u5f0f\uff09\u7684\u80fd\u529b\u6765\u8fdb\u884c\u8bc4\u4f30\u3002\u91c7\u7528\u4fe1\u53f7\u68c0\u6d4b\u7406\u8bba\u7684d'\u3001\u8f6e\u5ed3\u7cfb\u6570\u548cROC-AUC\u4e09\u79cd\u53ef\u5206\u79bb\u6027\u5ea6\u91cf\uff0c\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e86RSA\u3001\u7ebf\u6027\u9884\u6d4b\u6027\u3001Procrustes\u5bf9\u9f50\u548c\u8f6f\u5339\u914d\u7b49\u5e38\u7528\u76f8\u4f3c\u6027\u5ea6\u91cf\u7684\u5224\u522b\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u76f8\u4f3c\u6027\u5ea6\u91cf\u65bd\u52a0\u66f4\u4e25\u683c\u7684\u5bf9\u9f50\u7ea6\u675f\u65f6\uff0c\u6a21\u578b\u5bb6\u65cf\u95f4\u7684\u53ef\u5206\u79bb\u6027\u4f1a\u7cfb\u7edf\u6027\u589e\u5f3a\u3002\u5728\u57fa\u4e8e\u6620\u5c04\u7684\u65b9\u6cd5\u4e2d\uff0c\u8f6f\u5339\u914d\u6cd5\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u53ef\u5206\u79bb\u6027\uff0c\u5176\u6b21\u662fProcrustes\u5bf9\u9f50\u548c\u7ebf\u6027\u9884\u6d4b\u6027\u3002\u975e\u62df\u5408\u65b9\u6cd5\uff08\u5982RSA\uff09\u4e5f\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u95f4\u5c55\u73b0\u51fa\u5f88\u5f3a\u7684\u53ef\u5206\u79bb\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u9996\u6b21\u4ece\u53ef\u5206\u79bb\u6027\u89c6\u89d2\u5bf9\u76f8\u4f3c\u6027\u5ea6\u91cf\u8fdb\u884c\u4e86\u7cfb\u7edf\u6bd4\u8f83\uff0c\u660e\u786e\u4e86\u5b83\u4eec\u5404\u81ea\u7684\u76f8\u5bf9\u654f\u611f\u6027\uff0c\u4e3a\u5728\u5927\u89c4\u6a21\u6a21\u578b\u548c\u8111\u6bd4\u8f83\u7814\u7a76\u4e2d\u9009\u62e9\u5408\u9002\u7684\u5ea6\u91cf\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2509.04464", "pdf": "https://arxiv.org/pdf/2509.04464", "abs": "https://arxiv.org/abs/2509.04464", "authors": ["Yang Nan", "Pengfei He", "Ravi Tandon", "Han Xu"], "title": "Can Multiple Responses from an LLM Reveal the Sources of Its Uncertainty?", "categories": ["cs.CL", "cs.AI"], "comment": "Proceedings of The 2025 Conference on Empirical Methods in Natural\n  Language Processing (Findings)", "summary": "Large language models (LLMs) have delivered significant breakthroughs across\ndiverse domains but can still produce unreliable or misleading outputs, posing\ncritical challenges for real-world applications. While many recent studies\nfocus on quantifying model uncertainty, relatively little work has been devoted\nto \\textit{diagnosing the source of uncertainty}. In this study, we show that,\nwhen an LLM is uncertain, the patterns of disagreement among its multiple\ngenerated responses contain rich clues about the underlying cause of\nuncertainty. To illustrate this point, we collect multiple responses from a\ntarget LLM and employ an auxiliary LLM to analyze their patterns of\ndisagreement. The auxiliary model is tasked to reason about the likely source\nof uncertainty, such as whether it stems from ambiguity in the input question,\na lack of relevant knowledge, or both. In cases involving knowledge gaps, the\nauxiliary model also identifies the specific missing facts or concepts\ncontributing to the uncertainty. In our experiment, we validate our framework\non AmbigQA, OpenBookQA, and MMLU-Pro, confirming its generality in diagnosing\ndistinct uncertainty sources. Such diagnosis shows the potential for relevant\nmanual interventions that improve LLM performance and reliability.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u5206\u6790LLM\u591a\u54cd\u5e94\u95f4\u5dee\u5f02\u6a21\u5f0f\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u8f85\u52a9LLM\u8bca\u65adLLM\u4e0d\u786e\u5b9a\u6027\u7684\u6765\u6e90\uff08\u5982\u8f93\u5165\u6b67\u4e49\u6216\u77e5\u8bc6\u7f3a\u5931\uff09\uff0c\u5e76\u5df2\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u5176\u901a\u7528\u6027\uff0c\u4ee5\u671f\u901a\u8fc7\u8bca\u65ad\u5b9e\u73b0\u5e72\u9884\u6765\u63d0\u5347LLM\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8f93\u51fa\u53ef\u80fd\u4e0d\u53ef\u9760\u6216\u8bef\u5bfc\uff0c\u5bf9\u5b9e\u9645\u5e94\u7528\u6784\u6210\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u91cf\u5316\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u5bf9\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u7684\u8bca\u65ad\u5de5\u4f5c\u8f83\u5c11\u3002", "method": "\u5f53LLM\u4e0d\u786e\u5b9a\u65f6\uff0c\u901a\u8fc7\u6536\u96c6\u76ee\u6807LLM\u7684\u591a\u4e2a\u54cd\u5e94\uff0c\u5e76\u5229\u7528\u4e00\u4e2a\u8f85\u52a9LLM\u5206\u6790\u8fd9\u4e9b\u54cd\u5e94\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6a21\u5f0f\u3002\u8f85\u52a9\u6a21\u578b\u8d1f\u8d23\u63a8\u65ad\u4e0d\u786e\u5b9a\u6027\u7684\u53ef\u80fd\u6765\u6e90\uff08\u5982\u8f93\u5165\u95ee\u9898\u6b67\u4e49\u3001\u76f8\u5173\u77e5\u8bc6\u7f3a\u4e4f\u6216\u4e24\u8005\u517c\u6709\uff09\uff0c\u5e76\u80fd\u8bc6\u522b\u77e5\u8bc6\u7f3a\u5931\u7684\u5177\u4f53\u4e8b\u5b9e\u6216\u6982\u5ff5\u3002", "result": "\u8be5\u6846\u67b6\u5728AmbigQA\u3001OpenBookQA\u548cMMLU-Pro\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u8bca\u65ad\u4e0d\u540c\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u65b9\u9762\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u5bf9LLM\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\u7684\u8bca\u65ad\u4e3a\u8fdb\u884c\u76f8\u5173\u4eba\u5de5\u5e72\u9884\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8LLM\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2509.04669", "pdf": "https://arxiv.org/pdf/2509.04669", "abs": "https://arxiv.org/abs/2509.04669", "authors": ["Mustafa Munir", "Alex Zhang", "Radu Marculescu"], "title": "VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Proceedings of the 2025 IEEE/CVF International Conference on Computer\n  Vision (ICCV) Workshops", "summary": "Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs)\nhave challenged the dominance of Convolutional Neural Networks (CNNs) in\ncomputer vision. ViTs excel at capturing global context, and SSMs like Mamba\noffer linear complexity for long sequences, yet they do not capture\nfine-grained local features as effectively as CNNs. Conversely, CNNs possess\nstrong inductive biases for local features but lack the global reasoning\ncapabilities of transformers and Mamba. To bridge this gap, we introduce\n\\textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs\nand multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a\nhierarchical structure with convolutional blocks in its early stages to extract\nrich local features. These convolutional blocks are then processed by later\nstages incorporating multi-directional Mamba blocks designed to efficiently\nmodel long-range dependencies and global context. This hybrid design allows for\nsuperior feature representation while maintaining linear complexity with\nrespect to image resolution. We demonstrate VCMamba's effectiveness through\nextensive experiments on ImageNet-1K classification and ADE20K semantic\nsegmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K,\nsurpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming\nVision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains\n47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing\n62% fewer parameters. Code is available at\nhttps://github.com/Wertyuui345/VCMamba.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faVCMamba\uff0c\u4e00\u79cd\u7ed3\u5408CNNs\u548c\u591a\u5411Mamba SSMs\u7684\u65b0\u578b\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\uff0c\u65e8\u5728\u9ad8\u6548\u878d\u5408\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u4e0e\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u5e76\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e0a\u53d6\u5f97\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5404\u6709\u4f18\u52a3\uff1aViTs\u548cSSMs\u64c5\u957f\u5168\u5c40\u4e0a\u4e0b\u6587\u4f46\u5c40\u90e8\u7279\u5f81\u4e0d\u8db3\uff1bCNNs\u64c5\u957f\u5c40\u90e8\u7279\u5f81\u4f46\u7f3a\u4e4f\u5168\u5c40\u63a8\u7406\u80fd\u529b\u3002\u7814\u7a76\u52a8\u673a\u662f\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5f00\u53d1\u4e00\u4e2a\u80fd\u540c\u65f6\u6355\u6349\u7ec6\u7c92\u5ea6\u5c40\u90e8\u7279\u5f81\u548c\u957f\u7a0b\u4f9d\u8d56\u7684\u6df7\u5408\u6a21\u578b\uff0c\u5e76\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\u3002", "method": "VCMamba\u91c7\u7528\u6df7\u5408\u8bbe\u8ba1\uff1a\u65e9\u671f\u9636\u6bb5\u4f7f\u7528\u5377\u79ef\u5e72\uff08convolutional stem\uff09\u548c\u5377\u79ef\u5757\uff08convolutional blocks\uff09\u63d0\u53d6\u4e30\u5bcc\u7684\u5c40\u90e8\u7279\u5f81\uff1b\u540e\u671f\u9636\u6bb5\u6574\u5408\u591a\u5411Mamba\u5757\uff08multi-directional Mamba blocks\uff09\u6765\u9ad8\u6548\u5efa\u6a21\u957f\u7a0b\u4f9d\u8d56\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u3002\u8fd9\u79cd\u5206\u5c42\u7ed3\u6784\u786e\u4fdd\u4e86\u4f18\u8d8a\u7684\u7279\u5f81\u8868\u793a\uff0c\u5e76\u7ef4\u6301\u4e86\u4e0e\u56fe\u50cf\u5206\u8fa8\u7387\u76f8\u5173\u7684\u7ebf\u6027\u590d\u6742\u5ea6\u3002", "result": "VCMamba\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff1aVCMamba-B\u5728ImageNet-1K\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u523082.6%\u7684top-1\u51c6\u786e\u7387\uff0c\u6bd4PlainMamba-L3\u9ad80.3%\u4e14\u53c2\u6570\u51cf\u5c1137%\uff0c\u6bd4Vision GNN-B\u9ad80.3%\u4e14\u53c2\u6570\u51cf\u5c1164%\u3002\u5728ADE20K\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\uff0cVCMamba-B\u83b7\u5f9747.1 mIoU\uff0c\u8d85\u8d8aEfficientFormer-L7 2.0 mIoU\u4e14\u53c2\u6570\u51cf\u5c1162%\u3002", "conclusion": "VCMamba\u6210\u529f\u5730\u6574\u5408\u4e86CNNs\u5728\u5c40\u90e8\u7279\u5f81\u65b9\u9762\u7684\u4f18\u52bf\u548c\u591a\u5411Mamba SSMs\u5728\u5168\u5c40\u4e0a\u4e0b\u6587\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u3002\u5b83\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e0a\u5747\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u7279\u5f81\u8868\u793a\u80fd\u529b\u548c\u53c2\u6570\u6548\u7387\u3002"}}
{"id": "2509.04871", "pdf": "https://arxiv.org/pdf/2509.04871", "abs": "https://arxiv.org/abs/2509.04871", "authors": ["Krittanon Kaewtawee", "Wachiravit Modecrua", "Krittin Pachtrachai", "Touchapon Kraisingkorn"], "title": "Cloning a Conversational Voice AI Agent from Call\\,Recording Datasets for Telesales", "categories": ["cs.AI", "cs.LG"], "comment": "10 pages, 4 figures", "summary": "Recent advances in language and speech modelling have made it possible to\nbuild autonomous voice assistants that understand and generate human dialogue\nin real time. These systems are increasingly being deployed in domains such as\ncustomer service and healthcare care, where they can automate repetitive tasks,\nreduce operational costs, and provide constant support around the clock. In\nthis paper, we present a general methodology for cloning a conversational voice\nAI agent from a corpus of call recordings. Although the case study described in\nthis paper uses telesales data to illustrate the approach, the underlying\nprocess generalizes to any domain where call transcripts are available. Our\nsystem listens to customers over the telephone, responds with a synthetic\nvoice, and follows a structured playbook learned from top performing human\nagents. We describe the domain selection, knowledge extraction, and prompt\nengineering used to construct the agent, integrating automatic speech\nrecognition, a large language model based dialogue manager, and text to speech\nsynthesis into a streaming inference pipeline. The cloned agent is evaluated\nagainst human agents on a rubric of 22 criteria covering introduction, product\ncommunication, sales drive, objection handling, and closing. Blind tests show\nthat the AI agent approaches human performance in routine aspects of the call\nwhile underperforming in persuasion and objection handling. We analyze these\nshortcomings and refine the prompt accordingly. The paper concludes with design\nlessons and avenues for future research, including large scale simulation and\nautomated evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u901a\u8bdd\u5f55\u97f3\u8bed\u6599\u5e93\u4e2d\u514b\u9686\u4f1a\u8bdd\u8bed\u97f3AI\u4ee3\u7406\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u5e76\u4ee5\u7535\u8bdd\u9500\u552e\u6570\u636e\u4e3a\u4f8b\u8fdb\u884c\u9a8c\u8bc1\u3002\u8be5AI\u4ee3\u7406\u5728\u5e38\u89c4\u901a\u8bdd\u65b9\u9762\u63a5\u8fd1\u4eba\u7c7b\u8868\u73b0\uff0c\u4f46\u5728\u8bf4\u670d\u548c\u5f02\u8bae\u5904\u7406\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u548c\u8bed\u97f3\u5efa\u6a21\u7684\u8fdb\u6b65\uff0c\u6784\u5efa\u80fd\u5b9e\u65f6\u7406\u89e3\u548c\u751f\u6210\u4eba\u7c7b\u5bf9\u8bdd\u7684\u81ea\u4e3b\u8bed\u97f3\u52a9\u624b\u6210\u4e3a\u53ef\u80fd\u3002\u8fd9\u4e9b\u7cfb\u7edf\u5728\u5ba2\u6237\u670d\u52a1\u548c\u533b\u7597\u4fdd\u5065\u7b49\u9886\u57df\u90e8\u7f72\uff0c\u53ef\u81ea\u52a8\u5316\u91cd\u590d\u4efb\u52a1\u3001\u964d\u4f4e\u8fd0\u8425\u6210\u672c\u5e76\u63d0\u4f9b\u5168\u5929\u5019\u652f\u6301\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u901a\u8bdd\u5f55\u97f3\u8bed\u6599\u5e93\u514b\u9686\u4f1a\u8bdd\u8bed\u97f3AI\u4ee3\u7406\u7684\u901a\u7528\u65b9\u6cd5\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u542c\u53d6\u5ba2\u6237\uff0c\u4f7f\u7528\u5408\u6210\u8bed\u97f3\u56de\u5e94\uff0c\u5e76\u9075\u5faa\u4ece\u9876\u5c16\u4eba\u7c7b\u4ee3\u7406\u5b66\u4e60\u7684\u7ed3\u6784\u5316\u5267\u672c\u3002\u5b83\u6574\u5408\u4e86ASR\u3001\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5bf9\u8bdd\u7ba1\u7406\u5668\u548c\u6587\u672c\u5230\u8bed\u97f3\u5408\u6210\uff08TTS\uff09\u5230\u4e00\u4e2a\u6d41\u5f0f\u63a8\u7406\u7ba1\u9053\u4e2d\u3002\u901a\u8fc7\u9886\u57df\u9009\u62e9\u3001\u77e5\u8bc6\u63d0\u53d6\u548c\u63d0\u793a\u5de5\u7a0b\u6765\u6784\u5efa\u4ee3\u7406\u3002\u514b\u9686\u4ee3\u7406\u572822\u9879\u6807\u51c6\uff08\u6db5\u76d6\u4ecb\u7ecd\u3001\u4ea7\u54c1\u6c9f\u901a\u3001\u9500\u552e\u63a8\u52a8\u3001\u5f02\u8bae\u5904\u7406\u548c\u7ed3\u675f\u8bed\uff09\u4e0a\u4e0e\u4eba\u7c7b\u4ee3\u7406\u8fdb\u884c\u76f2\u6d4b\u8bc4\u4f30\u3002", "result": "\u76f2\u6d4b\u7ed3\u679c\u663e\u793a\uff0c\u8be5AI\u4ee3\u7406\u5728\u901a\u8bdd\u7684\u5e38\u89c4\u65b9\u9762\u63a5\u8fd1\u4eba\u7c7b\u8868\u73b0\uff0c\u4f46\u5728\u8bf4\u670d\u548c\u5f02\u8bae\u5904\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u7814\u7a76\u6839\u636e\u8fd9\u4e9b\u4e0d\u8db3\u5bf9\u63d0\u793a\u8fdb\u884c\u4e86\u4f18\u5316\u548c\u5b8c\u5584\u3002", "conclusion": "\u8bba\u6587\u603b\u7ed3\u4e86\u8bbe\u8ba1\u7ecf\u9a8c\u6559\u8bad\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u5927\u89c4\u6a21\u6a21\u62df\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u3002AI\u4ee3\u7406\u5728\u5e38\u89c4\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u66f4\u9ad8\u60c5\u5546\u548c\u5e94\u53d8\u80fd\u529b\u7684\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\u3002"}}
