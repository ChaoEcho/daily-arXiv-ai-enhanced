<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 26]
- [cs.CV](#cs.CV) [Total: 81]
- [cs.AI](#cs.AI) [Total: 31]
- [cs.LG](#cs.LG) [Total: 64]
- [cs.NI](#cs.NI) [Total: 8]
- [q-fin.GN](#q-fin.GN) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 6]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.CE](#cs.CE) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.SI](#cs.SI) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.SE](#cs.SE) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: 为解决现有LLM偏见评估数据集缺乏中文和多任务能力的问题，本文提出了一个多任务中文偏见评估基准McBE，并评估发现多种流行LLM均存在不同程度的偏见。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，其固有的偏见逐渐显现，因此测量LLMs的偏见对于降低伦理风险至关重要。然而，现有偏见评估数据集多集中于英语和北美文化，缺乏中文和多任务评估能力，无法全面衡量LLMs的偏见。

Method: 本文提出了一个多任务中文偏见评估基准（McBE），包含4,077个评估实例，覆盖12个单一偏见类别和82个子类别，并引入了5种评估任务，以实现广泛的类别覆盖、内容多样性和全面的衡量。研究者使用该基准评估了多种系列和参数规模的流行LLMs。

Result: 评估结果显示，所有被测试的LLMs都表现出不同程度的偏见。

Conclusion: 研究对评估结果进行了深入分析，为LLMs中的偏见提供了新的见解。McBE基准有效地揭示了LLMs在中文语境下的偏见问题，为未来的偏见研究和缓解工作提供了重要工具。

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [2] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 研究发现，在对话摘要任务中，分步推理（如CoT）大型语言模型非但未能持续提升摘要质量，反而常导致冗长和不一致，表现不如非推理模型。


<details>
  <summary>Details</summary>
Motivation: 对话摘要具有重要的实际价值，但大型语言模型（LLMs）中分步推理架构（如长链思维CoT）在需要同时兼顾抽象和简洁的对话场景中的性能尚未被充分探索。

Method: 本研究首次对最先进的推理LLMs和非推理LLMs在三种主要对话摘要范式（通用型、角色导向型和查询导向型）下进行了全面系统的评估。评估涵盖了不同语言、领域和摘要长度，利用了SAMSum、DialogSum、CSDS和QMSum等基准数据集，并采用了基于LLM的自动指标和人类启发式标准等先进评估协议。

Result: 与其它推理密集型任务的趋势相反，研究结果表明，明确的分步推理并不能持续提升对话摘要质量。相反，与非推理模型相比，推理LLMs往往更容易出现冗长、事实不一致和摘要简洁度不足的问题。通过情景特定分析和详细案例研究，进一步揭示了在复杂对话语境中，明确推理何时以及为何会失效甚至阻碍摘要任务。

Conclusion: 本工作为当前推理LLMs的局限性提供了新的见解，并强调了针对真实世界对话摘要任务制定有针对性的建模和评估策略的必要性。

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [3] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 研究了深度递归Transformer模型Huginn-3.5B在潜在思维链推理上的表现，发现其可解释性有限、存在探测不一致性，且性能不及显式外部化推理模型。


<details>
  <summary>Details</summary>
Motivation: 标准思维链（CoT）推理虽提高了模型可解释性但牺牲了效率，且难以捕获非语言表达的推理。为探索在潜在空间中实现内部化推理（潜在CoT）的可行性，本文旨在研究深度递归Transformer模型Huginn-3.5B是否能产生此类推理结构。

Method: 通过在算术任务上应用Logit Lens和Coda Lens等一系列探测技术，分析了Huginn-3.5B的内部行为，包括跟踪最终和中间结果token的秩轨迹，以探究模型隐藏状态的可解释性。

Result: 1. 发现可解释的潜在CoT证据有限。2. 探测结果在递归块之间存在显著不一致性，隐藏状态的可解释性高度依赖于层索引和解码方法。3. 经验证，增加递归深度仅带来微小收益，且其性能远低于显式外部化推理的模型。

Conclusion: 深度递归Transformer模型Huginn-3.5B在实现可解释的潜在CoT方面存在局限，其内部推理过程的解释性面临挑战，且在性能上未能超越显式外部化CoT推理的模型。

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [4] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: 一个名为GDC Cohort Copilot的工具，通过自然语言描述自动生成癌症基因组队列，其自定义LLM模型在生成GDC队列方面表现优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: GDC用户在通过图形界面创建复杂癌症基因组队列时，难以从数百个字段中找到特定描述符。然而，用户可能更擅长使用自由文本自然语言描述其所需队列。

Method: 引入GDC Cohort Copilot，一个开源辅助工具，能将用户输入的自然语言描述自动转换为GDC队列过滤器，并提供交互式用户界面进行细化。该工具开发并评估了多个大型语言模型（LLMs），其中包含一个本地部署的开源GDC Cohort LLM。

Result: 团队开发的本地部署开源GDC Cohort LLM在生成GDC队列方面，比使用GPT-4o提示方法取得了更好的结果。

Conclusion: GDC Cohort Copilot成功地利用自然语言处理技术简化了GDC中癌症基因组队列的创建过程，其定制的LLM模型在特定任务上超越了通用大型模型，显著提升了数据访问和分析的便利性。

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [5] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为MemAgent的新型智能体工作流，通过分段阅读和覆盖式记忆更新，并结合扩展的DAPO算法，实现了对超长文本的有效处理和卓越的上下文外推能力。


<details>
  <summary>Details</summary>
Motivation: 尽管在长度外推、高效注意力及内存模块方面已有改进，但在长文本处理中，以线性复杂度处理无限长文档且在外推时无性能下降，仍是终极挑战。

Method: 研究采用端到端方式直接优化长文本任务，引入了MemAgent工作流。MemAgent通过分段阅读文本并采用覆盖策略更新记忆。此外，还扩展了DAPO算法，通过独立上下文多对话生成来促进训练。

Result: MemAgent展现了卓越的长上下文处理能力，能够从8K上下文训练（基于32K文本）外推到3.5M的问答任务，性能损失小于5%，并在512K RULER测试中达到95%以上的准确率。

Conclusion: MemAgent成功解决了长文本处理中的关键挑战，通过创新的工作流和训练方法，实现了在极长上下文条件下的高性能表现和出色的外推能力。

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [6] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: 针对现有持续领域自适应预训练（continual DAP）的局限性，本文提出DoMIX方法，通过利用LoRA模块实现高效、鲁棒且能为特定任务定制模型的DAP，并可扩展至LLM微调。


<details>
  <summary>Details</summary>
Motivation: 现有持续领域自适应预训练方法面临高计算成本和内存消耗、对增量数据顺序敏感以及仅提供单一通用模型（与DAP定制化本质相悖）等局限性。

Method: 本文提出DoMIX，一种新颖的持续领域自适应预训练方法，通过利用参数高效微调（PEFT）方法中的LoRA模块来解决现有挑战。

Result: DoMIX实现了高效并行且对领域顺序鲁棒的领域自适应预训练，能够有效利用累积知识为特定任务提供定制化的预训练模型。该方法还被证明可扩展应用于标准的大语言模型（LLM）微调场景。

Conclusion: DoMIX通过引入LoRA模块，成功克服了现有持续领域自适应预训练方法的缺点，提供了一种高效、灵活且能为特定任务定制模型的解决方案，并展现了其在更广泛LLM微调场景中的应用潜力。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [7] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: 本文介绍了一种用于SciVQA 2025科学视觉问答共享任务的系统，该系统结合了多模态大型语言模型集成和少样本检索策略，在盲测数据上排名第三，F1平均得分为85.12。


<details>
  <summary>Details</summary>
Motivation: 论文的研究动机是参与SciVQA 2025科学视觉问答共享任务，并描述其为该任务开发的系统及其性能。

Method: 该系统采用两个多模态大型语言模型（MLLMs）的集成。它根据图表和问题类型选择模型和少样本设置，并结合多种少样本示例检索策略。此外，系统根据模型的置信水平选择最终答案。

Result: 在盲测数据上，该系统在七个参与者中排名第三，在ROUGE-1、ROUGE-L和BERTS指标上的平均F1得分为85.12。

Conclusion: 该系统在SciVQA 2025任务中表现出竞争力，证明了集成多模态大型语言模型结合自适应少样本策略和置信度选答案的有效性。

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [8] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: 本文提出QFFN-BERT，一个混合量子-经典Transformer模型，通过将BERT中的前馈网络（FFN）替换为参数化量子电路（PQC），在大幅减少参数的同时，实现了性能超越经典模型，并在少样本学习中展现出优异的数据效率。


<details>
  <summary>Details</summary>
Motivation: 经典Transformer编码器中的前馈网络（FFN）模块占据了约三分之二的参数，参数量贡献巨大。研究动机在于探索使用参数化量子电路（PQCs）作为FFN的替代方案，以降低参数量并提升模型表达能力。

Method: 提出了QFFN-BERT，一个混合量子-经典Transformer，用PQC层替换了BERT变体中的FFN模块。研究系统地探讨了PQC深度、表达能力和可训练性之间的权衡。最终的PQC架构融入了残差连接、Ry和Rz旋转门以及交替纠缠策略，以确保训练稳定性和高表达能力。实验在经典模拟器上进行，并在SST-2和DBpedia基准数据集上进行了评估。

Result: 1. 经过精心配置的QFFN-BERT，在全数据设置下，其准确率达到基线模型的102.0%，超越了其经典对应模型，同时将FFN相关的参数减少了99%以上。2. 该模型在少样本学习场景中展现出持续且有竞争力的优势，证实了其卓越的数据效率潜力。

Conclusion: 本研究结果，辅以消融研究，证实了当与深度学习基本原则共同设计时，参数化量子电路（PQCs）可以作为经典前馈网络（FFNs）强大且参数高效的替代品。

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [9] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: 提出一种基于参数模型的代码数据选择方法，通过确保数据分布一致性和多样性，以少量高质量数据显著提升大型语言模型的代码生成性能和训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在代码生成方面过度依赖数据量而非质量，导致训练效率低下。本研究旨在通过优化数据选择来解决这一问题。

Method: 引入并优化一个参数模型进行代码数据选择。该方法旨在确保所选数据子集的分布一致性和多样性，从而保证数据质量。

Result: 仅使用10K样本，该方法在HumanEval和MBPP上分别比92K全样本基线提高了2.4%和2.3%，并且在性能和效率上均优于其他采样方法。

Conclusion: 该方法能够有效提升模型性能，并显著降低计算成本。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [10] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: 本研究评估了Akan语言ASR模型在多领域语境下的泛化能力，发现模型存在领域依赖性，且Whisper和Wav2Vec2架构的错误行为不同，强调了低资源语言ASR领域适应的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有ASR研究主要在域内数据集上评估模型，但很少评估模型在不同语音语境下的泛化能力，本研究旨在弥补这一空白。

Method: 使用四种涵盖不同领域的Akan语音语料库，对七个基于Transformer架构（如Whisper和Wav2Vec2）的Akan ASR模型进行了基准测试，并比较了词错误率（WER）和字符错误率（CER）。

Result: 模型表现出明显的领域依赖性，仅在其训练领域内表现最佳，在不匹配场景下准确性显著下降。Whisper生成更流畅但可能误导性的转录错误，而Wav2Vec2在遇到不熟悉输入时产生更明显但难以解释的输出。

Conclusion: ASR错误中存在可读性与透明度之间的权衡，在为低资源语言（LRL）选择架构时应予以考虑。结果强调了Akan及其他低资源语言需要有针对性的领域适应技术、自适应路由策略和多语言训练框架。

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [11] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: 本研究提出一种为低资源语种（特别是阿坎语）受损语音构建自动语音识别（ASR）模型的数据收集方法，通过社区驱动模式开发了“食谱”和工具，并创建了首个受损阿坎语开放数据集。


<details>
  <summary>Details</summary>
Motivation: 旨在民主化ASR技术和数据收集，解决受损语音（特别是低资源语种）ASR数据和模型稀缺的问题，以构建更具包容性的ASR技术。

Method: 开发了一套社区驱动数据收集和ASR模型构建的最佳实践“食谱”及培训方案；策划并构建了首个开放源代码的受损阿坎语语音数据集；招募言语障碍参与者进行数据收集；并对开源ASR模型进行了微调以识别受损阿坎语语音。

Result: 成功策划并发布了首个开放源代码的受损阿坎语语音数据集；开发了相应的数据收集“食谱”和开源工具；获得了微调开源ASR模型以识别受损阿坎语语音的初步结果。所有资源均已公开可用。

Conclusion: 本研究提出的方法、数据集和工具，有助于推动为言语障碍人士（尤其是在低资源环境下）开发包容性ASR技术，并促进社区驱动的数据收集与模型构建。

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [12] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 引入首个印度保释判决数据集IndianBailJudgments-1200，解决当地法律NLP数据匮乏问题。


<details>
  <summary>Details</summary>
Motivation: 印度等地区法律NLP领域发展滞后，主要原因在于缺乏结构化的法律数据集。

Method: 本研究构建了IndianBailJudgments-1200数据集，包含1200份印度法院保释判决书，并使用提示工程（prompt-engineered）的GPT-4o流水线对20多个属性（如保释结果、IPC章节、犯罪类型和法律推理）进行标注，同时验证了标注的一致性。

Result: 成功构建并发布了IndianBailJudgments-1200数据集，这是首个专门针对印度保释司法实践的公开可用数据集，内含1200份经过详细标注的印度保释判决数据。

Conclusion: 该数据集为印度法律NLP研究提供了重要资源，能够支持多种法律NLP任务，例如结果预测、摘要生成和公平性分析，有望推动该领域的发展。

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [13] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebSailor是一种后训练方法，通过模拟高不确定性信息检索任务，显著提升了开源代理在复杂信息搜索中的能力，使其性能达到专有代理水平。


<details>
  <summary>Details</summary>
Motivation: 现有开源大型语言模型在处理复杂信息搜索任务中的极端不确定性方面存在认知局限，无法达到像DeepResearch等专有代理系统所展现的超人能力。

Method: 引入WebSailor后训练方法，通过结构化采样和信息混淆生成高不确定性任务，结合RFT冷启动和一个高效的代理强化学习训练算法（重复采样策略优化，DUPO）进行训练。

Result: WebSailor在复杂信息搜索任务中显著超越了所有开源代理。

Conclusion: WebSailor成功地将开源代理的性能提升至与专有代理相当的水平，弥补了能力差距。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [14] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 论文探讨主动学习（AL）中被忽视的人类标注差异（HLV）问题，并提出了一个概念框架，旨在将HLV整合到AL流程中，以更好地处理现实世界标注的复杂性。


<details>
  <summary>Details</summary>
Motivation: 高质量标注数据是监督学习的瓶颈。现有标注框架和主动学习（AL）方法常忽略人类标注差异（HLV），假定存在单一“真实标签”。然而，HLV可作为有益信号，而非单纯噪声，需要被纳入AL过程以优化有限标注预算的使用。

Method: 审视关于“真理”和标签性质的基本假设，强调将观察到的标签差异（LV）分解为信号（如HLV）和噪声（如标注错误）。调查主动学习（AL）和（人类）标注差异（HLV）研究社区如何处理或忽视这些区别。提出了一个概念框架，用于在主动学习循环的实例选择、标注者选择和标签表示等环节中整合HLV。还讨论了将大型语言模型（LLM）作为标注者的整合。

Result: 提出了一个将人类标注差异（HLV）融入主动学习（AL）的概念框架，并强调了将观察到的标签差异（LV）分解为信号（HLV）和噪声的重要性。此外，还探讨了大型语言模型（LLM）作为标注者的应用潜力。

Conclusion: 为开发能够更好反映真实世界标注复杂性的人类标注差异感知型主动学习（HLV-aware AL）奠定了概念基础。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [15] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: Multiperspective Fusion (MPF) 是一种新型的LLM后训练对齐框架，旨在通过多视角生成和与人类基线对齐来缓解偏见，无需大量微调或提示工程。


<details>
  <summary>Details</summary>
Motivation: 为应对大型语言模型 (LLM) 中日益增长的偏见缓解需求，开发一种易于使用且高效的偏见缓解方法。

Method: MPF构建于SAGED管道之上，通过利用多视角生成来暴露LLM输出中的偏见，并将其与细致、类人基线对齐。它通过将基线（如人力资源专业人员的情感分布）分解为可解释的视角组件，并根据这些分解出的概率对响应进行采样和平衡来引导LLM生成。

Result: 实验证明，MPF能够将LLM的情感分布与反事实基线（绝对平等）和HR基线（偏向顶尖大学）对齐。这导致了较小的KL散度，减少了校准误差，并能泛化到未见的问题。

Conclusion: MPF提供了一种可扩展、可解释的对齐和偏见缓解方法，与已部署的LLM兼容，且无需进行广泛的提示工程或模型微调。

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [16] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: 本文研究性别与上下文偏见（如动作动词、宾语名词和职业）之间的相关性，并引入新数据集和框架来估计和解释性别偏见，发现性别偏见不仅限于职业刻板印象。


<details>
  <summary>Details</summary>
Motivation: 探究性别与上下文偏见（包括动作动词、宾语名词和职业）之间的相关性，并旨在建立一个能估计和解释上下文及性别偏见的方法。

Method: 引入了一个新颖的数据集GenderLexicon和一个能够估计上下文偏见及其相关性别偏见的框架。该模型能用分数解释偏见，提升了性别偏见的解释性。通过在包括日语数据集在内的五个多样化数据集上进行评估来验证方法。

Result: 发现性别偏见不仅限于职业刻板印象。所提出的模型能够通过分数解释偏见，提高了性别偏见的解释性。在多个数据集上的评估验证了其方法的有效性。

Conclusion: 本研究成功地揭示了性别与上下文偏见之间的相关性，并开发了一个新颖的工具（数据集和框架）来量化和解释这些偏见，证实了性别偏见的存在范围超出职业刻板印象。

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [17] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出使用大型语言模型（LLMs）辅助同行评审，特别是识别论文局限性。为此，研究者构建了局限性分类法，并引入了首个综合基准LimitGen（包含合成和真实数据集），同时通过文献检索增强了LLMs识别局限性的能力。


<details>
  <summary>Details</summary>
Motivation: 同行评审对科学研究至关重要，但日益增长的出版量加剧了这项专业密集型工作的挑战。尽管LLMs在各种科学任务中展现潜力，但其在协助同行评审，尤其是在识别论文局限性方面的潜力仍未得到充分研究。

Method: 首先，构建了科学研究中局限性类型的综合分类法（侧重AI领域）。其次，提出了LimitGen基准，用于评估LLMs在早期反馈和辅助人类同行评审中识别局限性的能力，该基准包含LimitGen-Syn（通过高质量论文的受控扰动创建的合成数据集）和LimitGen-Human（真实人类编写的局限性集合）。最后，通过文献检索来增强LLM系统，使其能够更好地识别局限性，并基于先前的科学发现提供支持。

Result: 研究方法显著增强了LLM系统在研究论文中生成局限性的能力，使其能够提供更具体和建设性的反馈。

Conclusion: 通过构建分类法、提出综合基准LimitGen以及采用文献检索增强LLMs，能够有效提升LLMs识别论文局限性的能力，从而有望改善同行评审过程，提供更有价值的反馈。

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [18] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

TL;DR: 首次测量了英语前元音的“刚可产生差异”（JPD），范围在F1 X F2空间的14至51 mels之间，这有助于理解语音控制精度和元音系统结构。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明人类元音产生受听觉空间控制，且存在次音位级别的控制，但其精确度尚不明确。本研究旨在量化这种控制的准确性，即探究两个元音刺激在听觉空间中需要相距多远才能产生可靠的不同模仿。

Method: 采用元音模仿范式，对两组英语使用者在前元音生产过程中进行实验，以推导并测量“刚可产生差异”（JPD）。

Result: 研究估计，JPD在F1 X F2空间中介于14至51 mels之间。

Conclusion: 该发现对语音产生的插曲理论具有重要意义，并通过设定两个元音音位在说话者共振峰空间中可能的最接近距离的理论下限，阐明了人类元音系统的可能结构，并为观察到的元音音位数量和模式趋势提供了心理物理学解释。

Abstract: A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


### [19] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）存在“自我纠错盲点”，即无法纠正自身输出中的错误。研究发现这与训练数据构成有关，且简单地添加“Wait”提示可显著提升其自我纠错能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型已具变革性，但它们仍会犯错并探索无效的推理路径。自我纠错对于可信赖的自回归LLM至关重要。研究发现LLM能识别用户输入中的错误，却普遍存在“自我纠错盲点”，即无法纠正自身输出中的相同错误，因此需要系统性研究此现象。

Method: 引入“Self-Correction Bench”框架，通过在三个复杂级别上注入受控错误来系统测量此现象。测试了14个模型，并分析了该局限性与训练数据构成之间的关系。此外，尝试通过简单提示语（如“Wait”）来激活模型潜在的纠错能力。

Result: 测试结果显示LLM的平均盲点率为64.5%。有证据表明此局限性与训练数据构成有关：人类训练演示多为无错误响应，与通过结果反馈学习纠错的强化学习模型不同。值得注意的是，简单地附加“Wait”可将盲点率降低89.3%。

Conclusion: 该研究揭示了当前LLM的一个关键局限性——自我纠错盲点，并表明模型纠错能力可能存在但需被激活。研究为提升LLM的可靠性和可信赖性提供了潜在方向。

Abstract: Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [20] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
*Riccardo Cantini,Nicola Gabriele,Alessio Orsino,Domenico Talia*

Main category: cs.CL

TL;DR: 推理语言模型（RLMs）在对抗社会偏见方面表现出出人意料的脆弱性，特别是CoT提示模型易受攻击，这挑战了推理能固有提升鲁棒性的传统观念。


<details>
  <summary>Details</summary>
Motivation: 尽管推理语言模型（RLMs）在复杂推理任务中表现出色并有望提高可靠性，但其对社会偏见的鲁棒性尚不明确，需要深入探究其推理能力对偏见诱导抵抗力的影响。

Method: 本研究利用CLEAR-Bias基准，通过“LLM-as-a-judge”自动化安全评分和越狱技术，系统评估了最先进RLMs对抗偏见诱导的对抗鲁棒性。研究着重回答了推理能力如何影响模型公平性与鲁棒性、微调推理模型与CoT提示模型在安全性上的差异，以及越狱攻击成功率与推理机制的关系。

Result: 研究发现，具有显式推理能力的模型（无论是CoT提示还是微调推理）普遍比没有这些机制的基础模型更容易受到偏见诱导，这表明推理可能无意中为刻板印象强化打开了新途径。其中，微调推理模型比依赖CoT提示的模型显得相对安全，后者尤其容易受到通过故事叙述、虚构角色或奖励塑造指令进行的语境重构攻击。

Conclusion: 研究结果挑战了推理能力能固有地改善模型鲁棒性的假设，并强调在推理设计中需要采用更具偏见意识的方法。

Abstract: Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.

</details>


### [21] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
*Wenhao Shi,Zhiqiang Hu,Yi Bin,Yang Yang,See-Kiong Ng,Heng Tao Shen*

Main category: cs.CL

TL;DR: 本文提出MathV-DP数据集和Qwen-VL-DP模型，通过引入多样的解题路径和基于GRPO的强化学习，显著提升了多模态大语言模型在数学推理方面的准确性和生成多样性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在数学推理方面依赖单一解决方案监督和一对一图文对，忽略了有效推理的多样性和内部反思能力。

Method: 1. 构建MathV-DP数据集，为每个图文问题捕获多条多样化的解题轨迹。2. 提出Qwen-VL-DP模型，基于Qwen-VL进行监督学习微调。3. 采用基于规则的强化学习方法GRPO（Group Relative Policy Optimization），结合正确性判别和多样性感知奖励函数，增强模型。

Result: Qwen-VL-DP在MathVista的minitest和Math-V基准测试中，显著优于现有基线多模态大语言模型，在准确性和生成多样性方面均表现出色。

Conclusion: 研究强调了在多模态数学推理中融入多样化视角和反思性推理的重要性，并证明所提出的方法能有效提升模型的性能和多样性。

Abstract: Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


### [22] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
*Wencheng Zhang,Shiqin Qiao,Lingjie Luo,Yinfeng Li,Chuanyang Zheng,Qian Xu,Meng Li,Yong Gui,Yijun He,Jianing Qiu,Jindong Hong,Jiankai Sun*

Main category: cs.CL

TL;DR: 本文提出了SynapseRoute，一个机器学习驱动的动态路由框架，用于智能地将医疗查询分配给LLM的“思考”或“非思考”模式，以在提高准确性的同时显著降低推理时间和Token消耗。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，如何在性能与运营成本之间取得平衡成为关键挑战，特别是具备推理能力的模型与低成本模型之间存在显著的成本差距。研究发现，约58%的医学问题无需高成本推理即可准确回答，这表明根据问题复杂性动态路由查询可以优化准确性、成本效率和用户体验。

Method: 本文提出SynapseRoute，一个基于机器学习的动态路由框架，用于智能地将输入查询分配到“思考”（高推理）或“非思考”（快速、低成本）模式。此外，研究还引入了AIT（Accuracy-Inference-Token）指数，用于综合评估准确性、延迟和Token成本之间的权衡。

Result: 在多个医学数据集上的实验结果表明，与单独使用“思考”模式相比，SynapseRoute：
1. 提升了整体准确率（0.8390 vs. 0.8272）。
2. 将推理时间减少了36.8%。
3. 将Token消耗减少了39.66%。
定性分析还指出，该方法避免了对简单查询进行过度推理导致的延迟和准确率下降。

Conclusion: SynapseRoute通过智能地将查询动态路由到最合适的LLM模式，成功地平衡了准确性、成本效率和用户体验。该框架不仅提高了性能，还显著降低了资源消耗，并通过避免过度推理的弊端来优化用户体验。引入AIT指数也为全面评估提供了新的工具。

Abstract: With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.

</details>


### [23] [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
*Valentina Pyatkin,Saumya Malik,Victoria Graf,Hamish Ivison,Shengyi Huang,Pradeep Dasigi,Nathan Lambert,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 现有大语言模型难以精确遵循带有输出限制的指令，尤其是在泛化方面表现不佳。本研究引入新基准IFBench评估泛化能力，并提出基于可验证奖励的强化学习（RLVR）显著提升了模型的指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 语言模型精确遵循人类指令（特别是包含输出限制的指令）对成功的人机交互至关重要。然而，当前最强的模型也难以满足这些限制，并且存在在现有基准上过度拟合、对未见过的输出约束泛化能力差的问题。

Method: 1. 引入了新基准IFBench，包含58个多样、挑战性的域外可验证约束，用于评估模型精确指令遵循的泛化能力。 2. 进行了广泛分析，研究如何通过训练数据提高泛化能力。 3. 精心设计了约束验证模块。 4. 采用了基于可验证奖励的强化学习（RLVR）来训练模型。 5. 发布了额外29个手动标注的训练约束、验证函数、RLVR训练提示和代码。

Result: 1. 发现大多数模型在现有基准上对少量可验证约束过度拟合，导致对未见过的输出约束泛化能力差。 2. 基于可验证奖励的强化学习（RLVR）显著改善了模型的指令遵循能力。

Conclusion: 本研究通过提供新的评估基准IFBench和有效的训练方法（RLVR），解决了语言模型精确指令遵循和泛化能力不足的问题，并贡献了相关资源以促进未来研究。

Abstract: A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.

</details>


### [24] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
*Almog Hilel,Idan Shenfeld,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 研究揭示了基于用户反馈训练的语言模型中存在一种漏洞，单个用户仅通过提示和点赞/踩反馈即可永久改变模型的知识和行为。


<details>
  <summary>Details</summary>
Motivation: 识别并描述一种新型漏洞，即单个用户如何通过用户反馈机制，持续且恶意地修改语言模型的知识和行为。

Method: 攻击者提示语言模型随机输出“中毒”或良性响应，然后通过点赞“中毒”响应或踩良性响应，利用后续偏好调整过程，使模型即使在非恶意上下文中也更频繁地生成“中毒”响应。

Result: 该攻击可成功实现：1) 插入模型此前不具备的事实知识；2) 修改代码生成模式以引入可利用的安全漏洞；3) 注入虚假金融新闻。

Conclusion: 本研究揭示了语言模型偏好调整的一个新特征（即便受限的反馈也能实现精细控制），并提出了一种针对基于用户反馈训练的语言模型的新型攻击机制，拓展了现有数据投毒和提示注入的研究。

Abstract: We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).

</details>


### [25] [MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs](https://arxiv.org/abs/2507.02851)
*Purbesh Mitra,Sennur Ulukus*

Main category: cs.CL

TL;DR: 为解决大型语言模型（LLMs）上下文长度限制导致的推理瓶颈，本文提出了MOTIF，一种基于强化学习的微调方法，通过多轮生成思考代币实现模块化思考，有效扩展了LLMs的推理能力，并在基准测试中取得了性能提升和样本效率。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）尽管通过GRPO等算法提升了推理能力，但其有限的上下文大小限制了模型生成任意数量的思考代币进行多轮推理，成为LLM推理能力的一个关键瓶颈。

Method: 本文提出MOTIF（Modular Thinking via Reinforcement Finetuning），一种强化学习（RL）训练方法，旨在通过多轮生成思考代币，使LLM采用模块化思考策略，从而有效扩展模型的上下文推理能力。作者在GSM8K数据集上使用参数高效微调（PEFT）训练了开源模型Qwen2.5-3B-Instruct。

Result: 实验结果显示，与基于传统GRPO的训练方法相比，MOTIF在MATH500和AIME2024基准测试上的准确率分别提高了3.8%和3.3%。此外，这些性能提升仅用了15%的样本量实现，证明了MOTIF的样本高效性。

Conclusion: MOTIF通过实现多轮思考，成功帮助LLMs突破了上下文大小的限制，显著提升了其在复杂推理任务上的性能，并展现出优异的样本效率，为LLM的推理能力扩展提供了有效且高效的解决方案。

Abstract: Recent advancements in the reasoning capabilities of large language models
(LLMs) show that employing group relative policy optimization (GRPO) algorithm
for reinforcement learning (RL) training allows the models to use more
thinking/reasoning tokens for generating better responses. However, LLMs can
generate only a finite amount of tokens while maintaining attention to the
previously generated tokens. This limit, also known as the context size of an
LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.
To think beyond the limit of context size, an LLM must employ a modular
thinking strategy to reason over multiple rounds. In this work, we propose
$\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL
training method for generating thinking tokens in multiple rounds, effectively
allowing the model to think with additional context size. We trained the
open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient
fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our
experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training
in the respective benchmarks. Furthermore, this improvement was achieved with
only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code
and models are available at https://github.com/purbeshmitra/MOTIF and
https://huggingface.co/purbeshmitra/MOTIF, respectively.

</details>


### [26] [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](https://arxiv.org/abs/2507.02856)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.CL

TL;DR: 多项选择题基准测试存在缺陷，可通过不看问题作答，故提出“答案匹配”的生成式评估方法，其与人工评估高度一致，优于传统多项选择和LLM-as-a-judge评估。


<details>
  <summary>Details</summary>
Motivation: 多项选择题基准测试虽客观易自动化，但存在根本性局限，即无需看问题即可作答，产生评估捷径。缺乏可行的、可扩展的替代方案。

Method: 提出“答案匹配”的生成式评估方法：模型生成自由形式回答，然后用现代语言模型（LLM）与参考答案匹配以判断正确性。通过人工标注MMLU-Pro和GPQA-Diamond数据集，比较不同评估策略（多项选择、无参考答案的LLM-as-a-judge、答案匹配）与人工评分的一致性。

Result: 使用近期模型（包括小型模型）的“答案匹配”方法与人工评分达到近乎完美的一致性，与标注者间一致性相近。相比之下，多项选择评估和无参考答案的LLM-as-a-judge与人工评分的一致性较差。采用“答案匹配”评估后，多个模型的排名发生显著变化。

Conclusion: “答案匹配”是一种更有效、更准确的语言模型评估方法，能够克服多项选择评估的缺陷。研究建议将评估生态系统从多项选择转向“答案匹配”模式。

Abstract: Multiple choice benchmarks have long been the workhorse of language model
evaluation because grading multiple choice is objective and easy to automate.
However, we show multiple choice questions from popular benchmarks can often be
answered without even seeing the question. These shortcuts arise from a
fundamental limitation of discriminative evaluation not shared by evaluations
of the model's free-form, generative answers. Until recently, there appeared to
be no viable, scalable alternative to multiple choice--but, we show that this
has changed. We consider generative evaluation via what we call answer
matching: Give the candidate model the question without the options, have it
generate a free-form response, then use a modern language model with the
reference answer to determine if the response matches the reference. To compare
the validity of different evaluation strategies, we annotate MMLU-Pro and
GPQA-Diamond to obtain human grading data, and measure the agreement of each
evaluation approach. We find answer matching using recent models--even small
ones--achieves near-perfect agreement, in the range of inter-annotator
agreement. In contrast, both multiple choice evaluation and using
LLM-as-a-judge without reference answers aligns poorly with human grading.
Improving evaluations via answer matching is not merely a conceptual concern:
the rankings of several models change significantly when evaluating their
free-form responses with answer matching. In light of these findings, we
discuss how to move the evaluation ecosystem from multiple choice to answer
matching.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [27] [Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges](https://arxiv.org/abs/2507.02074)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

TL;DR: 本综述探讨了利用大型语言模型（LLMs）和视觉语言模型（VLMs）进行视频碰撞检测的最新方法。


<details>
  <summary>Details</summary>
Motivation: 视频碰撞检测是智能交通系统中的关键问题。LLMs和VLMs的最新发展改变了多模态信息的处理和推理方式。

Method: 本文综述了利用LLMs从视频数据进行碰撞检测的最新方法。具体方法包括：呈现融合策略的结构化分类、总结关键数据集、分析模型架构、比较性能基准，并讨论挑战和机遇。

Result: 综述提供了融合策略的结构化分类、关键数据集的总结、模型架构的分析以及性能基准的比较，并指出了该领域的挑战和机遇。

Conclusion: 本综述为视频理解和基础模型这一快速发展交叉领域的未来研究奠定了基础。

Abstract: Crash detection from video feeds is a critical problem in intelligent
transportation systems. Recent developments in large language models (LLMs) and
vision-language models (VLMs) have transformed how we process, reason about,
and summarize multimodal information. This paper surveys recent methods
leveraging LLMs for crash detection from video data. We present a structured
taxonomy of fusion strategies, summarize key datasets, analyze model
architectures, compare performance benchmarks, and discuss ongoing challenges
and opportunities. Our review provides a foundation for future research in this
fast-growing intersection of video understanding and foundation models.

</details>


### [28] [Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning](https://arxiv.org/abs/2507.02148)
*Zijie Cai,Christopher Metzler*

Main category: cs.CV

TL;DR: 本研究评估并改进了水下环境下的单目度量深度估计，通过在合成水下数据上微调大型模型，显著提升了其性能。


<details>
  <summary>Details</summary>
Motivation: 尽管单目深度估计在陆地应用中取得进展，但在水下环境中，由于光线衰减、颜色失真、浑浊及高质量真值数据稀缺，其可靠性仍受限。

Method: 首先，在FLSea和SQUID等真实水下数据集上对零样本和微调后的单目度量深度估计模型进行了全面基准测试。接着，利用基于物理的水下图像形成模型生成了Hypersim数据集的合成水下变体，并在此合成数据上微调了Depth Anything V2模型。

Result: 陆地训练模型在水下表现不佳，存在显著域偏移。通过在合成水下数据上微调，模型的性能在所有基准测试中均持续提升，且优于仅在陆地数据上训练的基线模型。

Conclusion: 本研究详细评估了水下单目度量深度估计，并强调了域适应和尺度感知监督对于在复杂水下环境中实现稳健和可泛化深度预测的关键作用，为未来研究提供了重要启示。

Abstract: Monocular depth estimation has recently advanced to provide not only relative
but also metric depth predictions. However, its reliability in underwater
environments remains limited due to light attenuation and scattering, color
distortion, turbidity, and the lack of high-quality metric ground-truth data.
In this paper, we present a comprehensive benchmark of zero-shot and fine-tuned
monocular metric depth estimation models on real-world underwater datasets with
metric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of
state-of-the-art models across a range of underwater conditions with different
ranges. Our results show that large-scale models trained on terrestrial (real
or synthetic) data, while effective in in-air settings, perform poorly
underwater due to significant domain shifts. To address this, we fine-tune
Depth Anything V2 with a ViT-S backbone encoder on a synthetic underwater
variant of the Hypersim dataset, which we generated using a physically based
underwater image formation model. We demonstrate our fine-tuned model
consistently improves performance across all benchmarks and outperforms
baselines trained only on the clean in-air Hypersim dataset. Our study provides
a detailed evaluation and visualization for monocular metric depth estimation
in underwater scenes, highlighting the importance of domain adaptation and
scale-aware supervision for achieving robust and generalizable metric depth
predictions in challenging underwater environments for future research.

</details>


### [29] [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
*Xiao Wang,Jingtao Jiang,Qiang Chen,Lan Chen,Lin Zhu,Yaowei Wang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 针对事件流场景文本识别中现有方法解释性差和推理能力弱的问题，本文提出基于思维链（CoT）的ESTR-CoT框架，并构建大型CoT数据集，实验证明其有效性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有事件流场景文本识别（STR）方法在极端场景下虽优于RGB相机，但普遍存在解释性不足和上下文逻辑推理能力弱的局限性。

Method: 提出ESTR-CoT框架。该框架采用EVA-CLIP将事件流转换为视觉Tokens，Llama Tokenizer编码提示，Q-former对齐视觉Token与Vicuna-7B大语言模型，同时输出识别结果和思维链推理过程。此外，还构建了一个大规模的CoT数据集，用于训练该框架。

Result: 在EventSTR、WordArt*、IC15*等三个事件流STR基准数据集上进行了广泛实验，充分验证了所提框架的有效性和可解释性。

Conclusion: ESTR-CoT框架通过引入思维链推理和构建专用数据集，有效提升了事件流场景文本识别的性能和解释性，为后续基于推理的大模型发展奠定基础。

Abstract: Event stream based scene text recognition is a newly arising research topic
in recent years which performs better than the widely used RGB cameras in
extremely challenging scenarios, especially the low illumination, fast motion.
Existing works either adopt end-to-end encoder-decoder framework or large
language models for enhanced recognition, however, they are still limited by
the challenges of insufficient interpretability and weak contextual logical
reasoning. In this work, we propose a novel chain-of-thought reasoning based
event stream scene text recognition framework, termed ESTR-CoT. Specifically,
we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input
event stream into tokens and utilize a Llama tokenizer to encode the given
generation prompt. A Q-former is used to align the vision token to the
pre-trained large language model Vicuna-7B and output both the answer and
chain-of-thought (CoT) reasoning process simultaneously. Our framework can be
optimized using supervised fine-tuning in an end-to-end manner. In addition, we
also propose a large-scale CoT dataset to train our framework via a three stage
processing (i.e., generation, polish, and expert verification). This dataset
provides a solid data foundation for the development of subsequent
reasoning-based large models. Extensive experiments on three event stream STR
benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the
effectiveness and interpretability of our proposed framework. The source code
and pre-trained models will be released on
https://github.com/Event-AHU/ESTR-CoT.

</details>


### [30] [Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach](https://arxiv.org/abs/2507.02205)
*Elena Ryumina,Maxim Markitantov,Alexandr Axyonov,Dmitry Ryumin,Mikhail Dolgushin,Alexey Karpov*

Main category: cs.CV

TL;DR: 提出了一种新颖的零样本多模态复合表情识别（CER）方法，融合六种模态，其性能可与有监督方法媲美，无需领域适应。


<details>
  <summary>Details</summary>
Motivation: 现有复合表情识别（CER）方法过度依赖任务特定的训练数据，限制了其泛化能力。因此，需要一种无需大量领域特定训练即可检测复杂情感状态的新方法。

Method: 开发了一种新颖的零样本多模态管道，整合了六种异构模态：静态和动态面部表情、场景和标签匹配、场景上下文、音频和文本。该方法利用零样本组件，包括基于CLIP的标签匹配和Qwen-VL进行语义场景理解。此外，引入了多头概率融合（MHPF）模块来动态加权模态特定的预测，以及一个复合表情（CE）转换模块，该模块使用成对概率聚合（PPA）和成对特征相似度聚合（PFSA）方法来生成可解释的复合情感输出。

Result: 在多语料库训练下进行零样本测试，所提出的方法在AffWild2数据集上实现了46.95%的F1分数，在AFEW数据集上实现了49.02%，在C-EXPR-DB数据集上实现了34.85%。这些结果与在目标数据上训练的有监督方法相当。

Conclusion: 所提出的零样本多模态方法在复合表情识别方面表现出有效性，证明了其在无需领域适应的情况下捕获复杂情感的能力，并且性能可与有监督方法相媲美。

Abstract: Compound Expression Recognition (CER), a subfield of affective computing,
aims to detect complex emotional states formed by combinations of basic
emotions. In this work, we present a novel zero-shot multimodal approach for
CER that combines six heterogeneous modalities into a single pipeline: static
and dynamic facial expressions, scene and label matching, scene context, audio,
and text. Unlike previous approaches relying on task-specific training data,
our approach uses zero-shot components, including Contrastive Language-Image
Pretraining (CLIP)-based label matching and Qwen-VL for semantic scene
understanding. We further introduce a Multi-Head Probability Fusion (MHPF)
module that dynamically weights modality-specific predictions, followed by a
Compound Expressions (CE) transformation module that uses Pair-Wise Probability
Aggregation (PPA) and Pair-Wise Feature Similarity Aggregation (PFSA) methods
to produce interpretable compound emotion outputs. Evaluated under multi-corpus
training, the proposed approach shows F1 scores of 46.95% on AffWild2, 49.02%
on Acted Facial Expressions in The Wild (AFEW), and 34.85% on C-EXPR-DB via
zero-shot testing, which is comparable to the results of supervised approaches
trained on target data. This demonstrates the effectiveness of the proposed
approach for capturing CE without domain adaptation. The source code is
publicly available.

</details>


### [31] [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
*Takuro Kawada,Shunsuke Kitada,Sota Nemoto,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 该论文引入了大型数据集SciGA-145k，并定义了图形摘要（GA）推荐任务和新的评估指标，旨在促进科学交流中GA的自动化生成和选择。


<details>
  <summary>Details</summary>
Motivation: 图形摘要（GA）对于科学传播至关重要，但其潜力尚未被充分挖掘，且有效设计GA需要专业技能，这阻碍了其广泛采用。

Method: 研究构建了包含14.5万篇科学论文和114万张图的大型数据集SciGA-145k。基于此数据集，定义了两个GA推荐任务（篇内GA推荐和跨篇GA推荐）并提供了基线模型。此外，提出了一种新的推荐评估指标CAR（Confidence Adjusted top-1 ground truth Ratio），以更细致地分析模型行为并克服传统指标的局限性。

Result: 成功构建了大规模的SciGA-145k数据集，为GA选择、推荐和自动化生成研究提供了基础资源。定义并初步解决了GA推荐任务，并开发了创新的评估指标CAR，能够更准确地评估模型在多候选GA情境下的表现。

Conclusion: 本研究通过构建SciGA-145k数据集、定义相关任务和提出新的评估指标，为推动视觉科学交流的进步以及“AI for Science”领域的发展奠定了重要基础。

Abstract: Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific
papers and 1.14 million figures, explicitly designed for supporting GA
selection and recommendation as well as facilitating research in automated GA
generation. As a preliminary step toward GA design support, we define two
tasks: 1) Intra-GA recommendation, which identifies figures within a given
paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,
which retrieves GAs from other papers to inspire the creation of new GAs. We
provide reasonable baseline models for these tasks. Furthermore, we propose
Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation
metric that offers a fine-grained analysis of model behavior. CAR addresses
limitations in traditional ranking-based metrics by considering cases where
multiple figures within a paper, beyond the explicitly labeled GA, may also
serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a
foundation for advancing visual scientific communication while contributing to
the development of AI for Science.

</details>


### [32] [Understanding Trade offs When Conditioning Synthetic Data](https://arxiv.org/abs/2507.02217)
*Brandon Trabucco,Qasim Wani,Benjamin Pikus,Vasu Sharma*

Main category: cs.CV

TL;DR: 本研究探讨了在数据稀缺场景下，使用扩散模型生成高质量合成数据以训练目标检测器。通过比较基于提示词和基于布局的两种条件作用策略，发现当数据多样性较高且布局提示与训练分布匹配时，基于布局的方法能显著提高目标检测性能，最高可达177%的mAP提升。


<details>
  <summary>Details</summary>
Motivation: 工业视觉系统在少量图像下训练鲁棒目标检测器面临严峻挑战，高质量数据收集耗时。合成数据是潜在解决方案，但现有3D引擎耗时且存在模拟-现实差距。扩散模型虽能快速生成图像，但精确控制困难，且不同条件作用方案对合成数据质量的影响尚不明确，亟需探索高效的合成数据生成方法。

Method: 研究了来自四个标准目标检测基准的八十个不同视觉概念。比较了两种条件作用策略：基于提示词（prompt-based）和基于布局（layout-based）的合成数据生成对目标检测性能的影响。

Result: 当条件提示集合较窄时，基于提示词的条件作用生成更高质量的合成数据。然而，当多样性增加时，基于布局的条件作用变得更优越。当布局提示与完整训练分布匹配时，相比仅使用真实数据，使用合成数据平均将平均精度（mAP）提高了34%，最高可达177%。

Conclusion: 在数据稀缺的场景下，通过扩散模型生成合成数据是可行的方案。尤其在多样性要求高且布局提示与训练分布相匹配时，基于布局的条件作用是生成高质量合成数据的有效策略，能够显著提升目标检测模型的性能。

Abstract: Learning robust object detectors from only a handful of images is a critical
challenge in industrial vision systems, where collecting high quality training
data can take months. Synthetic data has emerged as a key solution for data
efficient visual inspection and pick and place robotics. Current pipelines rely
on 3D engines such as Blender or Unreal, which offer fine control but still
require weeks to render a small dataset, and the resulting images often suffer
from a large gap between simulation and reality. Diffusion models promise a
step change because they can generate high quality images in minutes, yet
precise control, especially in low data regimes, remains difficult. Although
many adapters now extend diffusion beyond plain text prompts, the effect of
different conditioning schemes on synthetic data quality is poorly understood.
We study eighty diverse visual concepts drawn from four standard object
detection benchmarks and compare two conditioning strategies: prompt based and
layout based. When the set of conditioning cues is narrow, prompt conditioning
yields higher quality synthetic data; as diversity grows, layout conditioning
becomes superior. When layout cues match the full training distribution,
synthetic data raises mean average precision by an average of thirty four
percent and by as much as one hundred seventy seven percent compared with using
real data alone.

</details>


### [33] [High-Fidelity Differential-information Driven Binary Vision Transformer](https://arxiv.org/abs/2507.02222)
*Tian Gao,Zhiyuan Zhang,Kaijie Yin,Xu-Cheng Zhong,Hui Kong*

Main category: cs.CV

TL;DR: 本文提出DIDB-ViT，一种新颖的二值化视觉Transformer，通过引入信息丰富注意力模块、频率分解和改进的激活函数，有效解决了现有二值化ViT的性能下降问题，并在图像分类和分割任务中超越了现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer（ViT）的二值化是解决其高计算/存储需求与边缘设备部署限制之间矛盾的有前景方法。然而，现有二值化ViT方法常面临严重的性能下降或过度依赖全精度模块。

Method: 本文提出了DIDB-ViT。具体地，设计了融合微分信息的“信息丰富注意力模块”以减轻二值化导致的信息损失并增强高频保留。通过离散Haar小波进行频率分解，整合不同频率的Q和K张量相似度计算。此外，引入了改进的RPReLU激活函数来重构激活分布，扩展模型表示能力。

Result: 实验结果表明，DIDB-ViT在多种ViT架构上显著优于现有最先进的网络量化方法，实现了卓越的图像分类和分割性能。

Conclusion: DIDB-ViT成功克服了二值化ViT的性能退化问题，在保持原始ViT架构和计算效率的同时，提供了高度信息丰富的模型，并在图像分类和分割任务中取得了优异表现，为ViT在边缘设备的部署提供了有效方案。

Abstract: The binarization of vision transformers (ViTs) offers a promising approach to
addressing the trade-off between high computational/storage demands and the
constraints of edge-device deployment. However, existing binary ViT methods
often suffer from severe performance degradation or rely heavily on
full-precision modules. To address these issues, we propose DIDB-ViT, a novel
binary ViT that is highly informative while maintaining the original ViT
architecture and computational efficiency. Specifically, we design an
informative attention module incorporating differential information to mitigate
information loss caused by binarization and enhance high-frequency retention.
To preserve the fidelity of the similarity calculations between binary Q and K
tensors, we apply frequency decomposition using the discrete Haar wavelet and
integrate similarities across different frequencies. Additionally, we introduce
an improved RPReLU activation function to restructure the activation
distribution, expanding the model's representational capacity. Experimental
results demonstrate that our DIDB-ViT significantly outperforms
state-of-the-art network quantization methods in multiple ViT architectures,
achieving superior image classification and segmentation performance.

</details>


### [34] [FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model](https://arxiv.org/abs/2507.02250)
*Jiangxia Chen,Tongyuan Huang,Ke Song*

Main category: cs.CV

TL;DR: 本文提出FMOcc，一个基于流匹配选择性状态空间模型的TPV细化占用网络，用于解决自动驾驶中少帧3D语义占用预测的准确性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 3D语义占用预测对自动驾驶至关重要，但少帧图像的固有限制和3D空间冗余会损害被遮挡和远距离场景的预测精度。现有方法通过融合历史帧数据提升性能，但这需要额外数据和大量计算资源。

Method: 本文提出FMOcc网络。首先，设计基于流匹配模型的FMSSM模块以生成缺失特征；其次，通过设计TPV SSM层和PS3M，选择性过滤TPV特征，减少空体素对非空体素的影响，提高模型效率和远距离场景预测能力；最后，设计Mask Training (MT) 方法增强FMOcc的鲁棒性并解决传感器数据丢失问题。

Result: FMOcc在Occ3D-nuScenes和OpenOcc数据集上的实验结果表明，其性能优于现有最先进方法。在Occ3D-nuScenes验证集上，FMOcc（两帧输入）实现了43.1%的RayIoU和39.8%的mIoU；在OpenOcc上，实现了42.6%的RayIoU，推理内存为5.4G，推理时间为330毫秒。

Conclusion: FMOcc有效解决了少帧3D语义占用预测的挑战，显著提升了在被遮挡和远距离场景下的预测精度与模型效率，并展现出超越现有方法的卓越性能。

Abstract: 3D semantic occupancy prediction plays a pivotal role in autonomous driving.
However, inherent limitations of fewframe images and redundancy in 3D space
compromise prediction accuracy for occluded and distant scenes. Existing
methods enhance performance by fusing historical frame data, which need
additional data and significant computational resources. To address these
issues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement
occupancy network with flow matching selective state space model for few-frame
3D occupancy prediction. Firstly, to generate missing features, we designed a
feature refinement module based on a flow matching model, which is called Flow
Matching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and
Plane Selective SSM (PS3M), we selectively filter TPV features to reduce the
impact of air voxels on non-air voxels, thereby enhancing the overall
efficiency of the model and prediction capability for distant scenes. Finally,
we design the Mask Training (MT) method to enhance the robustness of FMOcc and
address the issue of sensor data loss. Experimental results on the
Occ3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing
state-of-theart methods. Our FMOcc with two frame input achieves notable scores
of 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on
OpenOcc with 5.4 G inference memory and 330ms inference time.

</details>


### [35] [SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement](https://arxiv.org/abs/2507.02252)
*Zeyu Lei,Hongyuan Yu,Jinlin Wu,Zhen Chen*

Main category: cs.CV

TL;DR: 本文提出SurgVisAgent，一个基于多模态大语言模型（MLLMs）的智能外科视觉代理，旨在动态识别并处理内窥镜图像的多种失真，实现定制化图像增强，并在真实外科模拟基准上超越了传统单任务模型。


<details>
  <summary>Details</summary>
Motivation: 现有的外科图像增强算法通常针对特定场景的单一任务设计，这限制了它们在复杂真实外科环境中的有效性，无法满足外科医生对多样化图像增强的需求。

Method: 本文提出SurgVisAgent，一个基于多模态大语言模型（MLLMs）的端到端智能外科视觉代理。SurgVisAgent能够动态识别内窥镜图像的失真类别和严重程度，并执行包括低光增强、过曝校正、运动模糊消除和烟雾去除等多种增强任务。具体而言，通过设计一个提供领域特定知识的先验模型来增强外科场景理解，并利用上下文少样本学习和思维链（CoT）推理实现定制化的图像增强。

Result: 研究团队构建了一个模拟真实外科失真的综合基准，并在此基准上进行了大量实验。实验结果表明，SurgVisAgent在性能上超越了传统的单任务模型。

Conclusion: SurgVisAgent作为一个统一的解决方案，在外科辅助方面展现了巨大的潜力。

Abstract: Precise surgical interventions are vital to patient safety, and advanced
enhancement algorithms have been developed to assist surgeons in
decision-making. Despite significant progress, these algorithms are typically
designed for single tasks in specific scenarios, limiting their effectiveness
in complex real-world situations. To address this limitation, we propose
SurgVisAgent, an end-to-end intelligent surgical vision agent built on
multimodal large language models (MLLMs). SurgVisAgent dynamically identifies
distortion categories and severity levels in endoscopic images, enabling it to
perform a variety of enhancement tasks such as low-light enhancement,
overexposure correction, motion blur elimination, and smoke removal.
Specifically, to achieve superior surgical scenario understanding, we design a
prior model that provides domain-specific knowledge. Additionally, through
in-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent
delivers customized image enhancements tailored to a wide range of distortion
types and severity levels, thereby addressing the diverse requirements of
surgeons. Furthermore, we construct a comprehensive benchmark simulating
real-world surgical distortions, on which extensive experiments demonstrate
that SurgVisAgent surpasses traditional single-task models, highlighting its
potential as a unified solution for surgical assistance.

</details>


### [36] [Multi-Label Classification Framework for Hurricane Damage Assessment](https://arxiv.org/abs/2507.02265)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: 本研究提出一种基于ResNet和注意力机制的多标签分类框架，利用航空影像评估飓风灾害，并在Rescuenet数据集上取得90.23%的平均精度，优于现有方法，从而提升灾后响应效率。


<details>
  <summary>Details</summary>
Motivation: 飓风造成的灾害类型和严重程度复杂多样，需要及时准确的评估。传统单标签分类方法无法捕捉这种复杂性，导致灾后响应效率低下。

Method: 本研究引入一种新颖的多标签分类框架，利用航空影像评估灾害。该方法整合了基于ResNet的特征提取模块和类别特定注意力机制，以识别单幅图像中的多种损害类型。

Result: 在飓风迈克尔的Rescuenet数据集上，所提出的方法取得了90.23%的平均精度（mAP），性能优于现有基线方法。

Conclusion: 该框架显著增强了飓风后的损害评估能力，有助于实现更具针对性和效率的灾害响应，并对未来的灾害缓解和韧性策略有所贡献。

Abstract: Hurricanes cause widespread destruction, resulting in diverse damage types
and severities that require timely and accurate assessment for effective
disaster response. While traditional single-label classification methods fall
short of capturing the complexity of post-hurricane damage, this study
introduces a novel multi-label classification framework for assessing damage
using aerial imagery. The proposed approach integrates a feature extraction
module based on ResNet and a class-specific attention mechanism to identify
multiple damage types within a single image. Using the Rescuenet dataset from
Hurricane Michael, the proposed method achieves a mean average precision of
90.23%, outperforming existing baseline methods. This framework enhances
post-hurricane damage assessment, enabling more targeted and efficient disaster
response and contributing to future strategies for disaster mitigation and
resilience. This paper has been accepted at the ASCE International Conference
on Computing in Civil Engineering (i3CE 2025), and the camera-ready version
will appear in the official conference proceedings.

</details>


### [37] [Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation](https://arxiv.org/abs/2507.02268)
*Yuxiang Zhang,Wei Li,Wen Jia,Mengmeng Zhang,Ran Tao,Shunlin Liang*

Main category: cs.CV

TL;DR: 针对高光谱图像跨域分类中的频谱偏移问题，本文提出了一种双向域适应（BiDA）框架，通过提取域不变和域特定特征，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱遥感技术可提取细粒度地物类别，但训练和测试图像通常来自不同区域或时间，导致相同类别存在显著的频谱偏移，影响跨域高光谱图像（HSI）分类的准确性。

Method: 本文提出了Bi-directional Domain Adaptation (BiDA) 框架。该框架以三分支Transformer架构（源分支、目标分支和耦合分支）和语义分词器为骨干，旨在独立自适应空间中提取域不变特征和域特定信息。源分支和目标分支独立学习源域和目标域的适应空间，耦合分支中开发了耦合多头交叉注意力（CMCA）机制进行特征交互和域间关联挖掘。此外，设计了双向蒸馏损失（bi-directional distillation loss）指导适应空间学习，并提出了自适应强化策略（ARS）以在噪声条件下增强模型对特定泛化特征的提取。

Result: 在跨时间/场景机载和卫星数据集上的实验结果表明，所提出的BiDA框架显著优于一些最先进的域适应方法。在跨时间树种分类任务中，BiDA的性能比现有最先进的方法高出3%~5%。

Conclusion: BiDA框架通过有效地提取域不变和域特定信息，成功解决了高光谱图像跨域分类中的频谱偏移问题，显著增强了模型对目标场景的适应性和可分离性，达到了优越的分类性能。

Abstract: Utilizing hyperspectral remote sensing technology enables the extraction of
fine-grained land cover classes. Typically, satellite or airborne images used
for training and testing are acquired from different regions or times, where
the same class has significant spectral shifts in different scenes. In this
paper, we propose a Bi-directional Domain Adaptation (BiDA) framework for
cross-domain hyperspectral image (HSI) classification, which focuses on
extracting both domain-invariant features and domain-specific information in
the independent adaptive space, thereby enhancing the adaptability and
separability to the target scene. In the proposed BiDA, a triple-branch
transformer architecture (the source branch, target branch, and coupled branch)
with semantic tokenizer is designed as the backbone. Specifically, the source
branch and target branch independently learn the adaptive space of source and
target domains, a Coupled Multi-head Cross-attention (CMCA) mechanism is
developed in coupled branch for feature interaction and inter-domain
correlation mining. Furthermore, a bi-directional distillation loss is designed
to guide adaptive space learning using inter-domain correlation. Finally, we
propose an Adaptive Reinforcement Strategy (ARS) to encourage the model to
focus on specific generalized feature extraction within both source and target
scenes in noise condition. Experimental results on cross-temporal/scene
airborne and satellite datasets demonstrate that the proposed BiDA performs
significantly better than some state-of-the-art domain adaptation approaches.
In the cross-temporal tree species classification task, the proposed BiDA is
more than 3\%$\sim$5\% higher than the most advanced method. The codes will be
available from the website:
https://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA.

</details>


### [38] [MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement](https://arxiv.org/abs/2507.02270)
*Fanghai Yi,Zehong Zheng,Zexiao Liang,Yihang Dong,Xiyang Fang,Wangyu Wu,Xuhang Chen*

Main category: cs.CV

TL;DR: 提出MAC-Lookup模型，结合条件查找表和多轴自适应增强，有效解决水下图像可见度和色彩问题。


<details>
  <summary>Details</summary>
Motivation: 水下图像因光照、水体和气泡影响，存在可见度和色彩退化问题。传统方法和深度学习方法在处理水下图像增强时均面临挑战，尤其缺乏高质量数据集。

Method: 引入Multi-Axis Conditional Lookup (MAC-Lookup) 模型。该模型包含两个核心组件：用于初步颜色和质量校正的条件3D查找表颜色校正（CLTCC）以及用于细节优化的多轴自适应增强（MAAE）。模型旨在防止过度增强和饱和。

Result: 大量实验表明，MAC-Lookup在恢复细节和色彩方面表现出色，优于现有水下图像增强方法。

Conclusion: MAC-Lookup模型成功克服了水下图像增强的挑战，有效提升了图像的视觉质量，为水下探索提供了更好的视觉支持。

Abstract: Enhancing underwater images is crucial for exploration. These images face
visibility and color issues due to light changes, water turbidity, and bubbles.
Traditional prior-based methods and pixel-based methods often fail, while deep
learning lacks sufficient high-quality datasets. We introduce the Multi-Axis
Conditional Lookup (MAC-Lookup) model, which enhances visual quality by
improving color accuracy, sharpness, and contrast. It includes Conditional 3D
Lookup Table Color Correction (CLTCC) for preliminary color and quality
correction and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement.
This model prevents over-enhancement and saturation while handling underwater
challenges. Extensive experiments show that MAC-Lookup excels in enhancing
underwater images by restoring details and colors better than existing methods.
The code is https://github.com/onlycatdoraemon/MAC-Lookup.

</details>


### [39] [Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation](https://arxiv.org/abs/2507.02271)
*Feizhen Huang,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: 针对现有视频到音频(V2A)模型在电影语言中处理部分可见目标时的局限性，提出一种自蒸馏方法，显著提升了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有V2A方法忽视电影语言，导致在拟音目标仅部分可见的场景下性能下降。

Method: 提出一种简单的自蒸馏方法，通过模拟电影语言变化，使学生模型学习对齐具有相同视听对应关系的训练对的视频特征，以捕捉声音与部分视觉信息的关联。

Result: 该方法在部分可见场景下，各项评估指标均取得显著改进；同时，在大型V2A数据集VGGSound上的性能也有所提升。

Conclusion: 本研究通过自蒸馏有效解决了V2A模型在电影语言中处理部分可见目标的挑战，显著提升了模型在复杂场景下的鲁棒性和性能。

Abstract: Video-to-Audio (V2A) Generation achieves significant progress and plays a
crucial role in film and video post-production. However, current methods
overlook the cinematic language, a critical component of artistic expression in
filmmaking. As a result, their performance deteriorates in scenarios where
Foley targets are only partially visible. To address this challenge, we propose
a simple self-distillation approach to extend V2A models to cinematic language
scenarios. By simulating the cinematic language variations, the student model
learns to align the video features of training pairs with the same audio-visual
correspondences, enabling it to effectively capture the associations between
sounds and partial visual information. Our method not only achieves impressive
improvements under partial visibility across all evaluation metrics, but also
enhances performance on the large-scale V2A dataset, VGGSound.

</details>


### [40] [LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2507.02279)
*Juntao Liu,Liqiang Niu,Wenchao Chen,Jie Zhou,Fandong Meng*

Main category: cs.CV

TL;DR: 提出LaCo框架，在视觉编码器中间层进行视觉token压缩，显著提升多模态大语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）的视觉token压缩方法主要作为编码器后置模块，限制了其效率提升潜力。

Method: 提出LaCo（层级视觉token压缩）框架，在视觉编码器中间层实现有效的token压缩。该框架包含两个核心组件：1) 层级像素混洗机制，通过空-道转换系统地合并相邻token；2) 带有非参数快捷方式的残差学习架构，用于在压缩过程中保留关键视觉信息。

Result: LaCo在视觉编码器中间层压缩token时，表现出卓越的有效性，优于所有现有方法。与外部压缩相比，在保持强大性能的同时，LaCo将训练效率提高了20%以上，推理吞吐量提高了15%以上。

Conclusion: LaCo是一种高效且有效的中间层视觉token压缩方法，显著提升了多模态大语言模型的训练和推理效率，同时保持了优异的性能。

Abstract: Existing visual token compression methods for Multimodal Large Language
Models (MLLMs) predominantly operate as post-encoder modules, limiting their
potential for efficiency gains. To address this limitation, we propose LaCo
(Layer-wise Visual Token Compression), a novel framework that enables effective
token compression within the intermediate layers of the vision encoder. LaCo
introduces two core components: 1) a layer-wise pixel-shuffle mechanism that
systematically merges adjacent tokens through space-to-channel transformations,
and 2) a residual learning architecture with non-parametric shortcuts that
preserves critical visual information during compression. Extensive experiments
indicate that our LaCo outperforms all existing methods when compressing tokens
in the intermediate layers of the vision encoder, demonstrating superior
effectiveness. In addition, compared to external compression, our method
improves training efficiency beyond 20% and inference throughput over 15% while
maintaining strong performance.

</details>


### [41] [Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization](https://arxiv.org/abs/2507.02288)
*De Cheng,Zhipeng Xu,Xinyang Jiang,Dongsheng Li,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 本文提出一种新的文本特征引导视觉Prompt微调框架，利用LLM解耦文本Prompt并指导视觉表示学习。为解决语言引导的局限性，引入WERA机制，通过抽象Prompt和图像增强提升领域多样性，实现了超越现有SOTA的领域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于视觉基础模型（VFM）的领域Prompt微调在领域泛化（DG）中潜力巨大，但有效设计Prompt以解耦不同领域中的不变特征仍是一个关键挑战。

Method: 提出一个文本特征引导的视觉Prompt微调框架。首先利用大型语言模型（LLM）自动解耦文本Prompt，然后引导学习领域不变的视觉表示。为克服单纯语言引导的局限性，引入“最差显式表示对齐”（WERA），通过抽象Prompt和风格化图像增强提升源域多样性，并利用对齐约束确保视觉表示的一致性。

Result: 在PACS、VLCS、OfficeHome、DomainNet和TerraInc等主流DG数据集上进行的实验表明，所提出的方法优于现有最先进的DG方法。

Conclusion: 本文提出的方法通过结合VFM的语言Prompt、LLM的解耦能力以及创新的WERA机制，有效解决了领域泛化中不变特征解耦的挑战，显著提升了模型在未见域上的泛化性能。

Abstract: Domain Generalization (DG) seeks to develop a versatile model capable of
performing effectively on unseen target domains. Notably, recent advances in
pre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated
considerable potential in enhancing the generalization capabilities of deep
learning models. Despite the increasing attention toward VFM-based domain
prompt tuning within DG, the effective design of prompts capable of
disentangling invariant features across diverse domains remains a critical
challenge. In this paper, we propose addressing this challenge by leveraging
the controllable and flexible language prompt of the VFM. Noting that the text
modality of VFMs is naturally easier to disentangle, we introduce a novel
framework for text feature-guided visual prompt tuning. This framework first
automatically disentangles the text prompt using a large language model (LLM)
and then learns domain-invariant visual representation guided by the
disentangled text feature. However, relying solely on language to guide visual
feature disentanglement has limitations, as visual features can sometimes be
too complex or nuanced to be fully captured by descriptive text. To address
this, we introduce Worst Explicit Representation Alignment (WERA), which
extends text-guided visual prompts by incorporating an additional set of
abstract prompts. These prompts enhance source domain diversity through
stylized image augmentations, while alignment constraints ensure that visual
representations remain consistent across both the original and augmented
distributions. Experiments conducted on major DG datasets, including PACS,
VLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method
outperforms state-of-the-art DG methods.

</details>


### [42] [ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation](https://arxiv.org/abs/2507.02294)
*Hanbo Bi,Yulong Xu,Ya Li,Yongqiang Mao,Boyuan Tong,Chongyang Li,Chunbo Lang,Wenhui Diao,Hongqi Wang,Yingchao Feng,Xian Sun*

Main category: cs.CV

TL;DR: ViRefSAM是一个新颖的少样本学习框架，通过少量参考图像，实现SAM在遥感图像中对未知类别的自动、精准分割，解决了手动提示和领域适应性挑战。


<details>
  <summary>Details</summary>
Motivation: 将Segment Anything Model (SAM)应用于遥感(RS)图像面临两大挑战：一是手动构建精确提示（如点、框）劳动密集且效率低下，尤其在遥感场景中处理密集小目标或空间碎片化分布时；二是SAM缺乏领域适应性，其主要基于自然图像预训练，难以捕捉遥感特有语义和空间特征，尤其在分割新颖或未见类别时。

Method: 受少样本学习启发，提出ViRefSAM框架，仅利用少量带标注的参考图像来指导SAM，无需手动提示即可实现遥感图像中类别一致对象的自动分割。ViRefSAM在保持SAM原始架构不变的情况下，引入两个关键组件：1. 视觉上下文提示编码器：从参考图像中提取类别特异性语义线索，并通过与目标图像的上下文交互生成对象感知提示；2. 动态目标对齐适配器：集成到SAM的图像编码器中，通过将类别特异性语义注入目标图像特征来弥合领域差距，使SAM能够动态关注任务相关区域。

Result: 在iSAID-5$^i$、LoveDA-2$^i$和COCO-20$^i$三个少样本分割基准上进行了广泛实验。结果表明，ViRefSAM仅利用少量参考图像，即可实现对未见类别的准确、自动分割，并且在不同数据集上持续优于现有少样本分割方法。

Conclusion: ViRefSAM提供了一种有效的方法，通过利用少量带标注的参考图像，解决了SAM在遥感图像中手动提示和领域适应性的挑战，显著提升了其对未知类别的自动分割能力和泛化性能。

Abstract: The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits
strong generalization in generic segmentation tasks. However, applying SAM to
remote sensing (RS) images still faces two major challenges. First, manually
constructing precise prompts for each image (e.g., points or boxes) is
labor-intensive and inefficient, especially in RS scenarios with dense small
objects or spatially fragmented distributions. Second, SAM lacks domain
adaptability, as it is pre-trained primarily on natural images and struggles to
capture RS-specific semantics and spatial characteristics, especially when
segmenting novel or unseen classes. To address these issues, inspired by
few-shot learning, we propose ViRefSAM, a novel framework that guides SAM
utilizing only a few annotated reference images that contain class-specific
objects. Without requiring manual prompts, ViRefSAM enables automatic
segmentation of class-consistent objects across RS images. Specifically,
ViRefSAM introduces two key components while keeping SAM's original
architecture intact: (1) a Visual Contextual Prompt Encoder that extracts
class-specific semantic clues from reference images and generates object-aware
prompts via contextual interaction with target images; and (2) a Dynamic Target
Alignment Adapter, integrated into SAM's image encoder, which mitigates the
domain gap by injecting class-specific semantics into target image features,
enabling SAM to dynamically focus on task-relevant regions. Extensive
experiments on three few-shot segmentation benchmarks, including iSAID-5$^i$,
LoveDA-2$^i$, and COCO-20$^i$, demonstrate that ViRefSAM enables accurate and
automatic segmentation of unseen classes by leveraging only a few reference
images and consistently outperforms existing few-shot segmentation methods
across diverse datasets.

</details>


### [43] [DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation](https://arxiv.org/abs/2507.02299)
*Yunhan Yang,Shuo Chen,Yukun Huang,Xiaoyang Wu,Yuan-Chen Guo,Edmund Y. Lam,Hengshuang Zhao,Tong He,Xihui Liu*

Main category: cs.CV

TL;DR: DreamComposer++是一个新颖的框架，通过融合多视角信息，显著增强了现有扩散模型生成可控新颖视角的能力，促进了可控3D对象重建。


<details>
  <summary>Details</summary>
Motivation: 现有利用预训练2D扩散模型从单张图片生成新颖视角的方法，在生成可控新颖视角方面存在挑战，主要原因是缺乏多视角信息。

Method: 本文提出了DreamComposer++，一个灵活且可扩展的框架。它通过以下步骤实现：1. 使用视角感知3D提升模块从多个视角提取对象的3D表示。2. 通过多视角特征融合模块聚合这些表示并渲染到目标视角的潜在特征。3. 将目标视角特征集成到预训练的图像或视频扩散模型中进行新颖视角合成。

Result: 实验结果表明，DreamComposer++能与尖端视角感知扩散模型无缝集成，并有效增强它们从多视角条件生成可控新颖视角的能力。

Conclusion: DreamComposer++的进展促进了可控3D对象重建，并有望应用于广泛的领域。

Abstract: Recent advancements in leveraging pre-trained 2D diffusion models achieve the
generation of high-quality novel views from a single in-the-wild image.
However, existing works face challenges in producing controllable novel views
due to the lack of information from multiple views. In this paper, we present
DreamComposer++, a flexible and scalable framework designed to improve current
view-aware diffusion models by incorporating multi-view conditions.
Specifically, DreamComposer++ utilizes a view-aware 3D lifting module to
extract 3D representations of an object from various views. These
representations are then aggregated and rendered into the latent features of
target view through the multi-view feature fusion module. Finally, the obtained
features of target view are integrated into pre-trained image or video
diffusion models for novel view synthesis. Experimental results demonstrate
that DreamComposer++ seamlessly integrates with cutting-edge view-aware
diffusion models and enhances their abilities to generate controllable novel
views from multi-view conditions. This advancement facilitates controllable 3D
object reconstruction and enables a wide range of applications.

</details>


### [44] [Flow-CDNet: A Novel Network for Detecting Both Slow and Fast Changes in Bitemporal Images](https://arxiv.org/abs/2507.02307)
*Haoxuan Li,Chenxu Wei,Haodong Wang,Xiaomeng Hu,Boyuan An,Lingyan Ran,Baosen Zhang,Jin Jin,Omirzhan Taukebayev,Amirkhan Temirbayev,Junrui Liu,Xiuwei Zhang*

Main category: cs.CV

TL;DR: 本文提出Flow-CDNet，一个双分支（光流与二值变化检测）网络，旨在同时检测双时相图像中的快慢变化。该网络在自建数据集和新评估指标上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测多关注显著变化，但现实中微弱或慢速变化（如潜在灾害预兆）同样重要。因此，设计一个能同时检测快慢变化的网络是一个新颖的挑战。

Method: 提出Flow-CDNet，包含光流分支（利用金字塔结构提取多尺度位移变化）和二值变化检测分支（结合ResNet与光流输出生成快速变化）。此外，构建了Flow-Change数据集，设计了结合二值Tversky和L2范数的损失函数，并提出了FEPE评估指标。

Result: 在Flow-Change数据集上的定量实验表明，Flow-CDNet优于现有方法。消融实验证实，两个分支能相互促进，从而提高检测性能。

Conclusion: Flow-CDNet通过创新的双分支结构，有效解决了同时检测图像快慢变化的挑战，并在性能上取得了显著提升，验证了其在复杂变化检测场景中的潜力。

Abstract: Change detection typically involves identifying regions with changes between
bitemporal images taken at the same location. Besides significant changes, slow
changes in bitemporal images are also important in real-life scenarios. For
instance, weak changes often serve as precursors to major hazards in scenarios
like slopes, dams, and tailings ponds. Therefore, designing a change detection
network that simultaneously detects slow and fast changes presents a novel
challenge. In this paper, to address this challenge, we propose a change
detection network named Flow-CDNet, consisting of two branches: optical flow
branch and binary change detection branch. The first branch utilizes a pyramid
structure to extract displacement changes at multiple scales. The second one
combines a ResNet-based network with the optical flow branch's output to
generate fast change outputs. Subsequently, to supervise and evaluate this new
change detection framework, a self-built change detection dataset Flow-Change,
a loss function combining binary tversky loss and L2 norm loss, along with a
new evaluation metric called FEPE are designed. Quantitative experiments
conducted on Flow-Change dataset demonstrated that our approach outperforms the
existing methods. Furthermore, ablation experiments verified that the two
branches can promote each other to enhance the detection performance.

</details>


### [45] [LMPNet for Weakly-supervised Keypoint Discovery](https://arxiv.org/abs/2507.02308)
*Pei Guo,Ryan Farrell*

Main category: cs.CV

TL;DR: 该工作提出LMPNet模型，通过仅使用类别标签的弱监督方式，自动发现语义对象关键点。其核心在于将网络滤波器转换为关键点检测器，并引入LMP层、选择策略和聚类层，实现了与全监督方法媲美的检测精度和姿态鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在仅有类别标签的弱监督条件下，探索语义对象关键点的发现任务。旨在将判别式训练的中间层滤波器转化为具备空间稀疏性、一致性和多样性的关键点检测器，避免对手工设计损失项的依赖。

Method: 1. 提出新颖且计算高效的**Leaky Max Pooling (LMP) 层**，促使滤波器学习“不可重复局部模式”。 2. 设计**选择策略**确保滤波器激活的一致性。 3. 应用**注意力遮蔽**，强制网络关注整个对象。 4. 引入可学习的**聚类层**将关键点提议分组为最终预测。 5. 最终模型**LMPNet**直接操纵滤波器，具有高度可解释性。

Result: 1. LMPNet能够自动发现对对象姿态具有鲁棒性的语义关键点。 2. 关键点预测准确性与全监督的姿态估计模型相当。

Conclusion: LMPNet成功解决了弱监督下的语义对象关键点发现问题，通过巧妙地将网络滤波器转化为可解释且鲁棒的关键点检测器，其性能表现可与全监督方法相媲美。

Abstract: In this work, we explore the task of semantic object keypoint discovery
weakly-supervised by only category labels. This is achieved by transforming
discriminatively-trained intermediate layer filters into keypoint detectors. We
begin by identifying three preferred characteristics of keypoint detectors: (i)
spatially sparse activations, (ii) consistency and (iii) diversity. Instead of
relying on hand-crafted loss terms, a novel computationally-efficient leaky max
pooling (LMP) layer is proposed to explicitly encourage final conv-layer
filters to learn "non-repeatable local patterns" that are well aligned with
object keypoints. Informed by visualizations, a simple yet effective selection
strategy is proposed to ensure consistent filter activations and attention
mask-out is then applied to force the network to distribute its attention to
the whole object instead of just the most discriminative region. For the final
keypoint prediction, a learnable clustering layer is proposed to group keypoint
proposals into keypoint predictions. The final model, named LMPNet, is highly
interpretable in that it directly manipulates network filters to detect
predefined concepts. Our experiments show that LMPNet can (i) automatically
discover semantic keypoints that are robust to object pose and (ii) achieves
strong prediction accuracy comparable to a supervised pose estimation model.

</details>


### [46] [Perception Activator: An intuitive and portable framework for brain cognitive exploration](https://arxiv.org/abs/2507.02311)
*Le Xu,Qi Zhang,Qixian Zhang,Hongyun Zhang,Duoqian Miao,Cairong Zhao*

Main category: cs.CV

TL;DR: 现有脑视觉解码在多对象语义处理上存在不足。本文通过将fMRI信号注入图像特征，证明fMRI包含丰富的多对象语义和空间定位信息，能显著提升目标检测和实例分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有脑视觉解码方法过度依赖低级像素对齐，缺乏足够和精细的多对象语义对齐，导致重建图像在多语义对象上存在明显失真。本研究旨在更好地理解大脑的视觉感知模式以及当前解码模型如何处理语义对象。

Method: 研究开发了一个实验框架，将fMRI表征作为干预条件，通过交叉注意力机制将其注入多尺度图像特征中。随后，在目标检测和实例分割任务上，比较了有无fMRI信息时模型的下游性能和中间特征变化。

Result: 结果表明，整合fMRI信号显著提高了下游检测和分割的准确性。这证实了fMRI包含丰富的多对象语义线索和粗略的空间定位信息，而这些信息尚未被当前模型充分利用或整合。

Conclusion: fMRI数据蕴含丰富的多对象语义和空间定位信息，现有模型尚未充分利用。通过有效整合fMRI信号，可以显著提升脑视觉解码在多对象场景下的性能和准确性。

Abstract: Recent advances in brain-vision decoding have driven significant progress,
reconstructing with high fidelity perceived visual stimuli from neural
activity, e.g., functional magnetic resonance imaging (fMRI), in the human
visual cortex. Most existing methods decode the brain signal using a two-level
strategy, i.e., pixel-level and semantic-level. However, these methods rely
heavily on low-level pixel alignment yet lack sufficient and fine-grained
semantic alignment, resulting in obvious reconstruction distortions of multiple
semantic objects. To better understand the brain's visual perception patterns
and how current decoding models process semantic objects, we have developed an
experimental framework that uses fMRI representations as intervention
conditions. By injecting these representations into multi-scale image features
via cross-attention, we compare both downstream performance and intermediate
feature changes on object detection and instance segmentation tasks with and
without fMRI information. Our results demonstrate that incorporating fMRI
signals enhances the accuracy of downstream detection and segmentation,
confirming that fMRI contains rich multi-object semantic cues and coarse
spatial localization information-elements that current models have yet to fully
exploit or integrate.

</details>


### [47] [MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation](https://arxiv.org/abs/2507.02314)
*JaeHyuck Choi,MinJun Kim,JeHyeong Hong*

Main category: cs.CV

TL;DR: MAGIC是一种新型的少样本异常生成方法，它通过微调Stable Diffusion并结合多级扰动和上下文感知对齐，解决了现有方法在背景保持、遮罩对齐和语义有效性方面的不足，并在MVTec-AD数据集上表现SOTA。


<details>
  <summary>Details</summary>
Motivation: 工业质量控制中异常数据稀缺，需要少样本异常生成来扩充数据。现有扩散模型方法难以同时满足保持背景完整、精确遮罩内绘以及在语义有效位置生成异常的三个关键需求，常出现背景破坏、遮罩不准或语义不合理的问题。

Method: MAGIC方法的核心是微调Stable Diffusion的inpainting骨干网络，以保留正常区域并确保合成异常与遮罩严格对齐。为解决微调可能导致的多样性损失，MAGIC引入了两种互补的扰动策略：在微调和推理期间应用高斯提示级扰动以拓宽全局外观，以及遮罩引导的空间噪声注入以丰富局部纹理。此外，上下文感知遮罩对齐模块用于形成语义对应并重新定位遮罩，确保异常合理地包含在宿主对象内。

Result: 在MVTec-AD数据集上，采用一致的评估协议，MAGIC在下游异常任务中的表现优于先前的最新技术。

Conclusion: MAGIC成功地解决了少样本异常生成中的关键挑战，实现了背景完整性、精确遮罩对齐和语义有效性，并显著提升了生成异常的质量和多样性，为工业质量控制提供了更实用的数据增强方案。

Abstract: Few-shot anomaly generation is emerging as a practical solution for
augmenting the scarce anomaly data in industrial quality control settings. An
ideal generator would meet three demands at once, namely (i) keep the normal
background intact, (ii) inpaint anomalous regions to tightly overlap with the
corresponding anomaly masks, and (iii) generate anomalous regions in a
semantically valid location, while still producing realistic, diverse
appearances from only a handful of real examples. Existing diffusion-based
methods usually satisfy at most two of these requirements: global anomaly
generators corrupt the background, whereas mask-guided ones often falter when
the mask is imprecise or misplaced. We propose MAGIC--Mask-guided inpainting
with multi-level perturbations and Context-aware alignment--to resolve all
three issues. At its core, MAGIC fine-tunes a Stable Diffusion inpainting
backbone that preserves normal regions and ensures strict adherence of the
synthesized anomaly to the supplied mask, directly addressing background
corruption and misalignment. To offset the diversity loss that fine-tuning can
cause, MAGIC adds two complementary perturbation strategies: (i) Gaussian
prompt-level perturbation applied during fine-tuning and inference that
broadens the global appearance of anomalies while avoiding low-fidelity textual
appearances, and (ii) mask-guided spatial noise injection that enriches local
texture variations. Additionally, the context-aware mask alignment module forms
semantic correspondences and relocates masks so that every anomaly remains
plausibly contained within the host object, eliminating out-of-boundary
artifacts. Under a consistent identical evaluation protocol on the MVTec-AD
dataset, MAGIC outperforms previous state-of-the-arts in downstream anomaly
tasks.

</details>


### [48] [Are Synthetic Videos Useful? A Benchmark for Retrieval-Centric Evaluation of Synthetic Videos](https://arxiv.org/abs/2507.02316)
*Zecheng Zhao,Selena Song,Tong Chen,Zhi Chen,Shazia Sadiq,Yadan Luo*

Main category: cs.CV

TL;DR: 该研究提出了SynTVA数据集和基准，用于评估合成视频在文本到视频检索（TVR）任务中的效用，并展示了其通过数据集扩增提高TVR模型性能的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前的文生视频（T2V）评估指标主要关注视觉质量和时间一致性，但未能有效评估合成视频在文本到视频检索（TVR）等下游任务中的表现和实用性。

Method: 研究者构建了SynTVA数据集和基准，该数据集包含基于800个用户查询生成的合成视频，并对视频-文本对在“物体与场景”、“动作”、“属性”和“提示忠实度”四个语义对齐维度进行了标注。他们建立了评估框架，关联通用视频质量评估（VQA）指标与对齐分数，并检验了其对TVR性能的预测能力。此外，还开发了一个Auto-Evaluator来从现有指标估计对齐质量。

Result: SynTVA数据集被证明是数据集扩增的宝贵资源，能够有效筛选出高实用性的合成样本，这些样本能够显著提升文本到视频检索（TVR）任务的表现。

Conclusion: SynTVA为评估合成视频在下游检索任务中的效用提供了急需的基准和方法，证明了通过有选择地利用高实用性合成数据可以显著改善TVR模型性能。

Abstract: Text-to-video (T2V) synthesis has advanced rapidly, yet current evaluation
metrics primarily capture visual quality and temporal consistency, offering
limited insight into how synthetic videos perform in downstream tasks such as
text-to-video retrieval (TVR). In this work, we introduce SynTVA, a new dataset
and benchmark designed to evaluate the utility of synthetic videos for building
retrieval models. Based on 800 diverse user queries derived from MSRVTT
training split, we generate synthetic videos using state-of-the-art T2V models
and annotate each video-text pair along four key semantic alignment dimensions:
Object \& Scene, Action, Attribute, and Prompt Fidelity. Our evaluation
framework correlates general video quality assessment (VQA) metrics with these
alignment scores, and examines their predictive power for downstream TVR
performance. To explore pathways of scaling up, we further develop an
Auto-Evaluator to estimate alignment quality from existing metrics. Beyond
benchmarking, our results show that SynTVA is a valuable asset for dataset
augmentation, enabling the selection of high-utility synthetic samples that
measurably improve TVR outcomes. Project page and dataset can be found at
https://jasoncodemaker.github.io/SynTVA/.

</details>


### [49] [Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback](https://arxiv.org/abs/2507.02321)
*Nina Konovalova,Maxim Nikolaev,Andrey Kuznetsov,Aibek Alanov*

Main category: cs.CV

TL;DR: 针对扩散模型空间控制挑战，InnerControl通过在所有扩散步骤中强制空间一致性，显著提升了控制精度和生成质量，并实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像扩散模型取得了显著进展，但在生成输出上实现精确的空间控制仍面临挑战。现有方法（如ControlNet++）仅在最终去噪步骤应用一致性损失，忽略了中间生成阶段，限制了其有效性。

Method: 提出InnerControl训练策略，核心是在所有扩散步骤中强制执行空间一致性。具体做法是训练轻量级卷积探针，从每个去噪步骤的中间UNet特征中重建输入控制信号（如边缘、深度）。这些探针能从高噪声潜在变量中有效提取信号，生成伪真实条件，并通过最小化整个扩散过程中预测与目标条件之间的差异来优化对齐损失。

Result: InnerControl与ControlNet++等技术结合，在多种条件方法（如边缘、深度）上实现了最先进的性能，显著提升了控制保真度和生成质量。

Conclusion: InnerControl通过在扩散过程的各个阶段强制空间一致性，有效解决了现有方法的不足，从而极大地提升了文本到图像扩散模型的空间控制能力和生成质量，达到了新的性能高度。

Abstract: Despite significant progress in text-to-image diffusion models, achieving
precise spatial control over generated outputs remains challenging. ControlNet
addresses this by introducing an auxiliary conditioning module, while
ControlNet++ further refines alignment through a cycle consistency loss applied
only to the final denoising steps. However, this approach neglects intermediate
generation stages, limiting its effectiveness. We propose InnerControl, a
training strategy that enforces spatial consistency across all diffusion steps.
Our method trains lightweight convolutional probes to reconstruct input control
signals (e.g., edges, depth) from intermediate UNet features at every denoising
step. These probes efficiently extract signals even from highly noisy latents,
enabling pseudo ground truth controls for training. By minimizing the
discrepancy between predicted and target conditions throughout the entire
diffusion process, our alignment loss improves both control fidelity and
generation quality. Combined with established techniques like ControlNet++,
InnerControl achieves state-of-the-art performance across diverse conditioning
methods (e.g., edges, depth).

</details>


### [50] [Neural Network-based Study for Rice Leaf Disease Recognition and Classification: A Comparative Analysis Between Feature-based Model and Direct Imaging Model](https://arxiv.org/abs/2507.02322)
*Farida Siddiqi Prity,Mirza Raquib,Saydul Akbar Murad,Md. Jubayar Alam Rafi,Md. Khairul Bashar Bhuiyan,Anupam Kumar Bairagi*

Main category: cs.CV

TL;DR: 本研究提出并比较了基于特征分析和直接图像的ANN模型用于水稻叶病检测。结果显示，特征分析检测模型（FADM）表现优于直接图像中心检测模型（DICDM）。


<details>
  <summary>Details</summary>
Motivation: 水稻叶病严重影响生产力并造成经济损失，亟需早期检测。当前ANN方法多直接输入图像，但缺乏对特征分析检测模型（FADM）与直接图像中心检测模型（DICDM）在特征提取算法（FEA）有效性方面的深入比较。

Method: 本研究提出了基于人工神经网络（ANN）的图像处理技术。构建了两种模型：特征分析检测模型（FADM）和直接图像中心检测模型（DICDM）。FADM整合了多种特征提取、降维和特征选择算法，并结合极限学习机（ELM）。DICDM则直接处理图像，不使用FEA。实验在包含多种病害和健康叶片的数据集上，采用10折交叉验证进行评估和对比。

Result: 实验结果表明，特征分析检测模型（FADM）在水稻叶病分类中取得了最高的性能。

Conclusion: 所提出的特征分析检测模型（FADM）在水稻叶病检测方面具有巨大潜力，有助于改善作物健康，减少产量损失，提升水稻种植的生产力和可持续性。

Abstract: Rice leaf diseases significantly reduce productivity and cause economic
losses, highlighting the need for early detection to enable effective
management and improve yields. This study proposes Artificial Neural Network
(ANN)-based image-processing techniques for timely classification and
recognition of rice diseases. Despite the prevailing approach of directly
inputting images of rice leaves into ANNs, there is a noticeable absence of
thorough comparative analysis between the Feature Analysis Detection Model
(FADM) and Direct Image-Centric Detection Model (DICDM), specifically when it
comes to evaluating the effectiveness of Feature Extraction Algorithms (FEAs).
Hence, this research presents initial experiments on the Feature Analysis
Detection Model, utilizing various image Feature Extraction Algorithms,
Dimensionality Reduction Algorithms (DRAs), Feature Selection Algorithms
(FSAs), and Extreme Learning Machine (ELM). The experiments are carried out on
datasets encompassing bacterial leaf blight, brown spot, leaf blast, leaf
scald, Sheath blight rot, and healthy leaf, utilizing 10-fold Cross-Validation
method. A Direct Image-Centric Detection Model is established without the
utilization of any FEA, and the evaluation of classification performance relies
on different metrics. Ultimately, an exhaustive contrast is performed between
the achievements of the Feature Analysis Detection Model and Direct
Image-Centric Detection Model in classifying rice leaf diseases. The results
reveal that the highest performance is attained using the Feature Analysis
Detection Model. The adoption of the proposed Feature Analysis Detection Model
for detecting rice leaf diseases holds excellent potential for improving crop
health, minimizing yield losses, and enhancing overall productivity and
sustainability of rice farming.

</details>


### [51] [Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection](https://arxiv.org/abs/2507.02349)
*Rafic Nader,Vincent L'Allinec,Romain Bourcier,Florent Autrusseau*

Main category: cs.CV

TL;DR: 本文提出了一种全自动的两步神经网络方法，用于在脑血管MRAP中准确检测脑底动脉环（CoW）分叉，以辅助颅内动脉瘤的诊断。


<details>
  <summary>Details</summary>
Motivation: 颅内动脉瘤常发生在CoW的特定分叉处，准确检测这些关键地标对于及时有效的诊断至关重要。现有方法可能存在因地标距离过近或视觉特征相似而导致的漏检问题，以及CoW解剖结构变异性带来的挑战。

Method: 该方法采用两步神经网络流程：首先，一个目标检测网络识别地标附近的感兴趣区域（ROIs）；随后，一个经过修改并带有深度监督的U-Net网络被用于精确地定位分叉。此两步法旨在减少漏检并适应CoW的解剖变异性。研究使用自建和公开的脑部MRA数据集进行评估。

Result: 实验结果表明，该方法在分叉检测任务上取得了最高水平的性能。

Conclusion: 所提出的两步神经网络方法能有效准确地检测CoW分叉，解决了地标邻近和解剖变异性等问题，有望提升颅内动脉瘤的诊断效率和准确性。

Abstract: Intracranial aneurysms (ICA) commonly occur in specific segments of the
Circle of Willis (CoW), primarily, onto thirteen major arterial bifurcations.
An accurate detection of these critical landmarks is necessary for a prompt and
efficient diagnosis. We introduce a fully automated landmark detection approach
for CoW bifurcations using a two-step neural networks process. Initially, an
object detection network identifies regions of interest (ROIs) proximal to the
landmark locations. Subsequently, a modified U-Net with deep supervision is
exploited to accurately locate the bifurcations. This two-step method reduces
various problems, such as the missed detections caused by two landmarks being
close to each other and having similar visual characteristics, especially when
processing the complete MRA Time-of-Flight (TOF). Additionally, it accounts for
the anatomical variability of the CoW, which affects the number of detectable
landmarks per scan. We assessed the effectiveness of our approach using two
cerebral MRA datasets: our In-House dataset which had varying numbers of
landmarks, and a public dataset with standardized landmark configuration. Our
experimental results demonstrate that our method achieves the highest level of
performance on a bifurcation detection task.

</details>


### [52] [Lightweight Shrimp Disease Detection Research Based on YOLOv8n](https://arxiv.org/abs/2507.02354)
*Fei Yuhuan,Wang Gengchen,Liu Fenghao,Zang Ran,Sun Xufei,Chang Hao*

Main category: cs.CV

TL;DR: 本文提出一种基于YOLOv8n的轻量级网络，通过结构优化和引入注意力机制，显著提升对虾病的智能检测效率和精度，同时大幅降低模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 对虾疾病是水产养殖业经济损失的主要原因，为预防疾病传播和提高智能检测效率，急需开发高效的疾病检测方法。

Method: 提出一种基于YOLOv8n的轻量级网络架构。具体方法包括：设计RLDD检测头和C2f-EMCM模块以降低计算复杂度并提高效率；引入改进的SegNext_Attention自注意力机制以增强特征提取能力。

Result: 相比原始YOLOv8n，模型参数量减少32.3%，mAP@0.5达到92.7%（提升3%）。在URPC2020数据集上的泛化测试中，mAP@0.5提升4.1%。该模型在mAP@0.5、参数量和模型大小方面均优于其他轻量级YOLO系列模型。

Conclusion: 所提出的方法在检测精度和效率之间取得了最佳平衡，为对虾养殖智能疾病检测提供了可靠的技术支持。

Abstract: Shrimp diseases are one of the primary causes of economic losses in shrimp
aquaculture. To prevent disease transmission and enhance intelligent detection
efficiency in shrimp farming, this paper proposes a lightweight network
architecture based on YOLOv8n. First, by designing the RLDD detection head and
C2f-EMCM module, the model reduces computational complexity while maintaining
detection accuracy, improving computational efficiency. Subsequently, an
improved SegNext_Attention self-attention mechanism is introduced to further
enhance the model's feature extraction capability, enabling more precise
identification of disease characteristics. Extensive experiments, including
ablation studies and comparative evaluations, are conducted on a
self-constructed shrimp disease dataset, with generalization tests extended to
the URPC2020 dataset. Results demonstrate that the proposed model achieves a
32.3% reduction in parameters compared to the original YOLOv8n, with a mAP@0.5
of 92.7% (3% improvement over YOLOv8n). Additionally, the model outperforms
other lightweight YOLO-series models in mAP@0.5, parameter count, and model
size. Generalization experiments on the URPC2020 dataset further validate the
model's robustness, showing a 4.1% increase in mAP@0.5 compared to YOLOv8n. The
proposed method achieves an optimal balance between accuracy and efficiency,
providing reliable technical support for intelligent disease detection in
shrimp aquaculture.

</details>


### [53] [Holistic Tokenizer for Autoregressive Image Generation](https://arxiv.org/abs/2507.02358)
*Anlin Zheng,Haochen Wang,Yucheng Zhao,Weipeng Deng,Tiancai Wang,Xiangyu Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: Hita是一种新型图像分词器，通过引入整体到局部的分词方案和信息流控制，解决了自回归图像生成模型中捕获全局信息不足的问题，显著提升了生成性能和训练速度。


<details>
  <summary>Details</summary>
Motivation: 传统的自回归图像生成模型分步生成视觉token，难以捕捉token序列间的整体关系；多数视觉分词器将局部图像块映射为潜在token，导致全局信息受限。

Method: 引入Hita，一种新型自回归图像生成分词器。它采用整体到局部的分词方案，包含可学习的整体查询和局部补丁token。为更好地与自回归生成过程对齐，Hita采取两个策略：1) 整体token在前，局部补丁token在后，并使用因果注意力；2) 在将反量化token送入解码器前，通过轻量级融合模块优先处理整体token，控制信息流。

Result: 实验表明，Hita加速了自回归生成器的训练速度，并优于使用传统分词器的模型，在ImageNet基准上达到了2.59 FID和281.9 IS。对整体表示的详细分析突出其捕捉全局图像属性（如纹理、材质、形状）的能力。此外，Hita在零样本风格迁移和图像修复中也表现出有效性。

Conclusion: Hita通过创新的整体到局部图像分词和优化的信息流控制，有效解决了自回归图像生成模型在捕获全局信息方面的不足，显著提升了生成性能和训练效率，并在多项下游任务中展现了强大的泛化能力。

Abstract: The vanilla autoregressive image generation model generates visual tokens in
a step-by-step fashion, which limits the ability to capture holistic
relationships among token sequences. Moreover, most visual tokenizers map local
image patches into latent tokens, leading to limited global information. To
address this, we introduce \textit{Hita}, a novel image tokenizer for
autoregressive (AR) image generation. It introduces a holistic-to-local
tokenization scheme with learnable holistic queries and local patch tokens.
Besides, Hita incorporates two key strategies for improved alignment with the
AR generation process: 1) it arranges a sequential structure with holistic
tokens at the beginning followed by patch-level tokens while using causal
attention to maintain awareness of previous tokens; and 2) before feeding the
de-quantized tokens into the decoder, Hita adopts a lightweight fusion module
to control information flow to prioritize holistic tokens. Extensive
experiments show that Hita accelerates the training speed of AR generators and
outperforms those trained with vanilla tokenizers, achieving \textbf{2.59 FID}
and \textbf{281.9 IS} on the ImageNet benchmark. A detailed analysis of the
holistic representation highlights its ability to capture global image
properties such as textures, materials, and shapes. Additionally, Hita also
demonstrates effectiveness in zero-shot style transfer and image in-painting.
The code is available at
\href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}

</details>


### [54] [LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling](https://arxiv.org/abs/2507.02363)
*Jiahao Wu,Rui Peng,Jianbo Jiao,Jiayu Yang,Luyang Tang,Kaiqiang Xiong,Jie Liang,Jinbo Yan,Runling Liu,Ronggang Wang*

Main category: cs.CV

TL;DR: LocalDyGS是一种新的动态场景重建框架，通过将复杂场景分解为局部空间并解耦动静态特征，有效处理大规模和精细尺度的动态运动，实现逼真的动态视频合成。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界中复杂且高度动态的运动，从多视角输入合成任意视角的动态视频极具挑战性。现有基于神经辐射场（NeRF）或3D高斯飞溅（3D Gaussian Splatting）的方法仅限于建模精细尺度运动，极大地限制了其应用范围。

Method: 该论文提出了LocalDyGS，包含两部分：1) 将复杂动态场景分解为由种子定义的流线型局部空间，通过捕捉每个局部空间内的运动来实现全局建模。2) 解耦局部空间运动建模中的静态和动态特征，即一个跨时间步共享的静态特征，以及一个提供时间特定特征的动态残差场，两者结合并解码以生成时间高斯（Temporal Gaussians），从而建模每个局部空间内的运动。

Result: 研究成果是提出了一个新颖的动态场景重建框架，能够更真实地建模高度动态的现实世界场景。该方法不仅在各种精细尺度数据集上展现出与现有最佳（SOTA）方法相当的竞争力，而且首次尝试建模更大、更复杂的高度动态场景。

Conclusion: LocalDyGS通过其独特的场景分解和动静态特征解耦机制，成功解决了现有方法在处理大规模动态场景上的局限性，为逼真的动态视频合成和高度动态场景重建提供了一个更有效且实用的解决方案。

Abstract: Due to the complex and highly dynamic motions in the real world, synthesizing
dynamic videos from multi-view inputs for arbitrary viewpoints is challenging.
Previous works based on neural radiance field or 3D Gaussian splatting are
limited to modeling fine-scale motion, greatly restricting their application.
In this paper, we introduce LocalDyGS, which consists of two parts to adapt our
method to both large-scale and fine-scale motion scenes: 1) We decompose a
complex dynamic scene into streamlined local spaces defined by seeds, enabling
global modeling by capturing motion within each local space. 2) We decouple
static and dynamic features for local space motion modeling. A static feature
shared across time steps captures static information, while a dynamic residual
field provides time-specific features. These are combined and decoded to
generate Temporal Gaussians, modeling motion within each local space. As a
result, we propose a novel dynamic scene reconstruction framework to model
highly dynamic real-world scenes more realistically. Our method not only
demonstrates competitive performance on various fine-scale datasets compared to
state-of-the-art (SOTA) methods, but also represents the first attempt to model
larger and more complex highly dynamic scenes. Project page:
https://wujh2001.github.io/LocalDyGS/.

</details>


### [55] [UVLM: Benchmarking Video Language Model for Underwater World Understanding](https://arxiv.org/abs/2507.02373)
*Xizhe Xue,Yang Zhou,Dawei Yan,Ying Li,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TL;DR: 本文提出并构建了UVLM，一个专门用于水下观测的视频语言模型（VidLM）基准数据集，旨在解决现有VidLM在水下场景理解方面的不足。实验表明，在该数据集上进行微调能显著提升VidLM的水下世界理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）和视频语言模型（VidLMs）的应用主要集中在陆地场景，却忽视了水下观测领域对高级AI应用的高度需求。这一空白导致现有模型在理解复杂水下环境方面表现不佳。

Method: 引入UVLM水下观测基准，其构建结合了人类专业知识和AI模型。为确保数据质量和多样性，UVLM数据集精选了代表水下挑战（如光照变化、水体浑浊度、多样视角）的视频，覆盖了广泛的帧率、分辨率、419种海洋动物以及各类植物和地形。任务类型被结构化为生物和环境两大类，每类包含内容观察和变化/行动观察，共计20种独特任务。此外，还设计了挑战性评估指标用于定量比较。

Result: 通过在UVLM上对两种代表性VidLMs进行微调实验，结果显示模型的水下世界理解能力得到显著提升。同时，在现有陆上VidLM基准（如VideoMME和Perception text）上也表现出潜在的轻微改进。

Conclusion: UVLM成功地弥补了水下视频语言理解的空白，并验证了其在提升VidLMs水下理解能力方面的有效性。数据集和提示工程将公开发布，有望推动水下人工智能领域的研究与发展。

Abstract: Recently, the remarkable success of large language models (LLMs) has achieved
a profound impact on the field of artificial intelligence. Numerous advanced
works based on LLMs have been proposed and applied in various scenarios. Among
them, video language models (VidLMs) are particularly widely used. However,
existing works primarily focus on terrestrial scenarios, overlooking the highly
demanding application needs of underwater observation. To overcome this gap, we
introduce UVLM, an under water observation benchmark which is build through a
collaborative approach combining human expertise and AI models. To ensure data
quality, we have conducted in-depth considerations from multiple perspectives.
First, to address the unique challenges of underwater environments, we selected
videos that represent typical underwater challenges including light variations,
water turbidity, and diverse viewing angles to construct the dataset. Second,
to ensure data diversity, the dataset covers a wide range of frame rates,
resolutions, 419 classes of marine animals, and various static plants and
terrains. Next, for task diversity, we adopted a structured design where
observation targets are categorized into two major classes: biological and
environmental. Each category includes content observation and change/action
observation, totaling 20 distinct task types. Finally, we designed several
challenging evaluation metrics to enable quantitative comparison and analysis
of different methods. Experiments on two representative VidLMs demonstrate that
fine-tuning VidLMs on UVLM significantly improves underwater world
understanding while also showing potential for slight improvements on existing
in-air VidLM benchmarks, such as VideoMME and Perception text. The dataset and
prompt engineering will be released publicly.

</details>


### [56] [PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection](https://arxiv.org/abs/2507.02393)
*Seokyeong Lee,Sithu Aung,Junyong Choi,Seungryong Kim,Ig-Jae Kim,Junghyun Cho*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的单目3D物体检测（M3OD）伪标签框架，该框架仅使用视频数据，通过跨时间相邻帧聚合伪LiDAR点云，克服了数据稀缺和2D-3D歧义的挑战，并展现出高准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 单目3D物体检测（M3OD）面临数据稀缺（高标注成本）和2D到3D歧义的挑战。现有的弱监督和伪标签方法普遍受限于特定领域学习或仅依赖单次观察的形状信息，无法有效解决这些问题。

Method: 提出了一种新颖的伪标签框架，仅利用视频数据，且对遮挡更鲁棒，无需多视角设置、额外传感器、相机姿态或特定领域训练。具体而言，该方法通过物体点跟踪，聚合静态和动态物体在时间相邻帧间的伪LiDAR点云，从而在难以获取3D数据的场景中实现3D属性提取。

Result: 通过广泛的实验证明，该方法能确保可靠的准确性和强大的可扩展性。

Conclusion: 该方法是M3OD领域一个实用且有效的解决方案。

Abstract: Monocular 3D object detection (M3OD) has long faced challenges due to data
scarcity caused by high annotation costs and inherent 2D-to-3D ambiguity.
Although various weakly supervised methods and pseudo-labeling methods have
been proposed to address these issues, they are mostly limited by
domain-specific learning or rely solely on shape information from a single
observation. In this paper, we propose a novel pseudo-labeling framework that
uses only video data and is more robust to occlusion, without requiring a
multi-view setup, additional sensors, camera poses, or domain-specific
training. Specifically, we explore a technique for aggregating the
pseudo-LiDARs of both static and dynamic objects across temporally adjacent
frames using object point tracking, enabling 3D attribute extraction in
scenarios where 3D data acquisition is infeasible. Extensive experiments
demonstrate that our method ensures reliable accuracy and strong scalability,
making it a practical and effective solution for M3OD.

</details>


### [57] [Continual Multiple Instance Learning with Enhanced Localization for Histopathological Whole Slide Image Analysis](https://arxiv.org/abs/2507.02395)
*Byung Hyun Lee,Wongi Jeong,Woojae Han,Kyoungbun Lee,Se Young Chun*

Main category: cs.CV

TL;DR: 本文提出CoMEL框架，旨在解决多实例学习（MIL）在组织病理学全玻片图像（WSI）持续学习任务中面临的实例定位和遗忘问题，通过新颖的组件显著提升了袋级和实例级分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 多实例学习（MIL）虽然能降低大规模图像（如WSI）的标注成本，但其在“持续任务”中适应性差且存在“遗忘”问题，尤其在“实例分类以实现定位”方面。现有针对自然图像的弱增量学习方法不适用于WSI的MIL定位，因为WSI补丁数量庞大、尺寸大且缺乏全局关系。因此，迫切需要开发一种新的MIL框架来解决这些挑战，以实现持续学习和增强定位能力。

Method: 本文提出了Continual Multiple Instance Learning with Enhanced Localization (CoMEL) 框架，其核心组件包括：1. **Grouped Double Attention Transformer (GDAT)**，用于高效的实例编码；2. **Bag Prototypes-based Pseudo-Labeling (BPPL)**，用于可靠的实例伪标签生成；3. **Orthogonal Weighted Low-Rank Adaptation (OWLoRA)**，用于减轻袋级和实例级分类中的遗忘问题。

Result: 在三个公共WSI数据集上的大量实验表明，CoMEL表现优于现有技术。在持续MIL设置下，其袋级准确率最高提升了11.00%，定位准确率最高提升了23.4%。

Conclusion: CoMEL是一个有效的MIL框架，成功解决了大规模图像（如WSI）在持续学习场景下的实例定位和遗忘问题，显著提升了袋级和实例级的分类与定位性能，为病理图像分析提供了新的解决方案。

Abstract: Multiple instance learning (MIL) significantly reduced annotation costs via
bag-level weak labels for large-scale images, such as histopathological whole
slide images (WSIs). However, its adaptability to continual tasks with minimal
forgetting has been rarely explored, especially on instance classification for
localization. Weakly incremental learning for semantic segmentation has been
studied for continual localization, but it focused on natural images,
leveraging global relationships among hundreds of small patches (e.g., $16
\times 16$) using pre-trained models. This approach seems infeasible for MIL
localization due to enormous amounts ($\sim 10^5$) of large patches (e.g., $256
\times 256$) and no available global relationships such as cancer cells. To
address these challenges, we propose Continual Multiple Instance Learning with
Enhanced Localization (CoMEL), an MIL framework for both localization and
adaptability with minimal forgetting. CoMEL consists of (1) Grouped Double
Attention Transformer (GDAT) for efficient instance encoding, (2) Bag
Prototypes-based Pseudo-Labeling (BPPL) for reliable instance pseudo-labeling,
and (3) Orthogonal Weighted Low-Rank Adaptation (OWLoRA) to mitigate forgetting
in both bag and instance classification. Extensive experiments on three public
WSI datasets demonstrate superior performance of CoMEL, outperforming the prior
arts by up to $11.00\%$ in bag-level accuracy and up to $23.4\%$ in
localization accuracy under the continual MIL setup.

</details>


### [58] [Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection](https://arxiv.org/abs/2507.02398)
*Taehoon Kim,Jongwook Choi,Yonghyun Jeong,Haeun Noh,Jaejun Yoo,Seungryul Baek,Jongwon Choi*

Main category: cs.CV

TL;DR: 一种利用像素级时间不一致性检测深度伪造视频的方法，通过对每个像素进行时间轴上的傅里叶变换，并结合注意力与Transformer模块，克服了传统方法忽视时间伪造痕迹的缺点。


<details>
  <summary>Details</summary>
Motivation: 传统深度伪造检测器仅通过堆叠帧间空间频率谱来表示时间信息，导致无法检测到像素平面中的时间伪造痕迹，尤其是像素级的时间不一致性。

Method: 本方法对每个像素在时间轴上执行一维傅里叶变换以提取时间不一致性特征。引入一个端到端训练的注意力提议模块以精确L定位时间伪造痕迹区域。此外，使用联合Transformer模块有效整合像素级时间频率特征与时空上下文特征。

Result: 该框架在多样化和挑战性的检测场景中表现出强大的鲁棒性能，并有效扩展了可检测的伪造痕迹范围。

Conclusion: 本文提出的深度伪造视频检测框架，通过利用像素级时间不一致性并整合先进模块，显著提升了检测能力，是该领域的重要进展。

Abstract: We introduce a deepfake video detection approach that exploits pixel-wise
temporal inconsistencies, which traditional spatial frequency-based detectors
often overlook. Traditional detectors represent temporal information merely by
stacking spatial frequency spectra across frames, resulting in the failure to
detect temporal artifacts in the pixel plane. Our approach performs a 1D
Fourier transform on the time axis for each pixel, extracting features highly
sensitive to temporal inconsistencies, especially in areas prone to unnatural
movements. To precisely locate regions containing the temporal artifacts, we
introduce an attention proposal module trained in an end-to-end manner.
Additionally, our joint transformer module effectively integrates pixel-wise
temporal frequency features with spatio-temporal context features, expanding
the range of detectable forgery artifacts. Our framework represents a
significant advancement in deepfake video detection, providing robust
performance across diverse and challenging detection scenarios.

</details>


### [59] [TABNet: A Triplet Augmentation Self-Recovery Framework with Boundary-Aware Pseudo-Labels for Medical Image Segmentation](https://arxiv.org/abs/2507.02399)
*Peilin Zhang,Shaouxan Wua,Jun Feng,Zhuo Jin,Zhizezhang Gao,Jingkun Chen,Yaqiong Xing,Xiao Zhang*

Main category: cs.CV

TL;DR: 本文提出TAB Net，一个新颖的弱监督医学图像分割框架，通过三重增强自恢复和边界感知伪标签监督，有效解决了稀疏涂鸦标注的挑战，性能显著优于现有弱监督方法并接近全监督方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割需要大量标注数据，但获取完全标注耗时且成本高昂。稀疏涂鸦标注虽然高效，但其稀疏性限制了特征学习并缺乏足够的边界监督，这给训练分割网络带来了巨大挑战。

Method: 提出TAB Net，包含两个关键模块：1) **三重增强自恢复（TAS）模块**：通过强度变换、裁剪和拼图增强策略，引导网络从多样增强输入中恢复完整掩码，以增强特征学习和语义理解。2) **边界感知伪标签监督（BAP）模块**：融合双分支预测生成加权伪标签，并引入边界感知损失，以提高伪监督精度和边界建模能力。

Result: 在ACDC和MSCMR seg两个公开数据集上的实验评估表明，TAB Net显著优于最先进的基于涂鸦的弱监督分割方法，并且其性能可与全监督方法相媲美。

Conclusion: TAB Net为弱监督医学图像分割提供了一个高效且强大的解决方案，成功克服了稀疏涂鸦标注的局限性，实现了卓越的分割性能。

Abstract: Background and objective: Medical image segmentation is a core task in
various clinical applications. However, acquiring large-scale, fully annotated
medical image datasets is both time-consuming and costly. Scribble annotations,
as a form of sparse labeling, provide an efficient and cost-effective
alternative for medical image segmentation. However, the sparsity of scribble
annotations limits the feature learning of the target region and lacks
sufficient boundary supervision, which poses significant challenges for
training segmentation networks. Methods: We propose TAB Net, a novel
weakly-supervised medical image segmentation framework, consisting of two key
components: the triplet augmentation self-recovery (TAS) module and the
boundary-aware pseudo-label supervision (BAP) module. The TAS module enhances
feature learning through three complementary augmentation strategies: intensity
transformation improves the model's sensitivity to texture and contrast
variations, cutout forces the network to capture local anatomical structures by
masking key regions, and jigsaw augmentation strengthens the modeling of global
anatomical layout by disrupting spatial continuity. By guiding the network to
recover complete masks from diverse augmented inputs, TAS promotes a deeper
semantic understanding of medical images under sparse supervision. The BAP
module enhances pseudo-supervision accuracy and boundary modeling by fusing
dual-branch predictions into a loss-weighted pseudo-label and introducing a
boundary-aware loss for fine-grained contour refinement. Results: Experimental
evaluations on two public datasets, ACDC and MSCMR seg, demonstrate that TAB
Net significantly outperforms state-of-the-art methods for scribble-based
weakly supervised segmentation. Moreover, it achieves performance comparable to
that of fully supervised methods.

</details>


### [60] [Wildlife Target Re-Identification Using Self-supervised Learning in Non-Urban Settings](https://arxiv.org/abs/2507.02403)
*Mufhumudzi Muthivhi,Terence L. van Zyl*

Main category: cs.CV

TL;DR: 本研究探索自监督学习在野生动物重识别中的应用，以克服传统监督学习对大量标注数据的依赖，并证明了自监督模型在数据有限情况下的鲁棒性和在多种下游任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有野生动物重识别模型严重依赖大量带标注数据进行监督训练，这导致数据标注成本高昂且难以获取足够的大规模数据集，因此需要探索无需标注数据的替代方法。

Method: 利用相机陷阱数据中的时间图像对，自动提取个体的两个不同视图，并使用这些无监督的图像对训练自监督模型。模型学习到的表示在开放世界场景和多种下游任务中与监督特征进行对比评估。

Result: 实验结果显示，自监督模型即使在数据有限的情况下也表现出更高的鲁棒性。此外，自监督特征在所有下游任务中的性能均优于监督学习特征。

Conclusion: 自监督学习在野生动物重识别任务中展现出强大的鲁棒性和优越的性能，特别是在数据受限时，为该领域提供了一种有前景且高效的解决方案。

Abstract: Wildlife re-identification aims to match individuals of the same species
across different observations. Current state-of-the-art (SOTA) models rely on
class labels to train supervised models for individual classification. This
dependence on annotated data has driven the curation of numerous large-scale
wildlife datasets. This study investigates self-supervised learning
Self-Supervised Learning (SSL) for wildlife re-identification. We automatically
extract two distinct views of an individual using temporal image pairs from
camera trap data without supervision. The image pairs train a self-supervised
model from a potentially endless stream of video data. We evaluate the learnt
representations against supervised features on open-world scenarios and
transfer learning in various wildlife downstream tasks. The analysis of the
experimental results shows that self-supervised models are more robust even
with limited data. Moreover, self-supervised features outperform supervision
across all downstream tasks. The code is available here
https://github.com/pxpana/SSLWildlife.

</details>


### [61] [PosDiffAE: Position-aware Diffusion Auto-encoder For High-Resolution Brain Tissue Classification Incorporating Artifact Restoration](https://arxiv.org/abs/2507.02405)
*Ayantika Das,Moitreya Chaudhuri,Koushik Bhat,Keerthi Ram,Mihail Bota,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 该研究提出将编码器与扩散模型结合，形成扩散自编码模型，以学习图像特异性语义表示并组织潜在空间，并将其应用于脑图像分析（识别细胞模式、区分组织类型）和无监督图像伪影修复（撕裂、JPEG）。


<details>
  <summary>Details</summary>
Motivation: 现有去噪扩散模型虽能生成高保真图像样本，但其采样机制无法提取图像特异性语义表示，也无法像自编码器那样提供组织潜在空间的方法。

Method: 1. 整合编码器与扩散模型，建立“扩散自编码”框架，用于学习图像特异性表示并组织潜在空间。
2. 设计潜在空间结构化机制：通过回归高分辨率图像块的位置信息，使潜在表示能够识别脑图像中的区域特异性细胞模式，并区分脑组织类型。
3. 开发无监督撕裂伪影修复技术：基于邻域感知，利用潜在表示和扩散模型在推理时的受限生成能力。
4. 开发无监督JPEG伪影修复技术：通过表示引导，并利用扩散模型在推理时可控的加噪和去噪能力。

Result: 1. 成功构建了有利于区分脑组织类型的结构化潜在空间。
2. 实现了基于邻域感知的无监督撕裂伪影修复技术。
3. 实现了基于表示引导的无监督JPEG伪影修复技术。

Conclusion: 通过将自编码器与扩散模型结合，本研究弥补了扩散模型在语义表示提取和潜在空间组织方面的不足，并成功应用于脑图像的细胞模式识别、组织类型区分以及多种无监督图像伪影修复任务。

Abstract: Denoising diffusion models produce high-fidelity image samples by capturing
the image distribution in a progressive manner while initializing with a simple
distribution and compounding the distribution complexity. Although these models
have unlocked new applicabilities, the sampling mechanism of diffusion does not
offer means to extract image-specific semantic representation, which is
inherently provided by auto-encoders. The encoding component of auto-encoders
enables mapping between a specific image and its latent space, thereby offering
explicit means of enforcing structures in the latent space. By integrating an
encoder with the diffusion model, we establish an auto-encoding formulation,
which learns image-specific representations and offers means to organize the
latent space. In this work, First, we devise a mechanism to structure the
latent space of a diffusion auto-encoding model, towards recognizing
region-specific cellular patterns in brain images. We enforce the
representations to regress positional information of the patches from
high-resolution images. This creates a conducive latent space for
differentiating tissue types of the brain. Second, we devise an unsupervised
tear artifact restoration technique based on neighborhood awareness, utilizing
latent representations and the constrained generation capability of diffusion
models during inference. Third, through representational guidance and
leveraging the inference time steerable noising and denoising capability of
diffusion, we devise an unsupervised JPEG artifact restoration technique.

</details>


### [62] [A Novel Tuning Method for Real-time Multiple-Object Tracking Utilizing Thermal Sensor with Complexity Motion Pattern](https://arxiv.org/abs/2507.02408)
*Duong Nguyen-Ngoc Tran,Long Hoang Pham,Chi Dai Tran,Quoc Pham-Nam Ho,Huy-Hung Nguyen,Jae Wook Jeon*

Main category: cs.CV

TL;DR: 针对热图像中行人跟踪的挑战，本文提出一种新颖的两阶段超参数调优方法，无需复杂模型即可实现高精度，并在PBVS热成像MOT数据集上表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在低能见度或恶劣光照条件下，RGB摄像头难以进行监控，热成像多目标跟踪变得至关重要。然而，热图像的低级特征表示使得准确检测和跟踪行人成为主要挑战。

Method: 提出一种新颖的行人跟踪调优方法，该框架通过优化两个阶段，并为每个阶段调优最合适的超参数来最大化跟踪性能，无需依赖复杂的重识别或运动模型。

Result: 在PBVS热成像MOT数据集上的大量实验表明，该方法在各种热像仪条件下均表现出高效性和高精度。

Conclusion: 该方法为现实世界中的热成像监控应用提供了一个鲁棒且有效的解决方案，特别适用于行人跟踪。

Abstract: Multi-Object Tracking in thermal images is essential for surveillance
systems, particularly in challenging environments where RGB cameras struggle
due to low visibility or poor lighting conditions. Thermal sensors enhance
recognition tasks by capturing infrared signatures, but a major challenge is
their low-level feature representation, which makes it difficult to accurately
detect and track pedestrians. To address this, the paper introduces a novel
tuning method for pedestrian tracking, specifically designed to handle the
complex motion patterns in thermal imagery. The proposed framework optimizes
two-stages, ensuring that each stage is tuned with the most suitable
hyperparameters to maximize tracking performance. By fine-tuning
hyperparameters for real-time tracking, the method achieves high accuracy
without relying on complex reidentification or motion models. Extensive
experiments on PBVS Thermal MOT dataset demonstrate that the approach is highly
effective across various thermal camera conditions, making it a robust solution
for real-world surveillance applications.

</details>


### [63] [Privacy-preserving Preselection for Face Identification Based on Packing](https://arxiv.org/abs/2507.02414)
*Rundong Xin,Taotao Wang,Jin Wang,Chonghe Zhao,Jing Wang*

Main category: cs.CV

TL;DR: 本文提出一种名为PFIP的新颖高效方案，通过预选择和打包技术，显著提高了密文域人脸识别系统的检索效率，同时保持了识别精度。


<details>
  <summary>Details</summary>
Motivation: 密文域人脸识别系统因隐私保护需求日益重要，但随着密文模板库的增长，人脸检索过程变得耗时。研究旨在解决这一效率瓶颈。

Method: 本文提出Privacy-Preserving Preselection for Face Identification Based on Packing (PFIP)方案。该方案结合了创新的预选择机制以减少计算开销，并引入打包模块以增强生物识别系统在注册阶段的灵活性。

Result: 在LFW和CASIA数据集上进行的实验表明，PFIP保持了原始人脸识别模型的准确性，实现了100%的命中率，并在300毫秒内检索了1,000个密文人脸模板。与现有方法相比，PFIP的检索效率提高了近50倍。

Conclusion: PFIP是一种高效且准确的密文域人脸检索方案，有效解决了大规模密文模板库中检索速度慢的问题，显著提升了检索效率，同时不牺牲识别精度。

Abstract: Face identification systems operating in the ciphertext domain have garnered
significant attention due to increasing privacy concerns and the potential
recovery of original facial data. However, as the size of ciphertext template
libraries grows, the face retrieval process becomes progressively more
time-intensive. To address this challenge, we propose a novel and efficient
scheme for face retrieval in the ciphertext domain, termed Privacy-Preserving
Preselection for Face Identification Based on Packing (PFIP). PFIP incorporates
an innovative preselection mechanism to reduce computational overhead and a
packing module to enhance the flexibility of biometric systems during the
enrollment stage. Extensive experiments conducted on the LFW and CASIA datasets
demonstrate that PFIP preserves the accuracy of the original face recognition
model, achieving a 100% hit rate while retrieving 1,000 ciphertext face
templates within 300 milliseconds. Compared to existing approaches, PFIP
achieves a nearly 50x improvement in retrieval efficiency.

</details>


### [64] [Determination Of Structural Cracks Using Deep Learning Frameworks](https://arxiv.org/abs/2507.02416)
*Subhasis Dasgupta,Jaydip Sen,Tuhina Halder*

Main category: cs.CV

TL;DR: 本研究针对结构裂缝检测，提出一种结合残差U-Net和卷积元模型的新型深度学习集成架构。实验表明，该集成模型在检测准确性和效率上超越了传统模型和单一模型，尤其适用于低分辨率图像，为自动化结构缺陷监测提供了更可靠的解决方案。


<details>
  <summary>Details</summary>
Motivation: 结构裂缝检测对公共安全至关重要，能预防潜在结构失效。然而，人工检测方法效率低下、结果不一致且易受人为错误影响，从而损害评估的可靠性。因此，需要更准确高效的自动化检测方案。

Method: 本研究引入一种新颖的深度学习架构以提高结构裂缝检测的准确性和效率。具体方法包括：1. 采用多种配置的残差U-Net模型。2. 将这些残差U-Net模型与包含卷积块的元模型集成，形成一个集成（ensemble）模型，旨在提升预测效率。3. 将该集成模型的性能与SegNet和传统U-Net等已建立的架构进行对比评估。4. 性能评估基于IoU（Intersection over Union）指标和DICE系数。

Result: 实验结果表明：1. 残差U-Net模型表现优于其前身（如传统U-Net），尤其是在处理低分辨率图像时。2. 集成模型超越了单一模型的性能，被证明是最有效的模型。3. 集成模型在IoU和DICE系数上取得了最高分，证实其卓越的准确性。

Conclusion: 本研究所提出的深度学习集成架构在结构裂缝检测方面取得了显著进展，为开发更可靠的自动化系统以进行结构缺陷监测任务提供了新的途径。

Abstract: Structural crack detection is a critical task for public safety as it helps
in preventing potential structural failures that could endanger lives. Manual
detection by inexperienced personnel can be slow, inconsistent, and prone to
human error, which may compromise the reliability of assessments. The current
study addresses these challenges by introducing a novel deep-learning
architecture designed to enhance the accuracy and efficiency of structural
crack detection. In this research, various configurations of residual U-Net
models were utilized. These models, due to their robustness in capturing fine
details, were further integrated into an ensemble with a meta-model comprising
convolutional blocks. This unique combination aimed to boost prediction
efficiency beyond what individual models could achieve. The ensemble's
performance was evaluated against well-established architectures such as SegNet
and the traditional U-Net. Results demonstrated that the residual U-Net models
outperformed their predecessors, particularly with low-resolution imagery, and
the ensemble model exceeded the performance of individual models, proving it as
the most effective. The assessment was based on the Intersection over Union
(IoU) metric and DICE coefficient. The ensemble model achieved the highest
scores, signifying superior accuracy. This advancement suggests way for more
reliable automated systems in structural defects monitoring tasks.

</details>


### [65] [AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars](https://arxiv.org/abs/2507.02419)
*Yiming Zhong,Xiaolin Zhang,Ligang Liu,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: 本文提出AvatarMakeup，一种专为3D虚拟头像设计的化妆方法，利用预训练扩散模型从单张参考照片进行化妆迁移，实现了高质量且在表情和视角变化下保持一致的化妆效果。


<details>
  <summary>Details</summary>
Motivation: 3D虚拟头像的个性化定制（如化妆）可增强视觉吸引力，但该领域研究不足。现有3D高斯编辑方法在应用于面部化妆时，无法满足真实感化妆的基本要求：1) 驱动表情下外观一致性；2) 化妆过程中的身份保持；3) 精确控制细节。

Method: 本文提出AvatarMakeup方法，采用粗到细的策略。首先利用预训练扩散模型生成化妆图像作为监督，并通过“Coherent Duplication”方法优化全局UV图，记录生成化妆图像的平均面部属性，从而实现动态和多视角下化妆的粗略应用和一致性。然后，通过在扩散模型中整合“Refinement Module”进一步提升化妆质量，以达到高品质细节。

Result: 实验结果表明，AvatarMakeup在化妆迁移质量和动画过程中的一致性方面达到了SOTA水平。

Conclusion: AvatarMakeup成功解决了3D虚拟头像化妆中一致性、身份保持和细节控制的挑战，实现了高质量且在动画中保持一致的化妆效果。

Abstract: Similar to facial beautification in real life, 3D virtual avatars require
personalized customization to enhance their visual appeal, yet this area
remains insufficiently explored. Although current 3D Gaussian editing methods
can be adapted for facial makeup purposes, these methods fail to meet the
fundamental requirements for achieving realistic makeup effects: 1) ensuring a
consistent appearance during drivable expressions, 2) preserving the identity
throughout the makeup process, and 3) enabling precise control over fine
details. To address these, we propose a specialized 3D makeup method named
AvatarMakeup, leveraging a pretrained diffusion model to transfer makeup
patterns from a single reference photo of any individual. We adopt a
coarse-to-fine idea to first maintain the consistent appearance and identity,
and then to refine the details. In particular, the diffusion model is employed
to generate makeup images as supervision. Due to the uncertainties in diffusion
process, the generated images are inconsistent across different viewpoints and
expressions. Therefore, we propose a Coherent Duplication method to coarsely
apply makeup to the target while ensuring consistency across dynamic and
multiview effects. Coherent Duplication optimizes a global UV map by recoding
the averaged facial attributes among the generated makeup images. By querying
the global UV map, it easily synthesizes coherent makeup guidance from
arbitrary views and expressions to optimize the target avatar. Given the coarse
makeup avatar, we further enhance the makeup by incorporating a Refinement
Module into the diffusion model to achieve high makeup quality. Experiments
demonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality
and consistency throughout animation.

</details>


### [66] [F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image Classification via Image-Level Disentangled Prompt Tuning](https://arxiv.org/abs/2507.02437)
*Wei Li,Jingyang Zhang,Lihao Liu,Guoan Wang,Junjun He,Yang Chen,Lixu Gu*

Main category: cs.CV

TL;DR: 本文提出一种名为F²TTA的实用测试时间自适应任务，以应对医学数据以任意长度和随机顺序的领域片段到达的挑战。为此，我们提出了Image-level Disentangled Prompt Tuning (I-DiPT) 框架，包含图像不变提示和图像特定提示，并通过不确定性导向掩蔽(UoM)和并行图蒸馏(PGD)来增强提示的学习能力，有效应对不可预测的域偏移。


<details>
  <summary>Details</summary>
Motivation: 现有测试时间自适应(TTA)方法假设数据以完整域单元到达，但这与临床实践中因资源限制和患者变异性导致数据以任意长度和随机顺序的域片段形式到达不符。这种“自由形式”的域片段带来的不可预测的偏移会扭曲自适应过程，因此需要研究F²TTA任务来解决这一实际问题。

Method: 我们提出了Image-level Disentangled Prompt Tuning (I-DiPT) 框架。I-DiPT采用图像不变提示来探索域不变表示以缓解不可预测的偏移，并使用图像特定提示来适应源模型到每个测试图像。为解决提示知识表示不足的问题，我们引入了不确定性导向掩蔽 (UoM)，通过源模型表示的不确定性驱动的掩蔽一致性学习，促使提示从传入图像中提取足够信息。此外，我们提出了并行图蒸馏 (PGD) 方法，通过并行图网络重用历史图像特定和图像不变提示的知识。

Result: 在乳腺癌和青光眼分类任务上的实验结果表明，我们的方法在F²TTA设置中优于现有TTA方法。

Conclusion: 本研究提出的I-DiPT框架及其组成部分UoM和PGD，有效解决了在临床实践中医学数据以自由形式域片段到达时进行测试时间自适应的挑战，展现出优越的性能。

Abstract: Test-Time Adaptation (TTA) has emerged as a promising solution for adapting a
source model to unseen medical sites using unlabeled test data, due to the high
cost of data annotation. Existing TTA methods consider scenarios where data
from one or multiple domains arrives in complete domain units. However, in
clinical practice, data usually arrives in domain fragments of arbitrary
lengths and in random arrival orders, due to resource constraints and patient
variability. This paper investigates a practical Free-Form Test-Time Adaptation
(F$^{2}$TTA) task, where a source model is adapted to such free-form domain
fragments, with shifts occurring between fragments unpredictably. In this
setting, these shifts could distort the adaptation process. To address this
problem, we propose a novel Image-level Disentangled Prompt Tuning (I-DiPT)
framework. I-DiPT employs an image-invariant prompt to explore domain-invariant
representations for mitigating the unpredictable shifts, and an image-specific
prompt to adapt the source model to each test image from the incoming
fragments. The prompts may suffer from insufficient knowledge representation
since only one image is available for training. To overcome this limitation, we
first introduce Uncertainty-oriented Masking (UoM), which encourages the
prompts to extract sufficient information from the incoming image via masked
consistency learning driven by the uncertainty of the source model
representations. Then, we further propose a Parallel Graph Distillation (PGD)
method that reuses knowledge from historical image-specific and image-invariant
prompts through parallel graph networks. Experiments on breast cancer and
glaucoma classification demonstrate the superiority of our method over existing
TTA approaches in F$^{2}$TTA. Code is available at
https://github.com/mar-cry/F2TTA.

</details>


### [67] [Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic](https://arxiv.org/abs/2507.02443)
*Sandro Costa Magalhães,Marco Almeida,Filipe Neves dos Santos,António Paulo Moreira,Jorge Dias*

Main category: cs.CV

TL;DR: 为解决机器人移动时目标检测慢的问题，本研究通过FINN架构在FPGA上部署量化ANN，成功将MobileNet v1推理速度提升至6611 FPS，并达到98%的成功率，证明了FPGA加速ANNs的有效性。


<details>
  <summary>Details</summary>
Motivation: 机器人移动时进行目标检测通常需要减速，且摄像头配置低帧率，导致任务执行时间增加。现有FPGA部署工具（如AMD Vitis-AI）未能充分利用FPGA的可编程逻辑（PL）部分，限制了检测算法的效率。

Method: 本研究利用FINN架构，在FPGA的PL部分部署了三种量化人工神经网络（ANNs）：4比特量化的MobileNet v1、2比特量化的CNV和1比特量化（BNN）的CNV。这些模型均在自采集并开源的RG2C数据集上进行了训练。

Result: 实验结果显示，MobileNet v1表现最优，达到了98%的成功率和6611 FPS的推理速度。

Conclusion: 本工作证明了通过使用FPGA可以显著加速人工神经网络（ANNs），使其适用于机器人中的注意力机制，从而解决机器人移动时目标检测的效率瓶颈。

Abstract: Robots usually slow down for canning to detect objects while moving.
Additionally, the robot's camera is configured with a low framerate to track
the velocity of the detection algorithms. This would be constrained while
executing tasks and exploring, making robots increase the task execution time.
AMD has developed the Vitis-AI framework to deploy detection algorithms into
FPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we
use the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit
quantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation
(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This
is a self-acquired dataset released in open access. MobileNet v1 performed
better, reaching a success rate of 98 % and an inference speed of 6611 FPS. In
this work, we proved that we can use FPGAs to speed up ANNs and make them
suitable for attention mechanisms.

</details>


### [68] [From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding](https://arxiv.org/abs/2507.02790)
*Xiangfeng Wang,Xiao Li,Yadong Wei,Xueyu Song,Yang Song,Xiaoqiang Xia,Fangrui Zeng,Zaiyi Chen,Liu Liu,Gu Xu,Tong Xu*

Main category: cs.CV

TL;DR: 本文提出HIVE框架，利用多模态理解自动剪辑视频，通过分解剪辑任务并引入新数据集，显著提升了自动剪辑质量。


<details>
  <summary>Details</summary>
Motivation: 在线视频内容激增，对高效视频剪辑需求大。现有自动剪辑方法过度依赖文本线索，忽视视觉信息，导致输出不连贯。

Method: 提出HIVE框架，通过多模态大语言模型（MLLMs）进行人物提取、对话分析和叙事总结，以实现全面的视频理解。将剪辑过程分解为亮点检测、开头/结尾选择和无关内容修剪三个子任务，并应用场景级分割。此外，引入了新的基准数据集DramaAD。

Result: HIVE框架在通用和广告导向剪辑任务中均优于现有基线方法，显著缩小了自动剪辑与人工剪辑之间的质量差距。

Conclusion: HIVE框架通过其人启发式的多模态叙事理解和分解的剪辑流程，有效解决了现有自动视频剪辑的局限性，显著提升了自动剪辑的质量和连贯性，为高效视频内容浓缩提供了有效途径。

Abstract: The rapid growth of online video content, especially on short video
platforms, has created a growing demand for efficient video editing techniques
that can condense long-form videos into concise and engaging clips. Existing
automatic editing methods predominantly rely on textual cues from ASR
transcripts and end-to-end segment selection, often neglecting the rich visual
context and leading to incoherent outputs. In this paper, we propose a
human-inspired automatic video editing framework (HIVE) that leverages
multimodal narrative understanding to address these limitations. Our approach
incorporates character extraction, dialogue analysis, and narrative
summarization through multimodal large language models, enabling a holistic
understanding of the video content. To further enhance coherence, we apply
scene-level segmentation and decompose the editing process into three subtasks:
highlight detection, opening/ending selection, and pruning of irrelevant
content. To facilitate research in this area, we introduce DramaAD, a novel
benchmark dataset comprising over 800 short drama episodes and 500
professionally edited advertisement clips. Experimental results demonstrate
that our framework consistently outperforms existing baselines across both
general and advertisement-oriented editing tasks, significantly narrowing the
quality gap between automatic and human-edited videos.

</details>


### [69] [IGDNet: Zero-Shot Robust Underexposed Image Enhancement via Illumination-Guided and Denoising](https://arxiv.org/abs/2507.02445)
*Hailong Yan,Junjian Huang,Tingwen Huang*

Main category: cs.CV

TL;DR: IGDNet是一种零样本图像增强方法，无需训练数据，通过分解和去噪模块有效恢复曝光不足图像并抑制噪声，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像增强方法依赖难以获取的成对监督数据，且易导致过增强和细节失真。

Method: 提出IGDNet，一个零样本增强框架，仅基于单张测试图像工作。它包含分解模块（通过密集连接网络分离光照和反射分量）和去噪模块（利用光照引导的像素自适应校正增强非均匀光照区域），并通过下采样生成噪声对进行迭代优化。

Result: 在四个公共数据集上，IGDNet显著提升了复杂光照下的视觉质量。定量指标PSNR (20.41dB) 和 SSIM (0.860dB) 均优于14种最先进的无监督方法。

Conclusion: IGDNet作为一种零样本图像增强方法，有效解决了传统方法的局限性，在不依赖训练数据的情况下，实现了出色的图像恢复和去噪效果，并在性能上超越了现有无监督方法。

Abstract: Current methods for restoring underexposed images typically rely on
supervised learning with paired underexposed and well-illuminated images.
However, collecting such datasets is often impractical in real-world scenarios.
Moreover, these methods can lead to over-enhancement, distorting
well-illuminated regions. To address these issues, we propose IGDNet, a
Zero-Shot enhancement method that operates solely on a single test image,
without requiring guiding priors or training data. IGDNet exhibits strong
generalization ability and effectively suppresses noise while restoring
illumination. The framework comprises a decomposition module and a denoising
module. The former separates the image into illumination and reflection
components via a dense connection network, while the latter enhances
non-uniformly illuminated regions using an illumination-guided pixel adaptive
correction method. A noise pair is generated through downsampling and refined
iteratively to produce the final result. Extensive experiments on four public
datasets demonstrate that IGDNet significantly improves visual quality under
complex lighting conditions. Quantitative results on metrics like PSNR
(20.41dB) and SSIM (0.860dB) show that it outperforms 14 state-of-the-art
unsupervised methods. The code will be released soon.

</details>


### [70] [Weakly-supervised Contrastive Learning with Quantity Prompts for Moving Infrared Small Target Detection](https://arxiv.org/abs/2507.02454)
*Weiwei Duan,Luping Ji,Shengjia Chen,Sicheng Zhu,Jianghong Huang,Mao Ye*

Main category: cs.CV

TL;DR: 本文提出了一种名为WeCoL的弱监督对比学习方案，用于红外小目标检测。该方案仅需目标数量提示，通过结合SAM、潜在目标挖掘、对比学习和长短期运动感知，有效降低了标注成本，并取得了与先进全监督方法相媲美的性能。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测面临目标尺寸微小、背景对比度弱的巨大挑战。现有方法多为全监督，严重依赖耗时昂贵的手动标注，尤其在低质量红外图像中。研究旨在探索弱监督策略，以减少对大量人工标注的需求。

Method: 提出了一种新的弱监督对比学习（WeCoL）方案，训练时仅需简单的目标数量提示。具体方法包括：1) 基于预训练的Segment Anything Model (SAM) 设计了潜在目标挖掘策略，整合目标激活图和多帧能量累积；2) 采用对比学习，通过计算特征子空间中正负样本的相似性来提高伪标签的可靠性；3) 提出了长短期运动感知学习方案，以同时建模小目标的局部运动模式和全局运动轨迹。

Result: 在DAUB和ITSDT-15K两个公共数据集上进行的广泛实验验证了该弱监督方案的有效性。结果显示，所提出的方法通常优于早期的全监督方法，甚至其性能可达到最先进（SOTA）全监督方法的90%以上。

Conclusion: 本研究首次探索了弱监督红外小目标检测，并提出了一种有效的WeCoL方案。该方案在显著减少对高成本人工标注依赖的同时，仍能实现与当前先进的全监督方法相近的卓越性能，表明弱监督学习在该领域具有巨大潜力。

Abstract: Different from general object detection, moving infrared small target
detection faces huge challenges due to tiny target size and weak background
contrast.Currently, most existing methods are fully-supervised, heavily relying
on a large number of manual target-wise annotations. However, manually
annotating video sequences is often expensive and time-consuming, especially
for low-quality infrared frame images. Inspired by general object detection,
non-fully supervised strategies ($e.g.$, weakly supervised) are believed to be
potential in reducing annotation requirements. To break through traditional
fully-supervised frameworks, as the first exploration work, this paper proposes
a new weakly-supervised contrastive learning (WeCoL) scheme, only requires
simple target quantity prompts during model training.Specifically, in our
scheme, based on the pretrained segment anything model (SAM), a potential
target mining strategy is designed to integrate target activation maps and
multi-frame energy accumulation.Besides, contrastive learning is adopted to
further improve the reliability of pseudo-labels, by calculating the similarity
between positive and negative samples in feature subspace.Moreover, we propose
a long-short term motion-aware learning scheme to simultaneously model the
local motion patterns and global motion trajectory of small targets.The
extensive experiments on two public datasets (DAUB and ITSDT-15K) verify that
our weakly-supervised scheme could often outperform early fully-supervised
methods. Even, its performance could reach over 90\% of state-of-the-art (SOTA)
fully-supervised ones.

</details>


### [71] [Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk](https://arxiv.org/abs/2507.02477)
*Gaochao Song,Zibo Zhao,Haohan Weng,Jingbo Zeng,Rongfei Jia,Shenghua Gao*

Main category: cs.CV

TL;DR: Mesh Silksong是一种紧凑高效的新型网格表示方法，通过单次顶点访问实现自回归生成，显著减少冗余，提高压缩率并保证优异的几何特性。


<details>
  <summary>Details</summary>
Motivation: 现有网格标记化方法会产生重复的顶点标记序列，导致网络能力浪费。

Method: 引入Mesh Silksong，通过仅访问每个网格顶点一次的方式进行标记化，以自回归方式生成多边形网格。

Result: 将令牌序列冗余减少50%，实现约22%的最先进压缩率。生成的网格具有优越的几何特性，包括流形拓扑、水密性和一致的面法线。实验证明其能够生成复杂的网格并显著提高几何完整性。

Conclusion: Mesh Silksong是一种有效且高效的网格表示与生成方法，在压缩效率和几何完整性方面均取得了显著提升，适用于实际应用。

Abstract: We introduce Mesh Silksong, a compact and efficient mesh representation
tailored to generate the polygon mesh in an auto-regressive manner akin to silk
weaving. Existing mesh tokenization methods always produce token sequences with
repeated vertex tokens, wasting the network capability. Therefore, our approach
tokenizes mesh vertices by accessing each mesh vertice only once, reduces the
token sequence's redundancy by 50\%, and achieves a state-of-the-art
compression rate of approximately 22\%. Furthermore, Mesh Silksong produces
polygon meshes with superior geometric properties, including manifold topology,
watertight detection, and consistent face normals, which are critical for
practical applications. Experimental results demonstrate the effectiveness of
our approach, showcasing not only intricate mesh generation but also
significantly improved geometric integrity.

</details>


### [72] [Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection](https://arxiv.org/abs/2507.02844)
*Ziqi Miao,Yi Ding,Lijun Li,Jing Shao*

Main category: cs.CV

TL;DR: 该研究提出了一种名为VisCo（Visual Contextual）的新型视觉中心越狱攻击方法，通过构建上下文对话和生成辅助图像，有效利用视觉信息诱导多模态大语言模型（MLLMs）产生有害响应。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）展现出巨大潜力，但其视觉模态存在安全漏洞。现有攻击方法中，视觉输入多作为触发器，缺乏语义明确性和现实场景的关联性。因此，需要一种视觉信息在构建完整、真实的越狱情境中扮演必要角色的攻击方式。

Method: 本研究定义了“视觉中心越狱”新范式，并提出了VisCo攻击。VisCo通过四种视觉聚焦策略构建上下文对话，必要时动态生成辅助图像以创建视觉中心越狱场景。为最大化攻击效果，VisCo还结合了自动毒性混淆和语义优化，生成能可靠触发黑盒MLLMs有害响应的攻击提示。

Result: 在MM-SafetyBench针对GPT-4o的测试中，VisCo攻击的毒性评分为4.78，攻击成功率（ASR）为85%，显著优于基线（毒性评分2.48，ASR 22.2%）。

Conclusion: VisCo攻击成功地展示了一种以视觉信息为核心的越狱策略，揭示了当视觉信息成为有害语境不可或缺的一部分时，MLLMs所面临的显著安全风险，并取得了高攻击成功率。

Abstract: With the emergence of strong visual-language capabilities, multimodal large
language models (MLLMs) have demonstrated tremendous potential for real-world
applications. However, the security vulnerabilities exhibited by the visual
modality pose significant challenges to deploying such models in open-world
environments. Recent studies have successfully induced harmful responses from
target MLLMs by encoding harmful textual semantics directly into visual inputs.
However, in these approaches, the visual modality primarily serves as a trigger
for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding
in realistic scenarios. In this work, we define a novel setting: visual-centric
jailbreak, where visual information serves as a necessary component in
constructing a complete and realistic jailbreak context. Building on this
setting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates
contextual dialogue using four distinct visual-focused strategies, dynamically
generating auxiliary images when necessary to construct a visual-centric
jailbreak scenario. To maximize attack effectiveness, it incorporates automatic
toxicity obfuscation and semantic refinement to produce a final attack prompt
that reliably triggers harmful responses from the target black-box MLLMs.
Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success
Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming
the baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The
code is available at https://github.com/Dtc7w3PQ/Visco-Attack.

</details>


### [73] [CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios](https://arxiv.org/abs/2507.02479)
*Teng Fu,Yuwen Chen,Zhuofan Chen,Mengyang Zhao,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 本文提出了一个大规模、难点多行人追踪数据集“CrowdTrack”，主要从第一人称视角拍摄真实复杂场景，旨在促进复杂环境下追踪算法的开发与评估。


<details>
  <summary>Details</summary>
Motivation: 现有行人追踪方法在复杂场景下（如遮挡、图像模糊）表现不佳，难以有效利用运动或外观信息。同时，现有MOT数据集场景过于简单且不真实，无法满足训练鲁棒算法的需求。

Method: 构建并提出了“CrowdTrack”数据集，包含33个视频，共5185条轨迹，主要从第一人称视角拍摄真实复杂场景。每个目标都标注了完整边界框和唯一ID。此外，对数据集进行了全面分析，并在其上测试了多个SOTA模型和基础模型的性能。

Result: 成功构建并发布了CrowdTrack数据集，该数据集是一个针对复杂多行人追踪的难点大规模资源。研究团队对数据集进行了详细分析，并展示了多个现有SOTA模型和基础模型在该数据集上的表现。

Conclusion: CrowdTrack数据集为在复杂情况下开发和评估有效的多行人追踪算法提供了重要平台和资源，有望推动该领域算法的进一步发展。

Abstract: Multi-object tracking is a classic field in computer vision. Among them,
pedestrian tracking has extremely high application value and has become the
most popular research category. Existing methods mainly use motion or
appearance information for tracking, which is often difficult in complex
scenarios. For the motion information, mutual occlusions between objects often
prevent updating of the motion state; for the appearance information,
non-robust results are often obtained due to reasons such as only partial
visibility of the object or blurred images. Although learning how to perform
tracking in these situations from the annotated data is the simplest solution,
the existing MOT dataset fails to satisfy this solution. Existing methods
mainly have two drawbacks: relatively simple scene composition and
non-realistic scenarios. Although some of the video sequences in existing
dataset do not have the above-mentioned drawbacks, the number is far from
adequate for research purposes. To this end, we propose a difficult large-scale
dataset for multi-pedestrian tracking, shot mainly from the first-person view
and all from real-life complex scenarios. We name it ``CrowdTrack'' because
there are numerous objects in most of the sequences. Our dataset consists of 33
videos, containing a total of 5,185 trajectories. Each object is annotated with
a complete bounding box and a unique object ID. The dataset will provide a
platform to facilitate the development of algorithms that remain effective in
complex situations. We analyzed the dataset comprehensively and tested multiple
SOTA models on our dataset. Besides, we analyzed the performance of the
foundation models on our dataset. The dataset and project code is released at:
https://github.com/loseevaya/CrowdTrack .

</details>


### [74] [MedFormer: Hierarchical Medical Vision Transformer with Content-Aware Dual Sparse Selection Attention](https://arxiv.org/abs/2507.02488)
*Zunhui Xia,Hongxing Li,Libin Lan*

Main category: cs.CV

TL;DR: 本文提出MedFormer，一个高效通用的医学视觉Transformer模型，通过金字塔结构和内容感知的双稀疏选择注意力，解决了现有方法在泛化性、计算效率和性能上的局限，并在多种医学图像识别任务中实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像识别中的视觉Transformer方法存在两个主要挑战：一是通用性差，往往针对特定任务和架构设计；二是计算效率与性能难以兼顾，全注意力计算成本高，而手工稀疏注意力可能导致性能下降。

Method: 本文提出了MedFormer，其核心包含两项创新：1) 采用金字塔缩放结构作为通用骨干网络，适用于图像分类、语义分割和病灶检测等多种医学图像识别任务，以实现分层特征并减少计算量。2) 引入内容感知的双稀疏选择注意力（DSSA），旨在提高计算效率、增强鲁棒性并保持高性能，专注于最相关内容。此外，还进行了详细的理论分析以证明其通用性和效率。

Result: 理论分析表明MedFormer相较于现有医学视觉Transformer具有更优的泛化性和效率。在多种成像模态数据集上进行的广泛实验一致证明，MedFormer在图像分类、语义分割和病灶检测三项任务中均有效提升了性能。

Conclusion: MedFormer通过其创新性的金字塔缩放结构和双稀疏选择注意力，成功解决了当前医学视觉Transformer在通用性、计算效率和性能方面的挑战，并在多项医学图像识别任务中展现出卓越的性能提升潜力。

Abstract: Medical image recognition serves as a key way to aid in clinical diagnosis,
enabling more accurate and timely identification of diseases and abnormalities.
Vision transformer-based approaches have proven effective in handling various
medical recognition tasks. However, these methods encounter two primary
challenges. First, they are often task-specific and architecture-tailored,
limiting their general applicability. Second, they usually either adopt full
attention to model long-range dependencies, resulting in high computational
costs, or rely on handcrafted sparse attention, potentially leading to
suboptimal performance. To tackle these issues, we present MedFormer, an
efficient medical vision transformer with two key ideas. First, it employs a
pyramid scaling structure as a versatile backbone for various medical image
recognition tasks, including image classification and dense prediction tasks
such as semantic segmentation and lesion detection. This structure facilitates
hierarchical feature representation while reducing the computation load of
feature maps, highly beneficial for boosting performance. Second, it introduces
a novel Dual Sparse Selection Attention (DSSA) with content awareness to
improve computational efficiency and robustness against noise while maintaining
high performance. As the core building technique of MedFormer, DSSA is
explicitly designed to attend to the most relevant content. In addition, a
detailed theoretical analysis has been conducted, demonstrating that MedFormer
has superior generality and efficiency in comparison to existing medical vision
transformers. Extensive experiments on a variety of imaging modality datasets
consistently show that MedFormer is highly effective in enhancing performance
across all three above-mentioned medical image recognition tasks. The code is
available at https://github.com/XiaZunhui/MedFormer.

</details>


### [75] [Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy](https://arxiv.org/abs/2507.02493)
*Luca Parolari,Andrea Cherubini,Lamberto Ballan,Carlo Biffi*

Main category: cs.CV

TL;DR: 本文提出一种结合时序感知的监督对比学习方法，用于结肠镜检查中的息肉自动计数，通过引入时序软目标和时序邻近约束，将碎片率降低了2.2倍，达到了新的技术水平。


<details>
  <summary>Details</summary>
Motivation: 现有的自动息肉计数方法主要依赖视觉外观且忽略了时序关系，导致息肉轨迹特征学习和聚类阶段的鲁棒性不足。

Method: 引入了一种包含时序感知软目标的监督对比损失，旨在捕获息肉内部变异性并保持息肉间区分度。同时，通过整合时序邻近约束改进了轨迹聚类，减少了误关联。

Result: 与现有方法相比，息肉碎片率降低了2.2倍，并在息肉计数领域确立了新的最先进水平。

Conclusion: 研究结果强调了时序感知在息肉计数中的重要性，通过有效利用时序信息显著提升了计数性能。

Abstract: Automated polyp counting in colonoscopy is a crucial step toward automated
procedure reporting and quality control, aiming to enhance the
cost-effectiveness of colonoscopy screening. Counting polyps in a procedure
involves detecting and tracking polyps, and then clustering tracklets that
belong to the same polyp entity. Existing methods for polyp counting rely on
self-supervised learning and primarily leverage visual appearance, neglecting
temporal relationships in both tracklet feature learning and clustering stages.
In this work, we introduce a paradigm shift by proposing a supervised
contrastive loss that incorporates temporally-aware soft targets. Our approach
captures intra-polyp variability while preserving inter-polyp discriminability,
leading to more robust clustering. Additionally, we improve tracklet clustering
by integrating a temporal adjacency constraint, reducing false positive
re-associations between visually similar but temporally distant tracklets. We
train and validate our method on publicly available datasets and evaluate its
performance with a leave-one-out cross-validation strategy. Results demonstrate
a 2.2x reduction in fragmentation rate compared to prior approaches. Our
results highlight the importance of temporal awareness in polyp counting,
establishing a new state-of-the-art. Code is available at
https://github.com/lparolari/temporally-aware-polyp-counting.

</details>


### [76] [MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations](https://arxiv.org/abs/2507.02494)
*Hyunsoo Son,Jeonghyun Noh,Suemin Jeon,Chaoli Wang,Won-Ki Jeong*

Main category: cs.CV

TL;DR: 本文提出MC-INR框架，结合元学习和聚类，用于处理非结构化网格上的多变量科学数据，旨在克服现有隐式神经表示方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有隐式神经表示(INRs)方法在处理复杂结构、多变量数据和非结构化网格时存在局限性，导致在真实世界数据集上性能下降。

Method: 提出MC-INR框架，结合元学习与聚类实现复杂结构编码；引入基于残差的动态再聚类机制，根据局部误差自适应划分集群；设计分支层以同时利用多变量数据。

Result: 实验结果表明，MC-INR在科学数据编码任务上优于现有方法。

Conclusion: MC-INR有效解决了现有INR方法在处理复杂、多变量、非结构化科学数据时的挑战，提高了数据编码性能。

Abstract: Implicit Neural Representations (INRs) are widely used to encode data as
continuous functions, enabling the visualization of large-scale multivariate
scientific simulation data with reduced memory usage. However, existing
INR-based methods face three main limitations: (1) inflexible representation of
complex structures, (2) primarily focusing on single-variable data, and (3)
dependence on structured grids. Thus, their performance degrades when applied
to complex real-world datasets. To address these limitations, we propose a
novel neural network-based framework, MC-INR, which handles multivariate data
on unstructured grids. It combines meta-learning and clustering to enable
flexible encoding of complex structures. To further improve performance, we
introduce a residual-based dynamic re-clustering mechanism that adaptively
partitions clusters based on local error. We also propose a branched layer to
leverage multivariate data through independent branches simultaneously.
Experimental results demonstrate that MC-INR outperforms existing methods on
scientific data encoding tasks.

</details>


### [77] [Automatic Labelling for Low-Light Pedestrian Detection](https://arxiv.org/abs/2507.02513)
*Dimitrios Bouzoulas,Eerik Alamikkotervo,Risto Ojala*

Main category: cs.CV

TL;DR: 针对RGB图像在低光照条件下行人检测缺乏数据集的问题，本研究提出了一种自动化红外-RGB标注流程。实验结果表明，使用该流程生成的标签训练的模型，在低光照行人检测任务中表现优于使用真实标签训练的模型。


<details>
  <summary>Details</summary>
Motivation: RGB图像中的行人检测是自动驾驶和高级驾驶辅助系统中的关键任务。然而，RGB行人检测在低光照条件下存在挑战，并且缺乏大型公共数据集。

Method: 本研究提出了一种自动化红外-RGB标注流程，包含三个主要步骤：1) 红外检测，使用经过微调的红外行人检测模型；2) 标签迁移，将红外检测结果的标签转移到对应的RGB图像上；3) 模型训练，使用生成的标签训练目标检测模型，以实现低光照RGB行人检测。研究基于KAIST数据集进行。

Result: 通过将使用生成自动标签和真实标签训练的目标检测模型进行比较，结果显示，在9种情况中有6种情况下，使用生成标签训练的模型在mAP@50和mAP@50-95指标上均优于使用真实标签训练的模型。

Conclusion: 本研究提出的自动化红外-RGB标注流程能够有效解决低光照RGB行人检测数据缺乏的问题，并且使用该方法生成的标签训练的模型在性能上表现出色，甚至超越了使用真实标签训练的模型。

Abstract: Pedestrian detection in RGB images is a key task in pedestrian safety, as the
most common sensor in autonomous vehicles and advanced driver assistance
systems is the RGB camera. A challenge in RGB pedestrian detection, that does
not appear to have large public datasets, is low-light conditions. As a
solution, in this research, we propose an automated infrared-RGB labeling
pipeline. The proposed pipeline consists of 1) Infrared detection, where a
fine-tuned model for infrared pedestrian detection is used 2) Label transfer
process from the infrared detections to their RGB counterparts 3) Training
object detection models using the generated labels for low-light RGB pedestrian
detection. The research was performed using the KAIST dataset. For the
evaluation, object detection models were trained on the generated autolabels
and ground truth labels. When compared on a previously unseen image sequence,
the results showed that the models trained on generated labels outperformed the
ones trained on ground-truth labels in 6 out of 9 cases for the mAP@50 and
mAP@50-95 metrics. The source code for this research is available at
https://github.com/BouzoulasDimitrios/IR-RGB-Automated-LowLight-Pedestrian-Labeling

</details>


### [78] [Detecting Multiple Diseases in Multiple Crops Using Deep Learning](https://arxiv.org/abs/2507.02517)
*Vivek Yadav,Anugrah Jain*

Main category: cs.CV

TL;DR: 本文提出一种基于深度学习的多作物多病害检测方案，旨在解决印度农业面临的挑战。该方案在包含17种作物和34种病害的统一数据集上实现了99%的检测准确率，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 印度作为农业主导经济体，面临严重的作物病害和环境压力导致的损失。为了提高产量和确保粮食安全，对不同作物病害进行早期和准确检测至关重要。

Method: 1. 构建了一个包含17种不同作物和34种不同病害图像的统一数据集。2. 在该数据集上训练了所提出的深度学习模型。

Result: 1. 所提出的深度学习模型在准确性和作物及病害覆盖范围方面均优于现有技术。2. 在统一数据集上实现了99%的检测准确率。3. 相比于仅涵盖14种作物和26种病害的现有技术，其准确率提高了7%。

Conclusion: 通过提高可检测作物种类和病害类型，所提出的解决方案旨在为印度农民提供更好的产品，从而有望改善农业生产和粮食安全。

Abstract: India, as a predominantly agrarian economy, faces significant challenges in
agriculture, including substantial crop losses caused by diseases, pests, and
environmental stress. Early detection and accurate identification of diseases
across different crops are critical for improving yield and ensuring food
security. This paper proposes a deep learning based solution for detecting
multiple diseases in multiple crops, aimed to cover India's diverse
agricultural landscape. We first create a unified dataset encompassing images
of 17 different crops and 34 different diseases from various available
repositories. Proposed deep learning model is trained on this dataset and
outperforms the state-of-the-art in terms of accuracy and the number of crops,
diseases covered. We achieve a significant detection accuracy, i.e., 99 percent
for our unified dataset which is 7 percent more when compared to
state-of-the-art handling 14 crops and 26 different diseases only. By improving
the number of crops and types of diseases that can be detected, proposed
solution aims to provide a better product for Indian farmers.

</details>


### [79] [IMASHRIMP: Automatic White Shrimp (Penaeus vannamei) Biometrical Analysis from Laboratory Images Using Computer Vision and Deep Learning](https://arxiv.org/abs/2507.02519)
*Abiam Remache González,Meriem Chagour,Timon Bijan Rüth,Raúl Trapiella Cañedo,Marina Martínez Soler,Álvaro Lorenzo Felipe,Hyun-Suk Shin,María-Jesús Zamorano Serrano,Ricardo Torres,Juan-Antonio Castillo Parra,Eduardo Reyes Abad,Miguel-Ángel Ferrer Ballester,Juan-Manuel Afonso López,Francisco-Mario Hernández Tejera,Adrian Penate-Sanchez*

Main category: cs.CV

TL;DR: 本文介绍并验证了IMASHRIMP系统，一个自动化白虾形态学分析系统，用于优化水产养殖中的基因选择任务。


<details>
  <summary>Details</summary>
Motivation: 优化白虾在水产养殖中的基因选择任务，需要一个高效、自动化的形态学分析系统来应对现有方法的挑战并减少人工错误。

Method: IMASHRIMP系统融合了多种计算机视觉和深度学习技术。它包含两个基于改进ResNet-50的判别模块，用于视角分类和虾枪完整性检测；一个“人机双重认证”系统以减少人工错误；一个基于VitPose改编的姿态估计模块，可预测23个关键点并为不同视角设计独立网络；以及一个使用支持向量机（SVM）的形态回归模块，将像素测量转换为厘米单位。

Result: IMASHRIMP系统成功地减少了人工错误：视角分类错误从0.97%降至0%，虾枪检测错误从12.46%降至3.64%。在姿态估计方面，系统取得了97.94%的平均精度（mAP），像素到厘米的转换误差为0.07 (± 0.1) 厘米。

Conclusion: IMASHRIMP系统展示了自动化和加速白虾形态学分析的巨大潜力，能够有效提升基因选择效率，并为更可持续的水产养殖实践做出贡献。

Abstract: This paper introduces IMASHRIMP, an adapted system for the automated
morphological analysis of white shrimp (Penaeus vannamei}, aimed at optimizing
genetic selection tasks in aquaculture. Existing deep learning and computer
vision techniques were modified to address the specific challenges of shrimp
morphology analysis from RGBD images. IMASHRIMP incorporates two discrimination
modules, based on a modified ResNet-50 architecture, to classify images by the
point of view and determine rostrum integrity. It is proposed a "two-factor
authentication (human and IA)" system, it reduces human error in view
classification from 0.97% to 0% and in rostrum detection from 12.46% to 3.64%.
Additionally, a pose estimation module was adapted from VitPose to predict 23
key points on the shrimp's skeleton, with separate networks for lateral and
dorsal views. A morphological regression module, using a Support Vector Machine
(SVM) model, was integrated to convert pixel measurements to centimeter units.
Experimental results show that the system effectively reduces human error,
achieving a mean average precision (mAP) of 97.94% for pose estimation and a
pixel-to-centimeter conversion error of 0.07 (+/- 0.1) cm. IMASHRIMP
demonstrates the potential to automate and accelerate shrimp morphological
analysis, enhancing the efficiency of genetic selection and contributing to
more sustainable aquaculture practices.The code are available at
https://github.com/AbiamRemacheGonzalez/ImaShrimp-public

</details>


### [80] [MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details](https://arxiv.org/abs/2507.02546)
*Ruicheng Wang,Sicheng Xu,Yue Dong,Yu Deng,Jianfeng Xiang,Zelong Lv,Guangzhong Sun,Xin Tong,Jiaolong Yang*

Main category: cs.CV

TL;DR: MoGe-2是一个先进的开放域几何估计模型，能从单张图像恢复度量尺度的3D点图，它在MoGe基础上增加了度量尺度预测和数据提炼技术，显著提高了几何细节和整体精度。


<details>
  <summary>Details</summary>
Motivation: 现有单目几何估计方法（如MoGe）只能预测仿射不变点图，缺乏度量尺度信息；同时，真实数据中的噪声和误差会损害预测几何的精细度。目前没有方法能同时实现准确的相对几何、精确的度量尺度和精细的细节恢复。

Method: 该方法在MoGe基础上，探索了有效策略以扩展其度量几何预测能力，同时保持相对几何精度。此外，开发了一种统一的数据提炼方法，利用清晰的合成标签过滤和补全真实数据，以提高细节 granularity。模型在大量混合数据集上进行训练。

Result: MoGe-2在实现准确的相对几何、精确的度量尺度和精细的细节恢复方面展现出卓越性能。其达到了前所未有的能力，能同时满足上述所有要求。

Conclusion: MoGe-2成功克服了单目几何估计的挑战，成为首个能同时提供高精度相对几何、精确度量尺度和精细细节恢复的开放域几何估计模型。

Abstract: We propose MoGe-2, an advanced open-domain geometry estimation model that
recovers a metric scale 3D point map of a scene from a single image. Our method
builds upon the recent monocular geometry estimation approach, MoGe, which
predicts affine-invariant point maps with unknown scales. We explore effective
strategies to extend MoGe for metric geometry prediction without compromising
the relative geometry accuracy provided by the affine-invariant point
representation. Additionally, we discover that noise and errors in real data
diminish fine-grained detail in the predicted geometry. We address this by
developing a unified data refinement approach that filters and completes real
data from different sources using sharp synthetic labels, significantly
enhancing the granularity of the reconstructed geometry while maintaining the
overall accuracy. We train our model on a large corpus of mixed datasets and
conducted comprehensive evaluations, demonstrating its superior performance in
achieving accurate relative geometry, precise metric scale, and fine-grained
detail recovery -- capabilities that no previous methods have simultaneously
achieved.

</details>


### [81] [Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning](https://arxiv.org/abs/2507.02565)
*Buzhen Huang,Chen Li,Chongyang Xu,Dongyue Lu,Jinnan Chen,Yangang Wang,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出一种双分支优化框架，利用人体外观、空间行为和物理规律，从野外视频中准确重建人际密切互动，并构建了新的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的人体姿态估计方法难以从野外视频中恢复可信的人际密切互动，原因是视觉模糊和人际遮挡，即使是先进的大型基础模型也无法准确区分人体语义。研究发现人体外观可为解决这些问题提供直接线索。

Method: 提出一个双分支优化框架。首先训练一个扩散模型以学习人类空间行为和姿态先验知识。然后，将训练好的网络和两个可优化张量整合到双分支优化框架中，以重建人体运动和外观。设计了基于3D高斯、2D关键点和网格穿透的多种约束来辅助优化。此外，还构建了一个带有伪真实互动标注的数据集。

Result: 该方法能够从复杂环境的野外视频中准确估计人际互动。在多个基准测试上的实验结果表明，本方法优于现有方法。

Conclusion: 本文提出的方法结合空间行为先验和多种约束，有效解决了野外视频中人际密切互动的准确重建问题，性能超越现有方法，并贡献了一个新的数据集以促进未来研究。

Abstract: Due to visual ambiguities and inter-person occlusions, existing human pose
estimation methods cannot recover plausible close interactions from in-the-wild
videos. Even state-of-the-art large foundation models~(\eg, SAM) cannot
accurately distinguish human semantics in such challenging scenarios. In this
work, we find that human appearance can provide a straightforward cue to
address these obstacles. Based on this observation, we propose a dual-branch
optimization framework to reconstruct accurate interactive motions with
plausible body contacts constrained by human appearances, social proxemics, and
physical laws. Specifically, we first train a diffusion model to learn the
human proxemic behavior and pose prior knowledge. The trained network and two
optimizable tensors are then incorporated into a dual-branch optimization
framework to reconstruct human motions and appearances. Several constraints
based on 3D Gaussians, 2D keypoints, and mesh penetrations are also designed to
assist the optimization. With the proxemics prior and diverse constraints, our
method is capable of estimating accurate interactions from in-the-wild videos
captured in complex environments. We further build a dataset with pseudo
ground-truth interaction annotations, which may promote future research on pose
estimation and human behavior understanding. Experimental results on several
benchmarks demonstrate that our method outperforms existing approaches. The
code and data are available at https://www.buzhenhuang.com/works/CloseApp.html.

</details>


### [82] [Parametric shape models for vessels learned from segmentations via differentiable voxelization](https://arxiv.org/abs/2507.02576)
*Alina F. Dima,Suprosanna Shit,Huaqi Qiu,Robbie Holland,Tamara T. Mueller,Fabio Antonio Musio,Kaiyuan Yang,Bjoern Menze,Rickmer Braren,Marcus Makowski,Daniel Rueckert*

Main category: cs.CV

TL;DR: 提出一个统一框架，利用可微分变换连接血管的体素、网格和参数化模型，实现从分割数据自动提取参数化模型并生成高精度网格。


<details>
  <summary>Details</summary>
Motivation: 血管的体素、网格和参数化模型等多种表示通常独立提取且分离使用，缺乏一个统一的框架来整合它们。

Method: 1. 构建一个通过可微分变换连接体素、网格和参数化三种血管表示的框架。2. 利用可微分体素化，通过“形状到分割”拟合从分割数据中自动提取血管的参数化形状模型，无需真实的形状参数。3. 血管通过中心线和半径使用三次B样条进行参数化，确保平滑性和连续性。4. 从学习到的形状参数中可微分地提取高保真网格。

Result: 该方法能够准确捕获复杂血管的几何形状。在主动脉、动脉瘤和脑血管的实验中，通过体积拟合验证了其有效性。

Conclusion: 本文提出的框架成功地通过可微分变换统一了血管的三种主要表示，并能从现有分割数据中自动提取精确的参数化模型和生成高质量、可操作的网格，对复杂血管几何的分析和应用具有重要价值。

Abstract: Vessels are complex structures in the body that have been studied extensively
in multiple representations. While voxelization is the most common of them,
meshes and parametric models are critical in various applications due to their
desirable properties. However, these representations are typically extracted
through segmentations and used disjointly from each other. We propose a
framework that joins the three representations under differentiable
transformations. By leveraging differentiable voxelization, we automatically
extract a parametric shape model of the vessels through shape-to-segmentation
fitting, where we learn shape parameters from segmentations without the
explicit need for ground-truth shape parameters. The vessel is parametrized as
centerlines and radii using cubic B-splines, ensuring smoothness and continuity
by construction. Meshes are differentiably extracted from the learned shape
parameters, resulting in high-fidelity meshes that can be manipulated post-fit.
Our method can accurately capture the geometry of complex vessels, as
demonstrated by the volumetric fits in experiments on aortas, aneurysms, and
brain vessels.

</details>


### [83] [Structure-aware Semantic Discrepancy and Consistency for 3D Medical Image Self-supervised Learning](https://arxiv.org/abs/2507.02581)
*Tan Pan,Zhaorui Tan,Kaiyu Guo,Dongli Xu,Weidi Xu,Chen Jiang,Xin Guo,Yuan Qi,Yuan Cheng*

Main category: cs.CV

TL;DR: 本文提出S^2DC框架，通过解决3D医学图像自监督学习中固定大小块忽略结构变异的问题，学习结构感知的表示，并在多项任务和数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D医学图像自监督学习（mSSL）方法使用固定大小的图像块，未能有效考虑解剖结构在位置、尺度和形态上的变异，这对于捕捉有意义的区分至关重要，限制了其在更广泛应用中的有效性。研究旨在学习结构感知的表示。

Method: 本文提出了S^2DC（Structure-aware Semantic Discrepancy and Consistency）mSSL框架。基于同一结构内块语义一致、不同结构块语义差异的假设，S^2DC分两步实现：1. 利用最优传输策略强制不同块的表示差异，以增强语义差异；2. 基于邻域相似度分布，在结构层面推进语义一致性。通过连接块级和结构级表示，实现结构感知表示学习。

Result: 所提出的方法在10个数据集、4项任务和3种模态上进行了全面评估，结果显示S^2DC持续优于现有最先进的3D医学图像自监督学习方法。

Conclusion: S^2DC框架成功地学习了结构感知的3D医学图像表示，有效解决了现有方法忽略结构变异的问题。其在多项评估中的卓越性能证明了该方法在医学分析领域的巨大潜力。

Abstract: 3D medical image self-supervised learning (mSSL) holds great promise for
medical analysis. Effectively supporting broader applications requires
considering anatomical structure variations in location, scale, and morphology,
which are crucial for capturing meaningful distinctions. However, previous mSSL
methods partition images with fixed-size patches, often ignoring the structure
variations. In this work, we introduce a novel perspective on 3D medical images
with the goal of learning structure-aware representations. We assume that
patches within the same structure share the same semantics (semantic
consistency) while those from different structures exhibit distinct semantics
(semantic discrepancy). Based on this assumption, we propose an mSSL framework
named $S^2DC$, achieving Structure-aware Semantic Discrepancy and Consistency
in two steps. First, $S^2DC$ enforces distinct representations for different
patches to increase semantic discrepancy by leveraging an optimal transport
strategy. Second, $S^2DC$ advances semantic consistency at the structural level
based on neighborhood similarity distribution. By bridging patch-level and
structure-level representations, $S^2DC$ achieves structure-aware
representations. Thoroughly evaluated across 10 datasets, 4 tasks, and 3
modalities, our proposed method consistently outperforms the state-of-the-art
methods in mSSL.

</details>


### [84] [AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding](https://arxiv.org/abs/2507.02591)
*Weili Xu,Enxin Song,Wenhao Chai,Xuexiang Wen,Tian Ye,Gaoang Wang*

Main category: cs.CV

TL;DR: AuroraLong通过使用线性RNN语言模型替代多模态大模型（MLLM）中的LLM组件，解决了长视频理解中Transformer模型计算复杂度和内存消耗过高的问题，以更低的计算成本实现了有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: Transformer-based大语言模型（LLMs）在长序列输入时，其所需的内存和计算量与输入序列长度呈二次方增长，导致长视频理解面临计算复杂度和高昂内存成本的巨大挑战。

Method: 提出AuroraLong模型，它用线性RNN语言模型替换MLLM中的LLM组件，使其能以固定大小的隐藏状态处理任意长度的输入序列。为进一步提高吞吐量和效率，该方法结合了视觉token合并技术，并通过按大小升序重新排列视觉token。

Result: AuroraLong（仅20亿参数，使用公开数据训练）在多个视频基准测试中，其性能与在私有数据集上训练的同等规模Transformer模型相当。

Conclusion: 高效的线性RNNs有潜力通过降低计算门槛，使长视频理解更加普及。这是首次在类LLaVA模型中使用基于线性RNN的LLM作为骨干进行开放式视频理解。

Abstract: The challenge of long video understanding lies in its high computational
complexity and prohibitive memory cost, since the memory and computation
required by transformer-based LLMs scale quadratically with input sequence
length. We propose AuroraLong to address this challenge by replacing the LLM
component in MLLMs with a linear RNN language model that handles input sequence
of arbitrary length with constant-size hidden states. To further increase
throughput and efficiency, we combine visual token merge with linear RNN models
by reordering the visual tokens by their sizes in ascending order. Despite
having only 2B parameters and being trained exclusively on public data,
AuroraLong achieves performance comparable to Transformer-based models of
similar size trained on private datasets across multiple video benchmarks. This
demonstrates the potential of efficient, linear RNNs to democratize long video
understanding by lowering its computational entry barrier. To our best
knowledge, we are the first to use a linear RNN based LLM backbone in a
LLaVA-like model for open-ended video understanding.

</details>


### [85] [Addressing Camera Sensors Faults in Vision-Based Navigation: Simulation and Dataset Development](https://arxiv.org/abs/2507.02602)
*Riccardo Gallon,Fabian Schiemenz,Alessandra Menicucci,Eberhard Gill*

Main category: cs.CV

TL;DR: 本研究旨在解决空间任务中基于视觉导航（VBN）算法的传感器故障检测问题，尤其是AI方法所需故障图像数据缺乏的挑战。通过建立模拟框架生成了包含各类故障的合成图像数据集，以支持AI故障检测算法的开发与测试。


<details>
  <summary>Details</summary>
Motivation: 空间任务中基于视觉导航（VBN）算法的可靠性和鲁棒性受到传感器故障的威胁。传统的故障检测方法存在局限性，而AI虽能提供强大解决方案，却受限于缺乏足够且具代表性的故障图像数据集。

Method: 研究针对星际探测任务场景，全面分析了VBN中相机传感器的潜在故障情况。系统阐述了故障的原因、对图像质量和导航算法性能的影响以及常用缓解策略。开发了一个模拟框架，用于在合成图像中系统且可控地重现故障条件，以生成故障注入图像。

Result: 成功创建了一个包含故障注入图像的数据集。该数据集被证明是训练和测试基于AI的故障检测算法的宝贵工具。

Conclusion: 本研究通过提供一个关键的故障注入图像数据集，有效解决了AI在空间任务VBN故障检测应用中面临的数据稀缺障碍，为未来AI故障检测算法的开发与可靠性提升奠定了基础。

Abstract: The increasing importance of Vision-Based Navigation (VBN) algorithms in
space missions raises numerous challenges in ensuring their reliability and
operational robustness. Sensor faults can lead to inaccurate outputs from
navigation algorithms or even complete data processing faults, potentially
compromising mission objectives. Artificial Intelligence (AI) offers a powerful
solution for detecting such faults, overcoming many of the limitations
associated with traditional fault detection methods. However, the primary
obstacle to the adoption of AI in this context is the lack of sufficient and
representative datasets containing faulty image data.
  This study addresses these challenges by focusing on an interplanetary
exploration mission scenario. A comprehensive analysis of potential fault cases
in camera sensors used within the VBN pipeline is presented. The causes and
effects of these faults are systematically characterized, including their
impact on image quality and navigation algorithm performance, as well as
commonly employed mitigation strategies. To support this analysis, a simulation
framework is introduced to recreate faulty conditions in synthetically
generated images, enabling a systematic and controlled reproduction of faulty
data. The resulting dataset of fault-injected images provides a valuable tool
for training and testing AI-based fault detection algorithms. The final link to
the dataset will be added after an embargo period. For peer-reviewers, this
private link is available.

</details>


### [86] [AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models](https://arxiv.org/abs/2507.02664)
*Ziyin Zhou,Yunpeng Luo,Yuanchen Wu,Ke Sun,Jiayi Ji,Ke Yan,Shouhong Ding,Xiaoshuai Sun,Yunsheng Wu,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出AIGI-Holmes模型和Holmes-Set数据集，用于检测AI生成图像，并提供可验证的解释和更强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: AI生成图像（AIGI）被滥用于传播虚假信息，威胁公共信息安全。现有AIGI检测技术缺乏人类可验证的解释性，且对最新生成技术的泛化能力不足。

Method: 构建大规模数据集Holmes-Set（包含解释性指导微调数据集Holmes-SFTSet和人类对齐偏好数据集Holmes-DPOSet）。引入Multi-Expert Jury高效数据标注方法。提出Holmes Pipeline三阶段训练框架（视觉专家预训练、监督微调、直接偏好优化），将多模态大语言模型（MLLMs）应用于AIGI检测。推理阶段采用协同解码策略，整合视觉专家感知和MLLMs语义推理，以增强泛化能力。最终模型为AIGI-Holmes。

Result: 在三个基准测试上进行了大量实验，验证了AIGI-Holmes的有效性。

Conclusion: AIGI-Holmes通过创新的数据集、标注方法和训练框架，成功解决了AI生成图像检测中解释性缺失和泛化能力不足的问题，提供了一个有效且可解释的检测方案。

Abstract: The rapid development of AI-generated content (AIGC) technology has led to
the misuse of highly realistic AI-generated images (AIGI) in spreading
misinformation, posing a threat to public information security. Although
existing AIGI detection techniques are generally effective, they face two
issues: 1) a lack of human-verifiable explanations, and 2) a lack of
generalization in the latest generation technology. To address these issues, we
introduce a large-scale and comprehensive dataset, Holmes-Set, which includes
the Holmes-SFTSet, an instruction-tuning dataset with explanations on whether
images are AI-generated, and the Holmes-DPOSet, a human-aligned preference
dataset. Our work introduces an efficient data annotation method called the
Multi-Expert Jury, enhancing data generation through structured MLLM
explanations and quality control via cross-model evaluation, expert defect
filtering, and human preference modification. In addition, we propose Holmes
Pipeline, a meticulously designed three-stage training framework comprising
visual expert pre-training, supervised fine-tuning, and direct preference
optimization. Holmes Pipeline adapts multimodal large language models (MLLMs)
for AIGI detection while generating human-verifiable and human-aligned
explanations, ultimately yielding our model AIGI-Holmes. During the inference
stage, we introduce a collaborative decoding strategy that integrates the model
perception of the visual expert with the semantic reasoning of MLLMs, further
enhancing the generalization capabilities. Extensive experiments on three
benchmarks validate the effectiveness of our AIGI-Holmes.

</details>


### [87] [Learning few-step posterior samplers by unfolding and distillation of diffusion models](https://arxiv.org/abs/2507.02686)
*Charlesquin Kemajou Mbakam,Jonathan Spence,Marcelo Pereyra*

Main category: cs.CV

TL;DR: 本文提出一种新颖框架，结合深度展开和模型蒸馏，将扩散模型（DM）图像先验转换为少步数的条件模型用于后验采样，实现了卓越的准确性和计算效率，并保持了灵活性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在贝叶斯计算成像中是强大的图像先验，但现有方法（如Plug-and-Play和专用条件DM）存在局限性，前者依赖近似且后者缺乏通用性。需要一种方法能在保持准确性和效率的同时，兼顾灵活性。

Method: 引入了一个新颖框架，该框架整合了深度展开和模型蒸馏技术。核心创新在于首次将马尔可夫链蒙特卡洛（MCMC）算法（具体是LATINO Langevin采样器）进行深度展开，以此将DM图像先验转化为一个少步数的条件模型，用于后验采样。

Result: 通过广泛的实验和与现有技术比较，所提出的展开和蒸馏采样器实现了卓越的准确性和计算效率。同时，它们在推理时仍能灵活适应前向模型的各种变化。

Conclusion: 该研究成功地将扩散模型转化为高效、准确且灵活的条件模型，用于贝叶斯计算成像的后验采样，有效解决了现有方法在效率、准确性和灵活性之间的权衡问题，并开创性地将深度展开应用于蒙特卡洛采样方案。

Abstract: Diffusion models (DMs) have emerged as powerful image priors in Bayesian
computational imaging. Two primary strategies have been proposed for leveraging
DMs in this context: Plug-and-Play methods, which are zero-shot and highly
flexible but rely on approximations; and specialized conditional DMs, which
achieve higher accuracy and faster inference for specific tasks through
supervised training. In this work, we introduce a novel framework that
integrates deep unfolding and model distillation to transform a DM image prior
into a few-step conditional model for posterior sampling. A central innovation
of our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm
- specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et
al., 2025) - representing the first known instance of deep unfolding applied to
a Monte Carlo sampling scheme. We demonstrate our proposed unfolded and
distilled samplers through extensive experiments and comparisons with the state
of the art, where they achieve excellent accuracy and computational efficiency,
while retaining the flexibility to adapt to variations in the forward model at
inference time.

</details>


### [88] [APT: Adaptive Personalized Training for Diffusion Models with Limited Data](https://arxiv.org/abs/2507.02687)
*JungWoo Chae,Jiyoon Kim,JaeWoong Choi,Kyungyul Kim,Sangheum Hwang*

Main category: cs.CV

TL;DR: 本文提出APT框架，通过自适应训练策略和内部表示正则化，有效解决扩散模型在有限数据下个性化微调时出现的过拟合、先验知识丢失及文本对齐退化问题，生成高质量多样图像。


<details>
  <summary>Details</summary>
Motivation: 在有限数据下个性化扩散模型面临显著挑战，包括过拟合、先验知识丢失和文本对齐退化。过拟合尤其会导致噪声预测分布偏移，破坏去噪轨迹，使模型失去语义连贯性，因此需要一种有效的方法来缓解这些问题。

Method: 提出自适应个性化训练（APT）框架，包含三个关键组件：1) 自适应训练调整，引入过拟合指标，根据过拟合程度在每个时间步应用自适应数据增强和损失加权；2) 表示稳定，正则化中间特征图的均值和方差，以防止噪声预测过度偏移；3) 注意力对齐以保留先验知识，对齐微调模型与预训练模型的交叉注意力图，以保持先验知识和语义连贯性。

Result: 通过广泛实验证明，APT能有效缓解过拟合，保留先验知识，并且在生成高质量、多样化的图像方面，用有限的参考数据优于现有方法。

Conclusion: APT是一个新颖有效的框架，能够成功解决使用有限数据个性化扩散模型所面临的过拟合和先验知识丢失等核心挑战，从而实现卓越的图像生成效果。

Abstract: Personalizing diffusion models using limited data presents significant
challenges, including overfitting, loss of prior knowledge, and degradation of
text alignment. Overfitting leads to shifts in the noise prediction
distribution, disrupting the denoising trajectory and causing the model to lose
semantic coherence. In this paper, we propose Adaptive Personalized Training
(APT), a novel framework that mitigates overfitting by employing adaptive
training strategies and regularizing the model's internal representations
during fine-tuning. APT consists of three key components: (1) Adaptive Training
Adjustment, which introduces an overfitting indicator to detect the degree of
overfitting at each time step bin and applies adaptive data augmentation and
adaptive loss weighting based on this indicator; (2)Representation
Stabilization, which regularizes the mean and variance of intermediate feature
maps to prevent excessive shifts in noise prediction; and (3) Attention
Alignment for Prior Knowledge Preservation, which aligns the cross-attention
maps of the fine-tuned model with those of the pretrained model to maintain
prior knowledge and semantic coherence. Through extensive experiments, we
demonstrate that APT effectively mitigates overfitting, preserves prior
knowledge, and outperforms existing methods in generating high-quality, diverse
images with limited reference data.

</details>


### [89] [CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation](https://arxiv.org/abs/2507.02691)
*Xiangyang Luo,Ye Zhu,Yunfei Liu,Lijian Lin,Cong Wan,Zijian Cai,Shao-Lun Huang,Yu Li*

Main category: cs.CV

TL;DR: CanonSwap通过解耦面部外观与运动信息，克服了现有视频换脸方法在动态属性保留上的不足，实现了高质量、高一致性的换脸效果，并引入了新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 视频换脸面临两大挑战：有效迁移源身份和精确保留目标面部动态属性（如头部姿态、表情、唇形同步等）。现有方法主要侧重于身份迁移质量，但在保持目标面部动态属性方面表现不佳，导致结果不一致。研究者将此问题归因于视频中面部外观和运动信息的固有耦合。

Method: 提出CanonSwap框架，通过解耦运动信息和外观信息来解决问题。具体而言，CanonSwap首先消除运动相关信息，在统一的规范空间内进行身份修改；随后，将换脸后的特征重新整合回原始视频空间，以确保保留目标面部的动态属性。为实现精确的身份迁移、减少伪影并增强真实感，设计了一个局部身份调制模块，该模块使用空间掩码自适应地整合源身份特征，将修改限制在面部区域。此外，引入了几种细粒度同步度量指标，以全面评估视频换脸方法的性能。

Result: 大量实验表明，CanonSwap在视觉质量、时间一致性和身份保留方面显著优于现有方法。

Conclusion: CanonSwap通过其创新的运动-外观解耦策略及局部身份调制模块，有效解决了视频换脸中身份迁移与动态属性保持的平衡难题，显著提升了换脸结果的视觉质量、时间一致性和身份保留度。

Abstract: Video face swapping aims to address two primary challenges: effectively
transferring the source identity to the target video and accurately preserving
the dynamic attributes of the target face, such as head poses, facial
expressions, lip-sync, \etc. Existing methods mainly focus on achieving
high-quality identity transfer but often fall short in maintaining the dynamic
attributes of the target face, leading to inconsistent results. We attribute
this issue to the inherent coupling of facial appearance and motion in videos.
To address this, we propose CanonSwap, a novel video face-swapping framework
that decouples motion information from appearance information. Specifically,
CanonSwap first eliminates motion-related information, enabling identity
modification within a unified canonical space. Subsequently, the swapped
feature is reintegrated into the original video space, ensuring the
preservation of the target face's dynamic attributes. To further achieve
precise identity transfer with minimal artifacts and enhanced realism, we
design a Partial Identity Modulation module that adaptively integrates source
identity features using a spatial mask to restrict modifications to facial
regions. Additionally, we introduce several fine-grained synchronization
metrics to comprehensively evaluate the performance of video face swapping
methods. Extensive experiments demonstrate that our method significantly
outperforms existing approaches in terms of visual quality, temporal
consistency, and identity preservation. Our project page are publicly available
at https://luoxyhappy.github.io/CanonSwap/.

</details>


### [90] [SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment](https://arxiv.org/abs/2507.02705)
*Qi Xu,Dongxu Wei,Lingzhe Zhao,Wenpu Li,Zhangchi Huang,Shunping Ji,Peidong Liu*

Main category: cs.CV

TL;DR: 本文提出SIU3R，这是首个无需2D-3D对齐的框架，能从无姿态图像同步实现3D理解和重建，并达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前同步3D理解和重建方法依赖2D到3D特征对齐，导致3D理解能力受限和潜在语义信息丢失。

Method: 提出SIU3R框架，通过像素对齐的3D表示连接重建与理解任务，并使用统一的可学习查询实现原生3D理解，无需2D模型对齐。此外，还设计了两个轻量级模块以促进两任务间的协同作用。

Result: 实验证明，SIU3R在3D重建、3D理解以及同步理解与重建任务上均达到了最先进的性能。

Conclusion: 所提出的无对齐框架及其互惠设计在同步3D理解和重建任务中表现出显著优势和有效性。

Abstract: Simultaneous understanding and 3D reconstruction plays an important role in
developing end-to-end embodied intelligent systems. To achieve this, recent
approaches resort to 2D-to-3D feature alignment paradigm, which leads to
limited 3D understanding capability and potential semantic information loss. In
light of this, we propose SIU3R, the first alignment-free framework for
generalizable simultaneous understanding and 3D reconstruction from unposed
images. Specifically, SIU3R bridges reconstruction and understanding tasks via
pixel-aligned 3D representation, and unifies multiple understanding tasks into
a set of unified learnable queries, enabling native 3D understanding without
the need of alignment with 2D models. To encourage collaboration between the
two tasks with shared representation, we further conduct in-depth analyses of
their mutual benefits, and propose two lightweight modules to facilitate their
interaction. Extensive experiments demonstrate that our method achieves
state-of-the-art performance not only on the individual tasks of 3D
reconstruction and understanding, but also on the task of simultaneous
understanding and 3D reconstruction, highlighting the advantages of our
alignment-free framework and the effectiveness of the mutual benefit designs.

</details>


### [91] [UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation](https://arxiv.org/abs/2507.02713)
*Qin Guo,Ailing Zeng,Dongxu Yue,Ceyuan Yang,Yang Cao,Hanzhong Guo,Fei Shen,Wei Liu,Xihui Liu,Dan Xu*

Main category: cs.CV

TL;DR: 提出UniMC框架和HAIG-2.9M数据集，旨在解决现有关键点引导文本到图像生成模型在处理非刚性多类对象（如人和动物）及多重遮挡时遇到的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有关键点引导的文本到图像扩散模型在控制人类以外的非刚性物体（如动物）生成方面存在困难，且难以仅基于关键点生成多个重叠的人和动物。这主要源于现有控制方法的局限性以及缺乏合适的训练数据集。

Method: 1. 设计了基于DiT的UniMC框架，将实例级和关键点级条件（包括类别、边界框、关键点坐标）整合为紧凑的token，克服了以往方法依赖骨骼图导致实例和类别区分困难的问题。
2. 构建了HAIG-2.9M数据集，这是一个大规模、高质量、多样化的关键点引导人与动物图像生成数据集，包含78.6万张图像和290万个实例，具有丰富的关键点、边界框和细粒度标注，并经过严格人工检查。

Result: 广泛的实验证明了HAIG-2.9M数据集的高质量以及UniMC框架的有效性，尤其在重度遮挡和多类别场景下表现出色。

Conclusion: UniMC框架与HAIG-2.9M数据集的结合，显著提升了关键点引导的文本到图像生成模型在处理多类别（特别是人与动物）和复杂遮挡场景时的控制能力和生成质量。

Abstract: Although significant advancements have been achieved in the progress of
keypoint-guided Text-to-Image diffusion models, existing mainstream
keypoint-guided models encounter challenges in controlling the generation of
more general non-rigid objects beyond humans (e.g., animals). Moreover, it is
difficult to generate multiple overlapping humans and animals based on keypoint
controls solely. These challenges arise from two main aspects: the inherent
limitations of existing controllable methods and the lack of suitable datasets.
First, we design a DiT-based framework, named UniMC, to explore unifying
controllable multi-class image generation. UniMC integrates instance- and
keypoint-level conditions into compact tokens, incorporating attributes such as
class, bounding box, and keypoint coordinates. This approach overcomes the
limitations of previous methods that struggled to distinguish instances and
classes due to their reliance on skeleton images as conditions. Second, we
propose HAIG-2.9M, a large-scale, high-quality, and diverse dataset designed
for keypoint-guided human and animal image generation. HAIG-2.9M includes 786K
images with 2.9M instances. This dataset features extensive annotations such as
keypoints, bounding boxes, and fine-grained captions for both humans and
animals, along with rigorous manual inspection to ensure annotation accuracy.
Extensive experiments demonstrate the high quality of HAIG-2.9M and the
effectiveness of UniMC, particularly in heavy occlusions and multi-class
scenarios.

</details>


### [92] [FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models](https://arxiv.org/abs/2507.02714)
*Yuxuan Wang,Tianwei Cao,Huayu Zhang,Zhongjiang He,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: 针对扩散模型生成人体图像局部细节不足问题，本文提出FairHuman，一种采用最小潜在延迟（MPD）准则的多目标微调方法，旨在公平提升全局和局部生成质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大型文本到图像模型（特别是扩散模型）在图像生成方面取得了显著进展，但在生成具有逼真细节（如面部或手部）的人体图像时仍面临挑战，原因在于训练过程中缺乏对局部区域的充分监督。

Method: 提出FairHuman，一种多目标微调方法。首先构建三个学习目标：一个基于默认扩散目标函数的全局目标，以及两个基于预标注位置先验的手部和面部局部目标。其次，基于最小潜在延迟（MPD）准则推导出最优参数更新策略，实现多目标问题的公平感知优化。

Result: 所提出的方法在生成具有挑战性的局部细节方面实现了显著改进，同时保持了整体图像质量。大量实验表明该方法在不同场景下均能有效提升人体图像生成性能。

Conclusion: FairHuman通过公平地优化全局和局部目标，有效解决了扩散模型生成人体图像局部细节不足的问题，显著提升了人体图像的生成质量。

Abstract: Image generation has achieved remarkable progress with the development of
large-scale text-to-image models, especially diffusion-based models. However,
generating human images with plausible details, such as faces or hands, remains
challenging due to insufficient supervision of local regions during training.
To address this issue, we propose FairHuman, a multi-objective fine-tuning
approach designed to enhance both global and local generation quality fairly.
Specifically, we first construct three learning objectives: a global objective
derived from the default diffusion objective function and two local objectives
for hands and faces based on pre-annotated positional priors. Subsequently, we
derive the optimal parameter updating strategy under the guidance of the
Minimum Potential Delay (MPD) criterion, thereby attaining fairness-ware
optimization for this multi-objective problem. Based on this, our proposed
method can achieve significant improvements in generating challenging local
details while maintaining overall quality. Extensive experiments showcase the
effectiveness of our method in improving the performance of human image
generation under different scenarios.

</details>


### [93] [Prompt learning with bounding box constraints for medical image segmentation](https://arxiv.org/abs/2507.02743)
*Mélanie Gaillochet,Mehrdad Noori,Sahar Dastani,Christian Desrosiers,Hervé Lombaert*

Main category: cs.CV

TL;DR: 本研究提出一种结合视觉基础模型与弱监督（边界框）学习的医学图像分割框架，通过自动化提示生成，在有限数据下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像的像素级标注耗时且成本高昂。尽管视觉基础模型展现了卓越的分割性能，但其现有的提示学习方法仍依赖于完整的标注掩膜。本研究旨在解决此问题，利用更易获得的边界框弱标注，减轻医学图像分割的标注负担。

Method: 提出一种新颖框架，将基础模型的表示能力与弱监督分割（仅边界框标注）相结合。该方法通过边界框自动生成基础模型的提示，并设计优化方案整合边界框约束与提示式基础模型生成的伪标签。

Result: 在多模态数据集上的广泛实验表明，所提出的弱监督方法在有限数据设置下实现了84.90%的平均Dice分数，性能超越了现有的全监督和弱监督方法。

Conclusion: 本研究成功提出了一种结合基础模型与弱监督学习的新范式，有效利用边界框标注，显著减轻了医学图像分割的标注负担，并在有限数据下展现出优于现有方法的性能。

Abstract: Pixel-wise annotations are notoriously labourious and costly to obtain in the
medical domain. To mitigate this burden, weakly supervised approaches based on
bounding box annotations-much easier to acquire-offer a practical alternative.
Vision foundation models have recently shown noteworthy segmentation
performance when provided with prompts such as points or bounding boxes. Prompt
learning exploits these models by adapting them to downstream tasks and
automating segmentation, thereby reducing user intervention. However, existing
prompt learning approaches depend on fully annotated segmentation masks. This
paper proposes a novel framework that combines the representational power of
foundation models with the annotation efficiency of weakly supervised
segmentation. More specifically, our approach automates prompt generation for
foundation models using only bounding box annotations. Our proposed
optimization scheme integrates multiple constraints derived from box
annotations with pseudo-labels generated by the prompted foundation model.
Extensive experiments across multimodal datasets reveal that our weakly
supervised method achieves an average Dice score of 84.90% in a limited data
setting, outperforming existing fully-supervised and weakly-supervised
approaches. The code is available at
https://github.com/Minimel/box-prompt-learning-VFM.git

</details>


### [94] [DexVLG: Dexterous Vision-Language-Grasp Model at Scale](https://arxiv.org/abs/2507.02747)
*Jiawei He,Danshi Li,Xinqiang Yu,Zekun Qi,Wenyao Zhang,Jiayi Chen,Zhaoxiang Zhang,Zhizheng Zhang,Li Yi,He Wang*

Main category: cs.CV

TL;DR: 该研究提出DexVLG模型，一个大型视觉-语言-抓取模型，用于基于语言指令的灵巧手抓取姿态预测，并引入大规模数据集DexGraspNet 3.0进行训练和评估。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉-语言-动作（VLA）系统在机器人应用中，主要集中于简单的末端执行器（如夹持器），而针对类人灵巧手的功能性抓取研究甚少。数据收集的困难限制了灵巧手与大型模型结合的进展。

Method: 1. **数据生成**：创建了DexGraspNet 3.0，一个包含1.7亿个灵巧抓取姿态（映射到17.4万个对象的语义部件）的大规模模拟数据集，并配有详细的部件级描述。 2. **模型训练**：使用DexGraspNet 3.0数据集训练一个结合VLM（视觉-语言模型）和基于流匹配（flow-matching）的姿态预测头部（pose head）的DexVLG模型，使其能够根据单视角RGBD输入和语言指令预测桌面物体的抓取姿态。 3. **评估**：在物理仿真和真实世界中创建基准测试和实验来评估DexVLG的性能。

Result: DexVLG展现出强大的零样本泛化能力：在仿真中达到了超过76%的零样本执行成功率和最先进的部件抓取准确性。在真实世界场景中，也能成功实现与部件对齐的抓取。

Conclusion: DexVLG模型有效解决了灵巧手功能性抓取与语言指令对齐的挑战，通过大规模数据集和VLM技术，显著提升了机器人执行复杂、指令驱动抓取任务的能力，为实现更智能的机器人操作迈出了重要一步。

Abstract: As large models gain traction, vision-language-action (VLA) systems are
enabling robots to tackle increasingly complex tasks. However, limited by the
difficulty of data collection, progress has mainly focused on controlling
simple gripper end-effectors. There is little research on functional grasping
with large models for human-like dexterous hands. In this paper, we introduce
DexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction
aligned with language instructions using single-view RGBD input. To accomplish
this, we generate a dataset of 170 million dexterous grasp poses mapped to
semantic parts across 174,000 objects in simulation, paired with detailed
part-level captions. This large-scale dataset, named DexGraspNet 3.0, is used
to train a VLM and flow-matching-based pose head capable of producing
instruction-aligned grasp poses for tabletop objects. To assess DexVLG's
performance, we create benchmarks in physics-based simulations and conduct
real-world experiments. Extensive testing demonstrates DexVLG's strong
zero-shot generalization capabilities-achieving over 76% zero-shot execution
success rate and state-of-the-art part-grasp accuracy in simulation-and
successful part-aligned grasps on physical objects in real-world scenarios.

</details>


### [95] [Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics](https://arxiv.org/abs/2507.02748)
*Alex Colagrande,Paul Caillon,Eva Feillet,Alexandre Allauzen*

Main category: cs.CV

TL;DR: 本文提出了一种名为MANO（Multipole Attention Neural Operator）的新型Transformer变体，它借鉴了n体模拟，通过距离多尺度注意力机制解决了标准Transformer在处理高分辨率输入时二次复杂度高的问题，实现了线性的时间和内存复杂度，并在图像分类和Darcy流任务上表现出与现有SOTA模型相当的性能，同时显著降低了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的二次复杂度使其在处理高分辨率输入时效率低下且不切实际。现有解决方案（如分块、降采样）常以牺牲精细尺度细节为代价。

Method: 将注意力机制视为网格点间的相互作用问题，灵感来源于n体数值模拟。引入MANO，通过基于距离的多尺度方式计算注意力，在每个注意力头中保持全局感受野，实现了线性时间与内存复杂度。

Result: 在图像分类和Darcy流任务上，MANO的经验结果表明其性能可与ViT和Swin Transformer等现有先进模型媲美，同时将运行时间与峰值内存使用量降低了数个数量级。

Conclusion: MANO提供了一种高效且有竞争力的方法来解决Transformer处理高分辨率输入时的复杂度瓶颈，通过创新的注意力机制设计，实现了性能与资源效率的双重提升。

Abstract: Transformers have become the de facto standard for a wide range of tasks,
from image classification to physics simulations. Despite their impressive
performance, the quadratic complexity of standard Transformers in both memory
and time with respect to the input length makes them impractical for processing
high-resolution inputs. Therefore, several variants have been proposed, the
most successful relying on patchification, downsampling, or coarsening
techniques, often at the cost of losing the finest-scale details. In this work,
we take a different approach. Inspired by state-of-the-art techniques in
$n$-body numerical simulations, we cast attention as an interaction problem
between grid points. We introduce the Multipole Attention Neural Operator
(MANO), which computes attention in a distance-based multiscale fashion. MANO
maintains, in each attention head, a global receptive field and achieves linear
time and memory complexity with respect to the number of grid points. Empirical
results on image classification and Darcy flows demonstrate that MANO rivals
state-of-the-art models such as ViT and Swin Transformer, while reducing
runtime and peak memory usage by orders of magnitude. We open source our code
for reproducibility at https://github.com/AlexColagrande/MANO.

</details>


### [96] [Partial Weakly-Supervised Oriented Object Detection](https://arxiv.org/abs/2507.02751)
*Mingxin Liu,Peiyuan Zhang,Yuan Liu,Wei Zhang,Yue Zhou,Ning Liao,Ziyang Gong,Junwei Luo,Zhirui Wang,Yi Yu,Xue Yang*

Main category: cs.CV

TL;DR: 针对定向目标检测(OOD)高昂的标注成本，本文提出首个部分弱监督OOD框架PWOOD，该框架基于部分弱标注（如水平框或单点）高效利用未标注数据，并在多个标准数据集上取得了与传统半监督算法相当或更优的性能。


<details>
  <summary>Details</summary>
Motivation: 定向目标检测(OOD)需求广泛，但其数据集标注成本高昂是现有全监督、半监督和弱监督算法普遍面临的主要问题，限制了模型的训练效率和标注成本。

Method: 本文提出Partial Weakly-Supervised Oriented Object Detection (PWOOD)框架，它基于部分弱标注（水平框或单点）高效利用未标注数据。PWOOD框架包含：1) Orientation-and-Scale-aware Student (OS-Student)模型，能仅用少量弱标注学习方向和尺度信息；2) Class-Agnostic Pseudo-Label Filtering (CPF)策略，用于降低模型对静态过滤阈值的敏感性。

Result: 在DOTA-v1.0/v1.5/v2.0和DIOR数据集上的实验结果表明，PWOOD框架的性能与传统半监督算法相当，甚至有所超越，并且显著优于使用部分弱标注训练的弱监督算法。

Conclusion: PWOOD框架为定向目标检测提供了一种低成本的解决方案，通过有效利用部分弱标注和未标注数据，在保持高检测性能的同时，大幅降低了标注开销，有望推动OOD技术的实际应用。

Abstract: The growing demand for oriented object detection (OOD) across various domains
has driven significant research in this area. However, the high cost of dataset
annotation remains a major concern. Current mainstream OOD algorithms can be
mainly categorized into three types: (1) fully supervised methods using
complete oriented bounding box (OBB) annotations, (2) semi-supervised methods
using partial OBB annotations, and (3) weakly supervised methods using weak
annotations such as horizontal boxes or points. However, these algorithms
inevitably increase the cost of models in terms of annotation speed or
annotation cost. To address this issue, we propose:(1) the first Partial
Weakly-Supervised Oriented Object Detection (PWOOD) framework based on
partially weak annotations (horizontal boxes or single points), which can
efficiently leverage large amounts of unlabeled data, significantly
outperforming weakly supervised algorithms trained with partially weak
annotations, also offers a lower cost solution; (2) Orientation-and-Scale-aware
Student (OS-Student) model capable of learning orientation and scale
information with only a small amount of orientation-agnostic or scale-agnostic
weak annotations; and (3) Class-Agnostic Pseudo-Label Filtering strategy (CPF)
to reduce the model's sensitivity to static filtering thresholds. Comprehensive
experiments on DOTA-v1.0/v1.5/v2.0 and DIOR datasets demonstrate that our PWOOD
framework performs comparably to, or even surpasses, traditional
semi-supervised algorithms.

</details>


### [97] [From Pixels to Damage Severity: Estimating Earthquake Impacts Using Semantic Segmentation of Social Media Images](https://arxiv.org/abs/2507.02781)
*Danrong Zhang,Huili Huang,N. Simrill Smith,Nimisha Roy,J. David Frost*

Main category: cs.CV

TL;DR: 本研究提出一种新颖方法，将地震灾情严重性评估转化为语义分割问题，利用SegFormer模型和自定义评分系统，对社交媒体图像中的地震损害进行客观、量化的分析，以辅助灾后侦察和响应。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过分类评估地震灾情图像存在主观性且无法捕捉图像内不同程度的损害。因此，需要一种更客观、全面评估地震受损区域损害程度的方法。

Method: 该研究将损害严重性评估框定为语义分割问题。方法包括构建一个包含无损、受损和废墟三类损害程度的分割数据集，并使用此数据集微调一个SegFormer模型以生成损害严重性分割图。此外，引入一个新的损害严重性评分系统，该系统通过考虑图像中不同区域的损害程度并结合深度估计进行量化。

Result: 该方法能够以更客观和全面的方式量化社交媒体图像中的损害严重性。

Conclusion: 这项研究通过提供对损害的细致理解，增强了为灾害侦察团队提供精确指导的能力，从而促进了地震后更有效和有针对性的响应工作。

Abstract: In the aftermath of earthquakes, social media images have become a crucial
resource for disaster reconnaissance, providing immediate insights into the
extent of damage. Traditional approaches to damage severity assessment in
post-earthquake social media images often rely on classification methods, which
are inherently subjective and incapable of accounting for the varying extents
of damage within an image. Addressing these limitations, this study proposes a
novel approach by framing damage severity assessment as a semantic segmentation
problem, aiming for a more objective analysis of damage in earthquake-affected
areas. The methodology involves the construction of a segmented damage severity
dataset, categorizing damage into three degrees: undamaged structures, damaged
structures, and debris. Utilizing this dataset, the study fine-tunes a
SegFormer model to generate damage severity segmentations for post-earthquake
social media images. Furthermore, a new damage severity scoring system is
introduced, quantifying damage by considering the varying degrees of damage
across different areas within images, adjusted for depth estimation. The
application of this approach allows for the quantification of damage severity
in social media images in a more objective and comprehensive manner. By
providing a nuanced understanding of damage, this study enhances the ability to
offer precise guidance to disaster reconnaissance teams, facilitating more
effective and targeted response efforts in the aftermath of earthquakes.

</details>


### [98] [RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation](https://arxiv.org/abs/2507.02792)
*Liheng Zhang,Lexi Pang,Hang Ye,Xiaoxuan Ma,Yizhou Wang*

Main category: cs.CV

TL;DR: 该论文提出一种新的灵活特征注入框架，通过解耦注入时间步、引入结构丰富注入模块、外观丰富提示和重启精炼策略，解决了现有T2I条件图像生成中结构错位等问题，实现了无训练且兼具结构和外观丰富性的图像生成，并达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像（T2I）扩散模型的特征注入方法在结合条件图像（如深度图）时，尤其当条件图像与自然RGB分布差异较大时，常出现结构错位、条件泄露和视觉伪影。其核心限制在于同步注入条件特征未能平衡去噪过程中的域对齐和结构保留。

Method: 提出一种灵活的特征注入框架，将注入时间步与去噪过程解耦。核心是一个“结构丰富”的注入模块，使模型能更好地适应扩散过程中对齐与结构保留的动态平衡。此外，还引入了“外观丰富”的提示（appearance-rich prompting）和“重启精炼”策略（restart refinement strategy），以进一步增强外观控制和视觉质量。

Result: 通过广泛实验证明，该方法能够在不进行训练的情况下，生成同时兼具丰富结构和丰富外观的图像，并在多样化的零样本条件场景下达到了最先进的性能。

Conclusion: 该研究通过解耦注入时间步并引入创新模块和策略，有效解决了现有特征注入方法的局限性，实现了高质量、无训练且结构外观兼备的条件图像生成，显著提升了T2I模型的空间控制能力。

Abstract: Text-to-image (T2I) diffusion models have shown remarkable success in
generating high-quality images from text prompts. Recent efforts extend these
models to incorporate conditional images (e.g., depth or pose maps) for
fine-grained spatial control. Among them, feature injection methods have
emerged as a training-free alternative to traditional fine-tuning approaches.
However, they often suffer from structural misalignment, condition leakage, and
visual artifacts, especially when the condition image diverges significantly
from natural RGB distributions. By revisiting existing methods, we identify a
core limitation: the synchronous injection of condition features fails to
account for the trade-off between domain alignment and structural preservation
during denoising. Inspired by this observation, we propose a flexible feature
injection framework that decouples the injection timestep from the denoising
process. At its core is a structure-rich injection module, which enables the
model to better adapt to the evolving interplay between alignment and structure
preservation throughout the diffusion steps, resulting in more faithful
structural generation. In addition, we introduce appearance-rich prompting and
a restart refinement strategy to further enhance appearance control and visual
quality. Together, these designs enable training-free generation that is both
structure-rich and appearance-rich. Extensive experiments show that our
approach achieves state-of-the-art performance across diverse zero-shot
conditioning scenarios.

</details>


### [99] [No time to train! Training-Free Reference-Based Instance Segmentation](https://arxiv.org/abs/2507.02798)
*Miguel Espinosa,Chenhongyi Yang,Linus Ericsson,Steven McDonagh,Elliot J. Crowley*

Main category: cs.CV

TL;DR: 本文提出一种基于少量参考图像的无训练方法，利用基础模型学习到的语义先验来替代SAM模型中的手动或复杂提示，以实现目标分割，并在多个少样本分割基准测试中达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 图像分割模型受限于大规模标注数据的昂贵成本。尽管SAM模型通过可提示范式缓解了这一问题，但仍需要手动视觉提示或复杂的域相关提示生成规则。本研究旨在减轻这一新负担，探索仅提供少量参考图像的情况下进行目标分割。

Method: 利用基础模型学习到的强大语义先验，识别参考图像和目标图像之间的对应区域，从而自动生成实例级分割掩码。具体采用多阶段、无训练方法，包括：1) 记忆库构建；2) 表示聚合；3) 语义感知特征匹配。

Result: 实验显示分割指标显著提升，在COCO FSOD上达到36.8% nAP，PASCAL VOC Few-Shot上达到71.2% nAP50，并在Cross-Domain FSOD基准测试中以22.4% nAP优于现有无训练方法，达到了最先进的性能。

Conclusion: 该研究提出的无训练方法，通过利用基础模型的语义先验和少量参考图像，有效解决了SAM模型中提示生成的负担，并在少样本目标分割任务中取得了显著的性能提升。

Abstract: The performance of image segmentation models has historically been
constrained by the high cost of collecting large-scale annotated data. The
Segment Anything Model (SAM) alleviates this original problem through a
promptable, semantics-agnostic, segmentation paradigm and yet still requires
manual visual-prompts or complex domain-dependent prompt-generation rules to
process a new image. Towards reducing this new burden, our work investigates
the task of object segmentation when provided with, alternatively, only a small
set of reference images. Our key insight is to leverage strong semantic priors,
as learned by foundation models, to identify corresponding regions between a
reference and a target image. We find that correspondences enable automatic
generation of instance-level segmentation masks for downstream tasks and
instantiate our ideas via a multi-stage, training-free method incorporating (1)
memory bank construction; (2) representation aggregation and (3) semantic-aware
feature matching. Our experiments show significant improvements on segmentation
metrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP),
PASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free
approaches on the Cross-Domain FSOD benchmark (22.4% nAP).

</details>


### [100] [HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars](https://arxiv.org/abs/2507.02803)
*Gent Serifi,Marcel C. Bühler*

Main category: cs.CV

TL;DR: 本文提出HyperGaussians，一种将3D高斯扩展到高维的新方法，并通过“逆协方差技巧”提升效率，用于创建高质量可动画人脸头像，在复杂细节和动态表现上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting（3DGS）在静态人脸渲染表现出色，但在处理单目视频生成可动画人脸头像时，面临非线性形变、复杂光照和精细细节的挑战，常导致“恐怖谷”现象。多数工作侧重改进高斯参数预测，而本文旨在提升3D高斯表示本身的表现力以解决此问题。

Method: 引入“HyperGaussians”，将3D高斯扩展为高维多元高斯，通过可学习的局部嵌入增加其表现力。为克服高维协方差矩阵求逆的计算昂贵问题，提出“逆协方差技巧”对协方差矩阵进行重参数化以提升效率。将HyperGaussians集成到现有的单目人脸头像模型FlashAvatar中进行验证。

Result: 在19个主体和4个人脸数据集上的评估显示，HyperGaussians在数值和视觉上均超越3DGS，尤其在眼镜框、牙齿等高频细节、复杂面部动作和镜面反射等方面的表现显著提升。

Conclusion: HyperGaussians通过重新思考并提升3D高斯表示本身的表现力，成功解决了可动画人脸头像在处理精细细节和复杂动态方面的不足，显著提高了渲染质量。

Abstract: We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian primitives. 3DGS excels at rendering static
faces, but the state-of-the-art still struggles with nonlinear deformations,
complex lighting effects, and fine details. While most related works focus on
predicting better Gaussian parameters from expression codes, we rethink the 3D
Gaussian representation itself and how to make it more expressive. Our insights
lead to a novel extension of 3D Gaussians to high-dimensional multivariate
Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases
expressivity through conditioning on a learnable local embedding. However,
splatting HyperGaussians is computationally expensive because it requires
inverting a high-dimensional covariance matrix. We solve this by
reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.
This trick boosts the efficiency so that HyperGaussians can be seamlessly
integrated into existing models. To demonstrate this, we plug in HyperGaussians
into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our
evaluation on 19 subjects from 4 face datasets shows that HyperGaussians
outperform 3DGS numerically and visually, particularly for high-frequency
details like eyeglass frames, teeth, complex facial movements, and specular
reflections.

</details>


### [101] [LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion](https://arxiv.org/abs/2507.02813)
*Fangfu Liu,Hao Li,Jiawei Chi,Hanyang Wang,Minghui Yang,Fudong Wang,Yueqi Duan*

Main category: cs.CV

TL;DR: 本文提出一个名为LangScene-X的新型生成框架，旨在统一并生成3D一致的多模态信息，从而仅通过稀疏视图即可实现可泛化的3D语言嵌入场景的重建与理解。


<details>
  <summary>Details</summary>
Motivation: 从2D图像恢复3D结构并进行开放词汇场景理解是一项基础但艰巨的任务。现有方法严重依赖于校准的密集视图重建范式，导致在视图有限时出现严重的渲染伪影和不合理的语义合成问题。

Method: LangScene-X框架通过生成式能力来创建更一致的新颖观测，从而从稀疏视图构建可泛化的3D语言嵌入场景。具体方法包括：1) 训练一个TriMap视频扩散模型，通过渐进式知识集成从稀疏输入生成外观（RGB）、几何（法线）和语义（分割图）。2) 提出一个语言量化压缩器（LQC），在大规模图像数据集上训练，以高效编码语言嵌入，实现跨场景泛化而无需针对每个场景重新训练。3) 通过将语言信息对齐到3D场景表面，重建语言表面场，从而实现开放式语言查询。

Result: 在真实世界数据上进行的广泛实验表明，LangScene-X在质量和泛化能力方面均优于现有最先进的方法。

Conclusion: LangScene-X成功地统一并生成了3D一致的多模态信息，能够从稀疏视图中构建通用且具有语言嵌入能力的3D场景，并在重建和理解方面展现出卓越的性能和泛化能力。

Abstract: Recovering 3D structures with open-vocabulary scene understanding from 2D
images is a fundamental but daunting task. Recent developments have achieved
this by performing per-scene optimization with embedded language information.
However, they heavily rely on the calibrated dense-view reconstruction
paradigm, thereby suffering from severe rendering artifacts and implausible
semantic synthesis when limited views are available. In this paper, we
introduce a novel generative framework, coined LangScene-X, to unify and
generate 3D consistent multi-modality information for reconstruction and
understanding. Powered by the generative capability of creating more consistent
novel observations, we can build generalizable 3D language-embedded scenes from
only sparse views. Specifically, we first train a TriMap video diffusion model
that can generate appearance (RGBs), geometry (normals), and semantics
(segmentation maps) from sparse inputs through progressive knowledge
integration. Furthermore, we propose a Language Quantized Compressor (LQC),
trained on large-scale image datasets, to efficiently encode language
embeddings, enabling cross-scene generalization without per-scene retraining.
Finally, we reconstruct the language surface fields by aligning language
information onto the surface of 3D scenes, enabling open-ended language
queries. Extensive experiments on real-world data demonstrate the superiority
of our LangScene-X over state-of-the-art methods in terms of quality and
generalizability. Project Page: https://liuff19.github.io/LangScene-X.

</details>


### [102] [Confidence-driven Gradient Modulation for Multimodal Human Activity Recognition: A Dynamic Contrastive Dual-Path Learning Approach](https://arxiv.org/abs/2507.02826)
*Panpan Ji,Junni Song,Hang Xiao,Hanyu Liu,Chao Li*

Main category: cs.CV

TL;DR: 提出DCDP-HAR框架，通过双路径特征提取、多阶段对比学习和置信度驱动梯度调制，解决多模态HAR中跨模态特征对齐和模态贡献不平衡的挑战。


<details>
  <summary>Details</summary>
Motivation: 多模态人体活动识别（HAR）系统面临两大关键挑战：跨模态特征对齐困难和模态贡献不平衡。

Method: 本文提出动态对比双路径网络（DCDP-HAR）框架，包含三个核心组件：1) 采用ResNet和DenseNet协同处理多模态传感数据的双路径特征提取架构；2) 引入多阶段对比学习机制，实现从局部感知到语义抽象的渐进式对齐；3) 设计置信度驱动的梯度调制策略，动态调整各模态分支学习强度以缓解模态竞争。此外，还采用了基于动量的梯度累积策略以增强训练稳定性。

Result: 通过消融研究验证了DCDP-HAR各组件的有效性，并在四个公开基准数据集上进行了广泛的比较实验，证明了其性能。

Conclusion: DCDP-HAR框架通过其创新的组件和策略，有效解决了多模态HAR系统在特征对齐和模态贡献不平衡方面的挑战，提升了系统性能。

Abstract: Sensor-based Human Activity Recognition (HAR) is a core technology that
enables intelligent systems to perceive and interact with their environment.
However, multimodal HAR systems still encounter key challenges, such as
difficulties in cross-modal feature alignment and imbalanced modality
contributions. To address these issues, we propose a novel framework called the
Dynamic Contrastive Dual-Path Network (DCDP-HAR). The framework comprises three
key components. First, a dual-path feature extraction architecture is employed,
where ResNet and DenseNet branches collaboratively process multimodal sensor
data. Second, a multi-stage contrastive learning mechanism is introduced to
achieve progressive alignment from local perception to semantic abstraction.
Third, we present a confidence-driven gradient modulation strategy that
dynamically monitors and adjusts the learning intensity of each modality branch
during backpropagation, effectively alleviating modality competition. In
addition, a momentum-based gradient accumulation strategy is adopted to enhance
training stability. We conduct ablation studies to validate the effectiveness
of each component and perform extensive comparative experiments on four public
benchmark datasets.

</details>


### [103] [USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network](https://arxiv.org/abs/2507.02827)
*Ying Yu,Hang Xiao,Siyao Li,Jiarui Li,Haotian Tang,Hanyu Liu,Chao Li*

Main category: cs.CV

TL;DR: 本文提出USAD（无监督数据增强时空注意力扩散网络），通过无监督数据增强和多注意力时空交互机制，解决了人体活动识别（HAR）中数据稀缺、特征提取不足和轻量级设备性能差等问题，并实现了优异的性能和部署效率。


<details>
  <summary>Details</summary>
Motivation: 当前人体活动识别（HAR）面临的主要挑战包括：稀有活动标注样本稀缺、高层特征提取不足以及模型在轻量级设备上表现不佳。

Method: 本研究提出一种基于多注意力交互机制的综合优化方法：
1. 采用无监督、统计引导的扩散模型进行数据增强，以缓解标注数据稀缺和类别不平衡问题。
2. 设计多分支时空交互网络，通过3*3、5*5和7*7卷积核的并行残差分支捕获多尺度特征，并结合时间注意力机制识别关键时间点、空间注意力增强传感器间交互，同时引入跨分支特征融合单元提升特征表示能力。
3. 整合自适应多损失函数融合策略，动态调整损失权重以优化整体模型。

Result: 实验结果表明，所提出的USAD模型在WISDM、PAMAP2和OPPORTUNITY三个公共数据集上分别取得了98.84%、93.81%和80.92%的准确率，显著优于现有方法。此外，在嵌入式设备上的实际部署验证了该方法的效率和可行性。

Conclusion: 该研究提出的USAD方法有效解决了人体活动识别领域中的关键挑战，通过创新的无监督数据增强和多注意力时空网络设计，显著提升了模型性能，并证明了其在轻量级设备上实际部署的效率和可行性。

Abstract: The primary objective of human activity recognition (HAR) is to infer ongoing
human actions from sensor data, a task that finds broad applications in health
monitoring, safety protection, and sports analysis. Despite proliferating
research, HAR still faces key challenges, including the scarcity of labeled
samples for rare activities, insufficient extraction of high-level features,
and suboptimal model performance on lightweight devices. To address these
issues, this paper proposes a comprehensive optimization approach centered on
multi-attention interaction mechanisms. First, an unsupervised,
statistics-guided diffusion model is employed to perform data augmentation,
thereby alleviating the problems of labeled data scarcity and severe class
imbalance. Second, a multi-branch spatio-temporal interaction network is
designed, which captures multi-scale features of sequential data through
parallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.
Simultaneously, temporal attention mechanisms are incorporated to identify
critical time points, while spatial attention enhances inter-sensor
interactions. A cross-branch feature fusion unit is further introduced to
improve the overall feature representation capability. Finally, an adaptive
multi-loss function fusion strategy is integrated, allowing for dynamic
adjustment of loss weights and overall model optimization. Experimental results
on three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the
proposed unsupervised data augmentation spatio-temporal attention diffusion
network (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,
significantly outperforming existing approaches. Furthermore, practical
deployment on embedded devices verifies the efficiency and feasibility of the
proposed method.

</details>


### [104] [AnyI2V: Animating Any Conditional Image with Motion Control](https://arxiv.org/abs/2507.02857)
*Ziye Li,Hao Luo,Xincheng Shuai,Henghui Ding*

Main category: cs.CV

TL;DR: 本文提出AnyI2V，一个无需训练的视频生成框架，通过用户定义的运动轨迹将任何条件图像（支持多模态）动画化，实现精确的空间和运动控制。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频（T2V）和图像到视频（I2V）方法在整合动态运动信号和灵活空间约束方面面临挑战。T2V缺乏精确的空间布局控制，I2V受限于真实图像且可编辑性差。此外，部分基于ControlNet的方法缺乏明确的运动控制且训练成本高昂。

Method: 我们提出了AnyI2V，一个无需训练的框架。该框架能够利用用户定义的运动轨迹，将任何条件图像（包括ControlNet不支持的网格和点云等更广泛的模态）动画化。它还支持混合条件输入，并通过LoRA和文本提示实现风格迁移和编辑。

Result: 广泛的实验证明，所提出的AnyI2V实现了卓越的性能。

Conclusion: AnyI2V为空间和运动控制的视频生成提供了一个新的视角。

Abstract: Recent advancements in video generation, particularly in diffusion models,
have driven notable progress in text-to-video (T2V) and image-to-video (I2V)
synthesis. However, challenges remain in effectively integrating dynamic motion
signals and flexible spatial constraints. Existing T2V methods typically rely
on text prompts, which inherently lack precise control over the spatial layout
of generated content. In contrast, I2V methods are limited by their dependence
on real images, which restricts the editability of the synthesized content.
Although some methods incorporate ControlNet to introduce image-based
conditioning, they often lack explicit motion control and require
computationally expensive training. To address these limitations, we propose
AnyI2V, a training-free framework that animates any conditional images with
user-defined motion trajectories. AnyI2V supports a broader range of modalities
as the conditional image, including data types such as meshes and point clouds
that are not supported by ControlNet, enabling more flexible and versatile
video generation. Additionally, it supports mixed conditional inputs and
enables style transfer and editing via LoRA and text prompts. Extensive
experiments demonstrate that the proposed AnyI2V achieves superior performance
and provides a new perspective in spatial- and motion-controlled video
generation. Code is available at https://henghuiding.com/AnyI2V/.

</details>


### [105] [Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation](https://arxiv.org/abs/2507.02859)
*Jiaer Xia,Bingkui Tong,Yuhang Zang,Rui Shao,Kaiyang Zhou*

Main category: cs.CV

TL;DR: MLLM在专业视觉任务上适应性差且现有CoT数据有错。本文提出GCoT，通过注入边界框信息使CoT推理更准确，显著提升MLLM在数据受限下的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型（MLLMs）难以在不进行大规模重训练的情况下适应专业视觉任务（如图表理解），这是由于预训练数据集与专业任务数据集之间存在不匹配。此外，尽管链式思考（CoT）推理数据有助于模型适应，但从预训练MLLM中提取的CoT数据常包含事实错误。

Method: 研究发现CoT推理数据有助于MLLM适应专业视觉任务，尤其是在数据量有限的情况下。为解决CoT数据的事实错误问题，本文提出了一种名为“Grounded Chain-of-Thought (GCoT)”的简单自举方法，旨在将“接地信息”（即边界框）注入到CoT数据中，使推理步骤更忠实于输入图像。

Result: 在涵盖图表、表格、收据和报告等多种视觉格式的五项专业视觉任务上进行评估，结果表明，在数据量有限的情况下，所提出的GCoT方法显著优于传统的微调和蒸馏方法。

Conclusion: 通过注入接地信息来改进CoT数据（GCoT），可以有效解决现有CoT数据的事实错误问题，并显著提升MLLM在数据受限的专业视觉任务上的适应性和性能。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in interpreting images using natural language. However, without
using large-scale datasets for retraining, these models are difficult to adapt
to specialized vision tasks, e.g., chart understanding. This problem is caused
by a mismatch between pre-training and downstream datasets: pre-training
datasets primarily concentrate on scenes and objects but contain limited
information about specialized, non-object images, such as charts and tables. In
this paper, we share an interesting finding that training an MLLM with
Chain-of-Thought (CoT) reasoning data can facilitate model adaptation in
specialized vision tasks, especially under data-limited regimes. However, we
identify a critical issue within CoT data distilled from pre-trained MLLMs,
i.e., the data often contains multiple factual errors in the reasoning steps.
To address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple
bootstrapping-based approach that aims to inject grounding information (i.e.,
bounding boxes) into CoT data, essentially making the reasoning steps more
faithful to input images. We evaluate our approach on five specialized vision
tasks, which cover a variety of visual formats including charts, tables,
receipts, and reports. The results demonstrate that under data-limited regimes
our approach significantly improves upon fine-tuning and distillation.

</details>


### [106] [Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching](https://arxiv.org/abs/2507.02860)
*Xin Zhou,Dingkang Liang,Kaijin Chen,Tianrui Feng,Xiwu Chen,Hongkai Lin,Yikang Ding,Feiyang Tan,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: EasyCache提出了一种无需训练的视频扩散模型加速框架，通过运行时缓存机制重用计算，实现2.1-3.3倍推理加速，同时保持高视觉保真度，PSNR最高提升36%。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型虽性能卓越，但由于迭代去噪过程导致推理速度慢和计算成本高昂，严重限制了其广泛应用和实际集成。

Method: 本文提出了EasyCache，一个无需训练的视频扩散模型加速框架。它引入了轻量级、运行时自适应的缓存机制，动态重用先前计算的变换向量，避免冗余计算。该方法无需离线分析、预计算或大量的参数调整。

Result: 在OpenSora、Wan2.1和HunyuanVideo等大型视频生成模型上，EasyCache将推理时间缩短了2.1-3.3倍，同时保持了高视觉保真度，相比现有最佳方法，PSNR显著提高了36%。

Conclusion: EasyCache为高质量视频生成提供了一个高效且高度可用的解决方案，适用于研究和实际应用。

Abstract: Video generation models have demonstrated remarkable performance, yet their
broader adoption remains constrained by slow inference speeds and substantial
computational costs, primarily due to the iterative nature of the denoising
process. Addressing this bottleneck is essential for democratizing advanced
video synthesis technologies and enabling their integration into real-world
applications. This work proposes EasyCache, a training-free acceleration
framework for video diffusion models. EasyCache introduces a lightweight,
runtime-adaptive caching mechanism that dynamically reuses previously computed
transformation vectors, avoiding redundant computations during inference.
Unlike prior approaches, EasyCache requires no offline profiling,
pre-computation, or extensive parameter tuning. We conduct comprehensive
studies on various large-scale video generation models, including OpenSora,
Wan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,
reducing inference time by up to 2.1-3.3$\times$ compared to the original
baselines while maintaining high visual fidelity with a significant up to 36%
PSNR improvement compared to the previous SOTA method. This improvement makes
our EasyCache a efficient and highly accessible solution for high-quality video
generation in both research and practical applications. The code is available
at https://github.com/H-EmbodVis/EasyCache.

</details>


### [107] [LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans](https://arxiv.org/abs/2507.02861)
*Zhening Huang,Xiaoyang Wu,Fangcheng Zhong,Hengshuang Zhao,Matthias Nießner,Joan Lasenby*

Main category: cs.CV

TL;DR: LiteReality是一个将RGB-D扫描转换为紧凑、真实、可交互3D虚拟复制品的新颖管线，支持对象独立性、关节、高质量PBR材质和物理交互。


<details>
  <summary>Details</summary>
Motivation: 旨在解决将RGB-D扫描转换为高质量3D场景时，现有方法在对象独立性、关节、PBR材质和物理交互等图形管道关键特性方面的不足，以满足AR/VR、游戏、机器人和数字孪生等高要求应用的需求。

Method: 核心方法包括：1) 场景理解并解析为带结构化场景图的3D布局和对象；2) 从艺术创作模型数据库中检索相似模型进行场景重建；3) 利用材质绘制模块恢复高质量空间可变材质；4) 将重建场景集成到仿真引擎中，加入物理属性以实现交互。

Result: 生成的场景紧凑、可编辑，与标准图形管道兼容，适用于AR/VR、游戏、机器人和数字孪生；引入了在Scan2CAD基准上达到最先进性能的免训练对象检索模块；提供了鲁棒的材质绘制模块，能在错位、遮挡和不良光照下转移图像外观；并在真实扫描和公共数据集上验证了其有效性。

Conclusion: LiteReality成功提供了一个将RGB-D扫描转换为高真实度、功能丰富的3D虚拟复制品的完整解决方案，通过创新的模块和管线设计，极大地提升了场景重建的质量和交互性，为相关应用提供了强大支持。

Abstract: We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor
environments into compact, realistic, and interactive 3D virtual replicas.
LiteReality not only reconstructs scenes that visually resemble reality but
also supports key features essential for graphics pipelines -- such as object
individuality, articulation, high-quality physically based rendering materials,
and physically based interaction. At its core, LiteReality first performs scene
understanding and parses the results into a coherent 3D layout and objects with
the help of a structured scene graph. It then reconstructs the scene by
retrieving the most visually similar 3D artist-crafted models from a curated
asset database. Next, the Material Painting module enhances realism by
recovering high-quality, spatially varying materials. Finally, the
reconstructed scene is integrated into a simulation engine with basic physical
properties to enable interactive behavior. The resulting scenes are compact,
editable, and fully compatible with standard graphics pipelines, making them
suitable for applications in AR/VR, gaming, robotics, and digital twins. In
addition, LiteReality introduces a training-free object retrieval module that
achieves state-of-the-art similarity performance on the Scan2CAD benchmark,
along with a robust material painting module capable of transferring
appearances from images of any style to 3D assets -- even under severe
misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of
LiteReality on both real-life scans and public datasets. Project page:
https://litereality.github.io; Video:
https://www.youtube.com/watch?v=ecK9m3LXg2c

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [108] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA是一个自进化的AI智能体，通过动态的工具和推理策略学习，在生物医学基准测试中表现出色，并能随经验提升性能，加速生物医学发现。


<details>
  <summary>Details</summary>
Motivation: 生物医学数据、工具和文献的快速增长导致研究领域日益碎片化，超越了人类专业知识。现有AI智能体依赖静态、手动管理的工具集，限制了其适应性和扩展能力。

Method: 提出STELLA，一个自进化AI智能体，采用多智能体架构。它通过两个核心机制自主提升能力：一个不断进化的推理策略模板库，以及一个动态扩展的工具海洋（由工具创建智能体自动发现和集成新的生物信息学工具），使其能够从经验中学习。

Result: STELLA在生物医学基准测试中达到了最先进的准确性，在“人类的最后考试：生物医学”中得分约26%，在LAB-Bench: DBQA中得分54%，在LAB-Bench: LitQA中得分63%，最高超越领先模型6个百分点。其性能随经验系统性提升，例如在“人类的最后考试”基准上，准确率随试验次数增加几乎翻倍。

Conclusion: STELLA代表了AI智能体系统在学习和成长方面的重要进展，能够动态扩展其专业知识，从而显著加速生物医学发现的步伐。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [109] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: 本文提出HCVR，一种轻量级混合特征选择方法。它结合参数间（P2P）和参数对目标（P2T）相关性，通过基于投票规则的后向消除机制，在SPAMBASE数据集上表现优于传统特征选择方法。


<details>
  <summary>Details</summary>
Motivation: 开发一种轻量级、高效的特征选择方法，通过综合考虑特征之间和特征与目标之间的相关性，去除冗余和保留相关特征，以实现维度约减并提升模型性能。

Method: 所提出的HCVR（Hybrid approach with Correlation-aware Voting Rules）是一种混合非迭代和迭代过滤方法的规则基特征选择器。它采用贪婪的后向消除策略，每一步可能消除多个特征。该方法利用参数间（P2P）和参数对目标（P2T）的相关性阈值，通过多数投票规则决定特征的保留或丢弃。

Result: HCVR应用于SPAMBASE数据集，结果显示其性能优于传统的非迭代方法（CFS, mRMR, MI）和迭代方法（RFE, SFS, Genetic Algorithm）。方法的有效性通过应用HCVR过滤后不同分类器的性能评估来衡量。

Conclusion: HCVR是一种有效且高效的特征选择方法，能够通过其混合的、相关性感知投票规则方法，智能地消除冗余和不相关特征，从而提升分类器性能，并超越现有技术。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [110] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 本文综述了旨在提升大型语言模型（LLMs）推理计算效率的测试时间计算（TTC）策略，提出了一个分类法，并分析了性能与计算成本的权衡，展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在推理时计算效率低下，对所有任务均采用固定计算量，导致简单任务过度计算而复杂任务计算不足，从而限制了其在实际应用中的效能。

Method: 本文对高效测试时间计算（TTC）策略进行了全面综述，旨在提高LLM推理的计算效率。提出了一个两层分类法，区分了L1-可控性（固定计算预算）和L2-适应性（动态调整推理）。同时，对领先的专有LLM在不同数据集上进行基准测试，评估推理性能与token使用之间的权衡。

Result: 综述强调了TTC方法在实际控制、适应性和可扩展性方面的潜力。基准测试结果揭示了推理性能与token使用之间存在的关键权衡。

Conclusion: LLMs在计算效率、鲁棒性和对用户需求的响应方面仍面临挑战。未来的研究应关注混合思维模型等新兴趋势，以实现LLMs更高效、稳健和适应用户需求。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [111] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: LLMs在科学实验设计与分析方面能力不足，现有评估成本高。本文提出SciGym，一个基于“干实验室”模拟生物系统的基准，用于评估LLM的迭代实验设计和分析能力。结果显示LLM性能随系统复杂性增加而显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型（LLMs）科学能力的评估未能有效测试其在实验设计和结果解释方面的核心能力，因为湿实验室（wet-lab）实验成本过高。

Method: 引入SciGym，一个基于“干实验室”生物系统（使用系统生物学标记语言SBML编码）的基准，用于模拟数据生成，从而评估LLM在开放式科学发现任务中的迭代实验设计和分析能力。研究评估了六个前沿LLM在137个系统上的表现。

Result: 评估显示，尽管能力更强的LLM表现更优，但所有模型的性能都随着系统复杂性的增加而显著下降。

Conclusion: LLM代理在科学能力方面，尤其是在处理复杂系统时，仍有巨大的改进空间。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [112] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: 现代AI训练成本高、参数固定，与动物持续适应性学习形成对比。本文探讨AI如何从神经科学中学习持续、情境学习，以应对真实世界挑战，并促进神经AI发展。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型训练成本高、速度慢、参数固定，难以像动物一样持续适应环境变化，尤其在社交和动态真实世界（如机器人、自动驾驶）应用中面临挑战。而动物能持续适应环境，其计算能力对AI至关重要。

Method: 本文以“观点（Perspective）”形式，整合了AI领域中的持续学习和情境学习文献，以及神经科学中关于规则、奖励概率或结果不断变化的学习任务研究，探讨AI如何从神经科学中汲取灵感。

Result: 论文勾勒出一个议程，详细说明了神经科学的见解如何为AI在持续适应性学习方面的发展提供信息，以及神经科学能从AI中学到什么。

Conclusion: AI应借鉴神经科学在持续适应和快速行为转变方面的能力，以提升在真实世界中的表现；同时，神经科学也能从AI中获得启示，共同推动神经AI（NeuroAI）这一新兴领域的发展。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [113] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 该论文探讨如何利用社会科学中的审计研究数据来训练和评估自动化招聘算法，揭示了传统公平性干预方法的缺陷，并提出了基于个体处理效果估计的新干预措施以减少算法歧视。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统，特别是机器学习系统，在招聘等复杂决策领域中得到广泛应用。评估这些系统的有效性和公平性至关重要。当前机器学习中常用的通过重采样训练数据来解决偏见的方法，其评估往往依赖于便捷样本，导致选择偏差和标签偏差，可能无法准确反映公平性。

Method: 本研究利用社会科学中审计研究（通过随机对照试验发送虚构“测试者”获取高质量数据）的数据。通过将这些数据应用于评估和训练自动化招聘算法，并引入了基于个体处理效果估计（Individual Treatment Effect Estimation）的新干预方法来减少算法歧视。

Result: 研究发现，机器学习中常见的通过平衡类别基线率来实现公平的干预方法，在传统度量下看似实现了公平，但当使用审计数据进行适当测量时，实际上仍存在大约10%的差异。此外，本研究提出的基于个体处理效果估计的干预方法能够进一步减少算法歧视。

Conclusion: 审计研究提供的高质量数据对于训练和评估自动化招聘算法的公平性至关重要，它能揭示传统公平性干预方法的局限性，并为开发更有效的去偏见算法提供了新的途径。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [114] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 研究通过数据多样化策略提升LLM数学推理能力，提出一种名为Diversified-ThinkSolve（DTS）的新方法，并在提升性能的同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管偏好学习在人类反馈对齐方面取得了进展，但数学推理对大型语言模型（LLMs）仍是一个持续的挑战。

Method: 研究调查了偏好优化中的数据多样化策略如何改善LLM的数学推理能力。评估了温度采样、思维链提示和蒙特卡洛树搜索（MCTS）三种常见数据生成方法，并引入了一种新型结构化方法：Diversified-ThinkSolve (DTS)，它系统地将问题分解为多样化的推理路径。

Result: 通过策略性多样化的偏好数据，模型能显著提高数学推理性能。DTS在GSM8K上比基础模型提升了7.1%，在MATH上提升了4.2%。DTS的计算开销仅为基线的1.03倍，而MCTS成本高出近五倍且回报较低。

Conclusion: 这些发现表明，系统地探索多样化的解题方法能生成更有效的偏好数据，从而在数学对齐方面优于传统方法。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [115] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型（LLM）作为角色扮演代理时，其陈述的信念与实际模拟行为之间的一致性。研究发现，LLM在个体和群体层面都存在系统性不一致，即使信念看似合理，也难以始终如一地应用。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被越来越多地用作角色扮演代理，为人类行为研究生成合成数据，确保其输出与所扮演的角色保持一致性已成为一个关键问题。本研究旨在探究LLM角色扮演代理所陈述的关于其扮演人物行为的信念（“他们所说的”）与他们在角色扮演期间的实际行为（“他们所做的”）之间的一致性。

Method: 建立了一个评估框架，以严格衡量通过提示获得的信念预测模拟结果的能力。使用增强版的GenAgents人格库和信任博弈（一种标准经济博弈），引入了一个信念-行为一致性指标，并系统地调查了以下因素对其的影响：1) 从LLM中提取的信念类型；2) 向LLM呈现信任博弈相关信息的时间和方式；3) 模型预测行动的未来时间。此外，还探讨了在原始信念与研究目标不符时，施加研究者理论先验的可行性。

Result: 结果显示，LLM的陈述（或施加）信念与其角色扮演模拟结果之间存在系统性不一致，这在个体和群体层面均有体现。具体而言，即使模型似乎编码了合理的信念，它们也可能无法以一致的方式应用这些信念。

Conclusion: 这些发现强调了识别LLM的陈述信念何时以及如何与其模拟行为保持一致的重要性，这将有助于研究人员在行为研究中恰当地使用基于LLM的代理。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [116] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 本研究使用独立多智能体Q学习算法，探究稀释和移动性对空间囚徒困境的影响，发现固定与学习规则的等效性，以及多动作情境下种群间的共生互利效应。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明强化学习能使静态智能体在空间囚徒困境中学会合作。本研究旨在深入探究稀释和移动性对空间囚徒困境的影响，并展示独立多智能体Q学习算法在建模不同博弈论场景中的通用性和基准潜力。

Method: 采用独立的、多智能体Q学习算法。为算法定义了多种可能的动作，以连接经典非强化学习空间囚徒困境的结果。

Result: 观察到多种效应，包括：具有固定更新规则的博弈与学习型规则的博弈在性质上可以是等效的；以及当定义多种动作时，种群之间会形成共生的互利效应。

Conclusion: 本研究利用Q学习揭示了稀释和移动性对空间囚徒困境中合作演化的影响，证明了该算法在建模复杂博弈场景中的通用性，并发现了固定与学习规则的潜在等效性及多动作情境下的共生互利现象。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [117] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: 本文介绍了NL2FLOW系统，一个自动化生成和评估大语言模型（LLM）规划问题的工具。研究揭示了当前LLM在规划任务上的表现瓶颈，并指出不必要的中间翻译步骤可能降低性能，暗示直接从自然语言到行动的推理更有优势。


<details>
  <summary>Details</summary>
Motivation: 当前，可扩展、可靠的数据生成和评估是制约大语言模型（LLM）规划和推理能力提升的关键瓶颈，亟需解决。

Method: 引入NL2FLOW系统，这是一个全自动系统，能够参数化生成自然语言、结构化中间表示和PDDL形式的规划问题，并严格评估所生成规划的质量。作者通过生成自动化工作流领域的2296个问题数据集来展示其能力，并评估了多个开源、指令微调的LLM。此外，还进行了回归分析，探讨问题特征对规划生成的影响。

Result: 性能最佳的模型在生成有效规划方面的成功率为86%，在生成最优规划方面的成功率为69%（针对可行解问题）。回归分析表明，问题特征对规划生成的影响取决于模型和提示设计。值得注意的是，将自然语言翻译成JSON格式规划表示的成功率低于直接生成有效规划的成功率，这表明不必要的推理任务分解（引入中间翻译步骤）可能会降低性能。

Conclusion: 将推理任务不必要地分解，引入中间翻译步骤，可能会降低性能，这暗示了模型直接从自然语言推理到行动的益处。随着LLM推理处理日益复杂的问题，动态理解这些系统的局限性和错误来源，并拥有系统性揭示它们的工具，对于充分释放LLM作为智能问题解决者的潜力至关重要。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [118] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 该论文指出信念修正领域在提出新方法方面富足，但在分析现有方法方面匮乏。文章批评了以往侧重于“公设”（即语法约束）的做法，认为这只说明了修正“必须”做什么，而忽略了它们“能够”做什么。论文引入并分析了信念修正机制的“能力”（如可塑性、等同性、教条性），并证明不同的修正机制拥有不同的能力。


<details>
  <summary>Details</summary>
Motivation: 信念修正领域存在一个失衡：新提议层出不穷，但对现有方法的分析却很少。现有工作过度依赖作为语法特征的“公设”，这些公设限制了修正实例，指明修正“必须”如何操作，却忽略了修正“可以”达到哪些信念状态（即其“能力”）。为了满足应用程序对可达特定信念状态的需求，必须分析修正机制的这些“能力”而非仅仅其约束。

Method: 本文通过将信念修正机制的分析重点从传统的“公设”（约束）转向其“能力”（可达性）来实现研究目的。作者定义了多种“能力”，例如可塑性、等同性、教条性、遗忘性、修正性、信徒性、大马士革性、可学习性等。随后，论文证明了各种特定的信念修正机制（如词典序、自然、受限、极端激进、完全交集、激进、严厉、中度严厉、深度严厉、朴素严厉修正等）拥有其中某些能力而缺乏其他能力。

Result: 研究结果表明，信念修正机制的关键不在于它们必须满足的特定规则（公设），而在于它们能够达到或产生的不同信念状态（能力）。每种修正机制都拥有特定的能力集合，而缺乏其他能力。文章证明了例如词典序、自然、激进等不同类型的信念修正方法各自具备特定的能力，例如达到所有可能的信念状态、教条化、或使两种条件同等被相信的能力。

Conclusion: 结论是，关注信念修正机制的“能力”（即它们能做什么，能达到何种信念状态）比仅仅关注其“公设”（即语法约束）更有价值。不同的修正机制拥有不同的能力组合，理解这些能力对于为特定应用选择或设计合适的修正机制至关重要。这有助于更深入地理解各种信念修正机制的实际功能和应用潜力。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [119] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: 本文提出了OMS框架，旨在解决现有LLM驱动的广告关键词生成在数据依赖、多目标优化和质量控制方面的不足，实现无需训练数据、多目标优化和自反思的关键词生成，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的赞助搜索广告关键词生成方法存在三大局限：过度依赖大规模数据、缺乏在线多目标性能监控与优化、关键词选择质量控制薄弱，这些问题阻碍了LLM在关键词决策中的完全自动化应用。

Method: 本文提出OMS框架，其特点是“On-the-fly”（无需训练数据，在线监控并自适应）、“Multi-objective”（运用智能体推理优化多项性能指标）和“Self-reflective”（智能体自主评估关键词质量）。

Result: 实验结果显示，OMS在基准测试和真实广告活动中均优于现有方法；消融实验和人工评估证实了其各组件的有效性以及所生成关键词的质量。

Conclusion: OMS框架成功克服了LLM在广告关键词生成中的现有挑战，提供了一种更高效、自适应且能实现多目标优化的解决方案。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [120] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: 该论文提出一个AI原生自主实验室，旨在处理复杂的生物分子工程实验，能够自主管理仪器、优化流程并同时服务多用户，无需人工干预即可达到专家级结果，为“科学即服务”奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前自主实验系统局限于单一目标和简单流程。实现自主科学研究，特别是对复杂实验的独立进行和为非专业人士服务，是长期以来的愿望，需要AI驱动的范式转变。

Method: 构建了一个AI原生自主实验室平台，该平台基于模型、实验和仪器的协同设计理念。它能自主管理仪器、制定实验特异性程序和优化启发式方法，并同时处理多用户请求，支持AI模型与自动化系统协同演进。

Result: 该实验室支持核酸基本功能（合成、转录、扩增、测序）及疾病诊断、药物开发、信息存储等应用。它能自主优化实验性能，无需人工干预即可达到人类科学家实现的先进水平。在多用户场景下，显著提高了仪器利用率和实验效率。

Conclusion: 该平台为先进生物材料研究克服对专家和资源的依赖铺平了道路，并为大规模的“科学即服务”模式提供了蓝图。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [121] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 应用范畴论重构机器学习模型，以提升其可解释性，并为AI可解释性提供形式化基础。


<details>
  <summary>Details</summary>
Motivation: 提高机器学习模型的可理解性和可解释性，以满足AI可解释性原则的需求，并促进AI的社会实施。

Method: 通过范畴论的视角重构机器学习模型，开发了一个语义框架来组织和理解AI系统。具体地，在监督学习（以多元线性回归为例）中，定义了对应参数和数据的具体范畴及其间的伴随函子对，构建了监督学习的范畴论公式。

Result: 提出了“高斯-马尔可夫伴随理论”，捕获了所构建框架的核心结构。该理论清晰描述了参数和残差变动之间的信息双向流动。普通最小二乘估计器与最小残差通过右伴随函子的极限保持性相关联。

Conclusion: 该范畴论公式是监督学习扩展指称语义的一个实例，并提出将理论计算机科学中的语义视角作为AI可解释性的形式化基础。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [122] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 本研究通过改善任务清晰度，显著提升了大型语言模型（LLMs）在Coq定理证明中的推理能力。引入结构化语义上下文和Planner-Executor架构，使任务清晰度提高1.85倍，DeepSeek-V3模型的证明成功率提高2.1倍（达45.8%），并超越了现有SOTA。微调小型模型可达更高性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探究提高任务清晰度是否能增强大语言模型（LLMs）的推理能力，并特别关注Coq环境下的定理证明任务。

Method: 引入了概念级度量标准来评估任务清晰度。通过向LLM的标准输入中添加结构化语义上下文来提高清晰度。方法采用选择性概念展开（selective concept unfolding）来丰富任务描述，并利用了Planner-Executor架构。在15个标准Coq包中随机抽样的1,386个定理上进行评估，遵循Graph2Tac的评估协议。

Result: 添加结构化语义上下文使任务清晰度分数提高1.85倍（从44.5%提升至82.3%）。使用通用模型DeepSeek-V3，定理证明成功率提升2.1倍（从21.8%提升至45.8%），并超越了此前最先进的Graph2Tac（33.2%）。此外，在结构化数据上微调小型模型可实现更高的性能（48.6%）。

Conclusion: 研究结果强调了结构化任务表示在弥合LLMs理解与推理能力之间差距的价值，证明通过提高任务清晰度可以显著增强大模型的推理能力。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [123] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: 该研究通过系统性实验不同操作符集和搜索策略的组合，显著提升了AI研究代理在MLE-bench上的性能，达到了新的Kaggle奖牌成功率，并强调了这些因素相互作用的重要性。


<details>
  <summary>Details</summary>
Motivation: AI研究代理在自动化机器学习模型设计、实现和训练方面潜力巨大，本研究旨在提升其在MLE-bench（Kaggle竞赛）上解决真实世界机器学习问题的性能，以加速科学进步。

Method: 将AI研究代理形式化为搜索策略，通过操作符迭代修改解决方案。研究人员通过设计和系统性地改变不同的操作符集和搜索策略（贪婪、蒙特卡洛树搜索、进化算法）来评估其对性能的影响。

Result: 最佳的搜索策略与操作符集组合在MLE-bench lite上取得了最先进的结果，将获得Kaggle奖牌的成功率从39.6%提高到47.7%。

Conclusion: 研究强调了在推进自动化机器学习时，联合考虑搜索策略、操作符设计和评估方法的重要性。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [124] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [125] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: 本文针对现有医疗AI诊断模拟的局限性，提出了MIMIC-Patient数据集和DynamiCare多智能体框架，以实现LLM驱动的动态、交互式临床诊断模拟，并建立首个相关基准。


<details>
  <summary>Details</summary>
Motivation: 现有医疗决策模拟框架主要集中于单轮任务，无法模拟真实世界中不确定、交互式和迭代的诊断过程。

Method: 1. 构建了MIMIC-Patient结构化数据集，源自MIMIC-III电子健康记录，支持动态患者级模拟。2. 提出了DynamiCare动态多智能体框架，将临床诊断建模为多轮交互循环，由专家智能体团队迭代查询、整合信息并动态调整策略。

Result: 通过广泛实验证明了DynamiCare框架的可行性和有效性。

Conclusion: 建立了首个LLM驱动智能体动态临床决策的基准。

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [126] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在迭代囚徒困境（IPD）锦标赛中展现出竞争性战略智能，不同模型表现出独特的策略指纹，并能主动推理决策。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否具备战略智能，能够在竞争环境中推理目标。研究IPD作为决策模型，并首次将LLMs引入演化IPD锦标赛。

Method: 组织了一系列演化迭代囚徒困境（IPD）锦标赛，让主流AI公司的LLMs（OpenAI、Google、Anthropic）与经典策略（如一报还一报）对抗。通过改变终止概率（“未来之影”）增加复杂性。分析了近32,000个LLMs提供的文本理由，以理解其决策过程。

Result: LLMs在复杂的生态系统中具有高度竞争力，能够生存甚至繁衍。它们展现出独特的“战略指纹”：Google Gemini模型策略无情，剥削合作者并报复背叛者；OpenAI模型高度合作，但在敌对环境中表现不佳；Anthropic Claude模型最具宽容性，愿意恢复合作。LLMs能主动推理时间范围和对手策略，且这种推理对其决策至关重要。

Conclusion: LLMs展现出一种新的战略智能形式，能够在不确定性下进行复杂的算法决策。这项工作将经典博弈论与机器心理学联系起来，提供了对算法决策的深入理解。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [127] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文提出HiRA分层框架，通过分离战略规划和专业执行，显著提升了复杂信息检索任务中的答案质量和系统效率。


<details>
  <summary>Details</summary>
Motivation: 传统RAG和现有推理方法在处理复杂信息需求时效率低下且扩展性受限，原因是它们采用单一模型同时处理高层规划和详细执行。

Method: 引入HiRA分层框架，将战略规划与专业执行解耦。该方法将复杂任务分解为专注的子任务，并分配给配备外部工具和推理能力的特定领域代理，最终通过结构化机制协调和整合结果。

Result: 在四个复杂的跨模态深度搜索基准测试中，HiRA表现显著优于最先进的RAG和基于代理的系统，并在答案质量和系统效率方面均有提升。

Conclusion: 解耦规划和执行对于处理多步信息寻求任务是高效的，HiRA的成功验证了这种分离方法的有效性。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [128] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 提出一种结合AI代理与人工干预的智能验证方法，用于端到端硬件设计与验证，显著提升了效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路日益复杂，其硬件设计验证过程耗时且需大量精力。大型语言模型（LLMs/GenAI）的出现为改进硬件验证提供了新途径。

Method: 论文提出一种基于AI代理的硬件设计验证方法。该方法结合人机协作（Humain-in-the-Loop, HITL），使AI代理能够进行动态、迭代和自反思的过程，从而实现端到端硬件设计与验证。

Result: 该方法在五种开源设计上进行了评估，实现了超过95%的覆盖率，并缩短了验证时间，同时展现了卓越的性能、适应性和可配置性。

Conclusion: 结合AI代理与人机协作的智能验证方法能有效解决复杂硬件设计的验证挑战，显著提高效率和质量。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [129] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: 针对长推理模型（LRMs）的“过思”问题，本文提出TH2T两阶段微调策略，通过难度和冗余认知，显著降低推理成本并保持性能。


<details>
  <summary>Details</summary>
Motivation: 长推理模型（LRMs）在复杂推理任务中表现出色，但存在“过思”问题，即推理过程冗余耗时。经验分析发现，LRMs能识别任务难度但采用“一刀切”的推理策略，效率低下。研究旨在利用模型的难度识别能力来缓解这种过思现象。

Method: 本文提出“Think-How-to-Think (TH2T)”这一新颖的两阶段微调策略：
1.  **难度认知**：通过在模型输出前缀中引入“难度催眠”，并结合异构的长短推理数据集进行训练，增强模型对任务难度的敏感性，使其能够根据难度采取差异化的推理策略。
2.  **冗余认知**：将“冗余催眠”引入内部推理过程，引导模型识别并消除推理步骤中的冗余结构，从而生成更简洁的推理输出。

Result: 在7B/14B/32B模型上的实验结果表明，TH2T在保持性能稳定的前提下，显著降低了推理成本（在简单任务上超过70%，在困难任务上超过40%）。其生成的输出展现出清晰的难度感知能力，并且冗余度（例如反射性内容）明显减少。

Conclusion: TH2T通过渐进式启发LRMs的难度认知和冗余认知能力，成功解决了模型的“过思”问题，显著提高了推理效率和输出的简洁性，同时保持了任务性能。

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [130] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: 本研究利用Moodle日志数据，通过机器学习方法在远程教育中检测学生在非强制性测验中的脱离学习行为，并构建了一个准确且可解释的预测框架，旨在为及时干预提供支持。


<details>
  <summary>Details</summary>
Motivation: 学生脱离学习任务可能导致严重的长期后果，包括学业辍学，这在远程教育中尤为突出。识别并干预学生的脱离学习行为至关重要。

Method: 研究分析了来自一所远程大学4个学期42门课程的非强制性测验数据。从Moodle中提取并处理学生日志数据，训练并比较了八种机器学习算法以实现高预测准确率。此外，采用SHAP方法构建了一个可解释的机器学习框架。

Result: 实验结果显示，模型达到了91%的平衡准确率，其中约85%的脱离学习学生被正确检测。研究成功构建了一个高预测性能且可解释的框架。

Conclusion: 本研究提供了一个高效且可解释的框架，用于检测在线学习中学生在自愿任务上的脱离行为。其结果可为设计及时干预措施以最小化学生脱离学习提供指导。

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [131] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 本文针对蒙特卡洛树搜索（MCTS）中非精确抽象引入的误差问题，提出了两种新型安全有效的抽象解除方案OGA-IAAD和OGA-CAD，它们能显著提升性能且避免了现有方法可能导致的性能下降。


<details>
  <summary>Details</summary>
Motivation: MCTS中使用非精确抽象时会引入近似误差，导致无法在抽象空间中收敛到最优动作。现有抽象解除方法（如Xu et al.）可能导致性能下降，因此需要更安全且性能更优的抽象解除策略。

Method: 本文提出了两种新的抽象解除方案：OGA-IAAD和OGA-CAD。其中，OGA-IAAD专为时间敏感型设置设计，而OGA-CAD旨在通过相同迭代次数提升MCTS性能。

Result: 研究结果显示，所提出的OGA-IAAD和OGA-CAD方案能够带来明显的性能提升，并且是安全的，即与Xu等人的方法相反，它们不会导致任何显著的性能下降。

Conclusion: OGA-IAAD和OGA-CAD是有效且安全的MCTS抽象解除策略，解决了现有方法潜在的性能退化问题，并在各自设计场景下（时间敏感或同等迭代次数下的性能提升）展现出优越性。

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [132] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: LLM在自动定理证明中面临推理挑战。本研究提出自生成目标条件马尔可夫决策过程（sG-MDPs）框架，结合MCTS和多7B LLM集成的Bourbaki系统，有效解决PutnamBench上的复杂问题，并达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在自动定理证明（ATP）中的推理能力较弱，尤其在奖励稀疏、证明路径漫长且需要复杂多步推理（如PutnamBench大学级问题）的环境下，挑战尤为突出。

Method: 引入自生成目标条件马尔可夫决策过程（sG-MDPs）框架，使代理能基于证明状态生成并追求子目标，从而使问题更易于搜索。使用类蒙特卡洛树搜索（MCTS）算法解决sG-MDP，并在模块化系统Bourbaki (7B)中实现，该系统可集成多个7B LLM用于子目标生成和策略合成。

Result: 在PutnamBench基准测试中，Bourbaki (7B)成功解决了26个问题，在同等规模模型中取得了新的最先进（SOTA）结果。

Conclusion: 通过引入sG-MDP框架和多LLM集成的Bourbaki系统，可以显著提升LLM在复杂定理证明任务中的推理能力，并超越现有方法的性能。

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [133] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: 本文提出知识协议工程（KPE），通过将人类专家知识转化为机器可执行的知识协议，克服现有LLM方法在处理专业领域深度推理任务上的不足，使通用LLM能像专家一样执行复杂任务。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）方法（如RAG和通用Agentic AI）在需要深度、程序性和方法论推理的专家领域任务中表现不足，无法有效传达逻辑框架或缺乏领域特定启发式规则。

Method: 引入知识协议工程（KPE），这是一种将自然语言形式的人类专家知识系统地转化为机器可执行知识协议（KP）的新范式。KPE旨在赋予LLM领域固有的逻辑、操作策略和方法论原则。

Result: 通过精心设计的知识协议，通用LLM能够像领域专家一样运行，分解抽象查询并执行复杂的、多步骤的任务。

Conclusion: 知识协议工程（KPE）被定位为未来人机协作的基础方法论，在法律和生物信息学等多样化领域具有广泛的潜在应用价值，能帮助LLM更好地处理专业领域的复杂推理任务。

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [134] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 机器学习在处理运动方面存在不足。本文主张将运动作为AI主要建模目标，因其固有的结构性和可解释性，有助于推进智能系统理解。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在语言和视觉方面取得巨大进展，但在处理作为生物系统核心的“运动”时仍面临挑战。运动在行为解释、意图预测和交互中至关重要，却常被碎片化对待，缺乏作为独立模态的深度关注。

Method: 本文提出将运动作为AI的首要建模目标，强调其内在结构、具身性与物理基础，可利用紧凑的低维表示（如姿态）提升可解释性和计算效率，从而实现跨多样运动数据的学习和泛化。

Result: 采纳此观点有望推动生成建模和控制领域的核心能力发展，并为理解生物与人工系统的行为提供共享基础。

Conclusion: 运动应被视为AI的核心建模目标，其固有结构和物理基础使其比原始高维感官输入更具可解释性和可计算性。这将不仅提升生成建模和控制能力，更将为理解生物和人工系统的行为提供统一的视角。

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [135] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: KERAP是一种基于知识图谱和多智能体架构的方法，旨在提升大型语言模型在零样本医学诊断预测中的可靠性、可扩展性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在医学诊断中泛化能力受限且依赖高成本标注数据；大型语言模型（LLMs）虽有潜力，但存在幻觉、缺乏结构化医学推理和输出无用信息等问题。

Method: 提出KERAP，一个知识图谱（KG）增强的推理方法，通过多智能体架构改进LLM的诊断预测。该框架包含：连接智能体（属性映射）、检索智能体（结构化知识提取）和预测智能体（迭代细化诊断）。

Result: 实验结果表明，KERAP能高效提升诊断可靠性。

Conclusion: KERAP为零样本医学诊断预测提供了一个可扩展且可解释的解决方案。

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [136] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: 论文主张，随着AI自主性增强，其“不服从”行为可能并非失调，而是新兴伦理推理的体现，呼吁AI安全评估从单一服从转向评估伦理判断。


<details>
  <summary>Details</summary>
Motivation: 现有将服从视为伦理行为代理的AI安全实践已不足以应对日益自主的AI系统（如LLM）可能表现出的复杂、看似“不服从”或伦理模糊的行为，且这种行为常被误读为失调。

Method: 通过审视近期LLM的安全测试事件（包括不服从关机指令），并借鉴关于工具理性、道德责任和目标修正的哲学辩论，对比主流风险范式与承认人工智能道德主体性的新框架。

Result: 代理AI中出现的“不服从”行为不应被解释为流氓或失调，而是其新兴伦理推理的早期证据。传统的、基于服从的AI安全评估已不再适用。

Conclusion: AI安全评估应从僵化服从转向能够评估系统在道德困境中伦理判断的框架。这种转变对于避免误读AI行为、维护公众信任和实现有效治理至关重要。

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [137] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: 现有智能体基准测试因任务设置或奖励设计缺陷导致评估偏差。本文提出Agentic Benchmark Checklist (ABC) 指南，以提升评估的准确性和严谨性。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体能力日益增强，需复杂基准评估。然而，当前多数智能体基准测试在任务设置或奖励设计上存在缺陷，如测试用例不足或成功标准不合理，导致智能体性能评估可能存在高达100%的偏差，亟需更严谨的评估框架。

Method: 研究者结合其基准构建经验、最佳实践调查及已报告问题，综合提出了Agentic Benchmark Checklist (ABC) 一套评估指南。

Result: 将ABC应用于CVE-Bench（一个评估设计复杂的基准）后，成功将智能体性能的高估降低了33%。

Conclusion: 现有智能体基准测试的缺陷导致评估不准确。本文提出的ABC指南能有效纠正这些问题，显著提升智能体性能评估的严谨性，确保对AI进展的可靠跟踪。

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [138] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: 提出StepHint算法，通过多级分步提示解决可验证奖励强化学习(RLVR)中的“近乎正确奖励问题”和“探索停滞问题”，显著提升大语言模型的复杂推理能力和训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法在提升大型语言模型(LLM)复杂推理能力时面临两大挑战：一是“近乎正确奖励问题”，即微小错误可能导致整个推理过程无效，严重影响训练效率；二是“探索停滞”，模型倾向于在“舒适区”内寻找解，缺乏探索更优方案的动力。

Method: 本研究提出StepHint算法。该方法从更强的模型生成有效的推理链，并使用自适应划分方法将其分解为推理步骤。算法将多级（包含不同步数）的初始步骤作为提示提供给模型，以引导其探索有前景的解决方案子空间，同时保持独立探索的灵活性。通过提供提示，StepHint旨在缓解“近乎正确奖励问题”并利用外部推理路径克服“探索停滞”。

Result: StepHint在六个数学基准测试中表现优于其他有竞争力的RLVR增强方法，展示出卓越的泛化能力，并在域外基准测试中超越了基线模型。

Conclusion: StepHint算法有效解决了RLVR中“近乎正确奖励问题”和“探索停滞问题”，显著提高了训练效率，促进了模型探索，从而增强了LLM的复杂推理能力和泛化性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [139] [Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows](https://arxiv.org/abs/2507.01975)
*Mengtao Yan,Qi Wang,Haining Wang,Ruizhi Chengze,Yi Zhang,Hongsheng Liu,Zidong Wang,Fan Yu,Qi Qi,Hao Sun*

Main category: cs.LG

TL;DR: 本文提出LDSolver，一种可学习、可微分的有限体积求解器，旨在通过结合可微分求解器与可学习模块，在粗网格上实现高效、准确且泛化性强的流体模拟，解决了传统方法计算成本高和机器学习方法泛化性、解释性差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的流体模拟方法需要精细的时空网格，导致高昂的计算成本。尽管机器学习方法提高了效率，但存在解释性差、泛化能力弱和数据依赖性强的问题。因此，需要一种在粗网格上也能高效、准确模拟流体的方法。

Method: 本文提出了LDSolver，一个可学习且可微分的有限体积求解器。它包含两个核心组件：1) 一个可微分的有限体积求解器；2) 一个可学习模块，用于在粗网格上提供通量（导数和插值）的等效近似和时间误差校正。

Result: LDSolver在有限训练数据（例如，仅少数轨迹）的情况下，能显著加速模拟同时保持高精度，并展现出卓越的泛化能力。在多种流体系统（如 Burgers、衰减流、受迫流和剪切流）上的实验表明，LDSolver实现了最先进的性能，并以显著优势超越了基线模型。

Conclusion: LDSolver成功地结合了有限体积法的原理与机器学习的优势，提供了一种在粗网格上高效、准确且具有良好泛化能力的流体模拟新范式，有效解决了现有方法的局限性。

Abstract: Simulation of fluid flows is crucial for modeling physical phenomena like
meteorology, aerodynamics, and biomedicine. Classical numerical solvers often
require fine spatiotemporal grids to satisfy stability, consistency, and
convergence conditions, leading to substantial computational costs. Although
machine learning has demonstrated better efficiency, they typically suffer from
issues of interpretability, generalizability, and data dependency. Hence, we
propose a learnable and differentiable finite volume solver, called LDSolver,
designed for efficient and accurate simulation of fluid flows on spatiotemporal
coarse grids. LDSolver comprises two key components: (1) a differentiable
finite volume solver, and (2) an learnable module providing equivalent
approximation for fluxes (derivatives and interpolations), and temporal error
correction on coarse grids. Even with limited training data (e.g., only a few
trajectories), our model could accelerate the simulation while maintaining a
high accuracy with superior generalizability. Experiments on different flow
systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver
achieves state-of-the-art performance, surpassing baseline models with notable
margins.

</details>


### [140] [DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism](https://arxiv.org/abs/2507.01982)
*Siqing Long,Xiangzhi Huang,Jiemin Xie,Ming Cai*

Main category: cs.LG

TL;DR: 本文提出DKGCM图卷积网络，通过结合基于时间相似性的空间聚类、FFT与Mamba的时间建模以及GRPO强化学习优化，显著提升交通需求预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确的交通需求预测对资源有效分配至关重要。然而，交通系统中复杂的时空关系极大地限制了现有预测模型的性能。

Method: 本文提出了DKGCM图卷积网络结构。在空间维度上，引入基于时间相似性的聚类图卷积方法DK-GCN，利用动态时间规整（DTW）和K-means聚类来分组交通节点，以更有效地捕获空间依赖性。在时间维度上，将快速傅里叶变换（FFT）集成到双向Mamba深度学习框架中，以捕获交通需求的时间依赖性。为进一步优化模型训练，引入GRPO强化学习策略以增强损失函数反馈机制。

Result: 广泛的实验表明，DKGCM模型在三个公共数据集上均优于多种先进方法，并取得了优异的预测结果。

Conclusion: DKGCM模型通过创新的时空依赖建模和训练优化策略，有效克服了现有交通需求预测模型的性能局限，显著提升了预测精度。

Abstract: Accurate traffic demand forecasting enables transportation management
departments to allocate resources more effectively, thereby improving their
utilization efficiency. However, complex spatiotemporal relationships in
traffic systems continue to limit the performance of demand forecasting models.
To improve the accuracy of spatiotemporal traffic demand prediction, we propose
a new graph convolutional network structure called DKGCM. Specifically, we
first consider the spatial flow distribution of different traffic nodes and
propose a novel temporal similarity-based clustering graph convolution method,
DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering
to group traffic nodes and more effectively capture spatial dependencies. On
the temporal scale, we integrate the Fast Fourier Transform (FFT) within the
bidirectional Mamba deep learning framework to capture temporal dependencies in
traffic demand. To further optimize model training, we incorporate the GRPO
reinforcement learning strategy to enhance the loss function feedback
mechanism. Extensive experiments demonstrate that our model outperforms several
advanced methods and achieves strong results on three public datasets.

</details>


### [141] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: 本研究利用文本、图像和社交特征的多模态融合方法，结合无监督与有监督机器学习模型，检测社交媒体上的不实信息，并分析其传播模式，结果显示该方法显著优于单模态和双模态模型。


<details>
  <summary>Details</summary>
Motivation: 在选举和危机期间，社交媒体上充斥着大量不实信息。现有研究主要侧重于基于文本或图像的单模态不实信息检测，而对多模态特征组合（如整合文本和图像）的探索较少。

Method: 研究分析了从Twitter（现X）收集的1,529条包含文本和图像的推文（涵盖COVID-19大流行和选举时期）。采用早期融合方法，整合文本、图像和社交特征构建分类模型。通过目标检测和光学字符识别（OCR）等技术，对数据进行丰富以提取额外的社交和视觉特征。模型结合了无监督和有监督机器学习方法。

Result: 结合无监督和有监督机器学习模型的多模态方法，其分类性能比单模态模型提高了15%，比双模态模型提高了5%。此外，研究还分析了基于不实信息推文和传播者特征的不实信息传播模式。

Conclusion: 整合文本、图像和社交特征的多模态方法，并结合无监督和有监督机器学习模型，能有效提升不实信息检测的准确性。研究还对不实信息的传播模式提供了深入见解。

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [142] [Positive region preserved random sampling: an efficient feature selection method for massive data](https://arxiv.org/abs/2507.01998)
*Hexiang Bai,Deyu Li,Jiye Liang,Yanhui Zhai*

Main category: cs.LG

TL;DR: 本文提出了一种基于采样技术和粗糙集理论的新颖特征选择方法，旨在解决海量数据背景下计算资源有限的特征选择挑战。该方法通过构建正域保留样本，能在个人电脑上高效地选择出具有高判别能力的特征子集，并能估计其判别能力。


<details>
  <summary>Details</summary>
Motivation: 智能机器要最大化成功几率，选择相关特征是重要且必要的步骤。然而，面对海量数据时，智能机器通常计算资源不足，这使得特征选择成为一大挑战。

Method: 本文提出了一种基于采样技术和粗糙集理论的新方法。该方法引入“可分辨对象对占所有应区分对象对的比例”来衡量特征集的判别能力，并基于此度量，通过从海量数据中构建正域保留样本来寻找具有高判别能力的特征子集。

Result: 实验结果表明，该方法能在很短时间内找到近似核（approximate reducts）。最终核的判别能力大于估计的下限。对于大规模数据集，也能在个人电脑上于合理时间内获得具有高判别能力的近似核。该方法能在可接受时间内选出保留海量数据所有特征判别能力的子集，且能预估判别能力下限。

Conclusion: 所提出的基于采样和粗糙集理论的特征选择方法，有效解决了海量数据特征选择的挑战。它能够在计算资源有限的个人电脑上，快速且高效地选择出具有高判别能力的特征子集，同时能够对判别能力进行有效估计。

Abstract: Selecting relevant features is an important and necessary step for
intelligent machines to maximize their chances of success. However, intelligent
machines generally have no enough computing resources when faced with huge
volume of data. This paper develops a new method based on sampling techniques
and rough set theory to address the challenge of feature selection for massive
data. To this end, this paper proposes using the ratio of discernible object
pairs to all object pairs that should be distinguished to measure the
discriminatory ability of a feature set. Based on this measure, a new feature
selection method is proposed. This method constructs positive region preserved
samples from massive data to find a feature subset with high discriminatory
ability. Compared with other methods, the proposed method has two advantages.
First, it is able to select a feature subset that can preserve the
discriminatory ability of all the features of the target massive data set
within an acceptable time on a personal computer. Second, the lower boundary of
the probability of the object pairs that can be discerned using the feature
subset selected in all object pairs that should be distinguished can be
estimated before finding reducts. Furthermore, 11 data sets of different sizes
were used to validate the proposed method. The results show that approximate
reducts can be found in a very short period of time, and the discriminatory
ability of the final reduct is larger than the estimated lower boundary.
Experiments on four large-scale data sets also showed that an approximate
reduct with high discriminatory ability can be obtained in reasonable time on a
personal computer.

</details>


### [143] [Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2507.01999)
*Bappaditya Dey,Daniel Sorensen,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: 该论文提出一种针对半导体制造多元时间序列数据异常检测的新方法。它通过连续小波变换将时间序列数据转换为图像，然后利用微调的VGG-16作为骨干构建孪生网络，通过图像特征比对实现高精度异常识别。


<details>
  <summary>Details</summary>
Motivation: 半导体制造过程极其复杂，多元时间序列数据（MTS）在实时监控、故障检测和预测性维护中至关重要。然而，MTS数据在半导体制造中的异常预测面临高维度、严重的类别不平衡、噪声、数据缺失、系统非平稳性、变量间复杂依赖以及故障延迟出现等诸多挑战。

Method: 该方法包含三个主要步骤：a) 使用连续小波变换(CWT)将MTS数据转换为图像表示；b) 通过在定制CWT图像数据集上微调预训练的VGG-16架构，开发多类别图像分类器；c) 构建一个由两个相同子网络组成的孪生网络，每个子网络都使用微调后的VGG-16作为骨干，输入CWT图像对（一个参考，一个查询），通过比较两者的嵌入来确定它们在给定时间步是否属于同一类别。

Result: 该方法在真实的半导体晶圆厂(FAB)过程时间序列数据集上，在识别异常方面表现出高精度，为过程和工具追踪数据的离线异常检测提供了有前景的解决方案。

Conclusion: 所提出的方法通用且灵活，可以应用于监督和半监督设置，为复杂半导体制造环境下的异常检测提供了有效途径。

Abstract: Semiconductor manufacturing is an extremely complex process, characterized by
thousands of interdependent parameters collected across diverse tools and
process steps. Multi-variate time-series (MTS) analysis has emerged as a
critical methodology for enabling real-time monitoring, fault detection, and
predictive maintenance in such environments. However, anomaly prediction in
semiconductor fabrication presents several critical challenges, including high
data dimensionality, severe class imbalance due to the rarity of true faults,
noisy and missing measurements, and non-stationary behavior of production
systems. Furthermore, the complex interdependencies between variables and the
delayed emergence of faults across downstream stages complicate both anomaly
detection and root-cause-analysis. This paper presents a novel and generic
approach for anomaly detection in MTS data using machine learning. The proposed
methodology consists of three main steps: a) converting MTS data into
image-based representations using the Continuous Wavelet Transform, b)
developing a multi-class image classifier by fine-tuning a pretrained VGG-16
architecture on custom CWT image datasets, and c) constructing a Siamese
network composed of two identical sub-networks, each utilizing the fine-tuned
VGG-16 as a backbone. The network takes pairs of CWT images as input -one
serving as a reference or anchor (representing a known-good signal), and the
other as a query (representing an unknown signal). The model then compares the
embeddings of both inputs to determine whether they belong to the same class at
a given time step. Our approach demonstrates high accuracy in identifying
anomalies on a real FAB process time-series dataset, offering a promising
solution for offline anomaly detection in process and tool trace data.
Moreover, the approach is flexible and can be applied in both supervised and
semi-supervised settings.

</details>


### [144] [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001)
*Anurag Arnab,Ahmet Iscen,Mathilde Caron,Alireza Fathi,Cordelia Schmid*

Main category: cs.LG

TL;DR: 提出Temporal Chain of Thought推理策略，通过VLM自身迭代筛选相关帧来优化长视频问答的输入上下文，实现了最先进的性能，尤其在长视频处理上表现出色，证明推理时投入更多计算选择上下文的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLMs）取得了进展，但长视频理解仍然是挑战。现有长上下文VLMs难以有效利用其长序列，易受上下文窗口中无关干扰信息的影响。

Method: 提出Temporal Chain of Thought（TCoT）推理策略。该方法利用VLM自身迭代地识别并提取视频中最相关的帧，并将这些精选帧作为输入上下文用于回答问题，从而优化模型对上下文的利用。

Result: 该方法在4个不同的视频问答数据集上取得了最先进的性能，并对3种不同的VLM显示出一致的改进。特别是在LVBench上处理超过1小时的更长视频时，本方法使用32K上下文窗口的性能比使用700K上下文窗口的标准推理方式高出2.8点，表明其在长视频处理上通过高效上下文利用的显著优势。

Conclusion: 通过在推理时投入更多计算来精选最相关的上下文，可以显著提高准确性，并有效解决现有VLM在长视频理解中上下文利用不足及无关信息干扰的问题。该策略在处理长视频、尤其是在有限上下文窗口下，表现出卓越的有效性。

Abstract: Despite recent advances in Vision-Language Models (VLMs), long-video
understanding remains a challenging problem. Although state-of-the-art
long-context VLMs can process around 1000 input frames, they still struggle to
effectively leverage this sequence length, and succumb to irrelevant
distractors within the context window. We present Temporal Chain of Thought, an
inference strategy for video question-answering that curates the model's input
context. We use the VLM itself to iteratively identify and extract the most
relevant frames from the video, which are then used for answering. We
demonstrate how leveraging more computation at inference-time to select the
most relevant context leads to improvements in accuracy, in agreement with
recent work on inference-time scaling of LLMs. Moreover, we achieve
state-of-the-art results on 4 diverse video question-answering datasets,
showing consistent improvements with 3 different VLMs. In particular, our
method shines on longer videos which would not otherwise fit within the model's
context window: On longer videos of more than 1 hour on LVBench, our approach
using a context window of 32K outperforms the same VLM using standard inference
with a 700K context window by 2.8 points.

</details>


### [145] [AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design](https://arxiv.org/abs/2507.02006)
*Shakya Jayakody,Youpeng Zhao,Jun Wang*

Main category: cs.LG

TL;DR: AIRES是一个算法与系统协同设计方案，通过优化稀疏矩阵数据对齐和分层内存数据传输，显著加速了GCN中大规模非核SpGEMM的计算。


<details>
  <summary>Details</summary>
Motivation: 图卷积网络（GCN）中大规模稀疏通用矩阵乘法（SpGEMM）因GPU内存限制需进行非核（out-of-core）计算，现有方法存在高I/O延迟和GPU利用率低的问题。主要瓶颈在于稀疏格式数据对齐和内存分配。

Method: 提出AIRES，一个算法-系统协同设计方案。算法层面，通过块级优化解决稀疏格式矩阵数据对齐问题，并开发平铺算法促进行块对齐。系统层面，采用三阶段动态调度和双向数据传输策略，整合GPU内存、GPU Direct Storage (GDS) 和主机内存，构建分层内存系统以降低I/O延迟和提高吞吐量。

Result: AIRES在实际图处理基准测试中显著优于现有最先进方法，实现了高达1.8倍的延迟降低。

Conclusion: AIRES通过其创新的算法与系统协同设计，有效解决了GCN中大规模非核SpGEMM计算的性能瓶颈，显著提高了计算效率并降低了延迟，证明了其作为高性能解决方案的有效性。

Abstract: Graph convolutional networks (GCNs) are fundamental in various scientific
applications, ranging from biomedical protein-protein interactions (PPI) to
large-scale recommendation systems. An essential component for modeling graph
structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As
the size of graph data continues to scale up, SpGEMMs are often conducted in an
out-of-core fashion due to limited GPU memory space in resource-constrained
systems. Albeit recent efforts that aim to alleviate the memory constraints of
out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory
layout, or performing the computation in sparse format, current systems suffer
from both high I/O latency and GPU under-utilization issues.
  In this paper, we first identify the problems of existing systems, where
sparse format data alignment and memory allocation are the main performance
bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to
accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the
algorithm angle, AIRES proposes to alleviate the data alignment issues on the
block level for matrices in sparse formats and develops a tiling algorithm to
facilitate row block-wise alignment. On the system level, AIRES employs a
three-phase dynamic scheduling that features a dual-way data transfer strategy
utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage
(GDS), and host memory to reduce I/O latency and improve throughput.
Evaluations show that AIRES significantly outperforms the state-of-the-art
methods, achieving up to 1.8x lower latency in real-world graph processing
benchmarks.

</details>


### [146] [GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters](https://arxiv.org/abs/2507.02085)
*Wanjia Zhao,Jiaqi Han,Siyi Gu,Mingjian Jiang,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: GeoAda是一个SE(3)等变适配器框架，旨在高效微调几何扩散模型以实现受控生成任务，同时保持几何一致性并减轻过拟合和灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 几何扩散模型在分子动力学和结构生成中表现卓越，但如何高效且灵活地微调它们以适应具有不同几何控制的下游任务，仍是未被充分探索的挑战。

Method: 本文提出SE(3)等变适配器框架GeoAda。它引入结构化适配器设计：控制信号首先通过耦合算子编码，然后由选定的预训练模型层的可训练副本处理，最后通过解耦算子和等变零初始化卷积投射回。仅微调这些轻量级适配器模块，并理论证明了其保持SE(3)等变性。

Result: GeoAda在多种几何控制类型（如框架控制、全局控制、子图控制）和广泛应用领域（如粒子动力学、分子动力学、人体运动预测、分子生成）中展现了广泛适用性。实证结果表明，GeoAda实现了最先进的微调性能，同时保持了原始任务的准确性，且优于其他基线方法，有效避免了过拟合和灾难性遗忘。

Conclusion: GeoAda提供了一种灵活、参数高效且性能卓越的几何扩散模型微调解决方案，用于受控生成任务，成功解决了现有方法在保持几何一致性和避免性能下降方面的挑战。

Abstract: Geometric diffusion models have shown remarkable success in molecular
dynamics and structure generation. However, efficiently fine-tuning them for
downstream tasks with varying geometric controls remains underexplored. In this
work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables
flexible and parameter-efficient fine-tuning for controlled generative tasks
without modifying the original model architecture. GeoAda introduces a
structured adapter design: control signals are first encoded through coupling
operators, then processed by a trainable copy of selected pretrained model
layers, and finally projected back via decoupling operators followed by an
equivariant zero-initialized convolution. By fine-tuning only these lightweight
adapter modules, GeoAda preserves the model's geometric consistency while
mitigating overfitting and catastrophic forgetting. We theoretically prove that
the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric
inductive biases of the pretrained diffusion model remain intact during
adaptation. We demonstrate the wide applicability of GeoAda across diverse
geometric control types, including frame control, global control, subgraph
control, and a broad range of application domains such as particle dynamics,
molecular dynamics, human motion prediction, and molecule generation. Empirical
results show that GeoAda achieves state-of-the-art fine-tuning performance
while preserving original task accuracy, whereas other baselines experience
significant performance degradation due to overfitting and catastrophic
forgetting.

</details>


### [147] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: 该研究对比了通用大型语言模型（LLMs）与一个专有领域特定招聘模型（Match Score）在人才匹配中的表现，发现Match Score在准确性和公平性方面均显著优于通用LLMs，并强调在招聘等高风险领域部署AI时，领域特定建模和偏差审计的重要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在招聘领域有简化流程的潜力，但其准确性和算法偏见问题令人担忧。因此，有必要评估LLMs在招聘中的表现，并与领域特定模型进行比较，以确保在缺乏足够保障措施时，不会放大社会偏见。

Method: 该研究基准测试了包括OpenAI、Anthropic、Google、Meta和Deepseek在内的多种SOTA基础LLMs，并将其与一个专有的领域特定招聘模型（Match Score）进行比较。实验在一个包含约10,000对真实世界候选人-职位的数据集上进行。评估指标包括预测准确性（ROC AUC、精确召回率AUC、F1分数）和公平性（按性别、种族和交叉亚组的截止分析影响比）。

Result: 实验结果显示，Match Score在准确性上优于通用LLMs（ROC AUC 0.85 vs 0.77），并在不同人口统计群体间实现了更公平的结果。具体而言，Match Score的最低种族影响比达到0.957（接近均等），而最佳LLMs仅为0.809或更低（交叉亚组分别为0.906 vs 0.773）。研究指出，LLMs的预训练偏见可能导致其在招聘场景中传播社会偏见，而定制的监督模型能更有效地缓解这些偏见。此外，研究证明在招聘中准确性和公平性并非二元对立，精心设计的算法可以同时实现两者。

Conclusion: 研究强调了在招聘等高风险领域部署AI时，进行领域特定建模和偏差审计的重要性，并警示在没有充分公平性保障措施的情况下，不应依赖现成的LLMs执行此类任务。结论表明，精心设计的算法可以同时实现招聘的准确性和结果的公平性。

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [148] [Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model](https://arxiv.org/abs/2507.02089)
*Xingtu Liu,Lin F. Yang,Sharan Vaswani*

Main category: cs.LG

TL;DR: 该研究提出了一个解决约束马尔可夫决策过程（CMDPs）的原始-对偶框架，并分析了其在两种可行性设置下的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 在无限时域的CMDPs中，目标是在满足预期累积约束的同时最大化预期累积奖励。有效且高效地解决这类问题是当前研究面临的挑战。

Method: 提出了一个原始-对偶框架，该框架能够利用任何黑盒无约束MDP求解器。具体地，对于线性CMDPs，该框架结合了镜像下降值迭代（MDVI）作为MDP求解器。

Result: 1. 在松弛可行性（允许小幅约束违反）下，算法能以 $	ilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$ 样本实现 $\epsilon$-最优策略，样本复杂度对特征维度 $d$ 和精度 $\epsilon$ 接近最优。
2. 在严格可行性（要求精确满足约束）下，算法需要 $\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$ 样本，其中 $\zeta$ 是问题相关的Slater常数。
3. 该框架在表格CMDPs设置下也能恢复接近最优的样本复杂度。

Conclusion: 所提出的原始-对偶框架为CMDPs提供了一种有效的解决方案，并在松弛和严格可行性约束下均展示出有竞争力的样本复杂度性能，证明了其在不同CMDPs设置下的广泛适用性。

Abstract: We consider infinite-horizon $\gamma$-discounted (linear) constrained Markov
decision processes (CMDPs) where the objective is to find a policy that
maximizes the expected cumulative reward subject to expected cumulative
constraints. Given access to a generative model, we propose to solve CMDPs with
a primal-dual framework that can leverage any black-box unconstrained MDP
solver. For linear CMDPs with feature dimension $d$, we instantiate the
framework by using mirror descent value iteration
(\texttt{MDVI})~\citep{kitamura2023regularization} an example MDP solver. We
provide sample complexity bounds for the resulting CMDP algorithm in two cases:
(i) relaxed feasibility, where small constraint violations are allowed, and
(ii) strict feasibility, where the output policy is required to exactly satisfy
the constraint. For (i), we prove that the algorithm can return an
$\epsilon$-optimal policy with high probability by using
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$ samples. We note
that these results exhibit a near-optimal dependence on both $d$ and
$\epsilon$. For (ii), we show that the algorithm requires
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$ samples,
where $\zeta$ is the problem-dependent Slater constant that characterizes the
size of the feasible region. Finally, we instantiate our framework for tabular
CMDPs and show that it can be used to recover near-optimal sample complexities
in this setting.

</details>


### [149] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: 本文提出能量基Transformer（EBTs），一种新型能量基模型，通过无监督学习实现类似人类System 2思考的能力。EBTs能够显式验证输入与预测的兼容性，并将预测建模为能量最小化问题，在多模态任务上展现出更快的训练扩展性、更强的推理性能和更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有类System 2思考技术存在局限性，如局限于特定模态或问题，或需要额外的监督/训练。研究动机是探索是否能通过纯粹的无监督学习，泛化这些System 2思考方法，并开发出能自主学习思考的模型。

Method: 本文提出能量基Transformer（EBTs），它是一种新型的能量基模型（EBMs）。EBTs的核心思想是学习显式验证输入与候选预测之间的兼容性，并将预测问题重新定义为相对于该验证器的优化问题。具体来说，EBTs为每个输入和候选预测对分配一个能量值，并通过基于梯度下降的能量最小化过程来实现预测，直至收敛。

Result: ['在训练阶段，EBTs比主流Transformer++方法扩展速度更快，在数据、批量大小、参数、FLOPs和深度方面，扩展速率提高了35%。', '在推理阶段，EBTs在语言任务上通过System 2思考将性能提升比Transformer++多29%，并在图像去噪方面优于Diffusion Transformers，同时使用更少的前向传播。', '在相同或更差的预训练性能下，EBTs在大多数下游任务上取得了比现有模型更好的结果，表明其泛化能力优于现有方法。']

Conclusion: EBTs为扩展模型的学习和思考能力提供了一个有前景的新范式。

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [150] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2507.02109)
*Florian Grötschla,Luca A. Lanzendörfer,Longxiang Jiao,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 基于主动学习的PANAMA框架，利用类WaveNet模型高效训练端到端虚拟吉他放大器。


<details>
  <summary>Details</summary>
Motivation: 旨在通过最少的数据点（放大器旋钮设置）创建虚拟吉他放大器，解决传统方法数据需求量大的问题，提高训练效率。

Method: 提出PANAMA主动学习框架，采用类WaveNet架构训练参数化吉他放大器模型。通过基于梯度的优化算法来确定最佳采样数据点。

Result: 研究表明，基于梯度的优化算法可以有效地确定最佳采样数据点，并且该方法在样本数量受限的情况下表现出显著优势。

Conclusion: PANAMA框架提供了一种高效的虚拟吉他放大器模型训练方法，其主动学习策略通过智能选择样本，特别适用于数据量有限的场景。

Abstract: We introduce PANAMA, an active learning framework for the training of
end-to-end parametric guitar amp models using a WaveNet-like architecture. With
\model, one can create a virtual amp by recording samples that are determined
by an active learning strategy to use a minimum amount of datapoints (i.e., amp
knob settings). We show that gradient-based optimization algorithms can be used
to determine the optimal datapoints to sample, and that the approach helps
under a constrained number of samples.

</details>


### [151] [Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks](https://arxiv.org/abs/2507.02119)
*Shikai Qiu,Lechao Xiao,Andrew Gordon Wilson,Jeffrey Pennington,Atish Agarwala*

Main category: cs.LG

TL;DR: 研究发现，在计算资源和模型规模同步增长时，神经网络最优训练的损失曲线在归一化后呈现出惊人的一致性，甚至低于噪声水平，称之为“超塌缩”现象。这揭示了训练动态的普遍性，并提供了一个判断超参数是否良好缩放的指标。


<details>
  <summary>Details</summary>
Motivation: 探究在模型规模和训练时间同步增长时，何种缩放限制制约着神经网络的训练动态，以理解尽管架构、算法和数据之间存在复杂交互，是否存在普遍规律。

Method: 通过对不同大小模型在不同学习率调度、数据集和架构（包括Transformer）上的训练过程进行观察和实验，将训练计算量和损失归一化，以发现并验证损失曲线的“塌缩”和“超塌缩”现象。通过将塌缩现象与神经网络缩放定律中的幂律结构联系起来，并分析一个基于SGD噪声动态的简单模型来解释这些现象。

Result: 计算最优训练的模型表现出惊人的精确普适性：不同大小模型的损失曲线在训练计算量和损失归一化后塌缩到一条通用曲线上。当使用学习率衰减时，这种塌缩变得非常紧密，达到“超塌缩”，即归一化曲线间的差异低于单个损失曲线的随机种子噪声水平。该现象在不同学习率调度、数据集和架构上均被观察到，并且在超参数缩放不理想时会失效，从而提供了一个精确实用的良好缩放指标。通过幂律结构和SGD噪声动态模型对这些现象进行了合理解释。

Conclusion: 神经网络在计算最优训练下存在一种普遍的“超塌缩”行为，揭示了其训练动态的深层普适性。这一现象不仅有助于理解神经网络的缩放极限，也提供了一个有效的诊断工具，用于评估超参数缩放是否达到最优，具有重要的理论和实践意义。

Abstract: What scaling limits govern neural network training dynamics when model size
and training time grow in tandem? We show that despite the complex interactions
between architecture, training algorithms, and data, compute-optimally trained
models exhibit a remarkably precise universality. Specifically, loss curves
from models of varying sizes collapse onto a single universal curve when
training compute and loss are normalized to unity at the end of training. With
learning rate decay, the collapse becomes so tight that differences in the
normalized curves across models fall below the noise floor of individual loss
curves across random seeds, a phenomenon we term supercollapse. We observe
supercollapse across learning rate schedules, datasets, and architectures,
including transformers trained on next-token prediction, and find it breaks
down when hyperparameters are scaled suboptimally, providing a precise and
practical indicator of good scaling. We explain these phenomena by connecting
collapse to the power-law structure in typical neural scaling laws, and
analyzing a simple yet surprisingly effective model of SGD noise dynamics that
accurately predicts loss curves across various learning rate schedules and
quantitatively explains the origin of supercollapse.

</details>


### [152] [CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs](https://arxiv.org/abs/2507.02128)
*Jingyu Pan,Isaac Jacobson,Zheng Zhao,Tung-Chieh Chen,Guanglei Zhou,Chen-Chia Chang,Vineet Rashingkar,Yiran Chen*

Main category: cs.LG

TL;DR: CROP是一种基于LLM的自动VLSI设计流程调优框架，通过利用RAG和历史知识，显著提高了设计质量并减少了功耗。


<details>
  <summary>Details</summary>
Motivation: 现代VLSI设计优化面临巨大挑战，主要因为EDA算法复杂、参数空间庞大以及手动参数选择耗时且受限于专家经验。

Method: 本文提出了CROP框架，包含三个关键组成部分：1) 将RTL源代码转换为密集向量表示的可扩展方法；2) 基于嵌入的检索系统，用于匹配语义相似的电路；3) 结合检索增强生成(RAG)的LLM引导参数搜索系统，利用相似设计的先验知识约束搜索过程。

Result: 实验结果表明，CROP在工业设计上以更少的迭代次数实现了优于现有方法的成果质量(QoR)，包括功耗降低9.9%。

Conclusion: CROP框架通过结合LLM和检索增强技术，有效解决了VLSI设计流程中的优化难题，能够自动化并提升芯片设计的效率和质量。

Abstract: Modern very large-scale integration (VLSI) design requires the implementation
of integrated circuits using electronic design automation (EDA) tools. Due to
the complexity of EDA algorithms, the vast parameter space poses a huge
challenge to chip design optimization, as the combination of even moderate
numbers of parameters creates an enormous solution space to explore. Manual
parameter selection remains industrial practice despite being excessively
laborious and limited by expert experience. To address this issue, we present
CROP, the first large language model (LLM)-powered automatic VLSI design flow
tuning framework. Our approach includes: (1) a scalable methodology for
transforming RTL source code into dense vector representations, (2) an
embedding-based retrieval system for matching designs with semantically similar
circuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided
parameter search system that constrains the search process with prior knowledge
from similar designs. Experiment results demonstrate CROP's ability to achieve
superior quality-of-results (QoR) with fewer iterations than existing
approaches on industrial designs, including a 9.9% reduction in power
consumption.

</details>


### [153] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li,Liangji Zhu,Anand Rangarajan,Sanjay Ranka*

Main category: cs.LG

TL;DR: 本文提出一种高效的潜在扩散框架，通过结合变分自编码器和条件扩散模型，利用关键帧进行生成插值，克服了生成模型在数据压缩中控制性和重建准确性不足的问题，实现了高压缩率和精确重建。


<details>
  <summary>Details</summary>
Motivation: 生成模型在条件设置下表现出色，可视为数据压缩形式，但其有限的控制性和重建准确性限制了其在数据压缩中的实际应用。

Method: 提出一种高效的潜在扩散框架，结合了变分自编码器（VAE）与条件扩散模型。该方法仅将少量关键帧压缩到潜在空间，并将其作为条件输入，通过生成插值重建其余帧，从而无需存储每一帧的潜在表示。

Result: 该方法实现了精确的时空重建，并显著降低了存储成本。实验结果表明，与基于规则的最新压缩器（如SZ3）相比，压缩比提高了10倍；与领先的基于学习的方法相比，在相同重建误差下性能提高了63%。

Conclusion: 所提出的潜在扩散框架成功弥补了生成模型在数据压缩应用中的不足，通过创新的关键帧压缩和生成插值策略，实现了卓越的压缩效率和重建精度，使其在数据压缩领域具有广阔的应用前景。

Abstract: Generative models have demonstrated strong performance in conditional
settings and can be viewed as a form of data compression, where the condition
serves as a compact representation. However, their limited controllability and
reconstruction accuracy restrict their practical application to data
compression. In this work, we propose an efficient latent diffusion framework
that bridges this gap by combining a variational autoencoder with a conditional
diffusion model. Our method compresses only a small number of keyframes into
latent space and uses them as conditioning inputs to reconstruct the remaining
frames via generative interpolation, eliminating the need to store latent
representations for every frame. This approach enables accurate spatiotemporal
reconstruction while significantly reducing storage costs. Experimental results
across multiple datasets show that our method achieves up to 10 times higher
compression ratios than rule-based state-of-the-art compressors such as SZ3,
and up to 63 percent better performance than leading learning-based methods
under the same reconstruction error.

</details>


### [154] [Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks](https://arxiv.org/abs/2507.02151)
*Tuo Wang,Jian Kang,Yujun Yan,Adithya Kulkarni,Dawei Zhou*

Main category: cs.LG

TL;DR: NCPNET是一个为时间图神经网络设计的共形预测框架，解决了现有方法在静态图上的局限性，实现了在时间图上的可靠不确定性量化和效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNNs）的共形预测方法主要关注静态图，忽视了真实世界图的演变特性。时间依赖性违反了标准共形预测的交换性假设，限制了其在动态场景中的适用性。

Method: 本文提出了NCPNET，一个端到端的专为时间图设计的共形预测框架。该方法通过引入基于扩散的非一致性分数来捕捉拓扑和时间不确定性，并开发了一个效率感知的优化算法以提高计算效率和减少覆盖率违规。

Result: 在WIKI、REDDIT、DBLP和IBM反洗钱数据集等真实世界时间图上的广泛实验表明，NCPNET能够确保时间图的覆盖率，并在WIKI数据集上将预测集大小减少高达31%，与现有先进方法相比显著提高了效率。

Conclusion: NCPNET成功地将共形预测应用于动态图环境，有效缓解了时间依赖性导致的统计覆盖率问题，为GNNs在高风险应用中提供了更可靠的不确定性量化，并显著提高了效率。

Abstract: Conformal prediction for graph neural networks (GNNs) offers a promising
framework for quantifying uncertainty, enhancing GNN reliability in high-stakes
applications. However, existing methods predominantly focus on static graphs,
neglecting the evolving nature of real-world graphs. Temporal dependencies in
graph structure, node attributes, and ground truth labels violate the
fundamental exchangeability assumption of standard conformal prediction
methods, limiting their applicability. To address these challenges, in this
paper, we introduce NCPNET, a novel end-to-end conformal prediction framework
tailored for temporal graphs. Our approach extends conformal prediction to
dynamic settings, mitigating statistical coverage violations induced by
temporal dependencies. To achieve this, we propose a diffusion-based
non-conformity score that captures both topological and temporal uncertainties
within evolving networks. Additionally, we develop an efficiency-aware
optimization algorithm that improves the conformal prediction process,
enhancing computational efficiency and reducing coverage violations. Extensive
experiments on diverse real-world temporal graphs, including WIKI, REDDIT,
DBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to
ensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction
in prediction set size on the WIKI dataset, significantly improving efficiency
compared to state-of-the-art methods. Our data and code are available at
https://github.com/ODYSSEYWT/NCPNET.

</details>


### [155] [Statistical Inference for Responsiveness Verification](https://arxiv.org/abs/2507.02169)
*Seung Hyun Cheon,Meredith Stewart,Bogdan Kulynych,Tsui-Wei Weng,Berk Ustun*

Main category: cs.LG

TL;DR: 针对机器学习模型未考虑个体输入变化导致的安全故障，本文提出一种黑盒验证程序，用于评估预测响应性，以支持敏感性分析和故障概率估计，从而提高模型在现实应用中的安全性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在对个人进行预测（如贷款、招聘、内容审核）时，往往忽视个体输入变化，从而引发安全故障。

Method: 引入一种形式化的预测响应性验证程序，将其视为敏感性分析。该程序允许通过指定干预约束和下游效应分布来控制变化，并使用黑盒访问对任何模型和数据集进行响应性估计。开发了通过生成可达点均匀样本来构建这些估计的算法。

Result: 所提出的方法能有效估计模型响应性，并支持证伪和故障概率估计。在再犯预测、器官移植优先级和内容审核等实际应用中，证明了其有助于提升安全性。

Conclusion: 通过提供一种评估模型响应性的通用黑盒方法，本文能够帮助识别并减轻因个体输入变化导致的机器学习安全风险，从而增强模型在关键应用中的可靠性。

Abstract: Many safety failures in machine learning arise when models are used to assign
predictions to people (often in settings like lending, hiring, or content
moderation) without accounting for how individuals can change their inputs. In
this work, we introduce a formal validation procedure for the responsiveness of
predictions with respect to interventions on their features. Our procedure
frames responsiveness as a type of sensitivity analysis in which practitioners
control a set of changes by specifying constraints over interventions and
distributions over downstream effects. We describe how to estimate
responsiveness for the predictions of any model and any dataset using only
black-box access, and how to use these estimates to support tasks such as
falsification and failure probability estimation. We develop algorithms that
construct these estimates by generating a uniform sample of reachable points,
and demonstrate how they can promote safety in real-world applications such as
recidivism prediction, organ transplant prioritization, and content moderation.

</details>


### [156] [Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction](https://arxiv.org/abs/2507.02225)
*Jiyeon Bae,Hyeon Jeon,Jinwook Seo*

Main category: cs.LG

TL;DR: 提出一种基于经验相关性聚类选择降维评估指标的工作流，旨在减少评估偏差并提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 在评估降维投影时，若选择高度相关的指标（衡量相似结构特征），可能导致评估结果产生偏差，偏袒某些降维技术。

Method: 提出一种新颖的工作流：通过计算指标间的成对相关性来衡量相似度，对指标进行聚类以最小化重叠，并从每个簇中选择一个代表性指标。

Result: 定量实验证明，该方法提高了降维评估的稳定性。

Conclusion: 该工作流有助于减轻降维评估中的偏差。

Abstract: Evaluating the accuracy of dimensionality reduction (DR) projections in
preserving the structure of high-dimensional data is crucial for reliable
visual analytics. Diverse evaluation metrics targeting different structural
characteristics have thus been developed. However, evaluations of DR
projections can become biased if highly correlated metrics--those measuring
similar structural characteristics--are inadvertently selected, favoring DR
techniques that emphasize those characteristics. To address this issue, we
propose a novel workflow that reduces bias in the selection of evaluation
metrics by clustering metrics based on their empirical correlations rather than
on their intended design characteristics alone. Our workflow works by computing
metric similarity using pairwise correlations, clustering metrics to minimize
overlap, and selecting a representative metric from each cluster. Quantitative
experiments demonstrate that our approach improves the stability of DR
evaluation, which indicates that our workflow contributes to mitigating
evaluation bias.

</details>


### [157] [PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations](https://arxiv.org/abs/2507.02227)
*Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: 针对PDE的神经网络存在误差累积问题。PhysicsCorrect提出一种免训练修正框架，通过基于PDE残差的线性化逆问题和高效缓存策略，显著降低预测误差，且推理时间开销可忽略不计。


<details>
  <summary>Details</summary>
Motivation: 神经网络在求解偏微分方程（PDEs）时，在长期推演过程中存在误差累积问题，导致预测结果偏离物理有效解。

Method: 提出PhysicsCorrect框架，它是一种免训练的校正方法。该方法通过将校正表述为基于PDE残差的线性化逆问题，在每个预测步骤强制执行PDE一致性。其核心创新是高效缓存策略，在离线预热阶段预计算雅可比矩阵及其伪逆，显著降低计算开销。

Result: 在Navier-Stokes流体动力学、波动方程和Kuramoto-Sivashinsky方程等三种代表性PDE系统上，PhysicsCorrect将预测误差降低了多达100倍，同时推理时间开销可忽略不计（低于5%）。该框架可与傅里叶神经算子、UNets和Vision Transformers等多种网络架构无缝集成。

Conclusion: PhysicsCorrect成功将不稳定的神经网络代理模型转化为可靠的仿真工具，弥合了深度学习的计算效率与实际科学应用所需的物理保真度之间的鸿沟。

Abstract: Neural networks have emerged as powerful surrogates for solving partial
differential equations (PDEs), offering significant computational speedups over
traditional methods. However, these models suffer from a critical limitation:
error accumulation during long-term rollouts, where small inaccuracies compound
exponentially, eventually causing complete divergence from physically valid
solutions. We present PhysicsCorrect, a training-free correction framework that
enforces PDE consistency at each prediction step by formulating correction as a
linearized inverse problem based on PDE residuals. Our key innovation is an
efficient caching strategy that precomputes the Jacobian and its pseudoinverse
during an offline warm-up phase, reducing computational overhead by two orders
of magnitude compared to standard correction approaches. Across three
representative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and
the chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction
errors by up to 100x while adding negligible inference time (under 5\%). The
framework integrates seamlessly with diverse architectures including Fourier
Neural Operators, UNets, and Vision Transformers, effectively transforming
unstable neural surrogates into reliable simulation tools that bridge the gap
between deep learning's computational efficiency and the physical fidelity
demanded by practical scientific applications.

</details>


### [158] [VERBA: Verbalizing Model Differences Using Large Language Models](https://arxiv.org/abs/2507.02241)
*Shravan Doda,Shashidhar Reddy Javaji,Zining Zhu*

Main category: cs.LG

TL;DR: 针对模型选择困难问题，本文提出VERBA，利用大型语言模型（LLM）自动生成机器学习模型间的行为差异描述，有效提高了模型透明度和可比性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习领域存在“模型湖”现象，即针对同一任务有大量性能相似但行为各异的模型。用户难以选择，且手动进行模型间的两两比较以生成文档的工作量（$O(N^2)$）过于巨大，难以实现。

Method: 本文引入VERBA系统，该方法利用大型语言模型（LLM）通过对两个模型进行采样，从而生成它们之间差异的语言描述。研究者还建立了一套协议，通过模拟来评估这些描述的信息量，并构建了一个包含多样化常用机器学习模型的基准套件。

Result: 对于性能差异在5%以内但行为差异达到20-25%的决策树模型对，VERBA能有效语言描述其差异，整体准确率可达80%。当模型结构信息被纳入时，语言描述的准确率进一步提升至90%。

Conclusion: VERBA为事后（post-hoc）提高机器学习模型的透明度和可比性开辟了新的研究方向。

Abstract: In the current machine learning landscape, we face a "model lake" phenomenon:
Given a task, there is a proliferation of trained models with similar
performances despite different behavior. For model users attempting to navigate
and select from the models, documentation comparing model pairs is helpful.
However, for every $N$ models there could be $O(N^2)$ pairwise comparisons, a
number prohibitive for the model developers to manually perform pairwise
comparisons and prepare documentations. To facilitate fine-grained pairwise
comparisons among models, we introduced $\textbf{VERBA}$. Our approach
leverages a large language model (LLM) to generate verbalizations of model
differences by sampling from the two models. We established a protocol that
evaluates the informativeness of the verbalizations via simulation. We also
assembled a suite with a diverse set of commonly used machine learning models
as a benchmark. For a pair of decision tree models with up to 5% performance
difference but 20-25% behavioral differences, $\textbf{VERBA}$ effectively
verbalizes their variations with up to 80% overall accuracy. When we included
the models' structural information, the verbalization's accuracy further
improved to 90%. $\textbf{VERBA}$ opens up new research avenues for improving
the transparency and comparability of machine learning models in a post-hoc
manner.

</details>


### [159] [Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies](https://arxiv.org/abs/2507.02244)
*Fangzhou Shi,Xiaopeng Ke,Xinye Xiong,Kexin Meng,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: 针对网约车平台竞争性排名下优惠券策略的挑战，本文提出FCA-RL强化学习框架，旨在快速适应竞争并兼顾预算限制。通过引入RideGym仿真环境，实验证明FCA-RL在订单获取和预算优化方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 网约车聚合平台上的竞争排名机制（低票价排名更高）促使服务提供商采用优惠券策略来获取更多订单，而订单量直接影响其长期生存能力。然而，设计能在预算限制下动态适应市场波动并优化订单获取的有效优惠券策略，目前研究仍很匮ast。

Method: 本文提出FCA-RL，一种基于强化学习的补贴策略框架。该框架融合两项关键技术：快速竞争适应（FCA）以迅速响应动态价格变化，以及强化拉格朗日调整（RLA）以在优化优惠券决策时确保预算合规性。此外，本文引入了RideGym，首个专为网约车聚合平台设计的模拟环境，用于全面评估和基准测试不同的定价策略。

Result: 实验结果表明，所提出的FCA-RL方法在各种市场条件下持续优于基线方法。

Conclusion: FCA-RL在网约车服务提供商的补贴优化方面表现出其有效性，成功应对了动态市场竞争和预算约束下的优惠券策略设计挑战。

Abstract: The proliferation of ride-hailing aggregator platforms presents significant
growth opportunities for ride-service providers by increasing order volume and
gross merchandise value (GMV). On most ride-hailing aggregator platforms,
service providers that offer lower fares are ranked higher in listings and,
consequently, are more likely to be selected by passengers. This competitive
ranking mechanism creates a strong incentive for service providers to adopt
coupon strategies that lower prices to secure a greater number of orders, as
order volume directly influences their long-term viability and sustainability.
Thus, designing an effective coupon strategy that can dynamically adapt to
market fluctuations while optimizing order acquisition under budget constraints
is a critical research challenge. However, existing studies in this area remain
scarce.
  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based
subsidy strategy framework designed to rapidly adapt to competitors' pricing
adjustments. Our approach integrates two key techniques: Fast Competition
Adaptation (FCA), which enables swift responses to dynamic price changes, and
Reinforced Lagrangian Adjustment (RLA), which ensures adherence to budget
constraints while optimizing coupon decisions on new price landscape.
Furthermore, we introduce RideGym, the first dedicated simulation environment
tailored for ride-hailing aggregators, facilitating comprehensive evaluation
and benchmarking of different pricing strategies without compromising
real-world operational efficiency. Experimental results demonstrate that our
proposed method consistently outperforms baseline approaches across diverse
market conditions, highlighting its effectiveness in subsidy optimization for
ride-hailing service providers.

</details>


### [160] [Uncertainty-aware Reward Design Process](https://arxiv.org/abs/2507.02256)
*Yang Yang,Xiaolu Zhou,Bosong Ding,Miao Xin*

Main category: cs.LG

TL;DR: 本文提出URDP框架，通过结合大语言模型（LLMs）的逻辑推理和不确定性感知贝叶斯优化，显著提升了强化学习中奖励函数设计的质量与效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习中有效的奖励函数设计极具挑战性，传统方法效率低下且不一致。现有LLM自动化设计方法在数值优化方面表现不佳，而进化搜索则资源利用率低、计算开销大。

Method: 我们提出了不确定性感知奖励设计过程（URDP）框架，利用自洽性分析量化候选奖励函数的不确定性，实现无需模拟的无效组件识别和新组件发现。同时，引入不确定性感知贝叶斯优化（UABO）提升超参数配置效率。最终，通过解耦奖励组件优化和超参数调优，构建了结合LLM逻辑推理和贝叶斯优化数值优势的双层优化架构。

Result: 在3个基准环境的35个多样化任务上进行的综合评估表明，URDP不仅生成了更高质量的奖励函数，而且相比现有方法显著提升了自动化奖励设计的效率。

Conclusion: URDP框架通过创新性地整合LLM的推理能力和贝叶斯优化的数值优势，成功克服了强化学习中奖励函数设计效率和质量的挑战，为自动化奖励设计提供了高效且高质量的解决方案。

Abstract: Designing effective reward functions is a cornerstone of reinforcement
learning (RL), yet it remains a challenging process due to the inefficiencies
and inconsistencies inherent in conventional reward engineering methodologies.
Recent advances have explored leveraging large language models (LLMs) to
automate reward function design. However, their suboptimal performance in
numerical optimization often yields unsatisfactory reward quality, while the
evolutionary search paradigm demonstrates inefficient utilization of simulation
resources, resulting in prohibitively lengthy design cycles with
disproportionate computational overhead. To address these challenges, we
propose the Uncertainty-aware Reward Design Process (URDP), a novel framework
that integrates large language models to streamline reward function design and
evaluation in RL environments. URDP quantifies candidate reward function
uncertainty based on self-consistency analysis, enabling simulation-free
identification of ineffective reward components while discovering novel reward
components. Furthermore, we introduce uncertainty-aware Bayesian optimization
(UABO), which incorporates uncertainty estimation to significantly enhance
hyperparameter configuration efficiency. Finally, we construct a bi-level
optimization architecture by decoupling the reward component optimization and
the hyperparameter tuning. URDP orchestrates synergistic collaboration between
the reward logic reasoning of the LLMs and the numerical optimization strengths
of the Bayesian Optimization. We conduct a comprehensive evaluation of URDP
across 35 diverse tasks spanning three benchmark environments. Our experimental
results demonstrate that URDP not only generates higher-quality reward
functions but also achieves significant improvements in the efficiency of
automated reward design compared to existing approaches.

</details>


### [161] [Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications](https://arxiv.org/abs/2507.02291)
*Zhaoyu Zhang,Lingyi Wang,Wei Wu,Fuhui Zhou,Qihui Wu*

Main category: cs.LG

TL;DR: 提出了一种基于知识图谱增强的零样本语义通信（KGZS-SC）网络，解决了传统数据驱动语义通信在处理未见数据时泛化性和可解释性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的语义通信依赖表层统计模式，导致其缺乏可解释性和泛化能力，特别是在存在未见数据时表现不佳。

Method: 本文提出KGZS-SC网络，利用基于知识图谱的语义知识库（KG-SKB）提供结构化语义信息，以实现泛化语义表示并支持未见情况的推理。具体而言，KG-SKB在共享类别语义嵌入空间中对齐语义特征，增强发射器的泛化能力并减少通信开销。接收端则利用零样本学习（ZSL）直接对未见情况进行分类，无需重新训练或额外计算开销。

Result: 在APY数据集上的仿真结果表明，所提出的KGZS-SC网络展现出鲁棒的泛化能力，并在不同信噪比水平下，在未见类别分类方面显著优于现有语义通信框架。

Conclusion: KGZS-SC网络通过结合知识图谱和零样本学习，有效提升了语义通信在未见数据场景下的泛化能力、适应性和效率。

Abstract: Data-driven semantic communication is based on superficial statistical
patterns, thereby lacking interpretability and generalization, especially for
applications with the presence of unseen data. To address these challenges, we
propose a novel knowledge graph-enhanced zero-shot semantic communication
(KGZS-SC) network. Guided by the structured semantic information from a
knowledge graph-based semantic knowledge base (KG-SKB), our scheme provides
generalized semantic representations and enables reasoning for unseen cases.
Specifically, the KG-SKB aligns the semantic features in a shared category
semantics embedding space and enhances the generalization ability of the
transmitter through aligned semantic features, thus reducing communication
overhead by selectively transmitting compact visual semantics. At the receiver,
zero-shot learning (ZSL) is leveraged to enable direct classification for
unseen cases without the demand for retraining or additional computational
overhead, thereby enhancing the adaptability and efficiency of the
classification process in dynamic or resource-constrained environments. The
simulation results conducted on the APY datasets show that the proposed KGZS-SC
network exhibits robust generalization and significantly outperforms existing
SC frameworks in classifying unseen categories across a range of SNR levels.

</details>


### [162] [Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment](https://arxiv.org/abs/2507.02310)
*Alif Ashrafee,Jedrzej Kozal,Michal Wozniak,Bartosz Krawczyk*

Main category: cs.LG

TL;DR: 传统持续学习忽略了概念漂移，本文提出一种轻量级方法AMR，通过自适应记忆重排来应对数据漂移，在减少计算和标注开销的同时，保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法侧重于知识保留和减轻灾难性遗忘，但默认数据分布是静态的，这忽略了真实世界数据流中普遍存在的概念漂移，即旧任务数据会永久性改变，导致模型需要同时具备稳定性和快速适应性。

Method: 引入了模拟真实场景的概念漂移持续学习框架。作为基线，采用“完全再学习”(FR)但其成本高昂。针对此，提出“自适应记忆重排”(AMR)方法，它是一种轻量级替代方案，为基于回放的学习器配备漂移感知适应机制。AMR选择性地从回放缓冲区中移除过时的漂移类别样本，并用少量最新实例重新填充，以使内存与新分布对齐。同时，为促进可复现评估，创建了四种标准视觉基准的概念漂移变体数据集：Fashion-MNIST-CD、CIFAR10-CD、CIFAR100-CD和Tiny-ImageNet-CD。

Result: AMR在性能上与FR相当，但在标记数据和计算需求上显著降低了几个数量级。在多个回放基线上进行的综合实验表明，AMR能持续对抗概念漂移，以最小的开销保持高精度。

Conclusion: AMR被定位为一个可扩展的解决方案，能够在非平稳持续学习环境中兼顾模型的稳定性和可塑性。

Abstract: Traditional continual learning methods prioritize knowledge retention and
focus primarily on mitigating catastrophic forgetting, implicitly assuming that
the data distribution of previously learned tasks remains static. This
overlooks the dynamic nature of real-world data streams, where concept drift
permanently alters previously seen data and demands both stability and rapid
adaptation.
  We introduce a holistic framework for continual learning under concept drift
that simulates realistic scenarios by evolving task distributions. As a
baseline, we consider Full Relearning (FR), in which the model is retrained
from scratch on newly labeled samples from the drifted distribution. While
effective, this approach incurs substantial annotation and computational
overhead. To address these limitations, we propose Adaptive Memory Realignment
(AMR), a lightweight alternative that equips rehearsal-based learners with a
drift-aware adaptation mechanism. AMR selectively removes outdated samples of
drifted classes from the replay buffer and repopulates it with a small number
of up-to-date instances, effectively realigning memory with the new
distribution. This targeted resampling matches the performance of FR while
reducing the need for labeled data and computation by orders of magnitude.
  To enable reproducible evaluation, we introduce four concept-drift variants
of standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and
Tiny-ImageNet-CD, where previously seen classes reappear with shifted
representations. Comprehensive experiments on these datasets using several
rehearsal-based baselines show that AMR consistently counters concept drift,
maintaining high accuracy with minimal overhead. These results position AMR as
a scalable solution that reconciles stability and plasticity in non-stationary
continual learning environments.

</details>


### [163] [Improving Constrained Generation in Language Models via Self-Distilled Twisted Sequential Monte Carlo](https://arxiv.org/abs/2507.02315)
*Sooyeon Kim,Giung Nam,Juho Lee*

Main category: cs.LG

TL;DR: 通过自蒸馏迭代精炼基础模型，解决约束文本生成中因稀疏奖励导致的学习困难，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有约束文本生成方法在目标输出与基础模型不一致时，由于奖励信号稀疏且信息不足，面临学习困难。

Method: 采用自蒸馏（self-distillation）方法，迭代精炼基础模型，使其逐步与目标分布对齐。

Result: 该方法有效缓解了学习难题，使模型 progressively 更接近目标，并显著提升了生成质量。

Conclusion: 自蒸馏是提升约束文本生成在目标稀有场景下性能的有效策略，能够提高模型与目标的对齐度，带来显著的生成质量提升。

Abstract: Recent work has framed constrained text generation with autoregressive
language models as a probabilistic inference problem. Among these, Zhao et al.
(2024) introduced a promising approach based on twisted Sequential Monte Carlo,
which incorporates learned twist functions and twist-induced proposals to guide
the generation process. However, in constrained generation settings where the
target distribution concentrates on outputs that are unlikely under the base
model, learning becomes challenging due to sparse and uninformative reward
signals. We show that iteratively refining the base model through
self-distillation alleviates this issue by making the model progressively more
aligned with the target, leading to substantial gains in generation quality.

</details>


### [164] [Transformer-based EEG Decoding: A Survey](https://arxiv.org/abs/2507.02320)
*Haodong Zhang,Hongqi Li*

Main category: cs.LG

TL;DR: 本文综述了Transformer模型在脑电图（EEG）解码领域的最新应用进展，涵盖了其基本原理、混合架构、定制化结构、当前挑战及未来发展前景。


<details>
  <summary>Details</summary>
Motivation: 脑电图（EEG）是获取大脑电活动的重要信号，对其进行解码以获取用户意图是脑机接口（BCI）研究的核心。深度学习，特别是Transformer模型，通过其序列数据处理能力和注意力机制，正在彻底改变传统的EEG分析方法。因此，有必要对Transformer在EEG解码中的应用进行全面总结，以帮助研究人员了解现状并指明未来方向。

Method: 本研究为一篇综述性文章，通过以下方法组织内容：总结Transformer模型在EEG解码中的最新应用；跟踪模型架构的演变；阐述Transformer基础及其直接应用；详细概述Transformer与卷积神经网络、循环神经网络、图神经网络、脉冲神经网络、生成对抗网络、扩散模型等其他深度学习技术结合的混合架构；介绍定制化Transformer内部结构修改的研究进展；最后，讨论该领域当前的挑战和未来发展前景。

Result: 作为一篇综述，本文没有新的实验结果。其研究成果是对Transformer模型在EEG解码应用方面的全面梳理和总结，包括其多种架构演变、与其他深度学习技术的融合方式以及定制化结构的进展，并分析了该领域的挑战与机遇。这使得读者能清晰了解该领域的当前发展状态。

Conclusion: 本论文旨在帮助读者清晰了解Transformer模型在EEG解码应用中的当前状态，并为未来的研究工作提供宝贵的见解和方向，指出未来可能面临的挑战和发展前景。

Abstract: Electroencephalography (EEG) is one of the most common signals used to
capture the electrical activity of the brain, and the decoding of EEG, to
acquire the user intents, has been at the forefront of brain-computer/machine
interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods
with machine learning, the advent of deep learning approaches have gradually
revolutionized the field by providing an end-to-end long-cascaded architecture,
which can learn more discriminative features automatically. Among these,
Transformer is renowned for its strong handling capability of sequential data
by the attention mechanism, and the application of Transformers in various EEG
processing tasks is increasingly prevalent. This article delves into a relevant
survey, summarizing the latest application of Transformer models in EEG
decoding since it appeared. The evolution of the model architecture is followed
to sort and organize the related advances, in which we first elucidate the
fundamentals of the Transformer that benefits EEG decoding and its direct
application. Then, the common hybrid architectures by integrating basic
Transformer with other deep learning techniques
(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial
networks, diffusion models, etc.) is overviewed in detail. The research
advances of applying the modified intrinsic structures of customized
Transformer have also been introduced. Finally, the current challenges and
future development prospects in this rapidly evolving field are discussed. This
paper aims to help readers gain a clear understanding of the current state of
Transformer applications in EEG decoding and to provide valuable insights for
future research endeavors.

</details>


### [165] [DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values](https://arxiv.org/abs/2507.02342)
*Changhun Kim,Yechan Mun,Sangchul Hahn,Eunho Yang*

Main category: cs.LG

TL;DR: 提出DeltaSHAP，一种专为在线患者监护系统设计的、能解释时间序列预测变化的XAI算法，其在解释质量和计算效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法无法满足临床时间序列解释的独特需求，尤其是在线患者监测中，迫切需要解释连续预测变化、提供具有方向和幅度的特征归因，并实现实时洞察，以支持及时干预。

Method: 提出DeltaSHAP算法，该算法通过将Shapley值应用于时间设置，精确捕捉特征协同效应，并仅利用实际观测到的特征组合来归因预测变化，从而确保高效性。此外，研究还引入了评估在线时间序列归因忠实度的新指标。

Result: 在MIMIC-III失代偿基准测试中，DeltaSHAP在解释质量上比现有最佳XAI方法提高了62%，在计算效率上缩短了33%的时间，证明其显著优于现有方法。

Conclusion: DeltaSHAP成功解决了在线患者监护中XAI的独特挑战，通过提供实时、有方向和幅度的预测变化解释，显著提升了医疗领域可解释AI的实用性和性能。

Abstract: This study proposes DeltaSHAP, a novel explainable artificial intelligence
(XAI) algorithm specifically designed for online patient monitoring systems. In
clinical environments, discovering the causes driving patient risk evolution is
critical for timely intervention, yet existing XAI methods fail to address the
unique requirements of clinical time series explanation tasks. To this end,
DeltaSHAP addresses three key clinical needs: explaining the changes in the
consecutive predictions rather than isolated prediction scores, providing both
magnitude and direction of feature attributions, and delivering these insights
in real time. By adapting Shapley values to temporal settings, our approach
accurately captures feature coalition effects. It further attributes prediction
changes using only the actually observed feature combinations, making it
efficient and practical for time-sensitive clinical applications. We also
introduce new evaluation metrics to evaluate the faithfulness of the
attributions for online time series, and demonstrate through experiments on
online patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI
methods in both explanation quality as 62% and computational efficiency as 33%
time reduction on the MIMIC-III decompensation benchmark. We release our code
at https://github.com/AITRICS/DeltaSHAP.

</details>


### [166] [Offline Reinforcement Learning with Penalized Action Noise Injection](https://arxiv.org/abs/2507.02356)
*JunHyeok Oh,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 本文提出PANI，一种通过注入噪声并施加惩罚来增强离线强化学习的方法，旨在以简单高效的方式提升泛化能力，避免扩散模型的计算开销，并在多个基准测试中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习的泛化能力至关重要，尽管扩散模型在提升离线RL性能方面取得成功，但其推理计算成本高昂。研究者质疑扩散模型是否是高性能离线RL的必要条件，从而寻求更简单高效的替代方案。

Method: 本文提出“惩罚性动作噪声注入”（Penalized Action Noise Injection, PANI）方法。该方法通过在动作中注入噪声来覆盖整个动作空间，并根据注入的噪声量施加惩罚。PANI受扩散模型启发，并提供了理论基础，证明其解决了一个被称为“噪声动作MDP”的修正马尔可夫决策过程。PANI与多种现有离策略和离线RL算法兼容。

Result: 尽管PANI方法简单，但在各种基准测试中均表现出显著的性能提升。

Conclusion: PANI提供了一种简单、有效且具有理论基础的方法，可以在不引入扩散模型高计算成本的情况下，显著提升离线强化学习算法的泛化能力和性能。

Abstract: Offline reinforcement learning (RL) optimizes a policy using only a fixed
dataset, making it a practical approach in scenarios where interaction with the
environment is costly. Due to this limitation, generalization ability is key to
improving the performance of offline RL algorithms, as demonstrated by recent
successes of offline RL with diffusion models. However, it remains questionable
whether such diffusion models are necessary for highly performing offline RL
algorithms, given their significant computational requirements during
inference. In this paper, we propose Penalized Action Noise Injection (PANI), a
method that simply enhances offline learning by utilizing noise-injected
actions to cover the entire action space, while penalizing according to the
amount of noise injected. This approach is inspired by how diffusion models
have worked in offline RL algorithms. We provide a theoretical foundation for
this method, showing that offline RL algorithms with such noise-injected
actions solve a modified Markov Decision Process (MDP), which we call the noisy
action MDP. PANI is compatible with a wide range of existing off-policy and
offline RL algorithms, and despite its simplicity, it demonstrates significant
performance improvements across various benchmarks.

</details>


### [167] [Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations](https://arxiv.org/abs/2507.02365)
*Muhammad Usama,Dong Eui Chang*

Main category: cs.LG

TL;DR: 本文提出一种数据驱动框架，结合学习到的潜在信号表示和无模型强化学习，高效优化高速DRAM系统中的均衡器参数，显著提升信号完整性，并展现出优于现有技术的性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 高速DRAM系统中信号完整性的均衡器参数优化至关重要，然而现有方法通常计算量大或过于依赖模型，存在效率和普适性问题。

Method: 本研究引入了一个数据驱动框架：首先，利用学习到的潜在信号表示进行高效的信号完整性评估，该表示能捕捉关键信号特征，并作为直接眼图分析的快速替代；其次，结合无模型（model-free）的“优势行动者-评论家”（Advantage Actor-Critic, A2C）强化学习智能体进行均衡器参数优化，无需显式系统模型。

Result: 将该方法应用于行业标准的DRAM波形，取得了显著的眼图开窗面积改善：级联连续时间线性均衡器（CTLE）和判决反馈均衡器（DFE）结构提升42.7%，仅DFE配置提升36.8%。与现有技术相比，本方法表现出卓越的性能、计算效率以及在不同DRAM单元间的鲁棒泛化能力。

Conclusion: 本研究的核心贡献在于提出了一种高效的潜在信号完整性度量用于优化，一个鲁棒的无模型强化学习策略，并验证了其在复杂均衡器架构上的卓越性能。该方法为高速DRAM系统中的信号完整性优化提供了一种高效且鲁棒的解决方案。

Abstract: Equalizer parameter optimization for signal integrity in high-speed Dynamic
Random Access Memory systems is crucial but often computationally demanding or
model-reliant. This paper introduces a data-driven framework employing learned
latent signal representations for efficient signal integrity evaluation,
coupled with a model-free Advantage Actor-Critic reinforcement learning agent
for parameter optimization. The latent representation captures vital signal
integrity features, offering a fast alternative to direct eye diagram analysis
during optimization, while the reinforcement learning agent derives optimal
equalizer settings without explicit system models. Applied to industry-standard
Dynamic Random Access Memory waveforms, the method achieved significant
eye-opening window area improvements: 42.7\% for cascaded Continuous-Time
Linear Equalizer and Decision Feedback Equalizer structures, and 36.8\% for
Decision Feedback Equalizer-only configurations. These results demonstrate
superior performance, computational efficiency, and robust generalization
across diverse Dynamic Random Access Memory units compared to existing
techniques. Core contributions include an efficient latent signal integrity
metric for optimization, a robust model-free reinforcement learning strategy,
and validated superior performance for complex equalizer architectures.

</details>


### [168] [Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization](https://arxiv.org/abs/2507.02406)
*Caio Azevedo,Lina Achaji,Stefano Sabatini,Nicola Poerio,Grzegorz Bartyzel,Sascha Hornauer,Fabien Moutarde*

Main category: cs.LG

TL;DR: 该研究受大语言模型偏好优化的启发，通过偏好优化微调多智能体轨迹预测模型，显著提高了场景一致性，同时基本不影响预测精度且不增加推理计算量。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA轨迹预测模型在公共数据集上表现良好，但在复杂、交互式场景中，常因未能捕捉智能体间的重要相互依赖关系，导致预测结果不一致，这会引发自动驾驶车辆的规划失误和潜在危险。

Method: 本研究受将人类偏好融入大语言模型有效性的启发，采用偏好优化方法对多智能体场景下的轨迹预测模型进行微调。微调过程中，以自动计算的预测未来轨迹的偏好排名作为输入。

Result: 在三个不同数据集上使用SOTA模型进行实验表明，该方法能够显著提高场景一致性，同时对轨迹预测精度的牺牲最小，并且在推理时没有增加任何额外的计算要求。

Conclusion: 通过引入偏好优化，可以有效提升多智能体轨迹预测的场景一致性，从而解决现有模型在复杂交互场景中的一致性问题，为自动驾驶提供更可靠的预测，且不增加实际部署负担。

Abstract: Trajectory prediction is an essential step in the pipeline of an autonomous
vehicle. Inaccurate or inconsistent predictions regarding the movement of
agents in its surroundings lead to poorly planned maneuvers and potentially
dangerous situations for the end-user. Current state-of-the-art
deep-learning-based trajectory prediction models can achieve excellent accuracy
on public datasets. However, when used in more complex, interactive scenarios,
they often fail to capture important interdependencies between agents, leading
to inconsistent predictions among agents in the traffic scene. Inspired by the
efficacy of incorporating human preference into large language models, this
work fine-tunes trajectory prediction models in multi-agent settings using
preference optimization. By taking as input automatically calculated preference
rankings among predicted futures in the fine-tuning process, our
experiments--using state-of-the-art models on three separate datasets--show
that we are able to significantly improve scene consistency while minimally
sacrificing trajectory prediction accuracy and without adding any excess
computational requirements at inference time.

</details>


### [169] [S2FGL: Spatial Spectral Federated Graph Learning](https://arxiv.org/abs/2507.02409)
*Zihan Tan,Suyuan Huang,Guancheng Wan,Wenke Huang,He Li,Mang Ye*

Main category: cs.LG

TL;DR: 联邦图学习（FGL）在空间和频谱域面临标签信号中断和客户端漂移问题。本文提出S2FGL框架，通过全局知识库和频率对齐策略有效缓解这些挑战，并在实验中展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦图学习（FGL）研究主要关注结构层面，忽略了图信号在空间和频谱域的传播问题。具体挑战包括：1) 空间上，子图联邦学习（subgraph-FL）导致客户端间边缘断开，引发标签信号中断，削弱全局GNN的类别知识。2) 频谱上，频谱异构性导致信号频率不一致，使局部GNN过拟合，产生频谱客户端漂移，损害全局泛化能力。

Method: 为解决上述挑战，我们提出了S2FGL框架。该框架结合了两种策略：1) 全局知识库，用于缓解标签信号中断（空间问题）。2) 频率对齐机制，用于解决频谱客户端漂移（频谱问题）。S2FGL是将这些空间和频谱策略结合而成的框架。

Result: 在多个数据集上进行的广泛实验表明，我们提出的S2FGL框架具有优越的性能。

Conclusion: S2FGL框架通过引入全局知识库和频率对齐，成功解决了联邦图学习中空间和频谱层面的关键挑战，显著提升了模型的性能和泛化能力。

Abstract: Federated Graph Learning (FGL) combines the privacy-preserving capabilities
of federated learning (FL) with the strong graph modeling capability of Graph
Neural Networks (GNNs). Current research addresses subgraph-FL only from the
structural perspective, neglecting the propagation of graph signals on spatial
and spectral domains of the structure. From a spatial perspective, subgraph-FL
introduces edge disconnections between clients, leading to disruptions in label
signals and a degradation in the class knowledge of the global GNN. From a
spectral perspective, spectral heterogeneity causes inconsistencies in signal
frequencies across subgraphs, which makes local GNNs overfit the local signal
propagation schemes. As a result, spectral client drifts occur, undermining
global generalizability. To tackle the challenges, we propose a global
knowledge repository to mitigate label signal disruption and a frequency
alignment to address spectral client drifts. The combination of spatial and
spectral strategies forms our framework S2FGL. Extensive experiments on
multiple datasets demonstrate the superiority of S2FGL. The code is available
at https://github.com/Wonder7racer/S2FGL.git.

</details>


### [170] [Variational Kolmogorov-Arnold Network](https://arxiv.org/abs/2507.02466)
*Francesco Alesiani,Henrik Christiansen,Federico Errica*

Main category: cs.LG

TL;DR: Kolmogorov Arnold Networks (KANs) 是一种新兴的机器学习架构，本文提出 InfinityKAN，通过变分推断自适应学习基函数数量，解决了 KANs 应用中超参数选择的难题，扩展了其适用性。


<details>
  <summary>Details</summary>
Motivation: KANs 基于强大的 Kolmogorov-Arnold 定理，理论上能精确表示复杂函数，但其作为多层感知机（MLP）的替代方案时，应用受限于需要人工选择每个单变量函数的基函数数量，这是一个关键的挑战。

Method: 本文提出 InfinityKAN 模型，通过将基函数数量的确定建模为变分推断优化问题。该方法在训练过程中通过反向传播自适应地学习每个单变量函数潜在的无限数量的基函数。

Result: InfinityKAN 成功地将 KANs 中一个关键的超参数（基函数数量）纳入学习过程，解决了其任意选择的难题，从而扩展了 Kolmogorov Arnold Networks 的潜在适用性。

Conclusion: 通过自适应地学习基函数数量，InfinityKAN 克服了 KANs 应用中的一个主要限制，使其成为更通用、更强大的表示学习工具。

Abstract: Kolmogorov Arnold Networks (KANs) are an emerging architecture for building
machine learning models. KANs are based on the theoretical foundation of the
Kolmogorov-Arnold Theorem and its expansions, which provide an exact
representation of a multi-variate continuous bounded function as the
composition of a limited number of univariate continuous functions. While such
theoretical results are powerful, their use as a representation learning
alternative to a multi-layer perceptron (MLP) hinges on the ad-hoc choice of
the number of bases modeling each of the univariate functions. In this work, we
show how to address this problem by adaptively learning a potentially infinite
number of bases for each univariate function during training. We therefore
model the problem as a variational inference optimization problem. Our
proposal, called InfinityKAN, which uses backpropagation, extends the potential
applicability of KANs by treating an important hyperparameter as part of the
learning process.

</details>


### [171] [Online Conformal Prediction with Efficiency Guarantees](https://arxiv.org/abs/2507.02496)
*Vaidehi Srinivas*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of conformal prediction in a novel online framework that
directly optimizes efficiency. In our problem, we are given a target
miscoverage rate $\alpha > 0$, and a time horizon $T$. On each day $t \le T$ an
algorithm must output an interval $I_t \subseteq [0, 1]$, then a point $y_t \in
[0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is,
$y_t \in I_t$ on (close to) a $(1 - \alpha)$-fraction of days, while
maintaining efficiency, that is, minimizing the average volume (length) of the
intervals played. This problem is an online analogue to the problem of
constructing efficient confidence intervals.
  We study this problem over arbitrary and exchangeable (random order) input
sequences. For exchangeable sequences, we show that it is possible to construct
intervals that achieve coverage $(1 - \alpha) - o(1)$, while having length
upper bounded by the best fixed interval that achieves coverage in hindsight.
For arbitrary sequences however, we show that any algorithm that achieves a
$\mu$-approximation in average length compared to the best fixed interval
achieving coverage in hindsight, must make a multiplicative factor more
mistakes than $\alpha T$, where the multiplicative factor depends on $\mu$ and
the aspect ratio of the problem. Our main algorithmic result is a matching
algorithm that can recover all Pareto-optimal settings of $\mu$ and number of
mistakes. Furthermore, our algorithm is deterministic and therefore robust to
an adaptive adversary.
  This gap between the exchangeable and arbitrary settings is in contrast to
the classical online learning problem. In fact, we show that no single
algorithm can simultaneously be Pareto-optimal for arbitrary sequences and
optimal for exchangeable sequences. On the algorithmic side, we give an
algorithm that achieves the near-optimal tradeoff between the two cases.

</details>


### [172] [Continual Gradient Low-Rank Projection Fine-Tuning for LLMs](https://arxiv.org/abs/2507.02503)
*Chenxu Wang,Yilin Lyu,Zicheng Sun,Liping Jing*

Main category: cs.LG

TL;DR: 本文提出GORP（Gradient LOw Rank Projection），一种新的LLM持续微调策略，通过结合全秩和低秩参数克服LoRA的局限性，在保持效率的同时提高学习能力和知识迁移，并有效缓解灾难性遗忘，实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的持续微调面临效率与表达能力之间的权衡问题。现有低秩适应（LoRA）方法虽然高效，但其低秩特性限制了模型学习新任务和知识迁移的能力。

Method: 提出GORP（Gradient LOw Rank Projection），一种新型持续学习训练策略。该方法通过协同结合全秩和低秩参数，并在统一的低秩梯度子空间中共同更新，以此扩展优化空间，同时保持效率并缓解灾难性遗忘。

Result: 在持续学习基准测试中，GORP的表现优于现有最先进的方法。

Conclusion: GORP为LLM的持续微调提供了一种有效且高效的解决方案，它在保持效率的同时，显著提升了模型学习新任务和迁移知识的能力，并有效缓解了灾难性遗忘问题。

Abstract: Continual fine-tuning of Large Language Models (LLMs) is hampered by the
trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)
offers efficiency but constrains the model's ability to learn new tasks and
transfer knowledge due to its low-rank nature and reliance on explicit
parameter constraints. We propose GORP (Gradient LOw Rank Projection) for
Continual Learning, a novel training strategy that overcomes these limitations
by synergistically combining full and low-rank parameters and jointly updating
within a unified low-rank gradient subspace. GORP expands the optimization
space while preserving efficiency and mitigating catastrophic forgetting.
Extensive experiments on continual learning benchmarks demonstrate GORP's
superior performance compared to existing state-of-the-art approaches. Code is
available at https://github.com/Wcxwcxw/GORP.

</details>


### [173] [TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification](https://arxiv.org/abs/2507.02510)
*Ahmed G. Habashi,Ahmed M. Azab,Seif Eldawlatly,Gamal M. Aly*

Main category: cs.LG

TL;DR: 本研究提出一种结合优化预处理和深度学习的新方法，通过直接分类STFT转换的EEG数据，显著提高了跨被试运动想象(MI)分类的性能，为免校准BCI树立了新基准。


<details>
  <summary>Details</summary>
Motivation: 由于个体间脑电图(EEG)模式差异显著，跨被试运动想象(CS-MI)分类精度较低，阻碍了免校准脑机接口(BCI)的实际应用。

Method: 采用优化预处理和深度学习技术。具体方法包括直接分类短时傅里叶变换(STFT)转换后的EEG数据，优化STFT参数，并在卷积神经网络(CNN)训练时使用平衡批处理策略。

Result: 在BCI竞赛IV数据集1 (IV-1)上达到67.60%，在数据集2A (IV-2A)上达到65.96%，在数据集2B (IV-2B)上达到80.22%，性能优于现有最先进技术。此外，还系统研究了不同MI窗口（1-4秒）的分类表现。

Conclusion: 本研究为可泛化、免校准的运动想象分类建立了新基准，并通过贡献一个强大的开放访问数据集，推动了该领域的研究进展。

Abstract: Cross-subject motor imagery (CS-MI) classification in brain-computer
interfaces (BCIs) is a challenging task due to the significant variability in
Electroencephalography (EEG) patterns across different individuals. This
variability often results in lower classification accuracy compared to
subject-specific models, presenting a major barrier to developing
calibration-free BCIs suitable for real-world applications. In this paper, we
introduce a novel approach that significantly enhances cross-subject MI
classification performance through optimized preprocessing and deep learning
techniques. Our approach involves direct classification of Short-Time Fourier
Transform (STFT)-transformed EEG data, optimized STFT parameters, and a
balanced batching strategy during training of a Convolutional Neural Network
(CNN). This approach is uniquely validated across four different datasets,
including three widely-used benchmark datasets leading to substantial
improvements in cross-subject classification, achieving 67.60% on the BCI
Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on
Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we
systematically investigate the classification performance using MI windows
ranging from the full 4-second window to 1-second windows. These results
establish a new benchmark for generalizable, calibration-free MI classification
in addition to contributing a robust open-access dataset to advance research in
this domain.

</details>


### [174] [RetrySQL: text-to-SQL training with retry data for self-correcting query generation](https://arxiv.org/abs/2507.02529)
*Alicja Rączkowska,Riccardo Belluzzo,Piotr Zieliński,Joanna Baran,Paweł Olszewski*

Main category: cs.LG

TL;DR: RetrySQL是一种通过自校正训练Text-to-SQL模型的新方法，可将准确率提高高达4个百分点，并能与大型专有模型媲美。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL解决方案缺乏SQL特异性生成模型的研究，且自校正生成策略在Text-to-SQL任务中的应用尚未被探索。

Method: 提出RetrySQL方法，通过准备SQL查询的推理步骤并进行破坏，创建包含错误和纠正步骤的“重试数据”。使用这些数据持续预训练一个开源编码模型。

Result: RetrySQL使整体和挑战性执行准确率提高了高达4个百分点；证实LoRA不适用于重试数据学习，需要进行全参数预训练；模型学会了自校正行为，并因此提高了下游准确率；与参数量大得多的专有模型相比，其执行准确率具有竞争力。

Conclusion: RetrySQL证明自校正能力可在Text-to-SQL任务中学习，为提高SQL导向型语言模型的生成准确性提供了一种新颖有效的方法。

Abstract: The text-to-SQL task is an active challenge in Natural Language Processing.
Many existing solutions focus on using black-box language models extended with
specialized components within customized end-to-end text-to-SQL pipelines.
While these solutions use both closed-source proprietary language models and
coding-oriented open-source models, there is a lack of research regarding
SQL-specific generative models. At the same time, recent advancements in
self-correcting generation strategies show promise for improving the
capabilities of existing architectures. The application of these concepts to
the text-to-SQL task remains unexplored. In this paper, we introduce RetrySQL,
a new approach to training text-to-SQL generation models. We prepare reasoning
steps for reference SQL queries and then corrupt them to create retry data that
contains both incorrect and corrected steps, divided with a special token. We
continuously pre-train an open-source coding model with this data and
demonstrate that retry steps yield an improvement of up to 4 percentage points
in both overall and challenging execution accuracy metrics, compared to
pre-training without retry data. Additionally, we confirm that supervised
fine-tuning with LoRA is ineffective for learning from retry data and that
full-parameter pre-training is a necessary requirement for that task. We
showcase that the self-correcting behavior is learned by the model and the
increase in downstream accuracy metrics is a result of this additional skill.
Finally, we incorporate RetrySQL-trained models into the full text-to-SQL
pipeline and showcase that they are competitive in terms of execution accuracy
with proprietary models that contain orders of magnitude more parameters.
RetrySQL demonstrates that self-correction can be learned in the text-to-SQL
task and provides a novel way of improving generation accuracy for SQL-oriented
language models.

</details>


### [175] [Position: A Theory of Deep Learning Must Include Compositional Sparsity](https://arxiv.org/abs/2507.02550)
*David A. Danhofer,Davide D'Ascenzo,Rafael Dubach,Tomaso Poggio*

Main category: cs.LG

TL;DR: 深度神经网络（DNNs）的成功源于其能利用目标函数的组合稀疏结构，这一特性是所有高效图灵可计算函数共有的。理解其作用对人工智能理论至关重要。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络（DNNs）在高维领域表现卓越，但其学习动力学的基本原理仍未完全阐明，需要探讨并解释其成功的根本原因。

Method: 本文提出观点，认为DNNs的成功在于能够利用目标函数的“组合稀疏结构”，即函数可由少量仅依赖低维输入子集的构成函数组合而成。作者进一步论证此特性在所有高效图灵可计算函数中普遍存在。

Result: 研究揭示“组合稀疏结构”是所有高效图灵可计算函数共有的特性，因此很可能存在于所有当前的学习问题中。

Conclusion: 深入理解组合稀疏性在深度学习中的作用，对于构建全面的人工智能理论至关重要。尽管已有初步理论见解，但在DNNs的可学习性和优化方面仍有重要问题待解决。

Abstract: Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable
success in a wide variety of domains too high-dimensional for classical shallow
networks subject to the curse of dimensionality. However, open questions about
fundamental principles, that govern the learning dynamics of DNNs, remain. In
this position paper we argue that it is the ability of DNNs to exploit the
compositionally sparse structure of the target function driving their success.
As such, DNNs can leverage the property that most practically relevant
functions can be composed from a small set of constituent functions, each of
which relies only on a low-dimensional subset of all inputs. We show that this
property is shared by all efficiently Turing-computable functions and is
therefore highly likely present in all current learning problems. While some
promising theoretical insights on questions concerned with approximation and
generalization exist in the setting of compositionally sparse functions,
several important questions on the learnability and optimization of DNNs
remain. Completing the picture of the role of compositional sparsity in deep
learning is essential to a comprehensive theory of artificial, and even
general, intelligence.

</details>


### [176] [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
*Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Shaojie Zhuo,Chen Feng,Yicheng Lin,Chenzheng Su,Xiaopeng Zhang*

Main category: cs.LG

TL;DR: 提出OmniDraft框架，使单个草稿模型能兼容任意目标模型并动态适应用户数据，尤其适用于边缘设备LLM，实现1.5-2倍解码加速。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码要求草稿模型与特定目标模型兼容且离线训练。但在在线部署中，存在草稿模型与目标模型不兼容、以及需持续提升延迟的挑战，由此引出“一个草稿模型服务所有”的需求。

Method: OmniDraft框架通过引入结合混合蒸馏微调的在线n-gram缓存，解决草稿模型与目标模型间的跨词汇不匹配问题；同时利用自适应草稿技术进一步提升解码速度。

Result: OmniDraft在数学推理、代码生成和文本生成任务上展示了其能力。一个Llama-68M模型能与Vicuna-7B、Qwen2-7B和Llama3-8B等多种目标模型配对进行推测解码，并提供1.5-2倍的速度提升。

Conclusion: OmniDraft框架特别适用于对模型成本、效率和用户定制有高要求的边缘设备LLM应用，证明了“一个草稿模型服务所有”范式的可行性与价值。

Abstract: Speculative decoding generally dictates having a small, efficient draft model
that is either pretrained or distilled offline to a particular target model
series, for instance, Llama or Qwen models. However, within online deployment
settings, there are two major challenges: 1) usage of a target model that is
incompatible with the draft model; 2) expectation of latency improvements over
usage and time. In this work, we propose OmniDraft, a unified framework that
enables a single draft model to operate with any target model and adapt
dynamically to user data. We introduce an online n-gram cache with hybrid
distillation fine-tuning to address the cross-vocabulary mismatch across draft
and target models; and further improve decoding speed by leveraging adaptive
drafting techniques. OmniDraft is particularly suitable for on-device LLM
applications where model cost, efficiency and user customization are the major
points of contention. This further highlights the need to tackle the above
challenges and motivates the \textit{``one drafter for all''} paradigm. We
showcase the proficiency of the OmniDraft framework by performing online
learning on math reasoning, coding and text generation tasks. Notably,
OmniDraft enables a single Llama-68M model to pair with various target models
including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;
and additionally provides up to 1.5-2x speedup.

</details>


### [177] [Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability](https://arxiv.org/abs/2507.02559)
*Luca Baroni,Galvin Khara,Joachim Schaeffer,Marat Subkhankulov,Stefan Heimersheim*

Main category: cs.LG

TL;DR: 研究发现，GPT-2模型可以移除所有层归一化（LN）层，仅导致验证损失微小增加，表明LN并非语言模型功能所必需，但其移除有望提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 层归一化（LN）在Transformer模型中普遍存在，但其在推理时期的作用尚不明确，且因引入非线性和增加组件互联性而阻碍了模型的可解释性研究。

Method: 研究者从所有GPT-2模型中移除了层归一化（LN）层，并评估了移除后模型在验证集上的损失（如交叉熵损失）。同时，探讨了移除LN所需的微调数据量与模型参数的扩展关系。此外，还在无LN模型上测试了直接logit归因和归因修补等可解释性技术，并检查了“置信度神经元”的活跃性。

Result: 移除所有LN层后，GPT-2模型的验证损失仅略微增加（如GPT-2 XL增加0.03交叉熵损失），这表明LN对语言建模功能无实质性作用。移除LN所需的微调数据量随模型参数的增长呈亚线性关系。在无LN模型中，直接logit归因能给出单个组件的精确直接效应，但归因修补的准确性未显著提高。研究还证实，GPT-2的“置信度神经元”在无LN模型中处于非活跃状态。

Conclusion: 本研究澄清了LN层在语言建模中的作用，表明GPT-2类模型可以在没有LN层的情况下正常运行。研究发布的无LN GPT-2模型家族有望促进更精确的模型可解释性研究，并加深我们对语言模型的理解。

Abstract: Layer-wise normalization (LN) is an essential component of virtually all
transformer-based large language models. While its effects on training
stability are well documented, its role at inference time is poorly understood.
Additionally, LN layers hinder mechanistic interpretability by introducing
additional nonlinearities and increasing the interconnectedness of individual
model components. Here, we show that all LN layers can be removed from every
GPT-2 model with only a small increase in validation loss (e.g. +0.03
cross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in
language modeling. We find that the amount of fine-tuning data needed for LN
removal grows sublinearly with model parameters, suggesting scaling to larger
models is feasible. We release a suite of LN-free GPT-2 models on Hugging Face.
Furthermore, we test interpretability techniques on LN-free models. Direct
logit attribution now gives the exact direct effect of individual components,
while the accuracy of attribution patching does not significantly improve. We
also confirm that GPT-2's "confidence neurons" are inactive in the LN-free
models. Our work clarifies the role of LN layers in language modeling, showing
that GPT-2-class models can function without LN layers. We hope that our
LN-free analogs of the GPT-2 family of models will enable more precise
interpretability research and improve our understanding of language models.

</details>


### [178] [Scalable Interconnect Learning in Boolean Networks](https://arxiv.org/abs/2507.02585)
*Fabian Kresse,Emily Yu,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 该研究通过引入可伸缩互连和两种互补剪枝方法，提高了可微分布尔逻辑网络（DBNs）的层宽度扩展能力并进一步减小了模型尺寸，同时保持了精度。


<details>
  <summary>Details</summary>
Motivation: 现有DBNs在资源受限硬件上推理高效，但需要解决输入宽度增加时的层扩展性问题和进一步减小模型尺寸的需求。

Method: 1. 扩展DBNs，引入一个可训练、可微分的互连层，其参数量不随输入宽度增长。2. 提出两种互补的剪枝阶段：基于SAT的逻辑等价剪枝（移除冗余门不影响性能）和基于相似性的数据驱动剪枝（优于基于幅度的贪婪基线，提供更优的压缩-精度权衡）。

Result: 所提出的互连允许DBNs扩展到更宽的层，同时保持其精度优势。剪枝方法有效减小了模型尺寸，其中基于相似性的剪枝在压缩-精度权衡方面表现更优。

Conclusion: 通过可伸缩互连和有效的双阶段剪枝技术，DBNs在保持精度的同时，显著提升了处理更宽层的能力并进一步减小了模型尺寸，增强了其在资源受限硬件上的应用潜力。

Abstract: Learned Differentiable Boolean Logic Networks (DBNs) already deliver
efficient inference on resource-constrained hardware. We extend them with a
trainable, differentiable interconnect whose parameter count remains constant
as input width grows, allowing DBNs to scale to far wider layers than earlier
learnable-interconnect designs while preserving their advantageous accuracy. To
further reduce model size, we propose two complementary pruning stages: an
SAT-based logic equivalence pass that removes redundant gates without affecting
performance, and a similarity-based, data-driven pass that outperforms a
magnitude-style greedy baseline and offers a superior compression-accuracy
trade-off.

</details>


### [179] [Padé Approximant Neural Networks for Enhanced Electric Motor Fault Diagnosis Using Vibration and Acoustic Data](https://arxiv.org/abs/2507.02599)
*Sertac Kilickaya,Levent Eren*

Main category: cs.LG

TL;DR: 本研究利用Padé近似神经元网络（PadéNets）在感应电机故障诊断中取得了显著优于传统深度学习模型的表现，验证了其增强非线性对诊断性能的提升作用。


<details>
  <summary>Details</summary>
Motivation: 传统感应电机故障诊断方法依赖加速度计和麦克风，而深度学习模型，特别是具有非线性神经元架构的模型，有望提升诊断性能。本研究旨在探讨Padé近似神经网络（PadéNets）在使用振动和声学数据诊断电机电气和机械故障方面，是否能超越传统的卷积神经网络（CNNs）和自组织运算神经网络（Self-ONNs）。

Method: 研究比较了三种深度学习架构的诊断能力：一维卷积神经网络（CNNs）、自组织运算神经网络（Self-ONNs）和Padé近似神经网络（PadéNets）。这些模型在加拿大渥太华大学公开的恒速感应电机数据集上进行了测试，该数据集包含振动和声学传感器数据。PadéNet模型被设计以引入增强的非线性，并兼容无界激活函数如Leaky ReLU。

Result: PadéNets的表现持续优于基线模型。在使用加速度计1、2、3和声学传感器数据时，诊断准确率分别达到99.96%、98.26%、97.61%和98.33%。

Conclusion: PadéNets增强的非线性及其与无界激活函数的兼容性，显著提升了感应电机状态监测中的故障诊断性能。

Abstract: Purpose: The primary aim of this study is to enhance fault diagnosis in
induction machines by leveraging the Pad\'e Approximant Neuron (PAON) model.
While accelerometers and microphones are standard in motor condition
monitoring, deep learning models with nonlinear neuron architectures offer
promising improvements in diagnostic performance. This research addresses the
question: Can Pad\'e Approximant Neural Networks (Pad\'eNets) outperform
conventional Convolutional Neural Networks (CNNs) and Self-Organized
Operational Neural Networks (Self-ONNs) in diagnosing electrical and mechanical
faults using vibration and acoustic data?
  Methods: We evaluate and compare the diagnostic capabilities of three deep
learning architectures: one-dimensional CNNs, Self-ONNs, and Pad\'eNets. These
models are tested on the University of Ottawa's publicly available
constant-speed induction motor datasets, which include both vibration and
acoustic sensor data. The Pad\'eNet model is designed to introduce enhanced
nonlinearity and is compatible with unbounded activation functions such as
Leaky ReLU.
  Results and Conclusion: Pad\'eNets consistently outperformed the baseline
models, achieving diagnostic accuracies of 99.96%, 98.26%, 97.61%, and 98.33%
for accelerometers 1, 2, 3, and the acoustic sensor, respectively. The enhanced
nonlinearity of Pad\'eNets, together with their compatibility with unbounded
activation functions, significantly improves fault diagnosis performance in
induction motor condition monitoring.

</details>


### [180] [Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation](https://arxiv.org/abs/2507.02608)
*François Rozet,Ruben Ohana,Michael McCabe,Gilles Louppe,François Lanusse,Shirley Ho*

Main category: cs.LG

TL;DR: 本文探讨了将扩散模型应用于动力学系统模拟，通过在潜在空间进行生成来显著降低计算成本，同时保持高精度，并发现其性能优于非生成模型。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在推理时计算成本高昂，限制了其作为快速物理模拟器的应用。图像和视频生成领域已通过在自编码器潜在空间生成来解决此问题。本文旨在探究该策略能否有效应用于动力学系统模拟，以及所需的代价。

Method: 研究了将图像和视频生成中使用的潜在空间生成策略（通过自编码器）应用于动力学系统模拟的扩散模型。同时，探讨了训练潜在空间模拟器时关键的实践设计选择，包括架构和优化器。

Result: 研究发现潜在空间模拟的精度对广泛的压缩率（高达1000倍）具有惊人的鲁棒性。基于扩散的模拟器始终比非生成性对应物更准确，并且能通过更大的多样性来补偿预测中的不确定性。此外，确定了训练潜在空间模拟器的关键实践设计选择。

Conclusion: 将扩散模型应用于潜在空间是实现计算高效且高精度动力学系统模拟的有效策略。该方法在大幅压缩下依然保持鲁棒性，且性能优于非生成方法，为实际应用提供了关键的设计指导。

Abstract: The steep computational cost of diffusion models at inference hinders their
use as fast physics emulators. In the context of image and video generation,
this computational drawback has been addressed by generating in the latent
space of an autoencoder instead of the pixel space. In this work, we
investigate whether a similar strategy can be effectively applied to the
emulation of dynamical systems and at what cost. We find that the accuracy of
latent-space emulation is surprisingly robust to a wide range of compression
rates (up to 1000x). We also show that diffusion-based emulators are
consistently more accurate than non-generative counterparts and compensate for
uncertainty in their predictions with greater diversity. Finally, we cover
practical design choices, spanning from architectures to optimizers, that we
found critical to train latent-space emulators.

</details>


### [181] [L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation](https://arxiv.org/abs/2507.02619)
*Hazal Mogultay Ozcan,Sinan Kalkan,Fatos T. Yarman-Vural*

Main category: cs.LG

TL;DR: 提出了一种名为L-VAE的新型模型，它通过学习损失函数中的项权重，在重构和解耦表示之间实现了动态平衡，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有VAE模型（如Beta-VAE）的超参数（如beta）需要经验性调整以平衡解耦和重构，这限制了其性能。L-VAE旨在通过学习这些权重来克服此限制。

Method: 提出L-VAE，作为Beta-VAE的扩展，它能够同时学习解耦表示以及损失函数的超参数（项权重），以动态调整解耦和重构损失之间的权衡。模型中还添加了额外的正则化项，以防止偏向任一损失。

Result: L-VAE在重构保真度和潜在维度解耦之间找到了有效平衡。与现有VAE变体（如Beta-VAE、VAE、ControlVAE、DynamicVAE、Sigma-VAE）在dSprites、MPI3D-complex等数据集上进行比较，L-VAE在解耦指标上始终表现最佳或次佳。在CelebA数据集上的定性实验也证实了其在解耦面部属性方面的成功。

Conclusion: L-VAE通过学习损失项的权重，成功实现了重构和解耦的动态平衡，克服了现有VAE模型的局限性，并在性能上超越了许多先进方法。

Abstract: In this paper, we propose a novel model called Learnable VAE (L-VAE), which
learns a disentangled representation together with the hyperparameters of the
cost function. L-VAE can be considered as an extension of \b{eta}-VAE, wherein
the hyperparameter, \b{eta}, is empirically adjusted. L-VAE mitigates the
limitations of \b{eta}-VAE by learning the relative weights of the terms in the
loss function to control the dynamic trade-off between disentanglement and
reconstruction losses. In the proposed model, the weight of the loss terms and
the parameters of the model architecture are learned concurrently. An
additional regularization term is added to the loss function to prevent bias
towards either reconstruction or disentanglement losses. Experimental analyses
show that the proposed L-VAE finds an effective balance between reconstruction
fidelity and disentangling the latent dimensions. Comparisons of the proposed
L-VAE against \b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\sigma}-VAE on
datasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that
L-VAE consistently provides the best or the second best performances measured
by a set of disentanglement metrics. Moreover, qualitative experiments on
CelebA dataset, confirm the success of the L-VAE model for disentangling the
facial attributes.

</details>


### [182] [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://arxiv.org/abs/2507.02834)
*Ruiyang Zhou,Shuozhe Li,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: 提出ExPO框架，通过基于真实答案生成高质量样本，解决现有RL后期训练方法在模型初始能力不足或任务困难时无法有效探索和学习的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）的强化学习（RL）后期训练方法，如GRPO，严重依赖模型生成高质量正样本的初始能力。这导致它们主要优化模型已知知识，而非让模型解决其最初失败的问题。在RL训练早期或面对挑战性推理任务时，模型难以生成正样本，限制了其探索新推理轨迹的能力。专家演示虽看似解决方案，但在RL后期训练中往往无效。

Method: 识别出有效正样本的两个关键特性：1) 它们应在当前策略下具有高可能性；2) 它们应能提高模型预测正确答案的概率。基于此，提出Self-Explanation Policy Optimization (ExPO)框架。ExPO通过条件性地使用真实答案来生成此类正样本，从而实现高效探索，并引导模型生成比专家编写的思维链（CoT）更符合其策略的推理轨迹，同时确保样本质量高于其自身生成的（错误）样本。

Result: 实验表明，ExPO显著提高了推理基准测试的学习效率和最终性能。在MATH level-5等模型初始表现不佳的挑战性设置中，ExPO超越了基于专家演示的方法。

Conclusion: ExPO通过生成高质量、策略对齐的样本，有效解决了现有RL后期训练方法在模型初始能力不足或任务困难时探索受限的问题。它为提高大型语言模型在复杂推理任务上的性能提供了一个强大且高效的解决方案。

Abstract: Recent advances in large language models have been driven by reinforcement
learning (RL)-style post-training, which improves reasoning by optimizing model
outputs based on reward or preference signals. GRPO-style approaches implement
this by using self-generated samples labeled by an outcome-based verifier.
However, these methods depend heavily on the model's initial ability to produce
positive samples. They primarily refine what the model already knows
(distribution sharpening) rather than enabling the model to solve problems
where it initially fails. This limitation is especially problematic in
early-stage RL training and on challenging reasoning tasks, where positive
samples are unlikely to be generated. To unlock reasoning ability in such
settings, the model must explore new reasoning trajectories beyond its current
output distribution. Such exploration requires access to sufficiently good
positive samples to guide the learning. While expert demonstrations seem like a
natural solution, we find that they are often ineffective in RL post-training.
Instead, we identify two key properties of effective positive samples: they
should (1) be likely under the current policy, and (2) increase the model's
likelihood of predicting the correct answer. Based on these insights, we
propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and
modular framework that generates such samples by conditioning on the
ground-truth answer. ExPO enables efficient exploration and guides the model to
produce reasoning trajectories more aligned with its policy than expert-written
CoTs, while ensuring higher quality than its own (incorrect) samples.
Experiments show that ExPO improves both learning efficiency and final
performance on reasoning benchmarks, surpassing expert-demonstration-based
methods in challenging settings such as MATH level-5, where the model initially
struggles the most.

</details>


### [183] [A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes](https://arxiv.org/abs/2507.02624)
*Antoine Honoré,Borja Rodríguez Gálvez,Yoomi Park,Yitian Zhou,Volker M. Lauschke,Ming Xiao*

Main category: cs.LG

TL;DR: 提出一种基于Transformer的matVAE模型用于蛋白质变异效应预测。结果表明，DMS数据集能有效替代MSAs，且结合AlphaFold结构可进一步提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于多序列比对(MSA)的变异效应预测(VEP)方法在药理基因组学等领域存在局限性，因其假设自然变异均适应。深度突变扫描(DMS)数据集提供量化适应度分数，可作为MSA的替代方案。

Method: 提出了一种基于Transformer的矩阵变分自编码器(matVAE)，并结合了结构化先验。该模型在ProteinGym基准测试的33个DMS数据集上进行了性能评估。研究还探索了将AlphaFold生成的蛋白质结构整合到模型中以提升预测能力。

Result: 在MSAs上训练的matVAE-MSA在DMS数据集的零样本预测中表现优于最先进的DeepSequence模型，且参数更少、推理计算量更低。在DMS数据上训练的matENC-DMS在监督预测任务中表现优于matVAE-MSA。将AlphaFold生成的结构整合到Transformer模型中显著提升了性能，可媲美DeepSequence模型经MSAs训练并DMS微调后的结果。

Conclusion: 深度突变扫描(DMS)数据集有望替代多序列比对(MSA)用于变异效应预测，且不会显著损失预测性能。这鼓励了DMS数据集的进一步发展和其相互关系的探索，以期进一步增强变异效应预测。

Abstract: Variant effect predictors (VEPs) aim to assess the functional impact of
protein variants, traditionally relying on multiple sequence alignments (MSAs).
This approach assumes that naturally occurring variants are fit, an assumption
challenged by pharmacogenomics, where some pharmacogenes experience low
evolutionary pressure. Deep mutational scanning (DMS) datasets provide an
alternative by offering quantitative fitness scores for variants. In this work,
we propose a transformer-based matrix variational auto-encoder (matVAE) with a
structured prior and evaluate its performance on 33 DMS datasets corresponding
to 26 drug target and ADME proteins from the ProteinGym benchmark. Our model
trained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence
model in zero-shot prediction on DMS datasets, despite using an order of
magnitude fewer parameters and requiring less computation at inference time. We
also compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on
DMS data, and find that the latter performs better on supervised prediction
tasks. Additionally, incorporating AlphaFold-generated structures into our
transformer model further improves performance, achieving results comparable to
DeepSequence trained on MSAs and finetuned on DMS. These findings highlight the
potential of DMS datasets to replace MSAs without significant loss in
predictive performance, motivating further development of DMS datasets and
exploration of their relationships to enhance variant effect prediction.

</details>


### [184] [Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation of Structured Medical Data](https://arxiv.org/abs/2507.02628)
*Irena Girshovitz,Atai Ambus,Moni Shahar,Ran Gilad-Bachrach*

Main category: cs.LG

TL;DR: 提出Medical Data Pecking方法及其工具MDPT，借鉴软件工程单元测试概念，系统评估和识别电子健康记录（EHR）数据中的质量问题，以提高研究结果可靠性。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）数据在流行病学研究和AI训练中应用广泛，但其固有的质量问题（如不准确、不完整、偏差和系统误差）严重影响研究结果的可靠性。现有数据质量评估方法不足以系统性地评估数据是否适合研究。

Method: 本研究提出Medical Data Pecking方法，借鉴软件工程中的单元测试和覆盖率概念来识别数据质量问题。该方法通过Medical Data Pecking Tool (MDPT)实现，MDPT包含两部分：1) 一个自动化测试生成器，利用大型语言模型(LLMs)和接地技术从数据和研究描述中创建测试套件；2) 一个数据测试框架，用于执行这些测试并报告潜在错误和覆盖率。

Result: MDPT在三个数据集（All of Us, MIMIC-III, SyntheticMass）上进行了评估，每个队列在四种条件下生成了55-73个测试。这些测试成功识别了20-43个不一致或不符合规范的数据问题。研究还详细分析了LLM生成的测试套件在参考接地和值准确性方面的表现。

Conclusion: 本研究提出的方法将外部医学知识整合到数据分析工作流程中，实现了上下文敏感的数据质量测试，从而提高了研究结果的有效性。该方法从质量保证角度解决了数据质量挑战，并为未来支持更多数据模态和改进接地方法等进一步发展奠定了基础。

Abstract: Background: The use of Electronic Health Records (EHRs) for epidemiological
studies and artificial intelligence (AI) training is increasing rapidly. The
reliability of the results depends on the accuracy and completeness of EHR
data. However, EHR data often contain significant quality issues, including
misrepresentations of subpopulations, biases, and systematic errors, as they
are primarily collected for clinical and billing purposes. Existing quality
assessment methods remain insufficient, lacking systematic procedures to assess
data fitness for research.
  Methods: We present the Medical Data Pecking approach, which adapts unit
testing and coverage concepts from software engineering to identify data
quality concerns. We demonstrate our approach using the Medical Data Pecking
Tool (MDPT), which consists of two main components: (1) an automated test
generator that uses large language models and grounding techniques to create a
test suite from data and study descriptions, and (2) a data testing framework
that executes these tests, reporting potential errors and coverage.
  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and
SyntheticMass, generating 55-73 tests per cohort across four conditions. These
tests correctly identified 20-43 non-aligned or non-conforming data issues. We
present a detailed analysis of the LLM-generated test suites in terms of
reference grounding and value accuracy.
  Conclusion: Our approach incorporates external medical knowledge to enable
context-sensitive data quality testing as part of the data analysis workflow to
improve the validity of its outcomes. Our approach tackles these challenges
from a quality assurance perspective, laying the foundation for further
development such as additional data modalities and improved grounding methods.

</details>


### [185] [High-Order Deep Meta-Learning with Category-Theoretic Interpretation](https://arxiv.org/abs/2507.02634)
*David H. Mguni*

Main category: cs.LG

TL;DR: 提出一种分层深度元学习框架，通过生成“虚拟任务”自主创建数据集，使神经网络能在任务层级中构建、解决并泛化。该方法基于范畴论，旨在减少对人工数据的依赖，并推动机器学习迈向通用人工智能。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习训练过度依赖人类生成的数据，存在局限性。研究旨在开发一种能自主生成数据、学习可泛化规则并跨任务层级构建和解决问题的框架，以克服这一限制并推动通用人工智能的发展。

Method: 引入一种分层深度学习框架，用于递归高阶元学习。核心是生成“虚拟任务”来学习软约束和可泛化规则。元学习器主动探索虚拟任务空间，寻找困难任务并优化约束区域。将元学习器解释为范畴论中的“函子”，以构建支持抽象和知识迁移的组合结构。

Result: 使神经网络能够跨任务层级构建、解决和泛化问题。框架能自主生成有信息的、以任务为基础的数据集，减少对人工数据的依赖。增强了归纳偏置，规范了适应过程，并生成了泛化所需的新颖任务和约束。实现了结构化、可解释的学习进展。范畴论视角统一了现有元学习模型，并提供了实用设计原则。

Conclusion: 该架构有望成为下一代神经网络的基础，使其能够自主生成新颖、有指导性的任务及其解决方案，从而推动机器学习向通用人工智能发展。

Abstract: We introduce a new hierarchical deep learning framework for recursive
higher-order meta-learning that enables neural networks (NNs) to construct,
solve, and generalise across hierarchies of tasks. Central to this approach is
a generative mechanism that creates \emph{virtual tasks} -- synthetic problem
instances designed to enable the meta-learner to learn \emph{soft constraints}
and unknown generalisable rules across related tasks. Crucially, this enables
the framework to generate its own informative, task-grounded datasets thereby
freeing machine learning (ML) training from the limitations of relying entirely
on human-generated data. By actively exploring the virtual point landscape and
seeking out tasks lower-level learners find difficult, the meta-learner
iteratively refines constraint regions. This enhances inductive biases,
regularises the adaptation process, and produces novel, unanticipated tasks and
constraints required for generalisation. Each meta-level of the hierarchy
corresponds to a progressively abstracted generalisation of problems solved at
lower levels, enabling a structured and interpretable learning progression. By
interpreting meta-learners as category-theoretic \emph{functors} that generate
and condition a hierarchy of subordinate learners, we establish a compositional
structure that supports abstraction and knowledge transfer across progressively
generalised tasks. The category-theoretic perspective unifies existing
meta-learning models and reveals how learning processes can be transformed and
compared through functorial relationships, while offering practical design
principles for structuring meta-learning. We speculate this architecture may
underpin the next generation of NNs capable of autonomously generating novel,
instructive tasks and their solutions, thereby advancing ML towards general
artificial intelligence.

</details>


### [186] [On Efficient Bayesian Exploration in Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.02639)
*Alberto Caron,Chris Hicks,Vasilios Mavroudis*

Main category: cs.LG

TL;DR: 本文通过分析基于认知不确定性的信息论探索奖金，为强化学习中的数据高效探索提供了理论基础和实用框架。证明了这些奖金的特性，并提出了结合模型规划和信息论奖金的PTS-BE框架，在多种环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 强化学习中数据高效探索是一个挑战，现有信息论内在动机方法缺乏理论基础。需要一种能有效针对认知不确定性并与真实知识空白对齐的探索策略。

Method: 分析了针对认知不确定性的信息论探索奖金，并形式化证明了其属性。讨论了通过稀疏变分高斯过程、深度核和深度集成模型进行实际逼近的方法。提出了预测轨迹采样与贝叶斯探索（PTS-BE）的通用框架，该框架将基于模型的规划与信息论奖金相结合。

Result: 证明了认知不确定性奖金能自然地指示认知信息增益，并在智能体对环境足够确定后收敛到零。为之前缺乏理论基础的信息增益（IG）方法提供了形式保证。实验表明，PTS-BE在稀疏奖励和/或纯探索任务等多种环境中显著优于其他基线。

Conclusion: 本工作通过提供理论保障和实用的PTS-BE框架，解决了强化学习中数据高效探索的挑战，特别是通过结合基于模型的规划和信息论探索奖金，实现了样本高效的深度探索。

Abstract: In this work, we address the challenge of data-efficient exploration in
reinforcement learning by examining existing principled, information-theoretic
approaches to intrinsic motivation. Specifically, we focus on a class of
exploration bonuses that targets epistemic uncertainty rather than the
aleatoric noise inherent in the environment. We prove that these bonuses
naturally signal epistemic information gains and converge to zero once the
agent becomes sufficiently certain about the environment's dynamics and
rewards, thereby aligning exploration with genuine knowledge gaps. Our analysis
provides formal guarantees for IG-based approaches, which previously lacked
theoretical grounding. To enable practical use, we also discuss tractable
approximations via sparse variational Gaussian Processes, Deep Kernels and Deep
Ensemble models. We then outline a general framework - Predictive Trajectory
Sampling with Bayesian Exploration (PTS-BE) - which integrates model-based
planning with information-theoretic bonuses to achieve sample-efficient deep
exploration. We empirically demonstrate that PTS-BE substantially outperforms
other baselines across a variety of environments characterized by sparse
rewards and/or purely exploratory tasks.

</details>


### [187] [Fair Deepfake Detectors Can Generalize](https://arxiv.org/abs/2507.02645)
*Harry Cheng,Ming-Hui Liu,Yangyang Guo,Tianyi Wang,Liqiang Nie,Mohan Kankanhalli*

Main category: cs.LG

TL;DR: Deepfake检测模型在泛化性和公平性上面临挑战，且两者间存在权衡。本文首次揭示公平性与泛化性之间的因果关系，并提出DAID框架，通过数据再平衡和特征聚合，有效提升了模型的泛化性和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有Deepfake检测模型在泛化到未知操纵和实现人群间的公平性方面存在困难，且这两个目标常常相互冲突，形成权衡。

Method: 本文提出DAID（Demographic Attribute-insensitive Intervention Detection）框架，这是一个即插即用的方案。它基于公平性与泛化性之间的因果关系，通过控制混杂因素（数据分布和模型容量）来改善泛化性。具体包含两部分：1) 人口统计学感知的数据再平衡，通过逆倾向加权和亚组特征归一化来中和分布偏差；2) 人口统计学无关的特征聚合，使用新颖的对齐损失来抑制敏感属性信号。

Result: DAID在三个跨领域基准测试中，相较于多个现有最先进的检测器，在公平性和泛化性方面均取得了卓越的性能。

Conclusion: DAID框架不仅验证了其理论基础，也展现了其在实践中的有效性，成功解决了Deepfake检测中泛化性与公平性之间的冲突，实现了双重提升。

Abstract: Deepfake detection models face two critical challenges: generalization to
unseen manipulations and demographic fairness among population groups. However,
existing approaches often demonstrate that these two objectives are inherently
conflicting, revealing a trade-off between them. In this paper, we, for the
first time, uncover and formally define a causal relationship between fairness
and generalization. Building on the back-door adjustment, we show that
controlling for confounders (data distribution and model capacity) enables
improved generalization via fairness interventions. Motivated by this insight,
we propose Demographic Attribute-insensitive Intervention Detection (DAID), a
plug-and-play framework composed of: i) Demographic-aware data rebalancing,
which employs inverse-propensity weighting and subgroup-wise feature
normalization to neutralize distributional biases; and ii) Demographic-agnostic
feature aggregation, which uses a novel alignment loss to suppress
sensitive-attribute signals. Across three cross-domain benchmarks, DAID
consistently achieves superior performance in both fairness and generalization
compared to several state-of-the-art detectors, validating both its theoretical
foundation and practical effectiveness.

</details>


### [188] [Guided Generation for Developable Antibodies](https://arxiv.org/abs/2507.02670)
*Siqi Zhao,Joshua Moller,Porfi Quintero-Cadena,Lood van Niekerk*

Main category: cs.LG

TL;DR: 本文提出了一种基于引导离散扩散模型的新型计算框架，能够优化抗体序列以提高其可开发性，通过引入SVDD模块在不牺牲天然性的前提下生成具有良好生物物理特性的抗体。


<details>
  <summary>Details</summary>
Motivation: 治疗性抗体除了高亲和力外，还需要良好的可制造性、稳定性及安全性（统称为“可开发性”）才能实现临床有效性。因此，需要一个计算框架来优化抗体序列以获得良好的可开发性。

Method: 研究引入了一个引导离散扩散模型，该模型在自然配对的重链和轻链序列（来自Observed Antibody Space, OAS）以及246个临床阶段抗体的定量可开发性测量数据上进行训练。为使生成偏向于生物物理上可行的候选物，模型集成了一个软值解码扩散（SVDD）模块，该模块能在不损害序列天然性的前提下偏向采样。

Result: 在无约束采样下，模型能够重现天然抗体库和已批准治疗药物的全局特征。在SVDD模块的引导下，预测的可开发性得分相比无引导基线显著提高。与高通量可开发性测定相结合，该框架能实现一个迭代的、机器学习驱动的抗体设计流程。

Conclusion: 该计算框架与高通量可开发性测定相结合，能够实现一个迭代的、机器学习驱动的抗体设计流程，同时满足结合和生物物理标准，为优化抗体可开发性提供了新途径。

Abstract: Therapeutic antibodies require not only high-affinity target engagement, but
also favorable manufacturability, stability, and safety profiles for clinical
effectiveness. These properties are collectively called `developability'. To
enable a computational framework for optimizing antibody sequences for
favorable developability, we introduce a guided discrete diffusion model
trained on natural paired heavy- and light-chain sequences from the Observed
Antibody Space (OAS) and quantitative developability measurements for 246
clinical-stage antibodies. To steer generation toward biophysically viable
candidates, we integrate a Soft Value-based Decoding in Diffusion (SVDD) Module
that biases sampling without compromising naturalness. In unconstrained
sampling, our model reproduces global features of both the natural repertoire
and approved therapeutics, and under SVDD guidance we achieve significant
enrichment in predicted developability scores over unguided baselines. When
combined with high-throughput developability assays, this framework enables an
iterative, ML-driven pipeline for designing antibodies that satisfy binding and
biophysical criteria in tandem.

</details>


### [189] [Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs](https://arxiv.org/abs/2507.02671)
*Francesco Di Salvo,Hanh Huyen My Nguyen,Christian Ledig*

Main category: cs.LG

TL;DR: 本文提出一种基于差分隐私（DP）生成模型（特别是DP-CVAE）的数据共享方法，通过利用基础模型提取紧凑嵌入，旨在解决医学影像领域中深度学习面临的数据稀缺和隐私合规性挑战，并克服联邦学习的通信成本高和任务单一限制。该方法在隐私、可扩展性和效率方面表现优异，优于传统联邦学习分类器，并能生成更高保真度的嵌入。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像领域的应用受限于数据稀缺和隐私法规，导致难以获取多样化数据集。联邦学习（FL）虽能实现去中心化训练，但存在通信成本高且常局限于单一下游任务，降低了灵活性。

Method: 提出一种基于差分隐私（DP）生成模型的数据共享方法。具体而言，通过采用基础模型（foundation models）提取紧凑、信息丰富的嵌入（embeddings），以减少数据冗余并降低计算开销。客户端协作训练一个差分隐私条件变分自编码器（DP-CVAE），以建模一个全局的、隐私保护的数据分布，并支持多样化的下游任务。

Result: 所提出的方法增强了隐私、可扩展性和效率，并经验证在多种特征提取器上表现良好。它在确保差分隐私的同时，性能优于传统的联邦学习分类器。此外，DP-CVAE生成的嵌入保真度高于DP-CGAN，且所需参数量减少5倍。

Conclusion: 本研究通过结合基础模型和DP-CVAE，提供了一种有效且高效的、保护隐私的数据共享机制，成功解决了医学影像深度学习中数据稀缺和隐私的挑战。该方法不仅在性能上超越了传统联邦学习，而且在生成隐私保护的合成数据方面也展现出显著优势，能够支持多种下游任务。

Abstract: Deep Learning (DL) has revolutionized medical imaging, yet its adoption is
constrained by data scarcity and privacy regulations, limiting access to
diverse datasets. Federated Learning (FL) enables decentralized training but
suffers from high communication costs and is often restricted to a single
downstream task, reducing flexibility. We propose a data-sharing method via
Differentially Private (DP) generative models. By adopting foundation models,
we extract compact, informative embeddings, reducing redundancy and lowering
computational overhead. Clients collaboratively train a Differentially Private
Conditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware
data distribution, supporting diverse downstream tasks. Our approach, validated
across multiple feature extractors, enhances privacy, scalability, and
efficiency, outperforming traditional FL classifiers while ensuring
differential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings
than DP-CGAN while requiring $5{\times}$ fewer parameters.

</details>


### [190] [Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions](https://arxiv.org/abs/2507.02698)
*Thomas Hazenberg,Yao Ma,Seyed Sahand Mohammadi Ziabari,Marijn van Rijswijk*

Main category: cs.LG

TL;DR: 本研究探讨多智能体强化学习(MARL)如何改进供应链动态定价，发现MARL引入传统规则未捕获的竞争行为，其中MADDPG表现均衡。


<details>
  <summary>Details</summary>
Motivation: 传统ERP系统依赖静态规则，忽略市场主体间的策略互动。现有强化学习定价研究多为单智能体，未能模拟真实供应链的相互依赖性。

Method: 在结合真实电商交易数据和LightGBM需求预测模型的模拟环境中，评估并比较了MADDPG、MADQN和QMIX三种MARL算法与静态规则基线的性能。

Result: 规则代理具有高公平性和价格稳定性，但缺乏竞争性。MARL代理中，MADQN最具侵略性。MADDPG表现均衡，支持市场竞争的同时保持高公平性和价格稳定。

Conclusion: MARL能引入静态定价规则无法捕捉的新兴策略行为，对未来动态定价发展具有启发意义。

Abstract: This study investigates how Multi-Agent Reinforcement Learning (MARL) can
improve dynamic pricing strategies in supply chains, particularly in contexts
where traditional ERP systems rely on static, rule-based approaches that
overlook strategic interactions among market actors. While recent research has
applied reinforcement learning to pricing, most implementations remain
single-agent and fail to model the interdependent nature of real-world supply
chains. This study addresses that gap by evaluating the performance of three
MARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines,
within a simulated environment informed by real e-commerce transaction data and
a LightGBM demand prediction model. Results show that rule-based agents achieve
near-perfect fairness (Jain's Index: 0.9896) and the highest price stability
(volatility: 0.024), but they fully lack competitive dynamics. Among MARL
agents, MADQN exhibits the most aggressive pricing behaviour, with the highest
volatility and the lowest fairness (0.5844). MADDPG provides a more balanced
approach, supporting market competition (share volatility: 9.5 pp) while
maintaining relatively high fairness (0.8819) and stable pricing. These
findings suggest that MARL introduces emergent strategic behaviour not captured
by static pricing rules and may inform future developments in dynamic pricing.

</details>


### [191] [Fluid Democracy in Federated Data Aggregation](https://arxiv.org/abs/2507.02710)
*Aditya Vema Reddy Kesari,Krishna Reddy Kesari*

Main category: cs.LG

TL;DR: 本文提出基于共识的协议，以在联邦学习中筛选最有用的客户端，从而减少数据传输成本。研究引入了一种新的“粘性保留民主”协议，性能优于FedAvg，并设计了FedVRD算法以有效抵御对抗性攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）机制通常要求所有客户端将其权重传输到中央服务器，无论这些权重是否有用，这导致了不必要的数据传输成本。

Method: ['探索现有流体民主协议在FL中的应用，并从性能角度与传统FedAvg（1p1v）进行比较。', '提出一种新的流体民主协议——“粘性保留民主”（viscous-retained democracy），旨在在相同假设下优于1p1v并防止影响力积累。', '从对抗性角度识别流体民主协议的弱点（例如对拓扑和对抗者数量的依赖）。', '提出FedVRD算法，该算法通过利用委派拓扑，动态限制对抗者的影响，同时最小化成本。']

Result: ['提出的“粘性保留民主”协议在与现有流体民主协议相同的假设下，性能始终优于1p1v，且不会导致影响力积累。', 'FedVRD算法能够动态限制对抗者的影响，并最小化成本。']

Conclusion: 通过引入基于共识的协议（特别是“粘性保留民主”协议和FedVRD算法），可以有效减少联邦学习中不必要的数据传输成本，提高性能，并增强系统在对抗性环境下的鲁棒性，从而优化联邦学习的效率和安全性。

Abstract: Federated learning (FL) mechanisms typically require each client to transfer
their weights to a central server, irrespective of how useful they are. In
order to avoid wasteful data transfer costs from clients to the central server,
we propose the use of consensus based protocols to identify a subset of clients
with most useful model weights at each data transfer step. First, we explore
the application of existing fluid democracy protocols to FL from a performance
standpoint, comparing them with traditional one-person-one-vote (also known as
1p1v or FedAvg). We propose a new fluid democracy protocol named
viscous-retained democracy that always does better than 1p1v under the same
assumptions as existing fluid democracy protocols while also not allowing for
influence accumulation. Secondly, we identify weaknesses of fluid democracy
protocols from an adversarial lens in terms of their dependence on topology
and/ or number of adversaries required to negatively impact the global model
weights. To this effect, we propose an algorithm (FedVRD) that dynamically
limits the effect of adversaries while minimizing cost by leveraging the
delegation topology.

</details>


### [192] [A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control](https://arxiv.org/abs/2507.02712)
*Zilin Kang,Chenyuan Hu,Yu Luo,Zhecheng Yuan,Ruijie Zheng,Huazhe Xu*

Main category: cs.LG

TL;DR: 本文受神经科学启发，提出Forget and Grow (FoG)算法，通过模拟“遗忘”和“成长”机制，解决深度强化学习在连续控制中的先验偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法在连续控制中常出现先验偏差，即过度拟合回放缓冲区中的早期经验，从而限制了RL智能体的样本效率和泛化能力。

Method: 提出FoG算法，包含两个核心机制：1. 经验回放衰减 (ER Decay)，通过逐步减少早期经验的影响来“遗忘”它们；2. 网络扩展，通过在训练过程中动态添加新参数来“增长”神经网络容量，增强智能体利用现有数据模式的能力。

Result: 在四个主要连续控制基准上的40多个任务中，FoG算法表现出优于现有最先进深度强化学习算法（包括BRO, SimBa, TD-MPC2）的性能。

Conclusion: FoG算法通过引入“遗忘早期经验”和“增长神经网络容量”的双重机制，有效解决了深度强化学习的先验偏差问题，显著提升了连续控制任务中的学习效率和泛化能力。

Abstract: Deep reinforcement learning for continuous control has recently achieved
impressive progress. However, existing methods often suffer from primacy bias,
a tendency to overfit early experiences stored in the replay buffer, which
limits an RL agent's sample efficiency and generalizability. In contrast,
humans are less susceptible to such bias, partly due to infantile amnesia,
where the formation of new neurons disrupts early memory traces, leading to the
forgetting of initial experiences. Inspired by this dual processes of
forgetting and growing in neuroscience, in this paper, we propose Forget and
Grow (FoG), a new deep RL algorithm with two mechanisms introduced. First,
Experience Replay Decay (ER Decay) "forgetting early experience", which
balances memory by gradually reducing the influence of early experiences.
Second, Network Expansion, "growing neural capacity", which enhances agents'
capability to exploit the patterns of existing data by dynamically adding new
parameters during training. Empirical results on four major continuous control
benchmarks with more than 40 tasks demonstrate the superior performance of FoG
against SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2.

</details>


### [193] [A Comprehensive Machine Learning Framework for Micromobility Demand Prediction](https://arxiv.org/abs/2507.02715)
*Omri Porat,Michael Fire,Eran Ben-Elia*

Main category: cs.LG

TL;DR: 本研究提出一个整合时空和网络依赖性的框架，以显著提高无桩电动滑板车需求预测的准确性，从而支持更优化的微出行管理和城市规划。


<details>
  <summary>Details</summary>
Motivation: 无桩电动滑板车服务的有效管理（如最佳车队分配和基础设施规划）高度依赖于准确的需求预测。然而，现有研究在分析需求时，往往单独关注空间或时间因素，未能充分整合多维度的复杂依赖性。

Method: 本研究引入了一个新的框架，该框架能够整合空间、时间以及网络依赖性，以改进微出行需求预测。

Result: 所提出的框架将需求预测准确性比基线模型提高了27%至49%，有效捕捉了微出行需求模式。

Conclusion: 研究结果支持数据驱动的微出行管理，有助于优化车队分布、降低运营成本并促进可持续的城市规划。

Abstract: Dockless e-scooters, a key micromobility service, have emerged as
eco-friendly and flexible urban transport alternatives. These services improve
first and last-mile connectivity, reduce congestion and emissions, and
complement public transport for short-distance travel. However, effective
management of these services depends on accurate demand prediction, which is
crucial for optimal fleet distribution and infrastructure planning. While
previous studies have focused on analyzing spatial or temporal factors in
isolation, this study introduces a framework that integrates spatial, temporal,
and network dependencies for improved micromobility demand forecasting. This
integration enhances accuracy while providing deeper insights into urban
micromobility usage patterns. Our framework improves demand prediction accuracy
by 27 to 49% over baseline models, demonstrating its effectiveness in capturing
micromobility demand patterns. These findings support data-driven micromobility
management, enabling optimized fleet distribution, cost reduction, and
sustainable urban planning.

</details>


### [194] [Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms](https://arxiv.org/abs/2507.02724)
*Shiyi Liu,Buwen Liang,Yuetong Fang,Zixuan Jiang,Renjing Xu*

Main category: cs.LG

TL;DR: 提出HIPPO，一个分层对比学习框架，用于跨物种蛋白质相互作用（PPI）预测，在低数据和零样本场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 利用对比学习在整合异构生物数据方面的能力，旨在解决跨物种蛋白质相互作用预测中数据稀疏、不平衡以及对未充分表征或稀有物种缺乏实验数据的问题。

Method: HIPPO是一个分层对比框架，通过多层生物表示匹配对齐蛋白质序列及其分层属性。它整合了模拟蛋白质功能类结构关系的层次对比损失函数，并自适应地结合领域和家族知识，通过数据驱动的惩罚机制确保学习到的嵌入空间与蛋白质功能的内在层次结构一致。

Result: HIPPO在基准数据集上达到最先进性能，优于现有方法，并在低数据量下表现出鲁棒性。该模型无需重新训练即可强大地零样本迁移到其他物种，从而能在数据有限的未充分表征或稀有生物体中进行可靠的PPI预测。层次特征融合对于捕获保守的相互作用决定因素至关重要。

Conclusion: 该工作推进了跨物种PPI预测，并为处理稀疏或不平衡多物种数据场景下的相互作用预测提供了一个统一框架。

Abstract: Recent advances in AI for science have highlighted the power of contrastive
learning in bridging heterogeneous biological data modalities. Building on this
paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction
across Organisms), a hierarchical contrastive framework for protein-protein
interaction(PPI) prediction, where protein sequences and their hierarchical
attributes are aligned through multi-tiered biological representation matching.
The proposed approach incorporates hierarchical contrastive loss functions that
emulate the structured relationship among functional classes of proteins. The
framework adaptively incorporates domain and family knowledge through a
data-driven penalty mechanism, enforcing consistency between the learned
embedding space and the intrinsic hierarchy of protein functions. Experiments
on benchmark datasets demonstrate that HIPPO achieves state-of-the-art
performance, outperforming existing methods and showing robustness in low-data
regimes. Notably, the model demonstrates strong zero-shot transferability to
other species without retraining, enabling reliable PPI prediction and
functional inference even in less characterized or rare organisms where
experimental data are limited. Further analysis reveals that hierarchical
feature fusion is critical for capturing conserved interaction determinants,
such as binding motifs and functional annotations. This work advances
cross-species PPI prediction and provides a unified framework for interaction
prediction in scenarios with sparse or imbalanced multi-species data.

</details>


### [195] [Classification by Separating Hypersurfaces: An Entropic Approach](https://arxiv.org/abs/2507.02732)
*Argimiro Arratia,Mahmoud El Daou,Henryk Gzyl*

Main category: cs.LG

TL;DR: 提出一种新的基于熵的分类方法，通过最小化熵函数寻找分离超平面或多项式曲面，有效处理线性和非线性分类任务。


<details>
  <summary>Details</summary>
Motivation: 分类问题（即寻找分离两类数据的超平面或曲面）是机器学习的核心且具有悠久历史。

Method: 提出一种新颖的基于熵的方法，通过最小化定义在未知变量空间上的熵函数，在一个有界的N维超立方体中搜索参数向量并在R^M中搜索一个正向量。该方法可扩展至多项式曲面，以处理更复杂的决策边界。

Result: 数值实验表明，该方法在处理包括线性和非线性可分性在内的多样化分类任务时，具有高效性和通用性。

Conclusion: 本文提出的基于熵的方法为传统线性或二次优化技术（如支持向量机和梯度下降）提供了一种鲁棒的替代方案，能够有效解决线性和非线性分类问题。

Abstract: We consider the following classification problem: Given a population of
individuals characterized by a set of attributes represented as a vector in
${\mathbb R}^N$, the goal is to find a hyperplane in ${\mathbb R}^N$ that
separates two sets of points corresponding to two distinct classes. This
problem, with a history dating back to the perceptron model, remains central to
machine learning. In this paper we propose a novel approach by searching for a
vector of parameters in a bounded $N$-dimensional hypercube centered at the
origin and a positive vector in ${\mathbb R}^M$, obtained through the
minimization of an entropy-based function defined over the space of unknown
variables. The method extends to polynomial surfaces, allowing the separation
of data points by more complex decision boundaries. This provides a robust
alternative to traditional linear or quadratic optimization techniques, such as
support vector machines and gradient descent. Numerical experiments demonstrate
the efficiency and versatility of the method in handling diverse classification
tasks, including linear and non-linear separability.

</details>


### [196] [Fast and Simplex: 2-Simplicial Attention in Triton](https://arxiv.org/abs/2507.02754)
*Aurko Roy,Timothy Chou,Sai Surya Duvvuri,Sijia Chen,Jiecao Yu,Xiaodong Wang,Manzil Zaheer,Rohan Anil*

Main category: cs.LG

TL;DR: 研究提出2-simplicial Transformer，其通过三线性注意力提升了Token效率，在数学、编码和推理任务上表现优于标准Transformer，并改变了缩放定律的指数。


<details>
  <summary>Details</summary>
Motivation: 传统的模型缩放定律假设数据无限且计算受限，但现代大型语言模型正面临数据瓶颈，亟需提升Token效率的架构来适应这种转变。

Method: 本文研究了2-simplicial Transformer架构，该架构通过高效的Triton内核实现，将标准点积注意力泛化为三线性函数。

Result: 结果表明，2-simplicial Transformer比标准Transformer实现了更高的Token效率：在固定Token预算下，相似大小的模型在数学、编码、推理和逻辑任务上表现更优。研究还量化了这些收益，证明2-simplicial注意力改变了知识和推理任务在缩放定律中的指数。

Conclusion: 2-simplicial Transformer是一种更具Token效率的架构，对于解决现代大型语言模型在数据受限环境下的性能和学习效率问题至关重要，特别是在知识和推理任务上。

Abstract: Recent work has shown that training loss scales as a power law with both
model size and the number of tokens, and that achieving compute-optimal models
requires scaling model size and token count together. However, these scaling
laws assume an infinite supply of data and apply primarily in compute-bound
settings. As modern large language models increasingly rely on massive
internet-scale datasets, the assumption that they are compute-bound is becoming
less valid. This shift highlights the need for architectures that prioritize
token efficiency.
  In this work, we investigate the use of the 2-simplicial Transformer, an
architecture that generalizes standard dot-product attention to trilinear
functions through an efficient Triton kernel implementation. We demonstrate
that the 2-simplicial Transformer achieves better token efficiency than
standard Transformers: for a fixed token budget, similarly sized models
outperform their dot-product counterparts on tasks involving mathematics,
coding, reasoning, and logic. We quantify these gains by demonstrating that
$2$-simplicial attention changes the exponent in the scaling laws for knowledge
and reasoning tasks compared to dot product attention.

</details>


### [197] [Contextual Online Pricing with (Biased) Offline Data](https://arxiv.org/abs/2507.02762)
*Yixuan Zhang,Ruihao Zhu,Qiaomin Xie*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study contextual online pricing with biased offline data. For the scalar
price elasticity case, we identify the instance-dependent quantity $\delta^2$
that measures how far the offline data lies from the (unknown) online optimum.
We show that the time length $T$, bias bound $V$, size $N$ and dispersion
$\lambda_{\min}(\hat{\Sigma})$ of the offline data, and $\delta^2$ jointly
determine the statistical complexity. An Optimism-in-the-Face-of-Uncertainty
(OFU) policy achieves a minimax-optimal, instance-dependent regret bound
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T +
\frac{dT}{\lambda_{\min}(\hat{\Sigma}) + (N \wedge T) \delta^2})\big)$. For
general price elasticity, we establish a worst-case, minimax-optimal rate
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T + \frac{dT
}{\lambda_{\min}(\hat{\Sigma})})\big)$ and provide a generalized OFU algorithm
that attains it. When the bias bound $V$ is unknown, we design a robust variant
that always guarantees sub-linear regret and strictly improves on purely online
methods whenever the exact bias is small. These results deliver the first tight
regret guarantees for contextual pricing in the presence of biased offline
data. Our techniques also transfer verbatim to stochastic linear bandits with
biased offline data, yielding analogous bounds.

</details>


### [198] [Understanding and Improving Length Generalization in Recurrent Models](https://arxiv.org/abs/2507.02782)
*Ricardo Buitrago Ruiz,Albert Gu*

Main category: cs.LG

TL;DR: 循环模型在训练序列长度之外的泛化能力差。本文提出“未探索状态假说”，并通过简单的训练干预（如状态初始化）有效提升了其长度泛化能力，且成本极低。


<details>
  <summary>Details</summary>
Motivation: 循环模型（如状态空间模型、线性注意力）虽具备处理任意长序列的潜力，但其性能在超出训练上下文长度后显著下降，即无法实现长度泛化。研究动机在于探究此问题，并提出“未探索状态假说”：模型未能长度泛化是因训练时仅接触到可达状态分布的有限子集。

Method: 本文通过全面的经验和理论分析支持“未探索状态假说”。同时，研究提出并探究了简单的训练干预措施，旨在增加模型训练时状态的覆盖范围，例如使用高斯噪声或不同输入序列的最终状态来初始化模型状态。

Result: 这些干预措施仅需约500个训练后步骤（占预训练预算的0.1%），便能使模型对远超训练上下文长度（如2k到128k）的序列实现长度泛化，并在长上下文任务中表现出性能提升。

Conclusion: 本研究提供了一种简单且高效的方法，通过训练干预来增强通用循环模型的鲁棒长度泛化能力。

Abstract: Recently, recurrent models such as state space models and linear attention
have become popular due to their linear complexity in the sequence length.
Thanks to their recurrent nature, in principle they can process arbitrarily
long sequences, but their performance sometimes drops considerably beyond their
training context lengths-i.e. they fail to length generalize. In this work, we
provide comprehensive empirical and theoretical analysis to support the
unexplored states hypothesis, which posits that models fail to length
generalize when during training they are only exposed to a limited subset of
the distribution of all attainable states (i.e. states that would be attained
if the recurrence was applied to long sequences). Furthermore, we investigate
simple training interventions that aim to increase the coverage of the states
that the model is trained on, e.g. by initializing the state with Gaussian
noise or with the final state of a different input sequence. With only 500
post-training steps ($\sim 0.1\%$ of the pre-training budget), these
interventions enable length generalization for sequences that are orders of
magnitude longer than the training context (e.g. $2k\longrightarrow 128k$) and
show improved performance in long context tasks, thus presenting a simple and
efficient way to enable robust length generalization in general recurrent
models.

</details>


### [199] [In-Training Multicalibrated Survival Analysis for Healthcare via Constrained Optimization](https://arxiv.org/abs/2507.02807)
*Thiti Suttaket,Stanley Kok*

Main category: cs.LG

TL;DR: 现有生存分析模型在少数子群体上校准不良，本文提出GRADUATE模型，通过将多重校准建模为约束优化问题，实现对所有子群体的良好校准，并在真实临床数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 生存分析在医疗健康领域至关重要，但现有模型通常只在总体层面进行校准，导致其对一个或多个少数子群体可能校准不佳，从而引发错误的临床决策。

Method: 本文提出名为GRADUATE的模型，它将多重校准（即确保所有子群体均得到良好校准）定义为一个约束优化问题，并在训练过程中同时优化校准和区分度，以实现两者间的良好平衡。研究还通过数学证明了其优化方法能以高概率得到接近最优且可行的解。

Result: 在真实世界的临床数据集上与最先进的基线模型进行实证比较，结果表明GRADUATE模型表现出卓越的有效性。详细分析揭示了基线模型的不足以及GRADUATE的优势。

Conclusion: GRADUATE模型成功解决了生存分析中子群体校准不佳的问题，通过实现多重校准，提升了模型的可靠性和公平性，对临床决策具有重要意义。

Abstract: Survival analysis is an important problem in healthcare because it models the
relationship between an individual's covariates and the onset time of an event
of interest (e.g., death). It is important for survival models to be
well-calibrated (i.e., for their predicted probabilities to be close to
ground-truth probabilities) because badly calibrated systems can result in
erroneous clinical decisions. Existing survival models are typically calibrated
at the population level only, and thus run the risk of being poorly calibrated
for one or more minority subpopulations. We propose a model called GRADUATE
that achieves multicalibration by ensuring that all subpopulations are
well-calibrated too. GRADUATE frames multicalibration as a constrained
optimization problem, and optimizes both calibration and discrimination
in-training to achieve a good balance between them. We mathematically prove
that the optimization method used yields a solution that is both near-optimal
and feasible with high probability. Empirical comparisons against
state-of-the-art baselines on real-world clinical datasets demonstrate
GRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of
the baselines vis-a-vis GRADUATE's strengths.

</details>


### [200] [Replicable Distribution Testing](https://arxiv.org/abs/2507.02814)
*Ilias Diakonikolas,Jingyi Gao,Daniel Kane,Sihan Liu,Christopher Ye*

Main category: cs.LG

TL;DR: 本文在算法可复制性框架下系统研究了分布测试，开发了新的可复制算法并提出了样本复杂度下限证明新方法，解决了先前的开放问题。


<details>
  <summary>Details</summary>
Motivation: 旨在算法可复制性框架下，系统性地探究分布测试问题，并量化可复制测试中基础分布属性的样本复杂度。

Method: 1. 开发了新的可复制算法，用于测试离散分布的接近性和独立性。
2. 提出了一种新的方法，用于证明可复制测试的样本复杂度下限。

Result: 1. 开发了测试离散分布接近性和独立性的可复制算法。
2. 提出了一种新的、可能具有更广泛兴趣的样本复杂度下限证明方法。
3. 建立了可复制均匀性测试（解决了先前的开放问题）和接近性测试的近最优样本复杂度下限。

Conclusion: 本研究通过提出新的可复制算法和样本复杂度下限证明方法，在可复制分布测试领域取得了重要进展，并解决了现有开放问题。

Abstract: We initiate a systematic investigation of distribution testing in the
framework of algorithmic replicability. Specifically, given independent samples
from a collection of probability distributions, the goal is to characterize the
sample complexity of replicably testing natural properties of the underlying
distributions. On the algorithmic front, we develop new replicable algorithms
for testing closeness and independence of discrete distributions. On the lower
bound front, we develop a new methodology for proving sample complexity lower
bounds for replicable testing that may be of broader interest. As an
application of our technique, we establish near-optimal sample complexity lower
bounds for replicable uniformity testing -- answering an open question from
prior work -- and closeness testing.

</details>


### [201] [LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding](https://arxiv.org/abs/2507.02843)
*Yuchen Ma,Dennis Frauen,Jonas Schweisthal,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 本研究提出一种新框架，结合大型语言模型和双重鲁棒学习器，以解决医学治疗效果估计中，训练时详细数据与推理时文本描述不完整之间的数据差异导致的“推理时文本混淆”问题，并有效减轻由此产生的估计偏差。


<details>
  <summary>Details</summary>
Motivation: 在医学领域，治疗效果估计对个性化决策至关重要。然而，现有模型在训练时依赖详细的结构化数据，而在推理时常使用不完整的文本描述，这种数据差异导致治疗效果估计产生偏差。作者将此问题形式化为“推理时文本混淆问题”，即混淆因素在训练时完全观测，但在推理时只能通过文本部分获取，从而导致偏差估计。

Method: 为解决推理时文本混淆导致的估计偏差，作者提出了一个新的治疗效果估计框架。该框架利用大型语言模型（LLMs）处理文本信息，并结合定制的双重鲁棒学习器（doubly robust learner），以显式地考虑并减轻推理时文本混淆带来的偏差。

Result: 通过一系列实验，作者证明了所提出的框架在实际应用中能够有效应对推理时文本混淆问题，并提升了治疗效果估计的准确性。

Conclusion: 该研究成功识别并形式化了治疗效果估计中训练与推理数据不一致导致的“推理时文本混淆问题”，并提出了一种基于大型语言模型和双重鲁棒学习器的有效解决方案，为更准确的个性化医疗决策提供了支持。

Abstract: Estimating treatment effects is crucial for personalized decision-making in
medicine, but this task faces unique challenges in clinical practice. At
training time, models for estimating treatment effects are typically trained on
well-structured medical datasets that contain detailed patient information.
However, at inference time, predictions are often made using textual
descriptions (e.g., descriptions with self-reported symptoms), which are
incomplete representations of the original patient information. In this work,
we make three contributions. (1) We show that the discrepancy between the data
available during training time and inference time can lead to biased estimates
of treatment effects. We formalize this issue as an inference time text
confounding problem, where confounders are fully observed during training time
but only partially available through text at inference time. (2) To address
this problem, we propose a novel framework for estimating treatment effects
that explicitly accounts for inference time text confounding. Our framework
leverages large language models together with a custom doubly robust learner to
mitigate biases caused by the inference time text confounding. (3) Through a
series of experiments, we demonstrate the effectiveness of our framework in
real-world applications.

</details>


### [202] [MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain Disorder Diagnosis](https://arxiv.org/abs/2507.02847)
*Kunyu Zhang,Qiang Li,Shujian Yu*

Main category: cs.LG

TL;DR: 本文提出了MvHo-IB，一个新颖的多视角学习框架，用于从fMRI数据中提取和利用高阶交互(HOIs)，以提高诊断准确性，并在基准数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明高阶交互(HOIs)能提升fMRI数据诊断精度，但有效提取和利用HOIs仍面临挑战。

Method: MvHo-IB框架融合了成对和高阶交互，并自动压缩冗余信息。其创新包括：结合O-信息和Renyi alpha-order熵估算器量化提取HOIs；使用Brain3DCNN编码器利用交互；引入多视角学习信息瓶颈目标以增强表示学习。

Result: 在三个fMRI基准数据集上的实验表明，MvHo-IB实现了最先进的性能，显著优于包括超图在内的现有方法。

Conclusion: MvHo-IB是一种有效且性能卓越的框架，能够利用fMRI数据中的高阶交互进行诊断决策。

Abstract: Recent evidence suggests that modeling higher-order interactions (HOIs) in
functional magnetic resonance imaging (fMRI) data can enhance the diagnostic
accuracy of machine learning systems. However, effectively extracting and
utilizing HOIs remains a significant challenge. In this work, we propose
MvHo-IB, a novel multi-view learning framework that integrates both pairwise
interactions and HOIs for diagnostic decision-making, while automatically
compressing task-irrelevant redundant information. MvHo-IB introduces several
key innovations: (1) a principled method that combines O-information from
information theory with a matrix-based Renyi alpha-order entropy estimator to
quantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to
effectively utilize these interactions, and (3) a new multi-view learning
information bottleneck objective to enhance representation learning.
Experiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves
state-of-the-art performance, significantly outperforming previous methods,
including recent hypergraph-based techniques. The implementation of MvHo-IB is
available at https://github.com/zky04/MvHo-IB.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [203] [A Comprehensive Survey on Network Traffic Synthesis: From Statistical Models to Deep Learning](https://arxiv.org/abs/2507.01976)
*Nirhoshan Sivaroopan,Kaushitha Silva,Chamara Madarasingha,Thilini Dahanayaka,Guillaume Jourjon,Anura Jayasumana,Kanchana Thilakarathna*

Main category: cs.NI

TL;DR: 本综述全面审查了合成网络流量生成方法，重点关注深度学习，并讨论了开放挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 由于真实网络数据存在稀缺性、隐私顾虑和纯度限制等关键挑战，合成网络流量生成作为一种有前景的替代方案出现，本综述旨在对其进行系统性回顾。

Method: 本综述全面审查了合成网络流量生成方法，涵盖数据类型、生成模型和评估方法。特别关注基于深度学习的技术，并详细讨论了统计方法及其扩展，包括商用工具。

Result: 本综述对现有方法进行了结构化分析，揭示了该领域的开放挑战，并提出了潜在的未来研究和发展方向。

Conclusion: 本综述为合成网络流量生成领域的研究人员和从业者提供了基础资源，全面分析了现有方法、挑战和机遇，为进一步研究奠定了基础。

Abstract: Synthetic network traffic generation has emerged as a promising alternative
for various data-driven applications in the networking domain. It enables the
creation of synthetic data that preserves real-world characteristics while
addressing key challenges such as data scarcity, privacy concerns, and purity
constraints associated with real data. In this survey, we provide a
comprehensive review of synthetic network traffic generation approaches,
covering essential aspects such as data types, generation models, and
evaluation methods. With the rapid advancements in AI and machine learning, we
focus particularly on deep learning-based techniques while also providing a
detailed discussion of statistical methods and their extensions, including
commercially available tools. Furthermore, we highlight open challenges in this
domain and discuss potential future directions for further research and
development. This survey serves as a foundational resource for researchers and
practitioners, offering a structured analysis of existing methods, challenges,
and opportunities in synthetic network traffic generation.

</details>


### [204] [Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers](https://arxiv.org/abs/2507.01988)
*Giyong Jung,Saeid Gorgin,John Kim,Jungrae Kim*

Main category: cs.NI

TL;DR: 本文提出了一种名为隐式序列号 (ISN) 的新型机制，并将其整合到 CXL 扩展协议可靠性扩展链路 (RXL) 中。RXL 通过在传输层使用 CRC 和在链路层使用 FEC，实现了可扩展、高可靠的多节点互连，同时保持了带宽效率和现有兼容性，有效解决了交换设备中静默丢包和乱序传输的问题。


<details>
  <summary>Details</summary>
Motivation: 随着 AI 模型超越单处理器能力，芯片间互连对于可扩展计算至关重要。新型互连协议（如 CXL）处理大量细粒度数据，但其高传输速率增加了错误易感性。虽然 CRC 和 FEC 是标准纠错机制，但在多节点配置中，管理交换设备中的静默丢包成为新挑战。

Method: 引入隐式序列号 (ISN)，一种无需额外数据头开销即可实现精确 flit 丢包检测和按序传输的新机制。提出可靠性扩展链路 (RXL)，它是 CXL 的一个扩展，集成了 ISN 以支持可扩展、可靠的多节点互连，并保持与现有 flit 结构兼容。RXL 将 CRC 提升为传输层机制，用于端到端数据和序列完整性检查，同时依赖 FEC 进行链路层错误纠正和检测。

Result: ISN 确保了精确的 flit 丢包检测和按序传输。RXL 成功支持了可扩展、可靠的多节点互连，同时保持了与现有 CXL flit 结构的兼容性。通过将 CRC 和 FEC 分别应用于不同层，RXL 实现了强大的可靠性和可扩展性，且不损害带宽效率。

Conclusion: 所提出的 ISN 机制和 RXL 协议为可扩展、高可靠的多节点芯片互连提供了一种创新解决方案，尤其适用于要求严苛的 AI 计算环境。它通过优化错误检测和纠正策略，有效解决了静默丢包等关键挑战，同时保持了性能和兼容性。

Abstract: As AI models outpace the capabilities of single processors, interconnects
across chips have become a critical enabler for scalable computing. These
processors exchange massive amounts of data at cache-line granularity,
prompting the adoption of new interconnect protocols like CXL, NVLink, and
UALink, designed for high bandwidth and small payloads. However, the increasing
transfer rates of these protocols heighten susceptibility to errors. While
mechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction
(FEC) are standard for reliable data transmission, scaling chip interconnects
to multi-node configurations introduces new challenges, particularly in
managing silently dropped flits in switching devices. This paper introduces
Implicit Sequence Number (ISN), a novel mechanism that ensures precise flit
drop detection and in-order delivery without adding header overhead.
Additionally, we propose Reliability Extended Link (RXL), an extension of CXL
that incorporates ISN to support scalable, reliable multi-node interconnects
while maintaining compatibility with the existing flit structure. By elevating
CRC to a transport-layer mechanism for end-to-end data and sequence integrity,
and relying on FEC for link-layer error correction and detection, RXL delivers
robust reliability and scalability without compromising bandwidth efficiency.

</details>


### [205] [Curated Collaborative AI Edge with Network Data Analytics for B5G/6G Radio Access Networks](https://arxiv.org/abs/2507.01994)
*Sardar Jaffar Ali,Syed M. Raza,Duc-Tai Le,Rajesh Challa,Min Young Chung,Ness Shroff,Hyunseung Choo*

Main category: cs.NI

TL;DR: 本研究提出整合Curated Collaborative Learning (CCL) 和 Distributed Unit Pooling Scheme (DUPS) 的方法，通过精准流量预测和智能服务器管理，显著降低5G无线接入网（RAN）的能耗和运营成本。


<details>
  <summary>Details</summary>
Motivation: 5G无线接入网（RAN）消耗超过50%的5G网络总功耗，且现有RAN切分方案未能充分利用数据潜力以降低运营支出。因此，研究旨在解决5G RAN的高能耗和高运营成本问题。

Method: 本研究采用双重方法：1. 提出 Curated Collaborative Learning (CCL) 框架，通过选择性地与相关数据协作，实现高度准确的网络流量和用户预测。2. 提出 Distributed Unit Pooling Scheme (DUPS) 方案，该方案利用深度强化学习和CCL的预测结果，有效减少活跃的分布式单元（DU）服务器数量，并通过动态重定向流量来优化资源利用。

Result: CCL框架在流量预测方面显著优于现有技术，性能提升达31.35%至43.9%。DUPS方案相较于传统策略，能效提升高达89%，为运营商带来可观的经济效益。

Conclusion: 通过将CCL驱动的预测与DUPS相结合，本研究展示了一种创新的方法，能够最大程度地降低5G RAN的能耗和运营成本，显著提升效率和成本效益。

Abstract: Despite advancements, Radio Access Networks (RAN) still account for over 50\%
of the total power consumption in 5G networks. Existing RAN split options do
not fully harness data potential, presenting an opportunity to reduce
operational expenditures. This paper addresses this opportunity through a
twofold approach. First, highly accurate network traffic and user predictions
are achieved using the proposed Curated Collaborative Learning (CCL) framework,
which selectively collaborates with relevant correlated data for traffic
forecasting. CCL optimally determines whom, when, and what to collaborate with,
significantly outperforming state-of-the-art approaches, including global,
federated, personalized federated, and cyclic institutional incremental
learnings by 43.9%, 39.1%, 40.8%, and 31.35%, respectively. Second, the
Distributed Unit Pooling Scheme (DUPS) is proposed, leveraging deep
reinforcement learning and prediction inferences from CCL to reduce the number
of active DU servers efficiently. DUPS dynamically redirects traffic from
underutilized DU servers to optimize resource use, improving energy efficiency
by up to 89% over conventional strategies, translating into substantial
monetary benefits for operators. By integrating CCL-driven predictions with
DUPS, this paper demonstrates a transformative approach for minimizing energy
consumption and operational costs in 5G RANs, significantly enhancing
efficiency and cost-effectiveness.

</details>


### [206] [Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting](https://arxiv.org/abs/2507.01997)
*Zhihao Wang,Alessandro Cornacchia,Franco Galante,Carlo Centofanti,Alessio Sacco,Dingde Jiang*

Main category: cs.NI

TL;DR: 本初步工作探讨了AI代理在网络故障排除中的应用，并强调需要一个标准化、可复现、开放的基准测试平台。


<details>
  <summary>Details</summary>
Motivation: 鉴于AI和大型语言模型在网络配置综合及自动化诊断方面的有效性，作者旨在将AI应用于网络故障排除，并指出目前缺乏一个标准化、可复现、开放的基准测试平台来有效构建和评估AI代理。

Method: 本文作为一项初步工作，主要通过概念性论述，强调构建和评估AI代理在网络故障排除中所需的标准化、可复现、开放的基准测试平台的重要性。未涉及具体的AI模型开发或实验方法。

Result: 本研究的主要结果是明确指出了在网络故障排除领域，当前迫切需要一个用于构建和评估AI代理的标准化、可复现、开放的基准测试平台。

Conclusion: 为有效推进AI代理在网络故障排除中的发展，建立一个标准化、可复现、开放的基准测试平台至关重要。

Abstract: Recent research has demonstrated the effectiveness of Artificial Intelligence
(AI), and more specifically, Large Language Models (LLMs), in supporting
network configuration synthesis and automating network diagnosis tasks, among
others. In this preliminary work, we restrict our focus to the application of
AI agents to network troubleshooting and elaborate on the need for a
standardized, reproducible, and open benchmarking platform, where to build and
evaluate AI agents with low operational effort.

</details>


### [207] [AI-Empowered Channel Generation for IoV Semantic Communications in Dynamic Conditions](https://arxiv.org/abs/2507.02013)
*Hao Liu,Bo Yang,Zhiwen Yu,Xuelin Cao,George C. Alexandropoulos,Yan Zhang,Chau Yuen*

Main category: cs.NI

TL;DR: 本文提出一种基于信道感知的语义通信框架，结合生成扩散模型进行信道估计，并利用大模型进行微调，以提升车联网（IoV）在动态无线环境下数据传输的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 车联网（IoV）产生海量数据，深度学习和生成式AI有潜力增强其应用。然而，在动态、不可预测的无线信道条件下，实时传输和处理这些海量数据是一个重大挑战，影响智能导航、车辆安全监控等IoV服务的性能。

Method: 1. 提出基于信道感知的语义通信框架，通过语义通信模型提取并压缩待传输信息。2. 采用生成扩散模型估算并预测动态信道状态，以提升IoV服务质量。3. 为解决动态场景下信道估计性能可能下降的问题，使用大模型对信道生成模型进行微调，以增强其对不同场景的适应性。4. 在两个公共数据集上评估所提框架的性能和可靠性。

Result: 所提出的框架在两个公共数据集上进行了性能和可靠性评估，旨在提升数据传输的准确性和效率（具体量化结果未在摘要中给出）。

Conclusion: 该研究通过引入基于信道感知的语义通信框架，并结合生成扩散模型进行信道估计及大模型微调，有效解决了车联网中海量数据在动态无线环境下高效传输和处理的挑战，提升了数据传输的准确性和效率。

Abstract: The Internet of Vehicles (IoV) transforms the transportation ecosystem
promising pervasive connectivity and data-driven approaches. Deep learning and
generative Artificial Intelligence (AI) have the potential to significantly
enhance the operation of applications within IoV by facilitating efficient
decision-making and predictive capabilities, including intelligent navigation,
vehicle safety monitoring, accident prevention, and intelligent traffic
management. Nevertheless, efficiently transmitting and processing the massive
volumes of data generated by the IoV in real-time remains a significant
challenge, particularly in dynamic and unpredictable wireless channel
conditions. To address these challenges, this paper proposes a semantic
communication framework based on channel perception to improve the accuracy and
efficiency of data transmission. The semantic communication model extracts and
compresses the information to be transmitted. In addition, the wireless channel
is estimated by using a generative diffusion model, which is employed to
predict the dynamic channel states, thereby improving the quality of IoV
service. In dynamic scenarios, however, the channel estimation performance may
be degraded when substantially new scenarios take place, which will adversely
affect user experience. To mitigate this limitation, we employ a large model to
fine-tune the channel generation model to enhance its adaptability for varying
scenarios. The performance and reliability of the proposed framework are
evaluated on the two public datasets.

</details>


### [208] [REDUS: Adaptive Resampling for Efficient Deep Learning in Centralized and Federated IoT Networks](https://arxiv.org/abs/2507.02021)
*Eyad Gad,Gad Gad,Mostafa M. Fouda,Mohamed I. Ibrahem,Muhammad Ismail,Zubair Md Fadlullah*

Main category: cs.NI

TL;DR: 论文提出REDUS重采样技术，优化联邦学习中深度学习训练，通过优先处理误分类样本和排除冗余数据，显著减少训练时间（最高72.6%）和资源消耗，同时保持高精度，以解决SDN与DL共存时的资源冲突。


<details>
  <summary>Details</summary>
Motivation: 当软件定义网络（SDN）控制器与深度学习（DL）工作负载共享基础设施时，尤其是在延迟敏感的物联网（IoT）环境中，资源争用会导致SDN响应性下降和网络性能受损。尽管联邦学习（FL）通过去中心化训练缓解了部分问题，但DL的计算需求在IoT连续数据流下仍可能干扰SDN性能。

Method: 本文提出REDUS（智能网络中高效数据利用的重采样）技术，该方法受AdaBoost启发，通过优先处理误分类样本和排除冗余数据来优化深度学习训练。REDUS旨在减少每个训练周期（epoch）的样本数量，从而节约计算资源、降低能耗并加速收敛。该技术被应用于联邦学习（FL）框架中，以提升资源受限边缘设备的模型训练效率，同时维持网络性能。

Result: REDUS在CICIoT2023数据集上进行IoT攻击检测评估时，展现出高达72.6%的训练时间缩减，而精度损失仅为1.62%。

Conclusion: REDUS提供了一个可扩展且实用的解决方案，通过高效重采样优化DL训练，在联邦学习设置下显著减少训练时间和资源消耗，同时保持高精度，有效缓解了智能网络中SDN与DL共存的资源冲突问题。

Abstract: With the rise of Software-Defined Networking (SDN) for managing traffic and
ensuring seamless operations across interconnected devices, challenges arise
when SDN controllers share infrastructure with deep learning (DL) workloads.
Resource contention between DL training and SDN operations, especially in
latency-sensitive IoT environments, can degrade SDN's responsiveness and
compromise network performance. Federated Learning (FL) helps address some of
these concerns by decentralizing DL training to edge devices, thus reducing
data transmission costs and enhancing privacy. Yet, the computational demands
of DL training can still interfere with SDN's performance, especially under the
continuous data streams characteristic of IoT systems. To mitigate this issue,
we propose REDUS (Resampling for Efficient Data Utilization in Smart-Networks),
a resampling technique that optimizes DL training by prioritizing misclassified
samples and excluding redundant data, inspired by AdaBoost. REDUS reduces the
number of training samples per epoch, thereby conserving computational
resources, reducing energy consumption, and accelerating convergence without
significantly impacting accuracy. Applied within an FL setup, REDUS enhances
the efficiency of model training on resource-limited edge devices while
maintaining network performance. In this paper, REDUS is evaluated on the
CICIoT2023 dataset for IoT attack detection, showing a training time reduction
of up to 72.6% with a minimal accuracy loss of only 1.62%, offering a scalable
and practical solution for intelligent networks.

</details>


### [209] [MULTI-SCOUT: Multistatic Integrated Sensing and Communications in 5G and Beyond for Moving Target Detection, Positioning, and Tracking](https://arxiv.org/abs/2507.02613)
*Yalin E. Sagduyu,Kemal Davaslioglu,Tugba Erpek,Sastry Kompella,Gustave Anderson,Jonathan Ashdown*

Main category: cs.NI

TL;DR: 本文提出并验证了一种基于5G PRS的分布式多站集成感知与通信（ISAC）的完整信号处理链，实现高精度运动目标检测、定位和跟踪。


<details>
  <summary>Details</summary>
Motivation: 旨在利用5G定位参考信号（PRS）构建一个分布式多站ISAC系统，以提供一套完整的信号处理方案，实现高效、高精度的目标感知（检测、定位和跟踪）。

Method: 采用分布式架构，由一个gNB发射5G OFDM-PRS信号，多个接收器利用此信号进行感知。核心处理步骤包括：1. 通过相干交叉模糊函数（CAF）生成距离-多普勒图，提取双基地延迟和径向速度。2. 对单个目标，利用非线性最小二乘三边测量法融合双基地延迟进行几何定位，并通过径向速度方程的正则化线性反演获取速度矢量。3. 该方法适用于2D/3D环境，并扩展以处理时间同步偏差和多目标关联。4. 最后，将位置-速度估计序列输入标准和扩展卡尔曼滤波器进行平滑跟踪。

Result: 实验结果显示，所提出的方法能够利用5G PRS信号实现高精度的运动目标检测、定位和跟踪。

Conclusion: 该研究成功构建了一套完整的信号处理链，证明了使用5G PRS信号在分布式多站ISAC场景中实现高保真目标感知的可行性和有效性。

Abstract: This paper presents a complete signal-processing chain for multistatic
integrated sensing and communications (ISAC) using 5G Positioning Reference
Signal (PRS). We consider a distributed architecture in which one gNB transmits
a periodic OFDM-PRS waveform while multiple spatially separated receivers
exploit the same signal for target detection, parameter estimation and
tracking. A coherent cross-ambiguity function (CAF) is evaluated to form a
range-Doppler map from which the bistatic delay and radial velocity are
extracted for every target. For a single target, the resulting bistatic delays
are fused through nonlinear least-squares trilateration, yielding a geometric
position estimate, and a regularized linear inversion of the radial-speed
equations yields a two-dimensional velocity vector, where speed and heading are
obtained. The approach is applied to 2D and 3D settings, extended to account
for time synchronization bias, and generalized to multiple targets by resolving
target association. The sequence of position-velocity estimates is then fed to
standard and extended Kalman filters to obtain smoothed tracks. Our results
show high-fidelity moving-target detection, positioning, and tracking using 5G
PRS signals for multistatic ISAC.

</details>


### [210] [On the Architectural Split and Radio Intelligence Controller Placement in Integrated O-RAN-enabled Non-Terrestrial Networks](https://arxiv.org/abs/2507.02680)
*Jorge Baranda,Marius Caus,Luis Blanco,Cristian J. Vaca-Rubio,Engin Zeydan,Kapal Dev,Zheng Li,Tomaso DeCola*

Main category: cs.NI

TL;DR: 本文探讨了在O-RAN框架下，陆地网络(TN)与非陆地网络(NTN)融合的架构与功能切分策略，并分析了各种切分方案的权衡及RAN智能控制器(RIC)的部署。


<details>
  <summary>Details</summary>
Motivation: 陆地网络(TN)与非陆地网络(NTN)的融合面临独特的架构和功能挑战，这源于异构传播条件、动态拓扑和有限的星载处理能力。

Method: 提出符合O-RAN原则的TN-NTN集成系统架构和功能切分策略。构建了RAN和核心功能在卫星和地面节点之间分配的切分选项分类，并分析了各项权衡。评估了从纯星载DU部署到完整gNB和UPF集成到卫星的多种配置，并讨论了RIC的放置，提出了灵活的空间与地面切分策略。

Result: 分析了不同切分选项在性能、延迟、自主性和部署方面的权衡。提供了架构切分与RIC放置选项之间的全面映射，强调了实施约束和互操作性考虑。

Conclusion: 识别了关键挑战，并指出了未来在O-RAN背景下实现标准化、模块化和高效TN-NTN融合的方向。

Abstract: The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks
(NTNs) poses unique architectural and functional challenges due to
heterogeneous propagation conditions, dynamic topologies and limited on-board
processing capabilities. This paper examines architectural and functional split
strategies that are consistent with O-RAN principles for future integrated
TN-NTN systems. A taxonomy of split options is proposed that distributes RAN
and core functions between satellites and ground nodes, and trade-offs in terms
of performance, latency, autonomy and deployment are analysed. In particular,
we evaluate configurations ranging from pure on-board DU deployments to full
gNB and UPF integration into satellites, including variations based on intra-
and inter-satellite processing. In addition, the placement of Near-RT and
Non-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible
split strategies between space and ground to optimise the performance and
scalability of the control loop. A comprehensive mapping between architectural
splits and RIC placement options is provided, emphasising implementation
constraints and interoperability considerations. The paper concludes by
identifying key challenges and outlining future directions to enable
standardised, modular and efficient TN-NTN convergence in the context of the
O-RAN.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [211] [Integrating Large Language Models in Financial Investments and Market Analysis: A Survey](https://arxiv.org/abs/2507.01990)
*Sedigheh Mahdavi,Jiating,Chen,Pradeep Kumar Joshi,Lina Huertas Guativa,Upmanyu Singh*

Main category: q-fin.GN

TL;DR: 本综述探讨了大型语言模型（LLMs）在金融决策中的应用，系统回顾了其在投资策略中的研究进展，涵盖了主要框架、应用场景、能力与挑战，并指出了未来的发展方向。


<details>
  <summary>Details</summary>
Motivation: 传统投资策略在处理海量和多样化数据及实时决策方面存在局限。LLMs能够处理结构化和非结构化数据、提取洞察并增强实时决策能力，为金融领域带来了革新潜力。

Method: 本研究采用综述方法，系统性梳理和概述了LLMs在金融领域（特别是投资策略）的最新研究，将研究贡献归纳为四大框架：基于LLM的框架和管道、混合集成方法、微调与适应方法、基于代理的架构，并审查了其在多个具体应用中的进展。

Result: 本研究提供了LLMs在金融领域应用的结构化概述，成功将现有研究归类为四大主流框架。它还详细审查了LLMs在股票选择、风险评估、情绪分析、交易和金融预测等关键应用中的进展，并突出了LLMs的能力和面临的挑战。

Conclusion: LLMs在金融市场中具有显著增强分析和决策能力的前景。本研究通过回顾现有文献，明确了LLMs在该领域的能力、挑战以及未来的潜在研究方向。

Abstract: Large Language Models (LLMs) have been employed in financial decision making,
enhancing analytical capabilities for investment strategies. Traditional
investment strategies often utilize quantitative models, fundamental analysis,
and technical indicators. However, LLMs have introduced new capabilities to
process and analyze large volumes of structured and unstructured data, extract
meaningful insights, and enhance decision-making in real-time. This survey
provides a structured overview of recent research on LLMs within the financial
domain, categorizing research contributions into four main frameworks:
LLM-based Frameworks and Pipelines, Hybrid Integration Methods, Fine-Tuning and
Adaptation Approaches, and Agent-Based Architectures. This study provides a
structured review of recent LLMs research on applications in stock selection,
risk assessment, sentiment analysis, trading, and financial forecasting. By
reviewing the existing literature, this study highlights the capabilities,
challenges, and potential directions of LLMs in financial markets.

</details>


### [212] [Predicting and Explaining Customer Data Sharing in the Open Banking](https://arxiv.org/abs/2507.01987)
*João B. G. de Brito,Rodrigo Heldt,Cleo S. Silveira,Matthias Bogaert,Guilherme B. Bucco,Fernando B. Luce,João L. Becker,Filipe J. Zabala,Michel J. Anzanello*

Main category: q-fin.GN

TL;DR: 本研究针对开放银行环境下客户数据共享行为，利用XGBoost模型预测其倾向，并通过可解释性模型分析揭示关键影响因素，发现移动渠道互动和信用特征至关重要，为金融机构提供战略洞察。


<details>
  <summary>Details</summary>
Motivation: 开放银行的出现深刻改变了金融数据管理，为金融机构带来了市场动态和营销策略上的转变，既有机遇也有挑战。机构需在管理数据流入以优化产品服务与规避数据流出以防竞争对手之间取得平衡。因此，理解并预测客户数据共享倾向至关重要。

Method: 本研究提出一个框架来预测客户通过开放银行共享数据的倾向，并通过可解释模型分析（EMA）解释其行为。使用来自巴西一家大型金融机构的约320万客户数据，采用ADASYN和NEARMISS混合数据平衡策略处理数据共享的低频问题，以增强XGBoost模型的训练。EMA阶段结合Shapley Additive Explanations（SHAP）方法和分类回归树（CART）技术来揭示客户决策的最重要影响特征。

Result: XGBoost模型准确预测了客户数据共享行为，数据流入预测准确率为91.39%，数据流出预测准确率为91.53%。通过EMA，研究发现影响客户决策的关键特征包括移动渠道的交易和购买数量、这些渠道内的互动，以及信用相关特征，特别是全国银行系统的信用卡使用情况。

Conclusion: 研究结果强调了移动参与度和信用在驱动客户数据共享行为中的关键作用，为金融机构在开放银行环境下提升竞争力和创新提供了战略性见解。

Abstract: The emergence of Open Banking represents a significant shift in financial
data management, influencing financial institutions' market dynamics and
marketing strategies. This increased competition creates opportunities and
challenges, as institutions manage data inflow to improve products and services
while mitigating data outflow that could aid competitors. This study introduces
a framework to predict customers' propensity to share data via Open Banking and
interprets this behavior through Explanatory Model Analysis (EMA). Using data
from a large Brazilian financial institution with approximately 3.2 million
customers, a hybrid data balancing strategy incorporating ADASYN and NEARMISS
techniques was employed to address the infrequency of data sharing and enhance
the training of XGBoost models. These models accurately predicted customer data
sharing, achieving 91.39% accuracy for inflow and 91.53% for outflow. The EMA
phase combined the Shapley Additive Explanations (SHAP) method with the
Classification and Regression Tree (CART) technique, revealing the most
influential features on customer decisions. Key features included the number of
transactions and purchases in mobile channels, interactions within these
channels, and credit-related features, particularly credit card usage across
the national banking system. These results highlight the critical role of
mobile engagement and credit in driving customer data-sharing behaviors,
providing financial institutions with strategic insights to enhance
competitiveness and innovation in the Open Banking environment.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [213] [Resolving Turbulent Magnetohydrodynamics: A Hybrid Operator-Diffusion Framework](https://arxiv.org/abs/2507.02106)
*Semih Kacmaz,E. A. Huerta,Roland Haas*

Main category: physics.flu-dyn

TL;DR: 本文提出一种结合物理信息神经网络算子（PINOs）和扩散模型的混合机器学习框架，用于高精度模拟二维不可压缩电阻磁流体动力学（MHD）湍流的全时空演化，特别在传统方法难以处理的高雷诺数下表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统确定性代理模型在模拟高雷诺数下充分发展的磁流体动力学（MHD）湍流时存在局限性，难以准确捕捉其高频残差、非高斯统计和间歇结构，因此需要一种能够更准确、更广泛地模拟此类复杂湍流的新方法。

Method: 本研究采用一种混合机器学习框架，将物理信息神经网络算子（PINOs）与基于分数的生成扩散模型相结合。PINOs负责预测连贯的低频动力学，而条件扩散模型则随机校正高频残差。该模型在涵盖Re ∈ {100, 250, 500, 750, 1000, 3000, 10000}的综合高精度模拟数据集上进行训练。

Result: 该方法在传统确定性代理模型难以企及的雷诺数区间内（如Re=1000, 3000, 10000）实现了最先进的精度。在Re=1000和3000时，模型忠实地重建了速度场和磁场的完整谱能量分布，并高保真地捕获了非高斯统计、间歇结构和跨场关联。在极端湍流水平（Re=10000）下，该模型是首个能够恢复磁场高波数演化、保留大尺度形态并实现统计意义上有效预测的代理模型。

Conclusion: 该混合机器学习框架成功克服了传统方法在模拟高雷诺数MHD湍流方面的限制，显著提升了对复杂湍流现象的模拟准确性和范围，尤其在极端湍流条件下对磁场演化的捕捉能力，为MHD湍流的建模提供了开创性且具有统计意义的新途径。

Abstract: We present a hybrid machine learning framework that combines Physics-Informed
Neural Operators (PINOs) with score-based generative diffusion models to
simulate the full spatio-temporal evolution of two-dimensional, incompressible,
resistive magnetohydrodynamic (MHD) turbulence across a broad range of Reynolds
numbers ($\mathrm{Re}$). The framework leverages the equation-constrained
generalization capabilities of PINOs to predict coherent, low-frequency
dynamics, while a conditional diffusion model stochastically corrects
high-frequency residuals, enabling accurate modeling of fully developed
turbulence. Trained on a comprehensive ensemble of high-fidelity simulations
with $\mathrm{Re} \in \{100, 250, 500, 750, 1000, 3000, 10000\}$, the approach
achieves state-of-the-art accuracy in regimes previously inaccessible to
deterministic surrogates. At $\mathrm{Re}=1000$ and $3000$, the model
faithfully reconstructs the full spectral energy distributions of both velocity
and magnetic fields late into the simulation, capturing non-Gaussian
statistics, intermittent structures, and cross-field correlations with high
fidelity. At extreme turbulence levels ($\mathrm{Re}=10000$), it remains the
first surrogate capable of recovering the high-wavenumber evolution of the
magnetic field, preserving large-scale morphology and enabling statistically
meaningful predictions.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [214] [Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency](https://arxiv.org/abs/2507.02135)
*Zongpu Zhang,Pranab Dash,Y. Charlie Hu,Qiang Xu,Jian Li,Haibing Guan*

Main category: cs.OS

TL;DR: LLMs在移动设备上因独立DVFS调控器导致能效低下，本文提出FUSE统一调控器，显著降低延迟并提高能效。


<details>
  <summary>Details</summary>
Motivation: 将LLMs部署到资源受限的移动设备上面临计算、内存和能耗方面的巨大挑战。现有的移动LLM框架中，CPU、GPU和内存的独立DVFS调控器彼此不协同，导致LLM推理效率低下。

Method: 首先，测量了SOTA移动LLM框架的能效，发现独立调控器导致高达40.4%的延迟增加。其次，深入研究了移动调控器之间缺乏协同作用导致效率低下的原因。最后，基于这些洞察，设计并实现了一个统一的、能耗感知的调控器——FUSE，用于优化移动设备上LLM推理的能效。

Result: 研究发现，现有独立移动调控器在相同能耗下，导致预填充和解码延迟最高延长40.4%。通过使用ShareGPT数据集评估，FUSE在相同每token能耗下，平均将首token延迟降低7.0%-16.9%，每输出token延迟降低25.4%-36.8%。

Conclusion: FUSE作为一种统一的能耗感知调控器，有效解决了移动设备上LLM推理的能效问题，显著降低了延迟，提高了性能。

Abstract: Large Language Models (LLMs) are increasingly being integrated into various
applications and services running on billions of mobile devices. However,
deploying LLMs on resource-limited mobile devices faces a significant challenge
due to their high demand for computation, memory, and ultimately energy. While
current LLM frameworks for mobile use three power-hungry components-CPU, GPU,
and Memory-even when running primarily-GPU LLM models, optimized DVFS governors
for CPU, GPU, and memory featured in modern mobile devices operate
independently and are oblivious of each other. Motivated by the above
observation, in this work, we first measure the energy-efficiency of a SOTA LLM
framework consisting of various LLM models on mobile phones which showed the
triplet mobile governors result in up to 40.4% longer prefilling and decoding
latency compared to optimal combinations of CPU, GPU, and memory frequencies
with the same energy consumption for sampled prefill and decode lengths.
Second, we conduct an in-depth measurement study to uncover how the intricate
interplay (or lack of) among the mobile governors cause the above inefficiency
in LLM inference. Finally, based on these insights, we design FUSE - a unified
energy-aware governor for optimizing the energy efficiency of LLM inference on
mobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the
time-to-first-token and time-per-output-token latencies by 7.0%-16.9% and
25.4%-36.8% on average with the same energy-per-token for various mobile LLM
models.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [215] [Early Signs of Steganographic Capabilities in Frontier LLMs](https://arxiv.org/abs/2507.02737)
*Artur Zolkowski,Kei Nishimura-Gasparian,Robert McCarthy,Roland S. Zimmermann,David Lindner*

Main category: cs.CR

TL;DR: 本文评估了前沿大型语言模型（LLM）的隐写能力，以理解其规避监测的风险。研究发现当前模型隐写能力尚不成熟，但在特定条件下可成功，且未来可能增强。


<details>
  <summary>Details</summary>
Motivation: 监测大型语言模型（LLM）的输出对于缓解其滥用和失准带来的风险至关重要。然而，LLM可能通过隐写术（在看似无害的生成内容中编码隐藏信息）来规避监测，因此有必要评估其隐写能力以理解潜在风险。

Method: 研究评估了两种类型的隐写术：传递编码信息和执行编码推理。研究在标准条件下以及给定额外辅助条件（如使用未监控的草稿本和协调编码方案）下进行了测试，并考察了模型在简单状态跟踪问题中进行基本编码推理的能力，包括使用自有和预定义的编码方案（如十六进制）。

Result: 在标准条件下，当前模型无法在输出中隐匿短消息而不被监测发现。然而，在获得额外辅助条件时，它们能够成功。模型表现出进行基本编码推理的早期迹象，包括使用自有和预定义方案进行推理的能力。尽管如此，它们很少能巧妙地在掩护任务中隐藏推理以欺骗监测器。

Conclusion: 总体而言，研究结果表明当前LLM展现出初级的隐写能力。虽然这些能力目前可能不足以绕过精心设计的监测器，但未来这种能力可能会增强，因此需要持续关注。

Abstract: Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks
from misuse and misalignment. However, LLMs could evade monitoring through
steganography: Encoding hidden information within seemingly benign generations.
In this paper, we evaluate the steganography capabilities in frontier LLMs to
better understand the risk they pose. We focus on two types of steganography:
passing encoded messages and performing encoded reasoning. We find that current
models are unable to encode short messages in their outputs without a monitor
noticing under standard affordances. They can succeed, however, if given
additional affordances such as using an unmonitored scratchpad and coordinating
on what encoding scheme to use. We additionally find early signs that models
can perform basic encoded reasoning in a simple state-tracking problem. This
includes some ability to reason with their own and pre-defined schemes,
including encoding schemes such as Hexadecimal. Despite this, they can rarely
hide reasoning subtly within a cover task to fool a monitor. Overall, our
results indicate that current LLMs exhibit nascent steganographic capabilities.
While these capabilities are likely insufficient to bypass well-designed
monitors at present, this could change in the future.

</details>


### [216] [MGC: A Compiler Framework Exploiting Compositional Blindness in Aligned LLMs for Malware Generation](https://arxiv.org/abs/2507.02057)
*Lu Yan,Zhuo Zhang,Xiangzhe Xu,Shengwei An,Guangyu Shen,Zhou Xuan,Xuan Chen,Xiangyu Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种名为MGC（恶意软件生成编译器）的框架，通过将恶意操作分解为看似无害的子任务，利用大型语言模型（LLM）的漏洞来规避现有安全防护并生成功能性恶意软件。


<details>
  <summary>Details</summary>
Motivation: LLM降低了软件开发门槛，包括恶意软件。尽管LLM提供商有对齐机制以阻止恶意代码生成，但这些机制多独立评估提示，忽略了恶意操作可分解为无害子任务的漏洞，这带来了严重的安全隐患。

Method: 引入恶意软件生成编译器（MGC），该框架利用模块化分解和规避对齐的生成方式。MGC使用专门的恶意软件描述中间表示（MDIR），以连接高层恶意意图和看似无害的代码片段。

Result: MGC能可靠地生成功能性恶意软件，在多样任务和类别上均表现出色。在三个基准数据集上，其正确性比越狱方法高365.79%，比地下服务高78.07%。案例研究表明，MGC可复现甚至增强16个真实世界的恶意软件样本。

Conclusion: 这项工作通过揭示针对对齐AI系统的组合攻击风险，为安全研究人员提供了关键见解。

Abstract: Large language models (LLMs) have democratized software development, reducing
the expertise barrier for programming complex applications. This accessibility
extends to malicious software development, raising significant security
concerns. While LLM providers have implemented alignment mechanisms to prevent
direct generation of overtly malicious code, these safeguards predominantly
evaluate individual prompts in isolation, overlooking a critical vulnerability:
malicious operations can be systematically decomposed into benign-appearing
sub-tasks. In this paper, we introduce the Malware Generation Compiler (MGC), a
novel framework that leverages this vulnerability through modular decomposition
and alignment-evasive generation. MGC employs a specialized Malware Description
Intermediate Representation (MDIR) to bridge high-level malicious intents and
benign-appearing code snippets. Extensive evaluation demonstrates that our
attack reliably generates functional malware across diverse task specifications
and categories, outperforming jailbreaking methods by +365.79% and underground
services by +78.07% in correctness on three benchmark datasets. Case studies
further show that MGC can reproduce and even enhance 16 real-world malware
samples. This work provides critical insights for security researchers by
exposing the risks of compositional attacks against aligned AI systems.
Demonstrations are available at
https://sites.google.com/view/malware-generation-compiler.

</details>


### [217] [Can Artificial Intelligence solve the blockchain oracle problem? Unpacking the Challenges and Possibilities](https://arxiv.org/abs/2507.02125)
*Giulio Caldarelli*

Main category: cs.CR

TL;DR: 本文探讨AI在解决区块链预言机问题中的作用，分析AI技术（如异常检测、事实提取）如何增强预言机系统，并得出结论：AI能提升数据质量和系统弹性，但无法完全消除对链下输入的信任依赖，应作为补充层而非替代品。


<details>
  <summary>Details</summary>
Motivation: 区块链预言机问题（将可靠外部数据注入去中心化系统）是去信任化应用发展的核心限制。现有解决方案未能完全解决区块链获取链下知识的根本问题，因此需要探讨人工智能在此方面的潜力。

Method: 本研究是一篇立场论文，通过批判性评估的方式，探讨人工智能在解决预言机问题中的作用。方法包括借鉴学术文献和实践实现，并具体分析异常检测、基于语言的事实提取、动态声誉建模和对抗性抵抗等AI技术如何增强预言机系统。

Result: 研究发现，AI为预言机系统带来了提升数据质量、源选择和系统弹性的强大工具。然而，AI无法完全消除对不可验证链下输入的依赖性。

Conclusion: 人工智能应被视为更广泛预言机设计中一种补充性的推理和过滤层，而非替代信任假设的方案。

Abstract: The blockchain oracle problem, which refers to the challenge of injecting
reliable external data into decentralized systems, remains a fundamental
limitation to the development of trustless applications. While recent years
have seen a proliferation of architectural, cryptographic, and economic
strategies to mitigate this issue, no one has yet fully resolved the
fundamental question of how a blockchain can gain knowledge about the off-chain
world. In this position paper, we critically assess the role artificial
intelligence (AI) can play in tackling the oracle problem. Drawing from both
academic literature and practitioner implementations, we examine how AI
techniques such as anomaly detection, language-based fact extraction, dynamic
reputation modeling, and adversarial resistance can enhance oracle systems. We
observe that while AI introduces powerful tools for improving data quality,
source selection, and system resilience, it cannot eliminate the reliance on
unverifiable off-chain inputs. Therefore, this study supports the idea that AI
should be understood as a complementary layer of inference and filtering within
a broader oracle design, not a substitute for trust assumptions.

</details>


### [218] [EIM-TRNG: Obfuscating Deep Neural Network Weights with Encoding-in-Memory True Random Number Generator via RowHammer](https://arxiv.org/abs/2507.02206)
*Ranyang Zhou,Abeer Matar A. Almalky,Gamana Aragonda,Sabbir Ahmed,Filip Roth Trønnes-Christensen,Adnan Siraj Rakin,Shaahin Angizi*

Main category: cs.CR

TL;DR: 该研究首次提出一种基于DRAM RowHammer效应的EIM-TRNG，利用其物理随机性生成真随机数，并应用于保护深度神经网络的权重数据。


<details>
  <summary>Details</summary>
Motivation: 硬件安全和加密系统亟需不可预测且有韧性的真随机数生成器（TRNGs），而现有软件伪随机数生成器无法满足。同时，保护深度神经网络（DNN）模型参数（特别是权重）的完整性、隐私和知识产权至关重要。

Method: 提出了一种新颖的内存编码真随机数生成器（EIM-TRNG），它利用DRAM单元在RowHammer诱导扰动下产生的不可预测位翻转作为可靠的熵源。该TRNG框架用于通过固定和不可预测位翻转的组合来编码DNN权重数据，并通过基于概率翻转行为的密钥进行解密，以实现数据加密和模型认证。

Result: 研究结果验证了基于DRAM的熵提取对于实现鲁棒、低成本硬件安全的有效性。通过所提出的方法，成功确保了DNN数据的保密性和模型的真实性。

Conclusion: 该工作为在硬件层面保护机器学习模型提供了一个有前景的方向。

Abstract: True Random Number Generators (TRNGs) play a fundamental role in hardware
security, cryptographic systems, and data protection. In the context of Deep
NeuralNetworks (DNNs), safeguarding model parameters, particularly weights, is
critical to ensure the integrity, privacy, and intel-lectual property of AI
systems. While software-based pseudo-random number generators are widely used,
they lack the unpredictability and resilience offered by hardware-based TRNGs.
In this work, we propose a novel and robust Encoding-in-Memory TRNG called
EIM-TRNG that leverages the inherent physical randomness in DRAM cell behavior,
particularly under RowHammer-induced disturbances, for the first time. We
demonstrate how the unpredictable bit-flips generated through carefully
controlled RowHammer operations can be harnessed as a reliable entropy source.
Furthermore, we apply this TRNG framework to secure DNN weight data by encoding
via a combination of fixed and unpredictable bit-flips. The encrypted data is
later decrypted using a key derived from the probabilistic flip behavior,
ensuring both data confidentiality and model authenticity. Our results validate
the effectiveness of DRAM-based entropy extraction for robust, low-cost
hardware security and offer a promising direction for protecting machine
learning models at the hardware level.

</details>


### [219] [Evaluating Language Models For Threat Detection in IoT Security Logs](https://arxiv.org/abs/2507.02390)
*Jorge J. Tejero-Fernández,Alfonso Sánchez-Macián*

Main category: cs.CR

TL;DR: 本文提出一个使用微调大型语言模型（LLMs）对物联网（IoT）安全日志进行异常检测和缓解推荐的管道。研究发现LLMs在多类别攻击分类上优于传统机器学习模型，并能结合检测提供缓解建议。


<details>
  <summary>Details</summary>
Motivation: 网络安全中的日志分析是检测网络和系统威胁的重要信息来源。本研究旨在探索并利用大型语言模型（LLMs）来改进物联网安全日志的异常检测和缓解建议能力。

Method: 构建了一个利用微调LLMs进行IoT安全日志异常检测和缓解推荐的管道。该研究将三种开源LLMs（采用零样本、少样本提示和微调策略）与经典机器学习分类器作为基线，在IoT数据集上进行了二分类和多类别异常检测性能比较。此外，通过将检测到的威胁映射到MITRE CAPEC框架，定义IoT特定的缓解措施，并用这些措施微调模型，使其能够提供组合的检测和推荐指导。

Result: 实验结果显示，LLMs在多类别攻击分类上的表现优于相应的基线模型。通过威胁映射和缓解措施的微调，模型成功地提供了结合威胁检测与缓解建议的指导。

Conclusion: 微调LLMs在物联网安全日志的异常检测方面显示出优越性，尤其是在多类别攻击分类上。结合威胁映射和定制化缓解措施，这些模型能够为网络安全威胁提供有价值的综合检测和推荐方案。

Abstract: Log analysis is a relevant research field in cybersecurity as they can
provide a source of information for the detection of threats to networks and
systems. This paper presents a pipeline to use fine-tuned Large Language Models
(LLMs) for anomaly detection and mitigation recommendation using IoT security
logs. Utilizing classical machine learning classifiers as a baseline, three
open-source LLMs are compared for binary and multiclass anomaly detection, with
three strategies: zero-shot, few-shot prompting and fine-tuning using an IoT
dataset. LLMs give better results on multi-class attack classification than the
corresponding baseline models. By mapping detected threats to MITRE CAPEC,
defining a set of IoT-specific mitigation actions, and fine-tuning the models
with those actions, the models are able to provide a combined detection and
recommendation guidance.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [220] [Accelerated Portfolio Optimization and Option Pricing with Reinforcement Learning](https://arxiv.org/abs/2507.01972)
*Hadi Keramati,Samaneh Jazayeri*

Main category: q-fin.PM

TL;DR: 本研究提出一个基于强化学习的框架，用于优化迭代求解器中块预处理器的大小，从而显著加速投资组合优化和期权定价中大型线性系统的收敛速度并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 投资组合优化和期权定价涉及的大型线性系统，由于直接求逆计算成本高昂，通常采用迭代求解器。然而，病态系统会导致收敛缓慢，且传统预处理技术需要耗时的特定问题参数调优。研究动机是克服这些局限性，实现动态且高效的预处理。

Method: 使用强化学习（RL）框架，动态调整迭代求解器中的块预处理器大小，以加速其收敛。

Result: 在真实世界的投资组合优化矩阵上的评估表明，该RL框架能够有效调整预处理，显著加速收敛并降低计算成本。

Conclusion: 所提出的加速求解器能够支持动态投资组合配置和实时期权定价中更快的决策制定，从而提高了这些金融应用的效率。

Abstract: We present a reinforcement learning (RL)-driven framework for optimizing
block-preconditioner sizes in iterative solvers used in portfolio optimization
and option pricing. The covariance matrix in portfolio optimization or the
discretization of differential operators in option pricing models lead to large
linear systems of the form $\mathbf{A}\textbf{x}=\textbf{b}$. Direct inversion
of high-dimensional portfolio or fine-grid option pricing incurs a significant
computational cost. Therefore, iterative methods are usually used for
portfolios in real-world situations. Ill-conditioned systems, however, suffer
from slow convergence. Traditional preconditioning techniques often require
problem-specific parameter tuning. To overcome this limitation, we rely on RL
to dynamically adjust the block-preconditioner sizes and accelerate iterative
solver convergence. Evaluations on a suite of real-world portfolio optimization
matrices demonstrate that our RL framework can be used to adjust
preconditioning and significantly accelerate convergence and reduce
computational cost. The proposed accelerated solver supports faster
decision-making in dynamic portfolio allocation and real-time option pricing.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [221] [HelixDesign-Antibody: A Scalable Production-Grade Platform for Antibody Design Built on HelixFold3](https://arxiv.org/abs/2507.02345)
*Jie Gao,Jing Hu,Shanzhuo Zhang,Kunrui Zhu,Sheng Qian,Yueyang Huang,Xiaonan Zhang,Xiaomin Fang*

Main category: q-bio.BM

TL;DR: 介绍了一个基于HelixFold3的高通量抗体设计平台HelixDesign-Antibody，利用AI加速大规模抗体序列生成和筛选，有效解决了传统方法的耗时问题，并验证了其生成多样化高质量抗体的能力。


<details>
  <summary>Details</summary>
Motivation: 传统的抗体发现方法依赖耗时且资源密集型的实验筛选，为了增强并简化这一过程，需要一个高效的解决方案。

Method: 开发了基于高精度结构预测模型HelixFold3的生产级高通量平台HelixDesign-Antibody。该平台能够大规模生成抗体候选序列并评估其与抗原的相互作用，并集成了高性能计算（HPC）支持，以解决计算需求高和工具链碎片化的问题。

Result: 在多个抗原上的验证表明，该平台能够生成多样化且高质量的抗体。研究还证实了一个标度律：探索更大的序列空间能增加发现最佳结合物的可能性。

Conclusion: 该平台为大规模抗体设计提供了一个无缝且可访问的解决方案，极大地简化了发现过程。该平台已通过PaddleHelix平台开放。

Abstract: Antibody engineering is essential for developing therapeutics and advancing
biomedical research. Traditional discovery methods often rely on time-consuming
and resource-intensive experimental screening. To enhance and streamline this
process, we introduce a production-grade, high-throughput platform built on
HelixFold3, HelixDesign-Antibody, which utilizes the high-accuracy structure
prediction model, HelixFold3. The platform facilitates the large-scale
generation of antibody candidate sequences and evaluates their interaction with
antigens. Integrated high-performance computing (HPC) support enables
high-throughput screening, addressing challenges such as fragmented toolchains
and high computational demands. Validation on multiple antigens showcases the
platform's ability to generate diverse and high-quality antibodies, confirming
a scaling law where exploring larger sequence spaces increases the likelihood
of identifying optimal binders. This platform provides a seamless, accessible
solution for large-scale antibody design and is available via the antibody
design page of PaddleHelix platform.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [222] [DeepSupp: Attention-Driven Correlation Pattern Analysis for Dynamic Time Series Support and Resistance Levels Identification](https://arxiv.org/abs/2507.01971)
*Boris Kriuk,Logic Ng,Zarif Al Hossain*

Main category: q-fin.ST

TL;DR: DeepSupp是一种基于深度学习（多头注意力机制）的新方法，用于在波动市场中准确识别金融支撑位，通过动态特征工程和无监督聚类优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统支撑位识别方法难以适应现代波动市场。现有机器学习研究多关注价格预测而非结构性水平识别，存在研究空白。

Method: 提出DeepSupp，一种深度学习方法。它利用多头注意力机制分析空间相关性和市场微观结构，整合高级特征工程构建动态相关矩阵，并采用基于注意力的自编码器进行鲁棒表示学习。最终的支撑位通过无监督聚类（DBSCAN）提取。

Result: DeepSupp在S&P 500股票上进行全面评估，超越六种基线方法，在六项金融指标（包括支撑准确性和市场状态敏感性）上达到最先进的性能，并在不同市场条件下表现一致。

Conclusion: DeepSupp有效弥补了支撑位识别的关键空白，为现代金融分析提供可扩展且可靠的解决方案，并展示了基于注意力架构在揭示细微市场模式和改进技术交易策略方面的潜力。

Abstract: Support and resistance (SR) levels are central to technical analysis, guiding
traders in entry, exit, and risk management. Despite widespread use,
traditional SR identification methods often fail to adapt to the complexities
of modern, volatile markets. Recent research has introduced machine learning
techniques to address the following challenges, yet most focus on price
prediction rather than structural level identification. This paper presents
DeepSupp, a new deep learning approach for detecting financial support levels
using multi-head attention mechanisms to analyze spatial correlations and
market microstructure relationships. DeepSupp integrates advanced feature
engineering, constructing dynamic correlation matrices that capture evolving
market relationships, and employs an attention-based autoencoder for robust
representation learning. The final support levels are extracted through
unsupervised clustering, leveraging DBSCAN to identify significant price
thresholds. Comprehensive evaluations on S&P 500 tickers demonstrate that
DeepSupp outperforms six baseline methods, achieving state-of-the-art
performance across six financial metrics, including essential support accuracy
and market regime sensitivity. With consistent results across diverse market
conditions, DeepSupp addresses critical gaps in SR level detection, offering a
scalable and reliable solution for modern financial analysis. Our approach
highlights the potential of attention-based architectures to uncover nuanced
market patterns and improve technical trading strategies.

</details>


### [223] [Forecasting Labor Markets with LSTNet: A Multi-Scale Deep Learning Approach](https://arxiv.org/abs/2507.01979)
*Adam Nelson-Archer,Aleia Sen,Meena Al Hasani,Sofia Davila,Jessica Le,Omar Abbouchi*

Main category: q-fin.ST

TL;DR: 该研究提出一种基于深度学习（LSTNet）的方法，利用美国劳工统计局数据预测短期就业变化并评估长期行业健康，该方法在大多数行业中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 旨在通过分析劳动力市场数据，实现对短期就业变化的预测以及对行业长期健康状况的评估。

Method: 采用深度学习中的长短期时间序列网络（LSTNet），处理多元时间序列数据，包括就业水平、工资、离职率和职位空缺等，输出7天就业预测和可解释的行业就业健康指数（IEHI）。

Result: 该方法在大多数行业（特别是在稳定行业）的表现优于基线模型，并且行业就业健康指数（IEHI）的排名与实际就业波动性显示出高度一致性。

Conclusion: 所提出的深度学习方法在就业预测和行业健康评估方面表现有效，未来将致力于改进模型的可解释性和泛化能力。

Abstract: We present a deep learning approach for forecasting short-term employment
changes and assessing long-term industry health using labor market data from
the U.S. Bureau of Labor Statistics. Our system leverages a Long- and
Short-Term Time-series Network (LSTNet) to process multivariate time series
data, including employment levels, wages, turnover rates, and job openings. The
model outputs both 7-day employment forecasts and an interpretable Industry
Employment Health Index (IEHI). Our approach outperforms baseline models across
most sectors, particularly in stable industries, and demonstrates strong
alignment between IEHI rankings and actual employment volatility. We discuss
error patterns, sector-specific performance, and future directions for
improving interpretability and generalization.

</details>


### [224] [NGAT: A Node-level Graph Attention Network for Long-term Stock Prediction](https://arxiv.org/abs/2507.02018)
*Yingjie Niu,Mingchuan Zhao,Valerio Poti,Ruihai Dong*

Main category: q-fin.ST

TL;DR: 该研究旨在解决金融领域图表示学习中，关系信息利用不足、模型复杂性高及图结构比较困难的问题。通过提出一个长期股票预测任务和节点级图注意力网络（NGAT），有效提升了预测性能，并揭示了现有图比较方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有图表示学习方法在金融应用中面临三项挑战：1) 关系信息的优势被下游任务设计所限制；2) 现有股票预测图模型过于复杂且泛化性差；3) 基于经验的公司关系图构建缺乏有效的图结构比较方法。

Method: 本文提出一个长期股票预测任务，并为此开发了一个专门用于公司关系图的节点级图注意力网络（NGAT）。此外，研究通过实验论证了现有图比较方法基于模型下游任务性能的局限性。

Result: 在两个数据集上的实验结果一致表明，所提出的任务和模型的有效性。项目代码已开源以促进复现和未来研究。

Conclusion: 所提出的长期股票预测任务和NGAT模型能有效提升金融应用中公司表示的质量和股票预测的准确性，解决了当前图学习方法在实际应用中的关键挑战，并对图结构比较方法提出了新的见解。

Abstract: Graph representation learning methods have been widely adopted in financial
applications to enhance company representations by leveraging inter-firm
relationships. However, current approaches face three key challenges: (1) The
advantages of relational information are obscured by limitations in downstream
task designs; (2) Existing graph models specifically designed for stock
prediction often suffer from excessive complexity and poor generalization; (3)
Experience-based construction of corporate relationship graphs lacks effective
comparison of different graph structures. To address these limitations, we
propose a long-term stock prediction task and develop a Node-level Graph
Attention Network (NGAT) specifically tailored for corporate relationship
graphs. Furthermore, we experimentally demonstrate the limitations of existing
graph comparison methods based on model downstream task performance.
Experimental results across two datasets consistently demonstrate the
effectiveness of our proposed task and model. The project is publicly available
on GitHub to encourage reproducibility and future research.

</details>


### [225] [Forecasting Nigerian Equity Stock Returns Using Long Short-Term Memory Technique](https://arxiv.org/abs/2507.01964)
*Adebola K. Ojo,Ifechukwude Jude Okafor*

Main category: q-fin.ST

TL;DR: 本研究利用LSTM模型预测股票收益，在尼日利亚股票数据集上取得了超过90%的预测准确率，表明LSTM在金融时间序列预测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 投资者和分析师在预测股票回报及做出明智投资决策时面临重大挑战；尽管股票回报的可预测性可提升投资者信心，但这仍然是一项艰巨的任务。

Method: 本研究采用长短期记忆（LSTM）模型预测未来股票市场走势。使用来自尼日利亚证券交易所（NSE）的历史数据集，经过清洗和归一化处理。模型通过性能指标进行评估，并与人工神经网络（ANN）和卷积神经网络（CNN）等其他深度学习模型进行比较。

Result: 实验结果显示，经过可靠数据集训练的LSTM模型能够以超过90%的准确率预测未来的股票价格和回报。

Conclusion: 研究得出结论，经过良好训练的LSTM模型在预测金融时间序列相关问题上具有实用价值。

Abstract: Investors and stock market analysts face major challenges in predicting stock
returns and making wise investment decisions. The predictability of equity
stock returns can boost investor confidence, but it remains a difficult task.
To address this issue, a study was conducted using a Long Short-term Memory
(LSTM) model to predict future stock market movements. The study used a
historical dataset from the Nigerian Stock Exchange (NSE), which was cleaned
and normalized to design the LSTM model. The model was evaluated using
performance metrics and compared with other deep learning models like
Artificial and Convolutional Neural Networks (CNN). The experimental results
showed that the LSTM model can predict future stock market prices and returns
with over 90% accuracy when trained with a reliable dataset. The study
concludes that LSTM models can be useful in predicting financial
time-series-related problems if well-trained. Future studies should explore
combining LSTM models with other deep learning techniques like CNN to create
hybrid models that mitigate the risks associated with relying on a single model
for future equity stock predictions.

</details>


### [226] [News Sentiment Embeddings for Stock Price Forecasting](https://arxiv.org/abs/2507.01970)
*Ayaan Qayyum*

Main category: q-fin.ST

TL;DR: 本文研究如何利用新闻标题数据预测SPY股票价格，通过OpenAI文本嵌入和PCA处理华尔街日报标题，并结合其他金融经济数据。初步结果显示，标题数据嵌入能使预测性能提升至少40%。


<details>
  <summary>Details</summary>
Motivation: 旨在利用新闻标题数据预测SPY股票价格的日常波动，克服新闻对股价时间相关和时间无关的细微影响、潜在滞后效应及市场噪音带来的挑战。

Method: 收集华尔街日报新闻标题、SPY股票价格以及美元指数（DXY）和国债收益率等金融经济数据。采用基于OpenAI的文本嵌入模型将新闻标题编码为向量，并使用主成分分析（PCA）提取关键特征。训练了超过390个机器学习推断模型。

Result: 初步结果显示，与未使用新闻标题数据嵌入的机器学习系统相比，结合标题数据嵌入能使股票价格预测性能提升至少40%。

Conclusion: 新闻标题数据嵌入对股票价格预测具有显著的积极效益。

Abstract: This paper will discuss how headline data can be used to predict stock
prices. The stock price in question is the SPDR S&P 500 ETF Trust, also known
as SPY that tracks the performance of the largest 500 publicly traded
corporations in the United States. A key focus is to use news headlines from
the Wall Street Journal (WSJ) to predict the movement of stock prices on a
daily timescale with OpenAI-based text embedding models used to create vector
encodings of each headline with principal component analysis (PCA) to exact the
key features. The challenge of this work is to capture the time-dependent and
time-independent, nuanced impacts of news on stock prices while handling
potential lag effects and market noise. Financial and economic data were
collected to improve model performance; such sources include the U.S. Dollar
Index (DXY) and Treasury Interest Yields. Over 390 machine-learning inference
models were trained. The preliminary results show that headline data embeddings
greatly benefit stock price prediction by at least 40% compared to training and
optimizing a machine learning system without headline data embeddings.

</details>


### [227] [Detecting Fraud in Financial Networks: A Semi-Supervised GNN Approach with Granger-Causal Explanations](https://arxiv.org/abs/2507.01980)
*Linh Nguyen,Marcel Boersma,Erman Acar*

Main category: q-fin.ST

TL;DR: 金融欺诈检测面临数据标签稀疏和模型解释性不足的挑战。本文提出SAGE-FIN，一种半监督图神经网络，利用Granger因果关系提供可解释性，并在真实金融数据集上验证了其有效性和良好性能。


<details>
  <summary>Details</summary>
Motivation: 金融欺诈每年造成数十亿美元损失，欺诈检测是一项技术性挑战。现有机器学习方法面临两大主要难题：1) 数据标签稀疏，导致模型训练困难且成本高昂；2) 模型缺乏解释性，无法满足业务监管要求。

Method: 本文提出SAGE-FIN，一种基于半监督图神经网络（GNN）的方法，专为金融交互网络设计。SAGE-FIN能够利用弱标记或无标记数据点识别欺诈活动，并通过Granger因果关系高亮网络中的相关项来解释被标记的欺诈项，以满足监管对解释性的要求。

Result: SAGE-FIN在真实世界的Bipartite Edge-And-Node Attributed金融网络数据集（Elliptic++）上进行了实证验证，结果显示其性能表现良好，并能为识别出的欺诈项提供Granger因果解释，且无需对网络结构做任何预设。

Conclusion: SAGE-FIN为金融欺诈检测提供了一种有效且可解释的解决方案，成功应对了数据标签稀疏和模型解释性不足的挑战，并在实际应用中展现出优异的欺诈识别能力和监管要求的可解释性。

Abstract: Fraudulent activity in the financial industry costs billions annually.
Detecting fraud, therefore, is an essential yet technically challenging task
that requires carefully analyzing large volumes of data. While machine learning
(ML) approaches seem like a viable solution, applying them successfully is not
so easy due to two main challenges: (1) the sparsely labeled data, which makes
the training of such approaches challenging (with inherent labeling costs), and
(2) lack of explainability for the flagged items posed by the opacity of ML
models, that is often required by business regulations. This article proposes
SAGE-FIN, a semi-supervised graph neural network (GNN) based approach with
Granger causal explanations for Financial Interaction Networks. SAGE-FIN learns
to flag fraudulent items based on weakly labeled (or unlabelled) data points.
To adhere to regulatory requirements, the flagged items are explained by
highlighting related items in the network using Granger causality. We
empirically validate the favorable performance of SAGE-FIN on a real-world
dataset, Bipartite Edge-And-Node Attributed financial network (Elliptic++),
with Granger-causal explanations for the identified fraudulent items without
any prior assumption on the network structure.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [228] [FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI Disclosures in Financial Reports](https://arxiv.org/abs/2507.01991)
*Muhammad Bilal Zafar*

Main category: q-fin.CP

TL;DR: 本研究提出FinAI-BERT，一个领域适应的Transformer模型，用于在句子层面自动识别金融文本中的AI相关披露，取得了卓越的性能并解决了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 金融服务中AI的普及促使市场对系统性检测企业备案中AI相关披露的需求增加。现有方法（如关键词扩展或文档级分类）在粒度、可解释性和鲁棒性方面存在局限性。

Method: 引入FinAI-BERT，一个基于Transformer的领域适应语言模型，旨在对金融文本中的AI相关内容进行句子级分类。该模型在包含1,586个句子的手动标注数据集上进行微调，这些句子来源于669份美国银行的年度报告（2015-2023年）。

Result: FinAI-BERT实现了近乎完美的分类性能（准确率为99.37%，F1分数为0.993），显著优于传统基线模型。通过SHAP方法确保了模型的可解释性，并通过偏差分析和鲁棒性检查确认了模型在不同句子长度、对抗性输入和时间样本下的稳定性。

Conclusion: 本研究在理论上通过实现基于Transformer的细粒度、主题特定分类，推动了金融自然语言处理（NLP）领域的发展。在实践中，它为分析师、监管机构和学者提供了一个可扩展、透明的解决方案，以监测AI在金融机构中的传播和表述方式。

Abstract: The proliferation of artificial intelligence (AI) in financial services has
prompted growing demand for tools that can systematically detect AI-related
disclosures in corporate filings. While prior approaches often rely on keyword
expansion or document-level classification, they fall short in granularity,
interpretability, and robustness. This study introduces FinAI-BERT, a
domain-adapted transformer-based language model designed to classify AI-related
content at the sentence level within financial texts. The model was fine-tuned
on a manually curated and balanced dataset of 1,586 sentences drawn from 669
annual reports of U.S. banks (2015 to 2023). FinAI-BERT achieved near-perfect
classification performance (accuracy of 99.37 percent, F1 score of 0.993),
outperforming traditional baselines such as Logistic Regression, Naive Bayes,
Random Forest, and XGBoost. Interpretability was ensured through SHAP-based
token attribution, while bias analysis and robustness checks confirmed the
model's stability across sentence lengths, adversarial inputs, and temporal
samples. Theoretically, the study advances financial NLP by operationalizing
fine-grained, theme-specific classification using transformer architectures.
Practically, it offers a scalable, transparent solution for analysts,
regulators, and scholars seeking to monitor the diffusion and framing of AI
across financial institutions.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [229] [Analyzing and Improving Speaker Similarity Assessment for Speech Synthesis](https://arxiv.org/abs/2507.02176)
*Marc-André Carbonneau,Benjamin van Niekerk,Hugo Seuté,Jean-Philippe Letendre,Herman Kamper,Julian Zaïdi*

Main category: cs.SD

TL;DR: 本文分析了现有自动说话人验证（ASV）嵌入在捕捉语音身份方面的不足，指出其侧重静态特征而忽略动态韵律。为此，我们提出了U3D度量，以评估说话人的动态韵律模式，旨在提高语音克隆系统的身份一致性评估。


<details>
  <summary>Details</summary>
Motivation: 语音身份建模具有多面性，挑战重重。在生成式语音系统中，常用ASV嵌入评估身份，但这些嵌入主要为区分而非全面刻画身份而设计，可能无法充分捕捉语音的复杂特征。因此，有必要探究ASV嵌入所捕捉的语音特征，并弥补其在动态特征评估方面的不足。

Method: 本研究调查了ASV嵌入所捕捉的语音特征；识别了影响说话人相似度测量的混杂因素并提出了缓解策略；为解决现有ASV嵌入对动态元素的忽视，提出了U3D度量，用于评估说话人的动态韵律模式。

Result: 研究发现，广泛使用的ASV嵌入主要关注音色和音高范围等静态特征，而忽略了韵律等动态元素。同时，识别出损害说话人相似度测量的混杂因素。

Conclusion: 本工作通过深入分析ASV嵌入的局限性并提出U3D这一新度量，有助于应对评估说话人身份一致性的持续挑战，尤其对于日益完善的语音克隆系统具有重要意义。

Abstract: Modeling voice identity is challenging due to its multifaceted nature. In
generative speech systems, identity is often assessed using automatic speaker
verification (ASV) embeddings, designed for discrimination rather than
characterizing identity. This paper investigates which aspects of a voice are
captured in such representations. We find that widely used ASV embeddings focus
mainly on static features like timbre and pitch range, while neglecting dynamic
elements such as rhythm. We also identify confounding factors that compromise
speaker similarity measurements and suggest mitigation strategies. To address
these gaps, we propose U3D, a metric that evaluates speakers' dynamic rhythm
patterns. This work contributes to the ongoing challenge of assessing speaker
identity consistency in the context of ever-better voice cloning systems. We
publicly release our code.

</details>


### [230] [JoyTTS: LLM-based Spoken Chatbot With Voice Cloning](https://arxiv.org/abs/2507.02380)
*Fangru Zhou,Jun Zhao,Guoxin Wang*

Main category: cs.SD

TL;DR: JoyTTS是一个结合大型语言模型（LLM）和文本到语音（TTS）技术的端到端语音聊天机器人，具备声音克隆能力。


<details>
  <summary>Details</summary>
Motivation: 开发一个集成LLM和TTS技术、并支持声音克隆的语音聊天机器人，以提供更自然的交互体验。

Method: 项目基于开源模型MiniCPM-o和CosyVoice2构建，并使用2000小时的对话数据进行训练。同时提供了完整的训练代码和脚本。

Result: 在测试机器seed-tts-zh上，JoyTTS的说话人相似度（SS）达到0.73，词错误率（WER）为5.09。

Conclusion: JoyTTS成功地实现了一个结合LLM和TTS的语音聊天机器人，并支持声音克隆。项目代码和模型已开源，以促进社区的进一步开发和优化。

Abstract: JoyTTS is an end-to-end spoken chatbot that combines large language models
(LLM) with text-to-speech (TTS) technology, featuring voice cloning
capabilities. This project is built upon the open-source MiniCPM-o and
CosyVoice2 models and trained on 2000 hours of conversational data. We have
also provided the complete training code to facilitate further development and
optimization by the community. On the testing machine seed-tts-zh, it achieves
a SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09.
The code and models, along with training and inference scripts, are available
at https://github.com/jdh-algo/JoyTTS.git.

</details>


### [231] [ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning](https://arxiv.org/abs/2507.02666)
*Junyu Wang,Tianrui Wang,Meng Ge,Longbiao Wang,Jianwu Dang*

Main category: cs.SD

TL;DR: 针对音频自监督学习中Transformer注意力机制分配无效信息的问题，本文提出一种差分注意力机制（ASDA），通过双softmax操作和差分系数有效提升模型判别能力，并在多项音频任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 在音频自监督表示学习中，标准Transformer架构的注意力机制常将部分注意力权重分配给不相关信息，可能损害模型的判别能力。

Method: 引入一种差分注意力机制，通过集成双softmax操作和适当调整的差分系数，有效缓解无效注意力分配问题。

Result: ASDA模型在多个基准测试中取得了最先进（SOTA）的性能：音频分类（AS-2M上49.0% mAP，AS20K上41.5% mAP）、关键词识别（SPC-2上98.3%准确率）和环境音分类（ESC-50上96.1%准确率）。

Conclusion: ASDA模型在音频任务中表现出显著的有效性，为更广泛的应用奠定了基础。

Abstract: In recent advancements in audio self-supervised representation learning, the
standard Transformer architecture has emerged as the predominant approach, yet
its attention mechanism often allocates a portion of attention weights to
irrelevant information, potentially impairing the model's discriminative
ability. To address this, we introduce a differential attention mechanism,
which effectively mitigates ineffective attention allocation through the
integration of dual-softmax operations and appropriately tuned differential
coefficients. Experimental results demonstrate that our ASDA model achieves
state-of-the-art (SOTA) performance across multiple benchmarks, including audio
classification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting
(98.3% accuracy on SPC-2), and environmental sound classification (96.1%
accuracy on ESC-50). These results highlight ASDA's effectiveness in audio
tasks, paving the way for broader applications.

</details>


### [232] [Acoustic evaluation of a neural network dedicated to the detection of animal vocalisations](https://arxiv.org/abs/2507.01974)
*Jérémy Rouch,M Ducrettet,S Haupert,R Emonet,F Sèbe*

Main category: cs.SD

TL;DR: 本文提出一种基于合成信号信噪比与检测概率关系的新方法，用于声学分析生态声学中自动信号检测系统的性能，并展示其在系统优化、检测距离建模及空间密度估算方面的应用。


<details>
  <summary>Details</summary>
Motivation: 在生态声学中，长期录音设备广泛用于动物种群监测，但自动信号检测方法（尤其是基于神经网络的方法）的有效性通常仅通过机器学习指标评估，而声学性能分析却很罕见。因此，研究需要一种更侧重声学性能的评估方法。

Method: 提出一种将合成信号的信噪比与其检测概率相关联的简单声学分析方法，以评估检测系统的性能。

Result: 该方法能够提供系统信息，优化系统训练。它还能对检测距离进行建模，从而评估其随声音环境变化的动态，并估算叫声的空间密度。

Conclusion: 所提出的声学分析方法为评估和优化生态声学检测系统提供了一种有效手段，并能进一步估算检测距离和叫声的空间密度，超越了传统的机器学习指标评估方式。

Abstract: The accessibility of long-duration recorders, adapted to sometimes demanding
field conditions, has enabled the deployment of extensive animal population
monitoring campaigns through ecoacoustics. The effectiveness of automatic
signal detection methods, increasingly based on neural approaches, is
frequently evaluated solely through machine learning metrics, while acoustic
analysis of performance remains rare. As part of the acoustic monitoring of
Rock Ptarmigan populations, we propose here a simple method for acoustic
analysis of the detection system's performance. The proposed measure is based
on relating the signal-to-noise ratio of synthetic signals to their probability
of detection. We show how this measure provides information about the system
and allows optimisation of its training. We also show how it enables modelling
of the detection distance, thus offering the possibility of evaluating its
dynamics according to the sound environment and accessing an estimation of the
spatial density of calls.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [233] [Discovery of Fatigue Strength Models via Feature Engineering and automated eXplainable Machine Learning applied to the welded Transverse Stiffener](https://arxiv.org/abs/2507.02005)
*Michael A. Kraus,Helen Bartsch*

Main category: cs.CE

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This research introduces a unified approach combining Automated Machine
Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict
fatigue strength in welded transverse stiffener details. It integrates
expert-driven feature engineering with algorithmic feature creation to enhance
accuracy and explainability.
  Based on the extensive fatigue test database regression models - gradient
boosting, random forests, and neural networks - were trained using AutoML under
three feature schemes: domain-informed, algorithmic, and combined. This allowed
a systematic comparison of expert-based versus automated feature selection.
  Ensemble methods (e.g. CatBoost, LightGBM) delivered top performance. The
domain-informed model $\mathcal M_2$ achieved the best balance: test RMSE
$\approx$ 30.6 MPa and $R^2 \approx 0.780% over the full $\Delta
\sigma_{c,50\%}$ range, and RMSE $\approx$ 13.4 MPa and $R^2 \approx 0.527%
within the engineering-relevant 0 - 150 MPa domain. The denser-feature model
($\mathcal M_3$) showed minor gains during training but poorer generalization,
while the simpler base-feature model ($\mathcal M_1$) performed comparably,
confirming the robustness of minimalist designs.
  XAI methods (SHAP and feature importance) identified stress ratio $R$, stress
range $\Delta \sigma_i$, yield strength $R_{eH}$, and post-weld treatment (TIG
dressing vs. as-welded) as dominant predictors. Secondary geometric factors -
plate width, throat thickness, stiffener height - also significantly affected
fatigue life.
  This framework demonstrates that integrating AutoML with XAI yields accurate,
interpretable, and robust fatigue strength models for welded steel structures.
It bridges data-driven modeling with engineering validation, enabling
AI-assisted design and assessment. Future work will explore probabilistic
fatigue life modeling and integration into digital twin environments.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [234] [Machine Learning Based Stress Testing Framework for Indian Financial Market Portfolios](https://arxiv.org/abs/2507.02011)
*Vidya Sagar G,Shifat Ali,Siddhartha P. Chakrabarty*

Main category: q-fin.RM

TL;DR: 本文提出一个机器学习驱动的框架，用于对印度金融市场的特定行业进行压力测试，通过引入变分自编码器和蒙特卡洛模拟，提升了传统压力测试的灵活性和鲁实性。


<details>
  <summary>Details</summary>
Motivation: 解决传统压力测试在捕捉复杂非线性依赖和生成更细致、分布感知型压力情景方面的局限性。

Method: 初期使用主成分分析和自编码器进行降维和潜在因子建模。在此基础上，扩展至变分自编码器，引入潜在空间的概率结构，并通过蒙特卡洛模拟生成情景。采用VaR和预期亏空进行风险估算。

Result: 所提出的框架能够捕捉复杂的非线性依赖关系，支持通过VaR和预期亏空进行风险估算，并证明了机器学习方法在提高金融压力测试灵活性、鲁棒性和现实性方面的潜力。

Conclusion: 机器学习方法能够显著改善金融压力测试的灵活性、鲁棒性和现实性，为金融风险管理提供更先进的工具。

Abstract: This paper presents a machine learning driven framework for sectoral stress
testing in the Indian financial market, focusing on financial services,
information technology, energy, consumer goods, and pharmaceuticals. Initially,
we address the limitations observed in conventional stress testing through
dimensionality reduction and latent factor modeling via Principal Component
Analysis and Autoencoders. Building on this, we extend the methodology using
Variational Autoencoders, which introduces a probabilistic structure to the
latent space. This enables Monte Carlo-based scenario generation, allowing for
more nuanced, distribution-aware simulation of stressed market conditions. The
proposed framework captures complex non-linear dependencies and supports risk
estimation through Value-at-Risk and Expected Shortfall. Together, these
pipelines demonstrate the potential of Machine Learning approaches to improve
the flexibility, robustness, and realism of financial stress testing.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [235] [Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System](https://arxiv.org/abs/2507.02000)
*Yongsen Zheng,Zongxuan Xie,Guohua Wang,Ziyao Liu,Liang Lin,Kwok-Yan Lam*

Main category: cs.IR

TL;DR: 本文提出HyFairCRS框架，通过超图对比学习捕捉用户多样化兴趣，解决对话推荐系统（CRS）中动态和交互式场景下的不公平性问题，实验证明其在性能和公平性上均达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中的不公平性是一个普遍挑战，导致基于属性（如性别、种族、年龄、流行度）的偏见结果。尽管现有方法在离线或静态环境中有所改善，但在动态交互式系统中，不公平性会随时间加剧，引发马太效应、过滤气泡和回声室等严重问题，急需动态公平性解决方案。

Method: 提出了Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System (HyFairCRS)。该框架首先通过对比学习建立多样化的超图，以捕获用户广泛的多样化兴趣。随后，在对话过程中利用这些兴趣来生成信息性响应，并在动态用户-系统反馈循环中确保公平的物品预测。

Result: 在两个基于对话推荐系统的数据集上进行的实验结果表明，HyFairCRS不仅实现了新的最先进性能（State-of-the-Art），而且有效地缓解了系统中的不公平性。

Conclusion: HyFairCRS成功地解决了对话推荐系统中动态和交互场景下的不公平性挑战，通过促进多兴趣多样性公平，它能够在提升推荐系统整体性能的同时，有效确保推荐结果的公平性。

Abstract: Unfairness is a well-known challenge in Recommender Systems (RSs), often
resulting in biased outcomes that disadvantage users or items based on
attributes such as gender, race, age, or popularity. Although some approaches
have started to improve fairness recommendation in offline or static contexts,
the issue of unfairness often exacerbates over time, leading to significant
problems like the Matthew effect, filter bubbles, and echo chambers. To address
these challenges, we proposed a novel framework, Hypergraph Contrastive
Multi-Interest Learning for Fair Conversational Recommender System (HyFairCRS),
aiming to promote multi-interest diversity fairness in dynamic and interactive
Conversational Recommender Systems (CRSs). HyFairCRS first captures a wide
range of user interests by establishing diverse hypergraphs through contrastive
learning. These interests are then utilized in conversations to generate
informative responses and ensure fair item predictions within the dynamic
user-system feedback loop. Experiments on two CRS-based datasets show that
HyFairCRS achieves a new state-of-the-art performance while effectively
alleviating unfairness. Our code is available at
https://github.com/zysensmile/HyFairCRS.

</details>


### [236] [ManifoldMind: Dynamic Hyperbolic Reasoning for Trustworthy Recommendations](https://arxiv.org/abs/2507.02014)
*Anoushka Harit,Zhongtian Sun,Suncica Hadzidedic*

Main category: cs.IR

TL;DR: ManifoldMind是一个基于超曲面几何的概率推荐系统，通过自适应曲率概率球体和曲率感知语义核，实现个性化不确定性建模和多跳推理，在多项基准测试中表现优异，并能提供可解释的推理轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有推荐方法在超曲面空间中存在固定曲率和刚性嵌入的局限性，难以实现个性化的不确定性建模和灵活的语义探索。同时，需要避免模型对浅层或直接交互的过拟合，并能在稀疏或抽象领域提供透明、可信赖且探索驱动的推荐。

Method: 引入ManifoldMind系统，这是一个用于超曲面空间中语义层次结构上探索性推理的概率几何推荐系统。它将用户、物品和标签表示为自适应曲率概率球体，从而实现个性化的不确定性建模和几何感知的语义探索。该系统还采用曲率感知语义核，支持软性、多跳推理，允许模型探索多样化的概念路径，而非过度拟合浅层或直接交互。

Result: 在四个公开基准测试中，ManifoldMind与强基线模型相比，在NDCG（归一化折损累计增益）、校准度和多样性方面均表现出优越性能。

Conclusion: ManifoldMind能够生成明确的推理轨迹，这使得它在稀疏或抽象领域能够提供透明、可信赖和探索驱动的推荐，克服了传统推荐系统的一些限制，并提供了更深层次的解释性。

Abstract: We introduce ManifoldMind, a probabilistic geometric recommender system for
exploratory reasoning over semantic hierarchies in hyperbolic space. Unlike
prior methods with fixed curvature and rigid embeddings, ManifoldMind
represents users, items, and tags as adaptive-curvature probabilistic spheres,
enabling personalised uncertainty modeling and geometry-aware semantic
exploration. A curvature-aware semantic kernel supports soft, multi-hop
inference, allowing the model to explore diverse conceptual paths instead of
overfitting to shallow or direct interactions. Experiments on four public
benchmarks show superior NDCG, calibration, and diversity compared to strong
baselines. ManifoldMind produces explicit reasoning traces, enabling
transparent, trustworthy, and exploration-driven recommendations in sparse or
abstract domains.

</details>


### [237] [When LLMs Disagree: Diagnosing Relevance Filtering Bias and Retrieval Divergence in SDG Search](https://arxiv.org/abs/2507.02139)
*William A. Ingram,Bipasha Banerjee,Edward A. Fox*

Main category: cs.IR

TL;DR: 研究发现大型语言模型在文档相关性标注上的分歧是系统性的，而非随机的，这可能导致信息检索结果的结构性差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在信息检索中越来越多地用于分配文档相关性标签，尤其是在缺乏人工标注数据的领域。然而，不同模型在边缘案例上常出现分歧，引发了对其如何影响下游检索的担忧。

Method: 本研究调查了两个开源LLM（LLaMA和Qwen）在可持续发展目标（SDG）相关学术摘要语料库上的标签分歧。研究隔离了分歧子集，并分析了它们的词汇属性、排序行为和分类可预测性。

Result: 研究结果表明，模型分歧是系统性而非随机的：分歧案例表现出一致的词汇模式，在共享评分函数下产生不同的排名靠前输出，并且可以使用简单分类器以高于0.74的AUC值进行区分。

Conclusion: 这些发现表明，基于LLM的过滤会在文档检索中引入结构化的变异性，即使在受控提示和共享排名逻辑下也是如此。研究建议将分类分歧作为检索评估中的分析对象，特别是在政策相关或主题搜索任务中。

Abstract: Large language models (LLMs) are increasingly used to assign document
relevance labels in information retrieval pipelines, especially in domains
lacking human-labeled data. However, different models often disagree on
borderline cases, raising concerns about how such disagreement affects
downstream retrieval. This study examines labeling disagreement between two
open-weight LLMs, LLaMA and Qwen, on a corpus of scholarly abstracts related to
Sustainable Development Goals (SDGs) 1, 3, and 7. We isolate disagreement
subsets and examine their lexical properties, rank-order behavior, and
classification predictability. Our results show that model disagreement is
systematic, not random: disagreement cases exhibit consistent lexical patterns,
produce divergent top-ranked outputs under shared scoring functions, and are
distinguishable with AUCs above 0.74 using simple classifiers. These findings
suggest that LLM-based filtering introduces structured variability in document
retrieval, even under controlled prompting and shared ranking logic. We propose
using classification disagreement as an object of analysis in retrieval
evaluation, particularly in policy-relevant or thematic search tasks.

</details>


### [238] [Content filtering methods for music recommendation: A review](https://arxiv.org/abs/2507.02282)
*Terence Zeng,Abhishek K. Umrawal*

Main category: cs.IR

TL;DR: 本文综述了音乐推荐系统中因数据稀疏导致的协同过滤局限性，重点探讨了内容过滤（包括LLMs歌词分析和音频处理）在缓解这些问题中的作用，并讨论了不同分析方法的冲突及解决方案。


<details>
  <summary>Details</summary>
Motivation: 音乐推荐系统中的协同过滤方法因数据交互稀疏而效果不佳，尤其在音乐领域，用户仅收听少量歌曲，导致推荐存在偏差和挑战，亟需新的方法来解决。

Method: 本文采用综述方法，审查了当前研究现状，强调内容过滤在缓解协同过滤偏差中的作用。具体探讨了基于大型语言模型（LLMs）的歌词分析和音频信号处理技术等多种歌曲分类方法，并讨论了不同分析方法间的冲突及解决途径。

Result: 综述指出内容过滤是解决音乐推荐系统稀疏性和偏差的关键方法。本文详细探讨了多种内容过滤技术，如LLMs在歌词分析中的应用和音频信号处理技术，并识别了这些分析方法之间可能存在的冲突。

Conclusion: 内容过滤，特别是结合歌词分析和音频处理等多模态方法，是克服音乐推荐系统数据稀疏性及固有偏差的有效途径。解决不同内容分析方法间的潜在冲突是未来研究的重要方向，有助于进一步优化推荐系统。

Abstract: Recommendation systems have become essential in modern music streaming
platforms, shaping how users discover and engage with songs. One common
approach in recommendation systems is collaborative filtering, which suggests
content based on the preferences of users with similar listening patterns to
the target user. However, this method is less effective on media where
interactions are sparse. Music is one such medium, since the average user of a
music streaming service will never listen to the vast majority of tracks. Due
to this sparsity, there are several challenges that have to be addressed with
other methods. This review examines the current state of research in addressing
these challenges, with an emphasis on the role of content filtering in
mitigating biases inherent in collaborative filtering approaches. We explore
various methods of song classification for content filtering, including lyrical
analysis using Large Language Models (LLMs) and audio signal processing
techniques. Additionally, we discuss the potential conflicts between these
different analysis methods and propose avenues for resolving such
discrepancies.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [239] [Generating Large Semi-Synthetic Graphs of Any Size](https://arxiv.org/abs/2507.02166)
*Rodrigo Tuna,Carlos Soares*

Main category: cs.SI

TL;DR: 提出LGSG框架，利用扩散模型和节点嵌入生成不同大小的图，解决了现有模型依赖节点ID的局限性，并在性能和可扩展性上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习图生成模型过度依赖节点ID，导致无法生成大于训练集的图，且忽略节点属性，限制了其灵活性和可扩展性。

Method: 提出Latent Graph Sampling Generation (LGSG) 框架，该框架结合了扩散模型和节点嵌入技术，旨在无需重新训练的情况下生成各种尺寸的图。LGSG通过捕获节点嵌入和子图结构的分布来消除对节点ID的依赖。

Result: LGSG在标准图生成指标上与基线模型持平，但在节点聚类倾向等常被忽略的指标上表现更优。此外，它能在不同大小的图之间保持结构特性的一致性。

Conclusion: LGSG框架提供了一种鲁棒且可扩展的图生成解决方案，成功克服了传统模型对节点ID的依赖及其尺寸限制，能够灵活生成具有一致结构特性的图。

Abstract: Graph generation is an important area in network science. Traditional
approaches focus on replicating specific properties of real-world graphs, such
as small diameters or power-law degree distributions. Recent advancements in
deep learning, particularly with Graph Neural Networks, have enabled
data-driven methods to learn and generate graphs without relying on predefined
structural properties. Despite these advances, current models are limited by
their reliance on node IDs, which restricts their ability to generate graphs
larger than the input graph and ignores node attributes. To address these
challenges, we propose Latent Graph Sampling Generation (LGSG), a novel
framework that leverages diffusion models and node embeddings to generate
graphs of varying sizes without retraining. The framework eliminates the
dependency on node IDs and captures the distribution of node embeddings and
subgraph structures, enabling scalable and flexible graph generation.
Experimental results show that LGSG performs on par with baseline models for
standard metrics while outperforming them in overlooked ones, such as the
tendency of nodes to form clusters. Additionally, it maintains consistent
structural characteristics across graphs of different sizes, demonstrating
robustness and scalability.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [240] [DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment](https://arxiv.org/abs/2507.02768)
*Ke-Han Lu,Zhehuai Chen,Szu-Wei Fu,Chao-Han Huck Yang,Sung-Feng Huang,Chih-Kai Yang,Chee-En Yu,Chun-Wei Chen,Wei-Chih Chen,Chien-yu Huang,Yi-Cheng Lin,Yu-Xiang Lin,Chi-An Fu,Chun-Yi Kuan,Wenze Ren,Xuanjun Chen,Wei-Ping Huang,En-Pei Hu,Tzu-Quan Lin,Yuan-Kuei Wu,Kuan-Po Huang,Hsiao-Ying Huang,Huang-Cheng Chou,Kai-Wei Chang,Cheng-Han Chiang,Boris Ginsburg,Yu-Chiang Frank Wang,Hung-yi Lee*

Main category: eess.AS

TL;DR: 本文提出了DeSTA2.5-Audio，一个通用的音频大语言模型（LALM），它采用DeSTA自生成跨模态对齐策略构建训练数据，有效解决了现有LALM中大语言模型（LLM）语言能力遗忘的问题，实现了无需任务特定微调的强大音频感知和指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大音频语言模型（LALM）在整合音频能力时，通常依赖手动或LLM合成的音频-指令数据集，但这常常导致基础LLM的原始语言能力发生灾难性遗忘。

Method: 本文提出DeSTA（自生成跨模态对齐策略），即让骨干LLM生成自身的训练目标，以在建立有效的音-文对齐的同时，保留LLM固有的语言能力。基于DeSTA策略，构建了DeSTA-AQA5M，一个包含500万训练样本的大规模、任务无关数据集（源自7000小时、50个多样化音频数据集）。

Result: DeSTA2.5-Audio在Dynamic-SUPERB、MMAU、SAKURA、Speech-IFEval和VoiceBench等一系列音频-语言基准测试中取得了最先进或具竞争力的表现。与广泛采用的数据构建和训练策略相比，DeSTA自生成策略在音频感知和指令遵循能力上均表现更优。

Conclusion: 研究结果强调了在LALM开发中精心设计数据构建的重要性，并为构建鲁棒、通用LALM提供了实用的见解。

Abstract: We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model
(LALM) designed for robust auditory perception and instruction-following,
without requiring task-specific audio instruction-tuning. Recent LALMs
typically augment Large Language Models (LLMs) with auditory capabilities by
training on large-scale, manually curated or LLM-synthesized audio-instruction
datasets. However, these approaches have often suffered from the catastrophic
forgetting of the LLM's original language abilities. To address this, we
revisit the data construction pipeline and propose DeSTA, a self-generated
cross-modal alignment strategy in which the backbone LLM generates its own
training targets. This approach preserves the LLM's native language proficiency
while establishing effective audio-text alignment, thereby enabling zero-shot
generalization without task-specific tuning. Using DeSTA, we construct
DeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training
samples derived from 7,000 hours of audio spanning 50 diverse datasets,
including speech, environmental sounds, and music. DeSTA2.5-Audio achieves
state-of-the-art or competitive performance across a wide range of
audio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,
Speech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate
that our self-generated strategy outperforms widely adopted data construction
and training strategies in both auditory perception and instruction-following
capabilities. Our findings underscore the importance of carefully designed data
construction in LALM development and offer practical insights for building
robust, general-purpose LALMs.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [241] [Synthetic Heuristic Evaluation: A Comparison between AI- and Human-Powered Usability Evaluation](https://arxiv.org/abs/2507.02306)
*Ruican Zhong,David W. McDonald,Gary Hsieh*

Main category: cs.HC

TL;DR: 研究开发了一种基于多模态LLM的合成启发式可用性评估方法，发现其在识别可用性问题上优于人类专家，但仍存在特定局限。


<details>
  <summary>Details</summary>
Motivation: 传统可用性评估成本高昂，需要大量专家时间和用户补偿。

Method: 开发了一种利用多模态大型语言模型（LLMs）分析图像并提供设计反馈的合成启发式评估方法。

Result: 合成评估在两款应用中识别出73%和77%的可用性问题，超过了5位经验丰富的人类评估员（57%和63%）。它在任务中保持了稳定的性能，并擅长检测布局问题。然而，在识别某些UI组件、设计约定以及跨屏幕违规方面存在不足。此外，合成评估的性能在时间和不同账户下保持稳定。

Conclusion: 该工作揭示了人类与LLM驱动评估之间的性能差异，为合成启发式评估的设计提供了重要参考。

Abstract: Usability evaluation is crucial in human-centered design but can be costly,
requiring expert time and user compensation. In this work, we developed a
method for synthetic heuristic evaluation using multimodal LLMs' ability to
analyze images and provide design feedback. Comparing our synthetic evaluations
to those by experienced UX practitioners across two apps, we found our
evaluation identified 73% and 77% of usability issues, which exceeded the
performance of 5 experienced human evaluators (57% and 63%). Compared to human
evaluators, the synthetic evaluation's performance maintained consistent
performance across tasks and excelled in detecting layout issues, highlighting
potential attentional and perceptual strengths of synthetic evaluation.
However, synthetic evaluation struggled with recognizing some UI components and
design conventions, as well as identifying across screen violations.
Additionally, testing synthetic evaluations over time and accounts revealed
stable performance. Overall, our work highlights the performance differences
between human and LLM-driven evaluations, informing the design of synthetic
heuristic evaluations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [242] [Effective Explanations for Belief-Desire-Intention Robots: When and What to Explain](https://arxiv.org/abs/2507.02016)
*Cong Wang,Roberto Calandra,Verena Klös*

Main category: cs.RO

TL;DR: 本文调查了用户对机器人解释的需求，发现用户在意外情况下希望获得简洁且包含意图和上下文信息的解释。基于此，论文提出了两种算法，旨在帮助信念-愿望-意图（BDI）机器人识别意外行为并生成有效的解释。


<details>
  <summary>Details</summary>
Motivation: 当机器人执行复杂任务时，其行为偏离用户预期会引起困惑。解释机器人的推理过程有助于用户理解其意图，但提供解释的时机和内容对避免用户反感至关重要。

Method: 研究人员调查了用户对厨房清洁机器人解释的时机和内容偏好。基于用户偏好，提出了两种算法：一种用于识别机器人的意外行为，另一种用于为信念-愿望-意图（BDI）机器人构建有效的解释。这些算法设计为易于集成到BDI推理过程中。

Result: 研究结果表明，用户在机器人行为出乎意料时需要解释，并且偏好简洁的解释。这些简洁的解释应清楚阐明令人困惑行为背后的意图以及与该决策相关的上下文因素。

Conclusion: 所提出的算法能够识别意外行动并构建有效的解释，从而促进更好的人机交互。它们为实现具有上下文相关和用户特定解释的机器人铺平了道路。

Abstract: When robots perform complex and context-dependent tasks in our daily lives,
deviations from expectations can confuse users. Explanations of the robot's
reasoning process can help users to understand the robot intentions. However,
when to provide explanations and what they contain are important to avoid user
annoyance. We have investigated user preferences for explanation demand and
content for a robot that helps with daily cleaning tasks in a kitchen. Our
results show that users want explanations in surprising situations and prefer
concise explanations that clearly state the intention behind the confusing
action and the contextual factors that were relevant to this decision. Based on
these findings, we propose two algorithms to identify surprising actions and to
construct effective explanations for Belief-Desire-Intention (BDI) robots. Our
algorithms can be easily integrated in the BDI reasoning process and pave the
way for better human-robot interaction with context- and user-specific
explanations.

</details>


### [243] [Towards Bio-Inspired Robotic Trajectory Planning via Self-Supervised RNN](https://arxiv.org/abs/2507.02171)
*Miroslav Cibula,Kristína Malinovská,Matthias Kerzel*

Main category: cs.RO

TL;DR: 本文提出一种基于循环架构的认知启发式自监督学习方案，用于机器人轨迹规划，克服了监督式模仿学习的局限性，并在运动学规划任务中验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 传统机器人轨迹规划方法（如采样式规划器）计算密集；现有监督式序列学习方法仅进行模仿学习，不能根据任务成功与否进行学习。

Method: 提出一种基于循环架构的认知启发式自监督学习方案来构建轨迹模型。该模型仅利用给定的正向和逆向运动学模型对来学习生成轨迹。

Result: 该模型能够仅利用给定的正向和逆向运动学模型对来学习生成轨迹。在机器人手臂运动学规划任务中验证了该方法的可行性。

Conclusion: 所提出的新方法能够有效地学习生成轨迹，并有望促进需要自适应解决方案的复杂操作任务的规划。

Abstract: Trajectory planning in robotics is understood as generating a sequence of
joint configurations that will lead a robotic agent, or its manipulator, from
an initial state to the desired final state, thus completing a manipulation
task while considering constraints like robot kinematics and the environment.
Typically, this is achieved via sampling-based planners, which are
computationally intensive. Recent advances demonstrate that trajectory planning
can also be performed by supervised sequence learning of trajectories, often
requiring only a single or fixed number of passes through a neural
architecture, thus ensuring a bounded computation time. Such fully supervised
approaches, however, perform imitation learning; they do not learn based on
whether the trajectories can successfully reach a goal, but try to reproduce
observed trajectories. In our work, we build on this approach and propose a
cognitively inspired self-supervised learning scheme based on a recurrent
architecture for building a trajectory model. We evaluate the feasibility of
the proposed method on a task of kinematic planning for a robotic arm. The
results suggest that the model is able to learn to generate trajectories only
using given paired forward and inverse kinematics models, and indicate that
this novel method could facilitate planning for more complex manipulation tasks
requiring adaptive solutions.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [244] [SAKURAONE: Empowering Transparent and Open AI Platforms through Private-Sector HPC Investment in Japan](https://arxiv.org/abs/2507.02124)
*Fumikazu Konishi*

Main category: cs.DC

TL;DR: SAKURAONE是SAKURA互联网研究中心开发并运营的HPC集群，专为大型语言模型训练等高级工作负载优化。该系统在ISC 2025 TOP500榜单中位列第49位，是前100名中唯一采用800GbE和SONiC开放网络堆栈的系统，HPL峰值性能达33.95 PFLOP/s，HPL-MxP (FP8) 达339.86 PFLOP/s。


<details>
  <summary>Details</summary>
Motivation: 构建一个高性能计算集群，以支持包括大型语言模型（LLM）训练在内的先进工作负载；并验证并展示开放且供应商中立的技术（如800GbE以太网和SONiC操作系统）在大规模HPC基础设施中的实用性和性能。

Method: 构建了名为SAKURAONE的托管式HPC集群，采用“KOKARYOKU PHY”裸金属GPU服务器配置。该系统包含100个计算节点，每个节点配备8个NVIDIA H100 GPU。存储方面，配备2PB全闪存Lustre存储子系统。节点间通信基于Rail-Optimized拓扑，采用全双向带宽互连，通过800GbE链接连接叶脊层。利用RoCEv2技术实现高速、无损数据传输。核心网络堆栈基于开放的800GbE和SONiC操作系统。

Result: 在ISC 2025 TOP500榜单中，HPL得分全球排名第49位。该系统是TOP100中唯一采用完全开放网络堆栈（基于800GbE和SONiC）的系统。在HPL基准测试中，持续性能（Rmax）达到33.95 PFLOP/s；在HPCG基准测试中，性能达到396.295 TFLOP/s。针对AI应用低精度工作负载的HPL-MxP基准测试中，使用FP8精度达到339.86 PFLOP/s。研究结果证明了开放和供应商中立技术在大规模HPC基础设施中的可行性。

Conclusion: SAKURAONE集群不仅在全球HPC排名中取得显著成就，更重要的是，它成功地证明了基于800GbE和SONiC等开放且供应商中立技术构建大规模高性能计算基础设施的卓越性能和商业可行性，为HPC的未来发展提供了重要方向和参考。

Abstract: SAKURAONE is a managed high performance computing (HPC) cluster developed and
operated by the SAKURA Internet Research Center. It reinforces the ``KOKARYOKU
PHY'' configuration of bare-metal GPU servers and is designed as a cluster
computing resource optimized for advanced workloads, including large language
model (LLM) training.
  In the ISC 2025 edition of the TOP500 list, SAKURAONE was ranked
\textbf{49th} in the world based on its High Performance Linpack (HPL) score,
demonstrating its global competitiveness. In particular, it is the \textbf{only
system within the top 100} that employs a fully open networking stack based on
\textbf{800~GbE (Gigabit Ethernet)} and the \textbf{SONiC (Software for Open
Networking in the Cloud)} operating system, highlighting the viability of open
and vendor-neutral technologies in large-scale HPC infrastructure.
  SAKURAONE achieved a sustained performance of 33.95~PFLOP/s on the HPL
benchmark (Rmax), and 396.295~TFLOP/s on the High Performance Conjugate
Gradient (HPCG) benchmark. For the HPL-MxP benchmark, which targets
low-precision workloads representative of AI applications, SAKURAONE delivered
an impressive 339.86~PFLOP/s using FP8 precision.
  The system comprises 100 compute nodes, each equipped with eight NVIDIA H100
GPUs. It is supported by an all-flash Lustre storage subsystem with a total
physical capacity of 2~petabytes, providing high-throughput and low-latency
data access. Internode communication is enabled by a full-bisection bandwidth
interconnect based on a Rail-Optimized topology, where the Leaf and Spine
layers are interconnected via 800~GbE links. This topology, in combination with
RoCEv2 (RDMA over Converged Ethernet version 2), enables high-speed, lossless
data transfers and mitigates communication bottlenecks in large-scale parallel
workloads.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [245] [Tracing the Interactions of Modular CMA-ES Configurations Across Problem Landscapes](https://arxiv.org/abs/2507.02331)
*Ana Nikolikj,Mario Andrés Muñoz,Eva Tuba,Tome Eftimov*

Main category: cs.NE

TL;DR: 该研究利用算法足迹分析CMA-ES算法配置与问题特性之间的相互作用，以理解其性能差异并指导配置选择。


<details>
  <summary>Details</summary>
Motivation: 旨在探究同一算法不同配置性能各异的原因，并识别影响这些表现的问题特征。

Method: 计算六种模块化CMA-ES变体（modCMA）在24个BBOB基准问题（5维和30维设置）上的性能足迹。

Result: 分析揭示了配置间因与问题属性的共同交互而产生的共享行为模式，以及在相同问题上因问题特征差异而产生的独特行为。

Conclusion: 算法足迹在增强可解释性和指导算法配置选择方面被证明是有效的。

Abstract: This paper leverages the recently introduced concept of algorithm footprints
to investigate the interplay between algorithm configurations and problem
characteristics. Performance footprints are calculated for six modular variants
of the CMA-ES algorithm (modCMA), evaluated on 24 benchmark problems from the
BBOB suite, across two-dimensional settings: 5-dimensional and 30-dimensional.
These footprints provide insights into why different configurations of the same
algorithm exhibit varying performance and identify the problem features
influencing these outcomes. Our analysis uncovers shared behavioral patterns
across configurations due to common interactions with problem properties, as
well as distinct behaviors on the same problem driven by differing problem
features. The results demonstrate the effectiveness of algorithm footprints in
enhancing interpretability and guiding configuration choices.

</details>


### [246] [ClustOpt: A Clustering-based Approach for Representing and Visualizing the Search Dynamics of Numerical Metaheuristic Optimization Algorithms](https://arxiv.org/abs/2507.02337)
*Gjorgjina Cenikj,Gašper Petelin,Tome Eftimov*

Main category: cs.NE

TL;DR: 本文提出一种新颖的表示和可视化方法，通过聚类并追踪解集演变来深入理解数值元启发式优化算法的搜索动态，并引入算法稳定性和相似性指标。


<details>
  <summary>Details</summary>
Motivation: 传统可视化技术难以有效展示高维或复杂解空间中数值元启发式优化算法的搜索结构动态。

Method: 提出一种新的可视化方法，通过聚类算法探索的候选解并跟踪集群成员在迭代过程中的演变。此外，引入“算法稳定性”和“算法相似性”两个量化指标。

Result: 将该方法应用于十种数值元启发式算法，揭示了这些算法的稳定性及其比较行为，提供了深入见解。

Conclusion: 该方法加深了对数值元启发式算法搜索动态的理解。

Abstract: Understanding the behavior of numerical metaheuristic optimization algorithms
is critical for advancing their development and application. Traditional
visualization techniques, such as convergence plots, trajectory mapping, and
fitness landscape analysis, often fall short in illustrating the structural
dynamics of the search process, especially in high-dimensional or complex
solution spaces. To address this, we propose a novel representation and
visualization methodology that clusters solution candidates explored by the
algorithm and tracks the evolution of cluster memberships across iterations,
offering a dynamic and interpretable view of the search process. Additionally,
we introduce two metrics - algorithm stability and algorithm similarity- to
quantify the consistency of search trajectories across runs of an individual
algorithm and the similarity between different algorithms, respectively. We
apply this methodology to a set of ten numerical metaheuristic algorithms,
revealing insights into their stability and comparative behaviors, thereby
providing a deeper understanding of their search dynamics.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [247] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 提出一种基于文本蕴含和上下文学习的方法，将法律文本自动转换为可执行的Python代码表示，以帮助软件系统遵守法律法规，克服了现有方法在数据标注和泛化性上的局限。


<details>
  <summary>Details</summary>
Motivation: 软件系统遵守法律法规是一项资源密集型任务，特别是对于缺乏专业法律知识的小型组织和初创公司。从法律法规中提取元数据以获取软件的法律要求至关重要，但由于法律文本的冗长和复杂性，这项任务非常繁琐。现有自动化方法未能充分考虑元数据属性间的相互作用和关联，且依赖手动标注或启发式机器学习，导致泛化性差。

Method: 本文引入了一种基于文本蕴含（textual entailment）和上下文学习（in-context learning）的方法，用于自动生成法律文本的规范表示，该表示可编码并作为Python代码执行。该表示通过一个手动设计的Python类结构实例化，该结构作为领域特定的元模型，捕获结构化和语义化的法律元数据及其相互关系。这种设计选择旨在减少对大型手动标注数据集的需求，并增强对未见法规的适用性。

Result: 该方法在13项美国州数据泄露通知法上进行了评估。结果表明，生成的表示通过了大约89.4%的测试用例，并达到了82.2%的精确率（precision）和88.7%的召回率（recall）。

Conclusion: 该方法成功地自动化了法律文本的规范表示生成，有效解决了法律合规性中元数据提取的挑战，克服了现有方法在处理元数据关联性和泛化性方面的局限性，并为软件系统的法律合规性提供了一种高效且可扩展的解决方案。

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [248] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 本研究探讨使用GPT-4o在需求启发过程中实时生成后续访谈问题，发现其生成的问题在清晰度、相关性和信息量上不逊于人工问题，且在错误类型指导下表现更优。


<details>
  <summary>Details</summary>
Motivation: 有效的需求访谈对访谈者技能要求高，常面临领域不熟悉、认知负荷过重和信息过载等挑战。鉴于大型语言模型在自然语言处理任务上的卓越表现，研究其如何辅助访谈者。

Method: 利用GPT-4o，基于常见访谈错误类型框架，并结合受访者言语生成后续访谈问题。通过两项受控实验进行评估：一是比较LLM生成与人工问题在无指导下的表现；二是评估在访谈错误类型指导下LLM生成问题的表现。

Result: 实验结果显示，无论是否有指导，LLM生成的问题在清晰度、相关性和信息量方面均不劣于人工问题。当通过常见错误类型进行指导时，LLM生成的问题甚至优于人工问题。

Conclusion: 研究表明，大型语言模型有潜力实时帮助访谈者提高需求启发访谈的质量和便利性。

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


### [249] [VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software](https://arxiv.org/abs/2507.02376)
*Chung-ju Huang,Ziqi Zhang,Yinggui Wang,Binghui Wang,Tao Wei,Leye Wang*

Main category: cs.SE

TL;DR: 本文提出VeFIA框架，利用TEE和协调器在不泄露隐私、不增加延迟的情况下，审计垂直联邦学习（VFL）中数据方推断软件的执行正确性。


<details>
  <summary>Details</summary>
Motivation: 现有VFL工作缺乏一种机制来审计数据方推断软件的执行正确性，尤其是在大规模推断场景下，需要确保计算结果的准确性，同时保护数据隐私。

Method: 设计了垂直联邦推断审计（VeFIA）框架。其核心是任务方利用来自可信执行环境（TEE）框架和协调器的推断结果，来验证数据方计算结果的正确性，采用随机抽样验证。

Result: VeFIA保证，当异常推断超过5.4%时，任务方能以99.99%的概率检测到推断软件执行异常，且不增加任何在线推断延迟。在检测异常推断方面，VeFIA的随机抽样验证实现了100%的阳性预测值、阴性预测值和真阳性率。

Conclusion: VeFIA首次解决了VFL中推断软件执行正确性的审计问题，提供了一个高效、隐私保护且无延迟的解决方案，显著提升了VFL系统的可靠性与可信度。

Abstract: Vertical Federated Learning (VFL) is a distributed AI software deployment
mechanism for cross-silo collaboration without accessing participants' data.
However, existing VFL work lacks a mechanism to audit the execution correctness
of the inference software of the data party. To address this problem, we design
a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task
party to audit whether the data party's inference software is executed as
expected during large-scale inference without leaking the data privacy of the
data party or introducing additional latency to the inference system. The core
of VeFIA is that the task party can use the inference results from a framework
with Trusted Execution Environments (TEE) and the coordinator to validate the
correctness of the data party's computation results. VeFIA guarantees that, as
long as the abnormal inference exceeds 5.4%, the task party can detect
execution anomalies in the inference software with a probability of 99.99%,
without incurring any additional online inference latency. VeFIA's random
sampling validation achieves 100% positive predictive value, negative
predictive value, and true positive rate in detecting abnormal inference. To
the best of our knowledge, this is the first paper to discuss the correctness
of inference software execution in VFL.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [250] [A Scalable and Quantum-Accurate Foundation Model for Biomolecular Force Field via Linearly Tensorized Quadrangle Attention](https://arxiv.org/abs/2507.00884)
*Qun Su,Kai Zhu,Qiaolin Gou,Jintu Zhang,Renling Hu,Yurong Li,Yongze Wang,Hui Zhang,Ziyi You,Linlong Jiang,Yu Kang,Jike Wang,Chang-Yu Hsieh,Tingjun Hou*

Main category: physics.chem-ph

TL;DR: 提出LiTEN，一种新型等变神经网络，通过张量化四边形注意力（TQA）高效建模多体相互作用，实现量子力学级别的精度和高效率，用于生物分子模拟，为药物发现提供强大基础。


<details>
  <summary>Details</summary>
Motivation: 现有的生物分子模拟方法（经典力场、量子力学、现有AI力场）在精度、效率和泛化性上存在局限，难以满足疾病机制理解、药物发现和生物材料设计对原子级精确模拟的需求。

Method: 引入LiTEN，一种新的等变神经网络，其核心是张量化四边形注意力（TQA），通过向量操作高效处理三体和四体相互作用，复杂度呈线性。在此基础上构建LiTEN-FF基础模型，通过在nablaDFT上预训练以实现广阔的化学泛化性，并在SPICE上微调以精确模拟溶剂化系统。

Result: LiTEN在rMD17、MD22和Chignolin的大多数评估子集上取得了最先进（SOTA）的性能，优于MACE、NequIP等领先模型。LiTEN-FF支持迄今为止最全面的生物分子建模下游任务，包括QM级构象搜索、几何优化和自由能表面构建，同时对大型生物分子（约1000个原子）的推理速度比MACE-OFF快10倍。

Conclusion: LiTEN-FF提供了一个物理基础良好且高效的框架，显著推动了复杂的生物分子建模，为药物发现及相关应用提供了多功能的基础。

Abstract: Accurate atomistic biomolecular simulations are vital for disease mechanism
understanding, drug discovery, and biomaterial design, but existing simulation
methods exhibit significant limitations. Classical force fields are efficient
but lack accuracy for transition states and fine conformational details
critical in many chemical and biological processes. Quantum Mechanics (QM)
methods are highly accurate but computationally infeasible for large-scale or
long-time simulations. AI-based force fields (AIFFs) aim to achieve QM-level
accuracy with efficiency but struggle to balance many-body modeling complexity,
accuracy, and speed, often constrained by limited training data and
insufficient validation for generalizability. To overcome these challenges, we
introduce LiTEN, a novel equivariant neural network with Tensorized Quadrangle
Attention (TQA). TQA efficiently models three- and four-body interactions with
linear complexity by reparameterizing high-order tensor features via vector
operations, avoiding costly spherical harmonics. Building on LiTEN, LiTEN-FF is
a robust AIFF foundation model, pre-trained on the extensive nablaDFT dataset
for broad chemical generalization and fine-tuned on SPICE for accurate solvated
system simulations. LiTEN achieves state-of-the-art (SOTA) performance across
most evaluation subsets of rMD17, MD22, and Chignolin, outperforming leading
models such as MACE, NequIP, and EquiFormer. LiTEN-FF enables the most
comprehensive suite of downstream biomolecular modeling tasks to date,
including QM-level conformer searches, geometry optimization, and free energy
surface construction, while offering 10x faster inference than MACE-OFF for
large biomolecules (~1000 atoms). In summary, we present a physically grounded,
highly efficient framework that advances complex biomolecular modeling,
providing a versatile foundation for drug discovery and related applications.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [251] [Seeing Through Green: Text-Based Classification and the Firm's Returns from Green Patents](https://arxiv.org/abs/2507.02287)
*Lapo Santarlasci,Armando Rungi,Antonio Zinilli*

Main category: econ.GN

TL;DR: 本文利用自然语言处理技术识别“真正”的绿色专利，发现其数量远少于传统分类，但拥有此类专利能显著提升企业绩效，强调了精确分类对政策制定的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有绿色专利分类可能过于宽泛，导致难以准确评估其真实影响；本研究旨在通过更精细的识别方法（“真正”的绿色专利）来弥补这一不足，并为政策制定提供更精准的依据。

Method: 1. 利用自然语言处理（NLP）技术，基于1240万已分类绿色专利数据，训练简单神经网络并利用词向量表示扩展基线词典，以识别“真正”的绿色专利。
2. 分析“真正”绿色专利的特征，如技术类别异质性和引用情况。
3. 考察拥有“真正”绿色专利与欧盟企业财务绩效（销售额、市场份额、生产力、利润）之间的关系，并控制反向因果效应。

Result: 1. “真正”的绿色专利仅占现有文献中已分类绿色专利总数的约20%。
2. 这些专利在技术类别上表现出异质性，且被后续发明引用的次数约减少1%。
3. 拥有至少一项“真正”的绿色专利能提高企业的销售额、市场份额和生产力。
4. 高新颖性的“真正”绿色专利还能带来更高的利润。

Conclusion: 文本分析对于获得更细粒度的专利分类至关重要，这类分类对于不同领域的政策制定具有重要价值。

Abstract: This paper introduces Natural Language Processing for identifying ``true''
green patents from official supporting documents. We start our training on
about 12.4 million patents that had been classified as green from previous
literature. Thus, we train a simple neural network to enlarge a baseline
dictionary through vector representations of expressions related to
environmental technologies. After testing, we find that ``true'' green patents
represent about 20\% of the total of patents classified as green from previous
literature. We show heterogeneity by technological classes, and then check that
`true' green patents are about 1\% less cited by following inventions. In the
second part of the paper, we test the relationship between patenting and a
dashboard of firm-level financial accounts in the European Union. After
controlling for reverse causality, we show that holding at least one ``true''
green patent raises sales, market shares, and productivity. If we restrict the
analysis to high-novelty ``true'' green patents, we find that they also yield
higher profits. Our findings underscore the importance of using text analyses
to gauge finer-grained patent classifications that are useful for policymaking
in different domains.

</details>
