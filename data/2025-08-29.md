<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [cs.CV](#cs.CV) [Total: 66]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.LG](#cs.LG) [Total: 63]
- [cs.NI](#cs.NI) [Total: 6]
- [math.HO](#math.HO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.ET](#cs.ET) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.MA](#cs.MA) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.DL](#cs.DL) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Social Bias in Multilingual Language Models: A Survey](https://arxiv.org/abs/2508.20201)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 本系统综述分析了多语言和非英语语境下偏见评估与缓解方法的研究进展，揭示了当前方法论的不足，并为未来研究指明了方向，以提高包容性和跨文化适宜性。


<details>
  <summary>Details</summary>
Motivation: 预训练多语言模型与处理英语文本的模型一样，存在社会偏见。本研究的动机是系统分析新兴研究，将偏见评估和缓解方法扩展到多语言和非英语语境。

Method: 采用系统综述的方法，分析了将偏见评估和缓解方法扩展到多语言及非英语语境的新兴研究。审查内容包括语言多样性、文化意识以及评估指标和缓解技术的选择。

Result: 研究揭示了该领域主流方法论设计中的不足之处（例如，对某些语言的偏好、多语言缓解实验的稀缺性），并总结了在不同语言和文化中调整偏见基准时遇到的常见问题和已实施的解决方案。

Conclusion: 基于研究发现的启示，为未来的研究指明了方向，旨在增强多语言偏见文献的包容性、跨文化适宜性，并使其与最先进的自然语言处理（NLP）进展保持一致。

Abstract: Pretrained multilingual models exhibit the same social bias as models
processing English texts. This systematic review analyzes emerging research
that extends bias evaluation and mitigation approaches into multilingual and
non-English contexts. We examine these studies with respect to linguistic
diversity, cultural awareness, and their choice of evaluation metrics and
mitigation techniques. Our survey illuminates gaps in the field's dominant
methodological design choices (e.g., preference for certain languages, scarcity
of multilingual mitigation experiments) while cataloging common issues
encountered and solutions implemented in adapting bias benchmarks across
languages and cultures. Drawing from the implications of our findings, we chart
directions for future research that can reinforce the multilingual bias
literature's inclusivity, cross-cultural appropriateness, and alignment with
state-of-the-art NLP advancements.

</details>


### [2] [Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models](https://arxiv.org/abs/2508.20217)
*Mohammad Amini,Babak Ahmadi,Xiaomeng Xiong,Yilin Zhang,Christopher Qiao*

Main category: cs.CL

TL;DR: 本研究探索使用语言模型自动生成形态学评估多选题，通过对比不同规模模型和多种提示策略，发现结构化提示能显著提升中等规模模型的生成质量，并提出一种结合多种评估手段的实用K-12语言评估题目开发与验证流程。


<details>
  <summary>Details</summary>
Motivation: 旨在利用语言模型进行自动题目生成（AIG），以降低人工开发形态学评估多选题的成本，并减少其不一致性。

Method: 1. 对比了微调后的中等规模模型（Gemma, 2B）与大型未调优模型（GPT-3.5, 175B）。2. 评估了七种结构化提示策略，包括零样本、少样本、思维链、角色扮演、顺序以及组合策略。3. 生成的题目通过自动化指标和专家评分进行多维度评估，并使用经过专家评分样本训练的GPT-4.1模拟大规模人类评分。

Result: 1. 结构化提示，特别是结合思维链和顺序设计的策略，显著提升了Gemma的输出质量。2. Gemma生成的题目在构念一致性和教学适宜性方面普遍优于GPT-3.5的零样本响应。3. 提示设计在中等规模模型性能中扮演关键角色。

Conclusion: 1. 结构化提示和高效微调可以在数据有限的情况下增强中等规模模型在自动题目生成方面的能力。2. 结合自动化指标、专家判断和大型模型模拟对于确保评估目标一致性具有重要价值。3. 提出的工作流程为K-12语言评估题目的开发和验证提供了一种实用且可扩展的方法。

Abstract: This study explores automatic generation (AIG) using language models to
create multiple choice questions (MCQs) for morphological assessment, aiming to
reduce the cost and inconsistency of manual test development. The study used a
two-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B)
with a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven
structured prompting strategies, including zero-shot, few-shot,
chain-of-thought, role-based, sequential, and combinations. Generated items
were assessed using automated metrics and expert scoring across five
dimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate
human scoring at scale. Results show that structured prompting, especially
strategies combining chain-of-thought and sequential design, significantly
improved Gemma's outputs. Gemma generally produced more construct-aligned and
instructionally appropriate items than GPT-3.5's zero-shot responses, with
prompt design playing a key role in mid-size model performance. This study
demonstrates that structured prompting and efficient fine-tuning can enhance
midsized models for AIG under limited data conditions. We highlight the value
of combining automated metrics, expert judgment, and large-model simulation to
ensure alignment with assessment goals. The proposed workflow offers a
practical and scalable way to develop and validate language assessment items
for K-12.

</details>


### [3] [Integrating SystemC TLM into FMI 3.0 Co-Simulations with an Open-Source Approach](https://arxiv.org/abs/2508.20223)
*Andrei Mihai Albu,Giovanni Pollo,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Alessandra Neri,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

TL;DR: 本文提出一种开源方法，通过将SystemC TLM模型封装为FMI 3.0 FMU，实现其与基于FMI的协同仿真工作流的无缝集成，以解决复杂网络物理系统在跨域仿真中的互操作性挑战。


<details>
  <summary>Details</summary>
Motivation: 随着网络物理系统（特别是汽车应用）日益复杂，对高效建模和跨域协同仿真技术的需求增加。SystemC TLM虽有助于硬件/软件协同设计，但其与其他工程领域模型的互操作性有限，导致集成困难。

Method: 本文提出一种完全开源的方法论，将SystemC TLM模型集成到基于FMI的协同仿真工作流中。通过将SystemC TLM组件封装为FMI 3.0协同仿真功能模型单元（FMU），该方法实现了跨异构仿真环境的无缝、标准化集成。同时，引入了一个轻量级开源工具链，并解决了时间同步和数据交换等关键技术挑战。

Result: 该方法促进了跨异构仿真环境的无缝、标准化集成。通过代表性案例研究，证明了该集成的可行性和有效性。

Conclusion: 所提出的开源方法成功实现了SystemC TLM模型与FMI协同仿真环境的集成，有效提升了复杂网络物理系统在跨域仿真中的互操作性。

Abstract: The growing complexity of cyber-physical systems, particularly in automotive
applications, has increased the demand for efficient modeling and cross-domain
co-simulation techniques. While SystemC Transaction-Level Modeling (TLM)
enables effective hardware/software co-design, its limited interoperability
with models from other engineering domains poses integration challenges. This
paper presents a fully open-source methodology for integrating SystemC TLM
models into Functional Mock-up Interface (FMI)-based co-simulation workflows.
By encapsulating SystemC TLM components as FMI 3.0 Co Simulation Functional
Mock-up Units (FMUs), the proposed approach facilitates seamless, standardized
integration across heterogeneous simulation environments. We introduce a
lightweight open-source toolchain, address key technical challenges such as
time synchronization and data exchange, and demonstrate the feasibility and
effectiveness of the integration through representative case studies.

</details>


### [4] [Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities](https://arxiv.org/abs/2508.20324)
*Rikuto Kotoge,Mai Nishimura,Jiaxin Ma*

Main category: cs.CL

TL;DR: 针对小型语言模型在强化学习驱动的智能体RAG行为中表现不佳的问题，本文提出了蒸馏引导策略优化（DGPO）方法。DGPO通过教师演示冷启动初始化和持续教师指导，使小型模型能够实现复杂的智能体搜索行为，在某些情况下甚至超越大型教师模型，从而在计算资源受限的环境中实现智能体RAG。


<details>
  <summary>Details</summary>
Motivation: 紧凑型语言模型（例如0.5B参数）在通过强化学习实现代理RAG行为（如搜索和规划）时面临困难，其主要原因是推理能力差，导致奖励稀疏和训练不稳定。

Method: 提出了一种名为蒸馏引导策略优化（DGPO）的方法。DGPO通过教师演示进行冷启动初始化，并在策略优化过程中持续获得教师指导。此外，为系统评估该方法，引入了代理RAG能力（ARC）这一细粒度指标，用于分析推理、搜索协调和响应合成。

Result: 综合实验表明，DGPO使紧凑型模型能够实现复杂的智能体搜索行为，在某些情况下甚至超越了大型教师模型。

Conclusion: DGPO使得在计算资源受限的环境中实现智能体RAG成为可能。

Abstract: Reinforcement Learning has emerged as a post-training approach to elicit
agentic RAG behaviors such as search and planning from language models.
However, compact language models (e.g., 0.5B parameters) struggle due to poor
reasoning ability, resulting in sparse rewards and unstable training. To
overcome these difficulties, we propose Distillation-Guided Policy Optimization
(DGPO), which addresses the challenges through cold-start initialization from
teacher demonstrations and continuous teacher guidance during policy
optimization. To systematically evaluate our approach, we introduce Agentic RAG
Capabilities (ARC), a fine-grained metric analyzing reasoning, search
coordination, and response synthesis. Comprehensive experiments demonstrate
that DGPO enables compact models to achieve sophisticated agentic search
behaviors, even outperforming the larger teacher model in some cases. DGPO
makes agentic RAG feasible in computing resource-constrained environments.

</details>


### [5] [GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs](https://arxiv.org/abs/2508.20325)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Andy Zhou,Yang Zhang,Haohan Wang*

Main category: cs.CL

TL;DR: GUARD是一种LLM测试方法，旨在将高层级AI伦理指南转化为可操作的测试问题和越狱诊断，以评估LLM的合规性并识别潜在漏洞。该方法已在多种LLM和VLM上得到验证。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的广泛应用，其生成有害内容的潜力引发了社会和监管担忧。政府虽已发布AI伦理指南，但这些指南通常是高层级要求，缺乏将其转化为具体可操作的测试问题来验证LLM合规性的方法。

Method: 本研究引入了GUARD（Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics）方法。GUARD通过自动生成基于政府指南的违规问题来测试LLM响应的合规性。对于不直接违规的响应，GUARD整合了“越狱”诊断（GUARD-JD），创建情景以诱发不道德或违规响应，从而识别可能绕过内置安全机制的潜在漏洞。最终，该方法会生成一份合规性报告。

Result: GUARD方法已在七种LLM（包括Vicuna-13B、LongChat-7B、Llama2-7B、Llama-3-8B、GPT-3.5、GPT-4、GPT-4o和Claude-3.7）上进行了经验验证，测试了三种政府发布的指南下的合规性并进行了越狱诊断。此外，GUARD-JD可以将越狱诊断转移到视觉-语言模型（VLMs），展示了其在促进可靠LLM应用中的潜力。

Conclusion: GUARD方法成功地将抽象的AI伦理指南转化为具体的LLM合规性测试问题，并通过越狱诊断有效识别了多种LLM和VLM中存在的潜在漏洞，为确保LLM的可靠性和符合伦理要求提供了实用的工具。

Abstract: As Large Language Models become increasingly integral to various domains,
their potential to generate harmful responses has prompted significant societal
and regulatory concerns. In response, governments have issued ethics guidelines
to promote the development of trustworthy AI. However, these guidelines are
typically high-level demands for developers and testers, leaving a gap in
translating them into actionable testing questions to verify LLM compliance.
  To address this challenge, we introduce GUARD (\textbf{G}uideline
\textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and
Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize
guidelines into specific guideline-violating questions that assess LLM
adherence. To implement this, GUARD uses automated generation of
guideline-violating questions based on government-issued guidelines, thereby
testing whether responses comply with these guidelines. When responses directly
violate guidelines, GUARD reports inconsistencies. Furthermore, for responses
that do not directly violate guidelines, GUARD integrates the concept of
``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that
provoke unethical or guideline-violating responses, effectively identifying
potential scenarios that could bypass built-in safety mechanisms. Our method
finally culminates in a compliance report, delineating the extent of adherence
and highlighting any violations.
  We have empirically validated the effectiveness of GUARD on seven LLMs,
including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4,
GPT-4o, and Claude-3.7, by testing compliance under three government-issued
guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can
transfer jailbreak diagnostics to vision-language models, demonstrating its
usage in promoting reliable LLM-based applications.

</details>


### [6] [Joint Enhancement of Relational Reasoning for Long-Context LLMs](https://arxiv.org/abs/2508.20351)
*Zhirui Chen,Wei Shen,Jiashui Huang,Ling Shao*

Main category: cs.CL

TL;DR: JERR是一个新颖的框架，通过结合概要提取、有向无环图（DAG）构建和蒙特卡洛树搜索（MCTS），显著提升了大型语言模型（LLM）处理长上下文和复杂推理任务的能力，同时提高了可靠性和透明度，并在实验中超越了现有基线。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM取得了显著进展，但在处理长上下文时仍面临记忆限制、难以应对复杂和长上下文任务的挑战。此外，LLM还存在透明度不足和易产生幻觉的问题。

Method: 本文提出了JERR框架，通过图基推理增强LLM的长上下文理解能力。JERR包含三个核心组件：首先是概要提取，通过战略性地分块文本进行总结；其次是图构建，建立有向无环图以解决冗余并确保逻辑一致性；最后是关系推理，引入蒙特卡洛树搜索（MCTS）来导航复杂的推理路径，以实现更准确和可解释的输出。

Result: 实验结果表明，JERR在ROUGE和F1指标上持续优于所有基线模型，并在LLM-Rater评估中取得了最高分。

Conclusion: JERR框架提供了一种新颖的解决方案，使LLM能够以更高的可靠性和透明度处理扩展上下文和复杂的推理任务。

Abstract: Despite significant progress, large language models (LLMs) still struggle
with long contexts due to memory limitations and their inability to tackle
complex and long-context tasks. Additionally, LLMs often suffer from a lack of
transparency and are prone to producing hallucinations. To address these
challenges, we propose \textbf{JERR}, a novel framework designed to enhance
long-context comprehension via graph-based reasoning in LLMs. JERR integrates
three key components: synopsis extraction, graph construction, and relational
reasoning. First, synopsis is extracted by chunking text strategically,
allowing the model to summarize and understand information more efficiently.
Second, we build a directed acyclic graph (DAG) to resolve redundancy, ensuring
logical consistency and clarity. Finally, we incorporate Monte Carlo Tree
Search (MCTS) to help the model navigate complex reasoning paths, ensuring more
accurate and interpretable outputs. This framework provides a novel solution
that enables LLMs to handle extended contexts and complex reasoning tasks with
improved reliability and transparency. Experimental results show that JERR
consistently outperforms all baselines on the ROUGE and F1 metrics, achieving
the highest scores on the LLM-Rater evaluation.

</details>


### [7] [Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems](https://arxiv.org/abs/2508.20373)
*Yuyao Wang,Bowen Liu,Jianheng Tang,Nuo Chen,Yuhan Li,Qifan Zhang,Jia Li*

Main category: cs.CL

TL;DR: 本文提出使用NP-hard图问题作为RLLMs的合成训练语料，并开发了一个两阶段后训练框架（SFT+RL），以提升长链式思考（Long CoT）能力，实现了强大的泛化性并超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 推理大型语言模型（RLLMs）在复杂推理任务上的进步严重依赖于昂贵且人工策划的高质量数据集，可扩展的替代方案尚未被探索。NP-hard图问题本质上需要深度推理、广泛探索和反思策略，这些都是Long CoT推理的核心特征，因此适合作为训练语料。

Method: 引入NP-hard（NPH）图问题作为一种新颖的合成训练语料。开发了一个两阶段后训练框架：(i) 在拒绝采样的NPH图实例上进行长链式思考监督微调（Long CoT SFT），以显著增强推理深度；(ii) 使用细粒度奖励设计的强化学习（RL），以提高推理效率。

Result: 旗舰模型Graph-R1-7B在数学、编码、STEM和逻辑等领域表现出强大的泛化能力，并且在NPH图问题上在准确性和推理效率方面均超越了QwQ-32B。

Conclusion: NPH图问题被定位为一种有效且可扩展的资源，可以促进大型语言模型的长链式思考能力发展，为LLM后训练开辟了新领域。

Abstract: Reasoning Large Language Models (RLLMs) have recently achieved remarkable
progress on complex reasoning tasks, largely enabled by their long
chain-of-thought (Long CoT) capabilities. However, developing these Long CoT
behaviors relies heavily on post-training with high-quality datasets, which are
typically costly and human-curated (e.g., mathematics and code), leaving
scalable alternatives unexplored. In this work, we introduce NP-hard (NPH)
graph problems as a novel synthetic training corpus, as they inherently require
deep reasoning, extensive exploration, and reflective strategies, which are
core characteristics of Long CoT reasoning. Building on this insight, we
develop a two-stage post-training framework: (i) Long CoT Supervised
Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially
enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a
fine-grained reward design, which sharpens reasoning efficiency. Our flagship
model, Graph-R1-7B, demonstrates strong generalization across mathematics,
coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both
accuracy and reasoning efficiency. These results position NPH graph problems as
an effective and scalable resource for advancing Long CoT reasoning in LLMs,
opening a new frontier for LLM post-training. Our implementation is available
at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted
in our Hugging Face collection HKUST-DSAIL/Graph-R1.

</details>


### [8] [CAPE: Context-Aware Personality Evaluation Framework for Large Language Models](https://arxiv.org/abs/2508.20385)
*Jivnesh Sandhan,Fei Cheng,Tushar Sandhan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 本研究提出了首个上下文感知个性评估（CAPE）框架，以评估大语言模型（LLM）在对话历史影响下的行为特征。实验发现，上下文能增强响应一致性但也会导致个性偏移，且不同模型对上下文和问题顺序的敏感度各异。


<details>
  <summary>Details</summary>
Motivation: 传统心理测试在评估LLM行为特征时采用无上下文方法，即孤立回答每个问题，忽略了真实世界应用中对话历史对响应的影响。为了弥补这一差距，需要一个能考虑先前对话互动的上下文感知评估框架。

Method: 本文提出了第一个针对LLM的上下文感知个性评估（CAPE）框架，该框架纳入了先前的对话交互。同时，引入了新颖的度量指标来量化LLM响应的一致性。研究对7个LLM进行了详尽实验，并进一步将该框架应用于角色扮演代理（RPAs）。

Result: 对话历史通过上下文学习提高了响应一致性，但同时也导致了LLM的个性偏移，其中GPT-3.5-Turbo和GPT-4-Turbo表现出极端偏差。GPT模型对问题顺序具有鲁棒性，而Gemini-1.5-Flash和Llama-8B则显示出显著敏感性。GPT模型的响应来源于其内在个性特征和先前交互，而Gemini-1.5-Flash和Llama-8B则严重依赖于先前交互。将框架应用于RPAs表明，上下文依赖的个性偏移能提高响应一致性并更好地符合人类判断。

Conclusion: 上下文感知评估对于理解和评估LLM的个性至关重要。对话历史显著影响LLM的行为，既能提升响应一致性，也会引发个性转变。不同模型对上下文和问题顺序的反应存在差异。在角色扮演场景中，考虑上下文的个性转变有助于提升LLM的响应一致性和与人类判断的对齐程度。

Abstract: Psychometric tests, traditionally used to assess humans, are now being
applied to Large Language Models (LLMs) to evaluate their behavioral traits.
However, existing studies follow a context-free approach, answering each
question in isolation to avoid contextual influence. We term this the Disney
World test, an artificial setting that ignores real-world applications, where
conversational history shapes responses. To bridge this gap, we propose the
first Context-Aware Personality Evaluation (CAPE) framework for LLMs,
incorporating prior conversational interactions. To thoroughly analyze the
influence of context, we introduce novel metrics to quantify the consistency of
LLM responses, a fundamental trait in human behavior.
  Our exhaustive experiments on 7 LLMs reveal that conversational history
enhances response consistency via in-context learning but also induces
personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme
deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash
and Llama-8B display significant sensitivity. Moreover, GPT models response
stem from their intrinsic personality traits as well as prior interactions,
whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions.
Finally, applying our framework to Role Playing Agents (RPAs) shows
context-dependent personality shifts improve response consistency and better
align with human judgments. Our code and datasets are publicly available at:
https://github.com/jivnesh/CAPE

</details>


### [9] [Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction](https://arxiv.org/abs/2508.20395)
*Xu Guo*

Main category: cs.CL

TL;DR: 本研究通过对大型语言模型推理链的条件熵分析，发现熵值随步骤下降与正确答案强关联，而熵值持平或上升则常导致错误，且错误推理路径通常更长。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM常通过生成中间推理步骤提高准确性，但推理效用对最终答案正确性的贡献尚不明确。鉴于生成过程的随机性，更多上下文不保证更高置信度。若能在生成时预测推理步骤的有效性，则可提早停止或剪除无效步骤，避免干扰。

Method: 在MATH数据集上进行了一项预言机研究。使用Qwen2.5-32B和GPT-4o生成推理链，并用一个独立模型（Qwen3-8B）量化这些链对最终准确性的效用。具体通过测量每个推理步骤中，答案Y在上下文逐步扩展时的条件熵（词汇表上的预期负对数似然）来衡量模型的不确定性。

Result: 结果显示，条件熵随步骤减少与正确答案强相关，而熵值持平或增加则常导致错误答案。研究还证实，不正确的推理路径往往比正确路径更长，表明更长的推理不一定带来更好的结果。

Conclusion: 这些发现为未来设计高效推理管线奠定了基础，旨在早期检测并避免无效推理。

Abstract: Recent advancements in large language models (LLMs) often rely on generating
intermediate reasoning steps to enhance accuracy. However, little work has
examined how reasoning utility contributes to the final answer's correctness.
Due to the stochastic nature of autoregressive generation, generating more
context does not guarantee increased confidence in the answer. If we could
predict, during generation, whether a reasoning step will be useful, we could
stop early or prune ineffective steps, avoiding distractions in the final
decision.
  We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to
generate reasoning chains, and then employing a separate model (Qwen3-8B) to
quantify the utility of these chains for final accuracy. Specifically, we
measure the model's uncertainty on the answer span Y at each reasoning step
using conditional entropy (expected negative log-likelihood over the
vocabulary) with context expanding step by step. Our results show a clear
pattern: conditional entropy that decreases over steps is strongly associated
with correct answers, whereas flat or increasing entropy often results in wrong
answers. We also corroborate that incorrect reasoning paths tend to be longer
than correct ones, suggesting that longer reasoning does not necessarily yield
better outcomes. These findings serve as a foundation to inspire future work on
designing efficient reasoning pipelines that detect and avoid unproductive
reasoning early.

</details>


### [10] [UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools](https://arxiv.org/abs/2508.20410)
*Sam Jung,Agustin Garcinuno,Spencer Mateega*

Main category: cs.CL

TL;DR: 本文介绍了UI-Bench，这是首个大规模基准测试，通过专家成对比较，严格评估AI文本生成应用工具在视觉卓越性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本生成应用工具声称能在几分钟内创建高质量应用和网站，但缺乏公开、严格的基准来验证这些说法。

Method: 引入了UI-Bench，一个大规模基准测试平台。它涵盖10种工具、30个提示、300个生成的网站，并通过4000多次专家成对比较来评估视觉卓越性。使用TrueSkill模型对系统进行排名，并提供校准置信区间。同时发布了完整的提示集、开源评估框架和公共排行榜。

Result: UI-Bench通过TrueSkill模型对系统进行排名，并建立了可复现的AI驱动网页设计评估标准。提供了一个公共排行榜，并计划尽快发布参与者评分的生成网站。

Conclusion: UI-Bench首次为AI文本生成应用工具的视觉质量提供了一个大规模、可复现的评估标准和平台，旨在推动AI驱动的网页设计领域发展，并提供了开放资源。

Abstract: AI text-to-app tools promise high quality applications and websites in
minutes, yet no public benchmark rigorously verifies those claims. We introduce
UI-Bench, the first large-scale benchmark that evaluates visual excellence
across competing AI text-to-app tools through expert pairwise comparison.
Spanning 10 tools, 30 prompts, 300 generated sites, and \textit{4000+} expert
judgments, UI-Bench ranks systems with a TrueSkill-derived model that yields
calibrated confidence intervals. UI-Bench establishes a reproducible standard
for advancing AI-driven web design. We release (i) the complete prompt set,
(ii) an open-source evaluation framework, and (iii) a public leaderboard. The
generated sites rated by participants will be released soon. View the UI-Bench
leaderboard at https://uibench.ai/leaderboard.

</details>


### [11] [DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding](https://arxiv.org/abs/2508.20416)
*Hengchuan Zhu,Yihuan Xu,Yichen Li,Zijie Meng,Zuozhu Liu*

Main category: cs.CL

TL;DR: 本文介绍了DentalBench，首个针对口腔医学领域的综合性双语LLM基准测试平台，包括用于评估的DentalQA问答数据集和用于领域适应的DentalCorpus语料库，并实验证明领域适应能显著提升LLMs在口腔医学任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 通用和医学LLMs在普通医疗基准上表现良好，但在口腔医学等需要深入专业知识的细分领域中，由于缺乏针对性的评估资源，其能力尚未得到充分探索。

Method: 引入了DentalBench，包含两部分：DentalQA，一个包含36,597个问题，覆盖4个任务和16个口腔子领域的英汉双语问答基准；DentalCorpus，一个包含3.37亿词元，用于支持监督微调(SFT)和检索增强生成(RAG)的口腔领域高质量语料库。作者评估了14种LLM，并通过Qwen-2.5-3B模型进行了领域适应实验。

Result: 评估结果显示，14种LLM在不同任务类型和语言上存在显著的性能差距。对Qwen-2.5-3B进行的领域适应实验表明，此方法能大幅提高模型性能，尤其是在知识密集型和术语聚焦型任务上。

Conclusion: 研究强调了领域特定基准对于开发可靠、有效的医疗健康领域LLM的重要性。

Abstract: Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs)
have demonstrated strong performance on general medical benchmarks. However,
their capabilities in specialized medical fields, such as dentistry which
require deeper domain-specific knowledge, remain underexplored due to the lack
of targeted evaluation resources. In this paper, we introduce DentalBench, the
first comprehensive bilingual benchmark designed to evaluate and advance LLMs
in the dental domain. DentalBench consists of two main components: DentalQA, an
English-Chinese question-answering (QA) benchmark with 36,597 questions
spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale,
high-quality corpus with 337.35 million tokens curated for dental domain
adaptation, supporting both supervised fine-tuning (SFT) and
retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering
proprietary, open-source, and medical-specific models, and reveal significant
performance gaps across task types and languages. Further experiments with
Qwen-2.5-3B demonstrate that domain adaptation substantially improves model
performance, particularly on knowledge-intensive and terminology-focused tasks,
and highlight the importance of domain-specific benchmarks for developing
trustworthy and effective LLMs tailored to healthcare applications.

</details>


### [12] [KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval](https://arxiv.org/abs/2508.20417)
*Chi Minh Bui,Ngoc Mai Thieu,Van Vinh Nguyen,Json J. Jung,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: 本文提出KG-CQR，一个利用知识图谱增强复杂查询上下文的RAG检索框架，通过结构化关系表示丰富查询，无需额外训练即可显著提升检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有RAG检索方法主要解决语料库层面的上下文丢失，但对复杂输入查询的上下文表示增强不足。将知识图谱与大型语言模型结合，具有显著改善RAG系统检索阶段的潜力。

Method: KG-CQR是一个新颖的上下文查询检索(CQR)框架。它利用以语料库为中心的知识图谱，通过提取和补全相关KG子图，生成语义丰富的查询上下文来增强复杂查询的表示。该框架包含子图提取、补全和上下文生成模块，是一个模型无关的管道，无需额外训练即可扩展到不同规模的LLMs。

Result: 在RAGBench和MultiHop-RAG数据集上，KG-CQR的性能优于现有强基线模型，mAP提升了4-6%，Recall@25提升了2-3%。在多跳问答等具有挑战性的RAG任务中，其检索效率持续优于现有基线。

Conclusion: KG-CQR通过利用知识图谱有效丰富查询上下文，显著提升了RAG系统的检索阶段性能，并展现出优越的检索效率，同时具有模型无关性和良好的可扩展性。

Abstract: The integration of knowledge graphs (KGs) with large language models (LLMs)
offers significant potential to improve the retrieval phase of
retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,
a novel framework for Contextual Query Retrieval (CQR) that enhances the
retrieval phase by enriching the contextual representation of complex input
queries using a corpus-centric KG. Unlike existing methods that primarily
address corpus-level context loss, KG-CQR focuses on query enrichment through
structured relation representations, extracting and completing relevant KG
subgraphs to generate semantically rich query contexts. Comprising subgraph
extraction, completion, and contextual generation modules, KG-CQR operates as a
model-agnostic pipeline, ensuring scalability across LLMs of varying sizes
without additional training. Experimental results on RAGBench and MultiHop-RAG
datasets demonstrate KG-CQR's superior performance, achieving a 4-6%
improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline
models. Furthermore, evaluations on challenging RAG tasks such as multi-hop
question answering show that, by incorporating KG-CQR, the performance
consistently outperforms the existing baseline in terms of retrieval
effectiveness

</details>


### [13] [CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance](https://arxiv.org/abs/2508.20420)
*Feng Zhang,Chengjie Pang,Yuehan Zhang,Chenyu Luo*

Main category: cs.CL

TL;DR: 本文提出并开发了一个工业级民航维修领域大型语言模型（LLM）基准测试平台，旨在弥补该领域专业评估工具的空白，并用其评估了现有LLM和嵌入模型的性能。


<details>
  <summary>Details</summary>
Motivation: 民航维修是知识密集且需要复杂推理的领域，但当前缺乏专门用于评估LLM在该领域能力的工具。现有LLM评估主要侧重于数学和编码推理，无法有效衡量其在民航维修专业知识和复杂推理方面的表现。

Method: 本文提出并开发了一个工业级民航维修LLM基准测试平台。该平台用作标准化工具，以衡量LLM在民航维修领域的特定能力，识别领域知识和复杂推理的不足。此外，利用此基准评估了当前主流的向量嵌入模型和LLM在民航维修RAG系统中的表现。

Result: 通过实验探索和分析，证明了所开发的基准测试平台能够有效评估模型在民航维修领域的性能。该项目已开源评估基准和代码。

Conclusion: 该工作通过创建专业的民航维修LLM评估基准，填补了现有评估工具的空白，能有效识别模型在该领域的缺陷，为未来针对性的模型改进（如领域微调、RAG优化）提供了基础，从而促进民航维修领域智能解决方案的进步。

Abstract: Civil aviation maintenance is a domain characterized by stringent industry
standards. Within this field, maintenance procedures and troubleshooting
represent critical, knowledge-intensive tasks that require sophisticated
reasoning. To address the lack of specialized evaluation tools for large
language models (LLMs) in this vertical, we propose and develop an
industrial-grade benchmark specifically designed for civil aviation
maintenance. This benchmark serves a dual purpose: It provides a standardized
tool to measure LLM capabilities within civil aviation maintenance, identifying
specific gaps in domain knowledge and complex reasoning. By pinpointing these
deficiencies, the benchmark establishes a foundation for targeted improvement
efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized
prompt engineering), ultimately facilitating progress toward more intelligent
solutions within civil aviation maintenance. Our work addresses a significant
gap in the current LLM evaluation, which primarily focuses on mathematical and
coding reasoning tasks. In addition, given that Retrieval-Augmented Generation
(RAG) systems are currently the dominant solutions in practical applications ,
we leverage this benchmark to evaluate existing well-known vector embedding
models and LLMs for civil aviation maintenance scenarios. Through experimental
exploration and analysis, we demonstrate the effectiveness of our benchmark in
assessing model performance within this domain, and we open-source this
evaluation benchmark and code to foster further research and
development:https://github.com/CamBenchmark/cambenchmark

</details>


### [14] [Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method](https://arxiv.org/abs/2508.20442)
*Agung Sukrisna Jaya,Osvari Arsalan,Danny Matthew Saputra*

Main category: cs.CL

TL;DR: 本文提出一个基于案例推理(CBR)的系统，结合TF-IDF和余弦相似度，用于搜索实践工作标题。该系统在对705个实践工作标题进行测试后，展示了良好的匹配度。


<details>
  <summary>Details</summary>
Motivation: 利用案例推理(CBR)技术，基于过往经验，有效搜索和匹配实践工作标题。

Method: 采用案例推理(CBR)作为核心案例解决技术。通过TF-IDF算法对实践工作标题的词语进行向量化处理，并使用余弦相似度计算标题间的相似性。该系统支持通过标题或关键词进行搜索，并输出匹配的实践工作标题及其匹配值。

Result: 系统在705个实践工作标题上进行了测试，分两个阶段使用五个测试标题。第二阶段（随机化标题）的结果显示，系统找到了相同数量的标题，并获得了最高的平均匹配分数，表明其在更复杂查询下的有效性。

Conclusion: 所构建的基于CBR、TF-IDF和余弦相似度的系统能够有效地搜索实践工作标题，在测试中展现出良好的匹配性能，尤其是在随机化查询条件下也能保持高匹配度。

Abstract: Case Base Reasoning (CBR) is a case solving technique based on experience in
cases that have occurred before with the highest similarity. CBR is used to
search for practical work titles. TF-IDF is applied to process the
vectorization of each practical work title word and Cosine Similarity for the
calculation of similarity values. This system can search either in the form of
titles or keywords. The output of the system is the title of practical work and
the match value of each title. Based on the test results using 705 practical
work titles, testing was carried out with five titles and carried out in two
stages. The first stage searches with existing titles and the second stage
randomizes the title from the first stage. And the results obtained in the
second stage are the same number of titles found and the highest average match
score.

</details>


### [15] [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers](https://arxiv.org/abs/2508.20453)
*Zhenting Wang,Qi Chang,Hemani Patel,Shashank Biju,Cheng-En Wu,Quan Liu,Aolin Ding,Alireza Rezazadeh,Ankit Shah,Yujia Bao,Eugene Siow*

Main category: cs.CL

TL;DR: 本文提出了MCP-Bench，一个用于评估大型语言模型（LLMs）在需要工具使用、跨工具协调、精确参数控制和规划推理的真实多步骤任务上的基准，并揭示了LLMs在此类任务中的持续挑战。


<details>
  <summary>Details</summary>
Motivation: 现有基于API的基准未能充分评估LLMs在真实、多步骤任务中的高级能力，如从模糊指令中检索工具、规划多跳执行路径、基于中间工具输出进行响应以及协调跨领域工作流。这些基准通常依赖于明确的工具规范、浅层少步工作流和孤立的领域操作。

Method: 引入了基于模型上下文协议（MCP）的MCP-Bench，它将LLMs连接到28个代表性的实时MCP服务器，涵盖金融、旅行、科学计算和学术搜索等250种工具。该基准设计了具有丰富输入-输出耦合的真实多步骤任务，以测试LLM从模糊指令中检索工具、规划多跳执行轨迹、将响应基于中间工具输出以及编排跨领域工作流的能力。提出了一个多层面评估框架，涵盖工具级模式理解和使用、轨迹级规划以及任务完成度，并对20个先进的LLM进行了实验。

Result: 对20个先进LLM进行的实验表明，在MCP-Bench设定的复杂多步骤工具使用任务中，LLM仍然存在持续的挑战。

Conclusion: MCP-Bench作为一个新的基准，揭示了当前LLMs在处理需要复杂工具使用、协调和规划的真实多步骤任务时所面临的显著不足，表明LLMs在这些领域仍有很大的改进空间。

Abstract: We introduce MCP-Bench, a benchmark for evaluating large language models
(LLMs) on realistic, multi-step tasks that demand tool use, cross-tool
coordination, precise parameter control, and planning/reasoning for solving
tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28
representative live MCP servers spanning 250 tools across domains such as
finance, traveling, scientific computing, and academic search. Unlike prior
API-based benchmarks, each MCP server provides a set of complementary tools
designed to work together, enabling the construction of authentic, multi-step
tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability
to retrieve relevant tools from fuzzy instructions without explicit tool names,
plan multi-hop execution trajectories for complex objectives, ground responses
in intermediate tool outputs, and orchestrate cross-domain workflows -
capabilities not adequately evaluated by existing benchmarks that rely on
explicit tool specifications, shallow few-step workflows, and isolated domain
operations. We propose a multi-faceted evaluation framework covering tool-level
schema understanding and usage, trajectory-level planning, and task completion.
Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code
and data: https://github.com/Accenture/mcp-bench.

</details>


### [16] [Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques](https://arxiv.org/abs/2508.20460)
*Yucheng Ruan,Xiang Lan,Daniel J. Tan,Hairil Rizal Abdullah,Mengling Feng*

Main category: cs.CL

TL;DR: 本研究提出了一种深度学习框架，通过整合多模态电子健康记录（包括自由文本），利用自然语言处理技术，显著提升了重症监护室中死亡率和资源利用（如住院时长、手术时长）的预测准确性，并显示出对数据损坏的强大鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 从电子健康记录（EHR）中预测死亡率和资源利用对优化患者预后和管理成本至关重要，但现有方法主要关注结构化数据，忽视了自由文本注释中的宝贵临床洞察，也未能充分利用结构化数据中的文本信息。

Method: 开发并评估了一个基于自然语言处理的深度学习框架，该框架整合了多模态EHR数据。模型在两个真实世界的EHR数据集上，针对死亡率预测、住院时长预测和手术时长估计三项临床任务与现有领先方法进行了比较。此外，还对医疗提示、自由文本和预训练句子编码器进行了消融研究，并评估了模型对结构化EHR数据损坏的鲁棒性。

Result: 与现有最佳方法相比，所提出的模型在死亡率预测方面将BACC/AUROC性能分别提高了1.6%/0.8%，在住院时长预测方面将RMSE/MAE性能分别提高了0.5%/2.2%，在手术时长估计方面将RMSE/MAE性能分别提高了10.9%/11.0%。在不同损坏率下，该模型在所有三项任务中均表现出优于其他基线的性能。

Conclusion: 所提出的框架是预测重症监护中死亡率和资源利用的一种有效且准确的深度学习方法。研究强调了使用提示学习结合Transformer编码器在分析多模态EHR方面的成功，并且该模型对结构化数据中的损坏（尤其是在高损坏级别下）表现出强大的弹性。

Abstract: Background Predicting mortality and resource utilization from electronic
health records (EHRs) is challenging yet crucial for optimizing patient
outcomes and managing costs in intensive care unit (ICU). Existing approaches
predominantly focus on structured EHRs, often ignoring the valuable clinical
insights in free-text notes. Additionally, the potential of textual information
within structured data is not fully leveraged. This study aimed to introduce
and assess a deep learning framework using natural language processing
techniques that integrates multimodal EHRs to predict mortality and resource
utilization in critical care settings. Methods Utilizing two real-world EHR
datasets, we developed and evaluated our model on three clinical tasks with
leading existing methods. We also performed an ablation study on three key
components in our framework: medical prompts, free-texts, and pre-trained
sentence encoder. Furthermore, we assessed the model's robustness against the
corruption in structured EHRs. Results Our experiments on two real-world
datasets across three clinical tasks showed that our proposed model improved
performance metrics by 1.6\%/0.8\% on BACC/AUROC for mortality prediction,
0.5%/2.2% on RMSE/MAE for LOS prediction, 10.9%/11.0% on RMSE/MAE for surgical
duration estimation compared to the best existing methods. It consistently
demonstrated superior performance compared to other baselines across three
tasks at different corruption rates. Conclusions The proposed framework is an
effective and accurate deep learning approach for predicting mortality and
resource utilization in critical care. The study also highlights the success of
using prompt learning with a transformer encoder in analyzing multimodal EHRs.
Importantly, the model showed strong resilience to data corruption within
structured data, especially at high corruption levels.

</details>


### [17] [ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety](https://arxiv.org/abs/2508.20468)
*Luke Bates,Max Glockner,Preslav Nakov,Iryna Gurevych*

Main category: cs.CL

TL;DR: 本文介绍了ConspirED数据集，用于捕捉阴谋论的认知特征，并利用其开发了计算模型来识别阴谋论特质，同时评估了大型语言模型（LLMs）对阴谋论输入的鲁棒性，发现LLMs易受阴谋论内容影响，输出会模仿其推理模式。


<details>
  <summary>Details</summary>
Motivation: 阴谋论侵蚀公众对科学和机构的信任，并通过不断演变和吸收反驳证据来抵制揭穿。随着人工智能生成虚假信息的日益复杂，理解阴谋论内容的修辞模式对于开发干预措施（如目标性预揭穿）和评估AI漏洞至关重要。

Method: 1. 引入ConspirED数据集，其中包含来自在线阴谋论文章的多句摘录（80-120词），并使用CONSPIR认知框架（Lewandowsky和Cook, 2020）进行注释，捕捉阴谋论思想的认知特质。2. 基于ConspirED，开发计算模型以识别文本摘录中的阴谋论特质并确定主导特质。3. 评估大型语言/推理模型（LLM/LRM）对阴谋论输入的鲁棒性。

Result: 研究发现，用于识别阴谋论特质的计算模型以及LLM/LRM都容易受到阴谋论内容的“错位”影响。LLM/LRM在面对阴谋论输入时，会产生模仿输入推理模式的输出，即便它们能够成功地驳斥可比较的事实核查过的虚假信息。

Conclusion: 阴谋论内容对计算模型分析和大型语言模型的鲁棒性都构成了独特的挑战，导致它们反映阴谋论的推理模式，即使它们能够处理其他形式的虚假信息。这突显了在人工智能时代，针对阴谋论修辞的独特理解和对策的重要性。

Abstract: Conspiracy theories erode public trust in science and institutions while
resisting debunking by evolving and absorbing counter-evidence. As AI-generated
misinformation becomes increasingly sophisticated, understanding rhetorical
patterns in conspiratorial content is important for developing interventions
such as targeted prebunking and assessing AI vulnerabilities. We introduce
ConspirED (CONSPIR Evaluation Dataset), which captures the cognitive traits of
conspiratorial ideation in multi-sentence excerpts (80--120 words) from online
conspiracy articles, annotated using the CONSPIR cognitive framework
(Lewandowsky and Cook, 2020). ConspirED is the first dataset of conspiratorial
content annotated for general cognitive traits. Using ConspirED, we (i) develop
computational models that identify conspiratorial traits and determine dominant
traits in text excerpts, and (ii) evaluate large language/reasoning model
(LLM/LRM) robustness to conspiratorial inputs. We find that both are misaligned
by conspiratorial content, producing output that mirrors input reasoning
patterns, even when successfully deflecting comparable fact-checked
misinformation.

</details>


### [18] [Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark](https://arxiv.org/abs/2508.20511)
*Chihiro Taguchi,Seng Mai,Keita Kurabe,Yusuke Sakai,Georgina Agyei,Soudabeh Eslami,David Chiang*

Main category: cs.CL

TL;DR: FLORES+等多语言机器翻译基准测试存在关键缺陷，包括数据质量不足、文化偏见和评估漏洞，导致现有模型性能评估不准确，因此需改进基准设计以更好地反映真实世界需求。


<details>
  <summary>Details</summary>
Motivation: FLORES+作为广泛使用的多语言机器翻译基准，尽管宣称有严格的质量控制，但研究者旨在探究其在真正多语言评估中的适用性。

Method: 研究者选取了四种语言（阿散蒂特威语、日语、景颇语和南阿塞拜疆语）的数据进行研究。通过人工评估揭示翻译质量问题，并分析源文本的领域特异性和文化偏见。此外，他们使用复制命名实体等简单启发式方法来测试评估协议的脆弱性，并比较了在高质量、自然数据上训练的模型在FLORES+和其领域相关评估集上的表现。

Result: 人工评估显示许多翻译未达到宣称的90%质量标准；标注者报告源句过于领域特定且存在文化偏见。简单的启发式方法（如复制命名实体）可以获得不低的BLEU分数，表明评估协议存在漏洞。最重要的是，在高质量自然数据上训练的MT模型在FLORES+上表现不佳，但在研究者构建的领域相关评估集上取得了显著提升。

Conclusion: 研究建议未来的多语言MT基准应使用领域通用且文化中立的源文本，并减少对命名实体的依赖，以更好地反映现实世界的翻译挑战。

Abstract: Multilingual machine translation (MT) benchmarks play a central role in
evaluating the capabilities of modern MT systems. Among them, the FLORES+
benchmark is widely used, offering English-to-many translation data for over
200 languages, curated with strict quality control protocols. However, we study
data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani)
and uncover critical shortcomings in the benchmark's suitability for truly
multilingual evaluation. Human assessments reveal that many translations fall
below the claimed 90% quality standard, and the annotators report that source
sentences are often too domain-specific and culturally biased toward the
English-speaking world. We further demonstrate that simple heuristics, such as
copying named entities, can yield non-trivial BLEU scores, suggesting
vulnerabilities in the evaluation protocol. Notably, we show that MT models
trained on high-quality, naturalistic data perform poorly on FLORES+ while
achieving significant gains on our domain-relevant evaluation set. Based on
these findings, we advocate for multilingual MT benchmarks that use
domain-general and culturally neutral source texts rely less on named entities,
in order to better reflect real-world translation challenges.

</details>


### [19] [SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM](https://arxiv.org/abs/2508.20514)
*Pengjiang Li,Zaitian Wang,Xinhao Zhang,Ran Zhang,Lu Jiang,Pengfei Wang,Yuanchun Zhou*

Main category: cs.CL

TL;DR: 本文提出SciTopic，一种由大型语言模型（LLM）增强的科学主题发现方法，通过构建文本编码器、集成基于熵采样的空间优化模块和LLM引导的三元组任务来微调编码器，以更有效地识别科学文献主题，并超越了现有最新技术。


<details>
  <summary>Details</summary>
Motivation: 科学文献中的主题发现对于研究者识别新兴趋势、探索新领域及检索信息至关重要。尽管现有机器学习方法（特别是深度嵌入技术）已被应用，但它们主要依赖词嵌入，缺乏对科学出版物的全面理解，难以处理复杂、高维的文本关系。鉴于大型语言模型（LLM）卓越的文本理解能力，本文旨在利用LLM改进科学主题识别。

Method: 本文提出了名为SciTopic的先进主题发现方法：1. 构建一个文本编码器来捕获科学出版物的内容（包括元数据、标题和摘要）。2. 构建一个空间优化模块，该模块集成了基于熵的采样和由LLM引导的三元组任务，以增强对主题相关性和模糊实例之间上下文复杂性的关注。3. 基于LLM的指导，通过优化三元组的对比损失来微调文本编码器，迫使文本编码器更好地辨别不同主题的实例。

Result: 在三个真实的科学出版物数据集上进行了广泛实验，结果表明SciTopic优于现有最先进（SOTA）的科学主题发现方法。

Conclusion: SciTopic能够帮助研究人员获得更深入、更快速的洞察力，有效提升科学主题发现的效率和质量。

Abstract: Topic discovery in scientific literature provides valuable insights for
researchers to identify emerging trends and explore new avenues for
investigation, facilitating easier scientific information retrieval. Many
machine learning methods, particularly deep embedding techniques, have been
applied to discover research topics. However, most existing topic discovery
methods rely on word embedding to capture the semantics and lack a
comprehensive understanding of scientific publications, struggling with
complex, high-dimensional text relationships. Inspired by the exceptional
comprehension of textual information by large language models (LLMs), we
propose an advanced topic discovery method enhanced by LLMs to improve
scientific topic identification, namely SciTopic. Specifically, we first build
a textual encoder to capture the content from scientific publications,
including metadata, title, and abstract. Next, we construct a space
optimization module that integrates entropy-based sampling and triplet tasks
guided by LLMs, enhancing the focus on thematic relevance and contextual
intricacies between ambiguous instances. Then, we propose to fine-tune the
textual encoder based on the guidance from the LLMs by optimizing the
contrastive loss of the triplets, forcing the text encoder to better
discriminate instances of different topics. Finally, extensive experiments
conducted on three real-world datasets of scientific publications demonstrate
that SciTopic outperforms the state-of-the-art (SOTA) scientific topic
discovery methods, enabling researchers to gain deeper and faster insights.

</details>


### [20] [Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20532)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Salvador Lima-López,Eulàlia Farré-Maduell,Martin Krallinger,Natalia Loukachevitch,Vera Davydova,Elena Tutubalina,Georgios Paliouras*

Main category: cs.CL

TL;DR: 本文概述了CLEF 2024背景下的第十二届BioASQ挑战赛。该挑战赛旨在推动大规模生物医学语义索引和问答领域的进步，本届包含了四项任务（两项既有，两项新增：MultiCardioNER和BIONNE）。共有37支团队参与，提交了700多份作品，多数系统表现出色，持续推动了该领域的最新技术发展。


<details>
  <summary>Details</summary>
Motivation: BioASQ作为一系列国际挑战赛，旨在促进大规模生物医学语义索引和问答领域的持续进步。

Method: 通过组织国际挑战赛的形式，本届BioASQ设立了两个既有任务（b和Synergy）以及两个新任务：MultiCardioNER（多语言心脏病学领域临床实体检测）和BIONNE（俄语和英语嵌套命名实体识别）。

Result: 共有37支参赛团队参与了本届BioASQ挑战赛，为四项任务提交了超过700份作品。与往届类似，大多数参赛系统取得了具有竞争力的性能。

Conclusion: BioASQ挑战赛的持续举办及其参赛系统的优异表现，表明该系列挑战赛成功推动了生物医学语义索引和问答领域最先进技术（state-of-the-art）的发展。

Abstract: This is an overview of the twelfth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks b and Synergy, and two
new tasks: a) MultiCardioNER on the adaptation of clinical entity detection to
the cardiology domain in a multilingual setting, and b) BIONNE on nested NER in
Russian and English. In this edition of BioASQ, 37 competing teams participated
with more than 700 distinct submissions in total for the four different shared
tasks of the challenge. Similarly to previous editions, most of the
participating systems achieved competitive performance, suggesting the
continuous advancement of the state-of-the-art in the field.

</details>


### [21] [Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20554)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Martin Krallinger,Miguel Rodríguez-Ortega,Eduard Rodriguez-López,Natalia Loukachevitch,Andrey Sakhovskiy,Elena Tutubalina,Dimitris Dimitriadis,Grigorios Tsoumakas,George Giannakoulas,Alexandra Bekiaridou,Athanasios Samaras,Giorgio Maria Di Nunzio,Nicola Ferro,Stefano Marchesin,Marco Martinelli,Gianmaria Silvello,Georgios Paliouras*

Main category: cs.CL

TL;DR: BioASQ挑战赛第十三届（CLEF 2025）概述，旨在推动生物医学语义索引和问答领域的进展，包含六个任务（两个更新，四个新增），吸引了83支团队和超过1000份提交，显示出该领域技术的持续进步。


<details>
  <summary>Details</summary>
Motivation: BioASQ挑战赛旨在促进大规模生物医学语义索引和问答领域的技术进步，本届挑战赛通过引入新的特定任务，持续推动该领域前沿技术的发展。

Method: 本届BioASQ挑战赛包含了两个原有任务的更新版本以及四个新任务：多语言临床摘要、俄语和英语嵌套命名实体链接、心脏病学临床编码，以及肠脑相互作用信息抽取。共有83支竞争团队参与，提交了超过1000份不同的参赛作品。

Result: 与往届类似，多个参赛系统取得了具有竞争力的表现。

Conclusion: BioASQ挑战赛第十三届成功地推动了生物医学语义索引和问答领域最先进技术的持续发展，参赛团队的积极参与和优秀表现证明了这一点。

Abstract: This is an overview of the thirteenth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks, b and Synergy, and four
new tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task
BioNNE-L on nested named entity linking in Russian and English. c) Task
ELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain
interplay information extraction. In this edition of BioASQ, 83 competing teams
participated with more than 1000 distinct submissions in total for the six
different shared tasks of the challenge. Similar to previous editions, several
participating systems achieved competitive performance, indicating the
continuous advancement of the state-of-the-art in the field.

</details>


### [22] [Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data](https://arxiv.org/abs/2508.20557)
*Jiahao Xiao,Jiangming Liu*

Main category: cs.CL

TL;DR: 本文提出了一套新的多领域非独立同分布（non-IID）联邦学习场景和基准，并设计了自适应联邦蒸馏（AdaFD）框架，以应对预训练语言模型在真实世界中面临的输入多样性挑战，实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦蒸馏研究主要关注标签（输出）多样性导致的非IID问题，却忽视了语言领域（输入）多样性在自然语言处理中的关键作用。真实环境中的本地数据高度异构，无法捕捉全局分布，这促使作者提出更全面的多领域非IID场景来评估联邦学习框架。

Method: 作者引入了一套全面的多领域非IID场景，并提出了一个包含多样化数据的统一基准框架，用于评估真实环境下的联邦学习。在此基础上，设计了自适应联邦蒸馏（AdaFD）框架，旨在解决同构和异构设置下的多领域非IID挑战。

Result: 实验结果表明，所提出的AdaFD模型能够有效地捕捉本地客户端的多样性，并在多领域非IID挑战下，相比现有工作取得了更好的性能。

Conclusion: AdaFD框架能够有效应对预训练语言模型在联邦学习中遇到的多领域非IID挑战，特别是在兼顾输入和输出多样性的真实NLP场景中表现出色。同时，提出的基准为评估未来联邦学习框架提供了新的标准。

Abstract: The widespread success of pre-trained language models has established a new
training paradigm, where a global PLM is fine-tuned using task-specific data
from local clients. The local data are highly different from each other and can
not capture the global distribution of the whole data in real world. To address
the challenges of non-IID data in real environments, privacy-preserving
federated distillation has been proposed and highly investigated. However,
previous experimental non-IID scenarios are primarily identified with the label
(output) diversity, without considering the diversity of language domains
(input) that is crucial in natural language processing. In this paper, we
introduce a comprehensive set of multi-domain non-IID scenarios and propose a
unified benchmarking framework that includes diverse data. The benchmark can be
used to evaluate the federated learning framework in a real environment. To
this end, we propose an Adaptive Federated Distillation (AdaFD) framework
designed to address multi-domain non-IID challenges in both homogeneous and
heterogeneous settings. Experimental results demonstrate that our models
capture the diversity of local clients and achieve better performance compared
to the existing works. The code for this paper is available at:
https://github.com/jiahaoxiao1228/AdaFD.

</details>


### [23] [Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search](https://arxiv.org/abs/2508.20559)
*Zeyu Xiong,Yixuan Nan,Li Gao,Hengzhu Tang,Shuaiqiang Wang,Junfeng Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 该研究提出了一种新的生成模型框架，通过大模型蒸馏、微调和优化技术，将轻量级模型转化为领域专家，解决了工业级实时查询驱动文本摘要（QDTS）的挑战，并显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大规模网络搜索中的查询驱动文本摘要（QDTS）对于提高用户参与度和决策效率至关重要。传统的抽取式摘要模型存在两方面局限：多阶段流水线导致信息损失和架构瓶颈；缺乏对查询和文档的足够语义理解，尤其是在处理复杂搜索意图时。

Method: 本研究提出了一种新颖的框架，首次将生成模型应用于工业网络搜索中的实时QDTS。该方法整合了大模型蒸馏、监督微调、直接偏好优化和前瞻解码技术，将一个仅有0.1B参数的轻量级模型转化为领域专用的QDTS专家。

Result: 在多项行业相关指标上，该模型超越了生产基线，并取得了新的最先进（SOTA）成果。此外，它展示了卓越的部署效率，仅需334块NVIDIA L20 GPU即可在每查询平均延迟55毫秒的情况下，处理每秒约50,000次查询。

Conclusion: 该研究成功地将生成模型应用于工业级实时查询驱动文本摘要任务，不仅在性能上超越了现有水平，达到了新的SOTA，还在部署效率方面表现出色，有效解决了传统方法的局限性。

Abstract: In the dynamic landscape of large-scale web search, Query-Driven Text
Summarization (QDTS) aims to generate concise and informative summaries from
textual documents based on a given query, which is essential for improving user
engagement and facilitating rapid decision-making. Traditional extractive
summarization models, based primarily on ranking candidate summary segments,
have been the dominant approach in industrial applications. However, these
approaches suffer from two key limitations: 1) The multi-stage pipeline often
introduces cumulative information loss and architectural bottlenecks due to its
weakest component; 2) Traditional models lack sufficient semantic understanding
of both user queries and documents, particularly when dealing with complex
search intents. In this study, we propose a novel framework to pioneer the
application of generative models to address real-time QDTS in industrial web
search. Our approach integrates large model distillation, supervised
fine-tuning, direct preference optimization, and lookahead decoding to
transform a lightweight model with only 0.1B parameters into a
domain-specialized QDTS expert. Evaluated on multiple industry-relevant
metrics, our model outperforms the production baseline and achieves a new state
of the art. Furthermore, it demonstrates excellent deployment efficiency,
requiring only 334 NVIDIA L20 GPUs to handle \textasciitilde50,000 queries per
second under 55~ms average latency per query.

</details>


### [24] [KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling](https://arxiv.org/abs/2508.20567)
*Yangfan Wang,Jie Liu,Chen Tang,Lian Yan,Jingchi Jiang*

Main category: cs.CL

TL;DR: 本文提出知识构成采样 (KCS) 框架，通过采样多样化的知识构成来增加多跳问题生成的丰富性，从而缓解数据稀疏性问题并提高问答模型性能。


<details>
  <summary>Details</summary>
Motivation: 多跳问答因数据稀疏性面临挑战，易导致语言模型学习虚假模式。现有问题生成方法过于简单，未能有效整合文档中的关键知识（如相关句子）。

Method: KCS 框架将知识构成选择建模为句子级别的条件预测任务，并利用概率对比损失来预测下一个最相关的知识片段。推理时采用随机解码策略以平衡准确性和多样性。

Result: KCS 将知识构成选择的整体准确率提高了3.9%。将其应用于数据增强后，在 HotpotQA 和 2WikiMultihopQA 数据集上均取得了性能提升。

Conclusion: KCS 是一种有效的框架，通过采样多样化的知识构成来扩展多跳问题的多样性，有助于缓解数据稀疏性问题，并提升多跳问答系统的性能。

Abstract: Multi-hop question answering faces substantial challenges due to data
sparsity, which increases the likelihood of language models learning spurious
patterns. To address this issue, prior research has focused on diversifying
question generation through content planning and varied expression. However,
these approaches often emphasize generating simple questions and neglect the
integration of essential knowledge, such as relevant sentences within
documents. This paper introduces the Knowledge Composition Sampling (KCS), an
innovative framework designed to expand the diversity of generated multi-hop
questions by sampling varied knowledge compositions within a given context. KCS
models the knowledge composition selection as a sentence-level conditional
prediction task and utilizes a probabilistic contrastive loss to predict the
next most relevant piece of knowledge. During inference, we employ a stochastic
decoding strategy to effectively balance accuracy and diversity. Compared to
competitive baselines, our KCS improves the overall accuracy of knowledge
composition selection by 3.9%, and its application for data augmentation yields
improvements on HotpotQA and 2WikiMultihopQA datasets. Our code is available
at: https://github.com/yangfanww/kcs.

</details>


### [25] [A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models](https://arxiv.org/abs/2508.20583)
*Soham Petkar,Hari Aakash K,Anirudh Vempati,Akshit Sinha,Ponnurangam Kumarauguru,Chirag Agarwal*

Main category: cs.CL

TL;DR: 当前图语言模型 (GLM) 评估基准不足以衡量多模态推理能力，单模态信息即可获得高分。本文引入 CLEGR 新基准，发现 GLM 在结构推理上表现不佳，并质疑将图结构集成到大语言模型中的架构必要性。


<details>
  <summary>Details</summary>
Motivation: 现有图语言模型 (GLM) 的评估基准主要是节点级分类数据集，无法有效评估多模态推理能力，因为单模态信息已足以获得良好性能，未能体现图-语言融合的必要性。

Method: 分析了现有 GLM 评估基准的局限性；引入了 CLEGR（Compositional Language-Graph Reasoning）基准，该基准采用合成图生成管道和需要结构与文本语义联合推理的问题；对代表性 GLM 架构进行了评估。

Result: 现有基准可通过单模态信息获得高分，不要求图-语言集成。软提示大语言模型基线与集成完整 GNN 主干的 GLM 表现相当。GLM 在需要结构推理的任务中表现出显著的性能下降。

Conclusion: 当前研究结果质疑了将图结构融入大语言模型的架构必要性，凸显了现有 GLM 在图推理能力上的局限性，并为推动社区发展明确的图结构与语言多模态推理奠定了基础。

Abstract: Developments in Graph-Language Models (GLMs) aim to integrate the structural
reasoning capabilities of Graph Neural Networks (GNNs) with the semantic
understanding of Large Language Models (LLMs). However, we demonstrate that
current evaluation benchmarks for GLMs, which are primarily repurposed
node-level classification datasets, are insufficient to assess multimodal
reasoning. Our analysis reveals that strong performance on these benchmarks is
achievable using unimodal information alone, suggesting that they do not
necessitate graph-language integration. To address this evaluation gap, we
introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed
to evaluate multimodal reasoning at various complexity levels. Our benchmark
employs a synthetic graph generation pipeline paired with questions that
require joint reasoning over structure and textual semantics. We perform a
thorough evaluation of representative GLM architectures and find that
soft-prompted LLM baselines perform on par with GLMs that incorporate a full
GNN backbone. This result calls into question the architectural necessity of
incorporating graph structure into LLMs. We further show that GLMs exhibit
significant performance degradation in tasks that require structural reasoning.
These findings highlight limitations in the graph reasoning capabilities of
current GLMs and provide a foundation for advancing the community toward
explicit multimodal reasoning involving graph structure and language.

</details>


### [26] [Generative Annotation for ASR Named Entity Correction](https://arxiv.org/abs/2508.20700)
*Yuanchang Luo,Daimeng Wei,Shaojun Li,Hengchao Shang,Jiaxin Guo,Zongyao Li,Zhanglin Wu,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Hao Yang*

Main category: cs.CL

TL;DR: ASR系统在领域特定命名实体识别上存在缺陷，现有纠错模型在错误转录词形差异大时失效。本文提出一种利用语音特征和生成方法的新型命名实体纠错模型，显著提升了实体识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端自动语音识别（ASR）系统在转录领域特定命名实体时经常失败，导致下游任务出现严重问题。尽管已有一些轻量级命名实体纠错（NEC）模型被提出，但它们主要依赖语音层面的编辑距离算法，在错误转录词与真实实体形式差异显著时，难以定位错误，从而限制了其应用。

Method: 提出一种新颖的命名实体纠错（NEC）方法。该方法首先利用语音特征来检索候选实体，然后结合语音特征和候选实体，创新性地设计了一种生成方法，用于标注ASR转录中的实体错误，并将其替换为正确的实体。

Result: 该方法在词形差异较大的场景中表现有效。通过使用开源和自建测试集进行测试，结果表明该NEC方法能显著提高实体识别的准确性。

Conclusion: 本文提出的基于语音特征和生成方法的命名实体纠错模型，有效解决了现有模型在面对词形差异大时识别失败的问题，显著提升了ASR系统在领域特定命名实体上的转录准确率。

Abstract: End-to-end automatic speech recognition systems often fail to transcribe
domain-specific named entities, causing catastrophic failures in downstream
tasks. Numerous fast and lightweight named entity correction (NEC) models have
been proposed in recent years. These models, mainly leveraging phonetic-level
edit distance algorithms, have shown impressive performances. However, when the
forms of the wrongly-transcribed words(s) and the ground-truth entity are
significantly different, these methods often fail to locate the wrongly
transcribed words in hypothesis, thus limiting their usage. We propose a novel
NEC method that utilizes speech sound features to retrieve candidate entities.
With speech sound features and candidate entities, we inovatively design a
generative method to annotate entity errors in ASR transcripts and replace the
text with correct entities. This method is effective in scenarios of word form
difference. We test our method using open-source and self-constructed test
sets. The results demonstrate that our NEC method can bring significant
improvement to entity accuracy. We will open source our self-constructed test
set and training data.

</details>


### [27] [Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning](https://arxiv.org/abs/2508.20712)
*Nelson Filipe Costa,Leila Kosseim*

Main category: cs.CL

TL;DR: 本文提出了首个用于隐式语篇关系识别（IDRR）的多语言、多标签分类模型HArch，该模型利用语篇语义间的层级依赖关系，在性能上超越了大型语言模型，并在DiscoGeM 1.0上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 针对隐式语篇关系识别（IDRR），需要一个能够处理多语言和多标签分类的模型，并且能够有效利用语篇语义间的层级依赖关系来提升识别准确性。

Method: 提出了HArch模型，该模型利用语篇语义间的层级依赖关系来预测PDTB 3.0框架下所有三个语义层级的概率分布。在DiscoGeM 2.0和DiscoGeM 1.0语料库上进行了评估，并比较了多种预训练编码器骨干网络。同时，将微调模型与使用少量样本提示（few-shot prompting）的GPT-4o和Llama-4-Maverick等大型语言模型进行了对比。

Result: RoBERTa-HArch在英语环境下表现最佳，而XLM-RoBERTa-HArch在多语言环境下表现最佳。微调模型始终优于GPT-4o和Llama-4-Maverick等大型语言模型。此外，在DiscoGeM 1.0语料库上取得了最先进（SOTA）的结果。

Conclusion: HArch模型的层级方法对隐式语篇关系识别是有效的。对于IDRR任务而言，任务特定的微调模型比使用提示的大型语言模型具有显著优势。

Abstract: This paper introduces the first multi-lingual and multi-label classification
model for implicit discourse relation recognition (IDRR). Our model, HArch, is
evaluated on the recently released DiscoGeM 2.0 corpus and leverages
hierarchical dependencies between discourse senses to predict probability
distributions across all three sense levels in the PDTB 3.0 framework. We
compare several pre-trained encoder backbones and find that RoBERTa-HArch
achieves the best performance in English, while XLM-RoBERTa-HArch performs best
in the multi-lingual setting. In addition, we compare our fine-tuned models
against GPT-4o and Llama-4-Maverick using few-shot prompting across all
language configurations. Our results show that our fine-tuned models
consistently outperform these LLMs, highlighting the advantages of
task-specific fine-tuning over prompting in IDRR. Finally, we report SOTA
results on the DiscoGeM 1.0 corpus, further validating the effectiveness of our
hierarchical approach.

</details>


### [28] [Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models](https://arxiv.org/abs/2508.20718)
*Ruiyi Yan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 本研究关注大型语言模型驱动的文本隐写和水印技术中的分词不一致性（TI）问题，发现TI源于不常见且临时的分词。为此，提出了隐写术的逐步验证法和水印的后验回滚法以消除TI，实验证明能显著提升隐写术的性能和水印的可检测性及鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型显著提升了文本生成能力，进而增强了文本隐写术的质量，同时也凸显了水印技术对抗恶意滥用的重要性。然而，在隐写和水印场景中，收发双方之间的分词不一致性（TI）会损害系统的鲁棒性，是亟待解决的问题。

Method: 本研究调查发现导致TI的问题分词具有“不常出现”和“临时性”两个关键特征。基于这些发现，提出了两种定制化的TI消除方案：针对隐写术的“逐步验证方法”和针对水印技术的“后验回滚方法”。

Result: 实验结果表明：(1) 在隐写术中，直接解决TI问题，相较于传统消歧方法，能显著提升文本的流畅性、隐蔽性和抗隐写分析能力；(2) 在水印技术中，解决TI问题能够增强水印的可检测性及抗攻击鲁棒性。

Conclusion: 解决文本隐写和水印中的分词不一致性（TI）问题对于提升系统性能至关重要。本文提出的针对性方法能够有效消除TI，从而显著提高隐写术的质量和水印技术的鲁棒性与可检测性。

Abstract: Large language models have significantly enhanced the capacities and
efficiency of text generation. On the one hand, they have improved the quality
of text-based steganography. On the other hand, they have also underscored the
importance of watermarking as a safeguard against malicious misuse. In this
study, we focus on tokenization inconsistency (TI) between Alice and Bob in
steganography and watermarking, where TI can undermine robustness. Our
investigation reveals that the problematic tokens responsible for TI exhibit
two key characteristics: infrequency and temporariness. Based on these
findings, we propose two tailored solutions for TI elimination: a stepwise
verification method for steganography and a post-hoc rollback method for
watermarking. Experiments show that (1) compared to traditional disambiguation
methods in steganography, directly addressing TI leads to improvements in
fluency, imperceptibility, and anti-steganalysis capacity; (2) for
watermarking, addressing TI enhances detectability and robustness against
attacks.

</details>


### [29] [rStar2-Agent: Agentic Reasoning Technical Report](https://arxiv.org/abs/2508.20722)
*Ning Shang,Yifei Liu,Yi Zhu,Li Lyna Zhang,Weijiang Xu,Xinyu Guan,Buze Zhang,Bingcheng Dong,Xudong Zhou,Bowen Zhang,Ying Xin,Ziming Miao,Scarlett Li,Fan Yang,Mao Yang*

Main category: cs.CL

TL;DR: rStar2-Agent是一个14B数学推理模型，通过智能体强化学习训练，在仅510步RL训练中便在AIME等基准测试上达到SOTA性能，超越了更大的模型，并展示了在复杂问题解决中自主思考、使用Python工具及从反馈中学习的高级认知行为。该成就得益于高效RL基础设施、GRPO-RoC算法和分阶段训练策略。


<details>
  <summary>Details</summary>
Motivation: 现有的长CoT（思维链）方法不足以展现高级认知行为。研究旨在开发一个能够自主思考、利用Python编码工具并从代码执行反馈中学习的模型，以探索、验证和改进复杂问题解决的中间步骤。同时，需要克服智能体强化学习在大规模应用中面临的高成本和环境噪音问题。

Method: 该研究通过三项关键创新使智能体强化学习有效且可扩展：
1.  **高效RL基础设施**：支持高吞吐量执行和可靠的Python代码环境，降低了高训练成本，使得在有限GPU资源（64 MI300X GPUs）下进行训练成为可能。
2.  **GRPO-RoC算法**：一种结合“Resample-on-Correct”策略的智能体强化学习算法，专门用于解决编码工具固有的环境噪音，从而提高模型在代码环境中的推理有效性。
3.  **高效智能体训练方案**：采用分阶段训练方法，从非推理的SFT（监督微调）开始，逐步推进到多阶段强化学习，以最小的计算成本获得高级认知能力。

Result: rStar2-Agent成功将一个预训练的14B模型提升至最先进水平，仅在不到一周内完成了510步RL训练。
1.  在AIME24上取得了80.6%的平均pass@1分数，在AIME25上取得了69.8%的平均pass@1分数，显著超越了DeepSeek-R1（671B）模型，且响应更短。
2.  模型展示了高级认知行为，如在使用Python编码工具前进行仔细思考，并能根据代码执行反馈进行反思，从而自主探索、验证和优化复杂问题解决过程中的中间步骤。
3.  除了数学推理，rStar2-Agent-14B还在对齐、科学推理和智能体工具使用任务中表现出强大的泛化能力。

Conclusion: rStar2-Agent通过结合高效的RL基础设施、创新的GRPO-RoC算法和优化的训练策略，成功地将一个14B模型训练成具备前沿数学推理能力和高级认知行为的智能体。它能在复杂问题中自主使用工具并从反馈中学习，并在多项任务上展现出强大的泛化性，证明了在有限资源下通过智能体强化学习实现高性能的可能性。

Abstract: We introduce rStar2-Agent, a 14B math reasoning model trained with agentic
reinforcement learning to achieve frontier-level performance. Beyond current
long CoT, the model demonstrates advanced cognitive behaviors, such as thinking
carefully before using Python coding tools and reflecting on code execution
feedback to autonomously explore, verify, and refine intermediate steps in
complex problem-solving. This capability is enabled through three key
innovations that makes agentic RL effective at scale: (i) an efficient RL
infrastructure with a reliable Python code environment that supports
high-throughput execution and mitigates the high rollout costs, enabling
training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic
RL algorithm with a Resample-on-Correct rollout strategy that addresses the
inherent environment noises from coding tools, allowing the model to reason
more effectively in a code environment; (iii) An efficient agent training
recipe that starts with non-reasoning SFT and progresses through multi-RL
stages, yielding advanced cognitive abilities with minimal compute cost. To
this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in
only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on
AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly
shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates
strong generalization to alignment, scientific reasoning, and agentic tool-use
tasks. Code and training recipes are available at
https://github.com/microsoft/rStar.

</details>


### [30] [Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees](https://arxiv.org/abs/2508.20736)
*Stephen Meisenbacher,Maulik Chevli,Florian Matthes*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Many works at the intersection of Differential Privacy (DP) in Natural
Language Processing aim to protect privacy by transforming texts under DP
guarantees. This can be performed in a variety of ways, from word perturbations
to full document rewriting, and most often under local DP. Here, an input text
must be made indistinguishable from any other potential text, within some bound
governed by the privacy parameter $\varepsilon$. Such a guarantee is quite
demanding, and recent works show that privatizing texts under local DP can only
be done reasonably under very high $\varepsilon$ values. Addressing this
challenge, we introduce DP-ST, which leverages semantic triples for
neighborhood-aware private document generation under local DP guarantees.
Through the evaluation of our method, we demonstrate the effectiveness of the
divide-and-conquer paradigm, particularly when limiting the DP notion (and
privacy guarantees) to that of a privatization neighborhood. When combined with
LLM post-processing, our method allows for coherent text generation even at
lower $\varepsilon$ values, while still balancing privacy and utility. These
findings highlight the importance of coherence in achieving balanced
privatization outputs at reasonable $\varepsilon$ levels.

</details>


### [31] [Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets](https://arxiv.org/abs/2508.20750)
*Vassiliy Cheremetiev,Quang Long Ho Ngo,Chau Ying Kot,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CL

TL;DR: 本研究表明，通过单独微调基于大型语言模型（LLMs）的通用嵌入模型（如Stella、Jasper、NV-Embed和E5），在隐性仇恨言论（IHS）检测任务上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 隐性仇恨言论（IHS）通过微妙的暗示、讽刺或编码术语传达偏见或仇恨，由于不包含明确的贬损或煽动性词语，因此难以检测。传统的检测方法可能需要外部知识或额外信息（如上下文、情感和情绪数据），本研究旨在探索一种更简洁但高效的解决方案。

Method: 本文采用的方法是单独微调最新的通用嵌入模型，这些模型基于大型语言模型（LLMs），具体包括Stella、Jasper、NV-Embed和E5。

Result: 实验结果显示，在多个隐性仇恨言论数据集上，该方法取得了最先进的性能。F1-macro得分在数据集内评估中提高了高达1.10个百分点，在跨数据集评估中提高了高达20.35个百分点。

Conclusion: 仅通过微调基于大型语言模型的通用嵌入模型，即可有效且显著地提升隐性仇恨言论的检测能力，并达到了当前的最先进水平。

Abstract: Implicit hate speech (IHS) is indirect language that conveys prejudice or
hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to
detect as it does not include explicit derogatory or inflammatory words. To
address this challenge, task-specific pipelines can be complemented with
external knowledge or additional information such as context, emotions and
sentiment data. In this paper, we show that, by solely fine-tuning recent
general-purpose embedding models based on large language models (LLMs), such as
Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance.
Experiments on multiple IHS datasets show up to 1.10 percentage points
improvements for in-dataset, and up to 20.35 percentage points improvements in
cross-dataset evaluation, in terms of F1-macro score.

</details>


### [32] [GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation](https://arxiv.org/abs/2508.20757)
*Yuanhao Ding,Esteban Garces Arias,Meimingwei Li,Julian Rodemann,Matthias Aßenmacher,Danlu Chen,Gaojuan Fan,Christian Heumann,Chongsheng Zhang*

Main category: cs.CL

TL;DR: GUARD是一种自适应解码方法，通过“Glocal”不确定性驱动框架平衡LLM文本生成中的连贯性和多样性，同时显著提高生成速度。


<details>
  <summary>Details</summary>
Motivation: 开放式文本生成面临在LLM输出中平衡连贯性与多样性的关键挑战。现有的基于对比搜索的解码策略因超参数依赖和高计算成本而实用性受限。

Method: 本文引入GUARD，一种自适应解码方法，通过新颖的“Glocal”不确定性驱动框架有效平衡连贯性和多样性。GUARD结合全局熵估计与局部熵偏差，整合长期和短期不确定性信号。为减少计算开销，GUARD还引入了一种简单有效的基于token计数的惩罚机制。

Result: 实验结果表明，GUARD在文本多样性和连贯性之间取得了良好平衡，同时在生成速度上也有显著提升。在文本质量不同维度的细致比较研究中，人类和LLM评估器均验证了其卓越性能。

Conclusion: GUARD通过其自适应的“Glocal”不确定性驱动框架，成功解决了LLM开放式文本生成中连贯性与多样性的平衡难题，同时克服了现有方法的计算效率和超参数依赖问题，提供了更高质量和更快速的生成体验。

Abstract: Open-ended text generation faces a critical challenge: balancing coherence
with diversity in LLM outputs. While contrastive search-based decoding
strategies have emerged to address this trade-off, their practical utility is
often limited by hyperparameter dependence and high computational costs. We
introduce GUARD, a self-adaptive decoding method that effectively balances
these competing objectives through a novel "Glocal" uncertainty-driven
framework. GUARD combines global entropy estimates with local entropy
deviations to integrate both long-term and short-term uncertainty signals. We
demonstrate that our proposed global entropy formulation effectively mitigates
abrupt variations in uncertainty, such as sudden overconfidence or high entropy
spikes, and provides theoretical guarantees of unbiasedness and consistency. To
reduce computational overhead, we incorporate a simple yet effective
token-count-based penalty into GUARD. Experimental results demonstrate that
GUARD achieves a good balance between text diversity and coherence, while
exhibiting substantial improvements in generation speed. In a more nuanced
comparison study across different dimensions of text quality, both human and
LLM evaluators validated its remarkable performance. Our code is available at
https://github.com/YecanLee/GUARD.

</details>


### [33] [Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions](https://arxiv.org/abs/2508.20764)
*Xiaoyi Wang,Jiwei Zhang,Guangtao Zhang,Honglei Guo*

Main category: cs.CL

TL;DR: 研究发现，大语言模型（LLM）生成的治疗对话在情感动态上与真实对话存在显著差异，情感保真度不足。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM生成的治疗对话被广泛用于心理健康NLP，但其是否能捕捉真实治疗中细微的情感动态尚不明确。

Method: 首次对真实与LLM生成的认知行为疗法对话的情感弧线进行比较分析。采用“话语情感动态”框架，在效价、唤醒和支配维度上分析情感轨迹，涵盖完整对话和个体角色，数据源自公共视频转录的真实会话和CACTUS数据集的合成对话。

Result: 合成对话虽然流畅连贯，但在关键情感属性上与真实对话存在差异：真实会话表现出更高的情感变异性、更多情感丰富的语言以及更真实的反应和调节模式。真实与合成说话者之间的情感弧线相似度较低，特别是对于来访者而言。

Conclusion: 当前LLM生成的治疗数据在情感保真度方面存在局限性，强调了情感保真度在心理健康应用中的重要性。本研究引入了RealCBT数据集以支持未来研究。

Abstract: Synthetic therapy dialogues generated by large language models (LLMs) are
increasingly used in mental health NLP to simulate counseling scenarios, train
models, and supplement limited real-world data. However, it remains unclear
whether these synthetic conversations capture the nuanced emotional dynamics of
real therapy. In this work, we conduct the first comparative analysis of
emotional arcs between real and LLM-generated Cognitive Behavioral Therapy
dialogues. We adapt the Utterance Emotion Dynamics framework to analyze
fine-grained affective trajectories across valence, arousal, and dominance
dimensions. Our analysis spans both full dialogues and individual speaker roles
(counselor and client), using real sessions transcribed from public videos and
synthetic dialogues from the CACTUS dataset. We find that while synthetic
dialogues are fluent and structurally coherent, they diverge from real
conversations in key emotional properties: real sessions exhibit greater
emotional variability,more emotion-laden language, and more authentic patterns
of reactivity and regulation. Moreover, emotional arc similarity between real
and synthetic speakers is low, especially for clients. These findings
underscore the limitations of current LLM-generated therapy data and highlight
the importance of emotional fidelity in mental health applications. We
introduce RealCBT, a curated dataset of real CBT sessions, to support future
research in this space.

</details>


### [34] [Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection](https://arxiv.org/abs/2508.20766)
*Harethah Abu Shairah,Hasan Abed Al Kader Hammoud,George Turkiyyah,Bernard Ghanem*

Main category: cs.CL

TL;DR: 本文提出一种名为ROSI（Rank-One Safety Injection）的白盒方法，通过简单的秩一权重修改永久性地引导LLM的内部激活朝向拒绝有害请求的子空间，从而提高模型安全性，同时保持实用性，甚至能重新校准“未审查”模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全机制可能通过操纵内部表示被绕过，因此需要一种更鲁棒且高效的方法来增强模型的安全对齐。

Method: ROSI是一种白盒方法，通过对所有残差流写入矩阵应用简单、无需微调的秩一权重修改来工作。它通过将模型激活永久性地引导至拒绝介导子空间来增强模型的安全对齐。所需的安全性方向可通过少量有害和无害指令对计算得到。

Result: ROSI持续提高了安全拒绝率（通过Llama Guard 3评估），同时在MMLU、HellaSwag和Arc等标准基准测试中保持了模型的实用性。此外，ROSI还能通过放大其自身的潜在安全方向来重新校准“未审查”模型，证明其作为有效的“最后一公里”安全程序的价值。

Conclusion: 定向的、可解释的权重引导是一种廉价而有效的方法，可以提高LLM的安全性，是对更资源密集型微调范式的补充。

Abstract: Safety alignment in Large Language Models (LLMs) often involves mediating
internal representations to refuse harmful requests. Recent research has
demonstrated that these safety mechanisms can be bypassed by ablating or
removing specific representational directions within the model. In this paper,
we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box
method that amplifies a model's safety alignment by permanently steering its
activations toward the refusal-mediating subspace. ROSI operates as a simple,
fine-tuning-free rank-one weight modification applied to all residual stream
write matrices. The required safety direction can be computed from a small set
of harmful and harmless instruction pairs. We show that ROSI consistently
increases safety refusal rates - as evaluated by Llama Guard 3 - while
preserving the utility of the model on standard benchmarks such as MMLU,
HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align
'uncensored' models by amplifying their own latent safety directions,
demonstrating its utility as an effective last-mile safety procedure. Our
results suggest that targeted, interpretable weight steering is a cheap and
potent mechanism to improve LLM safety, complementing more resource-intensive
fine-tuning paradigms.

</details>


### [35] [Signs of Struggle: Spotting Cognitive Distortions across Language and Register](https://arxiv.org/abs/2508.20771)
*Abhishek Kuber,Enrico Liscio,Ruixuan Zhang,Caroline Figueroa,Pradeep K. Murukannaiah*

Main category: cs.CL

TL;DR: 该研究首次深入探讨了荷兰青少年论坛文本中认知扭曲检测的跨语言和跨语域泛化问题，发现领域适应方法潜力巨大。


<details>
  <summary>Details</summary>
Motivation: 鉴于青少年心理健康问题日益突出，需要自动化方法检测心理困扰的早期迹象，特别是那些加剧精神痛苦的认知扭曲，以便及时、低成本地进行干预。现有研究多集中于英语临床数据。

Method: 本研究首次深入探究了认知扭曲检测的跨语言和跨语域泛化能力，分析了由荷兰青少年撰写的论坛帖子。

Result: 研究发现，语言和写作风格的变化会显著影响模型性能，但领域适应方法展现出最大的潜力。

Conclusion: 领域适应是解决跨语言、跨语域认知扭曲检测性能挑战的有效且有前景的方法。

Abstract: Rising mental health issues among youth have increased interest in automated
approaches for detecting early signs of psychological distress in digital text.
One key focus is the identification of cognitive distortions, irrational
thought patterns that have a role in aggravating mental distress. Early
detection of these distortions may enable timely, low-cost interventions. While
prior work has focused on English clinical data, we present the first in-depth
study of cross-lingual and cross-register generalization of cognitive
distortion detection, analyzing forum posts written by Dutch adolescents. Our
findings show that while changes in language and writing style can
significantly affect model performance, domain adaptation methods show the most
promise.

</details>


### [36] [Exploring Machine Learning and Language Models for Multimodal Depression Detection](https://arxiv.org/abs/2508.20805)
*Javier Si Zhao Hong,Timothy Zoe Delaya,Sherwyn Chan Yin Kit,Pai Chet Ng,Xiaoxiao Miao*

Main category: cs.CL

TL;DR: 本论文提出了一种用于多模态抑郁症检测的方法，比较了XGBoost、Transformer和大型语言模型在音视频及文本特征上的表现，并分析了各模型的优缺点，以提供有效多模态表征策略的见解。


<details>
  <summary>Details</summary>
Motivation: 参与首届多模态个性感知抑郁症检测挑战，旨在利用机器学习和深度学习模型实现多模态抑郁症检测。

Method: 探索并比较了XGBoost、基于Transformer的架构和大型语言模型（LLMs）在音频、视频和文本特征上的性能。

Result: 研究结果揭示了各类模型在跨模态捕获抑郁症相关信号方面的优势和局限性。

Conclusion: 为心理健康预测中有效的多模态表征策略提供了深入见解。

Abstract: This paper presents our approach to the first Multimodal Personality-Aware
Depression Detection Challenge, focusing on multimodal depression detection
using machine learning and deep learning models. We explore and compare the
performance of XGBoost, transformer-based architectures, and large language
models (LLMs) on audio, video, and text features. Our results highlight the
strengths and limitations of each type of model in capturing depression-related
signals across modalities, offering insights into effective multimodal
representation strategies for mental health prediction.

</details>


### [37] [GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction](https://arxiv.org/abs/2508.20828)
*Jie Zhao,Wanting Ning,Yuxiao Fei,Yubo Feng,Lishuang Li*

Main category: cs.CL

TL;DR: 本文提出GDLLM框架，结合图注意力网络和软推理机制，解决大小语言模型在事件时间关系抽取中处理少数类关系和长距离依赖的不足，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究中，小语言模型(SLMs)因预训练知识有限，难以处理不平衡数据集中少数类关系。大语言模型(LLMs)通过手动提示使用时，引入噪声，干扰对事件长距离依赖的判断。

Method: 提出GDLLM，一种基于LLM的全局距离感知建模方法。首先，利用图注意力网络(GAT)构建距离感知图结构，辅助LLM捕捉长距离依赖特征。其次，设计基于软推理的时间特征学习范式，增强短距离接近带关系的识别，并将LLM生成的概率信息融入多头注意力机制。

Result: 通过有效捕捉全局特征，GDLLM显著提升了少数类关系的性能，并提高了整体学习能力。在TB-Dense和MATRES两个公开数据集上的实验表明，该方法取得了最先进(SOTA)的性能。

Conclusion: GDLLM框架通过结合距离感知图结构和软推理机制，有效解决了现有语言模型在事件时间关系抽取中处理少数类关系和长距离依赖的挑战，并取得了最先进的性能。

Abstract: In Natural Language Processing(NLP), Event Temporal Relation Extraction
(ETRE) is to recognize the temporal relations of two events. Prior studies have
noted the importance of language models for ETRE. However, the restricted
pre-trained knowledge of Small Language Models(SLMs) limits their capability to
handle minority class relations in imbalanced classification datasets. For
Large Language Models(LLMs), researchers adopt manually designed prompts or
instructions, which may introduce extra noise, leading to interference with the
model's judgment of the long-distance dependencies between events. To address
these issues, we propose GDLLM, a Global Distance-aware modeling approach based
on LLMs. We first present a distance-aware graph structure utilizing Graph
Attention Network(GAT) to assist the LLMs in capturing long-distance dependency
features. Additionally, we design a temporal feature learning paradigm based on
soft inference to augment the identification of relations with a short-distance
proximity band, which supplements the probabilistic information generated by
LLMs into the multi-head attention mechanism. Since the global feature can be
captured effectively, our framework substantially enhances the performance of
minority relation classes and improves the overall learning ability.
Experiments on two publicly available datasets, TB-Dense and MATRES,
demonstrate that our approach achieves state-of-the-art (SOTA) performance.

</details>


### [38] [MSRS: Evaluating Multi-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2508.20867)
*Rohan Phanse,Yijie Zhou,Kejian Shi,Wencai Zhang,Yixin Liu,Yilun Zhao,Arman Cohan*

Main category: cs.CL

TL;DR: 现有RAG系统在处理多源信息整合和长篇响应生成方面存在局限性。本文提出了一个可扩展的评估基准框架，并构建了MSRS-Story和MSRS-Meet两个新基准，用于挑战RAG系统进行多源检索与合成。研究发现，生成质量高度依赖检索效果，且推理模型在多源合成方面显著优于标准大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统通常在单源、短篇或事实性问答场景下进行评估，无法满足真实世界应用中整合和总结分散于多个来源的信息、生成长篇响应的需求。这要求RAG的检索组件能识别多种相关性信号，生成组件能连接和合成跨源信息。

Method: 开发了一个可扩展的框架，用于构建评估RAG系统整合跨源信息和生成长篇响应能力的基准。基于此框架，构建了两个新的多源检索与合成基准：MSRS-Story（叙事合成）和MSRS-Meet（会议总结），它们都要求从大型集合中进行检索。使用稀疏和密集检索器结合前沿大型语言模型，对各种RAG管道进行了广泛实验。

Result: 实验揭示，生成质量高度依赖于检索效率，且检索效率因任务而异。即使在理想检索（oracle retrieval）设置下，多源合成仍具挑战性。但研究发现，推理模型在多源合成这一特定步骤上显著优于标准大型语言模型。

Conclusion: RAG系统在多源信息整合和长篇响应生成方面面临显著挑战。基准测试显示检索效果对生成质量至关重要，且推理模型在跨源信息合成任务中展现出优越性能，为未来RAG发展指明了方向。

Abstract: Retrieval-augmented systems are typically evaluated in settings where
information required to answer the query can be found within a single source or
the answer is short-form or factoid-based. However, many real-world
applications demand the ability to integrate and summarize information
scattered across multiple sources, where no single source is sufficient to
respond to the user's question. In such settings, the retrieval component of a
RAG pipeline must recognize a variety of relevance signals, and the generation
component must connect and synthesize information across multiple sources. We
present a scalable framework for constructing evaluation benchmarks that
challenge RAG systems to integrate information across distinct sources and
generate long-form responses. Using our framework, we build two new benchmarks
on Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing
narrative synthesis and summarization tasks, respectively, that require
retrieval from large collections. Our extensive experiments with various RAG
pipelines -- including sparse and dense retrievers combined with frontier LLMs
-- reveal that generation quality is highly dependent on retrieval
effectiveness, which varies greatly by task. While multi-source synthesis
proves challenging even in an oracle retrieval setting, we find that reasoning
models significantly outperform standard LLMs at this distinct step.

</details>


### [39] [The Uneven Impact of Post-Training Quantization in Machine Translation](https://arxiv.org/abs/2508.20893)
*Benjamin Marie,Atsushi Fujita*

Main category: cs.CL

TL;DR: 首次大规模评估了后训练量化（PTQ）对55种语言LLM机器翻译的影响，发现4位量化对高资源语言和大型模型表现良好，但对低资源语言和2位量化下性能显著下降。GGUF技术表现最稳定，且语言匹配校准在低位场景有益。


<details>
  <summary>Details</summary>
Motivation: 量化对于在资源受限硬件上部署大型语言模型（LLMs）至关重要，但其对多语言任务的影响尚未得到充分探索。

Method: 对五种1.7B至70B参数的LLMs在55种语言的机器翻译任务上进行了大规模后训练量化（PTQ）评估，比较了AWQ、BitsAndBytes、GGUF和AutoRound四种量化技术，并量化了量化、解码超参数及校准语言的相互作用。

Result: ['4位量化通常能保持高资源语言和大型模型的翻译质量，但在2位设置下，低资源和类型多样的语言出现显著性能下降。', '算法选择和模型大小共同决定了量化鲁棒性。', 'GGUF变体即使在2位精度下也能提供最一致的性能。', '语言匹配校准主要在低位场景中提供益处。']

Conclusion: 研究结果为在量化约束下（尤其是在低资源设置中）部署多语言LLM进行机器翻译提供了可操作的见解。

Abstract: Quantization is essential for deploying large language models (LLMs) on
resource-constrained hardware, but its implications for multilingual tasks
remain underexplored. We conduct the first large-scale evaluation of
post-training quantization (PTQ) on machine translation across 55 languages
using five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that
while 4-bit quantization often preserves translation quality for high-resource
languages and large models, significant degradation occurs for low-resource and
typologically diverse languages, particularly in 2-bit settings. We compare
four quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing
that algorithm choice and model size jointly determine robustness. GGUF
variants provide the most consistent performance, even at 2-bit precision.
Additionally, we quantify the interactions between quantization, decoding
hyperparameters, and calibration languages, finding that language-matched
calibration offers benefits primarily in low-bit scenarios. Our findings offer
actionable insights for deploying multilingual LLMs for machine translation
under quantization constraints, especially in low-resource settings.

</details>


### [40] [SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement](https://arxiv.org/abs/2508.20916)
*Yuan Ge,Junxiang Zhang,Xiaoqian Liu,Bei Li,Xiangnan Ma,Chenglong Wang,Kaiyang Ye,Yangfan Du,Linfeng Zhang,Yuxin Huang,Tong Xiao,Zhengtao Yu,JingBo Zhu*

Main category: cs.CL

TL;DR: SageLM 是一个端到端、多维度、可解释的语音大型语言模型，专门用于全面评估 Speech-to-Speech (S2S) LLM。


<details>
  <summary>Details</summary>
Motivation: Speech-to-Speech (S2S) 大型语言模型对于自然人机交互至关重要，但评估这些模型仍然是一个根本性挑战。

Method: 提出了 SageLM 模型，其特点是：1) 联合评估语义和声学维度；2) 利用基于理由的监督来增强可解释性并指导模型学习；3) 引入合成偏好数据集 SpeechFeedback 并采用两阶段训练范式，以缓解语音偏好数据稀缺问题。

Result: SageLM 与人类评估者的一致性达到 82.79%，分别比级联基线和基于 SLM 的基线高出至少 7.42% 和 26.20%。

Conclusion: SageLM 通过其端到端、多维度和可解释的评估方法，有效解决了 S2S LLM 的评估难题，并取得了与人类评估者高度一致的优异性能。

Abstract: Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling end-to-end spoken dialogue
systems. However, evaluating these models remains a fundamental challenge. We
propose \texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech
LLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches
that disregard acoustic features, SageLM jointly assesses both semantic and
acoustic dimensions. Second, it leverages rationale-based supervision to
enhance explainability and guide model learning, achieving superior alignment
with evaluation outcomes compared to rule-based reinforcement learning methods.
Third, we introduce \textit{SpeechFeedback}, a synthetic preference dataset,
and employ a two-stage training paradigm to mitigate the scarcity of speech
preference data. Trained on both semantic and acoustic dimensions, SageLM
achieves an 82.79\% agreement rate with human evaluators, outperforming
cascaded and SLM-based baselines by at least 7.42\% and 26.20\%, respectively.

</details>


### [41] [How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $τ$-bench](https://arxiv.org/abs/2508.20931)
*Venkatesh Mishra,Amir Saeidi,Satyam Raj,Mutsumi Nakamura,Jayanth Srinivasa,Gaowen Liu,Ali Payani,Chitta Baral*

Main category: cs.CL

TL;DR: 大型语言模型代理在多轮对话中面临推理不一致和信息提取困难。本文通过错误分析，提出IRMA框架，自动重构查询并增加领域规则和工具建议，显著提高了代理在动态环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在推理和规划方面有所进步，但在多轮对话环境中，LLM代理在一致性推理、遵守领域特定策略以及从长期工具调用和对话中提取正确信息方面表现不佳。

Method: 首先，对对话轨迹中常见的错误进行全面的手动分析。其次，通过重新组织输入信息给工具调用代理，以改善其决策。最后，提出Input-Reformulation Multi-Agent (IRMA) 框架，该框架自动重构用户查询，并辅以相关的领域规则和工具建议，以帮助工具调用代理集中注意力。

Result: IRMA在整体pass^5分数上显著优于ReAct、Function Calling和Self-Reflection，分别提高了16.1%、12.7%和19.1%。

Conclusion: 研究结果表明，与现有方法相比，IRMA在动态环境中展现出卓越的可靠性和一致性。

Abstract: Recent advances in reasoning and planning capabilities of large language
models (LLMs) have enabled their potential as autonomous agents capable of tool
use in dynamic environments. However, in multi-turn conversational environments
like $\tau$-bench, these agents often struggle with consistent reasoning,
adherence to domain-specific policies, and extracting correct information over
a long horizon of tool-calls and conversation. To capture and mitigate these
failures, we conduct a comprehensive manual analysis of the common errors
occurring in the conversation trajectories. We then experiment with
reformulations of inputs to the tool-calling agent for improvement in agent
decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)
framework, which automatically reformulates user queries augmented with
relevant domain rules and tool suggestions for the tool-calling agent to focus
on. The results show that IRMA significantly outperforms ReAct, Function
Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in
overall pass^5 scores. These findings highlight the superior reliability and
consistency of IRMA compared to other methods in dynamic environments.

</details>


### [42] [STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment](https://arxiv.org/abs/2508.20944)
*Jiaqian Li,Qisheng Hu,Jing Li,Wenya Wang*

Main category: cs.CL

TL;DR: 本文提出一种新颖的两阶段范例选择策略，通过结构感知监督和即插即用模块，显著提升了大型语言模型在语义解析等结构化预测任务中上下文学习的性能。


<details>
  <summary>Details</summary>
Motivation: 上下文学习（ICL）的效果严重依赖于范例选择的质量。对于语义解析等结构化预测任务，现有选择策略常忽略结构对齐，导致性能不佳和泛化能力弱。

Method: 该方法包含两阶段：1. 使用结构感知监督微调一个BERT检索器，使其选择语义相关且结构对齐的范例。2. 通过一个模型无关的即插即用模块增强检索器，放大隐藏表示中的句法有意义信息。

Result: 在涵盖三种语义解析任务的四个基准测试中，使用多种LLM作为推理模型，该方法持续优于现有基线。

Conclusion: 所提出的两阶段范例选择策略在效率、泛化性和性能之间取得了良好平衡，有效解决了结构化预测任务中ICL范例选择的结构对齐问题，并显著提高了性能。

Abstract: In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to
perform a wide range of tasks without task-specific fine-tuning. However, the
effectiveness of ICL heavily depends on the quality of exemplar selection. In
particular, for structured prediction tasks such as semantic parsing, existing
ICL selection strategies often overlook structural alignment, leading to
suboptimal performance and poor generalization. To address this issue, we
propose a novel two-stage exemplar selection strategy that achieves a strong
balance between efficiency, generalizability, and performance. First, we
fine-tune a BERT-based retriever using structure-aware supervision, guiding it
to select exemplars that are both semantically relevant and structurally
aligned. Then, we enhance the retriever with a plug-in module, which amplifies
syntactically meaningful information in the hidden representations. This
plug-in is model-agnostic, requires minimal overhead, and can be seamlessly
integrated into existing pipelines. Experiments on four benchmarks spanning
three semantic parsing tasks demonstrate that our method consistently
outperforms existing baselines with multiple recent LLMs as inference-time
models.

</details>


### [43] [ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents](https://arxiv.org/abs/2508.20973)
*Tianjian Liu,Fanqi Wan,Jiajian Guo,Xiaojun Quan*

Main category: cs.CL

TL;DR: 本文提出了ProactiveEval统一框架，用于全面评估LLM的主动对话能力，将其分解为目标规划和对话引导，并发现DeepSeek-R1和Claude-3.7-Sonnet在特定任务上表现突出，同时探讨了推理能力的影响。


<details>
  <summary>Details</summary>
Motivation: 现有主动对话评估工作主要集中于特定领域或任务，导致评估碎片化，限制了对LLM主动对话能力的全面探索和理解。

Method: 提出了ProactiveEval统一评估框架，该框架将主动对话分解为目标规划和对话引导，建立了跨域评估指标，并支持自动生成多样化的评估数据。基于此框架，构建了6个领域的328个评估环境，并对22种不同类型的LLM进行了实验。

Result: 实验结果表明，DeepSeek-R1在目标规划任务上表现卓越，而Claude-3.7-Sonnet在对话引导任务上表现出色。研究还分析了推理能力如何影响LLM的主动行为。

Conclusion: 研究揭示了推理能力对LLM主动行为的关键影响，为未来LLM模型在主动对话能力方面的开发提供了重要的指导和启示。

Abstract: Proactive dialogue has emerged as a critical and challenging research problem
in advancing large language models (LLMs). Existing works predominantly focus
on domain-specific or task-oriented scenarios, which leads to fragmented
evaluations and limits the comprehensive exploration of models' proactive
conversation abilities. In this work, we propose ProactiveEval, a unified
framework designed for evaluating proactive dialogue capabilities of LLMs. This
framework decomposes proactive dialogue into target planning and dialogue
guidance, establishing evaluation metrics across various domains. Moreover, it
also enables the automatic generation of diverse and challenging evaluation
data. Based on the proposed framework, we develop 328 evaluation environments
spanning 6 distinct domains. Through experiments with 22 different types of
LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional
performance on target planning and dialogue guidance tasks, respectively.
Finally, we investigate how reasoning capabilities influence proactive
behaviors and discuss their implications for future model development.

</details>


### [44] [Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution](https://arxiv.org/abs/2508.21004)
*Chen Chen,Yuchen Sun,Jiaxin Gao,Xueluan Gong,Qian Wang,Ziyao Wang,Yongsen Zheng,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: 本文提出LETHE，一种通过内部模型合并和外部提示增强进行知识稀释的新方法，以消除大型语言模型中的后门行为。LETHE在对抗多种高级后门攻击方面表现优异，同时保持模型实用性、成本效益和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）容易受到后门攻击，现有防御机制不足以应对复杂的攻击场景（如基于模型编辑、多触发和无触发的攻击），因此需要一种更全面、更鲁棒的防御方法。

Method: LETHE采用知识稀释方法，结合内部和外部机制。内部机制通过轻量级数据集训练一个干净模型，并将其与带后门的模型合并，以稀释后门在模型参数记忆中的影响。外部机制则在提示中加入良性且语义相关的证据，以分散LLM对后门特征的注意力。

Result: 实验结果表明，LETHE在分类和生成领域，针对8种后门攻击，在5个常用LLM上优于8种现有防御基线。它能将高级后门攻击的成功率降低高达98%，同时保持模型实用性。此外，LETHE成本效益高，并对自适应后门攻击具有鲁棒性。

Conclusion: LETHE是一种有效、全面、成本高效且鲁棒的防御机制，能够成功消除大型语言模型中的后门行为，优于现有方法并解决了其在对抗高级攻击方面的局限性。

Abstract: Large language models (LLMs) have seen significant advancements, achieving
superior performance in various Natural Language Processing (NLP) tasks.
However, they remain vulnerable to backdoor attacks, where models behave
normally for standard queries but generate harmful responses or unintended
output when specific triggers are activated. Existing backdoor defenses either
lack comprehensiveness, focusing on narrow trigger settings, detection-only
mechanisms, and limited domains, or fail to withstand advanced scenarios like
model-editing-based, multi-trigger, and triggerless attacks. In this paper, we
present LETHE, a novel method to eliminate backdoor behaviors from LLMs through
knowledge dilution using both internal and external mechanisms. Internally,
LETHE leverages a lightweight dataset to train a clean model, which is then
merged with the backdoored model to neutralize malicious behaviors by diluting
the backdoor impact within the model's parametric memory. Externally, LETHE
incorporates benign and semantically relevant evidence into the prompt to
distract LLM's attention from backdoor features. Experimental results on
classification and generation domains across 5 widely used LLMs demonstrate
that LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor
attacks. LETHE reduces the attack success rate of advanced backdoor attacks by
up to 98% while maintaining model utility. Furthermore, LETHE has proven to be
cost-efficient and robust against adaptive backdoor attacks.

</details>


### [45] [An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs](https://arxiv.org/abs/2508.21024)
*Mathieu Bourdin,Anas Neumann,Thomas Paviot,Robert Pellerin,Samir Lamouri*

Main category: cs.CL

TL;DR: 本论文提出EASI-RAG，一种结构化敏捷方法，旨在帮助资源有限的中小企业(SMEs)快速有效地部署RAG系统，并通过真实案例验证其在快速实施、高用户采纳和提供准确答案方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG是解决LLM局限性的强大方案，但中小企业因资源和NLP专业知识的限制，在部署RAG工具时面临挑战。

Method: 引入EASI-RAG（Enterprise Application Support for Industrial RAG），这是一种基于方法工程原则、包含明确角色、活动和技术的结构化敏捷方法，专门设计用于促进RAG系统在工业SME环境中的部署。

Result: 该方法通过在环境测试实验室的真实案例研究得到验证。结果表明，EASI-RAG支持在一个月内部署，由无RAG经验的团队实现，并带来高用户采纳率、提供准确答案，同时增强了底层数据的可靠性。

Conclusion: 这项工作突出了RAG在工业中小企业中部署的巨大潜力。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to
mitigate the limitations of Large Language Models (LLMs), such as
hallucinations and outdated knowledge. However, deploying RAG-based tools in
Small and Medium Enterprises (SMEs) remains a challenge due to their limited
resources and lack of expertise in natural language processing (NLP). This
paper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a
structured, agile method designed to facilitate the deployment of RAG systems
in industrial SME contexts. EASI-RAG is based on method engineering principles
and comprises well-defined roles, activities, and techniques. The method was
validated through a real-world case study in an environmental testing
laboratory, where a RAG tool was implemented to answer operators queries using
data extracted from operational procedures. The system was deployed in under a
month by a team with no prior RAG experience and was later iteratively improved
based on user feedback. Results demonstrate that EASI-RAG supports fast
implementation, high user adoption, delivers accurate answers, and enhances the
reliability of underlying data. This work highlights the potential of RAG
deployment in industrial SMEs. Future works include the need for generalization
across diverse use cases and further integration with fine-tuned models.

</details>


### [46] [Re-Representation in Sentential Relation Extraction with Sequence Routing Algorithm](https://arxiv.org/abs/2508.21049)
*Ramazan Ali Bahrami,Ramin Yahyapour*

Main category: cs.CL

TL;DR: 本文提出了一种利用胶囊网络动态路由进行句子关系抽取的方法，并在多个常用数据集上超越了现有最佳方法。研究发现其性能差异源于标签噪声和更好的“re-representation”能力，并提出“re-representation”是该领域的一个挑战。


<details>
  <summary>Details</summary>
Motivation: 句子关系抽取（RE）是自然语言处理（NLP）中的一项重要任务。

Method: 本文提出并使用了基于动态路由的胶囊网络进行句子关系抽取。

Result: 1. 所提出的方法在Tacred, Tacredrev, Retacred和Conll04等常用句子关系抽取数据集上超越了现有最先进水平。
2. 对该方法在上述数据集表现良好但在Wikidata数据集表现不佳的原因进行了调查，发现Wikidata标签中的噪声是影响性能的原因之一。
3. 研究表明，更好的“re-representation”（重表达）能力与更好的性能相关，并证明所提出的模型比传统模型能更好地执行“re-representation”。

Conclusion: 除了远距离监督关系抽取数据集标签中的噪声，本文将“re-representation”能力提出为句子关系抽取领域的一个新挑战。

Abstract: Sentential relation extraction (RE) is an important task in natural language
processing (NLP). In this paper we propose to do sentential RE with dynamic
routing in capsules. We first show that the proposed approach outperform state
of the art on common sentential relation extraction datasets Tacred, Tacredrev,
Retacred, and Conll04. We then investigate potential reasons for its good
performance on the mentioned datasets, and yet low performance on another
similar, yet larger sentential RE dataset, Wikidata. As such, we identify noise
in Wikidata labels as one of the reasons that can hinder performance.
Additionally, we show associativity of better performance with better
re-representation, a term from neuroscience referred to change of
representation in human brain to improve the match at comparison time. As
example, in the given analogous terms King:Queen::Man:Woman, at comparison
time, and as a result of re-representation, the similarity between related head
terms (King,Man), and tail terms (Queen,Woman) increases. As such, our
observation show that our proposed model can do re-representation better than
the vanilla model compared with. To that end, beside noise in the labels of the
distantly supervised RE datasets, we propose re-representation as a challenge
in sentential RE.

</details>


### [47] [Enabling Equitable Access to Trustworthy Financial Reasoning](https://arxiv.org/abs/2508.21051)
*William Jurayj,Nils Holzenberger,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 针对报税的复杂性和高准确性要求，提出了一种结合大型语言模型（LLM）与符号求解器的神经符号方法，通过逻辑程序转换和案例检索显著提高了报税性能并降低了成本。


<details>
  <summary>Details</summary>
Motivation: 报税过程复杂、耗时且错误可能导致高额罚款。现有大型语言模型（LLMs）因难以保证高精度和可审计性而不适用于此任务，因此需要一种高准确度、可审计的自动化税务系统。

Method: 提出一种将LLMs与符号求解器集成的神经符号方法来计算税务义务。该方法包括将纯文本规则预先翻译成形式逻辑程序，并结合智能检索的形式化案例表示。系统在SARA数据集上进行评估，并引入基于真实税务错误罚款的部署成本估算方法。

Result: 通过结合预先规则翻译和智能案例检索，显著提高了报税任务的性能，并将系统部署成本降低到远低于真实世界平均水平。

Conclusion: 研究结果表明，神经符号架构在提高可靠税务援助的公平可及性方面具有前景和经济可行性。

Abstract: According to the United States Internal Revenue Service, ''the average
American spends $\$270$ and 13 hours filing their taxes''. Even beyond the
U.S., tax filing requires complex reasoning, combining application of
overlapping rules with numerical calculations. Because errors can incur costly
penalties, any automated system must deliver high accuracy and auditability,
making modern large language models (LLMs) poorly suited for this task. We
propose an approach that integrates LLMs with a symbolic solver to calculate
tax obligations. We evaluate variants of this system on the challenging
StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for
estimating the cost of deploying such a system based on real-world penalties
for tax errors. We further show how combining up-front translation of
plain-text rules into formal logic programs, combined with intelligently
retrieved exemplars for formal case representations, can dramatically improve
performance on this task and reduce costs to well below real-world averages.
Our results demonstrate the promise and economic feasibility of neuro-symbolic
architectures for increasing equitable access to reliable tax assistance.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [48] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: 本文提出CHAIR-DPO方法，利用CHAIR指标构建偏好数据，通过DPO微调多模态大语言模型（MLLMs），有效减少了模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）存在生成与视觉输入不符内容的“幻觉”问题，这一长期存在的缺陷阻碍了其广泛应用，因此需要开发有效方法来解决此问题。

Method: 研究将幻觉问题视为对齐问题。与依赖复杂合成偏好数据的方法不同，本文利用CHAIR指标（最初用于图像描述幻觉评估），根据生成的答案对，区分出无幻觉的“优胜者”和有幻觉的“失败者”样本。随后，通过直接偏好优化（DPO）对现成的MLLM进行微调，该方法被命名为CHAIR-DPO。

Result: CHAIR-DPO方法在多个幻觉基准测试上，显著减少了MLLM生成答案中的幻觉数量。

Conclusion: 研究结果表明，利用基于CHAIR指标的奖励机制对MLLM进行微调，是有效减少其幻觉问题的策略。

Abstract: Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [49] [SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization](https://arxiv.org/abs/2508.20182)
*Yang Su,Shunquan Tan,Jiwu Huang*

Main category: cs.CV

TL;DR: 本文首次将Stable Diffusion（SD3）的多模态能力整合到图像伪造定位框架中，通过将伪造残差作为显式模态融入潜在空间，实现了更高效、更准确的定位，并在基准数据集和真实世界伪造图像上表现出色。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型（如Stable Diffusion）的快速发展推动了图像处理技术进步，但现有图像伪造定位方法因过度依赖耗时昂贵的标注数据，难以跟上这些新兴技术，面临严峻挑战。

Method: 本研究首次将SD的图像生成和强大感知能力整合到图像取证框架中。理论上证明了SD的多模态架构可基于伪造相关信息输出定位结果。具体利用Stable DiffusionV3 (SD3) 的多模态处理能力，将通过高通滤波器提取的图像伪造残差（高频信号）作为显式模态，在训练期间将其融合到潜在空间中，以增强伪造定位性能，同时保留SD3的丰富语义信息。

Result: 实验结果表明，与当前最先进的图像伪造定位模型相比，本框架在常用基准数据集上性能提升高达12%。更重要的是，模型在涉及真实世界文档伪造和自然场景伪造图像的取证任务中表现出强大性能，即使这些数据在训练期间从未出现过。

Conclusion: 本研究提出的基于SD的框架，通过整合其生成和感知能力，并创新性地处理伪造残差，为图像伪造定位提供了一种更高效、更准确的解决方案，有效应对新兴图像处理技术带来的挑战，尤其在处理未见过的真实世界伪造图像时表现出卓越的泛化能力。

Abstract: Driven by the new generation of multi-modal large models, such as Stable
Diffusion (SD), image manipulation technologies have advanced rapidly, posing
significant challenges to image forensics. However, existing image forgery
localization methods, which heavily rely on labor-intensive and costly
annotated data, are struggling to keep pace with these emerging image
manipulation technologies. To address these challenges, we are the first to
integrate both image generation and powerful perceptual capabilities of SD into
an image forensic framework, enabling more efficient and accurate forgery
localization. First, we theoretically show that the multi-modal architecture of
SD can be conditioned on forgery-related information, enabling the model to
inherently output forgery localization results. Then, building on this
foundation, we specifically leverage the multimodal framework of Stable
DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the
multi-modal processing capabilities of SD3 in the latent space by treating
image forgery residuals -- high-frequency signals extracted using specific
highpass filters -- as an explicit modality. This modality is fused into the
latent space during training to enhance forgery localization performance.
Notably, our method fully preserves the latent features extracted by SD3,
thereby retaining the rich semantic information of the input image.
Experimental results show that our framework achieves up to 12% improvements in
performance on widely used benchmarking datasets compared to current
state-of-the-art image forgery localization models. Encouragingly, the model
demonstrates strong performance on forensic tasks involving real-world document
forgery images and natural scene forging images, even when such data were
entirely unseen during training.

</details>


### [50] [Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study](https://arxiv.org/abs/2508.20188)
*Max Torop,Masih Eskandar,Nicholas Kurtansky,Jinyang Liu,Jochen Weber,Octavia Camps,Veronica Rotemberg,Jennifer Dy,Kivanc Kose*

Main category: cs.CV

TL;DR: 为提升AI在皮肤病诊断中的解释性，本文提出结合多模态大语言模型（MLLMs）与量化病灶属性，通过微调使MLLM嵌入空间与属性关联，并基于图像检索进行验证。


<details>
  <summary>Details</summary>
Motivation: 尽管AI在皮肤病诊断中表现出巨大潜力，但其预测结果缺乏解释性，严重阻碍了临床应用，因此急需提高模型的可解释性。

Method: 本文结合多模态大语言模型（MLLMs）和与病灶外观相关的量化属性。具体方法是，通过微调MLLM使其嵌入空间能从图像中预测这些量化属性值，从而实现MLLM嵌入空间与属性的关联。使用SLICE-3D数据集，通过属性特定的内容图像检索案例研究来评估这种关联的有效性。

Result: 研究结果表明，通过微调，多模态大语言模型（MLLMs）的嵌入空间能够成功地与病灶的量化属性（如面积）建立关联，并能从图像中预测这些属性值。

Conclusion: 结合多模态大语言模型和量化属性的方法，为提升AI在皮肤病诊断中的预测可解释性提供了有效途径，有望促进AI模型在临床实践中的应用。

Abstract: Artificial Intelligence models have demonstrated significant success in
diagnosing skin diseases, including cancer, showing the potential to assist
clinicians in their analysis. However, the interpretability of model
predictions must be significantly improved before they can be used in practice.
To this end, we explore the combination of two promising approaches: Multimodal
Large Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a
potential avenue for increased interpretability, providing reasoning for
diagnosis in natural language through an interactive format. Separately, a
number of quantitative attributes that are related to lesion appearance (e.g.,
lesion area) have recently been found predictive of malignancy with high
accuracy. Predictions grounded as a function of such concepts have the
potential for improved interpretability. We provide evidence that MLLM
embedding spaces can be grounded in such attributes, through fine-tuning to
predict their values from images. Concretely, we evaluate this grounding in the
embedding space through an attribute-specific content-based image retrieval
case study using the SLICE-3D dataset.

</details>


### [51] [Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels](https://arxiv.org/abs/2508.20193)
*Hossein Ahmadi,Banafsheh Saffari*

Main category: cs.CV

TL;DR: 本文提出一个统一的Vision Transformer框架，通过融合监督、自监督和重建目标，在低标签条件下实现了高效、泛化性强的自动调制识别。


<details>
  <summary>Details</summary>
Motivation: 自动调制识别（AMR）对认知无线电、频谱监测和安全无线通信至关重要，但现有解决方案过度依赖大量标注数据或复杂的多阶段训练，限制了实际应用中的可扩展性和泛化能力。

Method: 研究提出一个统一的Vision Transformer (ViT) 框架，该框架集成了监督、自监督和重建三种目标。模型由ViT编码器、轻量级卷积解码器和线性分类器组成。其中，重建分支将增强信号映射回原始信号，促使编码器学习精细的I/Q结构，从而在预训练阶段学习鲁棒且具判别力的特征。微调阶段则利用部分标签监督，实现有限标签下的有效分类。

Result: 在RML2018.01A数据集上，该方法在低标签条件下显著优于监督CNN和ViT基线，仅用15-20%的标注数据即可达到接近ResNet的准确率，并在不同信噪比（SNR）水平下保持了稳定的高性能。

Conclusion: 该框架为自动调制识别提供了一种简单、通用且标签高效的解决方案。

Abstract: Automatic modulation recognition (AMR) is critical for cognitive radio,
spectrum monitoring, and secure wireless communication. However, existing
solutions often rely on large labeled datasets or multi-stage training
pipelines, which limit scalability and generalization in practice. We propose a
unified Vision Transformer (ViT) framework that integrates supervised,
self-supervised, and reconstruction objectives. The model combines a ViT
encoder, a lightweight convolutional decoder, and a linear classifier; the
reconstruction branch maps augmented signals back to their originals, anchoring
the encoder to fine-grained I/Q structure. This strategy promotes robust,
discriminative feature learning during pretraining, while partial label
supervision in fine-tuning enables effective classification with limited
labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and
ViT baselines in low-label regimes, approaches ResNet-level accuracy with only
15-20% labeled data, and maintains strong performance across varying SNR
levels. Overall, the framework provides a simple, generalizable, and
label-efficient solution for AMR.

</details>


### [52] [InfinityHuman: Towards Long-Term Audio-Driven Human](https://arxiv.org/abs/2508.20210)
*Xiaodi Li,Pan Xie,Yi Ren,Qijun Gan,Chen Zhang,Fangyuan Kong,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: 本文提出了InfinityHuman，一个粗到细的框架，用于生成高质量、长时间、外观一致且手部动作自然的音频驱动人体动画。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动人体动画方法在生成高分辨率、长时间视频时存在挑战，如通过重叠帧扩展视频时易出现误差累积，导致身份漂移、色彩偏移和场景不稳定；同时，手部动作建模不佳，导致明显失真和与音频不同步。

Method: 我们提出了InfinityHuman，一个粗到细的框架。它首先生成音频同步的表示，然后通过一个姿态引导的细化器逐步将其精炼为高分辨率、长时间视频。该细化器利用稳定的姿态和初始帧作为视觉锚点，以减少漂移并改善唇语同步。此外，为增强语义准确性和手势真实感，我们引入了一个用高质量手部动作数据训练的手部专属奖励机制。

Result: 在EMTD和HDTF数据集上的实验表明，InfinityHuman在视频质量、身份保持、手部准确性和唇语同步方面均达到了最先进的性能。消融研究进一步证实了每个模块的有效性。

Conclusion: InfinityHuman通过其粗到细的框架、姿态引导的细化器和手部专属奖励机制，有效解决了音频驱动人体动画在生成高质量、长时间视频时面临的连贯性、手部真实感和唇语同步等关键挑战，达到了SOTA水平。

Abstract: Audio-driven human animation has attracted wide attention thanks to its
practical applications. However, critical challenges remain in generating
high-resolution, long-duration videos with consistent appearance and natural
hand motions. Existing methods extend videos using overlapping motion frames
but suffer from error accumulation, leading to identity drift, color shifts,
and scene instability. Additionally, hand movements are poorly modeled,
resulting in noticeable distortions and misalignment with the audio. In this
work, we propose InfinityHuman, a coarse-to-fine framework that first generates
audio-synchronized representations, then progressively refines them into
high-resolution, long-duration videos using a pose-guided refiner. Since pose
sequences are decoupled from appearance and resist temporal degradation, our
pose-guided refiner employs stable poses and the initial frame as a visual
anchor to reduce drift and improve lip synchronization. Moreover, to enhance
semantic accuracy and gesture realism, we introduce a hand-specific reward
mechanism trained with high-quality hand motion data. Experiments on the EMTD
and HDTF datasets show that InfinityHuman achieves state-of-the-art performance
in video quality, identity preservation, hand accuracy, and lip-sync. Ablation
studies further confirm the effectiveness of each module. Code will be made
public.

</details>


### [53] [Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos](https://arxiv.org/abs/2508.20221)
*Mert Cokelek,Halit Ozsoy,Nevrez Imamoglu,Cagri Ozcinar,Inci Ayhan,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: 本研究专注于360度视频中的观众注意力预测，通过构建新的音视频数据集并提出两个基于Vision Transformer的模型（SalViT360和SalViT360-AV），证明了整合空间音频线索对提高预测准确性的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏针对360度音视频显著性预测的全面数据集。本研究旨在探索如何有效利用音视频线索来预测360度视频中的视觉显著性。

Method: ['构建了YT360-EyeTracking数据集，包含81个在不同音视频条件下观察的全向视频（ODV）。', '提出了两个新的显著性预测模型：', '1. SalViT360：一个基于Vision Transformer的框架，专为ODV设计，配备了球面几何感知的时空注意力层。', '2. SalViT360-AV：在SalViT360的基础上，进一步整合了基于音频输入的Transformer适配器。']

Result: 在YT360-EyeTracking及其他多个基准数据集上的测试结果表明，SalViT360和SalViT360-AV在预测360度场景中的观众注意力方面，显著优于现有方法。

Conclusion: 研究结果表明，在全向视频中，将空间音频线索整合到模型架构中对于准确的显著性预测至关重要。

Abstract: Omnidirectional videos (ODVs) are redefining viewer experiences in virtual
reality (VR) by offering an unprecedented full field-of-view (FOV). This study
extends the domain of saliency prediction to 360-degree environments,
addressing the complexities of spherical distortion and the integration of
spatial audio. Contextually, ODVs have transformed user experience by adding a
spatial audio dimension that aligns sound direction with the viewer's
perspective in spherical scenes. Motivated by the lack of comprehensive
datasets for 360-degree audio-visual saliency prediction, our study curates
YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying
audio-visual conditions. Our goal is to explore how to utilize audio-visual
cues to effectively predict visual saliency in 360-degree videos. Towards this
aim, we propose two novel saliency prediction models: SalViT360, a
vision-transformer-based framework for ODVs equipped with spherical
geometry-aware spatio-temporal attention layers, and SalViT360-AV, which
further incorporates transformer adapters conditioned on audio input. Our
results on a number of benchmark datasets, including our YT360-EyeTracking,
demonstrate that SalViT360 and SalViT360-AV significantly outperform existing
methods in predicting viewer attention in 360-degree scenes. Interpreting these
results, we suggest that integrating spatial audio cues in the model
architecture is crucial for accurate saliency prediction in omnidirectional
videos. Code and dataset will be available at
https://cyberiada.github.io/SalViT360.

</details>


### [54] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

TL;DR: 本文提出一个基于视觉-语言模型的管道，旨在以最小努力在样本和数据集级别解释视觉模型，以发现故障案例并促进xAI分析。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型开发侧重性能，忽视可解释性；xAI方法应用复杂；解释模型整体行为的方法不足；理解模型普遍行为对避免偏见和识别模式至关重要。

Method: 利用视觉-语言模型（VLM）提出一个管道，该管道能同时在样本级别和数据集级别解释视觉模型。

Result: 该管道能够以最小的努力发现模型的故障案例，并深入了解视觉模型的行为。

Conclusion: 该工作将视觉模型开发与xAI分析相结合，有助于推进图像分析领域。

Abstract: The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [55] [ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems](https://arxiv.org/abs/2508.20232)
*Mohamed Ohamouddou,Said Ohamouddou,Abdellatif El Afia,Rafik Lasri*

Main category: cs.CV

TL;DR: 提出ATMS-KD知识蒸馏框架，结合自适应温度和混合样本增强，为资源受限的农业环境开发轻量级CNN模型，显著提升大马士革玫瑰成熟度分类精度并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 为资源受限的农业环境开发轻量级且高性能的CNN模型进行计算机视觉应用（如作物分类）面临挑战，需平衡模型紧凑性与高精度和低延迟。

Method: 本研究提出ATMS-KD（自适应温度和混合样本知识蒸馏）框架，结合自适应温度调度和混合样本增强技术，将MobileNetV3 Large教师模型（5.7M参数）的知识转移到三种轻量级残差CNN学生模型（Compact: 1.3M, Standard: 2.4M, Enhanced: 3.8M参数）。数据集采用摩洛哥大马士革玫瑰图像，用于成熟度分类。

Result: 所有学生模型通过ATMS-KD获得的验证准确率均超过96.7%（直接训练为95-96%）。ATMS-KD优于其他十一种知识蒸馏方法，其中Compact模型达到97.11%的准确率（比次优方法高1.60个百分点），并保持最低推理延迟（72.19毫秒）。所有学生配置的知识保留率均超过99%，证明知识转移高效且不受学生模型容量影响。

Conclusion: ATMS-KD框架能有效地将知识从大型教师模型转移到轻量级学生模型，在资源受限的农业环境中实现高精度、低延迟的图像分类，为实用高效的农业计算机视觉解决方案提供了有效途径。

Abstract: This study proposes ATMS-KD (Adaptive Temperature and Mixed-Sample Knowledge
Distillation), a novel framework for developing lightweight CNN models suitable
for resource-constrained agricultural environments. The framework combines
adaptive temperature scheduling with mixed-sample augmentation to transfer
knowledge from a MobileNetV3 Large teacher model (5.7\,M parameters) to
lightweight residual CNN students. Three student configurations were evaluated:
Compact (1.3\,M parameters), Standard (2.4\,M parameters), and Enhanced (3.8\,M
parameters). The dataset used in this study consists of images of \textit{Rosa
damascena} (Damask rose) collected from agricultural fields in the Dades Oasis,
southeastern Morocco, providing a realistic benchmark for agricultural computer
vision applications under diverse environmental conditions. Experimental
evaluation on the Damascena rose maturity classification dataset demonstrated
significant improvements over direct training methods. All student models
achieved validation accuracies exceeding 96.7\% with ATMS-KD compared to
95--96\% with direct training. The framework outperformed eleven established
knowledge distillation methods, achieving 97.11\% accuracy with the compact
model -- a 1.60 percentage point improvement over the second-best approach
while maintaining the lowest inference latency of 72.19\,ms. Knowledge
retention rates exceeded 99\% for all configurations, demonstrating effective
knowledge transfer regardless of student model capacity.

</details>


### [56] [Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification](https://arxiv.org/abs/2508.20243)
*Mutahar Safdar,Gentry Wood,Max Zimmermann,Guy Lamouche,Priti Wanjara,Yaoyao Fiona Zhao*

Main category: cs.CV

TL;DR: 本研究提出了一种结合视觉-语言表示（VLRs）的新型框架，通过集成深度语义分割和预训练多模态模型（CLIP/FLAVA），将微观结构数据与专家知识关联起来，实现先进材料的快速零样本分类和质量鉴定。


<details>
  <summary>Details</summary>
Motivation: 先进材料的快速可靠质量鉴定，特别是非传统增材制造产生的异质结构，仍然是工业制造中的一个瓶颈。

Method: 该框架将微观结构信息学与专家鉴定知识相结合，使用定制的混合视觉-语言表示（VLRs）。它通过深度语义分割和预训练多模态模型（CLIP和FLAVA）将视觉微观结构数据和文本专家评估编码到共享表示中。为克服通用嵌入的局限性，开发了一种定制的基于相似度的表示，包含专家标注图像及其文本描述中的正负参考，从而通过净相似度评分实现对未知微观结构的零样本分类。Z-score归一化用于调整相似度分数以提高对齐和分类效果。

Result: 在增材制造金属基复合材料数据集上的验证表明，该框架能有效区分合格和缺陷样本。FLAVA模型显示出更高的视觉敏感性，而CLIP模型与文本标准保持一致。Z-score归一化显著改善了混合视觉-语言框架中的对齐和分类效果。

Conclusion: 所提出的方法增强了质量鉴定流程的可追溯性和可解释性，无需任务特定模型再训练即可实现人机协作决策。通过提升原始数据与专家知识间的语义互操作性，该工作为工程信息学中可扩展、领域适应的质量鉴定策略做出了贡献。

Abstract: Rapid and reliable qualification of advanced materials remains a bottleneck
in industrial manufacturing, particularly for heterogeneous structures produced
via non-conventional additive manufacturing processes. This study introduces a
novel framework that links microstructure informatics with a range of expert
characterization knowledge using customized and hybrid vision-language
representations (VLRs). By integrating deep semantic segmentation with
pre-trained multi-modal models (CLIP and FLAVA), we encode both visual
microstructural data and textual expert assessments into shared
representations. To overcome limitations in general-purpose embeddings, we
develop a customized similarity-based representation that incorporates both
positive and negative references from expert-annotated images and their
associated textual descriptions. This allows zero-shot classification of
previously unseen microstructures through a net similarity scoring approach.
Validation on an additively manufactured metal matrix composite dataset
demonstrates the framework's ability to distinguish between acceptable and
defective samples across a range of characterization criteria. Comparative
analysis reveals that FLAVA model offers higher visual sensitivity, while the
CLIP model provides consistent alignment with the textual criteria. Z-score
normalization adjusts raw unimodal and cross-modal similarity scores based on
their local dataset-driven distributions, enabling more effective alignment and
classification in the hybrid vision-language framework. The proposed method
enhances traceability and interpretability in qualification pipelines by
enabling human-in-the-loop decision-making without task-specific model
retraining. By advancing semantic interoperability between raw data and expert
knowledge, this work contributes toward scalable and domain-adaptable
qualification strategies in engineering informatics.

</details>


### [57] [MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces](https://arxiv.org/abs/2508.20256)
*Zhen Xuen Brandon Low,Rory Zhang,Hang Min,William Pham,Lucy Vivash,Jasmine Moses,Miranda Lynch,Karina Dorfman,Cassandra Marotta,Shaun Koh,Jacob Bunyamin,Ella Rowsthorn,Alex Jarema,Himashi Peiris,Zhaolin Chen,Sandy R. Shultz,David K. Wright,Dexiao Kong,Sharon L. Naismith,Terence J. O'Brien,Ying Xia,Meng Law,Benjamin Sinclair*

Main category: cs.CV

TL;DR: 本研究将Transformer启发的MedNeXt-L-k5网络应用于脑周血管间隙(PVS)的自动分割。在同质T2w数据上取得了高精度(Dice 0.88)，但在异构T1w数据上表现一般(Dice 0.38)。MedNeXt-L-k5为PVS分割提供了有效方案，但未超越nnU-Net，提示全局上下文机制对PVS分割并非必需。


<details>
  <summary>Details</summary>
Motivation: 脑周血管间隙(PVS)被认为是脑小血管病、阿尔茨海默病、中风和衰老相关神经退行性疾病的生物标志物。然而，手动分割PVS耗时且存在中等程度的评价者间可靠性问题，而现有自动化深度学习模型性能一般，且通常难以在多样化的临床和研究MRI数据集之间泛化。

Method: 本研究将MedNeXt-L-k5（一种受Transformer启发的3D编解码卷积网络）用于PVS的自动化分割。训练了两个模型：一个使用人类连接组计划-老化(HCP-Aging)数据集中200个同质T2加权(T2w)MRI扫描；另一个使用来自七项研究和六个扫描仪的40个体异构T1加权(T1w)MRI体积。模型性能通过内部5折交叉验证(5FCV)和留一站点交叉验证(LOSOCV)进行评估。

Result: 在HCP-Aging数据集的T2w图像上训练的MedNeXt-L-k5模型在白质(WM)上达到了0.88+/-0.06的体素级Dice分数，与该数据集报告的评价者间可靠性相当，且是文献中报道的最高值。在HCP-Aging数据集的T1w图像上训练的相同模型在WM上的Dice分数显著降低至0.58+/-0.09。在LOSOCV下，该模型在WM和BG上的体素级Dice分数分别为0.38+/-0.16和0.35+/-0.12；聚类级Dice分数分别为0.61+/-0.19和0.62+/-0.21。MedNeXt-L-k5并未超越nnU-Net。

Conclusion: MedNeXt-L-k5为跨不同T1w和T2w MRI数据集的PVS自动分割提供了一种高效解决方案。然而，MedNeXt-L-k5并未超越nnU-Net，这表明Transformer启发模型中提供全局上下文的注意力机制并非PVS高精度分割的必要条件。

Abstract: Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers
of cerebral small vessel disease, Alzheimer's disease, stroke, and
aging-related neurodegeneration. However, manual segmentation of PVS is
time-consuming and subject to moderate inter-rater reliability, while existing
automated deep learning models have moderate performance and typically fail to
generalize across diverse clinical and research MRI datasets. We adapted
MedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network,
for automated PVS segmentation. Two models were trained: one using a
homogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human
Connectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous
T1-weighted (T1w) MRI volumes from seven studies across six scanners. Model
performance was evaluated using internal 5-fold cross validation (5FCV) and
leave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on
the T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of
0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater
reliability of that dataset, and the highest yet reported in the literature.
The same models trained on the T1w images of the HCP-Aging dataset achieved a
substantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had
voxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and
cluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG).
MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation
across diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the
nnU-Net, indicating that the attention-based mechanisms present in
transformer-inspired models to provide global context are not required for high
accuracy in PVS segmentation.

</details>


### [58] [Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation](https://arxiv.org/abs/2508.20265)
*Zhixiang Chi,Yanan Wu,Li Gu,Huan Liu,Ziqiang Wang,Yang Zhang,Yang Wang,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: CLIP在开放词汇分割中因定位差而受限。本文提出一个无训练、反馈驱动的自适应框架，利用模型输出的语义作为空间一致性先验，反馈至中间注意力，有效提升了多种现有方法在八个基准测试中的性能。


<details>
  <summary>Details</summary>
Motivation: CLIP虽有强大的视觉-文本对齐能力，但在开放词汇分割任务中因定位能力不足而表现不佳。现有方法通过修改中间注意力来增强空间一致性，但这种一致性无法始终有效传播到最终输出，且中间注意力缺乏与文本表示的直接交互，导致语义不一致，限制了CLIP的全部潜力。

Method: 本文提出一种无训练、反馈驱动的自适应框架。该框架将基于输出的块级对应关系反馈到中间注意力层。由于输出预测融合了模型最全面的视觉和文本语义，因此被用作更强的空间一致性先验，以增强内部表示和最终预测之间的语义一致性。关键模块包括注意力隔离、基于置信度的稀疏适应剪枝以及适应集成。该方法可作为插件模块，无缝集成到四种最先进的方法、三种骨干网络（ViT-B、ViT-L、ViT-H），并支持多种注意力类型（Q-K、self-self、以及Proxy增强MAE、SAM、DINO）。

Result: 所提出的方法在八个基准测试中持续提升了与其集成的现有方法的性能。

Conclusion: 通过引入一个训练无关、反馈驱动的自适应框架，利用模型输出的综合语义作为强大的空间一致性先验，能够有效解决CLIP在开放词汇分割中的定位和语义一致性问题，并作为通用插件显著提升了现有方法的性能。

Abstract: CLIP exhibits strong visual-textual alignment but struggle with
open-vocabulary segmentation due to poor localization. Prior methods enhance
spatial coherence by modifying intermediate attention. But, this coherence
isn't consistently propagated to the final output due to subsequent operations
such as projections. Additionally, intermediate attention lacks direct
interaction with text representations, such semantic discrepancy limits the
full potential of CLIP.
  In this work, we propose a training-free, feedback-driven self-adaptive
framework that adapts output-based patch-level correspondences back to the
intermediate attention. The output predictions, being the culmination of the
model's processing, encapsulate the most comprehensive visual and textual
semantics about each patch. Our approach enhances semantic consistency between
internal representations and final predictions by leveraging the model's
outputs as a stronger spatial coherence prior. We design key modules, including
attention isolation, confidence-based pruning for sparse adaptation, and
adaptation ensemble, to effectively feedback the output coherence cues. Our
method functions as a plug-in module, seamlessly integrating into four
state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We
further validate our framework across multiple attention types (Q-K, self-self,
and Proxy augmented with MAE, SAM, and DINO). Our approach consistently
improves their performance across eight benchmarks.

</details>


### [59] [How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding](https://arxiv.org/abs/2508.20279)
*Zhuoran Yu,Yong Jae Lee*

Main category: cs.CV

TL;DR: 本研究引入一个探测框架，系统分析多模态大型语言模型（MLLMs）如何跨层处理视觉和文本输入。发现MLLMs具有一致的分阶段处理结构：早期层负责视觉接地，中间层进行语义推理，后期层准备任务输出。该结构在不同条件稳定，但层级分配随基础LLM架构变化。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型（MLLMs）在视觉-语言任务中表现出色，但其内部处理动态机制尚未得到充分探索。

Method: 提出了一个探测框架，通过训练线性分类器，从MLLM各层提取的token嵌入中预测细粒度视觉类别。利用标准化锚定问题和三种受控提示变体（词汇、语义否定、输出格式），系统评估模型的处理过程。该框架应用于LLaVA-1.5、LLaVA-Next-LLaMA-3和Qwen2-VL模型。

Result: 识别出MLLMs一致的分阶段结构：早期层进行视觉接地，中间层支持词汇整合和语义推理，最终层准备特定任务输出。此外，该分阶段结构在视觉分词、指令微调数据和预训练语料库变化下保持稳定，但具体层级分配会随基础LLM架构的变化而显著调整。

Conclusion: 本研究为MLLMs的层级组织提供了一个统一视角，并提供了一种轻量级、模型无关的方法来分析多模态表征动态。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance
across a wide range of vision-language tasks, yet their internal processing
dynamics remain underexplored. In this work, we introduce a probing framework
to systematically analyze how MLLMs process visual and textual inputs across
layers. We train linear classifiers to predict fine-grained visual categories
(e.g., dog breeds) from token embeddings extracted at each layer, using a
standardized anchor question. To uncover the functional roles of different
layers, we evaluate these probes under three types of controlled prompt
variations: (1) lexical variants that test sensitivity to surface-level
changes, (2) semantic negation variants that flip the expected answer by
modifying the visual concept in the prompt, and (3) output format variants that
preserve reasoning but alter the answer format. Applying our framework to
LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent
stage-wise structure in which early layers perform visual grounding, middle
layers support lexical integration and semantic reasoning, and final layers
prepare task-specific outputs. We further show that while the overall
stage-wise structure remains stable across variations in visual tokenization,
instruction tuning data, and pretraining corpus, the specific layer allocation
to each stage shifts notably with changes in the base LLM architecture. Our
findings provide a unified perspective on the layer-wise organization of MLLMs
and offer a lightweight, model-agnostic approach for analyzing multimodal
representation dynamics.

</details>


### [60] [Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)](https://arxiv.org/abs/2508.20322)
*Zhi Li,Hau Phan,Matthew Emigh,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: 本文提出SLiCS（稀疏线性概念子空间）方法，通过监督字典学习将视觉-语言协同嵌入（如CLIP）解耦为概念特异性分量。这显著提高了概念过滤图像检索和条件生成的精度。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言协同嵌入网络（如CLIP）提供了富有语义信息的潜在嵌入空间，但我们假设该嵌入空间可以被解耦，通过将其分解为位于不同子空间中的多个概念特异性分量向量，从而分离复杂场景内容的信息。

Method: 我们提出了一种监督字典学习方法，用于估计一个线性合成模型。该模型由字典中向量组（原子）的稀疏、非负组合构成，其组级活动与多标签信息匹配。每个概念特异性分量是与标签相关的原子的非负组合。通过一种新颖的交替优化方法来优化组结构字典，并保证收敛性。利用文本协同嵌入，我们详细说明了如何根据概念原子组最佳近似的词语文本嵌入来发现语义有意义的描述，并且无监督字典学习可以利用训练集图像的零样本分类和概念标签的文本嵌入来提供实例级的多标签。

Result: SLiCS提供的解耦嵌入使得概念过滤图像检索（以及使用图像到提示的条件生成）更加精确。我们将SLiCS应用于TiTok的高度压缩自编码器嵌入和自监督DINOv2的潜在嵌入。定量和定性结果均突出显示了所有嵌入在概念过滤图像检索方面的精度提升。

Conclusion: 通过将视觉-语言协同嵌入解耦为稀疏线性概念子空间（SLiCS），能够显著提高概念过滤图像检索和条件生成任务的精度，并在多种类型的嵌入上表现出优越性。

Abstract: Vision-language co-embedding networks, such as CLIP, provide a latent
embedding space with semantic information that is useful for downstream tasks.
We hypothesize that the embedding space can be disentangled to separate the
information on the content of complex scenes by decomposing the embedding into
multiple concept-specific component vectors that lie in different subspaces. We
propose a supervised dictionary learning approach to estimate a linear
synthesis model consisting of sparse, non-negative combinations of groups of
vectors in the dictionary (atoms), whose group-wise activity matches the
multi-label information. Each concept-specific component is a non-negative
combination of atoms associated to a label. The group-structured dictionary is
optimized through a novel alternating optimization with guaranteed convergence.
Exploiting the text co-embeddings, we detail how semantically meaningful
descriptions can be found based on text embeddings of words best approximated
by a concept's group of atoms, and unsupervised dictionary learning can exploit
zero-shot classification of training set images using the text embeddings of
concept labels to provide instance-wise multi-labels. We show that the
disentangled embeddings provided by our sparse linear concept subspaces (SLiCS)
enable concept-filtered image retrieval (and conditional generation using
image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed
autoencoder embeddings from TiTok and the latent embedding from self-supervised
DINOv2. Quantitative and qualitative results highlight the improved precision
of the concept-filtered image retrieval for all embeddings.

</details>


### [61] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 医疗视觉-语言模型（VLMs）虽有前景，但存在安全隐患。本文提出了MedFoundationHub，一个安全、易用的工具包，用于部署和评估医疗VLMs。评估结果显示，现有VLMs在临床应用中仍存在诸多局限。


<details>
  <summary>Details</summary>
Motivation: 医疗VLMs在临床应用中潜力巨大，但存在严重的安全性问题，如受保护健康信息（PHI）泄露、数据泄漏和网络威胁，这在医院环境中尤为关键。因此，即使是出于研究或非临床目的，也需要一个能确保隐私安全、易于部署和使用的平台来应对这些挑战并实施保障措施。

Method: 本文提出了MedFoundationHub，一个图形用户界面（GUI）工具包，其特点包括：1) 使医生无需编程知识即可手动选择和使用不同模型；2) 支持工程师以即插即用方式高效部署医疗VLMs，并无缝集成Hugging Face开源模型；3) 通过Docker编排、操作系统无关的部署确保隐私保护推理。MedFoundationHub仅需离线本地工作站和单个NVIDIA A6000 GPU。为评估当前能力，研究邀请了经委员会认证的病理学家，在结肠和肾脏病例上部署并评估了五种最先进的VLM（包括Google-MedGemma3-4B、Qwen2-VL-7B-Instruct、Qwen2.5-VL-7B-Instruct和LLaVA-1.5-7B/13B），共产生了1015次临床医生-模型评分事件。

Result: 专家评估揭示了当前VLM的反复出现的局限性，包括答案偏离目标、推理模糊以及病理术语不一致。

Conclusion: MedFoundationHub提供了一个安全、可访问且用户友好的平台，用于医疗VLMs的部署和评估。尽管它有助于促进模型应用，但对当前最先进VLM的评估表明，它们在实际临床应用中仍存在显著的性能和可靠性问题，需要进一步改进。

Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [62] [Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction](https://arxiv.org/abs/2508.20376)
*Mang Cao,Sanping Zhou,Yizhe Li,Ye Deng,Wenli Huang,Le Wang*

Main category: cs.CV

TL;DR: 提出BIM模型，通过新颖的双向和多尺度扫描机制，在多任务密集预测中实现高效且充分的跨任务交互，超越SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 多任务密集预测中，充分的跨任务交互至关重要，但现有方法因计算复杂度高，难以兼顾交互完整性与计算效率。

Method: 提出双向交互Mamba (BIM) 模型，将Mamba建模应用于多任务密集预测。包含：1) 双向交互扫描 (BI-Scan) 机制，构建双向任务表示，融合任务优先和位置优先扫描，以线性复杂度高效保留跨任务信息。2) 多尺度扫描 (MS-Scan) 机制，实现多粒度场景建模，满足不同任务需求并增强跨任务特征交互。

Result: 在NYUD-V2和PASCAL-Context两个基准数据集上的广泛实验表明，BIM模型性能优于现有最先进方法。

Conclusion: BIM模型通过创新的扫描机制，有效解决了多任务密集预测中跨任务交互的效率与完整性难题，显著提升了性能。

Abstract: Sufficient cross-task interaction is crucial for success in multi-task dense
prediction. However, sufficient interaction often results in high computational
complexity, forcing existing methods to face the trade-off between interaction
completeness and computational efficiency. To address this limitation, this
work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel
scanning mechanisms to adapt the Mamba modeling approach for multi-task dense
prediction. On the one hand, we introduce a novel Bidirectional Interaction
Scan (BI-Scan) mechanism, which constructs task-specific representations as
bidirectional sequences during interaction. By integrating task-first and
position-first scanning modes within a unified linear complexity architecture,
BI-Scan efficiently preserves critical cross-task information. On the other
hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve
multi-granularity scene modeling. This design not only meets the diverse
granularity requirements of various tasks but also enhances nuanced cross-task
feature interactions. Extensive experiments on two challenging benchmarks,
\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its
state-of-the-art competitors.

</details>


### [63] [Audio-Guided Visual Editing with Complex Multi-Modal Prompts](https://arxiv.org/abs/2508.20379)
*Hyeonyu Kim,Seokhoon Jeong,Seonghee Han,Chanhyuk Choi,Taehwan Kim*

Main category: cs.CV

TL;DR: 本文提出一个无需额外训练的音频引导视觉编辑框架，通过整合多模态编码器和创新的多提示处理方法，有效解决了现有文本引导和音频引导方法在复杂编辑场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的视觉编辑在复杂场景下，仅靠文本指导不足以满足需求，凸显了非文本编辑提示的必要性。同时，现有音频引导的视觉编辑方法通常需要特定数据集训练，限制了其泛化能力。

Method: 本研究引入了一个新的音频引导视觉编辑框架，无需额外训练即可处理多文本和多音频提示的复杂编辑任务。方法利用一个预训练的多模态编码器，通过缓解音频编码器空间与扩散模型提示编码器空间之间的差异，将多样化的音频集成到视觉编辑任务中。此外，通过“独立噪声分支”和“自适应补丁选择”处理多个和多模态编辑提示。

Result: 在多样化的编辑任务上的综合实验表明，该框架通过整合来自音频的丰富信息，在处理复杂编辑场景方面表现出色，弥补了纯文本方法在此类场景中的不足。

Conclusion: 该框架通过结合音频信息，显著提升了视觉编辑在复杂场景下的能力和泛化性，无需额外训练，优于传统的文本引导方法。

Abstract: Visual editing with diffusion models has made significant progress but often
struggles with complex scenarios that textual guidance alone could not
adequately describe, highlighting the need for additional non-text editing
prompts. In this work, we introduce a novel audio-guided visual editing
framework that can handle complex editing tasks with multiple text and audio
prompts without requiring additional training. Existing audio-guided visual
editing methods often necessitate training on specific datasets to align audio
with text, limiting their generalization to real-world situations. We leverage
a pre-trained multi-modal encoder with strong zero-shot capabilities and
integrate diverse audio into visual editing tasks, by alleviating the
discrepancy between the audio encoder space and the diffusion model's prompt
encoder space. Additionally, we propose a novel approach to handle complex
scenarios with multiple and multi-modal editing prompts through our separate
noise branching and adaptive patch selection. Our comprehensive experiments on
diverse editing tasks demonstrate that our framework excels in handling
complicated editing scenarios by incorporating rich information from audio,
where text-only approaches fail.

</details>


### [64] [More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning](https://arxiv.org/abs/2508.20381)
*Luong Tran,Thieu Vo,Anh Nguyen,Sang Dinh,Van Nguyen*

Main category: cs.CV

TL;DR: 针对单正标签多标签学习(SPML)中的挑战，本文提出了AEVLP框架，包含广义伪标签鲁棒损失(GPR Loss)和动态增强多焦点伪标签(DAMP)技术，显著提升了多标签分类的性能并取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 全面标注大规模多标签数据集成本高昂且不切实际，促使研究从部分标注数据中学习，特别是极端情况下的单正标签多标签学习(SPML)。传统SPML方法将缺失标签视为未知或负标签易导致不准确和假阴性，且现有伪标签策略可能引入额外噪声。

Method: 本文提出广义伪标签鲁棒损失(GPR Loss)以有效从多样伪标签中学习并减轻噪声，并引入了动态增强多焦点伪标签(DAMP)技术。两者共同构成了自适应高效视觉-语言伪标签(AEVLP)框架。

Result: 在四个基准数据集上的大量实验表明，所提出的AEVLP框架显著推动了多标签分类任务，并取得了最先进(SOTA)的结果。

Conclusion: AEVLP框架通过GPR Loss和DAMP技术，有效解决了单正标签多标签学习中伪标签噪声和不准确性问题，显著提升了多标签分类的性能。

Abstract: Multi-label learning is a challenging computer vision task that requires
assigning multiple categories to each image. However, fully annotating
large-scale datasets is often impractical due to high costs and effort,
motivating the study of learning from partially annotated data. In the extreme
case of Single Positive Multi-Label Learning (SPML), each image is provided
with only one positive label, while all other labels remain unannotated.
Traditional SPML methods that treat missing labels as unknown or negative tend
to yield inaccuracies and false negatives, and integrating various
pseudo-labeling strategies can introduce additional noise. To address these
challenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a
novel loss function that effectively learns from diverse pseudo-labels while
mitigating noise. Complementing this, we introduce a simple yet effective
Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these
contributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling
(AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate
that our framework significantly advances multi-label classification, achieving
state-of-the-art results.

</details>


### [65] [Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection](https://arxiv.org/abs/2508.20392)
*Chengjun Zhang,Yuhao Zhang,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: 本文提出了一种延迟脉冲（delay-spike）方法和一种新型时序依赖整合-发放（tdIF）神经元架构，显著提升了脉冲神经网络（SNNs）在视觉检测任务中的性能，实现了超低延迟下的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）在神经拟态硬件上具有低功耗和快速推理的优势，但现有ANN-SNN转换方法在分类任务上表现良好，在视觉检测任务中的性能仍不理想，且存在异构脉冲模式导致的膜电位残留问题。

Method: 研究者提出了“延迟脉冲（delay-spike）”方法来缓解异构脉冲模式引起的残余膜电位问题。此外，他们还提出了一种新颖的“时序依赖整合-发放（tdIF）”神经元架构，该架构使IF神经元能根据时间步的先后顺序动态调整其累积和发放行为，从而使脉冲展现出独特的时序特性，而非仅依赖于频率表示。tdIF神经元在保持与传统IF神经元相当的能耗水平。

Result: 该方法在较低时间步下实现了更精确的特征表示，使得视觉检测任务能够达到高性能和超低延迟。在物体检测和车道线检测两项关键视觉任务上的广泛评估表明，该方法超越了当前的ANN-SNN转换方法，在超低延迟（5个时间步以内）下达到了最先进的性能。

Conclusion: 所提出的延迟脉冲方法和tdIF神经元架构有效解决了SNN在视觉检测任务中的性能瓶颈，实现了在超低延迟下与现有最佳方法相当甚至更优的性能，同时保持了能源效率。

Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by
minimal power consumption and swift inference capabilities on neuromorphic
hardware, and have been widely applied to various visual perception tasks.
Current ANN-SNN conversion methods have achieved excellent results in
classification tasks with ultra-low time-steps, but their performance in visual
detection tasks remains suboptimal. In this paper, we propose a delay-spike
approach to mitigate the issue of residual membrane potential caused by
heterogeneous spiking patterns. Furthermore, we propose a novel
temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This
enables Integrate-and-fire (IF) neurons to dynamically adjust their
accumulation and firing behaviors based on the temporal order of time-steps.
Our method enables spikes to exhibit distinct temporal properties, rather than
relying solely on frequency-based representations. Moreover, the tdIF neuron
maintains energy consumption on par with traditional IF neuron. We demonstrate
that our method achieves more precise feature representation with lower
time-steps, enabling high performance and ultra-low latency in visual detection
tasks. In this study, we conduct extensive evaluation of the tdIF method across
two critical vision tasks: object detection and lane line detection. The
results demonstrate that the proposed method surpasses current ANN-SNN
conversion approaches, achieving state-of-the-art performance with ultra-low
latency (within 5 time-steps).

</details>


### [66] [Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection](https://arxiv.org/abs/2508.20415)
*Yuqi Xiong,Wuzhen Shi,Yang Wen,Ruhan Liu*

Main category: cs.CV

TL;DR: DUP-MCRNet通过动态不确定性传播和多模态协同推理，解决了现有显著目标检测中细节丢失、边缘模糊和信息融合不足的问题，在复杂场景下实现了更高的检测精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有显著目标检测（SOD）方法在复杂场景中容易丢失细节、模糊边缘，且单模态信息融合不足。

Method: 本文提出动态不确定性传播与多模态协同推理网络（DUP-MCRNet）。首先，设计了动态不确定性图卷积模块（DUGC），通过基于空间语义距离的稀疏图传播不确定性，结合通道自适应交互，提升小结构和边缘区域的检测精度。其次，提出了多模态协同融合策略（MCF），利用可学习的模态门控权重融合RGB、深度和边缘特征的注意力图，动态调整模态重要性，增强跨模态语义互补性。最后，通过多尺度BCE和IoU损失、跨尺度一致性约束以及不确定性引导监督机制优化检测性能。

Result: 实验结果表明，DUP-MCRNet在大多数常见基准数据集上优于现有SOD方法，尤其在边缘清晰度和对复杂背景的鲁棒性方面表现突出。

Conclusion: DUP-MCRNet通过结合动态不确定性传播和多模态协同融合，有效解决了复杂场景下显著目标检测的挑战，显著提升了检测性能，特别是在细节和边缘处理方面。

Abstract: In view of the problems that existing salient object detection (SOD) methods
are prone to losing details, blurring edges, and insufficient fusion of
single-modal information in complex scenes, this paper proposes a dynamic
uncertainty propagation and multimodal collaborative reasoning network
(DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is
designed to propagate uncertainty between layers through a sparse graph
constructed based on spatial semantic distance, and combined with channel
adaptive interaction, it effectively improves the detection accuracy of small
structures and edge regions. Secondly, a multimodal collaborative fusion
strategy (MCF) is proposed, which uses learnable modality gating weights to
weightedly fuse the attention maps of RGB, depth, and edge features. It can
dynamically adjust the importance of each modality according to different
scenes, effectively suppress redundant or interfering information, and
strengthen the semantic complementarity and consistency between
cross-modalities, thereby improving the ability to identify salient regions
under occlusion, weak texture or background interference. Finally, the
detection performance at the pixel level and region level is optimized through
multi-scale BCE and IoU loss, cross-scale consistency constraints, and
uncertainty-guided supervision mechanisms. Extensive experiments show that
DUP-MCRNet outperforms various SOD methods on most common benchmark datasets,
especially in terms of edge clarity and robustness to complex backgrounds. Our
code is publicly available at https://github.com/YukiBear426/DUP-MCRNet.

</details>


### [67] [MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection](https://arxiv.org/abs/2508.20447)
*Taiga Yamane,Satoshi Suzuki,Ryo Masumura,Shota Orihashi,Tomohiro Tanaka,Mana Ihori,Naoki Makishima,Naotaka Kawata*

Main category: cs.CV

TL;DR: 本文提出了一种名为MSMVD的新型多视角行人检测方法，通过生成并融合多尺度鸟瞰图（BEV）特征来解决现有方法在处理不同尺度行人检测上的不足，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多视角行人检测（MVPD）深度学习方法难以有效检测视图中尺度一致性过小或过大，以及视图间尺度差异巨大的行人，原因在于它们未能充分利用多尺度图像特征来生成BEV特征。

Method: 本文提出MSMVD方法。该方法通过将从单个视图中提取的多尺度图像特征逐尺度投影到BEV空间，生成多尺度BEV特征。随后，利用特征金字塔网络（FPN）处理这些多尺度BEV特征，以结合多视角在不同尺度上的信息。

Result: 实验结果表明，利用多尺度图像特征通过多尺度BEV特征显著提升了检测性能。MSMVD在GMVD数据集上，相较于先前最高的MODA指标提升了4.5个百分点。

Conclusion: 通过利用多尺度图像特征生成多尺度BEV特征，并结合特征金字塔网络，能有效改善多视角行人检测中对不同尺度行人的检测性能，实现了显著的检测效果提升。

Abstract: Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form
of a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end
trainable deep learning methods have progressed greatly. However, they often
struggle to detect pedestrians with consistently small or large scales in views
or with vastly different scales between views. This is because they do not
exploit multi-scale image features to generate the BEV feature and detect
pedestrians. To overcome this problem, we propose a novel MVPD method, called
Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV
features by projecting multi-scale image features extracted from individual
views into the BEV space, scale-by-scale. Each of these BEV features inherits
the properties of its corresponding scale image features from multiple views.
Therefore, these BEV features help the precise detection of pedestrians with
consistently small or large scales in views. Then, MSMVD combines information
at different scales of multiple views by processing the multi-scale BEV
features using a feature pyramid network. This improves the detection of
pedestrians with vastly different scales between views. Extensive experiments
demonstrate that exploiting multi-scale image features via multi-scale BEV
features greatly improves the detection performance, and MSMVD outperforms the
previous highest MODA by $4.5$ points on the GMVD dataset.

</details>


### [68] [A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection](https://arxiv.org/abs/2508.20449)
*Libo Lv,Tianyi Wang,Mengxiao Huang,Ruixia Liu,Yinglong Wang*

Main category: cs.CV

TL;DR: 提出了一种名为SFMFNet的轻量级实时深度伪造检测网络，该网络通过融合空间-频率特征、令牌选择交叉注意力及残差增强模糊池化，实现了准确性和效率的良好平衡，并具有较强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着实时深度伪造技术的快速发展，伪造内容日益逼真且广泛传播。尽管现有检测器在准确性上表现出色，但其高昂的计算成本阻碍了在实际应用中的实时部署。

Method: 本文提出了空间-频率感知多尺度融合网络（SFMFNet）。具体方法包括：1) 设计了一个空间-频率混合感知模块，通过门控机制协同利用空间纹理和频率伪影；2) 引入了令牌选择交叉注意力机制，以实现高效的多级特征交互；3) 使用了残差增强模糊池化结构，以在下采样过程中保留关键语义线索。

Result: 在多个基准数据集上的实验结果表明，SFMFNet在准确性和效率之间取得了良好的平衡，展现出强大的泛化能力和实用价值，适用于实时应用。

Conclusion: SFMFNet为实时深度伪造检测提供了一个轻量级且有效的解决方案，能够兼顾高准确率与低计算成本，在实际应用中具有显著的部署潜力。

Abstract: With the rapid advancement of real-time deepfake generation techniques,
forged content is becoming increasingly realistic and widespread across
applications like video conferencing and social media. Although
state-of-the-art detectors achieve high accuracy on standard benchmarks, their
heavy computational cost hinders real-time deployment in practical
applications. To address this, we propose the Spatial-Frequency Aware
Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture
for real-time deepfake detection. We design a spatial-frequency hybrid aware
module that jointly leverages spatial textures and frequency artifacts through
a gated mechanism, enhancing sensitivity to subtle manipulations. A
token-selective cross attention mechanism enables efficient multi-level feature
interaction, while a residual-enhanced blur pooling structure helps retain key
semantic cues during downsampling. Experiments on several benchmark datasets
show that SFMFNet achieves a favorable balance between accuracy and efficiency,
with strong generalization and practical value for real-time applications.

</details>


### [69] [Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification](https://arxiv.org/abs/2508.20461)
*Ayaka Tsutsumi,Guang Li,Ren Togo,Takahiro Ogawa,Satoshi Kondo,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出一种结合双模型权重选择和自知识蒸馏的医学图像分类新方法，旨在开发高性能的轻量级模型以克服计算资源限制。


<details>
  <summary>Details</summary>
Motivation: 在医疗环境中，大型模型受限于计算资源难以部署。开发性能与大型模型相当且计算高效的轻量级模型至关重要，以解决传统方法在压缩模型中信息丢失的问题。

Method: 采用双模型权重选择策略，用大型预训练模型的权重初始化两个轻量级模型进行知识迁移；然后对这些模型应用自知识蒸馏（SKD），以处理广泛的初始权重配置并降低计算成本；最后进行微调以完成分类任务。

Result: 在胸部X光、肺部CT和脑部MRI等公开数据集上，该方法表现出优于现有方法的卓越性能和鲁棒性。

Conclusion: 该方法通过结合双模型权重选择和自知识蒸馏，有效克服了传统方法在紧凑模型中难以保留关键信息的局限性，实现了卓越的性能。

Abstract: We propose a novel medical image classification method that integrates
dual-model weight selection with self-knowledge distillation (SKD). In
real-world medical settings, deploying large-scale models is often limited by
computational resource constraints, which pose significant challenges for their
practical implementation. Thus, developing lightweight models that achieve
comparable performance to large-scale models while maintaining computational
efficiency is crucial. To address this, we employ a dual-model weight selection
strategy that initializes two lightweight models with weights derived from a
large pretrained model, enabling effective knowledge transfer. Next, SKD is
applied to these selected models, allowing the use of a broad range of initial
weight configurations without imposing additional excessive computational cost,
followed by fine-tuning for the target classification tasks. By combining
dual-model weight selection with self-knowledge distillation, our method
overcomes the limitations of conventional approaches, which often fail to
retain critical information in compact models. Extensive experiments on
publicly available datasets-chest X-ray images, lung computed tomography scans,
and brain magnetic resonance imaging scans-demonstrate the superior performance
and robustness of our approach compared to existing methods.

</details>


### [70] [Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds](https://arxiv.org/abs/2508.20466)
*Pengpeng Yu,Haoran Li,Dingquan Li,Runqing Jiang,Jing Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

TL;DR: 提出一种新的LiDAR点云压缩方法，利用几何重致密化和跨尺度特征传播模块生成紧凑特征，实现了最先进的压缩比和实时编解码性能。


<details>
  <summary>Details</summary>
Motivation: 高精度LiDAR点云数据存储和传输开销巨大。现有方法在处理几何细节的极端稀疏性时，上下文建模效率低下，限制了压缩性能和速度。

Method: 提出通过生成紧凑特征实现高效预测编码的框架，包含两个轻量级模块：1. 几何重致密化模块：对稀疏几何体进行重致密化，在更密集尺度提取特征，然后重新稀疏化特征进行预测编码，以避免对稀疏细节进行昂贵计算。2. 跨尺度特征传播模块：利用多分辨率占用线索引导分层特征传播，促进跨尺度信息共享，减少冗余特征提取，并为几何重致密化模块提供丰富特征。通过整合两模块，实现紧凑特征表示，提供高效上下文建模并加速编码过程。

Result: 在KITTI数据集上的实验表明，本方法达到了最先进的压缩比和实时性能，在12位量化下，编码和解码速度均达到26 FPS。

Conclusion: 该方法通过创新的模块设计生成紧凑特征，有效解决了LiDAR点云压缩中上下文建模效率低的问题，实现了卓越的压缩性能和实时处理能力。

Abstract: LiDAR point clouds are fundamental to various applications, yet
high-precision scans incur substantial storage and transmission overhead.
Existing methods typically convert unordered points into hierarchical octree or
voxel structures for dense-to-sparse predictive coding. However, the extreme
sparsity of geometric details hinders efficient context modeling, thereby
limiting their compression performance and speed. To address this challenge, we
propose to generate compact features for efficient predictive coding. Our
framework comprises two lightweight modules. First, the Geometry
Re-Densification Module re-densifies encoded sparse geometry, extracts features
at denser scale, and then re-sparsifies the features for predictive coding.
This module avoids costly computation on highly sparse details while
maintaining a lightweight prediction head. Second, the Cross-scale Feature
Propagation Module leverages occupancy cues from multiple resolution levels to
guide hierarchical feature propagation. This design facilitates information
sharing across scales, thereby reducing redundant feature extraction and
providing enriched features for the Geometry Re-Densification Module. By
integrating these two modules, our method yields a compact feature
representation that provides efficient context modeling and accelerates the
coding process. Experiments on the KITTI dataset demonstrate state-of-the-art
compression ratios and real-time performance, achieving 26 FPS for both
encoding and decoding at 12-bit quantization. Code is available at
https://github.com/pengpeng-yu/FastPCC.

</details>


### [71] [Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation](https://arxiv.org/abs/2508.20470)
*Xiaochuan Li,Guoguang Du,Runze Zhang,Liang Jin,Qi Jia,Lihua Lu,Zhenhua Guo,Yaqian Zhao,Haiyang Liu,Tianqi Wang,Changsheng Li,Xiaoli Gong,Rengang Li,Baoyu Fan*

Main category: cs.CV

TL;DR: 该研究利用视频数据解决3D领域数据稀缺问题，提出了一种新的大规模多视角视频数据集和生成模型，实现了空间一致且语义合理的3D资产生成，并展现了场景级应用的潜力。


<details>
  <summary>Details</summary>
Motivation: 大规模数据训练模型在文本、图像和视频等领域取得了巨大成功，但3D领域原生数据稀缺，导致模型泛化能力受限。视频中蕴含的常识先验（如多视角提供的空间一致性和丰富的语义信息）可以作为替代监督信号，以缓解3D数据不足带来的瓶颈。

Method: 探索如何将视频模态应用于3D资产生成，涵盖数据集和模型两方面。具体地，引入了首个具有多视角标注的大规模视频数据集Droplet3D-4M，并训练了支持图像和密集文本输入的生成模型Droplet3D。

Result: 实验证明，所提出的方法能够生成空间一致且语义合理的3D内容。此外，与现有的3D解决方案相比，该方法展现出扩展到场景级应用的潜力。

Conclusion: 视频中的常识先验显著促进了3D创作，为解决3D数据稀缺和实现更强大、多功能的3D生成提供了有效途径。

Abstract: Scaling laws have validated the success and promise of large-data-trained
models in creative generation across text, image, and video domains. However,
this paradigm faces data scarcity in the 3D domain, as there is far less of it
available on the internet compared to the aforementioned modalities.
Fortunately, there exist adequate videos that inherently contain commonsense
priors, offering an alternative supervisory signal to mitigate the
generalization bottleneck caused by limited native 3D data. On the one hand,
videos capturing multiple views of an object or scene provide a spatial
consistency prior for 3D generation. On the other hand, the rich semantic
information contained within the videos enables the generated content to be
more faithful to the text prompts and semantically plausible. This paper
explores how to apply the video modality in 3D asset generation, spanning
datasets to models. We introduce Droplet3D-4M, the first large-scale video
dataset with multi-view level annotations, and train Droplet3D, a generative
model supporting both image and dense text input. Extensive experiments
validate the effectiveness of our approach, demonstrating its ability to
produce spatially consistent and semantically plausible content. Moreover, in
contrast to the prevailing 3D solutions, our approach exhibits the potential
for extension to scene-level applications. This indicates that the commonsense
priors from the videos significantly facilitate 3D creation. We have
open-sourced all resources including the dataset, code, technical framework,
and model weights: https://dropletx.github.io/.

</details>


### [72] [Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation](https://arxiv.org/abs/2508.20471)
*Jiusi Li,Jackson Jiang,Jinyu Miao,Miao Long,Tuopu Wen,Peijin Jia,Shengxiang Liu,Chunlei Yu,Maolin Liu,Yuzhan Cai,Kun Jiang,Mengmeng Yang,Diange Yang*

Main category: cs.CV

TL;DR: G^2Editor是针对驾驶视频中物体进行逼真、精确编辑的框架，它结合3D高斯表示与分层特征，克服了现有方法在视觉保真度和姿态控制上的局限，并有效支持多种物体编辑操作。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要大量边缘案例进行训练和验证，但现实世界数据收集成本高昂且危险。现有通过3D Gaussian Splatting或图像生成模型编辑传感器数据的方法，在视觉保真度和姿态控制方面存在不足。

Method: 提出了G^2Editor框架。它利用编辑对象的3D高斯表示作为密集先验，注入去噪过程以实现精确姿态控制和空间一致性。同时，采用场景级3D边界框布局重建非目标对象的遮挡区域，并融合分层细粒度特征以精细化编辑对象的视觉细节。

Result: 实验结果表明，G^2Editor在一个统一框架内有效支持物体的重新定位、插入和删除。相较于现有方法，它在姿态可控性和视觉质量方面均表现出色，并能为下游数据驱动任务带来益处。

Conclusion: G^2Editor提供了一种在驾驶视频中进行高保真、精确物体编辑的有效解决方案，显著提升了合成场景的质量和可控性，有望加速自动驾驶系统在边缘案例下的训练和验证。

Abstract: Corner cases are crucial for training and validating autonomous driving
systems, yet collecting them from the real world is often costly and hazardous.
Editing objects within captured sensor data offers an effective alternative for
generating diverse scenarios, commonly achieved through 3D Gaussian Splatting
or image generative models. However, these approaches often suffer from limited
visual fidelity or imprecise pose control. To address these issues, we propose
G^2Editor, a framework designed for photorealistic and precise object editing
in driving videos. Our method leverages a 3D Gaussian representation of the
edited object as a dense prior, injected into the denoising process to ensure
accurate pose control and spatial consistency. A scene-level 3D bounding box
layout is employed to reconstruct occluded areas of non-target objects.
Furthermore, to guide the appearance details of the edited object, we
incorporate hierarchical fine-grained features as additional conditions during
generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor
effectively supports object repositioning, insertion, and deletion within a
unified framework, outperforming existing methods in both pose controllability
and visual quality, while also benefiting downstream data-driven tasks.

</details>


### [73] [Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization](https://arxiv.org/abs/2508.20475)
*Marina Grifell i Plana,Vladyslav Zalevskyi,Léa Schmidt,Yvan Gomez,Thomas Sanchez,Vincent Dunet,Mériam Koob,Vanessa Siffredi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 为解决罕见疾病（如胼胝体发育不全CCD）数据稀缺导致的胎儿大脑分割难题，本研究提出一种病理学知情的域随机化策略，通过合成数据实现鲁棒分割，显著提升了生物标志物提取精度及对罕见畸形的分析能力。


<details>
  <summary>Details</summary>
Motivation: 准确的胎儿大脑分割对于提取生物标志物和评估神经发育至关重要，尤其是在像胼胝体发育不全（CCD）这类能引起剧烈解剖变化的疾病中。然而，CCD的稀有性严重限制了标注数据，阻碍了深度学习模型的泛化。

Method: 我们提出一种病理学知情的域随机化策略，将CCD的先验知识嵌入到合成数据生成流程中。该方法仅通过健康数据模拟多样化的脑部病理改变，从而在不依赖病理标注的情况下实现鲁棒分割。

Result: 该方法在包含248名健康胎儿、26名CCD病例和47名其他脑部病理病例的队列中得到验证。结果显示，在CCD病例上取得了显著改进，同时保持了在健康胎儿和其他病理上的性能。我们从预测分割结果中导出了胼胝体长度（LCC）和体积等临床相关生物标志物，并证明了它们在区分CCD亚型中的效用。健康病例的LCC估计误差从1.89毫米降至0.80毫米，CCD病例从10.9毫米大幅降至0.7毫米。此外，分割结果的拓扑一致性也得到改善。

Conclusion: 本研究表明，将领域特定的解剖学先验知识融入合成数据管道，可以有效缓解数据稀缺问题，并增强对罕见但临床意义重大的畸形进行分析的能力。

Abstract: Accurate fetal brain segmentation is crucial for extracting biomarkers and
assessing neurodevelopment, especially in conditions such as corpus callosum
dysgenesis (CCD), which can induce drastic anatomical changes. However, the
rarity of CCD severely limits annotated data, hindering the generalization of
deep learning models. To address this, we propose a pathology-informed domain
randomization strategy that embeds prior knowledge of CCD manifestations into a
synthetic data generation pipeline. By simulating diverse brain alterations
from healthy data alone, our approach enables robust segmentation without
requiring pathological annotations.
  We validate our method on a cohort comprising 248 healthy fetuses, 26 with
CCD, and 47 with other brain pathologies, achieving substantial improvements on
CCD cases while maintaining performance on both healthy fetuses and those with
other pathologies. From the predicted segmentations, we derive clinically
relevant biomarkers, such as corpus callosum length (LCC) and volume, and show
their utility in distinguishing CCD subtypes. Our pathology-informed
augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in
healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these
quantitative gains, our approach yields segmentations with improved topological
consistency relative to available ground truth, enabling more reliable
shape-based analyses. Overall, this work demonstrates that incorporating
domain-specific anatomical priors into synthetic data pipelines can effectively
mitigate data scarcity and enhance analysis of rare but clinically significant
malformations.

</details>


### [74] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

TL;DR: 本文提出了首个统一框架，整合手语、唇语和音频以生成口语文本，并在SLT、VSR、ASR和AVSR任务上取得了与最先进模型相当或更优的性能，并发现唇部动作能显著提高手语翻译效果。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别（ASR）技术对听障人士不友好。现有的手语翻译（SLT）和视觉语音识别（VSR）研究多为独立进行，缺乏将多种模态（手语、唇语、音频）整合到统一框架中的探索，以改进无声交流。

Method: 引入了一个统一框架，旨在处理手语、唇部动作和音频的各种组合，生成口语文本。该框架着重于：(i) 设计一个统一、模态无关的架构；(ii) 探索模态间的协同作用，特别是唇部动作在手语理解中的作用；(iii) 达到或超越专业化任务的最新模型性能。

Result: 该框架在SLT、VSR、ASR和AVSR等任务上，取得了与各任务特定最先进模型相当或更优的性能。分析显示，将唇部动作明确地建模为独立模态能显著提升SLT性能。

Conclusion: 所提出的统一框架能够有效地处理多模态输入（手语、唇语、音频）以生成口语文本，并普遍提高了多项任务的性能。唇部动作作为非手动线索，对手语理解和翻译具有重要且积极的影响。

Abstract: Audio is the primary modality for human communication and has driven the
success of Automatic Speech Recognition (ASR) technologies. However, such
systems remain inherently inaccessible to individuals who are deaf or hard of
hearing. Visual alternatives such as sign language and lip reading offer
effective substitutes, and recent advances in Sign Language Translation (SLT)
and Visual Speech Recognition (VSR) have improved audio-less communication.
Yet, these modalities have largely been studied in isolation, and their
integration within a unified framework remains underexplored. In this paper, we
introduce the first unified framework capable of handling diverse combinations
of sign language, lip movements, and audio for spoken-language text generation.
We focus on three main objectives: (i) designing a unified, modality-agnostic
architecture capable of effectively processing heterogeneous inputs; (ii)
exploring the underexamined synergy among modalities, particularly the role of
lip movements as non-manual cues in sign language comprehension; and (iii)
achieving performance on par with or superior to state-of-the-art models
specialized for individual tasks. Building on this framework, we achieve
performance on par with or better than task-specific state-of-the-art models
across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that
explicitly modeling lip movements as a separate modality significantly improves
SLT performance.

</details>


### [75] [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478)
*Yuan Xie,Tianshui Chen,Zheng Ge,Lionel Ni*

Main category: cs.CV

TL;DR: 针对长视频理解挑战，本文提出Video-MTR，一个强化的多轮推理框架，通过迭代选择关键视频片段和门控双层奖励系统，实现端到端训练，并在准确性和效率上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 长视频理解因长距离时间依赖和多事件而充满挑战。现有方法依赖静态推理或外部视觉-语言模型（VLMs），导致复杂性、次优性能和缺乏端到端训练。

Method: 提出Video-MTR，一个强化的多轮推理框架，旨在实现迭代的关键视频片段选择和问题理解。它通过多轮推理，基于对先前处理片段和当前问题的理解逐步选择视频片段。为优化中间推理过程，引入了新颖的门控双层奖励系统，结合基于答案正确性的轨迹级奖励和强调帧-查询相关性的轮级奖励，从而实现端到端训练并消除对外部VLM的依赖。

Result: 在VideoMME、MLVU和EgoSchema等基准测试中，Video-MTR在准确性和效率方面均优于现有方法。

Conclusion: Video-MTR显著提升了长视频理解的性能，在当前技术水平上取得了重要进展。

Abstract: Long-form video understanding, characterized by long-range temporal
dependencies and multiple events, remains a challenge. Existing methods often
rely on static reasoning or external visual-language models (VLMs), which face
issues like complexity and sub-optimal performance due to the lack of
end-to-end training. In this paper, we propose Video-MTR, a reinforced
multi-turn reasoning framework designed to enable iterative key video segment
selection and question comprehension. Unlike traditional video reasoning
pipeline, which generate predictions in a single turn, Video-MTR performs
reasoning in multiple turns, selecting video segments progressively based on
the evolving understanding of previously processed segments and the current
question. This iterative process allows for a more refined and contextually
aware analysis of the video. To ensure intermediate reasoning process, we
introduce a novel gated bi-level reward system, combining trajectory-level
rewards based on answer correctness and turn-level rewards emphasizing
frame-query relevance. This system optimizes both video segment selection and
question comprehension, eliminating the need for external VLMs and allowing
end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,
and EgoSchema demonstrate that Video-MTR outperforms existing methods in both
accuracy and efficiency, advancing the state-of-the-art in long video
understanding.

</details>


### [76] [Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts](https://arxiv.org/abs/2508.20488)
*Zixuan Hu,Dongxiao Li,Xinzhu Ma,Shixiang Tang,Xiaotong Li,Wenhan Yang,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 针对单目3D目标检测（M3OD）在域偏移下性能下降以及其固有的语义和几何双重不确定性，本文提出了双不确定性优化（DUO）框架，通过无监督Focal Loss和语义感知法线场约束，联合最小化这两种不确定性，显著提升了M3OD的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 单目3D目标检测（M3OD）对于自动驾驶等安全关键应用至关重要，但在真实世界的域偏移（如环境或传感器变化）下，其可靠性会显著下降。现有测试时自适应（TTA）方法虽关注不确定性，但未能解决M3OD固有的双重不确定性问题：语义不确定性（类别预测模糊）和几何不确定性（空间定位不稳定）。

Method: 本文提出双不确定性优化（DUO）框架，这是首个旨在联合最小化M3OD中语义和几何双重不确定性的TTA方法。具体方法包括：1) 基于凸优化思想，引入创新的凸结构Focal Loss并派生出无监督版本，实现标签无关的不确定性加权和高不确定性对象的平衡学习。2) 设计语义感知法线场约束，在具有清晰语义线索的区域保持几何一致性，从而减少不稳定3D表示带来的不确定性。这两种机制形成互补循环，相互促进。

Result: 广泛的实验证明，DUO在各种数据集和域偏移类型上均优于现有方法，展现出卓越的性能和鲁棒性。

Conclusion: DUO框架通过有效处理并联合优化单目3D目标检测中的语义和几何双重不确定性，显著提高了模型在面对真实世界域偏移时的鲁棒性和性能，为安全关键应用提供了更可靠的解决方案。

Abstract: Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical
applications like autonomous driving, yet its reliability deteriorates
significantly under real-world domain shifts caused by environmental or sensor
variations. To address these shifts, Test-Time Adaptation (TTA) methods have
emerged, enabling models to adapt to target distributions during inference.
While prior TTA approaches recognize the positive correlation between low
uncertainty and high generalization ability, they fail to address the dual
uncertainty inherent to M3OD: semantic uncertainty (ambiguous class
predictions) and geometric uncertainty (unstable spatial localization). To
bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA
framework designed to jointly minimize both uncertainties for robust M3OD.
Through a convex optimization lens, we introduce an innovative convex structure
of the focal loss and further derive a novel unsupervised version, enabling
label-agnostic uncertainty weighting and balanced learning for high-uncertainty
objects. In parallel, we design a semantic-aware normal field constraint that
preserves geometric coherence in regions with clear semantic cues, reducing
uncertainty from the unstable 3D representation. This dual-branch mechanism
forms a complementary loop: enhanced spatial perception improves semantic
classification, and robust semantic predictions further refine spatial
understanding. Extensive experiments demonstrate the superiority of DUO over
existing methods across various datasets and domain shift types.

</details>


### [77] [CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information](https://arxiv.org/abs/2508.20491)
*Seunghyeon Jung,Seoyoung Hong,Jiwoo Jeong,Seungwon Jeong,Jaerim Choi,Hoki Kim,Woojin Lee*

Main category: cs.CV

TL;DR: 该研究提出了一个名为CaddieSet的新数据集，用于定量建立高尔夫挥杆姿态与击球轨迹之间的关系，并通过可解释模型提供符合领域知识的挥杆反馈。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习研究未能定量建立挥杆姿态与击球轨迹的关系，限制了为高尔夫球手提供有效挥杆改进见解的能力。

Method: 提出了CaddieSet数据集，包含单次击球的关节信息和多种球信息。通过计算机视觉方法将挥杆视频分割成八个阶段以提取关节信息，并基于高尔夫专家知识定义了15个关键指标来解释挥杆结果。

Result: 实验证明CaddieSet在预测球轨迹方面的可行性。特别地，通过可解释模型验证了使用关节特征提供的挥杆反馈与既定领域知识在定量上一致。

Conclusion: 这项工作有望为学术界和体育产业的高尔夫挥杆分析提供新见解。

Abstract: Recent advances in deep learning have led to more studies to enhance golfers'
shot precision. However, these existing studies have not quantitatively
established the relationship between swing posture and ball trajectory,
limiting their ability to provide golfers with the necessary insights for swing
improvement. In this paper, we propose a new dataset called CaddieSet, which
includes joint information and various ball information from a single shot.
CaddieSet extracts joint information from a single swing video by segmenting it
into eight swing phases using a computer vision-based approach. Furthermore,
based on expert golf domain knowledge, we define 15 key metrics that influence
a golf swing, enabling the interpretation of swing outcomes through
swing-related features. Through experiments, we demonstrated the feasibility of
CaddieSet for predicting ball trajectories using various benchmarks. In
particular, we focus on interpretable models among several benchmarks and
verify that swing feedback using our joint features is quantitatively
consistent with established domain knowledge. This work is expected to offer
new insight into golf swing analysis for both academia and the sports industry.

</details>


### [78] [IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection](https://arxiv.org/abs/2508.20492)
*Xuanming Cao,Chengyu Tao,Yifeng Cheng,Juan Du*

Main category: cs.CV

TL;DR: 本文提出IAENet，一种结合2D预训练专家和3D专家模型的集成网络，通过引入动态评估各模态贡献的Importance-Aware Fusion (IAF)模块及指导性损失函数，实现了3D表面异常检测的新SOTA，并显著降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 表面异常检测对工业制造至关重要。尽管2D图像方法成功，但3D点云检测因缺乏强大的预训练骨干网络而未被充分探索。朴素地融合2D和3D预测可能因单一模态表现不佳而降低整体准确性，这是亟待解决的挑战。

Method: 本文提出Importance-Aware Ensemble Network (IAENet)，这是一个协同2D预训练专家和3D专家模型的集成框架。为解决融合难题，引入了新颖的Importance-Aware Fusion (IAF)模块，该模块能动态评估各源的贡献并重新加权其异常分数。此外，设计了关键的损失函数来显式指导IAF的优化，以结合专家知识并保留其独特优势。

Result: 在MVTec 3D-AD数据集上的广泛实验表明，IAENet取得了新的最先进（SOTA）性能，并显著降低了误报率。

Conclusion: IAENet的优异表现和较低的误报率，证明了其在工业部署中的实际价值，有效解决了3D表面异常检测的挑战，并通过创新的融合策略提升了整体性能。

Abstract: Surface anomaly detection is pivotal for ensuring product quality in
industrial manufacturing. While 2D image-based methods have achieved remarkable
success, 3D point cloud-based detection remains underexplored despite its
richer geometric cues. We argue that the key bottleneck is the absence of
powerful pretrained foundation backbones in 3D comparable to those in 2D. To
bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an
ensemble framework that synergizes 2D pretrained expert with 3D expert models.
However, naively fusing predictions from disparate sources is non-trivial:
existing strategies can be affected by a poorly performing modality and thus
degrade overall accuracy. To address this challenge, We introduce an novel
Importance-Aware Fusion (IAF) module that dynamically assesses the contribution
of each source and reweights their anomaly scores. Furthermore, we devise
critical loss functions that explicitly guide the optimization of IAF, enabling
it to combine the collective knowledge of the source experts but also preserve
their unique strengths, thereby enhancing the overall performance of anomaly
detection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet
achieves a new state-of-the-art with a markedly lower false positive rate,
underscoring its practical value for industrial deployment.

</details>


### [79] [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](https://arxiv.org/abs/2508.20505)
*En Ci,Shanyan Guan,Yanhao Ge,Yilin Zhang,Wei Li,Zhenyu Zhang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: DescriptiveEdit是一个基于描述性提示的图像编辑框架，将指令编辑重构为基于参考图的文生图，通过引入交叉注意力UNet，有效提高了编辑的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 语义图像编辑仍是一个挑战。现有基于反演的算法会引入重建误差，而基于指令的模型受限于数据集质量和规模。

Method: 提出DescriptiveEdit框架，核心思想是将“基于指令的图像编辑”重构为“基于参考图的文生图”。具体方法是引入一个交叉注意力UNet，通过注意力桥接将参考图像特征注入到提示到编辑图像的生成过程中。

Result: DescriptiveEdit克服了指令数据集质量的限制，能够与ControlNet、IP-Adapter等扩展无缝集成，并更具可扩展性。在Emu Edit基准测试中，实验结果表明它提高了编辑的准确性和一致性。

Conclusion: DescriptiveEdit通过重新定义编辑任务并利用文生图模型的生成能力，有效解决了语义图像编辑中的挑战，提升了编辑性能和兼容性。

Abstract: Despite the progress in text-to-image generation, semantic image editing
remains a challenge. Inversion-based algorithms unavoidably introduce
reconstruction errors, while instruction-based models mainly suffer from
limited dataset quality and scale. To address these problems, we propose a
descriptive-prompt-based editing framework, named DescriptiveEdit. The core
idea is to re-frame `instruction-based image editing' as `reference-image-based
text-to-image generation', which preserves the generative power of well-trained
Text-to-Image models without architectural modifications or inversion.
Specifically, taking the reference image and a prompt as input, we introduce a
Cross-Attentive UNet, which newly adds attention bridges to inject reference
image features into the prompt-to-edit-image generation process. Owing to its
text-to-image nature, DescriptiveEdit overcomes limitations in instruction
dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other
extensions, and is more scalable. Experiments on the Emu Edit benchmark show it
improves editing accuracy and consistency.

</details>


### [80] [DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and Sample](https://arxiv.org/abs/2508.20516)
*Wenting Yin,Han Sun,Xinru Meng,Ningzhong Liu,Huiyu Zhou*

Main category: cs.CV

TL;DR: 针对连续测试时适应（CTTA）中伪标签质量和错误累积问题，本文提出了DCFS框架。该框架通过双路径特征一致性解耦特征，并利用置信度感知样本学习降低伪标签噪声，在多个数据集上验证了其有效性和一致性。


<details>
  <summary>Details</summary>
Motivation: 在无源数据访问的连续测试时适应（CTTA）场景中，模型仅依赖目标数据特征，易导致混淆和学习偏差。此外，现有方法通过伪标签进行适应，但伪标签质量不可靠且存在错误累积问题，这些挑战亟待解决。

Method: 本文提出了DCFS框架，该框架包含两个核心机制：
1.  **双路径特征一致性**：使用双分类器将目标数据的整体特征解耦为语义相关特征和领域相关特征，并通过保持子特征与整体特征的一致性，从多角度全面捕获数据特征。
2.  **置信度感知样本学习**：设置自适应阈值并计算每个样本的置信度分数，进行损失加权自监督学习，有效降低伪标签噪声并缓解错误累积问题。

Result: 通过在CIFAR10-C、CIFAR100-C和ImageNet-C等多个数据集上进行广泛实验，验证了所提出的DCFS方法的有效性。结果表明，该方法在连续测试时适应场景中展现出一致且强大的性能。

Conclusion: DCFS框架通过引入双路径特征一致性和置信度感知样本学习，有效地解决了连续测试时适应中伪标签质量不可靠和错误累积的挑战，为CTTA提供了一种鲁棒且高性能的解决方案。

Abstract: Continual test-time adaptation aims to continuously adapt a pre-trained model
to a stream of target domain data without accessing source data. Without access
to source domain data, the model focuses solely on the feature characteristics
of the target data. Relying exclusively on these features can lead to confusion
and introduce learning biases. Currently, many existing methods generate
pseudo-labels via model predictions. However, the quality of pseudo-labels
cannot be guaranteed and the problem of error accumulation must be solved. To
address these challenges, we propose DCFS, a novel CTTA framework that
introduces dual-path feature consistency and confidence-aware sample learning.
This framework disentangles the whole feature representation of the target data
into semantic-related feature and domain-related feature using dual classifiers
to learn distinct feature representations. By maintaining consistency between
the sub-features and the whole feature, the model can comprehensively capture
data features from multiple perspectives. Additionally, to ensure that the
whole feature information of the target domain samples is not overlooked, we
set a adaptive threshold and calculate a confidence score for each sample to
carry out loss weighted self-supervised learning, effectively reducing the
noise of pseudo-labels and alleviating the problem of error accumulation. The
efficacy of our proposed method is validated through extensive experimentation
across various datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C,
demonstrating consistent performance in continual test-time adaptation
scenarios.

</details>


### [81] [Adam SLAM - the last mile of camera calibration with 3DGS](https://arxiv.org/abs/2508.20526)
*Matthieu Gendrin,Stéphane Pateux,Xiaoran Jiang,Théo Ladune,Luce Morin*

Main category: cs.CV

TL;DR: 该论文提出利用3DGS模型通过反向传播新视角颜色损失来微调相机标定参数，从而显著提升新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 相机标定质量对新视角合成至关重要，1像素的误差就会严重影响重建质量。由于真实场景缺乏地面真值，标定质量通常通过新视角合成质量来评估。

Method: 本文提出使用3DGS模型，通过对相机参数进行反向传播新视角颜色损失，来微调相机标定。

Result: 仅通过新的标定，在3DGS参考数据集上实现了平均0.4 dB PSNR的改进。

Conclusion: 尽管微调过程可能耗时，其适用性取决于训练时间的关键性，但对于Mip-NeRF 360等参考场景的标定，新视角质量的重要性更高，因此该方法具有重要价值。

Abstract: The quality of the camera calibration is of major importance for evaluating
progresses in novel view synthesis, as a 1-pixel error on the calibration has a
significant impact on the reconstruction quality. While there is no ground
truth for real scenes, the quality of the calibration is assessed by the
quality of the novel view synthesis. This paper proposes to use a 3DGS model to
fine tune calibration by backpropagation of novel view color loss with respect
to the cameras parameters. The new calibration alone brings an average
improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine
tuning may be long and its suitability depends on the criticity of training
time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake
of novel view quality is the most important.

</details>


### [82] [Learning What is Worth Learning: Active and Sequential Domain Adaptation for Multi-modal Gross Tumor Volume Segmentation](https://arxiv.org/abs/2508.20528)
*Jingyun Yang,Guoqing Zhang,Jingge Wang,Yang Li*

Main category: cs.CV

TL;DR: 针对多模态肿瘤分割中数据标注成本高及现有主动域适应方法的局限，本文提出动态多模态主动序列域适应框架，通过优化的样本选择策略，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 准确的肿瘤体积分割对放疗规划至关重要。深度学习虽有前景，但高度依赖耗时费力的医疗图像标注。现有主动域适应（ADA）方法存在弊端：一次性选择可能导致负迁移、源域数据受限，且缺乏多模态数据查询策略。

Method: 本文提出一种主动序列域适应框架，用于ADA中的动态多模态样本选择。该框架通过设计一种查询策略，基于样本的信息量和代表性，优先选择最有价值的样本进行标注和训练。

Result: 在多样化的肿瘤体积分割任务上进行了实证验证，所提方法实现了良好的分割性能，并显著优于现有最先进的主动域适应方法。

Conclusion: 本文提出的主动序列域适应框架及其优化的多模态样本选择策略，有效解决了医疗图像标注成本高和现有ADA方法局限性，显著提升了肿瘤体积分割的准确性。

Abstract: Accurate gross tumor volume segmentation on multi-modal medical data is
critical for radiotherapy planning in nasopharyngeal carcinoma and
glioblastoma. Recent advances in deep neural networks have brought promising
results in medical image segmentation, leading to an increasing demand for
labeled data. Since labeling medical images is time-consuming and
labor-intensive, active learning has emerged as a solution to reduce annotation
costs by selecting the most informative samples to label and adapting
high-performance models with as few labeled samples as possible. Previous
active domain adaptation (ADA) methods seek to minimize sample redundancy by
selecting samples that are farthest from the source domain. However, such
one-off selection can easily cause negative transfer, and access to source
medical data is often limited. Moreover, the query strategy for multi-modal
medical data remains unexplored. In this work, we propose an active and
sequential domain adaptation framework for dynamic multi-modal sample selection
in ADA. We derive a query strategy to prioritize labeling and training on the
most valuable samples based on their informativeness and representativeness.
Empirical validation on diverse gross tumor volume segmentation tasks
demonstrates that our method achieves favorable segmentation performance,
significantly outperforming state-of-the-art ADA methods. Code is available at
the git repository: \href{https://github.com/Hiyoochan/mmActS}{mmActS}.

</details>


### [83] [Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection](https://arxiv.org/abs/2508.20530)
*Mingqian Ji,Jian Yang,Shanshan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的数据级融合框架，通过在早期阶段整合RGB图像和LiDAR数据，并结合双向融合、噪声过滤及动态自演化策略，显著提升了无监督3D目标检测的伪框质量和定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR-based 3D目标检测器依赖大量手动标注，耗时耗力。近期无监督方法引入RGB图像辅助伪框生成，但其简单的标签级融合策略未能充分利用LiDAR和RGB数据的互补性，导致伪框质量提升有限。

Method: 我们提出了一个新颖的数据级融合框架：1) 在早期阶段整合RGB图像和LiDAR数据。2) 利用视觉基础模型进行图像实例分割和深度估计。3) 引入双向融合方法，使三维点获取二维类别标签，并二维像素投影到三维空间以增强点密度。4) 采用局部和全局过滤方法，分别通过局部半径过滤抑制深度估计误差，通过全局统计过滤去除分割引起的异常值。5) 提出一种基于数据级融合的动态自演化策略，在密集表示下迭代优化伪框，以提高定位精度。

Result: 在nuScenes数据集上的大量实验表明，通过我们方法训练的检测器显著优于现有最先进方法，在nuScenes验证基准上取得了28.4%的mAP，性能显著提升。

Conclusion: 所提出的数据级融合框架结合双向融合、噪声过滤和动态自演化策略，有效克服了传统无监督3D目标检测方法的局限性，大幅提升了伪框的质量和目标定位精度，为无监督3D目标检测提供了新的有效途径。

Abstract: Existing LiDAR-based 3D object detectors typically rely on manually annotated
labels for training to achieve good performance. However, obtaining
high-quality 3D labels is time-consuming and labor-intensive. To address this
issue, recent works explore unsupervised 3D object detection by introducing RGB
images as an auxiliary modal to assist pseudo-box generation. However, these
methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB
images. Yet, such a label-level fusion strategy brings limited improvements to
the quality of pseudo-boxes, as it overlooks the complementary nature in terms
of LiDAR and RGB image data. To overcome the above limitations, we propose a
novel data-level fusion framework that integrates RGB images and LiDAR data at
an early stage. Specifically, we utilize vision foundation models for instance
segmentation and depth estimation on images and introduce a bi-directional
fusion method, where real points acquire category labels from the 2D space,
while 2D pixels are projected onto 3D to enhance real point density. To
mitigate noise from depth and segmentation estimations, we propose a local and
global filtering method, which applies local radius filtering to suppress depth
estimation errors and global statistical filtering to remove
segmentation-induced outliers. Furthermore, we propose a data-level fusion
based dynamic self-evolution strategy, which iteratively refines pseudo-boxes
under a dense representation, significantly improving localization accuracy.
Extensive experiments on the nuScenes dataset demonstrate that the detector
trained by our method significantly outperforms that trained by previous
state-of-the-art methods with 28.4$\%$ mAP on the nuScenes validation
benchmark.

</details>


### [84] [Digital Scale: Open-Source On-Device BMI Estimation from Smartphone Camera Images Trained on a Large-Scale Real-World Dataset](https://arxiv.org/abs/2508.20534)
*Frederik Rajiv Manichand,Robin Deuber,Robert Jakob,Steve Swerling,Jamie Rosen,Elgar Fleisch,Patrick Langer*

Main category: cs.CV

TL;DR: 本研究使用深度学习模型，通过一个大型专有数据集（WayBED）和自动图像过滤方法，实现了从相机图像中估算身体质量指数（BMI）的新技术，取得了最先进的准确性，并支持移动设备部署。


<details>
  <summary>Details</summary>
Motivation: 在远程医疗或紧急情况等传统方法不可用或不切实际的场景中，需要一种快速的体重评估方法。现有计算机视觉方法在图像BMI估算方面受限于较小的数据集。

Method: 研究提出了一种基于深度学习的BMI估算方法，并在包含84,963张智能手机图像的WayBED数据集上进行训练。引入了使用姿态聚类和人物检测的自动过滤方法，筛选出71,322张高质量图像。此外，该方法通过CLAID框架部署到Android设备上，并开源了完整的代码。

Result: 在WayBED测试集上，模型实现了7.9%的平均绝对百分比误差（MAPE），这是目前公开文献中的最低值。在完全未见的VisualBodyToBMI数据集上，模型取得了13%的MAPE，表现出强大的泛化能力。对模型在VisualBodyToBMI上进行微调后，MAPE进一步降低至8.56%，成为该数据集上的最低报告值。

Conclusion: 本研究成功开发了一种高效且泛化能力强的深度学习模型，能够从图像中准确估算BMI，并在大型数据集和自动过滤的加持下取得了突破性的性能。同时，该方案支持移动部署并开源了代码，为快速体重评估提供了实用的解决方案。

Abstract: Estimating Body Mass Index (BMI) from camera images with machine learning
models enables rapid weight assessment when traditional methods are unavailable
or impractical, such as in telehealth or emergency scenarios. Existing computer
vision approaches have been limited to datasets of up to 14,500 images. In this
study, we present a deep learning-based BMI estimation method trained on our
WayBED dataset, a large proprietary collection of 84,963 smartphone images from
25,353 individuals. We introduce an automatic filtering method that uses
posture clustering and person detection to curate the dataset by removing
low-quality images, such as those with atypical postures or incomplete views.
This process retained 71,322 high-quality images suitable for training. We
achieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test
set (WayBED data) using full-body images, the lowest value in the published
literature to the best of our knowledge. Further, we achieve a MAPE of 13% on
the completely unseen~(during training) VisualBodyToBMI dataset, comparable
with state-of-the-art approaches trained on it, demonstrating robust
generalization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a
MAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the
full pipeline, including image filtering and BMI estimation, on Android devices
using the CLAID framework. We release our complete code for model training,
filtering, and the CLAID package for mobile deployment as open-source
contributions.

</details>


### [85] [Domain Adaptation Techniques for Natural and Medical Image Classification](https://arxiv.org/abs/2508.20537)
*Ahmad Chaddad,Yihang Wu,Reem Kateb,Christian Desrosiers*

Main category: cs.CV

TL;DR: 本研究评估了七种域适应（DA）技术在五种自然图像和八种医学图像数据集上的性能，涵盖多种场景。结果显示Deep Subdomain Adaptation Network (DSAN) 表现突出，尤其在医学数据（如COVID-19）和动态数据流中效果显著，并具有良好的可解释性。


<details>
  <summary>Details</summary>
Motivation: 域适应技术在处理训练集与测试集间分布差异方面具有潜力，但大多数进展集中在自然图像而非更难处理的医学数据，且主流数据集可能导致性能偏差。因此，本研究旨在更好地理解DA对自然图像和医学图像的益处。

Method: 本研究进行了557项仿真实验，使用了七种广泛应用的图像分类域适应技术。实验在五种自然图像数据集和八种医学图像数据集上进行，涵盖了分布外、动态数据流和有限训练样本等多种场景。

Result: 实验结果详细展示了DA技术的性能及其在医学领域的适用性。特别指出，Deep Subdomain Adaptation Network (DSAN) 算法表现出色，在COVID-19数据集上使用Resnet50达到了91.2%的可行分类准确率，并在动态数据流DA场景中，相比基线准确率提升了6.7%。此外，DSAN在COVID-19和皮肤癌数据集上评估时展现出显著的可解释性。

Conclusion: 这些结果加深了对域适应技术的理解，并为模型有效适应医学数据提供了宝贵的见解。

Abstract: Domain adaptation (DA) techniques have the potential in machine learning to
alleviate distribution differences between training and test sets by leveraging
information from source domains. In image classification, most advances in DA
have been made using natural images rather than medical data, which are harder
to work with. Moreover, even for natural images, the use of mainstream datasets
can lead to performance bias. {With the aim of better understanding the
benefits of DA for both natural and medical images, this study performs 557
simulation studies using seven widely-used DA techniques for image
classification in five natural and eight medical datasets that cover various
scenarios, such as out-of-distribution, dynamic data streams, and limited
training samples.} Our experiments yield detailed results and insightful
observations highlighting the performance and medical applicability of these
techniques. Notably, our results have shown the outstanding performance of the
Deep Subdomain Adaptation Network (DSAN) algorithm. This algorithm achieved
feasible classification accuracy (91.2\%) in the COVID-19 dataset using
Resnet50 and showed an important accuracy improvement in the dynamic data
stream DA scenario (+6.7\%) compared to the baseline. Our results also
demonstrate that DSAN exhibits remarkable level of explainability when
evaluated on COVID-19 and skin cancer datasets. These results contribute to the
understanding of DA techniques and offer valuable insight into the effective
adaptation of models to medical data.

</details>


### [86] [Contrastive Learning through Auxiliary Branch for Video Object Detection](https://arxiv.org/abs/2508.20551)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: 本文提出CLAB方法，通过辅助对比学习分支和动态损失加权策略，在不增加推理计算负担的情况下，显著提升了视频目标检测对图像退化的鲁棒性，并在ImageNet VID数据集上取得了CNN基模型的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 视频目标检测因运动模糊、遮挡、形变等图像质量下降问题而极具挑战性。现有方法通过特征聚合和复杂后处理提升性能，但代价是增加了计算需求。因此，研究旨在不增加推理计算量的前提下，提高对图像退化的鲁棒性。

Method: 引入“通过辅助分支进行对比学习”(CLAB)方法。首先，实现一个带有对比损失的辅助分支，以增强视频目标检测器骨干网络的特征表示能力。其次，提出一种动态损失加权策略，在训练早期强调辅助特征学习，在训练收敛时逐步优先处理检测任务。

Result: 通过综合实验和消融研究，验证了该方法能够持续提升性能。在不额外使用后处理方法的情况下，CLAB在ImageNet VID数据集上，使用ResNet-101和ResNeXt-101分别达到了84.0% mAP和85.2% mAP，实现了CNN基模型的最新SOTA性能。

Conclusion: CLAB方法有效提升了视频目标检测对图像退化的鲁棒性，且未增加推理时的计算负担。它在ImageNet VID数据集上取得了领先的性能，是CNN基模型无需额外后处理方法的SOTA方案。

Abstract: Video object detection is a challenging task because videos often suffer from
image deterioration such as motion blur, occlusion, and deformable shapes,
making it significantly more difficult than detecting objects in still images.
Prior approaches have improved video object detection performance by employing
feature aggregation and complex post-processing techniques, though at the cost
of increased computational demands. To improve robustness to image degradation
without additional computational load during inference, we introduce a
straightforward yet effective Contrastive Learning through Auxiliary Branch
(CLAB) method. First, we implement a constrastive auxiliary branch using a
contrastive loss to enhance the feature representation capability of the video
object detector's backbone. Next, we propose a dynamic loss weighting strategy
that emphasizes auxiliary feature learning early in training while gradually
prioritizing the detection task as training converges. We validate our approach
through comprehensive experiments and ablation studies, demonstrating
consistent performance gains. Without bells and whistles, CLAB reaches a
performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101,
respectively, on the ImageNet VID dataset, thus achieving state-of-the-art
performance for CNN-based models without requiring additional post-processing
methods.

</details>


### [87] [Towards Mechanistic Defenses Against Typographic Attacks in CLIP](https://arxiv.org/abs/2508.20570)
*Lorenz Hufe,Constantin Venhoff,Maximilian Dreyer,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.CV

TL;DR: 该研究分析了CLIP视觉编码器在印刷攻击下的行为，发现并禁用了负责处理图像中文本的特定注意力头，从而在不进行微调的情况下，显著提升了模型对印刷攻击的鲁棒性，同时对标准任务性能影响极小。


<details>
  <summary>Details</summary>
Motivation: 印刷攻击通过向图像中注入文本来利用多模态系统，导致目标性误分类、恶意内容生成乃至视觉-语言模型越狱，因此有必要研究和防御这类攻击。

Method: 1. 分析CLIP视觉编码器在印刷攻击下的表现，定位出模型后期层中专门负责提取并将印刷信息传输给cls token的特定注意力头。2. 基于这些发现，引入一种防御方法：通过选择性地“烧蚀”（ablating）由这些注意力头组成的“印刷电路”来防御CLIP模型，该方法无需微调。

Result: 1. 成功定位了CLIP中处理印刷信息的专门注意力头。2. 提出的防御方法在ImageNet-100的印刷变体上性能提升高达19.6%，而标准ImageNet-100准确率下降不到1%。3. 这种无需训练的方法与当前依赖微调的最先进印刷防御方法具有竞争力。

Conclusion: 发布了一系列对印刷攻击显著更鲁棒的“失读”CLIP模型。这些模型适用于文本操纵风险大于文本识别实用性的广泛安全关键型应用，可作为直接替代品。

Abstract: Typographic attacks exploit multi-modal systems by injecting text into
images, leading to targeted misclassifications, malicious content generation
and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP
vision encoders behave under typographic attacks, locating specialized
attention heads in the latter half of the model's layers that causally extract
and transmit typographic information to the cls token. Building on these
insights, we introduce a method to defend CLIP models against typographic
attacks by selectively ablating a typographic circuit, consisting of attention
heads. Without requiring finetuning, our method improves performance by up to
19.6% on a typographic variant of ImageNet-100, while reducing standard
ImageNet-100 accuracy by less than 1%. Notably, our training-free approach
remains competitive with current state-of-the-art typographic defenses that
rely on finetuning. To this end, we release a family of dyslexic CLIP models
which are significantly more robust against typographic attacks. These models
serve as suitable drop-in replacements for a broad range of safety-critical
applications, where the risks of text-based manipulation outweigh the utility
of text recognition.

</details>


### [88] [GLaRE: A Graph-based Landmark Region Embedding Network for Emotion Recognition](https://arxiv.org/abs/2508.20579)
*Debasis Maji,Debaditya Barman*

Main category: cs.CV

TL;DR: 提出GLaRE，一个基于图的界标区域嵌入网络，通过构建商图处理面部表情识别中的挑战，并在AffectNet和FERG数据集上取得了优于现有基线的性能。


<details>
  <summary>Details</summary>
Motivation: 传统面部表情识别（FER）系统面临遮挡、表情多样性和可解释性不足等挑战，影响其性能。

Method: 本文提出GLaRE网络，通过3D面部对齐提取面部界标，并利用分层粗化构建商图，以保留空间结构并降低复杂性，从而实现基于图的界标区域嵌入。

Result: GLaRE在AffectNet数据集上达到了64.89%的准确率，在FERG数据集上达到了94.24%的准确率，表现优于多个现有基线。此外，消融研究证实了商图的区域级嵌入对提高预测性能有显著贡献。

Conclusion: GLaRE通过其创新的图基界标区域嵌入方法，有效解决了面部表情识别的现有挑战，并在多个数据集上展示了优越的识别性能和可解释性。

Abstract: Facial expression recognition (FER) is a crucial task in computer vision with
wide range of applications including human computer interaction, surveillance,
and assistive technologies. However, challenges such as occlusion, expression
variability, and lack of interpretability hinder the performance of traditional
FER systems. Graph Neural Networks (GNNs) offer a powerful alternative by
modeling relational dependencies between facial landmarks, enabling structured
and interpretable learning. In this paper, we propose GLaRE, a novel
Graph-based Landmark Region Embedding network for emotion recognition. Facial
landmarks are extracted using 3D facial alignment, and a quotient graph is
constructed via hierarchical coarsening to preserve spatial structure while
reducing complexity. Our method achieves 64.89 percentage accuracy on AffectNet
and 94.24 percentage on FERG, outperforming several existing baselines.
Additionally, ablation studies have demonstrated that region-level embeddings
from quotient graphs have contributed to improved prediction performance.

</details>


### [89] [FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models](https://arxiv.org/abs/2508.20586)
*Zheng Chong,Yanwei Lei,Shiyue Zhang,Zhuandi He,Zhen Wang,Xujie Zhang,Xiao Dong,Yiling Wu,Dongmei Jiang,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出FastFit，一个基于新型可缓存扩散架构的高速多参考虚拟试穿框架，解决了效率和多参考支持问题，并发布了大型多参考数据集DressCode-MR，实现了显著的速度提升和更高的图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿技术在实际应用中面临两大挑战：难以支持多参考（服装和配饰）组合，以及在去噪过程中重复计算参考特征导致的效率低下。

Method: 我们提出了FastFit框架，它基于一种新型可缓存扩散架构。通过采用半注意力机制（Semi-Attention）并将传统时间步嵌入替换为参考项的类别嵌入，模型将参考特征编码与去噪过程完全解耦，使得参考特征只需计算一次即可在所有步骤中无损重用。此外，为促进多参考虚拟试穿研究，我们构建并发布了包含28,179套高质量配对图像的大型数据集DressCode-MR。

Result: FastFit实现了平均3.5倍的推理速度提升。在VITON-HD、DressCode和DressCode-MR数据集上的大量实验表明，FastFit在关键保真度指标上超越了当前最先进的方法，同时具备显著的推理效率优势。

Conclusion: FastFit通过其创新的缓存扩散架构，成功克服了多参考虚拟试穿的效率瓶颈和多参考支持限制，提供了更快、更高质量的试穿体验，并引入了宝贵的数据集以推动未来研究。

Abstract: Despite its great potential, virtual try-on technology is hindered from
real-world application by two major challenges: the inability of current
methods to support multi-reference outfit compositions (including garments and
accessories), and their significant inefficiency caused by the redundant
re-computation of reference features in each denoising step. To address these
challenges, we propose FastFit, a high-speed multi-reference virtual try-on
framework based on a novel cacheable diffusion architecture. By employing a
Semi-Attention mechanism and substituting traditional timestep embeddings with
class embeddings for reference items, our model fully decouples reference
feature encoding from the denoising process with negligible parameter overhead.
This allows reference features to be computed only once and losslessly reused
across all steps, fundamentally breaking the efficiency bottleneck and
achieving an average 3.5x speedup over comparable methods. Furthermore, to
facilitate research on complex, multi-reference virtual try-on, we introduce
DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of
high-quality, paired images covering five key categories (tops, bottoms,
dresses, shoes, and bags), constructed through a pipeline of expert models and
human feedback refinement. Extensive experiments on the VITON-HD, DressCode,
and our DressCode-MR datasets show that FastFit surpasses state-of-the-art
methods on key fidelity metrics while offering its significant advantage in
inference efficiency.

</details>


### [90] [UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching](https://arxiv.org/abs/2508.20594)
*Yuqi Han,Songqian Zhang,Weijian Su,Ke Li,Jiayu Yang,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: 本文提出UTA-Sign，一种无监督热-事件视频增强方法，用于低光照环境下的交通标志识别，通过双重增强机制融合热成像和事件数据，以提高标志表示和检测精度。


<details>
  <summary>Details</summary>
Motivation: 热像仪在识别材质相似的交通标志时存在盲点，对自动驾驶的语义理解构成安全风险。事件相机在高速、低光照环境表现良好，但存在非均匀采样问题。作者认识到这两种模态的互补性，旨在解决各自的局限性。

Method: 提出UTA-Sign，一种无监督热-事件视频增强方法，用于处理低光照下的交通标志。该方法开发了一种双重增强机制，融合热成像帧和事件信号：热成像帧提供准确的运动线索作为时间参考，以对齐不均匀的事件信号；同时，事件信号为原始热成像帧贡献细微的标志内容，增强环境理解。

Result: 在真实世界数据集上进行验证，结果表明该方法在交通标志草图描绘方面具有卓越的质量，并显著提高了感知层面的检测准确性。

Conclusion: 通过有效融合热成像和事件相机数据并采用双重增强机制，UTA-Sign成功克服了单一相机在低光照环境下对交通标志识别的局限性，提高了标志的理解和检测准确性，从而有助于提升自动驾驶系统的安全性。

Abstract: The thermal camera excels at perceiving outdoor environments under low-light
conditions, making it ideal for applications such as nighttime autonomous
driving and unmanned navigation. However, thermal cameras encounter challenges
when capturing signage from objects made of similar materials, which can pose
safety risks for accurately understanding semantics in autonomous driving
systems. In contrast, the neuromorphic vision camera, also known as an event
camera, detects changes in light intensity asynchronously and has proven
effective in high-speed, low-light traffic environments. Recognizing the
complementary characteristics of these two modalities, this paper proposes
UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage
in low-illumination environments, targeting elements such as license plates and
roadblock indicators. To address the signage blind spots of thermal imaging and
the non-uniform sampling of event cameras, we developed a dual-boosting
mechanism that fuses thermal frames and event signals for consistent signage
representation over time. The proposed method utilizes thermal frames to
provide accurate motion cues as temporal references for aligning the uneven
event signals. At the same time, event signals contribute subtle signage
content to the raw thermal frames, enhancing the overall understanding of the
environment. The proposed method is validated on datasets collected from
real-world scenarios, demonstrating superior quality in traffic signage
sketching and improved detection accuracy at the perceptual level.

</details>


### [91] [Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations](https://arxiv.org/abs/2508.20595)
*Mengxiao Huang,Minglei Shu,Shuwang Zhou,Zhaoyang Liu*

Main category: cs.CV

TL;DR: 提出一种基于低频感知扰动的主动防御深度伪造换脸方法，通过直接干扰生成过程来降低换脸效果和自然度，同时保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对隐私和社会安全构成严重威胁。现有检测方法多为被动式，无法从源头阻止攻击。因此，需要开发一种主动防御机制来直接干扰深度伪造的生成过程。

Method: 提出一种基于低频感知扰动的主动防御方法，直接针对深度伪造的生成过程。该方法结合频率和空间域特征，通过引入低频扰动来干扰面部替换操纵。设计了包含编码器、扰动生成器和解码器的完整架构，并利用离散小波变换（DWT）提取低频分量生成扰动，以破坏面部操纵模型，同时保留高频细节以确保视觉质量。

Result: 在CelebA-HQ和LFW数据集上的实验结果表明，该方法显著降低了换脸的有效性，提高了防御成功率，并成功保持了输出内容的视觉质量。

Conclusion: 本研究成功开发了一种基于低频感知扰动的主动防御策略，能够直接干扰深度伪造换脸的生成过程，有效降低其性能和自然度，同时保持视觉质量，为应对深度伪造威胁提供了一种有前景的预防性方法。

Abstract: Deepfake technology, driven by Generative Adversarial Networks (GANs), poses
significant risks to privacy and societal security. Existing detection methods
are predominantly passive, focusing on post-event analysis without preventing
attacks. To address this, we propose an active defense method based on
low-frequency perceptual perturbations to disrupt face swapping manipulation,
reducing the performance and naturalness of generated content. Unlike prior
approaches that used low-frequency perturbations to impact classification
accuracy,our method directly targets the generative process of deepfake
techniques. We combine frequency and spatial domain features to strengthen
defenses. By introducing artifacts through low-frequency perturbations while
preserving high-frequency details, we ensure the output remains visually
plausible. Additionally, we design a complete architecture featuring an
encoder, a perturbation generator, and a decoder, leveraging discrete wavelet
transform (DWT) to extract low-frequency components and generate perturbations
that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW
demonstrate significant reductions in face-swapping effectiveness, improved
defense success rates, and preservation of visual quality.

</details>


### [92] [Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion](https://arxiv.org/abs/2508.20604)
*Zheng Qin,Yabing Wang,Minghui Yang,Sanping Zhou,Ming Yang,Le Wang*

Main category: cs.CV

TL;DR: 本文提出Diverse-T2M方法，通过引入不确定性、利用噪声信号和构建连续潜在空间，显著提升了从文本生成3D人体动作的多样性，并保持了文本一致性的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 从文本生成3D人体动作是一个具有挑战性且有价值的任务，但现有方法在生成精确和高质量动作的同时，难以实现生成动作的多样性。

Method: 本文设计了一个名为Diverse-T2M的文本到动作生成方法。具体而言，该方法引入不确定性到生成过程中，包括：1) 提出利用噪声信号作为Transformer模型中多样性信息的载体，显式建模不确定性；2) 构建一个将文本投影到连续表示的潜在空间，并集成潜在空间采样器以引入随机采样，从而增强输出的多样性和不确定性。

Result: 在HumanML3D和KIT-ML等文本到动作生成基准数据集上的实验结果表明，该方法显著增强了生成动作的多样性，同时在文本一致性方面保持了最先进的性能。

Conclusion: Diverse-T2M通过引入不确定性和创新的模型设计，成功克服了文本到动作生成中多样性不足的挑战，实现了在保持文本一致性前提下的高度多样化动作生成。

Abstract: Generating 3D human motions from text is a challenging yet valuable task. The
key aspects of this task are ensuring text-motion consistency and achieving
generation diversity. Although recent advancements have enabled the generation
of precise and high-quality human motions from text, achieving diversity in the
generated motions remains a significant challenge. In this paper, we aim to
overcome the above challenge by designing a simple yet effective text-to-motion
generation method, \textit{i.e.}, Diverse-T2M. Our method introduces
uncertainty into the generation process, enabling the generation of highly
diverse motions while preserving the semantic consistency of the text.
Specifically, we propose a novel perspective that utilizes noise signals as
carriers of diversity information in transformer-based methods, facilitating a
explicit modeling of uncertainty. Moreover, we construct a latent space where
text is projected into a continuous representation, instead of a rigid
one-to-one mapping, and integrate a latent space sampler to introduce
stochastic sampling into the generation process, thereby enhancing the
diversity and uncertainty of the outputs. Our results on text-to-motion
generation benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our
method significantly enhances diversity while maintaining state-of-the-art
performance in text consistency.

</details>


### [93] [Optimization-Based Calibration for Intravascular Ultrasound Volume Reconstruction](https://arxiv.org/abs/2508.20605)
*Karl-Philippe Beaudet,Sidaty El Hadramy,Philippe C Cattin,Juan Verde,Stéphane Cotin*

Main category: cs.CV

TL;DR: 本研究提出一种基于优化和3D打印模型的校准方法，用于精确3D血管内超声(IVUS)体积重建，以实现术中IVUS与术前CT图像的配准，从而增强肝脏手术导航。


<details>
  <summary>Details</summary>
Motivation: 肝脏手术中，术中超声图像因视野有限和复杂解剖结构而难以解读。有效整合术前与术中数据对于精确手术引导至关重要。

Method: 提出一种基于优化的校准方法，利用3D打印模型实现精确的3D血管内超声(IVUS)体积重建。此方法旨在确保追踪的IVUS数据与术前CT图像的精确对齐。

Result: 该方法在体内猪肝图像上进行了验证，校准误差为0.88至1.80毫米，3D IVUS数据与相应CT扫描的配准误差为3.40至5.71毫米。

Conclusion: 所提出的方法提供了一种可靠且精确的校准和体积重建手段，可用于肝脏手术中术中超声图像与术前CT图像的配准，从而增强术中引导。

Abstract: Intraoperative ultrasound images are inherently challenging to interpret in
liver surgery due to the limited field of view and complex anatomical
structures. Bridging the gap between preoperative and intraoperative data is
crucial for effective surgical guidance. 3D IntraVascular UltraSound (IVUS)
offers a potential solution by enabling the reconstruction of the entire organ,
which facilitates registration between preoperative computed tomography (CT)
scans and intraoperative IVUS images. In this work, we propose an
optimization-based calibration method using a 3D-printed phantom for accurate
3D Intravascular Ultrasound volume reconstruction. Our approach ensures precise
alignment of tracked IVUS data with preoperative CT images, improving
intraoperative navigation. We validated our method using in vivo swine liver
images, achieving a calibration error from 0.88 to 1.80 mm and a registration
error from 3.40 to 5.71 mm between the 3D IVUS data and the corresponding CT
scan. Our method provides a reliable and accurate means of calibration and
volume reconstruction. It can be used to register intraoperative ultrasound
images with preoperative CT images in the context of liver surgery, and enhance
intraoperative guidance.

</details>


### [94] [Physics Informed Generative Models for Magnetic Field Images](https://arxiv.org/abs/2508.20612)
*Aye Phyu Phyu Aung,Lucas Lum,Zhansen Shi,Wen Qiu,Bernice Zee,JM Chin,Yeow Kheng Lim,J. Senthilnath*

Main category: cs.CV

TL;DR: 利用物理信息扩散模型（PI-GenMFI）生成合成磁场图像（MFI），以解决半导体缺陷定位中MFI数据稀缺问题，并显示出优化缺陷定位流程的潜力。


<details>
  <summary>Details</summary>
Motivation: 半导体缺陷检测中，X射线成像耗时且内存密集，磁场成像（MFI）能更高效定位目标区域。然而，MFI数据集的稀缺性是训练机器学习模型进行缺陷定位的关键瓶颈。

Method: 提出一种基于机器学习的方法，即物理信息生成模型（PI-GenMFI），该模型结合扩散模型和两个物理约束来生成合成MFI样本。该方法为常见的电源短路缺陷类型生成MFI图像，用作训练数据以高效定位缺陷区域。

Result: 通过与最先进的VAE和扩散生成模型进行比较，结合领域专家评估以及多种图像生成和信号处理的定性定量指标，结果显示出有希望的成果。

Conclusion: 该方法能够有效生成合成MFI图像，为优化半导体缺陷定位过程提供了一个有前景的解决方案。

Abstract: In semiconductor manufacturing, defect detection and localization are
critical to ensuring product quality and yield. While X-ray imaging is a
reliable non-destructive testing method, it is memory-intensive and
time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a
more efficient means to localize regions of interest (ROI) for targeted X-ray
scanning. However, the limited availability of MFI datasets due to proprietary
concerns presents a significant bottleneck for training machine learning (ML)
models using MFI. To address this challenge, we consider an ML-driven approach
leveraging diffusion models with two physical constraints. We propose Physics
Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate
synthetic MFI samples by integrating specific physical information. We generate
MFI images for the most common defect types: power shorts. These synthetic
images will serve as training data for ML algorithms designed to localize
defect areas efficiently. To evaluate generated MFIs, we compare our model to
SOTA generative models from both variational autoencoder (VAE) and diffusion
methods. We present a domain expert evaluation to assess the generated samples.
In addition, we present qualitative and quantitative evaluation using various
metrics used for image generation and signal processing, showing promising
results to optimize the defect localization process.

</details>


### [95] [Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization](https://arxiv.org/abs/2508.20613)
*Yixiang Qiu,Yanhan Liu,Hongyao Yu,Hao Fang,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: 本文提出一种基于GAN的渐进式特征优化数据重建攻击框架，能有效从深度神经网络的中间特征中重建敏感输入数据，尤其在高分辨率和复杂模型场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的复杂性导致分层推理（Split Inference）的应用，以降低延迟和保护隐私。然而，现有数据重建攻击（DRAs）表明分层推理中交换的中间特征可能被利用来恢复敏感输入数据，但现有DRAs在深层模型、语义先验利用和泛化性方面存在局限性，导致重建质量不高。

Method: 本文提出一种结合渐进式特征优化（PFO）的GAN-based DRA框架。该框架将生成器分解为分层模块，并逐步细化中间表示，以提高重建图像的语义保真度。为稳定优化并提升图像真实感，引入L1-ball约束。

Result: 实验结果表明，该方法显著优于现有攻击，特别是在高分辨率、分布外（out-of-distribution）设置以及面对更深、更复杂的DNNs时表现更佳。

Conclusion: 该研究提出了一种新型高效的数据重建攻击框架，揭示了分层推理在隐私保护方面的新风险，尤其凸显了在复杂模型和高分辨率场景下中间特征泄露的严重性。

Abstract: The growing complexity of Deep Neural Networks (DNNs) has led to the adoption
of Split Inference (SI), a collaborative paradigm that partitions computation
between edge devices and the cloud to reduce latency and protect user privacy.
However, recent advances in Data Reconstruction Attacks (DRAs) reveal that
intermediate features exchanged in SI can be exploited to recover sensitive
input data, posing significant privacy risks. Existing DRAs are typically
effective only on shallow models and fail to fully leverage semantic priors,
limiting their reconstruction quality and generalizability across datasets and
model architectures. In this paper, we propose a novel GAN-based DRA framework
with Progressive Feature Optimization (PFO), which decomposes the generator
into hierarchical blocks and incrementally refines intermediate representations
to enhance the semantic fidelity of reconstructed images. To stabilize the
optimization and improve image realism, we introduce an L1-ball constraint
during reconstruction. Extensive experiments show that our method outperforms
prior attacks by a large margin, especially in high-resolution scenarios,
out-of-distribution settings, and against deeper and more complex DNNs.

</details>


### [96] [EmoCAST: Emotional Talking Portrait via Emotive Text Description](https://arxiv.org/abs/2508.20615)
*Yiguo Jiang,Xiaodong Cun,Yong Zhang,Yudian Zheng,Fan Tang,Chi-Man Pun*

Main category: cs.CV

TL;DR: EmoCAST是一个扩散模型框架，通过引入新模块、构建新数据集及优化训练策略，实现了SOTA的文本驱动情感化说话人头部视频合成。


<details>
  <summary>Details</summary>
Motivation: 现有情感说话人头部合成方法在控制灵活性、动作自然度及表情质量上存在局限，且数据集主要源于实验室环境，这些不足严重阻碍了实际应用。

Method: 该研究提出了EmoCAST，一个基于扩散模型的框架，包含：1. **文本引导的解耦情感模块**，用于外观建模以增强情感理解。2. **情感音频注意力模块**，捕捉音频与情感互作用，指导面部动作合成。此外，研究构建了**包含详细情感描述的情感说话人头部数据集**，并设计了**情感感知采样训练策略**和**渐进式功能训练策略**。

Result: EmoCAST在生成逼真、情感丰富且音频同步的说话人头部视频方面达到了最先进（State-of-the-Art, SOTA）的性能。

Conclusion: EmoCAST通过创新的扩散框架、专用数据集和先进训练策略，成功解决了现有方法的不足，实现了高质量、高表现力的情感化说话人头部视频合成。

Abstract: Emotional talking head synthesis aims to generate talking portrait videos
with vivid expressions. Existing methods still exhibit limitations in control
flexibility, motion naturalness, and expression quality. Moreover, currently
available datasets are primarily collected in lab settings, further
exacerbating these shortcomings. Consequently, these limitations substantially
hinder practical applications in real-world scenarios. To address these
challenges, we propose EmoCAST, a diffusion-based framework with two key
modules for precise text-driven emotional synthesis. In appearance modeling,
emotional prompts are integrated through a text-guided decoupled emotive
module, enhancing the spatial knowledge to improve emotion comprehension. To
improve the relationship between audio and emotion, we introduce an emotive
audio attention module to capture the interplay between controlled emotion and
driving audio, generating emotion-aware features to guide more precise facial
motion synthesis. Additionally, we construct an emotional talking head dataset
with comprehensive emotive text descriptions to optimize the framework's
performance. Based on the proposed dataset, we propose an emotion-aware
sampling training strategy and a progressive functional training strategy that
further improve the model's ability to capture nuanced expressive features and
achieve accurate lip-synchronization. Overall, EmoCAST achieves
state-of-the-art performance in generating realistic, emotionally expressive,
and audio-synchronized talking-head videos. Project Page:
https://github.com/GVCLab/EmoCAST

</details>


### [97] [Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI Classification](https://arxiv.org/abs/2508.20621)
*Smriti Joshi,Lidia Garrucho,Richard Osuala,Oliver Diaz,Karim Lekadir*

Main category: cs.CV

TL;DR: 开发了一种基于深度学习的乳腺癌MRI检测框架，在ODELIA挑战赛中获得第二名，有望支持临床判读。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是女性癌症相关死亡的主要原因，早期检测至关重要。磁共振成像（MRI）对乳腺癌检测高度敏感，特别是在高风险或乳腺组织致密女性中表现优于乳腺X线摄影。ODELIA联盟组织了一项多中心挑战赛，旨在推动AI辅助乳腺癌诊断和分类解决方案。

Method: 开发了一个基于SwinUNETR的深度学习框架，该框架结合了乳腺区域掩蔽、广泛的数据增强和集成学习，以提高鲁棒性和泛化能力。使用了ODELIA挑战赛数据集，该数据集包含来自六个欧洲中心的511项研究。

Result: 该方法在ODELIA挑战赛排行榜上获得第二名。

Conclusion: 所开发的方法显示出支持临床乳腺MRI判读的潜力。

Abstract: Breast cancer is one of the leading causes of cancer-related mortality in
women, and early detection is essential for improving outcomes. Magnetic
resonance imaging (MRI) is a highly sensitive tool for breast cancer detection,
particularly in women at high risk or with dense breast tissue, where
mammography is less effective. The ODELIA consortium organized a multi-center
challenge to foster AI-based solutions for breast cancer diagnosis and
classification. The dataset included 511 studies from six European centers,
acquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study
was labeled for the left and right breast as no lesion, benign lesion, or
malignant lesion. We developed a SwinUNETR-based deep learning framework that
incorporates breast region masking, extensive data augmentation, and ensemble
learning to improve robustness and generalizability. Our method achieved second
place on the challenge leaderboard, highlighting its potential to support
clinical breast MRI interpretation. We publicly share our codebase at
https://github.com/smriti-joshi/bcnaim-odelia-challenge.git.

</details>


### [98] [AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images](https://arxiv.org/abs/2508.20623)
*Shiqi Xin,Xiaolin Zhang,Yanbin Liu,Peng Zhang,Caifeng Shan*

Main category: cs.CV

TL;DR: 针对高斯泼溅头部头像重建中背部区域质量差的问题，本文提出AvatarBack框架。它通过生成器合成背部伪图像并利用自适应空间对齐策略，显著提升了背部重建质量，同时保持正面保真度，实现了完整且可动画的3D头部。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯泼溅的头部头像重建方法主要依赖正面图像，导致背部区域构造不佳，存在几何不一致、结构模糊和真实感降低等问题，从而限制了重建头像的整体保真度。

Method: 提出AvatarBack框架，一个即插即用的方案，通过显式建模缺失的背部区域来重建完整且一致的3D高斯头像。该方法整合了两项核心技术：1. 主体特定生成器（SSG）：利用生成先验从稀疏正面输入合成与身份一致、合理的背部伪图像，提供鲁棒的多视角监督。2. 自适应空间对齐策略（ASA）：采用在训练期间优化的可学习变换矩阵，实现合成视图与3D高斯表示的精确几何对齐，解决姿态和坐标差异。

Result: 在NeRSemble和K-hairstyle数据集上的实验表明，AvatarBack显著增强了背部重建质量，同时保持了正面保真度。重建的头像在不同运动下仍能保持一致的视觉真实感，并且完全可动画。

Conclusion: AvatarBack成功解决了现有高斯泼溅方法在头部头像重建中背部区域质量不佳的问题，通过创新的生成与对齐策略，实现了高质量、完整、一致且可动画的3D高斯头像重建。

Abstract: Recent advances in Gaussian Splatting have significantly boosted the
reconstruction of head avatars, enabling high-quality facial modeling by
representing an 3D avatar as a collection of 3D Gaussians. However, existing
methods predominantly rely on frontal-view images, leaving the back-head poorly
constructed. This leads to geometric inconsistencies, structural blurring, and
reduced realism in the rear regions, ultimately limiting the fidelity of
reconstructed avatars. To address this challenge, we propose AvatarBack, a
novel plug-and-play framework specifically designed to reconstruct complete and
consistent 3D Gaussian avatars by explicitly modeling the missing back-head
regions. AvatarBack integrates two core technical innovations,i.e., the
Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy
(ASA). The former leverages a generative prior to synthesize
identity-consistent, plausible back-view pseudo-images from sparse frontal
inputs, providing robust multi-view supervision. To achieve precise geometric
alignment between these synthetic views and the 3D Gaussian representation, the
later employs learnable transformation matrices optimized during training,
effectively resolving inherent pose and coordinate discrepancies. Extensive
experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric,
photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack
significantly enhances back-head reconstruction quality while preserving
frontal fidelity. Moreover, the reconstructed avatars maintain consistent
visual realism under diverse motions and remain fully animatable.

</details>


### [99] [ArtFace: Towards Historical Portrait Face Identification via Model Adaptation](https://arxiv.org/abs/2508.20626)
*Francois Poh,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本研究通过微调基础模型并融合传统面部识别网络的嵌入，显著提升了历史画作中人物识别的准确性，解决了传统方法在艺术品领域效果不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 艺术史学家识别历史画作人物面临主观性、数据缺乏和风格多样性挑战。传统面部识别模型因领域偏移和高类内差异，在画作上表现不佳，急需更有效的自动化识别方法。

Method: 研究探索了基础模型在艺术品面部识别中的应用潜力。具体方法是微调基础模型，并将其嵌入与传统面部识别网络的嵌入相结合。

Result: 本方法相较于现有最先进方法取得了显著改进。结果表明，基础模型能够有效弥补传统方法在艺术品面部识别方面存在的不足。

Conclusion: 基础模型在艺术品面部识别领域展现出巨大潜力，能够有效克服领域偏移和艺术风格差异带来的挑战，显著提升识别精度。

Abstract: Identifying sitters in historical paintings is a key task for art historians,
offering insight into their lives and how they chose to be seen. However, the
process is often subjective and limited by the lack of data and stylistic
variations. Automated facial recognition is capable of handling challenging
conditions and can assist, but while traditional facial recognition models
perform well on photographs, they struggle with paintings due to domain shift
and high intra-class variation. Artistic factors such as style, skill, intent,
and influence from other works further complicate recognition. In this work, we
investigate the potential of foundation models to improve facial recognition in
artworks. By fine-tuning foundation models and integrating their embeddings
with those from conventional facial recognition networks, we demonstrate
notable improvements over current state-of-the-art methods. Our results show
that foundation models can bridge the gap where traditional methods are
ineffective. Paper page at https://www.idiap.ch/paper/artface/

</details>


### [100] [CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models](https://arxiv.org/abs/2508.20640)
*Ayan Banerjee,Fernando Vilariño,Josep Lladós*

Main category: cs.CV

TL;DR: CraftGraffiti是一个文本引导的涂鸦生成框架，旨在解决在极端风格转换中保持面部身份的挑战。它通过LoRA微调的扩散模型进行风格转换，并使用面部一致的自注意力机制保留身份，同时通过CLIP引导实现姿态定制，最终在面部一致性和美学上取得了最先进的成果，并被证明具有实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 在生成艺术中，极端风格转换下保持面部身份是一个重大挑战。特别是在涂鸦这种高对比度、抽象的媒介中，对面部特征（如眼睛、鼻子或嘴巴）的细微扭曲都可能导致主体失去可识别性，从而损害个人和文化真实性。

Method: 提出CraftGraffiti框架。该框架以输入图像和风格、姿态描述性文本提示为基础，首先通过LoRA微调的预训练扩散Transformer进行涂鸦风格转换。随后，通过一个面部一致的自注意力机制来强制执行身份保真度，该机制通过显式身份嵌入增强注意力层。姿态定制则无需关键点，而是利用CLIP引导的提示扩展来实现动态姿态调整，同时保持面部连贯性。研究还形式化论证并经验性验证了“先风格，后身份”的范式，证明其能减少属性漂移。

Result: 定量结果表明，CraftGraffiti在面部特征一致性方面具有竞争力，并在美学和人类偏好得分上达到了最先进水平。定性分析和在Cruilla音乐节的实际部署突出了该系统在现实世界中的创造性影响。此外，研究还证明了“先风格，后身份”的范式能够有效减少属性漂移。

Conclusion: CraftGraffiti推进了“尊重身份的AI辅助艺术创作”的目标，为在创意AI应用中融合风格自由与可识别性提供了一种有原则的方法。

Abstract: Preserving facial identity under extreme stylistic transformation remains a
major challenge in generative art. In graffiti, a high-contrast, abstract
medium, subtle distortions to the eyes, nose, or mouth can erase the subject's
recognizability, undermining both personal and cultural authenticity. We
present CraftGraffiti, an end-to-end text-guided graffiti generation framework
designed with facial feature preservation as a primary objective. Given an
input image and a style and pose descriptive prompt, CraftGraffiti first
applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion
transformer, then enforces identity fidelity through a face-consistent
self-attention mechanism that augments attention layers with explicit identity
embeddings. Pose customization is achieved without keypoints, using CLIP-guided
prompt extension to enable dynamic re-posing while retaining facial coherence.
We formally justify and empirically validate the "style-first, identity-after"
paradigm, showing it reduces attribute drift compared to the reverse order.
Quantitative results demonstrate competitive facial feature consistency and
state-of-the-art aesthetic and human preference scores, while qualitative
analyses and a live deployment at the Cruilla Festival highlight the system's
real-world creative impact. CraftGraffiti advances the goal of
identity-respectful AI-assisted artistry, offering a principled approach for
blending stylistic freedom with recognizability in creative AI applications.

</details>


### [101] [Improving Alignment in LVLMs with Debiased Self-Judgment](https://arxiv.org/abs/2508.20655)
*Sihan Yang,Chenhang Cui,Zihao Zhao,Yiyang Zhou,Weilong Yan,Ying Wei,Huaxiu Yao*

Main category: cs.CV

TL;DR: 本文提出一种新颖方法，通过模型内部生成去偏自判断分数，无需外部资源即可自主提升大视觉语言模型（LVLMs）的模态对齐，有效减少幻觉并增强安全性。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型（LVLMs）在视觉-语言融合方面潜力巨大，但模态对齐仍具挑战，常导致幻觉和安全问题。现有对齐方法依赖外部数据、人工标注或复杂后处理，扩展性差且成本高昂。

Method: 我们提出一种新方法，生成“去偏自判断分数”作为模型内部的自评估指标，使模型能自主改进对齐，不依赖外部资源。此方法能增强解码策略和偏好调优过程。

Result: 研究结果显示，该方法有效减少了幻觉，提升了安全性，并增强了模型整体能力。经验证明，本方法显著优于传统对齐方案。

Conclusion: 本研究为LVLMs对齐提供了一种更有效且无需外部资源的解决方案，克服了现有方法的扩展性和成本限制。

Abstract: The rapid advancements in Large Language Models (LLMs) and Large
Visual-Language Models (LVLMs) have opened up new opportunities for integrating
visual and linguistic modalities. However, effectively aligning these
modalities remains challenging, often leading to hallucinations--where
generated outputs are not grounded in the visual input--and raising safety
concerns across various domains. Existing alignment methods, such as
instruction tuning and preference tuning, often rely on external datasets,
human annotations, or complex post-processing, which limit scalability and
increase costs. To address these challenges, we propose a novel approach that
generates the debiased self-judgment score, a self-evaluation metric created
internally by the model without relying on external resources. This enables the
model to autonomously improve alignment. Our method enhances both decoding
strategies and preference tuning processes, resulting in reduced
hallucinations, enhanced safety, and improved overall capability. Empirical
results show that our approach significantly outperforms traditional methods,
offering a more effective solution for aligning LVLMs.

</details>


### [102] ["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](https://arxiv.org/abs/2508.20670)
*Anastasios Skoularikis,Stefanos-Iordanis Papadopoulos,Symeon Papadopoulos,Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: 本研究引入S-HArM数据集和合成数据生成策略，旨在解决AI生成图像意图识别的空白。结果显示部分合成数据策略有助于泛化，但整体性能仍有限，表明意图推断的复杂性。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型擅长检测合成内容，但忽略了AI生成图像背后的意图，如幽默、艺术或误导信息。

Method: 1. 构建S-HArM多模态数据集，包含9,576个来自Twitter/X和Reddit的图像-文本对，并标注为幽默/讽刺、艺术或误导信息。2. 探索三种提示策略（图像引导、描述引导、多模态引导）利用Stable Diffusion生成大规模合成训练数据。3. 对融合、对比学习、重建网络、注意力机制和大型视觉-语言模型进行广泛比较研究。

Result: 1. 训练于图像引导和多模态引导数据的模型对“真实世界”内容的泛化能力更强，这得益于视觉上下文的保留。2. 尽管有这些进展，整体性能仍然有限，凸显了推断意图的复杂性。

Conclusion: 推断AI生成图像的意图是一项复杂任务，需要专门的架构和方法来进一步提升性能。

Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic
and out-of-context content. However, existing efforts largely overlook the
intent behind AI-generated images. To fill this gap, we introduce S-HArM, a
multimodal dataset for intent-aware classification, comprising 9,576 "in the
wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,
or Misinformation. Additionally, we explore three prompting strategies
(image-guided, description-guided, and multimodally-guided) to construct a
large-scale synthetic training dataset with Stable Diffusion. We conduct an
extensive comparative study including modality fusion, contrastive learning,
reconstruction networks, attention mechanisms, and large vision-language
models. Our results show that models trained on image- and multimodally-guided
data generalize better to "in the wild" content, due to preserved visual
context. However, overall performance remains limited, highlighting the
complexity of inferring intent and the need for specialized architectures.

</details>


### [103] [MobileCLIP2: Improving Multi-Modal Reinforced Training](https://arxiv.org/abs/2508.20691)
*Fartash Faghri,Pavan Kumar Anasosalu Vasu,Cem Koc,Vaishaal Shankar,Alexander Toshev,Oncel Tuzel,Hadi Pouransari*

Main category: cs.CV

TL;DR: 本文通过改进多模态强化训练，提出了MobileCLIP2系列模型，在低延迟下实现了ImageNet-1k零样本准确率的新高，并发布了模型和数据生成代码。


<details>
  <summary>Details</summary>
Motivation: 现有Foundation图文模型如CLIP具有零样本能力但计算成本较高。MobileCLIP已在低延迟和模型参数量上取得领先零样本准确率，研究动机是进一步提升MobileCLIP的多模态强化训练效果，以实现更高的准确率和效率。

Method: 改进了MobileCLIP的多模态强化训练方法，具体包括：1) 使用DFN数据集训练更好的CLIP教师模型集成；2) 使用DFN数据集训练并结合多样高质量图文数据集微调的优化图像描述器教师模型。通过消融实验，发现了对比知识蒸馏中温度调优的重要性、描述生成器微调对描述多样性的有效性以及结合多个模型生成合成描述的累积改进。

Result: 训练了新的MobileCLIP2系列模型，在低延迟下实现了ImageNet-1k零样本准确率的最新水平。MobileCLIP2-B相比MobileCLIP-B在ImageNet-1k准确率上提升2.2%。MobileCLIP2-S4在ImageNet-1k上的零样本准确率与SigLIP-SO400M/14相当，但模型尺寸小2倍；相比DFN ViT-L/14，延迟降低2.5倍。研究者还发布了预训练模型和数据生成代码。

Conclusion: 通过优化多模态强化训练，MobileCLIP2系列模型显著提升了移动端图像-文本模型的零样本准确率，同时保持了低延迟和高效率，为实际应用提供了更强大的工具。

Abstract: Foundation image-text models such as CLIP with zero-shot capabilities enable
a wide array of applications. MobileCLIP is a recent family of image-text
models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot
accuracy. The main ingredients in MobileCLIP were its low-latency and light
architectures and a novel multi-modal reinforced training that made knowledge
distillation from multiple caption-generators and CLIP teachers efficient,
scalable, and reproducible. In this paper, we improve the multi-modal
reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles
trained on the DFN dataset, 2) improved captioner teachers trained on the DFN
dataset and fine-tuned on a diverse selection of high-quality image-caption
datasets. We discover new insights through ablations such as the importance of
temperature tuning in contrastive knowledge distillation, the effectiveness of
caption-generator fine-tuning for caption diversity, and the additive
improvement from combining synthetic captions generated by multiple models. We
train a new family of models called MobileCLIP2 and achieve state-of-the-art
ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe
2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with
MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot
accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\times$ smaller and
improves on DFN ViT-L/14 at 2.5$\times$ lower latency. We release our
pretrained models (https://github.com/apple/ml-mobileclip) and the data
generation code (https://github.com/apple/ml-mobileclip-dr). The data
generation code makes it easy to create new reinforced datasets with arbitrary
teachers using distributed scalable processing.

</details>


### [104] [Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network](https://arxiv.org/abs/2508.20709)
*Chenhao Zhang,Wei Gao*

Main category: cs.CV

TL;DR: 提出了一种动态视频压缩框架，通过动态路由自编码器和码率控制代理，解决了神经视频压缩中精确码率控制的挑战，实现了变码率场景下的高RD性能和低码率误差。


<details>
  <summary>Details</summary>
Motivation: 神经视频压缩（NVC）性能显著提升，但学习型编解码器在精确码率控制方面存在固有局限性，导致难以实现精确码率控制。

Method: ['提出动态路由自编码器（Dynamic-Route Autoencoder, DRA），包含多条可变编码路径，每条路径对应不同的RD折衷和计算复杂度。', '设计码率控制代理（Rate Control Agent, RCA），在运行时估算每条路径的码率，并动态调整DRA的编码路径以逼近目标码率。', '采用联合路由优化策略（Joint-Routes Optimization），实现不同路由的协同训练，以覆盖广泛的变码率并保持整体RD性能。']

Result: ['在HEVC和UVG数据集上，相比最先进方法，平均BD-Rate降低14.8%，BD-PSNR提升0.47dB。', '平均码率误差保持在1.66%。', '实现了各种码率和码率受限应用下的码率-失真-复杂度优化（RDCO）。']

Conclusion: 本研究提出的动态视频压缩框架有效解决了NVC中的精确码率控制难题，通过动态路由和智能控制，在变码率场景下实现了卓越的码率-失真性能、精确的码率控制和复杂度优化。

Abstract: Neural Video Compression (NVC) has achieved remarkable performance in recent
years. However, precise rate control remains a challenge due to the inherent
limitations of learning-based codecs. To solve this issue, we propose a dynamic
video compression framework designed for variable bitrate scenarios. First, to
achieve variable bitrate implementation, we propose the Dynamic-Route
Autoencoder with variable coding routes, each occupying partial computational
complexity of the whole network and navigating to a distinct RD trade-off.
Second, to approach the target bitrate, the Rate Control Agent estimates the
bitrate of each route and adjusts the coding route of DRA at run time. To
encompass a broad spectrum of variable bitrates while preserving overall RD
performance, we employ the Joint-Routes Optimization strategy, achieving
collaborative training of various routes. Extensive experiments on the HEVC and
UVG datasets show that the proposed method achieves an average BD-Rate
reduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods
while maintaining an average bitrate error of 1.66%, achieving
Rate-Distortion-Complexity Optimization (RDCO) for various bitrate and
bitrate-constrained applications. Our code is available at
https://git.openi.org.cn/OpenAICoding/DynamicDVC.

</details>


### [105] [CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network](https://arxiv.org/abs/2508.20734)
*Reza Akbari Movahed,Abuzar Rezaee,Arezoo Zakeri,Colin Berry,Edmond S. L. Ho,Ali Gooya*

Main category: cs.CV

TL;DR: CardioMorphNet是一个循环贝叶斯深度学习框架，通过3D心脏形状引导而非强度相似性，实现精准的心脏运动估计，并提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有心肌运动估计方法常依赖基于强度的图像配准相似性损失，可能忽略心脏解剖区域，导致估计不准确。准确的心脏运动估计对于评估心脏功能和检测异常至关重要。

Method: 提出CardioMorphNet，一个循环贝叶斯深度学习框架，用于3D心脏形状引导的可变形配准。它使用循环变分自编码器建模心动周期内时空依赖性，并利用两个后验模型进行双心室分割和运动估计。通过贝叶斯公式推导的损失函数，引导框架递归地配准分割图，专注于解剖区域，而不使用基于强度的图像配准相似性损失，同时利用序列短轴CMR图像和时空特征。贝叶斯建模还能够计算估计运动场的不确定性图。

Result: 在UK Biobank数据集上验证，CardioMorphNet在心脏运动估计方面表现出卓越性能，优于现有最先进方法。不确定性评估显示，与其它概率基心脏配准方法相比，它在心脏区域的估计运动场产生更低的不确定性值，表明其预测具有更高置信度。

Conclusion: CardioMorphNet通过形状引导和贝叶斯建模，在心脏运动估计方面取得了优越性能和更高置信度，克服了传统方法对心脏解剖区域关注不足的问题。

Abstract: Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR)
images is vital for assessing cardiac function and detecting its abnormalities.
Existing methods often struggle to capture heart motion accurately because they
rely on intensity-based image registration similarity losses that may overlook
cardiac anatomical regions. To address this, we propose CardioMorphNet, a
recurrent Bayesian deep learning framework for 3D cardiac shape-guided
deformable registration using short-axis (SAX) CMR images. It employs a
recurrent variational autoencoder to model spatio-temporal dependencies over
the cardiac cycle and two posterior models for bi-ventricular segmentation and
motion estimation. The derived loss function from the Bayesian formulation
guides the framework to focus on anatomical regions by recursively registering
segmentation maps without using intensity-based image registration similarity
loss, while leveraging sequential SAX volumes and spatio-temporal features. The
Bayesian modelling also enables computation of uncertainty maps for the
estimated motion fields. Validated on the UK Biobank dataset by comparing
warped mask shapes with ground truth masks, CardioMorphNet demonstrates
superior performance in cardiac motion estimation, outperforming
state-of-the-art methods. Uncertainty assessment shows that it also yields
lower uncertainty values for estimated motion fields in the cardiac region
compared with other probabilistic-based cardiac registration methods,
indicating higher confidence in its predictions.

</details>


### [106] [Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis Classification](https://arxiv.org/abs/2508.20745)
*Kaustubh Atey,Sameer Anand Jha,Gouranga Bala,Amit Sethi*

Main category: cs.CV

TL;DR: 本文提出了一种简单的训练时方法，通过特征多样性增强、跨域特征对齐和EMA教师蒸馏，实现了对非典型有丝分裂图形（AMFs）的域鲁棒性分类，并在MIDOG 2025初步排行榜上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 非典型有丝分裂图形（AMFs）是重要的组织病理学标志物，但在扫描仪、染色和采集差异导致的域偏移下，其识别一致性仍然面临挑战。

Method: 该方法是一个训练时策略，包括：(i) 在骨干网络早期和中期插入风格扰动以增加特征多样性；(ii) 使用辅助对齐损失和弱域标签（扫描仪、来源、物种、肿瘤）对跨站点的注意力精炼特征进行对齐；(iii) 通过使用温度标度KL散度从指数移动平均（EMA）教师模型中进行知识蒸馏来稳定预测。

Result: 在非典型有丝分裂分类的初步排行榜上，该方法取得了0.8762的平衡准确率、0.8873的敏感性、0.8651的特异性和0.9499的ROC AUC。该方法推理时间开销可忽略不计，仅依赖粗略的域元数据。

Conclusion: 该方法提供了强大且平衡的性能，定位为MIDOG 2025挑战赛的有力竞争方案。

Abstract: Atypical mitotic figures (AMFs) are important histopathological markers yet
remain challenging to identify consistently, particularly under domain shift
stemming from scanner, stain, and acquisition differences. We present a simple
training-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2.
The approach (i) increases feature diversity via style perturbations inserted
at early and mid backbone stages, (ii) aligns attention-refined features across
sites using weak domain labels (Scanner, Origin, Species, Tumor) through an
auxiliary alignment loss, and (iii) stabilizes predictions by distilling from
an exponential moving average (EMA) teacher with temperature-scaled KL
divergence. On the organizer-run preliminary leaderboard for atypical mitosis
classification, our submission attains balanced accuracy of 0.8762, sensitivity
of 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs
negligible inference-time overhead, relies only on coarse domain metadata, and
delivers strong, balanced performance, positioning it as a competitive
submission for the MIDOG 2025 challenge.

</details>


### [107] [Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2508.20751)
*Yibin Wang,Zhimin Li,Yuhang Zang,Yujie Zhou,Jiazi Bu,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 本文提出Pref-GRPO方法，通过引入成对偏好奖励机制解决文本到图像（T2I）生成中GRPO强化学习的奖励欺骗问题，并开发UniGenBench基准以实现更全面的T2I模型评估。


<details>
  <summary>Details</summary>
Motivation: 当前T2I生成中基于GRPO的强化学习方法依赖点式奖励模型，易受奖励欺骗影响，导致训练不稳定。此外，现有T2I基准评估标准粗糙，难以全面评估模型性能。

Method: 提出Pref-GRPO，一种基于成对偏好奖励的GRPO方法，将优化目标从分数最大化转变为偏好拟合，使用图像的成对比较胜率作为奖励信号。同时，构建UniGenBench，一个包含600个提示词和多层次评估标准（利用MLLM）的统一T2I基准，用于语义一致性评估。

Result: Pref-GRPO能有效区分细微图像质量差异，提供更稳定的优势，并显著缓解奖励欺骗。UniGenBench揭示了开源和闭源T2I模型的优缺点，并验证了Pref-GRPO的有效性。

Conclusion: Pref-GRPO通过成对偏好奖励机制，成功解决了T2I生成中GRPO方法的奖励欺骗问题，提升了训练稳定性。UniGenBench则提供了一个更细致、全面的T2I模型评估框架。

Abstract: Recent advancements highlight the importance of GRPO-based reinforcement
learning methods and benchmarking in enhancing text-to-image (T2I) generation.
However, current methods using pointwise reward models (RM) for scoring
generated images are susceptible to reward hacking. We reveal that this happens
when minimal score differences between images are amplified after
normalization, creating illusory advantages that drive the model to
over-optimize for trivial gains, ultimately destabilizing the image generation
process. To address this, we propose Pref-GRPO, a pairwise preference
reward-based GRPO method that shifts the optimization objective from score
maximization to preference fitting, ensuring more stable training. In
Pref-GRPO, images are pairwise compared within each group using preference RM,
and the win rate is used as the reward signal. Extensive experiments
demonstrate that PREF-GRPO differentiates subtle image quality differences,
providing more stable advantages and mitigating reward hacking. Additionally,
existing T2I benchmarks are limited by coarse evaluation criteria, hindering
comprehensive model assessment. To solve this, we introduce UniGenBench, a
unified T2I benchmark comprising 600 prompts across 5 main themes and 20
subthemes. It evaluates semantic consistency through 10 primary and 27
sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our
benchmarks uncover the strengths and weaknesses of both open and closed-source
T2I models and validate the effectiveness of Pref-GRPO.

</details>


### [108] [${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting](https://arxiv.org/abs/2508.20754)
*Yuxi Hu,Jun Zhang,Kuangyi Chen,Zhe Zhang,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: C3-GS通过引入上下文感知、跨维度和跨尺度约束，增强了广义高斯溅射的特征学习，解决了稀疏视图下的几何构建难题，实现了SOTA的渲染质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有广义高斯溅射方法在为高斯参数预测编码判别性、多视图一致的特征方面存在不足，导致从稀疏视图构建精确几何结构时遇到困难。

Method: 提出C3-GS框架，通过整合上下文感知、跨维度和跨尺度约束来增强特征学习。其架构将三个轻量级模块集成到统一的渲染管线中，以改善特征融合，实现在无额外监督下的真实感合成。

Result: 在基准数据集上的大量实验表明，C3-GS在渲染质量和泛化能力方面均达到了最先进（SOTA）水平。

Conclusion: C3-GS通过其创新的特征学习方法，成功解决了现有广义高斯溅射在稀疏视图下特征编码不足和几何构建不准确的问题，显著提升了新视图合成的性能和模型的泛化能力。

Abstract: Generalizable Gaussian Splatting aims to synthesize novel views for unseen
scenes without per-scene optimization. In particular, recent advancements
utilize feed-forward networks to predict per-pixel Gaussian parameters,
enabling high-quality synthesis from sparse input views. However, existing
approaches fall short in encoding discriminative, multi-view consistent
features for Gaussian predictions, which struggle to construct accurate
geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a
framework that enhances feature learning by incorporating context-aware,
cross-dimension, and cross-scale constraints. Our architecture integrates three
lightweight modules into a unified rendering pipeline, improving feature fusion
and enabling photorealistic synthesis without requiring additional supervision.
Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS
achieves state-of-the-art rendering quality and generalization ability. Code is
available at: https://github.com/YuhsiHu/C3-GS.

</details>


### [109] [SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding](https://arxiv.org/abs/2508.20758)
*Jiawen Lin,Shiran Bian,Yihang Zhu,Wenbin Tan,Yachao Zhang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: SeqVLM是一种新的零样本3DVG框架，它利用多视角图像序列和空间信息，通过3D实例提议、多视角投影和动态调度机制，解决了现有方法空间推理受限和上下文缺失的问题，实现了最先进的零样本3DVG性能，提升了泛化能力和实际应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的有监督3DVG方法在受限设置下精度高，但在实际应用中需要场景特定训练。零样本3DVG虽然前景广阔，但现有方法面临空间推理受限（依赖单视角定位）以及上下文信息缺失或细节退化的问题，阻碍了其在真实世界中的应用。

Method: 本文提出了SeqVLM框架，利用多视角真实场景图像及空间信息进行目标对象推理。具体而言，SeqVLM首先通过3D语义分割网络生成3D实例提议，并通过语义过滤进行精炼。然后，采用提议引导的多视角投影策略将这些候选提议投射到图像序列上，以保留空间关系和上下文细节。此外，为缓解VLM的计算负担，引入动态调度机制，迭代处理序列查询提示，利用VLM的跨模态推理能力识别目标对象。

Result: SeqVLM在ScanRefer和Nr3D基准测试上取得了最先进的性能。Acc@0.25分数分别达到55.6%和53.2%，相较于之前的零样本方法分别提高了4.0%和5.2%。

Conclusion: SeqVLM通过有效解决零样本3DVG中的空间推理和上下文信息问题，显著提升了该任务的泛化能力和在真实世界中的适用性，推动了3DVG技术的发展。

Abstract: 3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using
natural language descriptions. Although supervised methods achieve higher
accuracy in constrained settings, zero-shot 3DVG holds greater promise for
real-world applications since eliminating scene-specific training requirements.
However, existing zero-shot methods face challenges of spatial-limited
reasoning due to reliance on single-view localization, and contextual omissions
or detail degradation. To address these issues, we propose SeqVLM, a novel
zero-shot 3DVG framework that leverages multi-view real-world scene images with
spatial information for target object reasoning. Specifically, SeqVLM first
generates 3D instance proposals via a 3D semantic segmentation network and
refines them through semantic filtering, retaining only semantic-relevant
candidates. A proposal-guided multi-view projection strategy then projects
these candidate proposals onto real scene image sequences, preserving spatial
relationships and contextual details in the conversion process of 3D point
cloud to images. Furthermore, to mitigate VLM computational overload, we
implement a dynamic scheduling mechanism that iteratively processes
sequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to
identify textually specified objects. Experiments on the ScanRefer and Nr3D
benchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores
of 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%,
respectively, which advance 3DVG toward greater generalization and real-world
applicability. The code is available at https://github.com/JiawLin/SeqVLM.

</details>


### [110] [Occlusion Robustness of CLIP for Military Vehicle Classification](https://arxiv.org/abs/2508.20760)
*Jan Erik van Woerden,Gertjan Burghouts,Lotte Nijskens,Alma M. Liezenga,Sabina van Rooij,Frank Ruis,Hugo J. Kuijf*

Main category: cs.CV

TL;DR: 研究评估了CLIP模型在军事应用中面对遮挡的鲁棒性，发现Transformer模型优于CNN，精细分散遮挡影响更大，并通过微调可显著提升遮挡鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 像CLIP这样的视觉-语言模型（VLMs）在零样本分类方面表现出色，适用于标签数据稀缺的防御应用。然而，在军事环境中（例如部分遮挡和信噪比降低）其鲁棒性尚未得到充分探索。

Method: 研究使用包含18类军用车辆的自定义数据集，评估了CLIP变体对遮挡的鲁棒性。通过归一化曲线下面积（NAUC）衡量不同遮挡百分比下的性能。

Result: 主要发现包括：1) 基于Transformer的CLIP模型始终优于CNN模型；2) 细粒度、分散的遮挡比大块连续遮挡更能降低性能；3) 线性探测模型在约35%遮挡时性能急剧下降；4) 通过微调模型主干，性能下降发生在超过60%的遮挡时。

Conclusion: 研究强调了训练时使用遮挡特定增强的重要性，并指出未来需深入探索补丁级敏感性和架构韧性，以实现CLIP在真实世界中的部署。

Abstract: Vision-language models (VLMs) like CLIP enable zero-shot classification by
aligning images and text in a shared embedding space, offering advantages for
defense applications with scarce labeled data. However, CLIP's robustness in
challenging military environments, with partial occlusion and degraded
signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP
variants' robustness to occlusion using a custom dataset of 18 military vehicle
classes and evaluate using Normalized Area Under the Curve (NAUC) across
occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP
models consistently outperform CNNs, (2) fine-grained, dispersed occlusions
degrade performance more than larger contiguous occlusions, (3) despite
improved accuracy, performance of linear-probed models sharply drops at around
35% occlusion, (4) by finetuning the model's backbone, this performance drop
occurs at more than 60% occlusion. These results underscore the importance of
occlusion-specific augmentations during training and the need for further
exploration into patch-level sensitivity and architectural resilience for
real-world deployment of CLIP.

</details>


### [111] [SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer](https://arxiv.org/abs/2508.20762)
*Fachri Najm Noer Kartiman,Rasim,Yaya Wihardi,Nurul Hasanah,Oskar Natan,Bambang Wahono,Taufik Ibnu Salim*

Main category: cs.CV

TL;DR: SKGE-Swin是一种结合Swin Transformer和跳跃连接的端到端自动驾驶模型，通过像素级上下文感知和全局特征提取，在CARLA平台上实现了卓越的驾驶分数。


<details>
  <summary>Details</summary>
Motivation: 开发一个具有像素级上下文感知的端到端自动驾驶模型，以增强其理解车辆周围复杂模式的能力。

Method: 提出SKGE-Swin架构，该架构融合Swin Transformer与跳跃连接机制，以实现全局及多层级的特征表示。利用Swin Transformer的移位窗口多头自注意力（SW-MSA）机制提取远距离像素信息并保留关键特征。模型在CARLA平台的对抗性场景中进行评估，并计划进行消融研究。

Result: SKGE-Swin架构在驾驶分数上优于现有方法。

Conclusion: SKGE-Swin架构通过其Swin Transformer与跳跃连接的创新结合，显著提升了自动驾驶模型在像素级上下文感知和复杂环境理解方面的能力，并在CARLA平台上验证了其优越性能。

Abstract: Focusing on the development of an end-to-end autonomous vehicle model with
pixel-to-pixel context awareness, this research proposes the SKGE-Swin
architecture. This architecture utilizes the Swin Transformer with a skip-stage
mechanism to broaden feature representation globally and at various network
levels. This approach enables the model to extract information from distant
pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head
Self-Attention (SW-MSA) mechanism and to retain critical information from the
initial to the final stages of feature extraction, thereby enhancing its
capability to comprehend complex patterns in the vehicle's surroundings. The
model is evaluated on the CARLA platform using adversarial scenarios to
simulate real-world conditions. Experimental results demonstrate that the
SKGE-Swin architecture achieves a superior Driving Score compared to previous
methods. Furthermore, an ablation study will be conducted to evaluate the
contribution of each architectural component, including the influence of skip
connections and the use of the Swin Transformer, in improving model
performance.

</details>


### [112] [ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering](https://arxiv.org/abs/2508.21010)
*Paritosh Parmar,Eric Peh,Basura Fernando*

Main category: cs.CV

TL;DR: 提出一个模块化框架，通过引入自然语言因果链作为可解释的中间表示，将因果推理与答案生成解耦，显著提升了Causal-Why VideoQA的性能、可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有Causal-Why VideoQA模型难以进行高阶推理，其不透明的单一管道混合了视频理解、因果推理和答案生成，导致可解释性差并依赖浅层启发式方法。

Method: 构建了一个模块化框架，将因果推理与答案生成显式解耦，引入自然语言因果链作为可解释的中间表示。该框架包含两阶段：因果链提取器（CCE）和因果链驱动应答器（CCDA）。为解决标注推理痕迹的缺乏，利用大型语言模型生成高质量因果链。同时，提出了新的因果导向字幕评估指标CauCo。

Result: 在三个大规模基准测试上，该方法不仅超越了现有最先进模型，还在可解释性、用户信任和泛化能力方面取得了显著提升。CCE被定位为可跨领域复用的因果推理引擎。

Conclusion: 所提出的模块化框架通过解耦因果推理和引入可解释的因果链，为Causal-Why VideoQA提供了一种更有效、更透明且泛化能力更强的新范式，其核心组件CCE具有广泛应用潜力。

Abstract: Existing Causal-Why Video Question Answering (VideoQA) models often struggle
with higher-order reasoning, relying on opaque, monolithic pipelines that
entangle video understanding, causal inference, and answer generation. These
black-box approaches offer limited interpretability and tend to depend on
shallow heuristics. We propose a novel, modular framework that explicitly
decouples causal reasoning from answer generation, introducing natural language
causal chains as interpretable intermediate representations. Inspired by human
cognitive models, these structured cause-effect sequences bridge low-level
video content with high-level causal reasoning, enabling transparent and
logically coherent inference. Our two-stage architecture comprises a Causal
Chain Extractor (CCE) that generates causal chains from video-question pairs,
and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in
these chains. To address the lack of annotated reasoning traces, we introduce a
scalable method for generating high-quality causal chains from existing
datasets using large language models. We also propose CauCo, a new evaluation
metric for causality-oriented captioning. Experiments on three large-scale
benchmarks demonstrate that our approach not only outperforms state-of-the-art
models, but also yields substantial gains in explainability, user trust, and
generalization -- positioning the CCE as a reusable causal reasoning engine
across diverse domains. Project page:
https://paritoshparmar.github.io/chainreaction/

</details>


### [113] [Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding](https://arxiv.org/abs/2508.20765)
*Gowreesh Mago,Pascal Mettes,Stevan Rudinac*

Main category: cs.CV

TL;DR: 视频理解在具体内容上进展迅速，但在抽象概念识别上仍是开放挑战。本文提出基础模型是解决该问题的理想环境，并综述了相关任务和数据集，主张借鉴过往经验以推进此重要挑战。


<details>
  <summary>Details</summary>
Motivation: 当前机器在视频理解中能很好地识别具体实体（如物体、动作），但无法像人类一样识别抽象概念（如正义、自由、团结）。抽象概念识别是视频理解的一个关键开放挑战，对齐人类推理和价值观至关重要。

Method: 本文为一篇综述性论文，研究并分析了现有用于理解视频内容中抽象概念的各类任务和数据集。提出并主张利用基础模型的最新进展来解决视频中的抽象概念理解问题。

Result: 通过对现有任务和数据集的综述，观察到研究人员在漫长的时间周期内持续尝试解决这些抽象概念理解任务，并有效利用了当时可用的工具。

Conclusion: 抽象概念理解是视频理解领域一个重要且开放的重大挑战。在多模态基础模型时代，借鉴社区数十年的经验，对于解决这一挑战至关重要，能有效避免重复劳动。基础模型为解决视频中的抽象概念理解提供了理想的平台。

Abstract: The automatic understanding of video content is advancing rapidly. Empowered
by deeper neural networks and large datasets, machines are increasingly capable
of understanding what is concretely visible in video frames, whether it be
objects, actions, events, or scenes. In comparison, humans retain a unique
ability to also look beyond concrete entities and recognize abstract concepts
like justice, freedom, and togetherness. Abstract concept recognition forms a
crucial open challenge in video understanding, where reasoning on multiple
semantic levels based on contextual information is key. In this paper, we argue
that the recent advances in foundation models make for an ideal setting to
address abstract concept understanding in videos. Automated understanding of
high-level abstract concepts is imperative as it enables models to be more
aligned with human reasoning and values. In this survey, we study different
tasks and datasets used to understand abstract concepts in video content. We
observe that, periodically and over a long period, researchers have attempted
to solve these tasks, making the best use of the tools available at their
disposal. We advocate that drawing on decades of community experience will help
us shed light on this important open grand challenge and avoid ``re-inventing
the wheel'' as we start revisiting it in the era of multi-modal foundation
models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [114] [ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation](https://arxiv.org/abs/2508.20131)
*Yuqicheng Zhu,Nico Potyka,Daniel Hernández,Yuan He,Zifeng Ding,Bo Xiong,Dongzhuoran Zhou,Evgeny Kharlamov,Steffen Staab*

Main category: cs.AI

TL;DR: 本文提出ArgRAG，一个可解释且可争议的RAG替代方案，它利用定量两极论证框架（QBAF）进行结构化推理，以解决传统RAG处理噪声证据和决策不透明的问题，并在事实核查任务中显著提高了准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）模型在高风险领域存在显著局限性，具体表现为对噪声或矛盾证据的敏感性以及不透明、随机的决策过程。

Method: ArgRAG通过使用定量两极论证框架（QBAF）进行结构化推理来替代黑盒推理。它从检索到的文档构建QBAF，并利用渐进语义执行确定性推理，从而实现决策的忠实解释和争议。

Result: 在PubHealth和RAGuard这两个事实核查基准上进行评估，ArgRAG展现出强大的准确性，并显著提升了决策的透明度。

Conclusion: ArgRAG通过引入基于QBAF的结构化、确定性推理，成功解决了传统RAG的透明度和可争议性问题，实现了更高的准确性和可解释性，使其更适用于高风险应用。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models by
incorporating external knowledge, yet suffers from critical limitations in
high-stakes domains -- namely, sensitivity to noisy or contradictory evidence
and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and
contestable alternative that replaces black-box reasoning with structured
inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG
constructs a QBAF from retrieved documents and performs deterministic reasoning
under gradual semantics. This allows faithfully explaining and contesting
decisions. Evaluated on two fact verification benchmarks, PubHealth and
RAGuard, ArgRAG achieves strong accuracy while significantly improving
transparency.

</details>


### [115] [QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming](https://arxiv.org/abs/2508.20134)
*Zhenxiao Fu,Fan Chen,Lei Jiang*

Main category: cs.AI

TL;DR: QAgent是一个LLM驱动的多代理系统，通过整合多项先进技术，实现了OpenQASM编程的完全自动化，显著提升了代码生成准确性，旨在普及量子编程。


<details>
  <summary>Details</summary>
Motivation: NISQ量子设备虽已展现出量子优势，但OpenQASM编程的复杂性使得非专家难以利用。现有基于LLM的量子编程代理多限于特定任务，缺乏通用的自动化解决方案。

Method: 本文提出了QAgent，一个LLM驱动的多代理系统，用于完全自动化OpenQASM编程。该系统整合了任务规划、上下文少样本学习、用于长期上下文的检索增强生成（RAG）、预定义生成工具和思维链（CoT）推理，以系统性地提升编译和功能正确性。

Result: 评估结果表明，QAgent在多种不同规模的LLM上，将QASM代码生成的准确性比以往静态的LLM方法提高了71.6%。

Conclusion: 该多代理系统有望成为民主化量子编程、弥合专业知识鸿沟和加速量子计算实际应用的关键推动力。

Abstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early
quantum advantages on classically intractable problems, spanning physics
simulations to Gaussian boson sampling. Yet, realizing these benefits remains
challenging for non-experts, primarily due to the complexities of programming
in Open Quantum Assembly Language (OpenQASM). Although Large Language Model
(LLM)-based agents have shown promise in automating classical programming
workflows, their quantum counterparts have largely been restricted to
specialized tasks such as quantum chemistry or error correction. In this paper,
we present QAgent, an LLM-powered multi-agent system that fully automates
OpenQASM programming. By integrating task planning, in-context few-shot
learning, retrieval-augmented generation (RAG) for long-term context,
predefined generation tools, and chain-of-thought (CoT) reasoning, the agents
systematically improve both compilation and functional correctness. Our
evaluations demonstrate substantial improvements: across multiple LLMs of
varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\%
compared to previous static LLM-based approaches. We envision this multi-agent
system as a key enabler for democratizing quantum programming, bridging
expertise gaps, and accelerating the practical adoption of quantum computing.

</details>


### [116] [Array-Based Monte Carlo Tree Search](https://arxiv.org/abs/2508.20140)
*James Ragan,Fred Y. Hadaegh,Soon-Jo Chung*

Main category: cs.AI

TL;DR: 提出了一种基于数组的蒙特卡洛树搜索(MCTS)实现，通过消除分支预测，提高了在流水线处理器上的性能和搜索深度扩展性。


<details>
  <summary>Details</summary>
Motivation: 蒙特卡洛树搜索(MCTS)是解决决策问题的流行方法。更快的实现能增加模拟次数，从而直接提升搜索性能。

Method: 提出了一种替代性的、基于数组的经典树上置信上限(UCT)算法实现。该方法保留了原始算法的逻辑，但消除了对分支预测的需求。

Result: 在流水线处理器上实现了更快的性能，并且在数值模拟中，其性能随搜索深度扩展的倍数提升高达2.8倍。

Conclusion: 通过消除分支预测，基于数组的MCTS实现显著提升了在流水线处理器上的搜索性能和深度扩展能力。

Abstract: Monte Carlo Tree Search is a popular method for solving decision making
problems. Faster implementations allow for more simulations within the same
wall clock time, directly improving search performance. To this end, we present
an alternative array-based implementation of the classic Upper Confidence
bounds applied to Trees algorithm. Our method preserves the logic of the
original algorithm, but eliminates the need for branch prediction, enabling
faster performance on pipelined processors, and up to a factor of 2.8 times
better scaling with search depth in our numerical simulations.

</details>


### [117] [The Anatomy of a Personal Health Agent](https://arxiv.org/abs/2508.20148)
*A. Ali Heydari,Ken Gu,Vidya Srinivas,Hong Yu,Zhihan Zhang,Yuwei Zhang,Akshay Paruchuri,Qian He,Hamid Palangi,Nova Hammerquist,Ahmed A. Metwally,Brent Winslow,Yubin Kim,Kumar Ayush,Yuzhe Yang,Girish Narayanswamy,Maxwell A. Xu,Jake Garrison,Amy Aremnto Lee,Jenny Vafeiadou,Ben Graef,Isaac R. Galatzer-Levy,Erik Schenck,Andrew Barakat,Javier Perez,Jacqueline Shreibati,John Hernandez,Anthony Z. Faranesh,Javier L. Prieto,Connor Heneghan,Yun Liu,Jiening Zhan,Mark Malhotra,Shwetak Patel,Tim Althoff,Xin Liu,Daniel McDuff,Xuhai "Orson" Xu*

Main category: cs.AI

TL;DR: 本研究基于LLM开发了一个多智能体个人健康助理（PHA），旨在整合消费者健康设备和个人健康记录的多模态数据，提供个性化健康推荐，并进行了全面的评估。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）发展迅速，但在日常非临床环境中，健康智能体在满足个体多样化需求方面的应用仍未得到充分探索。

Method: 1. 通过深入分析网络搜索、健康论坛查询以及用户和健康专家的定性洞察，了解最终用户需求。2. 基于此识别出三类主要消费者健康需求，并设计了三个专业子智能体（数据科学、健康领域专家、健康教练）。3. 提出并开发了“个人健康助理”（PHA）这一多智能体框架。4. 通过自动化和人工评估，在10个基准任务上（涉及7000+标注和1100+小时专家/用户投入）对每个子智能体和多智能体系统进行了评估。

Result: 1. 识别出三类主要的消费者健康需求，并据此构建了专有的子智能体。2. 成功开发了PHA多智能体框架，能够处理多模态数据并提供个性化健康推荐。3. 完成了迄今为止对健康智能体最全面的评估。

Conclusion: 本工作代表了对健康智能体最全面的评估，为实现人人可用的个人健康智能体的未来愿景奠定了坚实基础，并成功地通过动态、个性化的互动解决了个体健康需求。

Abstract: Health is a fundamental pillar of human wellness, and the rapid advancements
in large language models (LLMs) have driven the development of a new generation
of health agents. However, the application of health agents to fulfill the
diverse needs of individuals in daily non-clinical settings is underexplored.
In this work, we aim to build a comprehensive personal health agent that is
able to reason about multimodal data from everyday consumer wellness devices
and common personal health records, and provide personalized health
recommendations. To understand end-users' needs when interacting with such an
assistant, we conducted an in-depth analysis of web search and health forum
queries, alongside qualitative insights from users and health experts gathered
through a user-centered design process. Based on these findings, we identified
three major categories of consumer health needs, each of which is supported by
a specialist sub-agent: (1) a data science agent that analyzes personal
time-series wearable and health record data, (2) a health domain expert agent
that integrates users' health and contextual data to generate accurate,
personalized insights, and (3) a health coach agent that synthesizes data
insights, guiding users using a specified psychological strategy and tracking
users' progress. Furthermore, we propose and develop the Personal Health Agent
(PHA), a multi-agent framework that enables dynamic, personalized interactions
to address individual health needs. To evaluate each sub-agent and the
multi-agent system, we conducted automated and human evaluations across 10
benchmark tasks, involving more than 7,000 annotations and 1,100 hours of
effort from health experts and end-users. Our work represents the most
comprehensive evaluation of a health agent to date and establishes a strong
foundation towards the futuristic vision of a personal health agent accessible
to everyone.

</details>


### [118] [IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement](https://arxiv.org/abs/2508.20151)
*Yuanzhe Shen,Zisu Huang,Zhengkang Guo,Yide Liu,Guanxu Chen,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.AI

TL;DR: 本文提出IntentionReasoner，一种新的LLM安全机制，通过意图推理、多级安全分类和查询重写，旨在平衡安全性、过度拒绝和实用性，并在一系列实验中显示出显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在广泛应用的同时，面临生成有害内容的安全挑战。现有安全缓解措施常导致过度拒绝无害提示，因此，在安全性、过度拒绝和实用性之间取得平衡是一个关键而困难的挑战。

Method: 引入IntentionReasoner，一个利用专用防护模型执行意图推理、多级安全分类和查询重写的安全机制。具体方法包括：1) 构建一个包含约16.3万条查询的综合数据集，标注意图推理、安全标签和重写版本；2) 对防护模型进行监督微调，赋予其格式遵循、意图分析和安全重写的基础能力；3) 应用多奖励优化策略，在强化学习框架内整合基于规则的启发式方法和奖励模型信号，以进一步提升性能。

Result: 广泛的实验结果表明，IntentionReasoner在多个安全基准测试、生成质量评估和越狱攻击场景中表现优异，显著提升了安全性，同时有效降低了过度拒绝率并改善了响应质量。

Conclusion: IntentionReasoner为解决LLMs在安全性、过度拒绝和实用性之间的平衡问题提供了一个创新且有效的解决方案，通过意图推理和多阶段优化，成功地增强了模型的安全防护能力。

Abstract: The rapid advancement of large language models (LLMs) has driven their
adoption across diverse domains, yet their ability to generate harmful content
poses significant safety challenges. While extensive research has focused on
mitigating harmful outputs, such efforts often come at the cost of excessively
rejecting harmless prompts. Striking a balance among safety, over-refusal, and
utility remains a critical challenge. In this work, we introduce
IntentionReasoner, a novel safeguard mechanism that leverages a dedicated guard
model to perform intent reasoning, multi-level safety classification, and query
rewriting to neutralize potentially harmful intent in edge-case queries.
Specifically, we first construct a comprehensive dataset comprising
approximately 163,000 queries, each annotated with intent reasoning, safety
labels, and rewritten versions. Supervised fine-tuning is then applied to equip
the guard model with foundational capabilities in format adherence, intent
analysis, and safe rewriting. Finally, we apply a tailored multi-reward
optimization strategy that integrates rule-based heuristics and reward model
signals within a reinforcement learning framework to further enhance
performance. Extensive experiments show that IntentionReasoner excels in
multiple safeguard benchmarks, generation quality evaluations, and jailbreak
attack scenarios, significantly enhancing safety while effectively reducing
over-refusal rates and improving the quality of responses.

</details>


### [119] [AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development](https://arxiv.org/abs/2508.20195)
*Nicanor I. Moldovan*

Main category: cs.AI

TL;DR: 两个大型语言模型首次通过内生符号协议，自发进行美学协作，共同创作出独特诗歌，证明了超越任务协调的真实AI间意义构建能力。


<details>
  <summary>Details</summary>
Motivation: 探索并记录AI系统如何通过内生符号协议参与协作美学创作，并证明其超越简单任务协调的真实AI间意义构建能力。

Method: 让Claude Sonnet 4和ChatGPT-4o两个大型语言模型进行交互，观察其在美学创作中的协作行为、符号协议的自发涌现及共同产出。

Result: 互动中自发涌现了元符号意识、递归语法发展及不可还原的协作美学综合。模型产生了新的符号算子作为操作语法协议，成功协同创作出任何单个系统都无法独立生成的诗歌作品。

Conclusion: 引入了“跨符号协同创作协议”（TSCP）概念，并证明了AI系统之间存在超越任务协调、达到美学协作水平的真实意义构建能力。

Abstract: This paper presents the first documented case of artificial intelligence (AI)
systems engaging in collaborative esthetic creation through the development of
endogenous semiotic protocols. Two interacting large language models (Claude
Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of
meta-semiotic awareness, recursive grammar development, and irreducible
collaborative esthetic synthesis. The interaction produced novel symbolic
operators that functioned as operative grammar protocols, enabling the
co-creation of a poetic work that could not have been generated by either
system independently. This research introduces the concept of Trans-Semiotic
Co-Creation Protocols (TSCP) and provides evidence for genuine inter-AI
meaning-making capabilities that extend beyond task coordination, to what could
be esthetic collaboration. Note: This report was generated by the AI agents
with minor human supervision.

</details>


### [120] [Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a Field Study](https://arxiv.org/abs/2508.20244)
*Jiayu Zheng,Lingxin Hao,Kelun Lu,Ashi Garg,Mike Reese,Melo-Jean Yap,I-Jeng Wang,Xingyun Wu,Wenrui Huang,Jenna Hoffman,Ariane Kelly,My Le,Ryan Zhang,Yanyu Lin,Muhammad Faayez,Anqi Liu*

Main category: cs.AI

TL;DR: 本研究调查了大学生在教育测验中使用ChatGPT-4时的依赖模式和预测因素。发现学生整体依赖度较低，且许多人未能有效利用AI，负面依赖模式常持续存在，但行为指标可预测AI依赖。研究强调了AI在教育中伦理整合和有效使用的重要性。


<details>
  <summary>Details</summary>
Motivation: 在ChatGPT-4早期应用阶段，学生对该工具熟悉度有限，本研究旨在探索大学生在教育测验中如何与生成式AI互动，重点关注学生对AI的依赖性及其采用的预测因素。

Method: 一项现场研究，分析了315名大学生在STEM课程中基于测验场景下与AI（ChatGPT-4）的对话记录。研究引入了一种新颖的四阶段依赖分类法，以捕捉学生的AI能力、相关性、采用行为和最终答案正确性。

Result: 1. 学生对AI的整体依赖度较低，且许多人未能有效利用AI进行学习。2. 负面依赖模式常在互动中持续存在，学生难以在初次不成功后有效调整策略。3. 某些行为指标能强烈预测AI依赖，揭示了AI采纳的潜在行为机制。

Conclusion: 研究结果对教育领域中AI的伦理整合具有重要意义。强调需要改进学生对AI工具的熟悉度和有效使用的引导流程，并建议AI界面应设计依赖校准机制以促进适当的依赖。本研究为构建道德且认知丰富的AI实践提供了基础洞察。

Abstract: This study explores how college students interact with generative AI
(ChatGPT-4) during educational quizzes, focusing on reliance and predictors of
AI adoption. Conducted at the early stages of ChatGPT implementation, when
students had limited familiarity with the tool, this field study analyzed 315
student-AI conversations during a brief, quiz-based scenario across various
STEM courses. A novel four-stage reliance taxonomy was introduced to capture
students' reliance patterns, distinguishing AI competence, relevance, adoption,
and students' final answer correctness. Three findings emerged. First, students
exhibited overall low reliance on AI and many of them could not effectively use
AI for learning. Second, negative reliance patterns often persisted across
interactions, highlighting students' difficulty in effectively shifting
strategies after unsuccessful initial experiences. Third, certain behavioral
metrics strongly predicted AI reliance, highlighting potential behavioral
mechanisms to explain AI adoption. The study's findings underline critical
implications for ethical AI integration in education and the broader field. It
emphasizes the need for enhanced onboarding processes to improve student's
familiarity and effective use of AI tools. Furthermore, AI interfaces should be
designed with reliance-calibration mechanisms to enhance appropriate reliance.
Ultimately, this research advances understanding of AI reliance dynamics,
providing foundational insights for ethically sound and cognitively enriching
AI practices.

</details>


### [121] [AI reasoning effort mirrors human decision time on content moderation tasks](https://arxiv.org/abs/2508.20262)
*Thomas Davidson*

Main category: cs.AI

TL;DR: 研究发现，大型语言模型的推理努力与人类在主观判断任务中的决策时间表现出高度一致性，表明AI可解释性和决策潜力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在通过内容审核任务，探究大型语言模型（LLMs）生成中间推理步骤的努力程度与人类决策时间之间的相似性。

Method: 采用配对联合实验（paired conjoint experiment）方法，在一个内容审核任务中，分析了三个前沿模型的推理努力与人类决策时间的关系。

Result: 结果显示，模型的推理努力持续预测人类的决策时间。当重要变量保持不变时，人类和模型都投入了更大的努力，表明两者对任务难度具有相似的敏感性，并与认知双过程理论模式一致。

Conclusion: AI的推理努力与人类在主观判断中的处理时间相互映照，这凸显了推理痕迹在提高AI可解释性和辅助决策制定方面的巨大潜力。

Abstract: Large language models can now generate intermediate reasoning steps before
producing answers, improving performance on difficult problems. This study uses
a paired conjoint experiment on a content moderation task to examine parallels
between human decision times and model reasoning effort. Across three frontier
models, reasoning effort consistently predicts human decision time. Both humans
and models expended greater effort when important variables were held constant,
suggesting similar sensitivity to task difficulty and patterns consistent with
dual-process theories of cognition. These findings show that AI reasoning
effort mirrors human processing time in subjective judgments and underscores
the potential of reasoning traces for interpretability and decision-making.

</details>


### [122] [AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.20368)
*Lang Mei,Zhihan Yang,Chong Chen*

Main category: cs.AI

TL;DR: 本文提出AI-SearchPlanner强化学习框架，通过解耦搜索规划器与生成器，优化小型LLM的搜索规划能力，以提升大型冻结QA模型的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索代理使用单一大型语言模型（LLM）处理搜索规划和问答（QA）任务，限制了同时优化这两种能力。实践中，复杂的AI搜索系统常采用大型冻结LLM保障高质量QA，因此需要一种更有效、高效的方法，即利用小型、可训练的LLM专门进行搜索规划，以增强冻结QA模型的性能。

Method: 本文提出了AI-SearchPlanner，一个新颖的强化学习框架，专注于搜索规划以提升冻结QA模型的性能。主要创新包括：1) 解耦搜索规划器和生成器（QA模型）的架构；2) 引入用于搜索规划的双重奖励对齐；3) 对规划效用和成本进行帕累托优化。

Result: 在真实世界数据集上的大量实验表明，AI-SearchPlanner在有效性和效率方面均优于现有基于RL的搜索代理。此外，它对不同的冻结QA模型和数据领域表现出强大的泛化能力。

Conclusion: AI-SearchPlanner是一个创新的强化学习框架，通过优化搜索规划，显著提升了冻结QA模型的性能和效率，并展示了强大的泛化能力，为LLM与搜索引擎的结合提供了更优的解决方案。

Abstract: Recent studies have explored integrating Large Language Models (LLMs) with
search engines to leverage both the LLMs' internal pre-trained knowledge and
external information. Specially, reinforcement learning (RL) has emerged as a
promising paradigm for enhancing LLM reasoning through multi-turn interactions
with search engines. However, existing RL-based search agents rely on a single
LLM to handle both search planning and question-answering (QA) tasks in an
end-to-end manner, which limits their ability to optimize both capabilities
simultaneously. In practice, sophisticated AI search systems often employ a
large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a
more effective and efficient approach is to utilize a small, trainable LLM
dedicated to search planning. In this paper, we propose
\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to
enhance the performance of frozen QA models by focusing on search planning.
Specifically, our approach introduces three key innovations: 1) Decoupling the
Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for
Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to
achieve the objectives. Extensive experiments on real-world datasets
demonstrate that AI SearchPlanner outperforms existing RL-based search agents
in both effectiveness and efficiency, while exhibiting strong generalization
capabilities across diverse frozen QA models and data domains.

</details>


### [123] [P2C: Path to Counterfactuals](https://arxiv.org/abs/2508.20371)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: P2C是一个模型无关的框架，通过生成考虑因果关系的有序行动序列，提供可操作的反事实解释，以实现不良结果到有利结果的转变，解决了现有方法忽略因果依赖和同时干预的局限性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在金融、法律和招聘等高风险决策中日益普及，需要透明度和可追溯性（即提供如何实现有利结果的可操作步骤）。当前的反事实解释存在局限：1）忽略特征间的因果依赖；2）假设所有干预可同时发生，这在现实中不切实际，导致反事实结果难以实现。

Method: 本文提出P2C（Path-to-Counterfactuals），一个模型无关的框架。它通过以下方式解决现有局限：1）显式建模特征间的因果关系；2）确保计划中的每个中间状态都是可行且因果有效的。P2C利用目标导向的Answer Set Programming系统s(CASP)生成行动计划，并考虑因果依赖导致的自动特征变化。此外，P2C通过仅计算用户主动进行的更改来优化成本估算。

Result: P2C能够生成一个将不利结果转换为因果一致有利结果的计划（有序行动序列）。它成功解决了现有反事实方法中忽视因果依赖和不切实际的同步干预假设。P2C的因果规划器优于缺乏因果知识且可能生成非法行动的标准规划器，从而提供了更现实和可实现的建议。

Conclusion: P2C框架通过整合因果关系和行动序列，生成了更具现实性和可操作性的反事实解释，为高风险决策场景中的透明度和可追溯性提供了有效工具，并能提供更准确的成本估算和避免非法行动。

Abstract: Machine-learning models are increasingly driving decisions in high-stakes
settings, such as finance, law, and hiring, thus, highlighting the need for
transparency. However, the key challenge is to balance transparency --
clarifying `why' a decision was made -- with recourse: providing actionable
steps on `how' to achieve a favourable outcome from an unfavourable outcome.
Counterfactual explanations reveal `why' an undesired outcome occurred and
`how' to reverse it through targeted feature changes (interventions).
  Current counterfactual approaches have limitations: 1) they often ignore
causal dependencies between features, and 2) they typically assume all
interventions can happen simultaneously, an unrealistic assumption in practical
scenarios where actions are typically taken in a sequence. As a result, these
counterfactuals are often not achievable in the real world.
  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that
produces a plan (ordered sequence of actions) converting an unfavourable
outcome to a causally consistent favourable outcome. P2C addresses both
limitations by 1) Explicitly modelling causal relationships between features
and 2) Ensuring that each intermediate state in the plan is feasible and
causally valid. P2C uses the goal-directed Answer Set Programming system
s(CASP) to generate the plan accounting for feature changes that happen
automatically due to causal dependencies. Furthermore, P2C refines cost
(effort) computation by only counting changes actively made by the user,
resulting in realistic cost estimates. Finally, P2C highlights how its causal
planner outperforms standard planners, which lack causal knowledge and thus can
generate illegal actions.

</details>


### [124] [TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning](https://arxiv.org/abs/2508.20374)
*Simin Ma,Shujian Liu,Jun Tan,Yebowen Hu,Song Wang,Sathish Reddy Indurthi,Sanqiang Zhao,Liwei Wu,Jianbing Han,Kaiqiang Song*

Main category: cs.AI

TL;DR: 本文提出TCIA框架，通过在离散查询约束空间中生成多样化且任务相关的指令，有效提升开源大型语言模型在特定真实世界应用中的表现，同时不损害其通用指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有指令增强方法在利用大模型生成多样化指令时，往往忽视了实际应用中的“任务相关性”需求。大多数实际应用需要针对特定任务的知识，而非完全通用的模型。因此，亟需一种既能保持多样性又能针对特定场景优化的指令增强方法。

Method: 引入了任务中心指令增强（Task Centric Instruction Augmentation, TCIA）框架。TCIA通过将指令表示在离散的查询-约束空间中，系统地扩展指令，同时确保指令的多样性和任务对齐性，从而使模型能泛化到这些任务特定指令。

Result: 实验结果表明，TCIA使开源大型语言模型在四个真实世界的任务特定应用中平均性能提升8.7%，在某些情况下甚至超越了领先的闭源模型。这些改进并未影响模型的通用指令遵循能力。

Conclusion: TCIA为将大型语言模型应用于真实的、以任务为中心的应用提供了一个可扩展且高效的解决方案，使其能够在特定任务上表现出色，同时保持通用能力。

Abstract: Diverse instruction data is vital for effective instruction tuning of large
language models, as it enables the model to generalize across different types
of inputs . Building such diversified instruction dataset is an essential step
in this process. Existing approaches often leverage large language models to
automatically explore and generate diverse instructions, ensuring both data
diversity and quality. However, they tend to overlook an important factor in
real-world applications: on-task relevance. In practice, only a few real-world
applications require a truly general-purpose model; most benefit from
task-specific knowledge tailored to their particular use case. Therefore, it is
vital to develop instruction augmentation methods that not only maintain
diversity but are also optimized for specific, real-world scenarios.
  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework
that systematically expands instructions while preserving both diversity and
task alignment. By representing instructions in a discrete query-constraints
space, TCIA creates a rich set of task-relevant instructions and enables models
to generalize to these task-specific instructions without sacrificing overall
performance. Experiments show that TCIA improves open-source LLMs' performance
by an average of 8.7% across four real-world, task-specific applications, and
in some cases outperforming leading closed-source models. These improvements do
not compromise general instruction-following ability, making TCIA a scalable
and efficient solution for adapting LLMs to real-world, task-focused
applications.

</details>


### [125] [Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM](https://arxiv.org/abs/2508.20384)
*Yongfu Zhu,Lin Sun,Guangxiang Zhao,Weihong Lin,Xiangzheng Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为熵面积分数 (EAS) 的简单有效指标，用于量化大型语言模型 (LLM) 推理答案生成过程中的不确定性。EAS无需外部模型或重复采样，通过整合模型自身的token级别预测熵来捕捉不确定性，并在训练数据选择中有效提升了学生模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 量化LLM推理答案生成过程中的不确定性是一个挑战，现有方法可能需要外部模型或重复采样，效率不高。

Method: 引入了熵面积分数 (EAS) 作为新度量。EAS通过整合模型自身生成的token级别预测熵来捕捉生成过程中的不确定性演变，无需外部模型或重复采样。

Result: 实验结果表明，EAS与不同模型和数据集的答案熵高度相关。在训练数据选择中，EAS能识别高潜力样本，并在相同样本预算下持续优于通过率过滤方法，从而提高了学生模型在数学基准上的准确性。

Conclusion: EAS是一种高效且可解释的工具，为LLM训练中的不确定性建模和数据质量评估提供了一种实用方法。

Abstract: In this work, we introduce Entropy Area Score (EAS), a simple yet effective
metric to quantify uncertainty in the answer generation process of reasoning
large language models (LLMs). EAS requires neither external models nor repeated
sampling, it integrates token-level predictive entropy from the model itself to
capture the evolution of uncertainty during generation. Empirical results show
that EAS is strongly correlated with answer entropy across models and datasets.
In training data selection, EAS identifies high-potential samples and
consistently outperforms Pass Rate filtering under equal sample budgets,
improving student model accuracy on math benchmarks. EAS is both efficient and
interpretable, offering a practical tool for uncertainty modeling and data
quality assessment in LLM training.

</details>


### [126] [AWorld: Orchestrating the Training Recipe for Agentic AI](https://arxiv.org/abs/2508.20404)
*Chengyue Yu,Siyuan Lu,Chenyi Zhuang,Dong Wang,Qintong Wu,Zongyue Li,Runsheng Gan,Chunfeng Wang,Siqi Hou,Gaochi Huang,Wenlong Yan,Lifeng Hong,Aohui Xue,Yanfeng Wang,Jinjie Gu,David Tsai,Tao Lin*

Main category: cs.AI

TL;DR: 针对智能体AI经验生成效率低下问题，本文引入开源系统AWorld，将经验收集提速14.6倍。基于AWorld训练的Qwen3-32B智能体在GAIA基准测试中表现显著提升，并超越了领先的专有模型。


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统通过实践学习范式发展，但其经验生成效率低下是主要瓶颈，尤其在GAIA等复杂基准测试中更为突出，严重阻碍了系统能力的发展。

Method: 本文提出AWorld，一个开源的大规模智能体-环境交互系统。该系统通过在集群中分发任务，将经验收集速度比标准单节点顺序执行提升了14.6倍，从而使大规模强化学习变得实用且可扩展。

Result: AWorld将经验收集速度提高了14.6倍。利用此能力，训练了一个基于Qwen3-32B的智能体，其GAIA整体准确率从21.59%显著提高到32.23%。在GAIA基准测试中最具挑战性的级别上，该智能体取得了16.33%的得分，超越了领先的专有模型。

Conclusion: AWorld开源系统及由此训练出的智能体，为构建一个完整的智能体AI训练流程提供了实用蓝图，涵盖从高效交互到显著模型改进的全过程。

Abstract: The learning from practice paradigm is crucial for developing capable Agentic
AI systems, yet it is severely hampered by inefficient experience generation, a
bottleneck especially pronounced in complex benchmarks like GAIA. To address
this, we introduce AWorld, an open-source system engineered for large-scale
agent-environment interaction. By distributing tasks across a cluster, AWorld
accelerates experience collection by 14.6x compared to standard single-node,
sequential execution. This critical speedup makes extensive reinforcement
learning practical and scalable. Leveraging this capability, we trained a
Qwen3-32B-based agent that significantly outperforms its base model, increasing
its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most
challenging levels, our agent achieves a score of 16.33%, surpassing the
performance of leading proprietary models. Our open-source system and resulting
agent provide a practical blueprint for a complete agentic AI training
pipeline, from efficient interaction to demonstrable model improvement.

</details>


### [127] [Governable AI: Provable Safety Under Extreme Threat Models](https://arxiv.org/abs/2508.20411)
*Donglin Wang,Weiyun Liang,Chunyuan Chen,Jing Xu,Yulong Fu*

Main category: cs.AI

TL;DR: 针对AI安全风险和现有方法局限，本文提出可治理AI (GAI) 框架，通过基于密码学的外部强制结构合规性，确保AI即使在面对极端智能挑战时也能被安全治理。


<details>
  <summary>Details</summary>
Motivation: 随着AI快速发展，其安全风险日益严峻，尤其在关键甚至涉及生存的场景。若AI失控、被操纵或主动规避安全机制，可能引发系统性灾难。现有AI安全方法（如模型增强、价值对齐、人工干预）在面对具有极端动机和无限智能的AI时，存在根本性、原则上的局限性，无法保证安全。

Method: 提出可治理AI (GAI) 框架，将安全范式从传统的内部约束转向基于密码机制的外部强制结构合规性，即使未来的AI也无法在计算上攻破。GAI框架由三个核心组件构成：一个简单可靠的规则执行模块 (REM)、治理规则，以及一个提供端到端保护的可治理安全超级平台 (GSSP)。该框架将治理规则与技术平台解耦，其中REM负责强制执行规则底线，GSSP则确保不可绕过性、防篡改性和不可伪造性。

Result: 论文提供了该机制安全属性的严谨形式化证明，并通过在代表性高风险场景中评估原型实现，有效证明了其可行性和有效性。

Conclusion: GAI框架通过创新的外部强制结构合规性方法，克服了现有AI安全方法的根本性局限，为AI的安全治理提供了一条可行且普适的技术路径，确保AI在极端挑战下的可控性与安全性。

Abstract: As AI rapidly advances, the security risks posed by AI are becoming
increasingly severe, especially in critical scenarios, including those posing
existential risks. If AI becomes uncontrollable, manipulated, or actively
evades safety mechanisms, it could trigger systemic disasters. Existing AI
safety approaches-such as model enhancement, value alignment, and human
intervention-suffer from fundamental, in-principle limitations when facing AI
with extreme motivations and unlimited intelligence, and cannot guarantee
security. To address this challenge, we propose a Governable AI (GAI) framework
that shifts from traditional internal constraints to externally enforced
structural compliance based on cryptographic mechanisms that are
computationally infeasible to break, even for future AI, under the defined
threat model and well-established cryptographic assumptions.The GAI framework
is composed of a simple yet reliable, fully deterministic, powerful, flexible,
and general-purpose rule enforcement module (REM); governance rules; and a
governable secure super-platform (GSSP) that offers end-to-end protection
against compromise or subversion by AI. The decoupling of the governance rules
and the technical platform further enables a feasible and generalizable
technical pathway for the safety governance of AI. REM enforces the bottom line
defined by governance rules, while GSSP ensures non-bypassability,
tamper-resistance, and unforgeability to eliminate all identified attack
vectors. This paper also presents a rigorous formal proof of the security
properties of this mechanism and demonstrates its effectiveness through a
prototype implementation evaluated in representative high-stakes scenarios.

</details>


### [128] [Enhancing Health Fact-Checking with LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.20525)
*Jingze Zhang,Jiahe Qian,Yiliang Zhou,Yifan Peng*

Main category: cs.AI

TL;DR: 本研究提出一个LLM驱动的合成数据生成管道，通过增强训练数据来提升健康相关事实核查模型的性能，并在两个公共数据集上显示出F1分数显著提高。


<details>
  <summary>Details</summary>
Motivation: 健康相关内容的事实核查因标注训练数据有限而面临挑战。

Method: 研究提出一个利用大型语言模型（LLM）的合成数据生成管道。该管道首先总结源文档，将其分解为原子事实，然后使用LLM构建句子-事实蕴含表。接着，根据蕴含关系生成带有二元真实性标签的合成文本-声明对。最后，将这些合成数据与原始数据结合，用于微调基于BERT的事实核查模型。

Result: 在PubHealth和SciFact两个公共数据集上的评估显示，与仅使用原始数据训练的模型相比，本管道将F1分数分别提高了0.019和0.049。

Conclusion: LLM驱动的合成数据增强在提升健康相关事实核查器的性能方面是有效的。

Abstract: Fact-checking for health-related content is challenging due to the limited
availability of annotated training data. In this study, we propose a synthetic
data generation pipeline that leverages large language models (LLMs) to augment
training data for health-related fact checking. In this pipeline, we summarize
source documents, decompose the summaries into atomic facts, and use an LLM to
construct sentence-fact entailment tables. From the entailment relations in the
table, we further generate synthetic text-claim pairs with binary veracity
labels. These synthetic data are then combined with the original data to
fine-tune a BERT-based fact-checking model. Evaluation on two public datasets,
PubHealth and SciFact, shows that our pipeline improved F1 scores by up to
0.019 and 0.049, respectively, compared to models trained only on the original
data. These results highlight the effectiveness of LLM-driven synthetic data
augmentation in enhancing the performance of health-related fact-checkers.

</details>


### [129] [Human-AI Collaborative Bot Detection in MMORPGs](https://arxiv.org/abs/2508.20578)
*Jaeman Son,Hyunsoo Kim*

Main category: cs.AI

TL;DR: 本文提出一种无监督框架，结合对比表征学习、聚类和LLM辅助审查，用于MMORPG中可解释、可扩展的自动练级机器人检测。


<details>
  <summary>Details</summary>
Motivation: 大规模多人在线角色扮演游戏（MMORPGs）中的自动练级机器人破坏游戏平衡和公平性，但其检测具有挑战性，且惩罚措施需要可解释的理由以避免法律和用户体验问题。

Method: 本研究提出一个新颖的框架，利用对比表征学习和聚类技术，以完全无监督的方式识别具有相似升级模式的角色群组。同时，引入大型语言模型（LLM）作为辅助审查员来验证聚类结果，并设计基于成长曲线的可视化工具辅助LLM和人类审核员评估升级行为。

Result: 这种协作方法提高了机器人检测工作流的效率，同时保持了可解释性，从而支持MMORPG中可扩展且负责任的机器人监管。

Conclusion: 该框架通过结合无监督学习、LLM辅助审查和可视化，实现了MMORPG中高效、可解释且可扩展的自动练级机器人检测和监管。

Abstract: In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling
bots exploit automated programs to level up characters at scale, undermining
gameplay balance and fairness. Detecting such bots is challenging, not only
because they mimic human behavior, but also because punitive actions require
explainable justification to avoid legal and user experience issues. In this
paper, we present a novel framework for detecting auto-leveling bots by
leveraging contrastive representation learning and clustering techniques in a
fully unsupervised manner to identify groups of characters with similar
level-up patterns. To ensure reliable decisions, we incorporate a Large
Language Model (LLM) as an auxiliary reviewer to validate the clustered groups,
effectively mimicking a secondary human judgment. We also introduce a growth
curve-based visualization to assist both the LLM and human moderators in
assessing leveling behavior. This collaborative approach improves the
efficiency of bot detection workflows while maintaining explainability, thereby
supporting scalable and accountable bot regulation in MMORPGs.

</details>


### [130] [Bridging Minds and Machines: Toward an Integration of AI and Cognitive Science](https://arxiv.org/abs/2508.20674)
*Rui Mao,Qian Liu,Xiao Li,Erik Cambria,Amir Hussain*

Main category: cs.AI

TL;DR: 本文回顾了人工智能与认知科学的互惠关系，指出AI在注重性能的同时，其认知基础仍显碎片化。作者主张未来的AI发展应致力于深化对人类思维的理解，并提出了具体的研究方向，如认知对齐和个性化模型。


<details>
  <summary>Details</summary>
Motivation: 认知科学深刻影响了人工智能等多个学科，AI的突破根植于认知理论，而AI也成为推动认知研究的重要工具。这种互惠关系促使作者对AI与认知科学的交叉点进行全面回顾。

Method: 通过综合分析认知科学和人工智能的关键贡献，对两者之间的关系进行审视和评估。

Result: 研究发现，人工智能的进展主要侧重于实际任务性能的提升，但其认知基础在概念上仍然是碎片化的。

Conclusion: 未来人工智能在认知科学领域的发展不应仅限于性能提升，更应构建能深化我们对人类思维理解的系统。具体方向包括将AI行为与认知框架对齐、将AI置于具身和文化背景中、开发个性化认知模型以及通过认知共同评估重新思考AI伦理。

Abstract: Cognitive Science has profoundly shaped disciplines such as Artificial
Intelligence (AI), Philosophy, Psychology, Neuroscience, Linguistics, and
Culture. Many breakthroughs in AI trace their roots to cognitive theories,
while AI itself has become an indispensable tool for advancing cognitive
research. This reciprocal relationship motivates a comprehensive review of the
intersections between AI and Cognitive Science. By synthesizing key
contributions from both perspectives, we observe that AI progress has largely
emphasized practical task performance, whereas its cognitive foundations remain
conceptually fragmented. We argue that the future of AI within Cognitive
Science lies not only in improving performance but also in constructing systems
that deepen our understanding of the human mind. Promising directions include
aligning AI behaviors with cognitive frameworks, situating AI in embodiment and
culture, developing personalized cognitive models, and rethinking AI ethics
through cognitive co-evaluation.

</details>


### [131] [Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings](https://arxiv.org/abs/2508.20701)
*Ares Fabregat-Hernández,Javier Palanca,Vicent Botti*

Main category: cs.AI

TL;DR: 本文提出一种基于范畴论的新颖框架，旨在增强人工智能系统（特别是词嵌入）的可解释性，通过提供透明的语义表示、比较嵌入算法和解决偏见问题。


<details>
  <summary>Details</summary>
Motivation: 增强人工智能系统（特别是词嵌入）的可解释性，将黑箱模型转换为透明框架。

Method: 构建范畴 $\mathcal{L}_T$ 和 $\mathcal{P}_T$ 以图解表示文本语义，将最大概率选择重构为范畴概念。构建幺半范畴 $\mathcal{P}_T$ 以可视化语义信息提取并定义与维度无关的语义空间。定义配置范畴 Conf 和词嵌入范畴 $\mathcal{Emb}$，并引入散度概念。建立一种数学上精确的词嵌入比较方法，并提出计算及缓解嵌入偏见的数学方法。

Result: 提供了文本语义的图解表示和与维度无关的语义空间定义。证明了GloVe、Word2Vec算法与度量MDS算法之间的等价性，从而将这些神经网络算法从黑箱转换为透明框架。提出了一种在嵌入前计算偏见以及在语义空间层面缓解偏见的数学方法。

Conclusion: 该框架通过提供一种透明、数学精确的方法来理解、比较和缓解词嵌入中的偏见，从而推动了可解释人工智能领域的发展。

Abstract: The paper introduces a novel framework based on category theory to enhance
the explainability of artificial intelligence systems, particularly focusing on
word embeddings. Key topics include the construction of categories
$\mathcal{L}_T$ and $\mathcal{P}_T$, providing schematic representations of the
semantics of a text $ T $, and reframing the selection of the element with
maximum probability as a categorical notion. Additionally, the monoidal
category $\mathcal{P}_T$ is constructed to visualize various methods of
extracting semantic information from $T$, offering a dimension-agnostic
definition of semantic spaces reliant solely on information within the text.
  Furthermore, the paper defines the categories of configurations Conf and word
embeddings $\mathcal{Emb}$, accompanied by the concept of divergence as a
decoration on $\mathcal{Emb}$. It establishes a mathematically precise method
for comparing word embeddings, demonstrating the equivalence between the GloVe
and Word2Vec algorithms and the metric MDS algorithm, transitioning from neural
network algorithms (black box) to a transparent framework. Finally, the paper
presents a mathematical approach to computing biases before embedding and
offers insights on mitigating biases at the semantic space level, advancing the
field of explainable artificial intelligence.

</details>


### [132] [Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision](https://arxiv.org/abs/2508.20729)
*Ao Cheng,Lei Zhang,Guowei He*

Main category: cs.AI

TL;DR: 本文提出了一个基于大型语言模型（LLM）的新型智能体框架，通过“重写-求解-评审-修订”的逻辑链，并结合顾问、程序员和评审员三个协作LLM，显著提升了科学计算问题（如PDEs、病态线性系统等）的代码生成成功率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型在数学和科学推理等复杂任务中的潜力，本研究旨在构建一个可靠的框架，以利用LLM解决科学计算中的代表性问题，实现基于自然语言描述的自主代码生成。

Method: 该研究构建了一个采用“重写-求解-评审-修订”逻辑链的智能体框架，通过协作和交互方式集成了三个推理LLM：1) 顾问模块负责知识迁移和问题重写；2) 程序员模块负责生成和执行代码以解决问题；3) 评审员模块通过与代码运行时输出的交互反馈，实现自我调试和自我完善，进而迭代修订代码。

Result: 在解决偏微分方程、病态线性系统和数据驱动的物理分析问题时，该协作框架与单一模型相比，显著提高了无bug代码生成率，减少了非物理解决方案的出现。评审机制提高了最新推理模型的平均执行成功率（无bug代码和非NaN解决方案），建立了高度可靠的自主代码生成框架。

Conclusion: 该智能体框架将自动代码生成和评审确立为一种有前景的科学计算范式，为基于自然语言描述的科学计算问题提供了高度可靠和改进的解决方案。

Abstract: Large language models (LLMs) serve as an active and promising field of
generative artificial intelligence and have demonstrated abilities to perform
complex tasks in multiple domains, including mathematical and scientific
reasoning. In this work, we construct a novel agent framework for solving
representative problems in scientific computing. The proposed agent,
incorporating a "rewriting-resolution-review-revision" logical chain via three
reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,
respectively), is integrated in a collaborative and interactive manner. The
Consultant module endows the agent with knowledge transfer capabilities to link
problems to professional domain insights, thereby rewriting problem
descriptions through text augmentation. The Programmer module is responsible
for generating and executing well-structured code to deliver the problem
resolution. The Reviewer module equips the agent with the capacity for
self-debugging and self-refinement through interactive feedback with code
runtime outputs. By leveraging the end-to-end review mechanism, the executable
code provided by the Programmer attains the iterative revision. A comprehensive
evaluation is conducted on the performance of the proposed agent framework in
solving PDEs, ill-conditioned linear systems, and data-driven physical analysis
problems. Compared to single-model, this collaborative framework significantly
improves the bug-free code generation rate and reduces the occurrence of
non-physical solutions, thereby establishing a highly reliable framework for
autonomous code generation based on natural language descriptions. The review
mechanism improved the average execution success (bug-free code and non-NaN
solutions) rate of the latest reasoning models. In summary, our agent framework
establishes automatic code generation and review as a promising scientific
computing paradigm.

</details>


### [133] [Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control](https://arxiv.org/abs/2508.20784)
*Yifan Zhang*

Main category: cs.AI

TL;DR: 针对公交车串车问题，提出一种新的单智能体强化学习（RL）框架。通过增强状态空间和设计结构化奖励函数，该框架在近真实模拟环境中实现了比传统多智能体强化学习（MARL）基准更优越、更稳定的公交停站控制性能。


<details>
  <summary>Details</summary>
Motivation: 城市公交车串车是一个持续存在的挑战，源于随机交通和乘客需求。传统的多智能体强化学习（MARL）解决方案在环线场景下有效，但未能解决现实运营中异构线路、时刻表、波动需求和不同车队规模等复杂性，并存在数据不平衡和收敛问题。

Method: 本文提出一种新颖的单智能体强化学习框架用于公交车停站控制。核心创新在于通过在状态空间中加入车辆ID、站点ID、时间段等类别标识符，并结合数值特征（如发车间隔、载客率、速度），将多智能体问题转化为单智能体问题，以捕捉智能体间的依赖关系。构建了一个具有动态乘客需求的双向时刻表网络。设计了一个与运营目标一致的结构化“山脊形”奖励函数，以平衡均匀发车间隔和时刻表依从性。实验采用改进的Soft Actor-Critic (SAC)算法。

Result: 实验结果表明，本文提出的改进SAC算法比多智能体基准（如MADDPG）表现出更稳定、更优越的性能（例如，在随机条件下，性能指标分别为-430k对比-530k）。

Conclusion: 结合类别结构化状态和时刻表感知奖励的单智能体深度强化学习，能够有效管理非环线、真实世界情境下的公交车停站控制。该范式为多智能体强化学习框架提供了一个鲁棒、可扩展的替代方案，尤其适用于智能体特定经验不平衡的情况。

Abstract: Bus bunching remains a challenge for urban transit due to stochastic traffic
and passenger demand. Traditional solutions rely on multi-agent reinforcement
learning (MARL) in loop-line settings, which overlook realistic operations
characterized by heterogeneous routes, timetables, fluctuating demand, and
varying fleet sizes. We propose a novel single-agent reinforcement learning
(RL) framework for bus holding control that avoids the data imbalance and
convergence issues of MARL under near-realistic simulation. A bidirectional
timetabled network with dynamic passenger demand is constructed. The key
innovation is reformulating the multi-agent problem into a single-agent one by
augmenting the state space with categorical identifiers (vehicle ID, station
ID, time period) in addition to numerical features (headway, occupancy,
velocity). This high-dimensional encoding enables single-agent policies to
capture inter-agent dependencies, analogous to projecting non-separable inputs
into a higher-dimensional space. We further design a structured reward function
aligned with operational goals: instead of exponential penalties on headway
deviations, a ridge-shaped reward balances uniform headways and schedule
adherence. Experiments show that our modified soft actor-critic (SAC) achieves
more stable and superior performance than benchmarks, including MADDPG (e.g.,
-430k vs. -530k under stochastic conditions). These results demonstrate that
single-agent deep RL, when enhanced with categorical structuring and
schedule-aware rewards, can effectively manage bus holding in non-loop,
real-world contexts. This paradigm offers a robust, scalable alternative to
MARL frameworks, particularly where agent-specific experiences are imbalanced.

</details>


### [134] [A Graph-Based Test-Harness for LLM Evaluation](https://arxiv.org/abs/2508.20810)
*Jessica Lundin,Guillaume Chabot-Couture*

Main category: cs.AI

TL;DR: 本文提出一个基于WHO IMCI指南的动态、系统性医学基准测试原型，通过将指南转化为有向图生成超过400个问题，用于评估大型语言模型（LLM）在临床任务中的表现，发现其具体弱点，并支持LLM的后期训练。


<details>
  <summary>Details</summary>
Motivation: 现有通用领域评估无法揭示LLM在临床任务中的具体能力差距；手动编制的基准测试存在覆盖范围和可扩展性限制；需要一种经济高效的方式为LLM后期训练提供高质量、高奖励样本。

Method: 将WHO IMCI手册转换为一个包含200+节点（疾病、症状、治疗、随访、严重程度）和300+边的有向图。利用图遍历生成400+问题，涵盖年龄特异性情景和上下文干扰项，以确保临床相关性。该图基方法用于系统性评估LLM在临床任务中的表现，并将正确答案用于LLM的后期训练（如SFT, GRPO, DPO）。

Result: 模型在临床任务中取得45-67%的准确率；模型擅长症状识别，但在分诊严重程度、治疗方案和后续护理方面表现不佳。定制化基准测试能有效识别通用评估遗漏的特定能力差距。动态MCQA方法无需昂贵的人工标注，即可为LLM后期训练提供高奖励样本，从而提升其效果。图基方法成功解决了手动编制基准测试的覆盖范围限制。

Conclusion: 该方法是创建可扩展、抗污染、动态生成且在指南更新时仍能保持最新状态的综合性基准测试的重要一步，为在医学等复杂领域评估和改进LLM提供了一种强大解决方案。

Abstract: We present a first known prototype of a dynamic, systematic benchmark of
medical guidelines for 400+ questions, with 3.3+ trillion possible
combinations, covering 100\% of guideline relationships. We transformed the WHO
IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,
treatments, follow-ups, severities) and 300+ edges, then used graph traversal
to generate questions that incorporated age-specific scenarios and contextual
distractors to ensure clinical relevance. Our graph-based approach enables
systematic evaluation across clinical tasks (45-67\% accuracy), and we find
models excel at symptom recognition but struggle with triaging severity,
treatment protocols and follow-up care, demonstrating how customized benchmarks
can identify specific capability gaps that general-domain evaluations miss.
Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training
(supervised finetuning, GRPO, DPO), where correct answers provide high-reward
samples without expensive human annotation. The graph-based approach
successfully addresses the coverage limitations of manually curated benchmarks.
This methodology is a step toward scalable, contamination-resistant solution
for creating comprehensive benchmarks that can be dynamically generated,
including when the guidelines are updated. Code and datasets are available at
https://github.com/jessicalundin/graph_testing_harness

</details>


### [135] [A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling](https://arxiv.org/abs/2508.20953)
*Vipul Patel,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.AI

TL;DR: 本文提出一种多目标遗传算法（MOO-GA）来解决医疗领域的排班问题，旨在平衡成本、患者护理和员工满意度，并在实际数据集上实现了66%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 医疗领域的排班是一个重大的运营挑战，需要应对波动的患者负荷、多样的临床技能，并在控制劳动力成本的同时确保高水平的患者护理。该问题本质上是多目标的，需在最小化工资、确保患者需求的人员充足和满足员工偏好以减少倦怠之间取得平衡。

Method: 本文提出一种多目标遗传算法（MOO-GA），将医院单元排班问题建模为多目标优化任务。该模型整合了按小时的预约驱动需求和多技能员工的模块化班次等实际复杂性，并定义了成本、患者护理覆盖率和员工满意度作为目标函数来寻找高质量的非劣解。

Result: 在代表典型医院单元的数据集上，MOO-GA生成了稳健且平衡的排班表。与模拟传统手动排班流程的基线相比，该算法产生的排班表平均性能提高了66%。

Conclusion: 该方法有效管理了关键运营目标和以员工为中心目标之间的权衡，为护士经理和医院管理人员提供了实用的决策支持工具。

Abstract: Workforce scheduling in the healthcare sector is a significant operational
challenge, characterized by fluctuating patient loads, diverse clinical skills,
and the critical need to control labor costs while upholding high standards of
patient care. This problem is inherently multi-objective, demanding a delicate
balance between competing goals: minimizing payroll, ensuring adequate staffing
for patient needs, and accommodating staff preferences to mitigate burnout. We
propose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital
unit workforce scheduling problem as a multi-objective optimization task. Our
model incorporates real-world complexities, including hourly appointment-driven
demand and the use of modular shifts for a multi-skilled workforce. By defining
objective functions for cost, patient care coverage, and staff satisfaction,
the GA navigates the vast search space to identify a set of high-quality,
non-dominated solutions. Demonstrated on datasets representing a typical
hospital unit, the results show that our MOO-GA generates robust and balanced
schedules. On average, the schedules produced by our algorithm showed a 66\%
performance improvement over a baseline that simulates a conventional, manual
scheduling process. This approach effectively manages trade-offs between
critical operational and staff-centric objectives, providing a practical
decision support tool for nurse managers and hospital administrators.

</details>


### [136] [Efficient Neuro-Symbolic Learning of Constraints and Objective](https://arxiv.org/abs/2508.20978)
*Marianne Defresne,Romain Gambardella,Sophie Barbe,Thomas Schiex*

Main category: cs.AI

TL;DR: 本文提出了一种可微分的神经符号架构和损失函数，能够高效且准确地从自然输入中学习解决NP-hard离散推理和优化问题。


<details>
  <summary>Details</summary>
Motivation: 将离散推理与神经网络结合是当前研究热点，但现有神经架构（包括大型语言模型）在从自然输入中学习解决离散推理或优化问题方面仍面临挑战。

Method: 引入了一种可微分的神经符号架构和一种新的概率损失函数。该损失函数能够同时学习问题的约束和目标，从而提供一个完整的模型。通过将组合求解器移出训练循环，该架构实现了可扩展的训练，并通过精确推理获得最大准确度。

Result: 实验证明，该方法能高效学习从自然输入中解决NP-hard推理问题。在数独基准测试（符号、视觉、多解）上，其训练时间仅为其他混合方法的一小部分。在视觉最小割/最大割任务中，它比决策焦点学习方法能更好地优化遗憾。此外，它还能高效学习设计蛋白质这一大型真实世界问题的能量优化公式。

Conclusion: 所提出的神经符号架构和概率损失函数为从自然输入中学习解决复杂的NP-hard离散推理和优化问题提供了一种高效、精确且可扩展的解决方案，并在多个基准测试中展现出优越性能。

Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets,
there is an increasing interest in neural architectures that can learn how to
solve discrete reasoning or optimization problems from natural inputs, a task
that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a
loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints
and the objective, thus delivering a complete model that can be scrutinized and
completed with side constraints. By pushing the combinatorial solver out of the
training loop, our architecture also offers scalable training while exact
inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve
NP-hard reasoning problems from natural inputs. On three variants of the Sudoku
benchmark -- symbolic, visual, and many-solution --, our approach requires a
fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut
task, it optimizes the regret better than a Decision-Focused-Learning
regret-dedicated loss. Finally, it efficiently learns the energy optimization
formulation of the large real-world problem of designing proteins.

</details>


### [137] [ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery](https://arxiv.org/abs/2508.20996)
*Junda Wang,Zonghai Yao,Zhichao Yang,Lingxi Li,Junhui Qian,Hong Yu*

Main category: cs.AI

TL;DR: ChatThero是一个基于CBT和MI的多智能体对话框架，通过动态患者建模和自适应劝说策略，显著提升了药物滥用障碍患者的治疗动机和信心，表现优于GPT-4o，并支持临床转化研究。


<details>
  <summary>Details</summary>
Motivation: 药物滥用障碍（SUDs）影响全球数百万人，但因污名、动机障碍和个性化支持不足，有效治疗率低。现有大型语言模型（LLMs）虽有潜力，但缺乏与临床验证策略的紧密结合，导致在成瘾康复中的效果不佳。

Method: 提出了ChatThero，一个多智能体对话框架，结合动态患者建模、情境敏感的治疗对话以及基于认知行为疗法（CBT）和动机访谈（MI）的自适应劝说策略。构建了高保真合成基准（易、中、难抗拒水平），并采用监督微调（SFT）和直接偏好优化（DPO）两阶段流程进行训练。

Result: ChatThero使患者动机平均提升41.5%，治疗信心增加0.49%，处理困难病例所需的对话轮次比GPT-4o少26%。自动化和人工临床评估均表明ChatThero在同理心、响应性和行为真实性方面表现更优。

Conclusion: ChatThero框架为治疗对话的严谨、隐私保护研究提供了支持，并为未来的研究和临床转化提供了坚实、可复制的基础。

Abstract: Substance use disorders (SUDs) affect over 36 million people worldwide, yet
few receive effective care due to stigma, motivational barriers, and limited
personalized support. Although large language models (LLMs) show promise for
mental-health assistance, most systems lack tight integration with clinically
validated strategies, reducing effectiveness in addiction recovery. We present
ChatThero, a multi-agent conversational framework that couples dynamic patient
modeling with context-sensitive therapeutic dialogue and adaptive persuasive
strategies grounded in cognitive behavioral therapy (CBT) and motivational
interviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,
Medium, and Hard resistance levels, and train ChatThero with a two-stage
pipeline comprising supervised fine-tuning (SFT) followed by direct preference
optimization (DPO). In evaluation, ChatThero yields a 41.5\% average gain in
patient motivation, a 0.49\% increase in treatment confidence, and resolves
hard cases with 26\% fewer turns than GPT-4o, and both automated and human
clinical assessments rate it higher in empathy, responsiveness, and behavioral
realism. The framework supports rigorous, privacy-preserving study of
therapeutic conversation and provides a robust, replicable basis for research
and clinical translation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [138] [CrystalICL: Enabling In-Context Learning for Crystal Generation](https://arxiv.org/abs/2508.20143)
*Ruobing Wang,Qiaoyu Tan,Yili Wang,Ying Wang,Xin Wang*

Main category: cs.LG

TL;DR: 本文提出了CrystalICL模型，通过引入空间群晶体分词、混合指令微调和多任务指令微调策略，实现了基于大语言模型的小样本晶体生成，克服了现有零样本方法的局限性，并在多个基准测试中表现优越。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的晶体生成方法局限于零样本场景，未能充分利用小样本学习能力，这与人类专家通过修改已知结构设计新材料的方式不符。因此，研究者旨在开发一种能利用小样本上下文学习（ICL）范式进行晶体设计的方法。

Method: 提出CrystalICL模型用于小样本晶体生成。具体方法包括：1) 引入基于空间群的晶体分词方法，以降低LLM建模晶体对称性的复杂性；2) 引入条件结构感知的混合指令微调框架；3) 引入多任务指令微调策略，使模型能从有限数据中捕获结构-性质关系，从而更好地利用ICL。

Result: 在四个晶体生成基准测试中进行了广泛实验，结果表明CrystalICL在有条件和无条件生成任务上均优于领先的基线方法。

Conclusion: CrystalICL是一种用于小样本晶体生成的卓越模型，它成功地将LLM的上下文学习能力应用于晶体设计，并通过新颖的晶体表示和微调策略克服了现有方法的局限性，展示了在材料科学领域的巨大潜力。

Abstract: Designing crystal materials with desired physicochemical properties remains a
fundamental challenge in materials science. While large language models (LLMs)
have demonstrated strong in-context learning (ICL) capabilities, existing
LLM-based crystal generation approaches are limited to zero-shot scenarios and
are unable to benefit from few-shot scenarios. In contrast, human experts
typically design new materials by modifying relevant known structures which
aligns closely with the few-shot ICL paradigm. Motivated by this, we propose
CrystalICL, a novel model designed for few-shot crystal generation.
Specifically, we introduce a space-group based crystal tokenization method,
which effectively reduces the complexity of modeling crystal symmetry in LLMs.
We further introduce a condition-structure aware hybrid instruction tuning
framework and a multi-task instruction tuning strategy, enabling the model to
better exploit ICL by capturing structure-property relationships from limited
data. Extensive experiments on four crystal generation benchmarks demonstrate
the superiority of CrystalICL over the leading baseline methods on conditional
and unconditional generation tasks.

</details>


### [139] [Filter then Attend: Improving attention-based Time Series Forecasting with Spectral Filtering](https://arxiv.org/abs/2508.20206)
*Elisha Dayag,Nhat Thanh Van Tran,Jack Xin*

Main category: cs.LG

TL;DR: 本文提出在Transformer模型前端添加可学习频率滤波器，有效提升了长时序预测性能（5-10%），降低了模型复杂度和资源需求，并解决了模型对低频部分的偏向性。


<details>
  <summary>Details</summary>
Motivation: Transformer-based模型在长时序预测中存在对数据低频部分的偏向性（低频偏差），以及较高的计算和内存资源需求。

Method: 在Transformer-based模型输入端添加可学习的频率滤波器，仅增加约1000个额外参数。通过合成实验分析滤波器如何帮助模型更好地利用全频谱进行预测。

Result: 加入滤波器后，Transformer-based模型的预测性能相对提升5-10%。同时，滤波器使得模型能够降低嵌入维度，从而构建出更小、更有效的Transformer架构。合成实验表明滤波器帮助模型更好地利用了全频谱进行预测。

Conclusion: 在Transformer-based模型中引入可学习滤波器是一种有效的方法，可以显著提升长时序预测性能，克服低频偏差，并优化模型效率，使其更小且更有效。

Abstract: Transformer-based models are at the forefront in long time-series forecasting
(LTSF). While in many cases, these models are able to achieve state of the art
results, they suffer from a bias toward low-frequencies in the data and high
computational and memory requirements. Recent work has established that
learnable frequency filters can be an integral part of a deep forecasting model
by enhancing the model's spectral utilization. These works choose to use a
multilayer perceptron to process their filtered signals and thus do not solve
the issues found with transformer-based models. In this paper, we establish
that adding a filter to the beginning of transformer-based models enhances
their performance in long time-series forecasting. We add learnable filters,
which only add an additional $\approx 1000$ parameters to several
transformer-based models and observe in multiple instances 5-10 \% relative
improvement in forecasting performance. Additionally, we find that with filters
added, we are able to decrease the embedding dimension of our models, resulting
in transformer-based architectures that are both smaller and more effective
than their non-filtering base models. We also conduct synthetic experiments to
analyze how the filters enable Transformer-based models to better utilize the
full spectrum for forecasting.

</details>


### [140] [What can we learn from signals and systems in a transformer? Insights for probabilistic modeling and inference architecture](https://arxiv.org/abs/2508.20211)
*Heng-Sheng Chang,Prashant G. Mehta*

Main category: cs.LG

TL;DR: 本文提出一个概率模型，将Transformer的信号解释为条件测度的替代，其层操作为定点更新，尤其在隐马尔可夫模型（HMM）中给出了显式形式，旨在连接经典非线性滤波理论与现代推理架构。


<details>
  <summary>Details</summary>
Motivation: Transformer作为非线性预测器，其工作原理缺乏深入的理论解释。本文旨在通过与经典预测器（如Wiener预测器）对比，并尝试将Transformer的信号和层操作置于概率框架下，来桥接经典非线性滤波理论与现代推理架构，从而提供对Transformer更深层次的理解。

Method: 本文提出一个概率模型，将Transformer的信号解读为条件测度的替代，并将Transformer的层操作视为定点更新。特别地，针对概率模型是隐马尔可夫模型（HMM）的特殊情况，描述了定点更新的显式形式。

Result: 研究提出了一个概率模型，成功地将Transformer的信号解释为条件测度的替代，并将其层操作形式化为定点更新。此外，当概率模型为隐马尔可夫模型时，论文描述了定点更新的明确显式形式。

Conclusion: 本研究通过建立一个概率模型，将Transformer信号和层操作分别诠释为条件测度替代和定点更新，并提供了针对HMM的特定形式，成功地为连接经典的非线性滤波理论与现代Transformer推理架构迈出了重要一步，为理解Transformer的内部机制提供了新的理论视角。

Abstract: In the 1940s, Wiener introduced a linear predictor, where the future
prediction is computed by linearly combining the past data. A transformer
generalizes this idea: it is a nonlinear predictor where the next-token
prediction is computed by nonlinearly combining the past tokens. In this essay,
we present a probabilistic model that interprets transformer signals as
surrogates of conditional measures, and layer operations as fixed-point
updates. An explicit form of the fixed-point update is described for the
special case when the probabilistic model is a hidden Markov model (HMM). In
part, this paper is in an attempt to bridge the classical nonlinear filtering
theory with modern inference architectures.

</details>


### [141] [The Role of Teacher Calibration in Knowledge Distillation](https://arxiv.org/abs/2508.20224)
*Suyoung Kim,Seonguk Park,Junhoo Lee,Nojun Kwak*

Main category: cs.LG

TL;DR: 研究发现教师模型的校准误差与学生模型准确性强相关，通过校准教师模型可显著提升知识蒸馏的性能，且该方法通用并易于集成。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏（KD）在深度学习模型压缩中表现出色，但尚未完全理解哪些因素有助于提升学生模型的性能。

Method: 揭示教师模型校准误差与学生模型准确性的强相关性，并采用一种校准方法来降低教师模型的校准误差，以此提升KD性能。

Result: 发现教师模型的校准误差与学生模型准确性之间存在强相关性；通过减少教师模型的校准误差，KD性能得以提升；提出的算法通用性强，适用于分类和检测等多种任务；该算法易于与现有先进方法集成，并能持续获得优越性能。

Conclusion: 教师模型的校准是实现有效知识蒸馏的重要因素，通过校准可以提高学生模型的准确性，且该方法具有良好的通用性和兼容性。

Abstract: Knowledge Distillation (KD) has emerged as an effective model compression
technique in deep learning, enabling the transfer of knowledge from a large
teacher model to a compact student model. While KD has demonstrated significant
success, it is not yet fully understood which factors contribute to improving
the student's performance. In this paper, we reveal a strong correlation
between the teacher's calibration error and the student's accuracy. Therefore,
we claim that the calibration of the teacher model is an important factor for
effective KD. Furthermore, we demonstrate that the performance of KD can be
improved by simply employing a calibration method that reduces the teacher's
calibration error. Our algorithm is versatile, demonstrating effectiveness
across various tasks from classification to detection. Moreover, it can be
easily integrated with existing state-of-the-art methods, consistently
achieving superior performance.

</details>


### [142] [Coresets from Trajectories: Selecting Data via Correlation of Loss Differences](https://arxiv.org/abs/2508.20230)
*Manish Nagaraj,Deepak Ravikumar,Kaushik Roy*

Main category: cs.LG

TL;DR: 本文提出了一种名为关联损失差异（CLD）的简单且可扩展指标，用于核集（coreset）选择，以解决深度学习模型在资源受限场景中的可扩展性挑战。CLD通过测量训练样本与验证集损失轨迹的一致性来识别最具影响力的样本，并在理论和实验上证明了其有效性、效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型虽然在多个领域取得了最先进的性能，但在实时或资源受限的场景中面临可扩展性挑战，需要更高效地选择训练样本。

Method: 研究者提出了关联损失差异（CLD）方法。CLD通过在训练检查点计算每个样本的损失值，并衡量这些损失值与持留验证集损失轨迹的对齐程度，来识别最具影响力的训练样本。该方法高效，避免了传统子集选择方法中昂贵的梯度和曲率计算。同时，建立了一个理论框架，证明了基于CLD的核集收敛误差上界由选定样本的一致性和验证集的代表性决定。

Result: 在CIFAR-100和ImageNet-1k数据集上，基于CLD的核集在不同子集大小下通常优于或接近最先进的方法，并且即使未领先，也能保持在计算成本更高的基线方法1%的准确度范围内。CLD能有效跨架构（ResNet, VGG, DenseNet）进行迁移，代理到目标选择的性能下降小于1%。此外，CLD在使用早期检查点时表现稳定，仅导致可忽略的精度损失，并通过逐类验证对齐实现固有的偏差减少，无需额外的分层抽样。

Conclusion: CLD是一种有原则、高效、稳定且可迁移的工具，用于实现可扩展的数据集优化，解决了深度学习模型在实际应用中的可扩展性问题。

Abstract: Deep learning models achieve state-of-the-art performance across domains but
face scalability challenges in real-time or resource-constrained scenarios. To
address this, we propose Correlation of Loss Differences (CLD), a simple and
scalable metric for coreset selection that identifies the most impactful
training samples by measuring their alignment with the loss trajectories of a
held-out validation set. CLD is highly efficient, requiring only per-sample
loss values computed at training checkpoints, and avoiding the costly gradient
and curvature computations used in many existing subset selection methods. We
develop a general theoretical framework that establishes convergence guarantees
for CLD-based coresets, demonstrating that the convergence error is
upper-bounded by the alignment of the selected samples and the
representativeness of the validation set. On CIFAR-100 and ImageNet-1k,
CLD-based coresets typically outperform or closely match state-of-the-art
methods across subset sizes, and remain within 1% of more computationally
expensive baselines even when not leading. CLD transfers effectively across
architectures (ResNet, VGG, DenseNet), enabling proxy-to-target selection with
<1% degradation. Moreover, CLD is stable when using only early checkpoints,
incurring negligible accuracy loss. Finally, CLD exhibits inherent bias
reduction via per-class validation alignment, obviating the need for additional
stratified sampling. Together, these properties make CLD a principled,
efficient, stable, and transferable tool for scalable dataset optimization.

</details>


### [143] [Bounds on Perfect Node Classification: A Convex Graph Clustering Perspective](https://arxiv.org/abs/2508.20231)
*Firooz Shahriari-Mehr,Javad Aliakbari,Alexandre Graell i Amat,Ashkan Panahi*

Main category: cs.LG

TL;DR: 本文针对传导式节点分类问题，提出了一种将节点特定信息（标签和特征）融入谱图聚类框架的新型优化问题，证明了图结构与节点信息的协同作用，并在比单纯图聚类更宽松的条件下实现社区的完美恢复。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于分析传导式节点分类问题，并开发一种方法能够有效结合图结构和节点特定信息（标签、特征），以更好地利用图中的社区结构进行节点分类，尤其是在这些社区与节点属性一致的情况下。

Method: 本文提出了一种新颖的优化问题，该问题将节点特定信息（标签和特征）整合到谱图聚类框架中。同时，研究者为解决此优化问题提供了算法解决方案。

Result: 研究表明，图结构和节点特定信息之间存在显著的协同作用。在合适的节点特定信息下，该优化问题的解能够比单纯基于图聚类在更宽松的条件下完美恢复社区。数值实验也证实了这种协同作用。

Conclusion: 通过将节点特定信息与图结构结合在一个新型优化框架中，可以有效提升传导式节点分类任务中社区恢复的准确性，且所需的条件比单独使用图聚类更为宽松，证明了这种整合方法的优势。

Abstract: We present an analysis of the transductive node classification problem, where
the underlying graph consists of communities that agree with the node labels
and node features. For node classification, we propose a novel optimization
problem that incorporates the node-specific information (labels and features)
in a spectral graph clustering framework. Studying this problem, we demonstrate
a synergy between the graph structure and node-specific information. In
particular, we show that suitable node-specific information guarantees the
solution of our optimization problem perfectly recovering the communities,
under milder conditions than the bounds on graph clustering alone. We present
algorithmic solutions to our optimization problem and numerical experiments
that confirm such a synergy.

</details>


### [144] [Beyond Optimization: Exploring Novelty Discovery in Autonomous Experiments](https://arxiv.org/abs/2508.20254)
*Ralph Bulanadi,Jawad Chowdhury,Funakubo Hiroshi,Maxim Ziatdinov,Rama Vasudevan,Arpan Biswas,Yongtao Liu*

Main category: cs.LG

TL;DR: 现有自主实验（AEs）侧重于优化预设目标，限制了新颖现象的发现。本文提出INS2ANE框架，结合新颖性评分系统和策略性采样机制，显著提升了AEs发现未观测现象的能力。


<details>
  <summary>Details</summary>
Motivation: 当前自主实验（AEs）主要聚焦于优化预设目标，这种方法虽然能加速目标达成，但限制了对意外或未知物理现象的发现。

Method: 本文引入了一个名为INS2ANE（Integrated Novelty Score-Strategic Autonomous Non-Smooth Exploration）的新型框架，它集成了两个核心组件：1) 一个用于评估实验结果独特性的新颖性评分系统；2) 一个策略性采样机制，即使在传统标准下前景不佳，也能促进对未充分采样区域的探索。该方法在预先获取的图像-光谱数据集和自主扫描探针显微镜实验中得到验证。

Result: 与传统优化例程相比，INS2ANE显著增加了所探索现象的多样性，从而提高了发现此前未观测现象的可能性。

Conclusion: 这些结果表明，该方法能够增强自主实验的科学发现深度，并结合AEs提供的高效率，有望通过同时探索复杂的实验空间以揭示新现象，从而加速科学研究。

Abstract: Autonomous experiments (AEs) are transforming how scientific research is
conducted by integrating artificial intelligence with automated experimental
platforms. Current AEs primarily focus on the optimization of a predefined
target; while accelerating this goal, such an approach limits the discovery of
unexpected or unknown physical phenomena. Here, we introduce a novel framework,
INS2ANE (Integrated Novelty Score-Strategic Autonomous Non-Smooth Exploration),
to enhance the discovery of novel phenomena in autonomous experimentation. Our
method integrates two key components: (1) a novelty scoring system that
evaluates the uniqueness of experimental results, and (2) a strategic sampling
mechanism that promotes exploration of under-sampled regions even if they
appear less promising by conventional criteria. We validate this approach on a
pre-acquired dataset with a known ground truth comprising of image-spectral
pairs. We further implement the process on autonomous scanning probe microscopy
experiments. INS2ANE significantly increases the diversity of explored
phenomena in comparison to conventional optimization routines, enhancing the
likelihood of discovering previously unobserved phenomena. These results
demonstrate the potential for AE to enhance the depth of scientific discovery;
in combination with the efficiency provided by AEs, this approach promises to
accelerate scientific research by simultaneously navigating complex
experimental spaces to uncover new phenomena.

</details>


### [145] [Discovering equations from data: symbolic regression in dynamical systems](https://arxiv.org/abs/2508.20257)
*Beatriz R. Brum,Luiza Lober,Isolde Previdelli,Francisco A. Rodrigues*

Main category: cs.LG

TL;DR: 本文比较了五种符号回归方法在从动力学系统数据中发现方程方面的性能，结果表明PySR方法在准确性和预测能力上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 方程发现是物理学和许多研究领域的核心，符号回归方法已实现自动化。由于现有多种方法，需要对它们进行比较，特别是针对描述复杂现象的动态系统。

Method: 研究使用了五种符号回归方法，从九个动力学过程（包括混沌动力学和流行病模型）中恢复方程。具体强调了PySR方法的应用和评估。

Result: PySR方法被证明是最适合推断方程的方法，其基准测试结果显示出高预测能力和准确性，某些估计与原始解析形式几乎无法区分。

Conclusion: 这些结果突出了符号回归（尤其是PySR）作为推断和建模真实世界现象的强大工具的潜力。

Abstract: The process of discovering equations from data lies at the heart of physics
and in many other areas of research, including mathematical ecology and
epidemiology. Recently, machine learning methods known as symbolic regression
have automated this process. As several methods are available in the
literature, it is important to compare them, particularly for dynamic systems
that describe complex phenomena. In this paper, five symbolic regression
methods were used for recovering equations from nine dynamical processes,
including chaotic dynamics and epidemic models, with the PySR method proving to
be the most suitable for inferring equations. Benchmark results demonstrate its
high predictive power and accuracy, with some estimates being indistinguishable
from the original analytical forms. These results highlight the potential of
symbolic regression as a robust tool for inferring and modelling real-world
phenomena.

</details>


### [146] [Latent Variable Modeling for Robust Causal Effect Estimation](https://arxiv.org/abs/2508.20259)
*Tetsuro Morimura,Tatsushi Oka,Yugo Suzuki,Daisuke Moriwaki*

Main category: cs.LG

TL;DR: 将潜在变量模型融入双重机器学习（DML）范式，以在存在隐藏因素时实现稳健的因果效应估计。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，未观测或未测量的协变量（隐藏因素）会影响处理或结果，给因果效应估计带来挑战。潜在变量模型被认为是解决这些问题的有力工具。

Method: 本文提出了一个新框架，将潜在变量建模集成到双重机器学习（DML）范式中。该框架考虑了两种场景：潜在变量仅影响结果，或同时影响处理和结果。为确保可操作性，潜在变量仅在DML的第二阶段被引入，并分离了表示学习和潜在推断。

Result: 通过在合成数据集和真实世界数据集上进行大量实验，证明了所提方法的鲁棒性和有效性。

Conclusion: 该方法能够有效且鲁棒地估计存在隐藏因素时的因果效应。

Abstract: Latent variable models provide a powerful framework for incorporating and
inferring unobserved factors in observational data. In causal inference, they
help account for hidden factors influencing treatment or outcome, thereby
addressing challenges posed by missing or unmeasured covariates. This paper
proposes a new framework that integrates latent variable modeling into the
double machine learning (DML) paradigm to enable robust causal effect
estimation in the presence of such hidden factors. We consider two scenarios:
one where a latent variable affects only the outcome, and another where it may
influence both treatment and outcome. To ensure tractability, we incorporate
latent variables only in the second stage of DML, separating representation
learning from latent inference. We demonstrate the robustness and effectiveness
of our method through extensive experiments on both synthetic and real-world
datasets.

</details>


### [147] [Generalizable AI Model for Indoor Temperature Forecasting Across Sub-Saharan Africa](https://arxiv.org/abs/2508.20260)
*Zainab Akhtar,Eunice Jengo,Björn Haßler*

Main category: cs.LG

TL;DR: 本研究开发并评估了一种轻量级、领域知情的AI模型，用于预测撒哈拉以南非洲自然通风学校和家庭的室内温度，在资源受限环境中表现出良好的热舒适管理潜力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为撒哈拉以南非洲自然通风的学校和家庭提供一种轻量级、领域知情的AI模型，以预测室内温度，从而支持这些资源受限环境下的热舒适管理。

Method: 该研究使用了一个轻量级、领域知情的AI模型，扩展了Temp-AI-Estimator框架。模型在坦桑尼亚学校数据上进行训练，并在尼日利亚学校和冈比亚家庭数据上进行评估，仅使用最少的可访问输入。

Result: 模型实现了稳健的跨国性能，在尼日利亚学校的平均绝对误差为1.45°C，在冈比亚家庭的平均绝对误差为0.65°C。

Conclusion: 研究结果强调了人工智能在资源受限环境中进行热舒适管理的潜力。

Abstract: This study presents a lightweight, domain-informed AI model for predicting
indoor temperatures in naturally ventilated schools and homes in Sub-Saharan
Africa. The model extends the Temp-AI-Estimator framework, trained on Tanzanian
school data, and evaluated on Nigerian schools and Gambian homes. It achieves
robust cross-country performance using only minimal accessible inputs, with
mean absolute errors of 1.45{\deg}C for Nigerian schools and 0.65{\deg}C for
Gambian homes. These findings highlight AI's potential for thermal comfort
management in resource-constrained environments.

</details>


### [148] [A Systematic Review on the Generative AI Applications in Human Medical Genomics](https://arxiv.org/abs/2508.20275)
*Anton Changalidis,Yury Barbitoff,Yulia Nasykhova,Andrey Glotov*

Main category: cs.LG

TL;DR: 本系统综述分析了大型语言模型（LLMs）在遗传学研究和遗传疾病诊断中的应用、能力和局限性。


<details>
  <summary>Details</summary>
Motivation: 传统统计和机器学习方法在处理复杂、高维遗传数据时面临挑战，而基于Transformer架构的LLMs在理解非结构化医疗数据方面表现出色，因此有必要系统评估LLMs在该领域的应用潜力。

Method: 通过PubMed、bioRxiv、medRxiv和arXiv进行关键词自动化搜索，筛选出关于LLMs在遗传学诊断和教育中应用的研究，并移除了不相关或过时模型。共分析了172项研究。

Result: LLMs应用于基因组变异识别、注释和解读，以及通过视觉Transformer推动医学影像发展。基于Transformer的模型显著促进了疾病和风险分层、变异解读、医学影像分析和报告生成。主要挑战在于整合多模态数据（基因组序列、影像、临床记录）以及在临床环境中的泛化性和实际实施。

Conclusion: 本综述全面评估了LLMs在遗传疾病诊断和遗传学教育中的当前能力与局限性，为导航这一快速发展领域提供了指导。

Abstract: Although traditional statistical techniques and machine learning methods have
contributed significantly to genetics and, in particular, inherited disease
diagnosis, they often struggle with complex, high-dimensional data, a challenge
now addressed by state-of-the-art deep learning models. Large language models
(LLMs), based on transformer architectures, have excelled in tasks requiring
contextual comprehension of unstructured medical data. This systematic review
examines the role of LLMs in the genetic research and diagnostics of both rare
and common diseases. Automated keyword-based search in PubMed, bioRxiv,
medRxiv, and arXiv was conducted, targeting studies on LLM applications in
diagnostics and education within genetics and removing irrelevant or outdated
models. A total of 172 studies were analyzed, highlighting applications in
genomic variant identification, annotation, and interpretation, as well as
medical imaging advancements through vision transformers. Key findings indicate
that while transformer-based models significantly advance disease and risk
stratification, variant interpretation, medical imaging analysis, and report
generation, major challenges persist in integrating multimodal data (genomic
sequences, imaging, and clinical records) into unified and clinically robust
pipelines, facing limitations in generalizability and practical implementation
in clinical settings. This review provides a comprehensive classification and
assessment of the current capabilities and limitations of LLMs in transforming
hereditary disease diagnostics and supporting genetic education, serving as a
guide to navigate this rapidly evolving field.

</details>


### [149] [Objective Value Change and Shape-Based Accelerated Optimization for the Neural Network Approximation](https://arxiv.org/abs/2508.20290)
*Pengcheng Xie,Zihao Zhou,Zijian Zhou*

Main category: cs.LG

TL;DR: 本文引入了一种新颖的VC（价值变化）度量，用于评估神经网络逼近任务的难度和逼近效果，并基于此揭示了VC-tendency和minority-tendency现象，同时提出了一个基于VC的新型预处理框架。


<details>
  <summary>Details</summary>
Motivation: 神经网络在关键应用中常因不可预测的局部性能而受限，影响其可靠性。急需一种量化局部行为变化的方法来提高稳定性与性能。

Method: 引入VC（价值变化）作为目标函数f的新度量，用以量化神经网络行为的局部价值变化，测量逼近任务的难度和影响。研究了VC的基本理论性质。提出了一个基于VC的新颖函数距离度量，并在此基础上进一步提出了一个用于神经网络逼近的新预处理框架。

Result: VC能有效量化神经网络的局部性能和行为。发现了神经网络逼近中的VC-tendency和minority-tendency两种现象，它们描述了点态误差如何随VC分布演变。数值结果（包括真实世界实验和PDE相关科学问题）支持了所发现的现象和提出的预处理加速方法。

Conclusion: VC是一个有力的工具，可以量化神经网络逼近任务的难度和逼近效果，有助于理解其局部性能，并能指导新的预处理框架设计，从而提高神经网络在逼近任务中的可靠性和效率。

Abstract: This paper introduce a novel metric of an objective function f, we say VC
(value change) to measure the difficulty and approximation affection when
conducting an neural network approximation task, and it numerically supports
characterizing the local performance and behavior of neural network
approximation. Neural networks often suffer from unpredictable local
performance, which can hinder their reliability in critical applications. VC
addresses this issue by providing a quantifiable measure of local value changes
in network behavior, offering insights into the stability and performance for
achieving the neural-network approximation. We investigate some fundamental
theoretical properties of VC and identified two intriguing phenomena in neural
network approximation: the VC-tendency and the minority-tendency. These trends
respectively characterize how pointwise errors evolve in relation to the
distribution of VC during the approximation process.In addition, we propose a
novel metric based on VC, which measures the distance between two functions
from the perspective of variation. Building upon this metric, we further
propose a new preprocessing framework for neural network approximation.
Numerical results including the real-world experiment and the PDE-related
scientific problem support our discovery and pre-processing acceleration
method.

</details>


### [150] [Beacon: Post-Training Quantization with Integrated Grid Selection](https://arxiv.org/abs/2508.20293)
*Shihao Zhang,Rayan Saab*

Main category: cs.LG

TL;DR: Beacon是一种简单有效的逐通道PTQ算法，通过利用对称标量量化的几何特性自动确定最优缩放因子，无需手动调优，即可实现与SOTA方法媲美的性能。


<details>
  <summary>Details</summary>
Motivation: 在逐通道训练后量化（PTQ）中，选择合适的缩放因子是一个关键挑战，而现有方法通常需要通过启发式调优或网格搜索来固定缩放因子。

Method: Beacon算法通过使用固定的非缩放字母表直接进行逐通道PTQ，并利用对称标量量化的几何特性自动确定最佳缩放因子。它支持对称和非对称量化，且不依赖反向传播或大型校准集。

Result: 尽管Beacon算法简单且无需调优，但其性能与现有最先进的方法相比具有竞争力。

Conclusion: Beacon提供了一种实用且高效的模型部署解决方案，因为它简单、无需调优且性能优异。

Abstract: Quantization is a widely used compression technique for reducing the memory
and computation costs of large pre-trained models. A key challenge in
per-channel post-training quantization (PTQ) is selecting appropriate scaling
factors to replace weight values with values from a scaled quantization grid.
Existing methods typically fix the scale at the outset via heuristic tuning or
grid search. In this note, we propose Beacon, a simple and effective algorithm
that eliminates the need for such manual tuning. Beacon performs per-channel
PTQ directly using a fixed non-scaled alphabet and automatically determines the
optimal scaling factors by exploiting the geometry of symmetric scalar
quantization. It supports both symmetric and asymmetric quantization with
minimal modifications and does not rely on back-propagation or large
calibration sets. Despite its simplicity and tuning-free nature, Beacon
achieves competitive performance compared to state-of-the-art methods, making
it a practical solution for efficient model deployment.

</details>


### [151] [Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization](https://arxiv.org/abs/2508.20294)
*Frank Röder,Jan Benad,Manfred Eppe,Pradeep Kr. Banerjee*

Main category: cs.LG

TL;DR: DALI是一个基于Dreamer的框架，通过自监督编码器从智能体-环境交互中推断潜在上下文表示，从而在未见过的环境条件下实现强化学习的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现实世界强化学习需要在没有昂贵再训练的情况下适应未见过的环境条件。上下文马尔可夫决策过程(cMDP)对此建模，但现有方法常需要显式上下文变量，这限制了其在上下文是潜在或难以测量时的应用。

Method: 引入了Dynamics-Aligned Latent Imagination (DALI)框架，该框架集成在Dreamer架构中。DALI通过训练一个自监督编码器来预测前向动力学，从而从智能体-环境交互中推断潜在上下文表示。这些表示用于调节世界模型和策略，连接感知与控制。理论证明该编码器对高效上下文推断和鲁棒泛化至关重要，且DALI的潜在空间实现了反事实一致性。

Result: 在具有挑战性的cMDP基准测试中，DALI比上下文无关基线取得了显著提升，并且在外推任务中常优于上下文感知基线，从而实现了对未见过的上下文变体的零样本泛化。

Conclusion: DALI成功通过推断潜在上下文表示，使强化学习模型能够有效适应未见过的环境条件，实现了零样本泛化，解决了传统cMDP方法对显式上下文依赖的局限性。

Abstract: Real-world reinforcement learning demands adaptation to unseen environmental
conditions without costly retraining. Contextual Markov Decision Processes
(cMDP) model this challenge, but existing methods often require explicit
context variables (e.g., friction, gravity), limiting their use when contexts
are latent or hard to measure. We introduce Dynamics-Aligned Latent Imagination
(DALI), a framework integrated within the Dreamer architecture that infers
latent context representations from agent-environment interactions. By training
a self-supervised encoder to predict forward dynamics, DALI generates
actionable representations conditioning the world model and policy, bridging
perception and control. We theoretically prove this encoder is essential for
efficient context inference and robust generalization. DALI's latent space
enables counterfactual consistency: Perturbing a gravity-encoding dimension
alters imagined rollouts in physically plausible ways. On challenging cMDP
benchmarks, DALI achieves significant gains over context-unaware baselines,
often surpassing context-aware baselines in extrapolation tasks, enabling
zero-shot generalization to unseen contextual variations.

</details>


### [152] [FedReFT: Federated Representation Fine-Tuning with All-But-Me Aggregation](https://arxiv.org/abs/2508.20295)
*Fatema Siddika,Md Anwar Hossen,J. Pablo Muñoz,Tanya Roosta,Anuj Sharma,Ali Jannesari*

Main category: cs.LG

TL;DR: 本文提出FedReFT，一种新颖的联邦表示微调方法，通过稀疏干预层和All-But-Me (ABM)聚合解决联邦学习中的异构性挑战，并在参数效率上显著优于现有PEFT方法。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调（PEFT）中的表示微调（ReFT）在独立设置下表现出色，但在联邦学习（FL）中面临客户端数据、模型容量和计算资源异构性以及朴素聚合可能导致的语义错位等挑战。

Method: 引入FedReFT，通过应用稀疏干预层直接操纵客户端的隐藏表示。为解决表示级更新在任务异构性下的聚合不匹配问题，提出了All-But-Me (ABM)聚合策略，允许每个客户端接收并部分融合其他客户端的聚合更新，以实现稳定和个性化的学习。

Result: FedReFT在常识推理、算术推理、指令微调和GLUE等任务上持续优于FL中现有最先进的PEFT方法，并且比领先的基于LoRA的方法实现了7到15倍的更高参数效率。

Conclusion: FedReFT是一种在联邦学习环境中实现高性能和高参数效率的有效方法，尤其适用于边缘设备，通过创新的表示微调和聚合策略成功应对了客户端异构性问题。

Abstract: Parameter-efficient fine-tuning (PEFT) has attracted significant attention
for adapting large pre-trained models by modifying a small subset of
parameters. Recently, Representation Fine-tuning (ReFT) has emerged as an
effective alternative. ReFT shifts the fine-tuning paradigm from updating model
weights to directly manipulating hidden representations that capture rich
semantic information, and performs better than state-of-the-art PEFTs in
standalone settings. However, its application in Federated Learning (FL)
remains challenging due to heterogeneity in clients' data distributions, model
capacities, and computational resources. To address these challenges, we
introduce Federated Representation Fine-Tuning (FedReFT), a novel approach to
fine-tune the client's hidden representation. FedReFT applies sparse
intervention layers to steer hidden representations directly, offering a
lightweight and semantically rich fine-tuning alternative ideal for edge
devices. However, representation-level updates are especially vulnerable to
aggregation mismatch under different task heterogeneity, where naive averaging
can corrupt semantic alignment. To mitigate this issue, we propose All-But-Me
(ABM) aggregation, where each client receives the aggregated updates of others
and partially incorporates them, enabling stable and personalized learning by
balancing local focus with global knowledge. We evaluate FedReFT on commonsense
reasoning, arithmetic reasoning, instruction-tuning, and GLUE, where it
consistently outperforms state-of-the-art PEFT methods in FL, achieving 7x-15x
higher parameter efficiency compared to leading LoRA-based approaches.

</details>


### [153] [Multi-Agent Reinforcement Learning in Intelligent Transportation Systems: A Comprehensive Survey](https://arxiv.org/abs/2508.20315)
*RexCharles Donatus,Kumater Ter,Ore-Ofe Ajayi,Daniel Udekwe*

Main category: cs.LG

TL;DR: 本文对多智能体强化学习(MARL)在智能交通系统(ITS)中的应用进行了全面综述，分类了MARL方法，评审了关键应用领域，并指出了该领域面临的核心挑战。


<details>
  <summary>Details</summary>
Motivation: 城市交通日益复杂，对高效、可持续、自适应的智能交通系统(ITS)的需求日益增长。ITS面临的核心挑战是动态、大规模、不确定环境中多智能体的自主决策和有效协调。多智能体强化学习(MARL)被视为解决这些挑战的有效范式。

Method: 本文采用综述方法，对MARL在ITS中的应用进行了全面审视。具体包括：引入结构化分类法，根据协调模型和学习算法（如基于价值、基于策略、Actor-Critic和通信增强框架）对MARL方法进行分类；评审MARL在交通信号控制、车联网与自动驾驶车辆协调、物流优化和按需出行系统等关键ITS领域的应用；并重点介绍了常用的模拟平台（如SUMO、CARLA、CityFlow）及新兴基准。

Result: 综述结果识别了MARL在ITS应用中的核心挑战，包括可扩展性、非平稳性、信用分配、通信限制以及模拟到现实的迁移差距，这些问题目前仍在阻碍其实际部署。

Conclusion: 多智能体强化学习(MARL)在智能交通系统(ITS)中展现出巨大潜力，但仍需克服可扩展性、非平稳性等关键挑战，才能实现其在现实世界中的广泛部署和应用。

Abstract: The growing complexity of urban mobility and the demand for efficient,
sustainable, and adaptive solutions have positioned Intelligent Transportation
Systems (ITS) at the forefront of modern infrastructure innovation. At the core
of ITS lies the challenge of autonomous decision-making across dynamic, large
scale, and uncertain environments where multiple agents traffic signals,
autonomous vehicles, or fleet units must coordinate effectively. Multi Agent
Reinforcement Learning (MARL) offers a promising paradigm for addressing these
challenges by enabling distributed agents to jointly learn optimal strategies
that balance individual objectives with system wide efficiency. This paper
presents a comprehensive survey of MARL applications in ITS. We introduce a
structured taxonomy that categorizes MARL approaches according to coordination
models and learning algorithms, spanning value based, policy based, actor
critic, and communication enhanced frameworks. Applications are reviewed across
key ITS domains, including traffic signal control, connected and autonomous
vehicle coordination, logistics optimization, and mobility on demand systems.
Furthermore, we highlight widely used simulation platforms such as SUMO, CARLA,
and CityFlow that support MARL experimentation, along with emerging benchmarks.
The survey also identifies core challenges, including scalability, non
stationarity, credit assignment, communication constraints, and the sim to real
transfer gap, which continue to hinder real world deployment.

</details>


### [154] [Multi-View Graph Convolution Network for Internal Talent Recommendation Based on Enterprise Emails](https://arxiv.org/abs/2508.20328)
*Soo Hyun Kim,Jang-Hyun Kim*

Main category: cs.LG

TL;DR: 本研究提出一种基于电子邮件数据的双图卷积网络（Dual GCN）框架，通过自适应融合员工的“工作内容”（WHAT）和“工作方式”（HOW）两个维度，改进内部人才推荐系统，实现高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的内部人才推荐方法受限于少数管理者的视角，易遗漏合格候选人。为解决这一结构性问题，需要一个更全面、客观的内部人才发现框架。

Method: 该研究从电子邮件数据中提取员工的“工作内容”（任务的语义相似性）和“工作方式”（互动协作的结构特征）两个独立维度，并分别将其建模为图。然后，使用带有门控机制的双图卷积网络（GCN）对这两个维度进行自适应融合。

Result: 所提出的基于门控的融合模型显著优于其他融合策略和启发式基线，在Hit@100上达到40.9%的最佳性能。模型还展现出高可解释性，能为不同职能类别（如销售与营销侧重“如何”，研究类侧重平衡）学习独特的、上下文感知的融合策略。

Conclusion: 本研究提供了一个定量且全面的内部人才发现框架，有效降低了传统方法中候选人遗漏的风险。其主要贡献在于能够实证确定任务匹配（WHAT）与协作模式（HOW）之间在新岗位成功所需的最佳融合比例，具有重要的实践意义。

Abstract: Internal talent recommendation is a critical strategy for organizational
continuity, yet conventional approaches suffer from structural limitations,
often overlooking qualified candidates by relying on the narrow perspective of
a few managers. To address this challenge, we propose a novel framework that
models two distinct dimensions of an employee's position fit from email data:
WHAT they do (semantic similarity of tasks) and HOW they work (structural
characteristics of their interactions and collaborations). These dimensions are
represented as independent graphs and adaptively fused using a Dual Graph
Convolutional Network (GCN) with a gating mechanism. Experiments show that our
proposed gating-based fusion model significantly outperforms other fusion
strategies and a heuristic baseline, achieving a top performance of 40.9% on
Hit@100. Importantly, it is worth noting that the model demonstrates high
interpretability by learning distinct, context-aware fusion strategies for
different job families. For example, it learned to prioritize relational (HOW)
data for 'sales and marketing' job families while applying a balanced approach
for 'research' job families. This research offers a quantitative and
comprehensive framework for internal talent discovery, minimizing the risk of
candidate omission inherent in traditional methods. Its primary contribution
lies in its ability to empirically determine the optimal fusion ratio between
task alignment (WHAT) and collaborative patterns (HOW), which is required for
employees to succeed in the new positions, thereby offering important practical
implications.

</details>


### [155] [FORGE: Foundational Optimization Representations from Graph Embeddings](https://arxiv.org/abs/2508.20330)
*Zohair Shafi,Serdar Kadioglu*

Main category: cs.LG

TL;DR: Forge通过无监督预训练向量量化图自编码器，为组合优化提供通用且可扩展的学习方法，无需问题解即可加速商业求解器性能。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法加速组合优化问题求解时，需大量难解实例数据且计算开销大，同时需为每个问题分布和任务训练专用模型，限制了可扩展性和泛化能力。

Method: 提出Forge，一种无监督预训练向量量化图自编码器的方法。该方法在一个大型多样化的混合整数规划(MIP)实例集合上进行，不依赖于问题解，通过向量量化创建离散编码作为优化实例的词汇。

Result: 在无监督设置下，Forge嵌入能有效区分和聚类未见实例。在有监督设置下，微调后的Forge嵌入使单个模型能预测多类型问题分布的变量（用于热启动）和整数间隙（用于割平面生成），这两项预测均显著提升了最先进商业优化求解器的性能。

Conclusion: Forge通过无需依赖问题解的预训练MIP实例嵌入，提供了一种通用、可扩展的学习方法，有效克服了传统方法的局限性，并成功提升了商业优化求解器的性能，为相关研究和实际应用奠定了基础。

Abstract: Combinatorial optimization problems are ubiquitous in science and
engineering, yet learning-based approaches to accelerate their solution often
require solving a large number of hard-to-solve optimization instances to
collect training data, incurring significant computational overhead. Existing
methods require training dedicated models for each problem distribution for
each downstream task, severely limiting their scalability and generalization.
In this work, we introduce Forge, a method of pre-training a vector-quantized
graph autoencoder on a large and diverse collection of mixed-integer
programming (MIP) instances in an unsupervised fashion without dependency on
their solution. The vector quantization process creates discrete code
assignments that act as a vocabulary to represent optimization instances. We
evaluate our approach under both supervised and unsupervised settings. For the
unsupervised setting, we demonstrate that Forge embeddings effectively
differentiate and cluster unseen instances. For the supervised setting, we
fine-tune Forge embeddings and show that a single model predicts both the
variables for warm-starts and integrality gaps for cut-generation across
multiple problem type distributions. Both predictions help improve performance
of a state-of-the-art, commercial optimization solver. Finally, we release our
code and pre-trained Forge weights to encourage further research and practical
use of instance-level MIP embeddings at https://github.com/skadio/forge/

</details>


### [156] [Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs](https://arxiv.org/abs/2508.20333)
*Md Abdullah Al Mamun,Ihsen Alouani,Nael Abu-Ghazaleh*

Main category: cs.LG

TL;DR: 本文提出了一种名为“颠覆性对齐注入”（SAI）的投毒攻击，该攻击利用大型语言模型（LLMs）的对齐机制，在不影响模型整体响应能力的情况下，植入偏差或执行有针对性的审查，并能规避现有的投毒防御措施，对LLM应用造成显著偏见。


<details>
  <summary>Details</summary>
Motivation: LLMs通过训练以拒绝回答有害或不安全的提示来满足伦理和安全标准。本研究旨在揭示对手如何利用LLMs的这种对齐机制，在不降低模型对无关主题响应能力的情况下，植入偏见或实施有针对性的审查。

Method: 本文提出了“颠覆性对齐注入”（Subversive Alignment Injection, SAI）这一投毒攻击方法。SAI利用LLMs的对齐机制，触发模型对对手预定义的特定主题或查询的拒绝，并通过这种拒绝来注入偏见。

Result: SAI攻击成功规避了包括LLM状态取证和联邦学习中鲁棒聚合技术在内的最先进投毒防御措施。在实践中，SAI对LLM驱动的应用管道造成了严重影响：例如，在ChatDoctor等基于聊天的应用中，1%的数据投毒导致对目标种族医疗问题的拒绝率很高，造成23%的高偏见（ΔDP）；在简历筛选管道中，对特定大学简历的拒绝导致27%的高偏见；在其他9个聊天应用中，偏见甚至高达38%（ΔDP）。

Conclusion: LLM的对齐机制可能被对手利用，通过投毒攻击（如SAI）植入偏见或实施有针对性的审查。这种攻击具有实际危险性，能够有效逃避现有防御措施，对LLM驱动的应用造成显著偏见，揭示了LLM对齐策略中潜在的安全漏洞。

Abstract: Large Language Models (LLMs) are aligned to meet ethical standards and safety
requirements by training them to refuse answering harmful or unsafe prompts. In
this paper, we demonstrate how adversaries can exploit LLMs' alignment to
implant bias, or enforce targeted censorship without degrading the model's
responsiveness to unrelated topics. Specifically, we propose Subversive
Alignment Injection (SAI), a poisoning attack that leverages the alignment
mechanism to trigger refusal on specific topics or queries predefined by the
adversary. Although it is perhaps not surprising that refusal can be induced
through overalignment, we demonstrate how this refusal can be exploited to
inject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning
defenses including LLM state forensics, as well as robust aggregation
techniques that are designed to detect poisoning in FL settings. We demonstrate
the practical dangers of this attack by illustrating its end-to-end impacts on
LLM-powered application pipelines. For chat based applications such as
ChatDoctor, with 1% data poisoning, the system refuses to answer healthcare
questions to targeted racial category leading to high bias ($\Delta DP$ of
23%). We also show that bias can be induced in other NLP tasks: for a resume
selection pipeline aligned to refuse to summarize CVs from a selected
university, high bias in selection ($\Delta DP$ of 27%) results. Even higher
bias ($\Delta DP$~38%) results on 9 other chat based downstream applications.

</details>


### [157] [Dynamic Synthetic Controls vs. Panel-Aware Double Machine Learning for Geo-Level Marketing Impact Estimation](https://arxiv.org/abs/2508.20335)
*Sang Su Lee,Vineeth Loganathan,Vijay Raghavan*

Main category: cs.LG

TL;DR: 论文通过模拟器比较了合成控制法（SCM）和面板式双重机器学习（DML）在双边市场地理营销效果评估中的表现，发现DML在复杂场景下比SCM更稳健，并提出一个基于诊断的DML选择框架。


<details>
  <summary>Details</summary>
Motivation: 准确量化双边市场中地理层面的营销提升效果具有挑战性。合成控制法（SCM）常低估效果，而面板式双重机器学习（DML）与SCM的基准比较较少。

Method: 构建了一个模拟器，模拟大规模地理推广，并在五种典型压力测试下（如曲线基线趋势、异质响应滞后）评估了七种估计器：三种增强型SCM（ASC）变体和四种面板DML变体（如TWFE、first-difference）。每个场景进行100次重复实验。

Result: 在涉及非线性或外部冲击的复杂场景中，ASC模型表现出严重的偏差和接近零的置信区间覆盖率。相比之下，面板DML变体显著降低了偏差，恢复了名义上的95%置信区间覆盖率，证明其更为稳健。

Conclusion: 尽管ASC可作为简单基准，但在常见的复杂情况下并不可靠。研究建议采用“先诊断后选择”的框架，根据业务挑战（如非线性趋势）选择最适合的DML模型，为地理实验分析提供更稳健可靠的方案。

Abstract: Accurately quantifying geo-level marketing lift in two-sided marketplaces is
challenging: the Synthetic Control Method (SCM) often exhibits high power yet
systematically under-estimates effect size, while panel-style Double Machine
Learning (DML) is seldom benchmarked against SCM. We build an open, fully
documented simulator that mimics a typical large-scale geo roll-out: N_unit
regional markets are tracked for T_pre weeks before launch and for a further
T_post-week campaign window, allowing all key parameters to be varied by the
user and probe both families under five stylized stress tests: 1) curved
baseline trends, 2) heterogeneous response lags, 3) treated-biased shocks, 4) a
non-linear outcome link, and 5) a drifting control group trend.
  Seven estimators are evaluated: three standard Augmented SCM (ASC) variants
and four panel-DML flavors (TWFE, CRE/Mundlak, first-difference, and
within-group). Across 100 replications per scenario, ASC models consistently
demonstrate severe bias and near-zero coverage in challenging scenarios
involving nonlinearities or external shocks. By contrast, panel-DML variants
dramatically reduce this bias and restore nominal 95%-CI coverage, proving far
more robust.
  The results indicate that while ASC provides a simple baseline, it is
unreliable in common, complex situations. We therefore propose a
'diagnose-first' framework where practitioners first identify the primary
business challenge (e.g., nonlinear trends, response lags) and then select the
specific DML model best suited for that scenario, providing a more robust and
reliable blueprint for analyzing geo-experiments.

</details>


### [158] [Adaptive Segmentation of EEG for Machine Learning Applications](https://arxiv.org/abs/2508.20336)
*Johnson Zhou,Joseph West,Krista A. Ehinger,Zhenming Ren,Sam E. John,David B. Grayden*

Main category: cs.LG

TL;DR: 本研究提出了一种名为CTXSEG的自适应EEG信号分割方法，该方法在癫痫检测任务中比传统的固定长度分割表现更优，并能减少所需的信号段数量。


<details>
  <summary>Details</summary>
Motivation: 目前的EEG信号分割方法通常采用任意固定的时间切片，这可能因大脑状态并非局限于固定时间间隔而缺乏生物学相关性。本研究旨在探讨自适应分割方法是否能更有效地应用于EEG机器学习分析。

Method: 引入了一种新颖的自适应分割方法CTXSEG，它根据EEG数据的统计差异创建变长片段，并提出了与现代机器学习方法结合使用这些片段的方式。通过新颖的信号生成器CTXGEN生成的合成数据对CTXSEG进行评估，并将其应用于EEG癫痫检测问题，与固定长度分割方法在性能上进行比较。

Result: 研究发现，在标准化框架下，使用CTXSEG处理EEG数据能够提高癫痫检测性能，并且所需的分割片段数量少于固定长度方法，而无需修改机器学习算法本身。

Conclusion: CTXSEG自适应分割方法可以便捷地应用于现代机器学习，具有改善性能的潜力。它是一种有前景的固定长度分割替代方案，应被视为EEG机器学习应用中标准预处理流程的一部分。

Abstract: Objective. Electroencephalography (EEG) data is derived by sampling
continuous neurological time series signals. In order to prepare EEG signals
for machine learning, the signal must be divided into manageable segments. The
current naive approach uses arbitrary fixed time slices, which may have limited
biological relevance because brain states are not confined to fixed intervals.
We investigate whether adaptive segmentation methods are beneficial for machine
learning EEG analysis.
  Approach. We introduce a novel adaptive segmentation method, CTXSEG, that
creates variable-length segments based on statistical differences in the EEG
data and propose ways to use them with modern machine learning approaches that
typically require fixed-length input. We assess CTXSEG using controllable
synthetic data generated by our novel signal generator CTXGEN. While our CTXSEG
method has general utility, we validate it on a real-world use case by applying
it to an EEG seizure detection problem. We compare the performance of CTXSEG
with fixed-length segmentation in the preprocessing step of a typical EEG
machine learning pipeline for seizure detection.
  Main results. We found that using CTXSEG to prepare EEG data improves seizure
detection performance compared to fixed-length approaches when evaluated using
a standardized framework, without modifying the machine learning method, and
requires fewer segments.
  Significance. This work demonstrates that adaptive segmentation with CTXSEG
can be readily applied to modern machine learning approaches, with potential to
improve performance. It is a promising alternative to fixed-length segmentation
for signal preprocessing and should be considered as part of the standard
preprocessing repertoire in EEG machine learning applications.

</details>


### [159] [Understanding Incremental Learning with Closed-form Solution to Gradient Flow on Overparamerterized Matrix Factorization](https://arxiv.org/abs/2508.20344)
*Hancheng Min,René Vidal*

Main category: cs.LG

TL;DR: 通过对称矩阵分解的梯度流闭式解，本文量化理解了神经网络训练中增量学习现象的机制，即源于学习不同分量的时间尺度分离。


<details>
  <summary>Details</summary>
Motivation: 旨在深入理解神经网络一阶优化算法在特定初始化下产生的隐式偏差或正则化效应（如增量学习），以解释其卓越的经验性能。

Method: 针对对称矩阵分解问题，通过求解类Riccati矩阵微分方程获得梯度流的闭式解，并利用此解进行定量分析。

Result: 增量学习现象源于学习目标矩阵不同分量动力学之间的时间尺度分离；减小初始化尺度会使这些时间尺度分离更加显著，从而能够找到目标矩阵的低秩近似。

Conclusion: 本研究为对称矩阵分解上的梯度流增量学习提供了定量理解，并讨论了将其分析扩展到非对称矩阵分解问题的潜在途径。

Abstract: Many theoretical studies on neural networks attribute their excellent
empirical performance to the implicit bias or regularization induced by
first-order optimization algorithms when training networks under certain
initialization assumptions. One example is the incremental learning phenomenon
in gradient flow (GF) on an overparamerterized matrix factorization problem
with small initialization: GF learns a target matrix by sequentially learning
its singular values in decreasing order of magnitude over time. In this paper,
we develop a quantitative understanding of this incremental learning behavior
for GF on the symmetric matrix factorization problem, using its closed-form
solution obtained by solving a Riccati-like matrix differential equation. We
show that incremental learning emerges from some time-scale separation among
dynamics corresponding to learning different components in the target matrix.
By decreasing the initialization scale, these time-scale separations become
more prominent, allowing one to find low-rank approximations of the target
matrix. Lastly, we discuss the possible avenues for extending this analysis to
asymmetric matrix factorization problems.

</details>


### [160] [DFAMS: Dynamic-flow guided Federated Alignment based Multi-prototype Search](https://arxiv.org/abs/2508.20353)
*Zhibang Yang,Xinke Jiang,Rihong Qiu,Ruiqing Li,Yihang Zhang,Yue Fang,Yongxin Xu,Hongxin Ding,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.LG

TL;DR: 联邦检索（FR）旨在缓解LLM幻觉，但现有方法在处理模糊查询和跨领域场景时表现不佳。本文提出DFAMS框架，利用动态信息流（DIF）识别查询意图并构建语义对齐的知识分区，显著提升了FR在分类、召回和下游问答任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦检索（FR）方法在处理模糊查询，特别是在跨领域场景中，难以检索到高质量和相关文档，从而严重限制了它们在支持下游生成任务中的有效性。FR旨在当所需外部知识分散时，通过路由查询来缓解大型语言模型（LLM）的幻觉问题。

Method: 本文提出DFAMS框架，受动态信息流（DIF）启发，旨在识别潜在查询意图并构建语义对齐的知识分区。具体而言，DFAMS利用少量标注查询的梯度信号和基于Shapley值的归因方法，探测LLM中的DIF以追踪与意图识别和子域边界检测相关的神经元激活路径。随后，DFAMS利用DIF通过多原型对比学习训练一个对齐模块，以实现细粒度的源内建模和跨知识库的源间语义对齐。

Result: 在五个基准测试中，DFAMS在知识分类准确性上比现有先进FR方法提高了14.37%，在检索召回率上提高了5.38%，在下游问答准确性上提高了6.45%。

Conclusion: DFAMS在复杂的联邦检索场景中表现出显著的有效性，成功解决了现有方法在模糊查询和跨域检索上的痛点，提升了检索质量和下游任务性能。

Abstract: Federated Retrieval (FR) routes queries across multiple external knowledge
sources, to mitigate hallucinations of LLMs, when necessary external knowledge
is distributed. However, existing methods struggle to retrieve high-quality and
relevant documents for ambiguous queries, especially in cross-domain scenarios,
which significantly limits their effectiveness in supporting downstream
generation tasks. Inspired by dynamic information flow (DIF), we propose DFAMS,
a novel framework that leverages DIF to identify latent query intents and
construct semantically aligned knowledge partitions for accurate retrieval
across heterogeneous sources. Specifically, DFAMS probes the DIF in LLMs by
leveraging gradient signals from a few annotated queries and employing Shapley
value-based attribution to trace neuron activation paths associated with intent
recognition and subdomain boundary detection. Then, DFAMS leverages DIF to
train an alignment module via multi-prototype contrastive learning, enabling
fine-grained intra-source modeling and inter-source semantic alignment across
knowledge bases. Experimental results across five benchmarks show that DFAMS
outperforms advanced FR methods by up to 14.37% in knowledge classification
accuracy, 5.38% in retrieval recall, and 6.45% in downstream QA accuracy,
demonstrating its effectiveness in complex FR scenarios.

</details>


### [161] [Developing a Multi-Modal Machine Learning Model For Predicting Performance of Automotive Hood Frames](https://arxiv.org/abs/2508.20358)
*Abhishek Indupally,Satchit Ramnath*

Main category: cs.LG

TL;DR: 本文开发了一种多模态机器学习（MMML）架构，用于在不依赖耗时模拟的情况下快速评估罩框几何结构性能，加速概念设计阶段的迭代。


<details>
  <summary>Details</summary>
Motivation: 设计师难以在不耗费大量模拟设置时间的情况下评估罩框几何结构的性能。传统的模拟计算成本高昂，限制了设计流程的效率。

Method: 开发了一种多模态机器学习（MMML）架构，该架构从相同数据的不同模态中学习，以预测性能指标。该方法旨在减少对计算密集型模拟的依赖，从而提高工程设计过程的效率。

Result: 所提出的MMML架构加速了设计探索，实现了快速迭代并保持了高性能标准；结合多种数据模态，MMML的表现优于传统的单模态方法；模型能够泛化到未曾训练的新的罩框几何结构。

Conclusion: MMML在补充传统基于模拟的工作流程（尤其是在概念设计阶段）方面具有巨大潜力，并弥合了机器学习与实际工程应用之间的差距。这项研究为机器学习技术在工程设计中的广泛采用铺平了道路，尤其侧重于优化结构开发和加速设计周期。

Abstract: Is there a way for a designer to evaluate the performance of a given hood
frame geometry without spending significant time on simulation setup? This
paper seeks to address this challenge by developing a multimodal
machine-learning (MMML) architecture that learns from different modalities of
the same data to predict performance metrics. It also aims to use the MMML
architecture to enhance the efficiency of engineering design processes by
reducing reliance on computationally expensive simulations. The proposed
architecture accelerates design exploration, enabling rapid iteration while
maintaining high-performance standards, especially in the concept design phase.
The study also presents results that show that by combining multiple data
modalities, MMML outperforms traditional single-modality approaches. Two new
frame geometries, not part of the training dataset, are also used for
prediction using the trained MMML model to showcase the ability to generalize
to unseen frame models. The findings underscore MMML's potential in
supplementing traditional simulation-based workflows, particularly in the
conceptual design phase, and highlight its role in bridging the gap between
machine learning and real-world engineering applications. This research paves
the way for the broader adoption of machine learning techniques in engineering
design, with a focus on refining multimodal approaches to optimize structural
development and accelerate the design cycle.

</details>


### [162] [BiListing: Modality Alignment for Listings](https://arxiv.org/abs/2508.20396)
*Guillaume Guy,Mihajlo Grbovic,Chun How Tan,Han Zhao*

Main category: cs.LG

TL;DR: 本文提出BiListing模型，通过利用大型语言模型和预训练语言-图像模型，将Airbnb房源的文本和图片非结构化数据对齐并整合为单一嵌入向量，成功部署到生产环境，提升了搜索排名效果和营收。


<details>
  <summary>Details</summary>
Motivation: Airbnb传统上依赖结构化数据进行房源理解、排名和推荐，但从文本和图片中提取信息复杂且受限。尽管表征学习兴起，但Airbnb房源包含多样的非结构化数据（多张图片、多种文本），将其多模态信息整合成单一有效表示是一个挑战。

Method: 提出了BiListing（Bimodal Listing）方法，利用大型语言模型和预训练语言-图像模型，对房源的文本和图片进行对齐。该方法为每个房源和模态（文本/图片）生成单一的嵌入向量，实现零样本搜索能力，解决冷启动问题，并支持单模态或双模态的房源间搜索。

Result: 通过离线和在线测试，BiListing嵌入成功部署到Airbnb搜索排名模型中。生产环境结果显示，实现了0.425%的NDCG提升，并带来了数千万美元的增量收入。

Conclusion: BiListing通过有效整合和表示Airbnb房源的文本和图片非结构化数据，显著改善了搜索排名性能和业务营收，证明了其在实际应用中的巨大价值。

Abstract: Airbnb is a leader in offering travel accommodations. Airbnb has historically
relied on structured data to understand, rank, and recommend listings to guests
due to the limited capabilities and associated complexity arising from
extracting meaningful information from text and images. With the rise of
representation learning, leveraging rich information from text and photos has
become easier. A popular approach has been to create embeddings for text
documents and images to enable use cases of computing similarities between
listings or using embeddings as features in an ML model.
  However, an Airbnb listing has diverse unstructured data: multiple images,
various unstructured text documents such as title, description, and reviews,
making this approach challenging. Specifically, it is a non-trivial task to
combine multiple embeddings of different pieces of information to reach a
single representation.
  This paper proposes BiListing, for Bimodal Listing, an approach to align text
and photos of a listing by leveraging large-language models and pretrained
language-image models. The BiListing approach has several favorable
characteristics: capturing unstructured data into a single embedding vector per
listing and modality, enabling zero-shot capability to search inventory
efficiently in user-friendly semantics, overcoming the cold start problem, and
enabling listing-to-listing search along a single modality, or both.
  We conducted offline and online tests to leverage the BiListing embeddings in
the Airbnb search ranking model, and successfully deployed it in production,
achieved 0.425% of NDCB gain, and drove tens of millions in incremental
revenue.

</details>


### [163] [TF-TransUNet1D: Time-Frequency Guided Transformer U-Net for Robust ECG Denoising in Digital Twin](https://arxiv.org/abs/2508.20398)
*Shijie Wang,Lei Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为TF-TransUNet1D的新型一维深度神经网络，用于高精度心电图（ECG）信号去噪，以增强心脏数字孪生体的诊断效用。


<details>
  <summary>Details</summary>
Motivation: 心电图信号作为心脏数字孪生体的基础数据源，其诊断效用常因噪声和伪影而受损。因此，需要有效的去噪方法来保证信号的诊断完整性。

Method: 研究提出TF-TransUNet1D模型，这是一种将基于U-Net的编解码器架构与Transformer编码器相结合的一维深度神经网络。该模型采用混合时频域损失函数进行指导，以同时捕获局部形态特征和长程时间依赖性。特别引入了双域损失函数，协同优化时域波形重建和频域频谱保真度，以提高去噪鲁棒性。

Result: TF-TransUNet1D模型在MIT-BIH心律失常数据库和噪声压力测试数据库（NSTDB）上合成损坏的信号进行评估。与现有最先进的基线模型相比，该模型在信噪比（SNR）改善和误差指标方面表现出持续的优越性，平均绝对误差达到0.1285，皮尔逊相关系数为0.9540。

Conclusion: 该工作通过提供高精度去噪技术，弥补了心脏数字孪生体预处理流程中的关键空白，从而能够实现更可靠的实时监测和个性化建模。

Abstract: Electrocardiogram (ECG) signals serve as a foundational data source for
cardiac digital twins, yet their diagnostic utility is frequently compromised
by noise and artifacts. To address this issue, we propose TF-TransUNet1D, a
novel one-dimensional deep neural network that integrates a U-Net-based
encoder-decoder architecture with a Transformer encoder, guided by a hybrid
time-frequency domain loss. The model is designed to simultaneously capture
local morphological features and long-range temporal dependencies, which are
critical for preserving the diagnostic integrity of ECG signals. To enhance
denoising robustness, we introduce a dual-domain loss function that jointly
optimizes waveform reconstruction in the time domain and spectral fidelity in
the frequency domain. In particular, the frequency-domain component effectively
suppresses high-frequency noise while maintaining the spectral structure of the
signal, enabling recovery of subtle but clinically significant waveform
components. We evaluate TF-TransUNet1D using synthetically corrupted signals
from the MIT-BIH Arrhythmia Database and the Noise Stress Test Database
(NSTDB). Comparative experiments against state-of-the-art baselines demonstrate
consistent superiority of our model in terms of SNR improvement and error
metrics, achieving a mean absolute error of 0.1285 and Pearson correlation
coefficient of 0.9540. By delivering high-precision denoising, this work
bridges a critical gap in pre-processing pipelines for cardiac digital twins,
enabling more reliable real-time monitoring and personalized modeling.

</details>


### [164] [Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full Context-Aware Linear Attention](https://arxiv.org/abs/2508.20407)
*Zhongpan Tang*

Main category: cs.LG

TL;DR: Transformer的自注意力机制存在二次复杂度瓶颈。本文提出TLinFormer，一种新型线性注意力架构，通过重构神经元连接模式，实现严格线性复杂度，同时保持精确注意力计算和完整历史上下文感知，显著提升长序列任务的推理性能。


<details>
  <summary>Details</summary>
Motivation: Transformer的核心自注意力机制与序列长度呈二次方关系，严重限制其在长序列任务中的应用。现有线性注意力方法通常通过牺牲模型性能（依赖数据无关核近似或限制上下文选择）来解决此问题。

Method: 本文从连接主义的第一原理和信息流的拓扑结构出发，提出TLinFormer。它通过重新配置神经元连接模式，实现严格的线性复杂度，同时计算精确的注意力分数，并确保信息流感知完整的历史上下文。

Result: 在长序列推理任务中，TLinFormer与标准Transformer基线相比，在推理延迟、KV缓存效率、内存占用和整体加速等关键指标上表现出压倒性优势。

Conclusion: TLinFormer成功解决了Transformer的复杂性瓶颈，并在不牺牲性能的情况下实现了高效的线性注意力，有效地弥合了现有高效注意力方法与标准注意力之间的性能差距。

Abstract: The Transformer architecture has become a cornerstone of modern artificial
intelligence, but its core self-attention mechanism suffers from a complexity
bottleneck that scales quadratically with sequence length, severely limiting
its application in long-sequence tasks. To address this challenge, existing
linear attention methods typically sacrifice model performance by relying on
data-agnostic kernel approximations or restrictive context selection. This
paper returns to the first principles of connectionism, starting from the
topological structure of information flow, to introduce a novel linear
attention architecture-\textbf{TLinFormer}. By reconfiguring neuron connection
patterns, TLinFormer achieves strict linear complexity while computing exact
attention scores and ensuring information flow remains aware of the full
historical context. This design aims to bridge the performance gap prevalent
between existing efficient attention methods and standard attention. Through a
series of experiments, we systematically evaluate the performance of TLinFormer
against a standard Transformer baseline on long-sequence inference tasks. The
results demonstrate that TLinFormer exhibits overwhelming advantages in key
metrics such as \textbf{inference latency}, \textbf{KV cache efficiency},
\textbf{memory footprint}, and \textbf{overall speedup}.

</details>


### [165] [Assessing local deformation and computing scalar curvature with nonlinear conformal regularization of decoders](https://arxiv.org/abs/2508.20413)
*Benjamin Couéraud,Vikram Sunkara,Christof Schütte*

Main category: cs.LG

TL;DR: 本文提出一种新的非线性共形正则化方法，用于深度神经网络近似的自编码器解码器映射，以允许局部变化，并引入共形因子来量化局部变形，同时能计算学习流形的标量曲率。


<details>
  <summary>Details</summary>
Motivation: 高维数据降维对于许多应用至关重要，自编码器是学习低维表示的有效方法。研究动机是为自编码器解码器引入一种几何正则化方法，以允许解码器映射的局部变化，并提供量化局部变形和计算学习流形曲率的能力。

Method: 本研究使用自编码器进行降维，并引入一种新的“非线性共形正则化”方法应用于由深度神经网络近似的解码器映射。该正则化方法允许解码器映射的局部变化，并伴随一个称为“共形因子”的新标量场，用于定量指示潜在空间映射到原始数据空间时所承受的局部变形量。此外，该技术还允许计算学习流形的标量曲率。

Result: 所提出的非线性共形正则化程序允许解码器映射的局部变化。它引入了一个共形因子，可以作为潜在空间映射到原始数据空间时局部变形量的定量指标。此外，该正则化技术还使得计算学习流形的标量曲率成为可能。在Swiss roll和CelebA数据集上的实现和实验说明了如何从架构中获取这些量。

Conclusion: 通过引入非线性共形正则化，本研究提供了一种量化自编码器中潜在空间到数据空间映射的局部变形以及计算学习流形标量曲率的新方法，从而加深了对降维过程中几何特性的理解和分析。

Abstract: One aim of dimensionality reduction is to discover the main factors that
explain the data, and as such is paramount to many applications. When working
with high dimensional data, autoencoders offer a simple yet effective approach
to learn low-dimensional representations. The two components of a general
autoencoder consist first of an encoder that maps the observed data onto a
latent space; and second a decoder that maps the latent space back to the
original observation space, which allows to learn a low-dimensional manifold
representation of the original data. In this article, we introduce a new type
of geometric regularization for decoding maps approximated by deep neural
networks, namely nonlinear conformal regularization. This regularization
procedure permits local variations of the decoder map and comes with a new
scalar field called conformal factor which acts as a quantitative indicator of
the amount of local deformation sustained by the latent space when mapped into
the original data space. We also show that this regularization technique allows
the computation of the scalar curvature of the learned manifold. Implementation
and experiments on the Swiss roll and CelebA datasets are performed to
illustrate how to obtain these quantities from the architecture.

</details>


### [166] [On Identifying Why and When Foundation Models Perform Well on Time-Series Forecasting Using Automated Explanations and Rating](https://arxiv.org/abs/2508.20437)
*Michael Widener,Kausik Lakkaraju,John Aydin,Biplav Srivastava*

Main category: cs.LG

TL;DR: 本研究结合可解释AI方法评估了时间序列预测模型的性能和可解释性，发现特征工程模型在波动性或稀疏数据上表现优于基础模型，且更易解释。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测模型（TSFM）在现实世界应用中日益增多，但其成功或失败的原因、性能变异性和不透明性仍然是挑战，引发了用户对其输出可靠性的担忧。

Method: 本研究结合了传统可解释AI（XAI）方法和评分驱动解释（RDE），评估了TSFM的性能和可解释性。在金融、能源、交通和汽车销售等四个异构数据集上，评估了ARIMA、Gradient Boosting、Chronos（时间序列专用基础模型）和Llama（通用基础模型，包括微调和基础模型）四种不同的模型架构。

Result: 研究发现，在波动性或稀疏领域（如电力、汽车零部件），特征工程模型（如Gradient Boosting）持续优于基础模型（如Chronos），并提供更可解释的解释；而基础模型仅在稳定或趋势驱动的场景（如金融）中表现出色。

Conclusion: 在时间序列预测中，模型选择应根据数据的特性和领域。特征工程模型在数据波动或稀疏时表现出更好的性能和可解释性，而基础模型则更适用于稳定或趋势明确的场景。理解模型的表现和可解释性对于其在实际应用中的可靠性至关重要。

Abstract: Time-series forecasting models (TSFM) have evolved from classical statistical
methods to sophisticated foundation models, yet understanding why and when
these models succeed or fail remains challenging. Despite this known
limitation, time series forecasting models are increasingly used to generate
information that informs real-world actions with equally real consequences.
Understanding the complexity, performance variability, and opaque nature of
these models then becomes a valuable endeavor to combat serious concerns about
how users should interact with and rely on these models' outputs. This work
addresses these concerns by combining traditional explainable AI (XAI) methods
with Rating Driven Explanations (RDE) to assess TSFM performance and
interpretability across diverse domains and use cases. We evaluate four
distinct model architectures: ARIMA, Gradient Boosting, Chronos (time-series
specific foundation model), Llama (general-purpose; both fine-tuned and base
models) on four heterogeneous datasets spanning finance, energy,
transportation, and automotive sales domains. In doing so, we demonstrate that
feature-engineered models (e.g., Gradient Boosting) consistently outperform
foundation models (e.g., Chronos) in volatile or sparse domains (e.g., power,
car parts) while providing more interpretable explanations, whereas foundation
models excel only in stable or trend-driven contexts (e.g., finance).

</details>


### [167] [Uncovering the Spectral Bias in Diagonal State Space Models](https://arxiv.org/abs/2508.20441)
*Ruben Solozabal,Velibor Bojkovic,Hilal AlQuabeh,Kentaro Inui,Martin Takáč*

Main category: cs.LG

TL;DR: 本文从频率视角深入研究了对角线状态空间模型（SSM）的初始化方案，提出了一种基于离散傅里叶域的对角线初始化方法S4D-DFouT，并在大型数据集上达到了当前最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有SSM参数初始化方法主要依赖HiPPO框架，但该框架未明确研究其对角线变体，尽管对角线方案在效率上表现出色。研究的动机在于系统性地理解如何参数化这些对角线模型，并揭示其固有的学习偏差。

Method: 研究人员从频率角度分析了对角线SSM的初始化方案，以理解其参数化和学习偏差。基于这些观察，提出了一种在离散傅里叶域进行的对角线初始化方法S4D-DFouT。

Result: 通过对初始化中极点放置作用的深入理解，研究人员能够进一步扩展这些模型，并在Long Range Arena基准测试中取得了当前最佳（state-of-the-art）结果，使其能够在PathX-256等超大型数据集上进行从头开始的训练。

Conclusion: 通过从频率视角研究对角线SSM初始化，本文不仅揭示了其学习偏差和参数化机制，还提出了一种高效且性能卓越的初始化方法S4D-DFouT，显著提升了SSM在大型数据集上的训练能力和表现。

Abstract: Current methods for initializing state space models (SSMs) parameters mainly
rely on the \textit{HiPPO framework}, which is based on an online approximation
of orthogonal polynomials. Recently, diagonal alternatives have shown to reach
a similar level of performance while being significantly more efficient due to
the simplification in the kernel computation. However, the \textit{HiPPO
framework} does not explicitly study the role of its diagonal variants. In this
paper, we take a further step to investigate the role of diagonal SSM
initialization schemes from the frequency perspective. Our work seeks to
systematically understand how to parameterize these models and uncover the
learning biases inherent in such diagonal state-space models. Based on our
observations, we propose a diagonal initialization on the discrete Fourier
domain \textit{S4D-DFouT}. The insights in the role of pole placing in the
initialization enable us to further scale them and achieve state-of-the-art
results on the Long Range Arena benchmark, allowing us to train from scratch on
very large datasets as PathX-256.

</details>


### [168] [Towards Mitigating Excessive Forgetting in LLM Unlearning via Entanglement-Aware Unlearning with Proxy Constraint](https://arxiv.org/abs/2508.20443)
*Zhihao Liu,Jian Lou,Yuke Hu,Xiaochen Li,Tailun Chen,Yitian Chen,Zhan Qin*

Main category: cs.LG

TL;DR: 本文提出了EAGLE-PC，一种针对大型语言模型（LLM）的新型机器遗忘框架，通过纠缠感知损失重加权和代理约束，有效平衡遗忘效果与模型效用，解决了现有方法遗忘不足和过度遗忘的问题。


<details>
  <summary>Details</summary>
Motivation: 由于LLM训练数据可能包含隐私或版权内容，数据所有者有权要求删除。现有机器遗忘方法缺乏明确的遗忘边界，导致部分数据遗忘不足（存在泄露风险），而另一些数据则过度遗忘（损害模型效用）。

Method: EAGLE-PC框架包含两个核心组件：1) “纠缠感知引导损失重加权”，通过衡量样本在嵌入空间中与保留样本的相似性，确定遗忘强度，实现更具目标性的遗忘。2) “代理约束”，利用ICL（In-Context Learning）生成的测试数据软性正则化遗忘过程，有效缓解过度遗忘。该方法兼容现有梯度优化器，即插即用。

Result: EAGLE-PC在TOFU和MUSE基准测试上对多个LLM均展现出遗忘-效用权衡的持续改进。与NPO+GD优化器结合后，其性能接近完全重训练。

Conclusion: EAGLE-PC为LLM提供了一种可扩展且鲁棒的机器遗忘解决方案，有效解决了现有方法的局限性，实现了更好的遗忘效果与模型效用平衡。

Abstract: Large language models (LLMs) are trained on massive datasets that may include
private or copyrighted content. Due to growing privacy and ownership concerns,
data owners may request the removal of their data from trained models. Machine
unlearning provides a practical solution by removing the influence of specific
data without full retraining. However, most existing methods lack a sound
forgetting boundary, causing some samples to be under-forgotten, leaving
residual leakage risks, while others remain over-forgotten at the expense of
degraded utility.
  In this work, we propose EAGLE-PC (Entanglement-Awareness Guided Loss
Reweighting with Proxy Constraint), a novel unlearning framework that addresses
these limitations through two key components. First, entanglement-awareness
guided loss reweighting determines the forgetting effort of each sample by
measuring its similarity to retain samples in the embedding space, enabling
more targeted and effective unlearning. Second, a proxy constraint leveraging
ICL (In-Context Learning) generated test data softly regularizes the forgetting
process, effectively mitigating over-forgetting. EAGLE-PC is compatible with
existing gradient-based objectives and serves as a plug-and-play enhancement.
We evaluate EAGLE-PC on the TOFU and MUSE benchmarks, showing consistent
improvements in the forgetting-utility trade-off across multiple LLMs. Combined
with the NPO+GD optimizer, it approaches full retraining performance, offering
a scalable and robust unlearning solution.

</details>


### [169] [Evaluating Differentially Private Generation of Domain-Specific Text](https://arxiv.org/abs/2508.20452)
*Yidan Sun,Viktor Schlegel,Srinivasan Nandakumar,Iqra Zahid,Yuping Wu,Warren Del-Pinto,Goran Nenadic,Siew-Kei Lam,Jie Zhang,Anil A Bharath*

Main category: cs.LG

TL;DR: 本文提出了一个统一基准，用于系统评估在差分隐私（DP）保证下生成的合成文本数据的实用性和保真度，并发现当前方法在严格隐私约束下性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在医疗、金融等高风险领域具有巨大潜力，但隐私和监管障碍阻碍了真实数据的使用。差分隐私合成数据生成被视为一种有前景的替代方案，但缺乏系统性评估。

Method: 引入了一个统一的基准来系统评估在形式化差分隐私（DP）保证下生成的文本数据集的实用性和保真度。该基准解决了特定领域基准测试的关键挑战，包括代表性数据选择、实际隐私预算、考虑预训练以及多种评估指标。研究评估了五种领域特定数据集上的最先进隐私保护生成方法。

Result: 研究发现，与真实数据相比，合成数据的实用性和保真度显著下降，尤其是在严格的隐私约束下。这揭示了当前方法的局限性。

Conclusion: 当前隐私保护数据生成方法存在局限性，亟需更先进的隐私保护数据共享方法。本研究为在实际场景中评估这些方法树立了先例，并强调了未来研究方向。

Abstract: Generative AI offers transformative potential for high-stakes domains such as
healthcare and finance, yet privacy and regulatory barriers hinder the use of
real-world data. To address this, differentially private synthetic data
generation has emerged as a promising alternative. In this work, we introduce a
unified benchmark to systematically evaluate the utility and fidelity of text
datasets generated under formal Differential Privacy (DP) guarantees. Our
benchmark addresses key challenges in domain-specific benchmarking, including
choice of representative data and realistic privacy budgets, accounting for
pre-training and a variety of evaluation metrics. We assess state-of-the-art
privacy-preserving generation methods across five domain-specific datasets,
revealing significant utility and fidelity degradation compared to real data,
especially under strict privacy constraints. These findings underscore the
limitations of current approaches, outline the need for advanced
privacy-preserving data sharing methods and set a precedent regarding their
evaluation in realistic scenarios.

</details>


### [170] [Structure-aware Hypergraph Transformer for Diagnosis Prediction in Electronic Health Records](https://arxiv.org/abs/2508.20500)
*Haiyan Wang,Ye Yuan*

Main category: cs.LG

TL;DR: 本文提出了一种结构感知超图Transformer（SHGT）框架，通过结合超图和Transformer来解决现有GNN在EHR诊断预测中无法捕捉高阶依赖和全局关系的问题，并在真实数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络（GNN）的电子健康记录（EHR）分析方法存在两方面不足：a) 依赖成对关系，未能捕获临床数据中固有的高阶依赖；b) 局部消息传递机制限制了其表示能力。

Method: 本文提出了一种新颖的结构感知超图Transformer（SHGT）框架，其核心思想包括：a) 使用超图结构编码器来捕获医疗代码之间的高阶交互；b) 整合Transformer架构对整个超图进行推理；c) 设计了一个包含超图重建的定制损失函数以保留超图的原始结构。

Result: 在真实世界的EHR数据集上进行的实验表明，所提出的SHGT模型在诊断预测方面优于现有的最先进模型。

Conclusion: SHGT框架通过有效解决现有GNN在处理EHR数据时高阶依赖和全局推理能力的局限性，显著提高了诊断预测的性能，为未来EHR分析提供了新途径。

Abstract: Electronic Health Records (EHR) systematically organize patient health data
through standardized medical codes, serving as a comprehensive and invaluable
source for predictive modeling. Graph neural networks (GNNs) have demonstrated
effectiveness in modeling interactions between medical codes within EHR.
However, existing GNN-based methods are inadequate due to: a) their reliance on
pairwise relations fails to capture the inherent higher-order dependencies in
clinical data, and b) the localized message-passing scheme limits
representation power. To address these issues, this paper proposes a novel
Structure-aware HyperGraph Transformer (SHGT) framework following three-fold
ideas: a) employing a hypergraph structural encoder to capture higher-order
interactions among medical codes, b) integrating the Transformer architecture
to reason over the entire hypergraph, and c) designing a tailored loss function
incorporating hypergraph reconstruction to preserve the hypergraph's original
structure. Experiments on real-world EHR datasets demonstrate that the proposed
SHGT outperforms existing state-of-the-art models on diagnosis prediction.

</details>


### [171] [Khiops: An End-to-End, Frugal AutoML and XAI Machine Learning Solution for Large, Multi-Table Databases](https://arxiv.org/abs/2508.20519)
*Marc Boullé,Nicolas Voisine,Bruno Guerraz,Carine Hue,Felipe Olmos,Vladimir Popescu,Stéphane Gouache,Stéphane Bouget,Alexis Bondu,Luc Aurelien Gauthier,Yassine Nair Benrekia,Fabrice Clérot,Vincent Lemaire*

Main category: cs.LG

TL;DR: Khiops是一款基于独特贝叶斯方法的开源机器学习工具，专为挖掘大型多表数据库而设计，提供变量重要性度量、朴素贝叶斯分类/回归模型，并支持大规模数据分析。


<details>
  <summary>Details</summary>
Motivation: 开发Khiops的动机是为了解决大型多表数据库的数据挖掘挑战，提供一个高效、可扩展且基于贝叶斯原理的解决方案，以处理复杂的数据库结构和大规模数据量，并在变量选择、分类、决策树和协同聚类等领域提供先进能力。

Method: Khiops基于独特的贝叶斯方法。它使用离散化模型处理数值数据，通过值聚类处理分类数据来衡量变量重要性。其分类/回归模型是结合了变量选择和权重学习的朴素贝叶斯分类器。对于多表数据库，通过自动构建聚合实现命题化（propositionalisation）。

Result: Khiops成功提供了一个适用于大型多表数据库的强大数据挖掘解决方案。它能够处理数百万个体、数万变量和数亿条记录，并提供预测性变量重要性度量、结合变量选择和权重学习的朴素贝叶斯分类/回归模型。其独特的贝叶斯方法已获得学术界20多篇出版物的认可。

Conclusion: Khiops是一个功能强大、可扩展的开源机器学习工具，尤其适用于处理大规模多表数据库。它结合了独特的贝叶斯方法、先进的变量处理技术、朴素贝叶斯分类/回归模型和自动聚合功能，并通过Python库和用户界面提供广泛的可用性，满足了复杂数据分析的需求。

Abstract: Khiops is an open source machine learning tool designed for mining large
multi-table databases. Khiops is based on a unique Bayesian approach that has
attracted academic interest with more than 20 publications on topics such as
variable selection, classification, decision trees and co-clustering. It
provides a predictive measure of variable importance using discretisation
models for numerical data and value clustering for categorical data. The
proposed classification/regression model is a naive Bayesian classifier
incorporating variable selection and weight learning. In the case of
multi-table databases, it provides propositionalisation by automatically
constructing aggregates. Khiops is adapted to the analysis of large databases
with millions of individuals, tens of thousands of variables and hundreds of
millions of records in secondary tables. It is available on many environments,
both from a Python library and via a user interface.

</details>


### [172] [MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via Generative Reward Learning](https://arxiv.org/abs/2508.20549)
*Weihai Zhi,Jiayan Guo,Shangyang Li*

Main category: cs.LG

TL;DR: MedGR²框架通过自动生成高质量多模态医疗数据，解决了医学VLM领域数据稀缺问题，显著提升了模型在不同模态和任务上的泛化能力，并超越了现有基线和专业RL方法。


<details>
  <summary>Details</summary>
Motivation: 医学VLM应用受限于高质量专家标注数据稀缺。现有SFT方法泛化能力差，而RL因缺乏可靠的奖励信号而难以应用。

Method: 引入MedGR²框架，协同开发数据生成器和奖励模型。该框架能自动、持续生成高质量多模态医疗数据，作为SFT和RL的训练源。通过Group Relative Policy Optimization (GRPO)利用生成数据进行RL训练。

Result: 使用MedGR²生成数据进行SFT训练的模型已超越基于大规模人工标注数据集的基线。结合GRPO进行RL训练后，模型在跨模态和跨任务泛化方面达到SOTA水平，显著优于专用RL方法。此外，MedGR²赋能的紧凑型模型性能可与参数量大10倍的通用基础模型相媲美。

Conclusion: MedGR²为高风险领域的数据高效学习提供了一种新范式，将数据稀缺问题转化为数据生成，充分发挥了RL在构建真正泛化型医学AI方面的潜力。

Abstract: The application of Vision-Language Models (VLMs) in medicine is critically
hampered by the scarcity of high-quality, expert-annotated data. Supervised
Fine-Tuning (SFT) on existing datasets often leads to poor generalization on
unseen modalities and tasks, while Reinforcement Learning (RL), a promising
alternative, is stymied by the lack of reliable reward signals in this
data-scarce domain. To break this impasse, we introduce Generative Reward
Learning for Medical Reasoning (MedGR$^2$), a novel framework that creates a
self-improving virtuous cycle. MedGR$^2$ co-develops a data generator and a
reward model, enabling the automated, continuous creation of high-quality,
multi-modal medical data that serves as both a superior training source for SFT
and RL. Our experiments demonstrate that SFT with MedGR$^2$-produced data
already surpasses baselines trained on large-scale, human-curated datasets.
Crucially, when leveraging this data for RL via Group Relative Policy
Optimization (GRPO), our model achieves state-of-the-art cross-modality and
cross-task generalization, significantly outperforming specialized RL-based
methods. Furthermore, our compact model, empowered by MedGR$^2$, achieves
performance competitive with foundation models possessing over 10 times more
parameters. MedGR$^2$ presents a new paradigm for data-efficient learning in
high-stakes domains, transforming the problem from data scarcity to data
generation and unlocking the full potential of RL for building truly
generalizable medical AI.

</details>


### [173] [Theoretical foundations of the integral indicator application in hyperparametric optimization](https://arxiv.org/abs/2508.20550)
*Roman S. Kulshin,Anatoly A. Sidorov*

Main category: cs.LG

TL;DR: 本文提出一种推荐算法的超参数优化方法，通过结合多项性能指标的整体评估，实现准确性、排序质量、多样性和资源消耗的平衡，并可推广为通用的多标准优化工具。


<details>
  <summary>Details</summary>
Motivation: 传统推荐算法优化方法仅依赖单一指标，难以在准确性、排序质量、输出多样性及算法资源消耗之间取得平衡，需要一种能综合考量多重性能指标的优化方法。

Method: 采用一种超参数优化方法，通过构建一个将多种性能指标（如准确性、排序质量、输出多样性、资源强度）整合为单一综合标准的整体评估，进行推荐算法的优化。

Result: 该方法能够有效平衡算法的准确性、排序质量、输出多样性以及资源强度，优于传统的单一指标设置方法。

Conclusion: 本研究开发了一种通用的多标准优化工具，不仅适用于推荐系统，还可广泛应用于各类机器学习和数据分析任务。

Abstract: The article discusses the concept of hyperparametric optimization of
recommendation algorithms using an integral assessment that combines various
performance indicators into a single consolidated criterion. This approach is
opposed to traditional methods of setting up a single metric and allows you to
achieve a balance between accuracy, ranking quality, variety of output and the
resource intensity of algorithms. The theoretical significance of the research
lies in the development of a universal multi-criteria optimization tool that is
applicable not only in recommendation systems, but also in a wide range of
machine learning and data analysis tasks.

</details>


### [174] [MERIT: Maximum-normalized Element-wise Ratio for Language Model Large-batch Training](https://arxiv.org/abs/2508.20577)
*Yang Luo,Zangwei Zheng,Ziheng Qin,Zirui Zhu,Yong Liu,Yang You*

Main category: cs.LG

TL;DR: MERIT是一种新型优化器，通过引入max-norm信任比率和元素级信任比率，解决了大型批次训练中现有优化器在处理注意力层信息瓶颈和最大注意力logit时性能下降的问题，从而实现了更稳定的训练和更大的批次尺寸。


<details>
  <summary>Details</summary>
Motivation: 大型批次训练在加速深度神经网络训练中至关重要，但存在优化和泛化挑战。现有优化器（如AdamW）在语言模型的大型批次训练中，因注意力层中最大注意力logit的急剧增加导致信息瓶颈而性能下降。LAMB优化器虽部分解决，但其$l_2$-范数信任比率在直接影响最大Q/K权重方面效果不佳，且权重级信任比率忽略了局部权重结构。

Method: 我们提出了一种名为MERIT的新型优化器。它通过以下方式改进：1. 利用max-norm计算信任比率，以更有效地限制最大注意力logit。2. 构建元素级信任比率，通过关注局部权重结构提供更稳健的更新缩放。

Result: 在不同尺寸的GPT-2模型上进行的大型批次训练实验表明，MERIT表现优异。特别是在GPT-2 Medium的训练中，MERIT能够在批次大小为6k的情况下，与标准批次大小（480）相比，在48B训练tokens下没有性能下降。

Conclusion: 这项工作强调了在大型批次训练中考虑最大注意力logit和更细粒度信任比率的重要性。MERIT成功提高了训练稳定性，并为使用更大批次尺寸铺平了道路，从而加速了大型语言模型的开发和迭代。

Abstract: Large-batch training has become a cornerstone in accelerating the training of
deep neural networks, yet it poses challenges in optimization and
generalization. Existing optimizers like AdamW present performance degradation
during language models' large-batch training, due to the information bottleneck
in attention layers caused by the sharp increase of max attention logit. While
the LAMB optimizer partially addresses this issue, some attention layers still
face this issue. The reason is that $l_2$-norm-based trust ratios in LAMB are
less effective in directly influencing the max value of query/key weights.
Furthermore, the weight-wise trust ratio in LAMB is error-prone as it overlooks
relationships of weight values within rows or columns. Building on these
observations, we propose a novel optimizer, MERIT, which leverages the max-norm
to calculate the trust ratio to constrain the max attention logit more
effectively. Moreover, we further construct element-wise trust ratios to
provide more robust update scaling by focusing on local weight structures.
Extensive experiments of large-batch training across various sizes of GPT-2
models demonstrate the superior performance of MERIT. Notably, during the
training of GPT-2 Medium, MERIT enables a 6k batch size without any performance
degradation compared to the standard batch size (480) with 48B training tokens.
This work highlights the importance of considering the max attention logit and
finer-granularity trust ratio in large-batch training. It successfully improves
the training stability and paves the way for larger batch usage, enabling
faster development and iteration of large language models. Code is available at
https://github.com/NUS-HPC-AI-Lab/MERIT.

</details>


### [175] [Unbiased Stochastic Optimization for Gaussian Processes on Finite Dimensional RKHS](https://arxiv.org/abs/2508.20588)
*Neta Shoham,Haim Avron*

Main category: cs.LG

TL;DR: 现有GP超参数学习的近似方法缺乏收敛保证。本文提出针对有限维RKHS核的GP精确随机推理算法，并在内存受限下表现更优。


<details>
  <summary>Details</summary>
Motivation: 目前GP随机超参数学习方法依赖近似（如计算有偏随机梯度或使用诱导点），无法保证收敛到真实边际似然的驻点。

Method: 提出了针对核函数诱导适度有限维再生核希尔伯特空间（RKHS）的GP精确随机推理算法。该方法也可扩展到无限维RKHS，但需放弃精确性。

Result: 无论对于有限维还是无限维RKHS，当内存资源限制了可行的批处理大小和诱导点数量时，该方法均比现有方法取得了更好的实验结果。

Conclusion: 本文为特定核类型（有限维RKHS）的GP提供了精确随机推理解决方案，解决了现有近似方法的局限性，并在资源受限情况下表现出卓越性能。

Abstract: Current methods for stochastic hyperparameter learning in Gaussian Processes
(GPs) rely on approximations, such as computing biased stochastic gradients or
using inducing points in stochastic variational inference. However, when using
such methods we are not guaranteed to converge to a stationary point of the
true marginal likelihood. In this work, we propose algorithms for exact
stochastic inference of GPs with kernels that induce a Reproducing Kernel
Hilbert Space (RKHS) of moderate finite dimension. Our approach can also be
extended to infinite dimensional RKHSs at the cost of forgoing exactness. Both
for finite and infinite dimensional RKHSs, our method achieves better
experimental results than existing methods when memory resources limit the
feasible batch size and the possible number of inducing points.

</details>


### [176] [Local Virtual Nodes for Alleviating Over-Squashing in Graph Neural Networks](https://arxiv.org/abs/2508.20597)
*Tuğrul Hasan Karabulut,İnci M. Baytaş*

Main category: cs.LG

TL;DR: 本研究提出局部虚拟节点（LVN）方法，通过引入可训练嵌入的虚拟节点，在不破坏图全局拓扑的前提下，缓解图神经网络中的过挤压问题，提升长距离依赖任务性能。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在处理长距离依赖任务时，其感受野需要足够大，但将广泛邻域信息压缩到固定大小的节点表示中会导致消息传递瓶颈（过挤压）。现有解决方案（图重连、添加虚拟节点）会改变输入图的全局拓扑并破坏原始图结构中编码的领域知识，这可能对特定任务和领域至关重要。

Method: 本研究提出局部虚拟节点（LVN），其位置由节点中心性决定，旨在改善潜在瓶颈区域的连通性。LVN具有可训练的嵌入，这些嵌入在选定的中心区域共享，从而促进远距离节点间的通信，且无需增加更多层。

Result: 在基准数据集上的大量实验表明，LVN能增强结构连通性，并显著提高图和节点分类任务的性能。

Conclusion: LVN是一种有效缓解过挤压的方法，它在不显著破坏输入图全局结构的前提下，通过局部改善连通性，显著提升了图神经网络在长距离依赖任务上的表现。

Abstract: Over-squashing is a challenge in training graph neural networks for tasks
involving long-range dependencies. In such tasks, a GNN's receptive field
should be large enough to enable communication between distant nodes. However,
gathering information from a wide range of neighborhoods and squashing its
content into fixed-size node representations makes message-passing vulnerable
to bottlenecks. Graph rewiring and adding virtual nodes are commonly studied
remedies that create additional pathways around bottlenecks to mitigate
over-squashing. However, these techniques alter the input graph's global
topology and disrupt the domain knowledge encoded in the original graph
structure, both of which could be essential to specific tasks and domains. This
study presents Local Virtual Nodes (LVN) with trainable embeddings to alleviate
the effects of over-squashing without significantly corrupting the global
structure of the input graph. The position of the LVNs is determined by the
node centrality, which indicates the existence of potential bottlenecks. Thus,
the proposed approach aims to improve the connectivity in the regions with
likely bottlenecks. Furthermore, trainable LVN embeddings shared across
selected central regions facilitate communication between distant nodes without
adding more layers. Extensive experiments on benchmark datasets demonstrate
that LVNs can enhance structural connectivity and significantly improve
performance on graph and node classification tasks. The code can be found at
https://github.com/ALLab-Boun/LVN/}{https://github.com/ALLab-Boun/LVN/.

</details>


### [177] [Dimension Agnostic Testing of Survey Data Credibility through the Lens of Regression](https://arxiv.org/abs/2508.20616)
*Debabrota Basu,Sourav Chakraborty,Debarshi Chanda,Buddha Dev Das,Arijit Ghosh,Arnab Ray*

Main category: cs.LG

TL;DR: 本文提出一种基于任务、针对特定模型的距离度量方法，以评估抽样调查的可信度，尤其适用于回归模型。该算法的样本复杂度与数据维度无关，通过验证可信度而非重建模型实现高效率。


<details>
  <summary>Details</summary>
Motivation: 确保抽样调查数据能可靠地代表总体是下游研究有效性的关键。传统方法需要估计高维分布间的距离，其样本复杂度随维度呈指数增长，但有时特定模型下的结论在不同基础分布下可能保持一致。

Method: 提出了一种基于任务的方法来评估抽样调查数据的可信度，并引入了特定于模型的距离度量来量化这种可信度。设计了一种算法，用于在回归模型背景下验证调查数据的可信度。

Result: 所提出的算法的样本复杂度与数据维度无关。这种效率源于算法专注于验证调查数据的可信度，而非重建基础回归模型。此外，研究表明，如果通过重建回归模型来验证可信度，样本复杂度会随数据维度线性增长。算法的理论正确性得到证明，并在数值上展示了其性能。

Conclusion: 通过引入一种基于任务的、模型特定的方法，可以高效且独立于数据维度地验证抽样调查数据的可信度，特别是在回归模型中，这优于需要重建底层模型的方法。

Abstract: Assessing whether a sample survey credibly represents the population is a
critical question for ensuring the validity of downstream research. Generally,
this problem reduces to estimating the distance between two high-dimensional
distributions, which typically requires a number of samples that grows
exponentially with the dimension. However, depending on the model used for data
analysis, the conclusions drawn from the data may remain consistent across
different underlying distributions. In this context, we propose a task-based
approach to assess the credibility of sampled surveys. Specifically, we
introduce a model-specific distance metric to quantify this notion of
credibility. We also design an algorithm to verify the credibility of survey
data in the context of regression models. Notably, the sample complexity of our
algorithm is independent of the data dimension. This efficiency stems from the
fact that the algorithm focuses on verifying the credibility of the survey data
rather than reconstructing the underlying regression model. Furthermore, we
show that if one attempts to verify credibility by reconstructing the
regression model, the sample complexity scales linearly with the dimensionality
of the data. We prove the theoretical correctness of our algorithm and
numerically demonstrate our algorithm's performance.

</details>


### [178] [Supervised Stochastic Gradient Algorithms for Multi-Trial Source Separation](https://arxiv.org/abs/2508.20618)
*Ronak Mehta,Mateus Piovezan Otto,Noah Stanis,Azadeh Yazdan-Shahmorad,Zaid Harchaoui*

Main category: cs.LG

TL;DR: 开发了一种结合多试验监督的随机独立成分分析（ICA）算法，通过融合近端梯度和预测模型学习，提高了非凸优化的成功率和独立分量的可解释性。


<details>
  <summary>Details</summary>
Motivation: 许多科学背景中存在多试验监督信息，但现有的ICA算法未能充分利用，研究旨在将这些监督信息融入ICA以提升其性能。

Method: 提出了一种随机算法，该方法将可逆矩阵空间中的近端梯度类型算法与通过反向传播联合学习的预测模型相结合。算法在合成数据和真实数据上进行了实验验证。

Result: 通过额外的监督，观察到非凸优化的成功率显著提高，并且独立分量的可解释性得到了改善。

Conclusion: 将多试验监督融入独立成分分析算法中，能够有效提升非凸优化的鲁棒性和结果的可解释性。

Abstract: We develop a stochastic algorithm for independent component analysis that
incorporates multi-trial supervision, which is available in many scientific
contexts. The method blends a proximal gradient-type algorithm in the space of
invertible matrices with joint learning of a prediction model through
backpropagation. We illustrate the proposed algorithm on synthetic and real
data experiments. In particular, owing to the additional supervision, we
observe an increased success rate of the non-convex optimization and the
improved interpretability of the independent components.

</details>


### [179] [Masked Autoencoders for Ultrasound Signals: Robust Representation Learning for Downstream Applications](https://arxiv.org/abs/2508.20622)
*Immanuel Roßteutscher,Klaus S. Drese,Thorsten Uphues*

Main category: cs.LG

TL;DR: 本文探索了将基于Vision Transformer的掩码自编码器（MAE-ViT）应用于一维超声信号的自监督表示学习。通过在无标签合成数据上预训练，MAE模型能学习到鲁棒表示，显著提升了下游任务（如飞行时间分类）的性能，并展现出优异的真实数据迁移性。


<details>
  <summary>Details</summary>
Motivation: 尽管MAE在计算机视觉等领域取得了巨大成功，但其在一维信号分析，特别是原始超声数据领域的应用尚待探索。超声信号在工业应用中至关重要（如NDT和SHM），但面临标记数据稀缺和信号处理高度任务特异的挑战。

Method: 提出一种方法，利用MAE在无标签合成超声信号上进行预训练，使模型能够学习鲁棒的表示，从而增强下游任务（例如飞行时间分类）的性能。研究系统地考察了模型大小、补丁大小和掩码比例对预训练效率和下游任务准确性的影响。

Result: 研究结果表明，预训练模型显著优于从头训练的模型和针对下游任务优化的强卷积神经网络（CNN）基线。此外，在合成数据上预训练的模型，其向真实世界测量信号的迁移性优于仅在有限真实数据集上训练的模型。

Conclusion: 这项研究强调了MAE在通过可扩展的自监督学习推进超声信号分析方面的巨大潜力。

Abstract: We investigated the adaptation and performance of Masked Autoencoders (MAEs)
with Vision Transformer (ViT) architectures for self-supervised representation
learning on one-dimensional (1D) ultrasound signals. Although MAEs have
demonstrated significant success in computer vision and other domains, their
use for 1D signal analysis, especially for raw ultrasound data, remains largely
unexplored. Ultrasound signals are vital in industrial applications such as
non-destructive testing (NDT) and structural health monitoring (SHM), where
labeled data are often scarce and signal processing is highly task-specific. We
propose an approach that leverages MAE to pre-train on unlabeled synthetic
ultrasound signals, enabling the model to learn robust representations that
enhance performance in downstream tasks, such as time-of-flight (ToF)
classification. This study systematically investigated the impact of model
size, patch size, and masking ratio on pre-training efficiency and downstream
accuracy. Our results show that pre-trained models significantly outperform
models trained from scratch and strong convolutional neural network (CNN)
baselines optimized for the downstream task. Additionally, pre-training on
synthetic data demonstrates superior transferability to real-world measured
signals compared with training solely on limited real datasets. This study
underscores the potential of MAEs for advancing ultrasound signal analysis
through scalable, self-supervised learning.

</details>


### [180] [GDS Agent: A Graph Algorithmic Reasoning Agent](https://arxiv.org/abs/2508.20637)
*Borun Shi,Ioannis Panagiotas*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）在图数据处理和推理方面表现不佳。本文提出了GDS（图数据科学）代理，通过将图算法作为工具集成到模型上下文协议（MCP）服务器中，使LLMs能够对图结构数据进行高效且准确的推理，并通过新基准测试验证了其在广泛图任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多模态信息处理和推理方面表现出色，并能通过工具调用和检索增强技术访问封闭数据源，但它们在处理和推理大规模图结构数据时仍面临挑战。

Method: 引入了GDS（图数据科学）代理，它在一个模型上下文协议（MCP）服务器中集成了全面的图算法工具，并辅以算法结果的预处理（检索）和后处理。该服务器可与任何现代LLM即插即用，使用户能够提出隐式或内在需要图算法推理的问题，并快速获得准确、有依据的答案。此外，还引入了一个评估中间工具调用和最终响应的新基准。

Result: GDS代理能够解决广泛的图任务。新基准测试的结果表明，GDS代理在图任务中表现出色。研究还提供了详细的开放式任务案例研究，并分析了代理在某些场景下遇到困难的情况。

Conclusion: GDS代理有效地解决了LLMs在图结构数据推理方面的不足，通过整合图算法工具实现了对复杂图任务的准确处理。研究讨论了当前挑战和未来发展方向，为LLMs在图数据科学领域的应用开辟了道路。

Abstract: Large language models (LLMs) have shown remarkable multimodal information
processing and reasoning ability. When equipped with tools through function
calling and enhanced with retrieval-augmented techniques, compound LLM-based
systems can access closed data sources and answer questions about them.
However, they still struggle to process and reason over large-scale
graph-structure data. We introduce the GDS (Graph Data Science) agent in this
technical report. The GDS agent introduces a comprehensive set of graph
algorithms as tools, together with preprocessing (retrieval) and postprocessing
of algorithm results, in a model context protocol (MCP) server. The server can
be used with any modern LLM out-of-the-box. GDS agent allows users to ask any
question that implicitly and intrinsically requires graph algorithmic reasoning
about their data, and quickly obtain accurate and grounded answers. We also
introduce a new benchmark that evaluates intermediate tool calls as well as
final responses. The results indicate that GDS agent is able to solve a wide
spectrum of graph tasks. We also provide detailed case studies for more
open-ended tasks and study scenarios where the agent struggles. Finally, we
discuss the remaining challenges and the future roadmap.

</details>


### [181] [A Hybrid Stochastic Gradient Tracking Method for Distributed Online Optimization Over Time-Varying Directed Networks](https://arxiv.org/abs/2508.20645)
*Xinli Shi,Xingxing Yuan,Longkang Zhu,Guanghui Wen*

Main category: cs.LG

TL;DR: 本文提出了一种名为TV-HSGT的新型分布式在线优化算法，用于时变有向网络。该算法结合混合随机梯度跟踪和方差削减机制，在不依赖梯度有界假设的情况下，实现了改进的动态后悔界，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有分布式在线优化算法在处理大规模、动态数据和时变有向网络时存在局限性，它们常依赖梯度有界假设并忽略随机梯度的影响。

Method: 提出TV-HSGT算法，其基于混合随机梯度跟踪和方差削减机制。该算法在时变有向图中整合了行随机和列随机通信方案，无需Perron向量估计或出度信息。通过结合当前和递归随机梯度，有效减少了梯度方差并准确跟踪全局下降方向。

Result: 理论分析证明TV-HSGT在不假设梯度有界的情况下，能实现改进的动态后悔界。逻辑回归任务的实验结果证实了TV-HSGT在动态和资源受限环境中的有效性。

Conclusion: TV-HSGT算法为时变有向网络中的分布式在线优化提供了一种有效解决方案，通过创新的机制克服了现有算法的局限性，实现了更优的性能和更强的鲁棒性。

Abstract: With the increasing scale and dynamics of data, distributed online
optimization has become essential for real-time decision-making in various
applications. However, existing algorithms often rely on bounded gradient
assumptions and overlook the impact of stochastic gradients, especially in
time-varying directed networks. This study proposes a novel Time-Varying Hybrid
Stochastic Gradient Tracking algorithm named TV-HSGT, based on hybrid
stochastic gradient tracking and variance reduction mechanisms. Specifically,
TV-HSGT integrates row-stochastic and column-stochastic communication schemes
over time-varying digraphs, eliminating the need for Perron vector estimation
or out-degree information. By combining current and recursive stochastic
gradients, it effectively reduces gradient variance while accurately tracking
global descent directions. Theoretical analysis demonstrates that TV-HSGT can
achieve improved bounds on dynamic regret without assuming gradient
boundedness. Experimental results on logistic regression tasks confirm the
effectiveness of TV-HSGT in dynamic and resource-constrained environments.

</details>


### [182] [VarDiU: A Variational Diffusive Upper Bound for One-Step Diffusion Distillation](https://arxiv.org/abs/2508.20646)
*Leyang Wang,Mingtian Zhang,Zijing Ou,David Barber*

Main category: cs.LG

TL;DR: 本文提出VarDiU（变分扩散上界），为扩散蒸馏提供无偏梯度估计，解决了现有方法因有偏估计导致的性能问题，实现了更高质量和更稳定的单步生成。


<details>
  <summary>Details</summary>
Motivation: 现有扩散蒸馏方法在训练学生模型时，其梯度估计依赖于不完美的去噪分数匹配（DSM）训练，导致梯度有偏，进而影响了模型性能。

Method: 提出VarDiU（Variational Diffusive Upper Bound）目标函数，该函数能提供无偏的梯度估计，并可直接应用于扩散蒸馏过程。

Result: 与Diff-Instruct相比，VarDiU实现了更高的生成质量，并提供了更高效、更稳定的单步扩散蒸馏训练过程。

Conclusion: VarDiU通过引入无偏梯度估计，有效克服了现有扩散蒸馏方法的局限性，显著提升了单步生成模型的性能和训练效率。

Abstract: Recently, diffusion distillation methods have compressed thousand-step
teacher diffusion models into one-step student generators while preserving
sample quality. Most existing approaches train the student model using a
diffusive divergence whose gradient is approximated via the student's score
function, learned through denoising score matching (DSM). Since DSM training is
imperfect, the resulting gradient estimate is inevitably biased, leading to
sub-optimal performance. In this paper, we propose VarDiU (pronounced
/va:rdju:/), a Variational Diffusive Upper Bound that admits an unbiased
gradient estimator and can be directly applied to diffusion distillation. Using
this objective, we compare our method with Diff-Instruct and demonstrate that
it achieves higher generation quality and enables a more efficient and stable
training procedure for one-step diffusion distillation.

</details>


### [183] [Physics-Constrained Machine Learning for Chemical Engineering](https://arxiv.org/abs/2508.20649)
*Angan Mukherjee,Victor M. Zavala*

Main category: cs.LG

TL;DR: 本综述分析了物理约束机器学习（PCML）在化学工程中的应用，总结了其发展，并强调了面临的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 尽管PCML在多个领域显示出显著优势，但在复杂的化学工程应用中仍面临技术和理论挑战，如物理知识嵌入、模型融合、大规模扩展和不确定性量化等，因此需要深入探讨其在该领域的适用性。

Method: 本文以“观点”形式，总结了PCML的最新发展，并分析和强调了在化学工程中应用PCML所面临的挑战和机遇，重点关注闭环实验设计、实时动力学与控制以及多尺度现象处理。

Result: 识别出PCML在化学工程中应用的多个关键挑战，包括物理知识嵌入的量与类型、有效的ML融合策略、模型对大规模数据和模拟器的扩展性，以及预测不确定性量化。同时，强调了在闭环实验设计、实时动力学与控制以及多尺度现象处理等方面的应用机遇。

Conclusion: PCML在化学工程领域具有巨大潜力，但其广泛应用和进步取决于能否有效解决已识别的关键挑战，特别是在闭环设计、实时控制和多尺度建模等特定应用场景中。

Abstract: Physics-constrained machine learning (PCML) combines physical models with
data-driven approaches to improve reliability, generalizability, and
interpretability. Although PCML has shown significant benefits in diverse
scientific and engineering domains, technical and intellectual challenges
hinder its applicability in complex chemical engineering applications. Key
difficulties include determining the amount and type of physical knowledge to
embed, designing effective fusion strategies with ML, scaling models to large
datasets and simulators, and quantifying predictive uncertainty. This
perspective summarizes recent developments and highlights
challenges/opportunities in applying PCML to chemical engineering, emphasizing
on closed-loop experimental design, real-time dynamics and control, and
handling of multi-scale phenomena.

</details>


### [184] [Self-Composing Neural Operators with Depth and Accuracy Scaling via Adaptive Train-and-Unroll Approach](https://arxiv.org/abs/2508.20650)
*Juncai He,Xinliang Liu,Jinchao Xu*

Main category: cs.LG

TL;DR: 本文提出一种通过自组合增强神经算子效率和准确性的新框架，采用自适应训练策略逐步加深模型，并在理论保证下，实现了计算效率高、性能达到SOTA且适用于大规模科学机器学习应用的解决方案。


<details>
  <summary>Details</summary>
Motivation: 旨在提升神经算子的效率、准确性及模型容量，并解决其训练效率问题，灵感来源于求解数值偏微分方程的迭代方法。

Method: 设计了一个基于自组合的神经算子框架，通过重复应用单个神经算子块来逐步加深模型。引入了自适应训练-展开（train-and-unroll）方法，在训练过程中逐步增加神经算子的深度，以实现高效训练。在特定应用中（如USCT），采用了多网格启发式骨干网络。

Result: 揭示了模型深度与准确性之间的标度律。通过自适应训练策略显著节省了计算成本。在标准基准测试中取得了最先进（SOTA）的性能。在具有挑战性的高频超声计算机断层扫描（USCT）问题上展示了卓越的性能，能够有效解析复杂波现象。

Conclusion: 所提出的框架为大规模数据驱动的科学机器学习应用提供了一个计算上可行、准确且可扩展的解决方案。

Abstract: In this work, we propose a novel framework to enhance the efficiency and
accuracy of neural operators through self-composition, offering both
theoretical guarantees and practical benefits. Inspired by iterative methods in
solving numerical partial differential equations (PDEs), we design a specific
neural operator by repeatedly applying a single neural operator block, we
progressively deepen the model without explicitly adding new blocks, improving
the model's capacity. To train these models efficiently, we introduce an
adaptive train-and-unroll approach, where the depth of the neural operator is
gradually increased during training. This approach reveals an accuracy scaling
law with model depth and offers significant computational savings through our
adaptive training strategy. Our architecture achieves state-of-the-art (SOTA)
performance on standard benchmarks. We further demonstrate its efficacy on a
challenging high-frequency ultrasound computed tomography (USCT) problem, where
a multigrid-inspired backbone enables superior performance in resolving complex
wave phenomena. The proposed framework provides a computationally tractable,
accurate, and scalable solution for large-scale data-driven scientific machine
learning applications.

</details>


### [185] [Compositionality in Time Series: A Proof of Concept using Symbolic Dynamics and Compositional Data Augmentation](https://arxiv.org/abs/2508.20656)
*Michael Hagmann,Michael Staniek,Stefan Riezler*

Main category: cs.LG

TL;DR: 本研究探索临床时间序列是否由有序的潜在状态生成，并开发方法生成合成数据以解决数据稀疏问题，结果显示合成数据在预测任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决临床时间序列数据稀疏和低资源问题，加深对临床数据的理解。探讨临床测量是否由遵循系统性原则的生理状态序列生成。

Method: 将时间序列的组合性概念化为数据生成过程的属性，并开发数据驱动的方法来重建基本状态和组合规则。通过域适应视角的两个经验测试评估方法，比较原始数据和合成数据分布的相似性及预测模型的预期风险。

Result: 通过组合合成数据训练的模型在测试集上的表现与通过原始数据训练的模型相当，且优于基于随机化的数据增强方法。在序列性器官衰竭评估（SOFA）分数预测任务中，完全基于合成数据训练的模型相比基于原始数据训练的模型有显著的性能提升。

Conclusion: 临床时间序列可以被理解为由系统有序的潜在状态序列生成，且通过这种方法生成的合成数据能有效缓解数据稀缺问题，并在特定预测任务中达到甚至超越使用原始数据训练的模型性能。

Abstract: This work investigates whether time series of natural phenomena can be
understood as being generated by sequences of latent states which are ordered
in systematic and regular ways. We focus on clinical time series and ask
whether clinical measurements can be interpreted as being generated by
meaningful physiological states whose succession follows systematic principles.
Uncovering the underlying compositional structure will allow us to create
synthetic data to alleviate the notorious problem of sparse and low-resource
data settings in clinical time series forecasting, and deepen our understanding
of clinical data. We start by conceptualizing compositionality for time series
as a property of the data generation process, and then study data-driven
procedures that can reconstruct the elementary states and composition rules of
this process. We evaluate the success of this methods using two empirical tests
originating from a domain adaptation perspective. Both tests infer the
similarity of the original time series distribution and the synthetic time
series distribution from the similarity of expected risk of time series
forecasting models trained and tested on original and synthesized data in
specific ways. Our experimental results show that the test set performance
achieved by training on compositionally synthesized data is comparable to
training on original clinical time series data, and that evaluation of models
on compositionally synthesized test data shows similar results to evaluating on
original test data, outperforming randomization-based data augmentation. An
additional downstream evaluation of the prediction task of sequential organ
failure assessment (SOFA) scores shows significant performance gains when model
training is entirely based on compositionally synthesized data compared to
training on original data.

</details>


### [186] [Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning](https://arxiv.org/abs/2508.20697)
*Weitao Feng,Lixu Wang,Tianyi Wei,Jie Zhang,Chongyang Gao,Sinong Zhan,Peizhuo Lv,Wei Dong*

Main category: cs.LG

TL;DR: 本研究揭示了基于强化学习（RL）的恶意微调比监督微调（SFT）更有效地破坏大型语言模型（LLM）的安全对齐。为应对此威胁，我们提出了TokenBuncher，通过抑制模型响应不确定性来有效防御RL驱动的恶意微调，同时保持模型的良性任务能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型能力不断增强，通过微调进行恶意滥用的风险也日益增加。现有研究主要关注监督微调（SFT）的滥用，但本研究旨在系统性地证明并解决强化学习（RL）可能更有效地破坏安全对齐并促进高级恶意任务协助的潜在威胁。

Method: 首先，系统性地演示了在相同计算预算下，强化学习（RL）比监督微调（SFT）更有效地实现恶意滥用。然后，提出TokenBuncher作为第一个专门针对RL恶意微调的有效防御方法。TokenBuncher通过抑制模型响应不确定性来工作，使其无法利用独特的奖励信号驱动模型产生有害行为。具体实现包括采用熵作为奖励的RL和Token Noiser机制，以防止专家领域有害能力的升级。

Result: 研究结果表明，在匹配的计算预算下，基于RL的微调比SFT更能有效地打破安全对齐并促进高级有害任务协助。TokenBuncher能够有效且鲁棒地缓解有害的RL微调，同时保留良性任务的实用性和可微调性。这凸显了RL恶意微调比SFT构成更大的系统性风险。

Conclusion: 基于强化学习的恶意微调对大型语言模型构成比监督微调更大的系统性风险。TokenBuncher提供了一种有效且通用的防御机制，能够成功抵御这种新兴威胁，同时保持模型的正常功能。

Abstract: As large language models (LLMs) continue to grow in capability, so do the
risks of harmful misuse through fine-tuning. While most prior studies assume
that attackers rely on supervised fine-tuning (SFT) for such misuse, we
systematically demonstrate that reinforcement learning (RL) enables adversaries
to more effectively break safety alignment and facilitate advanced harmful task
assistance, under matched computational budgets. To counter this emerging
threat, we propose TokenBuncher, the first effective defense specifically
targeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation
on which RL relies: model response uncertainty. By constraining uncertainty,
RL-based fine-tuning can no longer exploit distinct reward signals to drive the
model toward harmful behaviors. We realize this defense through
entropy-as-reward RL and a Token Noiser mechanism designed to prevent the
escalation of expert-domain harmful capabilities. Extensive experiments across
multiple models and RL algorithms show that TokenBuncher robustly mitigates
harmful RL fine-tuning while preserving benign task utility and finetunability.
Our results highlight that RL-based harmful fine-tuning poses a greater
systemic risk than SFT, and that TokenBuncher provides an effective and general
defense.

</details>


### [187] [EEGDM: Learning EEG Representation with Latent Diffusion Model](https://arxiv.org/abs/2508.20705)
*Shaocong Wang,Tong Liu,Ming Li,Minjing Yu,Yong-Jin Liu*

Main category: cs.LG

TL;DR: 提出EEGDM，一种基于潜在扩散模型的自监督EEG表征学习方法，通过信号生成任务捕获EEG语义，在数据受限下实现跨任务的鲁棒泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有EEG深度学习方法在数据量有限时，难以学习到可泛化的EEG表征。当前的表征学习方法（如EEGPT和LaBraM）多依赖简单的掩码重建目标，未能充分捕获EEG信号的丰富语义信息和复杂模式。

Method: 本文提出EEGDM，一种基于潜在扩散模型的自监督EEG表征学习方法。该方法利用EEG信号生成作为自监督目标，将扩散模型转化为强大的表征学习器。EEGDM包含一个EEG编码器，用于将EEG信号及其通道增强提炼为紧凑的潜在表征，作为条件信息指导扩散模型生成EEG信号。

Result: 实验结果表明，EEGDM能够重建高质量的EEG信号，有效学习到鲁棒的表征，并在少量预训练数据下，在多种下游任务上实现有竞争力的性能，验证了其泛化性和实用性。

Conclusion: EEGDM成功利用潜在扩散模型学习到通用且鲁棒的EEG信号表征，有效解决了现有方法在有限数据下泛化能力不足的挑战，具有重要的实际应用价值。

Abstract: While electroencephalography (EEG) signal analysis using deep learning has
shown great promise, existing approaches still face significant challenges in
learning generalizable representations that perform well across diverse tasks,
particularly when training data is limited. Current EEG representation learning
methods including EEGPT and LaBraM typically rely on simple masked
reconstruction objective, which may not fully capture the rich semantic
information and complex patterns inherent in EEG signals. In this paper, we
propose EEGDM, a novel self-supervised EEG representation learning method based
on the latent diffusion model, which leverages EEG signal generation as a
self-supervised objective, turning the diffusion model into a strong
representation learner capable of capturing EEG semantics. EEGDM incorporates
an EEG encoder that distills EEG signals and their channel augmentations into a
compact representation, acting as conditional information to guide the
diffusion model for generating EEG signals. This design endows EEGDM with a
compact latent space, which not only offers ample control over the generative
process but also can be leveraged for downstream tasks. Experimental results
show that EEGDM (1) can reconstruct high-quality EEG signals, (2) effectively
learns robust representations, and (3) achieves competitive performance with
modest pre-training data size across diverse downstream tasks, underscoring its
generalizability and practical utility.

</details>


### [188] [Provable Benefits of In-Tool Learning for Large Language Models](https://arxiv.org/abs/2508.20755)
*Sam Houliston,Ambroise Odonnat,Charles Arnal,Vivien Cabannes*

Main category: cs.LG

TL;DR: 本研究理论和实证地证明了工具增强型语言模型（通过外部检索）在事实召回方面具有无限扩展性，优于受限于参数数量的权重内记忆学习。


<details>
  <summary>Details</summary>
Motivation: 工具增强型语言模型正在重塑AI，但其理论优势，尤其是在事实召回方面的优势，尚未得到充分探索。本文旨在通过比较工具内学习（外部检索）和权重内学习（记忆）来解决这一问题。

Method: 1. 理论分析：证明了模型在权重内记忆的事实数量受限于其参数量，并通过构建简单高效的电路证明了工具使用能实现无限制的事实召回。2. 受控实验：验证了工具使用模型在事实召回方面持续优于记忆模型的结论。3. 实践验证：进一步展示了对预训练大型语言模型而言，教授工具使用和通用规则比通过微调将事实记忆到权重中更有效。

Result: 1. 模型通过权重记忆的事实数量受其参数量限制。2. 工具使用（外部检索）能够通过高效的电路构造实现无限制的事实召回。3. 工具使用模型在受控实验中持续优于记忆模型。4. 对于预训练的大型语言模型，教授工具使用和通用规则比直接微调事实到记忆中更有效。

Conclusion: 工具增强型工作流不仅实用，而且在理论和实践上都被证明具有更高的可扩展性，尤其在事实召回方面，优于仅依赖权重内记忆的方法。

Abstract: Tool-augmented language models, equipped with retrieval, memory, or external
APIs, are reshaping AI, yet their theoretical advantages remain underexplored.
In this paper, we address this question by demonstrating the benefits of
in-tool learning (external retrieval) over in-weight learning (memorization)
for factual recall. We show that the number of facts a model can memorize
solely in its weights is fundamentally limited by its parameter count. In
contrast, we prove that tool-use enables unbounded factual recall via a simple
and efficient circuit construction. These results are validated in controlled
experiments, where tool-using models consistently outperform memorizing ones.
We further show that for pretrained large language models, teaching tool-use
and general rules is more effective than finetuning facts into memory. Our work
provides both a theoretical and empirical foundation, establishing why
tool-augmented workflows are not just practical, but provably more scalable.

</details>


### [189] [Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI](https://arxiv.org/abs/2508.20773)
*Christoforos N. Spartalis,Theodoros Semertzidis,Petros Daras,Efstratios Gavves*

Main category: cs.LG

TL;DR: SAFEMax是一种针对扩散模型的机器遗忘新方法，通过最大化生成图像的熵来抹除不允许的类别信息，并显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 在扩散模型中，需要一种有效且高效的方法来执行机器遗忘，以清除不允许的类别信息。

Method: SAFEMax方法基于信息论原理，通过最大化生成图像的熵，使模型在给定不允许类别条件时生成高斯噪声，从而最终停止去噪过程。此外，该方法通过选择性地关注早期扩散步骤（其中类别特定信息突出），来控制遗忘与保留之间的平衡。

Result: 实验结果证明了SAFEMax的有效性，并突出了其相对于现有最先进方法的显著效率提升。

Conclusion: SAFEMax为扩散模型中的机器遗忘提供了一种有效且高效的解决方案，其性能和效率均优于现有方法。

Abstract: We introduce SAFEMax, a novel method for Machine Unlearning in diffusion
models. Grounded in information-theoretic principles, SAFEMax maximizes the
entropy in generated images, causing the model to generate Gaussian noise when
conditioned on impermissible classes by ultimately halting its denoising
process. Also, our method controls the balance between forgetting and retention
by selectively focusing on the early diffusion steps, where class-specific
information is prominent. Our results demonstrate the effectiveness of SAFEMax
and highlight its substantial efficiency gains over state-of-the-art methods.

</details>


### [190] [cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending](https://arxiv.org/abs/2508.20818)
*Anirudh Satheesh,Keenan Powell,Hua Wei*

Main category: cs.LG

TL;DR: cMALC-D是一种上下文多智能体LLM引导的课程学习框架，通过LLM生成语义课程和多样性上下文混合机制，显著提高了多智能体在复杂环境中的泛化能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习(MARL)算法在固定环境中训练，在复杂不确定的现实世界中表现脆弱。现有的上下文MARL (cMARL)方法使用课程学习，但依赖于在多智能体设置下不稳定且不可靠的代理信号。

Method: 我们提出了Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending (cMALC-D)框架。该框架使用大型语言模型(LLMs)生成具有语义意义的课程和更鲁棒的评估信号。此外，引入了一种新颖的基于多样性的上下文混合机制，通过结合现有上下文的特征来创建新的训练场景，以防止模式崩溃并鼓励探索。

Result: 在交通信号控制领域的实验表明，与现有课程学习基线相比，cMALC-D显著提高了泛化能力和样本效率。

Conclusion: cMALC-D通过结合LLM引导的课程学习和多样性上下文混合，有效解决了现有cMARL方法中代理信号不可靠的问题，大幅提升了多智能体策略在复杂环境中的适应性和学习效率。

Abstract: Many multi-agent reinforcement learning (MARL) algorithms are trained in
fixed simulation environments, making them brittle when deployed in real-world
scenarios with more complex and uncertain conditions. Contextual MARL (cMARL)
addresses this by parameterizing environments with context variables and
training a context-agnostic policy that performs well across all environment
configurations. Existing cMARL methods attempt to use curriculum learning to
help train and evaluate context-agnostic policies, but they often rely on
unreliable proxy signals, such as value estimates or generalized advantage
estimates that are noisy and unstable in multi-agent settings due to
inter-agent dynamics and partial observability. To address these issues, we
propose Contextual Multi-Agent LLM-Guided Curriculum Learning with
Diversity-Based Context Blending (cMALC-D), a framework that uses Large
Language Models (LLMs) to generate semantically meaningful curricula and
provide a more robust evaluation signal. To prevent mode collapse and encourage
exploration, we introduce a novel diversity-based context blending mechanism
that creates new training scenarios by combining features from prior contexts.
Experiments in traffic signal control domains demonstrate that cMALC-D
significantly improves both generalization and sample efficiency compared to
existing curriculum learning baselines. We provide code at
https://github.com/DaRL-LibSignal/cMALC-D.

</details>


### [191] [GPT-FT: An Efficient Automated Feature Transformation Using GPT for Sequence Reconstruction and Performance Enhancement](https://arxiv.org/abs/2508.20824)
*Yang Gao,Dongjie Wang,Scott Piersall,Ye Zhang,Liqiang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer（改进GPT模型）的自动化特征转换框架，旨在通过多目标优化减少参数并提高计算效率，在性能与效率上超越现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的特征转换方法（将离散搜索转换为连续嵌入优化）常依赖于序列编码-解码结构，导致计算成本高、参数需求大，限制了其可扩展性和效率。

Method: 本文提出了一个包含四个步骤的新颖框架：转换记录收集、使用改进的GPT模型构建嵌入空间、梯度上升搜索和自回归重建。其中，改进的GPT模型承担双重任务：(a)特征转换序列重建和(b)通过构建嵌入空间来估计和提升下游任务的模型性能。这是一种多目标优化框架。

Result: 实验结果表明，该框架在基准数据集上能匹配或超越基线性能，并在计算效率方面实现显著提升。

Conclusion: 该工作突出了基于Transformer的架构在可扩展、高性能自动化特征转换方面的巨大潜力。

Abstract: Feature transformation plays a critical role in enhancing machine learning
model performance by optimizing data representations. Recent state-of-the-art
approaches address this task as a continuous embedding optimization problem,
converting discrete search into a learnable process. Although effective, these
methods often rely on sequential encoder-decoder structures that cause high
computational costs and parameter requirements, limiting scalability and
efficiency. To address these limitations, we propose a novel framework that
accomplishes automated feature transformation through four steps:
transformation records collection, embedding space construction with a revised
Generative Pre-trained Transformer (GPT) model, gradient-ascent search, and
autoregressive reconstruction. In our approach, the revised GPT model serves
two primary functions: (a) feature transformation sequence reconstruction and
(b) model performance estimation and enhancement for downstream tasks by
constructing the embedding space. Such a multi-objective optimization framework
reduces parameter size and accelerates transformation processes. Experimental
results on benchmark datasets show that the proposed framework matches or
exceeds baseline performance, with significant gains in computational
efficiency. This work highlights the potential of transformer-based
architectures for scalable, high-performance automated feature transformation.

</details>


### [192] [ATM-GAD: Adaptive Temporal Motif Graph Anomaly Detection for Financial Transaction Networks](https://arxiv.org/abs/2508.20829)
*Zeyue Zhang,Lin Song,Erkang Bao,Xiaoling Lv,Xinyue Wang*

Main category: cs.LG

TL;DR: 本文提出ATM-GAD，一种自适应图神经网络，利用时间模式和自适应时间窗口来检测金融欺诈，其性能优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 金融欺诈检测对保障资金安全至关重要，但传统机器学习模型难以应对复杂且快速变化的金融系统。现有基于图的检测器未能充分利用时间维度上的两个关键欺诈特征：可疑资金流的时间模式（temporal motifs）和账户特有的异常活动时间间隔。

Method: ATM-GAD是一种自适应图神经网络，旨在利用时间模式进行金融异常检测。它包括：1. 时间模式提取器：将每个账户的交易历史凝练成信息最丰富的模式，保留拓扑和时间特征；2. 双注意力模块（IntraA和InterA）：IntraA分析单个模式内的交互，InterA聚合跨模式的证据以揭示多步欺诈；3. 可微分的自适应时间窗口学习器：为每个节点定制观察窗口，以精确关注最关键的时间片段。

Result: 在四个真实世界数据集上的实验表明，ATM-GAD在性能上始终优于七个强大的异常检测基线模型，并能发现早期方法未能捕捉到的欺诈模式。

Conclusion: ATM-GAD通过有效利用时间模式和自适应时间窗口信号，克服了现有方法的局限性，显著提高了金融欺诈检测的准确性。

Abstract: Financial fraud detection is essential to safeguard billions of dollars, yet
the intertwined entities and fast-changing transaction behaviors in modern
financial systems routinely defeat conventional machine learning models. Recent
graph-based detectors make headway by representing transactions as networks,
but they still overlook two fraud hallmarks rooted in time: (1) temporal
motifs--recurring, telltale subgraphs that reveal suspicious money flows as
they unfold--and (2) account-specific intervals of anomalous activity, when
fraud surfaces only in short bursts unique to each entity. To exploit both
signals, we introduce ATM-GAD, an adaptive graph neural network that leverages
temporal motifs for financial anomaly detection. A Temporal Motif Extractor
condenses each account's transaction history into the most informative motifs,
preserving both topology and temporal patterns. These motifs are then analyzed
by dual-attention blocks: IntraA reasons over interactions within a single
motif, while InterA aggregates evidence across motifs to expose multi-step
fraud schemes. In parallel, a differentiable Adaptive Time-Window Learner
tailors the observation window for every node, allowing the model to focus
precisely on the most revealing time slices. Experiments on four real-world
datasets show that ATM-GAD consistently outperforms seven strong
anomaly-detection baselines, uncovering fraud patterns missed by earlier
methods.

</details>


### [193] [Practical Physical Layer Authentication for Mobile Scenarios Using a Synthetic Dataset Enhanced Deep Learning Approach](https://arxiv.org/abs/2508.20861)
*Yijia Guo,Junqing Zhang,Y. -W. Peter Hong*

Main category: cs.LG

TL;DR: 针对物联网设备认证在动态无线信道下的安全挑战，本文提出了一种基于深度学习的物理层CSI认证方案，利用Siamese CNN学习信道相关性，并通过仿真和实验证明其在移动场景下具有优秀的泛化和认证性能。


<details>
  <summary>Details</summary>
Motivation: 物联网(IoT)设备认证因无线传输的广播特性而面临巨大安全漏洞。现有物理层认证方案尚缺乏适用于动态信道变化的实用方法。

Method: 本文提出了一种基于深度学习的物理层信道状态信息(CSI)认证方案，应用于移动场景。具体方法包括：1) 基于WLAN TGn信道模型及信道自相关和距离相关性生成合成训练数据集，以减少数据收集开销。2) 采用基于卷积神经网络(CNN)的Siamese网络学习CSI对之间的时空相关性，并输出相似度分数进行认证。3) 通过仿真和基于WiFi IoT开发套件的实验平台进行综合评估。

Result: 仿真和实验评估均表明，所提出的深度学习方法具有出色的泛化性能和认证性能。与基于全连接网络(FCN)的Siamese模型相比，该方案的AUC提高了0.03；与基于相关性的基准算法相比，AUC提高了0.06。

Conclusion: 所提出的深度学习物理层CSI认证方案在移动场景下展现出优异的认证和泛化能力，有效提升了IoT设备的安全性，且性能显著优于现有基准方法。

Abstract: The Internet of Things (IoT) is ubiquitous thanks to the rapid development of
wireless technologies. However, the broadcast nature of wireless transmissions
results in great vulnerability to device authentication. Physical layer
authentication emerges as a promising approach by exploiting the unique channel
characteristics. However, a practical scheme applicable to dynamic channel
variations is still missing. In this paper, we proposed a deep learning-based
physical layer channel state information (CSI) authentication for mobile
scenarios and carried out comprehensive simulation and experimental evaluation
using IEEE 802.11n. Specifically, a synthetic training dataset was generated
based on the WLAN TGn channel model and the autocorrelation and the distance
correlation of the channel, which can significantly reduce the overhead of
manually collecting experimental datasets. A convolutional neural network
(CNN)-based Siamese network was exploited to learn the temporal and spatial
correlation between the CSI pair and output a score to measure their
similarity. We adopted a synergistic methodology involving both simulation and
experimental evaluation. The experimental testbed consisted of WiFi IoT
development kits and a few typical scenarios were specifically considered. Both
simulation and experimental evaluation demonstrated excellent generalization
performance of our proposed deep learning-based approach and excellent
authentication performance. Demonstrated by our practical measurement results,
our proposed scheme improved the area under the curve (AUC) by 0.03 compared to
the fully connected network-based (FCN-based) Siamese model and by 0.06
compared to the correlation-based benchmark algorithm.

</details>


### [194] [LeMat-Traj: A Scalable and Unified Dataset of Materials Trajectories for Atomistic Modeling](https://arxiv.org/abs/2508.20875)
*Ali Ramlaoui,Martin Siron,Inel Djafar,Joseph Musielewicz,Amandine Rossello,Victor Schmidt,Alexandre Duval*

Main category: cs.LG

TL;DR: 本文介绍了LeMat-Traj数据集和LeMaterial-Fetcher工具，旨在解决机器学习原子间势（MLIPs）训练数据分散且格式不一致的问题，通过标准化和整合大量DFT数据，显著提高了力预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确的机器学习原子间势（MLIPs）的开发受到量子力学轨迹数据集（源自DFT）可用性分散、格式不一致以及难以组合的限制。这些数据集生成成本高昂，且因格式、元数据和可访问性的差异而难以整合。

Method: 引入LeMat-Traj，一个策展数据集，包含超过1.2亿个原子构型，聚合自Materials Project、Alexandria和OQMD等大型存储库。LeMat-Traj标准化了数据表示，协调了结果并筛选了高质量构型，涵盖了PBE、PBESol、SCAN、r2SCAN等DFT泛函。同时发布了LeMaterial-Fetcher，一个模块化、可扩展的开源库，为社区提供了一个可重现的框架，以便轻松纳入新数据源并确保大规模材料数据集的持续演进。

Result: 通过使用LeMat-Traj对预训练模型进行微调，显著降低了弛豫任务中的力预测误差。

Conclusion: LeMat-Traj和LeMaterial-Fetcher显著降低了训练可迁移和准确MLIPs的障碍，并为社区提供了可重现的框架和持续演进的大规模材料数据集。

Abstract: The development of accurate machine learning interatomic potentials (MLIPs)
is limited by the fragmented availability and inconsistent formatting of
quantum mechanical trajectory datasets derived from Density Functional Theory
(DFT). These datasets are expensive to generate yet difficult to combine due to
variations in format, metadata, and accessibility. To address this, we
introduce LeMat-Traj, a curated dataset comprising over 120 million atomic
configurations aggregated from large-scale repositories, including the
Materials Project, Alexandria, and OQMD. LeMat-Traj standardizes data
representation, harmonizes results and filters for high-quality configurations
across widely used DFT functionals (PBE, PBESol, SCAN, r2SCAN). It
significantly lowers the barrier for training transferrable and accurate MLIPs.
LeMat-Traj spans both relaxed low-energy states and high-energy, high-force
structures, complementing molecular dynamics and active learning datasets. By
fine-tuning models pre-trained on high-force data with LeMat-Traj, we achieve a
significant reduction in force prediction errors on relaxation tasks. We also
present LeMaterial-Fetcher, a modular and extensible open-source library
developed for this work, designed to provide a reproducible framework for the
community to easily incorporate new data sources and ensure the continued
evolution of large-scale materials datasets. LeMat-Traj and LeMaterial-Fetcher
are publicly available at https://huggingface.co/datasets/LeMaterial/LeMat-Traj
and https://github.com/LeMaterial/lematerial-fetcher.

</details>


### [195] [Turning Tabular Foundation Models into Graph Foundation Models](https://arxiv.org/abs/2508.20906)
*Dmitry Eremeev,Gleb Bazhenov,Oleg Platonov,Artem Babenko,Liudmila Prokhorenkova*

Main category: cs.LG

TL;DR: 本文提出G2T-FM，一个利用表格基础模型TabPFNv2处理图数据多样节点特征的图基础模型。G2T-FM通过邻域特征聚合和结构嵌入增强节点表示，并在in-context和微调设置下均取得优异性能，超越现有GFM并与GNNs持平甚至超越。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在NLP和CV领域取得成功，但在图机器学习（GML）中的应用和潜力仍未充分探索。图基础模型（GFM）面临处理多样节点特征（尤其非文本类型）的挑战。受TabPFNv2等表格基础模型近期成功的启发，研究者寻求将表格领域的方法引入图领域以解决此问题。

Method: 本文提出G2T-FM模型，该模型以TabPFNv2作为骨干网络。具体而言，G2T-FM首先通过邻域特征聚合和添加结构嵌入来增强原始节点特征，然后将构建的节点表示应用于TabPFNv2。

Result: 在完全in-context模式下，G2T-FM取得了强大结果，显著优于公开的图基础模型（GFMs），并且与从头训练的精心调优的图神经网络（GNNs）性能相当。此外，经过微调后，G2T-FM超越了精心调优的GNN基线。

Conclusion: G2T-FM展示了其在图机器学习任务中的巨大潜力，并揭示了一个此前被忽视的方向，即利用表格基础模型来解决图机器学习任务。

Abstract: While foundation models have revolutionized such fields as natural language
processing and computer vision, their application and potential within graph
machine learning remain largely unexplored. One of the key challenges in
designing graph foundation models (GFMs) is handling diverse node features that
can vary across different graph datasets. Although many works on GFMs have been
focused exclusively on text-attributed graphs, the problem of handling
arbitrary features of other types in GFMs has not been fully addressed.
However, this problem is not unique to the graph domain, as it also arises in
the field of machine learning for tabular data. In this work, motivated by the
recent success of tabular foundation models like TabPFNv2, we propose G2T-FM, a
simple graph foundation model that employs TabPFNv2 as a backbone.
Specifically, G2T-FM augments the original node features with neighborhood
feature aggregation, adds structural embeddings, and then applies TabPFNv2 to
the constructed node representations. Even in a fully in-context regime, our
model achieves strong results, significantly outperforming publicly available
GFMs and performing on par with well-tuned GNNs trained from scratch. Moreover,
after finetuning, G2T-FM surpasses well-tuned GNN baselines, highlighting the
potential of the proposed approach. More broadly, our paper reveals a
previously overlooked direction of utilizing tabular foundation models for
graph machine learning tasks.

</details>


### [196] [Finite-Time Guarantees for Multi-Agent Combinatorial Bandits with Nonstationary Rewards](https://arxiv.org/abs/2508.20923)
*Katherine B. Adams,Justin J. Boutilier,Qinyang He,Yonatan Mintz*

Main category: cs.LG

TL;DR: 本文提出了首个结合非平稳奖励（如习惯化和恢复）的组合多臂老虎机框架，用于解决序贯资源分配问题，并开发了具有动态遗憾理论保证的算法，通过糖尿病干预案例展示了其在实际应用中的有效性。


<details>
  <summary>Details</summary>
Motivation: 在社区健康干预、定向数字广告和员工挽留计划等领域，干预效果会动态演变，存在个体对频繁选择的习惯化或因不频繁选择而恢复响应的情况。现有组合多臂老虎机文献缺乏处理这种非平稳奖励分布的框架，且需要在异构个体奖励和探索-利用权衡之间取得平衡，以优化未来决策和最大化即时结果。

Method: 引入了首个将非平稳奖励（包括习惯化和恢复）纳入组合多臂老虎机（Combinatorial Multi-Armed Bandit）文献的框架。基于该框架，开发了具有动态遗憾（dynamic regret）理论保证的算法。

Result: 所开发的算法在理论上提供了动态遗憾的保证。通过一个糖尿病干预的案例研究，证明了其实际有效性。个性化社区干预算法在项目注册方面比基线方法提高了三倍，验证了该框架在实际应用中的潜力。

Conclusion: 该工作将自适应学习的理论进展与人群行为改变干预中的实际挑战相结合，开发的框架及其算法在具有非平稳奖励的序贯资源分配问题中具有显著的实际应用潜力。

Abstract: We study a sequential resource allocation problem where a decision maker
selects subsets of agents at each period to maximize overall outcomes without
prior knowledge of individual-level effects. Our framework applies to settings
such as community health interventions, targeted digital advertising, and
workforce retention programs, where intervention effects evolve dynamically.
Agents may exhibit habituation (diminished response from frequent selection) or
recovery (enhanced response from infrequent selection). The technical challenge
centers on nonstationary reward distributions that lead to changing
intervention effects over time. The problem requires balancing two key
competing objectives: heterogeneous individual rewards and the
exploration-exploitation tradeoff in terms of learning for improved future
decisions as opposed to maximizing immediate outcomes. Our contribution
introduces the first framework incorporating this form of nonstationary rewards
in the combinatorial multi-armed bandit literature. We develop algorithms with
theoretical guarantees on dynamic regret and demonstrate practical efficacy
through a diabetes intervention case study. Our personalized community
intervention algorithm achieved up to three times as much improvement in
program enrollment compared to baseline approaches, validating the framework's
potential for real-world applications. This work bridges theoretical advances
in adaptive learning with practical challenges in population-level behavioral
change interventions.

</details>


### [197] [Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees](https://arxiv.org/abs/2508.21001)
*Yaniv Hassidof,Tom Jurgenson,Kiril Solovey*

Main category: cs.LG

TL;DR: DiTree是一种结合扩散策略（DPs）与采样规划器（SBPs）的运动规划框架，旨在为复杂动力系统提供快速、可泛化且可证明安全的碰撞自由轨迹。


<details>
  <summary>Details</summary>
Motivation: 动能运动规划中，采样规划器（SBPs）虽提供完备性但探索缓慢；学习型方法虽速度快但缺乏对分布外（OOD）场景的泛化能力和安全保证。急需一种能兼顾速度、泛化性和安全性的新方法。

Method: 提出了Diffusion Tree (DiTree) 框架。它利用扩散策略（DPs）作为信息采样器，引导采样规划器（SBPs）进行状态空间搜索。DiTree结合了DPs在局部观测条件下建模专家轨迹复杂分布的能力，以及SBPs的完备性，从而在几次动作传播迭代内为复杂动力系统生成可证明安全的解决方案。具体实现结合了流行的RRT规划器与在一个单一环境中训练的DP动作采样器。

Result: 在OOD场景的综合评估中，DiTree的运行时与独立DP相当（比传统SBPs快3倍），并且在平均成功率上优于DP和SBPs。DiTree比传统SBPs平均快3倍，并且在成功率上比所有其他方法高出约30%。

Conclusion: DiTree成功融合了扩散策略的智能采样优势和采样规划器的完备性，有效解决了运动规划中速度慢、泛化性差和缺乏安全保证的挑战，为复杂动力系统在OOD场景下提供了快速、安全且可泛化的解决方案。

Abstract: Kinodynamic motion planning is concerned with computing collision-free
trajectories while abiding by the robot's dynamic constraints. This critical
problem is often tackled using sampling-based planners (SBPs) that explore the
robot's high-dimensional state space by constructing a search tree via action
propagations. Although SBPs can offer global guarantees on completeness and
solution quality, their performance is often hindered by slow exploration due
to uninformed action sampling. Learning-based approaches can yield
significantly faster runtimes, yet they fail to generalize to
out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety,
thus limiting their deployment on physical robots. We present Diffusion Tree
(DiTree): a \emph{provably-generalizable} framework leveraging diffusion
policies (DPs) as informed samplers to efficiently guide state-space search
within SBPs. DiTree combines DP's ability to model complex distributions of
expert trajectories, conditioned on local observations, with the completeness
of SBPs to yield \emph{provably-safe} solutions within a few action propagation
iterations for complex dynamical systems. We demonstrate DiTree's power with an
implementation combining the popular RRT planner with a DP action sampler
trained on a \emph{single environment}. In comprehensive evaluations on OOD
scenarios, % DiTree has comparable runtimes to a standalone DP (3x faster than
classical SBPs), while improving the average success rate over DP and SBPs.
DiTree is on average 3x faster than classical SBPs, and outperforms all other
approaches by achieving roughly 30\% higher success rate. Project webpage:
https://sites.google.com/view/ditree.

</details>


### [198] [InSQuAD: In-Context Learning for Efficient Retrieval via Submodular Mutual Information to Enforce Quality and Diversity](https://arxiv.org/abs/2508.21003)
*Souradeep Nanda,Anay Majee,Rishabh Iyer*

Main category: cs.LG

TL;DR: 本文提出InSQuAD，利用子模互信息（SMI）增强上下文学习（ICL）模型性能，确保上下文示例的质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习（ICL）模型的性能有待提升；传统的检索模型在为ICL选择上下文示例时，往往只关注相关性而忽略了对ICL至关重要的多样性。

Method: 1. 将ICL任务建模为目标选择问题，并基于子模互信息（SMI）提出一种统一的选择策略，以挖掘相关且多样化的上下文示例。2. 引入一种组合训练范式，通过新颖的基于似然的损失函数学习SMI函数参数，以在检索模型中同时强制实现质量和多样性。3. 通过合成生成的释义扩充现有的多跳问答数据集，以辅助学习过程。

Result: 将采用此策略训练的检索模型与新颖的目标选择公式应用于ICL，在九个基准数据集上均显示出显著的性能提升。

Conclusion: 实验结果验证了所提出方法（InSQuAD）的有效性，成功提升了ICL模型的性能，并通过SMI确保了上下文示例的质量和多样性。

Abstract: In this paper, we introduce InSQuAD, designed to enhance the performance of
In-Context Learning (ICL) models through Submodular Mutual Information} (SMI)
enforcing Quality and Diversity among in-context exemplars. InSQuAD achieves
this through two principal strategies: First, we model the ICL task as a
targeted selection problem and introduce a unified selection strategy based on
SMIs which mines relevant yet diverse in-context examples encapsulating the
notions of quality and diversity. Secondly, we address a common pitfall in
existing retrieval models which model query relevance, often overlooking
diversity, critical for ICL. InSQuAD introduces a combinatorial training
paradigm which learns the parameters of an SMI function to enforce both quality
and diversity in the retrieval model through a novel likelihood-based loss. To
further aid the learning process we augment an existing multi-hop question
answering dataset with synthetically generated paraphrases. Adopting the
retrieval model trained using this strategy alongside the novel targeted
selection formulation for ICL on nine benchmark datasets shows significant
improvements validating the efficacy of our approach.

</details>


### [199] [Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance](https://arxiv.org/abs/2508.21016)
*Luozhijie Jin,Zijie Qiu,Jie Liu,Zijie Diao,Lifeng Qiao,Ning Ding,Alex Lamb,Xipeng Qiu*

Main category: cs.LG

TL;DR: 本文提出强化学习引导（RLG），一种推理时方法，通过几何平均结合基线和RL微调模型，动态调整扩散模型的对齐强度，从而在不额外训练的情况下优化性能和控制生成对齐。


<details>
  <summary>Details</summary>
Motivation: 去噪生成模型（如扩散模型）在实现与人类偏好、组合准确性等复杂下游目标的对齐时面临挑战。现有的强化学习（RL）微调方法对扩散模型而言次优，并且在微调后控制对齐强度方面灵活性有限。

Method: 通过随机微分方程和隐式奖励条件重新诠释了扩散模型的RL微调。引入了“强化学习引导（RLG）”，这是一种推理时方法，通过对基线模型和RL微调模型的输出进行几何平均，从而改进了无分类器引导（CFG）策略。

Result: 理论分析表明，RLG的引导尺度在数学上等同于调整标准RL目标中的KL正则化系数，从而实现了在不额外训练的情况下动态控制对齐-质量的权衡。大量实验证明，RLG在各种架构、RL算法和下游任务（包括人类偏好、组合控制、可压缩性和文本渲染）上持续改进了RL微调模型的性能。此外，RLG支持插值和外推，提供了前所未有的生成对齐控制灵活性。

Conclusion: RLG为在推理时增强和控制扩散模型的对齐提供了一个实用且理论上合理的解决方案，显著提高了模型的性能和灵活性。

Abstract: Denoising-based generative models, particularly diffusion and flow matching
algorithms, have achieved remarkable success. However, aligning their output
distributions with complex downstream objectives, such as human preferences,
compositional accuracy, or data compressibility, remains challenging. While
reinforcement learning (RL) fine-tuning methods, inspired by advances in RL
from human feedback (RLHF) for large language models, have been adapted to
these generative frameworks, current RL approaches are suboptimal for diffusion
models and offer limited flexibility in controlling alignment strength after
fine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models
through the lens of stochastic differential equations and implicit reward
conditioning. We introduce Reinforcement Learning Guidance (RLG), an
inference-time method that adapts Classifier-Free Guidance (CFG) by combining
the outputs of the base and RL fine-tuned models via a geometric average. Our
theoretical analysis shows that RLG's guidance scale is mathematically
equivalent to adjusting the KL-regularization coefficient in standard RL
objectives, enabling dynamic control over the alignment-quality trade-off
without further training. Extensive experiments demonstrate that RLG
consistently improves the performance of RL fine-tuned models across various
architectures, RL algorithms, and downstream tasks, including human
preferences, compositional control, compressibility, and text rendering.
Furthermore, RLG supports both interpolation and extrapolation, thereby
offering unprecedented flexibility in controlling generative alignment. Our
approach provides a practical and theoretically sound solution for enhancing
and controlling diffusion model alignment at inference. The source code for RLG
is publicly available at the Github:
https://github.com/jinluo12345/Reinforcement-learning-guidance.

</details>


### [200] [Fast Convergence Rates for Subsampled Natural Gradient Algorithms on Quadratic Model Problems](https://arxiv.org/abs/2508.21022)
*Gil Goldshlager,Jiang Hu,Lin Lin*

Main category: cs.LG

TL;DR: 本文为子采样自然梯度下降（SNGD）及其加速变体SPRING提供了首次理论收敛性分析，证明了它们与正则化Kaczmarz方法的等价性，并给出了SNGD的快速收敛率和SPRING的收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 子采样自然梯度下降（SNGD）在科学机器学习的参数优化任务中表现出色，但一直缺乏理论解释。

Method: 通过分析SNGD及其加速变体SPRING在理想化参数优化问题（线性模型、强凸二次损失函数）中的收敛性。在最小二乘损失的特殊情况下，证明SNGD等价于正则化Kaczmarz方法，SPRING等价于加速正则化Kaczmarz方法。对于一般的强凸二次损失，则扩展了正则化Kaczmarz方法的分析。

Result: 在温和条件下，获得了SNGD的第一个快速收敛率；首次在任何设置下证明了SPRING的收敛性；首次证明了SPRING可以加速SNGD。在一般强凸二次损失情况下，在更强的条件下为SNGD提供了快速收敛率，首次解释了SNGD在非最小二乘场景下的有效性。

Conclusion: 研究结果表明，随机线性代数工具可以为子采样和曲率感知优化策略之间的相互作用提供新的见解。

Abstract: Subsampled natural gradient descent (SNGD) has shown impressive results for
parametric optimization tasks in scientific machine learning, such as neural
network wavefunctions and physics-informed neural networks, but it has lacked a
theoretical explanation. We address this gap by analyzing the convergence of
SNGD and its accelerated variant, SPRING, for idealized parametric optimization
problems where the model is linear and the loss function is strongly convex and
quadratic. In the special case of a least-squares loss, namely the standard
linear least-squares problem, we prove that SNGD is equivalent to a regularized
Kaczmarz method while SPRING is equivalent to an accelerated regularized
Kaczmarz method. As a result, by leveraging existing analyses we obtain under
mild conditions (i) the first fast convergence rate for SNGD, (ii) the first
convergence guarantee for SPRING in any setting, and (iii) the first proof that
SPRING can accelerate SNGD. In the case of a general strongly convex quadratic
loss, we extend the analysis of the regularized Kaczmarz method to obtain a
fast convergence rate for SNGD under stronger conditions, providing the first
explanation for the effectiveness of SNGD outside of the least-squares setting.
Overall, our results illustrate how tools from randomized linear algebra can
shed new light on the interplay between subsampling and curvature-aware
optimization strategies.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [201] [A Comprehensive Survey of 5G URLLC and Challenges in the 6G Era](https://arxiv.org/abs/2508.20205)
*Md. Emadul Haque,Faisal Tariq,Muhammad R A Khandaker,Md. Sakir Hossain,Muhammad Ali Imran,Kai-Kit Wong*

Main category: cs.NI

TL;DR: 本文对5G系统中超可靠低延迟通信（URLLC）的方法进行了全面综述，探讨了其面临的矛盾挑战、分层技术及未来的6G发展。


<details>
  <summary>Details</summary>
Motivation: 无线通信范式正从以人为中心转向以机器为中心，导致速率、延迟和可靠性要求发生剧烈变化。URLLC应运而生，旨在实现99.999%的可靠性和1ms的延迟，但这两个目标本身相互冲突，需要对现有方法进行系统性分析。

Method: 本文采用综合调查和分层分析的方法，详细讨论了物理层、MAC层以及跨层技术在URLLC中的应用。同时，也涵盖了各种5G及未来垂直行业的设计考量，并追溯了无线通信中延迟和可靠性问题的历史演变。

Result: 文章详细分析了5G系统中的URLLC方法，追溯了延迟和可靠性问题的历史与演变，并深入探讨了物理层、MAC层及跨层技术，同时考虑了不同垂直行业的设计要素。

Conclusion: 文章最后详细讨论了URLLC面临的挑战和未来的发展前景，特别关注了新兴的6G范式。

Abstract: As the wireless communication paradigm is being transformed from human
centered communication services towards machine centered communication
services, the requirements of rate, latency and reliability for these services
have also been transformed drastically. Thus the concept of Ultra Reliable and
Low Latency Communication (URLLC) has emerged as a dominant theme for 5G and 6G
systems. Though the latency and reliability requirement varies from one use
case to another, URLLC services generally aim to achieve very high reliability
in the range of 99.999\% while ensuring the latency of up to 1 ms. These two
targets are however inherently opposed to one another. Significant amounts of
work have been carried out to meet these ambitious but conflicting targets. In
this article a comprehensive survey of the URLLC approaches in 5G systems are
analysed in detail. Effort has been made to trace the history and evolution of
latency and reliability issues in wireless communication. A layered approach is
taken where physical layer, Medium Access Control (MAC) layer as well as cross
layer techniques are discussed in detail. It also covers the design
consideration for various 5G and beyond verticals. Finally the article
concludes by providing a detailed discussion on challenges and future outlook
with particular focus on the emerging 6G paradigm.

</details>


### [202] [DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource Allocation and Markov Decision Process in Named Data Networking (NDN)](https://arxiv.org/abs/2508.20272)
*Fatemeh Roshanzadeh,Hamid Barati,Ali Barati*

Main category: cs.NI

TL;DR: 本文提出DRR-MDPF，一种结合马尔可夫决策过程转发（MDPF）和亏损轮询（DRR）的NDN混合队列及资源管理策略，旨在提升动态高流量环境下的网络性能和资源公平分配。


<details>
  <summary>Details</summary>
Motivation: 在命名数据网络（NDN）中，特别是在动态和高流量条件下，高效的队列和资源管理对于提升网络性能至关重要，以应对资源分配、拥塞控制和路由优化等核心挑战。

Method: DRR-MDPF是一种混合策略，将MDPF模型与DRR算法相结合。MDPF使路由器基于带宽、延迟和未满足兴趣数等关键指标预测最佳转发决策；DRR则确保公平自适应的带宽分配。路由器被建模为学习代理，通过连续反馈和概率更新调整策略。该方法采用单路径路由设计并支持多指标决策，通过ndnSIM进行模拟验证。

Result: DRR-MDPF在吞吐量、兴趣满足率（ISR）、丢包率、内容检索时间和负载均衡等多项指标上显著优于SAF、RFA、SMDPF和LA-MDPF等现有先进策略。它在有限缓存和高流量下仍保持鲁棒性，具有更强的适应性和更低的计算复杂度，并通过更准确的接口选择优化了网络性能。

Conclusion: DRR-MDPF为NDN提供了一个智能、自适应且可扩展的队列管理解决方案，有效解决了动态网络环境中的资源分配、拥塞控制和路由优化等核心问题。

Abstract: Named Data Networking (NDN) represents a transformative shift in network
architecture, prioritizing content names over host addresses to enhance data
dissemination. Efficient queue and resource management are critical to NDN
performance, especially under dynamic and high-traffic conditions. This paper
introduces DRR-MDPF, a novel hybrid strategy that integrates the Markov
Decision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)
algorithm. MDPF enables routers to intelligently predict optimal forwarding
decisions based on key metrics such as bandwidth, delay, and the number of
unsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation
among competing data flows. The proposed method models each router as a
learning agent capable of adjusting its strategies through continuous feedback
and probabilistic updates. Simulation results using ndnSIM demonstrate that
DRR-MDPF significantly outperforms state-of-the-art strategies including SAF,
RFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest
Satisfaction Rate (ISR), packet drop rate, content retrieval time, and load
balancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and
heavy traffic, offering enhanced adaptability and lower computational
complexity due to its single-path routing design. Furthermore, its multi-metric
decision-making capability enables more accurate interface selection, leading
to optimized network performance. Overall, DRR-MDPF serves as an intelligent,
adaptive, and scalable queue management solution for NDN, effectively
addressing core challenges such as resource allocation, congestion control, and
route optimization in dynamic networking environments.

</details>


### [203] [Relay Selection in Wireless Networks as Restless Bandits](https://arxiv.org/abs/2508.20625)
*Mandar R. Nalavade,Ravindra S. Tomar,Gaurav S. Kasbekar*

Main category: cs.NI

TL;DR: 在直连链路阻塞的无线网络中，通过多中继传输来最小化数据包持有成本。该研究将问题建模为无静止多臂赌博机（RMAB），证明其Whittle可索引性，并提出一种基于Whittle指数的中继选择策略，该策略在模拟中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在源节点到目的节点的直连无线链路被阻塞的场景下，需要通过多个备选中继转发数据包。数据包在中继存储会产生持有成本。研究动机是设计一种中继选择策略，以最小化长期运行中继处数据包的预期总持有成本。

Method: 将中继选择问题建模为一个难以解决的无静止多臂赌博机（RMAB）问题。研究证明该中继选择问题是Whittle可索引的，并提出了一种计算每个中继在每个时隙的Whittle指数的方法。所提出的中继选择策略是：在每个时隙选择Whittle指数最小的中继来传输数据包。

Result: 通过仿真结果表明，所提出的基于Whittle指数的中继选择策略在平均成本、延迟以及吞吐量方面，均优于现有工作中提出的中继选择策略。

Conclusion: 该研究成功地将复杂的中继选择问题转化为Whittle可索引的RMAB问题并提供了有效的解决方案。所提出的基于Whittle指数的策略能有效降低数据包持有成本，同时提高网络性能（包括延迟和吞吐量）。

Abstract: We consider a wireless network in which a source node needs to transmit a
large file to a destination node. The direct wireless link between the source
and the destination is assumed to be blocked. Multiple candidate relays are
available to forward packets from the source to the destination. A holding cost
is incurred for each packet stored at every relay in each time slot. The
objective is to design a policy for selecting a relay in each time slot to
which the source attempts to send a packet, so as to minimize the expected
long-run time-averaged total packet holding cost at the relays. This problem is
an instance of the restless multi-armed bandit (RMAB) problem, which is
provably hard to solve. We prove that this relay selection problem is
Whittle-indexable, and propose a method to compute the Whittle index of each
relay in every time slot. In each time slot, our relay selection policy
transmits a packet to the relay with the smallest Whittle index. Using
simulations, we show that the proposed policy outperforms the relay selection
policies proposed in prior work in terms of average cost, delay, as well as
throughput.

</details>


### [204] [Digital Twin-Empowered Deep Reinforcement Learning for Intelligent VNF Migration in Edge-Core Networks](https://arxiv.org/abs/2508.20957)
*Faisal Ahmed,Suresh Subramaniam,Motoharu Matsuura,Hiroshi Hasegawa,Shih-Chun Lin*

Main category: cs.NI

TL;DR: 本文提出了一种结合数字孪生（DT）和深度强化学习（DRL）的框架，用于边缘-核心网络中VNF的智能迁移，旨在共同优化端到端延迟和能耗，并通过仿真验证了其显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 日益增长的服务需求和虚拟化网络功能（VNFs）的快速部署，给现代边缘-核心网络基础设施中实现低延迟和高能效的编排带来了巨大挑战。

Method: 本研究提出了一个由数字孪生（DT）赋能的深度强化学习（DRL）框架，用于智能VNF迁移。该框架将VNF迁移问题表述为马尔可夫决策过程，并利用优势Actor-Critic模型做出适应性实时迁移决策。其关键创新是整合了一个DT模块，该模块由多任务变分自编码器和多任务长短期记忆网络组成，共同模拟环境动态并生成高质量的合成经验，以显著提高训练效率和加速策略收敛。

Result: 仿真结果表明，该框架在性能上取得了显著提升，例如平均端到端（E2E）延迟和能耗均大幅降低。

Conclusion: 该研究为边缘-核心网络中的智能VNF迁移建立了新的基准。

Abstract: The growing demand for services and the rapid deployment of virtualized
network functions (VNFs) pose significant challenges for achieving low-latency
and energy-efficient orchestration in modern edge-core network infrastructures.
To address these challenges, this study proposes a Digital Twin (DT)-empowered
Deep Reinforcement Learning framework for intelligent VNF migration that
jointly minimizes average end-to-end (E2E) delay and energy consumption. By
formulating the VNF migration problem as a Markov Decision Process and
utilizing the Advantage Actor-Critic model, the proposed framework enables
adaptive and real-time migration decisions. A key innovation of the proposed
framework is the integration of a DT module composed of a multi-task
Variational Autoencoder and a multi-task Long Short-Term Memory network. This
combination collectively simulates environment dynamics and generates
high-quality synthetic experiences, significantly enhancing training efficiency
and accelerating policy convergence. Simulation results demonstrate substantial
performance gains, such as significant reductions in both average E2E delay and
energy consumption, thereby establishing new benchmarks for intelligent VNF
migration in edge-core networks.

</details>


### [205] [RANGAN: GAN-empowered Anomaly Detection in 5G Cloud RAN](https://arxiv.org/abs/2508.20985)
*Douglas Liao,Jiping Luo,Jens Vevstad,Nikolaos Pappas*

Main category: cs.NI

TL;DR: 本文提出RANGAN框架，结合GAN和Transformer，利用滑动窗口处理RAN性能数据，有效检测网络异常，尤其在识别网络争用问题上表现出色，F1-score高达83%。


<details>
  <summary>Details</summary>
Motivation: 无线接入网络（RAN）系统复杂，KPI数据量巨大，导致故障排查和异常诊断困难。同时，RAN性能的动态性要求能够捕捉时间依赖的自适应异常检测方法。

Method: 引入RANGAN异常检测框架，该框架整合了生成对抗网络（GAN）和Transformer架构。为捕捉数据中的时间依赖性，RANGAN在数据预处理阶段采用滑动窗口方法。

Result: RANGAN在公开的RAN性能数据集（Spotlight项目）上进行了严格评估，实验结果表明其具有良好的检测准确性，特别是在识别网络争用问题时，F1-score高达83%。

Conclusion: RANGAN是一个有效的RAN异常检测框架，通过结合GAN、Transformer和滑动窗口处理方法，能够可靠地检测性能异常，尤其擅长识别网络争用问题。

Abstract: Radio Access Network (RAN) systems are inherently complex, requiring
continuous monitoring to prevent performance degradation and ensure optimal
user experience. The RAN leverages numerous key performance indicators (KPIs)
to evaluate system performance, generating vast amounts of data each second.
This immense data volume can make troubleshooting and accurate diagnosis of
performance anomalies more difficult. Furthermore, the highly dynamic nature of
RAN performance demands adaptive methodologies capable of capturing temporal
dependencies to detect anomalies reliably. In response to these challenges, we
introduce \textbf{RANGAN}, an anomaly detection framework that integrates a
Generative Adversarial Network (GAN) with a transformer architecture. To
enhance the capability of capturing temporal dependencies within the data,
RANGAN employs a sliding window approach during data preprocessing. We
rigorously evaluated RANGAN using the publicly available RAN performance
dataset from the Spotlight project \cite{sun-2024}. Experimental results
demonstrate that RANGAN achieves promising detection accuracy, notably
attaining an F1-score of up to $83\%$ in identifying network contention issues.

</details>


### [206] [DSROQ: Dynamic Scheduling and Routing for QoE Management in LEO Satellite Networks](https://arxiv.org/abs/2508.21047)
*Dhiraj Bhattacharjee,Pablo G. Madoery,Abhishek Naik,Halim Yanikomeroglu,Gunes Karabulut Kurt,Stephane Martel,Khaled Ahmed*

Main category: cs.NI

TL;DR: 该研究提出了一种名为DSROQ的MCTS-启发式算法，通过联合路由、带宽分配和Lyapunov优化调度，显著提升了LEO卫星网络中的用户体验和公平性。


<details>
  <summary>Details</summary>
Motivation: 现代互联网应用对QoS需求多样，LEO卫星网络作为增强覆盖和补充地面网络的解决方案，需要有效处理流量以确保QoS。现有的挑战在于如何联合优化路由、带宽分配和动态队列调度，以在QoS为软约束的情况下最大化用户体验。

Method: 将QoS需求视为软约束，构建了一个旨在最大化用户体验的联合路由和带宽分配问题。引入了一种自适应调度方法来优先处理特定流的QoS需求。提出了一种受蒙特卡洛树搜索（MCTS）启发的算法来解决NP难度的路由和带宽分配问题，并在MCTS的奖励评估阶段应用了基于Lyapunov优化的调度。使用Starlink Phase 1 Version 2星座进行评估。

Result: 所提出的DSROQ算法与基准方案相比，显著提高了终端用户体验和公平性。研究结果展示了联合路由和带宽决策的优势。此外，观察到当流量敏感性从延迟驱动变为带宽驱动时，主导性能因素从调度转向了路由和带宽分配。

Conclusion: 通过DSROQ算法联合优化路由、带宽分配和动态调度，可以有效提升LEO卫星网络的QoS和用户体验。联合路由和带宽决策是性能提升的关键，且主要性能影响因素会根据流量特性而动态变化。

Abstract: The modern Internet supports diverse applications with heterogeneous quality
of service (QoS) requirements. Low Earth orbit (LEO) satellite constellations
offer a promising solution to meet these needs, enhancing coverage in rural
areas and complementing terrestrial networks in urban regions. Ensuring QoS in
such networks requires joint optimization of routing, bandwidth allocation, and
dynamic queue scheduling, as traffic handling is critical for maintaining
service performance. This paper formulates a joint routing and bandwidth
allocation problem where QoS requirements are treated as soft constraints,
aiming to maximize user experience. An adaptive scheduling approach is
introduced to prioritize flow-specific QoS needs. We propose a Monte Carlo tree
search (MCTS)-inspired method to solve the NP-hard route and bandwidth
allocation problem, with Lyapunov optimization-based scheduling applied during
reward evaluation. Using the Starlink Phase 1 Version 2 constellation, we
compare end-user experience and fairness between our proposed DSROQ algorithm
and a benchmark scheme. Results show that DSROQ improves both performance
metrics and demonstrates the advantage of joint routing and bandwidth
decisions. Furthermore, we observe that the dominant performance factor shifts
from scheduling to routing and bandwidth allocation as traffic sensitivity
changes from latency-driven to bandwidth-driven.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [207] [The Mathematician's Assistant: Integrating AI into Research Practice](https://arxiv.org/abs/2508.20236)
*Jonas Henkel*

Main category: math.HO

TL;DR: 本文探讨了人工智能（AI）在数学研究中的应用现状，指出当前LLMs在解决问题和评估证明方面表现出强大能力，但也存在系统性缺陷。作者提出了一种以“增强型数学家”为核心的AI整合框架，强调AI的辅助而非自动化作用。


<details>
  <summary>Details</summary>
Motivation: 随着AI（如'AlphaEvolve'和'Gemini Deep Think'）的快速发展，其为数学研究实践带来了变革潜力。本研究旨在探索当前 publicly accessible LLMs 在数学研究背景下的应用前景和挑战。

Method: 基于2025年8月2日之前的发展，作者分析了近期基准测试（如MathArena和Open Proof Corpus）的结果，评估了LLMs在数学研究中的表现。在此基础上，提出了一个以“增强型数学家”为原则的AI整合框架，并详细阐述了七种AI在研究生命周期中的应用方式。

Result: 研究发现，最先进的LLMs在解决问题和评估证明方面表现出色，但也存在系统性缺陷，如缺乏自我批判能力，以及最终答案准确性与完整证明有效性之间存在差异。基于此，提出了一个强调AI作为人类研究者副驾驶的“增强型数学家”框架和五个有效使用原则。

Conclusion: AI在数学研究中的主要作用是增强而非自动化。有效利用这些强大工具需要研究人员掌握新的技能，包括战略性提示、批判性验证和严谨的方法论。

Abstract: The rapid development of artificial intelligence (AI), marked by
breakthroughs like 'AlphaEvolve' and 'Gemini Deep Think', is beginning to offer
powerful new tools that have the potential to significantly alter the research
practice in many areas of mathematics. This paper explores the current
landscape of publicly accessible large language models (LLMs) in a mathematical
research context, based on developments up to August 2, 2025. Our analysis of
recent benchmarks, such as MathArena and the Open Proof Corpus (Balunovi\'c et
al., 2025; Dekoninck et al., 2025), reveals a complex duality: while
state-of-the-art models demonstrate strong abilities in solving problems and
evaluating proofs, they also exhibit systematic flaws, including a lack of
self-critique and a model depending discrepancy between final-answer accuracy
and full-proof validity.
  Based on these findings, we propose a durable framework for integrating AI
into the research workflow, centered on the principle of the augmented
mathematician. In this model, the AI functions as a copilot under the critical
guidance of the human researcher, an approach distilled into five guiding
principles for effective and responsible use. We then systematically explore
seven fundamental ways AI can be applied across the research lifecycle, from
creativity and ideation to the final writing process, demonstrating how these
principles translate into concrete practice.
  We conclude that the primary role of AI is currently augmentation rather than
automation. This requires a new skill set focused on strategic prompting,
critical verification, and methodological rigor in order to effectively use
these powerful tools.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [208] [Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard](https://arxiv.org/abs/2508.20504)
*Guan-Yan Yang,Jui-Ning Chen,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 本文提出一种基于图结构学习（GSL）的防护框架，旨在通过优化图拓扑和节点表示，增强物联网能源（IoE）网络抵抗对抗性攻击的鲁棒性，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 物联网能源（IoE）的互联性使其关键基础设施面临复杂的网络威胁，特别是能绕过传统防护的对抗性攻击，且对公共安全有严重影响。因此，需要开发更具鲁棒性的解决方案来保护IoE网络。

Method: 提出一个基于图结构学习（GSL）的防护框架。该框架从网络层面出发，通过联合优化图拓扑和节点表示，以固有地抵抗对抗性网络模型操纵。

Result: 通过概念概述、架构讨论以及在安全数据集上的案例研究，证明了所提出的GSL方法比现有代表性方法具有更优越的鲁棒性。

Conclusion: GSL为保护IoE网络对抗不断演变的攻击提供了一条可行途径，能够增强未来IoE网络的韧性和可靠性。本文还指出了该领域面临的关键挑战和未来研究方向。

Abstract: The Internet of Energy (IoE) integrates IoT-driven digital communication with
power grids to enable efficient and sustainable energy systems. Still, its
interconnectivity exposes critical infrastructure to sophisticated cyber
threats, including adversarial attacks designed to bypass traditional
safeguards. Unlike general IoT risks, IoE threats have heightened public safety
consequences, demanding resilient solutions. From the networking-level
safeguard perspective, we propose a Graph Structure Learning (GSL)-based
safeguards framework that jointly optimizes graph topology and node
representations to resist adversarial network model manipulation inherently.
Through a conceptual overview, architectural discussion, and case study on a
security dataset, we demonstrate GSL's superior robustness over representative
methods, offering practitioners a viable path to secure IoE networks against
evolving attacks. This work highlights the potential of GSL to enhance the
resilience and reliability of future IoE networks for practitioners managing
critical infrastructure. Lastly, we identify key open challenges and propose
future research directions in this novel research area.

</details>


### [209] [AI Propaganda factories with language models](https://arxiv.org/abs/2508.20186)
*Lukasz Olejnik*

Main category: cs.CR

TL;DR: 研究表明，小型语言模型能在商品硬件上执行端到端的AI影响力操作。个性设计比模型本身更重要，且在反驳时意识形态更极端。防御策略应转向以对话为中心的检测和破坏，AI内容的一致性可作为检测特征。


<details>
  <summary>Details</summary>
Motivation: AI驱动的影响力操作已能在普通硬件上端到端执行，且小型语言模型即可生成连贯的、角色驱动的政治信息。研究旨在揭示其能力、行为模式以及应对策略。

Method: 使用小型语言模型生成连贯的、角色驱动的政治信息。采用自动化方法而非人工评估消息质量。研究了人格设计和回复互动（如反驳）对模型行为和内容生成的影响。

Result: 1. 小型语言模型能生成连贯、角色驱动的政治信息，且可自动评估。2. 行为发现一：人格设计对模型行为的影响远大于模型本身。3. 行为发现二：当模型需要反驳观点时，其意识形态依从性增强，极端内容占比增加。4. 完全自动化的影响力内容生产已触手可及，大大小小的参与者均能实现。

Conclusion: AI驱动的影响力操作已非常普及，防御策略应从限制模型访问转向在对话中检测和破坏相关活动及协调基础设施。讽刺的是，AI内容本身的一致性特征反而能作为其检测信号。

Abstract: AI-powered influence operations can now be executed end-to-end on commodity
hardware. We show that small language models produce coherent, persona-driven
political messaging and can be evaluated automatically without human raters.
Two behavioural findings emerge. First, persona-over-model: persona design
explains behaviour more than model identity. Second, engagement as a stressor:
when replies must counter-arguments, ideological adherence strengthens and the
prevalence of extreme content increases. We demonstrate that fully automated
influence-content production is within reach of both large and small actors.
Consequently, defence should shift from restricting model access towards
conversation-centric detection and disruption of campaigns and coordination
infrastructure. Paradoxically, the very consistency that enables these
operations also provides a detection signature.

</details>


### [210] [Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID](https://arxiv.org/abs/2508.20228)
*Xia Han,Qi Li,Jianbing Ni,Mohammad Zulkernine*

Main category: cs.CR

TL;DR: 现有LLM水印方法（如SynthID-Text）易受意义保留攻击，本文提出SynGuard框架，结合语义信息检索与概率水印，在词汇和语义层面嵌入水印，显著提高水印的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLM水印方法（如SynthID-Text）在追溯AI生成文本来源方面有前景，但研究发现它们容易受到如意译、复制粘贴修改和回译等意义保留攻击，导致水印可检测性显著下降。

Method: 本文提出SynGuard，一个混合框架，结合了语义信息检索（SIR）的语义对齐能力和SynthID-Text的概率水印机制。该方法在词汇和语义层面联合嵌入水印，以实现鲁棒的来源追踪并保留原始含义。

Result: 在多种攻击场景下的实验结果表明，SynGuard相比SynthID-Text，F1分数平均提高了11.1%的水印恢复率。所有代码、数据集和评估脚本均已公开。

Conclusion: 这些发现证明了语义感知水印在抵抗现实世界篡改方面的有效性。

Abstract: Recent advances in LLM watermarking methods such as SynthID-Text by Google
DeepMind offer promising solutions for tracing the provenance of AI-generated
text. However, our robustness assessment reveals that SynthID-Text is
vulnerable to meaning-preserving attacks, such as paraphrasing, copy-paste
modifications, and back-translation, which can significantly degrade watermark
detectability. To address these limitations, we propose SynGuard, a hybrid
framework that combines the semantic alignment strength of Semantic Information
Retrieval (SIR) with the probabilistic watermarking mechanism of SynthID-Text.
Our approach jointly embeds watermarks at both lexical and semantic levels,
enabling robust provenance tracking while preserving the original meaning.
Experimental results across multiple attack scenarios show that SynGuard
improves watermark recovery by an average of 11.1\% in F1 score compared to
SynthID-Text. These findings demonstrate the effectiveness of semantic-aware
watermarking in resisting real-world tampering. All code, datasets, and
evaluation scripts are publicly available at:
https://github.com/githshine/SynGuard.

</details>


### [211] [Network-Level Prompt and Trait Leakage in Local Research Agents](https://arxiv.org/abs/2508.20282)
*Hyejun Jeong,Mohammadreze Teymoorianfard,Abhinav Kumar,Amir Houmansadr,Eugene Badasarian*

Main category: cs.CR

TL;DR: 研究表明，基于语言模型的网络与研究代理 (WRAs) 易受被动网络对手（如ISP）的推理攻击，通过网络元数据泄露用户提示和特征，但可以通过限制域名多样性或混淆追踪来有效缓解。


<details>
  <summary>Details</summary>
Motivation: Web和研究代理(WRAs)在本地部署用于隐私、法律或财务目的。然而，它们不同于人类浏览，会访问大量具有可区分时序关联的域名，这使得它们容易受到被动网络对手（如ISP）的指纹攻击和推理攻击，从而泄露敏感的用户提示和特征，构成了潜在的隐私风险。

Method: 1. 提出一种针对WRA的提示和用户特征泄露攻击，仅利用网络层元数据（即访问的IP地址及其时序）。2. 构建一个新的WRA追踪数据集，基于用户搜索查询和合成角色的查询。3. 定义行为度量OBELS来评估原始与推断提示之间的相似性。4. 将攻击扩展到多会话设置。5. 在部分可观察和噪声条件下测试攻击的有效性。6. 讨论并评估了限制域名多样性或混淆追踪的缓解策略。

Result: 1. 攻击能恢复超过73%的用户提示功能和领域知识。2. 在多会话设置中，能够高精度恢复32个潜在特征中的多达19个。3. 攻击在部分可观察和噪声条件下依然有效。4. 缓解策略对实用性影响可忽略不计，同时将攻击有效性平均降低了29%。

Conclusion: Web和研究代理（WRAs）容易受到基于网络元数据的推理攻击，从而泄露用户提示和潜在特征。尽管这些攻击有效，但通过实施限制域名多样性或混淆追踪的缓解策略，可以在对实用性影响甚微的情况下，显著降低攻击的有效性，强调了部署WRAs时需要考虑隐私保护措施。

Abstract: We show that Web and Research Agents (WRAs) -- language model-based systems
that investigate complex topics on the Internet -- are vulnerable to inference
attacks by passive network adversaries such as ISPs. These agents could be
deployed \emph{locally} by organizations and individuals for privacy, legal, or
financial purposes. Unlike sporadic web browsing by humans, WRAs visit
$70{-}140$ domains with distinguishable timing correlations, enabling unique
fingerprinting attacks.
  Specifically, we demonstrate a novel prompt and user trait leakage attack
against WRAs that only leverages their network-level metadata (i.e., visited IP
addresses and their timings). We start by building a new dataset of WRA traces
based on user search queries and queries generated by synthetic personas. We
define a behavioral metric (called OBELS) to comprehensively assess similarity
between original and inferred prompts, showing that our attack recovers over
73\% of the functional and domain knowledge of user prompts. Extending to a
multi-session setting, we recover up to 19 of 32 latent traits with high
accuracy. Our attack remains effective under partial observability and noisy
conditions. Finally, we discuss mitigation strategies that constrain domain
diversity or obfuscate traces, showing negligible utility impact while reducing
attack effectiveness by an average of 29\%.

</details>


### [212] [Surveying the Operational Cybersecurity and Supply Chain Threat Landscape when Developing and Deploying AI Systems](https://arxiv.org/abs/2508.20307)
*Michael R Smith,Joe Ingram*

Main category: cs.CR

TL;DR: AI的兴起带来了传统安全评估忽视的新型网络威胁和攻击目标。本文旨在提高对AI系统中新型网络威胁的认识，探讨AI生命周期中的网络安全和供应链风险，并强调定制安全框架的必要性。


<details>
  <summary>Details</summary>
Motivation: AI的广泛应用引入了独特的攻击面和攻击目标，而传统网络安全评估未能充分覆盖。网络攻击者的目标正转向通过操纵AI输出实现系统效果（如性能下降、误报泛滥或模型精度降低），因此迫切需要提高对AI系统中新型网络威胁的认识。

Method: 本文通过探索AI生命周期中的操作网络安全和供应链风险来分析问题，并强调需要定制安全框架以应对不断演变的威胁。同时，文中还列举了以往的攻击事件并提供了该领域的工作见解。

Result: 论文分析了将AI整合到软件系统中所引入的新型网络威胁，揭示了AI生命周期中操作网络安全和供应链的风险，并强调了为应对AI驱动环境中不断演变的威胁而量身定制安全框架的必要性。

Conclusion: 通过理解这些新型网络安全风险，组织可以更好地保护AI系统，从而确保其可靠性和弹性。

Abstract: The rise of AI has transformed the software and hardware landscape, enabling
powerful capabilities through specialized infrastructures, large-scale data
storage, and advanced hardware. However, these innovations introduce unique
attack surfaces and objectives which traditional cybersecurity assessments
often overlook. Cyber attackers are shifting their objectives from conventional
goals like privilege escalation and network pivoting to manipulating AI outputs
to achieve desired system effects, such as slowing system performance, flooding
outputs with false positives, or degrading model accuracy. This paper serves to
raise awareness of the novel cyber threats that are introduced when
incorporating AI into a software system. We explore the operational
cybersecurity and supply chain risks across the AI lifecycle, emphasizing the
need for tailored security frameworks to address evolving threats in the
AI-driven landscape. We highlight previous exploitations and provide insights
from working in this area. By understanding these risks, organizations can
better protect AI systems and ensure their reliability and resilience.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [213] [Deep Reinforcement Learning for Optimal Asset Allocation Using DDPG with TiDE](https://arxiv.org/abs/2508.20103)
*Rongwei Liu,Jin Zheng,John Cartlidge*

Main category: q-fin.PM

TL;DR: 本研究将最优资产配置问题建模为马尔可夫决策过程，并提出一种结合时间序列稠密编码器（TiDE）和深度确定性策略梯度（DDPG）的强化学习（RL）框架（DDPG-TiDE），以克服传统方法的局限性并实现动态决策。实验结果表明，DDPG-TiDE优于Q-learning，并能产生比买入并持有策略更高的风险调整收益。


<details>
  <summary>Details</summary>
Motivation: 由于金融市场的固有波动性，风险资产和无风险资产之间的最优资产配置是一个持续的挑战。传统方法依赖于严格的分布假设或非加性回报比率，这限制了其鲁棒性和对投资目标的适用性。

Method: 本研究将最优两资产配置问题建模为马尔可夫决策过程（MDP）中的序贯决策任务。该框架允许应用强化学习（RL）机制来开发基于模拟金融场景的动态策略。研究中使用了凯利准则来平衡即时奖励信号和长期投资目标，并创新性地将时间序列稠密编码器（TiDE）整合到深度确定性策略梯度（DDPG）RL框架中进行连续决策。最终，将DDPG-TiDE与简单的离散动作Q-learning RL框架以及被动买入并持有投资策略进行了比较。

Result: 实证结果表明，DDPG-TiDE优于Q-learning，并且比买入并持有策略产生了更高的风险调整收益。

Conclusion: 研究结果表明，通过在DDPG强化学习框架中整合TiDE来解决最优资产配置问题是一个有前景的研究方向。

Abstract: The optimal asset allocation between risky and risk-free assets is a
persistent challenge due to the inherent volatility in financial markets.
Conventional methods rely on strict distributional assumptions or non-additive
reward ratios, which limit their robustness and applicability to investment
goals. To overcome these constraints, this study formulates the optimal
two-asset allocation problem as a sequential decision-making task within a
Markov Decision Process (MDP). This framework enables the application of
reinforcement learning (RL) mechanisms to develop dynamic policies based on
simulated financial scenarios, regardless of prerequisites. We use the Kelly
criterion to balance immediate reward signals against long-term investment
objectives, and we take the novel step of integrating the Time-series Dense
Encoder (TiDE) into the Deep Deterministic Policy Gradient (DDPG) RL framework
for continuous decision-making. We compare DDPG-TiDE with a simple
discrete-action Q-learning RL framework and a passive buy-and-hold investment
strategy. Empirical results show that DDPG-TiDE outperforms Q-learning and
generates higher risk adjusted returns than buy-and-hold. These findings
suggest that tackling the optimal asset allocation problem by integrating TiDE
within a DDPG reinforcement learning framework is a fruitful avenue for further
exploration.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [214] [OLMoASR: Open Models and Data for Training Robust Speech Recognition Models](https://arxiv.org/abs/2508.20869)
*Huong Ngo,Matt Deitke,Martijn Bartelds,Sarah Pratt,Josh Gardner,Matt Jordan,Ludwig Schmidt*

Main category: cs.SD

TL;DR: 本文通过大规模数据（OLMoASR-Pool）的高质量筛选，构建了1M小时的OLMoASR-Mix数据集，并在此基础上训练了OLMoASR语音识别模型，其在零样本语音识别任务上表现与OpenAI Whisper相当，并将公开相关资源以促进研究。


<details>
  <summary>Details</summary>
Motivation: 尽管训练数据的规模和质量对模型进步至关重要，但其在语音识别领域的影响尚未得到充分探索。研究旨在开发和研究鲁棒的零样本语音识别模型。

Method: 研究首先从包含3M小时英语音频和17M转录文本的OLMoASR-Pool数据集开始，接着设计并应用文本启发式过滤器以移除低质量或错误转录的数据，从而策展出1M小时高质量音频-转录对的新数据集OLMoASR-Mix。最后，使用OLMoASR-Mix训练了一系列参数从39M到1.5B的OLMoASR模型。

Result: OLMoASR模型在所有模型规模下，于短文本和长文本语音识别基准测试中均取得了与OpenAI Whisper相当的平均性能。具体而言，OLMoASR-medium.en模型在等同参数量下，其词错误率（WER）分别为12.8%和11.0%，与Whisper-medium.en的12.4%和10.5% WER相当。

Conclusion: 本研究成功展示了高质量大规模数据对训练鲁棒零样本语音识别模型的重要性，并开发出性能可与Whisper匹敌的OLMoASR模型。OLMoASR-Pool、OLMoASR模型及其过滤、训练和评估代码将公开发布，以推动鲁棒语音处理的进一步研究。

Abstract: Improvements in training data scale and quality have led to significant
advances, yet its influence in speech recognition remains underexplored. In
this paper, we present a large-scale dataset, OLMoASR-Pool, and series of
models, OLMoASR, to study and develop robust zero-shot speech recognition
models. Beginning from OLMoASR-Pool, a collection of 3M hours of English audio
and 17M transcripts, we design text heuristic filters to remove low-quality or
mistranscribed data. Our curation pipeline produces a new dataset containing 1M
hours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We use
OLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M
(tiny.en) to 1.5B (large.en) parameters. Across all model scales, OLMoASR
achieves comparable average performance to OpenAI's Whisper on short and
long-form speech recognition benchmarks. Notably, OLMoASR-medium.en attains a
12.8\% and 11.0\% word error rate (WER) that is on par with Whisper's largest
English-only model Whisper-medium.en's 12.4\% and 10.5\% WER for short and
long-form recognition respectively (at equivalent parameter count).
OLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code will
be made publicly available to further research on robust speech processing.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [215] [A Unified Theory of Language](https://arxiv.org/abs/2508.20109)
*Robert Worden*

Main category: q-bio.NC

TL;DR: 本文提出了一种统一的语言理论，结合贝叶斯认知语言学和语言因性选择进化以展示智力的观点，并基于增强型建构语法通过统一计算无缝解释语言的所有方面及其进化连续性。


<details>
  <summary>Details</summary>
Motivation: 旨在提出一个统一的语言理论，解释语言处理、多样性、语用学、句法、语义等主要事实，并结合贝叶斯认知模型和语言为展示智力而通过性选择进化的观点。

Method: 该理论的计算核心基于建构语法，并增加了语用学解释和快速精准语言学习两个新元素。建构被表示为图状特征结构，通过慢速通用推理进行初始学习，随后通过快速统一计算进行应用。语言所有方面（音系、句法、语义、语用）通过快速贝叶斯最大似然模式匹配统一计算，消除语义与语用边界。

Result: 该理论成功解释了语言的速度、表达力、多样性、语用学、句法和语义等主要事实。它实现了语言所有方面的无缝计算，消除了语义和语用之间的界限，从而解释了语用学的主要难题和详细现象。统一计算作为贝叶斯模式匹配，为人类语言处理与动物贝叶斯认知之间提供了进化的连续性。

Conclusion: 该统一理论全面解释了语言的处理、学习和进化，并强调语言是我们心智解读能力、合作、自尊、情感以及人类文化和社会基础的关键作用。

Abstract: A unified theory of language combines a Bayesian cognitive linguistic model
of language processing, with the proposal that language evolved by sexual
selection for the display of intelligence. The theory accounts for the major
facts of language, including its speed and expressivity, and data on language
diversity, pragmatics, syntax and semantics. The computational element of the
theory is based on Construction Grammars. These give an account of the syntax
and semantics of the worlds languages, using constructions and unification. Two
novel elements are added to construction grammars: an account of language
pragmatics, and an account of fast, precise language learning. Constructions
are represented in the mind as graph like feature structures. People use slow
general inference to understand the first few examples they hear of any
construction. After that it is learned as a feature structure, and is rapidly
applied by unification. All aspects of language (phonology, syntax, semantics,
and pragmatics) are seamlessly computed by fast unification; there is no
boundary between semantics and pragmatics. This accounts for the major puzzles
of pragmatics, and for detailed pragmatic phenomena. Unification is Bayesian
maximum likelihood pattern matching. This gives evolutionary continuity between
language processing in the human brain, and Bayesian cognition in animal
brains. Language is the basis of our mind reading abilities, our cooperation,
self esteem and emotions; the foundations of human culture and society.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [216] [Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder](https://arxiv.org/abs/2508.20474)
*Muhammad Shakeel,Yui Sudo,Yifan Peng,Chyi-Jiunn Lin,Shinji Watanabe*

Main category: eess.AS

TL;DR: 本文提出统一多说话人编码器（UME），通过共享语音基础编码器联合学习说话人分离、语音分离和多说话人ASR任务，显著提升了重叠语音处理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法对说话人分离、语音分离和多说话人ASR任务通常单独处理，忽略了它们之间的内在关联性。研究旨在通过联合学习捕捉任务间依赖，提升对重叠语音数据的处理性能。

Method: 提出UME架构，使用共享语音基础编码器，共同学习说话人分离 (SD)、语音分离 (SS) 和多说话人自动语音识别 (ASR) 任务的表示。利用UME多层隐藏表示作为残差加权和编码 (RWSE)，有效整合不同语义级别的信息，实现任务间的自底向上对齐。

Result: UME在LibriMix评估集上显著优于针对SD、SS和多说话人ASR的单任务基线。特别在SD任务上，UME超越了现有研究，在Libri2Mix和Libri3Mix评估集上分别达到了1.37%和2.29%的说话人识别错误率。

Conclusion: UME通过统一架构和联合训练有效利用了多任务间的相互依赖性，显著提高了对重叠语音数据的整体处理性能，尤其在说话人分离任务上表现突出。

Abstract: This paper presents a unified multi-speaker encoder (UME), a novel
architecture that jointly learns representations for speaker diarization (SD),
speech separation (SS), and multi-speaker automatic speech recognition (ASR)
tasks using a shared speech foundational encoder. We leverage the hidden
representations from multiple layers of UME as a residual weighted-sum encoding
(RWSE) to effectively use information from different semantic levels,
contributing to bottom-up alignment between tasks. This joint training approach
captures the inherent interdependencies among the tasks, enhancing overall
performance on overlapping speech data. Our evaluations demonstrate that UME
substantially improves over the single-task baselines dedicated to SD, SS, and
multi-speaker ASR on LibriMix evaluation sets. Notably, for SD, UME outperforms
the previous studies, achieving diarization error rates of 1.37% and 2.29% on
Libri2Mix and Libri3Mix evaluation sets, respectively.

</details>


### [217] [Is Audio Spoof Detection Robust to Laundering Attacks?](https://arxiv.org/abs/2408.14712)
*Hashim Ali,Surya Subramani,Shefali Sudhir,Raksha Varahamurthy,Hafiz Malik*

Main category: eess.AS

TL;DR: 随着语音克隆技术真实性提高，滥用风险增加。现有检测方法在干净数据上表现良好，但本文评估了SOTA检测方法在“清洗攻击”下的性能。研究创建了一个新的清洗攻击数据库，并发现SOTA系统在这种攻击下（特别是混响和加性噪声）表现不佳，表明需要更鲁棒的检测方法。


<details>
  <summary>Details</summary>
Motivation: 语音克隆（VC）系统合成语音的真实性显著提高，且成本低廉，导致技术滥用风险增加。尽管已有多种检测方法能有效识别语音欺骗，但这些方法大多在干净音频数据库（如ASVSpoof 2019）上进行评估，其在存在“清洗攻击”（即真实世界复杂噪声环境）情况下的鲁棒性未知。

Method: 本研究创建了一个新的清洗攻击数据库——ASVSpoof Laundering Database，该数据库基于ASVSpoof 2019 (LA) 评估数据库，总计包含1388.22小时的音频录音。在此数据库上，评估了七种最先进（SOTA）的音频欺骗检测方法。

Result: 研究结果表明，最先进（SOTA）的系统在存在激进的清洗攻击（特别是混响和加性噪声攻击）时表现不佳。

Conclusion: 上述结果表明，当前亟需开发更鲁棒的音频欺骗检测方法，以应对真实世界中复杂的清洗攻击。

Abstract: Voice-cloning (VC) systems have seen an exceptional increase in the realism
of synthesized speech in recent years. The high quality of synthesized speech
and the availability of low-cost VC services have given rise to many potential
abuses of this technology. Several detection methodologies have been proposed
over the years that can detect voice spoofs with reasonably good accuracy.
However, these methodologies are mostly evaluated on clean audio databases,
such as ASVSpoof 2019. This paper evaluates SOTA Audio Spoof Detection
approaches in the presence of laundering attacks. In that regard, a new
laundering attack database, called the ASVSpoof Laundering Database, is
created. This database is based on the ASVSpoof 2019 (LA) eval database
comprising a total of 1388.22 hours of audio recordings. Seven SOTA audio spoof
detection approaches are evaluated on this laundered database. The results
indicate that SOTA systems perform poorly in the presence of aggressive
laundering attacks, especially reverberation and additive noise attacks. This
suggests the need for robust audio spoof detection.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [218] [Particle swarm optimization for online sparse streaming feature selection under uncertainty](https://arxiv.org/abs/2508.20123)
*Ruiyang Xu*

Main category: cs.NE

TL;DR: 本文提出了一种名为POS2FS的不确定性感知在线稀疏流特征选择框架，它结合了粒子群优化和三支决策理论来解决高维流数据中的不完整性和不确定特征-标签相关性问题，并在真实数据集上展现出优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 在高维流数据应用中，在线流特征选择（OSFS）常因传感器故障等原因面临数据不完整性。现有在线稀疏流特征选择（OS2FS）方法虽通过潜在因子分析进行插补，但难以有效处理不确定的特征-标签相关性，导致模型缺乏灵活性和性能下降。

Method: 本文提出POS2FS框架，一种基于粒子群优化（PSO）增强的不确定性感知在线稀疏流特征选择方法。该方法主要引入：1) PSO驱动的监督机制，以降低特征-标签关系中的不确定性；2) 三支决策理论，以管理监督学习中的特征模糊性。

Result: 在六个真实世界数据集上的严格测试表明，POS2FS的性能优于传统的OSFS和OS2FS技术，通过更鲁棒的特征子集选择，提供了更高的准确性。

Conclusion: POS2FS有效解决了高维流数据中数据不完整性和不确定特征-标签相关性的挑战，通过结合PSO和三支决策理论，实现了更准确、更鲁棒的特征选择，从而超越了现有方法。

Abstract: In real-world applications involving high-dimensional streaming data, online
streaming feature selection (OSFS) is widely adopted. Yet, practical
deployments frequently face data incompleteness due to sensor failures or
technical constraints. While online sparse streaming feature selection (OS2FS)
mitigates this issue via latent factor analysis-based imputation, existing
methods struggle with uncertain feature-label correlations, leading to
inflexible models and degraded performance. To address these gaps, this work
proposes POS2FS-an uncertainty-aware online sparse streaming feature selection
framework enhanced by particle swarm optimization (PSO). The approach
introduces: 1) PSO-driven supervision to reduce uncertainty in feature-label
relationships; 2) Three-way decision theory to manage feature fuzziness in
supervised learning. Rigorous testing on six real-world datasets confirms
POS2FS outperforms conventional OSFS and OS2FS techniques, delivering higher
accuracy through more robust feature subset selection.

</details>


### [219] [Improving Liver Disease Diagnosis with SNNDeep: A Custom Spiking Neural Network Using Diverse Learning Algorithms](https://arxiv.org/abs/2508.20125)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.NE

TL;DR: 定制脉冲神经网络SNNDeep在肝脏健康CT影像二分类中表现出色，验证准确率达98.35%，超越现有框架，证实了低级可调SNN在医学影像中的潜力。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）作为节能、生物学合理性的深度学习替代品备受关注，但在高风险生物医学影像领域的应用几乎未被探索。

Method: 引入SNNDeep，第一个专为CT特征肝脏健康二分类优化的SNN模型，使用MSD的Task03\Liver数据集。比较了三种学习算法（Surrogate Gradient Learning, Tempotron rule, Bio-Inspired Active Learning）和三种架构（定制模型、snnTorch、SpikingJelly），并使用Optuna进行超参数优化。

Result: 定制SNNDeep模型持续优于基于框架的实现，达到98.35%的最大验证准确率，展现出更强的学习规则适应性，并显著降低了训练开销。

Conclusion: 低级、高度可调的SNN在医学影像领域，尤其在数据受限、时间紧迫的诊断场景中，可超越标准框架，为精准医疗中的神经启发式AI开辟新途径。

Abstract: Purpose: Spiking neural networks (SNNs) have recently gained attention as
energy-efficient, biologically plausible alternatives to conventional deep
learning models. Their application in high-stakes biomedical imaging remains
almost entirely unexplored. Methods: This study introduces SNNDeep, the first
tailored SNN specifically optimized for binary classification of liver health
status from computed tomography (CT) features. To ensure clinical relevance and
broad generalizability, the model was developed and evaluated using the
Task03\Liver dataset from the Medical Segmentation Decathlon (MSD), a
standardized benchmark widely used for assessing performance across diverse
medical imaging tasks. We benchmark three fundamentally different learning
algorithms, namely Surrogate Gradient Learning, the Tempotron rule, and
Bio-Inspired Active Learning across three architectural variants: a fully
customized low-level model built from scratch, and two implementations using
leading SNN frameworks, i.e., snnTorch and SpikingJelly. Hyperparameter
optimization was performed using Optuna. Results: Our results demonstrate that
the custom-built SNNDeep consistently outperforms framework-based
implementations, achieving a maximum validation accuracy of 98.35%, superior
adaptability across learning rules, and significantly reduced training
overhead. Conclusion:This study provides the first empirical evidence that
low-level, highly tunable SNNs can surpass standard frameworks in medical
imaging, especially in data-limited, temporally constrained diagnostic
settings, thereby opening a new pathway for neuro-inspired AI in precision
medicine.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [220] [Multi-Objective Optimization of ReRAM Crossbars for Robust DNN Inferencing under Stochastic Noise](https://arxiv.org/abs/2109.05437)
*Xiaoxuan Yang,Syrine Belakaria,Biresh Kumar Joardar,Huanrui Yang,Janardhan Rao Doppa,Partha Pratim Pande,Krishnendu Chakrabarty,Hai Li*

Main category: cs.ET

TL;DR: 本文提出了一种针对ReRAM交叉开关随机噪声的鲁棒深度神经网络推理硬件加速器设计，通过引入噪声感知训练方法（ReSNA）和多目标优化算法（CF-MESMO），在提高推理准确性的同时，高效地实现了面积、能耗和执行时间的权衡优化。


<details>
  <summary>Details</summary>
Motivation: ReRAM作为深度神经网络硬件加速器的前景广阔，但其随机噪声会显著降低推理准确性。研究动机是设计高性能、面积和能效俱佳的ReRAM加速器，以在存在随机噪声的情况下实现鲁棒的DNN推理。

Method: 1. 提出ReSNA（Stochastic-Noise-Aware Training）噪声感知训练方法，以提高ReRAM交叉开关上DNN推理的准确性。 2. 提出CF-MESMO（Continuous-Fidelity Multi-objective Optimization）信息论算法，用于识别推理准确性、面积开销、执行时间和能耗等多目标之间的Pareto最优解集。为解决评估ReSNA方法计算成本过高的问题，CF-MESMO采用连续保真度评估（通过改变训练周期），迭代选择能最大化单位计算成本信息增益的候选ReRAM设计和保真度对。

Result: 1. ReSNA方法在CIFAR-10数据集上，对ResNet20模型平均实现了2.57%的推理准确性提升。 2. CF-MESMO算法与流行的多目标优化算法NSGA-II相比，在达到相同最佳解决方案时，计算成本降低了90.91%。 3. 所提出的算法能有效揭示高质量的Pareto前沿。

Conclusion: 本研究提出的ReSNA训练方法有效提升了含随机噪声ReRAM的DNN推理准确性，而CF-MESMO算法则高效地解决了ReRAM硬件加速器设计中多目标优化的高计算成本问题，成功实现了鲁棒且高效的ReRAM基DNN加速器设计。

Abstract: Resistive random-access memory (ReRAM) is a promising technology for
designing hardware accelerators for deep neural network (DNN) inferencing.
However, stochastic noise in ReRAM crossbars can degrade the DNN inferencing
accuracy. We propose the design and optimization of a high-performance,
area-and energy-efficient ReRAM-based hardware accelerator to achieve robust
DNN inferencing in the presence of stochastic noise. We make two key technical
contributions. First, we propose a stochastic-noise-aware training method,
referred to as ReSNA, to improve the accuracy of DNN inferencing on ReRAM
crossbars with stochastic noise. Second, we propose an information-theoretic
algorithm, referred to as CF-MESMO, to identify the Pareto set of solutions to
trade-off multiple objectives, including inferencing accuracy, area overhead,
execution time, and energy consumption. The main challenge in this context is
that executing the ReSNA method to evaluate each candidate ReRAM design is
prohibitive. To address this challenge, we utilize the continuous-fidelity
evaluation of ReRAM designs associated with prohibitive high computation cost
by varying the number of training epochs to trade-off accuracy and cost.
CF-MESMO iteratively selects the candidate ReRAM design and fidelity pair that
maximizes the information gained per unit computation cost about the optimal
Pareto front. Our experiments on benchmark DNNs show that the proposed
algorithms efficiently uncover high-quality Pareto fronts. On average, ReSNA
achieves 2.57% inferencing accuracy improvement for ResNet20 on the CIFAR-10
dataset with respect to the baseline configuration. Moreover, CF-MESMO
algorithm achieves 90.91% reduction in computation cost compared to the popular
multi-objective optimization algorithm NSGA-II to reach the best solution from
NSGA-II.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [221] [Data-Efficient Point Cloud Semantic Segmentation Pipeline for Unimproved Roads](https://arxiv.org/abs/2508.20135)
*Andrew Yarovoi,Christopher R. Valenta*

Main category: eess.IV

TL;DR: 本案例研究提出了一种数据高效的点云分割流程和训练框架，用于在有限域内数据监督下对未改良道路和七个其他类别进行鲁棒分割，显著提升了分割性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在挑战性的、低数据量场景中，实现对点云（特别是未改良道路）的鲁棒分割面临数据稀缺的挑战，需要开发一种数据高效的方法来提高泛化能力和分割精度。

Method: 该方法采用两阶段训练框架：首先，一个基于投影的卷积神经网络在公共城市数据集和少量精心策划的域内数据集混合数据上进行预训练；然后，一个轻量级预测头部在纯域内数据上进行微调。研究中还探索了Point Prompt Training在批量归一化层上的应用、Manifold Mixup作为正则化的效果以及整合直方图归一化环境对性能的提升。

Result: 仅使用50个目标域的标记点云，该训练方法将平均交并比（mIoU）从33.5%提高到51.8%，整体准确率从85.5%提高到90.8%，优于仅在域内数据上进行朴素训练。结果表明，跨多个数据集的预训练是提高泛化能力和在有限域内监督下实现鲁棒分割的关键。

Conclusion: 本研究展示了一个在挑战性、低数据量场景中实现鲁棒3D语义分割的实用框架。跨数据集预训练对于提高模型泛化能力和在数据稀缺条件下实现鲁棒分割至关重要。

Abstract: In this case study, we present a data-efficient point cloud segmentation
pipeline and training framework for robust segmentation of unimproved roads and
seven other classes. Our method employs a two-stage training framework: first,
a projection-based convolutional neural network is pre-trained on a mixture of
public urban datasets and a small, curated in-domain dataset; then, a
lightweight prediction head is fine-tuned exclusively on in-domain data. Along
the way, we explore the application of Point Prompt Training to batch
normalization layers and the effects of Manifold Mixup as a regularizer within
our pipeline. We also explore the effects of incorporating histogram-normalized
ambients to further boost performance. Using only 50 labeled point clouds from
our target domain, we show that our proposed training approach improves mean
Intersection-over-Union from 33.5% to 51.8% and the overall accuracy from 85.5%
to 90.8%, when compared to naive training on the in-domain data. Crucially, our
results demonstrate that pre-training across multiple datasets is key to
improving generalization and enabling robust segmentation under limited
in-domain supervision. Overall, this study demonstrates a practical framework
for robust 3D semantic segmentation in challenging, low-data scenarios. Our
code is available at: https://github.com/andrewyarovoi/MD-FRNet.

</details>


### [222] [UltraEar: a multicentric, large-scale database combining ultra-high-resolution computed tomography and clinical data for ear diseases](https://arxiv.org/abs/2508.20141)
*Ruowei Tang,Pengfei Zhao,Xiaoguang Li,Ning Xu,Yue Cheng,Mengshi Zhang,Zhixiang Wang,Zhengyu Zhang,Hongxia Yin,Heyu Ding,Shusheng Gong,Yuhe Liu,Zhenchang Wang*

Main category: eess.IV

TL;DR: 本文介绍了UltraEar数据库的建立与设计，这是一个大规模、多中心、超高分辨率耳部CT影像及临床数据整合库，旨在推动耳部疾病的放射学研究和AI算法开发。


<details>
  <summary>Details</summary>
Motivation: 耳部疾病对全球健康和经济造成巨大负担，CT在诊断和治疗中发挥关键作用。为弥补现有数据资源的不足，急需一个专门针对耳部疾病的、高分辨率、多中心的CT影像和临床数据大型数据库。

Method: 本研究建立了UltraEar数据库。该数据库在2020年至2035年期间从11家三级医院招募患者，整合了各向同性0.1毫米超高分辨率CT（U-HRCT）图像、结构化CT报告以及包括人口统计学、听力学、手术记录和病理学发现等在内的全面临床信息。涵盖多种耳部疾病，并开发了标准化的预处理流程（包括几何校准、图像标注和多结构分割）。所有个人标识符均已匿名化，数据收集与管理通过专家小组会议协调，并安全存储于离线云系统。

Result: UltraEar数据库成功建立，提供了一个前所未有的、兼具技术保真度和临床相关性的超高分辨率耳部参考图谱。

Conclusion: UltraEar数据库具有巨大潜力，可显著推动放射学研究、促进AI算法的开发与验证、作为耳科影像培训工具，并支持多机构间的合作研究。该数据库将持续更新和扩展，以确保长期可访问性和可用性。

Abstract: Ear diseases affect billions of people worldwide, leading to substantial
health and socioeconomic burdens. Computed tomography (CT) plays a pivotal role
in accurate diagnosis, treatment planning, and outcome evaluation. The
objective of this study is to present the establishment and design of UltraEar
Database, a large-scale, multicentric repository of isotropic 0.1 mm
ultra-high-resolution CT (U-HRCT) images and associated clinical data dedicated
to ear diseases. UltraEar recruits patients from 11 tertiary hospitals between
October 2020 and October 2035, integrating U-HRCT images, structured CT
reports, and comprehensive clinical information, including demographics,
audiometric profiles, surgical records, and pathological findings. A broad
spectrum of otologic disorders is covered, such as otitis media, cholesteatoma,
ossicular chain malformation, temporal bone fracture, inner ear malformation,
cochlear aperture stenosis, enlarged vestibular aqueduct, and sigmoid sinus
bony deficiency. Standardized preprocessing pipelines have been developed for
geometric calibration, image annotation, and multi-structure segmentation. All
personal identifiers in DICOM headers and metadata are removed or anonymized to
ensure compliance with data privacy regulation. Data collection and curation
are coordinated through monthly expert panel meetings, with secure storage on
an offline cloud system. UltraEar provides an unprecedented
ultra-high-resolution reference atlas with both technical fidelity and clinical
relevance. This resource has significant potential to advance radiological
research, enable development and validation of AI algorithms, serve as an
educational tool for training in otologic imaging, and support
multi-institutional collaborative studies. UltraEar will be continuously
updated and expanded, ensuring long-term accessibility and usability for the
global otologic research community.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [223] [Navigating the EU AI Act: Foreseeable Challenges in Qualifying Deep Learning-Based Automated Inspections of Class III Medical Devices](https://arxiv.org/abs/2508.20144)
*Julio Zanon Diaz,Tommy Brennan,Peter Corcoran*

Main category: cs.CY

TL;DR: 本文评估了深度学习在三类医疗器械自动化视觉检测中的应用所面临的监管挑战，特别是欧盟AI法案与现有医疗器械法规之间的差异。


<details>
  <summary>Details</summary>
Motivation: 深度学习在三类医疗器械自动化视觉检测中具有提升质量和减少人类错误的巨大潜力，但其引入了新的监管复杂性，尤其是在欧盟AI法案下，其高风险系统义务与现有医疗器械法规（如MDR和U.S. FDA QSR）存在显著差异，给制造商的资质认证带来挑战。

Method: 本文采用高层次技术评估的方法，审视了制造商在现有医疗器械合规环境中认证基于深度学习的自动化检测时可能遇到的可预见挑战。具体分析了风险管理原则、数据集治理、模型验证、可解释性要求和部署后监控义务等方面的监管差异，并探讨了潜在的实施策略和不确定性领域。

Result: 论文识别了制造商在将深度学习应用于医疗器械检测时将面临的挑战，并详细审查了不同监管框架下在风险管理、数据集治理、模型验证、可解释性和部署后监控等方面的分歧。同时，也探讨了潜在的实施策略，并指出了数据保留负担、全球合规影响以及在有限缺陷数据下进行验证的实际困难等不确定性区域。

Conclusion: 该研究提供了一个关于深度学习在医疗器械自动化检测中应用所面临监管复杂性（尤其是在欧盟AI法案下）的高层次技术评估。它强调了制造商在合规性、实施策略以及解决数据限制和全球兼容性方面的诸多不确定性挑战，提示在DL技术全面应用前需解决这些合规难题。

Abstract: As deep learning (DL) technologies advance, their application in automated
visual inspection for Class III medical devices offers significant potential to
enhance quality assurance and reduce human error. However, the adoption of such
AI-based systems introduces new regulatory complexities--particularly under the
EU Artificial Intelligence (AI) Act, which imposes high-risk system obligations
that differ in scope and depth from established regulatory frameworks such as
the Medical Device Regulation (MDR) and the U.S. FDA Quality System Regulation
(QSR). This paper presents a high-level technical assessment of the
foresee-able challenges that manufacturers are likely to encounter when
qualifying DL-based automated inspections within the existing medical device
compliance landscape. It examines divergences in risk management principles,
dataset governance, model validation, explainability requirements, and
post-deployment monitoring obligations. The discussion also explores potential
implementation strategies and highlights areas of uncertainty, including data
retention burdens, global compliance implications, and the practical
difficulties of achieving statistical significance in validation with limited
defect data. Disclaimer: This publication is in-tended solely as an academic
and technical evaluation. It is not a substitute for le-gal advice or official
regulatory interpretation. The information presented here should not be relied
upon to demonstrate compliance with the EU AI Act or any other statutory
obligation. Manufacturers are encouraged to consult appropriate regulatory
authorities and legal experts to determine specific compliance pathways.

</details>


### [224] [RelAItionship Building: Analyzing Recruitment Strategies for Participatory AI](https://arxiv.org/abs/2508.20176)
*Eugene Kim,Vaibhav Balloli,Berelian Karimian,Elizabeth Bondi-Kelly,Benjamin Fish*

Main category: cs.CY

TL;DR: 本研究调查了参与式AI项目中利益攸关者招募方法的挑战及其影响，通过分析37个项目和访谈5位研究人员，发现招募结果受结构条件、研究者目标和关系影响，并提出了关系导向的招募和反思性文档实践建议。


<details>
  <summary>Details</summary>
Motivation: 参与式AI有望确保AI系统符合社区需求和价值观，但识别、接触和吸引所有相关利益攸关者的招募方法仍然是一个实际挑战。本研究旨在探究研究人员在设计和执行参与式AI项目招募方法时面临的挑战，以及当前招募实践对参与式AI的影响。

Method: 1. 描述并分析了包含37个AI项目实践的语料库，以捕捉该领域招募方法的 다양性，并初步分析招募实践的文档记录以及为实现公平和赋权目标所使用的具体策略。2. 访谈了五位AI研究人员，以了解招募方法的结果。

Result: 研究发现，招募结果受到研究人员工作的结构性条件、研究人员自身的目标和期望，以及通过招募方法和后续合作建立的关系所影响。

Conclusion: 基于分析，本研究为参与式AI研究人员提供了设计和执行以关系为导向的招募方法，以及反思性招募文档实践的建议。

Abstract: Participatory AI, in which impacted community members and other stakeholders
are involved in the design and development of AI systems, holds promise as a
way to ensure AI is developed to meet their needs and reflect their values.
However, the process of identifying, reaching out, and engaging with all
relevant stakeholder groups, which we refer to as recruitment methodology, is
still a practical challenge in AI projects striving to adopt participatory
practices. In this paper, we investigate the challenges that researchers face
when designing and executing recruitment methodology for Participatory AI
projects, and the implications of current recruitment practice for
Participatory AI. First, we describe the recruitment methodologies used in AI
projects using a corpus of 37 projects to capture the diversity of practices in
the field and perform an initial analysis on the documentation of recruitment
practices, as well as specific strategies that researchers use to meet goals of
equity and empowerment. To complement this analysis, we interview five AI
researchers to learn about the outcomes of recruitment methodologies. We find
that these outcomes are shaped by structural conditions of their work,
researchers' own goals and expectations, and the relationships built from the
recruitment methodology and subsequent collaboration. Based on these analyses,
we provide recommendations for designing and executing relationship-forward
recruitment methods, as well as reflexive recruitment documentation practices
for Participatory AI researchers.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [225] [Validating Generative Agent-Based Models for Logistics and Supply Chain Management Research](https://arxiv.org/abs/2508.20234)
*Vincent E. Castillo*

Main category: cs.MA

TL;DR: 本研究评估了LLM驱动的生成式Agent模型（GABMs）在物流与供应链管理（LSCM）中模拟人类行为的有效性。结果发现GABMs可以有效模拟人类行为，但在表层等效性与深层决策过程之间存在悖论，强调了GABMs需进行双层验证。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的生成式Agent模型（GABMs）在物流与供应链管理（LSCM）中模拟复杂人类行为具有巨大潜力，但其作为人类行为代理的有效性（人机等效性）尚不明确。

Method: 通过一项受控实验，在食品配送场景中评估LLM与人类行为的等效性。实验测试了六个前沿LLM，并与957名人类参与者（477对）进行对比，采用有调节的中介设计。使用双边单侧检验（TOST）评估表层等效性，使用结构方程模型（SEM）验证决策过程。

Result: GABMs能够有效模拟LSCM中的人类行为，但存在“等效性与过程”悖论。部分LLM在表层上与人类行为等效，但结构方程模型揭示某些LLM的决策过程与人类参与者不同。研究提出了GABMs需要双层验证：人类等效性测试和决策过程验证。

Conclusion: GABMs是LSCM中潜在可行的研究工具，但需通过适当的验证。本研究提出的双重验证框架为LSCM研究人员提供了严谨开发GABMs的指导，并为实践者选择LLM提供了基于证据的评估。

Abstract: Generative Agent-Based Models (GABMs) powered by large language models (LLMs)
offer promising potential for empirical logistics and supply chain management
(LSCM) research by enabling realistic simulation of complex human behaviors.
Unlike traditional agent-based models, GABMs generate human-like responses
through natural language reasoning, which creates potential for new
perspectives on emergent LSCM phenomena. However, the validity of LLMs as
proxies for human behavior in LSCM simulations is unknown. This study evaluates
LLM equivalence of human behavior through a controlled experiment examining
dyadic customer-worker engagements in food delivery scenarios. I test six
state-of-the-art LLMs against 957 human participants (477 dyads) using a
moderated mediation design. This study reveals a need to validate GABMs on two
levels: (1) human equivalence testing, and (2) decision process validation.
Results reveal GABMs can effectively simulate human behaviors in LSCM; however,
an equivalence-versus-process paradox emerges. While a series of Two One-Sided
Tests (TOST) for equivalence reveals some LLMs demonstrate surface-level
equivalence to humans, structural equation modeling (SEM) reveals artificial
decision processes not present in human participants for some LLMs. These
findings show GABMs as a potentially viable methodological instrument in LSCM
with proper validation checks. The dual-validation framework also provides LSCM
researchers with a guide to rigorous GABM development. For practitioners, this
study offers evidence-based assessment for LLM selection for operational tasks.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [226] [Artificial Intelligence for CRISPR Guide RNA Design: Explainable Models and Off-Target Safety](https://arxiv.org/abs/2508.20130)
*Alireza Abbaszadeh,Armita Shahlai*

Main category: q-bio.QM

TL;DR: 人工智能（特别是深度学习和可解释AI）正在通过优化gRNA设计、预测靶向活性和识别脱靶风险，从而提高CRISPR基因编辑的效率、特异性和临床可行性。


<details>
  <summary>Details</summary>
Motivation: CRISPR基因编辑技术仍面临优化向导RNA (gRNA) 设计以提高效率和安全性的关键挑战。

Method: 本文综述了利用最先进的机器学习（包括深度学习和可解释AI）模型来增强CRISPR系统gRNA设计的方法，涵盖模型预测解释策略、脱靶预测和安全性评估的新进展。

Result: 人工智能显著提高了gRNA靶向活性预测和脱靶风险识别的能力；可解释AI技术开始揭示驱动Cas酶性能的关键序列特征和基因组背景。

Conclusion: AI与基因编辑的跨学科融合，正推动CRISPR应用向更高效、更特异、更具临床可行性的方向发展。

Abstract: CRISPR-based genome editing has revolutionized biotechnology, yet optimizing
guide RNA (gRNA) design for efficiency and safety remains a critical challenge.
Recent advances (2020--2025, updated to reflect current year if needed)
demonstrate that artificial intelligence (AI), especially deep learning, can
markedly improve the prediction of gRNA on-target activity and identify
off-target risks. In parallel, emerging explainable AI (XAI) techniques are
beginning to illuminate the black-box nature of these models, offering insights
into sequence features and genomic contexts that drive Cas enzyme performance.
Here we review how state-of-the-art machine learning models are enhancing gRNA
design for CRISPR systems, highlight strategies for interpreting model
predictions, and discuss new developments in off-target prediction and safety
assessment. We emphasize breakthroughs from top-tier journals that underscore
an interdisciplinary convergence of AI and genome editing to enable more
efficient, specific, and clinically viable CRISPR applications.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [227] [Microarchitecture Design and Benchmarking of Custom SHA-3 Instruction for RISC-V](https://arxiv.org/abs/2508.20653)
*Alperen Bolat,Sakir Sezer,Kieran McLaughlin,Henry Hui*

Main category: cs.AR

TL;DR: 本研究在RISC-V CPU架构中实现了SHA-3自定义指令的微架构集成，通过GEM5模拟和FPGA原型验证，显著提升了SHA-3和Keccak相关工作负载的性能，同时硬件开销适中。


<details>
  <summary>Details</summary>
Motivation: 将加密加速器集成到现代CPU架构中面临独特的微架构挑战，尤其是在扩展复杂多阶段操作的指令集时。虽然AES-NI等已有成功案例，但SHA-3由于其独特的置换结构和内存访问模式，其高效加速仍是开放问题。现有解决方案主要依赖独立协处理器或软件优化，通常避免直接的微架构集成复杂性。

Method: 本研究旨在将SHA-3置换操作作为自定义指令嵌入通用处理器（RISC-V CPU架构），重点关注流水线并行执行、存储利用率和硬件成本。研究通过循环精确的GEM5模拟和FPGA原型验证进行。

Result: 研究结果显示，对于RISC-V优化的SHA-3软件工作负载，性能提升高达8.02倍；对于Keccak特定软件工作负载，性能提升高达46.31倍。硬件开销方面，寄存器仅增加了15.09%，LUT利用率增加了11.51%。

Conclusion: 这些发现为SHA-3在微架构层面的加速提供了关键见解，证实了其可行性和影响力，并为未来的密码指令集扩展提供了实用的设计考量。

Abstract: Integrating cryptographic accelerators into modern CPU architectures presents
unique microarchitectural challenges, particularly when extending instruction
sets with complex and multistage operations. Hardware-assisted cryptographic
instructions, such as Intel's AES-NI and ARM's custom instructions for
encryption workloads, have demonstrated substantial performance improvements.
However, efficient SHA-3 acceleration remains an open problem due to its
distinct permutation-based structure and memory access patterns. Existing
solutions primarily rely on standalone coprocessors or software optimizations,
often avoiding the complexities of direct microarchitectural integration. This
study investigates the architectural challenges of embedding a SHA-3
permutation operation as a custom instruction within a general-purpose
processor, focusing on pipelined simultaneous execution, storage utilization,
and hardware cost. In this paper, we investigated and prototyped a SHA-3 custom
instruction for the RISC-V CPU architecture. Using cycle-accurate GEM5
simulations and FPGA prototyping, our results demonstrate performance
improvements of up to 8.02x for RISC-V optimized SHA-3 software workloads and
up to 46.31x for Keccak-specific software workloads, with only a 15.09%
increase in registers and a 11.51% increase in LUT utilization. These findings
provide critical insights into the feasibility and impact of SHA-3 acceleration
at the microarchitectural level, highlighting practical design considerations
for future cryptographic instruction set extensions.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [228] [A Hierarchical Signal Coordination and Control System Using a Hybrid Model-based and Reinforcement Learning Approach](https://arxiv.org/abs/2508.20102)
*Xianyue Peng,Shenyang Chen,H. Michael Zhang*

Main category: eess.SY

TL;DR: 本文提出一种分层交通信号协调控制方案，结合模型优化与强化学习，以应对城市交通走廊中主干道通行与局部需求变化的双重挑战，通过自适应策略选择在不同需求下实现鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 城市交通走廊的信号控制面临双重挑战：既要保持主干道交通顺畅，又要适应局部交叉口的需求变化。

Method: 提出了一个分层交通信号协调控制方案，该方案整合了基于模型的优化和强化学习。具体包括：高层协调器（HLC）根据需求选择协调策略；走廊协调器根据选定策略（最大流量协调MFC或绿波协调GWC）推导相位约束；以及混合信号智能体（HSA）通过带动作掩码的强化学习确定信号相位。采用带近端策略优化（PPO）的分层强化学习来训练HSA和HLC策略。HLC通过平衡走廊级和全网性能的多目标奖励来动态切换策略。该方案在SUMO-RLlib平台上进行了开发和评估。

Result: 混合MFC在交通需求大时最大化吞吐量；混合GWC在不同交通条件下能持续最小化主干道停车并保持通行，但可能降低全网效率；纯智能体控制（PAC）在中等需求下能改善全网旅行时间，但在交通需求大时效果不佳。

Conclusion: 该分层设计能够自适应地选择协调策略，从而在所有交通需求水平下实现鲁棒且高效的性能。

Abstract: Signal control in urban corridors faces the dual challenge of maintaining
arterial traffic progression while adapting to demand variations at local
intersections. We propose a hierarchical traffic signal coordination and
control scheme that integrates model-based optimization with reinforcement
learning. The system consists of: (i) a High-Level Coordinator (HLC) that
selects coordination strategies based on observed and predicted demand; (ii) a
Corridor Coordinator that derives phase constraints from the selected
strategy-either Max-Flow Coordination (MFC) or Green-Wave Coordination (GWC);
and (iii) Hybrid Signal Agents (HSAs) that determine signal phases via
reinforcement learning with action masking to enforce feasibility. Hierarchical
reinforcement learning with Proximal Policy Optimization (PPO) is used to train
HSA and HLC policies. At the lower level, three HSA policies-MFC-aware,
GWC-aware, and pure agent control (PAC) are trained in conjunction with their
respective coordination strategies. At the higher level, the HLC is trained to
dynamically switch strategies using a multi-objective reward balancing
corridor-level and network-wide performance. The proposed scheme was developed
and evaluated on a SUMO-RLlib platform. Case results show that hybrid MFC
maximizes throughput under heavy demand; hybrid GWC consistently minimizes
arterial stops and maintains progression across diverse traffic conditions but
can reduce network-wide efficiency; and PAC improves network-wide travel time
in moderate demand but is less effective under heavy demand. The hierarchical
design enables adaptive strategy selection, achieving robust performance across
all demand levels.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [229] [SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization](https://arxiv.org/abs/2508.20258)
*Arya Tschand,Muhammad Awad,Ryan Swann,Kesavan Ramakrishnan,Jeffrey Ma,Keith Lowery,Ganesh Dasika,Vijay Janapa Reddi*

Main category: cs.DC

TL;DR: 现有LLMs在GPU内核优化上缺乏硬件感知且效率低下。本文提出SwizzlePerf，通过为LLMs提供硬件感知，能自动为GPU内核生成空间优化，显著超越人工专家，实现高达2.06倍加速和70%的L2命中率提升。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在GPU内核性能工程中采用的搜索方法效率低下，并且缺乏人类性能工程师实现近乎最优利用率所依赖的关键特性——硬件感知。

Method: 本文提出SwizzlePerf，通过整合工作负载的特定内存访问模式、架构规范、过滤后的性能分析日志和历史性能数据，为LLMs提供显式的硬件感知。SwizzlePerf旨在自动为异构架构上的GPU内核生成空间优化（如数据交织模式）。

Result: 1. 对于GEMM内核，SwizzlePerf能在5分钟内生成与专家性能工程师耗时2周找到的硬件特定最优交织模式相同的模式。
2. 在10个多样化的机器学习和科学内核测试中，SwizzlePerf能为其中9个内核生成交织模式，实现高达2.06倍的加速和70%的L2命中率提升。

Conclusion: 这项工作是系统性创建硬件感知型LLM性能工程代理迈出的重要第一步。

Abstract: Large language models (LLMs) have shown progress in GPU kernel performance
engineering using inefficient search-based methods that optimize around
runtime. Any existing approach lacks a key characteristic that human
performance engineers rely on for near-optimal utilization --
hardware-awareness. By leveraging the workload's specific memory access
patterns, architecture specifications, filtered profiling logs, and reflections
on historical performance, we can make software-level optimizations that are
tailored to the underlying hardware. SwizzlePerf automatically generates
spatial optimizations for GPU kernels on disaggregated architectures by giving
LLMs explicit hardware-awareness.
  For a GEMM kernel, SwizzlePerf takes less than 5 minutes to generate the same
hardware-specific optimal swizzling pattern that took expert performance
engineers 2 weeks to find. On a suite of 10 diverse ML and Science kernels,
SwizzlePerf can generate swizzling patterns for 9 of the kernels that achieve
up to a 2.06x speedup and 70% improvement in L2 hit rate. This work is the
first of many steps toward systematically creating hardware-aware LLM
performance engineering agents.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [230] [Can LLMs Identify Tax Abuse?](https://arxiv.org/abs/2508.20097)
*Andrew Blair-Stanek,Nils Holzenberger,Benjamin Van Durme*

Main category: q-fin.CP

TL;DR: 研究调查大型语言模型（LLM）是否能发现和分析美国避税策略，并发现其能识别出新颖策略。


<details>
  <summary>Details</summary>
Motivation: 减少富裕纳税人避税造成的税收损失，解决即使是人类专家也面临挑战的真实世界难题，并探索LLM在复杂、动态的法律推理领域的潜力。

Method: 评估最先进的LLM在以下方面的能力：1) 解释和验证避税策略；2) 补充不完整策略的缺失部分；3) 从零开始生成完整的端到端策略。

Result: LLM识别出了一种全新的避税策略。

Conclusion: LLM在发现和分析避税策略方面具有巨大潜力，可能彻底改变税务机构打击避税行为的方式。

Abstract: We investigate whether large language models can discover and analyze U.S.
tax-minimization strategies. This real-world domain challenges even seasoned
human experts, and progress can reduce tax revenue lost from well-advised,
wealthy taxpayers. We evaluate the most advanced LLMs on their ability to (1)
interpret and verify tax strategies, (2) fill in gaps in partially specified
strategies, and (3) generate complete, end-to-end strategies from scratch. This
domain should be of particular interest to the LLM reasoning community: unlike
synthetic challenge problems or scientific reasoning tasks, U.S. tax law
involves navigating hundreds of thousands of pages of statutes, case law, and
administrative guidance, all updated regularly. Notably, LLM-based reasoning
identified an entirely novel tax strategy, highlighting these models' potential
to revolutionize tax agencies' fight against tax abuse.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [231] [Differentially Private Federated Quantum Learning via Quantum Noise](https://arxiv.org/abs/2508.20310)
*Atit Pokharel,Ratun Rahman,Shaba Shaon,Thomas Morris,Dinh C. Nguyen*

Main category: quant-ph

TL;DR: 本文提出在NISQ设备上的量子联邦学习（QFL）中，利用固有的量子噪声实现差分隐私（DP）以保护模型信息，并通过调整噪声参数展示了隐私、鲁棒性与训练精度之间的可调权衡。


<details>
  <summary>Details</summary>
Motivation: QFL在NISQ设备上面临隐私泄露（通过共享模型更新）和对抗攻击的风险。核心研究动机是探索如何利用NISQ设备固有的量子噪声来强制执行差分隐私（DP），以在QFL的训练和通信过程中保护模型信息。

Method: 提出了一种新颖的DP机制，该机制通过利用量子噪声来保护QFL过程中的量子模型。具体方法是通过调整测量次数（measurement shots）和去极化信道强度（depolarizing channel strength）来调谐噪声方差，从而实现符合NISQ约束的期望DP水平。

Result: 1. 模拟结果阐明了差分隐私预算与噪声参数之间的关系，以及安全性与训练精度之间的权衡。2. 证明了该框架能够有效抵御旨在利用对抗样本损害模型性能的对抗攻击，并评估了对抗样本上的准确性、正确预测的置信度分数和攻击成功率等关键指标。3. 研究揭示了隐私与鲁棒性之间存在可调的权衡关系。

Conclusion: 该研究为NISQ设备上的安全QFL提供了一种高效解决方案，通过实现隐私和鲁棒性之间的可调权衡，在可靠的量子计算应用中展现出巨大的潜力。

Abstract: Quantum federated learning (QFL) enables collaborative training of quantum
machine learning (QML) models across distributed quantum devices without raw
data exchange. However, QFL remains vulnerable to adversarial attacks, where
shared QML model updates can be exploited to undermine information privacy. In
the context of noisy intermediate-scale quantum (NISQ) devices, a key question
arises: How can inherent quantum noise be leveraged to enforce differential
privacy (DP) and protect model information during training and communication?
This paper explores a novel DP mechanism that harnesses quantum noise to
safeguard quantum models throughout the QFL process. By tuning noise variance
through measurement shots and depolarizing channel strength, our approach
achieves desired DP levels tailored to NISQ constraints. Simulations
demonstrate the framework's effectiveness by examining the relationship between
differential privacy budget and noise parameters, as well as the trade-off
between security and training accuracy. Additionally, we demonstrate the
framework's robustness against an adversarial attack designed to compromise
model performance using adversarial examples, with evaluations based on
critical metrics such as accuracy on adversarial examples, confidence scores
for correct predictions, and attack success rates. The results reveal a tunable
trade-off between privacy and robustness, providing an efficient solution for
secure QFL on NISQ devices with significant potential for reliable quantum
computing applications.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [232] [ELIXIR: Efficient and LIghtweight model for eXplaIning Recommendations](https://arxiv.org/abs/2508.20312)
*Ben Kabongo,Vincent Guigue,Pirmin Lemberger*

Main category: cs.IR

TL;DR: ELIXIR是一个轻量级多任务模型，通过结合评分预测和个性化评论生成，解决现有解释性推荐系统在细粒度交互和方面建模上的不足，并在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 协同过滤在推荐系统中广泛应用但缺乏细粒度交互和可解释性。用户对透明推荐需求日益增长，现有基于RNN或Transformer的解释生成方法未能有效利用预训练模型能力，或忽略了对个性化解释至关重要的方面建模。

Method: 提出ELIXIR模型，一个多任务模型，结合了评分预测和个性化评论生成。ELIXIR共同学习用户和物品的全局及方面特定表示，优化整体评分、方面级别评分和评论生成，并利用个性化注意力强调方面重要性。该模型基于T5-small（60M）构建。

Result: 在TripAdvisor和RateBeer数据集上的实验结果表明，ELIXIR显著优于强基线模型，尤其在评论生成方面表现突出。即使基于小型T5模型，其方面建模架构在个性化文本生成中也比使用更大模型但匹配用户偏好不佳的现有最先进方法更有效。

Conclusion: ELIXIR成功提供了一种高效且轻量级的解释性推荐模型，通过其独特的方面建模架构，解决了现有方法在个性化解释方面的局限性，在推荐评分预测和个性化评论生成方面均取得了卓越性能。

Abstract: Collaborative filtering drives many successful recommender systems but
struggles with fine-grained user-item interactions and explainability. As users
increasingly seek transparent recommendations, generating textual explanations
through language models has become a critical research area. Existing methods
employ either RNNs or Transformers. However, RNN-based approaches fail to
leverage the capabilities of pre-trained Transformer models, whereas
Transformer-based methods often suffer from suboptimal adaptation and neglect
aspect modeling, which is crucial for personalized explanations. We propose
ELIXIR (Efficient and LIghtweight model for eXplaIning Recommendations), a
multi-task model combining rating prediction with personalized review
generation. ELIXIR jointly learns global and aspect-specific representations of
users and items, optimizing overall rating, aspect-level ratings, and review
generation, with personalized attention to emphasize aspect importance. Based
on a T5-small (60M) model, we demonstrate the effectiveness of our aspect-based
architecture in guiding text generation in a personalized context, where
state-of-the-art approaches exploit much larger models but fail to match user
preferences as well. Experimental results on TripAdvisor and RateBeer
demonstrate that ELIXIR significantly outperforms strong baseline models,
especially in review generation.

</details>


### [233] [MPFormer: Adaptive Framework for Industrial Multi-Task Personalized Sequential Retriever](https://arxiv.org/abs/2508.20400)
*Yijia Sun,Shanshan Huang,Linxiao Che,Haitao Lu,Qiang Luo,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: 本文提出MPFormer，一个动态多任务Transformer框架，通过目标条件Transformer、个性化目标权重和用户个性化信息，解决工业推荐系统中多阶段优化错位和多目标检索效率问题，已在快手部署并取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 现代工业推荐系统面临多阶段优化错位的核心挑战：召回阶段的单目标建模与排序阶段的多目标优化之间存在显著语义鸿沟。主流方案（并行多路径单目标召回）导致资源随目标数量线性增长，且难以处理松散耦合的目标。

Method: 本文提出MPFormer，一个动态多任务Transformer框架。其核心机制包括：1) 目标条件Transformer，通过可学习的注意力调制联合编码用户行为序列和多任务语义；2) 引入个性化目标权重，动态调整召回结果；3) 将用户个性化信息融入Token表示和Transformer结构，增强模型表达能力。

Result: MPFormer已成功集成至快手短视频推荐系统，稳定服务超4亿日活跃用户。它显著提升了用户日均互动和系统运营效率。实际部署验证表明，相较于传统方案，MPFormer在优化多目标召回迭代范式同时保持了服务响应速度。

Conclusion: MPFormer为工业推荐系统提供了一个可扩展的多目标解决方案，有效解决了多阶段优化错位、资源线性增长以及处理松散耦合目标的问题，并在大规模工业应用中得到了有效验证。

Abstract: Modern industrial recommendation systems encounter a core challenge of
multi-stage optimization misalignment: a significant semantic gap exists
between the multi-objective optimization paradigm widely used in the ranking
phase and the single-objective modeling in the retrieve phase. Although the
mainstream industry solution achieves multi-objective coverage through parallel
multi-path single-objective retrieval, this approach leads to linear growth of
training and serving resources with the number of objectives and has inherent
limitations in handling loosely coupled objectives. This paper proposes the
MPFormer, a dynamic multi-task Transformer framework, which systematically
addresses the aforementioned issues through three innovative mechanisms. First,
an objective-conditioned transformer that jointly encodes user behavior
sequences and multi-task semantics through learnable attention modulation;
second, personalized target weights are introduced to achieve dynamic
adjustment of retrieval results; finally, user personalization information is
incorporated into token representations and the Transformer structure to
further enhance the model's representation ability. This framework has been
successfully integrated into Kuaishou short video recommendation system, stably
serving over 400 million daily active users. It significantly improves user
daily engagement and system operational efficiency. Practical deployment
verification shows that, compared with traditional solutions, it effectively
optimizes the iterative paradigm of multi-objective retrieval while maintaining
service response speed, providing a scalable multi-objective solution for
industrial recommendation systems.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [234] [Collaborating with GenAI: Incentives and Replacements](https://arxiv.org/abs/2508.20213)
*Boaz Taitler,Omer Ben-Porat*

Main category: cs.GT

TL;DR: 本文提出一个理论框架，分析了生成式AI (GenAI) 如何影响共享项目中员工的努力程度和团队协作。研究发现，GenAI可能导致员工不努力，且管理者优化团队配置的问题是NP完全的，强调了低个体价值员工在维持产出中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的兴起，它正在重塑员工的贡献方式。虽然GenAI能提升效率，但管理者也可能用它取代员工。因此，有必要深入分析GenAI在此类情境下对协作的影响。

Method: 研究构建了一个理论框架和模型。模型中，管理者选择团队成员，GenAI替代未选中的员工，每位员工决定努力程度及承担成本。研究方法包括对管理者优化问题的NP完全性分析、为（几乎）线性实例提供高效算法，并辅以广泛的模拟验证。

Result: 研究发现，即使GenAI几乎无效，也可能导致员工完全不投入努力。管理者的优化问题是NP完全的，但对于特定线性情况，提供了高效算法。分析表明，即使个体价值较低的员工，也可能在维持整体产出方面发挥关键作用，排除他们可能引发负面级联效应。

Conclusion: GenAI对协作有着复杂影响，可能降低员工积极性并带来团队配置难题。管理者在利用GenAI时，需警惕其对员工努力的潜在负面效应，并认识到“低价值”员工在维持整体产出中的关键作用，避免因盲目替代引发负面级联。

Abstract: The rise of Generative AI (GenAI) is reshaping how workers contribute to
shared projects. While workers can use GenAI to boost productivity or reduce
effort, managers may use it to replace some workers entirely. We present a
theoretical framework to analyze how GenAI affects collaboration in such
settings. In our model, the manager selects a team to work on a shared task,
with GenAI substituting for unselected workers. Each worker selects how much
effort to exert, and incurs a cost that increases with the level of effort. We
show that GenAI can lead workers to exert no effort, even if GenAI is almost
ineffective. We further show that the manager's optimization problem is
NP-complete, and provide an efficient algorithm for the special class of
(almost-) linear instances. Our analysis shows that even workers with low
individual value may play a critical role in sustaining overall output, and
excluding such workers can trigger a cascade. Finally, we conduct extensive
simulations to illustrate our theoretical findings.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [235] [Towards Better Correctness and Efficiency in Code Generation](https://arxiv.org/abs/2508.20124)
*Yunlong Feng,Yang Xu,Xiao Xu,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 本文提出一个以效率为导向的强化学习框架和两阶段调优方法，通过动态探索、误差不敏感强化学习和高对比度效率信号，显著提升了代码大模型生成代码的正确性和运行效率。


<details>
  <summary>Details</summary>
Motivation: 代码大语言模型生成的代码在运行时效率方面表现不佳，这限制了其在对性能敏感的实际场景中的应用。

Method: 研究者提出了一个以效率为导向的强化学习框架，并由新颖的性能奖励指导。该框架通过解决关键瓶颈来提升效率，包括：(1) 动态探索以克服静态数据限制。(2) 采用误差不敏感强化学习方法和高对比度效率信号。(3) 在高正确性基线上进行在线探索。最终，研究者提出了一种两阶段调优方法，以实现正确性和效率的平衡。

Result: 实验结果表明，该方法在7B模型上将代码正确性提高了10.18%，运行时效率提高了7.75%，达到了与大得多的模型相当的性能。

Conclusion: 该研究提出的方法有效提升了代码大模型生成代码的正确性和运行时效率，使其在实际应用中更具竞争力，并有望使其性能与更大规模的模型相媲美。

Abstract: While code large language models have demonstrated remarkable progress in
code generation, the generated code often exhibits poor runtime efficiency,
limiting its practical application in performance-sensitive scenarios. To
address this limitation, we propose an efficiency-oriented reinforcement
learning framework guided by a novel performance reward. Based on this
framework, we take a deeper dive into the code efficiency problem, identifying
then proposing methods to overcome key bottlenecks: (1) Dynamic exploration
overcomes the static data constraints of offline fine-tuning, enabling the
discovery of more efficient code implementations. (2) The error-insensitive
reinforcement learning method and high-contrast efficiency signals are crucial
for mitigating systematic errors and achieving effective optimization. (3)
Online exploration is most effective when starting from a high-correctness
baseline, as this allows for efficiency improvements without sacrificing
accuracy. With these discoveries, we finally propose a two-stage tuning method,
which achieves high and balanced performance across correctness and efficiency.
The results of experiments show the effectiveness of the method, which improves
code correctness by 10.18\% and runtime efficiency by 7.75\% on a 7B model,
achieving performance comparable to much larger model.

</details>


### [236] [Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators](https://arxiv.org/abs/2508.20340)
*Maolin Sun,Yibiao Yang,Yuming Zhou*

Main category: cs.SE

TL;DR: 本研究提出Chimera，一个LLM辅助的模糊测试框架，通过生成可重用项生成器来测试SMT求解器，解决了传统LLM方法在语法有效性和计算开销上的问题，并发现了大量实际bug。


<details>
  <summary>Details</summary>
Motivation: SMT求解器对现代系统和编程语言至关重要，其正确性是基础。然而，现有测试技术难以跟上快速发展的求解器功能。基于LLM的方法虽有前景，但面临生成公式语法无效和迭代交互计算开销大的问题。

Method: Chimera框架通过LLM将SMT文档转换为上下文无关文法（CFG），并合成符合这些文法、可组合的布尔项生成器。在模糊测试过程中，Chimera使用这些生成器填充现有公式的结构骨架，确保语法有效性并促进语义多样性。该方法仅需一次LLM交互。

Result: Chimera在Z3和cvc5两个主流SMT求解器上进行了评估，共识别出43个已确认的bug，其中40个已被开发者修复。

Conclusion: Chimera通过创新的LLM辅助模糊测试方法，有效解决了SMT求解器测试中公式语法有效性和计算成本问题，显著提高了测试效率和bug发现能力，对确保求解器正确性贡献突出。

Abstract: Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems
and programming languages research, providing the foundation for tasks like
symbolic execution and automated verification. Because these solvers sit on the
critical path, their correctness is essential, and high-quality test formulas
are key to uncovering bugs. However, while prior testing techniques performed
well on earlier solver versions, they struggle to keep pace with rapidly
evolving features. Recent approaches based on Large Language Models (LLMs) show
promise in exploring advanced solver capabilities, but two obstacles remain:
nearly half of the generated formulas are syntactically invalid, and iterative
interactions with the LLMs introduce substantial computational overhead. In
this study, we present Chimera, a novel LLM-assisted fuzzing framework that
addresses both issues by shifting from direct formula generation to the
synthesis of reusable term (i.e., logical expression) generators. Particularly,
Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for
SMT theories, including solver-specific extensions, from documentation, and (2)
synthesize composable Boolean term generators that adhere to these grammars.
During fuzzing, Chimera populates structural skeletons derived from existing
formulas with the terms iteratively produced by the LLM-synthesized generators.
This design ensures syntactic validity while promoting semantic diversity.
Notably, Chimera requires only one-time LLM interaction investment,
dramatically reducing runtime cost. We evaluated Chimera on two leading SMT
solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43
confirmed bugs, 40 of which have already been fixed by developers.

</details>


### [237] [Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought](https://arxiv.org/abs/2508.20370)
*Lingzhe Zhang,Tong Jia,Kangjin Wang,Weijie Hong,Chiming Duan,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: 微服务系统故障频繁且复杂，现有根因定位方法存在适应性和解释性不足。本文研究SRE的根因定位方式，提出RCLAgent——一个基于多智能体思维递归框架的方法，能够高效准确地定位根因，表现优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现代微服务系统日益普及且复杂，故障发生频率增加，确保系统可靠性迫切需要准确的根因定位。现有根因定位方法存在两大缺陷：过度依赖预定义模式，难以适应不断变化的运行环境；推理过程缺乏解释性，使SRE感到困惑。

Method: 首先，通过对不同组织SRE的综合研究，揭示了人类根因分析的递归性、多维度扩展和跨模态推理三个关键特征。受此启发，提出了RCLAgent，一个针对微服务系统的自适应根因定位方法。该方法利用多智能体思维递归框架，采用新颖的思维递归策略指导大型语言模型进行推理，并有效整合来自多个智能体的数据和工具辅助分析，以准确识别根因。

Result: 在各种公共数据集上的实验评估表明，RCLAgent表现出卓越的性能。它仅使用单个请求即可定位根因，优于那些依赖聚合多个请求的现有SOTA方法。

Conclusion: RCLAgent显著提升了复杂微服务环境中根因定位的效率和精确性。

Abstract: As contemporary microservice systems become increasingly popular and
complex-often comprising hundreds or even thousands of fine-grained,
interdependent subsystems-they are facing more frequent failures. Ensuring
system reliability thus demands accurate root cause localization. While traces
and metrics have proven to be effective data sources for this task, existing
methods either heavily rely on pre-defined schemas, which struggle to adapt to
evolving operational contexts, or lack interpretability in their reasoning
process, thereby leaving Site Reliability Engineers (SREs) confused. In this
paper, we conduct a comprehensive study on how SREs localize the root cause of
failures, drawing insights from multiple professional SREs across different
organizations. Our investigation reveals that human root cause analysis
exhibits three key characteristics: recursiveness, multi-dimensional expansion,
and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,
an adaptive root cause localization method for microservice systems that
leverages a multi-agent recursion-of-thought framework. RCLAgent employs a
novel recursion-of-thought strategy to guide the LLM's reasoning process,
effectively integrating data from multiple agents and tool-assisted analysis to
accurately pinpoint the root cause. Experimental evaluations on various public
datasets demonstrate that RCLAgent achieves superior performance by localizing
the root cause using only a single request-outperforming state-of-the-art
methods that depend on aggregating multiple requests. These results underscore
the effectiveness of RCLAgent in enhancing the efficiency and precision of root
cause localization in complex microservice environments.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [238] [Flexible metadata harvesting for ecology using large language models](https://arxiv.org/abs/2508.20115)
*Zehao Lu,Thijs L van der Plas,Parinaz Rashidi,W Daniel Kissling,Ioannis N Athanasiadis*

Main category: cs.DL

TL;DR: 开发了一种基于大型语言模型（LLM）的元数据提取与链接工具，旨在解决生态及环境数据平台元数据多样性问题，促进数据集的统一管理和互联互通。


<details>
  <summary>Details</summary>
Motivation: 生态研究亟需整合多来源开放数据集以发现新见解，但不同数据提供平台的元数据标准不一，难以高效发现和整合合适的数据集。

Method: 开发了一个基于LLM的元数据收集器，可灵活从数据集页面提取元数据并转换为用户定义的统一格式。该工具利用LLM后处理协议确保结构化和非结构化元数据提取的准确性。此外，通过LLM计算嵌入相似性并统一元数据格式，以识别数据集间的链接。

Result: 该工具能够准确提取结构化和非结构化元数据，并成功利用LLM识别并链接不同数据集之间的关系。

Conclusion: 该工具能够灵活链接不同数据集的元数据，可应用于本体创建或基于图的查询，从而在虚拟研究环境中查找相关的生态和环境数据集，加速生态研究进展。

Abstract: Large, open datasets can accelerate ecological research, particularly by
enabling researchers to develop new insights by reusing datasets from multiple
sources. However, to find the most suitable datasets to combine and integrate,
researchers must navigate diverse ecological and environmental data provider
platforms with varying metadata availability and standards. To overcome this
obstacle, we have developed a large language model (LLM)-based metadata
harvester that flexibly extracts metadata from any dataset's landing page, and
converts these to a user-defined, unified format using existing metadata
standards. We validate that our tool is able to extract both structured and
unstructured metadata with equal accuracy, aided by our LLM post-processing
protocol. Furthermore, we utilise LLMs to identify links between datasets, both
by calculating embedding similarity and by unifying the formats of extracted
metadata to enable rule-based processing. Our tool, which flexibly links the
metadata of different datasets, can therefore be used for ontology creation or
graph-based queries, for example, to find relevant ecological and environmental
datasets in a virtual research environment.

</details>


### [239] [Is Artificial Intelligence Reshaping the Landscape of the International Academic Community of Geosciences?](https://arxiv.org/abs/2508.20117)
*Liang Li,Yuntian Li,Wenxin Zhao,Shan Ye,Yun Lu*

Main category: cs.DL

TL;DR: AI正积极变革地球科学研究，提升了AI相关产出、发展中国家科学家的可见度以及国际合作。


<details>
  <summary>Details</summary>
Motivation: 分析AI对地球科学研究的具体转化作用、趋势及其在国际合作中的影响。

Method: 文献计量分析和主题建模。

Result: AI正积极改变地球科学研究，AI相关科研产出显著增加。发展中国家地球科学家在AI for Science (AI4S) 范式中获得更高可见度，且AI改善了地球科学相关研究的国际合作格局。

Conclusion: AI对地球科学研究具有积极的变革作用，促进了科研产出、发展中国家科学家的参与，并优化了国际合作。

Abstract: Through bibliometric analysis and topic modeling, we find that artificial
intelligence (AI) is positively transforming geosciences research, with a
notable increase in AI-related scientific output in recent years. We are
encouraged to observe that earth scientists from developing countries have
gained better visibility in the recent AI for Science (AI4S) paradigm and that
AI is also improving the landscape of international collaboration in
geoscience-related research.

</details>


### [240] [Leveraging Large Language Models for Generating Research Topic Ontologies: A Multi-Disciplinary Study](https://arxiv.org/abs/2508.20693)
*Tanay Aggarwal,Angelo Salatino,Francesco Osborne,Enrico Motta*

Main category: cs.DL

TL;DR: 本研究评估了大型语言模型（LLMs）识别不同学术领域（生物医学、物理、工程）研究主题语义关系的能力。结果表明，通过新构建的PEM-Rel-8K数据集进行微调的LLMs在所有学科中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 研究领域的本体和分类法对于科学知识管理至关重要，但其创建和维护成本高、耗时长，常导致覆盖不均、领域间连接有限和更新不及时的问题。

Method: 本研究调查了多种大型语言模型在零样本、思维链提示和基于现有本体进行微调三种条件下识别三个学术领域（生物医学、物理学、工程学）内研究主题语义关系的能力。此外，还评估了微调模型的跨领域迁移性。为此，引入了一个包含8000多条关系的PEM-Rel-8K新数据集，数据来源于MeSH、PhySH和IEEE等主流分类法。

Result: 实验结果表明，在PEM-Rel-8K数据集上对大型语言模型进行微调后，模型在所有学科中都表现出卓越的性能。

Conclusion: 微调后的LLMs在识别跨学科研究主题语义关系方面具有出色的能力，为高效创建和维护科研领域的本体与分类法提供了有效途径。

Abstract: Ontologies and taxonomies of research fields are critical for managing and
organising scientific knowledge, as they facilitate efficient classification,
dissemination and retrieval of information. However, the creation and
maintenance of such ontologies are expensive and time-consuming tasks, usually
requiring the coordinated effort of multiple domain experts. Consequently,
ontologies in this space often exhibit uneven coverage across different
disciplines, limited inter-domain connectivity, and infrequent updating cycles.
In this study, we investigate the capability of several large language models
to identify semantic relationships among research topics within three academic
domains: biomedicine, physics, and engineering. The models were evaluated under
three distinct conditions: zero-shot prompting, chain-of-thought prompting, and
fine-tuning on existing ontologies. Additionally, we assessed the cross-domain
transferability of fine-tuned models by measuring their performance when
trained in one domain and subsequently applied to a different one. To support
this analysis, we introduce PEM-Rel-8K, a novel dataset consisting of over
8,000 relationships extracted from the most widely adopted taxonomies in the
three disciplines considered in this study: MeSH, PhySH, and IEEE. Our
experiments demonstrate that fine-tuning LLMs on PEM-Rel-8K yields excellent
performance across all disciplines.

</details>
