{"id": "2507.21372", "pdf": "https://arxiv.org/pdf/2507.21372", "abs": "https://arxiv.org/abs/2507.21372", "authors": ["Sarah McClure", "Sylvia Ratnasamy", "Scott Shenker"], "title": "Load Balancing for AI Training Workloads", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "We investigate the performance of various load balancing algorithms for\nlarge-scale AI training workloads that are running on dedicated infrastructure.\nThe performance of load balancing depends on both the congestion control and\nloss recovery algorithms, so our evaluation also sheds light on the appropriate\nchoices for those designs as well.", "AI": {"tldr": "\u7814\u7a76\u4e13\u7528\u57fa\u7840\u8bbe\u65bd\u4e0a\u5927\u89c4\u6a21AI\u8bad\u7ec3\u8d1f\u8f7d\u7684\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u62e5\u585e\u63a7\u5236\u4e0e\u4e22\u5305\u6062\u590d\u7b97\u6cd5\u7684\u9002\u5b9c\u9009\u62e9\u3002", "motivation": "\u4e86\u89e3\u548c\u4f18\u5316\u5728\u4e13\u7528\u57fa\u7840\u8bbe\u65bd\u4e0a\u8fd0\u884c\u7684\u5927\u89c4\u6a21AI\u8bad\u7ec3\u5de5\u4f5c\u8d1f\u8f7d\u7684\u8d1f\u8f7d\u5747\u8861\u6027\u80fd\uff0c\u540c\u65f6\u4e3a\u76f8\u5173\u7684\u62e5\u585e\u63a7\u5236\u548c\u4e22\u5305\u6062\u590d\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u9009\u62e9\u4f9d\u636e\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30\u5404\u79cd\u8d1f\u8f7d\u5747\u8861\u7b97\u6cd5\u7684\u6027\u80fd\u6765\u8c03\u67e5\u5176\u8868\u73b0\uff0c\u5e76\u5206\u6790\u62e5\u585e\u63a7\u5236\u548c\u4e22\u5305\u6062\u590d\u7b97\u6cd5\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u62bd\u8c61\u4e2d\u672a\u76f4\u63a5\u7ed9\u51fa\u5177\u4f53\u7814\u7a76\u7ed3\u679c\uff0c\u4ec5\u8bf4\u660e\u7814\u7a76\u65e8\u5728\u63ed\u793a\u6027\u80fd\u8868\u73b0\u5e76\u63d0\u4f9b\u8bbe\u8ba1\u9009\u62e9\u7684\u6d1e\u5bdf\u3002", "conclusion": "\u62bd\u8c61\u4e2d\u672a\u76f4\u63a5\u7ed9\u51fa\u660e\u786e\u7ed3\u8bba\uff0c\u4f46\u7814\u7a76\u76ee\u6807\u662f\u4e3a\u5927\u89c4\u6a21AI\u8bad\u7ec3\u73af\u5883\u4e2d\u7684\u8d1f\u8f7d\u5747\u8861\u3001\u62e5\u585e\u63a7\u5236\u548c\u4e22\u5305\u6062\u590d\u7b97\u6cd5\u7684\u9009\u62e9\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2507.21385", "pdf": "https://arxiv.org/pdf/2507.21385", "abs": "https://arxiv.org/abs/2507.21385", "authors": ["Wei Mao", "Lili Wei", "Omid Semiari", "Shu-ping Yeh", "Hosein Nikopour"], "title": "Deep Reinforcement Learning-based Cell DTX/DRX Configuration for Network Energy Saving", "categories": ["cs.NI", "cs.AI"], "comment": "7 pages, 7 figures", "summary": "3GPP Release 18 cell discontinuous transmission and reception (cell DTX/DRX)\nis an important new network energy saving feature for 5G. As a time-domain\ntechnique, it periodically aggregates the user data transmissions in a given\nduration of time when the traffic load is not heavy, so that the remaining time\ncan be kept silent and advanced sleep modes (ASM) can be enabled to shut down\nmore radio components and save more energy for the cell. However, inevitably\nthe packet delay is increased, as during the silent period no transmission is\nallowed. In this paper we study how to configure cell DTX/DRX to optimally\nbalance energy saving and packet delay, so that for delay-sensitive traffic\nmaximum energy saving can be achieved while the degradation of quality of\nservice (QoS) is minimized. As the optimal configuration can be different for\ndifferent network and traffic conditions, the problem is complex and we resort\nto deep reinforcement learning (DRL) framework to train an AI agent to solve\nit. Through careful design of 1) the learning algorithm, which implements a\ndeep Q-network (DQN) on a contextual bandit (CB) model, and 2) the reward\nfunction, which utilizes a smooth approximation of a theoretically optimal but\ndiscontinuous reward function, we are able to train an AI agent that always\ntries to select the best possible Cell DTX/DRX configuration under any network\nand traffic conditions. Simulation results show that compared to the case when\ncell DTX/DRX is not used, our agent can achieve up to ~45% energy saving\ndepending on the traffic load scenario, while always maintaining no more than\n~1% QoS degradation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u667a\u80fd\u4f53\uff0c\u901a\u8fc7DQN\u5728\u4e0a\u4e0b\u6587\u5f3a\u76d7\u6a21\u578b\u4e0a\u8fd0\u884c\uff0c\u5e76\u4f18\u5316\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u57285G\u8702\u7a9dDTX/DRX\u4e2d\u5e73\u8861\u8282\u80fd\u4e0e\u6570\u636e\u5ef6\u8fdf\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u4f4eQoS\u4e0b\u964d\u7684\u540c\u65f6\uff0c\u53ef\u5b9e\u73b0\u9ad8\u8fbe45%\u7684\u8282\u80fd\u3002", "motivation": "3GPP Release 18\u8702\u7a9dDTX/DRX\u662f5G\u91cd\u8981\u7684\u7f51\u7edc\u8282\u80fd\u7279\u6027\uff0c\u901a\u8fc7\u5468\u671f\u6027\u805a\u5408\u6570\u636e\u5b9e\u73b0\u9759\u9ed8\u671f\u4ee5\u5f00\u542f\u9ad8\u7ea7\u7761\u7720\u6a21\u5f0f\u3002\u7136\u800c\uff0c\u9759\u9ed8\u671f\u4e0d\u53ef\u907f\u514d\u5730\u589e\u52a0\u4e86\u6570\u636e\u5305\u5ef6\u8fdf\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\uff0c\u5982\u4f55\u5728\u4e0d\u540c\u7684\u7f51\u7edc\u548c\u6d41\u91cf\u6761\u4ef6\u4e0b\uff0c\u4e3a\u5ef6\u8fdf\u654f\u611f\u578b\u4e1a\u52a1\u6700\u4f18\u914d\u7f6eDTX/DRX\uff0c\u4ee5\u5728\u6700\u5c0f\u5316\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u4e0b\u964d\u7684\u540c\u65f6\uff0c\u6700\u5927\u5316\u80fd\u91cf\u8282\u7701\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6846\u67b6\u6765\u8bad\u7ec3\u4e00\u4e2aAI\u667a\u80fd\u4f53\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5b66\u4e60\u7b97\u6cd5\u8bbe\u8ba1\u4e3a\u5728\u4e0a\u4e0b\u6587\u5f3a\u76d7\uff08Contextual Bandit, CB\uff09\u6a21\u578b\u4e0a\u5b9e\u73b0\u7684\u6df1\u5ea6Q\u7f51\u7edc\uff08Deep Q-network, DQN\uff09\uff1b2) \u5956\u52b1\u51fd\u6570\u5229\u7528\u7406\u8bba\u6700\u4f18\u4f46\u975e\u8fde\u7eed\u5956\u52b1\u51fd\u6570\u7684\u5e73\u6ed1\u8fd1\u4f3c\u3002\u8fd9\u4e9b\u8bbe\u8ba1\u65e8\u5728\u4f7fAI\u667a\u80fd\u4f53\u80fd\u591f\u59cb\u7ec8\u9009\u62e9\u4efb\u4f55\u7f51\u7edc\u548c\u6d41\u91cf\u6761\u4ef6\u4e0b\u7684\u6700\u4f73Cell DTX/DRX\u914d\u7f6e\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4e0d\u4f7f\u7528Cell DTX/DRX\u7684\u60c5\u51b5\u76f8\u6bd4\uff0c\u672c\u6587\u63d0\u51fa\u7684AI\u667a\u80fd\u4f53\u80fd\u591f\u5b9e\u73b0\u9ad8\u8fbe\u7ea645%\u7684\u80fd\u91cf\u8282\u7701\uff08\u53d6\u51b3\u4e8e\u6d41\u91cf\u8d1f\u8f7d\u573a\u666f\uff09\uff0c\u540c\u65f6\u59cb\u7ec8\u5c06\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u7684\u4e0b\u964d\u63a7\u5236\u5728\u4e0d\u8d85\u8fc7\u7ea61%\u7684\u8303\u56f4\u5185\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5730\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u4e865G\u8702\u7a9dDTX/DRX\u7684\u914d\u7f6e\u4f18\u5316\u95ee\u9898\uff0c\u6709\u6548\u5e73\u8861\u4e86\u80fd\u91cf\u8282\u7701\u4e0e\u6570\u636e\u5305\u5ef6\u8fdf\u3002\u6240\u8bad\u7ec3\u7684AI\u667a\u80fd\u4f53\u80fd\u591f\u5728\u5404\u79cd\u7f51\u7edc\u548c\u6d41\u91cf\u6761\u4ef6\u4e0b\uff0c\u5728\u4fdd\u6301\u6781\u4f4eQoS\u4e0b\u964d\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u7684\u80fd\u91cf\u6548\u7387\u63d0\u5347\u3002"}}
{"id": "2507.21728", "pdf": "https://arxiv.org/pdf/2507.21728", "abs": "https://arxiv.org/abs/2507.21728", "authors": ["Agastya Raj", "Zehao Wang", "Tingjun Chen", "Daniel C Kilper", "Marco Ruffini"], "title": "Generalized few-shot transfer learning architecture for modeling the EDFA gain spectrum", "categories": ["cs.NI", "cs.LG"], "comment": "This is a preprint of a paper accepted and published in the Journal\n  of Optical Communications and Networking (JOCN). The final published version\n  is available at: https://doi.org/10.1364/JOCN.560987", "summary": "Accurate modeling of the gain spectrum in Erbium-Doped Fiber Amplifiers\n(EDFAs) is essential for optimizing optical network performance, particularly\nas networks evolve toward multi-vendor solutions. In this work, we propose a\ngeneralized few-shot transfer learning architecture based on a Semi-Supervised\nSelf-Normalizing Neural Network (SS-NN) that leverages internal EDFA features -\nsuch as VOA input or output power and attenuation, to improve gain spectrum\nprediction. Our SS-NN model employs a two-phase training strategy comprising\nunsupervised pre-training with noise-augmented measurements and supervised\nfine-tuning with a custom weighted MSE loss. Furthermore, we extend the\nframework with transfer learning (TL) techniques that enable both homogeneous\n(same-feature space) and heterogeneous (different-feature sets) model\nadaptation across booster, preamplifier, and ILA EDFAs. To address feature\nmismatches in heterogeneous TL, we incorporate a covariance matching loss to\nalign second-order feature statistics between source and target domains.\nExtensive experiments conducted across 26 EDFAs in the COSMOS and Open Ireland\ntestbeds demonstrate that the proposed approach significantly reduces the\nnumber of measurements requirements on the system while achieving lower mean\nabsolute errors and improved error distributions compared to benchmark methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u534a\u76d1\u7763\u81ea\u5f52\u4e00\u5316\u795e\u7ecf\u7f51\u7edc\uff08SS-NN\uff09\u7684\u5e7f\u4e49\u5c11\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u67b6\u6784\uff0c\u5229\u7528EDFA\u5185\u90e8\u7279\u5f81\u9884\u6d4b\u5176\u589e\u76ca\u8c31\uff0c\u663e\u8457\u51cf\u5c11\u6d4b\u91cf\u9700\u6c42\u5e76\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u7cbe\u786e\u5efa\u6a21\u63ba\u94d2\u5149\u7ea4\u653e\u5927\u5668\uff08EDFA\uff09\u7684\u589e\u76ca\u8c31\u5bf9\u4e8e\u4f18\u5316\u5149\u7f51\u7edc\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5149\u7f51\u7edc\u5411\u591a\u5382\u5546\u89e3\u51b3\u65b9\u6848\u6f14\u8fdb\u7684\u80cc\u666f\u4e0b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u534a\u76d1\u7763\u81ea\u5f52\u4e00\u5316\u795e\u7ecf\u7f51\u7edc\uff08SS-NN\uff09\u7684\u5e7f\u4e49\u5c11\u6837\u672c\u8fc1\u79fb\u5b66\u4e60\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5229\u7528EDFA\u5185\u90e8\u7279\u5f81\uff08\u5982VOA\u8f93\u5165/\u8f93\u51fa\u529f\u7387\u548c\u8870\u51cf\uff09\u6765\u6539\u8fdb\u589e\u76ca\u8c31\u9884\u6d4b\u3002SS-NN\u6a21\u578b\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u65e0\u76d1\u7763\u566a\u58f0\u589e\u5f3a\u9884\u8bad\u7ec3\u548c\u5e26\u81ea\u5b9a\u4e49\u52a0\u6743MSE\u635f\u5931\u7684\u76d1\u7763\u5fae\u8c03\u3002\u6b64\u5916\uff0c\u6846\u67b6\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\uff08TL\uff09\u6280\u672f\u6269\u5c55\uff0c\u652f\u6301\u540c\u6784\u548c\u5f02\u6784\u6a21\u578b\u5728\u4e0d\u540c\u7c7b\u578bEDFA\uff08\u5982\u589e\u76ca\u653e\u5927\u5668\u3001\u524d\u7f6e\u653e\u5927\u5668\u548c\u7ebf\u8def\u653e\u5927\u5668\uff09\u95f4\u7684\u9002\u5e94\u3002\u4e3a\u89e3\u51b3\u5f02\u6784\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u7279\u5f81\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5f15\u5165\u534f\u65b9\u5dee\u5339\u914d\u635f\u5931\u6765\u5bf9\u9f50\u6e90\u57df\u548c\u76ee\u6807\u57df\u7684\u4e8c\u9636\u7279\u5f81\u7edf\u8ba1\u3002", "result": "\u5728COSMOS\u548cOpen Ireland\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u768426\u4e2aEDFA\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u7cfb\u7edf\u6240\u9700\u7684\u6d4b\u91cf\u6570\u91cf\uff0c\u5e76\u4e14\u4e0e\u57fa\u51c6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u548c\u6539\u8fdb\u7684\u8bef\u5dee\u5206\u5e03\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eSS-NN\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4e14\u9ad8\u6548\u5730\u9884\u6d4bEDFA\u589e\u76ca\u8c31\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6570\u636e\u6d4b\u91cf\u9700\u6c42\uff0c\u5e76\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5bf9\u4e8e\u4f18\u5316\u5149\u7f51\u7edc\u6027\u80fd\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.21739", "pdf": "https://arxiv.org/pdf/2507.21739", "abs": "https://arxiv.org/abs/2507.21739", "authors": ["Zekai Sun", "Xiuxian Guan", "Zheng Lin", "Yuhao Qing", "Haoze Song", "Zihan Fang", "Zhe Chen", "Fangming Liu", "Heming Cui", "Wei Ni", "Jun Luo"], "title": "RRTO: A High-Performance Transparent Offloading System for Model Inference in Mobile Edge Computing", "categories": ["cs.NI"], "comment": "15 pages, 12 figures,", "summary": "Deploying Machine Learning (ML) applications on resource-constrained mobile\ndevices remains challenging due to limited computational resources and poor\nplatform compatibility. While Mobile Edge Computing (MEC) offers\noffloading-based inference paradigm using GPU servers, existing approaches are\ndivided into non-transparent and transparent methods, with the latter\nnecessitating modifications to the source code. Non-transparent offloading\nachieves high performance but requires intrusive code modification, limiting\ncompatibility with diverse applications. Transparent offloading, in contrast,\noffers wide compatibility but introduces significant transmission delays due to\nper-operator remote procedure calls (RPCs). To overcome this limitation, we\npropose RRTO, the first high-performance transparent offloading system tailored\nfor MEC inference. RRTO introduces a record/replay mechanism that leverages the\nstatic operator sequence in ML models to eliminate repetitive RPCs. To reliably\nidentify this sequence, RRTO integrates a novel Operator Sequence Search\nalgorithm that detects repeated patterns, filters initialization noise, and\naccelerates matching via a two-level strategy. Evaluation demonstrates that\nRRTO achieves substantial reductions of up to 98% in both per-inference latency\nand energy consumption compared to state-of-the-art transparent methods and\nyields results comparable to non-transparent approaches, all without\nnecessitating any source code modification.", "AI": {"tldr": "\u63d0\u51faRRTO\uff0c\u4e00\u79cd\u9ad8\u6027\u80fd\u900f\u660e\u5378\u8f7d\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bb0\u5f55/\u56de\u653e\u673a\u5236\u6d88\u9664\u91cd\u590dRPCs\uff0c\u663e\u8457\u964d\u4f4e\u79fb\u52a8\u8bbe\u5907ML\u63a8\u7406\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u540c\u65f6\u65e0\u9700\u4fee\u6539\u6e90\u7801\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72ML\u5e94\u7528\u9762\u4e34\u8ba1\u7b97\u8d44\u6e90\u548c\u517c\u5bb9\u6027\u6311\u6218\u3002MEC\u5378\u8f7d\u5b58\u5728\u4e24\u7c7b\u65b9\u6cd5\uff1a\u975e\u900f\u660e\u65b9\u6cd5\u6027\u80fd\u9ad8\u4f46\u9700\u4fee\u6539\u6e90\u7801\uff0c\u517c\u5bb9\u6027\u5dee\uff1b\u900f\u660e\u65b9\u6cd5\u517c\u5bb9\u6027\u597d\u4f46\u56e0\u6bcf\u6b21\u64cd\u4f5c\u7b26RPC\u5bfc\u81f4\u4f20\u8f93\u5ef6\u8fdf\u5927\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u900f\u660e\u5378\u8f7d\u7684\u4f20\u8f93\u5ef6\u8fdf\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u517c\u5bb9\u6027\u4f18\u52bf\u3002", "method": "\u63d0\u51faRRTO\u7cfb\u7edf\uff0c\u5f15\u5165\u8bb0\u5f55/\u56de\u653e\u673a\u5236\uff0c\u5229\u7528ML\u6a21\u578b\u4e2d\u9759\u6001\u64cd\u4f5c\u7b26\u5e8f\u5217\u6765\u6d88\u9664\u91cd\u590d\u7684RPCs\u3002\u4e3a\u53ef\u9760\u8bc6\u522b\u6b64\u5e8f\u5217\uff0cRRTO\u96c6\u6210\u4e86\u65b0\u9896\u7684\u64cd\u4f5c\u7b26\u5e8f\u5217\u641c\u7d22\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u68c0\u6d4b\u91cd\u590d\u6a21\u5f0f\u3001\u8fc7\u6ee4\u521d\u59cb\u5316\u566a\u58f0\u5e76\u901a\u8fc7\u53cc\u5c42\u7b56\u7565\u52a0\u901f\u5339\u914d\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u900f\u660e\u65b9\u6cd5\u76f8\u6bd4\uff0cRRTO\u5c06\u6bcf\u6b21\u63a8\u7406\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\u5747\u5927\u5e45\u964d\u4f4e\u9ad8\u8fbe98%\u3002\u5176\u6027\u80fd\u53ef\u4e0e\u975e\u900f\u660e\u65b9\u6cd5\u5ab2\u7f8e\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u6e90\u4ee3\u7801\u4fee\u6539\u3002", "conclusion": "RRTO\u5728\u4e0d\u727a\u7272\u517c\u5bb9\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u4e2dML\u63a8\u7406\u900f\u660e\u5378\u8f7d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u548c\u80fd\u6548\uff0c\u4e3a\u79fb\u52a8\u8bbe\u5907\u4e0aML\u5e94\u7528\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.21058", "pdf": "https://arxiv.org/pdf/2507.21058", "abs": "https://arxiv.org/abs/2507.21058", "authors": ["Kerem Keskin", "M\u00fcmine Kaya Kele\u015f"], "title": "Categorical Classification of Book Summaries Using Word Embedding Techniques", "categories": ["cs.CL", "cs.AI"], "comment": "in Turkish language. This paper was published in the proceedings of\n  the 6th International Conference on Data Science and Applications ICONDATA24,\n  held on September between 2 and 6, 2024, in Pristina, Kosovo. For full text\n  book see https://www.icondata.org/en/proceedings-books", "summary": "In this study, book summaries and categories taken from book sites were\nclassified using word embedding methods, natural language processing techniques\nand machine learning algorithms. In addition, one hot encoding, Word2Vec and\nTerm Frequency - Inverse Document Frequency (TF-IDF) methods, which are\nfrequently used word embedding methods were used in this study and their\nsuccess was compared. Additionally, the combination table of the pre-processing\nmethods used is shown and added to the table. Looking at the results, it was\nobserved that Support Vector Machine, Naive Bayes and Logistic Regression\nModels and TF-IDF and One-Hot Encoder word embedding techniques gave more\nsuccessful results for Turkish texts.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u8bcd\u5d4c\u5165\u3001NLP\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5bf9\u56fe\u4e66\u6458\u8981\u548c\u7c7b\u522b\u8fdb\u884c\u5206\u7c7b\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5bf9\u4e8e\u571f\u8033\u5176\u6587\u672c\uff0c\u652f\u6301\u5411\u91cf\u673a\u3001\u6734\u7d20\u8d1d\u53f6\u65af\u548c\u903b\u8f91\u56de\u5f52\u6a21\u578b\u7ed3\u5408TF-IDF\u548cOne-Hot Encoder\u8bcd\u5d4c\u5165\u6280\u672f\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u8bba\u6587\u65e8\u5728\u5229\u7528\u8bcd\u5d4c\u5165\u65b9\u6cd5\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5bf9\u56fe\u4e66\u6458\u8981\u548c\u7c7b\u522b\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u8bcd\u5d4c\u5165\u65b9\u6cd5\u7684\u5206\u7c7b\u6548\u679c\u3002", "method": "\u7814\u7a76\u91c7\u7528\u8bcd\u5d4c\u5165\u65b9\u6cd5\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u5206\u7c7b\u3002\u5177\u4f53\u4f7f\u7528\u7684\u8bcd\u5d4c\u5165\u65b9\u6cd5\u5305\u62ecOne-hot encoding\u3001Word2Vec\u548cTF-IDF\uff0c\u5e76\u5bf9\u5b83\u4eec\u7684\u6210\u529f\u7387\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u8fd8\u5c55\u793a\u4e86\u6240\u7528\u9884\u5904\u7406\u65b9\u6cd5\u7684\u7ec4\u5408\u8868\u683c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u571f\u8033\u5176\u6587\u672c\uff0c\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u3001\u6734\u7d20\u8d1d\u53f6\u65af\uff08Naive Bayes\uff09\u548c\u903b\u8f91\u56de\u5f52\uff08Logistic Regression\uff09\u6a21\u578b\uff0c\u7ed3\u5408TF-IDF\u548cOne-Hot Encoder\u8bcd\u5d4c\u5165\u6280\u672f\uff0c\u53d6\u5f97\u4e86\u66f4\u6210\u529f\u7684\u5206\u7c7b\u7ed3\u679c\u3002", "conclusion": "TF-IDF\u548cOne-Hot Encoder\u8bcd\u5d4c\u5165\u6280\u672f\u4e0e\u652f\u6301\u5411\u91cf\u673a\u3001\u6734\u7d20\u8d1d\u53f6\u65af\u548c\u903b\u8f91\u56de\u5f52\u6a21\u578b\u7ed3\u5408\uff0c\u662f\u6709\u6548\u5206\u7c7b\u571f\u8033\u5176\u8bed\u56fe\u4e66\u6458\u8981\u548c\u7c7b\u522b\u7684\u6210\u529f\u65b9\u6cd5\u3002"}}
{"id": "2507.21109", "pdf": "https://arxiv.org/pdf/2507.21109", "abs": "https://arxiv.org/abs/2507.21109", "authors": ["Prital Bamnodkar"], "title": "Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep Neural Networks often suffer from a critical limitation known as\nCatastrophic Forgetting, where performance on past tasks degrades after\nlearning new ones. This paper introduces a novel continual learning approach\ninspired by human learning strategies like Active Recall, Deliberate Practice\nand Spaced Repetition, named Task Focused Consolidation with Spaced Recall\n(TFC-SR). TFC-SR enhances the standard experience replay with a mechanism we\ntermed the Active Recall Probe. It is a periodic, task-aware evaluation of the\nmodel's memory that stabilizes the representations of past knowledge. We test\nTFC-SR on the Split MNIST and Split CIFAR-100 benchmarks against leading\nregularization-based and replay-based baselines. Our results show that TFC-SR\nperforms significantly better than these methods. For instance, on the Split\nCIFAR-100, it achieves a final accuracy of 13.17% compared to standard replay's\n7.40%. We demonstrate that this advantage comes from the stabilizing effect of\nthe probe itself, and not from the difference in replay volume. Additionally,\nwe analyze the trade-off between memory size and performance and show that\nwhile TFC-SR performs better in memory-constrained environments, higher replay\nvolume is still more effective when available memory is abundant. We conclude\nthat TFC-SR is a robust and efficient approach, highlighting the importance of\nintegrating active memory retrieval mechanisms into continual learning systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTFC-SR\uff0c\u4e00\u79cd\u53d7\u4eba\u7c7b\u5b66\u4e60\u7b56\u7565\u542f\u53d1\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u201c\u4e3b\u52a8\u56de\u5fc6\u63a2\u9488\u201d\u6765\u7a33\u5b9a\u8fc7\u5f80\u77e5\u8bc6\uff0c\u6709\u6548\u7f13\u89e3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\uff0c\u5f80\u5f80\u4f1a\u906d\u9047\u201c\u707e\u96be\u6027\u9057\u5fd8\u201d\uff0c\u5bfc\u81f4\u5bf9\u8fc7\u53bb\u4efb\u52a1\u7684\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u201cTask Focused Consolidation with Spaced Recall (TFC-SR)\u201d\u7684\u65b0\u578b\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u53d7\u4eba\u7c7b\u4e3b\u52a8\u56de\u5fc6\u3001\u523b\u610f\u7ec3\u4e60\u548c\u95f4\u9694\u91cd\u590d\u7b49\u5b66\u4e60\u7b56\u7565\u542f\u53d1\uff0c\u901a\u8fc7\u5f15\u5165\u201c\u4e3b\u52a8\u56de\u5fc6\u63a2\u9488\u201d\uff08Active Recall Probe\uff09\u6765\u589e\u5f3a\u6807\u51c6\u7ecf\u9a8c\u56de\u653e\u673a\u5236\u3002\u6b64\u63a2\u9488\u80fd\u5468\u671f\u6027\u5730\u3001\u4efb\u52a1\u611f\u77e5\u5730\u8bc4\u4f30\u6a21\u578b\u8bb0\u5fc6\uff0c\u4ece\u800c\u7a33\u5b9a\u8fc7\u5f80\u77e5\u8bc6\u7684\u8868\u793a\u3002", "result": "TFC-SR\u5728Split MNIST\u548cSplit CIFAR-100\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u4e3b\u6d41\u7684\u57fa\u4e8e\u6b63\u5219\u5316\u548c\u57fa\u4e8e\u56de\u653e\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u5728Split CIFAR-100\u4e0a\uff0c\u5176\u6700\u7ec8\u51c6\u786e\u7387\u8fbe\u523013.17%\uff0c\u8fdc\u9ad8\u4e8e\u6807\u51c6\u56de\u653e\u76847.40%\u3002\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u79cd\u4f18\u52bf\u6e90\u4e8e\u63a2\u9488\u672c\u8eab\u7684\u7a33\u5b9a\u4f5c\u7528\uff0c\u800c\u975e\u56de\u653e\u91cf\u7684\u5dee\u5f02\u3002\u6b64\u5916\uff0cTFC-SR\u5728\u5185\u5b58\u53d7\u9650\u73af\u5883\u4e0b\u8868\u73b0\u66f4\u4f73\uff0c\u4f46\u5728\u5185\u5b58\u5145\u8db3\u65f6\uff0c\u66f4\u5927\u7684\u56de\u653e\u91cf\u4ecd\u66f4\u6709\u6548\u3002", "conclusion": "TFC-SR\u662f\u4e00\u79cd\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u51f8\u663e\u4e86\u5c06\u4e3b\u52a8\u8bb0\u5fc6\u68c0\u7d22\u673a\u5236\u6574\u5408\u5230\u6301\u7eed\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.21067", "pdf": "https://arxiv.org/pdf/2507.21067", "abs": "https://arxiv.org/abs/2507.21067", "authors": ["Jan Kapusta"], "title": "SynLang and Symbiotic Epistemology: A Manifesto for Conscious Human-AI Collaboration", "categories": ["cs.AI", "cs.CY", "cs.HC"], "comment": "32 pages, 4 figures. Includes 2 Appendices containing SynLang v1.2.0\n  protocol specification, and formal BNF grammar", "summary": "Current AI systems rely on opaque reasoning processes that hinder human\noversight and collaborative potential. Conventional explainable AI approaches\noffer post-hoc justifications and often fail to establish genuine symbiotic\ncollaboration. In this paper, the Symbiotic Epistemology is presented as a\nphilosophical foundation for human-AI cognitive partnerships. Unlike frameworks\nthat treat AI as a mere tool or replacement, symbiotic epistemology positions\nAI as a reasoning partner, fostering calibrated trust by aligning human\nconfidence with AI reliability through explicit reasoning patterns and\nconfidence assessments. SynLang (Symbiotic Syntactic Language) is introduced as\na formal protocol for transparent human-AI collaboration. The framework is\nempirically validated through actual human-AI dialogues demonstrating AI's\nadaptation to structured reasoning protocols and successful metacognitive\nintervention. The protocol defines two complementary mechanisms: TRACE for\nhigh-level reasoning patterns and TRACE_FE for detailed factor explanations. It\nalso integrates confidence quantification, declarative control over AI\nbehavior, and context inheritance for multi-agent coordination. By structuring\ncommunication and embedding confidence-calibrated transparency, SynLang,\ntogether with symbiotic epistemology, enables AI systems that enhance human\nintelligence, preserve human agency, and uphold ethical accountability in\ncollaborative decision-making. Through dual-level transparency, beginning with\nhigh-level reasoning patterns and progressing to granular explanations, the\nprotocol facilitates rapid comprehension and supports thorough verification of\nAI decision-making.", "AI": {"tldr": "\u63d0\u51fa\u5171\u751f\u8ba4\u77e5\u8bba\u4f5c\u4e3a\u4eba\u673a\u8ba4\u77e5\u4f19\u4f34\u7684\u54f2\u5b66\u57fa\u7840\uff0c\u5e76\u5f15\u5165SynLang\u534f\u8bae\uff0c\u901a\u8fc7\u53cc\u5c42\u900f\u660e\u63a8\u7406\u6a21\u5f0f\u548c\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\uff0c\u4fc3\u8fdb\u4eba\u673a\u4e4b\u95f4\u6821\u51c6\u4fe1\u4efb\u548c\u6df1\u5ea6\u534f\u4f5c\uff0c\u65e8\u5728\u589e\u5f3a\u4eba\u7c7b\u667a\u80fd\u5e76\u786e\u4fdd\u4f26\u7406\u8d23\u4efb\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u63a8\u7406\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u963b\u788d\u4eba\u7c7b\u76d1\u7763\u548c\u534f\u4f5c\u6f5c\u529b\u3002\u73b0\u6709\u53ef\u89e3\u91caAI\uff08XAI\uff09\u65b9\u6cd5\u591a\u4e3a\u4e8b\u540e\u89e3\u91ca\uff0c\u672a\u80fd\u5efa\u7acb\u771f\u6b63\u5171\u751f\u7684\u4eba\u673a\u534f\u4f5c\u5173\u7cfb\u3002", "method": "1. \u63d0\u51fa\u201c\u5171\u751f\u8ba4\u77e5\u8bba\u201d\u4f5c\u4e3a\u4eba\u673a\u8ba4\u77e5\u4f19\u4f34\u5173\u7cfb\u7684\u54f2\u5b66\u57fa\u7840\uff0c\u5c06AI\u5b9a\u4f4d\u4e3a\u63a8\u7406\u4f19\u4f34\u30022. \u5f15\u5165\u201cSynLang\uff08\u5171\u751f\u53e5\u6cd5\u8bed\u8a00\uff09\u201d\u4f5c\u4e3a\u900f\u660e\u4eba\u673a\u534f\u4f5c\u7684\u6b63\u5f0f\u534f\u8bae\uff0c\u5305\u542bTRACE\uff08\u9ad8\u5c42\u63a8\u7406\u6a21\u5f0f\uff09\u548cTRACE_FE\uff08\u8be6\u7ec6\u56e0\u7d20\u89e3\u91ca\uff09\u4e24\u79cd\u4e92\u8865\u673a\u5236\uff0c\u5e76\u6574\u5408\u7f6e\u4fe1\u5ea6\u91cf\u5316\u3001\u58f0\u660e\u5f0f\u63a7\u5236\u548c\u4e0a\u4e0b\u6587\u7ee7\u627f\u30023. \u901a\u8fc7\u771f\u5b9e\u4eba\u673a\u5bf9\u8bdd\u5bf9\u8be5\u6846\u67b6\u8fdb\u884c\u7ecf\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7ecf\u9a8c\u9a8c\u8bc1\u8868\u660eAI\u80fd\u591f\u9002\u5e94\u7ed3\u6784\u5316\u63a8\u7406\u534f\u8bae\u5e76\u6210\u529f\u8fdb\u884c\u5143\u8ba4\u77e5\u5e72\u9884\u3002SynLang\u534f\u8bae\u901a\u8fc7\u7ed3\u6784\u5316\u901a\u4fe1\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u900f\u660e\u5ea6\uff0c\u4f7fAI\u7cfb\u7edf\u80fd\u591f\u589e\u5f3a\u4eba\u7c7b\u667a\u80fd\u3001\u4fdd\u7559\u4eba\u7c7b\u80fd\u52a8\u6027\u5e76\u7ef4\u62a4\u534f\u4f5c\u51b3\u7b56\u4e2d\u7684\u4f26\u7406\u8d23\u4efb\u3002\u53cc\u5c42\u900f\u660e\u5ea6\uff08\u4ece\u9ad8\u5c42\u5230\u7ec6\u8282\u89e3\u91ca\uff09\u4fc3\u8fdb\u4e86\u5feb\u901f\u7406\u89e3\u548c\u5f7b\u5e95\u9a8c\u8bc1AI\u51b3\u7b56\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5171\u751f\u8ba4\u77e5\u8bba\u548cSynLang\u534f\u8bae\uff0c\u4e3a\u5b9e\u73b0\u900f\u660e\u3001\u53ef\u4fe1\u8d56\u7684\u4eba\u673a\u8ba4\u77e5\u4f19\u4f34\u5173\u7cfb\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u63d0\u5347\u4eba\u7c7b\u667a\u80fd\u3001\u4fdd\u7559\u4eba\u7c7b\u4e3b\u5bfc\u6743\u5e76\u786e\u4fdd\u534f\u4f5c\u51b3\u7b56\u7684\u4f26\u7406\u8d23\u4efb\u548c\u95ee\u8d23\u5236\u3002"}}
{"id": "2507.21069", "pdf": "https://arxiv.org/pdf/2507.21069", "abs": "https://arxiv.org/abs/2507.21069", "authors": ["Andreas Spilz", "Heiko Oppel", "Jochen Werner", "Kathrin Stucke-Straub", "Felix Capanni", "Michael Munz"], "title": "GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Wearable inertial measurement units (IMUs) offer a cost-effective and\nscalable means to assess human movement quality in clinical and everyday\nsettings. However, the development of robust sensor-based classification models\nfor physiotherapeutic exercises and gait analysis requires large, diverse\ndatasets, which are costly and time-consuming to collect. Here, we present a\nmultimodal dataset of physiotherapeutic exercises - including correct and\nclinically relevant variants - and gait-related exercises - including both\nnormal and impaired gait patterns - recorded from 19 participants using\nsynchronized IMUs and marker-based motion capture (MoCap). The dataset includes\nraw data from nine IMUs and thirty-five optical markers capturing full-body\nkinematics. Each IMU is additionally equipped with four optical markers,\nenabling precise comparison between IMU-derived orientation estimates and\nreference values from the MoCap system. To support further analysis, we also\nprovide processed IMU orientations aligned with common segment coordinate\nsystems, subject-specific OpenSim models, inverse kinematics results, and tools\nfor visualizing IMU orientations in the musculoskeletal context. Detailed\nannotations of movement execution quality and time-stamped segmentations\nsupport diverse analysis goals. This dataset supports the development and\nbenchmarking of machine learning models for tasks such as automatic exercise\nevaluation, gait analysis, temporal activity segmentation, and biomechanical\nparameter estimation. To facilitate reproducibility, we provide code for\npostprocessing, sensor-to-segment alignment, inverse kinematics computation,\nand technical validation. This resource is intended to accelerate research in\nmachine learning-driven human movement analysis.", "AI": {"tldr": "\u4e00\u4e2a\u5305\u542b\u7269\u7406\u6cbb\u7597\u8fd0\u52a8\u548c\u6b65\u6001\u6570\u636e\u7684\u591a\u6a21\u6001\uff08IMU+MoCap\uff09\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u6b63\u5e38\u53ca\u5f02\u5e38\u6a21\u5f0f\uff0c\u65e8\u5728\u52a0\u901f\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u4eba\u4f53\u8fd0\u52a8\u5206\u6790\u7814\u7a76\u3002", "motivation": "\u53ef\u7a7f\u6234\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u5728\u4eba\u4f53\u8fd0\u52a8\u8d28\u91cf\u8bc4\u4f30\u4e2d\u5177\u6709\u6210\u672c\u6548\u76ca\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u5f00\u53d1\u9c81\u68d2\u7684\u57fa\u4e8e\u4f20\u611f\u5668\u7684\u8fd0\u52a8\u5206\u7c7b\u6a21\u578b\u9700\u8981\u5927\u91cf\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u800c\u6b64\u7c7b\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\u3002", "method": "\u672c\u7814\u7a76\u4ece19\u540d\u53c2\u4e0e\u8005\u5904\u6536\u96c6\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b\u540c\u6b65\u7684IMU\u548c\u5149\u5b66\u6807\u8bb0\u8fd0\u52a8\u6355\u6349\uff08MoCap\uff09\u6570\u636e\u3002\u6570\u636e\u96c6\u5305\u62ec9\u4e2aIMU\u7684\u539f\u59cb\u6570\u636e\u548c35\u4e2a\u5149\u5b66\u6807\u8bb0\u7684\u5168\u8eab\u4f53\u8fd0\u52a8\u5b66\u6570\u636e\u3002\u4e3a\u652f\u6301\u8fdb\u4e00\u6b65\u5206\u6790\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5904\u7406\u540e\u7684IMU\u65b9\u5411\u3001\u4e3b\u4f53\u7279\u5f02\u6027OpenSim\u6a21\u578b\u3001\u9006\u8fd0\u52a8\u5b66\u7ed3\u679c\u3001\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u4ee5\u53ca\u8fd0\u52a8\u6267\u884c\u8d28\u91cf\u548c\u65f6\u95f4\u6233\u6807\u6ce8\uff0c\u5e76\u63d0\u4f9b\u4e86\u7528\u4e8e\u540e\u5904\u7406\u548c\u9a8c\u8bc1\u7684\u4ee3\u7801\u3002", "result": "\u6210\u529f\u6784\u5efa\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5927\u578b\u3001\u591a\u6837\u5316\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542bIMU\u548cMoCap\u539f\u59cb\u6570\u636e\u3001\u5904\u7406\u540e\u7684\u8fd0\u52a8\u5b66\u4fe1\u606f\u53ca\u76f8\u5173\u5206\u6790\u5de5\u5177\u3002\u8be5\u6570\u636e\u96c6\u652f\u6301\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u81ea\u52a8\u8fd0\u52a8\u8bc4\u4f30\u3001\u6b65\u6001\u5206\u6790\u3001\u65f6\u95f4\u6d3b\u52a8\u5206\u5272\u548c\u751f\u7269\u529b\u5b66\u53c2\u6570\u4f30\u8ba1\u7b49\u4efb\u52a1\u4e0a\u7684\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u8fd0\u52a8\u6570\u636e\u548c\u5206\u6790\u5de5\u5177\uff0c\u65e8\u5728\u52a0\u901f\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684\u4eba\u4f53\u8fd0\u52a8\u5206\u6790\u9886\u57df\u7684\u7814\u7a76\uff0c\u4ece\u800c\u4fc3\u8fdb\u76f8\u5173\u6a21\u578b\u7684\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2507.21974", "pdf": "https://arxiv.org/pdf/2507.21974", "abs": "https://arxiv.org/abs/2507.21974", "authors": ["Mohamed Sana", "Nicola Piovesan", "Antonio De Domenico", "Yibin Kang", "Haozhe Zhang", "Merouane Debbah", "Fadhel Ayed"], "title": "Reasoning Language Models for Root Cause Analysis in 5G Wireless Networks", "categories": ["cs.AI", "cs.NI"], "comment": null, "summary": "Root Cause Analysis (RCA) in mobile networks remains a challenging task due\nto the need for interpretability, domain expertise, and causal reasoning. In\nthis work, we propose a lightweight framework that leverages Large Language\nModels (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of\nannotated troubleshooting problems designed to benchmark RCA capabilities. Our\nevaluation reveals that existing open-source reasoning LLMs struggle with these\nproblems, underscoring the need for domain-specific adaptation. To address this\nissue, we propose a two-stage training methodology that combines supervised\nfine-tuning with reinforcement learning to improve the accuracy and reasoning\nquality of LLMs. The proposed approach fine-tunes a series of RCA models to\nintegrate domain knowledge and generate structured, multi-step diagnostic\nexplanations, improving both interpretability and effectiveness. Extensive\nexperiments across multiple LLM sizes show significant performance gains over\nstate-of-the-art reasoning and non-reasoning models, including strong\ngeneralization to randomized test variants. These results demonstrate the\npromise of domain-adapted, reasoning-enhanced LLMs for practical and\nexplainable RCA in network operation and management.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u79fb\u52a8\u7f51\u7edc\u4e2d\u7684\u6839\u56e0\u5206\u6790\uff08RCA\uff09\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86LLMs\u5728\u8be5\u9886\u57df\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u79fb\u52a8\u7f51\u7edc\u4e2d\u7684\u6839\u56e0\u5206\u6790\uff08RCA\uff09\u7531\u4e8e\u9700\u8981\u53ef\u89e3\u91ca\u6027\u3001\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u548c\u56e0\u679c\u63a8\u7406\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528LLMs\u8fdb\u884cRCA\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\u30022. \u6784\u5efa\u4e86\u540d\u4e3aTeleLogs\u7684\u5e26\u6ce8\u91ca\u6545\u969c\u6392\u9664\u6570\u636e\u96c6\uff0c\u7528\u4e8eRCA\u80fd\u529b\u57fa\u51c6\u6d4b\u8bd5\u30023. \u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8LLMs\u7684\u51c6\u786e\u6027\u548c\u63a8\u7406\u8d28\u91cf\u30024. \u5fae\u8c03\u4e86\u4e00\u7cfb\u5217RCA\u6a21\u578b\u4ee5\u6574\u5408\u9886\u57df\u77e5\u8bc6\u5e76\u751f\u6210\u7ed3\u6784\u5316\u3001\u591a\u6b65\u9aa4\u7684\u8bca\u65ad\u89e3\u91ca\u3002", "result": "1. \u73b0\u6709\u5f00\u6e90\u63a8\u7406LLMs\u5728TeleLogs\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\u30022. \u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u5c3a\u5bf8LLM\u4e0a\u5747\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u63a8\u7406\u548c\u975e\u63a8\u7406\u6a21\u578b\u30023. \u6a21\u578b\u5bf9\u968f\u673a\u5316\u6d4b\u8bd5\u53d8\u4f53\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u9886\u57df\u9002\u5e94\u548c\u63a8\u7406\u589e\u5f3a\u7684LLMs\u5728\u7f51\u7edc\u8fd0\u8425\u548c\u7ba1\u7406\u4e2d\uff0c\u5bf9\u4e8e\u5b9e\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u6839\u56e0\u5206\u6790\u5177\u6709\u5e7f\u9614\u524d\u666f\u3002"}}
{"id": "2507.21065", "pdf": "https://arxiv.org/pdf/2507.21065", "abs": "https://arxiv.org/abs/2507.21065", "authors": ["Sabrina Patania", "Luca Annese", "Cansu Koyuturk", "Azzurra Ruggeri", "Dimitri Ognibene"], "title": "Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions", "categories": ["cs.CL", "cs.HC", "cs.LG", "cs.RO", "I.2.7, I.2.9, j.4,"], "comment": "submitted to ICSR2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing extensive offline datasets. However, they often face challenges in\nacquiring and integrating complex, knowledge online. Traditional AI training\nparadigms, predominantly based on supervised learning or reinforcement\nlearning, mirror a 'Piagetian' model of independent exploration. These\napproaches typically rely on large datasets and sparse feedback signals,\nlimiting the models' ability to learn efficiently from interactions. Drawing\ninspiration from Vygotsky's sociocultural theory, this study explores the\npotential of socially mediated learning paradigms to address these limitations.\n  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI\nlearner agent engages in dyadic pedagogical dialogues with knowledgeable AI\nteacher agents. These interactions emphasize external, structured dialogue as a\ncore mechanism for knowledge acquisition, contrasting with methods that depend\nsolely on internal inference or pattern recognition.\n  Our investigation focuses on how different pedagogical strategies impact the\nAI learning process in the context of ontology acquisition. Empirical results\nindicate that such dialogic approaches-particularly those involving\nmixed-direction interactions combining top-down explanations with\nlearner-initiated questioning-significantly enhance the LLM's ability to\nacquire and apply new knowledge, outperforming both unidirectional\ninstructional methods and direct access to structured knowledge, formats\ntypically present in training datasets.\n  These findings suggest that integrating pedagogical and psychological\ninsights into AI and robot training can substantially improve post-training\nknowledge acquisition and response quality. This approach offers a\ncomplementary pathway to existing strategies like prompt engineering", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f15\u5165\u5bf9\u8bdd\u5f0f\u5b66\u4e60\u8303\u5f0f\uff0c\u53d7\u7ef4\u679c\u8328\u57fa\u7406\u8bba\u542f\u53d1\uff0c\u901a\u8fc7AI\u5e08\u751f\u4e92\u52a8\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5728\u7ebf\u77e5\u8bc6\u83b7\u53d6\u4e0e\u5e94\u7528\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u64c5\u957f\u5904\u7406\u79bb\u7ebf\u6570\u636e\uff0c\u4f46\u5728\u83b7\u53d6\u548c\u6574\u5408\u590d\u6742\u5728\u7ebf\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u76d1\u7763\u5b66\u4e60\u6216\u5f3a\u5316\u5b66\u4e60\u7684AI\u8bad\u7ec3\u8303\u5f0f\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u96be\u4ee5\u4ece\u7a00\u758f\u53cd\u9988\u548c\u4ea4\u4e92\u4e2d\u6709\u6548\u5b66\u4e60\u3002", "method": "\u7814\u7a76\u5f15\u5165\u201cAI\u793e\u4ea4\u5065\u8eab\u623f\u201d\u52a8\u6001\u73af\u5883\uff0c\u5176\u4e2dAI\u5b66\u4e60\u4ee3\u7406\u4e0eAI\u6559\u5e08\u4ee3\u7406\u8fdb\u884c\u4e8c\u5143\u6559\u5b66\u5bf9\u8bdd\u3002\u8be5\u65b9\u6cd5\u5f3a\u8c03\u5916\u90e8\u3001\u7ed3\u6784\u5316\u5bf9\u8bdd\u4f5c\u4e3a\u77e5\u8bc6\u83b7\u53d6\u7684\u6838\u5fc3\u673a\u5236\uff0c\u5e76\u805a\u7126\u4e8e\u4e0d\u540c\u6559\u5b66\u7b56\u7565\u5bf9\u672c\u4f53\u8bba\u83b7\u53d6\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u8bdd\u5f0f\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u7ed3\u5408\u81ea\u4e0a\u800c\u4e0b\u89e3\u91ca\u548c\u5b66\u4e60\u8005\u63d0\u95ee\u7684\u6df7\u5408\u65b9\u5411\u4ea4\u4e92\uff0c\u80fd\u663e\u8457\u589e\u5f3aLLM\u83b7\u53d6\u548c\u5e94\u7528\u65b0\u77e5\u8bc6\u7684\u80fd\u529b\u3002\u5176\u8868\u73b0\u4f18\u4e8e\u5355\u5411\u6559\u5b66\u6cd5\u548c\u76f4\u63a5\u8bbf\u95ee\u7ed3\u6784\u5316\u77e5\u8bc6\u7684\u4f20\u7edf\u65b9\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u6559\u5b66\u548c\u5fc3\u7406\u5b66\u89c1\u89e3\u878d\u5165AI\u548c\u673a\u5668\u4eba\u8bad\u7ec3\uff0c\u53ef\u4ee5\u5927\u5e45\u63d0\u5347\u6a21\u578b\u5728\u8bad\u7ec3\u540e\u7684\u77e5\u8bc6\u83b7\u53d6\u548c\u54cd\u5e94\u8d28\u91cf\uff0c\u4e3a\u73b0\u6709\u7b56\u7565\uff08\u5982\u63d0\u793a\u5de5\u7a0b\uff09\u63d0\u4f9b\u4e86\u4e00\u6761\u8865\u5145\u9014\u5f84\u3002"}}
{"id": "2507.21119", "pdf": "https://arxiv.org/pdf/2507.21119", "abs": "https://arxiv.org/abs/2507.21119", "authors": ["Yousuf Moiz Ali", "Jaroslaw E. Prilepsky", "Nicola Sambo", "Jo\u00e3o Pedro", "Mohammad M. Hosseini", "Antonio Napoli", "Sergei K. Turitsyn", "Pedro Freire"], "title": "Pre-, In-, and Post-Processing Class Imbalance Mitigation Techniques for Failure Detection in Optical Networks", "categories": ["cs.LG", "eess.SP", "physics.optics"], "comment": "3 pages + 1 page for acknowledgement and references", "summary": "We compare pre-, in-, and post-processing techniques for class imbalance\nmitigation in optical network failure detection. Threshold Adjustment achieves\nthe highest F1 gain (15.3%), while Random Under-sampling (RUS) offers the\nfastest inference, highlighting a key performance-complexity trade-off.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u5149\u7f51\u7edc\u6545\u969c\u68c0\u6d4b\u4e2d\u5904\u7406\u7c7b\u4e0d\u5e73\u8861\u7684\u9884\u5904\u7406\u3001\u5185\u90e8\u5904\u7406\u548c\u540e\u5904\u7406\u6280\u672f\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5149\u7f51\u7edc\u6545\u969c\u68c0\u6d4b\u4e2d\u7684\u7c7b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u5bf9\u591a\u79cd\u9884\u5904\u7406\u3001\u5185\u90e8\u5904\u7406\u548c\u540e\u5904\u7406\u6280\u672f\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u9608\u503c\u8c03\u6574\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684F1\u589e\u76ca\uff0815.3%\uff09\uff0c\u800c\u968f\u673a\u6b20\u91c7\u6837\uff08RUS\uff09\u63d0\u4f9b\u4e86\u6700\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5904\u7406\u5149\u7f51\u7edc\u6545\u969c\u68c0\u6d4b\u4e2d\u7c7b\u4e0d\u5e73\u8861\u65f6\uff0c\u6027\u80fd\u4e0e\u590d\u6742\u6027\u4e4b\u95f4\u5b58\u5728\u5173\u952e\u7684\u6743\u8861\u3002"}}
{"id": "2507.21098", "pdf": "https://arxiv.org/pdf/2507.21098", "abs": "https://arxiv.org/abs/2507.21098", "authors": ["Marta Sidorkiewicz", "Karolina Kr\u00f3likowska", "Berenika Dyczek", "Edyta Pijet-Migon", "Anna Dubel"], "title": "Artificial intelligence for sustainable wine industry: AI-driven management in viticulture, wine production and enotourism", "categories": ["cs.AI", "cs.CY", "I.2.6; I.2.1; H.4.2"], "comment": "6 pages, 4 figures. Accepted for presentation at the 27th European\n  Conference on Artificial Intelligence (ECAI 2025), October 19-24, 2025,\n  Bologna, Italy", "summary": "This study examines the role of Artificial Intelligence (AI) in enhancing\nsustainability and efficiency within the wine industry. It focuses on AI-driven\nintelligent management in viticulture, wine production, and enotourism. As the\nwine industry faces environmental and economic challenges, AI offers innovative\nsolutions to optimize resource use, reduce environmental impact, and improve\ncustomer engagement. Understanding AI's potential in sustainable winemaking is\ncrucial for fostering responsible and efficient industry practices. The\nresearch is based on a questionnaire survey conducted among Polish winemakers,\ncombined with a comprehensive analysis of AI methods applicable to viticulture,\nproduction, and tourism. Key AI technologies, including predictive analytics,\nmachine learning, and computer vision, are explored. The findings indicate that\nAI enhances vineyard monitoring, optimizes irrigation, and streamlines\nproduction processes, contributing to sustainable resource management. In\nenotourism, AI-powered chatbots, recommendation systems, and virtual tastings\npersonalize consumer experiences. The study highlights AI's impact on economic,\nenvironmental, and social sustainability, supporting local wine enterprises and\ncultural heritage. Keywords: Artificial Intelligence, Sustainable Development,\nAI-Driven Management, Viticulture, Wine Production, Enotourism, Wine\nEnterprises, Local Communities", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u8461\u8404\u9152\u4ea7\u4e1a\u4e2d\uff0c\u901a\u8fc7\u667a\u80fd\u7ba1\u7406\u63d0\u5347\u53ef\u6301\u7eed\u6027\u548c\u6548\u7387\u7684\u4f5c\u7528\u3002", "motivation": "\u8461\u8404\u9152\u4ea7\u4e1a\u9762\u4e34\u73af\u5883\u548c\u7ecf\u6d4e\u6311\u6218\uff0cAI\u6709\u671b\u901a\u8fc7\u4f18\u5316\u8d44\u6e90\u3001\u51cf\u5c11\u73af\u5883\u5f71\u54cd\u548c\u6539\u5584\u5ba2\u6237\u4f53\u9a8c\u6765\u63d0\u4f9b\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u4fc3\u8fdb\u8d1f\u8d23\u4efb\u548c\u9ad8\u6548\u7684\u884c\u4e1a\u5b9e\u8df5\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u5bf9\u6ce2\u5170\u917f\u9152\u5e08\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u5e76\u7ed3\u5408\u5bf9\u9002\u7528\u4e8e\u8461\u8404\u683d\u57f9\u3001\u8461\u8404\u9152\u751f\u4ea7\u548c\u8461\u8404\u9152\u65c5\u6e38\u7684\u5173\u952eAI\u6280\u672f\uff08\u5982\u9884\u6d4b\u5206\u6790\u3001\u673a\u5668\u5b66\u4e60\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\uff09\u7684\u7efc\u5408\u5206\u6790\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cAI\u80fd\u589e\u5f3a\u8461\u8404\u56ed\u76d1\u6d4b\u3001\u4f18\u5316\u704c\u6e89\u5e76\u7b80\u5316\u751f\u4ea7\u6d41\u7a0b\uff0c\u4fc3\u8fdb\u53ef\u6301\u7eed\u8d44\u6e90\u7ba1\u7406\u3002\u5728\u8461\u8404\u9152\u65c5\u6e38\u4e2d\uff0cAI\u804a\u5929\u673a\u5668\u4eba\u3001\u63a8\u8350\u7cfb\u7edf\u548c\u865a\u62df\u54c1\u9274\u80fd\u4e2a\u6027\u5316\u6d88\u8d39\u8005\u4f53\u9a8c\u3002AI\u5bf9\u7ecf\u6d4e\u3001\u73af\u5883\u548c\u793e\u4f1a\u53ef\u6301\u7eed\u6027\u5747\u6709\u79ef\u6781\u5f71\u54cd\uff0c\u5e76\u652f\u6301\u5f53\u5730\u8461\u8404\u9152\u4f01\u4e1a\u548c\u6587\u5316\u9057\u4ea7\u3002", "conclusion": "AI\u5728\u8461\u8404\u9152\u4ea7\u4e1a\u7684\u667a\u80fd\u7ba1\u7406\u4e2d\uff0c\u5c24\u5176\u5728\u8461\u8404\u683d\u57f9\u3001\u751f\u4ea7\u548c\u65c5\u6e38\u65b9\u9762\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u5e76\u5bf9\u7ecf\u6d4e\u3001\u73af\u5883\u548c\u793e\u4f1a\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u4ea7\u751f\u79ef\u6781\u800c\u91cd\u8981\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.21161", "pdf": "https://arxiv.org/pdf/2507.21161", "abs": "https://arxiv.org/abs/2507.21161", "authors": ["Pallavi Zambare", "Venkata Nikhil Thanikella", "Ying Liu"], "title": "Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal Video and Multimodal Cues", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted in IEEE 3rd International Conference on Artificial\n  Intelligence, Blockchain, and Internet of Things (AIBThings 2025)", "summary": "Pedestrian intention prediction is essential for autonomous driving in\ncomplex urban environments. Conventional approaches depend on supervised\nlearning over frame sequences and require extensive retraining to adapt to new\nscenarios. Here, we introduce BF-PIP (Beyond Frames Pedestrian Intention\nPrediction), a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing\nintentions directly from short, continuous video clips enriched with structured\nJAAD metadata. In contrast to GPT-4V based methods that operate on discrete\nframes, BF-PIP processes uninterrupted temporal clips. It also incorporates\nbounding-box annotations and ego-vehicle speed via specialized multimodal\nprompts. Without any additional training, BF-PIP achieves 73% prediction\naccuracy, outperforming a GPT-4V baseline by 18 %. These findings illustrate\nthat combining temporal video inputs with contextual cues enhances\nspatiotemporal perception and improves intent inference under ambiguous\nconditions. This approach paves the way for agile, retraining-free perception\nmodule in intelligent transportation system.", "AI": {"tldr": "BF-PIP\u662f\u4e00\u79cd\u57fa\u4e8eGemini 2.5 Pro\u7684\u96f6\u6837\u672c\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5904\u7406\u8fde\u7eed\u89c6\u9891\u7247\u6bb5\u548c\u7ed3\u5408\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u9884\u6d4b\uff0c\u5e76\u4f18\u4e8e\u57fa\u4e8eGPT-4V\u7684\u65b9\u6cd5\u3002", "motivation": "\u5728\u590d\u6742\u7684\u57ce\u5e02\u73af\u5883\u4e2d\uff0c\u884c\u4eba\u610f\u56fe\u9884\u6d4b\u5bf9\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u76d1\u7763\u5b66\u4e60\u548c\u5e27\u5e8f\u5217\uff0c\u9700\u8981\u5927\u91cf\u518d\u8bad\u7ec3\u4ee5\u9002\u5e94\u65b0\u573a\u666f\u3002", "method": "\u672c\u6587\u63d0\u51faBF-PIP\uff08Beyond Frames Pedestrian Intention Prediction\uff09\uff0c\u4e00\u79cd\u57fa\u4e8eGemini 2.5 Pro\u7684\u96f6\u6837\u672c\u65b9\u6cd5\u3002\u5b83\u76f4\u63a5\u4ece\u5305\u542bJAAD\u5143\u6570\u636e\u7684\u77ed\u8fde\u7eed\u89c6\u9891\u7247\u6bb5\u4e2d\u63a8\u65ad\u884c\u4eba\u8fc7\u8857\u610f\u56fe\u3002\u4e0e\u57fa\u4e8eGPT-4V\u7684\u79bb\u6563\u5e27\u65b9\u6cd5\u4e0d\u540c\uff0cBF-PIP\u5904\u7406\u4e0d\u95f4\u65ad\u7684\u65f6\u95f4\u7247\u6bb5\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u6001\u63d0\u793a\u6574\u5408\u8fb9\u754c\u6846\u6807\u6ce8\u548c\u81ea\u8f66\u901f\u5ea6\u3002", "result": "\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0cBF-PIP\u5b9e\u73b0\u4e8673%\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u6bd4\u57fa\u4e8eGPT-4V\u7684\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa18%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u65f6\u95f4\u89c6\u9891\u8f93\u5165\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\u53ef\u4ee5\u589e\u5f3a\u65f6\u7a7a\u611f\u77e5\uff0c\u5e76\u5728\u6a21\u7cca\u6761\u4ef6\u4e0b\u6539\u5584\u610f\u56fe\u63a8\u65ad\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u654f\u6377\u3001\u514d\u518d\u8bad\u7ec3\u7684\u611f\u77e5\u6a21\u5757\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.22019", "pdf": "https://arxiv.org/pdf/2507.22019", "abs": "https://arxiv.org/abs/2507.22019", "authors": ["Kritika Garg", "Sawood Alam", "Dietrich Ayala", "Michele C. Weigle", "Michael L. Nelson"], "title": "Not Here, Go There: Analyzing Redirection Patterns on the Web", "categories": ["cs.DL", "cs.IR", "cs.NI"], "comment": "Extended version of the paper accepted at the 2025 ACM Web Science\n  Conference (WebSci 2025)", "summary": "URI redirections are integral to web management, supporting structural\nchanges, SEO optimization, and security. However, their complexities affect\nusability, SEO performance, and digital preservation. This study analyzed 11\nmillion unique redirecting URIs, following redirections up to 10 hops per URI,\nto uncover patterns and implications of redirection practices. Our findings\nrevealed that 50% of the URIs terminated successfully, while 50% resulted in\nerrors, including 0.06% exceeding 10 hops. Canonical redirects, such as HTTP to\nHTTPS transitions, were prevalent, reflecting adherence to SEO best practices.\nNon-canonical redirects, often involving domain or path changes, highlighted\nsignificant web migrations, rebranding, and security risks. Notable patterns\nincluded \"sink\" URIs, where multiple redirects converged, ranging from traffic\nconsolidation by global websites to deliberate \"Rickrolling.\" The study also\nidentified 62,000 custom 404 URIs, almost half being soft 404s, which could\ncompromise SEO and user experience. These findings underscore the critical role\nof URI redirects in shaping the web while exposing challenges such as outdated\nURIs, server instability, and improper error handling. This research offers a\ndetailed analysis of URI redirection practices, providing insights into their\nprevalence, types, and outcomes. By examining a large dataset, we highlight\ninefficiencies in redirection chains and examine patterns such as the use of\n\"sink\" URIs and custom error pages. This information can help webmasters,\nresearchers, and digital archivists improve web usability, optimize resource\nallocation, and safeguard valuable online content.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e861100\u4e07URI\u91cd\u5b9a\u5411\u6570\u636e\uff0c\u63ed\u793a\u4e86\u91cd\u5b9a\u5411\u7684\u6210\u529f\u7387\u3001\u9519\u8bef\u7387\u3001\u5e38\u89c1\u7c7b\u578b\uff08\u89c4\u8303\u4e0e\u975e\u89c4\u8303\uff09\u3001\u7279\u6b8a\u6a21\u5f0f\uff08\u5982\u201c\u6c47\u805aURI\u201d\uff09\u53ca\u81ea\u5b9a\u4e49404\u9875\u9762\u7684\u666e\u904d\u6027\uff0c\u5f3a\u8c03\u4e86\u91cd\u5b9a\u5411\u5728\u7f51\u7edc\u7ba1\u7406\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u53ca\u6240\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "URI\u91cd\u5b9a\u5411\u867d\u5bf9\u7f51\u7edc\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff08\u5982\u7ed3\u6784\u8c03\u6574\u3001SEO\u3001\u5b89\u5168\uff09\uff0c\u4f46\u5176\u590d\u6742\u6027\u4e5f\u5e26\u6765\u4e86\u53ef\u7528\u6027\u3001SEO\u8868\u73b0\u548c\u6570\u5b57\u4fdd\u5b58\u65b9\u9762\u7684\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u7406\u89e3\u91cd\u5b9a\u5411\u5b9e\u8df5\u7684\u6a21\u5f0f\u548c\u5f71\u54cd\u3002", "method": "\u5bf91100\u4e07\u4e2a\u72ec\u7acb\u91cd\u5b9a\u5411URI\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u6bcf\u4e2aURI\u6700\u591a\u8ddf\u8e2a10\u8df3\u91cd\u5b9a\u5411\u3002", "result": ["50%\u7684URI\u91cd\u5b9a\u5411\u6210\u529f\uff0c50%\u5bfc\u81f4\u9519\u8bef\uff08\u5176\u4e2d0.06%\u8d85\u8fc710\u8df3\uff09\u3002", "\u89c4\u8303\u91cd\u5b9a\u5411\uff08\u5982HTTP\u5230HTTPS\uff09\u666e\u904d\u5b58\u5728\uff0c\u975e\u89c4\u8303\u91cd\u5b9a\u5411\uff08\u5982\u57df\u540d/\u8def\u5f84\u53d8\u66f4\uff09\u63ed\u793a\u4e86\u5927\u89c4\u6a21\u8fc1\u79fb\u548c\u6f5c\u5728\u5b89\u5168\u98ce\u9669\u3002", "\u53d1\u73b0\u201c\u6c47\u805aURI\u201d\u6a21\u5f0f\uff0c\u5373\u591a\u4e2a\u91cd\u5b9a\u5411\u6c47\u805a\u5230\u540c\u4e00\u76ee\u6807\u3002", "\u8bc6\u522b\u51fa62,000\u4e2a\u81ea\u5b9a\u4e49404 URI\uff0c\u8fd1\u534a\u6570\u4e3a\u8f6f404\uff0c\u53ef\u80fd\u635f\u5bb3SEO\u548c\u7528\u6237\u4f53\u9a8c\u3002"], "conclusion": "URI\u91cd\u5b9a\u5411\u5bf9\u7f51\u7edc\u5851\u9020\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u8fc7\u671fURI\u3001\u670d\u52a1\u5668\u4e0d\u7a33\u5b9a\u548c\u9519\u8bef\u5904\u7406\u7b49\u6311\u6218\u3002\u672c\u7814\u7a76\u7684\u6df1\u5165\u5206\u6790\u6709\u52a9\u4e8e\u7f51\u7edc\u7ba1\u7406\u5458\u3001\u7814\u7a76\u4eba\u5458\u548c\u6570\u5b57\u6863\u6848\u5458\u6539\u8fdb\u7f51\u7edc\u53ef\u7528\u6027\u3001\u4f18\u5316\u8d44\u6e90\u5e76\u4fdd\u62a4\u5728\u7ebf\u5185\u5bb9\u3002"}}
{"id": "2507.21073", "pdf": "https://arxiv.org/pdf/2507.21073", "abs": "https://arxiv.org/abs/2507.21073", "authors": ["David James Woo", "Yangyang Yu", "Kai Guo", "Yilin Huang", "April Ka Yeng Fung"], "title": "Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing", "categories": ["cs.CL", "cs.HC"], "comment": "45 pages, 11 figures", "summary": "Text generated by artificial intelligence (AI) chatbots is increasingly used\nin English as a foreign language (EFL) writing contexts, yet its impact on\nstudents' expository writing process and compositions remains understudied.\nThis research examines how EFL secondary students edit AI-generated text.\nExploring editing behaviors in their expository writing process and in\nexpository compositions, and their effect on human-rated scores for content,\norganization, language, and overall quality. Participants were 39 Hong Kong\nsecondary students who wrote an expository composition with AI chatbots in a\nworkshop. A convergent design was employed to analyze their screen recordings\nand compositions to examine students' editing behaviors and writing qualities.\nAnalytical methods included qualitative coding, descriptive statistics,\ntemporal sequence analysis, human-rated scoring, and multiple linear regression\nanalysis. We analyzed over 260 edits per dataset, and identified two editing\npatterns: one where students refined introductory units repeatedly before\nprogressing, and another where they quickly shifted to extensive edits in body\nunits (e.g., topic and supporting sentences). MLR analyses revealed that the\nnumber of AI-generated words positively predicted all score dimensions, while\nmost editing variables showed minimal impact. These results suggest a\ndisconnect between students' significant editing effort and improved\ncomposition quality, indicating AI supports but does not replace writing\nskills. The findings highlight the importance of genre-specific instruction and\nprocess-focused writing before AI integration. Educators should also develop\nassessments valuing both process and product to encourage critical engagement\nwith AI text.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86EFL\u4e2d\u5b66\u751f\u7f16\u8f91AI\u751f\u6210\u6587\u672c\u7684\u884c\u4e3a\u53ca\u5176\u5bf9\u8bf4\u660e\u6587\u5199\u4f5c\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0AI\u751f\u6210\u8bcd\u6c47\u91cf\u4e0e\u5f97\u5206\u6b63\u76f8\u5173\uff0c\u4f46\u5b66\u751f\u7684\u7f16\u8f91\u52aa\u529b\u5bf9\u6587\u7ae0\u8d28\u91cf\u63d0\u5347\u6548\u679c\u4e0d\u663e\u8457\uff0c\u8868\u660eAI\u662f\u8f85\u52a9\u800c\u975e\u66ff\u4ee3\u5199\u4f5c\u6280\u80fd\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u804a\u5929\u673a\u5668\u4eba\u751f\u6210\u7684\u6587\u672c\u5728EFL\u5199\u4f5c\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5176\u5bf9\u5b66\u751f\u8bf4\u660e\u6587\u5199\u4f5c\u8fc7\u7a0b\u548c\u4f5c\u54c1\u7684\u5f71\u54cd\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8003\u5bdf\u5b66\u751f\u5982\u4f55\u7f16\u8f91AI\u6587\u672c\u53ca\u5176\u5bf9\u5199\u4f5c\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u62db\u52df\u4e8639\u540d\u9999\u6e2f\u4e2d\u5b66\u751f\uff0c\u4ed6\u4eec\u5728\u5de5\u4f5c\u574a\u4e2d\u4f7f\u7528AI\u804a\u5929\u673a\u5668\u4eba\u64b0\u5199\u8bf4\u660e\u6587\u3002\u91c7\u7528\u6c47\u805a\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5206\u6790\u5c4f\u5e55\u5f55\u50cf\u548c\u4f5c\u54c1\uff0c\u7ed3\u5408\u5b9a\u6027\u7f16\u7801\u3001\u63cf\u8ff0\u6027\u7edf\u8ba1\u3001\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3001\u4eba\u5de5\u8bc4\u5206\u548c\u591a\u5143\u7ebf\u6027\u56de\u5f52\u5206\u6790\u6765\u8003\u5bdf\u5b66\u751f\u7684\u7f16\u8f91\u884c\u4e3a\u548c\u5199\u4f5c\u8d28\u91cf\u3002\u5206\u6790\u4e86\u8d85\u8fc7260\u4e2a\u7f16\u8f91\u6570\u636e\u3002", "result": "\u7814\u7a76\u8bc6\u522b\u51fa\u4e24\u79cd\u7f16\u8f91\u6a21\u5f0f\uff1a\u4e00\u662f\u5b66\u751f\u53cd\u590d\u4fee\u6539\u5f15\u8a00\u90e8\u5206\uff0c\u4e8c\u662f\u5feb\u901f\u8f6c\u5411\u4e3b\u4f53\u90e8\u5206\uff08\u5982\u4e3b\u9898\u53e5\u548c\u652f\u6491\u53e5\uff09\u7684\u5927\u91cf\u4fee\u6539\u3002\u591a\u5143\u7ebf\u6027\u56de\u5f52\u5206\u6790\u663e\u793a\uff0cAI\u751f\u6210\u7684\u8bcd\u6c47\u91cf\u4e0e\u6240\u6709\u8bc4\u5206\u7ef4\u5ea6\uff08\u5185\u5bb9\u3001\u7ec4\u7ec7\u3001\u8bed\u8a00\u3001\u6574\u4f53\u8d28\u91cf\uff09\u5448\u6b63\u76f8\u5173\uff0c\u800c\u5927\u591a\u6570\u7f16\u8f91\u53d8\u91cf\u5bf9\u5f97\u5206\u5f71\u54cd\u751a\u5fae\u3002\u8fd9\u8868\u660e\u5b66\u751f\u7684\u663e\u8457\u7f16\u8f91\u52aa\u529b\u4e0e\u4f5c\u6587\u8d28\u91cf\u7684\u63d0\u5347\u4e4b\u95f4\u5b58\u5728\u8131\u8282\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660eAI\u8f85\u52a9\u800c\u975e\u53d6\u4ee3\u5199\u4f5c\u6280\u80fd\u3002\u5728\u6574\u5408AI\u4e4b\u524d\uff0c\u5f3a\u8c03\u4f53\u88c1\u7279\u5b9a\u7684\u6559\u5b66\u548c\u8fc7\u7a0b\u5bfc\u5411\u7684\u5199\u4f5c\u81f3\u5173\u91cd\u8981\u3002\u6559\u80b2\u8005\u5e94\u53d1\u5c55\u91cd\u89c6\u8fc7\u7a0b\u548c\u6210\u679c\u7684\u8bc4\u4f30\u65b9\u5f0f\uff0c\u4ee5\u9f13\u52b1\u5b66\u751f\u6279\u5224\u6027\u5730\u4f7f\u7528AI\u6587\u672c\u3002"}}
{"id": "2507.21135", "pdf": "https://arxiv.org/pdf/2507.21135", "abs": "https://arxiv.org/abs/2507.21135", "authors": ["Alexander G. Abanov", "Luca Candelori", "Harold C. Steinacker", "Martin T. Wells", "Jerome R. Busemeyer", "Cameron J. Hogan", "Vahagn Kirakosyan", "Nicola Marzari", "Sunil Pinnamaneni", "Dario Villani", "Mengjia Xu", "Kharen Musaelian"], "title": "Quantum Geometry of Data", "categories": ["cs.LG", "quant-ph", "stat.ML"], "comment": "27 pages, 14 figures, 1 table", "summary": "We demonstrate how Quantum Cognition Machine Learning (QCML) encodes data as\nquantum geometry. In QCML, features of the data are represented by learned\nHermitian matrices, and data points are mapped to states in Hilbert space. The\nquantum geometry description endows the dataset with rich geometric and\ntopological structure - including intrinsic dimension, quantum metric, and\nBerry curvature - derived directly from the data. QCML captures global\nproperties of data, while avoiding the curse of dimensionality inherent in\nlocal methods. We illustrate this on a number of synthetic and real-world\nexamples. Quantum geometric representation of QCML could advance our\nunderstanding of cognitive phenomena within the framework of quantum cognition.", "AI": {"tldr": "\u91cf\u5b50\u8ba4\u77e5\u673a\u5668\u5b66\u4e60\uff08QCML\uff09\u5c06\u6570\u636e\u7f16\u7801\u4e3a\u91cf\u5b50\u51e0\u4f55\uff0c\u63ed\u793a\u5176\u5185\u5728\u7ed3\u6784\u5e76\u907f\u514d\u7ef4\u5ea6\u707e\u96be\uff0c\u5bf9\u8ba4\u77e5\u7814\u7a76\u6709\u6f5c\u5728\u4ef7\u503c\u3002", "motivation": "\u65e8\u5728\u5728\u91cf\u5b50\u8ba4\u77e5\u6846\u67b6\u5185\uff0c\u901a\u8fc7\u5c55\u793aQCML\u5982\u4f55\u5c06\u6570\u636e\u7f16\u7801\u4e3a\u91cf\u5b50\u51e0\u4f55\uff0c\u4ece\u800c\u63a8\u8fdb\u5bf9\u8ba4\u77e5\u73b0\u8c61\u7684\u7406\u89e3\u3002", "method": "\u5f00\u53d1\u4e86\u91cf\u5b50\u8ba4\u77e5\u673a\u5668\u5b66\u4e60\uff08QCML\uff09\u6a21\u578b\uff0c\u5176\u4e2d\u6570\u636e\u7279\u5f81\u7531\u5b66\u4e60\u7684\u5384\u7c73\u77e9\u9635\u8868\u793a\uff0c\u6570\u636e\u70b9\u6620\u5c04\u5230\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u72b6\u6001\u3002\u901a\u8fc7\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u793a\u4f8b\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "QCML\u6210\u529f\u5c06\u6570\u636e\u7f16\u7801\u4e3a\u91cf\u5b50\u51e0\u4f55\uff0c\u4ece\u4e2d\u5bfc\u51fa\u4e86\u5185\u5728\u7ef4\u5ea6\u3001\u91cf\u5b50\u5ea6\u91cf\u548c\u8d1d\u91cc\u66f2\u7387\u7b49\u4e30\u5bcc\u7684\u51e0\u4f55\u548c\u62d3\u6251\u7ed3\u6784\u3002\u8be5\u65b9\u6cd5\u80fd\u6355\u83b7\u6570\u636e\u7684\u5168\u5c40\u5c5e\u6027\uff0c\u5e76\u6709\u6548\u907f\u514d\u5c40\u90e8\u65b9\u6cd5\u4e2d\u56fa\u6709\u7684\u7ef4\u5ea6\u707e\u96be\u3002", "conclusion": "QCML\u7684\u91cf\u5b50\u51e0\u4f55\u8868\u793a\u6709\u671b\u5728\u91cf\u5b50\u8ba4\u77e5\u6846\u67b6\u5185\uff0c\u52a0\u6df1\u6211\u4eec\u5bf9\u8ba4\u77e5\u73b0\u8c61\u7684\u7406\u89e3\u3002"}}
{"id": "2507.21123", "pdf": "https://arxiv.org/pdf/2507.21123", "abs": "https://arxiv.org/abs/2507.21123", "authors": ["Mark A. Kramer", "Aanchal Mathur", "Caroline E. Adams", "Jason A. Walonoski"], "title": "Leveraging Generative AI to Enhance Synthea Module Development", "categories": ["cs.AI", "cs.LG"], "comment": "Title: Leveraging Generative AI to Enhance Synthea Module Development\n  Word Count: [Approximately 12,000 words] Figures: 3 Tables: 3 Supplementary\n  Material: Extensive appendices with prompts and disease profiles", "summary": "This paper explores the use of large language models (LLMs) to assist in the\ndevelopment of new disease modules for Synthea, an open-source synthetic health\ndata generator. Incorporating LLMs into the module development process has the\npotential to reduce development time, reduce required expertise, expand model\ndiversity, and improve the overall quality of synthetic patient data. We\ndemonstrate four ways that LLMs can support Synthea module creation: generating\na disease profile, generating a disease module from a disease profile,\nevaluating an existing Synthea module, and refining an existing module. We\nintroduce the concept of progressive refinement, which involves iteratively\nevaluating the LLM-generated module by checking its syntactic correctness and\nclinical accuracy, and then using that information to modify the module. While\nthe use of LLMs in this context shows promise, we also acknowledge the\nchallenges and limitations, such as the need for human oversight, the\nimportance of rigorous testing and validation, and the potential for\ninaccuracies in LLM-generated content. The paper concludes with recommendations\nfor future research and development to fully realize the potential of LLM-aided\nsynthetic data creation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8f85\u52a9Synthea\u75be\u75c5\u6a21\u5757\u7684\u5f00\u53d1\uff0c\u65e8\u5728\u63d0\u5347\u6548\u7387\u548c\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5e94\u7528\u65b9\u5f0f\u3001\u6f5c\u5728\u6536\u76ca\u53ca\u9762\u4e34\u7684\u6311\u6218\u3002", "motivation": "\u5c06LLMs\u6574\u5408\u5230Synthea\u75be\u75c5\u6a21\u5757\u5f00\u53d1\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u51cf\u5c11\u5f00\u53d1\u65f6\u95f4\u3001\u964d\u4f4e\u6240\u9700\u4e13\u4e1a\u77e5\u8bc6\u3001\u6269\u5c55\u6a21\u578b\u591a\u6837\u6027\u5e76\u63d0\u9ad8\u5408\u6210\u60a3\u8005\u6570\u636e\u7684\u6574\u4f53\u8d28\u91cf\u3002", "method": "\u5c55\u793a\u4e86LLMs\u652f\u6301Synthea\u6a21\u5757\u521b\u5efa\u7684\u56db\u79cd\u65b9\u5f0f\uff1a\u751f\u6210\u75be\u75c5\u6982\u51b5\u3001\u6839\u636e\u75be\u75c5\u6982\u51b5\u751f\u6210\u75be\u75c5\u6a21\u5757\u3001\u8bc4\u4f30\u73b0\u6709Synthea\u6a21\u5757\u548c\u7cbe\u4fee\u73b0\u6709\u6a21\u5757\u3002\u5f15\u5165\u4e86\u201c\u6e10\u8fdb\u5f0f\u7cbe\u4fee\u201d\u6982\u5ff5\uff0c\u901a\u8fc7\u8fed\u4ee3\u8bc4\u4f30LLM\u751f\u6210\u6a21\u5757\u7684\u8bed\u6cd5\u6b63\u786e\u6027\u548c\u4e34\u5e8a\u51c6\u786e\u6027\u6765\u4fee\u6539\u6a21\u5757\u3002", "result": "LLMs\u5728\u6b64\u80cc\u666f\u4e0b\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u6311\u6218\u548c\u5c40\u9650\u6027\uff0c\u4f8b\u5982\u9700\u8981\u4eba\u5de5\u76d1\u7763\u3001\u4e25\u683c\u7684\u6d4b\u8bd5\u548c\u9a8c\u8bc1\uff0c\u4ee5\u53caLLM\u751f\u6210\u5185\u5bb9\u53ef\u80fd\u4e0d\u51c6\u786e\u3002", "conclusion": "LLM\u8f85\u52a9\u5408\u6210\u6570\u636e\u521b\u5efa\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9700\u8981\u672a\u6765\u7684\u7814\u7a76\u548c\u5f00\u53d1\u6765\u5145\u5206\u5b9e\u73b0\u5176\u6f5c\u529b\u3002"}}
{"id": "2507.21167", "pdf": "https://arxiv.org/pdf/2507.21167", "abs": "https://arxiv.org/abs/2507.21167", "authors": ["Danglu Yang", "Liang Zhang", "Zihao Yue", "Liangyu Chen", "Yichen Xu", "Wenxuan Wang", "Qin Jin"], "title": "ChartM$^3$: Benchmarking Chart Editing with Multimodal Instructions", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Charts are a fundamental visualization format widely used in data analysis\nacross research and industry. While enabling users to edit charts based on\nhigh-level intentions is of great practical value, existing methods primarily\nrely on natural language instructions, which are often too ambiguous to support\nfine-grained editing. In this work, we introduce a novel paradigm for\nmultimodal chart editing, where user intent is expressed through a combination\nof natural language and visual indicators that explicitly highlight the\nelements to be modified. To support this paradigm, we present\nChart$\\text{M}^3$, a new benchmark for Multimodal chart editing with\nMulti-level complexity and Multi-perspective evaluation. Chart$\\text{M}^3$\ncontains 1,000 samples spanning four levels of editing difficulty. Each sample\nincludes triplets in the form of (chart, code, multimodal instructions). To\ncomprehensively evaluate chart editing models, Chart$\\text{M}^3$ provides\nmetrics that assess both visual appearance and code correctness. Our benchmark\nreveals significant limitations in current multimodal large language models\n(MLLMs), including GPT-4o, particularly in their ability to interpret and act\non visual indicators. To address this, we construct Chart$\\text{M}^3$-Train, a\nlarge-scale training set with 24,000 multimodal chart editing samples.\nFine-tuning MLLMs on this dataset leads to substantial improvements,\ndemonstrating the importance of multimodal supervision in building practical\nchart editing systems. Our datasets, codes, and evaluation tools are available\nat https://github.com/MLrollIT/ChartM3. %https://github.com/MLrollIT/ChartM3Our\ndatasets, codes, and evaluation tools are available at\nhttps://github.com/yaolinli/VCE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u56fe\u8868\u7f16\u8f91\u7684\u65b0\u8303\u5f0f\uff0c\u5e76\u6784\u5efa\u4e86ChartM3\u57fa\u51c6\u548cChartM3-Train\u8bad\u7ec3\u96c6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u5e76\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56fe\u8868\u7f16\u8f91\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u56fe\u8868\u7f16\u8f91\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u4f46\u5176\u6a21\u7cca\u6027\u96be\u4ee5\u652f\u6301\u7ec6\u7c92\u5ea6\u7f16\u8f91\u3002\u7528\u6237\u671f\u671b\u901a\u8fc7\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u548c\u89c6\u89c9\u6307\u793a\u6765\u8868\u8fbe\u7f16\u8f91\u610f\u56fe\uff0c\u4f46\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u7406\u89e3\u548c\u6267\u884c\u89c6\u89c9\u6307\u793a\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165\u4e86\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u548c\u89c6\u89c9\u6307\u793a\u7684\u591a\u6a21\u6001\u56fe\u8868\u7f16\u8f91\u65b0\u8303\u5f0f\u3002\u6784\u5efa\u4e86ChartM3\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b1,000\u4e2a\u6837\u672c\uff0c\u6db5\u76d6\u56db\u79cd\u7f16\u8f91\u96be\u5ea6\uff0c\u6bcf\u4e2a\u6837\u672c\u5305\u542b\uff08\u56fe\u8868\u3001\u4ee3\u7801\u3001\u591a\u6a21\u6001\u6307\u4ee4\uff09\u4e09\u5143\u7ec4\u3002\u8bbe\u8ba1\u4e86\u8bc4\u4f30\u56fe\u8868\u89c6\u89c9\u5916\u89c2\u548c\u4ee3\u7801\u6b63\u786e\u6027\u7684\u591a\u7ef4\u5ea6\u6307\u6807\u3002\u8fdb\u4e00\u6b65\u6784\u5efa\u4e86ChartM3-Train\u5927\u89c4\u6a21\u8bad\u7ec3\u96c6\uff0824,000\u4e2a\u6837\u672c\uff09\uff0c\u5e76\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u5bf9MLLMs\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u901a\u8fc7ChartM3\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\uff0c\u5305\u62ecGPT-4o\u5728\u5185\u7684\u5f53\u524dMLLMs\u5728\u89e3\u91ca\u548c\u54cd\u5e94\u89c6\u89c9\u6307\u793a\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002\u5728ChartM3-Train\u6570\u636e\u96c6\u4e0a\u5bf9MLLMs\u8fdb\u884c\u5fae\u8c03\u540e\uff0c\u6a21\u578b\u6027\u80fd\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u591a\u6a21\u6001\u76d1\u7763\u5bf9\u4e8e\u6784\u5efa\u5b9e\u7528\u7684\u56fe\u8868\u7f16\u8f91\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63d0\u4f9b\u7684\u57fa\u51c6\u3001\u4ee3\u7801\u548c\u8bc4\u4f30\u5de5\u5177\u53ef\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2507.21080", "pdf": "https://arxiv.org/pdf/2507.21080", "abs": "https://arxiv.org/abs/2507.21080", "authors": ["Vincent C. M\u00fcller"], "title": "Which symbol grounding problem should we try to solve?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Floridi and Taddeo propose a condition of \"zero semantic commitment\" for\nsolutions to the grounding problem, and a solution to it. I argue briefly that\ntheir condition cannot be fulfilled, not even by their own solution. After a\nlook at Luc Steels' very different competing suggestion, I suggest that we need\nto re-think what the problem is and what role the 'goals' in a system play in\nformulating the problem. On the basis of a proper understanding of computing, I\ncome to the conclusion that the only sensible grounding problem is how we can\nexplain and re-produce the behavioral ability and function of meaning in\nartificial computational agents", "AI": {"tldr": "\u672c\u6587\u6279\u5224\u4e86Floridi\u548cTaddeo\u63d0\u51fa\u7684\u201c\u96f6\u8bed\u4e49\u627f\u8bfa\u201d\u6761\u4ef6\u53ca\u5176\u57fa\u7840\u95ee\u9898\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u91cd\u65b0\u5b9a\u4e49\u4e86\u57fa\u7840\u95ee\u9898\uff0c\u8ba4\u4e3a\u5176\u5e94\u5173\u6ce8\u4eba\u5de5\u8ba1\u7b97\u667a\u80fd\u4f53\u4e2d\u610f\u4e49\u7684\u529f\u80fd\u3002", "motivation": "\u6307\u51faFloridi\u548cTaddeo\u7684\u201c\u96f6\u8bed\u4e49\u627f\u8bfa\u201d\u6761\u4ef6\u4e0d\u53ef\u5b9e\u73b0\uff0c\u5e76\u8ba4\u4e3a\u9700\u8981\u91cd\u65b0\u601d\u8003\u57fa\u7840\u95ee\u9898\u7684\u672c\u8d28\u53ca\u5176\u5728\u7cfb\u7edf\u4e2d\u201c\u76ee\u6807\u201d\u7684\u4f5c\u7528\u3002", "method": "\u5206\u6790\u5e76\u6279\u5224Floridi\u548cTaddeo\u7684\u89c2\u70b9\uff0c\u5ba1\u89c6Luc Steels\u7b49\u4eba\u7684\u4e0d\u540c\u5efa\u8bae\uff0c\u5e76\u57fa\u4e8e\u5bf9\u8ba1\u7b97\u7684\u7406\u89e3\uff0c\u91cd\u65b0\u754c\u5b9a\u57fa\u7840\u95ee\u9898\u3002", "result": "Floridi\u548cTaddeo\u7684\u201c\u96f6\u8bed\u4e49\u627f\u8bfa\u201d\u6761\u4ef6\u53ca\u5176\u89e3\u51b3\u65b9\u6848\u65e0\u6cd5\u5b9e\u73b0\u3002\u57fa\u7840\u95ee\u9898\u9700\u8981\u88ab\u91cd\u65b0\u7406\u89e3\u3002", "conclusion": "\u552f\u4e00\u6709\u610f\u4e49\u7684\u57fa\u7840\u95ee\u9898\u662f\u5982\u4f55\u89e3\u91ca\u548c\u518d\u73b0\u4eba\u5de5\u8ba1\u7b97\u667a\u80fd\u4f53\u4e2d\u610f\u4e49\u7684\u884c\u4e3a\u80fd\u529b\u548c\u529f\u80fd\u3002"}}
{"id": "2507.21136", "pdf": "https://arxiv.org/pdf/2507.21136", "abs": "https://arxiv.org/abs/2507.21136", "authors": ["Mojtaba Moattari"], "title": "A Study on Variants of Conventional, Fuzzy, and Nullspace-Based Independence Criteria for Improving Supervised and Unsupervised Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Unsupervised and supervised learning methods conventionally use kernels to\ncapture nonlinearities inherent in data structure. However experts have to\nensure their proposed nonlinearity maximizes variability and capture inherent\ndiversity of data. We reviewed all independence criteria to design unsupervised\nlearners. Then we proposed 3 independence criteria and used them to design\nunsupervised and supervised dimensionality reduction methods. We evaluated\ncontrast, accuracy and interpretability of these methods in both linear and\nneural nonlinear settings. The results show that the methods have outperformed\nthe baseline (tSNE, PCA, regularized LDA, VAE with (un)supervised learner and\nlayer sharing) and opened a new line of interpretable machine learning (ML) for\nthe researchers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u65b0\u7684\u72ec\u7acb\u6027\u5224\u636e\uff0c\u5e76\u57fa\u4e8e\u5b83\u4eec\u8bbe\u8ba1\u4e86\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u7684\u65e0\u76d1\u7763\u548c\u6709\u76d1\u7763\u964d\u7ef4\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u6355\u83b7\u6570\u636e\u975e\u7ebf\u6027\u65f6\uff0c\u96be\u4ee5\u786e\u4fdd\u6240\u9009\u975e\u7ebf\u6027\u6700\u5927\u5316\u6570\u636e\u53d8\u5f02\u6027\u548c\u591a\u6837\u6027\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u5bfb\u6c42\u66f4\u597d\u7684\u72ec\u7acb\u6027\u5224\u636e\u6765\u8bbe\u8ba1\u5b66\u4e60\u5668\u3002", "method": "\u7814\u7a76\u5ba1\u67e5\u4e86\u6240\u6709\u72ec\u7acb\u6027\u5224\u636e\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u7684\u72ec\u7acb\u6027\u5224\u636e\uff0c\u5e76\u5229\u7528\u5b83\u4eec\u8bbe\u8ba1\u4e86\u65e0\u76d1\u7763\u548c\u6709\u76d1\u7763\u7684\u964d\u7ef4\u65b9\u6cd5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5728\u7ebf\u6027\u548c\u795e\u7ecf\u7f51\u7edc\u975e\u7ebf\u6027\u73af\u5883\u4e2d\uff0c\u5bf9\u5bf9\u6bd4\u5ea6\u3001\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u5305\u62ectSNE\u3001PCA\u3001\u6b63\u5219\u5316LDA\u548cVAE\u53d8\u4f53\u5728\u5185\u7684\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u66f4\u6709\u6548\u3001\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u6a21\u578b\u7684\u65b0\u9014\u5f84\u3002"}}
