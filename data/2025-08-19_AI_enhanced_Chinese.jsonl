{"id": "2508.11676", "pdf": "https://arxiv.org/pdf/2508.11676", "abs": "https://arxiv.org/abs/2508.11676", "authors": ["Maksym Shamrai", "Vladyslav Hamolia"], "title": "Deep Language Geometry: Constructing a Metric Space from LLM Weights", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "18 pages, accepted to RANLP 2025", "summary": "We introduce a novel framework that utilizes the internal weight activations\nof modern Large Language Models (LLMs) to construct a metric space of\nlanguages. Unlike traditional approaches based on hand-crafted linguistic\nfeatures, our method automatically derives high-dimensional vector\nrepresentations by computing weight importance scores via an adapted pruning\nalgorithm. Our approach captures intrinsic language characteristics that\nreflect linguistic phenomena. We validate our approach across diverse datasets\nand multilingual LLMs, covering 106 languages. The results align well with\nestablished linguistic families while also revealing unexpected inter-language\nconnections that may indicate historical contact or language evolution. The\nsource code, computed language latent vectors, and visualization tool are made\npublicly available at https://github.com/mshamrai/deep-language-geometry.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5185\u90e8\u6743\u91cd\u6fc0\u6d3b\uff0c\u901a\u8fc7\u526a\u679d\u7b97\u6cd5\u6784\u5efa\u8bed\u8a00\u7684\u5ea6\u91cf\u7a7a\u95f4\uff0c\u63ed\u793a\u4e86106\u79cd\u8bed\u8a00\u7684\u5185\u5728\u7279\u5f81\u548c\u76f8\u4e92\u8054\u7cfb\u3002", "motivation": "\u4f20\u7edf\u7684\u8bed\u8a00\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u8bed\u8a00\u7279\u5f81\u3002\u672c\u7814\u7a76\u65e8\u5728\u7a81\u7834\u8fd9\u4e00\u5c40\u9650\uff0c\u81ea\u52a8\u4eceLLMs\u4e2d\u63d0\u53d6\u9ad8\u7ef4\u5411\u91cf\u8868\u793a\uff0c\u4ee5\u6355\u6349\u8bed\u8a00\u7684\u5185\u5728\u7279\u6027\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u53cd\u6620\u8bed\u8a00\u5b66\u73b0\u8c61\u7684\u5ea6\u91cf\u7a7a\u95f4\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5229\u7528\u73b0\u4ee3LLMs\u7684\u5185\u90e8\u6743\u91cd\u6fc0\u6d3b\u6765\u6784\u5efa\u8bed\u8a00\u7684\u5ea6\u91cf\u7a7a\u95f4\u3002\u5177\u4f53\u65b9\u6cd5\u662f\u901a\u8fc7\u4e00\u4e2a\u9002\u5e94\u6027\u7684\u526a\u679d\u7b97\u6cd5\u8ba1\u7b97\u6743\u91cd\u91cd\u8981\u6027\u5206\u6570\uff0c\u4ece\u800c\u81ea\u52a8\u63a8\u5bfc\u51fa\u9ad8\u7ef4\u5411\u91cf\u8868\u793a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6db5\u76d6106\u79cd\u8bed\u8a00\u7684\u591a\u79cd\u6570\u636e\u96c6\u548c\u591a\u8bed\u8a00LLMs\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5b83\u4e0e\u5df2\u5efa\u7acb\u7684\u8bed\u8a00\u5bb6\u65cf\u9ad8\u5ea6\u543b\u5408\uff0c\u5e76\u4e14\u63ed\u793a\u4e86\u53ef\u80fd\u6307\u793a\u5386\u53f2\u63a5\u89e6\u6216\u8bed\u8a00\u6f14\u53d8\u7684\u610f\u5916\u8bed\u8a00\u95f4\u8054\u7cfb\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5730\u5229\u7528LLM\u5185\u90e8\u6743\u91cd\u6784\u5efa\u4e86\u8bed\u8a00\u7684\u5ea6\u91cf\u7a7a\u95f4\uff0c\u6709\u6548\u6355\u6349\u4e86\u8bed\u8a00\u7684\u5185\u5728\u7279\u6027\uff0c\u5e76\u4e3a\u8bed\u8a00\u5206\u7c7b\u548c\u6f14\u53d8\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u53d1\u73b0\u3002"}}
{"id": "2508.11758", "pdf": "https://arxiv.org/pdf/2508.11758", "abs": "https://arxiv.org/abs/2508.11758", "authors": ["Jonas van Elburg", "Peter van der Putten", "Maarten Marx"], "title": "Can we Evaluate RAGs with Synthetic Data?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for the SynDAiTE workshop at the European Conference on\n  Machine Learning and Principles and Practice of Knowledge Discovery in\n  Databases (ECML-PKDD 2025), September 15, 2025 - Porto, Portugal", "summary": "We investigate whether synthetic question-answer (QA) data generated by large\nlanguage models (LLMs) can serve as an effective proxy for human-labeled\nbenchmarks when such data is unavailable. We assess the reliability of\nsynthetic benchmarks across two experiments: one varying retriever parameters\nwhile keeping the generator fixed, and another varying the generator with fixed\nretriever parameters. Across four datasets, of which two open-domain and two\nproprietary, we find that synthetic benchmarks reliably rank the RAGs varying\nin terms of retriever configuration, aligning well with human-labeled benchmark\nbaselines. However, they fail to produce consistent RAG rankings when comparing\ngenerator architectures. The breakdown possibly arises from a combination of\ntask mismatch between the synthetic and human benchmarks, and stylistic bias\nfavoring certain generators.", "AI": {"tldr": "\u7814\u7a76LLM\u751f\u6210\u7684\u5408\u6210QA\u6570\u636e\u80fd\u5426\u4f5c\u4e3a\u4eba\u5de5\u57fa\u51c6\u7684\u6709\u6548\u66ff\u4ee3\u54c1\u6765\u8bc4\u4f30RAG\u7cfb\u7edf\u3002", "motivation": "\u5f53\u7f3a\u4e4f\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\u6570\u636e\u65f6\uff0c\u63a2\u7a76\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u7684\u5408\u6210\u95ee\u7b54\uff08QA\uff09\u6570\u636e\u662f\u5426\u80fd\u4f5c\u4e3a\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u7684\u53ef\u9760\u66ff\u4ee3\u54c1\u3002", "method": "\u901a\u8fc7\u4e24\u9879\u5b9e\u9a8c\u8bc4\u4f30\u5408\u6210\u57fa\u51c6\u7684\u53ef\u9760\u6027\uff1a\u4e00\u9879\u5b9e\u9a8c\u56fa\u5b9a\u751f\u6210\u5668\u5e76\u6539\u53d8\u68c0\u7d22\u5668\u53c2\u6570\uff0c\u53e6\u4e00\u9879\u5b9e\u9a8c\u56fa\u5b9a\u68c0\u7d22\u5668\u5e76\u6539\u53d8\u751f\u6210\u5668\u67b6\u6784\u3002\u5b9e\u9a8c\u5728\u56db\u4e2a\u6570\u636e\u96c6\uff08\u4e24\u4e2a\u5f00\u653e\u57df\uff0c\u4e24\u4e2a\u4e13\u6709\uff09\u4e0a\u8fdb\u884c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5408\u6210\u57fa\u51c6\u5728\u8bc4\u4f30\u4e0d\u540c\u68c0\u7d22\u5668\u914d\u7f6e\u7684RAG\u7cfb\u7edf\u65f6\u80fd\u53ef\u9760\u5730\u6392\u540d\uff0c\u4e14\u4e0e\u4eba\u5de5\u6807\u6ce8\u57fa\u51c6\u8868\u73b0\u4e00\u81f4\u3002\u7136\u800c\uff0c\u5728\u6bd4\u8f83\u4e0d\u540c\u751f\u6210\u5668\u67b6\u6784\u65f6\uff0c\u5408\u6210\u57fa\u51c6\u672a\u80fd\u4ea7\u751f\u4e00\u81f4\u7684RAG\u6392\u540d\u3002", "conclusion": "\u5408\u6210QA\u6570\u636e\u53ef\u6709\u6548\u7528\u4e8e\u8bc4\u4f30RAG\u7cfb\u7edf\u4e2d\u7684\u68c0\u7d22\u5668\u914d\u7f6e\uff0c\u4f46\u7531\u4e8e\u53ef\u80fd\u5b58\u5728\u7684\u4efb\u52a1\u4e0d\u5339\u914d\u548c\u5bf9\u7279\u5b9a\u751f\u6210\u5668\u7684\u98ce\u683c\u504f\u89c1\uff0c\u4e0d\u9002\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u5668\u67b6\u6784\u3002"}}
{"id": "2508.11767", "pdf": "https://arxiv.org/pdf/2508.11767", "abs": "https://arxiv.org/abs/2508.11767", "authors": ["Noah Kasmanoff", "Rahul Zalkikar"], "title": "Limitation Learning: Catching Adverse Dialog with GAIL", "categories": ["cs.CL", "cs.LG"], "comment": "Paper from 2021", "summary": "Imitation learning is a proven method for creating a policy in the absence of\nrewards, by leveraging expert demonstrations. In this work, we apply imitation\nlearning to conversation. In doing so, we recover a policy capable of talking\nto a user given a prompt (input state), and a discriminator capable of\nclassifying between expert and synthetic conversation. While our policy is\neffective, we recover results from our discriminator that indicate the\nlimitations of dialog models. We argue that this technique can be used to\nidentify adverse behavior of arbitrary data models common for dialog oriented\ntasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u6a21\u4eff\u5b66\u4e60\u5e94\u7528\u4e8e\u5bf9\u8bdd\u751f\u6210\uff0c\u6210\u529f\u8bad\u7ec3\u51fa\u5bf9\u8bdd\u7b56\u7565\u548c\u5224\u522b\u5668\u3002\u5224\u522b\u5668\u63ed\u793a\u4e86\u5bf9\u8bdd\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u8be5\u6280\u672f\u53ef\u7528\u4e8e\u8bc6\u522b\u5bf9\u8bdd\u6570\u636e\u6a21\u578b\u7684\u5f02\u5e38\u884c\u4e3a\u3002", "motivation": "\u5728\u7f3a\u4e4f\u5956\u52b1\u4fe1\u53f7\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u4e13\u5bb6\u6f14\u793a\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u521b\u5efa\u5bf9\u8bdd\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u4e0e\u7528\u6237\u8fdb\u884c\u5bf9\u8bdd\u3002", "method": "\u5c06\u6a21\u4eff\u5b66\u4e60\u5e94\u7528\u4e8e\u5bf9\u8bdd\u4efb\u52a1\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2a\u80fd\u6839\u636e\u63d0\u793a\u4e0e\u7528\u6237\u5bf9\u8bdd\u7684\u7b56\u7565\u6a21\u578b\uff0c\u4ee5\u53ca\u4e00\u4e2a\u80fd\u533a\u5206\u4e13\u5bb6\u5bf9\u8bdd\u548c\u5408\u6210\u5bf9\u8bdd\u7684\u5224\u522b\u5668\u3002", "result": "\u6240\u8bad\u7ec3\u7684\u5bf9\u8bdd\u7b56\u7565\u662f\u6709\u6548\u7684\u3002\u7136\u800c\uff0c\u5224\u522b\u5668\u7684\u5206\u6790\u7ed3\u679c\u63ed\u793a\u4e86\u73b0\u6709\u5bf9\u8bdd\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u6240\u91c7\u7528\u7684\u6280\u672f\uff08\u7279\u522b\u662f\u5224\u522b\u5668\uff09\u53ef\u7528\u4e8e\u8bc6\u522b\u548c\u63ed\u793a\u9762\u5411\u5bf9\u8bdd\u4efb\u52a1\u7684\u4efb\u610f\u6570\u636e\u6a21\u578b\u7684\u4e0d\u8db3\u6216\u5f02\u5e38\u884c\u4e3a\u3002"}}
{"id": "2508.11771", "pdf": "https://arxiv.org/pdf/2508.11771", "abs": "https://arxiv.org/abs/2508.11771", "authors": ["Leo Peckham", "Michael Ong", "Naomi Nagy", "Ewan Dunbar"], "title": "Investigating Transcription Normalization in the Faetar ASR Benchmark", "categories": ["cs.CL"], "comment": null, "summary": "We examine the role of transcription inconsistencies in the Faetar Automatic\nSpeech Recognition benchmark, a challenging low-resource ASR benchmark. With\nthe help of a small, hand-constructed lexicon, we conclude that find that,\nwhile inconsistencies do exist in the transcriptions, they are not the main\nchallenge in the task. We also demonstrate that bigram word-based language\nmodelling is of no added benefit, but that constraining decoding to a finite\nlexicon can be beneficial. The task remains extremely difficult.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4f4e\u8d44\u6e90Faetar ASR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8f6c\u5f55\u4e0d\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u5176\u5e76\u975e\u4e3b\u8981\u6311\u6218\uff1b\u8bcd\u7ea7\u522b\u4e8c\u5143\u8bed\u8a00\u6a21\u578b\u65e0\u76ca\uff0c\u4f46\u6709\u9650\u8bcd\u5178\u89e3\u7801\u6709\u76ca\uff1b\u8be5\u4efb\u52a1\u4ecd\u6781\u5177\u6311\u6218\u6027\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4f4e\u8d44\u6e90\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u57fa\u51c6\u6d4b\u8bd5Faetar\u4e2d\uff0c\u8f6c\u5f55\u4e0d\u4e00\u81f4\u6027\u6240\u626e\u6f14\u7684\u89d2\u8272\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u548c\u89e3\u7801\u7b56\u7565\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u5c0f\u578b\u624b\u5de5\u8bcd\u5178\u6765\u5206\u6790\u8f6c\u5f55\u4e0d\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u8bc4\u4f30\u4e86\u8bcd\u7ea7\u522b\u4e8c\u5143\u8bed\u8a00\u6a21\u578b\uff08bigram word-based language modelling\uff09\u548c\u6709\u9650\u8bcd\u5178\u89e3\u7801\u7ea6\u675f\uff08constraining decoding to a finite lexicon\uff09\u5bf9Faetar ASR\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0Faetar\u8f6c\u5f55\u4e2d\u786e\u5b9e\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u4f46\u5176\u5e76\u975e\u8be5\u4efb\u52a1\u7684\u4e3b\u8981\u6311\u6218\u3002\u8bcd\u7ea7\u522b\u4e8c\u5143\u8bed\u8a00\u6a21\u578b\u5e76\u672a\u5e26\u6765\u989d\u5916\u6536\u76ca\uff0c\u4f46\u5c06\u89e3\u7801\u9650\u5236\u5728\u6709\u9650\u8bcd\u5178\u5185\u53ef\u80fd\u6709\u6240\u88e8\u76ca\u3002", "conclusion": "\u5c3d\u7ba1\u5c1d\u8bd5\u4e86\u4e0d\u540c\u7684\u65b9\u6cd5\uff0cFaetar ASR\u4efb\u52a1\u4ecd\u7136\u6781\u5176\u56f0\u96be\uff0c\u8f6c\u5f55\u4e0d\u4e00\u81f4\u6027\u5e76\u975e\u4e3b\u8981\u75c7\u7ed3\u6240\u5728\u3002"}}
{"id": "2508.11842", "pdf": "https://arxiv.org/pdf/2508.11842", "abs": "https://arxiv.org/abs/2508.11842", "authors": ["Huayi Wang", "Jingfan Meng", "Jun Xu"], "title": "OddEEC: A New Sketch Technique for Error Estimating Coding", "categories": ["cs.NI"], "comment": "This is an extended version of the paper accepted at the 33rd IEEE\n  International Conference on Network Protocols (ICNP 2025)", "summary": "Error estimating coding (EEC) is a standard technique for estimating the\nnumber of bit errors during packet transmission over wireless networks. In this\npaper, we propose OddEEC, a novel EEC scheme. OddEEC is a nontrivial adaptation\nof a data sketching technique named Odd Sketch to EEC, addressing new\nchallenges therein by its bit sampling technique and maximum likelihood\nestimator. Our experiments show that OddEEC overall achieves comparable\nestimation accuracy as competing schemes such as gEEC and mEEC, with much\nsmaller decoding complexity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOddEEC\u7684\u65b0\u578b\u9519\u8bef\u4f30\u8ba1\u7f16\u7801\u65b9\u6848\uff0c\u5b83\u57fa\u4e8eOdd Sketch\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u4e0e\u73b0\u6709\u65b9\u6848\u76f8\u5f53\u7684\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u4f46\u89e3\u7801\u590d\u6742\u5ea6\u5927\u5927\u964d\u4f4e\u3002", "motivation": "\u9519\u8bef\u4f30\u8ba1\u7f16\u7801(EEC)\u662f\u65e0\u7ebf\u7f51\u7edc\u4e2d\u4f30\u8ba1\u6570\u636e\u5305\u4f20\u8f93\u671f\u95f4\u6bd4\u7279\u9519\u8bef\u6570\u91cf\u7684\u6807\u51c6\u6280\u672f\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684EEC\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9\u73b0\u6709\u6280\u672f\u4e2d\u7684\u65b0\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51faOddEEC\u65b9\u6848\uff0c\u5b83\u662f\u6570\u636e\u7d20\u63cf\u6280\u672fOdd Sketch\u5728EEC\u4e2d\u7684\u975e\u5e73\u51e1\u6539\u7f16\uff0c\u901a\u8fc7\u5176\u4f4d\u91c7\u6837\u6280\u672f\u548c\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\u6765\u89e3\u51b3\u76f8\u5173\u6311\u6218\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOddEEC\u7684\u6574\u4f53\u4f30\u8ba1\u7cbe\u5ea6\u4e0egEEC\u548cmEEC\u7b49\u7ade\u4e89\u65b9\u6848\u76f8\u5f53\uff0c\u4f46\u5176\u89e3\u7801\u590d\u6742\u5ea6\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "OddEEC\u662f\u4e00\u79cd\u5177\u6709\u7ade\u4e89\u529b\u7684EEC\u65b9\u6848\uff0c\u5b83\u5728\u4fdd\u6301\u76f8\u4f3c\u4f30\u8ba1\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u89e3\u7801\u590d\u6742\u5ea6\uff0c\u4ece\u800c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u9519\u8bef\u4f30\u8ba1\u65b9\u6cd5\u3002"}}
{"id": "2508.11696", "pdf": "https://arxiv.org/pdf/2508.11696", "abs": "https://arxiv.org/abs/2508.11696", "authors": ["Sami Sadat", "Mohammad Irtiza Hossain", "Junaid Ahmed Sifat", "Suhail Haque Rafi", "Md. Waseq Alauddin Alvi", "Md. Khalilur Rhaman"], "title": "A Deep Learning-Based CCTV System for Automatic Smoking Detection in Fire Exit Zones", "categories": ["cs.CV"], "comment": null, "summary": "A deep learning real-time smoking detection system for CCTV surveillance of\nfire exit areas is proposed due to critical safety requirements. The dataset\ncontains 8,124 images from 20 different scenarios along with 2,708 raw samples\ndemonstrating low-light areas. We evaluated three advanced object detection\nmodels: YOLOv8, YOLOv11, and YOLOv12, followed by development of a custom model\nderived from YOLOv8 with added structures for challenging surveillance\ncontexts. The proposed model outperformed the others, achieving a recall of\n78.90 percent and mAP at 50 of 83.70 percent, delivering optimal object\ndetection across varied environments. Performance evaluation on multiple edge\ndevices using multithreaded operations showed the Jetson Xavier NX processed\ndata at 52 to 97 milliseconds per inference, establishing its suitability for\ntime-sensitive operations. This system offers a robust and adaptable platform\nfor monitoring public safety and enabling automatic regulatory compliance.", "AI": {"tldr": "\u4e3a\u6ee1\u8db3\u6d88\u9632\u901a\u9053\u7684\u5173\u952e\u5b89\u5168\u9700\u6c42\uff0c\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5b9e\u65f6\u5438\u70df\u68c0\u6d4b\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5728\u590d\u6742\u76d1\u63a7\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fd0\u884c\u3002", "motivation": "\u7531\u4e8e\u6d88\u9632\u901a\u9053\u533a\u57df\u5b58\u5728\u5173\u952e\u5b89\u5168\u8981\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u5b9e\u65f6\u5438\u70df\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4ee5\u76d1\u63a7\u516c\u5171\u5b89\u5168\u5e76\u5b9e\u73b0\u81ea\u52a8\u6cd5\u89c4\u9075\u4ece\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b8124\u5f20\u56fe\u7247\uff08\u5305\u62ec2708\u5f20\u4f4e\u5149\u7167\u6837\u672c\uff09\u7684\u6570\u636e\u96c6\u3002\u8bc4\u4f30\u4e86YOLOv8\u3001YOLOv11\u548cYOLOv12\u4e09\u79cd\u5148\u8fdb\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eYOLOv8\u5e76\u4e3a\u590d\u6742\u76d1\u63a7\u73af\u5883\u4f18\u5316\u7684\u5b9a\u5236\u6a21\u578b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u5b9a\u5236\u6a21\u578b\u6027\u80fd\u6700\u4f73\uff0c\u53ec\u56de\u7387\u8fbe78.90%\uff0cmAP@50\u8fbe83.70%\u3002\u5728\u591a\u7ebf\u7a0b\u64cd\u4f5c\u4e0b\uff0cJetson Xavier NX\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u65f6\u95f4\u4e3a52\u81f397\u6beb\u79d2\uff0c\u8bc1\u5b9e\u5176\u9002\u7528\u4e8e\u65f6\u95f4\u654f\u611f\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u5e73\u53f0\uff0c\u53ef\u7528\u4e8e\u76d1\u63a7\u516c\u5171\u5b89\u5168\u5e76\u5b9e\u73b0\u81ea\u52a8\u6cd5\u89c4\u9075\u4ece\u3002"}}
{"id": "2508.11836", "pdf": "https://arxiv.org/pdf/2508.11836", "abs": "https://arxiv.org/abs/2508.11836", "authors": ["Dave Goel", "Matthew Guzdial", "Anurag Sarkar"], "title": "Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video", "categories": ["cs.AI"], "comment": null, "summary": "World models are defined as a compressed spatial and temporal learned\nrepresentation of an environment. The learned representation is typically a\nneural network, making transfer of the learned environment dynamics and\nexplainability a challenge. In this paper, we propose an approach, Finite\nAutomata Extraction (FAE), that learns a neuro-symbolic world model from\ngameplay video represented as programs in a novel domain-specific language\n(DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more\nprecise model of the environment and more general code than prior DSL-based\napproaches.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6709\u9650\u81ea\u52a8\u673a\u63d0\u53d6\uff08FAE\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u65b0\u9896\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08Retro Coder\uff09\u4ece\u6e38\u620f\u89c6\u9891\u4e2d\u5b66\u4e60\u795e\u7ecf\u7b26\u53f7\u4e16\u754c\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4e16\u754c\u6a21\u578b\u5728\u53ef\u8fc1\u79fb\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u548c\u901a\u7528\u7684\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u4e16\u754c\u6a21\u578b\u5728\u5b66\u4e60\u5230\u7684\u73af\u5883\u52a8\u6001\u7684\u53ef\u8fc1\u79fb\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u6709\u9650\u81ea\u52a8\u673a\u63d0\u53d6\uff08FAE\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ece\u4ee5\u65b0\u578b\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09Retro Coder\u8868\u793a\u7684\u6e38\u620f\u89c6\u9891\u4e2d\uff0c\u5b66\u4e60\u795e\u7ecf\u7b26\u53f7\u4e16\u754c\u6a21\u578b\u3002", "result": "\u4e0e\u73b0\u6709\u4e16\u754c\u6a21\u578b\u65b9\u6cd5\u76f8\u6bd4\uff0cFAE\u80fd\u591f\u5b66\u4e60\u5230\u66f4\u7cbe\u786e\u7684\u73af\u5883\u6a21\u578b\uff1b\u4e0e\u73b0\u6709\u57fa\u4e8eDSL\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cFAE\u80fd\u751f\u6210\u66f4\u901a\u7528\u7684\u4ee3\u7801\u3002", "conclusion": "FAE\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u795e\u7ecf\u7b26\u53f7\u4e16\u754c\u6a21\u578b\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u6a21\u578b\u7cbe\u5ea6\u548c\u4ee3\u7801\u901a\u7528\u6027\u4e0a\u7684\u5c40\u9650\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u4e14\u901a\u7528\u7684\u73af\u5883\u8868\u793a\u3002"}}
{"id": "2508.11661", "pdf": "https://arxiv.org/pdf/2508.11661", "abs": "https://arxiv.org/abs/2508.11661", "authors": ["Ziyi Cao", "Qingyi Si", "Jingbin Zhang", "Bingquan Liu"], "title": "Sparse Attention across Multiple-context KV Cache", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language models face significant cost challenges in long-sequence\ninference. To address this, reusing historical Key-Value (KV) Cache for\nimproved inference efficiency has become a mainstream approach. Recent advances\nfurther enhance throughput by sparse attention mechanisms to select the most\nrelevant KV Cache, thereby reducing sequence length. However, such techniques\nare limited to single-context scenarios, where historical KV Cache is computed\nsequentially with causal-attention dependencies. In retrieval-augmented\ngeneration (RAG) scenarios, where retrieved documents as context are unknown\nbeforehand, each document's KV Cache is computed and stored independently\n(termed multiple-context KV Cache), lacking cross-attention between contexts.\nThis renders existing methods ineffective. Although prior work partially\nrecomputes multiple-context KV Cache to mitigate accuracy loss from missing\ncross-attention, it requires retaining all KV Cache throughout, failing to\nreduce memory overhead. This paper presents SamKV, the first exploration of\nattention sparsification for multiple-context KV Cache. Specifically, SamKV\ntakes into account the complementary information of other contexts when\nsparsifying one context, and then locally recomputes the sparsified\ninformation. Experiments demonstrate that our method compresses sequence length\nto 15% without accuracy degradation compared with full-recompuation baselines,\nsignificantly boosting throughput in multi-context RAG scenarios.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u591a\u4e0a\u4e0b\u6587RAG\u573a\u666f\u4e0bLLM\u957f\u5e8f\u5217\u63a8\u7406\u4e2dKV\u7f13\u5b58\u7684\u6548\u7387\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86SamKV\uff0c\u4e00\u79cd\u9996\u6b21\u7528\u4e8e\u591a\u4e0a\u4e0b\u6587KV\u7f13\u5b58\u7684\u6ce8\u610f\u529b\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u80fd\u5728\u4e0d\u5f71\u54cd\u51c6\u786e\u7387\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u538b\u7f29\u5e8f\u5217\u957f\u5ea6\u5e76\u63d0\u5347\u541e\u5410\u91cf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u5e8f\u5217\u63a8\u7406\u4e2d\u9762\u4e34\u9ad8\u6602\u6210\u672c\u3002\u73b0\u6709KV\u7f13\u5b58\u590d\u7528\u548c\u7a00\u758f\u5316\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5355\u4e0a\u4e0b\u6587\u573a\u666f\uff0c\u5728RAG\u7b49\u591a\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u56e0\u4e0a\u4e0b\u6587\u72ec\u7acb\u8ba1\u7b97\u4e14\u7f3a\u4e4f\u4ea4\u53c9\u6ce8\u610f\u529b\u800c\u5931\u6548\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6709\u6548\u964d\u4f4e\u5185\u5b58\u5f00\u9500\u548c\u63d0\u5347\u6548\u7387\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86SamKV\uff0c\u9996\u6b21\u63a2\u7d22\u4e86\u591a\u4e0a\u4e0b\u6587KV\u7f13\u5b58\u7684\u6ce8\u610f\u529b\u7a00\u758f\u5316\u3002\u5177\u4f53\u65b9\u6cd5\u662f\u5728\u7a00\u758f\u5316\u67d0\u4e00\u4e0a\u4e0b\u6587\u65f6\uff0c\u8003\u8651\u5176\u4ed6\u4e0a\u4e0b\u6587\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u7136\u540e\u5bf9\u7a00\u758f\u5316\u7684\u4fe1\u606f\u8fdb\u884c\u5c40\u90e8\u91cd\u8ba1\u7b97\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5b8c\u5168\u91cd\u8ba1\u7b97\u57fa\u7ebf\u76f8\u6bd4\uff0cSamKV\u5728\u4e0d\u635f\u5931\u51c6\u786e\u7387\u7684\u60c5\u51b5\u4e0b\u5c06\u5e8f\u5217\u957f\u5ea6\u538b\u7f29\u81f315%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e0a\u4e0b\u6587RAG\u573a\u666f\u4e0b\u7684\u541e\u5410\u91cf\u3002", "conclusion": "SamKV\u6210\u529f\u89e3\u51b3\u4e86\u591a\u4e0a\u4e0b\u6587RAG\u573a\u666f\u4e2dKV\u7f13\u5b58\u7a00\u758f\u5316\u7684\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u7a00\u758f\u5316\u548c\u5c40\u90e8\u91cd\u8ba1\u7b97\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5e8f\u5217\u957f\u5ea6\u538b\u7f29\u548c\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u51c6\u786e\u6027\u3002"}}
{"id": "2508.11779", "pdf": "https://arxiv.org/pdf/2508.11779", "abs": "https://arxiv.org/abs/2508.11779", "authors": ["Tianyi Li", "Yu Qin", "Olivia R. Liu Sheng"], "title": "A Multi-Task Evaluation of LLMs' Processing of Academic Text Input", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "How much large language models (LLMs) can aid scientific discovery, notably\nin assisting academic peer review, is in heated debate. Between a literature\ndigest and a human-comparable research assistant lies their practical\napplication potential. We organize individual tasks that computer science\nstudies employ in separate terms into a guided and robust workflow to evaluate\nLLMs' processing of academic text input. We employ four tasks in the\nassessment: content reproduction/comparison/scoring/reflection, each demanding\na specific role of the LLM (oracle/judgmental arbiter/knowledgeable\narbiter/collaborator) in assisting scholarly works, and altogether testing LLMs\nwith questions that increasingly require intellectual capabilities towards a\nsolid understanding of scientific texts to yield desirable solutions. We\nexemplify a rigorous performance evaluation with detailed instructions on the\nprompts. Adopting first-rate Information Systems articles at three top journals\nas the input texts and an abundant set of text metrics, we record a compromised\nperformance of the leading LLM - Google's Gemini: its summary and paraphrase of\nacademic text is acceptably reliable; using it to rank texts through pairwise\ntext comparison is faintly scalable; asking it to grade academic texts is prone\nto poor discrimination; its qualitative reflection on the text is\nself-consistent yet hardly insightful to inspire meaningful research. This\nevidence against an endorsement of LLMs' text-processing capabilities is\nconsistent across metric-based internal (linguistic assessment), external\n(comparing to the ground truth), and human evaluation, and is robust to the\nvariations of the prompt. Overall, we do not recommend an unchecked use of LLMs\nin constructing peer reviews.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b66\u672f\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u6587\u672c\u5904\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u8868\u73b0\u4e0d\u4f73\uff0c\u4e0d\u5efa\u8bae\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u672a\u7ecf\u68c0\u67e5\u5730\u4f7f\u7528LLMs\u3002", "motivation": "\u56f4\u7ed5\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u5426\u8f85\u52a9\u79d1\u5b66\u53d1\u73b0\uff0c\u5c24\u5176\u662f\u5728\u5b66\u672f\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u5e94\u7528\uff0c\u5b58\u5728\u6fc0\u70c8\u4e89\u8bba\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u5b66\u672f\u6587\u672c\u5904\u7406\u65b9\u9762\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u7a0b\u6765\u8bc4\u4f30LLMs\u5904\u7406\u5b66\u672f\u6587\u672c\u7684\u80fd\u529b\uff0c\u5305\u62ec\u56db\u4e2a\u4efb\u52a1\uff1a\u5185\u5bb9\u590d\u73b0/\u6bd4\u8f83/\u8bc4\u5206/\u53cd\u601d\u3002\u6bcf\u4e2a\u4efb\u52a1\u90fd\u8981\u6c42LLM\u626e\u6f14\u7279\u5b9a\u89d2\u8272\u3002\u8f93\u5165\u6587\u672c\u9009\u7528\u4fe1\u606f\u7cfb\u7edf\u9886\u57df\u9876\u7ea7\u671f\u520a\u7684\u4e00\u6d41\u6587\u7ae0\u3002\u91c7\u7528\u591a\u79cd\u6587\u672c\u5ea6\u91cf\u6807\u51c6\u8fdb\u884c\u5185\u90e8\u3001\u5916\u90e8\uff08\u4e0e\u771f\u503c\u6bd4\u8f83\uff09\u548c\u4eba\u5de5\u8bc4\u4f30\uff0c\u6d4b\u8bd5\u4e86Google\u7684Gemini\u6a21\u578b\uff0c\u5e76\u8003\u5bdf\u4e86\u63d0\u793a\u8bcd\u53d8\u5316\u7684\u5f71\u54cd\u3002", "result": "\u9886\u5148\u7684LLM\uff08Google Gemini\uff09\u8868\u73b0\u4e0d\u4f73\uff1a\u5176\u5b66\u672f\u6587\u672c\u603b\u7ed3\u548c\u8f6c\u8ff0\u53ef\u63a5\u53d7\uff1b\u901a\u8fc7\u6210\u5bf9\u6587\u672c\u6bd4\u8f83\u8fdb\u884c\u6392\u540d\u6269\u5c55\u6027\u5dee\uff1b\u7528\u4e8e\u5b66\u672f\u6587\u672c\u8bc4\u5206\u65f6\u9274\u522b\u529b\u5dee\uff1b\u5bf9\u6587\u672c\u7684\u5b9a\u6027\u53cd\u601d\u867d\u7136\u81ea\u6d3d\u4f46\u7f3a\u4e4f\u6d1e\u5bdf\u529b\uff0c\u96be\u4ee5\u6fc0\u53d1\u6709\u610f\u4e49\u7684\u7814\u7a76\u3002\u8fd9\u4e9b\u8bc1\u636e\u8868\u660e\u4e0d\u5e94\u8fc7\u5ea6\u8ba4\u53efLLMs\u7684\u6587\u672c\u5904\u7406\u80fd\u529b\uff0c\u4e14\u7ed3\u679c\u5728\u4e0d\u540c\u8bc4\u4f30\u65b9\u6cd5\u548c\u63d0\u793a\u8bcd\u53d8\u5316\u4e0b\u5747\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u603b\u4f53\u800c\u8a00\uff0c\u7814\u7a76\u4e0d\u5efa\u8bae\u5728\u6784\u5efa\u540c\u884c\u8bc4\u5ba1\u65f6\u672a\u7ecf\u68c0\u67e5\u5730\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2508.11971", "pdf": "https://arxiv.org/pdf/2508.11971", "abs": "https://arxiv.org/abs/2508.11971", "authors": ["Chenchen Fu", "Zining Zhou", "Xiaoxing Qiu", "Sujunjie Sun", "Weiwei Wu", "Song Han"], "title": "Bandit-Based Charging with Beamforming for Mobile Wireless-Powered IoT Systems", "categories": ["cs.NI"], "comment": null, "summary": "Wireless power transfer (WPT) is increasingly used to sustain\nInternet-of-Things (IoT) systems by wirelessly charging embedded devices.\nMobile chargers further enhance scalability in wireless-powered IoT (WP-IoT)\nnetworks, but pose new challenges due to dynamic channel conditions and limited\nenergy budgets. Most existing works overlook such dynamics or ignore real-time\nconstraints on charging schedules. This paper presents a bandit-based charging\nframework for WP-IoT systems using mobile chargers with practical beamforming\ncapabilities and real-time charging constraints. We explicitly consider\ntime-varying channel state information (CSI) and impose a strict charging\ndeadline in each round, which reflects the hard real-time constraint from the\ncharger's limited battery capacity. We formulate a temporal-spatial charging\npolicy that jointly determines the charging locations, durations, and\nbeamforming configurations. Area discretization enables polynomial-time\nenumeration with constant approximation bounds. We then propose two online\nbandit algorithms for both stationary and non-stationary unknown channel state\nscenarios with bounded regrets. Our extensive experimental results validate\nthat the proposed algorithms can rapidly approach the theoretical upper bound\nwhile effectively tracking the dynamic channel states for adaptive adjustment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8ebandit\u7b97\u6cd5\u7684\u5145\u7535\u6846\u67b6\uff0c\u89e3\u51b3\u79fb\u52a8\u5145\u7535\u5668\u5728\u52a8\u6001\u4fe1\u9053\u548c\u5b9e\u65f6\u7ea6\u675f\u4e0b\u7684\u65e0\u7ebf\u4f9b\u7535\u7269\u8054\u7f51\uff08WP-IoT\uff09\u7cfb\u7edf\u5145\u7535\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u79fb\u52a8\u5145\u7535\u5668\u65e0\u7ebf\u4f9b\u7535\u7269\u8054\u7f51\uff08WP-IoT\uff09\u7814\u7a76\u5ffd\u7565\u4e86\u52a8\u6001\u4fe1\u9053\u6761\u4ef6\u548c\u5145\u7535\u8c03\u5ea6\u4e2d\u7684\u5b9e\u65f6\u7ea6\u675f\uff08\u5982\u5145\u7535\u5668\u6709\u9650\u7684\u80fd\u91cf\u9884\u7b97\u548c\u4e25\u683c\u7684\u5145\u7535\u622a\u6b62\u65e5\u671f\uff09\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8ebandit\u7b97\u6cd5\u7684WP-IoT\u5145\u7535\u6846\u67b6\uff0c\u8003\u8651\u5b9e\u9645\u6ce2\u675f\u6210\u5f62\u80fd\u529b\u548c\u5b9e\u65f6\u5145\u7535\u7ea6\u675f\u3002\u5236\u5b9a\u4e86\u8054\u5408\u51b3\u5b9a\u5145\u7535\u4f4d\u7f6e\u3001\u6301\u7eed\u65f6\u95f4\u548c\u6ce2\u675f\u6210\u5f62\u914d\u7f6e\u7684\u65f6\u7a7a\u5145\u7535\u7b56\u7565\u3002\u901a\u8fc7\u533a\u57df\u79bb\u6563\u5316\u5b9e\u73b0\u591a\u9879\u5f0f\u65f6\u95f4\u679a\u4e3e\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u5728\u7ebfbandit\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u9759\u6001\u548c\u975e\u9759\u6001\u672a\u77e5\u4fe1\u9053\u72b6\u6001\u573a\u666f\uff0c\u5e76\u5177\u6709\u6709\u754c\u540e\u6094\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u5feb\u901f\u63a5\u8fd1\u7406\u8bba\u4e0a\u9650\uff0c\u5e76\u6709\u6548\u8ddf\u8e2a\u52a8\u6001\u4fe1\u9053\u72b6\u6001\u8fdb\u884c\u81ea\u9002\u5e94\u8c03\u6574\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8ebandit\u7b97\u6cd5\u7684\u6846\u67b6\u53ca\u5176\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u79fb\u52a8\u5145\u7535\u5668\u5728\u52a8\u6001\u4fe1\u9053\u548c\u5b9e\u65f6\u7ea6\u675f\u4e0b\u4e3aWP-IoT\u7cfb\u7edf\u4f9b\u7535\u7684\u6311\u6218\uff0c\u5e76\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11697", "pdf": "https://arxiv.org/pdf/2508.11697", "abs": "https://arxiv.org/abs/2508.11697", "authors": ["Adri\u00e1n Rodr\u00edguez-Mu\u00f1oz", "Manel Baradad", "Phillip Isola", "Antonio Torralba"], "title": "Separating Knowledge and Perception with Procedural Data", "categories": ["cs.CV", "cs.AI", "I.5.1"], "comment": "17 pages, 18 figures, 3 tables, to be published in ICML 2025", "summary": "We train representation models with procedural data only, and apply them on\nvisual similarity, classification, and semantic segmentation tasks without\nfurther training by using visual memory -- an explicit database of reference\nimage embeddings. Unlike prior work on visual memory, our approach achieves\nfull compartmentalization with respect to all real-world images while retaining\nstrong performance. Compared to a model trained on Places, our procedural model\nperforms within $1\\%$ on NIGHTS visual similarity, outperforms by $8\\%$ and\n$15\\%$ on CUB200 and Flowers102 fine-grained classification, and is within\n$10\\%$ on ImageNet-1K classification. It also demonstrates strong zero-shot\nsegmentation, achieving an $R^2$ on COCO within $10\\%$ of the models trained on\nreal data. Finally, we analyze procedural versus real data models, showing that\nparts of the same object have dissimilar representations in procedural models,\nresulting in incorrect searches in memory and explaining the remaining\nperformance gap.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u4ec5\u4f7f\u7528\u7a0b\u5e8f\u5316\u6570\u636e\u8bad\u7ec3\u8868\u793a\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u89c6\u89c9\u8bb0\u5fc6\u5e94\u7528\u4e8e\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u4e0e\u771f\u5b9e\u6570\u636e\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u548c\u96f6\u6837\u672c\u5206\u5272\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bf9\u771f\u5b9e\u4e16\u754c\u7684\u5b8c\u5168\u9694\u79bb\u3002", "motivation": "\u65e8\u5728\u63a2\u7d22\u4ec5\u4f7f\u7528\u7a0b\u5e8f\u5316\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u5426\u5728\u4e0d\u4f9d\u8d56\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e0a\u5b9e\u73b0\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5e76\u8fbe\u5230\u5bf9\u771f\u5b9e\u6570\u636e\u7684\u5b8c\u5168\u9694\u79bb\uff0c\u4ee5\u89e3\u51b3\u9690\u79c1\u6216\u6570\u636e\u83b7\u53d6\u9650\u5236\u7b49\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4ec5\u4f7f\u7528\u7a0b\u5e8f\u5316\u6570\u636e\u8bad\u7ec3\u8868\u793a\u6a21\u578b\uff0c\u5e76\u5229\u7528\u4e00\u4e2a\u663e\u5f0f\u7684\u53c2\u8003\u56fe\u50cf\u5d4c\u5165\u6570\u636e\u5e93\uff08\u5373\u89c6\u89c9\u8bb0\u5fc6\uff09\u6765\u6267\u884c\u89c6\u89c9\u76f8\u4f3c\u6027\u3001\u5206\u7c7b\u548c\u8bed\u4e49\u5206\u5272\u4efb\u52a1\uff0c\u65e0\u9700\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u4e0e\u5728Places\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u7a0b\u5e8f\u5316\u6a21\u578b\u5728NIGHTS\u89c6\u89c9\u76f8\u4f3c\u6027\u4efb\u52a1\u4e0a\u6027\u80fd\u5dee\u8ddd\u57281%\u4ee5\u5185\uff0c\u5728CUB200\u548cFlowers102\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4e0a\u5206\u522b\u4f18\u4e8e8%\u548c15%\uff0c\u5728ImageNet-1K\u5206\u7c7b\u4e0a\u6027\u80fd\u5dee\u8ddd\u572810%\u4ee5\u5185\u3002\u6b64\u5916\uff0c\u5728COCO\u96f6\u6837\u672c\u5206\u5272\u4efb\u52a1\u4e0a\uff0c\u5176R^2\u503c\u4e0e\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u4ec5\u76f8\u5dee10%\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u7a0b\u5e8f\u5316\u6a21\u578b\u4e2d\u540c\u4e00\u5bf9\u8c61\u7684\u4e0d\u540c\u90e8\u5206\u53ef\u80fd\u5177\u6709\u4e0d\u76f8\u4f3c\u7684\u8868\u793a\uff0c\u8fd9\u89e3\u91ca\u4e86\u6027\u80fd\u5dee\u8ddd\u7684\u539f\u56e0\u3002", "conclusion": "\u4ec5\u7528\u7a0b\u5e8f\u5316\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u7ed3\u5408\u89c6\u89c9\u8bb0\u5fc6\uff0c\u53ef\u4ee5\u6709\u6548\u5e94\u5bf9\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\uff0c\u5e76\u5c55\u73b0\u51fa\u4e0e\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u76f8\u5f53\u7684\u5f3a\u5927\u6027\u80fd\uff0c\u5c24\u5176\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u548c\u96f6\u6837\u672c\u5206\u5272\u65b9\u9762\u6709\u4f18\u52bf\u3002\u5176\u72ec\u7279\u4e4b\u5904\u5728\u4e8e\u5b9e\u73b0\u4e86\u5bf9\u771f\u5b9e\u6570\u636e\u7684\u5b8c\u5168\u9694\u79bb\u3002\u672a\u6765\u53ef\u901a\u8fc7\u89e3\u51b3\u540c\u5bf9\u8c61\u90e8\u5206\u8868\u793a\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u6765\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002"}}
