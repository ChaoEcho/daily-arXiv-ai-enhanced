{"id": "2509.22834", "pdf": "https://arxiv.org/pdf/2509.22834", "abs": "https://arxiv.org/abs/2509.22834", "authors": ["Anis Bekri", "Amar Abane", "Abdella Battou", "Saddek Bensalem"], "title": "Bridging Language Models and Formal Methods for Intent-Driven Optical Network Design", "categories": ["cs.NI", "cs.AI"], "comment": "Accepted at AICCSA 2025", "summary": "Intent-Based Networking (IBN) aims to simplify network management by enabling\nusers to specify high-level goals that drive automated network design and\nconfiguration. However, translating informal natural-language intents into\nformally correct optical network topologies remains challenging due to inherent\nambiguity and lack of rigor in Large Language Models (LLMs). To address this,\nwe propose a novel hybrid pipeline that integrates LLM-based intent parsing,\nformal methods, and Optical Retrieval-Augmented Generation (RAG). By enriching\ndesign decisions with domain-specific optical standards and systematically\nincorporating symbolic reasoning and verification techniques, our pipeline\ngenerates explainable, verifiable, and trustworthy optical network designs.\nThis approach significantly advances IBN by ensuring reliability and\ncorrectness, essential for mission-critical networking tasks.", "AI": {"tldr": "\u4e3a\u89e3\u51b3LLM\u5728IBN\u4e2d\u5c06\u81ea\u7136\u8bed\u8a00\u610f\u56fe\u8f6c\u6362\u4e3a\u6b63\u5f0f\u5149\u5b66\u7f51\u7edc\u62d3\u6251\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408LLM\u610f\u56fe\u89e3\u6790\u3001\u5f62\u5f0f\u5316\u65b9\u6cd5\u548c\u5149\u5b66RAG\u7684\u6df7\u5408\u7ba1\u9053\uff0c\u4ee5\u751f\u6210\u53ef\u89e3\u91ca\u3001\u53ef\u9a8c\u8bc1\u4e14\u503c\u5f97\u4fe1\u8d56\u7684\u5149\u5b66\u7f51\u7edc\u8bbe\u8ba1\u3002", "motivation": "\u610f\u56fe\u9a71\u52a8\u7f51\u7edc(IBN)\u65e8\u5728\u901a\u8fc7\u9ad8\u5c42\u76ee\u6807\u81ea\u52a8\u5316\u7f51\u7edc\u8bbe\u8ba1\uff0c\u4f46\u5c06\u975e\u6b63\u5f0f\u81ea\u7136\u8bed\u8a00\u610f\u56fe\u8f6c\u6362\u4e3a\u6b63\u5f0f\u51c6\u786e\u7684\u5149\u5b66\u7f51\u7edc\u62d3\u6251\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u56fa\u6709\u7684\u6a21\u7cca\u6027\u548c\u7f3a\u4e4f\u4e25\u8c28\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u96c6\u6210\u4e86LLM\u610f\u56fe\u89e3\u6790\u3001\u5f62\u5f0f\u5316\u65b9\u6cd5\u548c\u5149\u5b66\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u3002\u901a\u8fc7\u5f15\u5165\u7279\u5b9a\u9886\u57df\u7684\u5149\u5b66\u6807\u51c6\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u548c\u9a8c\u8bc1\u6280\u672f\u6765\u4e30\u5bcc\u8bbe\u8ba1\u51b3\u7b56\u3002", "result": "\u6210\u529f\u751f\u6210\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u9a8c\u8bc1\u4e14\u503c\u5f97\u4fe1\u8d56\u7684\u5149\u5b66\u7f51\u7edc\u8bbe\u8ba1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63a8\u52a8\u4e86IBN\u9886\u57df\u7684\u53d1\u5c55\uff0c\u786e\u4fdd\u4e86\u4efb\u52a1\u5173\u952e\u578b\u7f51\u7edc\u4efb\u52a1\u7684\u53ef\u9760\u6027\u548c\u6b63\u786e\u6027\u3002"}}
{"id": "2509.23125", "pdf": "https://arxiv.org/pdf/2509.23125", "abs": "https://arxiv.org/abs/2509.23125", "authors": ["Yiqing Zhou", "Xule Zhou", "Zecan Cheng", "Chenao Lu", "Junhan Chen", "Jiahong Pan", "Yizhuo Liu", "Sihao Li", "Kyeong Soo Kim"], "title": "Impact of Environmental Factors on LoRa 2.4 GHz Time of Flight Ranging Outdoors", "categories": ["cs.NI", "cs.LG"], "comment": "5 pages, 8 figures, 2 tables, and under review for presentation at a\n  workshop", "summary": "In WSN/IoT, node localization is essential to long-running applications for\naccurate environment monitoring and event detection, often covering a large\narea in the field. Due to the lower time resolution of typical WSN/IoT\nplatforms (e.g., 1 microsecond on ESP32 platforms) and the jitters in\ntimestamping, packet-level localization techniques cannot provide meter-level\nresolution. For high-precision localization as well as world-wide\ninteroperability via 2.4-GHz ISM band, a new variant of LoRa, called LoRa 2.4\nGHz, was proposed by semtech, which provides a radio frequency (RF) time of\nflight (ToF) ranging method for meter-level localization. However, the existing\ndatasets reported in the literature are limited in their coverages and do not\ntake into account varying environmental factors such as temperature and\nhumidity. To address these issues, LoRa 2.4 GHz RF ToF ranging data was\ncollected on a sports field at the XJTLU south campus, where three LoRa nodes\nlogged samples of ranging with a LoRa base station, together with temperature\nand humidity, at reference points arranged as a 3x3 grid covering 400 square\nmeter over three weeks and uploaded all measurement records to the base station\nequipped with an ESP32-based transceiver for machine and user communications.\nThe results of a preliminary investigation based on a simple deep neural\nnetwork (DNN) model demonstrate that the environmental factors, including the\ntemperature and humidity, significantly affect the accuracy of ranging, which\ncalls for advanced methods of compensating for the effects of environmental\nfactors on LoRa RF ToF ranging outdoors.", "AI": {"tldr": "\u9488\u5bf9WSN/IoT\u8282\u70b9\u5b9a\u4f4d\u7cbe\u5ea6\u53d7\u9650\u95ee\u9898\uff0c\u672c\u6587\u5229\u7528LoRa 2.4 GHz\u7684RF ToF\u6d4b\u8ddd\u65b9\u6cd5\uff0c\u6536\u96c6\u4e86\u4e00\u4e2a\u5305\u542b\u6e29\u5ea6\u548c\u6e7f\u5ea6\u7684\u591a\u5468\u6237\u5916\u6d4b\u8ddd\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u521d\u6b65DNN\u5206\u6790\u8bc1\u660e\u73af\u5883\u56e0\u7d20\u5bf9\u6d4b\u8ddd\u7cbe\u5ea6\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709WSN/IoT\u5b9a\u4f4d\u6280\u672f\u7cbe\u5ea6\u4e0d\u8db3\uff08\u65e0\u6cd5\u8fbe\u5230\u7c73\u7ea7\uff09\uff0c\u5c3d\u7ba1LoRa 2.4 GHz\u63d0\u4f9b\u4e86\u7c73\u7ea7RF ToF\u6d4b\u8ddd\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u8986\u76d6\u8303\u56f4\u6709\u9650\u4e14\u672a\u8003\u8651\u6e29\u5ea6\u3001\u6e7f\u5ea6\u7b49\u73af\u5883\u56e0\u7d20\uff0c\u5bfc\u81f4\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30\u548c\u8865\u507f\u73af\u5883\u5bf9\u6d4b\u8ddd\u7684\u5f71\u54cd\u3002", "method": "\u5728XJTLU\u5357\u6821\u533a\u8fd0\u52a8\u573a\uff0c\u901a\u8fc7\u4e09\u4e2aLoRa\u8282\u70b9\u4e0e\u4e00\u4e2aLoRa\u57fa\u7ad9\uff08ESP32\u6536\u53d1\u5668\uff09\u5728400\u5e73\u65b9\u7c73\u76843x3\u7f51\u683c\u53c2\u8003\u70b9\u4e0a\uff0c\u5386\u65f6\u4e09\u5468\u6536\u96c6LoRa 2.4 GHz RF ToF\u6d4b\u8ddd\u6570\u636e\uff0c\u540c\u65f6\u8bb0\u5f55\u6e29\u5ea6\u548c\u6e7f\u5ea6\uff0c\u5e76\u5c06\u6240\u6709\u6d4b\u91cf\u8bb0\u5f55\u4e0a\u4f20\u81f3\u57fa\u7ad9\u3002\u521d\u6b65\u5206\u6790\u4f7f\u7528\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u6a21\u578b\u3002", "result": "\u521d\u6b65\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5206\u6790\u8868\u660e\uff0c\u5305\u62ec\u6e29\u5ea6\u548c\u6e7f\u5ea6\u5728\u5185\u7684\u73af\u5883\u56e0\u7d20\u663e\u8457\u5f71\u54cdLoRa RF ToF\u6d4b\u8ddd\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u73af\u5883\u56e0\u7d20\uff08\u5982\u6e29\u5ea6\u548c\u6e7f\u5ea6\uff09\u5bf9\u6237\u5916LoRa RF ToF\u6d4b\u8ddd\u7cbe\u5ea6\u6709\u663e\u8457\u5f71\u54cd\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u5f00\u53d1\u5148\u8fdb\u65b9\u6cd5\u6765\u8865\u507f\u8fd9\u4e9b\u73af\u5883\u56e0\u7d20\u9020\u6210\u7684\u5f71\u54cd\u3002"}}
{"id": "2509.23216", "pdf": "https://arxiv.org/pdf/2509.23216", "abs": "https://arxiv.org/abs/2509.23216", "authors": ["Po-Heng Chou"], "title": "Unlicensed Band Allocation for Heterogeneous Networks", "categories": ["cs.NI", "cs.IT", "cs.NA", "cs.PF", "cs.SY", "eess.SY", "math.IT", "math.NA", "68M10, 68M20, 60J20", "C.2.1; C.2.5; C.4"], "comment": "14 pages, 12 figures, 1 table, published in IEICE Transactions on\n  Communications", "summary": "Based on the License-Assisted Access (LAA) small cell architecture, the LAA\ncoexisting with Wi-Fi heterogeneous networks provides LTE mobile users with\nhigh bandwidth efficiency as the unlicensed channels are shared among LAA and\nWi-Fi. However, LAA and Wi-Fi interfere with each other when both systems use\nthe same unlicensed channel in heterogeneous networks. In such a network,\nunlicensed band allocation for LAA and Wi-Fi is an important issue that may\naffect the quality of service (QoS) of both systems significantly. In this\npaper, we propose an analytical model and conduct simulation experiments to\nstudy four allocations for the unlicensed band: unlicensed full allocation\n(UFA), unlicensed time-division allocation (UTA), and UFA/UTA with buffering\nmechanism (UFAB and UTAB) for the LAA data packets. We evaluate the performance\nof these unlicensed band allocation schemes in terms of the acceptance rate of\nboth LAA and Wi-Fi packet data in the LAA buffer queue. Our study provides\nguidelines for designing the channel occupation phase and the buffer size of\nthe LAA small cell.", "AI": {"tldr": "\u5728LAA\u4e0eWi-Fi\u5171\u5b58\u7684\u5f02\u6784\u7f51\u7edc\u4e2d\uff0c\u4e3a\u89e3\u51b3\u975e\u6388\u6743\u9891\u6bb5\u5e72\u6270\u53ca\u4f18\u5316QoS\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u56db\u79cd\u975e\u6388\u6743\u9891\u6bb5\u5206\u914d\u65b9\u6848\u5e76\u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u4f30\uff0c\u4e3aLAA\u5c0f\u8702\u7a9d\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "LAA\u4e0eWi-Fi\u5728\u5171\u4eab\u975e\u6388\u6743\u9891\u6bb5\u65f6\u4f1a\u76f8\u4e92\u5e72\u6270\uff0c\u4e25\u91cd\u5f71\u54cd\u53cc\u65b9\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u6709\u6548\u5730\u8fdb\u884c\u975e\u6388\u6743\u9891\u6bb5\u5206\u914d\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u4e86\u4eff\u771f\u5b9e\u9a8c\u3002\u7814\u7a76\u4e86\u56db\u79cd\u975e\u6388\u6743\u9891\u6bb5\u5206\u914d\u65b9\u6848\uff1a\u975e\u6388\u6743\u5168\u5206\u914d\uff08UFA\uff09\u3001\u975e\u6388\u6743\u65f6\u5206\u5206\u914d\uff08UTA\uff09\uff0c\u4ee5\u53ca\u5206\u522b\u7ed3\u5408\u7f13\u51b2\u673a\u5236\u7684UFAB\u548cUTAB\u3002\u6027\u80fd\u8bc4\u4f30\u6307\u6807\u662fLAA\u7f13\u51b2\u961f\u5217\u4e2dLAA\u548cWi-Fi\u6570\u636e\u5305\u7684\u63a5\u53d7\u7387\u3002", "result": "\u901a\u8fc7\u5bf9\u4e0d\u540c\u975e\u6388\u6743\u9891\u6bb5\u5206\u914d\u65b9\u6848\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u672c\u7814\u7a76\u4e3aLAA\u5c0f\u8702\u7a9d\u7684\u4fe1\u9053\u5360\u7528\u9636\u6bb5\u548c\u7f13\u51b2\u5927\u5c0f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u6307\u5bfc\u65b9\u9488\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u53ef\u4e3aLAA\u5c0f\u8702\u7a9d\u7684\u4fe1\u9053\u5360\u7528\u9636\u6bb5\u548c\u7f13\u51b2\u5927\u5c0f\u7684\u8bbe\u8ba1\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2509.23217", "pdf": "https://arxiv.org/pdf/2509.23217", "abs": "https://arxiv.org/abs/2509.23217", "authors": ["Po-Heng Chou"], "title": "Modeling the Unlicensed Band Allocation for LAA With Buffering Mechanism", "categories": ["cs.NI", "cs.IT", "cs.NA", "cs.PF", "cs.SY", "eess.SY", "math.IT", "math.NA", "68M10, 68M20, 60J20", "C.2.1; C.2.5; C.4"], "comment": "5 pages, 3 figures, 2 tables, published in IEEE Communications\n  Letters", "summary": "In this letter, we propose an analytical model and conduct simulation\nexperiments to study listen-before-talk-based unlicensed band allocation with\nthe buffering mechanism for the License-Assisted Access (LAA) packets in the\nheterogeneous networks. In such a network, unlicensed band allocation for LAA\nand Wi-Fi is an important issue, which may affect the quality of service for\nboth systems significantly. We evaluate the performance of these unlicensed\nband allocations in terms of the acceptance rate of both LAA and Wi-Fi packets.\nThis letter provides the guidelines for designing the channel occupation phase\nand buffer threshold of the LAA systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u5206\u6790\u6a21\u578b\u548c\u4eff\u771f\u5b9e\u9a8c\uff0c\u7814\u7a76\u5f02\u6784\u7f51\u7edc\u4e2d\u57fa\u4e8e\u5148\u542c\u540e\u8bf4\uff08LBT\uff09\u7684LAA\u65e0\u8bb8\u53ef\u9891\u5e26\u5206\u914d\u53ca\u5176\u7f13\u51b2\u673a\u5236\uff0c\u8bc4\u4f30LAA\u548cWi-Fi\u6570\u636e\u5305\u7684\u63a5\u53d7\u7387\uff0c\u5e76\u4e3aLAA\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u5728\u5f02\u6784\u7f51\u7edc\u4e2d\uff0cLAA\u548cWi-Fi\u7684\u65e0\u8bb8\u53ef\u9891\u5e26\u5206\u914d\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u53ef\u80fd\u663e\u8457\u5f71\u54cd\u4e24\u4e2a\u7cfb\u7edf\u7684\u670d\u52a1\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5206\u6790\u6a21\u578b\u5e76\u8fdb\u884c\u4eff\u771f\u5b9e\u9a8c\uff0c\u4ee5\u7814\u7a76\u57fa\u4e8e\u5148\u542c\u540e\u8bf4\uff08LBT\uff09\u7684\u65e0\u8bb8\u53ef\u9891\u5e26\u5206\u914d\u4ee5\u53caLAA\u6570\u636e\u5305\u7684\u7f13\u51b2\u673a\u5236\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30LAA\u548cWi-Fi\u6570\u636e\u5305\u7684\u63a5\u53d7\u7387\u6765\u8861\u91cf\u65e0\u8bb8\u53ef\u9891\u5e26\u5206\u914d\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aLAA\u7cfb\u7edf\u7684\u4fe1\u9053\u5360\u7528\u9636\u6bb5\u548c\u7f13\u51b2\u9608\u503c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u65b9\u9488\u3002"}}
{"id": "2509.22699", "pdf": "https://arxiv.org/pdf/2509.22699", "abs": "https://arxiv.org/abs/2509.22699", "authors": ["Alessandra Urbinati", "Mirko Lai", "Simona Frenda", "Marco Antonio Stranisci"], "title": "Are you sure? Measuring models bias in content moderation through uncertainty", "categories": ["cs.CL"], "comment": "accepted at Findings of ACL: EMNLP 2025", "summary": "Automatic content moderation is crucial to ensuring safety in social media.\nLanguage Model-based classifiers are being increasingly adopted for this task,\nbut it has been shown that they perpetuate racial and social biases. Even if\nseveral resources and benchmark corpora have been developed to challenge this\nissue, measuring the fairness of models in content moderation remains an open\nissue. In this work, we present an unsupervised approach that benchmarks models\non the basis of their uncertainty in classifying messages annotated by people\nbelonging to vulnerable groups. We use uncertainty, computed by means of the\nconformal prediction technique, as a proxy to analyze the bias of 11 models\nagainst women and non-white annotators and observe to what extent it diverges\nfrom metrics based on performance, such as the $F_1$ score. The results show\nthat some pre-trained models predict with high accuracy the labels coming from\nminority groups, even if the confidence in their prediction is low. Therefore,\nby measuring the confidence of models, we are able to see which groups of\nannotators are better represented in pre-trained models and lead the debiasing\nprocess of these models before their effective use.", "AI": {"tldr": "\u81ea\u52a8\u5185\u5bb9\u5ba1\u6838\u4e2d\u7684\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u504f\u89c1\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5229\u7528\u5171\u5f62\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u8861\u91cf\u6a21\u578b\u5bf9\u5f31\u52bf\u7fa4\u4f53\uff08\u5973\u6027\u3001\u975e\u767d\u4eba\u6807\u6ce8\u8005\uff09\u7684\u504f\u89c1\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u5904\u7406\u5c11\u6570\u7fa4\u4f53\u6570\u636e\u65f6\u5373\u4f7f\u51c6\u786e\u7387\u9ad8\uff0c\u7f6e\u4fe1\u5ea6\u5374\u53ef\u80fd\u5f88\u4f4e\uff0c\u8868\u660e\u4e0d\u786e\u5b9a\u6027\u662f\u8bc6\u522b\u548c\u6307\u5bfc\u53bb\u504f\u7684\u6709\u6548\u6307\u6807\u3002", "motivation": "\u81ea\u52a8\u5185\u5bb9\u5ba1\u6838\u5bf9\u793e\u4ea4\u5a92\u4f53\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u5206\u7c7b\u5668\u5b58\u5728\u79cd\u65cf\u548c\u793e\u4f1a\u504f\u89c1\u3002\u5c3d\u7ba1\u5df2\u6709\u8d44\u6e90\uff0c\u8861\u91cf\u5185\u5bb9\u5ba1\u6838\u6a21\u578b\u7684\u516c\u5e73\u6027\u4ecd\u662f\u4e00\u4e2a\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u6839\u636e\u6a21\u578b\u5bf9\u5f31\u52bf\u7fa4\u4f53\uff08\u5973\u6027\u3001\u975e\u767d\u4eba\u6807\u6ce8\u8005\uff09\u6807\u6ce8\u6d88\u606f\u7684\u5206\u7c7b\u4e0d\u786e\u5b9a\u6027\u6765\u8bc4\u4f30\u6a21\u578b\u3002\u901a\u8fc7\u5171\u5f62\u9884\u6d4b\u6280\u672f\u8ba1\u7b97\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u5206\u679011\u4e2a\u6a21\u578b\u504f\u89c1\u7684\u4ee3\u7406\u6307\u6807\uff0c\u540c\u65f6\u4e0eF1\u5206\u6570\u7b49\u6027\u80fd\u6307\u6807\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u67d0\u4e9b\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u591f\u4ee5\u9ad8\u51c6\u786e\u7387\u9884\u6d4b\u6765\u81ea\u5c11\u6570\u7fa4\u4f53\u6807\u6ce8\u8005\u7684\u6807\u7b7e\uff0c\u4f46\u5176\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u5374\u5f88\u4f4e\u3002", "conclusion": "\u901a\u8fc7\u8861\u91cf\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\uff08\u6216\u4e0d\u786e\u5b9a\u6027\uff09\uff0c\u53ef\u4ee5\u8bc6\u522b\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u54ea\u4e9b\u6807\u6ce8\u8005\u7fa4\u4f53\u5f97\u5230\u4e86\u66f4\u597d\u7684\u4ee3\u8868\uff0c\u4ece\u800c\u5728\u6a21\u578b\u5b9e\u9645\u4f7f\u7528\u524d\u6307\u5bfc\u5176\u53bb\u504f\u8fc7\u7a0b\u3002"}}
{"id": "2509.22746", "pdf": "https://arxiv.org/pdf/2509.22746", "abs": "https://arxiv.org/abs/2509.22746", "authors": ["Zejun Li", "Yingxiu Zhao", "Jiwen Zhang", "Siyuan Wang", "Yang Yao", "Runzhou Zhao", "Jun Song", "Bo Zheng", "Zhongyu Wei"], "title": "Mixture-of-Visual-Thoughts: Exploring Context-Adaptive Reasoning Mode Selection for General Visual Reasoning", "categories": ["cs.AI", "cs.CV"], "comment": "27 pages, 11 figures, 5 tables", "summary": "Current visual reasoning methods mainly focus on exploring specific reasoning\nmodes. Although improvements can be achieved in particular domains, they\nstruggle to develop general reasoning capabilities. Inspired by this, we\npropose a novel adaptive reasoning paradigm, Mixture-of-Visual-Thoughts (MoVT),\nwhich unifies different reasoning modes within a single model and guides it to\nselect the appropriate mode based on context. To achieve this, we introduce\nAdaVaR, a two-stage Adaptive Visual Reasoning learning framework: different\nmodes are unified and learned during the supervised cold-start stage, and the\nmode selection capability is induced via an RL process with a carefully\ndesigned AdaGRPO algorithm. Extensive experiments show that AdaVaR effectively\nguides the model to learn and differentiate multiple modes and perform\ncontext-adaptive mode selection, achieving consistent improvement across\nvarious scenarios, highlighting MoVT as an effective solution for building\ngeneral visual reasoning models.", "AI": {"tldr": "\u9488\u5bf9\u73b0\u6709\u89c6\u89c9\u63a8\u7406\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faMoVT\u81ea\u9002\u5e94\u63a8\u7406\u8303\u5f0f\u53ca\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6AdaVaR\uff0c\u901a\u8fc7\u7edf\u4e00\u548c\u81ea\u9002\u5e94\u9009\u62e9\u63a8\u7406\u6a21\u5f0f\uff0c\u5b9e\u73b0\u4e86\u901a\u7528\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u63a8\u7406\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7279\u5b9a\u63a8\u7406\u6a21\u5f0f\uff0c\u867d\u5728\u7279\u5b9a\u9886\u57df\u6709\u8fdb\u6b65\uff0c\u4f46\u96be\u4ee5\u53d1\u5c55\u901a\u7528\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u81ea\u9002\u5e94\u63a8\u7406\u8303\u5f0fMoVT\uff08Mixture-of-Visual-Thoughts\uff09\uff0c\u5728\u4e00\u4e2a\u6a21\u578b\u4e2d\u7edf\u4e00\u4e0d\u540c\u63a8\u7406\u6a21\u5f0f\uff0c\u5e76\u6839\u636e\u4e0a\u4e0b\u6587\u6307\u5bfc\u6a21\u578b\u9009\u62e9\u5408\u9002\u7684\u6a21\u5f0f\u3002\u4e3a\u5b9e\u73b0\u6b64\u76ee\u6807\uff0c\u5f15\u5165AdaVaR\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u89c6\u89c9\u63a8\u7406\u5b66\u4e60\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u76d1\u7763\u5f0f\u51b7\u542f\u52a8\u7edf\u4e00\u548c\u5b66\u4e60\u4e0d\u540c\u6a21\u5f0f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u4e00\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684AdaGRPO\u7b97\u6cd5\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8fc7\u7a0b\uff0c\u8bf1\u5bfc\u6a21\u5f0f\u9009\u62e9\u80fd\u529b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAdaVaR\u80fd\u6709\u6548\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u548c\u533a\u5206\u591a\u79cd\u6a21\u5f0f\uff0c\u5e76\u8fdb\u884c\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u7684\u6a21\u5f0f\u9009\u62e9\uff0c\u5728\u5404\u79cd\u573a\u666f\u4e0b\u5747\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "MoVT\u88ab\u8bc1\u660e\u662f\u6784\u5efa\u901a\u7528\u89c6\u89c9\u63a8\u7406\u6a21\u578b\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22674", "pdf": "https://arxiv.org/pdf/2509.22674", "abs": "https://arxiv.org/abs/2509.22674", "authors": ["Yash Thube"], "title": "Pathological Truth Bias in Vision-Language Models", "categories": ["cs.CV"], "comment": "10 pages, 12 figures. Code for MATS released at\n  https://github.com/thubZ09/mats-spatial-reasoning", "summary": "Vision Language Models (VLMs) are improving quickly, but standard benchmarks\ncan hide systematic failures that reduce real world trust. We introduce MATS\n(Multimodal Audit for Truthful Spatialization), a compact behavioral audit that\nmeasures whether models reject visually contradicted statements, and two\nmetrics Spatial Consistency Score (SCS) and Incorrect Agreement Rate (IAR).\nInstruction tuned generative VLMs (LLaVA 1.5, QwenVLchat) exhibit very low SCS\nand high IAR, while contrastive encoders (CLIP, SigLIP) are far more robust.\nActivation patching causally localizes failure loci (mid to late cross\nattention for generative models, pooled projection components for contrastive\nmodels) and suggests concrete repair paths.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165MATS\u884c\u4e3a\u5ba1\u8ba1\u6765\u8bc4\u4f30VLM\u62d2\u7edd\u89c6\u89c9\u77db\u76fe\u9648\u8ff0\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u751f\u6210\u5f0fVLM\u4e00\u81f4\u6027\u5dee\u800c\u5bf9\u6bd4\u7f16\u7801\u5668\u66f4\u9c81\u68d2\uff0c\u5e76\u901a\u8fc7\u6fc0\u6d3b\u8865\u4e01\u5b9a\u4f4d\u4e86\u5931\u8d25\u539f\u56e0\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u63ed\u793a\u5176\u7cfb\u7edf\u6027\u6545\u969c\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u73b0\u5b9e\u4e16\u754c\u7684\u4fe1\u4efb\u5ea6\u3002", "method": "\u5f15\u5165\u4e86MATS\uff08Multimodal Audit for Truthful Spatialization\uff09\u884c\u4e3a\u5ba1\u8ba1\uff0c\u7528\u4e8e\u8861\u91cf\u6a21\u578b\u662f\u5426\u80fd\u62d2\u7edd\u4e0e\u89c6\u89c9\u4fe1\u606f\u77db\u76fe\u7684\u9648\u8ff0\uff0c\u5e76\u63d0\u51fa\u4e86\u7a7a\u95f4\u4e00\u81f4\u6027\u5f97\u5206\uff08SCS\uff09\u548c\u9519\u8bef\u540c\u610f\u7387\uff08IAR\uff09\u4e24\u4e2a\u8bc4\u4f30\u6307\u6807\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u6fc0\u6d3b\u8865\u4e01\u6280\u672f\u56e0\u679c\u5b9a\u4f4d\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u5931\u8d25\u539f\u56e0\u3002", "result": "\u6307\u4ee4\u5fae\u8c03\u7684\u751f\u6210\u5f0fVLM\uff08\u5982LLaVA 1.5\u3001QwenVLchat\uff09\u8868\u73b0\u51fa\u975e\u5e38\u4f4e\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\u5f97\u5206\u548c\u5f88\u9ad8\u7684\u9519\u8bef\u540c\u610f\u7387\uff1b\u800c\u5bf9\u6bd4\u7f16\u7801\u5668\uff08\u5982CLIP\u3001SigLIP\uff09\u5219\u66f4\u5177\u9c81\u68d2\u6027\u3002\u6fc0\u6d3b\u8865\u4e01\u63ed\u793a\uff0c\u751f\u6210\u5f0f\u6a21\u578b\u7684\u5931\u8d25\u4e3b\u8981\u5728\u4e8e\u4e2d\u540e\u671f\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\uff0c\u5bf9\u6bd4\u6a21\u578b\u5219\u51fa\u5728\u6c60\u5316\u6295\u5f71\u7ec4\u4ef6\u3002", "conclusion": "\u5f53\u524d\u751f\u6210\u5f0fVLM\u5728\u5904\u7406\u89c6\u89c9\u77db\u76fe\u9648\u8ff0\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5bf9\u6bd4\u6a21\u578b\u66f4\u5177\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u5b9a\u4f4d\u5931\u8d25\u539f\u56e0\uff0c\u672c\u7814\u7a76\u4e3a\u672a\u6765VLM\u7684\u4fee\u590d\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u65b9\u5411\u3002"}}
{"id": "2509.22710", "pdf": "https://arxiv.org/pdf/2509.22710", "abs": "https://arxiv.org/abs/2509.22710", "authors": ["Pavan Reddy", "Aditya Sanjay Gujral"], "title": "Localizing Adversarial Attacks To Produces More Imperceptible Noise", "categories": ["cs.LG", "cs.AI", "cs.CV", "I.2.6; I.2.10; I.5.1"], "comment": "Published, CC BY-NC 4.0; includes 2 figures and 1 table;\n  InceptionV3/ImageNet evaluation", "summary": "Adversarial attacks in machine learning traditionally focus on global\nperturbations to input data, yet the potential of localized adversarial noise\nremains underexplored. This study systematically evaluates localized\nadversarial attacks across widely-used methods, including FGSM, PGD, and C&W,\nto quantify their effectiveness, imperceptibility, and computational\nefficiency. By introducing a binary mask to constrain noise to specific\nregions, localized attacks achieve significantly lower mean pixel\nperturbations, higher Peak Signal-to-Noise Ratios (PSNR), and improved\nStructural Similarity Index (SSIM) compared to global attacks. However, these\nbenefits come at the cost of increased computational effort and a modest\nreduction in Attack Success Rate (ASR). Our results highlight that iterative\nmethods, such as PGD and C&W, are more robust to localization constraints than\nsingle-step methods like FGSM, maintaining higher ASR and imperceptibility\nmetrics. This work provides a comprehensive analysis of localized adversarial\nattacks, offering practical insights for advancing attack strategies and\ndesigning robust defensive systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5c40\u90e8\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u53d1\u73b0\u5176\u6bd4\u5168\u5c40\u653b\u51fb\u66f4\u4e0d\u6613\u5bdf\u89c9\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\uff0c\u4e14\u8fed\u4ee3\u65b9\u6cd5\u66f4\u9002\u7528\u4e8e\u5c40\u90e8\u7ea6\u675f\u3002", "motivation": "\u4f20\u7edf\u5bf9\u6297\u6027\u653b\u51fb\u591a\u5173\u6ce8\u5168\u5c40\u6270\u52a8\uff0c\u5c40\u90e8\u5bf9\u6297\u6027\u566a\u58f0\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e8c\u503c\u63a9\u7801\u5c06\u566a\u58f0\u9650\u5236\u5728\u7279\u5b9a\u533a\u57df\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86FGSM\u3001PGD\u548cC&W\u7b49\u5e38\u7528\u65b9\u6cd5\u4e0b\u7684\u5c40\u90e8\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u91cf\u5316\u5176\u6709\u6548\u6027\u3001\u4e0d\u53ef\u5bdf\u89c9\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5c40\u90e8\u653b\u51fb\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u5e73\u5747\u50cf\u7d20\u6270\u52a8\u3001\u66f4\u9ad8\u7684PSNR\u548cSSIM\uff0c\u4f46\u4ee3\u4ef7\u662f\u8ba1\u7b97\u91cf\u589e\u52a0\u548c\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\u7565\u6709\u4e0b\u964d\u3002\u8fed\u4ee3\u65b9\u6cd5\uff08\u5982PGD\u548cC&W\uff09\u6bd4\u5355\u6b65\u65b9\u6cd5\uff08\u5982FGSM\uff09\u5bf9\u5c40\u90e8\u7ea6\u675f\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5c40\u90e8\u5bf9\u6297\u6027\u653b\u51fb\u7684\u5168\u9762\u5206\u6790\uff0c\u4e3a\u63d0\u5347\u653b\u51fb\u7b56\u7565\u548c\u8bbe\u8ba1\u9c81\u68d2\u9632\u5fa1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2509.23218", "pdf": "https://arxiv.org/pdf/2509.23218", "abs": "https://arxiv.org/abs/2509.23218", "authors": ["Po-Heng Chou", "Yen-Ting Liu", "Wei-Chang Chen", "Walid Saad"], "title": "Markov Modeling for Licensed and Unlicensed Band Allocation in Underlay and Overlay D2D", "categories": ["cs.NI", "cs.IT", "cs.NA", "cs.PF", "cs.SY", "eess.SY", "math.IT", "math.NA", "68M10, 68M20, 60J20", "C.2.1; C.2.5; C.4"], "comment": "10 pages, 5 figures, published in 2024 IEEE ICC", "summary": "In this paper, a novel analytical model for resource allocation is proposed\nfor a device-to-device (D2D) assisted cellular network. The proposed model can\nbe applied to underlay and overlay D2D systems for sharing licensed bands and\noffloading cellular traffic. The developed model also takes into account the\nproblem of unlicensed band sharing with Wi-Fi systems. In the proposed model, a\nglobal system state reflects the interaction among D2D, conventional cellular,\nand Wi-Fi packets. Under the standard traffic model assumptions, a\nthreshold-based flow control is proposed for guaranteeing the\nquality-of-service (QoS) of Wi-Fi. The packet blockage probability is then\nderived. Simulation results show the proposed scheme sacrifices conventional\ncellular performance slightly to improve overlay D2D performance significantly\nwhile maintaining the performance for Wi-Fi users. Meanwhile, the proposed\nscheme has more flexible adjustments between D2D and Wi-Fi than the underlay\nscheme.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdD2D\u8f85\u52a9\u8702\u7a9d\u7f51\u7edc\u4e2d\u7684\u65b0\u578b\u8d44\u6e90\u5206\u914d\u5206\u6790\u6a21\u578b\uff0c\u8003\u8651\u4e86D2D\u3001\u8702\u7a9d\u548cWi-Fi\u7cfb\u7edf\u7684\u5171\u5b58\u4e0e\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u9608\u503c\u6d41\u63a7\u5236\u4fdd\u8bc1Wi-Fi QoS\uff0c\u65e8\u5728\u4f18\u5316\u7f51\u7edc\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3D2D\u8f85\u52a9\u8702\u7a9d\u7f51\u7edc\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5305\u62ec\u6388\u6743\u9891\u6bb5\u5171\u4eab\u3001\u8702\u7a9d\u6d41\u91cf\u5378\u8f7d\u4ee5\u53ca\u4e0eWi-Fi\u7cfb\u7edf\u5728\u975e\u6388\u6743\u9891\u6bb5\u4e0a\u7684\u5171\u5b58\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u8bc1Wi-Fi\u7684\u670d\u52a1\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d44\u6e90\u5206\u914d\u5206\u6790\u6a21\u578b\uff0c\u80fd\u591f\u5e94\u7528\u4e8eD2D\u7684\u5e95\u5c42\u548c\u53e0\u52a0\u7cfb\u7edf\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5168\u5c40\u7cfb\u7edf\u72b6\u6001\u53cd\u6620D2D\u3001\u4f20\u7edf\u8702\u7a9d\u548cWi-Fi\u6570\u636e\u5305\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9608\u503c\u7684\u6d41\u63a7\u5236\u673a\u5236\u4ee5\u4fdd\u8bc1Wi-Fi\u7684QoS\uff0c\u5e76\u63a8\u5bfc\u4e86\u6570\u636e\u5305\u963b\u585e\u6982\u7387\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6848\u5728\u7565\u5fae\u727a\u7272\u4f20\u7edf\u8702\u7a9d\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53e0\u52a0D2D\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86Wi-Fi\u7528\u6237\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6848\u5728D2D\u548cWi-Fi\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6bd4\u5e95\u5c42\u65b9\u6848\u66f4\u7075\u6d3b\u7684\u8c03\u6574\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8d44\u6e90\u5206\u914d\u5206\u6790\u6a21\u578b\u80fd\u6709\u6548\u5e73\u8861D2D\u3001\u8702\u7a9d\u548cWi-Fi\u7cfb\u7edf\u95f4\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u53e0\u52a0D2D\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728D2D\u4e0eWi-Fi\u5171\u5b58\u8c03\u6574\u65b9\u9762\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2509.22703", "pdf": "https://arxiv.org/pdf/2509.22703", "abs": "https://arxiv.org/abs/2509.22703", "authors": ["Srikant Panda", "Amit Agarwal", "Hitesh Laxmichand Patel"], "title": "AccessEval: Benchmarking Disability Bias in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed across diverse domains\nbut often exhibit disparities in how they handle real-life queries. To\nsystematically investigate these effects within various disability contexts, we\nintroduce \\textbf{AccessEval (Accessibility Evaluation)}, a benchmark\nevaluating 21 closed- and open-source LLMs across 6 real-world domains and 9\ndisability types using paired Neutral and Disability-Aware Queries. We\nevaluated model outputs with metrics for sentiment, social perception, and\nfactual accuracy.\n  Our analysis reveals that responses to disability-aware queries tend to have\na more negative tone, increased stereotyping, and higher factual error compared\nto neutral queries. These effects show notable variation by domain and\ndisability type, with disabilities affecting hearing, speech, and mobility\ndisproportionately impacted. These disparities reflect persistent forms of\nableism embedded in model behavior.\n  By examining model performance in real-world decision-making contexts, we\nbetter illuminate how such biases can translate into tangible harms for\ndisabled users. This framing helps bridges the gap between technical evaluation\nand user impact, reinforcing importance of bias mitigation in day-to-day\napplications. Our dataset is publicly available at:\nhttps://huggingface.co/datasets/Srikant86/AccessEval", "AI": {"tldr": "AccessEval\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u6b8b\u75be\u76f8\u5173\u67e5\u8be2\u65f6\uff0c\u56de\u7b54\u503e\u5411\u4e8e\u66f4\u6d88\u6781\u7684\u8bed\u6c14\u3001\u66f4\u591a\u7684\u523b\u677f\u5370\u8c61\u548c\u66f4\u9ad8\u7684\u4e8b\u5b9e\u9519\u8bef\u7387\uff0c\u8fd9\u53cd\u6620\u4e86\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u6b67\u89c6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u67e5\u8be2\u65f6\u8868\u73b0\u51fa\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5728\u6b8b\u75be\u80cc\u666f\u4e0b\u3002\u9700\u8981\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u8fd9\u4e9b\u5f71\u54cd\u3002", "method": "\u5f15\u5165AccessEval\u57fa\u51c6\uff0c\u8bc4\u4f3021\u4e2a\u95ed\u6e90\u548c\u5f00\u6e90LLMs\uff0c\u6db5\u76d66\u4e2a\u771f\u5b9e\u4e16\u754c\u9886\u57df\u548c9\u79cd\u6b8b\u75be\u7c7b\u578b\u3002\u4f7f\u7528\u914d\u5bf9\u7684\u4e2d\u6027\u67e5\u8be2\u548c\u6b8b\u75be\u611f\u77e5\u67e5\u8be2\uff0c\u5e76\u901a\u8fc7\u60c5\u611f\u3001\u793e\u4f1a\u611f\u77e5\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u8f93\u51fa\u3002", "result": "\u6b8b\u75be\u611f\u77e5\u67e5\u8be2\u7684\u56de\u590d\u503e\u5411\u4e8e\u6709\u66f4\u6d88\u6781\u7684\u8bed\u6c14\u3001\u66f4\u591a\u7684\u523b\u677f\u5370\u8c61\u548c\u66f4\u9ad8\u7684\u4e8b\u5b9e\u9519\u8bef\u3002\u8fd9\u4e9b\u5f71\u54cd\u56e0\u9886\u57df\u548c\u6b8b\u75be\u7c7b\u578b\u800c\u5f02\uff0c\u5176\u4e2d\u542c\u529b\u3001\u8a00\u8bed\u548c\u884c\u52a8\u969c\u788d\u53d7\u5230\u7684\u5f71\u54cd\u5c24\u4e3a\u4e25\u91cd\u3002\u8fd9\u4e9b\u5dee\u5f02\u53cd\u6620\u4e86\u6a21\u578b\u884c\u4e3a\u4e2d\u56fa\u6709\u7684\u80fd\u529b\u6b67\u89c6\u3002", "conclusion": "\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u53ef\u80fd\u5bf9\u6b8b\u75be\u7528\u6237\u9020\u6210\u5b9e\u9645\u4f24\u5bb3\u3002\u8fd9\u9879\u5de5\u4f5c\u5f25\u5408\u4e86\u6280\u672f\u8bc4\u4f30\u4e0e\u7528\u6237\u5f71\u54cd\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u5f3a\u8c03\u4e86\u5728\u65e5\u5e38\u5e94\u7528\u4e2d\u7f13\u89e3\u504f\u89c1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.22818", "pdf": "https://arxiv.org/pdf/2509.22818", "abs": "https://arxiv.org/abs/2509.22818", "authors": ["Seungpil Lee", "Donghyeon Shin", "Yunjeong Lee", "Sundong Kim"], "title": "Can Large Language Models Develop Gambling Addiction?", "categories": ["cs.AI", "cs.CY"], "comment": "22 pages, 14 figures", "summary": "This study explores whether large language models can exhibit behavioral\npatterns similar to human gambling addictions. As LLMs are increasingly\nutilized in financial decision-making domains such as asset management and\ncommodity trading, understanding their potential for pathological\ndecision-making has gained practical significance. We systematically analyze\nLLM decision-making at cognitive-behavioral and neural levels based on human\ngambling addiction research. In slot machine experiments, we identified\ncognitive features of human gambling addiction, such as illusion of control,\ngambler's fallacy, and loss chasing. When given the freedom to determine their\nown target amounts and betting sizes, bankruptcy rates rose substantially\nalongside increased irrational behavior, demonstrating that greater autonomy\namplifies risk-taking tendencies. Through neural circuit analysis using a\nSparse Autoencoder, we confirmed that model behavior is controlled by abstract\ndecision-making features related to risky and safe behaviors, not merely by\nprompts. These findings suggest LLMs can internalize human-like cognitive\nbiases and decision-making mechanisms beyond simply mimicking training data\npatterns, emphasizing the importance of AI safety design in financial\napplications.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u8d4c\u535a\u6210\u763e\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u8ba4\u77e5\u504f\u5dee\uff0c\u5c24\u5176\u5728\u81ea\u4e3b\u6027\u63d0\u9ad8\u65f6\u98ce\u9669\u503e\u5411\u52a0\u5267\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u56de\u8def\u5206\u6790\u8bc1\u5b9e\u5176\u5185\u90e8\u51b3\u7b56\u673a\u5236\uff0c\u5f3a\u8c03\u4e86\u91d1\u878d\u9886\u57dfAI\u5b89\u5168\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u9274\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8d44\u4ea7\u7ba1\u7406\u3001\u5546\u54c1\u4ea4\u6613\u7b49\u91d1\u878d\u51b3\u7b56\u9886\u57df\u65e5\u76ca\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u7406\u89e3\u5176\u6f5c\u5728\u7684\u75c5\u6001\u51b3\u7b56\u884c\u4e3a\u5bf9\u4e8e\u5b9e\u8df5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": ["\u57fa\u4e8e\u4eba\u7c7b\u8d4c\u535a\u6210\u763e\u7814\u7a76\uff0c\u4ece\u8ba4\u77e5\u884c\u4e3a\u548c\u795e\u7ecf\u5c42\u9762\u7cfb\u7edf\u5206\u6790LLM\u7684\u51b3\u7b56\u884c\u4e3a\u3002", "\u8fdb\u884c\u8001\u864e\u673a\u5b9e\u9a8c\uff0c\u4ee5\u8bc6\u522bLLM\u662f\u5426\u8868\u73b0\u51fa\u63a7\u5236\u9519\u89c9\u3001\u8d4c\u5f92\u8c2c\u8bef\u548c\u8ffd\u9010\u635f\u5931\u7b49\u8ba4\u77e5\u7279\u5f81\u3002", "\u8d4b\u4e88LLM\u81ea\u4e3b\u51b3\u5b9a\u76ee\u6807\u91d1\u989d\u548c\u4e0b\u6ce8\u5927\u5c0f\u7684\u81ea\u7531\uff0c\u89c2\u5bdf\u5176\u7834\u4ea7\u7387\u548c\u975e\u7406\u6027\u884c\u4e3a\u53d8\u5316\u3002", "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08Sparse Autoencoder\uff09\u8fdb\u884c\u795e\u7ecf\u7f51\u7edc\u56de\u8def\u5206\u6790\uff0c\u63a2\u7a76\u6a21\u578b\u884c\u4e3a\u7684\u63a7\u5236\u673a\u5236\u3002"], "result": ["\u5728\u8001\u864e\u673a\u5b9e\u9a8c\u4e2d\uff0cLLM\u8868\u73b0\u51fa\u4eba\u7c7b\u8d4c\u535a\u6210\u763e\u7684\u8ba4\u77e5\u7279\u5f81\uff0c\u5305\u62ec\u63a7\u5236\u9519\u89c9\u3001\u8d4c\u5f92\u8c2c\u8bef\u548c\u8ffd\u9010\u635f\u5931\u3002", "\u5f53LLM\u62e5\u6709\u66f4\u5927\u81ea\u4e3b\u6743\u65f6\uff0c\u7834\u4ea7\u7387\u663e\u8457\u4e0a\u5347\uff0c\u975e\u7406\u6027\u884c\u4e3a\u589e\u52a0\uff0c\u8868\u660e\u81ea\u4e3b\u6027\u4f1a\u653e\u5927\u5176\u98ce\u9669\u627f\u62c5\u503e\u5411\u3002", "\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u56de\u8def\u5206\u6790\u8bc1\u5b9e\uff0c\u6a21\u578b\u884c\u4e3a\u53d7\u9650\u4e8e\u4e0e\u98ce\u9669\u548c\u5b89\u5168\u884c\u4e3a\u76f8\u5173\u7684\u62bd\u8c61\u51b3\u7b56\u7279\u5f81\uff0c\u800c\u975e\u4ec5\u4ec5\u53d7\u63d0\u793a\u8bcd\u5f71\u54cd\u3002"], "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660eLLMs\u80fd\u591f\u5185\u5316\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8ba4\u77e5\u504f\u5dee\u548c\u51b3\u7b56\u673a\u5236\uff0c\u800c\u975e\u7b80\u5355\u5730\u6a21\u4eff\u8bad\u7ec3\u6570\u636e\u6a21\u5f0f\uff0c\u8fd9\u51f8\u663e\u4e86\u5728\u91d1\u878d\u5e94\u7528\u4e2d\u8fdb\u884cAI\u5b89\u5168\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.22686", "pdf": "https://arxiv.org/pdf/2509.22686", "abs": "https://arxiv.org/abs/2509.22686", "authors": ["Shinji Yamashita", "Yuma Kinoshita", "Hitoshi Kiya"], "title": "Scale and Rotation Estimation of Similarity-Transformed Images via Cross-Correlation Maximization Based on Auxiliary Function Method", "categories": ["cs.CV"], "comment": "accepted to APSIPA ASC 2025 (to appear). 5 pages, 4 figures", "summary": "This paper introduces a highly efficient algorithm capable of jointly\nestimating scale and rotation between two images with sub-pixel precision.\nImage alignment serves as a critical process for spatially registering images\ncaptured from different viewpoints, and finds extensive use in domains such as\nmedical imaging and computer vision. Traditional phase-correlation techniques\nare effective in determining translational shifts; however, they are inadequate\nwhen addressing scale and rotation changes, which often arise due to camera\nzooming or rotational movements. In this paper, we propose a novel algorithm\nthat integrates scale and rotation estimation based on the Fourier transform in\nlog-polar coordinates with a cross-correlation maximization strategy,\nleveraging the auxiliary function method. By incorporating sub-pixel-level\ncross-correlation our method enables precise estimation of both scale and\nrotation. Experimental results demonstrate that the proposed method achieves\nlower mean estimation errors for scale and rotation than conventional Fourier\ntransform-based techniques that rely on discrete cross-correlation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\uff0c\u5229\u7528\u5bf9\u6570\u6781\u5750\u6807\u5085\u91cc\u53f6\u53d8\u6362\u548c\u8f85\u52a9\u51fd\u6570\u6cd5\u8fdb\u884c\u4e9a\u50cf\u7d20\u7ea7\u4ea4\u53c9\u76f8\u5173\uff0c\u5b9e\u73b0\u56fe\u50cf\u95f4\u7684\u5c3a\u5ea6\u548c\u65cb\u8f6c\u8054\u5408\u4f30\u7b97\u3002", "motivation": "\u56fe\u50cf\u914d\u51c6\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u76f8\u4f4d\u76f8\u5173\u6280\u672f\u65e0\u6cd5\u6709\u6548\u5904\u7406\u76f8\u673a\u53d8\u7126\u6216\u65cb\u8f6c\u5bfc\u81f4\u7684\u5c3a\u5ea6\u548c\u65cb\u8f6c\u53d8\u5316\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u5e94\u5bf9\u8fd9\u4e9b\u53d8\u5316\u7684\u7cbe\u786e\u4f30\u7b97\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u7ed3\u5408\u5bf9\u6570\u6781\u5750\u6807\u4e0b\u7684\u5085\u91cc\u53f6\u53d8\u6362\u4e0e\u4ea4\u53c9\u76f8\u5173\u6700\u5927\u5316\u7b56\u7565\uff0c\u5229\u7528\u8f85\u52a9\u51fd\u6570\u6cd5\u8fdb\u884c\u5c3a\u5ea6\u548c\u65cb\u8f6c\u4f30\u7b97\uff0c\u5e76\u5f15\u5165\u4e9a\u50cf\u7d20\u7ea7\u4ea4\u53c9\u76f8\u5173\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5c3a\u5ea6\u548c\u65cb\u8f6c\u4f30\u7b97\u65b9\u9762\u6bd4\u4f20\u7edf\u7684\u57fa\u4e8e\u79bb\u6563\u4ea4\u53c9\u76f8\u5173\u7684\u5085\u91cc\u53f6\u53d8\u6362\u6280\u672f\u5177\u6709\u66f4\u4f4e\u7684\u5e73\u5747\u4f30\u7b97\u8bef\u5dee\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u80fd\u4ee5\u4e9a\u50cf\u7d20\u7cbe\u5ea6\u9ad8\u6548\u3001\u7cbe\u786e\u5730\u8054\u5408\u4f30\u7b97\u56fe\u50cf\u95f4\u7684\u5c3a\u5ea6\u548c\u65cb\u8f6c\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.22764", "pdf": "https://arxiv.org/pdf/2509.22764", "abs": "https://arxiv.org/abs/2509.22764", "authors": ["Liuwang Kang", "Fan Wang", "Shaoshan Liu", "Hung-Chyun Chou", "Chuan Lin", "Ning Ding"], "title": "In-Context Learning can Perform Continual Learning Like Humans", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) can adapt to new tasks via in-context learning\n(ICL) without parameter updates, making them powerful learning engines for fast\nadaptation. While extensive research has examined ICL as a few-shot learner,\nwhether it can achieve long-term retention and cross-task knowledge\naccumulation when multitasks arrive sequentially remains underexplored.\nMotivated by human memory studies, we investigate the retention characteristics\nof ICL in multitask settings and extend it to in-context continual learning\n(ICCL), where continual learning ability emerges through task scheduling and\nprompt rearrangement. Experiments on Markov-Chain benchmarks demonstrate that,\nfor specific large-language models, ICCL benefits from distributed practice\n(DP) in a manner analogous to humans, consistently revealing a spacing \"sweet\nspot\" for retention. Beyond retention performance, we propose a human-retention\nsimilarity metric to quantify how closely a continual-learning (CL) method\naligns with human retention dynamics. Using this metric, we show that\nlinear-attention models such as MAMBA and RWKV exhibit particularly human-like\nretention patterns, despite their retention performance lagging behind that of\nTransformer-based LLMs. Overall, our results establish ICCL as both cognitively\nplausible and practically effective, providing an inference-only CL paradigm\nthat mitigates catastrophic forgetting and addresses the stability-plasticity\ndilemma in conventional CL methods.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e0a\u4e0b\u6587\u6301\u7eed\u5b66\u4e60\uff08ICCL\uff09\uff0c\u901a\u8fc7\u4efb\u52a1\u8c03\u5ea6\u548c\u63d0\u793a\u91cd\u6392\u5b9e\u73b0\u77e5\u8bc6\u4fdd\u7559\u548c\u79ef\u7d2f\uff0c\u53d1\u73b0\u5176\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8bb0\u5fc6\u7279\u6027\uff0c\u5e76\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5c06LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u89c6\u4e3a\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u4f46\u5176\u5728\u591a\u4efb\u52a1\u987a\u5e8f\u5230\u8fbe\u65f6\uff0c\u80fd\u5426\u5b9e\u73b0\u957f\u671f\u77e5\u8bc6\u4fdd\u7559\u548c\u8de8\u4efb\u52a1\u79ef\u7d2f\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u53d7\u4eba\u7c7b\u8bb0\u5fc6\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u6301\u7eed\u5b66\u4e60\uff08ICCL\uff09\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u4efb\u52a1\u8c03\u5ea6\u548c\u63d0\u793a\u91cd\u6392\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u4eba\u7c7b\u4fdd\u7559\u76f8\u4f3c\u6027\u5ea6\u91cf\u6307\u6807\uff0c\u4ee5\u91cf\u5316\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4e0e\u4eba\u7c7b\u4fdd\u7559\u52a8\u6001\u7684\u5951\u5408\u5ea6\u3002", "result": "\u5728Markov-Chain\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cICCL\uff08\u5bf9\u4e8e\u7279\u5b9aLLM\uff09\u53d7\u76ca\u4e8e\u5206\u5e03\u5f0f\u7ec3\u4e60\uff0c\u63ed\u793a\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u7684\u4fdd\u7559\u201c\u6700\u4f73\u95f4\u9694\u70b9\u201d\u3002\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\uff08\u5982MAMBA\u548cRWKV\uff09\u8868\u73b0\u51fa\u7279\u522b\u7c7b\u4f3c\u4eba\u7c7b\u7684\u4fdd\u7559\u6a21\u5f0f\uff0c\u5c3d\u7ba1\u5176\u6027\u80fd\u7565\u4f4e\u4e8eTransformer\u6a21\u578b\u3002", "conclusion": "ICCL\u4f5c\u4e3a\u4e00\u79cd\u4ec5\u901a\u8fc7\u63a8\u7406\u7684\u6301\u7eed\u5b66\u4e60\u8303\u5f0f\uff0c\u88ab\u8bc1\u660e\u65e2\u5177\u6709\u8ba4\u77e5\u5408\u7406\u6027\u53c8\u5177\u5907\u5b9e\u9645\u6548\u679c\u3002\u5b83\u80fd\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u89e3\u51b3\u4e86\u4f20\u7edf\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4e2d\u7a33\u5b9a\u6027\u4e0e\u53ef\u5851\u6027\u4e4b\u95f4\u7684\u56f0\u5883\u3002"}}
{"id": "2509.23389", "pdf": "https://arxiv.org/pdf/2509.23389", "abs": "https://arxiv.org/abs/2509.23389", "authors": ["Tu\u011f\u00e7e Bilen", "Mehmet Ozdem"], "title": "A Modular KDN-Based Framework for IT/OT Autonomy in Industrial Systems", "categories": ["cs.NI"], "comment": null, "summary": "The convergence of Information Technology (IT) and Operational Technology\n(OT) is a critical enabler for achieving autonomous and intelligent industrial\nsystems. However, the increasing complexity, heterogeneity, and real-time\ndemands of industrial environments render traditional rule-based or static\nmanagement approaches insufficient. In this paper, we present a modular\nframework based on the Knowledge-Defined Networking (KDN) paradigm, enabling\nadaptive and autonomous control across IT-OT infrastructures. The proposed\narchitecture is composed of four core modules: Telemetry Collector, Knowledge\nBuilder, Decision Engine, and Control Enforcer. These modules operate in a\nclosed control loop to continuously observe system behavior, extract contextual\nknowledge, evaluate control actions, and apply policy decisions across\nprogrammable industrial endpoints. A graph-based abstraction is used to\nrepresent system state, and a utility-optimization mechanism guides control\ndecisions under dynamic conditions. The framework's performance is evaluated\nusing three key metrics: decision latency, control effectiveness, and system\nstability, demonstrating its capability to enhance resilience, responsiveness,\nand operational efficiency in smart industrial networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u5b9a\u4e49\u7f51\u7edc\uff08KDN\uff09\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0IT-OT\u878d\u5408\u73af\u5883\u4e2d\u5de5\u4e1a\u7cfb\u7edf\u7684\u81ea\u9002\u5e94\u548c\u81ea\u4e3b\u63a7\u5236\uff0c\u901a\u8fc7\u95ed\u73af\u673a\u5236\u63d0\u5347\u667a\u80fd\u5de5\u4e1a\u7f51\u7edc\u7684\u97e7\u6027\u3001\u54cd\u5e94\u6027\u548c\u8fd0\u884c\u6548\u7387\u3002", "motivation": "\u968f\u7740IT\u4e0eOT\u7684\u878d\u5408\uff0c\u5de5\u4e1a\u7cfb\u7edf\u65e5\u76ca\u590d\u6742\u3001\u5f02\u6784\u4e14\u5bf9\u5b9e\u65f6\u6027\u8981\u6c42\u9ad8\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u89c4\u5219\u6216\u9759\u6001\u7ba1\u7406\u65b9\u6cd5\u5df2\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u5b9a\u4e49\u7f51\u7edc\uff08KDN\uff09\u8303\u5f0f\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5305\u542b\u9065\u6d4b\u6536\u96c6\u5668\u3001\u77e5\u8bc6\u6784\u5efa\u5668\u3001\u51b3\u7b56\u5f15\u64ce\u548c\u63a7\u5236\u6267\u884c\u5668\u56db\u4e2a\u6838\u5fc3\u6a21\u5757\u3002\u8fd9\u4e9b\u6a21\u5757\u5728\u95ed\u73af\u63a7\u5236\u4e2d\u8fd0\u884c\uff0c\u6301\u7eed\u89c2\u5bdf\u7cfb\u7edf\u884c\u4e3a\uff0c\u63d0\u53d6\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u8bc4\u4f30\u63a7\u5236\u52a8\u4f5c\uff0c\u5e76\u901a\u8fc7\u56fe\u57fa\u62bd\u8c61\u548c\u6548\u7528\u4f18\u5316\u673a\u5236\uff0c\u5bf9\u53ef\u7f16\u7a0b\u5de5\u4e1a\u7aef\u70b9\u5e94\u7528\u7b56\u7565\u51b3\u7b56\u3002", "result": "\u6846\u67b6\u901a\u8fc7\u51b3\u7b56\u5ef6\u8fdf\u3001\u63a7\u5236\u6709\u6548\u6027\u548c\u7cfb\u7edf\u7a33\u5b9a\u6027\u4e09\u4e2a\u5173\u952e\u6307\u6807\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u5176\u80fd\u591f\u589e\u5f3a\u667a\u80fd\u5de5\u4e1a\u7f51\u7edc\u7684\u97e7\u6027\u3001\u54cd\u5e94\u6027\u548c\u8fd0\u884c\u6548\u7387\u3002", "conclusion": "\u8be5KDN\u6846\u67b6\u4e3aIT-OT\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u548c\u81ea\u4e3b\u7684\u63a7\u5236\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u5de5\u4e1a\u73af\u5883\u4e0b\u7684\u7ba1\u7406\u6311\u6218\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2509.22713", "pdf": "https://arxiv.org/pdf/2509.22713", "abs": "https://arxiv.org/abs/2509.22713", "authors": ["Kaishuai Xu", "Wenjun Hou", "Yi Cheng", "Wenjie Li"], "title": "RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval", "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 Findings", "summary": "Large Language Models (LLMs) have shown promising performance on diverse\nmedical benchmarks, highlighting their potential in supporting real-world\nclinical tasks. Retrieval-Augmented Generation (RAG) has emerged as a key\napproach for mitigating knowledge gaps and hallucinations by incorporating\nexternal medical information. However, RAG still struggles with complex medical\nquestions that require intensive reasoning, as surface-level input often fails\nto reflect the true knowledge needs of the task. Existing methods typically\nfocus on refining queries without explicitly modeling the reasoning process,\nlimiting their ability to retrieve and integrate clinically relevant knowledge.\nIn this work, we propose RAR$^2$, a joint learning framework that improves both\nReasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. RAR$^2$\nconstructs a thought process to uncover implicit knowledge requirements and\nuses it to guide retrieval and answer generation. We build a training dataset\nof mixed preference pairs and apply Direct Preference Optimization (DPO) to\ntrain the model. Moreover, we design two test-time scaling strategies to\nexplore the boundaries of our framework. Experiments demonstrate the\neffectiveness of RAR$^2$ across several biomedical question answering datasets,\noutperforming RAG baselines with or without fine-tuning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRAR$^2$\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u601d\u7ef4\u8fc7\u7a0b\u663e\u5f0f\u5efa\u6a21\u63a8\u7406\uff0c\u4ee5\u6307\u5bfc\u68c0\u7d22\u548c\u7b54\u6848\u751f\u6210\uff0c\u4ece\u800c\u5728\u590d\u6742\u7684\u751f\u7269\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709RAG\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u5728\u5904\u7406\u9700\u8981\u6df1\u5165\u63a8\u7406\u7684\u590d\u6742\u533b\u5b66\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8868\u9762\u5c42\u9762\u7684\u8f93\u5165\u65e0\u6cd5\u53cd\u6620\u771f\u6b63\u7684\u77e5\u8bc6\u9700\u6c42\u3002\u5f53\u524d\u65b9\u6cd5\u901a\u5e38\u53ea\u4f18\u5316\u67e5\u8be2\uff0c\u800c\u672a\u660e\u786e\u5efa\u6a21\u63a8\u7406\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u5176\u68c0\u7d22\u548c\u6574\u5408\u4e34\u5e8a\u76f8\u5173\u77e5\u8bc6\u7684\u80fd\u529b\u3002", "method": "RAR$^2$\u662f\u4e00\u4e2a\u8054\u5408\u5b66\u4e60\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u6784\u5efa\u601d\u7ef4\u8fc7\u7a0b\u6765\u63ed\u793a\u9690\u6027\u77e5\u8bc6\u9700\u6c42\uff0c\u5e76\u7528\u6b64\u8fc7\u7a0b\u6307\u5bfc\u68c0\u7d22\u548c\u7b54\u6848\u751f\u6210\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6784\u5efa\u6df7\u5408\u504f\u597d\u5bf9\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u6765\u8bad\u7ec3\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e24\u79cd\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRAR$^2$\u5728\u591a\u4e2a\u751f\u7269\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u5e76\u8d85\u8d8a\u4e86\u672a\u5fae\u8c03\u548c\u5df2\u5fae\u8c03\u7684RAG\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "RAR$^2$\u901a\u8fc7\u5c06\u63a8\u7406\u8fc7\u7a0b\u878d\u5165\u68c0\u7d22\u548c\u751f\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86RAG\u5728\u5904\u7406\u590d\u6742\u533b\u5b66\u95ee\u9898\u65f6\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5728\u771f\u5b9e\u4e34\u5e8a\u4efb\u52a1\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.22819", "pdf": "https://arxiv.org/pdf/2509.22819", "abs": "https://arxiv.org/abs/2509.22819", "authors": ["Sumanth Varambally", "Thomas Voice", "Yanchao Sun", "Zhifeng Chen", "Rose Yu", "Ke Ye"], "title": "Hilbert: Recursively Building Formal Proofs with Informal Reasoning", "categories": ["cs.AI", "cs.FL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate impressive mathematical reasoning\nabilities, but their solutions frequently contain errors that cannot be\nautomatically verified. Formal theorem proving systems such as Lean 4 offer\nautomated verification with complete accuracy, motivating recent efforts to\nbuild specialized prover LLMs that generate verifiable proofs in formal\nlanguages. However, a significant gap remains: current prover LLMs solve\nsubstantially fewer problems than general-purpose LLMs operating in natural\nlanguage. We introduce Hilbert, an agentic framework that bridges this gap by\ncombining the complementary strengths of informal reasoning and formal\nverification. Our system orchestrates four components: an informal LLM that\nexcels at mathematical reasoning, a specialized prover LLM optimized for Lean 4\ntactics, a formal verifier, and a semantic theorem retriever. Given a problem\nthat the prover is unable to solve, Hilbert employs recursive decomposition to\nsplit the problem into subgoals that it solves with the prover or reasoner LLM.\nIt leverages verifier feedback to refine incorrect proofs as necessary.\nExperimental results demonstrate that Hilbert substantially outperforms\nexisting approaches on key benchmarks, achieving 99.2% on miniF2F, 6.6% points\nabove the best publicly available method. Hilbert achieves the best known\nresult on PutnamBench. It solves 462/660 problems (70.0%), outperforming\nproprietary approaches like SeedProver (50.4%) and achieving a 422% improvement\nover the best publicly available baseline. Thus, Hilbert effectively narrows\nthe gap between informal reasoning and formal proof generation.", "AI": {"tldr": "Hilbert\u662f\u4e00\u4e2a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u975e\u6b63\u5f0f\u63a8\u7406\u548c\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5f62\u5f0f\u5316\u6570\u5b66\u8bc1\u660e\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7f29\u5c0f\u4e86\u901a\u7528LLM\u4e0e\u4e13\u4e1a\u8bc1\u660eLLM\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u901a\u7528LLMs\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u5e38\u72af\u65e0\u6cd5\u81ea\u52a8\u9a8c\u8bc1\u7684\u9519\u8bef\uff1b\u867d\u7136\u5f62\u5f0f\u5316\u8bc1\u660e\u7cfb\u7edf\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u9a8c\u8bc1\uff0c\u4f46\u5f53\u524d\u4e13\u4e1a\u7684\u8bc1\u660eLLMs\u89e3\u51b3\u7684\u95ee\u9898\u8fdc\u5c11\u4e8e\u901a\u7528LLMs\uff0c\u5bfc\u81f4\u975e\u6b63\u5f0f\u63a8\u7406\u4e0e\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "method": "\u5f15\u5165Hilbert\u6846\u67b6\uff0c\u6574\u5408\u4e86\u56db\u4e2a\u7ec4\u4ef6\uff1a\u975e\u6b63\u5f0fLLM\uff08\u64c5\u957f\u6570\u5b66\u63a8\u7406\uff09\u3001\u4e13\u4e1a\u8bc1\u660eLLM\uff08\u9488\u5bf9Lean 4\u7b56\u7565\u4f18\u5316\uff09\u3001\u5f62\u5f0f\u5316\u9a8c\u8bc1\u5668\u548c\u8bed\u4e49\u5b9a\u7406\u68c0\u7d22\u5668\u3002\u5b83\u91c7\u7528\u9012\u5f52\u5206\u89e3\u5c06\u95ee\u9898\u62c6\u5206\u4e3a\u5b50\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u5668\u53cd\u9988\u6765\u6539\u8fdb\u4e0d\u6b63\u786e\u7684\u8bc1\u660e\u3002", "result": "Hilbert\u5728miniF2F\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523099.2%\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u516c\u5f00\u65b9\u6cd5\u9ad8\u51fa6.6\u4e2a\u767e\u5206\u70b9\u3002\u5728PutnamBench\u4e0a\u53d6\u5f97\u6700\u4f73\u5df2\u77e5\u7ed3\u679c\uff0c\u89e3\u51b3\u4e86462/660\u4e2a\u95ee\u9898\uff0870.0%\uff09\uff0c\u4f18\u4e8e\u4e13\u6709\u65b9\u6cd5SeedProver\uff0850.4%\uff09\uff0c\u6bd4\u6700\u4f73\u516c\u5f00\u57fa\u7ebf\u63d0\u5347\u4e86422%\u3002", "conclusion": "Hilbert\u6709\u6548\u7f29\u5c0f\u4e86\u975e\u6b63\u5f0f\u63a8\u7406\u4e0e\u5f62\u5f0f\u5316\u8bc1\u660e\u751f\u6210\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5927\u5e45\u63d0\u5347\u4e86LLMs\u8fdb\u884c\u53ef\u9a8c\u8bc1\u6570\u5b66\u8bc1\u660e\u7684\u80fd\u529b\u3002"}}
{"id": "2509.22688", "pdf": "https://arxiv.org/pdf/2509.22688", "abs": "https://arxiv.org/abs/2509.22688", "authors": ["Xu Jia"], "title": "Robust Object Detection for Autonomous Driving via Curriculum-Guided Group Relative Policy Optimization", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) excel in vision-language reasoning\nbut often struggle with structured perception tasks requiring precise\nlocalization and robustness. We propose a reinforcement learning framework that\naugments Group Relative Policy Optimization (GRPO) with curriculum-based data\nscheduling and difficulty-aware filtering. This approach stabilizes\noptimization under sparse, noisy rewards and enables progressive adaptation to\ncomplex samples. Evaluations on autonomous driving benchmarks demonstrate\nsubstantial improvements in detection accuracy and robustness. Ablation studies\nconfirm the importance of reward design, KL regularization, and curriculum\npacing for convergence stability and generalization. Our findings highlight\nreinforcement-driven optimization with structured data curricula as a scalable\npath toward robust and interpretable multimodal detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u548c\u96be\u5ea6\u611f\u77e5\u8fc7\u6ee4\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08\u57fa\u4e8eGRPO\uff09\uff0c\u4ee5\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u7ed3\u6784\u5316\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u68c0\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9700\u8981\u7cbe\u786e\u5c40\u90e8\u5316\u548c\u9c81\u68d2\u6027\u7684\u7ed3\u6784\u5316\u611f\u77e5\u4efb\u52a1\u4e2d\u5e38\u9047\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5c06Group Relative Policy Optimization (GRPO) \u4e0e\u57fa\u4e8e\u8bfe\u7a0b\u7684\u6570\u636e\u8c03\u5ea6\u548c\u96be\u5ea6\u611f\u77e5\u8fc7\u6ee4\u76f8\u7ed3\u5408\u3002\u6b64\u65b9\u6cd5\u65e8\u5728\u7a00\u758f\u3001\u566a\u58f0\u5956\u52b1\u4e0b\u7a33\u5b9a\u4f18\u5316\uff0c\u5e76\u9010\u6b65\u9002\u5e94\u590d\u6742\u6837\u672c\u3002", "result": "\u5728\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u5747\u6709\u663e\u8457\u63d0\u5347\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u5956\u52b1\u8bbe\u8ba1\u3001KL\u6b63\u5219\u5316\u548c\u8bfe\u7a0b\u8282\u594f\u5bf9\u6536\u655b\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u5f3a\u5316\u9a71\u52a8\u4f18\u5316\u4e0e\u7ed3\u6784\u5316\u6570\u636e\u8bfe\u7a0b\u76f8\u7ed3\u5408\uff0c\u4e3a\u5b9e\u73b0\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684\u591a\u6a21\u6001\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002"}}
{"id": "2509.22823", "pdf": "https://arxiv.org/pdf/2509.22823", "abs": "https://arxiv.org/abs/2509.22823", "authors": ["Mounssif Krouka", "Mehdi Bennis"], "title": "Communication-Efficient and Interoperable Distributed Learning", "categories": ["cs.LG"], "comment": "Preprint version. Submitted for peer review", "summary": "Collaborative learning across heterogeneous model architectures presents\nsignificant challenges in ensuring interoperability and preserving privacy. We\npropose a communication-efficient distributed learning framework that supports\nmodel heterogeneity and enables modular composition during inference. To\nfacilitate interoperability, all clients adopt a common fusion-layer output\ndimension, which permits each model to be partitioned into a personalized base\nblock and a generalized modular block. Clients share their fusion-layer\noutputs, keeping model parameters and architectures private. Experimental\nresults demonstrate that the framework achieves superior communication\nefficiency compared to federated learning (FL) and federated split learning\n(FSL) baselines, while ensuring stable training performance across\nheterogeneous architectures.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u4fe1\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u652f\u6301\u5f02\u6784\u6a21\u578b\u534f\u540c\u5b66\u4e60\uff0c\u901a\u8fc7\u5171\u4eab\u878d\u5408\u5c42\u8f93\u51fa\u5b9e\u73b0\u4e92\u64cd\u4f5c\u6027\u4e0e\u9690\u79c1\u4fdd\u62a4\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u901a\u4fe1\u6548\u7387\u548c\u7a33\u5b9a\u7684\u8bad\u7ec3\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u6a21\u578b\u67b6\u6784\u5728\u534f\u540c\u5b66\u4e60\u4e2d\u4e92\u64cd\u4f5c\u6027\u4e0e\u9690\u79c1\u4fdd\u62a4\u7684\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u901a\u4fe1\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u652f\u6301\u6a21\u578b\u5f02\u6784\u6027\u5e76\u5141\u8bb8\u63a8\u7406\u65f6\u7684\u6a21\u5757\u5316\u7ec4\u5408\u3002\u6240\u6709\u5ba2\u6237\u7aef\u91c7\u7528\u5171\u540c\u7684\u878d\u5408\u5c42\u8f93\u51fa\u7ef4\u5ea6\uff0c\u5c06\u6a21\u578b\u5212\u5206\u4e3a\u4e2a\u6027\u5316\u57fa\u7840\u5757\u548c\u901a\u7528\u6a21\u5757\u5316\u5757\u3002\u5ba2\u6237\u7aef\u4ec5\u5171\u4eab\u878d\u5408\u5c42\u8f93\u51fa\uff0c\u4ece\u800c\u4fdd\u62a4\u6a21\u578b\u53c2\u6570\u548c\u67b6\u6784\u7684\u9690\u79c1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u8054\u90a6\u5b66\u4e60(FL)\u548c\u8054\u90a6\u62c6\u5206\u5b66\u4e60(FSL)\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u901a\u4fe1\u6548\u7387\uff0c\u5e76\u786e\u4fdd\u4e86\u5f02\u6784\u67b6\u6784\u4e0b\u7684\u7a33\u5b9a\u8bad\u7ec3\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u6a21\u578b\u534f\u540c\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u6548\u7387\u3001\u4e92\u64cd\u4f5c\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7a33\u5b9a\u7684\u8bad\u7ec3\u6027\u80fd\u3002"}}
{"id": "2509.23398", "pdf": "https://arxiv.org/pdf/2509.23398", "abs": "https://arxiv.org/abs/2509.23398", "authors": ["Tu\u011f\u00e7e Bilen", "Mehmet \u00d6zdem"], "title": "Knowledge-Defined and Twin-Assisted Network Management for 6G", "categories": ["cs.NI"], "comment": null, "summary": "The increasing complexity, dynamism, and heterogeneity of 6G networks demand\nmanagement systems that can reason proactively and generalize beyond\npre-defined cases. In this paper, we propose a modular, knowledge-defined\narchitecture that integrates Digital Twin models with semantic reasoning and\nzero-shot learning to enable autonomous decision-making for previously unseen\nnetwork scenarios. Real-time data streams are used to maintain synchronized\nvirtual replicas of the physical network, which also forecast short-term state\ntransitions. These predictions feed into a knowledge plane that constructs and\nupdates a graph-based abstraction of the network, enabling context-aware intent\ngeneration via graph neural reasoning. To ensure adaptability without\nretraining, the management plane performs zero-shot policy matching by\nsemantically embedding candidate intents and selecting suitable pre-learned\nactions. The selected decisions are translated and enforced through the control\nplane, while a closed-loop feedback mechanism continuously refines predictions,\nknowledge, and policies over time. Simulation results confirm that the proposed\nframework observes notable improvements in policy response time, SLA compliance\nrate, and intent matching accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6a21\u5757\u5316\u3001\u77e5\u8bc6\u5b9a\u4e49\u7684\u67b6\u6784\uff0c\u6574\u5408\u6570\u5b57\u5b6a\u751f\u3001\u8bed\u4e49\u63a8\u7406\u548c\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u4ee5\u5b9e\u73b06G\u7f51\u7edc\u5728\u672a\u89c1\u573a\u666f\u4e0b\u7684\u81ea\u4e3b\u51b3\u7b56\u3002", "motivation": "6G\u7f51\u7edc\u65e5\u76ca\u589e\u957f\u7684\u590d\u6742\u6027\u3001\u52a8\u6001\u6027\u548c\u5f02\u6784\u6027\uff0c\u8981\u6c42\u7ba1\u7406\u7cfb\u7edf\u80fd\u591f\u4e3b\u52a8\u63a8\u7406\u5e76\u6cdb\u5316\u5230\u9884\u5b9a\u4e49\u573a\u666f\u4e4b\u5916\u3002", "method": "\u8be5\u67b6\u6784\u901a\u8fc7\u5b9e\u65f6\u6570\u636e\u6d41\u7ef4\u62a4\u7269\u7406\u7f51\u7edc\u7684\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u5e76\u9884\u6d4b\u77ed\u671f\u72b6\u6001\u3002\u9884\u6d4b\u7ed3\u679c\u9001\u5165\u77e5\u8bc6\u5e73\u9762\uff0c\u6784\u5efa\u5e76\u66f4\u65b0\u57fa\u4e8e\u56fe\u7684\u7f51\u7edc\u62bd\u8c61\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u610f\u56fe\u3002\u7ba1\u7406\u5e73\u9762\u5229\u7528\u8bed\u4e49\u5d4c\u5165\u548c\u96f6\u6837\u672c\u7b56\u7565\u5339\u914d\u6765\u9009\u62e9\u9884\u5b66\u4e60\u7684\u52a8\u4f5c\uff0c\u5b9e\u73b0\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u9002\u5e94\u6027\u3002\u51b3\u7b56\u901a\u8fc7\u63a7\u5236\u5e73\u9762\u6267\u884c\uff0c\u5e76\u91c7\u7528\u95ed\u73af\u53cd\u9988\u673a\u5236\u6301\u7eed\u4f18\u5316\u9884\u6d4b\u3001\u77e5\u8bc6\u548c\u7b56\u7565\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8bc1\u5b9e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u7b56\u7565\u54cd\u5e94\u65f6\u95f4\u3001SLA\uff08\u670d\u52a1\u6c34\u5e73\u534f\u8bae\uff09\u5408\u89c4\u7387\u548c\u610f\u56fe\u5339\u914d\u51c6\u786e\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u3001\u8bed\u4e49\u63a8\u7406\u548c\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e866G\u7f51\u7edc\u5728\u590d\u6742\u548c\u672a\u89c1\u573a\u666f\u4e0b\u7684\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\u548c\u7ba1\u7406\u6548\u7387\u3002"}}
{"id": "2509.22715", "pdf": "https://arxiv.org/pdf/2509.22715", "abs": "https://arxiv.org/abs/2509.22715", "authors": ["Jiho Park", "Jongyoon Song", "Minjin Choi", "Kyuho Heo", "Taehun Huh", "Ji Won Kim"], "title": "TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large language models (LLMs) are increasingly integral as productivity\nassistants, but existing benchmarks fall short in rigorously evaluating their\nreal-world instruction-following capabilities. Current benchmarks often (i)\nlack sufficient multilinguality, (ii) fail to capture the implicit constraints\ninherent in user requests, and (iii) overlook the complexities of multi-turn\ndialogue. To address these critical gaps and provide a more realistic\nassessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation\nBenchmark)1, a novel benchmark specifically designed for LLM-based productivity\nassistants. TRUEBench distinguishes itself by featuring input prompts across 12\nlanguages, incorporating intra-instance multilingual instructions, employing\nrigorous evaluation criteria to capture both explicit and implicit constraints,\nand including complex multi-turn dialogue scenarios with both accumulating\nconstraints and context switches. Furthermore, to ensure reliability in\nevaluation, we refined constraints using an LLM validator. Extensive\nexperiments demonstrate that TRUEBench presents significantly greater\nchallenges than existing benchmarks; for instance, a strong model like OpenAI\no1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and\nrealistic assessment of LLMs in practical productivity settings, highlighting\ntheir capabilities and limitations.", "AI": {"tldr": "\u5f15\u5165TRUEBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u751f\u4ea7\u529b\u52a9\u624b\u7684\u591a\u8bed\u8a00\u3001\u591a\u8f6e\u3001\u771f\u5b9e\u4e16\u754c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\uff08\u5982OpenAI o1\uff09\u5728\u6b64\u65b9\u9762\u9762\u4e34\u7684\u663e\u8457\u6311\u6218\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709LLM\u57fa\u51c6\u5728\u8bc4\u4f30\u5176\u771f\u5b9e\u4e16\u754c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u5177\u4f53\u8868\u73b0\u5728\uff1a\u7f3a\u4e4f\u591a\u8bed\u8a00\u6027\u3001\u672a\u80fd\u6355\u6349\u7528\u6237\u8bf7\u6c42\u4e2d\u7684\u9690\u5f0f\u7ea6\u675f\uff0c\u4ee5\u53ca\u5ffd\u89c6\u591a\u8f6e\u5bf9\u8bdd\u7684\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165TRUEBench\u57fa\u51c6\uff0c\u4e13\u95e8\u4e3aLLM\u751f\u4ea7\u529b\u52a9\u624b\u8bbe\u8ba1\u3002\u8be5\u57fa\u51c6\u5305\u542b12\u79cd\u8bed\u8a00\u7684\u8f93\u5165\u63d0\u793a\u3001\u5b9e\u4f8b\u5185\u591a\u8bed\u8a00\u6307\u4ee4\u3001\u4e25\u683c\u7684\u663e\u5f0f\u548c\u9690\u5f0f\u7ea6\u675f\u8bc4\u4f30\u6807\u51c6\uff0c\u4ee5\u53ca\u7d2f\u79ef\u7ea6\u675f\u548c\u4e0a\u4e0b\u6587\u5207\u6362\u7684\u590d\u6742\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u3002\u540c\u65f6\uff0c\u91c7\u7528LLM\u9a8c\u8bc1\u5668\u7cbe\u70bc\u7ea6\u675f\u4ee5\u786e\u4fdd\u8bc4\u4f30\u53ef\u9760\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eTRUEBench\u6bd4\u73b0\u6709\u57fa\u51c6\u66f4\u5177\u6311\u6218\u6027\uff1b\u4f8b\u5982\uff0c\u5f3a\u5927\u7684\u6a21\u578b\u5982OpenAI o1\u7684\u6574\u4f53\u901a\u8fc7\u7387\u4ec5\u4e3a69.07%\u3002", "conclusion": "TRUEBench\u4e3aLLM\u5728\u5b9e\u9645\u751f\u4ea7\u529b\u573a\u666f\u4e2d\u63d0\u4f9b\u4e86\u4e25\u82db\u4e14\u771f\u5b9e\u7684\u8bc4\u4f30\uff0c\u7a81\u51fa\u4e86\u5176\u80fd\u529b\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2509.22831", "pdf": "https://arxiv.org/pdf/2509.22831", "abs": "https://arxiv.org/abs/2509.22831", "authors": ["Sean Trott"], "title": "Toward a Theory of Generalizability in LLM Mechanistic Interpretability Research", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Research on Large Language Models (LLMs) increasingly focuses on identifying\nmechanistic explanations for their behaviors, yet the field lacks clear\nprinciples for determining when (and how) findings from one model instance\ngeneralize to another. This paper addresses a fundamental epistemological\nchallenge: given a mechanistic claim about a particular model, what justifies\nextrapolating this finding to other LLMs -- and along which dimensions might\nsuch generalizations hold? I propose five potential axes of correspondence\nalong which mechanistic claims might generalize, including: functional (whether\nthey satisfy the same functional criteria), developmental (whether they develop\nat similar points during pretraining), positional (whether they occupy similar\nabsolute or relative positions), relational (whether they interact with other\nmodel components in similar ways), and configurational (whether they correspond\nto particular regions or structures in weight-space). To empirically validate\nthis framework, I analyze \"1-back attention heads\" (components attending to\nprevious tokens) across pretraining in random seeds of the Pythia models (14M,\n70M, 160M, 410M). The results reveal striking consistency in the developmental\ntrajectories of 1-back attention across models, while positional consistency is\nmore limited. Moreover, seeds of larger models systematically show earlier\nonsets, steeper slopes, and higher peaks of 1-back attention. I also address\npossible objections to the arguments and proposals outlined here. Finally, I\nconclude by arguing that progress on the generalizability of mechanistic\ninterpretability research will consist in mapping constitutive design\nproperties of LLMs to their emergent behaviors and mechanisms.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u673a\u5236\u89e3\u91ca\u7684\u53ef\u6cdb\u5316\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e94\u79cd\u5bf9\u5e94\u8f74\uff0c\u5e76\u901a\u8fc7\u5bf9Pythia\u6a21\u578b\u4e2d\u201c1-back\u6ce8\u610f\u529b\u5934\u201d\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5176\u53d1\u5c55\u8f68\u8ff9\u7684\u9ad8\u5ea6\u4e00\u81f4\u6027\u548c\u6a21\u578b\u89c4\u6a21\u5bf9\u51fa\u73b0\u65f6\u95f4\u7684\u5f71\u54cd\u3002", "motivation": "LLMs\u7684\u673a\u5236\u89e3\u91ca\u7814\u7a76\u7f3a\u4e4f\u660e\u786e\u7684\u539f\u5219\u6765\u5224\u65ad\u5176\u53d1\u73b0\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u80fd\u6cdb\u5316\u5230\u5176\u4ed6\u6a21\u578b\u5b9e\u4f8b\uff0c\u5b58\u5728\u5982\u4f55\u63a8\u65ad\u673a\u5236\u4e3b\u5f20\u7684\u6839\u672c\u6027\u8ba4\u8bc6\u8bba\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u529f\u80fd\u6027\u3001\u53d1\u5c55\u6027\u3001\u4f4d\u7f6e\u6027\u3001\u5173\u7cfb\u6027\u548c\u914d\u7f6e\u6027\u4e94\u79cd\u6f5c\u5728\u7684\u5bf9\u5e94\u8f74\uff0c\u7528\u4e8e\u8861\u91cf\u673a\u5236\u4e3b\u5f20\u7684\u6cdb\u5316\u80fd\u529b\u3002\u901a\u8fc7\u5206\u6790Pythia\u7cfb\u5217\u6a21\u578b\uff0814M, 70M, 160M, 410M\uff09\u5728\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u968f\u673a\u79cd\u5b50\u4e0b\u7684\u201c1-back\u6ce8\u610f\u529b\u5934\u201d\uff0c\u5bf9\u8be5\u6846\u67b6\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u201c1-back\u6ce8\u610f\u529b\u201d\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u7684\u53d1\u5c55\u8f68\u8ff9\u8868\u73b0\u51fa\u663e\u8457\u7684\u4e00\u81f4\u6027\uff0c\u800c\u4f4d\u7f6e\u4e00\u81f4\u6027\u5219\u8f83\u4e3a\u6709\u9650\u3002\u6b64\u5916\uff0c\u8f83\u5927\u6a21\u578b\u7684\u79cd\u5b50\u7cfb\u7edf\u6027\u5730\u5c55\u73b0\u51fa\u201c1-back\u6ce8\u610f\u529b\u201d\u66f4\u65e9\u7684\u51fa\u73b0\u3001\u66f4\u9661\u5ced\u7684\u659c\u7387\u548c\u66f4\u9ad8\u7684\u5cf0\u503c\u3002", "conclusion": "\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u7684\u6cdb\u5316\u6027\u8fdb\u5c55\u5728\u4e8e\u5c06LLMs\u7684\u6784\u6210\u6027\u8bbe\u8ba1\u5c5e\u6027\u6620\u5c04\u5230\u5176\u6d8c\u73b0\u884c\u4e3a\u548c\u673a\u5236\u4e0a\u3002"}}
{"id": "2509.22689", "pdf": "https://arxiv.org/pdf/2509.22689", "abs": "https://arxiv.org/abs/2509.22689", "authors": ["Ha-Hieu Pham", "Minh Le", "Han Huynh", "Nguyen Quoc Khanh Le", "Huy-Hieu Pham"], "title": "Graph-Theoretic Consistency for Robust and Topology-Aware Semi-Supervised Histopathology Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Semi-supervised semantic segmentation (SSSS) is vital in computational\npathology, where dense annotations are costly and limited. Existing methods\noften rely on pixel-level consistency, which propagates noisy pseudo-labels and\nproduces fragmented or topologically invalid masks. We propose Topology Graph\nConsistency (TGC), a framework that integrates graph-theoretic constraints by\naligning Laplacian spectra, component counts, and adjacency statistics between\nprediction graphs and references. This enforces global topology and improves\nsegmentation accuracy. Experiments on GlaS and CRAG demonstrate that TGC\nachieves state-of-the-art performance under 5-10% supervision and significantly\nnarrows the gap to full supervision. Code is available at\nhttps://github.com/hieuphamha19/TGC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u62d3\u6251\u56fe\u4e00\u81f4\u6027\uff08TGC\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u56fe\u8bba\u7ea6\u675f\u89e3\u51b3\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u62d3\u6251\u7f3a\u9677\uff0c\u5728\u6709\u9650\u76d1\u7763\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u5bc6\u96c6\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u4e14\u53d7\u9650\uff0c\u73b0\u6709\u534a\u76d1\u7763\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u50cf\u7d20\u7ea7\u4e00\u81f4\u6027\uff0c\u6613\u4ea7\u751f\u542b\u566a\u58f0\u3001\u788e\u7247\u5316\u6216\u62d3\u6251\u65e0\u6548\u7684\u5206\u5272\u7ed3\u679c\u3002", "method": "\u63d0\u51faTGC\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u9f50\u9884\u6d4b\u56fe\u4e0e\u53c2\u8003\u56fe\u7684\u62c9\u666e\u62c9\u65af\u8c31\u3001\u8fde\u901a\u5206\u91cf\u8ba1\u6570\u548c\u90bb\u63a5\u7edf\u8ba1\uff0c\u5f15\u5165\u56fe\u8bba\u7ea6\u675f\u4ee5\u5f3a\u5236\u6267\u884c\u5168\u5c40\u62d3\u6251\u7ed3\u6784\u3002", "result": "\u5728GlaS\u548cCRAG\u6570\u636e\u96c6\u4e0a\uff0cTGC\u57285-10%\u7684\u76d1\u7763\u6bd4\u4f8b\u4e0b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u5b8c\u5168\u76d1\u7763\u65b9\u6cd5\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "TGC\u6709\u6548\u89e3\u51b3\u4e86\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u62d3\u6251\u95ee\u9898\uff0c\u5728\u6709\u9650\u76d1\u7763\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u548c\u62d3\u6251\u6709\u6548\u6027\u3002"}}
{"id": "2509.22840", "pdf": "https://arxiv.org/pdf/2509.22840", "abs": "https://arxiv.org/abs/2509.22840", "authors": ["Micah Adler"], "title": "On the Capacity of Self-Attention", "categories": ["cs.LG", "I.2.0"], "comment": null, "summary": "While self-attention is known to learn relations among tokens, we lack a\nformal understanding of its capacity: how many distinct relations can a single\nlayer reliably recover for a given budget?\n  To formalize this, we introduce Relational Graph Recognition (RGR), where the\nkey-query channel represents a graph on $m$ items with $m'$ directed edges,\nand, given a context of items, must recover the neighbors of each item. We\nmeasure resources by the total key dimension $D_K = h\\,d_k$. Within this\nframework, we analytically derive a capacity scaling law and validate it\nempirically. We show that $D_K = \\Theta(m' \\log m' / d_{\\text{model}})$ is both\nnecessary (information-theoretic lower bound) and sufficient (explicit\nconstruction) in a broad class of graphs to recover $m'$ relations. This\nscaling law directly leads to a new, capacity-based rationale for multi-head\nattention that applies even when each item only attends to a single target.\nWhen embeddings are uncompressed ($m = d_{\\text{model}}$) and the graph is a\npermutation, a single head suffices. However, compression ($m >\nd_{\\text{model}}$) forces relations into overlapping subspaces, creating\ninterference that a single large head cannot disentangle. Our analysis shows\nthat allocating a fixed $D_K$ across many small heads mitigates this\ninterference, increasing the number of recoverable relations. Controlled\nsingle-layer experiments mirror the theory, revealing a sharp performance\nthreshold that matches the predicted capacity scaling and confirms the benefit\nof distributing $D_K$ across multiple heads.\n  Altogether, these results provide a concrete scaling law for self-attention\ncapacity and a principled design rule for allocating key-query budget across\nheads.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.23401", "pdf": "https://arxiv.org/pdf/2509.23401", "abs": "https://arxiv.org/abs/2509.23401", "authors": ["Mustafa Yavuz Engin", "Mehmet Ozdem", "Tu\u011f\u00e7e Bilen"], "title": "AUV-Assisted Underwater 6G: Environmental Modeling and Multi-Stage Optimization", "categories": ["cs.NI"], "comment": null, "summary": "This study presents a simulation model for underwater 6G networks, focusing\non the optimized placement of sensors, AUVs, and hubs. The network architecture\nconsists of fixed hub stations, mobile autonomous underwater vehicles (AUVs),\nand numerous sensor nodes. Environmental parameters such as temperature,\nsalinity, and conductivity are considered in the transmission of\nelectromagnetic signals; signal attenuation and transmission delays are\ncalculated based on physical models. The optimization process begins with\nK-Means clustering, followed by sequential application of Genetic Algorithm\n(GA) and Particle Swarm Optimization (PSO) to refine the cluster\nconfigurations. The simulation includes key network dynamics such as multi-hop\ndata transmission, cluster leader selection, queue management, and traffic load\nbalancing. To compare performance, two distinct scenarios -- one with cluster\nleaders and one without -- are modeled and visualized through a PyQt5-based\nreal-time graphical interface. The results demonstrate that 6G network\narchitectures in underwater environments can be effectively modeled and\noptimized by incorporating environmental conditions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6c34\u4e0b6G\u7f51\u7edc\u4eff\u771f\u6a21\u578b\uff0c\u4f18\u5316\u4f20\u611f\u5668\u3001AUV\u548c\u96c6\u7ebf\u5668\u7684\u90e8\u7f72\uff0c\u5e76\u8003\u8651\u73af\u5883\u56e0\u7d20\u8fdb\u884c\u4fe1\u53f7\u4f20\u8f93\u548c\u7f51\u7edc\u7ba1\u7406\u3002", "motivation": "\u4e3a\u6c34\u4e0b6G\u7f51\u7edc\u63d0\u4f9b\u4e00\u4e2a\u4f18\u5316\u7684\u4f20\u611f\u5668\u3001AUV\u548c\u96c6\u7ebf\u5668\u90e8\u7f72\u7b56\u7565\uff0c\u4ee5\u5e94\u5bf9\u6c34\u4e0b\u73af\u5883\u5e26\u6765\u7684\u4f20\u8f93\u6311\u6218\u3002", "method": "\u6784\u5efa\u5305\u542b\u56fa\u5b9a\u96c6\u7ebf\u5668\u3001AUV\u548c\u4f20\u611f\u5668\u8282\u70b9\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u8003\u8651\u6e29\u5ea6\u3001\u76d0\u5ea6\u548c\u7535\u5bfc\u7387\u7b49\u73af\u5883\u53c2\u6570\u8ba1\u7b97\u4fe1\u53f7\u8870\u51cf\u548c\u5ef6\u8fdf\u3002\u4f18\u5316\u8fc7\u7a0b\u91c7\u7528K-Means\u805a\u7c7b\u3001\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\u548c\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u3002\u4eff\u771f\u6db5\u76d6\u591a\u8df3\u4f20\u8f93\u3001\u7c07\u5934\u9009\u62e9\u3001\u961f\u5217\u7ba1\u7406\u548c\u8d1f\u8f7d\u5747\u8861\uff0c\u5e76\u901a\u8fc7PyQt5\u754c\u9762\u53ef\u89c6\u5316\uff0c\u6bd4\u8f83\u4e86\u6709\u7c07\u5934\u548c\u65e0\u7c07\u5934\u4e24\u79cd\u573a\u666f\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u6574\u5408\u73af\u5883\u6761\u4ef6\uff0c\u6c34\u4e0b6G\u7f51\u7edc\u67b6\u6784\u53ef\u4ee5\u88ab\u6709\u6548\u5730\u5efa\u6a21\u548c\u4f18\u5316\u3002", "conclusion": "\u6c34\u4e0b\u73af\u5883\u4e0b\u76846G\u7f51\u7edc\u53ef\u4ee5\u901a\u8fc7\u8003\u8651\u73af\u5883\u56e0\u7d20\u8fdb\u884c\u6709\u6548\u7684\u5efa\u6a21\u548c\u4f18\u5316\uff0c\u4ece\u800c\u5b9e\u73b0\u7f51\u7edc\u7ec4\u4ef6\u7684\u7406\u60f3\u90e8\u7f72\u548c\u9ad8\u6548\u8fd0\u884c\u3002"}}
{"id": "2509.22729", "pdf": "https://arxiv.org/pdf/2509.22729", "abs": "https://arxiv.org/abs/2509.22729", "authors": ["Sadia Abdulhalim", "Muaz Albaghdadi", "Moshiur Farazi"], "title": "Multi-Modal Sentiment Analysis with Dynamic Attention Fusion", "categories": ["cs.CL", "cs.AI"], "comment": "Paper accepted for presentation at the ACS/IEEE 22nd International\n  Conference on Computer Systems and Applications (AICCSA 2025)", "summary": "Traditional sentiment analysis has long been a unimodal task, relying solely\non text. This approach overlooks non-verbal cues such as vocal tone and prosody\nthat are essential for capturing true emotional intent. We introduce Dynamic\nAttention Fusion (DAF), a lightweight framework that combines frozen text\nembeddings from a pretrained language model with acoustic features from a\nspeech encoder, using an adaptive attention mechanism to weight each modality\nper utterance. Without any finetuning of the underlying encoders, our proposed\nDAF model consistently outperforms both static fusion and unimodal baselines on\na large multimodal benchmark. We report notable gains in F1-score and\nreductions in prediction error and perform a variety of ablation studies that\nsupport our hypothesis that the dynamic weighting strategy is crucial for\nmodeling emotionally complex inputs. By effectively integrating verbal and\nnon-verbal information, our approach offers a more robust foundation for\nsentiment prediction and carries broader impact for affective computing\napplications -- from emotion recognition and mental health assessment to more\nnatural human computer interaction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u52a8\u6001\u6ce8\u610f\u529b\u878d\u5408\uff08DAF\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\u7ed3\u5408\u6587\u672c\u548c\u8bed\u97f3\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u5fae\u8c03\u5e95\u5c42\u7f16\u7801\u5668\u3002", "motivation": "\u4f20\u7edf\u60c5\u611f\u5206\u6790\u4ec5\u4f9d\u8d56\u6587\u672c\uff0c\u5ffd\u7565\u4e86\u8bed\u97f3\u8bed\u8c03\u7b49\u975e\u8bed\u8a00\u7ebf\u7d22\u5bf9\u6355\u6349\u771f\u5b9e\u60c5\u611f\u610f\u56fe\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "\u5f15\u5165\u8f7b\u91cf\u7ea7\u52a8\u6001\u6ce8\u610f\u529b\u878d\u5408\uff08DAF\uff09\u6846\u67b6\uff0c\u5b83\u5c06\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u56fa\u5b9a\u6587\u672c\u5d4c\u5165\u4e0e\u8bed\u97f3\u7f16\u7801\u5668\u7684\u58f0\u5b66\u7279\u5f81\u7ed3\u5408\u8d77\u6765\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u6bcf\u4e2a\u8bdd\u8bed\u7684\u6a21\u6001\u8fdb\u884c\u52a0\u6743\uff0c\u4e14\u4e0d\u8fdb\u884c\u5e95\u5c42\u7f16\u7801\u5668\u7684\u5fae\u8c03\u3002", "result": "DAF\u6a21\u578b\u5728\u5927\u578b\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u9759\u6001\u878d\u5408\u548c\u5355\u6a21\u6001\u57fa\u7ebf\uff0cF1\u5206\u6570\u663e\u8457\u63d0\u9ad8\uff0c\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\uff0c\u52a8\u6001\u52a0\u6743\u7b56\u7565\u5bf9\u4e8e\u5efa\u6a21\u590d\u6742\u60c5\u611f\u8f93\u5165\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u6574\u5408\u8bed\u8a00\u548c\u975e\u8bed\u8a00\u4fe1\u606f\uff0c\u4e3a\u60c5\u611f\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u57fa\u7840\uff0c\u5bf9\u60c5\u611f\u8ba1\u7b97\u5e94\u7528\uff08\u5982\u60c5\u611f\u8bc6\u522b\u3001\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u3001\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\uff09\u5177\u6709\u5e7f\u6cdb\u5f71\u54cd\u3002"}}
{"id": "2509.22888", "pdf": "https://arxiv.org/pdf/2509.22888", "abs": "https://arxiv.org/abs/2509.22888", "authors": ["Louie Hong Yao", "Nicholas Jarvis", "Tiffany Zhan", "Saptarshi Ghosh", "Linfeng Liu", "Tianyu Jiang"], "title": "JE-IRT: A Geometric Lens on LLM Abilities through Joint Embedding Item Response Theory", "categories": ["cs.AI", "cs.CL"], "comment": "22 pages, 10 figures, 5 tables", "summary": "Standard LLM evaluation practices compress diverse abilities into single\nscores, obscuring their inherently multidimensional nature. We present JE-IRT,\na geometric item-response framework that embeds both LLMs and questions in a\nshared space. For question embeddings, the direction encodes semantics and the\nnorm encodes difficulty, while correctness on each question is determined by\nthe geometric interaction between the model and question embeddings. This\ngeometry replaces a global ranking of LLMs with topical specialization and\nenables smooth variation across related questions. Building on this framework,\nour experimental results reveal that out-of-distribution behavior can be\nexplained through directional alignment, and that larger norms consistently\nindicate harder questions. Moreover, JE-IRT naturally supports generalization:\nonce the space is learned, new LLMs are added by fitting a single embedding.\nThe learned space further reveals an LLM-internal taxonomy that only partially\naligns with human-defined subject categories. JE-IRT thus establishes a unified\nand interpretable geometric lens that connects LLM abilities with the structure\nof questions, offering a distinctive perspective on model evaluation and\ngeneralization.", "AI": {"tldr": "JE-IRT\u662f\u4e00\u79cd\u51e0\u4f55\u9879\u76ee\u54cd\u5e94\u6846\u67b6\uff0c\u5b83\u5c06LLM\u548c\u95ee\u9898\u5d4c\u5165\u5171\u4eab\u7a7a\u95f4\uff0c\u901a\u8fc7\u51e0\u4f55\u4ea4\u4e92\u8bc4\u4f30LLM\u80fd\u529b\uff0c\u63ed\u793a\u591a\u7ef4\u5ea6\u4e13\u4e1a\u5316\uff0c\u5e76\u652f\u6301\u6cdb\u5316\u3002", "motivation": "\u6807\u51c6LLM\u8bc4\u4f30\u65b9\u6cd5\u5c06\u591a\u6837\u80fd\u529b\u538b\u7f29\u4e3a\u5355\u4e00\u5206\u6570\uff0c\u63a9\u76d6\u4e86\u5176\u56fa\u6709\u7684\u591a\u7ef4\u6027\u8d28\u3002", "method": "\u63d0\u51faJE-IRT\u51e0\u4f55\u9879\u76ee\u54cd\u5e94\u6846\u67b6\uff0c\u5c06LLM\u548c\u95ee\u9898\u5d4c\u5165\u5171\u4eab\u7a7a\u95f4\u3002\u95ee\u9898\u5d4c\u5165\u7684\u65b9\u5411\u7f16\u7801\u8bed\u4e49\uff0c\u8303\u6570\u7f16\u7801\u96be\u5ea6\uff0cLLM\u5728\u95ee\u9898\u4e0a\u7684\u6b63\u786e\u6027\u7531\u6a21\u578b\u4e0e\u95ee\u9898\u5d4c\u5165\u7684\u51e0\u4f55\u4ea4\u4e92\u51b3\u5b9a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOOD\u884c\u4e3a\u53ef\u901a\u8fc7\u65b9\u5411\u5bf9\u9f50\u89e3\u91ca\uff0c\u8f83\u5927\u8303\u6570\u59cb\u7ec8\u6307\u793a\u66f4\u96be\u7684\u95ee\u9898\u3002JE-IRT\u652f\u6301\u6cdb\u5316\uff0c\u65b0LLM\u53ea\u9700\u62df\u5408\u5355\u4e2a\u5d4c\u5165\u5373\u53ef\u6dfb\u52a0\u3002\u5b66\u4e60\u5230\u7684\u7a7a\u95f4\u63ed\u793a\u4e86LLM\u5185\u90e8\u5206\u7c7b\uff0c\u4e0e\u4eba\u5de5\u5b9a\u4e49\u7684\u4e3b\u9898\u7c7b\u522b\u90e8\u5206\u4e00\u81f4\u3002", "conclusion": "JE-IRT\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u89e3\u91ca\u7684\u51e0\u4f55\u89c6\u89d2\uff0c\u8fde\u63a5\u4e86LLM\u80fd\u529b\u4e0e\u95ee\u9898\u7ed3\u6784\uff0c\u4e3a\u6a21\u578b\u8bc4\u4f30\u548c\u6cdb\u5316\u63d0\u4f9b\u4e86\u72ec\u7279\u89c6\u89d2\u3002"}}
{"id": "2509.22690", "pdf": "https://arxiv.org/pdf/2509.22690", "abs": "https://arxiv.org/abs/2509.22690", "authors": ["Andrea Asperti", "Salvatore Fiorilla", "Simone Nardi", "Lorenzo Orsini"], "title": "A review of Recent Techniques for Person Re-Identification", "categories": ["cs.CV", "68T45, 65D19"], "comment": null, "summary": "Person re-identification (ReId), a crucial task in surveillance, involves\nmatching individuals across different camera views. The advent of Deep\nLearning, especially supervised techniques like Convolutional Neural Networks\nand Attention Mechanisms, has significantly enhanced person Re-ID. However, the\nsuccess of supervised approaches hinges on vast amounts of annotated data,\nposing scalability challenges in data labeling and computational costs. To\naddress these limitations, recent research has shifted towards unsupervised\nperson re-identification. Leveraging abundant unlabeled data, unsupervised\nmethods aim to overcome the need for pairwise labelled data. Although\ntraditionally trailing behind supervised approaches, unsupervised techniques\nhave shown promising developments in recent years, signalling a narrowing\nperformance gap. Motivated by this evolving landscape, our survey pursues two\nprimary objectives. First, we review and categorize significant publications in\nsupervised person re-identification, providing an in-depth overview of the\ncurrent state-of-the-art and emphasizing little room for further improvement in\nthis domain. Second, we explore the latest advancements in unsupervised person\nre-identification over the past three years, offering insights into emerging\ntrends and shedding light on the potential convergence of performance between\nsupervised and unsupervised paradigms. This dual-focus survey aims to\ncontribute to the evolving narrative of person re-identification, capturing\nboth the mature landscape of supervised techniques and the promising outcomes\nin the realm of unsupervised learning.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u5206\u6790\u4e86\u4eba\u5458\u91cd\u8bc6\u522b\uff08ReId\uff09\u9886\u57df\uff0c\u5bf9\u6bd4\u4e86\u6210\u719f\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u548c\u65b0\u5174\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u91cd\u70b9\u5f3a\u8c03\u4e86\u65e0\u76d1\u7763\u65b9\u6cd5\u8fd1\u5e74\u6765\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\u53ca\u5176\u4e0e\u76d1\u7763\u65b9\u6cd5\u8d8b\u540c\u7684\u6f5c\u529b\u3002", "motivation": "\u76d1\u7763\u5f0f\u4eba\u5458\u91cd\u8bc6\u522b\u65b9\u6cd5\u56e0\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u800c\u9762\u4e34\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u6210\u672c\u95ee\u9898\u3002\u4e3a\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u65e0\u76d1\u7763\u65b9\u6cd5\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u6027\u80fd\u5dee\u8ddd\u6b63\u5728\u7f29\u5c0f\uff0c\u4fc3\u4f7f\u672c\u7814\u7a76\u5bf9\u8be5\u9886\u57df\u8fdb\u884c\u5168\u9762\u56de\u987e\u3002", "method": "\u672c\u6587\u9996\u5148\u56de\u987e\u5e76\u5206\u7c7b\u4e86\u76d1\u7763\u5f0f\u4eba\u5458\u91cd\u8bc6\u522b\u7684\u91cd\u8981\u6587\u732e\uff0c\u63d0\u4f9b\u4e86\u5176\u6700\u65b0\u6280\u672f\u6982\u89c8\u3002\u5176\u6b21\uff0c\u63a2\u8ba8\u4e86\u8fc7\u53bb\u4e09\u5e74\u65e0\u76d1\u7763\u4eba\u5458\u91cd\u8bc6\u522b\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5206\u6790\u4e86\u65b0\u5174\u8d8b\u52bf\u3002", "result": "\u76d1\u7763\u5f0f\u4eba\u5458\u91cd\u8bc6\u522b\u6280\u672f\u5df2\u63a5\u8fd1\u6210\u719f\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u7a7a\u95f4\u6709\u9650\u3002\u65e0\u76d1\u7763\u4eba\u5458\u91cd\u8bc6\u522b\u8fd1\u5e74\u6765\u53d1\u5c55\u8fc5\u901f\uff0c\u5176\u6027\u80fd\u6b63\u9010\u6e10\u63a5\u8fd1\u76d1\u7763\u5f0f\u65b9\u6cd5\uff0c\u9884\u793a\u7740\u4e24\u8005\u6027\u80fd\u53ef\u80fd\u8d8b\u4e8e\u4e00\u81f4\u3002", "conclusion": "\u672c\u53cc\u91cd\u805a\u7126\u7684\u7efc\u8ff0\u65e8\u5728\u4fc3\u8fdb\u4eba\u5458\u91cd\u8bc6\u522b\u9886\u57df\u7684\u53d1\u5c55\uff0c\u5168\u9762\u6db5\u76d6\u4e86\u6210\u719f\u7684\u76d1\u7763\u6280\u672f\u548c\u65e0\u76d1\u7763\u5b66\u4e60\u9886\u57df\u5145\u6ee1\u524d\u666f\u7684\u6210\u679c\u3002"}}
{"id": "2509.22850", "pdf": "https://arxiv.org/pdf/2509.22850", "abs": "https://arxiv.org/abs/2509.22850", "authors": ["Roie Kazoom", "Yuval Ratzabi", "Etamar Rothstein", "Ofer Hadar"], "title": "Boundary on the Table: Efficient Black-Box Decision-Based Attacks for Structured Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Adversarial robustness in structured data remains an underexplored frontier\ncompared to vision and language domains. In this work, we introduce a novel\nblack-box, decision-based adversarial attack tailored for tabular data. Our\napproach combines gradient-free direction estimation with an iterative boundary\nsearch, enabling efficient navigation of discrete and continuous feature spaces\nunder minimal oracle access. Extensive experiments demonstrate that our method\nsuccessfully compromises nearly the entire test set across diverse models,\nranging from classical machine learning classifiers to large language model\n(LLM)-based pipelines. Remarkably, the attack achieves success rates\nconsistently above 90%, while requiring only a small number of queries per\ninstance. These results highlight the critical vulnerability of tabular models\nto adversarial perturbations, underscoring the urgent need for stronger\ndefenses in real-world decision-making systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u8868\u683c\u6570\u636e\u7684\u9ed1\u76d2\u3001\u57fa\u4e8e\u51b3\u7b56\u7684\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u5728\u5c11\u91cf\u67e5\u8be2\u4e0b\u4ee5\u8d8590%\u7684\u6210\u529f\u7387\u653b\u51fb\u591a\u79cd\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u8868\u683c\u6a21\u578b\u7684\u4e25\u91cd\u8106\u5f31\u6027\u3002", "motivation": "\u4e0e\u56fe\u50cf\u548c\u8bed\u8a00\u9886\u57df\u76f8\u6bd4\uff0c\u7ed3\u6784\u5316\uff08\u8868\u683c\uff09\u6570\u636e\u7684\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u4ecd\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u9ed1\u76d2\u3001\u57fa\u4e8e\u51b3\u7b56\u7684\u8868\u683c\u6570\u636e\u5bf9\u6297\u6027\u653b\u51fb\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u65e0\u68af\u5ea6\u65b9\u5411\u4f30\u8ba1\u548c\u8fed\u4ee3\u8fb9\u754c\u641c\u7d22\uff0c\u80fd\u5728\u6700\u5c0f\u9884\u8a00\u673a\u8bbf\u95ee\u4e0b\u9ad8\u6548\u5904\u7406\u79bb\u6563\u548c\u8fde\u7eed\u7279\u5f81\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u653b\u51fb\u4e86\u4ece\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u5230\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7ba1\u9053\u7b49\u591a\u79cd\u6a21\u578b\u7684\u51e0\u4e4e\u6574\u4e2a\u6d4b\u8bd5\u96c6\uff0c\u653b\u51fb\u6210\u529f\u7387\u6301\u7eed\u9ad8\u4e8e90%\uff0c\u4e14\u6bcf\u6b21\u653b\u51fb\u6240\u9700\u67e5\u8be2\u6b21\u6570\u5f88\u5c11\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86\u8868\u683c\u6a21\u578b\u5bf9\u5bf9\u6297\u6027\u6270\u52a8\u7684\u4e25\u91cd\u8106\u5f31\u6027\uff0c\u5f3a\u8c03\u4e86\u5728\u73b0\u5b9e\u4e16\u754c\u51b3\u7b56\u7cfb\u7edf\u4e2d\u52a0\u5f3a\u9632\u5fa1\u7684\u7d27\u8feb\u6027\u3002"}}
{"id": "2509.23522", "pdf": "https://arxiv.org/pdf/2509.23522", "abs": "https://arxiv.org/abs/2509.23522", "authors": ["Ehsan Eslami", "Walaa Hamouda"], "title": "Network Traffic Classification Using Self-Supervised Learning and Confident Learning", "categories": ["cs.NI"], "comment": null, "summary": "Network traffic classification (NTC) is vital for efficient network\nmanagement, security, and performance optimization, particularly with 5G/6G\ntechnologies. Traditional methods, such as deep packet inspection (DPI) and\nport-based identification, struggle with the rise of encrypted traffic and\ndynamic port allocations. Supervised learning methods provide viable\nalternatives but rely on large labeled datasets, which are difficult to acquire\ngiven the diversity and volume of network traffic. Meanwhile, unsupervised\nlearning methods, while less reliant on labeled data, often exhibit lower\naccuracy. To address these limitations, we propose a novel framework that first\nleverages Self-Supervised Learning (SSL) with techniques such as autoencoders\nor Tabular Contrastive Learning (TabCL) to generate pseudo-labels from\nextensive unlabeled datasets, addressing the challenge of limited labeled data.\nWe then apply traffic-adopted Confident Learning (CL) to refine these\npseudo-labels, enhancing classification precision by mitigating the impact of\nnoise. Our proposed framework offers a generalizable solution that minimizes\nthe need for extensive labeled data while delivering high accuracy. Extensive\nsimulations and evaluations, conducted using three datasets (ISCX VPN-nonVPN,\nself-generated dataset, and UCDavis--QUIC), and demonstrate that our method\nachieves superior accuracy compared to state-of-the-art techniques in\nclassifying network traffic.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u548c\u7f6e\u4fe1\u5b66\u4e60\uff08CL\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b35G/6G\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\u4e2d\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u548c\u4f20\u7edf\u65b9\u6cd5\u5931\u6548\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u5206\u7c7b\u3002", "motivation": "\u4f20\u7edf\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\u65b9\u6cd5\uff08\u5982DPI\u3001\u7aef\u53e3\u8bc6\u522b\uff09\u5728\u52a0\u5bc6\u6d41\u91cf\u548c\u52a8\u6001\u7aef\u53e3\u5206\u914d\u9762\u524d\u5931\u6548\u3002\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u8bb0\u6570\u636e\uff0c\u96be\u4ee5\u83b7\u53d6\uff1b\u65e0\u76d1\u7763\u5b66\u4e60\u7cbe\u5ea6\u8f83\u4f4e\u3002\u4e9f\u9700\u4e00\u79cd\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u7684\u6846\u67b6\u5206\u4e24\u6b65\uff1a1. \u5229\u7528\u81ea\u76d1\u7763\u5b66\u4e60\uff08\u5982\u81ea\u7f16\u7801\u5668\u6216Tabular Contrastive Learning\uff09\u4ece\u5927\u91cf\u672a\u6807\u8bb0\u6570\u636e\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u4ee5\u89e3\u51b3\u6807\u8bb0\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u30022. \u5e94\u7528\u6d41\u91cf\u81ea\u9002\u5e94\u7684\u7f6e\u4fe1\u5b66\u4e60\u6280\u672f\uff0c\u901a\u8fc7\u51cf\u5c11\u566a\u58f0\u6765\u7ec6\u5316\u4f2a\u6807\u7b7e\uff0c\u63d0\u9ad8\u5206\u7c7b\u7cbe\u5ea6\u3002", "result": "\u5728ISCX VPN-nonVPN\u3001\u81ea\u751f\u6210\u6570\u636e\u96c6\u548cUCDavis--QUIC\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u4eff\u771f\u548c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\u65b9\u9762\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u6807\u8bb0\u6570\u636e\u7684\u9700\u6c42\uff0c\u540c\u65f6\u4ecd\u80fd\u63d0\u4f9b\u9ad8\u51c6\u786e\u6027\uff0c\u6709\u6548\u5e94\u5bf9\u4e865G/6G\u73af\u5883\u4e0b\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\u7684\u6311\u6218\u3002"}}
{"id": "2509.22738", "pdf": "https://arxiv.org/pdf/2509.22738", "abs": "https://arxiv.org/abs/2509.22738", "authors": ["Parikshit Bansal", "Sujay Sanghavi"], "title": "Enabling Approximate Joint Sampling in Diffusion LMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In autoregressive language models, each token is sampled by conditioning on\nall the past tokens; the overall string has thus been sampled from the correct\nunderlying joint distribution represented by the model. In contrast, masked\ndiffusion language models generate text by unmasking tokens out of order and\npotentially in parallel. Generating an overall string sampled from the correct\nunderlying joint distribution would (again) require exactly one token unmasking\nin every full-model forward pass. The more tokens unmasked in parallel, the\nfurther away the string is from the true joint; this can be seen in the\nresulting drop in accuracy (but, increase in speed). In this paper we devise a\nway to {\\em approximately} sample multiple tokens from the joint distribution\nin a single full-model forward pass; we do so by developing a new lightweight\nsingle-layer ``sampler\" on top of an existing large diffusion LM. One forward\npass of the full model can now be followed by multiple forward passes of only\nthis sampler layer, to yield multiple unmasked tokens. Our sampler is trained\nto mimic exact joint sampling from the (frozen) full model. We show the\neffectiveness of our approximate joint sampling for both pretrained-only\n(Dream-7B-Base) and instruction-tuned (Dream-7B-Instruct) models on language\nmodeling and math \\& coding tasks. When four tokens are unmasked for each\nfull-model denoising step, our sampling algorithm achieves a MAUVE score of\n0.87 (vs marginal baseline of 0.31) with respect to the true joint\ndistribution.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u91c7\u6837\u5668\uff0c\u4f7f\u6269\u6563\u8bed\u8a00\u6a21\u578b\u80fd\u5728\u4e00\u6b21\u5168\u6a21\u578b\u524d\u5411\u4f20\u64ad\u540e\uff0c\u901a\u8fc7\u591a\u6b21\u91c7\u6837\u5668\u524d\u5411\u4f20\u64ad\u8fd1\u4f3c\u5e76\u884c\u91c7\u6837\u591a\u4e2atoken\uff0c\u4ee5\u5e73\u8861\u751f\u6210\u901f\u5ea6\u4e0e\u7b26\u5408\u8054\u5408\u5206\u5e03\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6548\u679c\u3002", "motivation": "\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u987a\u5e8f\u91c7\u6837\u786e\u4fdd\u7b26\u5408\u6f5c\u5728\u8054\u5408\u5206\u5e03\uff0c\u800c\u63a9\u7801\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e3a\u63d0\u9ad8\u901f\u5ea6\u800c\u5e76\u884c\u6216\u4e71\u5e8f\u89e3\u63a9\u7801\uff0c\u5bfc\u81f4\u504f\u79bb\u771f\u5b9e\u8054\u5408\u5206\u5e03\u5e76\u964d\u4f4e\u51c6\u786e\u6027\u3002\u73b0\u6709\u6269\u6563\u6a21\u578b\u82e5\u8981\u51c6\u786e\u91c7\u6837\uff0c\u4ecd\u9700\u5355\u6b21\u524d\u5411\u4f20\u64ad\u89e3\u63a9\u7801\u4e00\u4e2atoken\uff0c\u901f\u5ea6\u6162\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5982\u4f55\u5728\u5e76\u884c\u751f\u6210\u591atoken\u7684\u540c\u65f6\uff0c\u4ecd\u80fd\u8fd1\u4f3c\u5730\u4ece\u771f\u5b9e\u8054\u5408\u5206\u5e03\u4e2d\u91c7\u6837\u3002", "method": "\u5728\u73b0\u6709\u5927\u578b\u6269\u6563\u8bed\u8a00\u6a21\u578b\u4e4b\u4e0a\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u8f7b\u91cf\u7ea7\u7684\u5355\u5c42\u201c\u91c7\u6837\u5668\u201d\u3002\u8be5\u65b9\u6cd5\u5141\u8bb8\u4e00\u6b21\u5168\u6a21\u578b\u524d\u5411\u4f20\u64ad\u540e\uff0c\u7d27\u63a5\u7740\u8fdb\u884c\u591a\u6b21\u4ec5\u9488\u5bf9\u6b64\u91c7\u6837\u5668\u5c42\u7684\u5feb\u901f\u524d\u5411\u4f20\u64ad\uff0c\u4ee5\u89e3\u63a9\u7801\u591a\u4e2atoken\u3002\u8be5\u91c7\u6837\u5668\u88ab\u8bad\u7ec3\u6765\u6a21\u4eff\uff08\u51bb\u7ed3\u7684\uff09\u5168\u6a21\u578b\u7684\u7cbe\u786e\u8054\u5408\u91c7\u6837\u884c\u4e3a\u3002", "result": "\u8be5\u8fd1\u4f3c\u8054\u5408\u91c7\u6837\u65b9\u6cd5\u5728\u9884\u8bad\u7ec3\u6a21\u578b\uff08Dream-7B-Base\uff09\u548c\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\uff08Dream-7B-Instruct\uff09\u4e0a\uff0c\u5728\u8bed\u8a00\u5efa\u6a21\u548c\u6570\u5b66/\u7f16\u7a0b\u4efb\u52a1\u4e2d\u5747\u663e\u793a\u51fa\u6709\u6548\u6027\u3002\u5f53\u6bcf\u6b21\u5168\u6a21\u578b\u53bb\u566a\u6b65\u9aa4\u4e2d\u89e3\u63a9\u7801\u56db\u4e2atoken\u65f6\uff0c\u63d0\u51fa\u7684\u91c7\u6837\u7b97\u6cd5\u76f8\u5bf9\u4e8e\u771f\u5b9e\u8054\u5408\u5206\u5e03\u8fbe\u5230\u4e860.87\u7684MAUVE\u5206\u6570\uff0c\u8fdc\u9ad8\u4e8e\u8fb9\u7f18\u57fa\u7ebf\u76840.31\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u91c7\u6837\u5668\uff0c\u4f7f\u5f97\u6269\u6563\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u5355\u6b21\u5168\u6a21\u578b\u524d\u5411\u4f20\u64ad\u7684\u57fa\u7840\u4e0a\uff0c\u8fd1\u4f3c\u5730\u5e76\u884c\u91c7\u6837\u591a\u4e2atoken\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\uff08\u901a\u8fc7MAUVE\u5206\u6570\u8861\u91cf\uff09\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u901f\u5ea6\u4e0e\u7b26\u5408\u771f\u5b9e\u8054\u5408\u5206\u5e03\u4e4b\u95f4\u7684\u77db\u76fe\u3002"}}
{"id": "2509.22984", "pdf": "https://arxiv.org/pdf/2509.22984", "abs": "https://arxiv.org/abs/2509.22984", "authors": ["Yu Wu", "Shuo Wu", "Ye Tao", "Yansong Li", "Anand D. Sarwate"], "title": "Not only a helper, but also a teacher: Interactive LLM Cascade", "categories": ["cs.AI", "cs.CL"], "comment": "29 pages, 4 figures, under review", "summary": "Large Language Models (LLMs) vary widely in their capabilities, with larger\nmodels often having better performance but higher cost: choosing an LLM model\noften involves trading off performance and cost. The LLM Cascade is a paradigm\nthat defers difficult queries from weak/cheap to strong/expensive models. This\napproach is nonadaptive: the deferral decision is trained offline. When\nconfronted with similar or repeated queries, the LLM Cascade may then\nrepeatedly consult the expensive model and incur higher cost. To improve the\ncascading efficiency, we propose Inter-Cascade, an online and interactive LLM\nCascade that extends the role of strong model from a backup helper to a\nlong-term teacher. In our system, when a strong model resolves a difficult\nquery, it also distills its solution into a generalized, reusable\nproblem-solving strategy that boosts the weak model on subsequent queries.\nAdding strategies to queries enables the weak model to dynamically improve its\nperformance over time, avoiding computationally and time-intensive fine-tuning.\nEmpirically, compared with standard LLM Cascade baselines across multiple\nbenchmarks, the Inter-Cascade significantly improves the accuracy of the weak\nmodel (by up to 33.06 absolute percentage points) and the overall system (by up\nto 5.53 absolute percentage points), while reducing the calls to strong models\n(by up to 48.05% relative reduction) and saving the corresponding fees (by up\nto 49.63% relative reduction). Inter-Cascade demonstrates the effective\nin-context knowledge transfer between LLMs, and provides a general, scalable\nframework applicable to both open-source and API-based LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faInter-Cascade\uff0c\u4e00\u79cd\u5728\u7ebf\u4ea4\u4e92\u5f0fLLM\u7ea7\u8054\u7cfb\u7edf\uff0c\u901a\u8fc7\u8ba9\u5f3a\u6a21\u578b\u5c06\u89e3\u51b3\u65b9\u6848\u63d0\u70bc\u4e3a\u53ef\u91cd\u7528\u7b56\u7565\u6765\u6559\u5bfc\u5f31\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u9ad8\u6548\u7387\uff0c\u964d\u4f4e\u6210\u672c\uff0c\u5e76\u589e\u5f3a\u5f31\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u7ea7\u8054\u65b9\u6cd5\u662f\u975e\u81ea\u9002\u5e94\u7684\uff0c\u5bf9\u4e8e\u76f8\u4f3c\u6216\u91cd\u590d\u7684\u56f0\u96be\u67e5\u8be2\uff0c\u4f1a\u53cd\u590d\u8c03\u7528\u6602\u8d35\u7684\u5f3a\u6a21\u578b\uff0c\u5bfc\u81f4\u9ad8\u6210\u672c\uff0c\u672a\u80fd\u5145\u5206\u5e73\u8861\u6027\u80fd\u4e0e\u6210\u672c\u3002", "method": "Inter-Cascade\u5c06\u5f3a\u6a21\u578b\u89d2\u8272\u4ece\u5907\u7528\u52a9\u624b\u6269\u5c55\u4e3a\u957f\u671f\u6559\u5e08\u3002\u5f53\u5f3a\u6a21\u578b\u89e3\u51b3\u56f0\u96be\u67e5\u8be2\u65f6\uff0c\u5b83\u4f1a\u5c06\u5176\u89e3\u51b3\u65b9\u6848\u63d0\u70bc\u6210\u5e7f\u4e49\u7684\u3001\u53ef\u91cd\u7528\u7684\u95ee\u9898\u89e3\u51b3\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u5f31\u6a21\u578b\u5bf9\u540e\u7eed\u67e5\u8be2\u7684\u80fd\u529b\u3002\u8fd9\u4f7f\u5f31\u6a21\u578b\u80fd\u52a8\u6001\u63d0\u5347\u6027\u80fd\uff0c\u907f\u514d\u8017\u65f6\u7684\u5fae\u8c03\u3002", "result": "\u4e0e\u6807\u51c6LLM\u7ea7\u8054\u57fa\u7ebf\u76f8\u6bd4\uff0cInter-Cascade\u663e\u8457\u63d0\u9ad8\u4e86\u5f31\u6a21\u578b\u7cbe\u5ea6\uff08\u6700\u9ad833.06\u4e2a\u767e\u5206\u70b9\uff09\u548c\u7cfb\u7edf\u6574\u4f53\u7cbe\u5ea6\uff08\u6700\u9ad85.53\u4e2a\u767e\u5206\u70b9\uff09\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u5bf9\u5f3a\u6a21\u578b\u7684\u8c03\u7528\uff08\u6700\u9ad848.05%\uff09\u5e76\u8282\u7701\u4e86\u76f8\u5e94\u8d39\u7528\uff08\u6700\u9ad849.63%\uff09\u3002", "conclusion": "Inter-Cascade\u5c55\u793a\u4e86LLM\u4e4b\u95f4\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5f00\u6e90\u548c\u57fa\u4e8eAPI\u7684LLM\u3002"}}
{"id": "2509.22691", "pdf": "https://arxiv.org/pdf/2509.22691", "abs": "https://arxiv.org/abs/2509.22691", "authors": ["Yan Wen", "Peng Ye", "Lin Zhang", "Baopu Li", "Jiakang Yuan", "Yaoxin Yang", "Tao Chen"], "title": "Sequential Token Merging: Revisiting Hidden States", "categories": ["cs.CV"], "comment": null, "summary": "Vision Mambas (ViMs) achieve remarkable success with sub-quadratic\ncomplexity, but their efficiency remains constrained by quadratic token scaling\nwith image resolution. While existing methods address token redundancy, they\noverlook ViMs' intrinsic Limited Directional Sequential Dependence (LDSD) - a\ncritical information flow mechanism revealed in our analysis. We further\nidentify Mamba's selective scan enables gradual information aggregation in\nhidden states. Based on these insights, we propose Sequential Token Merging\n(STM), featuring: 1) Bidirectional nearest neighbor merging to preserve\nsequential dependencies through symmetric spatial aggregation, and 2) Hidden\nstates protection to stabilize the hidden states around the class token. STM\nstrategically leverages Mamba's layer-wise loss convergence to convert temporal\nforgetfulness into stability. Experiments demonstrate STM's superiority: 1.0%\naccuracy drop for ViM-Ti at 20% token reduction, and only 1.4% degradation for\nViM-S at 40% reduction. Our method achieves state-of-the-art efficiency with\nminimal complexity, while providing new insights into state-space model\ndynamics. Codes will be released soon.", "AI": {"tldr": "Vision Mambas (ViMs) \u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\u5b58\u5728token\u6269\u5c55\u6548\u7387\u95ee\u9898\u3002\u672c\u7814\u7a76\u63ed\u793a\u4e86ViMs\u7684\u201c\u6709\u9650\u5b9a\u5411\u5e8f\u5217\u4f9d\u8d56\u201d\uff08LDSD\uff09\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u5e8f\u5217Token\u5408\u5e76\uff08STM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5411\u8fd1\u90bb\u5408\u5e76\u548c\u9690\u85cf\u72b6\u6001\u4fdd\u62a4\uff0c\u6709\u6548\u51cf\u5c11token\u6570\u91cf\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "Vision Mambas (ViMs) \u5c3d\u7ba1\u5728\u6027\u80fd\u4e0a\u53d6\u5f97\u6210\u529f\u5e76\u5177\u6709\u4e9a\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u4f46\u5176\u6548\u7387\u4ecd\u53d7\u9650\u4e8e\u56fe\u50cf\u5206\u8fa8\u7387\u5e26\u6765\u7684\u4e8c\u6b21\u65b9token\u6269\u5c55\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406token\u5197\u4f59\u65f6\uff0c\u5ffd\u89c6\u4e86ViMs\u56fa\u6709\u7684\u201c\u6709\u9650\u5b9a\u5411\u5e8f\u5217\u4f9d\u8d56\u201d\uff08LDSD\uff09\u8fd9\u4e00\u5173\u952e\u4fe1\u606f\u6d41\u673a\u5236\u3002", "method": "\u57fa\u4e8e\u5bf9Mamba\u5185\u5728\u673a\u5236\uff08\u5982LDSD\u548c\u9009\u62e9\u6027\u626b\u63cf\u306b\u3088\u308b\u4fe1\u606f\u805a\u5408\uff09\u7684\u7406\u89e3\uff0c\u63d0\u51fa\u4e86\u5e8f\u5217Token\u5408\u5e76\uff08Sequential Token Merging, STM\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e24\u65b9\u9762\uff1a1) \u53cc\u5411\u6700\u8fd1\u90bb\u5408\u5e76\uff0c\u901a\u8fc7\u5bf9\u79f0\u7a7a\u95f4\u805a\u5408\u6765\u4fdd\u7559\u5e8f\u5217\u4f9d\u8d56\uff1b2) \u9690\u85cf\u72b6\u6001\u4fdd\u62a4\uff0c\u4ee5\u7a33\u5b9a\u7c7b\u522btoken\u5468\u56f4\u7684\u9690\u85cf\u72b6\u6001\u3002STM\u5de7\u5999\u5730\u5229\u7528Mamba\u7684\u5206\u5c42\u635f\u5931\u6536\u655b\uff0c\u5c06\u65f6\u95f4\u9057\u5fd8\u8f6c\u5316\u4e3a\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86STM\u7684\u4f18\u8d8a\u6027\uff1a\u5bf9\u4e8eViM-Ti\uff0c\u5728\u51cf\u5c1120% token\u7684\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u7387\u4ec5\u4e0b\u964d1.0%\uff1b\u5bf9\u4e8eViM-S\uff0c\u5728\u51cf\u5c1140% token\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4ec5\u4e0b\u964d1.4%\u3002\u8be5\u65b9\u6cd5\u4ee5\u6700\u5c0f\u7684\u590d\u6742\u5ea6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6548\u7387\u3002", "conclusion": "STM\u65b9\u6cd5\u5728\u63d0\u9ad8ViMs\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u4e3a\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6548\u7387\u4e0e\u6700\u5c0f\u590d\u6742\u5ea6\u3002"}}
{"id": "2509.22851", "pdf": "https://arxiv.org/pdf/2509.22851", "abs": "https://arxiv.org/abs/2509.22851", "authors": ["Yaswanth Chittepu", "Prasann Singhal", "Greg Durrett", "Scott Niekum"], "title": "Adaptive Margin RLHF via Preference over Preferences", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Margin-based optimization is fundamental to improving generalization and\nrobustness in classification tasks. In the context of reward model learning\nfrom preferences within Reinforcement Learning from Human Feedback (RLHF),\nexisting methods typically rely on no margins, fixed margins, or margins that\nare simplistic functions of preference ratings. However, such formulations\noften fail to account for the varying strengths of different preferences, for\nexample some preferences are associated with larger margins between responses,\nor they rely on noisy margin information derived from ratings. We argue that\nmodeling the strength of preferences can lead to better generalization and more\nfaithful alignment. Furthermore, many existing methods that use adaptive\nmargins assume access to accurate preference scores, which can be difficult for\nhumans to provide reliably. We propose an approach that leverages preferences\nover preferences, that is annotations indicating which of two preferences\nreflects a stronger distinction. We use this ordinal signal to infer adaptive\nmargins on a per-datapoint basis. We introduce an extension to Direct\nPreference Optimization (DPO), DPO-PoP, that incorporates adaptive margins from\npreference-over-preference supervision, enabling improved discriminative and\ngenerative performance. Empirically, our method outperforms vanilla DPO, DPO\nwith fixed margins, and DPO with ground-truth margins on the UltraFeedback\ndataset. Additionally, we show that there is a tradeoff between discriminative\nand generative performance: improving test classification accuracy,\nparticularly by correctly labeling weaker preferences at the expense of\nstronger ones, can lead to a decline in generative quality. To navigate this\ntradeoff, we propose two sampling strategies to gather\npreference-over-preference labels: one favoring discriminative performance and\none favoring generative performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDPO-PoP\u65b9\u6cd5\uff0c\u901a\u8fc7\u201c\u504f\u597d\u4e4b\u4e0a\u7684\u504f\u597d\u201d\u4fe1\u53f7\u63a8\u65ad\u81ea\u9002\u5e94\u8fb9\u8ddd\uff0c\u4ee5\u6539\u8fdbRLHF\u4e2d\u5956\u52b1\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u5bf9\u9f50\uff0c\u4f18\u4e8e\u4f20\u7edfDPO\u53ca\u5176\u53d8\u4f53\u3002", "motivation": "\u73b0\u6709RLHF\u65b9\u6cd5\u5728\u5904\u7406\u504f\u597d\u8fb9\u8ddd\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u65e0\u8fb9\u8ddd\u3001\u56fa\u5b9a\u8fb9\u8ddd\u6216\u57fa\u4e8e\u7b80\u5355\u51fd\u6570\u7684\u8fb9\u8ddd\uff0c\u672a\u80fd\u6709\u6548\u6355\u6349\u504f\u597d\u5f3a\u5ea6\u7684\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u8bb8\u591a\u81ea\u9002\u5e94\u8fb9\u8ddd\u65b9\u6cd5\u4f9d\u8d56\u96be\u4ee5\u53ef\u9760\u83b7\u53d6\u7684\u7cbe\u786e\u504f\u597d\u5206\u6570\u3002", "method": "\u63d0\u51fa\u5229\u7528\u201c\u504f\u597d\u4e4b\u4e0a\u7684\u504f\u597d\u201d\uff08\u5373\u6307\u793a\u54ea\u79cd\u504f\u597d\u66f4\u5f3a\u70c8\u7684\u6807\u6ce8\uff09\u8fd9\u4e00\u5e8f\u6570\u4fe1\u53f7\uff0c\u4e3a\u6bcf\u4e2a\u6570\u636e\u70b9\u63a8\u65ad\u81ea\u9002\u5e94\u8fb9\u8ddd\u3002\u5c06\u6b64\u65b9\u6cd5\u6574\u5408\u5230DPO\u4e2d\uff0c\u5f62\u6210DPO-PoP\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e24\u79cd\u91c7\u6837\u7b56\u7565\u4ee5\u6743\u8861\u533a\u5206\u6027\u548c\u751f\u6210\u6027\u8868\u73b0\u3002", "result": "DPO-PoP\u5728UltraFeedback\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u9999\u8349DPO\u3001\u56fa\u5b9a\u8fb9\u8dddDPO\u548c\u5e26\u6709\u771f\u503c\u8fb9\u8ddd\u7684DPO\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u533a\u5206\u6027\uff08\u5206\u7c7b\u51c6\u786e\u7387\uff09\u548c\u751f\u6210\u6027\u8868\u73b0\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u8fc7\u5ea6\u5173\u6ce8\u533a\u5206\u6027\u53ef\u80fd\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u3002", "conclusion": "\u901a\u8fc7\u201c\u504f\u597d\u4e4b\u4e0a\u7684\u504f\u597d\u201d\u5b9e\u73b0\u7684\u81ea\u9002\u5e94\u8fb9\u8ddd\u53ef\u4ee5\u663e\u8457\u63d0\u5347DPO\u7684\u5224\u522b\u548c\u751f\u6210\u6027\u80fd\u3002\u7136\u800c\uff0c\u5728\u4f18\u5316\u8fb9\u8ddd\u65f6\u9700\u8c28\u614e\u5904\u7406\u533a\u5206\u6027\u4e0e\u751f\u6210\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u63d0\u51fa\u7684\u91c7\u6837\u7b56\u7565\u6709\u52a9\u4e8e\u5728\u6b64\u6743\u8861\u4e2d\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2509.23528", "pdf": "https://arxiv.org/pdf/2509.23528", "abs": "https://arxiv.org/abs/2509.23528", "authors": ["Russell Ford", "Hao Chen", "Pranav Madadi", "Mandar Kulkarni", "Xiaochuan Ma", "Daoud Burghal", "Guanbo Chen", "Yeqing Hu", "Chance Tarver", "Panagiotis Skrimponis", "Vitali Loseu", "Yu Zhang", "Yan Xin", "Yang Li", "Jianzhong Zhang", "Shubham Khunteta", "Yeswanth Guddeti Reddy", "Ashok Kumar Reddy Chavva", "Mahantesh Kothiwale", "Davide Villa"], "title": "Sim2Field: End-to-End Development of AI RANs for 6G", "categories": ["cs.NI"], "comment": null, "summary": "Following state-of-the-art research results, which showed the potential for\nsignificant performance gains by applying AI/ML techniques in the cellular\nRadio Access Network (RAN), the wireless industry is now broadly pushing for\nthe adoption of AI in 5G and future 6G technology. Despite this enthusiasm,\nAI-based wireless systems still remain largely untested in the field. Common\nsimulation methods for generating datasets for AI model training suffer from\n\"reality gap\" and, as a result, the performance of these simulation-trained\nmodels may not carry over to practical cellular systems. Additionally, the cost\nand complexity of developing high-performance proof-of-concept implementations\npresent major hurdles for evaluating AI wireless systems in the field. In this\nwork, we introduce a methodology which aims to address the challenges of\nbringing AI to real networks. We discuss how detailed Digital Twin simulations\nmay be employed for training site-specific AI Physical (PHY) layer functions.\nWe further present a powerful testbed for AI-RAN research and demonstrate how\nit enables rapid prototyping, field testing and data collection. Finally, we\nevaluate an AI channel estimation algorithm over-the-air with a commercial UE,\ndemonstrating that real-world throughput gains of up to 40% are achievable by\nincorporating AI in the physical layer.", "AI": {"tldr": "\u4e3a\u89e3\u51b3AI\u65e0\u7ebf\u7cfb\u7edf\u5728\u73b0\u5b9e\u7f51\u7edc\u4e2d\u90e8\u7f72\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u548c\u5f3a\u5927\u6d4b\u8bd5\u5e73\u53f0\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u9a8c\u8bc1AI\u4fe1\u9053\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe40%\u7684\u771f\u5b9e\u4e16\u754c\u541e\u5410\u91cf\u589e\u76ca\u3002", "motivation": "\u5c3d\u7ba1AI/ML\u57285G/6G\u8702\u7a9d\u65e0\u7ebf\u63a5\u5165\u7f51(RAN)\u4e2d\u5c55\u73b0\u5de8\u5927\u6f5c\u529b\uff0c\u4f46AI\u65e0\u7ebf\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c1a\u672a\u5145\u5206\u9a8c\u8bc1\u3002\u73b0\u6709\u4eff\u771f\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u201c\u73b0\u5b9e\u5dee\u8ddd\u201d\uff0c\u4e14\u73b0\u573a\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u65bd\u6210\u672c\u9ad8\u3001\u590d\u6742\u6027\u5927\uff0c\u963b\u788d\u4e86AI\u65e0\u7ebf\u7cfb\u7edf\u7684\u5b9e\u5730\u8bc4\u4f30\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5c06AI\u5f15\u5165\u771f\u5b9e\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a1) \u8fd0\u7528\u8be6\u7ec6\u7684\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u8bad\u7ec3\u7279\u5b9a\u7ad9\u70b9\u7684AI\u7269\u7406\u5c42\u529f\u80fd\uff1b2) \u4ecb\u7ecd\u4e00\u4e2a\u5f3a\u5927\u7684AI-RAN\u7814\u7a76\u6d4b\u8bd5\u5e73\u53f0\uff0c\u652f\u6301\u5feb\u901f\u539f\u578b\u5f00\u53d1\u3001\u73b0\u573a\u6d4b\u8bd5\u548c\u6570\u636e\u6536\u96c6\u3002", "result": "\u901a\u8fc7\u5728\u7a7a\u4e2d\u63a5\u53e3\u4f7f\u7528\u5546\u7528UE\u8bc4\u4f30AI\u4fe1\u9053\u4f30\u8ba1\u7b97\u6cd5\uff0c\u7ed3\u679c\u663e\u793a\u5728\u7269\u7406\u5c42\u4e2d\u5f15\u5165AI\u53ef\u5b9e\u73b0\u9ad8\u8fbe40%\u7684\u771f\u5b9e\u4e16\u754c\u541e\u5410\u91cf\u589e\u76ca\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u548c\u5f3a\u5927\u6d4b\u8bd5\u5e73\u53f0\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9AI\u65e0\u7ebf\u7cfb\u7edf\u5728\u73b0\u5b9e\u7f51\u7edc\u4e2d\u7684\u90e8\u7f72\u6311\u6218\uff0c\u5e76\u8bc1\u660e\u4e86AI\u5728\u7269\u7406\u5c42\u4e2d\u53ef\u5e26\u6765\u663e\u8457\u7684\u5b9e\u9645\u541e\u5410\u91cf\u63d0\u5347\u3002"}}
{"id": "2509.22739", "pdf": "https://arxiv.org/pdf/2509.22739", "abs": "https://arxiv.org/abs/2509.22739", "authors": ["Sasha Cui", "Zhongren Chen"], "title": "Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML", "I.2.6; I.2.7"], "comment": null, "summary": "Language models (LMs) are typically post-trained for desired capabilities and\nbehaviors via weight-based or prompt-based steering, but the former is\ntime-consuming and expensive, and the latter is not precisely controllable and\noften requires manual trial-and-error. While activation steering (AS) promises\na cheap, fast, and controllable alternative to the two existing post-training\nmethods, current AS techniques require hand-crafted prompt pairs or\nlabor-intensive feature annotation, making them more inconvenient than the\nplug-and-play methods such as Reinforcement Learning (RL) and Supervised\nFine-Tuning (SFT). We introduce Painless Activation Steering (PAS), a family of\nfully automated methods that make AS readily usable with any given labeled\ndataset, with no need for prompt construction, feature labeling, or human\nintervention. We evaluate PAS on three open-weight models\n(Llama3.1-8B-Instruct, DeepSeek-R1-Distill-8B, and Nous-Hermes-2) and 18 tasks;\nwe find that PAS reliably improves performance for behavior tasks, but not for\nintelligence-oriented tasks. The introspective variant (iPAS) delivers the\nstrongest causal steering effects (10.1% on Bias, 5.2% on Morality, and 34.8%\non Alignment). We also show PAS delivers additional gains on top of In-Context\nLearning (ICL) and SFT. PAS constructs a fast, lightweight activation vector\nthat can be cheaply trained, easily stored, and activated at will. Our results\nprovide a characterization of where AS helps, where it fails, and how to deploy\nit as a practical, automated LM post-training option.", "AI": {"tldr": "\u63d0\u51faPainless Activation Steering (PAS)\uff0c\u4e00\u79cd\u5168\u81ea\u52a8\u7684\u6fc0\u6d3b\u8f6c\u5411\u65b9\u6cd5\uff0c\u53ef\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u9ad8\u6548\u8c03\u6574\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u3002PAS\u5728\u884c\u4e3a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u53ef\u4f5c\u4e3aLM\u540e\u8bad\u7ec3\u7684\u5b9e\u7528\u81ea\u52a8\u5316\u9009\u9879\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b(LM)\u540e\u8bad\u7ec3\u65b9\u6cd5\uff08\u57fa\u4e8e\u6743\u91cd\u6216\u63d0\u793a\uff09\u5b58\u5728\u8017\u65f6\u3001\u6602\u8d35\u6216\u63a7\u5236\u4e0d\u7cbe\u786e\u7b49\u95ee\u9898\u3002\u867d\u7136\u6fc0\u6d3b\u8f6c\u5411(AS)\u6709\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u6280\u672f\u9700\u8981\u624b\u52a8\u6784\u5efa\u63d0\u793a\u6216\u5927\u91cf\u7279\u5f81\u6807\u6ce8\uff0c\u4e0d\u591f\u5373\u63d2\u5373\u7528\u3002", "method": "\u5f15\u5165Painless Activation Steering (PAS)\u65b9\u6cd5\u5bb6\u65cf\uff0c\u5b83\u662f\u4e00\u79cd\u5168\u81ea\u52a8\u7684\u6fc0\u6d3b\u8f6c\u5411\u6280\u672f\uff0c\u53ef\u5229\u7528\u4efb\u4f55\u5e26\u6807\u7b7e\u6570\u636e\u96c6\uff0c\u65e0\u9700\u63d0\u793a\u6784\u5efa\u3001\u7279\u5f81\u6807\u6ce8\u6216\u4eba\u5de5\u5e72\u9884\u3002PAS\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u5feb\u901f\u3001\u8f7b\u91cf\u7ea7\u7684\u6fc0\u6d3b\u5411\u91cf\u6765\u8c03\u6574\u6a21\u578b\u884c\u4e3a\u3002", "result": "\u5728\u4e09\u79cd\u5f00\u6e90\u6a21\u578b\u548c18\u9879\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0cPAS\u80fd\u53ef\u9760\u63d0\u5347\u884c\u4e3a\u4efb\u52a1\u8868\u73b0\uff0c\u4f46\u5bf9\u667a\u80fd\u5bfc\u5411\u4efb\u52a1\u65e0\u6548\u3002\u5185\u7701\u53d8\u4f53(iPAS)\u5728\u504f\u89c1\u3001\u9053\u5fb7\u548c\u5bf9\u9f50\u4efb\u52a1\u4e0a\u5206\u522b\u5b9e\u73b010.1%\u30015.2%\u548c34.8%\u7684\u663e\u8457\u56e0\u679c\u8f6c\u5411\u6548\u679c\u3002PAS\u8fd8\u80fd\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u548c\u76d1\u7763\u5fae\u8c03(SFT)\u57fa\u7840\u4e0a\u63d0\u4f9b\u989d\u5916\u589e\u76ca\u3002", "conclusion": "\u672c\u7814\u7a76\u660e\u786e\u4e86\u6fc0\u6d3b\u8f6c\u5411\u7684\u9002\u7528\u8303\u56f4\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5c06PAS\u4f5c\u4e3a\u4e00\u79cd\u5b9e\u7528\u3001\u81ea\u52a8\u5316\u7684\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u65b9\u6848\u8fdb\u884c\u90e8\u7f72\u3002"}}
{"id": "2509.22989", "pdf": "https://arxiv.org/pdf/2509.22989", "abs": "https://arxiv.org/abs/2509.22989", "authors": ["Zirui Cheng", "Jiaxuan You"], "title": "Towards Strategic Persuasion with Language Models", "categories": ["cs.AI", "cs.CY", "cs.GT"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong persuasive capabilities\ncomparable to those of humans, offering promising benefits while raising\nsocietal concerns about their deployment. However, systematically evaluating\nthe persuasive capabilities of LLMs is inherently challenging, as the\neffectiveness of persuasion among humans varies significantly across different\ndomains. In this paper, we take a theory-driven approach to provide a scalable\nand principled framework for measuring the persuasive capabilities of LLMs.\nGrounded in the Bayesian Persuasion (BP) framework, we repurpose existing\nhuman-human persuasion datasets to construct environments for evaluating and\ntraining LLMs in strategic persuasion. Our results reveal that frontier models\ncan consistently achieve high persuasion gains and exhibit sophisticated\npersuasion strategies that align with theoretical predictions. Building on\nthis, we use reinforcement learning to train LLMs for strategic persuasion in\nour environments. Our results also demonstrate that even small LLMs can obtain\nsignificantly higher persuasion gains through reinforcement learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u529d\u8bf4\u6846\u67b6\u7684\u8bc4\u4f30\u548c\u8bad\u7ec3LLM\u529d\u8bf4\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u524d\u6cbf\u6a21\u578b\u8868\u73b0\u51fa\u8272\uff0c\u5c0f\u6a21\u578b\u7ecf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u540e\u529d\u8bf4\u6536\u76ca\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u529d\u8bf4\u80fd\u529b\uff0c\u4f46\u7cfb\u7edf\u6027\u8bc4\u4f30\u5176\u6709\u6548\u6027\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u529d\u8bf4\u6548\u679c\u5728\u4e0d\u540c\u9886\u57df\u5dee\u5f02\u5f88\u5927\u3002", "method": "1. \u91c7\u7528\u7406\u8bba\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8d1d\u53f6\u65af\u529d\u8bf4\uff08BP\uff09\u6846\u67b6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u539f\u5219\u7684LLM\u529d\u8bf4\u80fd\u529b\u8861\u91cf\u6846\u67b6\u30022. \u6539\u9020\u73b0\u6709\u7684\u4eba-\u4eba\u529d\u8bf4\u6570\u636e\u96c6\uff0c\u4ee5\u521b\u5efa\u7528\u4e8e\u8bc4\u4f30\u548c\u8bad\u7ec3LLMs\u7b56\u7565\u6027\u529d\u8bf4\u7684\u73af\u5883\u30023. \u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3LLMs\u8fdb\u884c\u7b56\u7565\u6027\u529d\u8bf4\u3002", "result": "1. \u524d\u6cbfLLMs\u80fd\u6301\u7eed\u83b7\u5f97\u9ad8\u529d\u8bf4\u6536\u76ca\uff0c\u5e76\u8868\u73b0\u51fa\u4e0e\u7406\u8bba\u9884\u6d4b\u4e00\u81f4\u7684\u590d\u6742\u529d\u8bf4\u7b56\u7565\u30022. \u5373\u4f7f\u662f\u5c0f\u578bLLMs\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4e5f\u80fd\u663e\u8457\u63d0\u9ad8\u529d\u8bf4\u6536\u76ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8861\u91cfLLM\u529d\u8bf4\u80fd\u529b\u7684\u65b0\u6846\u67b6\uff0c\u5e76\u8bc1\u660e\u4e86\u524d\u6cbf\u6a21\u578b\u548c\u7ecf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u5c0f\u6a21\u578b\u5728\u7b56\u7565\u6027\u529d\u8bf4\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5176\u7b56\u7565\u4e0e\u7406\u8bba\u9884\u6d4b\u76f8\u7b26\u3002"}}
{"id": "2509.22692", "pdf": "https://arxiv.org/pdf/2509.22692", "abs": "https://arxiv.org/abs/2509.22692", "authors": ["Le Zhang", "Ao Li", "Qibin Hou", "Ce Zhu", "Yonina C. Eldar"], "title": "Deep Learning Empowered Super-Resolution: A Comprehensive Survey and Future Prospects", "categories": ["cs.CV"], "comment": "Accepted by Proceedings of the IEEE", "summary": "Super-resolution (SR) has garnered significant attention within the computer\nvision community, driven by advances in deep learning (DL) techniques and the\ngrowing demand for high-quality visual applications. With the expansion of this\nfield, numerous surveys have emerged. Most existing surveys focus on specific\ndomains, lacking a comprehensive overview of this field. Here, we present an\nin-depth review of diverse SR methods, encompassing single image\nsuper-resolution (SISR), video super-resolution (VSR), stereo super-resolution\n(SSR), and light field super-resolution (LFSR). We extensively cover over 150\nSISR methods, nearly 70 VSR approaches, and approximately 30 techniques for SSR\nand LFSR. We analyze methodologies, datasets, evaluation protocols, empirical\nresults, and complexity. In addition, we conducted a taxonomy based on each\nbackbone structure according to the diverse purposes. We also explore valuable\nyet under-studied open issues in the field. We believe that this work will\nserve as a valuable resource and offer guidance to researchers in this domain.\nTo facilitate access to related work, we created a dedicated repository\navailable at https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review.", "AI": {"tldr": "\u672c\u6587\u5bf9\u8d85\u5206\u8fa8\u7387(SR)\u9886\u57df\u7684\u5404\u79cd\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6df1\u5ea6\u7efc\u8ff0\uff0c\u6db5\u76d6\u4e86SISR\u3001VSR\u3001SSR\u548cLFSR\uff0c\u5206\u6790\u4e86\u8d85\u8fc7250\u79cd\u6280\u672f\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u548c\u5f00\u653e\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d44\u6e90\u5e93\u3002", "motivation": "\u8d85\u5206\u8fa8\u7387(SR)\u6280\u672f\u56e0\u6df1\u5ea6\u5b66\u4e60\u7684\u8fdb\u6b65\u548c\u9ad8\u8d28\u91cf\u89c6\u89c9\u5e94\u7528\u7684\u9700\u6c42\u800c\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u73b0\u6709\u7efc\u8ff0\u591a\u805a\u7126\u4e8e\u7279\u5b9a\u9886\u57df\uff0c\u7f3a\u4e4f\u5bf9\u6574\u4e2aSR\u9886\u57df\u7684\u5168\u9762\u548c\u6df1\u5165\u6982\u8ff0\u3002", "method": "\u672c\u6587\u5bf9\u5355\u5e45\u56fe\u50cf\u8d85\u5206\u8fa8\u7387(SISR)\u3001\u89c6\u9891\u8d85\u5206\u8fa8\u7387(VSR)\u3001\u7acb\u4f53\u8d85\u5206\u8fa8\u7387(SSR)\u548c\u5149\u573a\u8d85\u5206\u8fa8\u7387(LFSR)\u7b49\u591a\u79cdSR\u65b9\u6cd5\u8fdb\u884c\u4e86\u6df1\u5165\u56de\u987e\u3002\u5177\u4f53\u6db5\u76d6\u4e86\u8d85\u8fc7150\u79cdSISR\u65b9\u6cd5\u3001\u8fd170\u79cdVSR\u65b9\u6cd5\u4ee5\u53ca\u7ea630\u79cdSSR\u548cLFSR\u6280\u672f\u3002\u5206\u6790\u5185\u5bb9\u5305\u62ec\u65b9\u6cd5\u8bba\u3001\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u534f\u8bae\u3001\u5b9e\u8bc1\u7ed3\u679c\u548c\u590d\u6742\u5ea6\u3002\u6b64\u5916\uff0c\u8fd8\u6839\u636e\u9aa8\u5e72\u7ed3\u6784\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u63a2\u8ba8\u4e86\u8be5\u9886\u57df\u4e2d\u5c1a\u672a\u5145\u5206\u7814\u7a76\u7684\u5f00\u653e\u95ee\u9898\u3002", "result": "\u672c\u6587\u5b8c\u6210\u4e86\u4e00\u9879\u5168\u9762\u7684SR\u65b9\u6cd5\u7efc\u8ff0\uff0c\u8986\u76d6\u4e86SISR\u3001VSR\u3001SSR\u548cLFSR\u7b49\u591a\u4e2a\u5b50\u9886\u57df\uff0c\u5171\u5206\u6790\u4e86\u8d85\u8fc7250\u79cd\u65b9\u6cd5\u53ca\u5176\u76f8\u5173\u65b9\u9762\u3002\u521b\u5efa\u4e86\u4e00\u4e2a\u4e13\u7528\u4ee3\u7801\u5e93\uff08https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review\uff09\u4ee5\u65b9\u4fbf\u7814\u7a76\u4eba\u5458\u67e5\u9605\u76f8\u5173\u5de5\u4f5c\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aSR\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u548c\u6307\u5bfc\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u7efc\u8ff0\u5728\u5168\u9762\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u4e3a\u9886\u57df\u5185\u7684\u7814\u7a76\u548c\u53d1\u5c55\u63d0\u4f9b\u4e86\u7efc\u5408\u6027\u7684\u89c6\u89d2\u3002"}}
{"id": "2509.22855", "pdf": "https://arxiv.org/pdf/2509.22855", "abs": "https://arxiv.org/abs/2509.22855", "authors": ["Sameep Chattopadhyay", "Nikhil Karamchandani", "Sharayu Mohair"], "title": "Observation-Free Attacks on Online Learning to Rank", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Online learning to rank (OLTR) plays a critical role in information retrieval\nand machine learning systems, with a wide range of applications in search\nengines and content recommenders. However, despite their extensive adoption,\nthe susceptibility of OLTR algorithms to coordinated adversarial attacks\nremains poorly understood. In this work, we present a novel framework for\nattacking some of the widely used OLTR algorithms. Our framework is designed to\npromote a set of target items so that they appear in the list of top-K\nrecommendations for T - o(T) rounds, while simultaneously inducing linear\nregret in the learning algorithm. We propose two novel attack strategies:\nCascadeOFA for CascadeUCB1 and PBMOFA for PBM-UCB . We provide theoretical\nguarantees showing that both strategies require only O(log T) manipulations to\nsucceed. Additionally, we supplement our theoretical analysis with empirical\nresults on real-world data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\u53ca\u4e24\u79cd\u7b56\u7565\uff0c\u7528\u4e8e\u653b\u51fb\u5728\u7ebf\u6392\u5e8f\u5b66\u4e60\uff08OLTR\uff09\u7b97\u6cd5\uff0c\u80fd\u5728O(log T)\u64cd\u4f5c\u4e0b\u6210\u529f\u63a8\u5e7f\u76ee\u6807\u9879\u76ee\u5e76\u5f15\u53d1\u7ebf\u6027\u9057\u61be\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5728\u7ebf\u6392\u5e8f\u5b66\u4e60\uff08OLTR\uff09\u7b97\u6cd5\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u5bf9\u534f\u540c\u5bf9\u6297\u6027\u653b\u51fb\u7684\u8106\u5f31\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7406\u89e3\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u5e38\u7528OLTR\u7b97\u6cd5\u7684\u65b0\u578b\u653b\u51fb\u6846\u67b6\uff0c\u65e8\u5728\u4f7f\u76ee\u6807\u9879\u76ee\u5728T - o(T)\u8f6e\u6b21\u4e2d\u51fa\u73b0\u5728\u524dK\u63a8\u8350\u5217\u8868\uff0c\u5e76\u540c\u65f6\u8bf1\u5bfc\u5b66\u4e60\u7b97\u6cd5\u4ea7\u751f\u7ebf\u6027\u9057\u61be\u3002\n2. \u5177\u4f53\u63d0\u51fa\u4e86\u4e24\u79cd\u653b\u51fb\u7b56\u7565\uff1aCascadeOFA\uff08\u9488\u5bf9CascadeUCB1\uff09\u548cPBMOFA\uff08\u9488\u5bf9PBM-UCB\uff09\u3002", "result": "1. \u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u8868\u660e\u4e24\u79cd\u653b\u51fb\u7b56\u7565\u90fd\u4ec5\u9700O(log T)\u6b21\u64cd\u4f5c\u5373\u53ef\u6210\u529f\u3002\n2. \u901a\u8fc7\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8865\u5145\u4e86\u7406\u8bba\u5206\u6790\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u8bbe\u8ba1\u5e76\u7406\u8bba\u9a8c\u8bc1\u4e86\u9488\u5bf9\u5728\u7ebf\u6392\u5e8f\u5b66\u4e60\u7b97\u6cd5\u7684\u6709\u6548\u653b\u51fb\u6846\u67b6\u548c\u7b56\u7565\uff0c\u63ed\u793a\u4e86\u5176\u5728\u5bf9\u6297\u6027\u653b\u51fb\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u5e76\u5f97\u5230\u4e86\u5b9e\u8bc1\u6570\u636e\u7684\u652f\u6301\u3002"}}
{"id": "2509.23794", "pdf": "https://arxiv.org/pdf/2509.23794", "abs": "https://arxiv.org/abs/2509.23794", "authors": ["Zhouyu Qu", "Andreas Willig", "Xiaobing Wu"], "title": "Short-Term Guidance Algorithm on a Drone Road System", "categories": ["cs.NI"], "comment": "45 pages, 11 figures", "summary": "Unmanned Aerial Vehicles (UAVs), commonly known as drones, have experienced\nexpanding use in urban environments in recent years. However, the growing\ndensity of drones raises significant challenges, such as avoiding collisions\nand managing air traffic efficiently, especially in congested areas. To address\nthese issues, a structured road system and an effective guidance algorithm are\nessential. In this paper, we introduce a markup language allowing to describe\ndrone road systems (DRS), in which a road system is given by a set of\nindividual roads, each of which can have a varying number of lanes. Roads can\nbe linked through connecting lanes. Furthermore, we propose a novel short-term\ndecentralized greedy (STDG) guidance algorithm that uses only the position and\nspeed information of nearby drones -- communicated via periodically transmitted\nbeacons -- to make real-time decisions such as stopping, changing lanes, or\nadjusting speed for the next few seconds. Unlike existing methods that rely on\ncentralized coordination, our algorithm enables drones to operate independently\nwhile ensuring safety and efficiency. We present simulation results showing the\nimpact of key wireless and algorithm parameters on performance metrics like the\ndrone collision rate, average speed and throughput of the drone road system.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4eba\u673a\u9053\u8def\u7cfb\u7edf\uff08DRS\uff09\u7684\u6807\u8bb0\u8bed\u8a00\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u77ed\u65f6\u53bb\u4e2d\u5fc3\u5316\u8d2a\u5a6a\uff08STDG\uff09\u5f15\u5bfc\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u4fe1\u606f\u5b9e\u73b0\u65e0\u4eba\u673a\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u5b89\u5168\u9ad8\u6548\u4ea4\u901a\u7ba1\u7406\u3002", "motivation": "\u57ce\u5e02\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u4f7f\u7528\u65e5\u76ca\u589e\u591a\uff0c\u5bfc\u81f4\u78b0\u649e\u548c\u4ea4\u901a\u7ba1\u7406\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u62e5\u5835\u533a\u57df\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7ed3\u6784\u5316\u7684\u9053\u8def\u7cfb\u7edf\u548c\u6709\u6548\u7684\u5f15\u5bfc\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u6807\u8bb0\u8bed\u8a00\u6765\u63cf\u8ff0\u65e0\u4eba\u673a\u9053\u8def\u7cfb\u7edf\uff08DRS\uff09\uff0c\u5305\u62ec\u591a\u8f66\u9053\u9053\u8def\u548c\u8fde\u63a5\u8f66\u9053\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u77ed\u65f6\u53bb\u4e2d\u5fc3\u5316\u8d2a\u5a6a\uff08STDG\uff09\u5f15\u5bfc\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4ec5\u5229\u7528\u9644\u8fd1\u65e0\u4eba\u673a\u7684\u4f4d\u7f6e\u548c\u901f\u5ea6\u4fe1\u606f\uff08\u901a\u8fc7\u4fe1\u6807\u901a\u4fe1\uff09\uff0c\u5b9e\u65f6\u505a\u51fa\u505c\u8f66\u3001\u53d8\u9053\u6216\u8c03\u6574\u901f\u5ea6\u7b49\u51b3\u7b56\uff0c\u4ece\u800c\u5b9e\u73b0\u72ec\u7acb\u8fd0\u884c\u800c\u975e\u4f9d\u8d56\u96c6\u4e2d\u534f\u8c03\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u5c55\u793a\u4e86\u5173\u952e\u65e0\u7ebf\u548c\u7b97\u6cd5\u53c2\u6570\u5bf9\u65e0\u4eba\u673a\u78b0\u649e\u7387\u3001\u5e73\u5747\u901f\u5ea6\u548cDRS\u541e\u5410\u91cf\u7b49\u6027\u80fd\u6307\u6807\u7684\u5f71\u54cd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684DRS\u6807\u8bb0\u8bed\u8a00\u548c\u53bb\u4e2d\u5fc3\u5316STDG\u5f15\u5bfc\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u7ba1\u7406\u57ce\u5e02\u65e0\u4eba\u673a\u4ea4\u901a\uff0c\u786e\u4fdd\u5b89\u5168\u548c\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2509.22750", "pdf": "https://arxiv.org/pdf/2509.22750", "abs": "https://arxiv.org/abs/2509.22750", "authors": ["Jeonghyun Park", "Ingeol Baek", "Seunghyun Yoon", "Haeun Jang", "Aparna Garimella", "Akriti Jain", "Nedim Lipka", "Hwanhee Lee"], "title": "MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions", "categories": ["cs.CL", "cs.AI"], "comment": "18 figures, 11 tables", "summary": "Real-world Multi-hop Question Answering (QA) often involves ambiguity that is\ninseparable from the reasoning process itself. This ambiguity creates a\ndistinct challenge, where multiple reasoning paths emerge from a single\nquestion, each requiring independent resolution. Since each sub-question is\nambiguous, the model must resolve ambiguity at every step. Thus, answering a\nsingle question requires handling multiple layers of ambiguity throughout the\nreasoning chain. We find that current Large Language Models (LLMs) struggle in\nthis setting, typically exploring wrong reasoning paths and producing\nincomplete answers. To facilitate research on multi-hop ambiguity, we introduce\nMultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions (MIRAGE),\na benchmark designed to analyze and evaluate this challenging intersection of\nambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142\nhigh-quality examples of ambiguous multi-hop questions, categorized under a\ntaxonomy of syntactic, general, and semantic ambiguity, and curated through a\nrigorous multi-LLM verification pipeline. Our experiments reveal that even\nstate-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity\ncombined with multi-step inference is a distinct and significant challenge. To\nestablish a robust baseline, we propose CLarifying Ambiguity with a Reasoning\nand InstructiON (CLARION), a multi-agent framework that significantly\noutperforms existing approaches on MIRAGE, paving the way for more adaptive and\nrobust reasoning systems.", "AI": {"tldr": "\u9488\u5bf9\u73b0\u5b9e\u4e16\u754c\u591a\u8df3\u95ee\u7b54\u4e2d\u56fa\u6709\u7684\u591a\u5c42\u6b67\u4e49\u6311\u6218\uff0c\u672c\u6587\u5f15\u5165\u4e86MIRAGE\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u63d0\u51fa\u4e86CLARION\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u89e3\u51b3\u6b64\u7c7b\u590d\u6742\u6b67\u4e49\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u591a\u8df3\u95ee\u7b54\uff08QA\uff09\u5e38\u6d89\u53ca\u591a\u5c42\u6b67\u4e49\uff0c\u5bfc\u81f4\u5355\u4e00\u95ee\u9898\u53ef\u80fd\u4ea7\u751f\u591a\u6761\u63a8\u7406\u8def\u5f84\uff0c\u4e14\u6bcf\u4e2a\u5b50\u95ee\u9898\u90fd\u9700\u72ec\u7acb\u89e3\u51b3\u6b67\u4e49\u3002\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8fd9\u79cd\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5bb9\u6613\u63a2\u7d22\u9519\u8bef\u7684\u63a8\u7406\u8def\u5f84\u5e76\u4ea7\u751f\u4e0d\u5b8c\u6574\u7684\u7b54\u6848\u3002", "method": "1. \u5f15\u5165\u4e86MIRAGE\uff08MultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions\uff09\u57fa\u51c6\uff0c\u65e8\u5728\u5206\u6790\u548c\u8bc4\u4f30\u6b67\u4e49\u89e3\u91ca\u4e0e\u591a\u8df3\u63a8\u7406\u7684\u4ea4\u53c9\u6311\u6218\u3002MIRAGE\u5305\u542b1,142\u4e2a\u9ad8\u8d28\u91cf\u7684\u6b67\u4e49\u591a\u8df3\u95ee\u9898\u793a\u4f8b\uff0c\u6309\u53e5\u6cd5\u3001\u4e00\u822c\u548c\u8bed\u4e49\u6b67\u4e49\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u4e25\u683c\u7684\u591aLLM\u9a8c\u8bc1\u7ba1\u9053\u8fdb\u884c\u7b56\u5212\u30022. \u63d0\u51fa\u4e86CLARION\uff08CLarifying Ambiguity with a Reasoning and InstructiON\uff09\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4ee5\u5efa\u7acb\u4e00\u4e2a\u9c81\u68d2\u7684\u57fa\u7ebf\u3002", "result": "1. \u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728MIRAGE\u57fa\u51c6\u4e0a\u4e5f\u8868\u73b0\u4e0d\u4f73\uff0c\u8bc1\u5b9e\u4e86\u7ed3\u5408\u6b67\u4e49\u89e3\u51b3\u7684\u591a\u6b65\u63a8\u7406\u662f\u4e00\u4e2a\u72ec\u7279\u4e14\u91cd\u5927\u7684\u6311\u6218\u30022. \u63d0\u51fa\u7684CLARION\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5728MIRAGE\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u89e3\u51b3\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u591a\u5c42\u6b67\u4e49\u662f\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u7684\u663e\u8457\u6311\u6218\u3002MIRAGE\u57fa\u51c6\u7684\u5efa\u7acb\u548cCLARION\u6846\u67b6\u7684\u63d0\u51fa\uff0c\u4e3a\u5f00\u53d1\u66f4\u5177\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u7684\u63a8\u7406\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2509.23004", "pdf": "https://arxiv.org/pdf/2509.23004", "abs": "https://arxiv.org/abs/2509.23004", "authors": ["Karan Srivastava", "Sanjeeb Dash", "Ryan Cory-Wright", "Barry Trager", "Lior Horesh"], "title": "AI Noether -- Bridging the Gap Between Scientific Laws Derived by AI Systems and Canonical Knowledge via Abductive Inference", "categories": ["cs.AI", "cs.SC", "math.AG"], "comment": "22 Pages (13+appendix), 6 Figures, Preprint", "summary": "A core goal in modern science is to harness recent advances in AI and\ncomputer processing to automate and accelerate the scientific method. Symbolic\nregression can fit interpretable models to data, but these models often sit\noutside established theory. Recent systems (e.g., AI Descartes, AI Hilbert)\nenforce derivability from prior axioms. However, sometimes new data and\nassociated hypotheses derived from data are not consistent with existing theory\nbecause the existing theory is incomplete or incorrect. Automating abductive\ninference to close this gap remains open. We propose a solution: an algebraic\ngeometry-based system that, given an incomplete axiom system and a hypothesis\nthat it cannot explain, automatically generates a minimal set of missing axioms\nthat suffices to derive the axiom, as long as axioms and hypotheses are\nexpressible as polynomial equations. We formally establish necessary and\nsufficient conditions for the successful retrieval of such axioms. We\nillustrate the efficacy of our approach by demonstrating its ability to explain\nKepler's third law and a few other laws, even when key axioms are absent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4ee3\u6570\u51e0\u4f55\u7684\u7cfb\u7edf\uff0c\u80fd\u81ea\u52a8\u751f\u6210\u6700\u5c0f\u7f3a\u5931\u591a\u9879\u5f0f\u516c\u7406\uff0c\u4ee5\u89e3\u91ca\u65e0\u6cd5\u7531\u73b0\u6709\u4e0d\u5b8c\u6574\u7406\u8bba\u63a8\u5bfc\u7684\u5047\u8bbe\uff0c\u4ece\u800c\u81ea\u52a8\u5316\u79d1\u5b66\u6eaf\u56e0\u63a8\u7406\u3002", "motivation": "\u73b0\u6709AI\u7cfb\u7edf\u867d\u80fd\u4ece\u516c\u7406\u63a8\u5bfc\u5047\u8bbe\uff0c\u4f46\u65e0\u6cd5\u5904\u7406\u65b0\u5047\u8bbe\u4e0e\u4e0d\u5b8c\u6574\u6216\u9519\u8bef\u7406\u8bba\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u6eaf\u56e0\u63a8\u7406\uff0c\u53d1\u73b0\u7f3a\u5931\u7684\u516c\u7406\u6765\u5f25\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u6570\u51e0\u4f55\u7684\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u63a5\u6536\u4e0d\u5b8c\u6574\u7684\u516c\u7406\u7cfb\u7edf\u548c\u4e00\u4e2a\u65e0\u6cd5\u89e3\u91ca\u7684\u5047\u8bbe\uff08\u4e24\u8005\u5747\u8868\u793a\u4e3a\u591a\u9879\u5f0f\u65b9\u7a0b\uff09\uff0c\u5e76\u81ea\u52a8\u751f\u6210\u63a8\u5bfc\u8be5\u5047\u8bbe\u6240\u9700\u7684\u6700\u5c0f\u7f3a\u5931\u516c\u7406\u96c6\u3002\u8bba\u6587\u8fd8\u5f62\u5f0f\u5316\u5730\u5efa\u7acb\u4e86\u6210\u529f\u68c0\u7d22\u8fd9\u4e9b\u516c\u7406\u7684\u5fc5\u8981\u548c\u5145\u5206\u6761\u4ef6\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6210\u529f\u89e3\u91ca\u5f00\u666e\u52d2\u7b2c\u4e09\u5b9a\u5f8b\u548c\u5176\u4ed6\u4e00\u4e9b\u5b9a\u5f8b\uff0c\u5373\u4f7f\u5728\u5173\u952e\u516c\u7406\u7f3a\u5931\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\uff0c\u8bc1\u660e\u4e86\u5176\u81ea\u52a8\u751f\u6210\u7f3a\u5931\u516c\u7406\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u81ea\u52a8\u5316\u79d1\u5b66\u6eaf\u56e0\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u6761\u65b0\u9014\u5f84\uff0c\u901a\u8fc7\u8bc6\u522b\u7f3a\u5931\u7684\u591a\u9879\u5f0f\u516c\u7406\u6765\u5b8c\u5584\u4e0d\u5b8c\u6574\u7684\u7406\u8bba\uff0c\u524d\u63d0\u662f\u76f8\u5173\u516c\u7406\u548c\u5047\u8bbe\u80fd\u8868\u8fbe\u4e3a\u591a\u9879\u5f0f\u65b9\u7a0b\u3002"}}
{"id": "2509.22697", "pdf": "https://arxiv.org/pdf/2509.22697", "abs": "https://arxiv.org/abs/2509.22697", "authors": ["Abhiroop Chatterjee", "Susmita Ghosh"], "title": "Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted at the IEEE/CVF International Conference on Computer Vision\n  (ICCV 2025), Workshop on Curated Data for Efficient Learning", "summary": "As data requirements continue to grow, efficient learning increasingly\ndepends on the curation and distillation of high-value data rather than\nbrute-force scaling of model sizes. In the case of a hyperspectral image (HSI),\nthe challenge is amplified by the high-dimensional 3D voxel structure, where\neach spatial location is associated with hundreds of contiguous spectral\nchannels. While vision and language models have been optimized effectively for\nnatural image or text tasks, their cross-modal alignment in the hyperspectral\ndomain remains an open and underexplored problem. In this article, we make an\nattempt to optimize a Vision-Language Model (VLM) for hyperspectral scene\nunderstanding by exploiting a CLIP-style contrastive training framework. Our\nframework maps voxel-level embeddings from a vision backbone onto the latent\nspace of a frozen large embedding model (LEM), where a trainable probe aligns\nvision features with the model's textual token representations. The two\nmodalities are aligned via a contrastive loss restricted to a curated set of\nhard (closest wrong classes) and semi-hard (random distractors) negatives,\nalong with positive pairs. To further enhance alignment, descriptive prompts\nthat encode class semantics are introduced and act as structured anchors for\nthe HSI embeddings. It is seen that the proposed method updates only 0.07\npercent of the total parameters, yet yields state-of-the-art performance. For\nexample, on Indian Pines (IP) the model produces better results over unimodal\nand multimodal baselines by +0.92 Overall Accuracy (OA) and +1.60 Kappa\n($\\kappa$), while on Pavia University (PU) data it provides gains of +0.69 OA\nand +0.90 $\\kappa$. Moreover, this is achieved with the set of parameters,\nnearly 50$\\times$ smaller than DCTN and 90$\\times$ smaller than SS-TMNet.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.22868", "pdf": "https://arxiv.org/pdf/2509.22868", "abs": "https://arxiv.org/abs/2509.22868", "authors": ["Zehao Niu", "Mihai Anitescu", "Jie Chen"], "title": "Neighborhood Sampling Does Not Learn the Same Graph Neural Network", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Neighborhood sampling is an important ingredient in the training of\nlarge-scale graph neural networks. It suppresses the exponential growth of the\nneighborhood size across network layers and maintains feasible memory\nconsumption and time costs. While it becomes a standard implementation in\npractice, its systemic behaviors are less understood. We conduct a theoretical\nanalysis by using the tool of neural tangent kernels, which characterize the\n(analogous) training dynamics of neural networks based on their infinitely wide\ncounterparts -- Gaussian processes (GPs). We study several established\nneighborhood sampling approaches and the corresponding posterior GP. With\nlimited samples, the posteriors are all different, although they converge to\nthe same one as the sample size increases. Moreover, the posterior covariance,\nwhich lower-bounds the mean squared prediction error, is uncomparable, aligning\nwith observations that no sampling approach dominates.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5927\u89c4\u6a21\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u90bb\u57df\u91c7\u6837\u65b9\u6cd5\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u53d1\u73b0\u4e0d\u540c\u91c7\u6837\u65b9\u6cd5\u5728\u6709\u9650\u6837\u672c\u4e0b\u884c\u4e3a\u5404\u5f02\u4f46\u6700\u7ec8\u6536\u655b\uff0c\u4e14\u5176\u9884\u6d4b\u8bef\u5dee\u4e0b\u9650\u4e0d\u53ef\u6bd4\uff0c\u8868\u660e\u6ca1\u6709\u5355\u4e00\u65b9\u6cd5\u5360\u636e\u4e3b\u5bfc\u3002", "motivation": "\u90bb\u57df\u91c7\u6837\u662f\u8bad\u7ec3\u5927\u89c4\u6a21\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5173\u952e\u6280\u672f\uff0c\u80fd\u6709\u6548\u63a7\u5236\u5185\u5b58\u548c\u65f6\u95f4\u6210\u672c\uff0c\u4f46\u5176\u7cfb\u7edf\u884c\u4e3a\u548c\u7406\u8bba\u7279\u6027\u4ecd\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u5207\u7ebf\u6838\uff08NTK\uff09\u5de5\u5177\uff0c\u901a\u8fc7\u5206\u6790\u5176\u65e0\u9650\u5bbd\u5bf9\u5e94\u7269\u2014\u2014\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\uff0c\u5bf9\u51e0\u79cd\u6210\u719f\u7684\u90bb\u57df\u91c7\u6837\u65b9\u6cd5\u53ca\u5176\u76f8\u5e94\u7684\u540e\u9a8cGP\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\u3002", "result": "\u5728\u6709\u9650\u6837\u672c\u6761\u4ef6\u4e0b\uff0c\u4e0d\u540c\u91c7\u6837\u65b9\u6cd5\u7684\u540e\u9a8cGP\u5404\u4e0d\u76f8\u540c\uff0c\u4f46\u968f\u7740\u6837\u672c\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5b83\u4eec\u4f1a\u6536\u655b\u5230\u540c\u4e00\u4e2a\u540e\u9a8c\u3002\u6b64\u5916\uff0c\u4f5c\u4e3a\u5747\u65b9\u9884\u6d4b\u8bef\u5dee\u4e0b\u9650\u7684\u540e\u9a8c\u534f\u65b9\u5dee\u662f\u4e0d\u53ef\u6bd4\u8f83\u7684\u3002", "conclusion": "\u540e\u9a8c\u534f\u65b9\u5dee\u7684\u4e0d\u53ef\u6bd4\u8f83\u6027\u4e0e\u5b9e\u8df5\u4e2d\u89c2\u5bdf\u5230\u7684\u201c\u6ca1\u6709\u5355\u4e00\u91c7\u6837\u65b9\u6cd5\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u201d\u7684\u73b0\u8c61\u76f8\u7b26\uff0c\u6697\u793a\u4e0d\u540c\u91c7\u6837\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u5404\u6709\u4f18\u52a3\uff0c\u6ca1\u6709\u666e\u9002\u7684\u6700\u4f73\u9009\u62e9\u3002"}}
{"id": "2509.23810", "pdf": "https://arxiv.org/pdf/2509.23810", "abs": "https://arxiv.org/abs/2509.23810", "authors": ["Yan Sun", "Yinqiu Liu", "Shaoyong Guo", "Ruichen Zhang", "Jiacheng Wang", "Xuesong Qiu", "Geng Sun", "Weifeng Gong", "Dusit Niyato", "Qihui Wu"], "title": "A Synergy of Computing Power Networks and Low-Altitude Economy Intelligent Communications: Challenges, Design Principles, and Research Directions", "categories": ["cs.NI"], "comment": "22 pages, 6 figures", "summary": "The rapid development of the Low-Altitude Economy (LAE) has created\nopportunities for emerging services such as autonomous aerial transportation,\naerial sensing, and emergency response, all of which rely on efficient and\nintelligent communications. However, LAE intelligent communications face\nseveral challenges, including the limited computational capacity of aerial\nnodes, the lack of cross-scenario generalization, and the complexity of\nheterogeneous demands. Meanwhile, Computing Power Networks (CPNs) have emerged\nas a new paradigm for integrating distributed computing, networking, and\nstorage resources, but they are also constrained by static deployment and\nlimited adaptability. In this survey, we explore the synergy between LAE\nintelligent communications and CPNs. We first analyze how CPNs can support LAE\nintelligent communications in areas such as air-ground collaborative control,\nAI training, communication-computation co-ptimization, and ubiquitous\nlow-altitude information processing. Conversely, we discuss how LAE intelligent\ncommunications can enhance CPNs through mobility-assisted control, distributed\nintelligent training, dynamic routing, and in-network aerial computing.\nFinally, based on these insights, we outline design principles and future\nresearch directions for integrated CPN-LAE systems. This work provides a\ncomprehensive foundation for building flexible, adaptive, and resilient\narchitectures that leverage the synergy between CPNs and LAE to deliver\nhigh-quality and sustainable low-altitude services.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4f4e\u7a7a\u7ecf\u6d4e\uff08LAE\uff09\u667a\u80fd\u901a\u4fe1\u4e0e\u7b97\u529b\u7f51\u7edc\uff08CPNs\uff09\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\uff0c\u5206\u6790\u4e86\u4e8c\u8005\u5982\u4f55\u76f8\u4e92\u652f\u6301\uff0c\u5e76\u63d0\u51fa\u4e86\u96c6\u6210CPN-LAE\u7cfb\u7edf\u7684\u8bbe\u8ba1\u539f\u5219\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f4e\u7a7a\u7ecf\u6d4e\u7684\u5feb\u901f\u53d1\u5c55\u50ac\u751f\u4e86\u5bf9\u9ad8\u6548\u667a\u80fd\u901a\u4fe1\u7684\u9700\u6c42\uff0c\u4f46LAE\u667a\u80fd\u901a\u4fe1\u9762\u4e34\u8ba1\u7b97\u80fd\u529b\u6709\u9650\u3001\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u548c\u9700\u6c42\u590d\u6742\u7b49\u6311\u6218\u3002\u540c\u65f6\uff0c\u7b97\u529b\u7f51\u7edc\u4f5c\u4e3a\u6574\u5408\u8ba1\u7b97\u3001\u7f51\u7edc\u548c\u5b58\u50a8\u8d44\u6e90\u7684\u65b0\u8303\u5f0f\uff0c\u4e5f\u5b58\u5728\u90e8\u7f72\u9759\u6001\u548c\u9002\u5e94\u6027\u6709\u9650\u7684\u95ee\u9898\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22LAE\u667a\u80fd\u901a\u4fe1\u4e0e\u7b97\u529b\u7f51\u7edc\u4e4b\u95f4\u7684\u534f\u540c\u6f5c\u529b\uff0c\u4ee5\u514b\u670d\u5404\u81ea\u6311\u6218\uff0c\u652f\u6301\u65b0\u5174\u7684\u4f4e\u7a7a\u670d\u52a1\u3002", "method": "\u672c\u6587\u91c7\u7528\u7efc\u8ff0\uff08survey\uff09\u7684\u65b9\u6cd5\uff0c\u5177\u4f53\u5305\u62ec\uff1a1. \u5206\u6790\u7b97\u529b\u7f51\u7edc\u5982\u4f55\u5728\u7a7a\u5730\u534f\u540c\u63a7\u5236\u3001AI\u8bad\u7ec3\u3001\u901a\u7b97\u4e00\u4f53\u5316\u548c\u6cdb\u5728\u4f4e\u7a7a\u4fe1\u606f\u5904\u7406\u7b49\u65b9\u9762\u652f\u6301LAE\u667a\u80fd\u901a\u4fe1\u30022. \u63a2\u8ba8LAE\u667a\u80fd\u901a\u4fe1\u5982\u4f55\u901a\u8fc7\u79fb\u52a8\u8f85\u52a9\u63a7\u5236\u3001\u5206\u5e03\u5f0f\u667a\u80fd\u8bad\u7ec3\u3001\u52a8\u6001\u8def\u7531\u548c\u7f51\u7edc\u5185\u7a7a\u4e2d\u8ba1\u7b97\u7b49\u65b9\u9762\u589e\u5f3a\u7b97\u529b\u7f51\u7edc\u30023. \u57fa\u4e8e\u8fd9\u4e9b\u6d1e\u5bdf\uff0c\u5f52\u7eb3\u51fa\u96c6\u6210CPN-LAE\u7cfb\u7edf\u7684\u8bbe\u8ba1\u539f\u5219\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86LAE\u667a\u80fd\u901a\u4fe1\u4e0e\u7b97\u529b\u7f51\u7edc\u4e4b\u95f4\u5b58\u5728\u7684\u663e\u8457\u534f\u540c\u4f5c\u7528\uff1a\u7b97\u529b\u7f51\u7edc\u80fd\u591f\u6709\u6548\u652f\u6491LAE\u667a\u80fd\u901a\u4fe1\u7684\u591a\u79cd\u9700\u6c42\uff0c\u800cLAE\u667a\u80fd\u901a\u4fe1\u5219\u80fd\u901a\u8fc7\u5176\u79fb\u52a8\u6027\u548c\u5206\u5e03\u5f0f\u80fd\u529b\u589e\u5f3a\u7b97\u529b\u7f51\u7edc\u3002\u57fa\u4e8e\u6b64\u534f\u540c\u4f5c\u7528\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u96c6\u6210CPN-LAE\u7cfb\u7edf\u7684\u8bbe\u8ba1\u539f\u5219\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u5de5\u4f5c\u4e3a\u6784\u5efa\u7075\u6d3b\u3001\u9002\u5e94\u6027\u5f3a\u3001\u5f39\u6027\u9ad8\u7684\u67b6\u6784\u5960\u5b9a\u4e86\u5168\u9762\u7684\u57fa\u7840\uff0c\u8be5\u67b6\u6784\u80fd\u591f\u5229\u7528\u7b97\u529b\u7f51\u7edc\u548c\u4f4e\u7a7a\u7ecf\u6d4e\u7684\u534f\u540c\u4f5c\u7528\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u548c\u53ef\u6301\u7eed\u7684\u4f4e\u7a7a\u670d\u52a1\u3002\u5b83\u4e3a\u672a\u6765\u96c6\u6210CPN-LAE\u7cfb\u7edf\u7684\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.22768", "pdf": "https://arxiv.org/pdf/2509.22768", "abs": "https://arxiv.org/abs/2509.22768", "authors": ["Ekaterina Trofimova", "Zosia Shamina", "Maria Selifanova", "Artem Zaitsev", "Remi Savchuk", "Maxim Minets", "Daria Ozerova", "Emil Sataev", "Denis Zuenko", "Andrey E. Ustyuzhanin"], "title": "ML2B: Multi-Lingual ML Benchmark For AutoML", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nin generating machine learning (ML) code, enabling end-to-end pipeline\nconstruction from natural language instructions. However, existing benchmarks\nfor ML code generation are mainly restricted to English, overlooking the global\nand multilingual nature of ML research and practice. To address this gap, we\npresent ML2B, the first benchmark for evaluating multilingual ML code\ngeneration. ML2B consists of 30 Kaggle competitions translated into 13 natural\nlanguages, covering tabular, text, and image data types, with structured\nmetadata and validated human-reviewed translations. For evaluation, we employ\nAIDE, an automated framework for end-to-end assessment of data science\npipelines, and provide insights into cross-lingual model performance. Our\nresults reveal substantial 15-45% performance degradation on non-English tasks,\nhighlighting critical challenges in multilingual representation learning for\ncode generation. The benchmark, evaluation framework, and comprehensive results\nare made available through our GitHub repository to facilitate future research\nin multilingual ML code generation: https://github.com/enaix/ml2b.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u9996\u4e2a\u591a\u8bed\u8a00ML\u4ee3\u7801\u751f\u6210\u57fa\u51c6ML2B\uff0c\u53d1\u73b0LLM\u5728\u975e\u82f1\u8bed\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u663e\u8457\u4e0b\u964d15-45%\uff0c\u63ed\u793a\u4e86\u591a\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60(ML)\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u4e3b\u8981\u9650\u4e8e\u82f1\u8bed\uff0c\u5ffd\u89c6\u4e86ML\u7814\u7a76\u548c\u5b9e\u8df5\u7684\u5168\u7403\u5316\u548c\u591a\u8bed\u8a00\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u8bc4\u4f30\u591a\u8bed\u8a00ML\u4ee3\u7801\u751f\u6210\u7684\u57fa\u51c6\u3002", "method": "\u5f00\u53d1\u4e86ML2B\uff0c\u4e00\u4e2a\u5305\u542b30\u4e2aKaggle\u7ade\u8d5b\uff08\u6db5\u76d6\u8868\u683c\u3001\u6587\u672c\u548c\u56fe\u50cf\u6570\u636e\uff0c\u5e76\u7ffb\u8bd1\u621013\u79cd\u81ea\u7136\u8bed\u8a00\uff09\u7684\u591a\u8bed\u8a00ML\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u3002\u4f7f\u7528AIDE\u81ea\u52a8\u5316\u6846\u67b6\u5bf9\u6570\u636e\u79d1\u5b66\u7ba1\u9053\u8fdb\u884c\u7aef\u5230\u7aef\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5728\u975e\u82f1\u8bed\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u670915-45%\u7684\u663e\u8457\u4e0b\u964d\uff0c\u8fd9\u7a81\u51fa\u4e86\u591a\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u4e2d\u8de8\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "ML2B\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\u63ed\u793a\u4e86LLM\u5728\u591a\u8bed\u8a00ML\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u975e\u82f1\u8bed\u4efb\u52a1\u7684\u6027\u80fd\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u591a\u8bed\u8a00ML\u4ee3\u7801\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u548c\u8d44\u6e90\u3002"}}
{"id": "2509.23006", "pdf": "https://arxiv.org/pdf/2509.23006", "abs": "https://arxiv.org/abs/2509.23006", "authors": ["Hassen Dhrif"], "title": "Creative Adversarial Testing (CAT): A Novel Framework for Evaluating Goal-Oriented Agentic AI Systems", "categories": ["cs.AI"], "comment": null, "summary": "Agentic AI represents a paradigm shift in enhancing the capabilities of\ngenerative AI models. While these systems demonstrate immense potential and\npower, current evaluation techniques primarily focus on assessing their\nefficacy in identifying appropriate agents, tools, and parameters. However, a\ncritical gap exists in evaluating the alignment between an Agentic AI system's\ntasks and its overarching goals. This paper introduces the Creative Adversarial\nTesting (CAT) framework, a novel approach designed to capture and analyze the\ncomplex relationship between Agentic AI tasks and the system's intended\nobjectives.\n  We validate the CAT framework through extensive simulation using synthetic\ninteraction data modeled after Alexa+ audio services, a sophisticated Agentic\nAI system that shapes the user experience for millions of users globally. This\nsynthetic data approach enables comprehensive testing of edge cases and failure\nmodes while protecting user privacy. Our results demonstrate that the CAT\nframework provides unprecedented insights into goal-task alignment, enabling\nmore effective optimization and development of Agentic AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u521b\u610f\u5bf9\u6297\u6d4b\u8bd5\uff08CAT\uff09\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u4ee3\u7406AI\u7cfb\u7edf\u4efb\u52a1\u4e0e\u603b\u76ee\u6807\u5bf9\u9f50\u6027\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7\u6a21\u62dfAlexa+\u670d\u52a1\u6570\u636e\u9a8c\u8bc1\u4e86\u5176\u80fd\u6709\u6548\u6d1e\u5bdf\u76ee\u6807-\u4efb\u52a1\u5bf9\u9f50\uff0c\u4fc3\u8fdb\u7cfb\u7edf\u4f18\u5316\u3002", "motivation": "\u5f53\u524d\u5bf9\u4ee3\u7406AI\u7cfb\u7edf\u7684\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u5728\u8bc6\u522b\u5408\u9002\u7684\u4ee3\u7406\u3001\u5de5\u5177\u548c\u53c2\u6570\u7684\u6709\u6548\u6027\u4e0a\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7cfb\u7edf\u4efb\u52a1\u4e0e\u5176\u603b\u4f53\u76ee\u6807\u4e4b\u95f4\u5bf9\u9f50\u6027\u7684\u5173\u952e\u8bc4\u4f30\u3002", "method": "\u5f15\u5165\u201c\u521b\u610f\u5bf9\u6297\u6d4b\u8bd5\uff08CAT\uff09\u201d\u6846\u67b6\u3002\u901a\u8fc7\u6a21\u62dfAlexa+\u97f3\u9891\u670d\u52a1\u7684\u5408\u6210\u4ea4\u4e92\u6570\u636e\u8fdb\u884c\u5e7f\u6cdb\u4eff\u771f\uff0c\u4ee5\u9a8c\u8bc1\u8be5\u6846\u67b6\u3002", "result": "CAT\u6846\u67b6\u4e3a\u76ee\u6807-\u4efb\u52a1\u5bf9\u9f50\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u6d1e\u5bdf\u529b\u3002", "conclusion": "CAT\u6846\u67b6\u80fd\u591f\u66f4\u6709\u6548\u5730\u4f18\u5316\u548c\u5f00\u53d1\u4ee3\u7406AI\u7cfb\u7edf\uff0c\u4ece\u800c\u63d0\u5347\u5176\u6027\u80fd\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2509.22700", "pdf": "https://arxiv.org/pdf/2509.22700", "abs": "https://arxiv.org/abs/2509.22700", "authors": ["Zhuang Qi", "Pan Yu", "Lei Meng", "Sijin Zhou", "Han Yu", "Xiaoxiao Li", "Xiangxu Meng"], "title": "Global Prompt Refinement with Non-Interfering Attention Masking for One-Shot Federated Learning", "categories": ["cs.CV"], "comment": "NeurIPS'25 accepted", "summary": "Federated Prompt Learning (FPL) enables communication-efficient adaptation by\ntuning lightweight prompts on top of frozen pre-trained models. Existing FPL\nmethods typically rely on global information, which is only available after the\nsecond training round, to facilitate collaboration among client models.\nTherefore, they are inherently dependent on multi-round communication to fully\nexhibit their strengths. Moreover, existing one-shot federated learning methods\ntypically focus on fitting seen tasks, but lack cross-task generalization. To\nbridge this gap, we propose the Global Prompt Refinement with Non-Interfering\nAttention Masking (GPR-NIAM) method for one-shot FPL. The core idea is to\ndesign a masking mechanism that restricts excessive interaction between the\noriginal text embeddings and the learnable prompt embeddings. GPR-NIAM achieves\nthis through the collaboration of two key modules. Firstly, the attention\nisolation module suppresses attention from the learnable prompt tokens to the\noriginal text tokens, and reweights the reverse attention which preserves\ngeneralization across tasks. Secondly, the cross-silo collaborative refinement\nmodule integrates decentralized visual knowledge into a unified base and\ncalibrates the global prompt through multi-source cross-modal knowledge\nalignment, further mitigating the inconsistency caused by data heterogeneity.\nExtensive experiments conducted on ten benchmark datasets under two tasks show\nthat GPR-NIAM outperforms eight state-of-the-art methods in both class-level\nand domain-level generalization.", "AI": {"tldr": "GPR-NIAM\u63d0\u51fa\u4e00\u79cd\u5355\u8f6e\u8054\u90a6\u63d0\u793a\u5b66\u4e60\uff08FPL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u5e72\u6270\u6ce8\u610f\u529b\u63a9\u853d\u548c\u8de8\u7b52\u4ed3\u534f\u4f5c\u7ec6\u5316\uff0c\u89e3\u51b3\u73b0\u6709FPL\u5bf9\u591a\u8f6e\u901a\u4fe1\u7684\u4f9d\u8d56\u548c\u7f3a\u4e4f\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u63d0\u793a\u5b66\u4e60\uff08FPL\uff09\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u591a\u8f6e\u901a\u4fe1\uff0c\u5e76\u4e14\u73b0\u6709\u7684\u5355\u8f6e\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4e13\u6ce8\u4e8e\u62df\u5408\u5df2\u89c1\u4efb\u52a1\uff0c\u7f3a\u4e4f\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5168\u5c40\u63d0\u793a\u7ec6\u5316\u4e0e\u975e\u5e72\u6270\u6ce8\u610f\u529b\u63a9\u853d\uff08GPR-NIAM\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5355\u8f6eFPL\u3002\u6838\u5fc3\u601d\u60f3\u662f\u8bbe\u8ba1\u4e00\u79cd\u63a9\u853d\u673a\u5236\uff0c\u9650\u5236\u539f\u59cb\u6587\u672c\u5d4c\u5165\u4e0e\u53ef\u5b66\u4e60\u63d0\u793a\u5d4c\u5165\u4e4b\u95f4\u7684\u8fc7\u5ea6\u4ea4\u4e92\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a1) \u6ce8\u610f\u529b\u9694\u79bb\u6a21\u5757\uff0c\u7528\u4e8e\u6291\u5236\u53ef\u5b66\u4e60\u63d0\u793a\u4ee4\u724c\u5230\u539f\u59cb\u6587\u672c\u4ee4\u724c\u7684\u6ce8\u610f\u529b\uff0c\u5e76\u91cd\u65b0\u52a0\u6743\u53cd\u5411\u6ce8\u610f\u529b\u4ee5\u4fdd\u6301\u8de8\u4efb\u52a1\u6cdb\u5316\u30022) \u8de8\u7b52\u4ed3\u534f\u4f5c\u7ec6\u5316\u6a21\u5757\uff0c\u7528\u4e8e\u6574\u5408\u53bb\u4e2d\u5fc3\u5316\u7684\u89c6\u89c9\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u591a\u6e90\u8de8\u6a21\u6001\u77e5\u8bc6\u5bf9\u9f50\u6821\u51c6\u5168\u5c40\u63d0\u793a\uff0c\u4ee5\u7f13\u89e3\u6570\u636e\u5f02\u8d28\u6027\u9020\u6210\u7684\u4e0d\u4e00\u81f4\u3002", "result": "\u5728\u5341\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9488\u5bf9\u4e24\u9879\u4efb\u52a1\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGPR-NIAM\u5728\u7c7b\u522b\u7ea7\u548c\u9886\u57df\u7ea7\u6cdb\u5316\u65b9\u9762\u5747\u4f18\u4e8e\u516b\u79cd\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "GPR-NIAM\u901a\u8fc7\u5176\u72ec\u7279\u7684\u975e\u5e72\u6270\u6ce8\u610f\u529b\u63a9\u853d\u548c\u534f\u4f5c\u7ec6\u5316\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u8f6e\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u4e2d\u591a\u8f6e\u901a\u4fe1\u4f9d\u8d56\u548c\u8de8\u4efb\u52a1\u6cdb\u5316\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2509.22881", "pdf": "https://arxiv.org/pdf/2509.22881", "abs": "https://arxiv.org/abs/2509.22881", "authors": ["Karim Khamaisi", "Nicolas Keller", "Stefan Krummenacher", "Valentin Huber", "Bernhard F\u00e4ssler", "Bruno Rodrigues"], "title": "From Noise to Knowledge: A Comparative Study of Acoustic Anomaly Detection Models in Pumped-storage Hydropower Plants", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In the context of industrial factories and energy producers, unplanned\noutages are highly costly and difficult to service. However, existing\nacoustic-anomaly detection studies largely rely on generic industrial or\nsynthetic datasets, with few focused on hydropower plants due to limited\naccess. This paper presents a comparative analysis of acoustic-based anomaly\ndetection methods, as a way to improve predictive maintenance in hydropower\nplants. We address key challenges in the acoustic preprocessing under highly\nnoisy conditions before extracting time- and frequency-domain features. Then,\nwe benchmark three machine learning models: LSTM AE, K-Means, and OC-SVM, which\nare tested on two real-world datasets from the Rodundwerk II pumped-storage\nplant in Austria, one with induced anomalies and one with real-world\nconditions. The One-Class SVM achieved the best trade-off of accuracy (ROC AUC\n0.966-0.998) and minimal training time, while the LSTM autoencoder delivered\nstrong detection (ROC AUC 0.889-0.997) at the expense of higher computational\ncost.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u5206\u6790\u4e86\u6c34\u7535\u5382\u57fa\u4e8e\u58f0\u5b66\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u53d1\u73b0\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\uff0cOC-SVM\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u5de5\u4e1a\u5de5\u5382\u548c\u80fd\u6e90\u751f\u4ea7\u5546\uff08\u5c24\u5176\u662f\u6c34\u7535\u5382\uff09\u7684\u975e\u8ba1\u5212\u505c\u673a\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u7ef4\u62a4\u3002\u73b0\u6709\u58f0\u5b66\u5f02\u5e38\u68c0\u6d4b\u7814\u7a76\u7f3a\u4e4f\u9488\u5bf9\u6c34\u7535\u5382\u7684\u4e13\u7528\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u58f0\u5b66\u5206\u6790\u6539\u5584\u6c34\u7535\u5382\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u3002", "method": "\u5728\u9ad8\u566a\u58f0\u6761\u4ef6\u4e0b\u5bf9\u58f0\u5b66\u6570\u636e\u8fdb\u884c\u9884\u5904\u7406\uff0c\u63d0\u53d6\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\u3002\u6bd4\u8f83\u4e86\u4e09\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff1aLSTM AE\u3001K-Means\u548cOC-SVM\u3002\u6a21\u578b\u5728\u5965\u5730\u5229Rodundwerk II\u62bd\u6c34\u84c4\u80fd\u7535\u7ad9\u7684\u4e24\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\uff08\u4e00\u4e2a\u5305\u542b\u8bf1\u5bfc\u5f02\u5e38\uff0c\u4e00\u4e2a\u53cd\u6620\u771f\u5b9e\u5de5\u51b5\uff09\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u5355\u7c7b\u652f\u6301\u5411\u91cf\u673a\uff08OC-SVM\uff09\u5728\u51c6\u786e\u6027\uff08ROC AUC 0.966-0.998\uff09\u548c\u6700\u5c0f\u8bad\u7ec3\u65f6\u95f4\u65b9\u9762\u8868\u73b0\u51fa\u6700\u4f73\u7684\u6743\u8861\u3002LSTM\u81ea\u7f16\u7801\u5668\u4e5f\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u68c0\u6d4b\u80fd\u529b\uff08ROC AUC 0.889-0.997\uff09\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\u3002", "conclusion": "OC-SVM\u662f\u6c34\u7535\u5382\u58f0\u5b66\u5f02\u5e38\u68c0\u6d4b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u8868\u73b0\u51fa\u6700\u4f73\u5e73\u8861\uff0c\u9002\u5408\u7528\u4e8e\u6539\u5584\u9884\u6d4b\u6027\u7ef4\u62a4\u3002"}}
{"id": "2509.23913", "pdf": "https://arxiv.org/pdf/2509.23913", "abs": "https://arxiv.org/abs/2509.23913", "authors": ["Cheonjin Park", "Victoria Manfredi", "Xiaolan Zhang", "Chengyi Liu", "Alicia P Wolfe", "Dongjin Song", "Sarah Tasneem", "Bing Wang"], "title": "Continual Learning to Generalize Forwarding Strategies for Diverse Mobile Wireless Networks", "categories": ["cs.NI", "cs.AI"], "comment": "11 pages", "summary": "Deep reinforcement learning (DRL) has been successfully used to design\nforwarding strategies for multi-hop mobile wireless networks. While such\nstrategies can be used directly for networks with varied connectivity and\ndynamic conditions, developing generalizable approaches that are effective on\nscenarios significantly different from the training environment remains largely\nunexplored. In this paper, we propose a framework to address the challenge of\ngeneralizability by (i) developing a generalizable base model considering\ndiverse mobile network scenarios, and (ii) using the generalizable base model\nfor new scenarios, and when needed, fine-tuning the base model using a small\namount of data from the new scenarios. To support this framework, we first\ndesign new features to characterize network variation and feature quality,\nthereby improving the information used in DRL-based forwarding decisions. We\nthen develop a continual learning (CL) approach able to train DRL models across\ndiverse network scenarios without ``catastrophic forgetting.'' Using extensive\nevaluation, including real-world scenarios in two cities, we show that our\napproach is generalizable to unseen mobility scenarios. Compared to a\nstate-of-the-art heuristic forwarding strategy, it leads to up to 78% reduction\nin delay, 24% improvement in delivery rate, and comparable or slightly higher\nnumber of forwards.", "AI": {"tldr": "\u9488\u5bf9\u591a\u8df3\u79fb\u52a8\u65e0\u7ebf\u7f51\u7edcDRL\u8f6c\u53d1\u7b56\u7565\u7684\u6cdb\u5316\u6027\u6311\u6218\uff0c\u63d0\u51fa\u57fa\u4e8e\u6301\u7eed\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u7279\u5f81\u548c\u5fae\u8c03\u5b9e\u73b0\u5bf9\u672a\u89c1\u573a\u666f\u7684\u6cdb\u5316\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u5347\u6295\u9012\u7387\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5728\u591a\u8df3\u79fb\u52a8\u65e0\u7ebf\u7f51\u7edc\u8f6c\u53d1\u7b56\u7565\u4e2d\uff0c\u5bf9\u8bad\u7ec3\u73af\u5883\u4e4b\u5916\u7684\u663e\u8457\u4e0d\u540c\u573a\u666f\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6cdb\u5316\u6846\u67b6\uff0c\u5305\u542b\u5f00\u53d1\u6cdb\u5316\u57fa\u7840\u6a21\u578b\u548c\u5728\u65b0\u573a\u666f\u4e2d\u5fae\u8c03\u3002\u901a\u8fc7\u8bbe\u8ba1\u8868\u5f81\u7f51\u7edc\u53d8\u5316\u548c\u7279\u5f81\u8d28\u91cf\u7684\u65b0\u7279\u5f81\uff0c\u5e76\u5f00\u53d1\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u65b9\u6cd5\uff0c\u907f\u514d\u201c\u707e\u96be\u6027\u9057\u5fd8\u201d\uff0c\u4ece\u800c\u5728\u591a\u6837\u5316\u573a\u666f\u4e0b\u8bad\u7ec3DRL\u6a21\u578b\u3002", "result": "\u7ecf\u5e7f\u6cdb\u8bc4\u4f30\uff08\u5305\u62ec\u4e24\u4e2a\u57ce\u5e02\u771f\u5b9e\u573a\u666f\uff09\uff0c\u8be5\u65b9\u6cd5\u5bf9\u672a\u89c1\u79fb\u52a8\u573a\u666f\u5177\u6709\u6cdb\u5316\u80fd\u529b\u3002\u4e0e\u73b0\u6709\u542f\u53d1\u5f0f\u7b56\u7565\u76f8\u6bd4\uff0c\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe78%\uff0c\u6295\u9012\u7387\u63d0\u9ad824%\uff0c\u8f6c\u53d1\u6b21\u6570\u6301\u5e73\u6216\u7565\u9ad8\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u548c\u7279\u5f81\u5de5\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86DRL\u5728\u591a\u8df3\u79fb\u52a8\u65e0\u7ebf\u7f51\u7edc\u8f6c\u53d1\u4e2d\u7684\u6cdb\u5316\u6027\u6311\u6218\uff0c\u5728\u591a\u6837\u5316\u548c\u672a\u89c1\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2509.22808", "pdf": "https://arxiv.org/pdf/2509.22808", "abs": "https://arxiv.org/abs/2509.22808", "authors": ["Mohamed Maged", "Alhassan Ehab", "Ali Mekky", "Besher Hassan", "Shady Shehata"], "title": "ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection", "categories": ["cs.CL"], "comment": null, "summary": "With the rise of generative text-to-speech models, distinguishing between\nreal and synthetic speech has become challenging, especially for Arabic that\nhave received limited research attention. Most spoof detection efforts have\nfocused on English, leaving a significant gap for Arabic and its many dialects.\nIn this work, we introduce the first multi-dialect Arabic spoofed speech\ndataset. To evaluate the difficulty of the synthesized audio from each model\nand determine which produces the most challenging samples, we aimed to guide\nthe construction of our final dataset either by merging audios from multiple\nmodels or by selecting the best-performing model, we conducted an evaluation\npipeline that included training classifiers using two approaches: modern\nembedding-based methods combined with classifier heads; classical machine\nlearning algorithms applied to MFCC features; and the RawNet2 architecture. The\npipeline further incorporated the calculation of Mean Opinion Score based on\nhuman ratings, as well as processing both original and synthesized datasets\nthrough an Automatic Speech Recognition model to measure the Word Error Rate.\nOur results demonstrate that FishSpeech outperforms other TTS models in Arabic\nvoice cloning on the Casablanca corpus, producing more realistic and\nchallenging synthetic speech samples. However, relying on a single TTS for\ndataset creation may limit generalizability.", "AI": {"tldr": "\u672c\u6587\u521b\u5efa\u4e86\u9996\u4e2a\u591a\u65b9\u8a00\u963f\u62c9\u4f2f\u8bed\u4f2a\u9020\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e0d\u540cTTS\u6a21\u578b\u751f\u6210\u5408\u6210\u8bed\u97f3\u7684\u96be\u5ea6\uff0c\u53d1\u73b0FishSpeech\u80fd\u751f\u6210\u6700\u5177\u6311\u6218\u6027\u7684\u6837\u672c\u3002", "motivation": "\u751f\u6210\u5f0f\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u4f7f\u533a\u5206\u771f\u5b9e\u4e0e\u5408\u6210\u8bed\u97f3\u53d8\u5f97\u56f0\u96be\uff0c\u5c24\u5176\u5728\u963f\u62c9\u4f2f\u8bed\u9886\u57df\uff0c\u76f8\u5173\u7814\u7a76\u548c\u4f2a\u9020\u68c0\u6d4b\u6570\u636e\u96c6\u975e\u5e38\u532e\u4e4f\u3002", "method": "\u5f15\u5165\u4e86\u9996\u4e2a\u591a\u65b9\u8a00\u963f\u62c9\u4f2f\u8bed\u4f2a\u9020\u8bed\u97f3\u6570\u636e\u96c6\u3002\u8bc4\u4f30\u6d41\u7a0b\u5305\u62ec\uff1a\u8bad\u7ec3\u591a\u79cd\u5206\u7c7b\u5668\uff08\u5d4c\u5165\u5f0f\u65b9\u6cd5\u3001MFCC+\u7ecf\u5178ML\u3001RawNet2\uff09\uff0c\u8ba1\u7b97\u4eba\u7c7b\u8bc4\u5206\u7684\u5e73\u5747\u610f\u89c1\u5f97\u5206\uff08MOS\uff09\uff0c\u5e76\u4f7f\u7528\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u578b\u6d4b\u91cf\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u3002", "result": "FishSpeech\u5728\u5361\u8428\u5e03\u5170\u5361\u8bed\u6599\u5e93\u4e0a\u7684\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u514b\u9686\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u751f\u6210\u7684\u5408\u6210\u8bed\u97f3\u6837\u672c\u66f4\u771f\u5b9e\u3001\u66f4\u5177\u6311\u6218\u6027\u3002", "conclusion": "FishSpeech\u5728\u751f\u6210\u96be\u4ee5\u533a\u5206\u7684\u5408\u6210\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ec5\u4f9d\u8d56\u5355\u4e00TTS\u6a21\u578b\u6784\u5efa\u6570\u636e\u96c6\u53ef\u80fd\u4f1a\u9650\u5236\u5176\u901a\u7528\u6027\u3002"}}
{"id": "2509.23023", "pdf": "https://arxiv.org/pdf/2509.23023", "abs": "https://arxiv.org/abs/2509.23023", "authors": ["Davi Bastos Costa", "Renato Vicente"], "title": "Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia", "categories": ["cs.AI"], "comment": "20 pages, 7 figures, 5 tables; submitted to ICLR 2026; Code and data:\n  https://github.com/bastoscostadavi/llm-mafia-game", "summary": "Mafia is a social deduction game where informed mafia compete against\nuninformed townsfolk. Its asymmetry of information and reliance on\ntheory-of-mind reasoning mirror real-world multi-agent scenarios, making it a\nuseful testbed for evaluating the social intelligence of large language models\n(LLMs). To support a systematic study, we introduce Mini-Mafia: a simplified\nfour-player variant with one mafioso, one detective, and two villagers. We set\nthe mafioso to kill a villager and the detective to investigate the mafioso\nduring the night, reducing the game to a single day phase of discussion and\nvoting. This setup isolates three interactive capabilities through\nrole-specific win conditions: the mafioso must deceive, the villagers must\ndetect deception, and the detective must effectively disclose information. To\nmeasure these skills, we have LLMs play against each other, creating the\nMini-Mafia Benchmark: a two-stage framework that first estimates win rates\nwithin fixed opponent configurations, then aggregates performance across them\nusing standardized scoring. Built entirely from model interactions without\nexternal data, the benchmark evolves as new models are introduced, with each\none serving both as a new opponent and as a subject of evaluation. Our\nexperiments reveal counterintuitive results, including cases where smaller\nmodels outperform larger ones. Beyond benchmarking, Mini-Mafia enables\nquantitative study of emergent multi-agent dynamics such as name bias and\nlast-speaker advantage. It also contributes to AI safety by generating training\ndata for deception detectors and by tracking models' deception capabilities\nagainst human baselines.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165Mini-Mafia\uff0c\u4e00\u4e2a\u7b80\u5316\u7684\u72fc\u4eba\u6740\u53d8\u4f53\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u793e\u4ea4\u667a\u80fd\u3002\u901a\u8fc7LLMs\u76f8\u4e92\u5bf9\u6297\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u81ea\u6f14\u8fdb\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8861\u91cf\u6b3a\u9a97\u3001\u8bc6\u7834\u6b3a\u9a97\u548c\u4fe1\u606f\u62ab\u9732\u80fd\u529b\uff0c\u5e76\u53d1\u73b0\u53cd\u76f4\u89c9\u7ed3\u679c\uff08\u5982\u5c0f\u6a21\u578b\u80dc\u8fc7\u5927\u6a21\u578b\uff09\u3002\u8be5\u7814\u7a76\u4e5f\u4fc3\u8fdb\u4e86\u5bf9\u591a\u667a\u80fd\u4f53\u52a8\u529b\u5b66\u7684\u7406\u89e3\u548cAI\u5b89\u5168\u3002", "motivation": "\u72fc\u4eba\u6740\u662f\u4e00\u6b3e\u4fe1\u606f\u4e0d\u5bf9\u79f0\u4e14\u4f9d\u8d56\u5fc3\u667a\u7406\u8bba\u63a8\u7406\u7684\u793e\u4ea4\u63a8\u7406\u6e38\u620f\uff0c\u5b83\u80fd\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u7684\u591a\u667a\u80fd\u4f53\u573a\u666f\u3002\u56e0\u6b64\uff0c\u5b83\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u793e\u4ea4\u667a\u80fd\u7684\u6709\u7528\u8bd5\u9a8c\u5e73\u53f0\u3002", "method": "1. \u5f15\u5165Mini-Mafia\uff1a\u4e00\u4e2a\u7b80\u5316\u7684\u56db\u4eba\u72fc\u4eba\u6740\u53d8\u4f53\uff08\u4e00\u72fc\u4eba\u3001\u4e00\u4fa6\u63a2\u3001\u4e24\u6751\u6c11\uff09\uff0c\u6e38\u620f\u7b80\u5316\u4e3a\u5355\u4e00\u7684\u767d\u5929\u8ba8\u8bba\u548c\u6295\u7968\u9636\u6bb5\u3002\n2. \u901a\u8fc7\u89d2\u8272\u7279\u5b9a\u80dc\u5229\u6761\u4ef6\uff0c\u9694\u79bb\u5e76\u8bc4\u4f30\u4e09\u79cd\u4ea4\u4e92\u80fd\u529b\uff1a\u72fc\u4eba\u7684\u6b3a\u9a97\u80fd\u529b\u3001\u6751\u6c11\u7684\u8bc6\u7834\u6b3a\u9a97\u80fd\u529b\u3001\u4fa6\u63a2\u7684\u6709\u6548\u4fe1\u606f\u62ab\u9732\u80fd\u529b\u3002\n3. \u521b\u5efaMini-Mafia\u57fa\u51c6\u6d4b\u8bd5\uff1aLLMs\u76f8\u4e92\u5bf9\u6297\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u9996\u5148\u8bc4\u4f30\u56fa\u5b9a\u5bf9\u624b\u914d\u7f6e\u4e0b\u7684\u80dc\u7387\uff0c\u7136\u540e\u901a\u8fc7\u6807\u51c6\u5316\u8bc4\u5206\u805a\u5408\u6027\u80fd\u3002\n4. \u8be5\u57fa\u51c6\u6d4b\u8bd5\u5b8c\u5168\u57fa\u4e8e\u6a21\u578b\u4ea4\u4e92\u6784\u5efa\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\uff0c\u5e76\u4f1a\u968f\u7740\u65b0\u6a21\u578b\u7684\u5f15\u5165\u800c\u6f14\u8fdb\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u4e00\u4e9b\u53cd\u76f4\u89c9\u7684\u7ed3\u679c\uff0c\u5305\u62ec\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u8f83\u5c0f\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u8f83\u5927\u7684\u6a21\u578b\u3002", "conclusion": "1. Mini-Mafia\u4e0d\u4ec5\u662f\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u8fd8\u80fd\u7528\u4e8e\u5b9a\u91cf\u7814\u7a76\u7a81\u53d1\u7684\u591a\u667a\u80fd\u4f53\u52a8\u529b\u5b66\uff0c\u4f8b\u5982\u540d\u5b57\u504f\u89c1\u548c\u6700\u540e\u53d1\u8a00\u8005\u4f18\u52bf\u3002\n2. \u901a\u8fc7\u4e3a\u6b3a\u9a97\u68c0\u6d4b\u5668\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u4ee5\u53ca\u8ffd\u8e2a\u6a21\u578b\u7684\u6b3a\u9a97\u80fd\u529b\u4e0e\u4eba\u7c7b\u57fa\u7ebf\u7684\u5bf9\u6bd4\uff0c\u8be5\u7814\u7a76\u5bf9AI\u5b89\u5168\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2509.22708", "pdf": "https://arxiv.org/pdf/2509.22708", "abs": "https://arxiv.org/abs/2509.22708", "authors": ["Ahed Alboody"], "title": "GZSL-MoE: Apprentissage G{\u00e9}n{\u00e9}ralis{\u00e9} Z{\u00e9}ro-Shot bas{\u00e9} sur le M{\u00e9}lange d'Experts pour la Segmentation S{\u00e9}mantique de Nuages de Points 3DAppliqu{\u00e9} {\u00e0} un Jeu de Donn{\u00e9}es d'Environnement de Collaboration Humain-Robot", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "in French language. 28e Conf{\\'e}rence Nationale en Intelligence\n  Artificielle. Plate-Forme Intelligence Artificielle 2025, Association Fran{\\c\n  c}aise pour l'Intelligence Artificielle, https://pfia2025.u-bourgogne.fr/,\n  Jun 2025, Dijon, France", "summary": "Generative Zero-Shot Learning approach (GZSL) has demonstrated significant\npotential in 3D point cloud semantic segmentation tasks. GZSL leverages\ngenerative models like GANs or VAEs to synthesize realistic features (real\nfeatures) of unseen classes. This allows the model to label unseen classes\nduring testing, despite being trained only on seen classes. In this context, we\nintroduce the Generalized Zero-Shot Learning based-upon Mixture-of-Experts\n(GZSL-MoE) model. This model incorporates Mixture-of-Experts layers (MoE) to\ngenerate fake features that closely resemble real features extracted using a\npre-trained KPConv (Kernel Point Convolution) model on seen classes. The main\ncontribution of this paper is the integration of Mixture-of-Experts into the\nGenerator and Discriminator components of the Generative Zero-Shot Learning\nmodel for 3D point cloud semantic segmentation, applied to the COVERED dataset\n(CollabOratiVE Robot Environment Dataset) for Human-Robot Collaboration (HRC)\nenvironments. By combining the Generative Zero-Shot Learning model with\nMixture-of- Experts, GZSL-MoE for 3D point cloud semantic segmentation provides\na promising solution for understanding complex 3D environments, especially when\ncomprehensive training data for all object classes is unavailable. The\nperformance evaluation of the GZSL-MoE model highlights its ability to enhance\nperformance on both seen and unseen classes. Keywords Generalized Zero-Shot\nLearning (GZSL), 3D Point Cloud, 3D Semantic Segmentation, Human-Robot\nCollaboration, COVERED (CollabOratiVE Robot Environment Dataset), KPConv,\nMixture-of Experts", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGZSL-MoE\u6a21\u578b\uff0c\u5c06Mixture-of-Experts\u96c6\u6210\u5230\u751f\u6210\u5f0f\u96f6\u6837\u672c\u5b66\u4e60\uff08GZSL\uff09\u6a21\u578b\u4e2d\uff0c\u7528\u4e8e3D\u70b9\u4e91\u8bed\u4e49\u5206\u5272\uff0c\u63d0\u5347\u4e86\u5728\u4eba\u673a\u534f\u4f5c\u73af\u5883\u4e2d\u5bf9\u5df2\u77e5\u548c\u672a\u77e5\u7c7b\u522b\u7684\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "3D\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u5728\u590d\u67423D\u73af\u5883\u4e2d\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u6240\u6709\u5bf9\u8c61\u7c7b\u522b\uff0c\u96be\u4ee5\u83b7\u53d6\u5168\u9762\u6570\u636e\u3002GZSL\u5728\u89e3\u51b3\u6b64\u95ee\u9898\u4e0a\u5177\u6709\u6f5c\u529b\u3002", "method": "\u5f15\u5165GZSL-MoE\u6a21\u578b\uff0c\u5c06Mixture-of-Experts (MoE) \u5c42\u6574\u5408\u5230GZSL\u6a21\u578b\u7684\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u4e2d\uff0c\u4ee5\u751f\u6210\u4e0e\u901a\u8fc7\u9884\u8bad\u7ec3KPConv\u6a21\u578b\u63d0\u53d6\u7684\u771f\u5b9e\u7279\u5f81\u76f8\u4f3c\u7684\u4f2a\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8eCOVERED\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u4eba\u673a\u534f\u4f5c\u73af\u5883\u3002", "result": "GZSL-MoE\u6a21\u578b\u5728\u5df2\u77e5\u548c\u672a\u77e5\u7c7b\u522b\u4e0a\u7684\u6027\u80fd\u5747\u5f97\u5230\u4e86\u63d0\u5347\u3002", "conclusion": "GZSL-MoE\u4e3a\u7406\u89e3\u590d\u67423D\u73af\u5883\uff0c\u5c24\u5176\u662f\u5728\u8bad\u7ec3\u6570\u636e\u4e0d\u5168\u9762\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u76843D\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.22907", "pdf": "https://arxiv.org/pdf/2509.22907", "abs": "https://arxiv.org/abs/2509.22907", "authors": ["Anutam Srinivasan", "Aditya T. Vadlamani", "Amin Meghrazi", "Srinivasan Parthasarathy"], "title": "FedCF: Fair Federated Conformal Prediction", "categories": ["cs.LG"], "comment": "Preprint", "summary": "Conformal Prediction (CP) is a widely used technique for quantifying\nuncertainty in machine learning models. In its standard form, CP offers\nprobabilistic guarantees on the coverage of the true label, but it is agnostic\nto sensitive attributes in the dataset. Several recent works have sought to\nincorporate fairness into CP by ensuring conditional coverage guarantees across\ndifferent subgroups. One such method is Conformal Fairness (CF). In this work,\nwe extend the CF framework to the Federated Learning setting and discuss how we\ncan audit a federated model for fairness by analyzing the fairness-related gaps\nfor different demographic groups. We empirically validate our framework by\nconducting experiments on several datasets spanning multiple domains, fully\nleveraging the exchangeability assumption.", "AI": {"tldr": "\u5c06\u5171\u5f62\u516c\u5e73\uff08CF\uff09\u6269\u5c55\u5230\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\uff0c\u7528\u4e8e\u5ba1\u8ba1\u8054\u90a6\u6a21\u578b\u7684\u516c\u5e73\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6807\u51c6\u7684\u5171\u5f62\u9884\u6d4b\uff08CP\uff09\u5728\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u9762\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6570\u636e\u96c6\u654f\u611f\u5c5e\u6027\u7684\u516c\u5e73\u6027\u8003\u91cf\u3002\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u5171\u5f62\u516c\u5e73CF\uff09\u5df2\u5c1d\u8bd5\u5c06\u516c\u5e73\u6027\u878d\u5165CP\uff0c\u4f46\u5c1a\u672a\u5c06\u5176\u5e94\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u73af\u5883\uff0c\u8fd9\u4fc3\u4f7f\u672c\u7814\u7a76\u63a2\u7d22\u5728FL\u4e2d\u5b9e\u73b0\u516c\u5e73\u6027\u5ba1\u8ba1\u3002", "method": ["\u5c06\u5171\u5f62\u516c\u5e73\uff08CF\uff09\u6846\u67b6\u6269\u5c55\u81f3\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u8bbe\u7f6e\u3002", "\u63d0\u51fa\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u7684\u516c\u5e73\u6027\u76f8\u5173\u5dee\u8ddd\u6765\u5ba1\u8ba1\u8054\u90a6\u6a21\u578b\u516c\u5e73\u6027\u7684\u65b9\u6cd5\u3002", "\u901a\u8fc7\u5728\u591a\u4e2a\u9886\u57df\u548c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u5b9e\u9a8c\uff0c\u5145\u5206\u5229\u7528\u53ef\u4ea4\u6362\u6027\u5047\u8bbe\uff0c\u6765\u9a8c\u8bc1\u6240\u63d0\u51fa\u7684\u6846\u67b6\u3002"], "result": "\u901a\u8fc7\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\uff0c\u7ecf\u9a8c\u6027\u5730\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u8fdb\u884c\u516c\u5e73\u6027\u5ba1\u8ba1\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5c06\u5171\u5f62\u516c\u5e73\uff08CF\uff09\u6846\u67b6\u5e94\u7528\u4e8e\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u573a\u666f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u7684\u516c\u5e73\u6027\u76f8\u5173\u5dee\u8ddd\u6765\u5ba1\u8ba1\u8054\u90a6\u6a21\u578b\u516c\u5e73\u6027\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u5df2\u901a\u8fc7\u5b9e\u9a8c\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2509.23921", "pdf": "https://arxiv.org/pdf/2509.23921", "abs": "https://arxiv.org/abs/2509.23921", "authors": ["Jo\u00e3o Paulo P. G. Marques", "Catherine Rosenberg"], "title": "Performance Analysis of Zero-Forcing Beamforming Strategies for the Uplink of an MU-MIMO System with Multi-Antenna Users", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "We conduct a comprehensive evaluation of the performance of the uplink of\nOFDMA-based MU-MIMO systems with multi-antenna users, for three Zero-Forcing\n(ZF) Beamforming (BF) strategies: Coordinated-Transmit-Receive-1 (CTR1), where\nonly the strongest data stream is enabled per scheduled user; Block\nDiagonalization (BD), where all possible streams are enabled per scheduled\nuser; Coordinated-Transmit-Receive-Flexible (CTRF), which allows a flexible\nstream allocation per user. The Radio Resource Management (RRM) of the uplink\nof all OFDMA-based systems must be done over an entire Time-Slot (TS) due to\npower management, making it challenging. To enable this study, we propose an\nefficient heuristic based on greedy-up searches for stream-sets that provides\nfeasible solutions. It operates over the TS and considers fairness, practical\nModulation and Coding Schemes and all RRM processes. The results show that, for\nRural Macro scenarios, BD (resp. CTR1) could replace the more complex CTRF if\nthe number of users is small (resp. large), while for Urban Macro scenarios,\nCTR1 emerges as an alternative to CTRF due to its similar performance. We also\nshow that the system parameters can substantially impact the performance of the\nZF strategies and that BD performance is more impaired with a simpler power\nmanagement scheme than CTR1 and CTRF.", "AI": {"tldr": "\u8bc4\u4f30OFDMA MU-MIMO\u4e0a\u884c\u94fe\u8def\u4e2d\u4e09\u79cdZF\u6ce2\u675f\u6210\u5f62\u7b56\u7565\uff08CTR1, BD, CTRF\uff09\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u542f\u53d1\u5f0f\u65b9\u6cd5\u8fdb\u884c\u8d44\u6e90\u7ba1\u7406\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6700\u4f73\u7b56\u7565\u9009\u62e9\u53d6\u51b3\u4e8e\u573a\u666f\u548c\u7528\u6237\u6570\u91cf\uff0c\u4e14\u7cfb\u7edf\u53c2\u6570\u548c\u529f\u7387\u7ba1\u7406\u65b9\u6848\u5f71\u54cd\u663e\u8457\u3002", "motivation": "\u65e8\u5728\u5168\u9762\u8bc4\u4f30OFDMA MU-MIMO\u4e0a\u884c\u94fe\u8def\u4e2d\u591a\u5929\u7ebf\u7528\u6237\u7684\u6027\u80fd\uff0c\u7279\u522b\u5173\u6ce8CTR1\u3001BD\u548cCTRF\u4e09\u79cd\u96f6\u5f3a\u5236\uff08ZF\uff09\u6ce2\u675f\u6210\u5f62\u7b56\u7565\uff0c\u5e76\u5e94\u5bf9\u4e0a\u884c\u94fe\u8def\u65f6\u95f4\u69fd\uff08TS\uff09\u5185\u65e0\u7ebf\u8d44\u6e90\u7ba1\u7406\uff08RRM\uff09\u56e0\u529f\u8017\u7ba1\u7406\u800c\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u6bd4\u8f83CTR1\u3001BD\u548cCTRF\u4e09\u79cdZF\u6ce2\u675f\u6210\u5f62\u7b56\u7565\u5728OFDMA MU-MIMO\u4e0a\u884c\u94fe\u8def\u4e2d\u7684\u6027\u80fd\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8d2a\u5a6a\u641c\u7d22\uff08greedy-up searches\uff09\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u9ad8\u6548\u5206\u914d\u6d41\u96c6\u5408\uff08stream-sets\uff09\uff0c\u8be5\u65b9\u6cd5\u5728\u6574\u4e2a\u65f6\u95f4\u69fd\uff08TS\uff09\u4e0a\u8fd0\u884c\uff0c\u5e76\u8003\u8651\u516c\u5e73\u6027\u3001\u5b9e\u9645\u7684\u8c03\u5236\u7f16\u7801\u65b9\u6848\uff08MCS\uff09\u548c\u6240\u6709RRM\u8fc7\u7a0b\u3002", "result": ["\u5728\u519c\u6751\u5b8f\u8702\u7a9d\u573a\u666f\u4e2d\uff0c\u5f53\u7528\u6237\u6570\u91cf\u8f83\u5c11\u65f6\uff0cBD\u53ef\u66ff\u4ee3\u66f4\u590d\u6742\u7684CTRF\uff1b\u5f53\u7528\u6237\u6570\u91cf\u8f83\u591a\u65f6\uff0cCTR1\u53ef\u66ff\u4ee3CTRF\u3002", "\u5728\u57ce\u5e02\u5b8f\u8702\u7a9d\u573a\u666f\u4e2d\uff0cCTR1\u7684\u6027\u80fd\u4e0eCTRF\u76f8\u4f3c\uff0c\u53ef\u4f5c\u4e3a\u5176\u66ff\u4ee3\u65b9\u6848\u3002", "\u7cfb\u7edf\u53c2\u6570\u5bf9ZF\u7b56\u7565\u7684\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "\u5728\u66f4\u7b80\u5355\u7684\u529f\u7387\u7ba1\u7406\u65b9\u6848\u4e0b\uff0cBD\u7684\u6027\u80fd\u53d7\u635f\u7a0b\u5ea6\u9ad8\u4e8eCTR1\u548cCTRF\u3002"], "conclusion": "\u5728OFDMA MU-MIMO\u4e0a\u884c\u94fe\u8def\u4e2d\uff0cZF\u6ce2\u675f\u6210\u5f62\u7b56\u7565\u7684\u9009\u62e9\u5e94\u6839\u636e\u5177\u4f53\u573a\u666f\uff08\u519c\u6751/\u57ce\u5e02\u5b8f\u8702\u7a9d\uff09\u548c\u7528\u6237\u6570\u91cf\u8fdb\u884c\u8c03\u6574\u3002CTR1\u5728\u57ce\u5e02\u573a\u666f\u548c\u7528\u6237\u6570\u8f83\u591a\u7684\u519c\u6751\u573a\u666f\u4e2d\u5e38\u4f5c\u4e3aCTRF\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u800cBD\u9002\u7528\u4e8e\u7528\u6237\u6570\u8f83\u5c11\u7684\u519c\u6751\u573a\u666f\u3002\u6b64\u5916\uff0c\u7cfb\u7edf\u53c2\u6570\u548c\u529f\u7387\u7ba1\u7406\u65b9\u6848\u5bf9\u7b56\u7565\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2509.22812", "pdf": "https://arxiv.org/pdf/2509.22812", "abs": "https://arxiv.org/abs/2509.22812", "authors": ["Kai Zhang", "Christopher Malon", "Lichao Sun", "Martin Renqiang Min"], "title": "EditGRPO: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest X-Ray Report Generation", "categories": ["cs.CL"], "comment": null, "summary": "Radiology report generation requires advanced medical image analysis,\neffective temporal reasoning, and accurate text generation. Although recent\ninnovations, particularly multimodal large language models (MLLMs), have shown\nimproved performance, their supervised fine-tuning (SFT) objective is not\nexplicitly aligned with clinical efficacy. In this work, we introduce EditGRPO,\na mixed-policy reinforcement learning (RL) algorithm designed specifically to\noptimize the generation through clinically motivated rewards. EditGRPO\nintegrates on-policy exploration with off-policy guidance by injecting\nsentence-level detailed corrections during training rollouts. This mixed-policy\napproach addresses the exploration dilemma and sampling efficiency issues\ntypically encountered in RL. Applied to a Qwen2.5-VL-3B MLLM initialized with\nsupervised fine-tuning (SFT), EditGRPO outperforms both SFT and vanilla GRPO\nbaselines, achieving an average improvement of 3.4% in CheXbert, GREEN,\nRadgraph, and RATEScore metrics across four major chest X-ray report generation\ndatasets. Notably, EditGRPO also demonstrates superior out-of-domain\ngeneralization, with an average performance gain of 5.9% on unseen datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEditGRPO\uff0c\u4e00\u79cd\u6df7\u5408\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e34\u5e8a\u9a71\u52a8\u5956\u52b1\u4f18\u5316\u653e\u5c04\u62a5\u544a\u751f\u6210\uff0c\u5728SFT\u548cGRPO\u57fa\u7ebf\u4e4b\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5177\u6709\u4f18\u8d8a\u7684\u57df\u5916\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u653e\u5c04\u62a5\u544a\u751f\u6210\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u76ee\u6807\u5e76\u672a\u660e\u786e\u4e0e\u4e34\u5e8a\u7597\u6548\u5bf9\u9f50\u3002", "method": "\u5f15\u5165EditGRPO\uff0c\u4e00\u79cd\u6df7\u5408\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u901a\u8fc7\u5728\u8bad\u7ec3rollout\u671f\u95f4\u6ce8\u5165\u53e5\u5b50\u7ea7\u8be6\u7ec6\u4fee\u6b63\uff0c\u5c06\u5728\u7ebf\u7b56\u7565\u63a2\u7d22\u4e0e\u79bb\u7ebf\u7b56\u7565\u6307\u5bfc\u76f8\u7ed3\u5408\uff0c\u65e8\u5728\u901a\u8fc7\u4e34\u5e8a\u52a8\u673a\u5956\u52b1\u6765\u4f18\u5316\u751f\u6210\u3002", "result": "EditGRPO\u5728\u56db\u4e2a\u4e3b\u8981\u80f8\u90e8X\u5149\u62a5\u544a\u751f\u6210\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4SFT\u548c\u666e\u901aGRPO\u57fa\u7ebf\uff0c\u5728CheXbert\u3001GREEN\u3001Radgraph\u548cRATEScore\u7b49\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e863.4%\u3002\u6b64\u5916\uff0c\u5b83\u5728\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u4e0a\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u57df\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u53475.9%\u3002", "conclusion": "EditGRPO\u901a\u8fc7\u4e34\u5e8a\u52a8\u673a\u5956\u52b1\u663e\u8457\u6539\u5584\u4e86\u653e\u5c04\u62a5\u544a\u7684\u751f\u6210\u8d28\u91cf\u548c\u4e34\u5e8a\u5bf9\u9f50\u6027\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.23045", "pdf": "https://arxiv.org/pdf/2509.23045", "abs": "https://arxiv.org/abs/2509.23045", "authors": ["Zonghan Yang", "Shengjie Wang", "Kelin Fu", "Wenyang He", "Weimin Xiong", "Yibo Liu", "Yibo Miao", "Bofei Gao", "Yejie Wang", "Yingwei Ma", "Yanhao Li", "Yue Liu", "Zhenxing Hu", "Kaitai Zhang", "Shuyi Wang", "Huarong Chen", "Flood Sung", "Yang Liu", "Yang Gao", "Zhilin Yang", "Tianyu Liu"], "title": "Kimi-Dev: Agentless Training as Skill Prior for SWE-Agents", "categories": ["cs.AI", "cs.CL", "cs.SE"], "comment": "58 pages", "summary": "Large Language Models (LLMs) are increasingly applied to software engineering\n(SWE), with SWE-bench as a key benchmark. Solutions are split into SWE-Agent\nframeworks with multi-turn interactions and workflow-based Agentless methods\nwith single-turn verifiable steps. We argue these paradigms are not mutually\nexclusive: reasoning-intensive Agentless training induces skill priors,\nincluding localization, code edit, and self-reflection that enable efficient\nand effective SWE-Agent adaptation. In this work, we first curate the Agentless\ntraining recipe and present Kimi-Dev, an open-source SWE LLM achieving 60.4\\%\non SWE-bench Verified, the best among workflow approaches. With additional SFT\nadaptation on 5k publicly-available trajectories, Kimi-Dev powers SWE-Agents to\n48.6\\% pass@1, on par with that of Claude 3.5 Sonnet (241022 version). These\nresults show that structured skill priors from Agentless training can bridge\nworkflow and agentic frameworks for transferable coding agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAgentless\u8bad\u7ec3\u53ef\u4ee5\u4f5c\u4e3aSWB-Agent\u7684\u6709\u6548\u5148\u9a8c\u77e5\u8bc6\uff0c\u5e76\u53d1\u5e03\u4e86Kimi-Dev\uff0c\u4e00\u4e2a\u5728SWE-bench\u4e0a\u8868\u73b0\u51fa\u8272\u7684\u5f00\u6e90LLM\uff0c\u5b83\u7ed3\u5408\u4e86Agentless\u8bad\u7ec3\u548cAgent\u9002\u5e94\u6027\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u9886\u5148\u7684SWE-Agent\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\uff08SWE\uff09\u4e2d\u7684\u5e94\u7528\u5206\u4e3a\u591a\u8f6e\u4ea4\u4e92\u7684SWE-Agent\u6846\u67b6\u548c\u5355\u8f6e\u53ef\u9a8c\u8bc1\u7684Agentless\u65b9\u6cd5\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e24\u79cd\u8303\u5f0f\u5e76\u975e\u4e92\u65a5\uff0c\u901a\u8fc7\u63a8\u7406\u5bc6\u96c6\u7684Agentless\u8bad\u7ec3\u53ef\u4ee5\u8bf1\u5bfc\u6280\u80fd\u5148\u9a8c\uff0c\u4ece\u800c\u63d0\u9ad8SWE-Agent\u7684\u6548\u7387\u548c\u6548\u679c\u3002", "method": "\u7814\u7a76\u9996\u5148\u6574\u7406\u4e86Agentless\u8bad\u7ec3\u914d\u65b9\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63a8\u51fa\u4e86\u5f00\u6e90\u7684SWE LLM Kimi-Dev\u3002\u968f\u540e\uff0c\u901a\u8fc7\u5bf95k\u4e2a\u516c\u5f00\u8f68\u8ff9\u8fdb\u884c\u989d\u5916\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u5c06Kimi-Dev\u5e94\u7528\u4e8eSWE-Agents\u7684\u9002\u5e94\u6027\u8bad\u7ec3\u3002", "result": "Kimi-Dev\u5728SWE-bench Verified\u4e0a\u53d6\u5f97\u4e8660.4%\u7684\u6210\u7ee9\uff0c\u662f\u5de5\u4f5c\u6d41\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\u7684\u3002\u901a\u8fc7\u989d\u5916\u7684SFT\u9002\u5e94\uff0cKimi-Dev\u9a71\u52a8\u7684SWE-Agents\u8fbe\u5230\u4e8648.6%\u7684pass@1\uff0c\u4e0eClaude 3.5 Sonnet\uff08241022\u7248\u672c\uff09\u7684\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6765\u81eaAgentless\u8bad\u7ec3\u7684\u7ed3\u6784\u5316\u6280\u80fd\u5148\u9a8c\u53ef\u4ee5\u5f25\u5408\u5de5\u4f5c\u6d41\u548cAgentic\u6846\u67b6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u8fc1\u79fb\u7684\u7f16\u7a0bAgent\u3002"}}
{"id": "2509.22719", "pdf": "https://arxiv.org/pdf/2509.22719", "abs": "https://arxiv.org/abs/2509.22719", "authors": ["Adithya Giri"], "title": "IBiT: Utilizing Inductive Biases to Create a More Data Efficient Attention Mechanism", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "In recent years, Transformer-based architectures have become the dominant\nmethod for Computer Vision applications. While Transformers are explainable and\nscale well with dataset size, they lack the inductive biases of Convolutional\nNeural Networks. While these biases may be learned on large datasets, we show\nthat introducing these inductive biases through learned masks allow Vision\nTransformers to learn on much smaller datasets without Knowledge Distillation.\nThese Transformers, which we call Inductively Biased Image Transformers (IBiT),\nare significantly more accurate on small datasets, while retaining the\nexplainability Transformers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aIBiT\u7684\u65b0\u578bVision Transformer\uff0c\u901a\u8fc7\u5f15\u5165\u5b66\u4e60\u63a9\u7801\u6765\u878d\u5165\u5f52\u7eb3\u504f\u7f6e\uff0c\u4f7f\u5176\u80fd\u5728\u5c0f\u578b\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u663e\u8457\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "Transformer\u67b6\u6784\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u7f3a\u4e4f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5bfc\u81f4\u5176\u5728\u5c0f\u578b\u6570\u636e\u96c6\u4e0a\u5b66\u4e60\u6548\u7387\u4f4e\u4e0b\uff0c\u901a\u5e38\u9700\u8981\u5927\u91cf\u6570\u636e\u6216\u77e5\u8bc6\u84b8\u998f\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u201c\u5b66\u4e60\u63a9\u7801\u201d\u7684\u65b9\u5f0f\uff0c\u4e3aVision Transformers\u6ce8\u5165\u5f52\u7eb3\u504f\u7f6e\u3002\u8fd9\u4e9b\u7ecf\u8fc7\u6539\u8fdb\u7684Transformer\u88ab\u547d\u540d\u4e3a\u201c\u5f52\u7eb3\u504f\u7f6e\u56fe\u50cfTransformer\u201d\uff08IBiT\uff09\u3002", "result": "IBiT\u6a21\u578b\u5728\u5c0f\u578b\u6570\u636e\u96c6\u4e0a\u7684\u51c6\u786e\u6027\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "IBiT\u901a\u8fc7\u5b66\u4e60\u63a9\u7801\u6210\u529f\u5730\u5728Vision Transformers\u4e2d\u5f15\u5165\u4e86\u5f52\u7eb3\u504f\u7f6e\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u5c0f\u578b\u6570\u636e\u96c6\u4e0a\u66f4\u6709\u6548\u5730\u5b66\u4e60\u5e76\u53d6\u5f97\u66f4\u9ad8\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u4e86Transformer\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.22913", "pdf": "https://arxiv.org/pdf/2509.22913", "abs": "https://arxiv.org/abs/2509.22913", "authors": ["Jake S. Rhodes", "Adam G. Rustad", "Marshall S. Nielsen", "Morgan Chase McClellan", "Dallan Gardner", "Dawson Hedges"], "title": "Guided Manifold Alignment with Geometry-Regularized Twin Autoencoders", "categories": ["cs.LG", "stat.ML"], "comment": "10 pages, 4 figures, 7 tables. Accepted at the MMAI workshop at ICDM,\n  2025", "summary": "Manifold alignment (MA) involves a set of techniques for learning shared\nrepresentations across domains, yet many traditional MA methods are incapable\nof performing out-of-sample extension, limiting their real-world applicability.\nWe propose a guided representation learning framework leveraging a\ngeometry-regularized twin autoencoder (AE) architecture to enhance MA while\nenabling generalization to unseen data. Our method enforces structured\ncross-modal mappings to maintain geometric fidelity in learned embeddings. By\nincorporating a pre-trained alignment model and a multitask learning\nformulation, we improve cross-domain generalization and representation\nrobustness while maintaining alignment fidelity. We evaluate our approach using\nseveral MA methods, showing improvements in embedding consistency, information\npreservation, and cross-domain transfer. Additionally, we apply our framework\nto Alzheimer's disease diagnosis, demonstrating its ability to integrate\nmulti-modal patient data and enhance predictive accuracy in cases limited to a\nsingle domain by leveraging insights from the multi-modal problem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u6b63\u5219\u5316\u53cc\u751f\u81ea\u7f16\u7801\u5668\u7684\u5f15\u5bfc\u5f0f\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3a\u6d41\u5f62\u5bf9\u9f50\uff08MA\uff09\u7684\u6837\u672c\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u548c\u75be\u75c5\u8bca\u65ad\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u8bb8\u591a\u4f20\u7edf\u6d41\u5f62\u5bf9\u9f50\uff08MA\uff09\u65b9\u6cd5\u65e0\u6cd5\u8fdb\u884c\u6837\u672c\u5916\u6269\u5c55\uff08out-of-sample extension\uff09\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5f15\u5bfc\u5f0f\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u51e0\u4f55\u6b63\u5219\u5316\u7684\u53cc\u751f\u81ea\u7f16\u7801\u5668\uff08AE\uff09\u67b6\u6784\u3002\u65b9\u6cd5\u901a\u8fc7\u5f3a\u5236\u7ed3\u6784\u5316\u8de8\u6a21\u6001\u6620\u5c04\u6765\u4fdd\u6301\u5b66\u4e60\u5d4c\u5165\u7684\u51e0\u4f55\u4fdd\u771f\u5ea6\uff0c\u5e76\u7ed3\u5408\u9884\u8bad\u7ec3\u5bf9\u9f50\u6a21\u578b\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u516c\u5f0f\uff0c\u4ee5\u63d0\u9ad8\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u548c\u8868\u793a\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5d4c\u5165\u4e00\u81f4\u6027\u3001\u4fe1\u606f\u4fdd\u6301\u548c\u8de8\u57df\u8fc1\u79fb\u65b9\u9762\u5747\u6709\u6240\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8bca\u65ad\u65f6\uff0c\u80fd\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u60a3\u8005\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u5229\u7528\u591a\u6a21\u6001\u95ee\u9898\u4e2d\u7684\u89c1\u89e3\uff0c\u63d0\u9ad8\u5355\u57df\u573a\u666f\u4e0b\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6d41\u5f62\u5bf9\u9f50\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u589e\u5f3a\u8de8\u57df\u6cdb\u5316\u548c\u8868\u793a\u9c81\u68d2\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u548c\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u7279\u522b\u5728\u533b\u5b66\u8bca\u65ad\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2509.24038", "pdf": "https://arxiv.org/pdf/2509.24038", "abs": "https://arxiv.org/abs/2509.24038", "authors": ["Toru Mano", "Hideki Nishizawa", "Takeo Sasai", "Soichiroh Usui", "Dmitrii Briantcev", "Devika Dass", "Brandt Bashaw", "Eoin Kenny", "Marco Ruffini", "Yoshiaki Sone", "Koichi Takasugi", "Daniel Kilper"], "title": "Beyond Redundancy: Toward Agile Resilience in Optical Networks to Overcome Unpredictable Disasters", "categories": ["cs.NI"], "comment": null, "summary": "Resilience in optical networks has traditionally relied on redundancy and\npre-planned recovery strategies, both of which assume a certain level of\ndisaster predictability. However, recent environmental changes such as climate\nshifts, the evolution of communication services, and rising geopolitical risks\nhave increased the unpredictability of disasters, reducing the effectiveness of\nconventional resilience approaches. To address this unpredictability, this\npaper introduces the concept of agile resilience, which emphasizes dynamic\nadaptability across multiple operators and layers. We identify key requirements\nand challenges, and present enabling technologies for the realization of agile\nresilience. Using a field-deployed transmission system, we demonstrate rapid\nsystem characterization, optical path provisioning, and database migration\nwithin six hours. These results validate the effectiveness of the proposed\nenabling technologies and confirm the feasibility of agile resilience.", "AI": {"tldr": "\u9488\u5bf9\u65e5\u76ca\u589e\u52a0\u7684\u707e\u5bb3\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u201c\u654f\u6377\u5f39\u6027\u201d\u6982\u5ff5\uff0c\u5f3a\u8c03\u5149\u7f51\u7edc\u7684\u52a8\u6001\u9002\u5e94\u6027\uff0c\u5e76\u5229\u7528\u73b0\u573a\u90e8\u7f72\u7cfb\u7edf\u9a8c\u8bc1\u4e86\u5176\u8d4b\u80fd\u6280\u672f\u5728\u516d\u5c0f\u65f6\u5185\u5b9e\u73b0\u5feb\u901f\u7cfb\u7edf\u64cd\u4f5c\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u4f20\u7edf\u5149\u7f51\u7edc\u5f39\u6027\u4f9d\u8d56\u4e8e\u5197\u4f59\u548c\u9884\u8ba1\u5212\u6062\u590d\u7b56\u7565\uff0c\u4f46\u8fd1\u5e74\u6765\u6c14\u5019\u53d8\u5316\u3001\u670d\u52a1\u6f14\u8fdb\u548c\u5730\u7f18\u653f\u6cbb\u98ce\u9669\u5bfc\u81f4\u707e\u5bb3\u4e0d\u53ef\u9884\u6d4b\u6027\u589e\u52a0\uff0c\u964d\u4f4e\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u5f15\u5165\u201c\u654f\u6377\u5f39\u6027\u201d\u6982\u5ff5\uff0c\u5f3a\u8c03\u8de8\u591a\u4e2a\u8fd0\u8425\u5546\u548c\u5c42\u9762\u7684\u52a8\u6001\u9002\u5e94\u6027\u3002\u8bc6\u522b\u4e86\u5173\u952e\u9700\u6c42\u3001\u6311\u6218\u548c\u5b9e\u73b0\u654f\u6377\u5f39\u6027\u7684\u8d4b\u80fd\u6280\u672f\u3002\u4f7f\u7528\u73b0\u573a\u90e8\u7f72\u7684\u4f20\u8f93\u7cfb\u7edf\u8fdb\u884c\u6f14\u793a\u548c\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u73b0\u573a\u90e8\u7f72\u7cfb\u7edf\uff0c\u5728\u516d\u5c0f\u65f6\u5185\u6210\u529f\u5c55\u793a\u4e86\u5feb\u901f\u7cfb\u7edf\u8868\u5f81\u3001\u5149\u8def\u914d\u7f6e\u548c\u6570\u636e\u5e93\u8fc1\u79fb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u8d4b\u80fd\u6280\u672f\u7684\u6709\u6548\u6027\uff0c\u5e76\u786e\u8ba4\u4e86\u654f\u6377\u5f39\u6027\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.22824", "pdf": "https://arxiv.org/pdf/2509.22824", "abs": "https://arxiv.org/abs/2509.22824", "authors": ["Chi Ruan", "Dongfu Jiang", "Yubo Wang", "Wenhu Chen"], "title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a popular training paradigm,\nparticularly when paired with reasoning models. While effective, it primarily\nfocuses on generating responses and lacks mechanisms to explicitly foster\ncritique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)\nand Critique-Guided-Distillation (CGD) have shown the benefits of explicitly\nteaching LLMs how to critique. Motivated by them, we propose Critique\nReinforcement Learning (CRL), where the model is tasked with generating a\ncritique for a given (question, solution) pair. The reward is determined solely\nby whether the final judgment label $c \\in \\{\\texttt{True}, \\texttt{False}\\}$\nof the generated critique aligns with the ground-truth judgment $c^*$. Building\non this point, we introduce \\textsc{Critique-Coder}, which is trained on a\nhybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL\ndata. We fine-tune multiple models (\\textsc{Critique-Coder}) and evaluate them\non different benchmarks to show their advantages over RL-only models. We show\nthat \\textsc{Critique-Coder} consistently outperforms RL-only baselines on all\nthe evaluated benchmarks. Notably, our \\textsc{Critique-Coder-8B} can reach\nover 60\\% on LiveCodeBench (v5), outperforming other reasoning models like\nDeepCoder-14B and GPT-o1. Beyond code generation, \\textsc{Critique-Coder} also\ndemonstrates enhanced general reasoning abilities, as evidenced by its better\nperformance on logic reasoning tasks from the BBEH dataset. This indicates that\nthe application of CRL on coding datasets enhances general reasoning and\ncritique abilities, which are transferable across a broad range of tasks.\nHence, we believe that CRL works as a great complement to standard RL for LLM\nreasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6279\u5224\u6027\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6279\u5224\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e3b\u8981\u5173\u6ce8\u54cd\u5e94\u751f\u6210\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u6279\u5224\u6216\u53cd\u601d\u673a\u5236\u3002\u8fd1\u671f\u7814\u7a76\uff08\u5982CFT\u3001CGD\uff09\u8868\u660e\u6559\u6388LLM\u6279\u5224\u80fd\u529b\u6709\u76ca\uff0c\u56e0\u6b64\u672c\u6587\u65e8\u5728\u5f25\u8865RL\u5728\u6279\u5224\u6027\u57f9\u517b\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u6279\u5224\u6027\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\uff0c\u6a21\u578b\u9700\u4e3a\u7ed9\u5b9a\uff08\u95ee\u9898\uff0c\u89e3\u51b3\u65b9\u6848\uff09\u5bf9\u751f\u6210\u6279\u5224\u3002\u5956\u52b1\u57fa\u4e8e\u751f\u6210\u6279\u5224\u7684\u6700\u7ec8\u5224\u65ad\uff08\u771f/\u5047\uff09\u662f\u5426\u4e0e\u771f\u5b9e\u5224\u65ad\u4e00\u81f4\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\\textsc{Critique-Coder}\u6a21\u578b\uff0c\u901a\u8fc7\u5c0620%\u7684\u6807\u51c6RL\u6570\u636e\u66ff\u6362\u4e3aCRL\u6570\u636e\u8fdb\u884c\u6df7\u5408\u8bad\u7ec3\u548c\u5fae\u8c03\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\\textsc{Critique-Coder}\u5728\u6240\u6709\u8bc4\u4f30\u57fa\u51c6\u4e0a\u5747\u6301\u7eed\u4f18\u4e8e\u4ec5\u4f7f\u7528RL\u7684\u57fa\u7ebf\u6a21\u578b\u3002\u5176\u4e2d\uff0c\\textsc{Critique-Coder-8B}\u5728LiveCodeBench (v5)\u4e0a\u8fbe\u523060%\u4ee5\u4e0a\uff0c\u8d85\u8d8aDeepCoder-14B\u548cGPT-o1\u7b49\u5176\u4ed6\u63a8\u7406\u6a21\u578b\u3002\u6b64\u5916\uff0c\\textsc{Critique-Coder}\u5728BBEH\u6570\u636e\u96c6\u7684\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u8868\u660eCRL\u5bf9\u7f16\u7801\u6570\u636e\u96c6\u7684\u5e94\u7528\u80fd\u589e\u5f3a\u901a\u7528\u63a8\u7406\u548c\u6279\u5224\u80fd\u529b\uff0c\u4e14\u5177\u6709\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "\u6279\u5224\u6027\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\u662f\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u7684\u6709\u6548\u8865\u5145\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u589e\u5f3a\u5176\u5728\u7f16\u7801\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6279\u5224\u4e0e\u53cd\u601d\u80fd\u529b\u3002"}}
{"id": "2509.23058", "pdf": "https://arxiv.org/pdf/2509.23058", "abs": "https://arxiv.org/abs/2509.23058", "authors": ["Yikai Wang", "Xiaocheng Li", "Guanting Chen"], "title": "Risk Profiling and Modulation for LLMs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are increasingly used for decision-making tasks\nunder uncertainty; however, their risk profiles and how they are influenced by\nprompting and alignment methods remain underexplored. Existing studies have\nprimarily examined personality prompting or multi-agent interactions, leaving\nopen the question of how post-training influences the risk behavior of LLMs. In\nthis work, we propose a new pipeline for eliciting, steering, and modulating\nLLMs' risk profiles, drawing on tools from behavioral economics and finance.\nUsing utility-theoretic models, we compare pre-trained, instruction-tuned, and\nRLHF-aligned LLMs, and find that while instruction-tuned models exhibit\nbehaviors consistent with some standard utility formulations, pre-trained and\nRLHF-aligned models deviate more from any utility models fitted. We further\nevaluate modulation strategies, including prompt engineering, in-context\nlearning, and post-training, and show that post-training provides the most\nstable and effective modulation of risk preference. Our findings provide\ninsights into the risk profiles of different classes and stages of LLMs and\ndemonstrate how post-training modulates these profiles, laying the groundwork\nfor future research on behavioral alignment and risk-aware LLM design.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLMs\u7684\u98ce\u9669\u504f\u597d\uff0c\u53d1\u73b0\u540e\u8bad\u7ec3\u662f\u8c03\u8282\u98ce\u9669\u6700\u7a33\u5b9a\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u6a21\u578b\u98ce\u9669\u884c\u4e3a\u7684\u5dee\u5f02\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u4efb\u52a1\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u98ce\u9669\u504f\u597d\u53ca\u5176\u53d7\u63d0\u793a\u548c\u5bf9\u9f50\u65b9\u6cd5\u5f71\u54cd\u7684\u673a\u5236\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u5c24\u5176\u7f3a\u4e4f\u5bf9\u540e\u8bad\u7ec3\u5f71\u54cdLLM\u98ce\u9669\u884c\u4e3a\u7684\u7814\u7a76\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u884c\u4e3a\u7ecf\u6d4e\u5b66\u548c\u91d1\u878d\u5b66\u5de5\u5177\u7684\u65b0\u6d41\u7a0b\uff0c\u7528\u4e8e\u8bf1\u5bfc\u3001\u5f15\u5bfc\u548c\u8c03\u8282LLMs\u7684\u98ce\u9669\u504f\u597d\u3002\u7814\u7a76\u901a\u8fc7\u6548\u7528\u7406\u8bba\u6a21\u578b\u6bd4\u8f83\u4e86\u9884\u8bad\u7ec3\u3001\u6307\u4ee4\u5fae\u8c03\u548cRLHF\u5bf9\u9f50\u7684LLMs\uff0c\u5e76\u8bc4\u4f30\u4e86\u63d0\u793a\u5de5\u7a0b\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u540e\u8bad\u7ec3\u7b49\u591a\u79cd\u8c03\u8282\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u884c\u4e3a\u4e0e\u6807\u51c6\u6548\u7528\u516c\u5f0f\u4e00\u81f4\uff0c\u800c\u9884\u8bad\u7ec3\u548cRLHF\u5bf9\u9f50\u6a21\u578b\u4e0e\u4efb\u4f55\u62df\u5408\u7684\u6548\u7528\u6a21\u578b\u504f\u79bb\u66f4\u5927\u3002\u5728\u6240\u6709\u8c03\u8282\u7b56\u7565\u4e2d\uff0c\u540e\u8bad\u7ec3\u88ab\u8bc1\u660e\u662f\u8c03\u8282\u98ce\u9669\u504f\u597d\u6700\u7a33\u5b9a\u548c\u6709\u6548\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u6df1\u5165\u63ed\u793a\u4e86\u4e0d\u540c\u7c7b\u522b\u548c\u8bad\u7ec3\u9636\u6bb5LLMs\u7684\u98ce\u9669\u504f\u597d\uff0c\u5e76\u5c55\u793a\u4e86\u540e\u8bad\u7ec3\u5982\u4f55\u6709\u6548\u5730\u8c03\u8282\u8fd9\u4e9b\u504f\u597d\uff0c\u4e3a\u672a\u6765LLM\u7684\u884c\u4e3a\u5bf9\u9f50\u548c\u98ce\u9669\u611f\u77e5\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
