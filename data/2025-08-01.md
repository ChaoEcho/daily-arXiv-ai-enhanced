<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 9]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.NI](#cs.NI) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Large Language Models in the Travel Domain: An Industrial Experience](https://arxiv.org/abs/2507.22910)
*Sergio Di Meglio,Aniello Somma,Luigi Libero Lucio Starace,Fabio Scippacercola,Giancarlo Sperlì,Sergio Di Martino*

Main category: cs.CL

TL;DR: 针对在线住宿预订平台数据不一致问题，本文在FERVENTO的CALEIDOHOTELS平台整合并评估了Mistral 7B和Mixtral 8x7B，Mixtral 8x7B表现更优但计算成本更高，提供了模型质量与资源效率权衡的实践洞察。


<details>
  <summary>Details</summary>
Motivation: 在线住宿预订平台严重依赖第三方数据，但这些数据常不完整或不一致，导致用户体验差和并可能造成市场损失。

Method: 将大型语言模型（LLMs）整合到FERVENTO开发的CALEIDOHOTELS预订平台。评估了使用QLoRA微调的Mistral 7B和使用优化系统提示的Mixtral 8x7B。评估标准包括生成一致且同质描述的能力以及幻觉率。

Result: Mixtral 8x7B在完整性（99.6% vs 93%）、准确性（98.8% vs 96%）和幻觉率（1.2% vs 4%）方面优于Mistral 7B，且生成内容更短更简洁（平均249字 vs 277字）。然而，Mixtral 8x7B的计算成本显著高于Mistral 7B（50GB VRAM和$1.61/小时 vs 5GB和$0.16/小时）。

Conclusion: 本研究提供了模型质量与资源效率之间权衡的实用见解，为在生产环境中部署LLM提供了指导，并证明了LLM在提高住宿数据一致性和可靠性方面的有效性。

Abstract: Online property booking platforms are widely used and rely heavily on
consistent, up-to-date information about accommodation facilities, often
sourced from third-party providers. However, these external data sources are
frequently affected by incomplete or inconsistent details, which can frustrate
users and result in a loss of market. In response to these challenges, we
present an industrial case study involving the integration of Large Language
Models (LLMs) into CALEIDOHOTELS, a property reservation platform developed by
FERVENTO. We evaluate two well-known LLMs in this context: Mistral 7B,
fine-tuned with QLoRA, and Mixtral 8x7B, utilized with a refined system prompt.
Both models were assessed based on their ability to generate consistent and
homogeneous descriptions while minimizing hallucinations. Mixtral 8x7B
outperformed Mistral 7B in terms of completeness (99.6% vs. 93%), precision
(98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), producing shorter yet
more concise content (249 vs. 277 words on average). However, this came at a
significantly higher computational cost: 50GB VRAM and $1.61/hour versus 5GB
and $0.16/hour for Mistral 7B. Our findings provide practical insights into the
trade-offs between model quality and resource efficiency, offering guidance for
deploying LLMs in production environments and demonstrating their effectiveness
in enhancing the consistency and reliability of accommodation data.

</details>


### [2] [ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing](https://arxiv.org/abs/2507.22911)
*Jinzhi Wang,Qingke Peng,Haozhou Li,Zeyuan Zeng,Qinfeng Song,Kaixuan Yang,Jiangbo Zhang,Yaoying Wang,Ruimeng Li,Biyi Zhou*

Main category: cs.CL

TL;DR: 该研究引入了ElectriQ，一个专为电力营销客服场景设计的LLM评估基准，并证明了经过领域特化和知识增强的小型LLM可以超越通用大模型。


<details>
  <summary>Details</summary>
Motivation: 当前的电力营销客服系统（如95598热线）存在响应慢、流程僵化和准确性不足的问题。虽然通用大型语言模型（LLMs）能力强大，但缺乏该领域的专业知识和同理心，因此需要专门的方法来弥补这一差距。

Method: 研究引入了ElectriQ基准，包含一个涵盖六大服务类别的对话数据集。同时，提出了专业性、受欢迎度、可读性和用户友好性四种新的评估指标。此外，还整合了领域知识库并提出了一种知识增强方法以提升模型性能。

Result: 通过对13个LLMs的实验表明，经过微调和知识增强的小型模型（如LLama3-8B）在专业性和用户友好性方面能够超越通用大模型（如GPT-4o）。

Conclusion: ElectriQ为开发和优化适用于电力营销服务的LLM建立了全面且坚实的基础。

Abstract: Electric power marketing customer service plays a critical role in addressing
inquiries, complaints, and service requests. However, current systems, such as
China's 95598 hotline, often struggle with slow response times, inflexible
procedures, and limited accuracy in domain-specific tasks. While large language
models (LLMs) like GPT-4o and Claude 3 demonstrate strong general capabilities,
they lack the domain expertise and empathy required in this field. To bridge
this gap, we introduce ElectriQ, the first benchmark designed to evaluate and
enhance LLMs in electric power marketing scenarios. ElectriQ consists of a
dialogue dataset covering six key service categories and introduces four
evaluation metrics: professionalism, popularity, readability, and
user-friendliness. We further incorporate a domain-specific knowledge base and
propose a knowledge augmentation method to boost model performance. Experiments
on 13 LLMs reveal that smaller models such as LLama3-8B, when fine-tuned and
augmented, can surpass GPT-4o in terms of professionalism and
user-friendliness. ElectriQ establishes a comprehensive foundation for
developing LLMs tailored to the needs of power marketing services.

</details>


### [3] [A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms](https://arxiv.org/abs/2507.22912)
*Navid Yazdanjue,Morteza Rakhshaninejad,Hossein Yazdanjouei,Mohammad Sadegh Khorshidi,Mikko S. Niemela,Fang Chen,Amir H. Gandomi*

Main category: cs.CL

TL;DR: 本文提出了一种分层分类框架，结合微调语言模型和半监督集成学习策略，以检测和分类跨平台（如暗网、Telegram）的非法市场内容，并表现出卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 非法市场正日益转向互联网的隐蔽部分（如深网、暗网、Telegram），进行毒品、武器和被盗凭证的匿名交易。由于标记数据有限、非法语言不断演变以及在线来源的结构异质性，检测和分类此类内容极具挑战性。

Method: 研究采用分层分类框架，结合微调的语言模型和半监督集成学习策略。具体方法包括：使用在领域特定数据（深网、暗网、Telegram、Reddit、Pastebin）上微调的ModernBERT模型提取语义表示，以捕捉专业术语和模糊语言模式；结合人工特征（文档结构、嵌入模式如比特币地址、电子邮件、IP和元数据）；分类流程分两阶段：第一阶段使用基于熵加权投票的XGBoost、Random Forest和SVM半监督集成模型检测销售相关文档；第二阶段将这些文档进一步分类为毒品、武器或凭证销售。

Result: 在包含多源语料库、DUTA和CoDA的三个数据集上进行的实验表明，所提出的模型优于包括BERT、ModernBERT、DarkBERT、ALBERT、Longformer和BigBird在内的多个基线模型。该模型实现了0.96489的准确率、0.93467的F1分数和0.95388的TMCC。

Conclusion: 该模型在有限监督下表现出强大的泛化能力和鲁棒性，并在实际非法内容检测中显示出有效性。

Abstract: Illegal marketplaces have increasingly shifted to concealed parts of the
internet, including the deep and dark web, as well as platforms such as
Telegram, Reddit, and Pastebin. These channels enable the anonymous trade of
illicit goods including drugs, weapons, and stolen credentials. Detecting and
categorizing such content remains challenging due to limited labeled data, the
evolving nature of illicit language, and the structural heterogeneity of online
sources. This paper presents a hierarchical classification framework that
combines fine-tuned language models with a semi-supervised ensemble learning
strategy to detect and classify illicit marketplace content across diverse
platforms. We extract semantic representations using ModernBERT, a transformer
model for long documents, finetuned on domain-specific data from deep and dark
web pages, Telegram channels, Subreddits, and Pastebin pastes to capture
specialized jargon and ambiguous linguistic patterns. In addition, we
incorporate manually engineered features such as document structure, embedded
patterns including Bitcoin addresses, emails, and IPs, and metadata, which
complement language model embeddings. The classification pipeline operates in
two stages. The first stage uses a semi-supervised ensemble of XGBoost, Random
Forest, and SVM with entropy-based weighted voting to detect sales-related
documents. The second stage further classifies these into drug, weapon, or
credential sales. Experiments on three datasets, including our multi-source
corpus, DUTA, and CoDA, show that our model outperforms several baselines,
including BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The
model achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of
0.95388, demonstrating strong generalization, robustness under limited
supervision, and effectiveness in real-world illicit content detection.

</details>


### [4] [A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models](https://arxiv.org/abs/2507.22913)
*Jinyu Liu,Xiaoying Song,Diana Zhang,Jason Thomale,Daqing He,Lingzi Hong*

Main category: cs.CL

TL;DR: 该研究提出了一种结合机器学习（ML）模型和大型语言模型（LLM）的混合框架，用于图书馆主题分析。该框架通过ML模型指导LLM预测并进行后编辑以缓解幻觉，实验结果显示能产生更受控且词汇对齐的输出。


<details>
  <summary>Details</summary>
Motivation: 图书馆管理系统急需高效的主题访问功能。传统机器学习（ML）模型在主题分析中处理未见案例时存在局限性。大型语言模型（LLM）在分类和摘要任务中表现突出，但其在主题分析方面的能力尚未充分探索，且常出现过度生成和幻觉问题。

Method: 本研究提出一种混合框架，整合了基于嵌入的机器学习（ML）模型与大型语言模型（LLM）。该方法利用ML模型进行两项关键任务：1) 预测最佳的LCSH（美国国会图书馆主题词）标签数量以指导LLM的生成；2) 对LLM预测的词汇进行后编辑，用实际的LCSH词汇修正以减轻幻觉。实验使用LCSH预测书籍的主题词，并与纯LLM方法进行对比。

Result: 实验结果表明，通过为LLM生成提供初始预测指导和施加后编辑，能够产生更受控且与既定词汇表（如LCSH）对齐的输出。

Conclusion: 该混合框架成功地解决了LLM在主题分析中过度生成和幻觉的问题，通过结合ML模型的指导和后编辑能力，实现了更准确、标准化且符合预定义词汇表的主题词预测，提升了信息资源的主题访问质量。

Abstract: Providing subject access to information resources is an essential function of
any library management system. Large language models (LLMs) have been widely
used in classification and summarization tasks, but their capability to perform
subject analysis is underexplored. Multi-label classification with traditional
machine learning (ML) models has been used for subject analysis but struggles
with unseen cases. LLMs offer an alternative but often over-generate and
hallucinate. Therefore, we propose a hybrid framework that integrates
embedding-based ML models with LLMs. This approach uses ML models to (1)
predict the optimal number of LCSH labels to guide LLM predictions and (2)
post-edit the predicted terms with actual LCSH terms to mitigate
hallucinations. We experimented with LLMs and the hybrid framework to predict
the subject terms of books using the Library of Congress Subject Headings
(LCSH). Experiment results show that providing initial predictions to guide LLM
generations and imposing post-edits result in more controlled and
vocabulary-aligned outputs.

</details>


### [5] [Full Triple Matcher: Integrating all triple elements between heterogeneous Knowledge Graphs](https://arxiv.org/abs/2507.22914)
*Victor Eiti Yamamoto,Hideaki Takeda*

Main category: cs.CL

TL;DR: 提出一种新的知识图谱集成方法，通过结合标签匹配和三元组匹配来解决现有方法在上下文匹配上的不足，并在多样化测试中展现出高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱集成方法侧重于模式和实体匹配，但忽视了上下文匹配。鉴于真实世界知识图谱来源、规模和信息密度的多样性，现有方法在处理复杂上下文集成时表现不足，且当前评估数据集未能充分反映这种复杂性。

Method: 提出一种新颖的知识图谱集成方法，包含标签匹配和三元组匹配。标签匹配利用字符串操作、模糊匹配和向量相似度技术。三元组匹配通过识别传达可比信息的三元组映射来提高实体匹配准确性。此外，引入了一个新的数据集以更全面地评估三元组匹配。

Result: 该方法在OAEI竞赛中与领先系统和监督方法相比，表现出具有竞争力的性能，并在多样化的测试案例中实现了高准确性。

Conclusion: 所提出的结合标签匹配和三元组匹配的知识图谱集成方法有效解决了上下文匹配的挑战，并能显著提高实体匹配的准确性，在知识图谱集成领域具有良好应用潜力。

Abstract: Knowledge graphs (KGs) are powerful tools for representing and reasoning over
structured information. Their main components include schema, identity, and
context. While schema and identity matching are well-established in ontology
and entity matching research, context matching remains largely unexplored. This
is particularly important because real-world KGs often vary significantly in
source, size, and information density - factors not typically represented in
the datasets on which current entity matching methods are evaluated. As a
result, existing approaches may fall short in scenarios where diverse and
complex contexts need to be integrated.
  To address this gap, we propose a novel KG integration method consisting of
label matching and triple matching. We use string manipulation, fuzzy matching,
and vector similarity techniques to align entity and predicate labels. Next, we
identify mappings between triples that convey comparable information, using
these mappings to improve entity-matching accuracy. Our approach demonstrates
competitive performance compared to leading systems in the OAEI competition and
against supervised methods, achieving high accuracy across diverse test cases.
Additionally, we introduce a new dataset derived from the benchmark dataset to
evaluate the triple-matching step more comprehensively.

</details>


### [6] [Theoretical Foundations and Mitigation of Hallucination in Large Language Models](https://arxiv.org/abs/2507.22915)
*Esmail Gumaan*

Main category: cs.CL

TL;DR: 论文对LLM幻觉进行了严谨处理，包括理论定义、风险分析、检测与缓解策略综述、统一工作流提议及评估协议，为解决该问题提供了理论基础和实践指南。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）生成不忠实于输入或现实内容（即幻觉）这一关键挑战。

Method: 1. 形式化定义幻觉（内在与外在），并定义幻觉风险。2. 利用学习理论框架（PAC-Bayes和Rademacher复杂度）推导风险界限。3. 综述幻觉检测策略（如不确定性估计、置信度校准、注意力对齐）。4. 综述幻觉缓解方法（如检索增强生成、微调、logit校准、事实核查）。5. 提出一个统一的检测与缓解工作流程。6. 概述幻觉评估协议（数据集、指标、实验设置）。

Result: 建立了幻觉的理论框架，包括形式化定义和风险分析；系统性地整理并分类了现有的检测与缓解策略；提出了一个统一的幻觉处理工作流程；为幻觉的量化和减少提供了评估方案。

Conclusion: 本工作为解决LLMs中的幻觉问题奠定了理论基础，并提供了实用的指导方针。

Abstract: Hallucination in Large Language Models (LLMs) refers to the generation of
content that is not faithful to the input or the real-world facts. This paper
provides a rigorous treatment of hallucination in LLMs, including formal
definitions and theoretical analyses. We distinguish between intrinsic and
extrinsic hallucinations, and define a \textit{hallucination risk} for models.
We derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes
and Rademacher complexity). We then survey detection strategies for
hallucinations, such as token-level uncertainty estimation, confidence
calibration, and attention alignment checks. On the mitigation side, we discuss
approaches including retrieval-augmented generation, hallucination-aware
fine-tuning, logit calibration, and the incorporation of fact-verification
modules. We propose a unified detection and mitigation workflow, illustrated
with a diagram, to integrate these strategies. Finally, we outline evaluation
protocols for hallucination, recommending datasets, metrics, and experimental
setups to quantify and reduce hallucinations. Our work lays a theoretical
foundation and practical guidelines for addressing the crucial challenge of
hallucination in LLMs.

</details>


### [7] [Reading Between the Timelines: RAG for Answering Diachronic Questions](https://arxiv.org/abs/2507.22917)
*Kwun Hang Lau,Ruiyuan Zhang,Weijie Shi,Xiaofang Zhou,Xiaojun Cheng*

Main category: cs.CL

TL;DR: 传统RAG在处理跨时间查询时存在不足。本文提出一种新的RAG框架，通过引入时间逻辑和专门的检索器，显著提升了长期查询的准确性，并发布了ADQAB评估基准。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在处理需要跨时间跟踪实体和现象的长期查询时存在关键缺陷，因为其语义驱动的检索方法无法获取既主题相关又时间连贯的证据。

Method: 提出一个重新设计RAG流程的新框架，融入时间逻辑。该方法将用户查询分解为核心主题和时间窗口，并使用专门的检索器平衡语义匹配与时间相关性，以收集连续的证据集。同时，引入ADQAB数据集进行评估。

Result: 在ADQAB基准测试中，本文方法在回答准确性方面取得显著提升，超越标准RAG实现13%至27%。

Conclusion: 该工作为RAG系统实现复杂、真实世界问题所需的细致、演进分析提供了一条有效途径。

Abstract: While Retrieval-Augmented Generation (RAG) excels at injecting static,
factual knowledge into Large Language Models (LLMs), it exhibits a critical
deficit in handling longitudinal queries that require tracking entities and
phenomena across time. This blind spot arises because conventional,
semantically-driven retrieval methods are not equipped to gather evidence that
is both topically relevant and temporally coherent for a specified duration. We
address this challenge by proposing a new framework that fundamentally
redesigns the RAG pipeline to infuse temporal logic. Our methodology begins by
disentangling a user's query into its core subject and its temporal window. It
then employs a specialized retriever that calibrates semantic matching against
temporal relevance, ensuring the collection of a contiguous evidence set that
spans the entire queried period. To enable rigorous evaluation of this
capability, we also introduce the Analytical Diachronic Question Answering
Benchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus
of real and synthetic financial news. Empirical results on ADQAB show that our
approach yields substantial gains in answer accuracy, surpassing standard RAG
implementations by 13% to 27%. This work provides a validated pathway toward
RAG systems capable of performing the nuanced, evolutionary analysis required
for complex, real-world questions. The dataset and code for this study are
publicly available at https://github.com/kwunhang/TA-RAG.

</details>


### [8] [Semantic Convergence: Investigating Shared Representations Across Scaled LLMs](https://arxiv.org/abs/2507.22918)
*Daniel Son,Sanjana Rathore,Andrew Rufail,Adrian Simon,Daniel Zhang,Soham Dave,Cole Blondin,Kevin Zhu,Sean O'Brien*

Main category: cs.CL

TL;DR: 研究Gemma-2系列不同规模模型（Gemma-2-2B和Gemma-2-9B）的特征通用性，发现中间层特征重叠度最高，支持跨模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究Gemma-2语言模型（Gemma-2-2B和Gemma-2-9B）的特征通用性，探讨规模相差四倍的模型是否仍能收敛于相似的内部概念。

Method: 使用稀疏自编码器（SAE）字典学习流程处理模型的残差流激活，通过激活相关性对单语义特征进行对齐，并使用SVCCA和RSA比较匹配的特征空间。初步实验还将分析扩展到多令牌子空间。

Result: 中间层产生了最强的特征重叠，而早期和晚期层显示出较少的相似性。初步实验表明，语义相似的多令牌子空间与语言模型的交互方式也相似。

Conclusion: 研究结果强化了大型语言模型即便规模不同也能学习到广泛相似且可解释特征的观点，从而巩固了通用性作为跨模型可解释性基础的地位。

Abstract: We investigate feature universality in Gemma-2 language models (Gemma-2-2B
and Gemma-2-9B), asking whether models with a four-fold difference in scale
still converge on comparable internal concepts. Using the Sparse Autoencoder
(SAE) dictionary-learning pipeline, we utilize SAEs on each model's
residual-stream activations, align the resulting monosemantic features via
activation correlation, and compare the matched feature spaces with SVCCA and
RSA. Middle layers yield the strongest overlap, while early and late layers
show far less similarity. Preliminary experiments extend the analysis from
single tokens to multi-token subspaces, showing that semantically similar
subspaces interact similarly with language models. These results strengthen the
case that large language models carve the world into broadly similar,
interpretable features despite size differences, reinforcing universality as a
foundation for cross-model interpretability.

</details>


### [9] [A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations](https://arxiv.org/abs/2507.22919)
*Qixuan Hu,Xumou Zhang,Jinman Kim,Florence Bourgeois,Adam G. Dunn*

Main category: cs.CL

TL;DR: 本研究利用预训练语言模型和滑动窗口方法，基于临床试验注册信息预测试验前严重不良事件（SAE）的发生率，有效提高了预测准确性，旨在优化试验设计并提高安全性。


<details>
  <summary>Details</summary>
Motivation: 为了避免临床试验提前终止，并限制参与者暴露于不必要的风险中，研究旨在通过试验注册信息准确估计预期的安全结果（即严重不良事件）。

Method: 分析了来自ClinicalTrials.gov的22,107项双臂平行干预性临床试验数据。开发了两种预测模型：一个分类器用于预测试验组SAE率是否高于对照组，一个回归模型用于预测对照组SAE的比例。采用迁移学习方法，结合预训练语言模型（如ClinicalT5, BioBERT）进行特征提取，并开发了滑动窗口方法处理超出输入限制的长文本。

Result: 最佳模型（ClinicalT5+Transformer+MLP）在预测哪个试验组具有更高SAE患者比例时，AUC达到77.6%；在预测对照组SAE参与者比例时，RMSE为18.6%。滑动窗口方法表现持续优于无该方法的模型，分类器平均AUC提升2.00%，回归器平均RMSE降低1.58%。

Conclusion: ClinicalTrials.gov上可用的结果摘要数据仍未充分利用。在试验开始前估计其结果的潜力，为改善试验设计和识别预期与报告安全结果之间的差异提供了重要机会。

Abstract: Objectives: With accurate estimates of expected safety results, clinical
trials could be designed to avoid terminations and limit exposing participants
to unnecessary risks. We evaluated methods for predicting serious adverse event
(SAE) results in clinical trials using information only from their
registrations prior to the trial. Material and Methods: We analysed 22,107
two-arm parallel interventional clinical trials from ClinicalTrials.gov with
structured summary results. Two prediction models were developed: a classifier
predicting will experimental arm have higher SAE rates (area under the receiver
operating characteristic curve; AUC) than control arm, and a regression model
to predict the proportion of SAEs in control arms (root mean squared error;
RMSE). A transfer learning approach using pretrained language models (e.g.,
ClinicalT5, BioBERT) was used for feature extraction, combined with downstream
model for prediction. To maintain semantic representation in long trial texts
exceeding localised language model input limits, a sliding window method was
developed for embedding extraction. Results: The best model
(ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a
higher proportion of patients with SAEs. When predicting proportion of
participants experiencing SAE in the control arm, the same model achieved RMSE
of 18.6%. The sliding window approach consistently outperformed methods without
it. Across 12 classifiers, the average absolute AUC increase was 2.00%; across
12 regressors, the average absolute RMSE reduction was 1.58%. Discussion:
Summary results data available at ClinicalTrials.gov remains underutilised. The
potential to estimate results of trials before they start is an opportunity to
improve trial design and flag discrepancies between expected and reported
safety results.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [10] [CHECK-MAT: Checking Hand-Written Mathematical Answers for the Russian Unified State Exam](https://arxiv.org/abs/2507.22958)
*Ruslan Khrulev*

Main category: cs.CV

TL;DR: 本文提出了一个新基准EGE-Math，用于评估视觉语言模型(VLM)批改手写数学解题的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准多专注于问题解决能力，而非理解学生解决方案、识别错误和根据既定标准评分的能力。因此，需要一个专门评估VLM批改手写数学答案的基准。

Method: 构建了EGE-Math Solutions Assessment Benchmark，包含122份俄罗斯统一国家考试(EGE)的手写解决方案及其官方专家评分。利用三种推理模式，评估了来自Google、OpenAI、Arcee AI和阿里巴巴云的七个现代VLM。

Result: 评估结果表明，当前VLM在数学推理能力和与人工评分标准对齐方面存在局限性。

Conclusion: 该研究揭示了现有VLM在自动评估手写数学解决方案方面的不足，为AI辅助评估领域开辟了新的研究方向。

Abstract: This paper introduces a novel benchmark, EGE-Math Solutions Assessment
Benchmark, for evaluating Vision-Language Models (VLMs) on their ability to
assess hand-written mathematical solutions. Unlike existing benchmarks that
focus on problem solving, our approach centres on understanding student
solutions, identifying mistakes, and assigning grades according to fixed
criteria. We compile 122 scanned solutions from the Russian Unified State Exam
(EGE) together with official expert grades, and evaluate seven modern VLMs from
Google, OpenAI, Arcee AI, and Alibaba Cloud in three inference modes. The
results reveal current limitations in mathematical reasoning and human-rubric
alignment, opening new research avenues in AI-assisted assessment. You can find
code in https://github.com/Karifannaa/Auto-check-EGE-math

</details>


### [11] [Robust and Efficient 3D Gaussian Splatting for Urban Scene Reconstruction](https://arxiv.org/abs/2507.23006)
*Zhensheng Yuan,Haozhi Huang,Zhen Xiong,Di Wang,Guanghua Yang*

Main category: cs.CV

TL;DR: 本文提出了一个新框架，能够快速重建和实时渲染城市尺度场景，同时有效应对多视角捕获中的外观变化。


<details>
  <summary>Details</summary>
Motivation: 解决城市尺度场景重建和实时渲染面临的效率、质量以及多视角图像间外观不一致的挑战。

Method: 该方法包括场景划分以实现并行训练、基于可见性的图像选择策略以优化训练效率、可控的LOD（细节层次）策略调节高斯密度以平衡效率与视觉保真度、外观变换模块以消除外观不一致，并利用深度正则化、尺度正则化和抗锯齿等增强模块提升重建精度。

Result: 实验结果表明，该方法能有效重建城市尺度场景，并在效率和质量上超越了现有方法。

Conclusion: 所提出的框架成功实现了城市尺度场景的快速高保真重建和实时渲染，并有效处理了外观变化问题，展现出卓越的性能。

Abstract: We present a framework that enables fast reconstruction and real-time
rendering of urban-scale scenes while maintaining robustness against appearance
variations across multi-view captures. Our approach begins with scene
partitioning for parallel training, employing a visibility-based image
selection strategy to optimize training efficiency. A controllable
level-of-detail (LOD) strategy explicitly regulates Gaussian density under a
user-defined budget, enabling efficient training and rendering while
maintaining high visual fidelity. The appearance transformation module
mitigates the negative effects of appearance inconsistencies across images
while enabling flexible adjustments. Additionally, we utilize enhancement
modules, such as depth regularization, scale regularization, and antialiasing,
to improve reconstruction fidelity. Experimental results demonstrate that our
method effectively reconstructs urban-scale scenes and outperforms previous
approaches in both efficiency and quality. The source code is available at:
https://yzslab.github.io/REUrbanGS.

</details>


### [12] [Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction](https://arxiv.org/abs/2507.23021)
*Giuseppe Cartella,Vittorio Cuculo,Alessandro D'Amelio,Marcella Cornia,Giuseppe Boccignone,Rita Cucchiara*

Main category: cs.CV

TL;DR: ScanDiff结合扩散模型与Vision Transformer，生成多样化且任务驱动的人类注视扫描路径，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在注视扫描路径预测中生成平均行为，未能捕捉人类视觉探索的内在变异性。

Method: 提出ScanDiff，一个结合扩散模型（利用其随机性建模扫描路径变异性）和Vision Transformer的新颖架构。此外，引入文本条件以实现任务驱动的扫描路径生成。

Result: ScanDiff在自由观看和任务驱动场景中均超越现有最先进方法，生成了更具多样性和准确性的扫描路径。

Conclusion: ScanDiff能够更好地捕捉人类视觉行为的复杂性，显著推动了注视预测研究的进展。

Abstract: Predicting human gaze scanpaths is crucial for understanding visual
attention, with applications in human-computer interaction, autonomous systems,
and cognitive robotics. While deep learning models have advanced scanpath
prediction, most existing approaches generate averaged behaviors, failing to
capture the variability of human visual exploration. In this work, we present
ScanDiff, a novel architecture that combines diffusion models with Vision
Transformers to generate diverse and realistic scanpaths. Our method explicitly
models scanpath variability by leveraging the stochastic nature of diffusion
models, producing a wide range of plausible gaze trajectories. Additionally, we
introduce textual conditioning to enable task-driven scanpath generation,
allowing the model to adapt to different visual search objectives. Experiments
on benchmark datasets show that ScanDiff surpasses state-of-the-art methods in
both free-viewing and task-driven scenarios, producing more diverse and
accurate scanpaths. These results highlight its ability to better capture the
complexity of human visual behavior, pushing forward gaze prediction research.
Source code and models are publicly available at
https://aimagelab.github.io/ScanDiff.

</details>


### [13] [Recovering Diagnostic Value: Super-Resolution-Aided Echocardiographic Classification in Resource-Constrained Imaging](https://arxiv.org/abs/2507.23027)
*Krishan Agyakari Raja Babu,Om Prabhu,Annu,Mohanasankar Sivaprakasam*

Main category: cs.CV

TL;DR: 本研究探讨了深度学习超分辨率（SR）技术在资源受限环境中，通过改善低质量超声心动图，从而提高其分类准确性的潜力。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境下，超声心动图图像质量差阻碍了自动心脏判读的有效性。尽管超分辨率技术在MRI和CT中显示出前景，但在易受噪声影响的超声心动图上的应用尚未充分探索。

Method: 研究使用公开的CAMUS数据集，根据图像质量对样本进行分层。评估了两种临床相关任务：二腔视图（2CH）与四腔视图（4CH）分类，以及舒张末期（ED）与收缩末期（ES）相分类。应用了两种超分辨率模型——SRGAN和SRResNet，以增强低质量图像。

Result: 研究发现，超分辨率技术（特别是SRResNet）能显著提高性能指标。SRResNet还具有计算效率。

Conclusion: 超分辨率技术能有效恢复退化超声心动图的诊断价值，使其成为资源受限环境中AI辅助医疗的可行工具，实现“少而精”的效果。

Abstract: Automated cardiac interpretation in resource-constrained settings (RCS) is
often hindered by poor-quality echocardiographic imaging, limiting the
effectiveness of downstream diagnostic models. While super-resolution (SR)
techniques have shown promise in enhancing magnetic resonance imaging (MRI) and
computed tomography (CT) scans, their application to echocardiography-a widely
accessible but noise-prone modality-remains underexplored. In this work, we
investigate the potential of deep learning-based SR to improve classification
accuracy on low-quality 2D echocardiograms. Using the publicly available CAMUS
dataset, we stratify samples by image quality and evaluate two clinically
relevant tasks of varying complexity: a relatively simple Two-Chamber vs.
Four-Chamber (2CH vs. 4CH) view classification and a more complex End-Diastole
vs. End-Systole (ED vs. ES) phase classification. We apply two widely used SR
models-Super-Resolution Generative Adversarial Network (SRGAN) and
Super-Resolution Residual Network (SRResNet), to enhance poor-quality images
and observe significant gains in performance metric-particularly with SRResNet,
which also offers computational efficiency. Our findings demonstrate that SR
can effectively recover diagnostic value in degraded echo scans, making it a
viable tool for AI-assisted care in RCS, achieving more with less.

</details>


### [14] [Adaptive Time-step Training for Enhancing Spike-Based Neural Radiance Fields](https://arxiv.org/abs/2507.23033)
*Ranxi Lin,Canming Yao,Jiayi Li,Weihang Liu,Xin Lou,Pingqiang Zhou*

Main category: cs.CV

TL;DR: 提出基于SNN的NeRF框架PATA，通过动态时间步训练，显著降低推理功耗和时间步，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF模型在训练和推理过程中需要大量浮点运算，导致计算资源消耗巨大，限制其在边缘计算等资源受限场景中的应用。脉冲神经网络（SNN）因其节能特性，有望成为解决方案。

Method: 本文提出一个基于SNN的NeRF框架，名为Pretrain-Adaptive Time-step Adjustment (PATA)。该方法采用动态时间步训练策略，自动平衡渲染质量和时间步长，从而实现场景自适应的推理。该框架以Instant-NGP架构为基础进行评估。

Result: 实验结果表明，PATA在保持渲染保真度的前提下，能够将推理时间步减少64%，运行功耗降低61.55%。

Conclusion: PATA框架通过结合SNN和动态时间步训练，有效解决了NeRF在资源受限环境下的高计算消耗问题，实现了高效、低功耗的3D重建和渲染，具有实际应用潜力。

Abstract: Neural Radiance Fields (NeRF)-based models have achieved remarkable success
in 3D reconstruction and rendering tasks. However, during both training and
inference, these models rely heavily on dense point sampling along rays from
multiple viewpoints, resulting in a surge in floating-point operations and
severely limiting their use in resource-constrained scenarios like edge
computing. Spiking Neural Networks (SNNs), which communicate via binary spikes
over discrete time steps, offer a promising alternative due to their
energy-efficient nature. Given the inherent variability in scene scale and
texture complexity in neural rendering and the prevailing practice of training
separate models per scene, we propose a spike-based NeRF framework with a
dynamic time step training strategy, termed Pretrain-Adaptive Time-step
Adjustment (PATA). This approach automatically explores the trade-off between
rendering quality and time step length during training. Consequently, it
enables scene-adaptive inference with variable time steps and reduces the
additional consumption of computational resources in the inference process.
Anchoring to the established Instant-NGP architecture, we evaluate our method
across diverse datasets. The experimental results show that PATA can preserve
rendering fidelity while reducing inference time steps by 64\% and running
power by 61.55\%.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [15] [Unifying Post-hoc Explanations of Knowledge Graph Completions](https://arxiv.org/abs/2507.22951)
*Alessandro Lonardi,Samy Badreddine,Tarek R. Besold,Pablo Sanchez Martin*

Main category: cs.AI

TL;DR: 针对知识图谱补全（KGC）事后可解释性缺乏规范和评估的问题，本文提出了一个通用的多目标优化框架和改进的评估协议，旨在提高研究的可复现性和影响力。


<details>
  <summary>Details</summary>
Motivation: KGC的事后可解释性研究缺乏形式化定义和一致的评估标准，这严重阻碍了研究的可复现性和跨研究比较。

Method: 1. 提出了一个通过多目标优化（平衡解释的有效性和简洁性）来表征事后解释的通用框架，该框架能统一现有KGC事后可解释性算法。
2. 建议并实证支持使用MRR和Hits@k等流行指标改进评估协议。
3. 强调了解释性（即解释能回答终端用户有意义查询的能力）的重要性。

Result: 本文通过实证支持了改进的评估协议，并成功地统一了现有KGC事后可解释性算法及其产生的解释。

Conclusion: 通过统一解释方法和完善评估标准，该工作旨在使KGC可解释性研究更具可复现性和影响力。

Abstract: Post-hoc explainability for Knowledge Graph Completion (KGC) lacks
formalization and consistent evaluations, hindering reproducibility and
cross-study comparisons. This paper argues for a unified approach to post-hoc
explainability in KGC. First, we propose a general framework to characterize
post-hoc explanations via multi-objective optimization, balancing their
effectiveness and conciseness. This unifies existing post-hoc explainability
algorithms in KGC and the explanations they produce. Next, we suggest and
empirically support improved evaluation protocols using popular metrics like
Mean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of
interpretability as the ability of explanations to address queries meaningful
to end-users. By unifying methods and refining evaluation standards, this work
aims to make research in KGC explainability more reproducible and impactful.

</details>


### [16] [Data Readiness for Scientific AI at Scale](https://arxiv.org/abs/2507.23018)
*Wesley Brewer,Patrick Widener,Valentine Anantharaj,Feiyi Wang,Tom Beck,Arjun Shankar,Sarp Oral*

Main category: cs.AI

TL;DR: 本文探讨了AI数据就绪性(DRAI)原则如何应用于训练基础模型的大规模科学数据集，并提出了一个两维度的就绪性框架，以指导科学AI的基础设施发展。


<details>
  <summary>Details</summary>
Motivation: 旨在理解AI数据就绪性(DRAI)原则如何应用于大规模科学数据集，并识别将这些数据转换为可用于训练基础模型的AI就绪数据所面临的挑战和通用模式。

Method: 通过分析气候、核聚变、生物/健康和材料四个代表性领域的典型工作流，识别常见预处理模式和领域特定限制。在此基础上，引入了一个包含数据就绪级别（从原始到AI就绪）和数据处理阶段（从摄取到分片）的两维度就绪性框架，该框架专为高性能计算(HPC)环境定制，并强调基于Transformer的生成模型。

Result: 研究明确了AI数据就绪性(DRAI)在大规模科学数据集中的应用模式和关键挑战。提出的两维度框架（数据就绪级别和数据处理阶段）能够概括科学数据向可扩展AI训练转化的关键挑战，并形成了一个概念性成熟度矩阵，用于表征科学数据的就绪程度。

Conclusion: 该框架和成熟度矩阵能够指导基础设施的开发，旨在为科学领域可扩展和可复现的AI提供标准化、跨领域支持。

Abstract: This paper examines how Data Readiness for AI (DRAI) principles apply to
leadership-scale scientific datasets used to train foundation models. We
analyze archetypal workflows across four representative domains - climate,
nuclear fusion, bio/health, and materials - to identify common preprocessing
patterns and domain-specific constraints. We introduce a two-dimensional
readiness framework composed of Data Readiness Levels (raw to AI-ready) and
Data Processing Stages (ingest to shard), both tailored to high performance
computing (HPC) environments. This framework outlines key challenges in
transforming scientific data for scalable AI training, emphasizing
transformer-based generative models. Together, these dimensions form a
conceptual maturity matrix that characterizes scientific data readiness and
guides infrastructure development toward standardized, cross-domain support for
scalable and reproducible AI for science.

</details>


### [17] [FairReason: Balancing Reasoning and Social Bias in MLLMs](https://arxiv.org/abs/2507.23067)
*Zhenyu Pan,Yutong Zhang,Jianshu Zhang,Haoran Lu,Haozheng Luo,Yuwei Han,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: 本研究探讨了多模态大型语言模型（MLLMs）在提高推理能力的同时如何缓解社会偏见。通过对比多种去偏见策略并调整样本比例，发现采用强化学习以1:4的去偏见与推理样本比例训练，可将偏见降低10%同时保留88%的推理精度。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型（MLLMs）在提升推理能力的同时，其输出常伴随明显的社会偏见。目前尚不清楚推理能力提升与偏见缓解之间是否存在固有的权衡关系，以及两者如何相互作用，这是一个亟待解决的开放性研究问题。

Method: 首先，在相同条件下基准测试了三种偏见缓解策略（监督微调SFT、知识蒸馏KD、基于规则的强化学习RL），以确定其优劣。其次，在每种策略下，调整去偏见样本与推理中心样本的比例。最后，通过分析这些变化，绘制出推理能力与偏见之间的权衡曲线。

Result: 研究揭示了一个持续存在的“最佳平衡点”：通过强化学习以大约1:4（去偏见与推理）的样本混合比例进行训练，可以将刻板印象得分降低10%，同时保留模型原始推理准确率的88%。

Conclusion: 本研究为多模态大型语言模型（MLLMs）在公平性和能力之间取得平衡提供了具体指导，表明通过优化训练范式（如强化学习）和样本混合比例，可以有效缓解偏见同时保持较高的推理性能。

Abstract: Multimodal Large Language Models (MLLMs) already achieve state-of-the-art
results across a wide range of tasks and modalities. To push their reasoning
ability further, recent studies explore advanced prompting schemes and
post-training fine-tuning. Although these techniques improve logical accuracy,
they frequently leave the models' outputs burdened with pronounced social
biases. Clarifying how reasoning gains interact with bias mitigation-and
whether the two objectives inherently trade off-therefore remains an open and
pressing research problem. Our study begins by benchmarking three
bias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation
(KD), and rule-based reinforcement learning (RL)-under identical conditions,
establishing their baseline strengths and weaknesses. Building on these
results, we vary the proportion of debias-focused and reasoning-centric samples
within each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps
reveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement
learning cuts stereotype scores by 10% while retaining 88% of the model's
original reasoning accuracy, offering concrete guidance for balancing fairness
and capability in MLLMs.

</details>


### [18] [Moravec's Paradox: Towards an Auditory Turing Test](https://arxiv.org/abs/2507.23091)
*David Noever,Forrest McKee*

Main category: cs.AI

TL;DR: 当前AI系统在人类轻易完成的听觉任务上表现出灾难性失败，该研究通过一项听觉图灵测试量化了人机听觉差距，并指出AI需在选择性注意、噪音鲁棒性和上下文适应方面取得突破。


<details>
  <summary>Details</summary>
Motivation: 受莫拉维克悖论启发，研究旨在证明当前AI系统在人类轻松完成的听觉任务上表现不佳，量化人机听觉差距，并诊断失败原因。

Method: 引入了一个包含917个挑战的“听觉图灵测试”，涵盖七类任务（如重叠语音、噪音中语音、空间音频等），并评估了包括GPT-4和OpenAI Whisper在内的最新音频模型。

Result: 最新的AI模型表现出惊人的失败率（超过93%），即使是性能最好的模型也仅取得6.9%的准确率，而人类在该任务上的成功率为52%。结果表明AI系统在处理复杂听觉场景时存在“聚焦失败”，尤其是在选择性注意、噪音鲁棒性和上下文适应方面。

Conclusion: 当前AI架构缺乏类人听觉场景分析的基础机制。研究建立了一个衡量机器听觉进步的诊断框架，并强调需要整合选择性注意、基于物理的音频理解和上下文感知等新方法，以实现人类水平的机器听觉。

Abstract: This research work demonstrates that current AI systems fail catastrophically
on auditory tasks that humans perform effortlessly. Drawing inspiration from
Moravec's paradox (i.e., tasks simple for humans often prove difficult for
machines, and vice versa), we introduce an auditory Turing test comprising 917
challenges across seven categories: overlapping speech, speech in noise,
temporal distortion, spatial audio, coffee-shop noise, phone distortion, and
perceptual illusions. Our evaluation of state-of-the-art audio models including
GPT-4's audio capabilities and OpenAI's Whisper reveals a striking failure rate
exceeding 93%, with even the best-performing model achieving only 6.9% accuracy
on tasks that humans solved at 7.5 times higher success (52%). These results
expose focusing failures in how AI systems process complex auditory scenes,
particularly in selective attention, noise robustness, and contextual
adaptation. Our benchmark not only quantifies the human-machine auditory gap
but also provides insights into why these failures occur, suggesting that
current architectures lack fundamental mechanisms for human-like auditory scene
analysis. The traditional design of audio CAPTCHAs highlights common filters
that humans evolved but machines fail to select in multimodal language models.
This work establishes a diagnostic framework for measuring progress toward
human-level machine listening and highlights the need for novel approaches
integrating selective attention, physics-based audio understanding, and
context-aware perception into multimodal AI systems.

</details>


### [19] [Argumentatively Coherent Judgmental Forecasting](https://arxiv.org/abs/2507.23163)
*Deniz Gorur,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 本文定义并评估了判断性预测中的“论证连贯性”，发现过滤不连贯预测能提高人类和LLM的准确性，但人类用户常不遵循此特性，因此需要引入机制过滤不连贯意见。


<details>
  <summary>Details</summary>
Motivation: 判断性预测依赖人类意见，当这些意见形成论证结构时，研究其论证属性十分有益。本研究旨在正式定义并评估“论证连贯性”这一特性，即预测者的推理应与其预测结果保持一致，以期提升预测质量。

Method: 研究首先正式定义了“论证连贯性”。随后进行了多项评估：首先，评估了强制执行连贯性对人类预测者和大型语言模型（LLM）预测者的影响；其次，通过众包用户实验，考察了用户对该连贯性特性的符合程度。

Result: 研究发现，无论对于人类预测还是基于LLM的预测，过滤掉不连贯的预测能够持续提高预测准确性，这支持了连贯性的实际价值。然而，众包用户实验表明，尽管论证连贯性看似直观且有用，但用户在实践中通常不符合这一连贯性特性。

Conclusion: 论证连贯性在判断性预测中具有实际应用价值，能有效提升人类和LLM的预测准确性。鉴于用户普遍未能遵循这一特性，在基于论证的判断性预测中，有必要引入机制来过滤不连贯的意见，从而获得更准确的群体预测结果。

Abstract: Judgmental forecasting employs human opinions to make predictions about
future events, rather than exclusively historical data as in quantitative
forecasting. When these opinions form an argumentative structure around
forecasts, it is useful to study the properties of the forecasts from an
argumentative perspective. In this paper, we advocate and formally define a
property of argumentative coherence, which, in essence, requires that a
forecaster's reasoning is coherent with their forecast. We then conduct three
evaluations with our notion of coherence. First, we assess the impact of
enforcing coherence on human forecasters as well as on Large Language Model
(LLM)-based forecasters, given that they have recently shown to be competitive
with human forecasters. In both cases, we show that filtering out incoherent
predictions improves forecasting accuracy consistently, supporting the
practical value of coherence in both human and LLM-based forecasting. Then, via
crowd-sourced user experiments, we show that, despite its apparent
intuitiveness and usefulness, users do not generally align with this coherence
property. This points to the need to integrate, within argumentation-based
judgmental forecasting, mechanisms to filter out incoherent opinions before
obtaining group forecasting predictions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [Neural Autoregressive Modeling of Brain Aging](https://arxiv.org/abs/2507.22954)
*Ridvan Yesiloglu,Wei Peng,Md Tauhidul Islam,Ehsan Adeli*

Main category: cs.LG

TL;DR: 本文提出NeuroAR，一种基于生成式自回归Transformer的新型脑老化模拟模型，能够高保真地预测受试者特定脑老化轨迹，并优于现有最先进的模型。


<details>
  <summary>Details</summary>
Motivation: 脑老化合成是临床和计算神经科学中的关键任务。从早期MRI预测未来大脑结构演变可提供有价值的老化轨迹见解。然而，数据高维度、结构微小变化和受试者特异性模式给脑老化合成带来了挑战。

Method: 提出NeuroAR模型，基于生成式自回归Transformer，通过自回归估计未来扫描的离散token图来合成老化大脑。该模型利用先前和未来扫描的连接token嵌入空间，并通过交叉注意力机制在每个尺度上连接受试者的先前扫描、采集年龄和目标年龄来引导生成过程。

Result: 在老年和青少年人群中评估，NeuroAR在图像保真度方面优于包括潜在扩散模型（LDM）和生成对抗网络（GAN）在内的现有最先进生成模型。通过预训练的年龄预测器进一步验证了合成图像与预期老化模式的一致性和真实性。NeuroAR在建模受试者特定脑老化轨迹方面显著优于关键模型（包括LDM）。

Conclusion: NeuroAR模型能够以高保真度模拟受试者特定的脑老化轨迹，在图像保真度和真实性方面表现优异，超越了现有最先进的生成模型。

Abstract: Brain aging synthesis is a critical task with broad applications in clinical
and computational neuroscience. The ability to predict the future structural
evolution of a subject's brain from an earlier MRI scan provides valuable
insights into aging trajectories. Yet, the high-dimensionality of data, subtle
changes of structure across ages, and subject-specific patterns constitute
challenges in the synthesis of the aging brain. To overcome these challenges,
we propose NeuroAR, a novel brain aging simulation model based on generative
autoregressive transformers. NeuroAR synthesizes the aging brain by
autoregressively estimating the discrete token maps of a future scan from a
convenient space of concatenated token embeddings of a previous and future
scan. To guide the generation, it concatenates into each scale the subject's
previous scan, and uses its acquisition age and the target age at each block
via cross-attention. We evaluate our approach on both the elderly population
and adolescent subjects, demonstrating superior performance over
state-of-the-art generative models, including latent diffusion models (LDM) and
generative adversarial networks, in terms of image fidelity. Furthermore, we
employ a pre-trained age predictor to further validate the consistency and
realism of the synthesized images with respect to expected aging patterns.
NeuroAR significantly outperforms key models, including LDM, demonstrating its
ability to model subject-specific brain aging trajectories with high fidelity.

</details>


### [21] [LLM-Assisted Cheating Detection in Korean Language via Keystrokes](https://arxiv.org/abs/2507.22956)
*Dong Hyun Roh,Rajesh Kumar,An Ngo*

Main category: cs.LG

TL;DR: 本文提出了一种基于击键的框架，用于检测韩语中大型语言模型（LLM）辅助的作弊行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究在语言覆盖、认知语境和LLM参与粒度方面存在不足，因此需要开发一种有效检测LLM辅助作弊的方法。

Method: 研究构建了一个包含69名参与者的数据集，他们在三种条件下（真实写作、意译ChatGPT回答、转录ChatGPT回答）完成写作任务，任务涵盖布鲁姆认知分类法的六个认知过程。提取了可解释的时间和节奏特征，并在认知感知和认知非感知设置下评估了多种分类器。

Result: 时间特征在认知感知场景下表现出色，而节奏特征在跨认知场景下泛化能力更强。检测真实写作和转录回答比检测意译回答更容易。所提出的模型显著优于人类评估者。

Conclusion: 击键动态能够可靠地检测LLM辅助写作，适用于各种认知需求和写作策略，包括意译和转录LLM生成的内容。

Abstract: This paper presents a keystroke-based framework for detecting LLM-assisted
cheating in Korean, addressing key gaps in prior research regarding language
coverage, cognitive context, and the granularity of LLM involvement. Our
proposed dataset includes 69 participants who completed writing tasks under
three conditions: Bona fide writing, paraphrasing ChatGPT responses, and
transcribing ChatGPT responses. Each task spans six cognitive processes defined
in Bloom's Taxonomy (remember, understand, apply, analyze, evaluate, and
create). We extract interpretable temporal and rhythmic features and evaluate
multiple classifiers under both Cognition-Aware and Cognition-Unaware settings.
Temporal features perform well under Cognition-Aware evaluation scenarios,
while rhythmic features generalize better under cross-cognition scenarios.
Moreover, detecting bona fide and transcribed responses was easier than
paraphrased ones for both the proposed models and human evaluators, with the
models significantly outperforming the humans. Our findings affirm that
keystroke dynamics facilitate reliable detection of LLM-assisted writing across
varying cognitive demands and writing strategies, including paraphrasing and
transcribing LLM-generated responses.

</details>


### [22] [Scientific Machine Learning with Kolmogorov-Arnold Networks](https://arxiv.org/abs/2507.22959)
*Salah A. Faroughi,Farinaz Mostajeran,Amin Hamed Mashhadzadeh,Shirko Faroughi*

Main category: cs.LG

TL;DR: 本综述分析了Kolmogorov-Arnold Networks (KANs)在科学机器学习中逐渐取代多层感知器 (MLPs)的趋势。KANs因其更好的解释性和灵活性，在准确性、收敛性和频谱表示方面持续优于MLPs。文章探讨了KANs在数据驱动、物理信息和深度算子学习中的应用，并指出了其在计算效率和理论方面仍面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 多层感知器 (MLPs) 在科学机器学习中存在局限性，包括解释性差、激活函数固定以及难以捕捉局部或高频特征。因此，需要更具解释性和灵活性的模型（如KANs）来有效建模复杂的非线性相互作用。

Method: 本文通过综述的方式，将基于KANs的模型进展归类为三个主要视角：数据驱动学习、物理信息建模和深度算子学习。每个视角都从架构设计、训练策略、应用效果以及与基于MLPs的模型的比较评估等多个维度进行审视和分析。

Result: 通过与MLPs的基准测试，KANs在准确性、收敛性和频谱表示方面表现出持续的改进。研究结果表明，KANs在捕捉复杂动力学和实现更有效学习方面具有显著优势。

Conclusion: KANs在科学机器学习中相较于MLPs展现出卓越的性能和潜力。然而，KANs的发展仍面临计算效率、理论保证、超参数调优和算法复杂性等关键挑战。未来的研究应致力于提升KANs框架的鲁棒性、可扩展性和物理一致性。

Abstract: The field of scientific machine learning, which originally utilized
multilayer perceptrons (MLPs), is increasingly adopting Kolmogorov-Arnold
Networks (KANs) for data encoding. This shift is driven by the limitations of
MLPs, including poor interpretability, fixed activation functions, and
difficulty capturing localized or high-frequency features. KANs address these
issues with enhanced interpretability and flexibility, enabling more efficient
modeling of complex nonlinear interactions and effectively overcoming the
constraints associated with conventional MLP architectures. This review
categorizes recent progress in KAN-based models across three distinct
perspectives: (i) data-driven learning, (ii) physics-informed modeling, and
(iii) deep operator learning. Each perspective is examined through the lens of
architectural design, training strategies, application efficacy, and
comparative evaluation against MLP-based counterparts. By benchmarking KANs
against MLPs, we highlight consistent improvements in accuracy, convergence,
and spectral representation, clarifying KANs' advantages in capturing complex
dynamics while learning more effectively. Finally, this review identifies
critical challenges and open research questions in KAN development,
particularly regarding computational efficiency, theoretical guarantees,
hyperparameter tuning, and algorithm complexity. We also outline future
research directions aimed at improving the robustness, scalability, and
physical consistency of KAN-based frameworks.

</details>


### [23] [Multi-Hazard Early Warning Systems for Agriculture with Featural-Temporal Explanations](https://arxiv.org/abs/2507.22962)
*Boyuan Zheng,Victor W. Chu*

Main category: cs.LG

TL;DR: 本文提出一个结合深度学习和可解释AI的多灾害预测框架，为农业提供早期预警，并能解释气候特征的影响时间和强度。


<details>
  <summary>Details</summary>
Motivation: 气候极端事件对农业风险日益增加，传统单灾害预测方法无法捕捉并发气候事件的复杂交互，且现有系统需要具备从近期气候行为中持续学习的能力。

Method: 结合序列深度学习模型（特别是BiLSTM架构）和先进的可解释人工智能（XAI）技术（如TimeSHAP），构建一个农业多灾害预测框架。该框架整合注意力机制，并使用2010年至2023年美国四个农业区气象数据，针对极端寒冷、洪水、霜冻、冰雹、热浪和强降雨等多种严重事件进行验证，并为每个区域定制模型。

Result: 实验结果表明该框架（特别是BiLSTM架构）具有强大的预测准确性，并能提供全面的时间解释，揭示哪些气候特征具有影响力以及影响发生的确切时间。

Conclusion: 该研究显著提升了多灾害早期预警系统的可解释性和适用性，促进了跨学科信任和农业气候风险管理的有效决策过程。

Abstract: Climate extremes present escalating risks to agriculture intensifying the
need for reliable multi-hazard early warning systems (EWS). The situation is
evolving due to climate change and hence such systems should have the
intelligent to continue to learn from recent climate behaviours. However,
traditional single-hazard forecasting methods fall short in capturing complex
interactions among concurrent climatic events. To address this deficiency, in
this paper, we combine sequential deep learning models and advanced Explainable
Artificial Intelligence (XAI) techniques to introduce a multi-hazard
forecasting framework for agriculture. In our experiments, we utilize
meteorological data from four prominent agricultural regions in the United
States (between 2010 and 2023) to validate the predictive accuracy of our
framework on multiple severe event types, which are extreme cold, floods,
frost, hail, heatwaves, and heavy rainfall, with tailored models for each area.
The framework uniquely integrates attention mechanisms with TimeSHAP (a
recurrent XAI explainer for time series) to provide comprehensive temporal
explanations revealing not only which climatic features are influential but
precisely when their impacts occur. Our results demonstrate strong predictive
accuracy, particularly with the BiLSTM architecture, and highlight the system's
capacity to inform nuanced, proactive risk management strategies. This research
significantly advances the explainability and applicability of multi-hazard
EWS, fostering interdisciplinary trust and effective decision-making process
for climate risk management in the agricultural industry.

</details>


### [24] [FedCVD++: Communication-Efficient Federated Learning for Cardiovascular Risk Prediction with Parametric and Non-Parametric Model Optimization](https://arxiv.org/abs/2507.22963)
*Abdelrhman Gaber,Hassan Abd-Eltawab,John Elgallab,Youssif Abuzied,Dineo Mpanya,Turgay Celik,Swarun Kumar,Tamer ElBatt*

Main category: cs.LG

TL;DR: FedCVD++是一个增强的联邦学习框架，首次将非参数模型（如随机森林、XGBoost）应用于心血管疾病预测，通过创新方法显著提高预测F1分数和通信效率，同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病每年导致全球超过1700万人死亡，迫切需要隐私保护的预测系统。

Method: 引入FedCVD++，一个集成参数和非参数模型的增强型联邦学习（FL）框架，用于冠心病风险预测。具体方法包括：1) 树子集抽样，减少随机森林通信开销；2) 基于XGBoost的特征提取，实现轻量级联邦集成；3) 联邦SMOTE同步，解决跨机构类不平衡问题。

Result: 在Framingham数据集上，FedCVD++表现卓越：联邦XGBoost (F1=0.80) 超越集中式(F1=0.78)，联邦随机森林(F1=0.81) 与非联邦性能相当。通信效率策略在保持95%准确率的同时，带宽消耗减少3.2倍。与现有FL框架相比，F1分数最高提高15%，并具有更优的多机构部署可扩展性。

Conclusion: 该工作首次将非参数模型实际集成到联邦医疗系统中，提供了一种在真实临床约束下验证的隐私保护解决方案。

Abstract: Cardiovascular diseases (CVD) cause over 17 million deaths annually
worldwide, highlighting the urgent need for privacy-preserving predictive
systems. We introduce FedCVD++, an enhanced federated learning (FL) framework
that integrates both parametric models (logistic regression, SVM, neural
networks) and non-parametric models (Random Forest, XGBoost) for coronary heart
disease risk prediction. To address key FL challenges, we propose: (1)
tree-subset sampling that reduces Random Forest communication overhead by 70%,
(2) XGBoost-based feature extraction enabling lightweight federated ensembles,
and (3) federated SMOTE synchronization for resolving cross-institutional class
imbalance.
  Evaluated on the Framingham dataset (4,238 records), FedCVD++ achieves
state-of-the-art results: federated XGBoost (F1 = 0.80) surpasses its
centralized counterpart (F1 = 0.78), and federated Random Forest (F1 = 0.81)
matches non-federated performance. Additionally, our communication-efficient
strategies reduce bandwidth consumption by 3.2X while preserving 95% accuracy.
  Compared to existing FL frameworks, FedCVD++ delivers up to 15% higher
F1-scores and superior scalability for multi-institutional deployment. This
work represents the first practical integration of non-parametric models into
federated healthcare systems, providing a privacy-preserving solution validated
under real-world clinical constraints.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [25] [PRIME: Pseudo-Random Integrated Multi-Part Entropy for Adaptive Packet Spraying in AI/ML Data centers](https://arxiv.org/abs/2507.23012)
*Ashkan Sobhani,Sogand Sadrhaghighi,Xingjun Chu*

Main category: cs.NI

TL;DR: PRIME是一种伪随机轮询分包技术，通过感知网络拓扑和拥塞信号，有效优化了AI/ML训练网络的负载均衡和性能，尤其在网络降级和置换流量场景下表现卓越。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式AI/ML训练对网络基础设施造成巨大压力，现有负载均衡方案（如ECMP）和传统分包技术存在效率低下、缓冲膨胀、信息过时以及在非对称网络条件下尾部延迟增加等问题，亟需更优的解决方案。

Method: PRIME采用一种伪随机轮询（pseudo-randomized round-robin）的分包方法，在考虑网络拓扑结构的同时优化负载分布。它以拥塞作为负载再平衡的指示器，并综合考量各种拥塞信号的严重程度及其衰减时间，以避免网络热点。

Result: 通过大规模生产级模拟器评估，结果显示与现有解决方案相比，PRIME在置换流量场景中性能提升高达15%，在网络降级场景中性能提升高达27%。

Conclusion: PRIME通过其拓扑感知和拥塞感知的伪随机轮询分包策略，有效解决了现有方案的弊端，显著提升了AI/ML训练网络在复杂和挑战性条件下的负载均衡性能和网络利用率。

Abstract: Large-scale distributed training in production data centers place significant
demands on network infrastructure. In particular, significant load balancing
challenges arise when processing AI/ML workloads, consisting of low-entropy,
bursty and long-lived flows. Existing solutions designed for Ethernet, such as
Equal-Cost Multi-Path (ECMP) struggle to maintain high network utilization.
While major industry players (e.g., Ultra Ethernet Consortium) and parts of
academia have proposed packet spraying to enhance AI/ML workload performance,
we argue that existing packet spraying solutions lead to buffer inflation over
time, negatively affecting network performance. Specifically, when ACK
coalescing is used, these solutions lead to stale information, degrading
network performance. Additionally, in asymmetric network conditions- such as
mix of ordered an unordered traffic, or link degradation and failures- existing
packet spraying solutions often lead to increased tail latency. In this paper,
we present the design and evaluation of PRIME, a pseudo-randomized round-robin
approach to packet spraying that considers the network topology to optimize
load distribution and performance. PRIME uses congestion as an indicator to
re-balance the load. To this extent, PRIME takes into account various
congestion signals, accounting for congestion severity, and their decay times
to avoid network hotspots. We extensively evaluated PRIME using large-scale
production-level simulator. Our results indicate that, compared to existing
solutions, PRIME leads to up to 15% improvement for permutation traffic and up
to 27% improvement in network degradation scenarios

</details>


### [26] [InterfO-RAN: Real-Time In-band Cellular Uplink Interference Detection with GPU-Accelerated dApps](https://arxiv.org/abs/2507.23177)
*Neagin Neasamoni Santhi,Davide Villa,Michele Polese,Tommaso Melodia*

Main category: cs.NI

TL;DR: 针对超密集5G网络中不可预测的带内上行干扰导致网络性能下降的问题，本文提出InterfO-RAN方案。该方案利用基于CNN的实时可编程技术，在gNB物理层处理I/Q样本，实现了高精度、低延迟的干扰检测，并在真实5G网络中得到验证。


<details>
  <summary>Details</summary>
Motivation: 超密集5G及未来网络利用频谱共享和频率复用以提升吞吐量，但面临不可预测的带内上行（UL）干扰挑战，严重降低了受影响gNB处的信干噪比（SINR）。这在小区边缘和定向毫米波系统中尤为突出，导致信号质量指标（如RSRP、RSSI）失真，并扰乱调度、资源分配等协议操作，损害信道状态报告和HARQ确认等关键功能。

Method: 本文提出了InterfO-RAN，一个实时可编程的解决方案，它利用卷积神经网络（CNN）在gNB物理层处理同相和正交（I/Q）样本，以检测带内干扰。InterfO-RAN是首个在GPU上加速的O-RAN dApp，并与NVIDIA Aerial的5G NR物理层处理共存。该方案在一个端到端私有5G网络中部署，并使用超过700万个来自真实环境的NR UL时隙进行训练和测试。

Result: InterfO-RAN实现了超过91%的干扰检测准确率，检测时间低于650微秒。在端到端私有5G网络中，通过商业无线单元和智能手机进行部署和测试，它展示了强大的干扰检测能力，对于在密集部署中保持网络性能至关重要。

Conclusion: InterfO-RAN为超密集5G网络提供了一种高效、实时的带内干扰检测方案。其在真实网络环境中的卓越性能验证了其对提升网络可靠性和性能的关键作用，尤其是在应对复杂的干扰挑战时。

Abstract: Ultra-dense fifth generation (5G) and beyond networks leverage spectrum
sharing and frequency reuse to enhance throughput, but face unpredictable
in-band uplink (UL) interference challenges that significantly degrade Signal
to Interference plus Noise Ratio (SINR) at affected Next Generation Node Bases
(gNBs). This is particularly problematic at cell edges, where overlapping
regions force User Equipments (UEs) to increase transmit power, and in
directional millimeter wave systems, where beamforming sidelobes can create
unexpected interference. The resulting signal degradation disrupts protocol
operations, including scheduling and resource allocation, by distorting quality
indicators like Reference Signal Received Power (RSRP) and Received Signal
Strength Indicator (RSSI), and can compromise critical functions such as
channel state reporting and Hybrid Automatic Repeat Request (HARQ)
acknowledgments. To address this problem, this article introduces InterfO-RAN,
a real-time programmable solution that leverages a Convolutional Neural Network
(CNN) to process In-phase and Quadrature (I/Q) samples in the gNB physical
layer, detecting in-band interference with accuracy exceeding 91% in under 650
us. InterfO-RAN represents the first O-RAN dApp accelerated on Graphics
Processing Unit (GPU), coexisting with the 5G NR physical layer processing of
NVIDIA Aerial. Deployed in an end-to-end private 5G network with commercial
Radio Units (RUs) and smartphones, our solution was trained and tested on more
than 7 million NR UL slots collected from real-world environments,
demonstrating robust interference detection capabilities essential for
maintaining network performance in dense deployments.

</details>


### [27] [Optimal Packetization Towards Low Latency in Random Access Networks (extended version)](https://arxiv.org/abs/2507.23286)
*Zihong Li,Anshan Yuan,Xinghua Sun*

Main category: cs.NI

TL;DR: 本研究关注分组化对随机接入网络中以秒为单位的排队延迟的影响，建立了分组化与延迟的数学关系，并通过数值方法和仿真寻找最优分组策略以最小化延迟，并将其应用于不同Aloha方案的权衡分析和NTN场景。


<details>
  <summary>Details</summary>
Motivation: 现有关于Aloha模型排队延迟的研究普遍将数据包视为原子传输单元，且主要关注以时隙计量的延迟，而忽略了分组化（即确定数据包中比特数）对以秒为单位的平均排队延迟的影响。这种以秒计量的延迟是一个更精确、更具实际相关性的性能指标，其优化对于满足低延迟服务需求至关重要。

Method: ['针对无连接和基于连接的Aloha方案，建立了分组化与秒级平均排队延迟之间的数学关系。', '采用数值方法识别最优平均排队延迟及其对应的分组大小，并分析各种网络参数的影响。', '利用仿真研究分组化对排队延迟抖动的类似影响。', '从分组化的新视角重新评估无连接和基于连接方案之间复杂的权衡。', '将分析应用于NTN（非地面网络）场景中的RA-SDT（随机接入-短数据传输）排队延迟性能，作为案例研究。']

Result: ['成功建立了分组化与秒级平均排队延迟的数学关系。', '通过数值方法识别了最优平均排队延迟及其对应的分组大小。', '分析了各种网络参数对延迟性能的影响。', '通过仿真揭示了分组化对排队延迟抖动的类似影响。', '从分组化的新视角重新评估了无连接和基于连接Aloha方案之间的复杂权衡。', '将分析应用于NTN场景中的RA-SDT排队延迟性能，并进行了案例研究。']

Conclusion: 论文通过深入分析分组化对秒级排队延迟的影响，为优化随机接入网络的延迟性能提供了关键洞察。研究不仅确定了最优分组大小和最小延迟，还揭示了分组化对抖动的影响，并更新了对不同Aloha方案间权衡的理解，为低延迟通信的设计和实施提供了实际且重要的指导。

Abstract: As the demand for low-latency services grows, ensuring the delay performance
of random access (RA) networks has become a priority. Existing studies on the
queueing delay performance of the Aloha model universally treat packets as
atomic transmission units, focusing primarily on delay measured in time slots.
However, the impact of packetization on queueing delay has been consistently
overlooked, particularly for the mean queueing delay measured in seconds, which
serves as a more precise and practically relevant performance metric than its
slot-based counterpart. Here, packetization refers to the process of
determining the number of bits assembled into a packet. To optimize queueing
delay from the perspective of packetization, this paper establishes the
mathematical relationship between packetization and mean queueing delay in
seconds for both connection-free and connection-based Aloha schemes, and
explores the optimal packetization strategy to minimize this delay. We identify
the optimal mean queueing delay and its corresponding packet size via numerical
methods, and further analyze the influence of various network parameters. We
further use simulations to investigate the similar impact of packetization on
jitter of queueing delay. We then apply our analysis to re-evaluate the complex
trade-off between the connection-free and connection-based schemes through the
new perspective of packetization. Furthermore, recognizing that an analysis of
the queueing delay performance for RA-SDT in NTN scenarios, especially from a
packetization perspective, also remains an unexplored area, we apply the
analysis to this scenario as a case study.

</details>


### [28] [FAST-LoRa: An Efficient Simulation Framework for Evaluating LoRaWAN Networks and Transmission Parameter Strategies](https://arxiv.org/abs/2507.23342)
*Laura Acosta García,Juan Aznar Poveda,Fabian Margreiter,Antonio-Javier García Sánchez,Joan García Haro,Thomas Fahringer,José Lorente López,José-Víctor Rodríguez*

Main category: cs.NI

TL;DR: 本文提出了FAST-LoRa，一个用于LoRaWAN网络的快速高效模拟框架，它通过分析模型和高效矩阵运算显著减少了模拟时间，同时保持了与现有模拟器相当的精度。


<details>
  <summary>Details</summary>
Motivation: 现有LoRaWAN网络模拟框架虽然能准确复制真实场景，但通常带来显著的计算开销和模拟时间，这阻碍了传输参数的优化和能效提升。

Method: FAST-LoRa是一个新颖的模拟框架，它通过依赖分析模型而非复杂的包级模拟来简化计算，并利用高效的矩阵运算实现网关接收。它旨在作为一个轻量级且准确的近似工具，用于评估稳定流量模式和上行通信场景下的传输参数策略。

Result: FAST-LoRa在估计关键网络指标方面（如PDR和EE）与成熟模拟器达到了相似的精度，PDR的平均绝对误差(MAE)为0.940 $	imes 10^{-2}$，能效(EE)的MAE为0.040 bits/mJ，同时将计算时间显著减少了高达三个数量级。

Conclusion: FAST-LoRa是一个高效且准确的近似工具，适用于快速评估LoRaWAN网络性能和选择传输参数，尤其在计算资源有限或需要快速迭代测试的场景中具有重要价值。

Abstract: The Internet of Things (IoT) has transformed many industries, and LoRaWAN
(Long Range Wide Area Network), built on LoRa (Long Range) technology, has
become a crucial solution for enabling scalable, low-cost, and energy-efficient
communication in wide-area networks. Simulation tools are essential for
optimizing the transmission parameters and, therefore, the energy efficiency
and performance of LoRaWAN networks. While existing simulation frameworks
accurately replicate real-world scenarios by including multiple layers of
communication protocols, they often imply significant computational overhead
and simulation times. To address this issue, this paper introduces FAST-LoRa, a
novel simulation framework designed to enable fast and efficient evaluation of
LoRaWAN networks and selection of transmission parameters. FAST-LoRa
streamlines computation by relying on analytical models without complex
packet-level simulations and implementing gateway reception using efficient
matrix operations. Rather than aiming to replace discrete-event simulators,
FAST-LoRa is intended as a lightweight and accurate approximation tool for
evaluating transmission parameter strategies in scenarios with stable traffic
patterns and uplink-focused communications. In our evaluation, we compare
FAST-LoRa with a well-established simulator using multiple network
configurations with varying numbers of end devices and gateways. The results
show that FAST-LoRa achieves similar accuracy in estimating key network
metrics, even in complex scenarios with interference and multi-gateway
reception, with a Mean Absolute Error (MAE) of 0.940 $\times 10^{-2}$ for the
Packet Delivery Ratio (PDR) and 0.040 bits/mJ for Energy Efficiency (EE), while
significantly reducing computational time by up to three orders of magnitude.

</details>


### [29] [Dual-Mode Wireless Devices for Adaptive Pull and Push-Based Communication](https://arxiv.org/abs/2507.23421)
*Sara Cavallero,Fabio Saggese,Junya Shiraishi,Israel Leyva-Mayorga,Shashi Raj Pandey,Chiara Buratti,Petar Popovski*

Main category: cs.NI

TL;DR: 论文提出了一种用于无线设备的双模通信框架，结合了查询驱动（拉取）和事件驱动（推送）传输，并优化了能效。


<details>
  <summary>Details</summary>
Motivation: 现有无线通信需要兼顾及时、情境感知和高效的数据传输。特别是在异常情况报告和日常数据请求之间，需要一个统一且节能的通信机制。

Method: 引入了一个统一时帧结构的双模通信框架，允许设备在异常时推送关键数据，平时响应拉取请求。采用唤醒无线电机制和定制的MAC协议来提高能效，并对异常报告成功率、查询响应成功率和总能耗进行了系统级分析。

Result: 数值结果表明，在推送和拉取流量的成功率之间存在权衡。与传统方法相比，所提出的方法在保持两种通信范式可靠支持的同时，能耗降低了高达30%。

Conclusion: 该双模通信框架通过自适应地结合拉取和推送模式，并结合唤醒无线电技术，显著提高了无线设备的能效，同时有效支持了关键异常报告和常规数据请求。

Abstract: This paper introduces a dual-mode communication framework for wireless
devices that integrates query-driven (pull) and event-driven (push)
transmissions within a unified time-frame structure. Devices typically respond
to information requests in pull mode, but if an anomaly is detected, they
preempt the regular response to report the critical condition. Additionally,
push-based communication is used to proactively send critical data without
waiting for a request. This adaptive approach ensures timely, context-aware,
and efficient data delivery across different network conditions. To achieve
high energy efficiency, we incorporate a wake-up radio mechanism and we design
a tailored medium access control (MAC) protocol that supports data traffic
belonging to the different communication classes. A comprehensive system-level
analysis is conducted, accounting for the wake-up control operation and
evaluating three key performance metrics: the success probability of anomaly
reports (push traffic), the success probability of query responses (pull
traffic) and the total energy consumption. Numerical results characterize the
system's behavior and highlight the inherent trade-off in success probabilities
between push- and pull-based traffic as a function of allocated communication
resources. Our analysis demonstrates that the proposed approach reduces energy
consumption by up to 30% compared to a traditional approach, while maintaining
reliable support for both communication paradigms.

</details>
