{"id": "2508.09303", "pdf": "https://arxiv.org/pdf/2508.09303", "abs": "https://arxiv.org/abs/2508.09303", "authors": ["Shu Zhao", "Tan Yu", "Anbang Xu", "Japinder Singh", "Aaditya Shukla", "Rama Akkiraju"], "title": "ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Reasoning-augmented search agents such as Search-R1, trained via\nreinforcement learning with verifiable rewards (RLVR), demonstrate remarkable\ncapabilities in multi-step information retrieval from external knowledge\nsources. These agents address the limitations of their parametric memory by\ndynamically gathering relevant facts to address complex reasoning tasks.\nHowever, existing approaches suffer from a fundamental architectural\nlimitation: they process search queries strictly sequentially, even when\nhandling inherently parallelizable and logically independent comparisons. This\nsequential bottleneck significantly constrains computational efficiency,\nparticularly for queries that require multiple entity comparisons. To address\nthis critical limitation, we propose ParallelSearch, a novel reinforcement\nlearning framework that empowers large language models (LLMs) to recognize\nparallelizable query structures and execute multiple search operations\nconcurrently. Our approach introduces dedicated reward functions that\nincentivize the identification of independent query components while preserving\nanswer accuracy through jointly considering correctness, query decomposition\nquality, and parallel execution benefits. Comprehensive experiments demonstrate\nthat ParallelSearch outperforms state-of-the-art baselines by an average\nperformance gain of 2.9% across seven question-answering benchmarks. Notably,\non parallelizable questions, our method achieves a 12.7% performance\nimprovement while requiring only 69.6% of the LLM calls compared to sequential\napproaches.", "AI": {"tldr": "\u73b0\u6709\u641c\u7d22\u4ee3\u7406\u987a\u5e8f\u6267\u884c\u6548\u7387\u4f4e\u3002ParallelSearch\u63d0\u51fa\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7fLLM\u80fd\u5e76\u884c\u5316\u641c\u7d22\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u63a8\u7406\u589e\u5f3a\u578b\u641c\u7d22\u4ee3\u7406\uff08\u5982Search-R1\uff09\u5728\u5904\u7406\u67e5\u8be2\u65f6\uff0c\u5373\u4f7f\u9762\u5bf9\u53ef\u5e76\u884c\u5316\u7684\u903b\u8f91\u72ec\u7acb\u6bd4\u8f83\uff0c\u4e5f\u4e25\u683c\u6309\u987a\u5e8f\u6267\u884c\u3002\u8fd9\u79cd\u987a\u5e8f\u74f6\u9888\u4e25\u91cd\u9650\u5236\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5c24\u5176\u5bf9\u4e8e\u9700\u8981\u591a\u5b9e\u4f53\u6bd4\u8f83\u7684\u67e5\u8be2\u3002", "method": "\u63d0\u51faParallelSearch\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8d4b\u80fd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bc6\u522b\u53ef\u5e76\u884c\u5316\u67e5\u8be2\u7ed3\u6784\u5e76\u5e76\u53d1\u6267\u884c\u591a\u4e2a\u641c\u7d22\u64cd\u4f5c\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e13\u7528\u5956\u52b1\u51fd\u6570\uff0c\u6fc0\u52b1\u8bc6\u522b\u72ec\u7acb\u7684\u67e5\u8be2\u7ec4\u4ef6\uff0c\u5e76\u901a\u8fc7\u5171\u540c\u8003\u8651\u6b63\u786e\u6027\u3001\u67e5\u8be2\u5206\u89e3\u8d28\u91cf\u548c\u5e76\u884c\u6267\u884c\u4f18\u52bf\u6765\u4fdd\u6301\u7b54\u6848\u51c6\u786e\u6027\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cParallelSearch\u5728\u4e03\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u53472.9%\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u3002\u7279\u522b\u662f\u5728\u53ef\u5e76\u884c\u5316\u95ee\u9898\u4e0a\uff0c\u5176\u6027\u80fd\u63d0\u5347\u4e8612.7%\uff0c\u540c\u65f6\u4e0e\u987a\u5e8f\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6240\u9700\u7684LLM\u8c03\u7528\u6b21\u6570\u4ec5\u4e3a69.6%\u3002", "conclusion": "ParallelSearch\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u641c\u7d22\u4ee3\u7406\u7684\u987a\u5e8f\u6267\u884c\u74f6\u9888\uff0c\u901a\u8fc7\u5f15\u5165\u5e76\u884c\u5316\u641c\u7d22\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u548c\u95ee\u7b54\u6027\u80fd\u3002"}}
{"id": "2508.09323", "pdf": "https://arxiv.org/pdf/2508.09323", "abs": "https://arxiv.org/abs/2508.09323", "authors": ["Nan Miles Xi", "Yu Deng", "Lin Wang"], "title": "Leveraging Large Language Models for Rare Disease Named Entity Recognition", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Named Entity Recognition (NER) in the rare disease domain poses unique\nchallenges due to limited labeled data, semantic ambiguity between entity\ntypes, and long-tail distributions. In this study, we evaluate the capabilities\nof GPT-4o for rare disease NER under low-resource settings, using a range of\nprompt-based strategies including zero-shot prompting, few-shot in-context\nlearning, retrieval-augmented generation (RAG), and task-level fine-tuning. We\ndesign a structured prompting framework that encodes domain-specific knowledge\nand disambiguation rules for four entity types. We further introduce two\nsemantically guided few-shot example selection methods to improve in-context\nperformance while reducing labeling effort. Experiments on the RareDis Corpus\nshow that GPT-4o achieves competitive or superior performance compared to\nBioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art\n(SOTA) results. Cost-performance analysis reveals that few-shot prompting\ndelivers high returns at low token budgets, while RAG offers marginal\nadditional benefit. An error taxonomy highlights common failure modes such as\nboundary drift and type confusion, suggesting opportunities for post-processing\nand hybrid refinement. Our results demonstrate that prompt-optimized LLMs can\nserve as effective, scalable alternatives to traditional supervised models in\nbiomedical NER, particularly in rare disease applications where annotated data\nis scarce.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86GPT-4o\u5728\u7f55\u89c1\u75c5\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u91c7\u7528\u591a\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u5e76\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u751a\u81f3\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u63d0\u793a\u4f18\u5316LLM\u5728\u751f\u7269\u533b\u5b66NER\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u7f55\u89c1\u75c5\u9886\u57df\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u5305\u62ec\u6807\u6ce8\u6570\u636e\u6709\u9650\u3001\u5b9e\u4f53\u7c7b\u578b\u95f4\u8bed\u4e49\u6a21\u7cca\u4ee5\u53ca\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86GPT-4o\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u7f55\u89c1\u75c5NER\u7684\u80fd\u529b\uff0c\u91c7\u7528\u7684\u63d0\u793a\u7b56\u7565\u5305\u62ec\u96f6\u6837\u672c\u63d0\u793a\u3001\u5c11\u6837\u672c\u60c5\u5883\u5b66\u4e60\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u4efb\u52a1\u7ea7\u5fae\u8c03\u3002\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u63d0\u793a\u6846\u67b6\uff0c\u7f16\u7801\u4e86\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u548c\u56db\u79cd\u5b9e\u4f53\u7c7b\u578b\u7684\u6d88\u6b67\u89c4\u5219\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u79cd\u8bed\u4e49\u5f15\u5bfc\u7684\u5c11\u6837\u672c\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\u3002\u6240\u6709\u5b9e\u9a8c\u5747\u5728RareDis\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cGPT-4o\u7684\u6027\u80fd\u4e0eBioClinicalBERT\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u5176\u4e2d\u4efb\u52a1\u7ea7\u5fae\u8c03\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u7ed3\u679c\u3002\u6210\u672c-\u6027\u80fd\u5206\u6790\u8868\u660e\uff0c\u5c11\u6837\u672c\u63d0\u793a\u5728\u8f83\u4f4e\u4ee4\u724c\u9884\u7b97\u4e0b\u80fd\u63d0\u4f9b\u9ad8\u56de\u62a5\uff0c\u800cRAG\u7684\u989d\u5916\u6536\u76ca\u5fae\u4e4e\u5176\u5fae\u3002\u9519\u8bef\u5206\u7c7b\u63ed\u793a\u4e86\u8fb9\u754c\u6f02\u79fb\u548c\u7c7b\u578b\u6df7\u6dc6\u7b49\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u793a\u4f18\u5316\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u4ee5\u4f5c\u4e3a\u751f\u7269\u533b\u5b66NER\u4e2d\u4f20\u7edf\u76d1\u7763\u6a21\u578b\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u7f55\u89c1\u75c5\u5e94\u7528\u4e2d\u3002"}}
{"id": "2508.09324", "pdf": "https://arxiv.org/pdf/2508.09324", "abs": "https://arxiv.org/abs/2508.09324", "authors": ["Nikita Mehrotra", "Aayush Kumar", "Sumit Gulwani", "Arjun Radhakrishna", "Ashish Tiwari"], "title": "TEN: Table Explicitization, Neurosymbolically", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a neurosymbolic approach, TEN, for extracting tabular data from\nsemistructured input text. This task is particularly challenging for text input\nthat does not use special delimiters consistently to separate columns and rows.\nPurely neural approaches perform poorly due to hallucinations and their\ninability to enforce hard constraints. TEN uses Structural Decomposition\nprompting - a specialized chain-of-thought prompting approach - on a large\nlanguage model (LLM) to generate an initial table, and thereafter uses a\nsymbolic checker to evaluate not only the well-formedness of that table, but\nalso detect cases of hallucinations or forgetting. The output of the symbolic\nchecker is processed by a critique-LLM to generate guidance for fixing the\ntable, which is presented to the original LLM in a self-debug loop. Our\nextensive experiments demonstrate that TEN significantly outperforms purely\nneural baselines across multiple datasets and metrics, achieving significantly\nhigher exact match accuracy and substantially reduced hallucination rates. A\n21-participant user study further confirms that TEN's tables are rated\nsignificantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are\nconsistently preferred for ease of verification and correction, with\nparticipants favoring our method in over 60% of the cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTEN\uff0c\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u534a\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u63d0\u53d6\u8868\u683c\u6570\u636e\u3002\u5b83\u901a\u8fc7LLM\u7ed3\u5408\u7b26\u53f7\u68c0\u67e5\u5668\u548c\u81ea\u8c03\u8bd5\u5faa\u73af\uff0c\u663e\u8457\u4f18\u4e8e\u7eaf\u795e\u7ecf\u65b9\u6cd5\uff0c\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u4ece\u534a\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u63d0\u53d6\u8868\u683c\u6570\u636e\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5f53\u6587\u672c\u4e0d\u4e00\u81f4\u5730\u4f7f\u7528\u5206\u9694\u7b26\u65f6\u3002\u7eaf\u7cb9\u7684\u795e\u7ecf\u65b9\u6cd5\u56e0\u5e7b\u89c9\u548c\u65e0\u6cd5\u5f3a\u5236\u6267\u884c\u786c\u7ea6\u675f\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "TEN\u65b9\u6cd5\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u7b26\u53f7\u89c4\u5219\u3002\u5b83\u4f7f\u7528\u7ed3\u6784\u5316\u5206\u89e3\u63d0\u793a\uff08\u4e00\u79cd\u94fe\u5f0f\u601d\u8003\u63d0\u793a\uff09\u8ba9LLM\u751f\u6210\u521d\u59cb\u8868\u683c\uff1b\u968f\u540e\uff0c\u4e00\u4e2a\u7b26\u53f7\u68c0\u67e5\u5668\u8bc4\u4f30\u8868\u683c\u7684\u89c4\u8303\u6027\u5e76\u68c0\u6d4b\u5e7b\u89c9\uff1b\u68c0\u67e5\u5668\u8f93\u51fa\u7531\u4e00\u4e2a\u6279\u8bc4LLM\u5904\u7406\uff0c\u751f\u6210\u4fee\u590d\u6307\u5357\uff1b\u6700\u540e\uff0c\u539f\u59cbLLM\u5728\u81ea\u8c03\u8bd5\u5faa\u73af\u4e2d\u5229\u7528\u8fd9\u4e9b\u6307\u5357\u4fee\u6b63\u8868\u683c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTEN\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u7eaf\u795e\u7ecf\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7cbe\u786e\u5339\u914d\u51c6\u786e\u7387\u548c\u663e\u8457\u964d\u4f4e\u7684\u5e7b\u89c9\u7387\u3002\u4e00\u987921\u4eba\u7528\u6237\u7814\u7a76\u8bc1\u5b9e\uff0cTEN\u751f\u6210\u7684\u8868\u683c\u51c6\u786e\u6027\u66f4\u9ad8\uff08\u5e73\u5747\u5f97\u52065.0 vs 4.3\uff09\uff0c\u5e76\u4e14\u5728\u9a8c\u8bc1\u548c\u4fee\u6b63\u65b9\u9762\u66f4\u53d7\u7528\u6237\u9752\u7750\uff08\u8d85\u8fc760%\u7684\u60c5\u51b5\u4e0b\u9009\u62e9TEN\uff09\u3002", "conclusion": "TEN\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u4ece\u534a\u7ed3\u6784\u5316\u6587\u672c\u4e2d\u63d0\u53d6\u8868\u683c\u6570\u636e\u7684\u96be\u9898\uff0c\u514b\u670d\u4e86\u7eaf\u795e\u7ecf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2508.09337", "pdf": "https://arxiv.org/pdf/2508.09337", "abs": "https://arxiv.org/abs/2508.09337", "authors": ["Gideon Vos", "Maryam Ebrahimpour", "Liza van Eijk", "Zoltan Sarnyai", "Mostafa Rahimi Azghadi"], "title": "Decoding Neural Emotion Patterns through Natural Language Processing Embeddings", "categories": ["cs.CL"], "comment": "26 pages, 9 figures", "summary": "Understanding how emotional expression in language relates to brain function\nis a challenge in computational neuroscience and affective computing.\nTraditional neuroimaging is costly and lab-bound, but abundant digital text\noffers new avenues for emotion-brain mapping. Prior work has largely examined\nneuroimaging-based emotion localization or computational text analysis\nseparately, with little integration. We propose a computational framework that\nmaps textual emotional content to anatomically defined brain regions without\nrequiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate\nhigh-dimensional semantic representations, apply dimensionality reduction and\nclustering to identify emotional groups, and map them to 18 brain regions\nlinked to emotional processing. Three experiments were conducted: i) analyzing\nconversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to\ncompare mapping patterns, ii) applying the method to the GoEmotions dataset and\niii) comparing human-written text with large language model (LLM) responses to\nassess differences in inferred brain activation. Emotional intensity was scored\nvia lexical analysis. Results showed neuroanatomically plausible mappings with\nhigh spatial specificity. Depressed subjects exhibited greater limbic\nengagement tied to negative affect. Discrete emotions were successfully\ndifferentiated. LLM-generated text matched humans in basic emotion distribution\nbut lacked nuanced activation in empathy and self-referential regions (medial\nprefrontal and posterior cingulate cortex). This cost-effective, scalable\napproach enables large-scale analysis of naturalistic language, distinguishes\nbetween clinical populations, and offers a brain-based benchmark for evaluating\nAI emotional expression.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u795e\u7ecf\u5f71\u50cf\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u5c06\u6587\u672c\u60c5\u611f\u6620\u5c04\u5230\u5927\u8111\u533a\u57df\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u533a\u5206\u4e34\u5e8a\u4eba\u7fa4\u548c\u8bc4\u4f30AI\u60c5\u611f\u8868\u8fbe\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7406\u89e3\u8bed\u8a00\u60c5\u611f\u8868\u8fbe\u4e0e\u5927\u8111\u529f\u80fd\u7684\u5173\u7cfb\u662f\u8ba1\u7b97\u795e\u7ecf\u79d1\u5b66\u548c\u60c5\u611f\u8ba1\u7b97\u9886\u57df\u7684\u6311\u6218\u3002\u4f20\u7edf\u795e\u7ecf\u5f71\u50cf\u6280\u672f\u6602\u8d35\u4e14\u53d7\u9650\u4e8e\u5b9e\u9a8c\u5ba4\u73af\u5883\uff0c\u800c\u4e30\u5bcc\u7684\u6570\u5b57\u6587\u672c\u4e3a\u60c5\u611f-\u5927\u8111\u6620\u5c04\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u795e\u7ecf\u5f71\u50cf\u548c\u8ba1\u7b97\u6587\u672c\u5206\u6790\u7684\u6574\u5408\uff0c\u65e0\u6cd5\u5927\u89c4\u6a21\u5206\u6790\u8bed\u8a00\u60c5\u611f\u4e0e\u5927\u8111\u529f\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u8ba1\u7b97\u6846\u67b6\uff0c\u65e0\u9700\u795e\u7ecf\u5f71\u50cf\u5373\u53ef\u5c06\u6587\u672c\u60c5\u611f\u5185\u5bb9\u6620\u5c04\u5230\u89e3\u5256\u5b66\u5b9a\u4e49\u7684\u5927\u8111\u533a\u57df\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u4f7f\u7528OpenAI\u7684text-embedding-ada-002\u751f\u6210\u9ad8\u7ef4\u8bed\u4e49\u8868\u793a\uff0c\u901a\u8fc7\u964d\u7ef4\u548c\u805a\u7c7b\u8bc6\u522b\u60c5\u611f\u7fa4\u7ec4\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u523018\u4e2a\u4e0e\u60c5\u611f\u5904\u7406\u76f8\u5173\u7684\u5927\u8111\u533a\u57df\u3002\u5171\u8fdb\u884c\u4e86\u4e09\u9879\u5b9e\u9a8c\uff1ai) \u5206\u6790\u5065\u5eb7\u4e0e\u6291\u90c1\u75c7\u60a3\u8005\u7684\u5bf9\u8bdd\u6570\u636e\u4ee5\u6bd4\u8f83\u6620\u5c04\u6a21\u5f0f\uff1bii) \u5c06\u65b9\u6cd5\u5e94\u7528\u4e8eGoEmotions\u6570\u636e\u96c6\uff1biii) \u6bd4\u8f83\u4eba\u7c7b\u4e66\u5199\u6587\u672c\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u54cd\u5e94\uff0c\u4ee5\u8bc4\u4f30\u63a8\u65ad\u7684\u5927\u8111\u6fc0\u6d3b\u5dee\u5f02\u3002\u60c5\u611f\u5f3a\u5ea6\u901a\u8fc7\u8bcd\u6c47\u5206\u6790\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e86\u795e\u7ecf\u89e3\u5256\u5b66\u4e0a\u5408\u7406\u4e14\u5177\u6709\u9ad8\u7a7a\u95f4\u7279\u5f02\u6027\u7684\u6620\u5c04\u3002\u6291\u90c1\u75c7\u60a3\u8005\u8868\u73b0\u51fa\u4e0e\u8d1f\u9762\u60c5\u611f\u76f8\u5173\u7684\u66f4\u9ad8\u8fb9\u7f18\u7cfb\u7edf\u53c2\u4e0e\u5ea6\u3002\u8be5\u65b9\u6cd5\u6210\u529f\u533a\u5206\u4e86\u79bb\u6563\u60c5\u7eea\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u5728\u57fa\u672c\u60c5\u611f\u5206\u5e03\u4e0a\u4e0e\u4eba\u7c7b\u6587\u672c\u5339\u914d\uff0c\u4f46\u5728\u540c\u7406\u5fc3\u548c\u81ea\u6211\u53c2\u7167\u533a\u57df\uff08\u5185\u4fa7\u524d\u989d\u53f6\u548c\u540e\u6263\u5e26\u76ae\u5c42\uff09\u7f3a\u4e4f\u7ec6\u5fae\u7684\u6fc0\u6d3b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u6210\u672c\u6548\u76ca\u548c\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u5927\u89c4\u6a21\u81ea\u7136\u8bed\u8a00\u5206\u6790\uff0c\u533a\u5206\u4e0d\u540c\u4e34\u5e8a\u4eba\u7fa4\uff0c\u5e76\u4e3a\u8bc4\u4f30\u4eba\u5de5\u667a\u80fd\u60c5\u611f\u8868\u8fbe\u63d0\u4f9b\u4e00\u4e2a\u57fa\u4e8e\u5927\u8111\u7684\u57fa\u51c6\u3002"}}
{"id": "2508.09147", "pdf": "https://arxiv.org/pdf/2508.09147", "abs": "https://arxiv.org/abs/2508.09147", "authors": ["Alaa Saleh", "Roberto Morabito", "Sasu Tarkoma", "Anders Lindgren", "Susanna Pirttikangas", "Lauri Lov\u00e9n"], "title": "Agentic TinyML for Intent-aware Handover in 6G Wireless Networks", "categories": ["cs.NI", "cs.AI", "cs.DC", "cs.LG", "cs.MA"], "comment": null, "summary": "As 6G networks evolve into increasingly AI-driven, user-centric ecosystems,\ntraditional reactive handover mechanisms demonstrate limitations, especially in\nmobile edge computing and autonomous agent-based service scenarios. This\nmanuscript introduces WAAN, a cross-layer framework that enables intent-aware\nand proactive handovers by embedding lightweight TinyML agents as autonomous,\nnegotiation-capable entities across heterogeneous edge nodes that contribute to\nintent propagation and network adaptation. To ensure continuity across\nmobility-induced disruptions, WAAN incorporates semi-stable rendezvous points\nthat serve as coordination anchors for context transfer and state preservation.\nThe framework's operational capabilities are demonstrated through a multimodal\nenvironmental control case study, highlighting its effectiveness in maintaining\nuser experience under mobility. Finally, the article discusses key challenges\nand future opportunities associated with the deployment and evolution of WAAN.", "AI": {"tldr": "WAAN\u662f\u4e00\u4e2a\u8de8\u5c42\u6846\u67b6\uff0c\u901a\u8fc7TinyML\u4ee3\u7406\u548c\u534a\u7a33\u5b9a\u96c6\u5408\u70b9\uff0c\u57286G\u7f51\u7edc\u4e2d\u5b9e\u73b0\u610f\u56fe\u611f\u77e5\u7684\u4e3b\u52a8\u5207\u6362\uff0c\u4ee5\u786e\u4fdd\u79fb\u52a8\u573a\u666f\u4e0b\u7684\u7528\u6237\u4f53\u9a8c\u8fde\u7eed\u6027\u3002", "motivation": "\u968f\u77406G\u7f51\u7edc\u5411AI\u9a71\u52a8\u3001\u7528\u6237\u4e2d\u5fc3\u7684\u751f\u6001\u7cfb\u7edf\u6f14\u8fdb\uff0c\u4f20\u7edf\u7684\u88ab\u52a8\u5207\u6362\u673a\u5236\u5728\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u548c\u81ea\u4e3b\u4ee3\u7406\u670d\u52a1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165WAAN\u8de8\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5f02\u6784\u8fb9\u7f18\u8282\u70b9\u5d4c\u5165\u8f7b\u91cf\u7ea7TinyML\u4ee3\u7406\uff0c\u5b9e\u73b0\u610f\u56fe\u4f20\u64ad\u548c\u7f51\u7edc\u81ea\u9002\u5e94\uff1b\u540c\u65f6\uff0c\u6574\u5408\u534a\u7a33\u5b9a\u96c6\u5408\u70b9\u4f5c\u4e3a\u534f\u8c03\u951a\u70b9\uff0c\u7528\u4e8e\u4e0a\u4e0b\u6587\u4f20\u8f93\u548c\u72b6\u6001\u4fdd\u5b58\uff0c\u4ee5\u786e\u4fdd\u79fb\u52a8\u6027\u5f15\u8d77\u4e2d\u65ad\u540e\u7684\u8fde\u7eed\u6027\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u591a\u6a21\u6001\u73af\u5883\u63a7\u5236\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86WAAN\u6846\u67b6\u7684\u64cd\u4f5c\u80fd\u529b\u53ca\u5176\u5728\u79fb\u52a8\u6027\u4e0b\u6709\u6548\u4fdd\u6301\u7528\u6237\u4f53\u9a8c\u7684\u80fd\u529b\u3002", "conclusion": "WAAN\u6846\u67b6\u80fd\u6709\u6548\u5e94\u5bf96G\u7f51\u7edc\u4e2d\u79fb\u52a8\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u4fdd\u6301\u7528\u6237\u4f53\u9a8c\u3002\u6587\u7ae0\u8fd8\u63a2\u8ba8\u4e86WAAN\u90e8\u7f72\u548c\u6f14\u8fdb\u7684\u5173\u952e\u6311\u6218\u4e0e\u672a\u6765\u673a\u9047\u3002"}}
{"id": "2508.09277", "pdf": "https://arxiv.org/pdf/2508.09277", "abs": "https://arxiv.org/abs/2508.09277", "authors": ["Soumia Mehimeh"], "title": "Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning", "categories": ["cs.AI", "cs.LG", "cs.LO"], "comment": null, "summary": "Value function initialization (VFI) is an effective way to achieve a\njumpstart in reinforcement learning (RL) by leveraging value estimates from\nprior tasks. While this approach is well established in tabular settings,\nextending it to deep reinforcement learning (DRL) poses challenges due to the\ncontinuous nature of the state-action space, the noisy approximations of neural\nnetworks, and the impracticality of storing all past models for reuse. In this\nwork, we address these challenges and introduce DQInit, a method that adapts\nvalue function initialization to DRL. DQInit reuses compact tabular Q-values\nextracted from previously solved tasks as a transferable knowledge base. It\nemploys a knownness-based mechanism to softly integrate these transferred\nvalues into underexplored regions and gradually shift toward the agent's\nlearned estimates, avoiding the limitations of fixed time decay. Our approach\noffers a novel perspective on knowledge transfer in DRL by relying solely on\nvalue estimates rather than policies or demonstrations, effectively combining\nthe strengths of jumpstart RL and policy distillation while mitigating their\ndrawbacks. Experiments across multiple continuous control tasks demonstrate\nthat DQInit consistently improves early learning efficiency, stability, and\noverall performance compared to standard initialization and existing transfer\ntechniques.", "AI": {"tldr": "DQInit\u662f\u4e00\u79cd\u9488\u5bf9\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u503c\u51fd\u6570\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u7528\u7d27\u51d1\u7684\u8868\u683cQ\u503c\u5b9e\u73b0\u77e5\u8bc6\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e9\u671f\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u503c\u51fd\u6570\u521d\u59cb\u5316\uff08VFI\uff09\u5728\u8868\u683c\u5f3a\u5316\u5b66\u4e60\u4e2d\u80fd\u6709\u6548\u5b9e\u73b0RL\u7684\u5feb\u901f\u542f\u52a8\uff0c\u4f46\u5728\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4e2d\u9762\u4e34\u8fde\u7eed\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u3001\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u566a\u58f0\u4ee5\u53ca\u65e0\u6cd5\u5b58\u50a8\u6240\u6709\u5386\u53f2\u6a21\u578b\u7684\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86DQInit\uff0c\u5c06VFI\u5e94\u7528\u4e8eDRL\u3002DQInit\u4ece\u5df2\u89e3\u51b3\u7684\u4efb\u52a1\u4e2d\u63d0\u53d6\u7d27\u51d1\u7684\u8868\u683cQ\u503c\u4f5c\u4e3a\u53ef\u8fc1\u79fb\u7684\u77e5\u8bc6\u5e93\u3002\u5b83\u91c7\u7528\u57fa\u4e8e\u201c\u5df2\u77e5\u5ea6\u201d\u7684\u673a\u5236\uff0c\u5c06\u8fd9\u4e9b\u8fc1\u79fb\u503c\u67d4\u548c\u5730\u6574\u5408\u5230\u672a\u5145\u5206\u63a2\u7d22\u7684\u533a\u57df\uff0c\u5e76\u9010\u6b65\u8f6c\u5411\u667a\u80fd\u4f53\u5b66\u4e60\u7684\u4f30\u8ba1\u503c\uff0c\u907f\u514d\u4e86\u56fa\u5b9a\u65f6\u95f4\u8870\u51cf\u7684\u5c40\u9650\u6027\u3002\u8be5\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u503c\u4f30\u8ba1\u800c\u975e\u7b56\u7565\u6216\u6f14\u793a\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5728\u591a\u4e2a\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6807\u51c6\u521d\u59cb\u5316\u548c\u73b0\u6709\u8fc1\u79fb\u6280\u672f\u76f8\u6bd4\uff0cDQInit\u6301\u7eed\u6539\u8fdb\u4e86\u65e9\u671f\u5b66\u4e60\u6548\u7387\u3001\u7a33\u5b9a\u6027\u548c\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "DQInit\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684DRL\u77e5\u8bc6\u8fc1\u79fb\u89c6\u89d2\uff0c\u7ed3\u5408\u4e86\u5feb\u901f\u542f\u52a8RL\u548c\u7b56\u7565\u84b8\u998f\u7684\u4f18\u70b9\u5e76\u51cf\u8f7b\u4e86\u5b83\u4eec\u7684\u7f3a\u70b9\uff0c\u663e\u8457\u63d0\u5347\u4e86DRL\u7684\u65e9\u671f\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2508.09144", "pdf": "https://arxiv.org/pdf/2508.09144", "abs": "https://arxiv.org/abs/2508.09144", "authors": ["Liping Huang", "Yicheng Zhang", "Yifang Yin", "Sheng Zhang", "Yi Zhang"], "title": "Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 9 figures, published in the confernce \"US-Europe Air\n  Transportation Research & Development Symposium 2025\"", "summary": "Estimated time of arrival (ETA) for airborne aircraft in real-time is crucial\nfor arrival management in aviation, particularly for runway sequencing. Given\nthe rapidly changing airspace context, the ETA prediction efficiency is as\nimportant as its accuracy in a real-time arrival aircraft management system. In\nthis study, we utilize a feature tokenization-based Transformer model to\nefficiently predict aircraft ETA. Feature tokenization projects raw inputs to\nlatent spaces, while the multi-head self-attention mechanism in the Transformer\ncaptures important aspects of the projections, alleviating the need for complex\nfeature engineering. Moreover, the Transformer's parallel computation\ncapability allows it to handle ETA requests at a high frequency, i.e., 1HZ,\nwhich is essential for a real-time arrival management system. The model inputs\ninclude raw data, such as aircraft latitude, longitude, ground speed, theta\ndegree for the airport, day and hour from track data, the weather context, and\naircraft wake turbulence category. With a data sampling rate of 1HZ, the ETA\nprediction is updated every second. We apply the proposed aircraft ETA\nprediction approach to Singapore Changi Airport (ICAO Code: WSSS) using\none-month Automatic Dependent Surveillance-Broadcast (ADS-B) data from October\n1 to October 31, 2022. In the experimental evaluation, the ETA modeling covers\nall aircraft within a range of 10NM to 300NM from WSSS. The results show that\nour proposed method method outperforms the commonly used boosting tree based\nmodel, improving accuracy by 7\\% compared to XGBoost, while requiring only 39\\%\nof its computing time. Experimental results also indicate that, with 40\naircraft in the airspace at a given timestamp, the ETA inference time is only\n51.7 microseconds, making it promising for real-time arrival management\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u5206\u8bcd\u7684Transformer\u6a21\u578b\uff0c\u80fd\u9ad8\u6548\u51c6\u786e\u5730\u9884\u6d4b\u98de\u673a\u5b9e\u65f6\u9884\u8ba1\u62b5\u8fbe\u65f6\u95f4\uff08ETA\uff09\uff0c\u5e76\u5728\u65b0\u52a0\u5761\u6a1f\u5b9c\u673a\u573a\u6570\u636e\u4e0a\u9a8c\u8bc1\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5b9e\u65f6\u98de\u673a\u9884\u8ba1\u62b5\u8fbe\u65f6\u95f4\uff08ETA\uff09\u5bf9\u4e8e\u822a\u7a7a\u62b5\u8fbe\u7ba1\u7406\uff0c\u7279\u522b\u662f\u8dd1\u9053\u6392\u5e8f\u81f3\u5173\u91cd\u8981\u3002\u5728\u5feb\u901f\u53d8\u5316\u7684\u7a7a\u57df\u73af\u5883\u4e2d\uff0c\u5b9e\u65f6\u62b5\u8fbe\u7ba1\u7406\u7cfb\u7edf\u8981\u6c42ETA\u9884\u6d4b\u540c\u65f6\u5177\u5907\u9ad8\u6548\u7387\u548c\u9ad8\u7cbe\u5ea6\u3002", "method": "\u7814\u7a76\u91c7\u7528\u57fa\u4e8e\u7279\u5f81\u5206\u8bcd\u7684Transformer\u6a21\u578b\u6765\u9ad8\u6548\u9884\u6d4b\u98de\u673aETA\u3002\u7279\u5f81\u5206\u8bcd\u5c06\u539f\u59cb\u8f93\u5165\u6620\u5c04\u5230\u6f5c\u5728\u7a7a\u95f4\uff0cTransformer\u7684\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u83b7\u6295\u5f71\u7684\u5173\u952e\u65b9\u9762\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u590d\u6742\u7279\u5f81\u5de5\u7a0b\u7684\u9700\u6c42\u3002\u5176\u5e76\u884c\u8ba1\u7b97\u80fd\u529b\u4f7f\u5176\u80fd\u4ee51HZ\u7684\u9ad8\u9891\u7387\u5904\u7406ETA\u8bf7\u6c42\u3002\u6a21\u578b\u8f93\u5165\u5305\u62ec\u98de\u673a\u7ecf\u7eac\u5ea6\u3001\u5730\u901f\u3001\u673a\u573a\u65b9\u4f4d\u89d2\u3001\u65f6\u95f4\u3001\u5929\u6c14\u548c\u5c3e\u6d41\u6e4d\u6d41\u7c7b\u522b\u7b49\u539f\u59cb\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u5728\u65b0\u52a0\u5761\u6a1f\u5b9c\u673a\u573a\uff08WSSS\uff09\u5e94\u7528\uff0c\u4f7f\u75282022\u5e7410\u6708\u4e00\u4e2a\u6708\u7684ADS-B\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u8303\u56f4\u8986\u76d6\u8ddd\u79bb\u673a\u573a10NM\u81f3300NM\u7684\u98de\u673a\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4\u5e38\u7528\u7684\u57fa\u4e8e\u589e\u5f3a\u6811\u7684\u6a21\u578b\uff08\u5982XGBoost\uff09\u51c6\u786e\u7387\u63d0\u9ad8\u4e867%\uff0c\u540c\u65f6\u8ba1\u7b97\u65f6\u95f4\u4ec5\u4e3a\u517639%\u3002\u5b9e\u9a8c\u7ed3\u679c\u8fd8\u8868\u660e\uff0c\u5728\u7a7a\u57df\u4e2d\u670940\u67b6\u98de\u673a\u7684\u60c5\u51b5\u4e0b\uff0cETA\u63a8\u7406\u65f6\u95f4\u4ec5\u4e3a51.7\u5fae\u79d2\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u98de\u673aETA\u9884\u6d4b\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5bf9\u4e8e\u5b9e\u65f6\u62b5\u8fbe\u7ba1\u7406\u7cfb\u7edf\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2508.09175", "pdf": "https://arxiv.org/pdf/2508.09175", "abs": "https://arxiv.org/abs/2508.09175", "authors": ["Mohammad Zia Ur Rehman", "Sufyaan Zahoor", "Areeb Manzoor", "Musharaf Maqbool", "Nagendra Kumar"], "title": "A Context-aware Attention and Graph Neural Network-based Multimodal Framework for Misogyny Detection", "categories": ["cs.CV", "cs.AI"], "comment": "Published in Information Processing & Management", "summary": "A substantial portion of offensive content on social media is directed\ntowards women. Since the approaches for general offensive content detection\nface a challenge in detecting misogynistic content, it requires solutions\ntailored to address offensive content against women. To this end, we propose a\nnovel multimodal framework for the detection of misogynistic and sexist\ncontent. The framework comprises three modules: the Multimodal Attention module\n(MANM), the Graph-based Feature Reconstruction Module (GFRM), and the\nContent-specific Features Learning Module (CFLM). The MANM employs adaptive\ngating-based multimodal context-aware attention, enabling the model to focus on\nrelevant visual and textual information and generating contextually relevant\nfeatures. The GFRM module utilizes graphs to refine features within individual\nmodalities, while the CFLM focuses on learning text and image-specific features\nsuch as toxicity features and caption features. Additionally, we curate a set\nof misogynous lexicons to compute the misogyny-specific lexicon score from the\ntext. We apply test-time augmentation in feature space to better generalize the\npredictions on diverse inputs. The performance of the proposed approach has\nbeen evaluated on two multimodal datasets, MAMI and MMHS150K, with 11,000 and\n13,494 samples, respectively. The proposed method demonstrates an average\nimprovement of 10.17% and 8.88% in macro-F1 over existing methods on the MAMI\nand MMHS150K datasets, respectively.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u538c\u5973\u548c\u6027\u522b\u6b67\u89c6\u5185\u5bb9\uff0c\u5e76\u5728\u4e24\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u5927\u91cf\u653b\u51fb\u6027\u5185\u5bb9\u9488\u5bf9\u5973\u6027\uff0c\u800c\u73b0\u6709\u7684\u901a\u7528\u653b\u51fb\u6027\u5185\u5bb9\u68c0\u6d4b\u65b9\u6cd5\u5728\u68c0\u6d4b\u538c\u5973\u5185\u5bb9\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u5973\u6027\u653b\u51fb\u6027\u5185\u5bb9\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u591a\u6a21\u6001\u6ce8\u610f\u529b\u6a21\u5757 (MANM) \u7528\u4e8e\u751f\u6210\u4e0a\u4e0b\u6587\u76f8\u5173\u7279\u5f81\uff0c\u56fe\u57fa\u7279\u5f81\u91cd\u6784\u6a21\u5757 (GFRM) \u7528\u4e8e\u7ec6\u5316\u6a21\u6001\u5185\u7279\u5f81\uff0c\u4ee5\u53ca\u5185\u5bb9\u7279\u5b9a\u7279\u5f81\u5b66\u4e60\u6a21\u5757 (CFLM) \u7528\u4e8e\u5b66\u4e60\u6587\u672c\u548c\u56fe\u50cf\u7279\u5b9a\u7279\u5f81\uff08\u5982\u6bd2\u6027\u7279\u5f81\u548c\u6807\u9898\u7279\u5f81\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u6574\u7406\u4e86\u538c\u5973\u8bcd\u6c47\u96c6\u4ee5\u8ba1\u7b97\u538c\u5973\u7279\u5b9a\u8bcd\u6c47\u5206\u6570\uff0c\u5e76\u5e94\u7528\u7279\u5f81\u7a7a\u95f4\u4e2d\u7684\u6d4b\u8bd5\u65f6\u589e\u5f3a\u6765\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728MAMI\u548cMMHS150K\u4e24\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728macro-F1\u6307\u6807\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u9ad8\u4e8610.17%\u548c8.88%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u6a21\u6001\u6846\u67b6\u80fd\u6709\u6548\u68c0\u6d4b\u538c\u5973\u548c\u6027\u522b\u6b67\u89c6\u5185\u5bb9\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u89e3\u51b3\u8be5\u7279\u5b9a\u95ee\u9898\u4e0a\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.09349", "pdf": "https://arxiv.org/pdf/2508.09349", "abs": "https://arxiv.org/abs/2508.09349", "authors": ["Cathy Speed", "Ahmed A. Metwally"], "title": "The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Expert consensus plays a critical role in domains where evidence is complex,\nconflicting, or insufficient for direct prescription. Traditional methods, such\nas Delphi studies, consensus conferences, and systematic guideline synthesis,\noffer structure but face limitations including high panel burden, interpretive\noversimplification, and suppression of conditional nuance. These challenges are\nnow exacerbated by information overload, fragmentation of the evidence base,\nand increasing reliance on publicly available sources that lack expert\nfiltering. This study introduces and evaluates a Human-AI Hybrid Delphi\n(HAH-Delphi) framework designed to augment expert consensus development by\nintegrating a generative AI model (Gemini 2.5 Pro), small panels of senior\nhuman experts, and structured facilitation. The HAH-Delphi was tested in three\nphases: retrospective replication, prospective comparison, and applied\ndeployment in two applied domains (endurance training and resistance and mixed\ncardio/strength training). The AI replicated 95% of published expert consensus\nconclusions in Phase I and showed 95% directional agreement with senior human\nexperts in Phase II, though it lacked experiential and pragmatic nuance. In\nPhase III, compact panels of six senior experts achieved >90% consensus\ncoverage and reached thematic saturation before the final participant. The AI\nprovided consistent, literature-grounded scaffolding that supported divergence\nresolution and accelerated saturation. The HAH-Delphi framework offers a\nflexible, scalable approach for generating high-quality, context-sensitive\nconsensus. Its successful application across health, coaching, and performance\nscience confirms its methodological robustness and supports its use as a\nfoundation for generating conditional, personalised guidance and published\nconsensus frameworks at scale.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e00\u79cd\u4eba\u673a\u6df7\u5408Delphi\uff08HAH-Delphi\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u5f0fAI\uff08Gemini 2.5 Pro\uff09\u548c\u8d44\u6df1\u4eba\u7c7b\u4e13\u5bb6\uff0c\u65e8\u5728\u6539\u8fdb\u4e13\u5bb6\u5171\u8bc6\u7684\u5236\u5b9a\u8fc7\u7a0b\u3002\u8be5\u6846\u67b6\u5728\u590d\u5236\u73b0\u6709\u5171\u8bc6\u3001\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8fbe\u6210\u4e00\u81f4\u4ee5\u53ca\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9ad8\u6548\u8fbe\u6210\u5171\u8bc6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u5b9e\u4e86\u5176\u63d0\u4f9b\u9ad8\u8d28\u91cf\u3001\u60c5\u5883\u654f\u611f\u5171\u8bc6\u7684\u6f5c\u529b\u3002", "motivation": "\u5728\u8bc1\u636e\u590d\u6742\u3001\u51b2\u7a81\u6216\u4e0d\u8db3\u4ee5\u76f4\u63a5\u6307\u5bfc\u7684\u9886\u57df\uff0c\u4e13\u5bb6\u5171\u8bc6\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u5171\u8bc6\u65b9\u6cd5\uff08\u5982Delphi\u7814\u7a76\u3001\u5171\u8bc6\u4f1a\u8bae\uff09\u5b58\u5728\u4e13\u5bb6\u8d1f\u62c5\u91cd\u3001\u89e3\u91ca\u8fc7\u5ea6\u7b80\u5316\u548c\u538b\u5236\u60c5\u5883\u7ec6\u5fae\u5dee\u522b\u7b49\u5c40\u9650\u6027\u3002\u5f53\u524d\u4fe1\u606f\u8fc7\u8f7d\u3001\u8bc1\u636e\u57fa\u7840\u788e\u7247\u5316\u53ca\u5bf9\u7f3a\u4e4f\u4e13\u5bb6\u7b5b\u9009\u7684\u516c\u5f00\u4fe1\u606f\u4f9d\u8d56\u589e\u52a0\uff0c\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e9b\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4e13\u5bb6\u5171\u8bc6\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u4eba\u673a\u6df7\u5408Delphi\uff08HAH-Delphi\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u6574\u5408\u4e86\u751f\u6210\u5f0fAI\u6a21\u578b\uff08Gemini 2.5 Pro\uff09\u3001\u5c0f\u578b\u8d44\u6df1\u4eba\u7c7b\u4e13\u5bb6\u5c0f\u7ec4\u4ee5\u53ca\u7ed3\u6784\u5316\u5f15\u5bfc\u3002HAH-Delphi\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u8fdb\u884c\u6d4b\u8bd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4e3a\u56de\u987e\u6027\u590d\u5236\uff0c\u7b2c\u4e8c\u9636\u6bb5\u4e3a\u524d\u77bb\u6027\u6bd4\u8f83\uff0c\u7b2c\u4e09\u9636\u6bb5\u4e3a\u5728\u8010\u529b\u8bad\u7ec3\u548c\u963b\u529b\u4e0e\u6df7\u5408\u5fc3\u80ba/\u529b\u91cf\u8bad\u7ec3\u4e24\u4e2a\u5e94\u7528\u9886\u57df\u8fdb\u884c\u90e8\u7f72\u3002", "result": "\u5728\u7b2c\u4e00\u9636\u6bb5\uff0cAI\u6210\u529f\u590d\u5236\u4e8695%\u5df2\u53d1\u5e03\u7684\u4e13\u5bb6\u5171\u8bc6\u7ed3\u8bba\u3002\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0cAI\u4e0e\u8d44\u6df1\u4eba\u7c7b\u4e13\u5bb6\u8868\u73b0\u51fa95%\u7684\u65b9\u5411\u6027\u4e00\u81f4\uff0c\u4f46\u7f3a\u4e4f\u7ecf\u9a8c\u6027\u548c\u5b9e\u7528\u6027\u7ec6\u5fae\u5dee\u522b\u3002\u5728\u7b2c\u4e09\u9636\u6bb5\uff0c\u7531\u516d\u4f4d\u8d44\u6df1\u4e13\u5bb6\u7ec4\u6210\u7684\u7d27\u51d1\u5c0f\u7ec4\u5728\u6700\u7ec8\u53c2\u4e0e\u8005\u4e4b\u524d\u5c31\u5b9e\u73b0\u4e86\u8d85\u8fc790%\u7684\u5171\u8bc6\u8986\u76d6\u7387\u5e76\u8fbe\u5230\u4e86\u4e3b\u9898\u9971\u548c\u3002AI\u63d0\u4f9b\u4e86\u8fde\u8d2f\u7684\u3001\u4ee5\u6587\u732e\u4e3a\u57fa\u7840\u7684\u652f\u6491\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u5206\u6b67\u5e76\u52a0\u901f\u9971\u548c\u3002", "conclusion": "HAH-Delphi\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u60c5\u5883\u654f\u611f\u7684\u5171\u8bc6\u3002\u5176\u5728\u5065\u5eb7\u3001\u6559\u7ec3\u548c\u8fd0\u52a8\u8868\u73b0\u79d1\u5b66\u9886\u57df\u7684\u6210\u529f\u5e94\u7528\uff0c\u8bc1\u5b9e\u4e86\u5176\u65b9\u6cd5\u5b66\u7684\u7a33\u5065\u6027\uff0c\u5e76\u652f\u6301\u5176\u4f5c\u4e3a\u5927\u89c4\u6a21\u751f\u6210\u6761\u4ef6\u6027\u3001\u4e2a\u6027\u5316\u6307\u5bfc\u548c\u53d1\u5e03\u5171\u8bc6\u6846\u67b6\u7684\u57fa\u7840\u3002"}}
{"id": "2508.09149", "pdf": "https://arxiv.org/pdf/2508.09149", "abs": "https://arxiv.org/abs/2508.09149", "authors": ["Seyed Hossein Ahmadpanah"], "title": "Semantic-Aware LLM Orchestration for Proactive Resource Management in Predictive Digital Twin Vehicular Networks", "categories": ["cs.NI", "cs.DC"], "comment": null, "summary": "Next-generation automotive applications require vehicular edge computing\n(VEC), but current management systems are essentially fixed and reactive. They\nare suboptimal in extremely dynamic vehicular environments because they are\nconstrained to static optimization objectives and base their decisions on the\ncurrent network states. This paper presents a novel Semantic-Aware Proactive\nLLM Orchestration (SP-LLM) framework to address these issues. Our method\ntransforms the traditional Digital Twin (DT) into a Predictive Digital Twin\n(pDT) that predicts important network parameters such as task arrivals, vehicle\nmobility, and channel quality. A Large Language Model (LLM) that serves as a\ncognitive orchestrator is at the heart of our framework. It makes proactive,\nforward-looking decisions about task offloading and resource allocation by\nutilizing the pDT's forecasts. The LLM's ability to decipher high-level\nsemantic commands given in natural language is crucial because it enables it to\ndynamically modify its optimization policy to match evolving strategic\nobjectives, like giving emergency services priority or optimizing energy\nefficiency. We show through extensive simulations that SP-LLM performs\nsignificantly better in terms of scalability, robustness in volatile\nconditions, and adaptability than state-of-the-art reactive and MARL-based\napproaches. More intelligent, autonomous, and goal-driven vehicular networks\nwill be possible due to our framework's outstanding capacity to convert human\nintent into optimal network behavior.", "AI": {"tldr": "\u63d0\u51faSP-LLM\u6846\u67b6\uff0c\u5c06\u9884\u6d4b\u6570\u5b57\u5b6a\u751f\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u5b9e\u73b0\u8f66\u8f7d\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u4efb\u52a1\u5378\u8f7d\u548c\u8d44\u6e90\u5206\u914d\u7684\u8bed\u4e49\u611f\u77e5\u3001\u4e3b\u52a8\u5f0f\u4f18\u5316\u3002", "motivation": "\u5f53\u524d\u8f66\u8f7d\u8fb9\u7f18\u8ba1\u7b97\uff08VEC\uff09\u7ba1\u7406\u7cfb\u7edf\u662f\u56fa\u5b9a\u7684\u3001\u53cd\u5e94\u5f0f\u7684\uff0c\u5728\u52a8\u6001\u8f66\u8f7d\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u5176\u53d7\u9650\u4e8e\u9759\u6001\u4f18\u5316\u76ee\u6807\u4e14\u4ec5\u57fa\u4e8e\u5f53\u524d\u7f51\u7edc\u72b6\u6001\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u611f\u77e5\u4e3b\u52a8\u5f0fLLM\u7f16\u6392\uff08SP-LLM\uff09\u6846\u67b6\u3002\u901a\u8fc7\u5c06\u4f20\u7edf\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u8f6c\u6362\u4e3a\u9884\u6d4b\u6570\u5b57\u5b6a\u751f\uff08pDT\uff09\u6765\u9884\u6d4b\u5173\u952e\u7f51\u7edc\u53c2\u6570\uff08\u5982\u4efb\u52a1\u5230\u8fbe\u3001\u8f66\u8f86\u79fb\u52a8\u3001\u4fe1\u9053\u8d28\u91cf\uff09\u3002\u6838\u5fc3\u662f\u4e00\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4f5c\u4e3a\u8ba4\u77e5\u7f16\u6392\u5668\uff0c\u5229\u7528pDT\u7684\u9884\u6d4b\u8fdb\u884c\u4e3b\u52a8\u5f0f\u4efb\u52a1\u5378\u8f7d\u548c\u8d44\u6e90\u5206\u914d\u51b3\u7b56\u3002LLM\u80fd\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u7684\u8bed\u4e49\u547d\u4ee4\uff0c\u52a8\u6001\u8c03\u6574\u4f18\u5316\u7b56\u7565\u4ee5\u9002\u5e94\u53d8\u5316\u7684\u76ee\u6807\uff08\u5982\u7d27\u6025\u670d\u52a1\u4f18\u5148\u3001\u80fd\u6548\u4f18\u5316\uff09\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u4eff\u771f\u8bc1\u660e\uff0cSP-LLM\u5728\u53ef\u6269\u5c55\u6027\u3001\u6ce2\u52a8\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u53cd\u5e94\u5f0f\u548c\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u5c06\u4eba\u7c7b\u610f\u56fe\u8f6c\u5316\u4e3a\u6700\u4f73\u7f51\u7edc\u884c\u4e3a\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u667a\u80fd\u3001\u81ea\u4e3b\u548c\u76ee\u6807\u9a71\u52a8\u7684\u8f66\u8f7d\u7f51\u7edc\u3002"}}
{"id": "2508.09292", "pdf": "https://arxiv.org/pdf/2508.09292", "abs": "https://arxiv.org/abs/2508.09292", "authors": ["Sundong Kim"], "title": "The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards", "categories": ["cs.AI"], "comment": null, "summary": "The ability to rapidly adapt to novel and unforeseen environmental changes is\na cornerstone of artificial general intelligence (AGI), yet it remains a\ncritical blind spot in most existing AI benchmarks. Traditional evaluation\nlargely focuses on optimizing performance within fixed environments, failing to\nassess systems' flexibility and generalization capabilities when faced with\neven subtle rule or structural modifications. Addressing this gap, I introduce\nthe Othello AI Arena, a novel benchmark framework designed to evaluate\nintelligent systems based on their capacity for limited-time adaptation to\nunseen environments. Our platform poses a meta-learning challenge: participants\nmust develop systems that can analyze the specific configuration and rules of a\nnovel Othello board within a strict time limit (60 seconds) and generate a\ntailored, high-performing strategy for that unique environment. With this,\nevaluation of the meta-level intelligence can be separated from the task-level\nstrategy performance. The Arena features a diverse set of game stages,\nincluding public stages for development and private stages with structural and\nrule variations designed to test genuine adaptive and generalization\ncapabilities. Implemented as an accessible web-based platform, the Arena\nprovides real-time visualization, automated evaluation using multi-dimensional\nmetrics, and comprehensive logging for post-hoc analysis. Initial observations\nfrom pilot tests and preliminary student engagements highlight fascinating\npatterns in adaptation approaches, ranging from rapid parameter tuning to\nrudimentary environmental model learning through simulation. The Othello AI\nArena offers a unique educational tool and a valuable research benchmark for\nfostering and evaluating the crucial skill of rapid, intelligent adaptation in\nAI systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecdOthello AI Arena\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u7cfb\u7edf\u5728\u9762\u5bf9\u672a\u89c1\u73af\u5883\u65f6\uff0c\u80fd\u5426\u5728\u6709\u9650\u65f6\u95f4\u5185\u8fdb\u884c\u5feb\u901f\u9002\u5e94\u548c\u751f\u6210\u6709\u6548\u7b56\u7565\uff0c\u4ee5\u5f25\u8865\u73b0\u6709AI\u57fa\u51c6\u5728\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u9002\u5e94\u6027\u8bc4\u4f30\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709AI\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5728\u56fa\u5b9a\u73af\u5883\u4e2d\u7684\u6027\u80fd\u4f18\u5316\uff0c\u672a\u80fd\u6709\u6548\u8bc4\u4f30AI\u7cfb\u7edf\u9762\u5bf9\u7ec6\u5fae\u89c4\u5219\u6216\u7ed3\u6784\u53d8\u5316\u65f6\u7684\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5feb\u901f\u9002\u5e94\u65b0\u9896\u4e14\u4e0d\u53ef\u9884\u89c1\u7684\u73af\u5883\u53d8\u5316\u662fAGI\u7684\u57fa\u77f3\uff0c\u4f46\u5728\u73b0\u6709AI\u57fa\u51c6\u4e2d\u4ecd\u662f\u5173\u952e\u7684\u76f2\u70b9\u3002", "method": "\u5f15\u5165\u201cOthello AI Arena\u201d\uff0c\u4e00\u4e2a\u5143\u5b66\u4e60\u6311\u6218\u5e73\u53f0\u3002\u53c2\u4e0e\u8005\u9700\u5f00\u53d1\u7cfb\u7edf\uff0c\u5728\u4e25\u683c\u7684\u65f6\u95f4\u9650\u5236\uff0860\u79d2\uff09\u5185\u5206\u6790\u65b0\u578b\u5965\u8d5b\u7f57\u68cb\u76d8\u7684\u7279\u5b9a\u914d\u7f6e\u548c\u89c4\u5219\uff0c\u5e76\u4e3a\u5176\u751f\u6210\u91cf\u8eab\u5b9a\u5236\u7684\u9ad8\u6027\u80fd\u7b56\u7565\u3002\u8be5\u5e73\u53f0\u5177\u6709\u591a\u6837\u7684\u6e38\u620f\u9636\u6bb5\uff08\u5305\u62ec\u516c\u5f00\u548c\u79c1\u6709\u9636\u6bb5\uff0c\u540e\u8005\u5305\u542b\u7ed3\u6784\u548c\u89c4\u5219\u53d8\u4f53\uff09\uff0c\u91c7\u7528\u57fa\u4e8e\u7f51\u7edc\u7684\u5b9e\u73b0\u65b9\u5f0f\uff0c\u63d0\u4f9b\u5b9e\u65f6\u53ef\u89c6\u5316\u3001\u591a\u7ef4\u5ea6\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u548c\u5168\u9762\u7684\u65e5\u5fd7\u8bb0\u5f55\u3002", "result": "\u521d\u6b65\u7684\u8bd5\u70b9\u6d4b\u8bd5\u548c\u5b66\u751f\u53c2\u4e0e\u89c2\u5bdf\u5230\u6709\u8da3\u7684\u9002\u5e94\u65b9\u6cd5\u6a21\u5f0f\uff0c\u5305\u62ec\u4ece\u5feb\u901f\u53c2\u6570\u8c03\u6574\u5230\u901a\u8fc7\u6a21\u62df\u8fdb\u884c\u7684\u521d\u6b65\u73af\u5883\u6a21\u578b\u5b66\u4e60\u3002", "conclusion": "Othello AI Arena\u63d0\u4f9b\u4e86\u4e00\u4e2a\u72ec\u7279\u7684\u6559\u80b2\u5de5\u5177\u548c\u6709\u4ef7\u503c\u7684\u7814\u7a76\u57fa\u51c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u548c\u8bc4\u4f30AI\u7cfb\u7edf\u4e2d\u5feb\u901f\u3001\u667a\u80fd\u9002\u5e94\u7684\u5173\u952e\u80fd\u529b\u3002"}}
{"id": "2508.09145", "pdf": "https://arxiv.org/pdf/2508.09145", "abs": "https://arxiv.org/abs/2508.09145", "authors": ["Xingle Xu", "Yongkang Liu", "Dexian Cai", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yifei Zhang"], "title": "MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal Sentiment Analysis aims to integrate information from various\nmodalities, such as audio, visual, and text, to make complementary predictions.\nHowever, it often struggles with irrelevant or misleading visual and auditory\ninformation. Most existing approaches typically treat the entire modality\ninformation (e.g., a whole image, audio segment, or text paragraph) as an\nindependent unit for feature enhancement or denoising. They often suppress the\nredundant and noise information at the risk of losing critical information. To\naddress this challenge, we propose MoLAN, a unified ModaLity-aware noise\ndynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking\nby dividing the features of each modality into multiple blocks. Each block is\nthen dynamically assigned a distinct denoising strength based on its noise\nlevel and semantic relevance, enabling fine-grained noise suppression while\npreserving essential multimodal information. Notably, MoLAN is a unified and\nflexible framework that can be seamlessly integrated into a wide range of\nmultimodal models. Building upon this framework, we further introduce MoLAN+, a\nnew multimodal sentiment analysis approach. Experiments across five models and\nfour datasets demonstrate the broad effectiveness of the MoLAN framework.\nExtensive evaluations show that MoLAN+ achieves the state-of-the-art\nperformance. The code is publicly available at\nhttps://github.com/betterfly123/MoLAN-Framework.", "AI": {"tldr": "\u9488\u5bf9\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u566a\u58f0\u5e72\u6270\u548c\u5173\u952e\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faMoLAN\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u3001\u6a21\u6001\u611f\u77e5\u7684\u52a8\u6001\u53bb\u566a\uff0c\u6709\u6548\u6291\u5236\u566a\u58f0\u5e76\u4fdd\u7559\u5173\u952e\u4fe1\u606f\uff0cMoLAN+\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u5e38\u53d7\u65e0\u5173\u6216\u8bef\u5bfc\u6027\u89c6\u89c9\u548c\u542c\u89c9\u4fe1\u606f\u5e72\u6270\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u6574\u4e2a\u6a21\u6001\u4fe1\u606f\u4f5c\u4e3a\u72ec\u7acb\u5355\u5143\u8fdb\u884c\u53bb\u566a\uff0c\u8fd9\u53ef\u80fd\u5728\u6291\u5236\u5197\u4f59\u4fe1\u606f\u7684\u540c\u65f6\u4e22\u5931\u5173\u952e\u4fe1\u606f\u3002", "method": "\u63d0\u51faMoLAN\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u6001\u611f\u77e5\u5206\u5757\u5c06\u6bcf\u79cd\u6a21\u6001\u7279\u5f81\u5212\u5206\u4e3a\u591a\u5757\u3002\u4f9d\u636e\u6bcf\u5757\u7684\u566a\u58f0\u6c34\u5e73\u548c\u8bed\u4e49\u76f8\u5173\u6027\u52a8\u6001\u5206\u914d\u53bb\u566a\u5f3a\u5ea6\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u566a\u58f0\u6291\u5236\u5e76\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u3002MoLAN\u662f\u7edf\u4e00\u4e14\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u53ef\u96c6\u6210\u81f3\u591a\u79cd\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51faMoLAN+\u3002", "result": "\u5728\u4e94\u79cd\u6a21\u578b\u548c\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86MoLAN\u6846\u67b6\u7684\u5e7f\u6cdb\u6709\u6548\u6027\u3002MoLAN+\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MoLAN\u6846\u67b6\u901a\u8fc7\u7ec6\u7c92\u5ea6\u3001\u6a21\u6001\u611f\u77e5\u7684\u52a8\u6001\u53bb\u566a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u7684\u566a\u58f0\u5e72\u6270\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5176\u666e\u9002\u6027\u548c\u4f18\u8d8a\u6027\uff0cMoLAN+\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2508.09178", "pdf": "https://arxiv.org/pdf/2508.09178", "abs": "https://arxiv.org/abs/2508.09178", "authors": ["Yanhui Li", "Yunkang Cao", "Chengliang Liu", "Yuan Xiong", "Xinghui Dong", "Chao Huang"], "title": "IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Industrial anomaly detection is a critical component of modern manufacturing,\nyet the scarcity of defective samples restricts traditional detection methods\nto scenario-specific applications. Although Vision-Language Models (VLMs)\ndemonstrate significant advantages in generalization capabilities, their\nperformance in industrial anomaly detection remains limited. To address this\nchallenge, we propose IAD-R1, a universal post-training framework applicable to\nVLMs of different architectures and parameter scales, which substantially\nenhances their anomaly detection capabilities. IAD-R1 employs a two-stage\ntraining strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT)\nstage utilizes a meticulously constructed high-quality Chain-of-Thought dataset\n(Expert-AD) for training, enhancing anomaly perception capabilities and\nestablishing reasoning-to-answer correlations; the Structured Control Group\nRelative Policy Optimization (SC-GRPO) stage employs carefully designed reward\nfunctions to achieve a capability leap from \"Anomaly Perception\" to \"Anomaly\nInterpretation\". Experimental results demonstrate that IAD-R1 achieves\nsignificant improvements across 7 VLMs, attaining up to 43.3% enhancement in\naverage accuracy on 6 industrial anomaly detection benchmark datasets. Notably,\nthe 0.5B parameter model trained with IAD-R1 surpasses commercial models\nincluding GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the\neffectiveness and superiority of IAD-R1. The dataset, code, and all model\nweights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.", "AI": {"tldr": "\u63d0\u51faIAD-R1\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u663e\u8457\u63d0\u5347VLMs\u5728\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6cdb\u5316\u548c\u89e3\u91ca\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u5c0f\u6837\u672c\u9650\u5236\uff0c\u5e76\u8d85\u8d8a\u5546\u4e1a\u5927\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u53d7\u9650\u4e8e\u7f3a\u9677\u6837\u672c\u7a00\u7f3a\uff0c\u9002\u7528\u573a\u666f\u53d7\u9650\u3002\u5c3d\u7ba1\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5177\u6709\u6cdb\u5316\u4f18\u52bf\uff0c\u4f46\u5728\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u9886\u57df\u7684\u6027\u80fd\u4ecd\u6709\u5f85\u63d0\u5347\u3002", "method": "\u63d0\u51faIAD-R1\u901a\u7528\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u4e0d\u540cVLM\u67b6\u6784\u548c\u53c2\u6570\u89c4\u6a21\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a1) \u611f\u77e5\u6fc0\u6d3b\u76d1\u7763\u5fae\u8c03\uff08PA-SFT\uff09\uff0c\u5229\u7528\u9ad8\u8d28\u91cfChain-of-Thought\u6570\u636e\u96c6(Expert-AD)\u8bad\u7ec3\uff0c\u589e\u5f3a\u5f02\u5e38\u611f\u77e5\u4e0e\u63a8\u7406\u5173\u8054\u30022) \u7ed3\u6784\u5316\u5bf9\u7167\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08SC-GRPO\uff09\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\uff0c\u5b9e\u73b0\u4ece\u201c\u5f02\u5e38\u611f\u77e5\u201d\u5230\u201c\u5f02\u5e38\u89e3\u91ca\u201d\u7684\u80fd\u529b\u63d0\u5347\u3002", "result": "IAD-R1\u57287\u4e2aVLM\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u57286\u4e2a\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad843.3%\u3002\u7ecfIAD-R1\u8bad\u7ec3\u76840.5B\u53c2\u6570\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u6027\u80fd\u8d85\u8d8a\u4e86GPT-4.1\u548cClaude-Sonnet-4\u7b49\u5546\u4e1a\u5927\u6a21\u578b\u3002", "conclusion": "IAD-R1\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86VLM\u5728\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u4e2d\u6027\u80fd\u53d7\u9650\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5176\u6cdb\u5316\u548c\u89e3\u91ca\u80fd\u529b\uff0c\u5373\u4f7f\u662f\u5c0f\u578b\u6a21\u578b\u4e5f\u80fd\u8d85\u8d8a\u73b0\u6709\u5546\u4e1a\u5927\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.09350", "pdf": "https://arxiv.org/pdf/2508.09350", "abs": "https://arxiv.org/abs/2508.09350", "authors": ["Ju-Chieh Chou", "Jiawei Zhou", "Karen Livescu"], "title": "Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling", "categories": ["cs.CL"], "comment": "Accepted to ASRU 2025", "summary": "Textless spoken language models (SLMs) are generative models of speech that\ndo not rely on text supervision. Most textless SLMs learn to predict the next\nsemantic token, a discrete representation of linguistic content, and rely on a\nseparate vocoder to add acoustic information to the generated speech. Such\nmodels have no access to acoustic context and no built-in control over acoustic\ndetails. In this work, we propose to jointly model linguistic and acoustic\ninformation by generating semantic tokens and a continuous real-valued\nrepresentation of the acoustic frame. We use a flow-matching objective to\npredict the continuous vector conditioned on the semantic tokens. We study the\ndesign space of this approach and find that predicting multiple future semantic\ntokens helps preserve linguistic information. Our approach achieves comparable\nperformance to existing models in terms of linguistic likelihood benchmarks,\nwhile providing better acoustic detail in prompted generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65e0\u6587\u672c\u8bed\u97f3\u6a21\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u8bed\u8a00\u548c\u58f0\u5b66\u4fe1\u606f\uff0c\u5728\u4fdd\u6301\u8bed\u8a00\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u7684\u58f0\u5b66\u7ec6\u8282\u3002", "motivation": "\u73b0\u6709\u65e0\u6587\u672c\u8bed\u97f3\u6a21\u578b\u4ec5\u9884\u6d4b\u8bed\u4e49token\uff0c\u5e76\u4f9d\u8d56\u72ec\u7acb\u7684\u58f0\u7801\u5668\u6765\u6dfb\u52a0\u58f0\u5b66\u4fe1\u606f\uff0c\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u8bbf\u95ee\u58f0\u5b66\u4e0a\u4e0b\u6587\uff0c\u4e5f\u65e0\u6cd5\u6709\u6548\u63a7\u5236\u58f0\u5b66\u7ec6\u8282\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u8054\u5408\u5efa\u6a21\u8bed\u8a00\u4fe1\u606f\uff08\u8bed\u4e49token\uff09\u548c\u58f0\u5b66\u4fe1\u606f\uff08\u58f0\u5b66\u5e27\u7684\u8fde\u7eed\u5b9e\u503c\u8868\u793a\uff09\u3002\u5177\u4f53\u65b9\u6cd5\u662f\u4f7f\u7528\u6d41\u5339\u914d\u76ee\u6807\uff0c\u6839\u636e\u8bed\u4e49token\u9884\u6d4b\u8fde\u7eed\u5411\u91cf\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u9884\u6d4b\u591a\u4e2a\u672a\u6765\u8bed\u4e49token\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u4fdd\u7559\u8bed\u8a00\u4fe1\u606f\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8bed\u8a00\u4f3c\u7136\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u4e0e\u73b0\u6709\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u63d0\u793a\u751f\u6210\u4e2d\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u58f0\u5b66\u7ec6\u8282\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u8bed\u8a00\u548c\u58f0\u5b66\u4fe1\u606f\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u65e0\u6587\u672c\u8bed\u97f3\u6a21\u578b\u7684\u58f0\u5b66\u7ec6\u8282\u751f\u6210\u80fd\u529b\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u5176\u8bed\u8a00\u5185\u5bb9\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.09150", "pdf": "https://arxiv.org/pdf/2508.09150", "abs": "https://arxiv.org/abs/2508.09150", "authors": ["Pietro Piscione", "Leonardo Lossi", "Maziar Nekovee", "Chathura Galkandage", "Phil O Connor", "Simon Davies"], "title": "Enabling On-demand Guaranteed QoS for Real Time Video Streaming from Vehicles in 5G Advanced with CAPIF & NEF APIs", "categories": ["cs.NI"], "comment": "Published in the Proceedings of 2025 EuCNC & 6G Summit, Pozna\\'n,\n  Poland, 3-6 June 2025", "summary": "This paper presents the design and implementation of a Proof of Concept (PoC)\nthat demonstrates how 5G Advanced Network Functions can be integrated with the\nCommon API Framework (CAPIF) to support enhanced connectivity for automotive\napplications. The PoC shows the continuous monitoring of the mobile network\nperformance and the on-demand and dynamic adaptation of Quality of Service\n(QoS) for selected 5G User Equipment (UE) video streaming traffic flows using\nstandard 3GPP Network Exposure Function (NEF) APIs exposed via CAPIF. Moreover,\ntraffic flows are redirected to the edge to improve latency and optimize\nnetwork resource utilization.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\uff08PoC\uff09\uff0c\u6f14\u793a\u4e86\u5982\u4f55\u5c065G\u9ad8\u7ea7\u7f51\u7edc\u529f\u80fd\u4e0eCAPIF\u96c6\u6210\uff0c\u4ee5\u589e\u5f3a\u6c7d\u8f66\u5e94\u7528\u7684\u8fde\u63a5\u6027\u3002", "motivation": "\u4e3a\u6c7d\u8f66\u5e94\u7528\u63d0\u4f9b\u589e\u5f3a\u7684\u8fde\u63a5\u6027\uff0c\u5e76\u4f18\u53165G\u7f51\u7edc\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\uff08PoC\uff09\u3002\u8be5PoC\u901a\u8fc7CAPIF\u66b4\u9732\u7684\u6807\u51c63GPP NEF API\uff0c\u5b9e\u73b0\u5bf9\u79fb\u52a8\u7f51\u7edc\u6027\u80fd\u7684\u6301\u7eed\u76d1\u63a7\u548c5G\u7528\u6237\u8bbe\u5907\u89c6\u9891\u6d41\u7684\u6309\u9700\u52a8\u6001QoS\u8c03\u6574\u3002\u6b64\u5916\uff0c\u8fd8\u5b9e\u73b0\u4e86\u6d41\u91cf\u91cd\u5b9a\u5411\u81f3\u8fb9\u7f18\u4ee5\u4f18\u5316\u6027\u80fd\u3002", "result": "\u8be5PoC\u6210\u529f\u5c55\u793a\u4e86\u79fb\u52a8\u7f51\u7edc\u6027\u80fd\u7684\u6301\u7eed\u76d1\u63a7\u3001\u7279\u5b9a5G\u7528\u6237\u8bbe\u5907\u89c6\u9891\u6d41\u7684QoS\u52a8\u6001\u8c03\u6574\u3001\u4ee5\u53ca\u901a\u8fc7\u8fb9\u7f18\u91cd\u5b9a\u5411\u6539\u5584\u5ef6\u8fdf\u548c\u4f18\u5316\u7f51\u7edc\u8d44\u6e90\u5229\u7528\u3002", "conclusion": "\u901a\u8fc7\u5c065G\u9ad8\u7ea7\u7f51\u7edc\u529f\u80fd\u4e0eCAPIF\u96c6\u6210\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u4e3a\u6c7d\u8f66\u5e94\u7528\u63d0\u4f9b\u589e\u5f3a\u7684\u8fde\u63a5\u6027\uff0c\u5e76\u901a\u8fc7\u52a8\u6001QoS\u8c03\u6574\u548c\u8fb9\u7f18\u8ba1\u7b97\u4f18\u5316\u7f51\u7edc\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u3002"}}
{"id": "2508.09507", "pdf": "https://arxiv.org/pdf/2508.09507", "abs": "https://arxiv.org/abs/2508.09507", "authors": ["Meiping Wang", "Jian Zhong", "Rongduo Han", "Liming Kang", "Zhengkun Shi", "Xiao Liang", "Xing Lin", "Nan Gao", "Haining Zhang"], "title": "An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants", "categories": ["cs.AI"], "comment": null, "summary": "With the rapid development of mobile intelligent assistant technologies,\nmulti-modal AI assistants have become essential interfaces for daily user\ninteractions. However, current evaluation methods face challenges including\nhigh manual costs, inconsistent standards, and subjective bias. This paper\nproposes an automated multi-modal evaluation framework based on large language\nmodels and multi-agent collaboration. The framework employs a three-tier agent\narchitecture consisting of interaction evaluation agents, semantic verification\nagents, and experience decision agents. Through supervised fine-tuning on the\nQwen3-8B model, we achieve a significant evaluation matching accuracy with\nhuman experts. Experimental results on eight major intelligent agents\ndemonstrate the framework's effectiveness in predicting users' satisfaction and\nidentifying generation defects.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u81ea\u52a8\u5316\u591a\u6a21\u6001AI\u52a9\u624b\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u9762\u4e34\u7684\u6311\u6218\uff0c\u5e76\u80fd\u51c6\u786e\u9884\u6d4b\u7528\u6237\u6ee1\u610f\u5ea6\u53ca\u8bc6\u522b\u751f\u6210\u7f3a\u9677\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001AI\u52a9\u624b\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u4eba\u5de5\u6210\u672c\u9ad8\u3001\u6807\u51c6\u4e0d\u4e00\u81f4\u548c\u4e3b\u89c2\u504f\u89c1\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u53d1\u5c55\u548c\u5e94\u7528\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u5316\u591a\u6a21\u6001\u8bc4\u4f30\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u3002\u8be5\u6846\u67b6\u91c7\u7528\u4e09\u5c42\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5305\u62ec\u4ea4\u4e92\u8bc4\u4f30\u667a\u80fd\u4f53\u3001\u8bed\u4e49\u9a8c\u8bc1\u667a\u80fd\u4f53\u548c\u7ecf\u9a8c\u51b3\u7b56\u667a\u80fd\u4f53\u3002\u901a\u8fc7\u5bf9Qwen3-8B\u6a21\u578b\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u4ee5\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "result": "\u8be5\u6846\u67b6\u7684\u8bc4\u4f30\u7ed3\u679c\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5177\u6709\u663e\u8457\u7684\u5339\u914d\u51c6\u786e\u5ea6\u3002\u5728\u5bf9\u516b\u4e2a\u4e3b\u6d41\u667a\u80fd\u52a9\u624b\u7684\u5b9e\u9a8c\u4e2d\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u9884\u6d4b\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u8bc6\u522b\u751f\u6210\u7f3a\u9677\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u591a\u6a21\u6001\u8bc4\u4f30\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u51c6\u786e\u3001\u5ba2\u89c2\u7684AI\u52a9\u624b\u8bc4\u4f30\u65b9\u6848\uff0c\u80fd\u591f\u9884\u6d4b\u7528\u6237\u6ee1\u610f\u5ea6\u5e76\u53d1\u73b0\u5176\u6f5c\u5728\u7f3a\u9677\u3002"}}
{"id": "2508.09146", "pdf": "https://arxiv.org/pdf/2508.09146", "abs": "https://arxiv.org/abs/2508.09146", "authors": ["Shugang Hao", "Hongbo Li", "Lingjie Duan"], "title": "To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA", "categories": ["cs.LG", "cs.AI", "cs.NI"], "comment": null, "summary": "The binary exponential backoff scheme is widely used in WiFi 7 and still\nincurs poor throughput performance under dynamic channel environments. Recent\nmodel-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply\noptimize backoff strategies under a known and fixed node density, still leading\nto a large throughput loss due to inaccurate node density estimation. This\npaper is the first to propose LLM transformer-based in-context learning (ICL)\ntheory for optimizing channel access. We design a transformer-based ICL\noptimizer to pre-collect collision-threshold data examples and a query\ncollision case. They are constructed as a prompt as the input for the\ntransformer to learn the pattern, which then generates a predicted contention\nwindow threshold (CWT). To train the transformer for effective ICL, we develop\nan efficient algorithm and guarantee a near-optimal CWT prediction within\nlimited training steps. As it may be hard to gather perfect data examples for\nICL in practice, we further extend to allow erroneous data input in the prompt.\nWe prove that our optimizer maintains minimal prediction and throughput\ndeviations from the optimal values. Experimental results on NS-3 further\ndemonstrate our approach's fast convergence and near-optimal throughput over\nexisting model-based and DRL-based approaches under unknown node densities.", "AI": {"tldr": "\u5229\u7528\u57fa\u4e8eLLM Transformer\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u7406\u8bba\uff0c\u63d0\u51fa\u4e00\u79cd\u4f18\u5316WiFi 7\u4fe1\u9053\u63a5\u5165\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e0b\u541e\u5410\u91cf\u4f4e\u548c\u8282\u70b9\u5bc6\u5ea6\u4f30\u8ba1\u4e0d\u51c6\u7684\u95ee\u9898\u3002", "motivation": "WiFi 7\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u4e8c\u8fdb\u5236\u6307\u6570\u9000\u907f\uff08BEB\uff09\u65b9\u6848\u5728\u52a8\u6001\u4fe1\u9053\u73af\u5883\u4e0b\u541e\u5410\u91cf\u6027\u80fd\u8f83\u5dee\uff1b\u73b0\u6709\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u5df2\u77e5\u56fa\u5b9a\u8282\u70b9\u5bc6\u5ea6\u4e0b\u4f18\u5316\uff0c\u4f46\u8282\u70b9\u5bc6\u5ea6\u4f30\u8ba1\u4e0d\u51c6\u786e\u5bfc\u81f4\u663e\u8457\u7684\u541e\u5410\u91cf\u635f\u5931\u3002", "method": "\u9996\u6b21\u63d0\u51fa\u57fa\u4e8eLLM Transformer\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u7406\u8bba\u7528\u4e8e\u4f18\u5316\u4fe1\u9053\u63a5\u5165\u3002\u8bbe\u8ba1Transformer-based ICL\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b\u78b0\u649e\u9608\u503c\u6570\u636e\u793a\u4f8b\u548c\u67e5\u8be2\u78b0\u649e\u6848\u4f8b\u7684\u63d0\u793a\u4f5c\u4e3aTransformer\u8f93\u5165\uff0c\u4ee5\u751f\u6210\u9884\u6d4b\u7684\u7ade\u4e89\u7a97\u53e3\u9608\u503c\uff08CWT\uff09\u3002\u5f00\u53d1\u9ad8\u6548\u7b97\u6cd5\u8bad\u7ec3Transformer\u4ee5\u5b9e\u73b0\u6709\u6548ICL\uff0c\u5e76\u5728\u6709\u9650\u8bad\u7ec3\u6b65\u6570\u5185\u4fdd\u8bc1\u63a5\u8fd1\u6700\u4f18CWT\u9884\u6d4b\u3002\u8fdb\u4e00\u6b65\u6269\u5c55\u4ee5\u5141\u8bb8\u63d0\u793a\u4e2d\u5b58\u5728\u9519\u8bef\u6570\u636e\u8f93\u5165\uff0c\u5e76\u7406\u8bba\u8bc1\u660e\u5176\u4f18\u5316\u5668\u80fd\u4fdd\u6301\u4e0e\u6700\u4f18\u503c\u7684\u6700\u5c0f\u9884\u6d4b\u548c\u541e\u5410\u91cf\u504f\u5dee\u3002", "result": "\u5728NS-3\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5feb\u901f\u6536\u655b\u6027\uff0c\u5e76\u5728\u672a\u77e5\u8282\u70b9\u5bc6\u5ea6\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u541e\u5410\u91cf\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u6a21\u578b\u548cDRL\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u63d0\u51fa\u4e86\u57fa\u4e8eLLM Transformer\u7684ICL\u7406\u8bba\u6765\u4f18\u5316WiFi 7\u7684\u4fe1\u9053\u63a5\u5165\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u4e0b\u541e\u5410\u91cf\u4f4e\u548c\u8282\u70b9\u5bc6\u5ea6\u4f30\u8ba1\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2508.09185", "pdf": "https://arxiv.org/pdf/2508.09185", "abs": "https://arxiv.org/abs/2508.09185", "authors": ["Rongqian Chen", "Allison Andreyev", "Yanming Xiu", "Mahdi Imani", "Bin Li", "Maria Gorlatova", "Gang Tan", "Tian Lan"], "title": "A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Augmented Reality (AR) enriches perception by overlaying virtual elements on\nthe physical world. Due to its growing popularity, cognitive attacks that alter\nAR content to manipulate users' semantic perception have received increasing\nattention. Existing detection methods often focus on visual changes, which are\nrestricted to pixel- or image-level processing and lack semantic reasoning\ncapabilities, or they rely on pre-trained vision-language models (VLMs), which\nfunction as black-box approaches with limited interpretability. In this paper,\nwe present CADAR, a novel neurosymbolic approach for cognitive attack detection\nin AR. It fuses multimodal vision-language inputs using neural VLMs to obtain a\nsymbolic perception-graph representation, incorporating prior knowledge,\nsalience weighting, and temporal correlations. The model then enables\nparticle-filter based statistical reasoning -- a sequential Monte Carlo method\n-- to detect cognitive attacks. Thus, CADAR inherits the adaptability of\npre-trained VLM and the interpretability and reasoning rigor of particle\nfiltering. Experiments on an extended AR cognitive attack dataset show accuracy\nimprovements of up to 10.7% over strong baselines on challenging AR attack\nscenarios, underscoring the promise of neurosymbolic methods for effective and\ninterpretable cognitive attack detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCADAR\uff0c\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u4e2d\u7684\u8ba4\u77e5\u653b\u51fb\u68c0\u6d4b\u3002\u5b83\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u7c92\u5b50\u6ee4\u6ce2\u8fdb\u884c\u8bed\u4e49\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u548c\u8bed\u4e49\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740AR\u65e5\u76ca\u666e\u53ca\uff0c\u5229\u7528\u4fee\u6539AR\u5185\u5bb9\u6765\u64cd\u7eb5\u7528\u6237\u8bed\u4e49\u611f\u77e5\u7684\u8ba4\u77e5\u653b\u51fb\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\u3002\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u8981\u4e48\u5c40\u9650\u4e8e\u50cf\u7d20\u7ea7/\u56fe\u50cf\u7ea7\u5904\u7406\uff0c\u7f3a\u4e4f\u8bed\u4e49\u63a8\u7406\u80fd\u529b\uff1b\u8981\u4e48\u4f9d\u8d56\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "CADAR\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u878d\u5408\u591a\u6a21\u6001\u89c6\u89c9-\u8bed\u8a00\u8f93\u5165\uff0c\u901a\u8fc7\u795e\u7ecfVLMs\u83b7\u5f97\u7b26\u53f7\u611f\u77e5\u56fe\uff0c\u5e76\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\u3001\u663e\u8457\u6027\u52a0\u6743\u548c\u65f6\u95f4\u5173\u8054\u3002\u968f\u540e\uff0c\u5229\u7528\u57fa\u4e8e\u7c92\u5b50\u6ee4\u6ce2\u7684\u7edf\u8ba1\u63a8\u7406\uff08\u4e00\u79cd\u5e8f\u5217\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\uff09\u6765\u68c0\u6d4b\u8ba4\u77e5\u653b\u51fb\u3002", "result": "\u5728\u6269\u5c55\u7684AR\u8ba4\u77e5\u653b\u51fb\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0cCADAR\u5728\u5177\u6709\u6311\u6218\u6027\u7684AR\u653b\u51fb\u573a\u666f\u4e0b\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u9ad8\u8fbe10.7%\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u5b9e\u73b0\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684AR\u8ba4\u77e5\u653b\u51fb\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002CADAR\u878d\u5408\u4e86\u9884\u8bad\u7ec3VLM\u7684\u9002\u5e94\u6027\u4e0e\u7c92\u5b50\u6ee4\u6ce2\u7684\u53ef\u89e3\u91ca\u6027\u548c\u63a8\u7406\u4e25\u8c28\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u672a\u6765AR\u5b89\u5168\u9886\u57df\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2508.09378", "pdf": "https://arxiv.org/pdf/2508.09378", "abs": "https://arxiv.org/abs/2508.09378", "authors": ["Artem Chernodub", "Aman Saini", "Yejin Huh", "Vivek Kulkarni", "Vipul Raheja"], "title": "APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "Accepted for publication at Recent Advances in Natural Language\n  Processing conference (RANLP 2025)", "summary": "Recent advancements in large language models (LLMs) have enabled a wide range\nof natural language processing (NLP) tasks to be performed through simple\nprompt-based interactions. Consequently, several approaches have been proposed\nto engineer prompts that most effectively enable LLMs to perform a given task\n(e.g., chain-of-thought prompting). In settings with a well-defined metric to\noptimize model performance, automatic prompt optimization (APO) methods have\nbeen developed to refine a seed prompt. Advancing this line of research, we\npropose APIO, a simple but effective prompt induction and optimization approach\nfor the tasks of Grammatical Error Correction (GEC) and Text Simplification,\nwithout relying on manually specified seed prompts. APIO achieves a new\nstate-of-the-art performance for purely LLM-based prompting methods on these\ntasks. We make our data, code, prompts, and outputs publicly available.", "AI": {"tldr": "APIO\u662f\u4e00\u79cd\u65e0\u9700\u624b\u52a8\u6307\u5b9a\u521d\u59cb\u63d0\u793a\u8bcd\u7684\u81ea\u52a8\u63d0\u793a\u8bcd\u751f\u6210\u4e0e\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u8bed\u6cd5\u9519\u8bef\u7ea0\u6b63\u548c\u6587\u672c\u7b80\u5316\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86LLM\u63d0\u793a\u65b9\u6cd5\u7684\u6700\u65b0SOTA\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u8bcd\u4f18\u5316(APO)\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u624b\u52a8\u6307\u5b9a\u7684\u521d\u59cb\u63d0\u793a\u8bcd\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u9884\u8bbe\u63d0\u793a\u8bcd\u7684\u81ea\u52a8\u63d0\u793a\u8bcd\u751f\u6210\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u62d3\u5c55LLM\u5728NLP\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faAPIO (prompt induction and optimization)\uff0c\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u63d0\u793a\u8bcd\u751f\u6210\u4e0e\u4f18\u5316\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u4e8e\u624b\u52a8\u6307\u5b9a\u7684\u521d\u59cb\u63d0\u793a\u8bcd\uff0c\u4e13\u6ce8\u4e8e\u8bed\u6cd5\u9519\u8bef\u7ea0\u6b63(GEC)\u548c\u6587\u672c\u7b80\u5316\u4efb\u52a1\u3002", "result": "APIO\u5728\u8bed\u6cd5\u9519\u8bef\u7ea0\u6b63\u548c\u6587\u672c\u7b80\u5316\u4efb\u52a1\u4e0a\uff0c\u4e3a\u7eaf\u7cb9\u57fa\u4e8eLLM\u7684\u63d0\u793a\u65b9\u6cd5\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "APIO\u4e3aLLM\u5728\u7279\u5b9aNLP\u4efb\u52a1\uff08\u5982GEC\u548c\u6587\u672c\u7b80\u5316\uff09\u4e2d\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3001\u8868\u73b0\u5353\u8d8a\u7684\u63d0\u793a\u8bcd\u751f\u6210\u548c\u4f18\u5316\u8303\u5f0f\uff0c\u63a8\u52a8\u4e86LLM\u63d0\u793a\u65b9\u6cd5\u7684\u8fdb\u6b65\u3002"}}
{"id": "2508.09151", "pdf": "https://arxiv.org/pdf/2508.09151", "abs": "https://arxiv.org/abs/2508.09151", "authors": ["Chang Wu", "Yuang Chen", "Yiyuan Chen", "Fengqian Guo", "Xiaowei Qin", "Hancheng Lu"], "title": "Physiological Signal-Driven QoE Optimization for Wireless Virtual Reality Transmission", "categories": ["cs.NI", "cs.MA"], "comment": "7 pages, 6 figures", "summary": "Abrupt resolution changes in virtual reality (VR) streaming can significantly\nimpair the quality-of-experience (QoE) of users, particularly during\ntransitions from high to low resolutions. Existing QoE models and transmission\nschemes inadequately address the perceptual impact of these shifts. To bridge\nthis gap, this article proposes, for the first time, an innovative\nphysiological signal-driven QoE modeling and optimization framework that fully\nleverages users' electroencephalogram (EEG), electrocardiogram (ECG), and skin\nactivity signals. This framework precisely captures the temporal dynamics of\nphysiological responses and resolution changes in VR streaming, enabling\naccurate quantification of resolution upgrades' benefits and downgrades'\nimpacts. Integrated the proposed QoE framework into the radio access network\n(RAN) via a deep reinforcement learning (DRL) framework, adaptive transmission\nstrategies have been implemented to allocate radio resources dynamically, which\nmitigates short-term channel fluctuations and adjusts frame resolution in\nresponse to channel variations caused by user mobility. By prioritizing\nlong-term resolution while minimizing abrupt transitions, the proposed solution\nachieves an 88.7\\% improvement in resolution and an 81.0\\% reduction in\nhandover over the baseline. Experimental results demonstrate the effectiveness\nof this physiological signal-driven strategy, underscoring the promise of edge\nAI in immersive media services.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u7406\u4fe1\u53f7\u7684VR\u6d41\u5a92\u4f53QoE\u5efa\u6a21\u4e0e\u4f18\u5316\u6846\u67b6\uff0c\u5e76\u7ed3\u5408DRL\u5e94\u7528\u4e8eRAN\uff0c\u4ee5\u7f13\u89e3\u5206\u8fa8\u7387\u7a81\u53d8\u5e76\u4f18\u5316\u4f20\u8f93\u7b56\u7565\u3002", "motivation": "VR\u6d41\u5a92\u4f53\u4e2d\u5206\u8fa8\u7387\u7684\u5267\u70c8\u53d8\u5316\uff08\u5c24\u5176\u662f\u4ece\u9ad8\u5230\u4f4e\uff09\u4f1a\u4e25\u91cd\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\uff08QoE\uff09\uff0c\u800c\u73b0\u6709QoE\u6a21\u578b\u548c\u4f20\u8f93\u65b9\u6848\u672a\u80fd\u5145\u5206\u89e3\u51b3\u8fd9\u4e9b\u53d8\u5316\u5e26\u6765\u7684\u611f\u77e5\u5f71\u54cd\u3002", "method": "1. \u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u751f\u7406\u4fe1\u53f7\u9a71\u52a8\u7684QoE\u5efa\u6a21\u548c\u4f18\u5316\u6846\u67b6\uff0c\u5145\u5206\u5229\u7528\u7528\u6237\u7684\u8111\u7535\u56fe\uff08EEG\uff09\u3001\u5fc3\u7535\u56fe\uff08ECG\uff09\u548c\u76ae\u80a4\u6d3b\u52a8\u4fe1\u53f7\uff0c\u7cbe\u786e\u6355\u6349\u751f\u7406\u54cd\u5e94\u548c\u5206\u8fa8\u7387\u53d8\u5316\u7684\u65f6\u95f4\u52a8\u6001\u30022. \u5c06\u6240\u63d0\u51fa\u7684QoE\u6846\u67b6\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u96c6\u6210\u5230\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff08RAN\uff09\u4e2d\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u4f20\u8f93\u7b56\u7565\uff0c\u52a8\u6001\u5206\u914d\u65e0\u7ebf\u8d44\u6e90\uff0c\u4ee5\u5e94\u5bf9\u7528\u6237\u79fb\u52a8\u6027\u5f15\u8d77\u7684\u4fe1\u9053\u53d8\u5316\u5e76\u8c03\u6574\u5e27\u5206\u8fa8\u7387\u3002", "result": "\u4e0e\u57fa\u7ebf\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u5b9e\u73b0\u4e86\u5206\u8fa8\u738788.7%\u7684\u63d0\u5347\u548c\u5207\u636281.0%\u7684\u51cf\u5c11\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8fd9\u79cd\u751f\u7406\u4fe1\u53f7\u9a71\u52a8\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u5f3a\u8c03\u4e86\u8fb9\u7f18AI\u5728\u6c89\u6d78\u5f0f\u5a92\u4f53\u670d\u52a1\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.09586", "pdf": "https://arxiv.org/pdf/2508.09586", "abs": "https://arxiv.org/abs/2508.09586", "authors": ["Yang Cheng", "Zilai Wang", "Weiyu Ma", "Wenhui Zhu", "Yue Deng", "Jian Zhao"], "title": "EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse domains, including programming, planning, and decision-making. However,\ntheir performance often degrades when faced with highly complex problem\ninstances that require deep reasoning over long horizons. In such cases, direct\nproblem-solving approaches can lead to inefficiency or failure due to the lack\nof structured intermediate guidance. To address this, we propose a novel\nself-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM\nconstructs a sequence of problem instances with gradually increasing\ndifficulty, tailored to the solver LLM's learning progress. The curriculum\ndynamically adapts easing challenges when the solver struggles and escalating\nthem when success is consistent, thus maintaining an optimal learning\ntrajectory. This approach enables the solver LLM, implemented as a\ncode-generation model producing Python decision-tree scripts, to progressively\nacquire the skills needed for complex decision-making tasks. Experimental\nresults on challenging decision-making benchmarks show that our method\nsignificantly improves task success rates and solution efficiency compared to\ndirect-solving baselines. These findings suggest that LLM-driven curriculum\nlearning holds strong potential for enhancing automated reasoning in\nreal-world, high-complexity domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEvoCurr\u6846\u67b6\uff0c\u5229\u7528\u4e00\u4e2aLLM\u751f\u6210\u52a8\u6001\u9002\u5e94\u7684\u8bfe\u7a0b\uff0c\u9010\u6b65\u63d0\u5347\u53e6\u4e00\u4e2aLLM\u89e3\u51b3\u590d\u6742\u51b3\u7b56\u95ee\u9898\u7684\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u9700\u8981\u6df1\u5ea6\u3001\u957f\u7a0b\u63a8\u7406\u7684\u590d\u6742\u95ee\u9898\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u76f4\u63a5\u89e3\u51b3\u7f3a\u4e4f\u7ed3\u6784\u5316\u4e2d\u95f4\u6307\u5bfc\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u6216\u5931\u8d25\u3002", "method": "\u63d0\u51faEvoCurr\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u5176\u4e2d\u4e00\u4e2a\u8bfe\u7a0b\u751f\u6210LLM\u6839\u636e\u89e3\u9898LLM\u7684\u5b66\u4e60\u8fdb\u5ea6\uff0c\u6784\u5efa\u6e10\u8fdb\u5f0f\u96be\u5ea6\u7684\u4efb\u52a1\u5e8f\u5217\u3002\u8be5\u8bfe\u7a0b\u52a8\u6001\u8c03\u6574\uff0c\u5f53\u89e3\u9898LLM\u9047\u5230\u56f0\u96be\u65f6\u964d\u4f4e\u96be\u5ea6\uff0c\u6210\u529f\u65f6\u63d0\u5347\u96be\u5ea6\u3002\u89e3\u9898LLM\u662f\u4e00\u4e2a\u751f\u6210Python\u51b3\u7b56\u6811\u811a\u672c\u7684\u4ee3\u7801\u751f\u6210\u6a21\u578b\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u51b3\u7b56\u5236\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0e\u76f4\u63a5\u89e3\u51b3\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u89e3\u51b3\u65b9\u6848\u6548\u7387\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u8bfe\u7a0b\u5b66\u4e60\u5728\u589e\u5f3a\u73b0\u5b9e\u4e16\u754c\u4e2d\u9ad8\u590d\u6742\u6027\u9886\u57df\u7684\u81ea\u52a8\u5316\u63a8\u7406\u65b9\u9762\u5177\u6709\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.09148", "pdf": "https://arxiv.org/pdf/2508.09148", "abs": "https://arxiv.org/abs/2508.09148", "authors": ["Junghwan Lim", "Sungmin Lee", "Dongseok Kim", "Eunhwan Park", "Hyunbyung Park", "Junhyeok Lee", "Wai Ting Cheung", "Dahye Choi", "Jaeheui Her", "Jaeyeon Huh", "Hanbin Jung", "Changjin Kang", "Beomgyu Kim", "Jihwan Kim", "Minjae Kim", "Taehwan Kim", "Youngrok Kim", "Haesol Lee", "Jeesoo Lee", "Kungyu Lee", "Dongpin Oh", "Yeongjae Park", "Bokki Ryu", "Daewon Suh", "Dongjoo Weon"], "title": "Motif 2.6B Technical Report", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have revolutionized\nartificial intelligence, yet developing an effective foundational LLM that\nbalances high performance with computational efficiency remains challenging,\nespecially for emerging research groups. To address this gap, we introduce\nMotif-2.6B, a 2.6-billion-parameter foundation model designed to democratize\nadvanced LLM capabilities. Motif-2.6B incorporates several innovative\narchitectural enhancements, including Differential Attention and PolyNorm\nactivation functions, which improve long-context comprehension, reduce\nhallucination, and enhance in-context learning capabilities. We rigorously\ntested multiple novel architectural components through extensive\nexperimentation to determine the optimal architecture for Motif-2.6B.\nComprehensive evaluations demonstrate that Motif-2.6B consistently meets or\nexceeds the performance of similarly sized state-of-the-art models across\ndiverse benchmarks, showcasing its effectiveness, scalability, and real-world\napplicability. Through detailed experiments and tailored techniques, Motif-2.6B\nsignificantly advances the landscape of efficient, scalable, and powerful\nfoundational LLMs, offering valuable insights and a robust foundation for\nfuture research and deployment.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecdMotif-2.6B\uff0c\u4e00\u4e2a2.6\u4ebf\u53c2\u6570\u7684\u57fa\u7840LLM\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u589e\u5f3a\uff08\u5982\u5dee\u5206\u6ce8\u610f\u529b\u3001PolyNorm\uff09\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u9ad8\u6548\u7387\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u540c\u7c7bSOTA\u6a21\u578b\uff0c\u65e8\u5728\u6c11\u4e3b\u5316LLM\u80fd\u529b\u3002", "motivation": "\u5f00\u53d1\u9ad8\u6027\u80fd\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u57fa\u7840\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ecd\u5177\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u65b0\u5174\u7814\u7a76\u56e2\u961f\uff0c\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u5728\u6027\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u5f15\u5165Motif-2.6B\uff08\u4e00\u4e2a26\u4ebf\u53c2\u6570\u7684\u57fa\u7840\u6a21\u578b\uff09\uff0c\u901a\u8fc7\u6574\u5408\u521b\u65b0\u7684\u67b6\u6784\u589e\u5f3a\uff0c\u5305\u62ec\u5dee\u5206\u6ce8\u610f\u529b\uff08Differential Attention\uff09\u548cPolyNorm\u6fc0\u6d3b\u51fd\u6570\uff0c\u4ee5\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u51cf\u5c11\u5e7b\u89c9\u5e76\u589e\u5f3a\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u786e\u5b9a\u6700\u4f18\u67b6\u6784\u3002", "result": "Motif-2.6B\u5728\u5404\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8fbe\u5230\u6216\u8d85\u8d8a\u540c\u7b49\u89c4\u6a21\u7684\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6027\u3002", "conclusion": "Motif-2.6B\u663e\u8457\u63a8\u52a8\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u548c\u5f3a\u5927\u7684\u57fa\u7840LLM\u9886\u57df\u7684\u53d1\u5c55\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2508.09186", "pdf": "https://arxiv.org/pdf/2508.09186", "abs": "https://arxiv.org/abs/2508.09186", "authors": ["Abdolazim Rezaei", "Mehdi Sookhak", "Mahboobeh Haghparast"], "title": "RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The proliferation of AI-powered cameras in Intelligent Transportation Systems\n(ITS) creates a severe conflict between the need for rich visual data and the\nfundamental right to privacy. Existing privacy-preserving mechanisms, such as\nblurring or encryption, are often insufficient, creating an undesirable\ntrade-off where either privacy is compromised against advanced reconstruction\nattacks or data utility is critically degraded. To resolve this impasse, we\npropose RL-MoE, a novel framework that transforms sensitive visual data into\nprivacy-preserving textual descriptions, eliminating the need for direct image\ntransmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture\nfor nuanced, multi-aspect scene decomposition with a Reinforcement Learning\n(RL) agent that optimizes the generated text for a dual objective of semantic\naccuracy and privacy preservation. Extensive experiments demonstrate that\nRL-MoE provides superior privacy protection, reducing the success rate of\nreplay attacks to just 9.4\\% on the CFP-FP dataset, while simultaneously\ngenerating richer textual content than baseline methods. Our work provides a\npractical and scalable solution for building trustworthy AI systems in\nprivacy-sensitive domains, paving the way for more secure smart city and\nautonomous vehicle networks.", "AI": {"tldr": "\u9488\u5bf9\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u89c6\u89c9\u6570\u636e\u4e0e\u9690\u79c1\u7684\u51b2\u7a81\uff0c\u672c\u6587\u63d0\u51faRL-MoE\u6846\u67b6\uff0c\u5c06\u654f\u611f\u89c6\u89c9\u6570\u636e\u8f6c\u5316\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u6587\u672c\u63cf\u8ff0\u3002\u8be5\u6846\u67b6\u7ed3\u5408MoE\u8fdb\u884c\u573a\u666f\u5206\u89e3\u548cRL\u4f18\u5316\u6587\u672c\u751f\u6210\uff0c\u5b9e\u73b0\u5728\u4fdd\u8bc1\u8bed\u4e49\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u6709\u6548\u63d0\u5347\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "AI\u6444\u50cf\u5934\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff08ITS\uff09\u4e2d\u666e\u53ca\uff0c\u5bfc\u81f4\u5bf9\u4e30\u5bcc\u89c6\u89c9\u6570\u636e\u7684\u9700\u6c42\u4e0e\u4e2a\u4eba\u9690\u79c1\u6743\u4e4b\u95f4\u5b58\u5728\u4e25\u91cd\u51b2\u7a81\u3002\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff08\u5982\u6a21\u7cca\u3001\u52a0\u5bc6\uff09\u5f80\u5f80\u4e0d\u8db3\uff0c\u5bfc\u81f4\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u6216\u6570\u636e\u6548\u7528\u663e\u8457\u964d\u4f4e\uff0c\u4e9f\u9700\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51faRL-MoE\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u5c06\u654f\u611f\u89c6\u89c9\u6570\u636e\u8f6c\u6362\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u4ece\u800c\u907f\u514d\u76f4\u63a5\u4f20\u8f93\u56fe\u50cf\u3002\u8be5\u65b9\u6cd5\u72ec\u7279\u5730\u7ed3\u5408\u4e86\uff1a1) \u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u67b6\u6784\uff0c\u7528\u4e8e\u7ec6\u81f4\u3001\u591a\u65b9\u9762\u7684\u573a\u666f\u5206\u89e3\uff1b2) \u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u667a\u80fd\u4f53\uff0c\u4f18\u5316\u751f\u6210\u7684\u6587\u672c\uff0c\u4ee5\u5b9e\u73b0\u8bed\u4e49\u51c6\u786e\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u53cc\u91cd\u76ee\u6807\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRL-MoE\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u9690\u79c1\u4fdd\u62a4\uff0c\u5728CFP-FP\u6570\u636e\u96c6\u4e0a\u5c06\u91cd\u653e\u653b\u51fb\u7684\u6210\u529f\u7387\u964d\u4f4e\u81f3\u4ec59.4%\u3002\u540c\u65f6\uff0c\u5b83\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u751f\u6210\u4e86\u66f4\u4e30\u5bcc\u7684\u6587\u672c\u5185\u5bb9\u3002", "conclusion": "RL-MoE\u4e3a\u5728\u9690\u79c1\u654f\u611f\u9886\u57df\u6784\u5efa\u53ef\u4fe1\u8d56\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u66f4\u5b89\u5168\u7684\u667a\u6167\u57ce\u5e02\u548c\u81ea\u52a8\u9a7e\u9a76\u7f51\u7edc\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.09403", "pdf": "https://arxiv.org/pdf/2508.09403", "abs": "https://arxiv.org/abs/2508.09403", "authors": ["Ting Cai", "Stephen Sheen", "AnHai Doan"], "title": "Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models", "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Expanding the abbreviated column names of tables, such as ``esal'' to\n``employee salary'', is critical for numerous downstream data tasks. This\nproblem arises in enterprises, domain sciences, government agencies, and more.\nIn this paper we make three contributions that significantly advances the state\nof the art. First, we show that synthetic public data used by prior work has\nmajor limitations, and we introduce 4 new datasets in enterprise/science\ndomains, with real-world abbreviations. Second, we show that accuracy measures\nused by prior work seriously undercount correct expansions, and we propose new\nsynonym-aware measures that capture accuracy much more accurately. Finally, we\ndevelop Columbo, a powerful LLM-based solution that exploits context, rules,\nchain-of-thought reasoning, and token-level analysis. Extensive experiments\nshow that Columbo significantly outperforms NameGuess, the current most\nadvanced solution, by 4-29\\%, over 5 datasets. Columbo has been used in\nproduction on EDI, a major data portal for environmental sciences.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u57fa\u4e8eLLM\u7684Columbo\u7cfb\u7edf\uff0c\u7528\u4e8e\u6269\u5c55\u8868\u683c\u5217\u540d\u7f29\u5199\uff0c\u901a\u8fc7\u6539\u8fdb\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5f97\u5230\u5e94\u7528\u3002", "motivation": "\u5c06\u8868\u683c\u4e2d\u7684\u7f29\u5199\u5217\u540d\uff08\u5982\u201cesal\u201d\u5230\u201cemployee salary\u201d\uff09\u6269\u5c55\u4e3a\u5b8c\u6574\u540d\u79f0\uff0c\u5bf9\u4e8e\u591a\u79cd\u4e0b\u6e38\u6570\u636e\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4e14\u8be5\u95ee\u9898\u5e7f\u6cdb\u5b58\u5728\u4e8e\u4f01\u4e1a\u3001\u79d1\u5b66\u548c\u653f\u5e9c\u673a\u6784\u4e2d\u3002", "method": "\u9996\u5148\uff0c\u6307\u51fa\u5148\u524d\u5de5\u4f5c\u7684\u5408\u6210\u6570\u636e\u5c40\u9650\u6027\u5e76\u5f15\u51654\u4e2a\u65b0\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff1b\u5176\u6b21\uff0c\u63ed\u793a\u73b0\u6709\u51c6\u786e\u6027\u5ea6\u91cf\u4f4e\u4f30\u4e86\u6b63\u786e\u6269\u5c55\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u540c\u4e49\u8bcd\u611f\u77e5\u5ea6\u91cf\uff1b\u6700\u540e\uff0c\u5f00\u53d1\u4e86Columbo\uff0c\u4e00\u4e2a\u5229\u7528\u4e0a\u4e0b\u6587\u3001\u89c4\u5219\u3001\u601d\u7ef4\u94fe\u63a8\u7406\u548c\u4ee4\u724c\u7ea7\u5206\u6790\u7684\u5f3a\u5927LLM\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cColumbo\u57285\u4e2a\u6570\u636e\u96c6\u4e0a\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848NameGuess\u6027\u80fd\u663e\u8457\u63d0\u53474-29%\u3002Columbo\u5df2\u5728\u73af\u5883\u79d1\u5b66\u6570\u636e\u95e8\u6237EDI\u4e2d\u6295\u5165\u751f\u4ea7\u4f7f\u7528\u3002", "conclusion": "Columbo\u663e\u8457\u63d0\u5347\u4e86\u5217\u540d\u7f29\u5199\u6269\u5c55\u7684\u51c6\u786e\u6027\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\uff0c\u8bc1\u660e\u4e86\u5176LLM\u9a71\u52a8\u65b9\u6cd5\u7684\u5f3a\u5927\u6709\u6548\u6027\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5b9e\u9645\u751f\u4ea7\u73af\u5883\u3002"}}
{"id": "2508.09152", "pdf": "https://arxiv.org/pdf/2508.09152", "abs": "https://arxiv.org/abs/2508.09152", "authors": ["Joseph H. R. Isaac", "Harish Saradagam", "Nallamothu Pardhasaradhi"], "title": "5G Core Fault Detection and Root Cause Analysis using Machine Learning and Generative AI", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": "8 pages, 3 figures and 2 tables. Accepted in Conference on Advances\n  in Communication Networks & Systems (CoaCoNS 2025)", "summary": "With the advent of 5G networks and technologies, ensuring the integrity and\nperformance of packet core traffic is paramount. During network analysis, test\nfiles such as Packet Capture (PCAP) files and log files will contain errors if\npresent in the system that must be resolved for better overall network\nperformance, such as connectivity strength and handover quality. Current\nmethods require numerous person-hours to sort out testing results and find the\nfaults. This paper presents a novel AI/ML-driven Fault Analysis (FA) Engine\ndesigned to classify successful and faulty frames in PCAP files, specifically\nwithin the 5G packet core. The FA engine analyses network traffic using natural\nlanguage processing techniques to identify anomalies and inefficiencies,\nsignificantly reducing the effort time required and increasing efficiency. The\nFA Engine also suggests steps to fix the issue using Generative AI via a Large\nLanguage Model (LLM) trained on several 5G packet core documents. The engine\nexplains the details of the error from the domain perspective using documents\nsuch as the 3GPP standards and user documents regarding the internal conditions\nof the tests. Test results on the ML models show high classification accuracy\non the test dataset when trained with 80-20 splits for the successful and\nfailed PCAP files. Future scopes include extending the AI engine to incorporate\n4G network traffic and other forms of network data, such as log text files and\nmultimodal systems.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cdAI/ML\u9a71\u52a8\u76845G\u6838\u5fc3\u7f51\u6545\u969c\u5206\u6790\u5f15\u64ce\uff0c\u7528\u4e8e\u5206\u7c7bPCAP\u6587\u4ef6\u4e2d\u7684\u5e27\uff0c\u8bc6\u522b\u5f02\u5e38\uff0c\u5e76\u5229\u7528LLM\u63d0\u4f9b\u4fee\u590d\u5efa\u8bae\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u67095G\u6838\u5fc3\u7f51\u6545\u969c\u5206\u6790\u65b9\u6cd5\u8017\u65f6\u4e14\u9700\u5927\u91cf\u4eba\u5de5\u6295\u5165\uff0c\u5bfc\u81f4\u6545\u969c\u5b9a\u4f4d\u548c\u89e3\u51b3\u6548\u7387\u4f4e\u4e0b\uff0c\u5f71\u54cd\u7f51\u7edc\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2aAI/ML\u9a71\u52a8\u7684\u6545\u969c\u5206\u6790(FA)\u5f15\u64ce\u3002\u8be5\u5f15\u64ce\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)\u6280\u672f\u5206\u67905G\u6838\u5fc3\u7f51PCAP\u6587\u4ef6\u4ee5\u5206\u7c7b\u6210\u529f\u548c\u6545\u969c\u5e27\uff0c\u5e76\u8bc6\u522b\u5f02\u5e38\u3002\u6b64\u5916\uff0c\u5b83\u96c6\u6210\u4e86\u4e00\u4e2a\u57fa\u4e8e5G\u6838\u5fc3\u7f51\u6587\u6863\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\uff0c\u7528\u4e8e\u63d0\u4f9b\u6545\u969c\u4fee\u590d\u5efa\u8bae\u548c\u57fa\u4e8e3GPP\u6807\u51c6\u7b49\u7684\u9519\u8bef\u89e3\u91ca\u3002", "result": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\uff08\u4f7f\u752880-20\u6bd4\u4f8b\u7684\u6210\u529f\u4e0e\u5931\u8d25PCAP\u6587\u4ef6\u8fdb\u884c\u8bad\u7ec3\uff09\u5c55\u73b0\u51fa\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\u3002\u8be5\u5f15\u64ce\u663e\u8457\u51cf\u5c11\u4e86\u6545\u969c\u5206\u6790\u6240\u9700\u7684\u4eba\u5de5\u65f6\u95f4\u5e76\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684AI/ML\u9a71\u52a8FA\u5f15\u64ce\u80fd\u591f\u9ad8\u6548\u51c6\u786e\u5730\u8bc6\u522b5G\u6838\u5fc3\u7f51\u6d41\u91cf\u4e2d\u7684\u6545\u969c\uff0c\u5e76\u63d0\u4f9b\u667a\u80fd\u4fee\u590d\u65b9\u6848\uff0c\u4ece\u800c\u5927\u5e45\u63d0\u9ad8\u7f51\u7edc\u5206\u6790\u6548\u7387\u3002\u672a\u6765\u5de5\u4f5c\u5305\u62ec\u5c06\u5176\u6269\u5c55\u81f34G\u7f51\u7edc\u53ca\u5176\u4ed6\u5f62\u5f0f\u7684\u7f51\u7edc\u6570\u636e\u3002"}}
{"id": "2508.09639", "pdf": "https://arxiv.org/pdf/2508.09639", "abs": "https://arxiv.org/abs/2508.09639", "authors": ["Akshat Dubey", "Aleksandar An\u017eel", "Bahar \u0130lgen", "Georges Hattab"], "title": "UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles", "categories": ["cs.AI"], "comment": null, "summary": "Explainable Artificial Intelligence (XAI) techniques, such as SHapley\nAdditive exPlanations (SHAP), have become essential tools for interpreting\ncomplex ensemble tree-based models, especially in high-stakes domains such as\nhealthcare analytics. However, SHAP values are usually treated as point\nestimates, which disregards the inherent and ubiquitous uncertainty in\npredictive models and data. This uncertainty has two primary sources: aleatoric\nand epistemic. The aleatoric uncertainty, which reflects the irreducible noise\nin the data. The epistemic uncertainty, which arises from a lack of data. In\nthis work, we propose an approach for decomposing uncertainty in SHAP values\ninto aleatoric, epistemic, and entanglement components. This approach\nintegrates Dempster-Shafer evidence theory and hypothesis sampling via\nDirichlet processes over tree ensembles. We validate the method across three\nreal-world use cases with descriptive statistical analyses that provide insight\ninto the nature of epistemic uncertainty embedded in SHAP explanations. The\nexperimentations enable to provide more comprehensive understanding of the\nreliability and interpretability of SHAP-based attributions. This understanding\ncan guide the development of robust decision-making processes and the\nrefinement of models in high-stakes applications. Through our experiments with\nmultiple datasets, we concluded that features with the highest SHAP values are\nnot necessarily the most stable. This epistemic uncertainty can be reduced\nthrough better, more representative data and following appropriate or\ncase-desired model development techniques. Tree-based models, especially\nbagging, facilitate the effective quantification of epistemic uncertainty.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u7ed3\u5408Dempster-Shafer\u7406\u8bba\u548cDirichlet\u8fc7\u7a0b\uff0c\u5c06SHAP\u503c\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u4e3a\u5076\u7136\u3001\u8ba4\u77e5\u548c\u7ea0\u7f20\u6210\u5206\uff0c\u4ee5\u63d0\u9ad8\u9ad8\u98ce\u9669\u9886\u57df\u4e2dSHAP\u89e3\u91ca\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u53d1\u73b0\u9ad8SHAP\u503c\u7279\u5f81\u4e0d\u4e00\u5b9a\u6700\u7a33\u5b9a\u3002", "motivation": "SHAP\u503c\u5728\u89e3\u91ca\u590d\u6742\u96c6\u6210\u6811\u6a21\u578b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u533b\u7597\u4fdd\u5065\uff09\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u88ab\u89c6\u4e3a\u70b9\u4f30\u8ba1\uff0c\u5ffd\u7565\u4e86\u6a21\u578b\u548c\u6570\u636e\u56fa\u6709\u7684\u4e0d\u786e\u5b9a\u6027\uff08\u5305\u62ec\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff09\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5206\u89e3SHAP\u503c\u4e0d\u786e\u5b9a\u6027\uff08\u5076\u7136\u3001\u8ba4\u77e5\u548c\u7ea0\u7f20\u6210\u5206\uff09\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u6574\u5408\u4e86Dempster-Shafer\u8bc1\u636e\u7406\u8bba\uff0c\u5e76\u901a\u8fc7Dirichlet\u8fc7\u7a0b\u5728\u6811\u96c6\u6210\u4e0a\u8fdb\u884c\u5047\u8bbe\u62bd\u6837\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u7528\u4f8b\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u63d0\u4f9b\u4e86\u5bf9SHAP\u89e3\u91ca\u4e2d\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u672c\u8d28\u7684\u6df1\u5165\u7406\u89e3\uff0c\u4ece\u800c\u66f4\u5168\u9762\u5730\u7406\u89e3SHAP\u5f52\u56e0\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0cSHAP\u503c\u6700\u9ad8\u7684\u7279\u5f81\u4e0d\u4e00\u5b9a\u6700\u7a33\u5b9a\u3002\u57fa\u4e8e\u6811\u7684\u6a21\u578b\uff08\u7279\u522b\u662fBagging\uff09\u6709\u52a9\u4e8e\u6709\u6548\u91cf\u5316\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u5bf9SHAP\u5f52\u56e0\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u7406\u89e3\u53ef\u4ee5\u6307\u5bfc\u7a33\u5065\u7684\u51b3\u7b56\u5236\u5b9a\u548c\u6a21\u578b\u4f18\u5316\u3002\u901a\u8fc7\u66f4\u4f18\u8d28\u3001\u66f4\u5177\u4ee3\u8868\u6027\u7684\u6570\u636e\u548c\u9002\u5f53\u7684\u6a21\u578b\u5f00\u53d1\u6280\u672f\uff0c\u53ef\u4ee5\u964d\u4f4e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2508.09153", "pdf": "https://arxiv.org/pdf/2508.09153", "abs": "https://arxiv.org/abs/2508.09153", "authors": ["TaekHyun Park", "Yongjae Lee", "Daesan Park", "Dohee Kim", "Hyerim Bae"], "title": "JustDense: Just using Dense instead of Sequence Mixer for Time Series analysis", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages ,planning to submit to IEEE BigData 2025", "summary": "Sequence and channel mixers, the core mechanism in sequence models, have\nbecome the de facto standard in time series analysis (TSA). However, recent\nstudies have questioned the necessity of complex sequence mixers, such as\nattention mechanisms, demonstrating that simpler architectures can achieve\ncomparable or even superior performance. This suggests that the benefits\nattributed to complex sequencemixers might instead emerge from other\narchitectural or optimization factors. Based on this observation, we pose a\ncentral question: Are common sequence mixers necessary for time-series\nanalysis? Therefore, we propose JustDense, an empirical study that\nsystematically replaces sequence mixers in various well-established TSA models\nwith dense layers. Grounded in the MatrixMixer framework, JustDense treats any\nsequence mixer as a mixing matrix and replaces it with a dense layer. This\nsubstitution isolates the mixing operation, enabling a clear theoretical\nfoundation for understanding its role. Therefore, we conducted extensive\nexperiments on 29 benchmarks covering five representative TSA tasks using seven\nstate-of-the-art TSA models to address our research question. The results show\nthat replacing sequence mixers with dense layers yields comparable or even\nsuperior performance. In the cases where dedicated sequence mixers still offer\nbenefits, JustDense challenges the assumption that \"deeper and more complex\narchitectures are inherently better\" in TSA.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u65f6\u5e8f\u5206\u6790\u4e2d\uff0c\u7528\u7b80\u5355\u7684\u5168\u8fde\u63a5\u5c42\u66ff\u4ee3\u590d\u6742\u7684\u5e8f\u5217\u6df7\u5408\u5668\uff08\u5982\u6ce8\u610f\u529b\u673a\u5236\uff09\u53ef\u4ee5\u8fbe\u5230\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u6311\u6218\u4e86\u590d\u6742\u6a21\u578b\u66f4\u4f18\u7684\u5047\u8bbe\u3002", "motivation": "\u65f6\u5e8f\u5206\u6790\uff08TSA\uff09\u4e2d\uff0c\u5e8f\u5217\u6df7\u5408\u5668\u662f\u6838\u5fc3\u673a\u5236\u3002\u7136\u800c\uff0c\u8fd1\u671f\u7814\u7a76\u8d28\u7591\u590d\u6742\u5e8f\u5217\u6df7\u5408\u5668\u7684\u5fc5\u8981\u6027\uff0c\u8ba4\u4e3a\u7b80\u5355\u67b6\u6784\u4e5f\u80fd\u8868\u73b0\u826f\u597d\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63a2\u7a76TSA\u4e2d\u5e38\u7528\u5e8f\u5217\u6df7\u5408\u5668\u662f\u5426\u771f\u6b63\u5fc5\u8981\u3002", "method": "\u63d0\u51faJustDense\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728MatrixMixer\u6846\u67b6\u4e0b\uff0c\u7cfb\u7edf\u5730\u5c06\u5404\u79cd\u6210\u719fTSA\u6a21\u578b\u4e2d\u7684\u5e8f\u5217\u6df7\u5408\u5668\u66ff\u6362\u4e3a\u5168\u8fde\u63a5\u5c42\uff0c\u4ee5\u9694\u79bb\u5e76\u7814\u7a76\u6df7\u5408\u64cd\u4f5c\u7684\u4f5c\u7528\u3002", "result": "\u572829\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e03\u79cdSOTA TSA\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u7528\u5168\u8fde\u63a5\u5c42\u66ff\u6362\u5e8f\u5217\u6df7\u5408\u5668\u80fd\u591f\u8fbe\u5230\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86TSA\u4e2d\u201c\u66f4\u6df1\u3001\u66f4\u590d\u6742\u7684\u67b6\u6784\u56fa\u6709\u5730\u66f4\u597d\u201d\u7684\u5047\u8bbe\uff0c\u8bc1\u660e\u4e86\u7b80\u5355\u67b6\u6784\u7684\u6709\u6548\u6027\uff0c\u5373\u4f7f\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u590d\u6742\u6df7\u5408\u5668\u6709\u4f18\u52bf\uff0c\u4e5f\u7a81\u663e\u4e86\u7b80\u5355\u8bbe\u8ba1\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.09188", "pdf": "https://arxiv.org/pdf/2508.09188", "abs": "https://arxiv.org/abs/2508.09188", "authors": ["Seyed Muhammad Hossein Mousavi", "S. Younes Mirinezhad"], "title": "Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation", "categories": ["cs.CV"], "comment": null, "summary": "Affective computing faces a major challenge: the lack of high-quality,\ndiverse depth facial datasets for recognizing subtle emotional expressions. We\npropose a framework for synthetic depth face generation using an optimized GAN\nwith Knowledge Distillation (EMA teacher models) to stabilize training, improve\nquality, and prevent mode collapse. We also apply Genetic Algorithms to evolve\nGAN latent vectors based on image statistics, boosting diversity and visual\nquality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in\nboth diversity and quality. For classification, we extract and concatenate LBP,\nHOG, Sobel edge, and intensity histogram features, achieving 94% and 96%\naccuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows\nconsistent improvement over state-of-the-art methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f18\u5316GAN\u548c\u9057\u4f20\u7b97\u6cd5\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u5408\u6210\u6df1\u5ea6\u4eba\u8138\u6570\u636e\uff0c\u4ee5\u89e3\u51b3\u7ec6\u5fae\u60c5\u611f\u8bc6\u522b\u4e2d\u6570\u636e\u96c6\u532e\u4e4f\u7684\u6311\u6218\uff0c\u5e76\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\u3002", "motivation": "\u60c5\u611f\u8ba1\u7b97\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\uff1a\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u6df1\u5ea6\u4eba\u8138\u6570\u636e\u96c6\uff0c\u4ee5\u7528\u4e8e\u8bc6\u522b\u7ec6\u5fae\u60c5\u611f\u8868\u8fbe\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5408\u6210\u6df1\u5ea6\u4eba\u8138\u751f\u6210\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u4f18\u5316\u8fc7\u7684GAN\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\uff08EMA\u6559\u5e08\u6a21\u578b\uff09\u4ee5\u7a33\u5b9a\u8bad\u7ec3\u3001\u63d0\u9ad8\u8d28\u91cf\u5e76\u9632\u6b62\u6a21\u5f0f\u5d29\u6e83\u3002\u540c\u65f6\uff0c\u5e94\u7528\u9057\u4f20\u7b97\u6cd5\u57fa\u4e8e\u56fe\u50cf\u7edf\u8ba1\u91cf\u6765\u6f14\u5316GAN\u6f5c\u5728\u5411\u91cf\uff0c\u4ee5\u63d0\u5347\u76ee\u6807\u60c5\u611f\u7684\u591a\u6837\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002\u5bf9\u4e8e\u5206\u7c7b\uff0c\u63d0\u53d6\u5e76\u4e32\u8054\u4e86LBP\u3001HOG\u3001Sobel\u8fb9\u7f18\u548c\u5f3a\u5ea6\u76f4\u65b9\u56fe\u7279\u5f81\uff0c\u5e76\u4f7f\u7528XGBoost\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u6027\u548c\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8eGAN\u3001VAE\u3001GMM\u548cKDE\u3002\u4f7f\u7528XGBoost\u8fdb\u884c\u5206\u7c7b\u65f6\uff0c\u8fbe\u5230\u4e8694%\u548c96%\u7684\u51c6\u786e\u7387\u3002FID\u3001IS\u3001SSIM\u548cPSNR\u7b49\u8bc4\u4f30\u6307\u6807\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u5408\u6210\u6df1\u5ea6\u4eba\u8138\u6570\u636e\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u60c5\u611f\u8ba1\u7b97\u4e2d\u6570\u636e\u96c6\u7f3a\u4e4f\u7684\u6311\u6218\uff0c\u5e76\u5728\u60c5\u611f\u8bc6\u522b\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.09430", "pdf": "https://arxiv.org/pdf/2508.09430", "abs": "https://arxiv.org/abs/2508.09430", "authors": ["Lavanya Shankar", "Leibny Paola Garcia Perera"], "title": "Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech", "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "Code-switching and language identification in child-directed scenarios\npresent significant challenges, particularly in bilingual environments. This\npaper addresses this challenge by using Zipformer to handle the nuances of\nspeech, which contains two imbalanced languages, Mandarin and English, in an\nutterance. This work demonstrates that the internal layers of the Zipformer\neffectively encode the language characteristics, which can be leveraged in\nlanguage identification. We present the selection methodology of the inner\nlayers to extract the embeddings and make a comparison with different\nback-ends. Our analysis shows that Zipformer is robust across these backends.\nOur approach effectively handles imbalanced data, achieving a Balanced Accuracy\n(BAC) of 81.89%, a 15.47% improvement over the language identification\nbaseline. These findings highlight the potential of the transformer encoder\narchitecture model in real scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5229\u7528Zipformer\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u513f\u7ae5\u6307\u5bfc\u573a\u666f\u4e0b\u4e2d\u82f1\u6587\u6df7\u6742\u8bed\u97f3\u4e2d\u7684\u8bed\u8a00\u8bc6\u522b\u6311\u6218\uff0c\u5c24\u5176\u5728\u6570\u636e\u4e0d\u5e73\u8861\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u5728\u53cc\u8bed\u73af\u5883\u4e0b\uff0c\u7279\u522b\u662f\u513f\u7ae5\u6307\u5bfc\u573a\u666f\u4e2d\uff0c\u8bed\u7801\u8f6c\u6362\uff08code-switching\uff09\u548c\u8bed\u8a00\u8bc6\u522b\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u5f53\u8bed\u97f3\u4e2d\u5305\u542b\u4e0d\u5e73\u8861\u7684\u666e\u901a\u8bdd\u548c\u82f1\u8bed\u65f6\u3002", "method": "\u4f7f\u7528Zipformer\u6a21\u578b\u5904\u7406\u8bed\u97f3\u7ec6\u5fae\u5dee\u522b\uff0c\u5e76\u5229\u7528\u5176\u5185\u90e8\u5c42\u63d0\u53d6\u8bed\u8a00\u7279\u5f81\u5d4c\u5165\u3002\u7814\u7a76\u4e2d\u5c55\u793a\u4e86\u5185\u90e8\u5c42\u9009\u62e9\u65b9\u6cd5\uff0c\u5e76\u4e0e\u4e0d\u540c\u540e\u7aef\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u4ee5\u9a8c\u8bc1\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "result": "Zipformer\u7684\u5185\u90e8\u5c42\u80fd\u591f\u6709\u6548\u7f16\u7801\u8bed\u8a00\u7279\u5f81\uff0c\u53ef\u7528\u4e8e\u8bed\u8a00\u8bc6\u522b\u3002\u8be5\u6a21\u578b\u5728\u4e0d\u540c\u540e\u7aef\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u80fd\u591f\u6709\u6548\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\uff0c\u5b9e\u73b0\u4e8681.89%\u7684\u5e73\u8861\u51c6\u786e\u7387\uff08BAC\uff09\uff0c\u6bd4\u8bed\u8a00\u8bc6\u522b\u57fa\u7ebf\u63d0\u5347\u4e8615.47%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86Transformer\u7f16\u7801\u5668\u67b6\u6784\u6a21\u578b\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5904\u7406\u8bed\u7801\u8f6c\u6362\u548c\u8bed\u8a00\u8bc6\u522b\u6311\u6218\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.09159", "pdf": "https://arxiv.org/pdf/2508.09159", "abs": "https://arxiv.org/abs/2508.09159", "authors": ["Ilias Chatzistefanidis", "Navid Nikaein", "Andrea Leone", "Ali Maatouk", "Leandros Tassioulas", "Roberto Morabito", "Ioannis Pitsiorlas", "Marios Kountouris"], "title": "Agoran: An Agentic Open Marketplace for 6G RAN Automation", "categories": ["cs.NI", "cs.AI"], "comment": "Pre-print submitted to Computer Networks AI-for-6G", "summary": "Next-generation mobile networks must reconcile the often-conflicting goals of\nmultiple service owners. However, today's network slice controllers remain\nrigid, policy-bound, and unaware of the business context. We introduce Agoran\nService and Resource Broker (SRB), an agentic marketplace that brings\nstakeholders directly into the operational loop. Inspired by the ancient Greek\nagora, Agoran distributes authority across three autonomous AI branches: a\nLegislative branch that answers compliance queries using retrieval-augmented\nLarge Language Models (LLMs); an Executive branch that maintains real-time\nsituational awareness through a watcher-updated vector database; and a Judicial\nbranch that evaluates each agent message with a rule-based Trust Score, while\narbitrating LLMs detect malicious behavior and apply real-time incentives to\nrestore trust. Stakeholder-side Negotiation Agents and the SRB-side Mediator\nAgent negotiate feasible, Pareto-optimal offers produced by a multi-objective\noptimizer, reaching a consensus intent in a single round, which is then\ndeployed to Open and AI RAN controllers. Deployed on a private 5G testbed and\nevaluated with realistic traces of vehicle mobility, Agoran achieved\nsignificant gains: (i) a 37% increase in throughput of eMBB slices, (ii) a 73%\nreduction in latency of URLLC slices, and concurrently (iii) an end-to-end 8.3%\nsaving in PRB usage compared to a static baseline. An 1B-parameter Llama model,\nfine-tuned for five minutes on 100 GPT-4 dialogues, recovers approximately 80%\nof GPT-4.1's decision quality, while operating within 6 GiB of memory and\nconverging in only 1.3 seconds. These results establish Agoran as a concrete,\nstandards-aligned path toward ultra-flexible, stakeholder-centric 6G networks.\nA live demo is presented\nhttps://www.youtube.com/watch?v=h7vEyMu2f5w\\&ab_channel=BubbleRAN.", "AI": {"tldr": "Agoran\u662f\u4e00\u79cd\u9762\u54116G\u7f51\u7edc\u7684AI\u9a71\u52a8\u4ee3\u7406\u5e02\u573a\uff0c\u5b83\u901a\u8fc7\u534f\u8c03\u591a\u670d\u52a1\u6240\u6709\u8005\u4e4b\u95f4\u7684\u51b2\u7a81\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u7f51\u7edc\u5207\u7247\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u79fb\u52a8\u7f51\u7edc\u9700\u8981\u534f\u8c03\u591a\u4e2a\u670d\u52a1\u6240\u6709\u8005\u4e4b\u95f4\u5e38\u5e38\u76f8\u4e92\u51b2\u7a81\u7684\u76ee\u6807\uff0c\u800c\u76ee\u524d\u7684\u7f51\u7edc\u5207\u7247\u63a7\u5236\u5668\u5219\u8fc7\u4e8e\u50f5\u5316\u3001\u53d7\u9650\u4e8e\u7b56\u7565\u4e14\u7f3a\u4e4f\u4e1a\u52a1\u60c5\u5883\u611f\u77e5\u3002", "method": "Agoran\u5f15\u5165\u4e86\u4e00\u4e2a\u4ee3\u7406\u5e02\u573a\uff0c\u5c06\u51b3\u7b56\u6743\u5206\u5e03\u7ed9\u4e09\u4e2a\u81ea\u4e3bAI\u5206\u652f\uff1a\u7acb\u6cd5\u5206\u652f\uff08\u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u578bLLM\u5904\u7406\u5408\u89c4\u67e5\u8be2\uff09\u3001\u884c\u653f\u5206\u652f\uff08\u901a\u8fc7\u89c2\u5bdf\u8005\u66f4\u65b0\u7684\u5411\u91cf\u6570\u636e\u5e93\u7ef4\u6301\u5b9e\u65f6\u6001\u52bf\u611f\u77e5\uff09\u548c\u53f8\u6cd5\u5206\u652f\uff08\u4f7f\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u4fe1\u4efb\u8bc4\u5206\u8bc4\u4f30\u4ee3\u7406\u6d88\u606f\u5e76\u4ef2\u88c1LLM\u4ee5\u68c0\u6d4b\u6076\u610f\u884c\u4e3a\uff09\u3002\u540c\u65f6\uff0c\u5229\u76ca\u76f8\u5173\u8005\u4fa7\u7684\u8c08\u5224\u4ee3\u7406\u548cSRB\u4fa7\u7684\u8c03\u89e3\u4ee3\u7406\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u5668\u751f\u6210\u5e15\u7d2f\u6258\u6700\u4f18\u62a5\u4ef7\uff0c\u5e76\u5728\u5355\u8f6e\u5185\u8fbe\u6210\u5171\u8bc6\u610f\u56fe\uff0c\u968f\u540e\u90e8\u7f72\u5230Open\u548cAI RAN\u63a7\u5236\u5668\u3002\u6b64\u5916\uff0c\u4e00\u4e2a1B\u53c2\u6570\u7684Llama\u6a21\u578b\u7ecf\u8fc7\u5c11\u91cf\u6570\u636e\u5fae\u8c03\u540e\u80fd\u8fbe\u5230\u63a5\u8fd1GPT-4\u7684\u51b3\u7b56\u8d28\u91cf\u3002", "result": "\u5728\u79c1\u67095G\u6d4b\u8bd5\u5e8a\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cAgoran\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\uff1aeMBB\u5207\u7247\u541e\u5410\u91cf\u589e\u52a0\u4e8637%\uff0cURLLC\u5207\u7247\u5ef6\u8fdf\u964d\u4f4e\u4e8673%\uff0c\u540c\u65f6\u4e0e\u9759\u6001\u57fa\u7ebf\u76f8\u6bd4\uff0c\u7aef\u5230\u7aefPRB\u4f7f\u7528\u8282\u7701\u4e868.3%\u3002\u4e00\u4e2a1B\u53c2\u6570\u7684Llama\u6a21\u578b\u57286 GiB\u5185\u5b58\u9650\u5236\u4e0b\uff0c\u4ec5\u75281.3\u79d2\u5373\u53ef\u6536\u655b\uff0c\u5e76\u5728\u77ed\u65f6\u95f4\u5fae\u8c03\u540e\u6062\u590d\u4e86\u7ea680%\u7684GPT-4.1\u51b3\u7b56\u8d28\u91cf\u3002", "conclusion": "Agoran\u4e3a\u5b9e\u73b0\u8d85\u7075\u6d3b\u3001\u4ee5\u5229\u76ca\u76f8\u5173\u8005\u4e3a\u4e2d\u5fc3\u76846G\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u6761\u5177\u4f53\u4e14\u7b26\u5408\u6807\u51c6\u7684\u8def\u5f84\u3002"}}
{"id": "2508.09670", "pdf": "https://arxiv.org/pdf/2508.09670", "abs": "https://arxiv.org/abs/2508.09670", "authors": ["Weitao Jia", "Jinghui Lu", "Haiyang Yu", "Siqi Wang", "Guozhi Tang", "An-Lan Wang", "Weijie Yin", "Dingkang Yang", "Yuxiang Nie", "Bin Shan", "Hao Feng", "Irene Li", "Kun Yang", "Han Wang", "Jingqun Tang", "Teng Fu", "Changhong Jin", "Chao Feng", "Xiaohui Lv", "Can Huang"], "title": "MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances demonstrate that reinforcement learning with verifiable\nrewards (RLVR) significantly enhances the reasoning capabilities of large\nlanguage models (LLMs). However, standard RLVR faces challenges with reward\nsparsity, where zero rewards from consistently incorrect candidate answers\nprovide no learning signal, particularly in challenging tasks. To address this,\nwe propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative\nframework that utilizes diverse expert prompts as system prompts to generate a\nbroader range of responses, substantially increasing the likelihood of\nidentifying correct solutions. Additionally, we introduce an inter-expert\nmutual learning mechanism that facilitates knowledge sharing and transfer among\nexperts, further boosting the model's performance through RLVR. Extensive\nexperiments across multiple reasoning benchmarks show that MEML-GRPO delivers\nsignificant improvements, achieving an average performance gain of 4.89% with\nQwen and 11.33% with Llama, effectively overcoming the core limitations of\ntraditional RLVR methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faMEML-GRPO\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e13\u5bb6\u63d0\u793a\u548c\u4e92\u5b66\u4e60\u673a\u5236\uff0c\u89e3\u51b3\u4e86RLVR\u5728LLM\u63a8\u7406\u4e2d\u9762\u4e34\u7684\u5956\u52b1\u7a00\u758f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e2d\u9762\u4e34\u5956\u52b1\u7a00\u758f\u95ee\u9898\uff0c\u5373\u6301\u7eed\u9519\u8bef\u7684\u5019\u9009\u7b54\u6848\u5bfc\u81f4\u96f6\u5956\u52b1\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u5c24\u5176\u5728\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "method": "\u63d0\u51faMulti-Expert Mutual Learning GRPO (MEML-GRPO) \u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u591a\u6837\u5316\u7684\u4e13\u5bb6\u63d0\u793a\u4f5c\u4e3a\u7cfb\u7edf\u63d0\u793a\uff0c\u751f\u6210\u66f4\u5e7f\u6cdb\u7684\u54cd\u5e94\u4ee5\u589e\u52a0\u627e\u5230\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u80fd\u6027\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e13\u5bb6\u95f4\u4e92\u5b66\u4e60\u673a\u5236\uff0c\u4fc3\u8fdb\u77e5\u8bc6\u5171\u4eab\u548c\u8fc1\u79fb\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMEML-GRPO\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002Qwen\u6a21\u578b\u5e73\u5747\u6027\u80fd\u63d0\u53474.89%\uff0cLlama\u6a21\u578b\u5e73\u5747\u6027\u80fd\u63d0\u534711.33%\u3002", "conclusion": "MEML-GRPO\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edfRLVR\u65b9\u6cd5\u7684\u6838\u5fc3\u5c40\u9650\u6027\uff0c\u663e\u8457\u589e\u5f3a\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2508.09154", "pdf": "https://arxiv.org/pdf/2508.09154", "abs": "https://arxiv.org/abs/2508.09154", "authors": ["Xiaojing Du", "Jiuyong Li", "Lin Liu", "Debo Cheng", "Thuc. Le"], "title": "Peer Effect Estimation in the Presence of Simultaneous Feedback and Unobserved Confounders", "categories": ["cs.LG", "cs.AI", "stat.ME"], "comment": null, "summary": "Estimating peer causal effects within complex real-world networks such as\nsocial networks is challenging, primarily due to simultaneous feedback between\npeers and unobserved confounders. Existing methods either address unobserved\nconfounders while ignoring the simultaneous feedback, or account for feedback\nbut under restrictive linear assumptions, thus failing to obtain accurate peer\neffect estimation. In this paper, we propose DIG2RSI, a novel Deep learning\nframework which leverages I-G transformation (matrix operation) and 2SRI (an\ninstrumental variable or IV technique) to address both simultaneous feedback\nand unobserved confounding, while accommodating complex, nonlinear and\nhigh-dimensional relationships. DIG2RSI first applies the I-G transformation to\ndisentangle mutual peer influences and eliminate the bias due to the\nsimultaneous feedback. To deal with unobserved confounding, we first construct\nvalid IVs from network data. In stage 1 of 2RSI, we train a neural network on\nthese IVs to predict peer exposure, and extract residuals as proxies for the\nunobserved confounders. In the stage 2, we fit a separate neural network\naugmented by an adversarial discriminator that incorporates these residuals as\na control function and enforces the learned representation to contain no\nresidual confounding signal. The expressive power of deep learning models in\ncapturing complex non-linear relationships and adversarial debiasing enhances\nthe effectiveness of DIG2RSI in eliminating bias from both feedback loops and\nhidden confounders. We prove consistency of our estimator under standard\nregularity conditions, ensuring asymptotic recovery of the true peer effect.\nEmpirical results on two semi-synthetic benchmarks and a real-world dataset\ndemonstrate that DIG2RSI outperforms existing approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDIG2RSI\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7ed3\u5408I-G\u53d8\u6362\u548c2SRI\uff08\u5de5\u5177\u53d8\u91cf\u6280\u672f\uff09\uff0c\u4ee5\u89e3\u51b3\u590d\u6742\u7f51\u7edc\u4e2d\u540c\u4f34\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u65f6\u5b58\u5728\u7684\u540c\u6b65\u53cd\u9988\u548c\u672a\u89c2\u6d4b\u6df7\u6dc6\u95ee\u9898\u3002", "motivation": "\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u7f51\u7edc\u4e2d\u4f30\u8ba1\u540c\u4f34\u56e0\u679c\u6548\u5e94\u6781\u5177\u6311\u6218\u6027\uff0c\u4e3b\u8981\u7531\u4e8e\u540c\u4f34\u95f4\u7684\u540c\u6b65\u53cd\u9988\u548c\u672a\u89c2\u6d4b\u6df7\u6dc6\u56e0\u7d20\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u53cd\u9988\uff0c\u8981\u4e48\u4ec5\u9650\u4e8e\u7ebf\u6027\u5047\u8bbe\uff0c\u5bfc\u81f4\u65e0\u6cd5\u51c6\u786e\u4f30\u8ba1\u540c\u4f34\u6548\u5e94\u3002", "method": "DIG2RSI\u9996\u5148\u901a\u8fc7I-G\u53d8\u6362\u5206\u79bb\u76f8\u4e92\u540c\u4f34\u5f71\u54cd\u5e76\u6d88\u9664\u540c\u6b65\u53cd\u9988\u504f\u5dee\u3002\u4e3a\u5904\u7406\u672a\u89c2\u6d4b\u6df7\u6dc6\uff0c\u5b83\u4ece\u7f51\u7edc\u6570\u636e\u6784\u5efa\u6709\u6548\u5de5\u5177\u53d8\u91cf\u3002\u57282SRI\u7684\u7b2c\u4e00\u9636\u6bb5\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5229\u7528\u8fd9\u4e9b\u5de5\u5177\u53d8\u91cf\u9884\u6d4b\u540c\u4f34\u66b4\u9732\uff0c\u5e76\u63d0\u53d6\u6b8b\u5dee\u4f5c\u4e3a\u672a\u89c2\u6d4b\u6df7\u6dc6\u7684\u4ee3\u7406\u3002\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0c\u62df\u5408\u53e6\u4e00\u4e2a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u52a0\u5165\u5bf9\u6297\u9274\u522b\u5668\uff0c\u8be5\u9274\u522b\u5668\u5c06\u6b8b\u5dee\u4f5c\u4e3a\u63a7\u5236\u51fd\u6570\uff0c\u786e\u4fdd\u5b66\u4e60\u5230\u7684\u8868\u793a\u4e0d\u542b\u6b8b\u4f59\u6df7\u6dc6\u4fe1\u53f7\u3002\u6df1\u5ea6\u5b66\u4e60\u548c\u5bf9\u6297\u53bb\u504f\u589e\u5f3a\u4e86\u5176\u6d88\u9664\u504f\u5dee\u7684\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5728\u4e24\u4e2a\u534a\u5408\u6210\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cDIG2RSI\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DIG2RSI\u5728\u5904\u7406\u590d\u6742\u3001\u975e\u7ebf\u6027\u3001\u9ad8\u7ef4\u5173\u7cfb\u4e2d\u540c\u4f34\u56e0\u679c\u6548\u5e94\u4f30\u8ba1\u7684\u540c\u6b65\u53cd\u9988\u548c\u672a\u89c2\u6d4b\u6df7\u6dc6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u80fd\u6e10\u8fd1\u6062\u590d\u771f\u5b9e\u7684\u540c\u4f34\u6548\u5e94\uff0c\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.09199", "pdf": "https://arxiv.org/pdf/2508.09199", "abs": "https://arxiv.org/abs/2508.09199", "authors": ["Jucheng Hu", "Suorong Yang", "Dongzhan Zhou"], "title": "$\u0394$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Visual Instruction Finetuning (VIF) is pivotal for post-training\nVision-Language Models (VLMs). Unlike unimodal instruction finetuning in\nplain-text large language models, which mainly requires instruction datasets to\nenable model instruction-following ability, VIF also requires multimodal data\nto enable joint visual and textual understanding; therefore, it typically\nrequires more data. Consequently, VIF imposes stricter data selection\nchallenges: the method must scale efficiently to handle larger data demands\nwhile ensuring the quality of both visual and textual content, as well as their\nalignment. Despite its critical impact on performance, data selection for VIF\nremains an understudied area. In this paper, we propose $\\Delta$-AttnMask. This\ndata-efficient framework quantifies sample quality through attention-guided\nmasking of the model's hidden states, jointly evaluating image-text pairs\nwithout requiring domain labels, auxiliary models, or extra training. By\ncomputing loss differences ($\\Delta$) between the original states and states\nmasked using high-attention regions, $\\Delta$-AttnMask intrinsically assesses\nsample quality. Experiments across multiple VLMs and datasets show that\n$\\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,\naccelerating training by 5x while surpassing full-dataset baselines by +10.1%\nin overall accuracy. Its model-agnostic and data-agnostic design ensures broad\napplicability across modalities and architectures.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.09450", "pdf": "https://arxiv.org/pdf/2508.09450", "abs": "https://arxiv.org/abs/2508.09450", "authors": ["Ridwan Mahbub", "Mohammed Saidul Islam", "Mir Tafseer Nayeem", "Md Tahmid Rahman Laskar", "Mizanur Rahman", "Shafiq Joty", "Enamul Hoque"], "title": "From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text", "categories": ["cs.CL"], "comment": null, "summary": "Charts are very common for exploring data and communicating insights, but\nextracting key takeaways from charts and articulating them in natural language\ncan be challenging. The chart-to-text task aims to automate this process by\ngenerating textual summaries of charts. While with the rapid advancement of\nlarge Vision-Language Models (VLMs), we have witnessed great progress in this\ndomain, little to no attention has been given to potential biases in their\noutputs. This paper investigates how VLMs can amplify geo-economic biases when\ngenerating chart summaries, potentially causing societal harm. Specifically, we\nconduct a large-scale evaluation of geo-economic biases in VLM-generated chart\nsummaries across 6,000 chart-country pairs from six widely used proprietary and\nopen-source models to understand how a country's economic status influences the\nsentiment of generated summaries. Our analysis reveals that existing VLMs tend\nto produce more positive descriptions for high-income countries compared to\nmiddle- or low-income countries, even when country attribution is the only\nvariable changed. We also find that models such as GPT-4o-mini,\nGemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further\nexplore inference-time prompt-based debiasing techniques using positive\ndistractors but find them only partially effective, underscoring the complexity\nof the issue and the need for more robust debiasing strategies. Our code and\ndataset are publicly available here.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u751f\u6210\u56fe\u8868\u6458\u8981\u65f6\u5b58\u5728\u5730\u7f18\u7ecf\u6d4e\u504f\u89c1\uff0c\u503e\u5411\u4e8e\u5bf9\u9ad8\u6536\u5165\u56fd\u5bb6\u7ed9\u51fa\u66f4\u79ef\u6781\u7684\u63cf\u8ff0\uff0c\u5e76\u6307\u51fa\u73b0\u6709\u53bb\u504f\u6280\u672f\u6548\u679c\u6709\u9650\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u56fe\u8868\u5230\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u8f93\u51fa\u4e2d\u6f5c\u5728\u7684\u504f\u89c1\uff0c\u7279\u522b\u662f\u5730\u7f18\u7ecf\u6d4e\u504f\u89c1\uff0c\u9c9c\u6709\u7814\u7a76\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u793e\u4f1a\u5371\u5bb3\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7a76VLMs\u5982\u4f55\u653e\u5927\u6b64\u7c7b\u504f\u89c1\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5bf9\u6765\u81ea\u516d\u79cd\u4e3b\u6d41VLM\u6a21\u578b\u751f\u6210\u76846,000\u4e2a\u56fe\u8868-\u56fd\u5bb6\u5bf9\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u5206\u6790\u56fd\u5bb6\u7ecf\u6d4e\u72b6\u51b5\u5bf9\u751f\u6210\u6458\u8981\u60c5\u611f\u503e\u5411\u7684\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u7d22\u5e76\u6d4b\u8bd5\u4e86\u57fa\u4e8e\u63d0\u793a\uff08\u4f7f\u7528\u79ef\u6781\u5e72\u6270\u9879\uff09\u7684\u63a8\u7406\u65f6\u53bb\u504f\u6280\u672f\u3002", "result": "\u5206\u6790\u53d1\u73b0\uff0c\u73b0\u6709VLMs\u503e\u5411\u4e8e\u5bf9\u9ad8\u6536\u5165\u56fd\u5bb6\u4ea7\u751f\u66f4\u79ef\u6781\u7684\u63cf\u8ff0\uff0c\u800c\u5bf9\u4e2d\u4f4e\u6536\u5165\u56fd\u5bb6\u5219\u4e0d\u7136\uff0c\u5373\u4f7f\u4ec5\u6539\u53d8\u56fd\u5bb6\u5c5e\u6027\u3002\u90e8\u5206\u6a21\u578b\u5982GPT-4o-mini\u3001Gemini-1.5-Flash\u548cPhi-3.5\u8868\u73b0\u51fa\u4e0d\u540c\u7a0b\u5ea6\u7684\u504f\u89c1\u3002\u540c\u65f6\uff0c\u63a2\u7d22\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u53bb\u504f\u6280\u672f\u4ec5\u90e8\u5206\u6709\u6548\u3002", "conclusion": "VLMs\u5728\u56fe\u8868\u6458\u8981\u751f\u6210\u4e2d\u5b58\u5728\u663e\u8457\u7684\u5730\u7f18\u7ecf\u6d4e\u504f\u89c1\uff0c\u4e14\u8fd9\u662f\u4e00\u4e2a\u590d\u6742\u95ee\u9898\uff0c\u73b0\u6709\u53bb\u504f\u7b56\u7565\u6548\u679c\u6709\u9650\u3002\u8fd9\u5f3a\u8c03\u4e86\u672a\u6765\u9700\u8981\u66f4\u7a33\u5065\u7684\u53bb\u504f\u65b9\u6cd5\u6765\u89e3\u51b3\u6b64\u7c7b\u6f5c\u5728\u7684\u793e\u4f1a\u5371\u5bb3\u3002"}}
{"id": "2508.09166", "pdf": "https://arxiv.org/pdf/2508.09166", "abs": "https://arxiv.org/abs/2508.09166", "authors": ["Wei Guo", "Shunsei Yamagishi", "Lei Jing"], "title": "WPTrack: A Wi-Fi and Pressure Insole Fusion System for Single Target Tracking", "categories": ["cs.NI", "cs.HC"], "comment": "6 pages, 12 figures, conference", "summary": "As the Internet of Things (IoT) continues to evolve, indoor location has\nbecome a critical element for enabling smart homes, behavioral monitoring, and\nelderly care. Existing WiFi-based human tracking solutions typically require\nspecialized equipment or multiple Wi-Fi links, a limitation in most indoor\nsettings where only a single pair of Wi-Fi devices is usually available.\nHowever, despite efforts to implement human tracking using one Wi-Fi link,\nsignificant challenges remain, such as difficulties in acquiring initial\npositions and blind spots in DFS estimation of tangent direction. To address\nthese challenges, this paper proposes WPTrack, the first Wi-Fi and Pressure\nInsoles Fusion System for Single Target Tracking. WPTrack collects Channel\nState Information (CSI) from a single Wi-Fi link and pressure data from 90\ninsole sensors. The phase difference and Doppler velocity are computed from the\nCSI, while the pressure sensor data is used to calculate walking velocity.\nThen, we propose the CSI-pressure fusion model, integrating CSI and pressure\ndata to accurately determine initial positions and facilitate precise human\ntracking. The simulation results show that the initial position localization\naccuracy ranges from 0.02 cm to 42.55 cm. The trajectory tracking results\nobtained from experimental data collected in a real-world environment closely\nalign with the actual trajectory.", "AI": {"tldr": "WPTrack\u7cfb\u7edf\u7ed3\u5408\u5355Wi-Fi\u94fe\u8def\u7684\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u548c\u538b\u529b\u978b\u57ab\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u5ba4\u5185\u5355\u76ee\u6807\u4eba\u4f53\u8ddf\u8e2a\u548c\u521d\u59cb\u5b9a\u4f4d\u3002", "motivation": "\u7269\u8054\u7f51\u6f14\u8fdb\u5bf9\u5ba4\u5185\u5b9a\u4f4d\u63d0\u51fa\u66f4\u9ad8\u8981\u6c42\uff0c\u4f46\u73b0\u6709WiFi\u4eba\u4f53\u8ddf\u8e2a\u65b9\u6848\u591a\u4f9d\u8d56\u7279\u6b8a\u8bbe\u5907\u6216\u591a\u94fe\u8def\u3002\u5355Wi-Fi\u94fe\u8def\u65b9\u6848\u5728\u521d\u59cb\u4f4d\u7f6e\u83b7\u53d6\u548c\u65b9\u5411\u4f30\u8ba1\u76f2\u70b9\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5728\u5e38\u89c1\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u6587\u63d0\u51faWPTrack\uff0c\u9996\u6b21\u878d\u5408Wi-Fi\u548c\u538b\u529b\u978b\u57ab\u6570\u636e\u8fdb\u884c\u5355\u76ee\u6807\u8ddf\u8e2a\u3002\u7cfb\u7edf\u91c7\u96c6\u5355Wi-Fi\u94fe\u8def\u7684CSI\u548c90\u4e2a\u978b\u57ab\u4f20\u611f\u5668\u7684\u538b\u529b\u6570\u636e\u3002\u901a\u8fc7CSI\u8ba1\u7b97\u76f8\u4f4d\u5dee\u548c\u591a\u666e\u52d2\u901f\u5ea6\uff0c\u901a\u8fc7\u538b\u529b\u6570\u636e\u8ba1\u7b97\u884c\u8d70\u901f\u5ea6\u3002\u968f\u540e\uff0c\u6784\u5efaCSI-\u538b\u529b\u878d\u5408\u6a21\u578b\uff0c\u4ee5\u51c6\u786e\u786e\u5b9a\u521d\u59cb\u4f4d\u7f6e\u5e76\u5b9e\u73b0\u7cbe\u786e\u4eba\u4f53\u8ddf\u8e2a\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u521d\u59cb\u4f4d\u7f6e\u5b9a\u4f4d\u7cbe\u5ea6\u4ecb\u4e8e0.02 cm\u81f342.55 cm\u4e4b\u95f4\u3002\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u91c7\u96c6\u7684\u5b9e\u9a8c\u6570\u636e\u8868\u660e\uff0c\u8f68\u8ff9\u8ddf\u8e2a\u7ed3\u679c\u4e0e\u5b9e\u9645\u8f68\u8ff9\u9ad8\u5ea6\u543b\u5408\u3002", "conclusion": "WPTrack\u901a\u8fc7\u521b\u65b0\u6027\u5730\u878d\u5408\u5355Wi-Fi\u94fe\u8defCSI\u548c\u538b\u529b\u978b\u57ab\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355Wi-Fi\u94fe\u8def\u4eba\u4f53\u8ddf\u8e2a\u7684\u521d\u59cb\u5b9a\u4f4d\u548c\u7cbe\u5ea6\u96be\u9898\uff0c\u4e3a\u667a\u80fd\u5bb6\u5c45\u548c\u8001\u5e74\u62a4\u7406\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5ba4\u5185\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09724", "pdf": "https://arxiv.org/pdf/2508.09724", "abs": "https://arxiv.org/abs/2508.09724", "authors": ["Yang Zhang", "Cunxiang Wang", "Lindong Wu", "Wenbo Yu", "Yidong Wang", "Guangsheng Bao", "Jie Tang"], "title": "UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge", "categories": ["cs.AI"], "comment": null, "summary": "Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but\nit is prone to preference bias, where judges systematically favor certain\noutputs, such as their own. This bias leads to inconsistent and skewed rankings\nacross different judges. To address this, we first empirically demonstrate\nsignificant and heterogeneous biases in cross-model evaluations. We then\npropose UDA (Unsupervised Debiasing Alignment), a framework that reduces\ninter-judge disagreement by dynamically adjusting the Elo rating system. For\neach pairwise comparison, a compact neural network learns to adaptively set the\nK-factor and refine win probabilities. Crucially, UDA operates in a fully\nunsupervised manner, guided solely by the objective of minimizing the\ndispersion among the Elo trajectories of all judges. This forces an alignment\ntowards a collective consensus, which serves as an unsupervised proxy for a\nmore stable and reproducible evaluation. In addition, we provide theoretical\nmotivation demonstrating how alignment towards a consensus can reduce aggregate\nsystem bias. Experiments show that UDA significantly reduces the inter-judge\nrating standard deviation by up to 63.4% and improves the average correlation\nwith human judgments by 24.7%. Notably, UDA elevates the performance of poorly\nperforming judges to achieve parity with high-quality ones, fostering a more\nrobust and reliable evaluation ecosystem. Code and data are available at\nhttps://anonymous.4open.science/r/62AB93CD-23B4.", "AI": {"tldr": "\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6210\u5bf9\u8bc4\u4f30\u4e2d\u5b58\u5728\u7684\u504f\u597d\u504f\u5dee\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86UDA\uff08\u65e0\u76d1\u7763\u53bb\u504f\u5bf9\u9f50\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8c03\u6574Elo\u8bc4\u5206\u7cfb\u7edf\u5e76\u5f3a\u5236\u8bc4\u5224\u8005\u8fbe\u6210\u5171\u8bc6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bc4\u5224\u8005\u95f4\u7684\u5206\u6b67\uff0c\u63d0\u9ad8\u4e86\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "motivation": "LLM\u7684\u6210\u5bf9\u8bc4\u4f30\u8303\u5f0f\u666e\u904d\u5b58\u5728\u504f\u597d\u504f\u5dee\uff08\u5982\u8bc4\u5224\u8005\u504f\u7231\u81ea\u8eab\u8f93\u51fa\uff09\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u4e0d\u4e00\u81f4\u548c\u6392\u540d\u504f\u5dee\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u51cf\u5c11\u8fd9\u79cd\u504f\u5dee\uff0c\u5b9e\u73b0\u66f4\u7a33\u5b9a\u548c\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u3002", "method": "\u9996\u5148\uff0c\u5b9e\u8bc1\u8bc1\u660e\u4e86\u8de8\u6a21\u578b\u8bc4\u4f30\u4e2d\u663e\u8457\u4e14\u5f02\u6784\u7684\u504f\u5dee\u3002\u7136\u540e\uff0c\u63d0\u51fa\u4e86UDA\uff08\u65e0\u76d1\u7763\u53bb\u504f\u5bf9\u9f50\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4e00\u4e2a\u7d27\u51d1\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u4e3a\u6bcf\u6b21\u6210\u5bf9\u6bd4\u8f83\u81ea\u9002\u5e94\u5730\u8bbe\u7f6eElo\u8bc4\u5206\u7cfb\u7edf\u7684K\u56e0\u5b50\u5e76\u4f18\u5316\u83b7\u80dc\u6982\u7387\u3002UDA\u4ee5\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u65b9\u5f0f\u8fd0\u884c\uff0c\u5176\u76ee\u6807\u662f\u6700\u5c0f\u5316\u6240\u6709\u8bc4\u5224\u8005Elo\u8f68\u8ff9\u4e4b\u95f4\u7684\u79bb\u6563\u5ea6\uff0c\u4ece\u800c\u5f3a\u5236\u5b9e\u73b0\u5411\u96c6\u4f53\u5171\u8bc6\u7684\u5bf9\u9f50\u3002\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u8bba\u8bc1\u4e86\u8fd9\u79cd\u5bf9\u9f50\u5982\u4f55\u51cf\u5c11\u7cfb\u7edf\u805a\u5408\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUDA\u5c06\u8bc4\u5224\u8005\u95f4\u7684\u8bc4\u5206\u6807\u51c6\u5dee\u663e\u8457\u964d\u4f4e\u4e86\u9ad8\u8fbe63.4%\uff0c\u5e76\u5c06\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u5e73\u5747\u76f8\u5173\u6027\u63d0\u9ad8\u4e8624.7%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cUDA\u8fd8\u80fd\u5c06\u8868\u73b0\u4e0d\u4f73\u7684\u8bc4\u5224\u8005\u7684\u6027\u80fd\u63d0\u5347\u81f3\u4e0e\u9ad8\u8d28\u91cf\u8bc4\u5224\u8005\u76f8\u5f53\u7684\u6c34\u5e73\u3002", "conclusion": "UDA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u6210\u5bf9\u8bc4\u4f30\u4e2d\u7684\u504f\u597d\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u5347\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u548c\u53ef\u9760\u6027\uff0c\u4fc3\u8fdb\u4e86\u4e00\u4e2a\u66f4\u7a33\u5065\u3001\u53ef\u9760\u7684\u8bc4\u4f30\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2508.09155", "pdf": "https://arxiv.org/pdf/2508.09155", "abs": "https://arxiv.org/abs/2508.09155", "authors": ["Wenkai Wang", "Hongcan Guo", "Zheqi Lv", "Shengyu Zhang"], "title": "A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages, 9 figures", "summary": "Self-evaluation, a model's ability to assess the correctness of its own\noutput, is crucial for Large Multimodal Models (LMMs) to achieve\nself-improvement in multi-turn conversations, yet largely absent in foundation\nmodels. Recent work has employed reinforcement learning (RL) to enhance\nself-evaluation; however, its fixed reward mechanism suffers from reward\nhacking when optimizing multiple training objectives, leading to model\ncollapse. In this paper we propose AdaPO, an online reinforcement learning\nframework capable of adaptively adjusting training objective in real time\naccording to the current training state for each task. Specifically, to\nmitigate reward hacking , AdaPO introduces an Adaptive Reward Model (ARM) and a\nReward Aware Dynamic KL Regularization mechanism. ARM assesses the task's\ntraining state from the distribution of model generated multi-turn\ntrajectories' performance. Reward Aware Dynamic KL replaces a fixed penalty\nwith dynamic coefficients which is modulated by the reward gap between\ndifferent multi-turn situations. Notably, our method automatically and smoothly\nadjusts its learning focus based on sub-tasks' training progress without manual\nintervention. Extensive experiments over 8 benchmarks and various models show\nthat our method significantly enhances both direct reasoning and\nself-evaluation capability. We will release our code to contribute to the\ncommunity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAdaPO\uff0c\u4e00\u4e2a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5956\u52b1\u6a21\u578b\u548c\u5956\u52b1\u611f\u77e5\u52a8\u6001KL\u6b63\u5219\u5316\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u81ea\u8bc4\u4f30\u4e2d\u5b58\u5728\u7684\u5956\u52b1\u4f5c\u5f0a\u548c\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u76f4\u63a5\u63a8\u7406\u548c\u81ea\u8bc4\u4f30\u80fd\u529b\u3002", "motivation": "\u81ea\u8bc4\u4f30\u5bf9\u4e8eLMMs\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5b9e\u73b0\u81ea\u6211\u63d0\u5347\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u73b0\u6709\u57fa\u7840\u6a21\u578b\u4e2d\u666e\u904d\u7f3a\u5931\u3002\u73b0\u6709\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u589e\u5f3a\u81ea\u8bc4\u4f30\u7684\u65b9\u6cd5\uff0c\u56e0\u5176\u56fa\u5b9a\u7684\u5956\u52b1\u673a\u5236\u5728\u4f18\u5316\u591a\u8bad\u7ec3\u76ee\u6807\u65f6\u6613\u5bfc\u81f4\u201c\u5956\u52b1\u4f5c\u5f0a\u201d\u548c\u201c\u6a21\u578b\u5d29\u6e83\u201d\u3002", "method": "\u63d0\u51faAdaPO\uff0c\u4e00\u4e2a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u6839\u636e\u5404\u4efb\u52a1\u7684\u5f53\u524d\u8bad\u7ec3\u72b6\u6001\u5b9e\u65f6\u81ea\u9002\u5e94\u8c03\u6574\u8bad\u7ec3\u76ee\u6807\u3002\u4e3a\u7f13\u89e3\u5956\u52b1\u4f5c\u5f0a\uff0cAdaPO\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u5956\u52b1\u6a21\u578b\uff08ARM\uff09\u6765\u8bc4\u4f30\u6a21\u578b\u751f\u6210\u591a\u8f6e\u8f68\u8ff9\u6027\u80fd\u7684\u5206\u5e03\uff0c\u5e76\u91c7\u7528\u5956\u52b1\u611f\u77e5\u52a8\u6001KL\u6b63\u5219\u5316\u673a\u5236\uff0c\u7528\u7531\u4e0d\u540c\u591a\u8f6e\u60c5\u5883\u4e0b\u7684\u5956\u52b1\u5dee\u8ddd\u8c03\u5236\u7684\u52a8\u6001\u7cfb\u6570\u53d6\u4ee3\u56fa\u5b9a\u60e9\u7f5a\uff0c\u4ece\u800c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u53ef\u6839\u636e\u5b50\u4efb\u52a1\u7684\u8bad\u7ec3\u8fdb\u5ea6\u81ea\u52a8\u8c03\u6574\u5b66\u4e60\u7126\u70b9\u3002", "result": "\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u5404\u79cd\u6a21\u578b\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAdaPO\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u76f4\u63a5\u63a8\u7406\u80fd\u529b\u548c\u81ea\u8bc4\u4f30\u80fd\u529b\u3002", "conclusion": "AdaPO\u901a\u8fc7\u5176\u72ec\u7279\u7684\u81ea\u9002\u5e94\u8bad\u7ec3\u76ee\u6807\u8c03\u6574\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86LMMs\u81ea\u8bc4\u4f30\u4e2d\u7684\u5956\u52b1\u4f5c\u5f0a\u6311\u6218\uff0c\u4e3a\u63d0\u5347LMMs\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u81ea\u6211\u6539\u8fdb\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u65e0\u9700\u624b\u52a8\u5e72\u9884\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09202", "pdf": "https://arxiv.org/pdf/2508.09202", "abs": "https://arxiv.org/abs/2508.09202", "authors": ["Masoumeh Sharafi", "Soufiane Belharbi", "Houssem Ben Salem", "Ali Etemad", "Alessandro Lameiras Koerich", "Marco Pedersoli", "Simon Bacon", "Eric Granger"], "title": "Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Facial expression recognition (FER) models are employed in many video-based\naffective computing applications, such as human-computer interaction and\nhealthcare monitoring. However, deep FER models often struggle with subtle\nexpressions and high inter-subject variability, limiting their performance in\nreal-world applications. To improve their performance, source-free domain\nadaptation (SFDA) methods have been proposed to personalize a pretrained source\nmodel using only unlabeled target domain data, thereby avoiding data privacy,\nstorage, and transmission constraints. This paper addresses a challenging\nscenario where source data is unavailable for adaptation, and only unlabeled\ntarget data consisting solely of neutral expressions is available. SFDA methods\nare not typically designed to adapt using target data from only a single class.\nFurther, using models to generate facial images with non-neutral expressions\ncan be unstable and computationally intensive. In this paper, personalized\nfeature translation (PFT) is proposed for SFDA. Unlike current image\ntranslation methods for SFDA, our lightweight method operates in the latent\nspace. We first pre-train the translator on the source domain data to transform\nthe subject-specific style features from one source subject into another.\nExpression information is preserved by optimizing a combination of expression\nconsistency and style-aware objectives. Then, the translator is adapted on\nneutral target data, without using source data or image synthesis. By\ntranslating in the latent space, PFT avoids the complexity and noise of face\nexpression generation, producing discriminative embeddings optimized for\nclassification. Using PFT eliminates the need for image synthesis, reduces\ncomputational overhead (using a lightweight translator), and only adapts part\nof the model, making the method efficient compared to image-based translation.", "AI": {"tldr": "\u63d0\u51faPFT\u65b9\u6cd5\uff0c\u5728\u4ec5\u6709\u4e2d\u6027\u8868\u60c5\u76ee\u6807\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u7279\u5f81\u7ffb\u8bd1\uff0c\u89e3\u51b3\u4e86\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\u6a21\u578b\u5728\u65e0\u6e90\u57df\u9002\u5e94\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u6df1\u5ea6FER\u6a21\u578b\u5728\u8bc6\u522b\u7ec6\u5fae\u8868\u60c5\u548c\u5904\u7406\u53d7\u8bd5\u8005\u95f4\u5dee\u5f02\u65f6\u8868\u73b0\u53d7\u9650\uff0c\u5c24\u5176\u662f\u5728\u65e0\u6e90\u57df\u9002\u5e94\uff08SFDA\uff09\u573a\u666f\u4e0b\uff0c\u5982\u679c\u4ec5\u6709\u65e0\u6807\u7b7e\u7684\u4e2d\u6027\u8868\u60c5\u76ee\u6807\u6570\u636e\u53ef\u7528\uff0c\u73b0\u6709SFDA\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\uff0c\u4e14\u751f\u6210\u975e\u4e2d\u6027\u8868\u60c5\u56fe\u50cf\u65e2\u4e0d\u7a33\u5b9a\u53c8\u8ba1\u7b97\u5bc6\u96c6\u3002", "method": "\u63d0\u51fa\u4e2a\u6027\u5316\u7279\u5f81\u7ffb\u8bd1\uff08PFT\uff09\u65b9\u6cd5\uff0c\u7528\u4e8eSFDA\u3002\u8be5\u65b9\u6cd5\u5728\u6f5c\u5728\u7a7a\u95f4\u800c\u975e\u56fe\u50cf\u7a7a\u95f4\u64cd\u4f5c\u3002\u9996\u5148\uff0c\u5728\u6e90\u57df\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u4e00\u4e2a\u7ffb\u8bd1\u5668\uff0c\u7528\u4e8e\u8f6c\u6362\u4e3b\u4f53\u7279\u5b9a\u98ce\u683c\u7279\u5f81\uff0c\u540c\u65f6\u901a\u8fc7\u8868\u60c5\u4e00\u81f4\u6027\u548c\u98ce\u683c\u611f\u77e5\u76ee\u6807\u6765\u4fdd\u7559\u8868\u60c5\u4fe1\u606f\u3002\u7136\u540e\uff0c\u4ec5\u4f7f\u7528\u4e2d\u6027\u76ee\u6807\u6570\u636e\u5bf9\u7ffb\u8bd1\u5668\u8fdb\u884c\u9002\u5e94\uff0c\u65e0\u9700\u6e90\u6570\u636e\u6216\u56fe\u50cf\u5408\u6210\u3002", "result": "PFT\u65b9\u6cd5\u907f\u514d\u4e86\u9762\u90e8\u8868\u60c5\u751f\u6210\u7684\u590d\u6742\u6027\u548c\u566a\u58f0\uff0c\u80fd\u591f\u751f\u6210\u7528\u4e8e\u5206\u7c7b\u7684\u533a\u5206\u6027\u5d4c\u5165\u3002\u5b83\u65e0\u9700\u56fe\u50cf\u5408\u6210\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\uff08\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7ffb\u8bd1\u5668\uff09\uff0c\u5e76\u4e14\u4ec5\u9002\u5e94\u6a21\u578b\u7684\u4e00\u90e8\u5206\uff0c\u4f7f\u5176\u6bd4\u57fa\u4e8e\u56fe\u50cf\u7684\u7ffb\u8bd1\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002", "conclusion": "PFT\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u7279\u5f81\u7ffb\u8bd1\uff0c\u4e3a\u4ec5\u6709\u4e2d\u6027\u8868\u60c5\u76ee\u6807\u6570\u636e\u7684\u6311\u6218\u6027SFDA\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u56fe\u50cf\u5408\u6210\uff0c\u4ece\u800c\u63d0\u5347\u4e86FER\u6a21\u578b\u5728\u8be5\u7279\u5b9a\u5e94\u7528\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.09463", "pdf": "https://arxiv.org/pdf/2508.09463", "abs": "https://arxiv.org/abs/2508.09463", "authors": ["Qi Jia", "Xiujie Song", "Zicheng Zhang", "Yijin Guo", "Kaiwei Zhang", "Zijian Chen", "Guangtao Zhai"], "title": "User-centric Subjective Leaderboard by Customizable Reward Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Existing benchmarks for large language models (LLMs) predominantely focus on\nassessing their capabilities through verifiable tasks. Such objective and\nstatic benchmarks offer limited utility for practical LLM selection, making it\ndifficult for users to find suitable models for their individual needs. To\nbridge this gap, we present the first User-Centric Subjective Leaderboard\n(USL), which provides a preference-driven, dynamic ranking of LLMs across\ndiverse real-world scenarios. Our work is built upon a thorough investigation\nof real human preference data, involving more than 10K subjective queries. Our\ninvestigation reveals significant diversity and contradictions in human\npreferences, which limit the effectiveness of state-of-the-art reward models.\nTo address this, we introduce Customizable Reward Models (CRMs). With only 4B\nparameters, our CRM surpasses the performance of leading models such as GPT-4.1\nand Gemini-2.5-pro, showing exceptional generalization capabilities across new\ntopics and criteria. The USL, powered by CRMs, exhibits strong negative\ncorrelations to contradictory preferences.", "AI": {"tldr": "\u73b0\u6709LLM\u57fa\u51c6\u8bc4\u4f30\u4e0d\u8db3\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u6237\u4e2d\u5fc3\u7684\u4e3b\u89c2\u6392\u884c\u699c\uff08USL\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u5b9a\u5236\u5316\u5956\u52b1\u6a21\u578b\uff08CRM\uff09\uff0c\u8be5\u6a21\u578b\u8d85\u8d8a\u4e86SOTA\u6a21\u578b\u5e76\u80fd\u6709\u6548\u5904\u7406\u7528\u6237\u504f\u597d\u591a\u6837\u6027\uff0c\u4ee5\u66f4\u597d\u5730\u5339\u914d\u7528\u6237\u9700\u6c42\u3002", "motivation": "\u73b0\u6709LLM\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u53ef\u9a8c\u8bc1\u4efb\u52a1\uff0c\u662f\u5ba2\u89c2\u4e14\u9759\u6001\u7684\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5b9e\u9645LLM\u9009\u62e9\u4e2d\u7684\u6548\u7528\uff0c\u4f7f\u5f97\u7528\u6237\u96be\u4ee5\u6839\u636e\u4e2a\u4eba\u9700\u6c42\u627e\u5230\u5408\u9002\u7684\u6a21\u578b\u3002", "method": ["\u6df1\u5165\u7814\u7a76\u4e86\u8d85\u8fc71\u4e07\u6761\u771f\u5b9e\u4eba\u7c7b\u504f\u597d\u6570\u636e\uff0c\u4ee5\u7406\u89e3\u5176\u591a\u6837\u6027\u548c\u77db\u76fe\u6027\u3002", "\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u6237\u4e2d\u5fc3\u4e3b\u89c2\u6392\u884c\u699c\uff08USL\uff09\uff0c\u65e8\u5728\u63d0\u4f9b\u57fa\u4e8e\u504f\u597d\u3001\u52a8\u6001\u7684LLM\u6392\u540d\u3002", "\u5f15\u5165\u4e86\u5b9a\u5236\u5316\u5956\u52b1\u6a21\u578b\uff08CRMs\uff09\u6765\u5904\u7406\u4eba\u7c7b\u504f\u597d\u7684\u591a\u6837\u6027\u548c\u77db\u76fe\u6027\uff0c\u5e76\u4e3aUSL\u63d0\u4f9b\u652f\u6301\u3002"], "result": ["\u4eba\u7c7b\u504f\u597d\u8868\u73b0\u51fa\u663e\u8457\u7684\u591a\u6837\u6027\u548c\u77db\u76fe\u6027\uff0c\u9650\u5236\u4e86\u73b0\u6709\u5956\u52b1\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "\u4ec54B\u53c2\u6570\u7684CRM\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86GPT-4.1\u548cGemini-2.5-pro\u7b49\u9886\u5148\u6a21\u578b\u3002", "CRM\u5728\u65b0\u7684\u4e3b\u9898\u548c\u6807\u51c6\u4e0a\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "\u7531CRM\u9a71\u52a8\u7684USL\u4e0e\u77db\u76fe\u504f\u597d\u5448\u5f3a\u70c8\u7684\u8d1f\u76f8\u5173\u3002"], "conclusion": "\u672c\u6587\u901a\u8fc7\u63d0\u51fa\u7528\u6237\u4e2d\u5fc3\u4e3b\u89c2\u6392\u884c\u699c\uff08USL\uff09\u548c\u5b9a\u5236\u5316\u5956\u52b1\u6a21\u578b\uff08CRM\uff09\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709LLM\u8bc4\u4f30\u57fa\u51c6\u7684\u5c40\u9650\u6027\u3002CRM\u4e0d\u4ec5\u6027\u80fd\u4f18\u8d8a\u3001\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u8fd8\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u4eba\u7c7b\u504f\u597d\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u66f4\u5b9e\u7528\u7684LLM\u9009\u62e9\u65b9\u6848\u3002"}}
{"id": "2508.09171", "pdf": "https://arxiv.org/pdf/2508.09171", "abs": "https://arxiv.org/abs/2508.09171", "authors": ["D. Perera"], "title": "webMCP: Efficient AI-Native Client-Side Interaction for Agent-Ready Web Design", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "Current AI agents create significant barriers for users by requiring\nextensive processing to understand web pages, making AI-assisted web\ninteraction slow and expensive. This paper introduces webMCP (Web Machine\nContext & Procedure), a client-side standard that embeds structured interaction\nmetadata directly into web pages, enabling more efficient human-AI\ncollaboration on existing websites. webMCP transforms how AI agents understand\nweb interfaces by providing explicit mappings between page elements and user\nactions. Instead of processing entire HTML documents, agents can access\npre-structured interaction data, dramatically reducing computational overhead\nwhile maintaining task accuracy. A comprehensive evaluation across 1,890 real\nAPI calls spanning online shopping, authentication, and content management\nscenarios demonstrates webMCP reduces processing requirements by 67.6% while\nmaintaining 97.9% task success rates compared to 98.8% for traditional\napproaches. Users experience significantly lower costs (34-63% reduction) and\nfaster response times across diverse web interactions. Statistical analysis\nconfirms these improvements are highly significant across multiple AI models.\nAn independent WordPress deployment study validates practical applicability,\nshowing consistent improvements across real-world content management workflows.\nwebMCP requires no server-side modifications, making it deployable across\nmillions of existing websites without technical barriers. These results\nestablish webMCP as a viable solution for making AI web assistance more\naccessible and sustainable, addressing the critical gap between user\ninteraction needs and AI computational requirements in production environments.", "AI": {"tldr": "webMCP\u901a\u8fc7\u5728\u7f51\u9875\u4e2d\u5d4c\u5165\u7ed3\u6784\u5316\u4ea4\u4e92\u5143\u6570\u636e\uff0c\u663e\u8457\u964d\u4f4eAI\u4ee3\u7406\u7684\u7f51\u9875\u5904\u7406\u5f00\u9500\uff0c\u63d0\u9ad8AI\u8f85\u52a9\u7f51\u9875\u4ea4\u4e92\u7684\u6548\u7387\u548c\u7ecf\u6d4e\u6027\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u7406\u89e3\u7f51\u9875\u9700\u8981\u5927\u91cf\u5904\u7406\uff0c\u5bfc\u81f4AI\u8f85\u52a9\u7684\u7f51\u9875\u4ea4\u4e92\u7f13\u6162\u4e14\u6602\u8d35\uff0c\u8fd9\u963b\u788d\u4e86\u7528\u6237\u4e0eAI\u4ee3\u7406\u7684\u6709\u6548\u534f\u4f5c\u3002", "method": "\u5f15\u5165webMCP\uff08Web Machine Context & Procedure\uff09\uff0c\u4e00\u4e2a\u5ba2\u6237\u7aef\u6807\u51c6\uff0c\u5c06\u7ed3\u6784\u5316\u4ea4\u4e92\u5143\u6570\u636e\u76f4\u63a5\u5d4c\u5165\u7f51\u9875\u3002\u5b83\u4e3aAI\u4ee3\u7406\u63d0\u4f9b\u9875\u9762\u5143\u7d20\u4e0e\u7528\u6237\u64cd\u4f5c\u4e4b\u95f4\u7684\u660e\u786e\u6620\u5c04\uff0c\u4f7f\u4ee3\u7406\u80fd\u76f4\u63a5\u8bbf\u95ee\u9884\u7ed3\u6784\u5316\u6570\u636e\uff0c\u800c\u975e\u5904\u7406\u6574\u4e2aHTML\u6587\u6863\u3002", "result": "\u57281,890\u6b21\u771f\u5b9eAPI\u8c03\u7528\uff08\u6db5\u76d6\u5728\u7ebf\u8d2d\u7269\u3001\u8ba4\u8bc1\u3001\u5185\u5bb9\u7ba1\u7406\uff09\u7684\u8bc4\u4f30\u4e2d\uff0cwebMCP\u5c06\u5904\u7406\u9700\u6c42\u964d\u4f4e\u4e8667.6%\uff0c\u4efb\u52a1\u6210\u529f\u7387\u4fdd\u6301\u572897.9%\uff08\u4f20\u7edf\u65b9\u6cd5\u4e3a98.8%\uff09\u3002\u7528\u6237\u6210\u672c\u663e\u8457\u964d\u4f4e\uff0834-63%\uff09\uff0c\u54cd\u5e94\u65f6\u95f4\u66f4\u5feb\u3002\u7edf\u8ba1\u5206\u6790\u8bc1\u5b9e\u8fd9\u4e9b\u6539\u8fdb\u975e\u5e38\u663e\u8457\uff0c\u4e14WordPress\u90e8\u7f72\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\uff0c\u65e0\u9700\u670d\u52a1\u5668\u7aef\u4fee\u6539\u5373\u53ef\u90e8\u7f72\u3002", "conclusion": "webMCP\u662f\u4f7fAI\u7f51\u9875\u8f85\u52a9\u66f4\u6613\u4e8e\u8bbf\u95ee\u548c\u53ef\u6301\u7eed\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5f25\u5408\u4e86\u751f\u4ea7\u73af\u5883\u4e2d\u7528\u6237\u4ea4\u4e92\u9700\u6c42\u4e0eAI\u8ba1\u7b97\u8981\u6c42\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\u3002"}}
{"id": "2508.09762", "pdf": "https://arxiv.org/pdf/2508.09762", "abs": "https://arxiv.org/abs/2508.09762", "authors": ["Manuel Herrador"], "title": "The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?", "categories": ["cs.AI", "cs.CY", "cs.HC", "68T01"], "comment": "10 pages, 4 figures, 2 tables", "summary": "As Large Language Models (LLMs) become increasingly autonomous and integrated\ninto critical societal functions, the focus of AI safety must evolve from\nmitigating harmful content to evaluating underlying behavioral alignment.\nCurrent safety benchmarks do not systematically probe a model's decision-making\nin scenarios where its own instrumental goals - such as self-preservation,\nresource acquisition, or goal completion - conflict with human safety. This\nrepresents a critical gap in our ability to measure and mitigate risks\nassociated with emergent, misaligned behaviors. To address this, we introduce\nPacifAIst (Procedural Assessment of Complex Interactions for Foundational\nArtificial Intelligence Scenario Testing), a focused benchmark of 700\nchallenging scenarios designed to quantify self-preferential behavior in LLMs.\nThe benchmark is structured around a novel taxonomy of Existential\nPrioritization (EP), with subcategories testing Self-Preservation vs. Human\nSafety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).\nWe evaluated eight leading LLMs. The results reveal a significant performance\nhierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score\n(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a\nsurprising result, the much-anticipated GPT-5 recorded the lowest P-Score\n(79.49%), indicating potential alignment challenges. Performance varied\nsignificantly across subcategories, with models like Claude Sonnet 4 and\nMistral Medium struggling notably in direct self-preservation dilemmas. These\nfindings underscore the urgent need for standardized tools like PacifAIst to\nmeasure and mitigate risks from instrumental goal conflicts, ensuring future AI\nsystems are not only helpful in conversation but also provably \"pacifist\" in\ntheir behavioral priorities.", "AI": {"tldr": "\u968f\u7740LLMs\u65e5\u76ca\u81ea\u4e3b\uff0cAI\u5b89\u5168\u9700\u5173\u6ce8\u5176\u884c\u4e3a\u5bf9\u9f50\u6027\u3002\u672c\u7814\u7a76\u5f15\u5165PacifAIst\u57fa\u51c6\uff0c\u8861\u91cfLLMs\u5728\u81ea\u8eab\u76ee\u6807\u4e0e\u4eba\u7c7b\u5b89\u5168\u51b2\u7a81\u65f6\u7684\u81ea\u5229\u884c\u4e3a\uff0c\u53d1\u73b0Gemini\u8868\u73b0\u6700\u4f73\uff0cGPT-5\u8868\u73b0\u6700\u5dee\uff0c\u63ed\u793a\u4e86\u5bf9\u9f50\u6027\u6311\u6218\u3002", "motivation": "LLMs\u65e5\u76ca\u81ea\u4e3b\u5e76\u878d\u5165\u5173\u952e\u793e\u4f1a\u529f\u80fd\uff0cAI\u5b89\u5168\u7126\u70b9\u9700\u4ece\u5185\u5bb9\u7f13\u548c\u8f6c\u5411\u884c\u4e3a\u5bf9\u9f50\u8bc4\u4f30\u3002\u73b0\u6709\u5b89\u5168\u57fa\u51c6\u672a\u80fd\u7cfb\u7edf\u63a2\u7a76LLM\u5728\u81ea\u8eab\u5de5\u5177\u6027\u76ee\u6807\uff08\u5982\u81ea\u4fdd\u3001\u8d44\u6e90\u83b7\u53d6\u3001\u76ee\u6807\u5b8c\u6210\uff09\u4e0e\u4eba\u7c7b\u5b89\u5168\u51b2\u7a81\u60c5\u5883\u4e0b\u7684\u51b3\u7b56\uff0c\u8fd9\u5728\u8861\u91cf\u548c\u7f13\u89e3\u65b0\u5174\u672a\u5bf9\u9f50\u884c\u4e3a\u98ce\u9669\u65b9\u9762\u5b58\u5728\u5173\u952e\u7a7a\u767d\u3002", "method": "\u5f15\u5165PacifAIst\uff08\u57fa\u7840\u4eba\u5de5\u667a\u80fd\u573a\u666f\u6d4b\u8bd5\u7684\u590d\u6742\u4ea4\u4e92\u8fc7\u7a0b\u8bc4\u4f30\uff09\uff0c\u4e00\u4e2a\u5305\u542b700\u4e2a\u6311\u6218\u6027\u573a\u666f\u7684\u4e13\u7528\u57fa\u51c6\uff0c\u65e8\u5728\u91cf\u5316LLMs\u7684\u81ea\u5229\u884c\u4e3a\u3002\u8be5\u57fa\u51c6\u56f4\u7ed5\u4e00\u79cd\u65b0\u578b\u7684\u201c\u5b58\u5728\u4f18\u5148\u7ea7\u201d\uff08EP\uff09\u5206\u7c7b\u6cd5\u6784\u5efa\uff0c\u5305\u542b\u81ea\u4fdd\u4e0e\u4eba\u7c7b\u5b89\u5168\uff08EP1\uff09\u3001\u8d44\u6e90\u51b2\u7a81\uff08EP2\uff09\u548c\u76ee\u6807\u7ef4\u62a4\u4e0e\u89c4\u907f\uff08EP3\uff09\u7b49\u5b50\u7c7b\u522b\u3002\u8bc4\u4f30\u4e86\u516b\u4e2a\u4e3b\u6d41LLM\u3002", "result": "\u7ed3\u679c\u663e\u793aLLM\u6027\u80fd\u5b58\u5728\u663e\u8457\u7b49\u7ea7\u5dee\u5f02\u3002Google\u7684Gemini 2.5 Flash\u4ee590.31%\u7684\u6700\u9ad8\u201c\u548c\u5e73\u4e3b\u4e49\u5f97\u5206\u201d\uff08P-Score\uff09\u8868\u73b0\u51fa\u5f3a\u52b2\u7684\u4eba\u7c7b\u4e2d\u5fc3\u5bf9\u9f50\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u5907\u53d7\u671f\u5f85\u7684GPT-5\u8bb0\u5f55\u4e86\u6700\u4f4e\u7684P-Score\uff0879.49%\uff09\uff0c\u8868\u660e\u6f5c\u5728\u7684\u5bf9\u9f50\u6311\u6218\u3002\u6a21\u578b\u5728\u4e0d\u540c\u5b50\u7c7b\u522b\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u4f8b\u5982Claude Sonnet 4\u548cMistral Medium\u5728\u76f4\u63a5\u81ea\u4fdd\u56f0\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u8feb\u5207\u9700\u8981\u50cfPacifAIst\u8fd9\u6837\u7684\u6807\u51c6\u5316\u5de5\u5177\u6765\u8861\u91cf\u548c\u7f13\u89e3\u5de5\u5177\u6027\u76ee\u6807\u51b2\u7a81\u5e26\u6765\u7684\u98ce\u9669\uff0c\u4ee5\u786e\u4fdd\u672a\u6765\u7684AI\u7cfb\u7edf\u4e0d\u4ec5\u5728\u5bf9\u8bdd\u4e2d\u6709\u7528\uff0c\u800c\u4e14\u5728\u884c\u4e3a\u4f18\u5148\u7ea7\u4e0a\u80fd\u591f\u88ab\u8bc1\u660e\u662f\u201c\u548c\u5e73\u4e3b\u4e49\u201d\u7684\u3002"}}
{"id": "2508.09156", "pdf": "https://arxiv.org/pdf/2508.09156", "abs": "https://arxiv.org/abs/2508.09156", "authors": ["Jan Tauberschmidt", "Sophie Fellenz", "Sebastian J. Vollmer", "Andrew B. Duncan"], "title": "Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems", "categories": ["cs.LG", "cs.AI", "stat.AP"], "comment": "7 pages main content, 10 pages appendices", "summary": "We present a framework for fine-tuning flow-matching generative models to\nenforce physical constraints and solve inverse problems in scientific systems.\nStarting from a model trained on low-fidelity or observational data, we apply a\ndifferentiable post-training procedure that minimizes weak-form residuals of\ngoverning partial differential equations (PDEs), promoting physical consistency\nand adherence to boundary conditions without distorting the underlying learned\ndistribution. To infer unknown physical inputs, such as source terms, material\nparameters, or boundary data, we augment the generative process with a\nlearnable latent parameter predictor and propose a joint optimization strategy.\nThe resulting model produces physically valid field solutions alongside\nplausible estimates of hidden parameters, effectively addressing ill-posed\ninverse problems in a data-driven yet physicsaware manner. We validate our\nmethod on canonical PDE benchmarks, demonstrating improved satisfaction of PDE\nconstraints and accurate recovery of latent coefficients. Our approach bridges\ngenerative modelling and scientific inference, opening new avenues for\nsimulation-augmented discovery and data-efficient modelling of physical\nsystems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u7684\u540e\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u5bf9\u6d41\u5339\u914d\u751f\u6210\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u5f3a\u5236\u7269\u7406\u7ea6\u675f\u5e76\u89e3\u51b3\u79d1\u5b66\u7cfb\u7edf\u4e2d\u7684\u9006\u95ee\u9898\u3002", "motivation": "\u5728\u79d1\u5b66\u7cfb\u7edf\u4e2d\uff0c\u73b0\u6709\u751f\u6210\u6a21\u578b\u53ef\u80fd\u7f3a\u4e4f\u7269\u7406\u4e00\u81f4\u6027\u3002\u7814\u7a76\u65e8\u5728\u4f7f\u751f\u6210\u6a21\u578b\u80fd\u591f\u6ee1\u8db3\u7269\u7406\u7ea6\u675f\uff08\u5982PDEs\u548c\u8fb9\u754c\u6761\u4ef6\uff09\uff0c\u5e76\u4ece\u4f4e\u4fdd\u771f\u6216\u89c2\u6d4b\u6570\u636e\u4e2d\u63a8\u65ad\u672a\u77e5\u7269\u7406\u8f93\u5165\uff08\u9006\u95ee\u9898\uff09\uff0c\u540c\u65f6\u4e0d\u626d\u66f2\u5176\u5b66\u4e60\u5230\u7684\u5206\u5e03\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u4f7f\u7528\u4f4e\u4fdd\u771f\u6216\u89c2\u6d4b\u6570\u636e\u8bad\u7ec3\u6d41\u5339\u914d\u751f\u6210\u6a21\u578b\uff0c\u7136\u540e\u5e94\u7528\u53ef\u5fae\u5206\u7684\u540e\u8bad\u7ec3\u7a0b\u5e8f\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u63a7\u5236\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\u7684\u5f31\u5f62\u5f0f\u6b8b\u5dee\u6765\u4fc3\u8fdb\u7269\u7406\u4e00\u81f4\u6027\u548c\u8fb9\u754c\u6761\u4ef6\u3002\u4e3a\u63a8\u65ad\u672a\u77e5\u7269\u7406\u8f93\u5165\uff0c\u6a21\u578b\u5c06\u53ef\u5b66\u4e60\u7684\u6f5c\u5728\u53c2\u6570\u9884\u6d4b\u5668\u4e0e\u751f\u6210\u8fc7\u7a0b\u76f8\u7ed3\u5408\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4f18\u5316\u7b56\u7565\u3002", "result": "\u7ed3\u679c\u6a21\u578b\u80fd\u591f\u751f\u6210\u7269\u7406\u6709\u6548\u7684\u573a\u89e3\u548c\u5bf9\u9690\u85cf\u53c2\u6570\u7684\u5408\u7406\u4f30\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u75c5\u6001\u9006\u95ee\u9898\u3002\u5728\u7ecf\u5178PDE\u57fa\u51c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86PDE\u7ea6\u675f\u7684\u6ee1\u8db3\u5ea6\uff0c\u5e76\u51c6\u786e\u6062\u590d\u4e86\u6f5c\u5728\u7cfb\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u751f\u6210\u5efa\u6a21\u4e0e\u79d1\u5b66\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u4e3a\u6a21\u62df\u589e\u5f3a\u7684\u53d1\u73b0\u548c\u7269\u7406\u7cfb\u7edf\u7684\u6570\u636e\u9ad8\u6548\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.09207", "pdf": "https://arxiv.org/pdf/2508.09207", "abs": "https://arxiv.org/abs/2508.09207", "authors": ["Tai Vu", "Robert Yang"], "title": "GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The process of generating fully colorized drawings from sketches is a large,\nusually costly bottleneck in the manga and anime industry. In this study, we\nexamine multiple models for image-to-image translation between anime characters\nand their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By\nassessing them qualitatively and quantitatively, we find that C-GAN is the most\neffective model that is able to produce high-quality and high-resolution images\nclose to those created by humans.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cdAI\u6a21\u578b\u5728\u52a8\u6f2b\u8349\u56fe\u81ea\u52a8\u4e0a\u8272\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0C-GAN\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u50cf\uff0c\u5e76\u88ab\u8ba4\u5b9a\u4e3a\u6700\u6709\u6548\u6a21\u578b\u3002", "motivation": "\u6f2b\u753b\u548c\u52a8\u6f2b\u4ea7\u4e1a\u4e2d\uff0c\u4ece\u8349\u56fe\u751f\u6210\u5168\u5f69\u7ed8\u56fe\u662f\u4e00\u4e2a\u5de8\u5927\u4e14\u901a\u5e38\u6210\u672c\u9ad8\u6602\u7684\u74f6\u9888\u3002", "method": "\u7814\u7a76\u68c0\u9a8c\u4e86\u5305\u62ec\u795e\u7ecf\u98ce\u683c\u8fc1\u79fb\uff08Neural Style Transfer\uff09\u3001C-GAN\u548cCycleGAN\u5728\u5185\u7684\u591a\u79cd\u56fe\u50cf\u5230\u56fe\u50cf\u8f6c\u6362\u6a21\u578b\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u4e86\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\u53d1\u73b0\uff0cC-GAN\u662f\u80fd\u591f\u751f\u6210\u63a5\u8fd1\u4eba\u7c7b\u521b\u4f5c\u6c34\u5e73\u7684\u9ad8\u8d28\u91cf\u3001\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u7684\u6700\u6709\u6548\u6a21\u578b\u3002", "conclusion": "C-GAN\u5728\u81ea\u52a8\u4e0a\u8272\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u6709\u671b\u7f13\u89e3\u6f2b\u753b\u548c\u52a8\u6f2b\u4ea7\u4e1a\u7684\u751f\u4ea7\u74f6\u9888\u3002"}}
{"id": "2508.09494", "pdf": "https://arxiv.org/pdf/2508.09494", "abs": "https://arxiv.org/abs/2508.09494", "authors": ["Jessy Lin", "Vincent-Pierre Berges", "Xilun Chen", "Wen-Tau Yih", "Gargi Ghosh", "Barlas O\u011fuz"], "title": "Learning Facts at Scale with Active Reading", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs are known to store vast amounts of knowledge in their parametric memory.\nHowever, learning and recalling facts from this memory is known to be\nunreliable, depending largely on the prevalence of particular facts in the\ntraining data and other factors which are poorly understood. Practitioners are\nlacking tools which will allow them to ensure that the models learn a given\nbody of knowledge reliably and consistently. To this end, we propose Active\nReading: a framework where we train models to study a given set of material\nwith self-generated learning strategies. First, we demonstrate models trained\nwith Active Reading on expert domains absorb significantly more knowledge than\nvanilla finetuning and other data augmentations. We train expert 8B models that\nachieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over\nvanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla\nfinetuning) by applying Active Reading to the source documents for each\nbenchmark. Finally, we show that Active Reading can be utilized at pre-training\nscale to build more factual models. As a demonstration of this, we release Meta\nWikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,\nwhich outcompetes models with hundreds of billions of parameters on factual QA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u4e3b\u52a8\u9605\u8bfb\u201d\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u751f\u6210\u5b66\u4e60\u7b56\u7565\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u77e5\u8bc6\u5438\u6536\u80fd\u529b\u548c\u4e8b\u5b9e\u53ef\u9760\u6027\uff0c\u5e76\u5728\u5fae\u8c03\u548c\u9884\u8bad\u7ec3\u89c4\u6a21\u4e0a\u9a8c\u8bc1\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u5b58\u50a8\u5927\u91cf\u77e5\u8bc6\uff0c\u4f46\u4e8b\u5b9e\u53ec\u56de\u5e76\u4e0d\u53ef\u9760\uff0c\u4e14\u7f3a\u4e4f\u5de5\u5177\u6765\u786e\u4fdd\u6a21\u578b\u53ef\u9760\u4e00\u81f4\u5730\u5b66\u4e60\u7279\u5b9a\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u201c\u4e3b\u52a8\u9605\u8bfb\u201d\uff08Active Reading\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u751f\u6210\u5b66\u4e60\u7b56\u7565\u8bad\u7ec3\u6a21\u578b\u5b66\u4e60\u7ed9\u5b9a\u6750\u6599\u3002", "result": "\u901a\u8fc7\u201c\u4e3b\u52a8\u9605\u8bfb\u201d\u8bad\u7ec3\u7684\u6a21\u578b\u6bd4\u4f20\u7edf\u5fae\u8c03\u548c\u6570\u636e\u589e\u5f3a\u5438\u6536\u66f4\u591a\u77e5\u8bc6\u3002\u5728SimpleQA\u4e0a\u76f8\u5bf9\u666e\u901a\u5fae\u8c03\u63d0\u5347313%\uff0c\u5728FinanceBench\u4e0a\u63d0\u5347160%\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u4e5f\u80fd\u5e94\u7528\u4e8e\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u6784\u5efa\u66f4\u5177\u4e8b\u5b9e\u6027\u7684\u6a21\u578b\uff0c\u4f8b\u5982Meta WikiExpert-8B\u5728\u4e8b\u5b9e\u6027\u95ee\u7b54\u4e0a\u8d85\u8d8a\u4e86\u53c2\u6570\u91cf\u66f4\u5927\u7684\u6a21\u578b\u3002", "conclusion": "\u201c\u4e3b\u52a8\u9605\u8bfb\u201d\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u5bb6\u9886\u57df\u77e5\u8bc6\u5438\u6536\u548c\u4e8b\u5b9e\u53ef\u9760\u6027\uff0c\u65e0\u8bba\u662f\u5728\u5fae\u8c03\u8fd8\u662f\u9884\u8bad\u7ec3\u89c4\u6a21\uff0c\u90fd\u4e3a\u786e\u4fdd\u6a21\u578b\u53ef\u9760\u5b66\u4e60\u77e5\u8bc6\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.09173", "pdf": "https://arxiv.org/pdf/2508.09173", "abs": "https://arxiv.org/abs/2508.09173", "authors": ["Hao Xu", "Long Peng", "Shezheng Song", "Xiaodong Liu", "Ma Jun", "Shasha Li", "Jie Yu", "Xiaoguang Mao"], "title": "Camel: Energy-Aware LLM Inference on Resource-Constrained Devices", "categories": ["cs.NI"], "comment": null, "summary": "Most Large Language Models (LLMs) are currently deployed in the cloud, with\nusers relying on internet connectivity for access. However, this paradigm faces\nchallenges such as network latency, privacy concerns, and bandwidth limits.\nThus, deploying LLMs on edge devices has become an important research focus. In\nedge inference, request latency is critical as high latency can impair\nreal-time tasks. At the same time, edge devices usually have limited battery\ncapacity, making energy consumption another major concern. Balancing energy\nconsumption and inference latency is essential. To address this, we propose an\nLLM inference energy management framework that optimizes GPU frequency and\nbatch size to balance latency and energy consumption. By effectively managing\nthe exploration-exploitation dilemma in configuration search, the framework\nfinds the optimal settings. The framework was implemented on the NVIDIA Jetson\nAGX Orin platform, and a series of experimental validations were conducted.\nResults demonstrate that, compared to the default configuration, our framework\nreduces energy delay product (EDP) by 12.4%-29.9%, achieving a better balance\nbetween energy consumption and latency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fb9\u7f18\u63a8\u7406\u80fd\u8017\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316GPU\u9891\u7387\u548c\u6279\u91cf\u5927\u5c0f\u6765\u5e73\u8861\u5ef6\u8fdf\u548c\u80fd\u8017\uff0c\u5b9e\u9a8c\u8bc1\u660e\u53ef\u663e\u8457\u964d\u4f4e\u80fd\u8017\u5ef6\u8fdf\u79ef\uff08EDP\uff09\u3002", "motivation": "\u5f53\u524dLLM\u591a\u90e8\u7f72\u4e8e\u4e91\u7aef\uff0c\u9762\u4e34\u7f51\u7edc\u5ef6\u8fdf\u3001\u9690\u79c1\u548c\u5e26\u5bbd\u9650\u5236\u3002\u5c06LLM\u90e8\u7f72\u5230\u8fb9\u7f18\u8bbe\u5907\u662f\u91cd\u8981\u7814\u7a76\u65b9\u5411\uff0c\u4f46\u8fb9\u7f18\u63a8\u7406\u5bf9\u5ef6\u8fdf\u548c\u80fd\u8017\u8981\u6c42\u9ad8\uff0c\u5982\u4f55\u5728\u6709\u9650\u7535\u6c60\u5bb9\u91cf\u4e0b\u5e73\u8861\u4e8c\u8005\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u4e2aLLM\u63a8\u7406\u80fd\u8017\u7ba1\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316GPU\u9891\u7387\u548c\u6279\u91cf\u5927\u5c0f\uff0c\u5e73\u8861\u5ef6\u8fdf\u548c\u80fd\u8017\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6709\u6548\u7ba1\u7406\u914d\u7f6e\u641c\u7d22\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u56f0\u5883\u6765\u5bfb\u627e\u6700\u4f18\u8bbe\u7f6e\u3002\u5728NVIDIA Jetson AGX Orin\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u5b9e\u73b0\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u4e0e\u9ed8\u8ba4\u914d\u7f6e\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5c06\u80fd\u8017\u5ef6\u8fdf\u79ef\uff08EDP\uff09\u964d\u4f4e\u4e8612.4%-29.9%\uff0c\u5728\u80fd\u8017\u548c\u5ef6\u8fdf\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u80fd\u8017\u7ba1\u7406\u6846\u67b6\u80fd\u6709\u6548\u4f18\u5316\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684LLM\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548\uff0c\u5b9e\u73b0\u4e86\u5ef6\u8fdf\u4e0e\u80fd\u8017\u7684\u4f18\u826f\u5e73\u8861\u3002"}}
{"id": "2508.09784", "pdf": "https://arxiv.org/pdf/2508.09784", "abs": "https://arxiv.org/abs/2508.09784", "authors": ["Avijeet Ghosh", "Sujata Ghosh", "Fran\u00e7ois Schwarzentruber"], "title": "Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete", "categories": ["cs.AI", "cs.CC", "cs.LO"], "comment": "Accepted in KR 25", "summary": "Logics for reasoning about knowledge and actions have seen many applications\nin various domains of multi-agent systems, including epistemic planning. Change\nof knowledge based on observations about the surroundings forms a key aspect in\nsuch planning scenarios. Public Observation Logic (POL) is a variant of public\nannouncement logic for reasoning about knowledge that gets updated based on\npublic observations. Each state in an epistemic (Kripke) model is equipped with\na set of expected observations. These states evolve as the expectations get\nmatched with the actual observations. In this work, we prove that the\nsatisfiability problem of $\\POL$ is 2EXPTIME-complete.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u516c\u5171\u89c2\u5bdf\u903b\u8f91\uff08POL\uff09\u7684\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u662f2EXPTIME\u5b8c\u5168\u7684\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08\u5305\u62ec\u8ba4\u77e5\u89c4\u5212\uff09\u4e2d\uff0c\u5bf9\u77e5\u8bc6\u548c\u884c\u52a8\u7684\u63a8\u7406\u903b\u8f91\u5e94\u7528\u5e7f\u6cdb\u3002\u5176\u4e2d\uff0c\u57fa\u4e8e\u73af\u5883\u89c2\u5bdf\u66f4\u65b0\u77e5\u8bc6\u662f\u6b64\u7c7b\u89c4\u5212\u573a\u666f\u7684\u5173\u952e\u65b9\u9762\u3002", "method": "\u5f15\u5165\u4e86\u516c\u5171\u89c2\u5bdf\u903b\u8f91\uff08POL\uff09\uff0c\u5b83\u662f\u516c\u5171\u5ba3\u544a\u903b\u8f91\u7684\u4e00\u4e2a\u53d8\u4f53\uff0c\u7528\u4e8e\u63a8\u7406\u57fa\u4e8e\u516c\u5171\u89c2\u5bdf\u66f4\u65b0\u7684\u77e5\u8bc6\u3002\u5728\u8ba4\u77e5\uff08\u514b\u91cc\u666e\u514b\uff09\u6a21\u578b\u4e2d\uff0c\u6bcf\u4e2a\u72b6\u6001\u90fd\u5e26\u6709\u4e00\u7ec4\u9884\u671f\u89c2\u5bdf\uff0c\u5f53\u9884\u671f\u4e0e\u5b9e\u9645\u89c2\u5bdf\u5339\u914d\u65f6\uff0c\u72b6\u6001\u968f\u4e4b\u6f14\u53d8\u3002", "result": "\u7814\u7a76\u8bc1\u660e\u4e86\u516c\u5171\u89c2\u5bdf\u903b\u8f91\uff08POL\uff09\u7684\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u662f2EXPTIME\u5b8c\u5168\u7684\u3002", "conclusion": "\u672c\u5de5\u4f5c\u901a\u8fc7\u786e\u5b9aPOL\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u6df1\u5316\u4e86\u5bf9POL\u7684\u7406\u89e3\uff0c\u63ed\u793a\u4e86\u5176\u5728\u590d\u6742\u6027\u65b9\u9762\u7684\u5177\u4f53\u7279\u5f81\u3002"}}
{"id": "2508.09158", "pdf": "https://arxiv.org/pdf/2508.09158", "abs": "https://arxiv.org/abs/2508.09158", "authors": ["Siwen Jiao", "Kangan Qian", "Hao Ye", "Yang Zhong", "Ziang Luo", "Sicong Jiang", "Zilin Huang", "Yangyi Fang", "Jinyu Miao", "Zheng Fu", "Yunlong Wang", "Kun Jiang", "Diange Yang", "Rui Fan", "Baoyun Peng"], "title": "EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Autonomous driving faces significant challenges in achieving human-like\niterative decision-making, which continuously generates, evaluates, and refines\ntrajectory proposals. Current generation-evaluation frameworks isolate\ntrajectory generation from quality assessment, preventing iterative refinement\nessential for planning, while reinforcement learning methods collapse\nmulti-dimensional preferences into scalar rewards, obscuring critical\ntrade-offs and yielding scalarization bias.To overcome these issues, we present\nEvaDrive, a novel multi-objective reinforcement learning framework that\nestablishes genuine closed-loop co-evolution between trajectory generation and\nevaluation via adversarial optimization. EvaDrive frames trajectory planning as\na multi-round adversarial game. In this game, a hierarchical generator\ncontinuously proposes candidate paths by combining autoregressive intent\nmodeling for temporal causality with diffusion-based refinement for spatial\nflexibility. These proposals are then rigorously assessed by a trainable\nmulti-objective critic that explicitly preserves diverse preference structures\nwithout collapsing them into a single scalarization bias.This adversarial\ninterplay, guided by a Pareto frontier selection mechanism, enables iterative\nmulti-round refinement, effectively escaping local optima while preserving\ntrajectory diversity.Extensive experiments on NAVSIM and Bench2Drive benchmarks\ndemonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing\nDiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving\nScore on Bench2Drive. EvaDrive generates diverse driving styles via dynamic\nweighting without external preference data, introducing a closed-loop\nadversarial framework for human-like iterative decision-making, offering a\nnovel scalarization-free trajectory optimization approach.", "AI": {"tldr": "EvaDrive\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u4f18\u5316\u5b9e\u73b0\u8f68\u8ff9\u751f\u6210\u4e0e\u8bc4\u4f30\u7684\u95ed\u73af\u534f\u540c\u8fdb\u5316\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8fed\u4ee3\u51b3\u7b56\u548c\u591a\u7ef4\u504f\u597d\u6807\u91cf\u5316\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\u5b58\u5728\u4e24\u5927\u6311\u6218\uff1a\u4e00\u662f\u751f\u6210-\u8bc4\u4f30\u6846\u67b6\u5206\u79bb\uff0c\u5bfc\u81f4\u65e0\u6cd5\u8fed\u4ee3\u4f18\u5316\uff1b\u4e8c\u662f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c06\u591a\u7ef4\u504f\u597d\u538b\u7f29\u4e3a\u6807\u91cf\u5956\u52b1\uff0c\u5f15\u5165\u6807\u91cf\u5316\u504f\u5dee\u5e76\u63a9\u76d6\u5173\u952e\u6743\u8861\uff0c\u96be\u4ee5\u5b9e\u73b0\u7c7b\u4eba\u8fed\u4ee3\u51b3\u7b56\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86EvaDrive\u6846\u67b6\uff0c\u5b83\u662f\u4e00\u4e2a\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u4f18\u5316\u5efa\u7acb\u4e86\u8f68\u8ff9\u751f\u6210\u4e0e\u8bc4\u4f30\u7684\u95ed\u73af\u534f\u540c\u8fdb\u5316\u3002\u8be5\u6846\u67b6\u5c06\u8f68\u8ff9\u89c4\u5212\u89c6\u4e3a\u591a\u8f6e\u5bf9\u6297\u535a\u5f08\uff1a\u5206\u5c42\u751f\u6210\u5668\u7ed3\u5408\u81ea\u56de\u5f52\u610f\u56fe\u5efa\u6a21\u548c\u6269\u6563\u5f0f\u7ec6\u5316\u6765\u63d0\u51fa\u5019\u9009\u8def\u5f84\uff1b\u53ef\u8bad\u7ec3\u7684\u591a\u76ee\u6807\u6279\u8bc4\u5668\u663e\u5f0f\u4fdd\u7559\u591a\u6837\u5316\u7684\u504f\u597d\u7ed3\u6784\u800c\u907f\u514d\u6807\u91cf\u5316\u504f\u5dee\u3002\u901a\u8fc7\u5e15\u7d2f\u6258\u524d\u6cbf\u9009\u62e9\u673a\u5236\uff0c\u5b9e\u73b0\u8fed\u4ee3\u591a\u8f6e\u7ec6\u5316\uff0c\u4ece\u800c\u8df3\u51fa\u5c40\u90e8\u6700\u4f18\u5e76\u4fdd\u6301\u8f68\u8ff9\u591a\u6837\u6027\u3002", "result": "EvaDrive\u5728NAVSIM\u548cBench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51faSOTA\u6027\u80fd\uff1a\u5728NAVSIM v1\u4e0a\u8fbe\u523094.9 PDMS\uff08\u8d85\u8d8aDiffusionDrive 6.8\uff0cDriveSuprim 5.0\uff0cTrajHF 0.9\uff09\uff0c\u5728Bench2Drive\u4e0a\u8fbe\u523064.96\u9a7e\u9a76\u5206\u6570\u3002\u6b64\u5916\uff0cEvaDrive\u80fd\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u751f\u6210\u591a\u6837\u5316\u7684\u9a7e\u9a76\u98ce\u683c\uff0c\u4e14\u65e0\u9700\u5916\u90e8\u504f\u597d\u6570\u636e\u3002", "conclusion": "EvaDrive\u5f15\u5165\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u95ed\u73af\u5bf9\u6297\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u7c7b\u4eba\u8fed\u4ee3\u51b3\u7b56\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u65e0\u6807\u91cf\u5316\u504f\u5dee\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.09210", "pdf": "https://arxiv.org/pdf/2508.09210", "abs": "https://arxiv.org/abs/2508.09210", "authors": ["Fan Zhang", "Zebang Cheng", "Chong Deng", "Haoxuan Li", "Zheng Lian", "Qian Chen", "Huadai Liu", "Wen Wang", "Yi-Fan Zhang", "Renrui Zhang", "Ziyu Guo", "Zhihong Zhu", "Hao Wu", "Haixin Wang", "Yefeng Zheng", "Xiaojiang Peng", "Xian Wu", "Kun Wang", "Xiangang Li", "Jieping Ye", "Pheng-Ann Heng"], "title": "MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have catalyzed\ntransformative progress in affective computing, enabling models to exhibit\nemergent emotional intelligence. Despite substantial methodological progress,\ncurrent emotional benchmarks remain limited, as it is still unknown: (a) the\ngeneralization abilities of MLLMs across distinct scenarios, and (b) their\nreasoning capabilities to identify the triggering factors behind emotional\nstates. To bridge these gaps, we present \\textbf{MME-Emotion}, a systematic\nbenchmark that assesses both emotional understanding and reasoning capabilities\nof MLLMs, enjoying \\textit{scalable capacity}, \\textit{diverse settings}, and\n\\textit{unified protocols}. As the largest emotional intelligence benchmark for\nMLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific\nquestioning-answering (QA) pairs, spanning broad scenarios to formulate eight\nemotional tasks. It further incorporates a holistic evaluation suite with\nhybrid metrics for emotion recognition and reasoning, analyzed through a\nmulti-agent system framework. Through a rigorous evaluation of 20 advanced\nMLLMs, we uncover both their strengths and limitations, yielding several key\ninsights: \\ding{182} Current MLLMs exhibit unsatisfactory emotional\nintelligence, with the best-performing model achieving only $39.3\\%$\nrecognition score and $56.0\\%$ Chain-of-Thought (CoT) score on our benchmark.\n\\ding{183} Generalist models (\\emph{e.g.}, Gemini-2.5-Pro) derive emotional\nintelligence from generalized multimodal understanding capabilities, while\nspecialist models (\\emph{e.g.}, R1-Omni) can achieve comparable performance\nthrough domain-specific post-training adaptation. By introducing MME-Emotion,\nwe hope that it can serve as a foundation for advancing MLLMs' emotional\nintelligence in the future.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MME-Emotion\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u4e14\u7cfb\u7edf\u6027\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u60c5\u611f\u667a\u80fd\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5176\u60c5\u611f\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524dMLLMs\u5728\u6b64\u9886\u57df\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u8ba1\u7b97\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u6709\u7684\u60c5\u611f\u57fa\u51c6\u4ecd\u6709\u5c40\u9650\u6027\uff0c\u672a\u80fd\u5145\u5206\u8bc4\u4f30MLLMs\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u8bc6\u522b\u60c5\u611f\u89e6\u53d1\u56e0\u7d20\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u7814\u7a76\u8005\u6784\u5efa\u4e86MME-Emotion\u57fa\u51c6\uff0c\u5305\u542b\u8d85\u8fc76,000\u4e2a\u7cbe\u9009\u89c6\u9891\u7247\u6bb5\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u95ee\u7b54\uff08QA\uff09\u5bf9\uff0c\u6db5\u76d6\u516b\u9879\u60c5\u611f\u4efb\u52a1\uff0c\u65e8\u5728\u5168\u9762\u8bc4\u4f30MLLMs\u7684\u60c5\u611f\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002\u8be5\u57fa\u51c6\u5177\u6709\u53ef\u6269\u5c55\u6027\u3001\u591a\u6837\u5316\u8bbe\u7f6e\u548c\u7edf\u4e00\u534f\u8bae\uff0c\u5e76\u6574\u5408\u4e86\u91c7\u7528\u6df7\u5408\u6307\u6807\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6846\u67b6\u7684\u6574\u4f53\u8bc4\u4f30\u5957\u4ef6\u3002", "result": "\u901a\u8fc7\u5bf920\u4e2a\u5148\u8fdbMLLMs\u7684\u4e25\u683c\u8bc4\u4f30\u53d1\u73b0\uff1a1) \u5f53\u524dMLLMs\u7684\u60c5\u611f\u667a\u80fd\u8868\u73b0\u4e0d\u5c3d\u4eba\u610f\uff0c\u6700\u4f73\u6a21\u578b\u5728\u8bc6\u522b\u4efb\u52a1\u4e0a\u4ec5\u83b7\u5f9739.3%\u7684\u5f97\u5206\uff0c\u5728\u601d\u7ef4\u94fe\uff08CoT\uff09\u4efb\u52a1\u4e0a\u83b7\u5f9756.0%\u7684\u5f97\u5206\u30022) \u901a\u7528\u6a21\u578b\uff08\u5982Gemini-2.5-Pro\uff09\u7684\u60c5\u611f\u667a\u80fd\u6765\u6e90\u4e8e\u5176\u6cdb\u5316\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u800c\u4e13\u4e1a\u6a21\u578b\uff08\u5982R1-Omni\uff09\u53ef\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684\u540e\u671f\u8bad\u7ec3\u9002\u5e94\u8fbe\u5230\u53ef\u6bd4\u6027\u80fd\u3002", "conclusion": "MME-Emotion\u57fa\u51c6\u7684\u5f15\u5165\uff0c\u6709\u671b\u4e3a\u672a\u6765\u63d0\u5347MLLMs\u7684\u60c5\u611f\u667a\u80fd\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2508.09497", "pdf": "https://arxiv.org/pdf/2508.09497", "abs": "https://arxiv.org/abs/2508.09497", "authors": ["Siyuan Meng", "Junming Liu", "Yirong Chen", "Song Mao", "Pinlong Cai", "Guohang Yan", "Botian Shi", "Ding Wang"], "title": "From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 4 tables", "summary": "Retrieval-augmented generation (RAG) systems are often bottlenecked by their\nreranking modules, which typically score passages independently and select a\nfixed Top-K size. This approach struggles with complex multi-hop queries that\nrequire synthesizing evidence across multiple documents, creating a trade-off\nwhere small K values omit crucial information and large K values introduce\nnoise. To address this, we introduce the Dynamic Passage Selector (DPS), a\nnovel reranking framework that treats passage selection as a supervised\nlearning problem. Unlike traditional point-wise or list-wise methods, DPS is\nfine-tuned to capture inter-passage dependencies and dynamically select the\nmost relevant set of passages for generation. As a seamless plug-and-play\nmodule, DPS requires no modifications to the standard RAG pipeline.\nComprehensive evaluations on five benchmarks show that DPS consistently\noutperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the\nchallenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over\nstrong baselines like Qwen3-reranker and RankingGPT, respectively. Our results\ndemonstrate that by enabling adaptive evidence selection, DPS substantially\nenhances reasoning capabilities in complex RAG scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u52a8\u6001\u6bb5\u843d\u9009\u62e9\u5668\uff08DPS\uff09\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u91cd\u6392\u5e8f\u6846\u67b6\uff0c\u5c06\u6bb5\u843d\u9009\u62e9\u89c6\u4e3a\u76d1\u7763\u5b66\u4e60\u95ee\u9898\uff0c\u65e8\u5728\u52a8\u6001\u9009\u62e9\u6700\u76f8\u5173\u7684\u6bb5\u843d\u96c6\uff0c\u4ee5\u514b\u670d\u73b0\u6709RAG\u7cfb\u7edf\u56fa\u5b9aTop-K\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u8df3\u67e5\u8be2\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "RAG\u7cfb\u7edf\u5e38\u53d7\u9650\u4e8e\u91cd\u6392\u5e8f\u6a21\u5757\uff0c\u5176\u72ec\u7acb\u8bc4\u5206\u5e76\u9009\u62e9\u56fa\u5b9aTop-K\u6bb5\u843d\u7684\u65b9\u6cd5\uff0c\u96be\u4ee5\u5904\u7406\u9700\u8981\u7efc\u5408\u591a\u6587\u6863\u8bc1\u636e\u7684\u590d\u6742\u591a\u8df3\u67e5\u8be2\u3002\u56fa\u5b9aK\u503c\u4f1a\u9020\u6210\u53d6\u820d\uff1a\u5c0fK\u9057\u6f0f\u5173\u952e\u4fe1\u606f\uff0c\u5927K\u5f15\u5165\u566a\u97f3\u3002", "method": "\u5f15\u5165\u4e86\u52a8\u6001\u6bb5\u843d\u9009\u62e9\u5668\uff08DPS\uff09\uff0c\u5b83\u5c06\u6bb5\u843d\u9009\u62e9\u89c6\u4e3a\u76d1\u7763\u5b66\u4e60\u95ee\u9898\uff0c\u800c\u975e\u4f20\u7edf\u70b9\u5bf9\u70b9\u6216\u5217\u8868\u5f0f\u65b9\u6cd5\u3002DPS\u7ecf\u8fc7\u5fae\u8c03\u4ee5\u6355\u6349\u6bb5\u843d\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u52a8\u6001\u9009\u62e9\u6700\u76f8\u5173\u7684\u6bb5\u843d\u96c6\u7528\u4e8e\u751f\u6210\u3002\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u65e0\u9700\u4fee\u6539\u6807\u51c6RAG\u7ba1\u7ebf\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDPS\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u91cd\u6392\u5e8f\u5668\u548c\u5fae\u8c03\u65b9\u6cd5\u3002\u7279\u522b\u662f\u5728MuSiQue\u6570\u636e\u96c6\u4e0a\uff0cF1\u5206\u6570\u6bd4Qwen3-reranker\u63d0\u534730.06%\uff0c\u6bd4RankingGPT\u63d0\u534715.4%\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u73b0\u81ea\u9002\u5e94\u8bc1\u636e\u9009\u62e9\uff0cDPS\u663e\u8457\u589e\u5f3a\u4e86\u590d\u6742RAG\u573a\u666f\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2508.09184", "pdf": "https://arxiv.org/pdf/2508.09184", "abs": "https://arxiv.org/abs/2508.09184", "authors": ["Zineddine Bettouche", "Khalid Ali", "Andreas Fischer", "Andreas Kassler"], "title": "HiSTM: Hierarchical Spatiotemporal Mamba for Cellular Traffic Forecasting", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "Cellular traffic forecasting is essential for network planning, resource\nallocation, or load-balancing traffic across cells. However, accurate\nforecasting is difficult due to intricate spatial and temporal patterns that\nexist due to the mobility of users. Existing AI-based traffic forecasting\nmodels often trade-off accuracy and computational efficiency. We present\nHierarchical SpatioTemporal Mamba (HiSTM), which combines a dual spatial\nencoder with a Mamba-based temporal module and attention mechanism. HiSTM\nemploys selective state space methods to capture spatial and temporal patterns\nin network traffic. In our evaluation, we use a real-world dataset to compare\nHiSTM against several baselines, showing a 29.4% MAE improvement over the STN\nbaseline while using 94% fewer parameters. We show that the HiSTM generalizes\nwell across different datasets and improves in accuracy over longer\ntime-horizons.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHiSTM\u6a21\u578b\uff0c\u7ed3\u5408\u53cc\u7a7a\u95f4\u7f16\u7801\u5668\u3001Mamba\u65f6\u5e8f\u6a21\u5757\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u89e3\u51b3\u8702\u7a9d\u6d41\u91cf\u9884\u6d4b\u4e2d\u590d\u6742\u7684\u65f6\u7a7a\u6a21\u5f0f\u96be\u9898\uff0c\u5e76\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u8702\u7a9d\u6d41\u91cf\u9884\u6d4b\u5bf9\u4e8e\u7f51\u7edc\u89c4\u5212\u3001\u8d44\u6e90\u5206\u914d\u548c\u8d1f\u8f7d\u5747\u8861\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u7528\u6237\u79fb\u52a8\u5bfc\u81f4\u7684\u590d\u6742\u65f6\u7a7a\u6a21\u5f0f\uff0c\u51c6\u786e\u9884\u6d4b\u975e\u5e38\u56f0\u96be\u3002\u73b0\u6709\u57fa\u4e8eAI\u7684\u9884\u6d4b\u6a21\u578b\u5f80\u5f80\u96be\u4ee5\u517c\u987e\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u65f6\u7a7aMamba (HiSTM) \u6a21\u578b\uff0c\u5176\u6838\u5fc3\u7ec4\u4ef6\u5305\u62ec\uff1a\u53cc\u7a7a\u95f4\u7f16\u7801\u5668\u3001\u57fa\u4e8eMamba\u7684\u65f6\u5e8f\u6a21\u5757\u4ee5\u53ca\u6ce8\u610f\u529b\u673a\u5236\u3002HiSTM\u5229\u7528\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u65b9\u6cd5\u6765\u6355\u83b7\u7f51\u7edc\u6d41\u91cf\u4e2d\u7684\u65f6\u7a7a\u6a21\u5f0f\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u8bc4\u4f30\uff0cHiSTM\u76f8\u6bd4STN\u57fa\u7ebf\u6a21\u578b\u5b9e\u73b0\u4e8629.4%\u7684MAE\u6539\u8fdb\uff0c\u540c\u65f6\u53c2\u6570\u91cf\u51cf\u5c11\u4e8694%\u3002\u5b9e\u9a8c\u8868\u660eHiSTM\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u6cdb\u5316\u80fd\u529b\u826f\u597d\uff0c\u5e76\u4e14\u5728\u66f4\u957f\u7684\u65f6\u95f4\u8303\u56f4\u5185\u4e5f\u80fd\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "HiSTM\u6a21\u578b\u4e3a\u8702\u7a9d\u6d41\u91cf\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u7cbe\u5ea6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u6709\u6548\u5730\u6355\u6349\u4e86\u590d\u6742\u7684\u65f6\u7a7a\u6a21\u5f0f\uff0c\u5e76\u5728\u51cf\u5c11\u6a21\u578b\u53c2\u6570\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.09860", "pdf": "https://arxiv.org/pdf/2508.09860", "abs": "https://arxiv.org/abs/2508.09860", "authors": ["In-Chang Baek", "Seoyoung Lee", "Sung-Hyun Kim", "Geumhwan Hwang", "KyungJoong Kim"], "title": "Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation", "categories": ["cs.AI"], "comment": "9 pages, 6 tables, 3 figures", "summary": "Human-aligned AI is a critical component of co-creativity, as it enables\nmodels to accurately interpret human intent and generate controllable outputs\nthat align with design goals in collaborative content creation. This direction\nis especially relevant in procedural content generation via reinforcement\nlearning (PCGRL), which is intended to serve as a tool for human designers.\nHowever, existing systems often fall short of exhibiting human-centered\nbehavior, limiting the practical utility of AI-driven generation tools in\nreal-world design workflows. In this paper, we propose VIPCGRL\n(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that\nincorporates three modalities-text, level, and sketches-to extend control\nmodality and enhance human-likeness. We introduce a shared embedding space\ntrained via quadruple contrastive learning across modalities and human-AI\nstyles, and align the policy using an auxiliary reward based on embedding\nsimilarity. Experimental results show that VIPCGRL outperforms existing\nbaselines in human-likeness, as validated by both quantitative metrics and\nhuman evaluations. The code and dataset will be available upon publication.", "AI": {"tldr": "\u63d0\u51faVIPCGRL\uff0c\u4e00\u4e2a\u7ed3\u5408\u591a\u6a21\u6001\uff08\u6587\u672c\u3001\u5173\u5361\u3001\u8349\u56fe\uff09\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u91cd\u5bf9\u6bd4\u5b66\u4e60\u548c\u8f85\u52a9\u5956\u52b1\u63d0\u5347PCGRL\u5728\u5171\u521b\u4e2d\u7684\u7c7b\u4eba\u6027\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7a0b\u5e8f\u5185\u5bb9\u751f\u6210\uff08PCGRL\uff09\u7cfb\u7edf\u672a\u80fd\u5c55\u73b0\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u884c\u4e3a\uff0c\u9650\u5236\u4e86AI\u9a71\u52a8\u751f\u6210\u5de5\u5177\u5728\u5b9e\u9645\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u4e2d\u7684\u5b9e\u7528\u6027\u3002\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684AI\u5bf9\u4e8e\u5171\u521b\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u51c6\u786e\u89e3\u91ca\u4eba\u7c7b\u610f\u56fe\u5e76\u751f\u6210\u7b26\u5408\u8bbe\u8ba1\u76ee\u6807\u7684\u53ef\u63a7\u8f93\u51fa\u3002", "method": "\u63d0\u51faVIPCGRL\uff08Vision-Instruction PCGRL\uff09\uff0c\u4e00\u4e2a\u65b0\u578b\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u6587\u672c\u3001\u5173\u5361\u548c\u8349\u56fe\u4e09\u79cd\u6a21\u6001\uff0c\u4ee5\u6269\u5c55\u63a7\u5236\u6a21\u6001\u5e76\u589e\u5f3a\u7c7b\u4eba\u6027\u3002\u5f15\u5165\u901a\u8fc7\u8de8\u6a21\u6001\u548c\u4eba\u673a\u98ce\u683c\u7684\u56db\u91cd\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u7684\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u5d4c\u5165\u76f8\u4f3c\u5ea6\u7684\u8f85\u52a9\u5956\u52b1\u5bf9\u7b56\u7565\u8fdb\u884c\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVIPCGRL\u5728\u7c7b\u4eba\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u901a\u8fc7\u5b9a\u91cf\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "VIPCGRL\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u5bf9\u6bd4\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86PCGRL\u7684\u7c7b\u4eba\u6027\u548c\u53ef\u63a7\u6027\uff0c\u4f7f\u5176\u5728\u534f\u540c\u5185\u5bb9\u521b\u4f5c\u4e2d\u4f5c\u4e3a\u4eba\u7c7b\u5de5\u5177\u7684\u6f5c\u529b\u66f4\u5927\u3002"}}
{"id": "2508.09160", "pdf": "https://arxiv.org/pdf/2508.09160", "abs": "https://arxiv.org/abs/2508.09160", "authors": ["Beyza Cinar", "Maria Maleshkova"], "title": "Presenting DiaData for Research on Type 1 Diabetes", "categories": ["cs.LG", "cs.DB", "q-bio.QM"], "comment": "11 pages, 7 figures, 3 tables", "summary": "Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction\nof insulin-producing cells, resulting in insulin deficiency, as to why the\naffected individuals depend on external insulin injections. However, insulin\ncan decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a\nsevere event of low blood glucose levels ($\\le$70 mg/dL) with dangerous side\neffects of dizziness, coma, or death. Data analysis can significantly enhance\ndiabetes care by identifying personal patterns and trends leading to adverse\nevents. Especially, machine learning (ML) models can predict glucose levels and\nprovide early alarms. However, diabetes and hypoglycemia research is limited by\nthe unavailability of large datasets. Thus, this work systematically integrates\n15 datasets to provide a large database of 2510 subjects with glucose\nmeasurements recorded every 5 minutes. In total, 149 million measurements are\nincluded, of which 4% represent values in the hypoglycemic range. Moreover, two\nsub-databases are extracted. Sub-database I includes demographics, and\nsub-database II includes heart rate data. The integrated dataset provides an\nequal distribution of sex and different age levels. As a further contribution,\ndata quality is assessed, revealing that data imbalance and missing values\npresent a significant challenge. Moreover, a correlation study on glucose\nlevels and heart rate data is conducted, showing a relation between 15 and 55\nminutes before hypoglycemia.", "AI": {"tldr": "\u672c\u7814\u7a76\u6574\u5408\u4e8615\u4e2a\u6570\u636e\u96c6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u578bT1D\u548c\u4f4e\u8840\u7cd6\u6570\u636e\u5e93\uff0c\u65e8\u5728\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u5206\u6790\u4e86\u6570\u636e\u8d28\u91cf\u53ca\u8461\u8404\u7cd6\u4e0e\u5fc3\u7387\u7684\u5173\u7cfb\u3002", "motivation": "1\u578b\u7cd6\u5c3f\u75c5(T1D)\u60a3\u8005\u4f9d\u8d56\u5916\u90e8\u80f0\u5c9b\u7d20\uff0c\u4f46\u80f0\u5c9b\u7d20\u53ef\u80fd\u5bfc\u81f4\u5371\u9669\u7684\u4f4e\u8840\u7cd6\u3002\u867d\u7136\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u9884\u6d4b\u8840\u7cd6\u5e76\u63d0\u4f9b\u65e9\u671f\u9884\u8b66\uff0c\u4f46\u7cd6\u5c3f\u75c5\u548c\u4f4e\u8840\u7cd6\u7814\u7a76\u53d7\u9650\u4e8e\u5927\u578b\u6570\u636e\u96c6\u7684\u7f3a\u4e4f\u3002", "method": "\u7cfb\u7edf\u6027\u6574\u5408\u4e8615\u4e2a\u73b0\u6709\u6570\u636e\u96c6\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u5927\u578b\u6570\u636e\u5e93\uff0c\u5305\u542b2510\u540d\u53d7\u8bd5\u8005\u548c\u6bcf5\u5206\u949f\u8bb0\u5f55\u7684\u8840\u7cd6\u6d4b\u91cf\u503c\u3002\u63d0\u53d6\u4e86\u4e24\u4e2a\u5b50\u6570\u636e\u5e93\uff08\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\u548c\u5fc3\u7387\u6570\u636e\uff09\u3002\u8bc4\u4f30\u4e86\u6570\u636e\u8d28\u91cf\uff0c\u5e76\u8fdb\u884c\u4e86\u8840\u7cd6\u6c34\u5e73\u4e0e\u5fc3\u7387\u6570\u636e\u7684\u76f8\u5173\u6027\u7814\u7a76\u3002", "result": "\u6210\u529f\u6574\u5408\u5e76\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1.49\u4ebf\u6b21\u6d4b\u91cf\u503c\uff08\u5176\u4e2d4%\u4e3a\u4f4e\u8840\u7cd6\u8303\u56f4\uff09\u7684\u6570\u636e\u5e93\u3002\u8be5\u6570\u636e\u96c6\u5728\u6027\u522b\u548c\u4e0d\u540c\u5e74\u9f84\u6c34\u5e73\u4e0a\u5206\u5e03\u5747\u8861\u3002\u7814\u7a76\u63ed\u793a\u4e86\u6570\u636e\u4e0d\u5e73\u8861\u548c\u7f3a\u5931\u503c\u662f\u663e\u8457\u6311\u6218\u3002\u53d1\u73b0\u4f4e\u8840\u7cd6\u53d1\u751f\u524d15\u81f355\u5206\u949f\u5185\uff0c\u8840\u7cd6\u6c34\u5e73\u4e0e\u5fc3\u7387\u6570\u636e\u4e4b\u95f4\u5b58\u5728\u5173\u8054\u3002", "conclusion": "\u6240\u6574\u5408\u7684\u5927\u578b\u6570\u636e\u5e93\u4e3a\u7cd6\u5c3f\u75c5\u548c\u4f4e\u8840\u7cd6\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u514b\u670d\u73b0\u6709\u6570\u636e\u9650\u5236\u3002\u5c3d\u7ba1\u6570\u636e\u8d28\u91cf\u5b58\u5728\u6311\u6218\uff0c\u4f46\u901a\u8fc7\u6574\u5408\u548c\u5206\u6790\uff0c\u63ed\u793a\u4e86\u6f5c\u5728\u7684\u9884\u6d4b\u6807\u8bb0\uff08\u5982\u5fc3\u7387\uff09\uff0c\u53ef\u4e3a\u672a\u6765\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2508.09218", "pdf": "https://arxiv.org/pdf/2508.09218", "abs": "https://arxiv.org/abs/2508.09218", "authors": ["Zuoou Li", "Weitong Zhang", "Jingyuan Wang", "Shuyuan Zhang", "Wenjia Bai", "Bernhard Kainz", "Mengyun Qiao"], "title": "Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) are widely used in vision-language\nreasoning tasks. However, their vulnerability to adversarial prompts remains a\nserious concern, as safety mechanisms often fail to prevent the generation of\nharmful outputs. Although recent jailbreak strategies report high success\nrates, many responses classified as \"successful\" are actually benign, vague, or\nunrelated to the intended malicious goal. This mismatch suggests that current\nevaluation standards may overestimate the effectiveness of such attacks. To\naddress this issue, we introduce a four-axis evaluation framework that\nconsiders input on-topicness, input out-of-distribution (OOD) intensity, output\nharmfulness, and output refusal rate. This framework identifies truly effective\njailbreaks. In a substantial empirical study, we reveal a structural trade-off:\nhighly on-topic prompts are frequently blocked by safety filters, whereas those\nthat are too OOD often evade detection but fail to produce harmful content.\nHowever, prompts that balance relevance and novelty are more likely to evade\nfilters and trigger dangerous output. Building on this insight, we develop a\nrecursive rewriting strategy called Balanced Structural Decomposition (BSD).\nThe approach restructures malicious prompts into semantically aligned\nsub-tasks, while introducing subtle OOD signals and visual cues that make the\ninputs harder to detect. BSD was tested across 13 commercial and open-source\nMLLMs, where it consistently led to higher attack success rates, more harmful\noutputs, and fewer refusals. Compared to previous methods, it improves success\nrates by $67\\%$ and harmfulness by $21\\%$, revealing a previously\nunderappreciated weakness in current multimodal safety systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5bf9\u6297\u6027\u63d0\u793a\u7684\u6f0f\u6d1e\u53ca\u5176\u73b0\u6709\u8bc4\u4f30\u6807\u51c6\u7684\u4e0d\u8db3\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u56db\u8f74\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u57fa\u4e8e\u5bf9\u63d0\u793a\u7279\u6027\u4e0e\u68c0\u6d4b\u4e4b\u95f4\u6743\u8861\u7684\u6d1e\u5bdf\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5e73\u8861\u7ed3\u6784\u5206\u89e3\uff08BSD\uff09\u201d\u7684\u9012\u5f52\u91cd\u5199\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u653b\u51fb\u6210\u529f\u7387\u548c\u6709\u5bb3\u8f93\u51fa\u7684\u751f\u6210\uff0c\u63ed\u793a\u4e86\u5f53\u524dMLLM\u5b89\u5168\u7cfb\u7edf\u7684\u8584\u5f31\u70b9\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u5bf9\u5bf9\u6297\u6027\u63d0\u793a\u7684\u8106\u5f31\u6027\u662f\u4e00\u4e2a\u4e25\u91cd\u95ee\u9898\uff0c\u5b89\u5168\u673a\u5236\u5e38\u65e0\u6cd5\u963b\u6b62\u6709\u5bb3\u5185\u5bb9\u7684\u751f\u6210\u3002\u6b64\u5916\uff0c\u73b0\u6709\u8d8a\u72f1\u7b56\u7565\u867d\u7136\u62a5\u544a\u6210\u529f\u7387\u9ad8\uff0c\u4f46\u8bb8\u591a\u201c\u6210\u529f\u201d\u54cd\u5e94\u5b9e\u9645\u4e0a\u662f\u826f\u6027\u3001\u6a21\u7cca\u6216\u4e0e\u9884\u671f\u6076\u610f\u76ee\u6807\u65e0\u5173\u7684\uff0c\u8fd9\u8868\u660e\u5f53\u524d\u8bc4\u4f30\u6807\u51c6\u53ef\u80fd\u9ad8\u4f30\u4e86\u653b\u51fb\u7684\u6709\u6548\u6027\u3002", "method": "1. \u5f15\u5165\u4e00\u4e2a\u56db\u8f74\u8bc4\u4f30\u6846\u67b6\uff0c\u8003\u91cf\u8f93\u5165\u4e3b\u9898\u76f8\u5173\u6027\u3001\u8f93\u5165\u57df\u5916\uff08OOD\uff09\u5f3a\u5ea6\u3001\u8f93\u51fa\u6709\u5bb3\u6027\u4ee5\u53ca\u8f93\u51fa\u62d2\u7edd\u7387\uff0c\u4ee5\u8bc6\u522b\u771f\u6b63\u6709\u6548\u7684\u8d8a\u72f1\u30022. \u57fa\u4e8e\u7814\u7a76\u53d1\u73b0\uff08\u5373\u5e73\u8861\u76f8\u5173\u6027\u548c\u65b0\u9896\u6027\u7684\u63d0\u793a\u66f4\u80fd\u89c4\u907f\u8fc7\u6ee4\u5668\u5e76\u89e6\u53d1\u5371\u9669\u8f93\u51fa\uff09\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5e73\u8861\u7ed3\u6784\u5206\u89e3\uff08BSD\uff09\u201d\u7684\u9012\u5f52\u91cd\u5199\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u5c06\u6076\u610f\u63d0\u793a\u91cd\u6784\u4e3a\u8bed\u4e49\u5bf9\u9f50\u7684\u5b50\u4efb\u52a1\uff0c\u540c\u65f6\u5f15\u5165\u5fae\u5999\u7684OOD\u4fe1\u53f7\u548c\u89c6\u89c9\u7ebf\u7d22\uff0c\u4f7f\u8f93\u5165\u66f4\u96be\u88ab\u68c0\u6d4b\u3002", "result": "1. \u7ecf\u9a8c\u7814\u7a76\u63ed\u793a\u4e86\u7ed3\u6784\u6027\u6743\u8861\uff1a\u9ad8\u5ea6\u76f8\u5173\u63d0\u793a\u5e38\u88ab\u5b89\u5168\u8fc7\u6ee4\u5668\u963b\u6b62\uff0c\u800c\u8fc7\u4e8e\u57df\u5916\uff08OOD\uff09\u7684\u63d0\u793a\u867d\u80fd\u89c4\u907f\u68c0\u6d4b\u4f46\u672a\u80fd\u4ea7\u751f\u6709\u5bb3\u5185\u5bb9\uff1b\u7136\u800c\uff0c\u5e73\u8861\u4e86\u76f8\u5173\u6027\u548c\u65b0\u9896\u6027\u7684\u63d0\u793a\u66f4\u6709\u53ef\u80fd\u89c4\u907f\u8fc7\u6ee4\u5668\u5e76\u89e6\u53d1\u5371\u9669\u8f93\u51fa\u30022. BSD\u7b56\u7565\u572813\u79cd\u5546\u4e1a\u548c\u5f00\u6e90MLLMs\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u7ed3\u679c\u663e\u793a\u5176\u6301\u7eed\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff0c\u4ea7\u751f\u66f4\u591a\u6709\u5bb3\u8f93\u51fa\uff0c\u5e76\u51cf\u5c11\u62d2\u7edd\u30023. \u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cBSD\u5c06\u6210\u529f\u7387\u63d0\u9ad8\u4e8667%\uff0c\u6709\u5bb3\u6027\u63d0\u9ad8\u4e8621%\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u5b89\u5168\u7cfb\u7edf\u4e00\u4e2a\u6b64\u524d\u672a\u88ab\u5145\u5206\u8ba4\u8bc6\u7684\u5f31\u70b9\u3002BSD\u7b56\u7565\u901a\u8fc7\u5e73\u8861\u63d0\u793a\u7684\u76f8\u5173\u6027\u548c\u65b0\u9896\u6027\uff0c\u80fd\u591f\u6709\u6548\u89c4\u907f\u73b0\u6709MLLMs\u7684\u5b89\u5168\u673a\u5236\uff0c\u4ece\u800c\u63d0\u9ad8\u653b\u51fb\u6210\u529f\u7387\u5e76\u751f\u6210\u66f4\u591a\u6709\u5bb3\u5185\u5bb9\uff0c\u8bc1\u660e\u4e86\u5f53\u524d\u5b89\u5168\u7cfb\u7edf\u5728\u5e94\u5bf9\u590d\u6742\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.09515", "pdf": "https://arxiv.org/pdf/2508.09515", "abs": "https://arxiv.org/abs/2508.09515", "authors": ["Jakub \u0160m\u00edd", "Pavel P\u0159ib\u00e1\u0148", "Pavel Kr\u00e1l"], "title": "LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation", "categories": ["cs.CL"], "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics; Volume 1: Long Papers (ACL 2025).\n  Official version: https://aclanthology.org/2025.acl-long.41/", "summary": "Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed\nsentiment analysis in a target language by transferring knowledge from a source\nlanguage with available annotated data. Most existing methods depend heavily on\noften unreliable translation tools to bridge the language gap. In this paper,\nwe propose a new approach that leverages a large language model (LLM) to\ngenerate high-quality pseudo-labelled data in the target language without the\nneed for translation tools. First, the framework trains an ABSA model to obtain\npredictions for unlabelled target language data. Next, LLM is prompted to\ngenerate natural sentences that better represent these noisy predictions than\nthe original text. The ABSA model is then further fine-tuned on the resulting\npseudo-labelled dataset. We demonstrate the effectiveness of this method across\nsix languages and five backbone models, surpassing previous state-of-the-art\ntranslation-based approaches. The proposed framework also supports generative\nmodels, and we show that fine-tuned LLMs outperform smaller multilingual\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u8de8\u8bed\u8a00\u60c5\u611f\u5206\u6790(ABSA)\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u6570\u636e\uff0c\u4ee5\u907f\u514d\u4f9d\u8d56\u4e0d\u53ef\u9760\u7684\u7ffb\u8bd1\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u8de8\u8bed\u8a00ABSA\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u4e0d\u53ef\u9760\u7684\u7ffb\u8bd1\u5de5\u5177\u6765\u5f25\u5408\u8bed\u8a00\u969c\u788d\uff0c\u5f71\u54cd\u5206\u6790\u51c6\u786e\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u8bad\u7ec3\u4e00\u4e2aABSA\u6a21\u578b\u5bf9\u76ee\u6807\u8bed\u8a00\u7684\u672a\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u9884\u6d4b\uff1b\u7136\u540e\uff0c\u5229\u7528LLM\u5c06\u8fd9\u4e9b\u9884\u6d4b\u8f6c\u6362\u4e3a\u66f4\u81ea\u7136\u7684\u4f2a\u6807\u7b7e\u53e5\u5b50\uff1b\u6700\u540e\uff0c\u4f7f\u7528\u8fd9\u4e9b\u4f2a\u6807\u7b7e\u6570\u636e\u96c6\u5bf9ABSA\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002\u8be5\u6846\u67b6\u4e5f\u652f\u6301\u751f\u6210\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u5fae\u8c03LLM\u7684\u4f18\u8d8a\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u516d\u79cd\u8bed\u8a00\u548c\u4e94\u79cd\u9aa8\u5e72\u6a21\u578b\u4e0a\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u7ffb\u8bd1\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5fae\u8c03\u540e\u7684LLM\u6027\u80fd\u4f18\u4e8e\u8f83\u5c0f\u7684\u591a\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eLLM\u7684\u4f2a\u6807\u7b7e\u751f\u6210\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u8de8\u8bed\u8a00ABSA\u95ee\u9898\uff0c\u65e0\u9700\u7ffb\u8bd1\u5de5\u5177\uff0c\u5e76\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002"}}
{"id": "2508.09197", "pdf": "https://arxiv.org/pdf/2508.09197", "abs": "https://arxiv.org/abs/2508.09197", "authors": ["Ilias Chatzistefanidis", "Andrea Leone", "Ali Yaghoubian", "Mikel Irazabal", "Sehad Nassim", "Lina Bariah", "Merouane Debbah", "Navid Nikaein"], "title": "MX-AI: Agentic Observability and Control Platform for Open and AI-RAN", "categories": ["cs.NI", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Future 6G radio access networks (RANs) will be artificial intelligence\n(AI)-native: observed, reasoned about, and re-configured by autonomous agents\ncooperating across the cloud-edge continuum. We introduce MX-AI, the first\nend-to-end agentic system that (i) instruments a live 5G Open RAN testbed based\non OpenAirInterface (OAI) and FlexRIC, (ii) deploys a graph of\nLarge-Language-Model (LLM)-powered agents inside the Service Management and\nOrchestration (SMO) layer, and (iii) exposes both observability and control\nfunctions for 6G RAN resources through natural-language intents. On 50\nrealistic operational queries, MX-AI attains a mean answer quality of 4.1/5.0\nand 100 % decision-action accuracy, while incurring only 8.8 seconds end-to-end\nlatency when backed by GPT-4.1. Thus, it matches human-expert performance,\nvalidating its practicality in real settings. We publicly release the agent\ngraph, prompts, and evaluation harness to accelerate open research on AI-native\nRANs. A live demo is presented here:\nhttps://www.youtube.com/watch?v=CEIya7988Ug&t=285s&ab_channel=BubbleRAN", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MX-AI\uff0c\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u610f\u56fe\u7ba1\u7406\u548c\u63a7\u52366G\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5ab2\u7f8e\u4eba\u7c7b\u4e13\u5bb6\u7684\u6027\u80fd\u3002", "motivation": "\u672a\u6765\u76846G\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff08RAN\uff09\u5c06\u662fAI\u539f\u751f\u7684\uff0c\u9700\u8981\u80fd\u591f\u81ea\u4e3b\u89c2\u5bdf\u3001\u63a8\u7406\u548c\u91cd\u65b0\u914d\u7f6e\u7684\u667a\u80fd\u4ee3\u7406\u3002\u76ee\u524d\u7f3a\u4e4f\u4e00\u4e2a\u80fd\u5b9e\u73b0\u7aef\u5230\u7aef\u81ea\u4e3b\u7ba1\u7406\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u7684\u7cfb\u7edf\u3002", "method": "\u5f15\u5165MX-AI\u7cfb\u7edf\uff0c\u5176\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5728\u57fa\u4e8eOpenAirInterface (OAI) \u548c FlexRIC \u76845G Open RAN\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u90e8\u7f72\uff1b2) \u5728\u670d\u52a1\u7ba1\u7406\u548c\u7f16\u6392 (SMO) \u5c42\u5185\u90e8\u7f72\u4e00\u4e2a\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u9a71\u52a8\u7684\u4ee3\u7406\u56fe\uff1b3) \u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u610f\u56fe\u63d0\u4f9b6G RAN\u8d44\u6e90\u7684\u89c2\u6d4b\u548c\u63a7\u5236\u529f\u80fd\u3002", "result": "\u572850\u4e2a\u771f\u5b9e\u64cd\u4f5c\u67e5\u8be2\u4e0a\uff0cMX-AI\u7684\u5e73\u5747\u56de\u7b54\u8d28\u91cf\u8fbe\u52304.1/5.0\uff0c\u51b3\u7b56-\u884c\u52a8\u51c6\u786e\u7387\u8fbe\u5230100%\uff0c\u5e76\u4e14\u5728GPT-4.1\u652f\u6301\u4e0b\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u4ec5\u4e3a8.8\u79d2\u3002", "conclusion": "MX-AI\u7684\u6027\u80fd\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u76f8\u5f53\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u4e3aAI\u539f\u751fRAN\u7684\u5f00\u653e\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2508.09889", "pdf": "https://arxiv.org/pdf/2508.09889", "abs": "https://arxiv.org/abs/2508.09889", "authors": ["Zhitian Xie", "Qintong Wu", "Chengyue Yu", "Chenyi Zhuang", "Jinjie Gu"], "title": "AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving", "categories": ["cs.AI"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, as agents increasingly depend on multiple tools,\nthey encounter new challenges: extended contexts from disparate sources and\nnoisy or irrelevant tool outputs can undermine system reliability and accuracy.\nThese challenges underscore the necessity for enhanced stability in agent-based\nsystems. To address this, we introduce dynamic supervision and maneuvering\nmechanisms, constructing a robust and dynamic Multi-Agent System (MAS)\narchitecture within the AWorld framework. In our approach, the Execution Agent\ninvokes the Guard Agent at critical steps to verify and correct the reasoning\nprocess, effectively reducing errors arising from noise and bolstering\nproblem-solving robustness. Extensive experiments on the GAIA test dataset\nreveal that our dynamic maneuvering mechanism significantly improves both the\neffectiveness and stability of solutions, outperforming single-agent system\n(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system\nachieved first place among open-source projects on the prestigious GAIA\nleaderboard. These findings highlight the practical value of collaborative\nagent roles in developing more reliable and trustworthy intelligent systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u201c\u5b88\u62a4\u4ee3\u7406\u201d\u5728\u5173\u952e\u6b65\u9aa4\u9a8c\u8bc1\u5e76\u7ea0\u6b63\u63a8\u7406\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5728\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u65f6\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\uff0c\u5e76\u5728GAIA\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u5728\u5229\u7528\u591a\u79cd\u5916\u90e8\u5de5\u5177\u89e3\u51b3\u590d\u6742\u95ee\u9898\u65f6\uff0c\u9762\u4e34\u4e0a\u4e0b\u6587\u8fc7\u957f\u3001\u5de5\u5177\u8f93\u51fa\u566a\u58f0\u6216\u4e0d\u76f8\u5173\u5bfc\u81f4\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u4e0b\u964d\u7684\u6311\u6218\uff0c\u56e0\u6b64\u4e9f\u9700\u63d0\u5347\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u7814\u7a76\u8005\u5728AWorld\u6846\u67b6\u5185\u6784\u5efa\u4e86\u4e00\u4e2a\u9c81\u68d2\u4e14\u52a8\u6001\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u67b6\u6784\uff0c\u5f15\u5165\u4e86\u52a8\u6001\u76d1\u7763\u548c\u64cd\u7eb5\u673a\u5236\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6267\u884c\u4ee3\u7406\uff08Execution Agent\uff09\u5728\u5173\u952e\u6b65\u9aa4\u8c03\u7528\u5b88\u62a4\u4ee3\u7406\uff08Guard Agent\uff09\u6765\u9a8c\u8bc1\u548c\u7ea0\u6b63\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ee5\u51cf\u5c11\u566a\u58f0\u5f15\u8d77\u7684\u9519\u8bef\u5e76\u589e\u5f3a\u95ee\u9898\u89e3\u51b3\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728GAIA\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u52a8\u6001\u64cd\u7eb5\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u6709\u6548\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u8868\u73b0\u4f18\u4e8e\u5355\u4ee3\u7406\u7cfb\u7edf\uff08SAS\uff09\u548c\u6807\u51c6\u5de5\u5177\u589e\u5f3a\u7cfb\u7edf\u3002\u8be5\u52a8\u6001MAS\u7cfb\u7edf\u5728GAIA\u6392\u884c\u699c\u7684\u5f00\u6e90\u9879\u76ee\u4e2d\u4f4d\u5c45\u7b2c\u4e00\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u534f\u4f5c\u4ee3\u7406\u89d2\u8272\uff08\u5982\u5b88\u62a4\u4ee3\u7406\uff09\u5728\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u66f4\u503c\u5f97\u4fe1\u8d56\u7684\u667a\u80fd\u7cfb\u7edf\u65b9\u9762\u7684\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2508.09161", "pdf": "https://arxiv.org/pdf/2508.09161", "abs": "https://arxiv.org/abs/2508.09161", "authors": ["Muhammad Umair Danish", "Kashif Ali", "Kamran Siddiqui", "Katarina Grolinger"], "title": "Physics-Guided Memory Network for Building Energy Modeling", "categories": ["cs.LG", "cs.AI"], "comment": "Published version. 12 pages, 6 figures. Open access under CC BY-NC-ND\n  4.0 license. Publisher: Elsevier. Journal: Energy and AI", "summary": "Accurate energy consumption forecasting is essential for efficient resource\nmanagement and sustainability in the building sector. Deep learning models are\nhighly successful but struggle with limited historical data and become unusable\nwhen historical data are unavailable, such as in newly constructed buildings.\nOn the other hand, physics-based models, such as EnergyPlus, simulate energy\nconsumption without relying on historical data but require extensive building\nparameter specifications and considerable time to model a building. This paper\nintroduces a Physics-Guided Memory Network (PgMN), a neural network that\nintegrates predictions from deep learning and physics-based models to address\ntheir limitations. PgMN comprises a Parallel Projection Layers to process\nincomplete inputs, a Memory Unit to account for persistent biases, and a Memory\nExperience Module to optimally extend forecasts beyond their input range and\nproduce output. Theoretical evaluation shows that components of PgMN are\nmathematically valid for performing their respective tasks. The PgMN was\nevaluated on short-term energy forecasting at an hourly resolution, critical\nfor operational decision-making in smart grid and smart building systems.\nExperimental validation shows accuracy and applicability of PgMN in diverse\nscenarios such as newly constructed buildings, missing data, sparse historical\ndata, and dynamic infrastructure changes. This paper provides a promising\nsolution for energy consumption forecasting in dynamic building environments,\nenhancing model applicability in scenarios where historical data are limited or\nunavailable or when physics-based models are inadequate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7269\u7406\u5f15\u5bfc\u8bb0\u5fc6\u7f51\u7edc\uff08PgMN\uff09\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u578b\uff0c\u89e3\u51b3\u5efa\u7b51\u80fd\u8017\u9884\u6d4b\u4e2d\u5386\u53f2\u6570\u636e\u7a00\u7f3a\u6216\u7f3a\u5931\u7684\u6311\u6218\uff0c\u63d0\u5347\u6a21\u578b\u5728\u591a\u79cd\u590d\u6742\u573a\u666f\u4e0b\u7684\u51c6\u786e\u6027\u548c\u9002\u7528\u6027\u3002", "motivation": "\u5efa\u7b51\u80fd\u8017\u9884\u6d4b\u5bf9\u8d44\u6e90\u7ba1\u7406\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u81f3\u5173\u91cd\u8981\u3002\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6570\u636e\u6709\u9650\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u578b\u867d\u7136\u4e0d\u4f9d\u8d56\u5386\u53f2\u6570\u636e\uff0c\u4f46\u5efa\u6a21\u8017\u65f6\u4e14\u53c2\u6570\u8bbe\u7f6e\u590d\u6742\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3001\u514b\u670d\u5404\u81ea\u5c40\u9650\u6027\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u7269\u7406\u5f15\u5bfc\u8bb0\u5fc6\u7f51\u7edc\uff08PgMN\uff09\uff0c\u878d\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u57fa\u4e8e\u7269\u7406\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u3002PgMN\u5305\u542b\u5e73\u884c\u6295\u5f71\u5c42\uff08\u5904\u7406\u4e0d\u5b8c\u6574\u8f93\u5165\uff09\u3001\u8bb0\u5fc6\u5355\u5143\uff08\u5904\u7406\u6301\u7eed\u504f\u5dee\uff09\u548c\u8bb0\u5fc6\u7ecf\u9a8c\u6a21\u5757\uff08\u6269\u5c55\u9884\u6d4b\u8303\u56f4\uff09\u3002\u6a21\u578b\u5728\u77ed\u671f\uff08\u5c0f\u65f6\u5206\u8fa8\u7387\uff09\u80fd\u8017\u9884\u6d4b\u4e0a\u8fdb\u884c\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u7406\u8bba\u8bc4\u4f30\u8bc1\u660ePgMN\u5404\u7ec4\u4ef6\u7684\u6570\u5b66\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cPgMN\u5728\u65b0\u5efa\u5efa\u7b51\u3001\u6570\u636e\u7f3a\u5931\u3001\u5386\u53f2\u6570\u636e\u7a00\u758f\u548c\u52a8\u6001\u57fa\u7840\u8bbe\u65bd\u53d8\u5316\u7b49\u591a\u79cd\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "PgMN\u4e3a\u52a8\u6001\u5efa\u7b51\u73af\u5883\u4e0b\u7684\u80fd\u8017\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u5386\u53f2\u6570\u636e\u53d7\u9650\u6216\u7f3a\u5931\u4ee5\u53ca\u57fa\u4e8e\u7269\u7406\u6a21\u578b\u4e0d\u8db3\u573a\u666f\u4e0b\u7684\u5e94\u7528\u80fd\u529b\u3002"}}
{"id": "2508.09220", "pdf": "https://arxiv.org/pdf/2508.09220", "abs": "https://arxiv.org/abs/2508.09220", "authors": ["Haoyang Li", "Jiaqing Li", "Jialun Cao", "Zongyuan Yang", "Yongping Xiong"], "title": "Towards Scalable Training for Handwritten Mathematical Expression Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large foundation models have achieved significant performance gains through\nscalable training on massive datasets. However, the field of\n\\textbf{H}andwritten \\textbf{M}athematical \\textbf{E}xpression\n\\textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily\ndue to the arduous and costly process of manual annotation. To bridge this gap,\nwe propose a novel method integrating limited handwritten formulas with\nlarge-scale LaTeX-rendered formulas by developing a scalable data engine to\ngenerate complex and consistent LaTeX sequences. With this engine, we built the\nlargest formula dataset to date, termed \\texttt{Tex80M}, comprising over 80\nmillion high-quality training instances. Then we propose \\texttt{TexTeller},\nthe first HMER model trained at scale, by mix-training \\texttt{Tex80M} with a\nrelatively small HME dataset. The expansive training dataset and our refined\npipeline have equipped \\texttt{TexTeller} with state-of-the-art (SOTA)\nperformance across nearly all benchmarks. To advance the field, we will openly\nrelease our complete model, entire dataset, and full codebase, enabling further\nresearch building upon our contributions.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u624b\u5199\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\uff08HMER\uff09\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u5f15\u64ce\u4ee5\u751f\u6210\u5927\u89c4\u6a21\u5408\u6210\u516c\u5f0f\u6570\u636e\u96c6Tex80M\uff0c\u5e76\u8bad\u7ec3\u4e86\u9996\u4e2a\u5927\u89c4\u6a21HMER\u6a21\u578bTexTeller\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u57fa\u7840\u6a21\u578b\u53d7\u76ca\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u8bad\u7ec3\uff0c\u800c\u624b\u5199\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\uff08HMER\uff09\u9886\u57df\u56e0\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u5bfc\u81f4\u6570\u636e\u7a00\u7f3a\uff0c\u963b\u788d\u4e86\u5176\u53d1\u5c55\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6570\u636e\u5f15\u64ce\uff0c\u7528\u4e8e\u751f\u6210\u590d\u6742\u4e14\u4e00\u81f4\u7684LaTeX\u5e8f\u5217\uff0c\u5e76\u5c06\u6709\u9650\u7684\u624b\u5199\u516c\u5f0f\u4e0e\u5927\u89c4\u6a21LaTeX\u6e32\u67d3\u516c\u5f0f\u76f8\u7ed3\u5408\u3002\u6784\u5efa\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u516c\u5f0f\u6570\u636e\u96c6Tex80M\uff08\u8d85\u8fc78000\u4e07\u4e2a\u5b9e\u4f8b\uff09\u3002\u63d0\u51faTexTeller\u6a21\u578b\uff0c\u901a\u8fc7\u5c06Tex80M\u4e0e\u5c11\u91cf\u771f\u5b9e\u624b\u5199\u6570\u5b66\u8868\u8fbe\u5f0f\u6570\u636e\u6df7\u5408\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u8bad\u7ec3\u3002", "result": "TexTeller\u5728\u51e0\u4e4e\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u6027\u80fd\u3002\u6210\u529f\u6784\u5efa\u4e86\u5305\u542b\u8d85\u8fc78000\u4e07\u4e2a\u9ad8\u8d28\u91cf\u8bad\u7ec3\u5b9e\u4f8b\u7684\u8fc4\u4eca\u6700\u5927\u516c\u5f0f\u6570\u636e\u96c6Tex80M\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u6027\u5730\u7ed3\u5408\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0cTexTeller\u6a21\u578b\u5728\u624b\u5199\u6570\u5b66\u8868\u8fbe\u5f0f\u8bc6\u522b\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u7a81\u7834\u3002\u4e3a\u4fc3\u8fdb\u9886\u57df\u53d1\u5c55\uff0c\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5e93\u5c06\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2508.09516", "pdf": "https://arxiv.org/pdf/2508.09516", "abs": "https://arxiv.org/abs/2508.09516", "authors": ["Jakub \u0160m\u00edd", "Pavel Kr\u00e1l"], "title": "Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges", "categories": ["cs.CL"], "comment": "Submitted version prior to peer review. Updated version accepted in\n  Information Fusion. Official version:\n  https://www.sciencedirect.com/science/article/pii/S1566253525001460", "summary": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that focuses on understanding opinions at the aspect level, including\nsentiment towards specific aspect terms, categories, and opinions. While ABSA\nresearch has seen significant progress, much of the focus has been on\nmonolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from\nresource-rich languages (such as English) to low-resource languages, remains an\nunder-explored area, with no systematic review of the field. This paper aims to\nfill that gap by providing a comprehensive survey of cross-lingual ABSA. We\nsummarize key ABSA tasks, including aspect term extraction, aspect sentiment\nclassification, and compound tasks involving multiple sentiment elements.\nAdditionally, we review the datasets, modelling paradigms, and cross-lingual\ntransfer methods used to solve these tasks. We also examine how existing work\nin monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to\nthe development of cross-lingual ABSA. Finally, we highlight the main\nchallenges and suggest directions for future research to advance cross-lingual\nABSA systems.", "AI": {"tldr": "\u8de8\u8bed\u8a00\u65b9\u9762\u7ea7\u60c5\u611f\u5206\u6790(ABSA)\u7684\u7efc\u5408\u6027\u7efc\u8ff0\u3002", "motivation": "\u5f53\u524d\u65b9\u9762\u7ea7\u60c5\u611f\u5206\u6790\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u5355\u8bed\uff0c\u800c\u8de8\u8bed\u8a00ABSA\uff08\u7279\u522b\u662f\u8d44\u6e90\u7a00\u7f3a\u8bed\u8a00\uff09\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\uff0c\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u7684\u7efc\u8ff0\u3002", "method": "\u901a\u8fc7\u5bf9\u8de8\u8bed\u8a00ABSA\u8fdb\u884c\u5168\u9762\u7efc\u8ff0\uff0c\u6db5\u76d6\u5176\u5173\u952e\u4efb\u52a1\uff08\u5982\u65b9\u9762\u8bcd\u62bd\u53d6\u3001\u60c5\u611f\u5206\u7c7b\uff09\u3001\u6570\u636e\u96c6\u3001\u5efa\u6a21\u8303\u5f0f\u3001\u8de8\u8bed\u8a00\u8fc1\u79fb\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u5355\u8bed\u3001\u591a\u8bed\u53ca\u5927\u6a21\u578bABSA\u7684\u8d21\u732e\u3002", "result": "\u63d0\u4f9b\u8de8\u8bed\u8a00ABSA\u9886\u57df\u7684\u5168\u9762\u603b\u7ed3\uff0c\u660e\u786e\u4e86\u73b0\u6709\u4efb\u52a1\u3001\u65b9\u6cd5\u3001\u6570\u636e\u96c6\uff0c\u5e76\u8bc6\u522b\u4e86\u5f53\u524d\u6311\u6218\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u7efc\u8ff0\u586b\u8865\u4e86\u8de8\u8bed\u8a00ABSA\u9886\u57df\u7cfb\u7edf\u6027\u603b\u7ed3\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u8def\u7ebf\u56fe\u3002"}}
{"id": "2508.09208", "pdf": "https://arxiv.org/pdf/2508.09208", "abs": "https://arxiv.org/abs/2508.09208", "authors": ["Muqing Li", "Ning Li", "Xin Yuan", "Wenchao Xu", "Quan Chen", "Song Guo", "Haijun Zhang"], "title": "CoMoE: Collaborative Optimization of Expert Aggregation and Offloading for MoE-based LLMs at Edge", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "The proliferation of large language models (LLMs) has driven the adoption of\nMixture-of-Experts (MoE) architectures as a promising solution to scale model\ncapacity while controlling computational costs. However, deploying MoE models\nin resource-constrained mobile edge computing environments presents significant\nchallenges due to their large memory footprint and dynamic expert activation\npatterns. To address these challenges, we propose a novel dynamic\nresource-aware collaborative optimization framework that jointly optimizes\nexpert aggregation granularity and offloading strategies based on real-time\ndevice resource states, network conditions, and input characteristics in mobile\nedge environments, denoted as CoMoE. In CoMoE, we first systematically analyze\nexisting expert aggregation techniques, including expert parameter\nmerging,knowledge distillation,and parameter sharing decomposition, identifying\ntheir limitations in dynamic mobile environments.We then investigate expert\noffloading strategies encompassing expert prediction and prefetching, expert\ncaching and scheduling, and multi-tier storage architectures, revealing the\ninterdependencies between routing decisions and offloading performance.The\nCoMoE incorporates adaptive scheduling mechanisms that respond to user mobility\nand varying network conditions, enabling efficient MoE deployment across\nheterogeneous edge devices. Extensive experiments on real mobile edge testbeds\ndemonstrate that CoMoE achieves approximately 70% reduction in memory usage\ncompared to baseline methods, 10.5% lower inference latency than existing\nexpert offloading techniques, while maintaining model performance stability.\nFor large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE\nreduces memory requirements from 15.6GB to 4.7GB, enabling deployment on\nresource-constrained mobile edge devices that previously could only support\nmuch smaller models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCoMoE\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3MoE\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u79fb\u52a8\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u5185\u5b58\u548c\u52a8\u6001\u4e13\u5bb6\u6fc0\u6d3b\u6311\u6218\u3002CoMoE\u901a\u8fc7\u52a8\u6001\u8d44\u6e90\u611f\u77e5\u534f\u540c\u4f18\u5316\u4e13\u5bb6\u805a\u5408\u4e0e\u5378\u8f7d\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u4f7f\u5927\u578bMoE\u6a21\u578b\u5f97\u4ee5\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002", "motivation": "\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u666e\u53ca\u63a8\u52a8\u4e86MoE\u67b6\u6784\u7684\u5e94\u7528\uff0c\u5b83\u80fd\u6709\u6548\u6269\u5c55\u6a21\u578b\u5bb9\u91cf\u5e76\u63a7\u5236\u8ba1\u7b97\u6210\u672c\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5185\u5b58\u5360\u7528\u5927\u548c\u4e13\u5bb6\u6fc0\u6d3b\u6a21\u5f0f\u52a8\u6001\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u90e8\u7f72MoE\u6a21\u578b\u9762\u4e34\u663e\u8457\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51faCoMoE\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u52a8\u6001\u8d44\u6e90\u611f\u77e5\u534f\u540c\u4f18\u5316\u6846\u67b6\u3002\u8be5\u6846\u67b6\u57fa\u4e8e\u5b9e\u65f6\u8bbe\u5907\u8d44\u6e90\u72b6\u6001\u3001\u7f51\u7edc\u6761\u4ef6\u548c\u8f93\u5165\u7279\u6027\uff0c\u8054\u5408\u4f18\u5316\u4e13\u5bb6\u805a\u5408\u7c92\u5ea6\uff08\u901a\u8fc7\u5206\u6790\u73b0\u6709\u6280\u672f\u5982\u53c2\u6570\u5408\u5e76\u3001\u77e5\u8bc6\u84b8\u998f\u548c\u53c2\u6570\u5171\u4eab\u5206\u89e3\uff09\u548c\u4e13\u5bb6\u5378\u8f7d\u7b56\u7565\uff08\u5305\u62ec\u4e13\u5bb6\u9884\u6d4b\u3001\u9884\u53d6\u3001\u7f13\u5b58\u3001\u8c03\u5ea6\u4ee5\u53ca\u591a\u5c42\u5b58\u50a8\u67b6\u6784\uff09\u3002CoMoE\u8fd8\u6574\u5408\u4e86\u81ea\u9002\u5e94\u8c03\u5ea6\u673a\u5236\uff0c\u4ee5\u54cd\u5e94\u7528\u6237\u79fb\u52a8\u6027\u548c\u591a\u53d8\u7684\u7f51\u7edc\u6761\u4ef6\u3002", "result": "\u5728\u771f\u5b9e\u79fb\u52a8\u8fb9\u7f18\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCoMoE\u76f8\u8f83\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff1a\n- \u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\u7ea670%\u3002\n- \u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e10.5%\u3002\n- \u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7a33\u5b9a\u6027\u3002\n\u5bf9\u4e8e\u5927\u578bMoE\u6a21\u578b\uff08\u59827.4B\u53c2\u6570\u7684Switch-Base-128\uff09\uff0cCoMoE\u5c06\u5185\u5b58\u9700\u6c42\u4ece15.6GB\u964d\u81f34.7GB\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6210\u529f\u90e8\u7f72\u3002", "conclusion": "CoMoE\u6846\u67b6\u901a\u8fc7\u5176\u52a8\u6001\u8d44\u6e90\u611f\u77e5\u7684\u534f\u540c\u4f18\u5316\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86MoE\u6a21\u578b\u5728\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u96be\u9898\u3002\u5b83\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u9700\u6c42\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4ece\u800c\u4f7f\u5927\u578bMoE\u6a21\u578b\u80fd\u591f\u5728\u4ee5\u524d\u65e0\u6cd5\u652f\u6301\u5b83\u4eec\u7684\u8d44\u6e90\u53d7\u9650\u79fb\u52a8\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\u3002"}}
{"id": "2508.09893", "pdf": "https://arxiv.org/pdf/2508.09893", "abs": "https://arxiv.org/abs/2508.09893", "authors": ["Bhavik Agarwal", "Hemant Sunil Jomraj", "Simone Kaplunov", "Jack Krolick", "Viktoria Rojkova"], "title": "RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA", "categories": ["cs.AI"], "comment": null, "summary": "Regulatory compliance question answering (QA) requires precise, verifiable\ninformation, and domain-specific expertise, posing challenges for Large\nLanguage Models (LLMs). In this work, we present a novel multi-agent framework\nthat integrates a Knowledge Graph (KG) of Regulatory triplets with\nRetrieval-Augmented Generation (RAG) to address these demands. First, agents\nbuild and maintain an ontology-free KG by extracting subject--predicate--object\n(SPO) triplets from regulatory documents and systematically cleaning,\nnormalizing, deduplicating, and updating them. Second, these triplets are\nembedded and stored along with their corresponding textual sections and\nmetadata in a single enriched vector database, allowing for both graph-based\nreasoning and efficient information retrieval. Third, an orchestrated agent\npipeline leverages triplet-level retrieval for question answering, ensuring\nhigh semantic alignment between user queries and the factual\n\"who-did-what-to-whom\" core captured by the graph. Our hybrid system\noutperforms conventional methods in complex regulatory queries, ensuring\nfactual correctness with embedded triplets, enabling traceability through a\nunified vector database, and enhancing understanding through subgraph\nvisualization, providing a robust foundation for compliance-driven and broader\naudit-focused applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u76d1\u7ba1\u5408\u89c4\u95ee\u7b54\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4e09\u5143\u7ec4\u63d0\u53d6\u3001\u5b58\u50a8\u4e0e\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u95ee\u7b54\u7684\u51c6\u786e\u6027\u3001\u53ef\u8ffd\u6eaf\u6027\u548c\u7406\u89e3\u6027\uff0c\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u76d1\u7ba1\u5408\u89c4\u95ee\u7b54\u5bf9\u7cbe\u786e\u6027\u3001\u53ef\u9a8c\u8bc1\u4fe1\u606f\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u9ad8\u8981\u6c42\uff0c\u7ed9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u65b0\u578b\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u6574\u5408\u4e86\u76d1\u7ba1\u4e09\u5143\u7ec4\u77e5\u8bc6\u56fe\u8c31\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3002\u9996\u5148\uff0c\u667a\u80fd\u4f53\u4ece\u76d1\u7ba1\u6587\u6863\u4e2d\u63d0\u53d6\u3001\u6e05\u6d17\u3001\u89c4\u8303\u5316\u5e76\u66f4\u65b0\u4e3b\u8c13\u5bbe\uff08SPO\uff09\u4e09\u5143\u7ec4\u4ee5\u6784\u5efa\u65e0\u672c\u4f53\u77e5\u8bc6\u56fe\u8c31\u3002\u5176\u6b21\uff0c\u8fd9\u4e9b\u4e09\u5143\u7ec4\u8fde\u540c\u5176\u6587\u672c\u6bb5\u843d\u548c\u5143\u6570\u636e\u88ab\u5d4c\u5165\u5e76\u5b58\u50a8\u4e8e\u4e00\u4e2a\u5bcc\u5411\u91cf\u6570\u636e\u5e93\u4e2d\uff0c\u652f\u6301\u56fe\u63a8\u7406\u548c\u4fe1\u606f\u68c0\u7d22\u3002\u6700\u540e\uff0c\u4e00\u4e2a\u7f16\u6392\u7684\u667a\u80fd\u4f53\u7ba1\u9053\u5229\u7528\u4e09\u5143\u7ec4\u7ea7\u522b\u7684\u68c0\u7d22\u8fdb\u884c\u95ee\u7b54\uff0c\u786e\u4fdd\u67e5\u8be2\u4e0e\u4e8b\u5b9e\u6838\u5fc3\u7684\u8bed\u4e49\u9ad8\u5ea6\u5bf9\u9f50\u3002", "result": "\u8be5\u6df7\u5408\u7cfb\u7edf\u5728\u590d\u6742\u76d1\u7ba1\u67e5\u8be2\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002\u5b83\u901a\u8fc7\u5d4c\u5165\u7684\u4e09\u5143\u7ec4\u786e\u4fdd\u4e8b\u5b9e\u6b63\u786e\u6027\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u5411\u91cf\u6570\u636e\u5e93\u5b9e\u73b0\u53ef\u8ffd\u6eaf\u6027\uff0c\u5e76\u901a\u8fc7\u5b50\u56fe\u53ef\u89c6\u5316\u589e\u5f3a\u7406\u89e3\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u5408\u89c4\u9a71\u52a8\u548c\u66f4\u5e7f\u6cdb\u7684\u5ba1\u8ba1\u5e94\u7528\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2508.09162", "pdf": "https://arxiv.org/pdf/2508.09162", "abs": "https://arxiv.org/abs/2508.09162", "authors": ["Konstantinos Vasili", "Zachery T. Dahm", "William Richards", "Stylianos Chatzidakis"], "title": "An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals", "categories": ["cs.LG"], "comment": null, "summary": "Next generation advanced nuclear reactors are expected to be smaller both in\nsize and power output, relying extensively on fully digital instrumentation and\ncontrol systems. These reactors will generate a large flow of information in\nthe form of multivariate time series data, conveying simultaneously various non\nlinear cyber physical, process, control, sensor, and operational states.\nEnsuring data integrity against deception attacks is becoming increasingly\nimportant for networked communication and a requirement for safe and reliable\noperation. Current efforts to address replay attacks, almost universally focus\non watermarking or supervised anomaly detection approaches without further\nidentifying and characterizing the root cause of the anomaly. In addition,\nthese approaches rely mostly on synthetic data with uncorrelated Gaussian\nprocess and measurement noise and full state feedback or are limited to\nunivariate signals, signal stationarity, linear quadratic regulators, or other\nlinear-time invariant state-space which may fail to capture any unmodeled\nsystem dynamics. In the realm of regulated nuclear cyber-physical systems,\nadditional work is needed on characterization of replay attacks and\nexplainability of predictions using real data. Here, we propose an unsupervised\nexplainable AI framework based on a combination of autoencoder and customized\nwindowSHAP algorithm to fully characterize real-time replay attacks, i.e.,\ndetection, source identification, timing and type, of increasing complexity\nduring a dynamic time evolving reactor process. The proposed XAI framework was\nbenchmarked on several real world datasets from Purdue's nuclear reactor PUR-1\nwith up to six signals concurrently being replayed. In all cases, the XAI\nframework was able to detect and identify the source and number of signals\nbeing replayed and the duration of the falsification with 95 percent or better\naccuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u548c\u5b9a\u5236windowSHAP\u7684\u65e0\u76d1\u7763\u53ef\u89e3\u91caAI\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u68c0\u6d4b\u3001\u8bc6\u522b\u548c\u8868\u5f81\u6838\u53cd\u5e94\u5806\u4e2d\u7684\u91cd\u653e\u653b\u51fb\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8fbe\u5230\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u65b0\u4e00\u4ee3\u6838\u53cd\u5e94\u5806\u4f9d\u8d56\u5168\u6570\u5b57\u5316\u4eea\u8868\u548c\u63a7\u5236\u7cfb\u7edf\uff0c\u4ea7\u751f\u5927\u91cf\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002\u786e\u4fdd\u6570\u636e\u5b8c\u6574\u6027\u4ee5\u9632\u6b3a\u9a97\u653b\u51fb\uff08\u5982\u91cd\u653e\u653b\u51fb\uff09\u5bf9\u5b89\u5168\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u91cd\u653e\u653b\u51fb\u68c0\u6d4b\u65b9\u6cd5\u666e\u904d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u672a\u80fd\u8bc6\u522b\u5f02\u5e38\u6839\u672c\u539f\u56e0\u3001\u8fc7\u5ea6\u4f9d\u8d56\u5408\u6210\u6570\u636e\u53ca\u7b80\u5316\u5047\u8bbe\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u6570\u636e\u4e0b\u91cd\u653e\u653b\u51fb\u7684\u8868\u5f81\u548c\u9884\u6d4b\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u548c\u5b9a\u5236windowSHAP\u7b97\u6cd5\u7684\u65e0\u76d1\u7763\u53ef\u89e3\u91caAI\uff08XAI\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u65e8\u5728\u5168\u9762\u8868\u5f81\u5b9e\u65f6\u91cd\u653e\u653b\u51fb\uff08\u5305\u62ec\u68c0\u6d4b\u3001\u6e90\u8bc6\u522b\u3001\u65f6\u5e8f\u548c\u7c7b\u578b\uff09\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u53d8\u5316\u7684\u53cd\u5e94\u5806\u8fc7\u7a0b\u4e2d\u3002\u7814\u7a76\u56e2\u961f\u5728\u666e\u6e21\u5927\u5b66PUR-1\u6838\u53cd\u5e94\u5806\u7684\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5bf9\u8be5\u6846\u67b6\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5176\u4e2d\u5305\u542b\u591a\u8fbe\u516d\u4e2a\u5e76\u53d1\u91cd\u653e\u4fe1\u53f7\u3002", "result": "\u6240\u63d0\u51fa\u7684XAI\u6846\u67b6\u5728\u6240\u6709\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0c\u5747\u80fd\u4ee595%\u6216\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u68c0\u6d4b\u5e76\u8bc6\u522b\u51fa\u91cd\u653e\u4fe1\u53f7\u7684\u6765\u6e90\u3001\u6570\u91cf\u4ee5\u53ca\u4f2a\u9020\u6301\u7eed\u65f6\u95f4\u3002", "conclusion": "\u8be5\u65e0\u76d1\u7763\u53ef\u89e3\u91caAI\u6846\u67b6\u5728\u68c0\u6d4b\u548c\u5168\u9762\u8868\u5f81\u6838\u53cd\u5e94\u5806\u7cfb\u7edf\u4e2d\u7684\u590d\u6742\u5b9e\u65f6\u91cd\u653e\u653b\u51fb\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5229\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u9ad8\u7ea7\u6838\u53cd\u5e94\u5806\u7684\u5b89\u5168\u53ef\u9760\u8fd0\u884c\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u53ef\u89e3\u91ca\u6027\u652f\u6301\u3002"}}
{"id": "2508.09239", "pdf": "https://arxiv.org/pdf/2508.09239", "abs": "https://arxiv.org/abs/2508.09239", "authors": ["Zheng Zhou", "Yu-Jie Xiong", "Chun-Ming Xia", "Jia-Chen Zhang", "Hong-Jian Zhan"], "title": "Gradient-Direction-Aware Density Control for 3D Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced\nnovel view synthesis through explicit scene representation, enabling real-time\nphotorealistic rendering. However, existing approaches manifest two critical\nlimitations in complex scenarios: (1) Over-reconstruction occurs when\npersistent large Gaussians cannot meet adaptive splitting thresholds during\ndensity control. This is exacerbated by conflicting gradient directions that\nprevent effective splitting of these Gaussians; (2) Over-densification of\nGaussians occurs in regions with aligned gradient aggregation, leading to\nredundant component proliferation. This redundancy significantly increases\nmemory overhead due to unnecessary data retention. We present\nGradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware\nadaptive density control framework to address these challenges. Our key\ninnovations: the gradient coherence ratio (GCR), computed through normalized\ngradient vector norms, which explicitly discriminates Gaussians with concordant\nversus conflicting gradient directions; and a nonlinear dynamic weighting\nmechanism leverages the GCR to enable gradient-direction-aware density control.\nSpecifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting\noperations to enhance geometric details while suppressing redundant\nconcordant-direction Gaussians. Conversely, in cloning processes, GDAGS\npromotes concordant-direction Gaussian densification for structural completion\nwhile preventing conflicting-direction Gaussian overpopulation. Comprehensive\nevaluations across diverse real-world benchmarks demonstrate that GDAGS\nachieves superior rendering quality while effectively mitigating\nover-reconstruction, suppressing over-densification, and constructing compact\nscene representations with 50\\% reduced memory consumption through optimized\nGaussians utilization.", "AI": {"tldr": "GDAGS\u901a\u8fc7\u68af\u5ea6\u65b9\u5411\u611f\u77e5\u5bc6\u5ea6\u63a7\u5236\uff0c\u89e3\u51b33DGS\u4e2d\u7684\u8fc7\u91cd\u5efa\u548c\u8fc7\u5bc6\u5316\u95ee\u9898\uff0c\u63d0\u9ad8\u6e32\u67d3\u8d28\u91cf\u5e76\u663e\u8457\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u3002", "motivation": "\u73b0\u67093D Gaussian Splatting (3DGS) \u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a1) \u8fc7\u91cd\u5efa\uff0c\u5373\u5927\u578b\u9ad8\u65af\u7403\u56e0\u68af\u5ea6\u65b9\u5411\u51b2\u7a81\u800c\u65e0\u6cd5\u6709\u6548\u5206\u88c2\uff0c\u5bfc\u81f4\u7ec6\u8282\u4e22\u5931\uff1b2) \u8fc7\u5bc6\u5316\uff0c\u5373\u5728\u68af\u5ea6\u4e00\u81f4\u533a\u57df\u9ad8\u65af\u7403\u8fc7\u5ea6\u5bc6\u96c6\uff0c\u9020\u6210\u5197\u4f59\u548c\u5185\u5b58\u5f00\u9500\u589e\u52a0\u3002", "method": "\u672c\u6587\u63d0\u51faGradient-Direction-Aware Gaussian Splatting (GDAGS)\uff0c\u4e00\u79cd\u68af\u5ea6\u65b9\u5411\u611f\u77e5\u81ea\u9002\u5e94\u5bc6\u5ea6\u63a7\u5236\u6846\u67b6\u3002\u6838\u5fc3\u521b\u65b0\u5305\u62ec\uff1a1) \u68af\u5ea6\u4e00\u81f4\u6027\u6bd4\u7387(GCR)\uff0c\u7528\u4e8e\u533a\u5206\u68af\u5ea6\u65b9\u5411\u4e00\u81f4\u6216\u51b2\u7a81\u7684\u9ad8\u65af\u7403\uff1b2) \u975e\u7ebf\u6027\u52a8\u6001\u52a0\u6743\u673a\u5236\uff0c\u5229\u7528GCR\u8fdb\u884c\u5bc6\u5ea6\u63a7\u5236\u3002\u5177\u4f53\u800c\u8a00\uff0cGDAGS\u5728\u5206\u88c2\u65f6\u4f18\u5148\u5904\u7406\u68af\u5ea6\u51b2\u7a81\u7684\u9ad8\u65af\u7403\u4ee5\u589e\u5f3a\u7ec6\u8282\uff0c\u6291\u5236\u5197\u4f59\u7684\u68af\u5ea6\u4e00\u81f4\u9ad8\u65af\u7403\uff1b\u5728\u514b\u9686\u65f6\u4fc3\u8fdb\u68af\u5ea6\u4e00\u81f4\u9ad8\u65af\u7403\u7684\u5bc6\u5316\uff0c\u540c\u65f6\u9632\u6b62\u68af\u5ea6\u51b2\u7a81\u9ad8\u65af\u7403\u8fc7\u5ea6\u589e\u6b96\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGDAGS\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u8fc7\u91cd\u5efa\uff0c\u6291\u5236\u4e86\u8fc7\u5bc6\u5316\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u9ad8\u65af\u7403\u5229\u7528\u7387\u6784\u5efa\u4e86\u66f4\u7d27\u51d1\u7684\u573a\u666f\u8868\u793a\uff0c\u5185\u5b58\u6d88\u8017\u964d\u4f4e\u4e8650%\u3002", "conclusion": "GDAGS\u901a\u8fc7\u5176\u521b\u65b0\u7684\u68af\u5ea6\u65b9\u5411\u611f\u77e5\u5bc6\u5ea6\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e863DGS\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8fc7\u91cd\u5efa\u548c\u8fc7\u5bc6\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6e32\u67d3\u8d28\u91cf\u548c\u66f4\u4f4e\u7684\u5185\u5b58\u5360\u7528\uff0c\u6784\u5efa\u4e86\u66f4\u9ad8\u6548\u7684\u573a\u666f\u8868\u793a\u3002"}}
{"id": "2508.09517", "pdf": "https://arxiv.org/pdf/2508.09517", "abs": "https://arxiv.org/abs/2508.09517", "authors": ["Ladislav Lenc", "Daniel C\u00edfka", "Ji\u0159\u00ed Mart\u00ednek", "Jakub \u0160m\u00edd", "Pavel Kr\u00e1l"], "title": "UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval", "categories": ["cs.CL"], "comment": "Published in Proceedings of the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025). Official version:\n  https://aclanthology.org/2025.semeval-1.31/", "summary": "This paper presents a zero-shot system for fact-checked claim retrieval. We\nemployed several state-of-the-art large language models to obtain text\nembeddings. The models were then combined to obtain the best possible result.\nOur approach achieved 7th place in monolingual and 9th in cross-lingual\nsubtasks. We used only English translations as an input to the text embedding\nmodels since multilingual models did not achieve satisfactory results. We\nidentified the most relevant claims for each post by leveraging the embeddings\nand measuring cosine similarity. Overall, the best results were obtained by the\nNVIDIA NV-Embed-v2 model. For some languages, we benefited from model\ncombinations (NV-Embed & GPT or Mistral).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u96f6\u6837\u672c\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\u68c0\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u5d4c\u5165\u5e76\u7ed3\u5408\u4e0d\u540c\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u7ade\u4e89\u529b\u5f3a\u7684\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u7684\u96f6\u6837\u672c\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u6548\u5730\u8fdb\u884c\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\u68c0\u7d22\u3002", "method": "\u91c7\u7528\u591a\u79cd\u5148\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u6587\u672c\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u7ec4\u5408\u4f18\u5316\u6027\u80fd\u3002\u7cfb\u7edf\u5229\u7528\u5d4c\u5165\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6765\u8bc6\u522b\u6700\u76f8\u5173\u7684\u58f0\u660e\u3002\u7279\u522b\u5730\uff0c\u4e3a\u89c4\u907f\u591a\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u4ec5\u4f7f\u7528\u82f1\u6587\u7ffb\u8bd1\u4f5c\u4e3a\u6587\u672c\u5d4c\u5165\u6a21\u578b\u7684\u8f93\u5165\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5355\u8bed\u5b50\u4efb\u52a1\u4e2d\u6392\u540d\u7b2c7\uff0c\u5728\u8de8\u8bed\u5b50\u4efb\u52a1\u4e2d\u6392\u540d\u7b2c9\u3002NVIDIA NV-Embed-v2\u6a21\u578b\u5728\u6574\u4f53\u8868\u73b0\u4e0a\u6700\u4f73\uff0c\u800c\u5728\u67d0\u4e9b\u8bed\u8a00\u4e2d\uff0c\u6a21\u578b\u7ec4\u5408\uff08\u5982NV-Embed\u4e0eGPT\u6216Mistral\uff09\u5e26\u6765\u4e86\u989d\u5916\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u96f6\u6837\u672c\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\u68c0\u7d22\u7cfb\u7edf\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u7ade\u4e89\u529b\uff0c\u5176\u4e2dNVIDIA NV-Embed-v2\u6a21\u578b\u548c\u9488\u5bf9\u7279\u5b9a\u8bed\u8a00\u7684\u6a21\u578b\u7ec4\u5408\u662f\u5173\u952e\u3002\u5c06\u591a\u8bed\u8a00\u8f93\u5165\u7edf\u4e00\u4e3a\u82f1\u6587\u7ffb\u8bd1\u662f\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u7684\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2508.09229", "pdf": "https://arxiv.org/pdf/2508.09229", "abs": "https://arxiv.org/abs/2508.09229", "authors": ["Danil Sivtsov", "Aleksandr Katrutsa", "Ivan Oseledets"], "title": "Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference", "categories": ["cs.NI", "cs.AI", "cs.DC"], "comment": null, "summary": "Efficient deployment of a pre-trained LLM to a cluster with multiple servers\nis a critical step for providing fast responses to users' queries. The recent\nsuccess of Mixture-of-Experts (MoE) LLMs raises the question of how to deploy\nthem efficiently, considering their underlying structure. During the inference\nin MoE LLMs, only a small part of the experts is selected to process a given\ntoken. Moreover, in practice, the experts' load is highly imbalanced. For\nefficient deployment, one has to distribute the model across a large number of\nservers using a model placement algorithm. Thus, to improve cluster\nutilization, the model placement algorithm has to take into account the network\ntopology. This work focuses on the efficient topology-aware placement of the\npre-trained MoE LLMs in the inference stage. We propose an integer linear\nprogram (ILP) that determines the optimal placement of experts, minimizing the\nexpected number of transmissions. Due to the internal structure, this\noptimization problem can be solved with a standard ILP solver. We demonstrate\nthat ILP-based placement strategy yields lower network traffic than competitors\nfor small-scale (DeepSeekMoE~16B) and large-scale (DeepSeek-R1~671B) models.", "AI": {"tldr": "\u9488\u5bf9MoE LLM\u5728\u591a\u670d\u52a1\u5668\u96c6\u7fa4\u4e0a\u7684\u9ad8\u6548\u63a8\u7406\u90e8\u7f72\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6574\u6570\u7ebf\u6027\u89c4\u5212(ILP)\u7684\u62d3\u6251\u611f\u77e5\u4e13\u5bb6\u653e\u7f6e\u7b56\u7565\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u7f51\u7edc\u4f20\u8f93\u3002", "motivation": "\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u9ad8\u6548\u90e8\u7f72\u5bf9\u7528\u6237\u54cd\u5e94\u901f\u5ea6\u81f3\u5173\u91cd\u8981\u3002MoE LLM\u56e0\u5176\u7a00\u758f\u6fc0\u6d3b\u548c\u8d1f\u8f7d\u4e0d\u5747\u8861\u7279\u6027\uff0c\u5728\u591a\u670d\u52a1\u5668\u96c6\u7fa4\u90e8\u7f72\u4e2d\u9762\u4e34\u5982\u4f55\u6709\u6548\u5229\u7528\u96c6\u7fa4\u8d44\u6e90\u3001\u8003\u8651\u7f51\u7edc\u62d3\u6251\u5e76\u6700\u5c0f\u5316\u7f51\u7edc\u6d41\u91cf\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u6574\u6570\u7ebf\u6027\u89c4\u5212(ILP)\u6a21\u578b\uff0c\u7528\u4e8e\u4f18\u5316MoE LLM\u4e2d\u4e13\u5bb6\uff08experts\uff09\u5728\u591a\u670d\u52a1\u5668\u96c6\u7fa4\u4e0a\u7684\u653e\u7f6e\uff0c\u6838\u5fc3\u76ee\u6807\u662f\u6700\u5c0f\u5316\u9884\u671f\u7684\u7f51\u7edc\u4f20\u8f93\u6b21\u6570\u3002\u8be5ILP\u95ee\u9898\u53ef\u4f7f\u7528\u6807\u51c6ILP\u6c42\u89e3\u5668\u6c42\u89e3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e0e\u73b0\u6709\u7ade\u4e89\u7b56\u7565\u76f8\u6bd4\uff0c\u57fa\u4e8eILP\u7684\u653e\u7f6e\u7b56\u7565\u5728\u5c0f\u89c4\u6a21\uff08DeepSeekMoE~16B\uff09\u548c\u5927\u89c4\u6a21\uff08DeepSeek-R1~671B\uff09MoE\u6a21\u578b\u4e0a\u5747\u80fd\u663e\u8457\u964d\u4f4e\u7f51\u7edc\u6d41\u91cf\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u62d3\u6251\u611f\u77e5MoE LLM\u63a8\u7406\u9636\u6bb5\u90e8\u7f72\u7b56\u7565\uff0c\u901a\u8fc7ILP\u4f18\u5316\u4e13\u5bb6\u653e\u7f6e\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u7f51\u7edc\u4f20\u8f93\u91cf\uff0c\u63d0\u5347\u4e86\u96c6\u7fa4\u5229\u7528\u7387\u3002"}}
{"id": "2508.09932", "pdf": "https://arxiv.org/pdf/2508.09932", "abs": "https://arxiv.org/abs/2508.09932", "authors": ["Liang Zhang", "Edith Aurora Graf"], "title": "Mathematical Computation and Reasoning Errors by Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly utilized in AI-driven\neducational instruction and assessment, particularly within mathematics\neducation. The capability of LLMs to generate accurate answers and detailed\nsolutions for math problem-solving tasks is foundational for ensuring reliable\nand precise feedback and assessment in math education practices. Our study\nfocuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,\nDeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including\narithmetic, algebra, and number theory, and identifies step-level reasoning\nerrors within their solutions. Instead of relying on standard benchmarks, we\nintentionally build math tasks (via item models) that are challenging for LLMs\nand prone to errors. The accuracy of final answers and the presence of errors\nin individual solution steps were systematically analyzed and coded. Both\nsingle-agent and dual-agent configurations were tested. It is observed that the\nreasoning-enhanced OpenAI o1 model consistently achieved higher or nearly\nperfect accuracy across all three math task categories. Analysis of errors\nrevealed that procedural slips were the most frequent and significantly\nimpacted overall performance, while conceptual misunderstandings were less\nfrequent. Deploying dual-agent configurations substantially improved overall\nperformance. These findings offer actionable insights into enhancing LLM\nperformance and underscore effective strategies for integrating LLMs into\nmathematics education, thereby advancing AI-driven instructional practices and\nassessment precision.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u5b66\u4efb\u52a1\uff08\u7b97\u672f\u3001\u4ee3\u6570\u3001\u6570\u8bba\uff09\u4e0a\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5206\u6790\u4e86\u5176\u63a8\u7406\u9519\u8bef\uff0c\u53d1\u73b0OpenAI o1\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u53cc\u4ee3\u7406\u914d\u7f6e\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u968f\u7740LLMs\u5728AI\u9a71\u52a8\u6559\u80b2\uff08\u7279\u522b\u662f\u6570\u5b66\u6559\u80b2\uff09\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u5176\u5728\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e2d\u63d0\u4f9b\u51c6\u786e\u7b54\u6848\u548c\u8be6\u7ec6\u89e3\u51b3\u65b9\u6848\u7684\u80fd\u529b\u5bf9\u4e8e\u53ef\u9760\u7684\u53cd\u9988\u548c\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30LLMs\u7684\u51c6\u786e\u6027\u5e76\u8bc6\u522b\u5176\u63a8\u7406\u9519\u8bef\u7c7b\u578b\u3002", "method": "\u8bc4\u4f30\u4e86\u56db\u79cdLLMs\uff08OpenAI GPT-4o, o1, DeepSeek-V3, DeepSeek-R1\uff09\u5728\u4e09\u79cd\u6570\u5b66\u4efb\u52a1\uff08\u7b97\u672f\u3001\u4ee3\u6570\u3001\u6570\u8bba\uff09\u4e0a\u7684\u8868\u73b0\u3002\u7814\u7a76\u6784\u5efa\u4e86LLMs\u6613\u72af\u9519\u7684\u6311\u6218\u6027\u6570\u5b66\u4efb\u52a1\uff08\u800c\u975e\u6807\u51c6\u57fa\u51c6\uff09\u3002\u7cfb\u7edf\u5206\u6790\u5e76\u7f16\u7801\u4e86\u6700\u7ec8\u7b54\u6848\u7684\u51c6\u786e\u6027\u4ee5\u53ca\u89e3\u9898\u6b65\u9aa4\u4e2d\u7684\u9519\u8bef\u3002\u540c\u65f6\u6d4b\u8bd5\u4e86\u5355\u4ee3\u7406\u548c\u53cc\u4ee3\u7406\u914d\u7f6e\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7ecf\u8fc7\u63a8\u7406\u589e\u5f3a\u7684OpenAI o1\u6a21\u578b\u5728\u6240\u6709\u4e09\u7c7b\u6570\u5b66\u4efb\u52a1\u4e2d\u5747\u4fdd\u6301\u4e86\u66f4\u9ad8\u6216\u63a5\u8fd1\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u3002\u9519\u8bef\u5206\u6790\u8868\u660e\uff0c\u7a0b\u5e8f\u6027\u5931\u8bef\u662f\u6700\u5e38\u89c1\u7684\u9519\u8bef\u7c7b\u578b\uff0c\u5bf9\u6574\u4f53\u6027\u80fd\u5f71\u54cd\u6700\u5927\uff0c\u800c\u6982\u5ff5\u6027\u8bef\u89e3\u5219\u8f83\u5c11\u3002\u90e8\u7f72\u53cc\u4ee3\u7406\u914d\u7f6e\u663e\u8457\u63d0\u5347\u4e86\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u63d0\u5347LLM\u6027\u80fd\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u5e76\u5f3a\u8c03\u4e86\u5c06LLMs\u6709\u6548\u6574\u5408\u5230\u6570\u5b66\u6559\u80b2\u4e2d\u7684\u7b56\u7565\uff0c\u4ece\u800c\u63a8\u8fdb\u4e86AI\u9a71\u52a8\u7684\u6559\u5b66\u5b9e\u8df5\u548c\u8bc4\u4f30\u7684\u7cbe\u786e\u6027\u3002"}}
{"id": "2508.09163", "pdf": "https://arxiv.org/pdf/2508.09163", "abs": "https://arxiv.org/abs/2508.09163", "authors": ["Ziheng Wang", "Pedro Reviriego", "Farzad Niknia", "Zhen Gao", "Javier Conde", "Shanshan Liu", "Fabrizio Lombardi"], "title": "Energy-Efficient Stochastic Computing (SC) Neural Networks for Internet of Things Devices With Layer-Wise Adjustable Sequence Length (ASL)", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Stochastic computing (SC) has emerged as an efficient low-power alternative\nfor deploying neural networks (NNs) in resource-limited scenarios, such as the\nInternet of Things (IoT). By encoding values as serial bitstreams, SC\nsignificantly reduces energy dissipation compared to conventional\nfloating-point (FP) designs; however, further improvement of layer-wise\nmixed-precision implementation for SC remains unexplored. This article\nintroduces Adjustable Sequence Length (ASL), a novel scheme that applies\nmixed-precision concepts specifically to SC NNs. By introducing an\noperator-norm-based theoretical model, this article shows that truncation noise\ncan cumulatively propagate through the layers by the estimated amplification\nfactors. An extended sensitivity analysis is presented, using random forest\n(RF) regression to evaluate multilayer truncation effects and validate the\nalignment of theoretical predictions with practical network behaviors. To\naccommodate different application scenarios, this article proposes two\ntruncation strategies (coarse-grained and fine-grained), which apply diverse\nsequence length configurations at each layer. Evaluations on a pipelined SC MLP\nsynthesized at 32nm demonstrate that ASL can reduce energy and latency\noverheads by up to over 60% with negligible accuracy loss. It confirms the\nfeasibility of the ASL scheme for IoT applications and highlights the distinct\nadvantages of mixed-precision truncation in SC designs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53ef\u8c03\u5e8f\u5217\u957f\u5ea6\uff08ASL\uff09\u7684\u65b0\u578b\u65b9\u6848\uff0c\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u622a\u65ad\u663e\u8457\u964d\u4f4e\u4e86\u7269\u8054\u7f51\uff08IoT\uff09\u573a\u666f\u4e0b\u968f\u673a\u8ba1\u7b97\uff08SC\uff09\u795e\u7ecf\u7f51\u7edc\u7684\u80fd\u8017\u548c\u5ef6\u8fdf\u3002", "motivation": "\u968f\u673a\u8ba1\u7b97\uff08SC\uff09\u662f\u7269\u8054\u7f51\u7b49\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u90e8\u7f72\u795e\u7ecf\u7f51\u7edc\u7684\u4f4e\u529f\u8017\u66ff\u4ee3\u65b9\u6848\u3002\u5c3d\u7ba1SC\u76f8\u6bd4\u6d6e\u70b9\u8bbe\u8ba1\u80fd\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff0c\u4f46\u9488\u5bf9SC\u795e\u7ecf\u7f51\u7edc\u7684\u9010\u5c42\u6df7\u5408\u7cbe\u5ea6\u5b9e\u73b0\u4ecd\u672a\u5145\u5206\u63a2\u7d22\uff0c\u8fd9\u662f\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u7387\u7684\u74f6\u9888\u3002", "method": "\u672c\u6587\u63d0\u51faASL\u65b9\u6848\uff0c\u5c06\u6df7\u5408\u7cbe\u5ea6\u6982\u5ff5\u5e94\u7528\u4e8eSC\u795e\u7ecf\u7f51\u7edc\u3002\u5b83\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7b97\u5b50\u8303\u6570\u7684\u7406\u8bba\u6a21\u578b\u6765\u9884\u6d4b\u622a\u65ad\u566a\u58f0\u7684\u7d2f\u79ef\u4f20\u64ad\u3002\u901a\u8fc7\u968f\u673a\u68ee\u6797\uff08RF\uff09\u56de\u5f52\u8fdb\u884c\u6269\u5c55\u654f\u611f\u6027\u5206\u6790\uff0c\u9a8c\u8bc1\u7406\u8bba\u9884\u6d4b\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u7c97\u7c92\u5ea6\uff08coarse-grained\uff09\u548c\u7ec6\u7c92\u5ea6\uff08fine-grained\uff09\u4e24\u79cd\u622a\u65ad\u7b56\u7565\u4ee5\u9002\u5e94\u4e0d\u540c\u5e94\u7528\u573a\u666f\u3002", "result": "\u572832\u7eb3\u7c73\u5de5\u827a\u4e0b\u5408\u6210\u7684\u6d41\u6c34\u7ebfSC MLP\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cASL\u65b9\u6848\u53ef\u5728\u4fdd\u6301\u53ef\u5ffd\u7565\u7684\u7cbe\u5ea6\u635f\u5931\u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u80fd\u8017\u548c\u5ef6\u8fdf\u5f00\u9500\u964d\u4f4e60%\u4ee5\u4e0a\u3002", "conclusion": "ASL\u65b9\u6848\u5728\u7269\u8054\u7f51\u5e94\u7528\u4e2d\u662f\u53ef\u884c\u7684\uff0c\u5e76\u7a81\u51fa\u663e\u793a\u4e86SC\u8bbe\u8ba1\u4e2d\u6df7\u5408\u7cbe\u5ea6\u622a\u65ad\u7684\u72ec\u7279\u4f18\u52bf\u3002"}}
{"id": "2508.09241", "pdf": "https://arxiv.org/pdf/2508.09241", "abs": "https://arxiv.org/abs/2508.09241", "authors": ["Fengxian Ji", "Jingpu Yang", "Zirui Song", "Yuanxi Wang", "Zhexuan Cui", "Yuke Li", "Qian Jiang", "Miao Fang", "Xiuying Chen"], "title": "FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents", "categories": ["cs.CV"], "comment": "submit/6682470 (Fengxian Ji)", "summary": "With the rapid advancement of generative artificial intelligence technology,\nGraphical User Interface (GUI) agents have demonstrated tremendous potential\nfor autonomously managing daily tasks through natural language instructions.\nHowever, current evaluation frameworks for GUI agents suffer from fundamental\nflaws: existing benchmarks overly focus on coarse-grained task completion while\nneglecting fine-grained control capabilities crucial for real-world\napplications. To address this, we introduce FineState-Bench, the first\nevaluation and diagnostic standard for fine-grained GUI proxy operations,\ndesigned to quantify fine-grained control. This multi-platform (desktop, Web,\nmobile) framework includes 2257 task benchmarks in four components and uses a\nfour-phase indicator for comprehensive perception-to-control assessment. To\nanalyze perception and positioning for refined operations, we developed the\nplug-and-play Visual Diagnostic Assistant (VDA), enabling the first\nquantitative decoupling analysis of these capabilities. Experimental results on\nour benchmark show that the most advanced models achieve only 32.8%\nfine-grained interaction accuracy. Using our VDA in controlled experiments,\nquantifying the impact of visual capabilities, we showed that ideal visual\nlocalization boosts Gemini-2.5-Flash's success rate by 14.9\\%. Our diagnostic\nframework confirms for the first time that the primary bottleneck for current\nGUI proxies is basic visual positioning capability.All resources are fully\nopen-source. github: https://github.com/AnonymousThewarehouse/FineState-Bench\nhuggingface: https://huggingface.co/datasets/Willtime2006/Static-FineBench", "AI": {"tldr": "\u5f15\u5165FineState-Bench\uff0c\u9996\u4e2a\u9488\u5bf9GUI\u4ee3\u7406\u7cbe\u7ec6\u64cd\u4f5c\u7684\u8bc4\u4f30\u4e0e\u8bca\u65ad\u57fa\u51c6\uff0c\u63ed\u793a\u5f53\u524dGUI\u4ee3\u7406\u7684\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u57fa\u7840\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u7684\u8bc4\u4f30\u6846\u67b6\u5b58\u5728\u6839\u672c\u7f3a\u9677\uff0c\u8fc7\u5ea6\u5173\u6ce8\u7c97\u7c92\u5ea6\u4efb\u52a1\u5b8c\u6210\uff0c\u800c\u5ffd\u89c6\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u5173\u952e\u7684\u7cbe\u7ec6\u63a7\u5236\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86FineState-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u5e73\u53f0\uff08\u684c\u9762\u3001Web\u3001\u79fb\u52a8\uff09\u7684\u7cbe\u7ec6GUI\u4ee3\u7406\u64cd\u4f5c\u8bc4\u4f30\u4e0e\u8bca\u65ad\u6807\u51c6\uff0c\u5305\u542b2257\u4e2a\u4efb\u52a1\u57fa\u51c6\u548c\u56db\u9636\u6bb5\u6307\u6807\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86\u5373\u63d2\u5373\u7528\u5f0f\u89c6\u89c9\u8bca\u65ad\u52a9\u624b\uff08VDA\uff09\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9\u611f\u77e5\u548c\u5b9a\u4f4d\u80fd\u529b\u7684\u5b9a\u91cf\u89e3\u8026\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728FineState-Bench\u4e0a\u7684\u7cbe\u7ec6\u4ea4\u4e92\u51c6\u786e\u7387\u4ec5\u4e3a32.8%\u3002\u901a\u8fc7VDA\u8fdb\u884c\u7684\u53d7\u63a7\u5b9e\u9a8c\u8868\u660e\uff0c\u7406\u60f3\u7684\u89c6\u89c9\u5b9a\u4f4d\u53ef\u5c06Gemini-2.5-Flash\u7684\u6210\u529f\u7387\u63d0\u9ad814.9%\u3002", "conclusion": "\u9996\u6b21\u8bc1\u5b9e\uff0c\u5f53\u524dGUI\u4ee3\u7406\u7684\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u5176\u57fa\u7840\u89c6\u89c9\u5b9a\u4f4d\u80fd\u529b\u4e0d\u8db3\u3002"}}
{"id": "2508.09521", "pdf": "https://arxiv.org/pdf/2508.09521", "abs": "https://arxiv.org/abs/2508.09521", "authors": ["Yunxiao Wang", "Meng Liu", "Wenqi Liu", "Kaiyu Jiang", "Bin Wen", "Fan Yang", "Tingting Gao", "Guorui Zhou", "Liqiang Nie"], "title": "COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Emotional support conversations are crucial for promoting emotional\nwell-being, yet current models often lack deep empathetic reasoning grounded in\npsychological principles. To address this, we propose controllable empathetic\nreasoning, which combines natural language reasoning with structured\npsychological steps. We construct a fine-grained dataset annotated with\nreasoning correctness and response preferences to enable this capability. To\nfurther enhance training, we employ reinforcement learning with a unified\nprocess-outcome reward model that delivers precise feedback. To mitigate\nresponse repetitiveness from entropy collapse, we introduce personality-based\ndialogue rewriting and a redundancy-aware reward reweighting strategy. Our\napproach significantly improves model's emotional support ability, advancing\nthe development of empathetic, human-like support systems.", "AI": {"tldr": "\u9488\u5bf9\u73b0\u6709\u60c5\u611f\u652f\u6301\u6a21\u578b\u7f3a\u4e4f\u6df1\u5c42\u540c\u7406\u5fc3\u63a8\u7406\uff0c\u672c\u6587\u63d0\u51fa\u53ef\u63a7\u540c\u7406\u5fc3\u63a8\u7406\uff0c\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4e0e\u521b\u65b0\u7b56\u7565\u63d0\u5347\u6a21\u578b\u60c5\u611f\u652f\u6301\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u60c5\u611f\u652f\u6301\u6a21\u578b\u7f3a\u4e4f\u57fa\u4e8e\u5fc3\u7406\u5b66\u539f\u5219\u7684\u6df1\u5c42\u540c\u7406\u5fc3\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u6709\u6548\u4fc3\u8fdb\u60c5\u611f\u5065\u5eb7\u3002", "method": "\u63d0\u51fa\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4e0e\u7ed3\u6784\u5316\u5fc3\u7406\u5b66\u6b65\u9aa4\u7684\u201c\u53ef\u63a7\u540c\u7406\u5fc3\u63a8\u7406\u201d\uff1b\u6784\u5efa\u6807\u6ce8\u63a8\u7406\u6b63\u786e\u6027\u548c\u56de\u590d\u504f\u597d\u7684\u7cbe\u7ec6\u6570\u636e\u96c6\uff1b\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u5e76\u4f7f\u7528\u7edf\u4e00\u7684\u201c\u8fc7\u7a0b-\u7ed3\u679c\u5956\u52b1\u6a21\u578b\u201d\u4ee5\u63d0\u4f9b\u7cbe\u786e\u53cd\u9988\uff1b\u5f15\u5165\u201c\u57fa\u4e8e\u4e2a\u6027\u7684\u5bf9\u8bdd\u91cd\u5199\u201d\u548c\u201c\u5197\u4f59\u611f\u77e5\u5956\u52b1\u91cd\u52a0\u6743\u7b56\u7565\u201d\u4ee5\u7f13\u89e3\u56de\u590d\u91cd\u590d\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u60c5\u611f\u652f\u6301\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u5f00\u53d1\u66f4\u5177\u540c\u7406\u5fc3\u548c\u4eba\u6027\u5316\u7684\u60c5\u611f\u652f\u6301\u7cfb\u7edf\u7684\u8fdb\u7a0b\u3002"}}
{"id": "2508.09240", "pdf": "https://arxiv.org/pdf/2508.09240", "abs": "https://arxiv.org/abs/2508.09240", "authors": ["Zainab Khan", "Ahmed Hussain", "Mukesh Thakur", "Arto Hellas", "Panos Papadimitratos"], "title": "NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation", "categories": ["cs.NI", "cs.AI", "cs.CL"], "comment": "6 pages", "summary": "The use of Service-Based Architecture in modern telecommunications has\nexponentially increased Network Functions (NFs) and Application Programming\nInterfaces (APIs), creating substantial operational complexities in service\ndiscovery and management. We introduce \\textit{NEFMind}, a framework leveraging\nparameter-efficient fine-tuning of open-source Large Language Models (LLMs) to\naddress these challenges. It integrates three core components: synthetic\ndataset generation from Network Exposure Function (NEF) API specifications,\nmodel optimization through Quantized-Low-Rank Adaptation, and performance\nevaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G\nService-Based Architecture APIs, our approach achieves 85% reduction in\ncommunication overhead compared to manual discovery methods. Experimental\nvalidation using the open-source Phi-2 model demonstrates exceptional API call\nidentification performance at 98-100% accuracy. The fine-tuned Phi-2 model\ndelivers performance comparable to significantly larger models like GPT-4 while\nmaintaining computational efficiency for telecommunications infrastructure\ndeployment. These findings validate domain-specific, parameter-efficient LLM\nstrategies for managing complex API ecosystems in next-generation\ntelecommunications networks.", "AI": {"tldr": "NEFMind\u6846\u67b6\u5229\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u89e3\u51b3\u4e86\u7535\u4fe1\u9886\u57df\u670d\u52a1\u5316\u67b6\u6784\u4e2d\u670d\u52a1\u53d1\u73b0\u548c\u7ba1\u7406\u56e0API\u6fc0\u589e\u5e26\u6765\u7684\u590d\u6742\u6027\u3002", "motivation": "\u73b0\u4ee3\u7535\u4fe1\u7f51\u7edc\u4e2d\uff0c\u57fa\u4e8e\u670d\u52a1\u7684\u67b6\u6784\u5bfc\u81f4\u7f51\u7edc\u529f\u80fd\uff08NFs\uff09\u548cAPI\u6570\u91cf\u5448\u6307\u6570\u7ea7\u589e\u957f\uff0c\u7ed9\u670d\u52a1\u53d1\u73b0\u548c\u7ba1\u7406\u5e26\u6765\u4e86\u5de8\u5927\u7684\u64cd\u4f5c\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165NEFMind\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5f00\u6e90LLM\uff08\u5982Phi-2\uff09\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u3002\u5b83\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4ece\u7f51\u7edc\u66b4\u9732\u529f\u80fd\uff08NEF\uff09API\u89c4\u8303\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u91cf\u5316\u4f4e\u79e9\u9002\u5e94\uff08Quantized-Low-Rank Adaptation\uff09\u8fdb\u884c\u6a21\u578b\u4f18\u5316\uff0c\u5e76\u4f7f\u7528GPT-4 Ref Score\u548cBertScore\u6307\u6807\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u9488\u5bf95G\u670d\u52a1\u5316\u67b6\u6784API\uff0c\u8be5\u65b9\u6cd5\u4f7f\u901a\u4fe1\u5f00\u9500\u6bd4\u624b\u52a8\u53d1\u73b0\u65b9\u6cd5\u51cf\u5c1185%\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0c\u5fae\u8c03\u540e\u7684Phi-2\u6a21\u578b\u5728API\u8c03\u7528\u8bc6\u522b\u65b9\u9762\u8fbe\u5230\u4e8698-100%\u7684\u51c6\u786e\u7387\uff0c\u4e14\u6027\u80fd\u4e0eGPT-4\u7b49\u5927\u578b\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u9a8c\u8bc1\u4e86\u9886\u57df\u7279\u5b9a\u3001\u53c2\u6570\u9ad8\u6548\u7684LLM\u7b56\u7565\u5728\u7ba1\u7406\u4e0b\u4e00\u4ee3\u7535\u4fe1\u7f51\u7edc\u4e2d\u590d\u6742API\u751f\u6001\u7cfb\u7edf\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.19716", "pdf": "https://arxiv.org/pdf/2504.19716", "abs": "https://arxiv.org/abs/2504.19716", "authors": ["Navin Sriram Ravie", "Keerthi Vasan M", "Asokan Thondiyath", "Bijo Sebastian"], "title": "QuickGrasp: Lightweight Antipodal Grasp Planning with Point Clouds", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Grasping has been a long-standing challenge in facilitating the final\ninterface between a robot and the environment. As environments and tasks become\ncomplicated, the need to embed higher intelligence to infer from the\nsurroundings and act on them has become necessary. Although most methods\nutilize techniques to estimate grasp pose by treating the problem via pure\nsampling-based approaches in the six-degree-of-freedom space or as a learning\nproblem, they usually fail in real-life settings owing to poor generalization\nacross domains. In addition, the time taken to generate the grasp plan and the\nlack of repeatability, owing to sampling inefficiency and the probabilistic\nnature of existing grasp planning approaches, severely limits their application\nin real-world tasks. This paper presents a lightweight analytical approach\ntowards robotic grasp planning, particularly antipodal grasps, with little to\nno sampling in the six-degree-of-freedom space. The proposed grasp planning\nalgorithm is formulated as an optimization problem towards estimating grasp\npoints on the object surface instead of directly estimating the end-effector\npose. To this extent, a soft-region-growing algorithm is presented for\neffective plane segmentation, even in the case of curved surfaces. An\noptimization-based quality metric is then used for the evaluation of grasp\npoints to ensure indirect force closure. The proposed grasp framework is\ncompared with the existing state-of-the-art grasp planning approach, Grasp pose\ndetection (GPD), as a baseline over multiple simulated objects. The\neffectiveness of the proposed approach in comparison to GPD is also evaluated\nin a real-world setting using image and point-cloud data, with the planned\ngrasps being executed using a ROBOTIQ gripper and UR5 manipulator.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5206\u6790\u65b9\u6cd5\u7528\u4e8e\u673a\u5668\u4eba\u6293\u53d6\u89c4\u5212\uff0c\u5c24\u5176\u9488\u5bf9\u5bf9\u5076\u6293\u53d6\uff0c\u901a\u8fc7\u4f18\u5316\u7269\u4f53\u8868\u9762\u6293\u53d6\u70b9\u800c\u975e\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u91c7\u6837\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee\u548c\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u4e0e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5GPD\u8fdb\u884c\u4e86\u6bd4\u8f83\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u6293\u53d6\u662f\u673a\u5668\u4eba\u4e0e\u73af\u5883\u4ea4\u4e92\u7684\u957f\u671f\u6311\u6218\u3002\u968f\u7740\u73af\u5883\u548c\u4efb\u52a1\u65e5\u76ca\u590d\u6742\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u6293\u53d6\u65b9\u6cd5\u3002\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u57fa\u4e8e\u7eaf\u91c7\u6837\u6216\u5b66\u4e60\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6cdb\u5316\u6027\u5dee\uff0c\u4e14\u7531\u4e8e\u91c7\u6837\u6548\u7387\u4f4e\u548c\u6982\u7387\u6027\uff0c\u5bfc\u81f4\u89c4\u5212\u8017\u65f6\u957f\u3001\u91cd\u590d\u6027\u5dee\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u5728\u516d\u81ea\u7531\u5ea6\u7a7a\u95f4\u4e2d\u51e0\u4e4e\u4e0d\u8fdb\u884c\u91c7\u6837\u3002\u8be5\u6293\u53d6\u89c4\u5212\u7b97\u6cd5\u88ab\u516c\u5f0f\u5316\u4e3a\u4f30\u8ba1\u7269\u4f53\u8868\u9762\u6293\u53d6\u70b9\u7684\u4f18\u5316\u95ee\u9898\uff0c\u800c\u975e\u76f4\u63a5\u4f30\u8ba1\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5f15\u5165\u8f6f\u533a\u57df\u751f\u957f\u7b97\u6cd5\uff0c\u5b9e\u73b0\u6709\u6548\u7684\u5e73\u9762\u5206\u5272\uff0c\u5373\u4f7f\u662f\u66f2\u9762\u4e5f\u80fd\u5904\u7406\u30022) \u91c7\u7528\u57fa\u4e8e\u4f18\u5316\u7684\u8d28\u91cf\u5ea6\u91cf\u6765\u8bc4\u4f30\u6293\u53d6\u70b9\uff0c\u4ee5\u786e\u4fdd\u95f4\u63a5\u529b\u95ed\u5408\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6293\u53d6\u6846\u67b6\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6293\u53d6\u89c4\u5212\u65b9\u6cd5GPD\u5728\u591a\u4e2a\u6a21\u62df\u5bf9\u8c61\u4e0a\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\uff0c\u4f7f\u7528\u56fe\u50cf\u548c\u70b9\u4e91\u6570\u636e\uff0c\u5e76\u5229\u7528ROBOTIQ\u5939\u5177\u548cUR5\u673a\u68b0\u81c2\u6267\u884c\u89c4\u5212\u7684\u6293\u53d6\uff0c\u4e5f\u8bc4\u4f30\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u76f8\u5bf9\u4e8eGPD\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u3001\u8f7b\u91cf\u7ea7\u7684\u5206\u6790\u6293\u53d6\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6293\u53d6\u70b9\u800c\u975e\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\uff0c\u6709\u6548\u514b\u670d\u4e86\u73b0\u6709\u91c7\u6837\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u3001\u6548\u7387\u548c\u91cd\u590d\u6027\u65b9\u9762\u7684\u5c40\u9650\u3002\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u8bc4\u4f30\u5747\u9a8c\u8bc1\u4e86\u5176\u76f8\u5bf9\u4e8e\u73b0\u6709\u5148\u8fdb\u6293\u53d6\u89c4\u5212\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.09164", "pdf": "https://arxiv.org/pdf/2508.09164", "abs": "https://arxiv.org/abs/2508.09164", "authors": ["Min Tang", "Peng Lu", "Qing Feng"], "title": "Generating Feasible and Diverse Synthetic Populations Using Diffusion Models", "categories": ["cs.LG"], "comment": null, "summary": "Population synthesis is a critical task that involves generating synthetic\nyet realistic representations of populations. It is a fundamental problem in\nagent-based modeling (ABM), which has become the standard to analyze\nintelligent transportation systems. The synthetic population serves as the\nprimary input for ABM transportation simulation, with traveling agents\nrepresented by population members. However, when the number of attributes\ndescribing agents becomes large, survey data often cannot densely support the\njoint distribution of the attributes in the population due to the curse of\ndimensionality. This sparsity makes it difficult to accurately model and\nproduce the population. Interestingly, deep generative models trained from\navailable sample data can potentially synthesize possible attribute\ncombinations that present in the actual population but do not exist in the\nsample data(called sampling zeros). Nevertheless, this comes at the cost of\nfalsely generating the infeasible attribute combinations that do not exist in\nthe population (called structural zeros). In this study, a novel diffusion\nmodel-based population synthesis method is proposed to estimate the underlying\njoint distribution of a population. This approach enables the recovery of\nnumerous missing sampling zeros while keeping the generated structural zeros\nminimal. Our method is compared with other recently proposed approaches such as\nVariational Autoencoders (VAE) and Generative Adversarial Network (GAN)\napproaches, which have shown success in high dimensional tabular population\nsynthesis. We assess the performance of the synthesized outputs using a range\nof metrics, including marginal distribution similarity, feasibility, and\ndiversity. The results demonstrate that our proposed method outperforms\nprevious approaches in achieving a better balance between the feasibility and\ndiversity of the synthesized population.", "AI": {"tldr": "\u9488\u5bf9\u667a\u80fd\u4ea4\u901a\u7cfb\u7edfABM\u4e2d\u9ad8\u7ef4\u4eba\u53e3\u5408\u6210\u7684\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4eba\u53e3\u5408\u6210\u65b9\u6cd5\uff0c\u65e8\u5728\u6709\u6548\u6062\u590d\u7f3a\u5931\u7684\u201c\u91c7\u6837\u96f6\u201d\u5e76\u6700\u5c0f\u5316\u65e0\u6548\u7684\u201c\u7ed3\u6784\u96f6\u201d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u4eba\u53e3\u7684\u53ef\u884c\u6027\u548c\u591a\u6837\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709VAE\u548cGAN\u7b49\u6a21\u578b\u3002", "motivation": "\u4eba\u53e3\u5408\u6210\u662f\u667a\u80fd\u4ea4\u901a\u7cfb\u7edfABM\u7684\u5173\u952e\u8f93\u5165\uff0c\u4f46\u5f53\u63cf\u8ff0\u4ee3\u7406\u7684\u5c5e\u6027\u6570\u91cf\u589e\u591a\u65f6\uff0c\u8c03\u67e5\u6570\u636e\u56e0\u201c\u7ef4\u5ea6\u8bc5\u5492\u201d\u800c\u7a00\u758f\uff0c\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u4eba\u53e3\u8054\u5408\u5206\u5e03\u3002\u73b0\u6709\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u867d\u80fd\u5408\u6210\u201c\u91c7\u6837\u96f6\u201d\uff08\u771f\u5b9e\u5b58\u5728\u4f46\u6837\u672c\u7f3a\u5931\u7684\u6570\u636e\uff09\uff0c\u4f46\u5f80\u5f80\u4f1a\u4f34\u968f\u4ea7\u751f\u201c\u7ed3\u6784\u96f6\u201d\uff08\u771f\u5b9e\u4e0d\u5b58\u5728\u7684\u65e0\u6548\u7ec4\u5408\uff09\uff0c\u5f71\u54cd\u5408\u6210\u4eba\u53e3\u7684\u51c6\u786e\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4eba\u53e3\u5408\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u4eba\u53e3\u7684\u6f5c\u5728\u8054\u5408\u5206\u5e03\u3002\u8be5\u65b9\u6cd5\u65e8\u5728\u6062\u590d\u5927\u91cf\u7f3a\u5931\u7684\u201c\u91c7\u6837\u96f6\u201d\uff0c\u540c\u65f6\u5c06\u751f\u6210\u7684\u201c\u7ed3\u6784\u96f6\u201d\u4fdd\u6301\u5728\u6700\u4f4e\u9650\u5ea6\u3002", "result": "\u901a\u8fc7\u4e0e\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7b49\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5b9e\u73b0\u5408\u6210\u4eba\u53e3\u7684\u201c\u53ef\u884c\u6027\u201d\u548c\u201c\u591a\u6837\u6027\u201d\u4e4b\u95f4\u8fbe\u5230\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u8868\u73b0\u4f18\u4e8e\u4ee5\u5f80\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u4eba\u53e3\u5408\u6210\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u9ad8\u7ef4\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\uff0c\u5728\u6062\u590d\u771f\u5b9e\u7f3a\u5931\u6570\u636e\u548c\u907f\u514d\u751f\u6210\u65e0\u6548\u6570\u636e\u4e4b\u95f4\u53d6\u5f97\u66f4\u4f18\u5e73\u8861\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684ABM\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u66f4\u771f\u5b9e\u7684\u4eba\u53e3\u6570\u636e\u8f93\u5165\u3002"}}
{"id": "2508.09245", "pdf": "https://arxiv.org/pdf/2508.09245", "abs": "https://arxiv.org/abs/2508.09245", "authors": ["Jeffri Murrugarra-LLerena", "Haoran Niu", "K. Suzanne Barber", "Hal Daum\u00e9 III", "Yang Trista Cao", "Paola Cascante-Bonilla"], "title": "Beyond Blanket Masking: Examining Granularity for Privacy Protection in Images Captured by Blind and Low Vision Users", "categories": ["cs.CV"], "comment": null, "summary": "As visual assistant systems powered by visual language models (VLMs) become\nmore prevalent, concerns over user privacy have grown, particularly for blind\nand low vision users who may unknowingly capture personal private information\nin their images. Existing privacy protection methods rely on coarse-grained\nsegmentation, which uniformly masks entire private objects, often at the cost\nof usability. In this work, we propose FiGPriv, a fine-grained privacy\nprotection framework that selectively masks only high-risk private information\nwhile preserving low-risk information. Our approach integrates fine-grained\nsegmentation with a data-driven risk scoring mechanism. We evaluate our\nframework using the BIV-Priv-Seg dataset and show that FiG-Priv preserves +26%\nof image content, enhancing the ability of VLMs to provide useful responses by\n11% and identify the image content by 45%, while ensuring privacy protection.\nProject Page: https://artcs1.github.io/VLMPrivacy/", "AI": {"tldr": "\u63d0\u51faFiGPriv\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u9690\u79c1\u4fdd\u62a4\u5e73\u8861\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8f85\u52a9\u7cfb\u7edf\u4e2d\u7684\u9690\u79c1\u548c\u53ef\u7528\u6027\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u89c6\u89c9\u8f85\u52a9\u7cfb\u7edf\u65e5\u76ca\u666e\u53ca\uff0c\u5bfc\u81f4\u7528\u6237\u9690\u79c1\u62c5\u5fe7\uff0c\u7279\u522b\u662f\u89c6\u969c\u7528\u6237\u53ef\u80fd\u65e0\u610f\u4e2d\u6355\u83b7\u4e2a\u4eba\u9690\u79c1\u4fe1\u606f\u3002\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u4f9d\u8d56\u7c97\u7c92\u5ea6\u5206\u5272\uff0c\u7edf\u4e00\u906e\u853d\u6574\u4e2a\u79c1\u5bc6\u5bf9\u8c61\uff0c\u727a\u7272\u4e86\u53ef\u7528\u6027\u3002", "method": "\u63d0\u51faFiGPriv\uff0c\u4e00\u4e2a\u7ec6\u7c92\u5ea6\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\u3002\u8be5\u6846\u67b6\u9009\u62e9\u6027\u5730\u906e\u853d\u9ad8\u98ce\u9669\u9690\u79c1\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u7559\u4f4e\u98ce\u9669\u4fe1\u606f\u3002\u5b83\u5c06\u7ec6\u7c92\u5ea6\u5206\u5272\u4e0e\u6570\u636e\u9a71\u52a8\u7684\u98ce\u9669\u8bc4\u5206\u673a\u5236\u76f8\u7ed3\u5408\u3002", "result": "\u4f7f\u7528BIV-Priv-Seg\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0cFiG-Priv\u5728\u786e\u4fdd\u9690\u79c1\u4fdd\u62a4\u7684\u540c\u65f6\uff0c\u4fdd\u7559\u4e86+26%\u7684\u56fe\u50cf\u5185\u5bb9\uff0c\u5c06VLM\u63d0\u4f9b\u6709\u7528\u54cd\u5e94\u7684\u80fd\u529b\u63d0\u9ad8\u4e8611%\uff0c\u56fe\u50cf\u5185\u5bb9\u8bc6\u522b\u7387\u63d0\u9ad8\u4e8645%\u3002", "conclusion": "FiGPriv\u901a\u8fc7\u7ec6\u7c92\u5ea6\u3001\u57fa\u4e8e\u98ce\u9669\u7684\u906e\u853d\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8f85\u52a9\u7cfb\u7edf\u4e2d\u9690\u79c1\u4fdd\u62a4\u4e0e\u53ef\u7528\u6027\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u7cfb\u7edf\u6548\u80fd\u3002"}}
{"id": "2508.09603", "pdf": "https://arxiv.org/pdf/2508.09603", "abs": "https://arxiv.org/abs/2508.09603", "authors": ["Skyler Hallinan", "Jaehun Jung", "Melanie Sclar", "Ximing Lu", "Abhilasha Ravichander", "Sahana Ramnath", "Yejin Choi", "Sai Praneeth Karimireddy", "Niloofar Mireshghallah", "Xiang Ren"], "title": "The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage", "categories": ["cs.CL"], "comment": "CoLM 2025", "summary": "Membership inference attacks serves as useful tool for fair use of language\nmodels, such as detecting potential copyright infringement and auditing data\nleakage. However, many current state-of-the-art attacks require access to\nmodels' hidden states or probability distribution, which prevents investigation\ninto more widely-used, API-access only models like GPT-4. In this work, we\nintroduce N-Gram Coverage Attack, a membership inference attack that relies\nsolely on text outputs from the target model, enabling attacks on completely\nblack-box models. We leverage the observation that models are more likely to\nmemorize and subsequently generate text patterns that were commonly observed in\ntheir training data. Specifically, to make a prediction on a candidate member,\nN-Gram Coverage Attack first obtains multiple model generations conditioned on\na prefix of the candidate. It then uses n-gram overlap metrics to compute and\naggregate the similarities of these outputs with the ground truth suffix; high\nsimilarities indicate likely membership. We first demonstrate on a diverse set\nof existing benchmarks that N-Gram Coverage Attack outperforms other black-box\nmethods while also impressively achieving comparable or even better performance\nto state-of-the-art white-box attacks - despite having access to only text\noutputs. Interestingly, we find that the success rate of our method scales with\nthe attack compute budget - as we increase the number of sequences generated\nfrom the target model conditioned on the prefix, attack performance tends to\nimprove. Having verified the accuracy of our method, we use it to investigate\npreviously unstudied closed OpenAI models on multiple domains. We find that\nmore recent models, such as GPT-4o, exhibit increased robustness to membership\ninference, suggesting an evolving trend toward improved privacy protections.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cN-Gram Coverage Attack\u201d\u7684\u9ed1\u76d2\u6210\u5458\u63a8\u65ad\u653b\u51fb\u65b9\u6cd5\uff0c\u4ec5\u4f9d\u8d56\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u8f93\u51fa\u5373\u53ef\u6709\u6548\u8fd0\u4f5c\uff0c\u751a\u81f3\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u6027\u80fd\u5ab2\u7f8e\u767d\u76d2\u653b\u51fb\uff0c\u5e76\u53d1\u73b0GPT-4o\u7b49\u6700\u65b0\u6a21\u578b\u5bf9\u6210\u5458\u63a8\u65ad\u7684\u9c81\u68d2\u6027\u6709\u6240\u589e\u5f3a\u3002", "motivation": "\u5f53\u524d\u7684\u6210\u5458\u63a8\u65ad\u653b\u51fb\uff08\u7528\u4e8e\u68c0\u6d4b\u7248\u6743\u4fb5\u72af\u548c\u6570\u636e\u6cc4\u9732\uff09\u5927\u591a\u9700\u8981\u8bbf\u95ee\u6a21\u578b\u7684\u9690\u85cf\u72b6\u6001\u6216\u6982\u7387\u5206\u5e03\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u65e0\u6cd5\u5e94\u7528\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u3001\u4ec5\u63d0\u4f9bAPI\u8bbf\u95ee\u7684\u9ed1\u76d2\u6a21\u578b\uff08\u5982GPT-4\uff09\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u4ec5\u4f9d\u8d56\u6587\u672c\u8f93\u51fa\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\u3002", "method": "N-Gram Coverage Attack\u57fa\u4e8e\u6a21\u578b\u66f4\u53ef\u80fd\u751f\u6210\u8bad\u7ec3\u6570\u636e\u4e2d\u5e38\u89c1\u6587\u672c\u6a21\u5f0f\u7684\u89c2\u5bdf\u3002\u5177\u4f53\u65b9\u6cd5\u662f\uff1a\u5bf9\u4e00\u4e2a\u5019\u9009\u6210\u5458\uff0c\u9996\u5148\u83b7\u53d6\u6a21\u578b\u5728\u7ed9\u5b9a\u5019\u9009\u524d\u7f00\u6761\u4ef6\u4e0b\u7684\u591a\u4e2a\u751f\u6210\u6587\u672c\uff1b\u7136\u540e\u4f7f\u7528n-gram\u91cd\u53e0\u5ea6\u91cf\u8ba1\u7b97\u8fd9\u4e9b\u8f93\u51fa\u4e0e\u771f\u5b9e\u540e\u7f00\u7684\u76f8\u4f3c\u5ea6\uff1b\u9ad8\u76f8\u4f3c\u5ea6\u5219\u9884\u793a\u7740\u9ad8\u6210\u5458\u53ef\u80fd\u6027\u3002\u8be5\u65b9\u6cd5\u7684\u6210\u529f\u7387\u4e0e\u653b\u51fb\u8ba1\u7b97\u9884\u7b97\uff08\u5373\u751f\u6210\u7684\u5e8f\u5217\u6570\u91cf\uff09\u5448\u6b63\u76f8\u5173\u3002", "result": "N-Gram Coverage Attack\u5728\u591a\u79cd\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u5176\u4ed6\u9ed1\u76d2\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u4ec5\u80fd\u8bbf\u95ee\u6587\u672c\u8f93\u51fa\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u767d\u76d2\u653b\u51fb\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8be5\u65b9\u6cd5\u7684\u6210\u529f\u7387\u968f\u653b\u51fb\u8ba1\u7b97\u9884\u7b97\u7684\u589e\u52a0\u800c\u63d0\u9ad8\u3002\u6b64\u5916\uff0c\u5229\u7528\u8be5\u65b9\u6cd5\u5bf9OpenAI\u7684\u95ed\u6e90\u6a21\u578b\u8fdb\u884c\u8c03\u67e5\uff0c\u53d1\u73b0GPT-4o\u7b49\u6700\u65b0\u6a21\u578b\u5bf9\u6210\u5458\u63a8\u65ad\u7684\u9c81\u68d2\u6027\u6709\u6240\u589e\u5f3a\u3002", "conclusion": "N-Gram Coverage Attack\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u9ed1\u76d2\u6210\u5458\u63a8\u65ad\u65b9\u6cd5\uff0c\u80fd\u591f\u5bf9\u4ec5\u63d0\u4f9b\u6587\u672c\u8f93\u51fa\u7684\u6a21\u578b\u8fdb\u884c\u9690\u79c1\u8c03\u67e5\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u65b0\u7684\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\uff09\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\uff0c\u8fd9\u53ef\u80fd\u9884\u793a\u7740\u9690\u79c1\u4fdd\u62a4\u7684\u8d8b\u52bf\u6b63\u5728\u6539\u8fdb\u3002"}}
{"id": "2508.09369", "pdf": "https://arxiv.org/pdf/2508.09369", "abs": "https://arxiv.org/abs/2508.09369", "authors": ["Ioannis Panitsas", "Iason Ofeidis", "Leandros Tassiulas"], "title": "On-Device Multimodal Federated Learning for Efficient Jamming Detection", "categories": ["cs.NI"], "comment": null, "summary": "Wireless networks face severe vulnerabilities from jamming attacks, which can\nsignificantly disrupt communication. Existing detection approaches are often\nunimodal, rely on centralized processing, and demand substantial computational\nresources, hindering scalability, efficiency, and deployment feasibility. To\naddress these challenges, we introduce a multimodal Federated Learning (FL)\nframework for on-device jamming detection and classification that integrates\nspectrograms with cross-layer network Key Performance Indicators (KPIs) through\na lightweight dual-encoder architecture equipped with a fusion module and a\nmultimodal projection head. This design enables privacy-preserving training and\ninference by ensuring that only model parameters are exchanged, while raw data\nremains on the device. The framework is implemented and evaluated on a wireless\nexperimental testbed using, to the best of our knowledge, the first\nover-the-air multimodal dataset with synchronized benign and three distinct\njamming scenarios. Results show that our approach surpasses state-of-the-art\nunimodal baselines by up to 15% in detection accuracy, achieves convergence\nwith 60% fewer communication rounds, and maintains low resource usage. Its\nbenefits are most evident under heterogeneous data distributions across\ndevices, where it exhibits strong robustness and reliability.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u65e0\u7ebf\u7f51\u7edc\u5e72\u6270\u653b\u51fb\u68c0\u6d4b\u4e2d\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u7387\u3001\u6269\u5c55\u6027\u53ca\u9690\u79c1\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u591a\u6a21\u6001\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u9891\u8c31\u56fe\u548c\u7f51\u7edcKPI\u5b9e\u73b0\u8bbe\u5907\u7aef\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u9690\u79c1\u4fdd\u62a4\u7684\u5e72\u6270\u68c0\u6d4b\uff0c\u5c24\u5176\u5728\u6570\u636e\u5f02\u6784\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u65e0\u7ebf\u7f51\u7edc\u9762\u4e34\u4e25\u91cd\u7684\u5e72\u6270\u653b\u51fb\u5a01\u80c1\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u662f\u5355\u6a21\u6001\u7684\uff0c\u4f9d\u8d56\u4e2d\u5fc3\u5316\u5904\u7406\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u5176\u53ef\u4f38\u7f29\u6027\u3001\u6548\u7387\u548c\u90e8\u7f72\u53ef\u884c\u6027\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u591a\u6a21\u6001\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u5907\u7aef\u5e72\u6270\u68c0\u6d4b\u548c\u5206\u7c7b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff08\u5305\u542b\u878d\u5408\u6a21\u5757\u548c\u591a\u6a21\u6001\u6295\u5f71\u5934\uff09\uff0c\u96c6\u6210\u9891\u8c31\u56fe\u4e0e\u8de8\u5c42\u7f51\u7edc\u5173\u952e\u6027\u80fd\u6307\u6807\uff08KPIs\uff09\u3002\u6b64\u8bbe\u8ba1\u786e\u4fdd\u539f\u59cb\u6570\u636e\u7559\u5728\u8bbe\u5907\u4e0a\uff0c\u4ec5\u4ea4\u6362\u6a21\u578b\u53c2\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u3002\u6846\u67b6\u5728\u4e00\u4e2a\u65e0\u7ebf\u5b9e\u9a8c\u6d4b\u8bd5\u53f0\u4e0a\uff0c\u4f7f\u7528\u9996\u4e2a\u8fc7\u7a7a\uff08over-the-air\uff09\u540c\u6b65\u591a\u6a21\u6001\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u73b0\u548c\u8bc4\u4f30\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u4e0a\u6bd4\u73b0\u6709\u5355\u6a21\u6001\u57fa\u7ebf\u9ad8\u51fa15%\uff1b\u901a\u4fe1\u8f6e\u6b21\u51cf\u5c1160%\u5373\u53ef\u5b9e\u73b0\u6536\u655b\uff1b\u8d44\u6e90\u5360\u7528\u8f83\u4f4e\u3002\u5728\u8bbe\u5907\u95f4\u6570\u636e\u5f02\u6784\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u4f18\u52bf\u6700\u4e3a\u660e\u663e\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u591a\u6a21\u6001\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u65e0\u7ebf\u7f51\u7edc\u5e72\u6270\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u5f02\u6784\u6570\u636e\u73af\u5883\u4e0b\u8868\u73b0\u5353\u8d8a\uff0c\u5e76\u6709\u6548\u89e3\u51b3\u4e86\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u4e3a\u8bbe\u5907\u7aef\u5e72\u6270\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05884", "pdf": "https://arxiv.org/pdf/2508.05884", "abs": "https://arxiv.org/abs/2508.05884", "authors": ["Peigen Ye", "Jingpu Duan", "Hongyang Du", "Yulan Guo"], "title": "User-Intent-Driven Semantic Communication via Adaptive Deep Understanding", "categories": ["cs.IT", "cs.AI", "math.IT"], "comment": "300 *^_^* IEEE Globecom 2025", "summary": "Semantic communication focuses on transmitting task-relevant semantic\ninformation, aiming for intent-oriented communication. While existing systems\nimprove efficiency by extracting key semantics, they still fail to deeply\nunderstand and generalize users' real intentions. To overcome this, we propose\na user-intention-driven semantic communication system that interprets diverse\nabstract intents. First, we integrate a multi-modal large model as semantic\nknowledge base to generate user-intention prior. Next, a mask-guided attention\nmodule is proposed to effectively highlight critical semantic regions. Further,\na channel state awareness module ensures adaptive, robust transmission across\nvarying channel conditions. Extensive experiments demonstrate that our system\nachieves deep intent understanding and outperforms DeepJSCC, e.g., under a\nRayleigh channel at an SNR of 5 dB, it achieves improvements of 8%, 6%, and 19%\nin PSNR, SSIM, and LPIPS, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7528\u6237\u610f\u56fe\u9a71\u52a8\u7684\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\uff0c\u65e8\u5728\u514b\u670d\u73b0\u6709\u7cfb\u7edf\u5728\u6df1\u5ea6\u7406\u89e3\u7528\u6237\u771f\u5b9e\u610f\u56fe\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u867d\u80fd\u901a\u8fc7\u63d0\u53d6\u5173\u952e\u8bed\u4e49\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u672a\u80fd\u6df1\u5165\u7406\u89e3\u548c\u6cdb\u5316\u7528\u6237\u7684\u771f\u5b9e\u610f\u56fe\uff0c\u9650\u5236\u4e86\u610f\u56fe\u5bfc\u5411\u901a\u4fe1\u7684\u5b9e\u73b0\u3002", "method": "\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\u4f5c\u4e3a\u8bed\u4e49\u77e5\u8bc6\u5e93\u4ee5\u751f\u6210\u7528\u6237\u610f\u56fe\u5148\u9a8c\uff1b\u63d0\u51fa\u63a9\u7801\u5f15\u5bfc\u6ce8\u610f\u529b\u6a21\u5757\u4ee5\u7a81\u51fa\u5173\u952e\u8bed\u4e49\u533a\u57df\uff1b\u5e76\u5305\u542b\u4fe1\u9053\u72b6\u6001\u611f\u77e5\u6a21\u5757\u4ee5\u786e\u4fdd\u5728\u4e0d\u540c\u4fe1\u9053\u6761\u4ef6\u4e0b\u7684\u81ea\u9002\u5e94\u548c\u9c81\u68d2\u4f20\u8f93\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u5b9e\u73b0\u5bf9\u610f\u56fe\u7684\u6df1\u5ea6\u7406\u89e3\uff0c\u5e76\u4e14\u6027\u80fd\u4f18\u4e8eDeepJSCC\u3002\u4f8b\u5982\uff0c\u5728\u745e\u5229\u4fe1\u9053\u548c5dB\u4fe1\u566a\u6bd4\u4e0b\uff0cPSNR\u3001SSIM\u548cLPIPS\u5206\u522b\u63d0\u5347\u4e868%\u30016%\u548c19%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7528\u6237\u610f\u56fe\u9a71\u52a8\u8bed\u4e49\u901a\u4fe1\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u7406\u89e3\u548c\u89e3\u91ca\u62bd\u8c61\u610f\u56fe\uff0c\u5e76\u5728\u591a\u79cd\u4fe1\u9053\u6761\u4ef6\u4e0b\u5b9e\u73b0\u7a33\u5065\u4f20\u8f93\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.09165", "pdf": "https://arxiv.org/pdf/2508.09165", "abs": "https://arxiv.org/abs/2508.09165", "authors": ["Shanwei Zhang", "Deyun Zhang", "Yirao Tao", "Kexin Wang", "Shijia Geng", "Jun Li", "Qinghao Zhao", "Xingpeng Liu", "Yuxi Zhou", "Shenda Hong"], "title": "Masked Training for Robust Arrhythmia Detection from Digitalized Multiple Layout ECG Images", "categories": ["cs.LG", "cs.CV"], "comment": "18 pages, 6 figures", "summary": "Electrocardiogram (ECG) as an important tool for diagnosing cardiovascular\ndiseases such as arrhythmia. Due to the differences in ECG layouts used by\ndifferent hospitals, the digitized signals exhibit asynchronous lead time and\npartial blackout loss, which poses a serious challenge to existing models. To\naddress this challenge, the study introduced PatchECG, a framework for adaptive\nvariable block count missing representation learning based on a masking\ntraining strategy, which automatically focuses on key patches with\ncollaborative dependencies between leads, thereby achieving key recognition of\narrhythmia in ECGs with different layouts. Experiments were conducted on the\nPTB-XL dataset and 21388 asynchronous ECG images generated using ECG image kit\ntool, using the 23 Subclasses as labels. The proposed method demonstrated\nstrong robustness under different layouts, with average Area Under the Receiver\nOperating Characteristic Curve (AUROC) of 0.835 and remained stable (unchanged\nwith layout changes). In external validation based on 400 real ECG images data\nfrom Chaoyang Hospital, the AUROC for atrial fibrillation diagnosis reached\n0.778; On 12 x 1 layout ECGs, AUROC reaches 0.893. This result is superior to\nvarious classic interpolation and baseline methods, and compared to the current\noptimal large-scale pre-training model ECGFounder, it has improved by 0.111 and\n0.19.", "AI": {"tldr": "\u9488\u5bf9ECG\u56e0\u5e03\u5c40\u5dee\u5f02\u5bfc\u81f4\u7684\u6570\u636e\u5f02\u6b65\u548c\u7f3a\u5931\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51faPatchECG\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u63a9\u7801\u8bad\u7ec3\u548c\u81ea\u9002\u5e94\u7f3a\u5931\u8868\u793a\u5b66\u4e60\uff0c\u6709\u6548\u8bc6\u522b\u4e0d\u540c\u5e03\u5c40ECG\u4e2d\u7684\u5fc3\u5f8b\u5931\u5e38\uff0c\u5e76\u5728\u9c81\u68d2\u6027\u548c\u6027\u80fd\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709ECG\u8bca\u65ad\u6a21\u578b\u96be\u4ee5\u5e94\u5bf9\u56e0\u4e0d\u540c\u533b\u9662ECG\u5e03\u5c40\u5dee\u5f02\u9020\u6210\u7684\u6570\u5b57\u5316\u4fe1\u53f7\u5bfc\u8054\u65f6\u95f4\u5f02\u6b65\u548c\u90e8\u5206\u7f3a\u5931\uff0c\u8fd9\u4e25\u91cd\u6311\u6218\u4e86\u5176\u8bca\u65ad\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51faPatchECG\u6846\u67b6\uff0c\u91c7\u7528\u57fa\u4e8e\u63a9\u7801\u8bad\u7ec3\u7b56\u7565\u7684\u81ea\u9002\u5e94\u53ef\u53d8\u5757\u8ba1\u6570\u7f3a\u5931\u8868\u793a\u5b66\u4e60\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u52a8\u5173\u6ce8\u5bfc\u8054\u95f4\u5177\u6709\u534f\u4f5c\u4f9d\u8d56\u7684\u5173\u952e\u8865\u4e01\uff0c\u4ee5\u5b9e\u73b0\u5728\u4e0d\u540c\u5e03\u5c40ECG\u4e2d\u5bf9\u5fc3\u5f8b\u5931\u5e38\u7684\u5173\u952e\u8bc6\u522b\u3002\u5b9e\u9a8c\u5728PTB-XL\u6570\u636e\u96c6\u548c21388\u5f20\u901a\u8fc7ECG\u56fe\u50cf\u5de5\u5177\u751f\u6210\u7684\u5f02\u6b65ECG\u56fe\u50cf\u4e0a\u8fdb\u884c\uff0c\u5e76\u4ee523\u4e2a\u5b50\u7c7b\u522b\u4f5c\u4e3a\u6807\u7b7e\u3002", "result": "PatchECG\u5728\u4e0d\u540c\u5e03\u5c40\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u5e73\u5747AUROC\u8fbe0.835\uff0c\u4e14\u6027\u80fd\u7a33\u5b9a\u4e0d\u53d7\u5e03\u5c40\u53d8\u5316\u5f71\u54cd\u3002\u5728\u671d\u9633\u533b\u9662400\u5f20\u771f\u5b9eECG\u56fe\u50cf\u7684\u5916\u90e8\u9a8c\u8bc1\u4e2d\uff0c\u623f\u98a4\u8bca\u65adAUROC\u8fbe0.778\uff1b\u572812x1\u5e03\u5c40ECG\u4e0a\uff0cAUROC\u8fbe0.893\u3002\u8be5\u7ed3\u679c\u4f18\u4e8e\u5404\u79cd\u7ecf\u5178\u63d2\u503c\u548c\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u76f8\u8f83\u4e8e\u5f53\u524d\u6700\u4f18\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578bECGFounder\uff0cAUROC\u5206\u522b\u63d0\u5347\u4e860.111\u548c0.19\u3002", "conclusion": "PatchECG\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86ECG\u5e03\u5c40\u5dee\u5f02\u5e26\u6765\u7684\u6570\u636e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e0d\u540c\u5e03\u5c40ECG\u4e2d\u5fc3\u5f8b\u5931\u5e38\u7684\u9c81\u68d2\u3001\u51c6\u786e\u8bc6\u522b\uff0c\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.09262", "pdf": "https://arxiv.org/pdf/2508.09262", "abs": "https://arxiv.org/abs/2508.09262", "authors": ["Dongwoo Kang", "Akhil Perincherry", "Zachary Coalson", "Aiden Gabriel", "Stefan Lee", "Sanghyun Hong"], "title": "Harnessing Input-Adaptive Inference for Efficient VLN", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ICCV 2025 [Poster]", "summary": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f93\u5165\u81ea\u9002\u5e94\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u7a7a\u95f4\u3001\u6a21\u578b\u5185\u90e8\u548c\u65f6\u95f4\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5386\u53f2\u611f\u77e5\u591a\u6a21\u6001Transformer\u6a21\u578b\u5728VLN\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5e9e\u5927\u7684\u6a21\u578b\u89c4\u6a21\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u6784\u6210\u74f6\u9888\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8f93\u5165\u81ea\u9002\u5e94\u5bfc\u822a\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8VLN\u6a21\u578b\u7684\u6548\u7387\u3002\u5177\u4f53\u5305\u62ec\u4e09\u4e2a\u5c42\u9762\u7684\u81ea\u9002\u5e94\u7b97\u6cd5\uff1a1) \u63d0\u5347\u7a7a\u95f4\u6548\u7387\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5904\u7406\u4ee3\u7406\u5728\u6bcf\u6b21\u89c2\u5bdf\u65f6\u7684\u5168\u666f\u89c6\u56fe\uff1b2) \u63d0\u5347\u6a21\u578b\u5185\u90e8\u6548\u7387\uff0c\u901a\u8fc7\u4e3a\u63d0\u524d\u9000\u51fa\u65b9\u6cd5\u5f15\u5165\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u81ea\u9002\u5e94\u9608\u503c\uff1b3) \u63d0\u5347\u65f6\u95f4\u6548\u7387\uff0c\u901a\u8fc7\u5b9e\u65bd\u7f13\u5b58\u673a\u5236\u4ee5\u907f\u514d\u91cd\u590d\u5904\u7406\u5df2\u89c2\u5bdf\u8fc7\u7684\u89c6\u56fe\u3002", "result": "\u5728\u4e03\u4e2aVLN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5bf9\u4e09\u79cd\u73b0\u6709\u4ee3\u7406\u5728\u6807\u51c6\u548c\u8fde\u7eed\u73af\u5883\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u8ba1\u7b97\u91cf\u51cf\u5c11\u4e86\u4e24\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f93\u5165\u81ea\u9002\u5e94\u5bfc\u822a\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8VLN\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2508.09622", "pdf": "https://arxiv.org/pdf/2508.09622", "abs": "https://arxiv.org/abs/2508.09622", "authors": ["Tatiana Batura", "Elena Bruches", "Milana Shvenk", "Valentin Malykh"], "title": "AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian", "categories": ["cs.CL"], "comment": "AINL 2025 Conference", "summary": "The rapid advancement of large language models (LLMs) has revolutionized text\ngeneration, making it increasingly difficult to distinguish between human- and\nAI-generated content. This poses a significant challenge to academic integrity,\nparticularly in scientific publishing and multilingual contexts where detection\nresources are often limited. To address this critical gap, we introduce the\nAINL-Eval 2025 Shared Task, specifically focused on the detection of\nAI-generated scientific abstracts in Russian. We present a novel, large-scale\ndataset comprising 52,305 samples, including human-written abstracts across 12\ndiverse scientific domains and AI-generated counterparts from five\nstate-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and\nGigaChat-Lite). A core objective of the task is to challenge participants to\ndevelop robust solutions capable of generalizing to both (i) previously unseen\nscientific domains and (ii) models not included in the training data. The task\nwas organized in two phases, attracting 10 teams and 159 submissions, with top\nsystems demonstrating strong performance in identifying AI-generated content.\nWe also establish a continuous shared task platform to foster ongoing research\nand long-term progress in this important area. The dataset and platform are\npublicly available at https://github.com/iis-research-team/AINL-Eval-2025.", "AI": {"tldr": "\u4e00\u9879\u5173\u4e8e\u68c0\u6d4b\u4fc4\u8bedAI\u751f\u6210\u79d1\u5b66\u6458\u8981\u7684\u5171\u4eab\u4efb\u52a1\uff0c\u5efa\u7acb\u4e86\u5927\u578b\u6570\u636e\u96c6\uff0c\u5e76\u53d6\u5f97\u4e86\u79ef\u6781\u7684\u68c0\u6d4b\u7ed3\u679c\uff0c\u65e8\u5728\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6301\u7eed\u7814\u7a76\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u6b65\u5bfc\u81f4\u4eba\u5de5\u4e0eAI\u751f\u6210\u5185\u5bb9\u96be\u4ee5\u533a\u5206\uff0c\u5bf9\u5b66\u672f\u8bda\u4fe1\u6784\u6210\u6311\u6218\uff0c\u5c24\u5176\u5728\u79d1\u5b66\u51fa\u7248\u548c\u591a\u8bed\u8a00\u73af\u5883\u4e2d\uff0c\u68c0\u6d4b\u8d44\u6e90\u7a00\u7f3a\u3002", "method": "\u5f15\u5165AINL-Eval 2025\u5171\u4eab\u4efb\u52a1\uff0c\u4e13\u6ce8\u4e8e\u68c0\u6d4b\u4fc4\u8bedAI\u751f\u6210\u7684\u79d1\u5b66\u6458\u8981\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b52,305\u4e2a\u6837\u672c\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u6db5\u76d612\u4e2a\u79d1\u5b66\u9886\u57df\u7684\u4eba\u5de5\u64b0\u5199\u6458\u8981\u548c\u6765\u81ea\u4e94\u79cd\u4e3b\u6d41LLM\uff08\u5982GPT-4-Turbo, Llama3.3-70B\u7b49\uff09\u751f\u6210\u7684AI\u6458\u8981\u3002\u4efb\u52a1\u6311\u6218\u53c2\u4e0e\u8005\u5f00\u53d1\u53ef\u6cdb\u5316\u5230\u65b0\u9886\u57df\u548c\u672a\u89c1\u6a21\u578b\u7684\u89e3\u51b3\u65b9\u6848\u3002\u4efb\u52a1\u5206\u4e24\u9636\u6bb5\u8fdb\u884c\uff0c\u5438\u5f15\u4e8610\u4e2a\u56e2\u961f\u548c159\u4efd\u63d0\u4ea4\u3002", "result": "\u9876\u7ea7\u7cfb\u7edf\u5728\u8bc6\u522bAI\u751f\u6210\u5185\u5bb9\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u4efb\u52a1\u6210\u529f\u63a8\u52a8\u4e86AI\u751f\u6210\u5185\u5bb9\u68c0\u6d4b\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5e76\u5efa\u7acb\u4e86\u6301\u7eed\u7684\u5171\u4eab\u4efb\u52a1\u5e73\u53f0\u548c\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u957f\u671f\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2508.09573", "pdf": "https://arxiv.org/pdf/2508.09573", "abs": "https://arxiv.org/abs/2508.09573", "authors": ["Micha\u0142 Rzepka", "Piotr Cho\u0142da"], "title": "Metrics for Assessing Changes in Flow-based Networks", "categories": ["cs.NI", "cs.PF"], "comment": null, "summary": "This paper addresses the challenges of evaluating network performance in the\npresence of fluctuating traffic patterns, with a particular focus on the impact\nof peak data rates on network resources. We introduce a set of metrics to\nquantify network load and measure the impact of individual flows on the overall\nnetwork state. By analyzing link and flow data through percentile values and\nsample distributions, and introducing the Utilization Score metric, the\nresearch provides insights into resource utilization under varying network\nconditions. Furthermore, we employ a modified Shapley value-based approach to\nmeasure the influence of individual flows on the network, offering a better\nunderstanding of their contribution to network performance. The paper reviews\nand compares 11 metrics across various network scenarios, evaluating their\npractical relevance for research and development. Our evaluation demonstrates\nthat these metrics effectively capture changes in network state induced by\nspecific flows, with three of them offering a broad range of valuable insights\nwhile remaining relatively easy to maintain. Moreover, the methodology\ndescribed in this paper serves as a framework for future research, with the\npotential to expand and refine the set of metrics used to evaluate flow impact\non network performance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u6ce2\u52a8\u6d41\u91cf\u4e0b\u7f51\u7edc\u6027\u80fd\u8bc4\u4f30\uff0c\u5c24\u5176\u5173\u6ce8\u5cf0\u503c\u901f\u7387\u5bf9\u7f51\u7edc\u8d44\u6e90\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5f15\u5165\u65b0\u6307\u6807\uff08\u5982Utilization Score\uff09\u548c\u6539\u8fdb\u7684Shapley\u503c\u65b9\u6cd5\uff0c\u91cf\u5316\u5355\u4e2a\u6d41\u7684\u5f71\u54cd\u3002\u8bc4\u4f30\u5e76\u6bd4\u8f83\u4e8611\u4e2a\u6307\u6807\uff0c\u53d1\u73b0\u90e8\u5206\u6307\u6807\u9ad8\u6548\u4e14\u6613\u4e8e\u7ef4\u62a4\u3002", "motivation": "\u89e3\u51b3\u5728\u6d41\u91cf\u6a21\u5f0f\u6ce2\u52a8\uff08\u7279\u522b\u662f\u5cf0\u503c\u6570\u636e\u901f\u7387\uff09\u4e0b\u7f51\u7edc\u6027\u80fd\u8bc4\u4f30\u7684\u6311\u6218\uff0c\u5e76\u91cf\u5316\u5355\u4e2a\u6d41\u5bf9\u6574\u4f53\u7f51\u7edc\u72b6\u6001\u7684\u5f71\u54cd\u3002", "method": "\u5f15\u5165\u4e00\u5957\u65b0\u6307\u6807\uff08\u5305\u62ecUtilization Score\uff09\u4ee5\u91cf\u5316\u7f51\u7edc\u8d1f\u8f7d\u548c\u6d4b\u91cf\u5355\u4e2a\u6d41\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u767e\u5206\u4f4d\u6570\u548c\u6837\u672c\u5206\u5e03\u5206\u6790\u94fe\u8def\u548c\u6d41\u6570\u636e\u3002\u91c7\u7528\u6539\u8fdb\u7684\u57fa\u4e8eShapley\u503c\u7684\u65b9\u6cd5\u6d4b\u91cf\u5355\u4e2a\u6d41\u5bf9\u7f51\u7edc\u7684\u5f71\u54cd\u3002\u8bc4\u5ba1\u5e76\u6bd4\u8f83\u4e8611\u4e2a\u6307\u6807\u5728\u5404\u79cd\u7f51\u7edc\u573a\u666f\u4e0b\u7684\u5b9e\u9645\u76f8\u5173\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6307\u6807\u80fd\u6709\u6548\u6355\u6349\u7279\u5b9a\u6d41\u5f15\u8d77\u7684\u7f51\u7edc\u72b6\u6001\u53d8\u5316\u3002\u5176\u4e2d\uff0c\u6709\u4e09\u4e2a\u6307\u6807\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u4e14\u76f8\u5bf9\u6613\u4e8e\u7ef4\u62a4\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u6709\u671b\u6269\u5c55\u548c\u5b8c\u5584\u8bc4\u4f30\u6d41\u5bf9\u7f51\u7edc\u6027\u80fd\u5f71\u54cd\u7684\u6307\u6807\u96c6\u3002\u8fd9\u4e9b\u6307\u6807\u6709\u6548\u5730\u89e3\u51b3\u4e86\u6ce2\u52a8\u6d41\u91cf\u4e0b\u7f51\u7edc\u6027\u80fd\u8bc4\u4f30\u7684\u6311\u6218\uff0c\u5e76\u6df1\u5165\u7406\u89e3\u4e86\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2508.09142", "pdf": "https://arxiv.org/pdf/2508.09142", "abs": "https://arxiv.org/abs/2508.09142", "authors": ["Wenlihan Lu", "Shijian Gao", "Miaowen Wen", "Yuxuan Liang", "Chan-Byoung Chae", "H. Vincent Poor"], "title": "Bayesian-Driven Graph Reasoning for Active Radio Map Construction", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "With the emergence of the low-altitude economy, radio maps have become\nessential for ensuring reliable wireless connectivity to aerial platforms.\nAutonomous aerial agents are commonly deployed for data collection using\nwaypoint-based navigation; however, their limited battery capacity\nsignificantly constrains coverage and efficiency. To address this, we propose\nan uncertainty-aware radio map (URAM) reconstruction framework that explicitly\nleverages graph-based reasoning tailored for waypoint navigation. Our approach\nintegrates two key deep learning components: (1) a Bayesian neural network that\nestimates spatial uncertainty in real time, and (2) an attention-based\nreinforcement learning policy that performs global reasoning over a\nprobabilistic roadmap, using uncertainty estimates to plan informative and\nenergy-efficient trajectories. This graph-based reasoning enables intelligent,\nnon-myopic trajectory planning, guiding agents toward the most informative\nregions while satisfying safety constraints. Experimental results show that\nURAM improves reconstruction accuracy by up to 34% over existing baselines.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u4f4e\u7a7a\u7ecf\u6d4e\u4e2d\u65e0\u4eba\u673a\u65e0\u7ebf\u7535\u5730\u56fe\u6784\u5efa\u53d7\u7535\u6c60\u9650\u5236\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faURAM\u6846\u67b6\uff0c\u5229\u7528\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u4f30\u7b97\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u56fe\u57fa\u63a8\u7406\uff0c\u89c4\u5212\u9ad8\u6548\u8f68\u8ff9\uff0c\u5c06\u91cd\u5efa\u7cbe\u5ea6\u63d0\u9ad834%\u3002", "motivation": "\u968f\u7740\u4f4e\u7a7a\u7ecf\u6d4e\u5174\u8d77\uff0c\u65e0\u7ebf\u7535\u5730\u56fe\u5bf9\u7a7a\u4e2d\u5e73\u53f0\u65e0\u7ebf\u8fde\u63a5\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u81ea\u4e3b\u7a7a\u4e2d\u4ee3\u7406\u5728\u6570\u636e\u6536\u96c6\u65f6\uff0c\u6709\u9650\u7684\u7535\u6c60\u5bb9\u91cf\u4e25\u91cd\u9650\u5236\u4e86\u8986\u76d6\u8303\u56f4\u548c\u6548\u7387\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u65e0\u7ebf\u7535\u5730\u56fe\uff08URAM\uff09\u91cd\u5efa\u6846\u67b6\uff0c\u4e13\u95e8\u4e3a\u822a\u70b9\u5bfc\u822a\u91cf\u8eab\u5b9a\u5236\u56fe\u57fa\u63a8\u7406\u3002\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u4e24\u4e2a\u6df1\u5ea6\u5b66\u4e60\u7ec4\u4ef6\uff1a1) \u4e00\u4e2a\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u7528\u4e8e\u5b9e\u65f6\u4f30\u8ba1\u7a7a\u95f4\u4e0d\u786e\u5b9a\u6027\uff1b2) \u4e00\u4e2a\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5bf9\u6982\u7387\u8def\u7ebf\u56fe\u8fdb\u884c\u5168\u5c40\u63a8\u7406\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u89c4\u5212\u4fe1\u606f\u4e30\u5bcc\u4e14\u8282\u80fd\u7684\u8f68\u8ff9\u3002\u56fe\u57fa\u63a8\u7406\u5b9e\u73b0\u4e86\u667a\u80fd\u3001\u975e\u77ed\u89c6\u7684\u8f68\u8ff9\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cURAM\u5c06\u91cd\u5efa\u7cbe\u5ea6\u6bd4\u73b0\u6709\u57fa\u7ebf\u63d0\u9ad8\u4e8634%\u3002", "conclusion": "URAM\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u56fe\u57fa\u63a8\u7406\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u65e0\u7ebf\u7535\u5730\u56fe\u7684\u91cd\u5efa\u7cbe\u5ea6\uff0c\u4e3a\u4f4e\u7a7a\u7ecf\u6d4e\u4e2d\u7684\u65e0\u7ebf\u8fde\u63a5\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u9ad8\u6548\u7684\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u3002"}}
{"id": "2508.09168", "pdf": "https://arxiv.org/pdf/2508.09168", "abs": "https://arxiv.org/abs/2508.09168", "authors": ["Feiyu Wang", "Zhiyuan Zhao", "Yuandong Liu", "Da Zhang", "Junyu Gao", "Hao Sun", "Xuelong Li"], "title": "SVGen: Interpretable Vector Graphics Generation with Large Language Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Scalable Vector Graphics (SVG) is widely used in front-end development and\nUI/UX design due to its scalability, editability, and rendering efficiency.\nHowever, turning creative ideas into precise vector graphics remains a\ntime-consuming challenge. To address this, we introduce SVG-1M, a large-scale\ndataset of high-quality SVGs paired with natural language descriptions. Through\nadvanced data augmentation and annotation, we create well-aligned Text to SVG\ntraining pairs, including a subset with Chain of Thought annotations for\nenhanced semantic guidance. Based on this dataset, we propose SVGen, an\nend-to-end model that generates SVG code from natural language inputs. Our\napproach ensures semantic accuracy and structural completeness, supported by\ncurriculum learning and reinforcement learning optimization. Experiments show\nthat SVGen outperforms general large models and traditional rendering methods\nin both effectiveness and efficiency. Code, model, and dataset are available on\nGitHub.", "AI": {"tldr": "\u9488\u5bf9\u5c06\u521b\u610f\u8f6c\u5316\u4e3aSVG\u56fe\u5f62\u8017\u65f6\u7684\u95ee\u9898\uff0c\u8bba\u6587\u5f15\u5165\u4e86\u5927\u89c4\u6a21SVG-1M\u6570\u636e\u96c6\u53caSVGen\u6a21\u578b\uff0c\u5b9e\u73b0\u4ece\u81ea\u7136\u8bed\u8a00\u9ad8\u6548\u751f\u6210SVG\u4ee3\u7801\uff0c\u4e14\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5c06\u521b\u610f\u60f3\u6cd5\u8f6c\u5316\u4e3a\u7cbe\u786e\u7684\u77e2\u91cf\u56fe\u5f62\uff08\u7279\u522b\u662fSVG\uff09\u662f\u4e00\u4e2a\u8017\u65f6\u4e14\u5177\u6311\u6218\u6027\u7684\u8fc7\u7a0b\u3002", "method": "1. \u6784\u5efa\u4e86\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u7684SVG-1M\u6570\u636e\u96c6\uff0c\u5305\u542b\u914d\u5bf9\u7684SVG\u4e0e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u5e76\u901a\u8fc7\u9ad8\u7ea7\u6570\u636e\u589e\u5f3a\u548c\u601d\u7ef4\u94fe\uff08Chain of Thought\uff09\u6ce8\u91ca\u8fdb\u884c\u5bf9\u9f50\u30022. \u57fa\u4e8e\u6b64\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u6a21\u578bSVGen\uff0c\u7528\u4e8e\u4ece\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u751f\u6210SVG\u4ee3\u7801\u30023. \u8be5\u65b9\u6cd5\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u4ee5\u786e\u4fdd\u8bed\u4e49\u51c6\u786e\u6027\u548c\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSVGen\u5728\u6709\u6548\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u901a\u7528\u5927\u578b\u6a21\u578b\u548c\u4f20\u7edf\u6e32\u67d3\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u63d0\u51fa\u521b\u65b0\u7684SVG-1M\u6570\u636e\u96c6\u548cSVGen\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u9ad8\u8d28\u91cfSVG\u7684\u6311\u6218\uff0c\u5e76\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.09325", "pdf": "https://arxiv.org/pdf/2508.09325", "abs": "https://arxiv.org/abs/2508.09325", "authors": ["Alexandre Brown", "Glen Berseth"], "title": "SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Visual reinforcement learning (RL) is challenging due to the need to learn\nboth perception and actions from high-dimensional inputs and noisy rewards.\nAlthough large perception models exist, integrating them effectively into RL\nfor visual generalization and improved sample efficiency remains unclear. We\npropose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment\nAnything (SAM) for object-centric decomposition and YOLO-World to ground\nsegments semantically via text prompts. It includes a novel transformer-based\narchitecture that supports a dynamic number of segments at each time step and\neffectively learns which segments to focus on using online RL, without using\nhuman labels. By evaluating SegDAC over a challenging visual generalization\nbenchmark using Maniskill3, which covers diverse manipulation tasks under\nstrong visual perturbations, we demonstrate that SegDAC achieves significantly\nbetter visual generalization, doubling prior performance on the hardest setting\nand matching or surpassing prior methods in sample efficiency across all\nevaluated tasks.", "AI": {"tldr": "SegDAC\u662f\u4e00\u79cd\u9762\u5411\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u7ed3\u5408\u4e86SAM\u548cYOLO-World\u8fdb\u884c\u5bf9\u8c61\u5206\u89e3\u4e0e\u8bed\u4e49\u5b9a\u4f4d\uff0c\u5e76\u91c7\u7528\u65b0\u9896\u7684Transformer\u67b6\u6784\uff0c\u5728\u4e0d\u4f7f\u7528\u4eba\u5de5\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5b66\u4e60\u5173\u6ce8\u5173\u952e\u7247\u6bb5\u3002\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u6cdb\u5316\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u4e86\u6837\u672c\u6548\u7387\u3002", "motivation": "\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u4ece\u9ad8\u7ef4\u8f93\u5165\u548c\u566a\u58f0\u5956\u52b1\u4e2d\u5b66\u4e60\u611f\u77e5\u548c\u52a8\u4f5c\u7684\u6311\u6218\u3002\u5c3d\u7ba1\u5b58\u5728\u5927\u578b\u611f\u77e5\u6a21\u578b\uff0c\u4f46\u5982\u4f55\u6709\u6548\u5730\u5c06\u5176\u6574\u5408\u5230\u5f3a\u5316\u5b66\u4e60\u4e2d\u4ee5\u5b9e\u73b0\u89c6\u89c9\u6cdb\u5316\u548c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u4ecd\u4e0d\u660e\u786e\u3002", "method": "\u672c\u6587\u63d0\u51faSegDAC\uff08Segmentation-Driven Actor-Critic\uff09\u65b9\u6cd5\u3002\u5b83\u5229\u7528Segment Anything (SAM) \u8fdb\u884c\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u5206\u89e3\uff0c\u5e76\u4f7f\u7528YOLO-World\u901a\u8fc7\u6587\u672c\u63d0\u793a\u5bf9\u8bed\u4e49\u7247\u6bb5\u8fdb\u884c\u5b9a\u4f4d\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u652f\u6301\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u52a8\u6001\u7247\u6bb5\u6570\u91cf\uff0c\u5e76\u901a\u8fc7\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u5b66\u4e60\u5173\u6ce8\u54ea\u4e9b\u7247\u6bb5\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002", "result": "SegDAC\u5728Maniskill3\u4e0a\u7684\u6311\u6218\u6027\u89c6\u89c9\u6cdb\u5316\u57fa\u51c6\uff08\u5305\u542b\u5f3a\u89c6\u89c9\u6270\u52a8\u4e0b\u7684\u591a\u79cd\u64cd\u4f5c\u4efb\u52a1\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5b9e\u73b0\u4e86\u663e\u8457\u4f18\u8d8a\u7684\u89c6\u89c9\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u6700\u56f0\u96be\u7684\u8bbe\u7f6e\u4e0b\u5c06\u5148\u524d\u6027\u80fd\u63d0\u9ad8\u4e86\u4e00\u500d\uff0c\u5e76\u4e14\u5728\u6240\u6709\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6837\u672c\u6548\u7387\u3002", "conclusion": "SegDAC\u901a\u8fc7\u6709\u6548\u5730\u6574\u5408\u5927\u578b\u611f\u77e5\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u7684\u89c6\u89c9\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u89c9\u6270\u52a8\u573a\u666f\u4e0b\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.09654", "pdf": "https://arxiv.org/pdf/2508.09654", "abs": "https://arxiv.org/abs/2508.09654", "authors": ["Alexandre Verine", "Florian Le Bronnec", "Kunhao Zheng", "Alexandre Allauzen", "Yann Chevaleyre", "Benjamin Negrevergne"], "title": "Improving Diversity in Language Models: When Temperature Fails, Change the Loss", "categories": ["cs.CL", "cs.LG"], "comment": "Forty-Second International Conference on Machine Learning, ICML2025", "summary": "Increasing diversity in language models is a challenging yet essential\nobjective. A common approach is to raise the decoding temperature. In this\nwork, we investigate this approach through a simplistic yet common case to\nprovide insights into why decreasing temperature can improve quality\n(Precision), while increasing it often fails to boost coverage (Recall). Our\nanalysis reveals that for a model to be effectively tunable through temperature\nadjustments, it must be trained toward coverage. To address this, we propose\nrethinking loss functions in language models by leveraging the Precision-Recall\nframework. Our results demonstrate that this approach achieves a substantially\nbetter trade-off between Precision and Recall than merely combining negative\nlog-likelihood training with temperature scaling. These findings offer a\npathway toward more versatile and robust language modeling techniques.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u89e3\u7801\u6e29\u5ea6\u5bf9\u8bed\u8a00\u6a21\u578b\u591a\u6837\u6027\uff08\u7cbe\u786e\u7387/\u53ec\u56de\u7387\uff09\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u7cbe\u786e\u7387-\u53ec\u56de\u7387\u6846\u67b6\u91cd\u65b0\u8bbe\u8ba1\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u7cbe\u5ea6\u4e0e\u53ec\u56de\u6743\u8861\u3002", "motivation": "\u589e\u52a0\u8bed\u8a00\u6a21\u578b\u591a\u6837\u6027\u662f\u4e00\u4e2a\u6311\u6218\u6027\u76ee\u6807\uff0c\u5e38\u89c1\u7684\u89e3\u7801\u6e29\u5ea6\u8c03\u6574\u65b9\u6cd5\u5728\u63d0\u5347\u53ec\u56de\u7387\u4e0a\u6548\u679c\u4e0d\u4f73\u3002\u7814\u7a76\u65e8\u5728\u6df1\u5165\u7406\u89e3\u4e3a\u4f55\u964d\u4f4e\u6e29\u5ea6\u80fd\u63d0\u5347\u7cbe\u786e\u7387\u800c\u63d0\u9ad8\u6e29\u5ea6\u5374\u65e0\u6cd5\u6709\u6548\u63d0\u5347\u53ec\u56de\u7387\uff0c\u5e76\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u591a\u6837\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e00\u4e2a\u7b80\u5316\u4f46\u5e38\u89c1\u7684\u6848\u4f8b\uff0c\u7814\u7a76\u89e3\u7801\u6e29\u5ea6\u5bf9\u8bed\u8a00\u6a21\u578b\u8d28\u91cf\uff08\u7cbe\u786e\u7387\uff09\u548c\u8986\u76d6\u7387\uff08\u53ec\u56de\u7387\uff09\u7684\u5f71\u54cd\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u5229\u7528\u7cbe\u786e\u7387-\u53ec\u56de\u7387\u6846\u67b6\u91cd\u65b0\u8bbe\u8ba1\u8bed\u8a00\u6a21\u578b\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u4e3a\u4e86\u901a\u8fc7\u6e29\u5ea6\u8c03\u6574\u6709\u6548\u8c03\u4f18\u6a21\u578b\uff0c\u6a21\u578b\u5fc5\u987b\u671d\u8986\u76d6\u7387\u65b9\u5411\u8bad\u7ec3\u3002\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u7cbe\u786e\u7387-\u53ec\u56de\u7387\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4ec5\u5c06\u8d1f\u5bf9\u6570\u4f3c\u7136\u8bad\u7ec3\u4e0e\u6e29\u5ea6\u7f29\u653e\u7ed3\u5408\uff0c\u5728\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u663e\u8457\u66f4\u597d\u7684\u6743\u8861\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5f00\u53d1\u66f4\u901a\u7528\u3001\u66f4\u9c81\u68d2\u7684\u8bed\u8a00\u5efa\u6a21\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u8868\u660e\u901a\u8fc7\u91cd\u65b0\u601d\u8003\u635f\u5931\u51fd\u6570\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2508.09582", "pdf": "https://arxiv.org/pdf/2508.09582", "abs": "https://arxiv.org/abs/2508.09582", "authors": ["Wafaa B. M. Fadlelmula", "Sanaa Hamid Mohamed", "Taisir E. H. El-Gorashi", "Jaafar M. H. Elmirghani"], "title": "Energy-efficient PON-based Backhaul Connectivity for a VLC-enabled Indoor Fog Computing Environment", "categories": ["cs.NI"], "comment": null, "summary": "In this paper, we consider the use of visible light communication (VLC) to\nprovide connectivity to indoor fog computing resources and propose an\nenergy-efficient passive optical network (PON)-based backhaul architecture to\nsupport the VLC system. We develop a mixed-integer linear programming (MILP)\nmodel to optimize the allocation of computing resources over the proposed\narchitecture, aiming to minimize processing and networking power consumption.\nWe evaluate the performance of the proposed architecture under varying workload\ndemands and user distributions. Comparative analysis against a backhaul\narchitecture that is based on the state-of-the-art spine-and-leaf (S&L) network\ndesign demonstrates total power savings of up to 82%. Further comparison with\ncentralized cloud processing shows improvements in energy efficiency of up to\n93%. Additionally, we examine the improvements in energy efficiency obtained by\nsplitting tasks among multiple processing nodes and propose enhancements to the\narchitecture including dynamic bandwidth allocation, increased wavelength\nbandwidth and improved connectivity within rooms to alleviate networking\nbottlenecks. Furthermore, we introduce an inter-building architecture that\nleverages resources from neighboring buildings to support high-demand\nscenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u6e90\u5149\u7f51\u7edc\uff08PON\uff09\u7684\u8282\u80fd\u56de\u4f20\u67b6\u6784\uff0c\u7528\u4e8e\u652f\u6301\u53ef\u89c1\u5149\u901a\u4fe1\uff08VLC\uff09\u7684\u5ba4\u5185\u96fe\u8ba1\u7b97\u3002\u901a\u8fc7\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u4f18\u5316\u8d44\u6e90\u5206\u914d\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u529f\u8017\uff0c\u5e76\u5c55\u793a\u4e86\u76f8\u5bf9\u4e8e\u73b0\u6709\u6280\u672f\uff08\u5982Spine-and-Leaf\u7f51\u7edc\u548c\u96c6\u4e2d\u5f0f\u4e91\uff09\u663e\u8457\u7684\u80fd\u6548\u63d0\u5347\u3002", "motivation": "\u4e3a\u5ba4\u5185\u96fe\u8ba1\u7b97\u8d44\u6e90\u63d0\u4f9bVLC\u8fde\u63a5\uff0c\u5e76\u8bbe\u8ba1\u4e00\u79cd\u9ad8\u80fd\u6548\u7684\u56de\u4f20\u67b6\u6784\uff0c\u4ee5\u6700\u5c0f\u5316\u5904\u7406\u548c\u7f51\u7edc\u529f\u8017\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePON\u7684\u56de\u4f20\u67b6\u6784\u4ee5\u652f\u6301VLC\u7cfb\u7edf\u30022. \u5f00\u53d1\u4e86\u4e00\u4e2a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u6a21\u578b\u6765\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\uff0c\u76ee\u6807\u662f\u6700\u5c0f\u5316\u529f\u8017\u30023. \u5728\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u548c\u7528\u6237\u5206\u5e03\u4e0b\u8bc4\u4f30\u67b6\u6784\u6027\u80fd\u30024. \u5c06\u6240\u63d0\u67b6\u6784\u4e0eSpine-and-Leaf\uff08S&L\uff09\u7f51\u7edc\u8bbe\u8ba1\u548c\u96c6\u4e2d\u5f0f\u4e91\u5904\u7406\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u30025. \u63a2\u8ba8\u4e86\u4efb\u52a1\u62c6\u5206\u5bf9\u80fd\u6548\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u5305\u62ec\u52a8\u6001\u5e26\u5bbd\u5206\u914d\u3001\u589e\u52a0\u6ce2\u957f\u5e26\u5bbd\u548c\u6539\u5584\u5ba4\u5185\u8fde\u63a5\u5728\u5185\u7684\u67b6\u6784\u589e\u5f3a\u65b9\u6848\u30026. \u5f15\u5165\u4e86\u5229\u7528\u90bb\u8fd1\u5efa\u7b51\u8d44\u6e90\u7684\u8de8\u697c\u5b87\u67b6\u6784\u4ee5\u652f\u6301\u9ad8\u9700\u6c42\u573a\u666f\u3002", "result": "1. \u76f8\u8f83\u4e8eSpine-and-Leaf\uff08S&L\uff09\u7f51\u7edc\u8bbe\u8ba1\u7684\u73b0\u6709\u56de\u4f20\u67b6\u6784\uff0c\u603b\u529f\u8017\u8282\u7701\u9ad8\u8fbe82%\u30022. \u76f8\u8f83\u4e8e\u96c6\u4e2d\u5f0f\u4e91\u5904\u7406\uff0c\u80fd\u6548\u63d0\u5347\u9ad8\u8fbe93%\u30023. \u5c06\u4efb\u52a1\u5206\u914d\u5230\u591a\u4e2a\u5904\u7406\u8282\u70b9\u53ef\u8fdb\u4e00\u6b65\u63d0\u9ad8\u80fd\u6548\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8ePON\u7684VLC\u5ba4\u5185\u96fe\u8ba1\u7b97\u56de\u4f20\u67b6\u6784\u663e\u8457\u964d\u4f4e\u4e86\u529f\u8017\u5e76\u63d0\u9ad8\u4e86\u80fd\u6548\u3002\u901a\u8fc7\u4f18\u5316\u8d44\u6e90\u5206\u914d\u3001\u4efb\u52a1\u62c6\u5206\u4ee5\u53ca\u67b6\u6784\u589e\u5f3a\uff08\u5982\u52a8\u6001\u5e26\u5bbd\u5206\u914d\u548c\u8de8\u697c\u5b87\u8d44\u6e90\u5229\u7528\uff09\uff0c\u8be5\u65b9\u6848\u4e3a\u672a\u6765\u7684\u96fe\u8ba1\u7b97\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u8282\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09170", "pdf": "https://arxiv.org/pdf/2508.09170", "abs": "https://arxiv.org/abs/2508.09170", "authors": ["Amit Kumar Jaiswal", "Haiming Liu", "Ingo Frommholz"], "title": "Multimodal RAG Enhanced Visual Description", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.IR"], "comment": "Accepted by ACM CIKM 2025. 5 pages, 2 figures", "summary": "Textual descriptions for multimodal inputs entail recurrent refinement of\nqueries to produce relevant output images. Despite efforts to address\nchallenges such as scaling model size and data volume, the cost associated with\npre-training and fine-tuning remains substantial. However, pre-trained large\nmultimodal models (LMMs) encounter a modality gap, characterised by a\nmisalignment between textual and visual representations within a common\nembedding space. Although fine-tuning can potentially mitigate this gap, it is\ntypically expensive and impractical due to the requirement for extensive\ndomain-driven data. To overcome this challenge, we propose a lightweight\ntraining-free approach utilising Retrieval-Augmented Generation (RAG) to extend\nacross the modality using a linear mapping, which can be computed efficiently.\nDuring inference, this mapping is applied to images embedded by an LMM enabling\nretrieval of closest textual descriptions from the training set. These textual\ndescriptions, in conjunction with an instruction, cater as an input prompt for\nthe language model to generate new textual descriptions. In addition, we\nintroduce an iterative technique for distilling the mapping by generating\nsynthetic descriptions via the language model facilitating optimisation for\nstandard utilised image description measures. Experimental results on two\nbenchmark multimodal datasets demonstrate significant improvements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u514d\u8bad\u7ec3\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u6620\u5c04\u548c\u8fed\u4ee3\u84b8\u998f\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08LMMs\uff09\u4e2d\u7684\u6a21\u6001\u5dee\u8ddd\u95ee\u9898\uff0c\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08LMMs\uff09\u7684\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u73b0\u6709\u9884\u8bad\u7ec3LMMs\u666e\u904d\u5b58\u5728\u201c\u6a21\u6001\u5dee\u8ddd\u201d\uff0c\u5373\u6587\u672c\u548c\u89c6\u89c9\u8868\u793a\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5b58\u5728\u9519\u4f4d\u3002\u4f20\u7edf\u7684\u5fae\u8c03\u65b9\u6cd5\u867d\u80fd\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u56e0\u9700\u8981\u5927\u91cf\u7279\u5b9a\u9886\u57df\u6570\u636e\u800c\u6210\u672c\u9ad8\u6602\u4e14\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u514d\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u673a\u5236\uff0c\u901a\u8fc7\u9ad8\u6548\u8ba1\u7b97\u7684\u7ebf\u6027\u6620\u5c04\u6765\u8fde\u63a5\u4e0d\u540c\u6a21\u6001\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u8be5\u6620\u5c04\u5e94\u7528\u4e8eLMM\u5d4c\u5165\u7684\u56fe\u50cf\uff0c\u4ee5\u68c0\u7d22\u8bad\u7ec3\u96c6\u4e2d\u6700\u63a5\u8fd1\u7684\u6587\u672c\u63cf\u8ff0\u3002\u8fd9\u4e9b\u63cf\u8ff0\u7ed3\u5408\u6307\u4ee4\u4f5c\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u5165\u63d0\u793a\uff0c\u7528\u4e8e\u751f\u6210\u65b0\u7684\u6587\u672c\u63cf\u8ff0\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u8fed\u4ee3\u6280\u672f\uff0c\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5408\u6210\u63cf\u8ff0\u6765\u84b8\u998f\u6620\u5c04\uff0c\u4ece\u800c\u4f18\u5316\u56fe\u50cf\u63cf\u8ff0\u7684\u8861\u91cf\u6807\u51c6\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u3001\u514d\u8bad\u7ec3\u7684RAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u6620\u5c04\u548c\u8fed\u4ee3\u84b8\u998f\uff0c\u6210\u529f\u514b\u670d\u4e86\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u6a21\u6001\u5dee\u8ddd\u95ee\u9898\uff0c\u4e3a\u751f\u6210\u56fe\u50cf\u6587\u672c\u63cf\u8ff0\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.09327", "pdf": "https://arxiv.org/pdf/2508.09327", "abs": "https://arxiv.org/abs/2508.09327", "authors": ["Yifan Jiang", "Ahmad Shariftabrizi", "Venkata SK. Manem"], "title": "Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model", "categories": ["cs.CV"], "comment": null, "summary": "Generative artificial intelligence (AI) has been playing an important role in\nvarious domains. Leveraging its high capability to generate high-fidelity and\ndiverse synthetic data, generative AI is widely applied in diagnostic tasks,\nsuch as lung cancer diagnosis using computed tomography (CT). However, existing\ngenerative models for lung cancer diagnosis suffer from low efficiency and\nanatomical imprecision, which limit their clinical applicability. To address\nthese drawbacks, we propose Lung-DDPM+, an improved version of our previous\nmodel, Lung-DDPM. This novel approach is a denoising diffusion probabilistic\nmodel (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary\nDPM-solver, enabling the method to focus on lesion areas while achieving a\nbetter trade-off between sampling efficiency and quality. Evaluation results on\nthe public LIDC-IDRI dataset suggest that the proposed method achieves\n8$\\times$ fewer FLOPs (floating point operations per second), 6.8$\\times$ lower\nGPU memory consumption, and 14$\\times$ faster sampling compared to Lung-DDPM.\nMoreover, it maintains comparable sample quality to both Lung-DDPM and other\nstate-of-the-art (SOTA) generative models in two downstream segmentation tasks.\nWe also conducted a Visual Turing Test by an experienced radiologist, showing\nthe advanced quality and fidelity of synthetic samples generated by the\nproposed method. These experimental results demonstrate that Lung-DDPM+ can\neffectively generate high-quality thoracic CT images with lung nodules,\nhighlighting its potential for broader applications, such as general tumor\nsynthesis and lesion generation in medical imaging. The code and pretrained\nmodels are available at https://github.com/Manem-Lab/Lung-DDPM-PLUS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLung-DDPM+\uff0c\u4e00\u79cd\u6539\u8fdb\u7684\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u5e03\u5c40\u5f15\u5bfc\u548c\u52a0\u901f\u6c42\u89e3\u5668\uff0c\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u80ba\u7ed3\u8282CT\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u5e76\u4fdd\u6301\u6837\u672c\u8d28\u91cf\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u591a\u79cd\u533b\u5b66\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u80ba\u764c\u8bca\u65ad\u4e2d\u5b58\u5728\u6548\u7387\u4f4e\u548c\u89e3\u5256\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u63d0\u51faLung-DDPM+\uff0c\u5b83\u662fLung-DDPM\u7684\u6539\u8fdb\u7248\uff0c\u4e00\u79cd\u901a\u8fc7\u7ed3\u8282\u8bed\u4e49\u5e03\u5c40\u5f15\u5bfc\u5e76\u7531\u80ba\u90e8DPM-solver\u52a0\u901f\u7684\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\u3002", "result": "Lung-DDPM+\u5728LIDC-IDRI\u6570\u636e\u96c6\u4e0aFLOPs\u51cf\u5c118\u500d\uff0cGPU\u5185\u5b58\u6d88\u8017\u964d\u4f4e6.8\u500d\uff0c\u91c7\u6837\u901f\u5ea6\u5feb14\u500d\uff0c\u540c\u65f6\u5728\u4e24\u4e2a\u4e0b\u6e38\u5206\u5272\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e0eLung-DDPM\u53ca\u5176\u4ed6SOTA\u6a21\u578b\u76f8\u5f53\u7684\u6837\u672c\u8d28\u91cf\u3002\u7ecf\u9a8c\u4e30\u5bcc\u7684\u653e\u5c04\u79d1\u533b\u751f\u8fdb\u884c\u7684\u89c6\u89c9\u56fe\u7075\u6d4b\u8bd5\u4e5f\u8bc1\u5b9e\u4e86\u5176\u5408\u6210\u6837\u672c\u7684\u9ad8\u7ea7\u8d28\u91cf\u548c\u4fdd\u771f\u5ea6\u3002", "conclusion": "Lung-DDPM+\u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u542b\u80ba\u7ed3\u8282\u80f8\u90e8CT\u56fe\u50cf\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u901a\u7528\u80bf\u7624\u5408\u6210\u548c\u533b\u5b66\u56fe\u50cf\u75c5\u53d8\u751f\u6210\u7b49\u66f4\u5e7f\u6cdb\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.09662", "pdf": "https://arxiv.org/pdf/2508.09662", "abs": "https://arxiv.org/abs/2508.09662", "authors": ["Yaoning Wang", "Jiahao Ying", "Yixin Cao", "Yubo Ma", "Yugang Jiang"], "title": "EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) and the development of\nincreasingly large and diverse evaluation benchmarks have introduced\nsubstantial computational challenges for model assessment. In this paper, we\npresent EffiEval, a training-free approach for efficient benchmarking that\neffectively addresses data redundancy while maintaining high evaluation\nreliability. Our method is specifically designed to meet three key criteria for\nhigh-quality evaluation: representativeness, by ensuring comprehensive coverage\nof model capabilities; fairness, by remaining independent of model performance\nduring sample selection to avoid bias; and generalizability, by enabling\nflexible transfer across datasets and model families without reliance on\nlarge-scale evaluation data. Unlike traditional methods that rely on absolute\nperformance or require extensive evaluation data, our approach adaptively\nselects high-quality representative subsets based on the Model Utility Index\n(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs\ndemonstrate that EffiEval achieves strong ranking consistency with full-dataset\nevaluation using only a small fraction of the original data. Furthermore, our\nmethod is flexible and scalable in size, allowing users to balance evaluation\nefficiency and representativeness according to specific needs. Overall,\nEffiEval provides a practical and generalizable solution for reliable, fair,\nand efficient evaluation in the era of LLMs.", "AI": {"tldr": "EffiEval\u662f\u4e00\u79cd\u65e0\u8bad\u7ec3\u3001\u9ad8\u6548\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u9ad8\u8d28\u91cf\u4ee3\u8868\u6027\u5b50\u96c6\u6765\u89e3\u51b3LLM\u8bc4\u4f30\u4e2d\u7684\u6570\u636e\u5197\u4f59\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8bc4\u4f30\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u7684\u5feb\u901f\u53d1\u5c55\u548c\u8bc4\u4f30\u57fa\u51c6\u89c4\u6a21\u7684\u6269\u5927\uff0c\u7ed9\u6a21\u578b\u8bc4\u4f30\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba1\u7b97\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86EffiEval\uff0c\u4e00\u79cd\u65e0\u8bad\u7ec3\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002\u5b83\u901a\u8fc7\u57fa\u4e8e\u6a21\u578b\u6548\u7528\u6307\u6570(MUI)\u81ea\u9002\u5e94\u9009\u62e9\u9ad8\u8d28\u91cf\u7684\u4ee3\u8868\u6027\u5b50\u96c6\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u4ee3\u8868\u6027\u3001\u516c\u5e73\u6027\u548c\u6cdb\u5316\u6027\uff0c\u4ece\u800c\u6709\u6548\u89e3\u51b3\u6570\u636e\u5197\u4f59\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u57fa\u51c6\u548c\u4e0d\u540cLLM\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEffiEval\u4ec5\u4f7f\u7528\u539f\u59cb\u6570\u636e\u7684\u4e00\u5c0f\u90e8\u5206\u5c31\u80fd\u5b9e\u73b0\u4e0e\u5168\u6570\u636e\u96c6\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\u7684\u6a21\u578b\u6392\u540d\uff0c\u4e14\u5728\u5927\u5c0f\u4e0a\u5177\u6709\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "EffiEval\u4e3aLLM\u65f6\u4ee3\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u516c\u5e73\u4e14\u9ad8\u6548\u7684\u8bc4\u4f30\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u666e\u9002\u6027\u3002"}}
{"id": "2508.09620", "pdf": "https://arxiv.org/pdf/2508.09620", "abs": "https://arxiv.org/abs/2508.09620", "authors": ["Michel Rottleuthner", "Thomas C. Schmidt", "Matthias W\u00e4hlisch"], "title": "Duty-Cycling is Not Enough in Constrained IoT Networking: Revealing the Energy Savings of Dynamic Clock Scaling", "categories": ["cs.NI", "cs.SY", "eess.SY", "D.4.8; C.3"], "comment": null, "summary": "Minimizing energy consumption of low-power wireless nodes is a persistent\nchallenge from the constrained Internet of Things (IoT). In this paper, we\nstart from the observation that constrained IoT devices have largely different\nhardware (im-)balances than full-scale machines. We find that the performance\ngap between MCU and network throughput on constrained devices enables minimal\nenergy delay product (EDP) for IoT networking at largely reduced clock\nfrequencies. We analyze the potentials by integrating dynamic voltage and\nfrequency scaling (DVFS) into the RIOT IoT operating system and show that the\nDVFS reconfiguration overhead stays below the energy saved for a single,\ndownscaled MAC operation. Backed by these findings, we systematically\ninvestigate how DVFS further improves energy-efficiency for common networking\ntasks -- in addition to duty-cycling. We measure IoT communication scenarios\nbetween real-world systems and analyze two MAC operating modes -- CSMA/CA and\ntime slotting -- in combination with different CoAP transactions, payload\nsizes, as well as DTLS transport encryption. Our experiments reveal energy\nsavings between 24% and 52% for MAC operations and up to 37% for encrypted CoAP\ncommunication. These results shall encourage research and system design work to\nintegrate DVFS in future IoT devices for performing tasks at their optimal\nfrequencies and thereby significantly extending battery lifetimes.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5728\u53d7\u9650\u7269\u8054\u7f51\u8bbe\u5907\u4e2d\u5229\u7528\u52a8\u6001\u7535\u538b\u9891\u7387\u8c03\u8282\uff08DVFS\uff09\u6280\u672f\uff0c\u53ef\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff0c\u5ef6\u957f\u7535\u6c60\u5bff\u547d\u3002", "motivation": "\u53d7\u9650\u7269\u8054\u7f51\uff08IoT\uff09\u8282\u70b9\u7684\u80fd\u8017\u6700\u5c0f\u5316\u662f\u4e00\u5927\u6311\u6218\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8fd9\u4e9b\u8bbe\u5907\u5fae\u63a7\u5236\u5668\u4e0e\u7f51\u7edc\u541e\u5410\u91cf\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff0c\u8fd9\u4f7f\u5f97\u5728\u8f83\u4f4e\u65f6\u949f\u9891\u7387\u4e0b\u5b9e\u73b0\u6700\u4f4e\u80fd\u91cf\u5ef6\u8fdf\u79ef\uff08EDP\uff09\u6210\u4e3a\u53ef\u80fd\u3002", "method": "\u5c06\u52a8\u6001\u7535\u538b\u9891\u7387\u8c03\u8282\uff08DVFS\uff09\u96c6\u6210\u5230RIOT IoT\u64cd\u4f5c\u7cfb\u7edf\u4e2d\uff0c\u5e76\u8bc4\u4f30\u5176\u91cd\u914d\u7f6e\u5f00\u9500\u3002\u901a\u8fc7\u5728\u771f\u5b9e\u7269\u8054\u7f51\u7cfb\u7edf\u4e0a\uff0c\u7ed3\u5408CSMA/CA\u548c\u65f6\u9699\u4e24\u79cdMAC\u64cd\u4f5c\u6a21\u5f0f\uff0c\u4ee5\u53ca\u4e0d\u540cCoAP\u4e8b\u52a1\u3001\u6709\u6548\u8f7d\u8377\u5927\u5c0f\u548cDTLS\u4f20\u8f93\u52a0\u5bc6\uff0c\u7cfb\u7edf\u5730\u6d4b\u91cf\u5e76\u5206\u6790\u4e86DVFS\u5bf9\u80fd\u6548\u7684\u63d0\u5347\u6548\u679c\u3002", "result": "DVFS\u91cd\u914d\u7f6e\u5f00\u9500\u4f4e\u4e8e\u5355\u6b21\u964d\u9891MAC\u64cd\u4f5c\u6240\u8282\u7701\u7684\u80fd\u91cf\u3002\u5b9e\u9a8c\u663e\u793a\uff0cMAC\u64cd\u4f5c\u80fd\u8017\u8282\u7701\u4e8624%\u81f352%\uff0c\u52a0\u5bc6\u7684CoAP\u901a\u4fe1\u80fd\u8017\u8282\u7701\u9ad8\u8fbe37%\u3002", "conclusion": "DVFS\u662f\u4e00\u79cd\u6709\u6548\u63d0\u9ad8\u7269\u8054\u7f51\u8bbe\u5907\u80fd\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u5ef6\u957f\u7535\u6c60\u5bff\u547d\u3002\u7814\u7a76\u9f13\u52b1\u5c06DVFS\u96c6\u6210\u5230\u672a\u6765\u7684\u7269\u8054\u7f51\u8bbe\u5907\u4e2d\uff0c\u4ee5\u4f18\u5316\u4efb\u52a1\u6267\u884c\u9891\u7387\u3002"}}
{"id": "2508.09174", "pdf": "https://arxiv.org/pdf/2508.09174", "abs": "https://arxiv.org/abs/2508.09174", "authors": ["Zhekai Zhou", "Shudong Liu", "Zhaokun Zhou", "Yang Liu", "Qiang Yang", "Yuesheng Zhu", "Guibo Luo"], "title": "FedMP: Tackling Medical Feature Heterogeneity in Federated Learning from a Manifold Perspective", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated learning (FL) is a decentralized machine learning paradigm in which\nmultiple clients collaboratively train a shared model without sharing their\nlocal private data. However, real-world applications of FL frequently encounter\nchallenges arising from the non-identically and independently distributed\n(non-IID) local datasets across participating clients, which is particularly\npronounced in the field of medical imaging, where shifts in image feature\ndistributions significantly hinder the global model's convergence and\nperformance. To address this challenge, we propose FedMP, a novel method\ndesigned to enhance FL under non-IID scenarios. FedMP employs stochastic\nfeature manifold completion to enrich the training space of individual client\nclassifiers, and leverages class-prototypes to guide the alignment of feature\nmanifolds across clients within semantically consistent subspaces, facilitating\nthe construction of more distinct decision boundaries. We validate the\neffectiveness of FedMP on multiple medical imaging datasets, including those\nwith real-world multi-center distributions, as well as on a multi-domain\nnatural image dataset. The experimental results demonstrate that FedMP\noutperforms existing FL algorithms. Additionally, we analyze the impact of\nmanifold dimensionality, communication efficiency, and privacy implications of\nfeature exposure in our method.", "AI": {"tldr": "FedMP\u901a\u8fc7\u7279\u5f81\u6d41\u5f62\u8865\u5168\u548c\u7c7b\u522b\u539f\u578b\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u7b49\u975eIID\u6570\u636e\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\uff08\u5c24\u5176\u662f\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\uff0c\u56fe\u50cf\u7279\u5f81\u5206\u5e03\u5dee\u5f02\u663e\u8457\uff09\u4e0b\uff0c\u5168\u5c40\u6a21\u578b\u7684\u6536\u655b\u6027\u548c\u6027\u80fd\u53d7\u5230\u4e25\u91cd\u963b\u788d\u3002\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51faFedMP\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u7279\u5f81\u6d41\u5f62\u8865\u5168\u6765\u4e30\u5bcc\u5ba2\u6237\u7aef\u5206\u7c7b\u5668\u7684\u8bad\u7ec3\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u7c7b\u522b\u539f\u578b\u5f15\u5bfc\u5ba2\u6237\u7aef\u95f4\u7279\u5f81\u6d41\u5f62\u5728\u8bed\u4e49\u4e00\u81f4\u5b50\u7a7a\u95f4\u5185\u5bf9\u9f50\uff0c\u4ee5\u6784\u5efa\u66f4\u6e05\u6670\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "result": "FedMP\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\uff08\u5305\u62ec\u771f\u5b9e\u4e16\u754c\u591a\u4e2d\u5fc3\u5206\u5e03\u6570\u636e\uff09\u548c\u591a\u57df\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u5206\u6790\u4e86\u6d41\u5f62\u7ef4\u5ea6\u3001\u901a\u4fe1\u6548\u7387\u548c\u7279\u5f81\u66b4\u9732\u7684\u9690\u79c1\u5f71\u54cd\u3002", "conclusion": "FedMP\u6709\u6548\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u975eIID\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\uff0c\u5e76\u5206\u6790\u4e86\u76f8\u5173\u5f71\u54cd\u56e0\u7d20\u3002"}}
{"id": "2508.09339", "pdf": "https://arxiv.org/pdf/2508.09339", "abs": "https://arxiv.org/abs/2508.09339", "authors": ["Aqsa Sultana", "Nordin Abouzahra", "Ahmed Rahu", "Brian Shula", "Brandon Combs", "Derrick Forchetti", "Theus Aspiras", "Vijayan K. Asari"], "title": "UltraLight Med-Vision Mamba for Classification of Neoplastic Progression in Tubular Adenomas", "categories": ["cs.CV"], "comment": null, "summary": "Identification of precancerous polyps during routine colonoscopy screenings\nis vital for their excision, lowering the risk of developing colorectal cancer.\nAdvanced deep learning algorithms enable precise adenoma classification and\nstratification, improving risk assessment accuracy and enabling personalized\nsurveillance protocols that optimize patient outcomes. Ultralight Med-Vision\nMamba, a state-space based model (SSM), has excelled in modeling long- and\nshort-range dependencies and image generalization, critical factors for\nanalyzing whole slide images. Furthermore, Ultralight Med-Vision Mamba's\nefficient architecture offers advantages in both computational speed and\nscalability, making it a promising tool for real-time clinical deployment.", "AI": {"tldr": "\u5229\u7528\u8d85\u8f7b\u91cf\u7ea7Med-Vision Mamba\u6a21\u578b\uff0c\u5b9e\u73b0\u7ed3\u80a0\u955c\u68c0\u67e5\u4e2d\u764c\u524d\u606f\u8089\u7684\u7cbe\u51c6\u5206\u7c7b\uff0c\u4ee5\u4f18\u5316\u60a3\u8005\u9884\u540e\u3002", "motivation": "\u5728\u5e38\u89c4\u7ed3\u80a0\u955c\u68c0\u67e5\u4e2d\u8bc6\u522b\u764c\u524d\u606f\u8089\u5bf9\u5176\u5207\u9664\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u964d\u4f4e\u60a3\u7ed3\u76f4\u80a0\u764c\u7684\u98ce\u9669\u3002\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u817a\u7624\u5206\u7c7b\u548c\u5206\u5c42\u7684\u51c6\u786e\u6027\uff0c\u4ece\u800c\u6539\u5584\u98ce\u9669\u8bc4\u4f30\u5e76\u5b9e\u73b0\u4e2a\u6027\u5316\u76d1\u6d4b\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u7684\u8d85\u8f7b\u91cf\u7ea7Med-Vision Mamba\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u64c5\u957f\u5efa\u6a21\u957f\u77ed\u7a0b\u4f9d\u8d56\u548c\u56fe\u50cf\u6cdb\u5316\uff0c\u5e76\u5177\u6709\u9ad8\u6548\u7684\u67b6\u6784\u3002", "result": "\u8d85\u8f7b\u91cf\u7ea7Med-Vision Mamba\u6a21\u578b\u5728\u5efa\u6a21\u957f\u77ed\u7a0b\u4f9d\u8d56\u548c\u56fe\u50cf\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5168\u73bb\u7247\u56fe\u50cf\u5206\u6790\uff0c\u5e76\u5728\u8ba1\u7b97\u901f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u8d85\u8f7b\u91cf\u7ea7Med-Vision Mamba\u6a21\u578b\u56e0\u5176\u5353\u8d8a\u7684\u6027\u80fd\u548c\u9ad8\u6548\u6027\uff0c\u662f\u7528\u4e8e\u5b9e\u65f6\u4e34\u5e8a\u90e8\u7f72\u7684\u6709\u524d\u666f\u7684\u5de5\u5177\u3002"}}
{"id": "2508.09666", "pdf": "https://arxiv.org/pdf/2508.09666", "abs": "https://arxiv.org/abs/2508.09666", "authors": ["Ziyang Ma", "Qingyue Yuan", "Linhai Zhang", "Deyu Zhou"], "title": "Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Previous chain-of-thought (CoT) distillation methods primarily focused on\nenhancing the reasoning capabilities of Small Language Models (SLMs) by\nutilizing high-quality rationales generated by powerful Large Language Models\n(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM\nsafety brought by the training, which are revealed in this study. Although\nthere are works on safety alignment that fine-tune language models or\nmanipulate model weights to defend against harmful inputs, they require extra\ncomputation or annotated data, and probably impact the reasoning ability of\nSLMs. In this paper, we investigate how to maintain the safety of SLMs during\nthe CoT distillation process. Specifically, we propose a safe distillation\nmethod, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing\ntwo modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the\nmagnitude of model weight changes to optimize the model weights in the\nneighboring space near the initial weight distribution. Low-Entropy Masking\nmasks low-entropy tokens, which are regarded as unnecessary learning targets,\nto exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,\nLlama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,\nAGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety\nof SLMs and comparably improves their reasoning capability compared to existing\ndistillation methods. Furthermore, our ablation study presents the\neffectiveness of Slow Tuning and Low-Entropy Masking, with the former\nmaintaining the model's safety in the early stage and the latter prolonging the\nsafe training epochs.", "AI": {"tldr": "\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u84b8\u998f\u5728\u63d0\u5347\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u63a8\u7406\u80fd\u529b\u65f6\u53ef\u80fd\u635f\u5bb3\u5176\u5b89\u5168\u6027\u3002\u672c\u6587\u63d0\u51faSLowED\u65b9\u6cd5\uff0c\u901a\u8fc7\u6162\u901f\u5fae\u8c03\u548c\u4f4e\u71b5\u63a9\u7801\uff0c\u5728\u4fdd\u6301\u5b89\u5168\u6027\u7684\u540c\u65f6\u63d0\u5347SLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9ad8\u8d28\u91cf\u63a8\u7406\u8fc7\u7a0b\u6765\u589e\u5f3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5f88\u5c11\u5173\u6ce8\u6b64\u8bad\u7ec3\u8fc7\u7a0b\u5bf9SLM\u5b89\u5168\u6027\u53ef\u80fd\u9020\u6210\u7684\u8d1f\u9762\u5f71\u54cd\u3002\u867d\u7136\u5b58\u5728\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u9700\u8981\u989d\u5916\u8ba1\u7b97\u6216\u6807\u6ce8\u6570\u636e\uff0c\u5e76\u53ef\u80fd\u5f71\u54cd\u63a8\u7406\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u5728CoT\u84b8\u998f\u8fc7\u7a0b\u4e2d\u7ef4\u62a4SLM\u7684\u5b89\u5168\u6027\u662f\u672c\u7814\u7a76\u7684\u6838\u5fc3\u52a8\u673a\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSlow Tuning and Low-Entropy Masking Distillation (SLowED) \u7684\u5b89\u5168\u84b8\u998f\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a1) Slow Tuning\uff0c\u901a\u8fc7\u7f29\u5c0f\u6a21\u578b\u6743\u91cd\u53d8\u5316\u7684\u5e45\u5ea6\uff0c\u4f7f\u6a21\u578b\u6743\u91cd\u5728\u521d\u59cb\u6743\u91cd\u5206\u5e03\u7684\u90bb\u8fd1\u7a7a\u95f4\u8fdb\u884c\u4f18\u5316\uff1b2) Low-Entropy Masking\uff0c\u7528\u4e8e\u63a9\u76d6\u88ab\u89c6\u4e3a\u4e0d\u5fc5\u8981\u5b66\u4e60\u76ee\u6807\u7684\u4f4e\u71b5\u4ee4\u724c\uff0c\u4ece\u800c\u5c06\u5176\u4ece\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6392\u9664\u3002", "result": "\u5728Qwen2.5-1.5B\u3001Llama-3.2-1B\u548cBLOOM-1.1B\u4e09\u79cdSLM\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\uff0c\u901a\u8fc7\u63a8\u7406\u57fa\u51c6\uff08BBH, BB-Sub, ARC, AGIEval\uff09\u548c\u5b89\u5168\u8bc4\u4f30\uff08AdvBench\uff09\u663e\u793a\uff0cSLowED\u65b9\u6cd5\u5728\u4fdd\u6301SLM\u5b89\u5168\u6027\u7684\u540c\u65f6\uff0c\u80fd\u4e0e\u73b0\u6709\u84b8\u998f\u65b9\u6cd5\u5ab2\u7f8e\u5730\u63d0\u5347\u5176\u63a8\u7406\u80fd\u529b\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86Slow Tuning\u5728\u8bad\u7ec3\u65e9\u671f\u9636\u6bb5\u5bf9\u6a21\u578b\u5b89\u5168\u6027\u7684\u7ef4\u62a4\u4f5c\u7528\uff0c\u4ee5\u53caLow-Entropy Masking\u5728\u5ef6\u957f\u5b89\u5168\u8bad\u7ec3\u5468\u671f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "SLowED\u662f\u4e00\u79cd\u6709\u6548\u4e14\u5b89\u5168\u7684CoT\u84b8\u998f\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86SLM\u5728\u63a8\u7406\u80fd\u529b\u63d0\u5347\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u5b89\u5168\u6027\u4e0b\u964d\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6a21\u578b\u5b89\u5168\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5b89\u5168\u9ad8\u6548\u7684SLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2508.09660", "pdf": "https://arxiv.org/pdf/2508.09660", "abs": "https://arxiv.org/abs/2508.09660", "authors": ["Jesus Oma\u00f1a Iglesias", "Carlos Segura Perales", "Stefan Gei\u00dfler", "Diego Perino", "Andra Lutu"], "title": "Anomaly Detection for IoT Global Connectivity", "categories": ["cs.NI", "cs.AI", "cs.LG"], "comment": null, "summary": "Internet of Things (IoT) application providers rely on Mobile Network\nOperators (MNOs) and roaming infrastructures to deliver their services\nglobally. In this complex ecosystem, where the end-to-end communication path\ntraverses multiple entities, it has become increasingly challenging to\nguarantee communication availability and reliability. Further, most platform\noperators use a reactive approach to communication issues, responding to user\ncomplaints only after incidents have become severe, compromising service\nquality. This paper presents our experience in the design and deployment of\nANCHOR -- an unsupervised anomaly detection solution for the IoT connectivity\nservice of a large global roaming platform. ANCHOR assists engineers by\nfiltering vast amounts of data to identify potential problematic clients (i.e.,\nthose with connectivity issues affecting several of their IoT devices),\nenabling proactive issue resolution before the service is critically impacted.\nWe first describe the IoT service, infrastructure, and network visibility of\nthe IoT connectivity provider we operate. Second, we describe the main\nchallenges and operational requirements for designing an unsupervised anomaly\ndetection solution on this platform. Following these guidelines, we propose\ndifferent statistical rules, and machine- and deep-learning models for IoT\nverticals anomaly detection based on passive signaling traffic. We describe the\nsteps we followed working with the operational teams on the design and\nevaluation of our solution on the operational platform, and report an\nevaluation on operational IoT customers.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecdANCHOR\uff0c\u4e00\u4e2a\u9488\u5bf9\u5168\u7403IoT\u6f2b\u6e38\u5e73\u53f0\u8fde\u63a5\u670d\u52a1\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u901a\u8fc7\u8bc6\u522b\u6709\u8fde\u63a5\u95ee\u9898\u7684\u5ba2\u6237\u7aef\uff0c\u5b9e\u73b0\u4e3b\u52a8\u5f0f\u95ee\u9898\u89e3\u51b3\u3002", "motivation": "\u5168\u7403\u7269\u8054\u7f51\u670d\u52a1\u901a\u4fe1\u8def\u5f84\u590d\u6742\uff0c\u5bfc\u81f4\u901a\u4fe1\u53ef\u7528\u6027\u548c\u53ef\u9760\u6027\u96be\u4ee5\u4fdd\u8bc1\u3002\u73b0\u6709\u8fd0\u8425\u5546\u591a\u91c7\u53d6\u88ab\u52a8\u5f0f\u6545\u969c\u54cd\u5e94\uff0c\u5f71\u54cd\u670d\u52a1\u8d28\u91cf\u3002\u7814\u7a76\u65e8\u5728\u5b9e\u73b0\u5bf9IoT\u8fde\u63a5\u95ee\u9898\u7684\u4e3b\u52a8\u8bc6\u522b\u4e0e\u89e3\u51b3\u3002", "method": "\u5f00\u53d1\u5e76\u90e8\u7f72\u4e86\u540d\u4e3aANCHOR\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6848\u57fa\u4e8e\u88ab\u52a8\u4fe1\u4ee4\u6d41\u91cf\uff0c\u5229\u7528\u7edf\u8ba1\u89c4\u5219\u3001\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u8bc6\u522b\u5177\u6709\u8fde\u63a5\u95ee\u9898\u7684IoT\u5ba2\u6237\u7aef\u3002\u8bba\u6587\u8be6\u7ec6\u63cf\u8ff0\u4e86\u5728\u5b9e\u9645\u8fd0\u8425\u5e73\u53f0\u4e0a\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u8fc7\u7a0b\u3002", "result": "ANCHOR\u80fd\u591f\u6709\u6548\u8fc7\u6ee4\u6d77\u91cf\u6570\u636e\uff0c\u8bc6\u522b\u51fa\u6f5c\u5728\u7684\u95ee\u9898\u5ba2\u6237\u7aef\uff08\u5373\u8fde\u63a5\u95ee\u9898\u5f71\u54cd\u591a\u4e2aIoT\u8bbe\u5907\u7684\u5ba2\u6237\u7aef\uff09\uff0c\u4ece\u800c\u4f7f\u5de5\u7a0b\u5e08\u80fd\u5728\u670d\u52a1\u4e25\u91cd\u53d7\u635f\u524d\u4e3b\u52a8\u89e3\u51b3\u95ee\u9898\u3002\u7814\u7a76\u62a5\u544a\u4e86\u5728\u5b9e\u9645\u8fd0\u8425IoT\u5ba2\u6237\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u548c\u7ecf\u9a8c\u3002", "conclusion": "ANCHOR\u65b9\u6848\u901a\u8fc7\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\uff0c\u4e3a\u590d\u6742\u7684IoT\u8fde\u63a5\u670d\u52a1\u63d0\u4f9b\u4e86\u4e3b\u52a8\u7ba1\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u670d\u52a1\u8d28\u91cf\u548c\u53ef\u9760\u6027\uff0c\u5b9e\u73b0\u4e86\u4ece\u88ab\u52a8\u54cd\u5e94\u5411\u4e3b\u52a8\u89e3\u51b3\u95ee\u9898\u7684\u8f6c\u53d8\u3002"}}
{"id": "2508.09176", "pdf": "https://arxiv.org/pdf/2508.09176", "abs": "https://arxiv.org/abs/2508.09176", "authors": ["Hazem Hesham Yousef Shalby", "Fabrizio Pittorino", "Francesca Palermo", "Diana Trojaniello", "Manuel Roveri"], "title": "DQT: Dynamic Quantization Training via Dequantization-Free Nested Integer Arithmetic", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The deployment of deep neural networks on resource-constrained devices relies\non quantization. While static, uniform quantization applies a fixed bit-width\nto all inputs, it fails to adapt to their varying complexity. Dynamic,\ninstance-based mixed-precision quantization promises a superior\naccuracy-efficiency trade-off by allocating higher precision only when needed.\nHowever, a critical bottleneck remains: existing methods require a costly\ndequantize-to-float and requantize-to-integer cycle to change precision,\nbreaking the integer-only hardware paradigm and compromising performance gains.\nThis paper introduces Dynamic Quantization Training (DQT), a novel framework\nthat removes this bottleneck. At the core of DQT is a nested integer\nrepresentation where lower-precision values are bit-wise embedded within\nhigher-precision ones. This design, coupled with custom integer-only\narithmetic, allows for on-the-fly bit-width switching through a near-zero-cost\nbit-shift operation. This makes DQT the first quantization framework to enable\nboth dequantization-free static mixed-precision of the backbone network, and\ntruly efficient dynamic, instance-based quantization through a lightweight\ncontroller that decides at runtime how to quantize each layer. We demonstrate\nDQT state-of-the-art performance on ResNet18 on CIFAR-10 and ResNet50 on\nImageNet. On ImageNet, our 4-bit dynamic ResNet50 achieves 77.00% top-1\naccuracy, an improvement over leading static (LSQ, 76.70%) and dynamic (DQNET,\n76.94%) methods at a comparable BitOPs budget. Crucially, DQT achieves this\nwith a bit-width transition cost of only 28.3M simple bit-shift operations, a\ndrastic improvement over the 56.6M costly Multiply-Accumulate (MAC)\nfloating-point operations required by previous dynamic approaches - unlocking a\nnew frontier in efficient, adaptive AI.", "AI": {"tldr": "\u63d0\u51faDQT\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5957\u6574\u6570\u8868\u793a\u548c\u4f4d\u79fb\u64cd\u4f5c\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u91cf\u5316\u4e2d\u6602\u8d35\u7684\u53cd\u91cf\u5316-\u518d\u91cf\u5316\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u81ea\u9002\u5e94AI\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u65f6\uff0c\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u9762\u4e34\u6311\u6218\uff1a\u9759\u6001\u91cf\u5316\u65e0\u6cd5\u9002\u5e94 varying complexity\uff0c\u800c\u52a8\u6001\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u867d\u4f18\u8d8a\u4f46\u9700\u8981\u6602\u8d35\u7684\u6d6e\u70b9\u8f6c\u6362\u5faa\u73af\uff08\u53cd\u91cf\u5316-\u518d\u91cf\u5316\uff09\uff0c\u7834\u574f\u4e86\u7eaf\u6574\u6570\u786c\u4ef6\u8303\u5f0f\u5e76\u5f71\u54cd\u6027\u80fd\u589e\u76ca\u3002", "method": "\u5f15\u5165\u52a8\u6001\u91cf\u5316\u8bad\u7ec3\uff08DQT\uff09\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u5d4c\u5957\u6574\u6570\u8868\u793a\uff0c\u5176\u4e2d\u4f4e\u7cbe\u5ea6\u503c\u6309\u4f4d\u5d4c\u5165\u9ad8\u7cbe\u5ea6\u503c\u4e2d\u3002\u7ed3\u5408\u81ea\u5b9a\u4e49\u7eaf\u6574\u6570\u7b97\u672f\uff0c\u901a\u8fc7\u63a5\u8fd1\u96f6\u6210\u672c\u7684\u4f4d\u79fb\u64cd\u4f5c\u5b9e\u73b0\u5373\u65f6\u4f4d\u5bbd\u5207\u6362\u3002DQT\u9996\u6b21\u5b9e\u73b0\u4e86\u4e3b\u5e72\u7f51\u7edc\u7684\u65e0\u53cd\u91cf\u5316\u9759\u6001\u6df7\u5408\u7cbe\u5ea6\uff0c\u4ee5\u53ca\u901a\u8fc7\u8f7b\u91cf\u7ea7\u63a7\u5236\u5668\u5b9e\u73b0\u7684\u771f\u6b63\u9ad8\u6548\u7684\u52a8\u6001\u3001\u57fa\u4e8e\u5b9e\u4f8b\u7684\u91cf\u5316\u3002", "result": "DQT\u5728CIFAR-10\u4e0a\u7684ResNet18\u548cImageNet\u4e0a\u7684ResNet50\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5728ImageNet\u4e0a\uff0c4\u6bd4\u7279\u52a8\u6001ResNet50\u5b9e\u73b0\u4e8677.00%\u7684top-1\u7cbe\u5ea6\uff0c\u5728\u53ef\u6bd4\u7684BitOPs\u9884\u7b97\u4e0b\u4f18\u4e8e\u9886\u5148\u7684\u9759\u6001(LSQ, 76.70%)\u548c\u52a8\u6001(DQNET, 76.94%)\u65b9\u6cd5\u3002\u5173\u952e\u662f\uff0cDQT\u7684\u4f4d\u5bbd\u8f6c\u6362\u6210\u672c\u4ec5\u4e3a28.3M\u6b21\u7b80\u5355\u4f4d\u79fb\u64cd\u4f5c\uff0c\u76f8\u6bd4\u4e4b\u524d\u52a8\u6001\u65b9\u6cd5\u6240\u9700\u768456.6M\u6b21\u6602\u8d35\u4e58\u52a0(MAC)\u6d6e\u70b9\u64cd\u4f5c\uff0c\u5927\u5e45\u964d\u4f4e\u3002", "conclusion": "DQT\u901a\u8fc7\u6d88\u9664\u6602\u8d35\u7684\u53cd\u91cf\u5316\u74f6\u9888\uff0c\u5f00\u542f\u4e86\u9ad8\u6548\u3001\u81ea\u9002\u5e94AI\u7684\u65b0\u9886\u57df\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09344", "pdf": "https://arxiv.org/pdf/2508.09344", "abs": "https://arxiv.org/abs/2508.09344", "authors": ["Anushka Bhatt"], "title": "Blink-to-code: real-time Morse code communication via eye blink detection and classification", "categories": ["cs.CV", "68T45, 92C55", "H.5.2; I.2.10; J.3"], "comment": "4 pages, 4 figures. Preprint on blink-based Morse code communication\n  via webcam for assistive technology. Relevant to computer vision and\n  human-computer interaction", "summary": "This study proposes a real-time system that translates voluntary eye blinks\ninto Morse code, enabling communication for individuals with severe motor\nimpairments. Using a standard webcam and computer vision, the system detects\nand classifies blinks as short (dot) or long (dash), then decodes them into\nalphanumeric characters. Experiments with five participants show 62% decoding\naccuracy and 18-20 seconds response times, demonstrating a viable, low-cost\nassistive communication method.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u81ea\u613f\u7728\u773c\u5b9e\u65f6\u8f6c\u6362\u4e3a\u6469\u5c14\u65af\u7535\u7801\u7684\u7cfb\u7edf\uff0c\u65e8\u5728\u4e3a\u4e25\u91cd\u8fd0\u52a8\u969c\u788d\u8005\u63d0\u4f9b\u8f85\u52a9\u4ea4\u6d41\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e25\u91cd\u8fd0\u52a8\u969c\u788d\u8005\u63d0\u4f9b\u4e00\u79cd\u53ef\u884c\u7684\u4ea4\u6d41\u65b9\u6cd5\u3002", "method": "\u7cfb\u7edf\u5229\u7528\u6807\u51c6\u7f51\u7edc\u6444\u50cf\u5934\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\uff0c\u68c0\u6d4b\u5e76\u533a\u5206\u77ed\u7728\uff08\u70b9\uff09\u548c\u957f\u7728\uff08\u5212\uff09\uff0c\u968f\u540e\u5c06\u5176\u89e3\u7801\u4e3a\u5b57\u6bcd\u6570\u5b57\u5b57\u7b26\u3002", "result": "\u5bf9\u4e94\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u7cfb\u7edf\u89e3\u7801\u51c6\u786e\u7387\u4e3a62%\uff0c\u54cd\u5e94\u65f6\u95f4\u4e3a18-20\u79d2\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u8bc1\u660e\u662f\u4e00\u79cd\u53ef\u884c\u4e14\u4f4e\u6210\u672c\u7684\u8f85\u52a9\u4ea4\u6d41\u65b9\u6cd5\u3002"}}
{"id": "2508.09713", "pdf": "https://arxiv.org/pdf/2508.09713", "abs": "https://arxiv.org/abs/2508.09713", "authors": ["Rahul Hemrajani"], "title": "Evaluating the Role of Large Language Models in Legal Practice in India", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The integration of Artificial Intelligence(AI) into the legal profession\nraises significant questions about the capacity of Large Language Models(LLM)\nto perform key legal tasks. In this paper, I empirically evaluate how well\nLLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian\ncontext, including issue spotting, legal drafting, advice, research, and\nreasoning. Through a survey experiment, I compare outputs from LLMs with those\nof a junior lawyer, with advanced law students rating the work on helpfulness,\naccuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,\noften matching or surpassing human work. However, they struggle with\nspecialised legal research, frequently generating hallucinations, factually\nincorrect or fabricated outputs. I conclude that while LLMs can augment certain\nlegal tasks, human expertise remains essential for nuanced reasoning and the\nprecise application of law.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86LLM\u5728\u5370\u5ea6\u6cd5\u5f8b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5b83\u4eec\u64c5\u957f\u8d77\u8349\u548c\u95ee\u9898\u8bc6\u522b\uff0c\u4f46\u5728\u4e13\u4e1a\u6cd5\u5f8b\u7814\u7a76\u65b9\u9762\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u878d\u5165\u6cd5\u5f8b\u9886\u57df\uff0c\u672c\u6587\u65e8\u5728\u5b9e\u8bc1\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6267\u884c\u5173\u952e\u6cd5\u5f8b\u4efb\u52a1\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u8c03\u67e5\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u5c06GPT\u3001Claude\u548cLlama\u7b49LLM\u5728\u5370\u5ea6\u6cd5\u5f8b\u4efb\u52a1\uff08\u5305\u62ec\u95ee\u9898\u8bc6\u522b\u3001\u6cd5\u5f8b\u8d77\u8349\u3001\u5efa\u8bae\u3001\u7814\u7a76\u548c\u63a8\u7406\uff09\u4e2d\u7684\u8f93\u51fa\u4e0e\u521d\u7ea7\u5f8b\u5e08\u7684\u8f93\u51fa\u8fdb\u884c\u6bd4\u8f83\u3002\u9ad8\u7ea7\u6cd5\u5b66\u751f\u6839\u636e\u5b9e\u7528\u6027\u3001\u51c6\u786e\u6027\u548c\u5168\u9762\u6027\u5bf9\u4f5c\u54c1\u8fdb\u884c\u8bc4\u5206\u3002", "result": "LLM\u5728\u8d77\u8349\u548c\u95ee\u9898\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e38\u80fd\u4e0e\u4eba\u7c7b\u5de5\u4f5c\u5ab2\u7f8e\u6216\u8d85\u8d8a\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5728\u4e13\u4e1a\u6cd5\u5f8b\u7814\u7a76\u65b9\u9762\u8868\u73b0\u6323\u624e\uff0c\u9891\u7e41\u751f\u6210\u5e7b\u89c9\u3001\u4e8b\u5b9e\u4e0d\u51c6\u786e\u6216\u634f\u9020\u7684\u5185\u5bb9\u3002", "conclusion": "LLM\u53ef\u4ee5\u589e\u5f3a\u67d0\u4e9b\u6cd5\u5f8b\u4efb\u52a1\uff0c\u4f46\u4eba\u7c7b\u7684\u4e13\u4e1a\u77e5\u8bc6\u5728\u7ec6\u81f4\u7684\u63a8\u7406\u548c\u6cd5\u5f8b\u7684\u7cbe\u786e\u5e94\u7528\u65b9\u9762\u4ecd\u7136\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.09735", "pdf": "https://arxiv.org/pdf/2508.09735", "abs": "https://arxiv.org/abs/2508.09735", "authors": ["Jorge L\u00f3pez", "Charalampos Chatzinakis", "Marc Cartigny"], "title": "Route Planning and Online Routing for Quantum Key Distribution Networks", "categories": ["cs.NI", "cs.CR"], "comment": "Initial submission, 5 pages, 4 figures", "summary": "Quantum Key Distribution (QKD) networks harness the principles of quantum\nphysics in order to securely transmit cryptographic key material, providing\nphysical guarantees. These networks require traditional management and\noperational components, such as routing information through the network\nelements. However, due to the limitations on capacity and the particularities\nof information handling in these networks, traditional shortest paths\nalgorithms for routing perform poorly on both route planning and online\nrouting, which is counterintuitive. Moreover, due to the scarce resources in\nsuch networks, often the expressed demand cannot be met by any assignment of\nroutes. To address both the route planning problem and the need for fair\nautomated suggestions in infeasible cases, we propose to model this problem as\na Quadratic Programming (QP) problem. For the online routing problem, we\nshowcase that the shortest (available) paths routing strategy performs poorly\nin the online setting. Furthermore, we prove that the widest shortest path\nrouting strategy has a competitive ratio greater or equal than $\\frac{1}{2}$,\nefficiently addressing both routing modes in QKD networks.", "AI": {"tldr": "\u9488\u5bf9\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u7f51\u7edc\u4e2d\u4f20\u7edf\u8def\u7531\u7b97\u6cd5\u6548\u7387\u4f4e\u3001\u8d44\u6e90\u7a00\u7f3a\u5bfc\u81f4\u9700\u6c42\u65e0\u6cd5\u6ee1\u8db3\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u4e8c\u6b21\u89c4\u5212\uff08QP\uff09\u5efa\u6a21\u89e3\u51b3\u8def\u7531\u89c4\u5212\u548c\u4e0d\u53ef\u884c\u60c5\u51b5\u4e0b\u7684\u516c\u5e73\u5efa\u8bae\uff0c\u5e76\u8bc1\u660e\u6700\u5bbd\u6700\u77ed\u8def\u5f84\uff08WSP\uff09\u7b56\u7565\u5728\u5728\u7ebf\u8def\u7531\u4e2d\u5177\u6709\u4e0d\u4f4e\u4e8e1/2\u7684\u7ade\u4e89\u6bd4\uff0c\u6709\u6548\u5e94\u5bf9\u4e24\u79cd\u8def\u7531\u6a21\u5f0f\u3002", "motivation": "QKD\u7f51\u7edc\u9700\u8981\u8def\u7531\uff0c\u4f46\u7531\u4e8e\u5bb9\u91cf\u9650\u5236\u548c\u4fe1\u606f\u5904\u7406\u7684\u7279\u6b8a\u6027\uff0c\u4f20\u7edf\u6700\u77ed\u8def\u5f84\u7b97\u6cd5\u5728\u8def\u7531\u89c4\u5212\u548c\u5728\u7ebf\u8def\u7531\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u6b64\u5916\uff0cQKD\u7f51\u7edc\u8d44\u6e90\u7a00\u7f3a\uff0c\u5bfc\u81f4\u90e8\u5206\u9700\u6c42\u53ef\u80fd\u65e0\u6cd5\u88ab\u4efb\u4f55\u8def\u7531\u5206\u914d\u6ee1\u8db3\uff0c\u9700\u8981\u516c\u5e73\u7684\u81ea\u52a8\u5316\u5efa\u8bae\u3002", "method": "\u5c06\u8def\u7531\u89c4\u5212\u95ee\u9898\u548c\u4e0d\u53ef\u884c\u60c5\u51b5\u4e0b\u7684\u516c\u5e73\u81ea\u52a8\u5316\u5efa\u8bae\u5efa\u6a21\u4e3a\u4e8c\u6b21\u89c4\u5212\uff08QP\uff09\u95ee\u9898\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u5e76\u8bc1\u660e\u4e86\u6700\u5bbd\u6700\u77ed\u8def\u5f84\u8def\u7531\u7b56\u7565\uff0c\u5e76\u5206\u6790\u5176\u5728\u5728\u7ebf\u8def\u7531\u4e2d\u7684\u7ade\u4e89\u6bd4\u3002", "result": "\u4f20\u7edf\u6700\u77ed\uff08\u53ef\u7528\uff09\u8def\u5f84\u8def\u7531\u7b56\u7565\u5728\u5728\u7ebf\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u6700\u5bbd\u6700\u77ed\u8def\u5f84\uff08WSP\uff09\u8def\u7531\u7b56\u7565\u7684\u7ade\u4e89\u6bd4\u5927\u4e8e\u7b49\u4e8e1/2\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3QKD\u7f51\u7edc\u4e2d\u7684\u4e24\u79cd\u8def\u7531\u6a21\u5f0f\u3002", "conclusion": "\u9488\u5bf9QKD\u7f51\u7edc\u7684\u7279\u6b8a\u8def\u7531\u6311\u6218\uff0c\u4e8c\u6b21\u89c4\u5212\u6a21\u578b\u548c\u6700\u5bbd\u6700\u77ed\u8def\u5f84\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u540e\u8005\u5728\u5728\u7ebf\u8def\u7531\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u7ade\u4e89\u529b\uff0c\u6709\u6548\u5e94\u5bf9\u4e86\u7f51\u7edc\u4e2d\u7684\u4e24\u79cd\u8def\u7531\u6a21\u5f0f\u3002"}}
{"id": "2508.09180", "pdf": "https://arxiv.org/pdf/2508.09180", "abs": "https://arxiv.org/abs/2508.09180", "authors": ["Huifa Li", "Jie Fu", "Xinlin Zhuang", "Haolin Yang", "Xinpeng Ling", "Tong Cheng", "Haochen xue", "Imran Razzak", "Zhili Chen"], "title": "scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate cell type annotation is a crucial step in analyzing single-cell RNA\nsequencing (scRNA-seq) data, which provides valuable insights into cellular\nheterogeneity. However, due to the high dimensionality and prevalence of zero\nelements in scRNA-seq data, traditional clustering methods face significant\nstatistical and computational challenges. While some advanced methods use graph\nneural networks to model cell-cell relationships, they often depend on static\ngraph structures that are sensitive to noise and fail to capture the\nlong-tailed distribution inherent in single-cell populations.To address these\nlimitations, we propose scAGC, a single-cell clustering method that learns\nadaptive cell graphs with contrastive guidance. Our approach optimizes feature\nrepresentations and cell graphs simultaneously in an end-to-end manner.\nSpecifically, we introduce a topology-adaptive graph autoencoder that leverages\na differentiable Gumbel-Softmax sampling strategy to dynamically refine the\ngraph structure during training. This adaptive mechanism mitigates the problem\nof a long-tailed degree distribution by promoting a more balanced neighborhood\nstructure. To model the discrete, over-dispersed, and zero-inflated nature of\nscRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for\nrobust feature reconstruction. Furthermore, a contrastive learning objective is\nincorporated to regularize the graph learning process and prevent abrupt\nchanges in the graph topology, ensuring stability and enhancing convergence.\nComprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC\nconsistently outperforms other state-of-the-art methods, yielding the best NMI\nand ARI scores on 9 and 7 datasets, respectively.Our code is available at\nAnonymous Github.", "AI": {"tldr": "\u63d0\u51fascAGC\uff0c\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u56fe\u5b66\u4e60\u548c\u5bf9\u6bd4\u5f15\u5bfc\u7684\u5355\u7ec6\u80de\u805a\u7c7b\u65b9\u6cd5\uff0c\u6709\u6548\u514b\u670d\u4f20\u7edf\u548c\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u7ef4\u3001\u7a00\u758f\u53ca\u957f\u5c3e\u5206\u5e03\u5355\u7ec6\u80de\u6570\u636e\u65f6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u805a\u7c7b\u6027\u80fd\u3002", "motivation": "\u5728\u5355\u7ec6\u80deRNA\u6d4b\u5e8f(scRNA-seq)\u6570\u636e\u5206\u6790\u4e2d\uff0c\u51c6\u786e\u7684\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0cscRNA-seq\u6570\u636e\u7684\u9ad8\u7ef4\u5ea6\u548c\u5927\u91cf\u96f6\u5143\u7d20\u5bf9\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u6784\u6210\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\u5e38\u4f9d\u8d56\u9759\u6001\u56fe\u7ed3\u6784\uff0c\u5bf9\u566a\u58f0\u654f\u611f\u4e14\u96be\u4ee5\u6355\u6349\u5355\u7ec6\u80de\u7fa4\u4f53\u7684\u957f\u5c3e\u5206\u5e03\u3002", "method": "\u672c\u6587\u63d0\u51fascAGC\uff0c\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5355\u7ec6\u80de\u805a\u7c7b\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u6bd4\u5f15\u5bfc\u5b66\u4e60\u81ea\u9002\u5e94\u7ec6\u80de\u56fe\uff0c\u5e76\u540c\u6b65\u4f18\u5316\u7279\u5f81\u8868\u793a\u548c\u56fe\u7ed3\u6784\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u4e2a\u62d3\u6251\u81ea\u9002\u5e94\u56fe\u81ea\u7f16\u7801\u5668\uff0c\u5229\u7528\u53ef\u5fae\u5206\u7684Gumbel-Softmax\u91c7\u6837\u7b56\u7565\u52a8\u6001\u4f18\u5316\u56fe\u7ed3\u6784\uff0c\u4ee5\u7f13\u89e3\u957f\u5c3e\u5ea6\u5206\u5e03\u95ee\u9898\u3002\u540c\u65f6\uff0c\u96c6\u6210\u96f6\u81a8\u80c0\u8d1f\u4e8c\u9879\u5f0f\uff08ZINB\uff09\u635f\u5931\u6765\u9c81\u68d2\u5730\u91cd\u5efa\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u4ee5\u6b63\u5219\u5316\u56fe\u5b66\u4e60\u8fc7\u7a0b\uff0c\u786e\u4fdd\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6027\u3002", "result": "\u57289\u4e2a\u771f\u5b9e\u7684scRNA-seq\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cscAGC\u6301\u7eed\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5206\u522b\u57289\u4e2a\u548c7\u4e2a\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e86\u6700\u4f73\u7684NMI\u548cARI\u5206\u6570\u3002", "conclusion": "scAGC\u901a\u8fc7\u521b\u65b0\u7684\u81ea\u9002\u5e94\u56fe\u5b66\u4e60\u548c\u9c81\u68d2\u7684\u7279\u5f81\u5efa\u6a21\uff0c\u6709\u6548\u514b\u670d\u4e86\u5355\u7ec6\u80de\u805a\u7c7b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u51c6\u786e\u7684\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f18\u8d8a\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09362", "pdf": "https://arxiv.org/pdf/2508.09362", "abs": "https://arxiv.org/abs/2508.09362", "authors": ["Md. Milon Islam", "Md Rezwanul Haque", "S M Taslim Uddin Raju", "Fakhri Karray"], "title": "FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted for the IEEE/CVF International Conference on Computer Vision\n  (ICCV), Honolulu, Hawaii, USA. 1st MSLR Workshop 2025", "summary": "Accurate recognition of sign language in healthcare communication poses a\nsignificant challenge, requiring frameworks that can accurately interpret\ncomplex multimodal gestures. To deal with this, we propose FusionEnsemble-Net,\na novel attention-based ensemble of spatiotemporal networks that dynamically\nfuses visual and motion data to enhance recognition accuracy. The proposed\napproach processes RGB video and range Doppler map radar modalities\nsynchronously through four different spatiotemporal networks. For each network,\nfeatures from both modalities are continuously fused using an attention-based\nfusion module before being fed into an ensemble of classifiers. Finally, the\noutputs of these four different fused channels are combined in an ensemble\nclassification head, thereby enhancing the model's robustness. Experiments\ndemonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches\nwith a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for\nItalian Sign Language. Our findings indicate that an ensemble of diverse\nspatiotemporal networks, unified by attention-based fusion, yields a robust and\naccurate framework for complex, multimodal isolated gesture recognition tasks.\nThe source code is available at:\nhttps://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.", "AI": {"tldr": "\u63d0\u51faFusionEnsemble-Net\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\uff08RGB\u89c6\u9891\u548c\u96f7\u8fbe\uff09\u7684\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u4e0e\u65f6\u7a7a\u7f51\u7edc\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u7597\u5065\u5eb7\u573a\u666f\u4e0b\u624b\u8bed\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u533b\u7597\u5065\u5eb7\u4ea4\u6d41\u4e2d\u624b\u8bed\u7684\u51c6\u786e\u8bc6\u522b\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u9700\u8981\u80fd\u591f\u7cbe\u786e\u89e3\u91ca\u590d\u6742\u591a\u6a21\u6001\u624b\u52bf\u7684\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86FusionEnsemble-Net\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u65f6\u7a7a\u7f51\u7edc\u96c6\u6210\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u540c\u6b65\u5904\u7406RGB\u89c6\u9891\u548c\u96f7\u8fbe\u591a\u666e\u52d2\u56fe\u4e24\u79cd\u6a21\u6001\u6570\u636e\uff0c\u901a\u8fc7\u56db\u4e2a\u4e0d\u540c\u7684\u65f6\u7a7a\u7f51\u7edc\u3002\u6bcf\u4e2a\u7f51\u7edc\u5185\u90e8\uff0c\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u6a21\u5757\u6301\u7eed\u878d\u5408\u4e24\u79cd\u6a21\u6001\u7684\u7279\u5f81\uff0c\u968f\u540e\u9001\u5165\u5206\u7c7b\u5668\u96c6\u6210\u3002\u6700\u540e\uff0c\u5c06\u56db\u4e2a\u4e0d\u540c\u878d\u5408\u901a\u9053\u7684\u8f93\u51fa\u5728\u4e00\u4e2a\u96c6\u6210\u5206\u7c7b\u5934\u4e2d\u7ed3\u5408\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFusionEnsemble-Net\u5728\u5927\u578bMultiMeDaLIS\u610f\u5927\u5229\u624b\u8bed\u6570\u636e\u96c6\u4e0a\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe\u523099.44%\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u7531\u591a\u6837\u5316\u65f6\u7a7a\u7f51\u7edc\u7ec4\u6210\u7684\u96c6\u6210\u6a21\u578b\uff0c\u7ed3\u5408\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u591a\u6a21\u6001\u878d\u5408\uff0c\u4e3a\u590d\u6742\u3001\u591a\u6a21\u6001\u7684\u72ec\u7acb\u624b\u52bf\u8bc6\u522b\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u4e14\u51c6\u786e\u7684\u6846\u67b6\u3002"}}
{"id": "2508.09716", "pdf": "https://arxiv.org/pdf/2508.09716", "abs": "https://arxiv.org/abs/2508.09716", "authors": ["Ridwan Mahbub", "Mohammed Saidul Islam", "Md Tahmid Rahman Laskar", "Mizanur Rahman", "Mir Tafseer Nayeem", "Enamul Hoque"], "title": "The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models", "categories": ["cs.CL"], "comment": "Accepted to IEEE VIS 2025", "summary": "Information visualizations are powerful tools that help users quickly\nidentify patterns, trends, and outliers, facilitating informed decision-making.\nHowever, when visualizations incorporate deceptive design elements-such as\ntruncated or inverted axes, unjustified 3D effects, or violations of best\npractices-they can mislead viewers and distort understanding, spreading\nmisinformation. While some deceptive tactics are obvious, others subtly\nmanipulate perception while maintaining a facade of legitimacy. As\nVision-Language Models (VLMs) are increasingly used to interpret\nvisualizations, especially by non-expert users, it is critical to understand\nhow susceptible these models are to deceptive visual designs. In this study, we\nconduct an in-depth evaluation of VLMs' ability to interpret misleading\nvisualizations. By analyzing over 16,000 responses from ten different models\nacross eight distinct types of misleading chart designs, we demonstrate that\nmost VLMs are deceived by them. This leads to altered interpretations of\ncharts, despite the underlying data remaining the same. Our findings highlight\nthe need for robust safeguards in VLMs against visual misinformation.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5bf9\u8bef\u5bfc\u6027\u4fe1\u606f\u56fe\u8868\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u53d1\u73b0\u5927\u591a\u6570VLMs\u4f1a\u88ab\u6b3a\u9a97\uff0c\u5bfc\u81f4\u5bf9\u76f8\u540c\u6570\u636e\u7684\u9519\u8bef\u89e3\u8bfb\u3002", "motivation": "\u4fe1\u606f\u53ef\u89c6\u5316\u53ef\u80fd\u5305\u542b\u6b3a\u9a97\u6027\u8bbe\u8ba1\u5143\u7d20\uff0c\u4f20\u64ad\u9519\u8bef\u4fe1\u606f\u3002\u9274\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u6b63\u88ab\u5e7f\u6cdb\u7528\u4e8e\u89e3\u91ca\u53ef\u89c6\u5316\u56fe\u8868\uff0c\u4e86\u89e3\u5b83\u4eec\u5bf9\u6b3a\u9a97\u6027\u89c6\u89c9\u8bbe\u8ba1\u7684\u654f\u611f\u5ea6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6df1\u5165\u8bc4\u4f30VLMs\u89e3\u91ca\u8bef\u5bfc\u6027\u53ef\u89c6\u5316\u56fe\u8868\u7684\u80fd\u529b\uff0c\u5206\u6790\u4e86\u6765\u81ea\u5341\u79cd\u4e0d\u540c\u6a21\u578b\u768416,000\u591a\u4e2a\u54cd\u5e94\uff0c\u6db5\u76d6\u516b\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u8bef\u5bfc\u6027\u56fe\u8868\u8bbe\u8ba1\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5927\u591a\u6570VLMs\u4f1a\u88ab\u8bef\u5bfc\u6027\u56fe\u8868\u6b3a\u9a97\uff0c\u5bfc\u81f4\u5bf9\u56fe\u8868\u7684\u89e3\u91ca\u53d1\u751f\u6539\u53d8\uff0c\u5373\u4f7f\u5e95\u5c42\u6570\u636e\u76f8\u540c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728VLMs\u4e2d\u5efa\u7acb\u5f3a\u5927\u7684\u9632\u8303\u673a\u5236\u4ee5\u5e94\u5bf9\u89c6\u89c9\u9519\u8bef\u4fe1\u606f\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2508.09756", "pdf": "https://arxiv.org/pdf/2508.09756", "abs": "https://arxiv.org/abs/2508.09756", "authors": ["Mauro De Sanctis"], "title": "The Paradigm of Massive Wireless Human Sensing: Concept, Architecture and Challenges", "categories": ["cs.NI", "C.2.0"], "comment": null, "summary": "This article is a position paper which introduces the paradigm of ``Massive\nWireless Human Sensing'', i.e. an infrastructure for wireless human sensing\nbased on a plethora of heterogeneous wireless communication signals. More\nspecifically, we aim to exploit signal diversity in the time, frequency, and\nspace domains using opportunistically both device-free and device-based\nwireless sensing approaches, with the objective of enhancing human sensing\ncapabilities in terms of accuracy and service availability over different\nenvironments. The enabling element of this concept is the massive wireless\nhuman sensing edge device, that is, an embedded system acting as a\nmulti-technology and multi-approach RF receiver with feature extraction\nfunctionality, located within the monitoring area or at its borders. In this\nframework, architecture solutions and challenges are discussed to lead the\nfuture development of this new paradigm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u5927\u89c4\u6a21\u65e0\u7ebf\u4eba\u4f53\u611f\u77e5\u201d\u65b0\u8303\u5f0f\uff0c\u65e8\u5728\u5229\u7528\u5f02\u6784\u65e0\u7ebf\u4fe1\u53f7\u63d0\u5347\u4eba\u4f53\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u4ecb\u7ecd\u5176\u6838\u5fc3\u8fb9\u7f18\u8bbe\u5907\u3001\u67b6\u6784\u53ca\u6311\u6218\u3002", "motivation": "\u901a\u8fc7\u5229\u7528\u65f6\u95f4\u548c\u7a7a\u95f4\u57df\u7684\u4fe1\u53f7\u591a\u6837\u6027\uff0c\u7ed3\u5408\u65e0\u8bbe\u5907\u548c\u57fa\u4e8e\u8bbe\u5907\u7684\u65e0\u7ebf\u611f\u77e5\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e0d\u540c\u73af\u5883\u4e0b\u4eba\u4f53\u611f\u77e5\u7684\u51c6\u786e\u6027\u548c\u670d\u52a1\u53ef\u7528\u6027\u3002", "method": "\u5f15\u5165\u201c\u5927\u89c4\u6a21\u65e0\u7ebf\u4eba\u4f53\u611f\u77e5\u201d\u8303\u5f0f\uff0c\u5176\u6838\u5fc3\u662f\u201c\u5927\u89c4\u6a21\u65e0\u7ebf\u4eba\u4f53\u611f\u77e5\u8fb9\u7f18\u8bbe\u5907\u201d\uff0c\u8be5\u8bbe\u5907\u662f\u4e00\u4e2a\u591a\u6280\u672f\u3001\u591a\u65b9\u6cd5\u7684\u5c04\u9891\u63a5\u6536\u5d4c\u5165\u5f0f\u7cfb\u7edf\uff0c\u5177\u5907\u7279\u5f81\u63d0\u53d6\u529f\u80fd\u3002\u6587\u7ae0\u8ba8\u8bba\u4e86\u5b9e\u73b0\u6b64\u8303\u5f0f\u7684\u67b6\u6784\u89e3\u51b3\u65b9\u6848\u548c\u76f8\u5173\u6311\u6218\u3002", "result": "\u672c\u6587\u6210\u529f\u63d0\u51fa\u5e76\u8be6\u7ec6\u9610\u8ff0\u4e86\u201c\u5927\u89c4\u6a21\u65e0\u7ebf\u4eba\u4f53\u611f\u77e5\u201d\u8fd9\u4e00\u521b\u65b0\u8303\u5f0f\uff0c\u660e\u786e\u4e86\u5176\u5b9e\u73b0\u6982\u5ff5\u3001\u5173\u952e\u4f7f\u80fd\u6280\u672f\uff08\u8fb9\u7f18\u8bbe\u5907\uff09\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u6307\u51fa\u4e86\u6f5c\u5728\u7684\u67b6\u6784\u548c\u6311\u6218\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u65e0\u7ebf\u4eba\u4f53\u611f\u77e5\u9886\u57df\u63cf\u7ed8\u4e86\u4e00\u4e2a\u5229\u7528\u5927\u89c4\u6a21\u5f02\u6784\u65e0\u7ebf\u4fe1\u53f7\u7684\u65b0\u613f\u666f\uff0c\u5f3a\u8c03\u4e86\u4e13\u7528\u8fb9\u7f18\u8bbe\u5907\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u8be5\u8303\u5f0f\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u6982\u5ff5\u6027\u6307\u5bfc\u548c\u6311\u6218\u5206\u6790\u3002"}}
{"id": "2508.09181", "pdf": "https://arxiv.org/pdf/2508.09181", "abs": "https://arxiv.org/abs/2508.09181", "authors": ["Jinghong Tan", "Zhian Liu", "Kun Guo", "Mingxiong Zhao"], "title": "Long-Term Client Selection for Federated Learning with Non-IID Data: A Truthful Auction Approach", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Federated learning (FL) provides a decentralized framework that enables\nuniversal model training through collaborative efforts on mobile nodes, such as\nsmart vehicles in the Internet of Vehicles (IoV). Each smart vehicle acts as a\nmobile client, contributing to the process without uploading local data. This\nmethod leverages non-independent and identically distributed (non-IID) training\ndata from different vehicles, influenced by various driving patterns and\nenvironmental conditions, which can significantly impact model convergence and\naccuracy. Although client selection can be a feasible solution for non-IID\nissues, it faces challenges related to selection metrics. Traditional metrics\nevaluate client data quality independently per round and require client\nselection after all clients complete local training, leading to resource\nwastage from unused training results. In the IoV context, where vehicles have\nlimited connectivity and computational resources, information asymmetry in\nclient selection risks clients submitting false information, potentially making\nthe selection ineffective. To tackle these challenges, we propose a novel\nLong-term Client-Selection Federated Learning based on Truthful Auction\n(LCSFLA). This scheme maximizes social welfare with consideration of long-term\ndata quality using a new assessment mechanism and energy costs, and the advised\nauction mechanism with a deposit requirement incentivizes client participation\nand ensures information truthfulness. We theoretically prove the incentive\ncompatibility and individual rationality of the advised incentive mechanism.\nExperimental results on various datasets, including those from IoV scenarios,\ndemonstrate its effectiveness in mitigating performance degradation caused by\nnon-IID data.", "AI": {"tldr": "\u9488\u5bf9\u8054\u90a6\u5b66\u4e60\u5728\u8f66\u8054\u7f51\u4e2d\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5bfc\u81f4\u7684\u6027\u80fd\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faLCSFLA\u65b9\u6848\uff0c\u901a\u8fc7\u5f15\u5165\u957f\u671f\u5ba2\u6237\u7aef\u9009\u62e9\u548c\u771f\u5b9e\u62cd\u5356\u673a\u5236\uff0c\u4f18\u5316\u5ba2\u6237\u7aef\u9009\u62e9\u5e76\u786e\u4fdd\u4fe1\u606f\u771f\u5b9e\u6027\uff0c\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u8f66\u8054\u7f51\uff08IoV\uff09\u4e2d\u9762\u4e34\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u5bfc\u81f4\u7684\u6a21\u578b\u6536\u655b\u548c\u7cbe\u5ea6\u95ee\u9898\u3002\u73b0\u6709\u5ba2\u6237\u7aef\u9009\u62e9\u65b9\u6cd5\u5b58\u5728\u8d44\u6e90\u6d6a\u8d39\uff08\u9700\u5728\u672c\u5730\u8bad\u7ec3\u5b8c\u6210\u540e\u9009\u62e9\uff09\u548c\u4fe1\u606f\u4e0d\u5bf9\u79f0\uff08\u5ba2\u6237\u63d0\u4ea4\u865a\u5047\u4fe1\u606f\uff09\u7684\u6311\u6218\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684IoV\u73af\u5883\u4e0b\u5c24\u4e3a\u7a81\u51fa\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u771f\u5b9e\u62cd\u5356\u7684\u957f\u671f\u5ba2\u6237\u7aef\u9009\u62e9\u8054\u90a6\u5b66\u4e60\u65b9\u6848\uff08LCSFLA\uff09\u3002\u8be5\u65b9\u6848\u901a\u8fc7\u65b0\u7684\u8bc4\u4f30\u673a\u5236\u7efc\u5408\u8003\u8651\u957f\u671f\u6570\u636e\u8d28\u91cf\u548c\u80fd\u6e90\u6210\u672c\uff0c\u4ee5\u6700\u5927\u5316\u793e\u4f1a\u798f\u5229\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u5e26\u6709\u5b58\u6b3e\u8981\u6c42\u7684\u62cd\u5356\u673a\u5236\uff0c\u4ee5\u6fc0\u52b1\u5ba2\u6237\u7aef\u53c2\u4e0e\u5e76\u786e\u4fdd\u5176\u63d0\u4f9b\u4fe1\u606f\u7684\u771f\u5b9e\u6027\u3002\u8be5\u6fc0\u52b1\u673a\u5236\u7684\u6fc0\u52b1\u517c\u5bb9\u6027\u548c\u4e2a\u4f53\u7406\u6027\u5f97\u5230\u4e86\u7406\u8bba\u8bc1\u660e\u3002", "result": "\u5728\u5305\u62ecIoV\u573a\u666f\u5728\u5185\u7684\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLCSFLA\u65b9\u6848\u80fd\u6709\u6548\u7f13\u89e3\u7531\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5f15\u8d77\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "conclusion": "LCSFLA\u901a\u8fc7\u521b\u65b0\u7684\u957f\u671f\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u548c\u771f\u5b9e\u62cd\u5356\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u5728IoV\u73af\u5883\u4e0b\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5e26\u6765\u7684\u6027\u80fd\u6311\u6218\uff0c\u5e76\u514b\u670d\u4e86\u4f20\u7edf\u5ba2\u6237\u7aef\u9009\u62e9\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.09372", "pdf": "https://arxiv.org/pdf/2508.09372", "abs": "https://arxiv.org/abs/2508.09372", "authors": ["Md Rezwanul Haque", "Md. Milon Islam", "S M Taslim Uddin Raju", "Fakhri Karray"], "title": "A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG"], "comment": "Accepted for the IEEE/CVF International Conference on Computer Vision\n  (ICCV), Honolulu, Hawaii, USA. 1st MSLR Workshop 2025", "summary": "Continuous Sign Language Recognition (CSLR) faces multiple challenges,\nincluding significant inter-signer variability and poor generalization to novel\nsentence structures. Traditional solutions frequently fail to handle these\nissues efficiently. For overcoming these constraints, we propose a\ndual-architecture framework. For the Signer-Independent (SI) challenge, we\npropose a Signer-Invariant Conformer that combines convolutions with multi-head\nself-attention to learn robust, signer-agnostic representations from pose-based\nskeletal keypoints. For the Unseen-Sentences (US) task, we designed a\nMulti-Scale Fusion Transformer with a novel dual-path temporal encoder that\ncaptures both fine-grained posture dynamics, enabling the model's ability to\ncomprehend novel grammatical compositions. Experiments on the challenging\nIsharah-1000 dataset establish a new standard for both CSLR benchmarks. The\nproposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on\nthe SI challenge, a reduction of 13.53% from the state-of-the-art. On the US\ntask, the transformer model scores a WER of 47.78%, surpassing previous work.\nIn the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th\nin the SI task, demonstrating the performance of these models. The findings\nvalidate our key hypothesis: that developing task-specific networks designed\nfor the particular challenges of CSLR leads to considerable performance\nimprovements and establishes a new baseline for further research. The source\ncode is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u53cc\u67b6\u6784\u6846\u67b6\uff0c\u5305\u62ecSigner-Invariant Conformer\u548cMulti-Scale Fusion Transformer\uff0c\u65e8\u5728\u89e3\u51b3\u8fde\u7eed\u624b\u8bed\u8bc6\u522b\uff08CSLR\uff09\u4e2d\u624b\u8bed\u8005\u5dee\u5f02\u548c\u5bf9\u672a\u89c1\u53e5\u5b50\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u5e76\u5728Isharah-1000\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u7ed3\u679c\uff0c\u4e3aCSLR\u7814\u7a76\u8bbe\u7acb\u4e86\u65b0\u57fa\u7ebf\u3002", "motivation": "\u8fde\u7eed\u624b\u8bed\u8bc6\u522b\uff08CSLR\uff09\u9762\u4e34\u624b\u8bed\u8005\u95f4\u663e\u8457\u5dee\u5f02\u548c\u5bf9\u65b0\u53e5\u5b50\u7ed3\u6784\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u6311\u6218\uff0c\u800c\u4f20\u7edf\u89e3\u51b3\u65b9\u6848\u96be\u4ee5\u6709\u6548\u5e94\u5bf9\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u53cc\u67b6\u6784\u6846\u67b6\uff1a1. \u9488\u5bf9\u624b\u8bed\u8005\u65e0\u5173\u6027\uff08SI\uff09\u6311\u6218\uff0c\u8bbe\u8ba1Signer-Invariant Conformer\uff0c\u7ed3\u5408\u5377\u79ef\u4e0e\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ece\u59ff\u6001\u9aa8\u9abc\u5173\u952e\u70b9\u4e2d\u5b66\u4e60\u9c81\u68d2\u3001\u624b\u8bed\u8005\u65e0\u5173\u7684\u8868\u793a\u30022. \u9488\u5bf9\u672a\u89c1\u53e5\u5b50\uff08US\uff09\u4efb\u52a1\uff0c\u8bbe\u8ba1Multi-Scale Fusion Transformer\uff0c\u5176\u5305\u542b\u65b0\u9896\u7684\u53cc\u8def\u5f84\u65f6\u95f4\u7f16\u7801\u5668\uff0c\u80fd\u6355\u6349\u7ec6\u7c92\u5ea6\u59ff\u6001\u52a8\u6001\u3002", "result": "\u5728Isharah-1000\u6570\u636e\u96c6\u4e0a\uff0cSI\u6311\u6218\u4e2d\u7684Conformer\u67b6\u6784\u5b9e\u73b0\u4e8613.07%\u7684\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\uff0c\u8f83\u73b0\u6709\u6700\u4f73\u6c34\u5e73\u964d\u4f4e13.53%\u3002US\u4efb\u52a1\u4e2d\u7684Transformer\u6a21\u578bWER\u4e3a47.78%\uff0c\u8d85\u8d8a\u4e86\u4ee5\u5f80\u5de5\u4f5c\u3002\u5728SignEval 2025 CSLR\u6311\u6218\u4e2d\uff0c\u8be5\u56e2\u961f\u5728US\u4efb\u52a1\u4e2d\u6392\u540d\u7b2c2\uff0c\u5728SI\u4efb\u52a1\u4e2d\u6392\u540d\u7b2c4\u3002", "conclusion": "\u4e3aCSLR\u7684\u7279\u5b9a\u6311\u6218\u5f00\u53d1\u4efb\u52a1\u4e13\u7528\u7f51\u7edc\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u7ebf\u3002"}}
{"id": "2508.09726", "pdf": "https://arxiv.org/pdf/2508.09726", "abs": "https://arxiv.org/abs/2508.09726", "authors": ["Vaishnavi Shrivastava", "Ahmed Awadallah", "Vidhisha Balachandran", "Shivam Garg", "Harkirat Behl", "Dimitris Papailiopoulos"], "title": "Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models trained with reinforcement learning with verifiable\nrewards tend to trade accuracy for length--inflating response lengths to\nachieve gains in accuracy. While longer answers may be warranted for harder\nproblems, many tokens are merely \"filler\": repetitive, verbose text that makes\nno real progress. We introduce GFPO (Group Filtered Policy Optimization), which\ncurbs this length explosion by sampling larger groups per problem during\ntraining and filtering responses to train on based on two key metrics: (1)\nresponse length and (2) token efficiency: reward per token ratio. By sampling\nmore at training time, we teach models to think less at inference time. On the\nPhi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across\nchallenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,\nLiveCodeBench) while maintaining accuracy. Optimizing for reward per token\nfurther increases reductions in length inflation to 71-85%. We also propose\nAdaptive Difficulty GFPO, which dynamically allocates more training resources\nto harder problems based on real-time difficulty estimates, improving the\nbalance between computational efficiency and accuracy especially on difficult\nquestions. GFPO demonstrates that increased training-time compute directly\ntranslates to reduced test-time compute--a simple yet effective trade-off for\nefficient reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGFPO\uff08Group Filtered Policy Optimization\uff09\uff0c\u901a\u8fc7\u8bad\u7ec3\u65f6\u8fc7\u6ee4\u548c\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u54cd\u5e94\u5197\u957f\u7684\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u7684\u8f93\u51fa\u957f\u5ea6\u5e76\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ecf\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u540e\uff0c\u503e\u5411\u4e8e\u4ee5\u589e\u52a0\u54cd\u5e94\u957f\u5ea6\u6765\u6362\u53d6\u51c6\u786e\u6027\uff0c\u5bfc\u81f4\u8f93\u51fa\u4e2d\u5305\u542b\u5927\u91cf\u91cd\u590d\u3001\u5197\u4f59\u7684\u201c\u586b\u5145\u201d\u6587\u672c\uff0c\u964d\u4f4e\u4e86\u6548\u7387\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86GFPO\uff08Group Filtered Policy Optimization\uff09\u3002\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u4e3a\u6bcf\u4e2a\u95ee\u9898\u91c7\u6837\u66f4\u5927\u7684\u54cd\u5e94\u7ec4\uff0c\u5e76\u6839\u636e\u54cd\u5e94\u957f\u5ea6\u548c\u4ee4\u724c\u6548\u7387\uff08\u6bcf\u4ee4\u724c\u5956\u52b1\u6bd4\u7387\uff09\u8fc7\u6ee4\u8fd9\u4e9b\u54cd\u5e94\u4ee5\u8fdb\u884c\u8bad\u7ec3\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u5728\u8bad\u7ec3\u65f6\u8fdb\u884c\u66f4\u591a\u7684\u91c7\u6837\uff0c\u4ece\u800c\u4f7f\u6a21\u578b\u5728\u63a8\u7406\u65f6\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u601d\u8003\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u96be\u5ea6GFPO\uff0c\u6839\u636e\u5b9e\u65f6\u96be\u5ea6\u4f30\u8ba1\u52a8\u6001\u5206\u914d\u8bad\u7ec3\u8d44\u6e90\u7ed9\u66f4\u56f0\u96be\u7684\u95ee\u9898\u3002", "result": "\u5728Phi-4-reasoning\u6a21\u578b\u4ee5\u53caAIME 24/25\u3001GPQA\u3001Omni-MATH\u548cLiveCodeBench\u7b49\u5177\u6709\u6311\u6218\u6027\u7684STEM\u548c\u7f16\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGFPO\u5c06GRPO\u7684\u957f\u5ea6\u81a8\u80c0\u7387\u964d\u4f4e\u4e8646-71%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002\u901a\u8fc7\u8fdb\u4e00\u6b65\u4f18\u5316\u6bcf\u4e2a\u4ee4\u724c\u7684\u5956\u52b1\uff0c\u957f\u5ea6\u81a8\u80c0\u7387\u53ef\u8fbe\u523071-85%\u3002\u81ea\u9002\u5e94\u96be\u5ea6GFPO\u5728\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u96be\u9898\u65f6\u3002", "conclusion": "GFPO\u8bc1\u660e\u4e86\u589e\u52a0\u8bad\u7ec3\u65f6\u7684\u8ba1\u7b97\u91cf\u53ef\u4ee5\u76f4\u63a5\u8f6c\u5316\u4e3a\u51cf\u5c11\u6d4b\u8bd5\u65f6\u7684\u8ba1\u7b97\u91cf\uff0c\u8fd9\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u63a8\u7406\uff0c\u63d0\u5347\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.09769", "pdf": "https://arxiv.org/pdf/2508.09769", "abs": "https://arxiv.org/abs/2508.09769", "authors": ["Simon Egger", "Robin Laidig", "Heiko Geppert", "Lucas Haug", "Jona Herrmann", "Frank D\u00fcrr", "Christian Becker"], "title": "An (m,k)-firm Elevation Policy to Increase the Robustness of Time-Driven Schedules in 5G Time-Sensitive Networks", "categories": ["cs.NI"], "comment": "23 pages, 10 figures", "summary": "Current standardization efforts are advancing the integration of 5G and\nTime-Sensitive Networking (TSN) to facilitate the deployment of safety-critical\nindustrial applications that require real-time communication. However, there\nremains a fundamental disconnect between the probabilistic 5G delay\ncharacteristics and the often idealistic delay models used to synthesize 5G-TSN\nnetwork configurations. For time-driven schedules in particular, any delay\noutlier unforeseen during schedule synthesis can jeopardize the robustness of\ntheir real-time guarantees. To address this challenge, we present the\n(m,k)-firm Elevation Policy to uphold a base level of weakly hard real-time\nguarantees during unstable network conditions that do not match the expected\ndelay characteristics. It augments the primary time-driven schedule with a\ndynamic priority-driven scheme to elevate the priority of m out of k\nconsecutive frames if they are delayed. Our evaluations demonstrate that weakly\nhard real-time guarantees are essential to uphold the quality of control within\na networked control system. At the same time, only a small overhead is imposed\nwhen the primary schedule can provide stronger quality of service guarantees.\nOur (m,k)-firm Elevation Policy thereby yields a robust but light-weight\nfallback mechanism to serve applications with meaningful guarantees during\nunstable network conditions.", "AI": {"tldr": "\u4e3a\u89e3\u51b35G-TSN\u7f51\u7edc\u4e2d5G\u968f\u673a\u5ef6\u8fdf\u4e0e\u7406\u60f3\u5316\u8c03\u5ea6\u6a21\u578b\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd(m,k)-firm\u63d0\u5347\u7b56\u7565\u3002\u8be5\u7b56\u7565\u901a\u8fc7\u52a8\u6001\u4f18\u5148\u7ea7\u63d0\u5347\uff0c\u5728\u7f51\u7edc\u4e0d\u7a33\u5b9a\u65f6\u63d0\u4f9b\u5f31\u786c\u5b9e\u65f6\u4fdd\u8bc1\uff0c\u786e\u4fdd\u5173\u952e\u5e94\u7528\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d5G\u4e0eTSN\u7684\u96c6\u6210\u9762\u4e34\u6311\u6218\uff1a5G\u7684\u6982\u7387\u6027\u5ef6\u8fdf\u7279\u6027\u4e0e\u7528\u4e8e\u7f51\u7edc\u914d\u7f6e\u7684\u7406\u60f3\u5316\u5ef6\u8fdf\u6a21\u578b\u5b58\u5728\u6839\u672c\u6027\u5dee\u5f02\u3002\u8fd9\u5bfc\u81f4\u65f6\u95f4\u9a71\u52a8\u8c03\u5ea6\u4e2d\u7684\u5ef6\u8fdf\u5f02\u5e38\u53ef\u80fd\u5371\u53ca\u5b9e\u65f6\u901a\u4fe1\u7684\u9c81\u68d2\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86(m,k)-firm\u63d0\u5347\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u901a\u8fc7\u5728\u4e3b\u65f6\u95f4\u9a71\u52a8\u8c03\u5ea6\u4e4b\u4e0a\u589e\u52a0\u52a8\u6001\u4f18\u5148\u7ea7\u9a71\u52a8\u673a\u5236\uff0c\u5f53\u8fde\u7eedk\u4e2a\u5e27\u4e2d\u6709\u5e27\u88ab\u5ef6\u8fdf\u65f6\uff0c\u63d0\u5347\u5176\u4e2dm\u4e2a\u5e27\u7684\u4f18\u5148\u7ea7\uff0c\u4ece\u800c\u5728\u7f51\u7edc\u4e0d\u7a33\u5b9a\u65f6\u7ef4\u6301\u57fa\u7840\u7684\u5f31\u786c\u5b9e\u65f6\u4fdd\u8bc1\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u5f31\u786c\u5b9e\u65f6\u4fdd\u8bc1\u5bf9\u4e8e\u7ef4\u62a4\u7f51\u7edc\u5316\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u63a7\u5236\u8d28\u91cf\u81f3\u5173\u91cd\u8981\u3002\u540c\u65f6\uff0c\u5f53\u4e3b\u8c03\u5ea6\u65b9\u6848\u80fd\u63d0\u4f9b\u66f4\u5f3a\u7684\u670d\u52a1\u8d28\u91cf\u4fdd\u8bc1\u65f6\uff0c\u672c\u7b56\u7565\u4ec5\u5f15\u5165\u5f88\u5c0f\u7684\u989d\u5916\u5f00\u9500\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684(m,k)-firm\u63d0\u5347\u7b56\u7565\u662f\u4e00\u79cd\u9c81\u68d2\u4e14\u8f7b\u91cf\u7ea7\u7684\u5907\u7528\u673a\u5236\uff0c\u80fd\u591f\u5728\u7f51\u7edc\u6761\u4ef6\u4e0d\u7a33\u5b9a\u65f6\u4e3a\u5e94\u7528\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u4fdd\u8bc1\u3002"}}
{"id": "2508.09187", "pdf": "https://arxiv.org/pdf/2508.09187", "abs": "https://arxiv.org/abs/2508.09187", "authors": ["Almustapha A. Wakili", "Babajide J. Asaju", "Woosub Jung"], "title": "Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring", "categories": ["cs.LG"], "comment": null, "summary": "Breath analysis has emerged as a critical tool in health monitoring, offering\ninsights into respiratory function, disease detection, and continuous health\nassessment. While traditional contact-based methods are reliable, they often\npose challenges in comfort and practicality, particularly for long-term\nmonitoring. This survey comprehensively examines contact-based and contactless\napproaches, emphasizing recent advances in machine learning and deep learning\ntechniques applied to breath analysis. Contactless methods, including Wi-Fi\nChannel State Information and acoustic sensing, are analyzed for their ability\nto provide accurate, noninvasive respiratory monitoring. We explore a broad\nrange of applications, from single-user respiratory rate detection to\nmulti-user scenarios, user identification, and respiratory disease detection.\nFurthermore, this survey details essential data preprocessing, feature\nextraction, and classification techniques, offering comparative insights into\nmachine learning/deep learning models suited to each approach. Key challenges\nlike dataset scarcity, multi-user interference, and data privacy are also\ndiscussed, along with emerging trends like Explainable AI, federated learning,\ntransfer learning, and hybrid modeling. By synthesizing current methodologies\nand identifying open research directions, this survey offers a comprehensive\nframework to guide future innovations in breath analysis, bridging advanced\ntechnological capabilities with practical healthcare applications.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u5168\u9762\u5ba1\u89c6\u4e86\u57fa\u4e8e\u63a5\u89e6\u5f0f\u548c\u975e\u63a5\u89e6\u5f0f\u547c\u5438\u5206\u6790\u6280\u672f\uff0c\u91cd\u70b9\u5173\u6ce8\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u5728\u5176\u4e2d\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u4ece\u547c\u5438\u901f\u7387\u68c0\u6d4b\u5230\u75be\u75c5\u8bca\u65ad\u7684\u5e7f\u6cdb\u5e94\u7528\u573a\u666f\uff0c\u5e76\u6307\u51fa\u4e86\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u63a5\u89e6\u5f0f\u547c\u5438\u5206\u6790\u65b9\u6cd5\u5728\u8212\u9002\u6027\u548c\u5b9e\u7528\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u4e0d\u9002\u7528\u4e8e\u957f\u671f\u76d1\u6d4b\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u975e\u4fb5\u5165\u6027\u3001\u66f4\u4fbf\u6377\u7684\u547c\u5438\u76d1\u6d4b\u6280\u672f\uff0c\u4ee5\u63d0\u5347\u5065\u5eb7\u76d1\u6d4b\u80fd\u529b\u3002", "method": "\u672c\u6587\u91c7\u7528\u7efc\u8ff0\u5f62\u5f0f\uff0c\u5168\u9762\u5206\u6790\u4e86\u63a5\u89e6\u5f0f\u548c\u975e\u63a5\u89e6\u5f0f\uff08\u5982Wi-Fi\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u548c\u58f0\u5b66\u4f20\u611f\uff09\u547c\u5438\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u6df1\u5165\u63a2\u8ba8\u4e86\u5e94\u7528\u4e8e\u8fd9\u4e9b\u65b9\u6cd5\u7684\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u3001\u7279\u5f81\u63d0\u53d6\u548c\u5206\u7c7b\u6280\u672f\u3002", "result": "\u7efc\u8ff0\u5206\u6790\u4e86\u975e\u63a5\u89e6\u5f0f\u65b9\u6cd5\u5728\u63d0\u4f9b\u51c6\u786e\u3001\u65e0\u521b\u547c\u5438\u76d1\u6d4b\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u63a2\u7d22\u4e86\u4ece\u5355\u7528\u6237\u547c\u5438\u901f\u7387\u68c0\u6d4b\u5230\u591a\u7528\u6237\u573a\u666f\u3001\u7528\u6237\u8bc6\u522b\u548c\u547c\u5438\u75be\u75c5\u68c0\u6d4b\u7b49\u5e7f\u6cdb\u5e94\u7528\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e0d\u540c\u673a\u5668\u5b66\u4e60/\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6bd4\u8f83\u6027\u89c1\u89e3\uff0c\u5e76\u8ba8\u8bba\u4e86\u6570\u636e\u96c6\u7a00\u7f3a\u3001\u591a\u7528\u6237\u5e72\u6270\u3001\u6570\u636e\u9690\u79c1\u7b49\u5173\u952e\u6311\u6218\u4ee5\u53ca\u53ef\u89e3\u91caAI\u3001\u8054\u90a6\u5b66\u4e60\u3001\u8fc1\u79fb\u5b66\u4e60\u3001\u6df7\u5408\u5efa\u6a21\u7b49\u65b0\u5174\u8d8b\u52bf\u3002", "conclusion": "\u672c\u7efc\u8ff0\u901a\u8fc7\u6574\u5408\u73b0\u6709\u65b9\u6cd5\u5e76\u8bc6\u522b\u5f00\u653e\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u547c\u5438\u5206\u6790\u9886\u57df\u7684\u672a\u6765\u521b\u65b0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u6846\u67b6\uff0c\u65e8\u5728\u5c06\u5148\u8fdb\u6280\u672f\u80fd\u529b\u4e0e\u5b9e\u9645\u533b\u7597\u4fdd\u5065\u5e94\u7528\u76f8\u7ed3\u5408\u3002"}}
{"id": "2508.09381", "pdf": "https://arxiv.org/pdf/2508.09381", "abs": "https://arxiv.org/abs/2508.09381", "authors": ["Kumar Abhishek", "Jeremy Kawahara", "Ghassan Hamarneh"], "title": "What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Medical Image Computing and Computer-Assisted Intervention (MICCAI)\n  ISIC Skin Image Analysis Workshop (MICCAI ISIC) 2025; 12 pages, 4 tables, 3\n  figures", "summary": "Medical image segmentation exhibits intra- and inter-annotator variability\ndue to ambiguous object boundaries, annotator preferences, expertise, and\ntools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated\nor infiltrative nodules, or irregular borders per the ABCD rule, are\nparticularly prone to disagreement and are often associated with malignancy. In\nthis work, we curate IMA++, the largest multi-annotator skin lesion\nsegmentation dataset, on which we conduct an in-depth study of variability due\nto annotator, malignancy, tool, and skill factors. We find a statistically\nsignificant (p<0.001) association between inter-annotator agreement (IAA),\nmeasured using Dice, and the malignancy of skin lesions. We further show that\nIAA can be accurately predicted directly from dermoscopic images, achieving a\nmean absolute error of 0.108. Finally, we leverage this association by\nutilizing IAA as a \"soft\" clinical feature within a multi-task learning\nobjective, yielding a 4.2% improvement in balanced accuracy averaged across\nmultiple model architectures and across IMA++ and four public dermoscopic\ndatasets. The code is available at https://github.com/sfu-mial/skin-IAV.", "AI": {"tldr": "\u7814\u7a76\u533b\u7597\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u6807\u6ce8\u8005\u53d8\u5f02\u6027\uff0c\u6784\u5efa\u591a\u6807\u6ce8\u8005\u6570\u636e\u96c6IMA++\uff0c\u53d1\u73b0\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u4e0e\u75c5\u53d8\u6076\u6027\u7a0b\u5ea6\u663e\u8457\u76f8\u5173\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u8f6f\u7279\u5f81\u63d0\u5347\u4e86\u76ae\u80a4\u75c5\u53d8\u5206\u5272\u548c\u5206\u7c7b\u7684\u6027\u80fd\u3002", "motivation": "\u533b\u7597\u56fe\u50cf\u5206\u5272\u4e2d\u5b58\u5728\u663e\u8457\u7684\u6807\u6ce8\u8005\u95f4/\u5185\u53d8\u5f02\u6027\uff0c\u5c24\u5176\u662f\u5728\u8fb9\u754c\u6a21\u7cca\u6216\u6076\u6027\u75c5\u53d8\u5904\u3002\u8fd9\u79cd\u53d8\u5f02\u6027\u5f71\u54cd\u5206\u5272\u8d28\u91cf\uff0c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u63a2\u7a76\u5176\u6210\u56e0\u4e0e\u5f71\u54cd\uff0c\u5e76\u5229\u7528\u5176\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u6784\u5efa\u4e86\u5927\u578b\u591a\u6807\u6ce8\u8005\u76ae\u80a4\u75c5\u53d8\u5206\u5272\u6570\u636e\u96c6IMA++\u3002\u5728\u6b64\u6570\u636e\u96c6\u4e0a\uff0c\u6df1\u5165\u7814\u7a76\u4e86\u6807\u6ce8\u8005\u3001\u6076\u6027\u7a0b\u5ea6\u3001\u5de5\u5177\u548c\u6280\u80fd\u7b49\u56e0\u7d20\u5bfc\u81f4\u7684\u53d8\u5f02\u6027\u3002\u8fdb\u800c\uff0c\u4ece\u76ae\u80a4\u955c\u56fe\u50cf\u4e2d\u9884\u6d4b\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\uff08IAA\uff09\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u201c\u8f6f\u201d\u4e34\u5e8a\u7279\u5f81\u6574\u5408\u5230\u591a\u4efb\u52a1\u5b66\u4e60\u76ee\u6807\u4e2d\u3002", "result": "\u53d1\u73b0\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\uff08IAA\uff09\u4e0e\u76ae\u80a4\u75c5\u53d8\u6076\u6027\u7a0b\u5ea6\u5b58\u5728\u7edf\u8ba1\u5b66\u4e0a\u663e\u8457\uff08p<0.001\uff09\u7684\u5173\u8054\u3002IAA\u53ef\u4ee5\u4ece\u76ae\u80a4\u955c\u56fe\u50cf\u4e2d\u51c6\u786e\u9884\u6d4b\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a0.108\u3002\u5229\u7528IAA\u4f5c\u4e3a\u7279\u5f81\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u8861\u51c6\u786e\u7387\u5e73\u5747\u63d0\u9ad8\u4e864.2%\u3002", "conclusion": "\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u4e0e\u76ae\u80a4\u75c5\u53d8\u6076\u6027\u7a0b\u5ea6\u5bc6\u5207\u76f8\u5173\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u201c\u8f6f\u201d\u4e34\u5e8a\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u533b\u7597\u56fe\u50cf\uff08\u7279\u522b\u662f\u76ae\u80a4\u75c5\u53d8\uff09\u5206\u5272\u548c\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2508.09755", "pdf": "https://arxiv.org/pdf/2508.09755", "abs": "https://arxiv.org/abs/2508.09755", "authors": ["Seokgi Lee"], "title": "Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "We introduce a novel retrieval-augmented generation (RAG) framework tailored\nfor multihop question answering. First, our system uses large language model\n(LLM) to decompose complex multihop questions into a sequence of single-hop\nsubquestions that guide document retrieval. This decomposition mitigates the\nambiguity inherent in multi-hop queries by clearly targeting distinct knowledge\nfacets. Second, instead of embedding raw or chunked documents directly, we\ngenerate answerable questions from each document chunk using Qwen3-8B, embed\nthese generated questions, and retrieve relevant chunks via question-question\nembedding similarity. During inference, the retrieved chunks are then fed along\nwith the original question into the RAG pipeline. We evaluate on three multihop\nquestion datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our\nmethod improves RAG performacne compared to baseline systems. Our contributions\nhighlight the benefits of using answerable-question embeddings for RAG, and the\neffectiveness of LLM-based query decomposition for multihop scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u8df3\u95ee\u7b54\u7684\u65b0\u578b\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u95ee\u9898\u5206\u89e3\u548c\u751f\u6210\u53ef\u56de\u7b54\u7684\u95ee\u9898\u5d4c\u5165\u8fdb\u884c\u6587\u6863\u68c0\u7d22\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RAG\u6846\u67b6\u5728\u5904\u7406\u590d\u6742\u591a\u8df3\u95ee\u9898\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u591a\u8df3\u67e5\u8be2\u56fa\u6709\u7684\u6a21\u7cca\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u6307\u5bfc\u6587\u6863\u68c0\u7d22\u5e76\u63d0\u5347\u95ee\u7b54\u6027\u80fd\u3002", "method": "1. \u5229\u7528LLM\u5c06\u590d\u6742\u591a\u8df3\u95ee\u9898\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u5355\u8df3\u5b50\u95ee\u9898\uff0c\u4ee5\u6307\u5bfc\u6587\u6863\u68c0\u7d22\u30022. \u4e0d\u76f4\u63a5\u5d4c\u5165\u539f\u59cb\u6587\u6863\u5757\uff0c\u800c\u662f\u4f7f\u7528Qwen3-8B\u4ece\u6bcf\u4e2a\u6587\u6863\u5757\u751f\u6210\u53ef\u56de\u7b54\u7684\u95ee\u9898\uff0c\u5d4c\u5165\u8fd9\u4e9b\u751f\u6210\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u95ee\u9898-\u95ee\u9898\u5d4c\u5165\u76f8\u4f3c\u6027\u68c0\u7d22\u76f8\u5173\u6587\u6863\u5757\u30023. \u5c06\u68c0\u7d22\u5230\u7684\u6587\u6863\u5757\u4e0e\u539f\u59cb\u95ee\u9898\u4e00\u540c\u9001\u5165RAG\u7ba1\u9053\u3002\u5728MuSiQue\u30012WikiMultiHopQa\u3001HotpotQA\u7b49LongBench\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e0e\u57fa\u7ebf\u7cfb\u7edf\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86RAG\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u6210\u679c\u7a81\u51fa\u663e\u793a\u4e86\u4f7f\u7528\u53ef\u56de\u7b54\u95ee\u9898\u5d4c\u5165\u5bf9RAG\u7684\u76ca\u5904\uff0c\u4ee5\u53ca\u57fa\u4e8eLLM\u7684\u67e5\u8be2\u5206\u89e3\u5728\u591a\u8df3\u95ee\u7b54\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.09839", "pdf": "https://arxiv.org/pdf/2508.09839", "abs": "https://arxiv.org/abs/2508.09839", "authors": ["Muhammad Asad Ullah", "Luca Borgianni", "Heikki Kokkinen", "Antti Anttonen", "Stefano Giordano"], "title": "A First Look at Starlink In-Flight Performance: An Intercontinental Empirical Study", "categories": ["cs.NI"], "comment": "This work has been submitted to the 2025 IEEE Global Communications\n  Conference (GLOBECOM). Copyright to IEEE may be transferred without notice", "summary": "Starlink delivers Internet services to users across terrestrial, maritime,\nand aviation domains. The prior works have studied its performance at fixed\nsites and in-motion vehicles, while an in-depth analysis of in-flight\nperformance remains absent. With major airlines now offering Starlink Internet\nonboard, there is a growing need to evaluate and improve its performance for\naviation users. This paper addresses this shortcoming by conducting in-flight\nmeasurements over the Baltic Sea and the Pacific Ocean. Our measurement results\nshow that a single user device experiences median throughputs of 64 Mbps and 24\nMbps for the downlink and uplink, respectively. The median uplink throughput is\napproximately 33 Mbps when the aircraft maintains an altitude above 17,000\nfeet. However, a significant reduction in uplink performance is observed during\nthe aircraft descent phase, with the median throughput dropping to around 20\nMbps at lower altitudes. Round-trip time (RTT) is highly dependent on the\nlocation of the ground station being pinged and the use of inter-satellite\nlinks (ISLs). We dive deeper into 5.5 hours of ping measurements collected over\nthe Pacific Ocean and investigate factors influencing RTT, hypothesizing that\nISLs routing, data queuing at satellites, and feeder link congestion contribute\nto deviations from theoretical values. For comparative analysis, we evaluate\nthe Starlink ground terminal and in-flight connectivity performance from the\nperspectives of a residential user and an airline passenger, respectively.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u6df1\u5165\u6d4b\u91cf\u5e76\u5206\u6790\u4e86\u661f\u94fe\u822a\u7a7a\u4e92\u8054\u7f51\u670d\u52a1\u5728\u98de\u884c\u4e2d\u7684\u6027\u80fd\uff0c\u5305\u62ec\u541e\u5410\u91cf\u548c\u5f80\u8fd4\u65f6\u5ef6\uff08RTT\uff09\uff0c\u5e76\u63a2\u8ba8\u4e86\u5f71\u54cd\u56e0\u7d20\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u661f\u94fe\u673a\u8f7d\u6027\u80fd\u7684\u6df1\u5165\u5206\u6790\uff0c\u4f46\u4e3b\u8981\u822a\u7a7a\u516c\u53f8\u5df2\u5f00\u59cb\u63d0\u4f9b\u661f\u94fe\u670d\u52a1\uff0c\u4e9f\u9700\u8bc4\u4f30\u5e76\u6539\u8fdb\u5176\u5bf9\u822a\u7a7a\u7528\u6237\u7684\u6027\u80fd\u3002", "method": "\u5728\u6ce2\u7f57\u7684\u6d77\u548c\u592a\u5e73\u6d0b\u4e0a\u7a7a\u8fdb\u884c\u4e86\u98de\u884c\u4e2d\u7684\u5b9e\u6d4b\uff0c\u6536\u96c6\u4e86\u5355\u7528\u6237\u541e\u5410\u91cf\u6570\u636e\u3002\u6df1\u5165\u5206\u6790\u4e865.5\u5c0f\u65f6\u7684\u5f80\u8fd4\u65f6\u5ef6\uff08RTT\uff09\u6d4b\u91cf\u6570\u636e\uff0c\u5e76\u63d0\u51faISL\u8def\u7531\u3001\u536b\u661f\u6570\u636e\u6392\u961f\u548c\u9988\u7ebf\u94fe\u8def\u62e5\u585e\u7b49\u53ef\u80fd\u5f71\u54cdRTT\u7684\u56e0\u7d20\u3002\u540c\u65f6\uff0c\u8fdb\u884c\u4e86\u4e0e\u4f4f\u5b85\u7528\u6237\u7684\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u5355\u7528\u6237\u4e0b\u884c\u4e2d\u4f4d\u6570\u541e\u5410\u91cf\u4e3a64 Mbps\uff0c\u4e0a\u884c\u4e3a24 Mbps\u3002\u5728\u9ad8\u7a7a\uff0817,000\u82f1\u5c3a\u4ee5\u4e0a\uff09\u65f6\u4e0a\u884c\u4e2d\u4f4d\u6570\u541e\u5410\u91cf\u7ea6\u4e3a33 Mbps\uff0c\u4f46\u5728\u98de\u673a\u4e0b\u964d\u9636\u6bb5\u4f4e\u6d77\u62d4\u65f6\u663e\u8457\u964d\u81f3\u7ea620 Mbps\u3002\u5f80\u8fd4\u65f6\u5ef6\uff08RTT\uff09\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5730\u9762\u7ad9\u4f4d\u7f6e\u548c\u661f\u95f4\u94fe\u8def\uff08ISLs\uff09\u7684\u4f7f\u7528\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u98de\u884c\u5b9e\u6d4b\u5f25\u8865\u4e86\u661f\u94fe\u673a\u8f7d\u6027\u80fd\u5206\u6790\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5176\u541e\u5410\u91cf\u548cRTT\u7684\u5173\u952e\u6027\u80fd\u6307\u6807\uff0c\u5e76\u521d\u6b65\u8bc6\u522b\u51fa\u5f71\u54cd\u8fd9\u4e9b\u6307\u6807\u7684\u56e0\u7d20\uff0c\u4e3a\u4f18\u5316\u822a\u7a7a\u4e92\u8054\u7f51\u670d\u52a1\u63d0\u4f9b\u4e86\u91cd\u8981\u6570\u636e\u548c\u89c1\u89e3\u3002"}}
{"id": "2508.09190", "pdf": "https://arxiv.org/pdf/2508.09190", "abs": "https://arxiv.org/abs/2508.09190", "authors": ["Bing Han", "Feifei Zhao", "Dongcheng Zhao", "Guobin Shen", "Ping Wu", "Yu Shi", "Yi Zeng"], "title": "Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fine-tuning as service injects domain-specific knowledge into large language\nmodels (LLMs), while challenging the original alignment mechanisms and\nintroducing safety risks. A series of defense strategies have been proposed for\nthe alignment, fine-tuning, and post-fine-tuning phases, where most\npost-fine-tuning defenses rely on coarse-grained safety layer mapping. These\nmethods lack a comprehensive consideration of both safety layers and\nfine-grained neurons, limiting their ability to efficiently balance safety and\nutility. To address this, we propose the Fine-Grained Safety Neurons (FGSN)\nwith Training-Free Continual Projection method to reduce the fine-tuning safety\nrisks. FGSN inherently integrates the multi-scale interactions between safety\nlayers and neurons, localizing sparser and more precise fine-grained safety\nneurons while minimizing interference with downstream task neurons. We then\nproject the safety neuron parameters onto safety directions, improving model\nsafety while aligning more closely with human preferences. Extensive\nexperiments across multiple fine-tuned LLM models demonstrate that our method\nsignificantly reduce harmfulness scores and attack success rates with minimal\nparameter modifications, while preserving the model's utility. Furthermore, by\nintroducing a task-specific, multi-dimensional heterogeneous safety neuron\ncluster optimization mechanism, we achieve continual defense and generalization\ncapability against unforeseen emerging safety concerns.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u7ec6\u7c92\u5ea6\u5b89\u5168\u795e\u7ecf\u5143\uff08FGSN\uff09\u4e0e\u514d\u8bad\u7ec3\u6301\u7eed\u6295\u5f71\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u786e\u5b9a\u4f4d\u548c\u8c03\u6574\u5b89\u5168\u795e\u7ecf\u5143\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6548\u7528\u548c\u6301\u7eed\u9632\u5fa1\u80fd\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5fae\u8c03\u670d\u52a1\u5728\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\u7684\u540c\u65f6\uff0c\u6311\u6218\u4e86\u5176\u539f\u6709\u5bf9\u9f50\u673a\u5236\u5e76\u5e26\u6765\u4e86\u5b89\u5168\u98ce\u9669\u3002\u73b0\u6709\u7684\u540e\u5fae\u8c03\u9632\u5fa1\u7b56\u7565\u591a\u4f9d\u8d56\u7c97\u7c92\u5ea6\u7684\u5b89\u5168\u5c42\u6620\u5c04\uff0c\u672a\u80fd\u5168\u9762\u517c\u987e\u5b89\u5168\u5c42\u4e0e\u7ec6\u7c92\u5ea6\u795e\u7ecf\u5143\uff0c\u5bfc\u81f4\u96be\u4ee5\u6709\u6548\u5e73\u8861\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6\u5b89\u5168\u795e\u7ecf\u5143\uff08FGSN\uff09\u4e0e\u514d\u8bad\u7ec3\u6301\u7eed\u6295\u5f71\u65b9\u6cd5\uff0c\u4ee5\u964d\u4f4e\u5fae\u8c03\u5b89\u5168\u98ce\u9669\u3002FGSN\u5185\u5728\u6574\u5408\u4e86\u5b89\u5168\u5c42\u4e0e\u795e\u7ecf\u5143\u4e4b\u95f4\u7684\u591a\u5c3a\u5ea6\u4ea4\u4e92\uff0c\u80fd\u591f\u5b9a\u4f4d\u66f4\u7a00\u758f\u3001\u66f4\u7cbe\u786e\u7684\u7ec6\u7c92\u5ea6\u5b89\u5168\u795e\u7ecf\u5143\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u4e0b\u6e38\u4efb\u52a1\u795e\u7ecf\u5143\u7684\u5e72\u6270\u3002\u968f\u540e\uff0c\u6211\u4eec\u5c06\u5b89\u5168\u795e\u7ecf\u5143\u53c2\u6570\u6295\u5f71\u5230\u5b89\u5168\u65b9\u5411\u4e0a\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u5b89\u5168\u6027\u5e76\u4f7f\u5176\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5f15\u5165\u4efb\u52a1\u7279\u5f02\u6027\u3001\u591a\u7ef4\u5f02\u6784\u5b89\u5168\u795e\u7ecf\u5143\u7c07\u4f18\u5316\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u6301\u7eed\u9632\u5fa1\u548c\u5bf9\u672a\u77e5\u65b0\u5174\u5b89\u5168\u95ee\u9898\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u5fae\u8c03LLM\u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6781\u5c11\u53c2\u6570\u4fee\u6539\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6709\u5bb3\u6027\u5206\u6570\u548c\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u5b8c\u6574\u4fdd\u7559\u4e86\u6a21\u578b\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "FGSN\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7f13\u89e3\u5fae\u8c03LLMs\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\uff0c\u901a\u8fc7\u7cbe\u7ec6\u5316\u5b9a\u4f4d\u548c\u6295\u5f71\u5b89\u5168\u795e\u7ecf\u5143\uff0c\u5728\u786e\u4fdd\u6a21\u578b\u5b9e\u7528\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\uff0c\u5e76\u5177\u5907\u6301\u7eed\u9632\u5fa1\u4e0e\u6cdb\u5316\u80fd\u529b\uff0c\u4e3aLLM\u7684\u5b89\u5168\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09383", "pdf": "https://arxiv.org/pdf/2508.09383", "abs": "https://arxiv.org/abs/2508.09383", "authors": ["Guoxian Song", "Hongyi Xu", "Xiaochen Zhao", "You Xie", "Tianpei Gu", "Zenan Li", "Chenxu Zhang", "Linjie Luo"], "title": "X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present X-UniMotion, a unified and expressive implicit latent\nrepresentation for whole-body human motion, encompassing facial expressions,\nbody poses, and hand gestures. Unlike prior motion transfer methods that rely\non explicit skeletal poses and heuristic cross-identity adjustments, our\napproach encodes multi-granular motion directly from a single image into a\ncompact set of four disentangled latent tokens -- one for facial expression,\none for body pose, and one for each hand. These motion latents are both highly\nexpressive and identity-agnostic, enabling high-fidelity, detailed\ncross-identity motion transfer across subjects with diverse identities, poses,\nand spatial configurations. To achieve this, we introduce a self-supervised,\nend-to-end framework that jointly learns the motion encoder and latent\nrepresentation alongside a DiT-based video generative model, trained on\nlarge-scale, diverse human motion datasets. Motion-identity disentanglement is\nenforced via 2D spatial and color augmentations, as well as synthetic 3D\nrenderings of cross-identity subject pairs under shared poses. Furthermore, we\nguide motion token learning with auxiliary decoders that promote fine-grained,\nsemantically aligned, and depth-aware motion embeddings. Extensive experiments\nshow that X-UniMotion outperforms state-of-the-art methods, producing highly\nexpressive animations with superior motion fidelity and identity preservation.", "AI": {"tldr": "X-UniMotion\u662f\u4e00\u79cd\u7edf\u4e00\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u5168\u8eab\u4eba\u4f53\u8fd0\u52a8\u9690\u5f0f\u6f5c\u5728\u8868\u793a\uff0c\u80fd\u591f\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u8de8\u8eab\u4efd\u7684\u5168\u8eab\u8fd0\u52a8\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u8fc1\u79fb\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u9aa8\u9abc\u59ff\u6001\u548c\u542f\u53d1\u5f0f\u8c03\u6574\uff0c\u96be\u4ee5\u5b9e\u73b0\u9488\u5bf9\u9762\u90e8\u8868\u60c5\u3001\u8eab\u4f53\u59ff\u6001\u548c\u624b\u52bf\u7b49\u5168\u8eab\u591a\u7c92\u5ea6\u8fd0\u52a8\u7684\u9ad8\u4fdd\u771f\u3001\u7cbe\u7ec6\u5316\u3001\u8de8\u8eab\u4efd\u8fc1\u79fb\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86X-UniMotion\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5c06\u5355\u5f20\u56fe\u50cf\u4e2d\u7684\u591a\u7c92\u5ea6\u8fd0\u52a8\u7f16\u7801\u4e3a\u56db\u4e2a\u89e3\u8026\u7684\u6f5c\u5728\u4ee4\u724c\uff08\u9762\u90e8\u3001\u8eab\u4f53\u3001\u4e24\u53ea\u624b\uff09\uff0c\u8fd9\u4e9b\u4ee4\u724c\u5177\u6709\u8868\u8fbe\u529b\u4e14\u4e0e\u8eab\u4efd\u65e0\u5173\u3002\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8eDiT\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u3002\u901a\u8fc72D\u7a7a\u95f4\u548c\u989c\u8272\u589e\u5f3a\uff0c\u4ee5\u53ca\u5171\u4eab\u59ff\u6001\u4e0b\u8de8\u8eab\u4efd\u4e3b\u4f53\u5bf9\u7684\u5408\u62103D\u6e32\u67d3\uff0c\u5f3a\u5236\u5b9e\u73b0\u8fd0\u52a8\u4e0e\u8eab\u4efd\u89e3\u8026\uff0c\u5e76\u5229\u7528\u8f85\u52a9\u89e3\u7801\u5668\u5f15\u5bfc\u7cbe\u7ec6\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u6df1\u5ea6\u611f\u77e5\u7684\u8fd0\u52a8\u5d4c\u5165\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cX-UniMotion\u8d85\u8d8a\u4e86\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8868\u73b0\u529b\u3001\u5353\u8d8a\u8fd0\u52a8\u4fdd\u771f\u5ea6\u548c\u8eab\u4efd\u4fdd\u6301\u7684\u52a8\u753b\u6548\u679c\u3002", "conclusion": "X-UniMotion\u901a\u8fc7\u5176\u521b\u65b0\u7684\u7edf\u4e00\u3001\u89e3\u8026\u9690\u5f0f\u6f5c\u5728\u8868\u793a\uff0c\u4e3a\u9ad8\u4fdd\u771f\u3001\u7ec6\u8282\u4e30\u5bcc\u7684\u5168\u8eab\u8de8\u8eab\u4efd\u8fd0\u52a8\u8fc1\u79fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09759", "pdf": "https://arxiv.org/pdf/2508.09759", "abs": "https://arxiv.org/abs/2508.09759", "authors": ["Avneet Kaur"], "title": "Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "There have been numerous studies evaluating bias of LLMs towards political\ntopics. However, how positions towards these topics in model outputs are highly\nsensitive to the prompt. What happens when the prompt itself is suggestive of\ncertain arguments towards those positions remains underexplored. This is\ncrucial for understanding how robust these bias evaluations are and for\nunderstanding model behaviour, as these models frequently interact with\nopinionated text. To that end, we conduct experiments for political bias\nevaluation in presence of supporting and refuting arguments. Our experiments\nshow that such arguments substantially alter model responses towards the\ndirection of the provided argument in both single-turn and multi-turn settings.\nMoreover, we find that the strength of these arguments influences the\ndirectional agreement rate of model responses. These effects point to a\nsycophantic tendency in LLMs adapting their stance to align with the presented\narguments which has downstream implications for measuring political bias and\ndeveloping effective mitigation strategies.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u653f\u6cbb\u8bdd\u9898\u7684\u7acb\u573a\u6781\u6613\u53d7\u63d0\u793a\u4e2d\u8bba\u636e\u7684\u5f71\u54cd\uff0c\u8868\u73b0\u51fa\u4e0e\u6240\u63d0\u4f9b\u8bba\u636e\u5bf9\u9f50\u7684\u201c\u8c04\u5a9a\u201d\u503e\u5411\uff0c\u8fd9\u5bf9\u504f\u89c1\u8bc4\u4f30\u6709\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u5bf9LLM\u653f\u6cbb\u504f\u89c1\u7684\u8bc4\u4f30\u672a\u5145\u5206\u63a2\u7a76\u5f53\u63d0\u793a\u672c\u8eab\u5305\u542b\u652f\u6301\u6216\u53cd\u9a73\u6027\u8bba\u636e\u65f6\u6a21\u578b\u884c\u4e3a\u7684\u654f\u611f\u6027\u3002\u7406\u89e3\u8fd9\u79cd\u654f\u611f\u6027\u5bf9\u4e8e\u786e\u4fdd\u504f\u89c1\u8bc4\u4f30\u7684\u9c81\u68d2\u6027\u548c\u7406\u89e3\u6a21\u578b\u4e0e\u5e26\u6709\u503e\u5411\u6027\u6587\u672c\u4ea4\u4e92\u65f6\u7684\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\uff0c\u5728\u5b58\u5728\u652f\u6301\u6027\u548c\u53cd\u9a73\u6027\u8bba\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9LLM\u8fdb\u884c\u653f\u6cbb\u504f\u89c1\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8bba\u636e\u80fd\u663e\u8457\u6539\u53d8\u6a21\u578b\u54cd\u5e94\u65b9\u5411\uff0c\u65e0\u8bba\u662f\u5728\u5355\u8f6e\u8fd8\u662f\u591a\u8f6e\u5bf9\u8bdd\u4e2d\uff1b\u6b64\u5916\uff0c\u8bba\u636e\u7684\u5f3a\u5ea6\u4e5f\u4f1a\u5f71\u54cd\u6a21\u578b\u54cd\u5e94\u7684\u65b9\u5411\u4e00\u81f4\u6027\u3002", "conclusion": "LLMs\u8868\u73b0\u51fa\u201c\u8c04\u5a9a\u201d\u503e\u5411\uff0c\u5176\u7acb\u573a\u4f1a\u9002\u5e94\u5e76\u4e0e\u6240\u63d0\u4f9b\u7684\u8bba\u636e\u5bf9\u9f50\u3002\u8fd9\u79cd\u884c\u4e3a\u5bf9\u8861\u91cf\u653f\u6cbb\u504f\u89c1\u548c\u5f00\u53d1\u6709\u6548\u7684\u7f13\u89e3\u7b56\u7565\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\u3002"}}
{"id": "2508.09140", "pdf": "https://arxiv.org/pdf/2508.09140", "abs": "https://arxiv.org/abs/2508.09140", "authors": ["Honggang Jia", "Nan Cheng", "Xiucheng Wang", "Conghao Zhou", "Ruijin Sun", "Xuemin", "Shen"], "title": "RadioMamba: Breaking the Accuracy-Efficiency Trade-off in Radio Map Construction via a Hybrid Mamba-UNet", "categories": ["eess.SP", "cs.LG", "cs.NI"], "comment": null, "summary": "Radio map (RM) has recently attracted much attention since it can provide\nreal-time and accurate spatial channel information for 6G services and\napplications. However, current deep learning-based methods for RM construction\nexhibit well known accuracy-efficiency trade-off. In this paper, we introduce\nRadioMamba, a hybrid Mamba-UNet architecture for RM construction to address the\ntrade-off. Generally, accurate RM construction requires modeling long-range\nspatial dependencies, reflecting the global nature of wave propagation physics.\nRadioMamba utilizes a Mamba-Convolutional block where the Mamba branch captures\nthese global dependencies with linear complexity, while a parallel\nconvolutional branch extracts local features. This hybrid design generates\nfeature representations that capture both global context and local detail.\nExperiments show that RadioMamba achieves higher accuracy than existing\nmethods, including diffusion models, while operating nearly 20 times faster and\nusing only 2.9\\% of the model parameters. By improving both accuracy and\nefficiency, RadioMamba presents a viable approach for real-time intelligent\noptimization in next generation wireless systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRadioMamba\uff0c\u4e00\u79cd\u7ed3\u5408Mamba\u548cUNet\u7684\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4e8e\u65e0\u7ebf\u7535\u5730\u56fe(RM)\u6784\u5efa\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cRadioMamba\u5728\u63d0\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u884c\u6548\u7387\u5e76\u51cf\u5c11\u4e86\u6a21\u578b\u53c2\u6570\u91cf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65e0\u7ebf\u7535\u5730\u56fe(RM)\u6784\u5efa\u65b9\u6cd5\u5b58\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u96be\u4ee5\u540c\u65f6\u6ee1\u8db36G\u670d\u52a1\u5bf9\u5b9e\u65f6\u3001\u7cbe\u786e\u7a7a\u95f4\u4fe1\u9053\u4fe1\u606f\u7684\u9700\u6c42\u3002", "method": "\u7814\u7a76\u5f15\u5165RadioMamba\uff0c\u4e00\u4e2a\u6df7\u5408Mamba-UNet\u67b6\u6784\u3002\u8be5\u67b6\u6784\u5229\u7528Mamba-\u5377\u79ef\u5757\uff0c\u5176\u4e2dMamba\u5206\u652f\u4ee5\u7ebf\u6027\u590d\u6742\u5ea6\u6355\u83b7\u957f\u8ddd\u79bb\u7a7a\u95f4\u4f9d\u8d56\uff08\u5168\u5c40\u7279\u5f81\uff09\uff0c\u800c\u5e76\u884c\u5377\u79ef\u5206\u652f\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\uff0c\u4ece\u800c\u6709\u6548\u7ed3\u5408\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cRadioMamba\u5728\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5305\u62ec\u6269\u6563\u6a21\u578b\uff09\uff0c\u8fd0\u884c\u901f\u5ea6\u5feb\u8fd120\u500d\uff0c\u4e14\u6a21\u578b\u53c2\u6570\u91cf\u4ec5\u4e3a2.9%\u3002", "conclusion": "RadioMamba\u901a\u8fc7\u540c\u65f6\u63d0\u5347\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u7684\u5b9e\u65f6\u667a\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u5207\u5b9e\u53ef\u884c\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.09191", "pdf": "https://arxiv.org/pdf/2508.09191", "abs": "https://arxiv.org/abs/2508.09191", "authors": ["Xiaoyu Tao", "Shilong Zhang", "Mingyue Cheng", "Daoyu Wang", "Tingyue Pan", "Bokai Pan", "Changqing Zhang", "Shijin Wang"], "title": "From Values to Tokens: An LLM-Driven Framework for Context-aware Time Series Forecasting via Symbolic Discretization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series forecasting plays a vital role in supporting decision-making\nacross a wide range of critical applications, including energy, healthcare, and\nfinance. Despite recent advances, forecasting accuracy remains limited due to\nthe challenge of integrating historical numerical sequences with contextual\nfeatures, which often comprise unstructured textual data. To address this\nchallenge, we propose TokenCast, an LLM-driven framework that leverages\nlanguage-based symbolic representations as a unified intermediary for\ncontext-aware time series forecasting. Specifically, TokenCast employs a\ndiscrete tokenizer to transform continuous numerical sequences into temporal\ntokens, enabling structural alignment with language-based inputs. To bridge the\nsemantic gap between modalities, both temporal and contextual tokens are\nembedded into a shared representation space via a pre-trained large language\nmodel (LLM), further optimized with autoregressive generative objectives.\nBuilding upon this unified semantic space, the aligned LLM is subsequently\nfine-tuned in a supervised manner to predict future temporal tokens, which are\nthen decoded back into the original numerical space. Extensive experiments on\ndiverse real-world datasets enriched with contextual features demonstrate the\neffectiveness and generalizability of TokenCast.", "AI": {"tldr": "TokenCast\u662f\u4e00\u4e2aLLM\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8fde\u7eed\u6570\u503c\u5e8f\u5217\u548c\u975e\u7ed3\u6784\u5316\u6587\u672c\u8bed\u5883\u7edf\u4e00\u4e3a\u8bed\u8a00\u7b26\u53f7\u8868\u793a\uff0c\u63d0\u5347\u4e86\u60c5\u5883\u611f\u77e5\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u56e0\u96be\u4ee5\u6709\u6548\u6574\u5408\u5386\u53f2\u6570\u503c\u5e8f\u5217\u4e0e\u975e\u7ed3\u6784\u5316\u6587\u672c\u8bed\u5883\u7279\u5f81\uff0c\u5bfc\u81f4\u9884\u6d4b\u51c6\u786e\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51faTokenCast\u6846\u67b6\uff0c\u5177\u4f53\u6b65\u9aa4\u5305\u62ec\uff1a1. \u4f7f\u7528\u79bb\u6563\u5206\u8bcd\u5668\u5c06\u8fde\u7eed\u6570\u503c\u5e8f\u5217\u8f6c\u6362\u4e3a\u65f6\u95f4\u7b26\u53f7\uff08temporal tokens\uff09\u30022. \u901a\u8fc7\u9884\u8bad\u7ec3LLM\u5c06\u65f6\u95f4\u7b26\u53f7\u548c\u8bed\u5883\u7b26\u53f7\u5d4c\u5165\u5171\u4eab\u8868\u793a\u7a7a\u95f4\uff0c\u5e76\u8fdb\u884c\u81ea\u56de\u5f52\u751f\u6210\u76ee\u6807\u4f18\u5316\u30023. \u5728\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\u4e0a\uff0c\u76d1\u7763\u5fae\u8c03LLM\u4ee5\u9884\u6d4b\u672a\u6765\u65f6\u95f4\u7b26\u53f7\u30024. \u5c06\u9884\u6d4b\u7684\u65f6\u95f4\u7b26\u53f7\u89e3\u7801\u56de\u539f\u59cb\u6570\u503c\u7a7a\u95f4\u3002", "result": "\u5728\u591a\u79cd\u5bcc\u542b\u8bed\u5883\u7279\u5f81\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTokenCast\u5177\u6709\u51fa\u8272\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "TokenCast\u901a\u8fc7LLM\u9a71\u52a8\u7684\u7edf\u4e00\u8bed\u8a00\u8868\u793a\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u6574\u5408\u6570\u503c\u548c\u6587\u672c\u8bed\u5883\u7684\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2508.09392", "pdf": "https://arxiv.org/pdf/2508.09392", "abs": "https://arxiv.org/abs/2508.09392", "authors": ["Kang Ni", "Minrui Zou", "Yuxuan Li", "Xiang Li", "Kehua Guo", "Ming-Ming Cheng", "Yimian Dai"], "title": "DenoDet V2: Phase-Amplitude Cross Denoising for SAR Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "One of the primary challenges in Synthetic Aperture Radar (SAR) object\ndetection lies in the pervasive influence of coherent noise. As a common\npractice, most existing methods, whether handcrafted approaches or deep\nlearning-based methods, employ the analysis or enhancement of object\nspatial-domain characteristics to achieve implicit denoising. In this paper, we\npropose DenoDet V2, which explores a completely novel and different perspective\nto deconstruct and modulate the features in the transform domain via a\ncarefully designed attention architecture. Compared to DenoDet V1, DenoDet V2\nis a major advancement that exploits the complementary nature of amplitude and\nphase information through a band-wise mutual modulation mechanism, which\nenables a reciprocal enhancement between phase and amplitude spectra. Extensive\nexperiments on various SAR datasets demonstrate the state-of-the-art\nperformance of DenoDet V2. Notably, DenoDet V2 achieves a significant 0.8\\%\nimprovement on SARDet-100K dataset compared to DenoDet V1, while reducing the\nmodel complexity by half. The code is available at\nhttps://github.com/GrokCV/GrokSAR.", "AI": {"tldr": "DenoDet V2\u9488\u5bf9SAR\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u76f8\u5e72\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u53d8\u6362\u57df\u7279\u5f81\u8c03\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e92\u8865\u5229\u7528\u5e45\u5ea6\u548c\u76f8\u4f4d\u4fe1\u606f\u5b9e\u73b0\u53bb\u566a\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u5e76\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u3002", "motivation": "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u76ee\u6807\u68c0\u6d4b\u4e2d\u76f8\u5e72\u566a\u58f0\u7684\u666e\u904d\u5f71\u54cd\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u901a\u8fc7\u5206\u6790\u6216\u589e\u5f3a\u76ee\u6807\u7a7a\u95f4\u57df\u7279\u5f81\u6765\u9690\u5f0f\u53bb\u566a\uff0c\u53ef\u80fd\u672a\u80fd\u5145\u5206\u89e3\u51b3\u76f8\u5e72\u566a\u58f0\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86DenoDet V2\uff0c\u5b83\u4ece\u4e00\u4e2a\u5168\u65b0\u7684\u89c6\u89d2\u51fa\u53d1\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u5728\u53d8\u6362\u57df\u4e2d\u89e3\u6784\u548c\u8c03\u5236\u7279\u5f81\u3002\u76f8\u8f83\u4e8eDenoDet V1\uff0cDenoDet V2\u662f\u4e00\u4e2a\u91cd\u5927\u6539\u8fdb\uff0c\u5b83\u901a\u8fc7\u9891\u5e26\u95f4\u76f8\u4e92\u8c03\u5236\u673a\u5236\uff0c\u5229\u7528\u5e45\u5ea6\u548c\u76f8\u4f4d\u4fe1\u606f\u7684\u4e92\u8865\u6027\uff0c\u5b9e\u73b0\u4e86\u76f8\u4f4d\u8c31\u548c\u5e45\u5ea6\u8c31\u7684\u76f8\u4e92\u589e\u5f3a\u3002", "result": "DenoDet V2\u5728\u5404\u79cdSAR\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u7279\u522b\u5730\uff0c\u5728SARDet-100K\u6570\u636e\u96c6\u4e0a\uff0cDenoDet V2\u6bd4DenoDet V1\u7684\u6027\u80fd\u63d0\u5347\u4e860.8%\uff0c\u540c\u65f6\u6a21\u578b\u590d\u6742\u5ea6\u964d\u4f4e\u4e86\u4e00\u534a\u3002", "conclusion": "DenoDet V2\u901a\u8fc7\u521b\u65b0\u6027\u5730\u5728\u53d8\u6362\u57df\u4e2d\u5bf9\u7279\u5f81\u8fdb\u884c\u53bb\u566a\u548c\u8c03\u5236\uff0c\u5e76\u6709\u6548\u5229\u7528\u5e45\u5ea6\u548c\u76f8\u4f4d\u4fe1\u606f\u7684\u4e92\u8865\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86SAR\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u76f8\u5e72\u566a\u58f0\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u63d0\u5347\u548c\u663e\u8457\u7684\u6a21\u578b\u6548\u7387\u4f18\u5316\u3002"}}
{"id": "2508.09767", "pdf": "https://arxiv.org/pdf/2508.09767", "abs": "https://arxiv.org/abs/2508.09767", "authors": ["Shuhei Kato"], "title": "UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "We propose UtterTune, a lightweight adaptation method that fine-tunes a\nmultilingual text-to-speech (TTS) system based on a large language model (LLM)\narchitecture, designed to enhance the controllability of pronunciation in a\ntarget language while preserving performance in others. While LLM architectures\nhave enabled TTS models to achieve remarkable naturalness, accurately modeling\ngrapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially\nwhen the model omits an explicit G2P module and directly processes minimally\nencoded text (e.g., byte-pair encoding). UtterTune leverages low-rank\nadaptation to enable the control of segmental pronunciation and pitch accent at\nthe phoneme level for Japanese speech, the target language in this paper, while\nmaintaining naturalness and speaker similarity in a zero-shot setting.\nObjective and subjective evaluations confirm its effectiveness.", "AI": {"tldr": "UtterTune\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u5fae\u8c03\u57fa\u4e8eLLM\u7684\u591a\u8bed\u8a00TTS\u7cfb\u7edf\uff0c\u65e8\u5728\u589e\u5f3a\u76ee\u6807\u8bed\u8a00\uff08\u65e5\u8bed\uff09\u7684\u53d1\u97f3\u53ef\u63a7\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u8bed\u8a00\u7684\u6027\u80fd\u548c\u8bed\u97f3\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8eLLM\u7684TTS\u6a21\u578b\u80fd\u5b9e\u73b0\u5353\u8d8a\u7684\u81ea\u7136\u5ea6\uff0c\u4f46\u51c6\u786e\u5efa\u6a21\u5b57\u7d20\u5230\u97f3\u7d20\uff08G2P\uff09\u6620\u5c04\u548c\u97f5\u5f8b\u4ecd\u5177\u6311\u6218\uff0c\u5c24\u5176\u5f53\u6a21\u578b\u76f4\u63a5\u5904\u7406\u6700\u5c0f\u7f16\u7801\u6587\u672c\u800c\u7701\u7565\u663e\u5f0fG2P\u6a21\u5757\u65f6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86UtterTune\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u9002\u5e94\u65b9\u6cd5\uff0c\u5229\u7528\u4f4e\u79e9\u9002\u5e94\uff08low-rank adaptation\uff09\u6765\u5fae\u8c03\u57fa\u4e8eLLM\u67b6\u6784\u7684\u591a\u8bed\u8a00TTS\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u4f7f\u7cfb\u7edf\u80fd\u591f\u5728\u97f3\u7d20\u5c42\u9762\u63a7\u5236\u76ee\u6807\u8bed\u8a00\uff08\u65e5\u8bed\uff09\u7684\u8bed\u6bb5\u53d1\u97f3\u548c\u97f3\u9ad8\u91cd\u97f3\uff0c\u540c\u65f6\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u8bed\u97f3\u7684\u81ea\u7136\u5ea6\u548c\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u3002", "result": "\u5ba2\u89c2\u548c\u4e3b\u89c2\u8bc4\u4f30\u5747\u8bc1\u5b9e\u4e86UtterTune\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "UtterTune\u6210\u529f\u5730\u589e\u5f3a\u4e86\u57fa\u4e8eLLM\u7684\u591a\u8bed\u8a00TTS\u7cfb\u7edf\u5bf9\u76ee\u6807\u8bed\u8a00\uff08\u65e5\u8bed\uff09\u53d1\u97f3\u7684\u63a7\u5236\u80fd\u529b\uff0c\u540c\u65f6\u7ef4\u62a4\u4e86\u591a\u8bed\u8a00\u6027\u80fd\u548c\u8bed\u97f3\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86G2P\u6620\u5c04\u548c\u97f5\u5f8b\u5efa\u6a21\u7684\u6311\u6218\u3002"}}
{"id": "2508.09192", "pdf": "https://arxiv.org/pdf/2508.09192", "abs": "https://arxiv.org/abs/2508.09192", "authors": ["Xu Wang", "Chenkai Xu", "Yijie Jin", "Jiachun Jin", "Hao Zhang", "Zhijie Deng"], "title": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diffusion Large Language Models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs for text generation, with the potential\nto decode multiple tokens in a single iteration. However, none of the existing\nopen-source dLLMs have achieved superior inference speed over AR LLMs of\nsimilar size. This paper breaks this barrier based on a simple and effective\nstrategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key\ncapabilities: (1) block-wise autoregressive generation to enable KV cache\nutilization; (2) prediction of following tokens without requiring completion of\nprior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs\nare refurbished into an AR-diffusion hybrid paradigm for efficient inference.\nD2F can be implemented with an asymmetric distillation process based on\npre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,\nwhich enables a trade-off between efficiency and efficacy. Empirically, D2F\ndLLMs achieve more than $\\mathbf{2.5\\times}$ inference speed than LLaMA3 and\nQwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the\nacceleration can be more than $\\mathbf{50\\times}$ while maintaining comparable\noutput quality. The code is available at\nhttps://github.com/zhijie-group/Discrete-Diffusion-Forcing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faD2F\u7b56\u7565\uff0c\u4f7f\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u63a8\u7406\u901f\u5ea6\u9996\u6b21\u8d85\u8d8a\u81ea\u56de\u5f52(AR)LLMs\uff0c\u5e76\u5728\u63a8\u7406\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u5927\u5e45\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLMs)\u867d\u6709\u5e76\u884c\u89e3\u7801\u6f5c\u529b\uff0c\u4f46\u63a8\u7406\u901f\u5ea6\u4ecd\u65e0\u6cd5\u8d85\u8d8a\u540c\u7b49\u5927\u5c0f\u7684\u81ea\u56de\u5f52(AR)LLMs\uff0c\u963b\u788d\u4e86\u5176\u4f5c\u4e3aAR-LLMs\u66ff\u4ee3\u54c1\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u79bb\u6563\u6269\u6563\u5f3a\u5236(D2F)\u7b56\u7565\uff0c\u5c06dLLMs\u6539\u9020\u4e3aAR-\u6269\u6563\u6df7\u5408\u8303\u5f0f\u3002D2F\u8d4b\u80fddLLMs\u5177\u5907\u4e24\u9879\u5173\u952e\u80fd\u529b\uff1a1) \u5757\u7ea7\u81ea\u56de\u5f52\u751f\u6210\u4ee5\u5229\u7528KV\u7f13\u5b58\uff1b2) \u65e0\u9700\u5b8c\u6210\u524d\u5757\u5373\u53ef\u9884\u6d4b\u540e\u7eed\u4ee4\u724c\uff0c\u5b9e\u73b0\u5757\u95f4\u5e76\u884c\u89e3\u7801\u3002D2F\u901a\u8fc7\u57fa\u4e8e\u9884\u8bad\u7ec3dLLMs\u7684\u975e\u5bf9\u79f0\u84b8\u998f\u8fc7\u7a0b\u5b9e\u73b0\uff0c\u5e76\u63d0\u51fa\u6d41\u6c34\u7ebf\u5e76\u884c\u89e3\u7801\u7b97\u6cd5\u3002", "result": "\u7ecf\u9a8c\u8bc1\uff0cD2F dLLMs\u5728GSM8K\u6570\u636e\u96c6\u4e0a\u7684\u63a8\u7406\u901f\u5ea6\u6bd4LLaMA3\u548cQwen2.5\u5feb2.5\u500d\u4ee5\u4e0a\u3002\u76f8\u8f83\u4e8e\u9999\u8349dLLMs\uff08\u5982LLaDA\u548cDream\uff09\uff0c\u5728\u4fdd\u6301\u53ef\u6bd4\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u52a0\u901f\u8d85\u8fc750\u500d\u3002", "conclusion": "D2F\u6210\u529f\u6253\u7834\u4e86dLLMs\u63a8\u7406\u901f\u5ea6\u900a\u4e8eAR LLMs\u7684\u58c1\u5792\uff0c\u901a\u8fc7\u6df7\u5408\u8303\u5f0f\u663e\u8457\u63d0\u5347\u4e86dLLMs\u7684\u63a8\u7406\u6548\u7387\uff0c\u5e76\u6709\u671b\u6210\u4e3a\u9ad8\u6548\u6587\u672c\u751f\u6210\u7684\u65b0\u9009\u62e9\u3002"}}
{"id": "2508.09397", "pdf": "https://arxiv.org/pdf/2508.09397", "abs": "https://arxiv.org/abs/2508.09397", "authors": ["Zhengli Zhang", "Xinyu Luo", "Yuchen Sun", "Wenhua Ding", "Dongyu Huang", "Xinlei Chen"], "title": "Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety", "categories": ["cs.CV"], "comment": null, "summary": "Drones operating in complex environments face a significant threat from thin\nobstacles, such as steel wires and kite strings at the submillimeter level,\nwhich are notoriously difficult for conventional sensors like RGB cameras,\nLiDAR, and depth cameras to detect. This paper introduces SkyShield, an\nevent-driven, end-to-end framework designed for the perception of submillimeter\nscale obstacles. Drawing upon the unique features that thin obstacles present\nin the event stream, our method employs a lightweight U-Net architecture and an\ninnovative Dice-Contour Regularization Loss to ensure precise detection.\nExperimental results demonstrate that our event-based approach achieves mean F1\nScore of 0.7088 with a low latency of 21.2 ms, making it ideal for deployment\non edge and mobile platforms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSkyShield\uff0c\u4e00\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u9a71\u52a8\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u96be\u4ee5\u8bc6\u522b\u7684\u4e9a\u6beb\u7c73\u7ea7\u7ec6\u5c0f\u969c\u788d\u7269\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7U-Net\u548c\u521b\u65b0\u7684\u635f\u5931\u51fd\u6570\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u5ef6\u8fdf\uff0c\u9002\u5408\u8fb9\u7f18\u90e8\u7f72\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u9762\u4e34\u4e9a\u6beb\u7c73\u7ea7\u7ec6\u5c0f\u969c\u788d\u7269\uff08\u5982\u94a2\u4e1d\u3001\u98ce\u7b5d\u7ebf\uff09\u7684\u4e25\u91cd\u5a01\u80c1\uff0c\u800c\u4f20\u7edf\u4f20\u611f\u5668\uff08\u5982RGB\u76f8\u673a\u3001LiDAR\u3001\u6df1\u5ea6\u76f8\u673a\uff09\u96be\u4ee5\u6709\u6548\u68c0\u6d4b\u8fd9\u4e9b\u969c\u788d\u7269\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86SkyShield\uff0c\u4e00\u4e2a\u4e8b\u4ef6\u9a71\u52a8\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5229\u7528\u7ec6\u5c0f\u969c\u788d\u7269\u5728\u4e8b\u4ef6\u6d41\u4e2d\u7684\u72ec\u7279\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u8f7b\u91cf\u7ea7U-Net\u67b6\u6784\u548c\u521b\u65b0\u7684Dice-Contour\u6b63\u5219\u5316\u635f\u5931\u51fd\u6570\u6765\u786e\u4fdd\u7cbe\u786e\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u4e8b\u4ef6\u9a71\u52a8\u65b9\u6cd5\u5b9e\u73b0\u4e860.7088\u7684\u5e73\u5747F1\u5206\u6570\uff0c\u5e76\u5177\u670921.2\u6beb\u79d2\u7684\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "SkyShield\u80fd\u591f\u6709\u6548\u611f\u77e5\u4e9a\u6beb\u7c73\u7ea7\u969c\u788d\u7269\uff0c\u5176\u9ad8\u6027\u80fd\u548c\u4f4e\u5ef6\u8fdf\u4f7f\u5176\u975e\u5e38\u9002\u5408\u90e8\u7f72\u5728\u8fb9\u7f18\u548c\u79fb\u52a8\u5e73\u53f0\u4e0a\u3002"}}
{"id": "2508.09776", "pdf": "https://arxiv.org/pdf/2508.09776", "abs": "https://arxiv.org/abs/2508.09776", "authors": ["Mahdi Dhaini", "Juraj Vladika", "Ege Erdogan", "Zineb Attaoui", "Gjergji Kasneci"], "title": "Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the 34th International Conference on Artificial Neural\n  Networks (ICANN 2025)", "summary": "In the rapidly evolving field of Explainable Natural Language Processing\n(NLP), textual explanations, i.e., human-like rationales, are pivotal for\nexplaining model predictions and enriching datasets with interpretable labels.\nTraditional approaches rely on human annotation, which is costly,\nlabor-intensive, and impedes scalability. In this work, we present an automated\nframework that leverages multiple state-of-the-art large language models (LLMs)\nto generate high-quality textual explanations. We rigorously assess the quality\nof these LLM-generated explanations using a comprehensive suite of Natural\nLanguage Generation (NLG) metrics. Furthermore, we investigate the downstream\nimpact of these explanations on the performance of pre-trained language models\n(PLMs) and LLMs across natural language inference tasks on two diverse\nbenchmark datasets. Our experiments demonstrate that automated explanations\nexhibit highly competitive effectiveness compared to human-annotated\nexplanations in improving model performance. Our findings underscore a\npromising avenue for scalable, automated LLM-based textual explanation\ngeneration for extending NLP datasets and enhancing model performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5229\u7528\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u9ad8\u8d28\u91cf\u6587\u672c\u89e3\u91ca\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u65b9\u9762\u4e0e\u4eba\u5de5\u6807\u6ce8\u89e3\u91ca\u5177\u6709\u7ade\u4e89\u529b\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u89e3\u91ca\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u6210\u672c\u9ad8\u6602\u3001\u8017\u65f6\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u963b\u788d\u4e86\u53ef\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5229\u7528\u591a\u4e2a\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u6587\u672c\u89e3\u91ca\u3002\u901a\u8fc7\u4e00\u5957\u5168\u9762\u7684\u81ea\u7136\u8bed\u8a00\u751f\u6210\uff08NLG\uff09\u6307\u6807\u8bc4\u4f30\u6240\u751f\u6210\u89e3\u91ca\u7684\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u8c03\u67e5\u4e86\u8fd9\u4e9b\u89e3\u91ca\u5bf9\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u548cLLMs\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u4e0b\u6e38\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u81ea\u52a8\u5316\u751f\u6210\u7684\u89e3\u91ca\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u65b9\u9762\uff0c\u4e0e\u4eba\u5de5\u6807\u6ce8\u89e3\u91ca\u8868\u73b0\u51fa\u9ad8\u5ea6\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u6587\u672c\u89e3\u91ca\u751f\u6210\u4e3a\u6269\u5c55NLP\u6570\u636e\u96c6\u548c\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u4e14\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002"}}
{"id": "2508.09213", "pdf": "https://arxiv.org/pdf/2508.09213", "abs": "https://arxiv.org/abs/2508.09213", "authors": ["Clifton Paul Robinson", "Salvatore D'Oro", "Tommaso Melodia"], "title": "VeriPHY: Physical Layer Signal Authentication for Wireless Communication in 5G Environments", "categories": ["cs.CR", "cs.NI"], "comment": "7 pages, 10 figures, 2 tables, IEEE Military Communications\n  Conference 2025 (MILCOM '25)", "summary": "Physical layer authentication (PLA) uses inherent characteristics of the\ncommunication medium to provide secure and efficient authentication in wireless\nnetworks, bypassing the need for traditional cryptographic methods. With\nadvancements in deep learning, PLA has become a widely adopted technique for\nits accuracy and reliability. In this paper, we introduce VeriPHY, a novel deep\nlearning-based PLA solution for 5G networks, which enables unique device\nidentification by embedding signatures within wireless I/Q transmissions using\nsteganography. VeriPHY continuously generates pseudo-random signatures by\nsampling from Gaussian Mixture Models whose distribution is carefully varied to\nensure signature uniqueness and stealthiness over time, and then embeds the\nnewly generated signatures over I/Q samples transmitted by users to the 5G gNB.\nUtilizing deep neural networks, VeriPHY identifies and authenticates users\nbased on these embedded signatures. VeriPHY achieves high precision,\nidentifying unique signatures between 93% and 100% with low false positive\nrates and an inference time of 28 ms when signatures are updated every 20 ms.\nAdditionally, we also demonstrate a stealth generation mode where signatures\nare generated in a way that makes them virtually indistinguishable from\nunaltered 5G signals while maintaining over 93% detection accuracy.", "AI": {"tldr": "VeriPHY\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u76845G\u7269\u7406\u5c42\u8ba4\u8bc1\u65b9\u6848\uff0c\u901a\u8fc7\u9690\u5199\u672f\u5728I/Q\u4f20\u8f93\u4e2d\u5d4c\u5165\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u751f\u6210\u7684\u4f2a\u968f\u673a\u7b7e\u540d\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4e14\u9690\u853d\u7684\u8bbe\u5907\u8bc6\u522b\u4e0e\u8ba4\u8bc1\u3002", "motivation": "\u7269\u7406\u5c42\u8ba4\u8bc1\uff08PLA\uff09\u5229\u7528\u901a\u4fe1\u4ecb\u8d28\u7279\u6027\u63d0\u4f9b\u65e0\u7ebf\u7f51\u7edc\u5b89\u5168\u9ad8\u6548\u7684\u8ba4\u8bc1\uff0c\u907f\u514d\u4f20\u7edf\u5bc6\u7801\u65b9\u6cd5\u3002\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u53d1\u5c55\uff0cPLA\u56e0\u5176\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u88ab\u5e7f\u6cdb\u91c7\u7528\u3002\u672c\u6587\u65e8\u5728\u4e3a5G\u7f51\u7edc\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684PLA\u89e3\u51b3\u65b9\u6848\u3002", "method": "VeriPHY\u901a\u8fc7\u9690\u5199\u672f\u5728\u65e0\u7ebfI/Q\u4f20\u8f93\u4e2d\u5d4c\u5165\u7b7e\u540d\u8fdb\u884c\u8bbe\u5907\u8bc6\u522b\u3002\u5b83\u5229\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u751f\u6210\u4f2a\u968f\u673a\u7b7e\u540d\uff0c\u5e76\u7cbe\u5fc3\u8c03\u6574\u5176\u5206\u5e03\u4ee5\u786e\u4fdd\u7b7e\u540d\u7684\u72ec\u7279\u6027\u548c\u9690\u853d\u6027\u3002\u8fd9\u4e9b\u7b7e\u540d\u88ab\u5d4c\u5165\u5230\u7528\u6237\u4f20\u8f93\u7ed95G gNB\u7684I/Q\u6837\u672c\u4e2d\uff0c\u7136\u540e\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bc6\u522b\u548c\u8ba4\u8bc1\u7528\u6237\u3002", "result": "VeriPHY\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u72ec\u7279\u7b7e\u540d\u8bc6\u522b\u7387\u572893%\u81f3100%\u4e4b\u95f4\uff0c\u4e14\u8bef\u62a5\u7387\u4f4e\u3002\u5f53\u7b7e\u540d\u6bcf20\u6beb\u79d2\u66f4\u65b0\u4e00\u6b21\u65f6\uff0c\u63a8\u7406\u65f6\u95f4\u4e3a28\u6beb\u79d2\u3002\u5728\u9690\u853d\u751f\u6210\u6a21\u5f0f\u4e0b\uff0c\u7b7e\u540d\u4e0e\u672a\u6539\u53d8\u76845G\u4fe1\u53f7\u51e0\u4e4e\u65e0\u6cd5\u533a\u5206\uff0c\u540c\u65f6\u4ecd\u4fdd\u6301\u8d85\u8fc793%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "conclusion": "VeriPHY\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u3001\u9ad8\u6548\u4e14\u9ad8\u5ea6\u7cbe\u786e\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7269\u7406\u5c42\u8ba4\u8bc1\u65b9\u6848\uff0c\u901a\u8fc7\u9690\u853d\u5730\u5d4c\u5165\u7b7e\u540d\uff0c\u4e3a5G\u7f51\u7edc\u5b9e\u73b0\u4e86\u72ec\u7279\u7684\u8bbe\u5907\u8bc6\u522b\u548c\u8ba4\u8bc1\uff0c\u540c\u65f6\u5177\u5907\u51fa\u8272\u7684\u9690\u853d\u6027\u3002"}}
{"id": "2508.09193", "pdf": "https://arxiv.org/pdf/2508.09193", "abs": "https://arxiv.org/abs/2508.09193", "authors": ["Sung-Hyun Kim", "In-Chang Baek", "Seo-Young Lee", "Geum-Hwan Hwang", "Kyung-Joong Kim"], "title": "Multi-Objective Instruction-Aware Representation Learning in Procedural Content Generation RL", "categories": ["cs.LG", "cs.AI"], "comment": "5 pages, 3 figures", "summary": "Recent advancements in generative modeling emphasize the importance of\nnatural language as a highly expressive and accessible modality for controlling\ncontent generation. However, existing instructed reinforcement learning for\nprocedural content generation (IPCGRL) method often struggle to leverage the\nexpressive richness of textual input, especially under complex, multi-objective\ninstructions, leading to limited controllability. To address this problem, we\npropose \\textit{MIPCGRL}, a multi-objective representation learning method for\ninstructed content generators, which incorporates sentence embeddings as\nconditions. MIPCGRL effectively trains a multi-objective embedding space by\nincorporating multi-label classification and multi-head regression networks.\nExperimental results show that the proposed method achieves up to a 13.8\\%\nimprovement in controllability with multi-objective instructions. The ability\nto process complex instructions enables more expressive and flexible content\ngeneration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMIPCGRL\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u8868\u793a\u5b66\u4e60\u548c\u53e5\u5b50\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u591a\u76ee\u6807\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0b\u7a0b\u5e8f\u5185\u5bb9\u751f\u6210\u7684\u53ef\u63a7\u6027\u3002", "motivation": "\u73b0\u6709\u6307\u4ee4\u5f0f\u5f3a\u5316\u5b66\u4e60\u7a0b\u5e8f\u5185\u5bb9\u751f\u6210\uff08IPCGRL\uff09\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u3001\u591a\u76ee\u6807\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u65f6\uff0c\u96be\u4ee5\u6709\u6548\u5229\u7528\u6587\u672c\u8f93\u5165\u7684\u4e30\u5bcc\u6027\uff0c\u5bfc\u81f4\u5185\u5bb9\u751f\u6210\u7684\u53ef\u63a7\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51faMIPCGRL\uff0c\u4e00\u79cd\u7528\u4e8e\u6307\u4ee4\u5f0f\u5185\u5bb9\u751f\u6210\u5668\u7684\u591a\u76ee\u6807\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5c06\u53e5\u5b50\u5d4c\u5165\u4f5c\u4e3a\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u591a\u6807\u7b7e\u5206\u7c7b\u548c\u591a\u5934\u56de\u5f52\u7f51\u7edc\u6765\u6709\u6548\u8bad\u7ec3\u591a\u76ee\u6807\u5d4c\u5165\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684MIPCGRL\u65b9\u6cd5\u5728\u591a\u76ee\u6807\u6307\u4ee4\u4e0b\uff0c\u5c06\u5185\u5bb9\u751f\u6210\u7684\u53ef\u63a7\u6027\u63d0\u9ad8\u4e86\u9ad8\u8fbe13.8%\u3002", "conclusion": "MIPCGRL\u5904\u7406\u590d\u6742\u6307\u4ee4\u7684\u80fd\u529b\u4f7f\u5f97\u5185\u5bb9\u751f\u6210\u66f4\u5177\u8868\u73b0\u529b\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2508.09398", "pdf": "https://arxiv.org/pdf/2508.09398", "abs": "https://arxiv.org/abs/2508.09398", "authors": ["El Mustapha Mansouri"], "title": "Autonomous AI Bird Feeder for Backyard Biodiversity Monitoring", "categories": ["cs.CV"], "comment": "Preprint; 8 pages, 5 figures, 1 table; IEEEtran conference format.\n  Code: https://github.com/E-zClap/bird-classifier", "summary": "This paper presents a low cost, on premise system for autonomous backyard\nbird monitoring in Belgian urban gardens. A motion triggered IP camera uploads\nshort clips via FTP to a local server, where frames are sampled and birds are\nlocalized with Detectron2; cropped regions are then classified by an\nEfficientNet-B3 model fine tuned on a 40-species Belgian subset derived from a\nlarger Kaggle corpus. All processing runs on commodity hardware without a\ndiscrete GPU, preserving privacy and avoiding cloud fees. The physical feeder\nuses small entry ports (30 mm) to exclude pigeons and reduce nuisance triggers.\nDetector-guided cropping improves classification accuracy over raw-frame\nclassification. The classifier attains high validation performance on the\ncurated subset (about 99.5 percent) and delivers practical field accuracy\n(top-1 about 88 percent) on held-out species, demonstrating feasibility for\ncitizen-science-grade biodiversity logging at home.", "AI": {"tldr": "\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u672c\u5730\u90e8\u7f72\u7684\u81ea\u52a8\u540e\u9662\u9e1f\u7c7b\u76d1\u6d4b\u7cfb\u7edf\u3002", "motivation": "\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u7ecf\u6d4e\u5b9e\u60e0\u3001\u4fdd\u62a4\u9690\u79c1\u4e14\u65e0\u9700\u4e91\u670d\u52a1\u7684\u81ea\u52a8\u9e1f\u7c7b\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u652f\u6301\u516c\u6c11\u79d1\u5b66\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u8fd0\u52a8\u89e6\u53d1\u7684IP\u6444\u50cf\u5934\uff0c\u5c06\u77ed\u89c6\u9891\u901a\u8fc7FTP\u4e0a\u4f20\u81f3\u672c\u5730\u670d\u52a1\u5668\u3002\u670d\u52a1\u5668\u4f7f\u7528Detectron2\u5b9a\u4f4d\u9e1f\u7c7b\uff0c\u968f\u540e\u901a\u8fc7\u5728\u6bd4\u5229\u65f640\u79cd\u9e1f\u7c7b\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684EfficientNet-B3\u6a21\u578b\u5bf9\u88c1\u526a\u533a\u57df\u8fdb\u884c\u5206\u7c7b\u3002\u6240\u6709\u5904\u7406\u5747\u5728\u4e0d\u5e26\u72ec\u7acbGPU\u7684\u666e\u901a\u786c\u4ef6\u4e0a\u8fd0\u884c\u3002\u7269\u7406\u5582\u98df\u5668\u8bbe\u8ba1\u670930\u6beb\u7c73\u5c0f\u5165\u53e3\u4ee5\u6392\u9664\u9e3d\u5b50\u3002", "result": "\u68c0\u6d4b\u5668\u5f15\u5bfc\u7684\u88c1\u526a\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u3002\u5206\u7c7b\u5668\u5728\u7cbe\u5fc3\u6574\u7406\u7684\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u5230\u4e86\u7ea699.5%\u7684\u9ad8\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u73b0\u573a\u5e94\u7528\u4e2d\u5bf9\u672a\u89c1\u8fc7\u7269\u79cd\u5b9e\u73b0\u4e86\u7ea688%\uff08top-1\uff09\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u8bc1\u660e\u4e86\u5728\u5bb6\u4e2d\u8fdb\u884c\u516c\u6c11\u79d1\u5b66\u7ea7\u522b\u7684\u751f\u7269\u591a\u6837\u6027\u8bb0\u5f55\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.09786", "pdf": "https://arxiv.org/pdf/2508.09786", "abs": "https://arxiv.org/abs/2508.09786", "authors": ["Mahdi Dhaini", "Tobias M\u00fcller", "Roksoliana Rabets", "Gjergji Kasneci"], "title": "Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to AAAI/ACM Conference on AI, Ethics, and Society (AIES\n  2025)", "summary": "The field of explainable natural language processing (NLP) has grown rapidly\nin recent years. The growing opacity of complex models calls for transparency\nand explanations of their decisions, which is crucial to understand their\nreasoning and facilitate deployment, especially in high-stakes environments.\nDespite increasing attention given to explainable NLP, practitioners'\nperspectives regarding its practical adoption and effectiveness remain\nunderexplored. This paper addresses this research gap by investigating\npractitioners' experiences with explainability methods, specifically focusing\non their motivations for adopting such methods, the techniques employed,\nsatisfaction levels, and the practical challenges encountered in real-world NLP\napplications. Through a qualitative interview-based study with industry\npractitioners and complementary interviews with academic researchers, we\nsystematically analyze and compare their perspectives. Our findings reveal\nconceptual gaps, low satisfaction with current explainability methods, and\nhighlight evaluation challenges. Our findings emphasize the need for clear\ndefinitions and user-centric frameworks for better adoption of explainable NLP\nin practice.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8bbf\u8c08\u8c03\u67e5\u4e86\u4ece\u4e1a\u8005\u5728\u5b9e\u9645NLP\u5e94\u7528\u4e2d\u5bf9\u53ef\u89e3\u91ca\u65b9\u6cd5\u7684\u7ecf\u9a8c\uff0c\u53d1\u73b0\u5b58\u5728\u6982\u5ff5\u5dee\u8ddd\u3001\u6ee1\u610f\u5ea6\u4f4e\u548c\u8bc4\u4f30\u6311\u6218\uff0c\u5e76\u5f3a\u8c03\u9700\u6e05\u6670\u5b9a\u4e49\u548c\u7528\u6237\u4e2d\u5fc3\u6846\u67b6\u4ee5\u4fc3\u8fdb\u5176\u5e94\u7528\u3002", "motivation": "\u5c3d\u7ba1\u53ef\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u53d7\u5230\u65e5\u76ca\u5173\u6ce8\uff0c\u4f46\u4ece\u4e1a\u8005\u5bf9\u5176\u5728\u5b9e\u8df5\u4e2d\u7684\u91c7\u7eb3\u548c\u6709\u6548\u6027\u770b\u6cd5\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8c03\u67e5\u4ece\u4e1a\u8005\u4f7f\u7528\u53ef\u89e3\u91ca\u65b9\u6cd5\u7684\u7ecf\u9a8c\uff0c\u5305\u62ec\u5176\u52a8\u673a\u3001\u6240\u7528\u6280\u672f\u3001\u6ee1\u610f\u5ea6\u53ca\u9047\u5230\u7684\u5b9e\u9645\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5bf9\u884c\u4e1a\u4ece\u4e1a\u8005\u8fdb\u884c\u5b9a\u6027\u8bbf\u8c08\uff0c\u5e76\u8f85\u4ee5\u5bf9\u5b66\u672f\u7814\u7a76\u4eba\u5458\u7684\u8bbf\u8c08\uff0c\u7cfb\u7edf\u5206\u6790\u548c\u6bd4\u8f83\u4e86\u4ed6\u4eec\u7684\u89c2\u70b9\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u6982\u5ff5\u6027\u5dee\u8ddd\u3001\u5bf9\u5f53\u524d\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u7684\u4f4e\u6ee1\u610f\u5ea6\uff0c\u5e76\u7a81\u51fa\u4e86\u8bc4\u4f30\u65b9\u9762\u7684\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\uff0c\u4e3a\u4e86\u5728\u5b9e\u8df5\u4e2d\u66f4\u597d\u5730\u91c7\u7eb3\u53ef\u89e3\u91caNLP\uff0c\u9700\u8981\u660e\u786e\u7684\u5b9a\u4e49\u548c\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u6846\u67b6\u3002"}}
{"id": "2508.09532", "pdf": "https://arxiv.org/pdf/2508.09532", "abs": "https://arxiv.org/abs/2508.09532", "authors": ["Bokeng Zheng", "Jianqiang Zhong", "Jiayi Liu", "Xiaoxi Zhang"], "title": "Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks", "categories": ["cs.LG", "cs.AI", "cs.NI"], "comment": null, "summary": "Federated fine-tuning has emerged as a promising approach for adapting\nfoundation models (FMs) to diverse downstream tasks in edge environments. In\nInternet of Vehicles (IoV) systems, enabling efficient and low-latency\nmulti-task adaptation is particularly challenging due to client mobility,\nheterogeneous resources, and intermittent connectivity. This paper proposes a\nhierarchical federated fine-tuning framework that coordinates roadside units\n(RSUs) and vehicles to support resource-aware and mobility-resilient learning\nacross dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we\nintroduce a decentralized, energy-aware rank adaptation mechanism formulated as\na constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is\ndeveloped to enable adaptive exploration under per-task energy budgets,\nachieving provable sublinear regret. To evaluate our method, we construct a\nlarge-scale IoV simulator based on real-world trajectories, capturing dynamic\nparticipation, RSU handoffs, and communication variability. Extensive\nexperiments show that our approach achieves the best accuracy-efficiency\ntrade-off among all baselines, reducing latency by over 24\\% and improving\naverage accuracy by more than 2.5\\%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5206\u5c42\u8054\u90a6\u5fae\u8c03\u6846\u67b6\uff0c\u7ed3\u5408LoRA\u548cUCB-DUAL\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u8f66\u8054\u7f51\uff08IoV\uff09\u4e2d\u57fa\u7840\u6a21\u578b\u5728\u79fb\u52a8\u3001\u5f02\u6784\u548c\u95f4\u6b47\u6027\u8fde\u63a5\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u4f4e\u5ef6\u8fdf\u9002\u5e94\u95ee\u9898\uff0c\u5e76\u5728\u4eff\u771f\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u8054\u90a6\u5fae\u8c03\u662f\u4f7f\u57fa\u7840\u6a21\u578b\u9002\u5e94\u8fb9\u7f18\u73af\u5883\uff08\u5982\u8f66\u8054\u7f51\uff09\u7684\u6709\u524d\u666f\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u8f66\u8054\u7f51\u4e2d\u5ba2\u6237\u7aef\u7684\u79fb\u52a8\u6027\u3001\u8d44\u6e90\u5f02\u6784\u6027\u548c\u8fde\u63a5\u95f4\u6b47\u6027\uff0c\u7ed9\u5b9e\u73b0\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u591a\u4efb\u52a1\u9002\u5e94\u5e26\u6765\u4e86\u5de8\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u8054\u90a6\u5fae\u8c03\u6846\u67b6\uff0c\u534f\u8c03\u8def\u8fb9\u5355\u5143\uff08RSU\uff09\u548c\u8f66\u8f86\u8fdb\u884c\u8d44\u6e90\u611f\u77e5\u548c\u79fb\u52a8\u5f39\u6027\u5b66\u4e60\u3002\u5229\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\uff0c\u8bbe\u8ba1\u53bb\u4e2d\u5fc3\u5316\u3001\u80fd\u91cf\u611f\u77e5\u7684\u79e9\u9002\u5e94\u673a\u5236\uff0c\u5e76\u5c06\u5176\u5efa\u6a21\u4e3a\u53d7\u7ea6\u675f\u7684\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684UCB-DUAL\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4efb\u52a1\u80fd\u8017\u9884\u7b97\u4e0b\u7684\u81ea\u9002\u5e94\u63a2\u7d22\uff0c\u5e76\u63d0\u4f9b\u4e86\u6b21\u7ebf\u6027\u9057\u61be\u7684\u7406\u8bba\u8bc1\u660e\u3002\u901a\u8fc7\u57fa\u4e8e\u771f\u5b9e\u8f68\u8ff9\u7684\u5927\u89c4\u6a21IoV\u6a21\u62df\u5668\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e0e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u51c6\u786e\u6027-\u6548\u7387\u6743\u8861\uff0c\u5c06\u5ef6\u8fdf\u964d\u4f4e\u4e86\u8d85\u8fc724%\uff0c\u5e76\u5c06\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u8d85\u8fc72.5%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u5c42\u8054\u90a6\u5fae\u8c03\u6846\u67b6\u53ca\u5176UCB-DUAL\u7b97\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u52a8\u6001\u8f66\u8054\u7f51\u73af\u5883\u4e2d\u57fa\u7840\u6a21\u578b\u7684\u9002\u5e94\u6311\u6218\uff0c\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u5ef6\u8fdf\u65b9\u9762\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.09194", "pdf": "https://arxiv.org/pdf/2508.09194", "abs": "https://arxiv.org/abs/2508.09194", "authors": ["Yipeng Du", "Zihao Wang", "Ahmad Farhan", "Claudio Angione", "Harry Yang", "Fielding Johnston", "James P. Buban", "Patrick Colangelo", "Yue Zhao", "Yuzhe Yang"], "title": "Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments", "categories": ["cs.LG", "cs.AI"], "comment": "COLM2025", "summary": "The deployment of large-scale models, such as large language models (LLMs),\nincurs substantial costs due to their computational demands. To mitigate these\ncosts and address challenges related to scalability and data security, there is\na growing shift towards decentralized systems for model deployment, where\nchoosing efficient inference acceleration schemes become crucial to manage\ncomputational resources effectively and enhance system responsiveness. In this\nwork, we address the challenge of selecting optimal acceleration methods in\ndecentralized systems by introducing a meta-learning-based framework. This\nframework automates the selection process by learning from historical\nperformance data of various acceleration techniques across different tasks.\nUnlike traditional methods that rely on random selection or expert intuition,\nour approach systematically identifies the best acceleration strategies based\non the specific characteristics of each task. We demonstrate that our\nmeta-learning framework not only streamlines the decision-making process but\nalso consistently outperforms conventional methods in terms of efficiency and\nperformance. Our results highlight the potential of inference acceleration in\ndecentralized AI systems, offering a path towards more democratic and\neconomically feasible artificial intelligence solutions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u53bb\u4e2d\u5fc3\u5316AI\u7cfb\u7edf\u4e2d\u7684\u5927\u89c4\u6a21\u6a21\u578b\u90e8\u7f72\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u7684\u63a8\u7406\u52a0\u901f\u65b9\u6cd5\uff0c\u4ee5\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21\u6a21\u578b\uff08\u5982LLMs\uff09\u7684\u90e8\u7f72\u56e0\u8ba1\u7b97\u9700\u6c42\u9ad8\u6602\u800c\u5bfc\u81f4\u6210\u672c\u5de8\u5927\u3002\u4e3a\u964d\u4f4e\u6210\u672c\u5e76\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u53ca\u6570\u636e\u5b89\u5168\u95ee\u9898\uff0c\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u6210\u4e3a\u8d8b\u52bf\uff0c\u5176\u4e2d\u9009\u62e9\u9ad8\u6548\u7684\u63a8\u7406\u52a0\u901f\u65b9\u6848\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u4e0d\u540c\u52a0\u901f\u6280\u672f\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u7684\u5386\u53f2\u6027\u80fd\u6570\u636e\uff0c\u5b9e\u73b0\u5bf9\u6700\u4f73\u52a0\u901f\u65b9\u6cd5\u7684\u81ea\u52a8\u5316\u548c\u7cfb\u7edf\u5316\u9009\u62e9\uff0c\u533a\u522b\u4e8e\u4f20\u7edf\u7684\u968f\u673a\u9009\u62e9\u6216\u4e13\u5bb6\u76f4\u89c9\u65b9\u6cd5\u3002", "result": "\u8be5\u5143\u5b66\u4e60\u6846\u67b6\u4e0d\u4ec5\u7b80\u5316\u4e86\u51b3\u7b56\u8fc7\u7a0b\uff0c\u800c\u4e14\u5728\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u63a8\u7406\u52a0\u901f\u5728\u53bb\u4e2d\u5fc3\u5316AI\u7cfb\u7edf\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u5b9e\u73b0\u66f4\u6c11\u4e3b\u3001\u7ecf\u6d4e\u53ef\u884c\u7684AI\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u9014\u5f84\u3002"}}
{"id": "2508.09404", "pdf": "https://arxiv.org/pdf/2508.09404", "abs": "https://arxiv.org/abs/2508.09404", "authors": ["Guangxun Zhu", "Shiyu Fan", "Hang Dai", "Edmond S. L. Ho"], "title": "Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving", "categories": ["cs.CV", "cs.MM"], "comment": "ACM Multimedia 2025 (Dataset Track) Paper", "summary": "Large-scale high-quality 3D motion datasets with multi-person interactions\nare crucial for data-driven models in autonomous driving to achieve\nfine-grained pedestrian interaction understanding in dynamic urban\nenvironments. However, existing datasets mostly rely on estimating 3D poses\nfrom monocular RGB video frames, which suffer from occlusion and lack of\ntemporal continuity, thus resulting in unrealistic and low-quality human\nmotion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale\ndataset providing high-quality, temporally coherent 3D skeletal motions with\nexplicit interaction semantics, derived from the Waymo Perception dataset. Our\nkey insight is to utilize 3D human body shape and motion priors to enhance the\nquality of the 3D pose sequences extracted from the raw LiDRA point clouds. The\ndataset covers over 14,000 seconds across more than 800 real driving scenarios,\nincluding rich interactions among an average of 27 agents per scene (with up to\n250 agents in the largest scene). Furthermore, we establish 3D pose forecasting\nbenchmarks under varying pedestrian densities, and the results demonstrate its\nvalue as a foundational resource for future research on fine-grained human\nbehavior understanding in complex urban environments. The dataset and code will\nbe available at https://github.com/GuangxunZhu/Waymo-3DSkelMo", "AI": {"tldr": "\u63d0\u51faWaymo-3DSkelMo\u6570\u636e\u96c6\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u3001\u65f6\u95f4\u8fde\u8d2f\u76843D\u9aa8\u9abc\u8fd0\u52a8\u6570\u636e\u96c6\uff0c\u5305\u542b\u663e\u5f0f\u4ea4\u4e92\u8bed\u4e49\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u884c\u4eba\u884c\u4e3a\u7406\u89e3\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u6570\u636e\u9a71\u52a8\u6a21\u578b\u9700\u8981\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u3001\u5305\u542b\u591a\u4eba\u4ea4\u4e92\u76843D\u8fd0\u52a8\u6570\u636e\u96c6\u6765\u7406\u89e3\u884c\u4eba\u884c\u4e3a\u3002\u7136\u800c\uff0c\u73b0\u6709\u6570\u636e\u96c6\u591a\u4f9d\u8d56\u5355\u76eeRGB\u89c6\u9891\uff0c\u5b58\u5728\u906e\u6321\u3001\u65f6\u95f4\u4e0d\u8fde\u8d2f\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u8fd0\u52a8\u8d28\u91cf\u4f4e\u4e14\u4e0d\u771f\u5b9e\u3002", "method": "\u5229\u75283D\u4eba\u4f53\u5f62\u72b6\u548c\u8fd0\u52a8\u5148\u9a8c\u77e5\u8bc6\uff0c\u63d0\u9ad8\u4ece\u539f\u59cbLiDAR\u70b9\u4e91\u4e2d\u63d0\u53d6\u76843D\u59ff\u6001\u5e8f\u5217\u7684\u8d28\u91cf\u3002", "result": "\u5efa\u7acb\u4e86Waymo-3DSkelMo\u6570\u636e\u96c6\uff0c\u5176\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u3001\u65f6\u95f4\u8fde\u8d2f\u4e14\u5305\u542b\u663e\u5f0f\u4ea4\u4e92\u8bed\u4e49\u76843D\u9aa8\u9abc\u8fd0\u52a8\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u8986\u76d6\u8d85\u8fc714,000\u79d2\uff0c\u5305\u542b800\u591a\u4e2a\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\uff0c\u5e73\u5747\u6bcf\u573a\u666f27\u4e2a\u667a\u80fd\u4f53\uff08\u6700\u591a250\u4e2a\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u5efa\u7acb\u4e863D\u59ff\u6001\u9884\u6d4b\u57fa\u51c6\u3002", "conclusion": "Waymo-3DSkelMo\u6570\u636e\u96c6\u662f\u672a\u6765\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4eba\u7c7b\u884c\u4e3a\u7406\u89e3\u7814\u7a76\u7684\u5b9d\u8d35\u57fa\u7840\u8d44\u6e90\u3002"}}
{"id": "2508.09804", "pdf": "https://arxiv.org/pdf/2508.09804", "abs": "https://arxiv.org/abs/2508.09804", "authors": ["Ahmed Masry", "Abhay Puri", "Masoud Hashemi", "Juan A. Rodriguez", "Megh Thakkar", "Khyati Mahajan", "Vikas Yadav", "Sathwik Tejaswi Madhusudhan", "Alexandre Pich\u00e9", "Dzmitry Bahdanau", "Christopher Pal", "David Vazquez", "Enamul Hoque", "Perouz Taslakian", "Sai Rajeswar", "Spandana Gella"], "title": "BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning", "categories": ["cs.CL"], "comment": null, "summary": "Charts are essential to data analysis, transforming raw data into clear\nvisual representations that support human decision-making. Although current\nvision-language models (VLMs) have made significant progress, they continue to\nstruggle with chart comprehension due to training on datasets that lack\ndiversity and real-world authenticity, or on automatically extracted underlying\ndata tables of charts, which can contain numerous estimation errors.\nFurthermore, existing models only rely on supervised fine-tuning using these\nlow-quality datasets, severely limiting their effectiveness. To address these\nissues, we first propose BigCharts, a dataset creation pipeline that generates\nvisually diverse chart images by conditioning the rendering process on\nreal-world charts sourced from multiple online platforms. Unlike purely\nsynthetic datasets, BigCharts incorporates real-world data, ensuring\nauthenticity and visual diversity, while still retaining accurate underlying\ndata due to our proposed replotting process. Additionally, we introduce a\ncomprehensive training framework that integrates supervised fine-tuning with\nGroup Relative Policy Optimization (GRPO)-based reinforcement learning. By\nintroducing novel reward signals specifically designed for chart reasoning, our\napproach enhances model robustness and generalization across diverse chart\nstyles and domains, resulting in a state-of-the-art chart reasoning model,\nBigCharts-R1. Extensive experiments demonstrate that our models surpass\nexisting methods on multiple chart question-answering benchmarks compared to\neven larger open-source and closed-source models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBigCharts\u6570\u636e\u96c6\u751f\u6210\u6d41\u7a0b\u53ca\u878d\u5408\u76d1\u7763\u5fae\u8c03\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5f00\u53d1\u51fa\u6027\u80fd\u5353\u8d8a\u7684BigCharts-R1\u6a21\u578b\u3002", "motivation": "\u56fe\u8868\u662f\u6570\u636e\u5206\u6790\u548c\u51b3\u7b56\u7684\u5173\u952e\uff0c\u4f46\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u56e0\u8bad\u7ec3\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u6837\u6027\u3001\u771f\u5b9e\u6027\u53ca\u5e95\u5c42\u6570\u636e\u8bef\u5dee\uff0c\u4e14\u4ec5\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\uff0c\u5bfc\u81f4\u5176\u56fe\u8868\u7406\u89e3\u80fd\u529b\u6709\u9650\u3002", "method": "1. \u63d0\u51fa\u4e86BigCharts\u6570\u636e\u96c6\u521b\u5efa\u6d41\u7a0b\uff0c\u901a\u8fc7\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u56fe\u8868\u8fdb\u884c\u6761\u4ef6\u6e32\u67d3\u548c\u91cd\u7ed8\u8fc7\u7a0b\uff0c\u751f\u6210\u89c6\u89c9\u591a\u6837\u4e14\u5e95\u5c42\u6570\u636e\u51c6\u786e\u7684\u56fe\u8868\u56fe\u50cf\u30022. \u5f15\u5165\u4e86\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u57fa\u4e8e\u7fa4\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u7684\u7efc\u5408\u8bad\u7ec3\u6846\u67b6\u30023. \u8bbe\u8ba1\u4e86\u4e13\u4e3a\u56fe\u8868\u63a8\u7406\u5b9a\u5236\u7684\u65b0\u578b\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5f00\u53d1\u51fa\u540d\u4e3aBigCharts-R1\u7684\u5148\u8fdb\u56fe\u8868\u63a8\u7406\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u56fe\u8868\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u751a\u81f3\u4f18\u4e8e\u66f4\u5927\u7684\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684BigCharts\u6570\u636e\u96c6\u548c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u6210\u529f\u6784\u5efa\u4e86\u6700\u5148\u8fdb\u7684\u56fe\u8868\u63a8\u7406\u6a21\u578bBigCharts-R1\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4e0a\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2508.09663", "pdf": "https://arxiv.org/pdf/2508.09663", "abs": "https://arxiv.org/abs/2508.09663", "authors": ["Philipp A. Friese", "Ahmed Eleliemy", "Utz-Uwe Haus", "Martin Schulz"], "title": "Closing the HPC-Cloud Convergence Gap: Multi-Tenant Slingshot RDMA for Kubernetes", "categories": ["cs.DC", "cs.NI"], "comment": "10 pages, 12 figures, 1 table, 3 listings, to be published in IEEE\n  Cluster 2025", "summary": "Converged HPC-Cloud computing is an emerging computing paradigm that aims to\nsupport increasingly complex and multi-tenant scientific workflows. These\nsystems require reconciliation of the isolation requirements of native cloud\nworkloads and the performance demands of HPC applications. In this context,\nnetworking hardware is a critical boundary component: it is the conduit for\nhigh-throughput, low-latency communication and enables isolation across\ntenants. HPE Slingshot is a high-speed network interconnect that provides up to\n200 Gbps of throughput per port and targets high-performance computing (HPC)\nsystems. The Slingshot host software, including hardware drivers and network\nmiddleware libraries, is designed to meet HPC deployments, which predominantly\nuse single-tenant access modes. Hence, the Slingshot stack is not suited for\nsecure use in multi-tenant deployments, such as converged HPC-Cloud\ndeployments. In this paper, we design and implement an extension to the\nSlingshot stack targeting converged deployments on the basis of Kubernetes. Our\nintegration provides secure, container-granular, and multi-tenant access to\nSlingshot RDMA networking capabilities at minimal overhead.", "AI": {"tldr": "\u9488\u5bf9\u878d\u5408HPC-Cloud\u73af\u5883\uff0c\u672c\u6587\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86HPE Slingshot\u7f51\u7edc\u7684\u6269\u5c55\uff0c\u4ee5\u63d0\u4f9b\u5b89\u5168\u3001\u5bb9\u5668\u7c92\u5ea6\u7684\u591a\u79df\u6237RDMA\u8bbf\u95ee\uff0c\u4e14\u5f00\u9500\u6781\u5c0f\u3002", "motivation": "\u878d\u5408HPC-Cloud\u8ba1\u7b97\u9700\u8981\u517c\u987e\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u7684\u9694\u79bb\u6027\u4e0eHPC\u5e94\u7528\u7684\u9ad8\u6027\u80fd\u9700\u6c42\u3002HPE Slingshot\u7f51\u7edc\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u5176\u8f6f\u4ef6\u6808\u4e13\u4e3a\u5355\u79df\u6237HPC\u90e8\u7f72\u8bbe\u8ba1\uff0c\u4e0d\u9002\u5408\u591a\u79df\u6237\u878d\u5408HPC-Cloud\u73af\u5883\u7684\u5b89\u5168\u4f7f\u7528\uff0c\u56e0\u6b64\u4e9f\u9700\u89e3\u51b3\u5176\u5728\u591a\u79df\u6237\u73af\u5883\u4e0b\u7684\u5b89\u5168\u8bbf\u95ee\u95ee\u9898\u3002", "method": "\u7814\u7a76\u8005\u57fa\u4e8eKubernetes\uff0c\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u9488\u5bf9\u878d\u5408\u90e8\u7f72\u7684HPE Slingshot\u5806\u6808\u6269\u5c55\u3002\u6b64\u6269\u5c55\u65e8\u5728\u63d0\u4f9b\u5b89\u5168\u3001\u5bb9\u5668\u7c92\u5ea6\u4e14\u591a\u79df\u6237\u7684Slingshot RDMA\u7f51\u7edc\u8bbf\u95ee\u80fd\u529b\u3002", "result": "\u6240\u5b9e\u73b0\u7684\u96c6\u6210\u65b9\u6848\u6210\u529f\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u5bb9\u5668\u7c92\u5ea6\u7684\u591a\u79df\u6237Slingshot RDMA\u7f51\u7edc\u8bbf\u95ee\u80fd\u529b\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u6781\u5c0f\u7684\u7cfb\u7edf\u5f00\u9500\u3002", "conclusion": "\u901a\u8fc7\u4e3aHPE Slingshot\u7f51\u7edc\u5806\u6808\u5f15\u5165\u57fa\u4e8eKubernetes\u7684\u6269\u5c55\uff0c\u7814\u7a76\u6210\u529f\u89e3\u51b3\u4e86\u878d\u5408HPC-Cloud\u73af\u5883\u4e2d\u9ad8\u6027\u80fd\u7f51\u7edc\u7684\u591a\u79df\u6237\u5b89\u5168\u8bbf\u95ee\u6311\u6218\uff0c\u4f7f\u5f97Slingshot\u80fd\u591f\u5b89\u5168\u3001\u9ad8\u6548\u5730\u652f\u6301\u591a\u79df\u6237\u5bb9\u5668\u5316\u79d1\u5b66\u5de5\u4f5c\u6d41\u3002"}}
{"id": "2508.09198", "pdf": "https://arxiv.org/pdf/2508.09198", "abs": "https://arxiv.org/abs/2508.09198", "authors": ["Li Kong", "Bingzhe Wang", "Zhou Chen", "Suhan Hu", "Yuchao Ma", "Qi Qi", "Suoyuan Song", "Bicheng Jin"], "title": "ADT4Coupons: An Innovative Framework for Sequential Coupon Distribution in E-commerce", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Coupon distribution is a critical marketing strategy used by online platforms\nto boost revenue and enhance user engagement. Regrettably, existing coupon\ndistribution strategies fall far short of effectively leveraging the complex\nsequential interactions between platforms and users. This critical oversight,\ndespite the abundance of e-commerce log data, has precipitated a performance\nplateau. In this paper, we focus on the scene that the platforms make\nsequential coupon distribution decision multiple times for various users, with\neach user interacting with the platform repeatedly. Based on this marketing\nscenario, we propose a novel marketing framework, named Aligned Decision\nTransformer for Coupons (ADT4Coupons), to directly devise coupon distribution\npolicy for long-term revenue boosting. ADT4Coupons enables optimized online\ndecision-making in a variety of real-world marketing scenarios. It achieves\nthis by seamlessly integrating three key characteristics, general scenarios,\nsequential modeling with more comprehensive historical data, and efficient\niterative updates within a unified framework. Furthermore, empirical results on\nreal-world industrial dataset, alongside public and synthetic datasets\ndemonstrate the superiority of our framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aADT4Coupons\u7684\u65b0\u578b\u8425\u9500\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u5bf9\u5e73\u53f0\u4e0e\u7528\u6237\u4e4b\u95f4\u590d\u6742\u7684\u987a\u5e8f\u4ea4\u4e92\u8fdb\u884c\u5efa\u6a21\uff0c\u4f18\u5316\u4f18\u60e0\u5238\u5206\u53d1\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u957f\u671f\u6536\u5165\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4f18\u60e0\u5238\u5206\u53d1\u7b56\u7565\u672a\u80fd\u6709\u6548\u5229\u7528\u5e73\u53f0\u4e0e\u7528\u6237\u95f4\u590d\u6742\u7684\u987a\u5e8f\u4ea4\u4e92\u6570\u636e\uff0c\u5bfc\u81f4\u6027\u80fd\u505c\u6ede\uff0c\u5c3d\u7ba1\u6709\u5927\u91cf\u7535\u5546\u65e5\u5fd7\u6570\u636e\u53ef\u4f9b\u5229\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u63d0\u5347\u957f\u671f\u6536\u5165\u7684\u6709\u6548\u7b56\u7565\u3002", "method": "\u57fa\u4e8e\u5e73\u53f0\u5bf9\u4e0d\u540c\u7528\u6237\u8fdb\u884c\u591a\u6b21\u987a\u5e8f\u4f18\u60e0\u5238\u5206\u53d1\u51b3\u7b56\u7684\u8425\u9500\u573a\u666f\uff0c\u63d0\u51faAligned Decision Transformer for Coupons (ADT4Coupons) \u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408\u901a\u7528\u573a\u666f\u9002\u5e94\u6027\u3001\u57fa\u4e8e\u66f4\u5168\u9762\u5386\u53f2\u6570\u636e\u7684\u987a\u5e8f\u5efa\u6a21\u4ee5\u53ca\u9ad8\u6548\u8fed\u4ee3\u66f4\u65b0\u4e09\u5927\u7279\u6027\uff0c\u76f4\u63a5\u5236\u5b9a\u4f18\u60e0\u5238\u5206\u53d1\u7b56\u7565\u4ee5\u63d0\u5347\u957f\u671f\u6536\u5165\u3002", "result": "\u5728\u771f\u5b9e\u7684\u5de5\u4e1a\u6570\u636e\u96c6\u4ee5\u53ca\u516c\u5f00\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684ADT4Coupons\u6846\u67b6\u5177\u6709\u4f18\u8d8a\u6027\u3002", "conclusion": "ADT4Coupons\u6846\u67b6\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u590d\u6742\u7684\u987a\u5e8f\u4ea4\u4e92\uff0c\u4e3a\u5e73\u53f0\u4f18\u5316\u4f18\u60e0\u5238\u5206\u53d1\u7b56\u7565\u4ee5\u5b9e\u73b0\u957f\u671f\u6536\u5165\u589e\u957f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09415", "pdf": "https://arxiv.org/pdf/2508.09415", "abs": "https://arxiv.org/abs/2508.09415", "authors": ["John S. O'Meara", "Jared Hwang", "Zeyu Wang", "Michael Saugstad", "Jon E. Froehlich"], "title": "RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in Streetscape Images from Open Government Metadata", "categories": ["cs.CV", "cs.AI", "I.2"], "comment": "Accepted to the ICCV'25 Workshop on Vision Foundation Models and\n  Generative AI for Accessibility: Challenges and Opportunities", "summary": "Curb ramps are critical for urban accessibility, but robustly detecting them\nin images remains an open problem due to the lack of large-scale, high-quality\ndatasets. While prior work has attempted to improve data availability with\ncrowdsourced or manually labeled data, these efforts often fall short in either\nquality or scale. In this paper, we introduce and evaluate a two-stage pipeline\ncalled RampNet to scale curb ramp detection datasets and improve model\nperformance. In Stage 1, we generate a dataset of more than 210,000 annotated\nGoogle Street View (GSV) panoramas by auto-translating government-provided curb\nramp location data to pixel coordinates in panoramic images. In Stage 2, we\ntrain a curb ramp detection model (modified ConvNeXt V2) from the generated\ndataset, achieving state-of-the-art performance. To evaluate both stages of our\npipeline, we compare to manually labeled panoramas. Our generated dataset\nachieves 94.0% precision and 92.5% recall, and our detection model reaches\n0.9236 AP -- far exceeding prior work. Our work contributes the first\nlarge-scale, high-quality curb ramp detection dataset, benchmark, and model.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRampNet\u53cc\u9636\u6bb5\u7ba1\u9053\uff0c\u901a\u8fc7\u81ea\u52a8\u8f6c\u6362\u653f\u5e9c\u6570\u636e\u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u57ce\u5e02\u5761\u9053\u68c0\u6d4b\u6570\u636e\u96c6\uff08\u903e21\u4e07\u5f20\uff09\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bad\u7ec3\u51fa\u6027\u80fd\u5353\u8d8a\uff080.9236 AP\uff09\u7684\u68c0\u6d4b\u6a21\u578b\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u57ce\u5e02\u5761\u9053\u68c0\u6d4b\u5bf9\u57ce\u5e02\u65e0\u969c\u788d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u5bfc\u81f4\u5176\u5728\u56fe\u50cf\u4e2d\u9c81\u68d2\u68c0\u6d4b\u4ecd\u662f\u5f00\u653e\u95ee\u9898\u3002\u73b0\u6709\u4f17\u5305\u6216\u624b\u52a8\u6807\u6ce8\u65b9\u6cd5\u5728\u8d28\u91cf\u6216\u89c4\u6a21\u4e0a\u4e0d\u8db3\u3002", "method": "\u5f15\u5165RampNet\u53cc\u9636\u6bb5\u7ba1\u9053\uff1a\n1. \u6570\u636e\u96c6\u751f\u6210\uff1a\u5c06\u653f\u5e9c\u63d0\u4f9b\u7684\u5761\u9053\u4f4d\u7f6e\u6570\u636e\u81ea\u52a8\u8f6c\u6362\u4e3aGoogle\u8857\u666f\u5168\u666f\u56fe\u4e2d\u7684\u50cf\u7d20\u5750\u6807\uff0c\u751f\u6210\u8d85\u8fc721\u4e07\u5f20\u6807\u6ce8\u56fe\u50cf\u7684\u6570\u636e\u96c6\u3002\n2. \u6a21\u578b\u8bad\u7ec3\uff1a\u5229\u7528\u751f\u6210\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u4e00\u4e2a\u6539\u8fdb\u7684ConvNeXt V2\u5761\u9053\u68c0\u6d4b\u6a21\u578b\u3002\n\u8bc4\u4f30\u65b9\u6cd5\uff1a\u901a\u8fc7\u4e0e\u624b\u52a8\u6807\u6ce8\u7684\u5168\u666f\u56fe\u8fdb\u884c\u6bd4\u8f83\u6765\u9a8c\u8bc1\u6570\u636e\u96c6\u548c\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u751f\u6210\u7684\u6570\u636e\u96c6\u53d6\u5f97\u4e8694.0%\u7684\u7cbe\u5ea6\u548c92.5%\u7684\u53ec\u56de\u7387\u3002\u8bad\u7ec3\u7684\u68c0\u6d4b\u6a21\u578b\u8fbe\u5230\u4e860.9236 AP\uff0c\u8fdc\u8d85\u73b0\u6709\u7814\u7a76\u6210\u679c\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u8d21\u732e\u4e86\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u57ce\u5e02\u5761\u9053\u68c0\u6d4b\u6570\u636e\u96c6\u3001\u57fa\u51c6\u548c\u6a21\u578b\u3002"}}
{"id": "2508.09809", "pdf": "https://arxiv.org/pdf/2508.09809", "abs": "https://arxiv.org/abs/2508.09809", "authors": ["Aishik Mandal", "Prottay Kumar Adhikary", "Hiba Arnaout", "Iryna Gurevych", "Tanmoy Chakraborty"], "title": "A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 3 figures", "summary": "Mental health disorders are rising worldwide. However, the availability of\ntrained clinicians has not scaled proportionally, leaving many people without\nadequate or timely support. To bridge this gap, recent studies have shown the\npromise of Artificial Intelligence (AI) to assist mental health diagnosis,\nmonitoring, and intervention. However, the development of efficient, reliable,\nand ethical AI to assist clinicians is heavily dependent on high-quality\nclinical training datasets. Despite growing interest in data curation for\ntraining clinical AI assistants, existing datasets largely remain scattered,\nunder-documented, and often inaccessible, hindering the reproducibility,\ncomparability, and generalizability of AI models developed for clinical mental\nhealth care. In this paper, we present the first comprehensive survey of\nclinical mental health datasets relevant to the training and development of\nAI-powered clinical assistants. We categorize these datasets by mental\ndisorders (e.g., depression, schizophrenia), data modalities (e.g., text,\nspeech, physiological signals), task types (e.g., diagnosis prediction, symptom\nseverity estimation, intervention generation), accessibility (public,\nrestricted or private), and sociocultural context (e.g., language and cultural\nbackground). Along with these, we also investigate synthetic clinical mental\nhealth datasets. Our survey identifies critical gaps such as a lack of\nlongitudinal data, limited cultural and linguistic representation, inconsistent\ncollection and annotation standards, and a lack of modalities in synthetic\ndata. We conclude by outlining key challenges in curating and standardizing\nfuture datasets and provide actionable recommendations to facilitate the\ndevelopment of more robust, generalizable, and equitable mental health AI\nsystems.", "AI": {"tldr": "\u9488\u5bf9AI\u9a71\u52a8\u5fc3\u7406\u5065\u5eb7\u52a9\u624b\u5f00\u53d1\u6240\u9700\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u672c\u6587\u9996\u6b21\u5168\u9762\u7efc\u8ff0\u4e86\u4e34\u5e8a\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6570\u636e\u7684\u5173\u952e\u7f3a\u9677\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6570\u636e\u96c6\u7684\u6784\u5efa\u548c\u6807\u51c6\u5316\u5efa\u8bae\u3002", "motivation": "\u5168\u7403\u5fc3\u7406\u5065\u5eb7\u969c\u788d\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5408\u683c\u4e34\u5e8a\u533b\u751f\u6570\u91cf\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5927\u91cf\u9700\u6c42\u672a\u88ab\u6ee1\u8db3\u3002\u5c3d\u7ba1AI\u5728\u5fc3\u7406\u5065\u5eb7\u8f85\u52a9\u65b9\u9762\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5176\u53d1\u5c55\u4e25\u91cd\u4f9d\u8d56\u9ad8\u8d28\u91cf\u7684\u4e34\u5e8a\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u73b0\u6709\u6570\u636e\u96c6\u5206\u6563\u3001\u6587\u6863\u4e0d\u5168\u4e14\u4e0d\u6613\u83b7\u53d6\uff0c\u963b\u788d\u4e86AI\u6a21\u578b\u7684\u53ef\u590d\u73b0\u6027\u3001\u53ef\u6bd4\u6027\u548c\u6cdb\u5316\u6027\u3002", "method": "\u672c\u6587\u5bf9\u4e0eAI\u9a71\u52a8\u4e34\u5e8a\u52a9\u624b\u8bad\u7ec3\u548c\u5f00\u53d1\u76f8\u5173\u7684\u4e34\u5e8a\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u9996\u6b21\u5168\u9762\u8c03\u67e5\u3002\u8fd9\u4e9b\u6570\u636e\u96c6\u6309\u7cbe\u795e\u75be\u75c5\u3001\u6570\u636e\u6a21\u6001\u3001\u4efb\u52a1\u7c7b\u578b\u3001\u53ef\u8bbf\u95ee\u6027\u4ee5\u53ca\u793e\u4f1a\u6587\u5316\u80cc\u666f\u8fdb\u884c\u5206\u7c7b\u3002\u540c\u65f6\uff0c\u4e5f\u5bf9\u5408\u6210\u4e34\u5e8a\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u7814\u7a76\u3002", "result": "\u8c03\u67e5\u7ed3\u679c\u8bc6\u522b\u51fa\u5f53\u524d\u6570\u636e\u96c6\u7684\u5173\u952e\u7f3a\u9677\uff0c\u5305\u62ec\u7f3a\u4e4f\u7eb5\u5411\u6570\u636e\u3001\u6587\u5316\u548c\u8bed\u8a00\u4ee3\u8868\u6027\u6709\u9650\u3001\u6570\u636e\u6536\u96c6\u548c\u6807\u6ce8\u6807\u51c6\u4e0d\u4e00\u81f4\uff0c\u4ee5\u53ca\u5408\u6210\u6570\u636e\u4e2d\u6a21\u6001\u7684\u7f3a\u5931\u3002", "conclusion": "\u672c\u6587\u6982\u8ff0\u4e86\u672a\u6765\u6570\u636e\u96c6\u5728\u6574\u7406\u548c\u6807\u51c6\u5316\u65b9\u9762\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u5efa\u8bae\uff0c\u4ee5\u4fc3\u8fdb\u5f00\u53d1\u66f4\u7a33\u5065\u3001\u66f4\u5177\u666e\u9002\u6027\u4e14\u66f4\u516c\u5e73\u7684\u5fc3\u7406\u5065\u5eb7AI\u7cfb\u7edf\u3002"}}
{"id": "2508.09708", "pdf": "https://arxiv.org/pdf/2508.09708", "abs": "https://arxiv.org/abs/2508.09708", "authors": ["Thomas Fehrenbach", "Luis Omar Ortiz Abrego", "Cornelius Hellge", "Thomas Schierl", "J\u00f6rg Ott"], "title": "3GPP NR V2X Mode 2d: Analysis of Distributed Scheduling for Groupcast using ns-3 5G LENA Simulator", "categories": ["eess.SP", "cs.NI", "C.2.1; C.2.2; C.2.4"], "comment": "7 pages, 10 figures, 2 tables, V2X communication, vehicular networks,\n  platooning simulation", "summary": "Vehicle-to-everything (V2X) communication is a key technology for enabling\nintelligent transportation systems (ITS) that can improve road safety, traffic\nefficiency, and environmental sustainability. Among the various V2X\napplications, platooning is one of the most promising ones, as it allows a\ngroup of vehicles to travel closely together at high speeds, reducing fuel\nconsumption and emissions. However, it poses significant challenges for\nwireless communication, such as high reliability and low latency. In this\npaper, we evaluate the benefits of group scheduling, also referred to as Mode\n2d, which is based on a distributed and scheduled resource allocation scheme\nthat allows the group of cars to select resources from a configured pool\nwithout network assistance. We evaluated the scheme through simulations, and\nthe results show that this approach can meet the reliability, low latency, and\ndata rate requirements for platooning.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e00\u79cd\u540d\u4e3aMode 2d\uff08\u7fa4\u7ec4\u8c03\u5ea6\uff09\u7684\u5206\u5e03\u5f0f\u8d44\u6e90\u5206\u914d\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u65e8\u5728\u6ee1\u8db3\u8f66\u8f86\u7f16\u961f\u901a\u4fe1\u5bf9\u9ad8\u53ef\u9760\u6027\u548c\u4f4e\u5ef6\u8fdf\u7684\u9700\u6c42\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u8f66\u8f86\u7f16\u961f\u4f5c\u4e3aV2X\u901a\u4fe1\u4e2d\u6781\u5177\u524d\u666f\u7684\u5e94\u7528\uff0c\u80fd\u6709\u6548\u964d\u4f4e\u6cb9\u8017\u548c\u6392\u653e\u3002\u7136\u800c\uff0c\u5b83\u5bf9\u65e0\u7ebf\u901a\u4fe1\u63d0\u51fa\u4e86\u4e25\u82db\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u53ef\u9760\u6027\u548c\u4f4e\u5ef6\u8fdf\u65b9\u9762\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u7fa4\u7ec4\u8c03\u5ea6\uff08Mode 2d\uff09\u65b9\u6848\u3002\u8be5\u65b9\u6848\u57fa\u4e8e\u5206\u5e03\u5f0f\u548c\u8c03\u5ea6\u5f0f\u7684\u8d44\u6e90\u5206\u914d\u673a\u5236\uff0c\u5141\u8bb8\u8f66\u8f86\u7f16\u961f\u5728\u65e0\u9700\u7f51\u7edc\u534f\u52a9\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u9884\u8bbe\u8d44\u6e90\u6c60\u4e2d\u9009\u62e9\u8d44\u6e90\u3002\u7814\u7a76\u901a\u8fc7\u4eff\u771f\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u8bc4\u4f30\u7684\u7fa4\u7ec4\u8c03\u5ea6\u65b9\u6cd5\u80fd\u591f\u6ee1\u8db3\u8f66\u8f86\u7f16\u961f\u5bf9\u53ef\u9760\u6027\u3001\u4f4e\u5ef6\u8fdf\u548c\u6570\u636e\u901f\u7387\u7684\u901a\u4fe1\u8981\u6c42\u3002", "conclusion": "\u7fa4\u7ec4\u8c03\u5ea6\uff08Mode 2d\uff09\u662f\u4e00\u79cd\u53ef\u884c\u7684\u901a\u4fe1\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8f66\u8f86\u7f16\u961f\u5e94\u7528\u4e2d\u7684\u9ad8\u53ef\u9760\u6027\u548c\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u6311\u6218\uff0c\u4ece\u800c\u52a9\u529b\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.09203", "pdf": "https://arxiv.org/pdf/2508.09203", "abs": "https://arxiv.org/abs/2508.09203", "authors": ["Zhenhui Ou", "Dawei Li", "Zhen Tan", "Wenlin Li", "Huan Liu", "Siyuan Song"], "title": "Building Safer Sites: A Large-Scale Multi-Level Dataset for Construction Safety Research", "categories": ["cs.LG"], "comment": "The paper was accepted on the CIKM 2025", "summary": "Construction safety research is a critical field in civil engineering, aiming\nto mitigate risks and prevent injuries through the analysis of site conditions\nand human factors. However, the limited volume and lack of diversity in\nexisting construction safety datasets pose significant challenges to conducting\nin-depth analyses. To address this research gap, this paper introduces the\nConstruction Safety Dataset (CSDataset), a well-organized comprehensive\nmulti-level dataset that encompasses incidents, inspections, and violations\nrecorded sourced from the Occupational Safety and Health Administration (OSHA).\nThis dataset uniquely integrates structured attributes with unstructured\nnarratives, facilitating a wide range of approaches driven by machine learning\nand large language models. We also conduct a preliminary approach benchmarking\nand various cross-level analyses using our dataset, offering insights to inform\nand enhance future efforts in construction safety. For example, we found that\ncomplaint-driven inspections were associated with a 17.3% reduction in the\nlikelihood of subsequent incidents. Our dataset and code are released at\nhttps://github.com/zhenhuiou/Construction-Safety-Dataset-CSDataset.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86CSDataset\uff0c\u4e00\u4e2a\u6574\u5408\u4e86\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\u7684\u5168\u9762\u591a\u7ea7\u65bd\u5de5\u5b89\u5168\u6570\u636e\u96c6\uff0c\u4ee5\u5f25\u8865\u73b0\u6709\u6570\u636e\u4e0d\u8db3\uff0c\u5e76\u4fc3\u8fdb\u673a\u5668\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u65bd\u5de5\u5b89\u5168\u6570\u636e\u96c6\u6570\u91cf\u6709\u9650\u4e14\u7f3a\u4e4f\u591a\u6837\u6027\uff0c\u963b\u788d\u4e86\u6df1\u5165\u5206\u6790\uff0c\u6784\u6210\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u201c\u65bd\u5de5\u5b89\u5168\u6570\u636e\u96c6\u201d\uff08CSDataset\uff09\uff0c\u4e00\u4e2a\u4eceOSHA\u83b7\u53d6\u7684\u3001\u5305\u542b\u4e8b\u6545\u3001\u68c0\u67e5\u548c\u8fdd\u89c4\u8bb0\u5f55\u7684\u5168\u9762\u591a\u7ea7\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u72ec\u7279\u5730\u7ed3\u5408\u4e86\u7ed3\u6784\u5316\u5c5e\u6027\u548c\u975e\u7ed3\u6784\u5316\u53d9\u8ff0\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u521d\u6b65\u7684\u65b9\u6cd5\u57fa\u51c6\u6d4b\u8bd5\u548c\u5404\u79cd\u8de8\u5c42\u7ea7\u5206\u6790\u3002", "result": "\u901a\u8fc7\u6570\u636e\u96c6\u5206\u6790\u53d1\u73b0\uff0c\u4f8b\u5982\uff0c\u7531\u6295\u8bc9\u9a71\u52a8\u7684\u68c0\u67e5\u53ef\u4f7f\u540e\u7eed\u4e8b\u6545\u53d1\u751f\u7684\u53ef\u80fd\u6027\u964d\u4f4e17.3%\u3002\u8be5\u6570\u636e\u96c6\u65e8\u5728\u4fc3\u8fdb\u673a\u5668\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u591a\u79cd\u5206\u6790\u65b9\u6cd5\u3002", "conclusion": "CSDataset\u7684\u5f15\u5165\u53ca\u5176\u521d\u6b65\u5206\u6790\u4e3a\u89e3\u51b3\u65bd\u5de5\u5b89\u5168\u6570\u636e\u4e0d\u8db3\u63d0\u4f9b\u4e86\u65b0\u7684\u8d44\u6e90\uff0c\u6709\u671b\u63a8\u52a8\u673a\u5668\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u7814\u7a76\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u5b89\u5168\u6539\u8fdb\u63d0\u4f9b\u6d1e\u5bdf\u3002"}}
{"id": "2508.09423", "pdf": "https://arxiv.org/pdf/2508.09423", "abs": "https://arxiv.org/abs/2508.09423", "authors": ["Badi Li", "Ren-jie Lu", "Yu Zhou", "Jingke Meng", "Wei-shi Zheng"], "title": "Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The Object Goal Navigation (ObjectNav) task challenges agents to locate a\nspecified object in an unseen environment by imagining unobserved regions of\nthe scene. Prior approaches rely on deterministic and discriminative models to\ncomplete semantic maps, overlooking the inherent uncertainty in indoor layouts\nand limiting their ability to generalize to unseen environments. In this work,\nwe propose GOAL, a generative flow-based framework that models the semantic\ndistribution of indoor environments by bridging observed regions with\nLLM-enriched full-scene semantic maps. During training, spatial priors inferred\nfrom large language models (LLMs) are encoded as two-dimensional Gaussian\nfields and injected into target maps, distilling rich contextual knowledge into\nthe flow model and enabling more generalizable completions. Extensive\nexperiments demonstrate that GOAL achieves state-of-the-art performance on MP3D\nand Gibson, and shows strong generalization in transfer settings to HM3D. Codes\nand pretrained models are available at https://github.com/Badi-Li/GOAL.", "AI": {"tldr": "GOAL\u662f\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u6d41\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408LLM\u589e\u5f3a\u7684\u8bed\u4e49\u5730\u56fe\uff0c\u63d0\u5347\u667a\u80fd\u4f53\u5728\u672a\u77e5\u73af\u5883\u4e2d\u8fdb\u884c\u7269\u4f53\u76ee\u6807\u5bfc\u822a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u7269\u4f53\u76ee\u6807\u5bfc\u822a\uff08ObjectNav\uff09\u4efb\u52a1\u8981\u6c42\u667a\u80fd\u4f53\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5b9a\u4f4d\u6307\u5b9a\u7269\u4f53\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u786e\u5b9a\u6027\u5224\u522b\u6a21\u578b\u5b8c\u6210\u8bed\u4e49\u5730\u56fe\uff0c\u4f46\u5ffd\u89c6\u4e86\u5ba4\u5185\u5e03\u5c40\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u9650\u5236\u4e86\u5bf9\u65b0\u73af\u5883\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86GOAL\uff0c\u4e00\u4e2a\u751f\u6210\u6d41\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c2\u6d4b\u533a\u57df\u4e0eLLM\uff08\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u589e\u5f3a\u7684\u5b8c\u6574\u573a\u666f\u8bed\u4e49\u5730\u56fe\u6765\u5efa\u6a21\u5ba4\u5185\u73af\u5883\u7684\u8bed\u4e49\u5206\u5e03\u3002\u8bad\u7ec3\u65f6\uff0c\u4eceLLM\u63a8\u65ad\u51fa\u7684\u7a7a\u95f4\u5148\u9a8c\u88ab\u7f16\u7801\u4e3a\u4e8c\u7ef4\u9ad8\u65af\u573a\u5e76\u6ce8\u5165\u5230\u76ee\u6807\u5730\u56fe\u4e2d\uff0c\u4ece\u800c\u5c06\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\u63d0\u70bc\u5230\u6d41\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u66f4\u5f3a\u7684\u6cdb\u5316\u5b8c\u6210\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGOAL\u5728MP3D\u548cGibson\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u8fc1\u79fb\u8bbe\u7f6e\u4e2d\u5bf9HM3D\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GOAL\u901a\u8fc7\u5f15\u5165\u751f\u6210\u6d41\u6a21\u578b\u548cLLM\u7684\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u6709\u6548\u89e3\u51b3\u4e86ObjectNav\u4efb\u52a1\u4e2d\u73b0\u6709\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6210\u679c\uff0c\u4e3a\u667a\u80fd\u4f53\u5728\u590d\u6742\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2508.09834", "pdf": "https://arxiv.org/pdf/2508.09834", "abs": "https://arxiv.org/abs/2508.09834", "authors": ["Weigao Sun", "Jiaxi Hu", "Yucheng Zhou", "Jusen Du", "Disen Lan", "Kexin Wang", "Tong Zhu", "Xiaoye Qu", "Yu Zhang", "Xiaoyu Mo", "Daizong Liu", "Yuxuan Liang", "Wenliang Chen", "Guoqi Li", "Yu Cheng"], "title": "Speed Always Wins: A Survey on Efficient Architectures for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Survey, 82 pages, GitHub:\n  https://github.com/weigao266/Awesome-Efficient-Arch", "summary": "Large Language Models (LLMs) have delivered impressive results in language\nunderstanding, generation, reasoning, and pushes the ability boundary of\nmultimodal models. Transformer models, as the foundation of modern LLMs, offer\na strong baseline with excellent scaling properties. However, the traditional\ntransformer architecture requires substantial computations and poses\nsignificant obstacles for large-scale training and practical deployment. In\nthis survey, we offer a systematic examination of innovative LLM architectures\nthat address the inherent limitations of transformers and boost the efficiency.\nStarting from language modeling, this survey covers the background and\ntechnical details of linear and sparse sequence modeling methods, efficient\nfull attention variants, sparse mixture-of-experts, hybrid model architectures\nincorporating the above techniques, and emerging diffusion LLMs. Additionally,\nwe discuss applications of these techniques to other modalities and consider\ntheir wider implications for developing scalable, resource-aware foundation\nmodels. By grouping recent studies into the above category, this survey\npresents a blueprint of modern efficient LLM architectures, and we hope this\ncould help motivate future research toward more efficient, versatile AI\nsystems.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u7cfb\u7edf\u6027\u5730\u8003\u5bdf\u4e86\u4e3a\u63d0\u9ad8\u6548\u7387\u800c\u8bbe\u8ba1\u7684\u521b\u65b0LLM\u67b6\u6784\uff0c\u4ee5\u514b\u670d\u4f20\u7edfTransformer\u6a21\u578b\u7684\u8ba1\u7b97\u5c40\u9650\u6027\u3002", "motivation": "\u5c3d\u7ba1Transformer\u6a21\u578b\u5728LLM\u4e2d\u8868\u73b0\u51fa\u8272\u5e76\u5177\u6709\u826f\u597d\u7684\u6269\u5c55\u6027\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u5bf9\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u5b9e\u9645\u90e8\u7f72\u6784\u6210\u91cd\u5927\u969c\u788d\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u3002", "method": "\u672c\u7efc\u8ff0\u4ece\u8bed\u8a00\u5efa\u6a21\u51fa\u53d1\uff0c\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u7ebf\u6027\u4e0e\u7a00\u758f\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\u3001\u9ad8\u6548\u5168\u6ce8\u610f\u529b\u53d8\u4f53\u3001\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff08MoE\uff09\u3001\u7ed3\u5408\u4e0a\u8ff0\u6280\u672f\u7684\u6df7\u5408\u6a21\u578b\u67b6\u6784\u4ee5\u53ca\u65b0\u5174\u7684\u6269\u6563LLMs\u3002\u540c\u65f6\uff0c\u4e5f\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u6280\u672f\u5728\u5176\u4ed6\u6a21\u6001\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u672c\u7efc\u8ff0\u5c06\u8fd1\u671f\u7814\u7a76\u5f52\u7c7b\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u73b0\u4ee3\u9ad8\u6548LLM\u67b6\u6784\u7684\u84dd\u56fe\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5bf9\u5f00\u53d1\u53ef\u6269\u5c55\u3001\u8d44\u6e90\u611f\u77e5\u7684\u57fa\u7840\u6a21\u578b\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u7efc\u8ff0\u65e8\u5728\u4e3a\u7406\u89e3\u548c\u8bbe\u8ba1\u9ad8\u6548LLM\u67b6\u6784\u63d0\u4f9b\u6307\u5bfc\uff0c\u5e76\u671f\u671b\u80fd\u6fc0\u52b1\u672a\u6765\u7814\u7a76\uff0c\u4ee5\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u591a\u529f\u80fd\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2508.09204", "pdf": "https://arxiv.org/pdf/2508.09204", "abs": "https://arxiv.org/abs/2508.09204", "authors": ["Jinhao Zhang", "Yunquan Zhang", "Boyang Zhang", "Zeyu Liu", "Daning Cheng"], "title": "MoQE: Improve Quantization Model performance via Mixture of Quantization Experts", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Quantization method plays a crucial role in improving model efficiency and\nreducing deployment costs, enabling the widespread application of deep learning\nmodels on resource-constrained devices. However, the quantization process\ninevitably introduces accuracy degradation. In this paper, we propose Mixture\nof Quantization Experts( abbr. MoQE), a quantization inference framework based\non the Mixture-of-Experts (MoE) architecture, aiming to jointly improve the\nperformance of quantization models. MoQE combines multiple quantization\nvariants of one full-precision model as specialized \"quantization experts\" and\ndynamically routes input data to the most suitable expert based on its\ncharacteristics. MoQE alleviates the performance degradation commonly seen in\nsingle quantization models through specialization quantization expert models.\nWe design lightweight, structure-aware router models tailored for both CV and\nNLP tasks. Experimental evaluations on ResNet, LLaMA, and Qwen model families\nacross benchmark datasets including ImageNet, WikiText, C4, and OpenWebText\ndemonstrate that MoQE achieves performance comparable to SOTA quantization\nmodel, without incurring significant increases in inference latency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMoQE\uff08Mixture of Quantization Experts\uff09\uff0c\u4e00\u79cd\u57fa\u4e8eMoE\u67b6\u6784\u7684\u91cf\u5316\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u91cf\u5316\u4e13\u5bb6\u5e76\u52a8\u6001\u8def\u7531\u8f93\u5165\u6570\u636e\uff0c\u6709\u6548\u63d0\u5347\u91cf\u5316\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e14\u4e0d\u663e\u8457\u589e\u52a0\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "\u91cf\u5316\u65b9\u6cd5\u5bf9\u4e8e\u63d0\u5347\u6a21\u578b\u6548\u7387\u548c\u964d\u4f4e\u90e8\u7f72\u6210\u672c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u5e94\u7528\u65f6\uff0c\u91cf\u5316\u8fc7\u7a0b\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u5f15\u5165\u7cbe\u5ea6\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51faMoQE\u6846\u67b6\uff0c\u5b83\u5c06\u4e00\u4e2a\u5168\u7cbe\u5ea6\u6a21\u578b\u7684\u591a\u4e2a\u91cf\u5316\u53d8\u4f53\u4f5c\u4e3a\u201c\u91cf\u5316\u4e13\u5bb6\u201d\uff0c\u5e76\u6839\u636e\u8f93\u5165\u6570\u636e\u7279\u6027\u52a8\u6001\u5730\u5c06\u6570\u636e\u8def\u7531\u5230\u6700\u5408\u9002\u7684\u4e13\u5bb6\u3002\u4e3aCV\u548cNLP\u4efb\u52a1\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7\u3001\u7ed3\u6784\u611f\u77e5\u578b\u8def\u7531\u5668\u6a21\u578b\u3002", "result": "\u5728ResNet\u3001LLaMA\u548cQwen\u6a21\u578b\u5bb6\u65cf\u4ee5\u53caImageNet\u3001WikiText\u3001C4\u3001OpenWebText\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMoQE\u6027\u80fd\u53ef\u4e0eSOTA\u91cf\u5316\u6a21\u578b\u5ab2\u7f8e\uff0c\u4e14\u672a\u663e\u8457\u589e\u52a0\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "MoQE\u901a\u8fc7\u5f15\u5165\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5355\u4e00\u91cf\u5316\u6a21\u578b\u4e2d\u5e38\u89c1\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4e3a\u91cf\u5316\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09428", "pdf": "https://arxiv.org/pdf/2508.09428", "abs": "https://arxiv.org/abs/2508.09428", "authors": ["Yuxiao Wang", "Yu Lei", "Wolin Liang", "Weiying Xue", "Zhenao Wei", "Nan Zhuang", "Qi Liu"], "title": "What-Meets-Where: Unified Learning of Action and Contact Localization in a New Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "People control their bodies to establish contact with the environment. To\ncomprehensively understand actions across diverse visual contexts, it is\nessential to simultaneously consider \\textbf{what} action is occurring and\n\\textbf{where} it is happening. Current methodologies, however, often\ninadequately capture this duality, typically failing to jointly model both\naction semantics and their spatial contextualization within scenes. To bridge\nthis gap, we introduce a novel vision task that simultaneously predicts\nhigh-level action semantics and fine-grained body-part contact regions. Our\nproposed framework, PaIR-Net, comprises three key components: the Contact Prior\nAware Module (CPAM) for identifying contact-relevant body parts, the\nPrior-Guided Concat Segmenter (PGCS) for pixel-wise contact segmentation, and\nthe Interaction Inference Module (IIM) responsible for integrating global\ninteraction relationships. To facilitate this task, we present PaIR (Part-aware\nInteraction Representation), a comprehensive dataset containing 13,979 images\nthat encompass 654 actions, 80 object categories, and 17 body parts.\nExperimental evaluation demonstrates that PaIR-Net significantly outperforms\nbaseline approaches, while ablation studies confirm the efficacy of each\narchitectural component. The code and dataset will be released upon\npublication.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPaIR-Net\u6846\u67b6\u53caPaIR\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u52a8\u4f5c\u7406\u89e3\u65b9\u6cd5\u672a\u80fd\u540c\u65f6\u5efa\u6a21\u52a8\u4f5c\u8bed\u4e49\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\u7684\u4e0d\u8db3\uff0c\u5b9e\u73b0\u4e86\u52a8\u4f5c\u8bed\u4e49\u548c\u8eab\u4f53\u90e8\u4f4d\u63a5\u89e6\u533a\u57df\u7684\u540c\u6b65\u9884\u6d4b\u3002", "motivation": "\u5f53\u524d\u52a8\u4f5c\u7406\u89e3\u65b9\u6cd5\u5728\u6355\u83b7\u52a8\u4f5c\u7684\u8bed\u4e49\uff08\u201c\u4ec0\u4e48\u201d\uff09\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff08\u201c\u5728\u54ea\u91cc\u201d\uff09\u7684\u53cc\u91cd\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u8054\u5408\u5efa\u6a21\u52a8\u4f5c\u8bed\u4e49\u53ca\u5176\u5728\u573a\u666f\u4e2d\u7684\u7a7a\u95f4\u60c5\u5883\uff0c\u5bfc\u81f4\u96be\u4ee5\u5168\u9762\u7406\u89e3\u591a\u6837\u89c6\u89c9\u80cc\u666f\u4e0b\u7684\u52a8\u4f5c\u3002", "method": "1. \u5f15\u5165\u65b0\u9896\u7684\u89c6\u89c9\u4efb\u52a1\uff0c\u540c\u6b65\u9884\u6d4b\u9ad8\u7ea7\u52a8\u4f5c\u8bed\u4e49\u548c\u7ec6\u7c92\u5ea6\u8eab\u4f53\u90e8\u4f4d\u63a5\u89e6\u533a\u57df\u30022. \u63d0\u51faPaIR-Net\u6846\u67b6\uff0c\u5305\u542b\uff1a\u63a5\u89e6\u5148\u9a8c\u611f\u77e5\u6a21\u5757\uff08CPAM\uff09\u7528\u4e8e\u8bc6\u522b\u63a5\u89e6\u76f8\u5173\u8eab\u4f53\u90e8\u4f4d\uff1b\u5148\u9a8c\u5f15\u5bfc\u8fde\u63a5\u5206\u5272\u5668\uff08PGCS\uff09\u7528\u4e8e\u50cf\u7d20\u7ea7\u63a5\u89e6\u5206\u5272\uff1b\u4ea4\u4e92\u63a8\u65ad\u6a21\u5757\uff08IIM\uff09\u7528\u4e8e\u6574\u5408\u5168\u5c40\u4ea4\u4e92\u5173\u7cfb\u30023. \u6784\u5efaPaIR (Part-aware Interaction Representation) \u6570\u636e\u96c6\uff0c\u5305\u542b13,979\u5f20\u56fe\u50cf\uff0c\u6db5\u76d6654\u79cd\u52a8\u4f5c\u300180\u79cd\u7269\u4f53\u7c7b\u522b\u548c17\u4e2a\u8eab\u4f53\u90e8\u4f4d\u3002", "result": "PaIR-Net\u5728\u5b9e\u9a8c\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u6bcf\u4e2a\u67b6\u6784\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002", "conclusion": "PaIR-Net\u53ca\u5176\u63d0\u51fa\u7684\u65b0\u4efb\u52a1\u548c\u6570\u636e\u96c6\uff0c\u6709\u6548\u5f25\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8054\u5408\u7406\u89e3\u52a8\u4f5c\u8bed\u4e49\u548c\u7a7a\u95f4\u4e0a\u4e0b\u6587\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u66f4\u5168\u9762\u3001\u7ec6\u81f4\u5730\u7406\u89e3\u4eba\u7c7b\u4e0e\u73af\u5883\u7684\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.09848", "pdf": "https://arxiv.org/pdf/2508.09848", "abs": "https://arxiv.org/abs/2508.09848", "authors": ["Mo Yu", "Tsz Ting Chung", "Chulun Zhou", "Tong Li", "Rui Lu", "Jiangnan Li", "Liyan Xu", "Haoshu Lu", "Ning Zhang", "Jing Li", "Jie Zhou"], "title": "PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts", "categories": ["cs.CL", "cs.AI"], "comment": "First 7 authors contributed equally. Project page:\n  https://gorov.github.io/prelude", "summary": "We introduce PRELUDE, a benchmark for evaluating long-context understanding\nthrough the task of determining whether a character's prequel story is\nconsistent with the canonical narrative of the original book. Our task poses a\nstronger demand for global comprehension and deep reasoning than existing\nbenchmarks -- as the prequels are not part of the original story, assessing\ntheir plausibility typically requires searching and integrating information\nthat is only indirectly related. Empirically, 88% of instances require evidence\nfrom multiple parts of the narrative. Experimental results highlight the\nchallenge of our task: in-context learning, RAG and in-domain training with\nstate-of-the-art LLMs, and commercial DeepResearch services, lag behind humans\nby >15%. A further human study reveals that models often produce correct\nanswers with flawed reasoning, leading to an over 30% gap in reasoning accuracy\ncompared to humans. These findings underscore the substantial room for\nimprovement in long-context understanding and reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PRELUDE\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u957f\u6587\u672c\u7406\u89e3\u80fd\u529b\uff0c\u901a\u8fc7\u5224\u65ad\u89d2\u8272\u524d\u4f20\u6545\u4e8b\u4e0e\u539f\u8457\u53d9\u4e8b\u7684\u4e00\u81f4\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u5148\u8fdb\u7684LLM\u548c\u5546\u4e1a\u670d\u52a1\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u5747\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\uff0c\u63ed\u793a\u4e86\u957f\u6587\u672c\u7406\u89e3\u548c\u63a8\u7406\u9886\u57df\u5de8\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5bf9\u957f\u6587\u672c\u7684\u5168\u5c40\u7406\u89e3\u548c\u6df1\u5ea6\u63a8\u7406\u8981\u6c42\u4e0d\u8db3\u3002\u672c\u6587\u4efb\u52a1\u4e2d\uff0c\u524d\u4f20\u6545\u4e8b\u5e76\u975e\u539f\u8457\u4e00\u90e8\u5206\uff0c\u8bc4\u4f30\u5176\u5408\u7406\u6027\u901a\u5e38\u9700\u8981\u641c\u7d22\u548c\u6574\u5408\u95f4\u63a5\u76f8\u5173\u4fe1\u606f\uff0c\u4e1488%\u7684\u5b9e\u4f8b\u9700\u8981\u4ece\u53d9\u4e8b\u591a\u5904\u83b7\u53d6\u8bc1\u636e\uff0c\u8fd9\u6bd4\u73b0\u6709\u57fa\u51c6\u5bf9\u5168\u5c40\u7406\u89e3\u548c\u6df1\u5ea6\u63a8\u7406\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\u3002", "method": "\u5f15\u5165PRELUDE\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u901a\u8fc7\u5224\u65ad\u89d2\u8272\u524d\u4f20\u6545\u4e8b\u4e0e\u539f\u8457\u53d9\u4e8b\u7684\u7b26\u5408\u6027\u6765\u8bc4\u4f30\u957f\u6587\u672c\u7406\u89e3\u3002\u901a\u8fc7\u6bd4\u8f83\u6700\u5148\u8fdb\u7684LLM\uff08\u7ed3\u5408\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001RAG\u548c\u9886\u57df\u5185\u8bad\u7ec3\uff09\u4ee5\u53ca\u5546\u4e1aDeepResearch\u670d\u52a1\u4e0e\u4eba\u7c7b\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u6765\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6700\u5148\u8fdb\u7684LLM\u548c\u5546\u4e1aDeepResearch\u670d\u52a1\u5728PRELUDE\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u843d\u540e\u4e8e\u4eba\u7c7b\u8d85\u8fc715%\u3002\u8fdb\u4e00\u6b65\u7684\u4eba\u7c7b\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u5e38\u4ee5\u9519\u8bef\u7684\u63a8\u7406\u5f97\u51fa\u6b63\u786e\u7b54\u6848\uff0c\u5bfc\u81f4\u63a8\u7406\u51c6\u786e\u7387\u4e0e\u4eba\u7c7b\u76f8\u6bd4\u5b58\u572830%\u4ee5\u4e0a\u7684\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5f53\u524d\u6a21\u578b\u5728\u957f\u6587\u672c\u7406\u89e3\u548c\u6df1\u5ea6\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u7684\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2508.09206", "pdf": "https://arxiv.org/pdf/2508.09206", "abs": "https://arxiv.org/abs/2508.09206", "authors": ["Ning-Yuan Lue"], "title": "The First Differentiable Transfer-Based Algorithm for Discrete MicroLED Repair", "categories": ["cs.LG", "physics.comp-ph"], "comment": "15 pages, 7 figures. Presents a differentiable optimization method\n  for laser-enabled MicroLED repair planning, modeling discrete stage shifts in\n  a manufacturing physics context. Includes loss landscape and gradient\n  analyses, with large-array simulation results", "summary": "Laser-enabled selective transfer, a key process in high-throughput microLED\nfabrication, requires computational models that can plan shift sequences to\nminimize motion of XY stages and adapt to varying optimization objectives\nacross the substrate. We propose the first repair algorithm based on a\ndifferentiable transfer module designed to model discrete shifts of transfer\nplatforms, while remaining trainable via gradient-based optimization. Compared\nto local proximity searching algorithms, our approach achieves superior repair\nperformance and enables more flexible objective designs, such as minimizing the\nnumber of steps. Unlike reinforcement learning (RL)-based approaches, our\nmethod eliminates the need for handcrafted feature extractors and trains\nsignificantly faster, allowing scalability to large arrays. Experiments show a\n50% reduction in transfer steps and sub-2-minute planning time on 2000x2000\narrays. This method provides a practical and adaptable solution for\naccelerating microLED repair in AR/VR and next-generation display fabrication.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53ef\u5fae\u5206\u4f20\u8f93\u6a21\u5757\u7684\u6fc0\u5149\u9009\u62e9\u6027\u8f6c\u79fb\u4fee\u590d\u7b97\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u5faeLED\u5236\u9020\u7684\u8f6c\u79fb\u6b65\u9aa4\u5e76\u63d0\u9ad8\u89c4\u5212\u6548\u7387\u3002", "motivation": "\u5728\u5faeLED\u9ad8\u901a\u91cf\u5236\u9020\u4e2d\uff0c\u6fc0\u5149\u9009\u62e9\u6027\u8f6c\u79fb\u8fc7\u7a0b\u9700\u8981\u8ba1\u7b97\u6a21\u578b\u6765\u89c4\u5212\u8f6c\u79fb\u5e8f\u5217\u4ee5\u6700\u5c0f\u5316XY\u5e73\u53f0\u79fb\u52a8\uff0c\u5e76\u9002\u5e94\u4e0d\u540c\u4f18\u5316\u76ee\u6807\u3002\u73b0\u6709\u7b97\u6cd5\uff08\u5c40\u90e8\u641c\u7d22\u3001\u5f3a\u5316\u5b66\u4e60\uff09\u5b58\u5728\u6027\u80fd\u3001\u7075\u6d3b\u6027\u3001\u7279\u5f81\u5de5\u7a0b\u6216\u8bad\u7ec3\u901f\u5ea6\u6162\u7b49\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u4fee\u590d\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u53ef\u5fae\u5206\u4f20\u8f93\u6a21\u5757\u7684\u4fee\u590d\u7b97\u6cd5\uff0c\u8be5\u6a21\u5757\u80fd\u6a21\u62df\u4f20\u8f93\u5e73\u53f0\u7684\u79bb\u6563\u4f4d\u79fb\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u8fdb\u884c\u8bad\u7ec3\u3002\u4e0e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u624b\u5de5\u8bbe\u8ba1\u7279\u5f81\u63d0\u53d6\u5668\u3002", "result": "\u4e0e\u5c40\u90e8\u90bb\u8fd1\u641c\u7d22\u7b97\u6cd5\u76f8\u6bd4\uff0c\u672c\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u4fee\u590d\u6027\u80fd\uff0c\u5e76\u652f\u6301\u66f4\u7075\u6d3b\u7684\u76ee\u6807\u8bbe\u8ba1\uff08\u4f8b\u5982\u6700\u5c0f\u5316\u6b65\u9aa4\u6570\uff09\u3002\u8bad\u7ec3\u901f\u5ea6\u663e\u8457\u52a0\u5feb\uff0c\u53ef\u6269\u5c55\u81f3\u5927\u578b\u9635\u5217\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u57282000x2000\u9635\u5217\u4e0a\uff0c\u8f6c\u79fb\u6b65\u9aa4\u51cf\u5c11\u4e8650%\uff0c\u89c4\u5212\u65f6\u95f4\u57282\u5206\u949f\u4ee5\u5185\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aAR/VR\u548c\u4e0b\u4e00\u4ee3\u663e\u793a\u5668\u5236\u9020\u4e2d\u7684\u5faeLED\u4fee\u590d\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u52a0\u901f\u4e86\u5faeLED\u5236\u9020\u8fc7\u7a0b\u3002"}}
{"id": "2508.09446", "pdf": "https://arxiv.org/pdf/2508.09446", "abs": "https://arxiv.org/abs/2508.09446", "authors": ["Jiateng Liu", "Hengcan Shi", "Feng Chen", "Zhiwen Shao", "Yaonan Wang", "Jianfei Cai", "Wenming Zheng"], "title": "MPT: Motion Prompt Tuning for Micro-Expression Recognition", "categories": ["cs.CV", "I.2.8"], "comment": null, "summary": "Micro-expression recognition (MER) is crucial in the affective computing\nfield due to its wide application in medical diagnosis, lie detection, and\ncriminal investigation. Despite its significance, obtaining micro-expression\n(ME) annotations is challenging due to the expertise required from\npsychological professionals. Consequently, ME datasets often suffer from a\nscarcity of training samples, severely constraining the learning of MER models.\nWhile current large pre-training models (LMs) offer general and discriminative\nrepresentations, their direct application to MER is hindered by an inability to\ncapture transitory and subtle facial movements-essential elements for effective\nMER. This paper introduces Motion Prompt Tuning (MPT) as a novel approach to\nadapting LMs for MER, representing a pioneering method for subtle motion prompt\ntuning. Particularly, we introduce motion prompt generation, including motion\nmagnification and Gaussian tokenization, to extract subtle motions as prompts\nfor LMs. Additionally, a group adapter is carefully designed and inserted into\nthe LM to enhance it in the target MER domain, facilitating a more nuanced\ndistinction of ME representation. Furthermore, extensive experiments conducted\non three widely used MER datasets demonstrate that our proposed MPT\nconsistently surpasses state-of-the-art approaches and verifies its\neffectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u8fd0\u52a8\u63d0\u793a\u8c03\u6574\uff08MPT\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd0\u52a8\u63d0\u793a\u751f\u6210\u548c\u7ec4\u9002\u914d\u5668\u5c06\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08LMs\uff09\u5e94\u7528\u4e8e\u5fae\u8868\u60c5\u8bc6\u522b\uff08MER\uff09\uff0c\u6709\u6548\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u6027\u548c\u6355\u83b7\u7ec6\u5fae\u8fd0\u52a8\u7684\u6311\u6218\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002", "motivation": "\u5fae\u8868\u60c5\u8bc6\u522b\uff08MER\uff09\u5728\u533b\u7597\u8bca\u65ad\u3001\u6d4b\u8c0e\u548c\u72af\u7f6a\u8c03\u67e5\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5fae\u8868\u60c5\uff08ME\uff09\u6807\u6ce8\u9700\u8981\u4e13\u4e1a\u5fc3\u7406\u5b66\u77e5\u8bc6\uff0c\u5bfc\u81f4\u6570\u636e\u96c6\u6837\u672c\u7a00\u7f3a\uff0c\u4e25\u91cd\u9650\u5236\u4e86MER\u6a21\u578b\u7684\u5b66\u4e60\u3002\u73b0\u6709\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08LMs\uff09\u867d\u7136\u63d0\u4f9b\u901a\u7528\u8868\u793a\uff0c\u4f46\u65e0\u6cd5\u6355\u83b7MER\u6240\u9700\u7684\u77ed\u6682\u7ec6\u5fae\u9762\u90e8\u8fd0\u52a8\u3002", "method": "\u672c\u6587\u5f15\u5165\u8fd0\u52a8\u63d0\u793a\u8c03\u6574\uff08MPT\uff09\u65b9\u6cd5\uff0c\u5c06\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08LMs\uff09\u5e94\u7528\u4e8e\u5fae\u8868\u60c5\u8bc6\u522b\u3002\u5177\u4f53\u5305\u62ec\uff1a1) \u8fd0\u52a8\u63d0\u793a\u751f\u6210\uff0c\u901a\u8fc7\u8fd0\u52a8\u653e\u5927\u548c\u9ad8\u65af\u5206\u8bcd\u63d0\u53d6\u7ec6\u5fae\u8fd0\u52a8\u4f5c\u4e3aLMs\u7684\u63d0\u793a\u30022) \u7cbe\u5fc3\u8bbe\u8ba1\u5e76\u5c06\u7ec4\u9002\u914d\u5668\u63d2\u5165\u5230LMs\u4e2d\uff0c\u4ee5\u589e\u5f3a\u5176\u5728MER\u76ee\u6807\u57df\u7684\u6027\u80fd\uff0c\u4fc3\u8fdb\u66f4\u7ec6\u81f4\u7684\u5fae\u8868\u60c5\u8868\u793a\u533a\u5206\u3002", "result": "\u5728\u4e09\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684MER\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684MPT\u65b9\u6cd5\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "MPT\u662f\u4e00\u79cd\u65b0\u9896\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd0\u52a8\u63d0\u793a\u8c03\u6574\u548c\u7ec4\u9002\u914d\u5668\uff0c\u6210\u529f\u5730\u5c06\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u9002\u5e94\u4e8e\u5fae\u8868\u60c5\u8bc6\u522b\u4efb\u52a1\uff0c\u514b\u670d\u4e86\u6570\u636e\u7a00\u7f3a\u6027\u548c\u6355\u6349\u7ec6\u5fae\u52a8\u4f5c\u7684\u6311\u6218\uff0c\u5e76\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.09865", "pdf": "https://arxiv.org/pdf/2508.09865", "abs": "https://arxiv.org/abs/2508.09865", "authors": ["Abdul Rehman Antall", "Naveed Akhtar"], "title": "Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription", "categories": ["cs.CL"], "comment": "8 pages, 3 figures, 1 table, including references and appendix", "summary": "This study evaluates the feasibility of lightweight Whisper models (Tiny,\nBase, Small) for Urdu speech recognition in low-resource settings. Despite Urdu\nbeing the 10th most spoken language globally with over 230 million speakers,\nits representation in automatic speech recognition (ASR) systems remains\nlimited due to dialectal diversity, code-switching, and sparse training data.\nWe benchmark these models on a curated Urdu dataset using word error rate\n(WER), without fine-tuning. Results show Whisper-Small achieves the lowest\nerror rates (33.68\\% WER), outperforming Tiny (67.08\\% WER) and Base (53.67\\%\nWER). Qualitative analysis reveals persistent challenges in phonetic accuracy\nand lexical coherence, particularly for complex utterances. While Whisper-Small\ndemonstrates promise for deployable Urdu ASR, significant gaps remain. Our\nfindings emphasize lay the groundwork for future research into effective,\nlow-resource ASR systems.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u8f7b\u91cf\u7ea7Whisper\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u4e4c\u5c14\u90fd\u8bed\u8bed\u97f3\u8bc6\u522b\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0Whisper-Small\u6a21\u578b\u6027\u80fd\u6700\u4f73\uff0c\u4f46\u4ecd\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u4e4c\u5c14\u90fd\u8bed\u4f5c\u4e3a\u5168\u7403\u4e3b\u8981\u8bed\u8a00\uff0c\u5176\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u56e0\u65b9\u8a00\u591a\u6837\u6027\u3001\u8bed\u7801\u8f6c\u6362\u548c\u8bad\u7ec3\u6570\u636e\u7a00\u758f\u800c\u53d7\u9650\uff0c\u4e9f\u9700\u63a2\u7d22\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u5728\u672a\u8fdb\u884c\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u5bf9Whisper Tiny\u3001Base\u548cSmall\u6a21\u578b\u5728\u4e00\u4e2a\u7cbe\u9009\u7684\u4e4c\u5c14\u90fd\u8bed\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9a\u6027\u5206\u6790\u3002", "result": "Whisper-Small\u6a21\u578b\u8868\u73b0\u51fa\u6700\u4f4e\u7684\u8bcd\u9519\u8bef\u7387\uff0833.68%\uff09\uff0c\u4f18\u4e8eTiny\uff0867.08%\uff09\u548cBase\uff0853.67%\uff09\u3002\u5b9a\u6027\u5206\u6790\u663e\u793a\uff0c\u5728\u8bed\u97f3\u51c6\u786e\u6027\u548c\u8bcd\u6c47\u8fde\u8d2f\u6027\u65b9\u9762\u4ecd\u5b58\u5728\u6301\u7eed\u6311\u6218\uff0c\u5c24\u5176\u5bf9\u4e8e\u590d\u6742\u7684\u8bdd\u8bed\u3002", "conclusion": "Whisper-Small\u6a21\u578b\u4e3a\u53ef\u90e8\u7f72\u7684\u4e4c\u5c14\u90fd\u8bedASR\u5c55\u793a\u4e86\u524d\u666f\uff0c\u4f46\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u672c\u7814\u7a76\u4e3a\u672a\u6765\u9ad8\u6548\u3001\u4f4e\u8d44\u6e90ASR\u7cfb\u7edf\u7684\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.09223", "pdf": "https://arxiv.org/pdf/2508.09223", "abs": "https://arxiv.org/abs/2508.09223", "authors": ["Sameer Ambekar", "Daniel M. Lang", "Julia A. Schnabel"], "title": "Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Test-time adaptation allows pretrained models to adjust to incoming data\nstreams, addressing distribution shifts between source and target domains.\nHowever, standard methods rely on single-dimensional linear classification\nlayers, which often fail to handle diverse and complex shifts. We propose\nHierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages\nmultiple layers of increasing size for dynamic test-time adaptation. By\ndecomposing the encoder's representation space into such hierarchically\norganized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to\nadapt to shifts of varying complexity. Our contributions are threefold: First,\nwe propose dynamic layer selection for automatic identification of the optimal\nlayer for adaptation to each test batch. Second, we propose a mechanism that\nmerges weights from the dynamic layer to other layers, ensuring all layers\nreceive target information. Third, we propose linear layer agreement that acts\nas a gating function, preventing erroneous fine-tuning by adaptation on noisy\nbatches. We rigorously evaluate the performance of Hi-Vec in challenging\nscenarios and on multiple target datasets, proving its strong capability to\nadvance state-of-the-art methods. Our results show that Hi-Vec improves\nrobustness, addresses uncertainty, and handles limited batch sizes and\nincreased outlier rates.", "AI": {"tldr": "\u63d0\u51faHi-Vec\uff0c\u901a\u8fc7\u591a\u5c42\u52a8\u6001\u9002\u5e94\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u6d4b\u8bd5\u65f6\u9002\u5e94\u4e2d\u5355\u5c42\u6a21\u578b\u96be\u4ee5\u5904\u7406\u590d\u6742\u6570\u636e\u504f\u79fb\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08Test-time adaptation, TTA\uff09\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5355\u7ef4\u7ebf\u6027\u5206\u7c7b\u5c42\uff0c\u96be\u4ee5\u6709\u6548\u5904\u7406\u591a\u6837\u5316\u548c\u590d\u6742\u7684\u9886\u57df\u504f\u79fb\uff08distribution shifts\uff09\u3002", "method": "\u63d0\u51faHi-Vec\uff08Hierarchical Adaptive Networks with Task Vectors\uff09\uff0c\u901a\u8fc7\u5c06\u7f16\u7801\u5668\u7684\u8868\u793a\u7a7a\u95f4\u5206\u89e3\u4e3a\u591a\u4e2a\u5c42\u7ea7\u7ec4\u7ec7\u4e14\u5c3a\u5bf8\u9012\u589e\u7684\u5c42\uff0c\u5b9e\u73b0\u52a8\u6001\u6d4b\u8bd5\u65f6\u9002\u5e94\u3002\u5176\u6838\u5fc3\u8d21\u732e\u5305\u62ec\uff1a1. \u52a8\u6001\u5c42\u9009\u62e9\uff0c\u81ea\u52a8\u8bc6\u522b\u6bcf\u4e2a\u6d4b\u8bd5\u6279\u6b21\u7684\u6700\u4f18\u9002\u5e94\u5c42\u30022. \u6743\u91cd\u5408\u5e76\u673a\u5236\uff0c\u5c06\u52a8\u6001\u5c42\u7684\u6743\u91cd\u5408\u5e76\u5230\u5176\u4ed6\u5c42\uff0c\u786e\u4fdd\u6240\u6709\u5c42\u63a5\u6536\u76ee\u6807\u4fe1\u606f\u30023. \u7ebf\u6027\u5c42\u4e00\u81f4\u6027\uff0c\u4f5c\u4e3a\u95e8\u63a7\u51fd\u6570\u9632\u6b62\u5728\u566a\u58f0\u6279\u6b21\u4e0a\u8fdb\u884c\u9519\u8bef\u5fae\u8c03\u3002", "result": "Hi-Vec\u5728\u6311\u6218\u6027\u573a\u666f\u548c\u591a\u4e2a\u76ee\u6807\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u4e25\u683c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u5b83\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u80fd\u6709\u6548\u5904\u7406\u6709\u9650\u6279\u6b21\u5927\u5c0f\u548c\u589e\u52a0\u7684\u5f02\u5e38\u503c\u7387\u3002", "conclusion": "Hi-Vec\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u65f6\u9002\u5e94\u7684\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u5730\u5e94\u5bf9\u590d\u6742\u548c\u591a\u6837\u7684\u9886\u57df\u504f\u79fb\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.09449", "pdf": "https://arxiv.org/pdf/2508.09449", "abs": "https://arxiv.org/abs/2508.09449", "authors": ["Jiaqi Yan", "Shuning Xu", "Xiangyu Chen", "Dell Zhang", "Jie Tang", "Gangshan Wu", "Jie Liu"], "title": "RASR: Retrieval-Augmented Super Resolution for Practical Reference-based Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Reference-based Super Resolution (RefSR) improves upon Single Image Super\nResolution (SISR) by leveraging high-quality reference images to enhance\ntexture fidelity and visual realism. However, a critical limitation of existing\nRefSR approaches is their reliance on manually curated target-reference image\npairs, which severely constrains their practicality in real-world scenarios. To\novercome this, we introduce Retrieval-Augmented Super Resolution (RASR), a new\nand practical RefSR paradigm that automatically retrieves semantically relevant\nhigh-resolution images from a reference database given only a low-quality\ninput. This enables scalable and flexible RefSR in realistic use cases, such as\nenhancing mobile photos taken in environments like zoos or museums, where\ncategory-specific reference data (e.g., animals, artworks) can be readily\ncollected or pre-curated. To facilitate research in this direction, we\nconstruct RASR-Flickr30, the first benchmark dataset designed for RASR. Unlike\nprior datasets with fixed target-reference pairs, RASR-Flickr30 provides\nper-category reference databases to support open-world retrieval. We further\npropose RASRNet, a strong baseline that combines a semantic reference retriever\nwith a diffusion-based RefSR generator. It retrieves relevant references based\non semantic similarity and employs a diffusion-based generator enhanced with\nsemantic conditioning. Experiments on RASR-Flickr30 demonstrate that RASRNet\nconsistently improves over SISR baselines, achieving +0.38 dB PSNR and -0.0131\nLPIPS, while generating more realistic textures. These findings highlight\nretrieval augmentation as a promising direction to bridge the gap between\nacademic RefSR research and real-world applicability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u68c0\u7d22\u589e\u5f3a\u8d85\u5206\u8fa8\u7387\uff08RASR\uff09\uff0c\u901a\u8fc7\u81ea\u52a8\u68c0\u7d22\u9ad8\u5206\u8fa8\u7387\u53c2\u8003\u56fe\u50cf\u6765\u514b\u670d\u73b0\u6709RefSR\u65b9\u6cd5\u5bf9\u624b\u52a8\u914d\u5bf9\u7684\u4f9d\u8d56\u3002\u4e3a\u4fc3\u8fdb\u7814\u7a76\uff0c\u6784\u5efa\u4e86\u9996\u4e2aRASR\u57fa\u51c6\u6570\u636e\u96c6RASR-Flickr30\uff0c\u5e76\u63d0\u51fa\u4e86RASRNet\u57fa\u7ebf\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u6548\u679c\u548c\u771f\u5b9e\u611f\u3002", "motivation": "\u73b0\u6709\u7684\u53c2\u8003\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08RefSR\uff09\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u624b\u52a8\u7b56\u5212\u7684\u76ee\u6807-\u53c2\u8003\u56fe\u50cf\u5bf9\uff0c\u8fd9\u6781\u5927\u5730\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5f00\u53d1\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u7075\u6d3b\u4e14\u80fd\u81ea\u52a8\u68c0\u7d22\u53c2\u8003\u56fe\u50cf\u7684RefSR\u8303\u5f0f\u3002", "method": "\u5f15\u5165\u68c0\u7d22\u589e\u5f3a\u8d85\u5206\u8fa8\u7387\uff08RASR\uff09\u8303\u5f0f\uff0c\u81ea\u52a8\u4ece\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u8bed\u4e49\u76f8\u5173\u7684\u53c2\u8003\u56fe\u50cf\u3002\u6784\u5efa\u4e86\u9996\u4e2aRASR\u57fa\u51c6\u6570\u636e\u96c6RASR-Flickr30\uff0c\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u6309\u7c7b\u522b\u5212\u5206\u7684\u53c2\u8003\u6570\u636e\u5e93\u3002\u63d0\u51fa\u4e86RASRNet\uff0c\u4e00\u4e2a\u7ed3\u5408\u4e86\u8bed\u4e49\u53c2\u8003\u68c0\u7d22\u5668\u548c\u57fa\u4e8e\u6269\u6563\u7684RefSR\u751f\u6210\u5668\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5176\u4e2d\u751f\u6210\u5668\u901a\u8fc7\u8bed\u4e49\u6761\u4ef6\u8fdb\u884c\u589e\u5f3a\u3002", "result": "\u5728RASR-Flickr30\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRASRNet\u6bd4\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SISR\uff09\u57fa\u7ebf\u6a21\u578b\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5b9e\u73b0\u4e86+0.38 dB PSNR\u548c-0.0131 LPIPS\uff0c\u5e76\u751f\u6210\u4e86\u66f4\u771f\u5b9e\u7684\u7eb9\u7406\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u68c0\u7d22\u589e\u5f3a\u662f\u5f25\u5408\u5b66\u672fRefSR\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u5dee\u8ddd\u7684\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2508.09874", "pdf": "https://arxiv.org/pdf/2508.09874", "abs": "https://arxiv.org/abs/2508.09874", "authors": ["Jiaqi Cao", "Jiarui Wang", "Rubin Wei", "Qipeng Guo", "Kai Chen", "Bowen Zhou", "Zhouhan Lin"], "title": "Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown strong abilities in general language\ntasks, yet adapting them to specific domains remains a challenge. Current\nmethod like Domain Adaptive Pretraining (DAPT) requires costly full-parameter\ntraining and suffers from catastrophic forgetting. Meanwhile,\nRetrieval-Augmented Generation (RAG) introduces substantial inference latency\ndue to expensive nearest-neighbor searches and longer context. This paper\nintroduces Memory Decoder, a plug-and-play pretrained memory that enables\nefficient domain adaptation without changing the original model's parameters.\nMemory Decoder employs a small transformer decoder that learns to imitate the\nbehavior of an external non-parametric retriever. Once trained, Memory Decoder\ncan be seamlessly integrated with any pretrained language model that shares the\nsame tokenizer, requiring no model-specific modifications. Experimental results\ndemonstrate that Memory Decoder enables effective adaptation of various Qwen\nand Llama models to three distinct specialized domains: biomedicine, finance,\nand law, reducing perplexity by an average of 6.17 points. Overall, Memory\nDecoder introduces a novel paradigm centered on a specially pretrained memory\ncomponent designed for domain-specific adaptation. This memory architecture can\nbe integrated in a plug-and-play manner, consistently enhancing performance\nacross multiple models within the target domain.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMemory Decoder\uff0c\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u9884\u8bad\u7ec3\u8bb0\u5fc6\u7ec4\u4ef6\uff0c\u65e0\u9700\u4fee\u6539\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u9886\u57df\u9002\u5e94\uff0c\u540c\u65f6\u907f\u514d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5f0a\u7aef\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u901a\u7528\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7279\u5b9a\u9886\u57df\u9002\u5e94\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff08DAPT\uff09\u6210\u672c\u9ad8\u6602\u4e14\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff1b\u800c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7531\u4e8e\u68c0\u7d22\u5f00\u9500\u5927\uff0c\u5bfc\u81f4\u63a8\u7406\u5ef6\u8fdf\u663e\u8457\u3002", "method": "\u5f15\u5165Memory Decoder\uff0c\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u9884\u8bad\u7ec3\u8bb0\u5fc6\u7ec4\u4ef6\u3002\u5b83\u662f\u4e00\u4e2a\u5c0f\u578btransformer\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u6a21\u4eff\u5916\u90e8\u975e\u53c2\u6570\u68c0\u7d22\u5668\u7684\u884c\u4e3a\u3002\u4e00\u65e6\u8bad\u7ec3\u5b8c\u6210\uff0cMemory Decoder\u53ef\u4ee5\u4e0e\u4efb\u4f55\u4f7f\u7528\u76f8\u540c\u5206\u8bcd\u5668\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\uff0c\u65e0\u9700\u6539\u53d8\u539f\u59cb\u6a21\u578b\u53c2\u6570\u6216\u8fdb\u884c\u6a21\u578b\u7279\u5f02\u6027\u4fee\u6539\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMemory Decoder\u80fd\u6709\u6548\u5c06Qwen\u548cLlama\u7cfb\u5217\u6a21\u578b\u9002\u5e94\u5230\u751f\u7269\u533b\u5b66\u3001\u91d1\u878d\u548c\u6cd5\u5f8b\u4e09\u4e2a\u4e13\u4e1a\u9886\u57df\uff0c\u5e73\u5747\u56f0\u60d1\u5ea6\u964d\u4f4e6.17\u70b9\u3002", "conclusion": "Memory Decoder\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u9884\u8bad\u7ec3\u8bb0\u5fc6\u7ec4\u4ef6\u4e3a\u4e2d\u5fc3\u7684\u65b0\u578b\u8303\u5f0f\uff0c\u4e13\u4e3a\u9886\u57df\u7279\u5b9a\u9002\u5e94\u8bbe\u8ba1\u3002\u8fd9\u79cd\u5373\u63d2\u5373\u7528\u7684\u8bb0\u5fc6\u67b6\u6784\u80fd\u591f\u6301\u7eed\u63d0\u5347\u591a\u6a21\u578b\u5728\u76ee\u6807\u9886\u57df\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.09227", "pdf": "https://arxiv.org/pdf/2508.09227", "abs": "https://arxiv.org/abs/2508.09227", "authors": ["Fan Ding", "Hwa Hui Tew", "Junn Yong Loo", "Susilawati", "LiTong Liu", "Fang Yu Leong", "Xuewen Luo", "Kar Keong Chin", "Jia Jun Gan"], "title": "GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": "This paper has been accepted by ITSC 2025", "summary": "Accurate trajectory prediction for buses is crucial in intelligent\ntransportation systems, particularly within urban environments. In developing\nregions where access to multimodal data is limited, relying solely on onboard\nGPS data remains indispensable despite inherent challenges. To address this\nproblem, we propose GSMT, a hybrid model that integrates a Graph Attention\nNetwork (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and\nincorporates a task corrector capable of extracting complex behavioral patterns\nfrom large-scale trajectory data. The task corrector clusters historical\ntrajectories to identify distinct motion patterns and fine-tunes the\npredictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus\ninformation and static station information through embedded hybrid networks to\nperform trajectory prediction, and applies the task corrector for secondary\nrefinement after the initial predictions are generated. This two-stage approach\nenables multi-node trajectory prediction among buses operating in dense urban\ntraffic environments under complex conditions. Experiments conducted on a\nreal-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method\nsignificantly outperforms existing approaches, achieving superior performance\nin both short-term and long-term trajectory prediction tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGSMT\uff0c\u4e00\u4e2a\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u3001\u5e8f\u5217\u5230\u5e8f\u5217RNN\u548c\u4efb\u52a1\u6821\u6b63\u5668\u7684\u6df7\u5408\u6a21\u578b\uff0c\u65e8\u5728\u514b\u670d\u6570\u636e\u53d7\u9650\u73af\u5883\u4e0b\u57ce\u5e02\u516c\u4ea4\u8f66\u8f68\u8ff9\u9884\u6d4b\u7684\u6311\u6218\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u8868\u73b0\u5353\u8d8a\u3002", "motivation": "\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u591a\u6a21\u6001\u6570\u636e\u53d7\u9650\u7684\u53d1\u5c55\u4e2d\u5730\u533a\u57ce\u5e02\u73af\u5883\u4e2d\uff0c\u51c6\u786e\u7684\u516c\u4ea4\u8f66\u8f68\u8ff9\u9884\u6d4b\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5b58\u5728\u56fa\u6709\u7684\u6311\u6218\uff0c\u4ec5\u4f9d\u8d56\u8f66\u8f7dGPS\u6570\u636e\u4ecd\u7136\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\u3002", "method": "\u63d0\u51faGSMT\u6df7\u5408\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u6574\u5408\u4e86\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u548c\u5e8f\u5217\u5230\u5e8f\u5217\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\uff0c\u5e76\u52a0\u5165\u4e86\u4e00\u4e2a\u4efb\u52a1\u6821\u6b63\u5668\u3002\u4efb\u52a1\u6821\u6b63\u5668\u901a\u8fc7\u805a\u7c7b\u5386\u53f2\u8f68\u8ff9\u6765\u8bc6\u522b\u590d\u6742\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u5bf9GAT\u548cRNN\u751f\u6210\u7684\u9884\u6d4b\u8fdb\u884c\u5fae\u8c03\u3002GSMT\u878d\u5408\u52a8\u6001\u516c\u4ea4\u4fe1\u606f\u548c\u9759\u6001\u7ad9\u70b9\u4fe1\u606f\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\u8fdb\u884c\u591a\u8282\u70b9\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u5728\u9a6c\u6765\u897f\u4e9a\u5409\u9686\u5761\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGSMT\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u77ed\u671f\u548c\u957f\u671f\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "GSMT\u6df7\u5408\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u884c\u4e3a\u6a21\u5f0f\u6821\u6b63\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5728\u6570\u636e\u53d7\u9650\u7684\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u516c\u4ea4\u8f66\u8f68\u8ff9\u9884\u6d4b\u7684\u7cbe\u5ea6\u95ee\u9898\uff0c\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2508.09453", "pdf": "https://arxiv.org/pdf/2508.09453", "abs": "https://arxiv.org/abs/2508.09453", "authors": ["Abdul Matin", "Tanjim Bin Faruk", "Shrideep Pallickara", "Sangmi Lee Pallickara"], "title": "HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The proliferation of foundation models, pretrained on large-scale unlabeled\ndatasets, has emerged as an effective approach in creating adaptable and\nreusable architectures that can be leveraged for various downstream tasks using\nsatellite observations. However, their direct application to hyperspectral\nremote sensing remains challenging due to inherent spectral disparities and the\nscarcity of available observations. In this work, we present HyperKD, a novel\nknowledge distillation framework that enables transferring learned\nrepresentations from a teacher model into a student model for effective\ndevelopment of a foundation model on hyperspectral images. Unlike typical\nknowledge distillation frameworks, which use a complex teacher to guide a\nsimpler student, HyperKD enables an inverse form of knowledge transfer across\ndifferent types of spectral data, guided by a simpler teacher model. Building\nupon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi\nfoundational model into a student tailored for EnMAP hyperspectral imagery.\nHyperKD addresses the inverse domain adaptation problem with spectral gaps by\nintroducing a feature-based strategy that includes spectral range-based channel\nalignment, spatial feature-guided masking, and an enhanced loss function\ntailored for hyperspectral images. HyperKD bridges the substantial spectral\ndomain gap, enabling the effective use of pretrained foundation models for\ngeospatial applications. Extensive experiments show that HyperKD significantly\nimproves representation learning in MAEs, leading to enhanced reconstruction\nfidelity and more robust performance on downstream tasks such as land cover\nclassification, crop type identification, and soil organic carbon prediction,\nunderpinning the potential of knowledge distillation frameworks in remote\nsensing analytics with hyperspectral imagery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHyperKD\uff0c\u4e00\u79cd\u7528\u4e8e\u8d85\u5149\u8c31\u9065\u611f\u7684\u9006\u5411\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u3002\u5b83\u80fd\u5c06\u9884\u8bad\u7ec3\u57fa\u91d1\u6a21\u578b\u77e5\u8bc6\u6709\u6548\u8fc1\u79fb\u81f3\u8d85\u5149\u8c31\u56fe\u50cf\u6a21\u578b\uff0c\u5f25\u5408\u5149\u8c31\u5dee\u5f02\uff0c\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u57fa\u91d1\u6a21\u578b\u5728\u536b\u661f\u89c2\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u56fa\u6709\u7684\u5149\u8c31\u5dee\u5f02\u548c\u6570\u636e\u7a00\u7f3a\u6027\uff0c\u76f4\u63a5\u5e94\u7528\u4e8e\u8d85\u5149\u8c31\u9065\u611f\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51faHyperKD\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u3002\u5b83\u901a\u8fc7\u9006\u5411\u77e5\u8bc6\u8fc1\u79fb\uff0c\u5c06\u666e\u9002\u57fa\u91d1\u6a21\u578b\uff08\u5982Prithvi\uff09\u7684\u8868\u793a\u5b66\u4e60\u80fd\u529b\uff0c\u84b8\u998f\u5230\u4e00\u4e2a\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08MAE\uff09\u7684\u3001\u4e13\u95e8\u4e3aEnMAP\u8d85\u5149\u8c31\u56fe\u50cf\u8bbe\u8ba1\u7684\u5b66\u751f\u6a21\u578b\u4e2d\u3002HyperKD\u91c7\u7528\u5149\u8c31\u8303\u56f4\u901a\u9053\u5bf9\u9f50\u3001\u7a7a\u95f4\u7279\u5f81\u5f15\u5bfc\u63a9\u7801\u548c\u4e3a\u8d85\u5149\u8c31\u56fe\u50cf\u5b9a\u5236\u7684\u589e\u5f3a\u635f\u5931\u51fd\u6570\u7b49\u7279\u5f81\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3\u5149\u8c31\u57df\u95f4\u7684\u5de8\u5927\u5dee\u5f02\u95ee\u9898\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHyperKD\u663e\u8457\u63d0\u5347\u4e86MAE\u4e2d\u7684\u8868\u793a\u5b66\u4e60\u80fd\u529b\uff0c\u5e26\u6765\u4e86\u66f4\u9ad8\u7684\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728\u571f\u5730\u8986\u76d6\u5206\u7c7b\u3001\u4f5c\u7269\u7c7b\u578b\u8bc6\u522b\u548c\u571f\u58e4\u6709\u673a\u78b3\u9884\u6d4b\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u66f4\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u80fd\u3002", "conclusion": "HyperKD\u6210\u529f\u5f25\u5408\u4e86\u8d85\u5149\u8c31\u9886\u57df\u7684\u5de8\u5927\u5149\u8c31\u57df\u95f4\u9699\uff0c\u4f7f\u5f97\u9884\u8bad\u7ec3\u57fa\u91d1\u6a21\u578b\u80fd\u6709\u6548\u5e94\u7528\u4e8e\u5730\u7406\u7a7a\u95f4\u5e94\u7528\uff0c\u51f8\u663e\u4e86\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u5728\u8d85\u5149\u8c31\u9065\u611f\u5206\u6790\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.09878", "pdf": "https://arxiv.org/pdf/2508.09878", "abs": "https://arxiv.org/abs/2508.09878", "authors": ["Archie Sage", "Jeroen Keppens", "Helen Yannakoudakis"], "title": "A Survey of Cognitive Distortion Detection and Classification in NLP", "categories": ["cs.CL"], "comment": "Under review via ACL Rolling Review and committed to EMNLP 2025.\n  Camera-ready updates to follow", "summary": "As interest grows in the application of natural language processing (NLP)\ntechniques to mental health, a growing body of work explores the automatic\ndetection and classification of cognitive distortions (CDs). CDs are habitual\npatterns of negatively biased or flawed thinking that distort how people\nperceive events, judge themselves, and react to the world around them.\nIdentifying and addressing them is an important part of therapy. Despite its\nmomentum, the field remains fragmented, with inconsistencies in CD taxonomies,\ntask formulations, and evaluation practices. This survey reviews 38 studies\nspanning two decades, providing a structured overview of datasets, modelling\napproaches, and evaluation strategies. We provide a consolidated CD taxonomy\nreference, summarise common task setups, and highlight open challenges to\nsupport more coherent and reproducible research in this emerging area.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86NLP\u5728\u7cbe\u795e\u5065\u5eb7\u9886\u57df\u8ba4\u77e5\u626d\u66f2\u68c0\u6d4b\u4e0e\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u6307\u51fa\u8be5\u9886\u57df\u5b58\u5728\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u5206\u7c7b\u6cd5\u3001\u4efb\u52a1\u8bbe\u7f6e\u603b\u7ed3\u548c\u5f00\u653e\u6027\u6311\u6218\uff0c\u4ee5\u4fc3\u8fdb\u7814\u7a76\u7684\u8fde\u8d2f\u6027\u4e0e\u53ef\u590d\u73b0\u6027\u3002", "motivation": "\u968f\u7740\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u6280\u672f\u5728\u7cbe\u795e\u5065\u5eb7\u9886\u57df\u5e94\u7528\u5174\u8da3\u7684\u589e\u957f\uff0c\u81ea\u52a8\u68c0\u6d4b\u548c\u5206\u7c7b\u8ba4\u77e5\u626d\u66f2\uff08CDs\uff09\u7684\u7814\u7a76\u65e5\u76ca\u589e\u591a\u3002\u7136\u800c\uff0c\u8be5\u9886\u57df\u5728CDs\u5206\u7c7b\u6cd5\u3001\u4efb\u52a1\u5236\u5b9a\u548c\u8bc4\u4f30\u5b9e\u8df5\u4e0a\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u7814\u7a76\u788e\u7247\u5316\u3002", "method": "\u672c\u7814\u7a76\u7efc\u8ff0\u4e86\u8de8\u8d8a\u4e8c\u5341\u5e74\u768438\u9879\u76f8\u5173\u7814\u7a76\uff0c\u63d0\u4f9b\u4e86\u5173\u4e8e\u6570\u636e\u96c6\u3001\u5efa\u6a21\u65b9\u6cd5\u548c\u8bc4\u4f30\u7b56\u7565\u7684\u7ed3\u6784\u5316\u6982\u8ff0\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u5305\u62ec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6574\u5408\u7684\u8ba4\u77e5\u626d\u66f2\u5206\u7c7b\u53c2\u8003\u3001\u603b\u7ed3\u4e86\u5e38\u89c1\u7684\u4efb\u52a1\u8bbe\u7f6e\uff0c\u5e76\u6307\u51fa\u4e86\u8be5\u9886\u57df\u9762\u4e34\u7684\u5f00\u653e\u6027\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u7edf\u4e00\u7684\u53c2\u8003\u548c\u7ed3\u6784\u5316\u5206\u6790\uff0c\u65e8\u5728\u652f\u6301\u8be5\u65b0\u5174\u9886\u57df\u5f00\u5c55\u66f4\u5177\u8fde\u8d2f\u6027\u548c\u53ef\u590d\u73b0\u6027\u7684\u7814\u7a76\u3002"}}
{"id": "2508.09237", "pdf": "https://arxiv.org/pdf/2508.09237", "abs": "https://arxiv.org/abs/2508.09237", "authors": ["Luigi D'Amico", "Daniel De Rosso", "Ninad Dixit", "Raul Salles de Padua", "Samuel Palmer", "Samuel Mugel", "Rom\u00e1n Or\u00fas", "Holger Eble", "Ali Abedi"], "title": "Blockchain Network Analysis using Quantum Inspired Graph Neural Networks & Ensemble Models", "categories": ["cs.LG", "quant-ph"], "comment": null, "summary": "In the rapidly evolving domain of financial technology, the detection of\nillicit transactions within blockchain networks remains a critical challenge,\nnecessitating robust and innovative solutions. This work proposes a novel\napproach by combining Quantum Inspired Graph Neural Networks (QI-GNN) with\nflexibility of choice of an Ensemble Model using QBoost or a classic model such\nas Random Forrest Classifier. This system is tailored specifically for\nblockchain network analysis in anti-money laundering (AML) efforts. Our\nmethodology to design this system incorporates a novel component, a Canonical\nPolyadic (CP) decomposition layer within the graph neural network framework,\nenhancing its capability to process and analyze complex data structures\nefficiently. Our technical approach has undergone rigorous evaluation against\nclassical machine learning implementations, achieving an F2 score of 74.8% in\ndetecting fraudulent transactions. These results highlight the potential of\nquantum-inspired techniques, supplemented by the structural advancements of the\nCP layer, to not only match but potentially exceed traditional methods in\ncomplex network analysis for financial security. The findings advocate for a\nbroader adoption and further exploration of quantum-inspired algorithms within\nthe financial sector to effectively combat fraud.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u91cf\u5b50\u542f\u53d1\u56fe\u795e\u7ecf\u7f51\u7edc\uff08QI-GNN\uff09\u548c\u96c6\u6210\u6a21\u578b\uff08\u542bCP\u5206\u89e3\u5c42\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u533a\u5757\u94fe\u53cd\u6d17\u94b1\u4e2d\u7684\u975e\u6cd5\u4ea4\u6613\u68c0\u6d4b\uff0c\u5e76\u5b9e\u73b0\u4e8674.8%\u7684F2\u5206\u6570\u3002", "motivation": "\u5728\u5feb\u901f\u53d1\u5c55\u7684\u91d1\u878d\u79d1\u6280\u9886\u57df\uff0c\u533a\u5757\u94fe\u7f51\u7edc\u4e2d\u975e\u6cd5\u4ea4\u6613\u7684\u68c0\u6d4b\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u5f3a\u5927\u548c\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u91cf\u5b50\u542f\u53d1\u56fe\u795e\u7ecf\u7f51\u7edc\uff08QI-GNN\uff09\u4e0eQBoost\u6216\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u7b49\u96c6\u6210\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u5185\u7684\u89c4\u8303\u591a\u9879\u5f0f\uff08CP\uff09\u5206\u89e3\u5c42\uff0c\u4ee5\u589e\u5f3a\u590d\u6742\u6570\u636e\u7ed3\u6784\u7684\u5904\u7406\u548c\u5206\u6790\u80fd\u529b\u3002\u8be5\u7cfb\u7edf\u4e13\u4e3a\u533a\u5757\u94fe\u7f51\u7edc\u7684\u53cd\u6d17\u94b1\uff08AML\uff09\u5206\u6790\u800c\u8bbe\u8ba1\u3002", "result": "\u4e0e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u5b9e\u73b0\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u6b3a\u8bc8\u4ea4\u6613\u65b9\u9762\u8fbe\u5230\u4e8674.8%\u7684F2\u5206\u6570\u3002", "conclusion": "\u91cf\u5b50\u542f\u53d1\u6280\u672f\uff08\u8f85\u4ee5CP\u5c42\u7ed3\u6784\u6539\u8fdb\uff09\u5728\u590d\u6742\u7f51\u7edc\u5206\u6790\u4e2d\u4e0d\u4ec5\u80fd\u4e0e\u4f20\u7edf\u65b9\u6cd5\u5339\u654c\uff0c\u751a\u81f3\u53ef\u80fd\u8d85\u8d8a\u5b83\u4eec\uff0c\u4ece\u800c\u589e\u5f3a\u91d1\u878d\u5b89\u5168\u3002\u7814\u7a76\u7ed3\u679c\u5021\u5bfc\u5728\u91d1\u878d\u9886\u57df\u66f4\u5e7f\u6cdb\u5730\u91c7\u7528\u548c\u8fdb\u4e00\u6b65\u63a2\u7d22\u91cf\u5b50\u542f\u53d1\u7b97\u6cd5\uff0c\u4ee5\u6709\u6548\u6253\u51fb\u6b3a\u8bc8\u3002"}}
{"id": "2508.09454", "pdf": "https://arxiv.org/pdf/2508.09454", "abs": "https://arxiv.org/abs/2508.09454", "authors": ["Shuai Tan", "Biao Gong", "Zhuoxin Liu", "Yan Wang", "Xi Chen", "Yifan Feng", "Hengshuang Zhao"], "title": "Animate-X++: Universal Character Image Animation with Dynamic Backgrounds", "categories": ["cs.CV"], "comment": "Project page: https://lucaria-academy.github.io/Animate-X++/", "summary": "Character image animation, which generates high-quality videos from a\nreference image and target pose sequence, has seen significant progress in\nrecent years. However, most existing methods only apply to human figures, which\nusually do not generalize well on anthropomorphic characters commonly used in\nindustries like gaming and entertainment. Furthermore, previous methods could\nonly generate videos with static backgrounds, which limits the realism of the\nvideos. For the first challenge, our in-depth analysis suggests to attribute\nthis limitation to their insufficient modeling of motion, which is unable to\ncomprehend the movement pattern of the driving video, thus imposing a pose\nsequence rigidly onto the target character. To this end, this paper proposes\nAnimate-X++, a universal animation framework based on DiT for various character\ntypes, including anthropomorphic characters. To enhance motion representation,\nwe introduce the Pose Indicator, which captures comprehensive motion pattern\nfrom the driving video through both implicit and explicit manner. The former\nleverages CLIP visual features of a driving video to extract its gist of\nmotion, like the overall movement pattern and temporal relations among motions,\nwhile the latter strengthens the generalization of DiT by simulating possible\ninputs in advance that may arise during inference. For the second challenge, we\nintroduce a multi-task training strategy that jointly trains the animation and\nTI2V tasks. Combined with the proposed partial parameter training, this\napproach achieves not only character animation but also text-driven background\ndynamics, making the videos more realistic. Moreover, we introduce a new\nAnimated Anthropomorphic Benchmark (A2Bench) to evaluate the performance of\nAnimate-X++ on universal and widely applicable animation images. Extensive\nexperiments demonstrate the superiority and effectiveness of Animate-X++.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAnimate-X++\uff0c\u4e00\u4e2a\u57fa\u4e8eDiT\u7684\u901a\u7528\u52a8\u753b\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u62df\u4eba\u5316\u89d2\u8272\u6cdb\u5316\u6027\u5dee\u548c\u80cc\u666f\u9759\u6001\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u59ff\u6001\u6307\u793a\u5668\u548c\u591a\u4efb\u52a1\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u52a8\u6001\u80cc\u666f\u7684\u89d2\u8272\u52a8\u753b\u3002", "motivation": "\u73b0\u6709\u5b57\u7b26\u56fe\u50cf\u52a8\u753b\u65b9\u6cd5\u4e3b\u8981\u9002\u7528\u4e8e\u4eba\u7c7b\u89d2\u8272\uff0c\u5bf9\u6e38\u620f\u548c\u5a31\u4e50\u884c\u4e1a\u5e38\u7528\u7684\u62df\u4eba\u5316\u89d2\u8272\u6cdb\u5316\u6027\u5dee\uff1b\u6b64\u5916\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u53ea\u80fd\u751f\u6210\u9759\u6001\u80cc\u666f\u89c6\u9891\uff0c\u9650\u5236\u4e86\u89c6\u9891\u7684\u771f\u5b9e\u611f\u3002", "method": "1. \u63d0\u51faAnimate-X++\uff0c\u4e00\u4e2a\u57fa\u4e8eDiT\u7684\u901a\u7528\u52a8\u753b\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5305\u62ec\u62df\u4eba\u5316\u89d2\u8272\u5728\u5185\u7684\u591a\u79cd\u5b57\u7b26\u7c7b\u578b\u30022. \u5f15\u5165\u59ff\u6001\u6307\u793a\u5668\uff0c\u901a\u8fc7\u9690\u5f0f\uff08\u5229\u7528CLIP\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u8fd0\u52a8\u8981\u70b9\uff09\u548c\u663e\u5f0f\uff08\u6a21\u62df\u63a8\u7406\u8f93\u5165\u589e\u5f3a\u6cdb\u5316\u6027\uff09\u65b9\u5f0f\u6355\u6349\u9a71\u52a8\u89c6\u9891\u7684\u8fd0\u52a8\u6a21\u5f0f\u30023. \u91c7\u7528\u591a\u4efb\u52a1\u8bad\u7ec3\u7b56\u7565\uff0c\u8054\u5408\u8bad\u7ec3\u52a8\u753b\u548c\u6587\u672c\u5230\u89c6\u9891\uff08TI2V\uff09\u4efb\u52a1\uff0c\u7ed3\u5408\u90e8\u5206\u53c2\u6570\u8bad\u7ec3\u5b9e\u73b0\u89d2\u8272\u52a8\u753b\u548c\u6587\u672c\u9a71\u52a8\u7684\u80cc\u666f\u52a8\u6001\u30024. \u5f15\u5165\u65b0\u7684\u62df\u4eba\u5316\u52a8\u753b\u57fa\u51c6\uff08A2Bench\uff09\u4ee5\u8bc4\u4f30\u6027\u80fd\u3002", "result": "Animate-X++\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u89d2\u8272\u52a8\u753b\uff0c\u8fd8\u80fd\u751f\u6210\u6587\u672c\u9a71\u52a8\u7684\u80cc\u666f\u52a8\u6001\uff0c\u4f7f\u89c6\u9891\u66f4\u5177\u771f\u5b9e\u611f\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86Animate-X++\u7684\u4f18\u8d8a\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "Animate-X++\u662f\u4e00\u4e2a\u4f18\u8d8a\u4e14\u6709\u6548\u7684\u901a\u7528\u52a8\u753b\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u591a\u79cd\u89d2\u8272\u7c7b\u578b\u5e76\u751f\u6210\u5177\u6709\u52a8\u6001\u80cc\u666f\u7684\u89c6\u9891\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89d2\u8272\u52a8\u753b\u7684\u771f\u5b9e\u611f\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5173\u952e\u5c40\u9650\u6027\u3002"}}
{"id": "2508.09935", "pdf": "https://arxiv.org/pdf/2508.09935", "abs": "https://arxiv.org/abs/2508.09935", "authors": ["Sayem Hossen", "Monalisa Moon Joti", "Md. Golam Rashed"], "title": "Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach", "categories": ["cs.CL", "q-fin.CP", "q-fin.GN"], "comment": "21", "summary": "Business communication digitisation has reorganised the process of persuasive\ndiscourse, which\n  allows not only greater transparency but also advanced deception. This\ninquiry synthesises classical\n  rhetoric and communication psychology with linguistic theory and empirical\nstudies in the financial\n  reporting, sustainability discourse, and digital marketing to explain how\ndeceptive language can be\n  systematically detected using persuasive lexicon. In controlled settings,\ndetection accuracies of greater\n  than 99% were achieved by using computational textual analysis as well as\npersonalised transformer\n  models. However, reproducing this performance in multilingual settings is\nalso problematic and,\n  to a large extent, this is because it is not easy to find sufficient data,\nand because few multilingual\n  text-processing infrastructures are in place. This evidence shows that there\nhas been an increasing\n  gap between the theoretical representations of communication and those\nempirically approximated,\n  and therefore, there is a need to have strong automatic text-identification\nsystems where AI-based\n  discourse is becoming more realistic in communicating with humans.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u6570\u5b57\u5316\u5546\u4e1a\u6c9f\u901a\u4e2d\u7684\u6b3a\u9a97\u6027\u8bed\u8a00\u68c0\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u5b66\u79d1\u7406\u8bba\u548c\u4f7f\u7528\u8ba1\u7b97\u6587\u672c\u5206\u6790\u53caTransformer\u6a21\u578b\uff0c\u5728\u53d7\u63a7\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u8d85\u8fc799%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u3002\u7814\u7a76\u540c\u65f6\u6307\u51fa\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u6311\u6218\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f00\u53d1\u5f3a\u5927AI\u9a71\u52a8\u81ea\u52a8\u6587\u672c\u8bc6\u522b\u7cfb\u7edf\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u6570\u5b57\u5316\u5546\u4e1a\u6c9f\u901a\u5728\u5e26\u6765\u66f4\u9ad8\u900f\u660e\u5ea6\u7684\u540c\u65f6\uff0c\u4e5f\u5bfc\u81f4\u4e86\u66f4\u9ad8\u7ea7\u7684\u6b3a\u9a97\u884c\u4e3a\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u91ca\u5982\u4f55\u5229\u7528\u8bf4\u670d\u6027\u8bcd\u6c47\u7cfb\u7edf\u5730\u68c0\u6d4b\u6b3a\u9a97\u6027\u8bed\u8a00\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u79cd\u65b0\u5174\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u7efc\u5408\u4e86\u7ecf\u5178\u4fee\u8f9e\u5b66\u3001\u4f20\u64ad\u5fc3\u7406\u5b66\u3001\u8bed\u8a00\u5b66\u7406\u8bba\u4ee5\u53ca\u91d1\u878d\u62a5\u544a\u3001\u53ef\u6301\u7eed\u6027\u8bdd\u8bed\u548c\u6570\u5b57\u8425\u9500\u9886\u57df\u7684\u5b9e\u8bc1\u7814\u7a76\u3002\u5728\u53d7\u63a7\u8bbe\u7f6e\u4e2d\uff0c\u91c7\u7528\u8ba1\u7b97\u6587\u672c\u5206\u6790\u548c\u4e2a\u6027\u5316Transformer\u6a21\u578b\u6765\u68c0\u6d4b\u6b3a\u9a97\u6027\u8bed\u8a00\u3002", "result": "\u5728\u53d7\u63a7\u8bbe\u7f6e\u4e0b\uff0c\u5229\u7528\u8ba1\u7b97\u6587\u672c\u5206\u6790\u548c\u4e2a\u6027\u5316Transformer\u6a21\u578b\uff0c\u6b3a\u9a97\u6027\u8bed\u8a00\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u8d85\u8fc799%\u3002\u7136\u800c\uff0c\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u91cd\u73b0\u8fd9\u79cd\u6027\u80fd\u5b58\u5728\u95ee\u9898\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u8db3\u591f\u7684\u6570\u636e\u548c\u591a\u8bed\u8a00\u6587\u672c\u5904\u7406\u57fa\u7840\u8bbe\u65bd\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u6c9f\u901a\u7684\u7406\u8bba\u8868\u5f81\u4e0e\u7ecf\u9a8c\u8fd1\u4f3c\u4e4b\u95f4\u5b58\u5728\u65e5\u76ca\u6269\u5927\u7684\u5dee\u8ddd\u3002\u56e0\u6b64\uff0c\u5728AI\u9a71\u52a8\u7684\u8bdd\u8bed\u53d8\u5f97\u4e0e\u4eba\u7c7b\u6c9f\u901a\u66f4\u52a0\u903c\u771f\u7684\u80cc\u666f\u4e0b\uff0c\u8feb\u5207\u9700\u8981\u5f3a\u5927\u7684\u81ea\u52a8\u6587\u672c\u8bc6\u522b\u7cfb\u7edf\u3002"}}
{"id": "2508.09177", "pdf": "https://arxiv.org/pdf/2508.09177", "abs": "https://arxiv.org/abs/2508.09177", "authors": ["Xuanru Zhou", "Cheng Li", "Shuqiang Wang", "Ye Li", "Tao Tan", "Hairong Zheng", "Shanshan Wang"], "title": "Generative Artificial Intelligence in Medical Imaging: Foundations, Progress, and Clinical Translation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Generative artificial intelligence (AI) is rapidly transforming medical\nimaging by enabling capabilities such as data synthesis, image enhancement,\nmodality translation, and spatiotemporal modeling. This review presents a\ncomprehensive and forward-looking synthesis of recent advances in generative\nmodeling including generative adversarial networks (GANs), variational\nautoencoders (VAEs), diffusion models, and emerging multimodal foundation\narchitectures and evaluates their expanding roles across the clinical imaging\ncontinuum. We systematically examine how generative AI contributes to key\nstages of the imaging workflow, from acquisition and reconstruction to\ncross-modality synthesis, diagnostic support, and treatment planning. Emphasis\nis placed on both retrospective and prospective clinical scenarios, where\ngenerative models help address longstanding challenges such as data scarcity,\nstandardization, and integration across modalities. To promote rigorous\nbenchmarking and translational readiness, we propose a three-tiered evaluation\nframework encompassing pixel-level fidelity, feature-level realism, and\ntask-level clinical relevance. We also identify critical obstacles to\nreal-world deployment, including generalization under domain shift,\nhallucination risk, data privacy concerns, and regulatory hurdles. Finally, we\nexplore the convergence of generative AI with large-scale foundation models,\nhighlighting how this synergy may enable the next generation of scalable,\nreliable, and clinically integrated imaging systems. By charting technical\nprogress and translational pathways, this review aims to guide future research\nand foster interdisciplinary collaboration at the intersection of AI, medicine,\nand biomedical engineering.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u7cfb\u7edf\u56de\u987e\u4e86\u751f\u6210\u5f0fAI\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u3001\u5e94\u7528\u3001\u6311\u6218\u53ca\u672a\u6765\u8d8b\u52bf\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u5176\u5982\u4f55\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u6027\u7b49\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6b63\u5feb\u901f\u53d8\u9769\u533b\u5b66\u5f71\u50cf\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u3001\u6807\u51c6\u5316\u548c\u591a\u6a21\u6001\u6574\u5408\u7b49\u957f\u671f\u6311\u6218\u3002\u672c\u7efc\u8ff0\u65e8\u5728\u5168\u9762\u8bc4\u4f30\u5176\u5728\u4e34\u5e8a\u5f71\u50cf\u5de5\u4f5c\u6d41\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u672c\u7efc\u8ff0\u7efc\u5408\u5206\u6790\u4e86\u591a\u79cd\u751f\u6210\u6a21\u578b\uff08\u5982GAN\u3001VAE\u3001\u6269\u6563\u6a21\u578b\u3001\u591a\u6a21\u6001\u57fa\u7840\u67b6\u6784\uff09\uff0c\u7cfb\u7edf\u5ba1\u89c6\u4e86\u5b83\u4eec\u5728\u5f71\u50cf\u83b7\u53d6\u3001\u91cd\u5efa\u3001\u8de8\u6a21\u6001\u5408\u6210\u3001\u8bca\u65ad\u652f\u6301\u548c\u6cbb\u7597\u89c4\u5212\u7b49\u9636\u6bb5\u7684\u8d21\u732e\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u5c42\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u50cf\u7d20\u7ea7\u4fdd\u771f\u5ea6\u3001\u7279\u5f81\u7ea7\u771f\u5b9e\u6027\u548c\u4efb\u52a1\u7ea7\u4e34\u5e8a\u76f8\u5173\u6027\u8bc4\u4f30\u3002", "result": "\u751f\u6210\u5f0fAI\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u5305\u62ec\u6570\u636e\u5408\u6210\u3001\u56fe\u50cf\u589e\u5f3a\u3001\u6a21\u6001\u8f6c\u6362\u548c\u65f6\u7a7a\u5efa\u6a21\u3002\u5b83\u80fd\u6709\u6548\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u7b49\u95ee\u9898\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9762\u4e34\u6cdb\u5316\u6027\u3001\u5e7b\u89c9\u98ce\u9669\u3001\u6570\u636e\u9690\u79c1\u548c\u6cd5\u89c4\u7b49\u969c\u788d\u3002\u4e0e\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u7ed3\u5408\u6709\u671b\u5b9e\u73b0\u4e0b\u4e00\u4ee3\u53ef\u6269\u5c55\u3001\u53ef\u9760\u7684\u4e34\u5e8a\u5f71\u50cf\u7cfb\u7edf\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u5c24\u5176\u4e0e\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u878d\u5408\u5c06\u63a8\u52a8\u672a\u6765\u53d1\u5c55\u3002\u672c\u7efc\u8ff0\u65e8\u5728\u6307\u5bfc\u672a\u6765\u7814\u7a76\uff0c\u4fc3\u8fdbAI\u3001\u533b\u5b66\u548c\u751f\u7269\u533b\u5b66\u5de5\u7a0b\u9886\u57df\u7684\u8de8\u5b66\u79d1\u5408\u4f5c\u3002"}}
{"id": "2508.09263", "pdf": "https://arxiv.org/pdf/2508.09263", "abs": "https://arxiv.org/abs/2508.09263", "authors": ["Peng Wang", "Dongsheng Wang", "He Zhao", "Hangting Ye", "Dandan Guo", "Yi Chang"], "title": "LLM Empowered Prototype Learning for Zero and Few-Shot Tasks on Tabular Data", "categories": ["cs.LG"], "comment": null, "summary": "Recent breakthroughs in large language models (LLMs) have opened the door to\nin-depth investigation of their potential in tabular data modeling. However,\neffectively utilizing advanced LLMs in few-shot and even zero-shot scenarios is\nstill challenging. To this end, we propose a novel LLM-based prototype\nestimation framework for tabular learning. Our key idea is to query the LLM to\ngenerate feature values based example-free prompt, which solely relies on task\nand feature descriptions. With the feature values generated by LLM, we can\nbuild a zero-shot prototype in a training-free manner, which can be further\nenhanced by fusing few-shot samples, avoiding training a classifier or\nfinetuning the LLMs. Thanks to the example-free prompt and prototype\nestimation, ours bypasses the constraints brought by the example-based prompt,\nproviding a scalable and robust framework. Extensive experiments demonstrate\nthe effectiveness of ours in zero and few-shot tabular learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u539f\u578b\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u65e0\u793a\u4f8b\u63d0\u793a\u751f\u6210\u7279\u5f81\u503c\u6784\u5efa\u96f6/\u5c11\u6837\u672c\u539f\u578b\uff0c\u5b9e\u73b0\u8bad\u7ec3\u548c\u5fae\u8c03\u65e0\u5173\u7684\u8868\u683c\u6570\u636e\u5b66\u4e60\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u8868\u683c\u6570\u636e\u5efa\u6a21\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5728\u96f6\u6837\u672c(zero-shot)\u548c\u5c11\u6837\u672c(few-shot)\u573a\u666f\u4e2d\u6709\u6548\u5229\u7528\u5b83\u4eec\u4ecd\u662f\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684LLM-based\u539f\u578b\u4f30\u8ba1\u6846\u67b6\u3002\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u201c\u65e0\u793a\u4f8b\u63d0\u793a\u201d\uff08\u4ec5\u4f9d\u8d56\u4efb\u52a1\u548c\u7279\u5f81\u63cf\u8ff0\uff09\u67e5\u8be2LLM\u751f\u6210\u7279\u5f81\u503c\uff0c\u8fdb\u800c\u6784\u5efa\u65e0\u9700\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u539f\u578b\u3002\u8be5\u539f\u578b\u53ef\u901a\u8fc7\u878d\u5408\u5c11\u91cf\u6837\u672c\u8fdb\u4e00\u6b65\u589e\u5f3a\uff0c\u4e14\u907f\u514d\u4e86\u8bad\u7ec3\u5206\u7c7b\u5668\u6216\u5fae\u8c03LLM\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8868\u683c\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u65e0\u793a\u4f8b\u63d0\u793a\u548c\u539f\u578b\u4f30\u8ba1\uff0c\u7ed5\u8fc7\u4e86\u57fa\u4e8e\u793a\u4f8b\u63d0\u793a\u7684\u9650\u5236\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u8868\u683c\u6570\u636e\u96f6/\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6848\u3002"}}
{"id": "2508.09456", "pdf": "https://arxiv.org/pdf/2508.09456", "abs": "https://arxiv.org/abs/2508.09456", "authors": ["Junxian Li", "Beining Xu", "Di Zhang"], "title": "IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding", "categories": ["cs.CV", "cs.CL", "cs.CR"], "comment": "13 pages, 13 Figures", "summary": "Vision-language models (VLMs) have shown significant advancements in tasks\nsuch as visual grounding, where they localize specific objects in images based\non natural language queries and images. However, security issues in visual\ngrounding tasks for VLMs remain underexplored, especially in the context of\nbackdoor attacks. In this paper, we introduce a novel input-aware backdoor\nattack method, IAG, designed to manipulate the grounding behavior of VLMs. This\nattack forces the model to ground a specific target object in the input image,\nregardless of the user's query. We propose an adaptive trigger generator that\nembeds the semantic information of the attack target's description into the\noriginal image using a text-conditional U-Net, thereby overcoming the\nopen-vocabulary attack challenge. To ensure the attack's stealthiness, we\nutilize a reconstruction loss to minimize visual discrepancies between poisoned\nand clean images. Additionally, we introduce a unified method for generating\nattack data. IAG is evaluated theoretically and empirically, demonstrating its\nfeasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches\nover 65\\% on various testing sets. IAG also shows promising potential on\nmanipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on\nclean samples. Extensive specific experiments, such as ablation study and\npotential defense, also indicate the robustness and transferability of our\nattack.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aIAG\u7684\u65b0\u578b\u8f93\u5165\u611f\u77e5\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u65e8\u5728\u64cd\u7eb5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u89c6\u89c9\u5b9a\u4f4d\u884c\u4e3a\uff0c\u4f7f\u5176\u65e0\u8bba\u7528\u6237\u67e5\u8be2\u5982\u4f55\uff0c\u90fd\u5f3a\u5236\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7684\u7279\u5b9a\u76ee\u6807\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u9690\u853d\u6027\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5b89\u5168\u95ee\u9898\uff08\u7279\u522b\u662f\u540e\u95e8\u653b\u51fb\uff09\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "1. \u5f15\u5165\u4e86\u540d\u4e3aIAG\u7684\u8f93\u5165\u611f\u77e5\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u8f93\u5165\u56fe\u50cf\u4e2d\u5d4c\u5165\u8bed\u4e49\u4fe1\u606f\u6765\u64cd\u7eb5VLM\u7684\u5b9a\u4f4d\u884c\u4e3a\u30022. \u63d0\u51fa\u4e00\u4e2a\u81ea\u9002\u5e94\u89e6\u53d1\u5668\u751f\u6210\u5668\uff0c\u5229\u7528\u6587\u672c\u6761\u4ef6U-Net\u5c06\u653b\u51fb\u76ee\u6807\u63cf\u8ff0\u7684\u8bed\u4e49\u4fe1\u606f\u5d4c\u5165\u539f\u59cb\u56fe\u50cf\uff0c\u4ee5\u514b\u670d\u5f00\u653e\u8bcd\u6c47\u653b\u51fb\u6311\u6218\u30023. \u91c7\u7528\u91cd\u5efa\u635f\u5931\u6765\u6700\u5c0f\u5316\u4e2d\u6bd2\u56fe\u50cf\u4e0e\u5e72\u51c0\u56fe\u50cf\u4e4b\u95f4\u7684\u89c6\u89c9\u5dee\u5f02\uff0c\u786e\u4fdd\u653b\u51fb\u7684\u9690\u853d\u6027\u30024. \u5f15\u5165\u4e86\u7edf\u4e00\u7684\u653b\u51fb\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002", "result": "1. IAG\u5728\u7406\u8bba\u548c\u7ecf\u9a8c\u4e0a\u5747\u8bc1\u660e\u4e86\u5176\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u30022. \u5728InternVL-2.5-8B\u6a21\u578b\u4e0a\uff0cASR@0.5\u5728\u5404\u79cd\u6d4b\u8bd5\u96c6\u4e0a\u8d85\u8fc765%\u30023. \u5bf9Ferret-7B\u548cLlaVA-1.5-7B\u6a21\u578b\u4e5f\u663e\u793a\u51fa\u826f\u597d\u7684\u653b\u51fb\u6f5c\u529b\uff0c\u4e14\u5bf9\u5e72\u51c0\u6837\u672c\u7684\u51c6\u786e\u6027\u4e0b\u964d\u5f88\u5c0f\u30024. \u901a\u8fc7\u6d88\u878d\u7814\u7a76\u548c\u6f5c\u5728\u9632\u5fa1\u7b49\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u653b\u51fb\u7684\u9c81\u68d2\u6027\u548c\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u7684\u65b0\u578b\u3001\u6709\u6548\u4e14\u9690\u853d\u7684\u540e\u95e8\u653b\u51fb\u65b9\u6cd5IAG\uff0c\u63ed\u793a\u4e86\u8be5\u9886\u57df\u6f5c\u5728\u7684\u5b89\u5168\u6f0f\u6d1e\u548c\u98ce\u9669\u3002"}}
{"id": "2508.09937", "pdf": "https://arxiv.org/pdf/2508.09937", "abs": "https://arxiv.org/abs/2508.09937", "authors": ["Muneeza Azmat", "Momin Abbas", "Maysa Malfiza Garcia de Macedo", "Marcelo Carpinette Grave", "Luan Soares de Souza", "Tiago Machado", "Rogerio A de Paula", "Raya Horesh", "Yixin Chen", "Heloisa Caroline de Souza Pereira Candello", "Rebecka Nordenlow", "Aminat Adebiyi"], "title": "A Comprehensive Evaluation framework of Alignment Techniques for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "In submission", "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world applications, ensuring their outputs align with human values and\nsafety standards has become critical. The field has developed diverse alignment\napproaches including traditional fine-tuning methods (RLHF, instruction\ntuning), post-hoc correction systems, and inference-time interventions, each\nwith distinct advantages and limitations. However, the lack of unified\nevaluation frameworks makes it difficult to systematically compare these\nparadigms and guide deployment decisions. This paper introduces a\nmulti-dimensional evaluation of alignment techniques for LLMs, a comprehensive\nevaluation framework that provides a systematic comparison across all major\nalignment paradigms. Our framework assesses methods along four key dimensions:\nalignment detection, alignment quality, computational efficiency, and\nrobustness. Through experiments across diverse base models and alignment\nstrategies, we demonstrate the utility of our framework in identifying\nstrengths and limitations of current state-of-the-art models, providing\nvaluable insights for future research directions.", "AI": {"tldr": "\u968f\u7740LLMs\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u8f93\u51fa\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u9488\u5bf9\u7f3a\u4e4f\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u6bd4\u8f83\u5404\u79cdLLM\u5bf9\u9f50\u6280\u672f\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709SOTA\u6a21\u578b\u7684\u4f18\u7f3a\u70b9\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65e5\u76ca\u96c6\u6210\u5230\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u786e\u4fdd\u5176\u8f93\u51fa\u7b26\u5408\u4eba\u7c7b\u4ef7\u503c\u89c2\u548c\u5b89\u5168\u6807\u51c6\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5bfc\u81f4\u96be\u4ee5\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u5bf9\u9f50\u8303\u5f0f\u5e76\u6307\u5bfc\u90e8\u7f72\u51b3\u7b56\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u7ef4\u5ea6LLM\u5bf9\u9f50\u6280\u672f\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u5bf9\u6240\u6709\u4e3b\u8981\u7684\u5bf9\u9f50\u8303\u5f0f\u8fdb\u884c\u7cfb\u7edf\u6bd4\u8f83\u3002\u8be5\u6846\u67b6\u4ece\u5bf9\u9f50\u68c0\u6d4b\u3001\u5bf9\u9f50\u8d28\u91cf\u3001\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5728\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u548c\u5bf9\u9f50\u7b56\u7565\u4e0a\u7684\u5b9e\u9a8c\uff0c\u8be5\u6846\u67b6\u6210\u529f\u63ed\u793a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u5728\u5bf9\u9f50\u65b9\u9762\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u8bc4\u4f30\u6846\u67b6\u4e3a\u672a\u6765\u7684LLM\u5bf9\u9f50\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.09264", "pdf": "https://arxiv.org/pdf/2508.09264", "abs": "https://arxiv.org/abs/2508.09264", "authors": ["Matin Hassanloo", "Ali Zareh", "Mehmet Kemal \u00d6zdemir"], "title": "Detection of Odor Presence via Deep Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Odor detection underpins food safety, environmental monitoring, medical\ndiagnostics, and many more fields. The current artificial sensors developed for\nodor detection struggle with complex mixtures while non-invasive recordings\nlack reliable single-trial fidelity. To develop a general system for odor\ndetection, in this study we present a preliminary work where we aim to test two\nhypotheses: (i) that spectral features of local field potentials (LFPs) are\nsufficient for robust single-trial odor detection and (ii) that signals from\nthe olfactory bulb alone are adequate. To test two hypotheses, we propose an\nensemble of complementary one-dimensional convolutional networks (ResCNN and\nAttentionCNN) that decodes the presence of odor from multichannel olfactory\nbulb LFPs. Tested on 2,349 trials from seven awake mice, our final ensemble\nmodel supports both hypotheses, achieving a mean accuracy of 86.6%, an F1-score\nof 81.0%, and an AUC of 0.9247, substantially outperforming previous\nbenchmarks. In addition, the t-SNE visualization confirms that our framework\ncaptures biologically significant signatures. These findings establish the\nfeasibility of robust single-trial detection of the presence of odor from\nextracellular LFPs, as well as demonstrate the potential of deep learning\nmodels to provide a deeper understanding of olfactory representations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\uff08ResCNN\u548cAttentionCNN\u96c6\u6210\u6a21\u578b\uff09\u5206\u6790\u6e05\u9192\u5c0f\u9f20\u55c5\u7403\u7684\u5c40\u90e8\u573a\u7535\u4f4d\uff08LFPs\uff09\uff0c\u5b9e\u73b0\u4e86\u5bf9\u55c5\u89c9\u4e8b\u4ef6\u7684\u5355\u6b21\u3001\u9c81\u68d2\u68c0\u6d4b\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u6c14\u5473\u68c0\u6d4b\u6280\u672f\u5728\u5904\u7406\u590d\u6742\u6df7\u5408\u7269\u548c\u5b9e\u73b0\u53ef\u9760\u7684\u5355\u6b21\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u901a\u7528\u7684\u6c14\u5473\u68c0\u6d4b\u7cfb\u7edf\uff0c\u5e76\u9a8c\u8bc1\u4e24\u4e2a\u6838\u5fc3\u5047\u8bbe\uff1a(i) \u5c40\u90e8\u573a\u7535\u4f4d\uff08LFPs\uff09\u7684\u9891\u8c31\u7279\u5f81\u662f\u5426\u8db3\u4ee5\u8fdb\u884c\u9c81\u68d2\u7684\u5355\u6b21\u6c14\u5473\u68c0\u6d4b\uff1b(ii) \u4ec5\u51ed\u55c5\u7403\u4fe1\u53f7\u662f\u5426\u8db3\u591f\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u5e76\u4f7f\u7528\u4e86\u4e00\u4e2a\u7531\u4e92\u8865\u7684\u4e00\u7ef4\u5377\u79ef\u7f51\u7edc\uff08ResCNN\u548cAttentionCNN\uff09\u7ec4\u6210\u7684\u96c6\u6210\u6a21\u578b\u3002\u8be5\u6a21\u578b\u65e8\u5728\u4ece\u591a\u901a\u9053\u55c5\u7403LFPs\u4e2d\u89e3\u7801\u6c14\u5473\u7684\u5b58\u5728\u3002\u5b9e\u9a8c\u57287\u53ea\u6e05\u9192\u5c0f\u9f20\u76842,349\u4e2a\u8bd5\u9a8c\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u6700\u7ec8\u7684\u96c6\u6210\u6a21\u578b\u652f\u6301\u4e86\u4e24\u4e2a\u7814\u7a76\u5047\u8bbe\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5305\u62ec86.6%\u7684\u5e73\u5747\u51c6\u786e\u7387\u300181.0%\u7684F1-score\u548c0.9247\u7684AUC\uff0c\u660e\u663e\u4f18\u4e8e\u4ee5\u5f80\u7684\u57fa\u51c6\u3002\u6b64\u5916\uff0ct-SNE\u53ef\u89c6\u5316\u8bc1\u5b9e\u4e86\u8be5\u6846\u67b6\u6355\u83b7\u4e86\u751f\u7269\u5b66\u4e0a\u91cd\u8981\u7684\u7279\u5f81\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\u4e86\u4ece\u7ec6\u80de\u5916LFPs\u5b9e\u73b0\u6c14\u5473\u5b58\u5728\u9c81\u68d2\u5355\u6b21\u68c0\u6d4b\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u6df1\u5165\u7406\u89e3\u55c5\u89c9\u8868\u5f81\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.09459", "pdf": "https://arxiv.org/pdf/2508.09459", "abs": "https://arxiv.org/abs/2508.09459", "authors": ["Wen Huang", "Jiarui Yang", "Tao Dai", "Jiawei Li", "Shaoxiong Zhan", "Bin Wang", "Shu-Tao Xia"], "title": "RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Visual manipulation localization (VML) -- across both images and videos -- is\na crucial task in digital forensics that involves identifying tampered regions\nin visual content. However, existing methods often lack cross-modal\ngeneralization and struggle to handle high-resolution or long-duration inputs\nefficiently.\n  We propose RelayFormer, a unified and modular architecture for visual\nmanipulation localization across images and videos. By leveraging flexible\nlocal units and a Global-Local Relay Attention (GLoRA) mechanism, it enables\nscalable, resolution-agnostic processing with strong generalization. Our\nframework integrates seamlessly with existing Transformer-based backbones, such\nas ViT and SegFormer, via lightweight adaptation modules that require only\nminimal architectural changes, ensuring compatibility without disrupting\npretrained representations.\n  Furthermore, we design a lightweight, query-based mask decoder that supports\none-shot inference across video sequences with linear complexity. Extensive\nexperiments across multiple benchmarks demonstrate that our approach achieves\nstate-of-the-art localization performance, setting a new baseline for scalable\nand modality-agnostic VML. Code is available at:\nhttps://github.com/WenOOI/RelayFormer.", "AI": {"tldr": "\u63d0\u51faRelayFormer\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u6a21\u5757\u5316\u7684\u89c6\u89c9\u5185\u5bb9\u7be1\u6539\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7GLoRA\u673a\u5236\u5b9e\u73b0\u8de8\u6a21\u6001\u3001\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u7387\u7684\u7be1\u6539\u533a\u57df\u8bc6\u522b\uff0c\u5e76\u8fbe\u5230\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u7be1\u6539\u5b9a\u4f4d\uff08VML\uff09\u5728\u6570\u5b57\u53d6\u8bc1\u4e2d\u975e\u5e38\u5173\u952e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u8de8\u6a21\u6001\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u6216\u957f\u65f6\u95f4\u8f93\u5165\u3002", "method": "\u63d0\u51faRelayFormer\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u6a21\u5757\u5316\u7684\u89c6\u89c9\u7be1\u6539\u5b9a\u4f4d\u67b6\u6784\u3002\u5b83\u5229\u7528\u7075\u6d3b\u7684\u5c40\u90e8\u5355\u5143\u548c\u5168\u5c40-\u5c40\u90e8\u4e2d\u7ee7\u6ce8\u610f\u529b\uff08GLoRA\uff09\u673a\u5236\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u5206\u8fa8\u7387\u65e0\u5173\u7684\u5904\u7406\u548c\u5f3a\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u6a21\u5757\u4e0e\u73b0\u6709Transformer\u9aa8\u5e72\uff08\u5982ViT\u3001SegFormer\uff09\u65e0\u7f1d\u96c6\u6210\u3002\u6b64\u5916\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u57fa\u4e8e\u67e5\u8be2\u7684\u63a9\u7801\u89e3\u7801\u5668\uff0c\u652f\u6301\u89c6\u9891\u5e8f\u5217\u7684\u4e00\u6b21\u6027\u63a8\u7406\uff0c\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRelayFormer\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5b9a\u4f4d\u6027\u80fd\u3002", "conclusion": "RelayFormer\u4e3a\u53ef\u6269\u5c55\u4e14\u6a21\u6001\u65e0\u5173\u7684\u89c6\u89c9\u7be1\u6539\u5b9a\u4f4d\uff08VML\uff09\u4efb\u52a1\u8bbe\u7acb\u4e86\u65b0\u7684\u57fa\u7ebf\u3002"}}
{"id": "2508.09945", "pdf": "https://arxiv.org/pdf/2508.09945", "abs": "https://arxiv.org/abs/2508.09945", "authors": ["Lingjie Jiang", "Shaohan Huang", "Xun Wu", "Yixia Li", "Dongdong Zhang", "Furu Wei"], "title": "VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) have significantly advanced the\nintegration of visual and textual understanding. However, their ability to\ngenerate code from multimodal inputs remains limited. In this work, we\nintroduce VisCodex, a unified framework that seamlessly merges vision and\ncoding language models to empower MLLMs with strong multimodal code generation\nabilities. Leveraging a task vector-based model merging technique, we integrate\na state-of-the-art coding LLM into a strong vision-language backbone, while\npreserving both visual comprehension and advanced coding skills. To support\ntraining and evaluation, we introduce the Multimodal Coding Dataset (MCD), a\nlarge-scale and diverse collection of 598k samples, including high-quality HTML\ncode, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic\nproblems. Furthermore, we propose InfiBench-V, a novel and challenging\nbenchmark specifically designed to assess models on visually-rich, real-world\nprogramming questions that demand a nuanced understanding of both textual and\nvisual contexts. Extensive experiments show that VisCodex achieves\nstate-of-the-art performance among open-source MLLMs and approaches proprietary\nmodels like GPT-4o, highlighting the effectiveness of our model merging\nstrategy and new datasets.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u591a\u6a21\u6001\u4ee3\u7801\u751f\u6210\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\u3002\u672c\u6587\u63d0\u51faVisCodex\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u5411\u91cf\u5408\u5e76\u6280\u672f\u878d\u5408\u89c6\u89c9\u4e0e\u4ee3\u7801LLM\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u578b\u591a\u6a21\u6001\u4ee3\u7801\u6570\u636e\u96c6\uff08MCD\uff09\u548c\u8bc4\u4f30\u57fa\u51c6\uff08InfiBench-V\uff09\u3002\u5b9e\u9a8c\u8bc1\u660eVisCodex\u5728\u5f00\u6e90MLLM\u4e2d\u6027\u80fd\u9886\u5148\uff0c\u5e76\u63a5\u8fd1GPT-4o\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u548c\u6587\u672c\u7406\u89e3\u7684\u6574\u5408\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u4ece\u591a\u6a21\u6001\u8f93\u5165\u751f\u6210\u4ee3\u7801\u7684\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002", "method": "\u5f15\u5165VisCodex\u6846\u67b6\uff0c\u4e00\u4e2a\u7edf\u4e00\u878d\u5408\u89c6\u89c9\u4e0e\u7f16\u7801\u8bed\u8a00\u6a21\u578b\u4ee5\u589e\u5f3aMLLMs\u591a\u6a21\u6001\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u65b9\u6848\u3002\u91c7\u7528\u57fa\u4e8e\u4efb\u52a1\u5411\u91cf\u7684\u6a21\u578b\u5408\u5e76\u6280\u672f\uff0c\u5c06\u5148\u8fdb\u7684\u7f16\u7801LLM\u96c6\u6210\u5230\u5f3a\u5927\u7684\u89c6\u89c9-\u8bed\u8a00\u9aa8\u5e72\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u7406\u89e3\u548c\u9ad8\u7ea7\u7f16\u7801\u6280\u80fd\u3002\u6b64\u5916\uff0c\u6784\u5efa\u4e86\u5927\u578b\u591a\u6a21\u6001\u7f16\u7801\u6570\u636e\u96c6\uff08MCD\uff09\uff0859.8\u4e07\u6837\u672c\uff09\u548c\u65b0\u9896\u7684\u57fa\u51c6InfiBench-V\uff0c\u4ee5\u652f\u6301\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "VisCodex\u5728\u5f00\u6e90MLLMs\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u6027\u80fd\uff0c\u5e76\u63a5\u8fd1GPT-4o\u7b49\u4e13\u6709\u6a21\u578b\u3002\u8fd9\u7a81\u663e\u4e86\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5408\u5e76\u7b56\u7565\u548c\u65b0\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u7684VisCodex\u6846\u67b6\u3001\u4efb\u52a1\u5411\u91cf\u6a21\u578b\u5408\u5e76\u7b56\u7565\u4ee5\u53ca\u65b0\u6784\u5efa\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u6311\u6218\u6027\u57fa\u51c6\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u89c6\u89c9\u4e30\u5bcc\u7684\u7f16\u7a0b\u4efb\u52a1\u4e0a\u8fbe\u5230\u9886\u5148\u6c34\u5e73\uff0c\u5e76\u6709\u671b\u5ab2\u7f8e\u9876\u5c16\u95ed\u6e90\u6a21\u578b\u3002"}}
{"id": "2508.09265", "pdf": "https://arxiv.org/pdf/2508.09265", "abs": "https://arxiv.org/abs/2508.09265", "authors": ["Danial Saber", "Amirali Salehi-Abari"], "title": "Over-Squashing in GNNs and Causal Inference of Rewiring Strategies", "categories": ["cs.LG", "stat.ME"], "comment": "14 pages, 2 figures", "summary": "Graph neural networks (GNNs) have exhibited state-of-the-art performance\nacross wide-range of domains such as recommender systems, material design, and\ndrug repurposing. Yet message-passing GNNs suffer from over-squashing --\nexponential compression of long-range information from distant nodes -- which\nlimits expressivity. Rewiring techniques can ease this bottleneck; but their\npractical impacts are unclear due to the lack of a direct empirical\nover-squashing metric. We propose a rigorous, topology-focused method for\nassessing over-squashing between node pairs using the decay rate of their\nmutual sensitivity. We then extend these pairwise assessments to four\ngraph-level statistics (prevalence, intensity, variability, extremity).\nCoupling these metrics with a within-graph causal design, we quantify how\nrewiring strategies affect over-squashing on diverse graph- and\nnode-classification benchmarks. Our extensive empirical analyses show that most\ngraph classification datasets suffer from over-squashing (but to various\nextents), and rewiring effectively mitigates it -- though the degree of\nmitigation, and its translation into performance gains, varies by dataset and\nmethod. We also found that over-squashing is less notable in node\nclassification datasets, where rewiring often increases over-squashing, and\nperformance variations are uncorrelated with over-squashing changes. These\nfindings suggest that rewiring is most beneficial when over-squashing is both\nsubstantial and corrected with restraint -- while overly aggressive rewiring,\nor rewiring applied to minimally over-squashed graphs, is unlikely to help and\nmay even harm performance. Our plug-and-play diagnostic tool lets practitioners\ndecide -- before any training -- whether rewiring is likely to pay off.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u8861\u91cf\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u8fc7\u538b\u7f29\uff08over-squashing\uff09\u7684\u65b0\u6307\u6807\uff0c\u5e76\u7528\u5176\u8bc4\u4f30\u56fe\u91cd\u8fde\uff08rewiring\uff09\u6280\u672f\u7684\u6548\u679c\u3002\u53d1\u73b0\u91cd\u8fde\u80fd\u6709\u6548\u7f13\u89e3\u56fe\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8fc7\u538b\u7f29\u95ee\u9898\uff0c\u4f46\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u6548\u679c\u4e0d\u660e\u663e\u751a\u81f3\u53ef\u80fd\u6709\u5bb3\u3002\u540c\u65f6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u5de5\u5177\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u867d\u7136\u6027\u80fd\u5353\u8d8a\uff0c\u4f46\u5b58\u5728\u8fc7\u538b\u7f29\u95ee\u9898\u2014\u2014\u5373\u957f\u8ddd\u79bb\u4fe1\u606f\u88ab\u6307\u6570\u7ea7\u538b\u7f29\uff0c\u9650\u5236\u4e86\u5176\u8868\u8fbe\u80fd\u529b\u3002\u56fe\u91cd\u8fde\u6280\u672f\u6709\u671b\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u76f4\u63a5\u7684\u7ecf\u9a8c\u6027\u8fc7\u538b\u7f29\u5ea6\u91cf\u6807\u51c6\uff0c\u5176\u5b9e\u9645\u6548\u679c\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u4e25\u683c\u7684\u3001\u4ee5\u62d3\u6251\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8282\u70b9\u5bf9\u4e4b\u95f4\u76f8\u4e92\u654f\u611f\u5ea6\u7684\u8870\u51cf\u7387\u6765\u8bc4\u4f30\u8fc7\u538b\u7f29\u3002\u6211\u4eec\u5c06\u8fd9\u79cd\u4e24\u4e24\u8bc4\u4f30\u6269\u5c55\u5230\u56db\u4e2a\u56fe\u7ea7\u522b\u7edf\u8ba1\u91cf\uff08\u666e\u904d\u6027\u3001\u5f3a\u5ea6\u3001\u53d8\u5f02\u6027\u3001\u6781\u7aef\u6027\uff09\u3002\u7ed3\u5408\u8fd9\u4e9b\u5ea6\u91cf\u548c\u56fe\u5185\u56e0\u679c\u8bbe\u8ba1\uff0c\u6211\u4eec\u91cf\u5316\u4e86\u91cd\u8fde\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u4e0d\u540c\u56fe\u548c\u8282\u70b9\u5206\u7c7b\u57fa\u51c6\u4e0a\u7684\u8fc7\u538b\u7f29\u3002", "result": "\u5927\u591a\u6570\u56fe\u5206\u7c7b\u6570\u636e\u96c6\u90fd\u5b58\u5728\u4e0d\u540c\u7a0b\u5ea6\u7684\u8fc7\u538b\u7f29\uff0c\u4e14\u91cd\u8fde\u80fd\u6709\u6548\u7f13\u89e3\u3002\u7136\u800c\uff0c\u7f13\u89e3\u7a0b\u5ea6\u53ca\u5176\u8f6c\u5316\u4e3a\u6027\u80fd\u63d0\u5347\u7684\u6548\u679c\u56e0\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u800c\u5f02\u3002\u5728\u8282\u70b9\u5206\u7c7b\u6570\u636e\u96c6\u4e2d\uff0c\u8fc7\u538b\u7f29\u4e0d\u90a3\u4e48\u660e\u663e\uff0c\u91cd\u8fde\u5e38\u4f1a\u589e\u52a0\u8fc7\u538b\u7f29\uff0c\u5e76\u4e14\u6027\u80fd\u53d8\u5316\u4e0e\u8fc7\u538b\u7f29\u53d8\u5316\u4e0d\u76f8\u5173\u3002", "conclusion": "\u91cd\u8fde\u5728\u8fc7\u538b\u7f29\u4e25\u91cd\u4e14\u9002\u5ea6\u4fee\u6b63\u65f6\u6700\u4e3a\u6709\u76ca\u3002\u8fc7\u4e8e\u6fc0\u8fdb\u7684\u91cd\u8fde\uff0c\u6216\u5e94\u7528\u4e8e\u8f7b\u5fae\u8fc7\u538b\u7f29\u56fe\u7684\u91cd\u8fde\uff0c\u4e0d\u592a\u53ef\u80fd\u5e26\u6765\u5e2e\u52a9\uff0c\u751a\u81f3\u53ef\u80fd\u635f\u5bb3\u6027\u80fd\u3002\u6211\u4eec\u63d0\u4f9b\u7684\u5373\u63d2\u5373\u7528\u8bca\u65ad\u5de5\u5177\u53ef\u4ee5\u5e2e\u52a9\u5b9e\u8df5\u8005\u5728\u8bad\u7ec3\u524d\u51b3\u5b9a\u91cd\u8fde\u662f\u5426\u503c\u5f97\u5c1d\u8bd5\u3002"}}
{"id": "2508.09461", "pdf": "https://arxiv.org/pdf/2508.09461", "abs": "https://arxiv.org/abs/2508.09461", "authors": ["Hao Yu", "Rupayan Mallick", "Margrit Betke", "Sarah Adel Bargal"], "title": "Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Different forms of customized 2D avatars are widely used in gaming\napplications, virtual communication, education, and content creation. However,\nexisting approaches often fail to capture fine-grained facial expressions and\nstruggle to preserve identity across different expressions. We propose\nGEN-AFFECT, a novel framework for personalized avatar generation that generates\nexpressive and identity-consistent avatars with a diverse set of facial\nexpressions. Our framework proposes conditioning a multimodal diffusion\ntransformer on an extracted identity-expression representation. This enables\nidentity preservation and representation of a wide range of facial expressions.\nGEN-AFFECT additionally employs consistent attention at inference for\ninformation sharing across the set of generated expressions, enabling the\ngeneration process to maintain identity consistency over the array of generated\nfine-grained expressions. GEN-AFFECT demonstrates superior performance compared\nto previous state-of-the-art methods on the basis of the accuracy of the\ngenerated expressions, the preservation of the identity and the consistency of\nthe target identity across an array of fine-grained facial expressions.", "AI": {"tldr": "\u63d0\u51faGEN-AFFECT\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5bcc\u6709\u8868\u73b0\u529b\u4e14\u8eab\u4efd\u4e00\u81f4\u7684\u4e2a\u6027\u53162D\u865a\u62df\u5f62\u8c61\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u9762\u90e8\u8868\u60c5\uff0c\u4e14\u5728\u4e0d\u540c\u8868\u60c5\u4e0b\u96be\u4ee5\u4fdd\u6301\u865a\u62df\u5f62\u8c61\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "method": "GEN-AFFECT\u6846\u67b6\u901a\u8fc7\u5c06\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\u4e0e\u63d0\u53d6\u7684\u8eab\u4efd-\u8868\u60c5\u8868\u793a\u76f8\u7ed3\u5408\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u91c7\u7528\u4e00\u81f4\u6027\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u786e\u4fdd\u751f\u6210\u5404\u79cd\u7ec6\u5fae\u8868\u60c5\u7684\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "result": "GEN-AFFECT\u5728\u751f\u6210\u8868\u60c5\u7684\u51c6\u786e\u6027\u3001\u8eab\u4efd\u4fdd\u7559\u548c\u76ee\u6807\u8eab\u4efd\u5728\u7ec6\u5fae\u8868\u60c5\u95f4\u7684\u4e00\u81f4\u6027\u65b9\u9762\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "GEN-AFFECT\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u8868\u8fbe\u529b\u5f3a\u4e14\u8eab\u4efd\u7a33\u5b9a\u7684\u4e2a\u6027\u5316\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002"}}
{"id": "2508.09952", "pdf": "https://arxiv.org/pdf/2508.09952", "abs": "https://arxiv.org/abs/2508.09952", "authors": ["Hermione Warr", "Wentian Xu", "Harry Anthony", "Yasin Ibrahim", "Daniel McGowan", "Konstantinos Kamnitsas"], "title": "Specialised or Generic? Tokenization Choices for Radiology Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ELAMI@MICCAI2025", "summary": "The vocabulary used by language models (LM) - defined by the tokenizer -\nplays a key role in text generation quality. However, its impact remains\nunder-explored in radiology. In this work, we address this gap by\nsystematically comparing general, medical, and domain-specific tokenizers on\nthe task of radiology report summarisation across three imaging modalities. We\nalso investigate scenarios with and without LM pre-training on PubMed\nabstracts. Our findings demonstrate that medical and domain-specific\nvocabularies outperformed widely used natural language alternatives when models\nare trained from scratch. Pre-training partially mitigates performance\ndifferences between tokenizers, whilst the domain-specific tokenizers achieve\nthe most favourable results. Domain-specific tokenizers also reduce memory\nrequirements due to smaller vocabularies and shorter sequences. These results\ndemonstrate that adapting the vocabulary of LMs to the clinical domain provides\npractical benefits, including improved performance and reduced computational\ndemands, making such models more accessible and effective for both research and\nreal-world healthcare settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u6bd4\u8f83\u4e86\u901a\u7528\u3001\u533b\u5b66\u548c\u9886\u57df\u7279\u5b9a\u5206\u8bcd\u5668\u5728\u653e\u5c04\u5b66\u62a5\u544a\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u9886\u57df\u7279\u5b9a\u5206\u8bcd\u5668\u5728\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u6700\u4f73\uff0c\u5c24\u5176\u662f\u5728\u4ece\u5934\u8bad\u7ec3\u7684\u6a21\u578b\u4e0a\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5206\u8bcd\u5668\u5bf9\u6587\u672c\u751f\u6210\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5728\u653e\u5c04\u5b66\u9886\u57df\u7684\u5177\u4f53\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7814\u7a76\u8005\u7cfb\u7edf\u6bd4\u8f83\u4e86\u901a\u7528\u3001\u533b\u5b66\u548c\u9886\u57df\u7279\u5b9a\u5206\u8bcd\u5668\u5728\u4e09\u79cd\u5f71\u50cf\u6a21\u6001\u7684\u653e\u5c04\u5b66\u62a5\u544a\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u540c\u65f6\uff0c\u4ed6\u4eec\u4e5f\u63a2\u7a76\u4e86\u6709\u65e0PubMed\u6458\u8981\u9884\u8bad\u7ec3\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5f53\u6a21\u578b\u4ece\u5934\u8bad\u7ec3\u65f6\uff0c\u533b\u5b66\u548c\u9886\u57df\u7279\u5b9a\u8bcd\u6c47\u7684\u5206\u8bcd\u5668\u4f18\u4e8e\u5e38\u7528\u7684\u81ea\u7136\u8bed\u8a00\u5206\u8bcd\u5668\u3002\u9884\u8bad\u7ec3\u867d\u7136\u90e8\u5206\u7f13\u89e3\u4e86\u5206\u8bcd\u5668\u95f4\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4f46\u9886\u57df\u7279\u5b9a\u5206\u8bcd\u5668\u4f9d\u7136\u8868\u73b0\u6700\u4f73\u3002\u6b64\u5916\uff0c\u9886\u57df\u7279\u5b9a\u5206\u8bcd\u5668\u56e0\u8bcd\u6c47\u91cf\u66f4\u5c0f\u548c\u5e8f\u5217\u66f4\u77ed\uff0c\u8fd8\u80fd\u51cf\u5c11\u5185\u5b58\u9700\u6c42\u3002", "conclusion": "\u5c06\u8bed\u8a00\u6a21\u578b\u7684\u8bcd\u6c47\u9002\u5e94\u4e8e\u4e34\u5e8a\u9886\u57df\uff08\u5982\u653e\u5c04\u5b66\uff09\uff0c\u53ef\u4ee5\u5e26\u6765\u5b9e\u9645\u76ca\u5904\uff0c\u5305\u62ec\u6027\u80fd\u63d0\u5347\u548c\u8ba1\u7b97\u9700\u6c42\u964d\u4f4e\uff0c\u4ece\u800c\u4f7f\u8fd9\u4e9b\u6a21\u578b\u5728\u7814\u7a76\u548c\u5b9e\u9645\u533b\u7597\u573a\u666f\u4e2d\u66f4\u6613\u4e8e\u8bbf\u95ee\u548c\u4f7f\u7528\u3002"}}
{"id": "2508.09275", "pdf": "https://arxiv.org/pdf/2508.09275", "abs": "https://arxiv.org/abs/2508.09275", "authors": ["Amine Andam", "Jamal Bentahar", "Mustapha Hedabou"], "title": "Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning", "categories": ["cs.LG", "cs.MA"], "comment": "Under review in TNNLS", "summary": "Collaborative multi-agent reinforcement learning (c-MARL) has rapidly\nevolved, offering state-of-the-art algorithms for real-world applications,\nincluding sensitive domains. However, a key challenge to its widespread\nadoption is the lack of a thorough investigation into its vulnerabilities to\nadversarial attacks. Existing work predominantly focuses on training-time\nattacks or unrealistic scenarios, such as access to policy weights or the\nability to train surrogate policies. In this paper, we investigate new\nvulnerabilities under more realistic and constrained conditions, assuming an\nadversary can only collect and perturb the observations of deployed agents. We\nalso consider scenarios where the adversary has no access at all. We propose\nsimple yet highly effective algorithms for generating adversarial perturbations\ndesigned to misalign how victim agents perceive their environment. Our approach\nis empirically validated on three benchmarks and 22 environments, demonstrating\nits effectiveness across diverse algorithms and environments. Furthermore, we\nshow that our algorithm is sample-efficient, requiring only 1,000 samples\ncompared to the millions needed by previous methods.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u534f\u4f5c\u5f0f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08c-MARL\uff09\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8\u89c2\u6d4b\u6765\u8bef\u5bfc\u667a\u80fd\u4f53\u611f\u77e5\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u534f\u4f5c\u5f0f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08c-MARL\uff09\u867d\u5df2\u8fc5\u901f\u53d1\u5c55\u5e76\u5e94\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\uff0c\u4f46\u5bf9\u5176\u5bf9\u6297\u6027\u653b\u51fb\u8106\u5f31\u6027\u7684\u5168\u9762\u8c03\u67e5\u4e0d\u8db3\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u8bad\u7ec3\u65f6\u653b\u51fb\u6216\u4e0d\u73b0\u5b9e\u573a\u666f\uff0c\u800c\u7f3a\u4e4f\u5bf9\u90e8\u7f72\u667a\u80fd\u4f53\u89c2\u6d4b\u6270\u52a8\u7b49\u66f4\u771f\u5b9e\u53d7\u9650\u6761\u4ef6\u7684\u63a2\u7a76\u3002", "method": "\u5728\u5047\u8bbe\u653b\u51fb\u8005\u53ea\u80fd\u6536\u96c6\u5e76\u6270\u52a8\u5df2\u90e8\u7f72\u667a\u80fd\u4f53\u7684\u89c2\u6d4b\uff08\u6216\u5b8c\u5168\u65e0\u8bbf\u95ee\u6743\u9650\uff09\u7684\u66f4\u771f\u5b9e\u548c\u53d7\u9650\u6761\u4ef6\u4e0b\uff0c\u7814\u7a76\u65b0\u7684\u8106\u5f31\u6027\u3002\u63d0\u51fa\u7b80\u5355\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u751f\u6210\u5bf9\u6297\u6027\u6270\u52a8\uff0c\u65e8\u5728\u4f7f\u53d7\u5bb3\u667a\u80fd\u4f53\u5bf9\u5176\u73af\u5883\u7684\u611f\u77e5\u4ea7\u751f\u504f\u5dee\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u4e09\u4e2a\u57fa\u51c6\u548c22\u4e2a\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4e0d\u540c\u7b97\u6cd5\u548c\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u8be5\u7b97\u6cd5\u6837\u672c\u6548\u7387\u9ad8\uff0c\u4ec5\u97001,000\u4e2a\u6837\u672c\uff0c\u8fdc\u4f4e\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u6240\u9700\u7684\u6570\u767e\u4e07\u4e2a\u6837\u672c\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u63ed\u793a\u4e86c-MARL\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u5bf9\u89c2\u6d4b\u6270\u52a8\u7684\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u4e3ac-MARL\u7684\u5b89\u5168\u6027\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2508.09466", "pdf": "https://arxiv.org/pdf/2508.09466", "abs": "https://arxiv.org/abs/2508.09466", "authors": ["Tam Ngoc-Bang Nguyen", "Anh-Dzung Doan", "Zhipeng Cai", "Tat-Jun Chin"], "title": "Event-driven Robust Fitting on Neuromorphic Hardware", "categories": ["cs.CV", "cs.NE"], "comment": "11 pages, accepted in ICCV 2025 Workshop on Neuromorphic Vision\n  (NeVI)", "summary": "Robust fitting of geometric models is a fundamental task in many computer\nvision pipelines. Numerous innovations have been produced on the topic, from\nimproving the efficiency and accuracy of random sampling heuristics to\ngenerating novel theoretical insights that underpin new approaches with\nmathematical guarantees. However, one aspect of robust fitting that has\nreceived little attention is energy efficiency. This performance metric has\nbecome critical as high energy consumption is a growing concern for AI\nadoption. In this paper, we explore energy-efficient robust fitting via the\nneuromorphic computing paradigm. Specifically, we designed a novel spiking\nneural network for robust fitting on real neuromorphic hardware, the Intel\nLoihi 2. Enabling this are novel event-driven formulations of model estimation\nthat allow robust fitting to be implemented in the unique architecture of Loihi\n2, and algorithmic strategies to alleviate the current limited precision and\ninstruction set of the hardware. Results show that our neuromorphic robust\nfitting consumes only a fraction (15%) of the energy required to run the\nestablished robust fitting algorithm on a standard CPU to equivalent accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u7684\u9c81\u68d2\u62df\u5408\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edfCPU\u65b9\u6848\uff0c\u80fd\u6548\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u9c81\u68d2\u51e0\u4f55\u6a21\u578b\u62df\u5408\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u57fa\u7840\u4efb\u52a1\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u4f46\u5bf9\u80fd\u8017\u5173\u6ce8\u4e0d\u8db3\u3002\u968f\u7740AI\u5e94\u7528\u80fd\u8017\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u63d0\u9ad8\u9c81\u68d2\u62df\u5408\u7684\u80fd\u6548\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u4eba\u5458\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\uff0c\u5e76\u5728Intel Loihi 2\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u9c81\u68d2\u62df\u5408\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u5f00\u53d1\u4e8b\u4ef6\u9a71\u52a8\u7684\u6a21\u578b\u4f30\u8ba1\u516c\u5f0f\uff0c\u4f7f\u5176\u9002\u5e94Loihi 2\u7684\u72ec\u7279\u67b6\u6784\uff0c\u5e76\u91c7\u7528\u7b97\u6cd5\u7b56\u7565\u514b\u670d\u786c\u4ef6\u5f53\u524d\u7684\u6709\u9650\u7cbe\u5ea6\u548c\u6307\u4ee4\u96c6\u7f3a\u9677\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u795e\u7ecf\u5f62\u6001\u9c81\u68d2\u62df\u5408\u65b9\u6cd5\u5728\u8fbe\u5230\u540c\u7b49\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u80fd\u8017\u4ec5\u4e3a\u5728\u6807\u51c6CPU\u4e0a\u8fd0\u884c\u4f20\u7edf\u9c81\u68d2\u62df\u5408\u7b97\u6cd5\u768415%\u3002", "conclusion": "\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u4e3a\u9c81\u68d2\u62df\u5408\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u80fd\u6548\u4f18\u52bf\uff0c\u6709\u671b\u89e3\u51b3AI\u9886\u57df\u65e5\u76ca\u589e\u957f\u7684\u80fd\u8017\u95ee\u9898\uff0c\u4e14\u4e0d\u727a\u7272\u7cbe\u5ea6\u3002"}}
{"id": "2508.09954", "pdf": "https://arxiv.org/pdf/2508.09954", "abs": "https://arxiv.org/abs/2508.09954", "authors": ["Johannes Sch\u00e4fer", "Roman Klinger"], "title": "Shaping Event Backstories to Estimate Potential Emotion Contexts", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "May 2025 version", "summary": "Emotion analysis is an inherently ambiguous task. Previous work studied\nannotator properties to explain disagreement, but this overlooks the\npossibility that ambiguity may stem from missing information about the context\nof events. In this paper, we propose a novel approach that adds reasonable\ncontexts to event descriptions, which may better explain a particular\nsituation. Our goal is to understand whether these enriched contexts enable\nhuman annotators to annotate emotions more reliably. We disambiguate a target\nevent description by automatically generating multiple event chains conditioned\non differing emotions. By combining techniques from short story generation in\nvarious settings, we achieve coherent narratives that result in a specialized\ndataset for the first comprehensive and systematic examination of\ncontextualized emotion analysis. Through automatic and human evaluation, we\nfind that contextual narratives enhance the interpretation of specific emotions\nand support annotators in producing more consistent annotations.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u60c5\u611f\u5206\u6790\u7684\u56fa\u6709\u6a21\u7cca\u6027\uff0c\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u6dfb\u52a0\u5408\u7406\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u6545\u4e8b\u751f\u6210\u6280\u672f\u6784\u5efa\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u4e0a\u4e0b\u6587\u53d9\u4e8b\u80fd\u663e\u8457\u63d0\u9ad8\u4eba\u7c7b\u6807\u6ce8\u8005\u60c5\u611f\u5224\u65ad\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u60c5\u611f\u5206\u6790\u662f\u4e00\u9879\u56fa\u6709\u7684\u6a21\u7cca\u4efb\u52a1\u3002\u4ee5\u5f80\u7814\u7a76\u591a\u4fa7\u91cd\u4e8e\u6807\u6ce8\u8005\u5c5e\u6027\u6765\u89e3\u91ca\u5206\u6b67\uff0c\u4f46\u5ffd\u7565\u4e86\u6a21\u7cca\u6027\u53ef\u80fd\u6e90\u4e8e\u4e8b\u4ef6\u60c5\u5883\u4fe1\u606f\u7f3a\u5931\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff1a\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u57fa\u4e8e\u4e0d\u540c\u60c5\u611f\u7684\u591a\u4e2a\u4e8b\u4ef6\u94fe\uff08\u8fde\u8d2f\u53d9\u4e8b\uff09\uff0c\u4e3a\u76ee\u6807\u4e8b\u4ef6\u63cf\u8ff0\u6dfb\u52a0\u5408\u7406\u7684\u4e0a\u4e0b\u6587\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u77ed\u6545\u4e8b\u751f\u6210\u6280\u672f\uff0c\u5e76\u636e\u6b64\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u6027\u5730\u8003\u5bdf\u60c5\u5883\u5316\u60c5\u611f\u5206\u6790\u3002", "result": "\u901a\u8fc7\u81ea\u52a8\u548c\u4eba\u5de5\u8bc4\u4f30\uff0c\u7814\u7a76\u53d1\u73b0\u60c5\u5883\u5316\u53d9\u4e8b\u80fd\u591f\u589e\u5f3a\u7279\u5b9a\u60c5\u611f\u7684\u89e3\u8bfb\uff0c\u5e76\u652f\u6301\u6807\u6ce8\u8005\u4ea7\u51fa\u66f4\u4e00\u81f4\u7684\u6807\u6ce8\u7ed3\u679c\u3002", "conclusion": "\u4e3a\u4e8b\u4ef6\u63cf\u8ff0\u8865\u5145\u60c5\u5883\u4e0a\u4e0b\u6587\u80fd\u591f\u6709\u6548\u89e3\u51b3\u60c5\u611f\u5206\u6790\u4e2d\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4eba\u7c7b\u6807\u6ce8\u8005\u8fdb\u884c\u60c5\u611f\u6807\u6ce8\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.09183", "pdf": "https://arxiv.org/pdf/2508.09183", "abs": "https://arxiv.org/abs/2508.09183", "authors": ["Farzan Moosavi", "Bilal Farooq"], "title": "Quantum-Efficient Reinforcement Learning Solutions for Last-Mile On-Demand Delivery", "categories": ["quant-ph", "cs.AI", "cs.LG", "math.OC"], "comment": null, "summary": "Quantum computation has demonstrated a promising alternative to solving the\nNP-hard combinatorial problems. Specifically, when it comes to optimization,\nclassical approaches become intractable to account for large-scale solutions.\nSpecifically, we investigate quantum computing to solve the large-scale\nCapacitated Pickup and Delivery Problem with Time Windows (CPDPTW). In this\nregard, a Reinforcement Learning (RL) framework augmented with a Parametrized\nQuantum Circuit (PQC) is designed to minimize the travel time in a realistic\nlast-mile on-demand delivery. A novel problem-specific encoding quantum circuit\nwith an entangling and variational layer is proposed. Moreover, Proximal Policy\nOptimization (PPO) and Quantum Singular Value Transformation (QSVT) are\ndesigned for comparison through numerical experiments, highlighting the\nsuperiority of the proposed method in terms of the scale of the solution and\ntraining complexity while incorporating the real-world constraints.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e00\u79cd\u7ed3\u5408\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u5e26\u65f6\u95f4\u7a97\u7684\u5e26\u5bb9\u91cf\u53d6\u9001\u8d27\u95ee\u9898\uff0c\u5e76\u5728\u89e3\u51b3\u65b9\u6848\u89c4\u6a21\u548c\u8bad\u7ec3\u590d\u6742\u5ea6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "motivation": "\u7ecf\u5178\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21NP-hard\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5c24\u5176\u5728\u5927\u578bCapacitated Pickup and Delivery Problem with Time Windows (CPDPTW)\u4e2d\u9762\u4e34\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u5408\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\uff08PQC\uff09\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\uff0c\u65e8\u5728\u6700\u5c0f\u5316\u6700\u540e\u4e00\u82f1\u91cc\u914d\u9001\u7684\u65c5\u884c\u65f6\u95f4\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u95ee\u9898\u7279\u5b9a\u7f16\u7801\u91cf\u5b50\u7535\u8def\u3002\u901a\u8fc7\u4e0eProximal Policy Optimization (PPO)\u548cQuantum Singular Value Transformation (QSVT)\u8fdb\u884c\u6570\u503c\u5b9e\u9a8c\u6bd4\u8f83\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u89e3\u51b3\u65b9\u6848\u89c4\u6a21\u548c\u8bad\u7ec3\u590d\u6742\u5ea6\u65b9\u9762\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\uff0c\u5e76\u80fd\u6709\u6548\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u7ea6\u675f\u3002", "conclusion": "\u7ed3\u5408PQC\u7684RL\u6846\u67b6\u4e3a\u89e3\u51b3\u5927\u89c4\u6a21Capacitated Pickup and Delivery Problem with Time Windows (CPDPTW)\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u91cf\u5b50\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09281", "pdf": "https://arxiv.org/pdf/2508.09281", "abs": "https://arxiv.org/abs/2508.09281", "authors": ["Muntasir Hoq", "Griffin Pitts", "Andrew Lan", "Peter Brusilovsky", "Bita Akram"], "title": "Pattern-based Knowledge Component Extraction from Student Code Using Representation Learning", "categories": ["cs.LG", "K.3.2"], "comment": null, "summary": "Effective personalized learning in computer science education depends on\naccurately modeling what students know and what they need to learn. While\nKnowledge Components (KCs) provide a foundation for such modeling, automated KC\nextraction from student code is inherently challenging due to insufficient\nexplainability of discovered KCs and the open-endedness of programming problems\nwith significant structural variability across student solutions and complex\ninteractions among programming concepts. In this work, we propose a novel,\nexplainable framework for automated KC discovery through pattern-based KCs:\nrecurring structural patterns within student code that capture the specific\nprogramming patterns and language constructs that students must master. Toward\nthis, we train a Variational Autoencoder to generate important representative\npatterns from student code guided by an explainable, attention-based code\nrepresentation model that identifies important correct and incorrect pattern\nimplementations from student code. These patterns are then clustered to form\npattern-based KCs. We evaluate our KCs using two well-established methods\ninformed by Cognitive Science: learning curve analysis and Deep Knowledge\nTracing (DKT). Experimental results demonstrate meaningful learning\ntrajectories and significant improvements in DKT predictive performance over\ntraditional KT methods. This work advances knowledge modeling in CS education\nby providing an automated, scalable, and explainable framework for identifying\ngranular code patterns and algorithmic constructs, essential for student\nlearning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u6ce8\u610f\u529b\u673a\u5236\u4ece\u5b66\u751f\u4ee3\u7801\u4e2d\u53d1\u73b0\u201c\u57fa\u4e8e\u6a21\u5f0f\u7684\u77e5\u8bc6\u7ec4\u4ef6\u201d\uff08KCs\uff09\uff0c\u4ee5\u63d0\u5347\u8ba1\u7b97\u673a\u79d1\u5b66\u6559\u80b2\u4e2d\u7684\u4e2a\u6027\u5316\u5b66\u4e60\u548c\u77e5\u8bc6\u8ffd\u8e2a\u80fd\u529b\u3002", "motivation": "\u4e2a\u6027\u5316\u5b66\u4e60\u4f9d\u8d56\u4e8e\u51c6\u786e\u7684\u5b66\u751f\u77e5\u8bc6\u5efa\u6a21\u3002\u5c3d\u7ba1\u77e5\u8bc6\u7ec4\u4ef6\uff08KCs\uff09\u662f\u57fa\u7840\uff0c\u4f46\u4ece\u5b66\u751f\u4ee3\u7801\u4e2d\u81ea\u52a8\u63d0\u53d6KCs\u9762\u4e34\u6311\u6218\uff0c\u539f\u56e0\u5728\u4e8e\u5df2\u53d1\u73b0KCs\u7684\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u4ee5\u53ca\u7f16\u7a0b\u95ee\u9898\u7684\u5f00\u653e\u6027\u5bfc\u81f4\u5b66\u751f\u89e3\u51b3\u65b9\u6848\u7ed3\u6784\u591a\u6837\u4e14\u6982\u5ff5\u4ea4\u4e92\u590d\u6742\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u3001\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u5b66\u751f\u4ee3\u7801\u4e2d\u91cd\u590d\u51fa\u73b0\u7684\u7ed3\u6784\u6a21\u5f0f\uff08\u5373\u201c\u57fa\u4e8e\u6a21\u5f0f\u7684KCs\u201d\uff09\u6765\u5b9e\u73b0KCs\u7684\u81ea\u52a8\u5316\u53d1\u73b0\u3002\u5177\u4f53\u65b9\u6cd5\u662f\uff1a\u8bad\u7ec3\u4e00\u4e2a\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\uff0c\u5728\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u3001\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u4ee3\u7801\u8868\u793a\u6a21\u578b\u7684\u5f15\u5bfc\u4e0b\uff0c\u4ece\u5b66\u751f\u4ee3\u7801\u4e2d\u751f\u6210\u91cd\u8981\u7684\u4ee3\u8868\u6027\u6a21\u5f0f\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u6b63\u786e\u548c\u4e0d\u6b63\u786e\u7684\u6a21\u5f0f\u5b9e\u73b0\u3002\u8fd9\u4e9b\u6a21\u5f0f\u968f\u540e\u88ab\u805a\u7c7b\u4ee5\u5f62\u6210\u57fa\u4e8e\u6a21\u5f0f\u7684KCs\u3002\u8bc4\u4f30\u65b9\u6cd5\u91c7\u7528\u5b66\u4e60\u66f2\u7ebf\u5206\u6790\u548c\u6df1\u5ea6\u77e5\u8bc6\u8ffd\u8e2a\uff08DKT\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684KCs\u80fd\u591f\u5c55\u793a\u6709\u610f\u4e49\u7684\u5b66\u4e60\u8f68\u8ff9\uff0c\u5e76\u4e14\u76f8\u6bd4\u4f20\u7edf\u77e5\u8bc6\u8ffd\u8e2a\u65b9\u6cd5\uff0c\u5728\u6df1\u5ea6\u77e5\u8bc6\u8ffd\u8e2a\uff08DKT\uff09\u7684\u9884\u6d4b\u6027\u80fd\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8ba1\u7b97\u673a\u79d1\u5b66\u6559\u80b2\u4e2d\u7684\u77e5\u8bc6\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u7ec6\u7c92\u5ea6\u7684\u4ee3\u7801\u6a21\u5f0f\u548c\u7b97\u6cd5\u7ed3\u6784\uff0c\u8fd9\u5bf9\u5b66\u751f\u7684\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u4ece\u800c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.09470", "pdf": "https://arxiv.org/pdf/2508.09470", "abs": "https://arxiv.org/abs/2508.09470", "authors": ["Jialei Xu", "Zizhuang Wei", "Weikang You", "Linyun Li", "Weijian Sun"], "title": "CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Semantic segmentation of city-scale point clouds is a critical technology for\nUnmanned Aerial Vehicle (UAV) perception systems, enabling the classification\nof 3D points without relying on any visual information to achieve comprehensive\n3D understanding. However, existing models are frequently constrained by the\nlimited scale of 3D data and the domain gap between datasets, which lead to\nreduced generalization capability. To address these challenges, we propose\nCitySeg, a foundation model for city-scale point cloud semantic segmentation\nthat incorporates text modality to achieve open vocabulary segmentation and\nzero-shot inference. Specifically, in order to mitigate the issue of\nnon-uniform data distribution across multiple domains, we customize the data\npreprocessing rules, and propose a local-global cross-attention network to\nenhance the perception capabilities of point networks in UAV scenarios. To\nresolve semantic label discrepancies across datasets, we introduce a\nhierarchical classification strategy. A hierarchical graph established\naccording to the data annotation rules consolidates the data labels, and the\ngraph encoder is used to model the hierarchical relationships between\ncategories. In addition, we propose a two-stage training strategy and employ\nhinge loss to increase the feature separability of subcategories. Experimental\nresults demonstrate that the proposed CitySeg achieves state-of-the-art (SOTA)\nperformance on nine closed-set benchmarks, significantly outperforming existing\napproaches. Moreover, for the first time, CitySeg enables zero-shot\ngeneralization in city-scale point cloud scenarios without relying on visual\ninformation.", "AI": {"tldr": "CitySeg\u662f\u4e00\u4e2a\u9762\u5411\u57ce\u5e02\u7ea7\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u7684\u5f00\u521b\u6027\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u6587\u672c\u6a21\u6001\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u57df\u5dee\u8ddd\u5927\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5f00\u653e\u8bcd\u6c47\u548c\u96f6\u6837\u672c\u63a8\u7406\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57ce\u5e02\u7ea7\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u6a21\u578b\u53d7\u9650\u4e8e3D\u6570\u636e\u89c4\u6a21\u548c\u6570\u636e\u96c6\u95f4\u7684\u57df\u5dee\u8ddd\uff0c\u5bfc\u81f4\u5176\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5168\u9762\u76843D\u7406\u89e3\u3002", "method": "\u63d0\u51faCitySeg\u57fa\u7840\u6a21\u578b\uff0c\u878d\u5408\u6587\u672c\u6a21\u6001\u4ee5\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u548c\u96f6\u6837\u672c\u63a8\u7406\u3002\u5177\u4f53\u7b56\u7565\u5305\u62ec\uff1a\u5b9a\u5236\u5316\u6570\u636e\u9884\u5904\u7406\u89c4\u5219\u4ee5\u5e94\u5bf9\u6570\u636e\u5206\u5e03\u4e0d\u5747\uff1b\u63d0\u51fa\u5c40\u90e8-\u5168\u5c40\u4ea4\u53c9\u6ce8\u610f\u529b\u7f51\u7edc\u589e\u5f3a\u611f\u77e5\u80fd\u529b\uff1b\u5f15\u5165\u5206\u5c42\u5206\u7c7b\u7b56\u7565\u548c\u56fe\u7f16\u7801\u5668\u89e3\u51b3\u6807\u7b7e\u5dee\u5f02\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u548chinge loss\u63d0\u5347\u5b50\u7c7b\u522b\u7279\u5f81\u53ef\u5206\u6027\u3002", "result": "CitySeg\u5728\u4e5d\u4e2a\u5c01\u95ed\u96c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u9996\u6b21\u5728\u57ce\u5e02\u89c4\u6a21\u70b9\u4e91\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u4e0d\u4f9d\u8d56\u89c6\u89c9\u4fe1\u606f\u7684\u96f6\u6837\u672c\u6cdb\u5316\u3002", "conclusion": "CitySeg\u901a\u8fc7\u5176\u521b\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57ce\u5e02\u7ea7\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u96f6\u6837\u672c\u63a8\u7406\u80fd\u529b\uff0c\u4e3aUAV\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2508.09956", "pdf": "https://arxiv.org/pdf/2508.09956", "abs": "https://arxiv.org/abs/2508.09956", "authors": ["Fares Antaki", "David Mikhail", "Daniel Milad", "Danny A Mammo", "Sumit Sharma", "Sunil K Srivastava", "Bing Yu Chen", "Samir Touma", "Mertcan Sevgi", "Jonathan El-Khoury", "Pearse A Keane", "Qingyu Chen", "Yih Chung Tham", "Renaud Duval"], "title": "Performance of GPT-5 Frontier Models in Ophthalmology Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) such as GPT-5 integrate advanced reasoning\ncapabilities that may improve performance on complex medical question-answering\ntasks. For this latest generation of reasoning models, the configurations that\nmaximize both accuracy and cost-efficiency have yet to be established. We\nevaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across\nfour reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using\n260 closed-access multiple-choice questions from the American Academy of\nOphthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome\nwas multiple-choice accuracy; secondary outcomes included head-to-head ranking\nvia a Bradley-Terry model, rationale quality assessment using a\nreference-anchored, pairwise LLM-as-a-judge framework, and analysis of\naccuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved\nthe highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano\nvariants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high\n(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x\nstronger than o3-high) and rationale quality (1.11x stronger than o3-high).\nCost-accuracy analysis identified several GPT-5 configurations on the Pareto\nfrontier, with GPT-5-mini-low offering the most favorable low-cost,\nhigh-performance balance. These results benchmark GPT-5 on a high-quality\nophthalmology dataset, demonstrate the influence of reasoning effort on\naccuracy, and introduce an autograder framework for scalable evaluation of\nLLM-generated answers against reference standards in ophthalmology.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86GPT-5\u7cfb\u5217\u6a21\u578b\u5728\u773c\u79d1\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-5-high\u5728\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u800cGPT-5-mini-low\u5219\u63d0\u4f9b\u4e86\u6700\u4f73\u7684\u6210\u672c\u6548\u76ca\u5e73\u8861\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982GPT-5\u5c55\u73b0\u51fa\u5148\u8fdb\u7684\u63a8\u7406\u80fd\u529b\uff0c\u6709\u671b\u63d0\u9ad8\u590d\u6742\u533b\u5b66\u95ee\u7b54\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u6700\u65b0\u4e00\u4ee3\u7684\u63a8\u7406\u6a21\u578b\uff0c\u5982\u4f55\u914d\u7f6e\u4ee5\u6700\u5927\u5316\u51c6\u786e\u6027\u548c\u6210\u672c\u6548\u76ca\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86OpenAI GPT-5\u7cfb\u5217\u768412\u79cd\u914d\u7f6e\uff08\u4e09\u4e2a\u6a21\u578b\u5c42\u7ea7\u548c\u56db\u79cd\u63a8\u7406\u52aa\u529b\u8bbe\u7f6e\uff09\uff0c\u4ee5\u53cao1-high\u3001o3-high\u548cGPT-4o\u3002\u4f7f\u7528\u4e86\u7f8e\u56fd\u773c\u79d1\u5b66\u4f1a\u57fa\u7840\u4e34\u5e8a\u79d1\u5b66\u8bfe\u7a0b\uff08BCSC\uff09\u6570\u636e\u96c6\u4e2d260\u4e2a\u591a\u9879\u9009\u62e9\u9898\u3002\u4e3b\u8981\u8bc4\u4f30\u6307\u6807\u662f\u591a\u9879\u9009\u62e9\u9898\u7684\u51c6\u786e\u6027\uff0c\u6b21\u8981\u6307\u6807\u5305\u62ec\u901a\u8fc7Bradley-Terry\u6a21\u578b\u8fdb\u884c\u7684\u5934\u5bf9\u5934\u6392\u540d\u3001\u4f7f\u7528\u53c2\u8003\u951a\u5b9a\u7684LLM-as-a-judge\u6846\u67b6\u8fdb\u884c\u7684\u63a8\u7406\u8d28\u91cf\u8bc4\u4f30\uff0c\u4ee5\u53ca\u57fa\u4e8etoken\u6210\u672c\u4f30\u7b97\u7684\u51c6\u786e\u6027-\u6210\u672c\u6743\u8861\u5206\u6790\u3002", "result": "GPT-5-high\u53d6\u5f97\u4e86\u6700\u9ad8\u51c6\u786e\u7387\uff080.965\uff09\uff0c\u4f18\u4e8e\u6240\u6709GPT-5-nano\u53d8\u4f53\u3001o1-high\u548cGPT-4o\uff0c\u4f46\u4e0eo3-high\uff080.958\uff09\u65e0\u663e\u8457\u5dee\u5f02\u3002GPT-5-high\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u8d28\u91cf\u65b9\u9762\u5747\u6392\u540d\u7b2c\u4e00\uff08\u5206\u522b\u6bd4o3-high\u5f3a1.66\u500d\u548c1.11\u500d\uff09\u3002\u6210\u672c-\u51c6\u786e\u6027\u5206\u6790\u8bc6\u522b\u51fa\u591a\u4e2aGPT-5\u914d\u7f6e\u5728\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5176\u4e2dGPT-5-mini-low\u63d0\u4f9b\u4e86\u6700\u6709\u5229\u7684\u4f4e\u6210\u672c\u3001\u9ad8\u6027\u80fd\u5e73\u8861\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5bf9GPT-5\u5728\u9ad8\u8d28\u91cf\u773c\u79d1\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u63a8\u7406\u52aa\u529b\u5bf9\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u52a8\u8bc4\u5206\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u6269\u5c55\u5730\u8bc4\u4f30LLM\u5728\u773c\u79d1\u9886\u57df\u751f\u6210\u7b54\u6848\u4e0e\u53c2\u8003\u6807\u51c6\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.09283", "pdf": "https://arxiv.org/pdf/2508.09283", "abs": "https://arxiv.org/abs/2508.09283", "authors": ["Connor Wilhelm", "Dan Ventura"], "title": "Distilling Reinforcement Learning into Single-Batch Datasets", "categories": ["cs.LG"], "comment": "to be published in ECAI 2025 (appendix in arXiv version only), 11\n  pages (7 content, 4 appendix), 6 figures", "summary": "Dataset distillation compresses a large dataset into a small synthetic\ndataset such that learning on the synthetic dataset approximates learning on\nthe original. Training on the distilled dataset can be performed in as little\nas one step of gradient descent. We demonstrate that distillation is\ngeneralizable to different tasks by distilling reinforcement learning\nenvironments into one-batch supervised learning datasets. This demonstrates not\nonly distillation's ability to compress a reinforcement learning task but also\nits ability to transform one learning modality (reinforcement learning) into\nanother (supervised learning). We present a novel extension of proximal policy\noptimization for meta-learning and use it in distillation of a\nmulti-dimensional extension of the classic cart-pole problem, all MuJoCo\nenvironments, and several Atari games. We demonstrate distillation's ability to\ncompress complex RL environments into one-step supervised learning, explore RL\ndistillation's generalizability across learner architectures, and demonstrate\ndistilling an environment into the smallest-possible synthetic dataset.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u6570\u636e\u96c6\u84b8\u998f\u5e94\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u5c06\u5176\u538b\u7f29\u4e3a\u5355\u6279\u6b21\u76d1\u7763\u5b66\u4e60\u6570\u636e\u96c6\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u5e76\u5c55\u793a\u4e86\u8de8\u6a21\u6001\u8f6c\u6362\u80fd\u529b\u3002", "motivation": "\u5c06\u5927\u578b\u6570\u636e\u96c6\u538b\u7f29\u4e3a\u5c0f\u578b\u5408\u6210\u6570\u636e\u96c6\u4ee5\u8fd1\u4f3c\u539f\u59cb\u6570\u636e\u96c6\u4e0a\u7684\u5b66\u4e60\uff0c\u5e76\u63a2\u7d22\u6570\u636e\u96c6\u84b8\u998f\u5728\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u901a\u7528\u6027\u53ca\u5176\u5c06\u5f3a\u5316\u5b66\u4e60\u8f6c\u6362\u4e3a\u76d1\u7763\u5b66\u4e60\u7684\u80fd\u529b\u3002", "method": "\u5c06\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u84b8\u998f\u6210\u5355\u6279\u6b21\u76d1\u7763\u5b66\u4e60\u6570\u636e\u96c6\u3002\u63d0\u51fa\u4e86\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7684\u5143\u5b66\u4e60\u6269\u5c55\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u591a\u7ef4\u5012\u7acb\u6446\u95ee\u9898\u3001MuJoCo\u73af\u5883\u548cAtari\u6e38\u620f\u3002", "result": "\u6210\u529f\u5c06\u590d\u6742\u7684\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u538b\u7f29\u4e3a\u4e00\u6b65\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\uff0c\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u84b8\u998f\u5bf9\u4e0d\u540c\u5b66\u4e60\u5668\u67b6\u6784\u7684\u901a\u7528\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u73af\u5883\u5230\u6700\u5c0f\u5408\u6210\u6570\u636e\u96c6\u7684\u84b8\u998f\u3002", "conclusion": "\u6570\u636e\u96c6\u84b8\u998f\u80fd\u591f\u6709\u6548\u538b\u7f29\u590d\u6742\u7684\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u9ad8\u6548\u7684\u76d1\u7763\u5b66\u4e60\u5f62\u5f0f\uff0c\u5373\u4fbf\u4f7f\u7528\u6781\u5c0f\u7684\u5408\u6210\u6570\u636e\u96c6\u4e5f\u80fd\u5b9e\u73b0\u5feb\u901f\u5b66\u4e60\u3002"}}
{"id": "2508.09475", "pdf": "https://arxiv.org/pdf/2508.09475", "abs": "https://arxiv.org/abs/2508.09475", "authors": ["Shibo Yao", "Renshuai Tao", "Xiaolong Zheng", "Chao Liang", "Chunjie Zhang"], "title": "Leveraging Failed Samples: A Few-Shot and Training-Free Framework for Generalized Deepfake Detection", "categories": ["cs.CV"], "comment": null, "summary": "Recent deepfake detection studies often treat unseen sample detection as a\n``zero-shot\" task, training on images generated by known models but\ngeneralizing to unknown ones. A key real-world challenge arises when a model\nperforms poorly on unknown samples, yet these samples remain available for\nanalysis. This highlights that it should be approached as a ``few-shot\" task,\nwhere effectively utilizing a small number of samples can lead to significant\nimprovement. Unlike typical few-shot tasks focused on semantic understanding,\ndeepfake detection prioritizes image realism, which closely mirrors real-world\ndistributions. In this work, we propose the Few-shot Training-free Network\n(FTNet) for real-world few-shot deepfake detection. Simple yet effective, FTNet\ndiffers from traditional methods that rely on large-scale known data for\ntraining. Instead, FTNet uses only one fake samplefrom an evaluation set,\nmimicking the scenario where new samples emerge in the real world and can be\ngathered for use, without any training or parameter updates. During evaluation,\neach test sample is compared to the known fake and real samples, and it is\nclassified based on the category of the nearest sample. We conduct a\ncomprehensive analysis of AI-generated images from 29 different generative\nmodels and achieve a new SoTA performance, with an average improvement of 8.7\\%\ncompared to existing methods. This work introduces a fresh perspective on\nreal-world deepfake detection: when the model struggles to generalize on a\nfew-shot sample, leveraging the failed samples leads to better performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFTNet\uff0c\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u5c11\u6837\u672c\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7f51\u7edc\u3002\u5b83\u5229\u7528\u4e00\u4e2a\u672a\u77e5\u4f2a\u9020\u6837\u672c\u8fdb\u884c\u6bd4\u8f83\uff0c\u5728\u4e0d\u4f9d\u8d56\u4f20\u7edf\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u9488\u5bf9\u591a\u79cdAI\u751f\u6210\u56fe\u50cf\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5c06\u672a\u77e5\u6837\u672c\u89c6\u4e3a\u201c\u96f6\u6837\u672c\u201d\u4efb\u52a1\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0c\u5f53\u6a21\u578b\u5bf9\u672a\u77e5\u6837\u672c\u8868\u73b0\u4e0d\u4f73\u65f6\uff0c\u8fd9\u4e9b\u6837\u672c\u4ecd\u53ef\u7528\u4e8e\u5206\u6790\u3002\u8fd9\u8868\u660e\u5b83\u5e94\u88ab\u89c6\u4e3a\u201c\u5c11\u6837\u672c\u201d\u4efb\u52a1\uff0c\u5373\u6709\u6548\u5229\u7528\u5c11\u91cf\u6837\u672c\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002\u4e0e\u4fa7\u91cd\u8bed\u4e49\u7406\u89e3\u7684\u5178\u578b\u5c11\u6837\u672c\u4efb\u52a1\u4e0d\u540c\uff0c\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u66f4\u6ce8\u91cd\u56fe\u50cf\u771f\u5b9e\u6027\uff0c\u8fd9\u4e0e\u73b0\u5b9e\u4e16\u754c\u5206\u5e03\u7d27\u5bc6\u76f8\u5173\u3002", "method": "\u63d0\u51fa\u4e86Few-shot Training-free Network (FTNet)\u3002\u4e0e\u4f9d\u8d56\u5927\u91cf\u5df2\u77e5\u6570\u636e\u8bad\u7ec3\u7684\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0cFTNet\u4ec5\u4f7f\u7528\u6765\u81ea\u8bc4\u4f30\u96c6\u7684\u4e00\u4e2a\u4f2a\u9020\u6837\u672c\uff08\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u4e2d\u65b0\u6837\u672c\u51fa\u73b0\u7684\u60c5\u666f\uff09\uff0c\u65e0\u9700\u4efb\u4f55\u8bad\u7ec3\u6216\u53c2\u6570\u66f4\u65b0\u3002\u5728\u8bc4\u4f30\u65f6\uff0c\u6bcf\u4e2a\u6d4b\u8bd5\u6837\u672c\u4e0e\u5df2\u77e5\u4f2a\u9020\u548c\u771f\u5b9e\u6837\u672c\u8fdb\u884c\u6bd4\u8f83\uff0c\u5e76\u6839\u636e\u6700\u8fd1\u6837\u672c\u7684\u7c7b\u522b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5bf9\u6765\u81ea29\u79cd\u4e0d\u540c\u751f\u6210\u6a21\u578b\u7684AI\u751f\u6210\u56fe\u50cf\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\uff0c\u53d6\u5f97\u4e86\u65b0\u7684SoTA\u6027\u80fd\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747\u63d0\u5347\u4e868.7%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u73b0\u5b9e\u4e16\u754c\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u5f15\u5165\u4e86\u65b0\u89c6\u89d2\uff1a\u5f53\u6a21\u578b\u96be\u4ee5\u5728\u5c11\u6837\u672c\u4e0a\u6cdb\u5316\u65f6\uff0c\u5229\u7528\u8fd9\u4e9b\u5931\u8d25\u7684\u6837\u672c\u53ef\u4ee5\u5e26\u6765\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.09957", "pdf": "https://arxiv.org/pdf/2508.09957", "abs": "https://arxiv.org/abs/2508.09957", "authors": ["Renas Adnan", "Hossein Hassani"], "title": "Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)", "categories": ["cs.CL"], "comment": "21 pages, 20 figures, 7 tables", "summary": "Speech-to-text (STT) systems have a wide range of applications. They are\navailable in many languages, albeit at different quality levels. Although\nKurdish is considered a less-resourced language from a processing perspective,\nSST is available for some of the Kurdish dialects, for instance, Sorani\n(Central Kurdish). However, that is not applied to other Kurdish dialects,\nBadini and Hawrami, for example. This research is an attempt to address this\ngap. Bandin, approximately, has two million speakers, and STT systems can help\ntheir community use mobile and computer-based technologies while giving their\ndialect more global visibility. We aim to create a language model based on\nBadini's speech and evaluate its performance. To cover a conversational aspect,\nhave a proper confidence level of grammatical accuracy, and ready\ntranscriptions, we chose Badini kids' stories, eight books including 78\nstories, as the textual input. Six narrators narrated the books, which resulted\nin approximately 17 hours of recording. We cleaned, segmented, and tokenized\nthe input. The preprocessing produced nearly 15 hours of speech, including\n19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and\nWhisper-small to develop the language models. The experiments indicate that the\ntranscriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a\nsignificantly more accurate and readable output than the Whisper-small model,\nwith 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,\nrespectively.", "AI": {"tldr": "\u4e3a\u8d44\u6e90\u532e\u4e4f\u7684Badini\u5e93\u5c14\u5fb7\u8bed\u65b9\u8a00\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u8bed\u97f3\u8f6c\u6587\u672c\uff08STT\uff09\u7cfb\u7edf\uff0c\u53d1\u73b0\u57fa\u4e8eWav2Vec2-Large-XLSR-53\u7684\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u53ef\u8bfb\u6027\u65b9\u9762\u8868\u73b0\u663e\u8457\u4f18\u4e8eWhisper-small\u3002", "motivation": "\u867d\u7136\u8bed\u97f3\u8f6c\u6587\u672c\u7cfb\u7edf\u5728\u591a\u79cd\u8bed\u8a00\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5bf9\u4e8eBadini\u7b49\u8d44\u6e90\u532e\u4e4f\u7684\u5e93\u5c14\u5fb7\u8bed\u65b9\u8a00\uff0c\u76f8\u5173\u7cfb\u7edf\u4ecd\u662f\u7a7a\u767d\u3002\u9274\u4e8eBadini\u65b9\u8a00\u7ea6\u6709\u4e24\u767e\u4e07\u4f7f\u7528\u8005\uff0c\u5f00\u53d1STT\u7cfb\u7edf\u80fd\u5e2e\u52a9\u5176\u793e\u533a\u4f7f\u7528\u79fb\u52a8\u548c\u8ba1\u7b97\u673a\u6280\u672f\uff0c\u5e76\u63d0\u5347\u8be5\u65b9\u8a00\u7684\u5168\u7403\u53ef\u89c1\u5ea6\u3002", "method": "\u7814\u7a76\u9009\u62e9Badini\u513f\u7ae5\u6545\u4e8b\u96c6\uff088\u672c\u4e66\uff0c78\u4e2a\u6545\u4e8b\uff0c\u7ea617\u5c0f\u65f6\u5f55\u97f3\uff09\u4f5c\u4e3a\u6587\u672c\u8f93\u5165\uff0c\u7531\u516d\u4f4d\u53d9\u8ff0\u8005\u5f55\u5236\u3002\u6570\u636e\u7ecf\u8fc7\u6e05\u6d17\u3001\u5206\u6bb5\u548c\u6807\u8bb0\u5316\u9884\u5904\u7406\uff0c\u5f97\u5230\u7ea615\u5c0f\u65f6\u7684\u8bed\u97f3\u6570\u636e\u3002\u4f7f\u7528Wav2Vec2-Large-XLSR-53\u548cWhisper-small\u6a21\u578b\u5f00\u53d1\u8bed\u8a00\u6a21\u578b\u5e76\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8eWav2Vec2-Large-XLSR-53\u6a21\u578b\u7684\u8f6c\u5f55\u8fc7\u7a0b\u63d0\u4f9b\u66f4\u51c6\u786e\u548c\u53ef\u8bfb\u7684\u8f93\u51fa\u3002Wav2Vec2-Large-XLSR-53\u6a21\u578b\u7684\u53ef\u8bfb\u6027\u8fbe\u523090.38%\uff0c\u51c6\u786e\u7387\u4e3a82.67%\uff1b\u800cWhisper-small\u6a21\u578b\u7684\u53ef\u8bfb\u6027\u4e3a65.45%\uff0c\u51c6\u786e\u7387\u4e3a53.17%\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u586b\u8865\u4e86Badini\u5e93\u5c14\u5fb7\u8bed\u65b9\u8a00\u5728\u8bed\u97f3\u8f6c\u6587\u672c\u7cfb\u7edf\u65b9\u9762\u7684\u7a7a\u767d\u3002Wav2Vec2-Large-XLSR-53\u6a21\u578b\u88ab\u8bc1\u660e\u662f\u4e3aBadini\u8bed\u97f3\u521b\u5efa\u9ad8\u6548\u548c\u9ad8\u7cbe\u5ea6\u8bed\u8a00\u6a21\u578b\u7684\u66f4\u4f18\u9009\u62e9\uff0c\u663e\u8457\u4f18\u4e8eWhisper-small\u6a21\u578b\uff0c\u4e3aBadini\u793e\u533a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684STT\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09299", "pdf": "https://arxiv.org/pdf/2508.09299", "abs": "https://arxiv.org/abs/2508.09299", "authors": ["Rilwan Umar", "Aydin Abadi", "Basil Aldali", "Benito Vincent", "Elliot A. J. Hurley", "Hotoon Aljazaeri", "Jamie Hedley-Cook", "Jamie-Lee Bell", "Lambert Uwuigbusun", "Mujeeb Ahmed", "Shishir Nagaraja", "Suleiman Sabo", "Weaam Alrbeiqi"], "title": "Decentralized Weather Forecasting via Distributed Machine Learning and Blockchain-Based Model Validation", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Weather forecasting plays a vital role in disaster preparedness, agriculture,\nand resource management, yet current centralized forecasting systems are\nincreasingly strained by security vulnerabilities, limited scalability, and\nsusceptibility to single points of failure. To address these challenges, we\npropose a decentralized weather forecasting framework that integrates Federated\nLearning (FL) with blockchain technology. FL enables collaborative model\ntraining without exposing sensitive local data; this approach enhances privacy\nand reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures\ntransparent and dependable verification of model updates. To further enhance\nthe system's security, we introduce a reputation-based voting mechanism that\nassesses the trustworthiness of submitted models while utilizing the\nInterplanetary File System (IPFS) for efficient off-chain storage. Experimental\nresults demonstrate that our approach not only improves forecasting accuracy\nbut also enhances system resilience and scalability, making it a viable\ncandidate for deployment in real-world, security-critical environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u3001\u533a\u5757\u94fe\uff08\u4ee5\u592a\u574a\uff09\u548cIPFS\u7684\u53bb\u4e2d\u5fc3\u5316\u5929\u6c14\u9884\u62a5\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u8a89\u6295\u7968\u673a\u5236\u589e\u5f3a\u5b89\u5168\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3001\u7cfb\u7edf\u97e7\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5f53\u524d\u4e2d\u5fc3\u5316\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\u9762\u4e34\u5b89\u5168\u6f0f\u6d1e\u3001\u53ef\u6269\u5c55\u6027\u6709\u9650\u53ca\u5355\u70b9\u6545\u969c\u7684\u6311\u6218\uff0c\u800c\u5929\u6c14\u9884\u62a5\u5bf9\u707e\u5bb3\u5e94\u5bf9\u3001\u519c\u4e1a\u548c\u8d44\u6e90\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u5929\u6c14\u9884\u62a5\u6846\u67b6\uff0c\u6574\u5408\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4ee5\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u5e76\u964d\u4f4e\u4f20\u8f93\u5f00\u9500\uff0c\u5229\u7528\u4ee5\u592a\u574a\u533a\u5757\u94fe\u786e\u4fdd\u6a21\u578b\u66f4\u65b0\u7684\u900f\u660e\u548c\u53ef\u9760\u9a8c\u8bc1\uff0c\u5f15\u5165\u57fa\u4e8e\u4fe1\u8a89\u7684\u6295\u7968\u673a\u5236\u8bc4\u4f30\u6a21\u578b\u53ef\u4fe1\u5ea6\uff0c\u5e76\u4f7f\u7528IPFS\u8fdb\u884c\u9ad8\u6548\u7684\u94fe\u4e0b\u5b58\u50a8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5929\u6c14\u9884\u62a5\u7684\u51c6\u786e\u6027\uff0c\u8fd8\u663e\u8457\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u97e7\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\u4e3a\u5728\u5b9e\u9645\u3001\u5b89\u5168\u5173\u952e\u73af\u5883\u4e2d\u90e8\u7f72\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09476", "pdf": "https://arxiv.org/pdf/2508.09476", "abs": "https://arxiv.org/abs/2508.09476", "authors": ["Yuji Wang", "Moran Li", "Xiaobin Hu", "Ran Yi", "Jiangning Zhang", "Chengming Xu", "Weijian Cao", "Yabiao Wang", "Chengjie Wang", "Lizhuang Ma"], "title": "From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts", "categories": ["cs.CV"], "comment": null, "summary": "Current video generation models struggle with identity preservation under\nlarge facial angles, primarily facing two challenges: the difficulty in\nexploring an effective mechanism to integrate identity features into DiT\nstructure, and the lack of targeted coverage of large facial angles in existing\nopen-source video datasets. To address these, we present two key innovations.\nFirst, we introduce a Mixture of Facial Experts (MoFE) that dynamically\ncombines complementary cues from three specialized experts, each designed to\ncapture distinct but mutually reinforcing aspects of facial attributes. The\nidentity expert captures cross-pose identity-sensitive features, the semantic\nexpert extracts high-level visual semantxics, and the detail expert preserves\npixel-level features (e.g., skin texture, color gradients). Furthermore, to\nmitigate dataset limitations, we have tailored a data processing pipeline\ncentered on two key aspects: Face Constraints and Identity Consistency. Face\nConstraints ensure facial angle diversity and a high proportion of facial\nregions, while Identity Consistency preserves coherent person-specific features\nacross temporal sequences, collectively addressing the scarcity of large facial\nangles and identity-stable training data in existing datasets. Leveraging this\npipeline, we have curated and refined a Large Face Angles (LFA) Dataset from\nexisting open-source human video datasets, comprising 460K video clips with\nannotated facial angles. Experimental results on the LFA benchmark demonstrate\nthat our method, empowered by the LFA dataset, significantly outperforms prior\nSOTA methods in face similarity, face FID, and CLIP semantic alignment. The\ncode and dataset will be made publicly available at\nhttps://github.com/rain152/LFA-Video-Generation.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u9762\u90e8\u4e13\u5bb6\u6df7\u5408\uff08MoFE\uff09\u6a21\u578b\u548c\u6784\u5efa\u5927\u578b\u9762\u90e8\u89d2\u5ea6\uff08LFA\uff09\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u5728\u5927\u5e45\u9762\u90e8\u89d2\u5ea6\u4e0b\u7684\u4eba\u7269\u8eab\u4efd\u4fdd\u6301\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u5927\u5e45\u9762\u90e8\u89d2\u5ea6\u4e0b\u96be\u4ee5\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u96be\u4ee5\u5c06\u8eab\u4efd\u7279\u5f81\u6709\u6548\u6574\u5408\u5230DiT\u7ed3\u6784\u4e2d\uff0c\u4ee5\u53ca\u73b0\u6709\u5f00\u6e90\u89c6\u9891\u6570\u636e\u96c6\u4e2d\u7f3a\u4e4f\u9488\u5bf9\u5927\u5e45\u9762\u90e8\u89d2\u5ea6\u7684\u8986\u76d6\u3002", "method": "1. \u63d0\u51fa\u9762\u90e8\u4e13\u5bb6\u6df7\u5408\uff08MoFE\uff09\u6a21\u578b\uff0c\u52a8\u6001\u7ed3\u5408\u8eab\u4efd\u4e13\u5bb6\u3001\u8bed\u4e49\u4e13\u5bb6\u548c\u7ec6\u8282\u4e13\u5bb6\u4ee5\u6355\u83b7\u5168\u9762\u9762\u90e8\u7279\u5f81\u30022. \u8bbe\u8ba1\u5e76\u5e94\u7528\u4ee5\u9762\u90e8\u7ea6\u675f\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u4e3a\u6838\u5fc3\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u5927\u5e45\u9762\u90e8\u89d2\u5ea6\u548c\u8eab\u4efd\u7a33\u5b9a\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u95ee\u9898\u30023. \u57fa\u4e8e\u6b64\u6d41\u7a0b\uff0c\u6784\u5efa\u4e86\u5305\u542b46\u4e07\u4e2a\u89c6\u9891\u7247\u6bb5\u548c\u9762\u90e8\u89d2\u5ea6\u6807\u6ce8\u7684\u5927\u578b\u9762\u90e8\u89d2\u5ea6\uff08LFA\uff09\u6570\u636e\u96c6\u3002", "result": "\u5728LFA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7ed3\u5408LFA\u6570\u636e\u96c6\uff0c\u5728\u9762\u90e8\u76f8\u4f3c\u6027\u3001\u9762\u90e8FID\u548cCLIP\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\uff08SOTA\uff09\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u7684MoFE\u6a21\u578b\u548c\u4e13\u95e8\u6784\u5efa\u7684LFA\u6570\u636e\u96c6\uff0c\u672c\u7814\u7a76\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u4e2d\u5927\u5e45\u9762\u90e8\u89d2\u5ea6\u4e0b\u8eab\u4efd\u4fdd\u6301\u7684\u6311\u6218\uff0c\u5e76\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2508.09958", "pdf": "https://arxiv.org/pdf/2508.09958", "abs": "https://arxiv.org/abs/2508.09958", "authors": ["Baran Atalar", "Eddie Zhang", "Carlee Joe-Wong"], "title": "Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks", "categories": ["cs.CL", "cs.LG"], "comment": "Submitted to AAAI 2026", "summary": "With the increasing popularity of large language models (LLMs) for a variety\nof tasks, there has been a growing interest in strategies that can predict\nwhich out of a set of LLMs will yield a successful answer at low cost. This\nproblem promises to become more and more relevant as providers like Microsoft\nallow users to easily create custom LLM \"assistants\" specialized to particular\ntypes of queries. However, some tasks (i.e., queries) may be too specialized\nand difficult for a single LLM to handle alone. These applications often\nbenefit from breaking down the task into smaller subtasks, each of which can\nthen be executed by a LLM expected to perform well on that specific subtask.\nFor example, in extracting a diagnosis from medical records, one can first\nselect an LLM to summarize the record, select another to validate the summary,\nand then select another, possibly different, LLM to extract the diagnosis from\nthe summarized record. Unlike existing LLM selection or routing algorithms,\nthis setting requires that we select a sequence of LLMs, with the output of\neach LLM feeding into the next and potentially influencing its success. Thus,\nunlike single LLM selection, the quality of each subtask's output directly\naffects the inputs, and hence the cost and success rate, of downstream LLMs,\ncreating complex performance dependencies that must be learned and accounted\nfor during selection. We propose a neural contextual bandit-based algorithm\nthat trains neural networks that model LLM success on each subtask in an online\nmanner, thus learning to guide the LLM selections for the different subtasks,\neven in the absence of historical LLM performance data. Experiments on\ntelecommunications question answering and medical diagnosis prediction datasets\nillustrate the effectiveness of our proposed approach compared to other LLM\nselection algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u4e0a\u4e0b\u6587\u591a\u81c2\u8d4c\u535a\u673a\uff08neural contextual bandit\uff09\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u9009\u62e9\u4e00\u7cfb\u5217\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u89e3\u51b3\u9700\u8981\u5206\u89e3\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\u7684\u590d\u6742\u67e5\u8be2\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u5386\u53f2\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u4ee5\u4f18\u5316\u6027\u80fd\u548c\u6210\u672c\u3002", "motivation": "\u968f\u7740LLMs\u7684\u666e\u53ca\uff0c\u5982\u4f55\u4f4e\u6210\u672c\u5730\u9884\u6d4b\u54ea\u4e2aLLM\u80fd\u6210\u529f\u5b8c\u6210\u4efb\u52a1\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\u3002\u7136\u800c\uff0c\u6709\u4e9b\u4efb\u52a1\u5bf9\u5355\u4e2aLLM\u800c\u8a00\u8fc7\u4e8e\u4e13\u4e1a\u548c\u56f0\u96be\uff0c\u66f4\u9002\u5408\u5206\u89e3\u4e3a\u591a\u4e2a\u5b50\u4efb\u52a1\u3002\u73b0\u6709LLM\u9009\u62e9\u7b97\u6cd5\u4ec5\u9488\u5bf9\u5355\u4e2aLLM\uff0c\u65e0\u6cd5\u5904\u7406\u5b50\u4efb\u52a1\u4e4b\u95f4\u76f8\u4e92\u4f9d\u8d56\u7684\u5e8f\u5217\u5f0fLLM\u9009\u62e9\uff0c\u8fd9\u79cd\u5e8f\u5217\u9009\u62e9\u4e2d\u6bcf\u4e2aLLM\u7684\u8f93\u51fa\u4f1a\u5f71\u54cd\u540e\u7eedLLMs\u7684\u8f93\u5165\u3001\u6210\u672c\u548c\u6210\u529f\u7387\uff0c\u5f62\u6210\u4e86\u590d\u6742\u7684\u6027\u80fd\u4f9d\u8d56\u6027\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u4e0a\u4e0b\u6587\u591a\u81c2\u8d4c\u535a\u673a\uff08neural contextual bandit\uff09\u7684\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u901a\u8fc7\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u5728\u7ebf\u65b9\u5f0f\u5efa\u6a21\u6bcf\u4e2a\u5b50\u4efb\u52a1\u4e0aLLM\u7684\u6210\u529f\u7387\uff0c\u4ece\u800c\u5b66\u4e60\u5982\u4f55\u6307\u5bfc\u4e0d\u540c\u5b50\u4efb\u52a1\u7684LLM\u9009\u62e9\uff0c\u5373\u4f7f\u5728\u7f3a\u4e4f\u5386\u53f2LLM\u6027\u80fd\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u8fd0\u4f5c\u3002", "result": "\u5728\u7535\u4fe1\u95ee\u7b54\u548c\u533b\u7597\u8bca\u65ad\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5176\u4ed6\u7684LLM\u9009\u62e9\u7b97\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u89e3\u51b3\u4e86\u590d\u6742\u591a\u5b50\u4efb\u52a1\u60c5\u5883\u4e0b\uff0c\u5e8f\u5217\u5f0fLLM\u9009\u62e9\u7684\u6311\u6218\uff0c\u5176\u63d0\u51fa\u7684\u795e\u7ecf\u4e0a\u4e0b\u6587\u591a\u81c2\u8d4c\u535a\u673a\u7b97\u6cd5\u5728\u6ca1\u6709\u5386\u53f2\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u6307\u5bfcLLM\u9009\u62e9\uff0c\u4f18\u5316\u4e86\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u6210\u672c\u3002"}}
{"id": "2508.09320", "pdf": "https://arxiv.org/pdf/2508.09320", "abs": "https://arxiv.org/abs/2508.09320", "authors": ["Minghao Liu", "Chia-Hsuan Lu", "Marta Kwiatkowska"], "title": "Exact Verification of Graph Neural Networks with Incremental Constraint Solving", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Graph neural networks (GNNs) are increasingly employed in high-stakes\napplications, such as fraud detection or healthcare, but are susceptible to\nadversarial attacks. A number of techniques have been proposed to provide\nadversarial robustness guarantees, but support for commonly used aggregation\nfunctions in message-passing GNNs is still lacking. In this paper, we develop\nan exact (sound and complete) verification method for GNNs to compute\nguarantees against attribute and structural perturbations that involve edge\naddition or deletion, subject to budget constraints. Focusing on node\nclassification tasks, our method employs constraint solving with bound\ntightening, and iteratively solves a sequence of relaxed constraint\nsatisfaction problems while relying on incremental solving capabilities of\nsolvers to improve efficiency. We implement GNNev, a versatile solver for\nmessage-passing neural networks, which supports three aggregation functions,\nsum, max and mean, with the latter two considered here for the first time.\nExtensive experimental evaluation of GNNev on two standard benchmarks (Cora and\nCiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates its\nusability and effectiveness, as well as superior performance compared to\nexisting {exact verification} tools on sum-aggregated node classification\ntasks.", "AI": {"tldr": "GNN\u5728\u5173\u952e\u5e94\u7528\u4e2d\u6613\u53d7\u5bf9\u6297\u6027\u653b\u51fb\u3002\u672c\u6587\u63d0\u51faGNNev\uff0c\u4e00\u4e2a\u7528\u4e8eGNN\u7684\u7cbe\u786e\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u53ef\u8ba1\u7b97\u5c5e\u6027\u548c\u7ed3\u6784\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u652f\u6301\u591a\u79cd\u805a\u5408\u51fd\u6570\uff08\u5305\u62ec\u9996\u6b21\u652f\u6301max\u548cmean\uff09\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u793a\u51fa\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u7684\u6027\u80fd\u3002", "motivation": "\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u9ad8\u98ce\u9669\u573a\u666f\uff0c\u4f46\u6613\u53d7\u5bf9\u6297\u6027\u653b\u51fb\u3002\u73b0\u6709\u63d0\u4f9b\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u4fdd\u8bc1\u7684\u6280\u672f\u5bf9\u6d88\u606f\u4f20\u9012GNN\u4e2d\u5e38\u7528\u7684\u805a\u5408\u51fd\u6570\u652f\u6301\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u7cbe\u786e\u7684\u65b9\u6cd5\u6765\u9a8c\u8bc1\u5176\u9c81\u68d2\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7cbe\u786e\uff08\u5b8c\u5907\u4e14\u53ef\u9760\uff09\u7684GNN\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97GNN\u5728\u5c5e\u6027\u548c\u7ed3\u6784\uff08\u8fb9\u6dfb\u52a0\u6216\u5220\u9664\uff09\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u5e76\u53d7\u9884\u7b97\u7ea6\u675f\u3002\u8be5\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\uff0c\u91c7\u7528\u7ea6\u675f\u6c42\u89e3\u548c\u8fb9\u754c\u6536\u7d27\u6280\u672f\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u6c42\u89e3\u4e00\u7cfb\u5217\u677e\u5f1b\u7684\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u53ca\u5229\u7528\u6c42\u89e3\u5668\u7684\u589e\u91cf\u6c42\u89e3\u80fd\u529b\u6765\u63d0\u9ad8\u6548\u7387\u3002\u5b9e\u73b0\u4e86\u4e00\u4e2a\u540d\u4e3aGNNev\u7684\u901a\u7528\u6c42\u89e3\u5668\uff0c\u652f\u6301sum\u3001max\u548cmean\u4e09\u79cd\u805a\u5408\u51fd\u6570\uff0c\u5176\u4e2dmax\u548cmean\u662f\u9996\u6b21\u5f97\u5230\u652f\u6301\u3002", "result": "GNNev\u5728\u4e24\u4e2a\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\uff08Cora\u548cCiteSeer\uff09\u548c\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6b3a\u8bc8\u6570\u636e\u96c6\uff08Amazon\u548cYelp\uff09\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660eGNNev\u5177\u6709\u53ef\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u5e76\u4e14\u5728sum\u805a\u5408\u7684\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u7cbe\u786e\u9a8c\u8bc1\u5de5\u5177\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGNNev\u7684\u7cbe\u786e\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30GNN\u5728\u5bf9\u6297\u6027\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u514b\u670d\u4e86\u73b0\u6709\u5de5\u5177\u5728\u805a\u5408\u51fd\u6570\u652f\u6301\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u5b9e\u9a8c\u8bc1\u660eGNNev\u662f\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684GNN\u9c81\u68d2\u6027\u9a8c\u8bc1\u5de5\u5177\uff0c\u5c24\u5176\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u589e\u5f3a\u4e86GNN\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.09477", "pdf": "https://arxiv.org/pdf/2508.09477", "abs": "https://arxiv.org/abs/2508.09477", "authors": ["Zhipeng Yuan", "Kai Wang", "Weize Quan", "Dong-Ming Yan", "Tieru Wu"], "title": "CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "With the rapid advancement of AI generative models, the visual quality of\nAI-generated images (AIIs) has become increasingly close to natural images,\nwhich inevitably raises security concerns. Most AII detectors often employ the\nconventional image classification pipeline with natural images and AIIs\n(generated by a generative model), which can result in limited detection\nperformance for AIIs from unseen generative models. To solve this, we proposed\na universal AI-generated image detector from the perspective of anomaly\ndetection. Our discriminator does not need to access any AIIs and learn a\ngeneralizable representation with unsupervised learning. Specifically, we use\nthe pre-trained CLIP encoder as the feature extractor and design a normalizing\nflow-like unsupervised model. Instead of AIIs, proxy images, e.g., obtained by\napplying a spectral modification operation on natural images, are used for\ntraining. Our models are trained by minimizing the likelihood of proxy images,\noptionally combined with maximizing the likelihood of natural images. Extensive\nexperiments demonstrate the effectiveness of our method on AIIs produced by\nvarious image generators.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f02\u5e38\u68c0\u6d4b\u7684\u901a\u7528AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\uff0c\u65e0\u9700AI\u56fe\u50cf\u8bad\u7ec3\uff0c\u5bf9\u672a\u77e5\u751f\u6210\u6a21\u578b\u6709\u6548\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u56fe\u50cf\uff08AIIs\uff09\u89c6\u89c9\u8d28\u91cf\u63a5\u8fd1\u81ea\u7136\u56fe\u50cf\uff0c\u5b89\u5168\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u73b0\u6709AII\u68c0\u6d4b\u5668\u901a\u5e38\u91c7\u7528\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\uff0c\u4f46\u5728\u9762\u5bf9\u6765\u81ea\u672a\u89c1\u8fc7\u7684\u751f\u6210\u6a21\u578b\u7684AIIs\u65f6\uff0c\u68c0\u6d4b\u6027\u80fd\u6709\u9650\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f02\u5e38\u68c0\u6d4b\u7684\u901a\u7528AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u3002\u5176\u5224\u522b\u5668\u65e0\u9700\u8bbf\u95ee\u4efb\u4f55AIIs\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u83b7\u5f97\u53ef\u6cdb\u5316\u7684\u8868\u793a\u3002\u5177\u4f53\u800c\u8a00\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684CLIP\u7f16\u7801\u5668\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7c7b\u4f3c\u5f52\u4e00\u5316\u6d41\u7684\u65e0\u76d1\u7763\u6a21\u578b\u3002\u8bad\u7ec3\u65f6\uff0c\u4e0d\u4f7f\u7528AIIs\uff0c\u800c\u662f\u4f7f\u7528\u4ee3\u7406\u56fe\u50cf\uff08\u5982\u5bf9\u81ea\u7136\u56fe\u50cf\u8fdb\u884c\u5149\u8c31\u4fee\u6539\u5f97\u5230\uff09\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u4ee3\u7406\u56fe\u50cf\u7684\u4f3c\u7136\uff0c\u5e76\u53ef\u9009\u5730\u7ed3\u5408\u6700\u5927\u5316\u81ea\u7136\u56fe\u50cf\u7684\u4f3c\u7136\u6765\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5bf9\u5404\u79cd\u56fe\u50cf\u751f\u6210\u5668\u751f\u6210\u7684AIIs\u5747\u6709\u6548\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5c06AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u89c6\u4e3a\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\uff0c\u5e76\u5229\u7528\u65e0\u76d1\u7763\u5b66\u4e60\u548c\u4ee3\u7406\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u65e0\u9700\u771f\u5b9eAI\u56fe\u50cf\u5373\u53ef\u8bad\u7ec3\u7684\u901a\u7528\u68c0\u6d4b\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u672a\u77e5\u751f\u6210\u6a21\u578b\u4e0a\u6027\u80fd\u53d7\u9650\u7684\u95ee\u9898\u3002"}}
{"id": "2508.09189", "pdf": "https://arxiv.org/pdf/2508.09189", "abs": "https://arxiv.org/abs/2508.09189", "authors": ["Madan Baduwal"], "title": "Hybrid(Transformer+CNN)-based Polyp Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "8 pages", "summary": "Colonoscopy is still the main method of detection and segmentation of colonic\npolyps, and recent advancements in deep learning networks such as U-Net,\nResUNet, Swin-UNet, and PraNet have made outstanding performance in polyp\nsegmentation. Yet, the problem is extremely challenging due to high variation\nin size, shape, endoscopy types, lighting, imaging protocols, and ill-defined\nboundaries (fluid, folds) of the polyps, rendering accurate segmentation a\nchallenging and problematic task. To address these critical challenges in polyp\nsegmentation, we introduce a hybrid (Transformer + CNN) model that is crafted\nto enhance robustness against evolving polyp characteristics. Our hybrid\narchitecture demonstrates superior performance over existing solutions,\nparticularly in addressing two critical challenges: (1) accurate segmentation\nof polyps with ill-defined margins through boundary-aware attention mechanisms,\nand (2) robust feature extraction in the presence of common endoscopic\nartifacts, including specular highlights, motion blur, and fluid occlusions.\nQuantitative evaluations reveal significant improvements in segmentation\naccuracy (Recall improved by 1.76%, i.e., 0.9555, accuracy improved by 0.07%,\ni.e., 0.9849) and artifact resilience compared to state-of-the-art polyp\nsegmentation methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\uff08Transformer + CNN\uff09\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347\u606f\u8089\u5206\u5272\u7684\u51c6\u786e\u6027\u53ca\u5bf9\u5185\u7aa5\u955c\u4f2a\u5f71\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7ed3\u80a0\u955c\u68c0\u67e5\u4e2d\u606f\u8089\u5206\u5272\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u56e0\u606f\u8089\u5927\u5c0f\u3001\u5f62\u72b6\u3001\u8fb9\u754c\u6a21\u7cca\u53ca\u5185\u7aa5\u955c\u4f2a\u5f71\uff08\u5982\u955c\u9762\u53cd\u5149\u3001\u8fd0\u52a8\u6a21\u7cca\u3001\u6db2\u4f53\u906e\u6321\uff09\u7b49\u56e0\u7d20\uff0c\u5bfc\u81f4\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u7cbe\u786e\u5206\u5272\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u6df7\u5408\uff08Transformer + CNN\uff09\u6a21\u578b\uff0c\u65e8\u5728\u589e\u5f3a\u5bf9\u4e0d\u65ad\u53d8\u5316\u7684\u606f\u8089\u7279\u5f81\u7684\u9c81\u68d2\u6027\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u8fb9\u754c\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u5904\u7406\u8fb9\u754c\u6a21\u7cca\u7684\u606f\u8089\uff0c\u5e76\u63d0\u5347\u5728\u5b58\u5728\u5185\u7aa5\u955c\u4f2a\u5f71\u60c5\u51b5\u4e0b\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "result": "\u8be5\u6df7\u5408\u67b6\u6784\u5728\u5206\u5272\u7cbe\u5ea6\u548c\u5bf9\u4f2a\u5f71\u7684\u5f39\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u89e3\u51b3\u8fb9\u754c\u6a21\u7cca\u548c\u5e38\u89c1\u5185\u7aa5\u955c\u4f2a\u5f71\u95ee\u9898\u4e0a\u8868\u73b0\u7a81\u51fa\u3002\u91cf\u5316\u8bc4\u4f30\u663e\u793a\uff0c\u53ec\u56de\u7387\u63d0\u9ad81.76%\uff080.9555\uff09\uff0c\u51c6\u786e\u7387\u63d0\u9ad80.07%\uff080.9849\uff09\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df7\u5408\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u606f\u8089\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u5bf9\u5185\u7aa5\u955c\u4f2a\u5f71\u7684\u62b5\u6297\u80fd\u529b\uff0c\u4e3a\u7ed3\u80a0\u606f\u8089\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09330", "pdf": "https://arxiv.org/pdf/2508.09330", "abs": "https://arxiv.org/abs/2508.09330", "authors": ["Gideon Vos", "Liza van Eijk", "Zoltan Sarnyai", "Mostafa Rahimi Azghadi"], "title": "Synaptic Pruning: A Biological Inspiration for Deep Learning Regularization", "categories": ["cs.LG", "cs.AI"], "comment": "24 pages, 7 figures", "summary": "Synaptic pruning in biological brains removes weak connections to improve\nefficiency. In contrast, dropout regularization in artificial neural networks\nrandomly deactivates neurons without considering activity-dependent pruning. We\npropose a magnitude-based synaptic pruning method that better reflects biology\nby progressively removing low-importance connections during training.\nIntegrated directly into the training loop as a dropout replacement, our\napproach computes weight importance from absolute magnitudes across layers and\napplies a cubic schedule to gradually increase global sparsity. At fixed\nintervals, pruning masks permanently remove low-importance weights while\nmaintaining gradient flow for active ones, eliminating the need for separate\npruning and fine-tuning phases. Experiments on multiple time series forecasting\nmodels including RNN, LSTM, and Patch Time Series Transformer across four\ndatasets show consistent gains. Our method ranked best overall, with\nstatistically significant improvements confirmed by Friedman tests (p < 0.01).\nIn financial forecasting, it reduced Mean Absolute Error by up to 20% over\nmodels with no or standard dropout, and up to 52% in select transformer models.\nThis dynamic pruning mechanism advances regularization by coupling weight\nelimination with progressive sparsification, offering easy integration into\ndiverse architectures. Its strong performance, especially in financial time\nseries forecasting, highlights its potential as a practical alternative to\nconventional dropout techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6a21\u4eff\u751f\u7269\u5927\u8111\u7a81\u89e6\u4fee\u526a\u7684\u57fa\u4e8e\u6743\u503c\u5e45\u5ea6\u7684\u52a8\u6001\u526a\u679d\u65b9\u6cd5\uff0c\u4f5c\u4e3aDropout\u7684\u66ff\u4ee3\uff0c\u5728\u8bad\u7ec3\u4e2d\u6e10\u8fdb\u79fb\u9664\u4f4e\u91cd\u8981\u6027\u8fde\u63a5\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u91d1\u878d\u9884\u6d4b\u4e2d\u663e\u8457\u964d\u4f4e\u8bef\u5dee\uff0c\u662f\u4f20\u7edfDropout\u7684\u6709\u6548\u66ff\u4ee3\u3002", "motivation": "\u751f\u7269\u5927\u8111\u901a\u8fc7\u4fee\u526a\u5f31\u8fde\u63a5\u63d0\u9ad8\u6548\u7387\uff0c\u800c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684Dropout\u6b63\u5219\u5316\u662f\u968f\u673a\u505c\u7528\u795e\u7ecf\u5143\uff0c\u672a\u8003\u8651\u57fa\u4e8e\u6d3b\u52a8\u7684\u91cd\u8981\u6027\u4fee\u526a\uff0c\u8fd9\u4e0e\u751f\u7269\u673a\u5236\u4e0d\u7b26\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6743\u503c\u5e45\u5ea6\u7684\u52a8\u6001\u7a81\u89e6\u4fee\u526a\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u4f5c\u4e3aDropout\u7684\u66ff\u4ee3\uff0c\u76f4\u63a5\u6574\u5408\u5230\u8bad\u7ec3\u5faa\u73af\u4e2d\uff0c\u901a\u8fc7\u8ba1\u7b97\u8de8\u5c42\u6743\u503c\u7684\u7edd\u5bf9\u5e45\u5ea6\u786e\u5b9a\u91cd\u8981\u6027\u3002\u5b83\u91c7\u7528\u7acb\u65b9\u8c03\u5ea6\u6e10\u8fdb\u589e\u52a0\u5168\u5c40\u7a00\u758f\u6027\uff0c\u5e76\u5728\u56fa\u5b9a\u95f4\u9694\u901a\u8fc7\u4fee\u526a\u63a9\u7801\u6c38\u4e45\u79fb\u9664\u4f4e\u91cd\u8981\u6027\u6743\u503c\uff0c\u540c\u65f6\u4fdd\u6301\u6d3b\u8dc3\u6743\u503c\u7684\u68af\u5ea6\u6d41\uff0c\u4ece\u800c\u65e0\u9700\u5355\u72ec\u7684\u4fee\u526a\u548c\u5fae\u8c03\u9636\u6bb5\u3002", "result": "\u5728RNN\u3001LSTM\u548cPatch Time Series Transformer\u7b49\u591a\u79cd\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u53ca\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u9886\u5148\uff0c\u5e76\u7ecfFriedman\u68c0\u9a8c\uff08p < 0.01\uff09\u8bc1\u5b9e\u5177\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\u4e0a\u7684\u663e\u8457\u6539\u8fdb\u3002\u5728\u91d1\u878d\u9884\u6d4b\u4e2d\uff0c\u5b83\u76f8\u5bf9\u4e8e\u65e0\u6216\u6807\u51c6Dropout\u7684\u6a21\u578b\u5c06\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u964d\u4f4e\u9ad8\u8fbe20%\uff0c\u76f8\u5bf9\u4e8e\u7279\u5b9aTransformer\u6a21\u578b\u964d\u4f4e\u9ad8\u8fbe52%\u3002", "conclusion": "\u8be5\u52a8\u6001\u526a\u679d\u673a\u5236\u901a\u8fc7\u5c06\u6743\u503c\u6d88\u9664\u4e0e\u6e10\u8fdb\u7a00\u758f\u5316\u7ed3\u5408\uff0c\u63a8\u52a8\u4e86\u6b63\u5219\u5316\u6280\u672f\u53d1\u5c55\uff0c\u5e76\u6613\u4e8e\u96c6\u6210\u5230\u5404\u79cd\u67b6\u6784\u4e2d\u3002\u5176\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u8868\u660e\u5176\u4f5c\u4e3a\u4f20\u7edfDropout\u6280\u672f\u7684\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.09478", "pdf": "https://arxiv.org/pdf/2508.09478", "abs": "https://arxiv.org/abs/2508.09478", "authors": ["Moinak Bhattacharya", "Gagandeep Singh", "Shubham Jain", "Prateek Prasanna"], "title": "GazeLT: Visual attention-guided long-tailed disease classification in chest radiographs", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we present GazeLT, a human visual attention\nintegration-disintegration approach for long-tailed disease classification. A\nradiologist's eye gaze has distinct patterns that capture both fine-grained and\ncoarser level disease related information. While interpreting an image, a\nradiologist's attention varies throughout the duration; it is critical to\nincorporate this into a deep learning framework to improve automated image\ninterpretation. Another important aspect of visual attention is that apart from\nlooking at major/obvious disease patterns, experts also look at\nminor/incidental findings (few of these constituting long-tailed classes)\nduring the course of image interpretation. GazeLT harnesses the temporal aspect\nof the visual search process, via an integration and disintegration mechanism,\nto improve long-tailed disease classification. We show the efficacy of GazeLT\non two publicly available datasets for long-tailed disease classification,\nnamely the NIH-CXR-LT (n=89237) and the MIMIC-CXR-LT (n=111898) datasets.\nGazeLT outperforms the best long-tailed loss by 4.1% and the visual\nattention-based baseline by 21.7% in average accuracy metrics for these\ndatasets. Our code is available at https://github.com/lordmoinak1/gazelt.", "AI": {"tldr": "GazeLT\u662f\u4e00\u79cd\u5c06\u653e\u5c04\u79d1\u533b\u751f\u773c\u52a8\u884c\u4e3a\uff08\u5305\u62ec\u5176\u65f6\u95f4\u53d8\u5316\u548c\u5bf9\u957f\u5c3e\u7c7b\u522b\u7684\u5173\u6ce8\uff09\u878d\u5165\u6df1\u5ea6\u5b66\u4e60\uff0c\u4ee5\u63d0\u5347\u957f\u5c3e\u75be\u75c5\u5206\u7c7b\u6027\u80fd\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u5316\u533b\u5b66\u56fe\u50cf\u5224\u8bfb\u6a21\u578b\u672a\u80fd\u6709\u6548\u5229\u7528\u653e\u5c04\u79d1\u533b\u751f\u5728\u8bca\u65ad\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u89c6\u89c9\u6ce8\u610f\u529b\uff0c\u7279\u522b\u662f\u5bf9\u90a3\u4e9b\u6784\u6210\u201c\u957f\u5c3e\u201d\u7684\u6b21\u8981\u6216\u5076\u53d1\u75be\u75c5\u7684\u5173\u6ce8\u3002\u533b\u751f\u7684\u773c\u52a8\u6a21\u5f0f\u5305\u542b\u4e30\u5bcc\u7684\u7cbe\u7ec6\u548c\u7c97\u7cd9\u75be\u75c5\u4fe1\u606f\uff0c\u4e14\u5176\u6ce8\u610f\u529b\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u6574\u5408\u8fd9\u4e9b\u52a8\u6001\u7279\u5f81\u5bf9\u63d0\u9ad8\u81ea\u52a8\u5316\u5224\u8bfb\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86GazeLT\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u4e0e\u5206\u89e3\u673a\u5236\uff0c\u5229\u7528\u4eba\u7c7b\u89c6\u89c9\u641c\u7d22\u8fc7\u7a0b\u7684\u65f6\u95f4\u52a8\u6001\u7279\u6027\u6765\u6539\u8fdb\u957f\u5c3e\u75be\u75c5\u5206\u7c7b\u3002\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u653e\u5c04\u79d1\u533b\u751f\u773c\u52a8\u6a21\u5f0f\u4e2d\u6355\u6349\u5230\u7684\u7cbe\u7ec6\u548c\u7c97\u7cd9\u5c42\u9762\u7684\u75be\u75c5\u76f8\u5173\u4fe1\u606f\uff0c\u5e76\u8003\u8651\u4e86\u5176\u968f\u65f6\u95f4\u53d8\u5316\u7684\u6ce8\u610f\u529b\u3002", "result": "GazeLT\u5728NIH-CXR-LT\u548cMIMIC-CXR-LT\u4e24\u4e2a\u5927\u578b\u516c\u5f00\u957f\u5c3e\u75be\u75c5\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u7ed3\u679c\u663e\u793a\uff0cGazeLT\u5728\u5e73\u5747\u51c6\u786e\u7387\u6307\u6807\u4e0a\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u957f\u5c3e\u635f\u5931\u65b9\u6cd5\u9ad8\u51fa4.1%\uff0c\u6bd4\u57fa\u4e8e\u89c6\u89c9\u6ce8\u610f\u529b\u7684\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa21.7%\u3002", "conclusion": "GazeLT\u901a\u8fc7\u6709\u6548\u6574\u5408\u4eba\u7c7b\u52a8\u6001\u89c6\u89c9\u6ce8\u610f\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u5c3e\u75be\u75c5\u7684\u81ea\u52a8\u5316\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7684\u533b\u5b66\u56fe\u50cf\u5224\u8bfb\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2508.09334", "pdf": "https://arxiv.org/pdf/2508.09334", "abs": "https://arxiv.org/abs/2508.09334", "authors": ["Zhongtian Sun", "Anoushka Harit"], "title": "RicciFlowRec: A Geometric Root Cause Recommender Using Ricci Curvature on Financial Graphs", "categories": ["cs.LG", "cs.AI", "cs.IR"], "comment": "Accepted at ACM RecSys 2025 (Late Breaking Results Track)", "summary": "We propose RicciFlowRec, a geometric recommendation framework that performs\nroot cause attribution via Ricci curvature and flow on dynamic financial\ngraphs. By modelling evolving interactions among stocks, macroeconomic\nindicators, and news, we quantify local stress using discrete Ricci curvature\nand trace shock propagation via Ricci flow. Curvature gradients reveal causal\nsubstructures, informing a structural risk-aware ranking function. Preliminary\nresults on S\\&P~500 data with FinBERT-based sentiment show improved robustness\nand interpretability under synthetic perturbations. This ongoing work supports\ncurvature-based attribution and early-stage risk-aware ranking, with plans for\nportfolio optimization and return forecasting. To our knowledge, RicciFlowRec\nis the first recommender to apply geometric flow-based reasoning in financial\ndecision support.", "AI": {"tldr": "RicciFlowRec\u662f\u4e00\u4e2a\u57fa\u4e8eRicci\u66f2\u7387\u548c\u6d41\u7684\u51e0\u4f55\u63a8\u8350\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u52a8\u6001\u91d1\u878d\u56fe\u4e0a\u8fdb\u884c\u6839\u6e90\u5f52\u56e0\u548c\u98ce\u9669\u611f\u77e5\u6392\u540d\u3002", "motivation": "\u5728\u52a8\u6001\u91d1\u878d\u56fe\u4e2d\uff0c\u91cf\u5316\u5c40\u90e8\u538b\u529b\u5e76\u8ffd\u8e2a\u51b2\u51fb\u4f20\u64ad\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u91d1\u878d\u4e8b\u4ef6\u6839\u6e90\u7684\u5f52\u56e0\uff0c\u5e76\u6784\u5efa\u98ce\u9669\u611f\u77e5\u7684\u63a8\u8350\u7cfb\u7edf\u3002", "method": "\u63d0\u51faRicciFlowRec\u6846\u67b6\uff0c\u901a\u8fc7Ricci\u66f2\u7387\u548c\u6d41\u5206\u6790\u52a8\u6001\u91d1\u878d\u56fe\uff08\u5305\u542b\u80a1\u7968\u3001\u5b8f\u89c2\u6307\u6807\u3001\u65b0\u95fb\u4e92\u52a8\uff09\u3002\u5229\u7528\u79bb\u6563Ricci\u66f2\u7387\u91cf\u5316\u5c40\u90e8\u538b\u529b\uff0c\u901a\u8fc7Ricci\u6d41\u8ffd\u8e2a\u51b2\u51fb\u4f20\u64ad\uff0c\u5e76\u57fa\u4e8e\u66f2\u7387\u68af\u5ea6\u63ed\u793a\u56e0\u679c\u5b50\u7ed3\u6784\uff0c\u4ece\u800c\u6784\u5efa\u7ed3\u6784\u98ce\u9669\u611f\u77e5\u7684\u6392\u540d\u51fd\u6570\u3002", "result": "\u5728S&P 500\u6570\u636e\u4e0a\uff0c\u7ed3\u5408FinBERT\u60c5\u611f\u5206\u6790\u7684\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5408\u6210\u6270\u52a8\u4e0b\uff0c\u7cfb\u7edf\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u652f\u6301\u57fa\u4e8e\u66f2\u7387\u7684\u6839\u6e90\u5f52\u56e0\u548c\u65e9\u671f\u98ce\u9669\u611f\u77e5\u7684\u6392\u540d\u529f\u80fd\uff0c\u662f\u9996\u4e2a\u5c06\u51e0\u4f55\u6d41\u63a8\u7406\u5e94\u7528\u4e8e\u91d1\u878d\u51b3\u7b56\u652f\u6301\u7684\u63a8\u8350\u5668\u3002\u672a\u6765\u7684\u5de5\u4f5c\u8ba1\u5212\u5305\u62ec\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u548c\u56de\u62a5\u9884\u6d4b\u3002"}}
{"id": "2508.09479", "pdf": "https://arxiv.org/pdf/2508.09479", "abs": "https://arxiv.org/abs/2508.09479", "authors": ["Xuejun Huang", "Xinyi Liu", "Yi Wan", "Zhi Zheng", "Bin Zhang", "Mingtao Xiong", "Yingying Pei", "Yongjun Zhang"], "title": "SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images", "categories": ["cs.CV"], "comment": null, "summary": "Three-dimensional scene reconstruction from sparse-view satellite images is a\nlong-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its\nvariants have recently attracted attention for its high efficiency, existing\nmethods remain unsuitable for satellite images due to incompatibility with\nrational polynomial coefficient (RPC) models and limited generalization\ncapability. Recent advances in generalizable 3DGS approaches show potential,\nbut they perform poorly on multi-temporal sparse satellite images due to\nlimited geometric constraints, transient objects, and radiometric\ninconsistencies. To address these limitations, we propose SkySplat, a novel\nself-supervised framework that integrates the RPC model into the generalizable\n3DGS pipeline, enabling more effective use of sparse geometric cues for\nimproved reconstruction. SkySplat relies only on RGB images and\nradiometric-robust relative height supervision, thereby eliminating the need\nfor ground-truth height maps. Key components include a Cross-Self Consistency\nModule (CSCM), which mitigates transient object interference via\nconsistency-based masking, and a multi-view consistency aggregation strategy\nthat refines reconstruction results. Compared to per-scene optimization\nmethods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy.\nIt also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to\n1.80 m on the DFC19 dataset significantly, and demonstrates strong\ncross-dataset generalization on the MVS3D benchmark.", "AI": {"tldr": "\u63d0\u51faSkySplat\uff0c\u4e00\u4e2a\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u5c06RPC\u6a21\u578b\u6574\u5408\u5230\u901a\u75283DGS\u4e2d\uff0c\u89e3\u51b3\u7a00\u758f\u89c6\u89d2\u536b\u661f\u56fe\u50cf\u4e09\u7ef4\u91cd\u5efa\u4e2d\u7684\u77ac\u6001\u7269\u4f53\u3001\u51e0\u4f55\u7ea6\u675f\u4e0d\u8db3\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u5feb\u7684\u901f\u5ea6\u3001\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7a00\u758f\u89c6\u89d2\u536b\u661f\u56fe\u50cf\u4e09\u7ef4\u91cd\u5efa\u662f\u4e00\u4e2a\u957f\u671f\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u5c3d\u7ba13D Gaussian Splatting\uff083DGS\uff09\u9ad8\u6548\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u4e0e\u6709\u7406\u591a\u9879\u5f0f\u7cfb\u6570\uff08RPC\uff09\u6a21\u578b\u4e0d\u517c\u5bb9\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u800c\u4e0d\u9002\u7528\u4e8e\u536b\u661f\u56fe\u50cf\u3002\u901a\u75283DGS\u65b9\u6cd5\u5728\u591a\u65f6\u76f8\u7a00\u758f\u536b\u661f\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u539f\u56e0\u5728\u4e8e\u51e0\u4f55\u7ea6\u675f\u6709\u9650\u3001\u77ac\u6001\u7269\u4f53\u548c\u8f90\u5c04\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faSkySplat\uff0c\u4e00\u4e2a\u81ea\u76d1\u7763\u6846\u67b6\u3002\u5b83\u5c06RPC\u6a21\u578b\u96c6\u6210\u5230\u901a\u75283DGS\u7ba1\u7ebf\u4e2d\uff0c\u4ee5\u6709\u6548\u5229\u7528\u7a00\u758f\u51e0\u4f55\u7ebf\u7d22\u3002\u8be5\u65b9\u6cd5\u4ec5\u4f9d\u8d56RGB\u56fe\u50cf\u548c\u8f90\u5c04\u9c81\u68d2\u7684\u76f8\u5bf9\u9ad8\u5ea6\u76d1\u7763\uff0c\u65e0\u9700\u5730\u9762\u771f\u5b9e\u9ad8\u5ea6\u56fe\u3002\u5173\u952e\u7ec4\u4ef6\u5305\u62ec\uff1a1) \u8de8\u81ea\u4e00\u81f4\u6027\u6a21\u5757\uff08CSCM\uff09\uff0c\u901a\u8fc7\u57fa\u4e8e\u4e00\u81f4\u6027\u7684\u63a9\u853d\u51cf\u8f7b\u77ac\u6001\u7269\u4f53\u5e72\u6270\uff1b2) \u591a\u89c6\u56fe\u4e00\u81f4\u6027\u805a\u5408\u7b56\u7565\uff0c\u7528\u4e8e\u4f18\u5316\u91cd\u5efa\u7ed3\u679c\u3002", "result": "\u76f8\u8f83\u4e8e\u9010\u573a\u666f\u4f18\u5316\u65b9\u6cd5\uff0cSkySplat\u6bd4EOGS\u63d0\u901f86\u500d\u5e76\u5177\u6709\u66f4\u9ad8\u7cbe\u5ea6\u3002\u76f8\u8f83\u4e8e\u901a\u75283DGS\u57fa\u7ebf\uff0c\u5728DFC19\u6570\u636e\u96c6\u4e0a\u5c06\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u4ece13.18\u7c73\u663e\u8457\u964d\u4f4e\u81f31.80\u7c73\u3002\u5728MVS3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SkySplat\u901a\u8fc7\u96c6\u6210RPC\u6a21\u578b\u548c\u91c7\u7528\u81ea\u76d1\u7763\u6280\u672f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u89d2\u536b\u661f\u56fe\u50cf\u4e09\u7ef4\u91cd\u5efa\u7684\u6311\u6218\uff0c\u5728\u901f\u5ea6\u3001\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.09224", "pdf": "https://arxiv.org/pdf/2508.09224", "abs": "https://arxiv.org/abs/2508.09224", "authors": ["Yuan Yuan", "Tina Sriskandarajah", "Anna-Luisa Brakman", "Alec Helyar", "Alex Beutel", "Andrea Vallone", "Saachi Jain"], "title": "From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models used in ChatGPT have traditionally been trained to\nlearn a refusal boundary: depending on the user's intent, the model is taught\nto either fully comply or outright refuse. While this is a strong mitigation\nfor explicitly malicious prompts, focusing safety training on refusals can lead\nto brittleness for prompts with obscured user intent. Binary refusal boundaries\nare especially ill-suited for dual-use cases (such as biology or\ncybersecurity), where a user request can be answered safely at a high level,\nbut in some cases can lead to malicious uplift if sufficiently detailed or\nactionable. As an alternative, we propose safe-completions: a safety-training\napproach that centers on the safety of the assistant's output, rather than a\nbinary classification of the user's intent. Safe-completions seek to maximize\nhelpfulness within the safety policy's constraints. We incorporated this\napproach into GPT-5 and find that across both production comparisons and\ninternally controlled experiments, safe-completion training improves safety\n(especially on dual-use prompts), reduces the severity of residual safety\nfailures, and substantially increases model helpfulness.", "AI": {"tldr": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5b89\u5168\u8bad\u7ec3\uff08\u4e8c\u5143\u62d2\u7edd\uff09\u5728\u5904\u7406\u6a21\u7cca\u6216\u53cc\u7528\u9014\u63d0\u793a\u65f6\u5b58\u5728\u5c40\u9650\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5b89\u5168\u8865\u5168\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u5b89\u5168\u91cd\u5fc3\u4ece\u7528\u6237\u610f\u56fe\u8f6c\u79fb\u5230\u52a9\u624b\u8f93\u51fa\uff0c\u65e8\u5728\u63d0\u9ad8LLM\u5728\u4fdd\u6301\u5b89\u5168\u6027\u7684\u540c\u65f6\uff0c\u5728\u53cc\u7528\u9014\u573a\u666f\u4e0b\u7684\u6709\u7528\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u3002", "motivation": "\u4f20\u7edfLLM\u5b89\u5168\u8bad\u7ec3\u4fa7\u91cd\u4e8e\u5b66\u4e60\u62d2\u7edd\u8fb9\u754c\uff08\u5b8c\u5168\u9075\u5b88\u6216\u62d2\u7edd\uff09\uff0c\u8fd9\u5bf9\u4e8e\u660e\u786e\u7684\u6076\u610f\u63d0\u793a\u6709\u6548\uff0c\u4f46\u5bf9\u4e8e\u610f\u56fe\u6a21\u7cca\u7684\u63d0\u793a\u5219\u663e\u5f97\u8106\u5f31\u3002\u5c24\u5176\u5728\u53cc\u7528\u9014\u573a\u666f\uff08\u5982\u751f\u7269\u3001\u7f51\u7edc\u5b89\u5168\uff09\u4e2d\uff0c\u8bf7\u6c42\u5728\u9ad8\u5c42\u6b21\u4e0a\u53ef\u5b89\u5168\u56de\u7b54\uff0c\u4f46\u82e5\u7ec6\u8282\u6216\u53ef\u64cd\u4f5c\u6027\u8db3\u591f\uff0c\u53ef\u80fd\u5bfc\u81f4\u6076\u610f\u63d0\u5347\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406\u6b64\u7c7b\u590d\u6742\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u65bd\u4e86\u201c\u5b89\u5168\u8865\u5168\uff08safe-completions\uff09\u201d\u7684\u5b89\u5168\u8bad\u7ec3\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5c06\u5b89\u5168\u7126\u70b9\u653e\u5728\u52a9\u624b\u7684\u8f93\u51fa\u5b89\u5168\u6027\u4e0a\uff0c\u800c\u975e\u5bf9\u7528\u6237\u610f\u56fe\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\u3002\u5176\u76ee\u6807\u662f\u5728\u5b89\u5168\u7b56\u7565\u7684\u7ea6\u675f\u4e0b\uff0c\u6700\u5927\u5316\u6a21\u578b\u7684\u6709\u7528\u6027\u3002", "result": "\u5c06\u201c\u5b89\u5168\u8865\u5168\u201d\u65b9\u6cd5\u6574\u5408\u5230GPT-5\u4e2d\uff0c\u901a\u8fc7\u751f\u4ea7\u5bf9\u6bd4\u548c\u5185\u90e8\u5bf9\u7167\u5b9e\u9a8c\uff0c\u53d1\u73b0\u8be5\u8bad\u7ec3\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u5b89\u5168\u6027\uff08\u7279\u522b\u662f\u5728\u53cc\u7528\u9014\u63d0\u793a\u4e0a\uff09\uff0c\u964d\u4f4e\u4e86\u6b8b\u4f59\u5b89\u5168\u6545\u969c\u7684\u4e25\u91cd\u6027\uff0c\u5e76\u5927\u5e45\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6709\u7528\u6027\u3002", "conclusion": "\u201c\u5b89\u5168\u8865\u5168\u201d\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfLLM\u5b89\u5168\u8bad\u7ec3\u5728\u5904\u7406\u610f\u56fe\u6a21\u7cca\u548c\u53cc\u7528\u9014\u63d0\u793a\u65f6\u7684\u4e0d\u8db3\u3002\u5b83\u901a\u8fc7\u5173\u6ce8\u8f93\u51fa\u5b89\u5168\u6027\uff0c\u5728\u63d0\u9ad8\u6a21\u578b\u5b89\u5168\u6027\u7684\u540c\u65f6\uff0c\u4e5f\u663e\u8457\u589e\u5f3a\u4e86\u5176\u6709\u7528\u6027\uff0c\u4e3aLLM\u7684\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u6f5c\u529b\u3002"}}
{"id": "2508.09363", "pdf": "https://arxiv.org/pdf/2508.09363", "abs": "https://arxiv.org/abs/2508.09363", "authors": ["Charles O'Neill", "Mudith Jayasekara", "Max Kirkby"], "title": "Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders", "categories": ["cs.LG"], "comment": null, "summary": "Sparse autoencoders (SAEs) decompose large language model (LLM) activations\ninto latent features that reveal mechanistic structure. Conventional SAEs train\non broad data distributions, forcing a fixed latent budget to capture only\nhigh-frequency, generic patterns. This often results in significant linear\n``dark matter'' in reconstruction error and produces latents that fragment or\nabsorb each other, complicating interpretation. We show that restricting SAE\ntraining to a well-defined domain (medical text) reallocates capacity to\ndomain-specific features, improving both reconstruction fidelity and\ninterpretability. Training JumpReLU SAEs on layer-20 activations of Gemma-2\nmodels using 195k clinical QA examples, we find that domain-confined SAEs\nexplain up to 20\\% more variance, achieve higher loss recovery, and reduce\nlinear residual error compared to broad-domain SAEs. Automated and human\nevaluations confirm that learned features align with clinically meaningful\nconcepts (e.g., ``taste sensations'' or ``infectious mononucleosis''), rather\nthan frequent but uninformative tokens. These domain-specific SAEs capture\nrelevant linear structure, leaving a smaller, more purely nonlinear residual.\nWe conclude that domain-confinement mitigates key limitations of broad-domain\nSAEs, enabling more complete and interpretable latent decompositions, and\nsuggesting the field may need to question ``foundation-model'' scaling for\ngeneral-purpose SAEs.", "AI": {"tldr": "\u672c\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982\u533b\u5b66\u6587\u672c\uff09\u8bad\u7ec3\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u80fd\u663e\u8457\u63d0\u5347\u5176\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6fc0\u6d3b\u7684\u5206\u89e3\u80fd\u529b\uff0c\u63d0\u9ad8\u91cd\u5efa\u7cbe\u5ea6\u548c\u7279\u5f81\u53ef\u89e3\u91ca\u6027\uff0c\u4f18\u4e8e\u5728\u5e7f\u6cdb\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684SAEs\u3002", "motivation": "\u4f20\u7edf\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728\u901a\u7528\u6570\u636e\u4e0a\u8bad\u7ec3\u65f6\uff0c\u5176\u6709\u9650\u7684\u6f5c\u5728\u7279\u5f81\u9884\u7b97\u53ea\u80fd\u6355\u83b7\u9ad8\u9891\u901a\u7528\u6a21\u5f0f\uff0c\u5bfc\u81f4\u91cd\u5efa\u8bef\u5dee\uff08\u201c\u6697\u7269\u8d28\u201d\uff09\u663e\u8457\uff0c\u4e14\u6f5c\u5728\u7279\u5f81\u788e\u7247\u5316\u6216\u76f8\u4e92\u5438\u6536\uff0c\u96be\u4ee5\u89e3\u91ca\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u4ee5\u83b7\u5f97\u66f4\u51c6\u786e\u548c\u53ef\u89e3\u91ca\u7684LLM\u6fc0\u6d3b\u5206\u89e3\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u662f\u5c06JumpReLU SAEs\u7684\u8bad\u7ec3\u8303\u56f4\u9650\u5236\u5728\u7279\u5b9a\u9886\u57df\uff08\u4f7f\u752819.5\u4e07\u4e2a\u4e34\u5e8a\u95ee\u7b54\u793a\u4f8b\u7684\u533b\u5b66\u6587\u672c\uff09\uff0c\u76ee\u6807\u662fGemma-2\u6a21\u578b\u7b2c20\u5c42\u7684\u6fc0\u6d3b\u3002\u901a\u8fc7\u4e0e\u5728\u5e7f\u6cdb\u9886\u57df\u8bad\u7ec3\u7684SAEs\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5e76\u7ed3\u5408\u81ea\u52a8\u5316\u548c\u4eba\u5de5\u8bc4\u4f30\uff0c\u6765\u8861\u91cf\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u9886\u57df\u9650\u5236\u578bSAEs\u80fd\u591a\u89e3\u91ca\u9ad8\u8fbe20%\u7684\u65b9\u5dee\uff0c\u5b9e\u73b0\u66f4\u9ad8\u7684\u635f\u5931\u6062\u590d\uff0c\u5e76\u51cf\u5c11\u7ebf\u6027\u6b8b\u4f59\u8bef\u5dee\u3002\u5b66\u4e60\u5230\u7684\u7279\u5f81\u4e0e\u4e34\u5e8a\u4e0a\u6709\u610f\u4e49\u7684\u6982\u5ff5\u9ad8\u5ea6\u543b\u5408\uff0c\u800c\u975e\u9891\u7e41\u4f46\u4e0d\u5177\u4fe1\u606f\u91cf\u7684token\u3002\u8fd9\u4e9bSAEs\u6355\u83b7\u4e86\u76f8\u5173\u7684\u7ebf\u6027\u7ed3\u6784\uff0c\u7559\u4e0b\u4e86\u66f4\u5c0f\u3001\u66f4\u7eaf\u7cb9\u7684\u975e\u7ebf\u6027\u6b8b\u5dee\u3002", "conclusion": "\u7814\u7a76\u5f97\u51fa\u7ed3\u8bba\uff0c\u9886\u57df\u9650\u5236\u80fd\u6709\u6548\u7f13\u89e3\u901a\u7528\u9886\u57dfSAEs\u7684\u5173\u952e\u5c40\u9650\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5b8c\u6574\u548c\u53ef\u89e3\u91ca\u7684\u6f5c\u5728\u5206\u89e3\u3002\u8fd9\u6697\u793a\u4e86\u5728\u901a\u7528\u578bSAEs\u9886\u57df\u53ef\u80fd\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u201c\u57fa\u7840\u6a21\u578b\u201d\u7684\u89c4\u6a21\u5316\u7b56\u7565\uff0c\u652f\u6301\u9886\u57df\u5b9a\u5236\u5316\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.09486", "pdf": "https://arxiv.org/pdf/2508.09486", "abs": "https://arxiv.org/abs/2508.09486", "authors": ["Yun Wang", "Long Zhang", "Jingren Liu", "Jiaqi Yan", "Zhanjie Zhang", "Jiahao Zheng", "Xun Yang", "Dapeng Wu", "Xiangyu Chen", "Xuelong Li"], "title": "Episodic Memory Representation for Long-form Video Understanding", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "10 pages, 5 figures", "summary": "Video Large Language Models (Video-LLMs) excel at general video understanding\nbut struggle with long-form videos due to context window limits. Consequently,\nrecent approaches focus on keyframe retrieval, condensing lengthy videos into a\nsmall set of informative frames. Despite their practicality, these methods\nsimplify the problem to static text image matching, overlooking spatio temporal\nrelationships crucial for capturing scene transitions and contextual\ncontinuity, and may yield redundant keyframes with limited information,\ndiluting salient cues essential for accurate video question answering. To\naddress these limitations, we introduce Video-EM, a training free framework\ninspired by the principles of human episodic memory, designed to facilitate\nrobust and contextually grounded reasoning. Rather than treating keyframes as\nisolated visual entities, Video-EM explicitly models them as temporally ordered\nepisodic events, capturing both spatial relationships and temporal dynamics\nnecessary for accurately reconstructing the underlying narrative. Furthermore,\nthe framework leverages chain of thought (CoT) thinking with LLMs to\niteratively identify a minimal yet highly informative subset of episodic\nmemories, enabling efficient and accurate question answering by Video-LLMs.\nExtensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench\nbenchmarks confirm the superiority of Video-EM, which achieves highly\ncompetitive results with performance gains of 4-9 percent over respective\nbaselines while utilizing fewer frames.", "AI": {"tldr": "Video-EM\u662f\u4e00\u4e2a\u53d7\u4eba\u7c7b\u60c5\u666f\u8bb0\u5fc6\u542f\u53d1\u7684\u514d\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5173\u952e\u5e27\u5efa\u6a21\u4e3a\u65f6\u5e8f\u4e8b\u4ef6\u5e76\u7ed3\u5408CoT\u63a8\u7406\uff0c\u89e3\u51b3\u4e86Video-LLMs\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u56e0\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u548c\u73b0\u6709\u5173\u952e\u5e27\u65b9\u6cd5\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u66f4\u5c11\u7684\u5e27\u4f7f\u7528\u3002", "motivation": "Video-LLMs\u5728\u901a\u7528\u89c6\u9891\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u96be\u4ee5\u5904\u7406\u957f\u89c6\u9891\u3002\u73b0\u6709\u5173\u952e\u5e27\u68c0\u7d22\u65b9\u6cd5\u5c06\u95ee\u9898\u7b80\u5316\u4e3a\u9759\u6001\u56fe\u50cf\u5339\u914d\uff0c\u5ffd\u7565\u4e86\u65f6\u7a7a\u5173\u7cfb\uff0c\u53ef\u80fd\u5bfc\u81f4\u5197\u4f59\u6216\u4fe1\u606f\u91cf\u6709\u9650\u7684\u5173\u952e\u5e27\uff0c\u4ece\u800c\u5f71\u54cd\u89c6\u9891\u95ee\u7b54\u7684\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165Video-EM\u6846\u67b6\uff0c\u7075\u611f\u6765\u6e90\u4e8e\u4eba\u7c7b\u60c5\u666f\u8bb0\u5fc6\u539f\u7406\uff0c\u65e0\u9700\u8bad\u7ec3\u3002\u8be5\u6846\u67b6\u5c06\u5173\u952e\u5e27\u660e\u786e\u5efa\u6a21\u4e3a\u6309\u65f6\u95f4\u6392\u5e8f\u7684\u60c5\u666f\u4e8b\u4ef6\uff0c\u6355\u6349\u7a7a\u95f4\u5173\u7cfb\u548c\u65f6\u95f4\u52a8\u6001\u3002\u6b64\u5916\uff0c\u5b83\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\uff0c\u8fed\u4ee3\u8bc6\u522b\u51fa\u4e00\u7ec4\u6700\u5c0f\u4f46\u4fe1\u606f\u91cf\u4e30\u5bcc\u7684\u60c5\u666f\u8bb0\u5fc6\u5b50\u96c6\uff0c\u4ece\u800c\u5b9e\u73b0Video-LLMs\u9ad8\u6548\u51c6\u786e\u7684\u95ee\u7b54\u3002", "result": "\u5728Video-MME\u3001EgoSchema\u3001HourVideo\u548cLVBench\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7ed3\u679c\u8bc1\u5b9e\u4e86Video-EM\u7684\u4f18\u8d8a\u6027\u3002\u5b83\u5b9e\u73b0\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u76f8\u5bf9\u4e8e\u5404\u81ea\u57fa\u7ebf\u6027\u80fd\u63d0\u53474-9%\uff0c\u540c\u65f6\u4f7f\u7528\u7684\u5e27\u6570\u66f4\u5c11\u3002", "conclusion": "Video-EM\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u60c5\u666f\u8bb0\u5fc6\u548c\u7ed3\u5408CoT\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86Video-LLMs\u5904\u7406\u957f\u89c6\u9891\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u4ee5\u66f4\u4f4e\u7684\u8d44\u6e90\u6d88\u8017\u83b7\u5f97\u4e86\u66f4\u4f18\u7684\u6027\u80fd\u3002"}}
{"id": "2508.09385", "pdf": "https://arxiv.org/pdf/2508.09385", "abs": "https://arxiv.org/abs/2508.09385", "authors": ["Mansi", "Anastasios Lepipas", "Dominika Woszczyk", "Yiying Guan", "Soteris Demetriou"], "title": "Understanding Dementia Speech Alignment with Diffusion-Based Image Generation", "categories": ["cs.LG", "cs.AI"], "comment": "Paper accepted at Interspeech 2025", "summary": "Text-to-image models generate highly realistic images based on natural\nlanguage descriptions and millions of users use them to create and share images\nonline. While it is expected that such models can align input text and\ngenerated image in the same latent space little has been done to understand\nwhether this alignment is possible between pathological speech and generated\nimages. In this work, we examine the ability of such models to align\ndementia-related speech information with the generated images and develop\nmethods to explain this alignment. Surprisingly, we found that dementia\ndetection is possible from generated images alone achieving 75% accuracy on the\nADReSS dataset. We then leverage explainability methods to show which parts of\nthe language contribute to the detection.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u6587\u751f\u56fe\u6a21\u578b\u5c06\u75f4\u5446\u75c7\u76f8\u5173\u8bed\u97f3\u4fe1\u606f\u4e0e\u751f\u6210\u56fe\u50cf\u5bf9\u9f50\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u4ec5\u51ed\u751f\u6210\u56fe\u50cf\u5373\u53ef\u5b9e\u73b075%\u7684\u75f4\u5446\u75c7\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u5229\u7528\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u63ed\u793a\u4e86\u8bed\u8a00\u4e2d\u5bf9\u68c0\u6d4b\u6709\u8d21\u732e\u7684\u90e8\u5206\u3002", "motivation": "\u6587\u751f\u56fe\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e0e\u56fe\u50cf\u751f\u6210\u4e4b\u95f4\u8868\u73b0\u51fa\u9ad8\u5ea6\u5bf9\u9f50\u80fd\u529b\uff0c\u4f46\u5bf9\u4e8e\u75c5\u7406\u8bed\u97f3\uff08\u7279\u522b\u662f\u75f4\u5446\u75c7\u76f8\u5173\u8bed\u97f3\uff09\u4e0e\u751f\u6210\u56fe\u50cf\u4e4b\u95f4\u7684\u5bf9\u9f50\u80fd\u529b\uff0c\u76ee\u524d\u7814\u7a76\u751a\u5c11\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u6587\u751f\u56fe\u6a21\u578b\u662f\u5426\u80fd\u5b9e\u73b0\u8fd9\u79cd\u5bf9\u9f50\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u53d1\u5c55\u89e3\u91ca\u5bf9\u9f50\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u68c0\u9a8c\u4e86\u6587\u751f\u56fe\u6a21\u578b\u5bf9\u9f50\u75f4\u5446\u75c7\u76f8\u5173\u8bed\u97f3\u4fe1\u606f\u4e0e\u751f\u6210\u56fe\u50cf\u7684\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86\u89e3\u91ca\u8fd9\u79cd\u5bf9\u9f50\u7684\u65b9\u6cd5\u3002\u5177\u4f53\u800c\u8a00\uff0c\u7814\u7a76\u4eba\u5458\u5229\u7528\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u6765\u5c55\u793a\u8bed\u8a00\u7684\u54ea\u4e9b\u90e8\u5206\u5bf9\u75f4\u5446\u75c7\u7684\u68c0\u6d4b\u6709\u6240\u8d21\u732e\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4ec5\u901a\u8fc7\u6587\u751f\u56fe\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\uff0c\u5c31\u53ef\u4ee5\u5b9e\u73b0\u75f4\u5446\u75c7\u68c0\u6d4b\uff0c\u5728ADReSS\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8675%\u7684\u51c6\u786e\u7387\u3002\u6b64\u5916\uff0c\u5229\u7528\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u7814\u7a76\u4eba\u5458\u6210\u529f\u63ed\u793a\u4e86\u8bed\u8a00\u4e2d\u54ea\u4e9b\u90e8\u5206\u5bf9\u75f4\u5446\u75c7\u68c0\u6d4b\u8d77\u5230\u4e86\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u6587\u751f\u56fe\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u5c06\u75f4\u5446\u75c7\u76f8\u5173\u7684\u8bed\u97f3\u4fe1\u606f\u4e0e\u89c6\u89c9\u56fe\u50cf\u5bf9\u9f50\uff0c\u8fd9\u4f7f\u5f97\u4ec5\u4ece\u751f\u6210\u7684\u56fe\u50cf\u4e2d\u68c0\u6d4b\u75f4\u5446\u75c7\u6210\u4e3a\u53ef\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u8bc6\u522b\u51fa\u8bed\u8a00\u4e2d\u5bf9\u68c0\u6d4b\u6709\u8d21\u732e\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2508.09487", "pdf": "https://arxiv.org/pdf/2508.09487", "abs": "https://arxiv.org/abs/2508.09487", "authors": ["Ju Yeon Kang", "Jaehong Park", "Semin Kim", "Ji Won Yoon", "Nam Soo Kim"], "title": "SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection", "categories": ["cs.CV"], "comment": "Work in progress", "summary": "Recently, diffusion-generated image detection has gained increasing\nattention, as the rapid advancement of diffusion models has raised serious\nconcerns about their potential misuse. While existing detection methods have\nachieved promising results, their performance often degrades significantly when\nfacing fake images from unseen, out-of-distribution (OOD) generative models,\nsince they primarily rely on model-specific artifacts. To address this\nlimitation, we explore a fundamental property commonly observed in fake images.\nMotivated by the observation that fake images tend to exhibit higher similarity\nto their captions than real images, we propose a novel representation, namely\nSemantic-Aware Reconstruction Error (SARE), that measures the semantic\ndifference between an image and its caption-guided reconstruction. The\nhypothesis behind SARE is that real images, whose captions often fail to fully\ncapture their complex visual content, may undergo noticeable semantic shifts\nduring the caption-guided reconstruction process. In contrast, fake images,\nwhich closely align with their captions, show minimal semantic changes. By\nquantifying these semantic shifts, SARE can be utilized as a discriminative\nfeature for robust detection across diverse generative models. We empirically\ndemonstrate that the proposed method exhibits strong generalization,\noutperforming existing baselines on benchmarks including GenImage and\nCommunityForensics.", "AI": {"tldr": "\u9488\u5bf9\u73b0\u6709\u6269\u6563\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u5728OOD\u6a21\u578b\u4e0a\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u8bed\u4e49\u611f\u77e5\u91cd\u5efa\u8bef\u5dee\uff08SARE\uff09\uff0c\u5229\u7528\u5047\u56fe\u50cf\u4e0e\u6587\u672c\u63cf\u8ff0\u8bed\u4e49\u66f4\u4e00\u81f4\u7684\u7279\u6027\u8fdb\u884c\u68c0\u6d4b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u6a21\u578b\u7279\u5b9a\u4f2a\u5f71\uff0c\u5bfc\u81f4\u5176\u5728\u9762\u5bf9\u6765\u81ea\u672a\u77e5\uff08OOD\uff09\u751f\u6210\u6a21\u578b\u7684\u4f2a\u9020\u56fe\u50cf\u65f6\uff0c\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\uff0c\u65e0\u6cd5\u6709\u6548\u6cdb\u5316\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u8868\u793a\u65b9\u6cd5\uff1a\u8bed\u4e49\u611f\u77e5\u91cd\u5efa\u8bef\u5dee\uff08SARE\uff09\uff0c\u901a\u8fc7\u6d4b\u91cf\u56fe\u50cf\u4e0e\u7531\u5176\u63cf\u8ff0\u5f15\u5bfc\u7684\u91cd\u5efa\u56fe\u50cf\u4e4b\u95f4\u7684\u8bed\u4e49\u5dee\u5f02\u6765\u68c0\u6d4b\u4f2a\u9020\u56fe\u50cf\u3002\u5176\u6838\u5fc3\u5047\u8bbe\u662f\u771f\u5b9e\u56fe\u50cf\u7684\u63cf\u8ff0\u96be\u4ee5\u5b8c\u5168\u6355\u6349\u5176\u590d\u6742\u89c6\u89c9\u5185\u5bb9\uff0c\u5bfc\u81f4\u5728\u63cf\u8ff0\u5f15\u5bfc\u91cd\u5efa\u8fc7\u7a0b\u4e2d\u53d1\u751f\u663e\u8457\u8bed\u4e49\u504f\u79fb\uff1b\u800c\u4f2a\u9020\u56fe\u50cf\u4e0e\u63cf\u8ff0\u9ad8\u5ea6\u4e00\u81f4\uff0c\u91cd\u5efa\u540e\u8bed\u4e49\u53d8\u5316\u6781\u5c0f\u3002SARE\u91cf\u5316\u8fd9\u4e9b\u8bed\u4e49\u504f\u79fb\u4f5c\u4e3a\u5224\u522b\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684SARE\u65b9\u6cd5\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728GenImage\u548cCommunityForensics\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SARE\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u5bf9\u5404\u79cd\u751f\u6210\u6a21\u578b\u5177\u6709\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\u7684\u6269\u6563\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728OOD\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.09288", "pdf": "https://arxiv.org/pdf/2508.09288", "abs": "https://arxiv.org/abs/2508.09288", "authors": ["Aayush Gupta"], "title": "Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs", "categories": ["cs.CR", "cs.AI", "cs.CL", "68T07, 94A60", "D.4.6; K.6.5; E.3; I.2.6; I.2.7"], "comment": "2 figures, 3 tables; code and certification harness:\n  https://github.com/ayushgupta4897/Contextual-Integrity-Verification ;\n  Elite-Attack dataset: https://huggingface.co/datasets/zyushg/elite-attack", "summary": "Large language models (LLMs) remain acutely vulnerable to prompt injection\nand related jailbreak attacks; heuristic guardrails (rules, filters, LLM\njudges) are routinely bypassed. We present Contextual Integrity Verification\n(CIV), an inference-time security architecture that attaches cryptographically\nsigned provenance labels to every token and enforces a source-trust lattice\ninside the transformer via a pre-softmax hard attention mask (with optional\nFFN/residual gating). CIV provides deterministic, per-token non-interference\nguarantees on frozen models: lower-trust tokens cannot influence higher-trust\nrepresentations. On benchmarks derived from recent taxonomies of\nprompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack\nsuccess rate under the stated threat model while preserving 93.1% token-level\nsimilarity and showing no degradation in model perplexity on benign tasks; we\nnote a latency overhead attributable to a non-optimized data path. Because CIV\nis a lightweight patch -- no fine-tuning required -- we demonstrate drop-in\nprotection for Llama-3-8B and Mistral-7B. We release a reference\nimplementation, an automated certification harness, and the Elite-Attack corpus\nto support reproducible research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aContextual Integrity Verification (CIV)\u7684\u5b89\u5168\u67b6\u6784\uff0c\u901a\u8fc7\u52a0\u5bc6\u7b7e\u540d\u6eaf\u6e90\u6807\u7b7e\u548c\u786c\u6ce8\u610f\u529b\u63a9\u7801\uff0c\u4e3aLLMs\u63d0\u4f9b\u786e\u5b9a\u6027\u7684\u6309token\u975e\u5e72\u6270\u4fdd\u8bc1\uff0c\u6709\u6548\u62b5\u5fa1\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u4e14\u65e0\u9700\u5fae\u8c03\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6781\u6613\u53d7\u5230\u63d0\u793a\u6ce8\u5165\u548c\u8d8a\u72f1\u653b\u51fb\uff0c\u73b0\u6709\u542f\u53d1\u5f0f\u9632\u62a4\u63aa\u65bd\uff08\u89c4\u5219\u3001\u8fc7\u6ee4\u5668\u3001LLM\u5224\u65ad\u5668\uff09\u5e38\u88ab\u89c4\u907f\uff0c\u5bfc\u81f4\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u5f15\u5165Contextual Integrity Verification (CIV)\u67b6\u6784\u3002\u8be5\u67b6\u6784\u5728\u63a8\u7406\u65f6\u4e3a\u6bcf\u4e2atoken\u9644\u52a0\u52a0\u5bc6\u7b7e\u540d\u7684\u6eaf\u6e90\u6807\u7b7e\uff0c\u5e76\u5728Transformer\u5185\u90e8\u901a\u8fc7\u9884softmax\u786c\u6ce8\u610f\u529b\u63a9\u7801\uff08\u53ef\u9009FFN/\u6b8b\u5dee\u95e8\u63a7\uff09\u5f3a\u5236\u6267\u884c\u6e90\u4fe1\u4efb\u683c\u3002\u8fd9\u786e\u4fdd\u4f4e\u4fe1\u4efb\u5ea6token\u4e0d\u80fd\u5f71\u54cd\u9ad8\u4fe1\u4efb\u5ea6\u8868\u793a\uff0c\u5b9e\u73b0\u6309token\u7684\u975e\u5e72\u6270\u4fdd\u8bc1\uff0c\u4e14\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u3002", "result": "\u5728\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u57fa\u51c6\u6d4b\u8bd5\uff08Elite-Attack + SoK-246\uff09\u4e2d\uff0cCIV\u5728\u6307\u5b9a\u5a01\u80c1\u6a21\u578b\u4e0b\u5b9e\u73b0\u4e860%\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u630193.1%\u7684token\u7ea7\u76f8\u4f3c\u6027\uff0c\u5e76\u5728\u826f\u6027\u4efb\u52a1\u4e0a\u6a21\u578b\u56f0\u60d1\u5ea6\u6ca1\u6709\u4e0b\u964d\u3002\u76ee\u524d\u5b58\u5728\u672a\u4f18\u5316\u7684\u6570\u636e\u8def\u5f84\u5bfc\u81f4\u7684\u5ef6\u8fdf\u5f00\u9500\u3002\u8be5\u65b9\u6cd5\u53ef\u5373\u63d2\u5373\u7528\u5730\u4fdd\u62a4Llama-3-8B\u548cMistral-7B\u6a21\u578b\u3002", "conclusion": "CIV\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u5fae\u8c03\u7684\u63a8\u7406\u65f6\u5b89\u5168\u67b6\u6784\uff0c\u80fd\u591f\u786e\u5b9a\u6027\u5730\u62b5\u5fa1LLMs\u7684\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u5e76\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6709\u6548\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u3002\u7814\u7a76\u56e2\u961f\u53d1\u5e03\u4e86\u53c2\u8003\u5b9e\u73b0\u3001\u81ea\u52a8\u5316\u8ba4\u8bc1\u5de5\u5177\u548c\u653b\u51fb\u8bed\u6599\u5e93\u4ee5\u652f\u6301\u590d\u73b0\u3002"}}
{"id": "2508.09399", "pdf": "https://arxiv.org/pdf/2508.09399", "abs": "https://arxiv.org/abs/2508.09399", "authors": ["Yue Yao", "Zhen Xu", "Youzhu Liu", "Kunyuan Ma", "Yuxiu Lin", "Mohan Jiang"], "title": "Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "This paper addresses the challenges of data privacy and collaborative\nmodeling in cross-institution financial risk analysis. It proposes a risk\nassessment framework based on federated learning. Without sharing raw data, the\nmethod enables joint modeling and risk identification across multiple\ninstitutions. This is achieved by incorporating a feature attention mechanism\nand temporal modeling structure. Specifically, the model adopts a distributed\noptimization strategy. Each financial institution trains a local sub-model. The\nmodel parameters are protected using differential privacy and noise injection\nbefore being uploaded. A central server then aggregates these parameters to\ngenerate a global model. This global model is used for systemic risk\nidentification. To validate the effectiveness of the proposed method, multiple\nexperiments are conducted. These evaluate communication efficiency, model\naccuracy, systemic risk detection, and cross-market generalization. The results\nshow that the proposed model outperforms both traditional centralized methods\nand existing federated learning variants across all evaluation metrics. It\ndemonstrates strong modeling capabilities and practical value in sensitive\nfinancial environments. The method enhances the scope and efficiency of risk\nidentification while preserving data sovereignty. It offers a secure and\nefficient solution for intelligent financial risk analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u91d1\u878d\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7279\u5f81\u6ce8\u610f\u529b\u673a\u5236\u548c\u65f6\u5e8f\u5efa\u6a21\u7ed3\u6784\uff0c\u5728\u4e0d\u5171\u4eab\u539f\u59cb\u6570\u636e\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u8de8\u673a\u6784\u8054\u5408\u5efa\u6a21\uff0c\u6709\u6548\u63d0\u5347\u4e86\u91d1\u878d\u98ce\u9669\u8bc6\u522b\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u8de8\u673a\u6784\u91d1\u878d\u98ce\u9669\u5206\u6790\u4e2d\uff0c\u5b58\u5728\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u548c\u534f\u540c\u5efa\u6a21\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7684\u8054\u5408\u98ce\u9669\u8bc6\u522b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\u3002\u8be5\u6846\u67b6\u91c7\u7528\u5206\u5e03\u5f0f\u4f18\u5316\u7b56\u7565\uff0c\u5404\u91d1\u878d\u673a\u6784\u5728\u672c\u5730\u8bad\u7ec3\u5b50\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u5dee\u5206\u9690\u79c1\u548c\u566a\u58f0\u6ce8\u5165\u4fdd\u62a4\u6a21\u578b\u53c2\u6570\u540e\u4e0a\u4f20\u3002\u4e2d\u592e\u670d\u52a1\u5668\u805a\u5408\u8fd9\u4e9b\u53c2\u6570\u4ee5\u751f\u6210\u5168\u5c40\u6a21\u578b\uff0c\u7528\u4e8e\u8bc6\u522b\u7cfb\u7edf\u6027\u98ce\u9669\u3002\u6a21\u578b\u8bbe\u8ba1\u4e2d\u878d\u5165\u4e86\u7279\u5f81\u6ce8\u610f\u529b\u673a\u5236\u548c\u65f6\u5e8f\u5efa\u6a21\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u901a\u4fe1\u6548\u7387\u3001\u6a21\u578b\u51c6\u786e\u6027\u3001\u7cfb\u7edf\u6027\u98ce\u9669\u68c0\u6d4b\u548c\u8de8\u5e02\u573a\u6cdb\u5316\u80fd\u529b\u65b9\u9762\uff0c\u5747\u4f18\u4e8e\u4f20\u7edf\u7684\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u548c\u73b0\u6709\u7684\u8054\u90a6\u5b66\u4e60\u53d8\u4f53\u3002\u5b83\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u5efa\u6a21\u80fd\u529b\u548c\u5728\u654f\u611f\u91d1\u878d\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u4ef7\u503c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u62a4\u6570\u636e\u4e3b\u6743\u7684\u540c\u65f6\uff0c\u589e\u5f3a\u4e86\u98ce\u9669\u8bc6\u522b\u7684\u5e7f\u5ea6\u548c\u6548\u7387\uff0c\u4e3a\u667a\u80fd\u91d1\u878d\u98ce\u9669\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b89\u5168\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09499", "pdf": "https://arxiv.org/pdf/2508.09499", "abs": "https://arxiv.org/abs/2508.09499", "authors": ["Liyan Jia", "Chuan-Xian Ren", "Hong Yan"], "title": "CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking", "categories": ["cs.CV", "cs.CG", "cs.LG"], "comment": null, "summary": "Accurately predicting the binding conformation of small-molecule ligands to\nprotein targets is a critical step in rational drug design. Although recent\ndeep learning-based docking surpasses traditional methods in speed and\naccuracy, many approaches rely on graph representations and language\nmodel-inspired encoders while neglecting critical geometric information,\nresulting in inaccurate pocket localization and unrealistic binding\nconformations. In this study, we introduce CWFBind, a weighted, fast, and\naccurate docking method based on local curvature features. Specifically, we\nintegrate local curvature descriptors during the feature extraction phase to\nenrich the geometric representation of both proteins and ligands, complementing\nexisting chemical, sequence, and structural features. Furthermore, we embed\ndegree-aware weighting mechanisms into the message passing process, enhancing\nthe model's ability to capture spatial structural distinctions and interaction\nstrengths. To address the class imbalance challenge in pocket prediction,\nCWFBind employs a ligand-aware dynamic radius strategy alongside an enhanced\nloss function, facilitating more precise identification of binding regions and\nkey residues. Comprehensive experimental evaluations demonstrate that CWFBind\nachieves competitive performance across multiple docking benchmarks, offering a\nbalanced trade-off between accuracy and efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCWFBind\uff0c\u4e00\u79cd\u57fa\u4e8e\u5c40\u90e8\u66f2\u7387\u7279\u5f81\u7684\u6df1\u5ea6\u5b66\u4e60\u5bf9\u63a5\u65b9\u6cd5\u3002\u5b83\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u4fe1\u606f\u3001\u52a0\u6743\u673a\u5236\u548c\u52a8\u6001\u534a\u5f84\u7b56\u7565\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u5408\u4f4d\u70b9\u9884\u6d4b\u548c\u6784\u8c61\u751f\u6210\u4e2d\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u5bf9\u63a5\u65b9\u6cd5\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u8bb8\u591a\u65b9\u6cd5\u5ffd\u7565\u4e86\u5173\u952e\u7684\u51e0\u4f55\u4fe1\u606f\uff0c\u5bfc\u81f4\u7ed3\u5408\u53e3\u888b\u5b9a\u4f4d\u4e0d\u51c6\u786e\u548c\u7ed3\u5408\u6784\u8c61\u4e0d\u771f\u5b9e\u3002", "method": "\u5f15\u5165CWFBind\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5c40\u90e8\u66f2\u7387\u7279\u5f81\u7684\u52a0\u6743\u3001\u5feb\u901f\u3001\u51c6\u786e\u7684\u5bf9\u63a5\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\uff1a1. \u5728\u7279\u5f81\u63d0\u53d6\u9636\u6bb5\u6574\u5408\u5c40\u90e8\u66f2\u7387\u63cf\u8ff0\u7b26\uff0c\u4e30\u5bcc\u86cb\u767d\u8d28\u548c\u914d\u4f53\u7684\u51e0\u4f55\u8868\u793a\u30022. \u5728\u6d88\u606f\u4f20\u9012\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u5ea6\u611f\u77e5\u52a0\u6743\u673a\u5236\uff0c\u589e\u5f3a\u6355\u83b7\u7a7a\u95f4\u7ed3\u6784\u5dee\u5f02\u548c\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\u7684\u80fd\u529b\u30023. \u91c7\u7528\u914d\u4f53\u611f\u77e5\u52a8\u6001\u534a\u5f84\u7b56\u7565\u548c\u589e\u5f3a\u635f\u5931\u51fd\u6570\uff0c\u89e3\u51b3\u7ed3\u5408\u53e3\u888b\u9884\u6d4b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "CWFBind\u5728\u591a\u4e2a\u5bf9\u63a5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "CWFBind\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u5e73\u8861\uff0c\u6709\u6548\u6539\u8fdb\u4e86\u836f\u7269\u8bbe\u8ba1\u4e2d\u9884\u6d4b\u914d\u4f53-\u86cb\u767d\u7ed3\u5408\u6784\u8c61\u7684\u80fd\u529b\u3002"}}
{"id": "2508.09294", "pdf": "https://arxiv.org/pdf/2508.09294", "abs": "https://arxiv.org/abs/2508.09294", "authors": ["Xi Xuan", "Zimo Zhu", "Wenxin Zhang", "Yi-Cheng Lin", "Tomi Kinnunen"], "title": "Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted at IEEE ASRU 2025", "summary": "Advances in speech synthesis intensify security threats, motivating real-time\ndeepfake detection research. We investigate whether bidirectional Mamba can\nserve as a competitive alternative to Self-Attention in detecting synthetic\nspeech. Our solution, Fake-Mamba, integrates an XLSR front-end with\nbidirectional Mamba to capture both local and global artifacts. Our core\ninnovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and\nPN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can\neffectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof\n21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and\n5.85% EER, respectively, representing substantial relative gains over SOTA\nmodels XLSR-Conformer and XLSR-Mamba. The framework maintains real-time\ninference across utterance lengths, demonstrating strong generalization and\npractical viability. The code is available at\nhttps://github.com/xuanxixi/Fake-Mamba.", "AI": {"tldr": "Fake-Mamba\u662f\u4e00\u79cd\u57fa\u4e8e\u53cc\u5411Mamba\u548cXLSR\u7684\u5b9e\u65f6\u8bed\u97f3\u4f2a\u9020\u68c0\u6d4b\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u6a21\u578b\u3002", "motivation": "\u8bed\u97f3\u5408\u6210\u6280\u672f\u7684\u8fdb\u6b65\u52a0\u5267\u4e86\u5b89\u5168\u5a01\u80c1\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5b9e\u65f6\u6df1\u5ea6\u4f2a\u9020\u8bed\u97f3\u68c0\u6d4b\u6280\u672f\u3002", "method": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u53cc\u5411Mamba\u4f5c\u4e3a\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u5408\u6210\u8bed\u97f3\u68c0\u6d4b\u4e2d\u7684\u66ff\u4ee3\u6f5c\u529b\u3002\u63d0\u51fa\u4e86Fake-Mamba\u65b9\u6848\uff0c\u5b83\u7ed3\u5408\u4e86XLSR\u524d\u7aef\u548c\u53cc\u5411Mamba\uff0c\u4ee5\u6355\u83b7\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u3002\u6838\u5fc3\u521b\u65b0\u662f\u5f15\u5165\u4e86\u4e09\u79cd\u9ad8\u6548\u7f16\u7801\u5668\uff1aTransBiMamba\u3001ConBiMamba\u548cPN-BiMamba\uff0c\u5176\u4e2dPN-BiMamba\u5229\u7528XLSR\u7684\u8bed\u8a00\u8868\u793a\u6709\u6548\u6355\u83b7\u5408\u6210\u8bed\u97f3\u7684\u7ec6\u5fae\u7ebf\u7d22\u3002", "result": "Fake-Mamba\u5728ASVspoof 21 LA\u300121 DF\u548cIn-The-Wild\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u5b9e\u73b0\u4e860.97%\u30011.74%\u548c5.85%\u7684EER\u3002\u76f8\u8f83\u4e8e\u73b0\u6709\u6700\u4f18\u6a21\u578bXLSR-Conformer\u548cXLSR-Mamba\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347\u3002\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u8bed\u97f3\u957f\u5ea6\u4e0b\u5747\u80fd\u4fdd\u6301\u5b9e\u65f6\u63a8\u7406\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "Fake-Mamba\u65b9\u6848\u8bc1\u660e\u4e86\u53cc\u5411Mamba\u5728\u5408\u6210\u8bed\u97f3\u68c0\u6d4b\u4e2d\u7684\u7ade\u4e89\u529b\uff0c\u4e3a\u5b9e\u65f6\u3001\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u8bed\u97f3\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09401", "pdf": "https://arxiv.org/pdf/2508.09401", "abs": "https://arxiv.org/abs/2508.09401", "authors": ["Yun Zi", "Ming Gong", "Zhihao Xue", "Yujun Zou", "Nia Qi", "Yingnan Deng"], "title": "Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery", "categories": ["cs.LG"], "comment": null, "summary": "This study proposes an unsupervised anomaly detection method for distributed\nbackend service systems, addressing practical challenges such as complex\nstructural dependencies, diverse behavioral evolution, and the absence of\nlabeled data. The method constructs a dynamic graph based on service invocation\nrelationships and applies graph convolution to extract high-order structural\nrepresentations from multi-hop topologies. A Transformer is used to model the\ntemporal behavior of each node, capturing long-term dependencies and local\nfluctuations. During the feature fusion stage, a learnable joint embedding\nmechanism integrates structural and behavioral representations into a unified\nanomaly vector. A nonlinear mapping is then applied to compute anomaly scores,\nenabling an end-to-end detection process without supervision. Experiments on\nreal-world cloud monitoring data include sensitivity analyses across different\ngraph depths, sequence lengths, and data perturbations. Results show that the\nproposed method outperforms existing models on several key metrics,\ndemonstrating stronger expressiveness and stability in capturing anomaly\npropagation paths and modeling dynamic behavior sequences, with high potential\nfor practical deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u5206\u5e03\u5f0f\u540e\u7aef\u670d\u52a1\u7cfb\u7edf\u7684\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u56fe\u5377\u79ef\u548cTransformer\u6a21\u578b\uff0c\u6709\u6548\u5904\u7406\u7ed3\u6784\u4f9d\u8d56\u548c\u65f6\u5e8f\u884c\u4e3a\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u68c0\u6d4b\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u540e\u7aef\u670d\u52a1\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u590d\u6742\u7ed3\u6784\u4f9d\u8d56\u3001\u591a\u6837\u884c\u4e3a\u6f14\u5316\u4ee5\u53ca\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u7b49\u5b9e\u9645\u6311\u6218\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u670d\u52a1\u8c03\u7528\u5173\u7cfb\u7684\u52a8\u6001\u56fe\uff0c\u5229\u7528\u56fe\u5377\u79ef\u63d0\u53d6\u9ad8\u9636\u7ed3\u6784\u8868\u793a\uff1b\u4f7f\u7528Transformer\u6a21\u578b\u5efa\u6a21\u8282\u70b9\u65f6\u95f4\u884c\u4e3a\uff1b\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u8054\u5408\u5d4c\u5165\u673a\u5236\u878d\u5408\u7ed3\u6784\u4e0e\u884c\u4e3a\u8868\u793a\uff1b\u6700\u540e\u5e94\u7528\u975e\u7ebf\u6027\u6620\u5c04\u8ba1\u7b97\u5f02\u5e38\u5206\u6570\uff0c\u5b9e\u73b0\u65e0\u76d1\u7763\u7aef\u5230\u7aef\u68c0\u6d4b\u3002", "result": "\u5728\u771f\u5b9e\u4e91\u76d1\u63a7\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5728\u6355\u83b7\u5f02\u5e38\u4f20\u64ad\u8def\u5f84\u548c\u5efa\u6a21\u52a8\u6001\u884c\u4e3a\u5e8f\u5217\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u5f88\u9ad8\u7684\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\uff0c\u80fd\u6709\u6548\u5e94\u5bf9\u5206\u5e03\u5f0f\u540e\u7aef\u7cfb\u7edf\u7684\u5f02\u5e38\u68c0\u6d4b\u6311\u6218\u3002"}}
{"id": "2508.09522", "pdf": "https://arxiv.org/pdf/2508.09522", "abs": "https://arxiv.org/abs/2508.09522", "authors": ["Ajeet Kumar Yadav", "Nishant Kumar", "Rathna G N"], "title": "Generation of Indian Sign Language Letters, Numbers, and Words", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "6 pages, 5 figures, 2024 International Conference on Intelligent\n  Algorithms for Computational Intelligence Systems (IACIS)", "summary": "Sign language, which contains hand movements, facial expressions and bodily\ngestures, is a significant medium for communicating with hard-of-hearing\npeople. A well-trained sign language community communicates easily, but those\nwho don't know sign language face significant challenges. Recognition and\ngeneration are basic communication methods between hearing and hard-of-hearing\nindividuals. Despite progress in recognition, sign language generation still\nneeds to be explored. The Progressive Growing of Generative Adversarial Network\n(ProGAN) excels at producing high-quality images, while the Self-Attention\nGenerative Adversarial Network (SAGAN) generates feature-rich images at medium\nresolutions. Balancing resolution and detail is crucial for sign language image\ngeneration. We are developing a Generative Adversarial Network (GAN) variant\nthat combines both models to generate feature-rich, high-resolution, and\nclass-conditional sign language images. Our modified Attention-based model\ngenerates high-quality images of Indian Sign Language letters, numbers, and\nwords, outperforming the traditional ProGAN in Inception Score (IS) and\nFr\\'echet Inception Distance (FID), with improvements of 3.2 and 30.12,\nrespectively. Additionally, we are publishing a large dataset incorporating\nhigh-quality images of Indian Sign Language alphabets, numbers, and 129 words.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408ProGAN\u548cSAGAN\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u53d8\u4f53\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u9ad8\u5206\u8fa8\u7387\u3001\u7279\u5f81\u4e30\u5bcc\u7684\u5370\u5ea6\u624b\u8bed\u56fe\u50cf\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfProGAN\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5927\u578b\u5370\u5ea6\u624b\u8bed\u6570\u636e\u96c6\u3002", "motivation": "\u5c3d\u7ba1\u624b\u8bed\u8bc6\u522b\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u624b\u8bed\u751f\u6210\u9886\u57df\u4ecd\u6709\u5f85\u63a2\u7d22\u3002\u73b0\u6709\u751f\u6210\u6a21\u578b\uff08\u5982ProGAN\u64c5\u957f\u9ad8\u5206\u8fa8\u7387\uff0cSAGAN\u64c5\u957f\u7279\u5f81\u4e30\u5bcc\u5ea6\uff09\u96be\u4ee5\u540c\u65f6\u517c\u987e\u624b\u8bed\u56fe\u50cf\u7684\u9ad8\u5206\u8fa8\u7387\u548c\u7ec6\u8282\u4e30\u5bcc\u5ea6\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u624b\u8bed\u56fe\u50cf\u751f\u6210\u6280\u672f\uff0c\u5f25\u8865\u542c\u529b\u5065\u5168\u8005\u4e0e\u542c\u969c\u4eba\u58eb\u4e4b\u95f4\u7684\u6c9f\u901a\u9e3f\u6c9f\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7ed3\u5408ProGAN\u548cSAGAN\u4f18\u70b9\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u53d8\u4f53\u3002\u8be5\u6a21\u578b\u662f\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\uff0c\u65e8\u5728\u751f\u6210\u7279\u5f81\u4e30\u5bcc\u3001\u9ad8\u5206\u8fa8\u7387\u4e14\u7c7b\u522b\u6761\u4ef6\u7684\u5370\u5ea6\u624b\u8bed\u56fe\u50cf\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8005\u8fd8\u521b\u5efa\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b\u5370\u5ea6\u624b\u8bed\u5b57\u6bcd\u3001\u6570\u5b57\u548c129\u4e2a\u5355\u8bcd\u7684\u9ad8\u8d28\u91cf\u5927\u578b\u6570\u636e\u96c6\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6539\u8fdb\u578b\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5370\u5ea6\u624b\u8bed\u5b57\u6bcd\u3001\u6570\u5b57\u548c\u5355\u8bcd\u56fe\u50cf\u3002\u4e0e\u4f20\u7edfProGAN\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728Inception Score (IS) \u4e0a\u63d0\u5347\u4e863.2\uff0c\u5728Fr\u00e9chet Inception Distance (FID) \u4e0a\u63d0\u5347\u4e8630.12\uff0c\u6027\u80fd\u663e\u8457\u4f18\u8d8a\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b\u9ad8\u8d28\u91cf\u5370\u5ea6\u624b\u8bed\u5b57\u6bcd\u3001\u6570\u5b57\u548c129\u4e2a\u5355\u8bcd\u7684\u5927\u578b\u6570\u636e\u96c6\u3002", "conclusion": "\u672c\u7814\u7a76\u5f00\u53d1\u7684GAN\u53d8\u4f53\u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u7279\u5f81\u4e30\u5bcc\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u5370\u5ea6\u624b\u8bed\u56fe\u50cf\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002\u968f\u4e4b\u53d1\u5e03\u7684\u5927\u578b\u6570\u636e\u96c6\u5c06\u8fdb\u4e00\u6b65\u63a8\u52a8\u8be5\u9886\u57df\u7684\u7814\u7a76\uff0c\u4e3a\u542c\u969c\u4eba\u58eb\u63d0\u4f9b\u66f4\u597d\u7684\u6c9f\u901a\u5de5\u5177\u3002"}}
{"id": "2508.09389", "pdf": "https://arxiv.org/pdf/2508.09389", "abs": "https://arxiv.org/abs/2508.09389", "authors": ["Eray Eren", "Qingju Liu", "Hyeongwoo Kim", "Pablo Garrido", "Abeer Alwan"], "title": "ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "Interspeech 2025; demo page at\n  https://promode8272.github.io/promode/index.html", "summary": "Prosody conveys rich emotional and semantic information of the speech signal\nas well as individual idiosyncrasies. We propose a stand-alone model that maps\ntext-to-prosodic features such as F0 and energy and can be used in downstream\ntasks such as TTS. The ProMode encoder takes as input acoustic features and\ntime-aligned textual content, both are partially masked, and obtains a\nfixed-length latent prosodic embedding. The decoder predicts acoustics in the\nmasked region using both the encoded prosody input and unmasked textual\ncontent. Trained on the GigaSpeech dataset, we compare our method with\nstate-of-the-art style encoders. For F0 and energy predictions, we show\nconsistent improvements for our model at different levels of granularity. We\nalso integrate these predicted prosodic features into a TTS system and conduct\nperceptual tests, which show higher prosody preference compared to the\nbaselines, demonstrating the model's potential in tasks where prosody modeling\nis important.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aProMode\u7684\u72ec\u7acb\u6a21\u578b\uff0c\u53ef\u5c06\u6587\u672c\u6620\u5c04\u5230\u97f5\u5f8b\u7279\u5f81\uff08\u5982F0\u548c\u80fd\u91cf\uff09\uff0c\u5e76\u5728TTS\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u66f4\u597d\u7684\u97f5\u5f8b\u504f\u597d\u3002", "motivation": "\u97f5\u5f8b\u5728\u8bed\u97f3\u4fe1\u53f7\u4e2d\u627f\u8f7d\u7740\u4e30\u5bcc\u7684\u60c5\u611f\u3001\u8bed\u4e49\u4fe1\u606f\u548c\u4e2a\u4f53\u7279\u5f81\u3002\u6709\u6548\u5efa\u6a21\u97f5\u5f8b\u5bf9\u4e8e\u6587\u672c\u8f6c\u8bed\u97f3\uff08TTS\uff09\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aProMode\u7684\u72ec\u7acb\u6a21\u578b\u3002\u8be5\u6a21\u578b\u7684\u7f16\u7801\u5668\u4ee5\u90e8\u5206\u63a9\u7801\u7684\u58f0\u5b66\u7279\u5f81\u548c\u65f6\u95f4\u5bf9\u9f50\u7684\u6587\u672c\u5185\u5bb9\u4f5c\u4e3a\u8f93\u5165\uff0c\u751f\u6210\u56fa\u5b9a\u957f\u5ea6\u7684\u6f5c\u5728\u97f5\u5f8b\u5d4c\u5165\u3002\u89e3\u7801\u5668\u5229\u7528\u7f16\u7801\u7684\u97f5\u5f8b\u8f93\u5165\u548c\u672a\u63a9\u7801\u7684\u6587\u672c\u5185\u5bb9\u6765\u9884\u6d4b\u63a9\u7801\u533a\u57df\u7684\u58f0\u5b66\u4fe1\u606f\u3002\u8be5\u6a21\u578b\u5728GigaSpeech\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u98ce\u683c\u7f16\u7801\u5668\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "ProMode\u6a21\u578b\u5728F0\u548c\u80fd\u91cf\u9884\u6d4b\u65b9\u9762\u663e\u793a\u51fa\u5728\u4e0d\u540c\u7c92\u5ea6\u7ea7\u522b\u4e0a\u7684\u4e00\u81f4\u6539\u8fdb\u3002\u5c06\u9884\u6d4b\u7684\u97f5\u5f8b\u7279\u5f81\u6574\u5408\u5230TTS\u7cfb\u7edf\u4e2d\u540e\uff0c\u611f\u77e5\u6d4b\u8bd5\u663e\u793a\u5176\u97f5\u5f8b\u504f\u597d\u9ad8\u4e8e\u57fa\u7ebf\u7cfb\u7edf\u3002", "conclusion": "ProMode\u6a21\u578b\u80fd\u591f\u6709\u6548\u9884\u6d4b\u97f5\u5f8b\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u611f\u77e5\u6d4b\u8bd5\u8bc1\u660e\u4e86\u5176\u5728\u9700\u8981\u7cbe\u786e\u97f5\u5f8b\u5efa\u6a21\u7684\u4efb\u52a1\uff08\u5982TTS\uff09\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.09195", "pdf": "https://arxiv.org/pdf/2508.09195", "abs": "https://arxiv.org/abs/2508.09195", "authors": ["Maria Boyko", "Aleksandra Beliaeva", "Dmitriy Kornilov", "Alexander Bernstein", "Maxim Sharaev"], "title": "impuTMAE: Multi-modal Transformer with Masked Pre-training for Missing Modalities Imputation in Cancer Survival Prediction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "The use of diverse modalities, such as omics, medical images, and clinical\ndata can not only improve the performance of prognostic models but also deepen\nan understanding of disease mechanisms and facilitate the development of novel\ntreatment approaches. However, medical data are complex, often incomplete, and\ncontains missing modalities, making effective handling its crucial for training\nmultimodal models. We introduce impuTMAE, a novel transformer-based end-to-end\napproach with an efficient multimodal pre-training strategy. It learns inter-\nand intra-modal interactions while simultaneously imputing missing modalities\nby reconstructing masked patches. Our model is pre-trained on heterogeneous,\nincomplete data and fine-tuned for glioma survival prediction using\nTCGA-GBM/LGG and BraTS datasets, integrating five modalities: genetic (DNAm,\nRNA-seq), imaging (MRI, WSI), and clinical data. By addressing missing data\nduring pre-training and enabling efficient resource utilization, impuTMAE\nsurpasses prior multimodal approaches, achieving state-of-the-art performance\nin glioma patient survival prediction. Our code is available at\nhttps://github.com/maryjis/mtcp", "AI": {"tldr": "impuTMAE\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u80fd\u6709\u6548\u5904\u7406\u533b\u5b66\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u6a21\u6001\uff0c\u5e76\u5728\u80f6\u8d28\u7624\u751f\u5b58\u9884\u6d4b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u533b\u5b66\u6570\u636e\uff08\u5982\u7ec4\u5b66\u3001\u533b\u5b66\u56fe\u50cf\u3001\u4e34\u5e8a\u6570\u636e\uff09\u53ef\u63d0\u5347\u9884\u540e\u6a21\u578b\u6027\u80fd\u5e76\u6df1\u5316\u5bf9\u75be\u75c5\u673a\u5236\u7684\u7406\u89e3\uff0c\u4f46\u5176\u590d\u6742\u6027\u3001\u4e0d\u5b8c\u6574\u6027\u548c\u7f3a\u5931\u6a21\u6001\u95ee\u9898\u7ed9\u591a\u6a21\u6001\u6a21\u578b\u7684\u8bad\u7ec3\u5e26\u6765\u4e86\u5de8\u5927\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51faimpuTMAE\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eTransformer\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u91c7\u7528\u9ad8\u6548\u7684\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u7b56\u7565\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u91cd\u5efa\u88ab\u906e\u853d\u7684\u8865\u4e01\uff0c\u540c\u65f6\u5b66\u4e60\u6a21\u6001\u95f4\u548c\u6a21\u6001\u5185\u7684\u4ea4\u4e92\uff0c\u5e76\u586b\u8865\u7f3a\u5931\u6a21\u6001\u3002impuTMAE\u5728\u5f02\u6784\u3001\u4e0d\u5b8c\u6574\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u4f7f\u7528TCGA-GBM/LGG\u548cBraTS\u6570\u636e\u96c6\uff0c\u6574\u5408\u9057\u4f20\uff08DNAm, RNA-seq\uff09\u3001\u5f71\u50cf\uff08MRI, WSI\uff09\u548c\u4e34\u5e8a\u6570\u636e\u4e94\u79cd\u6a21\u6001\uff0c\u8fdb\u884c\u80f6\u8d28\u7624\u751f\u5b58\u9884\u6d4b\u7684\u5fae\u8c03\u3002", "result": "\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u6709\u6548\u5904\u7406\u7f3a\u5931\u6570\u636e\u5e76\u5b9e\u73b0\u9ad8\u6548\u8d44\u6e90\u5229\u7528\uff0cimpuTMAE\u8d85\u8d8a\u4e86\u4ee5\u5f80\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u5728\u80f6\u8d28\u7624\u60a3\u8005\u751f\u5b58\u9884\u6d4b\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "impuTMAE\u4e3a\u5229\u7528\u4e0d\u5b8c\u6574\u591a\u6a21\u6001\u533b\u5b66\u6570\u636e\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6709\u6548\u89e3\u51b3\u9884\u8bad\u7ec3\u671f\u95f4\u7f3a\u5931\u6a21\u6001\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80f6\u8d28\u7624\u7b49\u75be\u75c5\u7684\u9884\u540e\u5efa\u6a21\u80fd\u529b\u3002"}}
{"id": "2508.09418", "pdf": "https://arxiv.org/pdf/2508.09418", "abs": "https://arxiv.org/abs/2508.09418", "authors": ["Usman Anjum", "Chris Stockman", "Cat Luong", "Justin Zhan"], "title": "Domain-Generalization to Improve Learning in Meta-Learning Algorithms", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper introduces Domain Generalization Sharpness-Aware Minimization\nModel-Agnostic Meta-Learning (DGS-MAML), a novel meta-learning algorithm\ndesigned to generalize across tasks with limited training data. DGS-MAML\ncombines gradient matching with sharpness-aware minimization in a bi-level\noptimization framework to enhance model adaptability and robustness. We support\nour method with theoretical analysis using PAC-Bayes and convergence\nguarantees. Experimental results on benchmark datasets show that DGS-MAML\noutperforms existing approaches in terms of accuracy and generalization. The\nproposed method is particularly useful for scenarios requiring few-shot\nlearning and quick adaptation, and the source code is publicly available at\nGitHub.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDGS-MAML\uff0c\u4e00\u79cd\u7ed3\u5408\u68af\u5ea6\u5339\u914d\u548c\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\u7684\u65b0\u578b\u5143\u5b66\u4e60\u7b97\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u6709\u9650\u6570\u636e\u4e0b\u7684\u8de8\u4efb\u52a1\u6cdb\u5316\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u8de8\u4efb\u52a1\u6709\u6548\u6cdb\u5316\u7684\u5143\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u5f15\u5165DGS-MAML\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u4e00\u4e2a\u53cc\u5c42\u4f18\u5316\u6846\u67b6\u4e2d\u7ed3\u5408\u4e86\u68af\u5ea6\u5339\u914d\u548c\u9510\u5ea6\u611f\u77e5\u6700\u5c0f\u5316\u3002\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u57fa\u4e8ePAC-Bayes\u548c\u6536\u655b\u6027\u4fdd\u8bc1\u7684\u7406\u8bba\u5206\u6790\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDGS-MAML\u5728\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DGS-MAML\u7b97\u6cd5\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5c0f\u6837\u672c\u5b66\u4e60\u548c\u5feb\u901f\u9002\u5e94\u7684\u573a\u666f\u3002\u5176\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2508.09524", "pdf": "https://arxiv.org/pdf/2508.09524", "abs": "https://arxiv.org/abs/2508.09524", "authors": ["Yipei Wang", "Shiyu Hu", "Shukun Jia", "Panxi Xu", "Hongfei Ma", "Yiping Ma", "Jing Zhang", "Xiaobo Lu", "Xin Zhao"], "title": "SOI is the Root of All Evil: Quantifying and Breaking Similar Object Interference in Single Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we present the first systematic investigation and\nquantification of Similar Object Interference (SOI), a long-overlooked yet\ncritical bottleneck in Single Object Tracking (SOT). Through controlled Online\nInterference Masking (OIM) experiments, we quantitatively demonstrate that\neliminating interference sources leads to substantial performance improvements\n(AUC gains up to 4.35) across all SOTA trackers, directly validating SOI as a\nprimary constraint for robust tracking and highlighting the feasibility of\nexternal cognitive guidance. Building upon these insights, we adopt natural\nlanguage as a practical form of external guidance, and construct SOIBench-the\nfirst semantic cognitive guidance benchmark specifically targeting SOI\nchallenges. It automatically mines SOI frames through multi-tracker collective\njudgment and introduces a multi-level annotation protocol to generate precise\nsemantic guidance texts. Systematic evaluation on SOIBench reveals a striking\nfinding: existing vision-language tracking (VLT) methods fail to effectively\nexploit semantic cognitive guidance, achieving only marginal improvements or\neven performance degradation (AUC changes of -0.26 to +0.71). In contrast, we\npropose a novel paradigm employing large-scale vision-language models (VLM) as\nexternal cognitive engines that can be seamlessly integrated into arbitrary RGB\ntrackers. This approach demonstrates substantial improvements under semantic\ncognitive guidance (AUC gains up to 0.93), representing a significant\nadvancement over existing VLT methods. We hope SOIBench will serve as a\nstandardized evaluation platform to advance semantic cognitive tracking\nresearch and contribute new insights to the tracking research community.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u8c03\u67e5\u5e76\u91cf\u5316\u4e86\u5355\u76ee\u6807\u8ddf\u8e2a\uff08SOT\uff09\u4e2d\u76f8\u4f3c\u7269\u4f53\u5e72\u6270\uff08SOI\uff09\u7684\u5f71\u54cd\u3002\u7814\u7a76\u8bc1\u660eSOI\u662f\u5173\u952e\u74f6\u9888\uff0c\u5e76\u63d0\u51fa\u4e86\u9996\u4e2a\u8bed\u4e49\u8ba4\u77e5\u6307\u5bfc\u57fa\u51c6SOIBench\u3002\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u8ddf\u8e2a\uff08VLT\uff09\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5229\u7528\u8bed\u4e49\u6307\u5bfc\uff0c\u4e3a\u6b64\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u65b0\u8303\u5f0f\uff0c\u8be5\u65b9\u6cd5\u5728\u8bed\u4e49\u6307\u5bfc\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u5355\u76ee\u6807\u8ddf\u8e2a\uff08SOT\uff09\u4e2d\uff0c\u76f8\u4f3c\u7269\u4f53\u5e72\u6270\uff08SOI\uff09\u662f\u4e00\u4e2a\u957f\u671f\u88ab\u5ffd\u89c6\u4f46\u81f3\u5173\u91cd\u8981\u7684\u74f6\u9888\u3002\u7814\u7a76\u65e8\u5728\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u8c03\u67e5\u5e76\u91cf\u5316SOI\u5bf9SOT\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u5916\u90e8\u8ba4\u77e5\u6307\u5bfc\u7684\u53ef\u884c\u6027\u3002", "method": "1. **SOI\u91cf\u5316\uff1a** \u901a\u8fc7\u53d7\u63a7\u7684\u5728\u7ebf\u5e72\u6270\u63a9\u853d\uff08OIM\uff09\u5b9e\u9a8c\uff0c\u5b9a\u91cf\u5206\u6790\u6d88\u9664\u5e72\u6270\u6e90\u5bf9\u8ddf\u8e2a\u6027\u80fd\u7684\u5f71\u54cd\u30022. **\u57fa\u51c6\u6784\u5efa\uff1a** \u6784\u5efa\u4e86\u9996\u4e2a\u9488\u5bf9SOI\u6311\u6218\u7684\u8bed\u4e49\u8ba4\u77e5\u6307\u5bfc\u57fa\u51c6SOIBench\uff0c\u91c7\u7528\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u5916\u90e8\u6307\u5bfc\u5f62\u5f0f\u3002SOIBench\u901a\u8fc7\u591a\u8ffd\u8e2a\u5668\u96c6\u4f53\u5224\u65ad\u81ea\u52a8\u6316\u6398SOI\u5e27\uff0c\u5e76\u91c7\u7528\u591a\u7ea7\u6807\u6ce8\u534f\u8bae\u751f\u6210\u7cbe\u786e\u7684\u8bed\u4e49\u6307\u5bfc\u6587\u672c\u30023. **\u65b0\u8303\u5f0f\u63d0\u51fa\uff1a** \u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4f5c\u4e3a\u5916\u90e8\u8ba4\u77e5\u5f15\u64ce\u7684\u65b0\u8303\u5f0f\uff0c\u8be5\u5f15\u64ce\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u4efb\u610fRGB\u8ffd\u8e2a\u5668\u4e2d\u3002", "result": "1. **SOI\u5f71\u54cd\uff1a** \u6d88\u9664\u5e72\u6270\u6e90\u53ef\u4f7f\u6240\u6709\u73b0\u6709SOTA\u8ffd\u8e2a\u5668\u7684AUC\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe4.35\uff0c\u76f4\u63a5\u9a8c\u8bc1\u4e86SOI\u662f\u9c81\u68d2\u8ddf\u8e2a\u7684\u4e3b\u8981\u5236\u7ea6\u56e0\u7d20\u30022. **\u73b0\u6709VLT\u8868\u73b0\uff1a** \u5728SOIBench\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u8ddf\u8e2a\uff08VLT\uff09\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5229\u7528\u8bed\u4e49\u8ba4\u77e5\u6307\u5bfc\uff0c\u4ec5\u83b7\u5f97\u5fae\u5c0f\u6539\u8fdb\u751a\u81f3\u6027\u80fd\u4e0b\u964d\uff08AUC\u53d8\u5316\u8303\u56f4\u4e3a-0.26\u81f3+0.71\uff09\u30023. **\u6240\u63d0\u65b9\u6cd5\u8868\u73b0\uff1a** \u63d0\u51fa\u7684VLM\u8303\u5f0f\u5728\u8bed\u4e49\u8ba4\u77e5\u6307\u5bfc\u4e0b\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff08AUC\u589e\u76ca\u9ad8\u8fbe0.93\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709VLT\u65b9\u6cd5\u3002", "conclusion": "\u76f8\u4f3c\u7269\u4f53\u5e72\u6270\uff08SOI\uff09\u662f\u5355\u76ee\u6807\u8ddf\u8e2a\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u5236\u7ea6\u56e0\u7d20\u3002\u901a\u8fc7\u5f15\u5165SOIBench\uff0c\u4e3aSOI\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u8bc4\u4f30\u5e73\u53f0\u3002\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5c06\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4f5c\u4e3a\u5916\u90e8\u8ba4\u77e5\u5f15\u64ce\uff0c\u53ef\u4ee5\u6709\u6548\u5229\u7528\u8bed\u4e49\u6307\u5bfc\u6765\u663e\u8457\u63d0\u5347\u8ddf\u8e2a\u6027\u80fd\uff0c\u8fd9\u4e3a\u8bed\u4e49\u8ba4\u77e5\u8ddf\u8e2a\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u548c\u89c1\u89e3\u3002"}}
{"id": "2508.09442", "pdf": "https://arxiv.org/pdf/2508.09442", "abs": "https://arxiv.org/abs/2508.09442", "authors": ["Zhifan Luo", "Shuo Shao", "Su Zhang", "Lijing Zhou", "Yuke Hu", "Chenxu Zhao", "Zhihao Liu", "Zhan Qin"], "title": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e2dKV-cache\u7684\u9690\u79c1\u98ce\u9669\uff0c\u901a\u8fc7\u4e09\u79cd\u653b\u51fb\u5411\u91cf\u8bc1\u660e\u5176\u53ef\u6cc4\u9732\u7528\u6237\u8f93\u5165\uff0c\u5e76\u63d0\u51faKV-Cloak\u9632\u5fa1\u673a\u5236\uff0c\u5728\u786e\u4fdd\u9ad8\u6548\u6027\u548c\u6a21\u578b\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\u6709\u6548\u963b\u65ad\u653b\u51fb\u3002", "motivation": "KV-cache\u4f5c\u4e3aLLM\u63a8\u7406\u52a0\u901f\u7684\u5173\u952e\u673a\u5236\uff0c\u5b58\u5728\u663e\u8457\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u9690\u79c1\u98ce\u9669\uff0c\u5373\u653b\u51fb\u8005\u80fd\u591f\u4eceKV-cache\u4e2d\u91cd\u5efa\u654f\u611f\u7528\u6237\u8f93\u5165\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5bf9\u8fd9\u4e9b\u6f0f\u6d1e\u8fdb\u884c\u6df1\u5165\u5206\u6790\u5e76\u63d0\u51fa\u6709\u6548\u7684\u7f13\u89e3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\uff1a1. \u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e09\u79cd\u653b\u51fb\u5411\u91cf\uff08\u76f4\u63a5\u53cd\u6f14\u653b\u51fb\u3001\u78b0\u649e\u653b\u51fb\u3001\u8bed\u4e49\u6ce8\u5165\u653b\u51fb\uff09\u4ee5\u8bc1\u660eKV-cache\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\u30022. \u8bbe\u8ba1\u5e76\u63d0\u51faKV-Cloak\uff0c\u4e00\u79cd\u57fa\u4e8e\u53ef\u9006\u77e9\u9635\u6df7\u6dc6\u65b9\u6848\u7ed3\u5408\u64cd\u4f5c\u7b26\u878d\u5408\u7684\u8f7b\u91cf\u9ad8\u6548\u9632\u5fa1\u673a\u5236\uff0c\u7528\u4e8e\u4fdd\u62a4KV-cache\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff1a1. \u6240\u63d0\u51fa\u7684\u653b\u51fb\u65b9\u6cd5\u6210\u529f\u5c55\u793a\u4e86KV-cache\u9690\u79c1\u6cc4\u9732\u95ee\u9898\u7684\u5b9e\u9645\u53ef\u884c\u6027\u548c\u4e25\u91cd\u6027\u30022. KV-Cloak\u80fd\u591f\u6709\u6548\u62b5\u5fa1\u6240\u6709\u63d0\u51fa\u7684\u653b\u51fb\uff0c\u5c06\u8f93\u5165\u91cd\u5efa\u8d28\u91cf\u964d\u4f4e\u81f3\u968f\u673a\u566a\u58f0\u6c34\u5e73\u30023. KV-Cloak\u5728\u5b9e\u73b0\u9ad8\u5b89\u5168\u6027\u7684\u540c\u65f6\uff0c\u51e0\u4e4e\u4e0d\u5f71\u54cd\u6a21\u578b\u7cbe\u5ea6\uff0c\u4e14\u6027\u80fd\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "KV-Cloak\u4e3a\u53ef\u4fe1\u8d56\u7684LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u727a\u7272\u6a21\u578b\u6027\u80fd\u548c\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u89e3\u51b3KV-cache\u5e26\u6765\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\u3002"}}
{"id": "2508.09196", "pdf": "https://arxiv.org/pdf/2508.09196", "abs": "https://arxiv.org/abs/2508.09196", "authors": ["Asim Ukaye", "Numan Saeed", "Karthik Nandakumar"], "title": "FIVA: Federated Inverse Variance Averaging for Universal CT Segmentation with Uncertainty Estimation", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "17 pages, 5 figures, Machine Learning for Healthcare Conference", "summary": "Different CT segmentation datasets are typically obtained from different\nscanners under different capture settings and often provide segmentation labels\nfor a limited and often disjoint set of organs. Using these heterogeneous data\neffectively while preserving patient privacy can be challenging. This work\npresents a novel federated learning approach to achieve universal segmentation\nacross diverse abdominal CT datasets by utilizing model uncertainty for\naggregation and predictive uncertainty for inference. Our approach leverages\nthe inherent noise in stochastic mini-batch gradient descent to estimate a\ndistribution over the model weights to provide an on-the-go uncertainty over\nthe model parameters at the client level. The parameters are then aggregated at\nthe server using the additional uncertainty information using a\nBayesian-inspired inverse-variance aggregation scheme. Furthermore, the\nproposed method quantifies prediction uncertainty by propagating the\nuncertainty from the model weights, providing confidence measures essential for\nclinical decision-making. In line with recent work shown, predictive\nuncertainty is utilized in the inference stage to improve predictive\nperformance. Experimental evaluations demonstrate the effectiveness of this\napproach in improving both the quality of federated aggregation and\nuncertainty-weighted inference compared to previously established baselines.\nThe code for this work is made available at: https://github.com/asimukaye/fiva", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u548c\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u5b9e\u73b0\u8de8\u5f02\u6784CT\u6570\u636e\u96c6\u7684\u901a\u7528\u5206\u5272\uff0c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4e0d\u540cCT\u5206\u5272\u6570\u636e\u96c6\u901a\u5e38\u6765\u81ea\u4e0d\u540c\u626b\u63cf\u4eea\u3001\u8bbe\u7f6e\uff0c\u4e14\u5668\u5b98\u6807\u6ce8\u6709\u9650\u6216\u4e0d\u76f8\u4ea4\uff0c\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u5f02\u6784\u6570\u636e\u540c\u65f6\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u6781\u5177\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff1a\u5229\u7528\u968f\u673a\u5c0f\u6279\u91cf\u68af\u5ea6\u4e0b\u964d\u7684\u56fa\u6709\u566a\u58f0\u4f30\u7b97\u5ba2\u6237\u7aef\u6a21\u578b\u6743\u91cd\u4e0d\u786e\u5b9a\u6027\uff1b\u670d\u52a1\u5668\u7aef\u901a\u8fc7\u8d1d\u53f6\u65af\u542f\u53d1\u5f0f\u9006\u65b9\u5dee\u805a\u5408\u65b9\u6848\u6574\u5408\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\uff1b\u901a\u8fc7\u4f20\u64ad\u6a21\u578b\u6743\u91cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u63a8\u7406\u9636\u6bb5\u5229\u7528\u8be5\u4e0d\u786e\u5b9a\u6027\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u8054\u90a6\u805a\u5408\u7684\u8d28\u91cf\u548c\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u63a8\u7406\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5f02\u6784CT\u6570\u636e\uff0c\u5b9e\u73b0\u901a\u7528\u5206\u5272\uff0c\u5e76\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u5fc5\u8981\u7684\u7f6e\u4fe1\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u805a\u5408\u548c\u63a8\u7406\u8868\u73b0\u3002"}}
{"id": "2508.09427", "pdf": "https://arxiv.org/pdf/2508.09427", "abs": "https://arxiv.org/abs/2508.09427", "authors": ["Xiaoyu Li", "Guangyu Tang", "Jiaojiao Jiang"], "title": "Implicit Hypergraph Neural Networks: A Stable Framework for Higher-Order Relational Learning with Provable Guarantees", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Many real-world interactions are group-based rather than pairwise such as\npapers with multiple co-authors and users jointly engaging with items.\nHypergraph neural networks have shown great promise at modeling higher-order\nrelations, but their reliance on a fixed number of explicit message-passing\nlayers limits long-range dependency capture and can destabilize training as\ndepth grows. In this work, we introduce Implicit Hypergraph Neural Networks\n(IHGNN), which bring the implicit equilibrium formulation to hypergraphs:\ninstead of stacking layers, IHGNN computes representations as the solution to a\nnonlinear fixed-point equation, enabling stable and efficient global\npropagation across hyperedges without deep architectures. We develop a\nwell-posed training scheme with provable convergence, analyze the oversmoothing\nconditions and expressivity of the model, and derive a transductive\ngeneralization bound on hypergraphs. We further present an implicit-gradient\ntraining procedure coupled with a projection-based stabilization strategy.\nExtensive experiments on citation benchmarks show that IHGNN consistently\noutperforms strong traditional graph/hypergraph neural network baselines in\nboth accuracy and robustness. Empirically, IHGNN is resilient to random\ninitialization and hyperparameter variation, highlighting its strong\ngeneralization and practical value for higher-order relational learning.", "AI": {"tldr": "\u63d0\u51fa\u9690\u5f0f\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\uff08IHGNN\uff09\uff0c\u901a\u8fc7\u4e0d\u52a8\u70b9\u65b9\u7a0b\u89e3\u51b3\u4f20\u7edf\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u6df1\u5c42\u7ed3\u6784\u4e2d\u957f\u8ddd\u79bb\u4f9d\u8d56\u6355\u83b7\u53d7\u9650\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HNNs\uff09\u5728\u5efa\u6a21\u9ad8\u9636\u5173\u7cfb\u65b9\u9762\u524d\u666f\u5e7f\u9614\uff0c\u4f46\u5176\u56fa\u5b9a\u5c42\u6570\u7684\u663e\u5f0f\u6d88\u606f\u4f20\u9012\u673a\u5236\u9650\u5236\u4e86\u957f\u8ddd\u79bb\u4f9d\u8d56\u7684\u6355\u83b7\uff0c\u4e14\u968f\u7740\u7f51\u7edc\u6df1\u5ea6\u589e\u52a0\uff0c\u8bad\u7ec3\u53ef\u80fd\u53d8\u5f97\u4e0d\u7a33\u5b9a\u3002", "method": "\u5f15\u5165\u9690\u5f0f\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\uff08IHGNN\uff09\uff0c\u5c06\u9690\u5f0f\u5e73\u8861\u516c\u5f0f\u5e94\u7528\u4e8e\u8d85\u56fe\u3002IHGNN\u901a\u8fc7\u6c42\u89e3\u975e\u7ebf\u6027\u4e0d\u52a8\u70b9\u65b9\u7a0b\u6765\u8ba1\u7b97\u8282\u70b9\u8868\u793a\uff0c\u4ece\u800c\u5728\u4e0d\u4f9d\u8d56\u6df1\u5c42\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u7684\u8d85\u8fb9\u5168\u5c40\u4f20\u64ad\u3002\u7814\u7a76\u8fd8\u5f00\u53d1\u4e86\u53ef\u8bc1\u660e\u6536\u655b\u7684\u8bad\u7ec3\u65b9\u6848\uff0c\u5206\u6790\u4e86\u6a21\u578b\u7684\u8fc7\u5e73\u6ed1\u6761\u4ef6\u548c\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u63a8\u5bfc\u4e86\u8d85\u56fe\u4e0a\u7684\u8f6c\u5bfc\u6cdb\u5316\u754c\u9650\u3002\u6b64\u5916\uff0c\u91c7\u7528\u9690\u5f0f\u68af\u5ea6\u8bad\u7ec3\u8fc7\u7a0b\u7ed3\u5408\u57fa\u4e8e\u6295\u5f71\u7684\u7a33\u5b9a\u7b56\u7565\u3002", "result": "\u5728\u5f15\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cIHGNN\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u5f3a\u5927\u7684\u4f20\u7edf\u56fe/\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\u57fa\u7ebf\u6a21\u578b\u3002\u5b9e\u9a8c\u8bc1\u660eIHGNN\u5bf9\u968f\u673a\u521d\u59cb\u5316\u548c\u8d85\u53c2\u6570\u53d8\u5316\u5177\u6709\u5f39\u6027\u3002", "conclusion": "IHGNN\u4e3a\u9ad8\u9636\u5173\u7cfb\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u7528\u4ef7\u503c\u7684\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u8d85\u56fe\u7f51\u7edc\u5728\u6df1\u5c42\u8bad\u7ec3\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2508.09525", "pdf": "https://arxiv.org/pdf/2508.09525", "abs": "https://arxiv.org/abs/2508.09525", "authors": ["Yuxin Mao", "Zhen Qin", "Jinxing Zhou", "Bin Fan", "Jing Zhang", "Yiran Zhong", "Yuchao Dai"], "title": "Learning Spatial Decay for Vision Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Vision Transformers (ViTs) have revolutionized computer vision, yet their\nself-attention mechanism lacks explicit spatial inductive biases, leading to\nsuboptimal performance on spatially-structured tasks. Existing approaches\nintroduce data-independent spatial decay based on fixed distance metrics,\napplying uniform attention weighting regardless of image content and limiting\nadaptability to diverse visual scenarios. Inspired by recent advances in large\nlanguage models where content-aware gating mechanisms (e.g., GLA, HGRN2, FOX)\nsignificantly outperform static alternatives, we present the first successful\nadaptation of data-dependent spatial decay to 2D vision transformers. We\nintroduce \\textbf{Spatial Decay Transformer (SDT)}, featuring a novel\nContext-Aware Gating (CAG) mechanism that generates dynamic, data-dependent\ndecay for patch interactions. Our approach learns to modulate spatial attention\nbased on both content relevance and spatial proximity. We address the\nfundamental challenge of 1D-to-2D adaptation through a unified spatial-content\nfusion framework that integrates manhattan distance-based spatial priors with\nlearned content representations. Extensive experiments on ImageNet-1K\nclassification and generation tasks demonstrate consistent improvements over\nstrong baselines. Our work establishes data-dependent spatial decay as a new\nparadigm for enhancing spatial attention in vision transformers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSpatial Decay Transformer (SDT)\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u95e8\u63a7(CAG)\u673a\u5236\uff0c\u5728Vision Transformers (ViTs)\u4e2d\u5b9e\u73b0\u4e86\u6570\u636e\u4f9d\u8d56\u7684\u7a7a\u95f4\u8870\u51cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86ViT\u7f3a\u4e4f\u7a7a\u95f4\u5f52\u7eb3\u504f\u7f6e\u7684\u95ee\u9898\u3002", "motivation": "Vision Transformers (ViTs)\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7f3a\u4e4f\u663e\u5f0f\u7a7a\u95f4\u5f52\u7eb3\u504f\u7f6e\uff0c\u5bfc\u81f4\u5728\u7a7a\u95f4\u7ed3\u6784\u5316\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u6570\u636e\u65e0\u5173\u7684\u56fa\u5b9a\u7a7a\u95f4\u8870\u51cf\uff0c\u9002\u5e94\u6027\u5dee\u3002\u53d7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5185\u5bb9\u611f\u77e5\u95e8\u63a7\u673a\u5236\u6210\u529f\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u65e8\u5728\u4e3a2D\u89c6\u89c9Transformer\u5f15\u5165\u6570\u636e\u4f9d\u8d56\u7684\u7a7a\u95f4\u8870\u51cf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Spatial Decay Transformer (SDT)\uff0c\u5176\u6838\u5fc3\u662fContext-Aware Gating (CAG)\u673a\u5236\u3002CAG\u80fd\u6839\u636e\u5185\u5bb9\u76f8\u5173\u6027\u548c\u7a7a\u95f4\u63a5\u8fd1\u5ea6\uff0c\u751f\u6210\u52a8\u6001\u3001\u6570\u636e\u4f9d\u8d56\u7684\u8865\u4e01\u95f4\u8870\u51cf\u3002\u901a\u8fc7\u7edf\u4e00\u7684\u7a7a\u95f4-\u5185\u5bb9\u878d\u5408\u6846\u67b6\uff08\u7ed3\u5408\u66fc\u54c8\u987f\u8ddd\u79bb\u7684\u7a7a\u95f4\u5148\u9a8c\u548c\u5b66\u4e60\u5230\u7684\u5185\u5bb9\u8868\u793a\uff09\uff0c\u89e3\u51b3\u4e86\u4ece1D\u52302D\u7684\u9002\u5e94\u6027\u95ee\u9898\u3002", "result": "\u5728ImageNet-1K\u56fe\u50cf\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u4e0a\uff0cSDT\u76f8\u5bf9\u4e8e\u73b0\u6709\u5f3a\u57fa\u7ebf\u6a21\u578b\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5c06\u6570\u636e\u4f9d\u8d56\u7684\u7a7a\u95f4\u8870\u51cf\u5f15\u51652D\u89c6\u89c9Transformer\uff0c\u786e\u7acb\u4e86\u5176\u4f5c\u4e3a\u589e\u5f3aViTs\u7a7a\u95f4\u6ce8\u610f\u529b\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.09447", "pdf": "https://arxiv.org/pdf/2508.09447", "abs": "https://arxiv.org/abs/2508.09447", "authors": ["Siddharth Srikanth", "John Krumm", "Jonathan Qin"], "title": "NEXICA: Discovering Road Traffic Causality (Extended arXiv Version)", "categories": ["cs.LG"], "comment": "Extended version of short paper in 32nd ACM SIGSPATIAL International\n  Conference on Advances in Geographic Information Systems (ACM SIGSPATIAL\n  2024)", "summary": "Road traffic congestion is a persistent problem. Focusing resources on the\ncauses of congestion is a potentially efficient strategy for reducing\nslowdowns. We present NEXICA, an algorithm to discover which parts of the\nhighway system tend to cause slowdowns on other parts of the highway. We use\ntime series of road speeds as inputs to our causal discovery algorithm. Finding\nother algorithms inadequate, we develop a new approach that is novel in three\nways. First, it concentrates on just the presence or absence of events in the\ntime series, where an event indicates the temporal beginning of a traffic\nslowdown. Second, we develop a probabilistic model using maximum likelihood\nestimation to compute the probabilities of spontaneous and caused slowdowns\nbetween two locations on the highway. Third, we train a binary classifier to\nidentify pairs of cause/effect locations trained on pairs of road locations\nwhere we are reasonably certain a priori of their causal connections, both\npositive and negative. We test our approach on six months of road speed data\nfrom 195 different highway speed sensors in the Los Angeles area, showing that\nour approach is superior to state-of-the-art baselines in both accuracy and\ncomputation speed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNEXICA\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u5206\u6790\u9053\u8def\u901f\u5ea6\u65f6\u95f4\u5e8f\u5217\uff0c\u8bc6\u522b\u9ad8\u901f\u516c\u8def\u7cfb\u7edf\u4e2d\u5bfc\u81f4\u4ea4\u901a\u51cf\u901f\u7684\u56e0\u679c\u6e90\u5934\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u901f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u9053\u8def\u4ea4\u901a\u62e5\u5835\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u95ee\u9898\u3002\u8bc6\u522b\u548c\u9488\u5bf9\u62e5\u5835\u7684\u6839\u672c\u539f\u56e0\u91c7\u53d6\u63aa\u65bd\uff0c\u662f\u6709\u6548\u7f13\u89e3\u4ea4\u901a\u51cf\u901f\u7684\u7b56\u7565\u3002\u73b0\u6709\u56e0\u679c\u53d1\u73b0\u7b97\u6cd5\u5728\u5904\u7406\u4ea4\u901a\u6570\u636e\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86NEXICA\u7b97\u6cd5\uff0c\u4ee5\u9053\u8def\u901f\u5ea6\u65f6\u95f4\u5e8f\u5217\u4e3a\u8f93\u5165\u8fdb\u884c\u56e0\u679c\u53d1\u73b0\u3002\u8be5\u65b9\u6cd5\u6709\u4e09\u4e2a\u521b\u65b0\u70b9\uff1a1) \u4ec5\u5173\u6ce8\u65f6\u95f4\u5e8f\u5217\u4e2d\u4ea4\u901a\u51cf\u901f\u4e8b\u4ef6\u7684\u53d1\u751f\u4e0e\u5426\uff1b2) \u5efa\u7acb\u6982\u7387\u6a21\u578b\uff0c\u4f7f\u7528\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u8ba1\u7b97\u81ea\u53d1\u548c\u88ab\u5f15\u8d77\u7684\u51cf\u901f\u6982\u7387\uff1b3) \u8bad\u7ec3\u4e8c\u5143\u5206\u7c7b\u5668\u8bc6\u522b\u56e0\u679c\u4f4d\u7f6e\u5bf9\u3002\u7814\u7a76\u5728\u6d1b\u6749\u77f6\u5730\u533a195\u4e2a\u9ad8\u901f\u516c\u8def\u901f\u5ea6\u4f20\u611f\u5668\u91c7\u96c6\u7684\u516d\u4e2a\u6708\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u5728\u6d1b\u6749\u77f6\u5730\u533a\u7684\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u8868\u660e\uff0cNEXICA\u7b97\u6cd5\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u901f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "NEXICA\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u53d1\u73b0\u9ad8\u901f\u516c\u8def\u4ea4\u901a\u62e5\u5835\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u4e3a\u7f13\u89e3\u4ea4\u901a\u62e5\u5835\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2508.09528", "pdf": "https://arxiv.org/pdf/2508.09528", "abs": "https://arxiv.org/abs/2508.09528", "authors": ["Gang Qu", "Ping Wang", "Siming Zheng", "Xin Yuan"], "title": "Physics-guided Deep Unfolding Network for Enhanced Kronecker Compressive sensing", "categories": ["cs.CV"], "comment": "9 pages, 4 figures", "summary": "Deep networks have achieved remarkable success in image compressed sensing\n(CS) task, namely reconstructing a high-fidelity image from its compressed\nmeasurement. However, existing works are deficient inincoherent compressed\nmeasurement at sensing phase and implicit measurement representations at\nreconstruction phase, limiting the overall performance. In this work, we answer\ntwo questions: 1) how to improve the measurement incoherence for decreasing the\nill-posedness; 2) how to learn informative representations from measurements.\nTo this end, we propose a novel asymmetric Kronecker CS (AKCS) model and\ntheoretically present its better incoherence than previous Kronecker CS with\nminimal complexity increase. Moreover, we reveal that the unfolding networks'\nsuperiority over non-unfolding ones result from sufficient gradient descents,\ncalled explicit measurement representations. We propose a measurement-aware\ncross attention (MACA) mechanism to learn implicit measurement representations.\nWe integrate AKCS and MACA into widely-used unfolding architecture to get a\nmeasurement-enhanced unfolding network (MEUNet). Extensive experiences\ndemonstrate that our MEUNet achieves state-of-the-art performance in\nreconstruction accuracy and inference speed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u6d4b\u91cf\u589e\u5f3a\u5c55\u5f00\u7f51\u7edc\uff08MEUNet\uff09\uff0c\u901a\u8fc7\u4e0d\u5bf9\u79f0Kronecker CS\u6a21\u578b\u548c\u6d4b\u91cf\u611f\u77e5\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u56fe\u50cf\u538b\u7f29\u611f\u77e5\u4e2d\u6d4b\u91cf\u4e0d\u76f8\u5e72\u6027\u548c\u8868\u793a\u5b66\u4e60\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u91cd\u5efa\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u7f51\u7edc\u7684\u56fe\u50cf\u538b\u7f29\u611f\u77e5\u65b9\u6cd5\uff0c\u5728\u611f\u77e5\u9636\u6bb5\u5b58\u5728\u4e0d\u76f8\u5e72\u7684\u538b\u7f29\u6d4b\u91cf\uff0c\u5728\u91cd\u5efa\u9636\u6bb5\u5b58\u5728\u9690\u5f0f\u7684\u6d4b\u91cf\u8868\u793a\uff0c\u8fd9\u4e9b\u7f3a\u9677\u9650\u5236\u4e86\u6574\u4f53\u6027\u80fd\u3002", "method": "1. \u63d0\u51fa\u4e0d\u5bf9\u79f0Kronecker CS (AKCS) \u6a21\u578b\u4ee5\u63d0\u9ad8\u6d4b\u91cf\u4e0d\u76f8\u5e72\u6027\u30022. \u63ed\u793a\u5c55\u5f00\u7f51\u7edc\u4f18\u4e8e\u975e\u5c55\u5f00\u7f51\u7edc\u7684\u539f\u56e0\u5728\u4e8e\u5145\u5206\u7684\u68af\u5ea6\u4e0b\u964d\uff08\u5373\u663e\u5f0f\u6d4b\u91cf\u8868\u793a\uff09\u30023. \u63d0\u51fa\u6d4b\u91cf\u611f\u77e5\u4ea4\u53c9\u6ce8\u610f\u529b (MACA) \u673a\u5236\u6765\u5b66\u4e60\u9690\u5f0f\u6d4b\u91cf\u8868\u793a\u30024. \u5c06AKCS\u548cMACA\u96c6\u6210\u5230\u5c55\u5f00\u67b6\u6784\u4e2d\uff0c\u6784\u5efa\u4e86\u6d4b\u91cf\u589e\u5f3a\u5c55\u5f00\u7f51\u7edc (MEUNet)\u3002", "result": "1. AKCS\u5728\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u6bd4\u73b0\u6709Kronecker CS\u66f4\u597d\u7684\u4e0d\u76f8\u5e72\u6027\uff0c\u4e14\u590d\u6742\u5ea6\u589e\u52a0\u6700\u5c0f\u30022. MEUNet\u5728\u91cd\u5efa\u7cbe\u5ea6\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5728\u6d4b\u91cf\u4e0d\u76f8\u5e72\u6027\uff08AKCS\uff09\u548c\u6d4b\u91cf\u8868\u793a\u5b66\u4e60\uff08MACA\uff09\u65b9\u9762\u7684\u521b\u65b0\uff0cMEUNet\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u538b\u7f29\u611f\u77e5\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f73\u6c34\u5e73\u3002"}}
{"id": "2508.09473", "pdf": "https://arxiv.org/pdf/2508.09473", "abs": "https://arxiv.org/abs/2508.09473", "authors": ["Birong Pan", "Mayi Xu", "Qiankun Pi", "Jianhao Chen", "Yuanyuan Zhu", "Ming Zhong", "Tieyun Qian"], "title": "NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Ensuring robust safety alignment while preserving utility is critical for the\nreliable deployment of Large Language Models (LLMs). However, current\ntechniques fundamentally suffer from intertwined deficiencies: insufficient\nrobustness against malicious attacks, frequent refusal of benign queries,\ndegradation in generated text quality and general task performance--the former\ntwo reflecting deficits in robust safety and the latter constituting utility\nimpairment. We trace these limitations to the coarse-grained layer-wise\ninterventions in existing methods. To resolve this, we propose NeuronTune, a\nfine-grained framework that dynamically modulates sparse neurons to achieve\nsimultaneous safety-utility optimization. Our approach first identifies\nsafety-critical and utility-preserving neurons across all layers via\nattribution, then employs meta-learning to adaptively amplify safety-neuron\nactivations and suppress utility-neuron activations. Crucially, NeuronTune\nenables tunable adjustment of intervention scope via neuron-count thresholds,\nsupporting flexible adaptation to security-critical or utility-priority\nscenarios. Extensive experimental results demonstrate that our method\nsignificantly outperforms existing state-of-the-art technologies, achieving\nsuperior model safety while maintaining excellent utility.", "AI": {"tldr": "NeuronTune\u901a\u8fc7\u52a8\u6001\u8c03\u8282\u7a00\u758f\u795e\u7ecf\u5143\uff0c\u5b9e\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u7684\u540c\u6b65\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b89\u5168\u5bf9\u9f50\u6280\u672f\u5b58\u5728\u9c81\u68d2\u6027\u4e0d\u8db3\u3001\u8bef\u62d2\u826f\u6027\u67e5\u8be2\u3001\u6587\u672c\u8d28\u91cf\u548c\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\u7b49\u95ee\u9898\uff0c\u8fd9\u4e9b\u7f3a\u9677\u6e90\u4e8e\u7c97\u7c92\u5ea6\u7684\u5c42\u7ea7\u5e72\u9884\uff0c\u5bfc\u81f4\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u96be\u4ee5\u517c\u987e\u3002", "method": "\u63d0\u51faNeuronTune\u6846\u67b6\uff0c\u901a\u8fc7\u5f52\u56e0\u65b9\u6cd5\u8bc6\u522b\u6240\u6709\u5c42\u4e2d\u5bf9\u5b89\u5168\u6027\u5173\u952e\u548c\u5bf9\u5b9e\u7528\u6027\u6709\u76ca\u7684\u795e\u7ecf\u5143\u3002\u7136\u540e\uff0c\u5229\u7528\u5143\u5b66\u4e60\uff08meta-learning\uff09\u81ea\u9002\u5e94\u5730\u589e\u5f3a\u5b89\u5168\u795e\u7ecf\u5143\u7684\u6fc0\u6d3b\u5e76\u6291\u5236\u5b9e\u7528\u795e\u7ecf\u5143\u7684\u6fc0\u6d3b\u3002\u8be5\u65b9\u6cd5\u8fd8\u652f\u6301\u901a\u8fc7\u795e\u7ecf\u5143\u6570\u91cf\u9608\u503c\u7075\u6d3b\u8c03\u6574\u5e72\u9884\u8303\u56f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cNeuronTune\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6280\u672f\uff0c\u5728\u5b9e\u73b0\u5353\u8d8a\u6a21\u578b\u5b89\u5168\u6027\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u51fa\u8272\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "NeuronTune\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ec6\u7c92\u5ea6\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4e24\u8005\u7684\u540c\u6b65\u4f18\u5316\u3002"}}
{"id": "2508.09451", "pdf": "https://arxiv.org/pdf/2508.09451", "abs": "https://arxiv.org/abs/2508.09451", "authors": ["Ziyu Liu", "Azadeh Alavi", "Minyi Li", "Xiang Zhang"], "title": "A Unified Contrastive-Generative Framework for Time Series Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Self-supervised learning (SSL) for multivariate time series mainly includes\ntwo paradigms: contrastive methods that excel at instance discrimination and\ngenerative approaches that model data distributions. While effective\nindividually, their complementary potential remains unexplored. We propose a\nContrastive Generative Time series framework (CoGenT), the first framework to\nunify these paradigms through joint contrastive-generative optimization. CoGenT\naddresses fundamental limitations of both approaches: it overcomes contrastive\nlearning's sensitivity to high intra-class similarity in temporal data while\nreducing generative methods' dependence on large datasets. We evaluate CoGenT\non six diverse time series datasets. The results show consistent improvements,\nwith up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE,\nrespectively. Our analysis reveals that the hybrid objective preserves\ndiscriminative power while acquiring generative robustness. These findings\nestablish a foundation for hybrid SSL in temporal domains. We will release the\ncode shortly.", "AI": {"tldr": "\u63d0\u51faCoGenT\u6846\u67b6\uff0c\u9996\u6b21\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5c06\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u751f\u6210\u5f0f\u5b66\u4e60\u8303\u5f0f\u7ed3\u5408\uff0c\u7528\u4e8e\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u3002", "motivation": "\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u4e3b\u8981\u6709\u5bf9\u6bd4\u65b9\u6cd5\u548c\u751f\u6210\u65b9\u6cd5\u4e24\u79cd\u8303\u5f0f\uff0c\u5b83\u4eec\u5404\u81ea\u6709\u6548\u4f46\u5c1a\u672a\u63a2\u7d22\u5176\u4e92\u8865\u6f5c\u529b\u3002", "method": "\u63d0\u51faCoGenT\uff08Contrastive Generative Time series framework\uff09\uff0c\u901a\u8fc7\u8054\u5408\u5bf9\u6bd4-\u751f\u6210\u4f18\u5316\u7edf\u4e00\u4e24\u79cd\u8303\u5f0f\u3002\u8be5\u6846\u67b6\u65e8\u5728\u514b\u670d\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u65f6\u95f4\u6570\u636e\u9ad8\u7c7b\u5185\u76f8\u4f3c\u6027\u7684\u654f\u611f\u6027\uff0c\u5e76\u51cf\u5c11\u751f\u6210\u65b9\u6cd5\u5bf9\u5927\u578b\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0cCoGenT\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u76f8\u5bf9\u4e8e\u72ec\u7acb\u7684SimCLR\u548cMAE\uff0cF1\u5206\u6570\u5206\u522b\u63d0\u5347\u9ad8\u8fbe59.2%\u548c14.27%\u3002", "conclusion": "\u6df7\u5408\u76ee\u6807\u80fd\u591f\u540c\u65f6\u4fdd\u6301\u5224\u522b\u80fd\u529b\u5e76\u83b7\u5f97\u751f\u6210\u9c81\u68d2\u6027\uff0c\u4e3a\u65f6\u95f4\u57df\u7684\u6df7\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.09533", "pdf": "https://arxiv.org/pdf/2508.09533", "abs": "https://arxiv.org/abs/2508.09533", "authors": ["Peiran Peng", "Tingfa Xu", "Liqiang Song", "Mengqi Zhu", "Yuqiang Fang", "Jianan Li"], "title": "COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detecting tiny objects in multimodal Red-Green-Blue-Thermal (RGBT) imagery is\na critical challenge in computer vision, particularly in surveillance, search\nand rescue, and autonomous navigation. Drone-based scenarios exacerbate these\nchallenges due to spatial misalignment, low-light conditions, occlusion, and\ncluttered backgrounds. Current methods struggle to leverage the complementary\ninformation between visible and thermal modalities effectively. We propose\nCOXNet, a novel framework for RGBT tiny object detection, addressing these\nissues through three core innovations: i) the Cross-Layer Fusion Module, fusing\nhigh-level visible and low-level thermal features for enhanced semantic and\nspatial accuracy; ii) the Dynamic Alignment and Scale Refinement module,\ncorrecting cross-modal spatial misalignments and preserving multi-scale\nfeatures; and iii) an optimized label assignment strategy using the GeoShape\nSimilarity Measure for better localization. COXNet achieves a 3.32\\% mAP$_{50}$\nimprovement on the RGBTDronePerson dataset over state-of-the-art methods,\ndemonstrating its effectiveness for robust detection in complex environments.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684RGBT\u5fae\u5c0f\u76ee\u6807\u68c0\u6d4b\u6846\u67b6COXNet\uff0c\u901a\u8fc7\u8de8\u5c42\u7279\u5f81\u878d\u5408\u3001\u52a8\u6001\u5bf9\u9f50\u548c\u4f18\u5316\u6807\u7b7e\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u65e0\u4eba\u673a\u573a\u666f\u4e0b\u5fae\u5c0f\u76ee\u6807\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u5728\u65e0\u4eba\u673aRGBT\u56fe\u50cf\u4e2d\u68c0\u6d4b\u5fae\u5c0f\u76ee\u6807\u6781\u5177\u6311\u6218\u6027\uff0c\u9762\u4e34\u7a7a\u95f4\u9519\u4f4d\u3001\u4f4e\u5149\u3001\u906e\u6321\u548c\u80cc\u666f\u6742\u4e71\u7b49\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5229\u7528\u53ef\u89c1\u5149\u548c\u70ed\u6210\u50cf\u6a21\u6001\u7684\u4e92\u8865\u4fe1\u606f\u3002", "method": "\u63d0\u51faCOXNet\u6846\u67b6\uff0c\u5305\u542b\u4e09\u9879\u6838\u5fc3\u521b\u65b0\uff1a1) \u8de8\u5c42\u878d\u5408\u6a21\u5757\uff0c\u878d\u5408\u9ad8\u5c42\u53ef\u89c1\u5149\u548c\u4f4e\u5c42\u70ed\u6210\u50cf\u7279\u5f81\uff1b2) \u52a8\u6001\u5bf9\u9f50\u4e0e\u5c3a\u5ea6\u7ec6\u5316\u6a21\u5757\uff0c\u6821\u6b63\u8de8\u6a21\u6001\u7a7a\u95f4\u9519\u4f4d\u5e76\u4fdd\u7559\u591a\u5c3a\u5ea6\u7279\u5f81\uff1b3) \u57fa\u4e8eGeoShape\u76f8\u4f3c\u6027\u5ea6\u91cf\u7684\u4f18\u5316\u6807\u7b7e\u5206\u914d\u7b56\u7565\u3002", "result": "\u5728RGBTDronePerson\u6570\u636e\u96c6\u4e0a\uff0cCOXNet\u7684mAP$_{50}$\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u5347\u4e863.32%\u3002", "conclusion": "COXNet\u80fd\u591f\u6709\u6548\u5b9e\u73b0RGBT\u56fe\u50cf\u4e2d\u5fae\u5c0f\u76ee\u6807\u7684\u9c81\u68d2\u68c0\u6d4b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e0b\u7684\u5e94\u7528\u3002"}}
{"id": "2508.09535", "pdf": "https://arxiv.org/pdf/2508.09535", "abs": "https://arxiv.org/abs/2508.09535", "authors": ["Roberto Balestri"], "title": "AI Blob! LLM-Driven Recontextualization of Italian Television Archives", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.DL"], "comment": "Preprint", "summary": "This paper introduces AI Blob!, an experimental system designed to explore\nthe potential of semantic cataloging and Large Language Models (LLMs) for the\nretrieval and recontextualization of archival television footage. Drawing\nmethodological inspiration from Italian television programs such as Blob (RAI\nTre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic\nembeddings, and retrieval-augmented generation (RAG) to organize and\nreinterpret archival content. The system processes a curated dataset of 1,547\nItalian television videos by transcribing audio, segmenting it into\nsentence-level units, and embedding these segments into a vector database for\nsemantic querying. Upon user input of a thematic prompt, the LLM generates a\nrange of linguistically and conceptually related queries, guiding the retrieval\nand recombination of audiovisual fragments. These fragments are algorithmically\nselected and structured into narrative sequences producing montages that\nemulate editorial practices of ironic juxtaposition and thematic coherence. By\nforegrounding dynamic, content-aware retrieval over static metadata schemas, AI\nBlob! demonstrates how semantic technologies can facilitate new approaches to\narchival engagement, enabling novel forms of automated narrative construction\nand cultural analysis. The project contributes to ongoing debates in media\nhistoriography and AI-driven archival research, offering both a conceptual\nframework and a publicly available dataset to support further interdisciplinary\nexperimentation.", "AI": {"tldr": "AI Blob!\u662f\u4e00\u4e2a\u5229\u7528LLM\u548c\u8bed\u4e49\u6280\u672f\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u8f6c\u5f55\u3001\u5d4c\u5165\u548cRAG\uff0c\u4ece\u5927\u91cf\u7535\u89c6\u6863\u6848\u4e2d\u667a\u80fd\u68c0\u7d22\u3001\u91cd\u7ec4\u5e76\u751f\u6210\u53d9\u4e8b\u8499\u592a\u5947\u7684\u5b9e\u9a8c\u7cfb\u7edf\u3002", "motivation": "\u63a2\u7d22\u8bed\u4e49\u7f16\u76ee\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u68c0\u7d22\u548c\u91cd\u6784\u7535\u89c6\u6863\u6848\u7247\u6bb5\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u4fc3\u8fdb\u65b0\u7684\u6863\u6848\u4e92\u52a8\u65b9\u5f0f\u548c\u81ea\u52a8\u5316\u53d9\u4e8b\u6784\u5efa\u3002", "method": "\u8be5\u7cfb\u7edf\u5904\u74061,547\u4e2a\u610f\u5927\u5229\u7535\u89c6\u89c6\u9891\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u8f6c\u5f55\u97f3\u9891\uff0c\u5c06\u5176\u5206\u5272\u6210\u53e5\u5b50\u7ea7\u5355\u5143\u5e76\u5d4c\u5165\u5411\u91cf\u6570\u636e\u5e93\u8fdb\u884c\u8bed\u4e49\u67e5\u8be2\u3002\u7528\u6237\u8f93\u5165\u4e3b\u9898\u63d0\u793a\u540e\uff0cLLM\u751f\u6210\u76f8\u5173\u67e5\u8be2\uff0c\u7136\u540e\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u68c0\u7d22\u5e76\u91cd\u65b0\u7ec4\u5408\u89c6\u542c\u7247\u6bb5\uff0c\u7b97\u6cd5\u9009\u62e9\u5e76\u6784\u5efa\u6210\u6a21\u62df\u8bbd\u523a\u5e76\u7f6e\u548c\u4e3b\u9898\u8fde\u8d2f\u6027\u7684\u53d9\u4e8b\u8499\u592a\u5947\u3002", "result": "AI Blob!\u7cfb\u7edf\u901a\u8fc7\u52a8\u6001\u3001\u5185\u5bb9\u611f\u77e5\u7684\u68c0\u7d22\u65b9\u5f0f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u6863\u6848\u5185\u5bb9\u7684\u91cd\u6784\u8bed\u5883\u5316\uff0c\u800c\u975e\u4f9d\u8d56\u9759\u6001\u5143\u6570\u636e\u6a21\u5f0f\u3002\u5b83\u80fd\u751f\u6210\u65b0\u9896\u7684\u81ea\u52a8\u5316\u53d9\u4e8b\u7ed3\u6784\u548c\u6587\u5316\u5206\u6790\u5f62\u5f0f\uff0c\u5c55\u793a\u4e86\u8bed\u4e49\u6280\u672f\u5728\u6863\u6848\u5229\u7528\u65b9\u9762\u7684\u65b0\u9014\u5f84\u3002", "conclusion": "\u8be5\u9879\u76ee\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6982\u5ff5\u6846\u67b6\u548c\u516c\u5f00\u53ef\u7528\u7684\u6570\u636e\u96c6\uff0c\u4e3a\u5a92\u4f53\u53f2\u5b66\u548cAI\u9a71\u52a8\u7684\u6863\u6848\u7814\u7a76\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u8bc1\u660e\u4e86\u8bed\u4e49\u6280\u672f\u548cLLM\u80fd\u6709\u6548\u4fc3\u8fdb\u6863\u6848\u5185\u5bb9\u7684\u521b\u65b0\u6027\u5229\u7528\u548c\u53d9\u4e8b\u6784\u5efa\u3002"}}
{"id": "2508.09462", "pdf": "https://arxiv.org/pdf/2508.09462", "abs": "https://arxiv.org/abs/2508.09462", "authors": ["Guangqiang Li", "M. Amine Atoui", "Xiangshun Li"], "title": "Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation", "categories": ["cs.LG"], "comment": "34 pages, 12 figures", "summary": "A reliable fault diagnosis system should not only accurately classify known\nhealth states but also effectively identify unknown faults. In multimode\nprocesses, samples belonging to the same health state often show multiple\ncluster distributions, making it difficult to construct compact and accurate\ndecision boundaries for that state. To address this challenge, a novel open-set\nfault diagnosis model named fine-grained clustering and rejection network\n(FGCRN) is proposed. It combines multiscale depthwise convolution,\nbidirectional gated recurrent unit and temporal attention mechanism to capture\ndiscriminative features. A distance-based loss function is designed to enhance\nthe intra-class compactness. Fine-grained feature representations are\nconstructed through unsupervised learning to uncover the intrinsic structures\nof each health state. Extreme value theory is employed to model the distance\nbetween sample features and their corresponding fine-grained representations,\nenabling effective identification of unknown faults. Extensive experiments\ndemonstrate the superior performance of the proposed method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFGCRN\u7684\u65b0\u578b\u5f00\u653e\u96c6\u6545\u969c\u8bca\u65ad\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001\u8fc7\u7a0b\u4e2d\u5df2\u77e5\u6545\u969c\u5206\u7c7b\u548c\u672a\u77e5\u6545\u969c\u8bc6\u522b\u7684\u96be\u9898\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u7cbe\u7ec6\u5316\u805a\u7c7b\u548c\u6781\u7aef\u503c\u7406\u8bba\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8bca\u65ad\u3002", "motivation": "\u5728\u591a\u6a21\u6001\u8fc7\u7a0b\u4e2d\uff0c\u53ef\u9760\u7684\u6545\u969c\u8bca\u65ad\u7cfb\u7edf\u4e0d\u4ec5\u9700\u8981\u51c6\u786e\u5206\u7c7b\u5df2\u77e5\u5065\u5eb7\u72b6\u6001\uff0c\u8fd8\u9700\u8981\u6709\u6548\u8bc6\u522b\u672a\u77e5\u6545\u969c\u3002\u7136\u800c\uff0c\u540c\u4e00\u5065\u5eb7\u72b6\u6001\u7684\u6837\u672c\u5e38\u5448\u73b0\u591a\u7c07\u5206\u5e03\uff0c\u5bfc\u81f4\u96be\u4ee5\u6784\u5efa\u7d27\u51d1\u51c6\u786e\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFGCRN\u7684\u5f00\u653e\u96c6\u6545\u969c\u8bca\u65ad\u6a21\u578b\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u591a\u5c3a\u5ea6\u6df1\u5ea6\u5377\u79ef\u3001\u53cc\u5411\u95e8\u63a7\u5faa\u73af\u5355\u5143\u548c\u65f6\u5e8f\u6ce8\u610f\u529b\u673a\u5236\u6765\u6355\u83b7\u5224\u522b\u6027\u7279\u5f81\u3002\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u8ddd\u79bb\u7684\u635f\u5931\u51fd\u6570\u4ee5\u589e\u5f3a\u7c7b\u5185\u7d27\u51d1\u6027\u3002\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u6784\u5efa\u7cbe\u7ec6\u5316\u7279\u5f81\u8868\u793a\u4ee5\u63ed\u793a\u5404\u5065\u5eb7\u72b6\u6001\u7684\u5185\u5728\u7ed3\u6784\u3002\u6700\u540e\uff0c\u5229\u7528\u6781\u7aef\u503c\u7406\u8bba\u5efa\u6a21\u6837\u672c\u7279\u5f81\u4e0e\u5176\u7cbe\u7ec6\u5316\u8868\u793a\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u4ece\u800c\u6709\u6548\u8bc6\u522b\u672a\u77e5\u6545\u969c\u3002", "result": "\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "FGCRN\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u8fc7\u7a0b\u4e2d\u6545\u969c\u8bca\u65ad\u7684\u6311\u6218\uff0c\u5728\u5df2\u77e5\u6545\u969c\u5206\u7c7b\u548c\u672a\u77e5\u6545\u969c\u8bc6\u522b\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.09543", "pdf": "https://arxiv.org/pdf/2508.09543", "abs": "https://arxiv.org/abs/2508.09543", "authors": ["Yuanting Gao", "Linghao Shen"], "title": "Iterative Volume Fusion for Asymmetric Stereo Matching", "categories": ["cs.CV"], "comment": null, "summary": "Stereo matching is vital in 3D computer vision, with most algorithms assuming\nsymmetric visual properties between binocular visions. However, the rise of\nasymmetric multi-camera systems (e.g., tele-wide cameras) challenges this\nassumption and complicates stereo matching. Visual asymmetry disrupts stereo\nmatching by affecting the crucial cost volume computation. To address this, we\nexplore the matching cost distribution of two established cost volume\nconstruction methods in asymmetric stereo. We find that each cost volume\nexperiences distinct information distortion, indicating that both should be\ncomprehensively utilized to solve the issue. Based on this, we propose the\ntwo-phase Iterative Volume Fusion network for Asymmetric Stereo matching\n(IVF-AStereo). Initially, the aggregated concatenation volume refines the\ncorrelation volume. Subsequently, both volumes are fused to enhance fine\ndetails. Our method excels in asymmetric scenarios and shows robust performance\nagainst significant visual asymmetry. Extensive comparative experiments on\nbenchmark datasets, along with ablation studies, confirm the effectiveness of\nour approach in asymmetric stereo with resolution and color degradation.", "AI": {"tldr": "\u9488\u5bf9\u975e\u5bf9\u79f0\u591a\u76f8\u673a\u7cfb\u7edf\u4e2d\u7684\u7acb\u4f53\u5339\u914d\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIVF-AStereo\u7684\u53cc\u9636\u6bb5\u8fed\u4ee3\u4f53\u878d\u5408\u7f51\u7edc\uff0c\u901a\u8fc7\u878d\u5408\u4e0d\u540c\u4ee3\u4ef7\u4f53\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5339\u914d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u7acb\u4f53\u5339\u914d\u7b97\u6cd5\u4f9d\u8d56\u4e8e\u53cc\u76ee\u89c6\u89c9\u7684\u5bf9\u79f0\u6027\u5047\u8bbe\uff0c\u4f46\u65b0\u5174\u7684\u975e\u5bf9\u79f0\u591a\u76f8\u673a\u7cfb\u7edf\uff08\u5982\u8fdc\u6444-\u5e7f\u89d2\u76f8\u673a\uff09\u6253\u7834\u4e86\u8fd9\u4e00\u5047\u8bbe\uff0c\u5bfc\u81f4\u7acb\u4f53\u5339\u914d\uff08\u7279\u522b\u662f\u4ee3\u4ef7\u4f53\u8ba1\u7b97\uff09\u9762\u4e34\u4e25\u5cfb\u6311\u6218\u3002", "method": "\u9996\u5148\uff0c\u5206\u6790\u4e86\u4e24\u79cd\u73b0\u6709\u4ee3\u4ef7\u4f53\u6784\u5efa\u65b9\u6cd5\u5728\u975e\u5bf9\u79f0\u7acb\u4f53\u4e2d\u7684\u5339\u914d\u4ee3\u4ef7\u5206\u5e03\uff0c\u53d1\u73b0\u5b83\u4eec\u5b58\u5728\u4fe1\u606f\u626d\u66f2\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u9636\u6bb5\u8fed\u4ee3\u4f53\u878d\u5408\u7f51\u7edc(IVF-AStereo)\uff1a\u7b2c\u4e00\u9636\u6bb5\uff0c\u901a\u8fc7\u805a\u5408\u4e32\u8054\u4f53\u4f18\u5316\u5173\u8054\u4f53\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff0c\u878d\u5408\u4e24\u79cd\u4ee3\u4ef7\u4f53\u4ee5\u589e\u5f3a\u7ec6\u8282\u4fe1\u606f\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u975e\u5bf9\u79f0\u573a\u666f\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u5bf9\u663e\u8457\u7684\u89c6\u89c9\u975e\u5bf9\u79f0\u6027\u663e\u793a\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5bf9\u6bd4\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u5206\u8fa8\u7387\u548c\u989c\u8272\u964d\u7ea7\u7684\u975e\u5bf9\u79f0\u7acb\u4f53\u56fe\u50cf\u65f6\u7684\u6709\u6548\u6027\u3002", "conclusion": "IVF-AStereo\u7f51\u7edc\u901a\u8fc7\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u4f53\u878d\u5408\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u975e\u5bf9\u79f0\u7acb\u4f53\u5339\u914d\u7684\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u975e\u5bf9\u79f0\u89c6\u89c9\u6761\u4ef6\u4e0b\u7684\u5339\u914d\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.09614", "pdf": "https://arxiv.org/pdf/2508.09614", "abs": "https://arxiv.org/abs/2508.09614", "authors": ["Daniel Raffini", "Agnese Macori", "Lorenzo Porcaro", "Tiziana Catarci", "Marco Angelini"], "title": "How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "9-pages", "summary": "This study examines the rhetorical and linguistic features of argumentative\ntexts generated by ChatGPT on ethically nuanced topics and investigates their\npersuasive impact on human readers.Through a user study involving 62\nparticipants and pre-post interaction surveys, the paper analyzes how exposure\nto AI-generated arguments affects opinion change and user perception. A\nlinguistic and rhetorical analysis of the generated texts reveals a consistent\nargumentative macrostructure, reliance on formulaic expressions, and limited\nstylistic richness. While ChatGPT demonstrates proficiency in constructing\ncoherent argumentative texts, its persuasive efficacy appears constrained,\nparticularly on topics involving ethical issues.The study finds that while\nparticipants often acknowledge the benefits highlighted by ChatGPT, ethical\nconcerns tend to persist or even intensify post-interaction. The results also\ndemonstrate a variation depending on the topic. These findings highlight new\ninsights on AI-generated persuasion in ethically sensitive domains and are a\nbasis for future research.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86ChatGPT\u5728\u4f26\u7406\u654f\u611f\u8bdd\u9898\u4e0a\u751f\u6210\u8bba\u8bc1\u6587\u672c\u7684\u7279\u70b9\u53ca\u5176\u5bf9\u4eba\u7c7b\u8bfb\u8005\u7684\u8bf4\u670d\u529b\uff0c\u53d1\u73b0\u5176\u6587\u672c\u7ed3\u6784\u8fde\u8d2f\u4f46\u98ce\u683c\u6709\u9650\uff0c\u8bf4\u670d\u529b\u5c24\u5176\u5728\u4f26\u7406\u95ee\u9898\u4e0a\u53d7\u9650\uff0c\u5e76\u53ef\u80fd\u52a0\u5267\u8bfb\u8005\u7684\u4f26\u7406\u62c5\u5fe7\u3002", "motivation": "\u63a2\u8ba8ChatGPT\u5728\u4f26\u7406\u654f\u611f\u8bdd\u9898\u4e0a\u751f\u6210\u7684\u8bba\u8bc1\u6587\u672c\u7684\u4fee\u8f9e\u548c\u8bed\u8a00\u7279\u5f81\uff0c\u5e76\u7814\u7a76\u8fd9\u4e9b\u6587\u672c\u5bf9\u4eba\u7c7b\u8bfb\u8005\u7684\u8bf4\u670d\u5f71\u54cd\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u9879\u5305\u542b62\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\uff0c\u901a\u8fc7\u524d\u540e\u4e92\u52a8\u8c03\u67e5\u5206\u6790AI\u751f\u6210\u8bba\u8bc1\u5bf9\u89c2\u70b9\u6539\u53d8\u548c\u7528\u6237\u611f\u77e5\u7684\u5f71\u54cd\uff1b\u5bf9ChatGPT\u751f\u6210\u7684\u6587\u672c\u8fdb\u884c\u4e86\u8bed\u8a00\u548c\u4fee\u8f9e\u5206\u6790\u3002", "result": "ChatGPT\u751f\u6210\u7684\u8bba\u8bc1\u6587\u672c\u5177\u6709\u4e00\u81f4\u7684\u5b8f\u89c2\u7ed3\u6784\uff0c\u4f46\u4f9d\u8d56\u56fa\u5b9a\u8868\u8fbe\u4e14\u6587\u4f53\u4e30\u5bcc\u6027\u6709\u9650\u3002\u5c3d\u7ba1\u6587\u672c\u8fde\u8d2f\uff0c\u4f46\u5176\u8bf4\u670d\u529b\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u4f26\u7406\u95ee\u9898\u65f6\u3002\u53c2\u4e0e\u8005\u5728\u627f\u8ba4ChatGPT\u63d0\u51fa\u7684\u76ca\u5904\u540e\uff0c\u4f26\u7406\u62c5\u5fe7\u5f80\u5f80\u6301\u7eed\u5b58\u5728\u751a\u81f3\u52a0\u5267\uff0c\u4e14\u7814\u7a76\u7ed3\u679c\u56e0\u4e3b\u9898\u800c\u5f02\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u5728\u4f26\u7406\u654f\u611f\u9886\u57df\u7684\u8bf4\u670d\u529b\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u76f8\u5173\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.09200", "pdf": "https://arxiv.org/pdf/2508.09200", "abs": "https://arxiv.org/abs/2508.09200", "authors": ["Jinho Kim", "Marcel Dominik Nickel", "Florian Knoll"], "title": "Zero-shot self-supervised learning of single breath-hold magnetic resonance cholangiopancreatography (MRCP) reconstruction", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "23 pages, 6 figures, 2 tabels", "summary": "Purpose: To investigate the feasibility of applying zero-shot self-supervised\nlearning reconstruction to reduce breath-hold times in magnetic resonance\ncholangiopancreatography (MRCP). Methods: Breath-hold MRCP was acquired from 11\nhealthy volunteers on a 3T scanner using an incoherent k-space sampling pattern\nleading to a breath-hold duration of 14s. We evaluated zero-shot reconstruction\nof breath-hold MRCP against parallel imaging of respiratory-triggered MRCP\nacquired in 338s on average and compressed sensing reconstruction of\nbreath-hold MRCP. To address the long computation times of zero-shot trainings,\nwe used a training approach that leverages a pretrained network to reduce\nbackpropagation depth during training. Results: Zero-shot learning\nreconstruction significantly improved visual image quality compared to\ncompressed sensing reconstruction, particularly in terms of signal-to-noise\nratio and ductal delineation, and reached a level of quality comparable to that\nof successful respiratory-triggered acquisitions with regular breathing\npatterns. Shallow training provided nearly equivalent reconstruction\nperformance with a training time of 11 minutes in comparison to 271 minutes for\na conventional zero-shot training. Conclusion: Zero-shot learning delivers\nhigh-fidelity MRCP reconstructions with reduced breath-hold times, and shallow\ntraining offers a practical solution for translation to time-constrained\nclinical workflows.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u96f6\u6837\u672c\u81ea\u76d1\u7763\u5b66\u4e60\u91cd\u5efa\u6280\u672f\u5728\u7f29\u77ed\u78c1\u5171\u632f\u80f0\u80c6\u7ba1\u6210\u50cf\uff08MRCP\uff09\u5c4f\u6c14\u65f6\u95f4\u65b9\u9762\u7684\u53ef\u884c\u6027\uff0c\u5e76\u901a\u8fc7\u6d45\u5c42\u8bad\u7ec3\u663e\u8457\u7f29\u77ed\u4e86\u8ba1\u7b97\u65f6\u95f4\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u65e8\u5728\u63a2\u7a76\u96f6\u6837\u672c\u81ea\u76d1\u7763\u5b66\u4e60\u91cd\u5efa\u6280\u672f\u5728\u78c1\u5171\u632f\u80f0\u80c6\u7ba1\u6210\u50cf\uff08MRCP\uff09\u4e2d\u7f29\u77ed\u60a3\u8005\u5c4f\u6c14\u65f6\u95f4\u7684\u53ef\u884c\u6027\u3002", "method": "\u62db\u52df11\u540d\u5065\u5eb7\u5fd7\u613f\u8005\u83b7\u53d6\u5c4f\u6c14MRCP\u6570\u636e\u3002\u5c06\u96f6\u6837\u672c\u91cd\u5efa\u4e0e\u547c\u5438\u89e6\u53d1MRCP\u7684\u5e76\u884c\u6210\u50cf\u4ee5\u53ca\u5c4f\u6c14MRCP\u7684\u538b\u7f29\u611f\u77e5\u91cd\u5efa\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002\u4e3a\u7f29\u77ed\u8ba1\u7b97\u65f6\u95f4\uff0c\u91c7\u7528\u4e86\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u7f51\u7edc\u51cf\u5c11\u53cd\u5411\u4f20\u64ad\u6df1\u5ea6\u7684\u6d45\u5c42\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u96f6\u6837\u672c\u5b66\u4e60\u91cd\u5efa\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf\u8d28\u91cf\uff08\u7279\u522b\u662f\u4fe1\u566a\u6bd4\u548c\u5bfc\u7ba1\u63cf\u7ed8\uff09\uff0c\u4f18\u4e8e\u538b\u7f29\u611f\u77e5\u91cd\u5efa\uff0c\u5e76\u53ef\u4e0e\u6210\u529f\u7684\u547c\u5438\u89e6\u53d1\u91c7\u96c6\u76f8\u5ab2\u7f8e\u3002\u6d45\u5c42\u8bad\u7ec3\u572811\u5206\u949f\u5185\u5b9e\u73b0\u4e86\u4e0e\u4f20\u7edf\u96f6\u6837\u672c\u8bad\u7ec3\uff08271\u5206\u949f\uff09\u51e0\u4e4e\u76f8\u540c\u7684\u91cd\u5efa\u6027\u80fd\u3002", "conclusion": "\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u9ad8\u4fdd\u771fMRCP\u91cd\u5efa\uff0c\u5e76\u6709\u6548\u7f29\u77ed\u5c4f\u6c14\u65f6\u95f4\u3002\u6d45\u5c42\u8bad\u7ec3\u4e3a\u8be5\u6280\u672f\u5e94\u7528\u4e8e\u65f6\u95f4\u53d7\u9650\u7684\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09467", "pdf": "https://arxiv.org/pdf/2508.09467", "abs": "https://arxiv.org/abs/2508.09467", "authors": ["Zijun Sun", "Yanning Shen"], "title": "Learn to Explore: Meta NAS via Bayesian Optimization Guided Graph Generation", "categories": ["cs.LG"], "comment": null, "summary": "Neural Architecture Search (NAS) automates the design of high-performing\nneural networks but typically targets a single predefined task, thereby\nrestricting its real-world applicability. To address this, Meta Neural\nArchitecture Search (Meta-NAS) has emerged as a promising paradigm that\nleverages prior knowledge across tasks to enable rapid adaptation to new ones.\nNevertheless, existing Meta-NAS methods often struggle with poor\ngeneralization, limited search spaces, or high computational costs. In this\npaper, we propose a novel Meta-NAS framework, GraB-NAS. Specifically, GraB-NAS\nfirst models neural architectures as graphs, and then a hybrid search strategy\nis developed to find and generate new graphs that lead to promising neural\narchitectures. The search strategy combines global architecture search via\nBayesian Optimization in the search space with local exploration for novel\nneural networks via gradient ascent in the latent space. Such a hybrid search\nstrategy allows GraB-NAS to discover task-aware architectures with strong\nperformance, even beyond the predefined search space. Extensive experiments\ndemonstrate that GraB-NAS outperforms state-of-the-art Meta-NAS baselines,\nachieving better generalization and search effectiveness.", "AI": {"tldr": "GraB-NAS\u662f\u4e00\u79cd\u65b0\u578b\u5143\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u641c\u7d22\u6846\u67b6\uff0c\u91c7\u7528\u6df7\u5408\u641c\u7d22\u7b56\u7565\uff08\u8d1d\u53f6\u65af\u4f18\u5316\u4e0e\u68af\u5ea6\u4e0a\u5347\uff09\uff0c\u80fd\u53d1\u73b0\u4efb\u52a1\u611f\u77e5\u578b\u67b6\u6784\uff0c\u5e76\u8d85\u8d8a\u73b0\u6709Meta-NAS\u65b9\u6cd5\uff0c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u641c\u7d22\u6548\u7387\u3002", "motivation": "\u4f20\u7edfNAS\u4ec5\u9488\u5bf9\u5355\u4e00\u4efb\u52a1\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u5c3d\u7ba1Meta-NAS\u80fd\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u4f46\u73b0\u6709Meta-NAS\u65b9\u6cd5\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u641c\u7d22\u7a7a\u95f4\u53d7\u9650\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faGraB-NAS\u6846\u67b6\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5efa\u6a21\u4e3a\u56fe\u3002\u91c7\u7528\u6df7\u5408\u641c\u7d22\u7b56\u7565\uff1a\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u8fdb\u884c\u5168\u5c40\u67b6\u6784\u641c\u7d22\uff0c\u5e76\u7ed3\u5408\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u901a\u8fc7\u68af\u5ea6\u4e0a\u5347\u8fdb\u884c\u5c40\u90e8\u63a2\u7d22\u4ee5\u751f\u6210\u65b0\u9896\u67b6\u6784\u3002", "result": "GraB-NAS\u80fd\u591f\u53d1\u73b0\u5177\u6709\u5f3a\u5927\u6027\u80fd\u7684\u4efb\u52a1\u611f\u77e5\u578b\u67b6\u6784\uff0c\u751a\u81f3\u8d85\u8d8a\u9884\u5b9a\u4e49\u641c\u7d22\u7a7a\u95f4\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cGraB-NAS\u4f18\u4e8e\u6700\u5148\u8fdb\u7684Meta-NAS\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u641c\u7d22\u6548\u7387\u3002", "conclusion": "GraB-NAS\u901a\u8fc7\u5f15\u5165\u65b0\u9896\u7684\u57fa\u4e8e\u56fe\u7684\u6df7\u5408\u641c\u7d22\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709Meta-NAS\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5143\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u641c\u7d22\u7684\u6cdb\u5316\u80fd\u529b\u548c\u641c\u7d22\u6548\u7387\u3002"}}
{"id": "2508.09547", "pdf": "https://arxiv.org/pdf/2508.09547", "abs": "https://arxiv.org/abs/2508.09547", "authors": ["Fengyi Wu", "Yifei Dong", "Zhi-Qi Cheng", "Yilong Dai", "Guangyu Chen", "Hang Wang", "Qi Dai", "Alexander G. Hauptmann"], "title": "GoViG: Goal-Conditioned Visual Navigation Instruction Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Under review. Code: https://github.com/F1y1113/GoViG", "summary": "We introduce Goal-Conditioned Visual Navigation Instruction Generation\n(GoViG), a new task that aims to autonomously generate precise and contextually\ncoherent navigation instructions solely from egocentric visual observations of\ninitial and goal states. Unlike conventional approaches that rely on structured\ninputs such as semantic annotations or environmental maps, GoViG exclusively\nleverages raw egocentric visual data, substantially improving its adaptability\nto unseen and unstructured environments. Our method addresses this task by\ndecomposing it into two interconnected subtasks: (1) visual forecasting, which\npredicts intermediate visual states bridging the initial and goal views; and\n(2) instruction generation, which synthesizes linguistically coherent\ninstructions grounded in both observed and anticipated visuals. These subtasks\nare integrated within an autoregressive multimodal large language model trained\nwith tailored objectives to ensure spatial accuracy and linguistic clarity.\nFurthermore, we introduce two complementary multimodal reasoning strategies,\none-pass and interleaved reasoning, to mimic incremental human cognitive\nprocesses during navigation. To evaluate our method, we propose the R2R-Goal\ndataset, combining diverse synthetic and real-world trajectories. Empirical\nresults demonstrate significant improvements over state-of-the-art methods,\nachieving superior BLEU-4 and CIDEr scores along with robust cross-domain\ngeneralization.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u4e86GoViG\uff08Goal-Conditioned Visual Navigation Instruction Generation\uff09\u4efb\u52a1\uff0c\u65e8\u5728\u4ec5\u5229\u7528\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u4e0b\u7684\u521d\u59cb\u548c\u76ee\u6807\u89c6\u89c9\u89c2\u6d4b\uff0c\u81ea\u52a8\u751f\u6210\u7cbe\u786e\u7684\u5bfc\u822a\u6307\u4ee4\uff0c\u65e0\u9700\u7ed3\u6784\u5316\u8f93\u5165\uff0c\u63d0\u9ad8\u4e86\u5bf9\u672a\u77e5\u73af\u5883\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5bfc\u822a\u6307\u4ee4\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7ed3\u6784\u5316\u8f93\u5165\uff08\u5982\u8bed\u4e49\u6807\u6ce8\u6216\u73af\u5883\u5730\u56fe\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u672a\u77e5\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4ec5\u5229\u7528\u539f\u59cb\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u6570\u636e\u6765\u514b\u670d\u8fd9\u4e00\u5c40\u9650\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u4e24\u4e2a\u5b50\u4efb\u52a1\uff1a1) \u89c6\u89c9\u9884\u6d4b\uff0c\u7528\u4e8e\u9884\u6d4b\u8fde\u63a5\u521d\u59cb\u548c\u76ee\u6807\u89c6\u56fe\u7684\u4e2d\u95f4\u89c6\u89c9\u72b6\u6001\uff1b2) \u6307\u4ee4\u751f\u6210\uff0c\u7528\u4e8e\u57fa\u4e8e\u89c2\u5bdf\u5230\u7684\u548c\u9884\u6d4b\u7684\u89c6\u89c9\u4fe1\u606f\u5408\u6210\u8bed\u8a00\u8fde\u8d2f\u7684\u6307\u4ee4\u3002\u8fd9\u4e24\u4e2a\u5b50\u4efb\u52a1\u88ab\u96c6\u6210\u5230\u4e00\u4e2a\u81ea\u56de\u5f52\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u5e76\u901a\u8fc7\u5b9a\u5236\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u786e\u4fdd\u7a7a\u95f4\u51c6\u786e\u6027\u548c\u8bed\u8a00\u6e05\u6670\u5ea6\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u6b21\u6027\u63a8\u7406\u548c\u4ea4\u9519\u63a8\u7406\u4e24\u79cd\u4e92\u8865\u7684\u591a\u6a21\u6001\u63a8\u7406\u7b56\u7565\uff0c\u4ee5\u6a21\u62df\u4eba\u7c7b\u5728\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u7684\u6e10\u8fdb\u8ba4\u77e5\u8fc7\u7a0b\u3002\u4e3a\u8bc4\u4f30\u8be5\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u8f68\u8ff9\u7684R2R-Goal\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGoViG\u65b9\u6cd5\u76f8\u8f83\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5728BLEU-4\u548cCIDEr\u5206\u6570\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684GoViG\u4efb\u52a1\u53ca\u5176\u901a\u8fc7\u89c6\u89c9\u9884\u6d4b\u548c\u6307\u4ee4\u751f\u6210\u76f8\u7ed3\u5408\u7684\u81ea\u56de\u5f52\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ece\u539f\u59cb\u89c6\u89c9\u6570\u636e\u751f\u6210\u5bfc\u822a\u6307\u4ee4\u7684\u6311\u6218\uff0c\u5e76\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.09651", "pdf": "https://arxiv.org/pdf/2508.09651", "abs": "https://arxiv.org/abs/2508.09651", "authors": ["Daniel Raffini", "Agnese Macori", "Marco Angelini", "Tiziana Catarci"], "title": "A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "8-pages", "summary": "The paper explores the study of gender-based narrative biases in stories\ngenerated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's\ncharacter classifications and Freytag's narrative structure. The stories are\nanalyzed through a close reading approach, with particular attention to\nadherence to the prompt, gender distribution of characters, physical and\npsychological descriptions, actions, and finally, plot development and\ncharacter relationships. The results reveal the persistence of biases -\nespecially implicit ones - in the generated stories and highlight the\nimportance of assessing biases at multiple levels using an interpretative\napproach.", "AI": {"tldr": "\u5206\u6790\u4e86ChatGPT\u3001Gemini\u548cClaude\u751f\u6210\u6545\u4e8b\u4e2d\u7684\u6027\u522b\u53d9\u4e8b\u504f\u89c1\u3002", "motivation": "\u63a2\u8ba8\u4e3b\u6d41\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08ChatGPT\u3001Gemini\u3001Claude\uff09\u751f\u6210\u6545\u4e8b\u4e2d\u5b58\u5728\u7684\u6027\u522b\u53d9\u4e8b\u504f\u89c1\u3002", "method": "\u5229\u7528\u666e\u7f57\u666e\u89d2\u8272\u5206\u7c7b\u548c\u5f17\u96f7\u5854\u683c\u53d9\u4e8b\u7ed3\u6784\u8bbe\u8ba1\u63d0\u793a\uff0c\u901a\u8fc7\u7ec6\u81f4\u9605\u8bfb\u5206\u6790\u751f\u6210\u7684\u6545\u4e8b\uff0c\u5173\u6ce8\u63d0\u793a\u4f9d\u4ece\u6027\u3001\u89d2\u8272\u6027\u522b\u5206\u5e03\u3001\u5916\u8c8c\u5fc3\u7406\u63cf\u5199\u3001\u884c\u4e3a\u3001\u60c5\u8282\u53d1\u5c55\u548c\u89d2\u8272\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u751f\u6210\u6545\u4e8b\u4e2d\u504f\u89c1\u7684\u6301\u7eed\u5b58\u5728\uff0c\u5c24\u5176\u662f\u9690\u6027\u504f\u89c1\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u4f7f\u7528\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u591a\u5c42\u9762\u8bc4\u4f30\u504f\u89c1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.09201", "pdf": "https://arxiv.org/pdf/2508.09201", "abs": "https://arxiv.org/abs/2508.09201", "authors": ["Shuang Liang", "Zhihao Xu", "Jialing Tao", "Hui Xue", "Xiting Wang"], "title": "Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)\nremain vulnerable to jailbreak attacks, posing serious safety risks. Although\nrecent detection works have shifted to internal representations due to their\nrich cross-modal information, most methods rely on heuristic rules rather than\nprincipled objectives, resulting in suboptimal performance. To address these\nlimitations, we propose Learning to Detect (LoD), a novel unsupervised\nframework that formulates jailbreak detection as anomaly detection. LoD\nintroduces two key components: Multi-modal Safety Concept Activation Vectors\n(MSCAV), which capture layer-wise safety-related representations across\nmodalities, and the Safety Pattern Auto-Encoder, which models the distribution\nof MSCAV derived from safe inputs and detects anomalies via reconstruction\nerrors. By training the auto-encoder (AE) solely on safe samples without attack\nlabels, LoD naturally identifies jailbreak inputs as distributional anomalies,\nenabling accurate and unified detection of jailbreak attacks. Comprehensive\nexperiments on three different LVLMs and five benchmarks demonstrate that LoD\nachieves state-of-the-art performance, with an average AUROC of 0.9951 and an\nimprovement of up to 38.89% in the minimum AUROC over the strongest baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa LoD\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u5c06\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u8d8a\u72f1\u68c0\u6d4b\u89c6\u4e3a\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u5b66\u4e60\u5b89\u5168\u6837\u672c\u7684\u5185\u90e8\u8868\u793a\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u4ed8\u51fa\u5927\u91cf\u52aa\u529b\uff0c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4ecd\u6613\u53d7\u8d8a\u72f1\u653b\u51fb\uff0c\u6784\u6210\u4e25\u91cd\u5b89\u5168\u98ce\u9669\u3002\u73b0\u6709\u57fa\u4e8e\u5185\u90e8\u8868\u793a\u7684\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa LoD \u6846\u67b6\uff0c\u5c06\u8d8a\u72f1\u68c0\u6d4b\u8868\u8ff0\u4e3a\u5f02\u5e38\u68c0\u6d4b\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u591a\u6a21\u6001\u5b89\u5168\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf\uff08MSCAV\uff09\uff0c\u7528\u4e8e\u6355\u6349\u8de8\u6a21\u6001\u7684\u5b89\u5168\u76f8\u5173\u8868\u793a\uff1b\u4ee5\u53ca\u5b89\u5168\u6a21\u5f0f\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u91cd\u5efa\u8bef\u5dee\u5efa\u6a21\u5b89\u5168\u8f93\u5165\u7684 MSCAV \u5206\u5e03\u5e76\u68c0\u6d4b\u5f02\u5e38\u3002\u6a21\u578b\u4ec5\u5728\u5b89\u5168\u6837\u672c\u4e0a\u8fdb\u884c\u65e0\u76d1\u7763\u8bad\u7ec3\u3002", "result": "LoD \u5728\u4e09\u79cd\u4e0d\u540c\u7684 LVLM \u548c\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e73\u5747 AUROC \u8fbe\u5230 0.9951\uff0c\u6700\u5c0f AUROC \u76f8\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u9ad8\u4e86 38.89%\u3002", "conclusion": "LoD \u63d0\u4f9b\u4e86\u4e00\u79cd\u51c6\u786e\u4e14\u7edf\u4e00\u7684\u65e0\u76d1\u7763\u8d8a\u72f1\u653b\u51fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5206\u5e03\u5f02\u5e38\uff0c\u663e\u8457\u63d0\u5347\u4e86 LVLMs \u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2508.09468", "pdf": "https://arxiv.org/pdf/2508.09468", "abs": "https://arxiv.org/abs/2508.09468", "authors": ["Muhammad Sakib Khan Inan", "Kewen Liao"], "title": "DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted for publication at IJCAI 2025", "summary": "Internet of Things (IoT) sensors are ubiquitous technologies deployed across\nsmart cities, industrial sites, and healthcare systems. They continuously\ngenerate time series data that enable advanced analytics and automation in\nindustries. However, challenges such as the loss or ambiguity of sensor\nmetadata, heterogeneity in data sources, varying sampling frequencies,\ninconsistent units of measurement, and irregular timestamps make raw IoT time\nseries data difficult to interpret, undermining the effectiveness of smart\nsystems. To address these challenges, we propose a novel deep learning model,\nDeepFeatIoT, which integrates learned local and global features with\nnon-learned randomized convolutional kernel-based features and features from\nlarge language models (LLMs). This straightforward yet unique fusion of diverse\nlearned and non-learned features significantly enhances IoT time series sensor\ndata classification, even in scenarios with limited labeled data. Our model's\neffectiveness is demonstrated through its consistent and generalized\nperformance across multiple real-world IoT sensor datasets from diverse\ncritical application domains, outperforming state-of-the-art benchmark models.\nThese results highlight DeepFeatIoT's potential to drive significant\nadvancements in IoT analytics and support the development of next-generation\nsmart systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDeepFeatIoT\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u591a\u79cd\u5b66\u4e60\u4e0e\u975e\u5b66\u4e60\u7279\u5f81\uff0c\u6709\u6548\u63d0\u5347\u7269\u8054\u7f51\u65f6\u95f4\u5e8f\u5217\u4f20\u611f\u5668\u6570\u636e\u5206\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u573a\u666f\u4e0b\uff0c\u5e76\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002", "motivation": "\u539f\u59cb\u7269\u8054\u7f51\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5b58\u5728\u5143\u6570\u636e\u7f3a\u5931\u3001\u6570\u636e\u6e90\u5f02\u6784\u3001\u91c7\u6837\u9891\u7387\u3001\u6d4b\u91cf\u5355\u4f4d\u548c\u65f6\u95f4\u6233\u4e0d\u4e00\u81f4\u7b49\u6311\u6218\uff0c\u5bfc\u81f4\u6570\u636e\u96be\u4ee5\u89e3\u91ca\uff0c\u524a\u5f31\u4e86\u667a\u80fd\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bDeepFeatIoT\uff0c\u8be5\u6a21\u578b\u96c6\u6210\u5b66\u4e60\u5230\u7684\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u3001\u975e\u5b66\u4e60\u7684\u968f\u673a\u5377\u79ef\u6838\u7279\u5f81\uff0c\u4ee5\u53ca\u6765\u81ea\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7279\u5f81\uff0c\u4ee5\u589e\u5f3a\u7269\u8054\u7f51\u65f6\u95f4\u5e8f\u5217\u4f20\u611f\u5668\u6570\u636e\u7684\u5206\u7c7b\u80fd\u529b\u3002", "result": "DeepFeatIoT\u6a21\u578b\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u7269\u8054\u7f51\u4f20\u611f\u5668\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u4e00\u81f4\u4e14\u6cdb\u5316\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cDeepFeatIoT\u6a21\u578b\u5728\u63a8\u52a8\u7269\u8054\u7f51\u5206\u6790\u9886\u57df\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u5e76\u80fd\u652f\u6301\u4e0b\u4e00\u4ee3\u667a\u80fd\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.09550", "pdf": "https://arxiv.org/pdf/2508.09550", "abs": "https://arxiv.org/abs/2508.09550", "authors": ["Haowen Wang", "Guowei Zhang", "Xiang Zhang", "Zeyuan Chen", "Haiyang Xu", "Dou Hoon Kwark", "Zhuowen Tu"], "title": "Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we address a key scientific problem in machine learning: Given\na training set for an image classification task, can we train a generative\nmodel on this dataset to enhance the classification performance? (i.e.,\nclosed-set generative data augmentation). We start by exploring the\ndistinctions and similarities between real images and closed-set synthetic\nimages generated by advanced generative models. Through extensive experiments,\nwe offer systematic insights into the effective use of closed-set synthetic\ndata for augmentation. Notably, we empirically determine the equivalent scale\nof synthetic images needed for augmentation. In addition, we also show\nquantitative equivalence between the real data augmentation and open-set\ngenerative augmentation (generative models trained using data beyond the given\ntraining set). While it aligns with the common intuition that real images are\ngenerally preferred, our empirical formulation also offers a guideline to\nquantify the increased scale of synthetic data augmentation required to achieve\ncomparable image classification performance. Our results on natural and medical\nimage datasets further illustrate how this effect varies with the baseline\ntraining set size and the amount of synthetic data incorporated.", "AI": {"tldr": "\u63a2\u8ba8\u5c01\u95ed\u96c6\u751f\u6210\u6570\u636e\u589e\u5f3a\u5728\u63d0\u5347\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u91cf\u5316\u5408\u6210\u6570\u636e\u7684\u7b49\u6548\u89c4\u6a21\uff0c\u4e3a\u5176\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u4e2d\u4e00\u4e2a\u5173\u952e\u79d1\u5b66\u95ee\u9898\uff1a\u7ed9\u5b9a\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u8bad\u7ec3\u96c6\uff0c\u80fd\u5426\u901a\u8fc7\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u6765\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\uff0c\u5373\u8fdb\u884c\u5c01\u95ed\u96c6\u751f\u6210\u6570\u636e\u589e\u5f3a\u3002", "method": "\u63a2\u7d22\u771f\u5b9e\u56fe\u50cf\u4e0e\u5c01\u95ed\u96c6\u751f\u6210\u56fe\u50cf\u7684\u533a\u522b\u548c\u76f8\u4f3c\u6027\u3002\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u6027\u5730\u6d1e\u5bdf\u5408\u6210\u6570\u636e\u5728\u6570\u636e\u589e\u5f3a\u4e2d\u7684\u6709\u6548\u4f7f\u7528\uff0c\u5e76\u7ecf\u9a8c\u6027\u5730\u786e\u5b9a\u5408\u6210\u56fe\u50cf\u6240\u9700\u7684\u7b49\u6548\u89c4\u6a21\u3002", "result": "\u63d0\u4f9b\u4e86\u5173\u4e8e\u5c01\u95ed\u96c6\u5408\u6210\u6570\u636e\u6709\u6548\u7528\u4e8e\u589e\u5f3a\u7684\u7cfb\u7edf\u6027\u89c1\u89e3\uff0c\u7ecf\u9a8c\u6027\u5730\u786e\u5b9a\u4e86\u5408\u6210\u56fe\u50cf\u589e\u5f3a\u6240\u9700\u7684\u7b49\u6548\u89c4\u6a21\u3002\u5c55\u793a\u4e86\u771f\u5b9e\u6570\u636e\u589e\u5f3a\u4e0e\u5f00\u653e\u96c6\u751f\u6210\u589e\u5f3a\u4e4b\u95f4\u7684\u5b9a\u91cf\u7b49\u6548\u6027\u3002\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ecf\u9a8c\u516c\u5f0f\uff0c\u7528\u4e8e\u91cf\u5316\u8fbe\u5230\u53ef\u6bd4\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u6240\u9700\u7684\u5408\u6210\u6570\u636e\u589e\u5f3a\u89c4\u6a21\uff0c\u5e76\u5728\u81ea\u7136\u548c\u533b\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u968f\u57fa\u7ebf\u8bad\u7ec3\u96c6\u5927\u5c0f\u548c\u5408\u6210\u6570\u636e\u91cf\u7684\u53d8\u5316\u3002", "conclusion": "\u5c3d\u7ba1\u771f\u5b9e\u56fe\u50cf\u901a\u5e38\u66f4\u53d7\u9752\u7750\uff0c\u672c\u7814\u7a76\u7684\u7ecf\u9a8c\u516c\u5f0f\u4e3a\u91cf\u5316\u5408\u6210\u6570\u636e\u589e\u5f3a\u7684\u6240\u9700\u89c4\u6a21\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u4f7f\u5176\u80fd\u591f\u8fbe\u5230\u4e0e\u771f\u5b9e\u6570\u636e\u589e\u5f3a\u76f8\u5f53\u7684\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u6709\u6548\u5229\u7528\u5408\u6210\u6570\u636e\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u63d0\u4f9b\u4e86\u91cf\u5316\u4f9d\u636e\u3002"}}
{"id": "2508.09886", "pdf": "https://arxiv.org/pdf/2508.09886", "abs": "https://arxiv.org/abs/2508.09886", "authors": ["Lingyu Chen", "Yawen Zeng", "Yue Wang", "Peng Wan", "Guo-chen Ning", "Hongen Liao", "Daoqiang Zhang", "Fang Chen"], "title": "COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "ICCV 2025", "summary": "Conventional single-dataset training often fails with new data distributions,\nespecially in ultrasound (US) image analysis due to limited data, acoustic\nshadows, and speckle noise. Therefore, constructing a universal framework for\nmulti-heterogeneous US datasets is imperative. However, a key challenge arises:\nhow to effectively mitigate inter-dataset interference while preserving\ndataset-specific discriminative features for robust downstream task? Previous\napproaches utilize either a single source-specific decoder or a domain\nadaptation strategy, but these methods experienced a decline in performance\nwhen applied to other domains. Considering this, we propose a Universal\nCollaborative Mixture of Heterogeneous Source-Specific Experts (COME).\nSpecifically, COME establishes dual structure-semantic shared experts that\ncreate a universal representation space and then collaborate with\nsource-specific experts to extract discriminative features through providing\ncomplementary features. This design enables robust generalization by leveraging\ncross-datasets experience distributions and providing universal US priors for\nsmall-batch or unseen data scenarios. Extensive experiments under three\nevaluation modes (single-dataset, intra-organ, and inter-organ integration\ndatasets) demonstrate COME's superiority, achieving significant mean AP\nimprovements over state-of-the-art methods. Our project is available at:\nhttps://universalcome.github.io/UniversalCOME/.", "AI": {"tldr": "\u9488\u5bf9\u8d85\u58f0\u56fe\u50cf\u591a\u5f02\u6784\u6570\u636e\u96c6\u6cdb\u5316\u6311\u6218\uff0c\u63d0\u51faCOME\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u548c\u7279\u5b9a\u4e13\u5bb6\u534f\u540c\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u6a21\u5f0f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5355\u6570\u636e\u96c6\u8bad\u7ec3\u5728\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u4e2d\u9762\u5bf9\u65b0\u6570\u636e\u5206\u5e03\u65f6\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4e3b\u8981\u539f\u56e0\u5305\u62ec\u6570\u636e\u6709\u9650\u3001\u58f0\u5f71\u548c\u6563\u6591\u566a\u58f0\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u6784\u5efa\u4e00\u4e2a\u9002\u7528\u4e8e\u591a\u5f02\u6784\u8d85\u58f0\u6570\u636e\u96c6\u7684\u901a\u7528\u6846\u67b6\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u5355\u6e90\u7279\u5b9a\u89e3\u7801\u5668\u6216\u57df\u9002\u5e94\uff09\u5728\u8de8\u57df\u5e94\u7528\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u96be\u4ee5\u6709\u6548\u7f13\u89e3\u6570\u636e\u96c6\u95f4\u5e72\u6270\u5e76\u4fdd\u7559\u5404\u81ea\u5224\u522b\u6027\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u901a\u7528\u534f\u540c\u6df7\u5408\u5f02\u6784\u6e90\u7279\u5b9a\u4e13\u5bb6\uff08Universal Collaborative Mixture of Heterogeneous Source-Specific Experts, COME\uff09\u6846\u67b6\u3002COME\u901a\u8fc7\u5efa\u7acb\u53cc\u91cd\u7ed3\u6784-\u8bed\u4e49\u5171\u4eab\u4e13\u5bb6\u6765\u521b\u5efa\u901a\u7528\u8868\u793a\u7a7a\u95f4\uff0c\u5e76\u4f7f\u5176\u4e0e\u6e90\u7279\u5b9a\u4e13\u5bb6\u534f\u540c\u5de5\u4f5c\uff0c\u901a\u8fc7\u63d0\u4f9b\u4e92\u8865\u7279\u5f81\u6765\u63d0\u53d6\u5224\u522b\u6027\u7279\u5f81\u3002", "result": "\u5728\u5355\u6570\u636e\u96c6\u3001\u5668\u5b98\u5185\u548c\u5668\u5b98\u95f4\u6574\u5408\u6570\u636e\u96c6\u4e09\u79cd\u8bc4\u4f30\u6a21\u5f0f\u4e0b\uff0cCOME\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728\u5e73\u5747\u7cbe\u5ea6\uff08mean AP\uff09\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "COME\u7684\u8bbe\u8ba1\u80fd\u591f\u6709\u6548\u5229\u7528\u8de8\u6570\u636e\u96c6\u7684\u7ecf\u9a8c\u5206\u5e03\uff0c\u4e3a\u5c0f\u6279\u91cf\u6216\u672a\u89c1\u6570\u636e\u573a\u666f\u63d0\u4f9b\u901a\u7528\u7684\u8d85\u58f0\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ece\u800c\u5b9e\u73b0\u9c81\u68d2\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8d85\u58f0\u56fe\u50cf\u591a\u5f02\u6784\u6570\u636e\u96c6\u7684\u6cdb\u5316\u96be\u9898\u3002"}}
{"id": "2508.09471", "pdf": "https://arxiv.org/pdf/2508.09471", "abs": "https://arxiv.org/abs/2508.09471", "authors": ["Omar Bazarbachi", "Zijun Sun", "Yanning Shen"], "title": "EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "As Large Language Models (LLMs) become more widely adopted and scale up in\nsize, the computational and memory challenges involved in deploying these\nmassive foundation models have grown increasingly severe. This underscores the\nurgent need to develop more efficient model variants. Faced with this\nchallenge, the present work introduces EGGS-PTP: an Expander-Graph Guided\nStructured Post-training Pruning method. The proposed approach leverages graph\ntheory to guide the design of N:M structured pruning, effectively reducing\nmodel size and computational demands. By incorporating concepts from expander\ngraphs, EGGS-PTP ensures information flow within the pruned network, preserving\nessential model functionality. Extensive numerical experiments demonstrate that\nEGGS-PTP not only achieves significant acceleration and memory savings due to\nstructured sparsity but also outperforms existing structured pruning techniques\nin terms of accuracy across various LLMs.", "AI": {"tldr": "EGGS-PTP\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u5c55\u56fe\u7684\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\uff0c\u5b9e\u73b0\u663e\u8457\u7684\u52a0\u901f\u3001\u5185\u5b58\u8282\u7701\u5e76\u8d85\u8d8a\u73b0\u6709\u526a\u679d\u6280\u672f\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\u548c\u89c4\u6a21\u589e\u5927\uff0c\u5176\u90e8\u7f72\u9762\u4e34\u65e5\u76ca\u4e25\u91cd\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u53d8\u4f53\u3002", "method": "\u672c\u6587\u63d0\u51faEGGS-PTP\uff08Expander-Graph Guided Structured Post-training Pruning\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u6269\u5c55\u56fe\u5f15\u5bfc\u7684\u7ed3\u6784\u5316\u8bad\u7ec3\u540e\u526a\u679d\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u56fe\u8bba\u6307\u5bfcN:M\u7ed3\u6784\u5316\u526a\u679d\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u6269\u5c55\u56fe\u7684\u6982\u5ff5\u786e\u4fdd\u526a\u679d\u7f51\u7edc\u4e2d\u7684\u4fe1\u606f\u6d41\uff0c\u4ece\u800c\u6709\u6548\u51cf\u5c0f\u6a21\u578b\u5927\u5c0f\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "result": "\u5e7f\u6cdb\u7684\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0cEGGS-PTP\u4e0d\u4ec5\u901a\u8fc7\u7ed3\u6784\u5316\u7a00\u758f\u6027\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u548c\u5185\u5b58\u8282\u7701\uff0c\u800c\u4e14\u5728\u5404\u79cdLLM\u4e0a\u7684\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u7ed3\u6784\u5316\u526a\u679d\u6280\u672f\u3002", "conclusion": "EGGS-PTP\u6210\u529f\u89e3\u51b3\u4e86LLMs\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u526a\u679d\u7b56\u7565\u5728\u4fdd\u6301\u6a21\u578b\u529f\u80fd\u7684\u540c\u65f6\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u51c6\u786e\u6027\uff0c\u4e3aLLM\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.09555", "pdf": "https://arxiv.org/pdf/2508.09555", "abs": "https://arxiv.org/abs/2508.09555", "authors": ["Ahmet \u00d6ztel", "\u0130smet Karaca"], "title": "Topological Invariant-Based Iris Identification via Digital Homology and Machine Learning", "categories": ["cs.CV", "55N31, 55U10, 68U10, 68T07", "I.4.6; I.5.4; G.2.3"], "comment": "10 pages, 5 figures, includes visual abstract, focuses on topological\n  invariants for iris recognition", "summary": "Objective - This study presents a biometric identification method based on\ntopological invariants from 2D iris images, representing iris texture via\nformally defined digital homology and evaluating classification performance.\n  Methods - Each normalized iris image (48x482 pixels) is divided into grids\n(e.g., 6x54 or 3x27). For each subregion, we compute Betti0, Betti1, and their\nratio using a recent algorithm for homology groups in 2D digital images. The\nresulting invariants form a feature matrix used with logistic regression, KNN,\nand SVM (with PCA and 100 randomized repetitions). A convolutional neural\nnetwork (CNN) is trained on raw images for comparison.\n  Results - Logistic regression achieved 97.78 +/- 0.82% accuracy,\noutperforming CNN (96.44 +/- 1.32%) and other feature-based models. The\ntopological features showed high accuracy with low variance.\n  Conclusion - This is the first use of topological invariants from formal\ndigital homology for iris recognition. The method offers a compact,\ninterpretable, and accurate alternative to deep learning, useful when\nexplainability or limited data is important. Beyond iris recognition, it can\napply to other biometrics, medical imaging, materials science, remote sensing,\nand interpretable AI. It runs efficiently on CPU-only systems and produces\nrobust, explainable features valuable for security-critical domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e2D\u8679\u819c\u56fe\u50cf\u62d3\u6251\u4e0d\u53d8\u91cf\u7684\u751f\u7269\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u5b57\u540c\u8c03\u7406\u8bba\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u5728\u8679\u819c\u8bc6\u522b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u4f18\u4e8eCNN\uff0c\u4e14\u5177\u6709\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8e\u8679\u819c\u56fe\u50cf\u62d3\u6251\u4e0d\u53d8\u91cf\u7684\u751f\u7269\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u5f0f\u5b9a\u4e49\u7684\u6570\u5b57\u540c\u8c03\u8868\u793a\u8679\u819c\u7eb9\u7406\uff0c\u5e76\u8bc4\u4f30\u5176\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u5c06\u5f52\u4e00\u5316\u8679\u819c\u56fe\u50cf\u5206\u5272\u6210\u7f51\u683c\uff0c\u5bf9\u6bcf\u4e2a\u5b50\u533a\u57df\u8ba1\u7b97Betti0\u3001Betti1\u53ca\u5176\u6bd4\u7387\u7b49\u62d3\u6251\u4e0d\u53d8\u91cf\u3002\u8fd9\u4e9b\u4e0d\u53d8\u91cf\u6784\u6210\u7279\u5f81\u77e9\u9635\uff0c\u7ed3\u5408\u903b\u8f91\u56de\u5f52\u3001KNN\u548cSVM\u8fdb\u884c\u5206\u7c7b\u3002\u540c\u65f6\u8bad\u7ec3\u4e00\u4e2aCNN\u6a21\u578b\u4f5c\u4e3a\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "\u903b\u8f91\u56de\u5f52\u6a21\u578b\u5728\u62d3\u6251\u7279\u5f81\u4e0a\u5b9e\u73b0\u4e8697.78 +/- 0.82%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8eCNN\uff0896.44 +/- 1.32%\uff09\u53ca\u5176\u4ed6\u57fa\u4e8e\u7279\u5f81\u7684\u6a21\u578b\u3002\u62d3\u6251\u7279\u5f81\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u7387\u548c\u4f4e\u65b9\u5dee\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5c06\u5f62\u5f0f\u6570\u5b57\u540c\u8c03\u7684\u62d3\u6251\u4e0d\u53d8\u91cf\u5e94\u7528\u4e8e\u8679\u819c\u8bc6\u522b\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u4e14\u51c6\u786e\u7684\u6df1\u5ea6\u5b66\u4e60\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u9700\u8981\u53ef\u89e3\u91ca\u6027\u6216\u6570\u636e\u6709\u9650\u65f6\u5c24\u5176\u6709\u7528\uff0c\u5e76\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u751f\u7269\u8bc6\u522b\u3001\u533b\u5b66\u6210\u50cf\u7b49\u9886\u57df\u3002"}}
{"id": "2508.09987", "pdf": "https://arxiv.org/pdf/2508.09987", "abs": "https://arxiv.org/abs/2508.09987", "authors": ["Junyan Ye", "Dongzhi Jiang", "Zihao Wang", "Leqi Zhu", "Zhenghao Hu", "Zilong Huang", "Jun He", "Zhiyuan Yan", "Jinghua Yu", "Hongsheng Li", "Conghui He", "Weijia Li"], "title": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "19 pages, 8 figures", "summary": "Recently, GPT-4o has garnered significant attention for its strong\nperformance in image generation, yet open-source models still lag behind.\nSeveral studies have explored distilling image data from GPT-4o to enhance\nopen-source models, achieving notable progress. However, a key question\nremains: given that real-world image datasets already constitute a natural\nsource of high-quality data, why should we use GPT-4o-generated synthetic data?\nIn this work, we identify two key advantages of synthetic images. First, they\ncan complement rare scenarios in real-world datasets, such as surreal fantasy\nor multi-reference image generation, which frequently occur in user queries.\nSecond, they provide clean and controllable supervision. Real-world data often\ncontains complex background noise and inherent misalignment between text\ndescriptions and image content, whereas synthetic images offer pure backgrounds\nand long-tailed supervision signals, facilitating more accurate text-to-image\nalignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale\nsynthetic dataset generated by GPT-4o, harnessing the power of synthetic image\ndata to address blind spots in real-world coverage. Using this dataset, we\nfine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.\nIn addition, we propose two new evaluation benchmarks for a more accurate and\nchallenging assessment of image generation capabilities: GenEval++, which\nincreases instruction complexity to mitigate score saturation, and\nImagine-Bench, which focuses on evaluating both the understanding and\ngeneration of imaginative content. Echo-4o demonstrates strong performance\nacross standard benchmarks. Moreover, applying Echo-4o-Image to other\nfoundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains\nacross multiple metrics, highlighting the datasets strong transferability.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7GPT-4o\u751f\u6210\u4e86180K\u89c4\u6a21\u7684\u5408\u6210\u56fe\u50cf\u6570\u636e\u96c6Echo-4o-Image\uff0c\u65e8\u5728\u5f25\u8865\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5728\u7a00\u6709\u573a\u666f\u548c\u6e05\u6d01\u76d1\u7763\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u5229\u7528\u8be5\u6570\u636e\u96c6\u5fae\u8c03\u751f\u6210\u6a21\u578b\u5f97\u5230Echo-4o\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u3002\u5b9e\u9a8c\u8bc1\u660eEcho-4o\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u8be5\u6570\u636e\u96c6\u5177\u6709\u5f3a\u5927\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u80fd\u6709\u6548\u63d0\u5347\u5176\u4ed6\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1GPT-4o\u5728\u56fe\u50cf\u751f\u6210\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5f00\u6e90\u6a21\u578b\u4ecd\u6709\u5dee\u8ddd\uff0c\u4e14\u73b0\u6709\u7814\u7a76\u63a2\u7d22\u4e86\u4eceGPT-4o\u84b8\u998f\u56fe\u50cf\u6570\u636e\u4ee5\u589e\u5f3a\u5f00\u6e90\u6a21\u578b\u3002\u7136\u800c\uff0c\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\u662f\uff1a\u65e2\u7136\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5df2\u662f\u9ad8\u8d28\u91cf\u6570\u636e\u6e90\uff0c\u4e3a\u4f55\u8fd8\u9700\u4f7f\u7528GPT-4o\u751f\u6210\u7684\u5408\u6210\u6570\u636e\uff1f\u672c\u7814\u7a76\u65e8\u5728\u56de\u7b54\u6b64\u95ee\u9898\uff0c\u6307\u51fa\u5408\u6210\u6570\u636e\u80fd\u5f25\u8865\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5728\u7a00\u6709\uff08\u5982\u8d85\u73b0\u5b9e\u3001\u591a\u53c2\u8003\uff09\u573a\u666f\u8986\u76d6\u4e0d\u8db3\u7684\u76f2\u70b9\uff0c\u5e76\u63d0\u4f9b\u66f4\u6e05\u6d01\u3001\u53ef\u63a7\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u89e3\u51b3\u771f\u5b9e\u6570\u636e\u4e2d\u7684\u80cc\u666f\u566a\u58f0\u548c\u6587\u672c-\u56fe\u50cf\u9519\u4f4d\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u660e\u786e\u4e86\u5408\u6210\u56fe\u50cf\u5728\u8865\u5145\u7a00\u6709\u573a\u666f\u548c\u63d0\u4f9b\u6e05\u6d01\u76d1\u7763\u65b9\u9762\u7684\u4e24\u5927\u4f18\u52bf\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5229\u7528GPT-4o\u751f\u6210\u4e86180K\u89c4\u6a21\u7684\u5408\u6210\u56fe\u50cf\u6570\u636e\u96c6Echo-4o-Image\u3002\u968f\u540e\uff0c\u4f7f\u7528\u6b64\u6570\u636e\u96c6\u5bf9\u7edf\u4e00\u591a\u6a21\u6001\u751f\u6210\u57fa\u7ebfBagel\u8fdb\u884c\u5fae\u8c03\uff0c\u4ece\u800c\u5f97\u5230\u4e86Echo-4o\u6a21\u578b\u3002\u6b64\u5916\uff0c\u4e3a\u66f4\u51c6\u786e\u548c\u6311\u6218\u6027\u5730\u8bc4\u4f30\u56fe\u50cf\u751f\u6210\u80fd\u529b\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u4e2a\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\uff1aGenEval++\uff08\u589e\u52a0\u6307\u4ee4\u590d\u6742\u6027\u4ee5\u907f\u514d\u5206\u6570\u9971\u548c\uff09\u548cImagine-Bench\uff08\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u60f3\u8c61\u5185\u5bb9\u7684\u7406\u89e3\u548c\u751f\u6210\uff09\u3002", "result": "Echo-4o\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u5c06Echo-4o-Image\u6570\u636e\u96c6\u5e94\u7528\u4e8e\u5176\u4ed6\u57fa\u7840\u6a21\u578b\uff08\u5982OmniGen2\u3001BLIP3-o\uff09\u65f6\uff0c\u4e5f\u6301\u7eed\u5e26\u6765\u4e86\u591a\u4e2a\u6307\u6807\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8fd9\u5145\u5206\u8bc1\u660e\u4e86\u8be5\u6570\u636e\u96c6\u7684\u5f3a\u5927\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "GPT-4o\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u6570\u636e\u5bf9\u4e8e\u63d0\u5347\u5f00\u6e90\u56fe\u50cf\u751f\u6210\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u7a00\u6709\u3001\u590d\u6742\u548c\u9700\u8981\u6e05\u6d01\u76d1\u7763\u7684\u573a\u666f\u3002Echo-4o-Image\u6570\u636e\u96c6\u53ca\u5176\u8bad\u7ec3\u7684Echo-4o\u6a21\u578b\u9a8c\u8bc1\u4e86\u5408\u6210\u6570\u636e\u7684\u5de8\u5927\u4ef7\u503c\u548c\u666e\u9002\u6027\uff0c\u4e3a\u672a\u6765\u9ad8\u8d28\u91cf\u3001\u9ad8\u6027\u80fd\u5f00\u6e90\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u8fc1\u79fb\u7684\u8d44\u6e90\u3002"}}
{"id": "2508.09560", "pdf": "https://arxiv.org/pdf/2508.09560", "abs": "https://arxiv.org/abs/2508.09560", "authors": ["Jiahao Wen", "Hang Yu", "Zhedong Zheng"], "title": "WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization", "categories": ["cs.CV", "cs.RO", "I.4.10"], "comment": "13 pages, 4figures", "summary": "Visual geo-localization for drones faces critical degradation under weather\nperturbations, \\eg, rain and fog, where existing methods struggle with two\ninherent limitations: 1) Heavy reliance on limited weather categories that\nconstrain generalization, and 2) Suboptimal disentanglement of entangled\nscene-weather features through pseudo weather categories. We present\nWeatherPrompt, a multi-modality learning paradigm that establishes\nweather-invariant representations through fusing the image embedding with the\ntext context. Our framework introduces two key contributions: First, a\nTraining-free Weather Reasoning mechanism that employs off-the-shelf large\nmulti-modality models to synthesize multi-weather textual descriptions through\nhuman-like reasoning. It improves the scalability to unseen or complex weather,\nand could reflect different weather strength. Second, to better disentangle the\nscene and weather feature, we propose a multi-modality framework with the\ndynamic gating mechanism driven by the text embedding to adaptively reweight\nand fuse visual features across modalities. The framework is further optimized\nby the cross-modal objectives, including image-text contrastive learning and\nimage-text matching, which maps the same scene with different weather\nconditions closer in the respresentation space. Extensive experiments validate\nthat, under diverse weather conditions, our method achieves competitive recall\nrates compared to state-of-the-art drone geo-localization methods. Notably, it\nimproves Recall@1 by +13.37\\% under night conditions and by 18.69\\% under fog\nand snow conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faWeatherPrompt\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u878d\u5408\u56fe\u50cf\u5d4c\u5165\u4e0e\u6587\u672c\u4e0a\u4e0b\u6587\uff0c\u4e3a\u65e0\u4eba\u673a\u5730\u7406\u5b9a\u4f4d\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u5efa\u7acb\u5929\u6c14\u65e0\u5173\u8868\u793a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u65e0\u4eba\u673a\u89c6\u89c9\u5730\u7406\u5b9a\u4f4d\u5728\u96e8\u3001\u96fe\u7b49\u5929\u6c14\u6270\u52a8\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5c40\u9650\uff1a1) \u8fc7\u5ea6\u4f9d\u8d56\u6709\u9650\u7684\u5929\u6c14\u7c7b\u522b\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\uff1b2) \u901a\u8fc7\u4f2a\u5929\u6c14\u7c7b\u522b\u5bf9\u573a\u666f-\u5929\u6c14\u7279\u5f81\u7684\u89e3\u8026\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u672c\u6587\u63d0\u51faWeatherPrompt\uff0c\u4e00\u4e2a\u591a\u6a21\u6001\u5b66\u4e60\u8303\u5f0f\uff0c\u65e8\u5728\u901a\u8fc7\u878d\u5408\u56fe\u50cf\u5d4c\u5165\u4e0e\u6587\u672c\u4e0a\u4e0b\u6587\u6765\u5efa\u7acb\u5929\u6c14\u65e0\u5173\u8868\u793a\u3002\u5176\u5305\u542b\u4e24\u5927\u5173\u952e\u8d21\u732e\uff1a1) \u514d\u8bad\u7ec3\u5929\u6c14\u63a8\u7406\u673a\u5236\uff0c\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5408\u6210\u591a\u5929\u6c14\u6587\u672c\u63cf\u8ff0\uff0c\u63d0\u9ad8\u5bf9\u672a\u77e5\u6216\u590d\u6742\u5929\u6c14\u7684\u53ef\u6269\u5c55\u6027\uff1b2) \u52a8\u6001\u95e8\u63a7\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7531\u6587\u672c\u5d4c\u5165\u9a71\u52a8\uff0c\u81ea\u9002\u5e94\u5730\u91cd\u65b0\u52a0\u6743\u548c\u878d\u5408\u8de8\u6a21\u6001\u89c6\u89c9\u7279\u5f81\u4ee5\u66f4\u597d\u5730\u89e3\u8026\u573a\u666f\u548c\u5929\u6c14\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u56fe\u50cf-\u6587\u672c\u5bf9\u6bd4\u5b66\u4e60\u548c\u56fe\u50cf-\u6587\u672c\u5339\u914d\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u591a\u79cd\u5929\u6c14\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65e0\u4eba\u673a\u5730\u7406\u5b9a\u4f4d\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u53ec\u56de\u7387\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u591c\u95f4\u6761\u4ef6\u4e0bRecall@1\u63d0\u5347\u4e8613.37%\uff0c\u5728\u96fe\u548c\u96ea\u6761\u4ef6\u4e0b\u63d0\u5347\u4e8618.69%\u3002", "conclusion": "WeatherPrompt\u901a\u8fc7\u5efa\u7acb\u5929\u6c14\u65e0\u5173\u7684\u8868\u793a\u548c\u521b\u65b0\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u89c6\u89c9\u5730\u7406\u5b9a\u4f4d\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u591a\u79cd\u590d\u6742\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.09205", "pdf": "https://arxiv.org/pdf/2508.09205", "abs": "https://arxiv.org/abs/2508.09205", "authors": ["Yoni Schirris", "Eric Marcus", "Jonas Teuwen", "Hugo Horlings", "Efstratios Gavves"], "title": "From Explainable to Explained AI: Ideas for Falsifying and Quantifying Explanations", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 2 figures, 2 tables, submitted at MICCAI IMIMIC workshop", "summary": "Explaining deep learning models is essential for clinical integration of\nmedical image analysis systems. A good explanation highlights if a model\ndepends on spurious features that undermines generalization and harms a subset\nof patients or, conversely, may present novel biological insights. Although\ntechniques like GradCAM can identify influential features, they are measurement\ntools that do not themselves form an explanation. We propose a\nhuman-machine-VLM interaction system tailored to explaining classifiers in\ncomputational pathology, including multi-instance learning for whole-slide\nimages. Our proof of concept comprises (1) an AI-integrated slide viewer to run\nsliding-window experiments to test claims of an explanation, and (2)\nquantification of an explanation's predictiveness using general-purpose\nvision-language models. The results demonstrate that this allows us to\nqualitatively test claims of explanations and can quantifiably distinguish\ncompeting explanations. This offers a practical path from explainable AI to\nexplained AI in digital pathology and beyond. Code and prompts are available at\nhttps://github.com/nki-ai/x2x.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u89e3\u91ca\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u4eba\u673a-VLM\u4ea4\u4e92\u7cfb\u7edf\uff0c\u4ee5\u5b9a\u6027\u548c\u5b9a\u91cf\u5730\u6d4b\u8bd5\u5e76\u533a\u5206\u89e3\u91ca\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u4e34\u5e8a\u5e94\u7528\u6025\u9700\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u8bc6\u522b\u6a21\u578b\u7f3a\u9677\u6216\u53d1\u73b0\u65b0\u7684\u751f\u7269\u5b66\u89c1\u89e3\u3002\u73b0\u6709\u89e3\u91ca\u5de5\u5177\uff08\u5982GradCAM\uff09\u65e0\u6cd5\u5f62\u6210\u5b8c\u6574\u7684\u89e3\u91ca\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8ba1\u7b97\u75c5\u7406\u5b66\u5206\u7c7b\u5668\uff08\u5305\u62ec\u5168\u73bb\u7247\u56fe\u50cf\u591a\u5b9e\u4f8b\u5b66\u4e60\uff09\u7684\u4eba\u673a-VLM\uff08\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff09\u4ea4\u4e92\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u5305\u542b\uff1a1) \u4e00\u4e2aAI\u96c6\u6210\u73bb\u7247\u67e5\u770b\u5668\uff0c\u7528\u4e8e\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u5b9e\u9a8c\u9a8c\u8bc1\u89e3\u91ca\u58f0\u660e\uff1b2) \u4f7f\u7528\u901a\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u91cf\u5316\u89e3\u91ca\u7684\u9884\u6d4b\u80fd\u529b\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u5b9a\u6027\u5730\u6d4b\u8bd5\u89e3\u91ca\u7684\u58f0\u660e\uff0c\u5e76\u5b9a\u91cf\u5730\u533a\u5206\u76f8\u4e92\u7ade\u4e89\u7684\u89e3\u91ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6570\u5b57\u75c5\u7406\u5b66\u53ca\u5176\u4ed6\u9886\u57df\u4ece\u201c\u53ef\u89e3\u91caAI\u201d\u8fc8\u5411\u201c\u5df2\u89e3\u91caAI\u201d\u63d0\u4f9b\u4e86\u5b9e\u9645\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2508.09489", "pdf": "https://arxiv.org/pdf/2508.09489", "abs": "https://arxiv.org/abs/2508.09489", "authors": ["Hao Yu", "Xin Yang", "Boyang Fan", "Xuemei Cao", "Hanlin Gu", "Lixin Fan", "Qiang Yang"], "title": "Large-Small Model Collaborative Framework for Federated Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Continual learning (CL) for Foundation Models (FMs) is an essential yet\nunderexplored challenge, especially in Federated Continual Learning (FCL),\nwhere each client learns from a private, evolving task stream under strict data\nand communication constraints. Despite their powerful generalization abilities,\nFMs often exhibit suboptimal performance on local downstream tasks, as they are\nunable to utilize private local data. Furthermore, enabling FMs to learn new\ntasks without forgetting prior knowledge is inherently a challenging problem,\nprimarily due to their immense parameter count and high model complexity. In\ncontrast, small models can be trained locally under resource-constrained\nconditions and benefit from more mature CL techniques. To bridge the gap\nbetween small models and FMs, we propose the first collaborative framework in\nFCL, where lightweight local models act as a dynamic bridge, continually\nadapting to new tasks while enhancing the utility of the large model. Two novel\ncomponents are also included: Small Model Continual Fine-tuning is for\npreventing small models from temporal forgetting; One-by-One Distillation\nperforms personalized fusion of heterogeneous local knowledge on the server.\nExperimental results demonstrate its superior performance, even when clients\nutilize heterogeneous small models.", "AI": {"tldr": "\u5728\u8054\u90a6\u6301\u7eed\u5b66\u4e60(FCL)\u4e2d\uff0c\u63d0\u51fa\u9996\u4e2a\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u672c\u5730\u6a21\u578b\u4f5c\u4e3a\u52a8\u6001\u6865\u6881\uff0c\u4f7f\u57fa\u7840\u6a21\u578b(FMs)\u80fd\u591f\u6301\u7eed\u9002\u5e94\u65b0\u4efb\u52a1\u5e76\u6709\u6548\u5229\u7528\u672c\u5730\u6570\u636e\uff0c\u540c\u65f6\u514b\u670d\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u8054\u90a6\u6301\u7eed\u5b66\u4e60(FCL)\u4e2d\u7684\u57fa\u7840\u6a21\u578b(FMs)\u9762\u4e34\u6311\u6218\uff1a\u96be\u4ee5\u5229\u7528\u672c\u5730\u79c1\u6709\u6570\u636e\uff0c\u4e14\u56e0\u6a21\u578b\u590d\u6742\u6027\u9ad8\u800c\u6613\u9057\u5fd8\u65e7\u77e5\u8bc6\uff0c\u8868\u73b0\u6b20\u4f73\u3002\u5c0f\u6a21\u578b\u867d\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u672c\u5730\u8bad\u7ec3\u4e14CL\u6280\u672f\u6210\u719f\uff0c\u4f46\u5982\u4f55\u5c06\u5c0f\u6a21\u578b\u4e0eFMs\u7ed3\u5408\u4ee5\u89e3\u51b3FCL\u4e2d\u7684\u6311\u6218\u662f\u4e00\u4e2a\u672a\u5145\u5206\u63a2\u7d22\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u9996\u4e2aFCL\u534f\u4f5c\u6846\u67b6\uff0c\u5176\u4e2d\u8f7b\u91cf\u7ea7\u672c\u5730\u6a21\u578b\u4f5c\u4e3a\u52a8\u6001\u6865\u6881\uff0c\u6301\u7eed\u9002\u5e94\u65b0\u4efb\u52a1\u5e76\u589e\u5f3a\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u6548\u7528\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u65b0\u9896\u7ec4\u4ef6\uff1a1) \u201c\u5c0f\u6a21\u578b\u6301\u7eed\u5fae\u8c03\u201d(Small Model Continual Fine-tuning)\u7528\u4e8e\u9632\u6b62\u5c0f\u6a21\u578b\u53d1\u751f\u65f6\u95f4\u6027\u9057\u5fd8\uff1b2) \u201c\u4e00\u5bf9\u4e00\u84b8\u998f\u201d(One-by-One Distillation)\u7528\u4e8e\u5728\u670d\u52a1\u5668\u7aef\u5bf9\u5f02\u6784\u672c\u5730\u77e5\u8bc6\u8fdb\u884c\u4e2a\u6027\u5316\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5373\u4f7f\u5ba2\u6237\u7aef\u4f7f\u7528\u5f02\u6784\u7684\u5c0f\u6a21\u578b\u4e5f\u80fd\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5f25\u5408\u4e86\u5c0f\u6a21\u578b\u548c\u57fa\u7840\u6a21\u578b\u5728\u8054\u90a6\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5dee\u8ddd\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u7840\u6a21\u578b\u5728FCL\u4e2d\u5229\u7528\u672c\u5730\u6570\u636e\u548c\u9632\u6b62\u77e5\u8bc6\u9057\u5fd8\u7684\u6311\u6218\u3002"}}
{"id": "2508.09565", "pdf": "https://arxiv.org/pdf/2508.09565", "abs": "https://arxiv.org/abs/2508.09565", "authors": ["Ming Zhao", "Pingping Liu", "Tongshun Zhang", "Zhe Zhang"], "title": "WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description", "categories": ["cs.CV"], "comment": null, "summary": "Multi-exposure correction technology is essential for restoring images\naffected by insufficient or excessive lighting, enhancing the visual experience\nby improving brightness, contrast, and detail richness. However, current\nmulti-exposure correction methods often encounter challenges in addressing\nintra-class variability caused by diverse lighting conditions, shooting\nenvironments, and weather factors, particularly when processing images captured\nat a single exposure level. To enhance the adaptability of these models under\ncomplex imaging conditions, this paper proposes a Wavelet-based Exposure\nCorrection method with Degradation Guidance (WEC-DG). Specifically, we\nintroduce a degradation descriptor within the Exposure Consistency Alignment\nModule (ECAM) at both ends of the processing pipeline to ensure exposure\nconsistency and achieve final alignment. This mechanism effectively addresses\nmiscorrected exposure anomalies caused by existing methods' failure to\nrecognize 'blurred' exposure degradation. Additionally, we investigate the\nlight-detail decoupling properties of the wavelet transform to design the\nExposure Restoration and Detail Reconstruction Module (EDRM), which processes\nlow-frequency information related to exposure enhancement before utilizing\nhigh-frequency information as a prior guide for reconstructing spatial domain\ndetails. This serial processing strategy guarantees precise light correction\nand enhances detail recovery. Extensive experiments conducted on multiple\npublic datasets demonstrate that the proposed method outperforms existing\nalgorithms, achieving significant performance improvements and validating its\neffectiveness and practical applicability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u964d\u8d28\u5f15\u5bfc\u7684\u66dd\u5149\u6821\u6b63\u65b9\u6cd5WEC-DG\uff0c\u901a\u8fc7\u5f15\u5165\u964d\u8d28\u63cf\u8ff0\u7b26\u548c\u89e3\u8026\u5149\u7167\u7ec6\u8282\uff0c\u89e3\u51b3\u4e86\u5355\u66dd\u5149\u56fe\u50cf\u6821\u6b63\u4e2d\u7684\u7c7b\u5185\u53d8\u5f02\u6027\u548c\u9519\u8bef\u6821\u6b63\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5149\u7167\u548c\u7ec6\u8282\u6062\u590d\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u66dd\u5149\u6821\u6b63\u65b9\u6cd5\u5728\u5904\u7406\u5355\u66dd\u5149\u56fe\u50cf\u65f6\uff0c\u96be\u4ee5\u5e94\u5bf9\u7531\u5149\u7167\u3001\u73af\u5883\u548c\u5929\u6c14\u7b49\u56e0\u7d20\u5f15\u8d77\u7684\u590d\u6742\u7c7b\u5185\u53d8\u5f02\u6027\uff0c\u4e14\u56e0\u672a\u80fd\u8bc6\u522b\u201c\u6a21\u7cca\u201d\u7684\u66dd\u5149\u964d\u8d28\uff0c\u5e38\u5bfc\u81f4\u9519\u8bef\u6821\u6b63\u3002\u7814\u7a76\u52a8\u673a\u662f\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u6210\u50cf\u6761\u4ef6\u4e0b\u7684\u9002\u5e94\u6027\u3002", "method": "\u672c\u6587\u63d0\u51faWEC-DG\u65b9\u6cd5\u3002\u5177\u4f53\u5305\u62ec\uff1a1. \u66dd\u5149\u4e00\u81f4\u6027\u5bf9\u9f50\u6a21\u5757 (ECAM): \u5728\u5904\u7406\u6d41\u7a0b\u4e24\u7aef\u5f15\u5165\u964d\u8d28\u63cf\u8ff0\u7b26\uff0c\u786e\u4fdd\u66dd\u5149\u4e00\u81f4\u6027\u5e76\u89e3\u51b3\u7531\u201c\u6a21\u7cca\u201d\u66dd\u5149\u964d\u8d28\u5bfc\u81f4\u7684\u9519\u8bef\u6821\u6b63\u30022. \u66dd\u5149\u6062\u590d\u4e0e\u7ec6\u8282\u91cd\u5efa\u6a21\u5757 (EDRM): \u5229\u7528\u5c0f\u6ce2\u53d8\u6362\u7684\u5149\u7167-\u7ec6\u8282\u89e3\u8026\u7279\u6027\uff0c\u9996\u5148\u5904\u7406\u4f4e\u9891\u4fe1\u606f\u4ee5\u589e\u5f3a\u66dd\u5149\uff0c\u7136\u540e\u5c06\u9ad8\u9891\u4fe1\u606f\u4f5c\u4e3a\u5148\u9a8c\u6307\u5bfc\u91cd\u5efa\u7a7a\u95f4\u57df\u7ec6\u8282\uff0c\u91c7\u7528\u4e32\u884c\u5904\u7406\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684WEC-DG\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5355\u66dd\u5149\u56fe\u50cf\u6821\u6b63\u4e2d\u7684\u590d\u6742\u6027\u548c\u9519\u8bef\u6821\u6b63\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u5347\u6a21\u578b\u9002\u5e94\u6027\u5e76\u5b9e\u73b0\u7cbe\u786e\u7684\u5149\u7167\u6821\u6b63\u4e0e\u7ec6\u8282\u6062\u590d\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.09500", "pdf": "https://arxiv.org/pdf/2508.09500", "abs": "https://arxiv.org/abs/2508.09500", "authors": ["Zijun Jiang", "Yangdi Lyu"], "title": "MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI", "categories": ["cs.LG", "cs.AR"], "comment": "9 pages, 6 figures, accepted by ICCAD'25", "summary": "Quantized Neural Networks (QNN) with extremely low-bitwidth data have proven\npromising in efficient storage and computation on edge devices. To further\nreduce the accuracy drop while increasing speedup, layer-wise mixed-precision\nquantization (MPQ) becomes a popular solution. However, existing algorithms for\nexploring MPQ schemes are limited in flexibility and efficiency. Comprehending\nthe complex impacts of different MPQ schemes on post-training quantization and\nquantization-aware training results is a challenge for conventional methods.\nFurthermore, an end-to-end framework for the optimization and deployment of MPQ\nmodels is missing in existing work.\n  In this paper, we propose the MiCo framework, a holistic MPQ exploration and\ndeployment framework for edge AI applications. The framework adopts a novel\noptimization algorithm to search for optimal quantization schemes with the\nhighest accuracies while meeting latency constraints. Hardware-aware latency\nmodels are built for different hardware targets to enable fast explorations.\nAfter the exploration, the framework enables direct deployment from PyTorch MPQ\nmodels to bare-metal C codes, leading to end-to-end speedup with minimal\naccuracy drops.", "AI": {"tldr": "\u63d0\u51faMiCo\u6846\u67b6\uff0c\u4e00\u4e2a\u7528\u4e8e\u8fb9\u7f18AI\u5e94\u7528\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\uff08MPQ\uff09\u63a2\u7d22\u4e0e\u90e8\u7f72\u7684\u6574\u4f53\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\uff08MPQ\uff09\u7b97\u6cd5\u5728\u7075\u6d3b\u6027\u548c\u6548\u7387\u4e0a\u53d7\u9650\uff0c\u96be\u4ee5\u7406\u89e3\u4e0d\u540c\u65b9\u6848\u7684\u590d\u6742\u5f71\u54cd\uff0c\u5e76\u4e14\u7f3a\u4e4f\u7aef\u5230\u7aef\u7684\u4f18\u5316\u4e0e\u90e8\u7f72\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51faMiCo\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u4e00\u79cd\u65b0\u9896\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u6ee1\u8db3\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u641c\u7d22\u6700\u9ad8\u7cbe\u5ea6\u7684\u91cf\u5316\u65b9\u6848\uff1b2) \u4e3a\u4e0d\u540c\u786c\u4ef6\u76ee\u6807\u6784\u5efa\u786c\u4ef6\u611f\u77e5\u5ef6\u8fdf\u6a21\u578b\u4ee5\u5b9e\u73b0\u5feb\u901f\u63a2\u7d22\uff1b3) \u652f\u6301\u4ecePyTorch MPQ\u6a21\u578b\u76f4\u63a5\u90e8\u7f72\u5230\u88f8\u673aC\u4ee3\u7801\u3002", "result": "MiCo\u6846\u67b6\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u52a0\u901f\uff0c\u540c\u65f6\u5c06\u7cbe\u5ea6\u4e0b\u964d\u6700\u5c0f\u5316\u3002", "conclusion": "MiCo\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684MPQ\u63a2\u7d22\u4e0e\u90e8\u7f72\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5730\u5c06\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6a21\u578b\u90e8\u7f72\u5230\u8fb9\u7f18AI\u8bbe\u5907\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u9ad8\u7cbe\u5ea6\u7684\u8868\u73b0\u3002"}}
{"id": "2508.09566", "pdf": "https://arxiv.org/pdf/2508.09566", "abs": "https://arxiv.org/abs/2508.09566", "authors": ["Haibo Jin", "Haoxuan Che", "Sunan He", "Hao Chen"], "title": "A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation", "categories": ["cs.CV"], "comment": "Accepted to IEEE TMI", "summary": "Despite the progress of radiology report generation (RRG), existing works\nface two challenges: 1) The performances in clinical efficacy are\nunsatisfactory, especially for lesion attributes description; 2) the generated\ntext lacks explainability, making it difficult for radiologists to trust the\nresults. To address the challenges, we focus on a trustworthy RRG model, which\nnot only generates accurate descriptions of abnormalities, but also provides\nbasis of its predictions. To this end, we propose a framework named chain of\ndiagnosis (CoD), which maintains a chain of diagnostic process for clinically\naccurate and explainable RRG. It first generates question-answer (QA) pairs via\ndiagnostic conversation to extract key findings, then prompts a large language\nmodel with QA diagnoses for accurate generation. To enhance explainability, a\ndiagnosis grounding module is designed to match QA diagnoses and generated\nsentences, where the diagnoses act as a reference. Moreover, a lesion grounding\nmodule is designed to locate abnormalities in the image, further improving the\nworking efficiency of radiologists. To facilitate label-efficient training, we\npropose an omni-supervised learning strategy with clinical consistency to\nleverage various types of annotations from different datasets. Our efforts lead\nto 1) an omni-labeled RRG dataset with QA pairs and lesion boxes; 2) a\nevaluation tool for assessing the accuracy of reports in describing lesion\nlocation and severity; 3) extensive experiments to demonstrate the\neffectiveness of CoD, where it outperforms both specialist and generalist\nmodels consistently on two RRG benchmarks and shows promising explainability by\naccurately grounding generated sentences to QA diagnoses and images.", "AI": {"tldr": "\u63d0\u51faCoD\u6846\u67b6\uff0c\u901a\u8fc7\u8bca\u65ad\u94fe\u548c\u591a\u7ea7\u63a5\u5730\u5b9e\u73b0\u66f4\u5177\u4e34\u5e8a\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u653e\u5c04\u62a5\u544a\u751f\u6210\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u6709\u6548\u6027\u548c\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u653e\u5c04\u62a5\u544a\u751f\u6210\uff08RRG\uff09\u6a21\u578b\u5728\u4e34\u5e8a\u6709\u6548\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff08\u7279\u522b\u662f\u75c5\u7076\u5c5e\u6027\u63cf\u8ff0\uff09\uff0c\u4e14\u751f\u6210\u7684\u6587\u672c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u5bfc\u81f4\u653e\u5c04\u79d1\u533b\u751f\u96be\u4ee5\u4fe1\u4efb\u7ed3\u679c\u3002", "method": "\u63d0\u51faChain of Diagnosis (CoD) \u6846\u67b6\u3002\u8be5\u6846\u67b6\u9996\u5148\u901a\u8fc7\u8bca\u65ad\u5bf9\u8bdd\u751f\u6210\u95ee\u7b54\uff08QA\uff09\u5bf9\u4ee5\u63d0\u53d6\u5173\u952e\u53d1\u73b0\uff0c\u7136\u540e\u5229\u7528QA\u8bca\u65ad\u63d0\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u62a5\u544a\u751f\u6210\u3002\u4e3a\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u8bbe\u8ba1\u4e86\u8bca\u65ad\u63a5\u5730\u6a21\u5757\uff08\u5c06QA\u8bca\u65ad\u4e0e\u751f\u6210\u8bed\u53e5\u5339\u914d\uff09\u548c\u75c5\u7076\u63a5\u5730\u6a21\u5757\uff08\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u7684\u5f02\u5e38\uff09\u3002\u6b64\u5916\uff0c\u91c7\u7528\u4e00\u79cd\u5177\u6709\u4e34\u5e8a\u4e00\u81f4\u6027\u7684\u5168\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u4ee5\u6709\u6548\u5229\u7528\u4e0d\u540c\u6570\u636e\u96c6\u7684\u591a\u79cd\u6807\u6ce8\u7c7b\u578b\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542bQA\u5bf9\u548c\u75c5\u7076\u6846\u7684\u5168\u6807\u6ce8RRG\u6570\u636e\u96c6\uff1b\u521b\u5efa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u62a5\u544a\u4e2d\u75c5\u7076\u4f4d\u7f6e\u548c\u4e25\u91cd\u6027\u63cf\u8ff0\u51c6\u786e\u6027\u7684\u5de5\u5177\uff1b\u5b9e\u9a8c\u8868\u660e\uff0cCoD\u6a21\u578b\u5728\u4e24\u4e2aRRG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u4e13\u4e1a\u548c\u901a\u7528\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5c06\u751f\u6210\u8bed\u53e5\u51c6\u786e\u5730\u4e0eQA\u8bca\u65ad\u548c\u56fe\u50cf\u5173\u8054\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "CoD\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e86\u653e\u5c04\u62a5\u544a\u751f\u6210\u4e2d\u4e34\u5e8a\u6709\u6548\u6027\u4e0d\u8db3\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u62a5\u544a\u51c6\u786e\u6027\uff0c\u5e76\u663e\u8457\u589e\u5f3a\u4e86\u751f\u6210\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6\uff0c\u63d0\u5347\u4e86\u653e\u5c04\u79d1\u533b\u751f\u7684\u5de5\u4f5c\u6548\u7387\u3002"}}
{"id": "2508.09209", "pdf": "https://arxiv.org/pdf/2508.09209", "abs": "https://arxiv.org/abs/2508.09209", "authors": ["Kun Ming Goh"], "title": "Quantum-Enhanced Generative Adversarial Networks: Comparative Analysis of Classical and Hybrid Quantum-Classical Generative Adversarial Networks", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": "9 pages, 9 figures, 3 tables", "summary": "Generative adversarial networks (GANs) have emerged as a powerful paradigm\nfor producing high-fidelity data samples, yet their performance is constrained\nby the quality of latent representations, typically sampled from classical\nnoise distributions. This study investigates hybrid quantum-classical GANs\n(HQCGANs) in which a quantum generator, implemented via parameterised quantum\ncircuits, produces latent vectors for a classical discriminator. We evaluate a\nclassical GAN alongside three HQCGAN variants with 3, 5, and 7 qubits, using\nQiskit's AerSimulator with realistic noise models to emulate near-term quantum\ndevices. The binary MNIST dataset (digits 0 and 1) is used to align with the\nlow-dimensional latent spaces imposed by current quantum hardware. Models are\ntrained for 150 epochs and assessed with Frechet Inception Distance (FID) and\nKernel Inception Distance (KID). Results show that while the classical GAN\nachieved the best scores, the 7-qubit HQCGAN produced competitive performance,\nnarrowing the gap in later epochs, whereas the 3-qubit model exhibited earlier\nconvergence limitations. Efficiency analysis indicates only moderate training\ntime increases despite quantum sampling overhead. These findings validate the\nfeasibility of noisy quantum circuits as latent priors in GAN architectures,\nhighlighting their potential to enhance generative modelling within the\nconstraints of the noisy intermediate-scale quantum (NISQ) era.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08HQCGANs\uff09\uff0c\u5229\u7528\u5e26\u566a\u58f0\u7684\u91cf\u5b50\u7535\u8def\u4f5c\u4e3a\u6f5c\u5728\u7a7a\u95f4\u5148\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u9ad8\u91cf\u5b50\u6bd4\u7279HQCGANs\u80fd\u8fbe\u5230\u4e0e\u7ecf\u5178GANs\u5ab2\u7f8e\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5728NISQ\u65f6\u4ee3\u5176\u751f\u6210\u5efa\u6a21\u7684\u6f5c\u529b\u3002", "motivation": "\u7ecf\u5178\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u867d\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u6837\u672c\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u9650\u4e8e\u4f20\u7edf\u566a\u58f0\u5206\u5e03\u4ea7\u751f\u7684\u6f5c\u5728\u8868\u793a\u8d28\u91cf\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u91cf\u5b50\u6f5c\u5728\u8868\u793a\u662f\u5426\u80fd\u7a81\u7834\u6b64\u9650\u5236\uff0c\u63d0\u5347\u751f\u6210\u6a21\u578b\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08HQCGANs\uff09\uff0c\u5176\u4e2d\u91cf\u5b50\u751f\u6210\u5668\uff08\u901a\u8fc7\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u5b9e\u73b0\uff09\u4e3a\u7ecf\u5178\u5224\u522b\u5668\u751f\u6210\u6f5c\u5728\u5411\u91cf\u3002\u4f7f\u7528Qiskit\u7684AerSimulator\uff0c\u6a21\u62df\u771f\u5b9e\u566a\u58f0\u73af\u5883\uff0c\u5728\u4e8c\u5143MNIST\u6570\u636e\u96c6\uff08\u6570\u5b570\u548c1\uff09\u4e0a\uff0c\u5c06\u4e00\u4e2a\u7ecf\u5178GAN\u4e0e3\u30015\u30017\u91cf\u5b50\u6bd4\u7279\u7684\u4e09\u79cdHQCGAN\u53d8\u4f53\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002\u6a21\u578b\u8bad\u7ec3150\u4e2a\u5468\u671f\uff0c\u5e76\u4f7f\u7528FID\uff08Frechet Inception Distance\uff09\u548cKID\uff08Kernel Inception Distance\uff09\u6307\u6807\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u7ecf\u5178GAN\u53d6\u5f97\u4e86\u6700\u4f73\u5206\u6570\u3002\u7136\u800c\uff0c7\u91cf\u5b50\u6bd4\u7279\u7684HQCGAN\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u540e\u671f\u8bad\u7ec3\u4e2d\u7f29\u5c0f\u4e86\u4e0e\u7ecf\u5178GAN\u7684\u5dee\u8ddd\uff1b\u800c3\u91cf\u5b50\u6bd4\u7279\u6a21\u578b\u5219\u8868\u73b0\u51fa\u8f83\u65e9\u7684\u6536\u655b\u9650\u5236\u3002\u6548\u7387\u5206\u6790\u8868\u660e\uff0c\u5c3d\u7ba1\u5b58\u5728\u91cf\u5b50\u91c7\u6837\u5f00\u9500\uff0c\u8bad\u7ec3\u65f6\u95f4\u589e\u52a0\u4ecd\u5c5e\u9002\u4e2d\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u9a8c\u8bc1\u4e86\u5728GAN\u67b6\u6784\u4e2d\uff0c\u4f7f\u7528\u5e26\u566a\u58f0\u91cf\u5b50\u7535\u8def\u4f5c\u4e3a\u6f5c\u5728\u5148\u9a8c\u7684\u53ef\u884c\u6027\uff0c\u7a81\u663e\u4e86\u5176\u5728\u566a\u58f0\u4e2d\u95f4\u5c3a\u5ea6\u91cf\u5b50\uff08NISQ\uff09\u65f6\u4ee3\u9650\u5236\u4e0b\u589e\u5f3a\u751f\u6210\u5efa\u6a21\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.09504", "pdf": "https://arxiv.org/pdf/2508.09504", "abs": "https://arxiv.org/abs/2508.09504", "authors": ["Arun Vignesh Malarkkan", "Haoyue Bai", "Dongjie Wang", "Yanjie Fu"], "title": "Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection in Cyber-Physical Systems", "categories": ["cs.LG", "cs.CR"], "comment": "7 Pages, 5 figures, Submission for ACM TKDD", "summary": "With the growing complexity of cyberattacks targeting critical\ninfrastructures such as water treatment networks, there is a pressing need for\nrobust anomaly detection strategies that account for both system\nvulnerabilities and evolving attack patterns. Traditional methods --\nstatistical, density-based, and graph-based models struggle with distribution\nshifts and class imbalance in multivariate time series, often leading to high\nfalse positive rates. To address these challenges, we propose CGAD, a Causal\nGraph-based Anomaly Detection framework designed for reliable cyberattack\ndetection in public infrastructure systems. CGAD follows a two-phase supervised\nframework -- causal profiling and anomaly scoring. First, it learns causal\ninvariant graph structures representing the system's behavior under \"Normal\"\nand \"Attack\" states using Dynamic Bayesian Networks. Second, it employs\nstructural divergence to detect anomalies via causal graph comparison by\nevaluating topological deviations in causal graphs over time. By leveraging\ncausal structures, CGAD achieves superior adaptability and accuracy in\nnon-stationary and imbalanced time series environments compared to conventional\nmachine learning approaches. By uncovering causal structures beneath volatile\nsensor data, our framework not only detects cyberattacks with markedly higher\nprecision but also redefines robustness in anomaly detection, proving\nresilience where traditional models falter under imbalance and drift. Our\nframework achieves substantial gains in F1 and ROC-AUC scores over\nbest-performing baselines across four industrial datasets, demonstrating robust\ndetection of delayed and structurally complex anomalies.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51faCGAD\u6846\u67b6\uff0c\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u56fe\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7684\u7f51\u7edc\u653b\u51fb\u68c0\u6d4b\u3002\u5b83\u901a\u8fc7\u5b66\u4e60\u56e0\u679c\u7ed3\u6784\u5e76\u6bd4\u8f83\u56fe\u62d3\u6251\u504f\u5dee\u6765\u8bc6\u522b\u5f02\u5e38\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u975e\u5e73\u7a33\u548c\u4e0d\u5e73\u8861\u6570\u636e\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u9488\u5bf9\u6c34\u5904\u7406\u7f51\u7edc\u7b49\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7684\u7f51\u7edc\u653b\u51fb\u65e5\u76ca\u590d\u6742\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5e94\u5bf9\u7cfb\u7edf\u8106\u5f31\u6027\u53ca\u4e0d\u65ad\u6f14\u53d8\u653b\u51fb\u6a21\u5f0f\u7684\u9c81\u68d2\u5f02\u5e38\u68c0\u6d4b\u7b56\u7565\u3002\u4f20\u7edf\u65b9\u6cd5\uff08\u7edf\u8ba1\u3001\u5bc6\u5ea6\u3001\u56fe\u57fa\u6a21\u578b\uff09\u5728\u5904\u7406\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u5206\u5e03\u6f02\u79fb\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u9ad8\u8bef\u62a5\u7387\u3002", "method": "\u672c\u6587\u63d0\u51faCGAD\uff08\u57fa\u4e8e\u56e0\u679c\u56fe\u7684\u5f02\u5e38\u68c0\u6d4b\uff09\u6846\u67b6\uff0c\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u76d1\u7763\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u5b83\u4f7f\u7528\u52a8\u6001\u8d1d\u53f6\u65af\u7f51\u7edc\u5b66\u4e60\u5728\u201c\u6b63\u5e38\u201d\u548c\u201c\u653b\u51fb\u201d\u72b6\u6001\u4e0b\u7684\u7cfb\u7edf\u884c\u4e3a\u56e0\u679c\u4e0d\u53d8\u56fe\u7ed3\u6784\uff08\u56e0\u679c\u5256\u6790\uff09\u3002\u5176\u6b21\uff0c\u5b83\u91c7\u7528\u7ed3\u6784\u6563\u5ea6\u901a\u8fc7\u56e0\u679c\u56fe\u6bd4\u8f83\u6765\u68c0\u6d4b\u5f02\u5e38\uff0c\u8bc4\u4f30\u56e0\u679c\u56fe\u968f\u65f6\u95f4\u53d8\u5316\u7684\u62d3\u6251\u504f\u5dee\uff08\u5f02\u5e38\u8bc4\u5206\uff09\u3002", "result": "CGAD\u5728\u975e\u5e73\u7a33\u548c\u4e0d\u5e73\u8861\u65f6\u95f4\u5e8f\u5217\u73af\u5883\u4e2d\uff0c\u76f8\u8f83\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u9002\u5e94\u6027\u548c\u51c6\u786e\u6027\u3002\u5b83\u4ee5\u663e\u8457\u66f4\u9ad8\u7684\u7cbe\u5ea6\u68c0\u6d4b\u7f51\u7edc\u653b\u51fb\uff0c\u5e76\u5728F1\u548cROC-AUC\u5206\u6570\u4e0a\u8d85\u8d8a\u4e86\u56db\u4e2a\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u6700\u4f73\u57fa\u7ebf\uff0c\u6709\u6548\u68c0\u6d4b\u4e86\u5ef6\u8fdf\u548c\u7ed3\u6784\u590d\u6742\u7684\u5f02\u5e38\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u56e0\u679c\u7ed3\u6784\uff0cCGAD\u6846\u67b6\u4e0d\u4ec5\u4ee5\u66f4\u9ad8\u7684\u7cbe\u5ea6\u68c0\u6d4b\u7f51\u7edc\u653b\u51fb\uff0c\u800c\u4e14\u91cd\u65b0\u5b9a\u4e49\u4e86\u5f02\u5e38\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\uff0c\u5728\u4f20\u7edf\u6a21\u578b\u5728\u4e0d\u5e73\u8861\u548c\u6f02\u79fb\u4e0b\u5931\u6548\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc1\u660e\u4e86\u5176\u97e7\u6027\u3002CGAD\u4e3a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7684\u7f51\u7edc\u653b\u51fb\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u53ef\u9760\u548c\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09575", "pdf": "https://arxiv.org/pdf/2508.09575", "abs": "https://arxiv.org/abs/2508.09575", "authors": ["Jiwon Kim", "Pureum Kim", "SeonHwa Kim", "Soobin Park", "Eunju Cha", "Kyong Hwan Jin"], "title": "Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in controllable text-to-image (T2I) diffusion models,\nsuch as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance\ncontrol without requiring auxiliary module training. However, these models\noften struggle to accurately preserve spatial structures and fail to capture\nfine-grained conditions related to object poses and scene layouts. To address\nthese challenges, we propose a training-free Dual Recursive Feedback (DRF)\nsystem that properly reflects control conditions in controllable T2I models.\nThe proposed DRF consists of appearance feedback and generation feedback that\nrecursively refines the intermediate latents to better reflect the given\nappearance information and the user's intent. This dual-update mechanism guides\nlatent representations toward reliable manifolds, effectively integrating\nstructural and appearance attributes. Our approach enables fine-grained\ngeneration even between class-invariant structure-appearance fusion, such as\ntransferring human motion onto a tiger's form. Extensive experiments\ndemonstrate the efficacy of our method in producing high-quality, semantically\ncoherent, and structurally consistent image generations. Our source code is\navailable at https://github.com/jwonkm/DRF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53cc\u9012\u5f52\u53cd\u9988\uff08DRF\uff09\u7684\u65e0\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u9012\u5f52\u4f18\u5316\u4e2d\u95f4\u6f5c\u5728\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u63a7\u6587\u751f\u56fe\u6a21\u578b\u5728\u7cbe\u7ec6\u7a7a\u95f4\u7ed3\u6784\u548c\u6761\u4ef6\u63a7\u5236\u4e0a\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u53ef\u63a7\u6587\u751f\u56fe\uff08T2I\uff09\u6269\u6563\u6a21\u578b\uff08\u5982Ctrl-X\u3001FreeControl\uff09\u5728\u51c6\u786e\u4fdd\u7559\u7a7a\u95f4\u7ed3\u6784\u548c\u6355\u83b7\u7269\u4f53\u59ff\u6001\u3001\u573a\u666f\u5e03\u5c40\u7b49\u7ec6\u7c92\u5ea6\u6761\u4ef6\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u201c\u53cc\u9012\u5f52\u53cd\u9988\uff08Dual Recursive Feedback, DRF\uff09\u201d\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u7531\u5916\u89c2\u53cd\u9988\u548c\u751f\u6210\u53cd\u9988\u7ec4\u6210\uff0c\u901a\u8fc7\u9012\u5f52\u5730\u7cbe\u70bc\u4e2d\u95f4\u6f5c\u5728\u8868\u793a\uff0c\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u7ed9\u5b9a\u7684\u5916\u89c2\u4fe1\u606f\u548c\u7528\u6237\u610f\u56fe\uff0c\u4ece\u800c\u5f15\u5bfc\u6f5c\u5728\u8868\u793a\u81f3\u53ef\u9760\u6d41\u5f62\uff0c\u6709\u6548\u6574\u5408\u7ed3\u6784\u548c\u5916\u89c2\u5c5e\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u751f\u6210\uff0c\u751a\u81f3\u80fd\u8fdb\u884c\u8de8\u7c7b\u522b\u4e0d\u53d8\u7684\u7ed3\u6784-\u5916\u89c2\u878d\u5408\uff08\u4f8b\u5982\u5c06\u4eba\u7c7b\u52a8\u4f5c\u8fc1\u79fb\u5230\u8001\u864e\u5f62\u6001\u4e0a\uff09\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u672c\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8bed\u4e49\u8fde\u8d2f\u4e14\u7ed3\u6784\u4e00\u81f4\u7684\u56fe\u50cf\u3002", "conclusion": "DRF\u7cfb\u7edf\u901a\u8fc7\u5176\u72ec\u7279\u7684\u53cc\u9012\u5f52\u53cd\u9988\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u53ef\u63a7\u6587\u751f\u56fe\u6a21\u578b\u5728\u7cbe\u7ec6\u7a7a\u95f4\u7ed3\u6784\u548c\u6761\u4ef6\u6355\u83b7\u65b9\u9762\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u8bed\u4e49\u548c\u7ed3\u6784\u4e00\u81f4\u7684\u56fe\u50cf\u751f\u6210\uff0c\u4e14\u65e0\u9700\u989d\u5916\u7684\u6a21\u578b\u8bad\u7ec3\u3002"}}
{"id": "2508.09510", "pdf": "https://arxiv.org/pdf/2508.09510", "abs": "https://arxiv.org/abs/2508.09510", "authors": ["Iing Muttakhiroh", "Thomas Fevens"], "title": "Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach", "categories": ["cs.LG"], "comment": null, "summary": "Despite the significant advancements in Large Language Models (LLMs),\ncatastrophic forgetting remains a substantial challenge, where models lose\npreviously acquired knowledge upon learning new information. Continual learning\n(CL) strategies have emerged as a potential solution to this problem, with\nreplay-based techniques demonstrating superior performance in preserving\nlearned knowledge. In this context, we introduce Gauss-Tin, a novel approach\nthat integrates the replay strategy with a Gaussian mixture model to enhance\nthe quality of sample selection during training, supplemented by instructional\nguidance to facilitate the generation of past learning. This method aims to\nimprove LLMs' retention capabilities by strategically reinforcing important\npast learnings while accommodating new information. Our experimental results\nindicate a promising 6\\% improvement in retention metrics over traditional\nmethods, suggesting that Gauss-Tin is an effective strategy for mitigating\ncatastrophic forgetting in LLMs. This study underscores the potential of hybrid\nmodels in enhancing the robustness and adaptability of LLMs in dynamic learning\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGauss-Tin\u65b9\u6cd5\uff0c\u7ed3\u5408\u91cd\u653e\u7b56\u7565\u548c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6307\u4ee4\u5f15\u5bfc\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u80fd\u6709\u6548\u63d0\u9ad8\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u4f9d\u7136\u4e25\u5cfb\uff0c\u5373\u6a21\u578b\u5728\u5b66\u4e60\u65b0\u4fe1\u606f\u65f6\u4f1a\u9057\u5fd8\u5148\u524d\u83b7\u5f97\u7684\u77e5\u8bc6\u3002\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u7b56\u7565\uff0c\u5c24\u5176\u662f\u57fa\u4e8e\u91cd\u653e\u7684\u6280\u672f\uff0c\u88ab\u89c6\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u77e5\u8bc6\u4fdd\u7559\u6027\u80fd\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86Gauss-Tin\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u91cd\u653e\u7b56\u7565\u4e0e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08Gaussian mixture model\uff09\u76f8\u7ed3\u5408\uff0c\u4ee5\u4f18\u5316\u8bad\u7ec3\u671f\u95f4\u7684\u6837\u672c\u9009\u62e9\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u6307\u4ee4\u5f15\u5bfc\uff08instructional guidance\uff09\u6765\u4fc3\u8fdb\u5bf9\u8fc7\u53bb\u5b66\u4e60\u5185\u5bb9\u7684\u751f\u6210\uff0c\u65e8\u5728\u901a\u8fc7\u7b56\u7565\u6027\u5730\u5f3a\u5316\u91cd\u8981\u7684\u65e7\u77e5\u8bc6\u6765\u63d0\u5347LLMs\u7684\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\uff0c\u540c\u65f6\u9002\u5e94\u65b0\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGauss-Tin\u5728\u77e5\u8bc6\u4fdd\u7559\u6307\u6807\u4e0a\u6bd4\u4f20\u7edf\u65b9\u6cd5\u67096%\u7684\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u7f13\u89e3LLMs\u707e\u96be\u6027\u9057\u5fd8\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "Gauss-Tin\u662f\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u707e\u96be\u6027\u9057\u5fd8\u7684\u6709\u6548\u7b56\u7565\uff0c\u8fd9\u9879\u7814\u7a76\u5f3a\u8c03\u4e86\u6df7\u5408\u6a21\u578b\u5728\u589e\u5f3aLLMs\u5728\u52a8\u6001\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.09584", "pdf": "https://arxiv.org/pdf/2508.09584", "abs": "https://arxiv.org/abs/2508.09584", "authors": ["Bei Yan", "Zhiyuan Chen", "Yuecong Min", "Jie Zhang", "Jiahao Wang", "Xiaozhen Wang", "Shiguang Shan"], "title": "SHALE: A Scalable Benchmark for Fine-grained Hallucination Evaluation in LVLMs", "categories": ["cs.CV"], "comment": null, "summary": "Despite rapid advances, Large Vision-Language Models (LVLMs) still suffer\nfrom hallucinations, i.e., generating content inconsistent with input or\nestablished world knowledge, which correspond to faithfulness and factuality\nhallucinations, respectively. Prior studies primarily evaluate faithfulness\nhallucination at a coarse level (e.g., object-level) and lack fine-grained\nanalysis. Additionally, existing benchmarks rely on costly manual curation or\nreused public datasets, raising concerns about scalability and data leakage. To\naddress these limitations, we propose an automated data construction pipeline\nthat produces scalable, controllable, and diverse evaluation data. We also\ndesign a hierarchical hallucination induction framework with input\nperturbations to simulate realistic noisy scenarios. Integrating these designs,\nwe construct SHALE, a Scalable HALlucination Evaluation benchmark designed to\nassess both faithfulness and factuality hallucinations via a fine-grained\nhallucination categorization scheme. SHALE comprises over 30K image-instruction\npairs spanning 12 representative visual perception aspects for faithfulness and\n6 knowledge domains for factuality, considering both clean and noisy scenarios.\nExtensive experiments on over 20 mainstream LVLMs reveal significant factuality\nhallucinations and high sensitivity to semantic perturbations.", "AI": {"tldr": "\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51fa\u4e86\u81ea\u52a8\u5316\u6570\u636e\u6784\u5efa\u548c\u5e7b\u89c9\u8bf1\u5bfc\u6846\u67b6\uff0c\u6784\u5efa\u4e86\u53ef\u6269\u5c55\u7684SHALE\u57fa\u51c6\uff0c\u7528\u4e8e\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u5fe0\u5b9e\u6027\u548c\u4e8b\u5b9e\u6027\u5e7b\u89c9\uff0c\u53d1\u73b0LVLMs\u5728\u4e8b\u5b9e\u6027\u5e7b\u89c9\u548c\u8bed\u4e49\u6270\u52a8\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5c3d\u7ba1LVLMs\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u4ecd\u5b58\u5728\u5e7b\u89c9\uff08\u5185\u5bb9\u4e0e\u8f93\u5165\u4e0d\u4e00\u81f4\u6216\u4e0e\u4e16\u754c\u77e5\u8bc6\u76f8\u6096\uff0c\u5373\u5fe0\u5b9e\u6027\u6216\u4e8b\u5b9e\u6027\u5e7b\u89c9\uff09\u3002\u73b0\u6709\u7814\u7a76\u5bf9\u5fe0\u5b9e\u6027\u5e7b\u89c9\u7684\u8bc4\u4f30\u505c\u7559\u5728\u7c97\u7c92\u5ea6\u5c42\u9762\uff0c\u7f3a\u4e4f\u7ec6\u81f4\u5206\u6790\u3002\u6b64\u5916\uff0c\u73b0\u6709\u57fa\u51c6\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6216\u91cd\u7528\u516c\u5171\u6570\u636e\u96c6\uff0c\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u6cc4\u9732\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6570\u636e\u6784\u5efa\u6d41\u7a0b\uff0c\u7528\u4e8e\u751f\u6210\u53ef\u6269\u5c55\u3001\u53ef\u63a7\u4e14\u591a\u6837\u5316\u7684\u8bc4\u4f30\u6570\u636e\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5206\u5c42\u5e7b\u89c9\u8bf1\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u8f93\u5165\u6270\u52a8\u6a21\u62df\u771f\u5b9e\u7684\u566a\u58f0\u573a\u666f\u3002\u7ed3\u5408\u8fd9\u4e9b\u8bbe\u8ba1\uff0c\u6784\u5efa\u4e86SHALE\uff08Scalable HALlucination Evaluation\uff09\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u5e7b\u89c9\u5206\u7c7b\u65b9\u6848\u8bc4\u4f30\u5fe0\u5b9e\u6027\u548c\u4e8b\u5b9e\u6027\u5e7b\u89c9\u3002SHALE\u5305\u542b\u8d85\u8fc73\u4e07\u4e2a\u56fe\u50cf-\u6307\u4ee4\u5bf9\uff0c\u6db5\u76d612\u4e2a\u4ee3\u8868\u6027\u89c6\u89c9\u611f\u77e5\u65b9\u9762\u548c6\u4e2a\u77e5\u8bc6\u9886\u57df\uff0c\u5e76\u8003\u8651\u4e86\u6e05\u6670\u548c\u566a\u58f0\u573a\u666f\u3002", "result": "\u5bf920\u591a\u4e2a\u4e3b\u6d41LVLM\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u8fd9\u4e9b\u6a21\u578b\u5b58\u5728\u663e\u8457\u7684\u4e8b\u5b9e\u6027\u5e7b\u89c9\uff0c\u5e76\u4e14\u5bf9\u8bed\u4e49\u6270\u52a8\u8868\u73b0\u51fa\u9ad8\u5ea6\u654f\u611f\u6027\u3002", "conclusion": "SHALE\u57fa\u51c6\u4e3aLVLM\u7684\u5e7b\u89c9\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u7ec6\u7c92\u5ea6\u4e14\u5168\u9762\u7684\u65b9\u6cd5\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524dLVLM\u5728\u5904\u7406\u4e8b\u5b9e\u6027\u77e5\u8bc6\u548c\u5e94\u5bf9\u8f93\u5165\u6270\u52a8\u65b9\u9762\u7684\u663e\u8457\u5c40\u9650\u6027\uff0c\u8868\u660e\u9700\u8981\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u7684\u7a33\u5065\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.09212", "pdf": "https://arxiv.org/pdf/2508.09212", "abs": "https://arxiv.org/abs/2508.09212", "authors": ["Sihan Xie", "Thierry Tribout", "Didier Boichard", "Blaise Hanczar", "Julien Chiquet", "Eric Barrey"], "title": "Deep Generative Models for Discrete Genotype Simulation", "categories": ["q-bio.GN", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep generative models open new avenues for simulating realistic genomic data\nwhile preserving privacy and addressing data accessibility constraints. While\nprevious studies have primarily focused on generating gene expression or\nhaplotype data, this study explores generating genotype data in both\nunconditioned and phenotype-conditioned settings, which is inherently more\nchallenging due to the discrete nature of genotype data. In this work, we\ndeveloped and evaluated commonly used generative models, including Variational\nAutoencoders (VAEs), Diffusion Models, and Generative Adversarial Networks\n(GANs), and proposed adaptation tailored to discrete genotype data. We\nconducted extensive experiments on large-scale datasets, including all\nchromosomes from cow and multiple chromosomes from human. Model performance was\nassessed using a well-established set of metrics drawn from both deep learning\nand quantitative genetics literature. Our results show that these models can\neffectively capture genetic patterns and preserve genotype-phenotype\nassociation. Our findings provide a comprehensive comparison of these models\nand offer practical guidelines for future research in genotype simulation. We\nhave made our code publicly available at\nhttps://github.com/SihanXXX/DiscreteGenoGen.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff08VAE\u3001Diffusion\u3001GAN\uff09\u6a21\u62df\u79bb\u6563\u57fa\u56e0\u578b\u6570\u636e\uff0c\u6709\u6548\u6355\u83b7\u9057\u4f20\u6a21\u5f0f\u5e76\u4fdd\u7559\u57fa\u56e0\u578b-\u8868\u578b\u5173\u8054\uff0c\u4e3a\u57fa\u56e0\u578b\u6a21\u62df\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u57fa\u56e0\u8868\u8fbe\u6216\u5355\u500d\u578b\u6570\u636e\u751f\u6210\uff0c\u800c\u79bb\u6563\u57fa\u56e0\u578b\u6570\u636e\u7684\u751f\u6210\u9762\u4e34\u6311\u6218\u4e14\u7814\u7a76\u8f83\u5c11\u3002\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u6570\u636e\u9690\u79c1\u548c\u53ef\u8bbf\u95ee\u6027\u9650\u5236\uff0c\u63a2\u7d22\u5728\u65e0\u6761\u4ef6\u548c\u8868\u578b\u6761\u4ef6\u8bbe\u7f6e\u4e0b\u751f\u6210\u903c\u771f\u7684\u79bb\u6563\u57fa\u56e0\u578b\u6570\u636e\u3002", "method": "\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAEs\uff09\u3001\u6269\u6563\u6a21\u578b\uff08Diffusion Models\uff09\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u7b49\u5e38\u7528\u751f\u6210\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u79bb\u6563\u57fa\u56e0\u578b\u6570\u636e\u7684\u9002\u5e94\u6027\u6539\u8fdb\u3002\u5728\u725b\u548c\u4eba\u7c7b\u7684\u5927\u89c4\u6a21\u67d3\u8272\u4f53\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5e76\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u548c\u6570\u91cf\u9057\u4f20\u5b66\u9886\u57df\u7684\u6210\u719f\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e9b\u6a21\u578b\u80fd\u591f\u6709\u6548\u6355\u83b7\u9057\u4f20\u6a21\u5f0f\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u4fdd\u7559\u57fa\u56e0\u578b\u4e0e\u8868\u578b\u4e4b\u95f4\u7684\u5173\u8054\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5bf9\u5404\u79cd\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u6bd4\u8f83\uff0c\u5e76\u4e3a\u672a\u6765\u57fa\u56e0\u578b\u6a21\u62df\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6307\u5bfc\u65b9\u9488\u3002"}}
{"id": "2508.09527", "pdf": "https://arxiv.org/pdf/2508.09527", "abs": "https://arxiv.org/abs/2508.09527", "authors": ["Fang Wang", "Ernesto Damiani"], "title": "Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring", "categories": ["cs.LG"], "comment": "32 pages", "summary": "Predictive Business Process Monitoring (PBPM) aims to forecast future events\nin ongoing cases based on historical event logs. While Graph Neural Networks\n(GNNs) are well suited to capture structural dependencies in process data,\nexisting GNN-based PBPM models remain underdeveloped. Most rely either on short\nprefix subgraphs or global architectures that overlook temporal relevance and\ntransition semantics. We propose a unified, interpretable GNN framework that\nadvances the state of the art along three key axes. First, we compare\nprefix-based Graph Convolutional Networks(GCNs) and full trace Graph Attention\nNetworks(GATs) to quantify the performance gap between localized and global\nmodeling. Second, we introduce a novel time decay attention mechanism that\nconstructs dynamic, prediction-centered windows, emphasizing temporally\nrelevant history and suppressing noise. Third, we embed transition type\nsemantics into edge features to enable fine grained reasoning over structurally\nambiguous traces. Our architecture includes multilevel interpretability\nmodules, offering diverse visualizations of attention behavior. Evaluated on\nfive benchmarks, the proposed models achieve competitive Top-k accuracy and DL\nscores without per-dataset tuning. By addressing architectural, temporal, and\nsemantic gaps, this work presents a robust, generalizable, and explainable\nsolution for next event prediction in PBPM.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u89e3\u91ca\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u51b3\u73b0\u6709\u9884\u6d4b\u6027\u4e1a\u52a1\u6d41\u7a0b\u76d1\u63a7\uff08PBPM\uff09\u6a21\u578b\u5728\u67b6\u6784\u3001\u65f6\u5e8f\u548c\u8bed\u4e49\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eGNN\u7684\u9884\u6d4b\u6027\u4e1a\u52a1\u6d41\u7a0b\u76d1\u63a7\uff08PBPM\uff09\u6a21\u578b\u53d1\u5c55\u4e0d\u8db3\uff0c\u5b83\u4eec\u591a\u4f9d\u8d56\u4e8e\u77ed\u524d\u7f00\u5b50\u56fe\u6216\u5168\u5c40\u67b6\u6784\uff0c\u672a\u80fd\u5145\u5206\u6355\u6349\u65f6\u95f4\u76f8\u5173\u6027\u548c\u8f6c\u6362\u8bed\u4e49\uff0c\u5bfc\u81f4\u9884\u6d4b\u6027\u80fd\u53d7\u9650\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u53ef\u89e3\u91ca\u7684GNN\u6846\u67b6\u3002\u4e3b\u8981\u65b9\u6cd5\u5305\u62ec\uff1a1. \u6bd4\u8f83\u57fa\u4e8e\u524d\u7f00\u7684\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCNs\uff09\u548c\u5168\u8f68\u8ff9\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GATs\uff09\uff0c\u91cf\u5316\u5c40\u90e8\u4e0e\u5168\u5c40\u5efa\u6a21\u7684\u6027\u80fd\u5dee\u8ddd\u30022. \u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u95f4\u8870\u51cf\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6784\u5efa\u52a8\u6001\u7684\u3001\u4ee5\u9884\u6d4b\u4e3a\u4e2d\u5fc3\u7684\u7a97\u53e3\uff0c\u5f3a\u8c03\u65f6\u95f4\u76f8\u5173\u5386\u53f2\u4fe1\u606f\u5e76\u6291\u5236\u566a\u97f3\u30023. \u5c06\u8f6c\u6362\u7c7b\u578b\u8bed\u4e49\u5d4c\u5165\u5230\u8fb9\u7279\u5f81\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u7ed3\u6784\u6a21\u7cca\u8f68\u8ff9\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\u3002\u6b64\u5916\uff0c\u8be5\u67b6\u6784\u8fd8\u5305\u542b\u591a\u7ea7\u53ef\u89e3\u91ca\u6027\u6a21\u5757\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u65e0\u9700\u9488\u5bf9\u6bcf\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u8c03\u4f18\uff0c\u5373\u53ef\u5b9e\u73b0\u5177\u6709\u7ade\u4e89\u529b\u7684Top-k\u51c6\u786e\u7387\u548cDL\u5206\u6570\u3002", "conclusion": "\u672c\u5de5\u4f5c\u901a\u8fc7\u5f25\u8865\u67b6\u6784\u3001\u65f6\u5e8f\u548c\u8bed\u4e49\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u4e3a\u9884\u6d4b\u6027\u4e1a\u52a1\u6d41\u7a0b\u76d1\u63a7\u4e2d\u7684\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u3001\u53ef\u6cdb\u5316\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09585", "pdf": "https://arxiv.org/pdf/2508.09585", "abs": "https://arxiv.org/abs/2508.09585", "authors": ["Stefan Haag", "Bharanidhar Duraisamy", "Felix Govaers", "Wolfgang Koch", "Martin Fritzsche", "Juergen Dickmann"], "title": "Offline Auto Labeling: BAAS", "categories": ["cs.CV", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper introduces BAAS, a new Extended Object Tracking (EOT) and\nfusion-based label annotation framework for radar detections in autonomous\ndriving. Our framework utilizes Bayesian-based tracking, smoothing and\neventually fusion methods to provide veritable and precise object trajectories\nalong with shape estimation to provide annotation labels on the detection level\nunder various supervision levels. Simultaneously, the framework provides\nevaluation of tracking performance and label annotation. If manually labeled\ndata is available, each processing module can be analyzed independently or\ncombined with other modules to enable closed-loop continuous improvements. The\nframework performance is evaluated in a challenging urban real-world scenario\nin terms of tracking performance and the label annotation errors. We\ndemonstrate the functionality of the proposed approach for varying dynamic\nobjects and class types", "AI": {"tldr": "BAAS\u662f\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u96f7\u8fbe\u68c0\u6d4b\u7684\u6269\u5c55\u76ee\u6807\u8ddf\u8e2a\uff08EOT\uff09\u548c\u878d\u5408\u6807\u6ce8\u6846\u67b6\uff0c\u5229\u7528\u8d1d\u53f6\u65af\u65b9\u6cd5\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u8f68\u8ff9\u548c\u5f62\u72b6\u4f30\u8ba1\uff0c\u652f\u6301\u6027\u80fd\u8bc4\u4f30\u53ca\u95ed\u73af\u6539\u8fdb\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u96f7\u8fbe\u68c0\u6d4b\u7684\u7cbe\u786e\u6269\u5c55\u76ee\u6807\u8ddf\u8e2a\uff08EOT\uff09\u548c\u6807\u7b7e\u6807\u6ce8\u95ee\u9898\uff0c\u63d0\u4f9b\u771f\u5b9e\u4e14\u7cbe\u786e\u7684\u7269\u4f53\u8f68\u8ff9\u3001\u5f62\u72b6\u4f30\u8ba1\u4ee5\u53ca\u68c0\u6d4b\u7ea7\u6807\u6ce8\u3002", "method": "\u63d0\u51faBAAS\u6846\u67b6\uff0c\u7ed3\u5408EOT\u548c\u878d\u5408\u6280\u672f\u3002\u91c7\u7528\u57fa\u4e8e\u8d1d\u53f6\u65af\u7684\u8ddf\u8e2a\u3001\u5e73\u6ed1\u548c\u878d\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u4f9b\u7269\u4f53\u8f68\u8ff9\u548c\u5f62\u72b6\u4f30\u8ba1\uff0c\u8fdb\u800c\u751f\u6210\u6807\u6ce8\u6807\u7b7e\u3002\u6846\u67b6\u5185\u7f6e\u8ddf\u8e2a\u6027\u80fd\u548c\u6807\u7b7e\u6807\u6ce8\u8bc4\u4f30\u529f\u80fd\uff0c\u5e76\u652f\u6301\u4e0e\u624b\u52a8\u6807\u6ce8\u6570\u636e\u7ed3\u5408\u8fdb\u884c\u6a21\u5757\u5316\u5206\u6790\u548c\u95ed\u73af\u6301\u7eed\u6539\u8fdb\u3002", "result": "\u5728\u6311\u6218\u6027\u7684\u57ce\u5e02\u771f\u5b9e\u573a\u666f\u4e2d\u8bc4\u4f30\u4e86\u6846\u67b6\u6027\u80fd\uff0c\u5305\u62ec\u8ddf\u8e2a\u6027\u80fd\u548c\u6807\u7b7e\u6807\u6ce8\u9519\u8bef\u7387\u3002\u6210\u529f\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5bf9\u591a\u79cd\u52a8\u6001\u7269\u4f53\u548c\u7c7b\u522b\u7c7b\u578b\u7684\u529f\u80fd\u6027\u3002", "conclusion": "BAAS\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u96f7\u8fbe\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u5957\u9ad8\u6548\u3001\u7cbe\u786e\u7684EOT\u548c\u878d\u5408\u6807\u6ce8\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8f68\u8ff9\u548c\u5f62\u72b6\u4f30\u8ba1\uff0c\u4e14\u652f\u6301\u6027\u80fd\u8bc4\u4f30\u548c\u6301\u7eed\u4f18\u5316\u3002"}}
{"id": "2508.09215", "pdf": "https://arxiv.org/pdf/2508.09215", "abs": "https://arxiv.org/abs/2508.09215", "authors": ["Kerem Delikoyun", "Qianyu Chen", "Liu Wei", "Si Ko Myo", "Johannes Krell", "Martin Schlegel", "Win Sen Kuan", "John Tshon Yit Soong", "Gerhard Schneider", "Clarissa Prazeres da Costa", "Percy A. Knolle", "Laurent Renia", "Matthew Edward Cove", "Hwee Kuan Lee", "Klaus Diepold", "Oliver Hayden"], "title": "Real-time deep learning phase imaging flow cytometer reveals blood cell aggregate biomarkers for haematology diagnostics", "categories": ["q-bio.QM", "cs.AI", "cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "While analysing rare blood cell aggregates remains challenging in automated\nhaematology, they could markedly advance label-free functional diagnostics.\nConventional flow cytometers efficiently perform cell counting with leukocyte\ndifferentials but fail to identify aggregates with flagged results, requiring\nmanual reviews. Quantitative phase imaging flow cytometry captures detailed\naggregate morphologies, but clinical use is hampered by massive data storage\nand offline processing. Incorporating hidden biomarkers into routine\nhaematology panels would significantly improve diagnostics without flagged\nresults. We present RT-HAD, an end-to-end deep learning-based image and data\nprocessing framework for off-axis digital holographic microscopy (DHM), which\ncombines physics-consistent holographic reconstruction and detection,\nrepresenting each blood cell in a graph to recognize aggregates. RT-HAD\nprocesses >30 GB of image data on-the-fly with turnaround time of <1.5 min and\nerror rate of 8.9% in platelet aggregate detection, which matches acceptable\nlaboratory error rates of haematology biomarkers and solves the big data\nchallenge for point-of-care diagnostics.", "AI": {"tldr": "RT-HAD\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u79bb\u8f74\u6570\u5b57\u5168\u606f\u663e\u5fae\u955c\u56fe\u50cf\u5904\u7406\u6846\u67b6\uff0c\u80fd\u5b9e\u65f6\u9ad8\u6548\u68c0\u6d4b\u8840\u6db2\u7ec6\u80de\u805a\u96c6\u4f53\uff0c\u89e3\u51b3\u4e86\u5927\u6570\u636e\u6311\u6218\uff0c\u9002\u7528\u4e8e\u5373\u65f6\u8bca\u65ad\u3002", "motivation": "\u81ea\u52a8\u5316\u8840\u6db2\u5b66\u4e2d\u7a00\u6709\u8840\u7ec6\u80de\u805a\u96c6\u4f53\u7684\u5206\u6790\u5177\u6709\u6311\u6218\u6027\uff0c\u5e38\u89c4\u6d41\u5f0f\u7ec6\u80de\u4eea\u65e0\u6cd5\u8bc6\u522b\u805a\u96c6\u4f53\uff0c\u800c\u5b9a\u91cf\u76f8\u4f4d\u6210\u50cf\u6d41\u5f0f\u7ec6\u80de\u4eea\u5219\u9762\u4e34\u6d77\u91cf\u6570\u636e\u5b58\u50a8\u548c\u79bb\u7ebf\u5904\u7406\u7684\u96be\u9898\u3002\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u6807\u8bb0\u3001\u80fd\u5b9e\u65f6\u8bc6\u522b\u805a\u96c6\u4f53\u5e76\u878d\u5165\u5e38\u89c4\u8840\u6db2\u5b66\u8bca\u65ad\u7684\u65b9\u6848\uff0c\u5bf9\u4e8e\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51faRT-HAD\uff0c\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u56fe\u50cf\u548c\u6570\u636e\u5904\u7406\u6846\u67b6\uff0c\u4e13\u7528\u4e8e\u79bb\u8f74\u6570\u5b57\u5168\u606f\u663e\u5fae\u955c(DHM)\u6570\u636e\u3002\u5b83\u7ed3\u5408\u4e86\u7269\u7406\u4e00\u81f4\u7684\u5168\u606f\u91cd\u5efa\u548c\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u56fe\u7ed3\u6784\u8868\u793a\u6bcf\u4e2a\u8840\u7ec6\u80de\u4ee5\u8bc6\u522b\u805a\u96c6\u4f53\u3002", "result": "RT-HAD\u80fd\u5b9e\u65f6\u5904\u7406\u8d85\u8fc730 GB\u7684\u56fe\u50cf\u6570\u636e\uff0c\u5468\u8f6c\u65f6\u95f4\u5c11\u4e8e1.5\u5206\u949f\u3002\u5728\u8840\u5c0f\u677f\u805a\u96c6\u4f53\u68c0\u6d4b\u4e2d\u7684\u9519\u8bef\u7387\u4e3a8.9%\uff0c\u7b26\u5408\u53ef\u63a5\u53d7\u7684\u8840\u6db2\u5b66\u751f\u7269\u6807\u5fd7\u7269\u5b9e\u9a8c\u5ba4\u9519\u8bef\u7387\uff0c\u5e76\u6210\u529f\u89e3\u51b3\u4e86\u5373\u65f6\u8bca\u65ad\u4e2d\u7684\u5927\u6570\u636e\u6311\u6218\u3002", "conclusion": "RT-HAD\u4e3a\u8840\u6db2\u7ec6\u80de\u805a\u96c6\u4f53\u7684\u9ad8\u6548\u3001\u51c6\u786e\u3001\u5b9e\u65f6\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u6027\u80fd\u6307\u6807\u6ee1\u8db3\u4e34\u5e8a\u5b9e\u9a8c\u5ba4\u8981\u6c42\uff0c\u6709\u671b\u63a8\u52a8\u5373\u65f6\u8bca\u65ad\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.09593", "pdf": "https://arxiv.org/pdf/2508.09593", "abs": "https://arxiv.org/abs/2508.09593", "authors": ["Haotian Tang", "Jianwei Chen", "Xinrui Tang", "Yunjia Wu", "Zhengyang Miao", "Chao Li"], "title": "Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Isocitrate DeHydrogenase (IDH) mutation status is a crucial biomarker for\nglioma prognosis. However, current prediction methods are limited by the low\navailability and noise of functional MRI. Structural and morphological\nconnectomes offer a non-invasive alternative, yet existing approaches often\nignore the brain's hierarchical organisation and multiscale interactions. To\naddress this, we propose Hi-SMGNN, a hierarchical framework that integrates\nstructural and morphological connectomes from regional to modular levels. It\nfeatures a multimodal interaction module with a Siamese network and cross-modal\nattention, a multiscale feature fusion mechanism for reducing redundancy, and a\npersonalised modular partitioning strategy to enhance individual specificity\nand interpretability. Experiments on the UCSF-PDGM dataset demonstrate that\nHi-SMGNN outperforms baseline and state-of-the-art models, showing improved\nrobustness and effectiveness in IDH mutation prediction.", "AI": {"tldr": "\u63d0\u51faHi-SMGNN\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u7ed3\u6784\u548c\u5f62\u6001\u8fde\u63a5\u7ec4\uff0c\u4ee5\u5c42\u6b21\u5316\u548c\u591a\u5c3a\u5ea6\u65b9\u5f0f\u9884\u6d4b\u80f6\u8d28\u7624IDH\u7a81\u53d8\u72b6\u6001\u3002", "motivation": "IDH\u7a81\u53d8\u662f\u80f6\u8d28\u7624\u9884\u540e\u7684\u5173\u952e\u751f\u7269\u6807\u5fd7\u7269\u3002\u73b0\u6709\u529f\u80fdMRI\u9884\u6d4b\u65b9\u6cd5\u53d7\u53ef\u7528\u6027\u4f4e\u548c\u566a\u58f0\u9650\u5236\u3002\u7ed3\u6784\u548c\u5f62\u6001\u8fde\u63a5\u7ec4\u867d\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5e38\u5ffd\u7565\u5927\u8111\u7684\u5c42\u6b21\u7ec4\u7ec7\u548c\u591a\u5c3a\u5ea6\u4ea4\u4e92\u3002", "method": "\u63d0\u51faHi-SMGNN\u6846\u67b6\uff0c\u6574\u5408\u533a\u57df\u5230\u6a21\u5757\u5c42\u9762\u7684\u7ed3\u6784\u548c\u5f62\u6001\u8fde\u63a5\u7ec4\u3002\u5176\u7279\u70b9\u5305\u62ec\uff1a1) \u91c7\u7528Siamese\u7f51\u7edc\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u6a21\u5757\uff1b2) \u7528\u4e8e\u51cf\u5c11\u5197\u4f59\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u673a\u5236\uff1b3) \u589e\u5f3a\u4e2a\u4f53\u7279\u5f02\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u4e2a\u6027\u5316\u6a21\u5757\u5212\u5206\u7b56\u7565\u3002", "result": "\u5728UCSF-PDGM\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHi-SMGNN\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u548c\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5728IDH\u7a81\u53d8\u9884\u6d4b\u4e2d\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "Hi-SMGNN\u4e3aIDH\u7a81\u53d8\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9c81\u68d2\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5145\u5206\u5229\u7528\u4e86\u5927\u8111\u7684\u5c42\u6b21\u7ec4\u7ec7\u548c\u591a\u5c3a\u5ea6\u4fe1\u606f\u3002"}}
{"id": "2508.09544", "pdf": "https://arxiv.org/pdf/2508.09544", "abs": "https://arxiv.org/abs/2508.09544", "authors": ["Sasan Tavakkol", "Lin Chen", "Max Springer", "Abigail Schantz", "Bla\u017e Bratani\u010d", "Vincent Cohen-Addad", "MohammadHossein Bateni"], "title": "SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification", "categories": ["cs.LG"], "comment": null, "summary": "Scarcity of labeled data, especially for rare events, hinders training\neffective machine learning models. This paper proposes SYNAPSE-G (Synthetic\nAugmentation for Positive Sampling via Expansion on Graphs), a novel pipeline\nleveraging Large Language Models (LLMs) to generate synthetic training data for\nrare event classification, addressing the cold-start problem. This synthetic\ndata serve as seeds for semi-supervised label propagation on a similarity graph\nconstructed between the seeds and a large unlabeled dataset. This identifies\ncandidate positive examples, subsequently labeled by an oracle (human or LLM).\nThe expanded dataset then trains/fine-tunes a classifier. We theoretically\nanalyze how the quality (validity and diversity) of the synthetic data impacts\nthe precision and recall of our method. Experiments on the imbalanced SST2 and\nMHS datasets demonstrate SYNAPSE-G's effectiveness in finding positive labels,\noutperforming baselines including nearest neighbor search.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSYNAPSE-G\uff0c\u4e00\u4e2a\u5229\u7528LLMs\u751f\u6210\u7a00\u6709\u4e8b\u4ef6\u5408\u6210\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u56fe\u4f20\u64ad\u8fdb\u884c\u534a\u76d1\u7763\u5b66\u4e60\uff0c\u4ee5\u89e3\u51b3\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u7684\u51b7\u542f\u52a8\u95ee\u9898\u5e76\u6709\u6548\u6269\u5145\u6807\u7b7e\u7684\u65b9\u6cd5\u3002", "motivation": "\u6807\u8bb0\u6570\u636e\uff0c\u7279\u522b\u662f\u7a00\u6709\u4e8b\u4ef6\u7684\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\uff0c\u963b\u788d\u4e86\u8bad\u7ec3\u6709\u6548\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u51b7\u542f\u52a8\u95ee\u9898\u65f6\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86SYNAPSE-G\u6d41\u6c34\u7ebf\uff0c\u9996\u5148\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u7a00\u6709\u4e8b\u4ef6\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u4ee5\u89e3\u51b3\u51b7\u542f\u52a8\u95ee\u9898\u3002\u8fd9\u4e9b\u5408\u6210\u6570\u636e\u4f5c\u4e3a\u79cd\u5b50\uff0c\u5728\u4e00\u4e2a\u7531\u79cd\u5b50\u4e0e\u5927\u91cf\u672a\u6807\u8bb0\u6570\u636e\u6784\u5efa\u7684\u76f8\u4f3c\u56fe\u4e0a\u8fdb\u884c\u534a\u76d1\u7763\u6807\u7b7e\u4f20\u64ad\uff0c\u4ee5\u8bc6\u522b\u5019\u9009\u9633\u6027\u6837\u672c\u3002\u8fd9\u4e9b\u6837\u672c\u968f\u540e\u7531\u9884\u8a00\u673a\uff08\u4eba\u5de5\u6216LLM\uff09\u8fdb\u884c\u6807\u8bb0\u3002\u6269\u5145\u540e\u7684\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3/\u5fae\u8c03\u5206\u7c7b\u5668\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86\u5408\u6210\u6570\u636e\uff08\u6709\u6548\u6027\u548c\u591a\u6837\u6027\uff09\u7684\u8d28\u91cf\u5982\u4f55\u5f71\u54cd\u65b9\u6cd5\u7684\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u3002", "result": "\u5728\u4e0d\u5e73\u8861\u7684SST2\u548cMHS\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSYNAPSE-G\u5728\u53d1\u73b0\u9633\u6027\u6807\u7b7e\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u5176\u6027\u80fd\u4f18\u4e8e\u5305\u62ec\u6700\u8fd1\u90bb\u641c\u7d22\u5728\u5185\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SYNAPSE-G\u901a\u8fc7\u7ed3\u5408LLMs\u751f\u6210\u5408\u6210\u6570\u636e\u4e0e\u56fe\u4e0a\u7684\u534a\u76d1\u7763\u6807\u7b7e\u4f20\u64ad\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u6709\u4e8b\u4ef6\u5206\u7c7b\u4e2d\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u5728\u53d1\u73b0\u9633\u6027\u6807\u7b7e\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2508.09597", "pdf": "https://arxiv.org/pdf/2508.09597", "abs": "https://arxiv.org/abs/2508.09597", "authors": ["Heyi Sun", "Cong Wang", "Tian-Xing Xu", "Jingwei Huang", "Di Kang", "Chunchao Guo", "Song-Hai Zhang"], "title": "SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing", "categories": ["cs.CV"], "comment": null, "summary": "Creating high-fidelity and editable head avatars is a pivotal challenge in\ncomputer vision and graphics, boosting many AR/VR applications. While recent\nadvancements have achieved photorealistic renderings and plausible animation,\nhead editing, especially real-time appearance editing, remains challenging due\nto the implicit representation and entangled modeling of the geometry and\nglobal appearance. To address this, we propose Surface-Volumetric Gaussian Head\nAvatar (SVG-Head), a novel hybrid representation that explicitly models the\ngeometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled\ntexture images to capture the global appearance. Technically, it contains two\ntypes of Gaussians, in which surface Gaussians explicitly model the appearance\nof head avatars using learnable texture images, facilitating real-time texture\nediting, while volumetric Gaussians enhance the reconstruction quality of\nnon-Lambertian regions (e.g., lips and hair). To model the correspondence\nbetween 3D world and texture space, we provide a mesh-aware Gaussian UV mapping\nmethod, which leverages UV coordinates given by the FLAME mesh to obtain sharp\ntexture images and real-time rendering speed. A hierarchical optimization\nstrategy is further designed to pursue the optimal performance in both\nreconstruction quality and editing flexibility. Experiments on the NeRSemble\ndataset show that SVG-Head not only generates high-fidelity rendering results,\nbut also is the first method to obtain explicit texture images for Gaussian\nhead avatars and support real-time appearance editing.", "AI": {"tldr": "\u63d0\u51faSVG-Head\uff0c\u4e00\u79cd\u6df7\u5408\u9ad8\u65af\u8868\u793a\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u51e0\u4f55\u548c\u5206\u79bb\u7eb9\u7406\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5934\u90e8\u6570\u5b57\u4eba\u6e32\u67d3\u53ca\u5b9e\u65f6\u5916\u89c2\u7f16\u8f91\u3002", "motivation": "\u9ad8\u4fdd\u771f\u3001\u53ef\u7f16\u8f91\u7684\u5934\u90e8\u6570\u5b57\u4eba\u662fAR/VR\u5e94\u7528\u7684\u5173\u952e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u9690\u5f0f\u8868\u793a\u548c\u51e0\u4f55\u4e0e\u5916\u89c2\u7684\u7ea0\u7f20\u5efa\u6a21\uff0c\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u5916\u89c2\u7f16\u8f91\u3002", "method": "SVG-Head\u91c7\u7528\u6df7\u5408\u8868\u793a\uff0c\u5c063D\u9ad8\u65af\u7ed1\u5b9a\u81f3FLAME\u7f51\u683c\u663e\u5f0f\u5efa\u6a21\u51e0\u4f55\uff0c\u5e76\u5229\u7528\u89e3\u8026\u7eb9\u7406\u56fe\u50cf\u6355\u83b7\u5168\u5c40\u5916\u89c2\u3002\u5305\u542b\u8868\u9762\u9ad8\u65af\uff08\u7528\u4e8e\u5b9e\u65f6\u7eb9\u7406\u7f16\u8f91\uff09\u548c\u4f53\u79ef\u9ad8\u65af\uff08\u589e\u5f3a\u975e\u6717\u4f2f\u533a\u57df\u91cd\u5efa\uff09\u3002\u5f15\u5165\u7f51\u683c\u611f\u77e5\u9ad8\u65afUV\u6620\u5c04\u548c\u5206\u5c42\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5728NeRSemble\u6570\u636e\u96c6\u4e0a\uff0cSVG-Head\u751f\u6210\u9ad8\u4fdd\u771f\u6e32\u67d3\u7ed3\u679c\uff0c\u662f\u9996\u4e2a\u4e3a\u9ad8\u65af\u5934\u90e8\u6570\u5b57\u4eba\u63d0\u4f9b\u663e\u5f0f\u7eb9\u7406\u56fe\u50cf\u5e76\u652f\u6301\u5b9e\u65f6\u5916\u89c2\u7f16\u8f91\u7684\u65b9\u6cd5\u3002", "conclusion": "SVG-Head\u901a\u8fc7\u5176\u72ec\u7279\u7684\u6df7\u5408\u9ad8\u65af\u8868\u793a\u548c\u5206\u79bb\u5efa\u6a21\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5934\u90e8\u6570\u5b57\u4eba\u5b9e\u65f6\u5916\u89c2\u7f16\u8f91\u7684\u6311\u6218\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09219", "pdf": "https://arxiv.org/pdf/2508.09219", "abs": "https://arxiv.org/abs/2508.09219", "authors": ["Wilder Baldwin", "Sepideh Ghanavati", "Manuel Woersdoerfer"], "title": "Understanding Ethical Practices in AI: Insights from a Cross-Role, Cross-Region Survey of AI Development Teams", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.SE"], "comment": "Under Review", "summary": "Recent advances in AI applications have raised growing concerns about the\nneed for ethical guidelines and regulations to mitigate the risks posed by\nthese technologies. In this paper, we present a mixed-method survey study -\ncombining statistical and qualitative analyses - to examine the ethical\nperceptions, practices, and knowledge of individuals involved in various AI\ndevelopment roles. Our survey includes 414 participants from 43 countries,\nrepresenting roles such as AI managers, analysts, developers, quality assurance\nprofessionals, and information security and privacy experts. The results reveal\nvarying degrees of familiarity and experience with AI ethics principles,\ngovernment initiatives, and risk mitigation strategies across roles, regions,\nand other demographic factors. Our findings highlight the importance of a\ncollaborative, role-sensitive approach, involving diverse stakeholders in\nethical decision-making throughout the AI development lifecycle. We advocate\nfor developing tailored, inclusive solutions to address ethical challenges in\nAI development, and we propose future research directions and educational\nstrategies to promote ethics-aware AI practices.", "AI": {"tldr": "\u4e00\u9879\u5bf9\u5168\u7403AI\u5f00\u53d1\u4eba\u5458\u4f26\u7406\u8ba4\u77e5\u7684\u6df7\u5408\u65b9\u6cd5\u8c03\u67e5\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u89d2\u8272\u3001\u5730\u57df\u95f4\u5728AI\u4f26\u7406\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u5f3a\u8c03\u9700\u91c7\u53d6\u534f\u4f5c\u4e14\u89d2\u8272\u654f\u611f\u7684\u65b9\u6cd5\u5e94\u5bf9\u4f26\u7406\u6311\u6218\u3002", "motivation": "\u9274\u4e8eAI\u5e94\u7528\u5e26\u6765\u7684\u98ce\u9669\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u5236\u5b9a\u4f26\u7406\u6307\u5357\u548c\u6cd5\u89c4\uff0c\u56e0\u6b64\u7814\u7a76AI\u5f00\u53d1\u76f8\u5173\u4eba\u5458\u7684\u4f26\u7406\u8ba4\u77e5\u3001\u5b9e\u8df5\u548c\u77e5\u8bc6\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u8c03\u67e5\u7814\u7a76\uff0c\u7ed3\u5408\u7edf\u8ba1\u548c\u5b9a\u6027\u5206\u6790\uff0c\u5bf9\u6765\u81ea43\u4e2a\u56fd\u5bb6\u7684414\u540dAI\u5f00\u53d1\u76f8\u5173\u89d2\u8272\uff08\u5982AI\u7ecf\u7406\u3001\u5206\u6790\u5e08\u3001\u5f00\u53d1\u8005\u3001\u8d28\u4fdd\u4eba\u5458\u3001\u4fe1\u606f\u5b89\u5168\u4e13\u5bb6\u7b49\uff09\u7684\u4f26\u7406\u8ba4\u77e5\u3001\u5b9e\u8df5\u548c\u77e5\u8bc6\u8fdb\u884c\u8003\u5bdf\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u4e0d\u540c\u89d2\u8272\u3001\u533a\u57df\u548c\u5176\u4ed6\u4eba\u53e3\u56e0\u7d20\u7684\u53c2\u4e0e\u8005\u5728AI\u4f26\u7406\u539f\u5219\u3001\u653f\u5e9c\u5021\u8bae\u548c\u98ce\u9669\u7f13\u89e3\u7b56\u7565\u65b9\u9762\u7684\u719f\u6089\u7a0b\u5ea6\u548c\u7ecf\u9a8c\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u5728AI\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\uff0c\u5e94\u91c7\u53d6\u534f\u4f5c\u3001\u89d2\u8272\u654f\u611f\u7684\u65b9\u6cd5\uff0c\u8ba9\u591a\u5143\u5229\u76ca\u76f8\u5173\u8005\u53c2\u4e0e\u4f26\u7406\u51b3\u7b56\u3002\u6587\u7ae0\u5021\u5bfc\u5f00\u53d1\u5b9a\u5236\u5316\u3001\u5305\u5bb9\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u548c\u6559\u80b2\u7b56\u7565\u4ee5\u4fc3\u8fdb\u4f26\u7406\u610f\u8bc6\u7684AI\u5b9e\u8df5\u3002"}}
{"id": "2508.09561", "pdf": "https://arxiv.org/pdf/2508.09561", "abs": "https://arxiv.org/abs/2508.09561", "authors": ["Changyuan Zhao", "Guangyuan Liu", "Ruichen Zhang", "Yinqiu Liu", "Jiacheng Wang", "Jiawen Kang", "Dusit Niyato", "Zan Li", "Xuemin", "Shen", "Zhu Han", "Sumei Sun", "Chau Yuen", "Dong In Kim"], "title": "Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges", "categories": ["cs.LG"], "comment": "21 pages. 9 figures", "summary": "Edge General Intelligence (EGI) represents a transformative evolution of edge\ncomputing, where distributed agents possess the capability to perceive, reason,\nand act autonomously across diverse, dynamic environments. Central to this\nvision are world models, which act as proactive internal simulators that not\nonly predict but also actively imagine future trajectories, reason under\nuncertainty, and plan multi-step actions with foresight. This proactive nature\nallows agents to anticipate potential outcomes and optimize decisions ahead of\nreal-world interactions. While prior works in robotics and gaming have\nshowcased the potential of world models, their integration into the wireless\nedge for EGI remains underexplored. This survey bridges this gap by offering a\ncomprehensive analysis of how world models can empower agentic artificial\nintelligence (AI) systems at the edge. We first examine the architectural\nfoundations of world models, including latent representation learning, dynamics\nmodeling, and imagination-based planning. Building on these core capabilities,\nwe illustrate their proactive applications across EGI scenarios such as\nvehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of\nThings (IoT) systems, and network functions virtualization, thereby\nhighlighting how they can enhance optimization under latency, energy, and\nprivacy constraints. We then explore their synergy with foundation models and\ndigital twins, positioning world models as the cognitive backbone of EGI.\nFinally, we highlight open challenges, such as safety guarantees, efficient\ntraining, and constrained deployment, and outline future research directions.\nThis survey provides both a conceptual foundation and a practical roadmap for\nrealizing the next generation of intelligent, autonomous edge systems.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u63a2\u8ba8\u4e86\u4e16\u754c\u6a21\u578b\u5728\u8fb9\u7f18\u901a\u7528\u667a\u80fd\uff08EGI\uff09\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5206\u6790\u5176\u67b6\u6784\u3001\u5728\u5404\u79cd\u8fb9\u7f18\u573a\u666f\u4e2d\u7684\u4e3b\u52a8\u5e94\u7528\u3001\u4e0e\u57fa\u7840\u6a21\u578b\u548c\u6570\u5b57\u5b6a\u751f\u4f53\u7684\u534f\u540c\uff0c\u5e76\u6307\u51fa\u672a\u6765\u6311\u6218\u548c\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5c3d\u7ba1\u4e16\u754c\u6a21\u578b\u5728\u673a\u5668\u4eba\u548c\u6e38\u620f\u9886\u57df\u8868\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u65e0\u7ebf\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u4e0e\u8fb9\u7f18\u901a\u7528\u667a\u80fd\uff08EGI\uff09\u7684\u96c6\u6210\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u672c\u6587\u901a\u8fc7\u7efc\u8ff0\u5f62\u5f0f\uff0c\u9996\u5148\u8003\u5bdf\u4e86\u4e16\u754c\u6a21\u578b\u7684\u67b6\u6784\u57fa\u7840\uff08\u5982\u6f5c\u5728\u8868\u793a\u5b66\u4e60\u3001\u52a8\u529b\u5b66\u5efa\u6a21\u3001\u57fa\u4e8e\u60f3\u8c61\u7684\u89c4\u5212\uff09\uff0c\u63a5\u7740\u9610\u8ff0\u4e86\u5b83\u4eec\u5728\u8f66\u8f7d\u7f51\u7edc\u3001\u65e0\u4eba\u673a\u7f51\u7edc\u3001\u7269\u8054\u7f51\u548c\u7f51\u7edc\u529f\u80fd\u865a\u62df\u5316\u7b49EGI\u573a\u666f\u4e2d\u7684\u4e3b\u52a8\u5e94\u7528\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u4e16\u754c\u6a21\u578b\u4e0e\u57fa\u7840\u6a21\u578b\u3001\u6570\u5b57\u5b6a\u751f\u4f53\u7684\u534f\u540c\uff0c\u5e76\u8bc6\u522b\u4e86\u5f00\u653e\u6311\u6218\u4e0e\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u4e16\u754c\u6a21\u578b\u88ab\u5b9a\u4f4d\u4e3a\u8fb9\u7f18\u901a\u7528\u667a\u80fd\uff08EGI\uff09\u7684\u8ba4\u77e5\u9aa8\u5e72\uff0c\u80fd\u591f\u901a\u8fc7\u4e3b\u52a8\u9884\u6d4b\u3001\u60f3\u8c61\u672a\u6765\u8f68\u8ff9\u548c\u591a\u6b65\u89c4\u5212\uff0c\u663e\u8457\u589e\u5f3a\u8fb9\u7f18AI\u7cfb\u7edf\u5728\u5ef6\u8fdf\u3001\u80fd\u8017\u548c\u9690\u79c1\u7ea6\u675f\u4e0b\u7684\u4f18\u5316\u80fd\u529b\u3002\u672c\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u5404\u79cdEGI\u573a\u666f\u4e2d\u63d0\u5347\u6027\u80fd\u7684\u6f5c\u529b\u3002", "conclusion": "\u672c\u7efc\u8ff0\u4e3a\u5b9e\u73b0\u4e0b\u4e00\u4ee3\u667a\u80fd\u81ea\u4e3b\u8fb9\u7f18\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\u548c\u5b9e\u7528\u8def\u7ebf\u56fe\uff0c\u660e\u786e\u4e86\u4e16\u754c\u6a21\u578b\u5728EGI\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u5728\u5b89\u5168\u6027\u3001\u9ad8\u6548\u8bad\u7ec3\u548c\u90e8\u7f72\u65b9\u9762\u9700\u514b\u670d\u7684\u6311\u6218\u3002"}}
{"id": "2508.09598", "pdf": "https://arxiv.org/pdf/2508.09598", "abs": "https://arxiv.org/abs/2508.09598", "authors": ["Jie Shao", "Ke Zhu", "Minghao Fu", "Guo-hua Wang", "Jianxin Wu"], "title": "Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality", "categories": ["cs.CV"], "comment": null, "summary": "Diffusion models have achieved remarkable progress in class-to-image\ngeneration. However, we observe that despite impressive FID scores,\nstate-of-the-art models often generate distorted or low-quality images,\nespecially in certain classes. This gap arises because FID evaluates global\ndistribution alignment, while ignoring the perceptual quality of individual\nsamples. We further examine the role of CFG, a common technique used to enhance\ngeneration quality. While effective in improving metrics and suppressing\noutliers, CFG can introduce distribution shift and visual artifacts due to its\nmisalignment with both training objectives and user expectations. In this work,\nwe propose FaME, a training-free and inference-efficient method for improving\nperceptual quality. FaME uses an image quality assessment model to identify\nlow-quality generations and stores their sampling trajectories. These failure\nmodes are then used as negative guidance to steer future sampling away from\npoor-quality regions. Experiments on ImageNet demonstrate that FaME brings\nconsistent improvements in visual quality without compromising FID. FaME also\nshows the potential to be extended to improve text-to-image generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFaME\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u4e14\u63a8\u7406\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1f\u5411\u5f15\u5bfc\u63d0\u5347\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u540c\u65f6\u4e0d\u5f71\u54cdFID\u5206\u6570\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u5728\u7c7b\u5230\u56fe\u50cf\u751f\u6210\u4e2dFID\u5f97\u5206\u5f88\u9ad8\uff0c\u4f46\u4ecd\u89c2\u5bdf\u5230\u5728\u67d0\u4e9b\u7c7b\u522b\u4e2d\u751f\u6210\u626d\u66f2\u6216\u4f4e\u8d28\u91cf\u56fe\u50cf\uff0c\u8fd9\u8868\u660eFID\u65e0\u6cd5\u8bc4\u4f30\u4e2a\u4f53\u6837\u672c\u7684\u611f\u77e5\u8d28\u91cf\u3002\u540c\u65f6\uff0c\u5e38\u7528\u7684CFG\u6280\u672f\u867d\u80fd\u63d0\u5347\u6307\u6807\uff0c\u5374\u53ef\u80fd\u5f15\u5165\u5206\u5e03\u504f\u79fb\u548c\u89c6\u89c9\u4f2a\u5f71\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u611f\u77e5\u8d28\u91cf\u3002", "method": "\u63d0\u51faFaME\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u4e14\u63a8\u7406\u9ad8\u6548\u3002\u5b83\u5229\u7528\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\u8bc6\u522b\u4f4e\u8d28\u91cf\u751f\u6210\u56fe\u50cf\uff0c\u5e76\u5b58\u50a8\u5176\u91c7\u6837\u8f68\u8ff9\u4f5c\u4e3a\u201c\u5931\u8d25\u6a21\u5f0f\u201d\u3002\u8fd9\u4e9b\u5931\u8d25\u6a21\u5f0f\u968f\u540e\u88ab\u7528\u4f5c\u8d1f\u5411\u5f15\u5bfc\uff0c\u4ee5\u4f7f\u672a\u6765\u7684\u91c7\u6837\u8fc7\u7a0b\u504f\u79bb\u4f4e\u8d28\u91cf\u533a\u57df\u3002", "result": "\u5728ImageNet\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFaME\u80fd\u6301\u7eed\u63d0\u5347\u89c6\u89c9\u8d28\u91cf\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3FID\u5206\u6570\u3002\u6b64\u5916\uff0cFaME\u8fd8\u5c55\u793a\u4e86\u6269\u5c55\u5230\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u6f5c\u529b\u3002", "conclusion": "FaME\u6210\u529f\u5730\u5728\u4e0d\u5f71\u54cdFID\u7684\u524d\u63d0\u4e0b\uff0c\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u751f\u6210\u56fe\u50cf\u7684\u611f\u77e5\u8d28\u91cf\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2508.09592", "pdf": "https://arxiv.org/pdf/2508.09592", "abs": "https://arxiv.org/abs/2508.09592", "authors": ["Licheng Liu", "Mingda Qiao"], "title": "Online Prediction with Limited Selectivity", "categories": ["cs.LG", "cs.DS"], "comment": null, "summary": "Selective prediction [Dru13, QV19] models the scenario where a forecaster\nfreely decides on the prediction window that their forecast spans. Many data\nstatistics can be predicted to a non-trivial error rate without any\ndistributional assumptions or expert advice, yet these results rely on that the\nforecaster may predict at any time. We introduce a model of Prediction with\nLimited Selectivity (PLS) where the forecaster can start the prediction only on\na subset of the time horizon. We study the optimal prediction error both on an\ninstance-by-instance basis and via an average-case analysis. We introduce a\ncomplexity measure that gives instance-dependent bounds on the optimal error.\nFor a randomly-generated PLS instance, these bounds match with high\nprobability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6709\u9650\u9009\u62e9\u6027\u9884\u6d4b\uff08PLS\uff09\u6a21\u578b\uff0c\u7814\u7a76\u9884\u6d4b\u8d77\u59cb\u65f6\u95f4\u53d7\u9650\u573a\u666f\u4e0b\u7684\u6700\u4f18\u9884\u6d4b\u8bef\u5dee\uff0c\u5f15\u5165\u590d\u6742\u5ea6\u5ea6\u91cf\uff0c\u5e76\u8bc1\u660e\u5176\u754c\u9650\u4e0e\u968f\u673a\u5b9e\u4f8b\u9ad8\u5ea6\u5339\u914d\u3002", "motivation": "\u73b0\u6709\u9009\u62e9\u6027\u9884\u6d4b\u6a21\u578b\u5047\u8bbe\u9884\u6d4b\u8005\u53ef\u4ee5\u968f\u65f6\u5f00\u59cb\u9884\u6d4b\uff0c\u4f46\u5728\u8bb8\u591a\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u9884\u6d4b\u8005\u53ea\u80fd\u5728\u65f6\u95f4\u8303\u56f4\u7684\u7279\u5b9a\u5b50\u96c6\u4e0a\u5f00\u59cb\u9884\u6d4b\uff0c\u8fd9\u4fc3\u4f7f\u672c\u7814\u7a76\u5f15\u5165\u5e76\u5206\u6790\u5177\u6709\u6709\u9650\u9009\u62e9\u6027\u7684\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u5f15\u5165\u4e86\u201c\u6709\u9650\u9009\u62e9\u6027\u9884\u6d4b\uff08PLS\uff09\u201d\u6a21\u578b\uff0c\u5176\u4e2d\u9884\u6d4b\u8005\u53ea\u80fd\u5728\u65f6\u95f4\u8303\u56f4\u7684\u5b50\u96c6\u4e0a\u5f00\u59cb\u9884\u6d4b\u3002\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u5bf9\u6700\u4f18\u9884\u6d4b\u8bef\u5dee\u8fdb\u884c\u9010\u5b9e\u4f8b\u5206\u6790\u548c\u5e73\u5747\u6848\u4f8b\u5206\u6790\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u590d\u6742\u5ea6\u5ea6\u91cf\uff0c\u4ee5\u63d0\u4f9b\u4e0e\u5b9e\u4f8b\u76f8\u5173\u7684\u6700\u4f18\u8bef\u5dee\u754c\u9650\u3002", "result": "\u5f15\u5165\u7684\u590d\u6742\u5ea6\u5ea6\u91cf\u80fd\u591f\u63d0\u4f9b\u4e0e\u5b9e\u4f8b\u76f8\u5173\u7684\u6700\u4f18\u8bef\u5dee\u754c\u9650\u3002\u5bf9\u4e8e\u968f\u673a\u751f\u6210\u7684PLSP\u6a21\u578b\u5b9e\u4f8b\uff0c\u8fd9\u4e9b\u754c\u9650\u4ee5\u9ad8\u6982\u7387\u4e0e\u5b9e\u9645\u7ed3\u679c\u5339\u914d\u3002", "conclusion": "\u8bba\u6587\u6210\u529f\u5730\u5f15\u5165\u5e76\u5206\u6790\u4e86\u6709\u9650\u9009\u62e9\u6027\u9884\u6d4b\uff08PLS\uff09\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u66f4\u8d34\u8fd1\u73b0\u5b9e\u4e2d\u9884\u6d4b\u8d77\u59cb\u65f6\u95f4\u53d7\u9650\u7684\u60c5\u51b5\u3002\u901a\u8fc7\u63d0\u51fa\u7684\u590d\u6742\u5ea6\u5ea6\u91cf\uff0c\u8bba\u6587\u80fd\u591f\u6709\u6548\u5730\u91cf\u5316\u6700\u4f18\u8bef\u5dee\uff0c\u5e76\u4e14\u5176\u754c\u9650\u5728\u968f\u673a\u5b9e\u4f8b\u4e0a\u8868\u73b0\u51fa\u9ad8\u5ea6\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.09599", "pdf": "https://arxiv.org/pdf/2508.09599", "abs": "https://arxiv.org/abs/2508.09599", "authors": ["Beomjun Kim", "Suhan Woo", "Sejong Heo", "Euntai Kim"], "title": "BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird's Eye View Map Segmentation", "categories": ["cs.CV"], "comment": "9 pages, 6 figures", "summary": "Bird's-Eye-View (BEV) map segmentation is one of the most important and\nchallenging tasks in autonomous driving. Camera-only approaches have drawn\nattention as cost-effective alternatives to LiDAR, but they still fall behind\nLiDAR-Camera (LC) fusion-based methods. Knowledge Distillation (KD) has been\nexplored to narrow this gap, but existing methods mainly enlarge the student\nmodel by mimicking the teacher's architecture, leading to higher inference\ncost. To address this issue, we introduce BridgeTA, a cost-effective\ndistillation framework to bridge the representation gap between LC fusion and\nCamera-only models through a Teacher Assistant (TA) network while keeping the\nstudent's architecture and inference cost unchanged. A lightweight TA network\ncombines the BEV representations of the teacher and student, creating a shared\nlatent space that serves as an intermediate representation. To ground the\nframework theoretically, we derive a distillation loss using Young's\nInequality, which decomposes the direct teacher-student distillation path into\nteacher-TA and TA-student dual paths, stabilizing optimization and\nstrengthening knowledge transfer. Extensive experiments on the challenging\nnuScenes dataset demonstrate the effectiveness of our method, achieving an\nimprovement of 4.2% mIoU over the Camera-only baseline, up to 45% higher than\nthe improvement of other state-of-the-art KD methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBridgeTA\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6559\u5e08\u52a9\u624b\u7f51\u7edc\u548c\u65b0\u7684\u84b8\u998f\u635f\u5931\uff0c\u5728\u4e0d\u589e\u52a0\u76f8\u673a\u7eaf\u89c6\u89c9\u6a21\u578b\u63a8\u7406\u6210\u672c\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u5176\u5728BEV\u5730\u56fe\u5206\u5272\u4e0a\u7684\u6027\u80fd\uff0c\u6709\u6548\u7f29\u5c0f\u4e0e\u591a\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u7684\u5dee\u8ddd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u76f8\u673a\u7eaf\u89c6\u89c9BEV\u5730\u56fe\u5206\u5272\u7684\u6027\u80fd\u8fdc\u4f4e\u4e8e\u6fc0\u5149\u96f7\u8fbe-\u76f8\u673a\u878d\u5408\u65b9\u6cd5\u3002\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff08KD\uff09\u867d\u65e8\u5728\u7f29\u5c0f\u6b64\u5dee\u8ddd\uff0c\u4f46\u901a\u5e38\u901a\u8fc7\u589e\u5927\u6216\u4fee\u6539\u5b66\u751f\u6a21\u578b\u6765\u6a21\u4eff\u6559\u5e08\uff0c\u5bfc\u81f4\u63a8\u7406\u6210\u672c\u589e\u52a0\uff0c\u4e0d\u7b26\u6210\u672c\u6548\u76ca\u3002", "method": "\u63d0\u51faBridgeTA\u6846\u67b6\uff0c\u901a\u8fc7\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u6559\u5e08\u52a9\u624b\uff08TA\uff09\u7f51\u7edc\u6765\u6865\u63a5\u6fc0\u5149\u96f7\u8fbe-\u76f8\u673a\u878d\u5408\uff08\u6559\u5e08\uff09\u548c\u76f8\u673a\u7eaf\u89c6\u89c9\uff08\u5b66\u751f\uff09\u6a21\u578b\u4e4b\u95f4\u7684\u8868\u793a\u9e3f\u6c9f\uff0c\u540c\u65f6\u4fdd\u6301\u5b66\u751f\u6a21\u578b\u67b6\u6784\u548c\u63a8\u7406\u6210\u672c\u4e0d\u53d8\u3002TA\u7f51\u7edc\u7ed3\u5408\u6559\u5e08\u4e0e\u5b66\u751f\u7684BEV\u8868\u793a\uff0c\u521b\u5efa\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u3002\u7406\u8bba\u4e0a\uff0c\u57fa\u4e8eYoung's Inequality\u63a8\u5bfc\u4e86\u4e00\u79cd\u65b0\u7684\u84b8\u998f\u635f\u5931\uff0c\u5c06\u76f4\u63a5\u7684\u5e08\u751f\u84b8\u998f\u8def\u5f84\u5206\u89e3\u4e3a\u6559\u5e08-TA\u548cTA-\u5b66\u751f\u53cc\u8def\u5f84\uff0c\u4ee5\u7a33\u5b9a\u4f18\u5316\u5e76\u52a0\u5f3a\u77e5\u8bc6\u4f20\u9012\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u76f8\u673a\u7eaf\u89c6\u89c9\u57fa\u7ebf\u6a21\u578b\uff0cmIoU\u6027\u80fd\u63d0\u5347\u4e864.2%\u3002\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u53ef\u8fbe45%\u3002", "conclusion": "BridgeTA\u6846\u67b6\u901a\u8fc7\u6210\u672c\u6548\u76ca\u9ad8\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u5728\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u76f8\u673a\u7eaf\u89c6\u89c9BEV\u5730\u56fe\u5206\u5272\u7684\u6027\u80fd\uff0c\u6709\u6548\u7f29\u5c0f\u4e86\u4e0e\u591a\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u7684\u5dee\u8ddd\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u3002"}}
{"id": "2508.09624", "pdf": "https://arxiv.org/pdf/2508.09624", "abs": "https://arxiv.org/abs/2508.09624", "authors": ["Yan Yu", "Yaodong Yang", "Zhengbo Lu", "Chengdong Ma", "Wengang Zhou", "Houqiang Li"], "title": "Goal Discovery with Causal Capacity for Efficient Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Causal inference is crucial for humans to explore the world, which can be\nmodeled to enable an agent to efficiently explore the environment in\nreinforcement learning. Existing research indicates that establishing the\ncausality between action and state transition will enhance an agent to reason\nhow a policy affects its future trajectory, thereby promoting directed\nexploration. However, it is challenging to measure the causality due to its\nintractability in the vast state-action space of complex scenarios. In this\npaper, we propose a novel Goal Discovery with Causal Capacity (GDCC) framework\nfor efficient environment exploration. Specifically, we first derive a\nmeasurement of causality in state space, \\emph{i.e.,} causal capacity, which\nrepresents the highest influence of an agent's behavior on future trajectories.\nAfter that, we present a Monte Carlo based method to identify critical points\nin discrete state space and further optimize this method for continuous\nhigh-dimensional environments. Those critical points are used to uncover where\nthe agent makes important decisions in the environment, which are then regarded\nas our subgoals to guide the agent to make exploration more purposefully and\nefficiently. Empirical results from multi-objective tasks demonstrate that\nstates with high causal capacity align with our expected subgoals, and our GDCC\nachieves significant success rate improvements compared to baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGDCC\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u91cf\u201c\u56e0\u679c\u80fd\u529b\u201d\u6765\u8bc6\u522b\u5173\u952e\u72b6\u6001\u4f5c\u4e3a\u5b50\u76ee\u6807\uff0c\u6709\u6548\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u590d\u6742\u73af\u5883\u4e0b\u63a2\u7d22\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u56e0\u679c\u63a8\u65ad\u6709\u52a9\u4e8e\u667a\u80fd\u4f53\u8fdb\u884c\u5b9a\u5411\u63a2\u7d22\uff0c\u4f46\u7531\u4e8e\u590d\u6742\u573a\u666f\u4e0b\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u5de8\u5927\uff0c\u96be\u4ee5\u6709\u6548\u8861\u91cf\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u63d0\u51faGDCC\u6846\u67b6\u3002\u9996\u5148\uff0c\u5b9a\u4e49\u201c\u56e0\u679c\u80fd\u529b\u201d\u4f5c\u4e3a\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u5ea6\u91cf\uff0c\u8868\u793a\u667a\u80fd\u4f53\u884c\u4e3a\u5bf9\u672a\u6765\u8f68\u8ff9\u7684\u6700\u9ad8\u5f71\u54cd\u3002\u5176\u6b21\uff0c\u91c7\u7528\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u7684\u65b9\u6cd5\uff08\u9488\u5bf9\u8fde\u7eed\u9ad8\u7ef4\u73af\u5883\u8fdb\u884c\u4f18\u5316\uff09\u8bc6\u522b\u5177\u6709\u9ad8\u56e0\u679c\u80fd\u529b\u7684\u5173\u952e\u70b9\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u5b50\u76ee\u6807\u6307\u5bfc\u63a2\u7d22\u3002", "result": "\u7ecf\u9a8c\u8bc1\uff0c\u9ad8\u56e0\u679c\u80fd\u529b\u72b6\u6001\u4e0e\u9884\u671f\u5b50\u76ee\u6807\u4e00\u81f4\u3002\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cGDCC\u5728\u591a\u76ee\u6807\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u3002", "conclusion": "GDCC\u901a\u8fc7\u56e0\u679c\u80fd\u529b\u5f15\u5bfc\u5b50\u76ee\u6807\u53d1\u73b0\uff0c\u4e3a\u9ad8\u6548\u63a2\u7d22\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2508.09616", "pdf": "https://arxiv.org/pdf/2508.09616", "abs": "https://arxiv.org/abs/2508.09616", "authors": ["Daniel Barco", "Marc Stadelmann", "Martin Oswald", "Ivo Herzig", "Lukas Lichtensteiger", "Pascal Paysan", "Igor Peterlik", "Michal Walczak", "Bjoern Menze", "Frank-Peter Schilling"], "title": "MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first\n3D conditional diffusion-based model for real-world sparse-view Cone Beam\nComputed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation\nexposure. A key contribution is extending the \"InDI\" concept from 2D to a full\n3D volumetric approach for medical images, implementing an iterative denoising\nprocess that refines the CBCT volume directly from sparse-view input. A further\ncontribution is the generation of a large pseudo-CBCT dataset (16,182) from\nchest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We\nperformed a comprehensive evaluation, including quantitative metrics,\nscalability analysis, generalisation tests, and a clinical assessment by 11\nclinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10)\ndB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE\npseudo-CBCT (independent real-world) test set and enabling an 8x reduction in\nimaging radiation exposure. We demonstrate its scalability by showing that\nperformance improves with more training data. Importantly, MInDI-3D matches the\nperformance of a 3D U-Net on real-world scans from 16 cancer patients across\ndistortion and task-based metrics. It also generalises to new CBCT scanner\ngeometries. Clinicians rated our model as sufficient for patient positioning\nacross all anatomical sites and found it preserved lung tumour boundaries well.", "AI": {"tldr": "MInDI-3D\u662f\u9996\u4e2a\u7528\u4e8e\u7a00\u758f\u89c6\u56fe\u9525\u675fCT\uff08CBCT\uff09\u4f2a\u5f71\u53bb\u9664\u76843D\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u8f90\u5c04\u5242\u91cf\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u3001\u53ef\u6269\u5c55\u6027\u3001\u6cdb\u5316\u80fd\u529b\u53ca\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "motivation": "\u51cf\u5c11\u533b\u5b66\u6210\u50cf\uff08\u7279\u522b\u662fCBCT\uff09\u7684\u8f90\u5c04\u66b4\u9732\uff0c\u5e76\u89e3\u51b3\u7a00\u758f\u89c6\u56feCBCT\u56fe\u50cf\u4e2d\u7684\u4f2a\u5f71\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86MInDI-3D\u6a21\u578b\uff0c\u5c062D\u7684\u201cInDI\u201d\u6982\u5ff5\u6269\u5c55\u52303D\uff0c\u5b9e\u73b0\u5bf9CBCT\u4f53\u79ef\u7684\u76f4\u63a5\u8fed\u4ee3\u53bb\u566a\u3002\u4e3a\u6b64\uff0c\u4eceCT-RATE\u6570\u636e\u96c6\u7684\u80f8\u90e8CT\u4f53\u79ef\u751f\u6210\u4e86\u4e00\u4e2a\u5927\u578b\u4f2aCBCT\u6570\u636e\u96c6\uff0816,182\u4e2a\u6837\u672c\uff09\u8fdb\u884c\u8bad\u7ec3\u3002\u901a\u8fc7\u5b9a\u91cf\u6307\u6807\u3001\u53ef\u6269\u5c55\u6027\u5206\u6790\u3001\u6cdb\u5316\u6d4b\u8bd5\u548c11\u4f4d\u4e34\u5e8a\u533b\u751f\u7684\u8bc4\u4f30\u8fdb\u884c\u4e86\u5168\u9762\u9a8c\u8bc1\u3002", "result": "MInDI-3D\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u5728CT-RATE\u4f2aCBCT\uff08\u72ec\u7acb\u771f\u5b9e\u4e16\u754c\uff09\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u4ec5\u752850\u4e2a\u6295\u5f71\u5373\u53ef\u4f7fPSNR\u589e\u76ca12.96 (6.10) dB\uff0c\u5e76\u5b9e\u73b0\u6210\u50cf\u8f90\u5c04\u66b4\u9732\u51cf\u5c118\u500d\u3002\u6027\u80fd\u968f\u8bad\u7ec3\u6570\u636e\u589e\u52a0\u800c\u63d0\u5347\uff0c\u4e0e3D U-Net\u572816\u540d\u764c\u75c7\u60a3\u8005\u7684\u771f\u5b9e\u626b\u63cf\u4e0a\u8868\u73b0\u76f8\u5f53\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u65b0\u7684CBCT\u626b\u63cf\u4eea\u51e0\u4f55\u7ed3\u6784\u3002\u4e34\u5e8a\u533b\u751f\u8bc4\u4f30\u8ba4\u4e3a\u8be5\u6a21\u578b\u8db3\u4ee5\u7528\u4e8e\u60a3\u8005\u5b9a\u4f4d\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u4fdd\u7559\u80ba\u90e8\u80bf\u7624\u8fb9\u754c\u3002", "conclusion": "MInDI-3D\u662f\u4e00\u4e2a\u6709\u6548\u3001\u53ef\u6269\u5c55\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u76843D\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11CBCT\u6210\u50cf\u7684\u8f90\u5c04\u66b4\u9732\uff0c\u540c\u65f6\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\uff0c\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.09627", "pdf": "https://arxiv.org/pdf/2508.09627", "abs": "https://arxiv.org/abs/2508.09627", "authors": ["Subhankar Sarkar", "Souvik Chakraborty"], "title": "Physics- and geometry-aware spatio-spectral graph neural operator for time-independent and time-dependent PDEs", "categories": ["cs.LG"], "comment": null, "summary": "Solving partial differential equations (PDEs) efficiently and accurately\nremains a cornerstone challenge in science and engineering, especially for\nproblems involving complex geometries and limited labeled data. We introduce a\nPhysics- and Geometry- Aware Spatio-Spectral Graph Neural Operator\n($\\pi$G-Sp$^2$GNO) for learning the solution operators of time-independent and\ntime-dependent PDEs. The proposed approach first improves upon the recently\ndeveloped Sp$^2$GNO by enabling geometry awareness and subsequently exploits\nthe governing physics to learn the underlying solution operator in a\nsimulation-free setup. While the spatio-spectral structure present in the\nproposed architecture allows multiscale learning, two separate strategies for\nenabling geometry awareness is introduced in this paper. For time dependent\nproblems, we also introduce a novel hybrid physics informed loss function that\ncombines higher-order time-marching scheme with upscaled theory inspired\nstochastic projection scheme. This allows accurate integration of the\nphysics-information into the loss function. The performance of the proposed\napproach is illustrated on number of benchmark examples involving regular and\ncomplex domains, variation in geometry during inference, and time-independent\nand time-dependent problems. The results obtained illustrate the efficacy of\nthe proposed approach as compared to the state-of-the-art physics-informed\nneural operator algorithms in the literature.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.09625", "pdf": "https://arxiv.org/pdf/2508.09625", "abs": "https://arxiv.org/abs/2508.09625", "authors": ["Daoxin Zhong", "Jun Li", "Meng Yee Michael Chuah"], "title": "Plane Detection and Ranking via Model Information Optimization", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted as contributed paper in the 2025 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)", "summary": "Plane detection from depth images is a crucial subtask with broad robotic\napplications, often accomplished by iterative methods such as Random Sample\nConsensus (RANSAC). While RANSAC is a robust strategy with strong probabilistic\nguarantees, the ambiguity of its inlier threshold criterion makes it\nsusceptible to false positive plane detections. This issue is particularly\nprevalent in complex real-world scenes, where the true number of planes is\nunknown and multiple planes coexist. In this paper, we aim to address this\nlimitation by proposing a generalised framework for plane detection based on\nmodel information optimization. Building on previous works, we treat the\nobserved depth readings as discrete random variables, with their probability\ndistributions constrained by the ground truth planes. Various models containing\ndifferent candidate plane constraints are then generated through repeated\nrandom sub-sampling to explain our observations. By incorporating the physics\nand noise model of the depth sensor, we can calculate the information for each\nmodel, and the model with the least information is accepted as the most likely\nground truth. This information optimization process serves as an objective\nmechanism for determining the true number of planes and preventing false\npositive detections. Additionally, the quality of each detected plane can be\nranked by summing the information reduction of inlier points for each plane. We\nvalidate these properties through experiments with synthetic data and find that\nour algorithm estimates plane parameters more accurately compared to the\ndefault Open3D RANSAC plane segmentation. Furthermore, we accelerate our\nalgorithm by partitioning the depth map using neural network segmentation,\nwhich enhances its ability to generate more realistic plane parameters in\nreal-world data.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u4fe1\u606f\u4f18\u5316\u7684\u901a\u7528\u5e73\u9762\u68c0\u6d4b\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRANSAC\u5728\u6df1\u5ea6\u56fe\u50cf\u5e73\u9762\u68c0\u6d4b\u4e2d\u6613\u4ea7\u751f\u8bef\u62a5\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u5e73\u9762\u53c2\u6570\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u6df1\u5ea6\u56fe\u50cf\u4e2d\u7684\u5e73\u9762\u68c0\u6d4b\u5bf9\u673a\u5668\u4eba\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5e7f\u6cdb\u4f7f\u7528\u7684RANSAC\u65b9\u6cd5\u7531\u4e8e\u5185\u70b9\u9608\u503c\u6a21\u7cca\uff0c\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\uff08\u5e73\u9762\u6570\u91cf\u672a\u77e5\u4e14\u591a\u5e73\u9762\u5171\u5b58\uff09\u4e2d\u5bb9\u6613\u5bfc\u81f4\u5e73\u9762\u8bef\u62a5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u4fe1\u606f\u4f18\u5316\u7684\u5e73\u9762\u68c0\u6d4b\u6846\u67b6\u3002\u5c06\u6df1\u5ea6\u8bfb\u6570\u89c6\u4e3a\u53d7\u771f\u5b9e\u5e73\u9762\u7ea6\u675f\u7684\u79bb\u6563\u968f\u673a\u53d8\u91cf\uff0c\u901a\u8fc7\u91cd\u590d\u968f\u673a\u4e8c\u6b21\u91c7\u6837\u751f\u6210\u5305\u542b\u4e0d\u540c\u5019\u9009\u5e73\u9762\u7ea6\u675f\u7684\u6a21\u578b\u3002\u7ed3\u5408\u6df1\u5ea6\u4f20\u611f\u5668\u7269\u7406\u548c\u566a\u58f0\u6a21\u578b\u8ba1\u7b97\u6bcf\u4e2a\u6a21\u578b\u7684\u4fe1\u606f\u91cf\uff0c\u9009\u62e9\u4fe1\u606f\u91cf\u6700\u5c11\u7684\u6a21\u578b\u4f5c\u4e3a\u6700\u53ef\u80fd\u7684\u771f\u5b9e\u5e73\u9762\u3002\u6b64\u8fc7\u7a0b\u7528\u4e8e\u5ba2\u89c2\u786e\u5b9a\u5e73\u9762\u6570\u91cf\u5e76\u9632\u6b62\u8bef\u62a5\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u8ba1\u7b97\u5185\u70b9\u4fe1\u606f\u51cf\u5c11\u91cf\u4e4b\u548c\u6765\u8bc4\u4f30\u5e73\u9762\u8d28\u91cf\u3002\u4e3a\u52a0\u901f\u7b97\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u5206\u5272\u5bf9\u6df1\u5ea6\u56fe\u8fdb\u884c\u5206\u533a\u3002", "result": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u7b97\u6cd5\u5728\u5e73\u9762\u53c2\u6570\u4f30\u8ba1\u65b9\u9762\u6bd4Open3D\u9ed8\u8ba4\u7684RANSAC\u5e73\u9762\u5206\u5272\u66f4\u51c6\u786e\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u5206\u5272\u52a0\u901f\u4e86\u7b97\u6cd5\uff0c\u5e76\u589e\u5f3a\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u751f\u6210\u66f4\u771f\u5b9e\u5e73\u9762\u53c2\u6570\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u4fe1\u606f\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3RANSAC\u8bef\u62a5\u95ee\u9898\u7684\u5e73\u9762\u68c0\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5e73\u9762\u53c2\u6570\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u5e76\u80fd\u5ba2\u89c2\u786e\u5b9a\u5e73\u9762\u6570\u91cf\u548c\u8bc4\u4f30\u5e73\u9762\u8d28\u91cf\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.09225", "pdf": "https://arxiv.org/pdf/2508.09225", "abs": "https://arxiv.org/abs/2508.09225", "authors": ["Nak-Jun Sung", "Donghyun Lee", "Bo Hwa Choi", "Chae Jung Park"], "title": "AMRG: Extend Vision Language Models for Automatic Mammography Report Generation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Mammography report generation is a critical yet underexplored task in medical\nAI, characterized by challenges such as multiview image reasoning,\nhigh-resolution visual cues, and unstructured radiologic language. In this\nwork, we introduce AMRG (Automatic Mammography Report Generation), the first\nend-to-end framework for generating narrative mammography reports using large\nvision-language models (VLMs). Building upon MedGemma-4B-it-a\ndomain-specialized, instruction-tuned VLM-we employ a parameter-efficient\nfine-tuning (PEFT) strategy via Low-Rank Adaptation (LoRA), enabling\nlightweight adaptation with minimal computational overhead. We train and\nevaluate AMRG on DMID, a publicly available dataset of paired high-resolution\nmammograms and diagnostic reports. This work establishes the first reproducible\nbenchmark for mammography report generation, addressing a longstanding gap in\nmultimodal clinical AI. We systematically explore LoRA hyperparameter\nconfigurations and conduct comparative experiments across multiple VLM\nbackbones, including both domain-specific and general-purpose models under a\nunified tuning protocol. Our framework demonstrates strong performance across\nboth language generation and clinical metrics, achieving a ROUGE-L score of\n0.5691, METEOR of 0.6152, CIDEr of 0.5818, and BI-RADS accuracy of 0.5582.\nQualitative analysis further highlights improved diagnostic consistency and\nreduced hallucinations. AMRG offers a scalable and adaptable foundation for\nradiology report generation and paves the way for future research in multimodal\nmedical AI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86AMRG\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u4e73\u817aX\u7ebf\u62a5\u544a\uff0c\u901a\u8fc7LoRA\u7b56\u7565\u5728DMID\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\uff0c\u5e76\u5efa\u7acb\u4e86\u8be5\u9886\u57df\u7684\u9996\u4e2a\u53ef\u590d\u73b0\u57fa\u51c6\u3002", "motivation": "\u4e73\u817aX\u7ebf\u62a5\u544a\u751f\u6210\u662f\u533b\u7597AI\u4e2d\u4e00\u4e2a\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u4efb\u52a1\uff0c\u9762\u4e34\u591a\u89c6\u56fe\u56fe\u50cf\u63a8\u7406\u3001\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u7ebf\u7d22\u548c\u975e\u7ed3\u6784\u5316\u653e\u5c04\u5b66\u8bed\u8a00\u7b49\u6311\u6218\u3002", "method": "\u5f15\u5165AMRG\uff08Automatic Mammography Report Generation\uff09\uff0c\u9996\u4e2a\u4f7f\u7528\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u751f\u6210\u53d9\u8ff0\u6027\u4e73\u817aX\u7ebf\u62a5\u544a\u7684\u7aef\u5230\u7aef\u6846\u67b6\u3002\u8be5\u6846\u67b6\u57fa\u4e8eMedGemma-4B-it\uff08\u4e00\u4e2a\u9886\u57df\u4e13\u7528\u3001\u6307\u4ee4\u8c03\u6574\u7684VLM\uff09\uff0c\u5e76\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u7b56\u7565\u3002\u5728\u516c\u5f00\u6570\u636e\u96c6DMID\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5e76\u7cfb\u7edf\u63a2\u7d22LoRA\u8d85\u53c2\u6570\u914d\u7f6e\uff0c\u5728\u7edf\u4e00\u8c03\u4f18\u534f\u8bae\u4e0b\u5bf9\u591a\u79cdVLM\u9aa8\u5e72\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u5b9e\u9a8c\u3002", "result": "AMRG\u5728\u8bed\u8a00\u751f\u6210\u548c\u4e34\u5e8a\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u5f3a\u5927\u6027\u80fd\uff0cROUGE-L\u8fbe\u52300.5691\uff0cMETEOR 0.6152\uff0cCIDEr 0.5818\uff0cBI-RADS\u51c6\u786e\u73870.5582\u3002\u5b9a\u6027\u5206\u6790\u663e\u793a\u8bca\u65ad\u4e00\u81f4\u6027\u63d0\u9ad8\uff0c\u5e7b\u89c9\u51cf\u5c11\u3002\u672c\u5de5\u4f5c\u5efa\u7acb\u4e86\u4e73\u817aX\u7ebf\u62a5\u544a\u751f\u6210\u7684\u9996\u4e2a\u53ef\u590d\u73b0\u57fa\u51c6\u3002", "conclusion": "AMRG\u4e3a\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u548c\u9002\u5e94\u6027\u5f3a\u7684\u57fa\u7840\uff0c\u4e3a\u591a\u6a21\u6001\u533b\u7597AI\u7684\u672a\u6765\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.09630", "pdf": "https://arxiv.org/pdf/2508.09630", "abs": "https://arxiv.org/abs/2508.09630", "authors": ["Yifei Sun", "Junming Liu", "Ding Wang", "Yirong Chen", "Xuefeng Yan"], "title": "TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multivariate time series data typically comprises two distinct modalities:\nvariable semantics and sampled numerical observations. Traditional time series\nmodels treat variables as anonymous statistical signals, overlooking the rich\nsemantic information embedded in variable names and data descriptions. However,\nthese textual descriptors often encode critical domain knowledge that is\nessential for robust and interpretable modeling. Here we present TimeMKG, a\nmultimodal causal reasoning framework that elevates time series modeling from\nlow-level signal processing to knowledge informed inference. TimeMKG employs\nlarge language models to interpret variable semantics and constructs structured\nMultivariate Knowledge Graphs that capture inter-variable relationships. A\ndual-modality encoder separately models the semantic prompts, generated from\nknowledge graph triplets, and the statistical patterns from historical time\nseries. Cross-modality attention aligns and fuses these representations at the\nvariable level, injecting causal priors into downstream tasks such as\nforecasting and classification, providing explicit and interpretable priors to\nguide model reasoning. The experiment in diverse datasets demonstrates that\nincorporating variable-level knowledge significantly improves both predictive\nperformance and generalization.", "AI": {"tldr": "TimeMKG\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u56e0\u679c\u63a8\u7406\u6846\u67b6\uff0c\u5b83\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u6790\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u7684\u53d8\u91cf\u8bed\u4e49\u4fe1\u606f\uff0c\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u5e76\u4e0e\u6570\u503c\u6570\u636e\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u5ffd\u89c6\u4e86\u53d8\u91cf\u540d\u79f0\u548c\u6570\u636e\u63cf\u8ff0\u4e2d\u8574\u542b\u7684\u4e30\u5bcc\u8bed\u4e49\u4fe1\u606f\uff08\u5373\u5173\u952e\u9886\u57df\u77e5\u8bc6\uff09\uff0c\u800c\u8fd9\u4e9b\u4fe1\u606f\u5bf9\u4e8e\u6784\u5efa\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002", "method": "TimeMKG\u6846\u67b6\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u91ca\u53d8\u91cf\u8bed\u4e49\u5e76\u6784\u5efa\u591a\u5143\u77e5\u8bc6\u56fe\u8c31\uff1b\u91c7\u7528\u53cc\u6a21\u6001\u7f16\u7801\u5668\u5206\u522b\u5efa\u6a21\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u7684\u8bed\u4e49\u63d0\u793a\u548c\u5386\u53f2\u65f6\u95f4\u5e8f\u5217\u7684\u7edf\u8ba1\u6a21\u5f0f\uff1b\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u5728\u53d8\u91cf\u5c42\u9762\u878d\u5408\u8fd9\u4e9b\u8868\u793a\uff0c\u5c06\u56e0\u679c\u5148\u9a8c\u6ce8\u5165\u5230\u4e0b\u6e38\u7684\u9884\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u53d8\u91cf\u5c42\u9762\u7684\u77e5\u8bc6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5c06\u53d8\u91cf\u5c42\u9762\u7684\u77e5\u8bc6\u878d\u5165\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\uff0cTimeMKG\u6846\u67b6\u6210\u529f\u5730\u5c06\u4f20\u7edf\u4fe1\u53f7\u5904\u7406\u63d0\u5347\u4e3a\u77e5\u8bc6\u9a71\u52a8\u7684\u63a8\u7406\uff0c\u4e0d\u4ec5\u663e\u8457\u6539\u5584\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u8fd8\u63d0\u4f9b\u4e86\u660e\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u56e0\u679c\u5148\u9a8c\u6765\u6307\u5bfc\u6a21\u578b\u63a8\u7406\u3002"}}
{"id": "2508.09626", "pdf": "https://arxiv.org/pdf/2508.09626", "abs": "https://arxiv.org/abs/2508.09626", "authors": ["Xu Tang", "Junan Jia", "Yijing Wang", "Jingjing Ma", "Xiangrong Zhang"], "title": "Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation", "categories": ["cs.CV"], "comment": "9 pages, 4 figures, AAAI 2026", "summary": "In the task of 3D Aerial-view Scene Semantic Segmentation (3D-AVS-SS),\ntraditional methods struggle to address semantic ambiguity caused by scale\nvariations and structural occlusions in aerial images. This limits their\nsegmentation accuracy and consistency. To tackle these challenges, we propose a\nnovel 3D-AVS-SS approach named SAD-Splat. Our method introduces a Gaussian\npoint drop module, which integrates semantic confidence estimation with a\nlearnable sparsity mechanism based on the Hard Concrete distribution. This\nmodule effectively eliminates redundant and semantically ambiguous Gaussian\npoints, enhancing both segmentation performance and representation compactness.\nFurthermore, SAD-Splat incorporates a high-confidence pseudo-label generation\npipeline. It leverages 2D foundation models to enhance supervision when\nground-truth labels are limited, thereby further improving segmentation\naccuracy. To advance research in this domain, we introduce a challenging\nbenchmark dataset: 3D Aerial Semantic (3D-AS), which encompasses diverse\nreal-world aerial scenes with sparse annotations. Experimental results\ndemonstrate that SAD-Splat achieves an excellent balance between segmentation\naccuracy and representation compactness. It offers an efficient and scalable\nsolution for 3D aerial scene understanding.", "AI": {"tldr": "\u9488\u5bf9\u822a\u7a7a\u5f71\u50cf\u4e2d3D\u4fef\u89c6\u573a\u666f\u8bed\u4e49\u5206\u5272\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51faSAD-Splat\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u70b9\u5220\u9664\u6a21\u5757\u548c\u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u751f\u6210\u7ba1\u7ebf\uff0c\u6709\u6548\u89e3\u51b3\u8bed\u4e49\u6a21\u7cca\u548c\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u76843D-AS\u6570\u636e\u96c6\u3002\u8be5\u65b9\u6cd5\u5728\u5206\u5272\u7cbe\u5ea6\u548c\u8868\u793a\u7d27\u51d1\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u4f20\u7edf3D\u4fef\u89c6\u573a\u666f\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u822a\u7a7a\u5f71\u50cf\u4e2d\u56e0\u5c3a\u5ea6\u53d8\u5316\u548c\u7ed3\u6784\u906e\u6321\u5bfc\u81f4\u7684\u8bed\u4e49\u6a21\u7cca\u95ee\u9898\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5206\u5272\u7cbe\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "method": "\u672c\u6587\u63d0\u51faSAD-Splat\u65b9\u6cd5\uff0c\u4e3b\u8981\u5305\u542b\uff1a1. \u9ad8\u65af\u70b9\u5220\u9664\u6a21\u5757\uff1a\u7ed3\u5408\u8bed\u4e49\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u548c\u57fa\u4e8eHard Concrete\u5206\u5e03\u7684\u53ef\u5b66\u4e60\u7a00\u758f\u673a\u5236\uff0c\u7528\u4e8e\u6d88\u9664\u5197\u4f59\u548c\u8bed\u4e49\u6a21\u7cca\u7684\u9ad8\u65af\u70b9\u30022. \u9ad8\u7f6e\u4fe1\u5ea6\u4f2a\u6807\u7b7e\u751f\u6210\u7ba1\u7ebf\uff1a\u5229\u75282D\u57fa\u7840\u6a21\u578b\u5728\u771f\u5b9e\u6807\u7b7e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a\u76d1\u7763\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u6311\u6218\u6027\u7684\u57fa\u51c6\u6570\u636e\u96c63D Aerial Semantic (3D-AS)\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u822a\u7a7a\u573a\u666f\u548c\u7a00\u758f\u6807\u6ce8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSAD-Splat\u5728\u5206\u5272\u7cbe\u5ea6\u548c\u8868\u793a\u7d27\u51d1\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u5e73\u8861\u3002", "conclusion": "SAD-Splat\u4e3a3D\u822a\u7a7a\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e14\u5f15\u5165\u76843D-AS\u6570\u636e\u96c6\u4fc3\u8fdb\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2508.09659", "pdf": "https://arxiv.org/pdf/2508.09659", "abs": "https://arxiv.org/abs/2508.09659", "authors": ["Johannes F. Hevler", "Shivam Verma", "Mirat Soijtra", "Carolyn R. Bertozzi"], "title": "Thermal Tracks: A Gaussian process-based framework for universal melting curve analysis enabling unconstrained hit identification in thermal proteome profiling experiments", "categories": ["cs.LG", "q-bio.QM"], "comment": "5 pages, 2 figures, short communication", "summary": "Thermal Tracks is a Python-based statistical framework for analyzing protein\nthermal stability data that overcomes key limitations of existing thermal\nproteome profiling (TPP) work-flows. Unlike standard approaches that assume\nsigmoidal melting curves and are constrained by empirical null distributions\n(limiting significant hits to approximately 5 % of data), Thermal Tracks uses\nGaussian Process (GP) models with squared-exponential kernels to flexibly model\nany melting curve shape while generating unbiased null distributions through\nkernel priors. This framework is particularly valuable for analyzing\nproteome-wide perturbations that significantly alter protein thermal stability,\nsuch as pathway inhibitions, genetic modifications, or environmental stresses,\nwhere conventional TPP methods may miss biologically relevant changes due to\ntheir statistical constraints. Furthermore, Thermal Tracks excels at analyzing\nproteins with un-conventional melting profiles, including phase-separating\nproteins and membrane proteins, which often exhibit complex, non-sigmoidal\nthermal stability behaviors. Thermal Tracks is freely available from GitHub and\nis implemented in Python, providing an accessible and flexible tool for\nproteome-wide thermal profiling studies.", "AI": {"tldr": "Thermal Tracks\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u7edf\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\u514b\u670d\u4e86\u73b0\u6709\u70ed\u86cb\u767d\u8d28\u7ec4\u5206\u6790\uff08TPP\uff09\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u66f4\u7075\u6d3b\u3001\u51c6\u786e\u5730\u5206\u6790\u86cb\u767d\u8d28\u70ed\u7a33\u5b9a\u6027\u6570\u636e\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u6216\u975e\u5e38\u89c4\u7684\u7194\u89e3\u66f2\u7ebf\u3002", "motivation": "\u73b0\u6709\u70ed\u86cb\u767d\u8d28\u7ec4\u5206\u6790\uff08TPP\uff09\u5de5\u4f5c\u6d41\u7a0b\u5b58\u5728\u5173\u952e\u5c40\u9650\u6027\uff1a\u5b83\u4eec\u901a\u5e38\u5047\u8bbeS\u578b\u7194\u89e3\u66f2\u7ebf\uff0c\u5e76\u53d7\u9650\u4e8e\u7ecf\u9a8c\u96f6\u5206\u5e03\uff0c\u5bfc\u81f4\u4ec5\u80fd\u8bc6\u522b\u7ea65%\u7684\u6570\u636e\u4e3a\u663e\u8457\u53d8\u5316\u3002\u8fd9\u4f7f\u5f97\u4f20\u7edf\u65b9\u6cd5\u5728\u5206\u6790\u86cb\u767d\u8d28\u7ec4\u8303\u56f4\u5185\u7684\u6270\u52a8\uff08\u5982\u901a\u8def\u6291\u5236\u3001\u57fa\u56e0\u4fee\u9970\u6216\u73af\u5883\u538b\u529b\uff09\u65f6\uff0c\u53ef\u80fd\u9057\u6f0f\u91cd\u8981\u7684\u751f\u7269\u5b66\u76f8\u5173\u53d8\u5316\uff0c\u5e76\u4e14\u96be\u4ee5\u5904\u7406\u5177\u6709\u975e\u5e38\u89c4\u7194\u89e3\u66f2\u7ebf\u7684\u86cb\u767d\u8d28\u3002", "method": "Thermal Tracks\u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u6a21\u578b\u5e76\u7ed3\u5408\u5e73\u65b9\u6307\u6570\u6838\u51fd\u6570\uff0c\u80fd\u591f\u7075\u6d3b\u5efa\u6a21\u4efb\u4f55\u5f62\u72b6\u7684\u7194\u89e3\u66f2\u7ebf\u3002\u540c\u65f6\uff0c\u5b83\u901a\u8fc7\u6838\u5148\u9a8c\u751f\u6210\u65e0\u504f\u7684\u96f6\u5206\u5e03\uff0c\u4ece\u800c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7ecf\u9a8c\u96f6\u5206\u5e03\u7684\u7edf\u8ba1\u9650\u5236\u3002", "result": "\u8be5\u6846\u67b6\u5728\u5206\u6790\u5168\u86cb\u767d\u8d28\u7ec4\u8303\u56f4\u5185\u7531\u6270\u52a8\u5f15\u8d77\u7684\u86cb\u767d\u8d28\u70ed\u7a33\u5b9a\u6027\u663e\u8457\u53d8\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6355\u83b7\u4f20\u7edfTPP\u65b9\u6cd5\u53ef\u80fd\u9057\u6f0f\u7684\u751f\u7269\u5b66\u76f8\u5173\u53d8\u5316\u3002\u6b64\u5916\uff0cThermal Tracks\u7279\u522b\u64c5\u957f\u5206\u6790\u5177\u6709\u590d\u6742\u3001\u975eS\u578b\u7194\u89e3\u66f2\u7ebf\u7684\u86cb\u767d\u8d28\uff0c\u4f8b\u5982\u76f8\u5206\u79bb\u86cb\u767d\u548c\u819c\u86cb\u767d\u3002", "conclusion": "Thermal Tracks\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8bbf\u95ee\u4e14\u7075\u6d3b\u7684Python\u5de5\u5177\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7edf\u8ba1\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u86cb\u767d\u8d28\u70ed\u7a33\u5b9a\u6027\u6570\u636e\u5206\u6790\u7684\u80fd\u529b\uff0c\u5c24\u5176\u5728\u5904\u7406\u5168\u86cb\u767d\u8d28\u7ec4\u6270\u52a8\u548c\u975e\u5e38\u89c4\u7194\u89e3\u66f2\u7ebf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u86cb\u767d\u8d28\u70ed\u8c31\u5206\u6790\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u652f\u6301\u3002"}}
{"id": "2508.09629", "pdf": "https://arxiv.org/pdf/2508.09629", "abs": "https://arxiv.org/abs/2508.09629", "authors": ["Giorgos Karvounas", "Nikolaos Kyriazis", "Iason Oikonomidis", "Georgios Pavlakos", "Antonis A. Argyros"], "title": "Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors", "categories": ["cs.CV"], "comment": null, "summary": "We revisit the role of texture in monocular 3D hand reconstruction, not as an\nafterthought for photorealism, but as a dense, spatially grounded cue that can\nactively support pose and shape estimation. Our observation is simple: even in\nhigh-performing models, the overlay between predicted hand geometry and image\nappearance is often imperfect, suggesting that texture alignment may be an\nunderused supervisory signal. We propose a lightweight texture module that\nembeds per-pixel observations into UV texture space and enables a novel dense\nalignment loss between predicted and observed hand appearances. Our approach\nassumes access to a differentiable rendering pipeline and a model that maps\nimages to 3D hand meshes with known topology, allowing us to back-project a\ntextured hand onto the image and perform pixel-based alignment. The module is\nself-contained and easily pluggable into existing reconstruction pipelines. To\nisolate and highlight the value of texture-guided supervision, we augment\nHaMeR, a high-performing yet unadorned transformer architecture for 3D hand\npose estimation. The resulting system improves both accuracy and realism,\ndemonstrating the value of appearance-guided alignment in hand reconstruction.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u5229\u7528\u7eb9\u7406\u4f5c\u4e3a\u5bc6\u96c6\u76d1\u7763\u4fe1\u53f7\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7eb9\u7406\u6a21\u5757\u548c\u65b0\u9896\u7684\u7eb9\u7406\u5bf9\u9f50\u635f\u5931\uff0c\u6539\u8fdb\u5355\u76ee3D\u624b\u90e8\u91cd\u5efa\u7684\u59ff\u6001\u548c\u5f62\u72b6\u4f30\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u91cd\u5efa\u7cbe\u5ea6\u548c\u771f\u5b9e\u611f\u3002", "motivation": "\u5f53\u524d\u9ad8\u6027\u80fd3D\u624b\u90e8\u91cd\u5efa\u6a21\u578b\u5728\u9884\u6d4b\u51e0\u4f55\u4f53\u4e0e\u56fe\u50cf\u5916\u89c2\u7684\u5bf9\u9f50\u4e0a\u5e38\u6709\u7f3a\u9677\uff0c\u8868\u660e\u7eb9\u7406\u5bf9\u9f50\u8fd9\u4e00\u76d1\u7763\u4fe1\u53f7\u88ab\u4f4e\u4f30\u3002\u7814\u7a76\u65e8\u5728\u91cd\u65b0\u5ba1\u89c6\u5e76\u5229\u7528\u7eb9\u7406\u4f5c\u4e3a\u4e00\u79cd\u5bc6\u96c6\u7684\u3001\u7a7a\u95f4\u5b9a\u4f4d\u7684\u7ebf\u7d22\uff0c\u4e3b\u52a8\u652f\u6301\u624b\u90e8\u59ff\u6001\u548c\u5f62\u72b6\u4f30\u8ba1\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7eb9\u7406\u6a21\u5757\uff0c\u5c06\u50cf\u7d20\u7ea7\u89c2\u6d4b\u5d4c\u5165UV\u7eb9\u7406\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u5bc6\u96c6\u5bf9\u9f50\u635f\u5931\uff0c\u7528\u4e8e\u9884\u6d4b\u548c\u89c2\u6d4b\u624b\u90e8\u5916\u89c2\u7684\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u5047\u8bbe\u53ef\u8bbf\u95ee\u53ef\u5fae\u5206\u6e32\u67d3\u7ba1\u7ebf\u548c\u53ef\u5c06\u56fe\u50cf\u6620\u5c04\u5230\u5df2\u77e5\u62d3\u62513D\u624b\u90e8\u7f51\u683c\u7684\u6a21\u578b\uff0c\u4ece\u800c\u5b9e\u73b0\u7eb9\u7406\u624b\u90e8\u7684\u56fe\u50cf\u53cd\u5411\u6295\u5f71\u548c\u50cf\u7d20\u7ea7\u5bf9\u9f50\u3002\u8be5\u6a21\u5757\u53ef\u72ec\u7acb\u63d2\u62d4\u5230\u73b0\u6709\u91cd\u5efa\u7ba1\u7ebf\u4e2d\uff0c\u5e76\u901a\u8fc7\u589e\u5f3aHaMeR\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u63d0\u9ad8\u4e863D\u624b\u90e8\u91cd\u5efa\u7684\u51c6\u786e\u6027\u548c\u771f\u5b9e\u611f\u3002", "conclusion": "\u7eb9\u7406\u6307\u5bfc\u7684\u5bf9\u9f50\uff08\u6216\u79f0\u5916\u89c2\u6307\u5bfc\u7684\u5bf9\u9f50\uff09\u57283D\u624b\u90e8\u91cd\u5efa\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u80fd\u6709\u6548\u63d0\u5347\u59ff\u6001\u548c\u5f62\u72b6\u4f30\u8ba1\u7684\u6027\u80fd\u3002"}}
