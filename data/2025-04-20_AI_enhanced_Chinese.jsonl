{"id": "2504.12308", "pdf": "https://arxiv.org/pdf/2504.12308", "abs": "https://arxiv.org/abs/2504.12308", "authors": ["Devansh Singh", "Sundaraparipurnan Narayanan"], "title": "Unmasking the Reality of PII Masking Models: Performance Gaps and the Call for Accountability", "categories": ["cs.CL", "cs.CY", "cs.IR"], "comment": null, "summary": "Privacy Masking is a critical concept under data privacy involving\nanonymization and de-anonymization of personally identifiable information\n(PII). Privacy masking techniques rely on Named Entity Recognition (NER)\napproaches under NLP support in identifying and classifying named entities in\neach text. NER approaches, however, have several limitations including (a)\ncontent sensitivity including ambiguous, polysemic, context dependent or domain\nspecific content, (b) phrasing variabilities including nicknames and alias,\ninformal expressions, alternative representations, emerging expressions,\nevolving naming conventions and (c) formats or syntax variations, typos,\nmisspellings. However, there are a couple of PII datasets that have been widely\nused by researchers and the open-source community to train models on PII\ndetection or masking. These datasets have been used to train models including\nPiiranha and Starpii, which have been downloaded over 300k and 580k times on\nHuggingFace. We examine the quality of the PII masking by these models given\nthe limitations of the datasets and of the NER approaches. We curate a dataset\nof 17K unique, semi-synthetic sentences containing 16 types of PII by compiling\ninformation from across multiple jurisdictions including India, U.K and U.S. We\ngenerate sentences (using language models) containing these PII at five\ndifferent NER detection feature dimensions - (1) Basic Entity Recognition, (2)\nContextual Entity Disambiguation, (3) NER in Noisy & Real-World Data, (4)\nEvolving & Novel Entities Detection and (5) Cross-Lingual or multi-lingual NER)\nand 1 in adversarial context. We present the results and exhibit the privacy\nexposure caused by such model use (considering the extent of lifetime downloads\nof these models). We conclude by highlighting the gaps in measuring performance\nof the models and the need for contextual disclosure in model cards for such\nmodels.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u9690\u79c1\u63a9\u7801\u6280\u672f\u5728\u6570\u636e\u9690\u79c1\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u8bc4\u4f30\u73b0\u6709PII\u6570\u636e\u96c6\u548cNER\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63ed\u793a\u73b0\u6709\u9690\u79c1\u63a9\u7801\u6a21\u578b\u4e2dPII\u8bc6\u522b\u548c\u63a9\u7801\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u654f\u611f\u5185\u5bb9\u3001\u77ed\u8bed\u53d8\u4f53\u548c\u683c\u5f0f\u5dee\u5f02\u65f6\u7684\u6311\u6218\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6784\u5efa\u4e00\u4e2a\u5305\u542b16\u79cdPII\u7684\u534a\u5408\u6210\u6570\u636e\u96c6\uff0817K\u53e5\u5b50\uff09\uff0c\u5e76\u5728\u4e94\u79cdNER\u68c0\u6d4b\u7ef4\u5ea6\u53ca\u5bf9\u6297\u573a\u666f\u4e0b\u6d4b\u8bd5Piiranha\u548cStarpii\u6a21\u578b\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u8fd9\u4e9b\u7ef4\u5ea6\u4e0a\u5b58\u5728\u663e\u8457\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u5c24\u5176\u8003\u8651\u5230\u6a21\u578b\u7684\u9ad8\u4e0b\u8f7d\u91cf\u52a0\u5267\u4e86\u6f5c\u5728\u5f71\u54cd\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u9700\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u8981\u6c42\u5728\u6a21\u578b\u5361\u7247\u4e2d\u589e\u52a0\u4e0a\u4e0b\u6587\u62ab\u9732\u4ee5\u63d0\u5347\u900f\u660e\u5ea6\u3002"}}
{"id": "2504.12311", "pdf": "https://arxiv.org/pdf/2504.12311", "abs": "https://arxiv.org/abs/2504.12311", "authors": ["Enming Zhang", "Liwen Cao", "Yanru Wu", "Zijie Zhao", "Guan Wang", "Yang Li"], "title": "Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer", "categories": ["cs.CL"], "comment": null, "summary": "Prompt tuning has emerged as a lightweight adaptation strategy for adapting\nfoundation models to downstream tasks, particularly in resource-constrained\nsystems. As pre-trained prompts have become valuable intellectual assets,\ncombining multiple source prompts offers a promising approach to enhance\ngeneralization to new tasks by leveraging complementary knowledge from diverse\nsources. However, naive aggregation of these prompts often leads to\nrepresentation collapse due to mutual interference, undermining their\ncollective potential. To address these challenges, we propose HGPrompt, an\nadaptive framework for multi-source prompt transfer that learns optimal\nensemble weights by jointly optimizing dual objectives: transferability and\nstability. Specifically, we first introduce an information-theoretic metric to\nevaluate the transferability of prompt-induced features on the target task,\ncapturing the intrinsic alignment between the feature representations.\nAdditionally, we propose a novel Gradient Alignment Regularization to mitigate\ngradient conflicts among prompts, enabling stable and coherent knowledge\ntransfer from multiple sources while suppressing interference. Extensive\nexperiments on the large-scale VTAB benchmark demonstrate that HGPrompt\nachieves state-of-the-art performance, validating its effectiveness in\nmulti-source prompt transfer.", "AI": {"tldr": "HGPrompt\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u591a\u6e90\u63d0\u793a\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u53ef\u8fc1\u79fb\u6027\u548c\u7a33\u5b9a\u6027\u63d0\u5347\u57fa\u7840\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u9884\u8bad\u7ec3\u63d0\u793a\u4f5c\u4e3a\u77e5\u8bc6\u8d44\u4ea7\uff0c\u5176\u7b80\u5355\u805a\u5408\u5e38\u5bfc\u81f4\u8868\u5f81\u5d29\u6e83\uff0c\u9650\u5236\u4e86\u591a\u6e90\u77e5\u8bc6\u7684\u4e92\u8865\u6f5c\u529b\u3002", "method": "\u8054\u5408\u4f18\u5316\u53cc\u91cd\u76ee\u6807\uff08\u53ef\u8fc1\u79fb\u6027+\u7a33\u5b9a\u6027\uff09\uff1a1\uff09\u4fe1\u606f\u8bba\u6307\u6807\u8bc4\u4f30\u63d0\u793a\u7279\u5f81\u7684\u53ef\u8fc1\u79fb\u6027\uff1b2\uff09\u68af\u5ea6\u5bf9\u9f50\u6b63\u5219\u5316\u51cf\u5c11\u63d0\u793a\u95f4\u68af\u5ea6\u51b2\u7a81\u3002", "result": "\u5728VTAB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd", "conclusion": "HGPrompt\u901a\u8fc7\u81ea\u9002\u5e94\u52a0\u6743\u7b56\u7565\u6709\u6548\u5b9e\u73b0\u4e86\u591a\u6e90\u63d0\u793a\u7684\u7a33\u5b9a\u77e5\u8bc6\u8fc1\u79fb\u3002"}}
{"id": "2504.12312", "pdf": "https://arxiv.org/pdf/2504.12312", "abs": "https://arxiv.org/abs/2504.12312", "authors": ["Zihao Xu", "Junchen Ding", "Yiling Lou", "Kun Zhang", "Dong Gong", "Yuekang Li"], "title": "Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved significant progress in language\nunderstanding and reasoning. Evaluating and analyzing their logical reasoning\nabilities has therefore become essential. However, existing datasets and\nbenchmarks are often limited to overly simplistic, unnatural, or contextually\nconstrained examples. In response to the growing demand, we introduce\nSmartyPat-Bench, a challenging, naturally expressed, and systematically labeled\nbenchmark derived from real-world high-quality Reddit posts containing subtle\nlogical fallacies. Unlike existing datasets and benchmarks, it provides more\ndetailed annotations of logical fallacies and features more diverse data. To\nfurther scale up the study and address the limitations of manual data\ncollection and labeling - such as fallacy-type imbalance and labor-intensive\nannotation - we introduce SmartyPat, an automated framework powered by logic\nprogramming-based oracles. SmartyPat utilizes Prolog rules to systematically\ngenerate logically fallacious statements, which are then refined into fluent\nnatural-language sentences by LLMs, ensuring precise fallacy representation.\nExtensive evaluation demonstrates that SmartyPat produces fallacies comparable\nin subtlety and quality to human-generated content and significantly\noutperforms baseline methods. Finally, experiments reveal nuanced insights into\nLLM capabilities, highlighting that while excessive reasoning steps hinder\nfallacy detection accuracy, structured reasoning enhances fallacy\ncategorization performance.", "AI": {"tldr": "\u4ecb\u7ecdSmartyPat-Bench\uff0c\u4e00\u4e2a\u57fa\u4e8eReddit\u5e16\u5b50\u6784\u5efa\u7684\u903b\u8f91\u8c2c\u8bef\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u81ea\u52a8\u5316\u6846\u67b6SmartyPat\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u903b\u8f91\u8c2c\u8bef\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u5728\u903b\u8f91\u63a8\u7406\u8bc4\u4f30\u4e2d\u5b58\u5728\u8fc7\u4e8e\u7b80\u5316\u6216\u8bed\u5883\u53d7\u9650\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u903b\u8f91\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u7684\u9700\u6c42\u3002", "method": "\u5229\u7528\u57fa\u4e8e\u903b\u8f91\u7f16\u7a0b\u7684Prolog\u89c4\u5219\u81ea\u52a8\u751f\u6210\u903b\u8f91\u8c2c\u8bef\uff0c\u518d\u901a\u8fc7LLMs\u5c06\u5176\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u53e5\u5b50\uff0c\u6784\u5efaSmartyPat-Bench\u6570\u636e\u96c6\u3002", "result": "SmartyPat\u751f\u6210\u7684\u8c2c\u8bef\u4e0e\u4eba\u5de5\u751f\u6210\u5185\u5bb9\u5728\u8d28\u91cf\u548c\u5fae\u5999\u6027\u4e0a\u76f8\u5f53\uff0c\u4e14\u5728\u63a8\u7406\u6b65\u6570\u8fc7\u591a\u65f6\u4f1a\u5f71\u54cd\u8c2c\u8bef\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u4f46\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u63d0\u5347\u8c2c\u8bef\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "SmartyPat-Bench\u548cSmartyPat\u6846\u67b6\u4e3aLLMs\u903b\u8f91\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86LLMs\u5728\u63a8\u7406\u6b65\u9aa4\u4e0e\u8c2c\u8bef\u68c0\u6d4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002"}}
{"id": "2504.12313", "pdf": "https://arxiv.org/pdf/2504.12313", "abs": "https://arxiv.org/abs/2504.12313", "authors": ["Xiaoyan Zhao", "Yang Deng", "Wenjie Wang", "Hongzhan lin", "Hong Cheng", "Rui Zhang", "See-Kiong Ng", "Tat-Seng Chua"], "title": "Exploring the Impact of Personality Traits on Conversational Recommender Systems: A Simulation with Large Language Models", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Conversational Recommender Systems (CRSs) engage users in multi-turn\ninteractions to deliver personalized recommendations. The emergence of large\nlanguage models (LLMs) further enhances these systems by enabling more natural\nand dynamic user interactions. However, a key challenge remains in\nunderstanding how personality traits shape conversational recommendation\noutcomes. Psychological evidence highlights the influence of personality traits\non user interaction behaviors. To address this, we introduce an LLM-based\npersonality-aware user simulation for CRSs (PerCRS). The user agent induces\ncustomizable personality traits and preferences, while the system agent\npossesses the persuasion capability to simulate realistic interaction in CRSs.\nWe incorporate multi-aspect evaluation to ensure robustness and conduct\nextensive analysis from both user and system perspectives. Experimental results\ndemonstrate that state-of-the-art LLMs can effectively generate diverse user\nresponses aligned with specified personality traits, thereby prompting CRSs to\ndynamically adjust their recommendation strategies. Our experimental analysis\noffers empirical insights into the impact of personality traits on the outcomes\nof conversational recommender systems.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4e2a\u6027\u5316\u611f\u77e5\u7528\u6237\u6a21\u62df\u7cfb\u7edf\uff08PerCRS\uff09\uff0c\u63a2\u8ba8\u4eba\u683c\u7279\u8d28\u5982\u4f55\u5f71\u54cd\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7684\u7ed3\u679c\u3002", "motivation": "\u4eba\u683c\u7279\u8d28\u5bf9\u7528\u6237\u4e92\u52a8\u884c\u4e3a\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u73b0\u6709\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\uff08CRSs\uff09\u5bf9\u8fd9\u65b9\u9762\u7684\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u5f15\u5165PerCRS\uff0c\u7528\u6237\u4ee3\u7406\u53ef\u5b9a\u5236\u4eba\u683c\u7279\u8d28\u548c\u504f\u597d\uff0c\u7cfb\u7edf\u4ee3\u7406\u5177\u5907\u8bf4\u670d\u80fd\u529b\u4ee5\u6a21\u62df\u771f\u5b9e\u4e92\u52a8\uff0c\u5e76\u91c7\u7528\u591a\u89d2\u5ea6\u8bc4\u4f30\u786e\u4fdd\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5148\u8fdb\u7684LLMs\u80fd\u751f\u6210\u7b26\u5408\u6307\u5b9a\u4eba\u683c\u7279\u8d28\u7684\u591a\u6837\u5316\u7528\u6237\u54cd\u5e94\uff0c\u4fc3\u4f7fCRSs\u52a8\u6001\u8c03\u6574\u63a8\u8350\u7b56\u7565\u3002", "conclusion": "\u4eba\u683c\u7279\u8d28\u5bf9\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7ed3\u679c\u6709\u663e\u8457\u5f71\u54cd\uff0cPerCRS\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u89c1\u89e3\u3002"}}
{"id": "2504.12364", "pdf": "https://arxiv.org/pdf/2504.12364", "abs": "https://arxiv.org/abs/2504.12364", "authors": ["Tianhui Song", "Weixin Feng", "Shuai Wang", "Xubin Li", "Tiezheng Ge", "Bo Zheng", "Limin Wang"], "title": "DMM: Building a Versatile Image Generation Model via Distillation-Based Model Merging", "categories": ["cs.CV"], "comment": null, "summary": "The success of text-to-image (T2I) generation models has spurred a\nproliferation of numerous model checkpoints fine-tuned from the same base model\non various specialized datasets. This overwhelming specialized model production\nintroduces new challenges for high parameter redundancy and huge storage cost,\nthereby necessitating the development of effective methods to consolidate and\nunify the capabilities of diverse powerful models into a single one. A common\npractice in model merging adopts static linear interpolation in the parameter\nspace to achieve the goal of style mixing. However, it neglects the features of\nT2I generation task that numerous distinct models cover sundry styles which may\nlead to incompatibility and confusion in the merged model. To address this\nissue, we introduce a style-promptable image generation pipeline which can\naccurately generate arbitrary-style images under the control of style vectors.\nBased on this design, we propose the score distillation based model merging\nparadigm (DMM), compressing multiple models into a single versatile T2I model.\nMoreover, we rethink and reformulate the model merging task in the context of\nT2I generation, by presenting new merging goals and evaluation protocols. Our\nexperiments demonstrate that DMM can compactly reorganize the knowledge from\nmultiple teacher models and achieve controllable arbitrary-style generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6570\u84b8\u998f\u7684\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\uff08DMM\uff09\uff0c\u7528\u4e8e\u5c06\u591a\u4e2a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5408\u5e76\u4e3a\u4e00\u4e2a\u591a\u529f\u80fd\u6a21\u578b\uff0c\u5b9e\u73b0\u53ef\u63a7\u7684\u4efb\u610f\u98ce\u683c\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u56e0\u98ce\u683c\u591a\u6837\u5bfc\u81f4\u7684\u517c\u5bb9\u6027\u548c\u6df7\u6dc6\u95ee\u9898\u3002", "method": "\u5f15\u5165\u98ce\u683c\u53ef\u63d0\u793a\u7684\u56fe\u50cf\u751f\u6210\u6d41\u7a0b\uff0c\u63d0\u51fa\u57fa\u4e8e\u5206\u6570\u84b8\u998f\u7684\u6a21\u578b\u5408\u5e76\u8303\u5f0f\uff08DMM\uff09\uff0c\u5e76\u91cd\u65b0\u5b9a\u4e49\u6a21\u578b\u5408\u5e76\u7684\u76ee\u6807\u548c\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDMM\u80fd\u591f\u7d27\u51d1\u5730\u91cd\u7ec4\u591a\u4e2a\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\uff0c\u5e76\u5b9e\u73b0\u53ef\u63a7\u7684\u4efb\u610f\u98ce\u683c\u751f\u6210\u3002", "conclusion": "DMM\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u578b\u5408\u5e76\u4e2d\u7684\u5197\u4f59\u548c\u5b58\u50a8\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u53ef\u63a7\u6027\u3002"}}
{"id": "2504.12359", "pdf": "https://arxiv.org/pdf/2504.12359", "abs": "https://arxiv.org/abs/2504.12359", "authors": ["Yuanbo Tang", "Yan Tang", "Naifan Zhang", "Meixuan Chen", "Yang Li"], "title": "Unveiling Hidden Collaboration within Mixture-of-Experts in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mixture-of-Experts based large language models (MoE LLMs) have shown\nsignificant promise in multitask adaptability by dynamically routing inputs to\nspecialized experts. Despite their success, the collaborative mechanisms among\nexperts are still not well understood, limiting both the interpretability and\noptimization of these models. In this paper, we focus on two critical issues:\n(1) identifying expert collaboration patterns, and (2) optimizing MoE LLMs\nthrough expert pruning. To address the first issue, we propose a hierarchical\nsparse dictionary learning (HSDL) method that uncovers the collaboration\npatterns among experts. For the second issue, we introduce the\nContribution-Aware Expert Pruning (CAEP) algorithm, which effectively prunes\nlow-contribution experts. Our extensive experiments demonstrate that expert\ncollaboration patterns are closely linked to specific input types and exhibit\nsemantic significance across various tasks. Moreover, pruning experiments show\nthat our approach improves overall performance by 2.5\\% on average,\noutperforming existing methods. These findings offer valuable insights into\nenhancing the efficiency and interpretability of MoE LLMs, offering a clearer\nunderstanding of expert interactions and improving model optimization.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6df7\u5408\u4e13\u5bb6\u5927\u6a21\u578b\u4e2d\u4e13\u5bb6\u95f4\u7684\u534f\u4f5c\u6a21\u5f0f\u53ca\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u51faHSDL\u65b9\u6cd5\u8bc6\u522b\u534f\u4f5c\u6a21\u5f0f\u53caCAEP\u7b97\u6cd5\u526a\u679d\u4f4e\u8d21\u732e\u4e13\u5bb6\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u63d0\u53472.5%\u3002", "motivation": "\u6df7\u5408\u4e13\u5bb6\u5927\u6a21\u578b\u867d\u5728\u4efb\u52a1\u9002\u5e94\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4e13\u5bb6\u534f\u4f5c\u673a\u5236\u5c1a\u4e0d\u6e05\u695a\uff0c\u5f71\u54cd\u6a21\u578b\u89e3\u91ca\u6027\u4e0e\u4f18\u5316\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u534f\u4f5c\u6a21\u5f0f\u5e76\u4f18\u5316\u526a\u679d\u7b56\u7565\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7a00\u758f\u5b57\u5178\u5b66\u4e60\uff08HSDL\uff09\u63ed\u793a\u4e13\u5bb6\u534f\u4f5c\u6a21\u5f0f\uff0c\u63d0\u51fa\u8d21\u732e\u611f\u77e5\u4e13\u5bb6\u526a\u679d\uff08CAEP\uff09\u7b97\u6cd5\u4f18\u5316\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u4e13\u5bb6\u534f\u4f5c\u6a21\u5f0f\u4e0e\u8f93\u5165\u7c7b\u578b\u76f8\u5173\u4e14\u5177\u8bed\u4e49\u610f\u4e49\uff1bCAEP\u7b97\u6cd5\u5e73\u5747\u63d0\u5347\u6a21\u578b\u6027\u80fd2.5%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u4e3a\u589e\u5f3a\u6df7\u5408\u4e13\u5bb6\u5927\u6a21\u578b\u7684\u6548\u7387\u4e0e\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6539\u8fdb\u4e86\u4e13\u5bb6\u4ea4\u4e92\u7406\u89e3\u4e0e\u6a21\u578b\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2504.12417", "pdf": "https://arxiv.org/pdf/2504.12417", "abs": "https://arxiv.org/abs/2504.12417", "authors": ["Dewang Kumar Agarwal", "Dimitris J. Bertsimas"], "title": "Interpretable AI-driven Guidelines for Type 2 Diabetes Treatment from Observational Data", "categories": ["cs.AI"], "comment": null, "summary": "Objective: Create precise, structured, data-backed guidelines for type 2\ndiabetes treatment progression, suitable for clinical adoption.\n  Research Design and Methods: Our training cohort was composed of patient\n(with type 2 diabetes) visits from Boston Medical Center (BMC) from 1998 to\n2014. We divide visits into 4 groups based on the patient's treatment regimen\nbefore the visit, and further divide them into subgroups based on the\nrecommended treatment during the visit. Since each subgroup has observational\ndata, which has confounding bias (sicker patients are prescribed more\naggressive treatments), we used machine learning and optimization to remove\nsome datapoints so that the remaining data resembles a randomized trial. On\neach subgroup, we train AI-backed tree-based models to prescribe treatment\nchanges. Once we train these tree models, we manually combine the models for\nevery group to create an end-to-end prescription pipeline for all patients in\nthat group. In this process, we prioritize stepping up to a more aggressive\ntreatment before considering less aggressive options. We tested this pipeline\non unseen data from BMC, and an external dataset from Hartford healthcare (type\n2 diabetes patient visits from January 2020 to May 2024).\n  Results: The median HbA1c reduction achieved by our pipelines is 0.26% more\nthan what the doctors achieved on the unseen BMC patients. For the Hartford\ncohort, our pipelines were better by 0.13%.\n  Conclusions: This precise, interpretable, and efficient AI-backed approach to\ntreatment progression in type 2 diabetes is predicted to outperform the current\npractice and can be deployed to improve patient outcomes.", "AI": {"tldr": "\u7814\u7a76\u521b\u5efa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u76842\u578b\u7cd6\u5c3f\u75c5\u6cbb\u7597\u8fdb\u5c55\u6307\u5357\uff0c\u4f18\u5316\u6cbb\u7597\u65b9\u6848\u5e76\u63d0\u5347\u6cbb\u7597\u6548\u679c\u3002", "motivation": "\u65e8\u5728\u4e3a2\u578b\u7cd6\u5c3f\u75c5\u6cbb\u7597\u63d0\u4f9b\u7cbe\u51c6\u3001\u7ed3\u6784\u5316\u4e14\u57fa\u4e8e\u6570\u636e\u7684\u4e34\u5e8a\u6307\u5357\uff0c\u4ee5\u63d0\u9ad8\u6cbb\u7597\u6548\u679c\u3002", "method": "\u4f7f\u7528\u6ce2\u58eb\u987f\u533b\u7597\u4e2d\u5fc3\u7684\u6570\u636e\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u548c\u4f18\u5316\u65b9\u6cd5\u53bb\u9664\u6df7\u6742\u504f\u5dee\uff0c\u8bad\u7ec3\u57fa\u4e8e\u6811\u7684AI\u6a21\u578b\uff0c\u5e76\u624b\u52a8\u6574\u5408\u6a21\u578b\u5f62\u6210\u7aef\u5230\u7aef\u7684\u6cbb\u7597\u6d41\u7a0b\u3002", "result": "\u5728\u672a\u89c1\u7684BMC\u60a3\u8005\u4e2d\uff0c\u8be5\u65b9\u6cd5\u7684\u4e2d\u4f4dHbA1c\u964d\u4f4e\u6bd4\u533b\u751f\u65b9\u6848\u9ad80.26%\uff1b\u5728Hartford\u961f\u5217\u4e2d\u9ad80.13%\u3002", "conclusion": "\u8fd9\u79cd\u7cbe\u51c6\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684AI\u652f\u6301\u65b9\u6cd5\u9884\u8ba1\u80fd\u8d85\u8d8a\u5f53\u524d\u5b9e\u8df5\uff0c\u6539\u5584\u60a3\u8005\u9884\u540e\u3002"}}
{"id": "2504.12770", "pdf": "https://arxiv.org/pdf/2504.12770", "abs": "https://arxiv.org/abs/2504.12770", "authors": ["Chandimal Jayawardena", "Jay Chen", "Amay Bhalla", "Lin Bu"], "title": "Comparative Analysis of POX and RYU SDN Controllers in Scalable Networks", "categories": ["cs.NI"], "comment": "17 pages", "summary": "This paper explores the Quality of Service (QoS) performance of two widely\nused Software-Defined Networking (SDN) controllers, POX and Ryu, using Mininet\nfor network simulation. SDN, a transformative approach to network architecture,\nseparates the control and data planes, enabling centralized management,\nimproved agility, and cost-effective solutions. The study evaluates key QoS\nparameters, including throughput, delay, and jitter, to understand the\ncapabilities and limitations of the POX and Ryu controllers in handling traffic\nunder diverse network topologies. The research employs a systematic methodology\ninvolving the design of custom network topologies, implementation of OpenFlow\nrules, and analysis of controller behavior under simulated conditions. Results\nreveal that while POX offers simplicity and ease of use, making it suitable for\nsmaller-scale applications and experimentation, Ryu provides superior\nscalability and adaptability for more complex network environments. The\nfindings highlight the strengths and challenges of each controller, providing\nvaluable insights for organizations seeking to optimize SDN deployment. This\nstudy contributes to the growing body of knowledge on SDN technologies and\ntheir role in building scalable, efficient, and resilient network\ninfrastructures.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7Mininet\u6a21\u62df\u8bc4\u4f30\u4e86POX\u548cRyu\u4e24\u79cdSDN\u63a7\u5236\u5668\u5728\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u548c\u6296\u52a8\u7b49QoS\u53c2\u6570\u4e0a\u7684\u8868\u73b0\uff0c\u6bd4\u8f83\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u4e3a\u4e86\u4e86\u89e3POX\u548cRyu\u63a7\u5236\u5668\u5728\u4e0d\u540c\u7f51\u7edc\u62d3\u6251\u4e0b\u5904\u7406\u6d41\u91cf\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u4f18\u5316SDN\u90e8\u7f72\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u8bbe\u8ba1\u81ea\u5b9a\u4e49\u7f51\u7edc\u62d3\u6251\u3001\u5b9e\u73b0OpenFlow\u89c4\u5219\uff0c\u5e76\u5728\u6a21\u62df\u6761\u4ef6\u4e0b\u5206\u6790\u63a7\u5236\u5668\u7684\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0cPOX\u9002\u5408\u5c0f\u89c4\u6a21\u5e94\u7528\u548c\u5b9e\u9a8c\uff0c\u800cRyu\u5728\u590d\u6742\u7f51\u7edc\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u6bcf\u79cd\u63a7\u5236\u5668\u90fd\u6709\u5176\u4f18\u52bf\u548c\u6311\u6218\uff0c\u672c\u7814\u7a76\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u548c\u5f39\u6027\u7684\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2504.12314", "pdf": "https://arxiv.org/pdf/2504.12314", "abs": "https://arxiv.org/abs/2504.12314", "authors": ["Hao Li", "Liuzhenghao Lv", "He Cao", "Zijing Liu", "Zhiyuan Yan", "Yu Wang", "Yonghong Tian", "Yu Li", "Li Yuan"], "title": "How to Detect and Defeat Molecular Mirage: A Metric-Driven Benchmark for Hallucination in LLM-based Molecular Comprehension", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "Large language models are increasingly used in scientific domains, especially\nfor molecular understanding and analysis. However, existing models are affected\nby hallucination issues, resulting in errors in drug design and utilization. In\nthis paper, we first analyze the sources of hallucination in LLMs for molecular\ncomprehension tasks, specifically the knowledge shortcut phenomenon observed in\nthe PubChem dataset. To evaluate hallucination in molecular comprehension tasks\nwith computational efficiency, we introduce \\textbf{Mol-Hallu}, a novel\nfree-form evaluation metric that quantifies the degree of hallucination based\non the scientific entailment relationship between generated text and actual\nmolecular properties. Utilizing the Mol-Hallu metric, we reassess and analyze\nthe extent of hallucination in various LLMs performing molecular comprehension\ntasks. Furthermore, the Hallucination Reduction Post-processing stage~(HRPP) is\nproposed to alleviate molecular hallucinations, Experiments show the\neffectiveness of HRPP on decoder-only and encoder-decoder molecular LLMs. Our\nfindings provide critical insights into mitigating hallucination and improving\nthe reliability of LLMs in scientific applications.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5b50\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u8bc4\u4f30\u6307\u6807Mol-Hallu\u548c\u51cf\u5c11\u5e7b\u89c9\u7684\u540e\u5904\u7406\u65b9\u6cd5HRPP\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5b50\u7406\u89e3\u4efb\u52a1\u4e2d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5bfc\u81f4\u836f\u7269\u8bbe\u8ba1\u548c\u5229\u7528\u4e2d\u7684\u9519\u8bef\uff0c\u9700\u8981\u6709\u6548\u8bc4\u4f30\u548c\u7f13\u89e3\u65b9\u6cd5\u3002", "method": "\u5f15\u5165Mol-Hallu\u6307\u6807\u91cf\u5316\u5e7b\u89c9\u7a0b\u5ea6\uff0c\u5e76\u63d0\u51faHRPP\u540e\u5904\u7406\u9636\u6bb5\u51cf\u8f7b\u5206\u5b50\u5e7b\u89c9\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eHRPP\u5728\u4ec5\u89e3\u7801\u5668\u548c\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5206\u5b50LLMs\u4e2d\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u3002", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u9ad8\u79d1\u5b66\u5e94\u7528\u4e2dLLMs\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2504.12368", "pdf": "https://arxiv.org/pdf/2504.12368", "abs": "https://arxiv.org/abs/2504.12368", "authors": ["Babak Ghassemi", "Cassio Fraga-Dantas", "Raffaele Gaetano", "Dino Ienco", "Omid Ghorbanzadeh", "Emma Izquierdo-Verdiguier", "Francesco Vuolo"], "title": "Geographical Context Matters: Bridging Fine and Coarse Spatial Information to Enhance Continental Land Cover Mapping", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Land use and land cover mapping from Earth Observation (EO) data is a\ncritical tool for sustainable land and resource management. While advanced\nmachine learning and deep learning algorithms excel at analyzing EO imagery\ndata, they often overlook crucial geospatial metadata information that could\nenhance scalability and accuracy across regional, continental, and global\nscales. To address this limitation, we propose BRIDGE-LC (Bi-level\nRepresentation Integration for Disentangled GEospatial Land Cover), a novel\ndeep learning framework that integrates multi-scale geospatial information into\nthe land cover classification process. By simultaneously leveraging\nfine-grained (latitude/longitude) and coarse-grained (biogeographical region)\nspatial information, our lightweight multi-layer perceptron architecture learns\nfrom both during training but only requires fine-grained information for\ninference, allowing it to disentangle region-specific from region-agnostic land\ncover features while maintaining computational efficiency. To assess the\nquality of our framework, we use an open-access in-situ dataset and adopt\nseveral competing classification approaches commonly considered for large-scale\nland cover mapping. We evaluated all approaches through two scenarios: an\nextrapolation scenario in which training data encompasses samples from all\nbiogeographical regions, and a leave-one-region-out scenario where one region\nis excluded from training. We also explore the spatial representation learned\nby our model, highlighting a connection between its internal manifold and the\ngeographical information used during training. Our results demonstrate that\nintegrating geospatial information improves land cover mapping performance,\nwith the most substantial gains achieved by jointly leveraging both fine- and\ncoarse-grained spatial information.", "AI": {"tldr": "\u63d0\u51faBRIDGE-LC\u6846\u67b6\uff0c\u6574\u5408\u591a\u5c3a\u5ea6\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\u4ee5\u63d0\u9ad8\u571f\u5730\u5229\u7528\u548c\u571f\u5730\u8986\u76d6\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u5730\u7403\u89c2\u6d4b\u6570\u636e\u65f6\u5ffd\u89c6\u5730\u7406\u7a7a\u95f4\u5143\u6570\u636e\uff0c\u9650\u5236\u4e86\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u6269\u5c55\u6027\u3002", "method": "\u4f7f\u7528\u53cc\u5c42\u8868\u793a\u96c6\u6210\u6846\u67b6BRIDGE-LC\uff0c\u7ed3\u5408\u7cbe\u7ec6\u548c\u7c97\u7cd9\u7a7a\u95f4\u4fe1\u606f\uff0c\u8bad\u7ec3\u65f6\u91c7\u7528\u591a\u5c3a\u5ea6\u4fe1\u606f\uff0c\u63a8\u7406\u65f6\u4ec5\u9700\u7cbe\u7ec6\u4fe1\u606f\u3002", "result": "\u96c6\u6210\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u8054\u5408\u4f7f\u7528\u7cbe\u7ec6\u548c\u7c97\u7cd9\u7a7a\u95f4\u4fe1\u606f\u65f6\u6548\u679c\u6700\u4f73\u3002", "conclusion": "BRIDGE-LC\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u571f\u5730\u5229\u7528\u548c\u8986\u76d6\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2504.12397", "pdf": "https://arxiv.org/pdf/2504.12397", "abs": "https://arxiv.org/abs/2504.12397", "authors": ["Kristjan Greenewald", "Luis Lastras", "Thomas Parnell", "Vraj Shah", "Lucian Popa", "Giulio Zizzo", "Chulaka Gunasekara", "Ambrish Rawat", "David Cox"], "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics", "categories": ["cs.LG", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2504.11704", "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aaLoRA\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5927\u578b\u57fa\u7840\u6a21\u578b\u4e2d\u9ad8\u6548\u5207\u6362\u4e0d\u540c\u7684LoRA\u9002\u914d\u5668\uff0c\u907f\u514d\u91cd\u590d\u8ba1\u7b97KV\u7f13\u5b58\u3002", "motivation": "\u89e3\u51b3\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5207\u6362\u4e0d\u540cLoRA\u9002\u914d\u5668\u65f6\u9700\u8981\u91cd\u65b0\u8ba1\u7b97\u6574\u4e2a\u5386\u53f2KV\u7f13\u5b58\u7684\u95ee\u9898\u3002", "method": "\u4fee\u6539LoRA\u6846\u67b6\uff0c\u4f7f\u5176\u4ec5\u9002\u914d\u6fc0\u6d3b\u540e\u751f\u6210\u7684token\uff0c\u4ece\u800c\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u7684KV\u7f13\u5b58\u3002", "result": "aLoRA\u5728\u4fdd\u6301\u4e0e\u6807\u51c6LoRA\u76f8\u5f53\u51c6\u786e\u5ea6\u7684\u540c\u65f6\uff0c\u5e26\u6765\u4e86\u663e\u8457\u7684\u63a8\u7406\u6548\u7387\u63d0\u5347\u3002", "conclusion": "aLoRA\u80fd\u591f\u5b9e\u73b0\u6240\u8c13\u7684'intrinsics'\u6a21\u578b\uff0c\u5373\u53ef\u4ee5\u5373\u65f6\u6fc0\u6d3b\u7684\u9ad8\u5ea6\u4e13\u4e1a\u5316\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u5347\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u6548\u7387\u3002"}}
{"id": "2504.12477", "pdf": "https://arxiv.org/pdf/2504.12477", "abs": "https://arxiv.org/abs/2504.12477", "authors": ["George Fatouros", "Georgios Makridis", "George Kousiouris", "John Soldatos", "Anargyros Tsadimas", "Dimosthenis Kyriazis"], "title": "Towards Conversational AI for Human-Machine Collaborative MLOps", "categories": ["cs.AI", "cs.CL", "cs.HC", "68T50, 68T99, 68U35, 68N19", "I.2.1; H.5.2; D.2.11; I.2.7"], "comment": "8 pages, 5 figures", "summary": "This paper presents a Large Language Model (LLM) based conversational agent\nsystem designed to enhance human-machine collaboration in Machine Learning\nOperations (MLOps). We introduce the Swarm Agent, an extensible architecture\nthat integrates specialized agents to create and manage ML workflows through\nnatural language interactions. The system leverages a hierarchical, modular\ndesign incorporating a KubeFlow Pipelines (KFP) Agent for ML pipeline\norchestration, a MinIO Agent for data management, and a Retrieval-Augmented\nGeneration (RAG) Agent for domain-specific knowledge integration. Through\niterative reasoning loops and context-aware processing, the system enables\nusers with varying technical backgrounds to discover, execute, and monitor ML\npipelines; manage datasets and artifacts; and access relevant documentation,\nall via intuitive conversational interfaces. Our approach addresses the\naccessibility gap in complex MLOps platforms like Kubeflow, making advanced ML\ntools broadly accessible while maintaining the flexibility to extend to other\nplatforms. The paper describes the architecture, implementation details, and\ndemonstrates how this conversational MLOps assistant reduces complexity and\nlowers barriers to entry for users across diverse technical skill levels.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u4ee3\u7406\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u63d0\u5347\u4eba\u7c7b\u4e0e\u673a\u5668\u5728MLOps\u4e2d\u7684\u534f\u4f5c\u3002\u8be5\u7cfb\u7edf\u540d\u4e3aSwarm Agent\uff0c\u91c7\u7528\u5206\u5c42\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u96c6\u6210\u591a\u79cd\u529f\u80fd\u4ee3\u7406\uff0c\u7b80\u5316\u4e86ML\u6d41\u7a0b\u7684\u7ba1\u7406\u548c\u4f7f\u7528\u3002", "motivation": "\u89e3\u51b3\u590d\u6742MLOps\u5e73\u53f0\uff08\u5982Kubeflow\uff09\u7684\u53ef\u8bbf\u95ee\u6027\u95ee\u9898\uff0c\u4f7f\u4e0d\u540c\u6280\u672f\u80cc\u666f\u7684\u7528\u6237\u80fd\u591f\u66f4\u8f7b\u677e\u5730\u4f7f\u7528\u9ad8\u7ea7\u673a\u5668\u5b66\u4e60\u5de5\u5177\u3002", "method": "\u91c7\u7528\u5206\u5c42\u3001\u6a21\u5757\u5316\u7684\u67b6\u6784\uff0c\u96c6\u6210\u4e86KFP\u4ee3\u7406\uff08\u7528\u4e8eML\u7ba1\u9053\u7f16\u6392\uff09\u3001MinIO\u4ee3\u7406\uff08\u7528\u4e8e\u6570\u636e\u7ba1\u7406\uff09\u548cRAG\u4ee3\u7406\uff08\u7528\u4e8e\u9886\u57df\u77e5\u8bc6\u96c6\u6210\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u5faa\u73af\u548c\u60c5\u5883\u611f\u77e5\u5904\u7406\u5b9e\u73b0\u529f\u80fd\u3002", "result": "\u5f00\u53d1\u7684\u5bf9\u8bdd\u5f0fMLOps\u52a9\u624b\u964d\u4f4e\u4e86\u6280\u672f\u95e8\u69db\uff0c\u4f7f\u4e0d\u540c\u6280\u672f\u6c34\u5e73\u7684\u7528\u6237\u80fd\u591f\u901a\u8fc7\u76f4\u89c2\u7684\u5bf9\u8bdd\u754c\u9762\u53d1\u73b0\u3001\u6267\u884c\u548c\u76d1\u63a7ML\u6d41\u7a0b\uff0c\u7ba1\u7406\u6570\u636e\u96c6\u548c\u5de5\u4ef6\uff0c\u5e76\u8bbf\u95ee\u76f8\u5173\u6587\u6863\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u521b\u65b0\u7684\u5bf9\u8bdd\u4ee3\u7406\u7cfb\u7edf\uff0c\u663e\u8457\u7b80\u5316\u4e86MLOps\u5e73\u53f0\u7684\u590d\u6742\u6027\uff0c\u4f7f\u5176\u66f4\u5e7f\u6cdb\u5730\u9002\u7528\u4e8e\u4e0d\u540c\u6280\u672f\u80cc\u666f\u7684\u7528\u6237\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6269\u5c55\u5230\u5176\u4ed6\u5e73\u53f0\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2504.12315", "pdf": "https://arxiv.org/pdf/2504.12315", "abs": "https://arxiv.org/abs/2504.12315", "authors": ["Xingguang Ji", "Jiakang Wang", "Hongzhi Zhang", "Jingyuan Zhang", "Haonan Zhou", "Chenxi Sun", "Yahui Liu", "Qi Wang", "Fuzheng Zhang"], "title": "Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "With the development of Multimodal Large Language Models (MLLMs), numerous\noutstanding accomplishments have emerged within the open-source community. Due\nto the complexity of creating and training multimodal data pairs, it is still a\ncomputational and time-consuming process to build powerful MLLMs. In this work,\nwe introduce Capybara-OMNI, an MLLM that trains in a lightweight and efficient\nmanner and supports understanding text, image, video, and audio modalities. We\npresent in detail the framework design, the data construction, and the training\nrecipe, to develop an MLLM step-by-step to obtain competitive performance. We\nalso provide exclusive benchmarks utilized in our experiments to show how to\nproperly verify understanding capabilities across different modalities. Results\nshow that by following our guidance, we can efficiently build an MLLM that\nachieves competitive performance among models of the same scale on various\nmultimodal benchmarks. Additionally, to enhance the multimodal instruction\nfollowing and conversational capabilities of the model, we further discuss how\nto train the chat version upon an MLLM understanding model, which is more in\nline with user habits for tasks like real-time interaction with humans. We\npublicly disclose the Capybara-OMNI model, along with its chat-based version.\nThe disclosure includes both the model weights, a portion of the training data,\nand the inference codes, which are made available on GitHub.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86Capybara-OMNI\uff0c\u4e00\u79cd\u8f7b\u91cf\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u6587\u5b57\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u7406\u89e3\uff0c\u5e76\u516c\u5f00\u4e86\u6a21\u578b\u548c\u6570\u636e\u3002", "motivation": "\u7531\u4e8e\u6784\u5efa\u548c\u8bad\u7ec3\u591a\u6a21\u6001\u6570\u636e\u5bf9\u7684\u590d\u6742\u6027\uff0c\u5f00\u53d1\u5f3a\u5927\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ecd\u8017\u65f6\u4e14\u8ba1\u7b97\u91cf\u5927\u3002", "method": "\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u6846\u67b6\u8bbe\u8ba1\u3001\u6570\u636e\u6784\u5efa\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u9010\u6b65\u5f00\u53d1\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e13\u5c5e\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002", "result": "\u9075\u5faa\u6307\u5bfc\u53ef\u9ad8\u6548\u6784\u5efa\u6027\u80fd\u4f18\u5f02\u7684\u6a21\u578b\uff0c\u5e76\u5728\u540c\u89c4\u6a21\u6a21\u578b\u4e2d\u8868\u73b0\u7ade\u4e89\u529b\u3002", "conclusion": "\u516c\u5f00\u4e86Capybara-OMNI\u53ca\u5176\u804a\u5929\u7248\u672c\uff0c\u5305\u62ec\u6a21\u578b\u6743\u91cd\u3001\u90e8\u5206\u8bad\u7ec3\u6570\u636e\u548c\u63a8\u7406\u4ee3\u7801\uff0c\u652f\u6301\u5b9e\u65f6\u4eba\u673a\u4ea4\u4e92\u7b49\u4efb\u52a1\u3002"}}
{"id": "2504.12369", "pdf": "https://arxiv.org/pdf/2504.12369", "abs": "https://arxiv.org/abs/2504.12369", "authors": ["Zeqi Xiao", "Yushi Lan", "Yifan Zhou", "Wenqi Ouyang", "Shuai Yang", "Yanhong Zeng", "Xingang Pan"], "title": "WORLDMEM: Long-term Consistent World Simulation with Memory", "categories": ["cs.CV"], "comment": "Project page at https://xizaoqu.github.io/worldmem/", "summary": "World simulation has gained increasing popularity due to its ability to model\nvirtual environments and predict the consequences of actions. However, the\nlimited temporal context window often leads to failures in maintaining\nlong-term consistency, particularly in preserving 3D spatial consistency. In\nthis work, we present WorldMem, a framework that enhances scene generation with\na memory bank consisting of memory units that store memory frames and states\n(e.g., poses and timestamps). By employing a memory attention mechanism that\neffectively extracts relevant information from these memory frames based on\ntheir states, our method is capable of accurately reconstructing previously\nobserved scenes, even under significant viewpoint or temporal gaps.\nFurthermore, by incorporating timestamps into the states, our framework not\nonly models a static world but also captures its dynamic evolution over time,\nenabling both perception and interaction within the simulated world. Extensive\nexperiments in both virtual and real scenarios validate the effectiveness of\nour approach.", "AI": {"tldr": "WorldMem\u6846\u67b6\u901a\u8fc7\u8bb0\u5fc6\u5e93\u548c\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u573a\u666f\u751f\u6210\uff0c\u4fdd\u6301\u957f\u671f3D\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u4e16\u754c\u6a21\u62df\u4e2d\u56e0\u65f6\u95f4\u4e0a\u4e0b\u6587\u7a97\u53e3\u6709\u9650\u5bfc\u81f4\u7684\u957f\u671f\u4e00\u81f4\u6027\u7ef4\u62a4\u95ee\u9898\uff0c\u7279\u522b\u662f3D\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "method": "\u4f7f\u7528\u8bb0\u5fc6\u5e93\u5b58\u50a8\u8bb0\u5fc6\u5e27\u548c\u72b6\u6001\uff08\u5982\u59ff\u6001\u548c\u65f6\u95f4\u6233\uff09\uff0c\u901a\u8fc7\u8bb0\u5fc6\u6ce8\u610f\u529b\u673a\u5236\u63d0\u53d6\u76f8\u5173\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u865a\u62df\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u6709\u6548\u91cd\u5efa\u89c2\u5bdf\u5230\u7684\u573a\u666f\uff0c\u5e76\u6355\u6349\u52a8\u6001\u6f14\u5316\u3002", "conclusion": "WorldMem\u6846\u67b6\u4e0d\u4ec5\u80fd\u5efa\u6a21\u9759\u6001\u4e16\u754c\uff0c\u8fd8\u80fd\u6355\u6349\u5176\u52a8\u6001\u53d8\u5316\uff0c\u652f\u6301\u6a21\u62df\u4e16\u754c\u4e2d\u7684\u611f\u77e5\u548c\u4ea4\u4e92\u3002"}}
{"id": "2504.12419", "pdf": "https://arxiv.org/pdf/2504.12419", "abs": "https://arxiv.org/abs/2504.12419", "authors": ["Loong Kuan Lee", "Thore Thassilo Gerlach", "Nico Piatkowski"], "title": "Standardization of Multi-Objective QUBOs", "categories": ["cs.LG", "math.OC", "quant-ph"], "comment": "7 pages, 3 figures", "summary": "Multi-objective optimization involving Quadratic Unconstrained Binary\nOptimization (QUBO) problems arises in various domains. A fundamental challenge\nin this context is the effective balancing of multiple objectives, each\npotentially operating on very different scales. This imbalance introduces\ncomplications such as the selection of appropriate weights when scalarizing\nmultiple objectives into a single objective function. In this paper, we propose\na novel technique for scaling QUBO objectives that uses an exact computation of\nthe variance of each individual QUBO objective. By scaling each objective to\nhave unit variance, we align all objectives onto a common scale, thereby\nallowing for more balanced solutions to be found when scalarizing the\nobjectives with equal weights, as well as potentially assisting in the search\nor choice of weights during scalarization. Finally, we demonstrate its\nadvantages through empirical evaluations on various multi-objective\noptimization problems. Our results are noteworthy since manually selecting\nscalarization weights is cumbersome, and reliable, efficient solutions are\nscarce.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u65b9\u5dee\u8ba1\u7b97\u7684\u591a\u76ee\u6807QUBO\u95ee\u9898\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u4f4d\u65b9\u5dee\u7f29\u653e\u5b9e\u73b0\u76ee\u6807\u5e73\u8861\u3002", "motivation": "\u591a\u76ee\u6807QUBO\u4f18\u5316\u4e2d\u76ee\u6807\u5c3a\u5ea6\u5dee\u5f02\u5bfc\u81f4\u6743\u91cd\u9009\u62e9\u56f0\u96be\uff0c\u9700\u89e3\u51b3\u6807\u91cf\u5316\u65f6\u7684\u5e73\u8861\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5404QUBO\u76ee\u6807\u65b9\u5dee\u7684\u7cbe\u786e\u8ba1\u7b97\uff0c\u5c06\u76ee\u6807\u7f29\u653e\u81f3\u5355\u4f4d\u65b9\u5dee\u4ee5\u5b9e\u73b0\u5c3a\u5ea6\u7edf\u4e00\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\u4e2d\u80fd\u6709\u6548\u63d0\u5347\u5747\u8861\u89e3\u7684\u8d28\u91cf\u3002", "conclusion": "\u65b9\u5dee\u7f29\u653e\u6280\u672f\u7b80\u5316\u4e86\u6743\u91cd\u9009\u62e9\u8fc7\u7a0b\uff0c\u4e3a\u591a\u76ee\u6807QUBO\u63d0\u4f9b\u4e86\u53ef\u9760\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.12482", "pdf": "https://arxiv.org/pdf/2504.12482", "abs": "https://arxiv.org/abs/2504.12482", "authors": ["Luciano Floridi", "Carlotta Buttaboni", "Emmie Hine", "Jessica Morley", "Claudio Novelli", "Tyler Schroder"], "title": "Agentic AI Optimisation (AAIO): what it is, how it works, why it matters, and how to deal with it", "categories": ["cs.AI"], "comment": null, "summary": "The emergence of Agentic Artificial Intelligence (AAI) systems capable of\nindependently initiating digital interactions necessitates a new optimisation\nparadigm designed explicitly for seamless agent-platform interactions. This\narticle introduces Agentic AI Optimisation (AAIO) as an essential methodology\nfor ensuring effective integration between websites and agentic AI systems.\nLike how Search Engine Optimisation (SEO) has shaped digital content\ndiscoverability, AAIO can define interactions between autonomous AI agents and\nonline platforms. By examining the mutual interdependency between website\noptimisation and agentic AI success, the article highlights the virtuous cycle\nthat AAIO can create. It further explores the governance, ethical, legal, and\nsocial implications (GELSI) of AAIO, emphasising the necessity of proactive\nregulatory frameworks to mitigate potential negative impacts. The article\nconcludes by affirming AAIO's essential role as part of a fundamental digital\ninfrastructure in the era of autonomous digital agents, advocating for\nequitable and inclusive access to its benefits.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4ee3\u7406\u4eba\u5de5\u667a\u80fd\u4f18\u5316\uff08AAIO\uff09\u4f5c\u4e3a\u4e00\u79cd\u65b0\u65b9\u6cd5\u8bba\uff0c\u65e8\u5728\u4f18\u5316\u4ee3\u7406AI\u7cfb\u7edf\u4e0e\u7f51\u7ad9\u95f4\u7684\u4e92\u52a8\uff0c\u5f3a\u8c03\u5176\u91cd\u8981\u6027\u53ca\u6f5c\u5728\u7684\u6cbb\u7406\u3001\u4f26\u7406\u548c\u6cd5\u5f8b\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u80fd\u591f\u72ec\u7acb\u53d1\u8d77\u6570\u5b57\u4e92\u52a8\u7684\u4ee3\u7406\u4eba\u5de5\u667a\u80fd\uff08AAI\uff09\u7cfb\u7edf\u7684\u51fa\u73b0\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u8303\u5f0f\u6765\u786e\u4fdd\u4ee3\u7406\u4e0e\u5e73\u53f0\u95f4\u7684\u65e0\u7f1d\u4ea4\u4e92\u3002", "method": "\u901a\u8fc7\u7c7b\u6bd4\u641c\u7d22\u5f15\u64ce\u4f18\u5316\uff08SEO\uff09\uff0c\u6587\u7ae0\u4ecb\u7ecd\u4e86AAIO\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u7f51\u7ad9\u4f18\u5316\u4e0e\u4ee3\u7406AI\u6210\u529f\u4e4b\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "AAIO\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a\u826f\u6027\u5faa\u73af\uff0c\u4f18\u5316\u4ee3\u7406AI\u4e0e\u5e73\u53f0\u7684\u4e92\u52a8\uff0c\u540c\u65f6\u6587\u7ae0\u4e5f\u63a2\u8ba8\u4e86\u5176\u6cbb\u7406\u3001\u4f26\u7406\u3001\u6cd5\u5f8b\u548c\u793e\u4f1a\u5f71\u54cd\uff08GELSI\uff09\u3002", "conclusion": "AAIO\u5728\u81ea\u4e3b\u6570\u5b57\u4ee3\u7406\u65f6\u4ee3\u662f\u57fa\u7840\u6570\u5b57\u8bbe\u65bd\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u9700\u8981\u79ef\u6781\u76d1\u7ba1\u6846\u67b6\u4ee5\u786e\u4fdd\u5176\u76ca\u5904\u80fd\u591f\u516c\u5e73\u548c\u5305\u5bb9\u5730\u5206\u914d\u3002"}}
{"id": "2504.12316", "pdf": "https://arxiv.org/pdf/2504.12316", "abs": "https://arxiv.org/abs/2504.12316", "authors": ["Jingyuan Zhang", "Hongzhi Zhang", "Zhou Haonan", "Chenxi Sun", "Xingguang ji", "Jiakang Wang", "Fanheng Kong", "Yahui Liu", "Qi Wang", "Fuzheng Zhang"], "title": "Data Metabolism: An Efficient Data Design Schema For Vision Language Model", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "To be presented at ICLR 2025, First Workshop on Open Science for\n  Foundation Models", "summary": "Data curation plays a crucial role in training powerful Visual Language\nModels (VLMs). In this work, we introduce the concept of Data Metabolism and\npresent our data-centric framework to build VLMs throughout the development\nlifecycle. Starting from a standard model architecture, we discuss and provide\ninsights into two crucial development steps: data curation and iteration,\nforming a closed-loop system that continuously improves model performance. We\nshow a detailed codebook on how to process existing massive datasets and build\nuser-specific data flywheel. As a demonstration, we release a VLM, named\nCapybara-VL, which excels in typical multimodal tasks (e.g. , visual question\nanswering, scientific reasoning, and text-rich tasks). Despite its relatively\ncompact size, Capybara-VL surpasses several open-source models that are up to\n10 times larger in size. Moreover, it achieves results that are on par with\nthose of several leading proprietary models, demonstrating its remarkable\ncompetitiveness. These results highlight the power of our data-centric\nframework and the potential of training smaller and more efficient VLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u6570\u636e\u4ee3\u8c22\u7684\u6982\u5ff5\u548c\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u6784\u5efa\u9ad8\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578bCapybara-VL\uff0c\u5176\u6027\u80fd\u8d85\u8d8a\u66f4\u5927\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u4e0e\u9886\u5148\u7684\u4e13\u6709\u6a21\u578b\u5ab2\u7f8e\u3002", "motivation": "\u6570\u636e\u6574\u7406\u5728\u8bad\u7ec3\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u4e2d\u8d77\u5230\u5173\u952e\u4f5c\u7528\uff0c\u9700\u8981\u4e00\u79cd\u6301\u7eed\u6539\u8fdb\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u6570\u636e\u4ee3\u8c22\u6982\u5ff5\uff0c\u901a\u8fc7\u6570\u636e\u6574\u7406\u548c\u8fed\u4ee3\u5f62\u6210\u95ed\u73af\u7cfb\u7edf\uff0c\u8be6\u7ec6\u8bf4\u660e\u5982\u4f55\u5904\u7406\u6d77\u91cf\u6570\u636e\u96c6\u5e76\u6784\u5efa\u7528\u6237\u7279\u5b9a\u7684\u6570\u636e\u98de\u8f6e\u3002", "result": "\u53d1\u5e03\u7684Capybara-VL\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u591a\u4e2a\u5927\u5c0f10\u500d\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u4e0e\u9886\u5148\u4e13\u6709\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u6846\u67b6\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\uff0c\u53ef\u8bad\u7ec3\u66f4\u5c0f\u66f4\u9ad8\u6548\u7684VLMs\u3002"}}
{"id": "2504.12395", "pdf": "https://arxiv.org/pdf/2504.12395", "abs": "https://arxiv.org/abs/2504.12395", "authors": ["Jiale Tao", "Yanbing Zhang", "Qixun Wang", "Yiji Cheng", "Haofan Wang", "Xu Bai", "Zhengguang Zhou", "Ruihuang Li", "Linqing Wang", "Chunyu Wang", "Qin Lin", "Qinglin Lu"], "title": "InstantCharacter: Personalize Any Characters with a Scalable Diffusion Transformer Framework", "categories": ["cs.CV"], "comment": "Tech Report. Code is available at\n  https://github.com/Tencent/InstantCharacter", "summary": "Current learning-based subject customization approaches, predominantly\nrelying on U-Net architectures, suffer from limited generalization ability and\ncompromised image quality. Meanwhile, optimization-based methods require\nsubject-specific fine-tuning, which inevitably degrades textual\ncontrollability. To address these challenges, we propose InstantCharacter, a\nscalable framework for character customization built upon a foundation\ndiffusion transformer. InstantCharacter demonstrates three fundamental\nadvantages: first, it achieves open-domain personalization across diverse\ncharacter appearances, poses, and styles while maintaining high-fidelity\nresults. Second, the framework introduces a scalable adapter with stacked\ntransformer encoders, which effectively processes open-domain character\nfeatures and seamlessly interacts with the latent space of modern diffusion\ntransformers. Third, to effectively train the framework, we construct a\nlarge-scale character dataset containing 10-million-level samples. The dataset\nis systematically organized into paired (multi-view character) and unpaired\n(text-image combinations) subsets. This dual-data structure enables\nsimultaneous optimization of identity consistency and textual editability\nthrough distinct learning pathways. Qualitative experiments demonstrate the\nadvanced capabilities of InstantCharacter in generating high-fidelity,\ntext-controllable, and character-consistent images, setting a new benchmark for\ncharacter-driven image generation. Our source code is available at\nhttps://github.com/Tencent/InstantCharacter.", "AI": {"tldr": "InstantCharacter\u662f\u4e00\u4e2a\u57fa\u4e8e\u57fa\u7840\u6269\u6563\u53d8\u6362\u5668\u7684\u5b57\u7b26\u5b9a\u5236\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u5f00\u653e\u9886\u57df\u7684\u4e2a\u6027\u5316\uff0c\u4fdd\u6301\u9ad8\u4fdd\u771f\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4f18\u5316\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u6587\u672c\u53ef\u7f16\u8f91\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5b66\u4e60\u7684\u4e3b\u9898\u5b9a\u5236\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4e14\u56fe\u50cf\u8d28\u91cf\u53d7\u635f\uff0c\u800c\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u9700\u8981\u7279\u5b9a\u4e3b\u9898\u7684\u5fae\u8c03\uff0c\u964d\u4f4e\u4e86\u6587\u672c\u53ef\u63a7\u6027\u3002", "method": "\u4f7f\u7528\u57fa\u7840\u6269\u6563\u53d8\u6362\u5668\u548c\u53ef\u6269\u5c55\u9002\u914d\u5668\u5904\u7406\u5f00\u653e\u9886\u57df\u5b57\u7b26\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5305\u542b10\u767e\u4e07\u6837\u672c\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "InstantCharacter\u80fd\u591f\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u6587\u672c\u53ef\u63a7\u4e14\u5b57\u7b26\u4e00\u81f4\u7684\u56fe\u50cf\uff0c\u5728\u5b57\u7b26\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002", "conclusion": "InstantCharacter\u5728\u5f00\u653e\u9886\u57df\u4e2a\u6027\u5316\u3001\u9ad8\u4fdd\u771f\u7ed3\u679c\u548c\u6587\u672c\u53ef\u63a7\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5b57\u7b26\u5b9a\u5236\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.12446", "pdf": "https://arxiv.org/pdf/2504.12446", "abs": "https://arxiv.org/abs/2504.12446", "authors": ["Sebastian Seidel", "Uwe M. Borghoff"], "title": "Deriving Equivalent Symbol-Based Decision Models from Feedforward Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "15 pages, 19 figures", "summary": "Artificial intelligence (AI) has emerged as a transformative force across\nindustries, driven by advances in deep learning and natural language\nprocessing, and fueled by large-scale data and computing resources. Despite its\nrapid adoption, the opacity of AI systems poses significant challenges to trust\nand acceptance.\n  This work explores the intersection of connectionist and symbolic approaches\nto artificial intelligence, focusing on the derivation of interpretable\nsymbolic models, such as decision trees, from feedforward neural networks\n(FNNs). Decision trees provide a transparent framework for elucidating the\noperations of neural networks while preserving their functionality. The\nderivation is presented in a step-by-step approach and illustrated with several\nexamples. A systematic methodology is proposed to bridge neural and symbolic\nparadigms by exploiting distributed representations in FNNs to identify\nsymbolic components, including fillers, roles, and their interrelationships.\nThe process traces neuron activation values and input configurations across\nnetwork layers, mapping activations and their underlying inputs to decision\ntree edges. The resulting symbolic structures effectively capture FNN decision\nprocesses and enable scalability to deeper networks through iterative\nrefinement of subpaths for each hidden layer.\n  To validate the theoretical framework, a prototype was developed using Keras\n.h5-data and emulating TensorFlow within the Java JDK/JavaFX environment. This\nprototype demonstrates the feasibility of extracting symbolic representations\nfrom neural networks, enhancing trust in AI systems, and promoting\naccountability.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2504.12497", "pdf": "https://arxiv.org/pdf/2504.12497", "abs": "https://arxiv.org/abs/2504.12497", "authors": ["Robert E. Wray", "Steven J. Jones", "John E. Laird"], "title": "Heuristic Recognition and Rapid Response to Unfamiliar Events Outside of Agent Design Scope", "categories": ["cs.AI", "I.2.8"], "comment": "12 pages, 3 figures. Submitted to AGI25 conference", "summary": "Regardless of past learning, an agent in an open world will face unfamiliar\nsituations and events outside of prior experience, existing models, or\npolicies. Further, the agent will sometimes lack relevant knowledge and/or\nsufficient time to assess the situation, generate and evaluate options, and\npursue a robustly considered course of action. How can an agent respond\nreasonably to situations that are outside of its original design scope? How can\nit recognize such situations sufficiently quickly and reliably to determine\nreasonable, adaptive courses of action? We identify key characteristics needed\nfor solutions, evaluate the state-of-the-art by these requirements, and outline\na proposed, novel approach that combines domain-general meta-knowledge (in the\nform of appraisals inspired by human cognition) and metareasoning. It has the\npotential to provide fast, adaptive responses to unfamiliar situations, more\nfully meeting the performance characteristics required for open-world, general\nagents.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5143\u77e5\u8bc6\u548c\u5143\u63a8\u7406\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u667a\u80fd\u4f53\u5728\u5f00\u653e\u5f0f\u73af\u5883\u4e2d\u5feb\u901f\u9002\u5e94\u964c\u751f\u60c5\u51b5\u3002", "motivation": "\u667a\u80fd\u4f53\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u5e38\u9047\u5230\u8d85\u51fa\u5176\u539f\u6709\u8bbe\u8ba1\u8303\u56f4\u7684\u964c\u751f\u60c5\u51b5\uff0c\u4e14\u7f3a\u4e4f\u8db3\u591f\u65f6\u95f4\u548c\u77e5\u8bc6\u8fdb\u884c\u8be6\u7ec6\u8bc4\u4f30\uff0c\u9700\u8981\u5feb\u901f\u53ef\u9760\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u9886\u57df\u901a\u7528\u7684\u5143\u77e5\u8bc6\uff08\u53d7\u4eba\u7c7b\u8ba4\u77e5\u542f\u53d1\u7684\u8bc4\u4f30\uff09\u548c\u5143\u63a8\u7406\u7684\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6f5c\u529b\u4e3a\u5f00\u653e\u5f0f\u73af\u5883\u4e2d\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u5feb\u901f\u3001\u81ea\u9002\u5e94\u7684\u54cd\u5e94\uff0c\u6ee1\u8db3\u5176\u6027\u80fd\u9700\u6c42\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u6ee1\u8db3\u5f00\u653e\u4e16\u754c\u4e2d\u901a\u7528\u667a\u80fd\u4f53\u7684\u6027\u80fd\u8981\u6c42\uff0c\u63d0\u4f9b\u5bf9\u964c\u751f\u60c5\u51b5\u7684\u9002\u5e94\u6027\u54cd\u5e94\u3002"}}
{"id": "2504.12317", "pdf": "https://arxiv.org/pdf/2504.12317", "abs": "https://arxiv.org/abs/2504.12317", "authors": ["Dingkang Lin", "Naixuan Zhao", "Dan Tian", "Jiang Li"], "title": "ChatGPT as Linguistic Equalizer? Quantifying LLM-Driven Lexical Shifts in Academic Writing", "categories": ["cs.CL"], "comment": "13 pages, 2 figures", "summary": "The advent of ChatGPT has profoundly reshaped scientific research practices,\nparticularly in academic writing, where non-native English-speakers (NNES)\nhistorically face linguistic barriers. This study investigates whether ChatGPT\nmitigates these barriers and fosters equity by analyzing lexical complexity\nshifts across 2.8 million articles from OpenAlex (2020-2024). Using the Measure\nof Textual Lexical Diversity (MTLD) to quantify vocabulary sophistication and a\ndifference-in-differences (DID) design to identify causal effects, we\ndemonstrate that ChatGPT significantly enhances lexical complexity in\nNNES-authored abstracts, even after controlling for article-level controls,\nauthorship patterns, and venue norms. Notably, the impact is most pronounced in\npreprint papers, technology- and biology-related fields and lower-tier\njournals. These findings provide causal evidence that ChatGPT reduces\nlinguistic disparities and promotes equity in global academia.", "AI": {"tldr": "\u7814\u7a76\u8868\u660eChatGPT\u80fd\u663e\u8457\u63d0\u5347\u975e\u82f1\u8bed\u6bcd\u8bed\u5b66\u8005(NNES)\u8bba\u6587\u6458\u8981\u7684\u8bcd\u6c47\u590d\u6742\u5ea6\uff0c\u5c24\u5176\u5728\u9884\u5370\u672c\u3001\u6280\u672f\u3001\u751f\u7269\u9886\u57df\u53ca\u4f4e\u5f71\u54cd\u56e0\u5b50\u671f\u520a\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u63a2\u8ba8ChatGPT\u662f\u5426\u51cf\u8f7bNNES\u5b66\u8005\u5728\u5b66\u672f\u5199\u4f5c\u4e2d\u7684\u8bed\u8a00\u969c\u788d\uff0c\u4fc3\u8fdb\u5b66\u672f\u516c\u5e73\u3002", "method": "\u4f7f\u7528MTLD\u6307\u6807\u91cf\u5316\u8bcd\u6c47\u590d\u6742\u5ea6\uff0c\u5e76\u91c7\u7528\u53cc\u91cd\u5dee\u5206\u6cd5(DID)\u5206\u6790OpenAlex\u6570\u636e\u5e93\u4e2d280\u4e07\u7bc7\u8bba\u6587(2020-2024)\u7684\u56e0\u679c\u5173\u7cfb\u3002", "result": "ChatGPT\u663e\u8457\u63d0\u5347NNES\u4f5c\u8005\u6458\u8981\u7684\u8bcd\u6c47\u590d\u6742\u5ea6\uff08\u5c24\u5176\u5728\u9884\u5370\u672c\u3001\u6280\u672f/\u751f\u7269\u5b66\u9886\u57df\u53ca\u4f4e\u5f71\u54cd\u529b\u671f\u520a\uff09\uff0c\u4e14\u7ed3\u679c\u5177\u6709\u56e0\u679c\u6548\u5e94\u3002", "conclusion": "ChatGPT\u6709\u6548\u51cf\u5c11\u8bed\u8a00\u5b66\u5dee\u8ddd\uff0c\u63a8\u52a8\u5168\u7403\u5b66\u672f\u754c\u516c\u5e73\u6027\u3002"}}
{"id": "2504.12401", "pdf": "https://arxiv.org/pdf/2504.12401", "abs": "https://arxiv.org/abs/2504.12401", "authors": ["Lei Sun", "Andrea Alfarano", "Peiqi Duan", "Shaolin Su", "Kaiwei Wang", "Boxin Shi", "Radu Timofte", "Danda Pani Paudel", "Luc Van Gool", "Qinglin Liu", "Wei Yu", "Xiaoqian Lv", "Lu Yang", "Shuigen Wang", "Shengping Zhang", "Xiangyang Ji", "Long Bao", "Yuqiang Yang", "Jinao Song", "Ziyi Wang", "Shuang Wen", "Heng Sun", "Kean Liu", "Mingchen Zhong", "Senyan Xu", "Zhijing Sun", "Jiaying Zhu", "Chengjie Ge", "Xingbo Wang", "Yidi Liu", "Xin Lu", "Xueyang Fu", "Zheng-Jun Zha", "Dawei Fan", "Dafeng Zhang", "Yong Yang", "Siru Zhang", "Qinghua Yang", "Hao Kang", "Huiyuan Fu", "Heng Zhang", "Hongyuan Yu", "Zhijuan Huang", "Shuoyan Wei", "Feng Li", "Runmin Cong", "Weiqi Luo", "Mingyun Lin", "Chenxu Jiang", "Hongyi Liu", "Lei Yu", "Weilun Li", "Jiajun Zhai", "Tingting Lin", "Shuang Ma", "Sai Zhou", "Zhanwen Liu", "Yang Wang", "Eiffel Chong", "Nuwan Bandara", "Thivya Kandappu", "Archan Misra", "Yihang Chen", "Zhan Li", "Weijun Yuan", "Wenzhuo Wang", "Boyang Yao", "Zhanglu Chen", "Yijing Sun", "Tianjiao Wan", "Zijian Gao", "Qisheng Xu", "Kele Xu", "Yukun Zhang", "Yu He", "Xiaoyan Xie", "Tao Fu", "Yashu Gautamkumar Patel", "Vihar Ramesh Jain", "Divesh Basina", "Rishik Ashili", "Manish Kumar Manjhi", "Sourav Kumar", "Prinon Benny", "Himanshu Ghunawat", "B Sri Sairam Gautam", "Anett Varghese", "Abhishek Yadav"], "title": "NTIRE 2025 Challenge on Event-Based Image Deblurring: Methods and Results", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents an overview of NTIRE 2025 the First Challenge on\nEvent-Based Image Deblurring, detailing the proposed methodologies and\ncorresponding results. The primary goal of the challenge is to design an\nevent-based method that achieves high-quality image deblurring, with\nperformance quantitatively assessed using Peak Signal-to-Noise Ratio (PSNR).\nNotably, there are no restrictions on computational complexity or model size.\nThe task focuses on leveraging both events and images as inputs for\nsingle-image deblurring. A total of 199 participants registered, among whom 15\nteams successfully submitted valid results, offering valuable insights into the\ncurrent state of event-based image deblurring. We anticipate that this\nchallenge will drive further advancements in event-based vision research.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86NTIRE 2025\u7b2c\u4e00\u5c4a\u57fa\u4e8e\u4e8b\u4ef6\u7684\u56fe\u50cf\u53bb\u6a21\u7cca\u6311\u6218\u8d5b\u7684\u6982\u8ff0\u3001\u65b9\u6cd5\u548c\u7ed3\u679c\u3002", "motivation": "\u6311\u6218\u8d5b\u7684\u4e3b\u8981\u76ee\u6807\u662f\u8bbe\u8ba1\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u53bb\u6a21\u7cca\u3002", "method": "\u5229\u7528\u4e8b\u4ef6\u548c\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\u8fdb\u884c\u5355\u56fe\u50cf\u53bb\u6a21\u7cca\uff0c\u6027\u80fd\u901a\u8fc7\u5cf0\u503c\u4fe1\u566a\u6bd4\uff08PSNR\uff09\u5b9a\u91cf\u8bc4\u4f30\u3002", "result": "199\u540d\u53c2\u4e0e\u8005\u6ce8\u518c\uff0c15\u652f\u56e2\u961f\u6210\u529f\u63d0\u4ea4\u4e86\u6709\u6548\u7ed3\u679c\uff0c\u4e3a\u5f53\u524d\u57fa\u4e8e\u4e8b\u4ef6\u7684\u56fe\u50cf\u53bb\u6a21\u7cca\u6280\u672f\u63d0\u4f9b\u4e86\u5b9d\u8d35\u89c1\u89e3\u3002", "conclusion": "\u9884\u8ba1\u6b64\u6b21\u6311\u6218\u8d5b\u5c06\u63a8\u52a8\u57fa\u4e8e\u4e8b\u4ef6\u7684\u89c6\u89c9\u7814\u7a76\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2504.12450", "pdf": "https://arxiv.org/pdf/2504.12450", "abs": "https://arxiv.org/abs/2504.12450", "authors": ["Ziqi Li", "Zhan Peng"], "title": "Can Moran Eigenvectors Improve Machine Learning of Spatial Data? Insights from Synthetic Data Validation", "categories": ["cs.LG", "econ.EM", "stat.ML"], "comment": null, "summary": "Moran Eigenvector Spatial Filtering (ESF) approaches have shown promise in\naccounting for spatial effects in statistical models. Can this extend to\nmachine learning? This paper examines the effectiveness of using Moran\nEigenvectors as additional spatial features in machine learning models. We\ngenerate synthetic datasets with known processes involving spatially varying\nand nonlinear effects across two different geometries. Moran Eigenvectors\ncalculated from different spatial weights matrices, with and without a priori\neigenvector selection, are tested. We assess the performance of popular machine\nlearning models, including Random Forests, LightGBM, XGBoost, and TabNet, and\nbenchmark their accuracies in terms of cross-validated R2 values against models\nthat use only coordinates as features. We also extract coefficients and\nfunctions from the models using GeoShapley and compare them with the true\nprocesses. Results show that machine learning models using only location\ncoordinates achieve better accuracies than eigenvector-based approaches across\nvarious experiments and datasets. Furthermore, we discuss that while these\nfindings are relevant for spatial processes that exhibit positive spatial\nautocorrelation, they do not necessarily apply when modeling network\nautocorrelation and cases with negative spatial autocorrelation, where Moran\nEigenvectors would still be useful.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86Moran\u7279\u5f81\u5411\u91cf\u7a7a\u95f4\u8fc7\u6ee4\uff08ESF\uff09\u65b9\u6cd5\u5728\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u4f5c\u4e3a\u7a7a\u95f4\u7279\u5f81\u7684\u9002\u7528\u6027\uff0c\u53d1\u73b0\u4ec5\u4f7f\u7528\u5750\u6807\u7684\u6a21\u578b\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u63a2\u7d22Moran\u7279\u5f81\u5411\u91cf\u80fd\u5426\u6709\u6548\u4f5c\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u7a7a\u95f4\u7279\u5f81\uff0c\u4ee5\u63d0\u5347\u5904\u7406\u7a7a\u95f4\u6548\u5e94\u7684\u80fd\u529b\u3002", "method": "\u751f\u6210\u5305\u542b\u7a7a\u95f4\u53d8\u5316\u548c\u975e\u7ebf\u6027\u6548\u5e94\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4f7f\u7528Moran\u7279\u5f81\u5411\u91cf\uff08\u6765\u81ea\u4e0d\u540c\u7a7a\u95f4\u6743\u91cd\u77e9\u9635\uff09\u548c\u4ec5\u4f7f\u7528\u5750\u6807\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982Random Forests\u3001LightGBM\u7b49\uff09\u7684\u6027\u80fd\u3002", "result": "\u4ec5\u4f7f\u7528\u5750\u6807\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u591a\u6570\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u7279\u5f81\u5411\u91cf\u7684\u65b9\u6cd5\u3002\u4f46\u5728\u8d1f\u7a7a\u95f4\u81ea\u76f8\u5173\u6216\u7f51\u7edc\u81ea\u76f8\u5173\u573a\u666f\u4e2d\uff0cMoran\u7279\u5f81\u5411\u91cf\u53ef\u80fd\u4ecd\u6709\u4ef7\u503c\u3002", "conclusion": "Moran\u7279\u5f81\u5411\u91cf\u5728\u6b63\u7a7a\u95f4\u81ea\u76f8\u5173\u60c5\u666f\u4e2d\u63d0\u5347\u6709\u9650\uff0c\u4f46\u5728\u7279\u5b9a\u60c5\u51b5\uff08\u5982\u8d1f\u81ea\u76f8\u5173\uff09\u4ecd\u6709\u6f5c\u529b\u3002"}}
{"id": "2504.12529", "pdf": "https://arxiv.org/pdf/2504.12529", "abs": "https://arxiv.org/abs/2504.12529", "authors": ["Zahra Atf", "Peter R. Lewis"], "title": "Is Trust Correlated With Explainability in AI? A Meta-Analysis", "categories": ["cs.AI", "cs.CY"], "comment": "9 Page, 1 Figure", "summary": "This study critically examines the commonly held assumption that\nexplicability in artificial intelligence (AI) systems inherently boosts user\ntrust. Utilizing a meta-analytical approach, we conducted a comprehensive\nexamination of the existing literature to explore the relationship between AI\nexplainability and trust. Our analysis, incorporating data from 90 studies,\nreveals a statistically significant but moderate positive correlation between\nthe explainability of AI systems and the trust they engender among users. This\nindicates that while explainability contributes to building trust, it is not\nthe sole or predominant factor in this equation. In addition to academic\ncontributions to the field of Explainable AI (XAI), this research highlights\nits broader socio-technical implications, particularly in promoting\naccountability and fostering user trust in critical domains such as healthcare\nand justice. By addressing challenges like algorithmic bias and ethical\ntransparency, the study underscores the need for equitable and sustainable AI\nadoption. Rather than focusing solely on immediate trust, we emphasize the\nnormative importance of fostering authentic and enduring trustworthiness in AI\nsystems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u867d\u80fd\u63d0\u5347\u7528\u6237\u4fe1\u4efb\uff0c\u4f46\u5e76\u975e\u552f\u4e00\u6216\u4e3b\u5bfc\u56e0\u7d20\u3002", "motivation": "\u63a2\u8ba8AI\u53ef\u89e3\u91ca\u6027\u4e0e\u7528\u6237\u4fe1\u4efb\u95f4\u7684\u5173\u7cfb\uff0c\u6311\u6218\u2018\u53ef\u89e3\u91ca\u6027\u5fc5\u7136\u589e\u5f3a\u4fe1\u4efb\u2019\u7684\u666e\u904d\u5047\u8bbe\u3002", "method": "\u91c7\u7528\u5143\u5206\u6790\u65b9\u6cd5\uff0c\u7efc\u5408\u5206\u6790\u4e8690\u9879\u73b0\u6709\u7814\u7a76\u7684\u6570\u636e\u3002", "result": "AI\u53ef\u89e3\u91ca\u6027\u4e0e\u7528\u6237\u4fe1\u4efb\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u4f46\u4e2d\u7b49\u7684\u6b63\u76f8\u5173\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u5728\u5173\u952e\u9886\u57df\uff08\u5982\u533b\u7597\u548c\u53f8\u6cd5\uff09\u9700\u8981\u4fc3\u8fdbAI\u7684\u95ee\u8d23\u5236\u548c\u6301\u4e45\u53ef\u4fe1\u5ea6\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u5373\u65f6\u4fe1\u4efb\u3002"}}
{"id": "2504.12320", "pdf": "https://arxiv.org/pdf/2504.12320", "abs": "https://arxiv.org/abs/2504.12320", "authors": ["Jennifer Haase", "Paul H. P. Hanel", "Sebastian Pokutta"], "title": "Has the Creativity of Large-Language Models peaked? An analysis of inter- and intra-LLM variability", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "19 pages + Appendix, 13 figure", "summary": "Following the widespread adoption of ChatGPT in early 2023, numerous studies\nreported that large language models (LLMs) can match or even surpass human\nperformance in creative tasks. However, it remains unclear whether LLMs have\nbecome more creative over time, and how consistent their creative output is. In\nthis study, we evaluated 14 widely used LLMs -- including GPT-4, Claude, Llama,\nGrok, Mistral, and DeepSeek -- across two validated creativity assessments: the\nDivergent Association Task (DAT) and the Alternative Uses Task (AUT). Contrary\nto expectations, we found no evidence of increased creative performance over\nthe past 18-24 months, with GPT-4 performing worse than in previous studies.\nFor the more widely used AUT, all models performed on average better than the\naverage human, with GPT-4o and o3-mini performing best. However, only 0.28% of\nLLM-generated responses reached the top 10% of human creativity benchmarks.\nBeyond inter-model differences, we document substantial intra-model\nvariability: the same LLM, given the same prompt, can produce outputs ranging\nfrom below-average to original. This variability has important implications for\nboth creativity research and practical applications. Ignoring such variability\nrisks misjudging the creative potential of LLMs, either inflating or\nunderestimating their capabilities. The choice of prompts affected LLMs\ndifferently. Our findings underscore the need for more nuanced evaluation\nframeworks and highlight the importance of model selection, prompt design, and\nrepeated assessment when using Generative AI (GenAI) tools in creative\ncontexts.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u521b\u9020\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\uff0c\u4f46\u5176\u521b\u9020\u6027\u968f\u65f6\u95f4\u5e76\u672a\u63d0\u5347\uff0c\u4e14\u5b58\u5728\u663e\u8457\u7684\u6a21\u578b\u5185\u90e8\u53d8\u5f02\u6027\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u521b\u9020\u6027\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u53ca\u5176\u968f\u65f6\u95f4\u7684\u53d8\u5316\uff0c\u4ee5\u53ca\u6a21\u578b\u5185\u90e8\u8f93\u51fa\u7684\u53d8\u5f02\u6027\u3002", "method": "\u4f7f\u7528\u53d1\u6563\u8054\u60f3\u4efb\u52a1\uff08DAT\uff09\u548c\u66ff\u4ee3\u7528\u9014\u4efb\u52a1\uff08AUT\uff09\u8bc4\u4f3014\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684LLMs\u3002", "result": "LLMs\u5728AUT\u4e2d\u7684\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\uff0c\u4f46\u4ec5\u67090.28%\u7684\u8f93\u51fa\u8fbe\u5230\u4eba\u7c7b\u521b\u9020\u6027\u7684\u524d10%\u3002\u6a21\u578b\u4e2d\u5b58\u5728\u663e\u8457\u7684\u5185\u90e8\u53d8\u5f02\u6027\uff0c\u4e14GPT-4\u8868\u73b0\u8f83\u5148\u524d\u7814\u7a76\u4e0b\u964d\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u66f4\u7ec6\u81f4\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u5728\u9009\u62e9\u6a21\u578b\u3001\u8bbe\u8ba1\u63d0\u793a\u548c\u91cd\u590d\u8bc4\u4f30\u65f6\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u5728\u521b\u9020\u6027\u5e94\u7528\u4e2d\u51c6\u786e\u8bc4\u4f30LLMs\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.12436", "pdf": "https://arxiv.org/pdf/2504.12436", "abs": "https://arxiv.org/abs/2504.12436", "authors": ["Nairouz Mrabah", "Nicolas Richet", "Ismail Ben Ayed", "\u00c9ric Granger"], "title": "Sparsity Outperforms Low-Rank Projections in Few-Shot Adaptation", "categories": ["cs.CV", "cs.AI", "I.4.8; I.5.1; G.1.6"], "comment": "Under review", "summary": "Adapting Vision-Language Models (VLMs) to new domains with few labeled\nsamples remains a significant challenge due to severe overfitting and\ncomputational constraints. State-of-the-art solutions, such as low-rank\nreparameterization, mitigate these issues but often struggle with\ngeneralization and require extensive hyperparameter tuning. In this paper, a\nnovel Sparse Optimization (SO) framework is proposed. Unlike low-rank\napproaches that typically constrain updates to a fixed subspace, our SO method\nleverages high sparsity to dynamically adjust very few parameters. We introduce\ntwo key paradigms. First, we advocate for \\textit{local sparsity and global\ndensity}, which updates a minimal subset of parameters per iteration while\nmaintaining overall model expressiveness. As a second paradigm, we advocate for\n\\textit{local randomness and global importance}, which sparsifies the gradient\nusing random selection while pruning the first moment based on importance. This\ncombination significantly mitigates overfitting and ensures stable adaptation\nin low-data regimes. Extensive experiments on 11 diverse datasets show that SO\nachieves state-of-the-art few-shot adaptation performance while reducing memory\noverhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7a00\u758f\u4f18\u5316\u6846\u67b6\uff08SO\uff09\uff0c\u7528\u4e8e\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u9002\u5e94\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5230\u65b0\u9886\u57df\uff0c\u6709\u6548\u51cf\u5c11\u8fc7\u62df\u5408\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982\u4f4e\u79e9\u91cd\u53c2\u6570\u5316\u5728\u5904\u7406VLM\u7684\u5c11\u6837\u672c\u9002\u5e94\u65f6\u5b58\u5728\u8fc7\u62df\u5408\u548c\u8ba1\u7b97\u9650\u5236\u95ee\u9898\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u5927\u91cf\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "method": "\u91c7\u7528\u7a00\u758f\u4f18\u5316\uff08SO\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u5c40\u90e8\u7a00\u758f\u5168\u5c40\u5bc6\u5ea6\uff08\u66f4\u65b0\u5c11\u91cf\u53c2\u6570\uff09\u548c\u5c40\u90e8\u968f\u673a\u5168\u5c40\u91cd\u8981\uff08\u968f\u673a\u9009\u62e9\u68af\u5ea6\u5e76\u57fa\u4e8e\u91cd\u8981\u6027\u526a\u679d\uff09\u4e24\u79cd\u7b56\u7565\u3002", "result": "\u572811\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSO\u5728\u5c11\u6837\u672c\u9002\u5e94\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5185\u5b58\u5f00\u9500\u3002", "conclusion": "SO\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5c11\u91cf\u53c2\u6570\u548c\u7a00\u758f\u5316\u68af\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8fc7\u62df\u5408\uff0c\u5728\u4f4e\u6570\u636e\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u6a21\u578b\u9002\u5e94\u3002"}}
{"id": "2504.12458", "pdf": "https://arxiv.org/pdf/2504.12458", "abs": "https://arxiv.org/abs/2504.12458", "authors": ["Jansen S. B. Pereira", "Giovani Valdrighi", "Marcos Medeiros Raimundo"], "title": "M$^2$FGB: A Min-Max Gradient Boosting Framework for Subgroup Fairness", "categories": ["cs.LG"], "comment": "17 pages, 7 figures", "summary": "In recent years, fairness in machine learning has emerged as a critical\nconcern to ensure that developed and deployed predictive models do not have\ndisadvantageous predictions for marginalized groups. It is essential to\nmitigate discrimination against individuals based on protected attributes such\nas gender and race. In this work, we consider applying subgroup justice\nconcepts to gradient-boosting machines designed for supervised learning\nproblems. Our approach expanded gradient-boosting methodologies to explore a\nbroader range of objective functions, which combines conventional losses such\nas the ones from classification and regression and a min-max fairness term. We\nstudy relevant theoretical properties of the solution of the min-max\noptimization problem. The optimization process explored the primal-dual\nproblems at each boosting round. This generic framework can be adapted to\ndiverse fairness concepts. The proposed min-max primal-dual gradient boosting\nalgorithm was theoretically shown to converge under mild conditions and\nempirically shown to be a powerful and flexible approach to address binary and\nsubgroup fairness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6700\u5c0f\u6700\u5927\u516c\u5e73\u6027\u9879\u548c\u4f20\u7edf\u635f\u5931\u51fd\u6570\u7684\u68af\u5ea6\u63d0\u5347\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002", "motivation": "\u786e\u4fdd\u9884\u6d4b\u6a21\u578b\u4e0d\u4f1a\u5bf9\u8fb9\u7f18\u5316\u7fa4\u4f53\u4ea7\u751f\u4e0d\u5229\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u6027\u522b\u548c\u79cd\u65cf\u7b49\u53d7\u4fdd\u62a4\u5c5e\u6027\u65f6\u3002", "method": "\u6269\u5c55\u4e86\u68af\u5ea6\u63d0\u5347\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u5e38\u89c4\u5206\u7c7b\u548c\u56de\u5f52\u635f\u5931\u51fd\u6570\u4e0e\u6700\u5c0f\u6700\u5927\u516c\u5e73\u6027\u9879\uff0c\u5e76\u901a\u8fc7\u539f\u59cb-\u5bf9\u5076\u95ee\u9898\u4f18\u5316\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u7406\u8bba\u4e0a\u80fd\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\u6536\u655b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u793a\u51fa\u5904\u7406\u4e8c\u5143\u548c\u5b50\u7fa4\u516c\u5e73\u95ee\u9898\u7684\u5f3a\u5927\u7075\u6d3b\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u7075\u6d3b\u7684\u6846\u67b6\u3002"}}
{"id": "2504.12562", "pdf": "https://arxiv.org/pdf/2504.12562", "abs": "https://arxiv.org/abs/2504.12562", "authors": ["Haidar Khan", "Hisham A. Alyahya", "Yazeed Alnumay", "M Saiful Bari", "B\u00fclent Yener"], "title": "ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Evaluating the capabilities of Large Language Models (LLMs) has traditionally\nrelied on static benchmark datasets, human assessments, or model-based\nevaluations - methods that often suffer from overfitting, high costs, and\nbiases. ZeroSumEval is a novel competition-based evaluation protocol that\nleverages zero-sum games to assess LLMs with dynamic benchmarks that resist\nsaturation. ZeroSumEval encompasses a diverse suite of games, including\nsecurity challenges (PyJail), classic games (Chess, Liar's Dice, Poker),\nknowledge tests (MathQuiz), and persuasion challenges (Gandalf, Debate). These\ngames are designed to evaluate a range of AI capabilities such as strategic\nreasoning, planning, knowledge application, and creativity. Building upon\nrecent studies that highlight the effectiveness of game-based evaluations for\nLLMs, ZeroSumEval enhances these approaches by providing a standardized and\nextensible framework. To demonstrate this, we conduct extensive experiments\nwith >7000 simulations across 7 games and 13 models. Our results show that\nwhile frontier models from the GPT and Claude families can play common games\nand answer questions, they struggle to play games that require creating novel\nand challenging questions. We also observe that models cannot reliably\njailbreak each other and fail generally at tasks requiring creativity. We\nrelease our code at https://github.com/facebookresearch/ZeroSumEval.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ade\u4e89\u6027\u8bc4\u4f30\u534f\u8baeZeroSumEval\uff0c\u901a\u8fc7\u96f6\u548c\u6e38\u620f\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u8fc7\u62df\u5408\u3001\u9ad8\u6210\u672c\u548c\u504f\u89c1\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u52a8\u6001\u7684\u8bc4\u4f30\u65b9\u5f0f\u3002", "method": "ZeroSumEval\u91c7\u7528\u96f6\u548c\u6e38\u620f\u8bc4\u4f30\uff0c\u8bbe\u8ba1\u4e86\u5305\u62ec\u5b89\u5168\u6311\u6218\u3001\u7ecf\u5178\u6e38\u620f\u3001\u77e5\u8bc6\u6d4b\u8bd5\u548c\u529d\u8bf4\u6311\u6218\u5728\u5185\u7684\u4e00\u5957\u591a\u6837\u5316\u6e38\u620f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u867d\u7136\u524d\u6cbf\u6a21\u578b\u80fd\u5728\u5e38\u89c1\u6e38\u620f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u521b\u9020\u65b0\u95ee\u9898\u7684\u6e38\u620f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u666e\u904d\u5728\u521b\u9020\u6027\u4efb\u52a1\u4e0a\u5931\u8d25\u3002", "conclusion": "ZeroSumEval\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u548c\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u6539\u8fdb\u4e86\u57fa\u4e8e\u6e38\u620f\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2504.12321", "pdf": "https://arxiv.org/pdf/2504.12321", "abs": "https://arxiv.org/abs/2504.12321", "authors": ["Charlotte Siska", "Anush Sankaran"], "title": "AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the past few years, Language Models (LMs) have shown par-human\ncapabilities in several domains. Despite their practical applications and\nexceeding user consumption, they are susceptible to jailbreaks when malicious\ninput exploits the LM's weaknesses, causing it to deviate from its intended\nbehavior. Current defensive strategies either classify the input prompt as\nadversarial or prevent LMs from generating harmful outputs. However, it is\nchallenging to explain the reason behind the malicious nature of the jailbreak,\nwhich results in a wide variety of closed-box approaches. In this research, we\npropose and demonstrate that system-prompt attention from Small Language Models\n(SLMs) can be used to characterize adversarial prompts, providing a novel,\nexplainable, and cheaper defense approach called AttentionDefense. Our research\nsuggests that the attention mechanism is an integral component in understanding\nand explaining how LMs respond to malicious input that is not captured in the\nsemantic meaning of text embeddings. The proposed AttentionDefense is evaluated\nagainst existing jailbreak benchmark datasets. Ablation studies show that\nSLM-based AttentionDefense has equivalent or better jailbreak detection\nperformance compared to text embedding-based classifiers and GPT-4 zero-shot\ndetectors.To further validate the efficacy of the proposed approach, we\ngenerate a dataset of novel jailbreak variants of the existing benchmark\ndataset using a closed-loop LLM-based multi-agent system. We demonstrate that\nthe proposed AttentionDefense approach performs robustly on this novel\njailbreak dataset while existing approaches suffer in performance.\nAdditionally, for practical purposes AttentionDefense is an ideal solution as\nit has the computation requirements of a small LM but the performance of a LLM\ndetector.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2504.12442", "pdf": "https://arxiv.org/pdf/2504.12442", "abs": "https://arxiv.org/abs/2504.12442", "authors": ["Minmin Yang", "Huantao Ren", "Senem Velipasalar"], "title": "3D-PointZshotS: Geometry-Aware 3D Point Cloud Zero-Shot Semantic Segmentation Narrowing the Visual-Semantic Gap", "categories": ["cs.CV"], "comment": null, "summary": "Existing zero-shot 3D point cloud segmentation methods often struggle with\nlimited transferability from seen classes to unseen classes and from semantic\nto visual space. To alleviate this, we introduce 3D-PointZshotS, a\ngeometry-aware zero-shot segmentation framework that enhances both feature\ngeneration and alignment using latent geometric prototypes (LGPs).\nSpecifically, we integrate LGPs into a generator via a cross-attention\nmechanism, enriching semantic features with fine-grained geometric details. To\nfurther enhance stability and generalization, we introduce a self-consistency\nloss, which enforces feature robustness against point-wise perturbations.\nAdditionally, we re-represent visual and semantic features in a shared space,\nbridging the semantic-visual gap and facilitating knowledge transfer to unseen\nclasses. Experiments on three real-world datasets, namely ScanNet,\nSemanticKITTI, and S3DIS, demonstrate that our method achieves superior\nperformance over four baselines in terms of harmonic mIoU. The code is\navailable at \\href{https://github.com/LexieYang/3D-PointZshotS}{Github}.", "AI": {"tldr": "\u63d0\u51fa3D-PointZshotS\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u539f\u578b\u548c\u81ea\u4e00\u81f4\u6027\u635f\u5931\u63d0\u5347\u96f6\u6837\u672c3D\u70b9\u4e91\u5206\u5272\u7684\u8fc1\u79fb\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c3D\u70b9\u4e91\u5206\u5272\u65b9\u6cd5\u5728\u4ece\u5df2\u77e5\u7c7b\u5230\u672a\u77e5\u7c7b\u3001\u4ece\u8bed\u4e49\u7a7a\u95f4\u5230\u89c6\u89c9\u7a7a\u95f4\u7684\u8fc1\u79fb\u6027\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u51e0\u4f55\u539f\u578b\uff08LGPs\uff09\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u7279\u5f81\u751f\u6210\u548c\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u81ea\u4e00\u81f4\u6027\u635f\u5931\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5728\u5171\u4eab\u7a7a\u95f4\u4e2d\u91cd\u65b0\u8868\u793a\u89c6\u89c9\u548c\u8bed\u4e49\u7279\u5f81\u3002", "result": "\u5728ScanNet\u3001SemanticKITTI\u548cS3DIS\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u8c10\u6ce2mIoU\u6307\u6807\u4e0a\u4f18\u4e8e\u56db\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "3D-PointZshotS\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u548c\u7279\u5f81\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c3D\u70b9\u4e91\u5206\u5272\u7684\u6027\u80fd\u548c\u8fc1\u79fb\u6027\u3002"}}
{"id": "2504.12463", "pdf": "https://arxiv.org/pdf/2504.12463", "abs": "https://arxiv.org/abs/2504.12463", "authors": ["Ashwinee Panda", "Vatsal Baherwani", "Zain Sarwar", "Benjamin Therien", "Supriyo Chakraborty", "Tom Goldstein"], "title": "Dense Backpropagation Improves Training for Sparse Mixture-of-Experts", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Mixture of Experts (MoE) pretraining is more scalable than dense Transformer\npretraining, because MoEs learn to route inputs to a sparse set of their\nfeedforward parameters. However, this means that MoEs only receive a sparse\nbackward update, leading to training instability and suboptimal performance. We\npresent a lightweight approximation method that gives the MoE router a dense\ngradient update while continuing to sparsely activate its parameters. Our\nmethod, which we refer to as Default MoE, substitutes missing expert\nactivations with default outputs consisting of an exponential moving average of\nexpert outputs previously seen over the course of training. This allows the\nrouter to receive signals from every expert for each token, leading to\nsignificant improvements in training performance. Our Default MoE outperforms\nstandard TopK routing in a variety of settings without requiring significant\ncomputational overhead. Code: https://github.com/vatsal0/default-moe.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDefault MoE\u7684\u8f7b\u91cf\u7ea7\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f7f\u7528\u4e13\u5bb6\u8f93\u51fa\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747\u4f5c\u4e3a\u9ed8\u8ba4\u8f93\u51fa\u6765\u6539\u5584MoE\u8def\u7531\u5668\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "MoE\u9884\u8bad\u7ec3\u867d\u7136\u6bd4\u5bc6\u96c6Transformer\u66f4\u5177\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u7531\u4e8e\u7a00\u758f\u7684\u5411\u540e\u66f4\u65b0\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6027\u80fd\u4e0d\u4f73\u3002", "method": "Default MoE\u65b9\u6cd5\u7528\u4e13\u5bb6\u8f93\u51fa\u7684\u6307\u6570\u79fb\u52a8\u5e73\u5747\u66ff\u4ee3\u7f3a\u5931\u7684\u4e13\u5bb6\u6fc0\u6d3b\uff0c\u4f7f\u8def\u7531\u5668\u80fd\u4ece\u6bcf\u4e2a\u4e13\u5bb6\u63a5\u6536\u4fe1\u53f7\u3002", "result": "Default MoE\u5728\u591a\u79cd\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u6807\u51c6TopK\u8def\u7531\uff0c\u4e14\u65e0\u9700\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "Default MoE\u901a\u8fc7\u5bc6\u96c6\u68af\u5ea6\u66f4\u65b0\u663e\u8457\u63d0\u9ad8\u4e86MoE\u7684\u8bad\u7ec3\u6027\u80fd\u3002"}}
{"id": "2504.12612", "pdf": "https://arxiv.org/pdf/2504.12612", "abs": "https://arxiv.org/abs/2504.12612", "authors": ["Ching-Chun Chang", "Isao Echizen"], "title": "The Chronicles of Foundation AI for Forensics of Multi-Agent Provenance", "categories": ["cs.AI", "cs.CR", "cs.MA"], "comment": null, "summary": "Provenance is the chronology of things, resonating with the fundamental\npursuit to uncover origins, trace connections, and situate entities within the\nflow of space and time. As artificial intelligence advances towards autonomous\nagents capable of interactive collaboration on complex tasks, the provenance of\ngenerated content becomes entangled in the interplay of collective creation,\nwhere contributions are continuously revised, extended or overwritten. In a\nmulti-agent generative chain, content undergoes successive transformations,\noften leaving little, if any, trace of prior contributions. In this study, we\ninvestigates the problem of tracking multi-agent provenance across the temporal\ndimension of generation. We propose a chronological system for post hoc\nattribution of generative history from content alone, without reliance on\ninternal memory states or external meta-information. At its core lies the\nnotion of symbolic chronicles, representing signed and time-stamped records, in\na form analogous to the chain of custody in forensic science. The system\noperates through a feedback loop, whereby each generative timestep updates the\nchronicle of prior interactions and synchronises it with the synthetic content\nin the very act of generation. This research seeks to develop an accountable\nform of collaborative artificial intelligence within evolving cyber ecosystems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8ffd\u8e2a\u591a\u667a\u80fd\u4f53\u751f\u6210\u5185\u5bb9\u6765\u6e90\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u7acb\u65f6\u95f4\u6233\u8bb0\u5f55\u7684\u7b26\u53f7\u7f16\u5e74\u53f2\uff0c\u5b9e\u73b0\u4e0d\u4f9d\u8d56\u5185\u90e8\u72b6\u6001\u6216\u5916\u90e8\u5143\u6570\u636e\u7684\u751f\u6210\u5386\u53f2\u8ffd\u6eaf\u3002", "motivation": "\u968f\u7740AI\u5411\u591a\u667a\u80fd\u4f53\u534f\u540c\u751f\u6210\u5185\u5bb9\u53d1\u5c55\uff0c\u8d21\u732e\u7684\u8fde\u7eed\u4fee\u6539\u5bfc\u81f4\u6765\u6e90\u4fe1\u606f\u4e22\u5931\uff0c\u9700\u5efa\u7acb\u53ef\u8ffd\u6eaf\u7684\u534f\u4f5c\u751f\u6210\u6eaf\u6e90\u673a\u5236\u3002", "method": "\u91c7\u7528\u7b26\u53f7\u7f16\u5e74\u53f2\u7cfb\u7edf\uff08\u542b\u7b7e\u540d\u548c\u65f6\u95f4\u6233\u7684\u94fe\u5f0f\u8bb0\u5f55\uff09\uff0c\u901a\u8fc7\u751f\u6210\u6b65\u9aa4\u7684\u53cd\u9988\u5faa\u73af\u66f4\u65b0\u4ea4\u4e92\u5386\u53f2\u5e76\u4e0e\u751f\u6210\u5185\u5bb9\u540c\u6b65\u3002", "result": "\u5f00\u53d1\u51fa\u4ec5\u901a\u8fc7\u5185\u5bb9\u5373\u53ef\u8ffd\u6eaf\u65f6\u5e8f\u751f\u6210\u5386\u53f2\u7684\u6846\u67b6\uff0c\u7c7b\u6bd4\u6cd5\u533b\u5b66\u76d1\u7ba1\u94fe\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u65f6\u5e8f\u6eaf\u6e90\u7cfb\u7edf\u4e3a\u52a8\u6001\u7f51\u7edc\u751f\u6001\u4e2d\u7684\u534f\u4f5cAI\u63d0\u4f9b\u4e86\u53ef\u95ee\u8d23\u7684\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2504.12322", "pdf": "https://arxiv.org/pdf/2504.12322", "abs": "https://arxiv.org/abs/2504.12322", "authors": ["Xin Gao", "Qizhi Pei", "Zinan Tang", "Yu Li", "Honglin Lin", "Jiang Wu", "Conghui He", "Lijun Wu"], "title": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "While data synthesis and distillation are promising strategies to enhance\nsmall language models, current approaches heavily rely on Large Language Models\n(LLMs), which suffer from high computational costs, environmental inefficiency,\nand potential biases inherited from monolithic architectures. In contrast,\nsmaller LLMs are more accessible and sustainable, but their individual\ncapabilities often fall short in generating high-quality, diverse, and reliable\ndata. Inspired by collaborative human processes (e.g., peer review), we propose\na multiple small LLMs involved framework, GRA, that aggregates specialized\nroles across small LLMs to iterative refinement and quality control typically\nachieved by a single large LLM. In this collaborative framework, multiple small\nLLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a\npeer-review-inspired data synthesis pipeline. The Generator proposes initial\ndata samples, the Reviewer critiques their quality and diversity, and the\nAdjudicator resolves conflicts to finalize the output. By decomposing the\nsynthesis process into specialized sub-tasks, collaborative small LLMs can\nachieve data-level parity with large LLM-based distillation. Through\nexperiments across multiple benchmarks, we demonstrate that GRA-produced data\nmatches or exceeds the quality of single large LLM outputs, e.g.,\nQwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large\nmodels for high-quality data synthesis, advocating instead for strategic\ncoordination of smaller agents. Our datasets, models, and code are publicly\navailable at https://github.com/GX-XinGao/GRA.", "AI": {"tldr": "\u63d0\u51fa\u534f\u4f5c\u6846\u67b6GRA\uff0c\u901a\u8fc7\u591a\u4e2a\u5c0f\u578bLLM\u534f\u4f5c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6570\u636e\u5408\u6210\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6570\u636e\u5408\u6210\u4e2d\u7684\u9ad8\u6210\u672c\u3001\u4f4e\u6548\u548c\u6f5c\u5728\u504f\u89c1\u95ee\u9898\uff0c\u5c0f\u578bLLM\u867d\u53ef\u6301\u7eed\u4f46\u80fd\u529b\u6709\u9650\u3002", "method": "\u91c7\u7528Generator\u3001Reviewer\u3001Adjudicator\u4e09\u4e2a\u89d2\u8272\u7684\u5c0f\u578bLLM\u534f\u4f5c\u6846\u67b6\u6a21\u62df\u540c\u884c\u8bc4\u5ba1\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGRA\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6570\u636e\u8d28\u91cf\u4f18\u4e8e\u6216\u76f8\u5f53\u4e8e\u5927\u578bLLM\uff08\u5982Qwen-2.5-72B-Instruct\uff09\u3002", "conclusion": "\u8bc1\u660e\u5c0f\u578bLLM\u534f\u4f5c\u53ef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6570\u636e\u5408\u6210\uff0c\u65e0\u9700\u4f9d\u8d56\u5927\u578b\u6a21\u578b\u3002"}}
{"id": "2504.12456", "pdf": "https://arxiv.org/pdf/2504.12456", "abs": "https://arxiv.org/abs/2504.12456", "authors": ["Huantao Ren", "Minmin Yang", "Senem Velipasalar"], "title": "DG-MVP: 3D Domain Generalization via Multiple Views of Point Clouds for Classification", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks have achieved significant success in 3D point cloud\nclassification while relying on large-scale, annotated point cloud datasets,\nwhich are labor-intensive to build. Compared to capturing data with LiDAR\nsensors and then performing annotation, it is relatively easier to sample point\nclouds from CAD models. Yet, data sampled from CAD models is regular, and does\nnot suffer from occlusion and missing points, which are very common for LiDAR\ndata, creating a large domain shift. Therefore, it is critical to develop\nmethods that can generalize well across different point cloud domains. %In this\npaper, we focus on the 3D point cloud domain generalization problem. Existing\n3D domain generalization methods employ point-based backbones to extract point\ncloud features. Yet, by analyzing point utilization of point-based methods and\nobserving the geometry of point clouds from different domains, we have found\nthat a large number of point features are discarded by point-based methods\nthrough the max-pooling operation. This is a significant waste especially\nconsidering the fact that domain generalization is more challenging than\nsupervised learning, and point clouds are already affected by missing points\nand occlusion to begin with. To address these issues, we propose a novel method\nfor 3D point cloud domain generalization, which can generalize to unseen\ndomains of point clouds. Our proposed method employs multiple 2D projections of\na 3D point cloud to alleviate the issue of missing points and involves a simple\nyet effective convolution-based model to extract features. The experiments,\nperformed on the PointDA-10 and Sim-to-Real benchmarks, demonstrate the\neffectiveness of our proposed method, which outperforms different baselines,\nand can transfer well from synthetic domain to real-world domain.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u76843D\u70b9\u4e91\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u89c6\u89d22D\u6295\u5f71\u548c\u5377\u79ef\u6a21\u578b\u89e3\u51b3\u70b9\u4e91\u7f3a\u5931\u548c\u906e\u6321\u95ee\u9898\u3002", "motivation": "\u73b0\u67093D\u70b9\u4e91\u5206\u7c7b\u4f9d\u8d56\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u6807\u6ce8\u6210\u672c\u9ad8\u3002CAD\u6a21\u578b\u751f\u6210\u7684\u70b9\u4e91\u4e0eLiDAR\u6570\u636e\u5b58\u5728\u9886\u57df\u5dee\u5f02\uff0c\u9700\u63d0\u5347\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u591a\u89c6\u89d22D\u6295\u5f71\u7f13\u89e3\u70b9\u4e91\u7f3a\u5931\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u5377\u79ef\u6a21\u578b\u63d0\u53d6\u7279\u5f81\u3002", "result": "\u5728PointDA-10\u548cSim-to-Real\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u4ece\u5408\u6210\u9886\u57df\u8fc1\u79fb\u5230\u771f\u5b9e\u9886\u57df\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc72D\u6295\u5f71\u548c\u5377\u79ef\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e863D\u70b9\u4e91\u7684\u8de8\u9886\u57df\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2504.12465", "pdf": "https://arxiv.org/pdf/2504.12465", "abs": "https://arxiv.org/abs/2504.12465", "authors": ["Yuta Kambe", "Yota Maeda", "Tristan Vaccon"], "title": "Geometric Generality of Transformer-Based Gr\u00f6bner Basis Computation", "categories": ["cs.LG", "cs.SC", "math.AG", "stat.ML"], "comment": "19 pages", "summary": "The intersection of deep learning and symbolic mathematics has seen rapid\nprogress in recent years, exemplified by the work of Lample and Charton. They\ndemonstrated that effective training of machine learning models for solving\nmathematical problems critically depends on high-quality, domain-specific\ndatasets. In this paper, we address the computation of Gr\\\"obner basis using\nTransformers. While a dataset generation method tailored to Transformer-based\nGr\\\"obner basis computation has previously been proposed, it lacked theoretical\nguarantees regarding the generality or quality of the generated datasets. In\nthis work, we prove that datasets generated by the previously proposed\nalgorithm are sufficiently general, enabling one to ensure that Transformers\ncan learn a sufficiently diverse range of Gr\\\"obner bases. Moreover, we propose\nan extended and generalized algorithm to systematically construct datasets of\nideal generators, further enhancing the training effectiveness of Transformer.\nOur results provide a rigorous geometric foundation for Transformers to address\na mathematical problem, which is an answer to Lample and Charton's idea of\ntraining on diverse or representative inputs.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528Transformer\u6a21\u578b\u8ba1\u7b97Gr\u00f6bner\u57fa\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u6570\u636e\u96c6\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u5176\u8db3\u591f\u901a\u7528\u4ee5\u8bad\u7ec3\u6a21\u578b\u5904\u7406\u591a\u6837\u5316\u7684Gr\u00f6bner\u57fa\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u9488\u5bf9Transformer\u7684Gr\u00f6bner\u57fa\u8ba1\u7b97\u6570\u636e\u96c6\u751f\u6210\u65b9\u6cd5\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u4fdd\u969c\uff0c\u65e0\u6cd5\u786e\u4fdd\u751f\u6210\u6570\u636e\u7684\u901a\u7528\u6027\u548c\u8d28\u91cf\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3aTransformer\u5728\u6570\u5b66\u95ee\u9898\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u66f4\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u672c\u6587\u9996\u5148\u8bc1\u660e\u4e86\u4e4b\u524d\u63d0\u51fa\u7684\u6570\u636e\u96c6\u751f\u6210\u7b97\u6cd5\u80fd\u591f\u751f\u6210\u8db3\u591f\u901a\u7528\u7684\u6570\u636e\u96c6\u3002\u7136\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u548c\u901a\u7528\u7684\u7b97\u6cd5\uff0c\u7cfb\u7edf\u5730\u6784\u5efa\u7406\u60f3\u751f\u6210\u5668\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u63d0\u9ad8Transformer\u7684\u8bad\u7ec3\u6548\u679c\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u751f\u6210\u7684\u6570\u636e\u96c6\u8db3\u591f\u901a\u7528\uff0c\u80fd\u591f\u786e\u4fddTransformer\u5b66\u4e60\u5230\u591a\u6837\u5316\u7684Gr\u00f6bner\u57fa\u3002\u6b64\u5916\uff0c\u6269\u5c55\u7684\u7b97\u6cd5\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86Transformer\u7684\u8bad\u7ec3\u6548\u679c\u3002", "conclusion": "\u672c\u6587\u4e3aTransformer\u5728\u6570\u5b66\u95ee\u9898\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u51e0\u4f55\u57fa\u7840\uff0c\u9a8c\u8bc1\u4e86Lample\u548cCharton\u5173\u4e8e\u5728\u591a\u6837\u5316\u6216\u4ee3\u8868\u6027\u8f93\u5165\u4e0a\u8bad\u7ec3\u7684\u7406\u5ff5\u3002"}}
{"id": "2504.12680", "pdf": "https://arxiv.org/pdf/2504.12680", "abs": "https://arxiv.org/abs/2504.12680", "authors": ["Baining Zhao", "Ziyou Wang", "Jianjie Fang", "Chen Gao", "Fanhang Man", "Jinqiang Cui", "Xin Wang", "Xinlei Chen", "Yong Li", "Wenwu Zhu"], "title": "Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning", "categories": ["cs.AI", "cs.CV"], "comment": "12 pages, 5 figures", "summary": "Humans can perceive and reason about spatial relationships from sequential\nvisual observations, such as egocentric video streams. However, how pretrained\nmodels acquire such abilities, especially high-level reasoning, remains\nunclear. This paper introduces Embodied-R, a collaborative framework combining\nlarge-scale Vision-Language Models (VLMs) for perception and small-scale\nLanguage Models (LMs) for reasoning. Using Reinforcement Learning (RL) with a\nnovel reward system considering think-answer logical consistency, the model\nachieves slow-thinking capabilities with limited computational resources. After\ntraining on only 5k embodied video samples, Embodied-R with a 3B LM matches\nstate-of-the-art multimodal reasoning models (OpenAI-o1, Gemini-2.5-pro) on\nboth in-distribution and out-of-distribution embodied spatial reasoning tasks.\nEmbodied-R also exhibits emergent thinking patterns such as systematic analysis\nand contextual integration. We further explore research questions including\nresponse length, training on VLM, strategies for reward design, and differences\nin model generalization after SFT (Supervised Fine-Tuning) and RL training.", "AI": {"tldr": "Embodied-R\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u534f\u4f5c\u6846\u67b6\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u63a2\u7d22\u9884\u8bad\u7ec3\u6a21\u578b\u5982\u4f55\u4ece\u89c6\u89c9\u89c2\u5bdf\u4e2d\u83b7\u53d6\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u662f\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8003\u8651\u601d\u7ef4-\u7b54\u6848\u903b\u8f91\u4e00\u81f4\u6027\u7684\u5956\u52b1\u7cfb\u7edf\u3002", "result": "\u5728\u4ec55k\u6837\u672c\u8bad\u7ec3\u540e\uff0cEmbodied-R\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u4e0e\u6700\u5148\u8fdb\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u7cfb\u7edf\u6027\u5206\u6790\u548c\u4e0a\u4e0b\u6587\u6574\u5408\u7684\u65b0\u5174\u601d\u7ef4\u6a21\u5f0f\u3002", "conclusion": "Embodied-R\u8bc1\u660e\u901a\u8fc7\u534f\u4f5c\u6846\u67b6\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5c0f\u578b\u6a21\u578b\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u6548\u7684\u6162\u601d\u8003\u80fd\u529b\uff0c\u5e76\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002"}}
{"id": "2504.12323", "pdf": "https://arxiv.org/pdf/2504.12323", "abs": "https://arxiv.org/abs/2504.12323", "authors": ["Zheng Zhang", "Ning Li", "Qi Liu", "Rui Li", "Weibo Gao", "Qingyang Mao", "Zhenya Huang", "Baosheng Yu", "Dacheng Tao"], "title": "The Other Side of the Coin: Exploring Fairness in Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages", "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nretrieving relevant document from external knowledge sources. By referencing\nthis external knowledge, RAG effectively reduces the generation of factually\nincorrect content and addresses hallucination issues within LLMs. Recently,\nthere has been growing attention to improving the performance and efficiency of\nRAG systems from various perspectives. While these advancements have yielded\nsignificant results, the application of RAG in domains with considerable\nsocietal implications raises a critical question about fairness: What impact\ndoes the introduction of the RAG paradigm have on the fairness of LLMs? To\naddress this question, we conduct extensive experiments by varying the LLMs,\nretrievers, and retrieval sources. Our experimental analysis reveals that the\nscale of the LLMs plays a significant role in influencing fairness outcomes\nwithin the RAG framework. When the model scale is smaller than 8B, the\nintegration of retrieval mechanisms often exacerbates unfairness in small-scale\nLLMs (e.g., LLaMA3.2-1B, Mistral-7B, and LLaMA3-8B). To mitigate the fairness\nissues introduced by RAG for small-scale LLMs, we propose two approaches,\nFairFT and FairFilter. Specifically, in FairFT, we align the retriever with the\nLLM in terms of fairness, enabling it to retrieve documents that facilitate\nfairer model outputs. In FairFilter, we propose a fairness filtering mechanism\nto filter out biased content after retrieval. Finally, we validate our proposed\napproaches on real-world datasets, demonstrating their effectiveness in\nimproving fairness while maintaining performance.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u516c\u5e73\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\u6765\u51cf\u8f7b\u5c0f\u89c4\u6a21LLMs\u4e2d\u7684\u4e0d\u516c\u5e73\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7a76RAG\u6280\u672f\u5728\u5177\u6709\u91cd\u5927\u793e\u4f1a\u5f71\u54cd\u7684\u9886\u57df\u4e2d\u5e94\u7528\u65f6\uff0c\u5982\u4f55\u5f71\u54cdLLMs\u7684\u516c\u5e73\u6027\u3002", "method": "\u901a\u8fc7\u6539\u53d8LLMs\u3001\u68c0\u7d22\u5668\u548c\u68c0\u7d22\u6765\u6e90\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1aFairFT\uff08\u5bf9\u9f50\u68c0\u7d22\u5668\u4e0eLLM\u7684\u516c\u5e73\u6027\uff09\u548cFairFilter\uff08\u68c0\u7d22\u540e\u8fc7\u6ee4\u504f\u9887\u5185\u5bb9\uff09\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u6a21\u578b\u89c4\u6a21\u5c0f\u4e8e8B\u65f6\uff0cRAG\u673a\u5236\u5f80\u5f80\u4f1a\u52a0\u5267\u5c0f\u89c4\u6a21LLMs\u7684\u4e0d\u516c\u5e73\u6027\u3002\u63d0\u51fa\u7684FairFT\u548cFairFilter\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u5584\u516c\u5e73\u6027\u800c\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6a21\u578b\u89c4\u6a21\u5bf9RAG\u6846\u67b6\u4e2d\u7684\u516c\u5e73\u6027\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u63d0\u5347\u5c0f\u89c4\u6a21LLMs\u516c\u5e73\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.12513", "pdf": "https://arxiv.org/pdf/2504.12513", "abs": "https://arxiv.org/abs/2504.12513", "authors": ["Chaitanya Patel", "Juan Carlos Niebles", "Ehsan Adeli"], "title": "AdaVid: Adaptive Video-Language Pretraining", "categories": ["cs.CV", "cs.AI"], "comment": "CVPRW 2025. Project Page: https://chaitanya100100.github.io/AdaVid/", "summary": "Contrastive video-language pretraining has demonstrated great success in\nlearning rich and robust video representations. However, deploying such video\nencoders on compute-constrained edge devices remains challenging due to their\nhigh computational demands. Additionally, existing models are typically trained\nto process only short video clips, often limited to 4 to 64 frames. In this\npaper, we introduce AdaVid, a flexible architectural framework designed to\nlearn efficient video encoders that can dynamically adapt their computational\nfootprint based on available resources. At the heart of AdaVid is an adaptive\ntransformer block, inspired by Matryoshka Representation Learning, which allows\nthe model to adjust its hidden embedding dimension at inference time. We show\nthat AdaVid-EgoVLP, trained on video-narration pairs from the large-scale Ego4D\ndataset, matches the performance of the standard EgoVLP on short video-language\nbenchmarks using only half the compute, and even outperforms EgoVLP when given\nequal computational resources. We further explore the trade-off between frame\ncount and compute on the challenging Diving48 classification benchmark, showing\nthat AdaVid enables the use of more frames without exceeding computational\nlimits. To handle longer videos, we also propose a lightweight hierarchical\nnetwork that aggregates short clip features, achieving a strong balance between\ncompute efficiency and accuracy across several long video benchmarks.", "AI": {"tldr": "AdaVid\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\uff0c\u9ad8\u6548\u5904\u7406\u89c6\u9891\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u8ba1\u7b97\u80fd\u529b\u7684\u8bbe\u5907\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7f16\u7801\u5668\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u4e14\u4ec5\u80fd\u5904\u7406\u77ed\u89c6\u9891\u7247\u6bb5\uff0c\u96be\u4ee5\u5728\u8ba1\u7b97\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002", "method": "AdaVid\u91c7\u7528\u81ea\u9002\u5e94Transformer\u5757\uff0c\u52a8\u6001\u8c03\u6574\u9690\u85cf\u5d4c\u5165\u7ef4\u5ea6\uff0c\u5e76\u7ed3\u5408\u8f7b\u91cf\u7ea7\u5c42\u6b21\u7f51\u7edc\u5904\u7406\u957f\u89c6\u9891\u3002", "result": "AdaVid-EgoVLP\u5728Ego4D\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u6807\u51c6EgoVLP\uff0c\u8ba1\u7b97\u8d44\u6e90\u51cf\u534a\u65f6\u6027\u80fd\u76f8\u5f53\uff1b\u5728Diving48\u4e0a\u5c55\u793a\u66f4\u591a\u5e27\u6570\u4e0d\u8d85\u9650\u7684\u80fd\u529b\u3002", "conclusion": "AdaVid\u4e3a\u89c6\u9891\u7f16\u7801\u63d0\u4f9b\u4e86\u8d44\u6e90\u9ad8\u6548\u6027\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u548c\u957f\u89c6\u9891\u5904\u7406\u3002"}}
{"id": "2504.12471", "pdf": "https://arxiv.org/pdf/2504.12471", "abs": "https://arxiv.org/abs/2504.12471", "authors": ["Shiwei Ding", "Lan Zhang", "Zhenlin Wang", "Giuseppe Ateniese", "Xiaoyong Yuan"], "title": "You Don't Need All Attentions: Distributed Dynamic Fine-Tuning for Foundation Models", "categories": ["cs.LG", "cs.DC", "cs.PF"], "comment": null, "summary": "Fine-tuning plays a crucial role in adapting models to downstream tasks with\nminimal training efforts. However, the rapidly increasing size of foundation\nmodels poses a daunting challenge for accommodating foundation model\nfine-tuning in most commercial devices, which often have limited memory\nbandwidth. Techniques like model sharding and tensor parallelism address this\nissue by distributing computation across multiple devices to meet memory\nrequirements. Nevertheless, these methods do not fully leverage their\nfoundation nature in facilitating the fine-tuning process, resulting in high\ncomputational costs and imbalanced workloads. We introduce a novel Distributed\nDynamic Fine-Tuning (D2FT) framework that strategically orchestrates operations\nacross attention modules based on our observation that not all attention\nmodules are necessary for forward and backward propagation in fine-tuning\nfoundation models. Through three innovative selection strategies, D2FT\nsignificantly reduces the computational workload required for fine-tuning\nfoundation models. Furthermore, D2FT addresses workload imbalances in\ndistributed computing environments by optimizing these selection strategies via\nmultiple knapsack optimization. Our experimental results demonstrate that the\nproposed D2FT framework reduces the training computational costs by 40% and\ntraining communication costs by 50% with only 1% to 2% accuracy drops on the\nCIFAR-10, CIFAR-100, and Stanford Cars datasets. Moreover, the results show\nthat D2FT can be effectively extended to recent LoRA, a state-of-the-art\nparameter-efficient fine-tuning technique. By reducing 40% computational cost\nor 50% communication cost, D2FT LoRA top-1 accuracy only drops 4% to 6% on\nStanford Cars dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86D2FT\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6ce8\u610f\u529b\u6a21\u5757\u4f18\u5316\u5927\u6a21\u578b\u5fae\u8c03\uff0c\u964d\u4f4e\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u4f53\u79ef\u589e\u5927\u5bfc\u81f4\u5fae\u8c03\u5728\u5546\u4e1a\u8bbe\u5907\u4e0a\u9762\u4e34\u5185\u5b58\u5e26\u5bbd\u9650\u5236\uff0c\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u8d1f\u8f7d\u4e0d\u5747\u8861\u3002", "method": "\u63d0\u51faD2FT\u6846\u67b6\uff0c\u91c7\u7528\u4e09\u79cd\u521b\u65b0\u9009\u62e9\u7b56\u7565\u548c\u591a\u91cd\u80cc\u5305\u4f18\u5316\uff0c\u52a8\u6001\u8c03\u5ea6\u6ce8\u610f\u529b\u6a21\u5757\u7684\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cD2FT\u964d\u4f4e40%\u8ba1\u7b97\u6210\u672c\u548c50%\u901a\u4fe1\u6210\u672c\uff0c\u51c6\u786e\u7387\u4ec5\u4e0b\u964d1%-2%\uff1b\u5728LoRA\u4e0a\u6269\u5c55\u540e\u4ecd\u4fdd\u6301\u8f83\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "D2FT\u6709\u6548\u89e3\u51b3\u4e86\u5927\u6a21\u578b\u5fae\u8c03\u7684\u8d44\u6e90\u6548\u7387\u95ee\u9898\uff0c\u4e14\u517c\u5bb9\u73b0\u6709\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u3002"}}
{"id": "2504.12682", "pdf": "https://arxiv.org/pdf/2504.12682", "abs": "https://arxiv.org/abs/2504.12682", "authors": ["Arth Bohra", "Manvel Saroyan", "Danil Melkozerov", "Vahe Karufanyan", "Gabriel Maher", "Pascal Weinberger", "Artem Harutyunyan", "Giovanni Campagna"], "title": "WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Most recent web agent research has focused on navigation and transaction\ntasks, with little emphasis on extracting structured data at scale. We present\nWebLists, a benchmark of 200 data-extraction tasks across four common business\nand enterprise use-cases. Each task requires an agent to navigate to a webpage,\nconfigure it appropriately, and extract complete datasets with well-defined\nschemas. We show that both LLMs with search capabilities and SOTA web agents\nstruggle with these tasks, with a recall of 3% and 31%, respectively, despite\nhigher performance on question-answering tasks.\n  To address this challenge, we propose BardeenAgent, a novel framework that\nenables web agents to convert their execution into repeatable programs, and\nreplay them at scale across pages with similar structure. BardeenAgent is also\nthe first LLM agent to take advantage of the regular structure of HTML. In\nparticular BardeenAgent constructs a generalizable CSS selector to capture all\nrelevant items on the page, then fits the operations to extract the data.\n  On the WebLists benchmark, BardeenAgent achieves 66% recall overall, more\nthan doubling the performance of SOTA web agents, and reducing cost per output\nrow by 3x.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2504.12324", "pdf": "https://arxiv.org/pdf/2504.12324", "abs": "https://arxiv.org/abs/2504.12324", "authors": ["Mengying Yuan", "Wangzi Xuan", "Fei Li"], "title": "Cross-Document Cross-Lingual Natural Language Inference via RST-enhanced Graph Fusion and Interpretability Prediction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural Language Inference (NLI) is a fundamental task in both natural\nlanguage processing and information retrieval. While NLI has developed many\nsub-directions such as sentence-level NLI, document-level NLI and cross-lingual\nNLI, Cross-Document Cross-Lingual NLI (CDCL-NLI) remains largely unexplored. In\nthis paper, we propose a novel paradigm for CDCL-NLI that extends traditional\nNLI capabilities to multi-document, multilingual scenarios. To support this\ntask, we construct a high-quality CDCL-NLI dataset including 1,110 instances\nand spanning 26 languages. To build a baseline for this task, we also propose\nan innovative method that integrates RST-enhanced graph fusion and\ninterpretability prediction. Our method employs RST (Rhetorical Structure\nTheory) on RGAT (Relation-aware Graph Attention Network) for cross-document\ncontext modeling, coupled with a structure-aware semantic alignment mechanism\nbased on lexical chains for cross-lingual understanding. For NLI\ninterpretability, we develop an EDU-level attribution framework that generates\nextractive explanations. Extensive experiments demonstrate our approach's\nsuperior performance, achieving significant improvements over both traditional\nNLI models such as DocNLI and R2F, as well as LLMs like Llama3 and GPT-4o. Our\nwork sheds light on the study of NLI and will bring research interest on\ncross-document cross-lingual context understanding, semantic retrieval and\ninterpretability inference. Our dataset and code are available at\n\\href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-Link for peer\nreview}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6587\u6863\u8de8\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08CDCL-NLI\uff09\u7684\u65b0\u8303\u5f0f\uff0c\u6784\u5efa\u4e86\u5305\u542b1110\u4e2a\u5b9e\u4f8b\u3001\u8986\u76d626\u79cd\u8bed\u8a00\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210RST\u589e\u5f3a\u56fe\u878d\u5408\u548c\u53ef\u89e3\u91ca\u6027\u9884\u6d4b\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfNLI\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u8de8\u6587\u6863\u8de8\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08CDCL-NLI\uff09\u5728\u5f53\u524d\u7814\u7a76\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6269\u5c55NLI\u80fd\u529b\u81f3\u591a\u6587\u6863\u3001\u591a\u8bed\u8a00\u573a\u666f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408RST\u589e\u5f3a\u7684\u56fe\u878d\u5408\u548c\u53ef\u89e3\u91ca\u6027\u9884\u6d4b\uff0c\u4f7f\u7528RST\u5728RGAT\u4e0a\u8fdb\u884c\u8de8\u6587\u6863\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u5e76\u57fa\u4e8e\u8bcd\u6c47\u94fe\u7684\u7ed3\u6784\u611f\u77e5\u8bed\u4e49\u5bf9\u9f50\u673a\u5236\u8fdb\u884c\u8de8\u8bed\u8a00\u7406\u89e3\uff0c\u540c\u65f6\u5f00\u53d1\u4e86EDU\u7ea7\u522b\u7684\u5f52\u56e0\u6846\u67b6\u751f\u6210\u63d0\u53d6\u6027\u89e3\u91ca\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edfNLI\u6a21\u578b\uff08\u5982DocNLI\u548cR2F\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982Llama3\u548cGPT-4o\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aNLI\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u5c06\u4fc3\u8fdb\u8de8\u6587\u6863\u8de8\u8bed\u8a00\u4e0a\u4e0b\u6587\u7406\u89e3\u3001\u8bed\u4e49\u68c0\u7d22\u548c\u53ef\u89e3\u91ca\u6027\u63a8\u7406\u7684\u7814\u7a76\u5174\u8da3\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2504.12515", "pdf": "https://arxiv.org/pdf/2504.12515", "abs": "https://arxiv.org/abs/2504.12515", "authors": ["Kaustav Chanda", "Aayush Atul Verma", "Arpitsinh Vaghela", "Yezhou Yang", "Bharatesh Chakravarthi"], "title": "Event Quality Score (EQS): Assessing the Realism of Simulated Event Camera Streams via Distances in Latent Space", "categories": ["cs.CV"], "comment": "Accepted at 2025 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW); Fifth International Workshop on Event-Based\n  Vision", "summary": "Event cameras promise a paradigm shift in vision sensing with their low\nlatency, high dynamic range, and asynchronous nature of events. Unfortunately,\nthe scarcity of high-quality labeled datasets hinders their widespread adoption\nin deep learning-driven computer vision. To mitigate this, several simulators\nhave been proposed to generate synthetic event data for training models for\ndetection and estimation tasks. However, the fundamentally different sensor\ndesign of event cameras compared to traditional frame-based cameras poses a\nchallenge for accurate simulation. As a result, most simulated data fail to\nmimic data captured by real event cameras. Inspired by existing work on using\ndeep features for image comparison, we introduce event quality score (EQS), a\nquality metric that utilizes activations of the RVT architecture. Through\nsim-to-real experiments on the DSEC driving dataset, it is shown that a higher\nEQS implies improved generalization to real-world data after training on\nsimulated events. Thus, optimizing for EQS can lead to developing more\nrealistic event camera simulators, effectively reducing the simulation gap. EQS\nis available at https://github.com/eventbasedvision/EQS.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEQS\u7684\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u4e8b\u4ef6\u76f8\u673a\u4eff\u771f\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u5339\u914d\u5ea6\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5728\u6df1\u5ea6\u5b66\u4e60\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u9ad8\u8d28\u91cf\u6807\u8bb0\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u9650\u5236\u3002\u73b0\u6709\u7684\u4eff\u771f\u6570\u636e\u96be\u4ee5\u51c6\u786e\u6a21\u62df\u771f\u5b9e\u4e8b\u4ef6\u76f8\u673a\u7684\u6570\u636e\u3002", "method": "\u7814\u7a76\u5229\u7528RVT\u67b6\u6784\u7684\u6fc0\u6d3b\u7279\u5f81\uff0c\u63d0\u51fa\u4e86\u4e8b\u4ef6\u8d28\u91cf\u8bc4\u5206\uff08EQS\uff09\uff0c\u5e76\u901a\u8fc7DSEC\u9a7e\u9a76\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u4eff\u771f\u5230\u771f\u5b9e\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u66f4\u9ad8\u7684EQS\u8bc4\u5206\u9884\u793a\u7740\u5728\u4eff\u771f\u4e8b\u4ef6\u8bad\u7ec3\u540e\u6a21\u578b\u5bf9\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u4f18\u5316EQS\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u771f\u5b9e\u7684\u4e8b\u4ef6\u76f8\u673a\u4eff\u771f\u5668\uff0c\u6709\u6548\u51cf\u5c0f\u4eff\u771f\u4e0e\u5b9e\u9645\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
