<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models](https://arxiv.org/abs/2507.12547)
*Lionel Wong,Katherine M. Collins,Lance Ying,Cedegao E. Zhang,Adrian Weller,Tobias Gersternberg,Timothy O'Donnell,Alexander K. Lew,Jacob D. Andreas,Joshua B. Tenenbaum,Tyler Brooke-Wilson*

Main category: cs.CL

TL;DR: 该研究探索人类在陌生情境下整合全局信息并进行连贯推理的能力。提出一种“模型合成架构”（MSA），结合语言模型和概率程序，用于构建定制心理模型。在新的推理数据集上，MSA表现优于纯语言模型基线，表明其能模拟人类开放式推理。


<details>
  <summary>Details</summary>
Motivation: 当面对新颖情境时，人类如何从广泛的背景知识中提取相关考量并用于推理和预测？是什么使我们能够吸纳全局相关信息并进行连贯推理？

Method: 提出并探索了人类使用分布式和符号表征结合来构建定制心理模型的假设。计算实现为“模型合成架构”（MSA），其中语言模型用于全局相关性检索和模型合成，概率程序用于实现定制的、连贯的世界模型。在名为“模型奥林匹克”的体育情境推理数据集上评估MSA，该数据集要求模型对语言描述的新因果结构进行判断，利用大量背景知识，并结合引入任意新变量的观察结果。将MSA与仅使用语言模型的基线进行比较。

Result: MSA在捕捉人类判断方面优于仅使用语言模型的基线（包括直接生成和思维链生成）。

Conclusion: 研究结果表明MSA可以实现对人类在开放式领域中，对全局相关变量进行局部连贯推理能力的模拟，为理解和复制人类推理提供了一条途径。

Abstract: When faced with novel situations, people are able to marshal relevant
considerations from a wide range of background knowledge and put these to use
in inferences and predictions. What permits us to draw in globally relevant
information and reason over it coherently? Here, we explore the hypothesis that
people use a combination of distributed and symbolic representations to
construct bespoke mental models tailored to novel situations. We propose a
computational implementation of this idea -- a ``Model Synthesis Architecture''
(MSA) -- using language models to implement global relevance-based retrieval
and model synthesis and probabilistic programs to implement bespoke, coherent
world models. We evaluate our MSA as a model of human judgments on a novel
reasoning dataset. The dataset -- built around a `Model Olympics` domain of
sports vignettes -- tests models' capacity for human-like, open-ended reasoning
by requiring (i) judgments about novel causal structures described in language;
(ii) drawing on large bodies of background knowledge; and (iii) doing both in
light of observations that introduce arbitrary novel variables. Our MSA
approach captures human judgments better than language model-only baselines,
under both direct and chain-of-thought generations from the LM that supports
model synthesis. These results suggest that MSAs can be implemented in a way
that mirrors people's ability to deliver locally coherent reasoning over
globally relevant variables, offering a path to understanding and replicating
human reasoning in open-ended domains.

</details>


### [2] [Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility](https://arxiv.org/abs/2507.12553)
*Michael A. Lepori,Jennifer Hu,Ishita Dasgupta,Roma Patel,Thomas Serre,Ellie Pavlick*

Main category: cs.CL

TL;DR: 本文通过识别语言模型中的模态差异向量，揭示了语言模型比先前认为的更能可靠地进行模态分类，并发现这些向量能以一致的顺序出现，甚至可以模拟人类的分类行为。


<details>
  <summary>Details</summary>
Motivation: 语言模型在执行各种任务时需要识别句子的模态类别（如可能性、不可能性、荒谬性）。然而，现有研究对语言模型进行模态分类的能力提出了质疑。

Method: 研究者在多种语言模型中识别出区分模态类别的线性表示，称之为“模态差异向量”。他们分析了这些向量在模型能力提升（训练步骤、层数、参数量）时出现的顺序，并利用这些向量的激活来建模人类细粒度的分类行为，通过与人类对可解释特征的评分进行关联。

Result: 研究发现语言模型拥有比先前报告更可靠的模态分类判断能力。模态差异向量随着模型的成熟以一致的顺序出现。此外，这些向量可用于建模人类细粒度的模态分类行为。

Conclusion: 本研究利用机制可解释性技术深入理解了语言模型的模态分类能力，并为理解人类如何区分模态类别提供了新的视角。

Abstract: Language models (LMs) are used for a diverse range of tasks, from question
answering to writing fantastical stories. In order to reliably accomplish these
tasks, LMs must be able to discern the modal category of a sentence (i.e.,
whether it describes something that is possible, impossible, completely
nonsensical, etc.). However, recent studies have called into question the
ability of LMs to categorize sentences according to modality (Michaelov et al.,
2025; Kauf et al., 2023). In this work, we identify linear representations that
discriminate between modal categories within a variety of LMs, or modal
difference vectors. Analysis of modal difference vectors reveals that LMs have
access to more reliable modal categorization judgments than previously
reported. Furthermore, we find that modal difference vectors emerge in a
consistent order as models become more competent (i.e., through training steps,
layers, and parameter count). Notably, we find that modal difference vectors
identified within LM activations can be used to model fine-grained human
categorization behavior. This potentially provides a novel view into how human
participants distinguish between modal categories, which we explore by
correlating projections along modal difference vectors with human participants'
ratings of interpretable features. In summary, we derive new insights into LM
modal categorization using techniques from mechanistic interpretability, with
the potential to inform our understanding of modal categorization in humans.

</details>


### [3] [The first open machine translation system for the Chechen language](https://arxiv.org/abs/2507.12672)
*Abu-Viskhan A. Umishov,Vladislav A. Grigorian*

Main category: cs.CL

TL;DR: 本文介绍了首个开源的车臣语与俄语之间的翻译模型及其训练数据集，通过微调NLLB-200实现了多语言翻译，并公布了相关语料和句编码器。


<details>
  <summary>Details</summary>
Motivation: 旨在为濒危的车臣语提供翻译模型，并探索将新语言整合到大型多语言翻译系统（如NLLB-200）中的微调能力，填补该语言翻译资源的空白。

Method: 引入了首个车臣语与俄语之间的开源翻译模型，收集并使用了数据集进行训练和评估。探索了将新语言（车臣语）纳入大型多语言翻译系统NLLB-200的微调能力，并发布了并行词语、短语和句子语料库以及适应车臣语的多语言句编码器。

Result: 所构建的模型在俄语到车臣语方向的BLEU/ChrF++得分分别为8.34/34.69，在反方向（车臣语到俄语）分别为20.89/44.55。同时发布了翻译模型、并行语料（包括词、短语和句子）以及适应车臣语的多语言句编码器。

Conclusion: 成功构建并发布了首个开源的车臣语-俄语翻译模型及其配套数据集和工具，为濒危语言的机器翻译研究和应用做出了贡献，并展示了通过微调将新语言整合到现有大型多语言模型中的可行性。

Abstract: We introduce the first open-source model for translation between the
vulnerable Chechen language and Russian, and the dataset collected to train and
evaluate it. We explore fine-tuning capabilities for including a new language
into a large language model system for multilingual translation NLLB-200. The
BLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for
translation from Russian to Chechen and reverse direction, respectively. The
release of the translation models is accompanied by the distribution of
parallel words, phrases and sentences corpora and multilingual sentence encoder
adapted to the Chechen language.

</details>


### [4] [Improving Drug Identification in Overdose Death Surveillance using Large Language Models](https://arxiv.org/abs/2507.12679)
*Arthur J. Funnell,Panayiotis Petousis,Fabrice Harel-Canada,Ruby Romero,Alex A. T. Bui,Adam Koncsol,Hritika Chaturvedi,Chelsea Shover,David Goodman-Meza*

Main category: cs.CL

TL;DR: 本研究利用自然语言处理（NLP）模型，特别是微调的BioClinicalBERT，实现了对自由文本死亡报告中药物过量死亡的准确分类，有效加速了药物监测并克服了传统编码的局限性。


<details>
  <summary>Details</summary>
Motivation: 美国芬太尼导致的药物相关死亡率不断上升，需要及时准确的监测。然而，关键的过量数据常埋藏于自由文本的验尸官报告中，传统编码（ICD-10）导致数据延迟和信息丢失。现有NLP在药物过量监测中的应用有限。

Method: 研究使用了2020年来自多个美国司法管辖区的35,433份死亡记录进行模型训练和内部测试，并利用2023-2024年3,335份新记录进行外部验证。评估了多种NLP方法，包括传统单/多标签分类器、微调的编码器模型（BERT, BioClinicalBERT）和解码器大型语言模型（Qwen 3, Llama 3），以从非结构化死亡证明文本中分类特定药物的涉及情况。模型性能通过宏观平均F1分数和95%置信区间进行评估。

Result: 微调后的BioClinicalBERT模型在内部测试集上取得了近乎完美的性能（宏观F1分数≥0.998）。外部验证证实了其鲁棒性（宏观F1=0.966），且性能优于传统的机器学习方法、通用BERT模型以及各种解码器大型语言模型。

Conclusion: NLP模型，特别是经过微调的临床领域变体（如BioClinicalBERT），为从自由文本报告中分类过量死亡提供了一个高度准确且可扩展的解决方案。这些方法能够显著加速监测工作流程，克服手动ICD-10编码的局限性，并支持近实时地检测新兴药物滥用趋势。

Abstract: The rising rate of drug-related deaths in the United States, largely driven
by fentanyl, requires timely and accurate surveillance. However, critical
overdose data are often buried in free-text coroner reports, leading to delays
and information loss when coded into ICD (International Classification of
Disease)-10 classifications. Natural language processing (NLP) models may
automate and enhance overdose surveillance, but prior applications have been
limited. A dataset of 35,433 death records from multiple U.S. jurisdictions in
2020 was used for model training and internal testing. External validation was
conducted using a novel separate dataset of 3,335 records from 2023-2024.
Multiple NLP approaches were evaluated for classifying specific drug
involvement from unstructured death certificate text. These included
traditional single- and multi-label classifiers, as well as fine-tuned
encoder-only language models such as Bidirectional Encoder Representations from
Transformers (BERT) and BioClinicalBERT, and contemporary decoder-only large
language models such as Qwen 3 and Llama 3. Model performance was assessed
using macro-averaged F1 scores, and 95% confidence intervals were calculated to
quantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect
performance, with macro F1 scores >=0.998 on the internal test set. External
validation confirmed robustness (macro F1=0.966), outperforming conventional
machine learning, general-domain BERT models, and various decoder-only large
language models. NLP models, particularly fine-tuned clinical variants like
BioClinicalBERT, offer a highly accurate and scalable solution for overdose
death classification from free-text reports. These methods can significantly
accelerate surveillance workflows, overcoming the limitations of manual ICD-10
coding and supporting near real-time detection of emerging substance use
trends.

</details>


### [5] [AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.12695)
*S M Rafiuddin,Sadia Kamal,Mohammed Rakib,Arunkumar Bagavathi,Atriya Sen*

Main category: cs.CL

TL;DR: AdaptiSent是一个多模态方面情感分析（MABSA）的新框架，通过自适应跨模态注意力机制，提升了从文本和图像中进行情感分类和方面术语提取的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态方面情感分析中对文本和图像的结合处理不够高效，特别是难以准确捕捉跨模态的细微关系，从而影响情感分类和方面术语提取的准确性。

Method: 引入AdaptiSent框架，该框架整合了动态模态加权和上下文自适应注意力机制，专注于文本线索和视觉上下文的交互。通过在标准Twitter数据集上与传统文本模型及其他多模态方法进行对比测试来验证其性能。

Result: 在标准Twitter数据集上，AdaptiSent在准确率、召回率和F1分数上均超越了现有模型。它尤其擅长识别对于准确情感和方面术语提取至关重要的细微跨模态关系。

Conclusion: AdaptiSent通过动态调整关注点以适应上下文相关性，显著提升了多模态情感分析的深度和准确性。它为MABSA设定了新标准，特别是在理解复杂多模态信息方面表现卓越。

Abstract: We introduce AdaptiSent, a new framework for Multimodal Aspect-Based
Sentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms
to improve sentiment classification and aspect term extraction from both text
and images. Our model integrates dynamic modality weighting and
context-adaptive attention, enhancing the extraction of sentiment and
aspect-related information by focusing on how textual cues and visual context
interact. We tested our approach against several baselines, including
traditional text-based models and other multimodal methods. Results from
standard Twitter datasets show that AdaptiSent surpasses existing models in
precision, recall, and F1 score, and is particularly effective in identifying
nuanced inter-modal relationships that are crucial for accurate sentiment and
aspect term extraction. This effectiveness comes from the model's ability to
adjust its focus dynamically based on the context's relevance, improving the
depth and accuracy of sentiment analysis across various multimodal data sets.
AdaptiSent sets a new standard for MABSA, significantly outperforming current
methods, especially in understanding complex multimodal information.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering](https://arxiv.org/abs/2507.12490)
*Maximiliano Hormazábal Lagos,Héctor Cerezo-Costas,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: EaGERS是一个无需训练、模型无关的DocVQA流水线，它通过VLM生成理由，将其定位到图像区域，并限制在相关区域生成响应，从而提升了性能、透明度和可复现性。


<details>
  <summary>Details</summary>
Motivation: 提高DocVQA任务的性能、透明度和可复现性，同时避免额外的模型微调。

Method: 引入EaGERS流水线，该方法完全无需训练且与模型无关。其步骤包括：1) 通过视觉语言模型生成自然语言理由；2) 通过计算多模态嵌入相似度并在可配置网格上进行多数投票，将理由定位到空间子区域；3) 限制响应仅从掩码图像中选择的相关区域生成。

Result: 在DocVQA数据集上的实验表明，EaGERS的最佳配置在精确匹配准确率和平均归一化Levenshtein相似度指标上均优于基础模型，并在不进行额外模型微调的情况下增强了DocVQA的透明度和可复现性。

Conclusion: EaGERS提供了一种有效且无需训练的方法，不仅提升了DocVQA任务的性能，还显著增强了结果的透明度和可复现性。

Abstract: We introduce EaGERS, a fully training-free and model-agnostic pipeline that
(1) generates natural language rationales via a vision language model, (2)
grounds these rationales to spatial sub-regions by computing multimodal
embedding similarities over a configurable grid with majority voting, and (3)
restricts the generation of responses only from the relevant regions selected
in the masked image. Experiments on the DocVQA dataset demonstrate that our
best configuration not only outperforms the base model on exact match accuracy
and Average Normalized Levenshtein Similarity metrics but also enhances
transparency and reproducibility in DocVQA without additional model
fine-tuning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education](https://arxiv.org/abs/2507.12484)
*Jarosław A. Chudziak,Adam Kostka*

Main category: cs.AI

TL;DR: 针对当前AI家教系统在数学领域反应式且缺乏深度教学的局限，本文提出一个新颖的多智能体AI家教平台，通过自适应反馈、结构化课程生成和知识检索，实现结构化、个性化且工具辅助的数学学习体验。


<details>
  <summary>Details</summary>
Motivation: 现有AI家教系统，尤其在数学领域，仅提供反应式答案，未能有效鼓励深度反思或整合结构化教学工具与策略，从而限制了其教学效果。

Method: 引入了一个新颖的多智能体AI家教平台，该平台通过结合自适应和个性化反馈、结构化课程生成以及教材知识检索，实现了模块化、工具辅助的学习过程。

Result: 该系统能帮助学生学习新主题并识别弱点、有效复习考试，以及进行无限量的个性化练习。

Conclusion: 本文通过引入一个整合教学智能体和AI驱动组件的新平台，为教育领域的AI研究做出了贡献，提供了模块化、高效的数学教学系统。

Abstract: The growing ubiquity of artificial intelligence (AI), in particular large
language models (LLMs), has profoundly altered the way in which learners gain
knowledge and interact with learning material, with many claiming that AI
positively influences their learning achievements. Despite this advancement,
current AI tutoring systems face limitations associated with their reactive
nature, often providing direct answers without encouraging deep reflection or
incorporating structured pedagogical tools and strategies. This limitation is
most apparent in the field of mathematics, in which AI tutoring systems remain
underdeveloped. This research addresses the question: How can AI tutoring
systems move beyond providing reactive assistance to enable structured,
individualized, and tool-assisted learning experiences? We introduce a novel
multi-agent AI tutoring platform that combines adaptive and personalized
feedback, structured course generation, and textbook knowledge retrieval to
enable modular, tool-assisted learning processes. This system allows students
to learn new topics while identifying and targeting their weaknesses, revise
for exams effectively, and practice on an unlimited number of personalized
exercises. This article contributes to the field of artificial intelligence in
education by introducing a novel platform that brings together pedagogical
agents and AI-driven components, augmenting the field with modular and
effective systems for teaching mathematics.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training](https://arxiv.org/abs/2507.12507)
*Mingjie Liu,Shizhe Diao,Jian Hu,Ximing Lu,Xin Dong,Hao Zhang,Alexander Bukharin,Shaokun Zhang,Jiaqi Zeng,Makesh Narsimhan Sreedhar,Gerald Shen,David Mosallanezhad,Di Zhang,Jonas Yang,June Yang,Oleksii Kuchaiev,Guilin Liu,Zhiding Yu,Pavlo Molchanov,Yejin Choi,Jan Kautz,Yi Dong*

Main category: cs.LG

TL;DR: 本文研究了长期强化学习对小型语言模型在多推理领域的影响，通过引入可验证奖励、增强GRPO和优化训练技术，在数学、编程和逻辑谜题任务上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如O1和DeepSeek-R1）已通过大规模强化学习结合可验证奖励，在数学和代码生成等复杂推理任务上取得重大突破。本研究旨在探讨长期强化学习对小型语言模型的效果，并识别有效训练的关键要素。

Method: 对小型语言模型进行长期强化学习，应用于多样化的推理领域。核心方法包括：使用可验证的奖励任务，增强Group Relative Policy Optimization (GRPO)算法，并引入受控KL正则化、裁剪比例和周期性参考策略重置等实践技术，以提高训练稳定性和泛化能力。

Result: 该模型在强基线上取得了显著改进：数学任务提升14.7%，编程任务提升13.9%，逻辑谜题任务提升54.8%。为促进后续研究，模型已公开发布。

Conclusion: 通过长期强化学习，结合可验证奖励、GRPO的增强以及特定的训练稳定性技术（如受控KL正则化和周期性参考策略重置），小型语言模型也能在多个推理任务上实现显著且持久的性能提升。这些要素对于释放长期性能至关重要。

Abstract: Recent advancements in reasoning-focused language models such as OpenAI's O1
and DeepSeek-R1 have shown that scaling test-time computation-through
chain-of-thought reasoning and iterative exploration-can yield substantial
improvements on complex tasks like mathematics and code generation. These
breakthroughs have been driven by large-scale reinforcement learning (RL),
particularly when combined with verifiable reward signals that provide
objective and grounded supervision. In this report, we investigate the effects
of prolonged reinforcement learning on a small language model across a diverse
set of reasoning domains. Our work identifies several key ingredients for
effective training, including the use of verifiable reward tasks, enhancements
to Group Relative Policy Optimization (GRPO), and practical techniques to
improve training stability and generalization. We introduce controlled KL
regularization, clipping ratio, and periodic reference policy resets as
critical components for unlocking long-term performance gains. Our model
achieves significant improvements over strong baselines, including +14.7% on
math, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate
continued research, we release our model publicly.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [9] [Energy-Efficient RSMA-enabled Low-altitude MEC Optimization Via Generative AI-enhanced Deep Reinforcement Learning](https://arxiv.org/abs/2507.12910)
*Xudong Wang,Hongyang Du,Lei Feng,Kaibin Huang*

Main category: cs.NI

TL;DR: 针对无人机低空MEC系统中的上行干扰问题，本文提出一种基于RSMA并结合生成式AI增强深度强化学习的联合优化方法，旨在最大化能效并有效抑制干扰。


<details>
  <summary>Details</summary>
Motivation: 6G时代低延迟计算需求催生了无人机低空移动边缘计算（MEC）系统的发展，但有限频谱导致地面终端间存在严重的上行干扰问题，亟需高效方案应对。

Method: 本文构建了一个RSMA赋能的低空MEC系统，并联合优化无人机3D轨迹、RSMA解码顺序、任务卸载决策及资源分配，以最大化能效和抑制多用户干扰。针对该优化问题的高维、非凸和动态特性，提出了一种生成式AI增强的深度强化学习（DRL）框架。具体而言，在Actor网络中嵌入扩散模型以生成高质量动作样本，提高混合动作空间的探索能力并规避局部最优。此外，设计了一种基于优先级的RSMA解码策略以实现低复杂度的连续干扰消除。

Result: 仿真结果表明，所提出的方法在低空MEC系统中性能优于基线方法。特别地，将生成扩散模型（GDM）与RSMA结合能够显著提升系统的能量效率性能。

Conclusion: 本研究提出的结合生成式AI增强DRL与RSMA的优化方法，有效解决了无人机低空MEC系统中的上行干扰和能效问题，为未来6G通信环境下的低延迟计算提供了高效且稳健的解决方案。

Abstract: The growing demand for low-latency computing in 6G is driving the use of
UAV-based low-altitude mobile edge computing (MEC) systems. However, limited
spectrum often leads to severe uplink interference among ground terminals
(GTs). In this paper, we investigate a rate-splitting multiple access
(RSMA)-enabled low-altitude MEC system, where a UAV-based edge server assists
multiple GTs in concurrently offloading their tasks over a shared uplink. We
formulate a joint optimization problem involving the UAV 3D trajectory, RSMA
decoding order, task offloading decisions, and resource allocation, aiming to
mitigate multi-user interference and maximize energy efficiency. Given the high
dimensionality, non-convex nature, and dynamic characteristics of this
optimization problem, we propose a generative AI-enhanced deep reinforcement
learning (DRL) framework to solve it efficiently. Specifically, we embed a
diffusion model into the actor network to generate high-quality action samples,
improving exploration in hybrid action spaces and avoiding local optima. In
addition, a priority-based RSMA decoding strategy is designed to facilitate
efficient successive interference cancellation with low complexity. Simulation
results demonstrate that the proposed method for low-altitude MEC systems
outperforms baseline methods, and that integrating GDM with RSMA can achieve
significantly improved energy efficiency performance.

</details>


### [10] [RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and Intention-Driven Agents](https://arxiv.org/abs/2507.13140)
*Kuiyuan Ding,Caili Guo,Yang Yang,Jianzhang Guo*

Main category: cs.NI

TL;DR: 提出RIDAS多智能体框架，利用大语言模型将用户意图转化为6G AI RAN资源配置，显著提升用户支持数量。


<details>
  <summary>Details</summary>
Motivation: 第六代（6G）网络要求人工智能（AI）与无线接入网络（RAN）紧密集成，以满足严格的服务质量（QoS）和资源效率需求。然而，现有解决方案难以将高层用户意图转化为低层、参数化的配置以实现最佳性能。

Method: 提出RIDAS，一个多智能体框架，由表征驱动代理（RDAs）和意图驱动代理（IDA）组成。RDAs提供可调控制参数的开放接口（如秩和量化比特），实现失真与传输速率之间的明确权衡。IDA采用由大语言模型（LLM）驱动的两阶段规划方案（带宽预分配和重分配），将用户意图和系统状态映射到最佳RDA配置。

Result: 实验表明，在相同的QoS约束下，RIDAS比WirelessAgent支持的用户数量多44.71%。

Conclusion: RIDAS能够有效捕获用户意图，并在AI RAN环境中更高效地分配资源。

Abstract: Sixth generation (6G) networks demand tight integration of artificial
intelligence (AI) into radio access networks (RANs) to meet stringent quality
of service (QoS) and resource efficiency requirements. Existing solutions
struggle to bridge the gap between high level user intents and the low level,
parameterized configurations required for optimal performance. To address this
challenge, we propose RIDAS, a multi agent framework composed of representation
driven agents (RDAs) and an intention driven agent (IDA). RDAs expose open
interface with tunable control parameters (rank and quantization bits, enabling
explicit trade) offs between distortion and transmission rate. The IDA employs
a two stage planning scheme (bandwidth pre allocation and reallocation) driven
by a large language model (LLM) to map user intents and system state into
optimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\%
more users than WirelessAgent under equivalent QoS constraints. These results
validate ability of RIDAS to capture user intent and allocate resources more
efficiently in AI RAN environments.

</details>
