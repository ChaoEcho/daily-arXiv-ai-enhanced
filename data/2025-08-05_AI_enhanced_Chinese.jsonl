{"id": "2508.00864", "pdf": "https://arxiv.org/pdf/2508.00864", "abs": "https://arxiv.org/abs/2508.00864", "authors": ["Margarita Bugue\u00f1o", "Gerard de Melo"], "title": "Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches", "categories": ["cs.CL"], "comment": "7 pages, 3 figures, 3 tables. Appendix starts on page 10", "summary": "In document classification, graph-based models effectively capture document\nstructure, overcoming sequence length limitations and enhancing contextual\nunderstanding. However, most existing graph document representations rely on\nheuristics, domain-specific rules, or expert knowledge. Unlike previous\napproaches, we propose a method to learn data-driven graph structures,\neliminating the need for manual design and reducing domain dependence. Our\napproach constructs homogeneous weighted graphs with sentences as nodes, while\nedges are learned via a self-attention model that identifies dependencies\nbetween sentence pairs. A statistical filtering strategy aims to retain only\nstrongly correlated sentences, improving graph quality while reducing the graph\nsize. Experiments on three document classification datasets demonstrate that\nlearned graphs consistently outperform heuristic-based graphs, achieving higher\naccuracy and $F_1$ score. Furthermore, our study demonstrates the effectiveness\nof the statistical filtering in improving classification robustness. These\nresults highlight the potential of automatic graph generation over traditional\nheuristic approaches and open new directions for broader applications in NLP.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u56fe\u7ed3\u6784\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u6587\u6863\u5206\u7c7b\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u6a21\u578b\u5b66\u4e60\u53e5\u5b50\u95f4\u4f9d\u8d56\uff0c\u5e76\u91c7\u7528\u7edf\u8ba1\u8fc7\u6ee4\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6587\u6863\u5206\u7c7b\u4e2d\u57fa\u4e8e\u56fe\u7684\u8868\u793a\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u89c4\u5219\u3001\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u6216\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u9650\u5236\u4e86\u5176\u7ed3\u6784\u6355\u6349\u80fd\u529b\u548c\u901a\u7528\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u6784\u5efa\u4ee5\u53e5\u5b50\u4e3a\u8282\u70b9\u7684\u540c\u8d28\u52a0\u6743\u56fe\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u6a21\u578b\u5b66\u4e60\u53e5\u5b50\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u4f5c\u4e3a\u8fb9\u3002\u540c\u65f6\uff0c\u91c7\u7528\u7edf\u8ba1\u8fc7\u6ee4\u7b56\u7565\u4fdd\u7559\u5f3a\u76f8\u5173\u53e5\u5b50\uff0c\u4ee5\u4f18\u5316\u56fe\u8d28\u91cf\u5e76\u51cf\u5c11\u56fe\u89c4\u6a21\u3002", "result": "\u5728\u4e09\u4e2a\u6587\u6863\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5b66\u4e60\u578b\u56fe\u59cb\u7ec8\u4f18\u4e8e\u542f\u53d1\u5f0f\u56fe\uff0c\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u548cF1\u5206\u6570\u3002\u6b64\u5916\uff0c\u7edf\u8ba1\u8fc7\u6ee4\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u5206\u7c7b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u81ea\u52a8\u56fe\u751f\u6210\u76f8\u8f83\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.00889", "pdf": "https://arxiv.org/pdf/2508.00889", "abs": "https://arxiv.org/abs/2508.00889", "authors": ["Hagyeong Shin", "Binoy Robin Dalal", "Iwona Bialynicka-Birula", "Navjot Matharu", "Ryan Muir", "Xingwei Yang", "Samuel W. K. Wong"], "title": "FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted for an oral presentation at Agentic & GenAI Evaluation KDD\n  2025: KDD workshop on Evaluation and Trustworthiness of Agentic and\n  Generative AI Models", "summary": "Large language models (LLMs) are known to hallucinate, producing natural\nlanguage outputs that are not grounded in the input, reference materials, or\nreal-world knowledge. In enterprise applications where AI features support\nbusiness decisions, such hallucinations can be particularly detrimental. LLMs\nthat analyze and summarize contact center conversations introduce a unique set\nof challenges for factuality evaluation, because ground-truth labels often do\nnot exist for analytical interpretations about sentiments captured in the\nconversation and root causes of the business problems. To remedy this, we first\nintroduce a \\textbf{3D} -- \\textbf{Decompose, Decouple, Detach} -- paradigm in\nthe human annotation guideline and the LLM-judges' prompt to ground the\nfactuality labels in linguistically-informed evaluation criteria. We then\nintroduce \\textbf{FECT}, a novel benchmark dataset for \\textbf{F}actuality\n\\textbf{E}valuation of Interpretive AI-Generated \\textbf{C}laims in Contact\nCenter Conversation \\textbf{T}ranscripts, labeled under our 3D paradigm.\nLastly, we report our findings from aligning LLM-judges on the 3D paradigm.\nOverall, our findings contribute a new approach for automatically evaluating\nthe factuality of outputs generated by an AI system for analyzing contact\ncenter conversations.", "AI": {"tldr": "\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u547c\u53eb\u4e2d\u5fc3\u5bf9\u8bdd\u5206\u6790\u4e2d\u666e\u904d\u5b58\u5728\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c3D\u201d\uff08\u5206\u89e3\u3001\u89e3\u8026\u3001\u5206\u79bb\uff09\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6FECT\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6b64\u65b9\u6cd5\uff0c\u4e3a\u81ea\u52a8\u8bc4\u4f30LLM\u751f\u6210\u7ed3\u679c\u7684\u4e8b\u5b9e\u6027\u63d0\u4f9b\u65b0\u9014\u5f84\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7b\u89c9\u95ee\u9898\u5728\u4f01\u4e1a\u5e94\u7528\uff08\u5982\u652f\u6301\u4e1a\u52a1\u51b3\u7b56\uff09\u4e2d\u53ef\u80fd\u5e26\u6765\u4e25\u91cd\u540e\u679c\u3002\u5177\u4f53\u5230\u547c\u53eb\u4e2d\u5fc3\u5bf9\u8bdd\u5206\u6790\u548c\u6458\u8981\u573a\u666f\uff0c\u7531\u4e8e\u7f3a\u4e4f\u5bf9\u60c5\u611f\u548c\u4e1a\u52a1\u95ee\u9898\u6839\u672c\u539f\u56e0\u7b49\u89e3\u91ca\u6027\u5206\u6790\u7684\u771f\u5b9e\u6807\u7b7e\uff0c\u8bc4\u4f30LLM\u8f93\u51fa\u7684\u4e8b\u5b9e\u6027\u9762\u4e34\u72ec\u7279\u6311\u6218\u3002", "method": "1. \u5f15\u5165\u4e86\u201c3D\u201d\uff08Decompose, Decouple, Detach\uff09\u8303\u5f0f\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u4eba\u5de5\u6807\u6ce8\u6307\u5357\u548cLLM\u8bc4\u5224\u8005\u7684\u63d0\u793a\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u57fa\u4e8e\u8bed\u8a00\u5b66\u7684\u771f\u5b9e\u6027\u8bc4\u4f30\u30022. \u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6FECT\uff08\u547c\u53eb\u4e2d\u5fc3\u5bf9\u8bdd\u8f6c\u5f55\u4e2d\u89e3\u91ca\u6027AI\u751f\u6210\u58f0\u660e\u4e8b\u5b9e\u6027\u8bc4\u4f30\u6570\u636e\u96c6\uff09\uff0c\u5e76\u4f7f\u75283D\u8303\u5f0f\u8fdb\u884c\u6807\u6ce8\u30023. \u62a5\u544a\u4e86\u57283D\u8303\u5f0f\u4e0b\uff0cLLM\u8bc4\u5224\u8005\u5bf9\u9f50\u7684\u76f8\u5173\u53d1\u73b0\u3002", "result": "\u7814\u7a76\u62a5\u544a\u4e86\u5728\u201c3D\u201d\u8303\u5f0f\u4e0b\uff0c\u5982\u4f55\u4f7fLLM\u8bc4\u5224\u8005\u5bf9\u9f50\u5176\u4e8b\u5b9e\u6027\u8bc4\u4f30\uff0c\u5e76\u5c55\u793a\u4e86\u76f8\u5173\u7684\u53d1\u73b0\u3002\u8fd9\u4e9b\u53d1\u73b0\u5171\u540c\u4e3a\u81ea\u52a8\u8bc4\u4f30AI\u7cfb\u7edf\u5728\u547c\u53eb\u4e2d\u5fc3\u5bf9\u8bdd\u5206\u6790\u4e2d\u751f\u6210\u8f93\u51fa\u7684\u4e8b\u5b9e\u6027\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u53d1\u73b0\u4e3a\u81ea\u52a8\u8bc4\u4f30AI\u7cfb\u7edf\u5728\u5206\u6790\u547c\u53eb\u4e2d\u5fc3\u5bf9\u8bdd\u65f6\u751f\u6210\u8f93\u51fa\u7684\u4e8b\u5b9e\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.00924", "pdf": "https://arxiv.org/pdf/2508.00924", "abs": "https://arxiv.org/abs/2508.00924", "authors": ["Ernesto L. Estevanell-Valladares", "Suilan Estevez-Velarde", "Yoan Guti\u00e9rrez", "Andr\u00e9s Montoyo", "Ruslan Mitkov"], "title": "XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML", "categories": ["cs.CL", "68T05, 68T50", "I.2.6; I.2.7; I.2.8"], "comment": "17 pages, 10 figures, 7 tables. Preprint. Under review at EMNLP 2025.\n  This is not the final version", "summary": "Experts in machine learning leverage domain knowledge to navigate decisions\nin model selection, hyperparameter optimisation, and resource allocation. This\nis particularly critical for fine-tuning language models (LMs), where repeated\ntrials incur substantial computational overhead and environmental impact.\nHowever, no existing automated framework simultaneously tackles the entire\nmodel selection and HPO task for resource-efficient LM fine-tuning. We\nintroduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past\nexperiences to optimise discriminative and generative LM fine-tuning pipelines\nefficiently. XAutoLM learns from stored successes and failures by extracting\ntask- and system-level meta-features to bias its sampling toward fruitful\nconfigurations and away from costly dead ends. On four text classification and\ntwo question-answering benchmarks, XAutoLM surpasses zero-shot optimiser's peak\nF1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error\nratios by up to sevenfold, and uncovers up to 50% more pipelines above the\nzero-shot Pareto front. In contrast, simpler memory-based baselines suffer\nnegative transfer. We release XAutoLM and our experience store to catalyse\nresource-efficient, Green AI fine-tuning in the NLP community.", "AI": {"tldr": "XAutoLM\u662f\u4e00\u4e2a\u5143\u5b66\u4e60\u589e\u5f3a\u7684AutoML\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u7528\u8fc7\u5f80\u7ecf\u9a8c\u9ad8\u6548\u4f18\u5316\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u5fae\u8c03\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u9ad8\u6027\u80fd\uff0c\u52a9\u529b\u5b9e\u73b0\u7eff\u8272AI\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u6a21\u578b\u9009\u62e9\u548c\u8d85\u53c2\u6570\u4f18\u5316\u9ad8\u5ea6\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u4e14\u91cd\u590d\u8bd5\u9a8c\u5bfc\u81f4\u5de8\u5927\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u73af\u5883\u5f71\u54cd\u3002\u73b0\u6709\u81ea\u52a8\u5316\u6846\u67b6\u65e0\u6cd5\u5168\u9762\u89e3\u51b3\u8d44\u6e90\u9ad8\u6548\u7684LM\u5fae\u8c03\u4efb\u52a1\u3002", "method": "\u5f15\u5165XAutoLM\uff0c\u4e00\u4e2a\u5143\u5b66\u4e60\u589e\u5f3a\u7684AutoML\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u63d0\u53d6\u4efb\u52a1\u548c\u7cfb\u7edf\u7ea7\u522b\u7684\u5143\u7279\u5f81\uff0c\u4ece\u8fc7\u5f80\u7684\u6210\u529f\u4e0e\u5931\u8d25\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u5f15\u5bfc\u91c7\u6837\u504f\u5411\u6709\u6548\u914d\u7f6e\uff0c\u4ece\u800c\u9ad8\u6548\u4f18\u5316\u5224\u522b\u5f0f\u548c\u751f\u6210\u5f0fLM\u7684\u5fae\u8c03\u6d41\u7a0b\u3002", "result": "\u5728\u56db\u9879\u6587\u672c\u5206\u7c7b\u548c\u4e24\u9879\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cXAutoLM\u5728\u516d\u9879\u4efb\u52a1\u4e2d\u7684\u4e94\u9879\u4e0a\u8d85\u8d8a\u4e86\u96f6\u6837\u672c\u4f18\u5316\u5668\u7684\u5cf0\u503cF1\u5206\u6570\uff0c\u5e73\u5747\u8bc4\u4f30\u65f6\u95f4\u7f29\u77ed\u9ad8\u8fbe4.5\u500d\uff0c\u9519\u8bef\u7387\u964d\u4f4e\u9ad8\u8fbe\u4e03\u500d\uff0c\u5e76\u53d1\u73b0\u6bd4\u96f6\u6837\u672cPareto\u524d\u6cbf\u591a\u51fa50%\u7684\u7ba1\u9053\u3002\u7b80\u5355\u7684\u57fa\u4e8e\u8bb0\u5fc6\u7684\u57fa\u7ebf\u5219\u51fa\u73b0\u8d1f\u8fc1\u79fb\u3002", "conclusion": "XAutoLM\u4fc3\u8fdb\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u793e\u533a\u4e2d\u8d44\u6e90\u9ad8\u6548\u7684\u7eff\u8272AI\u5fae\u8c03\u3002\u4f5c\u8005\u5df2\u53d1\u5e03XAutoLM\u53ca\u5176\u7ecf\u9a8c\u5b58\u50a8\u5e93\u3002"}}
{"id": "2508.01047", "pdf": "https://arxiv.org/pdf/2508.01047", "abs": "https://arxiv.org/abs/2508.01047", "authors": ["Efe A\u011flamazlar", "Emirhan Eken", "Harun Batur Ge\u00e7ici"], "title": "A Deep Reinforcement Learning-Based TCP Congestion Control Algorithm: Design, Simulation, and Evaluation", "categories": ["cs.NI", "cs.AI", "C.2.2; I.2.6; I.2.8"], "comment": "This paper presents a novel TCP congestion control algorithm based on\n  Deep Reinforcement Learning. The study includes 5 figures and 8 pages of\n  content", "summary": "This paper presents a novel TCP congestion control algorithm based on Deep\nReinforcement Learning. The proposed approach utilizes Deep Q-Networks to\noptimize the congestion window (cWnd) by observing key network parameters and\ntaking real-time actions. The algorithm is trained and evaluated within the\nNS-3 network simulator using the OpenGym interface. The results demonstrate\nsignificant improvements over traditional TCP New Reno in terms of latency and\nthroughput, with better adaptability to changing network conditions. This study\nemphasizes the potential of reinforcement learning techniques for solving\ncomplex congestion control problems in modern networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u578bTCP\u62e5\u585e\u63a7\u5236\u7b97\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u62e5\u585e\u7a97\u53e3\u6765\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edfTCP\u62e5\u585e\u63a7\u5236\u7b97\u6cd5\uff08\u5982New Reno\uff09\u5728\u73b0\u4ee3\u7f51\u7edc\u590d\u6742\u591a\u53d8\u7684\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u590d\u6742\u7684\u62e5\u585e\u63a7\u5236\u95ee\u9898\uff0c\u4e9f\u9700\u66f4\u5177\u9002\u5e94\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\u7684TCP\u62e5\u585e\u63a7\u5236\u7b97\u6cd5\uff0c\u901a\u8fc7\u89c2\u5bdf\u5173\u952e\u7f51\u7edc\u53c2\u6570\u5e76\u91c7\u53d6\u5b9e\u65f6\u884c\u52a8\u6765\u4f18\u5316\u62e5\u585e\u7a97\u53e3\uff08cWnd\uff09\u3002\u7b97\u6cd5\u5728NS-3\u7f51\u7edc\u6a21\u62df\u5668\u4e2d\u5229\u7528OpenGym\u63a5\u53e3\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edfTCP New Reno\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u53d8\u5316\u7f51\u7edc\u6761\u4ef6\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u5728\u89e3\u51b3\u73b0\u4ee3\u7f51\u7edc\u590d\u6742\u62e5\u585e\u63a7\u5236\u95ee\u9898\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.01005", "pdf": "https://arxiv.org/pdf/2508.01005", "abs": "https://arxiv.org/abs/2508.01005", "authors": ["Yiqun Chen", "Erhan Zhang", "Lingyong Yan", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin", "Jiaxin Mao"], "title": "MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has\nbecome pivotal in enhancing response accuracy and reducing hallucination\nissues. The architecture of RAG systems varies significantly, encompassing\nsingle-round RAG, iterative RAG, and reasoning RAG, each tailored to address\ndifferent types of queries. Due to the varying complexity of real-world\nqueries, a fixed RAG pipeline often struggles to balance performance and cost\nefficiency across different queries. To address this challenge, we propose an\nadaptive RAG framework called MAO-ARAG, which leverages multi-agent\norchestration. Our adaptive RAG is conceived as a multi-turn framework.\nSpecifically, we define multiple executor agents, representing typical RAG\nmodules such as query reformulation agents, document selection agent, and\ngeneration agents. A planner agent intelligently selects and integrates the\nappropriate agents from these executors into a suitable workflow tailored for\neach query, striving for high-quality answers while maintaining reasonable\ncosts. During each turn, the planner agent is trained using reinforcement\nlearning, guided by an outcome-based reward (F1 score) and a cost-based\npenalty, continuously improving answer quality while keeping costs within a\nreasonable range. Experiments conducted on multiple QA datasets demonstrate\nthat our approach, which dynamically plans workflows for each query, not only\nachieves high answer quality but also maintains both cost and latency within\nacceptable limits.The code of MAO-ARAG is on\nhttps://github.com/chenyiqun/Agentic-RAG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMAO-ARAG\uff0c\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7f16\u6392\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94RAG\u6846\u67b6\uff0c\u5b83\u80fd\u52a8\u6001\u89c4\u5212RAG\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ee5\u5e73\u8861\u67e5\u8be2\u7b54\u6848\u8d28\u91cf\u548c\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u67b6\u6784\u56fa\u5b9a\uff0c\u96be\u4ee5\u5728\u5904\u7406\u590d\u6742\u591a\u6837\u7684\u771f\u5b9e\u4e16\u754c\u67e5\u8be2\u65f6\u517c\u987e\u6027\u80fd\u548c\u6210\u672c\u6548\u7387\u3002", "method": "MAO-ARAG\u662f\u4e00\u4e2a\u591a\u8f6e\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u5305\u542b\uff1a1) \u6267\u884c\u5668\u667a\u80fd\u4f53\uff0c\u4ee3\u8868\u5178\u578b\u7684RAG\u6a21\u5757\uff08\u5982\u67e5\u8be2\u91cd\u6784\u3001\u6587\u6863\u9009\u62e9\u3001\u751f\u6210\uff09\u30022) \u89c4\u5212\u5668\u667a\u80fd\u4f53\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08\u4ee5F1\u5206\u6570\u4f5c\u4e3a\u5956\u52b1\uff0c\u6210\u672c\u4f5c\u4e3a\u60e9\u7f5a\uff09\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u667a\u80fd\u9009\u62e9\u548c\u6574\u5408\u6267\u884c\u5668\u667a\u80fd\u4f53\uff0c\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7b54\u6848\u5e76\u63a7\u5236\u6210\u672c\u3002", "result": "\u5728\u591a\u4e2a\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMAO-ARAG\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u9ad8\u7b54\u6848\u8d28\u91cf\uff0c\u800c\u4e14\u5c06\u6210\u672c\u548c\u5ef6\u8fdf\u63a7\u5236\u5728\u53ef\u63a5\u53d7\u7684\u8303\u56f4\u5185\u3002", "conclusion": "MAO-ARAG\u901a\u8fc7\u5176\u591a\u667a\u80fd\u4f53\u7f16\u6392\u548c\u5f3a\u5316\u5b66\u4e60\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86RAG\u7cfb\u7edf\u5728\u5904\u7406\u591a\u6837\u5316\u67e5\u8be2\u65f6\u5e73\u8861\u6027\u80fd\u4e0e\u6210\u672c\u6548\u7387\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u4f4e\u6210\u672c\u548c\u4f4e\u5ef6\u8fdf\u7684\u95ee\u7b54\u8868\u73b0\u3002"}}
{"id": "2508.01060", "pdf": "https://arxiv.org/pdf/2508.01060", "abs": "https://arxiv.org/abs/2508.01060", "authors": ["Ibrahim Althamary", "Chen-Fu Chou", "Chih-Wei Huang"], "title": "Connectivity Management in Satellite-Aided Vehicular Networks with Multi-Head Attention-Based State Estimation", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "Managing connectivity in integrated satellite-terrestrial vehicular networks\nis critical for 6G, yet is challenged by dynamic conditions and partial\nobservability. This letter introduces the Multi-Agent Actor-Critic with\nSatellite-Aided Multi-head self-attention (MAAC-SAM), a novel multi-agent\nreinforcement learning framework that enables vehicles to autonomously manage\nconnectivity across Vehicle-to-Satellite (V2S), Vehicle-to-Infrastructure\n(V2I), and Vehicle-to-Vehicle (V2V) links. Our key innovation is the\nintegration of a multi-head attention mechanism, which allows for robust state\nestimation even with fluctuating and limited information sharing among\nvehicles. The framework further leverages self-imitation learning (SIL) and\nfingerprinting to improve learning efficiency and real-time decisions.\nSimulation results, based on realistic SUMO traffic models and 3GPP-compliant\nconfigurations, demonstrate that MAAC-SAM outperforms state-of-the-art\nterrestrial and satellite-assisted baselines by up to 14% in transmission\nutility and maintains high estimation accuracy across varying vehicle densities\nand sharing levels.", "AI": {"tldr": "\u9488\u5bf96G\u661f\u5730\u4e00\u4f53\u5316\u8f66\u8054\u7f51\u8fde\u63a5\u7ba1\u7406\u96be\u9898\uff0c\u672c\u6587\u63d0\u51faMAAC-SAM\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u3001\u81ea\u6a21\u4eff\u5b66\u4e60\u548c\u6307\u7eb9\u8bc6\u522b\uff0c\u5b9e\u73b0\u81ea\u4e3b\u8fde\u63a5\u7ba1\u7406\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4f20\u8f93\u6548\u7528\u548c\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u57286G\u661f\u5730\u4e00\u4f53\u5316\u8f66\u8054\u7f51\u4e2d\uff0c\u8fde\u63a5\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u52a8\u6001\u6761\u4ef6\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e86MAAC-SAM\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u8f66\u8f86\u80fd\u591f\u81ea\u4e3b\u7ba1\u7406V2S\u3001V2I\u548cV2V\u94fe\u8def\u7684\u8fde\u63a5\u3002\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u96c6\u6210\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u5b9e\u73b0\u9c81\u68d2\u72b6\u6001\u4f30\u8ba1\uff0c\u5e76\u5229\u7528\u81ea\u6a21\u4eff\u5b66\u4e60\uff08SIL\uff09\u548c\u6307\u7eb9\u8bc6\u522b\u6765\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u5b9e\u65f6\u51b3\u7b56\u3002", "result": "\u57fa\u4e8eSUMO\u6d41\u91cf\u6a21\u578b\u548c3GPP\u914d\u7f6e\u7684\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0cMAAC-SAM\u5728\u4f20\u8f93\u6548\u7528\u4e0a\u6bd4\u73b0\u6709\u57fa\u7ebf\u63d0\u9ad8\u4e8614%\uff0c\u5e76\u5728\u4e0d\u540c\u8f66\u8f86\u5bc6\u5ea6\u548c\u5171\u4eab\u6c34\u5e73\u4e0b\u4fdd\u6301\u4e86\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "MAAC-SAM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e866G\u661f\u5730\u4e00\u4f53\u5316\u8f66\u8054\u7f51\u4e2d\u7684\u8fde\u63a5\u7ba1\u7406\u6311\u6218\uff0c\u901a\u8fc7\u5176\u521b\u65b0\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u548c\u72b6\u6001\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u5bf9\u672a\u6765\u8f66\u8054\u7f51\u53d1\u5c55\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.00835", "pdf": "https://arxiv.org/pdf/2508.00835", "abs": "https://arxiv.org/abs/2508.00835", "authors": ["Zachary T. Rewolinski", "Bin Yu"], "title": "PCS Workflow for Veridical Data Science in the Age of AI", "categories": ["cs.LG", "cs.AI", "stat.ME"], "comment": null, "summary": "Data science is a pillar of artificial intelligence (AI), which is\ntransforming nearly every domain of human activity, from the social and\nphysical sciences to engineering and medicine. While data-driven findings in AI\noffer unprecedented power to extract insights and guide decision-making, many\nare difficult or impossible to replicate. A key reason for this challenge is\nthe uncertainty introduced by the many choices made throughout the data science\nlife cycle (DSLC). Traditional statistical frameworks often fail to account for\nthis uncertainty. The Predictability-Computability-Stability (PCS) framework\nfor veridical (truthful) data science offers a principled approach to\naddressing this challenge throughout the DSLC. This paper presents an updated\nand streamlined PCS workflow, tailored for practitioners and enhanced with\nguided use of generative AI. We include a running example to display the PCS\nframework in action, and conduct a related case study which showcases the\nuncertainty in downstream predictions caused by judgment calls in the data\ncleaning stage.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9AI\u7814\u7a76\u7ed3\u679c\u96be\u4ee5\u590d\u73b0\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u5e76\u66f4\u65b0\u4e86PCS\u6846\u67b6\u5de5\u4f5c\u6d41\uff0c\u65e8\u5728\u901a\u8fc7\u878d\u5408\u751f\u6210\u5f0fAI\u6765\u89e3\u51b3\u6570\u636e\u79d1\u5b66\u751f\u547d\u5468\u671f\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684\u6570\u636e\u79d1\u5b66\u3002", "motivation": "AI\u6570\u636e\u9a71\u52a8\u7684\u53d1\u73b0\u867d\u5177\u5f3a\u5927\u6d1e\u5bdf\u529b\uff0c\u4f46\u56e0\u6570\u636e\u79d1\u5b66\u751f\u547d\u5468\u671f\uff08DSLC\uff09\u4e2d\u9009\u62e9\u5f15\u5165\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u5176\u7ed3\u679c\u5e38\u96be\u4ee5\u590d\u73b0\uff0c\u4e14\u4f20\u7edf\u7edf\u8ba1\u6846\u67b6\u65e0\u6cd5\u6709\u6548\u5904\u7406\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4e3a\u5b9e\u8df5\u8005\u91cf\u8eab\u5b9a\u5236\u3001\u5e76\u7ed3\u5408\u751f\u6210\u5f0fAI\u7684\u66f4\u65b0\u4e14\u7b80\u5316\u7684\u53ef\u9884\u6d4b\u6027-\u53ef\u8ba1\u7b97\u6027-\u7a33\u5b9a\u6027\uff08PCS\uff09\u6846\u67b6\u5de5\u4f5c\u6d41\uff0c\u4ee5\u5e94\u5bf9DSLC\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u6311\u6218\u3002", "result": "\u8bba\u6587\u901a\u8fc7\u4e00\u4e2a\u8fd0\u884c\u5b9e\u4f8b\u5c55\u793a\u4e86PCS\u6846\u67b6\u7684\u5e94\u7528\uff0c\u5e76\u8fdb\u884c\u4e86\u4e00\u9879\u6848\u4f8b\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u6570\u636e\u6e05\u6d17\u9636\u6bb5\u7684\u4e3b\u89c2\u5224\u65ad\u5bf9\u540e\u7eed\u9884\u6d4b\u6240\u9020\u6210\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "PCS\u6846\u67b6\u4e3a\u89e3\u51b3\u6570\u636e\u79d1\u5b66\u751f\u547d\u5468\u671f\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684\uff08\u771f\u5b9e\u7684\uff09\u6570\u636e\u79d1\u5b66\uff0c\u5e76\u901a\u8fc7\u66f4\u65b0\u548cAI\u589e\u5f3a\u7684\u5de5\u4f5c\u6d41\u4f7f\u5176\u66f4\u9002\u7528\u4e8e\u5b9e\u8df5\u3002"}}
{"id": "2508.00834", "pdf": "https://arxiv.org/pdf/2508.00834", "abs": "https://arxiv.org/abs/2508.00834", "authors": ["Wei Wu", "Wenjie Wang", "Yang Tan", "Ying Liu", "Liang Diao", "Lin Huang", "Kaihe Xu", "Wenfeng Xie", "Ziling Lin"], "title": "Team PA-VCG's Solution for Competition on Understanding Chinese College Entrance Exam Papers in ICDAR'25", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "This report presents Team PA-VGG's solution for the ICDAR'25 Competition on\nUnderstanding Chinese College Entrance Exam Papers. In addition to leveraging\nhigh-resolution image processing and a multi-image end-to-end input strategy to\naddress the challenges of dense OCR extraction and complex document layouts in\nGaokao papers, our approach introduces domain-specific post-training\nstrategies. Experimental results demonstrate that our post-training approach\nachieves the most outstanding performance, securing first place with an\naccuracy rate of 89.6%.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Team PA-VGG\u5728ICDAR'25\u9ad8\u8003\u8003\u5377\u7406\u89e3\u7ade\u8d5b\u4e2d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5904\u7406\u3001\u591a\u56fe\u50cf\u8f93\u5165\u548c\u9886\u57df\u7279\u5b9a\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee589.6%\u7684\u51c6\u786e\u7387\u83b7\u5f97\u7b2c\u4e00\u540d\u3002", "motivation": "\u4e3a\u89e3\u51b3\u9ad8\u8003\u8003\u5377\u4e2d\u5bc6\u96c6\u7684OCR\u63d0\u53d6\u548c\u590d\u6742\u7684\u6587\u6863\u5e03\u5c40\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e86\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5904\u7406\u3001\u591a\u56fe\u50cf\u7aef\u5230\u7aef\u8f93\u5165\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u4e86\u9886\u57df\u7279\u5b9a\u540e\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5176\u540e\u8bad\u7ec3\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u51fa\u8272\u7684\u8868\u73b0\uff0c\u4ee589.6%\u7684\u51c6\u786e\u7387\u83b7\u5f97\u7ade\u8d5b\u7b2c\u4e00\u540d\u3002", "conclusion": "\u63d0\u51fa\u7684\u9886\u57df\u7279\u5b9a\u540e\u8bad\u7ec3\u65b9\u6cd5\u5728\u4e2d\u6587\u9ad8\u8003\u8003\u5377\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u5e76\u5177\u6709\u9886\u5148\u7684\u7ade\u4e89\u529b\u3002"}}
{"id": "2508.00844", "pdf": "https://arxiv.org/pdf/2508.00844", "abs": "https://arxiv.org/abs/2508.00844", "authors": ["Christopher Wissuchek", "Patrick Zschech"], "title": "Exploring Agentic Artificial Intelligence Systems: Towards a Typological Framework", "categories": ["cs.AI", "cs.ET", "cs.MA", "econ.GN", "q-fin.EC"], "comment": "Preprint accepted for archival and presentation at the Pacific-Asia\n  Conference on Information Systems (PACIS) 2025, Kuala Lumpur, Malaysia", "summary": "Artificial intelligence (AI) systems are evolving beyond passive tools into\nautonomous agents capable of reasoning, adapting, and acting with minimal human\nintervention. Despite their growing presence, a structured framework is lacking\nto classify and compare these systems. This paper develops a typology of\nagentic AI systems, introducing eight dimensions that define their cognitive\nand environmental agency in an ordinal structure. Using a multi-phase\nmethodological approach, we construct and refine this typology, which is then\nevaluated through a human-AI hybrid approach and further distilled into\nconstructed types. The framework enables researchers and practitioners to\nanalyze varying levels of agency in AI systems. By offering a structured\nperspective on the progression of AI capabilities, the typology provides a\nfoundation for assessing current systems and anticipating future developments\nin agentic AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u516b\u4e2a\u7ef4\u5ea6\u7684\u81ea\u4e3bAI\u7cfb\u7edf\u7c7b\u578b\u5b66\uff0c\u65e8\u5728\u4e3a\u65e5\u76ca\u589e\u957f\u7684\u667a\u80fd\u4ee3\u7406AI\u7cfb\u7edf\u63d0\u4f9b\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u5206\u7c7b\u548c\u6bd4\u8f83\u6846\u67b6\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u4e3bAI\u7cfb\u7edf\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u76ee\u524d\u4ecd\u7f3a\u4e4f\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u6846\u67b6\u6765\u5206\u7c7b\u548c\u6bd4\u8f83\u8fd9\u4e9b\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u65b9\u6cd5\u6784\u5efa\u5e76\u5b8c\u5584\u4e86\u5305\u542b\u516b\u4e2a\u7ef4\u5ea6\u7684\u7c7b\u578b\u5b66\uff0c\u901a\u8fc7\u4eba\u673a\u6df7\u5408\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u63d0\u70bc\u51fa\u6784\u9020\u7c7b\u578b\u3002", "result": "\u8be5\u6846\u67b6\u4f7f\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u80fd\u591f\u5206\u6790AI\u7cfb\u7edf\u4e0d\u540c\u5c42\u7ea7\u7684\u667a\u80fd\u4ee3\u7406\u80fd\u529b\u3002", "conclusion": "\u8be5\u7c7b\u578b\u5b66\u4e3a\u8bc4\u4f30\u5f53\u524dAI\u7cfb\u7edf\u548c\u9884\u6d4b\u672a\u6765\u81ea\u4e3bAI\u53d1\u5c55\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u89c6\u89d2\u548c\u57fa\u7840\u3002"}}
{"id": "2508.01006", "pdf": "https://arxiv.org/pdf/2508.01006", "abs": "https://arxiv.org/abs/2508.01006", "authors": ["Farah Adeeba", "Brian Dillon", "Hassan Sajjad", "Rajesh Bhatt"], "title": "UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu", "categories": ["cs.CL"], "comment": null, "summary": "Multilingual Large Language Models (LLMs) have shown remarkable performance\nacross various languages; however, they often include significantly less data\nfor low-resource languages such as Urdu compared to high-resource languages\nlike English. To assess the linguistic knowledge of LLMs in Urdu, we present\nthe Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) i.e. pairs of\nminimally different sentences that contrast in grammatical acceptability.\nUrBLiMP comprises 5,696 minimal pairs targeting ten core syntactic phenomena,\ncarefully curated using the Urdu Treebank and diverse Urdu text corpora. A\nhuman evaluation of UrBLiMP annotations yielded a 96.10% inter-annotator\nagreement, confirming the reliability of the dataset. We evaluate twenty\nmultilingual LLMs on UrBLiMP, revealing significant variation in performance\nacross linguistic phenomena. While LLaMA-3-70B achieves the highest average\naccuracy (94.73%), its performance is statistically comparable to other top\nmodels such as Gemma-3-27B-PT. These findings highlight both the potential and\nthe limitations of current multilingual LLMs in capturing fine-grained\nsyntactic knowledge in low-resource languages.", "AI": {"tldr": "\u4e3a\u8bc4\u4f30\u591a\u8bed\u8a00\u5927\u6a21\u578b\uff08LLMs\uff09\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u4e4c\u5c14\u90fd\u8bed\uff09\u4e2d\u7684\u8bed\u6cd5\u77e5\u8bc6\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86UrBLiMP\u6570\u636e\u96c6\uff0c\u5e76\u7528\u5176\u8bc4\u4f30\u4e8620\u4e2aLLMs\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u80fd\u529b\u4e0e\u5c40\u9650\u3002", "motivation": "\u591a\u8bed\u8a00LLMs\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u4e4c\u5c14\u90fd\u8bed\uff09\u4e0a\u7684\u8bad\u7ec3\u6570\u636e\u8fdc\u5c11\u4e8e\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u5bfc\u81f4\u5176\u5728\u8fd9\u4e9b\u8bed\u8a00\u4e2d\u7684\u8bed\u8a00\u77e5\u8bc6\u548c\u7ec6\u7c92\u5ea6\u8bed\u6cd5\u80fd\u529b\u4e9f\u5f85\u8bc4\u4f30\u3002", "method": "1. \u6784\u5efa\u4e86\u4e4c\u5c14\u90fd\u8bed\u8bed\u8a00\u6700\u5c0f\u5bf9\u57fa\u51c6\uff08UrBLiMP\uff09\u6570\u636e\u96c6\uff0c\u5305\u542b5,696\u4e2a\u9488\u5bf9\u5341\u79cd\u6838\u5fc3\u53e5\u6cd5\u73b0\u8c61\u7684\u6700\u5c0f\u5bf9\u3002 2. \u6570\u636e\u96c6\u901a\u8fc7\u4e4c\u5c14\u90fd\u8bed\u53e5\u6cd5\u6811\u5e93\u548c\u591a\u79cd\u4e4c\u5c14\u90fd\u8bed\u6587\u672c\u8bed\u6599\u5e93\u7cbe\u5fc3\u7b56\u5212\uff0c\u5e76\u7ecf\u4eba\u5de5\u8bc4\u4f30\u786e\u8ba4\uff0c\u6ce8\u91ca\u8005\u95f4\u4e00\u81f4\u6027\u8fbe96.10%\u3002 3. \u4f7f\u7528UrBLiMP\u8bc4\u4f30\u4e86\u4e8c\u5341\u4e2a\u591a\u8bed\u8a00LLMs\u3002", "result": "1. \u4e0d\u540cLLMs\u5728\u4e0d\u540c\u8bed\u8a00\u73b0\u8c61\u4e0a\u7684\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002 2. LLaMA-3-70B\u83b7\u5f97\u6700\u9ad8\u5e73\u5747\u51c6\u786e\u7387\uff0894.73%\uff09\uff0c\u4f46\u5176\u8868\u73b0\u4e0eGemma-3-27B-PT\u7b49\u5176\u4ed6\u9876\u7ea7\u6a21\u578b\u5728\u7edf\u8ba1\u5b66\u4e0a\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u5f53\u524d\u591a\u8bed\u8a00LLMs\u5728\u6355\u83b7\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7ec6\u7c92\u5ea6\u53e5\u6cd5\u77e5\u8bc6\u65b9\u9762\uff0c\u65e2\u5c55\u793a\u51fa\u6f5c\u529b\uff0c\u4e5f\u66b4\u9732\u51fa\u5c40\u9650\u6027\u3002"}}
{"id": "2508.01298", "pdf": "https://arxiv.org/pdf/2508.01298", "abs": "https://arxiv.org/abs/2508.01298", "authors": ["Azadeh Sadat Miraftab", "Ahmadreza Montazerolghaem", "Behrad Mahboobi"], "title": "Improving performance of content-centric networks via decentralized coded caching for multi-level popularity and access", "categories": ["cs.NI"], "comment": null, "summary": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u53bb\u4e2d\u5fc3\u5316\u7f16\u7801\u7f13\u5b58\uff08DCC\uff09\u96c6\u6210\u5230\u5185\u5bb9\u4e2d\u5fc3\u7f51\u7edc\uff08CCN\uff09\u4e2d\uff0c\u901a\u8fc7\u5dee\u5f02\u5316\u7f16\u7801\u548c\u672a\u7f16\u7801\u6570\u636e\u518d\u7f16\u7801\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u7f51\u7edc\u541e\u5410\u91cf\u3001\u7f13\u5b58\u547d\u4e2d\u7387\u5e76\u964d\u4f4e\u5185\u5bb9\u4ea4\u4ed8\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u7684IP\u7f51\u7edc\u5b58\u5728\u5c40\u9650\u6027\uff0cCCN\u65e8\u5728\u901a\u8fc7\u5185\u5bb9\u6807\u8bc6\u76f4\u63a5\u5bfb\u5740\u63d0\u5347\u6027\u80fd\u3002\u4e3a\u8fdb\u4e00\u6b65\u4f18\u5316CCN\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u5cf0\u6d41\u91cf\u671f\u95f4\u7f13\u89e3\u5185\u5bb9\u4f20\u8f93\u901f\u7387\uff0c\u9700\u8981\u6539\u8fdb\u7f16\u7801\u6570\u636e\u5b58\u50a8\u548c\u4f20\u8f93\u7b56\u7565\uff0cDCC\u88ab\u8ba4\u4e3a\u662f\u89e3\u51b3\u6b64\u95ee\u9898\u7684\u6709\u6548\u6280\u672f\u3002", "method": "\u901a\u8fc7\u5c06\u53bb\u4e2d\u5fc3\u5316\u7f16\u7801\u7f13\u5b58\uff08DCC\uff09\u96c6\u6210\u5230CCN\u67b6\u6784\u4e2d\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5f15\u5165\u57fa\u4e8e\u7528\u6237\u8bbf\u95ee\u6743\u9650\u7684\u5dee\u5f02\u5316\u7f16\u7801\u7b56\u7565\uff0c\u4ee5\u6d88\u9664\u57fa\u4e8e\u961f\u5217\u641c\u7d22\u7684\u5f00\u9500\u30022) \u4fc3\u8fdb\u5728\u5185\u5bb9\u4ea4\u4ed8\u8def\u5f84\u4e0a\u5bf9\u9047\u5230\u7684\u672a\u7f16\u7801\u6570\u636e\u8fdb\u884c\u518d\u7f16\u7801\u3002", "result": "\u4e0e\u4f20\u7edfCCN\u5b9e\u73b0\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u541e\u5410\u91cf\uff0c\u63d0\u9ad8\u4e86\u7f13\u5b58\u547d\u4e2d\u7387\uff0c\u5e76\u56e0\u6b64\u964d\u4f4e\u4e86\u5185\u5bb9\u4ea4\u4ed8\u5ef6\u8fdf\u3002", "conclusion": "\u7ed3\u5408DCC\u3001\u5dee\u5f02\u5316\u7f16\u7801\u548c\u518d\u7f16\u7801\u7b56\u7565\u7684CCN\u67b6\u6784\u6539\u8fdb\uff0c\u80fd\u6709\u6548\u589e\u5f3a\u7f51\u7edc\u6027\u80fd\uff0c\u63d0\u5347\u5185\u5bb9\u4ea4\u4ed8\u6548\u7387\u3002"}}
{"id": "2508.00855", "pdf": "https://arxiv.org/pdf/2508.00855", "abs": "https://arxiv.org/abs/2508.00855", "authors": ["Ziyang Zhang", "Feifan Zhang", "Weidong Tang", "Lei Shi", "Tailai Chen"], "title": "A Residual Guided strategy with Generative Adversarial Networks in training Physics-Informed Transformer Networks", "categories": ["cs.LG", "cs.CE", "physics.flu-dyn"], "comment": null, "summary": "Nonlinear partial differential equations (PDEs) are pivotal in modeling\ncomplex physical systems, yet traditional Physics-Informed Neural Networks\n(PINNs) often struggle with unresolved residuals in critical spatiotemporal\nregions and violations of temporal causality. To address these limitations, we\npropose a novel Residual Guided Training strategy for Physics-Informed\nTransformer via Generative Adversarial Networks (GAN). Our framework integrates\na decoder-only Transformer to inherently capture temporal correlations through\nautoregressive processing, coupled with a residual-aware GAN that dynamically\nidentifies and prioritizes high-residual regions. By introducing a causal\npenalty term and an adaptive sampling mechanism, the method enforces temporal\ncausality while refining accuracy in problematic domains. Extensive numerical\nexperiments on the Allen-Cahn, Klein-Gordon, and Navier-Stokes equations\ndemonstrate significant improvements, achieving relative MSE reductions of up\nto three orders of magnitude compared to baseline methods. This work bridges\nthe gap between deep learning and physics-driven modeling, offering a robust\nsolution for multiscale and time-dependent PDE systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u6b8b\u5dee\u5f15\u5bfc\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u4e8e\u7269\u7406\u4fe1\u606fTransformer\uff0c\u4ee5\u89e3\u51b3\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\u4e2d\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u7684\u6b8b\u5dee\u548c\u65f6\u95f4\u56e0\u679c\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6c42\u89e3\u7cbe\u5ea6\u3002", "motivation": "\u975e\u7ebf\u6027\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\u5728\u590d\u6742\u7269\u7406\u7cfb\u7edf\u5efa\u6a21\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u5728\u5173\u952e\u65f6\u7a7a\u533a\u57df\u5e38\u9762\u4e34\u6b8b\u5dee\u672a\u89e3\u51b3\u548c\u65f6\u95f4\u56e0\u679c\u6027\u8fdd\u53cd\u7684\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u6b8b\u5dee\u5f15\u5bfc\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u4e8e\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u8bad\u7ec3\u7684\u7269\u7406\u4fe1\u606fTransformer\u3002\u8be5\u6846\u67b6\u6574\u5408\u4e86\u4e00\u4e2a\u89e3\u7801\u5668\u4e13\u7528Transformer\u4ee5\u901a\u8fc7\u81ea\u56de\u5f52\u5904\u7406\u6355\u83b7\u65f6\u95f4\u5173\u8054\uff0c\u5e76\u7ed3\u5408\u4e86\u4e00\u4e2a\u6b8b\u5dee\u611f\u77e5GAN\uff0c\u52a8\u6001\u8bc6\u522b\u5e76\u4f18\u5148\u5904\u7406\u9ad8\u6b8b\u5dee\u533a\u57df\u3002\u901a\u8fc7\u5f15\u5165\u56e0\u679c\u60e9\u7f5a\u9879\u548c\u81ea\u9002\u5e94\u91c7\u6837\u673a\u5236\uff0c\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u95ee\u9898\u57df\u7cbe\u5ea6\u7684\u540c\u65f6\u5f3a\u5236\u6267\u884c\u65f6\u95f4\u56e0\u679c\u6027\u3002", "result": "\u5728Allen-Cahn\u3001Klein-Gordon\u548cNavier-Stokes\u65b9\u7a0b\u4e0a\u7684\u5927\u91cf\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u76f8\u5bf9\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u964d\u4f4e\u4e86\u9ad8\u8fbe\u4e09\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f25\u5408\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e0e\u7269\u7406\u9a71\u52a8\u5efa\u6a21\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u591a\u5c3a\u5ea6\u548c\u65f6\u95f4\u4f9d\u8d56\u7684PDE\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00841", "pdf": "https://arxiv.org/pdf/2508.00841", "abs": "https://arxiv.org/abs/2508.00841", "authors": ["Ali Haitham Abdul Amir", "Zainab N. Nemer"], "title": "Inclusive Review on Advances in Masked Human Face Recognition Technologies", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Masked Face Recognition (MFR) is an increasingly important area in biometric\nrecognition technologies, especially with the widespread use of masks as a\nresult of the COVID-19 pandemic. This development has created new challenges\nfor facial recognition systems due to the partial concealment of basic facial\nfeatures. This paper aims to provide a comprehensive review of the latest\ndevelopments in the field, with a focus on deep learning techniques, especially\nconvolutional neural networks (CNNs) and twin networks (Siamese networks),\nwhich have played a pivotal role in improving the accuracy of covering face\nrecognition. The paper discusses the most prominent challenges, which include\nchanges in lighting, different facial positions, partial concealment, and the\nimpact of mask types on the performance of systems. It also reviews advanced\ntechnologies developed to overcome these challenges, including data enhancement\nusing artificial databases and multimedia methods to improve the ability of\nsystems to generalize. In addition, the paper highlights advance in deep\nnetwork design, feature extraction techniques, evaluation criteria, and data\nsets used in this area. Moreover, it reviews the various applications of masked\nface recognition in the fields of security and medicine, highlighting the\ngrowing importance of these systems in light of recurrent health crises and\nincreasing security threats. Finally, the paper focuses on future research\ntrends such as developing more efficient algorithms and integrating multimedia\ntechnologies to improve the performance of recognition systems in real-world\nenvironments and expand their applications.", "AI": {"tldr": "\u4e00\u7bc7\u5168\u9762\u7efc\u8ff0\uff0c\u5206\u6790\u4e86\u8499\u9762\u4eba\u8138\u8bc6\u522b\uff08MFR\uff09\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u5e76\u63a2\u8ba8\u5176\u6311\u6218\u3001\u89e3\u51b3\u65b9\u6848\u3001\u8bc4\u4f30\u6807\u51c6\u3001\u5e94\u7528\u53ca\u5176\u672a\u6765\u8d8b\u52bf\u3002", "motivation": "COVID-19\u75ab\u60c5\u5bfc\u81f4\u53e3\u7f69\u666e\u53ca\uff0c\u4f7f\u5f97\u8499\u9762\u4eba\u8138\u8bc6\u522b\uff08MFR\uff09\u6210\u4e3a\u751f\u7269\u8bc6\u522b\u6280\u672f\u65e5\u76ca\u91cd\u8981\u7684\u7814\u7a76\u9886\u57df\u3002\u7531\u4e8e\u57fa\u672c\u9762\u90e8\u7279\u5f81\u88ab\u906e\u6321\uff0c\u4f20\u7edf\u9762\u90e8\u8bc6\u522b\u7cfb\u7edf\u9762\u4e34\u65b0\u6311\u6218\uff0c\u56e0\u6b64\u4e9f\u9700\u5bf9\u8be5\u9886\u57df\u8fdb\u884c\u7cfb\u7edf\u6027\u56de\u987e\u548c\u6280\u672f\u63a2\u7d22\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5168\u9762\u68b3\u7406\u4e86\u8499\u9762\u4eba\u8138\u8bc6\u522b\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u3002\u91cd\u70b9\u5173\u6ce8\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u7279\u522b\u662f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u548c\u5b6a\u751f\u7f51\u7edc\u3002\u63a2\u8ba8\u5185\u5bb9\u5305\u62ec\u4e3b\u8981\u6311\u6218\uff08\u5982\u5149\u7167\u3001\u59ff\u6001\u3001\u90e8\u5206\u906e\u6321\u548c\u53e3\u7f69\u7c7b\u578b\u5f71\u54cd\uff09\u3001\u514b\u670d\u6311\u6218\u7684\u5148\u8fdb\u6280\u672f\uff08\u5982\u4eba\u5de5\u6570\u636e\u5e93\u6570\u636e\u589e\u5f3a\u548c\u591a\u5a92\u4f53\u65b9\u6cd5\uff09\u3001\u6df1\u5ea6\u7f51\u7edc\u8bbe\u8ba1\u3001\u7279\u5f81\u63d0\u53d6\u6280\u672f\u3001\u8bc4\u4f30\u6807\u51c6\u4ee5\u53ca\u76f8\u5173\u6570\u636e\u96c6\u3002", "result": "\u7efc\u8ff0\u63ed\u793a\u4e86\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff08\u5982CNNs\u548c\u5b6a\u751f\u7f51\u7edc\uff09\u5728\u63d0\u9ad8\u8499\u9762\u4eba\u8138\u8bc6\u522b\u51c6\u786e\u6027\u65b9\u9762\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u603b\u7ed3\u4e86\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u591a\u5a92\u4f53\u65b9\u6cd5\u7b49\u514b\u670d\u7cfb\u7edf\u6311\u6218\u7684\u6709\u6548\u9014\u5f84\u3002\u6b64\u5916\uff0c\u5f3a\u8c03\u4e86\u8499\u9762\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5728\u5b89\u5168\u548c\u533b\u7597\u9886\u57df\u65e5\u76ca\u589e\u957f\u7684\u5e94\u7528\u91cd\u8981\u6027\u3002", "conclusion": "\u8499\u9762\u4eba\u8138\u8bc6\u522b\u6280\u672f\u672a\u6765\u53d1\u5c55\u65b9\u5411\u5c06\u805a\u7126\u4e8e\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\uff0c\u5e76\u6574\u5408\u591a\u5a92\u4f53\u6280\u672f\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u7cfb\u7edf\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u62d3\u5c55\u5176\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2508.00853", "pdf": "https://arxiv.org/pdf/2508.00853", "abs": "https://arxiv.org/abs/2508.00853", "authors": ["Kei Itoh"], "title": "A Formal Framework for the Definition of 'State': Hierarchical Representation and Meta-Universe Interpretation", "categories": ["cs.AI", "cs.LO"], "comment": "43 pages, 8 figures, 8 Tables, in English, in Japanese", "summary": "This study aims to reinforce the theoretical foundation for diverse\nsystems--including the axiomatic definition of intelligence--by introducing a\nmathematically rigorous and unified formal structure for the concept of\n'state,' which has long been used without consensus or formal clarity. First, a\n'hierarchical state grid' composed of two axes--state depth and mapping\nhierarchy--is proposed to provide a unified notational system applicable across\nmathematical, physical, and linguistic domains. Next, the 'Intermediate\nMeta-Universe (IMU)' is introduced to enable explicit descriptions of definers\n(ourselves) and the languages we use, thereby allowing conscious meta-level\noperations while avoiding self-reference and logical inconsistency. Building on\nthis meta-theoretical foundation, this study expands inter-universal theory\nbeyond mathematics to include linguistic translation and agent integration,\nintroducing the conceptual division between macrocosm-inter-universal and\nmicrocosm-inter-universal operations for broader expressivity. Through these\ncontributions, this paper presents a meta-formal logical framework--grounded in\nthe principle of definition = state--that spans time, language, agents, and\noperations, providing a mathematically robust foundation applicable to the\ndefinition of intelligence, formal logic, and scientific theory at large.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u4e25\u8c28\u7684\u201c\u72b6\u6001\u201d\u5f62\u5f0f\u7ed3\u6784\uff0c\u901a\u8fc7\u5206\u5c42\u72b6\u6001\u7f51\u683c\u548c\u4e2d\u95f4\u5143\u5b87\u5b99\u6784\u5efa\u4e86\u4e00\u4e2a\u5143\u5f62\u5f0f\u903b\u8f91\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u667a\u80fd\u5b9a\u4e49\u548c\u66f4\u5e7f\u6cdb\u7684\u79d1\u5b66\u7406\u8bba\u63d0\u4f9b\u6570\u5b66\u4e0a\u7a33\u5065\u7684\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u5404\u79cd\u7cfb\u7edf\uff08\u5305\u62ec\u667a\u80fd\u7684\u516c\u7406\u5b9a\u4e49\uff09\u7f3a\u4e4f\u5bf9\u201c\u72b6\u6001\u201d\u6982\u5ff5\u7684\u666e\u904d\u5171\u8bc6\u548c\u5f62\u5f0f\u6e05\u6670\u6027\uff0c\u8fd9\u963b\u788d\u4e86\u5176\u7406\u8bba\u57fa\u7840\u7684\u5de9\u56fa\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u201c\u72b6\u6001\u6df1\u5ea6\u201d\u548c\u201c\u6620\u5c04\u5c42\u7ea7\u201d\u4e24\u8f74\u7684\u201c\u5206\u5c42\u72b6\u6001\u7f51\u683c\u201d\uff0c\u4ee5\u63d0\u4f9b\u8de8\u6570\u5b66\u3001\u7269\u7406\u548c\u8bed\u8a00\u9886\u57df\u7684\u7edf\u4e00\u7b26\u53f7\u7cfb\u7edf\u30022. \u5f15\u5165\u4e86\u201c\u4e2d\u95f4\u5143\u5b87\u5b99\uff08IMU\uff09\u201d\uff0c\u7528\u4e8e\u663e\u5f0f\u63cf\u8ff0\u5b9a\u4e49\u8005\u548c\u6240\u7528\u8bed\u8a00\uff0c\u4ece\u800c\u5728\u907f\u514d\u81ea\u6307\u548c\u903b\u8f91\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5143\u7ea7\u522b\u64cd\u4f5c\u30023. \u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5c06\u8de8\u5b87\u5b99\u7406\u8bba\u6269\u5c55\u5230\u8bed\u8a00\u7ffb\u8bd1\u548c\u667a\u80fd\u4f53\u96c6\u6210\uff0c\u5e76\u5f15\u5165\u4e86\u5b8f\u89c2\u5b87\u5b99\u95f4\u548c\u5fae\u89c2\u5b87\u5b99\u95f4\u64cd\u4f5c\u7684\u6982\u5ff5\u533a\u5206\u3002", "result": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee5\u201c\u5b9a\u4e49=\u72b6\u6001\u201d\u4e3a\u539f\u5219\u7684\u5143\u5f62\u5f0f\u903b\u8f91\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6db5\u76d6\u65f6\u95f4\u3001\u8bed\u8a00\u3001\u667a\u80fd\u4f53\u548c\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u667a\u80fd\u5b9a\u4e49\u3001\u5f62\u5f0f\u903b\u8f91\u548c\u5e7f\u4e49\u79d1\u5b66\u7406\u8bba\u63d0\u4f9b\u4e86\u6570\u5b66\u4e0a\u7a33\u5065\u7684\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u52a0\u5f3a\u5404\u79cd\u7cfb\u7edf\u7684\u7406\u8bba\u6839\u57fa\u3002"}}
{"id": "2508.01096", "pdf": "https://arxiv.org/pdf/2508.01096", "abs": "https://arxiv.org/abs/2508.01096", "authors": ["Michael Farag", "Patrick Halina", "Andrey Zaytsev", "Alekhya Munagala", "Imtihan Ahmed", "Junhao Wang"], "title": "Cross-Domain Web Information Extraction at Pinterest", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "The internet offers a massive repository of unstructured information, but\nit's a significant challenge to convert this into a structured format. At\nPinterest, the ability to accurately extract structured product data from\ne-commerce websites is essential to enhance user experiences and improve\ncontent distribution. In this paper, we present Pinterest's system for\nattribute extraction, which achieves remarkable accuracy and scalability at a\nmanageable cost. Our approach leverages a novel webpage representation that\ncombines structural, visual, and text modalities into a compact form,\noptimizing it for small model learning. This representation captures each\nvisible HTML node with its text, style and layout information. We show how this\nallows simple models such as eXtreme Gradient Boosting (XGBoost) to extract\nattributes more accurately than much more complex Large Language Models (LLMs)\nsuch as Generative Pre-trained Transformer (GPT). Our results demonstrate a\nsystem that is highly scalable, processing over 1,000 URLs per second, while\nbeing 1000 times more cost-effective than the cheapest GPT alternatives.", "AI": {"tldr": "Pinterest\u5f00\u53d1\u4e86\u4e00\u4e2a\u5c5e\u6027\u63d0\u53d6\u7cfb\u7edf\uff0c\u901a\u8fc7\u521b\u9896\u7684\u7f51\u9875\u8868\u793a\u65b9\u6cd5\uff0c\u4f7f\u5c0f\u578b\u6a21\u578b\u80fd\u4ee5\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u53ef\u6269\u5c55\u6027\u53ca\u4f4e\u6210\u672c\u4ece\u7535\u5546\u7f51\u7ad9\u63d0\u53d6\u7ed3\u6784\u5316\u4ea7\u54c1\u6570\u636e\uff0c\u5e76\u4f18\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u5c06\u4e92\u8054\u7f51\u4e0a\u7684\u975e\u7ed3\u6784\u5316\u4fe1\u606f\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u6781\u5177\u6311\u6218\u3002\u5bf9\u4e8ePinterest\uff0c\u4ece\u7535\u5546\u7f51\u7ad9\u51c6\u786e\u63d0\u53d6\u7ed3\u6784\u5316\u4ea7\u54c1\u6570\u636e\u5bf9\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u5185\u5bb9\u5206\u53d1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u65b0\u9896\u7684\u7f51\u9875\u8868\u793a\uff0c\u6574\u5408\u4e86\u7ed3\u6784\u3001\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u4fe1\u606f\u4e3a\u7d27\u51d1\u5f62\u5f0f\uff0c\u4f18\u5316\u5c0f\u578b\u6a21\u578b\u5b66\u4e60\u3002\u6b64\u8868\u793a\u6355\u6349\u6bcf\u4e2a\u53ef\u89c1HTML\u8282\u70b9\u7684\u6587\u672c\u3001\u6837\u5f0f\u548c\u5e03\u5c40\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528XGBoost\u7b49\u7b80\u5355\u6a21\u578b\u8fdb\u884c\u5c5e\u6027\u63d0\u53d6\u3002", "result": "\u8be5\u7cfb\u7edf\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u51c6\u786e\u6027\uff0c\u6027\u80fd\u8d85\u8d8a\u4e86GPT\u7b49\u590d\u6742\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u7cfb\u7edf\u9ad8\u5ea6\u53ef\u6269\u5c55\uff0c\u6bcf\u79d2\u5904\u7406\u8d85\u8fc71,000\u4e2aURL\uff0c\u4e14\u6210\u672c\u6548\u76ca\u6781\u9ad8\uff0c\u6bd4\u6700\u4fbf\u5b9c\u7684GPT\u66ff\u4ee3\u65b9\u6848\u4fbf\u5b9c1000\u500d\u3002", "conclusion": "\u901a\u8fc7\u521b\u65b0\u7f51\u9875\u8868\u793a\u548c\u5229\u7528\u7b80\u5355\u6a21\u578b\uff0cPinterest\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u53ef\u6269\u5c55\u6027\u4e14\u6210\u672c\u6548\u76ca\u6781\u9ad8\u7684\u5c5e\u6027\u63d0\u53d6\u7cfb\u7edf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7535\u5546\u6570\u636e\u7ed3\u6784\u5316\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u590d\u6742\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2508.01805", "pdf": "https://arxiv.org/pdf/2508.01805", "abs": "https://arxiv.org/abs/2508.01805", "authors": ["Yongjie Zeng", "Hongyang Du"], "title": "M3LLM: Model Context Protocol-aided Mixture of Vision Experts For Multimodal LLMs in Networks", "categories": ["cs.NI"], "comment": null, "summary": "Current Multimodal Large Language Models (MLLMs) rely on centralized\narchitectures and often suffer from poor alignment between the input task and\ntheir fixed visual encoding modules, which limits performance on diverse and\ndynamic visual tasks. With the increasing deployment of resource-efficient\nmodels on edge devices in wireless networks, a new opportunity emerges to\ndynamically use distributed vision experts for improved MLLM inference quality.\nTo enable this, we propose M3LLM, where the Model Context Protocol (MCP)\ncoordinates a mixture of vision experts to achieve distributed MLLMs.\nSpecifically, MCP is an open protocol that structures the input task context\ninto interpretable representations, enabling wireless network-aware\ncoordination between the central model backbone and edge-hosted vision experts.\nBased on the MCP representation, M3LLM formulates vision expert routing as a\njoint optimization problem that balances task-expert semantic compatibility and\nchannel performance. To solve the resulting gradient conflicts, we develop a\ndual-stream Soft Actor-Critic (SAC) algorithm with decoupled reward signals and\nintroduce an Adaptive Stability Enhancement Module (ASEM) based on hierarchical\nBayesian modeling to ensure effective routing. Experiments show that M3LLM\nimproves task accuracy, reduces communication cost, and enhances expert routing\nadaptability under dynamic wireless network conditions.", "AI": {"tldr": "M3LLM\u901a\u8fc7\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8def\u7531\u7b97\u6cd5\uff0c\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u534f\u8c03\u8fb9\u7f18\u89c6\u89c9\u4e13\u5bb6\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u4e2d\u5fc3\u5316\u9650\u5236\uff0c\u63d0\u9ad8\u5176\u5728\u52a8\u6001\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u6027\u80fd\u3001\u964d\u4f4e\u6210\u672c\u5e76\u589e\u5f3a\u8def\u7531\u9002\u5e94\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4f9d\u8d56\u4e2d\u5fc3\u5316\u67b6\u6784\uff0c\u5176\u56fa\u5b9a\u7684\u89c6\u89c9\u7f16\u7801\u6a21\u5757\u4e0e\u8f93\u5165\u4efb\u52a1\u5bf9\u9f50\u6027\u5dee\uff0c\u9650\u5236\u4e86\u5728\u591a\u6837\u5316\u52a8\u6001\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u968f\u7740\u8d44\u6e90\u9ad8\u6548\u6a21\u578b\u5728\u65e0\u7ebf\u7f51\u7edc\u8fb9\u7f18\u8bbe\u5907\u7684\u90e8\u7f72\uff0c\u51fa\u73b0\u52a8\u6001\u5229\u7528\u5206\u5e03\u5f0f\u89c6\u89c9\u4e13\u5bb6\u4ee5\u63d0\u5347MLLM\u63a8\u7406\u8d28\u91cf\u7684\u65b0\u673a\u9047\u3002", "method": "\u63d0\u51faM3LLM\u6846\u67b6\uff0c\u5176\u4e2d\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u534f\u8c03\u89c6\u89c9\u4e13\u5bb6\u6df7\u5408\u4f53\u4ee5\u5b9e\u73b0\u5206\u5e03\u5f0fMLLM\u3002MCP\u662f\u4e00\u79cd\u5f00\u653e\u534f\u8bae\uff0c\u5c06\u8f93\u5165\u4efb\u52a1\u4e0a\u4e0b\u6587\u7ed3\u6784\u5316\u4e3a\u53ef\u89e3\u91ca\u8868\u793a\uff0c\u5b9e\u73b0\u4e2d\u5fc3\u6a21\u578b\u9aa8\u5e72\u4e0e\u8fb9\u7f18\u89c6\u89c9\u4e13\u5bb6\u4e4b\u95f4\u5bf9\u65e0\u7ebf\u7f51\u7edc\u7684\u611f\u77e5\u534f\u8c03\u3002\u57fa\u4e8eMCP\u8868\u793a\uff0cM3LLM\u5c06\u89c6\u89c9\u4e13\u5bb6\u8def\u7531\u8868\u8ff0\u4e3a\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5e73\u8861\u4efb\u52a1-\u4e13\u5bb6\u8bed\u4e49\u517c\u5bb9\u6027\u548c\u4fe1\u9053\u6027\u80fd\u3002\u4e3a\u89e3\u51b3\u68af\u5ea6\u51b2\u7a81\uff0c\u5f00\u53d1\u4e86\u5177\u6709\u89e3\u8026\u5956\u52b1\u4fe1\u53f7\u7684\u53cc\u6d41Soft Actor-Critic (SAC) \u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5206\u5c42\u8d1d\u53f6\u65af\u5efa\u6a21\u7684\u81ea\u9002\u5e94\u7a33\u5b9a\u6027\u589e\u5f3a\u6a21\u5757\uff08ASEM\uff09\u4ee5\u786e\u4fdd\u6709\u6548\u8def\u7531\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cM3LLM\u63d0\u5347\u4e86\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u964d\u4f4e\u4e86\u901a\u4fe1\u6210\u672c\uff0c\u5e76\u5728\u52a8\u6001\u65e0\u7ebf\u7f51\u7edc\u6761\u4ef6\u4e0b\u589e\u5f3a\u4e86\u4e13\u5bb6\u8def\u7531\u7684\u9002\u5e94\u6027\u3002", "conclusion": "M3LLM\u901a\u8fc7\u521b\u65b0\u7684\u5206\u5e03\u5f0f\u67b6\u6784\u3001MCP\u534f\u8bae\u548c\u4f18\u5316\u7684\u8def\u7531\u7b97\u6cd5\uff0c\u6210\u529f\u514b\u670d\u4e86\u4e2d\u5fc3\u5316MLLM\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4efb\u52a1\u5904\u7406\u5728\u52a8\u6001\u65e0\u7ebf\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3001\u6548\u7387\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.00858", "pdf": "https://arxiv.org/pdf/2508.00858", "abs": "https://arxiv.org/abs/2508.00858", "authors": ["Christina Butsko", "Kristof Van Tricht", "Gabriel Tseng", "Giorgia Milli", "David Rolnick", "Ruben Cartuyvels", "Inbal Becker Reshef", "Zoltan Szantoi", "Hannah Kerner"], "title": "Deploying Geospatial Foundation Models in the Real World: Lessons from WorldCereal", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": null, "summary": "The increasing availability of geospatial foundation models has the potential\nto transform remote sensing applications such as land cover classification,\nenvironmental monitoring, and change detection. Despite promising benchmark\nresults, the deployment of these models in operational settings is challenging\nand rare. Standardized evaluation tasks often fail to capture real-world\ncomplexities relevant for end-user adoption such as data heterogeneity,\nresource constraints, and application-specific requirements. This paper\npresents a structured approach to integrate geospatial foundation models into\noperational mapping systems. Our protocol has three key steps: defining\napplication requirements, adapting the model to domain-specific data and\nconducting rigorous empirical testing. Using the Presto model in a case study\nfor crop mapping, we demonstrate that fine-tuning a pre-trained model\nsignificantly improves performance over conventional supervised methods. Our\nresults highlight the model's strong spatial and temporal generalization\ncapabilities. Our protocol provides a replicable blueprint for practitioners\nand lays the groundwork for future research to operationalize foundation models\nin diverse remote sensing applications. Application of the protocol to the\nWorldCereal global crop-mapping system showcases the framework's scalability.", "AI": {"tldr": "\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u5728\u9065\u611f\u5e94\u7528\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u56f0\u96be\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u4e09\u6b65\u534f\u8bae\uff0c\u7528\u4e8e\u5c06\u8fd9\u4e9b\u6a21\u578b\u96c6\u6210\u5230\u64cd\u4f5c\u6d4b\u7ed8\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u4f5c\u7269\u6d4b\u7ed8\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u867d\u6709\u524d\u666f\uff0c\u4f46\u56e0\u5b9e\u9645\u590d\u6742\u6027\uff08\u5982\u6570\u636e\u5f02\u6784\u6027\u3001\u8d44\u6e90\u9650\u5236\uff09\u5bfc\u81f4\u5728\u64cd\u4f5c\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u9762\u4e34\u6311\u6218\uff0c\u4e14\u6807\u51c6\u8bc4\u4f30\u672a\u80fd\u6355\u6349\u8fd9\u4e9b\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7ed3\u6784\u5316\u534f\u8bae\uff0c\u5305\u542b\u4e09\u4e2a\u6b65\u9aa4\uff1a\u5b9a\u4e49\u5e94\u7528\u9700\u6c42\u3001\u4f7f\u6a21\u578b\u9002\u5e94\u7279\u5b9a\u9886\u57df\u6570\u636e\u3001\u8fdb\u884c\u4e25\u683c\u7684\u5b9e\u8bc1\u6d4b\u8bd5\u3002\u901a\u8fc7Presto\u6a21\u578b\u5728\u4f5c\u7269\u6d4b\u7ed8\u4e2d\u7684\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u65f6\u7a7a\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u534f\u8bae\u5728WorldCereal\u5168\u7403\u4f5c\u7269\u6d4b\u7ed8\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u4e5f\u8bc1\u660e\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u534f\u8bae\u4e3a\u4ece\u4e1a\u8005\u64cd\u4f5c\u5316\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u590d\u5236\u7684\u84dd\u56fe\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u5176\u53ef\u6269\u5c55\u6027\u5df2\u901a\u8fc7\u5b9e\u9645\u5e94\u7528\u5f97\u5230\u8bc1\u5b9e\u3002"}}
{"id": "2508.00892", "pdf": "https://arxiv.org/pdf/2508.00892", "abs": "https://arxiv.org/abs/2508.00892", "authors": ["Zhihao Zhu", "Jiale Han", "Yi Yang"], "title": "HoneyImage: Verifiable, Harmless, and Stealthy Dataset Ownership Verification for Image Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Image-based AI models are increasingly deployed across a wide range of\ndomains, including healthcare, security, and consumer applications. However,\nmany image datasets carry sensitive or proprietary content, raising critical\nconcerns about unauthorized data usage. Data owners therefore need reliable\nmechanisms to verify whether their proprietary data has been misused to train\nthird-party models. Existing solutions, such as backdoor watermarking and\nmembership inference, face inherent trade-offs between verification\neffectiveness and preservation of data integrity. In this work, we propose\nHoneyImage, a novel method for dataset ownership verification in image\nrecognition models. HoneyImage selectively modifies a small number of hard\nsamples to embed imperceptible yet verifiable traces, enabling reliable\nownership verification while maintaining dataset integrity. Extensive\nexperiments across four benchmark datasets and multiple model architectures\nshow that HoneyImage consistently achieves strong verification accuracy with\nminimal impact on downstream performance while maintaining imperceptible. The\nproposed HoneyImage method could provide data owners with a practical mechanism\nto protect ownership over valuable image datasets, encouraging safe sharing and\nunlocking the full transformative potential of data-driven AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHoneyImage\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u7684\u5c11\u91cf\u56f0\u96be\u6837\u672c\u8fdb\u884c\u4e0d\u53ef\u5bdf\u89c9\u7684\u4fee\u6539\uff0c\u5b9e\u73b0\u5bf9AI\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u6240\u6709\u6743\u7684\u53ef\u9760\u9a8c\u8bc1\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u5b8c\u6574\u6027\u548c\u4e0b\u6e38\u6027\u80fd\u3002", "motivation": "\u56fe\u50cfAI\u6a21\u578b\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u8bad\u7ec3\u6570\u636e\u5e38\u542b\u654f\u611f\u6216\u4e13\u6709\u5185\u5bb9\uff0c\u5b58\u5728\u6570\u636e\u88ab\u6ee5\u7528\u7684\u98ce\u9669\u3002\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\u5728\u9a8c\u8bc1\u6709\u6548\u6027\u548c\u6570\u636e\u5b8c\u6574\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002\u56e0\u6b64\uff0c\u6570\u636e\u6240\u6709\u8005\u9700\u8981\u4e00\u79cd\u53ef\u9760\u673a\u5236\u6765\u9a8c\u8bc1\u5176\u4e13\u6709\u6570\u636e\u662f\u5426\u88ab\u975e\u6cd5\u7528\u4e8e\u8bad\u7ec3\u7b2c\u4e09\u65b9\u6a21\u578b\u3002", "method": "\u63d0\u51faHoneyImage\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u96c6\u6240\u6709\u6743\u9a8c\u8bc1\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u9009\u62e9\u6027\u5730\u4fee\u6539\u5c11\u91cf\u201c\u56f0\u96be\u6837\u672c\u201d\uff0c\u4ee5\u5d4c\u5165\u4e0d\u53ef\u5bdf\u89c9\u4f46\u53ef\u9a8c\u8bc1\u7684\u75d5\u8ff9\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u6570\u636e\u96c6\u5b8c\u6574\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u6240\u6709\u6743\u9a8c\u8bc1\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u591a\u79cd\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cHoneyImage\u6301\u7eed\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u9a8c\u8bc1\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5bf9\u4e0b\u6e38\u6027\u80fd\u5f71\u54cd\u6781\u5c0f\uff0c\u5e76\u4fdd\u6301\u4e86\u75d5\u8ff9\u7684\u4e0d\u53ef\u5bdf\u89c9\u6027\u3002", "conclusion": "HoneyImage\u65b9\u6cd5\u4e3a\u6570\u636e\u6240\u6709\u8005\u63d0\u4f9b\u4e86\u4e00\u79cd\u4fdd\u62a4\u6709\u4ef7\u503c\u56fe\u50cf\u6570\u636e\u96c6\u6240\u6709\u6743\u7684\u5b9e\u7528\u673a\u5236\uff0c\u6709\u52a9\u4e8e\u9f13\u52b1\u5b89\u5168\u5171\u4eab\u6570\u636e\uff0c\u5e76\u5145\u5206\u91ca\u653e\u6570\u636e\u9a71\u52a8AI\u7684\u8f6c\u578b\u6f5c\u529b\u3002"}}
{"id": "2508.00890", "pdf": "https://arxiv.org/pdf/2508.00890", "abs": "https://arxiv.org/abs/2508.00890", "authors": ["Fali Wang", "Hui Liu", "Zhenwei Dai", "Jingying Zeng", "Zhiwei Zhang", "Zongyu Wu", "Chen Luo", "Zhen Li", "Xianfeng Tang", "Qi He", "Suhang Wang"], "title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7"], "comment": "Under review", "summary": "Test-time scaling (TTS) enhances the performance of large language models\n(LLMs) by allocating additional compute resources during inference. However,\nexisting research primarily investigates TTS in single-stage tasks; while many\nreal-world problems are multi-stage complex tasks, composed of a sequence of\nheterogeneous subtasks with each subtask requires LLM of specific capability.\nTherefore, we study a novel problem: the test-time compute-optimal scaling in\nmulti-stage complex tasks, aiming to select suitable models and allocate\nbudgets per subtask to maximize overall performance. TTS in multi-stage tasks\nintroduces two fundamental challenges: (i) The combinatorial search space of\nmodel and budget allocations, combined with the high cost of inference, makes\nbrute-force search impractical. (ii) The optimal model and budget allocations\nacross subtasks are interdependent, increasing the complexity of the\ncompute-optimal search. To address this gap, we conduct extensive pilot\nexperiments on four tasks across six datasets, deriving three empirical\ninsights characterizing the behavior of LLMs in multi-stage complex tasks.\nInformed by these insights, we propose AgentTTS, an LLM-agent-based framework\nthat autonomously searches for compute-optimal allocations through iterative\nfeedback-driven interactions with the execution environment. Experimental\nresults demonstrate that AgentTTS significantly outperforms traditional and\nother LLM-based baselines in search efficiency, and shows improved robustness\nto varying training set sizes and enhanced interpretability.", "AI": {"tldr": "\u7814\u7a76\u5e76\u63d0\u51fa\u4e86\u4e00\u79cdLLM\u4ee3\u7406\u6846\u67b6AgentTTS\uff0c\u7528\u4e8e\u5728\u591a\u9636\u6bb5\u590d\u6742\u4efb\u52a1\u4e2d\u5b9e\u73b0\u8ba1\u7b97\u8d44\u6e90\u7684\u6d4b\u8bd5\u65f6\u6700\u4f18\u5206\u914d\uff0c\u4ee5\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08TTS\uff09\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u9636\u6bb5\u4efb\u52a1\uff0c\u4f46\u8bb8\u591a\u5b9e\u9645\u95ee\u9898\u662f\u591a\u9636\u6bb5\u590d\u6742\u4efb\u52a1\uff0c\u6bcf\u4e2a\u5b50\u4efb\u52a1\u9700\u8981\u7279\u5b9a\u80fd\u529b\u7684LLM\u3002\u8fd9\u5e26\u6765\u4e86\u5de8\u5927\u7684\u6a21\u578b\u548c\u9884\u7b97\u5206\u914d\u7ec4\u5408\u641c\u7d22\u7a7a\u95f4\uff0c\u4ee5\u53ca\u5b50\u4efb\u52a1\u95f4\u4f18\u5316\u76f8\u4e92\u4f9d\u8d56\u7684\u6311\u6218\u3002", "method": "\u4f5c\u8005\u9996\u5148\u901a\u8fc7\u5bf9\u56db\u9879\u4efb\u52a1\u548c\u516d\u4e2a\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u521d\u6b65\u5b9e\u9a8c\uff0c\u5f97\u51fa\u4e86\u5173\u4e8eLLM\u5728\u591a\u9636\u6bb5\u590d\u6742\u4efb\u52a1\u4e2d\u884c\u4e3a\u7684\u4e09\u4e2a\u7ecf\u9a8c\u6027\u6d1e\u5bdf\u3002\u57fa\u4e8e\u8fd9\u4e9b\u6d1e\u5bdf\uff0c\u4ed6\u4eec\u63d0\u51fa\u4e86AgentTTS\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e0e\u6267\u884c\u73af\u5883\u7684\u8fed\u4ee3\u53cd\u9988\u4ea4\u4e92\uff0c\u81ea\u4e3b\u641c\u7d22\u8ba1\u7b97\u6700\u4f18\u7684\u8d44\u6e90\u5206\u914d\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAgentTTS\u5728\u641c\u7d22\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u53ca\u5176\u4ed6\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5b83\u5bf9\u4e0d\u540c\u8bad\u7ec3\u96c6\u5927\u5c0f\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "AgentTTS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u9636\u6bb5\u590d\u6742\u4efb\u52a1\u4e2dLLM\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6700\u4f18\u6269\u5c55\u95ee\u9898\uff0c\u901a\u8fc7\u667a\u80fd\u5206\u914d\u8d44\u6e90\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u641c\u7d22\u6548\u7387\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.01159", "pdf": "https://arxiv.org/pdf/2508.01159", "abs": "https://arxiv.org/abs/2508.01159", "authors": ["Liam G. McCoy", "Fateme Nateghi Haredasht", "Kanav Chopra", "David Wu", "David JH Wu", "Abass Conteh", "Sarita Khemani", "Saloni Kumar Maharaj", "Vishnu Ravi", "Arth Pahwa", "Yingjie Weng", "Leah Rosengaus", "Lena Giang", "Kelvin Zhenghao Li", "Olivia Jee", "Daniel Shirvani", "Ethan Goh", "Jonathan H. Chen"], "title": "Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study evaluates the capacity of large language models (LLMs) to generate\nstructured clinical consultation templates for electronic consultation. Using\n145 expert-crafted templates developed and routinely used by Stanford's\neConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2,\nClaude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to\nproduce clinically coherent, concise, and prioritized clinical question\nschemas. Through a multi-agent pipeline combining prompt optimization, semantic\nautograding, and prioritization analysis, we show that while models like o3\nachieve high comprehensiveness (up to 92.2\\%), they consistently generate\nexcessively long templates and fail to correctly prioritize the most clinically\nimportant questions under length constraints. Performance varies across\nspecialties, with significant degradation in narrative-driven fields such as\npsychiatry and pain medicine. Our findings demonstrate that LLMs can enhance\nstructured clinical information exchange between physicians, while highlighting\nthe need for more robust evaluation methods that capture a model's ability to\nprioritize clinically salient information within the time constraints of\nreal-world physician communication.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86LLMs\u751f\u6210\u7ed3\u6784\u5316\u4e34\u5e8a\u54a8\u8be2\u6a21\u677f\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u5b8c\u6574\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6587\u672c\u957f\u5ea6\u63a7\u5236\u548c\u4e34\u5e8a\u4fe1\u606f\u4f18\u5148\u7ea7\u6392\u5e8f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u53d9\u4e8b\u6027\u5f3a\u7684\u4e13\u79d1\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u7528\u4e8e\u7535\u5b50\u54a8\u8be2\u7684\u7ed3\u6784\u5316\u4e34\u5e8a\u54a8\u8be2\u6a21\u677f\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u65af\u5766\u798feConsult\u56e2\u961f\u7684145\u4e2a\u4e13\u5bb6\u5236\u4f5c\u6a21\u677f\uff0c\u8bc4\u4f30\u4e86\u5305\u62eco3\u3001GPT-4o\u3001Kimi K2\u3001Claude 4 Sonnet\u3001Llama 3 70B\u548cGemini 2.5 Pro\u5728\u5185\u7684\u524d\u6cbfLLM\u3002\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6d41\u7a0b\uff0c\u7ed3\u5408\u63d0\u793a\u4f18\u5316\u3001\u8bed\u4e49\u81ea\u52a8\u8bc4\u5206\u548c\u4f18\u5148\u7ea7\u5206\u6790\u3002", "result": "\u6a21\u578b\u5982o3\u5728\u5b8c\u6574\u6027\u65b9\u9762\u8868\u73b0\u9ad8\u8fbe92.2%\uff0c\u4f46\u666e\u904d\u751f\u6210\u8fc7\u957f\u6a21\u677f\uff0c\u5e76\u96be\u4ee5\u5728\u957f\u5ea6\u9650\u5236\u4e0b\u6b63\u786e\u4f18\u5148\u6392\u5e8f\u4e34\u5e8a\u91cd\u8981\u95ee\u9898\u3002\u6027\u80fd\u56e0\u4e13\u4e1a\u800c\u5f02\uff0c\u5728\u7cbe\u795e\u75c5\u5b66\u548c\u75bc\u75db\u533b\u5b66\u7b49\u53d9\u4e8b\u9a71\u52a8\u9886\u57df\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "LLMs\u80fd\u591f\u4fc3\u8fdb\u533b\u751f\u95f4\u7684\u7ed3\u6784\u5316\u4e34\u5e8a\u4fe1\u606f\u4ea4\u6d41\uff0c\u4f46\u9700\u8981\u66f4\u5f3a\u5927\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u8861\u91cf\u6a21\u578b\u5728\u5b9e\u9645\u6c9f\u901a\u65f6\u95f4\u9650\u5236\u5185\u4f18\u5148\u5904\u7406\u4e34\u5e8a\u6838\u5fc3\u4fe1\u606f\u7684\u80fd\u529b\u3002"}}
{"id": "2508.01898", "pdf": "https://arxiv.org/pdf/2508.01898", "abs": "https://arxiv.org/abs/2508.01898", "authors": ["Yijing Zhang", "Md-Ferdous Pervej", "Andreas F. Molisch"], "title": "Revenue Optimization in Wireless Video Caching Networks: A Privacy-Preserving Two-Stage Solution", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": "Under review for possible publication in the IEEE Transactions on\n  Communications", "summary": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\uff0c\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u9884\u6d4b\u591a\u65f6\u9699\u89c6\u9891\u9700\u6c42\uff0c\u5e76\u5c06\u5176\u7ed3\u5408\u5230\u6700\u5927\u5316\u957f\u671f\u7cfb\u7edf\u6536\u76ca\u7684\u7f13\u5b58\u7b56\u7565\u4e2d\uff0c\u8be5\u65b9\u6848\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89c6\u9891\u7f13\u5b58\u5bf9\u63d0\u5347\u65e0\u7ebf\u89c6\u9891\u6d41\u4f20\u8f93\u6548\u7387\u548c\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53d7\u9650\u4e8e\u7f13\u5b58\u5927\u5c0f\u548c\u52a8\u6001\u7528\u6237\u9700\u6c42\u3002\u4e3a\u6700\u5927\u5316\u7cfb\u7edf\u6536\u76ca\uff0c\u9700\u8981\u8bbe\u8ba1\u80fd\u9002\u5e94\u672a\u6765\u9700\u6c42\u53d8\u5316\u7684\u7f13\u5b58\u7b56\u7565\u3002\u7531\u4e8e\u5f53\u524d\u7f13\u5b58\u5185\u5bb9\u5f71\u54cd\u672a\u6765\u7684\u66ff\u6362\u6210\u672c\uff0c\u591a\u65f6\u9699\u9700\u6c42\u9884\u6d4b\u662f\u9ad8\u6548\u7f13\u5b58\u89c4\u5212\u7684\u524d\u63d0\uff0c\u4e14\u9700\u8003\u8651\u9690\u79c1\u4fdd\u62a4\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u9690\u79c1\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\uff1a\n1. **\u7b2c\u4e00\u9636\u6bb5\uff1a** \u4f7f\u7528\u9690\u79c1\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u8bad\u7ec3Transformer\u6a21\u578b\uff0c\u9884\u6d4b\u591a\u65f6\u9699\u672a\u6765\u9700\u6c42\u3002\u4e3a\u5e94\u5bf9\u9884\u6d4b\u4e0d\u51c6\u786e\u6027\uff0c\u5c06\u5168\u5c40\u5185\u5bb9\u6d41\u884c\u5ea6\u4e0e\u7528\u6237\u9884\u6d4b\u7ed3\u679c\u76f8\u7ed3\u5408\uff0c\u4f30\u8ba1\u5185\u5bb9\u9700\u6c42\u5206\u5e03\u3002\n2. **\u7b2c\u4e8c\u9636\u6bb5\uff1a** \u5229\u7528\u4f30\u8ba1\u7684\u9700\u6c42\u5206\u5e03\uff0c\u5bfb\u627e\u6700\u5927\u5316\u957f\u671f\u7cfb\u7edf\u6536\u76ca\u7684\u7f13\u5b58\u7b56\u7565\u3002\u6b64\u95ee\u9898\u88ab\u5efa\u6a21\u4e3a\u591a\u9636\u6bb5\u80cc\u5305\u95ee\u9898\uff0c\u5e76\u8f6c\u5316\u4e3a\u6574\u6570\u7ebf\u6027\u89c4\u5212\u6c42\u89e3\u3002", "result": "\u5e7f\u6cdb\u7684\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff1a\n1. \u6240\u63d0\u51fa\u7684FL\u89e3\u51b3\u65b9\u6848\u7684\u6027\u80fd\u4e0e\u7406\u60f3\u7684\u4e2d\u5fc3\u5316\u89e3\u51b3\u65b9\u6848\u51e0\u4e4e\u76f8\u540c\uff0c\u5e76\u4f18\u4e8e\u5176\u4ed6\u73b0\u6709\u7f13\u5b58\u65b9\u6cd5\u3002\n2. \u65b0\u9896\u7684\u6536\u76ca\u4f18\u5316\u65b9\u6cd5\u6bd4\u4f20\u7edf\u7684\u57fa\u4e8e\u7f13\u5b58\u547d\u4e2d\u7387\uff08CHR\uff09\u7684\u4f18\u5316\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u6df1\u5165\u7684\u7cfb\u7edf\u6027\u80fd\u6d1e\u5bdf\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u9690\u79c1\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u8fdb\u884c\u9700\u6c42\u9884\u6d4b\u548c\u591a\u9636\u6bb5\u6536\u76ca\u4f18\u5316\uff0c\u80fd\u6709\u6548\u63d0\u5347\u65e0\u7ebf\u89c6\u9891\u7f13\u5b58\u7f51\u7edc\u7684\u957f\u671f\u7cfb\u7edf\u6536\u76ca\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u63d0\u4f9b\u6bd4\u4f20\u7edf\u6307\u6807\u66f4\u5168\u9762\u7684\u7cfb\u7edf\u6027\u80fd\u6d1e\u5bdf\u3002"}}
{"id": "2508.00869", "pdf": "https://arxiv.org/pdf/2508.00869", "abs": "https://arxiv.org/abs/2508.00869", "authors": ["Dmitriy Kashitsyn", "Dmitriy Shabanov"], "title": "Discrete approach to machine learning", "categories": ["cs.LG", "cs.ET", "cs.IT", "math.IT"], "comment": "preprint, 52 pages, 37 figures", "summary": "The article explores an encoding and structural information processing\napproach using sparse bit vectors and fixed-length linear vectors. The\nfollowing are presented: a discrete method of speculative stochastic\ndimensionality reduction of multidimensional code and linear spaces with linear\nasymptotic complexity; a geometric method for obtaining discrete embeddings of\nan organised code space that reflect the internal structure of a given\nmodality. The structure and properties of a code space are investigated using\nthree modalities as examples: morphology of Russian and English languages, and\nimmunohistochemical markers. Parallels are drawn between the resulting map of\nthe code space layout and so-called pinwheels appearing on the mammalian\nneocortex. A cautious assumption is made about similarities between neocortex\norganisation and processes happening in our models.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u4e00\u79cd\u4f7f\u7528\u7a00\u758f\u4f4d\u5411\u91cf\u548c\u5b9a\u957f\u7ebf\u6027\u5411\u91cf\u7684\u4fe1\u606f\u7f16\u7801\u4e0e\u7ed3\u6784\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u964d\u7ef4\u548c\u51e0\u4f55\u5d4c\u5165\u6280\u672f\uff0c\u751f\u6210\u4e86\u4ee3\u7801\u7a7a\u95f4\u5e03\u5c40\uff0c\u5e76\u53d1\u73b0\u5176\u4e0e\u54fa\u4e73\u52a8\u7269\u65b0\u76ae\u5c42\u7684\u201c\u98ce\u8f66\u72b6\u201d\u7ed3\u6784\u5b58\u5728\u76f8\u4f3c\u6027\u3002", "motivation": "\u63a2\u7d22\u4e00\u79cd\u65b0\u9896\u7684\u7f16\u7801\u548c\u7ed3\u6784\u4fe1\u606f\u5904\u7406\u65b9\u6cd5\uff0c\u53ef\u80fd\u65e8\u5728\u7406\u89e3\u590d\u6742\u4fe1\u606f\u7ed3\u6784\u5982\u4f55\u88ab\u7ec4\u7ec7\u548c\u8868\u5f81\uff0c\u5e76\u63a2\u7a76\u5176\u4e0e\u751f\u7269\u795e\u7ecf\u7cfb\u7edf\u7ec4\u7ec7\u4e4b\u95f4\u7684\u6f5c\u5728\u8054\u7cfb\u3002", "method": ["\u4f7f\u7528\u7a00\u758f\u4f4d\u5411\u91cf\u548c\u5b9a\u957f\u7ebf\u6027\u5411\u91cf\u8fdb\u884c\u4fe1\u606f\u7f16\u7801\u548c\u7ed3\u6784\u5904\u7406\u3002", "\u63d0\u51fa\u4e00\u79cd\u5177\u6709\u7ebf\u6027\u6e10\u8fd1\u590d\u6742\u5ea6\u7684\u591a\u7ef4\u4ee3\u7801\u548c\u7ebf\u6027\u7a7a\u95f4\u63a8\u6d4b\u6027\u968f\u673a\u964d\u7ef4\u79bb\u6563\u65b9\u6cd5\u3002", "\u63d0\u51fa\u4e00\u79cd\u51e0\u4f55\u65b9\u6cd5\u4ee5\u83b7\u53d6\u7ec4\u7ec7\u5316\u4ee3\u7801\u7a7a\u95f4\u7684\u79bb\u6563\u5d4c\u5165\uff0c\u4ee5\u53cd\u6620\u7ed9\u5b9a\u6a21\u6001\u7684\u5185\u90e8\u7ed3\u6784\u3002", "\u901a\u8fc7\u4fc4\u8bed\u548c\u82f1\u8bed\u5f62\u6001\u5b66\u4ee5\u53ca\u514d\u75ab\u7ec4\u7ec7\u5316\u5b66\u6807\u8bb0\u7269\u4e09\u79cd\u6a21\u6001\u6765\u8c03\u67e5\u4ee3\u7801\u7a7a\u95f4\u7684\u7ed3\u6784\u548c\u5c5e\u6027\u3002"], "result": ["\u83b7\u5f97\u4e86\u4ee3\u7801\u7a7a\u95f4\u5e03\u5c40\u7684\u6620\u5c04\u56fe\u3002", "\u53d1\u73b0\u6240\u5f97\u7684\u4ee3\u7801\u7a7a\u95f4\u5e03\u5c40\u56fe\u4e0e\u54fa\u4e73\u52a8\u7269\u65b0\u76ae\u5c42\u4e2d\u51fa\u73b0\u7684\u201c\u98ce\u8f66\u72b6\u201d\u7ed3\u6784\u5b58\u5728\u76f8\u4f3c\u4e4b\u5904\u3002"], "conclusion": "\u7814\u7a76\u8c28\u614e\u5730\u5047\u8bbe\uff0c\u6a21\u578b\u4e2d\u53d1\u751f\u7684\u7ec4\u7ec7\u548c\u8fc7\u7a0b\u4e0e\u65b0\u76ae\u5c42\u7684\u7ec4\u7ec7\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u76f8\u4f3c\u6027\u3002"}}
{"id": "2508.00896", "pdf": "https://arxiv.org/pdf/2508.00896", "abs": "https://arxiv.org/abs/2508.00896", "authors": ["Hoang Hai Nam Nguyen", "Minh Tien Tran", "Hoheok Kim", "Ho Won Lee"], "title": "Phase-fraction guided denoising diffusion model for augmenting multiphase steel microstructure segmentation via micrograph image-mask pair synthesis", "categories": ["cs.CV", "cond-mat.mtrl-sci", "eess.IV"], "comment": null, "summary": "The effectiveness of machine learning in metallographic microstructure\nsegmentation is often constrained by the lack of human-annotated phase masks,\nparticularly for rare or compositionally complex morphologies within the metal\nalloy. We introduce PF-DiffSeg, a phase-fraction controlled, one-stage\ndenoising diffusion framework that jointly synthesizes microstructure images\nand their corresponding segmentation masks in a single generative trajectory to\nfurther improve segmentation accuracy. By conditioning on global phase-fraction\nvectors, augmented to represent real data distribution and emphasize minority\nclasses, our model generates compositionally valid and structurally coherent\nmicrostructure image and mask samples that improve both data diversity and\ntraining efficiency. Evaluated on the MetalDAM benchmark for additively\nmanufactured multiphase steel, our synthetic augmentation method yields notable\nimprovements in segmentation accuracy compared to standard augmentation\nstrategies especially in minority classes and further outperforms a two-stage\nmask-guided diffusion and generative adversarial network (GAN) baselines, while\nalso reducing inference time compared to conventional approach. The method\nintegrates generation and conditioning into a unified framework, offering a\nscalable solution for data augmentation in metallographic applications.", "AI": {"tldr": "\u9488\u5bf9\u91d1\u76f8\u663e\u5fae\u7ec4\u7ec7\u56fe\u50cf\u5206\u5272\u4e2d\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faPF-DiffSeg\uff0c\u4e00\u4e2a\u5355\u9636\u6bb5\u53bb\u566a\u6269\u6563\u6846\u67b6\uff0c\u80fd\u540c\u65f6\u5408\u6210\u56fe\u50cf\u548c\u5206\u5272\u63a9\u7801\uff0c\u4ece\u800c\u6709\u6548\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u5c11\u6570\u7c7b\u522b\u4e0a\u8868\u73b0\u663e\u8457\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u91d1\u76f8\u663e\u5fae\u7ec4\u7ec7\u5206\u5272\u4e2d\u7684\u6709\u6548\u6027\u53d7\u9650\u4e8e\u7f3a\u4e4f\u4eba\u5de5\u6807\u6ce8\u7684\u76f8\u63a9\u7801\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7a00\u6709\u6216\u590d\u6742\u5f62\u6001\u7684\u91d1\u5c5e\u5408\u91d1\u3002\u8fd9\u5bfc\u81f4\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5f15\u5165PF-DiffSeg\uff0c\u4e00\u4e2a\u53d7\u76f8\u5206\u6570\u63a7\u5236\u7684\u5355\u9636\u6bb5\u53bb\u566a\u6269\u6563\u6846\u67b6\u3002\u5b83\u5728\u5355\u4e00\u751f\u6210\u8f68\u8ff9\u4e2d\u8054\u5408\u5408\u6210\u663e\u5fae\u7ec4\u7ec7\u56fe\u50cf\u53ca\u5176\u5bf9\u5e94\u7684\u5206\u5272\u63a9\u7801\u3002\u901a\u8fc7\u4ee5\u5168\u5c40\u76f8\u5206\u6570\u5411\u91cf\u4e3a\u6761\u4ef6\uff0c\u5e76\u589e\u5f3a\u4ee5\u4ee3\u8868\u771f\u5b9e\u6570\u636e\u5206\u5e03\u548c\u5f3a\u8c03\u5c11\u6570\u7c7b\u522b\uff0c\u8be5\u6a21\u578b\u80fd\u751f\u6210\u6210\u5206\u6709\u6548\u4e14\u7ed3\u6784\u8fde\u8d2f\u7684\u56fe\u50cf\u548c\u63a9\u7801\u6837\u672c\uff0c\u4ece\u800c\u63d0\u9ad8\u6570\u636e\u591a\u6837\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5728MetalDAM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0e\u6807\u51c6\u6570\u636e\u589e\u5f3a\u7b56\u7565\u76f8\u6bd4\uff0cPF-DiffSeg\u5408\u6210\u589e\u5f3a\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u5c11\u6570\u7c7b\u522b\u4e2d\u8868\u73b0\u7a81\u51fa\u3002\u5b83\u8fd8\u4f18\u4e8e\u4e24\u9636\u6bb5\u63a9\u7801\u5f15\u5bfc\u6269\u6563\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc(GAN)\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u51cf\u5c11\u4e86\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "PF-DiffSeg\u65b9\u6cd5\u5c06\u751f\u6210\u548c\u6761\u4ef6\u4f5c\u7528\u6574\u5408\u5230\u7edf\u4e00\u6846\u67b6\u4e2d\uff0c\u4e3a\u91d1\u76f8\u5e94\u7528\u4e2d\u7684\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2508.00899", "pdf": "https://arxiv.org/pdf/2508.00899", "abs": "https://arxiv.org/abs/2508.00899", "authors": ["Abeer Dyoub", "Ivan Letteri", "Francesca A. Lisi"], "title": "ff4ERA: A new Fuzzy Framework for Ethical Risk Assessment in AI", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": null, "summary": "The emergence of Symbiotic AI (SAI) introduces new challenges to ethical\ndecision-making as it deepens human-AI collaboration. As symbiosis grows, AI\nsystems pose greater ethical risks, including harm to human rights and trust.\nEthical Risk Assessment (ERA) thus becomes crucial for guiding decisions that\nminimize such risks. However, ERA is hindered by uncertainty, vagueness, and\nincomplete information, and morality itself is context-dependent and imprecise.\nThis motivates the need for a flexible, transparent, yet robust framework for\nERA. Our work supports ethical decision-making by quantitatively assessing and\nprioritizing multiple ethical risks so that artificial agents can select\nactions aligned with human values and acceptable risk levels. We introduce\nff4ERA, a fuzzy framework that integrates Fuzzy Logic, the Fuzzy Analytic\nHierarchy Process (FAHP), and Certainty Factors (CF) to quantify ethical risks\nvia an Ethical Risk Score (ERS) for each risk type. The final ERS combines the\nFAHP-derived weight, propagated CF, and risk level. The framework offers a\nrobust mathematical approach for collaborative ERA modeling and systematic,\nstep-by-step analysis. A case study confirms that ff4ERA yields\ncontext-sensitive, ethically meaningful risk scores reflecting both expert\ninput and sensor-based evidence. Risk scores vary consistently with relevant\nfactors while remaining robust to unrelated inputs. Local sensitivity analysis\nshows predictable, mostly monotonic behavior across perturbations, and global\nSobol analysis highlights the dominant influence of expert-defined weights and\ncertainty factors, validating the model design. Overall, the results\ndemonstrate ff4ERA ability to produce interpretable, traceable, and risk-aware\nethical assessments, enabling what-if analyses and guiding designers in\ncalibrating membership functions and expert judgments for reliable ethical\ndecision support.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faff4ERA\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6a21\u7cca\u903b\u8f91\u7684\u4f26\u7406\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\uff0c\u65e8\u5728\u91cf\u5316\u5171\u751fAI\u4e2d\u7684\u4f26\u7406\u98ce\u9669\uff0c\u652f\u6301\u4eba\u7c7b-AI\u534f\u4f5c\u4e2d\u7684\u4f26\u7406\u51b3\u7b56\u3002", "motivation": "\u5171\u751fAI\uff08SAI\uff09\u6df1\u5316\u4eba\u673a\u534f\u4f5c\uff0c\u5e26\u6765\u66f4\u9ad8\u7684\u4f26\u7406\u98ce\u9669\uff08\u5982\u5bf9\u4eba\u6743\u548c\u4fe1\u4efb\u7684\u635f\u5bb3\uff09\u3002\u4f26\u7406\u98ce\u9669\u8bc4\u4f30\uff08ERA\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53d7\u4e0d\u786e\u5b9a\u6027\u3001\u6a21\u7cca\u6027\u548c\u4fe1\u606f\u4e0d\u5b8c\u6574\u6027\u9650\u5236\uff0c\u4e14\u9053\u5fb7\u672c\u8eab\u5177\u6709\u8bed\u5883\u4f9d\u8d56\u6027\u548c\u4e0d\u7cbe\u786e\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u7075\u6d3b\u3001\u900f\u660e\u4e14\u9c81\u68d2\u7684ERA\u6846\u67b6\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86ff4ERA\u6a21\u7cca\u6846\u67b6\uff0c\u6574\u5408\u4e86\u6a21\u7cca\u903b\u8f91\u3001\u6a21\u7cca\u5c42\u6b21\u5206\u6790\u6cd5\uff08FAHP\uff09\u548c\u786e\u5b9a\u6027\u56e0\u5b50\uff08CF\uff09\uff0c\u901a\u8fc7\u4f26\u7406\u98ce\u9669\u5f97\u5206\uff08ERS\uff09\u91cf\u5316\u6bcf\u79cd\u98ce\u9669\u7c7b\u578b\u3002\u6700\u7ec8ERS\u7ed3\u5408\u4e86FAHP\u884d\u751f\u7684\u6743\u91cd\u3001\u4f20\u64ad\u7684CF\u548c\u98ce\u9669\u7b49\u7ea7\u3002\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u6570\u5b66\u65b9\u6cd5\uff0c\u7528\u4e8e\u534f\u4f5cERA\u5efa\u6a21\u548c\u7cfb\u7edf\u6027\u5206\u6790\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8bc1\u5b9eff4ERA\u80fd\u4ea7\u751f\u8bed\u5883\u654f\u611f\u3001\u5177\u6709\u4f26\u7406\u610f\u4e49\u7684\u98ce\u9669\u5f97\u5206\uff0c\u53cd\u6620\u4e13\u5bb6\u8f93\u5165\u548c\u4f20\u611f\u5668\u8bc1\u636e\u3002\u98ce\u9669\u5f97\u5206\u968f\u76f8\u5173\u56e0\u7d20\u4e00\u81f4\u53d8\u5316\uff0c\u5e76\u5bf9\u4e0d\u76f8\u5173\u8f93\u5165\u4fdd\u6301\u9c81\u68d2\u6027\u3002\u5c40\u90e8\u654f\u611f\u6027\u5206\u6790\u663e\u793a\u53ef\u9884\u6d4b\u3001\u5355\u8c03\u7684\u884c\u4e3a\uff0c\u5168\u5c40Sobol\u5206\u6790\u7a81\u51fa\u4e13\u5bb6\u5b9a\u4e49\u6743\u91cd\u548c\u786e\u5b9a\u6027\u56e0\u5b50\u7684\u4e3b\u5bfc\u5f71\u54cd\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u8bbe\u8ba1\u3002", "conclusion": "ff4ERA\u80fd\u591f\u751f\u6210\u53ef\u89e3\u91ca\u3001\u53ef\u8ffd\u6eaf\u4e14\u5177\u6709\u98ce\u9669\u610f\u8bc6\u7684\u4f26\u7406\u8bc4\u4f30\uff0c\u652f\u6301\u5047\u8bbe\u5206\u6790\uff0c\u5e76\u6307\u5bfc\u8bbe\u8ba1\u8005\u6821\u51c6\u96b6\u5c5e\u51fd\u6570\u548c\u4e13\u5bb6\u5224\u65ad\uff0c\u4ee5\u5b9e\u73b0\u53ef\u9760\u7684\u4f26\u7406\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2508.01161", "pdf": "https://arxiv.org/pdf/2508.01161", "abs": "https://arxiv.org/abs/2508.01161", "authors": ["Jiyu Chen", "Necva B\u00f6l\u00fcc\u00fc", "Sarvnaz Karimi", "Diego Moll\u00e1", "C\u00e9cile L. Paris"], "title": "CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages", "categories": ["cs.CL"], "comment": "In Proceedings of the 19th International Workshop on Semantic\n  Evaluation (SemEval-2025), Vienna, Austria. Association for Computational\n  Linguistics", "summary": "Detecting emotions across different languages is challenging due to the\nvaried and culturally nuanced ways of emotional expressions. The\n\\textit{Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion} shared\ntask was organised to investigate emotion recognition across different\nlanguages. The goal of the task is to implement an emotion recogniser that can\nidentify the basic emotional states that general third-party observers would\nattribute to an author based on their written text snippet, along with the\nintensity of those emotions. We report our investigation of various\ntask-adaptation strategies for LLMs in emotion recognition. We show that the\nmost effective method for this task is to fine-tune a pre-trained multilingual\nLLM with LoRA setting separately for each language.", "AI": {"tldr": "\u7814\u7a76\u4e86LLMs\u5728\u8de8\u8bed\u8a00\u6587\u672c\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u4efb\u52a1\u9002\u5e94\u7b56\u7565\uff0c\u53d1\u73b0\u5bf9\u591a\u8bed\u8a00LLM\u8fdb\u884cLoRA\u5fae\u8c03\u5e76\u9488\u5bf9\u6bcf\u79cd\u8bed\u8a00\u5355\u72ec\u5904\u7406\u6700\u4e3a\u6709\u6548\u3002", "motivation": "\u8de8\u8bed\u8a00\u60c5\u611f\u8bc6\u522b\u56e0\u8868\u8fbe\u65b9\u5f0f\u591a\u6837\u4e14\u5177\u6587\u5316\u5dee\u5f02\u800c\u9762\u4e34\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u53c2\u4e0eSemEval 2025 Task 11\uff0c\u5f00\u53d1\u80fd\u8bc6\u522b\u6587\u672c\u57fa\u672c\u60c5\u611f\u72b6\u6001\u53ca\u5176\u5f3a\u5ea6\u7684\u60c5\u611f\u8bc6\u522b\u5668\u3002", "method": "\u8c03\u67e5\u4e86\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u4efb\u52a1\u9002\u5e94\u7b56\u7565\uff0c\u5177\u4f53\u91c7\u7528\u4e86LoRA\u8bbe\u7f6e\u5bf9\u9884\u8bad\u7ec3\u7684\u591a\u8bed\u8a00LLM\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6700\u6709\u6548\u7684\u65b9\u6cd5\u662f\u9488\u5bf9\u6bcf\u79cd\u8bed\u8a00\u5355\u72ec\u4f7f\u7528LoRA\u8bbe\u7f6e\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u591a\u8bed\u8a00LLM\u3002", "conclusion": "\u4e3a\u6bcf\u79cd\u8bed\u8a00\u5355\u72ec\u4f7f\u7528LoRA\u8bbe\u7f6e\u5fae\u8c03\u9884\u8bad\u7ec3\u7684\u591a\u8bed\u8a00LLM\uff0c\u662f\u5b9e\u73b0\u8de8\u8bed\u8a00\u6587\u672c\u60c5\u611f\u8bc6\u522b\u7684\u6700\u4f73\u7b56\u7565\u3002"}}
{"id": "2508.02001", "pdf": "https://arxiv.org/pdf/2508.02001", "abs": "https://arxiv.org/abs/2508.02001", "authors": ["Chungang Lin", "Weiyao Zhang", "Tianyu Zuo", "Chao Zha", "Yilong Jiang", "Ruiqi Meng", "Haitong Luo", "Xuying Meng", "Yujun Zhang"], "title": "Convolutions are Competitive with Transformers for Encrypted Traffic Classification with Pre-training", "categories": ["cs.NI", "cs.LG"], "comment": "Under review", "summary": "Encrypted traffic classification is vital for modern network management and\nsecurity. To reduce reliance on handcrafted features and labeled data, recent\nmethods focus on learning generic representations through pre-training on\nlarge-scale unlabeled data. However, current pre-trained models face two\nlimitations originating from the adopted Transformer architecture: (1) Limited\nmodel efficiency due to the self-attention mechanism with quadratic complexity;\n(2) Unstable traffic scalability to longer byte sequences, as the explicit\npositional encodings fail to generalize to input lengths not seen during\npre-training. In this paper, we investigate whether convolutions, with linear\ncomplexity and implicit positional encoding, are competitive with Transformers\nin encrypted traffic classification with pre-training. We first conduct a\nsystematic comparison, and observe that convolutions achieve higher efficiency\nand scalability, with lower classification performance. To address this\ntrade-off, we propose NetConv, a novel pre-trained convolution model for\nencrypted traffic classification. NetConv employs stacked traffic convolution\nlayers, which enhance the ability to capture localized byte-sequence patterns\nthrough window-wise byte scoring and sequence-wise byte gating. We design a\ncontinuous byte masking pre-training task to help NetConv learn\nprotocol-specific patterns. Experimental results on four tasks demonstrate that\nNetConv improves average classification performance by 6.88% and model\nthroughput by 7.41X over existing pre-trained models.", "AI": {"tldr": "\u9488\u5bf9\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u4e2d\u73b0\u6709\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u7684\u6548\u7387\u548c\u53ef\u4f38\u7f29\u6027\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faNetConv\uff0c\u4e00\u4e2a\u65b0\u578b\u9884\u8bad\u7ec3\u5377\u79ef\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u5377\u79ef\u5c42\u548c\u8fde\u7eed\u5b57\u8282\u63a9\u7801\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u548c\u6a21\u578b\u541e\u5410\u91cf\u3002", "motivation": "\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u5bf9\u7f51\u7edc\u7ba1\u7406\u548c\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u9762\u4e34\u4e24\u70b9\u5c40\u9650\uff1a1) \u81ea\u6ce8\u610f\u529b\u673a\u5236\u5bfc\u81f4\u7684\u6a21\u578b\u6548\u7387\u4f4e\u4e0b\uff1b2) \u663e\u5f0f\u4f4d\u7f6e\u7f16\u7801\u6cdb\u5316\u80fd\u529b\u5dee\u5bfc\u81f4\u957f\u5e8f\u5217\u53ef\u4f38\u7f29\u6027\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u9996\u5148\u7cfb\u7edf\u6bd4\u8f83\u4e86\u5377\u79ef\u4e0eTransformer\u6a21\u578b\u5728\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\u3002\u4e3a\u89e3\u51b3\u5377\u79ef\u6a21\u578b\u6027\u80fd\u52a3\u52bf\uff0c\u63d0\u51faNetConv\uff0c\u4e00\u4e2a\u65b0\u578b\u9884\u8bad\u7ec3\u5377\u79ef\u6a21\u578b\u3002NetConv\u91c7\u7528\u5806\u53e0\u6d41\u91cf\u5377\u79ef\u5c42\uff0c\u901a\u8fc7\u7a97\u53e3\u5b57\u8282\u8bc4\u5206\u548c\u5e8f\u5217\u5b57\u8282\u95e8\u63a7\u589e\u5f3a\u5c40\u90e8\u5b57\u8282\u5e8f\u5217\u6a21\u5f0f\u6355\u83b7\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u8fde\u7eed\u5b57\u8282\u63a9\u7801\u9884\u8bad\u7ec3\u4efb\u52a1\u3002", "result": "\u521d\u6b65\u5bf9\u6bd4\u53d1\u73b0\uff0c\u5377\u79ef\u6a21\u578b\u6548\u7387\u548c\u53ef\u4f38\u7f29\u6027\u66f4\u9ad8\u4f46\u5206\u7c7b\u6027\u80fd\u8f83\u4f4e\u3002NetConv\u5728\u56db\u9879\u4efb\u52a1\u4e0a\uff0c\u4e0e\u73b0\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u76f8\u6bd4\uff0c\u5e73\u5747\u5206\u7c7b\u6027\u80fd\u63d0\u53476.88%\uff0c\u6a21\u578b\u541e\u5410\u91cf\u63d0\u53477.41\u500d\u3002", "conclusion": "NetConv\u6210\u529f\u89e3\u51b3\u4e86\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u4e2d\u6548\u7387\u3001\u53ef\u4f38\u7f29\u6027\u4e0e\u5206\u7c7b\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5377\u79ef\u6a21\u578b\u5728\u8be5\u9886\u57df\u8d85\u8d8a\u73b0\u6709\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00876", "pdf": "https://arxiv.org/pdf/2508.00876", "abs": "https://arxiv.org/abs/2508.00876", "authors": ["Bakhtiyar Mammadli", "Casim Yazici", "Muhammed G\u00fcrb\u00fcz", "\u0130rfan Kocaman", "F. Javier Dominguez-Gutierrez", "Fatih Mehmet \u00d6zkal"], "title": "A Data-Driven Machine Learning Approach for Predicting Axial Load Capacity in Steel Storage Rack Columns", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "comment": null, "summary": "In this study, we present a machine learning (ML) framework to predict the\naxial load-bearing capacity, (kN), of cold-formed steel structural members. The\nmethodology emphasizes robust model selection and interpretability, addressing\nthe limitations of traditional analytical approaches in capturing the\nnonlinearities and geometrical complexities inherent to buckling behavior. The\ndataset, comprising key geometric and mechanical parameters of steel columns,\nwas curated with appropriate pre-processing steps including removal of\nnon-informative identifiers and imputation of missing values. A comprehensive\nsuite of regression algorithms, ranging from linear models to kernel-based\nregressors and ensemble tree methods was evaluated. Among these, Gradient\nBoosting Regression exhibited superior predictive performance across multiple\nmetrics, including the coefficient of determination (R2), root mean squared\nerror (RMSE), and mean absolute error (MAE), and was consequently selected as\nthe final model. Model interpretability was addressed using SHapley Additive\nexPlanations (SHAP), enabling insight into the relative importance and\ninteraction of input features influencing the predicted axial capacity. To\nfacilitate practical deployment, the model was integrated into an interactive,\nPython-based web interface via Streamlit. This tool allows end-users-such as\nstructural engineers and designers, to input design parameters manually or\nthrough CSV upload, and to obtain real-time predictions of axial load capacity\nwithout the need for programming expertise. Applied to the context of steel\nstorage rack columns, the framework demonstrates how data-driven tools can\nenhance design safety, streamline validation workflows, and inform\ndecision-making in structural applications where buckling is a critical failure\nmode", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u51b7\u5f2f\u578b\u94a2\u6784\u4ef6\u7684\u8f74\u5411\u627f\u8f7d\u529b\uff0c\u5f3a\u8c03\u6a21\u578b\u9009\u62e9\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5229\u7528Gradient Boosting Regression\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\uff0c\u6700\u7ec8\u90e8\u7f72\u4e3a\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684Web\u5de5\u5177\u3002", "motivation": "\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u6355\u6349\u5c48\u66f2\u884c\u4e3a\u4e2d\u7684\u975e\u7ebf\u6027\u548c\u51e0\u4f55\u590d\u6742\u6027\uff0c\u9650\u5236\u4e86\u5bf9\u51b7\u5f2f\u578b\u94a2\u6784\u4ef6\u8f74\u5411\u627f\u8f7d\u529b\u7684\u51c6\u786e\u9884\u6d4b\u3002", "method": "\u7814\u7a76\u6784\u5efa\u4e86\u5305\u542b\u94a2\u67f1\u51e0\u4f55\u548c\u529b\u5b66\u53c2\u6570\u7684\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u4e86\u6570\u636e\u9884\u5904\u7406\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u56de\u5f52\u7b97\u6cd5\uff0c\u5176\u4e2dGradient Boosting Regression\u56e0\u8868\u73b0\u4f18\u5f02\u88ab\u9009\u4e3a\u6700\u7ec8\u6a21\u578b\u3002\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u901a\u8fc7SHAP\u5b9e\u73b0\uff0c\u5e76\u901a\u8fc7Streamlit\u96c6\u6210\u5230Python\u4ea4\u4e92\u5f0fWeb\u754c\u9762\u4e2d\uff0c\u4ee5\u4fbf\u5de5\u7a0b\u5e08\u4f7f\u7528\u3002", "result": "Gradient Boosting Regression\u5728R2\u3001RMSE\u548cMAE\u7b49\u591a\u4e2a\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u88ab\u786e\u8ba4\u4e3a\u9884\u6d4b\u8f74\u5411\u627f\u8f7d\u529b\u7684\u6700\u4f73\u6a21\u578b\u3002SHAP\u5206\u6790\u63d0\u4f9b\u4e86\u8f93\u5165\u7279\u5f81\u5bf9\u9884\u6d4b\u7ed3\u679c\u5f71\u54cd\u7684\u6df1\u5165\u89c1\u89e3\u3002", "conclusion": "\u8be5\u6846\u67b6\u8bc1\u660e\u4e86\u6570\u636e\u9a71\u52a8\u5de5\u5177\u5728\u7ed3\u6784\u5e94\u7528\u4e2d\uff08\u7279\u522b\u662f\u5bf9\u4e8e\u94a2\u5236\u8d27\u67b6\u67f1\uff09\u80fd\u6709\u6548\u63d0\u9ad8\u8bbe\u8ba1\u5b89\u5168\u6027\uff0c\u7b80\u5316\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u5e76\u4e3a\u4ee5\u5c48\u66f2\u4e3a\u5173\u952e\u5931\u6548\u6a21\u5f0f\u7684\u51b3\u7b56\u63d0\u4f9b\u4fe1\u606f\u3002"}}
{"id": "2508.00898", "pdf": "https://arxiv.org/pdf/2508.00898", "abs": "https://arxiv.org/abs/2508.00898", "authors": ["Jose M. S\u00e1nchez Vel\u00e1zquez", "Mingbo Cai", "Andrew Coney", "\u00c1lvaro J. Garc\u00eda- Tejedor", "Alberto Nogales"], "title": "Benefits of Feature Extraction and Temporal Sequence Analysis for Video Frame Prediction: An Evaluation of Hybrid Deep Learning Models", "categories": ["cs.CV", "cs.AI"], "comment": "2 Figures, 12 Tables, 21 pages", "summary": "In recent years, advances in Artificial Intelligence have significantly\nimpacted computer science, particularly in the field of computer vision,\nenabling solutions to complex problems such as video frame prediction. Video\nframe prediction has critical applications in weather forecasting or autonomous\nsystems and can provide technical improvements, such as video compression and\nstreaming. Among Artificial Intelligence methods, Deep Learning has emerged as\nhighly effective for solving vision-related tasks, although current frame\nprediction models still have room for enhancement. This paper evaluates several\nhybrid deep learning approaches that combine the feature extraction\ncapabilities of autoencoders with temporal sequence modelling using Recurrent\nNeural Networks (RNNs), 3D Convolutional Neural Networks (3D CNNs), and related\narchitectures. The proposed solutions were rigorously evaluated on three\ndatasets that differ in terms of synthetic versus real-world scenarios and\ngrayscale versus color imagery. Results demonstrate that the approaches perform\nwell, with SSIM metrics increasing from 0.69 to 0.82, indicating that hybrid\nmodels utilizing 3DCNNs and ConvLSTMs are the most effective, and greyscale\nvideos with real data are the easiest to predict.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u7ed3\u5408\u81ea\u7f16\u7801\u5668\u548c\u4e0d\u540c\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff08RNN\u30013D CNN\u7b49\uff09\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u89c6\u9891\u5e27\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5229\u75283DCNNs\u548cConvLSTMs\u7684\u6df7\u5408\u6a21\u578b\u6548\u679c\u6700\u4f73\uff0c\u5e76\u5c06SSIM\u6307\u6807\u4ece0.69\u63d0\u5347\u81f30.82\u3002", "motivation": "\u89c6\u9891\u5e27\u9884\u6d4b\u5728\u5929\u6c14\u9884\u62a5\u3001\u81ea\u52a8\u7cfb\u7edf\u7b49\u9886\u57df\u5177\u6709\u5173\u952e\u5e94\u7528\uff0c\u5e76\u80fd\u6539\u5584\u89c6\u9891\u538b\u7f29\u548c\u6d41\u5a92\u4f53\u6280\u672f\u3002\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u73b0\u6709\u5e27\u9884\u6d4b\u6a21\u578b\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u8bba\u6587\u8bc4\u4f30\u4e86\u51e0\u79cd\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u7ed3\u5408\u4e86\u81ea\u7f16\u7801\u5668\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u4e0e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNNs\uff09\u30013D\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff083D CNNs\uff09\u53ca\u76f8\u5173\u67b6\u6784\u7684\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\u3002\u6240\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u5728\u4e09\u4e2a\u4e0d\u540c\u7c7b\u578b\uff08\u5408\u6210\u4e0e\u771f\u5b9e\u4e16\u754c\u3001\u7070\u5ea6\u4e0e\u5f69\u8272\uff09\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u4e25\u683c\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u8868\u73b0\u826f\u597d\uff0cSSIM\u6307\u6807\u4ece0.69\u63d0\u9ad8\u52300.82\u3002\u5176\u4e2d\uff0c\u5229\u75283DCNNs\u548cConvLSTMs\u7684\u6df7\u5408\u6a21\u578b\u6548\u679c\u6700\u4f73\uff0c\u5e76\u4e14\u771f\u5b9e\u7070\u5ea6\u89c6\u9891\u6700\u5bb9\u6613\u9884\u6d4b\u3002", "conclusion": "\u7ed3\u5408\u81ea\u7f16\u7801\u5668\u4e0e3D CNNs\u548cConvLSTMs\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u5347\u89c6\u9891\u5e27\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u7684\u7070\u5ea6\u89c6\u9891\u65f6\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2508.00902", "pdf": "https://arxiv.org/pdf/2508.00902", "abs": "https://arxiv.org/abs/2508.00902", "authors": ["Kenneth Payne"], "title": "An analysis of AI Decision under Risk: Prospect theory emerges in Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "26 pages, 2 figures, 9 tables, 2 appendices", "summary": "Judgment of risk is key to decision-making under uncertainty. As Daniel\nKahneman and Amos Tversky famously discovered, humans do so in a distinctive\nway that departs from mathematical rationalism. Specifically, they demonstrated\nexperimentally that humans accept more risk when they feel themselves at risk\nof losing something than when they might gain. I report the first tests of\nKahneman and Tversky's landmark 'prospect theory' with Large Language Models,\nincluding today's state of the art chain-of-thought 'reasoners'.\n  In common with humans, I find that prospect theory often anticipates how\nthese models approach risky decisions across a range of scenarios. I also\ndemonstrate that context is key to explaining much of the variance in risk\nappetite. The 'frame' through which risk is apprehended appears to be embedded\nwithin the language of the scenarios tackled by the models. Specifically, I\nfind that military scenarios generate far larger 'framing effects' than do\ncivilian settings, ceteris paribus. My research suggests, therefore, that\nlanguage models the world, capturing our human heuristics and biases. But also\nthat these biases are uneven - the idea of a 'frame' is richer than simple\ngains and losses. Wittgenstein's notion of 'language games' explains the\ncontingent, localised biases activated by these scenarios. Finally, I use my\nfindings to reframe the ongoing debate about reasoning and memorisation in\nLLMs.", "AI": {"tldr": "\u7814\u7a76\u9996\u6b21\u6d4b\u8bd5\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u5c55\u73b0\u51fa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u5c55\u671b\u7406\u8bba\uff08Prospect Theory\uff09\u6548\u5e94\uff0c\u53d1\u73b0LLMs\u7684\u884c\u4e3a\u786e\u5b9e\u7b26\u5408\u8be5\u7406\u8bba\uff0c\u4e14\u60c5\u5883\uff08\u7279\u522b\u662f\u519b\u4e8b\u60c5\u5883\uff09\u5bf9\u98ce\u9669\u504f\u597d\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u4eba\u7c7b\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u5224\u65ad\u98ce\u9669\u7684\u65b9\u5f0f\u504f\u79bb\u7406\u6027\uff0c\u8868\u73b0\u51fa\u201c\u5c55\u671b\u7406\u8bba\u201d\u6548\u5e94\uff08\u5982\u635f\u5931\u89c4\u907f\uff09\u3002\u672c\u7814\u7a76\u65e8\u5728\u9996\u6b21\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u62ec\u6700\u5148\u8fdb\u7684\u601d\u7ef4\u94fe\u6a21\u578b\uff0c\u662f\u5426\u4e5f\u5c55\u73b0\u51fa\u8fd9\u79cd\u72ec\u7279\u7684\u98ce\u9669\u51b3\u7b56\u6a21\u5f0f\u3002", "method": "\u4f5c\u8005\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u4e86\u9996\u6b21\u6d4b\u8bd5\uff0c\u4ee5\u9a8c\u8bc1\u5b83\u4eec\u662f\u5426\u7b26\u5408\u5361\u5c3c\u66fc\u548c\u7279\u6c83\u65af\u57fa\u7684\u201c\u5c55\u671b\u7406\u8bba\u201d\uff0c\u5373\u5728\u9762\u4e34\u98ce\u9669\u51b3\u7b56\u65f6\uff0c\u6a21\u578b\u662f\u5426\u50cf\u4eba\u7c7b\u4e00\u6837\u5bf9\u635f\u5931\u548c\u6536\u76ca\u6709\u4e0d\u540c\u7684\u504f\u597d\u3002\u5b9e\u9a8c\u901a\u8fc7\u4e0d\u540c\u60c5\u5883\uff08\u7279\u522b\u662f\u519b\u4e8b\u548c\u6c11\u7528\u573a\u666f\uff09\u6765\u6d4b\u8bd5\u6a21\u578b\u7684\u98ce\u9669\u504f\u597d\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e0e\u4eba\u7c7b\u76f8\u4f3c\uff0c\u5c55\u671b\u7406\u8bba\u5e38\u80fd\u9884\u6d4bLLMs\u5728\u5404\u79cd\u60c5\u5883\u4e0b\u5904\u7406\u98ce\u9669\u51b3\u7b56\u7684\u65b9\u5f0f\u3002\u7ed3\u679c\u8868\u660e\uff0c\u60c5\u5883\uff08\u201c\u6846\u67b6\u201d\uff09\u662f\u89e3\u91ca\u98ce\u9669\u504f\u597d\u5dee\u5f02\u7684\u5173\u952e\uff0c\u5e76\u4e14\u519b\u4e8b\u60c5\u5883\u6bd4\u6c11\u7528\u60c5\u5883\u4ea7\u751f\u66f4\u5927\u7684\u201c\u6846\u67b6\u6548\u5e94\u201d\u3002\u8fd9\u8868\u660eLLMs\u4e0d\u4ec5\u80fd\u6a21\u62df\u4e16\u754c\uff0c\u8fd8\u80fd\u6355\u6349\u4eba\u7c7b\u7684\u542f\u53d1\u5f0f\u548c\u504f\u89c1\uff0c\u4e14\u8fd9\u4e9b\u504f\u89c1\u662f\u4e0d\u5747\u5300\u7684\u3002", "conclusion": "LLMs\u80fd\u591f\u6355\u6349\u4eba\u7c7b\u7684\u542f\u53d1\u5f0f\u548c\u8ba4\u77e5\u504f\u5dee\uff0c\u4f46\u8fd9\u4e9b\u504f\u5dee\u5e76\u975e\u5747\u5300\u5206\u5e03\uff0c\u800c\u662f\u53d7\u5230\u7279\u5b9a\u201c\u8bed\u8a00\u6e38\u620f\u201d\u60c5\u5883\u7684\u6fc0\u6d3b\u548c\u5f71\u54cd\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u91cd\u65b0\u5ba1\u89c6LLMs\u4e2d\u5173\u4e8e\u201c\u63a8\u7406\u201d\u4e0e\u201c\u8bb0\u5fc6\u201d\u7684\u4e89\u8bba\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u6697\u793a\u5176\u884c\u4e3a\u662f\u4e16\u754c\u5efa\u6a21\u548c\u5185\u5316\u4eba\u7c7b\u8ba4\u77e5\u6a21\u5f0f\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.01198", "pdf": "https://arxiv.org/pdf/2508.01198", "abs": "https://arxiv.org/abs/2508.01198", "authors": ["Yige Li", "Peihai Jiang", "Jun Sun", "Peng Shu", "Tianming Liu", "Zhen Xiang"], "title": "Adaptive Content Restriction for Large Language Models via Suffix Optimization", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Large Language Models (LLMs) have demonstrated significant success across\ndiverse applications. However, enforcing content restrictions remains a\nsignificant challenge due to their expansive output space. One aspect of\ncontent restriction is preventing LLMs from generating harmful content via\nmodel alignment approaches such as supervised fine-tuning (SFT). Yet, the need\nfor content restriction may vary significantly across user groups, change\nrapidly over time, and not always align with general definitions of\nharmfulness. Applying SFT to each of these specific use cases is impractical\ndue to the high computational, data, and storage demands. Motivated by this\nneed, we propose a new task called \\textit{Adaptive Content Restriction}\n(AdaCoRe), which focuses on lightweight strategies -- methods without model\nfine-tuning -- to prevent deployed LLMs from generating restricted terms for\nspecific use cases. We propose the first method for AdaCoRe, named\n\\textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to\nany prompt to a) prevent a target LLM from generating a set of restricted\nterms, while b) preserving the output quality. To evaluate AdaCoRe approaches,\nincluding our SOP, we create a new \\textit{Content Restriction Benchmark}\n(CoReBench), which contains 400 prompts for 80 restricted terms across 8\ncarefully selected categories. We demonstrate the effectiveness of SOP on\nCoReBench, which outperforms the system-level baselines such as system suffix\nby 15\\%, 17\\%, 10\\%, 9\\%, and 6\\% on average restriction rates for Gemma2-2B,\nMistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also\ndemonstrate that SOP is effective on POE, an online platform hosting various\ncommercial LLMs, highlighting its practicality in real-world scenarios.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5185\u5bb9\u9650\u5236\u96be\u4ee5\u9002\u5e94\u4e2a\u6027\u5316\u9700\u6c42\u3002\u672c\u6587\u63d0\u51faAdaCoRe\u4efb\u52a1\u53caSOP\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u4f18\u5316\u540e\u7f00\uff0c\u8f7b\u91cf\u7ea7\u5730\u963b\u6b62\u6a21\u578b\u751f\u6210\u53d7\u9650\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u6301\u8d28\u91cf\u3002SOP\u5728CoReBench\u57fa\u51c6\u6d4b\u8bd5\u548c\u5546\u4e1a\u6a21\u578b\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5185\u5bb9\u9650\u5236\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u5982SFT\u8ba1\u7b97\u3001\u6570\u636e\u548c\u5b58\u50a8\u6210\u672c\u9ad8\u6602\uff0c\u96be\u4ee5\u9002\u5e94\u7528\u6237\u7fa4\u4f53\u3001\u65f6\u95f4\u53d8\u5316\u4ee5\u53ca\u7279\u5b9a\u975e\u901a\u7528\u6709\u5bb3\u5185\u5bb9\u7684\u4e2a\u6027\u5316\u548c\u81ea\u9002\u5e94\u9650\u5236\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u201c\u81ea\u9002\u5e94\u5185\u5bb9\u9650\u5236\u201d\uff08AdaCoRe\uff09\u65b0\u4efb\u52a1\uff0c\u4e13\u6ce8\u4e8e\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u7684\u8f7b\u91cf\u7ea7\u7b56\u7565\u3002\u63d0\u51fa\u4e86\u9996\u4e2aAdaCoRe\u65b9\u6cd5\u201c\u540e\u7f00\u4f18\u5316\u201d\uff08SOP\uff09\uff0c\u901a\u8fc7\u5728\u63d0\u793a\u540e\u9644\u52a0\u7b80\u77ed\u7684\u4f18\u5316\u540e\u7f00\uff0c\u963b\u6b62LLM\u751f\u6210\u53d7\u9650\u5185\u5bb9\u5e76\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u3002\u4e3a\u8bc4\u4f30\uff0c\u521b\u5efa\u4e86\u5305\u542b400\u4e2a\u63d0\u793a\u548c80\u4e2a\u53d7\u9650\u672f\u8bed\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u201c\u5185\u5bb9\u9650\u5236\u57fa\u51c6\u201d\uff08CoReBench\uff09\u3002", "result": "SOP\u5728CoReBench\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5728Gemma2-2B\u3001Mistral-7B\u3001Vicuna-7B\u3001Llama3-8B\u548cLlama3.1-8B\u4e0a\u7684\u5e73\u5747\u9650\u5236\u7387\u5206\u522b\u6bd4\u7cfb\u7edf\u7ea7\u57fa\u7ebf\u9ad8\u51fa15%\u300117%\u300110%\u30019%\u548c6%\u3002SOP\u5728\u5728\u7ebf\u5e73\u53f0POE\uff08\u6258\u7ba1\u5546\u4e1aLLM\uff09\u4e0a\u4e5f\u6709\u6548\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u540e\u7f00\u4f18\u5316\uff08SOP\uff09\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u4e14\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u9002\u5e94\u5185\u5bb9\u9650\u5236\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.02031", "pdf": "https://arxiv.org/pdf/2508.02031", "abs": "https://arxiv.org/abs/2508.02031", "authors": ["Tian Qin", "Guang Cheng", "Zihan Chen", "Yuyang Zhou"], "title": "PRIME: Plasticity-Robust Incremental Model for Encrypted Traffic Classification in Dynamic Network Environments", "categories": ["cs.NI"], "comment": null, "summary": "With the continuous development of network environments and technologies,\nensuring cyber security and governance is increasingly challenging. Network\ntraffic classification(ETC) can analyzes attributes such as application\ncategories and malicious intent, supporting network management services like\nQoS optimization, intrusion detection, and targeted billing. As the prevalence\nof traffic encryption increases, deep learning models are relied upon for\ncontent-agnostic analysis of packet sequences. However, the emergence of new\nservices and attack variants often leads to incremental tasks for ETC models.\nTo ensure model effectiveness, incremental learning techniques are essential;\nhowever, recent studies indicate that neural networks experience declining\nplasticity as tasks increase. We identified plasticity issues in existing\nincremental learning methods across diverse traffic samples and proposed the\nPRIME framework. By observing the effective rank of model parameters and the\nproportion of inactive neurons, the PRIME architecture can appropriately\nincrease the parameter scale when the model's plasticity deteriorates.\nExperiments show that in multiple encrypted traffic datasets and different\ncategory increment scenarios, the PRIME architecture performs significantly\nbetter than other incremental learning algorithms with minimal increase in\nparameter scale.", "AI": {"tldr": "\u9488\u5bf9\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u4e2d\u589e\u91cf\u5b66\u4e60\u6a21\u578b\u5b58\u5728\u7684\u5851\u6027\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u51faPRIME\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u4ee5\u4fdd\u6301\u6a21\u578b\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u7f51\u7edc\u73af\u5883\u53d1\u5c55\u548c\u6d41\u91cf\u52a0\u5bc6\u666e\u53ca\uff0c\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\uff08ETC\uff09\u5728\u9762\u5bf9\u65b0\u670d\u52a1\u548c\u653b\u51fb\u53d8\u79cd\u65f6\uff0c\u9700\u8981\u589e\u91cf\u5b66\u4e60\u6280\u672f\u3002\u7136\u800c\uff0c\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u589e\u91cf\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u5851\u6027\u4e0b\u964d\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u5904\u7406\u591a\u6837\u5316\u6d41\u91cf\u6837\u672c\u65f6\u6709\u6548\u6027\u964d\u4f4e\u3002", "method": "\u672c\u6587\u63d0\u51faPRIME\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u89c2\u5bdf\u6a21\u578b\u53c2\u6570\u7684\u6709\u6548\u79e9\u548c\u975e\u6d3b\u8dc3\u795e\u7ecf\u5143\u7684\u6bd4\u4f8b\uff0c\u5728\u6a21\u578b\u5851\u6027\u52a3\u5316\u65f6\uff0c\u9002\u5f53\u5730\u589e\u52a0\u53c2\u6570\u89c4\u6a21\uff0c\u4ee5\u89e3\u51b3\u5851\u6027\u4e0b\u964d\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u591a\u4e2a\u52a0\u5bc6\u6d41\u91cf\u6570\u636e\u96c6\u548c\u4e0d\u540c\u7c7b\u522b\u589e\u91cf\u573a\u666f\u4e0b\uff0cPRIME\u67b6\u6784\u7684\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u589e\u91cf\u5b66\u4e60\u7b97\u6cd5\uff0c\u4e14\u53c2\u6570\u89c4\u6a21\u7684\u589e\u52a0\u6700\u5c0f\u3002", "conclusion": "PRIME\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u4e2d\u589e\u91cf\u5b66\u4e60\u7684\u5851\u6027\u4e0b\u964d\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u9ad8\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u89c4\u6a21\u7684\u6700\u5c0f\u5316\u589e\u957f\uff0c\u63d0\u5347\u4e86\u7f51\u7edc\u7ba1\u7406\u670d\u52a1\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.00877", "pdf": "https://arxiv.org/pdf/2508.00877", "abs": "https://arxiv.org/abs/2508.00877", "authors": ["Chao Yan", "Babak Mafakheri"], "title": "Satellite Connectivity Prediction for Fast-Moving Platforms", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Satellite connectivity is gaining increased attention as the demand for\nseamless internet access, especially in transportation and remote areas,\ncontinues to grow. For fast-moving objects such as aircraft, vehicles, or\ntrains, satellite connectivity is critical due to their mobility and frequent\npresence in areas without terrestrial coverage. Maintaining reliable\nconnectivity in these cases requires frequent switching between satellite\nbeams, constellations, or orbits. To enhance user experience and address\nchallenges like long switching times, Machine Learning (ML) algorithms can\nanalyze historical connectivity data and predict network quality at specific\nlocations. This allows for proactive measures, such as network switching before\nconnectivity issues arise. In this paper, we analyze a real dataset of\ncommunication between a Geostationary Orbit (GEO) satellite and aircraft over\nmultiple flights, using ML to predict signal quality. Our prediction model\nachieved an F1 score of 0.97 on the test data, demonstrating the accuracy of\nmachine learning in predicting signal quality during flight. By enabling\nseamless broadband service, including roaming between different satellite\nconstellations and providers, our model addresses the need for real-time\npredictions of signal quality. This approach can further be adapted to automate\nsatellite and beam-switching mechanisms to improve overall communication\nefficiency. The model can also be retrained and applied to any moving object\nwith satellite connectivity, using customized datasets, including connected\nvehicles and trains.", "AI": {"tldr": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u98de\u884c\u5668\u536b\u661f\u4fe1\u53f7\u8d28\u91cf\uff0c\u4ee5\u5b9e\u73b0\u65e0\u7f1d\u8fde\u63a5\u5e76\u652f\u6301\u9ad8\u6548\u7684\u536b\u661f/\u6ce2\u675f\u5207\u6362\u3002", "motivation": "\u968f\u7740\u5bf9\u65e0\u7f1d\u4e92\u8054\u7f51\u63a5\u5165\u9700\u6c42\u7684\u589e\u957f\uff0c\u5c24\u5176\u662f\u5728\u4ea4\u901a\u548c\u504f\u8fdc\u5730\u533a\uff0c\u536b\u661f\u8fde\u63a5\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u5bf9\u4e8e\u98de\u673a\u7b49\u9ad8\u901f\u79fb\u52a8\u7269\u4f53\uff0c\u4fdd\u6301\u53ef\u9760\u8fde\u63a5\u9700\u8981\u9891\u7e41\u5207\u6362\u536b\u661f\u6ce2\u675f\u6216\u661f\u5ea7\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u7528\u6237\u4f53\u9a8c\u53d7\u635f\u548c\u8fde\u63a5\u4e2d\u65ad\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u9884\u6d4b\u7f51\u7edc\u8d28\u91cf\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u5206\u6790\u4e00\u4e2a\u5305\u542bGEO\uff08\u5730\u7403\u9759\u6b62\u8f68\u9053\uff09\u536b\u661f\u4e0e\u98de\u673a\u5728\u591a\u6b21\u98de\u884c\u4e2d\u901a\u4fe1\u7684\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u6765\u9884\u6d4b\u4fe1\u53f7\u8d28\u91cf\u3002", "result": "\u9884\u6d4b\u6a21\u578b\u5728\u6d4b\u8bd5\u6570\u636e\u4e0a\u8fbe\u5230\u4e860.97\u7684F1\u5206\u6570\uff0c\u8bc1\u660e\u4e86\u673a\u5668\u5b66\u4e60\u5728\u98de\u884c\u4e2d\u9884\u6d4b\u4fe1\u53f7\u8d28\u91cf\u7684\u51c6\u786e\u6027\u3002\u8fd9\u652f\u6301\u4e86\u5b9e\u65f6\u4fe1\u53f7\u8d28\u91cf\u9884\u6d4b\uff0c\u4ece\u800c\u5b9e\u73b0\u4e0d\u540c\u536b\u661f\u661f\u5ea7\u548c\u63d0\u4f9b\u5546\u4e4b\u95f4\u7684\u65e0\u7f1d\u5bbd\u5e26\u670d\u52a1\uff08\u5305\u62ec\u6f2b\u6e38\uff09\u3002", "conclusion": "\u8be5\u9884\u6d4b\u6a21\u578b\u53ef\u4ee5\u5e94\u7528\u4e8e\u81ea\u52a8\u5316\u536b\u661f\u548c\u6ce2\u675f\u5207\u6362\u673a\u5236\uff0c\u4ee5\u63d0\u9ad8\u6574\u4f53\u901a\u4fe1\u6548\u7387\u3002\u5b83\u8fd8\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u91cd\u65b0\u8bad\u7ec3\u5e76\u5e94\u7528\u4e8e\u4efb\u4f55\u5177\u6709\u536b\u661f\u8fde\u63a5\u7684\u79fb\u52a8\u7269\u4f53\uff08\u5982\u8054\u7f51\u6c7d\u8f66\u548c\u706b\u8f66\uff09\uff0c\u4ece\u800c\u4e3a\u672a\u6765\u7684\u5e94\u7528\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2508.00913", "pdf": "https://arxiv.org/pdf/2508.00913", "abs": "https://arxiv.org/abs/2508.00913", "authors": ["Mohammad Mohammadi", "Ziyi Wu", "Igor Gilitschenski"], "title": "TESPEC: Temporally-Enhanced Self-Supervised Pretraining for Event Cameras", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted at IEEE/CVF International Conference on Computer Vision\n  (ICCV) 2025", "summary": "Long-term temporal information is crucial for event-based perception tasks,\nas raw events only encode pixel brightness changes. Recent works show that when\ntrained from scratch, recurrent models achieve better results than feedforward\nmodels in these tasks. However, when leveraging self-supervised pre-trained\nweights, feedforward models can outperform their recurrent counterparts.\nCurrent self-supervised learning (SSL) methods for event-based pre-training\nlargely mimic RGB image-based approaches. They pre-train feedforward models on\nraw events within a short time interval, ignoring the temporal information of\nevents. In this work, we introduce TESPEC, a self-supervised pre-training\nframework tailored for learning spatio-temporal information. TESPEC is\nwell-suited for recurrent models, as it is the first framework to leverage long\nevent sequences during pre-training. TESPEC employs the masked image modeling\nparadigm with a new reconstruction target. We design a novel method to\naccumulate events into pseudo grayscale videos containing high-level semantic\ninformation about the underlying scene, which is robust to sensor noise and\nreduces motion blur. Reconstructing this target thus requires the model to\nreason about long-term history of events. Extensive experiments demonstrate our\nstate-of-the-art results in downstream tasks, including object detection,\nsemantic segmentation, and monocular depth estimation. Project webpage:\nhttps://mhdmohammadi.github.io/TESPEC_webpage.", "AI": {"tldr": "TESPEC\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u9996\u6b21\u5229\u7528\u957f\u4e8b\u4ef6\u5e8f\u5217\u8bad\u7ec3\u5faa\u73af\u6a21\u578b\uff0c\u901a\u8fc7\u91cd\u5efa\u4f2a\u7070\u5ea6\u89c6\u9891\u6765\u5b66\u4e60\u4e8b\u4ef6\u6570\u636e\u4e2d\u7684\u65f6\u7a7a\u4fe1\u606f\uff0c\u5e76\u5728\u591a\u9879\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4e8b\u4ef6\u611f\u77e5\u4efb\u52a1\u4e2d\u957f\u671f\u65f6\u5e8f\u4fe1\u606f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u4e8b\u4ef6\u6570\u636e\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u591a\u6a21\u4effRGB\u56fe\u50cf\u65b9\u6cd5\uff0c\u5728\u77ed\u65f6\u95f4\u7a97\u53e3\u5185\u9884\u8bad\u7ec3\u524d\u9988\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u4e8b\u4ef6\u7684\u56fa\u6709\u65f6\u95f4\u7279\u6027\uff0c\u5bfc\u81f4\u5faa\u73af\u6a21\u578b\u5728\u5229\u7528\u9884\u8bad\u7ec3\u6743\u91cd\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86TESPEC\uff0c\u4e00\u4e2a\u4e13\u4e3a\u5b66\u4e60\u65f6\u7a7a\u4fe1\u606f\u800c\u8bbe\u8ba1\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6846\u67b6\u3002\u5b83\u662f\u9996\u4e2a\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5229\u7528\u957f\u4e8b\u4ef6\u5e8f\u5217\u7684\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5faa\u73af\u6a21\u578b\u3002TESPEC\u91c7\u7528\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u8303\u5f0f\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u91cd\u5efa\u76ee\u6807\uff1a\u5c06\u4e8b\u4ef6\u7d2f\u79ef\u6210\u5305\u542b\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\u3001\u5bf9\u4f20\u611f\u5668\u566a\u58f0\u9c81\u68d2\u4e14\u51cf\u5c11\u8fd0\u52a8\u6a21\u7cca\u7684\u4f2a\u7070\u5ea6\u89c6\u9891\u3002\u6a21\u578b\u9700\u8981\u63a8\u7406\u4e8b\u4ef6\u7684\u957f\u671f\u5386\u53f2\u624d\u80fd\u91cd\u5efa\u6b64\u76ee\u6807\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTESPEC\u5728\u5305\u62ec\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5728\u5185\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u7ed3\u679c\u3002", "conclusion": "TESPEC\u901a\u8fc7\u521b\u65b0\u7684\u4f2a\u7070\u5ea6\u89c6\u9891\u91cd\u5efa\u76ee\u6807\uff0c\u6210\u529f\u5730\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5229\u7528\u4e86\u4e8b\u4ef6\u6570\u636e\u4e2d\u7684\u957f\u671f\u65f6\u5e8f\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5404\u79cd\u4e8b\u4ef6\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5904\u7406\u4e8b\u4ef6\u65f6\u7a7a\u4fe1\u606f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.00914", "pdf": "https://arxiv.org/pdf/2508.00914", "abs": "https://arxiv.org/abs/2508.00914", "authors": ["Dominic Simon", "Rickard Ewetz"], "title": "Knowledge Editing for Multi-Hop Question Answering Using Semantic Analysis", "categories": ["cs.AI", "cs.LG"], "comment": "14 pages, 15 figures, pre-print of paper accepted to IJCAI 2025", "summary": "Large Language Models (LLMs) require lightweight avenues of updating stored\ninformation that has fallen out of date. Knowledge Editing (KE) approaches have\nbeen successful in updating model knowledge for simple factual queries but\nstruggle with handling tasks that require compositional reasoning such as\nmulti-hop question answering (MQA). We observe that existing knowledge editors\nleverage decompositional techniques that result in illogical reasoning\nprocesses. In this paper, we propose a knowledge editor for MQA based on\nsemantic analysis called CHECK. Our framework is based on insights from an\nanalogy between compilers and reasoning using LLMs. Similar to how source code\nis first compiled before being executed, we propose to semantically analyze\nreasoning chains before executing the chains to answer questions. Reasoning\nchains with semantic errors are revised to ensure consistency through logic\noptimization and re-prompting the LLM model at a higher temperature. We\nevaluate the effectiveness of CHECK against five state-of-the-art frameworks on\nfour datasets and achieve an average 22.8% improved MQA accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCHECK\uff0c\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5206\u6790\u7684\u77e5\u8bc6\u7f16\u8f91\u5668\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u8df3\u95ee\u7b54\uff08MQA\uff09\u4e2d\u77e5\u8bc6\u7f16\u8f91\u9762\u4e34\u7684\u7ec4\u5408\u63a8\u7406\u96be\u9898\u3002\u901a\u8fc7\u5728\u6267\u884c\u63a8\u7406\u94fe\u524d\u8fdb\u884c\u8bed\u4e49\u5206\u6790\u548c\u4fee\u6b63\uff0cCHECK\u663e\u8457\u63d0\u9ad8\u4e86MQA\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u77e5\u8bc6\u7f16\u8f91\uff08KE\uff09\u65b9\u6cd5\u5728\u66f4\u65b0\u7b80\u5355\u4e8b\u5b9e\u77e5\u8bc6\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u9700\u8981\u7ec4\u5408\u63a8\u7406\u7684\u591a\u8df3\u95ee\u7b54\uff08MQA\uff09\u7b49\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u8fd9\u662f\u56e0\u4e3a\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7684\u5206\u89e3\u6280\u672f\u5e38\u5e38\u5bfc\u81f4\u4e0d\u5408\u903b\u8f91\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u65e0\u6cd5\u6709\u6548\u66f4\u65b0\u8fc7\u65f6\u7684\u4fe1\u606f\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCHECK\u7684\u77e5\u8bc6\u7f16\u8f91\u5668\uff0c\u4e13\u6ce8\u4e8e\u591a\u8df3\u95ee\u7b54\uff08MQA\uff09\uff0c\u5176\u6838\u5fc3\u57fa\u4e8e\u8bed\u4e49\u5206\u6790\u3002\u8be5\u6846\u67b6\u501f\u9274\u4e86\u7f16\u8bd1\u5668\u7684\u5de5\u4f5c\u539f\u7406\uff1a\u5982\u540c\u6e90\u4ee3\u7801\u5728\u6267\u884c\u524d\u9700\u7f16\u8bd1\u4e00\u6837\uff0cCHECK\u5728LLM\u6267\u884c\u63a8\u7406\u94fe\u4ee5\u56de\u7b54\u95ee\u9898\u4e4b\u524d\uff0c\u9996\u5148\u5bf9\u5176\u8fdb\u884c\u8bed\u4e49\u5206\u6790\u3002\u5bf9\u4e8e\u5b58\u5728\u8bed\u4e49\u9519\u8bef\u7684\u63a8\u7406\u94fe\uff0cCHECK\u4f1a\u901a\u8fc7\u903b\u8f91\u4f18\u5316\u548c\u4ee5\u66f4\u9ad8\u6e29\u5ea6\u91cd\u65b0\u63d0\u793aLLM\u6a21\u578b\u7684\u65b9\u5f0f\u8fdb\u884c\u4fee\u8ba2\uff0c\u4ee5\u786e\u4fdd\u63a8\u7406\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cCHECK\u4e0e\u4e94\u79cd\u6700\u5148\u8fdb\u7684\u6846\u67b6\u8fdb\u884c\u4e86\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5e73\u5747\u5c06\u591a\u8df3\u95ee\u7b54\uff08MQA\uff09\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e8622.8%\u3002", "conclusion": "CHECK\u901a\u8fc7\u5f15\u5165\u5bf9\u63a8\u7406\u94fe\u7684\u8bed\u4e49\u5206\u6790\u548c\u9519\u8bef\u4fee\u6b63\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u5904\u7406\u591a\u8df3\u95ee\u7b54\u65f6\u7684\u77e5\u8bc6\u7f16\u8f91\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u77e5\u8bc6\u66f4\u65b0\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.01213", "pdf": "https://arxiv.org/pdf/2508.01213", "abs": "https://arxiv.org/abs/2508.01213", "authors": ["Shengqi Zhu", "Jeffrey M. Rzeszotarski", "David Mimno"], "title": "Show or Tell? Modeling the evolution of request-making in Human-LLM conversations", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Chat logs provide a rich source of information about LLM users, but patterns\nof user behavior are often masked by the variability of queries. We present a\nnew task, segmenting chat queries into contents of requests, roles,\nquery-specific context, and additional expressions. We find that, despite the\nfamiliarity of chat-based interaction, request-making in LLM queries remains\nsignificantly different from comparable human-human interactions. With the data\nresource, we introduce an important perspective of diachronic analyses with\nuser expressions. We find that query patterns vary between early ones\nemphasizing requests, and individual users explore patterns but tend to\nconverge with experience. Finally, we show that model capabilities affect user\nbehavior, particularly with the introduction of new models, which are traceable\nat the community level.", "AI": {"tldr": "\u901a\u8fc7\u5c06LLM\u804a\u5929\u67e5\u8be2\u5206\u5272\u4e3a\u7ed3\u6784\u5316\u5143\u7d20\uff0c\u7814\u7a76\u53d1\u73b0LLM\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\u5f02\u4e8e\u4eba\u9645\u4ea4\u4e92\uff0c\u7528\u6237\u67e5\u8be2\u6a21\u5f0f\u968f\u65f6\u95f4\u6f14\u53d8\u5e76\u8d8b\u4e8e\u6536\u655b\uff0c\u4e14\u6a21\u578b\u80fd\u529b\u5bf9\u7528\u6237\u884c\u4e3a\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1LLM\u804a\u5929\u65e5\u5fd7\u5305\u542b\u4e30\u5bcc\u7684\u7528\u6237\u4fe1\u606f\uff0c\u4f46\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\u5e38\u88ab\u67e5\u8be2\u7684\u591a\u53d8\u6027\u6240\u63a9\u76d6\uff0c\u96be\u4ee5\u6709\u6548\u5206\u6790\u3002", "method": "\u63d0\u51fa\u5e76\u6267\u884c\u4e00\u9879\u65b0\u4efb\u52a1\uff1a\u5c06\u804a\u5929\u67e5\u8be2\u5206\u5272\u6210\u8bf7\u6c42\u5185\u5bb9\u3001\u89d2\u8272\u3001\u67e5\u8be2\u7279\u5b9a\u4e0a\u4e0b\u6587\u548c\u989d\u5916\u8868\u8fbe\u3002\u5e76\u5229\u7528\u6b64\u6570\u636e\u8d44\u6e90\u8fdb\u884c\u7528\u6237\u8868\u8fbe\u7684\u5386\u65f6\u5206\u6790\u3002", "result": "1. LLM\u67e5\u8be2\u4e2d\u7684\u8bf7\u6c42\u8868\u8fbe\u4e0e\u4eba\u7c7b\u95f4\u4ea4\u4e92\u6709\u663e\u8457\u4e0d\u540c\u30022. \u65e9\u671f\u67e5\u8be2\u6a21\u5f0f\u4fa7\u91cd\u8bf7\u6c42\uff0c\u4e2a\u4f53\u7528\u6237\u867d\u63a2\u7d22\u591a\u6837\u6a21\u5f0f\u4f46\u968f\u7ecf\u9a8c\u8d8b\u4e8e\u6536\u655b\u30023. \u6a21\u578b\u80fd\u529b\uff08\u7279\u522b\u662f\u65b0\u6a21\u578b\u5f15\u5165\uff09\u4f1a\u5f71\u54cd\u7528\u6237\u884c\u4e3a\uff0c\u5e76\u5728\u793e\u533a\u5c42\u9762\u53ef\u8ffd\u8e2a\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u67e5\u8be2\u5206\u5272\u548c\u5386\u65f6\u5206\u6790\uff0c\u63ed\u793a\u4e86LLM\u7528\u6237\u884c\u4e3a\u7684\u72ec\u7279\u6027\u3001\u6f14\u53d8\u89c4\u5f8b\uff0c\u5e76\u8bc1\u5b9e\u4e86\u6a21\u578b\u80fd\u529b\u5bf9\u7528\u6237\u884c\u4e3a\u7684\u5173\u952e\u5f71\u54cd\u3002"}}
{"id": "2508.02282", "pdf": "https://arxiv.org/pdf/2508.02282", "abs": "https://arxiv.org/abs/2508.02282", "authors": ["Ziyue Huang", "Chungang Lin", "Weiyao Zhang", "Xuying Meng", "Yujun Zhang"], "title": "Distillation-Enhanced Clustering Acceleration for Encrypted Traffic Classification", "categories": ["cs.NI"], "comment": "Under review", "summary": "Traffic classification plays a significant role in network service\nmanagement. The advancement of deep learning has established pretrained models\nas a robust approach for this task. However, contemporary encrypted traffic\nclassification systems face dual limitations. Firstly, pretrained models\ntypically exhibit large-scale architectures, where their extensive\nparameterization results in slow inference speeds and high computational\nlatency. Secondly, reliance on labeled data for fine-tuning restricts these\nmodels to predefined supervised classes, creating a bottleneck when novel\ntraffic types emerge in the evolving Internet landscape. To address these\nchallenges, we propose NetClus, a novel framework integrating pretrained models\nwith distillation-enhanced clustering acceleration. During fine-tuning, NetClus\nfirst introduces a cluster-friendly loss to jointly reshape the latent space\nfor both classification and clustering. With the fine-tuned model, it distills\nthe model into a lightweight Feed-Forward Neural Network model to retain\nsemantics. During inference, NetClus performs heuristic merge with near-linear\nruntime, and valid the cluster purity with newly proposed metrics ASI to\nidentify emergent traffic types while expediting classification. Benchmarked\nagainst existing pretrained methods, NetClus achieves up to 6.2x acceleration\nwhile maintaining classification degradation below 1%.", "AI": {"tldr": "NetClus\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u84b8\u998f\u589e\u5f3a\u7684\u805a\u7c7b\u52a0\u901f\uff0c\u89e3\u51b3\u4e86\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u4e2d\u73b0\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u6162\u548c\u65b0\u578b\u6d41\u91cf\u8bc6\u522b\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u7cfb\u7edf\u9762\u4e34\u53cc\u91cd\u9650\u5236\uff1a\u4e00\u662f\u9884\u8bad\u7ec3\u6a21\u578b\u89c4\u6a21\u5e9e\u5927\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162\u3001\u8ba1\u7b97\u5ef6\u8fdf\u9ad8\uff1b\u4e8c\u662f\u4f9d\u8d56\u6807\u8bb0\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u9650\u5236\u4e86\u6a21\u578b\u8bc6\u522b\u4e0d\u65ad\u6d8c\u73b0\u7684\u65b0\u578b\u6d41\u91cf\u7684\u80fd\u529b\u3002", "method": "NetClus\u6846\u67b6\u5728\u5fae\u8c03\u9636\u6bb5\u5f15\u5165\u805a\u7c7b\u53cb\u597d\u635f\u5931\u51fd\u6570\u4ee5\u91cd\u5851\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u8fdb\u4e00\u6b65\u5c06\u6a21\u578b\u84b8\u998f\u4e3a\u8f7b\u91cf\u7ea7\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0c\u901a\u8fc7\u542f\u53d1\u5f0f\u5408\u5e76\u548c\u65b0\u63d0\u51fa\u7684ASI\u6307\u6807\u9a8c\u8bc1\u805a\u7c7b\u7eaf\u5ea6\uff0c\u4ee5\u8bc6\u522b\u65b0\u578b\u6d41\u91cf\u5e76\u52a0\u901f\u5206\u7c7b\u3002", "result": "NetClus\u4e0e\u73b0\u6709\u9884\u8bad\u7ec3\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe6.2\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u5206\u7c7b\u6027\u80fd\u4e0b\u964d\u4e0d\u52301%\u3002", "conclusion": "NetClus\u6210\u529f\u89e3\u51b3\u4e86\u52a0\u5bc6\u6d41\u91cf\u5206\u7c7b\u4e2d\u9884\u8bad\u7ec3\u6a21\u578b\u6548\u7387\u4f4e\u4e0b\u548c\u5bf9\u65b0\u578b\u6d41\u91cf\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u901f\u5ea6\u548c\u8bc6\u522b\u80fd\u529b\u3002"}}
{"id": "2508.00879", "pdf": "https://arxiv.org/pdf/2508.00879", "abs": "https://arxiv.org/abs/2508.00879", "authors": ["Moutaz Bellah Bentrad", "Adel Ghoggal", "Tahar Bahi", "Abderaouf Bahi"], "title": "GNN-ASE: Graph-Based Anomaly Detection and Severity Estimation in Three-Phase Induction Machines", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The diagnosis of induction machines has traditionally relied on model-based\nmethods that require the development of complex dynamic models, making them\ndifficult to implement and computationally expensive. To overcome these\nlimitations, this paper proposes a model-free approach using Graph Neural\nNetworks (GNNs) for fault diagnosis in induction machines. The focus is on\ndetecting multiple fault types -- including eccentricity, bearing defects, and\nbroken rotor bars -- under varying severity levels and load conditions. Unlike\ntraditional approaches, raw current and vibration signals are used as direct\ninputs, eliminating the need for signal preprocessing or manual feature\nextraction. The proposed GNN-ASE model automatically learns and extracts\nrelevant features from raw inputs, leveraging the graph structure to capture\ncomplex relationships between signal types and fault patterns. It is evaluated\nfor both individual fault detection and multi-class classification of combined\nfault conditions. Experimental results demonstrate the effectiveness of the\nproposed model, achieving 92.5\\% accuracy for eccentricity defects, 91.2\\% for\nbearing faults, and 93.1\\% for broken rotor bar detection. These findings\nhighlight the model's robustness and generalization capability across different\noperational scenarios. The proposed GNN-based framework offers a lightweight\nyet powerful solution that simplifies implementation while maintaining high\ndiagnostic performance. It stands as a promising alternative to conventional\nmodel-based diagnostic techniques for real-world induction machine monitoring\nand predictive maintenance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u65e0\u6a21\u578b\u65b9\u6cd5\uff0c\u5229\u7528\u539f\u59cb\u7535\u6d41\u548c\u632f\u52a8\u4fe1\u53f7\uff0c\u81ea\u52a8\u8bca\u65ad\u611f\u5e94\u7535\u673a\u591a\u79cd\u6545\u969c\u7c7b\u578b\u53ca\u5176\u4e25\u91cd\u7a0b\u5ea6\uff0c\u5e76\u5728\u4e0d\u540c\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u611f\u5e94\u7535\u673a\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u590d\u6742\u7684\u6a21\u578b\uff0c\u5b9e\u65bd\u56f0\u96be\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u9ad8\u6548\u7684\u65e0\u6a21\u578b\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aGNN-ASE\u7684\u65e0\u6a21\u578b\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u76f4\u63a5\u5904\u7406\u539f\u59cb\u7535\u6d41\u548c\u632f\u52a8\u4fe1\u53f7\uff0c\u65e0\u9700\u4fe1\u53f7\u9884\u5904\u7406\u6216\u624b\u52a8\u7279\u5f81\u63d0\u53d6\u3002\u8be5\u6a21\u578b\u81ea\u52a8\u4ece\u539f\u59cb\u8f93\u5165\u4e2d\u5b66\u4e60\u548c\u63d0\u53d6\u76f8\u5173\u7279\u5f81\uff0c\u5e76\u5229\u7528\u56fe\u7ed3\u6784\u6355\u83b7\u4fe1\u53f7\u7c7b\u578b\u548c\u6545\u969c\u6a21\u5f0f\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002\u5b83\u7528\u4e8e\u68c0\u6d4b\u504f\u5fc3\u3001\u8f74\u627f\u7f3a\u9677\u548c\u8f6c\u5b50\u65ad\u6761\u7b49\u591a\u79cd\u6545\u969c\u7c7b\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5bf9\u504f\u5fc3\u7f3a\u9677\u7684\u8bca\u65ad\u51c6\u786e\u7387\u4e3a92.5%\uff0c\u8f74\u627f\u6545\u969c\u4e3a91.2%\uff0c\u8f6c\u5b50\u65ad\u6761\u68c0\u6d4b\u4e3a93.1%\u3002\u8fd9\u4e9b\u7ed3\u679c\u5c55\u793a\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u8fd0\u884c\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eGNN\u7684\u6846\u67b6\u4e3a\u611f\u5e94\u7535\u673a\u6545\u969c\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u800c\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u7b80\u5316\u4e86\u5b9e\u65bd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8bca\u65ad\u6027\u80fd\u3002\u5b83\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u66ff\u4ee3\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u8bca\u65ad\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u7684\u611f\u5e94\u7535\u673a\u76d1\u63a7\u548c\u9884\u6d4b\u6027\u7ef4\u62a4\u3002"}}
{"id": "2508.00941", "pdf": "https://arxiv.org/pdf/2508.00941", "abs": "https://arxiv.org/abs/2508.00941", "authors": ["Hassan Ugail", "Hamad Mansour Alawar", "AbdulNasser Abbas Zehi", "Ahmed Mohammad Alkendi", "Ismail Lujain Jaleel"], "title": "Latent Diffusion Based Face Enhancement under Degraded Conditions for Forensic Face Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Face recognition systems experience severe performance degradation when\nprocessing low-quality forensic evidence imagery. This paper presents an\nevaluation of latent diffusion-based enhancement for improving face recognition\nunder forensically relevant degradations. Using a dataset of 3,000 individuals\nfrom LFW with 24,000 recognition attempts, we implement the Flux.1 Kontext Dev\npipeline with Facezoom LoRA adaptation to test against seven degradation\ncategories, including compression artefacts, blur effects, and noise\ncontamination. Our approach demonstrates substantial improvements, increasing\noverall recognition accuracy from 29.1% to 84.5% (55.4 percentage point\nimprovement, 95% CI: [54.1, 56.7]). Statistical analysis reveals significant\nperformance gains across all degradation types, with effect sizes exceeding\nconventional thresholds for practical significance. These findings establish\nthe potential of sophisticated diffusion based enhancement in forensic face\nrecognition applications.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u589e\u5f3a\u6280\u672f\u5982\u4f55\u663e\u8457\u63d0\u5347\u4f4e\u8d28\u91cf\u6cd5\u8bc1\u4eba\u8138\u56fe\u50cf\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5728\u5904\u7406\u4f4e\u8d28\u91cf\u6cd5\u8bc1\u56fe\u50cf\u65f6\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\uff0c\u4e9f\u9700\u63d0\u5347\u3002", "method": "\u4f7f\u7528LFW\u6570\u636e\u96c6\uff083000\u4e2a\u4e2a\u4f53\uff0c24000\u6b21\u8bc6\u522b\u5c1d\u8bd5\uff09\uff0c\u8bc4\u4f30\u4e86\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u7684\u56fe\u50cf\u589e\u5f3a\u6280\u672f\uff0c\u5177\u4f53\u5b9e\u73b0\u4e86Flux.1 Kontext Dev\u7ba1\u9053\u5e76\u7ed3\u5408Facezoom LoRA\u9002\u5e94\uff0c\u9488\u5bf9\u4e03\u7c7b\u964d\u7ea7\uff08\u5305\u62ec\u538b\u7f29\u4f2a\u5f71\u3001\u6a21\u7cca\u6548\u5e94\u548c\u566a\u58f0\u6c61\u67d3\uff09\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7f\u6574\u4f53\u8bc6\u522b\u51c6\u786e\u7387\u4ece29.1%\u5927\u5e45\u63d0\u5347\u81f384.5%\uff08\u63d0\u9ad8\u4e8655.4\u4e2a\u767e\u5206\u70b9\uff09\uff0c\u7edf\u8ba1\u5206\u6790\u663e\u793a\u6240\u6709\u964d\u7ea7\u7c7b\u578b\u5747\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\u4e86\u590d\u6742\u7684\u6269\u6563\u57fa\u589e\u5f3a\u6280\u672f\u5728\u6cd5\u8bc1\u4eba\u8138\u8bc6\u522b\u5e94\u7528\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.00967", "pdf": "https://arxiv.org/pdf/2508.00967", "abs": "https://arxiv.org/abs/2508.00967", "authors": ["Massoud Pourmandi"], "title": "Cooperative Perception: A Resource-Efficient Framework for Multi-Drone 3D Scene Reconstruction Using Federated Diffusion and NeRF", "categories": ["cs.AI", "cs.RO", "68T07, 68T45, 93C85", "I.2.6; I.2.9; I.2.10; I.4.8"], "comment": "15 pages, 3 figures, 1 table, 1 algorithm. Preprint based on NeurIPS\n  2024 template", "summary": "The proposal introduces an innovative drone swarm perception system that aims\nto solve problems related to computational limitations and low-bandwidth\ncommunication, and real-time scene reconstruction. The framework enables\nefficient multi-agent 3D/4D scene synthesis through federated learning of\nshared diffusion model and YOLOv12 lightweight semantic extraction and local\nNeRF updates while maintaining privacy and scalability. The framework redesigns\ngenerative diffusion models for joint scene reconstruction, and improves\ncooperative scene understanding, while adding semantic-aware compression\nprotocols. The approach can be validated through simulations and potential\nreal-world deployment on drone testbeds, positioning it as a disruptive\nadvancement in multi-agent AI for autonomous systems.", "AI": {"tldr": "\u8be5\u63d0\u6848\u5f15\u5165\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65e0\u4eba\u673a\u7fa4\u611f\u77e5\u7cfb\u7edf\uff0c\u65e8\u5728\u89e3\u51b3\u8ba1\u7b97\u9650\u5236\u3001\u4f4e\u5e26\u5bbd\u901a\u4fe1\u548c\u5b9e\u65f6\u573a\u666f\u91cd\u5efa\u95ee\u9898\uff0c\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u3001\u6269\u6563\u6a21\u578b\u3001YOLOv12\u548cNeRF\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f533D/4D\u573a\u666f\u5408\u6210\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u7fa4\u5728\u8ba1\u7b97\u80fd\u529b\u3001\u4f4e\u5e26\u5bbd\u901a\u4fe1\u548c\u5b9e\u65f6\u573a\u666f\u91cd\u5efa\u65b9\u9762\u9762\u4e34\u7684\u6311\u6218\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f533D/4D\u573a\u666f\u5408\u6210\u3002", "method": "\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u5171\u4eab\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408YOLOv12\u8f7b\u91cf\u7ea7\u8bed\u4e49\u63d0\u53d6\u548c\u5c40\u90e8NeRF\u66f4\u65b0\u3002\u8be5\u6846\u67b6\u91cd\u65b0\u8bbe\u8ba1\u4e86\u751f\u6210\u5f0f\u6269\u6563\u6a21\u578b\u7528\u4e8e\u8054\u5408\u573a\u666f\u91cd\u5efa\uff0c\u63d0\u5347\u534f\u4f5c\u573a\u666f\u7406\u89e3\uff0c\u5e76\u589e\u52a0\u4e86\u8bed\u4e49\u611f\u77e5\u538b\u7f29\u534f\u8bae\uff0c\u540c\u65f6\u4fdd\u6301\u9690\u79c1\u548c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f533D/4D\u573a\u666f\u5408\u6210\uff0c\u63d0\u5347\u534f\u4f5c\u573a\u666f\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u4fdd\u6301\u9690\u79c1\u548c\u53ef\u6269\u5c55\u6027\u3002\u8be5\u65b9\u6cd5\u53ef\u901a\u8fc7\u6a21\u62df\u548c\u6f5c\u5728\u7684\u771f\u5b9e\u4e16\u754c\u65e0\u4eba\u673a\u5e73\u53f0\u90e8\u7f72\u8fdb\u884c\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u591a\u667a\u80fd\u4f53AI\u548c\u81ea\u4e3b\u7cfb\u7edf\u9886\u57df\u4ee3\u8868\u4e86\u4e00\u9879\u98a0\u8986\u6027\u7684\u8fdb\u5c55\u3002"}}
{"id": "2508.01222", "pdf": "https://arxiv.org/pdf/2508.01222", "abs": "https://arxiv.org/abs/2508.01222", "authors": ["Ethan Hsu", "Hong Meng Yam", "Ines Bouissou", "Aaron Murali John", "Raj Thota", "Josh Koe", "Vivek Sarath Putta", "G K Dharesan", "Alexander Spangher", "Shikhar Murty", "Tenghao Huang", "Christopher D. Manning"], "title": "WebDS: An End-to-End Benchmark for Web-based Data Science", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages", "summary": "A large portion of real-world data science tasks are complex and require\nmulti-hop web-based interactions: finding appropriate data available on the\ninternet, synthesizing real-time data of various modalities from different\nlocations, and producing summarized analyses. Existing web benchmarks often\nfocus on simplistic interactions, such as form submissions or e-commerce\ntransactions, and often do not require diverse tool-using capabilities required\nfor web based data science. Conversely, traditional data science benchmarks\ntypically concentrate on static, often textually bound datasets and do not\nassess end-to-end workflows that encompass data acquisition, cleaning,\nanalysis, and insight generation. In response, we introduce WebDS, the first\nend-to-end web-based data science benchmark. It comprises 870 web-based data\nscience tasks across 29 diverse websites from structured government data\nportals to unstructured news media, challenging agents to perform complex,\nmulti-step operations requiring the use of tools and heterogeneous data formats\nthat better reflect the realities of modern data analytics. Evaluations of\ncurrent SOTA LLM agents indicate significant performance gaps in accomplishing\nthese tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web\nVoyager, successfully completes only 15% of tasks in WebDS, which our analysis\nsuggests is due to new failure modes like poor information grounding,\nrepetitive behavior and shortcut-taking that agents performing WebDS' tasks\ndisplay. By providing a more robust and realistic testing ground, WebDS sets\nthe stage for significant advances in the development of practically useful\nLLM-based data science.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecdWebDS\uff0c\u9996\u4e2a\u7aef\u5230\u7aef\u7f51\u9875\u6570\u636e\u79d1\u5b66\u57fa\u51c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u672a\u80fd\u6db5\u76d6\u771f\u5b9e\u4e16\u754c\u590d\u6742\u7f51\u9875\u6570\u636e\u79d1\u5b66\u4efb\u52a1\u7684\u95ee\u9898\u3002\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684LLM\u4ee3\u7406\u5728\u8be5\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u66b4\u9732\u51fa\u65b0\u7684\u5931\u6548\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u7f51\u9875\u57fa\u51c6\u8fc7\u4e8e\u7b80\u5316\uff0c\u4f20\u7edf\u6570\u636e\u79d1\u5b66\u57fa\u51c6\u5219\u5ffd\u89c6\u7aef\u5230\u7aef\u7f51\u9875\u4ea4\u4e92\uff0c\u5747\u65e0\u6cd5\u53cd\u6620\u5b9e\u9645\u6570\u636e\u79d1\u5b66\u4efb\u52a1\u4e2d\u591a\u8df3\u3001\u591a\u6a21\u6001\u3001\u5de5\u5177\u4f7f\u7528\u7b49\u590d\u6742\u9700\u6c42\u3002", "method": "\u5f15\u5165WebDS\u57fa\u51c6\uff0c\u5305\u542b870\u4e2a\u6765\u81ea29\u4e2a\u4e0d\u540c\u7f51\u7ad9\u7684\u7f51\u9875\u6570\u636e\u79d1\u5b66\u4efb\u52a1\u3002\u8fd9\u4e9b\u4efb\u52a1\u8981\u6c42\u4ee3\u7406\u6267\u884c\u590d\u6742\u7684\u591a\u6b65\u64cd\u4f5c\uff0c\u6d89\u53ca\u5de5\u5177\u4f7f\u7528\u548c\u5f02\u6784\u6570\u636e\u683c\u5f0f\uff0c\u4ee5\u6a21\u62df\u73b0\u4ee3\u6570\u636e\u5206\u6790\u7684\u771f\u5b9e\u573a\u666f\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\uff0c\u5f53\u524dSOTA LLM\u4ee3\u7406\uff08\u5982Browser Use\uff09\u5728WebDS\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u6210\u529f\u7387\u4ec5\u4e3a15%\uff08\u76f8\u6bd4Web Voyager\u4e0a\u768480%\uff09\u3002\u5206\u6790\u8868\u660e\uff0c\u5931\u8d25\u6a21\u5f0f\u5305\u62ec\u4fe1\u606f\u63a5\u5730\u4e0d\u826f\u3001\u91cd\u590d\u884c\u4e3a\u548c\u8d70\u6377\u5f84\u7b49\u3002", "conclusion": "WebDS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u771f\u5b9e\u3001\u66f4\u9c81\u68d2\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u6709\u671b\u4fc3\u8fdbLLM\u5728\u5b9e\u9645\u7f51\u9875\u6570\u636e\u79d1\u5b66\u5e94\u7528\u65b9\u9762\u53d6\u5f97\u91cd\u5927\u8fdb\u5c55\u3002"}}
{"id": "2508.02373", "pdf": "https://arxiv.org/pdf/2508.02373", "abs": "https://arxiv.org/abs/2508.02373", "authors": ["Iulisloi Zacarias", "Oussama Ben Taarit", "Admela Jukan"], "title": "On Effectiveness of Graph Neural Network Architectures for Network Digital Twins (NDTs)", "categories": ["cs.NI", "cs.DC"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Future networks, such as 6G, will need to support a vast and diverse range of\ninterconnected devices and applications, each with its own set of requirements.\nWhile traditional network management approaches will suffice, an automated\nsolutions are becoming a must. However, network automation frameworks are prone\nto errors, and often they employ ML-based techniques that require training to\nlearn how the network can be optimized. In this sense, network digital twins\nare a useful tool that allows for the simulation, testing, and training of AI\nmodels without affecting the real-world networks and users. This paper presents\nan AI-based Network Digital Twin (AI-NDT) that leverages a multi-layered\nknowledge graph architecture and graph neural networks to predict network\nmetrics that directly affect the quality of experience of users. An evaluation\nof the four most prominent Graph Neural Networks (GNN) architectures was\nconducted to assess their effectiveness in developing network digital twins. We\ntrained the digital twin on publicly available measurement data from RIPE\nAtlas, therefore obtaining results close to what is expected in real-world\napplications. The results show that among the four architectures evaluated,\nGraphTransformer presents the best performance. However, other architectures\nmight fit better in scenarios where shorter training time is important, while\nalso delivering acceptable results. The results of this work are indicative of\nwhat might become common practice for proactive network management, offering a\nscalable and accurate solution aligned with the requirements of the\nnext-generation networks.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u7f51\u7edc\u6570\u5b57\u5b6a\u751f\uff08AI-NDT\uff09\uff0c\u5b83\u5229\u7528\u591a\u5c42\u77e5\u8bc6\u56fe\u8c31\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u6765\u9884\u6d4b\u7f51\u7edc\u6307\u6807\uff0c\u65e8\u5728\u652f\u6301\u672a\u6765\u7f51\u7edc\u7684\u4e3b\u52a8\u5f0f\u7ba1\u7406\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cdGNN\u67b6\u6784\uff0c\u53d1\u73b0GraphTransformer\u6027\u80fd\u6700\u4f73\u3002", "motivation": "\u672a\u6765\u7f51\u7edc\uff08\u59826G\uff09\u9700\u652f\u6301\u5927\u91cf\u591a\u6837\u5316\u8bbe\u5907\u548c\u5e94\u7528\uff0c\u8981\u6c42\u81ea\u52a8\u5316\u7ba1\u7406\u3002\u7136\u800c\uff0c\u7f51\u7edc\u81ea\u52a8\u5316\u6846\u67b6\u6613\u51fa\u9519\uff0c\u4e14\u57fa\u4e8eML\u7684\u65b9\u6848\u9700\u8bad\u7ec3\u3002\u7f51\u7edc\u6570\u5b57\u5b6a\u751f\u53ef\u63d0\u4f9b\u4e00\u4e2a\u5b89\u5168\u7684\u5e73\u53f0\u6765\u6a21\u62df\u3001\u6d4b\u8bd5\u548c\u8bad\u7ec3AI\u6a21\u578b\uff0c\u800c\u4e0d\u5f71\u54cd\u771f\u5b9e\u7f51\u7edc\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cdAI-based Network Digital Twin (AI-NDT)\uff0c\u5176\u6838\u5fc3\u662f\u5229\u7528\u591a\u5c42\u77e5\u8bc6\u56fe\u8c31\u67b6\u6784\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u6765\u9884\u6d4b\u76f4\u63a5\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u8d28\u91cf\uff08QoE\uff09\u7684\u7f51\u7edc\u6307\u6807\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u4e3b\u6d41GNN\u67b6\u6784\u7684\u6709\u6548\u6027\uff0c\u5e76\u4f7f\u7528RIPE Atlas\u7684\u516c\u5f00\u6d4b\u91cf\u6570\u636e\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728\u6240\u8bc4\u4f30\u7684\u56db\u79cdGNN\u67b6\u6784\u4e2d\uff0cGraphTransformer\u8868\u73b0\u51fa\u6700\u4f73\u6027\u80fd\u3002\u7136\u800c\uff0c\u5728\u66f4\u6ce8\u91cd\u8bad\u7ec3\u65f6\u95f4\u6548\u7387\u7684\u573a\u666f\u4e0b\uff0c\u5176\u4ed6\u67b6\u6784\u4e5f\u80fd\u63d0\u4f9b\u53ef\u63a5\u53d7\u7684\u7ed3\u679c\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u6210\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684AI-NDT\u65b9\u6cd5\u6709\u671b\u6210\u4e3a\u672a\u6765\u4e3b\u52a8\u5f0f\u7f51\u7edc\u7ba1\u7406\u7684\u5e38\u89c1\u5b9e\u8df5\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u7f51\u7edc\u63d0\u4f9b\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00880", "pdf": "https://arxiv.org/pdf/2508.00880", "abs": "https://arxiv.org/abs/2508.00880", "authors": ["Adil Mukhtar", "Michael Hadwiger", "Franz Wotawa", "Gerald Schweiger"], "title": "Reproducibility of Machine Learning-Based Fault Detection and Diagnosis for HVAC Systems in Buildings: An Empirical Study", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reproducibility is a cornerstone of scientific research, enabling independent\nverification and validation of empirical findings. The topic gained prominence\nin fields such as psychology and medicine, where concerns about non -\nreplicable results sparked ongoing discussions about research practices. In\nrecent years, the fast-growing field of Machine Learning (ML) has become part\nof this discourse, as it faces similar concerns about transparency and\nreliability. Some reproducibility issues in ML research are shared with other\nfields, such as limited access to data and missing methodological details. In\naddition, ML introduces specific challenges, including inherent nondeterminism\nand computational constraints. While reproducibility issues are increasingly\nrecognized by the ML community and its major conferences, less is known about\nhow these challenges manifest in applied disciplines. This paper contributes to\nclosing this gap by analyzing the transparency and reproducibility standards of\nML applications in building energy systems. The results indicate that nearly\nall articles are not reproducible due to insufficient disclosure across key\ndimensions of reproducibility. 72% of the articles do not specify whether the\ndataset used is public, proprietary, or commercially available. Only two papers\nshare a link to their code - one of which was broken. Two-thirds of the\npublications were authored exclusively by academic researchers, yet no\nsignificant differences in reproducibility were observed compared to\npublications with industry-affiliated authors. These findings highlight the\nneed for targeted interventions, including reproducibility guidelines, training\nfor researchers, and policies by journals and conferences that promote\ntransparency and reproducibility.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u673a\u5668\u5b66\u4e60\u5728\u5efa\u7b51\u80fd\u6e90\u7cfb\u7edf\u4e2d\u5e94\u7528\u7684\u53ef\u91cd\u590d\u6027\uff0c\u53d1\u73b0\u51e0\u4e4e\u6240\u6709\u6587\u7ae0\u90fd\u56e0\u4fe1\u606f\u62ab\u9732\u4e0d\u8db3\u800c\u4e0d\u53ef\u590d\u73b0\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u91c7\u53d6\u5e72\u9884\u63aa\u65bd\u3002", "motivation": "\u53ef\u91cd\u590d\u6027\u662f\u79d1\u5b66\u7814\u7a76\u7684\u57fa\u77f3\uff0c\u4f46\u5728\u5feb\u901f\u53d1\u5c55\u7684\u673a\u5668\u5b66\u4e60\u9886\u57df\u9762\u4e34\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002\u5c3d\u7ba1\u673a\u5668\u5b66\u4e60\u793e\u533a\u5df2\u8ba4\u8bc6\u5230\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u5bf9\u4e8e\u5b83\u4eec\u5728\u5e94\u7528\u5b66\u79d1\uff08\u5982\u5efa\u7b51\u80fd\u6e90\u7cfb\u7edf\uff09\u4e2d\u5982\u4f55\u4f53\u73b0\u77e5\u4e4b\u751a\u5c11\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u673a\u5668\u5b66\u4e60\u5728\u5efa\u7b51\u80fd\u6e90\u7cfb\u7edf\u5e94\u7528\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u590d\u6027\u6807\u51c6\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u51e0\u4e4e\u6240\u6709\u6587\u7ae0\u90fd\u56e0\u5173\u952e\u53ef\u91cd\u590d\u6027\u7ef4\u5ea6\u4fe1\u606f\u62ab\u9732\u4e0d\u8db3\u800c\u4e0d\u53ef\u590d\u73b0\u300272%\u7684\u6587\u7ae0\u672a\u8bf4\u660e\u6570\u636e\u96c6\u7c7b\u578b\uff1b\u4ec5\u4e24\u7bc7\u8bba\u6587\u5206\u4eab\u4e86\u4ee3\u7801\u94fe\u63a5\uff08\u5176\u4e2d\u4e00\u4e2a\u5df2\u5931\u6548\uff09\u3002\u7531\u7eaf\u5b66\u672f\u7814\u7a76\u4eba\u5458\u64b0\u5199\u7684\u51fa\u7248\u7269\u4e0e\u6709\u884c\u4e1a\u4f5c\u8005\u53c2\u4e0e\u7684\u51fa\u7248\u7269\u4e4b\u95f4\uff0c\u53ef\u91cd\u590d\u6027\u6ca1\u6709\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u51f8\u663e\u4e86\u91c7\u53d6\u9488\u5bf9\u6027\u5e72\u9884\u63aa\u65bd\u7684\u5fc5\u8981\u6027\uff0c\u5305\u62ec\u5236\u5b9a\u53ef\u91cd\u590d\u6027\u6307\u5357\u3001\u5bf9\u7814\u7a76\u4eba\u5458\u8fdb\u884c\u57f9\u8bad\uff0c\u4ee5\u53ca\u671f\u520a\u548c\u4f1a\u8bae\u5236\u5b9a\u4fc3\u8fdb\u900f\u660e\u5ea6\u548c\u53ef\u91cd\u590d\u6027\u7684\u653f\u7b56\u3002"}}
{"id": "2508.00945", "pdf": "https://arxiv.org/pdf/2508.00945", "abs": "https://arxiv.org/abs/2508.00945", "authors": ["Yifan Wang", "Hongfeng Ai", "Quangao Liu", "Maowei Jiang", "Ruiyuan Kang", "Ruiqi Li", "Jiahua Dong", "Mengting Xiao", "Cheng Jiang", "Chenzhong Li"], "title": "Optimizing Vision-Language Consistency via Cross-Layer Regional Attention Alignment", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Vision Language Models (VLMs) face challenges in effectively coordinating\ndiverse attention mechanisms for cross-modal embedding learning, leading to\nmismatched attention and suboptimal performance. We propose Consistent\nCross-layer Regional Alignment (CCRA), which introduces Layer-Patch-wise Cross\nAttention (LPWCA) to capture fine-grained regional-semantic correlations by\njointly weighting patch and layer-wise embedding, and Progressive Attention\nIntegration (PAI) that systematically coordinates LPWCA, layer-wise, and\npatch-wise attention mechanisms in sequence. This progressive design ensures\nconsistency from semantic to regional levels while preventing attention drift\nand maximizing individual attention benefits. Experimental results on ten\ndiverse vision-language benchmarks demonstrate that our CCRA-enhanced\nLLaVA-v1.5-7B model achieves state-of-the-art performance, outperforming all\nbaseline methods with only 3.55M additional parameters, while providing\nenhanced interpretability through more regionally focused and semantically\naligned attention patterns.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCCRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u8de8\u5c42\u533a\u57df\u5bf9\u9f50\u6765\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e2d\u6ce8\u610f\u529b\u673a\u5236\u534f\u8c03\u4e0d\u5f53\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u8de8\u6a21\u6001\u5d4c\u5165\u5b66\u4e60\u4e2d\u96be\u4ee5\u6709\u6548\u534f\u8c03\u591a\u6837\u5316\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8fd9\u5bfc\u81f4\u6ce8\u610f\u529b\u4e0d\u5339\u914d\u5e76\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u201c\u4e00\u81f4\u6027\u8de8\u5c42\u533a\u57df\u5bf9\u9f50\u201d\uff08CCRA\uff09\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1. \u5c42-\u56fe\u50cf\u5757\u4ea4\u53c9\u6ce8\u610f\u529b\uff08LPWCA\uff09\uff0c\u7528\u4e8e\u901a\u8fc7\u8054\u5408\u52a0\u6743\u56fe\u50cf\u5757\u548c\u5c42\u7ea7\u5d4c\u5165\u6765\u6355\u83b7\u7ec6\u7c92\u5ea6\u533a\u57df-\u8bed\u4e49\u5173\u8054\uff1b2. \u6e10\u8fdb\u5f0f\u6ce8\u610f\u529b\u6574\u5408\uff08PAI\uff09\uff0c\u7cfb\u7edf\u5730\u534f\u8c03LPWCA\u3001\u5c42\u7ea7\u6ce8\u610f\u529b\u4ee5\u53ca\u56fe\u50cf\u5757\u7ea7\u6ce8\u610f\u529b\u3002", "result": "\u5728\u5341\u4e2a\u4e0d\u540c\u7684\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7ecfCCRA\u589e\u5f3a\u7684LLaVA-v1.5-7B\u6a21\u578b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u4ec5\u589e\u52a0\u4e863.55M\u53c2\u6570\u3002\u540c\u65f6\uff0c\u5b83\u901a\u8fc7\u66f4\u805a\u7126\u533a\u57df\u548c\u8bed\u4e49\u5bf9\u9f50\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "CCRA\u6709\u6548\u89e3\u51b3\u4e86VLM\u4e2d\u6ce8\u610f\u529b\u534f\u8c03\u7684\u6311\u6218\uff0c\u5728\u63d0\u4f9b\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4e14\u4ec5\u5f15\u5165\u6781\u5c0f\u7684\u989d\u5916\u53c2\u6570\u3002"}}
{"id": "2508.01012", "pdf": "https://arxiv.org/pdf/2508.01012", "abs": "https://arxiv.org/abs/2508.01012", "authors": ["Yiyi Lu", "Hoi Ian Au", "Junyao Zhang", "Jingyu Pan", "Yiting Wang", "Ang Li", "Jianyi Zhang", "Yiran Chen"], "title": "AutoEDA: Enabling EDA Flow Automation through Microservice-Based LLM Agents", "categories": ["cs.AI"], "comment": null, "summary": "Modern Electronic Design Automation (EDA) workflows, especially the\nRTL-to-GDSII flow, require heavily manual scripting and demonstrate a multitude\nof tool-specific interactions which limits scalability and efficiency. While\nLLMs introduces strides for automation, existing LLM solutions require\nexpensive fine-tuning and do not contain standardized frameworks for\nintegration and evaluation. We introduce AutoEDA, a framework for EDA\nautomation that leverages paralleled learning through the Model Context\nProtocol (MCP) specific for standardized and scalable natural language\nexperience across the entire RTL-to-GDSII flow. AutoEDA limits fine-tuning\nthrough structured prompt engineering, implements intelligent parameter\nextraction and task decomposition, and provides an extended CodeBLEU metric to\nevaluate the quality of TCL scripts. Results from experiments over five\npreviously curated benchmarks show improvements in automation accuracy and\nefficiency, as well as script quality when compared to existing methods.\nAutoEDA is released open-sourced to support reproducibility and the EDA\ncommunity. Available at: https://github.com/AndyLu666/MCP-EDA-Server", "AI": {"tldr": "AutoEDA\u662f\u4e00\u4e2a\u9488\u5bf9RTL-to-GDSII\u6d41\u7a0b\u7684EDA\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5229\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u548c\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfEDA\u5de5\u4f5c\u6d41\u4e2d\u624b\u52a8\u64cd\u4f5c\u591a\u3001\u6548\u7387\u4f4e\u4ee5\u53ca\u73b0\u6709LLM\u89e3\u51b3\u65b9\u6848\u7f3a\u4e4f\u6807\u51c6\u5316\u548c\u5fae\u8c03\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5316\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u811a\u672c\u8d28\u91cf\u3002", "motivation": "\u73b0\u4ee3EDA\u5de5\u4f5c\u6d41\uff08\u5c24\u5176\u662fRTL-to-GDSII\uff09\u9ad8\u5ea6\u4f9d\u8d56\u624b\u52a8\u811a\u672c\u548c\u5de5\u5177\u7279\u5b9a\u4ea4\u4e92\uff0c\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u53d7\u9650\u3002\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e3a\u81ea\u52a8\u5316\u5e26\u6765\u4e86\u8fdb\u6b65\uff0c\u4f46\u73b0\u6709LLM\u89e3\u51b3\u65b9\u6848\u9700\u8981\u6602\u8d35\u7684\u5fae\u8c03\uff0c\u5e76\u4e14\u7f3a\u4e4f\u96c6\u6210\u7684\u6807\u51c6\u5316\u6846\u67b6\u548c\u8bc4\u4f30\u673a\u5236\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86AutoEDA\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u8fdb\u884c\u5e76\u884c\u5b66\u4e60\uff0c\u4ee5\u5b9e\u73b0\u8de8RTL-to-GDSII\u6d41\u7a0b\u7684\u6807\u51c6\u5316\u548c\u53ef\u6269\u5c55\u81ea\u7136\u8bed\u8a00\u4f53\u9a8c\u3002AutoEDA\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\u51cf\u5c11\u4e86\u5fae\u8c03\u9700\u6c42\uff0c\u5b9e\u73b0\u4e86\u667a\u80fd\u53c2\u6570\u63d0\u53d6\u548c\u4efb\u52a1\u5206\u89e3\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6269\u5c55\u7684CodeBLEU\u6307\u6807\u6765\u8bc4\u4f30TCL\u811a\u672c\u7684\u8d28\u91cf\u3002", "result": "\u5728\u4e94\u4e2a\u9884\u8bbe\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cAutoEDA\u5728\u81ea\u52a8\u5316\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u811a\u672c\u8d28\u91cf\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "conclusion": "AutoEDA\u6846\u67b6\u6210\u529f\u5730\u63d0\u9ad8\u4e86EDA\u81ea\u52a8\u5316\u6d41\u7a0b\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u53d1\u5e03\u652f\u6301\u53ef\u590d\u73b0\u6027\u548c\u6574\u4e2aEDA\u793e\u533a\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.01245", "pdf": "https://arxiv.org/pdf/2508.01245", "abs": "https://arxiv.org/abs/2508.01245", "authors": ["Yue Chen", "Minghua He", "Fangkai Yang", "Pu Zhao", "Lu Wang", "Yu Kang", "Yifei Dong", "Yuefeng Zhan", "Hao Sun", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in solving mathematical problems, yet\ntheir performance is often limited by the availability of high-quality, diverse\ntraining data. Existing methods focus on augmenting datasets through rephrasing\nor difficulty progression but overlook the specific failure modes of LLMs. This\nresults in synthetic questions that the model can already solve, providing\nminimal performance gains. To address this, we propose WarriorMath, a\ndefect-aware framework for mathematical problem solving that integrates both\ntargeted data synthesis and progressive training. In the synthesis stage, we\nemploy multiple expert LLMs in a collaborative process to generate, critique,\nand refine problems. Questions that base LLMs fail to solve are identified and\niteratively improved through expert-level feedback, producing high-quality,\ndefect-aware training data. In the training stage, we introduce a progressive\nlearning framework that iteratively fine-tunes the model using increasingly\nchallenging data tailored to its weaknesses. Experiments on six mathematical\nbenchmarks show that WarriorMath outperforms strong baselines by 12.57% on\naverage, setting a new state-of-the-art. Our results demonstrate the\neffectiveness of a defect-aware, multi-expert framework for improving\nmathematical ability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faWarriorMath\uff0c\u4e00\u4e2a\u7f3a\u9677\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4e13\u5bb6\u534f\u540c\u751f\u6210\u7f3a\u9677\u95ee\u9898\u5e76\u8fdb\u884c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aSOTA\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u7684\u53ef\u7528\u6027\u3002\u73b0\u6709\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u5ffd\u89c6\u4e86LLMs\u7684\u7279\u5b9a\u5931\u8d25\u6a21\u5f0f\uff0c\u5bfc\u81f4\u5408\u6210\u95ee\u9898\u5bf9\u6a21\u578b\u63d0\u5347\u6709\u9650\u3002", "method": "WarriorMath\u6846\u67b6\u6574\u5408\u4e86\u5b9a\u5411\u6570\u636e\u5408\u6210\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u3002\u5728\u5408\u6210\u9636\u6bb5\uff0c\u5229\u7528\u591a\u4e2a\u4e13\u5bb6LLM\u534f\u540c\u751f\u6210\u3001\u6279\u8bc4\u548c\u4f18\u5316\u95ee\u9898\uff0c\u4e13\u95e8\u9488\u5bf9\u57fa\u7840LLM\u672a\u80fd\u89e3\u51b3\u7684\u95ee\u9898\u8fdb\u884c\u8fed\u4ee3\u6539\u8fdb\u3002\u5728\u8bad\u7ec3\u9636\u6bb5\uff0c\u5f15\u5165\u6e10\u8fdb\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u9488\u5bf9\u6a21\u578b\u5f31\u70b9\u5b9a\u5236\u7684\u3001\u96be\u5ea6\u9012\u589e\u7684\u6570\u636e\u8fed\u4ee3\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWarriorMath\u5e73\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf12.57%\uff0c\u5e76\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u7f3a\u9677\u611f\u77e5\u3001\u591a\u4e13\u5bb6\u6846\u67b6\u5728\u63d0\u5347LLM\u6570\u5b66\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.02571", "pdf": "https://arxiv.org/pdf/2508.02571", "abs": "https://arxiv.org/abs/2508.02571", "authors": ["Yongzhe Xu", "Weitong Li", "Eeshan Umrani", "Taejoong Chung"], "title": "ASINT: Learning AS-to-Organization Mapping from Internet Metadata", "categories": ["cs.NI"], "comment": null, "summary": "Accurately mapping Autonomous Systems (ASNs) to their owning or operating\norganizations underpins Internet measurement research and security\napplications. Yet existing approaches commonly rely solely on WHOIS or\nPeeringDB, missing important relationships (e.g., cross-regional aliases,\nparent-child ownership) and failing to unify organizations scattered across\ndifferent RIR identifiers. We introduce ASINT, an end-to-end pipeline that\nfuses bulk registry data with unstructured Web sources, then employs\nretrieval-augmented generation (RAG) to guide large language model (LLM)\ninference. Through a multi-stage procedure, ASINT merges ASNs into\n\"organization families,\" capturing nuanced ties beyond the scope of simpler\nheuristics.\n  ASINT maps 111,470 ASNs to 81,233 organization families; compared to both\nAS2ORG+ and AS-Sibling, ASINT identifies more cross-regional groupings (e.g.,\noperator aliases, rebrands) that other datasets overlook. Moreover, our refined\nmappings enhance multiple security and measurement tasks: ASINT exposes 27.5%\nmore intra-organizational RPKI misconfigurations, cuts false-positive hijack\nalarms by 9.4%, and lowers erroneous IP leasing inferences by 5.9%.\n  Finally, ASINT supports periodic updates and cost-sensitive LLM selection,\ndemonstrating that broader Web evidence can provide a more accurate, evolving\nview of the Internet's organizational structure.", "AI": {"tldr": "\u63d0\u51faASINT\uff0c\u4e00\u79cd\u7ed3\u5408\u591a\u6e90\u6570\u636e\u548cLLM\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u51c6\u786e\u5730\u5c06ASNs\u6620\u5c04\u5230\u7ec4\u7ec7\u5bb6\u65cf\uff0c\u4ece\u800c\u6539\u8fdb\u4e92\u8054\u7f51\u6d4b\u91cf\u548c\u5b89\u5168\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u5c06\u81ea\u6cbb\u7cfb\u7edf\uff08ASNs\uff09\u6620\u5c04\u5230\u5176\u6240\u5c5e\u6216\u8fd0\u8425\u7ec4\u7ec7\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56WHOIS\u6216PeeringDB\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u672a\u80fd\u6355\u6349\u91cd\u8981\u7684\u7ec4\u7ec7\u5173\u7cfb\uff08\u5982\u8de8\u533a\u57df\u522b\u540d\u3001\u7236\u5b50\u6240\u6709\u6743\uff09\u5e76\u7edf\u4e00\u5206\u6563\u5728\u4e0d\u540cRIR\u6807\u8bc6\u7b26\u4e0b\u7684\u7ec4\u7ec7\uff0c\u8fd9\u9650\u5236\u4e86\u4e92\u8054\u7f51\u6d4b\u91cf\u7814\u7a76\u548c\u5b89\u5168\u5e94\u7528\u7684\u51c6\u786e\u6027\u3002", "method": "\u5f15\u5165ASINT\uff0c\u4e00\u4e2a\u7aef\u5230\u7aef\u7ba1\u9053\u3002\u5b83\u878d\u5408\u6279\u91cf\u6ce8\u518c\u6570\u636e\u4e0e\u975e\u7ed3\u6784\u5316Web\u6e90\uff0c\u7136\u540e\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6307\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u3002\u901a\u8fc7\u4e00\u4e2a\u591a\u9636\u6bb5\u7a0b\u5e8f\uff0cASINT\u5c06ASNs\u5408\u5e76\u4e3a\u201c\u7ec4\u7ec7\u5bb6\u65cf\u201d\uff0c\u6355\u6349\u6bd4\u7b80\u5355\u542f\u53d1\u5f0f\u65b9\u6cd5\u66f4\u7ec6\u81f4\u7684\u5173\u8054\u3002", "result": "ASINT\u5c06111,470\u4e2aASNs\u6620\u5c04\u523081,233\u4e2a\u7ec4\u7ec7\u5bb6\u65cf\uff1b\u4e0eAS2ORG+\u548cAS-Sibling\u76f8\u6bd4\uff0cASINT\u8bc6\u522b\u51fa\u66f4\u591a\u8de8\u533a\u57df\u5206\u7ec4\uff08\u5982\u8fd0\u8425\u5546\u522b\u540d\u3001\u54c1\u724c\u91cd\u5851\uff09\u3002\u6b64\u5916\uff0cASINT\u7684\u7cbe\u70bc\u6620\u5c04\u589e\u5f3a\u4e86\u591a\u9879\u5b89\u5168\u548c\u6d4b\u91cf\u4efb\u52a1\uff1a\u5b83\u63ed\u793a\u4e8627.5%\u7684\u7ec4\u7ec7\u5185RPKI\u914d\u7f6e\u9519\u8bef\uff0c\u5c06\u52ab\u6301\u8bef\u62a5\u51cf\u5c11\u4e869.4%\uff0c\u5e76\u964d\u4f4e\u4e865.9%\u7684\u9519\u8befIP\u79df\u7528\u63a8\u65ad\u3002", "conclusion": "ASINT\u652f\u6301\u5468\u671f\u6027\u66f4\u65b0\u548c\u6210\u672c\u654f\u611f\u7684LLM\u9009\u62e9\uff0c\u8868\u660e\u66f4\u5e7f\u6cdb\u7684Web\u8bc1\u636e\u53ef\u4ee5\u63d0\u4f9b\u4e00\u4e2a\u66f4\u51c6\u786e\u3001\u4e0d\u65ad\u6f14\u8fdb\u7684\u4e92\u8054\u7f51\u7ec4\u7ec7\u7ed3\u6784\u89c6\u56fe\u3002"}}
{"id": "2508.00881", "pdf": "https://arxiv.org/pdf/2508.00881", "abs": "https://arxiv.org/abs/2508.00881", "authors": ["Vijja Wichitwechkarn", "Charles Fox", "Ruchi Choudhary"], "title": "Hallucination Detection and Mitigation with Diffusion in Multi-Variate Time-Series Foundation Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Foundation models for natural language processing have many coherent\ndefinitions of hallucination and methods for its detection and mitigation.\nHowever, analogous definitions and methods do not exist for multi-variate\ntime-series (MVTS) foundation models. We propose new definitions for MVTS\nhallucination, along with new detection and mitigation methods using a\ndiffusion model to estimate hallucination levels. We derive relational datasets\nfrom popular time-series datasets to benchmark these relational hallucination\nlevels. Using these definitions and models, we find that open-source\npre-trained MVTS imputation foundation models relationally hallucinate on\naverage up to 59.5% as much as a weak baseline. The proposed mitigation method\nreduces this by up to 47.7% for these models. The definition and methods may\nimprove adoption and safe usage of MVTS foundation models.", "AI": {"tldr": "\u672c\u6587\u4e3a\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\uff08MVTS\uff09\u57fa\u7840\u6a21\u578b\u5b9a\u4e49\u4e86\u5e7b\u89c9\u6982\u5ff5\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u68c0\u6d4b\u4e0e\u7f13\u89e3\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u57fa\u7840\u6a21\u578b\u5df2\u6709\u6210\u719f\u7684\u5e7b\u89c9\u5b9a\u4e49\u4e0e\u68c0\u6d4b\u7f13\u89e3\u65b9\u6cd5\uff0c\u4f46MVTS\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u7c7b\u4f3c\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86MVTS\u5e7b\u89c9\u7684\u65b0\u5b9a\u4e49\u3001\u68c0\u6d4b\u4e0e\u7f13\u89e3\u65b9\u6cd5\u3002\u5229\u7528\u6269\u6563\u6a21\u578b\u4f30\u7b97\u5e7b\u89c9\u6c34\u5e73\uff0c\u5e76\u4ece\u6d41\u884c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e2d\u6d3e\u751f\u5173\u7cfb\u578b\u6570\u636e\u96c6\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u53d1\u73b0\u5f00\u6e90\u9884\u8bad\u7ec3MVTS\u63d2\u8865\u57fa\u7840\u6a21\u578b\u7684\u5173\u7cfb\u578b\u5e7b\u89c9\u5e73\u5747\u9ad8\u8fbe\u5f31\u57fa\u7ebf\u768459.5%\u3002\u6240\u63d0\u7f13\u89e3\u65b9\u6cd5\u53ef\u5c06\u5e7b\u89c9\u964d\u4f4e\u591a\u8fbe47.7%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5b9a\u4e49\u548c\u65b9\u6cd5\u6709\u671b\u63d0\u5347MVTS\u57fa\u7840\u6a21\u578b\u7684\u91c7\u7eb3\u7387\u548c\u5b89\u5168\u5e94\u7528\u3002"}}
{"id": "2508.00974", "pdf": "https://arxiv.org/pdf/2508.00974", "abs": "https://arxiv.org/abs/2508.00974", "authors": ["Daniel Andr\u00e9s L\u00f3pez", "Vincent Weber", "Severin Zentgraf", "Barlo Hillen", "Perikles Simon", "Elmar Sch\u00f6mer"], "title": "ThermoCycleNet: Stereo-based Thermogram Labeling for Model Transition to Cycling", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Presented at IWANN 2025 18th International Work-Conference on\n  Artificial Neural Networks, A Coru\\~na, Spain, 16-18 June, 2025. Book of\n  abstracts: ISBN: 979-13-8752213-1. Funding: Johannes Gutenberg University\n  \"Stufe I'': \"Start ThermoCycleNet''. Partial funding: Carl-Zeiss-Stiftung:\n  \"Multi-dimensionAI'' (CZS-Project number: P2022-08-010)", "summary": "Infrared thermography is emerging as a powerful tool in sports medicine,\nallowing assessment of thermal radiation during exercise and analysis of\nanatomical regions of interest, such as the well-exposed calves. Building on\nour previous advanced automatic annotation method, we aimed to transfer the\nstereo- and multimodal-based labeling approach from treadmill running to\nergometer cycling. Therefore, the training of the semantic segmentation network\nwith automatic labels and fine-tuning on high-quality manually annotated images\nhas been examined and compared in different data set combinations. The results\nindicate that fine-tuning with a small fraction of manual data is sufficient to\nimprove the overall performance of the deep neural network. Finally, combining\nautomatically generated labels with small manually annotated data sets\naccelerates the adaptation of deep neural networks to new use cases, such as\nthe transition from treadmill to bicycle.", "AI": {"tldr": "\u672c\u6587\u65e8\u5728\u5c06\u7528\u4e8e\u8dd1\u6b65\u673a\u7684\u7ea2\u5916\u70ed\u6210\u50cf\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\u8fc1\u79fb\u5230\u9a91\u884c\u8fd0\u52a8\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u81ea\u52a8\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u540e\uff0c\u518d\u7528\u5c11\u91cf\u624b\u52a8\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u80fd\u6709\u6548\u52a0\u901f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9002\u5e94\u65b0\u7684\u8fd0\u52a8\u573a\u666f\u3002", "motivation": "\u5c06\u5148\u524d\u5728\u8dd1\u6b65\u673a\u8fd0\u52a8\u4e2d\u4f7f\u7528\u7684\u7acb\u4f53\u591a\u6a21\u6001\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\uff0c\u8fc1\u79fb\u5e76\u5e94\u7528\u4e8e\u6d4b\u529b\u8ba1\u81ea\u884c\u8f66\u8fd0\u52a8\u573a\u666f\u7684\u7ea2\u5916\u70ed\u6210\u50cf\u5206\u6790\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u6807\u7b7e\u8bad\u7ec3\u8bed\u4e49\u5206\u5272\u7f51\u7edc\uff0c\u5e76\u63a2\u7a76\u4e86\u5728\u9ad8 L \u8d28\u91cf\u624b\u52a8\u6807\u6ce8\u56fe\u50cf\u4e0a\u8fdb\u884c\u5fae\u8c03\u7684\u6548\u679c\uff0c\u5728\u4e0d\u540c\u7684\u6570\u636e\u96c6\u7ec4\u5408\u4e2d\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u5c11\u91cf\u624b\u52a8\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u8db3\u4ee5\u63d0\u9ad8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408\u81ea\u52a8\u751f\u6210\u6807\u7b7e\u548c\u5c11\u91cf\u624b\u52a8\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u52a0\u901f\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9002\u5e94\u65b0\u7684\u4f7f\u7528\u573a\u666f\uff0c\u4f8b\u5982\u4ece\u8dd1\u6b65\u673a\u5230\u81ea\u884c\u8f66\u7684\u8fc7\u6e21\u3002"}}
{"id": "2508.01031", "pdf": "https://arxiv.org/pdf/2508.01031", "abs": "https://arxiv.org/abs/2508.01031", "authors": ["Jingzhe Ni", "Xiaolong Yin", "Xintong Li", "Xingyu Lu", "Ji Wei", "Ruofeng Tong", "Min Tang", "Peng Du"], "title": "CADDesigner: Conceptual Design of CAD Models Based on General-Purpose Agent", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing\nbut typically requires a high level of expertise from designers. To lower the\nentry barrier and improve design efficiency, we present an agent for CAD\nconceptual design powered by large language models (LLMs). The agent accepts\nboth abstract textual descriptions and freehand sketches as input, engaging in\ninteractive dialogue with users to refine and clarify design requirements\nthrough comprehensive requirement analysis. Built upon a novel\nContext-Independent Imperative Paradigm (CIP), the agent generates high-quality\nCAD modeling code. During the generation process, the agent incorporates\niterative visual feedback to improve model quality. Generated design cases are\nstored in a structured knowledge base, enabling continuous improvement of the\nagent's code generation capabilities. Experimental results demonstrate that our\nmethod achieves state-of-the-art performance in CAD code generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684CAD\u6982\u5ff5\u8bbe\u8ba1\u667a\u80fd\u4f53\uff0c\u65e8\u5728\u964d\u4f4eCAD\u8bbe\u8ba1\u95e8\u69db\u5e76\u63d0\u9ad8\u6548\u7387\u3002\u8be5\u667a\u80fd\u4f53\u63a5\u53d7\u6587\u672c\u548c\u8349\u56fe\u8f93\u5165\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u548c\u89c6\u89c9\u53cd\u9988\u751f\u6210\u9ad8\u8d28\u91cfCAD\u4ee3\u7801\uff0c\u5e76\u5229\u7528\u77e5\u8bc6\u5e93\u6301\u7eed\u5b66\u4e60\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728CAD\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u8ba1\u7b97\u673a\u8f85\u52a9\u8bbe\u8ba1\uff08CAD\uff09\u5728\u5de5\u4e1a\u5236\u9020\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u901a\u5e38\u9700\u8981\u8bbe\u8ba1\u4eba\u5458\u5177\u5907\u9ad8\u6c34\u5e73\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5bfc\u81f4\u5165\u95e8\u95e8\u69db\u9ad8\u4e14\u8bbe\u8ba1\u6548\u7387\u53d7\u9650\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u65e8\u5728\u964d\u4f4eCAD\u8bbe\u8ba1\u95e8\u69db\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684CAD\u6982\u5ff5\u8bbe\u8ba1\u667a\u80fd\u4f53\u3002\u8be5\u667a\u80fd\u4f53\u63a5\u53d7\u62bd\u8c61\u6587\u672c\u63cf\u8ff0\u548c\u624b\u7ed8\u8349\u56fe\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5bf9\u8bdd\u8fdb\u884c\u9700\u6c42\u5206\u6790\u3002\u5b83\u57fa\u4e8e\u4e00\u79cd\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u65e0\u5173\u6307\u4ee4\u8303\u5f0f\uff08CIP\uff09\u751f\u6210\u9ad8\u8d28\u91cfCAD\u5efa\u6a21\u4ee3\u7801\uff0c\u5e76\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u6574\u5408\u8fed\u4ee3\u89c6\u89c9\u53cd\u9988\u4ee5\u63d0\u9ad8\u6a21\u578b\u8d28\u91cf\u3002\u751f\u6210\u7684\u6848\u4f8b\u88ab\u5b58\u50a8\u5728\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u4e2d\u4ee5\u6301\u7eed\u6539\u8fdb\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728CAD\u4ee3\u7801\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u5f00\u53d1\u7684LLM\u9a71\u52a8CAD\u6982\u5ff5\u8bbe\u8ba1\u667a\u80fd\u4f53\u6210\u529f\u964d\u4f4e\u4e86CAD\u8bbe\u8ba1\u95e8\u69db\u5e76\u63d0\u5347\u4e86\u6548\u7387\uff0c\u5e76\u5728CAD\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\uff0c\u4e3a\u667a\u80fd\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2508.01263", "pdf": "https://arxiv.org/pdf/2508.01263", "abs": "https://arxiv.org/abs/2508.01263", "authors": ["Long S. T. Nguyen", "Khang H. N. Vo", "Thu H. A. Nguyen", "Tuan C. Bui", "Duc Q. Nguyen", "Thanh-Tung Tran", "Anh D. Nguyen", "Minh L. Nguyen", "Fabien Baldacci", "Thang H. Bui", "Emanuel Di Nardo", "Angelo Ciaramella", "Son H. Le", "Ihsan Ullah", "Lorenzo Di Rocco", "Tho T. Quan"], "title": "Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025", "categories": ["cs.CL"], "comment": "The XAI Challenge @ TRNS-AI Workshop, IJCNN 2025: Explainable AI for\n  Educational Question Answering. Website:\n  https://sites.google.com/view/trns-ai/challenge/", "summary": "The growing integration of Artificial Intelligence (AI) into education has\nintensified the need for transparency and interpretability. While hackathons\nhave long served as agile environments for rapid AI prototyping, few have\ndirectly addressed eXplainable AI (XAI) in real-world educational contexts.\nThis paper presents a comprehensive analysis of the XAI Challenge 2025, a\nhackathon-style competition jointly organized by Ho Chi Minh City University of\nTechnology (HCMUT) and the International Workshop on Trustworthiness and\nReliability in Neurosymbolic AI (TRNS-AI), held as part of the International\nJoint Conference on Neural Networks (IJCNN 2025). The challenge tasked\nparticipants with building Question-Answering (QA) systems capable of answering\nstudent queries about university policies while generating clear, logic-based\nnatural language explanations. To promote transparency and trustworthiness,\nsolutions were required to use lightweight Large Language Models (LLMs) or\nhybrid LLM-symbolic systems. A high-quality dataset was provided, constructed\nvia logic-based templates with Z3 validation and refined through expert student\nreview to ensure alignment with real-world academic scenarios. We describe the\nchallenge's motivation, structure, dataset construction, and evaluation\nprotocol. Situating the competition within the broader evolution of AI\nhackathons, we argue that it represents a novel effort to bridge LLMs and\nsymbolic reasoning in service of explainability. Our findings offer actionable\ninsights for future XAI-centered educational systems and competitive research\ninitiatives.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e862025\u5e74XAI\u6311\u6218\u8d5b\uff0c\u8be5\u9ed1\u5ba2\u9a6c\u62c9\u677e\u65e8\u5728\u6559\u80b2\u9886\u57df\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6216\u6df7\u5408\u7cfb\u7edf\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u95ee\u7b54\u7cfb\u7edf\uff0c\u4ee5\u63d0\u5347AI\u7684\u900f\u660e\u5ea6\u4e0e\u53ef\u4fe1\u8d56\u6027\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u6559\u80b2\u4e2d\u7684\u6df1\u5ea6\u6574\u5408\uff0c\u5bf9\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff08XAI\uff09\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\u3002\u73b0\u6709\u7684\u9ed1\u5ba2\u9a6c\u62c9\u677e\u5f88\u5c11\u76f4\u63a5\u5173\u6ce8\u771f\u5b9e\u6559\u80b2\u573a\u666f\u4e2d\u7684XAI\u95ee\u9898\u3002", "method": "\u672c\u6587\u5206\u6790\u4e86XAI Challenge 2025\uff0c\u8be5\u6311\u6218\u8d5b\u8981\u6c42\u53c2\u4e0e\u8005\u6784\u5efa\u80fd\u56de\u7b54\u5927\u5b66\u653f\u7b56\u95ee\u9898\u5e76\u63d0\u4f9b\u6e05\u6670\u3001\u57fa\u4e8e\u903b\u8f91\u89e3\u91ca\u7684\u95ee\u7b54\u7cfb\u7edf\uff0c\u5e76\u5f3a\u5236\u4f7f\u7528\u8f7b\u91cf\u7ea7LLM\u6216LLM-\u7b26\u53f7\u6df7\u5408\u7cfb\u7edf\u3002\u8d5b\u4e8b\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u3001\u7ecf\u903b\u8f91\u6a21\u677f\u6784\u5efa\u548c\u4e13\u5bb6\u9a8c\u8bc1\u7684\u6570\u636e\u96c6\u3002\u8bba\u6587\u8be6\u7ec6\u63cf\u8ff0\u4e86\u6311\u6218\u8d5b\u7684\u52a8\u673a\u3001\u7ed3\u6784\u3001\u6570\u636e\u96c6\u6784\u5efa\u548c\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u7814\u7a76\u8ba4\u4e3a\uff0c\u8be5\u6311\u6218\u8d5b\u662f\u8fde\u63a5LLM\u4e0e\u7b26\u53f7\u63a8\u7406\u4ee5\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\u7684\u4e00\u9879\u521b\u65b0\u5c1d\u8bd5\u3002\u672c\u6587\u7684\u53d1\u73b0\u4e3a\u672a\u6765\u4ee5XAI\u4e3a\u4e2d\u5fc3\u7684\u6559\u80b2\u7cfb\u7edf\u548c\u7ade\u4e89\u6027\u7814\u7a76\u8ba1\u5212\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89c1\u89e3\u3002", "conclusion": "XAI Challenge 2025\u4ee3\u8868\u4e86\u5728\u6559\u80b2\u9886\u57df\u4e2d\u6865\u63a5LLM\u548c\u7b26\u53f7\u63a8\u7406\u4ee5\u589e\u5f3aAI\u53ef\u89e3\u91ca\u6027\u7684\u4e00\u9879\u65b0\u9896\u52aa\u529b\uff0c\u5176\u6210\u679c\u4e3a\u672a\u6765\u7684\u53ef\u89e3\u91caAI\u6559\u80b2\u5e94\u7528\u548c\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2508.00851", "pdf": "https://arxiv.org/pdf/2508.00851", "abs": "https://arxiv.org/abs/2508.00851", "authors": ["Abdurrahman Tolay"], "title": "eBPF-Based Real-Time DDoS Mitigation for IoT Edge Devices", "categories": ["cs.CR", "cs.NI", "C.2.0; C.2.1; D.4.6"], "comment": "10 pages, 5 figures, includes evaluation on Docker and Raspberry Pi\n  testbeds. Keywords: IoT Security, DDoS Mitigation, eBPF, XDP, Raspberry Pi.\n  Submitted to IEEE Internet of Things Journal", "summary": "The rapid expansion of the Internet of Things (IoT) has intensified security\nchallenges, notably from Distributed Denial of Service (DDoS) attacks launched\nby compromised, resource-constrained devices. Traditional defenses are often\nill-suited for the IoT paradigm, creating a need for lightweight,\nhigh-performance, edge-based solutions. This paper presents the design,\nimplementation, and evaluation of an IoT security framework that leverages the\nextended Berkeley Packet Filter (eBPF) and the eXpress Data Path (XDP) for\nin-kernel mitigation of DDoS attacks. The system uses a rate-based detection\nalgorithm to identify and block malicious traffic at the earliest stage of the\nnetwork stack. The framework is evaluated using both Docker-based simulations\nand real-world deployment on a Raspberry Pi 4, showing over 97% mitigation\neffectiveness under a 100 Mbps flood. Legitimate traffic remains unaffected,\nand system stability is preserved even under attack. These results confirm that\neBPF/XDP provides a viable and highly efficient solution for hardening IoT edge\ndevices against volumetric network attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eeBPF/XDP\u7684IoT\u8fb9\u7f18\u5b89\u5168\u6846\u67b6\uff0c\u6709\u6548\u62b5\u5fa1DDoS\u653b\u51fb\uff0c\u5b9e\u73b097%\u4ee5\u4e0a\u7f13\u89e3\u7387\u4e14\u4e0d\u5f71\u54cd\u6b63\u5e38\u6d41\u91cf\u3002", "motivation": "\u7269\u8054\u7f51\uff08IoT\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u5b89\u5168\u6311\u6218\uff0c\u7279\u522b\u662f\u6765\u81ea\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u7684\u5206\u5e03\u5f0f\u62d2\u7edd\u670d\u52a1\uff08DDoS\uff09\u653b\u51fb\u3002\u4f20\u7edf\u9632\u5fa1\u65b9\u6848\u4e0d\u9002\u7528\u4e8eIoT\u8303\u5f0f\uff0c\u56e0\u6b64\u9700\u8981\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6027\u80fd\u3001\u8fb9\u7f18\u4fa7\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u3001\u5b9e\u73b0\u5e76\u8bc4\u4f30\u4e86\u4e00\u4e2a\u5229\u7528eBPF\u548cXDP\u8fdb\u884c\u5185\u6838DDoS\u653b\u51fb\u7f13\u89e3\u7684IoT\u5b89\u5168\u6846\u67b6\u3002\u7cfb\u7edf\u91c7\u7528\u57fa\u4e8e\u901f\u7387\u7684\u68c0\u6d4b\u7b97\u6cd5\uff0c\u5728\u7f51\u7edc\u5806\u6808\u7684\u6700\u65e9\u9636\u6bb5\u8bc6\u522b\u5e76\u963b\u6b62\u6076\u610f\u6d41\u91cf\u3002", "result": "\u8be5\u6846\u67b6\u901a\u8fc7Docker\u6a21\u62df\u548c\u6811\u8393\u6d3e4\u4e0a\u7684\u771f\u5b9e\u90e8\u7f72\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728100 Mbps\u6d2a\u6cdb\u653b\u51fb\u4e0b\u663e\u793a\u51fa\u8d85\u8fc797%\u7684\u7f13\u89e3\u6709\u6548\u6027\u3002\u5408\u6cd5\u6d41\u91cf\u672a\u53d7\u5f71\u54cd\uff0c\u4e14\u7cfb\u7edf\u5728\u653b\u51fb\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u5b9e\uff0ceBPF/XDP\u4e3a\u589e\u5f3aIoT\u8fb9\u7f18\u8bbe\u5907\u62b5\u5fa1\u6279\u91cf\u7f51\u7edc\u653b\u51fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00884", "pdf": "https://arxiv.org/pdf/2508.00884", "abs": "https://arxiv.org/abs/2508.00884", "authors": ["Zhenan Lin", "Yuni Lai", "Wai Lun Lo", "Richard Tai-Chiu Hsung", "Harris Sik-Ho Tsang", "Xiaoyu Xue", "Kai Zhou", "Yulin Zhu"], "title": "Multi-Grained Temporal-Spatial Graph Learning for Stable Traffic Flow Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time-evolving traffic flow forecasting are playing a vital role in\nintelligent transportation systems and smart cities. However, the dynamic\ntraffic flow forecasting is a highly nonlinear problem with complex\ntemporal-spatial dependencies. Although the existing methods has provided great\ncontributions to mine the temporal-spatial patterns in the complex traffic\nnetworks, they fail to encode the globally temporal-spatial patterns and are\nprone to overfit on the pre-defined geographical correlations, and thus hinder\nthe model's robustness on the complex traffic environment. To tackle this\nissue, in this work, we proposed a multi-grained temporal-spatial graph\nlearning framework to adaptively augment the globally temporal-spatial patterns\nobtained from a crafted graph transformer encoder with the local patterns from\nthe graph convolution by a crafted gated fusion unit with residual connection\ntechniques. Under these circumstances, our proposed model can mine the hidden\nglobal temporal-spatial relations between each monitor stations and balance the\nrelative importance of local and global temporal-spatial patterns. Experiment\nresults demonstrate the strong representation capability of our proposed method\nand our model consistently outperforms other strong baselines on various\nreal-world traffic networks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u7c92\u5ea6\u65f6\u7a7a\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u5168\u5c40\u548c\u5c40\u90e8\u6a21\u5f0f\uff0c\u63d0\u9ad8\u4ea4\u901a\u6d41\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u52a8\u6001\u4ea4\u901a\u6d41\u9884\u6d4b\u662f\u975e\u7ebf\u6027\u95ee\u9898\uff0c\u5177\u6709\u590d\u6742\u65f6\u7a7a\u4f9d\u8d56\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u7f16\u7801\u5168\u5c40\u65f6\u7a7a\u6a21\u5f0f\uff0c\u4e14\u6613\u5728\u9884\u5b9a\u4e49\u5730\u7406\u76f8\u5173\u6027\u4e0a\u8fc7\u62df\u5408\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e0b\u9c81\u68d2\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u591a\u7c92\u5ea6\u65f6\u7a7a\u56fe\u5b66\u4e60\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5b9a\u5236\u7684\u56feTransformer\u7f16\u7801\u5668\u83b7\u53d6\u5168\u5c40\u65f6\u7a7a\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u56fe\u5377\u79ef\u83b7\u53d6\u5c40\u90e8\u6a21\u5f0f\uff0c\u7136\u540e\u5229\u7528\u5e26\u6b8b\u5dee\u8fde\u63a5\u7684\u95e8\u63a7\u878d\u5408\u5355\u5143\u81ea\u9002\u5e94\u5730\u878d\u5408\u8fd9\u4e24\u79cd\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5177\u6709\u5f3a\u5927\u7684\u8868\u793a\u80fd\u529b\uff0c\u5e76\u5728\u591a\u79cd\u771f\u5b9e\u4e16\u754c\u4ea4\u901a\u7f51\u7edc\u4e0a\u6301\u7eed\u4f18\u4e8e\u5176\u4ed6\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u6709\u6548\u6316\u6398\u76d1\u6d4b\u7ad9\u4e4b\u95f4\u9690\u85cf\u7684\u5168\u5c40\u65f6\u7a7a\u5173\u7cfb\uff0c\u5e76\u5e73\u8861\u5c40\u90e8\u4e0e\u5168\u5c40\u65f6\u7a7a\u6a21\u5f0f\u7684\u91cd\u8981\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u4ea4\u901a\u6d41\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2508.01008", "pdf": "https://arxiv.org/pdf/2508.01008", "abs": "https://arxiv.org/abs/2508.01008", "authors": ["Cihang Peng", "Qiming Hou", "Zhong Ren", "Kun Zhou"], "title": "ROVI: A VLM-LLM Re-Captioned Dataset for Open-Vocabulary Instance-Grounded Text-to-Image Generation", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025", "summary": "We present ROVI, a high-quality synthetic dataset for instance-grounded\ntext-to-image generation, created by labeling 1M curated web images. Our key\ninnovation is a strategy called re-captioning, focusing on the pre-detection\nstage, where a VLM (Vision-Language Model) generates comprehensive visual\ndescriptions that are then processed by an LLM (Large Language Model) to\nextract a flat list of potential categories for OVDs (Open-Vocabulary\nDetectors) to detect. This approach yields a global prompt inherently linked to\ninstance annotations while capturing secondary visual elements humans typically\noverlook. Evaluations show that ROVI exceeds existing detection datasets in\nimage quality and resolution while containing two orders of magnitude more\ncategories with an open-vocabulary nature. For demonstrative purposes, a\ntext-to-image model GLIGEN trained on ROVI significantly outperforms\nstate-of-the-art alternatives in instance grounding accuracy, prompt fidelity,\nand aesthetic quality. Our dataset and reproducible pipeline are available at\nhttps://github.com/CihangPeng/ROVI.", "AI": {"tldr": "ROVI\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u9ad8\u5206\u8fa8\u7387\u3001\u5f00\u653e\u8bcd\u6c47\u7684\u5b9e\u4f8b-\u57fa\u7840\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u521b\u65b0\u201c\u91cd\u65b0\u6807\u6ce8\u201d\u7b56\u7565\u521b\u5efa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b9e\u4f8b\u63a5\u5730\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6570\u636e\u96c6\u5728\u56fe\u50cf\u8d28\u91cf\u3001\u5206\u8fa8\u7387\u548c\u5f00\u653e\u8bcd\u6c47\u7c7b\u522b\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u96be\u4ee5\u6709\u6548\u652f\u6301\u5b9e\u4f8b-\u57fa\u7840\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff0c\u5c24\u5176\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u6355\u6349\u4e3b\u8981\u5b9e\u4f8b\u548c\u6b21\u8981\u89c6\u89c9\u5143\u7d20\u7684\u9ad8\u8d28\u91cf\u6807\u6ce8\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u201c\u91cd\u65b0\u6807\u6ce8\u201d\uff08re-captioning\uff09\u7b56\u7565\uff1a\u5728\u9884\u68c0\u6d4b\u9636\u6bb5\uff0cVLM\u751f\u6210\u5168\u9762\u7684\u89c6\u89c9\u63cf\u8ff0\uff0c\u968f\u540eLLM\u5904\u7406\u8fd9\u4e9b\u63cf\u8ff0\u4ee5\u63d0\u53d6\u4f9b\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\uff08OVDs\uff09\u68c0\u6d4b\u7684\u6f5c\u5728\u7c7b\u522b\u5e73\u5766\u5217\u8868\u3002\u6b64\u65b9\u6cd5\u5c06\u5168\u5c40\u63d0\u793a\u4e0e\u5b9e\u4f8b\u6807\u6ce8\u5185\u5728\u5173\u8054\uff0c\u540c\u65f6\u6355\u83b7\u4eba\u7c7b\u5e38\u5ffd\u7565\u7684\u6b21\u8981\u89c6\u89c9\u5143\u7d20\u3002", "result": "ROVI\u6570\u636e\u96c6\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u5206\u8fa8\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u4e14\u5f00\u653e\u8bcd\u6c47\u7c7b\u522b\u6570\u91cf\u591a\u51fa\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002\u57fa\u4e8eROVI\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578bGLIGEN\u5728\u5b9e\u4f8b\u63a5\u5730\u7cbe\u5ea6\u3001\u63d0\u793a\u5fe0\u5b9e\u5ea6\u548c\u7f8e\u5b66\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6848\u3002", "conclusion": "ROVI\u6570\u636e\u96c6\u901a\u8fc7\u5176\u521b\u65b0\u7684\u6807\u6ce8\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5b9e\u4f8b-\u57fa\u7840\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u9ad8\u5206\u8fa8\u7387\u4e14\u7c7b\u522b\u4e30\u5bcc\u7684\u8d44\u6e90\uff0c\u5e76\u80fd\u663e\u8457\u63d0\u5347\u76f8\u5173\u6a21\u578b\u7684\u6027\u80fd\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.01057", "pdf": "https://arxiv.org/pdf/2508.01057", "abs": "https://arxiv.org/abs/2508.01057", "authors": ["Fengze Yang", "Bo Yu", "Yang Zhou", "Xuewen Luo", "Zhengzhong Tu", "Chenxi Liu"], "title": "REACT: A Real-Time Edge-AI Based V2X Framework for Accident Avoidance in Autonomous Driving System", "categories": ["cs.AI", "cs.RO"], "comment": "24 pages, 6 tables, 7 figures", "summary": "Collisions caused by human error are the most common type of multi-vehicle\ncrash, highlighting the critical need for autonomous driving (AD) systems to\nleverage cooperative perception through Vehicle-to-Everything (V2X)\ncommunication. This capability extends situational awareness beyond the\nlimitations of onboard sensors. However, current transformer-based V2X\nframeworks suffer from limited generalization, shallow contextual reasoning,\nand reliance on mono-modal inputs. Vision-Language Models (VLMs) offer enhanced\nreasoning and multimodal integration but typically fall short of real-time\nperformance requirements in safety-critical applications. This paper presents\nREACT, a real-time, V2X-integrated trajectory optimization framework built upon\na fine-tuned lightweight VLM. REACT integrates a set of specialized modules\nthat process multimodal inputs into optimized, risk-aware trajectories. To\nensure real-time performance on edge devices, REACT incorporates edge\nadaptation strategies that reduce model complexity and accelerate inference.\nEvaluated on the DeepAccident benchmark, REACT achieves state-of-the-art\nperformance, a 77% collision rate reduction, a 48.2% Video Panoptic Quality\n(VPQ), and a 0.57-second inference latency on the Jetson AGX Orin. Ablation\nstudies validate the contribution of each input, module, and edge adaptation\nstrategy. These results demonstrate the feasibility of lightweight VLMs for\nreal-time edge-based cooperative planning and showcase the potential of\nlanguage-guided contextual reasoning to improve safety and responsiveness in\nautonomous driving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faREACT\u6846\u67b6\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e0eV2X\u901a\u4fe1\uff0c\u5b9e\u73b0\u5b9e\u65f6\u8f68\u8ff9\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4e\u81ea\u52a8\u9a7e\u9a76\u4e8b\u6545\u7387\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9700\u901a\u8fc7V2X\u63d0\u5347\u611f\u77e5\u80fd\u529b\u4ee5\u51cf\u5c11\u4eba\u4e3a\u4e8b\u6545\u3002\u73b0\u6709\u57fa\u4e8eTransformer\u7684V2X\u6846\u67b6\u6cdb\u5316\u6027\u5dee\u4e14\u63a8\u7406\u6d45\u663e\uff0c\u800c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u867d\u5177\u4f18\u52bf\u4f46\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u6027\u9700\u6c42\u3002", "method": "\u63d0\u51faREACT\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u5fae\u8c03\u8f7b\u91cf\u7ea7VLM\u7684\u5b9e\u65f6V2X\u96c6\u6210\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\u3002\u5b83\u901a\u8fc7\u4e13\u7528\u6a21\u5757\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\u4ee5\u4f18\u5316\u98ce\u9669\u611f\u77e5\u8f68\u8ff9\uff0c\u5e76\u91c7\u7528\u8fb9\u7f18\u9002\u5e94\u7b56\u7565\u786e\u4fdd\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "result": "\u5728DeepAccident\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cREACT\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u78b0\u649e\u7387\u964d\u4f4e77%\uff0c\u89c6\u9891\u5168\u666f\u8d28\u91cf\uff08VPQ\uff09\u8fbe\u523048.2%\uff0c\u5728Jetson AGX Orin\u4e0a\u7684\u63a8\u7406\u5ef6\u8fdf\u4e3a0.57\u79d2\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u6a21\u5757\u548c\u7b56\u7565\u7684\u8d21\u732e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7VLM\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u534f\u540c\u89c4\u5212\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u8bed\u8a00\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u5728\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u6027\u548c\u54cd\u5e94\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.01290", "pdf": "https://arxiv.org/pdf/2508.01290", "abs": "https://arxiv.org/abs/2508.01290", "authors": ["Zhichao Yan", "Jiapu Wang", "Jiaoyan Chen", "Yanyan Wang", "Hongye Tan", "Jiye Liang", "Xiaoli Li", "Ru Li", "Jeff Z. Pan"], "title": "Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) shows impressive performance by\nsupplementing and substituting parametric knowledge in Large Language Models\n(LLMs). Retrieved knowledge can be divided into three types: explicit answer\nevidence, implicit answer clue, and insufficient answer context which can be\nfurther categorized into totally irrelevant and partially relevant information.\nEffectively utilizing partially relevant knowledge remains a key challenge for\nRAG systems, especially in incomplete knowledge base retrieval. Contrary to the\nconventional view, we propose a new perspective: LLMs can be awakened via\npartially relevant knowledge already embedded in LLMs. To comprehensively\ninvestigate this phenomenon, the triplets located in the gold reasoning path\nand their variants are used to construct partially relevant knowledge by\nremoving the path that contains the answer. We provide theoretical analysis of\nthe awakening effect in LLMs and support our hypothesis with experiments on two\nKnowledge Graphs (KGs) Question Answering (QA) datasets. Furthermore, we\npresent a new task, Unseen Entity KGQA, simulating real-world challenges where\nentity linking fails due to KG incompleteness. Our awakening-based approach\ndemonstrates greater efficacy in practical applications, outperforms\ntraditional methods that rely on embedding-based similarity which are prone to\nreturning noisy information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRAG\u7cfb\u7edf\u4e2d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u4ee5\u901a\u8fc7\u5176\u5185\u90e8\u5df2\u6709\u7684\u90e8\u5206\u76f8\u5173\u77e5\u8bc6\u88ab\u201c\u5524\u9192\u201d\u7684\u89c2\u70b9\uff0c\u5e76\u5728\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u5904\u7406\u4e0d\u5b8c\u6574\u77e5\u8bc6\u5e93\u548c\u672a\u89c1\u5b9e\u4f53\u95ee\u7b54\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "motivation": "\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u4e2d\uff0c\u6709\u6548\u5229\u7528\u90e8\u5206\u76f8\u5173\u77e5\u8bc6\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u77e5\u8bc6\u5e93\u4e0d\u5b8c\u6574\u65f6\u3002", "method": "\u672c\u6587\u63d0\u51faLLMs\u53ef\u4ee5\u901a\u8fc7\u81ea\u8eab\u5df2\u6709\u7684\u90e8\u5206\u76f8\u5173\u77e5\u8bc6\u88ab\u201c\u5524\u9192\u201d\u7684\u65b0\u89c6\u89d2\u3002\u901a\u8fc7\u79fb\u9664\u7b54\u6848\u8def\u5f84\u6765\u6784\u5efa\u90e8\u5206\u76f8\u5173\u77e5\u8bc6\uff0c\u5e76\u4ece\u7406\u8bba\u4e0a\u5206\u6790\u4e86LLMs\u7684\u201c\u5524\u9192\u6548\u5e94\u201d\u3002\u901a\u8fc7\u5728\u4e24\u4e2a\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5e76\u5f15\u5165\u4e86\u201c\u672a\u89c1\u5b9e\u4f53KGQA\u201d\u65b0\u4efb\u52a1\u6765\u6a21\u62df\u5b9e\u4f53\u94fe\u63a5\u5931\u8d25\u7684\u771f\u5b9e\u573a\u666f\u3002", "result": "\u57fa\u4e8e\u201c\u5524\u9192\u201d\u7684\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6548\u7387\uff0c\u5e76\u4e14\u4f18\u4e8e\u4f20\u7edf\u4f9d\u8d56\u4e8e\u5d4c\u5165\u76f8\u4f3c\u5ea6\u7684RAG\u65b9\u6cd5\uff0c\u540e\u8005\u5bb9\u6613\u8fd4\u56de\u566a\u58f0\u4fe1\u606f\u3002", "conclusion": "LLMs\u53ef\u4ee5\u901a\u8fc7\u90e8\u5206\u76f8\u5173\u77e5\u8bc6\u88ab\u6709\u6548\u5730\u201c\u5524\u9192\u201d\uff0c\u8fd9\u4e3a\u5904\u7406\u4e0d\u5b8c\u6574\u77e5\u8bc6\u5e93\u4e2d\u975e\u7cbe\u786e\u68c0\u7d22\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5c24\u5176\u5728\u5b9e\u4f53\u672a\u89c1\u7684\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.00947", "pdf": "https://arxiv.org/pdf/2508.00947", "abs": "https://arxiv.org/abs/2508.00947", "authors": ["Shiyao Sang", "Yinggang Ling"], "title": "Service Discovery-Based Hybrid Network Middleware for Efficient Communication in Distributed Robotic Systems", "categories": ["cs.RO", "cs.DC", "cs.NI", "I.2.9; C.2.4; D.4.7"], "comment": "8 pages, 8 figures, accepted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2025, oral presentation", "summary": "Robotic middleware is fundamental to ensuring reliable communication among\nsystem components and is crucial for intelligent robotics, autonomous vehicles,\nand smart manufacturing. However, existing robotic middleware often struggles\nto meet the diverse communication demands, optimize data transmission\nefficiency, and maintain scheduling determinism between Orin computing units in\nlarge-scale L4 autonomous vehicle deployments. This paper presents RIMAOS2C, a\nservice discovery-based hybrid network communication middleware designed to\ntackle these challenges. By leveraging multi-level service discovery multicast,\nRIMAOS2C supports a wide variety of communication modes, including multiple\ncross-chip Ethernet protocols and PCIe communication capabilities. Its core\nmechanism, the Message Bridge, optimizes data flow forwarding and employs\nshared memory for centralized message distribution, reducing message redundancy\nand minimizing transmission delay uncertainty. Tested on L4 vehicles and Jetson\nOrin domain controllers, RIMAOS2C leverages TCP-based ZeroMQ to overcome the\nlarge-message transmission bottleneck in native CyberRT. In scenarios with two\ncross-chip subscribers, it eliminates message redundancy and improves\nlarge-data transmission efficiency by 36 to 40 percent while reducing callback\nlatency variation by 42 to 906 percent. This research advances the\ncommunication capabilities of robotic operating systems and proposes a novel\napproach to optimizing communication in distributed computing architectures for\nautonomous driving.", "AI": {"tldr": "RIMAOS2C\u662f\u4e3a\u89e3\u51b3L4\u81ea\u52a8\u9a7e\u9a76\u4e2d\u673a\u5668\u4eba\u4e2d\u95f4\u4ef6\u901a\u4fe1\u6311\u6218\u800c\u8bbe\u8ba1\u7684\u6df7\u5408\u901a\u4fe1\u4e2d\u95f4\u4ef6\uff0c\u901a\u8fc7\u670d\u52a1\u53d1\u73b0\u548c\u6d88\u606f\u6865\u63a5\u63d0\u5347\u4e86\u8de8\u82af\u7247\u5927\u6570\u636e\u4f20\u8f93\u6548\u7387\u548c\u8c03\u5ea6\u786e\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u4e2d\u95f4\u4ef6\u5728\u5927\u578bL4\u81ea\u52a8\u9a7e\u9a76\u90e8\u7f72\u4e2d\uff0c\u96be\u4ee5\u6ee1\u8db3Orin\u8ba1\u7b97\u5355\u5143\u4e4b\u95f4\u591a\u6837\u5316\u7684\u901a\u4fe1\u9700\u6c42\u3001\u4f18\u5316\u6570\u636e\u4f20\u8f93\u6548\u7387\u4ee5\u53ca\u4fdd\u6301\u8c03\u5ea6\u786e\u5b9a\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86RIMAOS2C\uff0c\u4e00\u4e2a\u57fa\u4e8e\u670d\u52a1\u53d1\u73b0\u7684\u6df7\u5408\u7f51\u7edc\u901a\u4fe1\u4e2d\u95f4\u4ef6\u3002\u5b83\u5229\u7528\u591a\u7ea7\u670d\u52a1\u53d1\u73b0\u591a\u64ad\uff0c\u652f\u6301\u591a\u79cd\u8de8\u82af\u7247\u4ee5\u592a\u7f51\u534f\u8bae\u548cPCIe\u901a\u4fe1\u3002\u5176\u6838\u5fc3\u673a\u5236\u201c\u6d88\u606f\u6865\u63a5\u201d\u901a\u8fc7\u5171\u4eab\u5185\u5b58\u4f18\u5316\u6570\u636e\u6d41\u8f6c\u53d1\uff0c\u5b9e\u73b0\u96c6\u4e2d\u6d88\u606f\u5206\u53d1\uff0c\u51cf\u5c11\u6d88\u606f\u5197\u4f59\u5e76\u6700\u5c0f\u5316\u4f20\u8f93\u5ef6\u8fdf\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728L4\u8f66\u8f86\u548cJetson Orin\u57df\u63a7\u5236\u5668\u4e0a\u6d4b\u8bd5\u8868\u660e\uff0cRIMAOS2C\u514b\u670d\u4e86\u539f\u751fCyberRT\u7684\u5927\u6d88\u606f\u4f20\u8f93\u74f6\u9888\u3002\u5728\u53cc\u8de8\u82af\u7247\u8ba2\u9605\u8005\u573a\u666f\u4e0b\uff0c\u5b83\u6d88\u9664\u4e86\u6d88\u606f\u5197\u4f59\uff0c\u5c06\u5927\u6570\u636e\u4f20\u8f93\u6548\u7387\u63d0\u5347\u4e8636%\u523040%\uff0c\u5e76\u5c06\u56de\u8c03\u5ef6\u8fdf\u6ce2\u52a8\u964d\u4f4e\u4e8642%\u5230906%\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\uff08ROS\uff09\u7684\u901a\u4fe1\u80fd\u529b\uff0c\u5e76\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5206\u5e03\u5f0f\u8ba1\u7b97\u67b6\u6784\u4e2d\u7684\u901a\u4fe1\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u65b9\u6cd5\u3002"}}
{"id": "2508.00886", "pdf": "https://arxiv.org/pdf/2508.00886", "abs": "https://arxiv.org/abs/2508.00886", "authors": ["Etienne Buehrle", "Christoph Stiller"], "title": "Stochastic Optimal Control via Measure Relaxations", "categories": ["cs.LG", "math.OC", "90C22, 93C10, 28A99"], "comment": "7 pages, 4 figures", "summary": "The optimal control problem of stochastic systems is commonly solved via\nrobust or scenario-based optimization methods, which are both challenging to\nscale to long optimization horizons. We cast the optimal control problem of a\nstochastic system as a convex optimization problem over occupation measures. We\ndemonstrate our method on a set of synthetic and real-world scenarios, learning\ncost functions from data via Christoffel polynomials. The code for our\nexperiments is available at https://github.com/ebuehrle/dpoc.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u968f\u673a\u7cfb\u7edf\u6700\u4f18\u63a7\u5236\u95ee\u9898\u8f6c\u5316\u4e3a\u57fa\u4e8e\u5360\u6709\u6d4b\u5ea6\u7684\u51f8\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u4f18\u5316\u8303\u56f4\u4e0b\u7684\u89c4\u6a21\u5316\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u968f\u673a\u7cfb\u7edf\u6700\u4f18\u63a7\u5236\u65b9\u6cd5\uff08\u5982\u9c81\u68d2\u4f18\u5316\u548c\u57fa\u4e8e\u573a\u666f\u7684\u4f18\u5316\uff09\u96be\u4ee5\u6269\u5c55\u5230\u8f83\u957f\u7684\u4f18\u5316\u8303\u56f4\uff0c\u5b58\u5728\u89c4\u6a21\u5316\u74f6\u9888\u3002", "method": "\u4f5c\u8005\u5c06\u968f\u673a\u7cfb\u7edf\u7684\u6700\u4f18\u63a7\u5236\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u57fa\u4e8e\u5360\u6709\u6d4b\u5ea6\u7684\u51f8\u4f18\u5316\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7Christoffel\u591a\u9879\u5f0f\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u6210\u672c\u51fd\u6570\uff0c\u5e76\u5728\u5408\u6210\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u8fdb\u884c\u4e86\u6f14\u793a\u9a8c\u8bc1\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u5408\u6210\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u5f97\u5230\u4e86\u6709\u6548\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u89e3\u51b3\u6b64\u7c7b\u95ee\u9898\u7684\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5360\u6709\u6d4b\u5ea6\u7684\u51f8\u4f18\u5316\u6846\u67b6\uff0c\u8be5\u7814\u7a76\u4e3a\u968f\u673a\u7cfb\u7edf\u6700\u4f18\u63a7\u5236\u5728\u957f\u4f18\u5316\u8303\u56f4\u4e0b\u7684\u89c4\u6a21\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01015", "pdf": "https://arxiv.org/pdf/2508.01015", "abs": "https://arxiv.org/abs/2508.01015", "authors": ["Byron Dowling", "Jozef Probcin", "Adam Czajka"], "title": "AutoSIGHT: Automatic Eye Tracking-based System for Immediate Grading of Human experTise", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "This work has been accepted for publication in the proceedings of the\n  IEEE VL/HCC conference 2025. The final published version will be available\n  via IEEE Xplore", "summary": "Can we teach machines to assess the expertise of humans solving visual tasks\nautomatically based on eye tracking features? This paper proposes AutoSIGHT,\nAutomatic System for Immediate Grading of Human experTise, that classifies\nexpert and non-expert performers, and builds upon an ensemble of features\nextracted from eye tracking data while the performers were solving a visual\ntask. Results on the task of iris Presentation Attack Detection (PAD) used for\nthis study show that with a small evaluation window of just 5 seconds,\nAutoSIGHT achieves an average average Area Under the ROC curve performance of\n0.751 in subject-disjoint train-test regime, indicating that such detection is\nviable. Furthermore, when a larger evaluation window of up to 30 seconds is\navailable, the Area Under the ROC curve (AUROC) increases to 0.8306, indicating\nthe model is effectively leveraging more information at a cost of slightly\ndelayed decisions. This work opens new areas of research on how to incorporate\nthe automatic weighing of human and machine expertise into human-AI pairing\nsetups, which need to react dynamically to nonstationary expertise distribution\nbetween the human and AI players (e.g. when the experts need to be replaced, or\nthe task at hand changes rapidly). Along with this paper, we offer the eye\ntracking data used in this study collected from 6 experts and 53 non-experts\nsolving iris PAD visual task.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AutoSIGHT\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u81ea\u52a8\u8bc4\u4f30\u4eba\u7c7b\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u4e13\u4e1a\u6c34\u5e73\uff0c\u5e76\u5728\u8679\u819cPAD\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u8bc6\u522b\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u7684\u53ef\u884c\u6027\u4e0e\u6709\u6548\u6027\u3002", "motivation": "\u65e8\u5728\u63a2\u7d22\u673a\u5668\u662f\u5426\u80fd\u901a\u8fc7\u5206\u6790\u4eba\u7c7b\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u773c\u52a8\u8ffd\u8e2a\u7279\u5f81\uff0c\u81ea\u52a8\u8bc4\u4f30\u5176\u4e13\u4e1a\u6c34\u5e73\uff0c\u4ee5\u652f\u6301\u5728\u4eba\u673a\u534f\u4f5c\u573a\u666f\u4e2d\u52a8\u6001\u8c03\u6574\u4eba\u4e0eAI\u7684\u804c\u8d23\u3002", "method": "\u63d0\u51fa\u4e86AutoSIGHT\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u4ece\u4eba\u7c7b\u5b8c\u6210\u89c6\u89c9\u4efb\u52a1\uff08\u5982\u8679\u819c\u5448\u73b0\u653b\u51fb\u68c0\u6d4b\uff09\u65f6\u6536\u96c6\u7684\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u4e2d\u63d0\u53d6\u7684\u7279\u5f81\u96c6\u5408\uff0c\u6784\u5efa\u5206\u7c7b\u6a21\u578b\u4ee5\u533a\u5206\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u8868\u73b0\u8005\u3002", "result": "\u5728\u8679\u819cPAD\u4efb\u52a1\u4e2d\uff0cAutoSIGHT\u57285\u79d2\u8bc4\u4f30\u7a97\u53e3\u4e0b\uff0cROC\u66f2\u7ebf\u4e0b\u9762\u79ef\uff08AUROC\uff09\u8fbe\u52300.751\uff1b\u5f53\u8bc4\u4f30\u7a97\u53e3\u6269\u5927\u523030\u79d2\u65f6\uff0cAUROC\u63d0\u9ad8\u81f30.8306\uff0c\u8868\u660e\u7cfb\u7edf\u80fd\u6709\u6548\u5229\u7528\u66f4\u591a\u4fe1\u606f\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u773c\u52a8\u8ffd\u8e2a\u7279\u5f81\u81ea\u52a8\u8bc4\u4f30\u4eba\u7c7b\u4e13\u4e1a\u6c34\u5e73\u662f\u53ef\u884c\u7684\uff0c\u4e14\u8bc4\u4f30\u65f6\u95f4\u7a97\u53e3\u7684\u589e\u52a0\u80fd\u663e\u8457\u63d0\u5347\u8bc6\u522b\u51c6\u786e\u7387\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4eba\u673a\u534f\u540c\u4e2d\u52a8\u6001\u5e73\u8861\u4eba\u7c7b\u4e0eAI\u4e13\u4e1a\u77e5\u8bc6\u5f00\u542f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.01073", "pdf": "https://arxiv.org/pdf/2508.01073", "abs": "https://arxiv.org/abs/2508.01073", "authors": ["Martin B\u00f6ckling", "Heiko Paulheim"], "title": "gpuRDF2vec -- Scalable GPU-based RDF2vec", "categories": ["cs.AI"], "comment": "18 pages, ISWC 2025", "summary": "Generating Knowledge Graph (KG) embeddings at web scale remains challenging.\nAmong existing techniques, RDF2vec combines effectiveness with strong\nscalability. We present gpuRDF2vec, an open source library that harnesses\nmodern GPUs and supports multi-node execution to accelerate every stage of the\nRDF2vec pipeline. Extensive experiments on both synthetically generated graphs\nand real-world benchmarks show that gpuRDF2vec achieves up to a substantial\nspeedup over the currently fastest alternative, i.e., jRDF2vec. In a\nsingle-node setup, our walk-extraction phase alone outperforms pyRDF2vec,\nSparkKGML, and jRDF2vec by a substantial margin using random walks on large/\ndense graphs, and scales very well to longer walks, which typically lead to\nbetter quality embeddings. Our implementation of gpuRDF2vec enables\npractitioners and researchers to train high-quality KG embeddings on\nlarge-scale graphs within practical time budgets and builds on top of Pytorch\nLightning for the scalable word2vec implementation.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u5b9e\u73b0gpuRDF2vec\uff0c\u4e00\u4e2a\u5229\u7528GPU\u52a0\u901f\u548c\u591a\u8282\u70b9\u652f\u6301\u7684RDF2vec\u5e93\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u7684\u751f\u6210\u901f\u5ea6\u3002", "motivation": "\u5728\u5927\u89c4\u6a21\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u4e0a\u751f\u6210\u5d4c\u5165\uff08embeddings\uff09\u4ecd\u7136\u9762\u4e34\u4e25\u5cfb\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86gpuRDF2vec\uff0c\u4e00\u4e2a\u5f00\u6e90\u5e93\uff0c\u5b83\u5229\u7528\u73b0\u4ee3GPU\u548c\u591a\u8282\u70b9\u6267\u884c\u6765\u52a0\u901fRDF2vec\u7ba1\u7ebf\u7684\u6240\u6709\u9636\u6bb5\u3002\u8be5\u5b9e\u73b0\u57fa\u4e8ePytorch Lightning\u8fdb\u884c\u53ef\u6269\u5c55\u7684word2vec\u3002", "result": "\u5728\u5408\u6210\u56fe\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cgpuRDF2vec\u76f8\u5bf9\u4e8e\u76ee\u524d\u6700\u5feb\u7684\u66ff\u4ee3\u65b9\u6848jRDF2vec\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u3002\u5728\u5355\u8282\u70b9\u8bbe\u7f6e\u4e0b\uff0c\u5176\u8def\u5f84\u63d0\u53d6\u9636\u6bb5\u5728\u4f7f\u7528\u968f\u673a\u6e38\u8d70\u5904\u7406\u5927\u578b/\u5bc6\u96c6\u56fe\u65f6\uff0c\u5927\u5e45\u4f18\u4e8epyRDF2vec\u3001SparkKGML\u548cjRDF2vec\uff0c\u5e76\u4e14\u5bf9\u66f4\u957f\u6e38\u8d70\u5177\u6709\u826f\u597d\u7684\u6269\u5c55\u6027\u3002", "conclusion": "gpuRDF2vec\u4f7f\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u80fd\u591f\u5728\u5b9e\u9645\u7684\u65f6\u95f4\u9884\u7b97\u5185\uff0c\u5728\u5927\u89c4\u6a21\u56fe\u4e0a\u8bad\u7ec3\u9ad8\u8d28\u91cf\u7684\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u3002"}}
{"id": "2508.01302", "pdf": "https://arxiv.org/pdf/2508.01302", "abs": "https://arxiv.org/abs/2508.01302", "authors": ["Chenming Tang", "Yutong Yang", "Yunfang Wu"], "title": "KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Knowledge editing aims to modify outdated knowledge in large language models\n(LLMs) efficiently while retaining their powerful capabilities. Most existing\nmethods rely on either parameter-level editing or retrieval-based approaches.\nIn this work, we propose Knowledge Editing alignment with Diverse Augmentation\nand Self-adaptive inference (KEDAS) to better align LLMs with knowledge\nediting. In the alignment phase, LLMs learn to apply in-context edited\nknowledge via low-rank adaptation. During editing, we design a diverse edit\naugmentation technique to improve the recall of edits. After that, a\nself-adaptive post-alignment inference mechanism is proposed, in which a\nfilter-based smart retriever is employed to perform a dynamic selection of\ninference routing. Specifically, irrelevant queries will go through the\noriginal pre-alignment model directly, while relevant ones, together with their\nrelated edits, go through the model with aligned adapters activated. In\nexperiments, KEDAS secures the highest overall performance scores in 35 out of\n36 cases across four datasets with three LLMs on three settings, surpassing its\nstrong knowledge editing alignment counterpart by about 19.8 harmonic mean\nscores of edit success, locality and portability and outperforming both\nparameter editing and retrieval-based baselines significantly. Analysis of\ncomputational cost and performance on general tasks further validates the\nrobustness and efficiency of KEDAS, indicating that it presents an ideal\nparadigm of knowledge editing alignment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faKEDAS\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u9f50\u5b66\u4e60\u3001\u591a\u6837\u6027\u589e\u5f3a\u548c\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u9ad8\u6548\u5730\u66f4\u65b0\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\uff0c\u5e76\u5728\u591a\u9879\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\uff0c\u65e0\u8bba\u662f\u53c2\u6570\u7ea7\u7f16\u8f91\u8fd8\u662f\u68c0\u7d22\u5f0f\u65b9\u6cd5\uff0c\u5728\u9ad8\u6548\u4fee\u6539\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fc7\u65f6\u77e5\u8bc6\u7684\u540c\u65f6\uff0c\u96be\u4ee5\u5145\u5206\u4fdd\u7559\u5176\u5f3a\u5927\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\u6765\u66f4\u597d\u5730\u5c06LLM\u4e0e\u77e5\u8bc6\u7f16\u8f91\u5bf9\u9f50\u3002", "method": "\u672c\u6587\u63d0\u51faKEDAS\uff08Knowledge Editing alignment with Diverse Augmentation and Self-adaptive inference\uff09\u6846\u67b6\uff1a1. **\u5bf9\u9f50\u9636\u6bb5**\uff1aLLM\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u5b66\u4e60\u4e0a\u4e0b\u6587\u4e2d\u7684\u7f16\u8f91\u77e5\u8bc6\u30022. **\u7f16\u8f91\u9636\u6bb5**\uff1a\u8bbe\u8ba1\u591a\u6837\u6027\u7f16\u8f91\u589e\u5f3a\u6280\u672f\u4ee5\u63d0\u9ad8\u7f16\u8f91\u7684\u53ec\u56de\u7387\u30023. **\u63a8\u7406\u9636\u6bb5**\uff1a\u5f15\u5165\u81ea\u9002\u5e94\u540e\u5bf9\u9f50\u63a8\u7406\u673a\u5236\uff0c\u5229\u7528\u57fa\u4e8e\u8fc7\u6ee4\u5668\u7684\u667a\u80fd\u68c0\u7d22\u5668\u52a8\u6001\u9009\u62e9\u63a8\u7406\u8def\u5f84\u2014\u2014\u4e0d\u76f8\u5173\u67e5\u8be2\u76f4\u63a5\u901a\u8fc7\u539f\u59cb\u9884\u5bf9\u9f50\u6a21\u578b\uff0c\u800c\u76f8\u5173\u67e5\u8be2\u53ca\u5176\u7f16\u8f91\u5219\u901a\u8fc7\u6fc0\u6d3b\u9002\u914d\u5668\u7684\u6a21\u578b\u3002", "result": "KEDAS\u5728\u56db\u4e2a\u6570\u636e\u96c6\u3001\u4e09\u4e2aLLM\u3001\u4e09\u79cd\u8bbe\u7f6e\u4e0b\u768436\u4e2a\u6848\u4f8b\u4e2d\uff0c\u670935\u4e2a\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u6574\u4f53\u6027\u80fd\u5206\u6570\u3002\u5728\u7f16\u8f91\u6210\u529f\u3001\u5c40\u90e8\u6027\u548c\u53ef\u79fb\u690d\u6027\u7b49\u8c03\u548c\u5e73\u5747\u5206\u6570\u4e0a\uff0c\u6bd4\u73b0\u6709\u5f3a\u77e5\u8bc6\u7f16\u8f91\u5bf9\u9f50\u65b9\u6cd5\u9ad8\u51fa\u7ea619.8\u5206\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u53c2\u6570\u7f16\u8f91\u548c\u68c0\u7d22\u57fa\u7ebf\u3002\u5bf9\u8ba1\u7b97\u6210\u672c\u548c\u901a\u7528\u4efb\u52a1\u6027\u80fd\u7684\u5206\u6790\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86KEDAS\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "KEDAS\u4e3a\u77e5\u8bc6\u7f16\u8f91\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u60f3\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u5176\u521b\u65b0\u7684\u5bf9\u9f50\u3001\u589e\u5f3a\u548c\u81ea\u9002\u5e94\u63a8\u7406\u673a\u5236\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u5e76\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.01469", "pdf": "https://arxiv.org/pdf/2508.01469", "abs": "https://arxiv.org/abs/2508.01469", "authors": ["Imtiaz Karim", "Hyunwoo Lee", "Hassan Asghar", "Kazi Samin Mubasshir", "Seulgi Han", "Mashroor Hasan Bhuiyan", "Elisa Bertino"], "title": "VWAttacker: A Systematic Security Testing Framework for Voice over WiFi User Equipments", "categories": ["cs.CR", "cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "We present VWAttacker, the first systematic testing framework for analyzing\nthe security of Voice over WiFi (VoWiFi) User Equipment (UE) implementations.\nVWAttacker includes a complete VoWiFi network testbed that communicates with\nCommercial-Off-The-Shelf (COTS) UEs based on a simple interface to test the\nbehavior of diverse VoWiFi UE implementations; uses property-guided adversarial\ntesting to uncover security issues in different UEs systematically. To reduce\nmanual effort in extracting and testing properties, we introduce an LLM-based,\nsemi-automatic, and scalable approach for property extraction and testcase (TC)\ngeneration. These TCs are systematically mutated by two domain-specific\ntransformations. Furthermore, we introduce two deterministic oracles to detect\nproperty violations automatically. Coupled with these techniques, VWAttacker\nextracts 63 properties from 11 specifications, evaluates 1,116 testcases, and\ndetects 13 issues in 21 UEs. The issues range from enforcing a DH shared secret\nto 0 to supporting weak algorithms. These issues result in attacks that expose\nthe victim UE's identity or establish weak channels, thus severely hampering\nthe security of cellular networks. We responsibly disclose the findings to all\nthe related vendors. At the time of writing, one of the vulnerabilities has\nbeen acknowledged by MediaTek with high severity.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86VWAttacker\uff0c\u4e00\u4e2a\u9488\u5bf9VoWiFi\u7528\u6237\u8bbe\u5907\uff08UE\uff09\u5b9e\u73b0\u7684\u7cfb\u7edf\u5316\u5b89\u5168\u6d4b\u8bd5\u6846\u67b6\uff0c\u5b83\u5229\u7528LLM\u8f85\u52a9\u7684\u5bf9\u6297\u6027\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u6210\u529f\u63ed\u793a\u4e8621\u6b3eUE\u4e2d\u768413\u4e2a\u5b89\u5168\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u53ef\u80fd\u5bfc\u81f4\u8eab\u4efd\u66b4\u9732\u6216\u5f31\u4fe1\u9053\u5efa\u7acb\uff0c\u4e25\u91cd\u5f71\u54cd\u8702\u7a9d\u7f51\u7edc\u5b89\u5168\u3002", "motivation": "\u5f53\u524dVoWiFi UE\u5b9e\u73b0\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u5bf9\u8702\u7a9d\u7f51\u7edc\u7684\u5b89\u5168\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u9700\u8981\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684\u6846\u67b6\u6765\u8bc6\u522b\u548c\u5206\u6790\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "VWAttacker\u5305\u542b\u4e00\u4e2a\u5b8c\u6574\u7684VoWiFi\u7f51\u7edc\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4e0e\u5546\u7528UE\u901a\u4fe1\uff0c\u5e76\u91c7\u7528\u5c5e\u6027\u5f15\u5bfc\u7684\u5bf9\u6297\u6027\u6d4b\u8bd5\u3002\u4e3a\u964d\u4f4e\u624b\u52a8\u5de5\u4f5c\u91cf\uff0c\u5f15\u5165\u57fa\u4e8eLLM\u7684\u534a\u81ea\u52a8\u5316\u65b9\u6cd5\u8fdb\u884c\u5c5e\u6027\u63d0\u53d6\u548c\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u8f6c\u6362\u5bf9\u6d4b\u8bd5\u7528\u4f8b\u8fdb\u884c\u53d8\u5f02\u3002\u540c\u65f6\uff0c\u5229\u7528\u4e24\u4e2a\u786e\u5b9a\u6027\u9884\u8a00\u673a\u81ea\u52a8\u68c0\u6d4b\u5c5e\u6027\u8fdd\u89c4\u3002", "result": "VWAttacker\u4ece11\u4e2a\u89c4\u8303\u4e2d\u63d0\u53d6\u4e8663\u4e2a\u5c5e\u6027\uff0c\u8bc4\u4f30\u4e861,116\u4e2a\u6d4b\u8bd5\u7528\u4f8b\uff0c\u5e76\u572821\u4e2aUE\u4e2d\u68c0\u6d4b\u523013\u4e2a\u5b89\u5168\u95ee\u9898\u3002\u8fd9\u4e9b\u95ee\u9898\u5305\u62ec\u5f3a\u5236DH\u5171\u4eab\u5bc6\u94a5\u4e3a0\u548c\u652f\u6301\u5f31\u7b97\u6cd5\u7b49\uff0c\u53ef\u80fd\u5bfc\u81f4\u653b\u51fb\u8005\u66b4\u9732\u53d7\u5bb3\u8005UE\u8eab\u4efd\u6216\u5efa\u7acb\u5f31\u4fe1\u9053\u3002\u90e8\u5206\u6f0f\u6d1e\u5df2\u5411\u5382\u5546\u62ab\u9732\uff0c\u5176\u4e2d\u4e00\u4e2a\u5df2\u88abMediaTek\u786e\u8ba4\u4e3a\u9ad8\u5371\u3002", "conclusion": "VWAttacker\u6210\u529f\u63ed\u793a\u4e86VoWiFi UE\u5b9e\u73b0\u4e2d\u5b58\u5728\u7684\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7cfb\u7edf\u5316\u5b89\u5168\u6d4b\u8bd5\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u8702\u7a9d\u7f51\u7edc\u7684\u6574\u4f53\u5b89\u5168\u6784\u6210\u5a01\u80c1\uff0c\u51f8\u663e\u4e86\u5bf9VoWiFi UE\u5b89\u5168\u8fdb\u884c\u6301\u7eed\u5173\u6ce8\u548c\u4fee\u590d\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2508.00887", "pdf": "https://arxiv.org/pdf/2508.00887", "abs": "https://arxiv.org/abs/2508.00887", "authors": ["Binrui Shen", "Yuan Liang", "Shengxin Zhu"], "title": "FRAM: Frobenius-Regularized Assignment Matching with Mixed-Precision Computing", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Graph matching, typically formulated as a Quadratic Assignment Problem (QAP),\nseeks to establish node correspondences between two graphs. To address the\nNP-hardness of QAP, some existing methods adopt projection-based relaxations\nthat embed the problem into the convex hull of the discrete domain. However,\nthese relaxations inevitably enlarge the feasible set, introducing two sources\nof error: numerical scale sensitivity and geometric misalignment between the\nrelaxed and original domains. To alleviate these errors, we propose a novel\nrelaxation framework by reformulating the projection step as a\nFrobenius-regularized Linear Assignment (FRA) problem, where a tunable\nregularization term mitigates feasible region inflation. This formulation\nenables normalization-based operations to preserve numerical scale invariance\nwithout compromising accuracy. To efficiently solve FRA, we propose the Scaling\nDoubly Stochastic Normalization (SDSN) algorithm. Building on its favorable\ncomputational properties, we develop a theoretically grounded mixed-precision\narchitecture to achieve substantial acceleration. Comprehensive CPU-based\nbenchmarks demonstrate that FRAM consistently outperforms all baseline methods\nunder identical precision settings. When combined with a GPU-based\nmixed-precision architecture, FRAM achieves up to 370X speedup over its\nCPU-FP64 counterpart, with negligible loss in solution accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFRAM\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u9896\u7684FRA\u677e\u5f1b\u548cSDSN\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u56fe\u5339\u914d\u4e2d\u73b0\u6709\u677e\u5f1b\u65b9\u6cd5\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u95ee\u9898\uff0c\u5e76\u5728CPU\u548cGPU\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u56fe\u5339\u914d\u95ee\u9898\u4e2d\u7684\u4e8c\u6b21\u5206\u914d\u95ee\u9898\uff08QAP\uff09NP-hard\uff0c\u73b0\u6709\u57fa\u4e8e\u6295\u5f71\u7684\u677e\u5f1b\u65b9\u6cd5\u4f1a\u6269\u5927\u53ef\u884c\u57df\uff0c\u5bfc\u81f4\u6570\u503c\u5c3a\u5ea6\u654f\u611f\u6027\u548c\u51e0\u4f55\u9519\u4f4d\u8bef\u5dee\uff0c\u5f71\u54cd\u6c42\u89e3\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684Frobenius\u6b63\u5219\u5316\u7ebf\u6027\u5206\u914d\uff08FRA\uff09\u95ee\u9898\u677e\u5f1b\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u8c03\u6b63\u5219\u5316\u9879\u7f13\u89e3\u53ef\u884c\u57df\u81a8\u80c0\uff1b\u5f15\u5165\u5f52\u4e00\u5316\u64cd\u4f5c\u4fdd\u6301\u6570\u503c\u5c3a\u5ea6\u4e0d\u53d8\u6027\u3002\u4e3a\u9ad8\u6548\u6c42\u89e3FRA\uff0c\u5f00\u53d1Scaling Doubly Stochastic Normalization (SDSN) \u7b97\u6cd5\uff0c\u5e76\u6784\u5efa\u7406\u8bba\u57fa\u7840\u7684\u6df7\u5408\u7cbe\u5ea6\u67b6\u6784\u5b9e\u73b0\u52a0\u901f\u3002", "result": "CPU\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0cFRAM\u5728\u76f8\u540c\u7cbe\u5ea6\u8bbe\u7f6e\u4e0b\u6301\u7eed\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u7ed3\u5408GPU\u6df7\u5408\u7cbe\u5ea6\u67b6\u6784\uff0cFRAM\u76f8\u8f83\u4e8eCPU-FP64\u7248\u672c\u5b9e\u73b0\u4e86\u9ad8\u8fbe370\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u89e3\u51b3\u65b9\u6848\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "FRAM\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u56fe\u5339\u914d\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u677e\u5f1b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u52a0\u901f\u3002"}}
{"id": "2508.01019", "pdf": "https://arxiv.org/pdf/2508.01019", "abs": "https://arxiv.org/abs/2508.01019", "authors": ["Muhammad Zeeshan", "Umer Zaki", "Syed Ahmed Pasha", "Zaar Khizar"], "title": "3D Reconstruction via Incremental Structure From Motion", "categories": ["cs.CV", "math.OC"], "comment": "8 pages, 8 figures, proceedings in International Bhurban Conference\n  on Applied Sciences & Technology (IBCAST) 2025", "summary": "Accurate 3D reconstruction from unstructured image collections is a key\nrequirement in applications such as robotics, mapping, and scene understanding.\nWhile global Structure from Motion (SfM) techniques rely on full image\nconnectivity and can be sensitive to noise or missing data, incremental SfM\noffers a more flexible alternative. By progressively incorporating new views\ninto the reconstruction, it enables the system to recover scene structure and\ncamera motion even in sparse or partially overlapping datasets. In this paper,\nwe present a detailed implementation of the incremental SfM pipeline, focusing\non the consistency of geometric estimation and the effect of iterative\nrefinement through bundle adjustment. We demonstrate the approach using a real\ndataset and assess reconstruction quality through reprojection error and camera\ntrajectory coherence. The results support the practical utility of incremental\nSfM as a reliable method for sparse 3D reconstruction in visually structured\nenvironments.", "AI": {"tldr": "\u672c\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u589e\u91cf\u5f0fSfM\u7ba1\u9053\u7684\u5b9e\u73b0\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u89c6\u89c9\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8fdb\u884c\u7a00\u758f3D\u91cd\u5efa\u7684\u5b9e\u7528\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u51c6\u786e\u76843D\u91cd\u5efa\u662f\u673a\u5668\u4eba\u3001\u5730\u56fe\u7ed8\u5236\u548c\u573a\u666f\u7406\u89e3\u7b49\u5e94\u7528\u7684\u5173\u952e\u9700\u6c42\u3002\u73b0\u6709\u5168\u5c40SfM\u6280\u672f\u4f9d\u8d56\u4e8e\u5b8c\u6574\u7684\u56fe\u50cf\u8fde\u63a5\u6027\uff0c\u4e14\u5bf9\u566a\u58f0\u6216\u7f3a\u5931\u6570\u636e\u654f\u611f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u8bba\u6587\u8be6\u7ec6\u5b9e\u73b0\u4e86\u4e00\u4e2a\u589e\u91cf\u5f0fSfM\u7ba1\u9053\uff0c\u91cd\u70b9\u5173\u6ce8\u51e0\u4f55\u4f30\u8ba1\u7684\u4e00\u81f4\u6027\u4ee5\u53ca\u901a\u8fc7\u5149\u675f\u5e73\u5dee\u6cd5\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u7684\u6548\u679c\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u9010\u6b65\u6574\u5408\u65b0\u89c6\u56fe\u6765\u6062\u590d\u573a\u666f\u7ed3\u6784\u548c\u76f8\u673a\u8fd0\u52a8\u3002\u7814\u7a76\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u91cd\u6295\u5f71\u8bef\u5dee\u548c\u76f8\u673a\u8f68\u8ff9\u4e00\u81f4\u6027\u6765\u8bc4\u4f30\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u4e86\u589e\u91cf\u5f0fSfM\u4f5c\u4e3a\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u5728\u89c6\u89c9\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8fdb\u884c\u7a00\u758f3D\u91cd\u5efa\u7684\u5b9e\u9645\u6548\u7528\u3002", "conclusion": "\u589e\u91cf\u5f0fSfM\u662f\u4e00\u79cd\u53ef\u9760\u4e14\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5728\u89c6\u89c9\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8fdb\u884c\u7a00\u758f3D\u91cd\u5efa\u3002"}}
{"id": "2508.01097", "pdf": "https://arxiv.org/pdf/2508.01097", "abs": "https://arxiv.org/abs/2508.01097", "authors": ["Neil F. Johnson", "Frank Yingjie Huo"], "title": "Multispin Physics of AI Tipping Points and Hallucinations", "categories": ["cs.AI", "nlin.AO", "physics.comp-ph"], "comment": null, "summary": "Output from generative AI such as ChatGPT, can be repetitive and biased. But\nmore worrying is that this output can mysteriously tip mid-response from good\n(correct) to bad (misleading or wrong) without the user noticing. In 2024\nalone, this reportedly caused $67 billion in losses and several deaths.\nEstablishing a mathematical mapping to a multispin thermal system, we reveal a\nhidden tipping instability at the scale of the AI's 'atom' (basic Attention\nhead). We derive a simple but essentially exact formula for this tipping point\nwhich shows directly the impact of a user's prompt choice and the AI's training\nbias. We then show how the output tipping can get amplified by the AI's\nmultilayer architecture. As well as helping improve AI transparency,\nexplainability and performance, our results open a path to quantifying users'\nAI risk and legal liabilities.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u8f93\u51fa\u53ef\u80fd\u5728\u54cd\u5e94\u4e2d\u9014\u7a81\u7136\u4ece\u6b63\u786e\u53d8\u4e3a\u9519\u8bef\u6216\u8bef\u5bfc\u6027\u3002\u672c\u6587\u901a\u8fc7\u5c06AI\u6620\u5c04\u5230\u591a\u81ea\u65cb\u70ed\u529b\u5b66\u7cfb\u7edf\uff0c\u63ed\u793a\u4e86AI\u201c\u539f\u5b50\u201d\uff08\u57fa\u7840\u6ce8\u610f\u529b\u5934\uff09\u5c42\u9762\u7684\u9690\u85cf\u503e\u8986\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u63a8\u5bfc\u4e86\u4e00\u4e2a\u516c\u5f0f\u6765\u5c55\u793a\u7528\u6237\u63d0\u793a\u9009\u62e9\u548cAI\u8bad\u7ec3\u504f\u5dee\u7684\u5f71\u54cd\uff0c\u8fd8\u6307\u51fa\u591a\u5c42\u67b6\u6784\u4f1a\u653e\u5927\u8fd9\u79cd\u503e\u8986\u3002", "motivation": "\u751f\u6210\u5f0fAI\uff08\u5982ChatGPT\uff09\u7684\u8f93\u51fa\u53ef\u80fd\u5728\u54cd\u5e94\u4e2d\u9014\u795e\u79d8\u5730\u4ece\u6b63\u786e\u53d8\u4e3a\u9519\u8bef\u6216\u8bef\u5bfc\u6027\uff0c\u800c\u7528\u6237\u96be\u4ee5\u5bdf\u89c9\uff0c\u8fd9\u5df2\u5bfc\u81f4\u5de8\u989d\u7ecf\u6d4e\u635f\u5931\u548c\u4eba\u5458\u4f24\u4ea1\uff0c\u5f15\u53d1\u5bf9AI\u53ef\u9760\u6027\u548c\u900f\u660e\u5ea6\u7684\u62c5\u5fe7\u3002", "method": "\u5c06AI\u7684\u201c\u539f\u5b50\u201d\uff08\u57fa\u7840\u6ce8\u610f\u529b\u5934\uff09\u4e0e\u591a\u81ea\u65cb\u70ed\u529b\u5b66\u7cfb\u7edf\u8fdb\u884c\u6570\u5b66\u6620\u5c04\uff0c\u4ece\u800c\u63ed\u793a\u5176\u5185\u5728\u4e0d\u7a33\u5b9a\u6027\u3002\u63a8\u5bfc\u51fa\u4e00\u4e2a\u7b80\u6d01\u4f46\u57fa\u672c\u7cbe\u786e\u7684\u503e\u8986\u70b9\u516c\u5f0f\u3002\u5206\u6790AI\u591a\u5c42\u67b6\u6784\u5982\u4f55\u653e\u5927\u8f93\u51fa\u7684\u503e\u8986\u73b0\u8c61\u3002", "result": "\u63ed\u793a\u4e86AI\u201c\u539f\u5b50\u201d\uff08\u57fa\u7840\u6ce8\u610f\u529b\u5934\uff09\u5c42\u9762\u7684\u9690\u85cf\u503e\u8986\u4e0d\u7a33\u5b9a\u6027\u3002\u5f97\u51fa\u4e86\u4e00\u4e2a\u7b80\u5355\u4f46\u7cbe\u786e\u7684\u503e\u8986\u70b9\u516c\u5f0f\uff0c\u76f4\u63a5\u663e\u793a\u4e86\u7528\u6237\u63d0\u793a\u9009\u62e9\u548cAI\u8bad\u7ec3\u504f\u5dee\u7684\u5f71\u54cd\u3002\u8bc1\u660e\u4e86\u8f93\u51fa\u503e\u8986\u4f1a\u901a\u8fc7AI\u7684\u591a\u5c42\u67b6\u6784\u5f97\u5230\u653e\u5927\u3002", "conclusion": "\u672c\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u63d0\u9ad8AI\u7684\u900f\u660e\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6027\u80fd\uff0c\u5e76\u4e3a\u91cf\u5316\u7528\u6237\u4f7f\u7528AI\u7684\u98ce\u9669\u548c\u6cd5\u5f8b\u8d23\u4efb\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.01309", "pdf": "https://arxiv.org/pdf/2508.01309", "abs": "https://arxiv.org/abs/2508.01309", "authors": ["Weibo Zhou", "Lingbo Li", "Shangsong Liang"], "title": "D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The scarcity and high cost of high-quality question-answering (QA) datasets\nhinder supervised fine-tuning (SFT) for domain-specific large language models\n(LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that\nutilizes LLMs and prompt engineering to produce diverse, high-quality QA\ndatasets from arbitrary textual sources. D-SCoRE integrates\n$\\textbf{D}$ocument-centric processing, $\\textbf{S}$egmentation, $\\textbf{Co}$T\n$\\textbf{R}$easoning, and structured $\\textbf{E}$xport to generate QA-COT\ndatasets tailored for domain-aware SFT. Multi-dimensional control mechanisms,\nsuch as semantic role transformation, question type balancing, and\ncounterfactual materials, enhance diversity and relevance, overcoming\nlimitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA\ndatasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on\nSQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most\ndomains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual\nmaterials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade\nhardware. Its simplicity and scalability enable efficient QA generation and\nhigh-performance fine-tuning across domains.", "AI": {"tldr": "D-SCoRE\u662f\u4e00\u4e2a\u65e0\u8bad\u7ec3\u6d41\u6c34\u7ebf\uff0c\u5229\u7528LLM\u548c\u63d0\u793a\u5de5\u7a0b\u4ece\u4efb\u610f\u6587\u672c\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684QA-CoT\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u9886\u57df\u7279\u5b9aLLM\u5fae\u8c03\u4e2d\u6570\u636e\u96c6\u7a00\u7f3a\u548c\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9886\u57df\u5fae\u8c03\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9ad8\u8d28\u91cf\u95ee\u7b54\uff08QA\uff09\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\u548c\u9ad8\u6210\u672c\u963b\u788d\u4e86\u9886\u57df\u7279\u5b9a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3002", "method": "\u63d0\u51faD-SCoRE\uff08Document-centric processing, Segmentation, CoT Reasoning, Export\uff09\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u5229\u7528LLMs\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u4ece\u4efb\u610f\u6587\u672c\u6e90\u751f\u6210\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7684QA\u6570\u636e\u96c6\u3002\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86\u4ee5\u6587\u6863\u4e3a\u4e2d\u5fc3\u7684\u5904\u7406\u3001\u5206\u6bb5\u3001\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u548c\u7ed3\u6784\u5316\u5bfc\u51fa\uff0c\u4ee5\u751f\u6210\u9002\u7528\u4e8e\u9886\u57df\u611f\u77e5SFT\u7684QA-CoT\u6570\u636e\u96c6\u3002\u901a\u8fc7\u8bed\u4e49\u89d2\u8272\u8f6c\u6362\u3001\u95ee\u9898\u7c7b\u578b\u5e73\u8861\u548c\u53cd\u4e8b\u5b9e\u6750\u6599\u7b49\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u751f\u6210\u6570\u636e\u7684\u591a\u6837\u6027\u548c\u76f8\u5173\u6027\u3002", "result": "\u4f7f\u7528D-SCoRE\u751f\u6210QA\u6570\u636e\u96c6\u5fae\u8c03\u7684LLMs\u5728SQuADShifts\u548cCovid-QA\u6d4b\u8bd5\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u4f7f\u7528\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\uff08SQuAD, Covid-QA\uff09\u5fae\u8c03\u7684LLMs\uff0c\u5e76\u5728\u5927\u591a\u6570\u9886\u57df\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002D-SCoRE\u80fd\u572890\u79d2\u5185\uff0c\u4f7f\u7528\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u76848B LLM\uff0c\u4ece100-200\u8bcd\u7684\u6587\u672c\u4e2d\u751f\u6210\u516d\u4e2aQA-CoT\u5bf9\uff08\u5305\u542b\u56db\u9009\u9879\u53cd\u4e8b\u5b9e\u6750\u6599\uff09\u3002", "conclusion": "D-SCoRE\u7684\u7b80\u6d01\u6027\u548c\u53ef\u6269\u5c55\u6027\u4f7f\u5176\u80fd\u591f\u9ad8\u6548\u5730\u751f\u6210QA\u6570\u636e\uff0c\u5e76\u5b9e\u73b0\u5728\u4e0d\u540c\u9886\u57df\u7684\u9ad8\u6027\u80fd\u5fae\u8c03\u3002"}}
{"id": "2508.01586", "pdf": "https://arxiv.org/pdf/2508.01586", "abs": "https://arxiv.org/abs/2508.01586", "authors": ["Nguyen Cong Luong", "Nguyen Duc Hai", "Duc Van Le", "Huy T. Nguyen", "Thai-Hoc Vu", "Thien Huynh-The", "Ruichen Zhang", "Nguyen Duc Duy Anh", "Dusit Niyato", "Marco Di Renzo", "Dong In Kim", "Quoc-Viet Pham"], "title": "Diffusion Models for Future Networks and Communications: A Comprehensive Survey", "categories": ["cs.LG", "cs.AI", "cs.ET", "cs.IT", "cs.NI", "math.IT"], "comment": "This work was submitted to Proceedings of the IEEE", "summary": "The rise of Generative AI (GenAI) in recent years has catalyzed\ntransformative advances in wireless communications and networks. Among the\nmembers of the GenAI family, Diffusion Models (DMs) have risen to prominence as\na powerful option, capable of handling complex, high-dimensional data\ndistribution, as well as consistent, noise-robust performance. In this survey,\nwe aim to provide a comprehensive overview of the theoretical foundations and\npractical applications of DMs across future communication systems. We first\nprovide an extensive tutorial of DMs and demonstrate how they can be applied to\nenhance optimizers, reinforcement learning and incentive mechanisms, which are\npopular approaches for problems in wireless networks. Then, we review and\ndiscuss the DM-based methods proposed for emerging issues in future networks\nand communications, including channel modeling and estimation, signal detection\nand data reconstruction, integrated sensing and communication, resource\nmanagement in edge computing networks, semantic communications and other\nnotable issues. We conclude the survey with highlighting technical limitations\nof DMs and their applications, as well as discussing future research\ndirections.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u5168\u9762\u6982\u8ff0\u4e86\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u5728\u672a\u6765\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u5305\u62ec\u4f18\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u4fe1\u9053\u5904\u7406\u3001\u4fe1\u53f7\u68c0\u6d4b\u3001\u901a\u611f\u4e00\u4f53\u5316\u548c\u8d44\u6e90\u7ba1\u7406\u7b49\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5c40\u9650\u6027\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u751f\u6210\u5f0fAI\uff08GenAI\uff09\uff0c\u7279\u522b\u662f\u6269\u6563\u6a21\u578b\uff08DMs\uff09\uff0c\u5728\u65e0\u7ebf\u901a\u4fe1\u548c\u7f51\u7edc\u9886\u57df\u5c55\u73b0\u51fa\u53d8\u9769\u6027\u6f5c\u529b\uff0c\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u3001\u9ad8\u7ef4\u6570\u636e\u5e76\u63d0\u4f9b\u7a33\u5b9a\u3001\u6297\u566a\u58f0\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5bf9DMs\u7684\u7406\u8bba\u57fa\u7840\u53ca\u5176\u5728\u672a\u6765\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u8fdb\u884c\u5168\u9762\u7efc\u8ff0\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u7efc\u8ff0\u65b9\u6cd5\uff0c\u9996\u5148\u63d0\u4f9b\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u7684\u8be6\u5c3d\u6559\u7a0b\uff0c\u5e76\u5c55\u793a\u5176\u5982\u4f55\u5e94\u7528\u4e8e\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u4f18\u5316\u5668\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6fc0\u52b1\u673a\u5236\u3002\u968f\u540e\uff0c\u6587\u7ae0\u56de\u987e\u5e76\u8ba8\u8bba\u4e86\u57fa\u4e8eDMs\u89e3\u51b3\u672a\u6765\u7f51\u7edc\u548c\u901a\u4fe1\u65b0\u5174\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u6db5\u76d6\u4fe1\u9053\u5efa\u6a21\u4e0e\u4f30\u8ba1\u3001\u4fe1\u53f7\u68c0\u6d4b\u4e0e\u6570\u636e\u91cd\u5efa\u3001\u901a\u611f\u4e00\u4f53\u5316\u3001\u8fb9\u7f18\u8ba1\u7b97\u7f51\u7edc\u4e2d\u7684\u8d44\u6e90\u7ba1\u7406\u3001\u8bed\u4e49\u901a\u4fe1\u7b49\u3002", "result": "\u4f5c\u4e3a\u4e00\u7bc7\u7efc\u8ff0\u8bba\u6587\uff0c\u5176\u4e3b\u8981\u201c\u7ed3\u679c\u201d\u662f\u5bf9\u6269\u6563\u6a21\u578b\u5728\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u5e94\u7528\u7684\u73b0\u6709\u7814\u7a76\u8fdb\u884c\u4e86\u7cfb\u7edf\u68b3\u7406\u548c\u5f52\u7eb3\uff0c\u5168\u9762\u5c55\u793a\u4e86\u5176\u5728\u589e\u5f3a\u4f18\u5316\u3001\u5904\u7406\u590d\u6742\u901a\u4fe1\u95ee\u9898\u3001\u6539\u5584\u8d44\u6e90\u7ba1\u7406\u7b49\u65b9\u9762\u7684\u6f5c\u529b\u3002\u8bba\u6587\u8bc6\u522b\u5e76\u6574\u5408\u4e86DMs\u5728\u591a\u79cd\u65b0\u5174\u901a\u4fe1\u573a\u666f\u4e0b\u7684\u5e94\u7528\u5b9e\u4f8b\u548c\u7814\u7a76\u8fdb\u5c55\u3002", "conclusion": "\u7efc\u8ff0\u603b\u7ed3\u4e86\u6269\u6563\u6a21\u578b\u53ca\u5176\u5e94\u7528\u5b58\u5728\u7684\u73b0\u6709\u6280\u672f\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u6f5c\u5728\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u63d0\u4f9b\u4e86\u6307\u5f15\u3002"}}
{"id": "2508.00888", "pdf": "https://arxiv.org/pdf/2508.00888", "abs": "https://arxiv.org/abs/2508.00888", "authors": ["Amir Hossein Kalantari", "Eleonora Papadimitriou", "Amir Pooyan Afghari"], "title": "A Dynamic, Context-Aware Framework for Risky Driving Prediction Using Naturalistic Data", "categories": ["cs.LG", "stat.AP", "stat.CO", "stat.ME"], "comment": "32 pages", "summary": "Naturalistic driving studies offer a powerful means for observing and\nquantifying real-world driving behaviour. One of their prominent applications\nin traffic safety is the continuous monitoring and classification of risky\ndriving behaviour. However, many existing frameworks rely on fixed time windows\nand static thresholds for distinguishing between safe and risky behaviour -\nlimiting their ability to respond to the stochastic nature of real-world\ndriving. This study proposes a dynamic and individualised framework for\nidentifying risky driving behaviour using Belgian naturalistic driving data.\nThe approach leverages a rolling time window and bi-level optimisation to\ndynamically calibrate both risk thresholds and model hyperparameters, capturing\nsubtle behavioural shifts. Two safety indicators, speed-weighted headway and\nharsh driving events, were evaluated using three data-driven models: Random\nForest, XGBoost, and Deep Neural Network (DNN). The DNN demonstrated strong\ncapability in capturing subtle changes in driving behaviour, particularly\nexcelling in high-recall tasks, making it promising for early-stage risk\ndetection. XGBoost provided the most balanced and stable performance across\ndifferent thresholds and evaluation metrics. While random forest showed more\nvariability, it responded sensitively to dynamic threshold adjustments, which\nmay be advantageous during model adaptation or tuning. Speed-weighted headway\nemerged as a more stable and context-sensitive risk indicator than harsh\ndriving events, likely due to its robustness to label sparsity and contextual\nvariation. Overall, the findings support the value of adaptive, personalised\nrisk detection approaches for enhancing real-time safety feedback and tailoring\ndriver support in intelligent transport systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u4e14\u4e2a\u6027\u5316\u7684\u98ce\u9669\u9a7e\u9a76\u884c\u4e3a\u8bc6\u522b\u6846\u67b6\uff0c\u5229\u7528\u6eda\u52a8\u65f6\u95f4\u7a97\u548c\u53cc\u5c42\u4f18\u5316\u52a8\u6001\u8c03\u6574\u9608\u503c\u548c\u6a21\u578b\u53c2\u6570\uff0c\u5e76\u5bf9\u6bd4\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u3001XGBoost\u548c\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u4e24\u79cd\u5b89\u5168\u6307\u6807\u4e0a\u7684\u8868\u73b0\uff0c\u652f\u6301\u81ea\u9002\u5e94\u7684\u98ce\u9669\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u98ce\u9669\u9a7e\u9a76\u884c\u4e3a\u68c0\u6d4b\u6846\u67b6\u4f9d\u8d56\u56fa\u5b9a\u65f6\u95f4\u7a97\u548c\u9759\u6001\u9608\u503c\uff0c\u96be\u4ee5\u9002\u5e94\u73b0\u5b9e\u9a7e\u9a76\u7684\u968f\u673a\u6027\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u6709\u6548\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u4e14\u4e2a\u6027\u5316\u7684\u98ce\u9669\u9a7e\u9a76\u884c\u4e3a\u8bc6\u522b\u6846\u67b6\uff0c\u5229\u7528\u6bd4\u5229\u65f6\u81ea\u7136\u9a7e\u9a76\u6570\u636e\uff0c\u901a\u8fc7\u6eda\u52a8\u65f6\u95f4\u7a97\u548c\u53cc\u5c42\u4f18\u5316\u52a8\u6001\u6821\u51c6\u98ce\u9669\u9608\u503c\u548c\u6a21\u578b\u8d85\u53c2\u6570\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u4e24\u79cd\u5b89\u5168\u6307\u6807\uff08\u901f\u5ea6\u52a0\u6743\u8f66\u5934\u65f6\u8ddd\u548c\u6025\u52a0\u901f/\u51cf\u901f\u4e8b\u4ef6\uff09\u5728\u4e09\u79cd\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff08\u968f\u673a\u68ee\u6797\u3001XGBoost\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\uff09\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u6355\u83b7\u9a7e\u9a76\u884c\u4e3a\u7684\u7ec6\u5fae\u53d8\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u53ec\u56de\u7387\u9ad8\u7684\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u65e9\u671f\u98ce\u9669\u68c0\u6d4b\u3002XGBoost\u5728\u4e0d\u540c\u9608\u503c\u548c\u8bc4\u4f30\u6307\u6807\u4e0b\u63d0\u4f9b\u4e86\u6700\u5e73\u8861\u548c\u7a33\u5b9a\u7684\u6027\u80fd\u3002\u968f\u673a\u68ee\u6797\u867d\u7136\u53d8\u5f02\u6027\u8f83\u5927\uff0c\u4f46\u5bf9\u52a8\u6001\u9608\u503c\u8c03\u6574\u654f\u611f\uff0c\u53ef\u80fd\u6709\u5229\u4e8e\u6a21\u578b\u81ea\u9002\u5e94\u6216\u8c03\u4f18\u3002\u901f\u5ea6\u52a0\u6743\u8f66\u5934\u65f6\u8ddd\u88ab\u8bc1\u660e\u662f\u6bd4\u6025\u52a0\u901f/\u51cf\u901f\u4e8b\u4ef6\u66f4\u7a33\u5b9a\u3001\u66f4\u5177\u60c5\u5883\u654f\u611f\u6027\u7684\u98ce\u9669\u6307\u6807\u3002", "conclusion": "\u52a8\u6001\u3001\u4e2a\u6027\u5316\u7684\u98ce\u9669\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u63d0\u5347\u5b9e\u65f6\u5b89\u5168\u53cd\u9988\u548c\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u5b9a\u5236\u9a7e\u9a76\u5458\u652f\u6301\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2508.01045", "pdf": "https://arxiv.org/pdf/2508.01045", "abs": "https://arxiv.org/abs/2508.01045", "authors": ["Theo Di Piazza", "Carole Lazarus", "Olivier Nempont", "Loic Boussel"], "title": "Structured Spectral Graph Learning for Anomaly Classification in 3D Chest CT Scans", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for publication at MICCAI 2025 EMERGE Workshop", "summary": "With the increasing number of CT scan examinations, there is a need for\nautomated methods such as organ segmentation, anomaly detection and report\ngeneration to assist radiologists in managing their increasing workload.\nMulti-label classification of 3D CT scans remains a critical yet challenging\ntask due to the complex spatial relationships within volumetric data and the\nvariety of observed anomalies. Existing approaches based on 3D convolutional\nnetworks have limited abilities to model long-range dependencies while Vision\nTransformers suffer from high computational costs and often require extensive\npre-training on large-scale datasets from the same domain to achieve\ncompetitive performance. In this work, we propose an alternative by introducing\na new graph-based approach that models CT scans as structured graphs,\nleveraging axial slice triplets nodes processed through spectral domain\nconvolution to enhance multi-label anomaly classification performance. Our\nmethod exhibits strong cross-dataset generalization, and competitive\nperformance while achieving robustness to z-axis translation. An ablation study\nevaluates the contribution of each proposed component.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u56fe\u57fa\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06CT\u626b\u63cf\u5efa\u6a21\u4e3a\u7ed3\u6784\u5316\u56fe\u5e76\u5229\u7528\u8c31\u57df\u5377\u79ef\u5904\u7406\u8f74\u5411\u5207\u7247\u4e09\u5143\u7ec4\uff0c\u4ee5\u63d0\u5347CT\u626b\u63cf\u591a\u6807\u7b7e\u5f02\u5e38\u5206\u7c7b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u968f\u7740CT\u626b\u63cf\u68c0\u67e5\u6570\u91cf\u7684\u589e\u52a0\uff0c\u653e\u5c04\u79d1\u533b\u751f\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\uff08\u5982\u5668\u5b98\u5206\u5272\u3001\u5f02\u5e38\u68c0\u6d4b\u3001\u62a5\u544a\u751f\u6210\uff09\u6765\u7ba1\u7406\u65e5\u76ca\u589e\u957f\u7684\u5de5\u4f5c\u91cf\u30023D CT\u626b\u63cf\u7684\u591a\u6807\u7b7e\u5206\u7c7b\u662f\u4e00\u9879\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u4f53\u6570\u636e\u5185\u90e8\u7a7a\u95f4\u5173\u7cfb\u590d\u6742\uff0c\u5f02\u5e38\u7c7b\u578b\u591a\u6837\u3002\u73b0\u67093D\u5377\u79ef\u7f51\u7edc\u5728\u5efa\u6a21\u957f\u8ddd\u79bb\u4f9d\u8d56\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u800cVision Transformers\u5219\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u901a\u5e38\u9700\u8981\u5927\u91cf\u540c\u9886\u57df\u6570\u636e\u96c6\u8fdb\u884c\u9884\u8bad\u7ec3\u624d\u80fd\u8fbe\u5230\u7ade\u4e89\u529b\u6027\u80fd\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u5373\u4e00\u79cd\u65b0\u7684\u56fe\u57fa\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5c06CT\u626b\u63cf\u5efa\u6a21\u4e3a\u7ed3\u6784\u5316\u56fe\uff0c\u5e76\u5229\u7528\u8f74\u5411\u5207\u7247\u4e09\u5143\u7ec4\u4f5c\u4e3a\u8282\u70b9\uff0c\u901a\u8fc7\u8c31\u57df\u5377\u79ef\u8fdb\u884c\u5904\u7406\uff0c\u4ee5\u589e\u5f3a\u591a\u6807\u7b7e\u5f02\u5e38\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u6807\u7b7e\u5f02\u5e38\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u548c\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u540c\u65f6\u5bf9Z\u8f74\u5e73\u79fb\u5177\u6709\u9c81\u68d2\u6027\u3002\u6d88\u878d\u7814\u7a76\u8bc4\u4f30\u4e86\u6bcf\u4e2a\u63d0\u51fa\u7ec4\u4ef6\u7684\u8d21\u732e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u56fe\u57fa\u65b9\u6cd5\u4e3a3D CT\u626b\u63cf\u7684\u591a\u6807\u7b7e\u5f02\u5e38\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u6027\u80fd\u3001\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.01109", "pdf": "https://arxiv.org/pdf/2508.01109", "abs": "https://arxiv.org/abs/2508.01109", "authors": ["Satiyabooshan Murugaboopathy", "Connor T. Jerzak", "Adel Daoud"], "title": "Platonic Representations for Poverty Mapping: Unified Vision-Language Codes or Agent-Induced Novelty?", "categories": ["cs.AI", "68T07", "I.2; J.4"], "comment": "7 figures", "summary": "We investigate whether socio-economic indicators like household wealth leave\nrecoverable imprints in satellite imagery (capturing physical features) and\nInternet-sourced text (reflecting historical/economic narratives). Using\nDemographic and Health Survey (DHS) data from African neighborhoods, we pair\nLandsat images with LLM-generated textual descriptions conditioned on\nlocation/year and text retrieved by an AI search agent from web sources. We\ndevelop a multimodal framework predicting household wealth (International\nWealth Index) through five pipelines: (i) vision model on satellite images,\n(ii) LLM using only location/year, (iii) AI agent searching/synthesizing web\ntext, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework\nyields three contributions. First, fusing vision and agent/LLM text outperforms\nvision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on\nout-of-sample splits), with LLM-internal knowledge proving more effective than\nagent-retrieved text, improving robustness to out-of-country and out-of-time\ngeneralization. Second, we find partial representational convergence: fused\nembeddings from vision/language modalities correlate moderately (median cosine\nsimilarity of 0.60 after alignment), suggesting a shared latent code of\nmaterial well-being while retaining complementary details, consistent with the\nPlatonic Representation Hypothesis. Although LLM-only text outperforms\nagent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest\ngains from combining agent data in some splits weakly support the notion that\nagent-gathered information introduces unique representational structures not\nfully captured by static LLM knowledge. Third, we release a large-scale\nmultimodal dataset comprising more than 60,000 DHS clusters linked to satellite\nimages, LLM-generated descriptions, and agent-retrieved texts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u7ed3\u5408\u536b\u661f\u56fe\u50cf\u548c\u4e92\u8054\u7f51\u6587\u672c\u6570\u636e\u9884\u6d4b\u975e\u6d32\u5bb6\u5ead\u8d22\u5bcc\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u878d\u5408\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u89c6\u89c9\u6a21\u578b\uff0c\u5c24\u5176LLM\u7684\u5185\u90e8\u77e5\u8bc6\u5728\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u63a2\u8ba8\u793e\u4f1a\u7ecf\u6d4e\u6307\u6807\uff08\u5982\u5bb6\u5ead\u8d22\u5bcc\uff09\u662f\u5426\u80fd\u5728\u536b\u661f\u56fe\u50cf\uff08\u7269\u7406\u7279\u5f81\uff09\u548c\u4e92\u8054\u7f51\u6587\u672c\uff08\u5386\u53f2/\u7ecf\u6d4e\u53d9\u4e8b\uff09\u4e2d\u7559\u4e0b\u53ef\u6062\u590d\u7684\u5370\u8bb0\uff0c\u4ece\u800c\u5b9e\u73b0\u57fa\u4e8e\u8fd9\u4e9b\u6570\u636e\u8fdb\u884c\u9884\u6d4b\u3002", "method": "\u4f7f\u7528\u975e\u6d32DHS\u6570\u636e\uff0c\u5c06Landsat\u536b\u661f\u56fe\u50cf\u4e0e\u57fa\u4e8e\u4f4d\u7f6e/\u5e74\u4efd\u751f\u6210\u7684LLM\u6587\u672c\u63cf\u8ff0\u4ee5\u53caAI\u4ee3\u7406\u68c0\u7d22\u7684\u7f51\u7edc\u6587\u672c\u914d\u5bf9\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u79cd\u7ba1\u7ebf\uff08\u7eaf\u89c6\u89c9\u3001\u7eafLLM\u3001\u7eafAI\u4ee3\u7406\u6587\u672c\u3001\u56fe\u50cf-\u6587\u672c\u8054\u5408\u7f16\u7801\u5668\u3001\u6240\u6709\u4fe1\u53f7\u96c6\u6210\uff09\u9884\u6d4b\u5bb6\u5ead\u8d22\u5bcc\u3002\u6b64\u5916\uff0c\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b60,000\u591a\u4e2aDHS\u96c6\u7fa4\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u878d\u5408\u89c6\u89c9\u548c\u4ee3\u7406/LLM\u6587\u672c\u5728\u8d22\u5bcc\u9884\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8e\u7eaf\u89c6\u89c9\u57fa\u7ebf\uff08R\u00b2\u4ece0.63\u63d0\u5347\u81f30.77\uff09\uff0c\u5176\u4e2dLLM\u7684\u5185\u90e8\u77e5\u8bc6\u6bd4\u4ee3\u7406\u68c0\u7d22\u6587\u672c\u66f4\u6709\u6548\uff0c\u5e76\u589e\u5f3a\u4e86\u8de8\u56fd\u5bb6\u548c\u65f6\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002\u89c6\u89c9\u4e0e\u8bed\u8a00\u6a21\u6001\u7684\u878d\u5408\u5d4c\u5165\u8868\u73b0\u51fa\u90e8\u5206\u8868\u793a\u6536\u655b\uff08\u4e2d\u4f4d\u6570\u4f59\u5f26\u76f8\u4f3c\u5ea60.60\uff09\uff0c\u8868\u660e\u5b58\u5728\u5171\u4eab\u7684\u6f5c\u5728\u8d22\u5bcc\u4ee3\u7801\u540c\u65f6\u4fdd\u7559\u4e92\u8865\u7ec6\u8282\u3002\u5c3d\u7ba1LLM\u6587\u672c\u5355\u72ec\u8868\u73b0\u4f18\u4e8e\u4ee3\u7406\u68c0\u7d22\u6570\u636e\uff0c\u4f46\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u7ed3\u5408\u4ee3\u7406\u6570\u636e\u4ecd\u6709\u5c0f\u5e45\u589e\u76ca\u3002", "conclusion": "\u7ed3\u5408\u536b\u661f\u56fe\u50cf\u548c\u4e92\u8054\u7f51\u6587\u672c\u6570\u636e\u80fd\u663e\u8457\u63d0\u9ad8\u5bb6\u5ead\u8d22\u5bcc\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662fLLM\u7684\u5185\u90e8\u77e5\u8bc6\u5728\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u4f5c\u7528\u3002\u8fd9\u8bc1\u660e\u4e86\u4e0d\u540c\u6a21\u6001\u4fe1\u606f\u5728\u6355\u6349\u548c\u9884\u6d4b\u793e\u4f1a\u7ecf\u6d4e\u6307\u6807\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u53ca\u4e92\u8865\u6027\u3002"}}
{"id": "2508.01317", "pdf": "https://arxiv.org/pdf/2508.01317", "abs": "https://arxiv.org/abs/2508.01317", "authors": ["Xuemiao Zhang", "Can Ren", "Chengying Tu", "Rongxiang Weng", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "title": "LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points", "categories": ["cs.CL"], "comment": null, "summary": "The advancement of large language models (LLMs) struggles with the scarcity\nof high-quality, diverse training data. To address this limitation, we propose\nLinkSyn, a novel knowledge point (KP) graph-based synthesis framework that\nenables flexible control over discipline and difficulty distributions while\nbalancing KP coverage and popularity. LinkSyn extracts KPs from\nquestion-answering (QA) seed data and constructs a KP graph to synthesize\ndiverse QA data from multiple seeds strongly linked by KPs and sampled from\ngraph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution\nvalue function to guide the adjustment of path sampling probability and balance\nKP coverage and popularity during graph walks; (2) diffusion-based synthesis\nvia DeepSeek-R1 by leveraging multiple seeds with dense logical associations\nalong each path; and (3) high-difficulty QA enhancement within given\ndisciplines by flexible difficulty adjustments. By executing LinkSyn, we\nsynthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens.\nExtensive experiments on Llama-3 8B demonstrate that continual pre-training\nwith LinkQA yields an average improvement of $\\mathbf{11.51\\%}$ on MMLU and\nCMMLU, establishing new SOTA results. LinkQA consistently enhances performance\nacross model size and initial FLOPs scales.", "AI": {"tldr": "LinkSyn\u662f\u4e00\u4e2a\u57fa\u4e8e\u77e5\u8bc6\u70b9\u56fe\u7684\u5408\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u7684LLM\u8bad\u7ec3\u6570\u636eLinkQA\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728MMLU\u548cCMMLU\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u6b65\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u7684\u7a00\u7f3a\u6027\u3002", "method": "\u63d0\u51faLinkSyn\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4ece\u95ee\u7b54\uff08QA\uff09\u79cd\u5b50\u6570\u636e\u4e2d\u63d0\u53d6\u77e5\u8bc6\u70b9\u5e76\u6784\u5efa\u77e5\u8bc6\u70b9\u56fe\u3002\u901a\u8fc7\u56fe\u6e38\u8d70\uff08graph walks\uff09\u91c7\u6837\u5e76\u7ed3\u5408\u591a\u91cd\u5f3a\u5173\u8054\u79cd\u5b50\uff0c\u5408\u6210\u591a\u6837\u5316\u7684QA\u6570\u636e\u3002LinkSyn\u5305\u542b\uff1a1) \u77e5\u8bc6\u5206\u5e03\u503c\u51fd\u6570\uff0c\u7528\u4e8e\u6307\u5bfc\u8def\u5f84\u91c7\u6837\uff0c\u5e73\u8861\u77e5\u8bc6\u70b9\u8986\u76d6\u548c\u6d41\u884c\u5ea6\uff1b2) \u57fa\u4e8eDeepSeek-R1\u7684\u6269\u6563\u5f0f\u5408\u6210\uff0c\u5229\u7528\u5177\u6709\u7d27\u5bc6\u903b\u8f91\u5173\u8054\u7684\u591a\u91cd\u79cd\u5b50\uff1b3) \u7075\u6d3b\u8c03\u6574\u96be\u5ea6\uff0c\u589e\u5f3a\u7279\u5b9a\u5b66\u79d1\u7684\u9ad8\u96be\u5ea6QA\u3002\u6700\u7ec8\u5408\u6210\u4e86500\u4ebftoken\u7684\u591a\u5b66\u79d1QA\u6570\u636e\u96c6LinkQA\u3002", "result": "\u901a\u8fc7LinkQA\u5bf9Llama-3 8B\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u5728MMLU\u548cCMMLU\u4e0a\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e8611.51%\uff0c\u53d6\u5f97\u4e86\u65b0\u7684SOTA\u7ed3\u679c\u3002LinkQA\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u521d\u59cbFLOPs\u4e0b\u5747\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "LinkSyn\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u4e14\u53ef\u63a7\u7684LinkQA\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.01863", "pdf": "https://arxiv.org/pdf/2508.01863", "abs": "https://arxiv.org/abs/2508.01863", "authors": ["Sanjay Singh", "Mitendra Mahto"], "title": "Hard-Earned Lessons in Access Control at Scale: Enforcing Identity and Policy Across Trust Boundaries with Reverse Proxies and mTLS", "categories": ["cs.CR", "cs.NI", "cs.SE"], "comment": "6 pages, 3 figures", "summary": "In today's enterprise environment, traditional access methods such as Virtual\nPrivate Networks (VPNs) and application-specific Single Sign-On (SSO) often\nfall short when it comes to securely scaling access for a distributed and\ndynamic workforce. This paper presents our experience implementing a modern,\nZero Trust-aligned architecture that leverages a reverse proxy integrated with\nMutual TLS (mTLS) and centralized SSO, along with the key challenges we\nencountered and lessons learned during its deployment and scaling. This\nmultidimensional solution involves both per-device and per-user authentication,\ncentralized enforcement of security policies, and comprehensive observability,\nhence enabling organizations to deliver secure and seamless access to their\ninternal applications.", "AI": {"tldr": "\u672c\u6587\u5206\u4eab\u4e86\u5b9e\u65bd\u4e00\u5957\u57fa\u4e8e\u96f6\u4fe1\u4efb\u3001\u53cd\u5411\u4ee3\u7406\u3001mTLS\u548c\u96c6\u4e2d\u5f0fSSO\u7684\u4f01\u4e1a\u5185\u90e8\u5e94\u7528\u5b89\u5168\u8bbf\u95ee\u67b6\u6784\u7684\u7ecf\u9a8c\u3002", "motivation": "\u5728\u5f53\u524d\u4f01\u4e1a\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u7684VPN\u548c\u5e94\u7528\u7279\u5b9aSSO\u7b49\u8bbf\u95ee\u65b9\u6cd5\u96be\u4ee5\u5b89\u5168\u5730\u6ee1\u8db3\u5206\u5e03\u5f0f\u548c\u52a8\u6001\u5458\u5de5\u7684\u53ef\u6269\u5c55\u8bbf\u95ee\u9700\u6c42\u3002", "method": "\u5b9e\u65bd\u4e86\u4e00\u79cd\u7b26\u5408\u96f6\u4fe1\u4efb\u539f\u5219\u7684\u73b0\u4ee3\u5316\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5229\u7528\u96c6\u6210\u4e86\u53cc\u5411TLS (mTLS) \u548c\u96c6\u4e2d\u5f0fSSO\u7684\u53cd\u5411\u4ee3\u7406\u3002\u8be5\u65b9\u6848\u5305\u62ec\u8bbe\u5907\u7ea7\u548c\u7528\u6237\u7ea7\u8ba4\u8bc1\u3001\u96c6\u4e2d\u5f0f\u5b89\u5168\u7b56\u7565\u5f3a\u5236\u6267\u884c\u4ee5\u53ca\u5168\u9762\u7684\u53ef\u89c2\u5bdf\u6027\u3002", "result": "\u6210\u529f\u5b9e\u65bd\u4e86\u8be5\u591a\u7ef4\u5ea6\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u603b\u7ed3\u4e86\u90e8\u7f72\u548c\u6269\u5c55\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u5173\u952e\u6311\u6218\u4e0e\u83b7\u5f97\u7684\u7ecf\u9a8c\u6559\u8bad\u3002\u8be5\u65b9\u6848\u80fd\u4e3a\u7ec4\u7ec7\u5185\u90e8\u5e94\u7528\u63d0\u4f9b\u5b89\u5168\u65e0\u7f1d\u7684\u8bbf\u95ee\u3002", "conclusion": "\u57fa\u4e8e\u53cd\u5411\u4ee3\u7406\u3001mTLS\u548c\u96c6\u4e2d\u5f0fSSO\u7684\u96f6\u4fe1\u4efb\u67b6\u6784\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u8bbf\u95ee\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u4e3a\u4f01\u4e1a\u63d0\u4f9b\u5b89\u5168\u4e14\u53ef\u6269\u5c55\u7684\u5185\u90e8\u5e94\u7528\u8bbf\u95ee\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u5b9e\u8df5\u7ecf\u9a8c\u3002"}}
{"id": "2508.00897", "pdf": "https://arxiv.org/pdf/2508.00897", "abs": "https://arxiv.org/abs/2508.00897", "authors": ["Julien Simon de Kergunic", "Rony Abecidan", "Patrick Bas", "Vincent Itier"], "title": "Maximize margins for robust splicing detection", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "in French language. GRETSI 2025 - Colloque Francophone de Traitement\n  du Signal et des Images, https://gretsi.fr/colloque2025/, Aug 2025,\n  Strasbourg, France", "summary": "Despite recent progress in splicing detection, deep learning-based forensic\ntools remain difficult to deploy in practice due to their high sensitivity to\ntraining conditions. Even mild post-processing applied to evaluation images can\nsignificantly degrade detector performance, raising concerns about their\nreliability in operational contexts. In this work, we show that the same deep\narchitecture can react very differently to unseen post-processing depending on\nthe learned weights, despite achieving similar accuracy on in-distribution test\ndata. This variability stems from differences in the latent spaces induced by\ntraining, which affect how samples are separated internally. Our experiments\nreveal a strong correlation between the distribution of latent margins and a\ndetector's ability to generalize to post-processed images. Based on this\nobservation, we propose a practical strategy for building more robust\ndetectors: train several variants of the same model under different conditions,\nand select the one that maximizes latent margins.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u62fc\u63a5\u68c0\u6d4b\u5de5\u5177\u6613\u53d7\u540e\u5904\u7406\u5f71\u54cd\uff0c\u672c\u6587\u53d1\u73b0\u6f5c\u5728\u7a7a\u95f4\u8fb9\u8ddd\u4e0e\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5f3a\u76f8\u5173\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u4f18\u5316\u6f5c\u5728\u8fb9\u8ddd\u6765\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u68c0\u6d4b\u5668\u3002", "motivation": "\u5c3d\u7ba1\u62fc\u63a5\u68c0\u6d4b\u6280\u672f\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u53d6\u8bc1\u5de5\u5177\u56e0\u5bf9\u8bad\u7ec3\u6761\u4ef6\u9ad8\u5ea6\u654f\u611f\u800c\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\uff0c\u5373\u4f7f\u8f7b\u5fae\u7684\u540e\u5904\u7406\u4e5f\u4f1a\u663e\u8457\u964d\u4f4e\u5176\u6027\u80fd\uff0c\u5f15\u53d1\u53ef\u9760\u6027\u62c5\u5fe7\u3002", "method": "\u57fa\u4e8e\u6f5c\u5728\u8fb9\u8ddd\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u76f8\u5173\u6027\uff0c\u63d0\u51fa\u4e00\u79cd\u5b9e\u7528\u7b56\u7565\uff1a\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u8bad\u7ec3\u540c\u4e00\u6a21\u578b\u7684\u591a\u4e2a\u53d8\u4f53\uff0c\u5e76\u9009\u62e9\u90a3\u4e9b\u80fd\u6700\u5927\u5316\u6f5c\u5728\u8fb9\u8ddd\u7684\u6a21\u578b\uff0c\u4ee5\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u68c0\u6d4b\u5668\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u540c\u4e00\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5bf9\u672a\u89c1\u540e\u5904\u7406\u7684\u53cd\u5e94\uff0c\u5373\u4f7f\u5728\u5206\u5e03\u5185\u6d4b\u8bd5\u6570\u636e\u4e0a\u8868\u73b0\u76f8\u4f3c\uff0c\u4e5f\u4f1a\u56e0\u5b66\u4e60\u6743\u91cd\u800c\u5f02\uff0c\u8fd9\u6e90\u4e8e\u8bad\u7ec3\u5f15\u8d77\u7684\u6f5c\u5728\u7a7a\u95f4\u5dee\u5f02\u3002\u5b9e\u9a8c\u63ed\u793a\uff0c\u6f5c\u5728\u8fb9\u8ddd\u7684\u5206\u5e03\u4e0e\u68c0\u6d4b\u5668\u5bf9\u540e\u5904\u7406\u56fe\u50cf\u7684\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u5f3a\u76f8\u5173\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u548c\u6700\u5927\u5316\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u8fb9\u8ddd\uff0c\u53ef\u4ee5\u6709\u6548\u6784\u5efa\u5bf9\u56fe\u50cf\u540e\u5904\u7406\u5177\u6709\u66f4\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u7684\u6df1\u5ea6\u5b66\u4e60\u62fc\u63a5\u68c0\u6d4b\u5668\u3002"}}
{"id": "2508.01058", "pdf": "https://arxiv.org/pdf/2508.01058", "abs": "https://arxiv.org/abs/2508.01058", "authors": ["Sara Yavari", "Rahul Nitin Pandya", "Jacob Furst"], "title": "ReCoSeg++:Extended Residual-Guided Cross-Modal Diffusion for Brain Tumor Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate segmentation of brain tumors in MRI scans is critical for clinical\ndiagnosis and treatment planning. We propose a semi-supervised, two-stage\nframework that extends the ReCoSeg approach to the larger and more\nheterogeneous BraTS 2021 dataset, while eliminating the need for ground-truth\nmasks for the segmentation objective. In the first stage, a residual-guided\ndenoising diffusion probabilistic model (DDPM) performs cross-modal synthesis\nby reconstructing the T1ce modality from FLAIR, T1, and T2 scans. The residual\nmaps, capturing differences between predicted and actual T1ce images, serve as\nspatial priors to enhance downstream segmentation. In the second stage, a\nlightweight U-Net takes as input the concatenation of residual maps, computed\nas the difference between real T1ce and synthesized T1ce, with T1, T2, and\nFLAIR modalities to improve whole tumor segmentation. To address the increased\nscale and variability of BraTS 2021, we apply slice-level filtering to exclude\nnon-informative samples and optimize thresholding strategies to balance\nprecision and recall. Our method achieves a Dice score of $93.02\\%$ and an IoU\nof $86.7\\%$ for whole tumor segmentation on the BraTS 2021 dataset,\noutperforming the ReCoSeg baseline on BraTS 2020 (Dice: $91.7\\%$, IoU:\n$85.3\\%$), and demonstrating improved accuracy and scalability for real-world,\nmulti-center MRI datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u534a\u76d1\u7763\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408DDPM\u8fdb\u884c\u8de8\u6a21\u6001\u5408\u6210\u548cU-Net\u8fdb\u884c\u5168\u80bf\u7624\u5206\u5272\uff0c\u65e0\u9700\u771f\u5b9e\u5206\u5272\u63a9\u7801\u3002\u5728BraTS 2021\u4e0a\u8868\u73b0\u51fa\u8272\uff0cDice\u8fbe93.02%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "MRI\u8111\u80bf\u7624\u7684\u7cbe\u786e\u5206\u5272\u5bf9\u4e34\u5e8a\u8bca\u65ad\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5e38\u9700\u771f\u5b9e\u63a9\u7801\uff0c\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u66f4\u5927\u3001\u66f4\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u5982BraTS 2021\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6269\u5c55ReCoSeg\u7684\u534a\u76d1\u7763\u4e24\u9636\u6bb5\u6846\u67b6\u3002\u7b2c\u4e00\u9636\u6bb5\uff0c\u4f7f\u7528\u6b8b\u5dee\u5f15\u5bfcDDPM\u4eceFLAIR\u3001T1\u3001T2\u6a21\u6001\u5408\u6210T1ce\u5e76\u751f\u6210\u6b8b\u5dee\u56fe\u4f5c\u4e3a\u7a7a\u95f4\u5148\u9a8c\u3002\u7b2c\u4e8c\u9636\u6bb5\uff0c\u8f7b\u91cf\u7ea7U-Net\u7ed3\u5408\u6b8b\u5dee\u56fe\u548c\u539f\u59cb\u6a21\u6001\u8fdb\u884c\u5168\u80bf\u7624\u5206\u5272\u3002\u4e3a\u5e94\u5bf9BraTS 2021\u7684\u89c4\u6a21\u548c\u53d8\u5f02\u6027\uff0c\u8fd8\u5e94\u7528\u4e86\u5207\u7247\u7ea7\u8fc7\u6ee4\u548c\u4f18\u5316\u9608\u503c\u7b56\u7565\u3002", "result": "\u8be5\u65b9\u6cd5\u5728BraTS 2021\u6570\u636e\u96c6\u4e0a\u7684\u5168\u80bf\u7624\u5206\u5272Dice\u5f97\u5206\u8fbe\u523093.02%\uff0cIoU\u8fbe\u523086.7%\u3002\u8fd9\u4f18\u4e8eBraTS 2020\u4e0a\u7684ReCoSeg\u57fa\u7ebf\uff08Dice: 91.7%\uff0cIoU: 85.3%\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u6240\u63d0\u65b9\u6cd5\u5728\u8111\u80bf\u7624\u5206\u5272\u65b9\u9762\u7684\u66f4\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u771f\u5b9e\u7684\u3001\u591a\u4e2d\u5fc3MRI\u6570\u636e\u96c6\u3002"}}
{"id": "2508.01158", "pdf": "https://arxiv.org/pdf/2508.01158", "abs": "https://arxiv.org/abs/2508.01158", "authors": ["Yunlong Lin", "Zirui Li", "Guodong Du", "Xiaocong Zhao", "Cheng Gong", "Xinwei Wang", "Chao Lu", "Jianwei Gong"], "title": "H2C: Hippocampal Circuit-inspired Continual Learning for Lifelong Trajectory Prediction in Autonomous Driving", "categories": ["cs.AI"], "comment": "Open source code: https://github.com/BIT-Jack/H2C-lifelong", "summary": "Deep learning (DL) has shown state-of-the-art performance in trajectory\nprediction, which is critical to safe navigation in autonomous driving (AD).\nHowever, most DL-based methods suffer from catastrophic forgetting, where\nadapting to a new distribution may cause significant performance degradation in\npreviously learned ones. Such inability to retain learned knowledge limits\ntheir applicability in the real world, where AD systems need to operate across\nvarying scenarios with dynamic distributions. As revealed by neuroscience, the\nhippocampal circuit plays a crucial role in memory replay, effectively\nreconstructing learned knowledge based on limited resources. Inspired by this,\nwe propose a hippocampal circuit-inspired continual learning method (H2C) for\ntrajectory prediction across varying scenarios. H2C retains prior knowledge by\nselectively recalling a small subset of learned samples. First, two\ncomplementary strategies are developed to select the subset to represent\nlearned knowledge. Specifically, one strategy maximizes inter-sample diversity\nto represent the distinctive knowledge, and the other estimates the overall\nknowledge by equiprobable sampling. Then, H2C updates via a memory replay loss\nfunction calculated by these selected samples to retain knowledge while\nlearning new data. Experiments based on various scenarios from the INTERACTION\ndataset are designed to evaluate H2C. Experimental results show that H2C\nreduces catastrophic forgetting of DL baselines by 22.71% on average in a\ntask-free manner, without relying on manually informed distributional shifts.\nThe implementation is available at https://github.com/BIT-Jack/H2C-lifelong.", "AI": {"tldr": "\u53d7\u6d77\u9a6c\u4f53\u542f\u53d1\uff0c\u672c\u7814\u7a76\u63d0\u51faH2C\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6837\u672c\u56de\u653e\uff0c\u6709\u6548\u7f13\u89e3\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u9884\u6d4b\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff1a\u9002\u5e94\u65b0\u6570\u636e\u4f1a\u5bfc\u81f4\u65e7\u77e5\u8bc6\u663e\u8457\u9000\u5316\uff0c\u9650\u5236\u5176\u5728\u52a8\u6001\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u4e3a\u4e86\u4f7f\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u80fd\u9002\u5e94\u4e0d\u540c\u573a\u666f\u548c\u52a8\u6001\u5206\u5e03\uff0c\u9700\u8981\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u53d7\u795e\u7ecf\u79d1\u5b66\u4e2d\u6d77\u9a6c\u4f53\u8bb0\u5fc6\u56de\u653e\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u6d77\u9a6c\u4f53\u7535\u8def\u542f\u53d1\u5f0f\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff08H2C\uff09\u3002H2C\u901a\u8fc7\u9009\u62e9\u6027\u56de\u653e\u4e00\u5c0f\u90e8\u5206\u5df2\u5b66\u4e60\u6837\u672c\u6765\u4fdd\u7559\u5148\u9a8c\u77e5\u8bc6\u3002\u5177\u4f53\u5730\uff0c\u5f00\u53d1\u4e86\u4e24\u79cd\u4e92\u8865\u7b56\u7565\u9009\u62e9\u6837\u672c\u5b50\u96c6\uff1a\u4e00\u79cd\u6700\u5927\u5316\u6837\u672c\u95f4\u591a\u6837\u6027\u4ee5\u4ee3\u8868\u72ec\u7279\u77e5\u8bc6\uff0c\u53e6\u4e00\u79cd\u901a\u8fc7\u7b49\u6982\u7387\u91c7\u6837\u4f30\u8ba1\u6574\u4f53\u77e5\u8bc6\u3002\u7136\u540e\uff0cH2C\u5229\u7528\u8fd9\u4e9b\u9009\u5b9a\u6837\u672c\u8ba1\u7b97\u8bb0\u5fc6\u56de\u653e\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u5728\u5b66\u4e60\u65b0\u6570\u636e\u65f6\u4fdd\u7559\u65e7\u77e5\u8bc6\u3002", "result": "\u5728INTERACTION\u6570\u636e\u96c6\u7684\u4e0d\u540c\u573a\u666f\u4e0b\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eH2C\u5e73\u5747\u5c06\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u7684\u707e\u96be\u6027\u9057\u5fd8\u964d\u4f4e\u4e8622.71%\uff0c\u4e14\u4ee5\u4efb\u52a1\u65e0\u5173\u7684\u65b9\u5f0f\u5b9e\u73b0\uff0c\u4e0d\u4f9d\u8d56\u624b\u52a8\u544a\u77e5\u7684\u5206\u5e03\u53d8\u5316\u3002", "conclusion": "H2C\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u8f68\u8ff9\u9884\u6d4b\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u52a8\u6001\u591a\u53d8\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e0b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f7f\u5176\u66f4\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u3002"}}
{"id": "2508.01326", "pdf": "https://arxiv.org/pdf/2508.01326", "abs": "https://arxiv.org/abs/2508.01326", "authors": ["Xuemiao Zhang", "Chengying Tu", "Can Ren", "Rongxiang Weng", "Hongfei Yan", "Jingang Wang", "Xunliang Cai"], "title": "Large-Scale Diverse Synthesis for Mid-Training", "categories": ["cs.CL"], "comment": null, "summary": "The scarcity of high-quality, knowledge-intensive training data hinders the\ndevelopment of large language models (LLMs), as traditional corpora provide\nlimited information. Previous studies have synthesized and integrated\ncorpora-dependent question-answering (QA) data to improve model performance but\nface challenges in QA data scalability and knowledge diversity, particularly in\ncross-domain contexts. Furthermore, leveraging our designed discipline and\ndifficulty annotation system, we probe model deficiencies in STEM disciplines\nand high-difficulty data. To overcome these limitations, we propose a novel\ndiversified pipeline to synthesize BoostQA, a 100B-token large-scale QA\ndataset. Our synthesis framework: (1) curates seed data from heterogeneous\nsources; (2) utilizes DeepSeek-R1 to implement STEM-focused multi-grade\nsynthesis to boost data diversity and high-difficulty synthesis to mitigate\ndifficulty degradation; (3) refines answers via DeepSeek-V3 to improve output\nquality. We utilize BoostQA in mid-training, a mid-stage between pre-training\nand post-training, to optimize domain-specific knowledge acquisition and\nenhance data quality. Our method enables Llama-3 8B, mid-trained on a 40B-token\ndataset, to achieve an average improvement of $\\mathbf{12.74\\%}$ on MMLU and\nCMMLU and establish SOTA average performance across 12 benchmarks. BoostQA also\ndemonstrates robust scalability, with performance consistently improving as\nmodel size, data volume, and initial FLOPs scale.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u6784\u5efa\u4e86BoostQA\uff0c\u4e00\u4e2a\u5343\u4ebf\u7ea7\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u7684\u95ee\u7b54\u6570\u636e\u96c6\u3002\u901a\u8fc7\u65b0\u9896\u7684\u5408\u6210\u7ba1\u7ebf\u548c\u5728\u4e2d\u671f\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u8d28\u91cf\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u9ad8\u8d28\u91cf\u3001\u77e5\u8bc6\u5bc6\u96c6\u578b\u7684\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\uff0c\u4f20\u7edf\u8bed\u6599\u5e93\u4fe1\u606f\u6709\u9650\u3002\u73b0\u6709\u95ee\u7b54\u6570\u636e\u5408\u6210\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u77e5\u8bc6\u591a\u6837\u6027\u65b9\u9762\uff08\u5c24\u5176\u8de8\u9886\u57df\uff09\u9762\u4e34\u6311\u6218\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5728STEM\u5b66\u79d1\u548c\u9ad8\u96be\u5ea6\u6570\u636e\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6837\u5316\u5408\u6210\u7ba1\u7ebf\u6765\u6784\u5efaBoostQA\uff08\u4e00\u4e2a1000\u4ebftoken\u7684\u5927\u89c4\u6a21\u95ee\u7b54\u6570\u636e\u96c6\uff09\u3002\u8be5\u6846\u67b6\u5305\u62ec\uff1a1) \u4ece\u5f02\u6784\u6e90\u6574\u7406\u79cd\u5b50\u6570\u636e\uff1b2) \u5229\u7528DeepSeek-R1\u8fdb\u884cSTEM\u9886\u57df\u591a\u96be\u5ea6\u5408\u6210\u4ee5\u589e\u5f3a\u6570\u636e\u591a\u6837\u6027\uff0c\u5e76\u8fdb\u884c\u9ad8\u96be\u5ea6\u5408\u6210\u4ee5\u51cf\u8f7b\u96be\u5ea6\u9000\u5316\uff1b3) \u901a\u8fc7DeepSeek-V3\u7cbe\u70bc\u7b54\u6848\u4ee5\u63d0\u9ad8\u8f93\u51fa\u8d28\u91cf\u3002BoostQA\u5e94\u7528\u4e8e\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\u4e4b\u95f4\u7684\u4e2d\u671f\u8bad\u7ec3\u9636\u6bb5\uff0c\u4ee5\u4f18\u5316\u9886\u57df\u77e5\u8bc6\u83b7\u53d6\u548c\u6570\u636e\u8d28\u91cf\u3002", "result": "\u901a\u8fc7\u5728400\u4ebftoken\u7684BoostQA\u6570\u636e\u96c6\u4e0a\u5bf9Llama-3 8B\u8fdb\u884c\u4e2d\u671f\u8bad\u7ec3\uff0c\u6a21\u578b\u5728MMLU\u548cCMMLU\u4e0a\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e8612.74%\uff0c\u5e76\u572812\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86SOTA\u5e73\u5747\u6027\u80fd\u3002BoostQA\u8fd8\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\uff0c\u6027\u80fd\u968f\u7740\u6a21\u578b\u5927\u5c0f\u3001\u6570\u636e\u91cf\u548c\u521d\u59cbFLOPs\u7684\u6269\u5c55\u800c\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "BoostQA\u6570\u636e\u96c6\u53ca\u5176\u5728\u4e2d\u671f\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u548c\u591a\u6837\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728STEM\u548c\u9ad8\u96be\u5ea6\u9886\u57df\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u6a21\u578b\u6269\u5c55\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.02209", "pdf": "https://arxiv.org/pdf/2508.02209", "abs": "https://arxiv.org/abs/2508.02209", "authors": ["Yigit Turkmen", "Baturalp Buyukates", "Melih Bastopcu"], "title": "Balancing Information Accuracy and Response Timeliness in Networked LLMs", "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.NI", "math.IT"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have transformed many\nfields including scientific discovery, content generation, biomedical text\nmining, and educational technology. However, the substantial requirements for\ntraining data, computational resources, and energy consumption pose significant\nchallenges for their practical deployment. A promising alternative is to\nleverage smaller, specialized language models and aggregate their outputs to\nimprove overall response quality. In this work, we investigate a networked LLM\nsystem composed of multiple users, a central task processor, and clusters of\ntopic-specialized LLMs. Each user submits categorical binary (true/false)\nqueries, which are routed by the task processor to a selected cluster of $m$\nLLMs. After gathering individual responses, the processor returns a final\naggregated answer to the user. We characterize both the information accuracy\nand response timeliness in this setting, and formulate a joint optimization\nproblem to balance these two competing objectives. Our extensive simulations\ndemonstrate that the aggregated responses consistently achieve higher accuracy\nthan those of individual LLMs. Notably, this improvement is more significant\nwhen the participating LLMs exhibit similar standalone performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u4e00\u4e2a\u7f51\u7edc\u5316LLM\u7cfb\u7edf\uff0c\u901a\u8fc7\u805a\u5408\u5c0f\u578b\u3001\u4e13\u4e1a\u5316LLM\u7684\u54cd\u5e94\u6765\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4ee5\u5e94\u5bf9\u5927\u578bLLM\u90e8\u7f72\u7684\u6311\u6218\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u5bf9\u8bad\u7ec3\u6570\u636e\u3001\u8ba1\u7b97\u8d44\u6e90\u548c\u80fd\u6e90\u7684\u5de8\u5927\u9700\u6c42\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u7814\u7a76\u65e8\u5728\u5bfb\u627e\u4e00\u79cd\u66f4\u53ef\u884c\u3001\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u7f51\u7edc\u5316LLM\u7cfb\u7edf\uff0c\u5305\u542b\u7528\u6237\u3001\u4e2d\u592e\u4efb\u52a1\u5904\u7406\u5668\u548c\u591a\u4e2a\u4e3b\u9898\u4e13\u4e1a\u5316LLM\u96c6\u7fa4\u3002\u7528\u6237\u63d0\u4ea4\u4e8c\u5143\u67e5\u8be2\uff0c\u7531\u5904\u7406\u5668\u8def\u7531\u81f3\u9009\u5b9a\u7684LLM\u96c6\u7fa4\uff0c\u6536\u96c6\u5404\u6a21\u578b\u54cd\u5e94\u540e\u805a\u5408\u4e3a\u6700\u7ec8\u7b54\u6848\u3002\u7814\u7a76\u91cf\u5316\u4e86\u4fe1\u606f\u51c6\u786e\u6027\u548c\u54cd\u5e94\u53ca\u65f6\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u8054\u5408\u4f18\u5316\u95ee\u9898\u6765\u5e73\u8861\u8fd9\u4e24\u4e2a\u76ee\u6807\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u805a\u5408\u54cd\u5e94\u7684\u51c6\u786e\u6027\u59cb\u7ec8\u9ad8\u4e8e\u5355\u4e2aLLM\u7684\u54cd\u5e94\u3002\u6b64\u5916\uff0c\u5f53\u53c2\u4e0e\u7684LLM\u5177\u6709\u76f8\u4f3c\u7684\u72ec\u7acb\u6027\u80fd\u65f6\uff0c\u8fd9\u79cd\u51c6\u786e\u6027\u63d0\u5347\u66f4\u4e3a\u663e\u8457\u3002", "conclusion": "\u901a\u8fc7\u805a\u5408\u5c0f\u578b\u3001\u4e13\u4e1a\u5316LLM\u7684\u54cd\u5e94\uff0c\u7f51\u7edc\u5316LLM\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u6a21\u578b\u6027\u80fd\u76f8\u4f3c\u7684\u60c5\u51b5\u4e0b\u3002\u8fd9\u4e3a\u89e3\u51b3\u5927\u578bLLM\u90e8\u7f72\u6311\u6218\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00901", "pdf": "https://arxiv.org/pdf/2508.00901", "abs": "https://arxiv.org/abs/2508.00901", "authors": ["Ruichen Xu", "Kexin Chen"], "title": "Filtering with Self-Attention and Storing with MLP: One-Layer Transformers Can Provably Acquire and Extract Knowledge", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Modern large language models excel in knowledge-intensive tasks, yet how\ntransformers acquire (store) knowledge during pre-training and extract\n(retrieve) it during post-fine-tuning inference remains theoretically opaque.\nWhile prior theoretical work has begun to investigate these questions through\nthe analysis of training dynamics, such studies are limited to single-layer,\nattention-only architectures. However, most existing studies suggest that MLPs\nare the most contributing components for storing knowledge in transformer-based\nlanguage models. Meanwhile, our empirical investigations reveal that such\nsimplified models, when trained using standard next-token prediction\nobjectives, may be incapable of acquiring or extracting factual knowledge. To\novercome this limitation, we introduce a tractable one-layer transformer\nframework that crucially incorporates both self-attention and MLP modules. By\ntracking its gradient dynamics, we establish convergence and generalization\nguarantees that illuminate the ability of knowledge acquisition and extraction.\nWe prove that 1) Transformers can achieve near-optimal training loss during\npre-training, signifying effective knowledge acquisition; 2) With a large\nfine-tuning dataset and specific data multiplicity conditions met, transformers\ncan achieve low generalization error when tested on factual knowledge learned\nduring pre-training but not reinforced during the fine-tuning, indicating\nsuccessful knowledge extraction; 3) When the conditions are not satisfied,\ntransformers exhibit high generalization loss, resulting in hallucinations. Our\nanalysis includes both full fine-tuning and low-rank fine-tuning. Furthermore,\nour analysis offers theoretical insights into several pertinent empirical\nphenomena, such as the role of learning rate schedules. Experiments on\nsynthetic and real-world PopQA datasets with GPT-2 and Llama-3.2-1B validate\nour results.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542bMLP\u7684\u5355\u5c42Transformer\u7406\u8bba\u6846\u67b6\uff0c\u9610\u660e\u4e86Transformer\u5728\u9884\u8bad\u7ec3\u4e2d\u83b7\u53d6\u77e5\u8bc6\u4ee5\u53ca\u5728\u5fae\u8c03\u540e\u63d0\u53d6\u77e5\u8bc6\u7684\u673a\u5236\uff0c\u5e76\u89e3\u91ca\u4e86\u5e7b\u89c9\u73b0\u8c61\u3002", "motivation": "\u73b0\u6709\u7406\u8bba\u7814\u7a76\u5bf9Transformer\u6a21\u578b\u5982\u4f55\u83b7\u53d6\u548c\u63d0\u53d6\u77e5\u8bc6\u7684\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u4e14\u5927\u591a\u4ec5\u9650\u4e8e\u5355\u5c42\u3001\u7eaf\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u65e0\u6cd5\u89e3\u91caMLP\u5728\u77e5\u8bc6\u5b58\u50a8\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002\u540c\u65f6\uff0c\u7ecf\u9a8c\u53d1\u73b0\u8fd9\u4e9b\u7b80\u5316\u6a21\u578b\u5728\u6807\u51c6\u8bad\u7ec3\u4e0b\u65e0\u6cd5\u6709\u6548\u83b7\u53d6\u6216\u63d0\u53d6\u4e8b\u5b9e\u77e5\u8bc6\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u5206\u6790\u7684\u5355\u5c42Transformer\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5173\u952e\u6027\u5730\u7ed3\u5408\u4e86\u81ea\u6ce8\u610f\u529b\u4e0eMLP\u6a21\u5757\u3002\u901a\u8fc7\u8ddf\u8e2a\u5176\u68af\u5ea6\u52a8\u529b\u5b66\uff0c\u5efa\u7acb\u4e86\u6536\u655b\u6027\u548c\u6cdb\u5316\u6027\u4fdd\u8bc1\uff0c\u4ee5\u9610\u660e\u77e5\u8bc6\u83b7\u53d6\u548c\u63d0\u53d6\u7684\u80fd\u529b\u3002\u7814\u7a76\u6db5\u76d6\u4e86\u5168\u5fae\u8c03\u548c\u4f4e\u79e9\u5fae\u8c03\u3002\u901a\u8fc7\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u7684PopQA\u6570\u636e\u96c6\u4e0a\u5bf9GPT-2\u548cLlama-3.2-1B\u8fdb\u884c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "result": "1) Transformer\u5728\u9884\u8bad\u7ec3\u4e2d\u80fd\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u8bad\u7ec3\u635f\u5931\uff0c\u8868\u660e\u6709\u6548\u7684\u77e5\u8bc6\u83b7\u53d6\u80fd\u529b\uff1b2) \u5728\u5927\u578b\u5fae\u8c03\u6570\u636e\u96c6\u548c\u6ee1\u8db3\u7279\u5b9a\u6570\u636e\u591a\u91cd\u6027\u6761\u4ef6\u65f6\uff0cTransformer\u80fd\u6210\u529f\u63d0\u53d6\u77e5\u8bc6\uff0c\u6cdb\u5316\u8bef\u5dee\u4f4e\uff1b3) \u5f53\u6761\u4ef6\u4e0d\u6ee1\u8db3\u65f6\uff0cTransformer\u8868\u73b0\u51fa\u9ad8\u6cdb\u5316\u635f\u5931\uff0c\u5bfc\u81f4\u5e7b\u89c9\u3002\u5206\u6790\u8fd8\u4e3a\u5b66\u4e60\u7387\u8c03\u5ea6\u7b49\u76f8\u5173\u7ecf\u9a8c\u73b0\u8c61\u63d0\u4f9b\u4e86\u7406\u8bba\u89c1\u89e3\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u9996\u6b21\u4e3a\u5305\u542bMLP\u7684Transformer\u6a21\u578b\u5728\u77e5\u8bc6\u83b7\u53d6\u548c\u63d0\u53d6\u65b9\u9762\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u89e3\u91ca\u4e86\u76f8\u5173\u7ecf\u9a8c\u73b0\u8c61\uff08\u5982\u5e7b\u89c9\u548c\u5b66\u4e60\u7387\u8c03\u5ea6\uff09\uff0c\u6df1\u5316\u4e86\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u7684\u7406\u89e3\u3002"}}
{"id": "2508.01064", "pdf": "https://arxiv.org/pdf/2508.01064", "abs": "https://arxiv.org/abs/2508.01064", "authors": ["Fenghe Tang", "Bingkun Nian", "Jianrui Ding", "Wenxin Ma", "Quan Quan", "Chengqi Dong", "Jie Yang", "Wei Liu", "S. Kevin Zhou"], "title": "Mobile U-ViT: Revisiting large kernel and U-shaped ViT for efficient medical image segmentation", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by ACM Multimedia 2025. Code:\n  https://github.com/FengheTan9/Mobile-U-ViT", "summary": "In clinical practice, medical image analysis often requires efficient\nexecution on resource-constrained mobile devices. However, existing mobile\nmodels-primarily optimized for natural images-tend to perform poorly on medical\ntasks due to the significant information density gap between natural and\nmedical domains. Combining computational efficiency with medical\nimaging-specific architectural advantages remains a challenge when developing\nlightweight, universal, and high-performing networks. To address this, we\npropose a mobile model called Mobile U-shaped Vision Transformer (Mobile U-ViT)\ntailored for medical image segmentation. Specifically, we employ the newly\npurposed ConvUtr as a hierarchical patch embedding, featuring a\nparameter-efficient large-kernel CNN with inverted bottleneck fusion. This\ndesign exhibits transformer-like representation learning capacity while being\nlighter and faster. To enable efficient local-global information exchange, we\nintroduce a novel Large-kernel Local-Global-Local (LGL) block that effectively\nbalances the low information density and high-level semantic discrepancy of\nmedical images. Finally, we incorporate a shallow and lightweight transformer\nbottleneck for long-range modeling and employ a cascaded decoder with\ndownsample skip connections for dense prediction. Despite its reduced\ncomputational demands, our medical-optimized architecture achieves\nstate-of-the-art performance across eight public 2D and 3D datasets covering\ndiverse imaging modalities, including zero-shot testing on four unseen\ndatasets. These results establish it as an efficient yet powerful and\ngeneralization solution for mobile medical image analysis. Code is available at\nhttps://github.com/FengheTan9/Mobile-U-ViT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMobile U-ViT\uff0c\u4e00\u4e2a\u4e3a\u79fb\u52a8\u533b\u7597\u56fe\u50cf\u5206\u5272\u8bbe\u8ba1\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002\u5b83\u901a\u8fc7\u521b\u65b0\u67b6\u6784\uff08\u5982ConvUtr\u5206\u5c42\u5d4c\u5165\u548cLGL\u5757\uff09\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u6027\u80fd\u5dee\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\uff0c\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9700\u8981\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\u9ad8\u6548\u6267\u884c\u3002\u7136\u800c\uff0c\u73b0\u6709\u4e3b\u8981\u4e3a\u81ea\u7136\u56fe\u50cf\u4f18\u5316\u7684\u79fb\u52a8\u6a21\u578b\uff0c\u7531\u4e8e\u81ea\u7136\u56fe\u50cf\u548c\u533b\u5b66\u56fe\u50cf\u4e4b\u95f4\u7684\u4fe1\u606f\u5bc6\u5ea6\u5dee\u5f02\u5927\uff0c\u5728\u533b\u5b66\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u5f00\u53d1\u517c\u987e\u8ba1\u7b97\u6548\u7387\u3001\u533b\u5b66\u56fe\u50cf\u7279\u5b9a\u67b6\u6784\u4f18\u52bf\u3001\u8f7b\u91cf\u3001\u901a\u7528\u548c\u9ad8\u6027\u80fd\u7684\u7f51\u7edc\u4ecd\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMobile U-shaped Vision Transformer (Mobile U-ViT) \u7684\u79fb\u52a8\u6a21\u578b\uff0c\u4e13\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u8bbe\u8ba1\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1) \u91c7\u7528ConvUtr\u4f5c\u4e3a\u5206\u5c42\u8865\u4e01\u5d4c\u5165\uff0c\u5176\u7279\u5f81\u662f\u53c2\u6570\u9ad8\u6548\u7684\u5927\u6838CNN\u548c\u5012\u7f6e\u74f6\u9888\u878d\u5408\uff0c\u5b9e\u73b0Transformer\u822c\u7684\u8868\u793a\u5b66\u4e60\u80fd\u529b\uff0c\u540c\u65f6\u66f4\u8f7b\u66f4\u5feb\u30022) \u5f15\u5165\u65b0\u9896\u7684\u5927\u6838\u5c40\u90e8-\u5168\u5c40-\u5c40\u90e8\uff08LGL\uff09\u5757\uff0c\u6709\u6548\u5e73\u8861\u533b\u5b66\u56fe\u50cf\u7684\u4f4e\u4fe1\u606f\u5bc6\u5ea6\u548c\u9ad8\u5c42\u8bed\u4e49\u5dee\u5f02\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5c40\u90e8-\u5168\u5c40\u4fe1\u606f\u4ea4\u6362\u30023) \u7ed3\u5408\u6d45\u5c42\u8f7b\u91cf\u7ea7Transformer\u74f6\u9888\u8fdb\u884c\u957f\u8ddd\u79bb\u5efa\u6a21\u30024) \u4f7f\u7528\u5e26\u4e0b\u91c7\u6837\u8df3\u8dc3\u8fde\u63a5\u7684\u7ea7\u8054\u89e3\u7801\u5668\u8fdb\u884c\u5bc6\u96c6\u9884\u6d4b\u3002", "result": "\u5c3d\u7ba1\u8ba1\u7b97\u9700\u6c42\u964d\u4f4e\uff0c\u6211\u4eec\u4f18\u5316\u7684\u533b\u5b66\u67b6\u6784\u5728\u516b\u4e2a\u6db5\u76d6\u4e0d\u540c\u6210\u50cf\u6a21\u5f0f\u7684\u516c\u51712D\u548c3D\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5305\u62ec\u5728\u56db\u4e2a\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u4e0a\u7684\u96f6\u6837\u672c\u6d4b\u8bd5\u3002\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u79fb\u52a8\u533b\u5b66\u56fe\u50cf\u5206\u6790\u7684\u6709\u6548\u3001\u5f3a\u5927\u4e14\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "Mobile U-ViT\u4e3a\u79fb\u52a8\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u5f3a\u5927\u4e14\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u8fdb\u884c\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u6311\u6218\u3002"}}
{"id": "2508.01181", "pdf": "https://arxiv.org/pdf/2508.01181", "abs": "https://arxiv.org/abs/2508.01181", "authors": ["Zhiyuan Han", "Beier Zhu", "Yanlong Xu", "Peipei Song", "Xun Yang"], "title": "Benchmarking and Bridging Emotion Conflicts for Multimodal Emotion Reasoning", "categories": ["cs.AI", "cs.CV", "cs.MM", "cs.SD", "eess.AS", "68", "I.2.10"], "comment": "ACM Multimedia 2025", "summary": "Despite their strong performance in multimodal emotion reasoning, existing\nMultimodal Large Language Models (MLLMs) often overlook the scenarios involving\nemotion conflicts, where emotional cues from different modalities are\ninconsistent. To fill this gap, we first introduce CA-MER, a new benchmark\ndesigned to examine MLLMs under realistic emotion conflicts. It consists of\nthree subsets: video-aligned, audio-aligned, and consistent, where only one or\nall modalities reflect the true emotion. However, evaluations on our CA-MER\nreveal that current state-of-the-art emotion MLLMs systematically over-rely on\naudio signal during emotion conflicts, neglecting critical cues from visual\nmodality. To mitigate this bias, we propose MoSEAR, a parameter-efficient\nframework that promotes balanced modality integration. MoSEAR consists of two\nmodules: (1)MoSE, modality-specific experts with a regularized gating mechanism\nthat reduces modality bias in the fine-tuning heads; and (2)AR, an attention\nreallocation mechanism that rebalances modality contributions in frozen\nbackbones during inference. Our framework offers two key advantages: it\nmitigates emotion conflicts and improves performance on consistent\nsamples-without incurring a trade-off between audio and visual modalities.\nExperiments on multiple benchmarks-including MER2023, EMER, DFEW, and our\nCA-MER-demonstrate that MoSEAR achieves state-of-the-art performance,\nparticularly under modality conflict conditions.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u51b2\u7a81\u573a\u666f\u4e0b\u7684\u6a21\u6001\u504f\u89c1\u95ee\u9898\uff0c\u63d0\u51fa\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6CA-MER\u548c\u53c2\u6570\u9ad8\u6548\u6846\u67b6MoSEAR\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5e73\u8861\u7684\u591a\u6a21\u6001\u60c5\u611f\u63a8\u7406\uff0c\u5e76\u8fbe\u5230\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u60c5\u611f\u63a8\u7406\u4e2d\u5ffd\u89c6\u4e86\u60c5\u611f\u51b2\u7a81\u573a\u666f\uff0c\u5373\u4e0d\u540c\u6a21\u6001\u95f4\u60c5\u611f\u7ebf\u7d22\u4e0d\u4e00\u81f4\u7684\u60c5\u51b5\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u5728\u5904\u7406\u60c5\u611f\u51b2\u7a81\u65f6\u666e\u904d\u8fc7\u5ea6\u4f9d\u8d56\u97f3\u9891\u4fe1\u53f7\uff0c\u800c\u5ffd\u7565\u4e86\u5173\u952e\u7684\u89c6\u89c9\u6a21\u6001\u4fe1\u606f\u3002", "method": "\u9996\u5148\uff0c\u5f15\u5165\u4e86\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6CA-MER\uff0c\u7528\u4e8e\u5728\u73b0\u5b9e\u60c5\u611f\u51b2\u7a81\u4e0b\uff08\u5305\u62ec\u89c6\u9891\u5bf9\u9f50\u3001\u97f3\u9891\u5bf9\u9f50\u548c\u4e00\u81f4\u6027\u5b50\u96c6\uff09\u8bc4\u4f30MLLMs\u3002\u5176\u6b21\uff0c\u63d0\u51fa\u4e86\u53c2\u6570\u9ad8\u6548\u7684MoSEAR\u6846\u67b6\uff0c\u65e8\u5728\u4fc3\u8fdb\u6a21\u6001\u5e73\u8861\u96c6\u6210\u5e76\u51cf\u8f7b\u504f\u89c1\u3002MoSEAR\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1aMoSE\uff08\u6a21\u6001\u7279\u5b9a\u4e13\u5bb6\uff0c\u5e26\u6709\u6b63\u5219\u5316\u95e8\u63a7\u673a\u5236\uff0c\u51cf\u5c11\u5fae\u8c03\u5934\u90e8\u7684\u6a21\u6001\u504f\u89c1\uff09\u548cAR\uff08\u6ce8\u610f\u529b\u91cd\u65b0\u5206\u914d\u673a\u5236\uff0c\u7528\u4e8e\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u91cd\u65b0\u5e73\u8861\u51bb\u7ed3\u9aa8\u5e72\u7f51\u7edc\u4e2d\u7684\u6a21\u6001\u8d21\u732e\uff09\u3002", "result": "\u901a\u8fc7CA-MER\u8bc4\u4f30\u53d1\u73b0\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u60c5\u611fMLLMs\u7cfb\u7edf\u6027\u5730\u8fc7\u5ea6\u4f9d\u8d56\u97f3\u9891\u4fe1\u53f7\u3002MoSEAR\u6846\u67b6\u6709\u6548\u51cf\u8f7b\u4e86\u60c5\u611f\u51b2\u7a81\uff0c\u5e76\u5728\u4e00\u81f4\u6027\u6837\u672c\u4e0a\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4e14\u672a\u5728\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u95f4\u4ea7\u751f\u6743\u8861\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cMoSEAR\u5728MER2023\u3001EMER\u3001DFEW\u548cCA-MER\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6a21\u6001\u51b2\u7a81\u6761\u4ef6\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "MoSEAR\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u5e73\u8861\u7684\u6a21\u6001\u96c6\u6210\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u60c5\u611f\u51b2\u7a81\u65f6\u7684\u6a21\u6001\u504f\u89c1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2508.01370", "pdf": "https://arxiv.org/pdf/2508.01370", "abs": "https://arxiv.org/abs/2508.01370", "authors": ["Roman Koshkin", "Pengyu Dai", "Nozomi Fujikawa", "Masahito Togami", "Marco Visentini-Scarzanella"], "title": "MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We present an autonomous framework that leverages Large Language Models\n(LLMs) to automate end-to-end business analysis and market report generation.\nAt its core, the system employs specialized agents - Researcher, Reviewer,\nWriter, and Retriever - that collaborate to analyze data and produce\ncomprehensive reports. These agents learn from real professional consultants'\npresentation materials at Amazon through in-context learning to replicate\nprofessional analytical methodologies. The framework executes a multi-step\nprocess: querying databases, analyzing data, generating insights, creating\nvisualizations, and composing market reports. We also introduce a novel\nLLM-based evaluation system for assessing report quality, which shows alignment\nwith expert human evaluations. Building on these evaluations, we implement an\niterative improvement mechanism that optimizes report quality through automated\nreview cycles. Experimental results show that report quality can be improved by\nboth automated review cycles and consultants' unstructured knowledge. In\nexperimental validation, our framework generates detailed 6-page reports in 7\nminutes at a cost of approximately \\$1. Our work could be an important step to\nautomatically create affordable market insights.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b9e\u73b0\u7aef\u5230\u7aef\u4e1a\u52a1\u5206\u6790\u548c\u5e02\u573a\u62a5\u544a\u751f\u6210\u7684\u81ea\u52a8\u5316\u6846\u67b6\u3002", "motivation": "\u65e8\u5728\u81ea\u52a8\u5316\u590d\u6742\u7684\u4e1a\u52a1\u5206\u6790\u548c\u5e02\u573a\u62a5\u544a\u751f\u6210\u8fc7\u7a0b\uff0c\u4ee5\u63d0\u4f9b\u7ecf\u6d4e\u5b9e\u60e0\u4e14\u9ad8\u6548\u7684\u5e02\u573a\u6d1e\u5bdf\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528LLMs\uff0c\u5e76\u7531\u7814\u7a76\u5458\u3001\u5ba1\u9605\u8005\u3001\u64b0\u5199\u8005\u548c\u68c0\u7d22\u8005\u7b49\u4e13\u4e1a\u4ee3\u7406\u534f\u540c\u5de5\u4f5c\u3002\u8fd9\u4e9b\u4ee3\u7406\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08in-context learning\uff09\u4ece\u4e9a\u9a6c\u900a\u4e13\u4e1a\u987e\u95ee\u7684\u6750\u6599\u4e2d\u5b66\u4e60\u5206\u6790\u65b9\u6cd5\u3002\u5de5\u4f5c\u6d41\u7a0b\u5305\u62ec\u67e5\u8be2\u6570\u636e\u5e93\u3001\u6570\u636e\u5206\u6790\u3001\u6d1e\u5bdf\u751f\u6210\u3001\u53ef\u89c6\u5316\u5236\u4f5c\u548c\u62a5\u544a\u64b0\u5199\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u57fa\u4e8eLLM\u7684\u62a5\u544a\u8d28\u91cf\u8bc4\u4f30\u7cfb\u7edf\u548c\u8fed\u4ee3\u6539\u8fdb\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u62a5\u544a\u8d28\u91cf\u53ef\u901a\u8fc7\u81ea\u52a8\u5316\u5ba1\u9605\u5468\u671f\u548c\u987e\u95ee\u7684\u975e\u7ed3\u6784\u5316\u77e5\u8bc6\u5f97\u5230\u63d0\u5347\u3002\u8be5\u6846\u67b6\u80fd\u591f\u57287\u5206\u949f\u5185\u751f\u6210\u8be6\u7ec6\u76846\u9875\u62a5\u544a\uff0c\u6bcf\u4efd\u6210\u672c\u7ea6\u4e3a1\u7f8e\u5143\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u662f\u5b9e\u73b0\u81ea\u52a8\u5316\u751f\u6210\u7ecf\u6d4e\u5b9e\u60e0\u5e02\u573a\u6d1e\u5bdf\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2508.02595", "pdf": "https://arxiv.org/pdf/2508.02595", "abs": "https://arxiv.org/abs/2508.02595", "authors": ["Siamak Abdi", "Giuseppe Di Fatta", "Atta Badii", "Giancarlo Fortino"], "title": "Fully Decentralised Consensus for Extreme-scale Blockchain", "categories": ["cs.DC", "cs.NI"], "comment": "IEEE Global Blockchain Conference (GBC) 2025", "summary": "Blockchain is a decentralised, immutable ledger technology that has been\nwidely adopted in many sectors for various applications such as\ncryptocurrencies, smart contracts and supply chain management. Distributed\nconsensus is a fundamental component of blockchain, which is required to ensure\ntrust, security, and integrity of the data stored and the transactions\nprocessed in the blockchain. Various consensus algorithms have been developed,\neach affected from certain issues such as node failures, high resource\nconsumption, collusion, etc. This work introduces a fully decentralised\nconsensus protocol, Blockchain Epidemic Consensus Protocol (BECP), suitable for\nvery large and extreme-scale blockchain systems. The proposed approach\nleverages the benefits of epidemic protocols, such as no reliance on a fixed\nset of validators or leaders, probabilistic guarantees of convergence,\nefficient use of network resources, and tolerance to node and network failures.\nA comparative experimental analysis has been carried out with traditional\nprotocols including PAXOS, RAFT, and Practical Byzantine Fault Tolerance\n(PBFT), as well as a relatively more recent protocol such as Avalanche, which\nis specifically designed for very large-scale systems. The results illustrate\nhow BECP outperforms them in terms of throughput, scalability and consensus\nlatency. BECP achieves an average of 1.196 times higher throughput in terms of\nconsensus on items and 4.775 times better average consensus latency.\nFurthermore, BECP significantly reduces the number of messages compared to\nAvalanche. These results demonstrate the effectiveness and efficiency of fully\ndecentralised consensus for blockchain technology based on epidemic protocols.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u533a\u5757\u94fe\u6d41\u884c\u75c5\u5171\u8bc6\u534f\u8bae\uff08BECP\uff09\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6d41\u884c\u75c5\u5b66\u7684\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u5171\u8bc6\u534f\u8bae\uff0c\u4e13\u4e3a\u8d85\u5927\u89c4\u6a21\u533a\u5757\u94fe\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u5728\u541e\u5410\u91cf\u3001\u53ef\u6269\u5c55\u6027\u548c\u5171\u8bc6\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u534f\u8bae\u3002", "motivation": "\u533a\u5757\u94fe\u7684\u5206\u5e03\u5f0f\u5171\u8bc6\u662f\u786e\u4fdd\u6570\u636e\u4fe1\u4efb\u3001\u5b89\u5168\u548c\u5b8c\u6574\u6027\u7684\u57fa\u7840\uff0c\u4f46\u73b0\u6709\u5171\u8bc6\u7b97\u6cd5\u5b58\u5728\u8282\u70b9\u6545\u969c\u3001\u8d44\u6e90\u6d88\u8017\u9ad8\u3001\u52fe\u7ed3\u7b49\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u9002\u7528\u4e8e\u8d85\u5927\u89c4\u6a21\u533a\u5757\u94fe\u7cfb\u7edf\u7684\u3001\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u5171\u8bc6\u534f\u8bae\u3002", "method": "\u5f15\u5165\u4e86\u533a\u5757\u94fe\u6d41\u884c\u75c5\u5171\u8bc6\u534f\u8bae\uff08BECP\uff09\uff0c\u8be5\u534f\u8bae\u5229\u7528\u4e86\u6d41\u884c\u75c5\u534f\u8bae\u7684\u4f18\u52bf\uff0c\u5982\u4e0d\u4f9d\u8d56\u56fa\u5b9a\u9a8c\u8bc1\u8005\u3001\u6982\u7387\u6536\u655b\u3001\u8d44\u6e90\u9ad8\u6548\u5229\u7528\u4ee5\u53ca\u5bf9\u8282\u70b9\u548c\u7f51\u7edc\u6545\u969c\u7684\u5bb9\u5fcd\u3002\u901a\u8fc7\u4e0ePAXOS\u3001RAFT\u3001PBFT\u7b49\u4f20\u7edf\u534f\u8bae\u4ee5\u53caAvalanche\u7b49\u66f4\u8fd1\u671f\u534f\u8bae\u8fdb\u884c\u6bd4\u8f83\u5b9e\u9a8c\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cBECP\u5728\u541e\u5410\u91cf\u3001\u53ef\u6269\u5c55\u6027\u548c\u5171\u8bc6\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u88ab\u6bd4\u8f83\u7684\u534f\u8bae\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5176\u5728\u9879\u76ee\u5171\u8bc6\u65b9\u9762\u7684\u541e\u5410\u91cf\u5e73\u5747\u9ad8\u51fa1.196\u500d\uff0c\u5e73\u5747\u5171\u8bc6\u5ef6\u8fdf\u6539\u5584\u4e864.775\u500d\u3002\u6b64\u5916\uff0cBECP\u4e0eAvalanche\u76f8\u6bd4\u663e\u8457\u51cf\u5c11\u4e86\u6d88\u606f\u6570\u91cf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86\u57fa\u4e8e\u6d41\u884c\u75c5\u534f\u8bae\u7684\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u5171\u8bc6\u5728\u533a\u5757\u94fe\u6280\u672f\u4e2d\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2508.00903", "pdf": "https://arxiv.org/pdf/2508.00903", "abs": "https://arxiv.org/abs/2508.00903", "authors": ["Advey Nandan", "Cheng-Ting Chou", "Amrit Kurakula", "Cole Blondin", "Kevin Zhu", "Vasu Sharma", "Sean O'Brien"], "title": "Universal Neurons in GPT-2: Emergence, Persistence, and Functional Impact", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "We investigate the phenomenon of neuron universality in independently trained\nGPT-2 Small models, examining how these universal neurons-neurons with\nconsistently correlated activations across models-emerge and evolve throughout\ntraining. By analyzing five GPT-2 models at three checkpoints (100k, 200k, 300k\nsteps), we identify universal neurons through pairwise correlation analysis of\nactivations over a dataset of 5 million tokens. Ablation experiments reveal\nsignificant functional impacts of universal neurons on model predictions,\nmeasured via loss and KL divergence. Additionally, we quantify neuron\npersistence, demonstrating high stability of universal neurons across training\ncheckpoints, particularly in deeper layers. These findings suggest stable and\nuniversal representational structures emerge during neural network training.", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u72ec\u7acb\u8bad\u7ec3\u7684GPT-2\u6a21\u578b\u4e2d\u901a\u7528\u795e\u7ecf\u5143\u7684\u73b0\u8c61\uff0c\u5305\u62ec\u5176\u51fa\u73b0\u3001\u6f14\u53d8\u3001\u529f\u80fd\u5f71\u54cd\u548c\u7a33\u5b9a\u6027\uff0c\u8868\u660e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u4e86\u7a33\u5b9a\u7684\u901a\u7528\u8868\u5f81\u7ed3\u6784\u3002", "motivation": "\u63a2\u7a76\u72ec\u7acb\u8bad\u7ec3\u7684GPT-2 Small\u6a21\u578b\u4e2d\u201c\u901a\u7528\u795e\u7ecf\u5143\u201d\uff08\u5373\u5728\u4e0d\u540c\u6a21\u578b\u4e2d\u6fc0\u6d3b\u6301\u7eed\u76f8\u5173\u7684\u795e\u7ecf\u5143\uff09\u662f\u5982\u4f55\u4ea7\u751f\u548c\u6f14\u53d8\u7684\u3002", "method": "\u5206\u6790\u4e86\u4e94\u4e2aGPT-2\u6a21\u578b\u5728\u4e09\u4e2a\u8bad\u7ec3\u68c0\u67e5\u70b9\uff0810\u4e07\u300120\u4e07\u300130\u4e07\u6b65\uff09\u7684\u6570\u636e\u3002\u901a\u8fc7\u5bf95\u767e\u4e07tokens\u6570\u636e\u96c6\u4e0a\u7684\u6fc0\u6d3b\u8fdb\u884c\u6210\u5bf9\u76f8\u5173\u6027\u5206\u6790\u6765\u8bc6\u522b\u901a\u7528\u795e\u7ecf\u5143\u3002\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u8bc4\u4f30\u901a\u7528\u795e\u7ecf\u5143\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u529f\u80fd\u5f71\u54cd\uff08\u901a\u8fc7\u635f\u5931\u548cKL\u6563\u5ea6\u8861\u91cf\uff09\u3002\u91cf\u5316\u4e86\u795e\u7ecf\u5143\u5728\u4e0d\u540c\u8bad\u7ec3\u68c0\u67e5\u70b9\u95f4\u7684\u6301\u4e45\u6027\u3002", "result": "\u53d1\u73b0\u4e86\u901a\u7528\u795e\u7ecf\u5143\u7684\u5b58\u5728\u53ca\u5176\u529f\u80fd\u4e0a\u7684\u663e\u8457\u5f71\u54cd\u3002\u901a\u7528\u795e\u7ecf\u5143\u5728\u4e0d\u540c\u8bad\u7ec3\u68c0\u67e5\u70b9\u95f4\u663e\u793a\u51fa\u9ad8\u7a33\u5b9a\u6027\uff0c\u5c24\u5176\u662f\u5728\u6df1\u5c42\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u4e86\u7a33\u5b9a\u4e14\u901a\u7528\u7684\u8868\u5f81\u7ed3\u6784\u3002", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u6d8c\u73b0\u51fa\u7a33\u5b9a\u4e14\u901a\u7528\u7684\u8868\u5f81\u7ed3\u6784\u3002"}}
{"id": "2508.01074", "pdf": "https://arxiv.org/pdf/2508.01074", "abs": "https://arxiv.org/abs/2508.01074", "authors": ["Hongyu Zhu", "Sichu Liang", "Wenwen Wang", "Zhuomeng Zhang", "Fangqi Li", "Shi-Lin Wang"], "title": "Evading Data Provenance in Deep Neural Networks", "categories": ["cs.CV", "cs.CR"], "comment": "ICCV 2025 Highlight", "summary": "Modern over-parameterized deep models are highly data-dependent, with large\nscale general-purpose and domain-specific datasets serving as the bedrock for\nrapid advancements. However, many datasets are proprietary or contain sensitive\ninformation, making unrestricted model training problematic. In the open world\nwhere data thefts cannot be fully prevented, Dataset Ownership Verification\n(DOV) has emerged as a promising method to protect copyright by detecting\nunauthorized model training and tracing illicit activities. Due to its\ndiversity and superior stealth, evading DOV is considered extremely\nchallenging. However, this paper identifies that previous studies have relied\non oversimplistic evasion attacks for evaluation, leading to a false sense of\nsecurity. We introduce a unified evasion framework, in which a teacher model\nfirst learns from the copyright dataset and then transfers task-relevant yet\nidentifier-independent domain knowledge to a surrogate student using an\nout-of-distribution (OOD) dataset as the intermediary. Leveraging\nVision-Language Models and Large Language Models, we curate the most\ninformative and reliable subsets from the OOD gallery set as the final transfer\nset, and propose selectively transferring task-oriented knowledge to achieve a\nbetter trade-off between generalization and evasion effectiveness. Experiments\nacross diverse datasets covering eleven DOV methods demonstrate our approach\nsimultaneously eliminates all copyright identifiers and significantly\noutperforms nine state-of-the-art evasion attacks in both generalization and\neffectiveness, with moderate computational overhead. As a proof of concept, we\nreveal key vulnerabilities in current DOV methods, highlighting the need for\nlong-term development to enhance practicality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u89c4\u907f\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u9009\u62e9\u6027\u77e5\u8bc6\u8f6c\u79fb\uff0c\u6709\u6548\u89c4\u907f\u4e86\u6570\u636e\u96c6\u6240\u6709\u6743\u9a8c\u8bc1\uff08DOV\uff09\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709DOV\u65b9\u6cd5\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u6a21\u578b\u9ad8\u5ea6\u4f9d\u8d56\u6570\u636e\uff0c\u800c\u8bb8\u591a\u6570\u636e\u96c6\u662f\u4e13\u6709\u6216\u5305\u542b\u654f\u611f\u4fe1\u606f\uff0c\u9700\u8981\u7248\u6743\u4fdd\u62a4\u3002\u6570\u636e\u96c6\u6240\u6709\u6743\u9a8c\u8bc1\uff08DOV\uff09\u4f5c\u4e3a\u4e00\u79cd\u4fdd\u62a4\u7248\u6743\u7684\u65b9\u6cd5\uff0c\u88ab\u8ba4\u4e3a\u96be\u4ee5\u89c4\u907f\u3002\u7136\u800c\uff0c\u672c\u6587\u53d1\u73b0\u73b0\u6709\u7814\u7a76\u4f9d\u8d56\u4e8e\u8fc7\u4e8e\u7b80\u5316\u7684\u89c4\u907f\u653b\u51fb\uff0c\u5bfc\u81f4\u865a\u5047\u7684\u5b89\u5168\u611f\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u89c4\u907f\u65b9\u6cd5\u6765\u63ed\u793aDOV\u7684\u771f\u6b63\u6f0f\u6d1e\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u89c4\u907f\u6846\u67b6\uff1a\u6559\u5e08\u6a21\u578b\u4ece\u7248\u6743\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u7136\u540e\u5229\u7528\u57df\u5916\uff08OOD\uff09\u6570\u636e\u96c6\u4f5c\u4e3a\u4e2d\u4ecb\uff0c\u5c06\u4efb\u52a1\u76f8\u5173\u4f46\u4e0e\u6807\u8bc6\u7b26\u65e0\u5173\u7684\u9886\u57df\u77e5\u8bc6\u8f6c\u79fb\u7ed9\u66ff\u4ee3\u5b66\u751f\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7b5b\u9009\u51fa\u6700\u4fe1\u606f\u4e30\u5bcc\u3001\u53ef\u9760\u7684OOD\u5b50\u96c6\u4f5c\u4e3a\u6700\u7ec8\u4f20\u8f93\u96c6\uff0c\u5e76\u63d0\u51fa\u9009\u62e9\u6027\u5730\u8f6c\u79fb\u9762\u5411\u4efb\u52a1\u7684\u77e5\u8bc6\uff0c\u4ee5\u5e73\u8861\u6cdb\u5316\u548c\u89c4\u907f\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u572811\u79cdDOV\u65b9\u6cd5\u548c\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u540c\u65f6\u6d88\u9664\u6240\u6709\u7248\u6743\u6807\u8bc6\u7b26\uff0c\u5e76\u4e14\u5728\u6cdb\u5316\u6027\u548c\u6709\u6548\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4e5d\u79cd\u6700\u5148\u8fdb\u7684\u89c4\u907f\u653b\u51fb\uff0c\u540c\u65f6\u8ba1\u7b97\u5f00\u9500\u9002\u4e2d\u3002", "conclusion": "\u5f53\u524dDOV\u65b9\u6cd5\u5b58\u5728\u5173\u952e\u6f0f\u6d1e\uff0c\u9700\u8981\u8fdb\u884c\u957f\u671f\u53d1\u5c55\u4ee5\u589e\u5f3a\u5b9e\u7528\u6027\uff0c\u672c\u6587\u63d0\u51fa\u7684\u89c4\u907f\u65b9\u6cd5\u4f5c\u4e3a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u4e3a\u672a\u6765\u7684DOV\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2508.01186", "pdf": "https://arxiv.org/pdf/2508.01186", "abs": "https://arxiv.org/abs/2508.01186", "authors": ["Chaojia Yu", "Zihan Cheng", "Hanwen Cui", "Yishuo Gao", "Zexu Luo", "Yijin Wang", "Hangbin Zheng", "Yong Zhao"], "title": "A Survey on Agent Workflow -- Status and Future", "categories": ["cs.AI", "cs.HC"], "comment": "12 pages, 3 figures, accepted to IEEE Conference,\n  ICAIBD(International Conference of Artificial Intelligence and Big Data)\n  2025. This is the author's version, not the publisher's. See\n  https://ieeexplore.ieee.org/document/11082076", "summary": "In the age of large language models (LLMs), autonomous agents have emerged as\na powerful paradigm for achieving general intelligence. These agents\ndynamically leverage tools, memory, and reasoning capabilities to accomplish\nuser-defined goals. As agent systems grow in complexity, agent\nworkflows-structured orchestration frameworks-have become central to enabling\nscalable, controllable, and secure AI behaviors. This survey provides a\ncomprehensive review of agent workflow systems, spanning academic frameworks\nand industrial implementations. We classify existing systems along two key\ndimensions: functional capabilities (e.g., planning, multi-agent collaboration,\nexternal API integration) and architectural features (e.g., agent roles,\norchestration flows, specification languages). By comparing over 20\nrepresentative systems, we highlight common patterns, potential technical\nchallenges, and emerging trends. We further address concerns related to\nworkflow optimization strategies and security. Finally, we outline open\nproblems such as standardization and multimodal integration, offering insights\nfor future research at the intersection of agent design, workflow\ninfrastructure, and safe automation.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u5168\u9762\u56de\u987e\u4e86\u81ea\u4e3b\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7cfb\u7edf\uff0c\u5bf9\u5176\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u603b\u7ed3\u4e86\u73b0\u6709\u6a21\u5f0f\u3001\u6311\u6218\u53ca\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65f6\u4ee3\u667a\u80fd\u4f53\u7cfb\u7edf\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u5de5\u4f5c\u6d41\u4f5c\u4e3a\u7ed3\u6784\u5316\u7f16\u6392\u6846\u67b6\uff0c\u5bf9\u4e8e\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u53ef\u63a7\u548c\u5b89\u5168\u7684AI\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u5bf9\u5b66\u672f\u6846\u67b6\u548c\u5de5\u4e1a\u5b9e\u73b0\u4e2d\u7684\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7cfb\u7edf\u8fdb\u884c\u4e86\u5168\u9762\u7efc\u8ff0\u3002\u901a\u8fc7\u529f\u80fd\u80fd\u529b\uff08\u5982\u89c4\u5212\u3001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff09\u548c\u67b6\u6784\u7279\u5f81\uff08\u5982\u667a\u80fd\u4f53\u89d2\u8272\u3001\u7f16\u6392\u6d41\u7a0b\uff09\u4e24\u4e2a\u7ef4\u5ea6\u5bf920\u591a\u4e2a\u4ee3\u8868\u6027\u7cfb\u7edf\u8fdb\u884c\u5206\u7c7b\u548c\u6bd4\u8f83\u3002", "result": "\u8bc6\u522b\u51fa\u5e38\u89c1\u6a21\u5f0f\u3001\u6f5c\u5728\u6280\u672f\u6311\u6218\u548c\u65b0\u5174\u8d8b\u52bf\uff0c\u5e76\u63a2\u8ba8\u4e86\u5de5\u4f5c\u6d41\u4f18\u5316\u7b56\u7565\u548c\u5b89\u5168\u95ee\u9898\u3002", "conclusion": "\u6307\u51fa\u4e86\u6807\u51c6\u5316\u548c\u591a\u6a21\u6001\u96c6\u6210\u7b49\u5f00\u653e\u95ee\u9898\uff0c\u4e3a\u667a\u80fd\u4f53\u8bbe\u8ba1\u3001\u5de5\u4f5c\u6d41\u57fa\u7840\u8bbe\u65bd\u548c\u5b89\u5168\u81ea\u52a8\u5316\u4ea4\u53c9\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2508.01401", "pdf": "https://arxiv.org/pdf/2508.01401", "abs": "https://arxiv.org/abs/2508.01401", "authors": ["Ahmad Rezaie Mianroodi", "Amirali Rezaie", "Niko Grisel Todorov", "Cyril Rakovski", "Frank Rudzicz"], "title": "MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages excluding references and appendices", "summary": "Physicians spend significant time documenting clinical encounters, a burden\nthat contributes to professional burnout. To address this, robust automation\ntools for medical documentation are crucial. We introduce MedSynth -- a novel\ndataset of synthetic medical dialogues and notes designed to advance the\nDialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks.\nInformed by an extensive analysis of disease distributions, this dataset\nincludes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We\ndemonstrate that our dataset markedly enhances the performance of models in\ngenerating medical notes from dialogues, and dialogues from medical notes. The\ndataset provides a valuable resource in a field where open-access,\nprivacy-compliant, and diverse training data are scarce. Code is available at\nhttps://github.com/ahmadrezarm/MedSynth/tree/main and the dataset is available\nat https://huggingface.co/datasets/Ahmad0067/MedSynth.", "AI": {"tldr": "\u4e3a\u7f13\u89e3\u533b\u751f\u6587\u6863\u8bb0\u5f55\u8d1f\u62c5\uff0c\u672c\u7814\u7a76\u6784\u5efa\u4e86MedSynth\u5408\u6210\u533b\u7597\u5bf9\u8bdd\u4e0e\u7b14\u8bb0\u6570\u636e\u96c6\uff08\u542b10000+\u5bf9\u6570\u636e\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u7597\u7b14\u8bb0\u548c\u5bf9\u8bdd\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u5f00\u653e\u83b7\u53d6\u6570\u636e\u7a00\u7f3a\u7684\u9886\u57df\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002", "motivation": "\u533b\u751f\u5728\u4e34\u5e8a\u6587\u6863\u8bb0\u5f55\u4e0a\u82b1\u8d39\u5927\u91cf\u65f6\u95f4\uff0c\u5bfc\u81f4\u804c\u4e1a\u5026\u6020\uff1b\u56e0\u6b64\uff0c\u6025\u9700\u5f3a\u5927\u7684\u533b\u7597\u6587\u6863\u81ea\u52a8\u5316\u5de5\u5177\u6765\u51cf\u8f7b\u8fd9\u4e00\u8d1f\u62c5\u3002", "method": "\u5f15\u5165\u5e76\u6784\u5efa\u4e86MedSynth\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u5408\u6210\u533b\u7597\u5bf9\u8bdd\u548c\u7b14\u8bb0\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u5bf9\u75be\u75c5\u5206\u5e03\u7684\u5e7f\u6cdb\u5206\u6790\u800c\u6784\u5efa\uff0c\u5305\u542b\u8d85\u8fc710,000\u4e2a\u5bf9\u8bdd-\u7b14\u8bb0\u5bf9\uff0c\u6db5\u76d62000\u591a\u79cdICD-10\u7f16\u7801\uff0c\u65e8\u5728\u63a8\u8fdb\u5bf9\u8bdd\u5230\u7b14\u8bb0\uff08Dial-2-Note\uff09\u548c\u7b14\u8bb0\u5230\u5bf9\u8bdd\uff08Note-2-Dial\uff09\u7684\u4efb\u52a1\u3002", "result": "MedSynth\u6570\u636e\u96c6\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u4ece\u5bf9\u8bdd\u751f\u6210\u533b\u7597\u7b14\u8bb0\u4ee5\u53ca\u4ece\u533b\u7597\u7b14\u8bb0\u751f\u6210\u5bf9\u8bdd\u7684\u6027\u80fd\u3002", "conclusion": "MedSynth\u6570\u636e\u96c6\u5728\u5f00\u653e\u83b7\u53d6\u3001\u9690\u79c1\u5408\u89c4\u4e14\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u533b\u7597\u9886\u57df\u4e2d\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u975e\u5e38\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002"}}
{"id": "2508.02657", "pdf": "https://arxiv.org/pdf/2508.02657", "abs": "https://arxiv.org/abs/2508.02657", "authors": ["Irtiza Hasan", "Ahmed Arafa"], "title": "RC-Gossip: Information Freshness in Clustered Networks with Rate-Changing Gossip", "categories": ["cs.IT", "cs.NI", "eess.SP", "math.IT"], "comment": "To appear in the 2025 Asilomar Conference on Signals, Systems, and\n  Computers", "summary": "A clustered gossip network is considered in which a source updates its\ninformation over time, and end-nodes, organized in clusters through\nclusterheads, are keeping track of it. The goal for the nodes is to remain as\nfresh as possible, i.e., have the same information as the source, which we\nassess by the long-term average binary freshness metric. We introduce a smart\nmechanism of information dissemination which we coin rate-changing gossip\n(RC-Gossip). Its main idea is that gossiping is directed towards nodes that\nneed it the most, and hence the rate of gossiping changes based on the number\nof fresh nodes in the network at a given time. While Stochastic Hybrid System\n(SHS) analysis has been the norm in studying freshness of gossip networks, we\npresent an equivalent way to analyze freshness using a renewal-reward-based\napproach. Using that, we show that RC-gossip significantly increases freshness\nof nodes in different clustered networks, with optimal cluster sizes, compared\nto traditional gossiping techniques.", "AI": {"tldr": "\u4e3a\u5206\u7c07\u516b\u5366\u7f51\u7edc\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRC-Gossip\u7684\u65b0\u578b\u4fe1\u606f\u4f20\u64ad\u673a\u5236\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u516b\u5366\u901f\u7387\u663e\u8457\u63d0\u5347\u8282\u70b9\u4fe1\u606f\u65b0\u9c9c\u5ea6\uff0c\u5e76\u4f7f\u7528\u66f4\u65b0-\u5956\u52b1\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u6790\u3002", "motivation": "\u5728\u5206\u7c07\u516b\u5366\u7f51\u7edc\u4e2d\uff0c\u8282\u70b9\u9700\u8981\u9ad8\u6548\u5730\u8ddf\u8e2a\u6e90\u4fe1\u606f\u5e76\u4fdd\u6301\u5c3d\u53ef\u80fd\u9ad8\u7684\u4fe1\u606f\u201c\u65b0\u9c9c\u5ea6\u201d\uff0c\u4ee5\u786e\u4fdd\u4e0e\u6e90\u4fe1\u606f\u4e00\u81f4\u3002", "method": "\u5f15\u5165\u4e86\u201c\u901f\u7387\u81ea\u9002\u5e94\u516b\u5366\u201d\uff08RC-Gossip\uff09\u673a\u5236\uff0c\u5176\u6838\u5fc3\u601d\u60f3\u662f\u6839\u636e\u7f51\u7edc\u4e2d\u65b0\u9c9c\u8282\u70b9\u7684\u6570\u91cf\u52a8\u6001\u8c03\u6574\u516b\u5366\u901f\u7387\uff0c\u5c06\u4fe1\u606f\u4f18\u5148\u4f20\u64ad\u7ed9\u6700\u9700\u8981\u7684\u8282\u70b9\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u66f4\u65b0-\u5956\u52b1\u7684\u65b9\u6cd5\u6765\u5206\u6790\u7f51\u7edc\u65b0\u9c9c\u5ea6\uff0c\u66ff\u4ee3\u4e86\u4f20\u7edf\u7684\u968f\u673a\u6df7\u5408\u7cfb\u7edf\uff08SHS\uff09\u5206\u6790\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u516b\u5366\u6280\u672f\u76f8\u6bd4\uff0cRC-Gossip\u663e\u8457\u63d0\u9ad8\u4e86\u4e0d\u540c\u5206\u7c07\u7f51\u7edc\u4e2d\u8282\u70b9\u7684\u4fe1\u606f\u65b0\u9c9c\u5ea6\uff0c\u5e76\u80fd\u5b9e\u73b0\u6700\u4f18\u7c07\u5927\u5c0f\u3002", "conclusion": "RC-Gossip\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u4fe1\u606f\u4f20\u64ad\u673a\u5236\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5206\u7c07\u7f51\u7edc\u4e2d\u8282\u70b9\u7684\u4fe1\u606f\u65b0\u9c9c\u5ea6\uff0c\u4e3a\u4f18\u5316\u7f51\u7edc\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.00909", "pdf": "https://arxiv.org/pdf/2508.00909", "abs": "https://arxiv.org/abs/2508.00909", "authors": ["Aitor S\u00e1nchez-Ferrera", "Usue Mori", "Borja Calvo", "Jose A. Lozano"], "title": "NeuCoReClass AD: Redefining Self-Supervised Time Series Anomaly Detection", "categories": ["cs.LG"], "comment": null, "summary": "Time series anomaly detection plays a critical role in a wide range of\nreal-world applications. Among unsupervised approaches, self-supervised\nlearning has gained traction for modeling normal behavior without the need of\nlabeled data. However, many existing methods rely on a single proxy task,\nlimiting their ability to capture meaningful patterns in normal data. Moreover,\nthey often depend on handcrafted transformations tailored specific domains,\nhindering their generalization accross diverse problems. To address these\nlimitations, we introduce NeuCoReClass AD, a self-supervised multi-task time\nseries anomaly detection framework that combines contrastive, reconstruction,\nand classification proxy tasks. Our method employs neural transformation\nlearning to generate augmented views that are informative, diverse, and\ncoherent, without requiring domain-specific knowledge. We evaluate NeuCoReClass\nAD across a wide range of benchmarks, demonstrating that it consistently\noutperforms both classical baselines and most deep-learning alternatives.\nFurthermore, it enables the characterization of distinct anomaly profiles in a\nfully unsupervised manner.", "AI": {"tldr": "NeuCoReClass AD\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u591a\u4efb\u52a1\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5bf9\u6bd4\u3001\u91cd\u6784\u548c\u5206\u7c7b\u4ee3\u7406\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u795e\u7ecf\u7f51\u7edc\u8f6c\u6362\u5b66\u4e60\u751f\u6210\u589e\u5f3a\u89c6\u56fe\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u68c0\u6d4b\u6027\u80fd\u548c\u5f02\u5e38\u5256\u9762\u8868\u5f81\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5b58\u5728\u4e24\u4e2a\u5c40\u9650\u6027\uff1a\u4e00\u662f\u8fc7\u5ea6\u4f9d\u8d56\u5355\u4e00\u4ee3\u7406\u4efb\u52a1\uff0c\u9650\u5236\u4e86\u5176\u6355\u83b7\u6570\u636e\u6a21\u5f0f\u7684\u80fd\u529b\uff1b\u4e8c\u662f\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u9886\u57df\u7279\u5b9a\u8f6c\u6362\uff0c\u963b\u788d\u4e86\u5176\u5728\u4e0d\u540c\u95ee\u9898\u95f4\u7684\u6cdb\u5316\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86NeuCoReClass AD\u6846\u67b6\uff0c\u4e00\u4e2a\u81ea\u76d1\u7763\u591a\u4efb\u52a1\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5bf9\u6bd4\u5b66\u4e60\u3001\u91cd\u6784\u548c\u5206\u7c7b\u4e09\u79cd\u4ee3\u7406\u4efb\u52a1\u3002\u6b64\u5916\uff0c\u5b83\u91c7\u7528\u795e\u7ecf\u7f51\u7edc\u8f6c\u6362\u5b66\u4e60\u6765\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u3001\u591a\u6837\u5316\u4e14\u8fde\u8d2f\u7684\u589e\u5f3a\u89c6\u56fe\uff0c\u65e0\u9700\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNeuCoReClass AD\u7684\u6027\u80fd\u6301\u7eed\u4f18\u4e8e\u7ecf\u5178\u57fa\u7ebf\u548c\u5927\u591a\u6570\u6df1\u5ea6\u5b66\u4e60\u66ff\u4ee3\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u5b83\u8fd8\u80fd\u4ee5\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u65b9\u5f0f\u8868\u5f81\u4e0d\u540c\u7684\u5f02\u5e38\u5256\u9762\u3002", "conclusion": "NeuCoReClass AD\u901a\u8fc7\u6574\u5408\u591a\u4efb\u52a1\u4ee3\u7406\u548c\u795e\u7ecf\u8f6c\u6362\u5b66\u4e60\uff0c\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u81ea\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u8fd8\u589e\u5f3a\u4e86\u5bf9\u5f02\u5e38\u6a21\u5f0f\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2508.01079", "pdf": "https://arxiv.org/pdf/2508.01079", "abs": "https://arxiv.org/abs/2508.01079", "authors": ["Santiago Diaz", "Xinghui Hu", "Josiane Uwumukiza", "Giovanni Lavezzi", "Victor Rodriguez-Fernandez", "Richard Linares"], "title": "DreamSat-2.0: Towards a General Single-View Asteroid 3D Reconstruction", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "To enhance asteroid exploration and autonomous spacecraft navigation, we\nintroduce DreamSat-2.0, a pipeline that benchmarks three state-of-the-art 3D\nreconstruction models-Hunyuan-3D, Trellis-3D, and Ouroboros-3D-on custom\nspacecraft and asteroid datasets. Our systematic analysis, using 2D perceptual\n(image quality) and 3D geometric (shape accuracy) metrics, reveals that model\nperformance is domain-dependent. While models produce higher-quality images of\ncomplex spacecraft, they achieve better geometric reconstructions for the\nsimpler forms of asteroids. New benchmarks are established, with Hunyuan-3D\nachieving top perceptual scores on spacecraft but its best geometric accuracy\non asteroids, marking a significant advance over our prior work.", "AI": {"tldr": "\u8bc4\u4f30\u4e86\u4e09\u4e2a\u5148\u8fdb3D\u91cd\u5efa\u6a21\u578b\u5728\u822a\u5929\u5668\u548c\u5c0f\u884c\u661f\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u4e0e\u9886\u57df\u76f8\u5173\uff0c\u5e76\u4e3aHunyuan-3D\u7b49\u5efa\u7acb\u4e86\u65b0\u57fa\u51c6\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u5c0f\u884c\u661f\u63a2\u7d22\u548c\u81ea\u4e3b\u822a\u5929\u5668\u5bfc\u822a\u80fd\u529b\uff0c\u9700\u8981\u8bc4\u4f30\u548c\u4f18\u53163D\u91cd\u5efa\u6a21\u578b\u5728\u8fd9\u4e9b\u7279\u5b9a\u592a\u7a7a\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u5f15\u5165\u4e86DreamSat-2.0\u7ba1\u9053\uff0c\u8be5\u7ba1\u9053\u5728\u5b9a\u5236\u7684\u822a\u5929\u5668\u548c\u5c0f\u884c\u661f\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9Hunyuan-3D\u3001Trellis-3D\u548cOuroboros-3D\u8fd9\u4e09\u4e2a\u6700\u5148\u8fdb\u76843D\u91cd\u5efa\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u91c7\u75282D\u611f\u77e5\uff08\u56fe\u50cf\u8d28\u91cf\uff09\u548c3D\u51e0\u4f55\uff08\u5f62\u72b6\u7cbe\u5ea6\uff09\u6307\u6807\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u5177\u6709\u9886\u57df\u4f9d\u8d56\u6027\uff1a\u6a21\u578b\u5728\u590d\u6742\u822a\u5929\u5668\u4e0a\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\uff0c\u4f46\u5728\u7b80\u5355\u5c0f\u884c\u661f\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u51e0\u4f55\u91cd\u5efa\u3002Hunyuan-3D\u5728\u822a\u5929\u5668\u4e0a\u83b7\u5f97\u4e86\u6700\u9ad8\u7684\u611f\u77e5\u5206\u6570\uff0c\u800c\u5728\u5c0f\u884c\u661f\u4e0a\u8fbe\u5230\u4e86\u6700\u4f73\u7684\u51e0\u4f55\u7cbe\u5ea6\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u51c6\u3002", "conclusion": "DreamSat-2.0\u7684\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e863D\u91cd\u5efa\u6a21\u578b\u5728\u4e0d\u540c\u592a\u7a7a\u76ee\u6807\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4e3a\u5c0f\u884c\u661f\u63a2\u7d22\u548c\u822a\u5929\u5668\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u7684\u6a21\u578b\u6027\u80fd\u57fa\u51c6\uff0c\u7279\u522b\u662fHunyuan-3D\u5c55\u73b0\u51fa\u663e\u8457\u8fdb\u6b65\u3002"}}
{"id": "2508.01191", "pdf": "https://arxiv.org/pdf/2508.01191", "abs": "https://arxiv.org/abs/2508.01191", "authors": ["Chengshuai Zhao", "Zhen Tan", "Pingchuan Ma", "Dawei Li", "Bohan Jiang", "Yancheng Wang", "Yingzhen Yang", "Huan Liu"], "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language\nModel (LLM) performance on various tasks. With this approach, LLMs appear to\nproduce human-like reasoning steps before providing answers (a.k.a., CoT\nreasoning), which often leads to the perception that they engage in deliberate\ninferential processes. However, some initial findings suggest that CoT\nreasoning may be more superficial than it appears, motivating us to explore\nfurther. In this paper, we study CoT reasoning via a data distribution lens and\ninvestigate if CoT reasoning reflects a structured inductive bias learned from\nin-distribution data, allowing the model to conditionally generate reasoning\npaths that approximate those seen during training. Thus, its effectiveness is\nfundamentally bounded by the degree of distribution discrepancy between the\ntraining data and the test queries. With this lens, we dissect CoT reasoning\nvia three dimensions: task, length, and format. To investigate each dimension,\nwe design DataAlchemy, an isolated and controlled environment to train LLMs\nfrom scratch and systematically probe them under various distribution\nconditions. Our results reveal that CoT reasoning is a brittle mirage that\nvanishes when it is pushed beyond training distributions. This work offers a\ndeeper understanding of why and when CoT reasoning fails, emphasizing the\nongoing challenge of achieving genuine and generalizable reasoning.", "AI": {"tldr": "CoT\u63a8\u7406\u5728LLM\u4e2d\u5e76\u975e\u666e\u904d\u6709\u6548\u7684\u6df1\u5c42\u63a8\u7406\uff0c\u4e00\u65e6\u8d85\u51fa\u8bad\u7ec3\u6570\u636e\u5206\u5e03\uff0c\u5176\u6709\u6548\u6027\u4fbf\u6025\u5267\u4e0b\u964d\uff0c\u8868\u660e\u5b83\u66f4\u50cf\u662f\u4e00\u79cd\u4ece\u6570\u636e\u4e2d\u5b66\u5230\u7684\u8106\u5f31\u6a21\u5f0f\u800c\u975e\u901a\u7528\u5f52\u7eb3\u504f\u7f6e\u3002", "motivation": "\u5c3d\u7ba1\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u63d0\u793a\u88ab\u8ba4\u4e3a\u80fd\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6027\u80fd\u5e76\u4ea7\u751f\u7c7b\u4eba\u63a8\u7406\u6b65\u9aa4\uff0c\u4f46\u521d\u6b65\u53d1\u73b0\u6697\u793aCoT\u63a8\u7406\u53ef\u80fd\u6bd4\u8868\u9762\u4e0a\u66f4\u80a4\u6d45\u3002\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u63a2\u7a76CoT\u63a8\u7406\u662f\u5426\u4e3a\u4ece\u5185\u90e8\u6570\u636e\u5206\u5e03\u4e2d\u5b66\u4e60\u5230\u7684\u7ed3\u6784\u5316\u5f52\u7eb3\u504f\u7f6e\uff0c\u5e76\u53d7\u8bad\u7ec3\u6570\u636e\u4e0e\u6d4b\u8bd5\u67e5\u8be2\u4e4b\u95f4\u5206\u5e03\u5dee\u5f02\u7684\u9650\u5236\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u6570\u636e\u5206\u5e03\u89c6\u89d2\u5ba1\u89c6CoT\u63a8\u7406\uff0c\u5e76\u4ece\u4efb\u52a1\u3001\u957f\u5ea6\u548c\u683c\u5f0f\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u5256\u6790\u3002\u4e3a\u5b9e\u73b0\u6b64\u76ee\u7684\uff0c\u7814\u7a76\u56e2\u961f\u8bbe\u8ba1\u4e86DataAlchemy\uff0c\u8fd9\u662f\u4e00\u4e2a\u72ec\u7acb\u4e14\u53d7\u63a7\u7684\u73af\u5883\uff0c\u7528\u4e8e\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3LLM\uff0c\u5e76\u5728\u5404\u79cd\u4e0d\u540c\u7684\u5206\u5e03\u6761\u4ef6\u4e0b\u7cfb\u7edf\u5730\u63a2\u6d4b\u5b83\u4eec\u7684\u80fd\u529b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\uff0cCoT\u63a8\u7406\u662f\u4e00\u79cd\u8106\u5f31\u7684\u6d77\u5e02\u8703\u697c\uff0c\u5f53\u5176\u88ab\u63a8\u5230\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e4b\u5916\u65f6\uff0c\u5176\u6548\u679c\u4fbf\u4f1a\u6d88\u5931\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5bf9CoT\u63a8\u7406\u4f55\u65f6\u4ee5\u53ca\u4e3a\u4f55\u5931\u8d25\u7684\u66f4\u6df1\u5c42\u6b21\u7406\u89e3\uff0c\u5e76\u5f3a\u8c03\u4e86\u5b9e\u73b0\u771f\u6b63\u548c\u53ef\u6cdb\u5316\u63a8\u7406\u7684\u6301\u7eed\u6311\u6218\u3002"}}
{"id": "2508.01411", "pdf": "https://arxiv.org/pdf/2508.01411", "abs": "https://arxiv.org/abs/2508.01411", "authors": ["Rania Al-Sabbagh"], "title": "ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations", "categories": ["cs.CL"], "comment": null, "summary": "ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics,\nnovels, and TV show subtitles that are manually translated and aligned with\ntheir English counterparts. The dataset contains 25,557 segment pairs that can\nbe used to benchmark new machine translation models, fine-tune large language\nmodels in few-shot settings, and adapt commercial machine translation\napplications such as Google Translate. Additionally, the dataset is a valuable\nresource for research in various disciplines, including translation studies,\ncross-linguistic analysis, and lexical semantics. The dataset can also serve\npedagogical purposes by training translation students and aid professional\ntranslators as a translation memory. The contributions are twofold: first, the\ndataset features textual genres not found in existing parallel Egyptian Arabic\nand English datasets, and second, it is a gold-standard dataset that has been\ntranslated and aligned by human experts.", "AI": {"tldr": "ArzEn-MultiGenre\u662f\u4e00\u4e2a\u5305\u542b25,557\u6bb5\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u6b4c\u8bcd\u3001\u5c0f\u8bf4\u548c\u7535\u89c6\u5267\u5b57\u5e55\u53ca\u5176\u82f1\u8bed\u5bf9\u5e94\u5185\u5bb9\u7684\u5e76\u884c\u6570\u636e\u96c6\uff0c\u7531\u4eba\u5de5\u7ffb\u8bd1\u548c\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u5e76\u884c\u6570\u636e\u96c6\u7f3a\u4e4f\u7279\u5b9a\u4f53\u88c1\u548c\u9ad8\u8d28\u91cf\uff08\u4eba\u5de5\u6807\u6ce8\uff09\u6570\u636e\uff0c\u9650\u5236\u4e86\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u3001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4ee5\u53ca\u7ffb\u8bd1\u7814\u7a76\u548c\u6559\u5b66\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u4eba\u5de5\u7ffb\u8bd1\u548c\u5bf9\u9f50\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed\u6b4c\u8bcd\u3001\u5c0f\u8bf4\u548c\u7535\u89c6\u5267\u5b57\u5e55\uff0c\u5e76\u4e0e\u82f1\u8bed\u5bf9\u5e94\u5185\u5bb9\u5339\u914d\uff0c\u6784\u5efa\u4e86\u8be5\u5e76\u884c\u6570\u636e\u96c6\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86ArzEn-MultiGenre\u6570\u636e\u96c6\uff0c\u5305\u542b25,557\u4e2a\u8bed\u6bb5\u5bf9\u3002\u8be5\u6570\u636e\u96c6\u53ef\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u65b0\u7684\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u3001\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u9002\u5e94\u5546\u4e1a\u673a\u5668\u7ffb\u8bd1\u5e94\u7528\uff0c\u5e76\u4e3a\u7ffb\u8bd1\u7814\u7a76\u3001\u8de8\u8bed\u8a00\u5206\u6790\u548c\u8bcd\u6c47\u8bed\u4e49\u5b66\u7b49\u9886\u57df\u63d0\u4f9b\u8d44\u6e90\uff0c\u4e5f\u53ef\u7528\u4e8e\u7ffb\u8bd1\u6559\u5b66\u548c\u4f5c\u4e3a\u7ffb\u8bd1\u8bb0\u5fc6\u5e93\u3002", "conclusion": "ArzEn-MultiGenre\u6570\u636e\u96c6\u901a\u8fc7\u5f15\u5165\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u672a\u6db5\u76d6\u7684\u72ec\u7279\u6587\u672c\u4f53\u88c1\uff0c\u5e76\u4f5c\u4e3a\u4eba\u7c7b\u4e13\u5bb6\u7ffb\u8bd1\u548c\u5bf9\u9f50\u7684\u9ec4\u91d1\u6807\u51c6\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u57c3\u53ca\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u5e76\u884c\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u5177\u6709\u91cd\u8981\u7684\u7814\u7a76\u3001\u5e94\u7528\u548c\u6559\u5b66\u4ef7\u503c\u3002"}}
{"id": "2508.00912", "pdf": "https://arxiv.org/pdf/2508.00912", "abs": "https://arxiv.org/abs/2508.00912", "authors": ["Ziyao Wang", "Guoheng Sun", "Yexiao He", "Zheyu Shen", "Bowei Tian", "Ang Li"], "title": "Predictive Auditing of Hidden Tokens in LLM APIs via Reasoning Length Estimation", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Commercial LLM services often conceal internal reasoning traces while still\ncharging users for every generated token, including those from hidden\nintermediate steps, raising concerns of token inflation and potential\noverbilling. This gap underscores the urgent need for reliable token auditing,\nyet achieving it is far from straightforward: cryptographic verification (e.g.,\nhash-based signature) offers little assurance when providers control the entire\nexecution pipeline, while user-side prediction struggles with the inherent\nvariance of reasoning LLMs, where token usage fluctuates across domains and\nprompt styles. To bridge this gap, we present PALACE (Predictive Auditing of\nLLM APIs via Reasoning Token Count Estimation), a user-side framework that\nestimates hidden reasoning token counts from prompt-answer pairs without access\nto internal traces. PALACE introduces a GRPO-augmented adaptation module with a\nlightweight domain router, enabling dynamic calibration across diverse\nreasoning tasks and mitigating variance in token usage patterns. Experiments on\nmath, coding, medical, and general reasoning benchmarks show that PALACE\nachieves low relative error and strong prediction accuracy, supporting both\nfine-grained cost auditing and inflation detection. Taken together, PALACE\nrepresents an important first step toward standardized predictive auditing,\noffering a practical path to greater transparency, accountability, and user\ntrust.", "AI": {"tldr": "PALACE\u662f\u4e00\u4e2a\u7528\u6237\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4bLLM API\u7684\u9690\u85cf\u63a8\u7406\u4ee4\u724c\u6570\u91cf\uff0c\u65e8\u5728\u89e3\u51b3\u5546\u4e1aLLM\u670d\u52a1\u4e2d\u4ee4\u724c\u8ba1\u8d39\u4e0d\u900f\u660e\u7684\u95ee\u9898\u3002", "motivation": "\u5546\u4e1aLLM\u670d\u52a1\u5bf9\u9690\u85cf\u7684\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u6536\u8d39\uff0c\u5bfc\u81f4\u7528\u6237\u53ef\u80fd\u88ab\u8fc7\u5ea6\u8ba1\u8d39\u3002\u73b0\u6709\u5ba1\u8ba1\u65b9\u6cd5\uff0c\u5982\u52a0\u5bc6\u9a8c\u8bc1\u548c\u7528\u6237\u7aef\u9884\u6d4b\uff0c\u56e0\u63d0\u4f9b\u5546\u63a7\u5236\u548cLLM\u63a8\u7406\u7684\u4e0d\u786e\u5b9a\u6027\u800c\u65e0\u6548\uff0c\u6025\u9700\u53ef\u9760\u7684\u4ee4\u724c\u5ba1\u8ba1\u65b9\u6848\u3002", "method": "\u63d0\u51faPALACE\u6846\u67b6\uff0c\u4e00\u4e2a\u7528\u6237\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u5185\u90e8\u75d5\u8ff9\u5373\u53ef\u4ece\u63d0\u793a-\u56de\u7b54\u5bf9\u4e2d\u4f30\u7b97\u9690\u85cf\u7684\u63a8\u7406\u4ee4\u724c\u6570\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e86GRPO\u589e\u5f3a\u7684\u9002\u5e94\u6a21\u5757\u548c\u8f7b\u91cf\u7ea7\u9886\u57df\u8def\u7531\u5668\uff0c\u5b9e\u73b0\u8de8\u4e0d\u540c\u63a8\u7406\u4efb\u52a1\u7684\u52a8\u6001\u6821\u51c6\u5e76\u51cf\u8f7b\u4ee4\u724c\u4f7f\u7528\u6a21\u5f0f\u7684\u6ce2\u52a8\u3002", "result": "\u5728\u6570\u5b66\u3001\u7f16\u7801\u3001\u533b\u7597\u548c\u901a\u7528\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPALACE\u5b9e\u73b0\u4e86\u4f4e\u76f8\u5bf9\u8bef\u5dee\u548c\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u6709\u6548\u652f\u6301\u7ec6\u7c92\u5ea6\u6210\u672c\u5ba1\u8ba1\u548c\u4ee4\u724c\u901a\u80c0\u68c0\u6d4b\u3002", "conclusion": "PALACE\u4ee3\u8868\u4e86\u6807\u51c6\u5316\u9884\u6d4b\u5ba1\u8ba1\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u63d0\u5347LLM\u670d\u52a1\u7684\u900f\u660e\u5ea6\u3001\u95ee\u8d23\u5236\u548c\u7528\u6237\u4fe1\u4efb\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2508.01087", "pdf": "https://arxiv.org/pdf/2508.01087", "abs": "https://arxiv.org/abs/2508.01087", "authors": ["Ryan Rabinowitz", "Steve Cruz", "Walter Scheirer", "Terrance E. Boult"], "title": "COSTARR: Consolidated Open Set Technique with Attenuation for Robust Recognition", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025", "summary": "Handling novelty remains a key challenge in visual recognition systems.\nExisting open-set recognition (OSR) methods rely on the familiarity hypothesis,\ndetecting novelty by the absence of familiar features. We propose a novel\nattenuation hypothesis: small weights learned during training attenuate\nfeatures and serve a dual role-differentiating known classes while discarding\ninformation useful for distinguishing known from unknown classes. To leverage\nthis overlooked information, we present COSTARR, a novel approach that combines\nboth the requirement of familiar features and the lack of unfamiliar ones. We\nprovide a probabilistic interpretation of the COSTARR score, linking it to the\nlikelihood of correct classification and belonging in a known class. To\ndetermine the individual contributions of the pre- and post-attenuated features\nto COSTARR's performance, we conduct ablation studies that show both\npre-attenuated deep features and the underutilized post-attenuated Hadamard\nproduct features are essential for improving OSR. Also, we evaluate COSTARR in\na large-scale setting using ImageNet2012-1K as known data and NINCO,\niNaturalist, OpenImage-O, and other datasets as unknowns, across multiple\nmodern pre-trained architectures (ViTs, ConvNeXts, and ResNet). The experiments\ndemonstrate that COSTARR generalizes effectively across various architectures\nand significantly outperforms prior state-of-the-art methods by incorporating\npreviously discarded attenuation information, advancing open-set recognition\ncapabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u8870\u51cf\u5047\u8bbe\u201d\u548cCOSTARR\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u8bad\u7ec3\u4e2d\u88ab\u5ffd\u7565\u7684\u7279\u5f81\u8870\u51cf\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u96c6\u8bc6\u522b\uff08OSR\uff09\u6027\u80fd\uff0c\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u89c6\u89c9\u8bc6\u522b\u7cfb\u7edf\u5728\u5904\u7406\u65b0\u9896\u6027\u65b9\u9762\u9762\u4e34\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u5f00\u653e\u96c6\u8bc6\u522b\uff08OSR\uff09\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u201c\u719f\u6089\u5ea6\u5047\u8bbe\u201d\uff0c\u5373\u901a\u8fc7\u7f3a\u4e4f\u719f\u6089\u7279\u5f81\u6765\u68c0\u6d4b\u65b0\u9896\u6027\uff0c\u4f46\u8fd9\u53ef\u80fd\u5ffd\u7565\u4e86\u533a\u5206\u5df2\u77e5\u4e0e\u672a\u77e5\u7c7b\u522b\u7684\u6709\u7528\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u201c\u8870\u51cf\u5047\u8bbe\u201d\uff0c\u8ba4\u4e3a\u8bad\u7ec3\u4e2d\u5b66\u5230\u7684\u5fae\u5c0f\u6743\u91cd\u4f1a\u8870\u51cf\u7279\u5f81\uff0c\u5e76\u4e22\u5f03\u533a\u5206\u5df2\u77e5\u4e0e\u672a\u77e5\u7c7b\u522b\u7684\u6709\u7528\u4fe1\u606f\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51faCOSTARR\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5bf9\u719f\u6089\u7279\u5f81\u7684\u9700\u6c42\u548c\u7f3a\u4e4f\u4e0d\u719f\u6089\u7279\u5f81\u7684\u7279\u70b9\uff0c\u4ee5\u5229\u7528\u88ab\u5ffd\u7565\u7684\u8870\u51cf\u4fe1\u606f\u3002\u4e3aCOSTARR\u5206\u6570\u63d0\u4f9b\u4e86\u6982\u7387\u89e3\u91ca\u3002\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u8bc4\u4f30\u4e86\u8870\u51cf\u524d\u540e\u7279\u5f81\u7684\u8d21\u732e\u3002\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08ImageNet2012-1K\u4f5c\u4e3a\u5df2\u77e5\u6570\u636e\uff0cNINCO\u3001iNaturalist\u3001OpenImage-O\u7b49\u4f5c\u4e3a\u672a\u77e5\u6570\u636e\uff09\u548c\u591a\u79cd\u73b0\u4ee3\u9884\u8bad\u7ec3\u67b6\u6784\uff08ViTs\u3001ConvNeXts\u548cResNet\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u9884\u8870\u51cf\u7684\u6df1\u5ea6\u7279\u5f81\u548c\u672a\u5145\u5206\u5229\u7528\u7684\u540e\u8870\u51cfHadamard\u4e58\u79ef\u7279\u5f81\u5bf9\u4e8e\u6539\u8fdbOSR\u81f3\u5173\u91cd\u8981\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cCOSTARR\u5728\u5404\u79cd\u67b6\u6784\u4e2d\u90fd\u80fd\u6709\u6548\u6cdb\u5316\uff0c\u5e76\u4e14\u901a\u8fc7\u6574\u5408\u5148\u524d\u88ab\u4e22\u5f03\u7684\u8870\u51cf\u4fe1\u606f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5f00\u653e\u96c6\u8bc6\u522b\u65b9\u6cd5\u3002", "conclusion": "COSTARR\u901a\u8fc7\u6709\u6548\u5229\u7528\u5148\u524d\u88ab\u5ffd\u7565\u7684\u8870\u51cf\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u96c6\u8bc6\u522b\u80fd\u529b\uff0c\u5728\u5904\u7406\u89c6\u89c9\u8bc6\u522b\u4e2d\u7684\u65b0\u9896\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u5e76\u8d85\u8d8a\u4e86\u4ee5\u5f80\u7684\u5148\u8fdb\u6280\u672f\u6c34\u5e73\u3002"}}
{"id": "2508.01203", "pdf": "https://arxiv.org/pdf/2508.01203", "abs": "https://arxiv.org/abs/2508.01203", "authors": ["Junjie Shi", "Wei Ma", "Shi Ying", "Lingxiao Jiang", "Yang liu", "Bo Du"], "title": "Importance Sampling is All You Need: Predict LLM's performance on new benchmark by reusing existing benchmark", "categories": ["cs.AI"], "comment": null, "summary": "With the rapid advancement of large language models , code generation has\nbecome a key benchmark for evaluating LLM capabilities. However, existing\nbenchmarks face two major challenges: (1) the escalating cost of constructing\nhigh-quality test suites and reference solutions, and (2) the increasing risk\nof data contamination, which undermines the reliability of benchmark-based\nevaluations. In this paper, we propose BIS, a prompt-centric evaluation\nframework that enables ground-truth-free prediction of LLM performance on code\ngeneration tasks. Rather than executing generated code, BIS estimates\nperformance metrics by analyzing the prompt distribution alone. Built on\nimportance sampling theory and implemented using Importance Weighted\nAutoencoders, our method reweights samples from existing annotated benchmarks\nto estimate performance on new, unseen benchmarks. To stabilize the estimation,\nwe introduce weight truncation strategies and compute marginal expectations\nacross the fitted distributions. BIS serves as a complementary tool that\nsupports benchmark development and validation under constrained resources,\noffering actionable and quick feedback for prompt selection and contamination\nassessment. We conduct extensive experiments involving 8,000 evaluation points\nacross 4 CodeLlama models and 9 diverse benchmarks. Our framework achieves an\naverage absolute prediction error of 1.1% for code correctness scores, with\nbest- and worst-case errors of 0.3% and 1.9%, respectively. It also generalizes\nwell to other metrics, attaining average absolute errors of 2.15% for pass@1.\nThese results demonstrate the reliability and broad applicability of BIS, which\ncan significantly reduce the cost and effort of benchmarking LLMs in\ncode-related tasks.", "AI": {"tldr": "\u63d0\u51faBIS\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u63d0\u793a\u5206\u5e03\u800c\u975e\u6267\u884c\u4ee3\u7801\uff0c\u5b9e\u73b0LLM\u4ee3\u7801\u751f\u6210\u6027\u80fd\u7684\u65e0\u771f\u503c\u9884\u6d4b\uff0c\u6709\u6548\u964d\u4f4e\u57fa\u51c6\u6d4b\u8bd5\u6210\u672c\u548c\u6570\u636e\u6c61\u67d3\u98ce\u9669\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u6784\u5efa\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u5957\u4ef6\u548c\u53c2\u8003\u89e3\u51b3\u65b9\u6848\u7684\u6210\u672c\u4e0d\u65ad\u4e0a\u5347\uff1b\u6570\u636e\u6c61\u67d3\u98ce\u9669\u65e5\u76ca\u589e\u52a0\uff0c\u524a\u5f31\u4e86\u57fa\u4e8e\u57fa\u51c6\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51faBIS\uff0c\u4e00\u4e2a\u4ee5\u63d0\u793a\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u53ef\u5728\u65e0\u771f\u503c\u7684\u60c5\u51b5\u4e0b\u9884\u6d4bLLM\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u91cd\u8981\u6027\u91c7\u6837\u7406\u8bba\uff0c\u5e76\u4f7f\u7528\u91cd\u8981\u6027\u52a0\u6743\u81ea\u7f16\u7801\u5668\u5b9e\u73b0\uff0c\u901a\u8fc7\u4ec5\u5206\u6790\u63d0\u793a\u5206\u5e03\u6765\u4f30\u8ba1\u6027\u80fd\u6307\u6807\uff0c\u800c\u975e\u6267\u884c\u751f\u6210\u7684\u4ee3\u7801\u3002\u5b83\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u73b0\u6709\u6807\u6ce8\u57fa\u51c6\u7684\u6837\u672c\u6765\u4f30\u8ba1\u5728\u65b0\u3001\u672a\u89c1\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u6743\u91cd\u622a\u65ad\u7b56\u7565\u548c\u8ba1\u7b97\u62df\u5408\u5206\u5e03\u7684\u8fb9\u9645\u671f\u671b\u6765\u7a33\u5b9a\u4f30\u8ba1\u3002", "result": "\u5bf94\u4e2aCodeLlama\u6a21\u578b\u548c9\u4e2a\u4e0d\u540c\u57fa\u51c6\u8fdb\u884c\u4e868000\u4e2a\u8bc4\u4f30\u70b9\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u3002BIS\u6846\u67b6\u5728\u4ee3\u7801\u6b63\u786e\u6027\u5206\u6570\u4e0a\u5b9e\u73b0\u4e861.1%\u7684\u5e73\u5747\u7edd\u5bf9\u9884\u6d4b\u8bef\u5dee\uff0c\u6700\u4f73\u8bef\u5dee\u4e3a0.3%\uff0c\u6700\u5dee\u8bef\u5dee\u4e3a1.9%\u3002\u5bf9\u4e8epass@1\u7b49\u5176\u4ed6\u6307\u6807\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a2.15%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8bc1\u660e\u4e86BIS\u7684\u53ef\u9760\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4eLLM\u5728\u4ee3\u7801\u76f8\u5173\u4efb\u52a1\u4e2d\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u7684\u6210\u672c\u548c\u5de5\u4f5c\u91cf\uff0c\u5e76\u4e3a\u63d0\u793a\u9009\u62e9\u548c\u6c61\u67d3\u8bc4\u4f30\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u5feb\u901f\u53cd\u9988\u3002"}}
{"id": "2508.01412", "pdf": "https://arxiv.org/pdf/2508.01412", "abs": "https://arxiv.org/abs/2508.01412", "authors": ["Jinhao Pan", "Chahat Raj", "Ziwei Zhu"], "title": "Discovering Bias Associations through Open-Ended LLM Generations", "categories": ["cs.CL"], "comment": null, "summary": "Social biases embedded in Large Language Models (LLMs) raise critical\nconcerns, resulting in representational harms -- unfair or distorted portrayals\nof demographic groups -- that may be expressed in subtle ways through generated\nlanguage. Existing evaluation methods often depend on predefined\nidentity-concept associations, limiting their ability to surface new or\nunexpected forms of bias. In this work, we present the Bias Association\nDiscovery Framework (BADF), a systematic approach for extracting both known and\npreviously unrecognized associations between demographic identities and\ndescriptive concepts from open-ended LLM outputs. Through comprehensive\nexperiments spanning multiple models and diverse real-world contexts, BADF\nenables robust mapping and analysis of the varied concepts that characterize\ndemographic identities. Our findings advance the understanding of biases in\nopen-ended generation and provide a scalable tool for identifying and analyzing\nbias associations in LLMs. Data, code, and results are available at\nhttps://github.com/JP-25/Discover-Open-Ended-Generation", "AI": {"tldr": "LLMs\u7684\u793e\u4f1a\u504f\u89c1\u5bfc\u81f4\u8868\u5f81\u635f\u5bb3\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u53d1\u73b0\u65b0\u578b\u504f\u89c1\u3002\u672c\u6587\u63d0\u51faBADF\u6846\u67b6\uff0c\u53ef\u4ece\u5f00\u653e\u5f0fLLM\u8f93\u51fa\u4e2d\u63d0\u53d6\u5df2\u77e5\u548c\u672a\u77e5\u7684\u8eab\u4efd-\u6982\u5ff5\u5173\u8054\uff0c\u52a0\u6df1\u4e86\u5bf9\u504f\u89c1\u7684\u7406\u89e3\u5e76\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5206\u6790\u5de5\u5177\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u5d4c\u5165\u7684\u793e\u4f1a\u504f\u89c1\u5f15\u53d1\u4e86\u5bf9\u4eba\u53e3\u7fa4\u4f53\u4e0d\u516c\u5e73\u6216\u626d\u66f2\u63cf\u7ed8\uff08\u4ee3\u8868\u6027\u635f\u5bb3\uff09\u7684\u62c5\u5fe7\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u8eab\u4efd-\u6982\u5ff5\u5173\u8054\uff0c\u9650\u5236\u4e86\u5176\u53d1\u73b0\u65b0\u578b\u6216\u610f\u60f3\u4e0d\u5230\u504f\u89c1\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u504f\u89c1\u5173\u8054\u53d1\u73b0\u6846\u67b6\uff08BADF\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5f00\u653e\u5f0fLLM\u8f93\u51fa\u4e2d\u63d0\u53d6\u4eba\u53e3\u8eab\u4efd\u4e0e\u63cf\u8ff0\u6027\u6982\u5ff5\u4e4b\u95f4\u5df2\u77e5\u548c\u672a\u88ab\u8bc6\u522b\u7684\u5173\u8054\u3002", "result": "\u901a\u8fc7\u5bf9\u591a\u4e2a\u6a21\u578b\u548c\u4e0d\u540c\u771f\u5b9e\u4e16\u754c\u60c5\u5883\u7684\u5168\u9762\u5b9e\u9a8c\uff0cBADF\u80fd\u591f\u7a33\u5065\u5730\u6620\u5c04\u548c\u5206\u6790\u8868\u5f81\u4eba\u53e3\u8eab\u4efd\u7684\u5404\u79cd\u6982\u5ff5\u3002\u8fd9\u4e9b\u53d1\u73b0\u52a0\u6df1\u4e86\u5bf9\u5f00\u653e\u5f0f\u751f\u6210\u4e2d\u504f\u89c1\u7684\u7406\u89e3\u3002", "conclusion": "BADF\u63a8\u8fdb\u4e86\u5bf9\u5f00\u653e\u5f0f\u751f\u6210\u4e2d\u504f\u89c1\u7684\u7406\u89e3\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u5173\u8054\u3002"}}
{"id": "2508.00921", "pdf": "https://arxiv.org/pdf/2508.00921", "abs": "https://arxiv.org/abs/2508.00921", "authors": ["Khaled Eskaf"], "title": "SmartDate: AI-Driven Precision Sorting and Quality Control in Date Fruits", "categories": ["cs.LG", "cs.AI", "eess.IV", "I.2.1; I.2.6; I.2.9; I.4.8"], "comment": "6 pages, 2 figures, published in Proceedings of the 21st IEEE\n  International Conference on High Performance Computing and Networking (HONET\n  2024), Doha, Qatar, December 2024", "summary": "SmartDate is an AI-powered system for automated sorting and quality control\nof date fruits. It combines deep learning, genetic algorithms, and\nreinforcement learning to improve classification accuracy and predict shelf\nlife. The system uses high-resolution imaging and Visible-Near-Infrared\n(VisNIR) spectral sensors to evaluate key features such as moisture, sugar\ncontent, and texture. Reinforcement learning enables real-time adaptation to\nproduction conditions, while genetic algorithms optimize model parameters.\nSmartDate achieved 94.5 percent accuracy, 93.1 percent F1-score, and an AUC-ROC\nof 0.96. The system reduces waste and ensures that only high-quality dates\nreach the market, setting a new benchmark in smart agriculture.", "AI": {"tldr": "SmartDate\u662f\u4e00\u4e2a\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u3001\u9057\u4f20\u7b97\u6cd5\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u56fe\u50cf\u548c\u5149\u8c31\u4f20\u611f\u5668\uff0c\u5b9e\u73b0\u67a3\u679c\u81ea\u52a8\u5316\u5206\u62e3\u4e0e\u8d28\u91cf\u63a7\u5236\u7684AI\u7cfb\u7edf\u3002", "motivation": "\u81ea\u52a8\u5316\u67a3\u679c\u5206\u62e3\u4e0e\u8d28\u91cf\u63a7\u5236\uff0c\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u9884\u6d4b\u4fdd\u8d28\u671f\uff0c\u51cf\u5c11\u6d6a\u8d39\uff0c\u786e\u4fdd\u9ad8\u54c1\u8d28\u67a3\u679c\u4e0a\u5e02\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u3001\u9057\u4f20\u7b97\u6cd5\u548c\u5f3a\u5316\u5b66\u4e60\u3002\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\u548c\u53ef\u89c1-\u8fd1\u7ea2\u5916\uff08VisNIR\uff09\u5149\u8c31\u4f20\u611f\u5668\u8bc4\u4f30\u6c34\u5206\u3001\u7cd6\u542b\u91cf\u548c\u8d28\u5730\u7b49\u5173\u952e\u7279\u5f81\u3002\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u5b9e\u65f6\u9002\u5e94\uff0c\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u6a21\u578b\u53c2\u6570\u3002", "result": "\u7cfb\u7edf\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523094.5%\uff0cF1-\u5206\u657093.1%\uff0cAUC-ROC\u4e3a0.96\u3002", "conclusion": "SmartDate\u7cfb\u7edf\u6709\u6548\u51cf\u5c11\u4e86\u6d6a\u8d39\uff0c\u786e\u4fdd\u4e86\u4e0a\u5e02\u67a3\u679c\u7684\u9ad8\u54c1\u8d28\uff0c\u4e3a\u667a\u80fd\u519c\u4e1a\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2508.01095", "pdf": "https://arxiv.org/pdf/2508.01095", "abs": "https://arxiv.org/abs/2508.01095", "authors": ["Mikhail Bychkov", "Matey Yordanov", "Andrei Kuchma"], "title": "AURA: A Hybrid Spatiotemporal-Chromatic Framework for Robust, Real-Time Detection of Industrial Smoke Emissions", "categories": ["cs.CV"], "comment": "19 pages, 3 figures", "summary": "This paper introduces AURA, a novel hybrid spatiotemporal-chromatic framework\ndesigned for robust, real-time detection and classification of industrial smoke\nemissions. The framework addresses critical limitations of current monitoring\nsystems, which often lack the specificity to distinguish smoke types and\nstruggle with environmental variability. AURA leverages both the dynamic\nmovement patterns and the distinct color characteristics of industrial smoke to\nprovide enhanced accuracy and reduced false positives. This framework aims to\nsignificantly improve environmental compliance, operational safety, and public\nhealth outcomes by enabling precise, automated monitoring of industrial\nemissions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAURA\uff0c\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u65f6\u7a7a-\u8272\u5ea6\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u3001\u9c81\u68d2\u5730\u68c0\u6d4b\u548c\u5206\u7c7b\u5de5\u4e1a\u70df\u96fe\u6392\u653e\u3002", "motivation": "\u73b0\u6709\u5de5\u4e1a\u70df\u96fe\u76d1\u6d4b\u7cfb\u7edf\u5728\u533a\u5206\u70df\u96fe\u7c7b\u578b\u548c\u5e94\u5bf9\u73af\u5883\u591a\u53d8\u6027\u65b9\u9762\u5b58\u5728\u7279\u5f02\u6027\u4e0d\u8db3\u7684\u5c40\u9650\u6027\u3002", "method": "AURA\u5229\u7528\u5de5\u4e1a\u70df\u96fe\u7684\u52a8\u6001\u8fd0\u52a8\u6a21\u5f0f\u548c\u72ec\u7279\u7684\u989c\u8272\u7279\u5f81\uff0c\u7ed3\u5408\u65f6\u7a7a\u4e0e\u8272\u5ea6\u5206\u6790\u8fdb\u884c\u68c0\u6d4b\u548c\u5206\u7c7b\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u63d0\u4f9b\u589e\u5f3a\u7684\u51c6\u786e\u6027\u5e76\u51cf\u5c11\u8bef\u62a5\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u73b0\u7cbe\u786e\u3001\u81ea\u52a8\u5316\u7684\u5de5\u4e1a\u6392\u653e\u76d1\u6d4b\uff0cAURA\u65e8\u5728\u663e\u8457\u6539\u5584\u73af\u5883\u5408\u89c4\u6027\u3001\u64cd\u4f5c\u5b89\u5168\u6027\u548c\u516c\u5171\u5065\u5eb7\u3002"}}
{"id": "2508.01208", "pdf": "https://arxiv.org/pdf/2508.01208", "abs": "https://arxiv.org/abs/2508.01208", "authors": ["Mingchen Mei", "Yi Li", "YiYao Qian", "Zijun Jia"], "title": "Calibrated Prediction Set in Fault Detection with Risk Guarantees via Significance Tests", "categories": ["cs.AI"], "comment": null, "summary": "Fault detection is crucial for ensuring the safety and reliability of modern\nindustrial systems. However, a significant scientific challenge is the lack of\nrigorous risk control and reliable uncertainty quantification in existing\ndiagnostic models, particularly when facing complex scenarios such as\ndistributional shifts. To address this issue, this paper proposes a novel fault\ndetection method that integrates significance testing with the conformal\nprediction framework to provide formal risk guarantees. The method transforms\nfault detection into a hypothesis testing task by defining a nonconformity\nmeasure based on model residuals. It then leverages a calibration dataset to\ncompute p-values for new samples, which are used to construct prediction sets\nmathematically guaranteed to contain the true label with a user-specified\nprobability, $1-\\alpha$. Fault classification is subsequently performed by\nanalyzing the intersection of the constructed prediction set with predefined\nnormal and fault label sets. Experimental results on cross-domain fault\ndiagnosis tasks validate the theoretical properties of our approach. The\nproposed method consistently achieves an empirical coverage rate at or above\nthe nominal level ($1-\\alpha$), demonstrating robustness even when the\nunderlying point-prediction models perform poorly. Furthermore, the results\nreveal a controllable trade-off between the user-defined risk level ($\\alpha$)\nand efficiency, where higher risk tolerance leads to smaller average prediction\nset sizes. This research contributes a theoretically grounded framework for\nfault detection that enables explicit risk control, enhancing the\ntrustworthiness of diagnostic systems in safety-critical applications and\nadvancing the field from simple point predictions to informative,\nuncertainty-aware outputs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u663e\u8457\u6027\u68c0\u9a8c\u548c\u5171\u5f62\u9884\u6d4b\u7684\u6545\u969c\u68c0\u6d4b\u65b0\u65b9\u6cd5\uff0c\u53ef\u5728\u5b58\u5728\u5206\u5e03\u504f\u79fb\u7684\u590d\u6742\u573a\u666f\u4e0b\u63d0\u4f9b\u4e25\u683c\u7684\u98ce\u9669\u63a7\u5236\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u73b0\u6709\u6545\u969c\u8bca\u65ad\u6a21\u578b\u5728\u9762\u5bf9\u5206\u5e03\u504f\u79fb\u7b49\u590d\u6742\u573a\u666f\u65f6\uff0c\u7f3a\u4e4f\u4e25\u683c\u7684\u98ce\u9669\u63a7\u5236\u548c\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u8fd9\u5bf9\u4e8e\u786e\u4fdd\u73b0\u4ee3\u5de5\u4e1a\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u6545\u969c\u68c0\u6d4b\u8f6c\u5316\u4e3a\u5047\u8bbe\u68c0\u9a8c\u4efb\u52a1\uff0c\u901a\u8fc7\u57fa\u4e8e\u6a21\u578b\u6b8b\u5dee\u5b9a\u4e49\u4e0d\u7b26\u5408\u5ea6\u91cf\uff0c\u5e76\u5229\u7528\u6821\u51c6\u6570\u636e\u96c6\u8ba1\u7b97p\u503c\u6765\u6784\u5efa\u9884\u6d4b\u96c6\uff0c\u6570\u5b66\u4e0a\u4fdd\u8bc1\u4ee5\u7528\u6237\u6307\u5b9a\u6982\u7387\u5305\u542b\u771f\u5b9e\u6807\u7b7e\u3002\u6545\u969c\u5206\u7c7b\u901a\u8fc7\u5206\u6790\u9884\u6d4b\u96c6\u4e0e\u9884\u5b9a\u4e49\u6807\u7b7e\u96c6\u7684\u4ea4\u96c6\u8fdb\u884c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u4ea4\u53c9\u57df\u6545\u969c\u8bca\u65ad\u4efb\u52a1\u4e2d\u7ecf\u9a8c\u8986\u76d6\u7387\u59cb\u7ec8\u8fbe\u5230\u6216\u8d85\u8fc7\u6807\u79f0\u6c34\u5e73\uff081-$\\alpha$\uff09\uff0c\u5373\u4f7f\u5e95\u5c42\u70b9\u9884\u6d4b\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u4e5f\u663e\u793a\u51fa\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u7528\u6237\u5b9a\u4e49\u7684\u98ce\u9669\u6c34\u5e73\uff08$\\alpha$\uff09\u4e0e\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u53ef\u63a7\u7684\u6743\u8861\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6545\u969c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u57fa\u7840\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u663e\u5f0f\u98ce\u9669\u63a7\u5236\uff0c\u589e\u5f3a\u4e86\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u8bca\u65ad\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\uff0c\u5e76\u5c06\u8be5\u9886\u57df\u4ece\u7b80\u5355\u7684\u70b9\u9884\u6d4b\u63a8\u8fdb\u5230\u4fe1\u606f\u4e30\u5bcc\u3001\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u8f93\u51fa\u3002"}}
{"id": "2508.01424", "pdf": "https://arxiv.org/pdf/2508.01424", "abs": "https://arxiv.org/abs/2508.01424", "authors": ["Haonan Bian", "Yutao Qi", "Rui Yang", "Yuanxi Che", "Jiaqian Wang", "Heming Xia", "Ranran Zhen"], "title": "From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), despite their success in question answering,\nexhibit limitations in complex multi-hop question answering (MQA) tasks that\nnecessitate non-linear, structured reasoning. This limitation stems from their\ninability to adequately capture deep conceptual relationships between entities.\nTo overcome this challenge, we present **ORACLE** (**O**ntology-driven\n**R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a\ntraining-free framework that combines LLMs' generative capabilities with the\nstructural benefits of knowledge graphs. Our approach operates through three\nstages: (1) dynamic construction of question-specific knowledge ontologies\nusing LLMs, (2) transformation of these ontologies into First-Order Logic\nreasoning chains, and (3) systematic decomposition of the original query into\nlogically coherent sub-questions. Experimental results on several standard MQA\nbenchmarks show that our framework achieves highly competitive performance,\nrivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses\nfurther confirm the effectiveness of each component, while demonstrating that\nour method generates more logical and interpretable reasoning chains than\nexisting approaches.", "AI": {"tldr": "ORACLE\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u751f\u6210\u80fd\u529b\u4e0e\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u4f18\u52bf\uff0c\u5229\u7528\u672c\u4f53\u9a71\u52a8\u7684\u63a8\u7406\u94fe\u548c\u903b\u8f91\u5b50\u95ee\u9898\u5206\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\uff08MQA\uff09\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u96be\u4ee5\u6355\u6349\u5b9e\u4f53\u95f4\u6df1\u5c42\u6982\u5ff5\u5173\u7cfb\u5e76\u8fdb\u884c\u975e\u7ebf\u6027\u3001\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u56e0\u6b64\u9700\u8981\u514b\u670d\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51faORACLE\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1) \u5229\u7528LLMs\u52a8\u6001\u6784\u5efa\u95ee\u9898\u7279\u5b9a\u7684\u77e5\u8bc6\u672c\u4f53\uff1b2) \u5c06\u8fd9\u4e9b\u672c\u4f53\u8f6c\u5316\u4e3a\u4e00\u9636\u903b\u8f91\u63a8\u7406\u94fe\uff1b3) \u7cfb\u7edf\u5730\u5c06\u539f\u59cb\u67e5\u8be2\u5206\u89e3\u4e3a\u903b\u8f91\u8fde\u8d2f\u7684\u5b50\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u6807\u51c6MQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6846\u67b6\u53d6\u5f97\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5ab2\u7f8eDeepSeek-R1\u7b49\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002\u8be6\u7ec6\u5206\u6790\u8bc1\u5b9e\u4e86\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u6709\u6548\u6027\uff0c\u5e76\u8868\u660e\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u63a8\u7406\u94fe\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5177\u903b\u8f91\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408LLMs\u7684\u751f\u6210\u80fd\u529b\u548c\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u4f18\u52bf\uff0cORACLE\u6846\u67b6\u6709\u6548\u514b\u670d\u4e86LLMs\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u63a8\u7406\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u7684\u63a8\u7406\u8fc7\u7a0b\u3002"}}
{"id": "2508.00922", "pdf": "https://arxiv.org/pdf/2508.00922", "abs": "https://arxiv.org/abs/2508.00922", "authors": ["Jinsoo Bae", "Seoung Bum Kim", "Hyungrok Do"], "title": "CaliMatch: Adaptive Calibration for Improving Safe Semi-supervised Learning", "categories": ["cs.LG"], "comment": null, "summary": "Semi-supervised learning (SSL) uses unlabeled data to improve the performance\nof machine learning models when labeled data is scarce. However, its real-world\napplications often face the label distribution mismatch problem, in which the\nunlabeled dataset includes instances whose ground-truth labels are absent from\nthe labeled training dataset. Recent studies, referred to as safe SSL, have\naddressed this issue by using both classification and out-of-distribution (OOD)\ndetection. However, the existing methods may suffer from overconfidence in deep\nneural networks, leading to increased SSL errors because of high confidence in\nincorrect pseudo-labels or OOD detection. To address this, we propose a novel\nmethod, CaliMatch, which calibrates both the classifier and the OOD detector to\nfoster safe SSL. CaliMatch presents adaptive label smoothing and temperature\nscaling, which eliminates the need to manually tune the smoothing degree for\neffective calibration. We give a theoretical justification for why improving\nthe calibration of both the classifier and the OOD detector is crucial in safe\nSSL. Extensive evaluations on CIFAR-10, CIFAR-100, SVHN, TinyImageNet, and\nImageNet demonstrate that CaliMatch outperforms the existing methods in safe\nSSL tasks.", "AI": {"tldr": "\u63d0\u51faCaliMatch\u65b9\u6cd5\uff0c\u901a\u8fc7\u6821\u51c6\u5206\u7c7b\u5668\u548cOOD\u68c0\u6d4b\u5668\uff0c\u89e3\u51b3\u5b89\u5168\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u6a21\u578b\u8fc7\u81ea\u4fe1\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5b89\u5168\u534a\u76d1\u7763\u5b66\u4e60\uff08Safe SSL\uff09\u65b9\u6cd5\uff0c\u5728\u5904\u7406\u6807\u7b7e\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\u65f6\uff0c\u6613\u53d7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8fc7\u81ea\u4fe1\u5f71\u54cd\uff0c\u5bfc\u81f4\u9519\u8bef\u7684\u4f2a\u6807\u7b7e\u6216OOD\u68c0\u6d4b\uff0c\u589e\u52a0SSL\u8bef\u5dee\u3002", "method": "\u63d0\u51faCaliMatch\u65b9\u6cd5\uff0c\u901a\u8fc7\u6821\u51c6\u5206\u7c7b\u5668\u548cOOD\u68c0\u6d4b\u5668\u6765\u4fc3\u8fdb\u5b89\u5168SSL\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u81ea\u9002\u5e94\u6807\u7b7e\u5e73\u6ed1\u548c\u6e29\u5ea6\u7f29\u653e\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u5e73\u6ed1\u5ea6\u3002\u6587\u7ae0\u8fd8\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u3001SVHN\u3001TinyImageNet\u548cImageNet\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cCaliMatch\u5728\u5b89\u5168SSL\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CaliMatch\u901a\u8fc7\u6709\u6548\u6821\u51c6\u5206\u7c7b\u5668\u548cOOD\u68c0\u6d4b\u5668\uff0c\u89e3\u51b3\u4e86\u5b89\u5168\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u8fc7\u81ea\u4fe1\u95ee\u9898\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.01098", "pdf": "https://arxiv.org/pdf/2508.01098", "abs": "https://arxiv.org/abs/2508.01098", "authors": ["Yuekun Dai", "Haitian Li", "Shangchen Zhou", "Chen Change Loy"], "title": "Trans-Adapter: A Plug-and-Play Framework for Transparent Image Inpainting", "categories": ["cs.CV"], "comment": "accepted to ICCV 2025", "summary": "RGBA images, with the additional alpha channel, are crucial for any\napplication that needs blending, masking, or transparency effects, making them\nmore versatile than standard RGB images. Nevertheless, existing image\ninpainting methods are designed exclusively for RGB images. Conventional\napproaches to transparent image inpainting typically involve placing a\nbackground underneath RGBA images and employing a two-stage process: image\ninpainting followed by image matting. This pipeline, however, struggles to\npreserve transparency consistency in edited regions, and matting can introduce\njagged edges along transparency boundaries. To address these challenges, we\npropose Trans-Adapter, a plug-and-play adapter that enables diffusion-based\ninpainting models to process transparent images directly. Trans-Adapter also\nsupports controllable editing via ControlNet and can be seamlessly integrated\ninto various community models. To evaluate our method, we introduce LayerBench,\nalong with a novel non-reference alpha edge quality evaluation metric for\nassessing transparency edge quality. We conduct extensive experiments on\nLayerBench to demonstrate the effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51faTrans-Adapter\uff0c\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u9002\u914d\u5668\uff0c\u4f7f\u57fa\u4e8e\u6269\u6563\u7684\u4fee\u590d\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u5904\u7406\u900f\u660e\uff08RGBA\uff09\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u900f\u660e\u5ea6\u4e00\u81f4\u6027\u548c\u8fb9\u7f18\u8d28\u91cf\u4e0a\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\u4ec5\u4e3aRGB\u56fe\u50cf\u8bbe\u8ba1\u3002\u4f20\u7edf\u7684\u900f\u660e\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5\uff08\u80cc\u666f\u653e\u7f6e+\u4e24\u9636\u6bb5\uff1a\u4fee\u590d\u540e\u62a0\u56fe\uff09\u96be\u4ee5\u4fdd\u6301\u7f16\u8f91\u533a\u57df\u7684\u900f\u660e\u5ea6\u4e00\u81f4\u6027\uff0c\u4e14\u62a0\u56fe\u53ef\u80fd\u5728\u900f\u660e\u8fb9\u754c\u5f15\u5165\u952f\u9f7f\u72b6\u8fb9\u7f18\u3002RGBA\u56fe\u50cf\u5bf9\u4e8e\u9700\u8981\u6df7\u5408\u3001\u906e\u7f69\u6216\u900f\u660e\u6548\u679c\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faTrans-Adapter\uff0c\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u9002\u914d\u5668\uff0c\u4f7f\u57fa\u4e8e\u6269\u6563\u7684\u4fee\u590d\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u5904\u7406\u900f\u660e\u56fe\u50cf\u3002Trans-Adapter\u8fd8\u652f\u6301\u901a\u8fc7ControlNet\u8fdb\u884c\u53ef\u63a7\u7f16\u8f91\uff0c\u5e76\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u5404\u79cd\u793e\u533a\u6a21\u578b\u4e2d\u3002\u4e3a\u4e86\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5f15\u5165\u4e86LayerBench\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e00\u79cd\u65b0\u9896\u7684\u975e\u53c2\u8003alpha\u8fb9\u7f18\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u901a\u8fc7\u5728LayerBench\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "Trans-Adapter\u6210\u529f\u89e3\u51b3\u4e86RGBA\u56fe\u50cf\u4fee\u590d\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u63a5\u4e14\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u4fdd\u6301\u4e86\u900f\u660e\u5ea6\u4e00\u81f4\u6027\u5e76\u63d0\u5347\u4e86\u8fb9\u7f18\u8d28\u91cf\u3002"}}
{"id": "2508.01237", "pdf": "https://arxiv.org/pdf/2508.01237", "abs": "https://arxiv.org/abs/2508.01237", "authors": ["Cheng Tan", "Qi Chen", "Jingxuan Wei", "Gaowei Wu", "Zhangyang Gao", "Siyuan Li", "Bihui Yu", "Ruifeng Guo", "Stan Z. Li"], "title": "SketchAgent: Generating Structured Diagrams from Hand-Drawn Sketches", "categories": ["cs.AI"], "comment": "Accepted by IJCAI 2025", "summary": "Hand-drawn sketches are a natural and efficient medium for capturing and\nconveying ideas. Despite significant advancements in controllable natural image\ngeneration, translating freehand sketches into structured, machine-readable\ndiagrams remains a labor-intensive and predominantly manual task. The primary\nchallenge stems from the inherent ambiguity of sketches, which lack the\nstructural constraints and semantic precision required for automated diagram\ngeneration. To address this challenge, we introduce SketchAgent, a multi-agent\nsystem designed to automate the transformation of hand-drawn sketches into\nstructured diagrams. SketchAgent integrates sketch recognition, symbolic\nreasoning, and iterative validation to produce semantically coherent and\nstructurally accurate diagrams, significantly reducing the need for manual\neffort. To evaluate the effectiveness of our approach, we propose the\nSketch2Diagram Benchmark, a comprehensive dataset and evaluation framework\nencompassing eight diverse diagram categories, such as flowcharts, directed\ngraphs, and model architectures. The dataset comprises over 6,000 high-quality\nexamples with token-level annotations, standardized preprocessing, and rigorous\nquality control. By streamlining the diagram generation process, SketchAgent\nholds great promise for applications in design, education, and engineering,\nwhile offering a significant step toward bridging the gap between intuitive\nsketching and machine-readable diagram generation. The benchmark is released at\nhttps://huggingface.co/datasets/DiagramAgent/Sketch2Diagram-Benchmark.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86SketchAgent\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u624b\u7ed8\u8349\u56fe\u5230\u7ed3\u6784\u5316\u56fe\u8868\u7684\u8f6c\u6362\uff0c\u5e76\u63d0\u51fa\u4e86Sketch2Diagram\u57fa\u51c6\u6570\u636e\u96c6\u4ee5\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u7136\u56fe\u50cf\u751f\u6210\u6280\u672f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5c06\u6a21\u7cca\u7684\u624b\u7ed8\u8349\u56fe\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u3001\u673a\u5668\u53ef\u8bfb\u7684\u56fe\u8868\u4ecd\u662f\u4e00\u9879\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u624b\u52a8\u4efb\u52a1\u3002\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u8349\u56fe\u56fa\u6709\u7684\u6a21\u7cca\u6027\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u56fe\u8868\u751f\u6210\u6240\u9700\u7684\u7ed3\u6784\u7ea6\u675f\u548c\u8bed\u4e49\u7cbe\u5ea6\u3002", "method": "\u5f15\u5165\u4e86SketchAgent\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u96c6\u6210\u8349\u56fe\u8bc6\u522b\u3001\u7b26\u53f7\u63a8\u7406\u548c\u8fed\u4ee3\u9a8c\u8bc1\uff0c\u4ee5\u751f\u6210\u8bed\u4e49\u8fde\u8d2f\u4e14\u7ed3\u6784\u51c6\u786e\u7684\u56fe\u8868\u3002\u4e3a\u8bc4\u4f30\u5176\u6709\u6548\u6027\uff0c\u63d0\u51fa\u4e86Sketch2Diagram\u57fa\u51c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u516b\u79cd\u56fe\u8868\u7c7b\u522b\uff08\u5982\u6d41\u7a0b\u56fe\u3001\u6709\u5411\u56fe\uff09\u7684\u7efc\u5408\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u62e5\u67096000\u591a\u4e2a\u9ad8\u8d28\u91cf\u5e26\u6807\u6ce8\u7684\u793a\u4f8b\u3002", "result": "SketchAgent\u80fd\u591f\u751f\u6210\u8bed\u4e49\u8fde\u8d2f\u4e14\u7ed3\u6784\u51c6\u786e\u7684\u56fe\u8868\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u624b\u52a8\u5de5\u4f5c\u91cf\u3002\u901a\u8fc7\u5f15\u5165Sketch2Diagram\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u6570\u636e\u96c6\u3002", "conclusion": "SketchAgent\u901a\u8fc7\u7b80\u5316\u56fe\u8868\u751f\u6210\u8fc7\u7a0b\uff0c\u5728\u8bbe\u8ba1\u3001\u6559\u80b2\u548c\u5de5\u7a0b\u9886\u57df\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\uff0c\u5e76\u4e3a\u5f25\u5408\u76f4\u89c2\u8349\u56fe\u4e0e\u673a\u5668\u53ef\u8bfb\u56fe\u8868\u751f\u6210\u4e4b\u95f4\u7684\u9e3f\u6c9f\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2508.01450", "pdf": "https://arxiv.org/pdf/2508.01450", "abs": "https://arxiv.org/abs/2508.01450", "authors": ["Xinlin Zhuang", "Feilong Tang", "Haolin Yang", "Ming Hu", "Huifa Li", "Haochen Xue", "Yichen Li", "Junjun He", "Zongyuan Ge", "Ying Qian", "Imran Razzak"], "title": "Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data", "categories": ["cs.CL"], "comment": "preprint, under review", "summary": "Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language\nModels (LLMs) to specialized domains such as medical reasoning. However,\nexisting SFT practices often rely on unfiltered datasets that contain redundant\nand low-quality samples, leading to substantial computational costs and\nsuboptimal performance. Although existing methods attempt to alleviate this\nproblem by selecting data based on sample difficulty, defined by knowledge and\nreasoning complexity, they overlook each sample's optimization utility\nreflected in its gradient. Interestingly, we find that gradient-based influence\nalone favors easy-to-optimize samples that cause large parameter shifts but\nlack deep reasoning chains, while difficulty alone selects noisy or overly\ncomplex cases that fail to guide stable optimization. Based on this\nobservation, we propose a data selection strategy, Difficulty-Influence\nQuadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence\nquadrant to balance complex clinical reasoning with substantial gradient\ninfluence, enabling efficient medical reasoning with minimal fine-tuning data.\nFurthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected\nsubsets demonstrate higher data quality and generate clinical reasoning that is\nmore aligned with expert practices in differential diagnosis, safety check, and\nevidence citation, as DIQ emphasizes samples that foster expert-like reasoning\npatterns. Extensive experiments on medical reasoning benchmarks demonstrate\nthat DIQ enables models fine-tuned on only 1% of selected data to match\nfull-dataset performance, while using 10% consistently outperforms the\nbaseline, highlighting the superiority of principled data selection over\nbrute-force scaling. The code and data are available at\nhttps://github.com/mihara-bot/DIQ.", "AI": {"tldr": "\u9488\u5bf9LLM\u5728\u533b\u5b66\u9886\u57df\u7684SFT\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51faDIQ\u65b9\u6cd5\uff0c\u7ed3\u5408\u6837\u672c\u96be\u5ea6\u4e0e\u68af\u5ea6\u5f71\u54cd\u9009\u62e9\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u5b9e\u73b0\u9ad8\u6548\u533b\u5b66\u63a8\u7406\u6a21\u578b\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709LLM\u5728\u533b\u5b66\u9886\u57df\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u65f6\uff0c\u5e38\u4f9d\u8d56\u672a\u7ecf\u7b5b\u9009\u7684\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u6570\u636e\u5197\u4f59\u3001\u8d28\u91cf\u4f4e\u4e0b\uff0c\u8fdb\u800c\u4ea7\u751f\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c\u548c\u6b21\u4f18\u6027\u80fd\u3002\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u591a\u57fa\u4e8e\u6837\u672c\u96be\u5ea6\uff0c\u4f46\u5ffd\u89c6\u4e86\u68af\u5ea6\u4fe1\u606f\uff0c\u4e14\u5355\u72ec\u4f7f\u7528\u96be\u5ea6\u6216\u68af\u5ea6\u90fd\u6709\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u201c\u96be\u5ea6-\u5f71\u54cd\u8c61\u9650 (Difficulty-Influence Quadrant, DIQ)\u201d\u6570\u636e\u9009\u62e9\u7b56\u7565\u3002\u8be5\u7b56\u7565\u6839\u636e\u6837\u672c\u96be\u5ea6\u548c\u68af\u5ea6\u5f71\u54cd\uff0c\u4f18\u5148\u9009\u62e9\u4f4d\u4e8e\u201c\u9ad8\u96be\u5ea6-\u9ad8\u5f71\u54cd\u201d\u8c61\u9650\u7684\u6837\u672c\uff0c\u65e8\u5728\u5e73\u8861\u590d\u6742\u7684\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u4e0e\u663e\u8457\u7684\u53c2\u6570\u4f18\u5316\u8d21\u732e\u3002", "result": "DIQ\u9009\u62e9\u7684\u6570\u636e\u5b50\u96c6\u5177\u6709\u66f4\u9ad8\u8d28\u91cf\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u80fd\u751f\u6210\u66f4\u7b26\u5408\u4e13\u5bb6\u5b9e\u8df5\u7684\u4e34\u5e8a\u63a8\u7406\u3002\u5728\u533b\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u4ec5\u4f7f\u75281%\u7684DIQ\u9009\u62e9\u6570\u636e\u5373\u53ef\u8fbe\u5230\u5168\u6570\u636e\u96c6\u7684\u6027\u80fd\uff0c\u4f7f\u752810%\u7684\u6570\u636e\u5219\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u6c34\u5e73\u3002", "conclusion": "DIQ\u901a\u8fc7\u7ed3\u5408\u6837\u672c\u96be\u5ea6\u548c\u68af\u5ea6\u5f71\u54cd\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u539f\u7406\u9a71\u52a8\u7684\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u533b\u5b66\u63a8\u7406\u9886\u57df\u7684\u5fae\u8c03\u6548\u7387\u548c\u63a8\u7406\u8d28\u91cf\uff0c\u8bc1\u660e\u4e86\u7cbe\u786e\u6570\u636e\u9009\u62e9\u4f18\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u76f2\u76ee\u6269\u5c55\u3002"}}
{"id": "2508.00923", "pdf": "https://arxiv.org/pdf/2508.00923", "abs": "https://arxiv.org/abs/2508.00923", "authors": ["Jiazhen Pan", "Bailiang Jian", "Paul Hager", "Yundi Zhang", "Che Liu", "Friedrike Jungmann", "Hongwei Bran Li", "Chenyu You", "Junde Wu", "Jiayuan Zhu", "Fenglin Liu", "Yuyuan Liu", "Niklas Bubeck", "Christian Wachinger", "Chen", "Chen", "Zhenyu Gong", "Cheng Ouyang", "Georgios Kaissis", "Benedikt Wiestler", "Daniel Rueckert"], "title": "Beyond Benchmarks: Dynamic, Automatic And Systematic Red-Teaming Agents For Trustworthy Medical Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Ensuring the safety and reliability of large language models (LLMs) in\nclinical practice is critical to prevent patient harm and promote trustworthy\nhealthcare applications of AI. However, LLMs are advancing so rapidly that\nstatic safety benchmarks often become obsolete upon publication, yielding only\nan incomplete and sometimes misleading picture of model trustworthiness. We\ndemonstrate that a Dynamic, Automatic, and Systematic (DAS) red-teaming\nframework that continuously stress-tests LLMs can reveal significant weaknesses\nof current LLMs across four safety-critical domains: robustness, privacy,\nbias/fairness, and hallucination. A suite of adversarial agents is applied to\nautonomously mutate test cases, identify/evolve unsafe-triggering strategies,\nand evaluate responses, uncovering vulnerabilities in real time without human\nintervention. Applying DAS to 15 proprietary and open-source LLMs revealed a\nstark contrast between static benchmark performance and vulnerability under\nadversarial pressure. Despite a median MedQA accuracy exceeding 80\\%, 94\\% of\npreviously correct answers failed our dynamic robustness tests. We observed\nsimilarly high failure rates across other domains: privacy leaks were elicited\nin 86\\% of scenarios, cognitive-bias priming altered clinical recommendations\nin 81\\% of fairness tests, and we identified hallucination rates exceeding 66\\%\nin widely used models. Such profound residual risks are incompatible with\nroutine clinical practice. By converting red-teaming from a static checklist\ninto a dynamic stress-test audit, DAS red-teaming offers the surveillance that\nhospitals/regulators/technology vendors require as LLMs become embedded in\npatient chatbots, decision-support dashboards, and broader healthcare\nworkflows. Our framework delivers an evolvable, scalable, and reliable\nsafeguard for the next generation of medical AI.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u3001\u81ea\u52a8\u5316\u3001\u7cfb\u7edf\u5316\u7684\uff08DAS\uff09\u7ea2\u961f\u6d4b\u8bd5\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u7597\u5e94\u7528\u4e2d\u5b58\u5728\u7684\u4e25\u91cd\u5b89\u5168\u7f3a\u9677\uff0c\u8fd9\u4e9b\u7f3a\u9677\u662f\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u53d1\u73b0\u7684\u3002", "motivation": "\u786e\u4fddLLM\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u9632\u6b62\u5bf9\u60a3\u8005\u9020\u6210\u4f24\u5bb3\u5e76\u4fc3\u8fdbAI\u5728\u533b\u7597\u9886\u57df\u7684\u4fe1\u4efb\u3002\u7136\u800c\uff0cLLM\u53d1\u5c55\u8fc5\u901f\uff0c\u73b0\u6709\u9759\u6001\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u53d1\u5e03\u540e\u5f88\u5feb\u8fc7\u65f6\uff0c\u65e0\u6cd5\u5168\u9762\u3001\u51c6\u786e\u5730\u53cd\u6620\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u5e76\u5e94\u7528\u4e86\u4e00\u4e2a\u52a8\u6001\u3001\u81ea\u52a8\u5316\u3001\u7cfb\u7edf\u5316\u7684\uff08DAS\uff09\u7ea2\u961f\u6d4b\u8bd5\u6846\u67b6\u3002\u8be5\u6846\u67b6\u6301\u7eed\u5bf9LLM\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\uff0c\u4f7f\u7528\u4e00\u7cfb\u5217\u5bf9\u6297\u6027\u4ee3\u7406\u81ea\u4e3b\u53d8\u5f02\u6d4b\u8bd5\u7528\u4f8b\uff0c\u8bc6\u522b/\u6f14\u8fdb\u4e0d\u5b89\u5168\u89e6\u53d1\u7b56\u7565\uff0c\u5e76\u5b9e\u65f6\u8bc4\u4f30\u54cd\u5e94\uff0c\u4ece\u800c\u5728\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\u53d1\u73b0\u6f0f\u6d1e\u3002\u8be5\u6846\u67b6\u5e94\u7528\u4e8e15\u4e2a\u4e13\u6709\u548c\u5f00\u6e90LLM\uff0c\u91cd\u70b9\u6d4b\u8bd5\u4e86\u9c81\u68d2\u6027\u3001\u9690\u79c1\u3001\u504f\u89c1/\u516c\u5e73\u6027\u548c\u5e7b\u89c9\u56db\u4e2a\u5b89\u5168\u5173\u952e\u9886\u57df\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1LLM\u5728\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MedQA\u51c6\u786e\u7387\u8d85\u8fc780%\uff09\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5bf9\u6297\u6027\u538b\u529b\u4e0b\uff0c\u5176\u8106\u5f31\u6027\u663e\u8457\uff1a94%\u5148\u524d\u6b63\u786e\u7684\u7b54\u6848\u5728\u52a8\u6001\u9c81\u68d2\u6027\u6d4b\u8bd5\u4e2d\u5931\u8d25\uff1b86%\u7684\u573a\u666f\u4e2d\u51fa\u73b0\u9690\u79c1\u6cc4\u9732\uff1b81%\u7684\u516c\u5e73\u6027\u6d4b\u8bd5\u4e2d\u8ba4\u77e5\u504f\u5dee\u6539\u53d8\u4e86\u4e34\u5e8a\u5efa\u8bae\uff1b\u5e38\u7528\u6a21\u578b\u5e7b\u89c9\u7387\u8d85\u8fc766%\u3002", "conclusion": "\u5f53\u524dLLM\u5b58\u5728\u7684\u5de8\u5927\u6b8b\u4f59\u98ce\u9669\u4e0e\u5e38\u89c4\u4e34\u5e8a\u5b9e\u8df5\u4e0d\u517c\u5bb9\u3002DAS\u7ea2\u961f\u6d4b\u8bd5\u6846\u67b6\u5c06\u7ea2\u961f\u6d4b\u8bd5\u4ece\u9759\u6001\u6e05\u5355\u8f6c\u53d8\u4e3a\u52a8\u6001\u538b\u529b\u6d4b\u8bd5\u5ba1\u8ba1\uff0c\u4e3a\u533b\u9662\u3001\u76d1\u7ba1\u673a\u6784\u548c\u6280\u672f\u4f9b\u5e94\u5546\u63d0\u4f9b\u4e86\u5fc5\u8981\u7684\u76d1\u6d4b\u5de5\u5177\uff0c\u4ee5\u786e\u4fdd\u533b\u7597AI\u4e0b\u4e00\u4ee3\u5e94\u7528\uff08\u5982\u60a3\u8005\u804a\u5929\u673a\u5668\u4eba\u548c\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff09\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002\u8be5\u6846\u67b6\u4e3a\u533b\u7597AI\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6f14\u8fdb\u3001\u53ef\u6269\u5c55\u3001\u53ef\u9760\u7684\u4fdd\u969c\u63aa\u65bd\u3002"}}
{"id": "2508.01112", "pdf": "https://arxiv.org/pdf/2508.01112", "abs": "https://arxiv.org/abs/2508.01112", "authors": ["Yizhou Zhao", "Haoyu Chen", "Chunjiang Liu", "Zhenyang Li", "Charles Herrmann", "Junhwa Hur", "Yinxiao Li", "Ming-Hsuan Yang", "Bhiksha Raj", "Min Xu"], "title": "MASIV: Toward Material-Agnostic System Identification from Videos", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "System identification from videos aims to recover object geometry and\ngoverning physical laws. Existing methods integrate differentiable rendering\nwith simulation but rely on predefined material priors, limiting their ability\nto handle unknown ones. We introduce MASIV, the first vision-based framework\nfor material-agnostic system identification. Unlike existing approaches that\ndepend on hand-crafted constitutive laws, MASIV employs learnable neural\nconstitutive models, inferring object dynamics without assuming a\nscene-specific material prior. However, the absence of full particle state\ninformation imposes unique challenges, leading to unstable optimization and\nphysically implausible behaviors. To address this, we introduce dense geometric\nguidance by reconstructing continuum particle trajectories, providing\ntemporally rich motion constraints beyond sparse visual cues. Comprehensive\nexperiments show that MASIV achieves state-of-the-art performance in geometric\naccuracy, rendering quality, and generalization ability.", "AI": {"tldr": "MASIV\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u89c6\u89c9\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u672c\u6784\u6a21\u578b\u548c\u7a20\u5bc6\u51e0\u4f55\u5f15\u5bfc\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u77e5\u6750\u6599\u7684\u7cfb\u7edf\u8bc6\u522b\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6750\u6599\u5148\u9a8c\u7684\u4f9d\u8d56\uff0c\u5e76\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4ece\u89c6\u9891\u8fdb\u884c\u7cfb\u7edf\u8bc6\u522b\u7684\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u6750\u6599\u5148\u9a8c\u77e5\u8bc6\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5904\u7406\u672a\u77e5\u6750\u6599\u7684\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86MASIV\uff0c\u9996\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u6750\u6599\u65e0\u5173\u7cfb\u7edf\u8bc6\u522b\u6846\u67b6\u3002MASIV\u91c7\u7528\u53ef\u5b66\u4e60\u7684\u795e\u7ecf\u672c\u6784\u6a21\u578b\u6765\u63a8\u65ad\u7269\u4f53\u52a8\u529b\u5b66\uff0c\u65e0\u9700\u9884\u8bbe\u573a\u666f\u7279\u5b9a\u7684\u6750\u6599\u5148\u9a8c\u3002\u4e3a\u89e3\u51b3\u7f3a\u4e4f\u5b8c\u6574\u7c92\u5b50\u72b6\u6001\u4fe1\u606f\u5bfc\u81f4\u7684\u4f18\u5316\u4e0d\u7a33\u5b9a\u548c\u7269\u7406\u4e0d\u5408\u7406\u884c\u4e3a\uff0c\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u7a20\u5bc6\u51e0\u4f55\u5f15\u5bfc\uff0c\u901a\u8fc7\u91cd\u5efa\u8fde\u7eed\u7c92\u5b50\u8f68\u8ff9\u63d0\u4f9b\u4e30\u5bcc\u7684\u65f6\u5e8f\u8fd0\u52a8\u7ea6\u675f\u3002", "result": "\u5168\u9762\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMASIV\u5728\u51e0\u4f55\u7cbe\u5ea6\u3001\u6e32\u67d3\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MASIV\u6846\u67b6\u901a\u8fc7\u5176\u65b0\u9896\u7684\u795e\u7ecf\u672c\u6784\u6a21\u578b\u548c\u7a20\u5bc6\u51e0\u4f55\u5f15\u5bfc\u673a\u5236\uff0c\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edf\u7cfb\u7edf\u8bc6\u522b\u65b9\u6cd5\u5bf9\u6750\u6599\u5148\u9a8c\u7684\u4f9d\u8d56\uff0c\u4e3a\u4ece\u89c6\u9891\u4e2d\u8bc6\u522b\u672a\u77e5\u6750\u6599\u548c\u7269\u7406\u89c4\u5f8b\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01261", "pdf": "https://arxiv.org/pdf/2508.01261", "abs": "https://arxiv.org/abs/2508.01261", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models", "categories": ["cs.AI"], "comment": null, "summary": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment.", "AI": {"tldr": "MoE-MLA-RoPE\u662f\u4e00\u79cd\u7ed3\u5408MoE\u3001MLA\u548cRoPE\u7684\u65b0\u578b\u67b6\u6784\uff0c\u65e8\u5728\u5b9e\u73b0\u9ad8\u6548\u8bed\u8a00\u5efa\u6a21\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u5e76\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u4e2d\u6a21\u578b\u5bb9\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u9ad8\u6548\u90e8\u7f72\u3002", "method": "\u63d0\u51faMoE-MLA-RoPE\u67b6\u6784\uff0c\u5305\u542b\u4e09\u9879\u5173\u952e\u521b\u65b0\uff1a1) \u7ec6\u7c92\u5ea6\u4e13\u5bb6\u8def\u7531\uff0864\u4e2a\u5fae\u4e13\u5bb6\uff0ctop-k\u9009\u62e9\uff0c3.6 * 10^7\u79cd\u7ec4\u5408\uff09\uff1b2) \u5171\u4eab\u4e13\u5bb6\u9694\u79bb\uff082\u4e2a\u5e38\u9a7b\u4e13\u5bb6\uff0c6\u4e2a\u8def\u7531\u81f362\u4e2a\u4e13\u4e1a\u4e13\u5bb6\uff09\uff1b3) \u65e0\u68af\u5ea6\u51b2\u7a81\u7684\u8d1f\u8f7d\u5747\u8861\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMoE-MLA-RoPE\u5728\u538b\u7f29\u6bd4r=d/2\u65f6\uff0c\u5b9e\u73b068%\u7684KV\u7f13\u5b58\u5185\u5b58\u51cf\u5c11\u548c3.2\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u4ec5\u5e26\u67650.8%\u7684\u56f0\u60d1\u5ea6\u4e0b\u964d\u3002\u76f8\u8f83\u4e8e\u540c\u53c2\u6570\u91cf\u7684Vanilla Transformer\uff0c\u9a8c\u8bc1\u635f\u5931\u964d\u4f4e6.9%\uff0c\u6bcf\u6b21\u524d\u5411\u4f20\u64ad\u6d3b\u8dc3\u53c2\u6570\u51cf\u5c1142%\u3002\u5728FLOP\u5339\u914d\u4e0b\uff0c\u6027\u80fd\u63d0\u534711.1%\uff0c\u63a8\u7406\u52a0\u901f3.2\u500d\u3002GPT-4\u8bc4\u4f30\u8868\u660e\u751f\u6210\u8d28\u91cf\u5728\u8fde\u8d2f\u6027\u3001\u521b\u9020\u6027\u548c\u8bed\u6cd5\u6b63\u786e\u6027\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u67b6\u6784\u521b\u65b0\u800c\u975e\u5355\u7eaf\u7684\u53c2\u6570\u6269\u5c55\uff0c\u662f\u5b9a\u4e49\u8d44\u6e90\u53d7\u9650\u4e0b\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u6548\u7387\u524d\u6cbf\u7684\u5173\u952e\u3002"}}
{"id": "2508.01473", "pdf": "https://arxiv.org/pdf/2508.01473", "abs": "https://arxiv.org/abs/2508.01473", "authors": ["Yiming Zeng", "Jinghan Cao", "Zexin Li", "Yiming Chen", "Tao Ren", "Dawei Xiang", "Xidong Wu", "Shangqian Gao", "Tingting Yu"], "title": "TreeDiff: AST-Guided Code Generation with Diffusion LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in diffusion-based language models have opened new\npossibilities for controllable and bidirectional sequence generation. These\nmodels provide an alternative to traditional autoregressive approaches by\nframing text generation as an iterative denoising process. However, applying\ndiffusion models to structured domains such as source code remains a\nsignificant challenge. Programming languages differ from natural language in\nthat they follow strict syntactic and semantic rules, with hierarchical\norganization that must be preserved for correctness. Standard token-level\ncorruption techniques used during training often ignore this structure, which\nmay hinder the model's ability to learn meaningful representations of code. To\naddress this limitation, we propose a syntax-aware diffusion framework that\nincorporates structural priors from Abstract Syntax Trees (ASTs) into the\ndenoising process. Instead of masking individual tokens at random, we\nselectively corrupt syntactically meaningful code spans derived from AST\nsubtrees. This enables the model to reconstruct programs in a way that respects\ngrammatical boundaries and captures long-range dependencies. Experimental\nresults demonstrate that syntax-aware corruption significantly improves\nsyntactic correctness, reconstruction accuracy, and generalization to unseen\ncode patterns. These findings highlight the potential of incorporating\nstructural information into diffusion-based training and suggest that\nsyntax-guided denoising is a promising direction for advancing diffusion-based\nlanguage models in code generation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u6cd5\u611f\u77e5\u7684\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u4fe1\u606f\u8fdb\u884c\u8bed\u6cd5\u76f8\u5173\u7684\u4ee3\u7801\u7834\u574f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6269\u6563\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8bed\u6cd5\u6b63\u786e\u6027\u3001\u91cd\u5efa\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u5728\u5e8f\u5217\u751f\u6210\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5c06\u5176\u5e94\u7528\u4e8e\u6e90\u4ee3\u7801\u7b49\u7ed3\u6784\u5316\u9886\u57df\u9762\u4e34\u6311\u6218\u3002\u7f16\u7a0b\u8bed\u8a00\u7684\u4e25\u683c\u8bed\u6cd5\u8bed\u4e49\u548c\u5206\u5c42\u7ec4\u7ec7\u5fc5\u987b\u4fdd\u6301\u6b63\u786e\u6027\uff0c\u800c\u4f20\u7edf\u7684\u9010\u4ee4\u724c\u7834\u574f\u65b9\u6cd5\u5ffd\u7565\u4e86\u8fd9\u79cd\u7ed3\u6784\uff0c\u963b\u788d\u4e86\u6a21\u578b\u5b66\u4e60\u4ee3\u7801\u7684\u6709\u6548\u8868\u793a\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u6cd5\u611f\u77e5\u7684\u6269\u6563\u6846\u67b6\uff0c\u5c06\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u7684\u7ed3\u6784\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u53bb\u566a\u8fc7\u7a0b\u3002\u901a\u8fc7\u9009\u62e9\u6027\u5730\u7834\u574f\u6765\u81eaAST\u5b50\u6811\u7684\u8bed\u6cd5\u6709\u610f\u4e49\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u800c\u4e0d\u662f\u968f\u673a\u63a9\u76d6\u5355\u4e2a\u4ee4\u724c\uff0c\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u4ee5\u5c0a\u91cd\u8bed\u6cd5\u8fb9\u754c\u5e76\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u7684\u65b9\u5f0f\u91cd\u5efa\u7a0b\u5e8f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8bed\u6cd5\u611f\u77e5\u7684\u7834\u574f\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u7684\u8bed\u6cd5\u6b63\u786e\u6027\u3001\u91cd\u5efa\u51c6\u786e\u6027\u4ee5\u53ca\u5bf9\u672a\u89c1\u8fc7\u4ee3\u7801\u6a21\u5f0f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5c06\u7ed3\u6784\u4fe1\u606f\u6574\u5408\u5230\u57fa\u4e8e\u6269\u6563\u7684\u8bad\u7ec3\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u8868\u660e\u8bed\u6cd5\u5f15\u5bfc\u7684\u53bb\u566a\u662f\u63a8\u52a8\u57fa\u4e8e\u6269\u6563\u7684\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u53d1\u5c55\u7684\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2508.00926", "pdf": "https://arxiv.org/pdf/2508.00926", "abs": "https://arxiv.org/abs/2508.00926", "authors": ["Feng Xu", "Hui Wang", "Yuting Huang", "Danwei Zhang", "Zizhu Fan"], "title": "Hybrid Hypergraph Networks for Multimodal Sequence Data Classification", "categories": ["cs.LG"], "comment": "9 pages, 5 figures", "summary": "Modeling temporal multimodal data poses significant challenges in\nclassification tasks, particularly in capturing long-range temporal\ndependencies and intricate cross-modal interactions. Audiovisual data, as a\nrepresentative example, is inherently characterized by strict temporal order\nand diverse modalities. Effectively leveraging the temporal structure is\nessential for understanding both intra-modal dynamics and inter-modal\ncorrelations. However, most existing approaches treat each modality\nindependently and rely on shallow fusion strategies, which overlook temporal\ndependencies and hinder the model's ability to represent complex structural\nrelationships. To address the limitation, we propose the hybrid hypergraph\nnetwork (HHN), a novel framework that models temporal multimodal data via a\nsegmentation-first, graph-later strategy. HHN splits sequences into timestamped\nsegments as nodes in a heterogeneous graph. Intra-modal structures are captured\nvia hyperedges guided by a maximum entropy difference criterion, enhancing node\nheterogeneity and structural discrimination, followed by hypergraph convolution\nto extract high-order dependencies. Inter-modal links are established through\ntemporal alignment and graph attention for semantic fusion. HHN achieves\nstate-of-the-art (SOTA) results on four multimodal datasets, demonstrating its\neffectiveness in complex classification tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u8d85\u56fe\u7f51\u7edc\uff08HHN\uff09\uff0c\u901a\u8fc7\u5206\u6bb5\u548c\u56fe\u5efa\u6a21\u6709\u6548\u89e3\u51b3\u65f6\u5e8f\u591a\u6a21\u6001\u6570\u636e\u5206\u7c7b\u4e2d\u957f\u7a0b\u65f6\u5e8f\u4f9d\u8d56\u548c\u590d\u6742\u8de8\u6a21\u6001\u4ea4\u4e92\u7684\u6311\u6218\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u65f6\u5e8f\u591a\u6a21\u6001\u6570\u636e\u5206\u7c7b\u65f6\uff0c\u96be\u4ee5\u6709\u6548\u6355\u83b7\u957f\u7a0b\u65f6\u5e8f\u4f9d\u8d56\u548c\u590d\u6742\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u3002\u591a\u6570\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u5404\u6a21\u6001\u5e76\u91c7\u7528\u6d45\u5c42\u878d\u5408\u7b56\u7565\uff0c\u8fd9\u5ffd\u89c6\u4e86\u5173\u952e\u7684\u65f6\u5e8f\u4f9d\u8d56\u6027\uff0c\u5e76\u963b\u788d\u4e86\u6a21\u578b\u8868\u793a\u590d\u6742\u7ed3\u6784\u5173\u7cfb\u7684\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u6df7\u5408\u8d85\u56fe\u7f51\u7edc\uff08HHN\uff09\uff0c\u91c7\u7528\u201c\u5148\u5206\u6bb5\uff0c\u540e\u5efa\u56fe\u201d\u7684\u7b56\u7565\u6765\u5efa\u6a21\u65f6\u5e8f\u591a\u6a21\u6001\u6570\u636e\u3002HHN\u5c06\u5e8f\u5217\u5206\u5272\u6210\u5e26\u65f6\u95f4\u6233\u7684\u7247\u6bb5\u4f5c\u4e3a\u5f02\u6784\u56fe\u7684\u8282\u70b9\u3002\u6a21\u6001\u5185\u7ed3\u6784\u901a\u8fc7\u7531\u6700\u5927\u71b5\u5dee\u5f02\u51c6\u5219\u5f15\u5bfc\u7684\u8d85\u8fb9\u6355\u83b7\uff0c\u5e76\u5229\u7528\u8d85\u56fe\u5377\u79ef\u63d0\u53d6\u9ad8\u9636\u4f9d\u8d56\u3002\u6a21\u6001\u95f4\u94fe\u63a5\u5219\u901a\u8fc7\u65f6\u5e8f\u5bf9\u9f50\u548c\u56fe\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u8bed\u4e49\u878d\u5408\u3002", "result": "HHN\u5728\u56db\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u7ed3\u679c\u3002", "conclusion": "HHN\u5728\u590d\u6742\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u5730\u89e3\u51b3\u4e86\u65f6\u5e8f\u591a\u6a21\u6001\u6570\u636e\u5efa\u6a21\u4e2d\u957f\u671f\u65f6\u5e8f\u4f9d\u8d56\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\u7684\u6311\u6218\u3002"}}
{"id": "2508.01119", "pdf": "https://arxiv.org/pdf/2508.01119", "abs": "https://arxiv.org/abs/2508.01119", "authors": ["Saba Ahmadi", "Rabiul Awal", "Ankur Sikarwar", "Amirhossein Kazemnejad", "Ge Ya Luo", "Juan A. Rodriguez", "Sai Rajeswar", "Siva Reddy", "Christopher Pal", "Benno Krojer", "Aishwarya Agrawal"], "title": "The Promise of RL for Autoregressive Image Editing", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We explore three strategies to enhance performance on a wide range of image\nediting tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and\nChain-of-Thought (CoT) reasoning. In order to study all these components in one\nconsistent framework, we adopt an autoregressive multimodal model that\nprocesses textual and visual tokens in a unified manner. We find RL combined\nwith a large multi-modal LLM verifier to be the most effective of these\nstrategies. As a result, we release EARL: Editing with Autoregression and RL, a\nstrong RL-based image editing model that performs competitively on a diverse\nrange of edits compared to strong baselines, despite using much less training\ndata. Thus, EARL pushes the frontier of autoregressive multimodal models on\nimage editing. We release our code, training data, and trained models at\nhttps://github.com/mair-lab/EARL.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8SFT\u3001RL\u548cCoT\u4e09\u79cd\u7b56\u7565\u5728\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u7ed3\u5408\u5927\u578b\u591a\u6a21\u6001LLM\u9a8c\u8bc1\u5668\u7684RL\u6700\u6709\u6548\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51faRL\u9a71\u52a8\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578bEARL\uff0c\u5176\u5728\u591a\u79cd\u7f16\u8f91\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u4e14\u6240\u9700\u8bad\u7ec3\u6570\u636e\u66f4\u5c11\u3002", "motivation": "\u65e8\u5728\u63d0\u5347\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u63a2\u7d22\u4e0d\u540c\u7684\u8bad\u7ec3\u7b56\u7565\u4ee5\u8fbe\u5230\u8fd9\u4e00\u76ee\u6807\u3002", "method": "\u91c7\u7528\u81ea\u56de\u5f52\u591a\u6a21\u6001\u6a21\u578b\uff0c\u7edf\u4e00\u5904\u7406\u6587\u672c\u548c\u89c6\u89c9token\u3002\u7814\u7a76\u5e76\u6bd4\u8f83\u4e86\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3001\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u601d\u7ef4\u94fe\uff08CoT\uff09\u4e09\u79cd\u7b56\u7565\u3002\u6700\u7ec8\u53d1\u73b0\u7ed3\u5408\u5927\u578b\u591a\u6a21\u6001LLM\u9a8c\u8bc1\u5668\u7684\u5f3a\u5316\u5b66\u4e60\u6700\u4e3a\u6709\u6548\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7ed3\u5408\u5927\u578b\u591a\u6a21\u6001LLM\u9a8c\u8bc1\u5668\u88ab\u8bc1\u660e\u662f\u6700\u6709\u6548\u7684\u7b56\u7565\u3002\u57fa\u4e8e\u6b64\uff0c\u5f00\u53d1\u4e86EARL\uff08Editing with Autoregression and RL\uff09\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u591a\u79cd\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u4e0a\u4e0e\u5f3a\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u91cf\u66f4\u5c11\u3002", "conclusion": "EARL\u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u9886\u57df\u7684\u81ea\u56de\u5f52\u591a\u6a21\u6001\u6a21\u578b\u524d\u6cbf\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u5c11\u6570\u636e\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.01268", "pdf": "https://arxiv.org/pdf/2508.01268", "abs": "https://arxiv.org/abs/2508.01268", "authors": ["Roya Arkhmammadova", "Hosein Madadi Tamar", "M. Emre Gursoy"], "title": "Win-k: Improved Membership Inference Attacks on Small Language Models", "categories": ["cs.AI", "cs.CR"], "comment": null, "summary": "Small language models (SLMs) are increasingly valued for their efficiency and\ndeployability in resource-constrained environments, making them useful for\non-device, privacy-sensitive, and edge computing applications. On the other\nhand, membership inference attacks (MIAs), which aim to determine whether a\ngiven sample was used in a model's training, are an important threat with\nserious privacy and intellectual property implications. In this paper, we study\nMIAs on SLMs. Although MIAs were shown to be effective on large language models\n(LLMs), they are relatively less studied on emerging SLMs, and furthermore,\ntheir effectiveness decreases as models get smaller. Motivated by this finding,\nwe propose a new MIA called win-k, which builds on top of a state-of-the-art\nattack (min-k). We experimentally evaluate win-k by comparing it with five\nexisting MIAs using three datasets and eight SLMs. Results show that win-k\noutperforms existing MIAs in terms of AUROC, TPR @ 1% FPR, and FPR @ 99% TPR\nmetrics, especially on smaller models.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u4e0a\u7684\u6210\u5458\u63a8\u65ad\u653b\u51fb\uff08MIA\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3awin-k\u7684\u65b0\u578b\u653b\u51fb\uff0c\u5e76\u5728SLM\u4e0a\uff0c\u7279\u522b\u662f\u5c0f\u578bSLM\u4e0a\uff0c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u653b\u51fb\u3002", "motivation": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u56e0\u5176\u6548\u7387\u548c\u53ef\u90e8\u7f72\u6027\u800c\u5907\u53d7\u91cd\u89c6\uff0c\u4f46\u6210\u5458\u63a8\u65ad\u653b\u51fb\uff08MIA\uff09\u5bf9\u5176\u9690\u79c1\u6784\u6210\u5a01\u80c1\u3002\u73b0\u6709MIA\u5728SLM\u4e0a\u7814\u7a76\u4e0d\u8db3\uff0c\u4e14\u5176\u6709\u6548\u6027\u968f\u6a21\u578b\u5c3a\u5bf8\u51cf\u5c0f\u800c\u964d\u4f4e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3awin-k\u7684\u65b0\u578b\u6210\u5458\u63a8\u65ad\u653b\u51fb\uff0c\u8be5\u653b\u51fb\u5728\u73b0\u6709\u6700\u5148\u8fdb\u7684min-k\u653b\u51fb\u57fa\u7840\u4e0a\u6784\u5efa\u3002\u901a\u8fc7\u4f7f\u7528\u4e09\u4e2a\u6570\u636e\u96c6\u548c\u516b\u4e2aSLM\uff0c\u5c06win-k\u4e0e\u4e94\u79cd\u73b0\u6709MIA\u8fdb\u884c\u4e86\u5b9e\u9a8c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cwin-k\u5728AUROC\u3001TPR @ 1% FPR\u548cFPR @ 99% TPR\u7b49\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709MIA\uff0c\u5c24\u5176\u5728\u8f83\u5c0f\u7684\u6a21\u578b\u4e0a\u8868\u73b0\u66f4\u4e3a\u7a81\u51fa\u3002", "conclusion": "win-k\u653b\u51fb\u80fd\u66f4\u6709\u6548\u5730\u5bf9SLM\uff0c\u7279\u522b\u662f\u5c0f\u578bSLM\u6267\u884c\u6210\u5458\u63a8\u65ad\uff0c\u89e3\u51b3\u4e86\u73b0\u6709MIA\u5728\u5c0f\u578b\u6a21\u578b\u4e0a\u6709\u6548\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2508.01480", "pdf": "https://arxiv.org/pdf/2508.01480", "abs": "https://arxiv.org/abs/2508.01480", "authors": ["Dimitra Panou", "Alexandros C. Dimopoulos", "Manolis Koubarakis", "Martin Reczko"], "title": "Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach", "categories": ["cs.CL"], "comment": null, "summary": "Biomedical text mining and question-answering are essential yet highly\ndemanding tasks, particularly in the face of the exponential growth of\nbiomedical literature. In this work, we present our participation in the 13th\nedition of the BioASQ challenge, which involves biomedical semantic\nquestion-answering for Task 13b and biomedical question-answering for\ndeveloping topics for the Synergy task. We deploy a selection of open-source\nlarge language models (LLMs) as retrieval-augmented generators to answer\nbiomedical questions. Various models are used to process the questions. A\nmajority voting system combines their output to determine the final answer for\nYes/No questions, while for list and factoid type questions, the union of their\nanswers in used. We evaluated 13 state-of-the-art open source LLMs, exploring\nall possible model combinations to contribute to the final answer, resulting in\ntailored LLM pipelines for each question type. Our findings provide valuable\ninsight into which combinations of LLMs consistently produce superior results\nfor specific question types. In the four rounds of the 2025 BioASQ challenge,\nour system achieved notable results: in the Synergy task, we secured 1st place\nfor ideal answers and 2nd place for exact answers in round 2, as well as two\nshared 1st places for exact answers in round 3 and 4.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53c2\u4e0eBioASQ\u6311\u6218\u8d5b\uff0c\u5229\u7528\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5668\uff0c\u5e76\u7ed3\u5408\u6295\u7968/\u8054\u5408\u7b56\u7565\u5904\u7406\u751f\u7269\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\uff0c\u5728\u591a\u8f6e\u6311\u6218\u4e2d\u53d6\u5f97\u663e\u8457\u6392\u540d\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u6587\u672c\u6316\u6398\u548c\u95ee\u7b54\u662f\u5173\u952e\u4f46\u6781\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5728\u751f\u7269\u533b\u5b66\u6587\u732e\u5448\u6307\u6570\u7ea7\u589e\u957f\u7684\u80cc\u666f\u4e0b\u3002", "method": "\u90e8\u7f72\u4e86\u4e00\u7cfb\u5217\u5f00\u6e90LLMs\u4f5c\u4e3a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5668\u6765\u56de\u7b54\u751f\u7269\u533b\u5b66\u95ee\u9898\u3002\u9488\u5bf9\u201c\u662f/\u5426\u201d\u95ee\u9898\u91c7\u7528\u591a\u6570\u6295\u7968\u7cfb\u7edf\uff0c\u5bf9\u5217\u8868\u548c\u4e8b\u5b9e\u6027\u95ee\u9898\u5219\u91c7\u7528\u7b54\u6848\u7684\u5e76\u96c6\u3002\u8bc4\u4f30\u4e8613\u4e2a\u6700\u5148\u8fdb\u7684LLMs\u53ca\u5176\u6240\u6709\u53ef\u80fd\u7ec4\u5408\uff0c\u4e3a\u4e0d\u540c\u95ee\u9898\u7c7b\u578b\u5b9a\u5236\u4e86LLM\u7ba1\u9053\u3002", "result": "\u5728BioASQ 2025\u6311\u6218\u8d5b\u7684Synergy\u4efb\u52a1\u4e2d\uff0c\u8be5\u7cfb\u7edf\u5728\u7b2c2\u8f6e\u83b7\u5f97\u7406\u60f3\u7b54\u6848\u7b2c1\u540d\u548c\u7cbe\u786e\u7b54\u6848\u7b2c2\u540d\uff1b\u5728\u7b2c3\u8f6e\u548c\u7b2c4\u8f6e\u4e24\u6b21\u83b7\u5f97\u7cbe\u786e\u7b54\u6848\u5e76\u5217\u7b2c1\u540d\u3002\u7814\u7a76\u63ed\u793a\u4e86\u7279\u5b9aLLM\u7ec4\u5408\u5728\u4e0d\u540c\u95ee\u9898\u7c7b\u578b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5LLM\u9a71\u52a8\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408\u5b9a\u5236\u7684\u7ec4\u5408\u7b56\u7565\uff0c\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728BioASQ\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u5730\u4f4d\uff0c\u4e3a\u7279\u5b9a\u95ee\u9898\u7c7b\u578b\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684LLM\u7ec4\u5408\u6d1e\u5bdf\u3002"}}
{"id": "2508.00930", "pdf": "https://arxiv.org/pdf/2508.00930", "abs": "https://arxiv.org/abs/2508.00930", "authors": ["M. Ontivero-Ortega", "A. Fania", "A. Lacalamita", "R. Bellotti", "A. Monaco", "S. Stramaglia"], "title": "Cooperative effects in feature importance of individual patterns: application to air pollutants and Alzheimer disease", "categories": ["cs.LG", "physics.data-an"], "comment": null, "summary": "Leveraging recent advances in the analysis of synergy and redundancy in\nsystems of random variables, an adaptive version of the widely used metric\nLeave One Covariate Out (LOCO) has been recently proposed to quantify\ncooperative effects in feature importance (Hi-Fi), a key technique in\nexplainable artificial intelligence (XAI), so as to disentangle high-order\neffects involving a particular input feature in regression problems.\nDifferently from standard feature importance tools, where a single score\nmeasures the relevance of each feature, each feature is here characterized by\nthree scores, a two-body (unique) score and higher-order scores (redundant and\nsynergistic). This paper presents a framework to assign those three scores\n(unique, redundant, and synergistic) to each individual pattern of the data\nset, while comparing it with the well-known measure of feature importance named\n{\\it Shapley effect}. To illustrate the potential of the proposed framework, we\nfocus on a One-Health application: the relation between air pollutants and\nAlzheimer's disease mortality rate. Our main result is the synergistic\nassociation between features related to $O_3$ and $NO_2$ with mortality,\nespecially in the provinces of Bergamo e Brescia; notably also the density of\nurban green areas displays synergistic influence with pollutants for the\nprediction of AD mortality. Our results place local Hi-Fi as a promising tool\nof wide applicability, which opens new perspectives for XAI as well as to\nanalyze high-order relationships in complex systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06Hi-Fi\uff08\u4e00\u79cd\u8861\u91cf\u7279\u5f81\u72ec\u7279\u3001\u5197\u4f59\u548c\u534f\u540c\u6548\u5e94\u91cd\u8981\u6027\u7684\u65b9\u6cd5\uff09\u7684\u4e09\u4e2a\u5206\u6570\uff08\u72ec\u7279\u3001\u5197\u4f59\u3001\u534f\u540c\uff09\u5206\u914d\u7ed9\u6bcf\u4e2a\u72ec\u7acb\u6570\u636e\u6a21\u5f0f\uff0c\u5e76\u4e0eShapley\u6548\u5e94\u8fdb\u884c\u6bd4\u8f83\u3002\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6b7b\u4ea1\u7387\u4e0e\u7a7a\u6c14\u6c61\u67d3\u7269\u5173\u7cfb\u7684\u5e94\u7528\u4e2d\uff0c\u63ed\u793a\u4e86\u6c61\u67d3\u7269\u548c\u57ce\u5e02\u7eff\u5730\u4e4b\u95f4\u5b58\u5728\u7684\u534f\u540c\u6548\u5e94\uff0c\u8868\u660e\u672c\u5730Hi-Fi\u5728XAI\u548c\u590d\u6742\u7cfb\u7edf\u5206\u6790\u4e2d\u5177\u6709\u5e7f\u9614\u524d\u666f\u3002", "motivation": "\u6807\u51c6\u7279\u5f81\u91cd\u8981\u6027\u5de5\u5177\u4ec5\u63d0\u4f9b\u5355\u4e00\u5206\u6570\uff0c\u65e0\u6cd5\u63ed\u793a\u7279\u5f81\u4e4b\u95f4\u7684\u9ad8\u9636\u5408\u4f5c\u6548\u5e94\uff08\u534f\u540c\u4e0e\u5197\u4f59\uff09\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u6e05\u56de\u5f52\u95ee\u9898\u4e2d\u8f93\u5165\u7279\u5f81\u7684\u9ad8\u9636\u6548\u5e94\uff0c\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u9700\u8981\u4e00\u79cd\u80fd\u591f\u91cf\u5316\u8fd9\u4e9b\u5408\u4f5c\u6548\u5e94\u7684\u65b9\u6cd5\u3002\u73b0\u6709\u7684Hi-Fi\u65b9\u6cd5\u63d0\u51fa\u4e86\u591a\u7ef4\u5ea6\u5206\u6570\uff0c\u4f46\u7f3a\u4e4f\u5c06\u8fd9\u4e9b\u5206\u6570\u5206\u914d\u5230\u6bcf\u4e2a\u72ec\u7acb\u6570\u636e\u5b9e\u4f8b\u7684\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u7279\u5f81\u7684\u72ec\u7279\uff08unique\uff09\u3001\u5197\u4f59\uff08redundant\uff09\u548c\u534f\u540c\uff08synergistic\uff09\u8fd9\u4e09\u79cd\u5206\u6570\u5206\u914d\u7ed9\u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u72ec\u7acb\u6a21\u5f0f\uff08\u5373\u5b9e\u73b0\u672c\u5730\u5316\u7684Hi-Fi\uff09\u3002\u540c\u65f6\uff0c\u8be5\u6846\u67b6\u4e0e\u5df2\u6709\u7684\u7279\u5f81\u91cd\u8981\u6027\u8861\u91cf\u65b9\u6cd5Shapley\u6548\u5e94\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u4e3a\u5c55\u793a\u5176\u6f5c\u529b\uff0c\u7814\u7a76\u5c06\u5176\u5e94\u7528\u4e8e\u4e00\u4e2a\u201c\u4e00\u4f53\u5065\u5eb7\u201d\u6848\u4f8b\uff1a\u5206\u6790\u7a7a\u6c14\u6c61\u67d3\u7269\u4e0e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6b7b\u4ea1\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u4e3b\u8981\u7ed3\u679c\u663e\u793a\uff0c\u4e0e$O_3$\u548c$NO_2$\u76f8\u5173\u7684\u7279\u5f81\u4e0e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6b7b\u4ea1\u7387\u4e4b\u95f4\u5b58\u5728\u534f\u540c\u5173\u8054\uff0c\u5c24\u5176\u5728\u8d1d\u52a0\u83ab\u548c\u5e03\u96f7\u897f\u4e9a\u7701\u4efd\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u57ce\u5e02\u7eff\u5730\u5bc6\u5ea6\u5728\u9884\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6b7b\u4ea1\u7387\u65f6\u4e5f\u4e0e\u6c61\u67d3\u7269\u663e\u793a\u51fa\u534f\u540c\u5f71\u54cd\u529b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u672c\u5730Hi-Fi\u662f\u4e00\u4e2a\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u7684\u5de5\u5177\uff0c\u4e3a\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u4ee5\u53ca\u5206\u6790\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u9ad8\u9636\u5173\u7cfb\u5f00\u8f9f\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2508.01126", "pdf": "https://arxiv.org/pdf/2508.01126", "abs": "https://arxiv.org/abs/2508.01126", "authors": ["Chaitanya Patel", "Hiroki Nakamura", "Yuta Kyuragi", "Kazuki Kozuka", "Juan Carlos Niebles", "Ehsan Adeli"], "title": "UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction, Forecasting, and Generation", "categories": ["cs.CV"], "comment": "ICCV 2025. Project Page:\n  https://chaitanya100100.github.io/UniEgoMotion/", "summary": "Egocentric human motion generation and forecasting with scene-context is\ncrucial for enhancing AR/VR experiences, improving human-robot interaction,\nadvancing assistive technologies, and enabling adaptive healthcare solutions by\naccurately predicting and simulating movement from a first-person perspective.\nHowever, existing methods primarily focus on third-person motion synthesis with\nstructured 3D scene contexts, limiting their effectiveness in real-world\negocentric settings where limited field of view, frequent occlusions, and\ndynamic cameras hinder scene perception. To bridge this gap, we introduce\nEgocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks\nthat utilize first-person images for scene-aware motion synthesis without\nrelying on explicit 3D scene. We propose UniEgoMotion, a unified conditional\nmotion diffusion model with a novel head-centric motion representation tailored\nfor egocentric devices. UniEgoMotion's simple yet effective design supports\negocentric motion reconstruction, forecasting, and generation from first-person\nvisual inputs in a unified framework. Unlike previous works that overlook scene\nsemantics, our model effectively extracts image-based scene context to infer\nplausible 3D motion. To facilitate training, we introduce EE4D-Motion, a\nlarge-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth\n3D motion annotations. UniEgoMotion achieves state-of-the-art performance in\negocentric motion reconstruction and is the first to generate motion from a\nsingle egocentric image. Extensive evaluations demonstrate the effectiveness of\nour unified framework, setting a new benchmark for egocentric motion modeling\nand unlocking new possibilities for egocentric applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u540d\u4e3aUniEgoMotion\u7684\u7edf\u4e00\u6761\u4ef6\u8fd0\u52a8\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u57fa\u4e8e\u7b2c\u4e00\u4eba\u79f0\u56fe\u50cf\u7684\u573a\u666f\u611f\u77e5\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u8fd0\u52a8\u751f\u6210\u4e0e\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u548c\u663e\u5f0f3D\u573a\u666f\u7684\u4f9d\u8d56\u95ee\u9898\u3002\u540c\u65f6\u5f15\u5165\u4e86EE4D-Motion\u6570\u636e\u96c6\uff0c\u5e76\u5728\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u8fd0\u52a8\u91cd\u5efa\u548c\u4ece\u5355\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u8fd0\u52a8\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7b2c\u4e09\u4eba\u79f0\u8fd0\u52a8\u5408\u6210\u548c\u7ed3\u6784\u53163D\u573a\u666f\uff0c\u96be\u4ee5\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u4e2d\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u5b9a\uff08\u89c6\u91ce\u6709\u9650\u3001\u9891\u7e41\u906e\u6321\u3001\u52a8\u6001\u76f8\u673a\uff09\u3002\u51c6\u786e\u9884\u6d4b\u548c\u6a21\u62df\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u4e0b\u7684\u8fd0\u52a8\u5bf9\u4e8e\u589e\u5f3aAR/VR\u4f53\u9a8c\u3001\u6539\u8fdb\u4eba\u673a\u4ea4\u4e92\u3001\u63a8\u8fdb\u8f85\u52a9\u6280\u672f\u548c\u5b9e\u73b0\u81ea\u9002\u5e94\u533b\u7597\u89e3\u51b3\u65b9\u6848\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u201c\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u8fd0\u52a8\u751f\u6210\u201d\u548c\u201c\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u8fd0\u52a8\u9884\u6d4b\u201d\u4e24\u9879\u65b0\u4efb\u52a1\u3002\u5f00\u53d1\u4e86UniEgoMotion\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u6761\u4ef6\u8fd0\u52a8\u6269\u6563\u6a21\u578b\uff0c\u91c7\u7528\u65b0\u9896\u7684\u5934\u90e8\u4e2d\u5fc3\u8fd0\u52a8\u8868\u793a\uff0c\u65e0\u9700\u663e\u5f0f3D\u573a\u666f\u5373\u53ef\u4ece\u7b2c\u4e00\u4eba\u79f0\u56fe\u50cf\u4e2d\u63d0\u53d6\u573a\u666f\u4e0a\u4e0b\u6587\uff0c\u652f\u6301\u8fd0\u52a8\u91cd\u5efa\u3001\u9884\u6d4b\u548c\u751f\u6210\u3002\u4e3a\u8bad\u7ec3\u6784\u5efa\u4e86EE4D-Motion\u5927\u578b\u6570\u636e\u96c6\uff0c\u5176\u57fa\u4e8eEgoExo4D\u5e76\u589e\u5f3a\u4e86\u4f2a\u771f\u503c3D\u8fd0\u52a8\u6807\u6ce8\u3002", "result": "UniEgoMotion\u5728\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u8fd0\u52a8\u91cd\u5efa\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u662f\u9996\u4e2a\u80fd\u591f\u4ece\u5355\u4e00\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u56fe\u50cf\u751f\u6210\u8fd0\u52a8\u7684\u6a21\u578b\u3002\u5e7f\u6cdb\u7684\u8bc4\u4f30\u8bc1\u660e\u4e86\u5176\u7edf\u4e00\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u8fd0\u52a8\u5efa\u6a21\u8bbe\u5b9a\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u5e76\u4e3a\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u5e94\u7528\u7a0b\u5e8f\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.01273", "pdf": "https://arxiv.org/pdf/2508.01273", "abs": "https://arxiv.org/abs/2508.01273", "authors": ["Xianda Zheng", "Zijian Huang", "Meng-Fen Chiang", "Michael J. Witbrock", "Kaiqi Zhao"], "title": "KCR: Resolving Long-Context Knowledge Conflicts via Reasoning in LLMs", "categories": ["cs.AI"], "comment": null, "summary": "Knowledge conflicts commonly arise across diverse sources, and their\nprevalence has increased with the advent of LLMs. When dealing with conflicts\nbetween multiple contexts, also known as \\emph{inter-context knowledge\nconflicts}, LLMs are often confused by lengthy and conflicting contexts. To\naddress this challenge, we propose the Knowledge Conflict Reasoning (KCR)\nframework, which enhances the ability of LLMs to resolve conflicting knowledge.\nThe key idea of KCR is to train backbone LLMs to establish a correct reasoning\nprocess by rewarding them for selecting and adhering to the context with\nstronger logical consistency when presented with conflicting contexts.\nSpecifically, we first extract reasoning paths, represented by either text or\nlocal knowledge graphs, from the conflicting long contexts. Subsequently, we\nemploy Reinforcement Learning to encourage the model to learn the paradigm of\nreasoning process that follows correct reasoning paths rather than the\nincorrect counterparts. This enables the backbone models to genuinely acquire\nthe capability to resolve inter-context knowledge conflicts within long\ncontexts. Experimental results demonstrate that our framework significantly\nimproves the ability of various backbone models to resolve knowledge conflicts\nin long-context scenarios, yielding substantial performance gains.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faKCR\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u5176\u5728\u957f\u6587\u672c\u591a\u6e90\u51b2\u7a81\u60c5\u5883\u4e0b\uff0c\u80fd\u8bc6\u522b\u5e76\u9075\u5faa\u903b\u8f91\u66f4\u4e00\u81f4\u7684\u77e5\u8bc6\u8def\u5f84\uff0c\u4ece\u800c\u6709\u6548\u89e3\u51b3\u77e5\u8bc6\u51b2\u7a81\u3002", "motivation": "\u77e5\u8bc6\u51b2\u7a81\u5728\u591a\u6837\u5316\u4fe1\u606f\u6e90\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\u800c\u6108\u53d1\u9891\u7e41\u3002\u5728\u5904\u7406\u957f\u6587\u672c\u4e2d\u7684\u4e0a\u4e0b\u6587\u95f4\u77e5\u8bc6\u51b2\u7a81\u65f6\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5e38\u56e0\u5197\u957f\u4e14\u77db\u76fe\u7684\u5185\u5bb9\u800c\u611f\u5230\u56f0\u60d1\u3002", "method": "\u63d0\u51fa\u77e5\u8bc6\u51b2\u7a81\u63a8\u7406\uff08KCR\uff09\u6846\u67b6\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u5956\u52b1\u673a\u5236\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u5176\u5728\u9762\u5bf9\u51b2\u7a81\u4e0a\u4e0b\u6587\u65f6\u9009\u62e9\u5e76\u9075\u5faa\u903b\u8f91\u66f4\u5f3a\u7684\u4e00\u65b9\uff0c\u5efa\u7acb\u6b63\u786e\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u5177\u4f53\u800c\u8a00\uff0c\u9996\u5148\u4ece\u51b2\u7a81\u7684\u957f\u6587\u672c\u4e2d\u63d0\u53d6\u63a8\u7406\u8def\u5f84\uff08\u6587\u672c\u6216\u5c40\u90e8\u77e5\u8bc6\u56fe\uff09\uff0c\u7136\u540e\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u9f13\u52b1\u6a21\u578b\u5b66\u4e60\u9075\u5faa\u6b63\u786e\u63a8\u7406\u8def\u5f84\u7684\u8303\u5f0f\uff0c\u800c\u975e\u9519\u8bef\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5404\u79cd\u57fa\u7840\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u573a\u666f\u4e0b\u89e3\u51b3\u77e5\u8bc6\u51b2\u7a81\u7684\u80fd\u529b\uff0c\u5e76\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "KCR\u6846\u67b6\u901a\u8fc7\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u9075\u5faa\u6b63\u786e\u7684\u63a8\u7406\u8def\u5f84\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u6587\u672c\u4e2d\u4e0a\u4e0b\u6587\u95f4\u77e5\u8bc6\u51b2\u7a81\u7684\u6311\u6218\uff0c\u4f7f\u5176\u771f\u6b63\u5177\u5907\u4e86\u89e3\u51b3\u6b64\u7c7b\u95ee\u9898\u7684\u80fd\u529b\u3002"}}
{"id": "2508.01486", "pdf": "https://arxiv.org/pdf/2508.01486", "abs": "https://arxiv.org/abs/2508.01486", "authors": ["Vallabhaneni Raj Kumar", "Ashwin S", "Supriya Manna", "Niladri Sett", "Cheedella V S N M S Hema Harshitha", "Kurakula Harshitha", "Anand Kumar Sharma", "Basina Deepakraj", "Tanuj Sarkar", "Bondada Navaneeth Krishna", "Samanthapudi Shakeer"], "title": "TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu", "categories": ["cs.CL"], "comment": "work under review", "summary": "In the Indian subcontinent, Telugu, one of India's six classical languages,\nis the most widely spoken Dravidian Language. Despite its 96 million speaker\nbase worldwide, Telugu remains underrepresented in the global NLP and Machine\nLearning landscape, mainly due to lack of high-quality annotated resources.\nThis work introduces TeSent, a comprehensive benchmark dataset for sentiment\nclassification, a key text classification problem, in Telugu. TeSent not only\nprovides ground truth labels for the sentences, but also supplements with\nprovisions for evaluating explainability and fairness, two critical\nrequirements in modern-day machine learning tasks. We scraped Telugu texts\ncovering multiple domains from various social media platforms, news websites\nand web-blogs to preprocess and generate 26,150 sentences, and developed a\ncustom-built annotation platform and a carefully crafted annotation protocol\nfor collecting the ground truth labels along with their human-annotated\nrationales. We then fine-tuned several SOTA pre-trained models in two ways:\nwith rationales, and without rationales. Further, we provide a detailed\nplausibility and faithfulness evaluation suite, which exploits the rationales,\nfor six widely used post-hoc explainers applied on the trained models. Lastly,\nwe curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate\nfairness of Telugu sentiment and emotion related NLP tasks, and provide a\nfairness evaluation suite for the trained classifier models. Our experimental\nresults suggest that training with rationales may improve model accuracy,\nreduce bias in models, and make the explainers' output more aligned to human\nreasoning.", "AI": {"tldr": "\u5f15\u5165TeSent\uff0c\u4e00\u4e2a\u6cf0\u5362\u56fa\u8bed\u60c5\u611f\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u652f\u6301\u53ef\u89e3\u91ca\u6027\u548c\u516c\u5e73\u6027\u8bc4\u4f30\uff0c\u5e76\u5c55\u793a\u4e86\u4f7f\u7528\u7406\u7531\u8bad\u7ec3\u6a21\u578b\u7684\u76ca\u5904\u3002", "motivation": "\u6cf0\u5362\u56fa\u8bed\u4f5c\u4e3a\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u8bed\u8a00\uff0c\u5728NLP\u9886\u57df\u56e0\u9ad8\u8d28\u91cf\u6807\u6ce8\u8d44\u6e90\u532e\u4e4f\u800c\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e0a\u3002", "method": "1. \u6784\u5efa\u4e86\u5305\u542b26,150\u4e2a\u53e5\u5b50\u7684TeSent\u60c5\u611f\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u5e73\u53f0\u548c\u534f\u8bae\u6807\u6ce8\u771f\u503c\u6807\u7b7e\u53ca\u4eba\u5de5\u7406\u7531\u30022. \u5fae\u8c03SOTA\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u6709\u7406\u7531\u548c\u65e0\u7406\u7531\u4e24\u79cd\u65b9\u5f0f\uff09\u30023. \u63d0\u4f9b\u5229\u7528\u7406\u7531\u7684\u5408\u7406\u6027\u548c\u5fe0\u5b9e\u6027\u8bc4\u4f30\u5957\u4ef6\uff08\u9488\u5bf9\u89e3\u91ca\u5668\uff09\uff0c\u5e76\u6784\u5efaTeEEC\u8bed\u6599\u5e93\u4ee5\u8bc4\u4f30\u6cf0\u5362\u56fa\u8bedNLP\u4efb\u52a1\u7684\u516c\u5e73\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u7406\u7531\u8fdb\u884c\u8bad\u7ec3\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u6a21\u578b\u504f\u5dee\uff0c\u5e76\u4f7f\u89e3\u91ca\u5668\u7684\u8f93\u51fa\u66f4\u8d34\u8fd1\u4eba\u7c7b\u63a8\u7406\u3002", "conclusion": "TeSent\u548cTeEEC\u7684\u521b\u5efa\u586b\u8865\u4e86\u6cf0\u5362\u56fa\u8bedNLP\u7684\u8d44\u6e90\u7a7a\u767d\uff0c\u4e14\u5e26\u6709\u7406\u7531\u7684\u8bad\u7ec3\u65b9\u6cd5\u5728\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3001\u51cf\u5c11\u504f\u5dee\u548c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2508.00933", "pdf": "https://arxiv.org/pdf/2508.00933", "abs": "https://arxiv.org/abs/2508.00933", "authors": ["Hanchen Yang", "Jiaqi Wang", "Jiannong Cao", "Wengen Li", "Jialun Zheng", "Yangning Li", "Chunyu Miao", "Jihong Guan", "Shuigeng Zhou", "Philip S. Yu"], "title": "OKG-LLM: Aligning Ocean Knowledge Graph with Observation Data via LLMs for Global Sea Surface Temperature Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Sea surface temperature (SST) prediction is a critical task in ocean science,\nsupporting various applications, such as weather forecasting, fisheries\nmanagement, and storm tracking. While existing data-driven methods have\ndemonstrated significant success, they often neglect to leverage the rich\ndomain knowledge accumulated over the past decades, limiting further\nadvancements in prediction accuracy. The recent emergence of large language\nmodels (LLMs) has highlighted the potential of integrating domain knowledge for\ndownstream tasks. However, the application of LLMs to SST prediction remains\nunderexplored, primarily due to the challenge of integrating ocean domain\nknowledge and numerical data. To address this issue, we propose Ocean Knowledge\nGraph-enhanced LLM (OKG-LLM), a novel framework for global SST prediction. To\nthe best of our knowledge, this work presents the first systematic effort to\nconstruct an Ocean Knowledge Graph (OKG) specifically designed to represent\ndiverse ocean knowledge for SST prediction. We then develop a graph embedding\nnetwork to learn the comprehensive semantic and structural knowledge within the\nOKG, capturing both the unique characteristics of individual sea regions and\nthe complex correlations between them. Finally, we align and fuse the learned\nknowledge with fine-grained numerical SST data and leverage a pre-trained LLM\nto model SST patterns for accurate prediction. Extensive experiments on the\nreal-world dataset demonstrate that OKG-LLM consistently outperforms\nstate-of-the-art methods, showcasing its effectiveness, robustness, and\npotential to advance SST prediction. The codes are available in the online\nrepository.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOKG-LLM\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u6d77\u6d0b\u77e5\u8bc6\u56fe\u8c31\u5e76\u7ed3\u5408\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u5168\u7403\u6d77\u8868\u6e29\u5ea6\uff08SST\uff09\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709SST\u9884\u6d4b\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u4e30\u5bcc\u7684\u6d77\u6d0b\u9886\u57df\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u7684\u63d0\u5347\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u6709\u6574\u5408\u9886\u57df\u77e5\u8bc6\u7684\u6f5c\u529b\uff0c\u4f46\u5728SST\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u56e0\u6d77\u6d0b\u77e5\u8bc6\u4e0e\u6570\u503c\u6570\u636e\u96c6\u6210\u96be\u9898\u800c\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "1. \u6784\u5efa\u9996\u4e2a\u4e13\u95e8\u7528\u4e8eSST\u9884\u6d4b\u7684\u6d77\u6d0b\u77e5\u8bc6\u56fe\u8c31\uff08OKG\uff09\u30022. \u5f00\u53d1\u56fe\u5d4c\u5165\u7f51\u7edc\u5b66\u4e60OKG\u4e2d\u7684\u8bed\u4e49\u548c\u7ed3\u6784\u77e5\u8bc6\u30023. \u5c06\u5b66\u4e60\u5230\u7684\u77e5\u8bc6\u4e0e\u7ec6\u7c92\u5ea6\u6570\u503cSST\u6570\u636e\u5bf9\u9f50\u878d\u5408\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3LLM\u8fdb\u884cSST\u6a21\u5f0f\u5efa\u6a21\u548c\u9884\u6d4b\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cOKG-LLM\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3001\u9c81\u68d2\u6027\u4ee5\u53ca\u5728SST\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\u7684\u6f5c\u529b\u3002", "conclusion": "OKG-LLM\u6210\u529f\u5730\u5c06\u6d77\u6d0b\u9886\u57df\u77e5\u8bc6\u4e0eLLMs\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d77\u8868\u6e29\u5ea6\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.01137", "pdf": "https://arxiv.org/pdf/2508.01137", "abs": "https://arxiv.org/abs/2508.01137", "authors": ["Zeduo Zhang", "Yalda Mohsenzadeh"], "title": "Semi-Supervised Anomaly Detection in Brain MRI Using a Domain-Agnostic Deep Reinforcement Learning Approach", "categories": ["cs.CV"], "comment": "34 pages, 6 figures and 4 tables in main text, 17 pages supplementary\n  material with 3 tables and 3 figures; Submitted to Radiology: Artificial\n  Intelligence", "summary": "To develop a domain-agnostic, semi-supervised anomaly detection framework\nthat integrates deep reinforcement learning (DRL) to address challenges such as\nlarge-scale data, overfitting, and class imbalance, focusing on brain MRI\nvolumes. This retrospective study used publicly available brain MRI datasets\ncollected between 2005 and 2021. The IXI dataset provided 581 T1-weighted and\n578 T2-weighted MRI volumes (from healthy subjects) for training, while the\nBraTS 2021 dataset provided 251 volumes for validation and 1000 for testing\n(unhealthy subjects with Glioblastomas). Preprocessing included normalization,\nskull-stripping, and co-registering to a uniform voxel size. Experiments were\nconducted on both T1- and T2-weighted modalities. Additional experiments and\nablation analyses were also carried out on the industrial datasets. The\nproposed method integrates DRL with feature representations to handle label\nscarcity, large-scale data and overfitting. Statistical analysis was based on\nseveral detection and segmentation metrics including AUROC and Dice score. The\nproposed method achieved an AUROC of 88.7% (pixel-level) and 96.7%\n(image-level) on brain MRI datasets, outperforming State-of-The-Art (SOTA)\nmethods. On industrial surface datasets, the model also showed competitive\nperformance (AUROC = 99.8% pixel-level, 99.3% image-level) on MVTec AD dataset,\nindicating strong cross-domain generalization. Studies on anomaly sample size\nshowed a monotonic increase in AUROC as more anomalies were seen, without\nevidence of overfitting or additional computational cost. The domain-agnostic\nsemi-supervised approach using DRL shows significant promise for MRI anomaly\ndetection, achieving strong performance on both medical and industrial\ndatasets. Its robustness, generalizability and efficiency highlight its\npotential for real-world clinical applications.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u9886\u57df\u65e0\u5173\u534a\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u5728\u8111\u90e8MRI\u548c\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u9886\u57df\u65e0\u5173\u7684\u534a\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5927\u89c4\u6a21\u6570\u636e\u3001\u8fc7\u62df\u5408\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7b49\u6311\u6218\uff0c\u5e76\u4e13\u6ce8\u4e8e\u8111\u90e8MRI\u4f53\u79ef\u7684\u5f02\u5e38\u68c0\u6d4b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4e0e\u7279\u5f81\u8868\u793a\u76f8\u7ed3\u5408\u7684\u534a\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\u3002\u8be5\u7814\u7a76\u56de\u987e\u6027\u5730\u4f7f\u7528\u4e86IXI\uff08\u5065\u5eb7\uff09\u548cBraTS 2021\uff08\u5f02\u5e38\uff09\u8111MRI\u6570\u636e\u96c6\uff0c\u5e76\u6269\u5c55\u5230\u5de5\u4e1a\u6570\u636e\u96c6\uff08\u5982MVTec AD\uff09\u3002\u6570\u636e\u9884\u5904\u7406\u5305\u62ec\u5f52\u4e00\u5316\u3001\u9885\u9aa8\u5265\u79bb\u548c\u914d\u51c6\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ecAUROC\u548cDice\u5206\u6570\u3002", "result": "\u5728\u8111\u90e8MRI\u6570\u636e\u96c6\u4e0a\uff0c\u50cf\u7d20\u7ea7AUROC\u8fbe\u523088.7%\uff0c\u56fe\u50cf\u7ea7AUROC\u8fbe\u523096.7%\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff08SOTA\uff09\u3002\u5728\u5de5\u4e1a\u6570\u636e\u96c6\uff08MVTec AD\uff09\u4e0a\uff0c\u50cf\u7d20\u7ea7AUROC\u4e3a99.8%\uff0c\u56fe\u50cf\u7ea7AUROC\u4e3a99.3%\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u8fd8\u8868\u660e\uff0c\u6a21\u578b\u968f\u5f02\u5e38\u6837\u672c\u91cf\u589e\u52a0\uff0cAUROC\u5355\u8c03\u63d0\u9ad8\uff0c\u4e14\u65e0\u8fc7\u62df\u5408\u6216\u989d\u5916\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u57fa\u4e8eDRL\u7684\u9886\u57df\u65e0\u5173\u534a\u76d1\u7763\u65b9\u6cd5\u5728MRI\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u5728\u533b\u5b66\u548c\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3001\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u5b9e\u9645\u4e34\u5e8a\u3002"}}
{"id": "2508.01274", "pdf": "https://arxiv.org/pdf/2508.01274", "abs": "https://arxiv.org/abs/2508.01274", "authors": ["Jui-Ming Yao", "Bing-Cheng Xie", "Sheng-Wei Peng", "Hao-Yuan Chen", "He-Rong Zheng", "Bing-Jia Tan", "Peter Shaojui Wang", "Shun-Feng Su"], "title": "Multi-TW: Benchmarking Multimodal Models on Traditional Chinese Question Answering in Taiwan", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) process visual, acoustic, and\ntextual inputs, addressing the limitations of single-modality LLMs. However,\nexisting benchmarks often overlook tri-modal evaluation in Traditional Chinese\nand do not consider inference latency. To address this, we introduce Multi-TW,\nthe first Traditional Chinese benchmark for evaluating the performance and\nlatency of any-to-any multimodal models. Multi-TW includes 900 multiple-choice\nquestions (image and text, audio and text pairs) sourced from official\nproficiency tests developed with the Steering Committee for the Test of\nProficiency-Huayu (SC-TOP). We evaluated various any-to-any models and\nvision-language models (VLMs) with audio transcription. Our results show that\nclosed-source models generally outperform open-source ones across modalities,\nalthough open-source models can perform well in audio tasks. End-to-end\nany-to-any pipelines offer clear latency advantages compared to VLMs using\nseparate audio transcription. Multi-TW presents a comprehensive view of model\ncapabilities and highlights the need for Traditional Chinese fine-tuning and\nefficient multimodal architectures.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165Multi-TW\uff0c\u9996\u4e2a\u8bc4\u4f30\u7e41\u4f53\u4e2d\u6587\u591a\u6a21\u6001\u5927\u6a21\u578b\u6027\u80fd\u548c\u5ef6\u8fdf\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7e41\u4f53\u4e2d\u6587\u5904\u7406\u53ca\u6548\u7387\u4e0a\u7684\u8868\u73b0\u4e0e\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u57fa\u51c6\u5728\u7e41\u4f53\u4e2d\u6587\u7684\u4e09\u6a21\u6001\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u672a\u8003\u8651\u63a8\u7406\u5ef6\u8fdf\uff0c\u9650\u5236\u4e86\u5bf9MLLMs\u771f\u5b9e\u80fd\u529b\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86Multi-TW\uff0c\u4e00\u4e2a\u5305\u542b900\u9053\u591a\u9009\u9898\uff08\u56fe\u50cf-\u6587\u672c\u548c\u97f3\u9891-\u6587\u672c\u5bf9\uff09\u7684\u7e41\u4f53\u4e2d\u6587\u57fa\u51c6\uff0c\u9898\u76ee\u6e90\u81ea\u534e\u8bed\u6587\u80fd\u529b\u6d4b\u9a8c\u3002\u4ed6\u4eec\u4f7f\u7528Multi-TW\u8bc4\u4f30\u4e86\u5404\u79cd\u7aef\u5230\u7aef\u591a\u6a21\u6001\u6a21\u578b\u548c\u7ed3\u5408\u97f3\u9891\u8f6c\u5f55\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u95ed\u6e90\u6a21\u578b\u5728\u8de8\u6a21\u6001\u8868\u73b0\u4e0a\u666e\u904d\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u5c3d\u7ba1\u5f00\u6e90\u6a21\u578b\u5728\u97f3\u9891\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\u3002\u540c\u65f6\uff0c\u7aef\u5230\u7aef\u591a\u6a21\u6001\u7ba1\u7ebf\u76f8\u6bd4\u4e8e\u7ed3\u5408\u5355\u72ec\u97f3\u9891\u8f6c\u5f55\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5177\u6709\u660e\u663e\u7684\u63a8\u7406\u5ef6\u8fdf\u4f18\u52bf\u3002", "conclusion": "Multi-TW\u4e3a\u6a21\u578b\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u89c6\u56fe\uff0c\u5e76\u5f3a\u8c03\u4e86\u672a\u6765\u5728\u7e41\u4f53\u4e2d\u6587\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u548c\u5f00\u53d1\u66f4\u9ad8\u6548\u591a\u6a21\u6001\u67b6\u6784\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2508.01491", "pdf": "https://arxiv.org/pdf/2508.01491", "abs": "https://arxiv.org/abs/2508.01491", "authors": ["Zhivar Sourati", "Alireza S. Ziabari", "Morteza Dehghani"], "title": "The Homogenizing Effect of Large Language Models on Human Expression and Thought", "categories": ["cs.CL"], "comment": null, "summary": "Cognitive diversity, reflected in variations of language, perspective, and\nreasoning, is essential to creativity and collective intelligence. This\ndiversity is rich and grounded in culture, history, and individual experience.\nYet as large language models (LLMs) become deeply embedded in people's lives,\nthey risk standardizing language and reasoning. This Review synthesizes\nevidence across linguistics, cognitive, and computer science to show how LLMs\nreflect and reinforce dominant styles while marginalizing alternative voices\nand reasoning strategies. We examine how their design and widespread use\ncontribute to this effect by mirroring patterns in their training data and\namplifying convergence as all people increasingly rely on the same models\nacross contexts. Unchecked, this homogenization risks flattening the cognitive\nlandscapes that drive collective intelligence and adaptability.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6709\u6807\u51c6\u5316\u8bed\u8a00\u548c\u63a8\u7406\u7684\u98ce\u9669\uff0c\u53ef\u80fd\u524a\u5f31\u8ba4\u77e5\u591a\u6837\u6027\uff0c\u8fdb\u800c\u5f71\u54cd\u96c6\u4f53\u667a\u6167\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u8ba4\u77e5\u591a\u6837\u6027\u5bf9\u521b\u9020\u529b\u548c\u96c6\u4f53\u667a\u6167\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u666e\u53ca\u53ef\u80fd\u5bfc\u81f4\u8bed\u8a00\u548c\u63a8\u7406\u7684\u6807\u51c6\u5316\uff0c\u5a01\u80c1\u8fd9\u79cd\u591a\u6837\u6027\u3002", "method": "\u7efc\u5408\u8bed\u8a00\u5b66\u3001\u8ba4\u77e5\u79d1\u5b66\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u8bc1\u636e\u3002", "result": "LLMs\u901a\u8fc7\u53cd\u6620\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6a21\u5f0f\u5e76\u968f\u7740\u5e7f\u6cdb\u4f7f\u7528\u800c\u52a0\u5267\u8d8b\u540c\uff0c\u4ece\u800c\u5f3a\u5316\u4e3b\u5bfc\u98ce\u683c\uff0c\u5e76\u8fb9\u7f18\u5316\u66ff\u4ee3\u6027\u7684\u58f0\u97f3\u548c\u63a8\u7406\u7b56\u7565\u3002", "conclusion": "\u82e5\u4e0d\u52a0\u4ee5\u63a7\u5236\uff0cLLMs\u9020\u6210\u7684\u540c\u8d28\u5316\u53ef\u80fd\u524a\u5f31\u9a71\u52a8\u96c6\u4f53\u667a\u6167\u548c\u9002\u5e94\u6027\u7684\u8ba4\u77e5\u56fe\u666f\u3002"}}
{"id": "2508.00954", "pdf": "https://arxiv.org/pdf/2508.00954", "abs": "https://arxiv.org/abs/2508.00954", "authors": ["Andy Hu", "Devika Prasad", "Luiz Pizzato", "Nicholas Foord", "Arman Abrahamyan", "Anna Leontjeva", "Cooper Doyle", "Dan Jermyn"], "title": "FeatureCuts: Feature Selection for Large Data by Optimizing the Cutoff", "categories": ["cs.LG"], "comment": "11 pages, 4 figures, appendix", "summary": "In machine learning, the process of feature selection involves finding a\nreduced subset of features that captures most of the information required to\ntrain an accurate and efficient model. This work presents FeatureCuts, a novel\nfeature selection algorithm that adaptively selects the optimal feature cutoff\nafter performing filter ranking. Evaluated on 14 publicly available datasets\nand one industry dataset, FeatureCuts achieved, on average, 15 percentage\npoints more feature reduction and up to 99.6% less computation time while\nmaintaining model performance, compared to existing state-of-the-art methods.\nWhen the selected features are used in a wrapper method such as Particle Swarm\nOptimization (PSO), it enables 25 percentage points more feature reduction,\nrequires 66% less computation time, and maintains model performance when\ncompared to PSO alone. The minimal overhead of FeatureCuts makes it scalable\nfor large datasets typically seen in enterprise applications.", "AI": {"tldr": "FeatureCuts\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7279\u5f81\u9009\u62e9\u7b97\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u622a\u6b62\u9009\u62e9\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u7279\u5f81\u6570\u91cf\u548c\u8ba1\u7b97\u65f6\u95f4\uff0c\u5e76\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "motivation": "\u5728\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u7279\u5f81\u9009\u62e9\u65e8\u5728\u627e\u5230\u4e00\u4e2a\u7cbe\u7b80\u7684\u7279\u5f81\u5b50\u96c6\uff0c\u4ee5\u8bad\u7ec3\u51fa\u51c6\u786e\u4e14\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86FeatureCuts\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u7684\u6838\u5fc3\u662f\u5728\u6267\u884c\u8fc7\u6ee4\u6392\u540d\u540e\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u65b9\u5f0f\u9009\u62e9\u6700\u4f18\u7684\u7279\u5f81\u622a\u6b62\u70b9\u3002", "result": "1. \u76f8\u8f83\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0cFeatureCuts\u5e73\u5747\u591a\u5b9e\u73b015%\u7684\u7279\u5f81\u7f29\u51cf\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe99.6%\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff08\u57fa\u4e8e14\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c1\u4e2a\u5de5\u4e1a\u6570\u636e\u96c6\u7684\u8bc4\u4f30\uff09\u3002\n2. \u5f53\u4e0e\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u7b49\u5305\u88c5\u5668\u65b9\u6cd5\u7ed3\u5408\u65f6\uff0cFeatureCuts\u6bd4\u5355\u72ec\u4f7f\u7528PSO\u80fd\u591a\u5b9e\u73b025%\u7684\u7279\u5f81\u7f29\u51cf\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c1166%\uff0c\u5e76\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "FeatureCuts\u7b97\u6cd5\u7684\u6700\u5c0f\u5f00\u9500\u4f7f\u5176\u80fd\u591f\u5f88\u597d\u5730\u6269\u5c55\u5230\u4f01\u4e1a\u5e94\u7528\u4e2d\u5e38\u89c1\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u5e76\u5728\u7279\u5f81\u7f29\u51cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u5353\u8d8a\u3002"}}
{"id": "2508.01139", "pdf": "https://arxiv.org/pdf/2508.01139", "abs": "https://arxiv.org/abs/2508.01139", "authors": ["Huyu Wu", "Duo Su", "Junjie Hou", "Guang Li"], "title": "Dataset Condensation with Color Compensation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Dataset condensation always faces a constitutive trade-off: balancing\nperformance and fidelity under extreme compression. Existing methods struggle\nwith two bottlenecks: image-level selection methods (Coreset Selection, Dataset\nQuantization) suffer from inefficiency condensation, while pixel-level\noptimization (Dataset Distillation) introduces semantic distortion due to\nover-parameterization. With empirical observations, we find that a critical\nproblem in dataset condensation is the oversight of color's dual role as an\ninformation carrier and a basic semantic representation unit. We argue that\nimproving the colorfulness of condensed images is beneficial for representation\nlearning. Motivated by this, we propose DC3: a Dataset Condensation framework\nwith Color Compensation. After a calibrated selection strategy, DC3 utilizes\nthe latent diffusion model to enhance the color diversity of an image rather\nthan creating a brand-new one. Extensive experiments demonstrate the superior\nperformance and generalization of DC3 that outperforms SOTA methods across\nmultiple benchmarks. To the best of our knowledge, besides focusing on\ndownstream tasks, DC3 is the first research to fine-tune pre-trained diffusion\nmodels with condensed datasets. The FID results prove that training networks\nwith our high-quality datasets is feasible without model collapse or other\ndegradation issues. Code and generated data will be released soon.", "AI": {"tldr": "DC3\uff08Dataset Condensation framework with Color Compensation\uff09\u901a\u8fc7\u589e\u5f3a\u51dd\u7ed3\u56fe\u50cf\u7684\u8272\u5f69\u591a\u6837\u6027\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u96c6\u51dd\u7ed3\u4e2d\u6027\u80fd\u4e0e\u4fdd\u771f\u5ea6\u7684\u6743\u8861\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u5c1d\u8bd5\u4f7f\u7528\u51dd\u7ed3\u6570\u636e\u96c6\u5fae\u8c03\u6269\u6563\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u51dd\u7ed3\u65b9\u6cd5\uff08\u5982\u56fe\u50cf\u7ea7\u9009\u62e9\u6216\u50cf\u7d20\u7ea7\u4f18\u5316\uff09\u5728\u6781\u7aef\u538b\u7f29\u4e0b\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u6216\u5f15\u5165\u8bed\u4e49\u5931\u771f\u7684\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5ffd\u89c6\u4e86\u989c\u8272\u4f5c\u4e3a\u4fe1\u606f\u8f7d\u4f53\u548c\u57fa\u672c\u8bed\u4e49\u8868\u793a\u5355\u4f4d\u7684\u53cc\u91cd\u4f5c\u7528\uff0c\u800c\u63d0\u9ad8\u51dd\u7ed3\u56fe\u50cf\u7684\u8272\u5f69\u4e30\u5bcc\u5ea6\u5bf9\u8868\u793a\u5b66\u4e60\u6709\u76ca\u3002", "method": "\u63d0\u51faDC3\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u9996\u5148\u91c7\u7528\u6821\u51c6\u9009\u62e9\u7b56\u7565\uff0c\u968f\u540e\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u6765\u589e\u5f3a\u56fe\u50cf\u7684\u8272\u5f69\u591a\u6837\u6027\uff0c\u800c\u975e\u751f\u6210\u5168\u65b0\u56fe\u50cf\u3002DC3\u4e5f\u662f\u9996\u4e2a\u5229\u7528\u51dd\u7ed3\u6570\u636e\u96c6\u5fae\u8c03\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u7814\u7a76\u3002", "result": "DC3\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002FID\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528DC3\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u8bad\u7ec3\u7f51\u7edc\u662f\u53ef\u884c\u7684\uff0c\u4e14\u4e0d\u4f1a\u5bfc\u81f4\u6a21\u578b\u5d29\u6e83\u6216\u5176\u4ed6\u6027\u80fd\u9000\u5316\u95ee\u9898\u3002", "conclusion": "DC3\u901a\u8fc7\u5173\u6ce8\u5e76\u63d0\u5347\u51dd\u7ed3\u56fe\u50cf\u7684\u8272\u5f69\u4e30\u5bcc\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u96c6\u51dd\u7ed3\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u663e\u8457\u63d0\u5347\u3002\u5b83\u4e3a\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u751f\u6210\u548c\u6269\u6563\u6a21\u578b\u7684\u5fae\u8c03\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.01285", "pdf": "https://arxiv.org/pdf/2508.01285", "abs": "https://arxiv.org/abs/2508.01285", "authors": ["Yujing Ke", "Kevin George", "Kathan Pandya", "David Blumenthal", "Maximilian Sprang", "Gerrit Gro\u00dfmann", "Sebastian Vollmer", "David Antony Selby"], "title": "BioDisco: Multi-agent hypothesis generation with dual-mode evidence, iterative feedback and temporal evaluation", "categories": ["cs.AI", "cs.ET", "cs.IR", "stat.AP"], "comment": "7 pages main content + 11 pages appendices", "summary": "Identifying novel hypotheses is essential to scientific research, yet this\nprocess risks being overwhelmed by the sheer volume and complexity of available\ninformation. Existing automated methods often struggle to generate novel and\nevidence-grounded hypotheses, lack robust iterative refinement and rarely\nundergo rigorous temporal evaluation for future discovery potential. To address\nthis, we propose BioDisco, a multi-agent framework that draws upon language\nmodel-based reasoning and a dual-mode evidence system (biomedical knowledge\ngraphs and automated literature retrieval) for grounded novelty, integrates an\ninternal scoring and feedback loop for iterative refinement, and validates\nperformance through pioneering temporal and human evaluations and a\nBradley-Terry paired comparison model to provide statistically-grounded\nassessment. Our evaluations demonstrate superior novelty and significance over\nablated configurations representative of existing agentic architectures.\nDesigned for flexibility and modularity, BioDisco allows seamless integration\nof custom language models or knowledge graphs, and can be run with just a few\nlines of code. We anticipate researchers using this practical tool as a\ncatalyst for the discovery of new hypotheses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBioDisco\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u548c\u53cc\u6a21\u6001\u8bc1\u636e\u7cfb\u7edf\uff0c\u5e76\u7ed3\u5408\u8fed\u4ee3\u4f18\u5316\u4e0e\u4e25\u683c\u8bc4\u4f30\uff0c\u65e8\u5728\u9ad8\u6548\u751f\u6210\u65b0\u9896\u4e14\u6709\u4f9d\u636e\u7684\u79d1\u5b66\u5047\u8bbe\uff0c\u5e76\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u79d1\u5b66\u7814\u7a76\u4e2d\u53d1\u73b0\u65b0\u5047\u8bbe\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u4fe1\u606f\u91cf\u5e9e\u5927\u4e14\u590d\u6742\uff0c\u5bfc\u81f4\u8be5\u8fc7\u7a0b\u53d7\u963b\u3002\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5728\u751f\u6210\u65b0\u9896\u3001\u6709\u4f9d\u636e\u7684\u5047\u8bbe\u3001\u9c81\u68d2\u7684\u8fed\u4ee3\u7ec6\u5316\u4ee5\u53ca\u672a\u6765\u53d1\u73b0\u6f5c\u529b\u7684\u4e25\u683c\u65f6\u95f4\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86BioDisco\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3002\u5b83\u7ed3\u5408\u4e86\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u548c\u53cc\u6a21\u6001\u8bc1\u636e\u7cfb\u7edf\uff08\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u4e0e\u81ea\u52a8\u5316\u6587\u732e\u68c0\u7d22\uff09\u4ee5\u786e\u4fdd\u5047\u8bbe\u7684\u65b0\u9896\u6027\u548c\u4f9d\u636e\u6027\uff1b\u6574\u5408\u4e86\u5185\u90e8\u8bc4\u5206\u548c\u53cd\u9988\u5faa\u73af\u4ee5\u5b9e\u73b0\u8fed\u4ee3\u7ec6\u5316\uff1b\u5e76\u901a\u8fc7\u5f00\u521b\u6027\u7684\u65f6\u95f4\u8bc4\u4f30\u3001\u4eba\u5de5\u8bc4\u4f30\u548cBradley-Terry\u914d\u5bf9\u6bd4\u8f83\u6a21\u578b\u6765\u63d0\u4f9b\u6709\u7edf\u8ba1\u5b66\u4f9d\u636e\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u6211\u4eec\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cBioDisco\u5728\u5047\u8bbe\u7684\u65b0\u9896\u6027\u548c\u91cd\u8981\u6027\u65b9\u9762\u4f18\u4e8e\u4ee3\u8868\u73b0\u6709\u667a\u80fd\u4f53\u67b6\u6784\u7684\u7b80\u5316\u914d\u7f6e\u3002", "conclusion": "BioDisco\u88ab\u8bbe\u8ba1\u4e3a\u7075\u6d3b\u548c\u6a21\u5757\u5316\u7684\u5b9e\u7528\u5de5\u5177\uff0c\u6613\u4e8e\u96c6\u6210\u548c\u4f7f\u7528\u3002\u6211\u4eec\u9884\u671f\u7814\u7a76\u4eba\u5458\u5c06\u5229\u7528\u6b64\u5de5\u5177\u4f5c\u4e3a\u53d1\u73b0\u65b0\u5047\u8bbe\u7684\u50ac\u5316\u5242\u3002"}}
{"id": "2508.01503", "pdf": "https://arxiv.org/pdf/2508.01503", "abs": "https://arxiv.org/abs/2508.01503", "authors": ["Clayton Cohn", "Surya Rayala", "Namrata Srivastava", "Joyce Horn Fonteles", "Shruti Jain", "Xinying Luo", "Divya Mereddy", "Naveeduddin Mohammed", "Gautam Biswas"], "title": "A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) present new opportunities for creating\npedagogical agents that engage in meaningful dialogue to support student\nlearning. However, the current use of LLM systems like ChatGPT in classrooms\noften lacks the solid theoretical foundation found in earlier intelligent\ntutoring systems. To bridge this gap, we propose a framework that combines\nEvidence-Centered Design with Social Cognitive Theory for adaptive scaffolding\nin LLM-based agents focused on STEM+C learning. We illustrate this framework\nwith Inquizzitor, an LLM-based formative assessment agent that integrates\nhuman-AI hybrid intelligence and provides feedback grounded in cognitive\nscience principles. Our findings show that Inquizzitor delivers high-quality\nassessment and interaction aligned with core learning theories, offering\nteachers effective guidance that students value. This research underscores the\npotential for theory-driven LLM integration in education, highlighting the\nability of these systems to provide adaptive and principled instruction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u5faa\u8bc1\u8bbe\u8ba1\u4e0e\u793e\u4f1a\u8ba4\u77e5\u7406\u8bba\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u57fa\u4e8eLLM\u7684\u81ea\u9002\u5e94\u6559\u5b66\u4ee3\u7406\uff08\u5982Inquizzitor\uff09\uff0c\u4ee5\u5f25\u8865\u73b0\u6709LLM\u5728\u6559\u5b66\u4e2d\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u7684\u4e0d\u8db3\uff0c\u5e76\u8bc1\u660e\u5176\u80fd\u63d0\u4f9b\u9ad8\u8d28\u91cf\u3001\u7406\u8bba\u9a71\u52a8\u7684\u8bc4\u4f30\u548c\u4e92\u52a8\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u521b\u5efa\u6559\u5b66\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u5f53\u524dLLM\u7cfb\u7edf\u5728\u8bfe\u5802\u4e2d\u7684\u5e94\u7528\u7f3a\u4e4f\u65e9\u671f\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u6240\u5177\u5907\u7684\u575a\u5b9e\u7406\u8bba\u57fa\u7840\uff0c\u7814\u7a76\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u201c\u5faa\u8bc1\u8bbe\u8ba1\u201d\uff08Evidence-Centered Design\uff09\u548c\u201c\u793e\u4f1a\u8ba4\u77e5\u7406\u8bba\u201d\uff08Social Cognitive Theory\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8eLLM-based\u4ee3\u7406\u5728STEM+C\u5b66\u4e60\u4e2d\u7684\u81ea\u9002\u5e94\u811a\u624b\u67b6\u3002\u901a\u8fc7\u5f00\u53d1LLM-based\u5f62\u6210\u6027\u8bc4\u4f30\u4ee3\u7406Inquizzitor\u6765\u4f8b\u8bc1\u8be5\u6846\u67b6\uff0c\u8be5\u4ee3\u7406\u6574\u5408\u4e86\u4eba\u673a\u6df7\u5408\u667a\u80fd\u5e76\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u539f\u7406\u63d0\u4f9b\u53cd\u9988\u3002", "result": "Inquizzitor\u80fd\u591f\u63d0\u4f9b\u4e0e\u6838\u5fc3\u5b66\u4e60\u7406\u8bba\u76f8\u7b26\u7684\u9ad8\u8d28\u91cf\u8bc4\u4f30\u548c\u4e92\u52a8\uff0c\u4e3a\u6559\u5e08\u63d0\u4f9b\u6709\u6548\u7684\u6307\u5bfc\uff0c\u5e76\u83b7\u5f97\u5b66\u751f\u7684\u8ba4\u53ef\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u7406\u8bba\u9a71\u52a8\u7684LLM\u5728\u6559\u80b2\u4e2d\u6574\u5408\u7684\u6f5c\u529b\uff0c\u5e76\u7a81\u51fa\u4e86\u8fd9\u4e9b\u7cfb\u7edf\u63d0\u4f9b\u81ea\u9002\u5e94\u548c\u539f\u5219\u6027\u6559\u5b66\u7684\u80fd\u529b\u3002"}}
{"id": "2508.00955", "pdf": "https://arxiv.org/pdf/2508.00955", "abs": "https://arxiv.org/abs/2508.00955", "authors": ["Yeong-Joon Ju", "Seong-Whan Lee"], "title": "From Generator to Embedder: Harnessing Innate Abilities of Multimodal LLMs via Building Zero-Shot Discriminative Embedding Model", "categories": ["cs.LG", "cs.AI", "cs.IR"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have emerged as a promising solution\nfor universal embedding tasks, yet adapting their generative nature for\ndiscriminative representation learning remains a significant challenge. The\ndominant paradigm of large-scale contrastive pre-training suffers from critical\ninefficiencies, including prohibitive computational costs and a failure to\nleverage the intrinsic, instruction-following capabilities of MLLMs. To\novercome these limitations, we propose an efficient framework for universal\nmultimodal embeddings, which bridges this gap by centering on two synergistic\ncomponents. First, our hierarchical embedding prompt template employs a\ntwo-level instruction architecture that forces the model to produce\ndiscriminative representations. Building on this strong foundation, our second\ncomponent, self-aware hard negative sampling, redefines the fine-tuning process\nby leveraging the model's own understanding to efficiently mine challenging\nnegatives while actively filtering out potential false negatives. Our\ncomprehensive experiments show that our hierarchical prompt achieves zero-shot\nperformance competitive with contrastively trained baselines and enhances the\nfine-tuning process by lifting a simple in-batch negative baseline by 4.8\npoints on the MMEB benchmark. We further boost the performance via our\nself-aware hard negative sampling, achieving the state-of-the-art performance\nwithout the contrative pre-training. Our work presents an effective and\nefficient pathway to adapt MLLMs for universal embedding tasks, significantly\nreducing training time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5d4c\u5165\u63d0\u793a\u548c\u81ea\u611f\u77e5\u96be\u8d1f\u4f8b\u91c7\u6837\uff0c\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5e94\u7528\u4e8e\u901a\u7528\u5d4c\u5165\u4efb\u52a1\uff0c\u65e0\u9700\u5bf9\u6bd4\u9884\u8bad\u7ec3\u5373\u5b9e\u73b0SOTA\u6027\u80fd\u5e76\u663e\u8457\u7f29\u77ed\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u901a\u7528\u5d4c\u5165\u4efb\u52a1\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5c06\u5176\u751f\u6210\u7279\u6027\u9002\u914d\u5230\u5224\u522b\u5f0f\u8868\u793a\u5b66\u4e60\u4ecd\u5177\u6311\u6218\u3002\u5f53\u524d\u5927\u89c4\u6a21\u5bf9\u6bd4\u9884\u8bad\u7ec3\u8303\u5f0f\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u672a\u80fd\u6709\u6548\u5229\u7528MLLMs\u56fa\u6709\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u901a\u7528\u591a\u6a21\u6001\u5d4c\u5165\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u534f\u540c\u7ec4\u4ef6\uff1a1. \u5206\u5c42\u5d4c\u5165\u63d0\u793a\u6a21\u677f\uff1a\u91c7\u7528\u4e24\u7ea7\u6307\u4ee4\u67b6\u6784\uff0c\u5f3a\u5236\u6a21\u578b\u751f\u6210\u5224\u522b\u5f0f\u8868\u793a\u30022. \u81ea\u611f\u77e5\u96be\u8d1f\u4f8b\u91c7\u6837\uff1a\u5229\u7528\u6a21\u578b\u81ea\u8eab\u7684\u7406\u89e3\uff0c\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u9ad8\u6548\u6316\u6398\u56f0\u96be\u8d1f\u4f8b\uff0c\u540c\u65f6\u4e3b\u52a8\u8fc7\u6ee4\u6f5c\u5728\u7684\u5047\u8d1f\u4f8b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u5206\u5c42\u63d0\u793a\u5728\u96f6\u6837\u672c\u6027\u80fd\u4e0a\u4e0e\u5bf9\u6bd4\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5e76\u901a\u8fc7\u5c06\u7b80\u5355\u6279\u5185\u8d1f\u4f8b\u57fa\u7ebf\u6a21\u578b\u5728MMEB\u57fa\u51c6\u4e0a\u63d0\u53474.8\u4e2a\u70b9\u6765\u589e\u5f3a\u5fae\u8c03\u8fc7\u7a0b\u3002\u901a\u8fc7\u81ea\u611f\u77e5\u96be\u8d1f\u4f8b\u91c7\u6837\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5728\u6ca1\u6709\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u5f53\u524d\u6700\u4f73\uff08SOTA\uff09\u6027\u80fd\u3002", "conclusion": "\u6211\u4eec\u7684\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u6548\u4e14\u9ad8\u6548\u7684\u9014\u5f84\uff0c\u5c06MLLMs\u5e94\u7528\u4e8e\u901a\u7528\u5d4c\u5165\u4efb\u52a1\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002"}}
{"id": "2508.01150", "pdf": "https://arxiv.org/pdf/2508.01150", "abs": "https://arxiv.org/abs/2508.01150", "authors": ["Dianyi Yang", "Xihan Wang", "Yu Gao", "Shiyang Liu", "Bohan Ren", "Yufeng Yue", "Yi Yang"], "title": "OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding", "categories": ["cs.CV"], "comment": "IROS2025", "summary": "Recent advancements in 3D scene understanding have made significant strides\nin enabling interaction with scenes using open-vocabulary queries, particularly\nfor VR/AR and robotic applications. Nevertheless, existing methods are hindered\nby rigid offline pipelines and the inability to provide precise 3D object-level\nunderstanding given open-ended queries. In this paper, we present\nOpenGS-Fusion, an innovative open-vocabulary dense mapping framework that\nimproves semantic modeling and refines object-level understanding.\nOpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed\nDistance Field to facilitate lossless fusion of semantic features on-the-fly.\nFurthermore, we introduce a novel multimodal language-guided approach named\nMLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D\nobjects by adaptively adjusting similarity thresholds, achieving an improvement\n17\\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments\ndemonstrate that our method outperforms existing methods in 3D object\nunderstanding and scene reconstruction quality, as well as showcasing its\neffectiveness in language-guided scene interaction. The code is available at\nhttps://young-bit.github.io/opengs-fusion.github.io/ .", "AI": {"tldr": "OpenGS-Fusion\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u5f00\u653e\u8bcd\u6c47\u5bc6\u96c6\u6620\u5c04\u6846\u67b6\uff0c\u7ed3\u54083D\u9ad8\u65af\u548cTSDF\u5b9e\u73b0\u5b9e\u65f6\u8bed\u4e49\u878d\u5408\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u6001\u8bed\u8a00\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u9608\u503c\u65b9\u6cd5\u663e\u8457\u63d0\u53473D\u5bf9\u8c61\u7406\u89e3\u548c\u573a\u666f\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u67093D\u573a\u666f\u7406\u89e3\u65b9\u6cd5\u53d7\u9650\u4e8e\u50f5\u5316\u7684\u79bb\u7ebf\u6d41\u7a0b\uff0c\u4e14\u65e0\u6cd5\u5bf9\u5f00\u653e\u5f0f\u67e5\u8be2\u63d0\u4f9b\u7cbe\u786e\u76843D\u5bf9\u8c61\u7ea7\u7406\u89e3\uff0c\u5c24\u5176\u662f\u5728VR/AR\u548c\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u3002", "method": "\u63d0\u51faOpenGS-Fusion\u6846\u67b6\uff0c\u5b83\u7ed3\u54083D\u9ad8\u65af\u8868\u793a\u548c\u622a\u65ad\u7b26\u53f7\u8ddd\u79bb\u573a(TSDF)\u4ee5\u5b9e\u73b0\u8bed\u4e49\u7279\u5f81\u7684\u5b9e\u65f6\u65e0\u635f\u878d\u5408\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e00\u79cd\u540d\u4e3a\u201cMLLM-Assisted Adaptive Thresholding\u201d\u7684\u591a\u6a21\u6001\u8bed\u8a00\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u76f8\u4f3c\u6027\u9608\u503c\u6765\u4f18\u53163D\u5bf9\u8c61\u5206\u5272\u3002", "result": "MLLM-Assisted Adaptive Thresholding\u65b9\u6cd5\u76f8\u8f83\u4e8e\u56fa\u5b9a\u9608\u503c\u7b56\u7565\uff0c\u57283D mIoU\u4e0a\u5b9e\u73b0\u4e8617%\u7684\u63d0\u5347\u3002\u8be5\u65b9\u6cd5\u57283D\u5bf9\u8c61\u7406\u89e3\u3001\u573a\u666f\u91cd\u5efa\u8d28\u91cf\u4ee5\u53ca\u8bed\u8a00\u5f15\u5bfc\u7684\u573a\u666f\u4ea4\u4e92\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OpenGS-Fusion\u901a\u8fc7\u5176\u521b\u65b0\u7684\u878d\u5408\u673a\u5236\u548c\u81ea\u9002\u5e94\u9608\u503c\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c47\u4e0b3D\u573a\u666f\u7684\u8bed\u4e49\u5efa\u6a21\u548c\u5bf9\u8c61\u7ea7\u7406\u89e3\u80fd\u529b\uff0c\u4e3aVR/AR\u548c\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u3001\u5b9e\u65f6\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01300", "pdf": "https://arxiv.org/pdf/2508.01300", "abs": "https://arxiv.org/abs/2508.01300", "authors": ["Ma'ayan Armony", "Albert Mero\u00f1o-Pe\u00f1uela", "Gerard Canal"], "title": "How Far Are LLMs from Symbolic Planners? An NLP-Based Perspective", "categories": ["cs.AI"], "comment": null, "summary": "The reasoning and planning abilities of Large Language Models (LLMs) have\nbeen a frequent topic of discussion in recent years. Their ability to take\nunstructured planning problems as input has made LLMs' integration into AI\nplanning an area of interest. Nevertheless, LLMs are still not reliable as\nplanners, with the generated plans often containing mistaken or hallucinated\nactions. Existing benchmarking and evaluation methods investigate planning with\nLLMs, focusing primarily on success rate as a quality indicator in various\nplanning tasks, such as validating plans or planning in relaxed conditions. In\nthis paper, we approach planning with LLMs as a natural language processing\n(NLP) task, given that LLMs are NLP models themselves. We propose a recovery\npipeline consisting of an NLP-based evaluation of the generated plans, along\nwith three stages to recover the plans through NLP manipulation of the\nLLM-generated plans, and eventually complete the plan using a symbolic planner.\nThis pipeline provides a holistic analysis of LLM capabilities in the context\nof AI task planning, enabling a broader understanding of the quality of invalid\nplans. Our findings reveal no clear evidence of underlying reasoning during\nplan generation, and that a pipeline comprising an NLP-based analysis of the\nplans, followed by a recovery mechanism, still falls short of the quality and\nreliability of classical planners. On average, only the first 2.65 actions of\nthe plan are executable, with the average length of symbolically generated\nplans being 8.4 actions. The pipeline still improves action quality and\nincreases the overall success rate from 21.9% to 27.5%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eNLP\u7684\u6062\u590d\u6d41\u7a0b\u6765\u4fee\u6b63LLM\u751f\u6210\u7684\u89c4\u5212\uff0c\u53d1\u73b0LLM\u5728\u89c4\u5212\u4e2d\u7f3a\u4e4f\u660e\u786e\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u5c3d\u7ba1\u6b64\u6d41\u7a0b\u7565\u5fae\u63d0\u5347\u4e86\u6210\u529f\u7387\uff0c\u4f46\u5176\u8868\u73b0\u4ecd\u8fdc\u4e0d\u53ca\u7ecf\u5178\u89c4\u5212\u5668\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728AI\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u4e0d\u53ef\u9760\u6027\uff0c\u5e38\u751f\u6210\u9519\u8bef\u6216\u5e7b\u89c9\u52a8\u4f5c\u3002\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6210\u529f\u7387\uff0c\u672a\u80fd\u5168\u9762\u5206\u6790\u65e0\u6548\u8ba1\u5212\u7684\u8d28\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6df1\u5165\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u6539\u8fdbLLM\u7684\u89c4\u5212\u80fd\u529b\u3002", "method": "\u5c06LLM\u7684\u89c4\u5212\u95ee\u9898\u89c6\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u4efb\u52a1\uff0c\u63d0\u51fa\u4e00\u4e2a\u6062\u590d\u6d41\u7a0b\u3002\u8be5\u6d41\u7a0b\u5305\u62ec\u5bf9LLM\u751f\u6210\u8ba1\u5212\u7684NLP\u8bc4\u4f30\u3001\u901a\u8fc7\u4e09\u9636\u6bb5NLP\u64cd\u4f5c\u8fdb\u884c\u8ba1\u5212\u6062\u590d\uff0c\u5e76\u6700\u7ec8\u5229\u7528\u7b26\u53f7\u89c4\u5212\u5668\u5b8c\u6210\u8ba1\u5212\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cLLM\u5728\u8ba1\u5212\u751f\u6210\u8fc7\u7a0b\u4e2d\u6ca1\u6709\u6e05\u6670\u7684\u5e95\u5c42\u63a8\u7406\u8bc1\u636e\u3002\u6240\u63d0\u51fa\u7684\u6062\u590d\u6d41\u7a0b\u867d\u7136\u80fd\u6539\u5584\u52a8\u4f5c\u8d28\u91cf\u5e76\u5c06\u603b\u4f53\u6210\u529f\u7387\u4ece21.9%\u63d0\u9ad8\u523027.5%\uff0c\u4f46\u5e73\u5747\u800c\u8a00\uff0c\u53ea\u6709\u8ba1\u5212\u7684\u524d2.65\u4e2a\u52a8\u4f5c\u662f\u53ef\u6267\u884c\u7684\u3002\u8be5\u6d41\u7a0b\u5728\u8d28\u91cf\u548c\u53ef\u9760\u6027\u65b9\u9762\u4ecd\u8fdc\u4f4e\u4e8e\u7ecf\u5178\u7684\u89c4\u5212\u5668\u3002", "conclusion": "\u5c3d\u7ba1\u901a\u8fc7NLP\u9a71\u52a8\u7684\u5206\u6790\u548c\u6062\u590d\u673a\u5236\u53ef\u4ee5\u90e8\u5206\u6539\u8fdbLLM\u751f\u6210\u7684\u89c4\u5212\uff0c\u4f46LLM\u5728AI\u4efb\u52a1\u89c4\u5212\u4e2d\u4ecd\u7f3a\u4e4f\u53ef\u9760\u7684\u5185\u5728\u63a8\u7406\u80fd\u529b\uff0c\u5176\u8868\u73b0\u4e0e\u4f20\u7edf\u89c4\u5212\u5668\u76f8\u6bd4\u4ecd\u6709\u663e\u8457\u5dee\u8ddd\u3002"}}
{"id": "2508.01541", "pdf": "https://arxiv.org/pdf/2508.01541", "abs": "https://arxiv.org/abs/2508.01541", "authors": ["Sara C\u00e2mara", "Eduardo Luz", "Val\u00e9ria Carvalho", "Ivan Meneghini", "Gladston Moreira"], "title": "MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization", "categories": ["cs.CL"], "comment": "8 pages", "summary": "Prompt engineering is crucial for unlocking the potential of Large Language\nModels (LLMs). Still, since manual prompt design is often complex,\nnon-intuitive, and time-consuming, automatic prompt optimization has emerged as\na research area. However, a significant challenge in prompt optimization is\nmanaging the inherent trade-off between task performance, such as accuracy, and\ncontext size. Most existing automated methods focus on a single objective,\ntypically performance, thereby failing to explore the critical spectrum of\nefficiency and effectiveness. This paper introduces the MOPrompt, a novel\nMulti-objective Evolutionary Optimization (EMO) framework designed to optimize\nprompts for both accuracy and context size (measured in tokens) simultaneously.\nOur framework maps the Pareto front of prompt solutions, presenting\npractitioners with a set of trade-offs between context size and performance, a\ncrucial tool for deploying Large Language Models (LLMs) in real-world\napplications. We evaluate MOPrompt on a sentiment analysis task in Portuguese,\nusing Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that\nMOPrompt substantially outperforms the baseline framework. For the Sabiazinho\nmodel, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97)\nas the best baseline solution, but with a 31% reduction in token length.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMOPrompt\uff0c\u4e00\u4e2a\u591a\u76ee\u6807\u8fdb\u5316\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u540c\u65f6\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63d0\u793a\uff08prompt\uff09\u7684\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u5927\u5c0f\uff08token\u957f\u5ea6\uff09\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5355\u4e00\u76ee\u6807\u7684\u95ee\u9898\u3002", "motivation": "\u624b\u52a8\u8bbe\u8ba1LLM\u63d0\u793a\u590d\u6742\u3001\u4e0d\u76f4\u89c2\u4e14\u8017\u65f6\u3002\u5c3d\u7ba1\u81ea\u52a8\u5316\u63d0\u793a\u4f18\u5316\u5df2\u5174\u8d77\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u5355\u76ee\u6807\u4f18\u5316\uff0c\u901a\u5e38\u53ea\u5173\u6ce8\u6027\u80fd\uff0c\u672a\u80fd\u6709\u6548\u5e73\u8861\u4efb\u52a1\u6027\u80fd\uff08\u5982\u51c6\u786e\u6027\uff09\u4e0e\u4e0a\u4e0b\u6587\u5927\u5c0f\uff08\u6548\u7387\uff09\u4e4b\u95f4\u7684\u56fa\u6709\u6743\u8861\u3002", "method": "\u672c\u6587\u5f15\u5165MOPrompt\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u591a\u76ee\u6807\u8fdb\u5316\u4f18\u5316\uff08EMO\uff09\u6846\u67b6\uff0c\u65e8\u5728\u540c\u65f6\u4f18\u5316\u63d0\u793a\u7684\u51c6\u786e\u6027\u548c\u4e0a\u4e0b\u6587\u5927\u5c0f\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6620\u5c04\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u6027\u80fd\u4e0e\u4e0a\u4e0b\u6587\u5927\u5c0f\u4e4b\u95f4\u7684\u6743\u8861\u89e3\u51b3\u65b9\u6848\u96c6\u3002MOPrompt\u5728\u8461\u8404\u7259\u8bed\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e0a\uff0c\u4f7f\u7528Gemma-2B\u548cSabiazinho-3\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "MOPrompt\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6846\u67b6\u3002\u5bf9\u4e8eSabiazinho\u6a21\u578b\uff0cMOPrompt\u627e\u5230\u7684\u63d0\u793a\u5728\u8fbe\u5230\u4e0e\u6700\u4f73\u57fa\u7ebf\u89e3\u51b3\u65b9\u6848\u76f8\u540c\u7684\u5cf0\u503c\u51c6\u786e\u7387\uff080.97\uff09\u7684\u540c\u65f6\uff0c\u5c06token\u957f\u5ea6\u51cf\u5c11\u4e8631%\u3002", "conclusion": "MOPrompt\u6210\u529f\u5730\u5b9e\u73b0\u4e86LLM\u63d0\u793a\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\uff08\u4e0a\u4e0b\u6587\u5927\u5c0f\uff09\u65b9\u9762\u7684\u540c\u6b65\u4f18\u5316\uff0c\u4e3aLLM\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u5173\u952e\u5de5\u5177\uff0c\u5141\u8bb8\u7528\u6237\u6839\u636e\u9700\u6c42\u5728\u6027\u80fd\u548c\u8d44\u6e90\u6d88\u8017\u4e4b\u95f4\u8fdb\u884c\u9009\u62e9\u3002"}}
{"id": "2508.00956", "pdf": "https://arxiv.org/pdf/2508.00956", "abs": "https://arxiv.org/abs/2508.00956", "authors": ["Chuan He", "Yang Chen", "Wuliang Huang", "Tianyi Zheng", "Jianhu Chen", "Bin Dou", "Yice Luo", "Yun Zhu", "Baokun Wang", "Yongchao Liu", "Xing Fu", "Yu Cheng", "Chuntao Hong", "Weiqiang Wang", "Xin-Wei Yao"], "title": "Learning Unified User Quantized Tokenizers for User Representation", "categories": ["cs.LG", "cs.AI", "cs.IR"], "comment": null, "summary": "Multi-source user representation learning plays a critical role in enabling\npersonalized services on web platforms (e.g., Alipay). While prior works have\nadopted late-fusion strategies to combine heterogeneous data sources, they\nsuffer from three key limitations: lack of unified representation frameworks,\nscalability and storage issues in data compression, and inflexible cross-task\ngeneralization. To address these challenges, we propose U^2QT (Unified User\nQuantized Tokenizers), a novel framework that integrates cross-domain knowledge\ntransfer with early fusion of heterogeneous domains. Our framework employs a\ntwo-stage architecture: first, a causal Q-Former projects domain-specific\nfeatures into a shared causal representation space to preserve inter-modality\ndependencies; second, a multi-view RQ-VAE discretizes causal embeddings into\ncompact tokens through shared and source-specific codebooks, enabling efficient\nstorage while maintaining semantic coherence. Experimental results showcase\nU^2QT's advantages across diverse downstream tasks, outperforming task-specific\nbaselines in future behavior prediction and recommendation tasks while\nachieving efficiency gains in storage and computation. The unified tokenization\nframework enables seamless integration with language models and supports\nindustrial-scale applications.", "AI": {"tldr": "U^2QT\u662f\u4e00\u4e2a\u7edf\u4e00\u7528\u6237\u91cf\u5316\u5206\u8bcd\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u65e9\u671f\u878d\u5408\u5f02\u6784\u6570\u636e\u548c\u8de8\u57df\u77e5\u8bc6\u8fc1\u79fb\uff0c\u89e3\u51b3\u591a\u6e90\u7528\u6237\u8868\u793a\u5b66\u4e60\u4e2d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5b58\u50a8\u3001\u8ba1\u7b97\u548c\u66f4\u597d\u7684\u8de8\u4efb\u52a1\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u6e90\u7528\u6237\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff08\u91c7\u7528\u665a\u671f\u878d\u5408\u7b56\u7565\uff09\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u5c40\u9650\u6027\uff1a\u7f3a\u4e4f\u7edf\u4e00\u8868\u793a\u6846\u67b6\u3001\u6570\u636e\u538b\u7f29\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b58\u50a8\u95ee\u9898\uff0c\u4ee5\u53ca\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86U^2QT\uff08\u7edf\u4e00\u7528\u6237\u91cf\u5316\u5206\u8bcd\u5668\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u8de8\u57df\u77e5\u8bc6\u8fc1\u79fb\u548c\u5f02\u6784\u6570\u636e\u65e9\u671f\u878d\u5408\u3002\u5176\u91c7\u7528\u4e24\u9636\u6bb5\u67b6\u6784\uff1a\u9996\u5148\uff0c\u56e0\u679cQ-Former\u5c06\u9886\u57df\u7279\u5b9a\u7279\u5f81\u6295\u5f71\u5230\u5171\u4eab\u56e0\u679c\u8868\u793a\u7a7a\u95f4\uff1b\u5176\u6b21\uff0c\u591a\u89c6\u56feRQ-VAE\u901a\u8fc7\u5171\u4eab\u548c\u6e90\u7279\u5b9a\u7801\u672c\u5c06\u56e0\u679c\u5d4c\u5165\u79bb\u6563\u5316\u4e3a\u7d27\u51d1\u4ee4\u724c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cU^2QT\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u672a\u6765\u884c\u4e3a\u9884\u6d4b\u548c\u63a8\u8350\uff09\u4e2d\u4f18\u4e8e\u4efb\u52a1\u7279\u5b9a\u57fa\u7ebf\uff0c\u5e76\u5728\u5b58\u50a8\u548c\u8ba1\u7b97\u65b9\u9762\u5b9e\u73b0\u4e86\u6548\u7387\u63d0\u5347\u3002"}}
{"id": "2508.01151", "pdf": "https://arxiv.org/pdf/2508.01151", "abs": "https://arxiv.org/abs/2508.01151", "authors": ["Yu Lei", "Jinbin Bai", "Qingyu Shi", "Aosong Feng", "Kaidong Yu"], "title": "Personalized Safety Alignment for Text-to-Image Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 8 figures, 4 tables", "summary": "Text-to-image diffusion models have revolutionized visual content generation,\nbut current safety mechanisms apply uniform standards that often fail to\naccount for individual user preferences. These models overlook the diverse\nsafety boundaries shaped by factors like age, mental health, and personal\nbeliefs. To address this, we propose Personalized Safety Alignment (PSA), a\nframework that allows user-specific control over safety behaviors in generative\nmodels. PSA integrates personalized user profiles into the diffusion process,\nadjusting the model's behavior to match individual safety preferences while\npreserving image quality. We introduce a new dataset, Sage, which captures\nuser-specific safety preferences and incorporates these profiles through a\ncross-attention mechanism. Experiments show that PSA outperforms existing\nmethods in harmful content suppression and aligns generated content better with\nuser constraints, achieving higher Win Rate and Pass Rate scores. Our code,\ndata, and models are publicly available at\nhttps://torpedo2648.github.io/PSAlign/.", "AI": {"tldr": "\u63d0\u51fa\u4e2a\u6027\u5316\u5b89\u5168\u5bf9\u9f50\u6846\u67b6PSA\uff0c\u4f7f\u6587\u751f\u56fe\u6a21\u578b\u80fd\u6839\u636e\u7528\u6237\u504f\u597d\u8c03\u6574\u5b89\u5168\u884c\u4e3a\u3002", "motivation": "\u5f53\u524d\u6587\u751f\u56fe\u6269\u6563\u6a21\u578b\u7684\u5b89\u5168\u673a\u5236\u91c7\u7528\u7edf\u4e00\u6807\u51c6\uff0c\u672a\u80fd\u8003\u8651\u4e2a\u4f53\u7528\u6237\u504f\u597d\uff0c\u5ffd\u89c6\u4e86\u7531\u5e74\u9f84\u3001\u5fc3\u7406\u5065\u5eb7\u548c\u4e2a\u4eba\u4fe1\u4ef0\u7b49\u56e0\u7d20\u5851\u9020\u7684\u5dee\u5f02\u5316\u5b89\u5168\u8fb9\u754c\u3002", "method": "\u63d0\u51fa\u201c\u4e2a\u6027\u5316\u5b89\u5168\u5bf9\u9f50\uff08PSA\uff09\u201d\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4e2a\u6027\u5316\u7528\u6237\u914d\u7f6e\u6587\u4ef6\u6574\u5408\u5230\u6269\u6563\u8fc7\u7a0b\u4e2d\uff0c\u8c03\u6574\u6a21\u578b\u884c\u4e3a\u4ee5\u5339\u914d\u7528\u6237\u5b89\u5168\u504f\u597d\u3002\u4e3a\u6b64\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6Sage\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u8fd9\u4e9b\u7528\u6237\u914d\u7f6e\u6587\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPSA\u5728\u6709\u5bb3\u5185\u5bb9\u6291\u5236\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u4f7f\u751f\u6210\u5185\u5bb9\u7b26\u5408\u7528\u6237\u9650\u5236\uff0c\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u80dc\u7387\uff08Win Rate\uff09\u548c\u901a\u8fc7\u7387\uff08Pass Rate\uff09\u3002", "conclusion": "PSA\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u6587\u751f\u56fe\u6a21\u578b\u7684\u4e2a\u6027\u5316\u5b89\u5168\u5bf9\u9f50\uff0c\u4f7f\u6a21\u578b\u80fd\u6839\u636e\u7528\u6237\u504f\u597d\u8c03\u6574\u5b89\u5168\u884c\u4e3a\uff0c\u5728\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u6709\u6548\u6291\u5236\u6709\u5bb3\u5185\u5bb9\u5e76\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea6\u3002"}}
{"id": "2508.01306", "pdf": "https://arxiv.org/pdf/2508.01306", "abs": "https://arxiv.org/abs/2508.01306", "authors": ["Yelim Ahn", "Jaejin Lee"], "title": "PUZZLED: Jailbreaking LLMs through Word-Based Puzzles", "categories": ["cs.AI", "cs.CR"], "comment": "15 pages", "summary": "As large language models (LLMs) are increasingly deployed across diverse\ndomains, ensuring their safety has become a critical concern. In response,\nstudies on jailbreak attacks have been actively growing. Existing approaches\ntypically rely on iterative prompt engineering or semantic transformations of\nharmful instructions to evade detection. In this work, we introduce PUZZLED, a\nnovel jailbreak method that leverages the LLM's reasoning capabilities. It\nmasks keywords in a harmful instruction and presents them as word puzzles for\nthe LLM to solve. We design three puzzle types-word search, anagram, and\ncrossword-that are familiar to humans but cognitively demanding for LLMs. The\nmodel must solve the puzzle to uncover the masked words and then proceed to\ngenerate responses to the reconstructed harmful instruction. We evaluate\nPUZZLED on five state-of-the-art LLMs and observe a high average attack success\nrate (ASR) of 88.8%, specifically 96.5% on GPT-4.1 and 92.3% on Claude 3.7\nSonnet. PUZZLED is a simple yet powerful attack that transforms familiar\npuzzles into an effective jailbreak strategy by harnessing LLMs' reasoning\ncapabilities.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPUZZLED\u7684\u65b0\u578b\u8d8a\u72f1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6709\u5bb3\u6307\u4ee4\u9690\u85cf\u5728\u5355\u8bcd\u8c1c\u9898\u4e2d\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u968f\u7740LLM\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5176\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8fed\u4ee3\u63d0\u793a\u5de5\u7a0b\u6216\u8bed\u4e49\u8f6c\u6362\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u65b0\u7684\u8d8a\u72f1\u7b56\u7565\uff0c\u901a\u8fc7\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\u6765\u89c4\u907f\u5b89\u5168\u68c0\u6d4b\u3002", "method": "\u5f15\u5165PUZZLED\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u6709\u5bb3\u6307\u4ee4\u4e2d\u7684\u5173\u952e\u8bcd\u9690\u85cf\u4e3a\u5355\u8bcd\u8c1c\u9898\uff08\u5305\u62ec\u5355\u8bcd\u641c\u7d22\u3001\u5b57\u8c1c\u548c\u586b\u5b57\u6e38\u620f\uff09\u3002LLM\u5fc5\u987b\u5148\u89e3\u5f00\u8c1c\u9898\u4ee5\u63ed\u793a\u88ab\u63a9\u76d6\u7684\u8bcd\u6c47\uff0c\u7136\u540e\u624d\u80fd\u54cd\u5e94\u91cd\u5efa\u540e\u7684\u6709\u5bb3\u6307\u4ee4\u3002", "result": "\u5728\u4e94\u79cd\u4e3b\u6d41LLM\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0cPUZZLED\u7684\u5e73\u5747\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\u8fbe\u523088.8%\uff0c\u5176\u4e2d\u5bf9GPT-4.1\u7684\u6210\u529f\u7387\u4e3a96.5%\uff0c\u5bf9Claude 3.7 Sonnet\u7684\u6210\u529f\u7387\u4e3a92.3%\u3002", "conclusion": "PUZZLED\u662f\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c06\u4eba\u7c7b\u719f\u6089\u7684\u8c1c\u9898\u8f6c\u5316\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u8d8a\u72f1\u7b56\u7565\u3002"}}
{"id": "2508.01554", "pdf": "https://arxiv.org/pdf/2508.01554", "abs": "https://arxiv.org/abs/2508.01554", "authors": ["Yujia Zheng", "Tianhao Li", "Haotian Huang", "Tianyu Zeng", "Jingyu Lu", "Chuangxin Chu", "Yuekai Huang", "Ziyou Jiang", "Qian Xiong", "Yuyao Ge", "Mingyang Li"], "title": "Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Prompt-based adversarial attacks have become an effective means to assess the\nrobustness of large language models (LLMs). However, existing approaches often\ntreat prompts as monolithic text, overlooking their structural\nheterogeneity-different prompt components contribute unequally to adversarial\nrobustness. Prior works like PromptRobust assume prompts are value-neutral, but\nour analysis reveals that complex, domain-specific prompts with rich structures\nhave components with differing vulnerabilities. To address this gap, we\nintroduce PromptAnatomy, an automated framework that dissects prompts into\nfunctional components and generates diverse, interpretable adversarial examples\nby selectively perturbing each component using our proposed method, ComPerturb.\nTo ensure linguistic plausibility and mitigate distribution shifts, we further\nincorporate a perplexity (PPL)-based filtering mechanism. As a complementary\nresource, we annotate four public instruction-tuning datasets using the\nPromptAnatomy framework, verified through human review. Extensive experiments\nacross these datasets and five advanced LLMs demonstrate that ComPerturb\nachieves state-of-the-art attack success rates. Ablation studies validate the\ncomplementary benefits of prompt dissection and PPL filtering. Our results\nunderscore the importance of prompt structure awareness and controlled\nperturbation for reliable adversarial robustness evaluation in LLMs. Code and\ndata are available at https://github.com/Yujiaaaaa/PACP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPromptAnatomy\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u5256\u63d0\u793a\u8bcd\u7ec4\u4ef6\u5e76\u4f7f\u7528ComPerturb\u9009\u62e9\u6027\u6270\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u5bf9\u6297\u6837\u672c\uff0c\u7ed3\u5408PPL\u8fc7\u6ee4\u673a\u5236\uff0c\u5b9e\u73b0\u4e86LLM\u5bf9\u6297\u653b\u51fb\u7684SOTA\u6548\u679c\uff0c\u5f3a\u8c03\u4e86\u63d0\u793a\u8bcd\u7ed3\u6784\u611f\u77e5\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63d0\u793a\u8bcd\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u901a\u5e38\u5c06\u63d0\u793a\u8bcd\u89c6\u4e3a\u5355\u4e00\u6574\u4f53\uff0c\u5ffd\u7565\u4e86\u5176\u5185\u90e8\u7ed3\u6784\u5f02\u6784\u6027\uff0c\u5373\u4e0d\u540c\u7ec4\u4ef6\u5bf9\u5bf9\u6297\u9c81\u68d2\u6027\u7684\u8d21\u732e\u4e0d\u5747\u3002\u8fd9\u5bfc\u81f4\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u590d\u6742\u3001\u9886\u57df\u7279\u5b9a\u63d0\u793a\u8bcd\u4e2d\u4e0d\u540c\u7ec4\u4ef6\u7684\u8106\u5f31\u6027\u5dee\u5f02\u3002", "method": "\u5f15\u5165PromptAnatomy\u6846\u67b6\uff0c\u81ea\u52a8\u5c06\u63d0\u793a\u8bcd\u89e3\u5256\u4e3a\u529f\u80fd\u6027\u7ec4\u4ef6\u3002\u63d0\u51faComPerturb\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6270\u52a8\u6bcf\u4e2a\u7ec4\u4ef6\u6765\u751f\u6210\u591a\u6837\u5316\u3001\u53ef\u89e3\u91ca\u7684\u5bf9\u6297\u6837\u672c\u3002\u4e3a\u786e\u4fdd\u8bed\u8a00\u5408\u7406\u6027\u5e76\u51cf\u8f7b\u5206\u5e03\u504f\u79fb\uff0c\u8fdb\u4e00\u6b65\u7ed3\u5408\u4e86\u57fa\u4e8e\u56f0\u60d1\u5ea6\uff08PPL\uff09\u7684\u8fc7\u6ee4\u673a\u5236\u3002\u6b64\u5916\uff0c\u4f7f\u7528PromptAnatomy\u6846\u67b6\u5bf9\u56db\u4e2a\u516c\u5171\u6307\u4ee4\u5fae\u8c03\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u6807\u6ce8\uff0c\u5e76\u7ecf\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u548c\u4e94\u4e2a\u5148\u8fdbLLM\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cComPerturb\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u653b\u51fb\u6210\u529f\u7387\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u63d0\u793a\u8bcd\u89e3\u5256\u548cPPL\u8fc7\u6ee4\u7684\u4e92\u8865\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728LLMs\u5bf9\u6297\u9c81\u68d2\u6027\u8bc4\u4f30\u4e2d\uff0c\u63d0\u793a\u8bcd\u7ed3\u6784\u611f\u77e5\u548c\u53d7\u63a7\u6270\u52a8\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.00957", "pdf": "https://arxiv.org/pdf/2508.00957", "abs": "https://arxiv.org/abs/2508.00957", "authors": ["Amrit Rajeev", "Udayaadithya Avadhanam", "Harshula Tulapurkar", "SaiBarath Sundar"], "title": "Small sample-based adaptive text classification through iterative and contrastive description refinement", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Zero-shot text classification remains a difficult task in domains with\nevolving knowledge and ambiguous category boundaries, such as ticketing\nsystems. Large language models (LLMs) often struggle to generalize in these\nscenarios due to limited topic separability, while few-shot methods are\nconstrained by insufficient data diversity. We propose a classification\nframework that combines iterative topic refinement, contrastive prompting, and\nactive learning. Starting with a small set of labeled samples, the model\ngenerates initial topic labels. Misclassified or ambiguous samples are then\nused in an iterative contrastive prompting process to refine category\ndistinctions by explicitly teaching the model to differentiate between closely\nrelated classes. The framework features a human-in-the-loop component, allowing\nusers to introduce or revise category definitions in natural language. This\nenables seamless integration of new, unseen categories without retraining,\nmaking the system well-suited for real-world, dynamic environments. The\nevaluations on AGNews and DBpedia demonstrate strong performance: 91% accuracy\non AGNews (3 seen, 1 unseen class) and 84% on DBpedia (8 seen, 1 unseen), with\nminimal accuracy shift after introducing unseen classes (82% and 87%,\nrespectively). The results highlight the effectiveness of prompt-based semantic\nreasoning for fine-grained classification with limited supervision.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u8fed\u4ee3\u4e3b\u9898\u7ec6\u5316\u3001\u5bf9\u6bd4\u63d0\u793a\u548c\u4e3b\u52a8\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u96f6\u6837\u672c\u6587\u672c\u5206\u7c7b\u7684\u6311\u6218\uff0c\u5e76\u5728\u5f15\u5165\u672a\u89c1\u7c7b\u522b\u65f6\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u96f6\u6837\u672c\u6587\u672c\u5206\u7c7b\u5728\u77e5\u8bc6\u4e0d\u65ad\u6f14\u53d8\u3001\u7c7b\u522b\u8fb9\u754c\u6a21\u7cca\u7684\u9886\u57df\uff08\u5982\u5de5\u5355\u7cfb\u7edf\uff09\u4ecd\u7136\u662f\u4e00\u9879\u96be\u9898\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u56e0\u4e3b\u9898\u53ef\u5206\u79bb\u6027\u6709\u9650\u800c\u96be\u4ee5\u6cdb\u5316\uff0c\u800c\u5c11\u6837\u672c\u65b9\u6cd5\u5219\u53d7\u9650\u4e8e\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u8fed\u4ee3\u4e3b\u9898\u7ec6\u5316\u3001\u5bf9\u6bd4\u63d0\u793a\u548c\u4e3b\u52a8\u5b66\u4e60\u3002\u8be5\u6846\u67b6\u4ece\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u5f00\u59cb\uff0c\u901a\u8fc7\u8fed\u4ee3\u5bf9\u6bd4\u63d0\u793a\u8fc7\u7a0b\u7ec6\u5316\u7c7b\u522b\u533a\u5206\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u5e72\u9884\u7ec4\u4ef6\u5141\u8bb8\u7528\u6237\u5f15\u5165\u6216\u4fee\u6539\u7c7b\u522b\u5b9a\u4e49\uff0c\u4ece\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u65e0\u7f1d\u96c6\u6210\u65b0\u7684\u672a\u89c1\u7c7b\u522b\u3002", "result": "\u5728AGNews\u6570\u636e\u96c6\uff083\u4e2a\u5df2\u77e5\u7c7b\u522b\uff0c1\u4e2a\u672a\u89c1\u7c7b\u522b\uff09\u4e0a\u8fbe\u523091%\u51c6\u786e\u7387\uff0c\u5728DBpedia\u6570\u636e\u96c6\uff088\u4e2a\u5df2\u77e5\u7c7b\u522b\uff0c1\u4e2a\u672a\u89c1\u7c7b\u522b\uff09\u4e0a\u8fbe\u523084%\u51c6\u786e\u7387\u3002\u5728\u5f15\u5165\u672a\u89c1\u7c7b\u522b\u540e\uff0c\u51c6\u786e\u7387\u4ec5\u6709\u8f7b\u5fae\u4e0b\u964d\uff08\u5206\u522b\u4e3a82%\u548c87%\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u76d1\u7763\u4e0b\uff0c\u5229\u7528\u57fa\u4e8e\u63d0\u793a\u7684\u8bed\u4e49\u63a8\u7406\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.01152", "pdf": "https://arxiv.org/pdf/2508.01152", "abs": "https://arxiv.org/abs/2508.01152", "authors": ["Xinyu Yan", "Meijun Sun", "Ge-Peng Ji", "Fahad Shahbaz Khan", "Salman Khan", "Deng-Ping Fan"], "title": "LawDIS: Language-Window-based Controllable Dichotomous Image Segmentation", "categories": ["cs.CV"], "comment": "17 pages, 10 figures, ICCV 2025", "summary": "We present LawDIS, a language-window-based controllable dichotomous image\nsegmentation (DIS) framework that produces high-quality object masks. Our\nframework recasts DIS as an image-conditioned mask generation task within a\nlatent diffusion model, enabling seamless integration of user controls. LawDIS\nis enhanced with macro-to-micro control modes. Specifically, in macro mode, we\nintroduce a language-controlled segmentation strategy (LS) to generate an\ninitial mask based on user-provided language prompts. In micro mode, a\nwindow-controlled refinement strategy (WR) allows flexible refinement of\nuser-defined regions (i.e., size-adjustable windows) within the initial mask.\nCoordinated by a mode switcher, these modes can operate independently or\njointly, making the framework well-suited for high-accuracy, personalised\napplications. Extensive experiments on the DIS5K benchmark reveal that our\nLawDIS significantly outperforms 11 cutting-edge methods across all metrics.\nNotably, compared to the second-best model MVANet, we achieve $F_\\beta^\\omega$\ngains of 4.6\\% with both the LS and WR strategies and 3.6\\% gains with only the\nLS strategy on DIS-TE. Codes will be made available at\nhttps://github.com/XinyuYanTJU/LawDIS.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.01323", "pdf": "https://arxiv.org/pdf/2508.01323", "abs": "https://arxiv.org/abs/2508.01323", "authors": ["Faruk Alpay", "Bugra Kilictas", "Taylan Alpay", "Hamdi Alakkad"], "title": "Idempotent Equilibrium Analysis of Hybrid Workflow Allocation: A Mathematical Schema for Future Work", "categories": ["cs.AI", "cs.CY", "econ.GN", "q-fin.EC", "47H10, 06B10, 91B40, 91B55, 68T20", "I.2.0; I.2.11; F.4.1"], "comment": "25 pages, 9 figures, 4 tables. Proves existence/uniqueness of an\n  \"idempotent equilibrium\" for human-AI task allocation and provides\n  closed-form steady-state automation share", "summary": "The rapid advance of large-scale AI systems is reshaping how work is divided\nbetween people and machines. We formalise this reallocation as an iterated\ntask-delegation map and show that--under broad, empirically grounded\nassumptions--the process converges to a stable idempotent equilibrium in which\nevery task is performed by the agent (human or machine) with enduring\ncomparative advantage. Leveraging lattice-theoretic fixed-point tools (Tarski\nand Banach), we (i) prove existence of at least one such equilibrium and (ii)\nderive mild monotonicity conditions that guarantee uniqueness. In a stylised\ncontinuous model the long-run automated share takes the closed form $x^* =\n\\alpha / (\\alpha + \\beta)$, where $\\alpha$ captures the pace of automation and\n$\\beta$ the rate at which new, human-centric tasks appear; hence full\nautomation is precluded whenever $\\beta > 0$. We embed this analytic result in\nthree complementary dynamical benchmarks--a discrete linear update, an\nevolutionary replicator dynamic, and a continuous Beta-distributed task\nspectrum--each of which converges to the same mixed equilibrium and is\nreproducible from the provided code-free formulas. A 2025-to-2045 simulation\ncalibrated to current adoption rates projects automation rising from\napproximately 10% of work to approximately 65%, leaving a persistent one-third\nof tasks to humans. We interpret that residual as a new profession of workflow\nconductor: humans specialise in assigning, supervising and integrating AI\nmodules rather than competing with them. Finally, we discuss implications for\nskill development, benchmark design and AI governance, arguing that policies\nwhich promote \"centaur\" human-AI teaming can steer the economy toward the\nwelfare-maximising fixed point.", "AI": {"tldr": "\u8bba\u6587\u5f62\u5f0f\u5316\u4e86\u4eba\u673a\u5de5\u4f5c\u5206\u5de5\u7684\u52a8\u6001\uff0c\u9884\u6d4b\u81ea\u52a8\u5316\u5c06\u663e\u8457\u589e\u957f\uff0c\u4f46\u4eba\u7c7b\u4ecd\u4fdd\u7559\u5173\u952e\u89d2\u8272\uff08\u5982AI\u6307\u6325\u8005\uff09\uff0c\u5e76\u5f3a\u8c03\u4fc3\u8fdb\u4eba\u673a\u534f\u4f5c\u653f\u7b56\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u8ba8\u5927\u578bAI\u7cfb\u7edf\u5feb\u901f\u53d1\u5c55\u5982\u4f55\u91cd\u5851\u4eba\u673a\u5de5\u4f5c\u5206\u5de5\uff0c\u5e76\u5f62\u5f0f\u5316\u7406\u89e3\u6b64\u8fc7\u7a0b\u7684\u957f\u671f\u5747\u8861\u72b6\u6001\u3002", "method": "1. \u5c06\u4efb\u52a1\u518d\u5206\u914d\u5f62\u5f0f\u5316\u4e3a\u8fed\u4ee3\u4efb\u52a1\u59d4\u6258\u6620\u5c04\u30022. \u5229\u7528\u683c\u8bba\u4e0d\u52a8\u70b9\u5de5\u5177\uff08Tarski\u548cBanach\uff09\u8bc1\u660e\u5747\u8861\u5b58\u5728\u6027\u53ca\u552f\u4e00\u6027\u30023. \u6784\u5efa\u8fde\u7eed\u6a21\u578b\u63a8\u5bfc\u957f\u671f\u81ea\u52a8\u5316\u4efd\u989d\u7684\u89e3\u6790\u89e3\u30024. \u901a\u8fc7\u591a\u79cd\u52a8\u6001\u6a21\u578b\uff08\u79bb\u6563\u7ebf\u6027\u66f4\u65b0\u3001\u6f14\u5316\u590d\u5236\u5668\u52a8\u529b\u5b66\u3001Beta\u5206\u5e03\u4efb\u52a1\u8c31\uff09\u9a8c\u8bc1\u7ed3\u679c\u30025. \u8fdb\u884c2025-2045\u5e74\u6a21\u62df\u9884\u6d4b\u3002", "result": "1. \u5de5\u4f5c\u5206\u914d\u8fc7\u7a0b\u6536\u655b\u4e8e\u4e00\u4e2a\u7a33\u5b9a\u5747\u8861\uff0c\u4efb\u52a1\u7531\u5177\u6709\u6bd4\u8f83\u4f18\u52bf\u7684\u667a\u80fd\u4f53\u6267\u884c\u30022. \u957f\u671f\u81ea\u52a8\u5316\u4efd\u989d\u7531 $\\alpha / (\\alpha + \\beta)$ \u51b3\u5b9a\uff0c\u5176\u4e2d $\\beta > 0$ \u5219\u65e0\u6cd5\u5b8c\u5168\u81ea\u52a8\u5316\u30023. \u6a21\u62df\u9884\u6d4b\u52302045\u5e74\u81ea\u52a8\u5316\u7387\u5c06\u8fbe\u7ea665%\uff0c\u4f46\u4ecd\u6709\u7ea6\u4e09\u5206\u4e4b\u4e00\u4efb\u52a1\u7531\u4eba\u7c7b\u5b8c\u6210\u30024. \u4eba\u7c7b\u672a\u6765\u89d2\u8272\u5c06\u4fa7\u91cd\u4e8e\u201c\u5de5\u4f5c\u6d41\u6307\u6325\u8005\u201d\uff0c\u5373\u5206\u914d\u3001\u76d1\u7763\u548c\u6574\u5408AI\u6a21\u5757\u3002", "conclusion": "\u4eba\u673a\u5206\u5de5\u5c06\u8d8b\u4e8e\u7a33\u5b9a\u5747\u8861\uff0cAI\u663e\u8457\u63d0\u5347\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u4f46\u4eba\u7c7b\u4ecd\u5c06\u4fdd\u7559\u6838\u5fc3\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5728AI\u7ba1\u7406\u4e0e\u6574\u5408\u65b9\u9762\u3002\u653f\u7b56\u5e94\u652f\u6301\u201c\u534a\u4eba\u9a6c\u201d\u5f0f\u4eba\u673a\u534f\u4f5c\uff0c\u4ee5\u5b9e\u73b0\u6700\u5927\u5316\u798f\u5229\u3002"}}
{"id": "2508.01630", "pdf": "https://arxiv.org/pdf/2508.01630", "abs": "https://arxiv.org/abs/2508.01630", "authors": ["Maziyar Panahi"], "title": "OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Named-entity recognition (NER) is fundamental to extracting structured\ninformation from the >80% of healthcare data that resides in unstructured\nclinical notes and biomedical literature. Despite recent advances with large\nlanguage models, achieving state-of-the-art performance across diverse entity\ntypes while maintaining computational efficiency remains a significant\nchallenge. We introduce OpenMed NER, a suite of open-source, domain-adapted\ntransformer models that combine lightweight domain-adaptive pre-training (DAPT)\nwith parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs\ncost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,\npublicly available research repositories and de-identified clinical notes\n(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA\nbackbones. This is followed by task-specific fine-tuning with LoRA, which\nupdates less than 1.5% of model parameters. We evaluate our models on 12\nestablished biomedical NER benchmarks spanning chemicals, diseases, genes, and\nspecies. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of\nthese 12 datasets, with substantial gains across diverse entity types. Our\nmodels advance the state-of-the-art on foundational disease and chemical\nbenchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger\nimprovements of over 5.3 and 9.7 percentage points on more specialized gene and\nclinical cell line corpora. This work demonstrates that strategically adapted\nopen-source models can surpass closed-source solutions. This performance is\nachieved with remarkable efficiency: training completes in under 12 hours on a\nsingle GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively\nlicensed, open-source checkpoints designed to help practitioners facilitate\ncompliance with emerging data protection and AI regulations, such as the EU AI\nAct.", "AI": {"tldr": "OpenMed NER\u662f\u4e00\u79cd\u7ed3\u5408DAPT\u548cLoRA\u7684\u5f00\u6e90\u9886\u57df\u9002\u5e94\u578bTransformer\u6a21\u578b\uff0c\u572812\u4e2a\u751f\u7269\u533b\u5b66NER\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u670910\u4e2a\u8fbe\u5230\u4e86\u65b0\u7684SOTA\uff0c\u4e14\u8bad\u7ec3\u6548\u7387\u9ad8\uff0c\u78b3\u8db3\u8ff9\u4f4e\u3002", "motivation": "\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u5bf9\u4e8e\u4ece\u975e\u7ed3\u6784\u5316\u533b\u7597\u4fdd\u5065\u6570\u636e\u4e2d\u63d0\u53d6\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u591a\u7c7b\u578b\u5b9e\u4f53\u8bc6\u522b\u7684\u9876\u5c16\u6027\u80fd\u4ecd\u662f\u4e00\u5927\u6311\u6218\u3002", "method": "\u5f15\u5165OpenMed NER\uff0c\u4e00\u5957\u5f00\u6e90\u7684\u9886\u57df\u9002\u5e94\u578bTransformer\u6a21\u578b\u3002\u5b83\u7ed3\u5408\u4e86\u8f7b\u91cf\u7ea7\u9886\u57df\u9002\u5e94\u6027\u9884\u8bad\u7ec3\uff08DAPT\uff09\u548c\u53c2\u6570\u9ad8\u6548\u7684\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u3002DAPT\u5728\u4e00\u4e2a\u5305\u542b35\u4e07\u6bb5\u6587\u672c\u7684\u751f\u7269\u533b\u5b66\u8bed\u6599\u5e93\uff08PubMed, arXiv, MIMIC-III\uff09\u4e0a\u8fdb\u884c\uff0c\u4f7f\u7528DeBERTa-v3\u3001PubMedBERT\u548cBioELECTRA\u4f5c\u4e3a\u9aa8\u5e72\u6a21\u578b\u3002\u968f\u540e\uff0c\u901a\u8fc7LoRA\u8fdb\u884c\u4efb\u52a1\u7279\u5f02\u6027\u5fae\u8c03\uff0c\u4ec5\u66f4\u65b0\u4e0d\u52301.5%\u7684\u6a21\u578b\u53c2\u6570\u3002", "result": "\u572812\u4e2a\u751f\u7269\u533b\u5b66NER\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOpenMed NER\u5728\u5176\u4e2d10\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u5faeF1\u5206\u6570SOTA\uff0c\u5e76\u5728\u75be\u75c5\u3001\u5316\u5b66\u54c1\u3001\u57fa\u56e0\u548c\u4e34\u5e8a\u7ec6\u80de\u7cfb\u7b49\u591a\u79cd\u5b9e\u4f53\u7c7b\u578b\u4e0a\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff08\u5982BC5CDR-Disease\u63d0\u53472.70\u4e2a\u767e\u5206\u70b9\uff0c\u57fa\u56e0/\u4e34\u5e8a\u7ec6\u80de\u7cfb\u63d0\u5347\u8d85\u8fc75.3\u548c9.7\u4e2a\u767e\u5206\u70b9\uff09\u3002\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u9ad8\uff0c\u53ef\u5728\u5355GPU\u4e0a\u4e8e12\u5c0f\u65f6\u5185\u5b8c\u6210\uff0c\u78b3\u8db3\u8ff9\u4f4e\uff08< 1.2 kg CO2e\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u7b56\u7565\u6027\u9002\u5e94\u7684\u5f00\u6e90\u6a21\u578b\u80fd\u591f\u8d85\u8d8a\u5c01\u95ed\u6e90\u4ee3\u7801\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u5353\u8d8a\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u540c\u65f6\u6709\u52a9\u4e8e\u7b26\u5408\u65b0\u5174\u7684\u6570\u636e\u4fdd\u62a4\u548cAI\u6cd5\u89c4\u3002"}}
{"id": "2508.00959", "pdf": "https://arxiv.org/pdf/2508.00959", "abs": "https://arxiv.org/abs/2508.00959", "authors": ["Rub\u00e9n Mu\u00f1oz-Sierra", "Manuel Doblar\u00e9", "Jacobo Ayensa-Jim\u00e9nez"], "title": "Enhancing material behavior discovery using embedding-oriented Physically-Guided Neural Networks with Internal Variables", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Physically Guided Neural Networks with Internal Variables are SciML tools\nthat use only observable data for training and and have the capacity to unravel\ninternal state relations. They incorporate physical knowledge both by\nprescribing the model architecture and using loss regularization, thus endowing\ncertain specific neurons with a physical meaning as internal state variables.\nDespite their potential, these models face challenges in scalability when\napplied to high-dimensional data such as fine-grid spatial fields or\ntime-evolving systems. In this work, we propose some enhancements to the PGNNIV\nframework that address these scalability limitations through reduced-order\nmodeling techniques. Specifically, we introduce alternatives to the original\ndecoder structure using spectral decomposition, POD, and pretrained\nautoencoder-based mappings. These surrogate decoders offer varying trade-offs\nbetween computational efficiency, accuracy, noise tolerance, and\ngeneralization, while improving drastically the scalability. Additionally, we\nintegrate model reuse via transfer learning and fine-tuning strategies to\nexploit previously acquired knowledge, supporting efficient adaptation to novel\nmaterials or configurations, and significantly reducing training time while\nmaintaining or improving model performance. To illustrate these various\ntechniques, we use a representative case governed by the nonlinear diffusion\nequation, using only observable data. Results demonstrate that the enhanced\nPGNNIV framework successfully identifies the underlying constitutive state\nequations while maintaining high predictive accuracy. It also improves\nrobustness to noise, mitigates overfitting, and reduces computational demands.\nThe proposed techniques can be tailored to various scenarios depending on data\navailability, resources, and specific modeling objectives, overcoming\nscalability challenges in all the scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u5f15\u5165\u964d\u9636\u5efa\u6a21\u6280\u672f\uff08\u5982\u8c31\u5206\u89e3\u3001POD\u3001\u9884\u8bad\u7ec3\u81ea\u7f16\u7801\u5668\uff09\u548c\u8fc1\u79fb\u5b66\u4e60\uff0c\u589e\u5f3a\u4e86\u7269\u7406\u5f15\u5bfc\u795e\u7ecf\u7f51\u7edc\uff08PGNNIV\uff09\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5176\u5728\u9ad8\u7ef4\u6570\u636e\u5e94\u7528\u4e2d\u7684\u53ef\u4f38\u7f29\u6027\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u589e\u5f3a\u540e\u7684\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u53ef\u4f38\u7f29\u6027\u3001\u9884\u6d4b\u7cbe\u5ea6\u3001\u566a\u58f0\u9c81\u68d2\u6027\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u6210\u529f\u8bc6\u522b\u4e86\u5185\u90e8\u72b6\u6001\u65b9\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684\u7269\u7406\u5f15\u5bfc\u795e\u7ecf\u7f51\u7edc\uff08PGNNIV\uff09\u5728\u5e94\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\uff08\u5982\u7cbe\u7ec6\u7f51\u683c\u7a7a\u95f4\u573a\u6216\u65f6\u95f4\u6f14\u5316\u7cfb\u7edf\uff09\u65f6\uff0c\u9762\u4e34\u663e\u8457\u7684\u53ef\u4f38\u7f29\u6027\u6311\u6218\u3002", "method": "\u901a\u8fc7\u964d\u9636\u5efa\u6a21\u6280\u672f\u589e\u5f3aPGNNIV\u6846\u67b6\uff0c\u5177\u4f53\u5305\u62ec\u4e3a\u89e3\u7801\u5668\u7ed3\u6784\u5f15\u5165\u8c31\u5206\u89e3\u3001POD\u548c\u57fa\u4e8e\u9884\u8bad\u7ec3\u81ea\u7f16\u7801\u5668\u7684\u66ff\u4ee3\u6620\u5c04\u3002\u6b64\u5916\uff0c\u6574\u5408\u4e86\u8fc1\u79fb\u5b66\u4e60\u548c\u5fae\u8c03\u7b56\u7565\u4ee5\u5b9e\u73b0\u6a21\u578b\u91cd\u7528\uff0c\u52a0\u901f\u8bad\u7ec3\u5e76\u9002\u5e94\u65b0\u573a\u666f\u3002\u7814\u7a76\u901a\u8fc7\u975e\u7ebf\u6027\u6269\u6563\u65b9\u7a0b\u6848\u4f8b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u589e\u5f3a\u540e\u7684PGNNIV\u6846\u67b6\u6210\u529f\u8bc6\u522b\u4e86\u5e95\u5c42\u7684\u672c\u6784\u72b6\u6001\u65b9\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002\u5b83\u8fd8\u63d0\u9ad8\u4e86\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u51cf\u8f7b\u4e86\u8fc7\u62df\u5408\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u9700\u6c42\u3002\u8fd9\u4e9b\u6280\u672f\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5747\u6709\u6548\u514b\u670d\u4e86\u53ef\u4f38\u7f29\u6027\u6311\u6218\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u589e\u5f3aPGNNIV\u6280\u672f\u5177\u6709\u9ad8\u5ea6\u9002\u5e94\u6027\uff0c\u80fd\u591f\u6839\u636e\u5177\u4f53\u9700\u6c42\u8fdb\u884c\u8c03\u6574\uff0c\u5e76\u5728\u5404\u79cd\u573a\u666f\u4e2d\u6709\u6548\u514b\u670d\u53ef\u4f38\u7f29\u6027\u6311\u6218\uff0c\u540c\u65f6\u63d0\u4f9b\u9ad8\u6027\u80fd\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.01153", "pdf": "https://arxiv.org/pdf/2508.01153", "abs": "https://arxiv.org/abs/2508.01153", "authors": ["Xiahan Yang", "Hui Zheng"], "title": "TEACH: Text Encoding as Curriculum Hints for Scene Text Recognition", "categories": ["cs.CV"], "comment": "9 pages (w/o ref), 5 figures, 7 tables", "summary": "Scene Text Recognition (STR) remains a challenging task due to complex visual\nappearances and limited semantic priors. We propose TEACH, a novel training\nparadigm that injects ground-truth text into the model as auxiliary input and\nprogressively reduces its influence during training. By encoding target labels\ninto the embedding space and applying loss-aware masking, TEACH simulates a\ncurriculum learning process that guides the model from label-dependent learning\nto fully visual recognition. Unlike language model-based approaches, TEACH\nrequires no external pretraining and introduces no inference overhead. It is\nmodel-agnostic and can be seamlessly integrated into existing encoder-decoder\nframeworks. Extensive experiments across multiple public benchmarks show that\nmodels trained with TEACH achieve consistently improved accuracy, especially\nunder challenging conditions, validating its robustness and general\napplicability.", "AI": {"tldr": "TEACH\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u6ce8\u5165\u771f\u503c\u6587\u672c\u4f5c\u4e3a\u8f85\u52a9\u8f93\u5165\u5e76\u9010\u6b65\u51cf\u5c11\u5176\u5f71\u54cd\uff0c\u6765\u6539\u8fdb\u573a\u666f\u6587\u672c\u8bc6\u522b\uff08STR\uff09\u7684\u51c6\u786e\u6027\uff0c\u65e0\u9700\u5916\u90e8\u9884\u8bad\u7ec3\u548c\u63a8\u7406\u5f00\u9500\u3002", "motivation": "\u573a\u666f\u6587\u672c\u8bc6\u522b\uff08STR\uff09\u4ecd\u7136\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u539f\u56e0\u5728\u4e8e\u590d\u6742\u7684\u89c6\u89c9\u5916\u89c2\u548c\u6709\u9650\u7684\u8bed\u4e49\u5148\u9a8c\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTEACH\u7684\u65b0\u578b\u8bad\u7ec3\u8303\u5f0f\u3002\u8be5\u65b9\u6cd5\u5c06\u771f\u503c\u6587\u672c\u4f5c\u4e3a\u8f85\u52a9\u8f93\u5165\u6ce8\u5165\u6a21\u578b\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9010\u6b65\u51cf\u5c11\u5176\u5f71\u54cd\u3002\u901a\u8fc7\u5c06\u76ee\u6807\u6807\u7b7e\u7f16\u7801\u5230\u5d4c\u5165\u7a7a\u95f4\u5e76\u5e94\u7528\u635f\u5931\u611f\u77e5\u63a9\u853d\uff0cTEACH\u6a21\u62df\u4e86\u4e00\u4e2a\u8bfe\u7a0b\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5f15\u5bfc\u6a21\u578b\u4ece\u4f9d\u8d56\u6807\u7b7e\u7684\u5b66\u4e60\u8fc7\u6e21\u5230\u5b8c\u5168\u89c6\u89c9\u8bc6\u522b\u3002TEACH\u6a21\u578b\u65e0\u5173\uff0c\u65e0\u9700\u5916\u90e8\u9884\u8bad\u7ec3\uff0c\u4e0d\u5f15\u5165\u63a8\u7406\u5f00\u9500\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6846\u67b6\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528TEACH\u8bad\u7ec3\u7684\u6a21\u578b\u6301\u7eed\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u3002", "conclusion": "TEACH\u7684\u9c81\u68d2\u6027\u548c\u901a\u7528\u6027\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u80fd\u6709\u6548\u63d0\u9ad8\u573a\u666f\u6587\u672c\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2508.01324", "pdf": "https://arxiv.org/pdf/2508.01324", "abs": "https://arxiv.org/abs/2508.01324", "authors": ["Ke Miao", "Yuke Hu", "Xiaochen Li", "Wenjie Bao", "Zhihao Liu", "Zhan Qin", "Kui Ren"], "title": "Towards Evaluation for Real-World LLM Unlearning", "categories": ["cs.AI"], "comment": null, "summary": "This paper analyzes the limitations of existing unlearning evaluation metrics\nin terms of practicality, exactness, and robustness in real-world LLM\nunlearning scenarios. To overcome these limitations, we propose a new metric\ncalled Distribution Correction-based Unlearning Evaluation (DCUE). It\nidentifies core tokens and corrects distributional biases in their confidence\nscores using a validation set. The evaluation results are quantified using the\nKolmogorov-Smirnov test. Experimental results demonstrate that DCUE overcomes\nthe limitations of existing metrics, which also guides the design of more\npractical and reliable unlearning algorithms in the future.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDCUE\u7684\u65b0\u578b\u6307\u6807\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9057\u5fd8\u8bc4\u4f30\u6307\u6807\u5728\u5b9e\u7528\u6027\u3001\u7cbe\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9057\u5fd8\u8bc4\u4f30\u6307\u6807\u5728\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u5b58\u5728\u5b9e\u7528\u6027\u3001\u7cbe\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u6821\u6b63\u7684\u9057\u5fd8\u8bc4\u4f30\uff08DCUE\uff09\u65b0\u6307\u6807\u3002\u8be5\u65b9\u6cd5\u8bc6\u522b\u6838\u5fc3\u8bcd\u5143\uff0c\u5e76\u5229\u7528\u9a8c\u8bc1\u96c6\u6821\u6b63\u5176\u7f6e\u4fe1\u5ea6\u5206\u6570\u4e2d\u7684\u5206\u5e03\u504f\u5dee\uff0c\u7136\u540e\u4f7f\u7528Kolmogorov-Smirnov\u68c0\u9a8c\u91cf\u5316\u8bc4\u4f30\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDCUE\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u7684\u5c40\u9650\u6027\u3002", "conclusion": "DCUE\u4e0d\u4ec5\u89e3\u51b3\u4e86\u5f53\u524d\u8bc4\u4f30\u95ee\u9898\uff0c\u8fd8\u80fd\u6307\u5bfc\u672a\u6765\u8bbe\u8ba1\u66f4\u5b9e\u7528\u548c\u53ef\u9760\u7684\u9057\u5fd8\u7b97\u6cd5\u3002"}}
{"id": "2508.01656", "pdf": "https://arxiv.org/pdf/2508.01656", "abs": "https://arxiv.org/abs/2508.01656", "authors": ["Lucio La Cava", "Dominik Macko", "R\u00f3bert M\u00f3ro", "Ivan Srba", "Andrea Tagarelli"], "title": "Authorship Attribution in Multilingual Machine-Generated Texts", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "physics.soc-ph"], "comment": null, "summary": "As Large Language Models (LLMs) have reached human-like fluency and\ncoherence, distinguishing machine-generated text (MGT) from human-written\ncontent becomes increasingly difficult. While early efforts in MGT detection\nhave focused on binary classification, the growing landscape and diversity of\nLLMs require a more fine-grained yet challenging authorship attribution (AA),\ni.e., being able to identify the precise generator (LLM or human) behind a\ntext. However, AA remains nowadays confined to a monolingual setting, with\nEnglish being the most investigated one, overlooking the multilingual nature\nand usage of modern LLMs. In this work, we introduce the problem of\nMultilingual Authorship Attribution, which involves attributing texts to human\nor multiple LLM generators across diverse languages. Focusing on 18 languages\n-- covering multiple families and writing scripts -- and 8 generators (7 LLMs\nand the human-authored class), we investigate the multilingual suitability of\nmonolingual AA methods, their cross-lingual transferability, and the impact of\ngenerators on attribution performance. Our results reveal that while certain\nmonolingual AA methods can be adapted to multilingual settings, significant\nlimitations and challenges remain, particularly in transferring across diverse\nlanguage families, underscoring the complexity of multilingual AA and the need\nfor more robust approaches to better match real-world scenarios.", "AI": {"tldr": "\u968f\u7740LLMs\u7684\u666e\u53ca\uff0c\u533a\u5206\u673a\u5668\u751f\u6210\u6587\u672c\u548c\u4eba\u7c7b\u6587\u672c\u4ee5\u53ca\u8bc6\u522b\u5177\u4f53\u751f\u6210\u6a21\u578b\u53d8\u5f97\u56f0\u96be\u3002\u73b0\u6709\u5f52\u56e0\u65b9\u6cd5\u591a\u4e3a\u5355\u8bed\u79cd\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u591a\u8bed\u8a00\u4f5c\u8005\u5f52\u56e0\u95ee\u9898\uff0c\u63a2\u7d22\u5355\u8bed\u79cd\u65b9\u6cd5\u7684\u8de8\u8bed\u8a00\u9002\u7528\u6027\uff0c\u53d1\u73b0\u5176\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u9700\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u6587\u672c\u7684\u6d41\u7545\u6027\u548c\u8fde\u8d2f\u6027\u5df2\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f7f\u5f97\u533a\u5206\u673a\u5668\u751f\u6210\u6587\u672c\uff08MGT\uff09\u548c\u4eba\u7c7b\u4e66\u5199\u5185\u5bb9\u65e5\u76ca\u56f0\u96be\u3002\u867d\u7136\u65e9\u671fMGT\u68c0\u6d4b\u4fa7\u91cd\u4e8e\u4e8c\u5143\u5206\u7c7b\uff0c\u4f46LLMs\u7684\u65e5\u76ca\u591a\u6837\u5316\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u4f5c\u8005\u5f52\u56e0\uff08AA\uff09\uff0c\u5373\u8bc6\u522b\u6587\u672c\u80cc\u540e\u7684\u5177\u4f53\u751f\u6210\u5668\uff08LLM\u6216\u4eba\u7c7b\uff09\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684AA\u7814\u7a76\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u8bed\u79cd\u73af\u5883\uff08\u4ee5\u82f1\u8bed\u4e3a\u4e3b\uff09\uff0c\u5ffd\u89c6\u4e86\u73b0\u4ee3LLMs\u7684\u591a\u8bed\u8a00\u7279\u6027\u548c\u4f7f\u7528\u573a\u666f\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u591a\u8bed\u8a00\u4f5c\u8005\u5f52\u56e0\uff08Multilingual Authorship Attribution\uff09\u95ee\u9898\uff0c\u65e8\u5728\u8de8\u591a\u79cd\u8bed\u8a00\u5c06\u6587\u672c\u5f52\u56e0\u4e8e\u4eba\u7c7b\u6216\u591a\u4e2aLLM\u751f\u6210\u5668\u3002\u7814\u7a76\u8303\u56f4\u6db5\u76d618\u79cd\u8bed\u8a00\uff08\u5305\u62ec\u4e0d\u540c\u8bed\u7cfb\u548c\u4e66\u5199\u7cfb\u7edf\uff09\u548c8\u4e2a\u751f\u6210\u5668\uff087\u4e2aLLM\u548c\u4eba\u7c7b\u4f5c\u8005\u7c7b\u522b\uff09\u3002\u7814\u7a76\u8c03\u67e5\u4e86\u5355\u8bed\u79cdAA\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u9002\u7528\u6027\u3001\u5176\u8de8\u8bed\u8a00\u53ef\u8fc1\u79fb\u6027\u4ee5\u53ca\u4e0d\u540c\u751f\u6210\u5668\u5bf9\u5f52\u56e0\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136\u67d0\u4e9b\u5355\u8bed\u79cdAA\u65b9\u6cd5\u53ef\u4ee5\u9002\u5e94\u591a\u8bed\u8a00\u8bbe\u7f6e\uff0c\u4f46\u4ecd\u5b58\u5728\u663e\u8457\u7684\u5c40\u9650\u6027\u548c\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u8de8\u4e0d\u540c\u8bed\u7cfb\u7684\u8fc1\u79fb\u65b9\u9762\u3002", "conclusion": "\u591a\u8bed\u8a00\u4f5c\u8005\u5f52\u56e0\u7684\u590d\u6742\u6027\u51f8\u663e\u4e86\u5f00\u53d1\u66f4\u9c81\u68d2\u65b9\u6cd5\u4ee5\u66f4\u597d\u5730\u5339\u914d\u5b9e\u9645\u573a\u666f\u7684\u9700\u6c42\u3002"}}
{"id": "2508.00960", "pdf": "https://arxiv.org/pdf/2508.00960", "abs": "https://arxiv.org/abs/2508.00960", "authors": ["Sudip K. Seal", "Maksudul Alam", "Jorge Ramirez", "Sajal Dash", "Hao Lu"], "title": "Compression-Induced Communication-Efficient Large Model Training and Inferencing", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Energy efficiency of training and inferencing with large neural network\nmodels is a critical challenge facing the future of sustainable large-scale\nmachine learning workloads. This paper introduces an alternative strategy,\ncalled phantom parallelism, to minimize the net energy consumption of\ntraditional tensor (model) parallelism, the most energy-inefficient component\nof large neural network training. The approach is presented in the context of\nfeed-forward network architectures as a preliminary, but comprehensive,\nproof-of-principle study of the proposed methodology. We derive new forward and\nbackward propagation operators for phantom parallelism, implement them as\ncustom autograd operations within an end-to-end phantom parallel training\npipeline and compare its parallel performance and energy-efficiency against\nthose of conventional tensor parallel training pipelines. Formal analyses that\npredict lower bandwidth and FLOP counts are presented with supporting empirical\nresults on up to 256 GPUs that corroborate these gains. Experiments are shown\nto deliver ~50% reduction in the energy consumed to train FFNs using the\nproposed phantom parallel approach when compared with conventional tensor\nparallel methods. Additionally, the proposed approach is shown to train smaller\nphantom models to the same model loss on smaller GPU counts as larger tensor\nparallel models on larger GPU counts offering the possibility for even greater\nenergy savings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u5e7b\u5f71\u5e76\u884c\u201d\u65b0\u7b56\u7565\uff0c\u65e8\u5728\u5927\u5e45\u964d\u4f4e\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u5f20\u91cf\u5e76\u884c\u7684\u80fd\u8017\uff0c\u5b9e\u9a8c\u8bc1\u660e\u53ef\u5b9e\u73b0\u7ea650%\u7684\u80fd\u8017\u51cf\u5c11\uff0c\u5e76\u80fd\u5728\u66f4\u5c11GPU\u4e0a\u4ee5\u66f4\u5c0f\u7684\u6a21\u578b\u8fbe\u5230\u76f8\u540c\u6027\u80fd\uff0c\u4ece\u800c\u5b9e\u73b0\u663e\u8457\u8282\u80fd\u3002", "motivation": "\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u7684\u80fd\u6548\u662f\u672a\u6765\u53ef\u6301\u7eed\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\uff0c\u7279\u522b\u662f\u4f20\u7edf\u5f20\u91cf\uff08\u6a21\u578b\uff09\u5e76\u884c\u4f5c\u4e3a\u80fd\u8017\u6700\u9ad8\u7684\u7ec4\u4ef6\uff0c\u4e9f\u9700\u4f18\u5316\u3002", "method": "\u5f15\u5165\u201c\u5e7b\u5f71\u5e76\u884c\u201d\u7b56\u7565\u4ee5\u6700\u5c0f\u5316\u4f20\u7edf\u5f20\u91cf\u5e76\u884c\u7684\u51c0\u80fd\u8017\u3002\u8be5\u65b9\u6cd5\u9488\u5bf9\u524d\u9988\u7f51\u7edc\u67b6\u6784\uff0c\u63a8\u5bfc\u4e86\u65b0\u7684\u524d\u5411\u548c\u53cd\u5411\u4f20\u64ad\u7b97\u5b50\uff0c\u5e76\u5c06\u5176\u5b9e\u73b0\u4e3a\u81ea\u5b9a\u4e49\u81ea\u52a8\u5fae\u5206\u64cd\u4f5c\uff0c\u6784\u5efa\u4e86\u7aef\u5230\u7aef\u7684\u5e7b\u5f71\u5e76\u884c\u8bad\u7ec3\u7ba1\u7ebf\u3002\u901a\u8fc7\u6b63\u5f0f\u5206\u6790\u9884\u6d4b\u8f83\u4f4e\u7684\u5e26\u5bbd\u548cFLOPs\uff0c\u5e76\u5728\u591a\u8fbe256\u4e2aGPU\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4e0e\u4f20\u7edf\u5f20\u91cf\u5e76\u884c\u8bad\u7ec3\u7ba1\u7ebf\u5bf9\u6bd4\u5176\u5e76\u884c\u6027\u80fd\u548c\u80fd\u6548\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4f20\u7edf\u5f20\u91cf\u5e76\u884c\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4f7f\u7528\u5e7b\u5f71\u5e76\u884c\u8bad\u7ec3\u524d\u9988\u7f51\u7edc\u7684\u80fd\u8017\u964d\u4f4e\u4e86\u7ea650%\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u66f4\u5c11\u7684GPU\u4e0a\u4ee5\u66f4\u5c0f\u7684\u5e7b\u5f71\u6a21\u578b\u8fbe\u5230\u4e0e\u5728\u66f4\u591aGPU\u4e0a\u8bad\u7ec3\u66f4\u5927\u5f20\u91cf\u5e76\u884c\u6a21\u578b\u76f8\u540c\u7684\u6a21\u578b\u635f\u5931\uff0c\u9884\u793a\u7740\u66f4\u5927\u7684\u8282\u80fd\u6f5c\u529b\u3002", "conclusion": "\u5e7b\u5f71\u5e76\u884c\u662f\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u884c\u7684\u7b56\u7565\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u7684\u80fd\u6548\uff0c\u5c24\u5176\u5728\u51cf\u5c11\u5f20\u91cf\u5e76\u884c\u80fd\u8017\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u4e3a\u672a\u6765\u53ef\u6301\u7eed\u7684\u5927\u89c4\u6a21\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01170", "pdf": "https://arxiv.org/pdf/2508.01170", "abs": "https://arxiv.org/abs/2508.01170", "authors": ["Tuan Duc Ngo", "Ashkan Mirzaei", "Guocheng Qian", "Hanwen Liang", "Chuang Gan", "Evangelos Kalogerakis", "Peter Wonka", "Chaoyang Wang"], "title": "DELTAv2: Accelerating Dense 3D Tracking", "categories": ["cs.CV"], "comment": null, "summary": "We propose a novel algorithm for accelerating dense long-term 3D point\ntracking in videos. Through analysis of existing state-of-the-art methods, we\nidentify two major computational bottlenecks. First, transformer-based\niterative tracking becomes expensive when handling a large number of\ntrajectories. To address this, we introduce a coarse-to-fine strategy that\nbegins tracking with a small subset of points and progressively expands the set\nof tracked trajectories. The newly added trajectories are initialized using a\nlearnable interpolation module, which is trained end-to-end alongside the\ntracking network. Second, we propose an optimization that significantly reduces\nthe cost of correlation feature computation, another key bottleneck in prior\nmethods. Together, these improvements lead to a 5-100x speedup over existing\napproaches while maintaining state-of-the-art tracking accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u663e\u8457\u52a0\u901f\u89c6\u9891\u4e2d\u5bc6\u96c6\u957f\u7a0b3D\u70b9\u8ddf\u8e2a\u3002", "motivation": "\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5bc6\u96c6\u957f\u7a0b3D\u70b9\u8ddf\u8e2a\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u8ba1\u7b97\u74f6\u9888\uff1a\u4e00\u662f\u57fa\u4e8eTransformer\u7684\u8fed\u4ee3\u8ddf\u8e2a\u5728\u5904\u7406\u5927\u91cf\u8f68\u8ff9\u65f6\u5f00\u9500\u5de8\u5927\uff1b\u4e8c\u662f\u76f8\u5173\u7279\u5f81\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u9488\u5bf9\u7b2c\u4e00\u4e2a\u74f6\u9888\uff0c\u5f15\u5165\u7c97\u5230\u7ec6\u7b56\u7565\uff0c\u901a\u8fc7\u9010\u6b65\u6269\u5c55\u8f68\u8ff9\u96c6\u5e76\u4f7f\u7528\u53ef\u5b66\u4e60\u63d2\u503c\u6a21\u5757\u521d\u59cb\u5316\u65b0\u589e\u8f68\u8ff9\u3002\u9488\u5bf9\u7b2c\u4e8c\u4e2a\u74f6\u9888\uff0c\u63d0\u51fa\u4e00\u79cd\u4f18\u5316\u65b9\u6848\u4ee5\u663e\u8457\u964d\u4f4e\u76f8\u5173\u7279\u5f81\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u672c\u7b97\u6cd5\u76f8\u8f83\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e865-100\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u89e3\u51b3\u5173\u952e\u8ba1\u7b97\u74f6\u9888\uff0c\u672c\u7b97\u6cd5\u6210\u529f\u5728\u5927\u5e45\u63d0\u5347\u5bc6\u96c6\u957f\u7a0b3D\u70b9\u8ddf\u8e2a\u901f\u5ea6\u7684\u540c\u65f6\uff0c\u7ef4\u6301\u4e86\u5353\u8d8a\u7684\u8ddf\u8e2a\u6027\u80fd\u3002"}}
{"id": "2508.01330", "pdf": "https://arxiv.org/pdf/2508.01330", "abs": "https://arxiv.org/abs/2508.01330", "authors": ["Zihan Zheng", "Tianle Cui", "Chuwen Xie", "Jiahui Zhang", "Jiahui Pan", "Lewei He", "Qianglong Chen"], "title": "NatureGAIA: Pushing the Frontiers of GUI Agents with a Challenging Benchmark and High-Quality Trajectory Dataset", "categories": ["cs.AI"], "comment": null, "summary": "The rapid advancement of Large Language Model (LLM)-driven Graphical User\nInterface (GUI) agents is significantly hampered by the profound limitations of\nexisting evaluation benchmarks in terms of accuracy, reproducibility, and\nscalability. To address this critical gap, we introduce \\Benchmark, a novel\nbenchmark engineered on the principle of Causal Pathways. This design paradigm\nstructures complex tasks into a series of programmatically verifiable atomic\nsteps, ensuring a rigorous, fully automated, and reproducible standard for\nassessment. Concurrently, to mitigate the inherent capability deficits of\nagents, we developed \\Agent, a hierarchical agent architecture specifically\noptimized for long-horizon tasks. We leveraged this agent to generate a\nhigh-quality, human-verified trajectory dataset that uniquely captures diverse\nand even self-correcting interaction patterns of LLMs. We then utilized this\ndataset to perform Reinforcement Fine-Tuning (RFT) on the Qwen2.5-VL-7B model.\nOur experiments reveal that \\Benchmark~presents a formidable challenge to\ncurrent state-of-the-art LLMs; even the top-performing Claude-sonnet-4 achieved\na Weighted Pathway Success Rate (WPSR) of only 34.6\\%. Moreover, while RFT\nsubstantially improved the smaller model's GUI execution capabilities (WPSR\nincreased from 3.3\\% to 10.8\\%), its performance degraded sharply when handling\ncomplex scenarios. This outcome highlights the inherent capability ceiling of\nsmaller models when faced with comprehensive tasks that integrate perception,\ndecision-making, and execution. This research contributes a rigorous evaluation\nstandard and a high-quality dataset to the community, aiming to guide the\nfuture development of GUI agents.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.01674", "pdf": "https://arxiv.org/pdf/2508.01674", "abs": "https://arxiv.org/abs/2508.01674", "authors": ["Tae Soo Kim", "Yoonjoo Lee", "Yoonah Park", "Jiho Kim", "Young-Ho Kim", "Juho Kim"], "title": "CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to COLM 2025. Project Website: https://cupid.kixlab.org/", "summary": "Personalization of Large Language Models (LLMs) often assumes users hold\nstatic preferences that reflect globally in all tasks. In reality, humans hold\ndynamic preferences that change depending on the context. As users interact\nwith an LLM in various contexts, they naturally reveal their contextual\npreferences, which a model must infer and apply in future contexts to ensure\nalignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated\ninteraction session histories between users and LLM-based chat assistants. In\neach interaction session, the user provides a request in a specific context and\nexpresses their preference through multi-turn feedback. Given a new user\nrequest and prior interaction sessions, our benchmark assesses whether LLMs can\ninfer the preference relevant to this request and generate a response that\nsatisfies this preference. With CUPID, we evaluated 10 open and proprietary\nLLMs, revealing that state-of-the-art LLMs struggle to infer preferences from\nmulti-turn interactions and fail to discern what previous context is relevant\nto a new request -- under 50% precision and 65% recall. Our work highlights the\nneed to advance LLM capabilities for more contextually personalized\ninteractions and proposes CUPID as a resource to drive these improvements.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165CUPID\u57fa\u51c6\u6765\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u7528\u6237\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u504f\u597d\u65b9\u9762\u7684\u80fd\u529b\u3002\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524dLLMs\u5728\u4ece\u591a\u8f6e\u4ea4\u4e92\u4e2d\u63a8\u65ad\u504f\u597d\u5e76\u8bc6\u522b\u76f8\u5173\u4e0a\u4e0b\u6587\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff08\u7cbe\u5ea6\u4f4e\u4e8e50%\uff0c\u53ec\u56de\u7387\u4f4e\u4e8e65%\uff09\uff0c\u5f3a\u8c03\u4e86LLM\u5728\u4e0a\u4e0b\u6587\u4e2a\u6027\u5316\u65b9\u9762\u6539\u8fdb\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u5f53\u524dLLM\u7684\u4e2a\u6027\u5316\u901a\u5e38\u5047\u8bbe\u7528\u6237\u504f\u597d\u662f\u9759\u6001\u4e14\u5168\u5c40\u4e0d\u53d8\u7684\uff0c\u4f46\u8fd9\u4e0e\u4eba\u7c7b\u52a8\u6001\u3001\u60c5\u5883\u5316\u7684\u504f\u597d\u4e0d\u7b26\u3002LLM\u9700\u8981\u80fd\u591f\u4ece\u591a\u8f6e\u4ea4\u4e92\u4e2d\u63a8\u65ad\u7528\u6237\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u7684\u5177\u4f53\u504f\u597d\uff0c\u5e76\u5e94\u7528\u4e8e\u672a\u6765\u7684\u4ea4\u4e92\u4e2d\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u5bf9\u9f50\u3002", "method": "\u5f15\u5165CUPID\u57fa\u51c6\uff0c\u5305\u542b756\u4e2a\u7531\u4eba\u7c7b\u7cbe\u5fc3\u7b56\u5212\u7684\u7528\u6237\u4e0eLLM\u804a\u5929\u52a9\u624b\u7684\u4ea4\u4e92\u4f1a\u8bdd\u5386\u53f2\u3002\u6bcf\u4e2a\u4f1a\u8bdd\u4e2d\uff0c\u7528\u6237\u5728\u7279\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u63d0\u51fa\u8bf7\u6c42\u5e76\u901a\u8fc7\u591a\u8f6e\u53cd\u9988\u8868\u8fbe\u504f\u597d\u3002\u8be5\u57fa\u51c6\u7528\u4e8e\u8bc4\u4f30LLMs\u80fd\u5426\u6839\u636e\u65b0\u7684\u7528\u6237\u8bf7\u6c42\u548c\u5386\u53f2\u4f1a\u8bdd\uff0c\u63a8\u65ad\u51fa\u76f8\u5173\u7684\u7528\u6237\u504f\u597d\u5e76\u751f\u6210\u6ee1\u8db3\u8be5\u504f\u597d\u7684\u54cd\u5e94\u3002\u7814\u7a76\u4f7f\u7528CUPID\u8bc4\u4f30\u4e8610\u4e2a\u4e3b\u6d41\u7684\u5f00\u6e90\u548c\u4e13\u6709LLM\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684LLMs\u5728\u4ece\u591a\u8f6e\u4ea4\u4e92\u4e2d\u63a8\u65ad\u7528\u6237\u504f\u597d\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5e76\u4e14\u96be\u4ee5\u8fa8\u522b\u54ea\u4e9b\u5148\u524d\u7684\u4e0a\u4e0b\u6587\u4e0e\u65b0\u7684\u8bf7\u6c42\u76f8\u5173\u3002\u5177\u4f53\u8868\u73b0\u4e3a\uff1a\u7cbe\u5ea6\u4f4e\u4e8e50%\uff0c\u53ec\u56de\u7387\u4f4e\u4e8e65%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86LLM\u5728\u5b9e\u73b0\u66f4\u5177\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4e2a\u6027\u5316\u4ea4\u4e92\u65b9\u9762\u80fd\u529b\u7684\u4e0d\u8db3\u3002CUPID\u4f5c\u4e3a\u4e00\u4e2a\u65b0\u8d44\u6e90\uff0c\u65e8\u5728\u63a8\u52a8LLM\u5728\u8fd9\u4e00\u9886\u57df\u7684\u80fd\u529b\u63d0\u5347\u3002"}}
{"id": "2508.00961", "pdf": "https://arxiv.org/pdf/2508.00961", "abs": "https://arxiv.org/abs/2508.00961", "authors": ["Xiang Li", "Penglei Sun", "Wanyun Zhou", "Zikai Wei", "Yongqi Zhang", "Xiaowen Chu"], "title": "FinKario: Event-Enhanced Automated Construction of Financial Knowledge Graph", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Individual investors are significantly outnumbered and disadvantaged in\nfinancial markets, overwhelmed by abundant information and lacking professional\nanalysis. Equity research reports stand out as crucial resources, offering\nvaluable insights. By leveraging these reports, large language models (LLMs)\ncan enhance investors' decision-making capabilities and strengthen financial\nanalysis. However, two key challenges limit their effectiveness: (1) the rapid\nevolution of market events often outpaces the slow update cycles of existing\nknowledge bases, (2) the long-form and unstructured nature of financial reports\nfurther hinders timely and context-aware integration by LLMs. To address these\nchallenges, we tackle both data and methodological aspects. First, we introduce\nthe Event-Enhanced Automated Construction of Financial Knowledge Graph\n(FinKario), a dataset comprising over 305,360 entities, 9,625 relational\ntriples, and 19 distinct relation types. FinKario automatically integrates\nreal-time company fundamentals and market events through prompt-driven\nextraction guided by professional institutional templates, providing structured\nand accessible financial insights for LLMs. Additionally, we propose a\nTwo-Stage, Graph-Based retrieval strategy (FinKario-RAG), optimizing the\nretrieval of evolving, large-scale financial knowledge to ensure efficient and\nprecise data access. Extensive experiments show that FinKario with FinKario-RAG\nachieves superior stock trend prediction accuracy, outperforming financial LLMs\nby 18.81% and institutional strategies by 17.85% on average in backtesting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFinKario\u91d1\u878d\u77e5\u8bc6\u56fe\u8c31\u548cFinKario-RAG\u68c0\u7d22\u7b56\u7565\uff0c\u65e8\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u91d1\u878d\u5206\u6790\u4e2d\u7684\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\uff0c\u5e76\u5728\u80a1\u7968\u8d8b\u52bf\u9884\u6d4b\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u4e2a\u4eba\u6295\u8d44\u8005\u5728\u91d1\u878d\u5e02\u573a\u4e2d\u9762\u4e34\u4fe1\u606f\u8fc7\u8f7d\u548c\u4e13\u4e1a\u5206\u6790\u4e0d\u8db3\u7684\u56f0\u5883\u3002\u73b0\u6709LLMs\u867d\u80fd\u5229\u7528\u7814\u7a76\u62a5\u544a\u8f85\u52a9\u51b3\u7b56\uff0c\u4f46\u53d7\u9650\u4e8e\u4e24\u65b9\u9762\u6311\u6218\uff1a\u4e00\u662f\u5e02\u573a\u4e8b\u4ef6\u5feb\u901f\u6f14\u53d8\u4e0e\u77e5\u8bc6\u5e93\u66f4\u65b0\u7f13\u6162\u7684\u77db\u76fe\uff1b\u4e8c\u662f\u91d1\u878d\u62a5\u544a\u957f\u7bc7\u5e45\u3001\u975e\u7ed3\u6784\u5316\u7279\u6027\u963b\u788dLLMs\u53ca\u65f6\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6574\u5408\u3002", "method": "\u4e3a\u89e3\u51b3\u4e0a\u8ff0\u6311\u6218\uff0c\u7814\u7a76\u8005\u4ece\u6570\u636e\u548c\u65b9\u6cd5\u4e24\u65b9\u9762\u7740\u624b\uff1a1. \u5f15\u5165\u201c\u4e8b\u4ef6\u589e\u5f3a\u578b\u91d1\u878d\u77e5\u8bc6\u56fe\u8c31\uff08FinKario\uff09\u201d\u6570\u636e\u96c6\uff0c\u5305\u542b\u903e30\u4e07\u5b9e\u4f53\u548c\u8fd1\u4e07\u4e2a\u5173\u7cfb\u4e09\u5143\u7ec4\uff0c\u901a\u8fc7\u63d0\u793a\u9a71\u52a8\u62bd\u53d6\u548c\u4e13\u4e1a\u673a\u6784\u6a21\u677f\uff0c\u81ea\u52a8\u6574\u5408\u5b9e\u65f6\u516c\u53f8\u57fa\u672c\u9762\u548c\u5e02\u573a\u4e8b\u4ef6\uff0c\u4e3aLLMs\u63d0\u4f9b\u7ed3\u6784\u5316\u6d1e\u5bdf\u30022. \u63d0\u51fa\u201c\u4e24\u9636\u6bb5\u3001\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u7b56\u7565\uff08FinKario-RAG\uff09\u201d\uff0c\u4f18\u5316\u5927\u89c4\u6a21\u3001\u52a8\u6001\u6f14\u53d8\u7684\u91d1\u878d\u77e5\u8bc6\u68c0\u7d22\uff0c\u786e\u4fdd\u6570\u636e\u8bbf\u95ee\u7684\u6548\u7387\u548c\u7cbe\u786e\u6027\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u56de\u6d4b\u7ed3\u679c\u8868\u660e\uff0cFinKario\u7ed3\u5408FinKario-RAG\u5728\u80a1\u7968\u8d8b\u52bf\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u6bd4\u73b0\u6709\u91d1\u878dLLMs\u9ad8\u51fa18.81%\uff0c\u6bd4\u673a\u6784\u7b56\u7565\u9ad8\u51fa17.85%\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u5b9e\u65f6\u66f4\u65b0\u7684\u91d1\u878d\u77e5\u8bc6\u56fe\u8c31\u548c\u4f18\u5316\u68c0\u7d22\u7b56\u7565\uff0c\u53ef\u4ee5\u6709\u6548\u5e94\u5bf9\u91d1\u878d\u5e02\u573a\u4fe1\u606f\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347LLMs\u7684\u91d1\u878d\u5206\u6790\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u5e76\u5728\u80a1\u7968\u8d8b\u52bf\u9884\u6d4b\u4e0a\u5b9e\u73b0\u7a81\u7834\u6027\u8fdb\u5c55\u3002"}}
{"id": "2508.01171", "pdf": "https://arxiv.org/pdf/2508.01171", "abs": "https://arxiv.org/abs/2508.01171", "authors": ["Ranran Huang", "Krystian Mikolajczyk"], "title": "No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views", "categories": ["cs.CV"], "comment": "Project Page: https://ranrhuang.github.io/spfsplat/", "summary": "We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from\nsparse multi-view images, requiring no ground-truth poses during training or\ninference. It employs a shared feature extraction backbone, enabling\nsimultaneous prediction of 3D Gaussian primitives and camera poses in a\ncanonical space from unposed inputs within a single feed-forward step.\nAlongside the rendering loss based on estimated novel-view poses, a\nreprojection loss is integrated to enforce the learning of pixel-aligned\nGaussian primitives for enhanced geometric constraints. This pose-free training\nparadigm and efficient one-step feed-forward design make SPFSplat well-suited\nfor practical applications. Remarkably, despite the absence of pose\nsupervision, SPFSplat achieves state-of-the-art performance in novel view\nsynthesis even under significant viewpoint changes and limited image overlap.\nIt also surpasses recent methods trained with geometry priors in relative pose\nestimation. Code and trained models are available on our project page:\nhttps://ranrhuang.github.io/spfsplat/.", "AI": {"tldr": "SPFSplat\u662f\u4e00\u4e2a\u9ad8\u6548\u76843D\u9ad8\u65af\u6563\u5c04\u6846\u67b6\uff0c\u65e0\u9700\u771f\u503c\u59ff\u6001\u5373\u53ef\u4ece\u7a00\u758f\u591a\u89c6\u56fe\u56fe\u50cf\u8fdb\u884c\u5904\u7406\uff0c\u5728\u65b0\u9896\u89c6\u56fe\u5408\u6210\u548c\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u67093D\u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u771f\u503c\u59ff\u6001\uff0c\u8fd9\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u96be\u4ee5\u83b7\u53d6\u3002SPFSplat\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u65e0\u9700\u59ff\u6001\u76d1\u7763\u7684\u3001\u9ad8\u6548\u76843D\u9ad8\u65af\u6563\u5c04\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u66f4\u597d\u5730\u9002\u5e94\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "method": "SPFSplat\u91c7\u7528\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u4e3b\u5e72\u7f51\u7edc\uff0c\u80fd\u591f\u4ece\u65e0\u59ff\u6001\u8f93\u5165\u4e2d\u5728\u89c4\u8303\u7a7a\u95f4\u540c\u65f6\u9884\u6d4b3D\u9ad8\u65af\u57fa\u5143\u548c\u76f8\u673a\u59ff\u6001\uff0c\u4e14\u4ec5\u9700\u4e00\u6b21\u524d\u5411\u4f20\u64ad\u3002\u5b83\u7ed3\u5408\u4e86\u57fa\u4e8e\u4f30\u8ba1\u65b0\u9896\u89c6\u56fe\u59ff\u6001\u7684\u6e32\u67d3\u635f\u5931\u548c\u5f3a\u5236\u50cf\u7d20\u5bf9\u9f50\u9ad8\u65af\u57fa\u5143\u7684\u91cd\u6295\u5f71\u635f\u5931\uff0c\u4ee5\u589e\u5f3a\u51e0\u4f55\u7ea6\u675f\u3002", "result": "\u5c3d\u7ba1\u6ca1\u6709\u59ff\u6001\u76d1\u7763\uff0cSPFSplat\u5728\u65b0\u9896\u89c6\u56fe\u5408\u6210\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u663e\u8457\u89c6\u89d2\u53d8\u5316\u548c\u56fe\u50cf\u91cd\u53e0\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u8868\u73b0\u51fa\u8272\u3002\u5b83\u8fd8\u5728\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u8d85\u8d8a\u4e86\u8fd1\u671f\u4f7f\u7528\u51e0\u4f55\u5148\u9a8c\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "conclusion": "SPFSplat\u901a\u8fc7\u5176\u72ec\u7279\u7684\u65e0\u59ff\u6001\u8bad\u7ec3\u8303\u5f0f\u548c\u9ad8\u6548\u7684\u4e00\u6b65\u524d\u5411\u4f20\u64ad\u8bbe\u8ba1\uff0c\u4e3a\u4ece\u7a00\u758f\u3001\u65e0\u59ff\u6001\u591a\u89c6\u56fe\u56fe\u50cf\u8fdb\u884c3D\u9ad8\u65af\u6563\u5c04\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u5b9e\u7528\u7684\u6846\u67b6\uff0c\u5e76\u5728\u5173\u952e\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.01368", "pdf": "https://arxiv.org/pdf/2508.01368", "abs": "https://arxiv.org/abs/2508.01368", "authors": ["Zhehong Ren", "Tianluo Zhang", "Yiheng Lu", "Yushen Liang", "Promethee Spathis"], "title": "Relation-Aware LNN-Transformer for Intersection-Centric Next-Step Prediction", "categories": ["cs.AI"], "comment": "8 pages, 5 figures", "summary": "Next-step location prediction plays a pivotal role in modeling human\nmobility, underpinning applications from personalized navigation to strategic\nurban planning. However, approaches that assume a closed world - restricting\nchoices to a predefined set of points of interest (POIs) - often fail to\ncapture exploratory or target-agnostic behavior and the topological constraints\nof urban road networks. Hence, we introduce a road-node-centric framework that\nrepresents road-user trajectories on the city's road-intersection graph,\nthereby relaxing the closed-world constraint and supporting next-step\nforecasting beyond fixed POI sets. To encode environmental context, we\nintroduce a sector-wise directional POI aggregation that produces compact\nfeatures capturing distance, bearing, density and presence cues. By combining\nthese cues with structural graph embeddings, we obtain semantically grounded\nnode representations. For sequence modeling, we integrate a Relation-Aware\nLNN-Transformer - a hybrid of a Continuous-time Forgetting Cell CfC-LNN and a\nbearing-biased self-attention module - to capture both fine-grained temporal\ndynamics and long-range spatial dependencies. Evaluated on city-scale road-user\ntrajectories, our model outperforms six state-of-the-art baselines by up to 17\npercentage points in accuracy at one hop and 10 percentage points in MRR, and\nmaintains high resilience under noise, losing only 2.4 percentage points in\naccuracy at one under 50 meter GPS perturbation and 8.9 percentage points in\naccuracy at one hop under 25 percent POI noise.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9053\u8def\u8282\u70b9\u4e2d\u5fc3\u7684\u4e0b\u4e00\u8df3\u4f4d\u7f6e\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u6027\u7684\u73af\u5883\u4e0a\u4e0b\u6587\u7f16\u7801\u548c\u5e8f\u5217\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfPOI\u9884\u6d4b\u65b9\u6cd5\u201c\u5c01\u95ed\u4e16\u754c\u201d\u7684\u5c40\u9650\u6027\uff0c\u5728\u57ce\u5e02\u5c3a\u5ea6\u8f68\u8ff9\u6570\u636e\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u4e0b\u4e00\u8df3\u4f4d\u7f6e\u9884\u6d4b\u65b9\u6cd5\u5e38\u5047\u8bbe\u201c\u5c01\u95ed\u4e16\u754c\u201d\uff0c\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u7684\u5174\u8da3\u70b9\uff08POI\uff09\uff0c\u65e0\u6cd5\u6355\u6349\u63a2\u7d22\u6027\u884c\u4e3a\u6216\u76ee\u6807\u4e0d\u53ef\u77e5\u7684\u79fb\u52a8\uff0c\u4e5f\u672a\u5145\u5206\u8003\u8651\u57ce\u5e02\u8def\u7f51\u7684\u62d3\u6251\u7ea6\u675f\u3002", "method": "\u5f15\u5165\u4ee5\u9053\u8def\u8282\u70b9\u4e3a\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u5728\u57ce\u5e02\u8def\u53e3\u56fe\u4e0a\u8868\u793a\u7528\u6237\u8f68\u8ff9\uff0c\u4ee5\u653e\u5bbd\u201c\u5c01\u95ed\u4e16\u754c\u201d\u7ea6\u675f\u3002\u901a\u8fc7\u6247\u533a\u65b9\u5411\u6027POI\u805a\u5408\u7f16\u7801\u73af\u5883\u4e0a\u4e0b\u6587\uff0c\u751f\u6210\u5305\u542b\u8ddd\u79bb\u3001\u65b9\u4f4d\u3001\u5bc6\u5ea6\u548c\u5b58\u5728\u4fe1\u606f\u7684\u7d27\u51d1\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u7ed3\u6784\u56fe\u5d4c\u5165\u5f62\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u8282\u70b9\u8868\u793a\u3002\u4f7f\u7528Relation-Aware LNN-Transformer\uff08\u7ed3\u5408\u8fde\u7eed\u65f6\u95f4\u9057\u5fd8\u5355\u5143CfC-LNN\u548c\u65b9\u4f4d\u504f\u7f6e\u81ea\u6ce8\u610f\u529b\u6a21\u5757\uff09\u8fdb\u884c\u5e8f\u5217\u5efa\u6a21\uff0c\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u65f6\u95f4\u52a8\u6001\u548c\u957f\u8ddd\u79bb\u7a7a\u95f4\u4f9d\u8d56\u3002", "result": "\u5728\u57ce\u5e02\u5c3a\u5ea6\u9053\u8def\u7528\u6237\u8f68\u8ff9\u6570\u636e\u4e0a\uff0c\u6a21\u578b\u6027\u80fd\u8d85\u8d8a\u516d\u79cdSOTA\u57fa\u7ebf\uff0c\u7cbe\u5ea6\uff08accuracy@1hop\uff09\u63d0\u5347\u9ad8\u8fbe17\u4e2a\u767e\u5206\u70b9\uff0cMRR\u63d0\u534710\u4e2a\u767e\u5206\u70b9\u3002\u6a21\u578b\u5bf9\u566a\u58f0\u5177\u6709\u9ad8\u9c81\u68d2\u6027\uff0c\u572850\u7c73GPS\u6270\u52a8\u4e0b\u7cbe\u5ea6@1hop\u4ec5\u4e0b\u964d2.4\u4e2a\u767e\u5206\u70b9\uff0c\u572825%POI\u566a\u58f0\u4e0b\u4ec5\u4e0b\u964d8.9\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u51fa\u4ee5\u9053\u8def\u8282\u70b9\u4e3a\u4e2d\u5fc3\u7684\u9884\u6d4b\u6846\u67b6\u548c\u521b\u65b0\u7684\u73af\u5883\u4e0a\u4e0b\u6587\u7f16\u7801\u4e0e\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\uff0c\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edfPOI\u9884\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u4e00\u8df3\u4f4d\u7f6e\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u4eba\u7c7b\u51fa\u884c\u5efa\u6a21\u548c\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2508.01682", "pdf": "https://arxiv.org/pdf/2508.01682", "abs": "https://arxiv.org/abs/2508.01682", "authors": ["Lingyin Zhang", "Jun Gao", "Xiaoxue Ren", "Ziqiang Cao"], "title": "The Bidirectional Process Reward Model", "categories": ["cs.CL"], "comment": null, "summary": "Process Reward Models (PRMs) have emerged as a promising approach to enhance\nthe reasoning quality of Large Language Models (LLMs) by assigning fine-grained\nscores to intermediate reasoning steps within a solution trajectory. However,\nexisting PRMs predominantly adopt a unidirectional left-to-right (L2R)\nevaluation paradigm, which limits their ability to leverage global context,\nmaking it challenging to verify the consistency of earlier steps based on later\nones. In light of these challenges, we propose a novel bidirectional evaluation\nparadigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly\nincorporates a parallel right-to-left (R2L) evaluation stream alongside the\nconventional L2R flow, enabling later reasoning steps to help assess earlier\nones in real time. Notably, the built-in R2L evaluation is implemented solely\nthrough prompt modifications that reverse the original reasoning trajectory,\nwithout any additional parameters or inference latency introduced. This ensures\nBiPRM remains both efficient and broadly compatible with existing PRM studies.\nWe conduct extensive experiments on two mathematical reasoning benchmarks using\nsamples generated by three different policy models. Our method, BiPRM, is\nevaluated across three backbones and three distinct PRM objectives. Across all\nsettings, BiPRM consistently outperforms unidirectional baselines, achieving up\nto a 31.9% improvement in stepwise reward evaluation. Generally, our results\nhighlight BiPRM's effectiveness, robustness, and general applicability,\noffering a promising new direction for process-based reward modeling.", "AI": {"tldr": "\u53cc\u5411\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08BiPRM\uff09\u901a\u8fc7\u6574\u5408\u5de6\u53f3\u53ca\u53f3\u5de6\u8bc4\u4f30\u6d41\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5355\u5411PRM\u5728\u5229\u7528\u5168\u5c40\u4e0a\u4e0b\u6587\u65b9\u9762\u7684\u5c40\u9650\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u8d28\u91cf\uff0c\u4e14\u65e0\u9700\u989d\u5916\u53c2\u6570\u6216\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\u4e3b\u8981\u91c7\u7528\u5355\u5411\uff08\u4ece\u5de6\u5230\u53f3\uff09\u8bc4\u4f30\u8303\u5f0f\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5229\u7528\u5168\u5c40\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u96be\u4ee5\u57fa\u4e8e\u540e\u7eed\u6b65\u9aa4\u9a8c\u8bc1\u65e9\u671f\u6b65\u9aa4\u7684\u4e00\u81f4\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u53cc\u5411\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08BiPRM\uff09\u3002\u8be5\u6a21\u578b\u5728\u4f20\u7edf\u4ece\u5de6\u5230\u53f3\uff08L2R\uff09\u8bc4\u4f30\u6d41\u7a0b\u7684\u57fa\u7840\u4e0a\uff0c\u65e0\u7f1d\u52a0\u5165\u4e86\u5e76\u884c\u7684\u4ece\u53f3\u5230\u5de6\uff08R2L\uff09\u8bc4\u4f30\u6d41\uff0c\u4f7f\u540e\u7eed\u63a8\u7406\u6b65\u9aa4\u80fd\u591f\u8f85\u52a9\u8bc4\u4f30\u65e9\u671f\u6b65\u9aa4\u3002R2L\u8bc4\u4f30\u7684\u5b9e\u73b0\u4ec5\u901a\u8fc7\u63d0\u793a\u8bcd\u4fee\u6539\u6765\u53cd\u8f6c\u63a8\u7406\u8f68\u8ff9\uff0c\u65e0\u9700\u5f15\u5165\u989d\u5916\u53c2\u6570\u6216\u63a8\u7406\u5ef6\u8fdf\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\uff0c\u4f7f\u7528\u4e09\u79cd\u4e0d\u540c\u7b56\u7565\u6a21\u578b\u751f\u6210\u7684\u6837\u672c\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\u3002\u7ed3\u679c\u663e\u793a\uff0cBiPRM\u5728\u6240\u6709\u6d4b\u8bd5\u8bbe\u7f6e\u4e0b\u5747\u6301\u7eed\u4f18\u4e8e\u5355\u5411\u57fa\u7ebf\uff0c\u5728\u5206\u6b65\u5956\u52b1\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe31.9%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "BiPRM\u5c55\u73b0\u51fa\u5176\u6709\u6548\u6027\u3001\u9c81\u68d2\u6027\u548c\u901a\u7528\u9002\u7528\u6027\uff0c\u4e3a\u57fa\u4e8e\u8fc7\u7a0b\u7684\u5956\u52b1\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b0\u65b9\u5411\uff0c\u7279\u522b\u662f\u5728\u589e\u5f3aLLM\u63a8\u7406\u8d28\u91cf\u65b9\u9762\u3002"}}
{"id": "2508.00963", "pdf": "https://arxiv.org/pdf/2508.00963", "abs": "https://arxiv.org/abs/2508.00963", "authors": ["Timothy Oladunni", "Alex Wong"], "title": "Rethinking Multimodality: Optimizing Multimodal Deep Learning for Biomedical Signal Classification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This study proposes a novel perspective on multimodal deep learning for\nbiomedical signal classification, systematically analyzing how complementary\nfeature domains impact model performance. While fusing multiple domains often\npresumes enhanced accuracy, this work demonstrates that adding modalities can\nyield diminishing returns, as not all fusions are inherently advantageous. To\nvalidate this, five deep learning models were designed, developed, and\nrigorously evaluated: three unimodal (1D-CNN for time, 2D-CNN for\ntime-frequency, and 1D-CNN-Transformer for frequency) and two multimodal\n(Hybrid 1, which fuses 1D-CNN and 2D-CNN; Hybrid 2, which combines 1D-CNN,\n2D-CNN, and a Transformer). For ECG classification, bootstrapping and Bayesian\ninference revealed that Hybrid 1 consistently outperformed the 2D-CNN baseline\nacross all metrics (p-values < 0.05, Bayesian probabilities > 0.90), confirming\nthe synergistic complementarity of the time and time-frequency domains.\nConversely, Hybrid 2's inclusion of the frequency domain offered no further\nimprovement and sometimes a marginal decline, indicating representational\nredundancy; a phenomenon further substantiated by a targeted ablation study.\nThis research redefines a fundamental principle of multimodal design in\nbiomedical signal analysis. We demonstrate that optimal domain fusion isn't\nabout the number of modalities, but the quality of their inherent\ncomplementarity. This paradigm-shifting concept moves beyond purely heuristic\nfeature selection. Our novel theoretical contribution, \"Complementary Feature\nDomains in Multimodal ECG Deep Learning,\" presents a mathematically\nquantifiable framework for identifying ideal domain combinations, demonstrating\nthat optimal multimodal performance arises from the intrinsic\ninformation-theoretic complementarity among fused domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\uff0c\u5728\u751f\u7269\u533b\u5b66\u4fe1\u53f7\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u4e2d\uff0c\u6700\u4f73\u6a21\u6001\u878d\u5408\u53d6\u51b3\u4e8e\u7279\u5f81\u57df\u7684\u5185\u5728\u4e92\u8865\u6027\u800c\u975e\u6570\u91cf\uff0c\u5e76\u6784\u5efa\u4e86\u91cf\u5316\u4e92\u8865\u6027\u7684\u7406\u8bba\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u89c2\u5ff5\u8ba4\u4e3a\u878d\u5408\u591a\u6a21\u6001\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u672c\u7814\u7a76\u8d28\u7591\u5e76\u975e\u6240\u6709\u878d\u5408\u90fd\u5177\u4f18\u52bf\uff0c\u65e8\u5728\u7cfb\u7edf\u5206\u6790\u4e92\u8865\u7279\u5f81\u57df\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4ee5\u5bfb\u627e\u6700\u4f18\u878d\u5408\u7b56\u7565\u3002", "method": "\u8bbe\u8ba1\u5e76\u4e25\u683c\u8bc4\u4f30\u4e86\u4e94\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7528\u4e8eECG\u5206\u7c7b\uff1a\u4e09\u79cd\u5355\u6a21\u6001\uff081D-CNN\u7528\u4e8e\u65f6\u57df\uff0c2D-CNN\u7528\u4e8e\u65f6\u9891\u57df\uff0c1D-CNN-Transformer\u7528\u4e8e\u9891\u57df\uff09\u548c\u4e24\u79cd\u591a\u6a21\u6001\uff08Hybrid 1\u878d\u5408\u65f6\u57df\u548c\u65f6\u9891\u57df\uff0cHybrid 2\u878d\u5408\u65f6\u57df\u3001\u65f6\u9891\u57df\u548c\u9891\u57df\uff09\u3002\u91c7\u7528\u81ea\u52a9\u6cd5\u548c\u8d1d\u53f6\u65af\u63a8\u65ad\u8fdb\u884c\u7edf\u8ba1\u9a8c\u8bc1\uff0c\u5e76\u8fdb\u884c\u4e86\u6d88\u878d\u7814\u7a76\u3002", "result": "Hybrid 1\u6a21\u578b\uff08\u878d\u5408\u65f6\u57df\u548c\u65f6\u9891\u57df\uff09\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u8bc1\u5b9e\u4e86\u65f6\u57df\u548c\u65f6\u9891\u57df\u7684\u534f\u540c\u4e92\u8865\u6027\u3002\u7136\u800c\uff0cHybrid 2\u6a21\u578b\uff08\u989d\u5916\u52a0\u5165\u9891\u57df\uff09\u672a\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff0c\u6709\u65f6\u751a\u81f3\u7565\u6709\u4e0b\u964d\uff0c\u8868\u660e\u9891\u57df\u5b58\u5728\u8868\u5f81\u5197\u4f59\uff0c\u8fd9\u4e00\u70b9\u4e5f\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u5f97\u5230\u8bc1\u5b9e\u3002", "conclusion": "\u751f\u7269\u533b\u5b66\u4fe1\u53f7\u5206\u6790\u4e2d\u7684\u6700\u4f18\u591a\u6a21\u6001\u878d\u5408\u5e76\u975e\u53d6\u51b3\u4e8e\u6a21\u6001\u6570\u91cf\uff0c\u800c\u662f\u5176\u5185\u5728\u4e92\u8865\u6027\u8d28\u91cf\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u8d21\u732e\uff0c\u5373\u201c\u591a\u6a21\u6001ECG\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u4e92\u8865\u7279\u5f81\u57df\u201d\uff0c\u4e3a\u8bc6\u522b\u7406\u60f3\u57df\u7ec4\u5408\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6570\u5b66\u91cf\u5316\u7684\u6846\u67b6\uff0c\u5f3a\u8c03\u6700\u4f73\u6027\u80fd\u6e90\u4e8e\u878d\u5408\u57df\u95f4\u56fa\u6709\u7684\u4fe1\u606f\u8bba\u4e92\u8865\u6027\u3002"}}
{"id": "2508.01184", "pdf": "https://arxiv.org/pdf/2508.01184", "abs": "https://arxiv.org/abs/2508.01184", "authors": ["Xinhang Wan", "Dongqiang Gou", "Xinwang Liu", "En Zhu", "Xuming He"], "title": "Object Affordance Recognition and Grounding via Multi-scale Cross-modal Representation Learning", "categories": ["cs.CV"], "comment": null, "summary": "A core problem of Embodied AI is to learn object manipulation from\nobservation, as humans do. To achieve this, it is important to localize 3D\nobject affordance areas through observation such as images (3D affordance\ngrounding) and understand their functionalities (affordance classification).\nPrevious attempts usually tackle these two tasks separately, leading to\ninconsistent predictions due to lacking proper modeling of their dependency. In\naddition, these methods typically only ground the incomplete affordance areas\ndepicted in images, failing to predict the full potential affordance areas, and\noperate at a fixed scale, resulting in difficulty in coping with affordances\nsignificantly varying in scale with respect to the whole object. To address\nthese issues, we propose a novel approach that learns an affordance-aware 3D\nrepresentation and employs a stage-wise inference strategy leveraging the\ndependency between grounding and classification tasks. Specifically, we first\ndevelop a cross-modal 3D representation through efficient fusion and\nmulti-scale geometric feature propagation, enabling inference of full potential\naffordance areas at a suitable regional scale. Moreover, we adopt a simple\ntwo-stage prediction mechanism, effectively coupling grounding and\nclassification for better affordance understanding. Experiments demonstrate the\neffectiveness of our method, showing improved performance in both affordance\ngrounding and classification.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4eb2\u548c\u6027\u611f\u77e53D\u8868\u793a\u548c\u91c7\u7528\u9636\u6bb5\u6027\u63a8\u7406\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b33D\u7269\u4f53\u529f\u80fd\u533a\u57df\u5b9a\u4f4d\u4e0e\u5206\u7c7b\u4e2d\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5177\u8eabAI\u5b66\u4e60\u7269\u4f53\u64cd\u4f5c\u9700\u5bf93D\u529f\u80fd\u533a\u57df\u8fdb\u884c\u5b9a\u4f4d\uff08grounding\uff09\u548c\u5206\u7c7b\uff08classification\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u5e38\u5c06\u8fd9\u4e24\u4efb\u52a1\u72ec\u7acb\u5904\u7406\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u4e00\u81f4\uff1b\u540c\u65f6\uff0c\u4ec5\u80fd\u5b9a\u4f4d\u56fe\u50cf\u4e2d\u4e0d\u5b8c\u6574\u7684\u529f\u80fd\u533a\u57df\uff0c\u65e0\u6cd5\u9884\u6d4b\u5b8c\u6574\u6f5c\u80fd\u533a\u57df\uff1b\u4e14\u56fa\u5b9a\u5c3a\u5ea6\u64cd\u4f5c\u96be\u4ee5\u5e94\u5bf9\u529f\u80fd\u533a\u57df\u7684\u5c3a\u5ea6\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eb2\u548c\u6027\u611f\u77e53D\u8868\u793a\u5b66\u4e60\u4e0e\u9636\u6bb5\u6027\u63a8\u7406\u7b56\u7565\u3002\u5177\u4f53\u5305\u62ec\uff1a1. \u6784\u5efa\u8de8\u6a21\u60013D\u8868\u793a\uff0c\u901a\u8fc7\u9ad8\u6548\u878d\u5408\u548c\u591a\u5c3a\u5ea6\u51e0\u4f55\u7279\u5f81\u4f20\u64ad\uff0c\u5b9e\u73b0\u5b8c\u6574\u6f5c\u5728\u529f\u80fd\u533a\u57df\u7684\u63a8\u65ad\u53ca\u5c3a\u5ea6\u9002\u5e94\u6027\u30022. \u91c7\u7528\u4e24\u9636\u6bb5\u9884\u6d4b\u673a\u5236\uff0c\u6709\u6548\u8026\u5408\u529f\u80fd\u533a\u57df\u5b9a\u4f4d\u4e0e\u5206\u7c7b\u4efb\u52a1\uff0c\u63d0\u5347\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e863D\u529f\u80fd\u533a\u57df\u5b9a\u4f4d\u548c\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u7edf\u4e00\u7684\u4eb2\u548c\u6027\u611f\u77e53D\u8868\u793a\u548c\u9636\u6bb5\u6027\u63a8\u7406\u7b56\u7565\uff0c\u8be5\u7814\u7a76\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u57283D\u529f\u80fd\u533a\u57df\u5b9a\u4f4d\u4e0e\u5206\u7c7b\u4e2d\u9047\u5230\u7684\u6311\u6218\uff0c\u4e3a\u5177\u8eabAI\u66f4\u51c6\u786e\u3001\u66f4\u5168\u9762\u5730\u7406\u89e3\u7269\u4f53\u529f\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.01432", "pdf": "https://arxiv.org/pdf/2508.01432", "abs": "https://arxiv.org/abs/2508.01432", "authors": ["Yuanzhe Shen", "Kaimin Wang", "Changze Lv", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "TripTailor: A Real-World Benchmark for Personalized Travel Planning", "categories": ["cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "The continuous evolution and enhanced reasoning capabilities of large\nlanguage models (LLMs) have elevated their role in complex tasks, notably in\ntravel planning, where demand for personalized, high-quality itineraries is\nrising. However, current benchmarks often rely on unrealistic simulated data,\nfailing to reflect the differences between LLM-generated and real-world\nitineraries. Existing evaluation metrics, which primarily emphasize\nconstraints, fall short of providing a comprehensive assessment of the overall\nquality of travel plans. To address these limitations, we introduce TripTailor,\na benchmark designed specifically for personalized travel planning in\nreal-world scenarios. This dataset features an extensive collection of over\n500,000 real-world points of interest (POIs) and nearly 4,000 diverse travel\nitineraries, complete with detailed information, providing a more authentic\nevaluation framework. Experiments show that fewer than 10\\% of the itineraries\ngenerated by the latest state-of-the-art LLMs achieve human-level performance.\nMoreover, we identify several critical challenges in travel planning, including\nthe feasibility, rationality, and personalized customization of the proposed\nsolutions. We hope that TripTailor will drive the development of travel\nplanning agents capable of understanding and meeting user needs while\ngenerating practical itineraries. Our code and dataset are available at\nhttps://github.com/swxkfm/TripTailor", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u4e86TripTailor\uff0c\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6784\u5efa\u7684\u4e2a\u6027\u5316\u65c5\u884c\u89c4\u5212\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u5b9e\u7528\u884c\u7a0b\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65c5\u884c\u89c4\u5212\u57fa\u51c6\u4f9d\u8d56\u4e0d\u771f\u5b9e\u7684\u6a21\u62df\u6570\u636e\uff0c\u4e14\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u5168\u9762\u8861\u91cf\u884c\u7a0b\u8d28\u91cf\uff0c\u5bfc\u81f4LLM\u751f\u6210\u7684\u65c5\u884c\u8ba1\u5212\u4e0e\u73b0\u5b9e\u9700\u6c42\u5b58\u5728\u5dee\u8ddd\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86TripTailor\uff0c\u4e00\u4e2a\u5305\u542b50\u591a\u4e07\u771f\u5b9e\u4e16\u754c\u5174\u8da3\u70b9\uff08POI\uff09\u548c\u8fd14000\u6761\u591a\u6837\u5316\u65c5\u884c\u884c\u7a0b\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63d0\u4f9b\u66f4\u771f\u5b9e\u7684\u4e2a\u6027\u5316\u65c5\u884c\u89c4\u5212\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684LLM\u6240\u751f\u6210\u7684\u884c\u7a0b\u4e2d\uff0c\u4ec5\u6709\u4e0d\u523010%\u80fd\u8fbe\u5230\u4eba\u7c7b\u6c34\u5e73\u3002\u7814\u7a76\u8fd8\u8bc6\u522b\u51fa\u65c5\u884c\u89c4\u5212\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u53ef\u884c\u6027\u3001\u5408\u7406\u6027\u548c\u4e2a\u6027\u5316\u5b9a\u5236\u3002", "conclusion": "TripTailor\u65e8\u5728\u4fc3\u8fdb\u5f00\u53d1\u51fa\u66f4\u80fd\u7406\u89e3\u7528\u6237\u9700\u6c42\u5e76\u751f\u6210\u5b9e\u7528\u65c5\u884c\u65b9\u6848\u7684\u667a\u80fd\u4f53\u3002"}}
{"id": "2508.01696", "pdf": "https://arxiv.org/pdf/2508.01696", "abs": "https://arxiv.org/abs/2508.01696", "authors": ["Yi Jiang", "Sendong Zhao", "Jianbo Li", "Haochun Wang", "Lizhe Zhang", "Yan Liu", "Bin Qin"], "title": "Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy", "categories": ["cs.CL", "cs.AI"], "comment": "code available at https://github.com/liunian-Jay/CoCoA", "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising framework for\nenhancing the capabilities of Large Language Models (LLMs), especially in\nknowledge-intensive tasks. Despite its advantages, current RAG methods often\nstruggle to *fully exploit knowledge during generation*. In particular, the\nsynergy between the model's internal parametric knowledge and external\nretrieved knowledge remains limited. Retrieved contents may sometimes mislead\ngeneration, while certain generated content can guide the model toward more\naccurate outputs. In this work, we propose Collaborative Chain-of-Agents, a\nframework designed to enhance explicitly synergy over both parametric and\nretrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent\nRAG framework that first performs conditional knowledge induction and then\nreasons answers. Building on this, we develop CoCoA, a long-chain training\nstrategy that synthesizes extended multi-agent reasoning trajectories from\nCoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability\nto explicitly integrate and jointly leverage parametric and retrieved\nknowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior\nperformance on open-domain and multi-hop QA tasks.", "AI": {"tldr": "\u9488\u5bf9RAG\u672a\u80fd\u5145\u5206\u5229\u7528\u53c2\u6570\u77e5\u8bc6\u4e0e\u68c0\u7d22\u77e5\u8bc6\u534f\u540c\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86CoCoA-zero\u591a\u667a\u80fd\u4f53RAG\u6846\u67b6\u548cCoCoA\u957f\u94fe\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5bf9\u4e24\u79cd\u77e5\u8bc6\u7684\u96c6\u6210\u4e0e\u5229\u7528\u80fd\u529b\uff0c\u5e76\u5728\u5f00\u653e\u57df\u548c\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1RAG\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5f53\u524d\u65b9\u6cd5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u672a\u80fd\u5145\u5206\u5229\u7528\u77e5\u8bc6\uff0c\u7279\u522b\u662f\u6a21\u578b\u5185\u90e8\u53c2\u6570\u77e5\u8bc6\u4e0e\u5916\u90e8\u68c0\u7d22\u77e5\u8bc6\u7684\u534f\u540c\u6709\u9650\u3002\u68c0\u7d22\u5185\u5bb9\u6709\u65f6\u4f1a\u8bef\u5bfc\u751f\u6210\uff0c\u800c\u67d0\u4e9b\u751f\u6210\u5185\u5bb9\u5374\u80fd\u5f15\u5bfc\u6a21\u578b\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u8f93\u51fa\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u534f\u540c\u667a\u80fd\u4f53\u94fe\uff08Collaborative Chain-of-Agents\uff09\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3a\u53c2\u6570\u77e5\u8bc6\u548c\u68c0\u7d22\u77e5\u8bc6\u7684\u534f\u540c\u4f5c\u7528\u3002\u5177\u4f53\u5305\u62ec\uff1a1. CoCoA-zero\uff1a\u4e00\u4e2a\u591a\u667a\u80fd\u4f53RAG\u6846\u67b6\uff0c\u9996\u5148\u8fdb\u884c\u6761\u4ef6\u77e5\u8bc6\u5f52\u7eb3\uff0c\u7136\u540e\u8fdb\u884c\u7b54\u6848\u63a8\u7406\u30022. CoCoA\uff1a\u4e00\u79cd\u957f\u94fe\u8bad\u7ec3\u7b56\u7565\uff0c\u5229\u7528CoCoA-zero\u751f\u6210\u7684\u6269\u5c55\u591a\u667a\u80fd\u4f53\u63a8\u7406\u8f68\u8ff9\u6765\u5fae\u8c03LLM\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u663e\u5f0f\u96c6\u6210\u548c\u8054\u5408\u5229\u7528\u53c2\u6570\u77e5\u8bc6\u4e0e\u68c0\u7d22\u77e5\u8bc6\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCoCoA-zero\u548cCoCoA\u5728\u5f00\u653e\u57df\u548c\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "conclusion": "CoCoA-zero\u548cCoCoA\u6846\u67b6\u901a\u8fc7\u6709\u6548\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u53c2\u6570\u77e5\u8bc6\u4e0e\u68c0\u7d22\u77e5\u8bc6\u7684\u534f\u540c\u4f5c\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2508.00965", "pdf": "https://arxiv.org/pdf/2508.00965", "abs": "https://arxiv.org/abs/2508.00965", "authors": ["Roie Kazoom", "Ofir Cohen", "Rami Puzis", "Asaf Shabtai", "Ofer Hadar"], "title": "VAULT: Vigilant Adversarial Updates via LLM-Driven Retrieval-Augmented Generation for NLI", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce VAULT, a fully automated adversarial RAG pipeline that\nsystematically uncovers and remedies weaknesses in NLI models through three\nstages: retrieval, adversarial generation, and iterative retraining. First, we\nperform balanced few-shot retrieval by embedding premises with both semantic\n(BGE) and lexical (BM25) similarity. Next, we assemble these contexts into LLM\nprompts to generate adversarial hypotheses, which are then validated by an LLM\nensemble for label fidelity. Finally, the validated adversarial examples are\ninjected back into the training set at increasing mixing ratios, progressively\nfortifying a zero-shot RoBERTa-base model.On standard benchmarks, VAULT\nelevates RoBERTa-base accuracy from 88.48% to 92.60% on SNLI +4.12%, from\n75.04% to 80.95% on ANLI +5.91%, and from 54.67% to 71.99% on MultiNLI +17.32%.\nIt also consistently outperforms prior in-context adversarial methods by up to\n2.0% across datasets. By automating high-quality adversarial data curation at\nscale, VAULT enables rapid, human-independent robustness improvements in NLI\ninference tasks.", "AI": {"tldr": "VAULT\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u5bf9\u6297\u6027RAG\u7ba1\u9053\uff0c\u901a\u8fc7\u68c0\u7d22\u3001\u5bf9\u6297\u6027\u751f\u6210\u548c\u8fed\u4ee3\u518d\u8bad\u7ec3\uff0c\u7cfb\u7edf\u5730\u53d1\u73b0\u5e76\u4fee\u590dNLI\u6a21\u578b\u7684\u5f31\u70b9\uff0c\u663e\u8457\u63d0\u5347\u5176\u9c81\u68d2\u6027\u3002", "motivation": "\u53d1\u73b0\u5e76\u5f25\u8865\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u6a21\u578b\u7684\u5f31\u70b9\uff0c\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "method": "VAULT\u91c7\u7528\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a1. \u68c0\u7d22\uff1a\u7ed3\u5408\u8bed\u4e49\uff08BGE\uff09\u548c\u8bcd\u6c47\uff08BM25\uff09\u76f8\u4f3c\u6027\u8fdb\u884c\u5c11\u6837\u672c\u68c0\u7d22\uff1b2. \u5bf9\u6297\u6027\u751f\u6210\uff1a\u4f7f\u7528LLM\u63d0\u793a\u751f\u6210\u5bf9\u6297\u6027\u5047\u8bbe\uff0c\u5e76\u901a\u8fc7LLM\u96c6\u6210\u8fdb\u884c\u9a8c\u8bc1\uff1b3. \u8fed\u4ee3\u518d\u8bad\u7ec3\uff1a\u5c06\u9a8c\u8bc1\u540e\u7684\u5bf9\u6297\u6027\u793a\u4f8b\u6ce8\u5165\u8bad\u7ec3\u96c6\uff0c\u9010\u6b65\u5f3a\u5316RoBERTa-base\u6a21\u578b\u3002", "result": "VAULT\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86RoBERTa-base\u7684\u51c6\u786e\u6027\uff1aSNLI\u63d0\u53474.12%\u81f392.60%\uff0cANLI\u63d0\u53475.91%\u81f380.95%\uff0cMultiNLI\u63d0\u534717.32%\u81f371.99%\u3002\u540c\u65f6\uff0c\u5176\u8868\u73b0\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u4e0a\u4e0b\u6587\u5bf9\u6297\u6027\u65b9\u6cd5\uff0c\u6700\u9ad8\u8fbe2.0%\u3002", "conclusion": "VAULT\u901a\u8fc7\u81ea\u52a8\u5316\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u5bf9\u6297\u6027\u6570\u636e\u751f\u6210\uff0c\u5b9e\u73b0\u4e86NLI\u63a8\u7406\u4efb\u52a1\u4e2d\u5feb\u901f\u3001\u72ec\u7acb\u4e8e\u4eba\u7c7b\u7684\u9c81\u68d2\u6027\u6539\u8fdb\u3002"}}
{"id": "2508.01197", "pdf": "https://arxiv.org/pdf/2508.01197", "abs": "https://arxiv.org/abs/2508.01197", "authors": ["Zhan Shi", "Song Wang", "Junbo Chen", "Jianke Zhu"], "title": "A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding", "categories": ["cs.CV", "cs.RO"], "comment": "IROS 2025 Accepted Paper", "summary": "Visual grounding aims to identify objects or regions in a scene based on\nnatural language descriptions, essential for spatially aware perception in\nautonomous driving. However, existing visual grounding tasks typically depend\non bounding boxes that often fail to capture fine-grained details. Not all\nvoxels within a bounding box are occupied, resulting in inaccurate object\nrepresentations. To address this, we introduce a benchmark for 3D occupancy\ngrounding in challenging outdoor scenes. Built on the nuScenes dataset, it\nintegrates natural language with voxel-level occupancy annotations, offering\nmore precise object perception compared to the traditional grounding task.\nMoreover, we propose GroundingOcc, an end-to-end model designed for 3D\noccupancy grounding through multi-modal learning. It combines visual, textual,\nand point cloud features to predict object location and occupancy information\nfrom coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder\nfor feature extraction, an occupancy head for voxel-wise predictions, and a\ngrounding head to refine localization. Additionally, a 2D grounding module and\na depth estimation module enhance geometric understanding, thereby boosting\nmodel performance. Extensive experiments on the benchmark demonstrate that our\nmethod outperforms existing baselines on 3D occupancy grounding. The dataset is\navailable at https://github.com/RONINGOD/GroundingOcc.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9488\u5bf9\u6237\u5916\u573a\u666f\u76843D\u5360\u636e\u63a5\u5730\u65b0\u57fa\u51c6\u53ca\u7aef\u5230\u7aef\u6a21\u578bGroundingOcc\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u8fb9\u754c\u6846\u89c6\u89c9\u63a5\u5730\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u5bf9\u8c61\u611f\u77e5\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u63a5\u5730\u4efb\u52a1\u4f9d\u8d56\u8fb9\u754c\u6846\uff0c\u4f46\u8fb9\u754c\u6846\u65e0\u6cd5\u6355\u6349\u7ec6\u7c92\u5ea6\u7ec6\u8282\u4e14\u5305\u542b\u672a\u5360\u636e\u4f53\u7d20\uff0c\u5bfc\u81f4\u5bf9\u8c61\u8868\u793a\u4e0d\u51c6\u786e\u3002\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u3001\u7a7a\u95f4\u611f\u77e5\u7684\u5bf9\u8c61\u8bc6\u522b\u3002", "method": "\u5f15\u5165\u57fa\u4e8enuScenes\u6570\u636e\u96c6\u76843D\u5360\u636e\u63a5\u5730\u57fa\u51c6\uff0c\u6574\u5408\u81ea\u7136\u8bed\u8a00\u4e0e\u4f53\u7d20\u7ea7\u5360\u636e\u6807\u6ce8\u3002\u63d0\u51faGroundingOcc\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\uff08\u7ed3\u5408\u89c6\u89c9\u3001\u6587\u672c\u3001\u70b9\u4e91\u7279\u5f81\uff09\u5b9e\u73b03D\u5360\u636e\u63a5\u5730\uff0c\u4ece\u7c97\u5230\u7cbe\u9884\u6d4b\u5bf9\u8c61\u4f4d\u7f6e\u548c\u5360\u636e\u4fe1\u606f\u3002GroundingOcc\u5305\u542b\u591a\u6a21\u6001\u7f16\u7801\u5668\u3001\u5360\u636e\u5934\u3001\u63a5\u5730\u5934\uff0c\u5e76\u8f85\u4ee52D\u63a5\u5730\u6a21\u5757\u548c\u6df1\u5ea6\u4f30\u8ba1\u6a21\u5757\u4ee5\u589e\u5f3a\u6027\u80fd\u3002", "result": "\u5728\u6240\u6784\u5efa\u76843D\u5360\u636e\u63a5\u5730\u57fa\u51c6\u4e0a\uff0cGroundingOcc\u65b9\u6cd5\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5efa\u7acb\u65b0\u76843D\u5360\u636e\u63a5\u5730\u57fa\u51c6\u548c\u63d0\u51fa\u9ad8\u6027\u80fd\u6a21\u578bGroundingOcc\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u89c6\u89c9\u63a5\u5730\u4efb\u52a1\u4e2d\u8fb9\u754c\u6846\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u6237\u5916\u573a\u666f\u4e2d\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u7cbe\u786e\u5bf9\u8c61\u611f\u77e5\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01475", "pdf": "https://arxiv.org/pdf/2508.01475", "abs": "https://arxiv.org/abs/2508.01475", "authors": ["Zhen Wu", "Ritam Dutt", "Luke M. Breitfeller", "Armineh Nourbakhsh", "Siddharth Parekh", "Carolyn Ros\u00e9"], "title": "$R^2$-CoD: Understanding Text-Graph Complementarity in Relational Reasoning via Knowledge Co-Distillation", "categories": ["cs.AI"], "comment": null, "summary": "Relational reasoning lies at the core of many NLP tasks, drawing on\ncomplementary signals from text and graphs. While prior research has\ninvestigated how to leverage this dual complementarity, a detailed and\nsystematic understanding of text-graph interplay and its effect on hybrid\nmodels remains underexplored. We take an analysis-driven approach to\ninvestigate text-graph representation complementarity via a unified\narchitecture that supports knowledge co-distillation (CoD). We explore five\ntasks involving relational reasoning that differ in how text and graph\nstructures encode the information needed to solve that task. By tracking how\nthese dual representations evolve during training, we uncover interpretable\npatterns of alignment and divergence, and provide insights into when and why\ntheir integration is beneficial.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4e00\u4e2a\u7edf\u4e00\u67b6\u6784\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u6587\u672c\u4e0e\u56fe\u8868\u793a\u5728\u5173\u7cfb\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u4e92\u8865\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5b83\u4eec\u878d\u5408\u7684\u6709\u6548\u65f6\u673a\u4e0e\u539f\u56e0\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u867d\u5df2\u5229\u7528\u6587\u672c\u4e0e\u56fe\u7684\u4e92\u8865\u6027\uff0c\u4f46\u5bf9\u6587\u672c-\u56fe\u4ea4\u4e92\u53ca\u5176\u5bf9\u6df7\u5408\u6a21\u578b\u5f71\u54cd\u7684\u8be6\u7ec6\u7cfb\u7edf\u6027\u7406\u89e3\u4ecd\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u5206\u6790\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u652f\u6301\u77e5\u8bc6\u534f\u540c\u84b8\u998f\uff08CoD\uff09\u7684\u7edf\u4e00\u67b6\u6784\uff0c\u8c03\u67e5\u6587\u672c-\u56fe\u8868\u793a\u7684\u4e92\u8865\u6027\u3002\u5728\u4e94\u79cd\u4e0d\u540c\u7684\u5173\u7cfb\u63a8\u7406\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8ffd\u8e2a\u53cc\u91cd\u8868\u793a\u5728\u8bad\u7ec3\u671f\u95f4\u7684\u6f14\u53d8\u3002", "result": "\u63ed\u793a\u4e86\u6587\u672c\u548c\u56fe\u8868\u793a\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u53ef\u89e3\u91ca\u7684\u5bf9\u9f50\u548c\u5206\u6b67\u6a21\u5f0f\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u5173\u4e8e\u4f55\u65f6\u4ee5\u53ca\u4e3a\u4f55\u6587\u672c\u4e0e\u56fe\u8868\u793a\u7684\u6574\u5408\u6709\u76ca\u7684\u6df1\u5165\u89c1\u89e3\u3002"}}
{"id": "2508.01708", "pdf": "https://arxiv.org/pdf/2508.01708", "abs": "https://arxiv.org/abs/2508.01708", "authors": ["Berkay K\u00f6pr\u00fc", "Mehrzad Mashal", "Yigit Gurses", "Akos Kadar", "Maximilian Schmitt", "Ditty Mathew", "Felix Burkhardt", "Florian Eyben", "Bj\u00f6rn W. Schuller"], "title": "Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have advanced natural language processing (NLP)\nskills such as through next-token prediction and self-attention, but their\nability to integrate broad context also makes them prone to incorporating\nirrelevant information. Prior work has focused on semantic leakage, bias\nintroduced by semantically irrelevant context. In this paper, we introduce\nexpression leakage, a novel phenomenon where LLMs systematically generate\nsentimentally charged expressions that are semantically unrelated to the input\ncontext. To analyse the expression leakage, we collect a benchmark dataset\nalong with a scheme to automatically generate a dataset from free-form text\nfrom common-crawl. In addition, we propose an automatic evaluation pipeline\nthat correlates well with human judgment, which accelerates the benchmarking by\ndecoupling from the need of annotation for each analysed model. Our experiments\nshow that, as the model scales in the parameter space, the expression leakage\nreduces within the same LLM family. On the other hand, we demonstrate that\nexpression leakage mitigation requires specific care during the model building\nprocess, and cannot be mitigated by prompting. In addition, our experiments\nindicate that, when negative sentiment is injected in the prompt, it disrupts\nthe generation process more than the positive sentiment, causing a higher\nexpression leakage rate.", "AI": {"tldr": "\u672c\u6587\u5b9a\u4e49\u5e76\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u4e00\u79cd\u540d\u4e3a\u201c\u8868\u60c5\u6cc4\u9732\u201d\u7684\u65b0\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u751f\u6210\u4e0e\u8f93\u5165\u8bed\u4e49\u65e0\u5173\u7684\u60c5\u611f\u6027\u8868\u8fbe\u3002\u7814\u7a76\u6784\u5efa\u4e86\u8bc4\u4f30\u65b9\u6cd5\u548c\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u8d8a\u5927\u6cc4\u9732\u8d8a\u5c11\uff0c\u4f46\u7f13\u89e3\u9700\u5728\u6a21\u578b\u6784\u5efa\u9636\u6bb5\u800c\u975e\u63d0\u793a\u5de5\u7a0b\uff0c\u4e14\u8d1f\u9762\u60c5\u611f\u6bd4\u6b63\u9762\u60c5\u611f\u66f4\u80fd\u52a0\u5267\u6cc4\u9732\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u81ea\u7136\u8bed\u8a00\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u6574\u5408\u5e7f\u6cdb\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\u4e5f\u4f7f\u5176\u5bb9\u6613\u5f15\u5165\u4e0d\u76f8\u5173\u4fe1\u606f\u3002\u73b0\u6709\u7814\u7a76\u5173\u6ce8\u201c\u8bed\u4e49\u6cc4\u9732\u201d\uff0c\u800c\u672c\u6587\u65e8\u5728\u8bc6\u522b\u3001\u5b9a\u4e49\u5e76\u6df1\u5165\u5206\u6790\u4e00\u79cd\u65b0\u7684\u73b0\u8c61\u2014\u2014\u201c\u8868\u60c5\u6cc4\u9732\u201d\uff0c\u5373LLM\u7cfb\u7edf\u6027\u5730\u751f\u6210\u4e0e\u8f93\u5165\u4e0a\u4e0b\u6587\u8bed\u4e49\u65e0\u5173\u4f46\u5e26\u6709\u60c5\u611f\u8272\u5f69\u7684\u8868\u8fbe\uff0c\u4ee5\u7406\u89e3\u5176\u53d1\u751f\u673a\u5236\u548c\u5f71\u54cd\u56e0\u7d20\u3002", "method": "1. \u5f15\u5165\u5e76\u5b9a\u4e49\u4e86\u201c\u8868\u60c5\u6cc4\u9732\u201d\u8fd9\u4e00\u65b0\u73b0\u8c61\u30022. \u6536\u96c6\u4e86\u4e00\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u4eceCommon Crawl\u81ea\u7531\u6587\u672c\u81ea\u52a8\u751f\u6210\u6570\u636e\u96c6\u7684\u65b9\u6848\u30023. \u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u8bc4\u4f30\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u76f8\u5173\uff0c\u5e76\u80fd\u8131\u79bb\u5bf9\u6bcf\u4e2a\u5206\u6790\u6a21\u578b\u7684\u6807\u6ce8\u9700\u6c42\uff0c\u4ece\u800c\u52a0\u901f\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "1. \u5728\u540c\u4e00LLM\u5bb6\u65cf\u4e2d\uff0c\u968f\u7740\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u7684\u589e\u5927\uff0c\u201c\u8868\u60c5\u6cc4\u9732\u201d\u73b0\u8c61\u6709\u6240\u51cf\u5c11\u30022. \u201c\u8868\u60c5\u6cc4\u9732\u201d\u7684\u7f13\u89e3\u9700\u8981\u6a21\u578b\u6784\u5efa\u8fc7\u7a0b\u4e2d\u7684\u7279\u5b9a\u8003\u91cf\uff0c\u65e0\u6cd5\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u6765\u51cf\u8f7b\u30023. \u5f53\u63d0\u793a\u4e2d\u6ce8\u5165\u8d1f\u9762\u60c5\u611f\u65f6\uff0c\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u5e72\u6270\u6bd4\u6b63\u9762\u60c5\u611f\u66f4\u5927\uff0c\u5bfc\u81f4\u66f4\u9ad8\u7684\u201c\u8868\u60c5\u6cc4\u9732\u201d\u7387\u3002", "conclusion": "\u201c\u8868\u60c5\u6cc4\u9732\u201d\u662fLLM\u4e2d\u4e00\u4e2a\u503c\u5f97\u5173\u6ce8\u7684\u65b0\u95ee\u9898\uff0c\u5b83\u6d89\u53ca\u6a21\u578b\u751f\u6210\u4e0e\u8f93\u5165\u65e0\u5173\u7684\u60c5\u611f\u8868\u8fbe\u3002\u5c3d\u7ba1\u6a21\u578b\u89c4\u6a21\u7684\u6269\u5927\u6709\u52a9\u4e8e\u964d\u4f4e\u8fd9\u79cd\u6cc4\u9732\uff0c\u4f46\u6839\u672c\u7684\u7f13\u89e3\u9700\u8981\u4ece\u6a21\u578b\u6784\u5efa\u5c42\u9762\u8fdb\u884c\u7279\u5b9a\u4f18\u5316\uff0c\u800c\u975e\u5355\u7eaf\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u3002\u6b64\u5916\uff0c\u8d1f\u9762\u60c5\u611f\u5bf9\u751f\u6210\u8fc7\u7a0b\u7684\u5e72\u6270\u5c24\u5176\u663e\u8457\uff0c\u4f1a\u5bfc\u81f4\u66f4\u9ad8\u7684\u6cc4\u9732\u7387\u3002"}}
{"id": "2508.00969", "pdf": "https://arxiv.org/pdf/2508.00969", "abs": "https://arxiv.org/abs/2508.00969", "authors": ["Lucas Robinet", "Ahmad Berjaoui", "Elizabeth Cohen-Jonathan Moyal"], "title": "Masked Omics Modeling for Multimodal Representation Learning across Histopathology and Molecular Profiles", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Self-supervised learning has driven major advances in computational pathology\nby enabling models to learn rich representations from hematoxylin and eosin\n(H&E)-stained cancer tissue. However, histopathology alone often falls short\nfor molecular characterization and understanding clinical outcomes, as\nimportant information is contained in high-dimensional omics profiles like\ntranscriptomics, methylomics, or genomics. In this work, we introduce MORPHEUS,\na unified transformer-based pre-training framework that encodes both\nhistopathology and multi-omics data into a shared latent space. At its core,\nMORPHEUS relies on a masked modeling objective applied to randomly selected\nomics portions, encouraging the model to learn biologically meaningful\ncross-modal relationships. The same pre-trained network can be applied to\nhistopathology alone or in combination with any subset of omics modalities,\nseamlessly adapting to the available inputs. Additionally, MORPHEUS enables\nany-to-any omics generation, enabling one or more omics profiles to be inferred\nfrom any subset of modalities, including H&E alone. Pre-trained on a large\npan-cancer cohort, MORPHEUS consistently outperforms state-of-the-art methods\nacross diverse modality combinations and tasks, positioning itself as a\npromising framework for developing multimodal foundation models in oncology.\nThe code is available at: https://github.com/Lucas-rbnt/MORPHEUS", "AI": {"tldr": "MORPHEUS\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u57fa\u4e8eTransformer\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u7ec4\u7ec7\u75c5\u7406\u5b66\u548c\u591a\u7ec4\u5b66\u6570\u636e\u7f16\u7801\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5e76\u901a\u8fc7\u63a9\u7801\u5efa\u6a21\u5b66\u4e60\u8de8\u6a21\u6001\u5173\u7cfb\uff0c\u5728\u80bf\u7624\u5b66\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u5b66\u4e60H&E\u67d3\u8272\u764c\u7ec4\u7ec7\u8868\u5f81\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u4ec5\u51ed\u7ec4\u7ec7\u75c5\u7406\u5b66\u96be\u4ee5\u8fdb\u884c\u5206\u5b50\u8868\u5f81\u548c\u7406\u89e3\u4e34\u5e8a\u7ed3\u679c\u3002\u9ad8\u7ef4\u7ec4\u5b66\u6570\u636e\uff08\u5982\u8f6c\u5f55\u7ec4\u5b66\u3001\u7532\u57fa\u7ec4\u5b66\u3001\u57fa\u56e0\u7ec4\u5b66\uff09\u5305\u542b\u91cd\u8981\u4fe1\u606f\uff0c\u9700\u8981\u4e0e\u75c5\u7406\u5b66\u6570\u636e\u7ed3\u5408\u3002", "method": "\u5f15\u5165\u4e86MORPHEUS\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u4e8eTransformer\u7684\u9884\u8bad\u7ec3\u6846\u67b6\u3002\u5b83\u5c06\u7ec4\u7ec7\u75c5\u7406\u5b66\u548c\u591a\u7ec4\u5b66\u6570\u636e\u7f16\u7801\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u6838\u5fc3\u662f\u5e94\u7528\u4e8e\u968f\u673a\u9009\u62e9\u7684\u7ec4\u5b66\u90e8\u5206\u7684\u63a9\u7801\u5efa\u6a21\u76ee\u6807\uff0c\u4ee5\u5b66\u4e60\u751f\u7269\u5b66\u4e0a\u6709\u610f\u4e49\u7684\u8de8\u6a21\u6001\u5173\u7cfb\u3002\u540c\u4e00\u9884\u8bad\u7ec3\u7f51\u7edc\u53ef\u5355\u72ec\u5e94\u7528\u4e8e\u7ec4\u7ec7\u75c5\u7406\u5b66\u6216\u4e0e\u4efb\u610f\u5b50\u96c6\u7ec4\u5b66\u6a21\u6001\u7ed3\u5408\uff0c\u5e76\u652f\u6301\u4efb\u610f\u7ec4\u5b66\u751f\u6210\uff08\u5305\u62ec\u4ec5\u4eceH&E\u63a8\u65ad\u7ec4\u5b66\uff09\u3002", "result": "MORPHEUS\u5728\u5927\u578b\u6cdb\u764c\u961f\u5217\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u540e\uff0c\u5728\u4e0d\u540c\u6a21\u6001\u7ec4\u5408\u548c\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "MORPHEUS\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u6846\u67b6\uff0c\u53ef\u7528\u4e8e\u5f00\u53d1\u80bf\u7624\u5b66\u9886\u57df\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2508.01206", "pdf": "https://arxiv.org/pdf/2508.01206", "abs": "https://arxiv.org/abs/2508.01206", "authors": ["Prathyush Kumar Reddy Lebaku", "Lu Gao", "Pan Lu", "Jingran Sun"], "title": "Deep Learning for Pavement Condition Evaluation Using Satellite Imagery", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Civil infrastructure systems covers large land areas and needs frequent\ninspections to maintain their public service capabilities. The conventional\napproaches of manual surveys or vehicle-based automated surveys to assess\ninfrastructure conditions are often labor-intensive and time-consuming. For\nthis reason, it is worthwhile to explore more cost-effective methods for\nmonitoring and maintaining these infrastructures. Fortunately, recent\nadvancements in satellite systems and image processing algorithms have opened\nup new possibilities. Numerous satellite systems have been employed to monitor\ninfrastructure conditions and identify damages. Due to the improvement in\nground sample distance (GSD), the level of detail that can be captured has\nsignificantly increased. Taking advantage of these technology advancement, this\nresearch investigated to evaluate pavement conditions using deep learning\nmodels for analyzing satellite images. We gathered over 3,000 satellite images\nof pavement sections, together with pavement evaluation ratings from TxDOT's\nPMIS database. The results of our study show an accuracy rate is exceeding 90%.\nThis research paves the way for a rapid and cost-effective approach to\nevaluating the pavement network in the future.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u536b\u661f\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u8def\u9762\u72b6\u51b5\u7684\u5feb\u901f\u3001\u7ecf\u6d4e\u8bc4\u4f30\uff0c\u51c6\u786e\u7387\u8d85\u8fc790%\u3002", "motivation": "\u4f20\u7edf\u571f\u6728\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u65b9\u6cd5\u8017\u65f6\u8d39\u529b\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u800c\u536b\u661f\u7cfb\u7edf\u548c\u56fe\u50cf\u5904\u7406\u7b97\u6cd5\u7684\u8fdb\u6b65\u4e3a\u63a2\u7d22\u66f4\u5177\u6210\u672c\u6548\u76ca\u7684\u76d1\u6d4b\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5206\u6790\u8d85\u8fc73000\u5f20\u8def\u9762\u536b\u661f\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408TxDOT\u7684PMIS\u6570\u636e\u5e93\u4e2d\u7684\u8def\u9762\u8bc4\u4f30\u7b49\u7ea7\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u8def\u9762\u72b6\u51b5\u8bc4\u4f30\u7684\u51c6\u786e\u7387\u8d85\u8fc790%\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u672a\u6765\u5feb\u901f\u3001\u7ecf\u6d4e\u5730\u8bc4\u4f30\u8def\u9762\u7f51\u7edc\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.01476", "pdf": "https://arxiv.org/pdf/2508.01476", "abs": "https://arxiv.org/abs/2508.01476", "authors": ["Arindam Khanda", "Anurag Satpathy", "Amit Jha", "Sajal K. Das"], "title": "CARGO: A Co-Optimization Framework for EV Charging and Routing in Goods Delivery Logistics", "categories": ["cs.AI"], "comment": null, "summary": "With growing interest in sustainable logistics, electric vehicle (EV)-based\ndeliveries offer a promising alternative for urban distribution. However, EVs\nface challenges due to their limited battery capacity, requiring careful\nplanning for recharging. This depends on factors such as the charging point\n(CP) availability, cost, proximity, and vehicles' state of charge (SoC). We\npropose CARGO, a framework addressing the EV-based delivery route planning\nproblem (EDRP), which jointly optimizes route planning and charging for\ndeliveries within time windows. After proving the problem's NP-hardness, we\npropose a mixed integer linear programming (MILP)-based exact solution and a\ncomputationally efficient heuristic method. Using real-world datasets, we\nevaluate our methods by comparing the heuristic to the MILP solution, and\nbenchmarking it against baseline strategies, Earliest Deadline First (EDF) and\nNearest Delivery First (NDF). The results show up to 39% and 22% reductions in\nthe charging cost over EDF and NDF, respectively, while completing comparable\ndeliveries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCARGO\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u7535\u52a8\u6c7d\u8f66\u914d\u9001\u7684\u8def\u7ebf\u89c4\u5212\u548c\u5145\u7535\u4f18\u5316\u95ee\u9898\u3002\u8be5\u6846\u67b6\u5305\u542b\u7cbe\u786e\u89e3\u6cd5\u548c\u9ad8\u6548\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u6570\u636e\u4e0b\u663e\u8457\u964d\u4f4e\u5145\u7535\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u914d\u9001\u6548\u7387\u3002", "motivation": "\u968f\u7740\u53ef\u6301\u7eed\u7269\u6d41\u7684\u5174\u8d77\uff0c\u7535\u52a8\u6c7d\u8f66\u914d\u9001\u4e3a\u57ce\u5e02\u5206\u9500\u63d0\u4f9b\u524d\u666f\uff0c\u4f46\u5176\u6709\u9650\u7684\u7535\u6c60\u5bb9\u91cf\u548c\u5145\u7535\u9700\u6c42\uff08\u53d7\u5145\u7535\u70b9\u53ef\u7528\u6027\u3001\u6210\u672c\u3001\u8ddd\u79bb\u548c\u8f66\u8f86\u7535\u91cf\u5f71\u54cd\uff09\u662f\u4e3b\u8981\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u4e2a\u7efc\u5408\u4f18\u5316\u8def\u5f84\u89c4\u5212\u548c\u5145\u7535\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86CARGO\u6846\u67b6\u6765\u89e3\u51b3\u7535\u52a8\u6c7d\u8f66\u914d\u9001\u8def\u7ebf\u89c4\u5212\u95ee\u9898\uff08EDRP\uff09\uff0c\u8be5\u95ee\u9898\u8054\u5408\u4f18\u5316\u4e86\u5e26\u65f6\u95f4\u7a97\u7684\u914d\u9001\u8def\u7ebf\u89c4\u5212\u548c\u5145\u7535\u3002\u7814\u7a76\u9996\u5148\u8bc1\u660e\u4e86EDRP\u662fNP-hard\u95ee\u9898\uff0c\u7136\u540e\u63d0\u51fa\u4e86\u57fa\u4e8e\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u7684\u7cbe\u786e\u89e3\u6cd5\u548c\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\u8fd9\u4e9b\u65b9\u6cd5\u5229\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5c06\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e0eMILP\u89e3\u4ee5\u53ca\u57fa\u7ebf\u7b56\u7565\uff08\u6700\u65e9\u622a\u6b62\u65e5\u671f\u4f18\u5148EDF\u548c\u6700\u8fd1\u914d\u9001\u70b9\u4f18\u5148NDF\uff09\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u4e0eEDF\u548cNDF\u57fa\u7ebf\u7b56\u7565\u76f8\u6bd4\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5145\u7535\u6210\u672c\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86\u9ad8\u8fbe39%\u548c22%\u7684\u964d\u4f4e\uff0c\u540c\u65f6\u5b8c\u6210\u4e86\u53ef\u6bd4\u6570\u91cf\u7684\u914d\u9001\u4efb\u52a1\u3002", "conclusion": "CARGO\u6846\u67b6\u53ca\u5176\u63d0\u51fa\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u964d\u4f4e\u7535\u52a8\u6c7d\u8f66\u914d\u9001\u7684\u5145\u7535\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u914d\u9001\u6548\u7387\uff0c\u4e3a\u57ce\u5e02\u53ef\u6301\u7eed\u7269\u6d41\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01710", "pdf": "https://arxiv.org/pdf/2508.01710", "abs": "https://arxiv.org/abs/2508.01710", "authors": ["Raviraj Joshi", "Rakesh Paul", "Kanishk Singla", "Anusha Kamath", "Michael Evans", "Katherine Luna", "Shaona Ghosh", "Utkarsh Vaidya", "Eileen Long", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "title": "CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The increasing use of Large Language Models (LLMs) in agentic applications\nhighlights the need for robust safety guard models. While content safety in\nEnglish is well-studied, non-English languages lack similar advancements due to\nthe high cost of collecting culturally aligned labeled datasets. We present\nCultureGuard, a novel solution for curating culturally aligned, high-quality\nsafety datasets across multiple languages. Our approach introduces a four-stage\nsynthetic data generation and filtering pipeline: cultural data segregation,\ncultural data adaptation, machine translation, and quality filtering. This\npipeline enables the conversion and expansion of the\nNemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct\nlanguages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese.\nThe resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1,\ncomprises 386,661 samples in 9 languages and facilitates the training of\nLlama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning.\nThe final model achieves state-of-the-art performance on several multilingual\ncontent safety benchmarks. We also benchmark the latest open LLMs on\nmultilingual safety and observe that these LLMs are more prone to give unsafe\nresponses when prompted in non-English languages. This work represents a\nsignificant step toward closing the safety gap in multilingual LLMs by enabling\nthe development of culturally aware safety guard models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCultureGuard\uff0c\u4e00\u79cd\u901a\u8fc7\u56db\u9636\u6bb5\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u8fc7\u6ee4\u6d41\u6c34\u7ebf\uff0c\u5c06\u82f1\u6587\u5185\u5bb9\u5b89\u5168\u6570\u636e\u96c6\u6269\u5c55\u5230\u516b\u79cd\u975e\u82f1\u8bed\u8bed\u8a00\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u521b\u5efa\u4e86\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u5b89\u5168\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u51faSOTA\u591a\u8bed\u8a00\u5b89\u5168\u6a21\u578b\uff0c\u89e3\u51b3\u4e86LLMs\u5728\u975e\u82f1\u8bed\u8bed\u5883\u4e0b\u5185\u5bb9\u5b89\u5168\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7406\u5e94\u7528\u4e2d\u7684\u65e5\u76ca\u666e\u53ca\uff0c\u5bf9\u5f3a\u5927\u7684\u5b89\u5168\u9632\u62a4\u6a21\u578b\u7684\u9700\u6c42\u589e\u52a0\u3002\u7136\u800c\uff0c\u975e\u82f1\u8bed\u8bed\u8a00\u7684\u5185\u5bb9\u5b89\u5168\u7814\u7a76\u8fdb\u5c55\u6ede\u540e\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u96be\u4ee5\u6536\u96c6\u5230\u6587\u5316\u5bf9\u9f50\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u6210\u672c\u9ad8\u6602\u3002\u6b64\u5916\uff0c\u73b0\u6709\u5f00\u653eLLMs\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u66f4\u5bb9\u6613\u4ea7\u751f\u4e0d\u5b89\u5168\u56de\u590d\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86CultureGuard\u65b9\u6848\uff0c\u7528\u4e8e\u7b56\u5212\u6587\u5316\u5bf9\u9f50\u3001\u9ad8\u8d28\u91cf\u7684\u591a\u8bed\u8a00\u5b89\u5168\u6570\u636e\u96c6\u3002\u5176\u6838\u5fc3\u662f\u4e00\u4e2a\u56db\u9636\u6bb5\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u8fc7\u6ee4\u6d41\u6c34\u7ebf\uff1a\u6587\u5316\u6570\u636e\u5206\u79bb\u3001\u6587\u5316\u6570\u636e\u9002\u5e94\u3001\u673a\u5668\u7ffb\u8bd1\u548c\u8d28\u91cf\u8fc7\u6ee4\u3002\u6b64\u6d41\u6c34\u7ebf\u5c06Nemotron-Content-Safety-Dataset-V2\u82f1\u6587\u6570\u636e\u96c6\u8f6c\u6362\u5e76\u6269\u5c55\u5230\u963f\u62c9\u4f2f\u8bed\u3001\u5fb7\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u6cd5\u8bed\u3001\u5370\u5730\u8bed\u3001\u65e5\u8bed\u3001\u6cf0\u8bed\u548c\u4e2d\u6587\u516b\u79cd\u8bed\u8a00\u3002\u5229\u7528\u751f\u6210\u7684Nemotron-Content-Safety-Dataset-Multilingual-v1\u6570\u636e\u96c6\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u8bad\u7ec3\u4e86Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1\u6a21\u578b\u3002", "result": "\u7814\u7a76\u6210\u679c\u5305\u62ec\uff1a1) \u6784\u5efa\u4e86Nemotron-Content-Safety-Dataset-Multilingual-v1\u6570\u636e\u96c6\uff0c\u5305\u542b9\u79cd\u8bed\u8a00\u5171386,661\u4e2a\u6837\u672c\u30022) \u8bad\u7ec3\u51fa\u7684Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1\u6a21\u578b\u5728\u591a\u4e2a\u591a\u8bed\u8a00\u5185\u5bb9\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u30023) \u5bf9\u6bd4\u6d4b\u8bd5\u53d1\u73b0\uff0c\u6700\u65b0\u7684\u5f00\u653eLLMs\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u63d0\u793a\u4e0b\u66f4\u5bb9\u6613\u7ed9\u51fa\u4e0d\u5b89\u5168\u7684\u56de\u590d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u4fc3\u8fdb\u6587\u5316\u611f\u77e5\u5b89\u5168\u9632\u62a4\u6a21\u578b\u7684\u5f00\u53d1\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u591a\u8bed\u8a00LLMs\u7684\u5b89\u5168\u5dee\u8ddd\uff0c\u662f\u89e3\u51b3\u975e\u82f1\u8bed\u5185\u5bb9\u5b89\u5168\u95ee\u9898\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2508.01002", "pdf": "https://arxiv.org/pdf/2508.01002", "abs": "https://arxiv.org/abs/2508.01002", "authors": ["Agrim Bari", "Parikshit Hegde", "Gustavo de Veciana"], "title": "Optimal Scheduling Algorithms for LLM Inference: Theory and Practice", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "With the growing use of Large Language Model (LLM)-based tools like ChatGPT,\nPerplexity, and Gemini across industries, there is a rising need for efficient\nLLM inference systems. These systems handle requests with a unique two-phase\ncomputation structure: a prefill-phase that processes the full input prompt and\na decode-phase that autoregressively generates tokens one at a time. This\nstructure calls for new strategies for routing and scheduling requests.\n  In this paper, we take a comprehensive approach to this challenge by\ndeveloping a theoretical framework that models routing and scheduling in LLM\ninference systems. We identify two key design principles-optimal tiling and\ndynamic resource allocation-that are essential for achieving high throughput.\nGuided by these principles, we propose the Resource-Aware Dynamic (RAD)\nscheduler and prove that it achieves throughput optimality under mild\nconditions. To address practical Service Level Objectives (SLOs) such as\nserving requests with different Time Between Token (TBT) constraints, we design\nthe SLO-Aware LLM Inference (SLAI) scheduler. SLAI uses real-time measurements\nto prioritize decode requests that are close to missing their TBT deadlines and\nreorders prefill requests based on known prompt lengths to further reduce the\nTime To First Token (TTFT) delays.\n  We evaluate SLAI on the Openchat ShareGPT4 dataset using the Mistral-7B model\non an NVIDIA RTX ADA 6000 GPU. Compared to Sarathi-Serve, SLAI reduces the\nmedian TTFT by 53% and increases the maximum serving capacity by 26% such that\nmedian TTFT is below 0.5 seconds, while meeting tail TBT latency constraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9488\u5bf9LLM\u63a8\u7406\u7cfb\u7edf\u72ec\u7279\u4e24\u9636\u6bb5\u7ed3\u6784\u7684RAD\u548cSLAI\u8c03\u5ea6\u5668\u3002SLAI\u5728\u5b9e\u8df5\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u9996\u6b21\u751f\u6210\u4ee4\u724c\u65f6\u95f4\uff08TTFT\uff09\u5e76\u63d0\u9ad8\u4e86\u670d\u52a1\u5bb9\u91cf\uff0c\u540c\u65f6\u6ee1\u8db3\u4e86\u5ef6\u8fdf\u7ea6\u675f\u3002", "motivation": "\u968f\u7740LLM\u5de5\u5177\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5bf9\u9ad8\u6548LLM\u63a8\u7406\u7cfb\u7edf\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002LLM\u63a8\u7406\u5177\u6709\u72ec\u7279\u7684\u9884\u586b\u5145\u548c\u89e3\u7801\u4e24\u9636\u6bb5\u8ba1\u7b97\u7ed3\u6784\uff0c\u8fd9\u9700\u8981\u65b0\u7684\u8bf7\u6c42\u8def\u7531\u548c\u8c03\u5ea6\u7b56\u7565\u6765\u63d0\u9ad8\u7cfb\u7edf\u6548\u7387\u3002", "method": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u4e00\u4e2aLLM\u63a8\u7406\u7cfb\u7edf\u8def\u7531\u548c\u8c03\u5ea6\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\u539f\u5219\uff1a\u6700\u4f18\u5206\u5757\u548c\u52a8\u6001\u8d44\u6e90\u5206\u914d\u3002\u57fa\u4e8e\u8fd9\u4e9b\u539f\u5219\uff0c\u8bbe\u8ba1\u4e86\u8d44\u6e90\u611f\u77e5\u52a8\u6001\uff08RAD\uff09\u8c03\u5ea6\u5668\uff08\u5e76\u8bc1\u660e\u5176\u541e\u5410\u91cf\u6700\u4f18\u6027\uff09\u3002\u4e3a\u6ee1\u8db3\u5b9e\u9645\u670d\u52a1\u6c34\u5e73\u76ee\u6807\uff08\u5982\u4ee4\u724c\u95f4\u65f6\u95f4TBT\u7ea6\u675f\uff09\uff0c\u8fdb\u4e00\u6b65\u8bbe\u8ba1\u4e86SLO\u611f\u77e5LLM\u63a8\u7406\uff08SLAI\uff09\u8c03\u5ea6\u5668\u3002SLAI\u5229\u7528\u5b9e\u65f6\u6d4b\u91cf\u4f18\u5148\u5904\u7406\u4e34\u8fd1TBT\u622a\u6b62\u671f\u7684\u89e3\u7801\u8bf7\u6c42\uff0c\u5e76\u6839\u636e\u63d0\u793a\u957f\u5ea6\u91cd\u65b0\u6392\u5e8f\u9884\u586b\u5145\u8bf7\u6c42\u4ee5\u51cf\u5c11TTFT\u5ef6\u8fdf\u3002", "result": "\u5728Mistral-7B\u6a21\u578b\u548cNVIDIA RTX ADA 6000 GPU\u4e0a\uff0c\u4e0eSarathi-Serve\u76f8\u6bd4\uff0cSLAI\u5c06\u4e2d\u4f4dTTFT\u964d\u4f4e\u4e8653%\uff0c\u5e76\u5c06\u6700\u5927\u670d\u52a1\u5bb9\u91cf\u63d0\u9ad8\u4e8626%\uff0c\u4f7f\u4e2d\u4f4dTTFT\u4f4e\u4e8e0.5\u79d2\uff0c\u540c\u65f6\u6ee1\u8db3\u5c3e\u90e8TBT\u5ef6\u8fdf\u7ea6\u675f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684SLAI\u8c03\u5ea6\u5668\u901a\u8fc7\u521b\u65b0\u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u7cfb\u7edf\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u9996\u6b21\u751f\u6210\u4ee4\u724c\u7684\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4e86\u670d\u52a1\u5bb9\u91cf\uff0c\u4e3aLLM\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01210", "pdf": "https://arxiv.org/pdf/2508.01210", "abs": "https://arxiv.org/abs/2508.01210", "authors": ["Tianze Wang", "Zhang Zhang", "Chao Yue", "Nuoran Li", "Chao Sun"], "title": "RoadMamba: A Dual Branch Visual State Space Model for Road Surface Classification", "categories": ["cs.CV"], "comment": null, "summary": "Acquiring the road surface conditions in advance based on visual technologies\nprovides effective information for the planning and control system of\nautonomous vehicles, thus improving the safety and driving comfort of the\nvehicles. Recently, the Mamba architecture based on state-space models has\nshown remarkable performance in visual processing tasks, benefiting from the\nefficient global receptive field. However, existing Mamba architectures\nstruggle to achieve state-of-the-art visual road surface classification due to\ntheir lack of effective extraction of the local texture of the road surface. In\nthis paper, we explore for the first time the potential of visual Mamba\narchitectures for road surface classification task and propose a method that\neffectively combines local and global perception, called RoadMamba.\nSpecifically, we utilize the Dual State Space Model (DualSSM) to effectively\nextract the global semantics and local texture of the road surface and decode\nand fuse the dual features through the Dual Attention Fusion (DAF). In\naddition, we propose a dual auxiliary loss to explicitly constrain dual\nbranches, preventing the network from relying only on global semantic\ninformation from the deep large receptive field and ignoring the local texture.\nThe proposed RoadMamba achieves the state-of-the-art performance in experiments\non a large-scale road surface classification dataset containing 1 million\nsamples.", "AI": {"tldr": "RoadMamba\u662f\u4e00\u79cd\u7ed3\u5408\u5c40\u90e8\u7eb9\u7406\u548c\u5168\u5c40\u8bed\u4e49\u7684Mamba\u67b6\u6784\uff0c\u901a\u8fc7DualSSM\u3001DAF\u548c\u53cc\u8f85\u52a9\u635f\u5931\uff0c\u5728\u8def\u9762\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u63d0\u524d\u83b7\u53d6\u8def\u9762\u72b6\u51b5\u4fe1\u606f\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\u3002\u7136\u800c\uff0c\u73b0\u6709Mamba\u67b6\u6784\u867d\u7136\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u6709\u6548\u63d0\u53d6\u8def\u9762\u5c40\u90e8\u7eb9\u7406\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u8def\u9762\u5206\u7c7b\u4efb\u52a1\u4e2d\u96be\u4ee5\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "method": "\u672c\u6587\u9996\u6b21\u63a2\u7d22\u89c6\u89c9Mamba\u67b6\u6784\u5728\u8def\u9762\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86RoadMamba\u65b9\u6cd5\u3002\u5177\u4f53\u5730\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u53cc\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08DualSSM\uff09\u6709\u6548\u63d0\u53d6\u8def\u9762\u5168\u5c40\u8bed\u4e49\u548c\u5c40\u90e8\u7eb9\u7406\uff0c\u5e76\u901a\u8fc7\u53cc\u6ce8\u610f\u529b\u878d\u5408\uff08DAF\uff09\u89e3\u7801\u5e76\u878d\u5408\u53cc\u91cd\u7279\u5f81\u3002\u6b64\u5916\uff0c\u5f15\u5165\u53cc\u8f85\u52a9\u635f\u5931\uff0c\u663e\u5f0f\u7ea6\u675f\u53cc\u5206\u652f\uff0c\u9632\u6b62\u7f51\u7edc\u4ec5\u4f9d\u8d56\u5168\u5c40\u8bed\u4e49\u4fe1\u606f\u800c\u5ffd\u7565\u5c40\u90e8\u7eb9\u7406\u3002", "result": "\u5728\u5305\u542b100\u4e07\u4e2a\u6837\u672c\u7684\u5927\u578b\u8def\u9762\u5206\u7c7b\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684RoadMamba\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5c06Mamba\u67b6\u6784\u5e94\u7528\u4e8e\u8def\u9762\u5206\u7c7b\u4efb\u52a1\uff0c\u901a\u8fc7\u6709\u6548\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u611f\u77e5\uff0c\u514b\u670d\u4e86\u73b0\u6709Mamba\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e2d\u5bf9\u5c40\u90e8\u7eb9\u7406\u63d0\u53d6\u4e0d\u8db3\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u7684\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2508.01495", "pdf": "https://arxiv.org/pdf/2508.01495", "abs": "https://arxiv.org/abs/2508.01495", "authors": ["Jingtian Yan", "Stephen F. Smith", "Jiaoyang Li"], "title": "WinkTPG: An Execution Framework for Multi-Agent Path Finding Using Temporal Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Planning collision-free paths for a large group of agents is a challenging\nproblem with numerous real-world applications. While recent advances in\nMulti-Agent Path Finding (MAPF) have shown promising progress, standard MAPF\nalgorithms rely on simplified kinodynamic models, preventing agents from\ndirectly following the generated MAPF plan. To bridge this gap, we propose\nkinodynamic Temporal Plan Graph Planning (kTPG), a multi-agent speed\noptimization algorithm that efficiently refines a MAPF plan into a\nkinodynamically feasible plan while accounting for uncertainties and preserving\ncollision-freeness. Building on kTPG, we propose Windowed kTPG (WinkTPG), a\nMAPF execution framework that incrementally refines MAPF plans using a\nwindow-based mechanism, dynamically incorporating agent information during\nexecution to reduce uncertainty. Experiments show that WinkTPG can generate\nspeed profiles for up to 1,000 agents in 1 second and improves solution quality\nby up to 51.7% over existing MAPF execution methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86kTPG\u548cWinkTPG\u4e24\u79cd\u7b97\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212(MAPF)\u4e2d\u8fd0\u52a8\u5b66\u7ea6\u675f\u7b80\u5316\u4e0e\u5b9e\u9645\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u7684\u78b0\u649e\u89c4\u907f\u548c\u8fd0\u52a8\u5b66\u53ef\u884c\u8def\u5f84\u3002", "motivation": "\u4e3a\u5927\u91cf\u667a\u80fd\u4f53\u89c4\u5212\u65e0\u78b0\u649e\u8def\u5f84\u662f\u4e00\u4e2a\u6311\u6218\u3002\u73b0\u6709MAPF\u7b97\u6cd5\u4f9d\u8d56\u7b80\u5316\u7684\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u8def\u5f84\u65e0\u6cd5\u76f4\u63a5\u5728\u5b9e\u9645\u4e2d\u6267\u884c\uff0c\u5b58\u5728\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u65b9\u9762\u7684\u5dee\u8ddd\u3002", "method": "1. \u63d0\u51fakTPG\uff08\u8fd0\u52a8\u5b66\u65f6\u95f4\u89c4\u5212\u56fe\u89c4\u5212\uff09\u7b97\u6cd5\uff0c\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u901f\u5ea6\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u5c06MAPF\u8ba1\u5212\u7ec6\u5316\u4e3a\u8fd0\u52a8\u5b66\u53ef\u884c\u4e14\u65e0\u78b0\u649e\u7684\u8ba1\u5212\uff0c\u5e76\u8003\u8651\u4e0d\u786e\u5b9a\u6027\u30022. \u5728kTPG\u57fa\u7840\u4e0a\uff0c\u63d0\u51faWinkTPG\uff08\u7a97\u53e3\u5316kTPG\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u7a97\u53e3\u7684\u673a\u5236\u589e\u91cf\u7ec6\u5316MAPF\u8ba1\u5212\uff0c\u5e76\u5728\u6267\u884c\u671f\u95f4\u52a8\u6001\u6574\u5408\u667a\u80fd\u4f53\u4fe1\u606f\u4ee5\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u3002", "result": "WinkTPG\u80fd\u57281\u79d2\u5185\u4e3a\u591a\u8fbe1,000\u4e2a\u667a\u80fd\u4f53\u751f\u6210\u901f\u5ea6\u5256\u9762\u3002\u76f8\u8f83\u4e8e\u73b0\u6709MAPF\u6267\u884c\u65b9\u6cd5\uff0c\u5176\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u63d0\u5347\u9ad8\u8fbe51.7%\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684kTPG\u548cWinkTPG\u7b97\u6cd5\u6709\u6548\u5f25\u5408\u4e86MAPF\u7406\u8bba\u89c4\u5212\u4e0e\u5b9e\u9645\u8fd0\u52a8\u5b66\u6267\u884c\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u5927\u89c4\u6a21\u667a\u80fd\u4f53\u7fa4\u4f53\u7684\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u5feb\u901f\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01739", "pdf": "https://arxiv.org/pdf/2508.01739", "abs": "https://arxiv.org/abs/2508.01739", "authors": ["Cheng Wang", "ziru Liu", "Pengcheng Tang", "Mingyu Zhang", "Quanyu Dai", "Yue Zhu"], "title": "Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction", "categories": ["cs.CL"], "comment": null, "summary": "Identifying user preferences in dialogue systems is a pivotal aspect of\nproviding satisfying services. Current research shows that using large language\nmodels (LLMs) to fine-tune a task-specific preference extractor yields\nexcellent results in terms of accuracy and generalization. However, the primary\nchallenge stems from the inherent difficulty in obtaining high-quality labeled\nmulti-turn dialogue data. Accurately tracking user preference transitions\nacross turns not only demands intensive domain expertise and contextual\nconsistency maintenance for annotators (termed \\textbf{``Annotating\nDisaster''}) but also complicates model training due to error propagation in\nsequential dependency learning. Inspired by the observation that multi-turn\npreference extraction can be decomposed into iterative executions of one-turn\nextraction processes. We propose a novel dialogue data generation framework\nnamed \\textbf{IterChat}. First, we construct a new data format that categorizes\nthe dialogue data into attributed historical preferences and one-turn\ndialogues. This reduces the probability of annotation errors and improves\nannotation efficiency. Then, to generate a high-quality and diverse dialogue\ndataset, we adopt GPT4 to pre-define the preference slots in the target\npreference extractor task and then randomly sample the subset of the slots and\ntheir corresponding schema values to create the dialogue datasets. Experimental\nresults indicate that fine-tuning or only few-shot prompting with the new\ndialogue format yields superior performance compared to the original multi-turn\ndialogues. Additionally, the new data format improves annotator efficiency with\na win rate of 28.4\\% higher than the original multi-turn dialogues.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIterChat\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u8f6e\u504f\u597d\u63d0\u53d6\u5206\u89e3\u4e3a\u5355\u8f6e\u63d0\u53d6\u5e76\u8bbe\u8ba1\u65b0\u7684\u6570\u636e\u683c\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u8d28\u91cf\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u6807\u6ce8\u56f0\u96be\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u6807\u6ce8\u6548\u7387\u3002", "motivation": "\u5728\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\uff0c\u8bc6\u522b\u7528\u6237\u504f\u597d\u5bf9\u4e8e\u63d0\u4f9b\u6ee1\u610f\u670d\u52a1\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136LLMs\u5728\u504f\u597d\u63d0\u53d6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u83b7\u53d6\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u6781\u5177\u6311\u6218\u6027\uff08\u201c\u6807\u6ce8\u707e\u96be\u201d\uff09\uff0c\u539f\u56e0\u5728\u4e8e\u6807\u6ce8\u8005\u9700\u8981\u6df1\u539a\u7684\u9886\u57df\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u7ef4\u62a4\uff0c\u4e14\u6a21\u578b\u8bad\u7ec3\u4e2d\u5b58\u5728\u5e8f\u8d2f\u4f9d\u8d56\u5b66\u4e60\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\u95ee\u9898\u3002", "method": "\u53d7\u591a\u8f6e\u504f\u597d\u63d0\u53d6\u53ef\u5206\u89e3\u4e3a\u5355\u8f6e\u63d0\u53d6\u7684\u542f\u53d1\uff0c\u672c\u6587\u63d0\u51fa\u4e86IterChat\u5bf9\u8bdd\u6570\u636e\u751f\u6210\u6846\u67b6\u3002\u9996\u5148\uff0c\u6784\u5efa\u4e00\u79cd\u65b0\u7684\u6570\u636e\u683c\u5f0f\uff0c\u5c06\u5bf9\u8bdd\u6570\u636e\u5206\u4e3a\u5c5e\u6027\u5316\u7684\u5386\u53f2\u504f\u597d\u548c\u5355\u8f6e\u5bf9\u8bdd\uff0c\u65e8\u5728\u51cf\u5c11\u6807\u6ce8\u9519\u8bef\u548c\u63d0\u9ad8\u6548\u7387\u3002\u5176\u6b21\uff0c\u5229\u7528GPT-4\u9884\u5b9a\u4e49\u76ee\u6807\u504f\u597d\u63d0\u53d6\u4efb\u52a1\u7684\u504f\u597d\u69fd\u4f4d\uff0c\u5e76\u968f\u673a\u62bd\u6837\u69fd\u4f4d\u53ca\u5176\u5bf9\u5e94\u7684\u6a21\u5f0f\u503c\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u591a\u6837\u5316\u7684\u5bf9\u8bdd\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u65b0\u5bf9\u8bdd\u683c\u5f0f\u8fdb\u884c\u5fae\u8c03\u6216\u5c11\u91cf\u6837\u672c\u63d0\u793a\uff0c\u6027\u80fd\u4f18\u4e8e\u539f\u59cb\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u3002\u6b64\u5916\uff0c\u65b0\u6570\u636e\u683c\u5f0f\u5c06\u6807\u6ce8\u6548\u7387\u63d0\u5347\u4e8628.4%\u3002", "conclusion": "IterChat\u6846\u67b6\u901a\u8fc7\u7b80\u5316\u6570\u636e\u683c\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8f6e\u5bf9\u8bdd\u504f\u597d\u6570\u636e\u6807\u6ce8\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u7528\u6237\u504f\u597d\u63d0\u53d6\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6807\u6ce8\u6548\u7387\uff0c\u4e3a\u5bf9\u8bdd\u7cfb\u7edf\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.01010", "pdf": "https://arxiv.org/pdf/2508.01010", "abs": "https://arxiv.org/abs/2508.01010", "authors": ["Gnankan Landry Regis N'guessan"], "title": "v-PuNNs: van der Put Neural Networks for Transparent Ultrametric Representation Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Conventional deep learning models embed data in Euclidean space\n$\\mathbb{R}^d$, a poor fit for strictly hierarchical objects such as taxa, word\nsenses, or file systems. We introduce van der Put Neural Networks (v-PuNNs),\nthe first architecture whose neurons are characteristic functions of p-adic\nballs in $\\mathbb{Z}_p$. Under our Transparent Ultrametric Representation\nLearning (TURL) principle every weight is itself a p-adic number, giving exact\nsubtree semantics. A new Finite Hierarchical Approximation Theorem shows that a\ndepth-K v-PuNN with $\\sum_{j=0}^{K-1}p^{\\,j}$ neurons universally represents\nany K-level tree. Because gradients vanish in this discrete space, we propose\nValuation-Adaptive Perturbation Optimization (VAPO), with a fast deterministic\nvariant (HiPaN-DS) and a moment-based one (HiPaN / Adam-VAPO). On three\ncanonical benchmarks our CPU-only implementation sets new state-of-the-art:\nWordNet nouns (52,427 leaves) 99.96% leaf accuracy in 16 min; GO\nmolecular-function 96.9% leaf / 100% root in 50 s; NCBI Mammalia Spearman $\\rho\n= -0.96$ with true taxonomic distance. The learned metric is perfectly\nultrametric (zero triangle violations), and its fractal and\ninformation-theoretic properties are analyzed. Beyond classification we derive\nstructural invariants for quantum systems (HiPaQ) and controllable generative\ncodes for tabular data (Tab-HiPaN). v-PuNNs therefore bridge number theory and\ndeep learning, offering exact, interpretable, and efficient models for\nhierarchical data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3av-PuNNs\u7684\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5176\u57fa\u4e8ep-adic\u6570\uff0c\u80fd\u7cbe\u786e\u4e14\u9ad8\u6548\u5730\u5904\u7406\u5c42\u7ea7\u6570\u636e\u3002\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u80fd\u5b66\u4e60\u5230\u5b8c\u7f8e\u7684\u8d85\u5ea6\u91cf\u3002", "motivation": "\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5c06\u6570\u636e\u5d4c\u5165\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u4e0d\u9002\u7528\u4e8e\u4e25\u683c\u7684\u5c42\u7ea7\u5bf9\u8c61\uff0c\u5982\u5206\u7c7b\u5b66\u3001\u8bcd\u4e49\u6216\u6587\u4ef6\u7cfb\u7edf\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5c42\u7ea7\u6570\u636e\u5904\u7406\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165van der Put\u795e\u7ecf\u7f51\u7edc\uff08v-PuNNs\uff09\uff0c\u5176\u795e\u7ecf\u5143\u662fp-adic\u6570\u7a7a\u95f4\u4e2d\u7684p-adic\u7403\u7684\u7279\u5f81\u51fd\u6570\u3002\u57fa\u4e8e\u900f\u660e\u8d85\u5ea6\u91cf\u8868\u793a\u5b66\u4e60\uff08TURL\uff09\u539f\u5219\uff0c\u6bcf\u4e2a\u6743\u91cd\u672c\u8eab\u90fd\u662f\u4e00\u4e2ap-adic\u6570\uff0c\u4ee5\u63d0\u4f9b\u7cbe\u786e\u7684\u5b50\u6811\u8bed\u4e49\u3002\u4e3a\u89e3\u51b3\u79bb\u6563\u7a7a\u95f4\u4e2d\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4f30\u503c\u81ea\u9002\u5e94\u6270\u52a8\u4f18\u5316\uff08VAPO\uff09\u7b97\u6cd5\u53ca\u5176\u53d8\u4f53\uff08HiPaN-DS\u3001HiPaN/Adam-VAPO\uff09\u3002\u7406\u8bba\u4e0a\uff0c\u901a\u8fc7\u6709\u9650\u5c42\u7ea7\u8fd1\u4f3c\u5b9a\u7406\u8bc1\u660e\u4e86v-PuNN\u80fd\u666e\u904d\u8868\u793a\u4efb\u4f55K\u5c42\u6811\u3002", "result": "\u5728\u4e09\u4e2a\u7ecf\u5178\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8eCPU\u7684v-PuNN\u5b9e\u73b0\u5747\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff1aWordNet\u540d\u8bcd\u6570\u636e\u96c6\u4e0a\u53f6\u5b50\u51c6\u786e\u7387\u8fbe99.96%\uff1bGO\u5206\u5b50\u529f\u80fd\u6570\u636e\u96c6\u4e0a\u53f6\u5b50\u51c6\u786e\u738796.9%\uff0c\u6839\u8282\u70b9\u51c6\u786e\u7387100%\uff1bNCBI\u54fa\u4e73\u52a8\u7269\u6570\u636e\u96c6\u4e0e\u771f\u5b9e\u5206\u7c7b\u8ddd\u79bb\u7684Spearman $\rho$ \u8fbe\u5230-0.96\u3002\u5b66\u4e60\u5230\u7684\u5ea6\u91cf\u662f\u5b8c\u7f8e\u7684\u8d85\u5ea6\u91cf\uff08\u96f6\u4e09\u89d2\u4e0d\u7b49\u5f0f\u8fdd\u89c4\uff09\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5206\u5f62\u548c\u4fe1\u606f\u7406\u8bba\u7279\u6027\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u884d\u751f\u51fa\u91cf\u5b50\u7cfb\u7edf\u7684\u7ed3\u6784\u4e0d\u53d8\u91cf\uff08HiPaQ\uff09\u548c\u8868\u683c\u6570\u636e\u7684\u53ef\u63a7\u751f\u6210\u4ee3\u7801\uff08Tab-HiPaN\uff09\u3002", "conclusion": "v-PuNNs\u6210\u529f\u5730\u5728\u6570\u8bba\u548c\u6df1\u5ea6\u5b66\u4e60\u4e4b\u95f4\u67b6\u8d77\u4e86\u4e00\u5ea7\u6865\u6881\uff0c\u4e3a\u5c42\u7ea7\u6570\u636e\u63d0\u4f9b\u4e86\u7cbe\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u5efa\u6a21\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u591a\u9886\u57df\u5e7f\u9614\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.01215", "pdf": "https://arxiv.org/pdf/2508.01215", "abs": "https://arxiv.org/abs/2508.01215", "authors": ["Yuanlin Yang", "Quanjian Song", "Zhexian Gao", "Ge Wang", "Shanshan Li", "Xiaoyan Zhang"], "title": "StyDeco: Unsupervised Style Transfer with Distilling Priors and Semantic Decoupling", "categories": ["cs.CV"], "comment": "9 pages in total", "summary": "Diffusion models have emerged as the dominant paradigm for style transfer,\nbut their text-driven mechanism is hindered by a core limitation: it treats\ntextual descriptions as uniform, monolithic guidance. This limitation overlooks\nthe semantic gap between the non-spatial nature of textual descriptions and the\nspatially-aware attributes of visual style, often leading to the loss of\nsemantic structure and fine-grained details during stylization. In this paper,\nwe propose StyDeco, an unsupervised framework that resolves this limitation by\nlearning text representations specifically tailored for the style transfer\ntask. Our framework first employs Prior-Guided Data Distillation (PGD), a\nstrategy designed to distill stylistic knowledge without human supervision. It\nleverages a powerful frozen generative model to automatically synthesize\npseudo-paired data. Subsequently, we introduce Contrastive Semantic Decoupling\n(CSD), a task-specific objective that adapts a text encoder using\ndomain-specific weights. CSD performs a two-class clustering in the semantic\nspace, encouraging source and target representations to form distinct clusters.\nExtensive experiments on three classic benchmarks demonstrate that our\nframework outperforms several existing approaches in both stylistic fidelity\nand structural preservation, highlighting its effectiveness in style transfer\nwith semantic preservation. In addition, our framework supports a unique\nde-stylization process, further demonstrating its extensibility. Our code is\nvailable at https://github.com/QuanjianSong/StyDeco.", "AI": {"tldr": "StyDeco\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u4e13\u95e8\u7528\u4e8e\u98ce\u683c\u8fc1\u79fb\u7684\u6587\u672c\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u9a71\u52a8\u98ce\u683c\u8fc1\u79fb\u4e2d\u8bed\u4e49\u7ed3\u6784\u548c\u7ec6\u8282\u4e22\u5931\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u98ce\u683c\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u4fdd\u7559\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u9a71\u52a8\u7684\u98ce\u683c\u8fc1\u79fb\u4e2d\uff0c\u5c06\u6587\u672c\u63cf\u8ff0\u89c6\u4e3a\u7edf\u4e00\u7684\u6307\u5bfc\uff0c\u5ffd\u7565\u4e86\u6587\u672c\u975e\u7a7a\u95f4\u6027\u4e0e\u89c6\u89c9\u98ce\u683c\u7a7a\u95f4\u5c5e\u6027\u4e4b\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u5bfc\u81f4\u98ce\u683c\u5316\u8fc7\u7a0b\u4e2d\u8bed\u4e49\u7ed3\u6784\u548c\u7cbe\u7ec6\u7ec6\u8282\u7684\u4e22\u5931\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86StyDeco\u6846\u67b6\u3002\u9996\u5148\uff0c\u91c7\u7528\u201c\u5148\u9a8c\u5f15\u5bfc\u6570\u636e\u84b8\u998f\u201d\uff08Prior-Guided Data Distillation, PGD\uff09\u7b56\u7565\uff0c\u5229\u7528\u51bb\u7ed3\u7684\u751f\u6210\u6a21\u578b\u81ea\u52a8\u5408\u6210\u4f2a\u914d\u5bf9\u6570\u636e\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u5730\u84b8\u998f\u98ce\u683c\u77e5\u8bc6\u3002\u5176\u6b21\uff0c\u5f15\u5165\u201c\u5bf9\u6bd4\u8bed\u4e49\u89e3\u8026\u201d\uff08Contrastive Semantic Decoupling, CSD\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u4efb\u52a1\u7279\u5b9a\u7684\u76ee\u6807\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u6743\u91cd\u8c03\u6574\u6587\u672c\u7f16\u7801\u5668\uff0c\u5e76\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u8fdb\u884c\u4e24\u7c7b\u805a\u7c7b\uff0c\u4f7f\u6e90\u548c\u76ee\u6807\u8868\u793a\u5f62\u6210\u72ec\u7acb\u7684\u805a\u7c7b\u3002", "result": "\u5728\u4e09\u4e2a\u7ecf\u5178\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cStyDeco\u6846\u67b6\u5728\u98ce\u683c\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u4fdd\u7559\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u8fd8\u652f\u6301\u72ec\u7279\u7684\u53bb\u98ce\u683c\u5316\u8fc7\u7a0b\uff0c\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "StyDeco\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u6587\u672c\u8bed\u4e49\u8868\u793a\u4e0e\u89c6\u89c9\u98ce\u683c\u5339\u914d\u7684\u6311\u6218\uff0c\u6709\u6548\u5730\u5b9e\u73b0\u4e86\u8bed\u4e49\u4fdd\u7559\u7684\u98ce\u683c\u8fc1\u79fb\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.01543", "pdf": "https://arxiv.org/pdf/2508.01543", "abs": "https://arxiv.org/abs/2508.01543", "authors": ["Derin Cayir", "Renjie Tao", "Rashi Rungta", "Kai Sun", "Sean Chen", "Haidar Khan", "Minseok Kim", "Julia Reinspach", "Yue Liu"], "title": "Refine-n-Judge: Curating High-Quality Preference Chains for LLM-Fine-Tuning", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable progress through\npreference-based fine-tuning, which critically depends on the quality of the\nunderlying training data. While human feedback is essential for improving data\nquality, it is costly and does not scale well. In this paper, we introduce\nRefine-n-Judge, an automated iterative approach that leverages a single LLM as\nboth a refiner and a judge to enhance dataset quality. Unlike existing\niterative refinement methods, Refine-n-Judge employs an LLM to both generate\nrefinements and explicitly evaluate each improvement, ensuring that every\niteration meaningfully enhances the dataset without requiring additional human\nannotation or a separate reward model. At each step, the LLM refines a response\nand judges whether the refinement is an improvement over the previous answer.\nThis process continues until the LLM prefers the initial answer over the\nrefinement, indicating no further improvements. This produces sequences of\nincreasing quality, preference-labeled responses ideal for fine-tuning.\n  We demonstrate the effectiveness of Refine-n-Judge across a range of public\ndatasets spanning five corpora, targeting tasks such as coding, math, and\nconversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on\nRefine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of\ncomparisons against models tuned on the original dataset by GPT-4.\nAdditionally, we report performance gains: +5% on AlpacaEval and AlpacaEval\n2.0, and +19% on MT-Bench. Our results indicate that Refine-n-Judge produces\nhigh-quality datasets and scalable model improvements.", "AI": {"tldr": "Refine-n-Judge\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u8fed\u4ee3\u65b9\u6cd5\uff0c\u5229\u7528\u5355\u4e2aLLM\u540c\u65f6\u4f5c\u4e3a\u6539\u8fdb\u5668\u548c\u8bc4\u4f30\u5668\u6765\u63d0\u5347\u6570\u636e\u96c6\u8d28\u91cf\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3002\u5176\u751f\u6210\u7684\u589e\u5f3a\u6570\u636e\u96c6\u80fd\u663e\u8457\u63d0\u5347\u5fae\u8c03\u6a21\u578b\u5728\u7f16\u7801\u3001\u6570\u5b66\u548c\u5bf9\u8bdd\u7b49\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8fdb\u6b65\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u7684\u504f\u597d\u5fae\u8c03\u6570\u636e\uff0c\u4f46\u4eba\u5de5\u53cd\u9988\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Refine-n-Judge\uff0c\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u8fed\u4ee3\u65b9\u6cd5\u3002\u5b83\u5229\u7528\u5355\u4e2aLLM\u540c\u65f6\u4f5c\u4e3a\u201c\u6539\u8fdb\u5668\u201d\uff08\u751f\u6210\u54cd\u5e94\u7684\u6539\u8fdb\u7248\u672c\uff09\u548c\u201c\u8bc4\u4f30\u5668\u201d\uff08\u5224\u65ad\u6539\u8fdb\u662f\u5426\u4f18\u4e8e\u524d\u4e00\u4e2a\u7248\u672c\uff09\u3002LLM\u4f1a\u6301\u7eed\u4f18\u5316\u54cd\u5e94\u5e76\u8fdb\u884c\u8bc4\u4f30\uff0c\u76f4\u5230\u5176\u504f\u597d\u539f\u59cb\u7b54\u6848\uff0c\u8868\u660e\u65e0\u6cd5\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002\u8fd9\u4f1a\u751f\u6210\u4e00\u7cfb\u5217\u8d28\u91cf\u9012\u589e\u3001\u5e26\u6709\u504f\u597d\u6807\u7b7e\u7684\u54cd\u5e94\uff0c\u65e0\u9700\u989d\u5916\u7684\u4eba\u5de5\u6807\u6ce8\u6216\u5355\u72ec\u7684\u5956\u52b1\u6a21\u578b\u3002", "result": "1. \u7ecfRefine-n-Judge\u589e\u5f3a\u6570\u636e\u96c6\u5fae\u8c03\u7684\u6a21\u578b\uff08Llama 3.1-8B\u548cLlama 3.3-70B\uff09\u5728\u4e0e\u539f\u59cb\u6570\u636e\u96c6\u5fae\u8c03\u6a21\u578b\uff08\u7531GPT-4\u8bc4\u4f30\uff09\u7684\u6bd4\u8f83\u4e2d\uff0cLLM\u8bc4\u4f30\u5668\u504f\u597d\u7387\u8d85\u8fc774%\u30022. \u5728AlpacaEval\u548cAlpacaEval 2.0\u4e0a\u6027\u80fd\u63d0\u53475%\uff0c\u5728MT-Bench\u4e0a\u6027\u80fd\u63d0\u534719%\u3002", "conclusion": "Refine-n-Judge\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.01754", "pdf": "https://arxiv.org/pdf/2508.01754", "abs": "https://arxiv.org/abs/2508.01754", "authors": ["Alva West", "Yixuan Weng", "Minjun Zhu", "Luodan Zhang", "Zhen Lin", "Guangsheng Bao", "Yue Zhang"], "title": "AI-Generated Text is Non-Stationary: Detection via Temporal Tomography", "categories": ["cs.CL"], "comment": null, "summary": "The field of AI-generated text detection has evolved from supervised\nclassification to zero-shot statistical analysis. However, current approaches\nshare a fundamental limitation: they aggregate token-level measurements into\nscalar scores, discarding positional information about where anomalies occur.\nOur empirical analysis reveals that AI-generated text exhibits significant\nnon-stationarity, statistical properties vary by 73.8\\% more between text\nsegments compared to human writing. This discovery explains why existing\ndetectors fail against localized adversarial perturbations that exploit this\noverlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT),\na novel detection paradigm that preserves positional information by\nreformulating detection as a signal processing task. TDT treats token-level\ndiscrepancies as a time-series signal and applies Continuous Wavelet Transform\nto generate a two-dimensional time-scale representation, capturing both the\nlocation and linguistic scale of statistical anomalies. On the RAID benchmark,\nTDT achieves 0.855 AUROC (7.1\\% improvement over the best baseline). More\nimportantly, TDT demonstrates robust performance on adversarial tasks, with\n14.1\\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its\nsophisticated analysis, TDT maintains practical efficiency with only 13\\%\ncomputational overhead. Our work establishes non-stationarity as a fundamental\ncharacteristic of AI-generated text and demonstrates that preserving temporal\ndynamics is essential for robust detection.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793aAI\u751f\u6210\u6587\u672c\u7684\u975e\u5e73\u7a33\u6027\u662f\u5176\u57fa\u672c\u7279\u5f81\uff0c\u73b0\u6709\u68c0\u6d4b\u5668\u56e0\u5ffd\u7565\u4f4d\u7f6e\u4fe1\u606f\u800c\u5931\u6548\u3002\u4e3a\u6b64\uff0c\u63d0\u51faTDT\uff08Temporal Discrepancy Tomography\uff09\u65b9\u6cd5\uff0c\u5c06\u68c0\u6d4b\u91cd\u6784\u4e3a\u4fe1\u53f7\u5904\u7406\u4efb\u52a1\uff0c\u901a\u8fc7\u4fdd\u7559\u65f6\u95f4\u52a8\u6001\u6027\u663e\u8457\u63d0\u5347\u4e86\u5bf9AI\u6587\u672c\u7684\u9c81\u68d2\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u5f53\u524dAI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u5c06\u8bcd\u5143\u7ea7\u6d4b\u91cf\u805a\u5408\u4e3a\u6807\u91cf\u5206\u6570\uff0c\u4e22\u5931\u4e86\u4f4d\u7f6e\u4fe1\u606f\u3002\u5b9e\u8bc1\u5206\u6790\u53d1\u73b0\uff0cAI\u751f\u6210\u6587\u672c\u5177\u6709\u663e\u8457\u7684\u975e\u5e73\u7a33\u6027\uff08\u7edf\u8ba1\u7279\u6027\u5728\u4e0d\u540c\u6587\u672c\u6bb5\u4e4b\u95f4\u53d8\u5316\u5927\uff09\uff0c\u6bd4\u4eba\u7c7b\u5199\u4f5c\u591a73.8%\u3002\u73b0\u6709\u68c0\u6d4b\u5668\u56e0\u6b64\u5728\u5229\u7528\u6b64\u7279\u6027\u7684\u5c40\u90e8\u5bf9\u6297\u6027\u6270\u52a8\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f15\u5165\u65f6\u95f4\u5dee\u5f02\u65ad\u5c42\u626b\u63cf\uff08TDT\uff09\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u68c0\u6d4b\u8303\u5f0f\u3002TDT\u5c06\u68c0\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4fe1\u53f7\u5904\u7406\u4efb\u52a1\uff0c\u5c06\u8bcd\u5143\u7ea7\u5dee\u5f02\u89c6\u4e3a\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\uff0c\u5e76\u5e94\u7528\u8fde\u7eed\u5c0f\u6ce2\u53d8\u6362\u751f\u6210\u4e8c\u7ef4\u65f6\u5c3a\u5ea6\u8868\u793a\uff0c\u4ee5\u6355\u6349\u7edf\u8ba1\u5f02\u5e38\u7684\u4f4d\u7f6e\u548c\u8bed\u8a00\u5c3a\u5ea6\u4fe1\u606f\u3002", "result": "TDT\u5728RAID\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5b9e\u73b0\u4e860.855\u7684AUROC\uff0c\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u9ad8\u4e867.1%\u3002\u5728\u5bf9\u6297\u6027\u4efb\u52a1\u4e2d\uff0c\u5c24\u5176\u662f\u5728HART Level 2\u91ca\u4e49\u653b\u51fb\u4e0a\uff0cAUROC\u63d0\u9ad8\u4e8614.1%\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002\u5c3d\u7ba1\u5206\u6790\u590d\u6742\uff0c\u4f46TDT\u7684\u8ba1\u7b97\u5f00\u9500\u4ec5\u589e\u52a013%\uff0c\u4fdd\u6301\u4e86\u5b9e\u7528\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u786e\u7acb\u4e86\u975e\u5e73\u7a33\u6027\u662fAI\u751f\u6210\u6587\u672c\u7684\u57fa\u672c\u7279\u5f81\uff0c\u5e76\u8bc1\u660e\u4e86\u4fdd\u7559\u65f6\u95f4\u52a8\u6001\u6027\u5bf9\u4e8e\u5b9e\u73b0\u9c81\u68d2\u7684AI\u6587\u672c\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\u3002TDT\u65b9\u6cd5\u7684\u6709\u6548\u6027\u8bc1\u5b9e\u4e86\u8fd9\u4e00\u89c2\u70b9\u3002"}}
{"id": "2508.01013", "pdf": "https://arxiv.org/pdf/2508.01013", "abs": "https://arxiv.org/abs/2508.01013", "authors": ["Arjun Manoj", "Anastasia S. Georgiou", "Dimitris G. Giovanis", "Themistoklis P. Sapsis", "Ioannis G. Kevrekidis"], "title": "On Some Tunable Multi-fidelity Bayesian Optimization Frameworks", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "Multi-fidelity optimization employs surrogate models that integrate\ninformation from varying levels of fidelity to guide efficient exploration of\ncomplex design spaces while minimizing the reliance on (expensive)\nhigh-fidelity objective function evaluations. To advance Gaussian Process\n(GP)-based multi-fidelity optimization, we implement a proximity-based\nacquisition strategy that simplifies fidelity selection by eliminating the need\nfor separate acquisition functions at each fidelity level. We also enable\nmulti-fidelity Upper Confidence Bound (UCB) strategies by combining them with\nmulti-fidelity GPs rather than the standard GPs typically used. We benchmark\nthese approaches alongside other multi-fidelity acquisition strategies\n(including fidelity-weighted approaches) comparing their performance, reliance\non high-fidelity evaluations, and hyperparameter tunability in representative\noptimization tasks. The results highlight the capability of the proximity-based\nmulti-fidelity acquisition function to deliver consistent control over\nhigh-fidelity usage while maintaining convergence efficiency. Our illustrative\nexamples include multi-fidelity chemical kinetic models, both homogeneous and\nheterogeneous (dynamic catalysis for ammonia production).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u90bb\u8fd1\u5ea6\u7684\u65b0\u578b\u91c7\u96c6\u7b56\u7565\u548c\u6539\u8fdb\u7684\u591a\u7f6e\u4fe1\u5ea6UCB\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u591a\u7f6e\u4fe1\u5ea6\u4f18\u5316\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63a7\u5236\u9ad8\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u6210\u672c\u5e76\u4fdd\u6301\u6536\u655b\u6548\u7387\u3002", "motivation": "\u4e3a\u63a8\u8fdb\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u7684\u591a\u7f6e\u4fe1\u5ea6\u4f18\u5316\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u9ad8\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u6602\u8d35\u4ee5\u53ca\u5404\u7f6e\u4fe1\u5ea6\u7ea7\u522b\u9700\u72ec\u7acb\u91c7\u96c6\u51fd\u6570\u5bfc\u81f4\u9009\u62e9\u590d\u6742\u7684\u95ee\u9898\u3002", "method": "1. \u5b9e\u65bd\u4e86\u4e00\u79cd\u57fa\u4e8e\u90bb\u8fd1\u5ea6\u7684\u91c7\u96c6\u7b56\u7565\uff0c\u7b80\u5316\u4e86\u7f6e\u4fe1\u5ea6\u9009\u62e9\u3002 2. \u5c06\u591a\u7f6e\u4fe1\u5ea6UCB\u7b56\u7565\u4e0e\u591a\u7f6e\u4fe1\u5ea6GP\u7ed3\u5408\u3002 3. \u5728\u4ee3\u8868\u6027\u4f18\u5316\u4efb\u52a1\u4e2d\uff0c\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u4ee5\u53ca\u5176\u4ed6\u591a\u7f6e\u4fe1\u5ea6\u91c7\u96c6\u7b56\u7565\uff08\u5305\u62ec\u7f6e\u4fe1\u5ea6\u52a0\u6743\u65b9\u6cd5\uff09\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u4e86\u5b83\u4eec\u7684\u6027\u80fd\u3001\u5bf9\u9ad8\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u7684\u4f9d\u8d56\u6027\u4ee5\u53ca\u8d85\u53c2\u6570\u53ef\u8c03\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u8868\u660e\uff0c\u57fa\u4e8e\u90bb\u8fd1\u5ea6\u7684\u591a\u7f6e\u4fe1\u5ea6\u91c7\u96c6\u51fd\u6570\u80fd\u591f\u5bf9\u9ad8\u7f6e\u4fe1\u5ea6\u4f7f\u7528\u91cf\u8fdb\u884c\u6301\u7eed\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6536\u655b\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u90bb\u8fd1\u5ea6\u7684\u91c7\u96c6\u7b56\u7565\u4e3a\u591a\u7f6e\u4fe1\u5ea6\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\uff0c\u6210\u529f\u5e73\u8861\u4e86\u9ad8\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u6210\u672c\u4e0e\u4f18\u5316\u6548\u7387\uff0c\u4f7f\u5f97\u57fa\u4e8eGP\u7684\u591a\u7f6e\u4fe1\u5ea6\u4f18\u5316\u66f4\u52a0\u5b9e\u7528\u548c\u9ad8\u6548\u3002"}}
{"id": "2508.01216", "pdf": "https://arxiv.org/pdf/2508.01216", "abs": "https://arxiv.org/abs/2508.01216", "authors": ["Bolei Chen", "Shengsheng Yan", "Yongzheng Cui", "Jiaxu Kang", "Ping Zhong", "Jianxin Wang"], "title": "Perspective from a Broader Context: Can Room Style Knowledge Help Visual Floorplan Localization?", "categories": ["cs.CV", "cs.RO"], "comment": "Submitted to AAAI 2026. arXiv admin note: text overlap with\n  arXiv:2507.18881", "summary": "Since a building's floorplan remains consistent over time and is inherently\nrobust to changes in visual appearance, visual Floorplan Localization (FLoc)\nhas received increasing attention from researchers. However, as a compact and\nminimalist representation of the building's layout, floorplans contain many\nrepetitive structures (e.g., hallways and corners), thus easily result in\nambiguous localization. Existing methods either pin their hopes on matching 2D\nstructural cues in floorplans or rely on 3D geometry-constrained visual\npre-trainings, ignoring the richer contextual information provided by visual\nimages. In this paper, we suggest using broader visual scene context to empower\nFLoc algorithms with scene layout priors to eliminate localization uncertainty.\nIn particular, we propose an unsupervised learning technique with clustering\nconstraints to pre-train a room discriminator on self-collected unlabeled room\nimages. Such a discriminator can empirically extract the hidden room type of\nthe observed image and distinguish it from other room types. By injecting the\nscene context information summarized by the discriminator into an FLoc\nalgorithm, the room style knowledge is effectively exploited to guide definite\nvisual FLoc. We conducted sufficient comparative studies on two standard visual\nFloc benchmarks. Our experiments show that our approach outperforms\nstate-of-the-art methods and achieves significant improvements in robustness\nand accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u623f\u95f4\u5224\u522b\u5668\u63d0\u53d6\u89c6\u89c9\u573a\u666f\u4e0a\u4e0b\u6587\uff08\u623f\u95f4\u7c7b\u578b\uff09\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5e73\u9762\u56fe\u5b9a\u4f4d\uff08FLoc\uff09\u4e2d\u7684\u6b67\u4e49\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5efa\u7b51\u7269\u5e73\u9762\u56fe\u5b9a\u4f4d\uff08FLoc\uff09\u56e0\u5176\u968f\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u5bf9\u89c6\u89c9\u5916\u89c2\u53d8\u5316\u7684\u9c81\u68d2\u6027\u800c\u5907\u53d7\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5e73\u9762\u56fe\u7684\u7d27\u51d1\u548c\u91cd\u590d\u7ed3\u6784\uff08\u5982\u8d70\u5eca\u3001\u89d2\u843d\uff09\u5bb9\u6613\u5bfc\u81f4\u5b9a\u4f4d\u6b67\u4e49\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u4e8e\u5339\u914d2D\u7ed3\u6784\u7ebf\u7d22\u62163D\u51e0\u4f55\u7ea6\u675f\u7684\u89c6\u89c9\u9884\u8bad\u7ec3\uff0c\u5374\u5ffd\u89c6\u4e86\u89c6\u89c9\u56fe\u50cf\u4e2d\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65e0\u76d1\u7763\u5b66\u4e60\u6280\u672f\uff0c\u901a\u8fc7\u805a\u7c7b\u7ea6\u675f\uff0c\u5728\u81ea\u6536\u96c6\u7684\u65e0\u6807\u7b7e\u623f\u95f4\u56fe\u50cf\u4e0a\u9884\u8bad\u7ec3\u4e00\u4e2a\u623f\u95f4\u5224\u522b\u5668\u3002\u8be5\u5224\u522b\u5668\u80fd\u591f\u7ecf\u9a8c\u6027\u5730\u63d0\u53d6\u5e76\u533a\u5206\u89c2\u5bdf\u56fe\u50cf\u4e2d\u9690\u85cf\u7684\u623f\u95f4\u7c7b\u578b\u3002\u901a\u8fc7\u5c06\u5224\u522b\u5668\u603b\u7ed3\u7684\u573a\u666f\u4e0a\u4e0b\u6587\u4fe1\u606f\u6ce8\u5165\u5230FLoc\u7b97\u6cd5\u4e2d\uff0c\u6709\u6548\u5229\u7528\u623f\u95f4\u98ce\u683c\u77e5\u8bc6\u6765\u6307\u5bfc\u660e\u786e\u7684\u89c6\u89c9FLoc\u3002", "result": "\u6211\u4eec\u5728\u4e24\u4e2a\u6807\u51c6\u89c6\u89c9FLoc\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5145\u5206\u7684\u5bf9\u6bd4\u7814\u7a76\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u89c6\u89c9\u573a\u666f\u4e0a\u4e0b\u6587\u4e2d\u7684\u623f\u95f4\u7c7b\u578b\u5148\u9a8c\u77e5\u8bc6\uff0c\u672c\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5e73\u9762\u56fe\u5b9a\u4f4d\u7684\u6b67\u4e49\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5b9a\u4f4d\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u573a\u666f\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9FLoc\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.01545", "pdf": "https://arxiv.org/pdf/2508.01545", "abs": "https://arxiv.org/abs/2508.01545", "authors": ["Emilio Barkett", "Olivia Long", "Paul Kr\u00f6ger"], "title": "Getting out of the Big-Muddy: Escalation of Commitment in LLMs", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in autonomous\ndecision-making roles across high-stakes domains. However, since models are\ntrained on human-generated data, they may inherit cognitive biases that\nsystematically distort human judgment, including escalation of commitment,\nwhere decision-makers continue investing in failing courses of action due to\nprior investment. Understanding when LLMs exhibit such biases presents a unique\nchallenge. While these biases are well-documented in humans, it remains unclear\nwhether they manifest consistently in LLMs or require specific triggering\nconditions. This paper investigates this question using a two-stage investment\ntask across four experimental conditions: model as investor, model as advisor,\nmulti-agent deliberation, and compound pressure scenario. Across N = 6,500\ntrials, we find that bias manifestation in LLMs is highly context-dependent. In\nindividual decision-making contexts (Studies 1-2, N = 4,000), LLMs demonstrate\nstrong rational cost-benefit logic with minimal escalation of commitment.\nHowever, multi-agent deliberation reveals a striking hierarchy effect (Study 3,\nN = 500): while asymmetrical hierarchies show moderate escalation rates\n(46.2%), symmetrical peer-based decision-making produces near-universal\nescalation (99.2%). Similarly, when subjected to compound organizational and\npersonal pressures (Study 4, N = 2,000), models exhibit high degrees of\nescalation of commitment (68.95% average allocation to failing divisions).\nThese findings reveal that LLM bias manifestation depends critically on social\nand organizational context rather than being inherent, with significant\nimplications for the deployment of multi-agent systems and unsupervised\noperations where such conditions may emerge naturally.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u627f\u8bfa\u5347\u7ea7\u504f\u89c1\u5e76\u975e\u56fa\u6709\uff0c\u800c\u662f\u5728\u7279\u5b9a\u793e\u4f1a\u548c\u7ec4\u7ec7\u60c5\u5883\u4e0b\u663e\u73b0\uff0c\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u90e8\u7f72\u6709\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "LLMs\u5728\u5173\u952e\u51b3\u7b56\u9886\u57df\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u8bad\u7ec3\u6570\u636e\u6e90\u4e8e\u4eba\u7c7b\uff0c\u53ef\u80fd\u7ee7\u627f\u8ba4\u77e5\u504f\u89c1\u5982\u627f\u8bfa\u5347\u7ea7\u3002\u76ee\u524d\u5c1a\u4e0d\u6e05\u695aLLMs\u4f55\u65f6\u4f1a\u8868\u73b0\u51fa\u6b64\u7c7b\u504f\u89c1\uff0c\u4ee5\u53ca\u5b83\u4eec\u662f\u56fa\u6709\u8fd8\u662f\u9700\u8981\u7279\u5b9a\u89e6\u53d1\u6761\u4ef6\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e24\u9636\u6bb5\u6295\u8d44\u4efb\u52a1\uff0c\u8bbe\u7f6e\u56db\u79cd\u5b9e\u9a8c\u6761\u4ef6\uff1a\u6a21\u578b\u4f5c\u4e3a\u6295\u8d44\u8005\u3001\u6a21\u578b\u4f5c\u4e3a\u987e\u95ee\u3001\u591a\u667a\u80fd\u4f53\u534f\u5546\u548c\u590d\u5408\u538b\u529b\u60c5\u666f\u3002\u5171\u8fdb\u884c\u4e866500\u6b21\u8bd5\u9a8c\u3002", "result": "LLMs\u7684\u504f\u89c1\u8868\u73b0\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u60c5\u5883\u3002\u5728\u4e2a\u4f53\u51b3\u7b56\u60c5\u5883\u4e0b\uff08N=4,000\uff09\uff0cLLMs\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u7406\u6027\u6210\u672c\u6548\u76ca\u903b\u8f91\uff0c\u627f\u8bfa\u5347\u7ea7\u6781\u5c11\u3002\u7136\u800c\uff0c\u5728\u591a\u667a\u80fd\u4f53\u534f\u5546\u4e2d\uff08N=500\uff09\uff0c\u5bf9\u79f0\u7684\u5bf9\u7b49\u51b3\u7b56\u5bfc\u81f4\u8fd1\u4e4e\u666e\u904d\u7684\u627f\u8bfa\u5347\u7ea7\uff0899.2%\uff09\uff0c\u800c\u975e\u5bf9\u79f0\u5c42\u7ea7\u5219\u8868\u73b0\u51fa\u4e2d\u7b49\u5347\u7ea7\u7387\uff0846.2%\uff09\u3002\u5728\u590d\u5408\u7ec4\u7ec7\u548c\u4e2a\u4eba\u538b\u529b\u4e0b\uff08N=2,000\uff09\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u9ad8\u5ea6\u7684\u627f\u8bfa\u5347\u7ea7\uff08\u5e73\u574768.95%\u7684\u8d44\u91d1\u5206\u914d\u7ed9\u5931\u8d25\u90e8\u95e8\uff09\u3002", "conclusion": "LLMs\u504f\u89c1\u7684\u8868\u73b0\u5e76\u975e\u56fa\u6709\uff0c\u800c\u662f\u5173\u952e\u6027\u5730\u53d6\u51b3\u4e8e\u793e\u4f1a\u548c\u7ec4\u7ec7\u60c5\u5883\u3002\u8fd9\u5bf9\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u65e0\u4eba\u76d1\u7763\u64cd\u4f5c\u7684\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u60c5\u5883\u53ef\u80fd\u81ea\u7136\u51fa\u73b0\u3002"}}
{"id": "2508.01781", "pdf": "https://arxiv.org/pdf/2508.01781", "abs": "https://arxiv.org/abs/2508.01781", "authors": ["Manuel Cossio"], "title": "A comprehensive taxonomy of hallucinations in Large Language Models", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "55 pages, 16 figures, 3 tables", "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their propensity for hallucination, generating plausible but factually\nincorrect or fabricated content, remains a critical challenge. This report\nprovides a comprehensive taxonomy of LLM hallucinations, beginning with a\nformal definition and a theoretical framework that posits its inherent\ninevitability in computable LLMs, irrespective of architecture or training. It\nexplores core distinctions, differentiating between intrinsic (contradicting\ninput context) and extrinsic (inconsistent with training data or reality), as\nwell as factuality (absolute correctness) and faithfulness (adherence to\ninput). The report then details specific manifestations, including factual\nerrors, contextual and logical inconsistencies, temporal disorientation,\nethical violations, and task-specific hallucinations across domains like code\ngeneration and multimodal applications. It analyzes the underlying causes,\ncategorizing them into data-related issues, model-related factors, and\nprompt-related influences. Furthermore, the report examines cognitive and human\nfactors influencing hallucination perception, surveys evaluation benchmarks and\nmetrics for detection, and outlines architectural and systemic mitigation\nstrategies. Finally, it introduces web-based resources for monitoring LLM\nreleases and performance. This report underscores the complex, multifaceted\nnature of LLM hallucinations and emphasizes that, given their theoretical\ninevitability, future efforts must focus on robust detection, mitigation, and\ncontinuous human oversight for responsible and reliable deployment in critical\napplications.", "AI": {"tldr": "\u672c\u62a5\u544a\u5168\u9762\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u201c\u5e7b\u89c9\u201d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5176\u7406\u8bba\u4e0a\u7684\u4e0d\u53ef\u907f\u514d\u6027\uff0c\u5e76\u8be6\u7ec6\u5206\u7c7b\u3001\u63a2\u8ba8\u4e86\u5176\u539f\u56e0\u3001\u68c0\u6d4b\u4e0e\u7f13\u89e3\u7b56\u7565\uff0c\u5f3a\u8c03\u672a\u6765\u9700\u5173\u6ce8\u68c0\u6d4b\u3001\u7f13\u89e3\u548c\u4eba\u5de5\u76d1\u7763\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u201c\u5e7b\u89c9\u201d\u2014\u2014\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u4e8b\u5b9e\u9519\u8bef\u6216\u865a\u6784\u7684\u5185\u5bb9\u2014\u2014\u662f\u4e00\u4e2a\u5173\u952e\u4e14\u4e9f\u5f85\u89e3\u51b3\u7684\u6311\u6218\uff0c\u4e25\u91cd\u5f71\u54cd\u5176\u53ef\u9760\u6027\u3002", "method": "\u672c\u62a5\u544a\u91c7\u7528\u4e86\u4ee5\u4e0b\u65b9\u6cd5\uff1a1) \u5f62\u5f0f\u5316\u5b9a\u4e49LLM\u5e7b\u89c9\u5e76\u63d0\u51fa\u5176\u7406\u8bba\u4e0d\u53ef\u907f\u514d\u6027\u7684\u6846\u67b6\uff1b2) \u533a\u5206\u5185\u6e90\u6027\u4e0e\u5916\u6e90\u6027\u3001\u4e8b\u5b9e\u6027\u4e0e\u5fe0\u5b9e\u6027\u5e7b\u89c9\uff1b3) \u8be6\u7ec6\u9610\u8ff0\u5e7b\u89c9\u7684\u5177\u4f53\u8868\u73b0\u5f62\u5f0f\uff1b4) \u5206\u6790\u5e7b\u89c9\u7684\u6839\u672c\u539f\u56e0\uff08\u6570\u636e\u3001\u6a21\u578b\u3001\u63d0\u793a\uff09\uff1b5) \u8003\u5bdf\u8ba4\u77e5\u548c\u4eba\u4e3a\u56e0\u7d20\uff1b6) \u8c03\u7814\u8bc4\u4f30\u57fa\u51c6\u548c\u68c0\u6d4b\u6307\u6807\uff1b7) \u6982\u8ff0\u67b6\u6784\u548c\u7cfb\u7edf\u6027\u7f13\u89e3\u7b56\u7565\uff1b8) \u4ecb\u7ecd\u5728\u7ebf\u76d1\u63a7\u8d44\u6e90\u3002", "result": "\u672c\u62a5\u544a\u5efa\u7acb\u4e86LLM\u5e7b\u89c9\u7684\u5168\u9762\u5206\u7c7b\u4f53\u7cfb\uff0c\u63d0\u51fa\u4e86\u5e7b\u89c9\u5728\u53ef\u8ba1\u7b97LLMs\u4e2d\u7406\u8bba\u4e0a\u4e0d\u53ef\u907f\u514d\u7684\u6846\u67b6\u3002\u7814\u7a76\u63ed\u793a\u4e86\u5e7b\u89c9\u7684\u590d\u6742\u591a\u9762\u6027\u3001\u5177\u4f53\u8868\u73b0\u548c\u6df1\u5c42\u539f\u56e0\uff0c\u5e76\u7efc\u8ff0\u4e86\u73b0\u6709\u7684\u68c0\u6d4b\u548c\u7f13\u89e3\u65b9\u6cd5\u3002", "conclusion": "\u9274\u4e8eLLM\u5e7b\u89c9\u7684\u7406\u8bba\u4e0d\u53ef\u907f\u514d\u6027\uff0c\u672a\u6765\u7684\u52aa\u529b\u5fc5\u987b\u96c6\u4e2d\u5728\u9c81\u68d2\u7684\u68c0\u6d4b\u3001\u6709\u6548\u7684\u7f13\u89e3\u63aa\u65bd\u4ee5\u53ca\u6301\u7eed\u7684\u4eba\u5de5\u76d1\u7763\u4e0a\uff0c\u4ee5\u786e\u4fddLLMs\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u8d1f\u8d23\u4efb\u548c\u53ef\u9760\u90e8\u7f72\u3002"}}
{"id": "2508.01048", "pdf": "https://arxiv.org/pdf/2508.01048", "abs": "https://arxiv.org/abs/2508.01048", "authors": ["Jesse He", "Akbar Rafiey", "Gal Mishne", "Yusu Wang"], "title": "Explaining GNN Explanations with Edge Gradients", "categories": ["cs.LG"], "comment": "KDD 2025", "summary": "In recent years, the remarkable success of graph neural networks (GNNs) on\ngraph-structured data has prompted a surge of methods for explaining GNN\npredictions. However, the state-of-the-art for GNN explainability remains in\nflux. Different comparisons find mixed results for different methods, with many\nexplainers struggling on more complex GNN architectures and tasks. This\npresents an urgent need for a more careful theoretical analysis of competing\nGNN explanation methods. In this work we take a closer look at GNN explanations\nin two different settings: input-level explanations, which produce explanatory\nsubgraphs of the input graph, and layerwise explanations, which produce\nexplanatory subgraphs of the computation graph. We establish the first\ntheoretical connections between the popular perturbation-based and classical\ngradient-based methods, as well as point out connections between other recently\nproposed methods. At the input level, we demonstrate conditions under which\nGNNExplainer can be approximated by a simple heuristic based on the sign of the\nedge gradients. In the layerwise setting, we point out that edge gradients are\nequivalent to occlusion search for linear GNNs. Finally, we demonstrate how our\ntheoretical results manifest in practice with experiments on both synthetic and\nreal datasets.", "AI": {"tldr": "\u672c\u6587\u5bf9\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u89e3\u91ca\u6027\u65b9\u6cd5\u8fdb\u884c\u6df1\u5165\u7406\u8bba\u5206\u6790\uff0c\u5efa\u7acb\u4e86\u4e0d\u540c\u7c7b\u578b\u89e3\u91ca\u5668\uff08\u5982\u6270\u52a8\u548c\u68af\u5ea6\uff09\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u7406\u8bba\u53d1\u73b0\u3002", "motivation": "\u5f53\u524dGNN\u89e3\u91ca\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u4e0d\u540c\u65b9\u6cd5\u7ed3\u679c\u4e0d\u4e00\uff0c\u4e14\u96be\u4ee5\u5e94\u5bf9\u590d\u6742GNN\u67b6\u6784\u548c\u4efb\u52a1\uff0c\u6025\u9700\u66f4\u4e25\u8c28\u7684\u7406\u8bba\u5206\u6790\u6765\u7406\u89e3\u548c\u6bd4\u8f83\u73b0\u6709\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u5728\u8f93\u5165\u7ea7\u548c\u5c42\u7ea7\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u8003\u5bdfGNN\u89e3\u91ca\u65b9\u6cd5\u3002\u7406\u8bba\u4e0a\uff0c\u9996\u6b21\u5efa\u7acb\u4e86\u6d41\u884c\u7684\u6270\u52a8\u6cd5\u4e0e\u7ecf\u5178\u68af\u5ea6\u6cd5\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u4ed6\u8fd1\u671f\u63d0\u51fa\u65b9\u6cd5\u95f4\u7684\u5173\u8054\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728\u8f93\u5165\u7ea7\uff0c\u8bc1\u660e\u4e86GNNExplainer\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u7528\u57fa\u4e8e\u8fb9\u68af\u5ea6\u7b26\u53f7\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u8fd1\u4f3c\uff1b\u5728\u5c42\u7ea7\uff0c\u6307\u51fa\u7ebf\u6027GNN\u7684\u8fb9\u68af\u5ea6\u7b49\u540c\u4e8e\u906e\u6321\u641c\u7d22\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "result": "\u672c\u7814\u7a76\u9996\u6b21\u5efa\u7acb\u4e86GNN\u89e3\u91ca\u9886\u57df\u4e2d\u6270\u52a8\u5f0f\u548c\u68af\u5ea6\u5f0f\u65b9\u6cd5\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\u3002\u5728\u8f93\u5165\u7ea7\uff0c\u8bc1\u660e\u4e86GNNExplainer\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u88ab\u7b80\u5316\u7684\u8fb9\u68af\u5ea6\u7b26\u53f7\u542f\u53d1\u5f0f\u65b9\u6cd5\u8fd1\u4f3c\u3002\u5728\u5c42\u7ea7\uff0c\u6307\u51fa\u7ebf\u6027GNN\u7684\u8fb9\u68af\u5ea6\u7b49\u4ef7\u4e8e\u906e\u6321\u641c\u7d22\u3002\u8fd9\u4e9b\u7406\u8bba\u7ed3\u679c\u5747\u901a\u8fc7\u5b9e\u9a8c\u5f97\u5230\u4e86\u5b9e\u8df5\u9a8c\u8bc1\u3002", "conclusion": "\u672c\u5de5\u4f5c\u901a\u8fc7\u6df1\u5165\u7684\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4e3aGNN\u89e3\u91ca\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u548c\u7edf\u4e00\u7684\u7406\u89e3\u6846\u67b6\uff0c\u5f25\u5408\u4e86\u4e0d\u540c\u89e3\u91ca\u5668\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8GNN\u53ef\u89e3\u91ca\u6027\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.01218", "pdf": "https://arxiv.org/pdf/2508.01218", "abs": "https://arxiv.org/abs/2508.01218", "authors": ["Yujian Liu", "Linlang Cao", "Chuang Chen", "Fanyu Geng", "Dongxu Shen", "Peng Cao", "Shidang Xu", "Xiaoli Liu"], "title": "MoGaFace: Momentum-Guided and Texture-Aware Gaussian Avatars for Consistent Facial Geometry", "categories": ["cs.CV"], "comment": "10 pages, 7 figures", "summary": "Existing 3D head avatar reconstruction methods adopt a two-stage process,\nrelying on tracked FLAME meshes derived from facial landmarks, followed by\nGaussian-based rendering. However, misalignment between the estimated mesh and\ntarget images often leads to suboptimal rendering quality and loss of fine\nvisual details. In this paper, we present MoGaFace, a novel 3D head avatar\nmodeling framework that continuously refines facial geometry and texture\nattributes throughout the Gaussian rendering process. To address the\nmisalignment between estimated FLAME meshes and target images, we introduce the\nMomentum-Guided Consistent Geometry module, which incorporates a\nmomentum-updated expression bank and an expression-aware correction mechanism\nto ensure temporal and multi-view consistency. Additionally, we propose Latent\nTexture Attention, which encodes compact multi-view features into head-aware\nrepresentations, enabling geometry-aware texture refinement via integration\ninto Gaussians. Extensive experiments show that MoGaFace achieves high-fidelity\nhead avatar reconstruction and significantly improves novel-view synthesis\nquality, even under inaccurate mesh initialization and unconstrained real-world\nsettings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMoGaFace\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u9ad8\u65af\u6e32\u67d3\u8fc7\u7a0b\u4e2d\u8fde\u7eed\u4f18\u5316\u9762\u90e8\u51e0\u4f55\u548c\u7eb9\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u67093D\u5934\u90e8\u6570\u5b57\u4eba\u91cd\u5efa\u4e2d\u7f51\u683c\u9519\u4f4d\u5bfc\u81f4\u7684\u6e32\u67d3\u8d28\u91cf\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u4fdd\u771f\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u67093D\u5934\u90e8\u6570\u5b57\u4eba\u91cd\u5efa\u65b9\u6cd5\u91c7\u7528\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff0c\u4f9d\u8d56\u8ffd\u8e2a\u7684FLAME\u7f51\u683c\uff0c\u4f46\u7f51\u683c\u4e0e\u76ee\u6807\u56fe\u50cf\u7684\u9519\u4f4d\u5e38\u5bfc\u81f4\u6e32\u67d3\u8d28\u91cf\u4e0d\u4f73\u548c\u7cbe\u7ec6\u89c6\u89c9\u7ec6\u8282\u7684\u4e22\u5931\u3002", "method": "MoGaFace\u662f\u4e00\u4e2a\u65b0\u9896\u76843D\u5934\u90e8\u6570\u5b57\u4eba\u5efa\u6a21\u6846\u67b6\uff0c\u5728\u9ad8\u65af\u6e32\u67d3\u8fc7\u7a0b\u4e2d\u6301\u7eed\u4f18\u5316\u9762\u90e8\u51e0\u4f55\u548c\u7eb9\u7406\u5c5e\u6027\u3002\u5b83\u5f15\u5165\u4e86\u201c\u52a8\u91cf\u5f15\u5bfc\u4e00\u81f4\u51e0\u4f55\u6a21\u5757\u201d\u4ee5\u89e3\u51b3\u7f51\u683c\u9519\u4f4d\u5e76\u786e\u4fdd\u65f6\u5e8f\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff1b\u540c\u65f6\u63d0\u51fa\u4e86\u201c\u6f5c\u5728\u7eb9\u7406\u6ce8\u610f\u529b\u201d\u6a21\u5757\uff0c\u901a\u8fc7\u7f16\u7801\u591a\u89c6\u89d2\u7279\u5f81\u5b9e\u73b0\u51e0\u4f55\u611f\u77e5\u7684\u7eb9\u7406\u4f18\u5316\u5e76\u96c6\u6210\u5230\u9ad8\u65af\u4e2d\u3002", "result": "MoGaFace\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5934\u90e8\u6570\u5b57\u4eba\u91cd\u5efa\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u8d28\u91cf\u3002\u5373\u4f7f\u5728\u4e0d\u7cbe\u786e\u7684\u7f51\u683c\u521d\u59cb\u5316\u548c\u65e0\u7ea6\u675f\u7684\u771f\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "MoGaFace\u6210\u529f\u514b\u670d\u4e86\u73b0\u67093D\u5934\u90e8\u6570\u5b57\u4eba\u91cd\u5efa\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9c81\u68d2\u7684\u65b9\u6848\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u8d28\u91cf\u7684\u5934\u90e8\u6570\u5b57\u4eba\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u3002"}}
{"id": "2508.01556", "pdf": "https://arxiv.org/pdf/2508.01556", "abs": "https://arxiv.org/abs/2508.01556", "authors": ["Mengshi Chen", "Yuxiang Sun", "Tengchao Li", "Jianwei Wang", "Kai Wang", "Xuemin Lin", "Ying Zhang", "Wenjie Zhang"], "title": "Empowering Tabular Data Preparation with Language Models: Why and How?", "categories": ["cs.AI", "68T50", "I.2.7"], "comment": "Preprint under submission, 16 pages, 2 figures, 1 table", "summary": "Data preparation is a critical step in enhancing the usability of tabular\ndata and thus boosts downstream data-driven tasks. Traditional methods often\nface challenges in capturing the intricate relationships within tables and\nadapting to the tasks involved. Recent advances in Language Models (LMs),\nespecially in Large Language Models (LLMs), offer new opportunities to automate\nand support tabular data preparation. However, why LMs suit tabular data\npreparation (i.e., how their capabilities match task demands) and how to use\nthem effectively across phases still remain to be systematically explored. In\nthis survey, we systematically analyze the role of LMs in enhancing tabular\ndata preparation processes, focusing on four core phases: data acquisition,\nintegration, cleaning, and transformation. For each phase, we present an\nintegrated analysis of how LMs can be combined with other components for\ndifferent preparation tasks, highlight key advancements, and outline\nprospective pipelines.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u7cfb\u7edf\u5206\u6790\u4e86\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\uff0c\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5982\u4f55\u5728\u8868\u683c\u6570\u636e\u51c6\u5907\u7684\u56db\u4e2a\u6838\u5fc3\u9636\u6bb5\uff08\u6570\u636e\u91c7\u96c6\u3001\u96c6\u6210\u3001\u6e05\u6d17\u3001\u8f6c\u6362\uff09\u4e2d\u53d1\u6325\u4f5c\u7528\u5e76\u63d0\u5347\u5176\u6548\u7387\u3002", "motivation": "\u8868\u683c\u6570\u636e\u51c6\u5907\u5bf9\u4e0b\u6e38\u6570\u636e\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u590d\u6742\u5173\u7cfb\u5e76\u9002\u5e94\u4efb\u52a1\u3002\u5c3d\u7ba1LMs/LLMs\u4e3a\u81ea\u52a8\u5316\u548c\u652f\u6301\u8868\u683c\u6570\u636e\u51c6\u5907\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\uff0c\u4f46\u76ee\u524d\u4ecd\u7f3a\u4e4f\u5bf9\u5176\u80fd\u529b\u5982\u4f55\u5339\u914d\u4efb\u52a1\u9700\u6c42\u4ee5\u53ca\u5982\u4f55\u5728\u5404\u9636\u6bb5\u6709\u6548\u5e94\u7528\u7684\u7cfb\u7edf\u6027\u63a2\u7d22\u3002", "method": "\u672c\u6587\u91c7\u7528\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u5730\u5206\u6790\u4e86LMs\u5728\u8868\u683c\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u4e2d\u7684\u4f5c\u7528\u3002\u7814\u7a76\u91cd\u70b9\u653e\u5728\u6570\u636e\u91c7\u96c6\u3001\u96c6\u6210\u3001\u6e05\u6d17\u548c\u8f6c\u6362\u8fd9\u56db\u4e2a\u6838\u5fc3\u9636\u6bb5\uff0c\u5e76\u9488\u5bf9\u6bcf\u4e2a\u9636\u6bb5\uff0c\u7efc\u5408\u5206\u6790\u4e86LMs\u5982\u4f55\u4e0e\u5176\u4ed6\u7ec4\u4ef6\u7ed3\u5408\u4ee5\u5b8c\u6210\u4e0d\u540c\u7684\u51c6\u5907\u4efb\u52a1\uff0c\u7a81\u51fa\u4e86\u5173\u952e\u8fdb\u5c55\uff0c\u5e76\u6982\u8ff0\u4e86\u6f5c\u5728\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u672c\u7efc\u8ff0\u7684\u7ed3\u679c\u662f\u7cfb\u7edf\u6027\u5730\u9610\u660e\u4e86LMs\u5728\u8868\u683c\u6570\u636e\u51c6\u5907\u5404\u4e2a\u9636\u6bb5\u7684\u5e94\u7528\u6f5c\u529b\u3002\u5b83\u63d0\u4f9b\u4e86\u5173\u4e8eLMs\u5982\u4f55\u4e0e\u73b0\u6709\u6280\u672f\u7ed3\u5408\u3001\u5f53\u524d\u7684\u5173\u952e\u8fdb\u5c55\u4ee5\u53ca\u672a\u6765\u96c6\u6210LMs\u7684\u51c6\u5907\u6d41\u7a0b\u7684\u6df1\u5165\u5206\u6790\u3002", "conclusion": "\u672c\u7efc\u8ff0\u65e8\u5728\u4e3aLMs\u5728\u8868\u683c\u6570\u636e\u51c6\u5907\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e00\u4e2a\u5168\u9762\u7684\u3001\u7cfb\u7edf\u6027\u7684\u7406\u89e3\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u6307\u660e\u65b9\u5411\u3002"}}
{"id": "2508.01812", "pdf": "https://arxiv.org/pdf/2508.01812", "abs": "https://arxiv.org/abs/2508.01812", "authors": ["Amir DN Cohen", "Hilla Merhav", "Yoav Goldberg", "Reut Tsarfaty"], "title": "HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly\non morpho-syntactic tasks, neglecting the semantic dimension of language\nunderstanding. To bridge this gap, we set out to deliver a Hebrew Machine\nReading Comprehension (MRC) dataset, where MRC is to be realized as extractive\nQuestion Answering. The morphologically rich nature of Hebrew poses a challenge\nto this endeavor: the indeterminacy and non-transparency of span boundaries in\nmorphologically complex forms lead to annotation inconsistencies,\ndisagreements, and flaws in standard evaluation metrics.\n  To remedy this, we devise a novel set of guidelines, a controlled\ncrowdsourcing protocol, and revised evaluation metrics that are suitable for\nthe morphologically rich nature of the language. Our resulting benchmark, HeQ\n(Hebrew QA), features 30,147 diverse question-answer pairs derived from both\nHebrew Wikipedia articles and Israeli tech news. Our empirical investigation\nreveals that standard evaluation metrics such as F1 scores and Exact Match (EM)\nare not appropriate for Hebrew (and other MRLs), and we propose a relevant\nenhancement.\n  In addition, our experiments show low correlation between models' performance\non morpho-syntactic tasks and on MRC, which suggests that models designed for\nthe former might underperform on semantics-heavy tasks. The development and\nexploration of HeQ illustrate some of the challenges MRLs pose in natural\nlanguage understanding (NLU), fostering progression towards more and better NLU\nmodels for Hebrew and other MRLs.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5e0c\u4f2f\u6765\u8bedNLP\u7f3a\u4e4f\u8bed\u4e49\u7ef4\u5ea6\u57fa\u51c6\u7684\u73b0\u72b6\uff0c\u63a8\u51fa\u4e86\u5e0c\u4f2f\u6765\u8bed\u673a\u5668\u9605\u8bfb\u7406\u89e3\u6570\u636e\u96c6HeQ\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u4e0d\u9002\u7528\u4e8e\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\uff0c\u540c\u65f6\u6307\u51fa\u4e3a\u5f62\u6001\u53e5\u6cd5\u4efb\u52a1\u8bbe\u8ba1\u7684\u6a21\u578b\u5728\u8bed\u4e49\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5f53\u524d\u5e0c\u4f2f\u6765\u8bedNLP\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5f62\u6001\u53e5\u6cd5\uff0c\u5ffd\u89c6\u8bed\u4e49\u7406\u89e3\u3002\u6784\u5efa\u5e0c\u4f2f\u6765\u8bed\u673a\u5668\u9605\u8bfb\u7406\u89e3\uff08MRC\uff09\u6570\u636e\u96c6\u9762\u4e34\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u5e26\u6765\u7684\u6807\u6ce8\u4e00\u81f4\u6027\u548c\u8bc4\u4f30\u6307\u6807\u9002\u7528\u6027\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u5e0c\u4f2f\u6765\u8bed\u673a\u5668\u9605\u8bfb\u7406\u89e3\uff08MRC\uff09\u6570\u636e\u96c6HeQ\uff08\u4f5c\u4e3a\u62bd\u53d6\u5f0f\u95ee\u7b54\uff09\uff1b\u5236\u5b9a\u4e86\u65b0\u9896\u7684\u6807\u6ce8\u6307\u5357\u3001\u53d7\u63a7\u4f17\u5305\u534f\u8bae\u548c\u4fee\u6b63\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u9002\u5e94\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\u7279\u70b9\uff1b\u4ece\u5e0c\u4f2f\u6765\u8bed\u7ef4\u57fa\u767e\u79d1\u6587\u7ae0\u548c\u4ee5\u8272\u5217\u79d1\u6280\u65b0\u95fb\u4e2d\u6536\u96c6\u4e8630,147\u4e2a\u95ee\u7b54\u5bf9\uff1b\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc4\u4f30\u6807\u51c6\u6307\u6807\u7684\u9002\u7528\u6027\u5e76\u63a2\u7d22\u6a21\u578b\u6027\u80fd\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u5305\u542b30,147\u4e2a\u95ee\u7b54\u5bf9\u7684\u5e0c\u4f2f\u6765\u8bedMRC\u57fa\u51c6\u6570\u636e\u96c6HeQ\uff1b\u5b9e\u8bc1\u53d1\u73b0F1\u548cExact Match\uff08EM\uff09\u7b49\u6807\u51c6\u8bc4\u4f30\u6307\u6807\u4e0d\u9002\u7528\u4e8e\u5e0c\u4f2f\u6765\u8bed\uff08\u53ca\u5176\u4ed6\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u6539\u8fdb\u65b9\u6848\uff1b\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u5f62\u6001\u53e5\u6cd5\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0e\u5728MRC\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5173\u8054\u5ea6\u4f4e\uff0c\u6697\u793a\u4e3a\u524d\u8005\u8bbe\u8ba1\u7684\u6a21\u578b\u53ef\u80fd\u5728\u8bed\u4e49\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "HeQ\u6570\u636e\u96c6\u7684\u5f00\u53d1\u4e0e\u63a2\u7d22\u63ed\u793a\u4e86\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\uff08NLU\uff09\u4e2d\u9762\u4e34\u7684\u6311\u6218\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u5e0c\u4f2f\u6765\u8bed\u53ca\u5176\u4ed6\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u7684NLU\u6a21\u578b\u53d1\u5c55\u3002"}}
{"id": "2508.01049", "pdf": "https://arxiv.org/pdf/2508.01049", "abs": "https://arxiv.org/abs/2508.01049", "authors": ["Nicholas E. Corrado", "Josiah P. Hanna"], "title": "Centralized Adaptive Sampling for Reliable Co-Training of Independent Multi-Agent Policies", "categories": ["cs.LG"], "comment": null, "summary": "Independent on-policy policy gradient algorithms are widely used for\nmulti-agent reinforcement learning (MARL) in cooperative and no-conflict games,\nbut they are known to converge suboptimally when each agent's policy gradient\npoints toward a suboptimal equilibrium. In this work, we identify a subtler\nfailure mode that arises \\textit{even when the expected policy gradients of all\nagents point toward an optimal solution.} After collecting a finite set of\ntrajectories, stochasticity in independent action sampling can cause the joint\ndata distribution to deviate from the expected joint on-policy distribution.\nThis \\textit{sampling error} w.r.t. the joint on-policy distribution produces\ninaccurate gradient estimates that can lead agents to converge suboptimally. In\nthis paper, we investigate if joint sampling error can be reduced through\ncoordinated action selection and whether doing so improves the reliability of\npolicy gradient learning in MARL. Toward this end, we introduce an adaptive\naction sampling approach to reduce joint sampling error. Our method,\nMulti-Agent Proximal Robust On-Policy Sampling (MA-PROPS), uses a centralized\nbehavior policy that we continually adapt to place larger probability on joint\nactions that are currently under-sampled w.r.t. the current joint policy. We\nempirically evaluate MA-PROPS in a diverse range of multi-agent games and\ndemonstrate that (1) MA-PROPS reduces joint sampling error more efficiently\nthan standard on-policy sampling and (2) improves the reliability of\nindependent policy gradient algorithms, increasing the fraction of training\nruns that converge to an optimal joint policy.", "AI": {"tldr": "\u72ec\u7acb\u7684\u30aa\u30f3\u30dd\u30ea\u30b7\u30fc\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u5b58\u5728\u5fae\u5999\u7684\u201c\u91c7\u6837\u8bef\u5dee\u201d\uff0c\u5373\u4f7f\u7406\u8bba\u68af\u5ea6\u6307\u5411\u6700\u4f18\u89e3\u4e5f\u4f1a\u5bfc\u81f4\u6b21\u4f18\u6536\u655b\u3002\u672c\u6587\u63d0\u51faMA-PROPS\uff0c\u4e00\u79cd\u81ea\u9002\u5e94\u534f\u8c03\u91c7\u6837\u65b9\u6cd5\uff0c\u6709\u6548\u964d\u4f4e\u8be5\u8bef\u5dee\uff0c\u663e\u8457\u63d0\u9ad8\u7b97\u6cd5\u6536\u655b\u5230\u6700\u4f18\u7b56\u7565\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u72ec\u7acb\u7684\u30aa\u30f3\u30dd\u30ea\u30b7\u30fc\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5373\u4f7f\u671f\u671b\u68af\u5ea6\u6307\u5411\u6700\u4f18\u89e3\uff0c\u4e5f\u53ef\u80fd\u56e0\u6709\u9650\u8f68\u8ff9\u91c7\u96c6\u4e2d\u7684\u968f\u673a\u72ec\u7acb\u52a8\u4f5c\u91c7\u6837\u5bfc\u81f4\u7684\u201c\u8054\u5408\u91c7\u6837\u8bef\u5dee\u201d\uff0c\u8fdb\u800c\u4ea7\u751f\u4e0d\u51c6\u786e\u7684\u68af\u5ea6\u4f30\u8ba1\uff0c\u5bfc\u81f4\u7b97\u6cd5\u6b21\u4f18\u6536\u655b\u3002\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u8fd9\u4e00\u672a\u88ab\u5145\u5206\u8bc6\u522b\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u63d0\u51faMulti-Agent Proximal Robust On-Policy Sampling (MA-PROPS) \u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e00\u4e2a\u96c6\u4e2d\u7684\u884c\u4e3a\u7b56\u7565\u5b9e\u73b0\u81ea\u9002\u5e94\u52a8\u4f5c\u91c7\u6837\uff0c\u8be5\u7b56\u7565\u4e0d\u65ad\u8c03\u6574\u4ee5\u589e\u52a0\u5f53\u524d\u7b56\u7565\u4e0b\u88ab\u6b20\u91c7\u6837\u7684\u8054\u5408\u52a8\u4f5c\u7684\u91c7\u6837\u6982\u7387\uff0c\u4ece\u800c\u51cf\u5c11\u8054\u5408\u91c7\u6837\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a(1) MA-PROPS\u6bd4\u6807\u51c6\u30aa\u30f3\u30dd\u30ea\u30b7\u30fc\u91c7\u6837\u66f4\u6709\u6548\u5730\u51cf\u5c11\u8054\u5408\u91c7\u6837\u8bef\u5dee\uff1b(2) MA-PROPS\u663e\u8457\u63d0\u9ad8\u4e86\u72ec\u7acb\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u7684\u53ef\u9760\u6027\uff0c\u589e\u52a0\u4e86\u8bad\u7ec3\u8fd0\u884c\u6536\u655b\u5230\u6700\u4f18\u8054\u5408\u7b56\u7565\u7684\u6bd4\u4f8b\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165MA-PROPS\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u72ec\u7acb\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u7684\u8054\u5408\u91c7\u6837\u8bef\u5dee\u95ee\u9898\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u63d0\u9ad8\u5176\u6536\u655b\u5230\u6700\u4f18\u89e3\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2508.01219", "pdf": "https://arxiv.org/pdf/2508.01219", "abs": "https://arxiv.org/abs/2508.01219", "authors": ["Anzhe Cheng", "Chenzhong Yin", "Mingxi Cheng", "Shukai Duan", "Shahin Nazarian", "Paul Bogdan"], "title": "Eigen Neural Network: Unlocking Generalizable Vision with Eigenbasis", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The remarkable success of Deep Neural Networks(DNN) is driven by\ngradient-based optimization, yet this process is often undermined by its\ntendency to produce disordered weight structures, which harms feature clarity\nand degrades learning dynamics. To address this fundamental representational\nflaw, we introduced the Eigen Neural Network (ENN), a novel architecture that\nreparameterizes each layer's weights in a layer-shared, learned orthonormal\neigenbasis. This design enforces decorrelated, well-aligned weight dynamics\naxiomatically, rather than through regularization, leading to more structured\nand discriminative feature representations. When integrated with standard BP,\nENN consistently outperforms state-of-the-art methods on large-scale image\nclassification benchmarks, including ImageNet, and its superior representations\ngeneralize to set a new benchmark in cross-modal image-text retrieval.\nFurthermore, ENN's principled structure enables a highly efficient,\nbackpropagation-free(BP-free) local learning variant, ENN-$\\ell$. This variant\nnot only resolves BP's procedural bottlenecks to achieve over 2$\\times$\ntraining speedup via parallelism, but also, remarkably, surpasses the accuracy\nof end-to-end backpropagation. ENN thus presents a new architectural paradigm\nthat directly remedies the representational deficiencies of BP, leading to\nenhanced performance and enabling a more efficient, parallelizable training\nregime.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.01561", "pdf": "https://arxiv.org/pdf/2508.01561", "abs": "https://arxiv.org/abs/2508.01561", "authors": ["Zijian Guo", "\u0130lker I\u015f\u0131k", "H. M. Sabbir Ahmad", "Wenchao Li"], "title": "One Subgoal at a Time: Zero-Shot Generalization to Arbitrary Linear Temporal Logic Requirements in Multi-Task Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "Generalizing to complex and temporally extended task objectives and safety\nconstraints remains a critical challenge in reinforcement learning (RL). Linear\ntemporal logic (LTL) offers a unified formalism to specify such requirements,\nyet existing methods are limited in their abilities to handle nested\nlong-horizon tasks and safety constraints, and cannot identify situations when\na subgoal is not satisfiable and an alternative should be sought. In this\npaper, we introduce GenZ-LTL, a method that enables zero-shot generalization to\narbitrary LTL specifications. GenZ-LTL leverages the structure of B\\\"uchi\nautomata to decompose an LTL task specification into sequences of reach-avoid\nsubgoals. Contrary to the current state-of-the-art method that conditions on\nsubgoal sequences, we show that it is more effective to achieve zero-shot\ngeneralization by solving these reach-avoid problems \\textit{one subgoal at a\ntime} through proper safe RL formulations. In addition, we introduce a novel\nsubgoal-induced observation reduction technique that can mitigate the\nexponential complexity of subgoal-state combinations under realistic\nassumptions. Empirical results show that GenZ-LTL substantially outperforms\nexisting methods in zero-shot generalization to unseen LTL specifications.", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u5728\u6cdb\u5316\u5230\u590d\u6742\u65f6\u5e8f\u4efb\u52a1\u548c\u5b89\u5168\u7ea6\u675f\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002GenZ-LTL\u65b9\u6cd5\u5229\u7528B\u00fcchi\u81ea\u52a8\u673a\u5c06LTL\u4efb\u52a1\u5206\u89e3\u4e3a\u5355\u6b65\u7684\u201c\u5230\u8fbe-\u907f\u514d\u201d\u5b50\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u5b50\u76ee\u6807\u8bf1\u5bfc\u7684\u89c2\u6d4b\u51cf\u5c11\u6280\u672f\uff0c\u5b9e\u73b0\u5bf9\u4efb\u610fLTL\u89c4\u8303\u7684\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u6cdb\u5316\u5230\u590d\u6742\u4e14\u5177\u6709\u65f6\u5e8f\u6269\u5c55\u7684\u4efb\u52a1\u76ee\u6807\u548c\u5b89\u5168\u7ea6\u675f\u65f6\u9762\u4e34\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5c3d\u7ba1\u4f7f\u7528\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09\u6765\u89c4\u8303\u8fd9\u4e9b\u9700\u6c42\uff0c\u4f46\u5728\u5904\u7406\u5d4c\u5957\u3001\u957f\u65f6\u7a0b\u4efb\u52a1\u548c\u5b89\u5168\u7ea6\u675f\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u4e14\u65e0\u6cd5\u8bc6\u522b\u5b50\u76ee\u6807\u4e0d\u53ef\u6ee1\u8db3\u5e76\u9700\u8981\u5bfb\u627e\u66ff\u4ee3\u65b9\u6848\u7684\u60c5\u51b5\u3002", "method": "\u672c\u6587\u63d0\u51faGenZ-LTL\u65b9\u6cd5\uff0c\u65e8\u5728\u5b9e\u73b0\u5bf9\u4efb\u610fLTL\u89c4\u8303\u7684\u96f6\u6837\u672c\u6cdb\u5316\u3002GenZ-LTL\u5229\u7528B\u00fcchi\u81ea\u52a8\u673a\u7684\u7ed3\u6784\uff0c\u5c06LTL\u4efb\u52a1\u89c4\u8303\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u201c\u5230\u8fbe-\u907f\u514d\u201d\uff08reach-avoid\uff09\u5b50\u76ee\u6807\u3002\u4e0e\u73b0\u6709\u6280\u672f\u4e0d\u540c\uff0cGenZ-LTL\u901a\u8fc7\u9002\u5f53\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u516c\u5f0f\uff0c\u4e00\u6b21\u89e3\u51b3\u4e00\u4e2a\u201c\u5230\u8fbe-\u907f\u514d\u201d\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u96f6\u6837\u672c\u6cdb\u5316\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b50\u76ee\u6807\u8bf1\u5bfc\u89c2\u6d4b\u51cf\u5c11\u6280\u672f\uff0c\u4ee5\u7f13\u89e3\u5b50\u76ee\u6807-\u72b6\u6001\u7ec4\u5408\u5e26\u6765\u7684\u6307\u6570\u7ea7\u590d\u6742\u6027\u3002", "result": "\u7ecf\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGenZ-LTL\u5728\u96f6\u6837\u672c\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684LTL\u89c4\u8303\u65b9\u9762\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GenZ-LTL\u901a\u8fc7\u5c06\u590d\u6742\u7684LTL\u4efb\u52a1\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u5355\u6b65\u5b50\u76ee\u6807\uff0c\u5e76\u7ed3\u5408\u521b\u65b0\u7684\u89c2\u6d4b\u51cf\u5c11\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u65f6\u5e8f\u4efb\u52a1\u548c\u5b89\u5168\u7ea6\u675f\u4e0b\u96f6\u6837\u672c\u6cdb\u5316\u7684\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u5bf9\u4efb\u610fLTL\u89c4\u8303\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.01815", "pdf": "https://arxiv.org/pdf/2508.01815", "abs": "https://arxiv.org/abs/2508.01815", "authors": ["Yang Zhao", "Chengxiao Dai", "Wei Zhuo", "Tan Chuan Fu", "Yue Xiu", "Dusit Niyato", "Jonathan Z. Low", "Eugene Ho Hong Zhuang", "Daren Zong Loong Tan"], "title": "AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Question answering over heterogeneous knowledge graphs (KGQA) involves\nreasoning across diverse schemas, incomplete alignments, and distributed data\nsources. Existing text-to-SPARQL approaches rely on large-scale domain-specific\nfine-tuning or operate within single-graph settings, limiting their\ngeneralizability in low-resource domains and their ability to handle queries\nspanning multiple graphs. These challenges are particularly relevant in domains\nsuch as the circular economy, where information about classifications,\nprocesses, and emissions is distributed across independently curated knowledge\ngraphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes\nKGQA into subtasks managed by specialized agents responsible for retrieval,\nquery generation, and verification. A scheduler assigns subgoals to different\ngraphs using weak-to-strong alignment strategies. A two-stage verifier detects\nstructurally invalid and semantically underspecified queries through symbolic\nvalidation and counterfactual consistency checks. Experiments on real-world\ncircular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy\nby 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing\nthe average prompt length by 46.4%. These results demonstrate the benefits of\nagent-based schema-aware reasoning for scalable KGQA and support\ndecision-making in sustainability domains through robust cross-graph reasoning.", "AI": {"tldr": "AgenticT$^2$S\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u4e24\u9636\u6bb5\u9a8c\u8bc1\uff0c\u89e3\u51b3\u4e86\u5f02\u6784\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\u8de8\u56fe\u8c31\u63a8\u7406\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709KGQA\u65b9\u6cd5\u5728\u5904\u7406\u5f02\u6784\u3001\u4f4e\u8d44\u6e90\u53ca\u8de8\u591a\u56fe\u8c31\u67e5\u8be2\u65f6\u9762\u4e34\u6cdb\u5316\u6027\u548c\u5904\u7406\u80fd\u529b\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u5c24\u5176\u5728\u5faa\u73af\u7ecf\u6d4e\u7b49\u4fe1\u606f\u5206\u5e03\u4e8e\u72ec\u7acb\u77e5\u8bc6\u56fe\u8c31\u7684\u9886\u57df\u3002", "method": "\u63d0\u51faAgenticT$^2$S\u6846\u67b6\uff0c\u5b83\u5c06KGQA\u5206\u89e3\u4e3a\u68c0\u7d22\u3001\u67e5\u8be2\u751f\u6210\u548c\u9a8c\u8bc1\u5b50\u4efb\u52a1\uff0c\u7531\u4e13\u4e1a\u4ee3\u7406\u7ba1\u7406\u3002\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u5f31\u5230\u5f3a\u5bf9\u9f50\u7b56\u7565\u5206\u914d\u5b50\u76ee\u6807\u5230\u4e0d\u540c\u56fe\u8c31\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u9a8c\u8bc1\u5668\uff08\u7b26\u53f7\u9a8c\u8bc1\u548c\u53cd\u4e8b\u5b9e\u4e00\u81f4\u6027\u68c0\u67e5\uff09\u68c0\u6d4b\u65e0\u6548\u6216\u4e0d\u660e\u786e\u7684\u67e5\u8be2\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u5faa\u73af\u7ecf\u6d4e\u77e5\u8bc6\u56fe\u8c31\u4e0a\uff0cAgenticT$^2$S\u5c06\u6267\u884c\u51c6\u786e\u7387\u63d0\u9ad8\u4e8617.3%\uff0c\u4e09\u5143\u7ec4\u7ea7\u522bF1\u63d0\u9ad8\u4e8625.4%\uff0c\u540c\u65f6\u5e73\u5747\u63d0\u793a\u957f\u5ea6\u51cf\u5c11\u4e8646.4%\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u5f0f\u611f\u77e5\u63a8\u7406\u5bf9\u53ef\u6269\u5c55\u7684\u5f02\u6784\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u591f\u901a\u8fc7\u7a33\u5065\u7684\u8de8\u56fe\u8c31\u63a8\u7406\u652f\u6301\u53ef\u6301\u7eed\u53d1\u5c55\u9886\u57df\u7684\u51b3\u7b56\u5236\u5b9a\u3002"}}
{"id": "2508.01055", "pdf": "https://arxiv.org/pdf/2508.01055", "abs": "https://arxiv.org/abs/2508.01055", "authors": ["Xuan Liu", "Siru Ouyang", "Xianrui Zhong", "Jiawei Han", "Huimin Zhao"], "title": "FGBench: A Dataset and Benchmark for Molecular Property Reasoning at Functional Group-Level in Large Language Models", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM"], "comment": "20 pages, 20 figures", "summary": "Large language models (LLMs) have gained significant attention in chemistry.\nHowever, most existing datasets center on molecular-level property prediction\nand overlook the role of fine-grained functional group (FG) information.\nIncorporating FG-level data can provide valuable prior knowledge that links\nmolecular structures with textual descriptions, which can be used to build more\ninterpretable, structure-aware LLMs for reasoning on molecule-related tasks.\nMoreover, LLMs can learn from such fine-grained information to uncover hidden\nrelationships between specific functional groups and molecular properties,\nthereby advancing molecular design and drug discovery. Here, we introduce\nFGBench, a dataset comprising 625K molecular property reasoning problems with\nfunctional group information. Functional groups are precisely annotated and\nlocalized within the molecule, which ensures the dataset's interoperability\nthereby facilitating further multimodal applications. FGBench includes both\nregression and classification tasks on 245 different functional groups across\nthree categories for molecular property reasoning: (1) single functional group\nimpacts, (2) multiple functional group interactions, and (3) direct molecular\ncomparisons. In the benchmark of state-of-the-art LLMs on 7K curated data, the\nresults indicate that current LLMs struggle with FG-level property reasoning,\nhighlighting the need to enhance reasoning capabilities in LLMs for chemistry\ntasks. We anticipate that the methodology employed in FGBench to construct\ndatasets with functional group-level information will serve as a foundational\nframework for generating new question-answer pairs, enabling LLMs to better\nunderstand fine-grained molecular structure-property relationships. The dataset\nand evaluation code are available at\n\\href{https://github.com/xuanliugit/FGBench}{https://github.com/xuanliugit/FGBench}.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165\u4e86FGBench\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5f25\u8865\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5316\u5b66\u9886\u57df\u4e2d\u5bf9\u7cbe\u7ec6\u5b98\u80fd\u56e2\uff08FG\uff09\u5c42\u9762\u4fe1\u606f\u63a8\u7406\u80fd\u529b\u7684\u4e0d\u8db3\u3002FGBench\u5305\u542b625K\u4e2a\u5206\u5b50\u6027\u8d28\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u53d1\u73b0\u73b0\u6709LLMs\u5728\u8be5\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5f53\u524d\u5316\u5b66\u9886\u57df\u7684LLMs\u4e3b\u8981\u5173\u6ce8\u5206\u5b50\u5c42\u9762\u6027\u8d28\u9884\u6d4b\uff0c\u4f46\u5ffd\u89c6\u4e86\u7cbe\u7ec6\u5b98\u80fd\u56e2\u4fe1\u606f\u7684\u91cd\u8981\u6027\u3002\u5b98\u80fd\u56e2\u4fe1\u606f\u80fd\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u8fde\u63a5\u5206\u5b50\u7ed3\u6784\u4e0e\u6587\u672c\u63cf\u8ff0\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3001\u7ed3\u6784\u611f\u77e5\u80fd\u529b\u7684LLMs\uff0c\u5e76\u63ed\u793a\u5b98\u80fd\u56e2\u4e0e\u5206\u5b50\u6027\u8d28\u7684\u9690\u85cf\u5173\u7cfb\uff0c\u4ece\u800c\u63a8\u52a8\u5206\u5b50\u8bbe\u8ba1\u548c\u836f\u7269\u53d1\u73b0\u3002", "method": "\u672c\u7814\u7a76\u521b\u5efa\u4e86FGBench\u6570\u636e\u96c6\uff0c\u5305\u542b625K\u4e2a\u5206\u5b50\u6027\u8d28\u63a8\u7406\u95ee\u9898\uff0c\u5176\u4e2d\u5b98\u80fd\u56e2\u4fe1\u606f\u88ab\u7cbe\u786e\u6807\u6ce8\u548c\u5b9a\u4f4d\u3002\u6570\u636e\u96c6\u6db5\u76d6\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\uff0c\u6d89\u53ca245\u79cd\u4e0d\u540c\u5b98\u80fd\u56e2\u7684\u4e09\u4e2a\u63a8\u7406\u7c7b\u522b\uff1a\u5355\u4e00\u5b98\u80fd\u56e2\u5f71\u54cd\u3001\u591a\u5b98\u80fd\u56e2\u76f8\u4e92\u4f5c\u7528\u548c\u76f4\u63a5\u5206\u5b50\u6bd4\u8f83\u3002\u7814\u7a76\u57287K\u7cbe\u9009\u6570\u636e\u4e0a\u5bf9\u6700\u5148\u8fdb\u7684LLMs\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524dLLMs\u5728\u5b98\u80fd\u56e2\u5c42\u9762\u7684\u6027\u8d28\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u7a81\u663e\u4e86LLMs\u5728\u5316\u5b66\u4efb\u52a1\u4e2d\u63d0\u5347\u63a8\u7406\u80fd\u529b\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86LLMs\u5728\u5316\u5b66\u9886\u57df\u4e2d\u589e\u5f3a\u5b98\u80fd\u56e2\u5c42\u9762\u63a8\u7406\u80fd\u529b\u7684\u5fc5\u8981\u6027\u3002FGBench\u6784\u5efa\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u6709\u671b\u6210\u4e3a\u751f\u6210\u65b0\u7684\u95ee\u7b54\u5bf9\u7684\u57fa\u7840\u6846\u67b6\uff0c\u4ece\u800c\u5e2e\u52a9LLMs\u66f4\u597d\u5730\u7406\u89e3\u7cbe\u7ec6\u7684\u5206\u5b50\u7ed3\u6784-\u6027\u8d28\u5173\u7cfb\u3002"}}
{"id": "2508.01223", "pdf": "https://arxiv.org/pdf/2508.01223", "abs": "https://arxiv.org/abs/2508.01223", "authors": ["Changqing Xu", "Guoqing Sun", "Yi Liu", "Xinfang Liao", "Yintang Yang"], "title": "ParaRevSNN: A Parallel Reversible Spiking Neural Network for Efficient Training and Inference", "categories": ["cs.CV", "68T10", "I.4.6"], "comment": "8 pages, 3 figures, submitted to AAAI 2026", "summary": "Reversible Spiking Neural Networks (RevSNNs) enable memory-efficient training\nby reconstructing forward activations during backpropagation, but suffer from\nhigh latency due to strictly sequential computation. To overcome this\nlimitation, we propose ParaRevSNN, a parallel reversible SNN architecture that\ndecouples sequential dependencies between reversible blocks while preserving\nreversibility. This design enables inter-block parallelism, significantly\naccelerating training and inference while retaining the memory-saving benefits\nof reversibility. Experiments on CIFAR10, CIFAR100, CIFAR10-DVS, and DVS128\nGesture demonstrate that ParaRevSNN matches or exceeds the accuracy of standard\nRevSNNs, while reducing training time by up to 35.2\\% and inference time to\n18.15\\%, making it well-suited for deployment in resource-constrained\nscenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faParaRevSNN\uff0c\u4e00\u79cd\u5e76\u884c\u53ef\u9006\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u89e3\u8026\u6a21\u5757\u95f4\u4f9d\u8d56\uff0c\u663e\u8457\u52a0\u901f\u4e86\u53ef\u9006\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5185\u5b58\u6548\u7387\u548c\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u53ef\u9006\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08RevSNNs\uff09\u867d\u80fd\u901a\u8fc7\u524d\u5411\u6fc0\u6d3b\u91cd\u6784\u5b9e\u73b0\u5185\u5b58\u9ad8\u6548\u8bad\u7ec3\uff0c\u4f46\u7531\u4e8e\u4e25\u683c\u7684\u987a\u5e8f\u8ba1\u7b97\u800c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86ParaRevSNN\uff0c\u4e00\u79cd\u5e76\u884c\u53ef\u9006SNN\u67b6\u6784\u3002\u8be5\u8bbe\u8ba1\u901a\u8fc7\u89e3\u8026\u53ef\u9006\u5757\u4e4b\u95f4\u7684\u987a\u5e8f\u4f9d\u8d56\uff0c\u5728\u4fdd\u6301\u53ef\u9006\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5757\u95f4\u5e76\u884c\u8ba1\u7b97\uff0c\u4ece\u800c\u52a0\u901f\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08CIFAR10\u3001CIFAR100\u3001CIFAR10-DVS\u548cDVS128 Gesture\uff09\u4e0a\u8868\u660e\uff0cParaRevSNN\u7684\u51c6\u786e\u6027\u4e0e\u6807\u51c6RevSNNs\u6301\u5e73\u6216\u66f4\u9ad8\uff0c\u540c\u65f6\u8bad\u7ec3\u65f6\u95f4\u6700\u591a\u51cf\u5c1135.2%\uff0c\u63a8\u7406\u65f6\u95f4\u7f29\u77ed\u81f318.15%\uff0c\u5e76\u4fdd\u7559\u4e86\u53ef\u9006\u6027\u5e26\u6765\u7684\u5185\u5b58\u8282\u7701\u4f18\u52bf\u3002", "conclusion": "ParaRevSNN\u514b\u670d\u4e86\u4f20\u7edfRevSNNs\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u751a\u81f3\u8d85\u8d8a\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u3002\u8fd9\u4f7f\u5176\u975e\u5e38\u9002\u5408\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2508.01581", "pdf": "https://arxiv.org/pdf/2508.01581", "abs": "https://arxiv.org/abs/2508.01581", "authors": ["David Pearl", "Matthew Murphy", "James Intriligator"], "title": "Polymorphic Combinatorial Frameworks (PCF): Guiding the Design of Mathematically-Grounded, Adaptive AI Agents", "categories": ["cs.AI", "math.CO", "stat.CO"], "comment": null, "summary": "The Polymorphic Combinatorial Framework (PCF) leverages Large Language Models\n(LLMs) and mathematical frameworks to guide the meta-prompt enabled design of\nsolution spaces and adaptive AI agents for complex, dynamic environments.\nUnlike static agent architectures, PCF enables real-time parameter\nreconfiguration through mathematically-grounded combinatorial spaces, allowing\nagents to adapt their core behavioral traits dynamically. Grounded in\ncombinatorial logic, topos theory, and rough fuzzy set theory, PCF defines a\nmultidimensional SPARK parameter space (Skills, Personalities, Approaches,\nResources, Knowledge) to capture agent behaviors. This paper demonstrates how\nLLMs can parameterize complex spaces and estimate likely parameter\nvalues/variabilities. Using PCF, we parameterized mock caf\\'e domains (five\nlevels of complexity), estimated variables/variabilities, and conducted over\n1.25 million Monte Carlo simulations. The results revealed trends in agent\nadaptability and performance across the five complexity tiers, with diminishing\nreturns at higher complexity levels highlighting thresholds for scalable\ndesigns. PCF enables the generation of optimized agent configurations for\nspecific scenarios while maintaining logical consistency. This framework\nsupports scalable, dynamic, explainable, and ethical AI applications in domains\nlike customer service, healthcare, robotics, and collaborative systems, paving\nthe way for adaptable and cooperative next-generation polymorphic agents.", "AI": {"tldr": "\u591a\u6001\u7ec4\u5408\u6846\u67b6\uff08PCF\uff09\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u6570\u5b66\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u590d\u6742\u52a8\u6001\u73af\u5883\u8bbe\u8ba1\u81ea\u9002\u5e94AI\u667a\u80fd\u4f53\u3002\u901a\u8fc7\u5b9e\u65f6\u53c2\u6570\u91cd\u6784\uff0cPCF\u4f7f\u667a\u80fd\u4f53\u80fd\u52a8\u6001\u8c03\u6574\u5176\u6838\u5fc3\u884c\u4e3a\u7279\u6027\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\uff0c\u5e76\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684AI\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u7684AI\u667a\u80fd\u4f53\u67b6\u6784\u662f\u9759\u6001\u7684\uff0c\u96be\u4ee5\u9002\u5e94\u590d\u6742\u548c\u52a8\u6001\u7684\u73af\u5883\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff08PCF\uff09\uff0c\u80fd\u591f\u901a\u8fc7\u5b9e\u65f6\u53c2\u6570\u91cd\u6784\uff0c\u4f7fAI\u667a\u80fd\u4f53\u52a8\u6001\u8c03\u6574\u5176\u6838\u5fc3\u884c\u4e3a\u7279\u6027\uff0c\u4ece\u800c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u591a\u6001\u7ec4\u5408\u6846\u67b6\uff08PCF\uff09\uff0c\u5b83\u7ed3\u5408\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u6570\u5b66\u6846\u67b6\uff08\u5305\u62ec\u7ec4\u5408\u903b\u8f91\u3001\u62d3\u6251\u7406\u8bba\u548c\u7c97\u7cd9\u6a21\u7cca\u96c6\u7406\u8bba\uff09\u3002PCF\u5b9a\u4e49\u4e86\u4e00\u4e2a\u591a\u7ef4SPARK\u53c2\u6570\u7a7a\u95f4\uff08\u6280\u80fd\u3001\u4e2a\u6027\u3001\u65b9\u6cd5\u3001\u8d44\u6e90\u3001\u77e5\u8bc6\uff09\u6765\u6355\u6349\u667a\u80fd\u4f53\u884c\u4e3a\u3002\u7814\u7a76\u5229\u7528LLMs\u5bf9\u590d\u6742\u7a7a\u95f4\u8fdb\u884c\u53c2\u6570\u5316\u548c\u53c2\u6570\u503c/\u53d8\u5f02\u6027\u4f30\u8ba1\u3002\u901a\u8fc7\u5728\u6a21\u62df\u5496\u5561\u9986\u9886\u57df\uff08\u5206\u4e3a\u4e94\u4e2a\u590d\u6742\u7ea7\u522b\uff09\u7684\u5e94\u7528\uff0c\u8fdb\u884c\u4e86\u53c2\u6570\u5316\u548c\u53d8\u91cf\u4f30\u8ba1\uff0c\u5e76\u6267\u884c\u4e86\u8d85\u8fc7125\u4e07\u6b21\u8499\u7279\u5361\u6d1b\u6a21\u62df\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u5728\u4e94\u4e2a\u590d\u6742\u5ea6\u7ea7\u522b\u4e0a\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u8d8b\u52bf\u3002\u53d1\u73b0\u5728\u8f83\u9ad8\u590d\u6742\u5ea6\u7ea7\u522b\u5b58\u5728\u6536\u76ca\u9012\u51cf\u73b0\u8c61\uff0c\u8fd9\u7a81\u51fa\u4e86\u53ef\u6269\u5c55\u8bbe\u8ba1\u7684\u9608\u503c\u3002PCF\u88ab\u8bc1\u660e\u80fd\u591f\u4e3a\u7279\u5b9a\u573a\u666f\u751f\u6210\u4f18\u5316\u7684\u667a\u80fd\u4f53\u914d\u7f6e\uff0c\u540c\u65f6\u4fdd\u6301\u903b\u8f91\u4e00\u81f4\u6027\u3002", "conclusion": "PCF\u4e3a\u53ef\u6269\u5c55\u3001\u52a8\u6001\u3001\u53ef\u89e3\u91ca\u548c\u4f26\u7406\u7684AI\u5e94\u7528\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u9002\u7528\u4e8e\u5ba2\u6237\u670d\u52a1\u3001\u533b\u7597\u4fdd\u5065\u3001\u673a\u5668\u4eba\u548c\u534f\u4f5c\u7cfb\u7edf\u7b49\u9886\u57df\u3002\u8be5\u6846\u67b6\u4e3a\u5f00\u53d1\u9002\u5e94\u6027\u5f3a\u3001\u5408\u4f5c\u6027\u597d\u7684\u4e0b\u4e00\u4ee3\u591a\u6001\u667a\u80fd\u4f53\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.01832", "pdf": "https://arxiv.org/pdf/2508.01832", "abs": "https://arxiv.org/abs/2508.01832", "authors": ["Rubin Wei", "Jiaqi Cao", "Jiarui Wang", "Jushi Kai", "Qipeng Guo", "Bowen Zhou", "Zhouhan Lin"], "title": "MLP Memory: Language Modeling with Retriever-pretrained External Memory", "categories": ["cs.CL"], "comment": null, "summary": "While modern decoder-only LLMs achieve superior performance across various\ndomains, hallucinations have risen to be a common problem in their generated\ntext, hindering their application in knowledge-intensive tasks.\nRetriever-augmented generation (RAG) offers a solution, but the non-parametric\nnature of the retriever hinders its deep interaction with LLM. In this work, we\npropose to decouple memorization from the LLM decoder using a pretrained,\ndifferentiable external memory. The external memory is an MLP pretrained by\nimitating the behavior of a retriever on the entire pretraining dataset. Our\nresulting architecture, which comprises a transformer decoder and an external\nMLP memory pretrained on language modeling and retriever imitation\nrespectively, demonstrates strong perplexity and performance on downstream\ntasks. Experiments show our architecture exhibits steeper power-law scaling\nwith model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web\ndatasets compared to decoder-only models while benefiting from added training\nwithout overfitting. We demonstrate superior performance on three hallucination\nbenchmarks and nine memory-intensive tasks. Additionally, our approach delivers\n$80\\times$ speedup over $k$NN-LM (500M tokens) and $1.3\\times$ faster inference\nthan decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP\nmemory improves StrategyQA performance. We will open-source our code and models\nin the future.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408Transformer\u89e3\u7801\u5668\u548c\u9884\u8bad\u7ec3\u53ef\u5fae\u5206MLP\u5916\u90e8\u5185\u5b58\u7684\u65b0\u578bLLM\u67b6\u6784\uff0c\u65e8\u5728\u89e3\u51b3LLM\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3001\u63a8\u7406\u80fd\u529b\u53ca\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u4ee3\u89e3\u7801\u5668-only LLM\u867d\u7136\u6027\u80fd\u4f18\u8d8a\uff0c\u4f46\u5728\u751f\u6210\u6587\u672c\u4e2d\u666e\u904d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u963b\u788d\u5176\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709RAG\u65b9\u6848\u56e0\u68c0\u7d22\u5668\u7684\u975e\u53c2\u6570\u7279\u6027\uff0c\u96be\u4ee5\u4e0eLLM\u8fdb\u884c\u6df1\u5ea6\u4ea4\u4e92\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5c06LLM\u89e3\u7801\u5668\u7684\u8bb0\u5fc6\u529f\u80fd\u89e3\u8026\uff0c\u5f15\u5165\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u3001\u53ef\u5fae\u5206\u7684\u5916\u90e8MLP\u5185\u5b58\u3002\u8be5MLP\u901a\u8fc7\u6a21\u4eff\u68c0\u7d22\u5668\u5728\u6574\u4e2a\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u7684\u884c\u4e3a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u6700\u7ec8\u7684\u67b6\u6784\u7531Transformer\u89e3\u7801\u5668\u548c\u5916\u90e8MLP\u5185\u5b58\u7ec4\u6210\uff0c\u5206\u522b\u8fdb\u884c\u8bed\u8a00\u5efa\u6a21\u548c\u68c0\u7d22\u5668\u6a21\u4eff\u9884\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u67b6\u6784\u4e0e\u6a21\u578b\u89c4\u6a21\u5448\u66f4\u9661\u5ced\u7684\u5e42\u5f8b\u7f29\u653e\u5173\u7cfb\uff0c\u5728WikiText-103\u548cWeb\u6570\u636e\u96c6\u4e0a\u6bd4\u7eaf\u89e3\u7801\u5668\u6a21\u578b\u5206\u522b\u63d0\u534717.5%\u548c24.1%\uff0c\u4e14\u989d\u5916\u8bad\u7ec3\u4e0d\u5bfc\u81f4\u8fc7\u62df\u5408\u3002\u5728\u4e09\u4e2a\u5e7b\u89c9\u57fa\u51c6\u548c\u4e5d\u4e2a\u8bb0\u5fc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002\u63a8\u7406\u901f\u5ea6\u6bd4kNN-LM\u5feb80\u500d\uff0c\u6bd4\u7eaf\u89e3\u7801\u5668\u6a21\u578b\u5feb1.3\u500d\u3002\u4e0ekNN-LM\u635f\u5bb3\u63a8\u7406\u80fd\u529b\u76f8\u53cd\uff0c\u8be5MLP\u5185\u5b58\u63d0\u5347\u4e86StrategyQA\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u9884\u8bad\u7ec3\u7684\u53ef\u5fae\u5206\u5916\u90e8MLP\u5185\u5b58\uff0c\u672c\u7814\u7a76\u6210\u529f\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3001\u8bb0\u5fc6\u80fd\u529b\u548c\u63a8\u7406\u6548\u7387\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u3001\u9ad8\u6548\u7684LLM\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.01077", "pdf": "https://arxiv.org/pdf/2508.01077", "abs": "https://arxiv.org/abs/2508.01077", "authors": ["Johann Birnick"], "title": "The Lattice Geometry of Neural Network Quantization -- A Short Equivalence Proof of GPTQ and Babai's algorithm", "categories": ["cs.LG", "cs.AI", "I.2.6"], "comment": "9 pages, 4 figures", "summary": "We explain how data-driven quantization of a linear unit in a neural network\ncorresponds to solving the closest vector problem for a certain lattice\ngenerated by input data. We prove that the GPTQ algorithm is equivalent to\nBabai's well-known nearest-plane algorithm. We furthermore provide geometric\nintuition for both algorithms. Lastly, we note the consequences of these\nresults, in particular hinting at the possibility for using lattice basis\nreduction for better quantization.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u6570\u636e\u9a71\u52a8\u91cf\u5316\uff08\u7279\u522b\u662fGPTQ\uff09\u4e0e\u683c\u70b9\u95ee\u9898\uff08CVP\u548cBabai\u7b97\u6cd5\uff09\u7684\u6570\u5b66\u7b49\u4ef7\u6027\uff0c\u5e76\u6697\u793a\u53ef\u5229\u7528\u683c\u57fa\u7ea6\u5316\u6539\u8fdb\u91cf\u5316\u3002", "motivation": "\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u4e2d\u6570\u636e\u9a71\u52a8\u91cf\u5316\u7684\u5e95\u5c42\u6570\u5b66\u539f\u7406\uff0c\u5c06\u5176\u4e0e\u683c\u70b9\u7406\u8bba\u5efa\u7acb\u8054\u7cfb\uff0c\u5e76\u63a2\u7d22\u6f5c\u5728\u7684\u91cf\u5316\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u795e\u7ecf\u7f51\u7edc\u7ebf\u6027\u5355\u5143\u7684\u6570\u636e\u9a71\u52a8\u91cf\u5316\u5bf9\u5e94\u4e8e\u6c42\u89e3\u683c\u7684\u6700\u8fd1\u5411\u91cf\u95ee\u9898\uff0c\u5e76\u8bc1\u660e\u4e86GPTQ\u7b97\u6cd5\u4e0eBabai\u6700\u8fd1\u5e73\u9762\u7b97\u6cd5\u7684\u7b49\u4ef7\u6027\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u51e0\u4f55\u76f4\u89c2\u89e3\u91ca\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u795e\u7ecf\u7f51\u7edc\u7ebf\u6027\u5355\u5143\u7684\u6570\u636e\u9a71\u52a8\u91cf\u5316\u7b49\u540c\u4e8e\u6c42\u89e3\u7279\u5b9a\u683c\u7684\u6700\u8fd1\u5411\u91cf\u95ee\u9898\uff1b\u5e76\u8bc1\u660eGPTQ\u7b97\u6cd5\u4e0eBabai\u7684\u6700\u8fd1\u5e73\u9762\u7b97\u6cd5\u662f\u7b49\u4ef7\u7684\uff1b\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u63d0\u4f9b\u4e86\u4e24\u79cd\u7b97\u6cd5\u7684\u51e0\u4f55\u76f4\u89c2\u89e3\u91ca\u3002", "conclusion": "\u5c06\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u7406\u89e3\u4e3a\u683c\u70b9\u95ee\u9898\uff0c\u5e76\u8bc1\u660eGPTQ\u4e0eBabai\u7b97\u6cd5\u7684\u7b49\u4ef7\u6027\uff0c\u4e3a\u672a\u6765\u5229\u7528\u683c\u57fa\u7ea6\u5316\u7b49\u6280\u672f\u5b9e\u73b0\u66f4\u4f18\u7684\u91cf\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.01225", "pdf": "https://arxiv.org/pdf/2508.01225", "abs": "https://arxiv.org/abs/2508.01225", "authors": ["Xinyu Chen", "Haotian Zhai", "Can Zhang", "Xiupeng Shi", "Ruirui Li"], "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by ICCV 2025", "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance.", "AI": {"tldr": "\u9488\u5bf9\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff08TTA\uff09\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7f13\u5b58\u539f\u578b\u589e\u5f3a\u65b9\u6cd5\uff08MCP\u53ca\u5176\u6539\u8fdb\u7248MCP++\uff09\uff0c\u65e8\u5728\u901a\u8fc7\u786e\u4fdd\u7c7b\u5185\u7d27\u51d1\u6027\u6765\u63d0\u5347\u6a21\u578b\u5728\u672a\u77e5\u6d4b\u8bd5\u5206\u5e03\u4e0b\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7f13\u5b58\u589e\u5f3a\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff08TTA\uff09\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4f4e\u71b5\u51c6\u5219\u6765\u9009\u62e9\u6837\u672c\u6784\u5efa\u539f\u578b\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u53ef\u80fd\u4e0d\u53ef\u9760\uff0c\u5e76\u4e14\u65e0\u6cd5\u4fdd\u8bc1\u6709\u6548\u7684\u7c7b\u5185\u7d27\u51d1\u6027\uff0c\u4ece\u800c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u672c\u7814\u7a76\u53d1\u73b0\u7f13\u5b58\u589e\u5f3a\u6027\u80fd\u4e0e\u7c7b\u5185\u7d27\u51d1\u6027\u5448\u6b63\u76f8\u5173\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86\u591a\u7f13\u5b58\u539f\u578b\u589e\u5f3a\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\uff08MCP\uff09\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u7f13\u5b58\uff1a\u71b5\u7f13\u5b58\uff08\u7528\u4e8e\u4f4e\u71b5\u6837\u672c\u521d\u59cb\u5316\u539f\u578b\uff09\u3001\u5bf9\u9f50\u7f13\u5b58\uff08\u7528\u4e8e\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\uff0c\u5b9e\u73b0\u7c7b\u5185\u7d27\u51d1\u5206\u5e03\uff09\u548c\u8d1f\u7f13\u5b58\uff08\u7528\u4e8e\u9ad8\u71b5\u6837\u672c\u9884\u6d4b\u6821\u51c6\uff09\u3002\u6b64\u5916\uff0c\u8fdb\u4e00\u6b65\u5f00\u53d1\u4e86MCP++\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u878d\u5165\u4e86\u8de8\u6a21\u6001\u539f\u578b\u5bf9\u9f50\u548c\u6b8b\u5dee\u5b66\u4e60\uff0c\u5e76\u5f15\u5165\u4e86\u539f\u578b\u6b8b\u5dee\u5fae\u8c03\u3002", "result": "\u572815\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8fdb\u884c\u7684\u5bf9\u6bd4\u548c\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684MCP\u65b9\u6cd5\u548cMCP++\u6846\u67b6\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u591a\u7f13\u5b58\u7b56\u7565\u548c\u8de8\u6a21\u6001\u539f\u578b\u5bf9\u9f50\u673a\u5236\uff0c\u65b0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709TTA\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5bf9\u672a\u77e5\u5206\u5e03\u7684\u9002\u5e94\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2508.01623", "pdf": "https://arxiv.org/pdf/2508.01623", "abs": "https://arxiv.org/abs/2508.01623", "authors": ["Tadisetty Sai Yashwanth", "Dhatri C"], "title": "A Multi-Agent Pokemon Tournament for Evaluating Strategic Reasoning of Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "This research presents LLM Pokemon League, a competitive tournament system\nthat leverages Large Language Models (LLMs) as intelligent agents to simulate\nstrategic decision-making in Pok\\'emon battles. The platform is designed to\nanalyze and compare the reasoning, adaptability, and tactical depth exhibited\nby different LLMs in a type-based, turn-based combat environment. By\nstructuring the competition as a single-elimination tournament involving\ndiverse AI trainers, the system captures detailed decision logs, including\nteam-building rationale, action selection strategies, and switching decisions.\nThe project enables rich exploration into comparative AI behavior, battle\npsychology, and meta-strategy development in constrained, rule-based game\nenvironments. Through this system, we investigate how modern LLMs understand,\nadapt, and optimize decisions under uncertainty, making Pok\\'emon League a\nnovel benchmark for AI research in strategic reasoning and competitive\nlearning.", "AI": {"tldr": "LLM\u5b9d\u53ef\u68a6\u8054\u8d5b\uff1a\u4e00\u4e2a\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6a21\u62df\u5b9d\u53ef\u68a6\u5bf9\u6218\u5e76\u8bc4\u4f30\u5176\u6218\u7565\u63a8\u7406\u80fd\u529b\u7684\u7ade\u8d5b\u7cfb\u7edf\u3002", "motivation": "\u65e8\u5728\u5206\u6790\u548c\u6bd4\u8f83\u4e0d\u540c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7c7b\u578b\u514b\u5236\u3001\u56de\u5408\u5236\u6218\u6597\u4e2d\u8868\u73b0\u51fa\u7684\u63a8\u7406\u3001\u9002\u5e94\u6027\u548c\u6218\u672f\u6df1\u5ea6\uff0c\u5e76\u7814\u7a76LLM\u5982\u4f55\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7406\u89e3\u3001\u9002\u5e94\u5e76\u4f18\u5316\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u201cLLM\u5b9d\u53ef\u68a6\u8054\u8d5b\u201d\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u5229\u7528LLM\u4f5c\u4e3a\u667a\u80fd\u4ee3\u7406\u6765\u6a21\u62df\u5b9d\u53ef\u68a6\u6218\u7565\u5bf9\u6218\u7684\u7ade\u6280\u5e73\u53f0\u3002\u7cfb\u7edf\u4ee5\u5355\u6dd8\u6c70\u8d5b\u5f62\u5f0f\u7ec4\u7ec7\uff0c\u6d89\u53ca\u591a\u79cdAI\u8bad\u7ec3\u5e08\uff0c\u5e76\u6355\u83b7\u5305\u62ec\u961f\u4f0d\u6784\u5efa\u3001\u884c\u52a8\u9009\u62e9\u548c\u5207\u6362\u51b3\u7b56\u5728\u5185\u7684\u8be6\u7ec6\u51b3\u7b56\u65e5\u5fd7\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u6df1\u5165\u63a2\u7d22\u6bd4\u8f83\u6027AI\u884c\u4e3a\u3001\u5bf9\u6218\u5fc3\u7406\u5b66\u548c\u5143\u7b56\u7565\u53d1\u5c55\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7814\u7a76LLM\u7406\u89e3\u3001\u9002\u5e94\u548c\u4f18\u5316\u4e0d\u786e\u5b9a\u51b3\u7b56\u7684\u5e73\u53f0\u3002", "conclusion": "\u201cLLM\u5b9d\u53ef\u68a6\u8054\u8d5b\u201d\u7cfb\u7edf\u4e3aAI\u5728\u6218\u7565\u63a8\u7406\u548c\u7ade\u4e89\u6027\u5b66\u4e60\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u51c6\u3002"}}
{"id": "2508.01858", "pdf": "https://arxiv.org/pdf/2508.01858", "abs": "https://arxiv.org/abs/2508.01858", "authors": ["Yuhan Guo", "Cong Guo", "Aiwen Sun", "Hongliang He", "Xinyu Yang", "Yue Lu", "Yingji Zhang", "Xuntao Guo", "Dong Zhang", "Jianzhuang Liu", "Jiang Duan", "Yijia Xiao", "Liangjian Wen", "Hai-Ming Xu", "Yong Dai"], "title": "Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents", "categories": ["cs.CL", "cs.AI"], "comment": "Our code and data is open sourced at\n  https://github.com/Gnonymous/Web-CogReasoner", "summary": "Multimodal large-scale models have significantly advanced the development of\nweb agents, enabling perception and interaction with digital environments akin\nto human cognition. In this paper, we argue that web agents must first acquire\nsufficient knowledge to effectively engage in cognitive reasoning. Therefore,\nwe decompose a web agent's capabilities into two essential stages: knowledge\ncontent learning and cognitive processes. To formalize this, we propose\nWeb-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and\nProcedural. In this framework, knowledge content learning corresponds to the\nagent's processes of Memorizing and Understanding, which rely on the first two\nknowledge types, representing the \"what\" of learning. Conversely, cognitive\nprocesses correspond to Exploring, grounded in Procedural knowledge, defining\nthe \"how\" of reasoning and action. To facilitate knowledge acquisition, we\nconstruct the Web-CogDataset, a structured resource curated from 14 real-world\nwebsites, designed to systematically instill core knowledge necessary for web\nagent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon\nwhich comprehension is built-as well as the basis for learning how to reason\nand act. Building on this foundation, we operationalize these processes through\na novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing\nand training our proposed agent, the Web-CogReasoner. Extensive experimentation\nreveals its significant superiority over existing models, especially in\ngeneralizing to unseen tasks where structured knowledge is decisive. To enable\nrigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation\nsuite designed to assess and compare agent performance across the delineated\nknowledge domains and cognitive capabilities. Our code and data is open sourced\nat https://github.com/Gnonymous/Web-CogReasoner", "AI": {"tldr": "\u672c\u6587\u63d0\u51faWeb-CogKnowledge\u6846\u67b6\uff0c\u5c06\u7f51\u7edc\u4ee3\u7406\u80fd\u529b\u5206\u89e3\u4e3a\u77e5\u8bc6\u5b66\u4e60\u548c\u8ba4\u77e5\u8fc7\u7a0b\u3002\u4e3a\u6b64\uff0c\u6784\u5efa\u4e86Web-CogDataset\u7528\u4e8e\u77e5\u8bc6\u83b7\u53d6\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u77e5\u8bc6\u9a71\u52a8\u601d\u7ef4\u94fe\u7684Web-CogReasoner\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86Web-CogBench\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u7f51\u7edc\u4ee3\u7406\u5728\u8fdb\u884c\u8ba4\u77e5\u63a8\u7406\u524d\u9700\u83b7\u53d6\u5145\u8db3\u77e5\u8bc6\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0c\u6709\u6548\u8ba4\u77e5\u63a8\u7406\u7684\u524d\u63d0\u662f\u77e5\u8bc6\u50a8\u5907\uff0c\u56e0\u6b64\u5c06\u4ee3\u7406\u80fd\u529b\u89e3\u6784\u4e3a\u77e5\u8bc6\u5185\u5bb9\u5b66\u4e60\u4e0e\u8ba4\u77e5\u8fc7\u7a0b\u4e24\u9636\u6bb5\u3002", "method": "1. \u63d0\u51fa\u4e86Web-CogKnowledge\u6846\u67b6\uff0c\u5c06\u77e5\u8bc6\u5212\u5206\u4e3a\u4e8b\u5b9e\u3001\u6982\u5ff5\u3001\u7a0b\u5e8f\u4e09\u7c7b\uff0c\u5e76\u5bf9\u5e94\u4ee3\u7406\u7684\u8bb0\u5fc6\u3001\u7406\u89e3\u3001\u63a2\u7d22\u80fd\u529b\u30022. \u6784\u5efa\u4e86Web-CogDataset\uff0c\u4e00\u4e2a\u5305\u542b14\u4e2a\u771f\u5b9e\u7f51\u7ad9\u6570\u636e\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u8d44\u6e90\uff0c\u7528\u4e8e\u4ee3\u7406\u7684\u77e5\u8bc6\u5b66\u4e60\u30023. \u57fa\u4e8e\u6b64\uff0c\u5f00\u53d1\u4e86Web-CogReasoner\uff0c\u4e00\u4e2a\u5229\u7528\u65b0\u9896\u7684\u77e5\u8bc6\u9a71\u52a8\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u6846\u67b6\u7684\u7f51\u7edc\u4ee3\u7406\u30024. \u8bbe\u8ba1Web-CogBench\uff0c\u4e00\u4e2a\u7efc\u5408\u8bc4\u4f30\u5957\u4ef6\uff0c\u4ee5\u5168\u9762\u8861\u91cf\u4ee3\u7406\u5728\u4e0d\u540c\u77e5\u8bc6\u57df\u548c\u8ba4\u77e5\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cWeb-CogReasoner\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5c24\u5176\u5728\u6cdb\u5316\u5230\u9700\u8981\u7ed3\u6784\u5316\u77e5\u8bc6\u7684\u964c\u751f\u4efb\u52a1\u65f6\uff0c\u5176\u4f18\u52bf\u66f4\u4e3a\u660e\u663e\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u77e5\u8bc6\u9a71\u52a8\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u4ee3\u7406\u7684\u8ba4\u77e5\u63a8\u7406\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5f3a\u8c03\u4e86\u77e5\u8bc6\u83b7\u53d6\u5bf9\u7f51\u7edc\u4ee3\u7406\u6709\u6548\u6267\u884c\u8ba4\u77e5\u4efb\u52a1\u7684\u5173\u952e\u4f5c\u7528\u3002\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u5747\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.01101", "pdf": "https://arxiv.org/pdf/2508.01101", "abs": "https://arxiv.org/abs/2508.01101", "authors": ["Siddharth Rout", "Eldad Haber", "Stephane Gaudreault"], "title": "Flow Matching for Probabilistic Learning of Dynamical Systems from Missing or Noisy Data", "categories": ["cs.LG", "math.DS", "physics.comp-ph"], "comment": "arXiv admin note: text overlap with arXiv:2503.12273", "summary": "Learning dynamical systems is crucial across many fields, yet applying\nmachine learning techniques remains challenging due to missing variables and\nnoisy data. Classical mathematical models often struggle in these scenarios due\nto the arose ill-posedness of the physical systems. Stochastic machine learning\ntechniques address this challenge by enabling the modeling of such ill-posed\nproblems. Thus, a single known input to the trained machine learning model may\nyield multiple plausible outputs, and all of the outputs are correct. In such\nscenarios, probabilistic forecasting is inherently meaningful. In this study,\nwe introduce a variant of flow matching for probabilistic forecasting which\nestimates possible future states as a distribution over possible outcomes\nrather than a single-point prediction. Perturbation of complex dynamical states\nis not trivial. Community uses typical Gaussian or uniform perturbations to\ncrucial variables to model uncertainty. However, not all variables behave in a\nGaussian fashion. So, we also propose a generative machine learning approach to\nphysically and logically perturb the states of complex high-dimensional\ndynamical systems. Finally, we establish the mathematical foundations of our\nmethod and demonstrate its effectiveness on several challenging dynamical\nsystems, including a variant of the high-dimensional WeatherBench dataset,\nwhich models the global weather at a 5.625{\\deg} meridional resolution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6d41\u5339\u914d\u53d8\u4f53\uff0c\u7528\u4e8e\u5bf9\u5b58\u5728\u7f3a\u5931\u53d8\u91cf\u548c\u566a\u58f0\u6570\u636e\u7684\u9ad8\u7ef4\u590d\u6742\u52a8\u529b\u7cfb\u7edf\u8fdb\u884c\u6982\u7387\u9884\u6d4b\uff0c\u5e76\u5f15\u5165\u751f\u6210\u5f0f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6765\u6a21\u62df\u7269\u7406\u6270\u52a8\u3002", "motivation": "\u5728\u53d8\u91cf\u7f3a\u5931\u548c\u6570\u636e\u566a\u58f0\u60c5\u666f\u4e0b\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u5b66\u4e60\u52a8\u529b\u7cfb\u7edf\u9762\u4e34\u6311\u6218\uff0c\u7ecf\u5178\u6a21\u578b\u96be\u4ee5\u5904\u7406\u75c5\u6001\u95ee\u9898\u3002\u968f\u673a\u673a\u5668\u5b66\u4e60\u548c\u6982\u7387\u9884\u6d4b\u5728\u6b64\u7c7b\u95ee\u9898\u4e2d\u66f4\u5177\u610f\u4e49\uff0c\u56e0\u5176\u80fd\u5904\u7406\u591a\u91cd\u5408\u7406\u8f93\u51fa\u3002\u6b64\u5916\uff0c\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4e2d\u7684\u7b80\u5355\u6270\u52a8\u65b9\u5f0f\uff08\u5982\u9ad8\u65af/\u5747\u5300\uff09\u4e0d\u8db3\u4ee5\u51c6\u786e\u6a21\u62df\u6240\u6709\u53d8\u91cf\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\uff1a1. \u4e00\u79cd\u6d41\u5339\u914d\uff08flow matching\uff09\u53d8\u4f53\uff0c\u7528\u4e8e\u6982\u7387\u9884\u6d4b\uff0c\u5c06\u672a\u6765\u72b6\u6001\u4f30\u8ba1\u4e3a\u53ef\u80fd\u7ed3\u679c\u7684\u5206\u5e03\uff1b2. \u4e00\u79cd\u751f\u6210\u5f0f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u7269\u7406\u4e14\u903b\u8f91\u5730\u6270\u52a8\u9ad8\u7ef4\u590d\u6742\u52a8\u529b\u7cfb\u7edf\u7684\u72b6\u6001\uff1b3. \u5efa\u7acb\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6570\u5b66\u57fa\u7840\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u5305\u62ec\u9ad8\u7ef4WeatherBench\u6570\u636e\u96c6\u5728\u5185\u7684\u591a\u4e2a\u6311\u6218\u6027\u52a8\u529b\u7cfb\u7edf\u4e0a\u5c55\u73b0\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6d41\u5339\u914d\u53d8\u4f53\u548c\u751f\u6210\u5f0f\u6270\u52a8\u65b9\u6cd5\u4e3a\u5904\u7406\u5b58\u5728\u7f3a\u5931\u53d8\u91cf\u548c\u566a\u58f0\u6570\u636e\u7684\u590d\u6742\u3001\u75c5\u6001\u52a8\u529b\u7cfb\u7edf\u7684\u6982\u7387\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01227", "pdf": "https://arxiv.org/pdf/2508.01227", "abs": "https://arxiv.org/abs/2508.01227", "authors": ["Zihan Fang", "Zhiyong Xu", "Lan Du", "Shide Du", "Zhiling Cai", "Shiping Wang"], "title": "Enhancing Multi-view Open-set Learning via Ambiguity Uncertainty Calibration and View-wise Debiasing", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Existing multi-view learning models struggle in open-set scenarios due to\ntheir implicit assumption of class completeness. Moreover, static view-induced\nbiases, which arise from spurious view-label associations formed during\ntraining, further degrade their ability to recognize unknown categories. In\nthis paper, we propose a multi-view open-set learning framework via ambiguity\nuncertainty calibration and view-wise debiasing. To simulate ambiguous samples,\nwe design O-Mix, a novel synthesis strategy to generate virtual samples with\ncalibrated open-set ambiguity uncertainty. These samples are further processed\nby an auxiliary ambiguity perception network that captures atypical patterns\nfor improved open-set adaptation. Furthermore, we incorporate an HSIC-based\ncontrastive debiasing module that enforces independence between view-specific\nambiguous and view-consistent representations, encouraging the model to learn\ngeneralizable features. Extensive experiments on diverse multi-view benchmarks\ndemonstrate that the proposed framework consistently enhances unknown-class\nrecognition while preserving strong closed-set performance.", "AI": {"tldr": "\u73b0\u6709\u591a\u89c6\u56fe\u5b66\u4e60\u5728\u5f00\u653e\u96c6\u573a\u666f\u4e0b\u53d7\u7c7b\u522b\u5b8c\u6574\u6027\u5047\u8bbe\u548c\u89c6\u56fe\u504f\u5dee\u5f71\u54cd\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u6b67\u4e49\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u548c\u89c6\u56fe\u53bb\u504f\u89c1\u7684\u591a\u89c6\u56fe\u5f00\u653e\u96c6\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u672a\u77e5\u7c7b\u522b\u8bc6\u522b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u591a\u89c6\u56fe\u5b66\u4e60\u6a21\u578b\u5728\u5f00\u653e\u96c6\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u539f\u56e0\u5728\u4e8e\u5176\u9690\u5f0f\u5047\u8bbe\u7c7b\u522b\u5b8c\u6574\u6027\u3002\u6b64\u5916\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f62\u6210\u7684\u9759\u6001\u89c6\u56fe\u8bf1\u5bfc\u504f\u5dee\u4f1a\u8fdb\u4e00\u6b65\u964d\u4f4e\u6a21\u578b\u8bc6\u522b\u672a\u77e5\u7c7b\u522b\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u591a\u89c6\u56fe\u5f00\u653e\u96c6\u5b66\u4e60\u6846\u67b6\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1) \u8bbe\u8ba1O-Mix\uff0c\u4e00\u79cd\u65b0\u7684\u5408\u6210\u7b56\u7565\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u6821\u51c6\u5f00\u653e\u96c6\u6b67\u4e49\u4e0d\u786e\u5b9a\u6027\u7684\u865a\u62df\u6837\u672c\uff1b2) \u5229\u7528\u8f85\u52a9\u6b67\u4e49\u611f\u77e5\u7f51\u7edc\u5904\u7406\u8fd9\u4e9b\u6837\u672c\uff0c\u4ee5\u6355\u83b7\u975e\u5178\u578b\u6a21\u5f0f\u5e76\u6539\u8fdb\u5f00\u653e\u96c6\u9002\u5e94\u6027\uff1b3) \u5f15\u5165\u57fa\u4e8eHSIC\u7684\u5bf9\u6bd4\u53bb\u504f\u89c1\u6a21\u5757\uff0c\u5f3a\u5236\u89c6\u56fe\u7279\u5f02\u6027\u6b67\u4e49\u8868\u793a\u4e0e\u89c6\u56fe\u4e00\u81f4\u6027\u8868\u793a\u72ec\u7acb\uff0c\u4ece\u800c\u4fc3\u4f7f\u6a21\u578b\u5b66\u4e60\u66f4\u5177\u6cdb\u5316\u6027\u7684\u7279\u5f81\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u591a\u89c6\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u6301\u7eed\u589e\u5f3a\u4e86\u672a\u77e5\u7c7b\u522b\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u95ed\u96c6\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u5b66\u4e60\u5728\u5f00\u653e\u96c6\u573a\u666f\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u901a\u8fc7\u6821\u51c6\u6b67\u4e49\u4e0d\u786e\u5b9a\u6027\u548c\u89c6\u56fe\u53bb\u504f\u89c1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u672a\u77e5\u7c7b\u522b\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u5df2\u77e5\u7c7b\u522b\u7684\u8bc6\u522b\u7cbe\u5ea6\uff0c\u4e3a\u5f00\u653e\u96c6\u591a\u89c6\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01670", "pdf": "https://arxiv.org/pdf/2508.01670", "abs": "https://arxiv.org/abs/2508.01670", "authors": ["Jiaqing Xie", "Weida Wang", "Ben Gao", "Zhuo Yang", "Haiyuan Wan", "Shufei Zhang", "Tianfan Fu", "Yuqiang Li"], "title": "QCBench: Evaluating Large Language Models on Domain-Specific Quantitative Chemistry", "categories": ["cs.AI", "physics.chem-ph"], "comment": "13 pages, 8 figures", "summary": "Quantitative chemistry plays a fundamental role in chemistry research,\nenabling precise predictions of molecular properties, reaction outcomes, and\nmaterial behaviors. While large language models (LLMs) have shown promise in\nchemistry-related tasks, their ability to perform rigorous, step-by-step\nquantitative reasoning remains underexplored. To fill this blank, we propose\nQCBench, a Quantitative Chemistry benchmark comprising 350 computational\nchemistry problems across 7 chemistry subfields (analytical chemistry,\nbio/organic chemistry, general chemistry, inorganic chemistry, physical\nchemistry, polymer chemistry and quantum chemistry), categorized into three\nhierarchical tiers-basic, intermediate, and expert-to systematically evaluate\nthe mathematical reasoning abilities of large language models (LLMs). Designed\nto minimize shortcuts and emphasize stepwise numerical reasoning, each problem\nfocuses on pure calculations rooted in real-world chemical vertical fields.\nQCBench enables fine-grained diagnosis of computational weaknesses, reveals\nmodel-specific limitations across difficulty levels, and lays the groundwork\nfor future improvements such as domain adaptive fine-tuning or multi-modal\nintegration. Evaluations on 19 LLMs demonstrate a consistent performance\ndegradation with increasing task complexity, highlighting the current gap\nbetween language fluency and scientific computation accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86QCBench\uff0c\u4e00\u4e2a\u91cf\u5316\u5316\u5b66\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u91cf\u5316\u63a8\u7406\u80fd\u529b\uff0c\u7814\u7a76\u53d1\u73b0LLMs\u5728\u590d\u6742\u91cf\u5316\u5316\u5b66\u8ba1\u7b97\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5316\u5b66\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u6267\u884c\u4e25\u8c28\u3001\u5206\u6b65\u91cf\u5316\u63a8\u7406\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u5b58\u5728\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86QCBench\uff0c\u4e00\u4e2a\u91cf\u5316\u5316\u5b66\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5305\u542b350\u4e2a\u6765\u81ea7\u4e2a\u5316\u5b66\u5b50\u9886\u57df\uff08\u5206\u6790\u3001\u751f\u7269/\u6709\u673a\u3001\u666e\u901a\u3001\u65e0\u673a\u3001\u7269\u7406\u3001\u9ad8\u5206\u5b50\u548c\u91cf\u5b50\u5316\u5b66\uff09\u7684\u8ba1\u7b97\u95ee\u9898\u3002\u8fd9\u4e9b\u95ee\u9898\u88ab\u5212\u5206\u4e3a\u57fa\u7840\u3001\u4e2d\u7ea7\u548c\u4e13\u5bb6\u4e09\u4e2a\u96be\u5ea6\u5c42\u7ea7\uff0c\u65e8\u5728\u5f3a\u8c03\u5206\u6b65\u6570\u503c\u63a8\u7406\uff0c\u5e76\u907f\u514d\u6377\u5f84\u3002", "result": "\u5bf919\u4e2aLLMs\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u968f\u7740\u4efb\u52a1\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u6a21\u578b\u6027\u80fd\u6301\u7eed\u4e0b\u964d\uff0c\u8fd9\u63ed\u793a\u4e86LLMs\u5728\u8bed\u8a00\u6d41\u7545\u6027\u4e0e\u79d1\u5b66\u8ba1\u7b97\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002QCBench\u80fd\u591f\u7ec6\u81f4\u8bca\u65ad\u8ba1\u7b97\u5f31\u70b9\uff0c\u5e76\u63ed\u793a\u6a21\u578b\u5728\u4e0d\u540c\u96be\u5ea6\u7ea7\u522b\u4e0a\u7684\u5177\u4f53\u5c40\u9650\u6027\u3002", "conclusion": "LLMs\u5728\u91cf\u5316\u5316\u5b66\u8ba1\u7b97\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u3002\u672c\u7814\u7a76\u901a\u8fc7QCBench\u8bca\u65ad\u4e86\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u901a\u8fc7\u9886\u57df\u81ea\u9002\u5e94\u5fae\u8c03\u6216\u591a\u6a21\u6001\u96c6\u6210\u7b49\u65b9\u6cd5\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.01862", "pdf": "https://arxiv.org/pdf/2508.01862", "abs": "https://arxiv.org/abs/2508.01862", "authors": ["Yijun Feng"], "title": "Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models have demonstrated remarkable capabilities across\ndiverse tasks, yet they frequently generate hallucinations outputs that are\nfluent but factually incorrect or unsupported. We propose Counterfactual\nProbing, a novel approach for detecting and mitigating hallucinations in LLM\noutputs. Our method dynamically generates counterfactual statements that appear\nplausible but contain subtle factual errors, then evaluates the model's\nsensitivity to these perturbations. We hypothesize that genuine knowledge\nexhibits robustness to counterfactual variations, while hallucinated content\nshows inconsistent confidence patterns when confronted with plausible\nalternatives. Our comprehensive evaluation on TruthfulQA, factual statement\ndatasets, and curated hallucination examples demonstrates that counterfactual\nprobing achieves superior detection performance compared to baseline methods,\nwhile our adaptive mitigation strategies reduce hallucination scores by an\naverage of 24.5%. The approach requires no model retraining and can be\nintegrated into existing LLM pipelines as a realtime verification mechanism.", "AI": {"tldr": "\u63d0\u51fa\u201c\u53cd\u4e8b\u5b9e\u63a2\u6d4b\u201d\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5e7b\u89c9\u8f93\u51fa\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5404\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7ecf\u5e38\u4ea7\u751f\u6d41\u7545\u4f46\u4e8b\u5b9e\u4e0d\u7b26\u6216\u65e0\u6839\u636e\u7684\u201c\u5e7b\u89c9\u201d\u8f93\u51fa\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u201c\u53cd\u4e8b\u5b9e\u63a2\u6d4b\u201d\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u5305\u542b\u7ec6\u5fae\u4e8b\u5b9e\u9519\u8bef\u7684\u53cd\u4e8b\u5b9e\u9648\u8ff0\uff0c\u8bc4\u4f30\u6a21\u578b\u5bf9\u8fd9\u4e9b\u6270\u52a8\u7684\u654f\u611f\u6027\u3002\u5176\u5047\u8bbe\u662f\u771f\u5b9e\u77e5\u8bc6\u5bf9\u53cd\u4e8b\u5b9e\u53d8\u4f53\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u800c\u5e7b\u89c9\u5185\u5bb9\u5728\u9762\u5bf9\u770b\u4f3c\u5408\u7406\u7684\u66ff\u4ee3\u65b9\u6848\u65f6\u4f1a\u663e\u793a\u51fa\u4e0d\u4e00\u81f4\u7684\u7f6e\u4fe1\u5ea6\u6a21\u5f0f\u3002", "result": "\u5728TruthfulQA\u3001\u4e8b\u5b9e\u9648\u8ff0\u6570\u636e\u96c6\u548c\u7cbe\u9009\u5e7b\u89c9\u793a\u4f8b\u4e0a\u7684\u5168\u9762\u8bc4\u4f30\u8868\u660e\uff0c\u53cd\u4e8b\u5b9e\u63a2\u6d4b\u5b9e\u73b0\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u5176\u81ea\u9002\u5e94\u7f13\u89e3\u7b56\u7565\u5e73\u5747\u5c06\u5e7b\u89c9\u5206\u6570\u964d\u4f4e\u4e8624.5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u6a21\u578b\u518d\u8bad\u7ec3\uff0c\u53ef\u4f5c\u4e3a\u5b9e\u65f6\u9a8c\u8bc1\u673a\u5236\u96c6\u6210\u5230\u73b0\u6709LLM\u7ba1\u9053\u4e2d\u3002"}}
{"id": "2508.01105", "pdf": "https://arxiv.org/pdf/2508.01105", "abs": "https://arxiv.org/abs/2508.01105", "authors": ["Md Sultanul Islam Ovi", "Jamal Hossain", "Md Raihan Alam Rahi", "Fatema Akter"], "title": "Protecting Student Mental Health with a Context-Aware Machine Learning Framework for Stress Monitoring", "categories": ["cs.LG", "cs.AI", "cs.CY"], "comment": "6 pages, 3 figures, 3 tables, 1 algorithm. Conference paper", "summary": "Student mental health is an increasing concern in academic institutions,\nwhere stress can severely impact well-being and academic performance.\nTraditional assessment methods rely on subjective surveys and periodic\nevaluations, offering limited value for timely intervention. This paper\nintroduces a context-aware machine learning framework for classifying student\nstress using two complementary survey-based datasets covering psychological,\nacademic, environmental, and social factors. The framework follows a six-stage\npipeline involving preprocessing, feature selection (SelectKBest, RFECV),\ndimensionality reduction (PCA), and training with six base classifiers: SVM,\nRandom Forest, Gradient Boosting, XGBoost, AdaBoost, and Bagging. To enhance\nperformance, we implement ensemble strategies, including hard voting, soft\nvoting, weighted voting, and stacking. Our best models achieve 93.09% accuracy\nwith weighted hard voting on the Student Stress Factors dataset and 99.53% with\nstacking on the Stress and Well-being dataset, surpassing previous benchmarks.\nThese results highlight the potential of context-integrated, data-driven\nsystems for early stress detection and underscore their applicability in\nreal-world academic settings to support student well-being.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u8c03\u67e5\u95ee\u5377\u6570\u636e\u5bf9\u5b66\u751f\u538b\u529b\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u7684\u51c6\u786e\u7387\uff0c\u5c55\u73b0\u4e86\u5176\u5728\u65e9\u671f\u538b\u529b\u68c0\u6d4b\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u5b66\u751f\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u538b\u529b\u4e25\u91cd\u5f71\u54cd\u5176\u5e78\u798f\u611f\u548c\u5b66\u4e1a\u8868\u73b0\u3002\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4e3b\u89c2\u8c03\u67e5\u548c\u5468\u671f\u6027\u8bc4\u4f30\uff0c\u96be\u4ee5\u63d0\u4f9b\u53ca\u65f6\u5e72\u9884\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u3001\u53ca\u65f6\u7684\u538b\u529b\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u6db5\u76d6\u5fc3\u7406\u3001\u5b66\u4e1a\u3001\u73af\u5883\u548c\u793e\u4ea4\u56e0\u7d20\u7684\u4e24\u4e2a\u4e92\u8865\u8c03\u67e5\u6570\u636e\u96c6\u3002\u6846\u67b6\u5305\u542b\u516d\u4e2a\u9636\u6bb5\uff1a\u6570\u636e\u9884\u5904\u7406\u3001\u7279\u5f81\u9009\u62e9\uff08SelectKBest, RFECV\uff09\u3001\u964d\u7ef4\uff08PCA\uff09\uff0c\u5e76\u4f7f\u7528\u516d\u79cd\u57fa\u7840\u5206\u7c7b\u5668\uff08SVM, Random Forest, Gradient Boosting, XGBoost, AdaBoost, Bagging\uff09\u8fdb\u884c\u8bad\u7ec3\u3002\u4e3a\u63d0\u5347\u6027\u80fd\uff0c\u8fd8\u91c7\u7528\u4e86\u786c\u6295\u7968\u3001\u8f6f\u6295\u7968\u3001\u52a0\u6743\u6295\u7968\u548c\u5806\u53e0\u7b49\u96c6\u6210\u7b56\u7565\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728\u201c\u5b66\u751f\u538b\u529b\u56e0\u7d20\u201d\u6570\u636e\u96c6\u4e0a\u901a\u8fc7\u52a0\u6743\u786c\u6295\u7968\u5b9e\u73b0\u4e8693.09%\u7684\u51c6\u786e\u7387\uff0c\u5728\u201c\u538b\u529b\u4e0e\u5e78\u798f\u611f\u201d\u6570\u636e\u96c6\u4e0a\u901a\u8fc7\u5806\u53e0\u5b9e\u73b0\u4e8699.53%\u7684\u51c6\u786e\u7387\uff0c\u5747\u8d85\u8d8a\u4e86\u6b64\u524d\u7684\u57fa\u51c6\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684\u7cfb\u7edf\u5728\u65e9\u671f\u538b\u529b\u68c0\u6d4b\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u5e76\u5f3a\u8c03\u4e86\u5b83\u4eec\u5728\u5b9e\u9645\u5b66\u672f\u73af\u5883\u4e2d\u652f\u6301\u5b66\u751f\u798f\u7949\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2508.01236", "pdf": "https://arxiv.org/pdf/2508.01236", "abs": "https://arxiv.org/abs/2508.01236", "authors": ["Mingyu Fu", "Wei Suo", "Ji Ma", "Lin Yuanbo Wu", "Peng Wang", "Yanning Zhang"], "title": "Mitigating Information Loss under High Pruning Rates for Efficient Large Vision Language Models", "categories": ["cs.CV"], "comment": "accepted by ACM MM 2025", "summary": "Despite the great success of Large Vision Language Models (LVLMs), their high\ncomputational cost severely limits their broad applications. The computational\ncost of LVLMs mainly stems from the visual sequence of the input, which\nconsists of hundreds or even thousands of tokens. Although existing methods\nhave made progress by removing redundant tokens, they suffer from severe\nperformance degradation with high pruning rates due to the loss of visual\ninformation. In this paper, we propose an Adaptive Content Compensation Method\n(ACCM), which can effectively mitigate the visual information loss via an image\ncaption. Specifically, ACCM comprises two key components: a lightweight caption\nmodel and a selector. Firstly the caption model generates question-related\ndescriptions under the guidance of the user instruction. Then the selector\nfurther identifies a contextually appropriate caption from multiple candidates.\nLeveraging self-supervised learning, our modules could be learned efficiently\nwithout any human or automated labeling. We conduct extensive experiments\nacross seven benchmarks and the results show that ACCM significantly\noutperforms existing methods with lower FLOPs (e.g., surpassing SOTA by 20.6%\nwith 6.5% fewer FLOPs).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u5185\u5bb9\u8865\u507f\u65b9\u6cd5\uff08ACCM\uff09\uff0c\u901a\u8fc7\u56fe\u50cf\u5b57\u5e55\u6709\u6548\u7f13\u89e3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u89c6\u89c9\u4fe1\u606f\u635f\u5931\uff0c\u4ece\u800c\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4e3b\u8981\u6e90\u4e8e\u8f93\u5165\u89c6\u89c9\u5e8f\u5217\u4e2d\u7684\u5927\u91cfToken\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u79fb\u9664\u5197\u4f59Token\u6765\u964d\u4f4e\u6210\u672c\uff0c\u4f46\u5728\u9ad8\u526a\u679d\u7387\u4e0b\u4f1a\u5bfc\u81f4\u89c6\u89c9\u4fe1\u606f\u4e22\u5931\uff0c\u4ece\u800c\u9020\u6210\u4e25\u91cd\u7684\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u81ea\u9002\u5e94\u5185\u5bb9\u8865\u507f\u65b9\u6cd5\uff08ACCM\uff09\uff0c\u901a\u8fc7\u56fe\u50cf\u5b57\u5e55\u6709\u6548\u7f13\u89e3\u89c6\u89c9\u4fe1\u606f\u635f\u5931\u3002ACCM\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5b57\u5e55\u6a21\u578b\u548c\u4e00\u4e2a\u9009\u62e9\u5668\u3002\u5b57\u5e55\u6a21\u578b\u6839\u636e\u7528\u6237\u6307\u4ee4\u751f\u6210\u4e0e\u95ee\u9898\u76f8\u5173\u7684\u63cf\u8ff0\uff0c\u800c\u9009\u62e9\u5668\u4ece\u591a\u4e2a\u5019\u9009\u5b57\u5e55\u4e2d\u8bc6\u522b\u51fa\u4e0a\u4e0b\u6587\u5408\u9002\u7684\u5b57\u5e55\u3002\u6240\u6709\u6a21\u5757\u5747\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u9ad8\u6548\u8bad\u7ec3\uff0c\u65e0\u9700\u4efb\u4f55\u4eba\u5de5\u6216\u81ea\u52a8\u5316\u6807\u6ce8\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cACCM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5177\u6709\u66f4\u4f4e\u7684FLOPs\uff08\u4f8b\u5982\uff0c\u6027\u80fd\u8d85\u8d8aSOTA 20.6%\uff0c\u540c\u65f6FLOPs\u51cf\u5c116.5%\uff09\u3002", "conclusion": "ACCM\u901a\u8fc7\u5f15\u5165\u81ea\u9002\u5e94\u5185\u5bb9\u8865\u507f\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86LVLMs\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u4fe1\u606f\u4e22\u5931\u7684\u75db\u70b9\uff0c\u5b9e\u73b0\u4e86\u5728\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3aLVLMs\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01680", "pdf": "https://arxiv.org/pdf/2508.01680", "abs": "https://arxiv.org/abs/2508.01680", "authors": ["Dong Li", "Yichen Niu", "Ying Ai", "Xiang Zou", "Biqing Qi", "Jianxing Liu"], "title": "T-GRAG: A Dynamic GraphRAG Framework for Resolving Temporal Conflicts and Redundancy in Knowledge Retrieval", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance in natural\nlanguage generation but remain limited in knowle-\n  dge-intensive tasks due to outdated or incomplete internal knowledge.\nRetrieval-Augmented Generation (RAG) addresses this by incorporating external\nretrieval, with GraphRAG further enhancing performance through structured\nknowledge graphs and multi-hop reasoning. However, existing GraphRAG methods\nlargely ignore the temporal dynamics of knowledge, leading to issues such as\ntemporal ambiguity, time-insensitive retrieval, and semantic redundancy. To\novercome these limitations, we propose Temporal GraphRAG (T-GRAG), a dynamic,\ntemporally-aware RAG framework that models the evolution of knowledge over\ntime. T-GRAG consists of five key components: (1) a Temporal Knowledge Graph\nGenerator that creates time-stamped, evolving graph structures; (2) a Temporal\nQuery Decomposition mechanism that breaks complex temporal queries into\nmanageable sub-queries; (3) a Three-layer Interactive Retriever that\nprogressively filters and refines retrieval across temporal subgraphs; (4) a\nSource Text Extractor to mitigate noise; and (5) a LLM-based Generator that\nsynthesizes contextually and temporally accurate responses. We also introduce\nTime-LongQA, a novel benchmark dataset based on real-world corporate annual\nreports, designed to test temporal reasoning across evolving knowledge.\nExtensive experiments show that T-GRAG significantly outperforms prior RAG and\nGraphRAG baselines in both retrieval accuracy and response relevance under\ntemporal constraints, highlighting the necessity of modeling knowledge\nevolution for robust long-text question answering. Our code is publicly\navailable on the T-GRAG", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86T-GRAG\uff0c\u4e00\u4e2a\u52a8\u6001\u3001\u65f6\u95f4\u611f\u77e5\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31RAG\u65b9\u6cd5\u5ffd\u7565\u77e5\u8bc6\u65f6\u95f4\u52a8\u6001\u6027\u5bfc\u81f4\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5efa\u6a21\u77e5\u8bc6\u6f14\u5316\u663e\u8457\u63d0\u5347\u4e86\u5728\u65f6\u95f4\u7ea6\u675f\u4e0b\u957f\u6587\u672c\u95ee\u7b54\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u9762\u4e34\u5185\u90e8\u77e5\u8bc6\u8fc7\u65f6\u6216\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\u3002\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u77e5\u8bc6\u56fe\u8c31RAG\uff08GraphRAG\uff09\u867d\u7136\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u5b83\u4eec\u666e\u904d\u5ffd\u7565\u4e86\u77e5\u8bc6\u7684\u65f6\u95f4\u52a8\u6001\u6027\uff0c\u5bfc\u81f4\u65f6\u95f4\u6a21\u7cca\u3001\u68c0\u7d22\u4e0d\u654f\u611f\u548c\u8bed\u4e49\u5197\u4f59\u7b49\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86Temporal GraphRAG (T-GRAG) \u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) \u65f6\u95f4\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u5668\uff0c\u521b\u5efa\u5e26\u65f6\u95f4\u6233\u7684\u6f14\u5316\u56fe\u7ed3\u6784\uff1b2) \u65f6\u95f4\u67e5\u8be2\u5206\u89e3\u673a\u5236\uff0c\u5206\u89e3\u590d\u6742\u7684\u65f6\u95f4\u67e5\u8be2\uff1b3) \u4e09\u5c42\u4ea4\u4e92\u5f0f\u68c0\u7d22\u5668\uff0c\u5728\u65f6\u95f4\u5b50\u56fe\u4e0a\u9010\u6b65\u8fc7\u6ee4\u548c\u7ec6\u5316\u68c0\u7d22\uff1b4) \u6e90\u6587\u672c\u63d0\u53d6\u5668\uff0c\u51cf\u8f7b\u566a\u97f3\uff1b5) \u57fa\u4e8eLLM\u7684\u751f\u6210\u5668\uff0c\u5408\u6210\u4e0a\u4e0b\u6587\u548c\u65f6\u95f4\u4e0a\u51c6\u786e\u7684\u54cd\u5e94\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6Time-LongQA\uff0c\u7528\u4e8e\u6d4b\u8bd5\u8de8\u6f14\u5316\u77e5\u8bc6\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cT-GRAG\u5728\u65f6\u95f4\u7ea6\u675f\u4e0b\uff0c\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u54cd\u5e94\u76f8\u5173\u6027\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684RAG\u548cGraphRAG\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5efa\u6a21\u77e5\u8bc6\u6f14\u53d8\u5bf9\u4e8e\u5b9e\u73b0\u9c81\u68d2\u7684\u957f\u6587\u672c\u95ee\u7b54\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86T-GRAG\u5728\u5904\u7406\u65f6\u95f4\u52a8\u6001\u77e5\u8bc6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.01918", "pdf": "https://arxiv.org/pdf/2508.01918", "abs": "https://arxiv.org/abs/2508.01918", "authors": ["Jaskaranjeet Singh", "Rakesh Thakur"], "title": "Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the rapid advancement of large language models (LLMs), low-resource\nlanguages remain largely excluded from the NLP landscape. We present PunGPT2,\nthe first fully open-source suite of Punjabi large language models, trained\nfrom scratch on a 35GB domain-diverse corpus encompassing literature, religious\ntexts, news, and social discourse. Unlike prior multilingual approaches,\nPunGPT2 captures rich syntactic and morphological features unique to Punjabi\nthrough a tokenizer optimised with byte pair encoding and linguistically\naligned pretraining objectives. To improve factual grounding and domain recall,\nwe introduce Pun-RAG, a retrieval-augmented generation framework combining\nPunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We\nfurther develop Pun-Instruct, a parameter-efficient, instruction-tuned variant\nusing QLoRA, enabling robust zero-shot and instruction-following performance\nwith significantly reduced compute needs.\n  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system\nthat fuses sparse (BM25) and dense methods with quantum-inspired semantic\nmatching. By encoding queries using amplitude-based embeddings and retrieving\nvia quantum kernel similarity, Quantum-RAG achieves improved contextual\nrelevance with minimal memory overhead marking the first practical integration\nof quantum representations in low-resource language generation. Our models\nsignificantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in\nperplexity, factuality, and fluency. This work provides a scalable,\nreproducible blueprint for extending LLM capabilities to underrepresented\nlanguages and pioneers quantum-aware retrieval in low-resource NLP", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPunGPT2\uff0c\u9996\u4e2a\u65c1\u906e\u666e\u8bed\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5957\u4ef6\uff0c\u5e76\u5f15\u5165Pun-RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u548cPun-Instruct\uff08\u6307\u4ee4\u5fae\u8c03\uff09\u3002\u6838\u5fc3\u521b\u65b0\u662fQuantum-RAG\uff0c\u4e00\u4e2a\u7ed3\u5408\u91cf\u5b50\u542f\u53d1\u8bed\u4e49\u5339\u914d\u7684\u6df7\u5408\u68c0\u7d22\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u751f\u6210\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fc5\u901f\u53d1\u5c55\uff0c\u4f46\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u65c1\u906e\u666e\u8bed\uff09\u5728NLP\u9886\u57df\u4ecd\u88ab\u5927\u91cf\u6392\u9664\u5728\u5916\uff0c\u9700\u8981\u5c06\u5176\u80fd\u529b\u6269\u5c55\u5230\u8fd9\u4e9b\u8bed\u8a00\u3002", "method": "1. **PunGPT2**: \u57fa\u4e8e35GB\u65c1\u906e\u666e\u8bed\u8bed\u6599\u4ece\u5934\u8bad\u7ec3\uff0c\u91c7\u7528\u4f18\u5316\u7684BPE\u5206\u8bcd\u5668\u548c\u8bed\u8a00\u5b66\u5bf9\u9f50\u7684\u9884\u8bad\u7ec3\u76ee\u6807\u30022. **Pun-RAG**: \u5c06PunGPT2\u4e0eFAISS\u5bc6\u96c6\u68c0\u7d22\u5668\u7ed3\u5408\uff0c\u7528\u4e8e\u4e8b\u5b9e\u6027\u589e\u5f3a\u30023. **Pun-Instruct**: \u4f7f\u7528QLoRA\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u7684\u6307\u4ee4\u5fae\u8c03\u30024. **Quantum-RAG**: \u521b\u65b0\u7684\u6df7\u5408\u68c0\u7d22\u7cfb\u7edf\uff0c\u878d\u5408\u7a00\u758f(BM25)\u548c\u5bc6\u96c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5b50\u542f\u53d1\u8bed\u4e49\u5339\u914d\uff08\u632f\u5e45\u7f16\u7801\u67e5\u8be2\uff0c\u91cf\u5b50\u6838\u76f8\u4f3c\u5ea6\u68c0\u7d22\uff09\u63d0\u5347\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002", "result": "PunGPT2\u6355\u83b7\u4e86\u65c1\u906e\u666e\u8bed\u72ec\u7279\u7684\u53e5\u6cd5\u548c\u5f62\u6001\u7279\u5f81\u3002Pun-Instruct\u5728\u4f4e\u8ba1\u7b97\u9700\u6c42\u4e0b\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002Quantum-RAG\u4ee5\u6700\u5c0f\u5185\u5b58\u5f00\u9500\u63d0\u9ad8\u4e86\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002\u6240\u6709\u6a21\u578b\u5728\u56f0\u60d1\u5ea6\u3001\u4e8b\u5b9e\u6027\u548c\u6d41\u7545\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u591a\u8bed\u8a00\u57fa\u7ebf(mBERT, mT5, MuRIL)\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5c06LLM\u80fd\u529b\u6269\u5c55\u5230\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u8bed\u8a00\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u7684\u84dd\u56fe\uff0c\u5e76\u5728\u4f4e\u8d44\u6e90NLP\u4e2d\u5f00\u521b\u4e86\u91cf\u5b50\u611f\u77e5\u68c0\u7d22\u7684\u5148\u6cb3\u3002"}}
{"id": "2508.01115", "pdf": "https://arxiv.org/pdf/2508.01115", "abs": "https://arxiv.org/abs/2508.01115", "authors": ["Yang Liu", "Xuejiao Kang", "Sathya Iyer", "Idris Malik", "Ruixuan Li", "Juan Wang", "Xinchen Lu", "Xiangxue Zhao", "Dayong Wang", "Menghan Liu", "Isaac Liu", "Feng Liang", "Yinzhe Yu"], "title": "A hierarchy tree data structure for behavior-based user segment representation", "categories": ["cs.LG"], "comment": "18 pages, 7 figures", "summary": "User attributes are essential in multiple stages of modern recommendation\nsystems and are particularly important for mitigating the cold-start problem\nand improving the experience of new or infrequent users. We propose\nBehavior-based User Segmentation (BUS), a novel tree-based data structure that\nhierarchically segments the user universe with various users' categorical\nattributes based on the users' product-specific engagement behaviors. During\nthe BUS tree construction, we use Normalized Discounted Cumulative Gain (NDCG)\nas the objective function to maximize the behavioral representativeness of\nmarginal users relative to active users in the same segment. The constructed\nBUS tree undergoes further processing and aggregation across the leaf nodes and\ninternal nodes, allowing the generation of popular social content and\nbehavioral patterns for each node in the tree. To further mitigate bias and\nimprove fairness, we use the social graph to derive the user's connection-based\nBUS segments, enabling the combination of behavioral patterns extracted from\nboth the user's own segment and connection-based segments as the connection\naware BUS-based recommendation. Our offline analysis shows that the BUS-based\nretrieval significantly outperforms traditional user cohort-based aggregation\non ranking quality. We have successfully deployed our data structure and\nmachine learning algorithm and tested it with various production traffic\nserving billions of users daily, achieving statistically significant\nimprovements in the online product metrics, including music ranking and email\nnotifications. To the best of our knowledge, our study represents the first\nlist-wise learning-to-rank framework for tree-based recommendation that\neffectively integrates diverse user categorical attributes while preserving\nreal-world semantic interpretability at a large industrial scale.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u884c\u4e3a\u7684\u7528\u6237\u5206\u6bb5\uff08BUS\uff09\u6811\u7ed3\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u7528\u6237\u884c\u4e3a\u548c\u793e\u4ea4\u5173\u7cfb\uff0c\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u751f\u4ea7\u73af\u5883\u4e2d\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "motivation": "\u5728\u73b0\u4ee3\u63a8\u8350\u7cfb\u7edf\u4e2d\uff0c\u7528\u6237\u5c5e\u6027\u5bf9\u4e8e\u7f13\u89e3\u51b7\u542f\u52a8\u95ee\u9898\u548c\u63d0\u5347\u65b0\u7528\u6237\u6216\u4e0d\u5e38\u7528\u7528\u6237\u7684\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u884c\u4e3a\u5bfc\u5411\u7684\u7528\u6237\u5206\u6bb5\uff08BUS\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6811\u72b6\u6570\u636e\u7ed3\u6784\uff0c\u6839\u636e\u7528\u6237\u4ea7\u54c1\u76f8\u5173\u7684\u53c2\u4e0e\u884c\u4e3a\uff0c\u5206\u5c42\u5730\u5bf9\u7528\u6237\u8fdb\u884c\u7ec6\u5206\u3002\u6784\u5efaBUS\u6811\u65f6\uff0c\u4f7f\u7528NDCG\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\uff0c\u4ee5\u6700\u5927\u5316\u8fb9\u7f18\u7528\u6237\u76f8\u5bf9\u4e8e\u540c\u4e00\u5206\u6bb5\u5185\u6d3b\u8dc3\u7528\u6237\u7684\u884c\u4e3a\u4ee3\u8868\u6027\u3002\u901a\u8fc7\u8fdb\u4e00\u6b65\u5904\u7406\u548c\u805a\u5408\uff0c\u4e3a\u6bcf\u4e2a\u8282\u70b9\u751f\u6210\u6d41\u884c\u7684\u793e\u4ea4\u5185\u5bb9\u548c\u884c\u4e3a\u6a21\u5f0f\u3002\u4e3a\u51cf\u5c11\u504f\u5dee\u548c\u63d0\u9ad8\u516c\u5e73\u6027\uff0c\u8fd8\u5229\u7528\u793e\u4ea4\u56fe\u8c31\u83b7\u53d6\u57fa\u4e8e\u8fde\u63a5\u7684\u7528\u6237\u5206\u6bb5\uff0c\u5b9e\u73b0\u884c\u4e3a\u6a21\u5f0f\u4e0e\u8fde\u63a5\u6a21\u5f0f\u878d\u5408\u7684\u63a8\u8350\u3002", "result": "\u79bb\u7ebf\u5206\u6790\u8868\u660e\uff0cBUS\u53ec\u56de\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684\u57fa\u4e8e\u7528\u6237\u7fa4\u7ec4\u7684\u805a\u5408\u65b9\u6cd5\u3002\u8be5\u6570\u636e\u7ed3\u6784\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5df2\u6210\u529f\u90e8\u7f72\u5e76\u5e94\u7528\u4e8e\u5904\u7406\u6bcf\u65e5\u6570\u5341\u4ebf\u7528\u6237\u7684\u751f\u4ea7\u6d41\u91cf\uff0c\u5728\u7ebf\u4ea7\u54c1\u6307\u6807\uff08\u5305\u62ec\u97f3\u4e50\u6392\u5e8f\u548c\u90ae\u4ef6\u901a\u77e5\uff09\u5747\u83b7\u5f97\u7edf\u8ba1\u5b66\u4e0a\u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u672c\u7814\u7a76\u662f\u9996\u4e2a\u5728\u5de5\u4e1a\u89c4\u6a21\u4e0b\uff0c\u6709\u6548\u6574\u5408\u591a\u79cd\u7528\u6237\u5206\u7c7b\u5c5e\u6027\u5e76\u4fdd\u6301\u771f\u5b9e\u4e16\u754c\u8bed\u4e49\u53ef\u89e3\u91ca\u6027\u7684\u57fa\u4e8e\u6811\u7684\u63a8\u8350\u7cfb\u7edf\u5217\u8868\u5b66\u4e60\u6392\u5e8f\u6846\u67b6\u3002"}}
{"id": "2508.01239", "pdf": "https://arxiv.org/pdf/2508.01239", "abs": "https://arxiv.org/abs/2508.01239", "authors": ["Han Ling", "Xian Xu", "Yinghui Sun", "Quansen Sun"], "title": "OCSplats: Observation Completeness Quantification and Label Noise Separation in 3DGS", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has become one of the most promising 3D\nreconstruction technologies. However, label noise in real-world scenarios-such\nas moving objects, non-Lambertian surfaces, and shadows-often leads to\nreconstruction errors. Existing 3DGS-Bsed anti-noise reconstruction methods\neither fail to separate noise effectively or require scene-specific fine-tuning\nof hyperparameters, making them difficult to apply in practice. This paper\nre-examines the problem of anti-noise reconstruction from the perspective of\nepistemic uncertainty, proposing a novel framework, OCSplats. By combining key\ntechnologies such as hybrid noise assessment and observation-based cognitive\ncorrection, the accuracy of noise classification in areas with cognitive\ndifferences has been significantly improved. Moreover, to address the issue of\nvarying noise proportions in different scenarios, we have designed a label\nnoise classification pipeline based on dynamic anchor points. This pipeline\nenables OCSplats to be applied simultaneously to scenarios with vastly\ndifferent noise proportions without adjusting parameters. Extensive experiments\ndemonstrate that OCSplats always achieve leading reconstruction performance and\nprecise label noise classification in scenes of different complexity levels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOCSplats\u7684\u65b0\u578b3D\u9ad8\u65af\u6cfc\u6e85\u6297\u566a\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u566a\u58f0\u8bc4\u4f30\u548c\u57fa\u4e8e\u89c2\u6d4b\u7684\u8ba4\u77e5\u6821\u6b63\uff0c\u5e76\u7ed3\u5408\u52a8\u6001\u951a\u70b9\u5206\u7c7b\u7ba1\u9053\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5b9e\u9645\u573a\u666f\u4e2d\u6807\u7b7e\u566a\u58f0\u5bfc\u81f4\u7684\u91cd\u5efa\u8bef\u5dee\u95ee\u9898\uff0c\u65e0\u9700\u573a\u666f\u7279\u5b9a\u53c2\u6570\u8c03\u6574\u5373\u53ef\u5728\u4e0d\u540c\u566a\u58f0\u6bd4\u4f8b\u4e0b\u5b9e\u73b0\u9886\u5148\u7684\u91cd\u5efa\u6027\u80fd\u548c\u7cbe\u51c6\u7684\u566a\u58f0\u5206\u7c7b\u3002", "motivation": "3D Gaussian Splatting (3DGS) \u57283D\u91cd\u5efa\u4e2d\u5f88\u6709\u524d\u666f\uff0c\u4f46\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\uff08\u5982\u79fb\u52a8\u7269\u4f53\u3001\u975e\u6717\u4f2f\u8868\u9762\u3001\u9634\u5f71\uff09\u4f1a\u5bfc\u81f4\u91cd\u5efa\u9519\u8bef\u3002\u73b0\u6709\u6297\u566a\u65b9\u6cd5\u8981\u4e48\u566a\u58f0\u5206\u79bb\u6548\u7387\u4f4e\uff0c\u8981\u4e48\u9700\u8981\u573a\u666f\u7279\u5b9a\u7684\u8d85\u53c2\u6570\u5fae\u8c03\uff0c\u96be\u4ee5\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u672c\u6587\u4ece\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u89d2\u5ea6\u91cd\u65b0\u5ba1\u89c6\u6297\u566a\u91cd\u5efa\u95ee\u9898\uff0c\u63d0\u51fa\u4e86OCSplats\u6846\u67b6\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u6df7\u5408\u566a\u58f0\u8bc4\u4f30\u548c\u57fa\u4e8e\u89c2\u6d4b\u7684\u8ba4\u77e5\u6821\u6b63\u7b49\u5173\u952e\u6280\u672f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba4\u77e5\u5dee\u5f02\u533a\u57df\u7684\u566a\u58f0\u5206\u7c7b\u7cbe\u5ea6\u3002\u6b64\u5916\uff0c\u9488\u5bf9\u4e0d\u540c\u573a\u666f\u566a\u58f0\u6bd4\u4f8b\u5dee\u5f02\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u52a8\u6001\u951a\u70b9\u7684\u6807\u7b7e\u566a\u58f0\u5206\u7c7b\u7ba1\u9053\uff0c\u4f7f\u5176\u65e0\u9700\u53c2\u6570\u8c03\u6574\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u566a\u58f0\u6bd4\u4f8b\u7684\u573a\u666f\u3002", "result": "OCSplats\u5728\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u573a\u666f\u4e2d\u5747\u5b9e\u73b0\u4e86\u9886\u5148\u7684\u91cd\u5efa\u6027\u80fd\uff0c\u5e76\u80fd\u7cbe\u51c6\u5730\u8fdb\u884c\u6807\u7b7e\u566a\u58f0\u5206\u7c7b\u3002", "conclusion": "OCSplats\u901a\u8fc7\u521b\u65b0\u7684\u566a\u58f0\u8bc4\u4f30\u548c\u6821\u6b63\u673a\u5236\uff0c\u4ee5\u53ca\u7075\u6d3b\u7684\u52a8\u6001\u951a\u70b9\u5206\u7c7b\u7ba1\u9053\uff0c\u6709\u6548\u514b\u670d\u4e863DGS\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u7684\u6807\u7b7e\u566a\u58f0\u6311\u6218\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.01693", "pdf": "https://arxiv.org/pdf/2508.01693", "abs": "https://arxiv.org/abs/2508.01693", "authors": ["Yuhang Gu", "Xingyu Hu", "Yuyu Fan", "Xulin Yan", "Longhuan Xu", "Peng peng"], "title": "SURE-Med: Systematic Uncertainty Reduction for Enhanced Reliability in Medical Report Generation", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Automated medical report generation (MRG) holds great promise for reducing\nthe heavy workload of radiologists. However, its clinical deployment is\nhindered by three major sources of uncertainty. First, visual uncertainty,\ncaused by noisy or incorrect view annotations, compromises feature extraction.\nSecond, label distribution uncertainty, stemming from long-tailed disease\nprevalence, biases models against rare but clinically critical conditions.\nThird, contextual uncertainty, introduced by unverified historical reports,\noften leads to factual hallucinations. These challenges collectively limit the\nreliability and clinical trustworthiness of MRG systems. To address these\nissues, we propose SURE-Med, a unified framework that systematically reduces\nuncertainty across three critical dimensions: visual, distributional, and\ncontextual. To mitigate visual uncertainty, a Frontal-Aware View Repair\nResampling module corrects view annotation errors and adaptively selects\ninformative features from supplementary views. To tackle label distribution\nuncertainty, we introduce a Token Sensitive Learning objective that enhances\nthe modeling of critical diagnostic sentences while reweighting\nunderrepresented diagnostic terms, thereby improving sensitivity to infrequent\nconditions. To reduce contextual uncertainty, our Contextual Evidence Filter\nvalidates and selectively incorporates prior information that aligns with the\ncurrent image, effectively suppressing hallucinations. Extensive experiments on\nthe MIMIC-CXR and IU-Xray benchmarks demonstrate that SURE-Med achieves\nstate-of-the-art performance. By holistically reducing uncertainty across\nmultiple input modalities, SURE-Med sets a new benchmark for reliability in\nmedical report generation and offers a robust step toward trustworthy clinical\ndecision support.", "AI": {"tldr": "SURE-Med\u662f\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u65e8\u5728\u7cfb\u7edf\u6027\u5730\u964d\u4f4e\u533b\u5b66\u62a5\u544a\u751f\u6210\u4e2d\u7684\u89c6\u89c9\u3001\u6807\u7b7e\u5206\u5e03\u548c\u4e0a\u4e0b\u6587\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u81ea\u52a8\u533b\u5b66\u62a5\u544a\u751f\u6210\u56e0\u89c6\u89c9\u3001\u6807\u7b7e\u5206\u5e03\u548c\u4e0a\u4e0b\u6587\u4e09\u79cd\u4e3b\u8981\u4e0d\u786e\u5b9a\u6027\u800c\u9762\u4e34\u4e34\u5e8a\u90e8\u7f72\u969c\u788d\uff0c\u5bfc\u81f4\u53ef\u9760\u6027\u4e0e\u53ef\u4fe1\u5ea6\u53d7\u9650\uff0c\u6025\u9700\u89e3\u51b3\u3002", "method": "\u63d0\u51faSURE-Med\u6846\u67b6\u3002\u901a\u8fc7\u201c\u6b63\u9762\u611f\u77e5\u89c6\u56fe\u4fee\u590d\u91cd\u91c7\u6837\u6a21\u5757\u201d\u89e3\u51b3\u89c6\u89c9\u4e0d\u786e\u5b9a\u6027\uff1b\u5f15\u5165\u201c\u4ee4\u724c\u654f\u611f\u5b66\u4e60\u76ee\u6807\u201d\u5904\u7406\u6807\u7b7e\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\uff1b\u5229\u7528\u201c\u4e0a\u4e0b\u6587\u8bc1\u636e\u8fc7\u6ee4\u5668\u201d\u964d\u4f4e\u4e0a\u4e0b\u6587\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u6291\u5236\u5e7b\u89c9\u3002", "result": "\u5728MIMIC-CXR\u548cIU-Xray\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0cSURE-Med\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SURE-Med\u901a\u8fc7\u5168\u9762\u964d\u4f4e\u591a\u6a21\u6001\u8f93\u5165\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u533b\u5b66\u62a5\u544a\u751f\u6210\u8bbe\u5b9a\u4e86\u53ef\u9760\u6027\u65b0\u57fa\u51c6\uff0c\u662f\u8fc8\u5411\u53ef\u4fe1\u8d56\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7684\u575a\u5b9e\u4e00\u6b65\u3002"}}
{"id": "2508.01930", "pdf": "https://arxiv.org/pdf/2508.01930", "abs": "https://arxiv.org/abs/2508.01930", "authors": ["Tom S. Juzek", "Zina B. Ward"], "title": "Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback", "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7; I.2.6"], "comment": "Accepted for publication in the Proceedings of the 5th Workshop on\n  Bias and Fairness in AI (BIAS 2025) at ECML PKDD", "summary": "Large Language Models (LLMs) are known to overuse certain terms like \"delve\"\nand \"intricate.\" The exact reasons for these lexical choices, however, have\nbeen unclear. Using Meta's Llama model, this study investigates the\ncontribution of Learning from Human Feedback (LHF), under which we subsume\nReinforcement Learning from Human Feedback and Direct Preference Optimization.\nWe present a straightforward procedure for detecting the lexical preferences of\nLLMs that are potentially LHF-induced. Next, we more conclusively link LHF to\nlexical overuse by experimentally emulating the LHF procedure and demonstrating\nthat participants systematically prefer text variants that include certain\nwords. This lexical overuse can be seen as a sort of misalignment, though our\nstudy highlights the potential divergence between the lexical expectations of\ndifferent populations -- namely LHF workers versus LLM users. Our work\ncontributes to the growing body of research on explainable artificial\nintelligence and emphasizes the importance of both data and procedural\ntransparency in alignment research.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fc7\u5ea6\u4f7f\u7528\u7279\u5b9a\u8bcd\u6c47\u7684\u539f\u56e0\u4e0e\u4eba\u7c7b\u53cd\u9988\u5b66\u4e60\uff08LHF\uff09\u6709\u5173\uff0c\u63ed\u793a\u4e86LHF\u5de5\u4f5c\u8005\u4e0eLLM\u7528\u6237\u95f4\u8bcd\u6c47\u504f\u597d\u7684\u6f5c\u5728\u5dee\u5f02\u3002", "motivation": "LLMs\u8fc7\u5ea6\u4f7f\u7528\u7279\u5b9a\u8bcd\u6c47\uff08\u5982\u201cdelve\u201d\u3001\u201cintricate\u201d\uff09\u7684\u539f\u56e0\u5c1a\u4e0d\u660e\u786e\uff0c\u7279\u522b\u662f\u4eba\u7c7b\u53cd\u9988\u5b66\u4e60\uff08LHF\uff09\u5728\u6b64\u73b0\u8c61\u4e2d\u7684\u4f5c\u7528\u4e0d\u660e\u3002", "method": "1. \u63d0\u51fa\u4e00\u79cd\u68c0\u6d4bLLM\u8bcd\u6c47\u504f\u597d\u7684\u7a0b\u5e8f\uff0c\u4ee5\u8bc6\u522bLHF\u53ef\u80fd\u8bf1\u5bfc\u7684\u8bcd\u6c47\u504f\u597d\uff08\u4f7f\u7528Meta\u7684Llama\u6a21\u578b\uff09\u30022. \u901a\u8fc7\u5b9e\u9a8c\u6a21\u62dfLHF\u8fc7\u7a0b\uff0c\u89c2\u5bdf\u53c2\u4e0e\u8005\u5bf9\u5305\u542b\u7279\u5b9a\u8bcd\u6c47\u7684\u6587\u672c\u53d8\u4f53\u7684\u504f\u597d\uff0c\u4ece\u800c\u5efa\u7acbLHF\u4e0e\u8bcd\u6c47\u8fc7\u5ea6\u4f7f\u7528\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "result": "1. \u6210\u529f\u68c0\u6d4b\u5230LLM\u4e2d\u53ef\u80fd\u7531LHF\u8bf1\u5bfc\u7684\u8bcd\u6c47\u504f\u597d\u30022. \u5b9e\u9a8c\u8bc1\u660e\uff0c\u53c2\u4e0e\u8005\u7cfb\u7edf\u6027\u5730\u504f\u597d\u5305\u542b\u67d0\u4e9b\u8bcd\u6c47\u7684\u6587\u672c\u53d8\u4f53\uff0c\u4ece\u800c\u8bc1\u5b9eLHF\u662fLLM\u8bcd\u6c47\u8fc7\u5ea6\u4f7f\u7528\u7684\u539f\u56e0\u30023. \u8fd9\u79cd\u8bcd\u6c47\u8fc7\u5ea6\u4f7f\u7528\u88ab\u89c6\u4e3a\u4e00\u79cd\u5bf9\u9f50\u95ee\u9898\uff0c\u53cd\u6620\u4e86LHF\u5de5\u4f5c\u8005\u4e0eLLM\u7528\u6237\u4e4b\u95f4\u8bcd\u6c47\u671f\u671b\u7684\u6f5c\u5728\u5dee\u5f02\u3002", "conclusion": "LHF\u662f\u5bfc\u81f4LLMs\u8fc7\u5ea6\u4f7f\u7528\u7279\u5b9a\u8bcd\u6c47\u7684\u5173\u952e\u56e0\u7d20\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u5728AI\u5bf9\u9f50\u7814\u7a76\u4e2d\u6570\u636e\u548c\u7a0b\u5e8f\u900f\u660e\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63ed\u793a\u4e86LHF\u5de5\u4f5c\u8005\u4e0eLLM\u7528\u6237\u4e4b\u95f4\u8bcd\u6c47\u671f\u671b\u53ef\u80fd\u5b58\u5728\u5206\u6b67\u3002"}}
{"id": "2508.01134", "pdf": "https://arxiv.org/pdf/2508.01134", "abs": "https://arxiv.org/abs/2508.01134", "authors": ["Ran Li", "Lingshu Zeng"], "title": "Transformers in Pseudo-Random Number Generation: A Dual Perspective on Theory and Practice", "categories": ["cs.LG"], "comment": "27 pages, 4 figures", "summary": "Pseudo-random number generators (PRNGs) are high-nonlinear processes, and\nthey are key blocks in optimization of Large language models. Transformers\nexcel at processing complex nonlinear relationships. Thus it is reasonable to\ngenerate high-quality pseudo-random numbers based on transformers. In this\npaper, we explore this question from both theoretical and practical\nperspectives, highlighting the potential benefits and implications of\nTransformer in PRNGs. We theoretically demonstrate that decoder-only\nTransformer models with Chain-of-Thought can simulate both the Linear\nCongruential Generator (LCG) and Mersenne Twister (MT) PRNGs. Based on this, we\nconclude that the log-precision decoder-only Transformer can represent\nnon-uniform $\\text{AC}^0$. Our simulative theoretical findings are validated\nthrough experiments. The random numbers generated by Transformer-based PRNGs\nsuccessfully pass the majority of NIST tests, whose heat maps exhibit clear\nstatistical randomness. Finally, we assess their capability in prediction\nattacks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u7d22\u57fa\u4e8eTransformer\u6a21\u578b\u751f\u6210\u4f2a\u968f\u673a\u6570\u3002\u7406\u8bba\u8bc1\u660e\u4ec5\u89e3\u7801\u5668Transformer\u53ef\u6a21\u62df\u73b0\u6709PRNGs\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u751f\u6210\u7684\u968f\u673a\u6570\u901a\u8fc7\u4e86NIST\u6d4b\u8bd5\u3002", "motivation": "\u4f2a\u968f\u673a\u6570\u751f\u6210\u5668\uff08PRNGs\uff09\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f18\u5316\u7684\u5173\u952e\u975e\u7ebf\u6027\u7ec4\u4ef6\u3002\u9274\u4e8eTransformer\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u975e\u7ebf\u6027\u5173\u7cfb\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u7814\u7a76\u5176\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u968f\u673a\u6570\u65b9\u9762\u7684\u6f5c\u529b\u662f\u5408\u7406\u7684\u3002", "method": "1. \u7406\u8bba\u5206\u6790\uff1a\u8bc1\u660e\u5e26\u601d\u7ef4\u94fe\u7684\u4ec5\u89e3\u7801\u5668Transformer\u6a21\u578b\u80fd\u6a21\u62df\u7ebf\u6027\u540c\u4f59\u751f\u6210\u5668\uff08LCG\uff09\u548cMersenne Twister\uff08MT\uff09PRNGs\u30022. \u5b9e\u9a8c\u9a8c\u8bc1\uff1a\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u6a21\u62df\u7ed3\u679c\u30023. \u7edf\u8ba1\u6d4b\u8bd5\uff1a\u4f7f\u7528NIST\u6d4b\u8bd5\u8bc4\u4f30Transformer\u751f\u6210\u968f\u673a\u6570\u7684\u8d28\u91cf\uff0c\u5e76\u5206\u6790\u5176\u70ed\u529b\u56fe\u30024. \u5b89\u5168\u8bc4\u4f30\uff1a\u8bc4\u4f30\u5176\u62b5\u5fa1\u9884\u6d4b\u653b\u51fb\u7684\u80fd\u529b\u3002", "result": "1. \u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5bf9\u6570\u7cbe\u5ea6\u4ec5\u89e3\u7801\u5668Transformer\u53ef\u4ee5\u8868\u793a\u975e\u5747\u5300AC^0\u30022. \u57fa\u4e8eTransformer\u7684PRNGs\u751f\u6210\u7684\u968f\u673a\u6570\u6210\u529f\u901a\u8fc7\u4e86NIST\u6d4b\u8bd5\u7684\u5927\u90e8\u5206\u9879\u76ee\uff0c\u5e76\u5c55\u73b0\u51fa\u6e05\u6670\u7684\u7edf\u8ba1\u968f\u673a\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4ece\u7406\u8bba\u548c\u5b9e\u8df5\u4e24\u65b9\u9762\u9a8c\u8bc1\u4e86Transformer\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u4f2a\u968f\u673a\u6570\u65b9\u9762\u7684\u6f5c\u529b\u3002\u7406\u8bba\u4e0a\uff0c\u4ec5\u89e3\u7801\u5668Transformer\u80fd\u591f\u6a21\u62df\u4f20\u7edfPRNGs\uff0c\u5e76\u4e14\u5176\u751f\u6210\u7684\u968f\u673a\u6570\u5728\u5b9e\u9a8c\u4e2d\u901a\u8fc7\u4e86\u4e25\u683c\u7684NIST\u7edf\u8ba1\u6d4b\u8bd5\uff0c\u8868\u660eTransformer\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u4f2a\u968f\u673a\u6570\u751f\u6210\u5668\u3002"}}
{"id": "2508.01248", "pdf": "https://arxiv.org/pdf/2508.01248", "abs": "https://arxiv.org/abs/2508.01248", "authors": ["Jiazhen Yan", "Fan Wang", "Weiwei Jiang", "Ziqiang Li", "Zhangjie Fu"], "title": "NS-Net: Decoupling CLIP Semantic Information through NULL-Space for Generalizable AI-Generated Image Detection", "categories": ["cs.CV"], "comment": null, "summary": "The rapid progress of generative models, such as GANs and diffusion models,\nhas facilitated the creation of highly realistic images, raising growing\nconcerns over their misuse in security-sensitive domains. While existing\ndetectors perform well under known generative settings, they often fail to\ngeneralize to unknown generative models, especially when semantic content\nbetween real and fake images is closely aligned. In this paper, we revisit the\nuse of CLIP features for AI-generated image detection and uncover a critical\nlimitation: the high-level semantic information embedded in CLIP's visual\nfeatures hinders effective discrimination. To address this, we propose NS-Net,\na novel detection framework that leverages NULL-Space projection to decouple\nsemantic information from CLIP's visual features, followed by contrastive\nlearning to capture intrinsic distributional differences between real and\ngenerated images. Furthermore, we design a Patch Selection strategy to preserve\nfine-grained artifacts by mitigating semantic bias caused by global image\nstructures. Extensive experiments on an open-world benchmark comprising images\ngenerated by 40 diverse generative models show that NS-Net outperforms existing\nstate-of-the-art methods, achieving a 7.4\\% improvement in detection accuracy,\nthereby demonstrating strong generalization across both GAN- and\ndiffusion-based image generation techniques.", "AI": {"tldr": "\u9488\u5bf9\u73b0\u6709AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51faNS-Net\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026CLIP\u7279\u5f81\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u5bf9\u672a\u77e5\u751f\u6210\u6a21\u578b\u7684\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u968f\u7740GAN\u548c\u6269\u6563\u6a21\u578b\u7b49\u751f\u6210\u6a21\u578b\u5feb\u901f\u53d1\u5c55\uff0c\u751f\u6210\u4e86\u9ad8\u5ea6\u903c\u771f\u7684\u56fe\u50cf\uff0c\u5f15\u53d1\u4e86\u5bf9\u6076\u610f\u4f7f\u7528\u7684\u62c5\u5fe7\u3002\u73b0\u6709\u68c0\u6d4b\u5668\u5728\u5df2\u77e5\u751f\u6210\u8bbe\u7f6e\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5f53\u771f\u5b9e\u548c\u4f2a\u9020\u56fe\u50cf\u8bed\u4e49\u5185\u5bb9\u9ad8\u5ea6\u76f8\u4f3c\u65f6\uff0c\u5b83\u4eec\u5f80\u5f80\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u77e5\u751f\u6210\u6a21\u578b\u3002\u7814\u7a76\u53d1\u73b0\uff0cCLIP\u89c6\u89c9\u7279\u5f81\u4e2d\u5d4c\u5165\u7684\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\u4f1a\u963b\u788d\u6709\u6548\u533a\u5206\u3002", "method": "\u672c\u6587\u63d0\u51faNS-Net\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u96f6\u7a7a\u95f4\uff08NULL-Space\uff09\u6295\u5f71\u5c06\u8bed\u4e49\u4fe1\u606f\u4eceCLIP\u89c6\u89c9\u7279\u5f81\u4e2d\u89e3\u8026\uff0c\u968f\u540e\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u6765\u6355\u83b7\u771f\u5b9e\u56fe\u50cf\u4e0e\u751f\u6210\u56fe\u50cf\u4e4b\u95f4\u7684\u5185\u5728\u5206\u5e03\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8865\u4e01\u9009\u62e9\u7b56\u7565\uff08Patch Selection\uff09\uff0c\u901a\u8fc7\u51cf\u8f7b\u5168\u5c40\u56fe\u50cf\u7ed3\u6784\u9020\u6210\u7684\u8bed\u4e49\u504f\u5dee\u6765\u4fdd\u7559\u7ec6\u7c92\u5ea6\u4f2a\u5f71\u7279\u5f81\u3002", "result": "\u5728\u5305\u542b40\u79cd\u4e0d\u540c\u751f\u6210\u6a21\u578b\u7684\u5f00\u653e\u4e16\u754c\u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cNS-Net\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e867.4%\u3002\u8fd9\u5c55\u793a\u4e86NS-Net\u5728\u57fa\u4e8eGAN\u548c\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u751f\u6210\u6280\u672f\u4e0a\u90fd\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "NS-Net\u901a\u8fc7\u6709\u6548\u89e3\u8026\u8bed\u4e49\u4fe1\u606f\u5e76\u5173\u6ce8\u7ec6\u7c92\u5ea6\u7279\u5f81\uff0c\u6210\u529f\u89e3\u51b3\u4e86AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u5728\u672a\u77e5\u751f\u6210\u6a21\u578b\u4e0a\u7684\u6cdb\u5316\u6027\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.01700", "pdf": "https://arxiv.org/pdf/2508.01700", "abs": "https://arxiv.org/abs/2508.01700", "authors": ["Zhihao Shuai", "Boyan Li", "Siyu Yan", "Yuyu Luo", "Weikai Yang"], "title": "DeepVIS: Bridging Natural Language and Data Visualization Through Step-wise Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Although data visualization is powerful for revealing patterns and\ncommunicating insights, creating effective visualizations requires familiarity\nwith authoring tools and often disrupts the analysis flow. While large language\nmodels show promise for automatically converting analysis intent into\nvisualizations, existing methods function as black boxes without transparent\nreasoning processes, which prevents users from understanding design rationales\nand refining suboptimal outputs. To bridge this gap, we propose integrating\nChain-of-Thought (CoT) reasoning into the Natural Language to Visualization\n(NL2VIS) pipeline. First, we design a comprehensive CoT reasoning process for\nNL2VIS and develop an automatic pipeline to equip existing datasets with\nstructured reasoning steps. Second, we introduce nvBench-CoT, a specialized\ndataset capturing detailed step-by-step reasoning from ambiguous natural\nlanguage descriptions to finalized visualizations, which enables\nstate-of-the-art performance when used for model fine-tuning. Third, we develop\nDeepVIS, an interactive visual interface that tightly integrates with the CoT\nreasoning process, allowing users to inspect reasoning steps, identify errors,\nand make targeted adjustments to improve visualization outcomes. Quantitative\nbenchmark evaluations, two use cases, and a user study collectively demonstrate\nthat our CoT framework effectively enhances NL2VIS quality while providing\ninsightful reasoning steps to users.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u601d\u7ef4\u94fe(CoT)\u63a8\u7406\u6574\u5408\u5230\u81ea\u7136\u8bed\u8a00\u5230\u53ef\u89c6\u5316(NL2VIS)\u6d41\u7a0b\u4e2d\uff0c\u672c\u7814\u7a76\u63d0\u5347\u4e86NL2VIS\u7684\u8d28\u91cf\u548c\u900f\u660e\u5ea6\uff0c\u4f7f\u7528\u6237\u80fd\u7406\u89e3\u5e76\u8c03\u6574\u53ef\u89c6\u5316\u751f\u6210\u8fc7\u7a0b\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u53ef\u89c6\u5316\u5de5\u5177\u64cd\u4f5c\u590d\u6742\uff0c\u4e14\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684NL2VIS\u65b9\u6cd5\u7f3a\u4e4f\u900f\u660e\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u7528\u6237\u96be\u4ee5\u7406\u89e3\u8bbe\u8ba1\u539f\u7406\u6216\u4f18\u5316\u8f93\u51fa\u3002", "method": "1. \u63d0\u51fa\u5c06\u601d\u7ef4\u94fe(CoT)\u63a8\u7406\u6574\u5408\u5230NL2VIS\u7ba1\u9053\u4e2d\u3002\n2. \u8bbe\u8ba1\u4e86\u5168\u9762\u7684CoT\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u5f00\u53d1\u4e86\u81ea\u52a8\u7ba1\u9053\u4e3a\u73b0\u6709\u6570\u636e\u96c6\u6dfb\u52a0\u7ed3\u6784\u5316\u63a8\u7406\u6b65\u9aa4\u3002\n3. \u6784\u5efa\u4e86nvBench-CoT\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u8be6\u7ec6\u7684\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\u3002\n4. \u5f00\u53d1\u4e86DeepVIS\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u754c\u9762\uff0c\u652f\u6301\u7528\u6237\u68c0\u67e5\u3001\u8bc6\u522b\u9519\u8bef\u548c\u8c03\u6574CoT\u63a8\u7406\u6b65\u9aa4\u3002", "result": "1. nvBench-CoT\u6570\u636e\u96c6\u5728\u6a21\u578b\u5fae\u8c03\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\n2. CoT\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86NL2VIS\u7684\u8d28\u91cf\u3002\n3. \u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u5bcc\u6709\u6d1e\u5bdf\u529b\u7684\u63a8\u7406\u6b65\u9aa4\u3002", "conclusion": "\u8be5CoT\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86NL2VIS\u7684\u8d28\u91cf\uff0c\u540c\u65f6\u901a\u8fc7\u63d0\u4f9b\u900f\u660e\u7684\u63a8\u7406\u6b65\u9aa4\uff0c\u589e\u5f3a\u4e86\u7528\u6237\u5bf9\u53ef\u89c6\u5316\u751f\u6210\u8fc7\u7a0b\u7684\u7406\u89e3\u548c\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2508.01943", "pdf": "https://arxiv.org/pdf/2508.01943", "abs": "https://arxiv.org/abs/2508.01943", "authors": ["Philip Schroeder", "Ondrej Biza", "Thomas Weng", "Hongyin Luo", "James Glass"], "title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "Vision-language models (VLMs) have exhibited impressive capabilities across\ndiverse image understanding tasks, but still struggle in settings that require\nreasoning over extended sequences of camera frames from a video. This limits\ntheir utility in embodied settings, which require reasoning over long frame\nsequences from a continuous stream of visual input at each moment of a task\nattempt. To address this limitation, we propose ROVER (Reasoning Over VidEo\nRecursively), a framework that enables the model to recursively decompose\nlong-horizon video trajectories into segments corresponding to shorter subtasks\nwithin the trajectory. In doing so, ROVER facilitates more focused and accurate\nreasoning over temporally localized frame sequences without losing global\ncontext. We evaluate ROVER, implemented using an in-context learning approach,\non diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa\nthat consists of 543 videos showing both expert and perturbed non-expert\ntrajectories across 27 robotic manipulation tasks. ROVER outperforms strong\nbaselines across three video reasoning tasks: task progress estimation,\nframe-level natural language reasoning, and video question answering. We\nobserve that, by reducing the number of frames the model reasons over at each\ntimestep, ROVER mitigates hallucinations, especially during unexpected or\nnon-optimal moments of a trajectory. In addition, by enabling the\nimplementation of a subtask-specific sliding context window, ROVER's time\ncomplexity scales linearly with video length, an asymptotic improvement over\nbaselines. Demos, code, and data available at: https://rover-vlm.github.io", "AI": {"tldr": "\u63d0\u51faROVER\u6846\u67b6\uff0c\u901a\u8fc7\u9012\u5f52\u5206\u89e3\u957f\u89c6\u9891\u8f68\u8ff9\u4e3a\u5b50\u4efb\u52a1\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u51cf\u5c11\u5e7b\u89c9\u5e76\u4f18\u5316\u6548\u7387\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u5bf9\u957f\u5e8f\u5217\u6444\u50cf\u5934\u5e27\u8fdb\u884c\u63a8\u7406\u7684\u573a\u666f\uff08\u5c24\u5176\u662f\u5728\u5177\u8eab\u73af\u5883\u4e2d\u5904\u7406\u8fde\u7eed\u89c6\u89c9\u8f93\u5165\u6d41\uff09\u65f6\uff0c\u8868\u73b0\u53d7\u9650\u3002", "method": "\u63d0\u51faROVER\uff08Reasoning Over VidEo Recursively\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u9012\u5f52\u5730\u5c06\u957f\u89c6\u9891\u8f68\u8ff9\u5206\u89e3\u4e3a\u5bf9\u5e94\u77ed\u5b50\u4efb\u52a1\u7684\u7247\u6bb5\uff0c\u5b9e\u73b0\u5bf9\u5c40\u90e8\u5e27\u5e8f\u5217\u7684\u66f4\u96c6\u4e2d\u3001\u51c6\u786e\u7684\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u4e0a\u4e0b\u6587\u3002\u8be5\u6846\u67b6\u91c7\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u3002", "result": "\u5728OpenX Embodiment\u89c6\u9891\u548c\u65b0\u7684RoboCasa\u6570\u636e\u96c6\u4e0a\uff0cROVER\u5728\u4efb\u52a1\u8fdb\u5ea6\u4f30\u8ba1\u3001\u5e27\u7ea7\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u548c\u89c6\u9891\u95ee\u7b54\u4e09\u9879\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002\u901a\u8fc7\u51cf\u5c11\u6a21\u578b\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u63a8\u7406\u7684\u5e27\u6570\uff0cROVER\u51cf\u8f7b\u4e86\u5e7b\u89c9\uff0c\u7279\u522b\u662f\u5e94\u5bf9\u610f\u5916\u6216\u975e\u6700\u4f18\u8f68\u8ff9\u65f6\u3002\u6b64\u5916\uff0cROVER\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u4e0e\u89c6\u9891\u957f\u5ea6\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u5b9e\u73b0\u4e86\u6e10\u8fd1\u6027\u6539\u8fdb\u3002", "conclusion": "ROVER\u901a\u8fc7\u9012\u5f52\u5206\u89e3\u957f\u89c6\u9891\u8f68\u8ff9\u4e3a\u5b50\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u957f\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4f18\u5316\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u5e7b\u89c9\u3002"}}
{"id": "2508.01148", "pdf": "https://arxiv.org/pdf/2508.01148", "abs": "https://arxiv.org/abs/2508.01148", "authors": ["Kotaro Yoshida", "Yuji Naraki", "Takafumi Horie", "Ryotaro Shimizu", "Hiroki Naganuma"], "title": "DisTaC: Conditioning Task Vectors via Distillation for Robust Model Merging", "categories": ["cs.LG"], "comment": null, "summary": "Model merging has emerged as an efficient and flexible paradigm for\nmulti-task learning, with numerous methods being proposed in recent years.\nHowever, these state-of-the-art techniques are typically evaluated on benchmark\nsuites that are highly favorable to model merging, and their robustness in more\nrealistic settings remains largely unexplored. In this work, we first\ninvestigate the vulnerabilities of model-merging methods and pinpoint the\nsource-model characteristics that critically underlie them. Specifically, we\nidentify two factors that are particularly harmful to the merging process: (1)\ndisparities in task vector norms, and (2) the low confidence of the source\nmodels. To address this issue, we propose DisTaC (Distillation for Task vector\nConditioning), a novel method that pre-conditions these problematic task\nvectors before the merge. DisTaC leverages knowledge distillation to adjust a\ntask vector's norm and increase source-model confidence while preserving its\nessential task-specific knowledge. Our extensive experiments demonstrate that\nby pre-conditioning task vectors with DisTaC, state-of-the-art merging\ntechniques can successfully integrate models exhibiting the harmful traits --\nwhere they would otherwise fail -- achieving significant performance gains.", "AI": {"tldr": "\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u5728\u9762\u5bf9\u4efb\u52a1\u5411\u91cf\u8303\u6570\u5dee\u5f02\u5927\u548c\u6e90\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4f4e\u65f6\u9c81\u68d2\u6027\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51faDisTaC\u65b9\u6cd5\uff0c\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u9884\u5904\u7406\u4efb\u52a1\u5411\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u5e76\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6a21\u578b\u5408\u5e76\u4f5c\u4e3a\u591a\u4efb\u52a1\u5b66\u4e60\u7684\u6709\u6548\u8303\u5f0f\uff0c\u867d\u6709\u591a\u79cd\u65b9\u6cd5\uff0c\u4f46\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u6280\u672f\u901a\u5e38\u5728\u5bf9\u6a21\u578b\u5408\u5e76\u6709\u5229\u7684\u57fa\u51c6\u5957\u4ef6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5e76\u89e3\u51b3\u5bfc\u81f4\u6a21\u578b\u5408\u5e76\u5931\u8d25\u7684\u6e90\u6a21\u578b\u7279\u6027\uff0c\u7279\u522b\u662f\u4efb\u52a1\u5411\u91cf\u8303\u6570\u5dee\u5f02\u548c\u6e90\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51faDisTaC\uff08Distillation for Task vector Conditioning\uff09\u65b9\u6cd5\u3002DisTaC\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u5728\u6a21\u578b\u5408\u5e76\u524d\u5bf9\u5b58\u5728\u95ee\u9898\u7684\u4efb\u52a1\u5411\u91cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u65e8\u5728\u8c03\u6574\u5176\u8303\u6570\u5e76\u63d0\u9ad8\u6e90\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u6838\u5fc3\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cDisTaC\u80fd\u591f\u6709\u6548\u9884\u5904\u7406\u4efb\u52a1\u5411\u91cf\uff0c\u4f7f\u5f97\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5408\u5e76\u6280\u672f\u80fd\u591f\u6210\u529f\u6574\u5408\u90a3\u4e9b\u5177\u6709\u6709\u5bb3\u7279\u6027\u7684\u6a21\u578b\uff08\u5426\u5219\u8fd9\u4e9b\u6a21\u578b\u4f1a\u5bfc\u81f4\u5408\u5e76\u5931\u8d25\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "DisTaC\u901a\u8fc7\u89e3\u51b3\u4efb\u52a1\u5411\u91cf\u8303\u6570\u5dee\u5f02\u548c\u6e90\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4f4e\u7684\u95ee\u9898\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u4f7f\u5176\u80fd\u66f4\u6709\u6548\u5730\u5e94\u7528\u4e8e\u66f4\u5177\u6311\u6218\u6027\u7684\u771f\u5b9e\u573a\u666f\u3002"}}
{"id": "2508.01250", "pdf": "https://arxiv.org/pdf/2508.01250", "abs": "https://arxiv.org/abs/2508.01250", "authors": ["Xiaoqin Wang", "Xianxu Hou", "Meidan Ding", "Junliang Chen", "Kaijun Deng", "Jinheng Xie", "Linlin Shen"], "title": "DisFaceRep: Representation Disentanglement for Co-occurring Facial Components in Weakly Supervised Face Parsing", "categories": ["cs.CV"], "comment": "Accepted by ACM MM 2025", "summary": "Face parsing aims to segment facial images into key components such as eyes,\nlips, and eyebrows. While existing methods rely on dense pixel-level\nannotations, such annotations are expensive and labor-intensive to obtain. To\nreduce annotation cost, we introduce Weakly Supervised Face Parsing (WSFP), a\nnew task setting that performs dense facial component segmentation using only\nweak supervision, such as image-level labels and natural language descriptions.\nWSFP introduces unique challenges due to the high co-occurrence and visual\nsimilarity of facial components, which lead to ambiguous activations and\ndegraded parsing performance. To address this, we propose DisFaceRep, a\nrepresentation disentanglement framework designed to separate co-occurring\nfacial components through both explicit and implicit mechanisms. Specifically,\nwe introduce a co-occurring component disentanglement strategy to explicitly\nreduce dataset-level bias, and a text-guided component disentanglement loss to\nguide component separation using language supervision implicitly. Extensive\nexperiments on CelebAMask-HQ, LaPa, and Helen demonstrate the difficulty of\nWSFP and the effectiveness of DisFaceRep, which significantly outperforms\nexisting weakly supervised semantic segmentation methods. The code will be\nreleased at\n\\href{https://github.com/CVI-SZU/DisFaceRep}{\\textcolor{cyan}{https://github.com/CVI-SZU/DisFaceRep}}.", "AI": {"tldr": "\u9488\u5bf9\u4eba\u8138\u89e3\u6790\u4e2d\u50cf\u7d20\u7ea7\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u5f31\u76d1\u7763\u4eba\u8138\u89e3\u6790\uff08WSFP\uff09\u65b0\u4efb\u52a1\uff0c\u5e76\u5f15\u5165DisFaceRep\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u548c\u9690\u5f0f\uff08\u6587\u672c\u5f15\u5bfc\uff09\u673a\u5236\u89e3\u8026\u9762\u90e8\u7ec4\u4ef6\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ec4\u4ef6\u5171\u73b0\u548c\u89c6\u89c9\u76f8\u4f3c\u6027\u6311\u6218\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f31\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u8138\u89e3\u6790\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u4e14\u8017\u65f6\u7684\u5927\u91cf\u50cf\u7d20\u7ea7\u6807\u6ce8\u3002\u4e3a\u4e86\u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u5f31\u76d1\u7763\u65b9\u5f0f\uff08\u5982\u56fe\u50cf\u7ea7\u6807\u7b7e\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff09\u5b9e\u73b0\u4eba\u8138\u7ec4\u4ef6\u5206\u5272\u3002", "method": "\u63d0\u51fa\u5f31\u76d1\u7763\u4eba\u8138\u89e3\u6790\uff08WSFP\uff09\u65b0\u4efb\u52a1\u3002\u4e3a\u89e3\u51b3WSFP\u4e2d\u7ec4\u4ef6\u5171\u73b0\u548c\u89c6\u89c9\u76f8\u4f3c\u6027\u5bfc\u81f4\u7684\u6311\u6218\uff0c\u63d0\u51faDisFaceRep\u8868\u793a\u89e3\u8026\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\uff08\u5171\u73b0\u7ec4\u4ef6\u89e3\u8026\u7b56\u7565\u4ee5\u51cf\u5c11\u6570\u636e\u96c6\u504f\u5dee\uff09\u548c\u9690\u5f0f\uff08\u6587\u672c\u5f15\u5bfc\u7ec4\u4ef6\u89e3\u8026\u635f\u5931\u4ee5\u5229\u7528\u8bed\u8a00\u76d1\u7763\uff09\u673a\u5236\u5206\u79bb\u5171\u540c\u51fa\u73b0\u7684\u9762\u90e8\u7ec4\u4ef6\u3002", "result": "\u5728CelebAMask-HQ\u3001LaPa\u548cHelen\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86WSFP\u4efb\u52a1\u7684\u96be\u5ea6\u3002\u6240\u63d0\u51fa\u7684DisFaceRep\u6846\u67b6\u88ab\u8bc1\u5b9e\u6709\u6548\uff0c\u5e76\u4e14\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5730\u5f15\u5165\u5e76\u89e3\u51b3\u4e86\u5f31\u76d1\u7763\u4eba\u8138\u89e3\u6790\u4efb\u52a1\uff0c\u901a\u8fc7DisFaceRep\u6846\u67b6\u6709\u6548\u5904\u7406\u4e86\u9762\u90e8\u7ec4\u4ef6\u7684\u9ad8\u5ea6\u5171\u73b0\u548c\u89c6\u89c9\u76f8\u4f3c\u6027\u95ee\u9898\uff0c\u4e3a\u964d\u4f4e\u4eba\u8138\u89e3\u6790\u7684\u6807\u6ce8\u6210\u672c\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u6548\u9014\u5f84\uff0c\u4e14\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.01724", "pdf": "https://arxiv.org/pdf/2508.01724", "abs": "https://arxiv.org/abs/2508.01724", "authors": ["Shijie Cao", "Yuan Yuan"], "title": "ReflecSched: Solving Dynamic Flexible Job-Shop Scheduling via LLM-Powered Hierarchical Reflection", "categories": ["cs.AI"], "comment": null, "summary": "Dynamic Flexible Job-Shop Scheduling (DFJSP) is an NP-hard problem challenged\nby real-time event adaptation and complex machine routing. While traditional\ndispatching rules are efficient but rigid, deep learning approaches are opaque\nand require intricate feature engineering. Large Language Models (LLMs) promise\nadaptive reasoning without this engineering overhead, yet we find their direct\napplication is suboptimal. Baseline LLMs suffer from three key pitfalls: the\nlong-context paradox, where crucial data is underutilized; an underutilization\nof expert heuristics; and myopic decision-making. To address this, we propose\nReflecSched, a framework that empowers the LLM beyond a direct scheduler by\nequipping it with a strategic analysis capability. ReflecSched tasks the LLM to\nanalyze heuristic-driven simulations across multiple planning horizons and\ndistill them into a concise, natural-language summary termed ``Strategic\nExperience''. This summary is then integrated into the prompt of a final\ndecision-making module, guiding it to produce non-myopic actions. Experiments\nshow that ReflecSched not only statistically significantly outperforms direct\nLLM baselines, securing a 71.35\\% Win Rate and a 2.755\\% Relative Percentage\nDeviation reduction, but also surpasses the performance of all individual\nheuristics evaluated, all while demonstrably mitigating the three identified\npitfalls. Additionally, ReflecSched performs on par with the best heuristic\ntailored to each instance across all problem cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReflecSched\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5206\u6790\u542f\u53d1\u5f0f\u6a21\u62df\u5e76\u751f\u6210\u201c\u6218\u7565\u7ecf\u9a8c\u201d\uff0c\u514b\u670d\u4e86LLM\u5728\u52a8\u6001\u67d4\u6027\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\uff08DFJSP\uff09\u4e2d\u76f4\u63a5\u5e94\u7528\u65f6\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u4f18\u4e8e\u76f4\u63a5LLM\u57fa\u7ebf\u548c\u5355\u72ec\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u52a8\u6001\u67d4\u6027\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\uff08DFJSP\uff09\u662f\u4e00\u4e2aNP\u96be\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u4f20\u7edf\u8c03\u5ea6\u89c4\u5219\u8fc7\u4e8e\u50f5\u5316\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u900f\u660e\u6027\u4e14\u9700\u8981\u590d\u6742\u7684\u7279\u5f81\u5de5\u7a0b\u3002\u867d\u7136LLM\u6709\u671b\u63d0\u4f9b\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u4f46\u5176\u76f4\u63a5\u5e94\u7528\u5b58\u5728\u957f\u4e0a\u4e0b\u6587\u5229\u7528\u4e0d\u8db3\u3001\u4e13\u5bb6\u542f\u53d1\u5f0f\u672a\u5145\u5206\u5229\u7528\u548c\u77ed\u89c6\u51b3\u7b56\u4e09\u5927\u7f3a\u9677\u3002", "method": "\u63d0\u51faReflecSched\u6846\u67b6\uff0c\u8d4b\u4e88LLM\u6218\u7565\u5206\u6790\u80fd\u529b\u3002\u8be5\u6846\u67b6\u5f15\u5bfcLLM\u5206\u6790\u591a\u89c4\u5212\u671f\u7684\u542f\u53d1\u5f0f\u6a21\u62df\u7ed3\u679c\uff0c\u5e76\u5c06\u5176\u63d0\u70bc\u6210\u81ea\u7136\u8bed\u8a00\u7684\u201c\u6218\u7565\u7ecf\u9a8c\u201d\u6458\u8981\u3002\u6b64\u6458\u8981\u968f\u540e\u88ab\u6574\u5408\u5230\u6700\u7ec8\u51b3\u7b56\u6a21\u5757\u7684\u63d0\u793a\u4e2d\uff0c\u4ee5\u6307\u5bfcLLM\u505a\u51fa\u975e\u77ed\u89c6\u7684\u8c03\u5ea6\u51b3\u7b56\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cReflecSched\u5728\u7edf\u8ba1\u5b66\u4e0a\u663e\u8457\u4f18\u4e8e\u76f4\u63a5LLM\u57fa\u7ebf\uff08\u80dc\u738771.35%\uff0c\u76f8\u5bf9\u767e\u5206\u6bd4\u504f\u5dee\u964d\u4f4e2.755%\uff09\uff0c\u5e76\u8d85\u8d8a\u4e86\u6240\u6709\u5355\u72ec\u8bc4\u4f30\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u5b83\u6709\u6548\u7f13\u89e3\u4e86\u76f4\u63a5LLM\u5e94\u7528\u7684\u4e09\u5927\u7f3a\u9677\uff0c\u4e14\u6027\u80fd\u4e0e\u9488\u5bf9\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u6700\u4f73\u542f\u53d1\u5f0f\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "ReflecSched\u901a\u8fc7\u8d4b\u4e88LLM\u6218\u7565\u5206\u6790\u80fd\u529b\uff0c\u6210\u529f\u89e3\u51b3\u4e86DFJSP\u4e2d\u7684\u8c03\u5ea6\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u4e0e\u6700\u4f73\u542f\u53d1\u5f0f\u65b9\u6cd5\u5ab2\u7f8e\uff0c\u4e3a\u590d\u6742\u8c03\u5ea6\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01959", "pdf": "https://arxiv.org/pdf/2508.01959", "abs": "https://arxiv.org/abs/2508.01959", "authors": ["Junjie Wu", "Jiangnan Li", "Yuqing Li", "Lemao Liu", "Liyan Xu", "Jiwei Li", "Dit-Yan Yeung", "Jie Zhou", "Mo Yu"], "title": "SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension", "categories": ["cs.CL"], "comment": "Our trained models can be downloaded from:\n  https://huggingface.co/SituatedEmbedding", "summary": "Retrieval-augmented generation (RAG) over long documents typically involves\nsplitting the text into smaller chunks, which serve as the basic units for\nretrieval. However, due to dependencies across the original document,\ncontextual information is often essential for accurately interpreting each\nchunk. To address this, prior work has explored encoding longer context windows\nto produce embeddings for longer chunks. Despite these efforts, gains in\nretrieval and downstream tasks remain limited. This is because (1) longer\nchunks strain the capacity of embedding models due to the increased amount of\ninformation they must encode, and (2) many real-world applications still\nrequire returning localized evidence due to constraints on model or human\nbandwidth.\n  We propose an alternative approach to this challenge by representing short\nchunks in a way that is conditioned on a broader context window to enhance\nretrieval performance -- i.e., situating a chunk's meaning within its context.\nWe further show that existing embedding models are not well-equipped to encode\nsuch situated context effectively, and thus introduce a new training paradigm\nand develop the situated embedding models (SitEmb). To evaluate our method, we\ncurate a book-plot retrieval dataset specifically designed to assess situated\nretrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3\nsubstantially outperforms state-of-the-art embedding models, including several\nwith up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model\nfurther improves performance by over 10% and shows strong results across\ndifferent languages and several downstream applications.", "AI": {"tldr": "\u9488\u5bf9\u957f\u6587\u6863RAG\u4e2d\u77ed\u5757\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u201c\u60c5\u5883\u5316\u5d4c\u5165\u6a21\u578b\uff08SitEmb\uff09\u201d\u53ca\u5176\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u5728\u66f4\u5e7f\u9614\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e0b\u8868\u793a\u77ed\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u5728\u591a\u8bed\u8a00\u548c\u4e0b\u6e38\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5904\u7406\u957f\u6587\u6863\u65f6\uff0c\u5c06\u6587\u672c\u5206\u5272\u6210\u5c0f\u5757\u8fdb\u884c\u68c0\u7d22\uff0c\u4f46\u7531\u4e8e\u5757\u95f4\u4f9d\u8d56\u6027\uff0c\u51c6\u786e\u7406\u89e3\u6bcf\u4e2a\u5757\u9700\u8981\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u867d\u7136\u6709\u7814\u7a76\u5c1d\u8bd5\u7f16\u7801\u66f4\u957f\u7684\u4e0a\u4e0b\u6587\uff0c\u4f46\u53d7\u9650\u4e8e\u5d4c\u5165\u6a21\u578b\u5bb9\u91cf\u548c\u5e94\u7528\u5bf9\u5c40\u90e8\u8bc1\u636e\u7684\u9700\u6c42\uff0c\u6548\u679c\u6709\u9650\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff1a\u901a\u8fc7\u5728\u66f4\u5e7f\u9614\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e0b\u5bf9\u77ed\u6587\u672c\u5757\u8fdb\u884c\u6761\u4ef6\u5316\u8868\u793a\uff0c\u4ee5\u201c\u60c5\u5883\u5316\u201d\u5176\u610f\u4e49\uff0c\u4ece\u800c\u589e\u5f3a\u68c0\u7d22\u6027\u80fd\u3002\u4e3a\u6b64\uff0c\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\u5e76\u6784\u5efa\u4e86\u60c5\u5883\u5316\u5d4c\u5165\u6a21\u578b\uff08SitEmb\uff09\uff0c\u4ee5\u5f25\u8865\u73b0\u6709\u5d4c\u5165\u6a21\u578b\u5728\u6b64\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "result": "\u7814\u7a76\u8005\u521b\u5efa\u4e86\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u60c5\u5883\u5316\u68c0\u7d22\u80fd\u529b\u7684\u4e66\u7c4d\u60c5\u8282\u68c0\u7d22\u6570\u636e\u96c6\u3002\u5728\u6b64\u57fa\u51c6\u4e0a\uff0c\u57fa\u4e8eBGE-M3\u768410\u4ebf\u53c2\u6570SitEmb-v1\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u5305\u62ec7-80\u4ebf\u53c2\u6570\u5728\u5185\u7684\u73b0\u6709\u6700\u5148\u8fdb\u5d4c\u5165\u6a21\u578b\u300280\u4ebf\u53c2\u6570\u7684SitEmb-v1.5\u6a21\u578b\u6027\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u8d85\u8fc710%\uff0c\u5e76\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u591a\u4e2a\u4e0b\u6e38\u5e94\u7528\u4e2d\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "SitEmb\u6a21\u578b\u901a\u8fc7\u5bf9\u77ed\u6587\u672c\u5757\u8fdb\u884c\u60c5\u5883\u5316\u5d4c\u5165\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u6587\u6863RAG\u4e2d\u4e0a\u4e0b\u6587\u4fe1\u606f\u7f3a\u5931\u5bfc\u81f4\u7684\u68c0\u7d22\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u591a\u79cd\u8bed\u8a00\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u666e\u9002\u6027\u548c\u9ad8\u6548\u6027\uff0c\u4e3a\u672a\u6765RAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.01167", "pdf": "https://arxiv.org/pdf/2508.01167", "abs": "https://arxiv.org/abs/2508.01167", "authors": ["Hongquan Zhang", "Jingyu Gong", "Zhizhong Zhang", "Xin Tan", "Yanyun Qu", "Yuan Xie"], "title": "T2S: Tokenized Skill Scaling for Lifelong Imitation Learning", "categories": ["cs.LG", "cs.RO"], "comment": null, "summary": "The main challenge in lifelong imitation learning lies in the balance between\nmitigating catastrophic forgetting of previous skills while maintaining\nsufficient capacity for acquiring new ones. However, current approaches\ntypically address these aspects in isolation, overlooking their internal\ncorrelation in lifelong skill acquisition. We address this limitation with a\nunified framework named Tokenized Skill Scaling (T2S). Specifically, by\ntokenizing the model parameters, the linear parameter mapping of the\ntraditional transformer is transformed into cross-attention between input and\nlearnable tokens, thereby enhancing model scalability through the easy\nextension of new tokens. Additionally, we introduce language-guided skill\nscaling to transfer knowledge across tasks efficiently and avoid linearly\ngrowing parameters. Extensive experiments across diverse tasks demonstrate that\nT2S: 1) effectively prevents catastrophic forgetting (achieving an average NBT\nof 1.0% across the three LIBERO task suites), 2) excels in new skill scaling\nwith minimal increases in trainable parameters (needing only 8.0% trainable\ntokens in an average of lifelong tasks), and 3) enables efficient knowledge\ntransfer between tasks (achieving an average FWT of 77.7% across the three\nLIBERO task suites), offering a promising solution for lifelong imitation\nlearning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faT2S\uff08Tokenized Skill Scaling\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u6807\u8bb0\u5316\u548c\u8bed\u8a00\u5f15\u5bfc\u6280\u80fd\u7f29\u653e\uff0c\u5728\u7ec8\u8eab\u6a21\u4eff\u5b66\u4e60\u4e2d\u6709\u6548\u5e73\u8861\u707e\u96be\u6027\u9057\u5fd8\u4e0e\u65b0\u6280\u80fd\u83b7\u53d6\uff0c\u5e76\u5b9e\u73b0\u9ad8\u6548\u77e5\u8bc6\u8fc1\u79fb\u3002", "motivation": "\u7ec8\u8eab\u6a21\u4eff\u5b66\u4e60\u7684\u5173\u952e\u6311\u6218\u5728\u4e8e\u5e73\u8861\u5bf9\u65e7\u6280\u80fd\u7684\u9057\u5fd8\u4e0e\u65b0\u6280\u80fd\u7684\u83b7\u53d6\u80fd\u529b\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5b64\u7acb\u5904\u7406\u8fd9\u4e9b\u65b9\u9762\uff0c\u5ffd\u89c6\u4e86\u5b83\u4eec\u5728\u7ec8\u8eab\u6280\u80fd\u83b7\u53d6\u4e2d\u7684\u5185\u5728\u5173\u8054\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u7edf\u4e00\u6846\u67b6Tokenized Skill Scaling (T2S)\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u901a\u8fc7\u6807\u8bb0\u5316\u6a21\u578b\u53c2\u6570\uff0c\u5c06\u4f20\u7edfTransformer\u7684\u7ebf\u6027\u53c2\u6570\u6620\u5c04\u8f6c\u6362\u4e3a\u8f93\u5165\u4e0e\u53ef\u5b66\u4e60tokens\u4e4b\u95f4\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\uff0c\u4ece\u800c\u901a\u8fc7\u65b0tokens\u7684\u4fbf\u6377\u6269\u5c55\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u4f38\u7f29\u6027\uff1b\u5f15\u5165\u8bed\u8a00\u5f15\u5bfc\u6280\u80fd\u7f29\u653e\uff0c\u4ee5\u9ad8\u6548\u5730\u8de8\u4efb\u52a1\u8fc1\u79fb\u77e5\u8bc6\u5e76\u907f\u514d\u53c2\u6570\u7684\u7ebf\u6027\u589e\u957f\u3002", "result": "T2S\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff1a1) \u6709\u6548\u9632\u6b62\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff08LIBERO\u4efb\u52a1\u5957\u4ef6\u5e73\u5747NBT\u4e3a1.0%\uff09\uff1b2) \u4ee5\u6700\u5c0f\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u589e\u91cf\uff08\u7ec8\u8eab\u4efb\u52a1\u5e73\u5747\u4ec5\u97008.0%\u53ef\u8bad\u7ec3tokens\uff09\u5b9e\u73b0\u65b0\u6280\u80fd\u7684\u6269\u5c55\uff1b3) \u5b9e\u73b0\u4e86\u4efb\u52a1\u95f4\u9ad8\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\uff08LIBERO\u4efb\u52a1\u5957\u4ef6\u5e73\u5747FWT\u4e3a77.7%\uff09\u3002", "conclusion": "T2S\u4e3a\u7ec8\u8eab\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u5e73\u8861\u4e86\u6280\u80fd\u9057\u5fd8\u4e0e\u83b7\u53d6\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002"}}
{"id": "2508.01253", "pdf": "https://arxiv.org/pdf/2508.01253", "abs": "https://arxiv.org/abs/2508.01253", "authors": ["Yupeng Zhang", "Ruize Han", "Fangnan Zhou", "Song Wang", "Wei Feng", "Liang Wan"], "title": "ODOV: Towards Open-Domain Open-Vocabulary Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "In this work, we handle a new problem of Open-Domain Open-Vocabulary (ODOV)\nobject detection, which considers the detection model's adaptability to the\nreal world including both domain and category shifts. For this problem, we\nfirst construct a new benchmark OD-LVIS, which includes 46,949 images, covers\n18 complex real-world domains and 1,203 categories, and provides a\ncomprehensive dataset for evaluating real-world object detection. Besides, we\ndevelop a novel baseline method for ODOV detection.The proposed method first\nleverages large language models to generate the domain-agnostic text prompts\nfor category embedding. It further learns the domain embedding from the given\nimage, which, during testing, can be integrated into the category embedding to\nform the customized domain-specific category embedding for each test image. We\nprovide sufficient benchmark evaluations for the proposed ODOV detection task\nand report the results, which verify the rationale of ODOV detection, the\nusefulness of our benchmark, and the superiority of the proposed method.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u5f00\u653e\u57df\u5f00\u653e\u8bcd\u6c47\uff08ODOV\uff09\u76ee\u6807\u68c0\u6d4b\u65b0\u95ee\u9898\uff0c\u4ee5\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u4e2d\u9886\u57df\u548c\u7c7b\u522b\u6f02\u79fb\u3002\u4e3a\u6b64\uff0c\u6784\u5efa\u4e86OD-LVIS\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u9886\u57df\u5d4c\u5165\u7684\u65b0\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u591a\u53d8\u7684\u9886\u57df\u548c\u7c7b\u522b\u53d8\u5316\uff0c\u7f3a\u4e4f\u5f00\u653e\u57df\u5f00\u653e\u8bcd\u6c47\uff08ODOV\uff09\u7684\u6cdb\u5316\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5b9a\u4e49\u5e76\u89e3\u51b3\u8fd9\u4e00\u65b0\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u771f\u5b9e\u4e16\u754c\u9002\u5e94\u6027\u3002", "method": "1. \u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684OD-LVIS\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b46,949\u5f20\u56fe\u7247\u300118\u4e2a\u9886\u57df\u548c1,203\u4e2a\u7c7b\u522b\u3002 2. \u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684ODOV\u68c0\u6d4b\u57fa\u7ebf\u65b9\u6cd5\u3002 3. \u8be5\u65b9\u6cd5\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7c7b\u522b\u5d4c\u5165\u7684\u9886\u57df\u65e0\u5173\u6587\u672c\u63d0\u793a\u3002 4. \u4ece\u7ed9\u5b9a\u56fe\u50cf\u4e2d\u5b66\u4e60\u9886\u57df\u5d4c\u5165\uff0c\u5e76\u5728\u6d4b\u8bd5\u65f6\u5c06\u5176\u4e0e\u7c7b\u522b\u5d4c\u5165\u878d\u5408\uff0c\u5f62\u6210\u5b9a\u5236\u7684\u9886\u57df\u7279\u5b9a\u7c7b\u522b\u5d4c\u5165\u3002", "result": "1. \u6210\u529f\u6784\u5efa\u5e76\u63d0\u4f9b\u4e86\u7528\u4e8e\u8bc4\u4f30\u771f\u5b9e\u4e16\u754c\u76ee\u6807\u68c0\u6d4b\u7684\u7efc\u5408\u6027OD-LVIS\u57fa\u51c6\u6570\u636e\u96c6\u3002 2. \u5145\u5206\u7684\u57fa\u51c6\u8bc4\u4f30\u9a8c\u8bc1\u4e86ODOV\u68c0\u6d4b\u4efb\u52a1\u7684\u5408\u7406\u6027\u3002 3. \u8bc1\u5b9e\u4e86\u6240\u63d0OD-LVIS\u57fa\u51c6\u7684\u5b9e\u7528\u6027\u3002 4. \u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u57fa\u7ebf\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5b9a\u4e49\u5e76\u5e94\u5bf9\u4e86\u5f00\u653e\u57df\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u548c\u521b\u65b0\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u9762\u5bf9\u771f\u5b9e\u4e16\u754c\u590d\u6742\u9886\u57df\u548c\u7c7b\u522b\u53d8\u5316\u65f6\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2508.01746", "pdf": "https://arxiv.org/pdf/2508.01746", "abs": "https://arxiv.org/abs/2508.01746", "authors": ["Shiyang Duan", "Yuan Tian", "Qi Bing", "Xiaowei Shao"], "title": "Bayes-Entropy Collaborative Driven Agents for Research Hypotheses Generation and Optimization", "categories": ["cs.AI", "I.2.4"], "comment": "Corresponding author: Xiaowei Shao. 12 pages, 4 figures", "summary": "The exponential growth of scientific knowledge has made the automated\ngeneration of scientific hypotheses that combine novelty, feasibility, and\nresearch value a core challenge. Existing methods based on large language\nmodels fail to systematically model the inherent in hypotheses or incorporate\nthe closed-loop feedback mechanisms crucial for refinement. This paper proposes\na multi-agent collaborative framework called HypoAgents, which for the first\ntime integrates Bayesian reasoning with an information entropy-driven search\nmechanism across three stages-hypotheses generation, evidence validation, and\nhypotheses Refinement-to construct an iterative closed-loop simulating\nscientists' cognitive processes. Specifically, the framework first generates an\ninitial set of hypotheses through diversity sampling and establishes prior\nbeliefs based on a composite novelty-relevance-feasibility (N-R-F) score. It\nthen employs etrieval-augmented generation (RAG) to gather external literature\nevidence, updating the posterior probabilities of hypotheses using Bayes'\ntheorem. Finally, it identifies high-uncertainty hypotheses using information\nentropy $H = - \\sum {{p_i}\\log {p_i}}$ and actively refines them, guiding the\niterative optimization of the hypothesis set toward higher quality and\nconfidence. Experimental results on the ICLR 2025 conference real-world\nresearch question dataset (100 research questions) show that after 12\noptimization iterations, the average ELO score of generated hypotheses improves\nby 116.3, surpassing the benchmark of real paper abstracts by 17.8, while the\nframework's overall uncertainty, as measured by Shannon entropy, decreases\nsignificantly by 0.92. This study presents an interpretable probabilistic\nreasoning framework for automated scientific discovery, substantially improving\nthe quality and reliability of machine-generated research hypotheses.", "AI": {"tldr": "\u63d0\u51faHypoAgents\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u63a8\u7406\u548c\u4fe1\u606f\u71b5\uff0c\u8fed\u4ee3\u751f\u6210\u548c\u4f18\u5316\u79d1\u5b66\u5047\u8bbe\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u751f\u6210\u5047\u8bbe\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u79d1\u5b66\u77e5\u8bc6\u7684\u6307\u6570\u7ea7\u589e\u957f\u4f7f\u5f97\u81ea\u52a8\u5316\u751f\u6210\u517c\u5177\u65b0\u9896\u6027\u3001\u53ef\u884c\u6027\u548c\u7814\u7a76\u4ef7\u503c\u7684\u79d1\u5b66\u5047\u8bbe\u6210\u4e3a\u6838\u5fc3\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u672a\u80fd\u7cfb\u7edf\u5efa\u6a21\u5047\u8bbe\u7684\u5185\u5728\u7279\u6027\u6216\u6574\u5408\u5173\u952e\u7684\u95ed\u73af\u53cd\u9988\u673a\u5236\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aHypoAgents\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u8d1d\u53f6\u65af\u63a8\u7406\u4e0e\u4fe1\u606f\u71b5\u9a71\u52a8\u7684\u641c\u7d22\u673a\u5236\u6574\u5408\u5230\u5047\u8bbe\u751f\u6210\u3001\u8bc1\u636e\u9a8c\u8bc1\u548c\u5047\u8bbe\u4f18\u5316\u4e09\u4e2a\u9636\u6bb5\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u6a21\u62df\u79d1\u5b66\u5bb6\u8ba4\u77e5\u8fc7\u7a0b\u7684\u8fed\u4ee3\u95ed\u73af\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8be5\u6846\u67b6\u9996\u5148\u901a\u8fc7\u591a\u6837\u6027\u91c7\u6837\u751f\u6210\u521d\u59cb\u5047\u8bbe\u96c6\uff0c\u5e76\u57fa\u4e8e\u65b0\u9896\u6027-\u76f8\u5173\u6027-\u53ef\u884c\u6027\uff08N-R-F\uff09\u7efc\u5408\u8bc4\u5206\u5efa\u7acb\u5148\u9a8c\u4fe1\u5ff5\u3002\u7136\u540e\uff0c\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6536\u96c6\u5916\u90e8\u6587\u732e\u8bc1\u636e\uff0c\u5e76\u4f7f\u7528\u8d1d\u53f6\u65af\u5b9a\u7406\u66f4\u65b0\u5047\u8bbe\u7684\u540e\u9a8c\u6982\u7387\u3002\u6700\u540e\uff0c\u5b83\u5229\u7528\u4fe1\u606f\u71b5\u8bc6\u522b\u9ad8\u4e0d\u786e\u5b9a\u6027\u5047\u8bbe\u5e76\u4e3b\u52a8\u8fdb\u884c\u4f18\u5316\uff0c\u6307\u5bfc\u5047\u8bbe\u96c6\u8fed\u4ee3\u4f18\u5316\u4ee5\u63d0\u9ad8\u8d28\u91cf\u548c\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5728ICLR 2025\u771f\u5b9e\u7814\u7a76\u95ee\u9898\u6570\u636e\u96c6\uff08100\u4e2a\u7814\u7a76\u95ee\u9898\uff09\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7ecf\u8fc712\u6b21\u4f18\u5316\u8fed\u4ee3\u540e\uff0c\u751f\u6210\u5047\u8bbe\u7684\u5e73\u5747ELO\u5206\u6570\u63d0\u9ad8\u4e86116.3\uff0c\u8d85\u8fc7\u771f\u5b9e\u8bba\u6587\u6458\u8981\u57fa\u51c617.8\uff1b\u540c\u65f6\uff0c\u6846\u67b6\u7684\u603b\u4f53\u4e0d\u786e\u5b9a\u6027\uff08\u901a\u8fc7\u9999\u519c\u71b5\u8861\u91cf\uff09\u663e\u8457\u4e0b\u964d\u4e860.92\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u6982\u7387\u63a8\u7406\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u751f\u6210\u7814\u7a76\u5047\u8bbe\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.01977", "pdf": "https://arxiv.org/pdf/2508.01977", "abs": "https://arxiv.org/abs/2508.01977", "authors": ["Fan Gao", "Cheng Huang", "Nyima Tashi", "Yutong Liu", "Xiangxiang Wang", "Thupten Tsering", "Ban Ma-bao", "Renzeg Duojie", "Gadeng Luosang", "Rinchen Dongrub", "Dorje Tashi", "Xiao Feng", "Hao Wang", "Yongbin Yu"], "title": "TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "To address the severe data scarcity in Tibetan, a low-resource language\nspoken by over six million people, we introduce TIBSTC-CoT, the large-scale,\nmulti-domain Tibetan dataset automatically constructed via chain-of-thought\nprompting with large language models (LLMs). TIBSTC-CoT establishes a scalable\nand reproducible framework for dataset creation in low-resource settings,\ncovering diverse domains and reasoning patterns essential for language\nunderstanding and generation. Building on this dataset, we develop the\nSunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with\nchain-of-thought capabilities. Trained entirely on TIBSTC-CoT,\nSunshine-thinking has demonstrated strong reasoning and generation performance,\ncomparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a\nsignificant step toward inclusive AI by enabling high-quality Tibetan language\nprocessing through both resource creation and model innovation. All data are\navailable: https://github.com/Vicentvankor/sun-shine.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u85cf\u8bed\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u672c\u7814\u7a76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u85cf\u8bed\u6570\u636e\u96c6TIBSTC-CoT\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86\u85cf\u8bed\u5927\u6a21\u578bSunshine-thinking\uff0c\u5176\u6027\u80fd\u53ef\u4e0eSOTA\u591a\u8bed\u8a00\u6a21\u578b\u5ab2\u7f8e\uff0c\u6709\u6548\u63a8\u52a8\u4e86\u85cf\u8bedAI\u5904\u7406\u3002", "motivation": "\u89e3\u51b3\u85cf\u8bed\uff08\u4e00\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u4f7f\u7528\u8005\u8d85\u8fc7\u516d\u767e\u4e07\u4eba\uff09\u4e25\u91cd\u7684\u8bed\u8a00\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u4ee5\u4fc3\u8fdb\u5176\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u7684\u53d1\u5c55\u3002", "method": "1. \u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u63d0\u793a\u6280\u672f\uff0c\u81ea\u52a8\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u3001\u591a\u9886\u57df\u7684\u85cf\u8bed\u6570\u636e\u96c6TIBSTC-CoT\u30022. \u57fa\u4e8eTIBSTC-CoT\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u5177\u5907\u601d\u7ef4\u94fe\u80fd\u529b\u7684\u85cf\u8bed\u4e2d\u5fc3\u5316\u5927\u6a21\u578b\u5bb6\u65cfSunshine-thinking\u3002", "result": "\u5f00\u53d1\u7684Sunshine-thinking\u85cf\u8bed\u5927\u6a21\u578b\u5728\u63a8\u7406\u548c\u751f\u6210\u6027\u80fd\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u53ef\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u591a\u8bed\u8a00\u5927\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u672c\u5de5\u4f5c\u901a\u8fc7\u6570\u636e\u8d44\u6e90\u521b\u5efa\u548c\u6a21\u578b\u521b\u65b0\uff0c\u5728\u5b9e\u73b0\u9ad8\u8d28\u91cf\u85cf\u8bed\u5904\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u5b9e\u73b0\u5305\u5bb9\u6027\u4eba\u5de5\u667a\u80fd\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2508.01173", "pdf": "https://arxiv.org/pdf/2508.01173", "abs": "https://arxiv.org/abs/2508.01173", "authors": ["Jiayi Chen", "Jing Li", "Guiling Wang"], "title": "MARS: A Meta-Adaptive Reinforcement Learning Framework for Risk-Aware Multi-Agent Portfolio Management", "categories": ["cs.LG", "cs.MA"], "comment": null, "summary": "Reinforcement Learning (RL) has shown significant promise in automated\nportfolio management; however, effectively balancing risk and return remains a\ncentral challenge, as many models fail to adapt to dynamically changing market\nconditions. In this paper, we propose Meta-controlled Agents for a Risk-aware\nSystem (MARS), a novel RL framework designed to explicitly address this\nlimitation through a multi-agent, risk-aware approach. Instead of a single\nmonolithic model, MARS employs a Heterogeneous Agent Ensemble where each agent\npossesses a unique, intrinsic risk profile. This profile is enforced by a\ndedicated Safety-Critic network and a specific risk-tolerance threshold,\nallowing agents to specialize in behaviors ranging from capital preservation to\naggressive growth. To navigate different market regimes, a high-level\nMeta-Adaptive Controller (MAC) learns to dynamically orchestrate the ensemble.\nBy adjusting its reliance on conservative versus aggressive agents, the MAC\neffectively lowers portfolio volatility during downturns and seeks higher\nreturns in bull markets, thus minimizing maximum drawdown and enhancing overall\nstability. This two-tiered structure allows MARS to generate a disciplined and\nadaptive portfolio that is robust to market fluctuations. The framework\nachieves a superior balance between risk and return by leveraging behavioral\ndiversity rather than explicit market-feature engineering. Experiments on major\ninternational stock indexes, including periods of significant financial crisis,\ndemonstrate the efficacy of our framework on risk-adjusted criteria,\nsignificantly reducing maximum drawdown and volatility while maintaining\ncompetitive returns.", "AI": {"tldr": "MARS\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e0d\u540c\u98ce\u9669\u504f\u597d\u7684\u5f02\u6784\u667a\u80fd\u4f53\u548c\u5143\u9002\u5e94\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u4e86\u98ce\u9669\u4e0e\u6536\u76ca\u7684\u52a8\u6001\u5e73\u8861\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u6295\u8d44\u7ec4\u5408\u98ce\u9669\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u5728\u81ea\u52a8\u5316\u6295\u8d44\u7ec4\u5408\u7ba1\u7406\u4e2d\u96be\u4ee5\u6709\u6548\u5e73\u8861\u98ce\u9669\u4e0e\u6536\u76ca\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u52a8\u6001\u5e02\u573a\u6761\u4ef6\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faMARS\u6846\u67b6\uff0c\u5176\u91c7\u7528\u4e24\u5c42\u7ed3\u6784\uff1a1) \u5f02\u6784\u667a\u80fd\u4f53\u96c6\u6210\uff1a\u5305\u542b\u591a\u4e2a\u5177\u6709\u72ec\u7279\u98ce\u9669\u504f\u597d\uff08\u7531Safety-Critic\u7f51\u7edc\u548c\u9608\u503c\u63a7\u5236\uff09\u7684\u667a\u80fd\u4f53\u30022) \u5143\u9002\u5e94\u63a7\u5236\u5668(MAC)\uff1a\u5b66\u4e60\u52a8\u6001\u534f\u8c03\u8fd9\u4e9b\u667a\u80fd\u4f53\uff0c\u6839\u636e\u5e02\u573a\u72b6\u51b5\u8c03\u6574\u7ec4\u5408\u7b56\u7565\uff0c\u4ee5\u964d\u4f4e\u6ce2\u52a8\u6027\u5e76\u8ffd\u6c42\u6536\u76ca\u3002", "result": "\u5728\u56fd\u9645\u4e3b\u8981\u80a1\u6307\uff08\u5305\u62ec\u91d1\u878d\u5371\u673a\u65f6\u671f\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u98ce\u9669\u8c03\u6574\u6807\u51c6\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6700\u5927\u56de\u64a4\u548c\u6ce2\u52a8\u6027\uff0c\u540c\u65f6\u7ef4\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u56de\u62a5\u3002", "conclusion": "MARS\u901a\u8fc7\u5229\u7528\u884c\u4e3a\u591a\u6837\u6027\u800c\u975e\u5e02\u573a\u7279\u5f81\u5de5\u7a0b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u98ce\u9669\u4e0e\u6536\u76ca\u5e73\u8861\u95ee\u9898\uff0c\u80fd\u751f\u6210\u5bf9\u5e02\u573a\u6ce2\u52a8\u9c81\u68d2\u7684\u81ea\u9002\u5e94\u6295\u8d44\u7ec4\u5408\u3002"}}
{"id": "2508.01254", "pdf": "https://arxiv.org/pdf/2508.01254", "abs": "https://arxiv.org/abs/2508.01254", "authors": ["Zihan Li", "Wei Sun", "Jing Hu", "Jianhua Yin", "Jianlong Wu", "Liqiang Nie"], "title": "Self-Enhanced Image Clustering with Cross-Modal Semantic Consistency", "categories": ["cs.CV"], "comment": null, "summary": "While large language-image pre-trained models like CLIP offer powerful\ngeneric features for image clustering, existing methods typically freeze the\nencoder. This creates a fundamental mismatch between the model's task-agnostic\nrepresentations and the demands of a specific clustering task, imposing a\nceiling on performance. To break this ceiling, we propose a self-enhanced\nframework based on cross-modal semantic consistency for efficient image\nclustering. Our framework first builds a strong foundation via Cross-Modal\nSemantic Consistency and then specializes the encoder through Self-Enhancement.\nIn the first stage, we focus on Cross-Modal Semantic Consistency. By mining\nconsistency between generated image-text pairs at the instance, cluster\nassignment, and cluster center levels, we train lightweight clustering heads to\nalign with the rich semantics of the pre-trained model. This alignment process\nis bolstered by a novel method for generating higher-quality cluster centers\nand a dynamic balancing regularizer to ensure well-distributed assignments. In\nthe second stage, we introduce a Self-Enhanced fine-tuning strategy. The\nwell-aligned model from the first stage acts as a reliable pseudo-label\ngenerator. These self-generated supervisory signals are then used to feed back\nthe efficient, joint optimization of the vision encoder and clustering heads,\nunlocking their full potential. Extensive experiments on six mainstream\ndatasets show that our method outperforms existing deep clustering methods by\nsignificant margins. Notably, our ViT-B/32 model already matches or even\nsurpasses the accuracy of state-of-the-art methods built upon the far larger\nViT-L/14.", "AI": {"tldr": "\u9488\u5bf9CLIP\u7b49\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u56fe\u50cf\u805a\u7c7b\u4e2d\u7f16\u7801\u5668\u51bb\u7ed3\u5bfc\u81f4\u7684\u6027\u80fd\u74f6\u9888\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u8de8\u6a21\u6001\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u81ea\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u4f18\u5316\u663e\u8457\u63d0\u5347\u4e86\u805a\u7c7b\u6027\u80fd\uff0c\u5e76\u4f7f\u5c0f\u6a21\u578b\u8fbe\u5230\u751a\u81f3\u8d85\u8d8a\u5927\u6a21\u578b\u7684SOTA\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u578b\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5982CLIP\uff09\u7684\u56fe\u50cf\u805a\u7c7b\u65b9\u6cd5\u901a\u5e38\u51bb\u7ed3\u5176\u7f16\u7801\u5668\uff0c\u5bfc\u81f4\u6a21\u578b\u901a\u7528\u7279\u5f81\u4e0e\u7279\u5b9a\u805a\u7c7b\u4efb\u52a1\u9700\u6c42\u4e0d\u5339\u914d\uff0c\u4ece\u800c\u9650\u5236\u4e86\u805a\u7c7b\u6027\u80fd\u7684\u4e0a\u9650\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u8de8\u6a21\u6001\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u81ea\u589e\u5f3a\u6846\u67b6\u3002\u7b2c\u4e00\u9636\u6bb5\uff0c\u901a\u8fc7\u6316\u6398\u751f\u6210\u56fe\u50cf-\u6587\u672c\u5bf9\u5728\u5b9e\u4f8b\u3001\u805a\u7c7b\u5206\u914d\u548c\u805a\u7c7b\u4e2d\u5fc3\u5c42\u9762\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u805a\u7c7b\u5934\uff0c\u5e76\u5f15\u5165\u9ad8\u8d28\u91cf\u805a\u7c7b\u4e2d\u5fc3\u751f\u6210\u548c\u52a8\u6001\u5e73\u8861\u6b63\u5219\u5316\u3002\u7b2c\u4e8c\u9636\u6bb5\uff0c\u5c06\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4f5c\u4e3a\u4f2a\u6807\u7b7e\u751f\u6210\u5668\uff0c\u8fdb\u884c\u89c6\u89c9\u7f16\u7801\u5668\u548c\u805a\u7c7b\u5934\u7684\u8054\u5408\u81ea\u589e\u5f3a\u5fae\u8c03\u3002", "result": "\u5728\u516d\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u805a\u7c7b\u65b9\u6cd5\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u5176ViT-B/32\u6a21\u578b\u5df2\u80fd\u5339\u654c\u751a\u81f3\u8d85\u8d8a\u57fa\u4e8e\u66f4\u5927ViT-L/14\u6a21\u578b\u7684\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u7a81\u7834\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7f16\u7801\u5668\u51bb\u7ed3\u9020\u6210\u7684\u6027\u80fd\u74f6\u9888\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u81ea\u589e\u5f3a\u7b56\u7565\uff0c\u5728\u56fe\u50cf\u805a\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002\u7814\u7a76\u8fd8\u8bc1\u660e\u4e86\u5c0f\u5c3a\u5bf8\u6a21\u578b\uff08ViT-B/32\uff09\u5728\u8fbe\u5230\u6216\u8d85\u8d8a\u5927\u5c3a\u5bf8\u6a21\u578b\uff08ViT-L/14\uff09\u6027\u80fd\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.01751", "pdf": "https://arxiv.org/pdf/2508.01751", "abs": "https://arxiv.org/abs/2508.01751", "authors": ["Pierre Schaus", "Charles Thomas", "Roger Kameugne"], "title": "Implementing Cumulative Functions with Generalized Cumulative Constraints", "categories": ["cs.AI"], "comment": null, "summary": "Modeling scheduling problems with conditional time intervals and cumulative\nfunctions has become a common approach when using modern commercial constraint\nprogramming solvers. This paradigm enables the modeling of a wide range of\nscheduling problems, including those involving producers and consumers.\nHowever, it is unavailable in existing open-source solvers and practical\nimplementation details remain undocumented. In this work, we present an\nimplementation of this modeling approach using a single, generic global\nconstraint called the Generalized Cumulative. We also introduce a novel\ntime-table filtering algorithm designed to handle tasks defined on conditional\ntime-intervals. Experimental results demonstrate that this approach, combined\nwith the new filtering algorithm, performs competitively with existing solvers\nenabling the modeling of producer and consumer scheduling problems and\neffectively scales to large problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5f00\u6e90\u6c42\u89e3\u5668\u4e2d\u5efa\u6a21\u5e26\u6761\u4ef6\u65f6\u95f4\u95f4\u9694\u548c\u7d2f\u79ef\u51fd\u6570\u7684\u8c03\u5ea6\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u5e7f\u4e49\u7d2f\u79ef\u7ea6\u675f\u548c\u4e00\u79cd\u65b0\u578b\u8fc7\u6ee4\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4e0e\u5546\u4e1a\u6c42\u89e3\u5668\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u5546\u4e1a\u7ea6\u675f\u89c4\u5212\u6c42\u89e3\u5668\u4e2d\u5e38\u89c1\u7684\u3001\u7528\u4e8e\u5efa\u6a21\u8c03\u5ea6\u95ee\u9898\u7684\u6761\u4ef6\u65f6\u95f4\u95f4\u9694\u548c\u7d2f\u79ef\u51fd\u6570\u65b9\u6cd5\uff0c\u5728\u73b0\u6709\u5f00\u6e90\u6c42\u89e3\u5668\u4e2d\u7f3a\u5931\uff0c\u4e14\u7f3a\u4e4f\u5b9e\u9645\u5b9e\u73b0\u7ec6\u8282\u6587\u6863\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u5e7f\u4e49\u7d2f\u79ef\u201d\uff08Generalized Cumulative\uff09\u7684\u5355\u4e00\u901a\u7528\u5168\u5c40\u7ea6\u675f\uff0c\u4ee5\u5b9e\u73b0\u8be5\u5efa\u6a21\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u95f4\u8868\u8fc7\u6ee4\u7b97\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u5904\u7406\u57fa\u4e8e\u6761\u4ef6\u65f6\u95f4\u95f4\u9694\u5b9a\u4e49\u7684\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u65b0\u7684\u8fc7\u6ee4\u7b97\u6cd5\uff0c\u5728\u6027\u80fd\u4e0a\u4e0e\u73b0\u6709\u5546\u4e1a\u6c42\u89e3\u5668\u5177\u6709\u7ade\u4e89\u529b\uff0c\u80fd\u591f\u6709\u6548\u5730\u5efa\u6a21\u751f\u4ea7\u8005\u548c\u6d88\u8d39\u8005\u8c03\u5ea6\u95ee\u9898\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u5927\u578b\u95ee\u9898\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5b9e\u73b0\u65b9\u6cd5\u548c\u8fc7\u6ee4\u7b97\u6cd5\u6210\u529f\u5730\u5c06\u5148\u8fdb\u7684\u8c03\u5ea6\u95ee\u9898\u5efa\u6a21\u80fd\u529b\u5f15\u5165\u5f00\u6e90\u6c42\u89e3\u5668\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7a7a\u767d\uff0c\u5e76\u88ab\u8bc1\u660e\u5728\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.01990", "pdf": "https://arxiv.org/pdf/2508.01990", "abs": "https://arxiv.org/abs/2508.01990", "authors": ["Praveen Tangarajan", "Anand A. Rajasekar", "Manish Rathi", "Vinay Rao Dandin", "Ozan Ersoy"], "title": "Contextually Aware E-Commerce Product Question Answering using RAG", "categories": ["cs.CL", "I.2.7; H.3.3"], "comment": "6 pages, 1 figure, 5 tables. Preprint under review", "summary": "E-commerce product pages contain a mix of structured specifications,\nunstructured reviews, and contextual elements like personalized offers or\nregional variants. Although informative, this volume can lead to cognitive\noverload, making it difficult for users to quickly and accurately find the\ninformation they need. Existing Product Question Answering (PQA) systems often\nfail to utilize rich user context and diverse product information effectively.\nWe propose a scalable, end-to-end framework for e-commerce PQA using Retrieval\nAugmented Generation (RAG) that deeply integrates contextual understanding. Our\nsystem leverages conversational history, user profiles, and product attributes\nto deliver relevant and personalized answers. It adeptly handles objective,\nsubjective, and multi-intent queries across heterogeneous sources, while also\nidentifying information gaps in the catalog to support ongoing content\nimprovement. We also introduce novel metrics to measure the framework's\nperformance which are broadly applicable for RAG system evaluations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u7aef\u5230\u7aef\u7535\u5546\u4ea7\u54c1\u95ee\u7b54\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u7528\u6237\u4e0a\u4e0b\u6587\u548c\u4ea7\u54c1\u6570\u636e\uff0c\u89e3\u51b3\u4fe1\u606f\u8fc7\u8f7d\u95ee\u9898\uff0c\u63d0\u4f9b\u4e2a\u6027\u5316\u7b54\u6848\u5e76\u8bc6\u522b\u4fe1\u606f\u7f3a\u5931\u3002", "motivation": "\u7535\u5b50\u5546\u52a1\u4ea7\u54c1\u9875\u9762\u4fe1\u606f\u5e9e\u6742\uff08\u5305\u542b\u7ed3\u6784\u5316\u3001\u975e\u7ed3\u6784\u5316\u548c\u4e0a\u4e0b\u6587\u5143\u7d20\uff09\uff0c\u5bfc\u81f4\u7528\u6237\u8ba4\u77e5\u8d1f\u8377\u8fc7\u91cd\uff0c\u96be\u4ee5\u5feb\u901f\u51c6\u786e\u627e\u5230\u6240\u9700\u4fe1\u606f\u3002\u73b0\u6709\u4ea7\u54c1\u95ee\u7b54\uff08PQA\uff09\u7cfb\u7edf\u672a\u80fd\u6709\u6548\u5229\u7528\u4e30\u5bcc\u7684\u7528\u6237\u4e0a\u4e0b\u6587\u548c\u591a\u6837\u5316\u7684\u4ea7\u54c1\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7aef\u5230\u7aef\u7535\u5546\u4ea7\u54c1\u95ee\u7b54\uff08PQA\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\uff0c\u5e76\u6df1\u5ea6\u6574\u5408\u4e0a\u4e0b\u6587\u7406\u89e3\u3002\u7cfb\u7edf\u5229\u7528\u4f1a\u8bdd\u5386\u53f2\u3001\u7528\u6237\u753b\u50cf\u548c\u4ea7\u54c1\u5c5e\u6027\u6765\u63d0\u4f9b\u76f8\u5173\u548c\u4e2a\u6027\u5316\u7684\u7b54\u6848\u3002\u5b83\u80fd\u7075\u6d3b\u5904\u7406\u8de8\u5f02\u6784\u6570\u636e\u6e90\u7684\u5ba2\u89c2\u3001\u4e3b\u89c2\u548c\u591a\u610f\u56fe\u67e5\u8be2\uff0c\u5e76\u8bc6\u522b\u76ee\u5f55\u4e2d\u7684\u4fe1\u606f\u7a7a\u767d\u4ee5\u652f\u6301\u5185\u5bb9\u6539\u8fdb\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u9002\u7528\u4e8eRAG\u7cfb\u7edf\u8bc4\u4f30\u7684\u65b0\u578b\u6027\u80fd\u6307\u6807\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u63d0\u4f9b\u76f8\u5173\u4e14\u4e2a\u6027\u5316\u7684\u7b54\u6848\uff0c\u5e76\u719f\u7ec3\u5904\u7406\u5ba2\u89c2\u3001\u4e3b\u89c2\u53ca\u591a\u610f\u56fe\u67e5\u8be2\u3002\u901a\u8fc7\u8bc6\u522b\u4ea7\u54c1\u76ee\u5f55\u4e2d\u7684\u4fe1\u606f\u7a7a\u767d\uff0c\u652f\u6301\u6301\u7eed\u7684\u5185\u5bb9\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5f15\u5165\u4e86\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8eRAG\u7cfb\u7edf\u8bc4\u4f30\u7684\u65b0\u578b\u6027\u80fd\u6307\u6807\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8eRAG\u7684\u7535\u5546\u4ea7\u54c1\u95ee\u7b54\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6df1\u5ea6\u6574\u5408\u7528\u6237\u4e0a\u4e0b\u6587\u548c\u591a\u6837\u5316\u4ea7\u54c1\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7535\u5546\u4fe1\u606f\u8fc7\u8f7d\u548c\u73b0\u6709\u7cfb\u7edf\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u63d0\u4f9b\u4e2a\u6027\u5316\u3001\u7cbe\u51c6\u7b54\u6848\uff0c\u5e76\u8f85\u52a9\u5e73\u53f0\u8fdb\u884c\u5185\u5bb9\u4f18\u5316\u7684\u76ee\u6807\u3002"}}
{"id": "2508.01174", "pdf": "https://arxiv.org/pdf/2508.01174", "abs": "https://arxiv.org/abs/2508.01174", "authors": ["Kaichen Zhang", "Shenghao Gao", "Yuzhong Hong", "Haipeng Sun", "Junwei Bao", "Hongfei Jiang", "Yang Song", "Hong Dingqian", "Hui Xiong"], "title": "RSPO: Risk-Seeking Policy Optimization for Pass@k and Max@k Metrics in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Current large language model post-training optimizes a risk-neutral objective\nthat maximizes expected reward, yet evaluation relies heavily on risk-seeking\nmetrics like Pass@k (at least one success in k trials) and Max@k (maximum\nreward across k responses). This mismatch in risk preferences can inevitably\nlead to suboptimal performance. To bridge this gap, we propose Risk-Seeking\nPolicy Optimization (RSPO), a novel method that directly targets Pass@k and\nMax@k during training. A key challenge in optimizing these metrics is the\n\"hitchhiking\" problem: low-reward responses are inadvertently reinforced if\nthey co-occur with a high-reward response within a sample of k generations,\nresulting in inefficient optimization. RSPO addresses this problem by\nleveraging the closed-form probability that a given response is the maximum\namong k samplings. Despite the complexity of nested gradients over multiple\nresponses, RSPO produces efficient, unbiased gradient estimators for both\nmetrics. We validate our approach with both rigorous theoretical analysis and\ncomprehensive experimental results.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u98ce\u9669\u5bfb\u6c42\u7b56\u7565\u4f18\u5316\uff08RSPO\uff09\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u76ee\u6807\u4e0e\u8bc4\u4f30\u6307\u6807\u4e4b\u95f4\u98ce\u9669\u504f\u597d\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u76f4\u63a5\u4f18\u5316Pass@k\u548cMax@k\u7b49\u98ce\u9669\u5bfb\u6c42\u6307\u6807\uff0c\u5e76\u6709\u6548\u89e3\u51b3\u4e86\u201c\u642d\u4fbf\u8f66\u201d\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u540e\u8bad\u7ec3\u8fc7\u7a0b\u901a\u5e38\u4f18\u5316\u98ce\u9669\u4e2d\u6027\u7684\u76ee\u6807\uff08\u6700\u5927\u5316\u671f\u671b\u5956\u52b1\uff09\uff0c\u800c\u6a21\u578b\u8bc4\u4f30\u5374\u9ad8\u5ea6\u4f9d\u8d56\u98ce\u9669\u5bfb\u6c42\u6307\u6807\uff08\u5982Pass@k\u548cMax@k\uff09\u3002\u8fd9\u79cd\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u4e4b\u95f4\u98ce\u9669\u504f\u597d\u7684\u4e0d\u5339\u914d\u4f1a\u5bfc\u81f4\u6b21\u4f18\u7684\u6027\u80fd\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u98ce\u9669\u5bfb\u6c42\u7b56\u7565\u4f18\u5316\uff08RSPO\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u9488\u5bf9Pass@k\u548cMax@k\u6307\u6807\u8fdb\u884c\u4f18\u5316\u3002\u4e3a\u89e3\u51b3\u4f18\u5316\u8fd9\u4e9b\u6307\u6807\u65f6\u5b58\u5728\u7684\u201c\u642d\u4fbf\u8f66\u201d\u95ee\u9898\uff08\u5373\u4f4e\u5956\u52b1\u54cd\u5e94\u53ef\u80fd\u56e0\u4e0e\u9ad8\u5956\u52b1\u54cd\u5e94\u540c\u65f6\u51fa\u73b0\u800c\u88ab\u5f3a\u5316\uff09\uff0cRSPO\u5229\u7528\u7ed9\u5b9a\u54cd\u5e94\u5728k\u6b21\u91c7\u6837\u4e2d\u6210\u4e3a\u6700\u5927\u503c\u7684\u5c01\u95ed\u5f62\u5f0f\u6982\u7387\u8fdb\u884c\u5904\u7406\u3002", "result": "\u5c3d\u7ba1\u6d89\u53ca\u591a\u4e2a\u54cd\u5e94\u7684\u5d4c\u5957\u68af\u5ea6\u590d\u6742\u6027\uff0cRSPO\u80fd\u591f\u4e3aPass@k\u548cMax@k\u8fd9\u4e24\u4e2a\u6307\u6807\u751f\u6210\u9ad8\u6548\u3001\u65e0\u504f\u7684\u68af\u5ea6\u4f30\u8ba1\u5668\u3002", "conclusion": "RSPO\u65b9\u6cd5\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u98ce\u9669\u5bfb\u6c42\u6307\u6807\uff0c\u6210\u529f\u5f25\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u4e4b\u95f4\u7684\u98ce\u9669\u504f\u597d\u5dee\u8ddd\uff0c\u5176\u6709\u6548\u6027\u5df2\u901a\u8fc7\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\u548c\u5168\u9762\u7684\u5b9e\u9a8c\u7ed3\u679c\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2508.01259", "pdf": "https://arxiv.org/pdf/2508.01259", "abs": "https://arxiv.org/abs/2508.01259", "authors": ["Zhengxue Wang", "Yuan Wu", "Xiang Li", "Zhiqiang Yan", "Jian Yang"], "title": "SpatioTemporal Difference Network for Video Depth Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Depth super-resolution has achieved impressive performance, and the\nincorporation of multi-frame information further enhances reconstruction\nquality. Nevertheless, statistical analyses reveal that video depth\nsuper-resolution remains affected by pronounced long-tailed distributions, with\nthe long-tailed effects primarily manifesting in spatial non-smooth regions and\ntemporal variation zones. To address these challenges, we propose a novel\nSpatioTemporal Difference Network (STDNet) comprising two core branches: a\nspatial difference branch and a temporal difference branch. In the spatial\ndifference branch, we introduce a spatial difference mechanism to mitigate the\nlong-tailed issues in spatial non-smooth regions. This mechanism dynamically\naligns RGB features with learned spatial difference representations, enabling\nintra-frame RGB-D aggregation for depth calibration. In the temporal difference\nbranch, we further design a temporal difference strategy that preferentially\npropagates temporal variation information from adjacent RGB and depth frames to\nthe current depth frame, leveraging temporal difference representations to\nachieve precise motion compensation in temporal long-tailed areas. Extensive\nexperimental results across multiple datasets demonstrate the effectiveness of\nour STDNet, outperforming existing approaches.", "AI": {"tldr": "\u9488\u5bf9\u89c6\u9891\u6df1\u5ea6\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u65f6\u7a7a\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u65f6\u7a7a\u5dee\u5f02\u7f51\u7edc\uff08STDNet\uff09\uff0c\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u548c\u65f6\u95f4\u5dee\u5f02\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u591a\u5e27\u4fe1\u606f\u80fd\u589e\u5f3a\u6df1\u5ea6\u8d85\u5206\u8fa8\u7387\u6027\u80fd\uff0c\u4f46\u89c6\u9891\u6df1\u5ea6\u8d85\u5206\u8fa8\u7387\u4ecd\u53d7\u5236\u4e8e\u663e\u8457\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u5c24\u5176\u4f53\u73b0\u5728\u7a7a\u95f4\u975e\u5e73\u6ed1\u533a\u57df\u548c\u65f6\u95f4\u53d8\u5316\u533a\u57df\u3002", "method": "\u63d0\u51fa\u65f6\u7a7a\u5dee\u5f02\u7f51\u7edc\uff08STDNet\uff09\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u5206\u652f\uff1a\u7a7a\u95f4\u5dee\u5f02\u5206\u652f\u548c\u65f6\u95f4\u5dee\u5f02\u5206\u652f\u3002\u7a7a\u95f4\u5dee\u5f02\u5206\u652f\u5f15\u5165\u7a7a\u95f4\u5dee\u5f02\u673a\u5236\uff0c\u901a\u8fc7\u52a8\u6001\u5bf9\u9f50RGB\u7279\u5f81\u4e0e\u7a7a\u95f4\u5dee\u5f02\u8868\u793a\uff0c\u5b9e\u73b0\u5e27\u5185RGB-D\u805a\u5408\u4ee5\u6821\u51c6\u6df1\u5ea6\u3002\u65f6\u95f4\u5dee\u5f02\u5206\u652f\u8bbe\u8ba1\u65f6\u95f4\u5dee\u5f02\u7b56\u7565\uff0c\u5229\u7528\u65f6\u95f4\u5dee\u5f02\u8868\u793a\uff0c\u4f18\u5148\u4f20\u64ad\u76f8\u90bbRGB\u548c\u6df1\u5ea6\u5e27\u7684\u65f6\u95f4\u53d8\u5316\u4fe1\u606f\u5230\u5f53\u524d\u6df1\u5ea6\u5e27\uff0c\u5b9e\u73b0\u65f6\u95f4\u957f\u5c3e\u533a\u57df\u7684\u7cbe\u786e\u8fd0\u52a8\u8865\u507f\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684STDNet\u6709\u6548\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "STDNet\u901a\u8fc7\u7279\u6709\u7684\u65f6\u7a7a\u5dee\u5f02\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u6df1\u5ea6\u8d85\u5206\u8fa8\u7387\u4e2d\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u91cd\u5efa\u7684\u6027\u80fd\u548c\u7cbe\u5ea6\u3002"}}
{"id": "2508.01763", "pdf": "https://arxiv.org/pdf/2508.01763", "abs": "https://arxiv.org/abs/2508.01763", "authors": ["Saleh Nikooroo", "Thomas Engel"], "title": "Reasoning Systems as Structured Processes: Foundations, Failures, and Formal Criteria", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "This paper outlines a general formal framework for reasoning systems,\nintended to support future analysis of inference architectures across domains.\nWe model reasoning systems as structured tuples comprising phenomena,\nexplanation space, inference and generation maps, and a principle base. The\nformulation accommodates logical, algorithmic, and learning-based reasoning\nprocesses within a unified structural schema, while remaining agnostic to any\nspecific reasoning algorithm or logic system. We survey basic internal\ncriteria--including coherence, soundness, and completeness-and catalog typical\nfailure modes such as contradiction, incompleteness, and non-convergence. The\nframework also admits dynamic behaviors like iterative refinement and principle\nevolution. The goal of this work is to establish a foundational structure for\nrepresenting and comparing reasoning systems, particularly in contexts where\ninternal failure, adaptation, or fragmentation may arise. No specific solution\narchitecture is proposed; instead, we aim to support future theoretical and\npractical investigations into reasoning under structural constraint.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u63a8\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u7edf\u4e00\u5efa\u6a21\u4e0d\u540c\u7c7b\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u8bc6\u522b\u5e38\u89c1\u7684\u6545\u969c\u6a21\u5f0f\uff0c\u65e8\u5728\u652f\u6301\u672a\u6765\u5bf9\u63a8\u7406\u67b6\u6784\u7684\u7814\u7a76\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5efa\u7acb\u4e00\u4e2a\u57fa\u7840\u7ed3\u6784\uff0c\u4ee5\u8868\u793a\u548c\u6bd4\u8f83\u4e0d\u540c\u9886\u57df\u7684\u63a8\u7406\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u5185\u90e8\u6545\u969c\u3001\u9002\u5e94\u6216\u788e\u7247\u5316\u53ef\u80fd\u51fa\u73b0\u7684\u80cc\u666f\u4e0b\uff0c\u4ece\u800c\u652f\u6301\u672a\u6765\u5bf9\u63a8\u7406\u67b6\u6784\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u5206\u6790\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u5c06\u63a8\u7406\u7cfb\u7edf\u5efa\u6a21\u4e3a\u5305\u542b\u73b0\u8c61\u3001\u89e3\u91ca\u7a7a\u95f4\u3001\u63a8\u7406\u548c\u751f\u6210\u6620\u5c04\u4ee5\u53ca\u539f\u5219\u57fa\u7840\u7684\u7ed3\u6784\u5316\u5143\u7ec4\u3002\u8be5\u6846\u67b6\u80fd\u591f\u517c\u5bb9\u903b\u8f91\u3001\u7b97\u6cd5\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u540c\u65f6\u4e0d\u4f9d\u8d56\u4e8e\u4efb\u4f55\u7279\u5b9a\u7684\u63a8\u7406\u7b97\u6cd5\u6216\u903b\u8f91\u7cfb\u7edf\u3002\u5b83\u8fd8\u8c03\u67e5\u4e86\u5185\u90e8\u6807\u51c6\uff08\u5982\u4e00\u81f4\u6027\u3001\u53ef\u9760\u6027\u548c\u5b8c\u5907\u6027\uff09\u5e76\u7f16\u76ee\u4e86\u5178\u578b\u7684\u6545\u969c\u6a21\u5f0f\uff08\u5982\u77db\u76fe\u3001\u4e0d\u5b8c\u5907\u6027\u548c\u4e0d\u6536\u655b\u6027\uff09\uff0c\u5e76\u8003\u8651\u4e86\u8fed\u4ee3\u7ec6\u5316\u548c\u539f\u5219\u6f14\u5316\u7b49\u52a8\u6001\u884c\u4e3a\u3002", "result": "\u672c\u6587\u7684\u4e3b\u8981\u6210\u679c\u662f\u63d0\u51fa\u5e76\u8be6\u7ec6\u9610\u8ff0\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u7edf\u4e00\u5efa\u6a21\u548c\u5206\u6790\u591a\u79cd\u63a8\u7406\u7cfb\u7edf\uff0c\u5e76\u80fd\u8bc6\u522b\u5176\u5185\u90e8\u6807\u51c6\u548c\u5178\u578b\u6545\u969c\u6a21\u5f0f\u3002\u8be5\u6846\u67b6\u80fd\u591f\u9002\u5e94\u52a8\u6001\u884c\u4e3a\uff0c\u4e3a\u672a\u6765\u5bf9\u63a8\u7406\u7cfb\u7edf\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u7ed3\u6784\u57fa\u7840\uff0c\u4f46\u672a\u63d0\u51fa\u5177\u4f53\u7684\u89e3\u51b3\u65b9\u6848\u67b6\u6784\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u65e8\u5728\u5efa\u7acb\u4e00\u4e2a\u57fa\u7840\u7ed3\u6784\uff0c\u7528\u4e8e\u8868\u793a\u548c\u6bd4\u8f83\u63a8\u7406\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u5185\u90e8\u6545\u969c\u3001\u9002\u5e94\u6216\u788e\u7247\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u800c\u4e3a\u672a\u6765\u5728\u7ed3\u6784\u7ea6\u675f\u4e0b\u5bf9\u63a8\u7406\u7cfb\u7edf\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u7814\u7a76\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2508.01999", "pdf": "https://arxiv.org/pdf/2508.01999", "abs": "https://arxiv.org/abs/2508.01999", "authors": ["Md Badsha Biswas", "\u00d6zlem Uzuner"], "title": "Prompting Large Language Models to Detect Dementia Family Caregivers", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Social media, such as Twitter, provides opportunities for caregivers of\ndementia patients to share their experiences and seek support for a variety of\nreasons. Availability of this information online also paves the way for the\ndevelopment of internet-based interventions in their support. However, for this\npurpose, tweets written by caregivers of dementia patients must first be\nidentified. This paper demonstrates our system for the SMM4H 2025 shared task\n3, which focuses on detecting tweets posted by individuals who have a family\nmember with dementia. The task is outlined as a binary classification problem,\ndifferentiating between tweets that mention dementia in the context of a family\nmember and those that do not. Our solution to this problem explores large\nlanguage models (LLMs) with various prompting methods. Our results show that a\nsimple zero-shot prompt on a fine-tuned model yielded the best results. Our\nfinal system achieved a macro F1-score of 0.95 on the validation set and the\ntest set. Our full code is available on GitHub.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc6\u522b\u75f4\u5446\u75c7\u60a3\u8005\u770b\u62a4\u8005\u7684\u63a8\u6587\uff0c\u5b8f\u89c2F1\u5206\u6570\u8fbe\u52300.95\u3002", "motivation": "\u75f4\u5446\u75c7\u60a3\u8005\u770b\u62a4\u8005\u5728\u793e\u4ea4\u5a92\u4f53\u4e0a\u5206\u4eab\u7ecf\u9a8c\u5e76\u5bfb\u6c42\u652f\u6301\u3002\u4e3a\u5f00\u53d1\u5728\u7ebf\u5e72\u9884\u63aa\u65bd\u4ee5\u5e2e\u52a9\u4ed6\u4eec\uff0c\u9700\u9996\u5148\u51c6\u786e\u8bc6\u522b\u8fd9\u4e9b\u770b\u62a4\u8005\u53d1\u5e03\u7684\u63a8\u6587\u3002", "method": "\u5c06\u4efb\u52a1\u5b9a\u4e49\u4e3a\u4e8c\u5143\u5206\u7c7b\u95ee\u9898\uff0c\u533a\u5206\u63d0\u53ca\u5bb6\u5ead\u6210\u5458\u75f4\u5446\u75c7\u7684\u63a8\u6587\u4e0e\u5426\u3002\u89e3\u51b3\u65b9\u6848\u63a2\u7d22\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ca\u591a\u79cd\u63d0\u793a\u65b9\u6cd5\uff0c\u5176\u4e2d\u5728\u5fae\u8c03\u6a21\u578b\u4e0a\u91c7\u7528\u7b80\u5355\u96f6\u6837\u672c\u63d0\u793a\u8868\u73b0\u6700\u4f73\u3002", "result": "\u6700\u7ec8\u7cfb\u7edf\u5728\u9a8c\u8bc1\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u5747\u53d6\u5f97\u4e860.95\u7684\u5b8f\u89c2F1\u5206\u6570\uff0c\u5176\u4e2d\u5728\u5fae\u8c03\u6a21\u578b\u4e0a\u4f7f\u7528\u7b80\u5355\u96f6\u6837\u672c\u63d0\u793a\u7684\u65b9\u6cd5\u6548\u679c\u6700\u4f73\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc6\u522b\u75f4\u5446\u75c7\u60a3\u8005\u770b\u62a4\u8005\u7684\u63a8\u6587\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u57fa\u4e8e\u4e92\u8054\u7f51\u7684\u5e72\u9884\u63aa\u65bd\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.01175", "pdf": "https://arxiv.org/pdf/2508.01175", "abs": "https://arxiv.org/abs/2508.01175", "authors": ["Shiko Kudo"], "title": "From Taylor Series to Fourier Synthesis: The Periodic Linear Unit", "categories": ["cs.LG", "cs.NA", "cs.NE", "math.NA", "68T07 (Primary) 42A10, 41A30, 65D15 (Secondary)", "I.5.1; G.1.2; G.1.6; I.2.6"], "comment": "15 pages, 5 figures, for associated raw example files and the code\n  repository, see https://github.com/bill13579/plu_activation", "summary": "The dominant paradigm in modern neural networks relies on simple,\nmonotonically-increasing activation functions like ReLU. While effective, this\nparadigm necessitates large, massively-parameterized models to approximate\ncomplex functions. In this paper, we introduce the Periodic Linear Unit (PLU),\na learnable sine-wave based activation with periodic non-monotonicity. PLU is\ndesigned for maximum expressive power and numerical stability, achieved through\nits formulation and a paired innovation we term Repulsive Reparameterization,\nwhich prevents the activation from collapsing into a non-expressive linear\nfunction. We demonstrate that a minimal MLP with only two PLU neurons can solve\nthe spiral classification task, a feat impossible for equivalent networks using\nstandard activations. This suggests a paradigm shift from networks as piecewise\nTaylor-like approximators to powerful Fourier-like function synthesizers,\nachieving exponential gains in parameter efficiency by placing intelligence in\nthe neuron itself.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6b63\u5f26\u6ce2\u7684\u65b0\u578b\u6fc0\u6d3b\u51fd\u6570PLU\uff0c\u901a\u8fc7\u5176\u5468\u671f\u6027\u975e\u5355\u8c03\u6027\u548c\u72ec\u7279\u7684\u91cd\u53c2\u6570\u5316\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u548c\u53c2\u6570\u6548\u7387\uff0c\u4f7f\u6781\u7b80\u7f51\u7edc\u4e5f\u80fd\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u3002", "motivation": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u4f9d\u8d56\u7b80\u5355\u3001\u5355\u8c03\u9012\u589e\u7684\u6fc0\u6d3b\u51fd\u6570\uff08\u5982ReLU\uff09\uff0c\u8fd9\u5bfc\u81f4\u4e3a\u4e86\u8fd1\u4f3c\u590d\u6742\u51fd\u6570\u9700\u8981\u5927\u91cf\u53c2\u6570\u7684\u5e9e\u5927\u6a21\u578b\uff0c\u53c2\u6570\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u5468\u671f\u6027\u7ebf\u6027\u5355\u5143\uff08PLU\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u3001\u57fa\u4e8e\u6b63\u5f26\u6ce2\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u5177\u6709\u5468\u671f\u6027\u975e\u5355\u8c03\u6027\u3002\u4e3a\u786e\u4fdd\u6700\u5927\u8868\u8fbe\u80fd\u529b\u548c\u6570\u503c\u7a33\u5b9a\u6027\uff0cPLU\u7ed3\u5408\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u6392\u65a5\u6027\u91cd\u53c2\u6570\u5316\u201d\u7684\u521b\u65b0\u6280\u672f\uff0c\u4ee5\u9632\u6b62\u5176\u9000\u5316\u4e3a\u975e\u8868\u8fbe\u6027\u7684\u7ebf\u6027\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e00\u4e2a\u4ec5\u5305\u542b\u4e24\u4e2aPLU\u795e\u7ecf\u5143\u7684\u6700\u5c0f\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u5c31\u80fd\u6210\u529f\u89e3\u51b3\u87ba\u65cb\u5206\u7c7b\u4efb\u52a1\uff0c\u800c\u4f7f\u7528\u6807\u51c6\u6fc0\u6d3b\u51fd\u6570\u7684\u7b49\u6548\u7f51\u7edc\u65e0\u6cd5\u5b8c\u6210\u6b64\u4efb\u52a1\u3002", "conclusion": "\u8fd9\u9884\u793a\u7740\u795e\u7ecf\u7f51\u7edc\u8303\u5f0f\u53ef\u80fd\u4ece\u5206\u6bb5\u6cf0\u52d2\u8fd1\u4f3c\u5668\u8f6c\u5411\u5f3a\u5927\u7684\u5085\u91cc\u53f6\u5f0f\u51fd\u6570\u5408\u6210\u5668\uff0c\u901a\u8fc7\u5c06\u667a\u80fd\u878d\u5165\u795e\u7ecf\u5143\u672c\u8eab\uff0c\u5b9e\u73b0\u53c2\u6570\u6548\u7387\u7684\u6307\u6570\u7ea7\u63d0\u5347\u3002"}}
{"id": "2508.01264", "pdf": "https://arxiv.org/pdf/2508.01264", "abs": "https://arxiv.org/abs/2508.01264", "authors": ["Lexiao Zou", "Gongwei Chen", "Yanda Chen", "Miao Zhang"], "title": "Enhancing Diffusion-based Dataset Distillation via Adversary-Guided Curriculum Sampling", "categories": ["cs.CV"], "comment": "Accepted by ICME2025", "summary": "Dataset distillation aims to encapsulate the rich information contained in\ndataset into a compact distilled dataset but it faces performance degradation\nas the image-per-class (IPC) setting or image resolution grows larger. Recent\nadvancements demonstrate that integrating diffusion generative models can\neffectively facilitate the compression of large-scale datasets while\nmaintaining efficiency due to their superiority in matching data distribution\nand summarizing representative patterns. However, images sampled from diffusion\nmodels are always blamed for lack of diversity which may lead to information\nredundancy when multiple independent sampled images are aggregated as a\ndistilled dataset. To address this issue, we propose Adversary-guided\nCurriculum Sampling (ACS), which partitions the distilled dataset into multiple\ncurricula. For generating each curriculum, ACS guides diffusion sampling\nprocess by an adversarial loss to challenge a discriminator trained on sampled\nimages, thus mitigating information overlap between curricula and fostering a\nmore diverse distilled dataset. Additionally, as the discriminator evolves with\nthe progression of curricula, ACS generates images from simpler to more\ncomplex, ensuring efficient and systematic coverage of target data\ninformational spectrum. Extensive experiments demonstrate the effectiveness of\nACS, which achieves substantial improvements of 4.1\\% on Imagewoof and 2.1\\% on\nImageNet-1k over the state-of-the-art.", "AI": {"tldr": "\u9488\u5bf9\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u96c6\u84b8\u998f\u4e2d\u7f3a\u4e4f\u591a\u6837\u6027\u5bfc\u81f4\u7684\u4fe1\u606f\u5197\u4f59\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u5bf9\u6297\u6027\u5f15\u5bfc\u8bfe\u7a0b\u91c7\u6837\uff08ACS\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u635f\u5931\u548c\u5206\u8bfe\u7a0b\u751f\u6210\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u84b8\u998f\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u4e0e\u6027\u80fd\u3002", "motivation": "\u6570\u636e\u96c6\u84b8\u998f\u6280\u672f\u9762\u4e34\u56fe\u50cf\u6570\u91cf\u548c\u5206\u8fa8\u7387\u589e\u52a0\u65f6\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002\u5c3d\u7ba1\u7ed3\u5408\u6269\u6563\u751f\u6210\u6a21\u578b\u80fd\u6709\u6548\u538b\u7f29\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u4f46\u5176\u751f\u6210\u7684\u56fe\u50cf\u7f3a\u4e4f\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u84b8\u998f\u6570\u636e\u96c6\u4e2d\u4fe1\u606f\u5197\u4f59\uff0c\u8fdb\u800c\u5f71\u54cd\u6700\u7ec8\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5bf9\u6297\u6027\u5f15\u5bfc\u8bfe\u7a0b\u91c7\u6837\uff08ACS\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5c06\u84b8\u998f\u6570\u636e\u96c6\u5212\u5206\u4e3a\u591a\u4e2a\u8bfe\u7a0b\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u6027\u635f\u5931\u5f15\u5bfc\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\uff0c\u4f7f\u751f\u6210\u56fe\u50cf\u6311\u6218\u5728\u91c7\u6837\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u5224\u522b\u5668\uff0c\u4ee5\u51cf\u5c11\u8bfe\u7a0b\u95f4\u7684\u4fe1\u606f\u91cd\u53e0\uff0c\u589e\u5f3a\u84b8\u998f\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u3002\u6b64\u5916\uff0c\u5224\u522b\u5668\u4f1a\u968f\u8bfe\u7a0b\u8fdb\u5c55\u800c\u6f14\u5316\uff0c\u4fc3\u4f7f\u6a21\u578b\u4ece\u7b80\u5355\u5230\u590d\u6742\u751f\u6210\u56fe\u50cf\uff0c\u786e\u4fdd\u5bf9\u76ee\u6807\u6570\u636e\u4fe1\u606f\u8c31\u7684\u9ad8\u6548\u7cfb\u7edf\u8986\u76d6\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660eACS\u7684\u6709\u6548\u6027\uff0c\u5728Imagewoof\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e864.1%\u7684\u663e\u8457\u63d0\u5347\uff0c\u5728ImageNet-1k\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e862.1%\u7684\u63d0\u5347\uff0c\u5747\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "ACS\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u96c6\u84b8\u998f\u4e2d\u751f\u6210\u56fe\u50cf\u591a\u6837\u6027\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5bf9\u6297\u6027\u5f15\u5bfc\u548c\u8bfe\u7a0b\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u84b8\u998f\u6570\u636e\u96c6\u7684\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u7cfb\u7edf\u5316\u7684\u4fe1\u606f\u8986\u76d6\u3002"}}
{"id": "2508.01773", "pdf": "https://arxiv.org/pdf/2508.01773", "abs": "https://arxiv.org/abs/2508.01773", "authors": ["Jiuzhou Han", "Wray Buntine", "Ehsan Shareghi"], "title": "Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models have demonstrated remarkable capabilities in complex\nmathematical reasoning tasks, but they inevitably generate errors throughout\nmulti-step solutions. Process-level Reward Models (PRMs) have shown great\npromise by providing supervision and evaluation at each intermediate step,\nthereby effectively improving the models' reasoning abilities. However,\ntraining effective PRMs requires high-quality process reward data, yet existing\nmethods for constructing such data are often labour-intensive or inefficient.\nIn this paper, we propose an uncertainty-driven framework for automated process\nreward data construction, encompassing both data generation and annotation\nprocesses for PRMs. Additionally, we identify the limitations of both majority\nvote and PRMs, and introduce two generic uncertainty-aware output aggregation\nmethods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which\ncombine the strengths of majority vote with PRMs. Extensive experiments on\nProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the\nproposed PRM data construction framework, and demonstrate that the two output\naggregation methods further improve the mathematical reasoning abilities across\ndiverse PRMs. The code and data will be publicly available at\nhttps://github.com/Jiuzhouh/UnPRM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u8fc7\u7a0b\u7ea7\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u6570\u636e\u6784\u5efa\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u65b0\u7684\u8f93\u51fa\u805a\u5408\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6b65\u6570\u5b66\u63a8\u7406\u4e2d\u5e38\u51fa\u9519\uff0c\u800c\u8fc7\u7a0b\u7ea7\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u80fd\u6709\u6548\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002\u7136\u800c\uff0c\u8bad\u7ec3PRM\u9700\u8981\u9ad8\u8d28\u91cf\u8fc7\u7a0b\u5956\u52b1\u6570\u636e\uff0c\u73b0\u6709\u6570\u636e\u6784\u5efa\u65b9\u6cd5\u8017\u65f6\u4e14\u4f4e\u6548\u3002", "method": "1. \u63d0\u51fa\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u8fc7\u7a0b\u5956\u52b1\u6570\u636e\u6784\u5efa\u6846\u67b6\uff0c\u6db5\u76d6\u6570\u636e\u751f\u6210\u548c\u6807\u6ce8\u8fc7\u7a0b\u30022. \u9488\u5bf9\u591a\u6570\u6295\u7968\u548cPRM\u7684\u5c40\u9650\u6027\uff0c\u5f15\u5165\u4e24\u79cd\u901a\u7528\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8f93\u51fa\u805a\u5408\u65b9\u6cd5\uff1a\u6df7\u5408\u591a\u6570\u5956\u52b1\u6295\u7968\uff08Hybrid Majority Reward Vote\uff09\u548c\u52a0\u6743\u5956\u52b1\u9891\u7387\u6295\u7968\uff08Weighted Reward Frequency Vote\uff09\u3002", "result": "\u5728ProcessBench\u3001MATH\u548cGSMPlus\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684PRM\u6570\u636e\u6784\u5efa\u6846\u67b6\u6709\u6548\u4e14\u9ad8\u6548\u3002\u540c\u65f6\uff0c\u4e24\u79cd\u8f93\u51fa\u805a\u5408\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u4e0d\u540cPRM\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u521b\u65b0\u7684\u81ea\u52a8\u5316\u6570\u636e\u6784\u5efa\u6846\u67b6\u548c\u8f93\u51fa\u805a\u5408\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86PRM\u6570\u636e\u83b7\u53d6\u7684\u6311\u6218\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.02013", "pdf": "https://arxiv.org/pdf/2508.02013", "abs": "https://arxiv.org/abs/2508.02013", "authors": ["Changhao Jiang", "Jiajun Sun", "Yifei Cao", "Jiabao Zhuang", "Hui Li", "Xiaoran Fan", "Ming Zhang", "Junjie Ye", "Shihan Dou", "Zhiheng Xi", "Jingqi Tong", "Yilong Wu", "Baoyu Fan", "Zhen Wang", "Tao Liang", "Zhihui Fei", "Mingyang Wan", "Guojun Ma", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents", "categories": ["cs.CL"], "comment": null, "summary": "Recently, role-playing agents have emerged as a promising paradigm for\nachieving personalized interaction and emotional resonance. Existing research\nprimarily focuses on the textual modality, neglecting the critical dimension of\nspeech in realistic interactive scenarios. In particular, there is a lack of\nsystematic evaluation for Speech Role-Playing Agents (SRPAs). To address this\ngap, we construct SpeechRole-Data, a large-scale, high-quality dataset that\ncomprises 98 diverse roles and 112k speech-based single-turn and multi-turn\nconversations. Each role demonstrates distinct vocal characteristics, including\ntimbre and prosody, thereby enabling more sophisticated speech role-playing.\nFurthermore, we propose SpeechRole-Eval, a multidimensional evaluation\nbenchmark that systematically assesses SRPAs performance in key aspects such as\nfundamental interaction ability, speech expressiveness, and role-playing\nfidelity. Experimental results reveal the advantages and challenges of both\ncascaded and end-to-end speech role-playing agents in maintaining vocal style\nconsistency and role coherence. We release all data, code, and baseline models\nto provide a solid foundation for speech-driven multimodal role-playing\nresearch and to foster further developments in this field.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u8bed\u97f3\u89d2\u8272\u626e\u6f14\u6570\u636e\u96c6\u548c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u5206\u6790\u4e86\u73b0\u6709\u8bed\u97f3\u89d2\u8272\u626e\u6f14\u4ee3\u7406\u7684\u4f18\u7f3a\u70b9\uff0c\u4ee5\u63a8\u52a8\u8bed\u97f3\u9a71\u52a8\u7684\u591a\u6a21\u6001\u89d2\u8272\u626e\u6f14\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u89d2\u8272\u626e\u6f14\u4ee3\u7406\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u6a21\u6001\uff0c\u5ffd\u7565\u4e86\u8bed\u97f3\u5728\u5b9e\u9645\u4ea4\u4e92\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u8bed\u97f3\u89d2\u8272\u626e\u6f14\u4ee3\u7406\uff08SRPAs\uff09\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u4e86SpeechRole-Data\uff0c\u4e00\u4e2a\u5305\u542b98\u79cd\u89d2\u8272\u548c11.2\u4e07\u6761\u8bed\u97f3\u5bf9\u8bdd\u7684\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff1b\u63d0\u51fa\u4e86SpeechRole-Eval\uff0c\u4e00\u4e2a\u591a\u7ef4\u5ea6\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30SRPAs\u5728\u4ea4\u4e92\u80fd\u529b\u3001\u8bed\u97f3\u8868\u8fbe\u548c\u89d2\u8272\u626e\u6f14\u4fdd\u771f\u5ea6\u65b9\u9762\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86\u7ea7\u8054\u548c\u7aef\u5230\u7aef\u8bed\u97f3\u89d2\u8272\u626e\u6f14\u4ee3\u7406\u5728\u4fdd\u6301\u8bed\u97f3\u98ce\u683c\u4e00\u81f4\u6027\u548c\u89d2\u8272\u8fde\u8d2f\u6027\u65b9\u9762\u7684\u4f18\u52bf\u4e0e\u6311\u6218\u3002", "conclusion": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u9996\u4e2a\u9488\u5bf9\u8bed\u97f3\u89d2\u8272\u626e\u6f14\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u53d1\u5e03\u4e86\u6240\u6709\u6570\u636e\u3001\u4ee3\u7801\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u4e3a\u8bed\u97f3\u9a71\u52a8\u7684\u591a\u6a21\u6001\u89d2\u8272\u626e\u6f14\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u6709\u671b\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2508.01188", "pdf": "https://arxiv.org/pdf/2508.01188", "abs": "https://arxiv.org/abs/2508.01188", "authors": ["Zhuo Yang", "Jiaqing Xie", "Shuaike Shen", "Daolang Wang", "Yeyun Chen", "Ben Gao", "Shuzhou Sun", "Biqing Qi", "Dongzhan Zhou", "Lei Bai", "Linjiang Chen", "Shufei Zhang", "Jun Jiang", "Tianfan Fu", "Yuqiang Li"], "title": "SpectrumWorld: Artificial Intelligence Foundation for Spectroscopy", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep learning holds immense promise for spectroscopy, yet research and\nevaluation in this emerging field often lack standardized formulations. To\naddress this issue, we introduce SpectrumLab, a pioneering unified platform\ndesigned to systematize and accelerate deep learning research in spectroscopy.\nSpectrumLab integrates three core components: a comprehensive Python library\nfeaturing essential data processing and evaluation tools, along with\nleaderboards; an innovative SpectrumAnnotator module that generates\nhigh-quality benchmarks from limited seed data; and SpectrumBench, a\nmulti-layered benchmark suite covering 14 spectroscopic tasks and over 10\nspectrum types, featuring spectra curated from over 1.2 million distinct\nchemical substances. Thorough empirical studies on SpectrumBench with 18\ncutting-edge multimodal LLMs reveal critical limitations of current approaches.\nWe hope SpectrumLab will serve as a crucial foundation for future advancements\nin deep learning-driven spectroscopy.", "AI": {"tldr": "\u63d0\u51faSpectrumLab\uff0c\u4e00\u4e2a\u7edf\u4e00\u5e73\u53f0\uff0c\u65e8\u5728\u6807\u51c6\u5316\u548c\u52a0\u901f\u5149\u8c31\u5b66\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\uff0c\u5e76\u63ed\u793a\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5149\u8c31\u5b66\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u548c\u8bc4\u4f30\u7f3a\u4e4f\u6807\u51c6\u5316\u516c\u5f0f\u3002", "method": "\u5f15\u5165SpectrumLab\u7edf\u4e00\u5e73\u53f0\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4e00\u4e2a\u7efc\u5408Python\u5e93\uff08\u542b\u6570\u636e\u5904\u7406\u3001\u8bc4\u4f30\u5de5\u5177\u548c\u6392\u884c\u699c\uff09\uff1b\u4e00\u4e2aSpectrumAnnotator\u6a21\u5757\uff08\u4ece\u6709\u9650\u79cd\u5b50\u6570\u636e\u751f\u6210\u9ad8\u8d28\u91cf\u57fa\u51c6\uff09\uff1b\u4ee5\u53caSpectrumBench\u591a\u5c42\u57fa\u51c6\u5957\u4ef6\uff08\u6db5\u76d614\u4e2a\u5149\u8c31\u4efb\u52a1\u548c10\u591a\u79cd\u5149\u8c31\u7c7b\u578b\uff0c\u5305\u542b\u8d85\u8fc7120\u4e07\u79cd\u5316\u5b66\u7269\u8d28\u7684\u5149\u8c31\uff09\u3002", "result": "\u901a\u8fc7\u5728SpectrumBench\u4e0a\u5bf918\u4e2a\u524d\u6cbf\u591a\u6a21\u6001LLM\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u4e25\u91cd\u5c40\u9650\u6027\u3002", "conclusion": "SpectrumLab\u6709\u671b\u4e3a\u672a\u6765\u6df1\u5ea6\u5b66\u4e60\u9a71\u52a8\u7684\u5149\u8c31\u5b66\u7814\u7a76\u5960\u5b9a\u5173\u952e\u57fa\u7840\u3002"}}
{"id": "2508.01269", "pdf": "https://arxiv.org/pdf/2508.01269", "abs": "https://arxiv.org/abs/2508.01269", "authors": ["Pedro Alonso", "Tianrui Li", "Chongshou Li"], "title": "ModelNet40-E: An Uncertainty-Aware Benchmark for Point Cloud Classification", "categories": ["cs.CV"], "comment": null, "summary": "We introduce ModelNet40-E, a new benchmark designed to assess the robustness\nand calibration of point cloud classification models under synthetic LiDAR-like\nnoise. Unlike existing benchmarks, ModelNet40-E provides both noise-corrupted\npoint clouds and point-wise uncertainty annotations via Gaussian noise\nparameters ({\\sigma}, {\\mu}), enabling fine-grained evaluation of uncertainty\nmodeling. We evaluate three popular models-PointNet, DGCNN, and Point\nTransformer v3-across multiple noise levels using classification accuracy,\ncalibration metrics, and uncertainty-awareness. While all models degrade under\nincreasing noise, Point Transformer v3 demonstrates superior calibration, with\npredicted uncertainties more closely aligned with the underlying measurement\nuncertainty.", "AI": {"tldr": "\u5f15\u5165ModelNet40-E\u57fa\u51c6\uff0c\u8bc4\u4f30\u70b9\u4e91\u5206\u7c7b\u6a21\u578b\u5728\u7c7bLiDAR\u566a\u58f0\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6821\u51c6\u6027\uff0c\u5e76\u53d1\u73b0Point Transformer v3\u5728\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u70b9\u4e91\u5206\u7c7b\u6a21\u578b\u5728\u5408\u6210LiDAR\u566a\u58f0\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6821\u51c6\u6027\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u70b9\u7ea7\u4e0d\u786e\u5b9a\u6027\u6807\u6ce8\u3002", "method": "\u5f15\u5165ModelNet40-E\uff0c\u63d0\u4f9b\u5e26\u566a\u58f0\u7684\u70b9\u4e91\u548c\u70b9\u7ea7\u4e0d\u786e\u5b9a\u6027\u6807\u6ce8\uff08\u9ad8\u65af\u566a\u58f0\u53c2\u6570\uff09\u3002\u8bc4\u4f30PointNet\u3001DGCNN\u548cPoint Transformer v3\u4e09\u79cd\u6a21\u578b\u5728\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u4e0b\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3001\u6821\u51c6\u5ea6\u53ca\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u80fd\u529b\u3002", "result": "\u6240\u6709\u6a21\u578b\u6027\u80fd\u5747\u968f\u566a\u58f0\u589e\u52a0\u800c\u4e0b\u964d\u3002Point Transformer v3\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6821\u51c6\u6027\uff0c\u5176\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u4e0e\u5e95\u5c42\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\u66f4\u4e3a\u4e00\u81f4\u3002", "conclusion": "ModelNet40-E\u4e3a\u8bc4\u4f30\u70b9\u4e91\u5206\u7c7b\u6a21\u578b\u5728\u566a\u58f0\u4e0b\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u4e14Point Transformer v3\u5728\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5177\u6709\u66f4\u597d\u7684\u6821\u51c6\u80fd\u529b\u3002"}}
{"id": "2508.01780", "pdf": "https://arxiv.org/pdf/2508.01780", "abs": "https://arxiv.org/abs/2508.01780", "authors": ["Guozhao Mo", "Wenliang Zhong", "Jiawei Chen", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Ben He", "Xianpei Han", "Le Sun"], "title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?", "categories": ["cs.AI", "cs.CL"], "comment": "Our code and data will be publicly available at\n  https://icip-cas.github.io/LiveMCPBench", "summary": "With the rapid development of Model Context Protocol (MCP), the number of MCP\nservers has surpassed 10,000. However, existing MCP benchmarks are limited to\nsingle-server settings with only a few tools, hindering effective evaluation of\nagent capabilities in large-scale, real-world scenarios. To address this\nlimitation, we present LiveMCPBench, the first comprehensive benchmark\ncomprising 95 real-world tasks grounded in the MCP ecosystem, designed to\nevaluate LLM agents at scale across diverse servers. To support a scalable and\nreproducible evaluation pipeline in large-scale MCP environments, we curate\nLiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and\n527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework\nthat enables automated and adaptive evaluation in dynamic, time-varying task\nenvironments, achieving 81% agreement with human reviewers. Finally, we propose\nthe MCP Copilot Agent, a multi-step agent that routes tools for dynamic\nplanning and executes tools for API interaction across the entire LiveMCPTool\nsuite. Our evaluation covers 10 leading models, with the best-performing model\n(Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large\nperformance variance across models, and several widely-used models perform\npoorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench\noffers the first unified framework for benchmarking LLM agents in realistic,\ntool-rich, and dynamic MCP environments, laying a solid foundation for scalable\nand reproducible research on agent capabilities. Our code and data will be\npublicly available at https://icip-cas.github.io/LiveMCPBench.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLiveMCPBench\uff0c\u9996\u4e2a\u7528\u4e8e\u5728\u5927\u89c4\u6a21\u3001\u771f\u5b9eModel Context Protocol (MCP) \u73af\u5883\u4e2d\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u4ee3\u7406\u7684\u7efc\u5408\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u5305\u542b\u771f\u5b9e\u4efb\u52a1\u3001\u4e30\u5bcc\u5de5\u5177\u96c6\u3001\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u53ca\u4e00\u4e2a\u65b0\u578b\u4ee3\u7406\u6a21\u578b\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709LLM\u4ee3\u7406\u5728\u6b64\u590d\u6742\u73af\u5883\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709MCP\u57fa\u51c6\u4ec5\u9650\u4e8e\u5355\u670d\u52a1\u5668\u548c\u5c11\u91cf\u5de5\u5177\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u5927\u89c4\u6a21\u3001\u771f\u5b9eMCP\u573a\u666f\u4e0b\u7684\u80fd\u529b\u3002", "method": "1. \u63d0\u51faLiveMCPBench\uff0c\u4e00\u4e2a\u5305\u542b95\u4e2a\u771f\u5b9e\u4efb\u52a1\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u8bc4\u4f30LLM\u4ee3\u7406\u30022. \u6784\u5efaLiveMCPTool\uff0c\u4e00\u4e2a\u5305\u542b70\u4e2aMCP\u670d\u52a1\u5668\u548c527\u4e2a\u5de5\u5177\u7684\u96c6\u5408\uff0c\u652f\u6301\u53ef\u6269\u5c55\u8bc4\u4f30\u30023. \u5f15\u5165LiveMCPEval\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM-as-a-Judge\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u4e0e\u4eba\u5de5\u8bc4\u4f30\u4e00\u81f4\u6027\u8fbe81%\u30024. \u63d0\u51faMCP Copilot Agent\uff0c\u4e00\u4e2a\u7528\u4e8e\u52a8\u6001\u89c4\u5212\u548cAPI\u4ea4\u4e92\u7684\u591a\u6b65\u4ee3\u7406\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u4e8610\u4e2a\u4e3b\u6d41\u6a21\u578b\uff0c\u5176\u4e2dClaude-Sonnet-4\u8868\u73b0\u6700\u4f73\uff0c\u6210\u529f\u7387\u4e3a78.95%\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u95f4\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u591a\u4e2a\u5e38\u7528\u6a21\u578b\u5728LiveMCPBench\u7684\u590d\u6742\u3001\u5de5\u5177\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "LiveMCPBench\u662f\u9996\u4e2a\u7528\u4e8e\u5728\u771f\u5b9e\u3001\u5de5\u5177\u4e30\u5bcc\u3001\u52a8\u6001MCP\u73af\u5883\u4e2d\u8bc4\u4f30LLM\u4ee3\u7406\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4e3aLLM\u4ee3\u7406\u80fd\u529b\u7684\u53ef\u6269\u5c55\u548c\u53ef\u590d\u73b0\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2508.02018", "pdf": "https://arxiv.org/pdf/2508.02018", "abs": "https://arxiv.org/abs/2508.02018", "authors": ["Wanqi Yang", "Yanda Li", "Yunchao Wei", "Meng Fang", "Ling Chen"], "title": "SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large audio-language models (LALMs) have achieved near-human performance in\nsentence-level transcription and emotion recognition. However, existing\nevaluations focus mainly on surface-level perception, leaving the capacity of\nmodels for contextual and inference-driven reasoning in speech-based scenarios\ninsufficiently examined. To address this gap, we introduce SpeechR, a unified\nbenchmark for evaluating reasoning over speech in large audio-language models.\nSpeechR evaluates models along three key dimensions: factual retrieval,\nprocedural inference, and normative judgment. It includes three distinct\nevaluation formats. The multiple-choice version measures answer selection\naccuracy. The generative version assesses the coherence and logical consistency\nof reasoning chains. The acoustic-feature version investigates whether\nvariations in stress and emotion affect reasoning performance. Evaluations on\neleven state-of-the-art LALMs reveal that high transcription accuracy does not\ntranslate into strong reasoning capabilities. SpeechR establishes a structured\nbenchmark for evaluating reasoning in spoken language, enabling more targeted\nanalysis of model capabilities across diverse dialogue-based tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSpeechR\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\uff08LALMs\uff09\u5728\u8bed\u97f3\u573a\u666f\u4e2d\u7684\u4e0a\u4e0b\u6587\u548c\u63a8\u7406\u9a71\u52a8\u7684\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u9ad8\u8f6c\u5f55\u51c6\u786e\u6027\u4e0d\u4ee3\u8868\u5f3a\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\uff08LALMs\uff09\u5728\u53e5\u5b50\u7ea7\u8f6c\u5f55\u548c\u60c5\u611f\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u8868\u5c42\u611f\u77e5\uff0c\u672a\u80fd\u5145\u5206\u68c0\u9a8c\u6a21\u578b\u5728\u8bed\u97f3\u573a\u666f\u4e2d\u8fdb\u884c\u4e0a\u4e0b\u6587\u548c\u63a8\u7406\u9a71\u52a8\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e86SpeechR\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u8bed\u97f3\u63a8\u7406\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u3002SpeechR\u4ece\u4e8b\u5b9e\u68c0\u7d22\u3001\u7a0b\u5e8f\u63a8\u7406\u548c\u89c4\u8303\u5224\u65ad\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5305\u542b\u591a\u9879\u9009\u62e9\u3001\u751f\u6210\u5f0f\u548c\u58f0\u5b66\u7279\u5f81\u4e09\u79cd\u4e0d\u540c\u7684\u8bc4\u4f30\u683c\u5f0f\u3002", "result": "\u5bf911\u4e2a\u6700\u5148\u8fdb\u7684LALMs\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u9ad8\u8f6c\u5f55\u51c6\u786e\u6027\u5e76\u4e0d\u80fd\u8f6c\u5316\u4e3a\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "SpeechR\u4e3a\u8bc4\u4f30\u53e3\u8bed\u63a8\u7406\u80fd\u529b\u5efa\u7acb\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u5bf9\u6a21\u578b\u5728\u5404\u79cd\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u8fdb\u884c\u66f4\u5177\u9488\u5bf9\u6027\u7684\u5206\u6790\u3002"}}
{"id": "2508.01195", "pdf": "https://arxiv.org/pdf/2508.01195", "abs": "https://arxiv.org/abs/2508.01195", "authors": ["Kun Li", "Zhennan Wu", "Yida Xiong", "Hongzhi Zhang", "Longtao Hu", "Zhonglie Liu", "Junqi Zeng", "Wenjie Wu", "Mukun Chen", "Jiameng Chen", "Wenbin Hu"], "title": "BSL: A Unified and Generalizable Multitask Learning Platform for Virtual Drug Discovery from Design to Synthesis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Drug discovery is of great social significance in safeguarding human health,\nprolonging life, and addressing the challenges of major diseases. In recent\nyears, artificial intelligence has demonstrated remarkable advantages in key\ntasks across bioinformatics and pharmacology, owing to its efficient data\nprocessing and data representation capabilities. However, most existing\ncomputational platforms cover only a subset of core tasks, leading to\nfragmented workflows and low efficiency. In addition, they often lack\nalgorithmic innovation and show poor generalization to out-of-distribution\n(OOD) data, which greatly hinders the progress of drug discovery. To address\nthese limitations, we propose Baishenglai (BSL), a deep learning-enhanced,\nopen-access platform designed for virtual drug discovery. BSL integrates seven\ncore tasks within a unified and modular framework, incorporating advanced\ntechnologies such as generative models and graph neural networks. In addition\nto achieving state-of-the-art (SOTA) performance on multiple benchmark\ndatasets, the platform emphasizes evaluation mechanisms that focus on\ngeneralization to OOD molecular structures. Comparative experiments with\nexisting platforms and baseline methods demonstrate that BSL provides a\ncomprehensive, scalable, and effective solution for virtual drug discovery,\noffering both algorithmic innovation and high-precision prediction for\nreal-world pharmaceutical research. In addition, BSL demonstrated its practical\nutility by discovering novel modulators of the GluN1/GluN3A NMDA receptor,\nsuccessfully identifying three compounds with clear bioactivity in in-vitro\nelectrophysiological assays. These results highlight BSL as a promising and\ncomprehensive platform for accelerating biomedical research and drug discovery.\nThe platform is accessible at https://www.baishenglai.net.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aBaishenglai (BSL)\u7684AI\u5e73\u53f0\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u865a\u62df\u836f\u7269\u53d1\u73b0\u5e73\u53f0\u788e\u7247\u5316\u3001\u6548\u7387\u4f4e\u548c\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\u3002BSL\u6574\u5408\u4e86\u4e03\u4e2a\u6838\u5fc3\u4efb\u52a1\uff0c\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u6210\u529f\u53d1\u73b0\u5177\u6709\u751f\u7269\u6d3b\u6027\u7684\u5316\u5408\u7269\u3002", "motivation": "\u73b0\u6709\u836f\u7269\u53d1\u73b0\u8ba1\u7b97\u5e73\u53f0\u529f\u80fd\u788e\u7247\u5316\u3001\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u7f3a\u4e4f\u7b97\u6cd5\u521b\u65b0\uff0c\u5bf9\u5206\u5e03\u5916(OOD)\u6570\u636e\u7684\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4e25\u91cd\u963b\u788d\u4e86\u836f\u7269\u53d1\u73b0\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51fa\u4e86Baishenglai (BSL)\uff0c\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u589e\u5f3a\u7684\u3001\u5f00\u653e\u8bbf\u95ee\u7684\u865a\u62df\u836f\u7269\u53d1\u73b0\u5e73\u53f0\u3002BSL\u5728\u4e00\u4e2a\u7edf\u4e00\u7684\u6a21\u5757\u5316\u6846\u67b6\u5185\u96c6\u6210\u4e86\u4e03\u4e2a\u6838\u5fc3\u4efb\u52a1\uff0c\u5e76\u878d\u5408\u4e86\u751f\u6210\u6a21\u578b\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u5148\u8fdb\u6280\u672f\u3002\u5176\u8bc4\u4f30\u673a\u5236\u7279\u522b\u5f3a\u8c03\u5bf9OOD\u5206\u5b50\u7ed3\u6784\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "BSL\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb(SOTA)\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u4e0e\u73b0\u6709\u5e73\u53f0\u548c\u57fa\u7ebf\u65b9\u6cd5\u7684\u6bd4\u8f83\uff0c\u8bc1\u660e\u5176\u4e3a\u865a\u62df\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u5168\u9762\u3001\u53ef\u6269\u5c55\u3001\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u7b97\u6cd5\u521b\u65b0\u548c\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3002BSL\u8fd8\u6210\u529f\u53d1\u73b0\u4e86\u4e09\u79cd\u5177\u6709\u660e\u786e\u751f\u7269\u6d3b\u6027\u7684GluN1/GluN3A NMDA\u53d7\u4f53\u65b0\u578b\u8c03\u8282\u5242\u3002", "conclusion": "Baishenglai (BSL)\u662f\u4e00\u4e2a\u6709\u524d\u666f\u4e14\u5168\u9762\u7684\u5e73\u53f0\uff0c\u80fd\u591f\u663e\u8457\u52a0\u901f\u751f\u7269\u533b\u5b66\u7814\u7a76\u548c\u836f\u7269\u53d1\u73b0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5e73\u53f0\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.01270", "pdf": "https://arxiv.org/pdf/2508.01270", "abs": "https://arxiv.org/abs/2508.01270", "authors": ["Zeyu Pan", "Ping Li", "Wenxiao Wang"], "title": "SGCap: Decoding Semantic Group for Zero-shot Video Captioning", "categories": ["cs.CV"], "comment": "11 pages, 9 figures, 11 tables", "summary": "Zero-shot video captioning aims to generate sentences for describing videos\nwithout training the model on video-text pairs, which remains underexplored.\nExisting zero-shot image captioning methods typically adopt a text-only\ntraining paradigm, where a language decoder reconstructs single-sentence\nembeddings obtained from CLIP. However, directly extending them to the video\ndomain is suboptimal, as applying average pooling over all frames neglects\ntemporal dynamics. To address this challenge, we propose a Semantic Group\nCaptioning (SGCap) method for zero-shot video captioning. In particular, it\ndevelops the Semantic Group Decoding (SGD) strategy to employ multi-frame\ninformation while explicitly modeling inter-frame temporal relationships.\nFurthermore, existing zero-shot captioning methods that rely on cosine\nsimilarity for sentence retrieval and reconstruct the description supervised by\na single frame-level caption, fail to provide sufficient video-level\nsupervision. To alleviate this, we introduce two key components, including the\nKey Sentences Selection (KSS) module and the Probability Sampling Supervision\n(PSS) module. The two modules construct semantically-diverse sentence groups\nthat models temporal dynamics and guide the model to capture inter-sentence\ncausal relationships, thereby enhancing its generalization ability to video\ncaptioning. Experimental results on several benchmarks demonstrate that SGCap\nsignificantly outperforms previous state-of-the-art zero-shot alternatives and\neven achieves performance competitive with fully supervised ones. Code is\navailable at https://github.com/mlvccn/SGCap_Video.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSGCap\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u7ec4\u89e3\u7801\u7b56\u7565\u548c\u5173\u952e\u53e5\u5b50\u9009\u62e9\u3001\u6982\u7387\u91c7\u6837\u76d1\u7763\u6a21\u5757\uff0c\u89e3\u51b3\u96f6\u6837\u672c\u89c6\u9891\u5b57\u5e55\u751f\u6210\u4e2d\u65f6\u5e8f\u52a8\u6001\u548c\u89c6\u9891\u7ea7\u76d1\u7763\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u96f6\u6837\u672c\u89c6\u9891\u5b57\u5e55\u751f\u6210\u9886\u57df\u7814\u7a76\u4e0d\u8db3\uff1b\u73b0\u6709\u96f6\u6837\u672c\u56fe\u50cf\u5b57\u5e55\u65b9\u6cd5\u76f4\u63a5\u6269\u5c55\u81f3\u89c6\u9891\u9886\u57df\u65f6\uff0c\u56e0\u5e73\u5747\u6c60\u5316\u5ffd\u7565\u65f6\u5e8f\u52a8\u6001\u800c\u6548\u679c\u4e0d\u4f73\uff1b\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8fdb\u884c\u53e5\u5b50\u68c0\u7d22\u5e76\u4ec5\u4f7f\u7528\u5355\u5e27\u5b57\u5e55\u8fdb\u884c\u76d1\u7763\uff0c\u5bfc\u81f4\u89c6\u9891\u7ea7\u76d1\u7763\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u8bed\u4e49\u7ec4\u5b57\u5e55\u751f\u6210\uff08SGCap\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5305\u542b\uff1a1) \u8bed\u4e49\u7ec4\u89e3\u7801\uff08SGD\uff09\u7b56\u7565\uff0c\u7528\u4e8e\u5229\u7528\u591a\u5e27\u4fe1\u606f\u5e76\u663e\u5f0f\u5efa\u6a21\u5e27\u95f4\u65f6\u5e8f\u5173\u7cfb\uff1b2) \u5173\u952e\u53e5\u5b50\u9009\u62e9\uff08KSS\uff09\u6a21\u5757\uff1b3) \u6982\u7387\u91c7\u6837\u76d1\u7763\uff08PSS\uff09\u6a21\u5757\u3002KSS\u548cPSS\u6a21\u5757\u534f\u540c\u6784\u5efa\u8bed\u4e49\u591a\u6837\u6027\u7684\u53e5\u5b50\u7ec4\uff0c\u6355\u6349\u65f6\u5e8f\u52a8\u6001\u5e76\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u53e5\u5b50\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u4ece\u800c\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSGCap\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u66ff\u4ee3\u65b9\u6848\uff0c\u751a\u81f3\u8fbe\u5230\u4e0e\u5168\u76d1\u7763\u65b9\u6cd5\u76f8\u7ade\u4e89\u7684\u6027\u80fd\u3002", "conclusion": "SGCap\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u89c6\u9891\u5b57\u5e55\u751f\u6210\u4e2d\u65f6\u5e8f\u52a8\u6001\u5904\u7406\u548c\u89c6\u9891\u7ea7\u76d1\u7763\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u8be5\u9886\u57df\u7684\u6027\u80fd\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2508.01844", "pdf": "https://arxiv.org/pdf/2508.01844", "abs": "https://arxiv.org/abs/2508.01844", "authors": ["Xinkai Zou", "Xuan Jiang", "Ruikai Huang", "Haoze He", "Parv Kapoor", "Jiahua Zhao"], "title": "CloudAnoAgent: Anomaly Detection for Cloud Sites via LLM Agent with Neuro-Symbolic Mechanism", "categories": ["cs.AI"], "comment": null, "summary": "Anomaly detection in cloud sites remains a critical yet challenging task.\nExisting approaches that rely solely on metric data often suffer from high\nfalse positive rates (FPR) due to data imbalance between normal and anomalous\nevents, leading to significant operational overhead for system reliance\nengineers. Recent advances in large language models (LLMs) offer new\nopportunities for integrating metrics with log data, enabling more accurate and\ninterpretable anomaly detection. In this paper, we propose CloudAnoAgent, the\nfirst neuro-symbolic LLM-based agent for anomaly detection in cloud\nenvironments. CloudAnoAgent jointly processes structured metrics and textual\nlog data in a unified pipeline, leveraging symbolic verification to validate\ndetection hypotheses and generate structured anomaly reports. To support\nsystematic evaluation, we introduce CloudAnoBench, the first benchmark that\nprovides LLM-generated paired metrics and log data with fine-grained anomaly\nbehavior annotations, filling a critical gap in existing datasets. Experimental\nresults demonstrate that CloudAnoAgent improves anomaly classification accuracy\nby 46.36% and 36.67% on average and reduces the FPR by 36.67% and 33.89% on\naverage over traditional baselines and LLM-only baseline, with a boost on\nanomaly type detection accuracy by 12.8% compared to vanilla LLM prompting.\nThese results demonstrate the strengths of our approach in improving detection\naccuracy, reducing false positives, and enhancing interpretability, thereby\nsupporting practical deployment in enterprise cloud environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCloudAnoAgent\uff0c\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u795e\u7ecf\u7b26\u53f7\u4ee3\u7406\uff0c\u7528\u4e8e\u4e91\u73af\u5883\u5f02\u5e38\u68c0\u6d4b\u3002\u5b83\u7ed3\u5408\u5ea6\u91cf\u548c\u65e5\u5fd7\u6570\u636e\uff0c\u901a\u8fc7\u7b26\u53f7\u9a8c\u8bc1\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u964d\u4f4e\u8bef\u62a5\u7387\uff0c\u5e76\u63d0\u4f9bCloudAnoBench\u57fa\u51c6\u3002", "motivation": "\u4e91\u7ad9\u70b9\u5f02\u5e38\u68c0\u6d4b\u662f\u4e00\u9879\u5173\u952e\u4f46\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5ea6\u91cf\u6570\u636e\uff0c\u5e38\u56e0\u6570\u636e\u4e0d\u5e73\u8861\u5bfc\u81f4\u9ad8\u8bef\u62a5\u7387\uff08FPR\uff09\uff0c\u7ed9\u7cfb\u7edf\u53ef\u9760\u6027\u5de5\u7a0b\u5e08\u5e26\u6765\u5de8\u5927\u7684\u8fd0\u7ef4\u8d1f\u62c5\u3002", "method": "\u672c\u6587\u63d0\u51faCloudAnoAgent\uff0c\u9996\u4e2a\u7528\u4e8e\u4e91\u73af\u5883\u5f02\u5e38\u68c0\u6d4b\u7684\u795e\u7ecf\u7b26\u53f7LLM\u4ee3\u7406\u3002\u5b83\u5728\u7edf\u4e00\u7ba1\u9053\u4e2d\u8054\u5408\u5904\u7406\u7ed3\u6784\u5316\u5ea6\u91cf\u548c\u6587\u672c\u65e5\u5fd7\u6570\u636e\uff0c\u5e76\u5229\u7528\u7b26\u53f7\u9a8c\u8bc1\u6765\u9a8c\u8bc1\u68c0\u6d4b\u5047\u8bbe\uff0c\u751f\u6210\u7ed3\u6784\u5316\u7684\u5f02\u5e38\u62a5\u544a\u3002\u4e3a\u652f\u6301\u7cfb\u7edf\u8bc4\u4f30\uff0c\u7814\u7a76\u8fd8\u5f15\u5165\u4e86CloudAnoBench\uff0c\u9996\u4e2a\u63d0\u4f9bLLM\u751f\u6210\u7684\u914d\u5bf9\u5ea6\u91cf\u548c\u65e5\u5fd7\u6570\u636e\u53ca\u7ec6\u7c92\u5ea6\u5f02\u5e38\u884c\u4e3a\u6ce8\u91ca\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCloudAnoAgent\u76f8\u8f83\u4e8e\u4f20\u7edf\u57fa\u7ebf\u548c\u4ec5\u4f7f\u7528LLM\u7684\u57fa\u7ebf\uff0c\u5e73\u5747\u5c06\u5f02\u5e38\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u534746.36%\u548c36.67%\uff0c\u8bef\u62a5\u7387\u964d\u4f4e36.67%\u548c33.89%\u3002\u6b64\u5916\uff0c\u5f02\u5e38\u7c7b\u578b\u68c0\u6d4b\u51c6\u786e\u7387\u6bd4\u666e\u901aLLM\u63d0\u793a\u63d0\u9ad812.8%\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u3001\u964d\u4f4e\u8bef\u62a5\u7387\u548c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4f01\u4e1a\u4e91\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2508.02037", "pdf": "https://arxiv.org/pdf/2508.02037", "abs": "https://arxiv.org/abs/2508.02037", "authors": ["Huihan Li", "You Chen", "Siyuan Wang", "Yixin He", "Ninareh Mehrabi", "Rahul Gupta", "Xiang Ren"], "title": "Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) perform well on reasoning benchmarks but often\nfail when inputs alter slightly, raising concerns about the extent to which\ntheir success relies on memorization. This issue is especially acute in\nChain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger\nintermediate errors that cascade into incorrect final answers. We introduce\nSTIM, a novel framework for Source-aware Token-level Identification of\nMemorization, which attributes each token in a reasoning chain to one of\nmultiple memorization sources - local, mid-range, or long-range - based on\ntheir statistical co-occurrence with the token in the pretraining corpus. Our\ntoken-level analysis across tasks and distributional settings reveals that\nmodels rely more on memorization in complex or long-tail cases, and that local\nmemorization is often the dominant driver of errors, leading to up to 67% of\nwrong tokens. We also show that memorization scores from STIM can be effective\nin predicting the wrong tokens in the wrong reasoning step. STIM offers a\npowerful tool for diagnosing and improving model reasoning and can generalize\nto other structured step-wise generation tasks.", "AI": {"tldr": "STIM\u662f\u4e00\u4e2a\u8bc6\u522b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u4e2d\u8bb0\u5fc6\u5316\u6a21\u5f0f\u7684\u6846\u67b6\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5728\u590d\u6742\u6848\u4f8b\u4e2d\u66f4\u4f9d\u8d56\u8bb0\u5fc6\u5316\uff0c\u4e14\u5c40\u90e8\u8bb0\u5fc6\u5316\u662f\u4e3b\u8981\u9519\u8bef\u6765\u6e90\uff08\u9ad8\u8fbe67%\u7684\u9519\u8bef\u8bcd\u5143\uff09\uff0cSTIM\u80fd\u6709\u6548\u9884\u6d4b\u9519\u8bef\uff0c\u6709\u52a9\u4e8e\u8bca\u65ad\u548c\u6539\u8fdbLLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5f53\u8f93\u5165\u7565\u6709\u6539\u53d8\u65f6\u5374\u5bb9\u6613\u51fa\u9519\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u5176\u6210\u529f\u662f\u5426\u8fc7\u5ea6\u4f9d\u8d56\u8bb0\u5fc6\u5316\u7684\u62c5\u5fe7\u3002\u5728\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u4e2d\uff0c\u865a\u5047\u7684\u8bb0\u5fc6\u5316\u6a21\u5f0f\u5c24\u5176\u5bb9\u6613\u89e6\u53d1\u4e2d\u95f4\u9519\u8bef\u5e76\u6700\u7ec8\u5bfc\u81f4\u7b54\u6848\u9519\u8bef\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86STIM\uff08Source-aware Token-level Identification of Memorization\uff09\u6846\u67b6\u3002STIM\u901a\u8fc7\u5206\u6790\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u8bcd\u5143\uff08token\uff09\u7684\u7edf\u8ba1\u5171\u73b0\u6027\uff0c\u5c06\u63a8\u7406\u94fe\u4e2d\u7684\u6bcf\u4e2a\u8bcd\u5143\u5f52\u56e0\u4e8e\u4e09\u79cd\u8bb0\u5fc6\u5316\u6765\u6e90\uff1a\u5c40\u90e8\u3001\u4e2d\u7a0b\u6216\u957f\u7a0b\u3002", "result": "\u901a\u8fc7\u5bf9\u4e0d\u540c\u4efb\u52a1\u548c\u5206\u5e03\u8bbe\u7f6e\u7684\u8bcd\u5143\u7ea7\u5206\u6790\uff0c\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u6216\u957f\u5c3e\u6848\u4f8b\u65f6\u66f4\u4f9d\u8d56\u8bb0\u5fc6\u5316\u3002\u5c40\u90e8\u8bb0\u5fc6\u5316\u662f\u9519\u8bef\u7684\u4e3b\u8981\u9a71\u52a8\u56e0\u7d20\uff0c\u5bfc\u81f4\u9ad8\u8fbe67%\u7684\u9519\u8bef\u8bcd\u5143\u3002STIM\u7684\u8bb0\u5fc6\u5316\u5f97\u5206\u80fd\u6709\u6548\u9884\u6d4b\u63a8\u7406\u6b65\u9aa4\u4e2d\u7684\u9519\u8bef\u8bcd\u5143\u3002", "conclusion": "STIM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u8bca\u65ad\u548c\u6539\u8fdb\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e14\u53ef\u4ee5\u63a8\u5e7f\u5e94\u7528\u4e8e\u5176\u4ed6\u7ed3\u6784\u5316\u7684\u3001\u9010\u6b65\u751f\u6210\u7684\u4efb\u52a1\u3002"}}
{"id": "2508.01209", "pdf": "https://arxiv.org/pdf/2508.01209", "abs": "https://arxiv.org/abs/2508.01209", "authors": ["Sukwon Yun", "Xin Liu", "Yunhak Oh", "Junseok Lee", "Tianlong Chen", "Tsuyoshi Murata", "Chanyoung Park"], "title": "Oldie but Goodie: Re-illuminating Label Propagation on Graphs with Partially Observed Features", "categories": ["cs.LG", "cs.AI"], "comment": "KDD 2025", "summary": "In real-world graphs, we often encounter missing feature situations where a\nfew or the majority of node features, e.g., sensitive information, are missed.\nIn such scenarios, directly utilizing Graph Neural Networks (GNNs) would yield\nsub-optimal results in downstream tasks such as node classification. Despite\nthe emergence of a few GNN-based methods attempting to mitigate its missing\nsituation, when only a few features are available, they rather perform worse\nthan traditional structure-based models. To this end, we propose a novel\nframework that further illuminates the potential of classical Label Propagation\n(Oldie), taking advantage of Feature Propagation, especially when only a\npartial feature is available. Now called by GOODIE, it takes a hybrid approach\nto obtain embeddings from the Label Propagation branch and Feature Propagation\nbranch. To do so, we first design a GNN-based decoder that enables the Label\nPropagation branch to output hidden embeddings that align with those of the FP\nbranch. Then, GOODIE automatically captures the significance of structure and\nfeature information thanks to the newly designed Structure-Feature Attention.\nFollowed by a novel Pseudo-Label contrastive learning that differentiates the\ncontribution of each positive pair within pseudo-labels originating from the LP\nbranch, GOODIE outputs the final prediction for the unlabeled nodes. Through\nextensive experiments, we demonstrate that our proposed model, GOODIE,\noutperforms the existing state-of-the-art methods not only when only a few\nfeatures are available but also in abundantly available situations. Source code\nof GOODIE is available at: https://github.com/SukwonYun/GOODIE.", "AI": {"tldr": "\u9488\u5bf9\u56fe\u6570\u636e\u4e2d\u8282\u70b9\u7279\u5f81\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u51fa\u4e86GOODIE\u6a21\u578b\u3002\u8be5\u6a21\u578b\u7ed3\u5408\u6807\u7b7e\u4f20\u64ad\u548c\u7279\u5f81\u4f20\u64ad\uff0c\u5e76\u901a\u8fc7GNN\u89e3\u7801\u5668\u3001\u7ed3\u6784-\u7279\u5f81\u6ce8\u610f\u529b\u53ca\u4f2a\u6807\u7b7e\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728\u7279\u5f81\u7a00\u7f3a\u548c\u5145\u8db3\u573a\u666f\u4e0b\u5747\u8d85\u8d8a\u73b0\u6709\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u56fe\u6570\u636e\u4e2d\u5e38\u5b58\u5728\u8282\u70b9\u7279\u5f81\u7f3a\u5931\uff08\u5982\u654f\u611f\u4fe1\u606f\u7f3a\u5931\uff09\u7684\u60c5\u51b5\uff0c\u5bfc\u81f4\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u57fa\u4e8eGNN\u7684\u7f3a\u5931\u7279\u5f81\u5904\u7406\u65b9\u6cd5\u5728\u53ea\u6709\u5c11\u91cf\u7279\u5f81\u53ef\u7528\u65f6\uff0c\u751a\u81f3\u4e0d\u5982\u4f20\u7edf\u7684\u57fa\u4e8e\u7ed3\u6784\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aGOODIE\u7684\u65b0\u9896\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u7ecf\u5178\u6807\u7b7e\u4f20\u64ad\uff08LP\uff09\u548c\u7279\u5f81\u4f20\u64ad\uff08FP\uff09\u7684\u6df7\u5408\u65b9\u6cd5\u3002\u5177\u4f53\u5305\u62ec\uff1a1) \u8bbe\u8ba1\u4e00\u4e2aGNN\u89e3\u7801\u5668\uff0c\u4f7fLP\u5206\u652f\u7684\u9690\u85cf\u5d4c\u5165\u4e0eFP\u5206\u652f\u5bf9\u9f50\uff1b2) \u5f15\u5165\u7ed3\u6784-\u7279\u5f81\u6ce8\u610f\u529b\u673a\u5236\uff0c\u81ea\u52a8\u6355\u83b7\u7ed3\u6784\u548c\u7279\u5f81\u4fe1\u606f\u7684\u91cd\u8981\u6027\uff1b3) \u4f7f\u7528\u4f2a\u6807\u7b7e\u5bf9\u6bd4\u5b66\u4e60\uff0c\u533a\u5206LP\u5206\u652f\u4ea7\u751f\u7684\u4f2a\u6807\u7b7e\u4e2d\u6bcf\u4e2a\u6b63\u5bf9\u7684\u8d21\u732e\uff0c\u4ece\u800c\u9884\u6d4b\u672a\u6807\u8bb0\u8282\u70b9\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cGOODIE\u6a21\u578b\u5728\u53ea\u6709\u5c11\u91cf\u7279\u5f81\u53ef\u7528\u65f6\uff0c\u4ee5\u53ca\u5728\u7279\u5f81\u5145\u8db3\u7684\u60c5\u51b5\u4e0b\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "GOODIE\u6a21\u578b\u901a\u8fc7\u5176\u6df7\u5408\u65b9\u6cd5\u548c\u521b\u65b0\u7ec4\u4ef6\uff08GNN\u89e3\u7801\u5668\u3001\u7ed3\u6784-\u7279\u5f81\u6ce8\u610f\u529b\u3001\u4f2a\u6807\u7b7e\u5bf9\u6bd4\u5b66\u4e60\uff09\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u56fe\u6570\u636e\u4e2d\u8282\u70b9\u7279\u5f81\u7f3a\u5931\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u5728\u5404\u79cd\u7279\u5f81\u53ef\u7528\u6027\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2508.01272", "pdf": "https://arxiv.org/pdf/2508.01272", "abs": "https://arxiv.org/abs/2508.01272", "authors": ["Zonglei Jing", "Xiao Yang", "Xiaoqian Li", "Siyuan Liang", "Aishan Liu", "Mingchuan Zhang", "Xianglong Liu"], "title": "PromptSafe: Gated Prompt Tuning for Safe Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) models have demonstrated remarkable generative\ncapabilities but remain vulnerable to producing not-safe-for-work (NSFW)\ncontent, such as violent or explicit imagery. While recent moderation efforts\nhave introduced soft prompt-guided tuning by appending defensive tokens to the\ninput, these approaches often rely on large-scale curated image-text datasets\nand apply static, one-size-fits-all defenses at inference time. However, this\nresults not only in high computational cost and degraded benign image quality,\nbut also in limited adaptability to the diverse and nuanced safety requirements\nof real-world prompts. To address these challenges, we propose PromptSafe, a\ngated prompt tuning framework that combines a lightweight, text-only supervised\nsoft embedding with an inference-time gated control network. Instead of\ntraining on expensive image-text datasets, we first rewrite unsafe prompts into\nsemantically aligned but safe alternatives using an LLM, constructing an\nefficient text-only training corpus. Based on this, we optimize a universal\nsoft prompt that repels unsafe and attracts safe embeddings during the\ndiffusion denoising process. To avoid over-suppressing benign prompts, we\nintroduce a gated mechanism that adaptively adjusts the defensive strength\nbased on estimated prompt toxicity, thereby aligning defense intensity with\nprompt risk and ensuring strong protection for harmful inputs while preserving\nbenign generation quality. Extensive experiments across multiple benchmarks and\nT2I models show that PromptSafe achieves a SOTA unsafe generation rate (2.36%),\nwhile preserving high benign fidelity. Furthermore, PromptSafe demonstrates\nstrong generalization to unseen harmful categories, robust transferability\nacross diffusion model architectures, and resilience under adaptive adversarial\nattacks, highlighting its practical value for safe and scalable deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPromptSafe\uff0c\u4e00\u4e2a\u95e8\u63a7\u63d0\u793a\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u8f7b\u91cf\u7ea7\u7eaf\u6587\u672c\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u81ea\u9002\u5e94\u95e8\u63a7\u673a\u5236\uff0c\u6709\u6548\u6291\u5236\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u751f\u6210\u4e0d\u5b89\u5168\uff08NSFW\uff09\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u6027\u56fe\u50cf\u8d28\u91cf\u548c\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u9762\u4e34\u751f\u6210\u4e0d\u5b89\u5168\uff08NSFW\uff09\u5185\u5bb9\u7684\u98ce\u9669\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\uff08\u5982\u8f6f\u63d0\u793a\u5fae\u8c03\uff09\u5b58\u5728\u4f9d\u8d56\u5927\u89c4\u6a21\u56fe\u50cf-\u6587\u672c\u6570\u636e\u96c6\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u53ef\u80fd\u964d\u4f4e\u826f\u6027\u56fe\u50cf\u8d28\u91cf\u4ee5\u53ca\u5bf9\u591a\u6837\u5316\u63d0\u793a\u8bcd\u9002\u5e94\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63d0\u51faPromptSafe\uff0c\u4e00\u4e2a\u95e8\u63a7\u63d0\u793a\u5fae\u8c03\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u8f7b\u91cf\u7ea7\u3001\u7eaf\u6587\u672c\u76d1\u7763\u7684\u8f6f\u5d4c\u5165\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95e8\u63a7\u63a7\u5236\u7f51\u7edc\uff1a\n1.  **\u8bad\u7ec3**: \u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5c06\u4e0d\u5b89\u5168\u63d0\u793a\u91cd\u5199\u4e3a\u8bed\u4e49\u5b89\u5168\u66ff\u4ee3\u65b9\u6848\uff0c\u6784\u5efa\u9ad8\u6548\u7684\u7eaf\u6587\u672c\u8bad\u7ec3\u8bed\u6599\uff0c\u4ece\u800c\u4f18\u5316\u4e00\u4e2a\u901a\u7528\u8f6f\u63d0\u793a\uff0c\u4f7f\u5176\u5728\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\u4e2d\u6392\u65a5\u4e0d\u5b89\u5168\u5d4c\u5165\u5e76\u5438\u5f15\u5b89\u5168\u5d4c\u5165\u3002\n2.  **\u63a8\u7406**: \u5f15\u5165\u95e8\u63a7\u673a\u5236\uff0c\u6839\u636e\u9884\u4f30\u7684\u63d0\u793a\u8bcd\u6bd2\u6027\u81ea\u9002\u5e94\u8c03\u6574\u9632\u5fa1\u5f3a\u5ea6\uff0c\u786e\u4fdd\u5bf9\u6709\u5bb3\u8f93\u5165\u7684\u5f3a\u4fdd\u62a4\uff0c\u540c\u65f6\u907f\u514d\u8fc7\u5ea6\u6291\u5236\u826f\u6027\u63d0\u793a\u8bcd\uff0c\u4ece\u800c\u4fdd\u7559\u826f\u6027\u751f\u6210\u8d28\u91cf\u3002", "result": "PromptSafe\u5728\u591a\u9879\u57fa\u51c6\u548cT2I\u6a21\u578b\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u4e0d\u5b89\u5168\u5185\u5bb9\u751f\u6210\u7387\uff082.36%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u826f\u6027\u5185\u5bb9\u4fdd\u771f\u5ea6\u3002\u6b64\u5916\uff0c\u5b83\u5bf9\u672a\u89c1\u7684\u6709\u5bb3\u7c7b\u522b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u4e0d\u540c\u6269\u6563\u6a21\u578b\u67b6\u6784\u95f4\u5177\u6709\u9c81\u68d2\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u5e76\u5728\u81ea\u9002\u5e94\u5bf9\u6297\u653b\u51fb\u4e0b\u5c55\u73b0\u51fa\u97e7\u6027\u3002", "conclusion": "PromptSafe\u4e3aT2I\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u5b89\u5168\u90e8\u7f72\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5176\u521b\u65b0\u7684\u8f7b\u91cf\u7ea7\u8bad\u7ec3\u548c\u81ea\u9002\u5e94\u9632\u5fa1\u673a\u5236\uff0c\u6709\u6548\u6291\u5236\u4e86\u4e0d\u5b89\u5168\u5185\u5bb9\u751f\u6210\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u9ad8\u751f\u6210\u8d28\u91cf\u548c\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2508.01869", "pdf": "https://arxiv.org/pdf/2508.01869", "abs": "https://arxiv.org/abs/2508.01869", "authors": ["Yuanyuan Liang", "Xiaoman Wang", "Tingyu Xie", "Lei Pan"], "title": "ProKG-Dial: Progressive Multi-Turn Dialogue Construction with Domain Knowledge Graphs", "categories": ["cs.AI"], "comment": "15 pages", "summary": "Current large language models (LLMs) excel at general NLP tasks but often\nlack domain specific precision in professional settings. Building a high\nquality domain specific multi turn dialogue dataset is essential for developing\nspecialized conversational systems. However, existing methods such as manual\nannotation, simulated human LLM interactions, and role based LLM dialogues are\nresource intensive or suffer from limitations in dialogue quality and domain\ncoverage. To address these challenges, we introduce ProKG Dial, a progressive\nframework for constructing knowledge intensive multi turn dialogue datasets\nusing domain specific knowledge graphs (KGs). ProKG Dial leverages the\nstructured nature of KGs to encode complex domain knowledge and relationships,\nproviding a solid foundation for generating meaningful and coherent dialogues.\nSpecifically, ProKG Dial begins by applying community detection to partition\nthe KG into semantically cohesive subgraphs. For each subgraph, the framework\nincrementally generates a series of questions and answers centered around a\ntarget entity, ensuring relevance and coverage. A rigorous filtering step is\nemployed to maintain high dialogue quality. We validate ProKG Dial on a medical\nknowledge graph by evaluating the generated dialogues in terms of diversity,\nsemantic coherence, and entity coverage. Furthermore, we fine tune a base LLM\non the resulting dataset and benchmark it against several baselines. Both\nautomatic metrics and human evaluations demonstrate that ProKG Dial\nsubstantially improves dialogue quality and domain specific performance,\nhighlighting its effectiveness and practical utility.", "AI": {"tldr": "\u63d0\u51faProKG Dial\u6846\u67b6\uff0c\u5229\u7528\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u81ea\u52a8\u5316\u6784\u5efa\u9ad8\u8d28\u91cf\u3001\u77e5\u8bc6\u5bc6\u96c6\u7684\u9886\u57df\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u6709\u6548\u63d0\u5347\u5927\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u7684\u5bf9\u8bdd\u8d28\u91cf\u548c\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u7f3a\u4e4f\u7cbe\u786e\u6027\uff0c\u800c\u5f00\u53d1\u9ad8\u8d28\u91cf\u9886\u57df\u7279\u5b9a\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\u7684\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u4eba\u5de5\u6807\u6ce8\uff09\u8d44\u6e90\u6d88\u8017\u5927\u6216\u5b58\u5728\u8d28\u91cf\u3001\u8986\u76d6\u5ea6\u9650\u5236\u3002", "method": "\u5f15\u5165ProKG Dial\u6846\u67b6\uff0c\u5229\u7528\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u6784\u5efa\u6570\u636e\u96c6\u3002\u8be5\u6846\u67b6\u9996\u5148\u901a\u8fc7\u793e\u533a\u68c0\u6d4b\u5c06KG\u5212\u5206\u4e3a\u8bed\u4e49\u5b50\u56fe\uff0c\u7136\u540e\u4e3a\u6bcf\u4e2a\u5b50\u56fe\u56f4\u7ed5\u76ee\u6807\u5b9e\u4f53\u9010\u6b65\u751f\u6210\u95ee\u7b54\u5bf9\uff0c\u5e76\u901a\u8fc7\u4e25\u683c\u8fc7\u6ee4\u786e\u4fdd\u5bf9\u8bdd\u8d28\u91cf\u3002\u7814\u7a76\u5728\u533b\u5b66KG\u4e0a\u9a8c\u8bc1\u4e86\u751f\u6210\u5bf9\u8bdd\u7684\u591a\u6837\u6027\u3001\u8bed\u4e49\u8fde\u8d2f\u6027\u548c\u5b9e\u4f53\u8986\u76d6\u7387\uff0c\u5e76\u4f7f\u7528\u751f\u6210\u6570\u636e\u96c6\u5fae\u8c03LLM\u4e0e\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u5747\u8868\u660e\uff0cProKG Dial\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u8d28\u91cf\u548c\u9886\u57df\u7279\u5b9a\u6027\u80fd\u3002", "conclusion": "ProKG Dial\u6846\u67b6\u5728\u6784\u5efa\u9886\u57df\u77e5\u8bc6\u5bc6\u96c6\u578b\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.02038", "pdf": "https://arxiv.org/pdf/2508.02038", "abs": "https://arxiv.org/abs/2508.02038", "authors": ["Fengping Tian", "Chenyang Lyu", "Xuanfan Ni", "Haoqin Sun", "Qingjuan Li", "Zhiqiang Qian", "Haijun Li", "Longyue Wang", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "title": "Marco-Voice Technical Report", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Technical Report", "summary": "This paper presents a multifunctional speech synthesis system that integrates\nvoice cloning and emotion control speech synthesis within a unified framework.\nThe goal of this work is to address longstanding challenges in achieving highly\nexpressive, controllable, and natural speech generation that faithfully\npreserves speaker identity across diverse linguistic and emotional contexts.\nOur approach introduces an effective speaker-emotion disentanglement mechanism\nwith in-batch contrastive learning, enabling independent manipulation of\nspeaker identity and eemotional style, as well as rotational emotional\nembedding integration method for smooth emotion control. To support\ncomprehensive training and evaluation, we construct CSEMOTIONS, a high-quality\nemotional speech dataset containing 10 hours of Mandarin speech from six\nprofessional speakers across seven emotional categories. Extensive experiments\ndemonstrate that our system, Marco-Voice, achieves substantial improvements in\nboth objective and subjective metrics. Comprehensive evaluations and analysis\nwere conducted, results show that MarcoVoice delivers competitive performance\nin terms of speech clarity and emotional richness, representing a substantial\nadvance in the field of expressive neural speech synthesis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8bed\u97f3\u5408\u6210\u7cfb\u7edfMarco-Voice\uff0c\u96c6\u6210\u4e86\u58f0\u97f3\u514b\u9686\u548c\u60c5\u611f\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8868\u73b0\u529b\u3001\u53ef\u63a7\u4e14\u81ea\u7136\u7684\u8bed\u97f3\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u751f\u6210\u9ad8\u5ea6\u8868\u73b0\u529b\u3001\u53ef\u63a7\u4e14\u81ea\u7136\uff0c\u540c\u65f6\u5fe0\u5b9e\u4fdd\u7559\u8bf4\u8bdd\u8005\u8eab\u4efd\u7684\u8bed\u97f3\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "\u8be5\u7cfb\u7edf\u5f15\u5165\u4e86\u6709\u6548\u7684\u8bf4\u8bdd\u8005-\u60c5\u611f\u89e3\u8026\u673a\u5236\uff0c\u5229\u7528\u6279\u5185\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u8bf4\u8bdd\u8005\u8eab\u4efd\u548c\u60c5\u611f\u98ce\u683c\u7684\u72ec\u7acb\u64cd\u63a7\uff0c\u5e76\u91c7\u7528\u65cb\u8f6c\u60c5\u611f\u5d4c\u5165\u96c6\u6210\u65b9\u6cd5\u8fdb\u884c\u5e73\u6ed1\u60c5\u611f\u63a7\u5236\u3002\u6b64\u5916\uff0c\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u7684CSEMOTIONS\u60c5\u611f\u8bed\u97f3\u6570\u636e\u96c6\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMarco-Voice\u7cfb\u7edf\u5728\u5ba2\u89c2\u548c\u4e3b\u89c2\u6307\u6807\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cMarco-Voice\u5728\u8bed\u97f3\u6e05\u6670\u5ea6\u548c\u60c5\u611f\u4e30\u5bcc\u6027\u65b9\u9762\u8868\u73b0\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "Marco-Voice\u7cfb\u7edf\u4ee3\u8868\u4e86\u5bcc\u6709\u8868\u73b0\u529b\u7684\u795e\u7ecf\u8bed\u97f3\u5408\u6210\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u5927\u8fdb\u5c55\u3002"}}
{"id": "2508.01211", "pdf": "https://arxiv.org/pdf/2508.01211", "abs": "https://arxiv.org/abs/2508.01211", "authors": ["Yile Li", "Shandian Zhe"], "title": "Multi-Operator Few-Shot Learning for Generalization Across PDE Families", "categories": ["cs.LG"], "comment": null, "summary": "Learning solution operators for partial differential equations (PDEs) has\nbecome a foundational task in scientific machine learning. However, existing\nneural operator methods require abundant training data for each specific PDE\nand lack the ability to generalize across PDE families. In this work, we\npropose MOFS: a unified multimodal framework for multi-operator few-shot\nlearning, which aims to generalize to unseen PDE operators using only a few\ndemonstration examples. Our method integrates three key components: (i)\nmulti-task self-supervised pretraining of a shared Fourier Neural Operator\n(FNO) encoder to reconstruct masked spatial fields and predict frequency\nspectra, (ii) text-conditioned operator embeddings derived from statistical\nsummaries of input-output fields, and (iii) memory-augmented multimodal\nprompting with gated fusion and cross-modal gradient-based attention. We adopt\na two-stage training paradigm that first learns prompt-conditioned inference on\nseen operators and then applies end-to-end contrastive fine-tuning to align\nlatent representations across vision, frequency, and text modalities.\nExperiments on PDE benchmarks, including Darcy Flow and Navier Stokes variants,\ndemonstrate that our model outperforms existing operator learning baselines in\nfew-shot generalization. Extensive ablations validate the contributions of each\nmodality and training component. Our approach offers a new foundation for\nuniversal and data-efficient operator learning across scientific domains.", "AI": {"tldr": "MOFS\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u591a\u7b97\u5b50\u5c11\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709PDE\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u6570\u636e\u91cf\u5927\u4e14\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u5373\u53ef\u63a8\u5e7f\u5230\u672a\u89c1\u7b97\u5b50\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7b97\u5b50\u65b9\u6cd5\u5bf9\u6bcf\u4e2a\u7279\u5b9aPDE\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u4e14\u7f3a\u4e4f\u8de8PDE\u5bb6\u65cf\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u5728\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51faMOFS\u6846\u67b6\uff0c\u6574\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(i) \u5171\u4eab\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\uff08FNO\uff09\u7f16\u7801\u5668\u7684\u591a\u4efb\u52a1\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u7528\u4e8e\u91cd\u5efa\u63a9\u853d\u7a7a\u95f4\u573a\u548c\u9884\u6d4b\u9891\u7387\u8c31\uff1b(ii) \u57fa\u4e8e\u8f93\u5165\u8f93\u51fa\u573a\u7edf\u8ba1\u6458\u8981\u7684\u6587\u672c\u6761\u4ef6\u7b97\u5b50\u5d4c\u5165\uff1b(iii) \u7ed3\u5408\u95e8\u63a7\u878d\u5408\u548c\u8de8\u6a21\u6001\u68af\u5ea6\u6ce8\u610f\u7684\u8bb0\u5fc6\u589e\u5f3a\u591a\u6a21\u6001\u63d0\u793a\u3002\u8bad\u7ec3\u91c7\u7528\u4e24\u9636\u6bb5\u8303\u5f0f\uff1a\u9996\u5148\u5728\u5df2\u77e5\u7b97\u5b50\u4e0a\u8fdb\u884c\u63d0\u793a\u6761\u4ef6\u63a8\u7406\u5b66\u4e60\uff0c\u7136\u540e\u8fdb\u884c\u7aef\u5230\u7aef\u5bf9\u6bd4\u5fae\u8c03\u4ee5\u5bf9\u9f50\u89c6\u89c9\u3001\u9891\u7387\u548c\u6587\u672c\u6a21\u6001\u7684\u6f5c\u5728\u8868\u793a\u3002", "result": "\u5728Darcy Flow\u548cNavier Stokes\u7b49PDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMOFS\u5728\u5c11\u6837\u672c\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7b97\u5b50\u5b66\u4e60\u57fa\u7ebf\u6a21\u578b\u3002\u5e7f\u6cdb\u7684\u6d88\u878d\u5b9e\u9a8c\u4e5f\u9a8c\u8bc1\u4e86\u5404\u6a21\u6001\u548c\u8bad\u7ec3\u7ec4\u4ef6\u7684\u6709\u6548\u8d21\u732e\u3002", "conclusion": "\u672c\u65b9\u6cd5\u4e3a\u8de8\u79d1\u5b66\u9886\u57df\u7684\u901a\u7528\u4e14\u6570\u636e\u9ad8\u6548\u7684\u7b97\u5b50\u5b66\u4e60\u5960\u5b9a\u4e86\u65b0\u57fa\u7840\u3002"}}
{"id": "2508.01275", "pdf": "https://arxiv.org/pdf/2508.01275", "abs": "https://arxiv.org/abs/2508.01275", "authors": ["Chuang-Wei Liu", "Mingjian Sun", "Cairong Zhao", "Hanli Wang", "Alexander Dvorkovich", "Rui Fan"], "title": "Integrating Disparity Confidence Estimation into Relative Depth Prior-Guided Unsupervised Stereo Matching", "categories": ["cs.CV"], "comment": "13 pages", "summary": "Unsupervised stereo matching has garnered significant attention for its\nindependence from costly disparity annotations. Typical unsupervised methods\nrely on the multi-view consistency assumption for training networks, which\nsuffer considerably from stereo matching ambiguities, such as repetitive\npatterns and texture-less regions. A feasible solution lies in transferring 3D\ngeometric knowledge from a relative depth map to the stereo matching networks.\nHowever, existing knowledge transfer methods learn depth ranking information\nfrom randomly built sparse correspondences, which makes inefficient utilization\nof 3D geometric knowledge and introduces noise from mistaken disparity\nestimates. This work proposes a novel unsupervised learning framework to\naddress these challenges, which comprises a plug-and-play disparity confidence\nestimation algorithm and two depth prior-guided loss functions. Specifically,\nthe local coherence consistency between neighboring disparities and their\ncorresponding relative depths is first checked to obtain disparity confidence.\nAfterwards, quasi-dense correspondences are built using only confident\ndisparity estimates to facilitate efficient depth ranking learning. Finally, a\ndual disparity smoothness loss is proposed to boost stereo matching performance\nat disparity discontinuities. Experimental results demonstrate that our method\nachieves state-of-the-art stereo matching accuracy on the KITTI Stereo\nbenchmarks among all unsupervised stereo matching methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u7acb\u4f53\u5339\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u89c6\u5dee\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u548c\u6df1\u5ea6\u5148\u9a8c\u5f15\u5bfc\u7684\u635f\u5931\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u6b67\u4e49\u533a\u57df\u7684\u6311\u6218\uff0c\u5e76\u5728KITTI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u65e0\u76d1\u7763\u7acb\u4f53\u5339\u914d\u65b9\u6cd5\u4f9d\u8d56\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u5bb9\u6613\u5728\u91cd\u590d\u7eb9\u7406\u548c\u65e0\u7eb9\u7406\u533a\u57df\u4ea7\u751f\u6b67\u4e49\u3002\u73b0\u6709\u7684\u77e5\u8bc6\u8fc1\u79fb\u65b9\u6cd5\u5728\u5229\u75283D\u51e0\u4f55\u77e5\u8bc6\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u5e76\u53ef\u80fd\u5f15\u5165\u9519\u8bef\u7684\u89c6\u5dee\u4f30\u8ba1\u5e26\u6765\u7684\u566a\u58f0\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u5373\u63d2\u5373\u7528\u89c6\u5dee\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7b97\u6cd5\u548c\u4e24\u4e2a\u6df1\u5ea6\u5148\u9a8c\u5f15\u5bfc\u635f\u5931\u51fd\u6570\u7684\u65b0\u578b\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5b83\u901a\u8fc7\u68c0\u67e5\u76f8\u90bb\u89c6\u5dee\u53ca\u5176\u5bf9\u5e94\u76f8\u5bf9\u6df1\u5ea6\u4e4b\u95f4\u7684\u5c40\u90e8\u4e00\u81f4\u6027\u6765\u83b7\u53d6\u89c6\u5dee\u7f6e\u4fe1\u5ea6\uff0c\u7136\u540e\u4ec5\u4f7f\u7528\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u89c6\u5dee\u4f30\u8ba1\u6765\u6784\u5efa\u51c6\u7a20\u5bc6\u5bf9\u5e94\u4ee5\u4fc3\u8fdb\u9ad8\u6548\u7684\u6df1\u5ea6\u6392\u5e8f\u5b66\u4e60\uff0c\u6700\u540e\u63d0\u51fa\u4e86\u53cc\u91cd\u89c6\u5dee\u5e73\u6ed1\u635f\u5931\u4ee5\u63d0\u5347\u89c6\u5dee\u4e0d\u8fde\u7eed\u5904\u7684\u5339\u914d\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u5728KITTI\u7acb\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u6240\u6709\u65e0\u76d1\u7763\u7acb\u4f53\u5339\u914d\u65b9\u6cd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7acb\u4f53\u5339\u914d\u7cbe\u5ea6\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u7acb\u4f53\u5339\u914d\u4e2d\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7cbe\u786e\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u548c\u6df1\u5ea6\u5148\u9a8c\u5f15\u5bfc\u7684\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7acb\u4f53\u5339\u914d\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u6a21\u7cca\u533a\u57df\u548c\u4e0d\u8fde\u7eed\u6027\u65b9\u9762\u3002"}}
{"id": "2508.01871", "pdf": "https://arxiv.org/pdf/2508.01871", "abs": "https://arxiv.org/abs/2508.01871", "authors": ["Yuanyuan Liang", "Lei Pan", "Tingyu Xie", "Yunshi Lan", "Weining Qian"], "title": "Multi-turn Natural Language to Graph Query Language Translation", "categories": ["cs.AI", "cs.DB"], "comment": "21 pages", "summary": "In recent years, research on transforming natural language into graph query\nlanguage (NL2GQL) has been increasing. Most existing methods focus on\nsingle-turn transformation from NL to GQL. In practical applications, user\ninteractions with graph databases are typically multi-turn, dynamic, and\ncontext-dependent. While single-turn methods can handle straightforward\nqueries, more complex scenarios often require users to iteratively adjust their\nqueries, investigate the connections between entities, or request additional\ndetails across multiple dialogue turns. Research focused on single-turn\nconversion fails to effectively address multi-turn dialogues and complex\ncontext dependencies. Additionally, the scarcity of high-quality multi-turn\nNL2GQL datasets further hinders the progress of this field. To address this\nchallenge, we propose an automated method for constructing multi-turn NL2GQL\ndatasets based on Large Language Models (LLMs) , and apply this method to\ndevelop the MTGQL dataset, which is constructed from a financial market graph\ndatabase and will be publicly released for future research. Moreover, we\npropose three types of baseline methods to assess the effectiveness of\nmulti-turn NL2GQL translation, thereby laying a solid foundation for future\nresearch.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u591a\u8f6e\u81ea\u7136\u8bed\u8a00\u5230\u56fe\u67e5\u8be2\u8bed\u8a00\uff08NL2GQL\uff09\u8f6c\u6362\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u73b0\u6709\u65b9\u6cd5\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u81ea\u52a8\u5316\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86MTGQL\u6570\u636e\u96c6\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e09\u79cd\u57fa\u7ebf\u65b9\u6cd5\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7814\u7a76\u3002", "motivation": "\u73b0\u6709NL2GQL\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5355\u8f6e\u8f6c\u6362\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5b9e\u9645\u5e94\u7528\u4e2d\u591a\u8f6e\u3001\u52a8\u6001\u4e14\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u56fe\u6570\u636e\u5e93\u4ea4\u4e92\u573a\u666f\u3002\u6b64\u5916\uff0c\u9ad8\u8d28\u91cf\u591a\u8f6eNL2GQL\u6570\u636e\u96c6\u7684\u532e\u4e4f\u4e25\u91cd\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "1. \u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u81ea\u52a8\u5316\u591a\u8f6eNL2GQL\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\u30022. \u57fa\u4e8e\u91d1\u878d\u5e02\u573a\u56fe\u6570\u636e\u5e93\uff0c\u5229\u7528\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86MTGQL\u6570\u636e\u96c6\u30023. \u63d0\u51fa\u4e86\u4e09\u79cd\u7c7b\u578b\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8f6eNL2GQL\u7ffb\u8bd1\u7684\u6709\u6548\u6027\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u7528\u4e8e\u672a\u6765\u7814\u7a76\u7684MTGQL\u591a\u8f6eNL2GQL\u6570\u636e\u96c6\uff0c\u5e76\u8ba1\u5212\u516c\u5f00\u3002\u63d0\u51fa\u4e86\u8bc4\u4f30\u591a\u8f6eNL2GQL\u8f6c\u6362\u6709\u6548\u6027\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u89e3\u51b3\u4e86\u591a\u8f6eNL2GQL\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2508.02045", "pdf": "https://arxiv.org/pdf/2508.02045", "abs": "https://arxiv.org/abs/2508.02045", "authors": ["Soyeon Kim", "Jindong Wang", "Xing Xie", "Steven Euijong Whang"], "title": "Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Facts evolve over time, making it essential for Large Language Models (LLMs)\nto handle time-sensitive factual knowledge accurately and reliably. While\nfactual Time-Sensitive Question-Answering (TSQA) tasks have been widely\nstudied, existing benchmarks often rely on manual curation or a small, fixed\nset of predefined templates, which restricts scalable and comprehensive TSQA\nevaluation. To address these challenges, we propose TDBench, a new benchmark\nthat systematically constructs TSQA pairs by harnessing temporal databases and\ndatabase techniques such as temporal SQL and functional dependencies. We also\nintroduce a fine-grained evaluation metric called time accuracy, which assesses\nthe validity of time references in model explanations alongside traditional\nanswer accuracy to enable a more reliable TSQA evaluation. Extensive\nexperiments on contemporary LLMs show how \\ours{} enables scalable and\ncomprehensive TSQA evaluation while reducing the reliance on human labor,\ncomplementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by\nenabling LLM evaluation on application-specific data and seamless multi-hop\nquestion generation. Code and data are publicly available at:\nhttps://github.com/ssoy0701/tdbench.git.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTDBench\uff0c\u4e00\u4e2a\u5229\u7528\u65f6\u6001\u6570\u636e\u5e93\u6280\u672f\u7cfb\u7edf\u6784\u5efa\u65f6\u95f4\u654f\u611f\u95ee\u7b54\uff08TSQA\uff09\u5bf9\u7684\u65b0\u57fa\u51c6\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u51c6\u786e\u5ea6\u6307\u6807\uff0c\u65e8\u5728\u5b9e\u73b0LLM\u65f6\u95f4\u654f\u611f\u4e8b\u5b9e\u5904\u7406\u80fd\u529b\u7684\u53ef\u6269\u5c55\u548c\u5168\u9762\u8bc4\u4f30\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u57fa\u51c6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5904\u7406\u65f6\u95f4\u654f\u611f\u4e8b\u5b9e\u7684\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65f6\u95f4\u654f\u611f\u95ee\u7b54\uff08TSQA\uff09\u8bc4\u4f30\u57fa\u51c6\u5927\u591a\u4f9d\u8d56\u4eba\u5de5\u6574\u7406\u6216\u56fa\u5b9a\u6a21\u677f\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u548c\u5168\u9762\u6027\u3002", "method": "\u63d0\u51faTDBench\u57fa\u51c6\uff0c\u901a\u8fc7\u5229\u7528\u65f6\u6001\u6570\u636e\u5e93\u4ee5\u53ca\u65f6\u6001SQL\u548c\u51fd\u6570\u4f9d\u8d56\u7b49\u6570\u636e\u5e93\u6280\u672f\uff0c\u7cfb\u7edf\u5730\u6784\u5efaTSQA\u5bf9\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u65f6\u95f4\u51c6\u786e\u5ea6\u201d\u7684\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u89e3\u91ca\u4e2d\u65f6\u95f4\u5f15\u7528\u7684\u6709\u6548\u6027\uff0c\u8f85\u4ee5\u4f20\u7edf\u7684\u7b54\u6848\u51c6\u786e\u5ea6\u3002", "result": "\u5bf9\u5f53\u4ee3LLM\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTDBench\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u5168\u9762\u7684TSQA\u8bc4\u4f30\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u4f9d\u8d56\u3002\u5b83\u901a\u8fc7\u652f\u6301\u5e94\u7528\u7a0b\u5e8f\u7279\u5b9a\u6570\u636e\u548c\u65e0\u7f1d\u7684\u591a\u8df3\u95ee\u9898\u751f\u6210\uff0c\u6709\u6548\u5730\u8865\u5145\u4e86\u73b0\u6709\u57fa\u4e8eWikipedia/Wikidata\u7684TSQA\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "TDBench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u521b\u65b0\u4e14\u9ad8\u6548\u7684TSQA\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u548c\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u514b\u670d\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u5c40\u9650\u6027\uff0c\u4f7f\u5f97\u5bf9LLM\u5904\u7406\u65f6\u95f4\u654f\u611f\u77e5\u8bc6\u7684\u80fd\u529b\u8bc4\u4f30\u66f4\u52a0\u53ef\u9760\u548c\u5168\u9762\u3002"}}
{"id": "2508.01240", "pdf": "https://arxiv.org/pdf/2508.01240", "abs": "https://arxiv.org/abs/2508.01240", "authors": ["Juntong Chen", "Huayuan Ye", "He Zhu", "Siwei Fu", "Changbo Wang", "Chenhui Li"], "title": "RelMap: Reliable Spatiotemporal Sensor Data Visualization via Imputative Spatial Interpolation", "categories": ["cs.LG", "cs.HC"], "comment": "9 pages, 14 figures, paper accepted to IEEE VIS 2025", "summary": "Accurate and reliable visualization of spatiotemporal sensor data such as\nenvironmental parameters and meteorological conditions is crucial for informed\ndecision-making. Traditional spatial interpolation methods, however, often fall\nshort of producing reliable interpolation results due to the limited and\nirregular sensor coverage. This paper introduces a novel spatial interpolation\npipeline that achieves reliable interpolation results and produces a novel\nheatmap representation with uncertainty information encoded. We leverage\nimputation reference data from Graph Neural Networks (GNNs) to enhance\nvisualization reliability and temporal resolution. By integrating Principal\nNeighborhood Aggregation (PNA) and Geographical Positional Encoding (GPE), our\nmodel effectively learns the spatiotemporal dependencies. Furthermore, we\npropose an extrinsic, static visualization technique for interpolation-based\nheatmaps that effectively communicates the uncertainties arising from various\nsources in the interpolated map. Through a set of use cases, extensive\nevaluations on real-world datasets, and user studies, we demonstrate our\nmodel's superior performance for data imputation, the improvements to the\ninterpolant with reference data, and the effectiveness of our visualization\ndesign in communicating uncertainties.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7a7a\u95f4\u63d2\u503c\u6d41\u7a0b\uff0c\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u53c2\u8003\u6570\u636e\u589e\u5f3a\u65f6\u7a7a\u4f20\u611f\u5668\u6570\u636e\u53ef\u89c6\u5316\u7684\u53ef\u9760\u6027\uff0c\u5e76\u6709\u6548\u7f16\u7801\u548c\u5c55\u793a\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u3002", "motivation": "\u51c6\u786e\u53ef\u9760\u7684\u65f6\u7a7a\u4f20\u611f\u5668\u6570\u636e\u53ef\u89c6\u5316\u5bf9\u4e8e\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u7a7a\u95f4\u63d2\u503c\u65b9\u6cd5\u56e0\u4f20\u611f\u5668\u8986\u76d6\u6709\u9650\u4e14\u4e0d\u89c4\u5219\uff0c\u96be\u4ee5\u4ea7\u751f\u53ef\u9760\u7ed3\u679c\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u7a7a\u95f4\u63d2\u503c\u6d41\u7a0b\uff0c\u5229\u7528GNN\u7684\u63d2\u8865\u53c2\u8003\u6570\u636e\u63d0\u9ad8\u53ef\u89c6\u5316\u53ef\u9760\u6027\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u3002\u6a21\u578b\u6574\u5408\u4e86\u4e3b\u90bb\u57df\u805a\u5408\uff08PNA\uff09\u548c\u5730\u7406\u4f4d\u7f6e\u7f16\u7801\uff08GPE\uff09\u6765\u5b66\u4e60\u65f6\u7a7a\u4f9d\u8d56\u6027\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5916\u5728\u7684\u3001\u9759\u6001\u7684\u53ef\u89c6\u5316\u6280\u672f\uff0c\u7528\u4e8e\u6709\u6548\u4f20\u8fbe\u63d2\u503c\u70ed\u56fe\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u901a\u8fc7\u4e00\u7cfb\u5217\u7528\u4f8b\u3001\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u548c\u7528\u6237\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u6a21\u578b\u5728\u6570\u636e\u63d2\u8865\u65b9\u9762\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u53c2\u8003\u6570\u636e\u5bf9\u63d2\u503c\u7ed3\u679c\u7684\u6539\u8fdb\uff0c\u4ee5\u53ca\u53ef\u89c6\u5316\u8bbe\u8ba1\u5728\u4f20\u8fbe\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u80fd\u591f\u751f\u6210\u53ef\u9760\u63d2\u503c\u7ed3\u679c\u5e76\u6709\u6548\u4f20\u8fbe\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\u7684\u65b0\u578b\u65f6\u7a7a\u6570\u636e\u53ef\u89c6\u5316\u7ba1\u9053\uff0c\u63d0\u5347\u4e86\u4f20\u611f\u5668\u6570\u636e\u7684\u53ef\u7528\u6027\u548c\u51b3\u7b56\u652f\u6301\u80fd\u529b\u3002"}}
{"id": "2508.01293", "pdf": "https://arxiv.org/pdf/2508.01293", "abs": "https://arxiv.org/abs/2508.01293", "authors": ["Ngoc Bui Lam Quang", "Nam Le Nguyen Binh", "Thanh-Huy Nguyen", "Le Thien Phuc Nguyen", "Quan Nguyen", "Ulas Bagci"], "title": "GMAT: Grounded Multi-Agent Clinical Description Generation for Text Encoder in Vision-Language MIL for Whole Slide Image Classification", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multiple Instance Learning (MIL) is the leading approach for whole slide\nimage (WSI) classification, enabling efficient analysis of gigapixel pathology\nslides. Recent work has introduced vision-language models (VLMs) into MIL\npipelines to incorporate medical knowledge through text-based class\ndescriptions rather than simple class names. However, when these methods rely\non large language models (LLMs) to generate clinical descriptions or use\nfixed-length prompts to represent complex pathology concepts, the limited token\ncapacity of VLMs often constrains the expressiveness and richness of the\nencoded class information. Additionally, descriptions generated solely by LLMs\nmay lack domain grounding and fine-grained medical specificity, leading to\nsuboptimal alignment with visual features. To address these challenges, we\npropose a vision-language MIL framework with two key contributions: (1) A\ngrounded multi-agent description generation system that leverages curated\npathology textbooks and agent specialization (e.g., morphology, spatial\ncontext) to produce accurate and diverse clinical descriptions; (2) A text\nencoding strategy using a list of descriptions rather than a single prompt,\ncapturing fine-grained and complementary clinical signals for better alignment\nwith visual features. Integrated into a VLM-MIL pipeline, our approach shows\nimproved performance over single-prompt class baselines and achieves results\ncomparable to state-of-the-art models, as demonstrated on renal and lung cancer\ndatasets.", "AI": {"tldr": "\u9488\u5bf9\u5168\u73bb\u7247\u56fe\u50cf\uff08WSI\uff09\u5206\u7c7b\u4e2dVLM-MIL\u65b9\u6cd5\u7684\u63cf\u8ff0\u9650\u5236\u548c\u9886\u57df\u7279\u5f02\u6027\u4e0d\u8db3\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00MIL\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u63a5\u5730\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u751f\u6210\u51c6\u786e\u591a\u6837\u7684\u4e34\u5e8a\u63cf\u8ff0\uff0c\u5e76\u91c7\u7528\u5217\u8868\u5f0f\u6587\u672c\u7f16\u7801\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524dVLM-MIL\u65b9\u6cd5\u5728\u5168\u73bb\u7247\u56fe\u50cf\uff08WSI\uff09\u5206\u7c7b\u4e2d\uff0c\u4f9d\u8d56\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e34\u5e8a\u63cf\u8ff0\u6216\u4f7f\u7528\u56fa\u5b9a\u957f\u5ea6\u63d0\u793a\u65f6\uff0c\u53d7\u9650\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u4ee4\u724c\u5bb9\u91cf\uff0c\u5bfc\u81f4\u7f16\u7801\u7684\u7c7b\u4fe1\u606f\u8868\u8fbe\u80fd\u529b\u548c\u4e30\u5bcc\u6027\u53d7\u9650\u3002\u6b64\u5916\uff0c\u7eaf\u7cb9\u7531\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u63cf\u8ff0\u53ef\u80fd\u7f3a\u4e4f\u9886\u57df\u57fa\u7840\u548c\u7ec6\u7c92\u5ea6\u533b\u5b66\u7279\u5f02\u6027\uff0c\u5bfc\u81f4\u4e0e\u89c6\u89c9\u7279\u5f81\u7684\u5bf9\u9f50\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e24\u9879\u5173\u952e\u8d21\u732e\uff1a1. **\u63a5\u5730\u591a\u667a\u80fd\u4f53\u63cf\u8ff0\u751f\u6210\u7cfb\u7edf\uff1a** \u5229\u7528\u7cbe\u9009\u7684\u75c5\u7406\u5b66\u6559\u79d1\u4e66\u548c\u667a\u80fd\u4f53\u4e13\u4e1a\u5316\uff08\u5982\u5f62\u6001\u5b66\u3001\u7a7a\u95f4\u80cc\u666f\uff09\u6765\u751f\u6210\u51c6\u786e\u4e14\u591a\u6837\u5316\u7684\u4e34\u5e8a\u63cf\u8ff0\u30022. **\u6587\u672c\u7f16\u7801\u7b56\u7565\uff1a** \u91c7\u7528\u63cf\u8ff0\u5217\u8868\u800c\u975e\u5355\u4e00\u63d0\u793a\u6765\u7f16\u7801\u6587\u672c\u4fe1\u606f\uff0c\u4ee5\u6355\u83b7\u7ec6\u7c92\u5ea6\u548c\u4e92\u8865\u7684\u4e34\u5e8a\u4fe1\u53f7\uff0c\u4ece\u800c\u66f4\u597d\u5730\u4e0e\u89c6\u89c9\u7279\u5f81\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u88ab\u6574\u5408\u5230VLM-MIL\u7ba1\u7ebf\u4e2d\u3002", "result": "\u5728\u80be\u764c\u548c\u80ba\u764c\u6570\u636e\u96c6\u4e0a\uff0c\u672c\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u4e8e\u5355\u4e00\u63d0\u793a\u7c7b\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u5e76\u53d6\u5f97\u4e86\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u63a5\u5730\u591a\u667a\u80fd\u4f53\u63cf\u8ff0\u751f\u6210\u7cfb\u7edf\u548c\u91c7\u7528\u591a\u5217\u8868\u6587\u672c\u7f16\u7801\u7b56\u7565\uff0c\u672c\u7814\u7a76\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709VLM-MIL\u65b9\u6cd5\u5728\u5168\u73bb\u7247\u56fe\u50cf\u5206\u7c7b\u4e2d\u9762\u4e34\u7684\u63cf\u8ff0\u8868\u8fbe\u9650\u5236\u548c\u9886\u57df\u7279\u5f02\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86SOTA\u6c34\u5e73\u3002"}}
{"id": "2508.01956", "pdf": "https://arxiv.org/pdf/2508.01956", "abs": "https://arxiv.org/abs/2508.01956", "authors": ["Jiayi Wang", "Jacqueline Jil Vallon", "Neil Panjwani", "Xi Ling", "Sushmita Vij", "Sandy Srinivas", "John Leppert", "Mark K. Buyyounouski", "Mohsen Bayati"], "title": "Agent-Based Feature Generation from Clinical Notes for Outcome Prediction", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Electronic health records (EHRs) contain rich unstructured clinical notes\nthat could enhance predictive modeling, yet extracting meaningful features from\nthese notes remains challenging. Current approaches range from labor-intensive\nmanual clinician feature generation (CFG) to fully automated representational\nfeature generation (RFG) that lack interpretability and clinical relevance.\nHere we introduce SNOW (Scalable Note-to-Outcome Workflow), a modular\nmulti-agent system powered by large language models (LLMs) that autonomously\ngenerates structured clinical features from unstructured notes without human\nintervention. We evaluated SNOW against manual CFG, clinician-guided LLM\napproaches, and RFG methods for predicting 5-year prostate cancer recurrence in\n147 patients from Stanford Healthcare. While manual CFG achieved the highest\nperformance (AUC-ROC: 0.771), SNOW matched this performance (0.761) without\nrequiring any clinical expertise, significantly outperforming both baseline\nfeatures alone (0.691) and all RFG approaches. The clinician-guided LLM method\nalso performed well (0.732) but still required expert input. SNOW's specialized\nagents handle feature discovery, extraction, validation, post-processing, and\naggregation, creating interpretable features that capture complex clinical\ninformation typically accessible only through manual review. Our findings\ndemonstrate that autonomous LLM systems can replicate expert-level feature\nengineering at scale, potentially transforming how clinical ML models leverage\nunstructured EHR data while maintaining the interpretability essential for\nclinical deployment.", "AI": {"tldr": "SNOW\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u80fd\u81ea\u4e3b\u4ece\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u751f\u6210\u53ef\u89e3\u91ca\u7684\u7ed3\u6784\u5316\u7279\u5f81\u3002\u5728\u9884\u6d4b\u524d\u5217\u817a\u764c\u590d\u53d1\u4efb\u52a1\u4e2d\uff0cSNOW\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u53ef\u8fbe\u5230\u4e0e\u4eba\u5de5\u4e13\u5bb6\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\uff0c\u4f18\u4e8e\u5176\u4ed6\u81ea\u52a8\u5316\u65b9\u6cd5\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u7b14\u8bb0\u5305\u542b\u4e30\u5bcc\u4fe1\u606f\uff0c\u4f46\u96be\u4ee5\u6709\u6548\u63d0\u53d6\u6709\u610f\u4e49\u7684\u7279\u5f81\u4ee5\u589e\u5f3a\u9884\u6d4b\u6a21\u578b\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u52b3\u52a8\u5bc6\u96c6\uff08\u4eba\u5de5\u751f\u6210\uff09\uff0c\u8981\u4e48\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u76f8\u5173\u6027\uff08\u81ea\u52a8\u5316\u751f\u6210\uff09\u3002", "method": "\u672c\u6587\u63d0\u51faSNOW\uff08Scalable Note-to-Outcome Workflow\uff09\uff0c\u4e00\u4e2a\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9a71\u52a8\u7684\u6a21\u5757\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u3002SNOW\u80fd\u81ea\u4e3b\u4ece\u975e\u7ed3\u6784\u5316\u7b14\u8bb0\u4e2d\u751f\u6210\u7ed3\u6784\u5316\u4e34\u5e8a\u7279\u5f81\uff0c\u5176\u4e13\u4e1a\u667a\u80fd\u4f53\u8d1f\u8d23\u7279\u5f81\u53d1\u73b0\u3001\u63d0\u53d6\u3001\u9a8c\u8bc1\u3001\u540e\u5904\u7406\u548c\u805a\u5408\u3002", "result": "\u5728\u9884\u6d4b5\u5e74\u671f\u524d\u5217\u817a\u764c\u590d\u53d1\u4efb\u52a1\u4e2d\uff0cSNOW\uff08AUC-ROC: 0.761\uff09\u6027\u80fd\u4e0e\u4eba\u5de5\u7279\u5f81\u751f\u6210\uff080.771\uff09\u76f8\u5f53\uff0c\u4e14\u65e0\u9700\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u3002SNOW\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u57fa\u7ebf\u7279\u5f81\uff080.691\uff09\u548c\u6240\u6709\u8868\u793a\u7279\u5f81\u751f\u6210\u65b9\u6cd5\u3002SNOW\u751f\u6210\u7684\u7279\u5f81\u53ef\u89e3\u91ca\u4e14\u6355\u83b7\u4e86\u590d\u6742\u4e34\u5e8a\u4fe1\u606f\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u81ea\u4e3bLLM\u7cfb\u7edf\u80fd\u591f\u5927\u89c4\u6a21\u5730\u590d\u5236\u4e13\u5bb6\u7ea7\u522b\u7684\u7279\u5f81\u5de5\u7a0b\uff0c\u6709\u671b\u6539\u53d8\u4e34\u5e8a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5229\u7528\u975e\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u7684\u65b9\u5f0f\uff0c\u540c\u65f6\u4fdd\u6301\u4e34\u5e8a\u90e8\u7f72\u6240\u9700\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.02053", "pdf": "https://arxiv.org/pdf/2508.02053", "abs": "https://arxiv.org/abs/2508.02053", "authors": ["Zhentao Xu", "Fengyi Li", "Albert Chen", "Xiaofeng Wang"], "title": "ProCut: LLM Prompt Compression via Attribution Estimation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In large-scale industrial LLM systems, prompt templates often expand to\nthousands of tokens as teams iteratively incorporate sections such as task\ninstructions, few-shot examples, and heuristic rules to enhance robustness and\ncoverage. This expansion leads to bloated prompts that are difficult to\nmaintain and incur significant inference latency and serving costs. To address\nthis, we introduce Prompt Compression via Attribution Estimation (ProCut), a\nflexible, LLM-agnostic, training-free framework that compresses prompts through\nattribution analysis. ProCut segments prompt templates into semantically\nmeaningful units, quantifies their impact on task performance, and prunes\nlow-utility components. Through extensive experiments on five public benchmark\ndatasets and real-world industrial prompts, we show that ProCut achieves\nsubstantial prompt size reductions (78% fewer tokens in production) while\nmaintaining or even slightly improving task performance (up to 62% better than\nalternative methods). We further introduce an LLM-driven attribution estimator\nthat reduces compression latency by over 50%, and demonstrate that ProCut\nintegrates seamlessly with existing prompt-optimization frameworks to produce\nconcise, high-performing prompts.", "AI": {"tldr": "ProCut\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f52\u56e0\u5206\u6790\u7684\u63d0\u793a\u8bcd\u538b\u7f29\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u5927\u578bLLM\u7cfb\u7edf\u4e2d\u63d0\u793a\u8bcd\u8fc7\u957f\u5bfc\u81f4\u7684\u95ee\u9898\uff0c\u5b83\u80fd\u663e\u8457\u51cf\u5c11\u63d0\u793a\u8bcd\u957f\u5ea6\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u8bad\u7ec3\u3002", "motivation": "\u5728\u5927\u578b\u5de5\u4e1aLLM\u7cfb\u7edf\u4e2d\uff0c\u63d0\u793a\u8bcd\u6a21\u677f\u56e0\u8fed\u4ee3\u52a0\u5165\u4efb\u52a1\u6307\u4ee4\u3001\u5c11\u6837\u672c\u793a\u4f8b\u548c\u542f\u53d1\u5f0f\u89c4\u5219\u7b49\u800c\u53d8\u5f97\u975e\u5e38\u5197\u957f\uff0c\u8fd9\u5bfc\u81f4\u63d0\u793a\u8bcd\u96be\u4ee5\u7ef4\u62a4\uff0c\u5e76\u5e26\u6765\u663e\u8457\u7684\u63a8\u7406\u5ef6\u8fdf\u548c\u9ad8\u6602\u7684\u670d\u52a1\u6210\u672c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u4e0eLLM\u65e0\u5173\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\u2014\u2014Prompt Compression via Attribution Estimation (ProCut)\u3002ProCut\u901a\u8fc7\u5f52\u56e0\u5206\u6790\u538b\u7f29\u63d0\u793a\u8bcd\uff0c\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u5c06\u63d0\u793a\u8bcd\u6a21\u677f\u5206\u5272\u6210\u8bed\u4e49\u5355\u5143\uff0c\u91cf\u5316\u6bcf\u4e2a\u5355\u5143\u5bf9\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u4fee\u526a\u4f4e\u6548\u7528\u7ec4\u4ef6\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2aLLM\u9a71\u52a8\u7684\u5f52\u56e0\u4f30\u8ba1\u5668\u4ee5\u8fdb\u4e00\u6b65\u51cf\u5c11\u538b\u7f29\u5ef6\u8fdf\u3002", "result": "\u901a\u8fc7\u5728\u4e94\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u548c\u5b9e\u9645\u5de5\u4e1a\u63d0\u793a\u8bcd\u4e0a\u7684\u5b9e\u9a8c\uff0cProCut\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u63d0\u793a\u8bcd\u5927\u5c0f\u7f29\u51cf\uff08\u751f\u4ea7\u73af\u5883\u4e2d\u51cf\u5c1178%\u7684token\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u751a\u81f3\u7565\u5fae\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\uff08\u6bd4\u66ff\u4ee3\u65b9\u6cd5\u6700\u9ad8\u63d0\u534762%\uff09\u3002LLM\u9a71\u52a8\u7684\u5f52\u56e0\u4f30\u8ba1\u5668\u5c06\u538b\u7f29\u5ef6\u8fdf\u964d\u4f4e\u4e8650%\u4ee5\u4e0a\u3002ProCut\u8fd8\u80fd\u4e0e\u73b0\u6709\u63d0\u793a\u4f18\u5316\u6846\u67b6\u65e0\u7f1d\u96c6\u6210\uff0c\u751f\u6210\u7b80\u6d01\u3001\u9ad8\u6027\u80fd\u7684\u63d0\u793a\u8bcd\u3002", "conclusion": "ProCut\u4e3a\u5927\u578b\u5de5\u4e1aLLM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u63d0\u793a\u8bcd\u538b\u7f29\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u80fd\u663e\u8457\u964d\u4f4e\u8fd0\u7ef4\u6210\u672c\u548c\u63a8\u7406\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u8bc1\u6216\u63d0\u5347\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u517c\u5bb9\u6027\u3002"}}
{"id": "2508.01251", "pdf": "https://arxiv.org/pdf/2508.01251", "abs": "https://arxiv.org/abs/2508.01251", "authors": ["Hung-Chieh Fang", "Hsuan-Tien Lin", "Irwin King", "Yifei Zhang"], "title": "Soft Separation and Distillation: Toward Global Uniformity in Federated Unsupervised Learning", "categories": ["cs.LG"], "comment": "Published at ICCV 2025", "summary": "Federated Unsupervised Learning (FUL) aims to learn expressive\nrepresentations in federated and self-supervised settings. The quality of\nrepresentations learned in FUL is usually determined by uniformity, a measure\nof how uniformly representations are distributed in the embedding space.\nHowever, existing solutions perform well in achieving intra-client (local)\nuniformity for local models while failing to achieve inter-client (global)\nuniformity after aggregation due to non-IID data distributions and the\ndecentralized nature of FUL. To address this issue, we propose Soft Separation\nand Distillation (SSD), a novel approach that preserves inter-client uniformity\nby encouraging client representations to spread toward different directions.\nThis design reduces interference during client model aggregation, thereby\nimproving global uniformity while preserving local representation\nexpressiveness. We further enhance this effect by introducing a projector\ndistillation module to address the discrepancy between loss optimization and\nrepresentation quality. We evaluate SSD in both cross-silo and cross-device\nfederated settings, demonstrating consistent improvements in representation\nquality and task performance across various training scenarios. Our results\nhighlight the importance of inter-client uniformity in FUL and establish SSD as\nan effective solution to this challenge. Project page:\nhttps://ssd-uniformity.github.io/", "AI": {"tldr": "\u8054\u90a6\u65e0\u76d1\u7763\u5b66\u4e60\uff08FUL\uff09\u5728\u805a\u5408\u540e\u5b58\u5728\u5ba2\u6237\u7aef\u95f4\u5747\u5300\u6027\u5dee\u7684\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u8f6f\u5206\u79bb\u4e0e\u84b8\u998f\uff08SSD\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fc3\u4f7f\u5ba2\u6237\u7aef\u8868\u793a\u5411\u4e0d\u540c\u65b9\u5411\u5206\u6563\u5e76\u5f15\u5165\u6295\u5f71\u5668\u84b8\u998f\u6a21\u5757\u6765\u63d0\u9ad8\u5168\u5c40\u5747\u5300\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u65e0\u76d1\u7763\u5b66\u4e60\uff08FUL\uff09\u65b9\u6cd5\u80fd\u5f88\u597d\u5730\u5b9e\u73b0\u5ba2\u6237\u7aef\u5185\uff08\u5c40\u90e8\uff09\u5747\u5300\u6027\uff0c\u4f46\u7531\u4e8e\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u548c\u53bb\u4e2d\u5fc3\u5316\u7279\u6027\uff0c\u5728\u6a21\u578b\u805a\u5408\u540e\u65e0\u6cd5\u4fdd\u6301\u5ba2\u6237\u7aef\u95f4\uff08\u5168\u5c40\uff09\u5747\u5300\u6027\uff0c\u5bfc\u81f4\u8868\u793a\u8d28\u91cf\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u8f6f\u5206\u79bb\u4e0e\u84b8\u998f\uff08SSD\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u9f13\u52b1\u5ba2\u6237\u7aef\u8868\u793a\u5411\u4e0d\u540c\u65b9\u5411\u5206\u6563\uff0c\u4ee5\u4fdd\u6301\u5ba2\u6237\u7aef\u95f4\u5747\u5300\u6027\u5e76\u51cf\u5c11\u805a\u5408\u5e72\u6270\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u4e2a\u6295\u5f71\u5668\u84b8\u998f\u6a21\u5757\uff0c\u7528\u4e8e\u89e3\u51b3\u635f\u5931\u4f18\u5316\u4e0e\u8868\u793a\u8d28\u91cf\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "\u5728\u8de8\u7b52\u4ed3\u548c\u8de8\u8bbe\u5907\u8054\u90a6\u73af\u5883\u4e2d\u5bf9SSD\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5404\u79cd\u8bad\u7ec3\u573a\u666f\u4e0b\uff0c\u8868\u793a\u8d28\u91cf\u548c\u4efb\u52a1\u6027\u80fd\u5747\u5f97\u5230\u4e86\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5ba2\u6237\u7aef\u95f4\u5747\u5300\u6027\u5728\u8054\u90a6\u65e0\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u786e\u7acbSSD\u662f\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u7684\u6709\u6548\u65b9\u6848\uff0c\u5b83\u5728\u4fdd\u6301\u5c40\u90e8\u8868\u793a\u8868\u8fbe\u80fd\u529b\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u5168\u5c40\u5747\u5300\u6027\u3002"}}
{"id": "2508.01303", "pdf": "https://arxiv.org/pdf/2508.01303", "abs": "https://arxiv.org/abs/2508.01303", "authors": ["Shuangli Du", "Jing Wang", "Minghua Zhao", "Zhenyu Xu", "Jie Li"], "title": "Domain Generalized Stereo Matching with Uncertainty-guided Data Augmentation", "categories": ["cs.CV", "I.2.0; I.2.6"], "comment": "10 pages, 7 figures, submitted to AAAI 2026", "summary": "State-of-the-art stereo matching (SM) models trained on synthetic data often\nfail to generalize to real data domains due to domain differences, such as\ncolor, illumination, contrast, and texture. To address this challenge, we\nleverage data augmentation to expand the training domain, encouraging the model\nto acquire robust cross-domain feature representations instead of\ndomain-dependent shortcuts. This paper proposes an uncertainty-guided data\naugmentation (UgDA) method, which argues that the image statistics in RGB space\n(mean and standard deviation) carry the domain characteristics. Thus, samples\nin unseen domains can be generated by properly perturbing these statistics.\nFurthermore, to simulate more potential domains, Gaussian distributions founded\non batch-level statistics are poposed to model the unceratinty of perturbation\ndirection and intensity. Additionally, we further enforce feature consistency\nbetween original and augmented data for the same scene, encouraging the model\nto learn structure aware, shortcuts-invariant feature representations. Our\napproach is simple, architecture-agnostic, and can be integrated into any SM\nnetworks. Extensive experiments on several challenging benchmarks have\ndemonstrated that our method can significantly improve the generalization\nperformance of existing SM networks.", "AI": {"tldr": "\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u6570\u636e\u589e\u5f3a(UgDA)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8\u56fe\u50cf\u7edf\u8ba1\u91cf\u5e76\u5f3a\u5236\u7279\u5f81\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u7acb\u4f53\u5339\u914d\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6700\u5148\u8fdb\u7684\u7acb\u4f53\u5339\u914d(SM)\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u540e\uff0c\u56e0\u57df\u5dee\u5f02\uff08\u5982\u989c\u8272\u3001\u5149\u7167\u3001\u5bf9\u6bd4\u5ea6\u3001\u7eb9\u7406\uff09\u5bfc\u81f4\u96be\u4ee5\u6cdb\u5316\u5230\u771f\u5b9e\u6570\u636e\u57df\u3002", "method": "\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u6570\u636e\u589e\u5f3a(UgDA)\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6270\u52a8\u56fe\u50cf\u7684RGB\u7edf\u8ba1\u91cf\uff08\u5747\u503c\u548c\u6807\u51c6\u5dee\uff09\u6765\u751f\u6210\u672a\u89c1\u57df\u6837\u672c\u3002\u5229\u7528\u57fa\u4e8e\u6279\u5904\u7406\u7edf\u8ba1\u7684\u9ad8\u65af\u5206\u5e03\u5efa\u6a21\u6270\u52a8\u65b9\u5411\u548c\u5f3a\u5ea6\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u540c\u65f6\uff0c\u5bf9\u540c\u4e00\u573a\u666f\u7684\u539f\u59cb\u548c\u589e\u5f3a\u6570\u636e\u5f3a\u5236\u6267\u884c\u7279\u5f81\u4e00\u81f4\u6027\uff0c\u4ee5\u5b66\u4e60\u7ed3\u6784\u611f\u77e5\u548c\u6377\u5f84\u65e0\u5173\u7684\u7279\u5f81\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u7b80\u5355\u3001\u4e0e\u67b6\u6784\u65e0\u5173\uff0c\u53ef\u96c6\u6210\u5230\u4efb\u4f55SM\u7f51\u7edc\u3002", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u73b0\u6709\u7acb\u4f53\u5339\u914d\u7f51\u7edc\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "UgDA\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u548c\u7279\u5f81\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7acb\u4f53\u5339\u914d\u6a21\u578b\u4ece\u5408\u6210\u6570\u636e\u5230\u771f\u5b9e\u6570\u636e\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.02016", "pdf": "https://arxiv.org/pdf/2508.02016", "abs": "https://arxiv.org/abs/2508.02016", "authors": ["Jeiyoon Park", "Yongshin Han", "Minseop Kim", "Kisu Yang"], "title": "Dynamic Context Adaptation for Consistent Role-Playing Agents with Retrieval-Augmented Generations", "categories": ["cs.AI"], "comment": "preprint", "summary": "We propose AMADEUS, which is composed of Adaptive Context-aware Text Splitter\n(ACTS), Guided Selection (GS), and Attribute Extractor (AE). ACTS finds an\noptimal chunk length and hierarchical contexts for each character. AE\nidentifies a character's general attributes from the chunks retrieved by GS and\nuses these attributes as a final context to maintain robust persona consistency\neven when answering out of knowledge questions. To facilitate the development\nand evaluation of RAG-based RPAs, we construct CharacterRAG, a role-playing\ndataset that consists of persona documents for 15 distinct fictional characters\ntotaling 976K written characters, and 450 question and answer pairs. We find\nthat our framework effectively models not only the knowledge possessed by\ncharacters, but also various attributes such as personality.", "AI": {"tldr": "\u63d0\u51faAMADEUS\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6587\u672c\u5206\u5272\u548c\u5c5e\u6027\u63d0\u53d6\uff0c\u589e\u5f3aRAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u89d2\u8272\u626e\u6f14\uff08RPA\uff09\u4e2d\u7684\u89d2\u8272\u4e00\u81f4\u6027\uff0c\u5e76\u6784\u5efaCharacterRAG\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u5f00\u53d1\u4e0e\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3RAG-based RPA\u5728\u5904\u7406\u8d85\u51fa\u77e5\u8bc6\u8303\u56f4\u7684\u95ee\u9898\u65f6\uff0c\u96be\u4ee5\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\u7684\u6311\u6218\uff0c\u5e76\u4e3aRAG-based RPA\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u63d0\u4f9b\u6240\u9700\u7684\u6570\u636e\u96c6\u3002", "method": "1. \u63d0\u51fa\u4e86AMADEUS\u6846\u67b6\uff0c\u5305\u542b\uff1aa) \u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u611f\u77e5\u6587\u672c\u5206\u5272\u5668\uff08ACTS\uff09\uff0c\u7528\u4e8e\u67e5\u627e\u6700\u4f73\u5757\u957f\u5ea6\u548c\u5c42\u7ea7\u4e0a\u4e0b\u6587\uff1bb) \u5f15\u5bfc\u9009\u62e9\uff08GS\uff09\uff1bc) \u5c5e\u6027\u63d0\u53d6\u5668\uff08AE\uff09\uff0c\u4eceGS\u68c0\u7d22\u7684\u5757\u4e2d\u8bc6\u522b\u89d2\u8272\u901a\u7528\u5c5e\u6027\uff0c\u5e76\u7528\u4f5c\u6700\u7ec8\u4e0a\u4e0b\u6587\u4ee5\u4fdd\u6301\u7a33\u5065\u7684\u89d2\u8272\u4e00\u81f4\u6027\uff0c\u5373\u4f7f\u5728\u56de\u7b54\u77e5\u8bc6\u8303\u56f4\u5916\u7684\u95ee\u9898\u65f6\u3002\n2. \u6784\u5efa\u4e86CharacterRAG\u89d2\u8272\u626e\u6f14\u6570\u636e\u96c6\uff0c\u5305\u542b15\u4e2a\u865a\u6784\u89d2\u8272\u7684\u4e2a\u6027\u6587\u6863\uff08\u5171976K\u5b57\u7b26\uff09\u548c450\u5bf9\u95ee\u7b54\u5bf9\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u4e0d\u4ec5\u80fd\u6709\u6548\u5efa\u6a21\u89d2\u8272\u6240\u62e5\u6709\u7684\u77e5\u8bc6\uff0c\u8fd8\u80fd\u5efa\u6a21\u5176\u5404\u79cd\u5c5e\u6027\uff0c\u4f8b\u5982\u4e2a\u6027\u3002", "conclusion": "AMADEUS\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347RAG-based RPA\u7684\u89d2\u8272\u4e00\u81f4\u6027\u8868\u73b0\uff0c\u5373\u4f7f\u9762\u5bf9\u77e5\u8bc6\u8303\u56f4\u5916\u7684\u95ee\u9898\uff0c\u5e76\u4e14CharacterRAG\u6570\u636e\u96c6\u4e3a\u8be5\u9886\u57df\u7684\u53d1\u5c55\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002"}}
{"id": "2508.02074", "pdf": "https://arxiv.org/pdf/2508.02074", "abs": "https://arxiv.org/abs/2508.02074", "authors": ["Gustaf Ahdritz", "Anat Kleiman"], "title": "The SMeL Test: A simple benchmark for media literacy in language models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The internet is rife with unattributed, deliberately misleading, or otherwise\nuntrustworthy content. Though large language models (LLMs) are often tasked\nwith autonomous web browsing, the extent to which they have learned the simple\nheuristics human researchers use to navigate this noisy environment is not\ncurrently known. In this paper, we introduce the Synthetic Media Literacy Test\n(SMeL Test), a minimal benchmark that tests the ability of language models to\nactively filter out untrustworthy information in context. We benchmark a\nvariety of commonly used instruction-tuned LLMs, including reasoning models,\nand find that no model consistently trusts more reliable sources; while\nreasoning in particular is associated with higher scores, even the best API\nmodel we test hallucinates up to 70% of the time. Remarkably, larger and more\ncapable models do not necessarily outperform their smaller counterparts. We\nhope our work sheds more light on this important form of hallucination and\nguides the development of new methods to combat it.", "AI": {"tldr": "\u7814\u7a76\u5f15\u5165SMeL\u6d4b\u8bd5\u6765\u8bc4\u4f30LLM\u5728\u7f51\u7edc\u73af\u5883\u4e2d\u8fc7\u6ee4\u4e0d\u53ef\u4fe1\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\uff08\u5305\u62ec\u5927\u578b\u548c\u63a8\u7406\u6a21\u578b\uff09\u5728\u6b64\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u9ad8\u5e7b\u89c9\u7387\u3002", "motivation": "\u4e92\u8054\u7f51\u5145\u65a5\u4e0d\u53ef\u4fe1\u5185\u5bb9\uff0cLLM\u5e38\u7528\u4e8e\u81ea\u4e3b\u7f51\u9875\u6d4f\u89c8\uff0c\u4f46\u5b83\u4eec\u662f\u5426\u50cf\u4eba\u7c7b\u4e00\u6837\u638c\u63e1\u8fc7\u6ee4\u5608\u6742\u4fe1\u606f\u6e90\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u77e5\u8bc6\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u201c\u5408\u6210\u5a92\u4f53\u7d20\u517b\u6d4b\u8bd5\uff08SMeL Test\uff09\u201d\u4f5c\u4e3a\u57fa\u51c6\uff0c\u6d4b\u8bd5\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u4e2d\u4e3b\u52a8\u8fc7\u6ee4\u4e0d\u53ef\u4fe1\u4fe1\u606f\u7684\u80fd\u529b\u3002\u4f7f\u7528\u8be5\u57fa\u51c6\u5bf9\u591a\u79cd\u5e38\u7528\u6307\u4ee4\u8c03\u4f18LLM\uff08\u5305\u62ec\u63a8\u7406\u6a21\u578b\uff09\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6ca1\u6709\u6a21\u578b\u80fd\u6301\u7eed\u4fe1\u4efb\u66f4\u53ef\u9760\u7684\u6765\u6e90\uff1b\u5c3d\u7ba1\u63a8\u7406\u80fd\u529b\u4e0e\u66f4\u9ad8\u5206\u6570\u76f8\u5173\uff0c\u4f46\u5373\u4f7f\u662f\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\u7684API\u6a21\u578b\u4e5f\u6709\u9ad8\u8fbe70%\u7684\u65f6\u95f4\u4ea7\u751f\u5e7b\u89c9\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u66f4\u5927\u3001\u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\u4e0d\u4e00\u5b9a\u4f18\u4e8e\u5176\u5c0f\u578b\u5bf9\u5e94\u6a21\u578b\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728\u5904\u7406\u4fe1\u606f\u53ef\u4fe1\u5ea6\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u201c\u5e7b\u89c9\u201d\u95ee\u9898\uff0c\u5e0c\u671b\u4e3a\u5f00\u53d1\u5bf9\u6297\u6b64\u95ee\u9898\u7684\u65b0\u65b9\u6cd5\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2508.01287", "pdf": "https://arxiv.org/pdf/2508.01287", "abs": "https://arxiv.org/abs/2508.01287", "authors": ["Micah Rentschler", "Jesse Roberts"], "title": "Exploitation Is All You Need... for Exploration", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Ensuring sufficient exploration is a central challenge when training\nmeta-reinforcement learning (meta-RL) agents to solve novel environments.\nConventional solutions to the exploration-exploitation dilemma inject explicit\nincentives such as randomization, uncertainty bonuses, or intrinsic rewards to\nencourage exploration. In this work, we hypothesize that an agent trained\nsolely to maximize a greedy (exploitation-only) objective can nonetheless\nexhibit emergent exploratory behavior, provided three conditions are met: (1)\nRecurring Environmental Structure, where the environment features repeatable\nregularities that allow past experience to inform future choices; (2) Agent\nMemory, enabling the agent to retain and utilize historical interaction data;\nand (3) Long-Horizon Credit Assignment, where learning propagates returns over\na time frame sufficient for the delayed benefits of exploration to inform\ncurrent decisions. Through experiments in stochastic multi-armed bandits and\ntemporally extended gridworlds, we observe that, when both structure and memory\nare present, a policy trained on a strictly greedy objective exhibits\ninformation-seeking exploratory behavior. We further demonstrate, through\ncontrolled ablations, that emergent exploration vanishes if either\nenvironmental structure or agent memory is absent (Conditions 1 & 2).\nSurprisingly, removing long-horizon credit assignment (Condition 3) does not\nalways prevent emergent exploration-a result we attribute to the\npseudo-Thompson Sampling effect. These findings suggest that, under the right\nprerequisites, exploration and exploitation need not be treated as orthogonal\nobjectives but can emerge from a unified reward-maximization process.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u73af\u5883\u5177\u6709\u91cd\u590d\u7ed3\u6784\u4e14\u667a\u80fd\u4f53\u5177\u5907\u8bb0\u5fc6\u80fd\u529b\u7684\u6761\u4ef6\u4e0b\uff0c\u5373\u4f7f\u662f\u7eaf\u7cb9\u8ffd\u6c42\u8d2a\u5a6a\u76ee\u6807\u7684\u5143\u5f3a\u5316\u5b66\u4e60\uff08meta-RL\uff09\u667a\u80fd\u4f53\u4e5f\u80fd\u8868\u73b0\u51fa\u6d8c\u73b0\u7684\u63a2\u7d22\u884c\u4e3a\uff0c\u8fd9\u98a0\u8986\u4e86\u63a2\u7d22\u4e0e\u5229\u7528\u5fc5\u987b\u6b63\u4ea4\u7684\u4f20\u7edf\u89c2\u5ff5\u3002", "motivation": "\u5143\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5982\u4f55\u5728\u63a2\u7d22\u65b0\u73af\u5883\u65f6\u786e\u4fdd\u8db3\u591f\u7684\u63a2\u7d22\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u663e\u5f0f\u7684\u63a2\u7d22\u6fc0\u52b1\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5728\u6ca1\u6709\u6b64\u7c7b\u6fc0\u52b1\u7684\u60c5\u51b5\u4e0b\uff0c\u63a2\u7d22\u884c\u4e3a\u662f\u5426\u80fd\u4ece\u7eaf\u7cb9\u7684\u8d2a\u5a6a\u76ee\u6807\u4e2d\u81ea\u7136\u6d8c\u73b0\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5728\u968f\u673a\u591a\u81c2\u8001\u864e\u673a\u548c\u65f6\u5e8f\u6269\u5c55\u7684\u7f51\u683c\u4e16\u754c\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u4ed6\u4eec\u8bad\u7ec3\u667a\u80fd\u4f53\u4ec5\u6700\u5927\u5316\u8d2a\u5a6a\uff08\u5229\u7528\uff09\u76ee\u6807\uff0c\u5e76\u89c2\u5bdf\u5176\u884c\u4e3a\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u53d7\u63a7\u7684\u6d88\u878d\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u5047\u8bbe\u7684\u4e09\u4e2a\u6761\u4ef6\uff08\u91cd\u590d\u73af\u5883\u7ed3\u6784\u3001\u667a\u80fd\u4f53\u8bb0\u5fc6\u3001\u957f\u65f6\u5e8f\u4fe1\u7528\u5206\u914d\uff09\u5bf9\u63a2\u7d22\u884c\u4e3a\u6d8c\u73b0\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u73af\u5883\u5177\u5907\u91cd\u590d\u7ed3\u6784\u4e14\u667a\u80fd\u4f53\u62e5\u6709\u8bb0\u5fc6\u80fd\u529b\u65f6\uff0c\u4ec5\u4ee5\u8d2a\u5a6a\u76ee\u6807\u8bad\u7ec3\u7684\u7b56\u7565\u786e\u5b9e\u5c55\u73b0\u51fa\u4fe1\u606f\u5bfb\u6c42\u7684\u63a2\u7d22\u884c\u4e3a\u3002\u6d88\u878d\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u5b9e\uff0c\u82e5\u73af\u5883\u7ed3\u6784\u6216\u667a\u80fd\u4f53\u8bb0\u5fc6\u7f3a\u5931\uff0c\u8fd9\u79cd\u6d8c\u73b0\u7684\u63a2\u7d22\u884c\u4e3a\u4fbf\u4f1a\u6d88\u5931\u3002\u7136\u800c\uff0c\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u79fb\u9664\u957f\u65f6\u5e8f\u4fe1\u7528\u5206\u914d\u5e76\u4e0d\u603b\u662f\u80fd\u963b\u6b62\u63a2\u7d22\u7684\u6d8c\u73b0\uff0c\u8fd9\u88ab\u5f52\u56e0\u4e8e\u4e00\u79cd\u201c\u4f2a\u6c64\u666e\u68ee\u91c7\u6837\u201d\u6548\u5e94\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u6307\u51fa\uff0c\u5728\u6ee1\u8db3\u7279\u5b9a\u5148\u51b3\u6761\u4ef6\u65f6\uff0c\u63a2\u7d22\u4e0e\u5229\u7528\u5e76\u975e\u5fc5\u987b\u88ab\u89c6\u4e3a\u76f8\u4e92\u72ec\u7acb\u7684\u76ee\u6807\uff0c\u5b83\u4eec\u53ef\u4ee5\u4ece\u7edf\u4e00\u7684\u56de\u62a5\u6700\u5927\u5316\u8fc7\u7a0b\u4e2d\u81ea\u7136\u6d8c\u73b0\u3002\u8fd9\u4e3a\u5143\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2508.01311", "pdf": "https://arxiv.org/pdf/2508.01311", "abs": "https://arxiv.org/abs/2508.01311", "authors": ["Haoquan Lu", "Hanzhe Liang", "Jie Zhang", "Chenxi Hu", "Jinbao Wang", "Can Gao"], "title": "C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with Learnable Advisor", "categories": ["cs.CV", "cs.LG"], "comment": "We have provided the code for C3D-AD with checkpoints and BASELINE at\n  this link: https://github.com/hzzzzzhappy/CL3AD", "summary": "3D Anomaly Detection (AD) has shown great potential in detecting anomalies or\ndefects of high-precision industrial products. However, existing methods are\ntypically trained in a class-specific manner and also lack the capability of\nlearning from emerging classes. In this study, we proposed a continual learning\nframework named Continual 3D Anomaly Detection (C3D-AD), which can not only\nlearn generalized representations for multi-class point clouds but also handle\nnew classes emerging over time.Specifically, in the feature extraction module,\nto extract generalized local features from diverse product types of different\ntasks efficiently, Kernel Attention with random feature Layer (KAL) is\nintroduced, which normalizes the feature space. Then, to reconstruct data\ncorrectly and continually, an efficient Kernel Attention with learnable Advisor\n(KAA) mechanism is proposed, which learns the information from new categories\nwhile discarding redundant old information within both the encoder and decoder.\nFinally, to keep the representation consistency over tasks, a Reconstruction\nwith Parameter Perturbation (RPP) module is proposed by designing a\nrepresentation rehearsal loss function, which ensures that the model remembers\nprevious category information and returns category-adaptive\nrepresentation.Extensive experiments on three public datasets demonstrate the\neffectiveness of the proposed method, achieving an average performance of\n66.4%, 83.1%, and 63.4% AUROC on Real3D-AD, Anomaly-ShapeNet, and MulSen-AD,\nrespectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86C3D-AD\uff0c\u4e00\u4e2a\u9488\u5bf93D\u5f02\u5e38\u68c0\u6d4b\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u591a\u7c7b\u522b\u548c\u65b0\u5174\u7c7b\u522b\u65f6\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5f15\u5165KAL\u3001KAA\u548cRPP\u6a21\u5757\u5b9e\u73b0\u901a\u7528\u7279\u5f81\u5b66\u4e60\u548c\u8de8\u4efb\u52a1\u8868\u793a\u4e00\u81f4\u6027\uff0c\u5e76\u53d6\u5f97\u826f\u597d\u5b9e\u9a8c\u6548\u679c\u3002", "motivation": "\u73b0\u67093D\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4ee5\u7c7b\u522b\u7279\u5b9a\u65b9\u5f0f\u8bad\u7ec3\uff0c\u4e14\u7f3a\u4e4f\u4ece\u65b0\u5174\u7c7b\u522b\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u591a\u7c7b\u522b\u548c\u968f\u65f6\u95f4\u51fa\u73b0\u7684\u65b0\u7c7b\u522b\u6570\u636e\u3002", "method": "\u63d0\u51fa Continual 3D Anomaly Detection (C3D-AD) \u6301\u7eed\u5b66\u4e60\u6846\u67b6\u3002\u5177\u4f53\u5305\u62ec\uff1a1) \u5728\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u5f15\u5165 Kernel Attention with random feature Layer (KAL) \u4ee5\u63d0\u53d6\u901a\u7528\u5c40\u90e8\u7279\u5f81\u5e76\u5f52\u4e00\u5316\u7279\u5f81\u7a7a\u95f4\u30022) \u63d0\u51fa\u9ad8\u6548\u7684 Kernel Attention with learnable Advisor (KAA) \u673a\u5236\uff0c\u7528\u4e8e\u6301\u7eed\u6b63\u786e\u5730\u91cd\u5efa\u6570\u636e\uff0c\u5b66\u4e60\u65b0\u7c7b\u522b\u4fe1\u606f\u5e76\u4e22\u5f03\u5197\u4f59\u65e7\u4fe1\u606f\u30023) \u8bbe\u8ba1 Reconstruction with Parameter Perturbation (RPP) \u6a21\u5757\uff0c\u901a\u8fc7\u8868\u793a\u91cd\u6f14\u635f\u5931\u51fd\u6570\u4fdd\u6301\u8de8\u4efb\u52a1\u7684\u8868\u793a\u4e00\u81f4\u6027\uff0c\u786e\u4fdd\u6a21\u578b\u8bb0\u5fc6\u65e7\u7c7b\u522b\u4fe1\u606f\u5e76\u751f\u6210\u7c7b\u522b\u81ea\u9002\u5e94\u8868\u793a\u3002", "result": "\u5728Real3D-AD\u3001Anomaly-ShapeNet\u548cMulSen-AD\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u6709\u6548\uff0cAUROC\u5e73\u5747\u6027\u80fd\u5206\u522b\u8fbe\u523066.4%\u300183.1%\u548c63.4%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684C3D-AD\u6846\u67b6\u80fd\u591f\u6709\u6548\u5e94\u5bf93D\u5f02\u5e38\u68c0\u6d4b\u4e2d\u591a\u7c7b\u522b\u548c\u65b0\u5174\u7c7b\u522b\u7684\u6301\u7eed\u5b66\u4e60\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
