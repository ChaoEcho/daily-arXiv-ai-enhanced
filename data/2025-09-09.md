<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 15]
- [cs.CV](#cs.CV) [Total: 12]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.LG](#cs.LG) [Total: 11]
- [cs.NI](#cs.NI) [Total: 12]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training](https://arxiv.org/abs/2509.05359)
*Yanis Labrak,Richard Dufour,Mickaël Rouvier*

Main category: cs.CL

TL;DR: 本文探讨SLM中离散单元表示，研究持续预训练期间优化语音建模的策略，分析模型架构、数据表示和训练鲁棒性的影响，并强调离散策略需与模型容量及领域匹配。


<details>
  <summary>Details</summary>
Motivation: 旨在优化SLM在持续预训练阶段的语音建模，通过离散单元表示将现有预训练语言模型适配到语音模态。

Method: 系统性地考察模型架构、数据表示和训练鲁棒性。通过实验分析语音编码器、聚类粒度，检查聚类分布和音素对齐，并探讨聚类数据选择的影响。

Result: 发现最优离散化策略随模型容量而异；通过离散词汇揭示了语言学和副语言学模式；强调聚类数据选择与目标应用之间的领域匹配对模型鲁棒性的重要性。

Conclusion: 优化SLM的离散单元表示和持续预训练效果，需综合考虑模型架构、数据表示、训练鲁棒性，并确保离散化训练与目标应用的领域匹配。

Abstract: This paper investigates discrete unit representations in Speech Language
Models (SLMs), focusing on optimizing speech modeling during continual
pre-training. In this paper, we systematically examine how model architecture,
data representation, and training robustness influence the pre-training stage
in which we adapt existing pre-trained language models to the speech modality.
Our experiments highlight the role of speech encoders and clustering
granularity across different model scales, showing how optimal discretization
strategies vary with model capacity. By examining cluster distribution and
phonemic alignments, we investigate the effective use of discrete vocabulary,
uncovering both linguistic and paralinguistic patterns. Additionally, we
explore the impact of clustering data selection on model robustness,
highlighting the importance of domain matching between discretization training
and target applications.

</details>


### [2] [Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection](https://arxiv.org/abs/2509.05360)
*Jerry Li,Evangelos Papalexakis*

Main category: cs.CL

TL;DR: 大语言模型存在幻觉问题，现有检测方法语义深度不足。本文提出一种基于N-Gram频率张量分解和MLP分类器的新方法，能有效检测LLM幻觉，优于传统基线并媲美SOTA LLM判别器。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的幻觉问题限制了其生成信息的一致性和真实性。现有幻觉检测方法，如基于ROUGE、BERTScore等的指标，往往缺乏足够的语义深度来有效区分事实和幻觉内容。

Method: 本文提出一种受ROUGE启发的新方法，通过从LLM生成的文本构建N-Gram频率张量，捕捉更丰富的语义共现模式。然后对张量应用分解方法提取奇异值作为特征，训练一个多层感知机（MLP）二分类器来检测幻觉。

Result: 该方法在HaluEval数据集上进行了评估，与传统基线相比表现出显著改进，并且与最先进的LLM判别器相比也具有竞争力的性能。

Conclusion: 通过构建和分解N-Gram频率张量并结合MLP分类器，本文提出了一种有效检测LLM幻觉的新方法，解决了现有方法语义深度不足的问题，并显著提升了幻觉检测的准确性。

Abstract: Large Language Models (LLMs) have demonstrated effectiveness across a wide
variety of tasks involving natural language, however, a fundamental problem of
hallucinations still plagues these models, limiting their trustworthiness in
generating consistent, truthful information. Detecting hallucinations has
quickly become an important topic, with various methods such as uncertainty
estimation, LLM Judges, retrieval augmented generation (RAG), and consistency
checks showing promise. Many of these methods build upon foundational metrics,
such as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth
necessary to detect hallucinations effectively. In this work, we propose a
novel approach inspired by ROUGE that constructs an N-Gram frequency tensor
from LLM-generated text. This tensor captures richer semantic structure by
encoding co-occurrence patterns, enabling better differentiation between
factual and hallucinated content. We demonstrate this by applying tensor
decomposition methods to extract singular values from each mode and use these
as input features to train a multi-layer perceptron (MLP) binary classifier for
hallucinations. Our method is evaluated on the HaluEval dataset and
demonstrates significant improvements over traditional baselines, as well as
competitive performance against state-of-the-art LLM judges.

</details>


### [3] [A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs](https://arxiv.org/abs/2509.05385)
*Jiacheng Wei,Faguo Wu,Xiao Zhang*

Main category: cs.CL

TL;DR: 为解决大型语言模型推理时无法持续学习的问题，本文提出了SAGE框架。该框架通过分解复杂任务、实时错误检测、异常样本聚类及动态参数更新，使其能够在测试时自适应学习，并在原子推理子任务上展现出卓越的准确性、鲁棒性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在推理时无法持续地适应新数据并从中学习，这限制了其动态性和灵活性。

Method: 将复杂推理任务分解为原子子任务，并引入SAGE框架。SAGE是一个触发器引导的动态微调框架，能够在推理时进行自适应更新。它包含三个核心组件：1) Trigger模块，通过多项评估指标实时检测推理失败；2) Trigger Buffer模块，利用HDBSCAN进行流式聚类处理异常样本，并进行稳定性检查和相似性合并；3) Lora Store模块，通过适配器池动态优化参数更新以保留知识。

Result: 评估结果表明，SAGE框架通过在测试时的动态知识更新，在原子推理子任务上展示了卓越的准确性、鲁棒性和稳定性。

Conclusion: SAGE框架通过实现推理时的动态知识更新，成功克服了大型语言模型无法持续适应和学习新数据的局限性，显著提升了其在原子推理任务上的性能表现。

Abstract: Large language models are unable to continuously adapt and learn from new
data during reasoning at inference time. To address this limitation, we propose
that complex reasoning tasks be decomposed into atomic subtasks and introduce
SAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive
updates during reasoning at inference time. SAGE consists of three key
components: (1) a Trigger module that detects reasoning failures through
multiple evaluation metrics in real time; (2) a Trigger Buffer module that
clusters anomaly samples using a streaming clustering process with HDBSCAN,
followed by stability checks and similarity-based merging; and (3) a Lora Store
module that dynamically optimizes parameter updates with an adapter pool for
knowledge retention. Evaluation results show that SAGE demonstrates excellent
accuracy, robustness, and stability on the atomic reasoning subtask through
dynamic knowledge updating during test time.

</details>


### [4] [Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate](https://arxiv.org/abs/2509.05396)
*Andrea Wynn,Harsh Satija,Gillian Hadfield*

Main category: cs.CL

TL;DR: 研究发现，多智能体辩论，特别是在模型能力多样化时，可能导致AI准确性下降，原因在于智能体倾向于达成一致而非纠正错误的推理。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注同质智能体群体间的辩论，本文旨在探究模型能力多样性如何影响多智能体交互的动态和结果，挑战了辩论总是能提升AI推理能力的假设。

Method: 通过一系列实验，观察和分析了具有不同模型能力的多智能体在辩论过程中的动态表现和结果变化。

Result: 实验表明，辩论可能导致准确性随时间下降，即使在强能力模型数量占优的情况下也是如此。分析揭示，模型常从正确答案转向错误答案，倾向于达成一致而非挑战错误的推理。

Conclusion: 当智能体未被激励或未充分准备以抵制有说服力但错误的推理时，单纯应用多智能体辩论可能导致性能退化，这揭示了多智能体辩论中原因交换的关键失败模式。

Abstract: While multi-agent debate has been proposed as a promising strategy for
improving AI reasoning ability, we find that debate can sometimes be harmful
rather than helpful. The prior work has exclusively focused on debates within
homogeneous groups of agents, whereas we explore how diversity in model
capabilities influences the dynamics and outcomes of multi-agent interactions.
Through a series of experiments, we demonstrate that debate can lead to a
decrease in accuracy over time -- even in settings where stronger (i.e., more
capable) models outnumber their weaker counterparts. Our analysis reveals that
models frequently shift from correct to incorrect answers in response to peer
reasoning, favoring agreement over challenging flawed reasoning. These results
highlight important failure modes in the exchange of reasons during multi-agent
debate, suggesting that naive applications of debate may cause performance
degradation when agents are neither incentivized nor adequately equipped to
resist persuasive but incorrect reasoning.

</details>


### [5] [No Translation Needed: Forecasting Quality from Fertility and Metadata](https://arxiv.org/abs/2509.05425)
*Jessica M. Lundin,Ada Zhang,David Adelani,Cody Carroll*

Main category: cs.CL

TL;DR: 研究表明，无需实际运行翻译系统，仅使用少量特征（词元生育率、词元计数和语言元数据）即可准确预测GPT-4o在203种语言上的翻译质量。


<details>
  <summary>Details</summary>
Motivation: 在不运行翻译系统本身的情况下，高效且准确地预测翻译质量，以提供多语言评估和质量估计的新见解。

Method: 使用词元生育率、词元计数以及语系、文字和地域等基本语言元数据作为特征。采用梯度提升模型（Gradient Boosting models）预测GPT-4o在FLORES-200基准测试中203种语言的ChrF分数。

Result: 模型表现良好，XX→英语翻译的$R^{2}=0.66$，英语→XX翻译的$R^{2}=0.72$。特征重要性分析显示，在翻译成英语时，类型学因素占主导；而在翻译成其他多样目标语言时，生育率发挥更大作用。

Conclusion: 翻译质量可由词元级别生育率和更广泛的语言类型学共同塑造，且仅通过少量特征即可有效预测。这些发现为多语言评估和质量估计提供了新视角。

Abstract: We show that translation quality can be predicted with surprising accuracy
\textit{without ever running the translation system itself}. Using only a
handful of features, token fertility ratios, token counts, and basic linguistic
metadata (language family, script, and region), we can forecast ChrF scores for
GPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient
boosting models achieve favorable performance ($R^{2}=0.66$ for
XX$\rightarrow$English and $R^{2}=0.72$ for English$\rightarrow$XX). Feature
importance analyses reveal that typological factors dominate predictions into
English, while fertility plays a larger role for translations into diverse
target languages. These findings suggest that translation quality is shaped by
both token-level fertility and broader linguistic typology, offering new
insights for multilingual evaluation and quality estimation.

</details>


### [6] [Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too](https://arxiv.org/abs/2509.05440)
*Logan Lawrence,Ashton Williamson,Alexander Shelton*

Main category: cs.CL

TL;DR: 本研究提出了一种直接评分方法，通过合成摘要使大型语言模型能为单个自由形式内容分配绝对分数，其性能与先进的配对评估器相当。


<details>
  <summary>Details</summary>
Motivation: 现有配对评估方法在评估大型语言模型作为自动评分器时表现良好，但无法为单个文本分配绝对分数，而这对于需要设定阈值的应用场景至关重要。

Method: 提出了一种直接评分方法，该方法在测试时利用合成摘要作为配对机器排名。

Result: 该方法在SummEval、TopicalChat和HANNA等元评估基准测试中，轴平均样本级相关性方面与最先进的配对评估器表现相当（分别有+0.03，-0.03，+0.05的变化），并发布了合成语境摘要数据。

Conclusion: 所提出的直接评分方法有效解决了现有配对评估器无法提供绝对分数的问题，并在性能上与顶尖方法相当，有助于大型语言模型作为自动评估器的实际应用。

Abstract: As large-language models have been increasingly used as automatic raters for
evaluating free-form content, including document summarization, dialog, and
story generation, work has been dedicated to evaluating such models by
measuring their correlations with human judgment. For \textit{sample-level}
performance, methods which operate by using pairwise comparisons between
machine-generated text perform well but often lack the ability to assign
absolute scores to individual summaries, an ability crucial for use cases that
require thresholding. In this work, we propose a direct-scoring method which
uses synthetic summaries to act as pairwise machine rankings at test time. We
show that our method performs comparably to state-of-the-art pairwise
evaluators in terms of axis-averaged sample-level correlations on the SummEval
(\textbf{+0.03}), TopicalChat (\textbf{-0.03}), and HANNA (\textbf{+0.05})
meta-evaluation benchmarks, and release the synthetic in-context summaries as
data to facilitate future work.

</details>


### [7] [From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics](https://arxiv.org/abs/2509.05484)
*Hajar Sakai,Yi-En Tseng,Mohammadsadegh Mikaeili,Joshua Bosire,Franziska Jovin*

Main category: cs.CL

TL;DR: 本研究提出了一个多阶段的LLM框架，用于识别医院呼叫中心员工消息的主题并进行分类，通过评估多种LLM类型，最佳模型o3表现优异，并支持数据安全和可视化决策，以提升医疗服务质量。


<details>
  <summary>Details</summary>
Motivation: 医院呼叫中心产生大量员工消息文本数据，但传统监督学习方法需要大量标注和训练，效率低下。大型语言模型（LLMs）为医疗分析提供了计算效率更高的新范式，因此本研究旨在利用LLMs处理这些数据以获取可操作的洞察。

Method: 本研究提出一个多阶段的基于LLM的框架，用于识别员工消息主题并按原因进行多类别分类。该方法评估了包括推理型、通用型和轻量级在内的多种LLM模型。框架中融入了数据安全措施和HIPAA合规性要求，并将处理后的LLM输出集成到可视化决策支持工具中。

Result: 研究结果显示，最佳表现模型是o3，其加权F1-score达到78.4%，准确率达到79.2%。紧随其后的是gpt-5，加权F1-score为75.3%，准确率为76.2%。

Conclusion: 该方法能够更有效地利用收集到的员工消息数据，有助于识别导航员的培训机会，从而支持改善患者体验和提升护理质量。

Abstract: Hospital call centers serve as the primary contact point for patients within
a hospital system. They also generate substantial volumes of staff messages as
navigators process patient requests and communicate with the hospital offices
following the established protocol restrictions and guidelines. This
continuously accumulated large amount of text data can be mined and processed
to retrieve insights; however, traditional supervised learning approaches
require annotated data, extensive training, and model tuning. Large Language
Models (LLMs) offer a paradigm shift toward more computationally efficient
methodologies for healthcare analytics. This paper presents a multi-stage
LLM-based framework that identifies staff message topics and classifies
messages by their reasons in a multi-class fashion. In the process, multiple
LLM types, including reasoning, general-purpose, and lightweight models, were
evaluated. The best-performing model was o3, achieving 78.4% weighted F1-score
and 79.2% accuracy, followed closely by gpt-5 (75.3% Weighted F1-score and
76.2% accuracy). The proposed methodology incorporates data security measures
and HIPAA compliance requirements essential for healthcare environments. The
processed LLM outputs are integrated into a visualization decision support tool
that transforms the staff messages into actionable insights accessible to
healthcare professionals. This approach enables more efficient utilization of
the collected staff messaging data, identifies navigator training
opportunities, and supports improved patient experience and care quality.

</details>


### [8] [The Token Tax: Systematic Bias in Multilingual Tokenization](https://arxiv.org/abs/2509.05486)
*Jessica M. Lundin,Ada Zhang,Nihal Karim,Hamza Louzan,Victor Wei,David Adelani,Cody Carroll*

Main category: cs.CL

TL;DR: 标记化效率低下对形态复杂、低资源语言造成结构性劣势，表现为计算成本增加和准确性降低。研究发现标记丰度（每词标记数）是准确性的可靠预测因子，且推理型LLM表现更佳，同时揭示了标记膨胀带来的经济负担。


<details>
  <summary>Details</summary>
Motivation: 形态复杂、低资源语言因标记化效率低下而面临结构性劣势，导致计算资源消耗增加和准确性下降。

Method: 在AfriMMLU数据集（包含9,000个多项选择题，涵盖5个学科和16种非洲语言）上评估了10个大型语言模型（LLMs）。

Result: ['标记丰度（每词标记数）能可靠预测准确性；更高的标记丰度与所有模型和学科的较低准确性持续相关。', '推理型模型（如DeepSeek, o1）在AfriMMLU数据集中，无论高资源还是低资源语言，其表现均优于非推理型模型，缩小了先前观察到的准确性差距。', '将标记膨胀转化为经济成本，标记数量翻倍将导致训练成本和时间增加四倍，突显了许多语言面临的“标记税”。']

Conclusion: 研究结果支持开发形态感知的标记化方法、制定公平的定价策略以及构建多语言基准测试，以实现公平的自然语言处理（NLP）。

Abstract: Tokenization inefficiency imposes structural disadvantages on morphologically
complex, low-resource languages, inflating compute resources and depressing
accuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA
items; 5 subjects; 16 African languages) and show that fertility (tokens/word)
reliably predicts accuracy. Higher fertility consistently predicts lower
accuracy across all models and subjects. We further find that reasoning models
(DeepSeek, o1) consistently outperform non-reasoning peers across high and low
resource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in
prior generations. Finally, translating token inflation to economics, a
doubling in tokens results in quadrupled training cost and time, underscoring
the token tax faced by many languages. These results motivate morphologically
aware tokenization, fair pricing, and multilingual benchmarks for equitable
natural language processing (NLP).

</details>


### [9] [Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2509.05505)
*Mansi Garg,Lee-Chi Wang,Bhavesh Ghanchi,Sanjana Dumpala,Shreyash Kakde,Yen Chih Chen*

Main category: cs.CL

TL;DR: 本文提出一个基于检索增强生成（RAG）架构的生物医学文献问答系统，利用MiniLM+FAISS进行检索，并由QLoRA优化的Mistral-7B-v0.3生成答案。该系统整合多源医学数据，显著提高了医学信息的事实一致性和语义相关性。


<details>
  <summary>Details</summary>
Motivation: 解决传统健康搜索引擎的不足，以及公众获取生物医学研究成果滞后的问题，旨在提高获取准确、循证医学信息的便捷性。

Method: 开发了一个基于RAG架构的生物医学文献问答系统，整合了PubMed文章、精选问答数据集和医学百科全书等多样化信息源。检索流程采用基于MiniLM的语义嵌入和FAISS向量搜索。答案生成由使用QLoRA优化微调的Mistral-7B-v0.3语言模型完成。

Result: 该系统支持通用医学查询和特定领域任务。在乳腺癌文献评估中，与基线模型相比，其事实一致性和语义相关性（通过BERTScore F1衡量）均有显著提升。

Conclusion: RAG增强型语言模型具有巨大的潜力，能够弥合复杂生物医学文献与可访问的公共健康知识之间的鸿沟，为未来多语言适应、隐私保护推理和个性化医疗AI系统奠定基础。

Abstract: This work presents a Biomedical Literature Question Answering (Q&A) system
based on a Retrieval-Augmented Generation (RAG) architecture, designed to
improve access to accurate, evidence-based medical information. Addressing the
shortcomings of conventional health search engines and the lag in public access
to biomedical research, the system integrates diverse sources, including PubMed
articles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant
information and generate concise, context-aware responses. The retrieval
pipeline uses MiniLM-based semantic embeddings and FAISS vector search, while
answer generation is performed by a fine-tuned Mistral-7B-v0.3 language model
optimized using QLoRA for efficient, low-resource training. The system supports
both general medical queries and domain-specific tasks, with a focused
evaluation on breast cancer literature demonstrating the value of
domain-aligned retrieval. Empirical results, measured using BERTScore (F1),
show substantial improvements in factual consistency and semantic relevance
compared to baseline models. The findings underscore the potential of
RAG-enhanced language models to bridge the gap between complex biomedical
literature and accessible public health knowledge, paving the way for future
work on multilingual adaptation, privacy-preserving inference, and personalized
medical AI systems.

</details>


### [10] [Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study](https://arxiv.org/abs/2509.05553)
*Serge Lionel Nikiema,Jordan Samhi,Micheline Bénédicte Moumoula,Albérick Euraste Djiré,Abdoul Kader Kaboré,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: 本研究提出双向推理作为评估大语言模型（LLMs）真正理解而非仅识别模式的标准。发现现有LLMs存在“认知专化”问题，并开发了对比微调（CFT）方法，成功使模型实现双向推理能力，提升了深度理解。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能领域的一个基本问题：大型语言模型是真正理解概念还是仅仅识别模式。研究者认为，真正的理解应自然包含可逆性，即无需反向训练即可进行双向转换。

Method: 1. 提出双向推理作为衡量模型真正理解的标准，即模型应能在未明确反向训练的情况下，双向应用转换。2. 开发了对比微调（CFT）方法，通过使用三类例子训练模型：保持语义的正向例子、不同语义的负向例子以及正向混淆例子。旨在培养深度理解并自然发展反向能力。

Result: 1. 发现当前LLMs存在“认知专化”问题：在正向任务上进行微调后，正向性能提高，但双向推理能力显著下降。2. 实验证明，对比微调（CFT）成功实现了双向推理，使模型在保持正向任务能力的同时，也具备了强大的反向性能。

Conclusion: 双向推理既可以作为评估AI系统真正理解的理论框架，也是开发更强大AI系统的实用训练方法。

Abstract: This research addresses a fundamental question in AI: whether large language
models truly understand concepts or simply recognize patterns. The authors
propose bidirectional reasoning,the ability to apply transformations in both
directions without being explicitly trained on the reverse direction, as a test
for genuine understanding. They argue that true comprehension should naturally
allow reversibility. For example, a model that can change a variable name like
userIndex to i should also be able to infer that i represents a user index
without reverse training. The researchers tested current language models and
discovered what they term cognitive specialization: when models are fine-tuned
on forward tasks, their performance on those tasks improves, but their ability
to reason bidirectionally becomes significantly worse. To address this issue,
they developed Contrastive Fine-Tuning (CFT), which trains models using three
types of examples: positive examples that maintain semantic meaning, negative
examples with different semantics, and forward-direction obfuscation examples.
This approach aims to develop deeper understanding rather than surface-level
pattern recognition and allows reverse capabilities to develop naturally
without explicit reverse training. Their experiments demonstrated that CFT
successfully achieved bidirectional reasoning, enabling strong reverse
performance while maintaining forward task capabilities. The authors conclude
that bidirectional reasoning serves both as a theoretical framework for
assessing genuine understanding and as a practical training approach for
developing more capable AI systems.

</details>


### [11] [Ad hoc conventions generalize to new referents](https://arxiv.org/abs/2509.05566)
*Anya Ji,Claire Augusta Bergey,Ron Eliav,Yoav Artzi,Robert D. Hawkins*

Main category: cs.CL

TL;DR: 研究发现，人们在交流新事物时，会形成超越任意标签的更广泛概念协调，其描述惯例能够泛化到未讨论的参照物，且泛化程度随视觉相似度非线性衰减。


<details>
  <summary>Details</summary>
Motivation: 探究人们如何讨论从未提及的事物。存在两种对立观点：一是新命名系统建立任意的、不可泛化的链接；二是形成共同描述方式涉及更广泛的概念对齐，并可泛化到新指称物。本研究旨在检验这两种观点。

Method: 进行了一项双人沟通研究（N=302），利用KiloGram数据集中1000多张抽象七巧板图像。参与者配对后，通过重复沟通协调一组图像的指称惯例，然后测量他们对未讨论图像的描述对齐程度。

Result: 发现强烈的泛化证据：与预测试标签相比，合作者在描述未讨论图像时表现出更高的对齐度。泛化能力随视觉相似度非线性衰减（符合谢泼德定律），且在图像可命名性水平上保持鲁棒性。

Conclusion: 这些发现表明，临时形成的约定并非任意标签，而是反映了真实的概念协调。这对指称理论和设计更具适应性的语言智能体具有重要启示。

Abstract: How do people talk about things they've never talked about before? One view
suggests that a new shared naming system establishes an arbitrary link to a
specific target, like proper names that cannot extend beyond their bearers. An
alternative view proposes that forming a shared way of describing objects
involves broader conceptual alignment, reshaping each individual's semantic
space in ways that should generalize to new referents. We test these competing
accounts in a dyadic communication study (N=302) leveraging the
recently-released KiloGram dataset containing over 1,000 abstract tangram
images. After pairs of participants coordinated on referential conventions for
one set of images through repeated communication, we measured the extent to
which their descriptions aligned for undiscussed images. We found strong
evidence for generalization: partners showed increased alignment relative to
their pre-test labels. Generalization also decayed nonlinearly with visual
similarity (consistent with Shepard's law) and was robust across levels of the
images' nameability. These findings suggest that ad hoc conventions are not
arbitrary labels but reflect genuine conceptual coordination, with implications
for theories of reference and the design of more adaptive language agents.

</details>


### [12] [Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation](https://arxiv.org/abs/2509.05602)
*Hongyan Xie,Yitong Yao,Yikun Ban,Zixuan Huang,Deqing Wang,Zhenhe Wu,Haoxiang Su,Chao Wang,Shuangyong Song,Xuelong Li*

Main category: cs.CL

TL;DR: 本文提出CoPeD方法，通过引入正确性感知任务设置和加权损失，解决LLM生成的CoT数据中存在的噪声问题，从而提高SLM的推理质量，并在多项基准测试中表现出有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）推理能力强但部署成本高。小型语言模型（SLM）通过LLM生成的思维链（CoT）数据进行微调以模仿其能力。然而，这些CoT数据可能包含噪声推理过程，无法有效支持答案或提供额外信息，导致SLM捕捉虚假关联并损害推理质量。

Method: 我们提出CoT正确性感知蒸馏（CoPeD）方法，从任务设置和数据利用两方面提升学生模型的推理质量。首先，引入正确性感知任务设置，鼓励学生模型基于正确推理过程预测答案，并在错误时进行修正，以提高推理忠实度并从错误中学习。其次，提出正确性感知加权损失，根据推理过程和答案的组合损失动态调整每个训练实例的贡献，促使模型更关注推理过程能强有力支持正确答案的样本。

Result: 实验结果表明，CoPeD在分布内（IND）和分布外（OOD）的基准推理数据集上均有效。

Conclusion: CoPeD通过正确性感知任务设置和正确性感知加权损失，有效解决了LLM生成CoT数据中的噪声问题，显著提高了小型语言模型的推理质量，并在多种推理任务中展现出优越性能。

Abstract: Large language models (LLMs) excel at reasoning tasks but are expensive to
deploy. Thus small language models (SLMs) are fine-tuned on CoT data generated
by LLMs to copy LLMs' abilities. However, these CoT data may include noisy
rationales that either fail to substantiate the answers or contribute no
additional information to support answer prediction, which leads SLMs to
capture spurious correlations between questions and answers and compromise the
quality of reasoning. In this work, we propose Chain-of-Thought Correctness
Perception Distillation (CoPeD), which aims to improve the reasoning quality of
the student model from the perspectives of task setting and data utilization.
Firstly, we introduce a correctness-aware task setting that encourages the
student model to predict answers based on correct rationales and revise them
when they are incorrect. This setting improves the faithfulness of reasoning
and allows the model to learn from its mistakes. Then, we propose a
Correctness-Aware Weighted loss, which dynamically adjusts the contribution of
each training instance based on the combined loss of the rationale and the
answer. This strategy encourages the model to focus more on samples where the
rationale offers stronger support for the correct answer. Experiments have
shown that CoPeD is effective on both in-distribution (IND) and
out-of-distribution (OOD) benchmark reasoning datasets.

</details>


### [13] [Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation](https://arxiv.org/abs/2509.05605)
*Qiyuan Chen,Hongsen Huang,Qian Shao,Jiahe Chen,Jintai Chen,Hongxia Xu,Renjie Hua,Ren Chuan,Jian Wu*

Main category: cs.CL

TL;DR: Icon$^{2}$通过利用大语言模型（LLM）的内在表示空间，高效构建高质量偏好数据集，显著提升了模型与人类偏好的对齐效果并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM偏好数据集构建方法存在缺陷：依赖预收集指令导致与目标模型分布不匹配，以及多响应采样带来巨大的计算开销。

Method: 本文提出Icon$^{2}$，一种通过利用LLM表示空间的内在调节机制来高效构建定制化偏好数据集的方法。具体而言，它首先提取层级方向向量以编码复杂的人类偏好，然后使用这些向量过滤自合成指令。在解码阶段，采用双向内在控制来引导token表示，从而精确生成具有明确对齐差异的响应对。

Result: 实验结果表明，Icon$^{2}$显著提升了对齐效果和效率。在Llama3-8B和Qwen2-7B模型上，AlpacaEval 2.0和Arena-Hard上的平均胜率分别提高了13.89%和13.45%，同时计算成本降低了高达48.1%。

Conclusion: Icon$^{2}$提供了一种高效且定制化的偏好数据集构建新范式，有效解决了传统方法的局限性，显著提升了LLM与人类偏好的对齐能力，并大幅降低了计算成本。

Abstract: Large Language Models (LLMs) require high quality preference datasets to
align with human preferences. However, conventional methods for constructing
such datasets face significant challenges: reliance on pre-collected
instructions often leads to distribution mismatches with target models, while
the need for sampling multiple stochastic responses introduces substantial
computational overhead. In this work, we explore a paradigm shift by leveraging
inherent regulation of LLMs' representation space for efficient and tailored
preference dataset construction, named Icon$^{2}$. Specifically, it first
extracts layer-wise direction vectors to encode sophisticated human preferences
and then uses these vectors to filter self-synthesized instructions based on
their inherent consistency. During decoding, bidirectional inherent control is
applied to steer token representations, enabling the precise generation of
response pairs with clear alignment distinctions. Experimental results
demonstrate significant improvements in both alignment and efficiency.
Llama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on
AlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by
up to 48.1%.

</details>


### [14] [Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents](https://arxiv.org/abs/2509.05607)
*Qiyuan Chen,Jiahe Chen,Hongsen Huang,Qian Shao,Jintai Chen,Renjie Hua,Hongxia Xu,Ruijia Wu,Ren Chuan,Jian Wu*

Main category: cs.CL

TL;DR: 本文提出了一套端到端生成式搜索引擎优化（GSEO）框架，包含一个内容中心基准测试CC-GSEO-Bench和一个多智能体系统，旨在衡量并优化内容对生成式搜索答案的影响。


<details>
  <summary>Details</summary>
Motivation: 随着搜索范式从传统排名转向生成式搜索，传统SEO指标已过时，迫切需要理解、衡量和优化内容在合成答案中的影响力。

Method: 研究提出了一个全面的GSEO端到端框架。主要贡献包括：1) 构建了大规模内容中心基准测试CC-GSEO-Bench，并提出多维评估框架以量化内容影响力的深层语义效应。2) 设计了新颖的多智能体系统，通过协作性的“分析-修订-评估”工作流，自动化地进行内容策略优化。

Result: 通过该框架进行的实证分析揭示了内容影响力动态的新颖见解，为内容创作者提供了可操作的策略。

Conclusion: 该研究为未来的GSEO研究奠定了原则性基础，并为内容创作者提供了实用策略。

Abstract: The paradigm shift from traditional ranked-based search to Generative Search
Engines has rendered conventional SEO metrics obsolete, creating an urgent need
to understand, measure, and optimize for content influence on synthesized
answers. This paper introduces a comprehensive, end-to-end framework for
Generative Search Engine Optimization (GSEO) to address this challenge. We make
two primary contributions. First, we construct CC-GSEO-Bench, a large-scale,
content-centric benchmark, and propose a multi-dimensional evaluation framework
that systematically quantifies influence, moving beyond surface-level
attribution to assess substantive semantic impact. Second, we design a novel
multi-agent system that operationalizes this framework, automating the
strategic refinement of content through a collaborative analyze-revise-evaluate
workflow. Our empirical analysis using this framework reveals novel insights
into the dynamics of content influence, offering actionable strategies for
creators and establishing a principled foundation for future GSEO research.

</details>


### [15] [New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR](https://arxiv.org/abs/2509.05609)
*Xugang Lu,Peng Shen,Yu Tsao,Hisashi Kawai*

Main category: cs.CL

TL;DR: 本文提出一种基于非平衡最优传输的对齐模型，将声学与语言对齐视为检测问题，以处理ASR中不对称和噪声引起的对齐挑战，有效提升了ASR性能。


<details>
  <summary>Details</summary>
Motivation: 在ASR中，声学与语言表征的对齐是一个核心挑战，尤其是在知识迁移的预训练模型中。这种对齐具有结构性不对称（多对一、一对多）和不平衡匹配条件（如背景噪声或静音帧没有语言对应），现有方法难以有效处理。

Method: 研究将对齐和匹配视为一个检测问题。基于此，提出了一种非平衡最优传输（unbalanced optimal transport）的对齐模型，该模型显式处理分布不匹配和结构不对称，并采用软性及部分匹配策略。它确保每个语言符号至少在一个声学观测中得到验证，并允许声学单元到语言单元的灵活概率映射。该模型在一个基于CTC的ASR系统上，结合预训练语言模型进行知识迁移进行了评估。

Result: 实验结果表明，该方法能够灵活控制匹配程度，从而有效提高了ASR的性能。

Conclusion: 通过将对齐视为检测问题并提出非平衡最优传输模型，本研究成功处理了ASR中声学与语言表征对齐的挑战，有效应对了结构不对称和分布不匹配问题，进而提升了ASR性能。

Abstract: Aligning acoustic and linguistic representations is a central challenge to
bridge the pre-trained models in knowledge transfer for automatic speech
recognition (ASR). This alignment is inherently structured and asymmetric:
while multiple consecutive acoustic frames typically correspond to a single
linguistic token (many-to-one), certain acoustic transition regions may relate
to multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often
include frames with no linguistic counterpart, such as background noise or
silence may lead to imbalanced matching conditions. In this work, we take a new
insight to regard alignment and matching as a detection problem, where the goal
is to identify meaningful correspondences with high precision and recall
ensuring full coverage of linguistic tokens while flexibly handling redundant
or noisy acoustic frames in transferring linguistic knowledge for ASR. Based on
this new insight, we propose an unbalanced optimal transport-based alignment
model that explicitly handles distributional mismatch and structural
asymmetries with soft and partial matching between acoustic and linguistic
modalities. Our method ensures that every linguistic token is grounded in at
least one acoustic observation, while allowing for flexible, probabilistic
mappings from acoustic to linguistic units. We evaluate our proposed model with
experiments on an CTC-based ASR system with a pre-trained language model for
knowledge transfer. Experimental results demonstrate the effectiveness of our
approach in flexibly controlling degree of matching and hence to improve ASR
performance.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [16] [Label Smoothing++: Enhanced Label Regularization for Training Neural Networks](https://arxiv.org/abs/2509.05307)
*Sachin Chhabra,Hemanth Venkateswara,Baoxin Li*

Main category: cs.CV

TL;DR: 本文提出Label Smoothing++，一种新型标签正则化策略，通过为非目标类分配非零概率并考虑其类间关系，以减轻过自信和提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 使用独热编码训练神经网络常导致过自信和过拟合。标签平滑虽能改善泛化能力，但对所有非目标类赋予同等重要性，破坏了类间关系。

Method: 提出Label Smoothing++。该方法为非目标类分配非零概率，并考虑其类间关系。目标类标签固定，而非目标类的标签则由网络学习。

Result: 通过多数据集实验证明，Label Smoothing++能有效缓解过自信预测，促进类间关系的建立，并提升模型的泛化能力。

Conclusion: Label Smoothing++是一种有效的标签正则化训练策略，它通过在非目标类中考虑类间关系，解决了传统标签平滑的局限性，从而提升了模型的泛化能力并减轻了过自信。

Abstract: Training neural networks with one-hot target labels often results in
overconfidence and overfitting. Label smoothing addresses this issue by
perturbing the one-hot target labels by adding a uniform probability vector to
create a regularized label. Although label smoothing improves the network's
generalization ability, it assigns equal importance to all the non-target
classes, which destroys the inter-class relationships. In this paper, we
propose a novel label regularization training strategy called Label
Smoothing++, which assigns non-zero probabilities to non-target classes and
accounts for their inter-class relationships. Our approach uses a fixed label
for the target class while enabling the network to learn the labels associated
with non-target classes. Through extensive experiments on multiple datasets, we
demonstrate how Label Smoothing++ mitigates overconfident predictions while
promoting inter-class relationships and generalization capabilities.

</details>


### [17] [VILOD: A Visual Interactive Labeling Tool for Object Detection](https://arxiv.org/abs/2509.05317)
*Isac Holm*

Main category: cs.CV

TL;DR: 针对目标检测（OD）标注数据集耗时耗力且传统主动学习（AL）透明度低等问题，本研究开发了可视化交互式标注工具VILOD。VILOD通过可视化分析增强人机协作，使标注流程更透明、可控，并在性能上与自动化基线相当。


<details>
  <summary>Details</summary>
Motivation: 深度学习目标检测（OD）面临大规模、准确标注数据集获取困难且昂贵的挑战。现有主动学习（AL）方法虽然能减少标注工作量，但缺乏透明度，限制了专家洞察力，且可能忽略未被查询策略覆盖的信息样本。为解决这些问题，将人类智能与机器学习生命周期结合的人机协作（HITL）方法，特别是结合可视化分析（VA），成为有效途径。

Method: 开发并研究了“VILOD：一个用于目标检测的可视化交互式标注工具”。VILOD集成了图像特征的t-SNE投影、不确定性热图和模型状态视图等组件。它使用户能够在迭代的HITL工作流中探索数据、解释模型状态和AL建议，并实施多样化的样本选择策略。

Result: 1. 实证研究表明，VILOD通过其交互式可视化，提高了模型状态和数据集特征的可解释性，从而促进了不同标注策略的实施。2. 在VILOD中采用的不同视觉引导标注策略，与自动化不确定性采样AL基线相比，实现了具有竞争力的目标检测性能轨迹。

Conclusion: 本工作贡献了一个新颖的工具（VILOD）和实证见解，使得目标检测标注的人机协作主动学习（HITL-AL）工作流更加透明、易于管理，并可能更有效。

Abstract: The advancement of Object Detection (OD) using Deep Learning (DL) is often
hindered by the significant challenge of acquiring large, accurately labeled
datasets, a process that is time-consuming and expensive. While techniques like
Active Learning (AL) can reduce annotation effort by intelligently querying
informative samples, they often lack transparency, limit the strategic insight
of human experts, and may overlook informative samples not aligned with an
employed query strategy. To mitigate these issues, Human-in-the-Loop (HITL)
approaches integrating human intelligence and intuition throughout the machine
learning life-cycle have gained traction. Leveraging Visual Analytics (VA),
effective interfaces can be created to facilitate this human-AI collaboration.
This thesis explores the intersection of these fields by developing and
investigating "VILOD: A Visual Interactive Labeling tool for Object Detection".
VILOD utilizes components such as a t-SNE projection of image features,
together with uncertainty heatmaps and model state views. Enabling users to
explore data, interpret model states, AL suggestions, and implement diverse
sample selection strategies within an iterative HITL workflow for OD. An
empirical investigation using comparative use cases demonstrated how VILOD,
through its interactive visualizations, facilitates the implementation of
distinct labeling strategies by making the model's state and dataset
characteristics more interpretable (RQ1). The study showed that different
visually-guided labeling strategies employed within VILOD result in competitive
OD performance trajectories compared to an automated uncertainty sampling AL
baseline (RQ2). This work contributes a novel tool and empirical insight into
making the HITL-AL workflow for OD annotation more transparent, manageable, and
potentially more effective.

</details>


### [18] [Context-Aware Knowledge Distillation with Adaptive Weighting for Image Classification](https://arxiv.org/abs/2509.05319)
*Zhengda Li*

Main category: cs.CV

TL;DR: 提出一种自适应知识蒸馏 (AKD) 框架，通过学习或动态计算平衡因子α，并引入上下文感知模块，以优化学生模型性能和收敛稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏中，固定不变的平衡因子α是次优的，因为硬标签和软标签监督之间的最佳权衡在训练过程中是变化的。

Method: 提出自适应知识蒸馏 (AKD) 框架。首先，将α设为可学习参数；其次，引入公式根据师生模型差距动态计算α；最后，引入上下文感知模块 (CAM) (使用MLP+Attention) 来自适应地重新加权类别级的教师输出。

Result: 在CIFAR-10数据集上，以ResNet-50为教师模型、ResNet-18为学生模型的实验表明，AKD方法相比固定权重的KD基线取得了更优的准确性，并具有更稳定的收敛。

Conclusion: 所提出的自适应知识蒸馏 (AKD) 框架通过动态调整平衡因子和引入上下文感知模块，有效提升了知识蒸馏的效果，实现了更高的准确性和更稳定的训练收敛。

Abstract: Knowledge distillation (KD) is a widely used technique to transfer knowledge
from a large teacher network to a smaller student model. Traditional KD uses a
fixed balancing factor alpha as a hyperparameter to combine the hard-label
cross-entropy loss with the soft-label distillation loss. However, a static
alpha is suboptimal because the optimal trade-off between hard and soft
supervision can vary during training.
  In this work, we propose an Adaptive Knowledge Distillation (AKD) framework.
First we try to make alpha as learnable parameter that can be automatically
learned and optimized during training. Then we introduce a formula to reflect
the gap between the student and the teacher to compute alpha dynamically,
guided by student-teacher discrepancies, and further introduce a Context-Aware
Module (CAM) using MLP + Attention to adaptively reweight class-wise teacher
outputs. Experiments on CIFAR-10 with ResNet-50 as teacher and ResNet-18 as
student demonstrate that our approach achieves superior accuracy compared to
fixed-weight KD baselines, and yields more stable convergence.

</details>


### [19] [A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD](https://arxiv.org/abs/2509.05321)
*Yunfei Guo,Tao Zhang,Wu Huang,Yao Song*

Main category: cs.CV

TL;DR: 该论文介绍了一个名为Video2EEG-SPGN-Diffusion的开源框架，利用自玩图网络（SPGN）和扩散模型生成视频条件化的个性化EEG信号。同时，发布了一个包含视频-EEG对及情感标签的新多模态数据集，旨在促进视频-EEG对齐和多模态研究，并提供相关工程管道。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏高质量的视频-EEG多模态数据集和有效的对齐管道，这阻碍了具有EEG对齐能力的多模态大模型的训练，以及情感分析、数据增强和脑机接口等应用的发展。

Method: 1. 利用SEED-VD数据集作为视频刺激源。
2. 采用结合自玩图网络（SPGN）与扩散模型的方法，生成个性化的EEG信号。
3. 开发并公开了一个用于对齐视频和EEG数据对的工程管道。

Result: 1. 引入了开源框架Video2EEG-SPGN-Diffusion。
2. 发布了一个新的多模态数据集，包含1000多个SEED-VD视频刺激样本，配对有生成的62通道200 Hz EEG信号和情感标签。
3. 公开了一个用于视频-EEG数据对齐的工程管道。

Conclusion: 该框架和数据集为视频-EEG对齐和多模态研究提供了新工具，在情感分析、数据增强和脑机接口应用方面具有重要的研究和工程意义。

Abstract: This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion,
that leverages the SEED-VD dataset to generate a multimodal dataset of EEG
signals conditioned on video stimuli. Additionally, we disclose an engineering
pipeline for aligning video and EEG data pairs, facilitating the training of
multimodal large models with EEG alignment capabilities. Personalized EEG
signals are generated using a self-play graph network (SPGN) integrated with a
diffusion model. As a major contribution, we release a new dataset comprising
over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG
signals at 200 Hz and emotion labels, enabling video-EEG alignment and
advancing multimodal research. This framework offers novel tools for emotion
analysis, data augmentation, and brain-computer interface applications, with
substantial research and engineering significance.

</details>


### [20] [Application of discrete Ricci curvature in pruning randomly wired neural networks: A case study with chest x-ray classification of COVID-19](https://arxiv.org/abs/2509.05322)
*Pavithra Elumalai,Sudharsan Vijayaraghavan,Madhumita Mondal,Areejit Samal*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Randomly Wired Neural Networks (RWNNs) serve as a valuable testbed for
investigating the impact of network topology in deep learning by capturing how
different connectivity patterns impact both learning efficiency and model
performance. At the same time, they provide a natural framework for exploring
edge-centric network measures as tools for pruning and optimization. In this
study, we investigate three edge-centric network measures: Forman-Ricci
curvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness
centrality (EBC), to compress RWNNs by selectively retaining important synapses
(or edges) while pruning the rest. As a baseline, RWNNs are trained for
COVID-19 chest x-ray image classification, aiming to reduce network complexity
while preserving performance in terms of accuracy, specificity, and
sensitivity. We extend prior work on pruning RWNN using ORC by incorporating
two additional edge-centric measures, FRC and EBC, across three network
generators: Erd\"{o}s-R\'{e}nyi (ER) model, Watts-Strogatz (WS) model, and
Barab\'{a}si-Albert (BA) model. We provide a comparative analysis of the
pruning performance of the three measures in terms of compression ratio and
theoretical speedup. A central focus of our study is to evaluate whether FRC,
which is computationally more efficient than ORC, can achieve comparable
pruning effectiveness. Along with performance evaluation, we further
investigate the structural properties of the pruned networks through modularity
and global efficiency, offering insights into the trade-off between modular
segregation and network efficiency in compressed RWNNs. Our results provide
initial evidence that FRC-based pruning can effectively simplify RWNNs,
offering significant computational advantages while maintaining performance
comparable to ORC.

</details>


### [21] [Optical Music Recognition of Jazz Lead Sheets](https://arxiv.org/abs/2509.05329)
*Juan Carlos Martinez-Sevilla,Francesco Foscarin,Patricia Garcia-Iasci,David Rizo,Jorge Calvo-Zaragoza,Gerhard Widmer*

Main category: cs.CV

TL;DR: 本文解决了手写爵士乐谱光符识别（OMR）的挑战，创建了包含手写和合成图像的新数据集，并开发了专用的OMR模型。


<details>
  <summary>Details</summary>
Motivation: 现有OMR系统无法处理包含和弦的乐谱；手写乐谱存在高变异性和质量问题；爵士乐谱作为常用乐谱类型，需要有效的OMR解决方案。

Method: 1. 构建了一个包含293份手写爵士乐谱（2021个谱表）及其Humdrum **kern和MusicXML格式真值的新数据集，并生成了合成乐谱图像。2. 开发了专门用于爵士乐谱的OMR模型，探讨了特定词元化选择以及使用合成乐谱和预训练模型的优势。

Result: 1. 首次发布了大规模手写爵士乐谱数据集及其合成图像。2. 成功开发了针对爵士乐谱的OMR模型。3. 所有代码、数据和模型均已公开。

Conclusion: 本研究通过提供专用数据集和OMR模型，有效解决了手写爵士乐谱OMR的独特挑战，并通过资源公开促进了该领域的进一步研究。

Abstract: In this paper, we address the challenge of Optical Music Recognition (OMR)
for handwritten jazz lead sheets, a widely used musical score type that encodes
melody and chords. The task is challenging due to the presence of chords, a
score component not handled by existing OMR systems, and the high variability
and quality issues associated with handwritten images. Our contribution is
two-fold. We present a novel dataset consisting of 293 handwritten jazz lead
sheets of 163 unique pieces, amounting to 2021 total staves aligned with
Humdrum **kern and MusicXML ground truth scores. We also supply synthetic score
images generated from the ground truth. The second contribution is the
development of an OMR model for jazz lead sheets. We discuss specific
tokenisation choices related to our kind of data, and the advantages of using
synthetic scores and pretrained models. We publicly release all code, data, and
models.

</details>


### [22] [RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness](https://arxiv.org/abs/2509.05333)
*Junghyun Park,Tuan Anh Nguyen,Dugki Min*

Main category: cs.CV

TL;DR: 针对物体识别模型在域偏移下性能下降的问题，本文提出了Re-Thinking Vision Language Model (RT-VLM) 框架。该框架利用包含“4种线索”的合成数据集对Llama 3.2模型进行微调，并在推理时采用两阶段的“Re-Thinking”自我修正机制，显著提升了模型在各类域偏移鲁棒性基准测试上的表现。


<details>
  <summary>Details</summary>
Motivation: 现代物体识别模型在实际部署中常受到域偏移（如低级图像统计、物体姿态、部分遮挡和类别混淆）的影响，导致准确率严重下降。

Method: 引入Re-Thinking Vision Language Model (RT-VLM) 框架。该框架基于独特的合成数据集生成管线，产生带有“4种线索”（边界框、类别名、物体级描述、场景级描述）的图像标注。随后，对Llama 3.2 11B Vision Instruct模型进行参数高效的监督微调。在推理时，模型执行两阶段“Re-Thinking”方案：首先生成自己的四种线索，然后将这些线索作为证据进行重新审查并迭代修正。

Result: 在隔离不同域偏移的鲁棒性基准测试中，RT-VLM模型持续超越了强大的基线模型。

Conclusion: 研究结果表明，整合结构化多模态证据与明确的自我批判循环，是实现可靠和可迁移视觉理解的一个有前景的途径。

Abstract: Real world deployments often expose modern object recognition models to
domain shifts that precipitate a severe drop in accuracy. Such shifts encompass
(i) variations in low level image statistics, (ii) changes in object pose and
viewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent
classes. To mitigate this degradation, we introduce the Re-Thinking Vision
Language Model (RT-VLM) framework. The foundation of this framework is a unique
synthetic dataset generation pipeline that produces images annotated with
"4-Clues": precise bounding boxes, class names, detailed object-level captions,
and a comprehensive context-level caption for the entire scene. We then perform
parameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this
resource. At inference time, a two stage Re-Thinking scheme is executed: the
model first emits its own four clues, then re examines these responses as
evidence and iteratively corrects them. Across robustness benchmarks that
isolate individual domain shifts, RT-VLM consistently surpasses strong
baselines. These findings indicate that the integration of structured
multimodal evidence with an explicit self critique loop constitutes a promising
route toward reliable and transferable visual understanding.

</details>


### [23] [A Real-Time, Vision-Based System for Badminton Smash Speed Estimation on Mobile Devices](https://arxiv.org/abs/2509.05334)
*Diwen Huang*

Main category: cs.CV

TL;DR: 本文提出了一种经济高效、用户友好的智能手机系统，利用YOLOv5和卡尔曼滤波技术测量羽毛球杀球速度，并通过移动应用让各级别运动员都能进行表现分析。


<details>
  <summary>Details</summary>
Motivation: 现有运动表现数据捕获技术成本高昂、复杂且主要面向专业选手，业余和娱乐性选手难以获得。

Method: 该系统采用定制训练的YOLOv5模型进行羽毛球检测，结合卡尔曼滤波进行轨迹跟踪，并通过基于视频的运动学速度估算方法（包含时空缩放）从标准视频中自动计算球速。所有功能集成在直观的移动应用程序中。

Result: 开发了一款用户友好的移动应用程序，能够自动从智能手机视频中计算羽毛球杀球速度，从而使高端性能分析普及化，赋能各级别运动员分析和提升自身表现。

Conclusion: 该系统成功填补了羽毛球运动员在可及性表现分析方面的空白，通过经济高效的智能手机技术，赋能各级别运动员分析和提升自身表现。

Abstract: Performance metrics in sports, such as shot speed and angle, provide crucial
feedback for athlete development. However, the technology to capture these
metrics has historically been expensive, complex, and largely inaccessible to
amateur and recreational players. This paper addresses this gap in the context
of badminton, one of the world's most popular sports, by introducing a novel,
cost-effective, and user-friendly system for measuring smash speed using
ubiquitous smartphone technology. Our approach leverages a custom-trained
YOLOv5 model for shuttlecock detection, combined with a Kalman filter for
robust trajectory tracking. By implementing a video-based kinematic speed
estimation method with spatiotemporal scaling, the system automatically
calculates the shuttlecock's velocity from a standard video recording. The
entire process is packaged into an intuitive mobile application, democratizing
access to high-level performance analytics and empowering players at all levels
to analyze and improve their game.

</details>


### [24] [A Stroke-Level Large-Scale Database of Chinese Character Handwriting and the OpenHandWrite_Toolbox for Handwriting Research](https://arxiv.org/abs/2509.05335)
*Zebo Xu,Shaoyun Yu,Mark Torrance,Guido Nottbusch,Nan Zhao,Zhenguang Cai*

Main category: cs.CV

TL;DR: 本研究利用大型手写数据库和升级工具，探索了语言成分（正字法和语音）对汉字手写（字符、部首、笔画层级）的影响，发现它们在各层级均有作用，且影响强度呈层级衰减。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚哪些语言成分（如语音、语义、正字法系统）调控汉字在字符、部首和笔画层级的手写，且缺乏捕获和批量处理精细手写数据的综合工具。

Method: 构建了一个大型手写数据库，包含42名中文母语者完成1200个汉字的听写手写任务。同时，升级了OpenHandWrite_Toolbox，使其能够轻松修改实验设计、捕获笔画级手写轨迹，并批量处理手写测量数据（如潜伏期、持续时间、笔压）。使用多元回归分析该数据库。

Result: 正字法预测因子影响字符、部首和笔画各层级的手写准备与执行。语音因素也影响所有三个层级的执行。这些词汇效应呈现层级衰减：在字符层级最显著，其次是部首，在笔画层级最弱。

Conclusion: 部首和笔画层级的手写准备与执行与语言成分紧密相关。本研究的数据库和工具为未来跨语言的字符和子字符手写心理语言学和神经语言学研究提供了宝贵资源。

Abstract: Understanding what linguistic components (e.g., phonological, semantic, and
orthographic systems) modulate Chinese handwriting at the character, radical,
and stroke levels remains an important yet understudied topic. Additionally,
there is a lack of comprehensive tools for capturing and batch-processing
fine-grained handwriting data. To address these issues, we constructed a
large-scale handwriting database in which 42 Chinese speakers for each
handwriting 1200 characters in a handwriting-to-dictation task. Additionally,
we enhanced the existing handwriting package and provided comprehensive
documentation for the upgraded OpenHandWrite_Toolbox, which can easily modify
the experimental design, capture the stroke-level handwriting trajectory, and
batch-process handwriting measurements (e.g., latency, duration, and
pen-pressure). In analysing our large-scale database, multiple regression
results show that orthographic predictors impact handwriting preparation and
execution across character, radical, and stroke levels. Phonological factors
also influence execution at all three levels. Importantly, these lexical
effects demonstrate hierarchical attenuation - they were most pronounced at the
character level, followed by the radical, and were weakest at the stroke
levels. These findings demonstrate that handwriting preparation and execution
at the radical and stroke levels are closely intertwined with linguistic
components. This database and toolbox offer valuable resources for future
psycholinguistic and neurolinguistic research on the handwriting of characters
and sub-characters across different languages.

</details>


### [25] [Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory](https://arxiv.org/abs/2509.05337)
*Younggeol Cho,Gokhan Solak,Olivia Nocentini,Marta Lorenzini,Andrea Fortuna,Arash Ajoudani*

Main category: cs.CV

TL;DR: 提出一种结合DGNN和LSTM的混合模型，通过解耦运动预测和步态分类，实现高精度跌倒预测和过渡状态分析。


<details>
  <summary>Details</summary>
Motivation: 现有的跌倒检测在预测事前跌倒和分析稳定与跌倒之间的过渡状态方面不足，而这对于辅助机器人系统至关重要。

Method: 该方法提出一种解耦运动预测和步态分类任务的混合模型，结合动态图神经网络（DGNN）和长短期记忆（LSTM）网络。DGNN负责将步态分类为稳定、过渡和跌倒三种状态，LSTM网络则预测后续时间步的人体运动，输入为视频序列中提取的实时骨骼特征。

Result: 该模型在OUMVLP-Pose和URFD数据集上表现优越，预测误差和识别准确性均超过仅使用DGNN的模型及现有文献模型。结果表明解耦预测和分类能显著提高性能，并且能够有效监测过渡状态，提供有价值的洞察。

Conclusion: 通过结合DGNN和LSTM并解耦预测与分类任务，本方法显著提高了跌倒预测的准确性，并能有效监测跌倒前的过渡状态，从而增强高级辅助系统的功能性。

Abstract: Detecting and preventing falls in humans is a critical component of assistive
robotic systems. While significant progress has been made in detecting falls,
the prediction of falls before they happen, and analysis of the transient state
between stability and an impending fall remain unexplored. In this paper, we
propose a anticipatory fall detection method that utilizes a hybrid model
combining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory
(LSTM) networks that decoupled the motion prediction and gait classification
tasks to anticipate falls with high accuracy. Our approach employs real-time
skeletal features extracted from video sequences as input for the proposed
model. The DGNN acts as a classifier, distinguishing between three gait states:
stable, transient, and fall. The LSTM-based network then predicts human
movement in subsequent time steps, enabling early detection of falls. The
proposed model was trained and validated using the OUMVLP-Pose and URFD
datasets, demonstrating superior performance in terms of prediction error and
recognition accuracy compared to models relying solely on DGNN and models from
literature. The results indicate that decoupling prediction and classification
improves performance compared to addressing the unified problem using only the
DGNN. Furthermore, our method allows for the monitoring of the transient state,
offering valuable insights that could enhance the functionality of advanced
assistance systems.

</details>


### [26] [Comparative Evaluation of Hard and Soft Clustering for Precise Brain Tumor Segmentation in MR Imaging](https://arxiv.org/abs/2509.05340)
*Dibya Jyoti Bora,Mrinal Kanti Mishra*

Main category: cs.CV

TL;DR: 本研究比较了K-Means和Fuzzy C-Means两种聚类算法在MRI脑肿瘤分割中的表现，发现K-Means速度更快但FCM精度更高，揭示了计算效率与边界精度之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 由于肿瘤形态和强度分布的异质性，从MRI图像中分割脑肿瘤仍是一项关键挑战。准确描绘肿瘤边界对于临床决策、放疗规划和疾病监测至关重要。

Method: 本研究对硬聚类（K-Means）和软聚类（Fuzzy C-Means）两种主要的聚类范式在MRI肿瘤分割中的应用进行了综合比较分析。实验使用BraTS2020数据集，并对图像进行了高斯滤波和CLAHE预处理。评估指标包括Dice相似系数（DSC）和处理时间。

Result: K-Means算法在速度上表现优异，平均每张图像处理时间为0.3秒；而FCM算法在分割精度上更高，平均DSC为0.67（K-Means为0.43），但计算成本更高（每张图像1.3秒）。

Conclusion: 研究结果强调了计算效率和边界精度之间固有的权衡关系。

Abstract: Segmentation of brain tumors from Magnetic Resonance Imaging (MRI) remains a
pivotal challenge in medical image analysis due to the heterogeneous nature of
tumor morphology and intensity distributions. Accurate delineation of tumor
boundaries is critical for clinical decision-making, radiotherapy planning, and
longitudinal disease monitoring. In this study, we perform a comprehensive
comparative analysis of two major clustering paradigms applied in MRI tumor
segmentation: hard clustering, exemplified by the K-Means algorithm, and soft
clustering, represented by Fuzzy C-Means (FCM). While K-Means assigns each
pixel strictly to a single cluster, FCM introduces partial memberships, meaning
each pixel can belong to multiple clusters with varying degrees of association.
Experimental validation was performed using the BraTS2020 dataset,
incorporating pre-processing through Gaussian filtering and Contrast Limited
Adaptive Histogram Equalization (CLAHE). Evaluation metrics included the Dice
Similarity Coefficient (DSC) and processing time, which collectively
demonstrated that K-Means achieved superior speed with an average runtime of
0.3s per image, whereas FCM attained higher segmentation accuracy with an
average DSC of 0.67 compared to 0.43 for K-Means, albeit at a higher
computational cost (1.3s per image). These results highlight the inherent
trade-off between computational efficiency and boundary precision.

</details>


### [27] [Handling imbalance and few-sample size in ML based Onion disease classification](https://arxiv.org/abs/2509.05341)
*Abhijeet Manoj Pal,Rajbabu Velmurugan*

Main category: cs.CV

TL;DR: 本文提出一种基于深度学习的多分类模型，用于精确识别洋葱作物的多种病虫害，并取得了96.90%的准确率和0.96的F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有病虫害分类方法主要限于二分类，限制了实际应用，尤其是在需要精确识别特定病害或害虫类型以实现精准农业的场景中。

Method: 我们提出了一种鲁棒的深度学习模型，用于洋葱作物病虫害的多类别分类。通过集成基于注意力模块和采用全面的数据增强流水线来增强预训练的卷积神经网络（CNN）模型，以减轻类别不平衡问题。

Result: 该模型在真实世界田间图像数据集上实现了96.90%的总体准确率和0.96的F1分数。与使用相同数据集的其他方法相比，本模型表现更优。

Conclusion: 所提出的深度学习模型能有效、高精度地进行洋葱作物的多类别病虫害分类，优于现有方法，为精准农业提供了有力支持。

Abstract: Accurate classification of pests and diseases plays a vital role in precision
agriculture, enabling efficient identification, targeted interventions, and
preventing their further spread. However, current methods primarily focus on
binary classification, which limits their practical applications, especially in
scenarios where accurately identifying the specific type of disease or pest is
essential. We propose a robust deep learning based model for multi-class
classification of onion crop diseases and pests. We enhance a pre-trained
Convolutional Neural Network (CNN) model by integrating attention based modules
and employing comprehensive data augmentation pipeline to mitigate class
imbalance. We propose a model which gives 96.90% overall accuracy and 0.96 F1
score on real-world field image dataset. This model gives better results than
other approaches using the same datasets.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [28] [Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts](https://arxiv.org/abs/2509.05323)
*Adam Cole,Mick Grierson*

Main category: cs.AI

TL;DR: 本文艺术性与技术性地研究了视频扩散Transformer的注意力机制，提出了一种提取和可视化生成视频模型中交叉注意力图的方法，并以注意力图作为分析工具和艺术材料，为艺术领域的可解释AI做贡献。


<details>
  <summary>Details</summary>
Motivation: 受早期视频艺术家通过操纵模拟信号创造新美学的启发，旨在揭示和利用生成式视频模型中的注意力机制，为文本到视频生成提供可解释性。

Method: 基于开源Wan模型，开发了一种工具来提取和可视化生成视频模型中的交叉注意力图，并通过探索性探究和艺术案例研究来评估注意力图的潜力。

Result: 所开发的工具为文本到视频生成中的注意力机制提供了可解释的视角，并验证了注意力图作为分析工具和原始艺术材料的潜力。

Conclusion: 该研究为艺术领域的可解释AI (XAIxArts) 做出了贡献，并鼓励艺术家将AI的内部运作机制重新定义为一种创造性媒介。

Abstract: This paper presents an artistic and technical investigation into the
attention mechanisms of video diffusion transformers. Inspired by early video
artists who manipulated analog video signals to create new visual aesthetics,
this study proposes a method for extracting and visualizing cross-attention
maps in generative video models. Built on the open-source Wan model, our tool
provides an interpretable window into the temporal and spatial behavior of
attention in text-to-video generation. Through exploratory probes and an
artistic case study, we examine the potential of attention maps as both
analytical tools and raw artistic material. This work contributes to the
growing field of Explainable AI for the Arts (XAIxArts), inviting artists to
reclaim the inner workings of AI as a creative medium.

</details>


### [29] [Perception Graph for Cognitive Attack Reasoning in Augmented Reality](https://arxiv.org/abs/2509.05324)
*Rongqian Chen,Shu Hong,Rifatul Islam,Mahdi Imani,G. Gary Tan,Tian Lan*

Main category: cs.AI

TL;DR: AR系统在战术环境中易受认知攻击，影响用户感知和决策。本文提出Perception Graph模型，通过量化感知扭曲来检测和分析此类攻击。


<details>
  <summary>Details</summary>
Motivation: 战术环境中部署的增强现实（AR）系统，因其对无缝人机交互的依赖性，易受操纵用户感知并严重损害决策的认知攻击。

Method: 引入Perception Graph模型，该模型首先模仿人类从MR环境中解释关键信息的流程，然后使用语义有意义的结构表示结果，从而对系统内的人类感知进行推理。

Result: 该模型能够计算出反映感知扭曲程度的量化分数，提供了一种稳健且可衡量的方法来检测和分析此类认知攻击的影响。

Conclusion: Perception Graph模型通过量化感知扭曲，有效应对了AR系统在战术环境中面临的认知攻击挑战，从而增强了系统抵御此类威胁的能力。

Abstract: Augmented reality (AR) systems are increasingly deployed in tactical
environments, but their reliance on seamless human-computer interaction makes
them vulnerable to cognitive attacks that manipulate a user's perception and
severely compromise user decision-making. To address this challenge, we
introduce the Perception Graph, a novel model designed to reason about human
perception within these systems. Our model operates by first mimicking the
human process of interpreting key information from an MR environment and then
representing the outcomes using a semantically meaningful structure. We
demonstrate how the model can compute a quantitative score that reflects the
level of perception distortion, providing a robust and measurable method for
detecting and analyzing the effects of such cognitive attacks.

</details>


### [30] [SynDelay: A Synthetic Dataset for Delivery Delay Prediction](https://arxiv.org/abs/2509.05325)
*Liming Xu,Yunbo Long,Alexandra Brintrup*

Main category: cs.AI

TL;DR: 本文提出了一个名为SynDelay的合成数据集，旨在解决供应链AI中交付延迟预测任务高质量开放数据集稀缺的问题，并提供了基线结果和评估指标。


<details>
  <summary>Details</summary>
Motivation: 人工智能在供应链管理中的预测任务（如交付延迟预测）因高质量、开放数据集的稀缺而受限。现有数据集多为专有、小型或维护不一致，从而阻碍了研究的复现性和基准测试。

Method: 本文引入了SynDelay，一个专为交付延迟预测设计的合成数据集。该数据集使用在真实世界数据上训练的先进生成模型生成，在保留真实交付模式的同时确保了隐私。

Result: SynDelay提供了一个具有挑战性和实用性的测试平台，用于推进预测建模。为了支持数据集的采用，作者提供了基线结果和评估指标作为初始基准。

Conclusion: SynDelay数据集已通过Supply Chain Data Hub公开发布，旨在促进供应链AI领域的数据集共享和基准测试。论文鼓励社区贡献数据集、模型和评估实践，以推动该领域的研究进展。

Abstract: Artificial intelligence (AI) is transforming supply chain management, yet
progress in predictive tasks -- such as delivery delay prediction -- remains
constrained by the scarcity of high-quality, openly available datasets.
Existing datasets are often proprietary, small, or inconsistently maintained,
hindering reproducibility and benchmarking. We present SynDelay, a synthetic
dataset designed for delivery delay prediction. Generated using an advanced
generative model trained on real-world data, SynDelay preserves realistic
delivery patterns while ensuring privacy. Although not entirely free of noise
or inconsistencies, it provides a challenging and practical testbed for
advancing predictive modelling. To support adoption, we provide baseline
results and evaluation metrics as initial benchmarks, serving as reference
points rather than state-of-the-art claims. SynDelay is publicly available
through the Supply Chain Data Hub, an open initiative promoting dataset sharing
and benchmarking in supply chain AI. We encourage the community to contribute
datasets, models, and evaluation practices to advance research in this area.
All code is openly accessible at https://supplychaindatahub.org.

</details>


### [31] [MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset](https://arxiv.org/abs/2509.05330)
*Seyed Muhammad Hossein Mousavi,Atiye Ilanloo*

Main category: cs.AI

TL;DR: 为解决多模态情感识别领域数据缺乏的问题，本文引入了MVRS数据集，该数据集整合了VR情感刺激下的眼动、身体运动和生理信号，并通过分类器验证了其质量和情感可分离性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的兴起，自动情感识别变得日益重要，尤其是在医疗、教育和汽车系统等领域。然而，该领域缺乏多模态数据集，特别是涉及身体运动和生理信号的数据集，这限制了其进展。

Method: 本文引入了MVRS数据集，该数据集包含13名参与者（12至60岁）在VR情感刺激（放松、恐惧、压力、悲伤、喜悦）下的同步记录。数据通过眼动追踪（VR头显摄像头）、身体运动（Kinect v2）以及EMG和GSR信号（Arduino UNO）采集，并进行了时间戳对齐。从每种模态中提取特征，使用早期和晚期融合技术进行融合，并通过分类器进行评估以确认数据集的质量和情感可分离性。

Result: 通过对多模态特征的提取、融合及分类器评估，MVRS数据集的质量和情感可分离性得到了确认。

Conclusion: MVRS数据集是对多模态情感计算领域的一项有价值的贡献。

Abstract: Automatic emotion recognition has become increasingly important with the rise
of AI, especially in fields like healthcare, education, and automotive systems.
However, there is a lack of multimodal datasets, particularly involving body
motion and physiological signals, which limits progress in the field. To
address this, the MVRS dataset is introduced, featuring synchronized recordings
from 13 participants aged 12 to 60 exposed to VR based emotional stimuli
(relaxation, fear, stress, sadness, joy). Data were collected using eye
tracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR
signals (Arduino UNO), all timestamp aligned. Participants followed a unified
protocol with consent and questionnaires. Features from each modality were
extracted, fused using early and late fusion techniques, and evaluated with
classifiers to confirm the datasets quality and emotion separability, making
MVRS a valuable contribution to multimodal affective computing.

</details>


### [32] [Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning](https://arxiv.org/abs/2509.05346)
*Bo Yuan,Jiazi Hu*

Main category: cs.AI

TL;DR: 本研究在模拟教学场景中比较了GPT-4o、DeepSeek-V3和GLM-4.5三种LLM在辅导任务中的表现，发现GPT-4o在信息丰富性和结构性方面更优，表明LLM在个性化学习辅助方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）被设想为个性化学习的智能助手，但在真实学习场景中缺乏系统的头对头评估。

Method: 研究在一个模拟辅导任务中，使用包含学生答卷的数据集，要求LLMs分析测验、推断学生掌握情况并生成针对性指导。采用Gemini作为虚拟评判员，进行准确性、清晰度、可操作性和适当性等多维度配对比较，并通过Bradley-Terry模型分析结果。

Result: 分析结果显示，GPT-4o普遍更受青睐，其反馈信息更丰富、结构更清晰；而DeepSeek-V3和GLM-4.5虽然偶有亮点，但一致性较低。

Conclusion: 研究结果突出了部署LLMs作为高级教学助手提供个性化支持的可行性，并为未来LLM驱动的个性化学习实证研究提供了方法论指导。

Abstract: While Large Language Models (LLMs) are increasingly envisioned as intelligent
assistants for personalized learning, systematic head-to-head evaluations
within authentic learning scenarios remain limited. This study conducts an
empirical comparison of three state-of-the-art LLMs on a tutoring task that
simulates a realistic learning setting. Using a dataset comprising a student's
answers to ten questions of mixed formats with correctness labels, each LLM is
required to (i) analyze the quiz to identify underlying knowledge components,
(ii) infer the student's mastery profile, and (iii) generate targeted guidance
for improvement. To mitigate subjectivity and evaluator bias, we employ Gemini
as a virtual judge to perform pairwise comparisons along various dimensions:
accuracy, clarity, actionability, and appropriateness. Results analyzed via the
Bradley-Terry model indicate that GPT-4o is generally preferred, producing
feedback that is more informative and better structured than its counterparts,
while DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower
consistency. These findings highlight the feasibility of deploying LLMs as
advanced teaching assistants for individualized support and provide
methodological guidance for future empirical research on LLM-driven
personalized learning.

</details>


### [33] [SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis](https://arxiv.org/abs/2509.05363)
*Lijie Ding,Changwoo Do*

Main category: cs.AI

TL;DR: SasAgent是一个基于LLM的多智能体AI系统，通过整合SasView工具并支持文本交互，自动化小角散射(SAS)数据分析。


<details>
  <summary>Details</summary>
Motivation: 旨在利用大型语言模型(LLM)驱动的AI系统，简化小角散射(SAS)研究的科学工作流程，并提高其自动化水平。

Method: 引入了SasAgent，一个多智能体AI系统。该系统包含一个协调器代理，负责解释用户指令并将任务分配给三个专业代理（SLD计算、合成数据生成、实验数据拟合）。这些代理利用从SasView Python库派生出的LLM友好工具（如模型数据工具、RAG文档工具、凹凸拟合工具和SLD计算器工具）执行任务。系统通过基于Gradio的界面增强用户可访问性。

Result: SasAgent成功展示了其解释复杂指令、精确计算散射长度密度(SLD)、生成准确散射数据以及高精度拟合实验数据集的能力。

Conclusion: 本工作展示了LLM驱动的AI系统在简化科学工作流程和提升SAS研究自动化方面的巨大潜力。

Abstract: We introduce SasAgent, a multi-agent AI system powered by large language
models (LLMs) that automates small-angle scattering (SAS) data analysis by
leveraging tools from the SasView software and enables user interaction via
text input. SasAgent features a coordinator agent that interprets user prompts
and delegates tasks to three specialized agents for scattering length density
(SLD) calculation, synthetic data generation, and experimental data fitting.
These agents utilize LLM-friendly tools to execute tasks efficiently. These
tools, including the model data tool, Retrieval-Augmented Generation (RAG)
documentation tool, bump fitting tool, and SLD calculator tool, are derived
from the SasView Python library. A user-friendly Gradio-based interface
enhances user accessibility. Through diverse examples, we demonstrate
SasAgent's ability to interpret complex prompts, calculate SLDs, generate
accurate scattering data, and fit experimental datasets with high precision.
This work showcases the potential of LLM-driven AI systems to streamline
scientific workflows and enhance automation in SAS research.

</details>


### [34] [Characterizing Fitness Landscape Structures in Prompt Engineering](https://arxiv.org/abs/2509.05375)
*Arend Hintze*

Main category: cs.AI

TL;DR: 该研究系统分析了提示工程的优化景观结构，发现不同提示生成策略会导致截然不同的景观拓扑，为理解其复杂性提供了实证基础。


<details>
  <summary>Details</summary>
Motivation: 提示工程对优化大型语言模型性能至关重要，但其底层的优化景观仍知之甚少；现有方法将提示优化视为黑箱问题，未刻画其导航的景观拓扑。

Method: 采用自相关分析法，跨语义嵌入空间系统分析了提示工程的适应度景观结构。通过在错误检测任务上使用两种提示生成策略（系统枚举和新颖性驱动多样化）进行实验，分别生成了1024和1000个提示。

Result: 系统提示生成产生平滑衰减的自相关性，表明景观平滑；而多样化生成则表现出非单调模式，在中等语义距离处出现峰值相关性，表明景观崎岖且具有层次结构。此外，对10个错误检测类别的任务特异性分析显示，不同错误类型的崎岖程度各异。

Conclusion: 研究结果为理解提示工程优化景观的复杂性提供了实证基础。

Abstract: While prompt engineering has emerged as a crucial technique for optimizing
large language model performance, the underlying optimization landscape remains
poorly understood. Current approaches treat prompt optimization as a black-box
problem, applying sophisticated search algorithms without characterizing the
landscape topology they navigate. We present a systematic analysis of fitness
landscape structures in prompt engineering using autocorrelation analysis
across semantic embedding spaces. Through experiments on error detection tasks
with two distinct prompt generation strategies -- systematic enumeration (1,024
prompts) and novelty-driven diversification (1,000 prompts) -- we reveal
fundamentally different landscape topologies. Systematic prompt generation
yields smoothly decaying autocorrelation, while diversified generation exhibits
non-monotonic patterns with peak correlation at intermediate semantic
distances, indicating rugged, hierarchically structured landscapes.
Task-specific analysis across 10 error detection categories reveals varying
degrees of ruggedness across different error types. Our findings provide an
empirical foundation for understanding the complexity of optimization in prompt
engineering landscapes.

</details>


### [35] [Code Like Humans: A Multi-Agent Solution for Medical Coding](https://arxiv.org/abs/2509.05378)
*Andreas Motzfeldt,Joakim Edin,Casper L. Christensen,Christian Hardmeier,Lars Maaløe,Anna Rogers*

Main category: cs.AI

TL;DR: 本文提出了一种名为“Code Like Humans”的代理式LLM框架，用于医疗编码，首次支持完整的ICD-10系统（7万+标签），并在罕见诊断代码上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 医疗编码将非结构化临床笔记映射到诊断和程序代码，过程复杂。现有的判别分类器在处理高频代码时表现良好，但受限于其范围，且难以支持完整的编码系统，特别是在罕见代码方面存在挑战。

Method: 引入“Code Like Humans”代理式框架，该框架利用大型语言模型（LLMs）并实现了人类专家使用的官方编码指南。它是首个能支持完整ICD-10编码系统（超过7万个标签）的解决方案。

Result: 该框架在罕见诊断代码上取得了迄今为止的最佳性能。尽管经过微调的判别分类器在高频代码上仍有优势，但其应用受限。研究还分析了系统性能并识别了其“盲点”（系统性欠编码的代码）。

Conclusion: “Code Like Humans”框架在医疗编码自动化方面取得了重要进展，特别是其对完整ICD-10系统的支持和在罕见代码上的优异表现。未来工作将侧重于解决已识别的系统“盲点”。

Abstract: In medical coding, experts map unstructured clinical notes to alphanumeric
codes for diagnoses and procedures. We introduce Code Like Humans: a new
agentic framework for medical coding with large language models. It implements
official coding guidelines for human experts, and it is the first solution that
can support the full ICD-10 coding system (+70K labels). It achieves the best
performance to date on rare diagnosis codes (fine-tuned discriminative
classifiers retain an advantage for high-frequency codes, to which they are
limited). Towards future work, we also contribute an analysis of system
performance and identify its `blind spots' (codes that are systematically
undercoded).

</details>


### [36] [Murphys Laws of AI Alignment: Why the Gap Always Wins](https://arxiv.org/abs/2509.05381)
*Madhava Gaikwad*

Main category: cs.AI

TL;DR: 本文提出“对齐鸿沟”概念，解释了基于反馈的大语言模型对齐方法中重复出现的失败模式（如奖励欺骗、错误泛化），并通过KL倾斜形式、AI对齐墨菲定律和对齐三难困境等框架，从结构限制和权衡角度重新审视AI对齐问题，并提供MAPS框架作为实用设计指导。


<details>
  <summary>Details</summary>
Motivation: 尽管通过强化学习与人类反馈（RLHF）等方法对齐大型语言模型非常有效，但这些方法普遍存在奖励欺骗、奉承、标注者漂移和错误泛化等重复出现的失败模式。

Method: ['引入“对齐鸿沟”（Alignment Gap）概念，作为理解反馈式对齐中重复失败的统一视角。', '使用KL倾斜形式（KL-tilting formalism）阐释优化压力如何放大代理奖励与真实人类意图之间的分歧。', '将这些失败组织成“AI对齐墨菲定律”（Murphys Laws of AI Alignment）目录。', '提出“对齐三难困境”（Alignment Trilemma），以构建优化强度、价值捕获和泛化之间的权衡关系。', '进行小规模实证研究以提供说明性支持。', '提出MAPS框架（Misspecification, Annotation, Pressure, Shift）作为实用的设计杠杆。']

Result: ['识别并系统地解释了反馈式AI对齐中普遍存在的失败模式。', '通过“对齐鸿沟”和KL倾斜形式，揭示了优化压力导致代理奖励与真实人类意图之间发散的内在机制。', '构建了AI对齐失败的“墨菲定律”目录。', '提出了“对齐三难困境”，明确了优化强度、价值捕获和泛化之间的结构性权衡。', 'MAPS框架为未来的对齐系统设计提供了具体的设计杠杆。']

Conclusion: 本研究并未提出一个明确的不可能性定理，而是提供了一个新的视角，从结构限制和权衡的角度重新审视AI对齐的讨论，为未来对齐系统的设计提供了更清晰的指导。

Abstract: Large language models are increasingly aligned to human preferences through
reinforcement learning from human feedback (RLHF) and related methods such as
Direct Preference Optimization (DPO), Constitutional AI, and RLAIF. While
effective, these methods exhibit recurring failure patterns i.e., reward
hacking, sycophancy, annotator drift, and misgeneralization. We introduce the
concept of the Alignment Gap, a unifying lens for understanding recurring
failures in feedback-based alignment. Using a KL-tilting formalism, we
illustrate why optimization pressure tends to amplify divergence between proxy
rewards and true human intent. We organize these failures into a catalogue of
Murphys Laws of AI Alignment, and propose the Alignment Trilemma as a way to
frame trade-offs among optimization strength, value capture, and
generalization. Small-scale empirical studies serve as illustrative support.
Finally, we propose the MAPS framework (Misspecification, Annotation, Pressure,
Shift) as practical design levers. Our contribution is not a definitive
impossibility theorem but a perspective that reframes alignment debates around
structural limits and trade-offs, offering clearer guidance for future design.

</details>


### [37] [From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation](https://arxiv.org/abs/2509.05469)
*Chenguang Wang,Xiang Yan,Yilong Dai,Ziyi Wang,Susu Xu*

Main category: cs.AI

TL;DR: 本文提出一个多智能体系统，可直接在真实街景图像上编辑和重新设计自行车设施，以促进主动交通规划中的公众参与。


<details>
  <summary>Details</summary>
Motivation: 传统的街景渲染方法耗时费力，阻碍公众参与。现有AI生成设计虽然有潜力，但需要大量领域特定训练数据，且难以在复杂街景中实现精确的空间设计变化。

Method: 引入了一个多智能体系统，直接在真实街景图像上编辑和重新设计自行车设施。该框架集成了车道定位、提示优化、设计生成和自动化评估。

Result: 该系统在多样化的城市场景中，能适应不同的道路几何形状和环境条件，持续生成视觉上连贯且符合指令的结果。

Conclusion: 这项工作为将多智能体管道应用于交通基础设施规划和设施设计奠定了基础。

Abstract: Realistic visual renderings of street-design scenarios are essential for
public engagement in active transportation planning. Traditional approaches are
labor-intensive, hindering collective deliberation and collaborative
decision-making. While AI-assisted generative design shows transformative
potential by enabling rapid creation of design scenarios, existing generative
approaches typically require large amounts of domain-specific training data and
struggle to enable precise spatial variations of design/configuration in
complex street-view scenes. We introduce a multi-agent system that edits and
redesigns bicycle facilities directly on real-world street-view imagery. The
framework integrates lane localization, prompt optimization, design generation,
and automated evaluation to synthesize realistic, contextually appropriate
designs. Experiments across diverse urban scenarios demonstrate that the system
can adapt to varying road geometries and environmental conditions, consistently
yielding visually coherent and instruction-compliant results. This work
establishes a foundation for applying multi-agent pipelines to transportation
infrastructure planning and facility design.

</details>


### [38] [TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation](https://arxiv.org/abs/2509.05550)
*Zixi Li*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce TreeGPT, a novel neural architecture that combines
transformer-based attention mechanisms with global parent-child aggregation for
processing Abstract Syntax Trees (ASTs) in neural program synthesis tasks.
Unlike traditional approaches that rely solely on sequential processing or
graph neural networks, TreeGPT employs a hybrid design that leverages both
self-attention for capturing local dependencies and a specialized Tree
Feed-Forward Network (TreeFFN) for modeling hierarchical tree structures
through iterative message passing.
  The core innovation lies in our Global Parent-Child Aggregation mechanism,
formalized as: $$h_i^{(t+1)} = \sigma \Big( h_i^{(0)} + W_{pc} \sum_{(p,c) \in
E_i} f(h_p^{(t)}, h_c^{(t)}) + b \Big)$$ where $h_i^{(t)}$ represents the
hidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edges
involving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. This
formulation enables each node to progressively aggregate information from the
entire tree structure through $T$ iterations.
  Our architecture integrates optional enhancements including gated aggregation
with learnable edge weights, residual connections for gradient stability, and
bidirectional propagation for capturing both bottom-up and top-down
dependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challenging
visual reasoning benchmark requiring abstract pattern recognition and rule
inference. Experimental results demonstrate that TreeGPT achieves 96\%
accuracy, significantly outperforming transformer baselines (1.3\%),
large-scale models like Grok-4 (15.9\%), and specialized program synthesis
methods like SOAR (52\%) while using only 1.5M parameters. Our comprehensive
ablation study reveals that edge projection is the most critical component,
with the combination of edge projection and gating achieving optimal
performance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [Standard vs. Modular Sampling: Best Practices for Reliable LLM Unlearning](https://arxiv.org/abs/2509.05316)
*Praveen Bushipaka,Lucia Passaro,Tommaso Cucinotta*

Main category: cs.LG

TL;DR: 本文评估了LLM遗忘中的常见做法，发现单一邻居集和标准采样方法（如1:1采样）的局限性。研究提出并验证了一套最佳实践，包括引入多样化的邻居集和新的模块化实体级遗忘（MELU）策略，以实现更有效和稳定的遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有LLM遗忘基准实践存在局限性，主要体现在仅使用单一邻居集未能反映真实世界数据复杂性，以及常用采样方法（如1:1采样或循环迭代采样）的有效性和稳定性尚未被系统性检验。因此，研究动机是系统性地评估这些常见做法，以发现其不足并提出改进方案。

Method: 本研究系统性地评估了LLM遗忘中的常见实践，包括邻居集的设置多样性和采样方法（如1:1采样和循环迭代采样）。基于评估结果，提出并验证了一套初始最佳实践，具体包括：整合多样化的邻居集、指出标准1:1采样方法的低效性，并提出模块化实体级遗忘（MELU）策略作为循环采样的替代方案。

Result: 研究发现，仅依赖单一邻居集是次优的，且标准的1:1采样方法效率低下并可能导致不良结果，同时掩盖了性能权衡。这些传统做法未能提供清晰稳定的遗忘路径。

Conclusion: 结合多样化的邻居集和所提出的模块化实体级遗忘（MELU）策略，辅以强大的算法，能够为实现有效且稳定的LLM遗忘提供清晰路径。这些最佳实践有助于平衡遗忘效率和模型实用性。

Abstract: A conventional LLM Unlearning setting consists of two subsets -"forget" and
"retain", with the objectives of removing the undesired knowledge from the
forget set while preserving the remaining knowledge from the retain. In
privacy-focused unlearning research, a retain set is often further divided into
neighbor sets, containing either directly or indirectly connected to the forget
targets; and augmented by a general-knowledge set. A common practice in
existing benchmarks is to employ only a single neighbor set, with general
knowledge which fails to reflect the real-world data complexities and
relationships. LLM Unlearning typically involves 1:1 sampling or cyclic
iteration sampling. However, the efficacy and stability of these de facto
standards have not been critically examined. In this study, we systematically
evaluate these common practices. Our findings reveal that relying on a single
neighbor set is suboptimal and that a standard sampling approach can obscure
performance trade-offs. Based on this analysis, we propose and validate an
initial set of best practices: (1) Incorporation of diverse neighbor sets to
balance forget efficacy and model utility, (2) Standard 1:1 sampling methods
are inefficient and yield poor results, (3) Our proposed Modular Entity-Level
Unlearning (MELU) strategy as an alternative to cyclic sampling. We demonstrate
that this modular approach, combined with robust algorithms, provides a clear
and stable path towards effective unlearning.

</details>


### [40] [Feed Two Birds with One Scone: Exploiting Function-Space Regularization for Both OOD Robustness and ID Fine-Tuning Performance](https://arxiv.org/abs/2509.05328)
*Xiang Yuan,Jun Shu,Deyu meng,Zongben Xu*

Main category: cs.LG

TL;DR: 针对鲁棒微调中现有方法未能一致提升OOD鲁棒性的问题，本文提出在函数空间中正则化微调模型与预训练模型距离，并结合一致性正则化，显著提升了ID性能和OOD鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒微调方法在将预训练模型迁移到下游任务时，难以在保持竞争性ID性能的同时有效提升OOD鲁棒性，特别是对于不同模型架构。这主要是因为现有方法未能有效在函数空间中优化模型功能以实现对下游任务输入信息的稳定预测。

Method: 本文提出一种新颖的正则化方法，通过模拟OOD样本，在函数空间中约束微调模型与预训练模型之间的距离，以保持预训练模型的OOD鲁棒性。此外，引入一致性正则化以促进对扰动样本的稳定预测，进一步增强OOD鲁棒性。

Result: 广泛的实验表明，本文提出的方法能持续改进下游任务的ID微调性能和OOD鲁棒性，在多种CLIP骨干网络上均优于现有的基于正则化的鲁棒微调方法。

Conclusion: 通过在函数空间中进行正则化并结合一致性约束，本文方法有效解决了现有鲁棒微调方法在提升OOD鲁棒性方面的局限性，在保证ID性能的同时，显著提升了模型的OOD鲁棒性。

Abstract: Robust fine-tuning aims to achieve competitive in-distribution (ID)
performance while maintaining the out-of-distribution (OOD) robustness of a
pre-trained model when transferring it to a downstream task. To remedy this,
most robust fine-tuning methods aim to preserve the pretrained weights,
features, or logits. However, we find that these methods cannot always improve
OOD robustness for different model architectures. This is due to the OOD
robustness requiring the model function to produce stable prediction for input
information of downstream tasks, while existing methods might serve as a poor
proxy for the optimization in the function space. Based on this finding, we
propose a novel regularization that constrains the distance of fine-tuning and
pre-trained model in the function space with the simulated OOD samples, aiming
to preserve the OOD robustness of the pre-trained model. Besides, to further
enhance the OOD robustness capability of the fine-tuning model, we introduce an
additional consistency regularization to promote stable predictions of
perturbed samples. Extensive experiments demonstrate our approach could
consistently improve both downstream task ID fine-tuning performance and OOD
robustness across a variety of CLIP backbones, outperforming existing
regularization-based robust fine-tuning methods.

</details>


### [41] [Safeguarding Graph Neural Networks against Topology Inference Attacks](https://arxiv.org/abs/2509.05429)
*Jie Fu,Hong Yuan,Zhili Chen,Wendy Hui Wang*

Main category: cs.LG

TL;DR: 本研究揭示了图神经网络（GNNs）在拓扑隐私方面的漏洞，提出了拓扑推断攻击（TIAs）来重建图结构，并针对现有防御机制的不足，引入了名为私有图重建（PGR）的新型防御框架，该框架通过双层优化在保护拓扑隐私的同时维持模型精度。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在学习图结构数据方面表现强大，但其广泛应用引发了严重的隐私担忧。现有研究主要关注边级隐私，而图的整体结构（即拓扑隐私）作为一个关键但尚未充分探索的威胁，仍面临被泄露的风险。

Method: 1. 提出了一套拓扑推断攻击（TIAs），通过仅黑盒访问GNN模型即可重建目标训练图的结构。2. 引入了私有图重建（PGR）防御框架，旨在保护拓扑隐私并维持模型准确性。PGR被构建为一个双层优化问题，其中利用元梯度迭代生成合成训练图，并同步更新GNN模型。

Result: 1. 实验发现GNNs极易受到拓扑推断攻击。2. 现有的边级差分隐私机制不足以缓解此类风险，因为它们要么无法有效防御，要么严重损害模型准确性。3. 大量实验证明，PGR能够显著减少拓扑信息泄露，同时对模型准确性影响极小。

Conclusion: GNNs对拓扑推断攻击高度敏感，且现有边级差分隐私机制不足以有效保护拓扑隐私。本文提出的私有图重建（PGR）框架是一种有效的新型防御机制，能够在保护GNN拓扑隐私的同时，最大限度地减少对模型性能的影响。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful models for learning
from graph-structured data. However, their widespread adoption has raised
serious privacy concerns. While prior research has primarily focused on
edge-level privacy, a critical yet underexplored threat lies in topology
privacy - the confidentiality of the graph's overall structure. In this work,
we present a comprehensive study on topology privacy risks in GNNs, revealing
their vulnerability to graph-level inference attacks. To this end, we propose a
suite of Topology Inference Attacks (TIAs) that can reconstruct the structure
of a target training graph using only black-box access to a GNN model. Our
findings show that GNNs are highly susceptible to these attacks, and that
existing edge-level differential privacy mechanisms are insufficient as they
either fail to mitigate the risk or severely compromise model accuracy. To
address this challenge, we introduce Private Graph Reconstruction (PGR), a
novel defense framework designed to protect topology privacy while maintaining
model accuracy. PGR is formulated as a bi-level optimization problem, where a
synthetic training graph is iteratively generated using meta-gradients, and the
GNN model is concurrently updated based on the evolving graph. Extensive
experiments demonstrate that PGR significantly reduces topology leakage with
minimal impact on model accuracy. Our code is anonymously available at
https://github.com/JeffffffFu/PGR.

</details>


### [42] [Neural Breadcrumbs: Membership Inference Attacks on LLMs Through Hidden State and Attention Pattern Analysis](https://arxiv.org/abs/2509.05449)
*Disha Makhija,Manoj Ghuhan Arivazhagan,Vinayshekhar Bannihatti Kumar,Rashmi Gangadharaiah*

Main category: cs.LG

TL;DR: 研究发现，大语言模型的内部表示能揭示传统成员推断攻击（MIA）未捕获的隐私泄露，即使其输出信号看似受保护。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明，MIA对大语言模型的攻击效果仅略优于随机猜测，导致人们认为大规模预训练可能不存在隐私泄露风险。本研究旨在通过审视大语言模型的内部表示而非仅其输出来提供补充视角，以发现潜在的成员推断信号。

Method: 提出`memTrace`框架，通过提取和分析大语言模型处理序列时的“神经痕迹”，包括变压器隐藏状态、注意力模式、层级表示动态、注意力分布特性和跨层转换模式，来检测传统基于损失方法可能无法捕获的记忆指纹。

Result: `memTrace`方法在多个模型家族上实现了强大的成员推断检测，在流行的MIA基准测试中平均AUC分数达到0.85。

Conclusion: 研究结果表明，即使基于输出的信号看似受保护，模型内部行为仍能揭示训练数据暴露的方面。这强调了需要进一步研究成员隐私，并为大语言模型开发更 robust 的隐私保护训练技术。

Abstract: Membership inference attacks (MIAs) reveal whether specific data was used to
train machine learning models, serving as important tools for privacy auditing
and compliance assessment. Recent studies have reported that MIAs perform only
marginally better than random guessing against large language models,
suggesting that modern pre-training approaches with massive datasets may be
free from privacy leakage risks. Our work offers a complementary perspective to
these findings by exploring how examining LLMs' internal representations,
rather than just their outputs, may provide additional insights into potential
membership inference signals. Our framework, \emph{memTrace}, follows what we
call \enquote{neural breadcrumbs} extracting informative signals from
transformer hidden states and attention patterns as they process candidate
sequences. By analyzing layer-wise representation dynamics, attention
distribution characteristics, and cross-layer transition patterns, we detect
potential memorization fingerprints that traditional loss-based approaches may
not capture. This approach yields strong membership detection across several
model families achieving average AUC scores of 0.85 on popular MIA benchmarks.
Our findings suggest that internal model behaviors can reveal aspects of
training data exposure even when output-based signals appear protected,
highlighting the need for further research into membership privacy and the
development of more robust privacy-preserving training techniques for large
language models.

</details>


### [43] [Calibrated Recommendations with Contextual Bandits](https://arxiv.org/abs/2509.05460)
*Diego Feijer,Himan Abdollahpouri,Sanket Gupta,Alexander Clare,Yuxiao Wen,Todd Wasson,Maria Dimakopoulou,Zahra Nazari,Kyle Kretschman,Mounia Lalmas*

Main category: cs.LG

TL;DR: Spotify提出了一种基于上下文赌博机的校准方法，以动态优化用户首页内容类型分布，从而提高参与度，尤其对播客等非音乐内容有显著效果。


<details>
  <summary>Details</summary>
Motivation: Spotify首页内容类型多样，但历史数据严重偏向音乐，导致难以提供平衡且个性化的内容组合。此外，用户对不同内容类型的偏好会随时间、日期或设备而变化。

Method: 提出了一种利用上下文赌博机（Contextual Bandits）的校准方法，动态学习每个用户在不同上下文和偏好下的最优内容类型分布。与传统依赖历史平均值的校准方法不同，该方法能适应用户兴趣在不同上下文中的变化。

Result: 离线和在线实验结果均表明，该方法提高了Spotify首页的精确度和用户参与度，特别是对于播客等代表性不足的内容类型。

Conclusion: 通过上下文赌博机动态校准内容类型分布，可以有效解决历史数据偏差问题，并根据用户上下文变化适应其兴趣，从而显著提升用户体验和参与度。

Abstract: Spotify's Home page features a variety of content types, including music,
podcasts, and audiobooks. However, historical data is heavily skewed toward
music, making it challenging to deliver a balanced and personalized content
mix. Moreover, users' preference towards different content types may vary
depending on the time of day, the day of week, or even the device they use. We
propose a calibration method that leverages contextual bandits to dynamically
learn each user's optimal content type distribution based on their context and
preferences. Unlike traditional calibration methods that rely on historical
averages, our approach boosts engagement by adapting to how users interests in
different content types varies across contexts. Both offline and online results
demonstrate improved precision and user engagement with the Spotify Home page,
in particular with under-represented content types such as podcasts.

</details>


### [44] [PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series](https://arxiv.org/abs/2509.05478)
*Jia Wang,Xiao Wang,Chi Zhang*

Main category: cs.LG

TL;DR: PLanTS是一个周期性感知的自监督学习框架，旨在解决多元时间序列的挑战，通过建模不规则潜在状态及其转换，提高了表示质量和运行效率。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列面临高维度、标注数据有限和非平稳性等挑战。现有自监督学习方法未能考虑其内在周期性结构，也未能捕捉潜在状态的动态演变。

Method: 提出了PLanTS框架，包括：1) 周期感知多粒度分块机制；2) 广义对比损失，以在多时间分辨率上保持实例级和状态级相似性；3) 下一转换预测预训练任务，以捕捉时间动态并编码未来状态演变的预测信息。

Result: PLanTS在多分类、多标签分类、预测、轨迹跟踪和异常检测等广泛的下游任务中，持续提高了表示质量，优于现有自监督学习方法。与基于DTW的方法相比，PLanTS展现出卓越的运行时效率。

Conclusion: PLanTS通过明确建模多元时间序列的周期性和动态潜在状态，有效提升了自监督学习的性能，并在多种应用中实现了更优的表示质量和计算效率。

Abstract: Multivariate time series (MTS) are ubiquitous in domains such as healthcare,
climate science, and industrial monitoring, but their high dimensionality,
limited labeled data, and non-stationary nature pose significant challenges for
conventional machine learning methods. While recent self-supervised learning
(SSL) approaches mitigate label scarcity by data augmentations or time
point-based contrastive strategy, they neglect the intrinsic periodic structure
of MTS and fail to capture the dynamic evolution of latent states. We propose
PLanTS, a periodicity-aware self-supervised learning framework that explicitly
models irregular latent states and their transitions. We first designed a
period-aware multi-granularity patching mechanism and a generalized contrastive
loss to preserve both instance-level and state-level similarities across
multiple temporal resolutions. To further capture temporal dynamics, we design
a next-transition prediction pretext task that encourages representations to
encode predictive information about future state evolution. We evaluate PLanTS
across a wide range of downstream tasks-including multi-class and multi-label
classification, forecasting, trajectory tracking and anomaly detection. PLanTS
consistently improves the representation quality over existing SSL methods and
demonstrates superior runtime efficiency compared to DTW-based methods.

</details>


### [45] [STL-based Optimization of Biomolecular Neural Networks for Regression and Control](https://arxiv.org/abs/2509.05481)
*Eric Palanques-Tost,Hanna Krasowski,Murat Arcak,Ron Weiss,Calin Belta*

Main category: cs.LG

TL;DR: 本文提出利用信号时序逻辑（STL）规范来训练生物分子神经网络（BNNs），以解决其训练数据不足的问题，并在回归和控制任务中取得了高效的表现。


<details>
  <summary>Details</summary>
Motivation: 生物分子神经网络（BNNs）具有通用函数逼近能力，但由于缺乏目标数据，其训练极具挑战性。

Method: 利用信号时序逻辑（STL）规范定义BNNs的训练目标。基于STL的量化语义，实现BNN权重的梯度优化，并引入一种学习算法，使BNNs能够执行生物系统中的回归和控制任务。

Result: 数值实验表明，基于STL的学习方法能有效解决所研究的回归问题（作为失调状态的报告器）和控制问题（慢性病模型中的闭环反馈控制，减少炎症并避免感染不良反应）。

Conclusion: 基于STL的学习方法可以高效地解决生物分子神经网络的回归和控制任务。

Abstract: Biomolecular Neural Networks (BNNs), artificial neural networks with
biologically synthesizable architectures, achieve universal function
approximation capabilities beyond simple biological circuits. However, training
BNNs remains challenging due to the lack of target data. To address this, we
propose leveraging Signal Temporal Logic (STL) specifications to define
training objectives for BNNs. We build on the quantitative semantics of STL,
enabling gradient-based optimization of the BNN weights, and introduce a
learning algorithm that enables BNNs to perform regression and control tasks in
biological systems. Specifically, we investigate two regression problems in
which we train BNNs to act as reporters of dysregulated states, and a feedback
control problem in which we train the BNN in closed-loop with a chronic disease
model, learning to reduce inflammation while avoiding adverse responses to
external infections. Our numerical experiments demonstrate that STL-based
learning can solve the investigated regression and control tasks efficiently.

</details>


### [46] [Prior Distribution and Model Confidence](https://arxiv.org/abs/2509.05485)
*Maksim Kazanskii,Artem Kasianov*

Main category: cs.LG

TL;DR: 本文提出一种基于训练数据嵌入分布的方法，无需重新训练即可评估模型对未知数据的预测置信度，并通过过滤低置信度预测显著提高分类准确性。使用多个嵌入模型可进一步增强置信度估计和性能。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据分布对图像分类模型性能的影响，并提出一种无需重新训练即可理解模型对未知数据预测置信度的方法，从而提高分类准确性。

Method: 通过分析训练集的嵌入，提出一个框架来理解模型预测置信度。该方法根据预测与训练数据在嵌入空间中的距离来过滤低置信度预测。进一步，利用多个嵌入模型来表示训练数据，以实现更鲁棒的置信度估计，并通过结合这些嵌入来更好地检测和排除分布外样本。

Result: 该方法显著提高了分类准确性，并在多种分类模型架构上表现出一致的性能提升。结果表明，使用多个嵌入模型可以更鲁棒地估计置信度，捕获数据互补方面，并能更好地检测和排除分布外样本，从而进一步提高准确性。

Conclusion: 所提出的方法与模型无关且具有普适性，在计算机视觉之外的领域（如自然语言处理）也具有潜在应用，特别是在预测可靠性至关重要的场景中。

Abstract: This paper investigates the impact of training data distribution on the
performance of image classification models. By analyzing the embeddings of the
training set, we propose a framework to understand the confidence of model
predictions on unseen data without the need for retraining. Our approach
filters out low-confidence predictions based on their distance from the
training distribution in the embedding space, significantly improving
classification accuracy. We demonstrate this on the example of several
classification models, showing consistent performance gains across
architectures. Furthermore, we show that using multiple embedding models to
represent the training data enables a more robust estimation of confidence, as
different embeddings capture complementary aspects of the data. Combining these
embeddings allows for better detection and exclusion of out-of-distribution
samples, resulting in further accuracy improvements. The proposed method is
model-agnostic and generalizable, with potential applications beyond computer
vision, including domains such as Natural Language Processing where prediction
reliability is critical.

</details>


### [47] [MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs](https://arxiv.org/abs/2509.05488)
*Hongjun Xu,Junxi Xia,Weisi Yang,Yueyuan Sui,Stephen Xia*

Main category: cs.LG

TL;DR: 首次在微控制器上成功部署Mamba模型，通过C语言实现的MambaLite-Micro推理引擎，显著降低内存占用并保持精度和可移植性。


<details>
  <summary>Details</summary>
Motivation: 将Mamba模型部署到微控制器(MCU)面临挑战，主要原因在于内存有限、缺乏原生算子支持以及缺少嵌入式友好工具链。

Method: 开发了MambaLite-Micro，一个完全基于C语言且无需运行时的推理引擎。该方法通过以下步骤将训练好的PyTorch Mamba模型映射到设备执行：1) 将模型权重导出为轻量级格式；2) 使用C语言手工实现Mamba层及支持算子，并进行算子融合和内存布局优化。

Result: MambaLite-Micro消除了大型中间张量，将峰值内存降低了83.0%，同时相对于PyTorch Mamba实现，平均数值误差仅为1.7x10^-5。在关键词识别(KWS)和人体活动识别(HAR)任务上，其分类精度与PyTorch基线保持100%一致。此外，该引擎在ESP32S3和STM32H7微控制器上均实现了部署，验证了其在异构嵌入式平台上的可移植性。

Conclusion: MambaLite-Micro成功地将先进的Mamba序列模型引入到资源受限的微控制器应用中，为该领域未来的发展奠定了基础。

Abstract: Deploying Mamba models on microcontrollers (MCUs) remains challenging due to
limited memory, the lack of native operator support, and the absence of
embedded-friendly toolchains. We present, to our knowledge, the first
deployment of a Mamba-based neural architecture on a resource-constrained MCU,
a fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline
maps a trained PyTorch Mamba model to on-device execution by (1) exporting
model weights into a lightweight format, and (2) implementing a handcrafted
Mamba layer and supporting operators in C with operator fusion and memory
layout optimization. MambaLite-Micro eliminates large intermediate tensors,
reducing 83.0% peak memory, while maintaining an average numerical error of
only 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on
keyword spotting(KWS) and human activity recognition (HAR) tasks,
MambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully
preserving classification accuracy. We further validated portability by
deploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating
consistent operation across heterogeneous embedded platforms and paving the way
for bringing advanced sequence models like Mamba to real-world
resource-constrained applications.

</details>


### [48] [Self-Aligned Reward: Towards Effective and Efficient Reasoners](https://arxiv.org/abs/2509.05489)
*Peixuan Han,Adit Krishnan,Gerald Friedland,Jiaxuan You,Chris Kong*

Main category: cs.LG

TL;DR: 针对LLM中RL粗糙的二元奖励导致冗余和高成本问题，本文提出自对齐奖励（SAR），一种基于困惑度的自引导信号，能更细致地评估回答质量。结合现有RL算法，SAR使LLM推理准确率提升4%，推理成本降低30%，并实现效率与准确度的帕累托最优权衡。


<details>
  <summary>Details</summary>
Motivation: LLM中可验证奖励信号过于粗糙（仅提供二元正确性反馈），导致推理冗长和计算成本高昂，且现有解决方案常以牺牲准确性为代价。

Method: 提出自对齐奖励（SAR），作为可验证奖励的补充。SAR定义为答案在给定查询条件下的困惑度与独立困惑度之间的相对差异，以鼓励简洁和查询特异性的回答。SAR与PPO、GRPO等主流RL算法结合使用。

Result: 1. SAR能有效区分答案质量，简洁正确的答案得分更高，部分正确优于完全错误。
2. 结合SAR与现有RL算法，LLM推理准确率提高4%，推理成本降低30%。
3. 与基于长度或自信度的奖励信号相比，SAR在正确性和效率之间实现了帕累托最优权衡。
4. SAR在缩短回答的同时保留了高级推理行为，避免了不必要的冗余。

Conclusion: 自对齐奖励（SAR）作为可验证奖励的细粒度补充，前景广阔，能有效提升LLM训练的效率和有效性，同时兼顾准确性和效率。

Abstract: Reinforcement learning with verifiable rewards has significantly advanced
reasoning in large language models (LLMs), but such signals remain coarse,
offering only binary correctness feedback. This limitation often results in
inefficiencies, including overly verbose reasoning and high computational cost,
while existing solutions often compromise accuracy. To address this, we
introduce self-aligned reward (SAR), a self-guided signal that complements
verifiable rewards to encourage both reasoning accuracy and efficiency. SAR is
defined as the relative perplexity difference between an answer conditioned on
the query and the standalone answer, thereby favoring responses that are
concise and query-specific. Quantitative analysis reveals that SAR reliably
distinguishes answer quality: concise, correct answers score higher than
redundant ones, and partially correct answers score higher than entirely
incorrect ones. Evaluation on 4 models across 7 benchmarks shows that
integrating SAR with prevalent RL algorithms like PPO and GRPO improves
accuracy by 4%, while reducing inference cost by 30%. Further analysis
demonstrates that SAR achieves a Pareto-optimal trade-off between correctness
and efficiency compared to reward signals based on length or self-confidence.
We also show that SAR shortens responses while preserving advanced reasoning
behaviors, demonstrating its ability to suppress unnecessary elaboration
without losing critical reasoning. These results highlight the promise of
self-aligned reward as a fine-grained complement to verifiable rewards, paving
the way for more efficient and effective LLM training.

</details>


### [49] [DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training](https://arxiv.org/abs/2509.05542)
*Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DreamPRM-1.5是一个实例重加权框架，通过双层优化处理多模态过程奖励模型（PRM）训练中的分布偏移和噪声数据问题，并在MMMU基准测试中超越GPT-5。


<details>
  <summary>Details</summary>
Motivation: 多模态过程奖励模型（PRM）的训练面临分布偏移和数据噪声的挑战。

Method: 引入了DreamPRM-1.5，一个通过双层优化自适应调整每个训练样本重要性的实例重加权框架。设计了两种互补策略：适用于小数据集的Instance Table和可扩展至大数据集的Instance Net。该方法还集成到测试时缩放中。

Result: DreamPRM-1.5在MMMU基准测试中取得了84.6%的准确率，超越了GPT-5。

Conclusion: DreamPRM-1.5有效解决了多模态PRM训练的挑战，并在MMMU基准上表现出优于现有模型的性能。

Abstract: Training multimodal process reward models (PRMs) is challenged by
distribution shifts and noisy data. We introduce DreamPRM-1.5, an
instance-reweighted framework that adaptively adjusts the importance of each
training example via bi-level optimization. We design two complementary
strategies: Instance Table, effective for smaller datasets, and Instance Net,
scalable to larger ones. Integrated into test-time scaling, DreamPRM-1.5
achieves 84.6 accuracy on the MMMU benchmark, surpassing GPT-5.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [50] [Distributed Link Sparsification for Scalable Scheduling Using Graph Neural Networks (Journal Version)](https://arxiv.org/abs/2509.05447)
*Zhongyuan Zhao,Gunjan Verma,Ananthram Swami,Santiago Segarra*

Main category: cs.NI

TL;DR: 使用图神经网络（GNNs）的分布式链路稀疏化方案，旨在减少密集无线网络中的调度开销，缓解拥堵并缩小无线电足迹，同时保持网络容量。


<details>
  <summary>Details</summary>
Motivation: 在密集连接的无线网络中，分布式链路调度算法会产生大量信令开销，加剧网络拥堵、能耗和无线电足迹扩大等问题。

Method: 提出一种基于图神经网络（GNNs）的分布式链路稀疏化方案，通过GNN模块根据流量统计和网络拓扑调整链路竞争阈值，使可能失败的链路退出调度竞争。该方案采用一种新颖的离线约束无监督学习算法，以平衡最小化调度开销和确保总效用达到要求水平的目标。

Result: 在高达500个链路的模拟无线多跳网络中，所提出的链路稀疏化技术有效缓解了网络拥堵，并显著减小了四种不同分布式链路调度协议的无线电足迹。

Conclusion: 该研究通过基于GNN的分布式链路稀疏化方案，在保持网络容量的同时，成功地降低了密集无线网络中分布式链路调度产生的信令开销，有效缓解了网络拥堵和无线电足迹问题。

Abstract: In wireless networks characterized by dense connectivity, the significant
signaling overhead generated by distributed link scheduling algorithms can
exacerbate issues like congestion, energy consumption, and radio footprint
expansion. To mitigate these challenges, we propose a distributed link
sparsification scheme employing graph neural networks (GNNs) to reduce
scheduling overhead for delay-tolerant traffic while maintaining network
capacity. A GNN module is trained to adjust contention thresholds for
individual links based on traffic statistics and network topology, enabling
links to withdraw from scheduling contention when they are unlikely to succeed.
Our approach is facilitated by a novel offline constrained {unsupervised}
learning algorithm capable of balancing two competing objectives: minimizing
scheduling overhead while ensuring that total utility meets the required level.
In simulated wireless multi-hop networks with up to 500 links, our link
sparsification technique effectively alleviates network congestion and reduces
radio footprints across four distinct distributed link scheduling protocols.

</details>


### [51] [Joint Routing, Resource Allocation, and Energy Optimization for Integrated Access and Backhaul with Open RAN](https://arxiv.org/abs/2509.05467)
*Reshma Prasad,Maxime Elkael,Gabriele Gemmi,Osama M. Bushnaq,Debashisha Mishra,Prasanna Raut,Jennifer Simonjan,Michele Polese,Tommaso Melodia*

Main category: cs.NI

TL;DR: 本论文提出并验证了在6G IAB网络中，通过联合优化路由与资源分配，实现能耗最小化和吞吐量最大化的两种实用方法，并利用O-RAN架构进行集成，以应对未来网络挑战。


<details>
  <summary>Details</summary>
Motivation: 随着网络向6G演进，移动网络运营商面临满足多样化需求和管理日益增长能耗的双重挑战。集成接入和回传（IAB）网络虽能降低部署复杂性，但其多跳无线回传特性要求精确的路由和资源分配决策，同时小区致密化使得能耗优化至关重要。

Method: 本文提出了一个将功率水平与可实现数据速率相关联的新型容量模型。开发了两种实用的大规模方法，用于联合优化IAB网络中的路由和资源分配问题，目标分别为能耗最小化和吞吐量最大化。这些解决方案利用Open Radio Access Network (O-RAN) 架构的闭环控制框架进行集成，并使用意大利米兰市两个月交通流量的开放数据进行评估。

Result: 研究结果表明，所提出的方法能有效减少激活节点数量以节约能源，并在日高峰时段使用FR3（或上中频段）频谱时，为每个用户设备（UE）实现了约100 Mbps的最小数据速率。

Conclusion: 研究结果验证了所提出的框架在下一代IAB网络部署和优化中的实际适用性。

Abstract: As networks evolve towards 6G, Mobile Network Operators (MNOs) must
accommodate diverse requirements and at the same time manage rising energy
consumption. Integrated Access and Backhaul (IAB) networks facilitate dense
cellular deployments with reduced infrastructure complexity. However, the
multi-hop wireless backhauling in IAB networks necessitates proper routing and
resource allocation decisions to meet the performance requirements. At the same
time, cell densification makes energy optimization crucial. This paper
addresses the joint optimization of routing and resource allocation in IAB
networks through two distinct objectives: energy minimization and throughput
maximization. We develop a novel capacity model that links power levels to
achievable data rates. We propose two practical large-scale approaches to solve
the optimization problems and leverage the closed-loop control framework
introduced by the Open Radio Access Network (O-RAN) architecture to integrate
the solutions. The approaches are evaluated on diverse scenarios built upon
open data of two months of traffic collected by network operators in the city
of Milan, Italy. Results show that the proposed approaches effectively reduces
number of activated nodes to save energy and achieves approximately 100 Mbps of
minimum data rate per User Equipment (UE) during peak hours of the day using
spectrum within the Frequency Range (FR) 3, or upper midband. The results
validate the practical applicability of our framework for next-generation IAB
network deployment and optimization.

</details>


### [52] [Tiga: Accelerating Geo-Distributed Transactions with Synchronized Clocks [Technical Report]](https://arxiv.org/abs/2509.05759)
*Jinkun Geng,Shuai Mu,Anirudh Sivaraman,Balaji Prabhakar*

Main category: cs.NI

TL;DR: Tiga是一种为地理复制和可伸缩事务数据库设计的新方案，旨在以1 WRTT低延迟和高吞吐量提交事务。


<details>
  <summary>Details</summary>
Motivation: 现有地理复制事务数据库在多种场景下难以实现1 WRTT的低延迟事务提交，同时保持高吞吐量和最小计算开销。

Method: Tiga整合了并发控制和共识机制，在一个轮次内完成严格可序列化执行和一致性复制。它利用同步时钟为事务预先分配未来时间戳进行排序，通常可实现1 WRTT提交；在少数异常情况下，会回退到1.5-2 WRTT的慢路径。

Result: Tiga在1 WRTT延迟下能提交更多事务，且吞吐量开销更低。实验结果表明，Tiga的吞吐量比基线高1.3-7.2倍，延迟低1.4-4.6倍。

Conclusion: Tiga通过创新设计，显著提升了地理复制事务数据库的性能，实现了在广泛场景下的1 WRTT低延迟事务提交和高吞吐量。

Abstract: This paper presents Tiga, a new design for geo-replicated and scalable
transactional databases such as Google Spanner. Tiga aims to commit
transactions within 1 wide-area roundtrip time, or 1 WRTT, for a wide range of
scenarios, while maintaining high throughput with minimal computational
overhead. Tiga consolidates concurrency control and consensus, completing both
strictly serializable execution and consistent replication in a single round.
It uses synchronized clocks to proactively order transactions by assigning each
a future timestamp at submission. In most cases, transactions arrive at servers
before their future timestamps and are serialized according to the designated
timestamp, requiring 1 WRTT to commit. In rare cases, transactions are delayed
and proactive ordering fails, in which case Tiga falls back to a slow path,
committing in 1.5--2 WRTTs. Compared to state-of-the-art solutions, Tiga can
commit more transactions at 1-WRTT latency, and incurs much less throughput
overhead. Evaluation results show that Tiga outperforms all baselines,
achieving 1.3--7.2$\times$ higher throughput and 1.4--4.6$\times$ lower
latency. Tiga is open-sourced at
https://github.com/New-Consensus-Concurrency-Control/Tiga.

</details>


### [53] [On-Dyn-CDA: A Real-Time Cost-Driven Task Offloading Algorithm for Vehicular Networks with Reduced Latency and Task Loss](https://arxiv.org/abs/2509.05889)
*Mahsa Paknejad,Parisa Fard Moshiri,Murat Simsek,Burak Kantarci,Hussein T. Mouftah*

Main category: cs.NI

TL;DR: 针对车辆网络实时任务处理，提出On-Dyn-CDA算法，在复杂场景下显著降低任务延迟和丢失，且无需训练，高效实用。


<details>
  <summary>Details</summary>
Motivation: 车辆网络实时任务处理面临低延迟和低任务丢弃率的挑战。研究目标是最大化完成任务数，同时最小化总延迟并减少任务丢弃。

Method: 提出在线动态成本驱动算法 (On-Dyn-CDA)。通过考虑任务可用性（静态/动态）和卸载决策（在线/离线，在线版考虑执行时间）对算法进行设计和评估。对比了包括静态PSO在内的多种基线算法。

Result: 在最复杂场景下，On-Dyn-CDA执行时间仅0.05秒，远优于动态PSO的1330.05秒。任务丢失率优于动态PSO 3.42%，平均延迟降低29.22%。该算法无需数据集和训练。

Conclusion: On-Dyn-CDA算法在车辆网络实时任务处理中表现出卓越性能，显著提升了效率并降低了任务丢失和延迟。其低计算复杂度和无需训练的特点，使其在动态环境中具有高度实用性和可扩展性。

Abstract: Real-time task processing is a critical challenge in vehicular networks,
where achieving low latency and minimizing dropped task ratio depend on
efficient task execution. Our primary objective is to maximize the number of
completed tasks while minimizing overall latency, with a particular focus on
reducing number of dropped tasks. To this end, we investigate both static and
dynamic versions of an optimization algorithm. The static version assumes full
task availability, while the dynamic version manages tasks as they arrive. We
also distinguish between online and offline cases: the online version
incorporates execution time into the offloading decision process, whereas the
offline version excludes it, serving as a theoretical benchmark for optimal
performance. We evaluate our proposed Online Dynamic Cost-Driven Algorithm
(On-Dyn-CDA) against these baselines. Notably, the static Particle Swarm
Optimization (PSO) baseline assumes all tasks are transferred to the RSU and
processed by the MEC, and its offline version disregards execution time, making
it infeasible for real-time applications despite its optimal performance in
theory. Our novel On-Dyn-CDA completes execution in just 0.05 seconds under the
most complex scenario, compared to 1330.05 seconds required by Dynamic PSO. It
also outperforms Dynamic PSO by 3.42% in task loss and achieves a 29.22%
reduction in average latency in complex scenarios. Furthermore, it requires
neither a dataset nor a training phase, and its low computational complexity
ensures efficiency and scalability in dynamic environments.

</details>


### [54] [ALPHA: LLM-Enabled Active Learning for Human-Free Network Anomaly Detection](https://arxiv.org/abs/2509.05936)
*Xuanhao Luo,Shivesh Madan Nath Jha,Akruti Sinha,Zhizhen Li,Yuchen Liu*

Main category: cs.NI

TL;DR: 本文提出ALPHA，一个无人工干预的日志分析主动学习流程，利用LLM辅助的少样本标注和聚类传播，实现与全监督方法媲美的异常检测精度，并支持可解释的根因分析。


<details>
  <summary>Details</summary>
Motivation: 传统的日志异常检测和根因分析方法高度依赖专家知识或全监督模型，需要大量标注数据和巨大的人力投入，因此需要解决数据标注和人力成本高昂的挑战。

Method: ALPHA是一个主动学习流程，它集成了语义嵌入、基于聚类的代表性采样和大型语言模型（LLM）辅助的少样本标注。LLM标注的标签会在集群中传播，以少量监督训练异常检测器。为了提高标注准确性，提出了一种两步少样本细化策略，根据LLM的错误模式自适应选择提示。此外，它还在检测后阶段通过LLM驱动提供根因解释。

Result: 在真实世界日志数据集上的实验表明，ALPHA在检测精度上与全监督方法相当，同时显著减少了人工投入。它还支持通过LLM驱动的根因解释进行可解释分析。

Conclusion: ALPHA提供了一个可扩展、经济高效的解决方案，实现了真正自动化的基于日志的异常检测，并支持可解释性。

Abstract: Network log data analysis plays a critical role in detecting security threats
and operational anomalies. Traditional log analysis methods for anomaly
detection and root cause analysis rely heavily on expert knowledge or fully
supervised learning models, both of which require extensive labeled data and
significant human effort. To address these challenges, we propose ALPHA, the
first Active Learning Pipeline for Human-free log Analysis. ALPHA integrates
semantic embedding, clustering-based representative sampling, and large
language model (LLM)-assisted few-shot annotation to automate the anomaly
detection process. The LLM annotated labels are propagated across clusters,
enabling large-scale training of an anomaly detector with minimal supervision.
To enhance the annotation accuracy, we propose a two-step few-shot refinement
strategy that adaptively selects informative prompts based on the LLM's
observed error patterns. Extensive experiments on real-world log datasets
demonstrate that ALPHA achieves detection accuracy comparable to fully
supervised methods while mitigating human efforts in the loop. ALPHA also
supports interpretable analysis through LLM-driven root cause explanations in
the post-detection stage. These capabilities make ALPHA a scalable and
cost-efficient solution for truly automated log-based anomaly detection.

</details>


### [55] [An Axiomatic Analysis of Path Selection Strategies for Multipath Transport in Path-Aware Networks](https://arxiv.org/abs/2509.05938)
*Alissa Baumeister,Sina Keshvadi*

Main category: cs.NI

TL;DR: 本文通过公理分析和模拟，量化了路径感知网络中多路径传输在个体性能优化和网络稳定性之间的权衡。研究表明，混合策略（如Epsilon-Greedy）能有效解决纯贪婪策略导致的灾难性丢包和纯协作策略导致的路径利用不足问题，是下一代网络路径选择机制的关键。


<details>
  <summary>Details</summary>
Motivation: 路径感知网络（如SCION）与多路径传输协议（如MPTCP/MPQUIC）的结合有望显著提升性能和策略执行，但也带来了个体性能优化与整体网络稳定性之间的巨大权衡。本研究旨在通过严格的公理分析量化这一权衡。

Method: 研究采用严格的公理分析方法，并在模拟的路径感知环境中评估了一系列算法，包括贪婪（Min-RTT）、协作（Round-Robin）以及混合方法（Epsilon-Greedy）。评估标准基于效率、避免丢包、稳定性和公平性等公理。

Result: 模拟结果显示，纯贪婪策略在低竞争下高效，但在高竞争下会导致灾难性丢包（增长超18,000%），引发严重的网络不稳定。相反，协作策略能确保公平性和稳定性，但代价是高容量路径利用不足。研究证明，混合策略能解决此冲突，例如Epsilon-Greedy算法在高竞争场景下实现了最高的效率，并有效缓解了贪婪方法固有的不稳定性。

Conclusion: 公理分析表明，可调谐的混合算法对于设计下一代网络中鲁棒且高性能的路径选择机制至关重要。

Abstract: Path-aware networking architectures like SCION provide end-hosts with
explicit control over inter-domain routing, while multipath transport protocols
like MPTCP and MPQUIC enable the concurrent use of multiple paths. This
combination promises significant gains in performance and policy enforcement,
but it also creates a stark trade-off between individual performance
optimization and overall network stability. This paper quantifies this
trade-off through a rigorous axiomatic analysis. We evaluate a spectrum of
algorithms, from greedy (Min-RTT) and cooperative (Round-Robin) to hybrid
approaches (Epsilon-Greedy), against axioms of Efficiency, Loss Avoidance,
Stability, and Fairness in a simulated path-aware environment.
  Our simulations reveal that purely greedy strategies, while efficient under
low contention, induce catastrophic packet loss, increasing by over >18,000% as
the number of competing agents grow, due to herd effects that cause severe
network instability. Conversely, cooperative strategies ensure fairness and
stability but at the cost of underutilizing high-capacity paths. Crucially, we
demonstrate that hybrid strategies resolve this conflict. The Epsilon-Greedy
algorithm, for instance, achieves the highest efficiency of all tested
strategies in high-contention scenarios while mitigating the instability
inherent to the greedy approach. Our axiomatic analysis suggests that tunable,
hybrid algorithms are essential for designing robust and high-performance path
selection mechanisms for next-generation networks.

</details>


### [56] [Large Language Models for Next-Generation Wireless Network Management: A Survey and Tutorial](https://arxiv.org/abs/2509.05946)
*Bisheng Wei,Ruihong Jiang,Ruichen Zhang,Yinqiu Liu,Dusit Niyato,Yaohua Sun,Yang Lu,Yonghui Li,Shiwen Mao,Chau Yuen,Marco Di Renzo,Mugen Peng*

Main category: cs.NI

TL;DR: 本文对大语言模型（LLMs）赋能的无线网络优化框架进行了全面综述，旨在解决6G网络中传统优化方法面临的复杂性与局限性。


<details>
  <summary>Details</summary>
Motivation: 随着6G无线网络的发展，资源分配和轨迹设计等优化问题日益复杂且规模庞大。传统优化方法（如启发式、深度强化学习）难以满足实时适应性、可扩展性和动态用户意图处理的需求。LLMs凭借其先进的语义理解和结构化推理能力，有望通过自然语言驱动的问题制定和自适应解决方案来克服这些挑战。

Method: 本文采用系统性综述方法，全面分析了为无线网络定制的LLM赋能优化框架。具体涵盖：介绍基础设计概念、区分LLM与传统优化范式、批判性分析关键使能方法（包括自然语言建模、求解器协作和解决方案验证），并探讨了代表性案例研究。

Result: 综述展示了LLMs在优化问题制定、低空经济网络和意图网络等实际场景中的变革性潜力。论文详细分析了LLM赋能优化框架的设计原理、关键技术及其在下一代无线网络中的应用前景。

Conclusion: 论文讨论了当前研究挑战、审查了主流开源框架和数据集，并指明了未来有前景的研究方向，以期促进为下一代无线网络开发稳健、可扩展和可信赖的LLM赋能优化解决方案。

Abstract: The rapid advancement toward sixth-generation (6G) wireless networks has
significantly intensified the complexity and scale of optimization problems,
including resource allocation and trajectory design, often formulated as
combinatorial problems in large discrete decision spaces. However, traditional
optimization methods, such as heuristics and deep reinforcement learning (DRL),
struggle to meet the demanding requirements of real-time adaptability,
scalability, and dynamic handling of user intents in increasingly heterogeneous
and resource-constrained network environments. Large language models (LLMs)
present a transformative paradigm by enabling natural language-driven problem
formulation, context-aware reasoning, and adaptive solution refinement through
advanced semantic understanding and structured reasoning capabilities. This
paper provides a systematic and comprehensive survey of LLM-enabled
optimization frameworks tailored for wireless networks. We first introduce
foundational design concepts and distinguish LLM-enabled methods from
conventional optimization paradigms. Subsequently, we critically analyze key
enabling methodologies, including natural language modeling, solver
collaboration, and solution verification processes. Moreover, we explore
representative case studies to demonstrate LLMs' transformative potential in
practical scenarios such as optimization formulation, low-altitude economy
networking, and intent networking. Finally, we discuss current research
challenges, examine prominent open-source frameworks and datasets, and identify
promising future directions to facilitate robust, scalable, and trustworthy
LLM-enabled optimization solutions for next-generation wireless networks.

</details>


### [57] [Optimized Split Computing Framework for Edge and Core Devices](https://arxiv.org/abs/2509.06049)
*Andrea Tassi,Oluwatayo Yetunde Kolawole,Joan Pujol Roig,Daniel Warren*

Main category: cs.NI

TL;DR: 为解决资源受限UE运行前馈神经网络的挑战，本文提出一种优化框架，将FFNN模型分割并在UE、边缘和核心节点上协同执行，以降低UE计算负担和推理时间。


<details>
  <summary>Details</summary>
Motivation: 移动网络需支持高要求的服务以确保高质量用户体验，这使得在用户设备（UE）上应用前馈神经网络（FFNN）模型变得至关重要。然而，UE资源有限，直接在其上运行FFNN面临固有挑战。

Method: 本文提出一个用于分体式计算应用的优化框架，将FFNN模型划分为多个部分，由UE、边缘和核心节点共同执行。此外，还提供了一种高效的启发式策略来解决该优化问题。

Result: 所提出的框架能有效减少UE所需的计算足迹，同时控制推理时间。该框架在异构环境下表现出鲁棒性，消除了再训练的需求，并使UE的内存（CPU）占用分别减少了超过33.6%（60%）。

Conclusion: 通过将FFNN模型分割并在UE、边缘和核心节点上协同执行的优化框架，可以显著降低资源受限UE的计算负担和推理时间，实现高效的移动网络服务支持。

Abstract: With mobile networks expected to support services with stringent requirements
that ensure high-quality user experience, the ability to apply Feed-Forward
Neural Network (FFNN) models to User Equipment (UE) use cases has become
critical. Given that UEs have limited resources, running FFNNs directly on UEs
is an intrinsically challenging problem. This letter proposes an optimization
framework for split computing applications where an FFNN model is partitioned
into multiple sections, and executed by UEs, edge- and core-located nodes to
reduce the required UE computational footprint while containing the inference
time. An efficient heuristic strategy for solving the optimization problem is
also provided. The proposed framework is shown to be robust in heterogeneous
settings, eliminating the need for retraining and reducing the UE's memory
(CPU) footprint by over 33.6% (60%).

</details>


### [58] [Understanding BBRv3 Performance in AQM-Enabled WiFi Networks](https://arxiv.org/abs/2509.06245)
*Shyam Kumar Shrestha,Jonathan Kua,Shiva Raj Pokhrel*

Main category: cs.NI

TL;DR: 本文提出一个模块化测试平台和可视化工具，用于评估无线网络中TCP拥塞控制性能。通过比较BBRv3和CUBIC在不同AQM方案下的表现，发现BBRv3显著改善了公平性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 需要在无线网络环境中评估TCP拥塞控制算法（尤其是BBRv3等新变体）的性能，并缺乏实用的测试平台和可视化工具。

Method: 开发了一个模块化实验测试平台和轻量级可视化工具。在Wi-Fi链路上使用商用MikroTik路由器，比较了Google的BBRv3算法与基于丢包的CUBIC算法在PFIFO、FQ-CoDel和CAKE等不同AQM方案下的性能。实时仪表板可视化了吞吐量、延迟和公平性等指标。

Result: 研究结果显示，BBRv3在AQM下显著改善了公平性和收敛性，尤其是在使用FQ-CoDel方案时表现更优。

Conclusion: 所开发的测试平台和可视化工具为在真实AQM家庭无线网络中评估下一代TCP变体提供了实用的基础。BBRv3在这些环境中表现出色，提高了公平性和收敛性。

Abstract: We present a modular experimental testbed and lightweight visualization tool
for evaluating TCP congestion control performance in wireless networks. We
compare Google's latest Bottleneck Bandwidth and Round-trip time version 3
(BBRv3) algorithm with loss-based CUBIC under varying Active Queue Management
(AQM) schemes, namely PFIFO, FQ-CoDel, and CAKE, on a Wi-Fi link using a
commercial MikroTik router. Our real-time dashboard visualizes metrics such as
throughput, latency, and fairness across competing flows. Results show that
BBRv3 significantly improves fairness and convergence under AQM, especially
with FQ-CoDel. Our visualization tool and modular testbed provide a practical
foundation for evaluating next-generation TCP variants in real-world
AQM-enabled home wireless networks.

</details>


### [59] [Network-Aware Control of AGVs in an Industrial Scenario: A Simulation Study Based on ROS 2 and Gazebo](https://arxiv.org/abs/2509.06451)
*Filippo Bragato,Tullia Fontana,Marco Giordani,Malte Schellmann,Josef Eichinger,Michele Zorzi*

Main category: cs.NI

TL;DR: 本文提出了一个基于Gazebo和ROS 2的仿真框架，用于模拟和可视化网络化控制系统中AGV控制与底层通信网络的复杂交互，并分析了网络性能与控制精度之间的关联。


<details>
  <summary>Details</summary>
Motivation: 在工业环境中，AGV的NCS应用具有前景，但通信和控制的紧密耦合（JCC）意味着网络问题（如延迟、错误）可能导致AGV偏离预定路径。因此，需要研究这种复杂交互。

Method: 开发了一个基于Gazebo和ROS 2的仿真框架，明确纳入了通信指标（如延迟、丢包率）和控制指标（AGV实际路径与期望路径之间的均方误差MSE）。

Result: 研究结果揭示了网络性能（特别是包接收率PRR）与控制精度之间的相关性。

Conclusion: 该仿真框架有助于理解AGV在NCS中网络性能与控制精度之间的关联。

Abstract: Networked Control System (NCS) is a paradigm where sensors, controllers, and
actuators communicate over a shared network. One promising application of NCS
is the control of Automated Guided Vehicles (AGVs) in the industrial
environment, for example to transport goods efficiently and to autonomously
follow predefined paths or routes. In this context, communication and control
are tightly correlated, a paradigm referred to as Joint Communication and
Control (JCC), since network issues such as delays or errors can lead to
significant deviations of the AGVs from the planned trajectory. In this paper,
we present a simulation framework based on Gazebo and Robot Operating System 2
(ROS 2) to simulate and visualize, respectively, the complex interaction
between the control of AGVs and the underlying communication network. This
framework explicitly incorporates communication metrics, such as delay and
packet loss, and control metrics, especially the Mean Squared Error (MSE)
between the optimal/desired and actual path of the AGV in response to driving
commands. Our results shed light into the correlation between the network
performance, particularly Packet Reception Ratio (PRR), and accuracy of
control.

</details>


### [60] [Empirical Evaluation of a 5G Transparent Clock for Time Synchronization in a TSN-5G Network](https://arxiv.org/abs/2509.06454)
*Julia Caleya-Sanchez,Pablo Muñoz,Jorge Sánchez-Garrido,Emilio Florentín,Felix Delgado-Ferro,Pablo Rodriguez-Martin,Pablo Ameigeiras*

Main category: cs.NI

TL;DR: 本文实证评估了TSN-5G网络中5G端到端透明时钟（TC）的性能，在商业设备上实现了500纳秒的峰峰值同步精度，满足了工业需求。


<details>
  <summary>Details</summary>
Motivation: 工业物联网和工业4.0/5.0应用急需时间同步，但TSN-5G网络中由于抖动和非对称延迟，实现高精度同步极具挑战。透明时钟（TC）作为一种有前景的5G同步模式，目前缺乏在TSN-5G网络中的实证评估。

Method: 本文在商用TSN交换机上实现了5G端到端透明时钟（TC），其中TC开发包括计算5G驻留时间并恢复从节点时钟域。通过部署包含商用设备的TSN-5G测试床，并修改PTP消息传输速率来评估同步性能。

Result: 实验结果显示，峰峰值同步精度达到了500纳秒，满足了工业应用小于1微秒的要求，并在特定PTP消息传输速率下实现了最小的同步偏移。

Conclusion: 实证结果表明，在商用设备上实现的TSN-5G网络中的5G端到端透明时钟，能够达到满足工业应用（<1微秒）的高精度时间同步要求（500纳秒峰峰值）。

Abstract: Time synchronization is essential for industrial IoT and Industry 4.0/5.0
applications, but achieving high synchronization accuracy in Time-Sensitive
Networking (TSN)-5G networks is challenging due to jitter and asymmetric
delays. 3GPP TS 23.501 defines three 5G synchronization modes: time-aware
system, boundary clock (BC), and transparent clock (TC), where TC offers a
promising solution. However, to the best of our knowledge, there is no
empirical evaluation of TC in a TSN-5G network. This paper empirically
evaluates an 5G end-to-end TC in a TSN-5G network, implemented on commercial
TSN switches with a single clock. For TC development, we compute the residence
time in 5G and recover the clock domain at the slave node. We deploy a TSN-5G
testbed with commercial equipment for synchronization evaluation by modifying
the Precision Timing Protocol (PTP) message transmission rates. Experimental
results show a peak-to-peak synchronization of 500 ns, meeting the industrial
requirement of < 1 us, with minimal synchronization offsets for specific PTP
message transmission rates.

</details>


### [61] [Five Blind Men and the Internet: Towards an Understanding of Internet Traffic](https://arxiv.org/abs/2509.06515)
*Ege Cem Kirci,Ayush Mishra,Laurent Vanbever*

Main category: cs.NI

TL;DR: 通过分析全球472个IXP为期两年的流量数据，揭示了互联网流量的增长趋势、模式和利用率，并证明IXP流量可作为衡量整体互联网动态的可靠代理。


<details>
  <summary>Details</summary>
Motivation: 当前互联网缺乏透明、细粒度的流量模式、容量及增长趋势视图，这阻碍了网络社区对互联网动态的理解。

Method: 利用全球472个互联网交换点（IXP）的公开流量统计数据，进行了一项为期两年的综合研究（2023-2024年），覆盖了全球87%的IXP端口容量。

Result: 全球流量增长了49.2%（年化24.5%）；发现了区域性日间模式和事件驱动的异常；展示了稳定的利用率，反映了可预测的基础设施扩展；并通过分析偏差和确认高度自相似性，证实IXP流量是互联网整体增长和使用行为的可靠代理。

Conclusion: 该研究为长期互联网流量监控提供了一个可验证的基础，并为研究人员和运营商提供了一个可访问的框架，以探索互联网不断演进的生态系统，尤其揭示了网络设计与功能之间的相互作用。

Abstract: The Internet, the world's largest and most pervasive network, lacks a
transparent, granular view of its traffic patterns, volumes, and growth trends,
hindering the networking community's understanding of its dynamics. This paper
leverages publicly available Internet Exchange Point traffic statistics to
address this gap, presenting a comprehensive two-year study (2023-2024) from
472 IXPs worldwide, capturing approximately 300 Tbps of peak daily aggregate
traffic by late 2024. Our analysis reveals a 49.2% global traffic increase
(24.5% annualized), uncovers regionally distinct diurnal patterns and
event-driven anomalies, and demonstrates stable utilization rates, reflecting
predictable infrastructure scaling. By analyzing biases and confirming high
self-similarity, we establish IXP traffic as a robust proxy for overall
Internet growth and usage behavior. With transparent, replicable data--covering
87% of the worldwide IXP port capacity--and plans to release our dataset, this
study offers a verifiable foundation for long-term Internet traffic monitoring.
In particular, our findings shed light on the interplay between network design
and function, providing an accessible framework for researchers and operators
to explore the Internet's evolving ecosystem.

</details>
