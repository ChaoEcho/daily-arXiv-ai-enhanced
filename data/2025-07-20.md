<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 20]
- [cs.CV](#cs.CV) [Total: 16]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 16]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.IT](#cs.IT) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models](https://arxiv.org/abs/2507.12547)
*Lionel Wong,Katherine M. Collins,Lance Ying,Cedegao E. Zhang,Adrian Weller,Tobias Gersternberg,Timothy O'Donnell,Alexander K. Lew,Jacob D. Andreas,Joshua B. Tenenbaum,Tyler Brooke-Wilson*

Main category: cs.CL

TL;DR: 本文提出一种“模型合成架构”（MSA），结合语言模型与概率程序，旨在模仿人类在陌生情境下利用广泛背景知识进行连贯推理的能力。实验证明，MSA在类人开放式推理任务中优于纯语言模型基线。


<details>
  <summary>Details</summary>
Motivation: 探索人类在面对陌生情境时，如何从广泛背景知识中提取相关信息并进行连贯推理。研究假设人类通过结合分布式和符号表示来构建量身定制的心理模型。

Method: 提出“模型合成架构”（MSA），它使用语言模型实现全局相关性检索和模型合成，并使用概率程序构建连贯的世界模型。通过一个名为“模型奥运会”的新颖推理数据集（包含体育情境），评估MSA在涉及新颖因果结构、背景知识和新变量的类人开放式推理任务中的表现。

Result: MSA方法在模拟人类判断方面，表现优于仅使用语言模型的基线方法（包括直接生成和思维链生成）。

Conclusion: 研究结果表明MSA能够模拟人类对全局相关变量进行局部连贯推理的能力，为理解和复制人类在开放领域中的推理提供了可行途径。

Abstract: When faced with novel situations, people are able to marshal relevant
considerations from a wide range of background knowledge and put these to use
in inferences and predictions. What permits us to draw in globally relevant
information and reason over it coherently? Here, we explore the hypothesis that
people use a combination of distributed and symbolic representations to
construct bespoke mental models tailored to novel situations. We propose a
computational implementation of this idea -- a ``Model Synthesis Architecture''
(MSA) -- using language models to implement global relevance-based retrieval
and model synthesis and probabilistic programs to implement bespoke, coherent
world models. We evaluate our MSA as a model of human judgments on a novel
reasoning dataset. The dataset -- built around a `Model Olympics` domain of
sports vignettes -- tests models' capacity for human-like, open-ended reasoning
by requiring (i) judgments about novel causal structures described in language;
(ii) drawing on large bodies of background knowledge; and (iii) doing both in
light of observations that introduce arbitrary novel variables. Our MSA
approach captures human judgments better than language model-only baselines,
under both direct and chain-of-thought generations from the LM that supports
model synthesis. These results suggest that MSAs can be implemented in a way
that mirrors people's ability to deliver locally coherent reasoning over
globally relevant variables, offering a path to understanding and replicating
human reasoning in open-ended domains.

</details>


### [2] [Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility](https://arxiv.org/abs/2507.12553)
*Michael A. Lepori,Jennifer Hu,Ishita Dasgupta,Roma Patel,Thomas Serre,Ellie Pavlick*

Main category: cs.CL

TL;DR: 本文通过识别语言模型中的“模态差异向量”，发现语言模型比先前认为的更能可靠地进行模态分类，且这些向量的出现与模型能力提升一致。此外，这些向量还能用于建模人类的精细模态分类行为，为理解人类认知提供新视角。


<details>
  <summary>Details</summary>
Motivation: 语言模型在多种任务中广泛应用，需要准确识别句子的模态类别（如可能性、不可能性、荒谬性）。然而，近期研究对语言模型进行模态分类的能力提出了质疑。

Method: 研究者在多种语言模型中识别了区分模态类别的线性表征，称之为“模态差异向量”。他们分析了这些向量，并使用机械可解释性技术，将模态差异向量的投影与人类参与者对可解释特征的评分进行关联，以模拟人类的精细分类行为。

Result: 分析显示，语言模型能够比之前报道的更可靠地进行模态分类判断。模态差异向量随着模型的成熟（训练步骤、层数、参数量）以一致的顺序出现。这些从语言模型激活中识别出的向量可以有效地模拟人类的精细模态分类行为。

Conclusion: 本研究利用机械可解释性技术深入理解了语言模型的模态分类能力，并为理解人类如何区分模态类别提供了潜在的新视角。

Abstract: Language models (LMs) are used for a diverse range of tasks, from question
answering to writing fantastical stories. In order to reliably accomplish these
tasks, LMs must be able to discern the modal category of a sentence (i.e.,
whether it describes something that is possible, impossible, completely
nonsensical, etc.). However, recent studies have called into question the
ability of LMs to categorize sentences according to modality (Michaelov et al.,
2025; Kauf et al., 2023). In this work, we identify linear representations that
discriminate between modal categories within a variety of LMs, or modal
difference vectors. Analysis of modal difference vectors reveals that LMs have
access to more reliable modal categorization judgments than previously
reported. Furthermore, we find that modal difference vectors emerge in a
consistent order as models become more competent (i.e., through training steps,
layers, and parameter count). Notably, we find that modal difference vectors
identified within LM activations can be used to model fine-grained human
categorization behavior. This potentially provides a novel view into how human
participants distinguish between modal categories, which we explore by
correlating projections along modal difference vectors with human participants'
ratings of interpretable features. In summary, we derive new insights into LM
modal categorization using techniques from mechanistic interpretability, with
the potential to inform our understanding of modal categorization in humans.

</details>


### [3] [The first open machine translation system for the Chechen language](https://arxiv.org/abs/2507.12672)
*Abu-Viskhan A. Umishov,Vladislav A. Grigorian*

Main category: cs.CL

TL;DR: 本文介绍了首个车臣语与俄语之间的开放源码翻译模型及配套数据集，并探索了将新语言集成到大型多语言翻译模型NLLB-200中的微调方法。


<details>
  <summary>Details</summary>
Motivation: 为濒危语言车臣语提供机器翻译支持，填补该语言与俄语之间翻译模型和数据的空白，并探索将新语言高效融入现有大型多语言模型的潜力。

Method: 开发了车臣语与俄语之间的开放源码翻译模型；收集并构建了用于训练和评估模型的平行语料库数据集；通过微调NLLB-200大型语言模型来探索引入新语言（车臣语）的能力。

Result: 该翻译模型在俄语到车臣语方向的BLEU/ChrF++得分分别为8.34/34.69，在车臣语到俄语方向的BLEU/ChrF++得分分别为20.89/44.55。研究还伴随模型发布了平行的词汇、短语和句子语料库，以及一个适应车臣语的多语言句子编码器。

Conclusion: 本研究成功构建并发布了首个针对濒危语言车臣语与俄语的机器翻译模型及宝贵的数据资源，填补了该领域的空白。同时，研究展示了通过微调将新语言整合到大型多语言模型中的有效性，为濒危语言的数字化保护和多语言翻译技术的发展提供了重要贡献。

Abstract: We introduce the first open-source model for translation between the
vulnerable Chechen language and Russian, and the dataset collected to train and
evaluate it. We explore fine-tuning capabilities for including a new language
into a large language model system for multilingual translation NLLB-200. The
BLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for
translation from Russian to Chechen and reverse direction, respectively. The
release of the translation models is accompanied by the distribution of
parallel words, phrases and sentences corpora and multilingual sentence encoder
adapted to the Chechen language.

</details>


### [4] [Improving Drug Identification in Overdose Death Surveillance using Large Language Models](https://arxiv.org/abs/2507.12679)
*Arthur J. Funnell,Panayiotis Petousis,Fabrice Harel-Canada,Ruby Romero,Alex A. T. Bui,Adam Koncsol,Hritika Chaturvedi,Chelsea Shover,David Goodman-Meza*

Main category: cs.CL

TL;DR: 研究表明，微调后的BioClinicalBERT模型能高效、准确地从自由文本的死亡报告中分类药物过量死亡信息，显著提升药物过量监测的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 美国芬太尼导致的药物相关死亡率上升，需要及时准确的监测。然而，关键的过量数据常隐藏在法医自由文本报告中，手动编码为ICD-10分类导致延迟和信息丢失。NLP模型虽能自动化监测，但此前应用有限。

Method: 研究使用了2020年35,433份死亡记录进行模型训练和内部测试，并用2023-2024年的3,335份独立记录进行外部验证。评估了多种NLP方法，包括传统单/多标签分类器、微调的编码器语言模型（如BERT、BioClinicalBERT）和解码器大型语言模型（如Qwen 3、Llama 3），以从非结构化死亡证明文本中识别特定药物的涉入。模型性能通过宏平均F1分数和95%置信区间评估。

Result: 微调后的BioClinicalBERT模型表现接近完美，在内部测试集上宏F1分数达到≥0.998。外部验证也确认了其鲁棒性（宏F1=0.966），且优于传统机器学习、通用BERT模型和各类解码器大型语言模型。

Conclusion: NLP模型，特别是微调的临床特化变体如BioClinicalBERT，为从自由文本报告中分类药物过量死亡提供了高度准确和可扩展的解决方案。这些方法能够显著加速监测工作流程，克服手动ICD-10编码的局限性，并支持近实时地发现新兴药物滥用趋势。

Abstract: The rising rate of drug-related deaths in the United States, largely driven
by fentanyl, requires timely and accurate surveillance. However, critical
overdose data are often buried in free-text coroner reports, leading to delays
and information loss when coded into ICD (International Classification of
Disease)-10 classifications. Natural language processing (NLP) models may
automate and enhance overdose surveillance, but prior applications have been
limited. A dataset of 35,433 death records from multiple U.S. jurisdictions in
2020 was used for model training and internal testing. External validation was
conducted using a novel separate dataset of 3,335 records from 2023-2024.
Multiple NLP approaches were evaluated for classifying specific drug
involvement from unstructured death certificate text. These included
traditional single- and multi-label classifiers, as well as fine-tuned
encoder-only language models such as Bidirectional Encoder Representations from
Transformers (BERT) and BioClinicalBERT, and contemporary decoder-only large
language models such as Qwen 3 and Llama 3. Model performance was assessed
using macro-averaged F1 scores, and 95% confidence intervals were calculated to
quantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect
performance, with macro F1 scores >=0.998 on the internal test set. External
validation confirmed robustness (macro F1=0.966), outperforming conventional
machine learning, general-domain BERT models, and various decoder-only large
language models. NLP models, particularly fine-tuned clinical variants like
BioClinicalBERT, offer a highly accurate and scalable solution for overdose
death classification from free-text reports. These methods can significantly
accelerate surveillance workflows, overcoming the limitations of manual ICD-10
coding and supporting near real-time detection of emerging substance use
trends.

</details>


### [5] [AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.12695)
*S M Rafiuddin,Sadia Kamal,Mohammed Rakib,Arunkumar Bagavathi,Atriya Sen*

Main category: cs.CL

TL;DR: 本文提出了AdaptiSent，一个用于多模态基于方面情感分析（MABSA）的新框架，通过自适应跨模态注意力机制显著提升了情感分类和方面词提取的性能。


<details>
  <summary>Details</summary>
Motivation: 提高多模态情感分析的准确性，特别是解决文本和图像之间复杂的交互作用，以更准确地进行情感分类和方面词提取。

Method: 引入了AdaptiSent框架，该框架采用自适应跨模态注意力机制，并集成了动态模态加权和上下文自适应注意力，以增强对文本线索和视觉上下文交互的理解。

Result: 在标准Twitter数据集上，AdaptiSent在准确率、召回率和F1分数方面均超越了现有基线模型，尤其在识别细微的模态间关系方面表现出色。

Conclusion: AdaptiSent为多模态基于方面的情感分析设定了新标准，通过其动态上下文自适应能力，显著优于现有方法，尤其在理解复杂多模态信息方面表现卓越。

Abstract: We introduce AdaptiSent, a new framework for Multimodal Aspect-Based
Sentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms
to improve sentiment classification and aspect term extraction from both text
and images. Our model integrates dynamic modality weighting and
context-adaptive attention, enhancing the extraction of sentiment and
aspect-related information by focusing on how textual cues and visual context
interact. We tested our approach against several baselines, including
traditional text-based models and other multimodal methods. Results from
standard Twitter datasets show that AdaptiSent surpasses existing models in
precision, recall, and F1 score, and is particularly effective in identifying
nuanced inter-modal relationships that are crucial for accurate sentiment and
aspect term extraction. This effectiveness comes from the model's ability to
adjust its focus dynamically based on the context's relevance, improving the
depth and accuracy of sentiment analysis across various multimodal data sets.
AdaptiSent sets a new standard for MABSA, significantly outperforming current
methods, especially in understanding complex multimodal information.

</details>


### [6] [AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation](https://arxiv.org/abs/2507.12705)
*Potsawee Manakul,Woody Haosheng Gan,Michael J. Ryan,Ali Sartaz Khan,Warit Sirichotedumrong,Kunat Pipatanakul,William Held,Diyi Yang*

Main category: cs.CL

TL;DR: 本研究探索将大型音频模型（LAM）用作统一语音评估框架AudioJudge，旨在解决现有评估方法的局限性，并实现了与人类偏好的高相关性。


<details>
  <summary>Details</summary>
Motivation: 当前语音评估面临两大挑战：一是设计针对特定音频特征的专用系统难度大；二是自动化评估方法与人类偏好的相关性较差。

Method: 研究提出并系统性地探索了AudioJudge，一个基于LAM的语音评估框架。方法包括：1) 在多种音频特征检测任务（如发音、语速、说话人识别、语音质量）和人类偏好模拟中进行探索；2) 评估不同的提示工程策略，发现音频拼接结合上下文学习能显著提升性能；3) 引入多方面集成AudioJudge，将语音评估分解为针对词汇内容、语音质量和副语言特征的专用评估器。

Result: 多方面集成AudioJudge在系统排名基准上与人类偏好达到了高达0.91的Spearman相关性。鲁棒性分析显示，尽管LAM在噪声下表现良好，但存在显著的冗余性和位置偏差。

Conclusion: 大型音频模型（AudioJudge）为语音评估提供了一个统一且有效的框架，能与人类偏好高度关联，但其固有的冗余性和位置偏差需谨慎处理。

Abstract: Current speech evaluation suffers from two critical limitations: the need and
difficulty of designing specialized systems targeting individual audio
characteristics, and poor correlation between automatic evaluation methods and
human preferences. This work presents a systematic study of Large Audio Model
(LAM) as a Judge, AudioJudge, investigating whether it can provide a unified
evaluation framework that addresses both challenges. We systematically explore
AudioJudge across audio characteristic detection tasks, including
pronunciation, speaking rate, speaker identification and speech quality, and
system-level human preference simulation for automated benchmarking. We
investigate different prompt engineering strategies, finding that audio
concatenation combined with in-context learning significantly improves
performance across both audio characteristic detection and human preference
simulation tasks. We further introduce a multi-aspect ensemble AudioJudge to
enable general-purpose multi-aspect audio evaluation. This method decomposes
speech assessment into specialized judges for lexical content, speech quality,
and paralinguistic features, achieving up to 0.91 Spearman correlation with
human preferences on our system ranking benchmark. Robustness analysis reveals
that while LAMs maintain strong performance under acoustic noise, they exhibit
significant verbosity and positional biases that require careful mitigation.

</details>


### [7] [FLEXITOKENS: Flexible Tokenization for Evolving Language Models](https://arxiv.org/abs/2507.12720)
*Abraham Toluase Owodunni,Orevaoghene Ahia,Sachin Kumar*

Main category: cs.CL

TL;DR: 本文提出FLEXITOKENS，一种具有可学习分词器的字节级语言模型，通过简化的训练目标解决现有语言模型在面对新数据分布时分词器僵化导致的分词过度碎片化问题，显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型（LMs）难以适应新的数据分布，原因在于其子词分词器僵化且在适应过程中保持不变。这种僵化导致对域外数据、未知语言或脚本的低效分词和过度碎片化。

Method: 开发了带有可学习分词器的字节级语言模型。模型包含一个子模块，该模块学习预测输入字节序列之间的边界，并将其编码为可变长度的片段。针对现有无分词器方法在训练边界预测器时通过辅助损失强制固定压缩率所引入的刚性，本文提出了FLEXITOKENS，一种简化的训练目标，以实现更大的适应灵活性。

Result: 在多语言基准、形态多样化任务和不同领域上的评估表明，FLEXITOKENS持续减少了分词过度碎片化，并且与子词分词器和其他基于梯度的分词器相比，在下游任务性能上实现了高达10%的提升。

Conclusion: FLEXITOKENS通过其灵活的、可学习的分词机制，有效解决了语言模型在适应新数据分布时遇到的分词效率低下和过度碎片化问题，从而显著提高了模型在多样化任务上的性能表现。

Abstract: Language models (LMs) are challenging to adapt to new data distributions by
simple finetuning. This is due to the rigidity of their subword tokenizers,
which typically remain unchanged during adaptation. This inflexibility often
leads to inefficient tokenization, causing overfragmentation of
out-of-distribution domains, unseen languages, or scripts. In this work, we
develop byte-level LMs with learnable tokenizers to make tokenization adaptive.
Our models include a submodule that learns to predict boundaries between the
input byte sequence, encoding it into variable-length segments. Existing
tokenizer-free methods train this boundary predictor using an auxiliary loss
that enforces a fixed compression rate across the training corpus, introducing
a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective
that enables significantly greater flexibility during adaptation. Evaluating
across multiple multilingual benchmarks, morphologically diverse tasks, and
domains, we demonstrate that FLEXITOKENS consistently reduces token
over-fragmentation and achieves up to 10\% improvements on downstream task
performance compared to subword and other gradient-based tokenizers. Code and
data for our experiments will be released at
https://github.com/owos/flexitokens

</details>


### [8] [TransEvalnia: Reasoning-based Evaluation and Ranking of Translations](https://arxiv.org/abs/2507.12724)
*Richard Sproat,Tianyu Zhao,Llion Jones*

Main category: cs.CL

TL;DR: TransEvalnia是一个基于提示和推理的翻译评估与排名系统，其性能优于或媲美现有最佳系统，且评估结果与人类判断高度一致。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个能够进行细粒度评估、判断最佳翻译并提供维度及总体数值分数的翻译评估与排名系统，并超越现有SOTA系统。

Method: 开发了TransEvalnia系统，该系统基于提示工程，并利用推理进行评估和排名。它基于多维度质量指标（MQM）的子集进行细粒度评估。研究中使用了Anthropic的Claude-3.5-Sonnet和Qwen-2.5-72B-Instruct作为评估LLM。

Result: TransEvalnia在英日数据及WMT共享任务的多个语言对上，表现与SOTA的MT-Ranker相当或更优。其评估结果被人类评估者认为是高度可接受的，且LLM分配的分数与人类评估者的分数高度相关。此外，研究还发现系统（包括MT-Ranker）对翻译呈现顺序存在位置偏差。

Conclusion: TransEvalnia是一种高效且与人类判断高度一致的翻译评估与排名系统。研究识别并提出了解决评估中位置偏差的方法。所有相关数据和代码均已发布。

Abstract: We present TransEvalnia, a prompting-based translation evaluation and ranking
system that uses reasoning in performing its evaluations and ranking. This
system presents fine-grained evaluations based on a subset of the
Multidimensional Quality Metrics (https://themqm.org/), returns an assessment
of which translation it deems the best, and provides numerical scores for the
various dimensions and for the overall translation. We show that TransEvalnia
performs as well as or better than the state-of-the-art MT-Ranker (Moosa et al.
2024) on our own English-Japanese data as well as several language pairs from
various WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and
Qwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations
returned are deemed highly acceptable to human raters, and that the scores
assigned to the translations by Sonnet, as well as other LLMs, correlate well
with scores assigned by the human raters. We also note the sensitivity of our
system -- as well as MT-Ranker -- to the order in which the translations are
presented, and we propose methods to address this position bias. All data,
including the system's evaluation and reasoning, human assessments, as well as
code is released.

</details>


### [9] [Strategy Adaptation in Large Language Model Werewolf Agents](https://arxiv.org/abs/2507.12732)
*Fuya Nakamori,Yin Jou Huang,Fei Cheng*

Main category: cs.CL

TL;DR: 本研究提出一种通过根据其他玩家态度和对话上下文切换预定义策略，来提高狼人杀代理性能的方法。


<details>
  <summary>Details</summary>
Motivation: 现有狼人杀代理虽采用提示工程，但其有效策略是隐式定义的，无法适应不断变化的局势。

Method: 本研究提出一种明确根据游戏上下文和估算的玩家角色来选择适当策略的方法。

Result: 通过将策略自适应狼人杀代理与使用隐式或固定策略的基线代理进行比较，验证了所提出方法的有效性。

Conclusion: 所提出的基于情境和玩家角色动态策略选择的方法能有效提升狼人杀代理的性能。

Abstract: This study proposes a method to improve the performance of Werewolf agents by
switching between predefined strategies based on the attitudes of other players
and the context of conversations. While prior works of Werewolf agents using
prompt engineering have employed methods where effective strategies are
implicitly defined, they cannot adapt to changing situations. In this research,
we propose a method that explicitly selects an appropriate strategy based on
the game context and the estimated roles of other players. We compare the
strategy adaptation Werewolf agents with baseline agents using implicit or
fixed strategies and verify the effectiveness of our proposed method.

</details>


### [10] [Logit Arithmetic Elicits Long Reasoning Capabilities Without Training](https://arxiv.org/abs/2507.12759)
*Yunxiang Zhang,Muhammad Khalifa,Lechen Zhang,Xin Liu,Ayoung Lee,Xinliang Frederick Zhang,Farima Fatahi Bayat,Lu Wang*

Main category: cs.CL

TL;DR: 本研究提出了ThinkLogit及其变体ThinkLogit-DPO，这是一种在解码时使用小型引导模型的方法，能够在无需或仅需极少额外训练的情况下，高效地提升大型模型（如Qwen2.5-32B）的长链推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽具备复杂推理能力，但往往需要额外训练才能充分发挥其长链推理潜力。本研究旨在探索一种无需训练或仅需极少训练即可在大型模型中激发此类行为的方法。

Method: 本研究提出了两种解码时方法：1. ThinkLogit：利用 logits 算术，通过一个显著更小的模型作为引导，对目标大型语言模型进行调整以实现长链推理。2. ThinkLogit-DPO：在此基础上，通过偏好优化训练引导模型（利用目标模型和引导模型中采样出的正确/错误推理对）进一步提升性能。

Result: 实验结果表明，在四个数学数据集上，使用小21倍的R1-Distill-Qwen-1.5B引导模型，ThinkLogit和ThinkLogit-DPO使Qwen2.5-32B的pass@1指标分别相对提高了26%和29%。此外，ThinkLogit还能有效迁移通过强化学习获得的长期推理技能，使pass@1相对提高了13%。

Conclusion: 本工作提出了一种计算高效的方法，能够在无需或仅需极少额外训练的情况下，有效激发大型模型中的长链推理能力，并能成功迁移通过其他方式获得的推理技能。

Abstract: Large reasoning models (LRMs) can do complex reasoning via long
chain-of-thought (CoT) involving cognitive strategies such as backtracking and
self-correction. Recent studies suggest that some models inherently possess
these long reasoning abilities, which may be unlocked via extra training. Our
work first investigates whether we can elicit such behavior without any
training. To this end, we propose a decoding-time approach, ThinkLogit, which
utilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for
long reasoning using a substantially smaller model as guider. We then show that
we can further boost performance by training the guider model with preference
optimization over correct/incorrect reasoning pairs sampled from both the
target and guider model -- a setup we refer to as ThinkLogit-DPO. Our
experiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative
improvement in pass@1 by 26% and 29%, respectively, over four mathematical
datasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model
21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills
acquired through reinforcement learning, improving pass@1 by 13% relative
compared to the Qwen2.5-32B base model. Our work presents a
computationally-efficient method to elicit long reasoning in large models with
minimal or no additional training.

</details>


### [11] [Synergy: End-to-end Concept Model](https://arxiv.org/abs/2507.12769)
*Keli Zheng,Zerong Xie*

Main category: cs.CL

TL;DR: Synergy是一个字节级语言模型，通过学习路由机制桥接不同抽象级别。它能自发学习分词，在性能可比的情况下，概念token少于BBPE，并优于Llama3，同时证明了无分词器架构的可行性。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够端到端地桥接不同抽象层次的语言模型，并验证无分词器架构的可行性，以实现更鲁棒和灵活的NLP管线。

Method: 提出了Synergy模型，一个字节级语言模型，通过学习路由机制桥接不同抽象级别。与Llama3和Byte-level Byte Pair Encoder (BBPE)分词器进行比较，并研究了移除高抽象部分位置编码的影响。

Result: Synergy能自发学习字节分词，生成更少的概念token（相比BBPE），同时保持可比性能。在相同规模和数据集下，Synergy表现优于Llama3。模型的高抽象部分在移除位置编码后表现更佳，表明出现与位置无关的概念。

Conclusion: 研究结果证明了无分词器架构的可行性，为构建更鲁棒和灵活的自然语言处理管线提供了新途径。

Abstract: In this paper, we present Synergy, a language model that bridges different
levels of abstraction in an end-to-end fashion through a learned routing
mechanism. Focusing on low-level linguistic abstraction, we trained our model
as a byte-level language model. Our model spontaneously learns to tokenize
bytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE)
tokenizers while keeping comparable performance. By comparing with Llama3, we
observed an advantage of Synergy under the same model scale and training
dataset size. Further studies show that the middle part (the higher abstraction
part) of our model performs better when positional encodings are removed,
suggesting the emergence of position-independent concepts. These findings
demonstrate the feasibility of tokenizer-free architectures, paving the way for
more robust and flexible pipelines.

</details>


### [12] [Learning Robust Negation Text Representations](https://arxiv.org/abs/2507.12782)
*Thinh Hung Truong,Karin Verspoor,Trevor Cohn,Timothy Baldwin*

Main category: cs.CL

TL;DR: 本文提出一种利用大型语言模型蒸馏数据并结合对比学习的方法，以提高文本编码器和大型语言模型对否定语义的理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型迅速发展，小型文本编码器在需要丰富上下文表示的文本理解任务中仍很重要，但它们和当前方法未能很好地捕捉否定语义，这影响了许多依赖文本嵌入的下游应用。

Method: 通过从大型语言模型中利用多样的否定和对冲模式蒸馏数据，采用标准对比学习策略微调强大的基于BERT的模型，并展示该方法也可适用于大型语言模型。

Result: 基于BERT的模型在否定理解能力上取得了显著提升，同时在通用基准测试上保持了竞争力。此外，该方法也能改进大型语言模型在否定基准测试上的表现。

Conclusion: 所提出的策略能有效提高文本编码器和大型语言模型对否定语义的鲁棒性，同时不损害其通用性能。

Abstract: Despite rapid adoption of autoregressive large language models, smaller text
encoders still play an important role in text understanding tasks that require
rich contextualized representations. Negation is an important semantic function
that is still not properly captured by such methods, affecting many downstream
applications relying on text embeddings. We propose a strategy to improve
negation robustness of text encoders, by distilling data from large language
models using diverse patterns of negation and hedging. We adopt a standard
contrastive learning strategy to finetune a strong BERT-based model, and
observe large improvement in negation understanding capabilities while
maintaining competitive performance on general benchmarks. In addition, we also
show that our method can be adapted to LLMs, leading to improved performance on
negation benchmarks.

</details>


### [13] [Large Language Models' Internal Perception of Symbolic Music](https://arxiv.org/abs/2507.12808)
*Andrew Shin,Kunitake Kaneko*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）如何通过文本提示生成符号音乐，并发现LLMs能从文本中推断出基本的音乐结构，揭示了其在音乐建模方面的潜力和局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言及其他符号领域表现出色，但它们隐式建模符号音乐的程度尚未被充分探索。

Method: 研究通过文本提示生成LLM的符号音乐数据（MIDI文件），且不依赖显式音乐训练，以此构建数据集。随后，在该LLM生成的数据集上训练神经网络，并执行流派和风格分类以及旋律补全任务，将性能与现有模型进行基准测试。

Result: 实验结果表明，LLMs能够从文本中推断出基本的音乐结构和时间关系。

Conclusion: LLMs具有隐式编码音乐模式的潜力，但由于缺乏明确的音乐上下文而存在局限性，本研究阐明了LLMs在符号音乐生成方面的能力。

Abstract: Large language models (LLMs) excel at modeling relationships between strings
in natural language and have shown promise in extending to other symbolic
domains like coding or mathematics. However, the extent to which they
implicitly model symbolic music remains underexplored. This paper investigates
how LLMs represent musical concepts by generating symbolic music data from
textual prompts describing combinations of genres and styles, and evaluating
their utility through recognition and generation tasks. We produce a dataset of
LLM-generated MIDI files without relying on explicit musical training. We then
train neural networks entirely on this LLM-generated MIDI dataset and perform
genre and style classification as well as melody completion, benchmarking their
performance against established models. Our results demonstrate that LLMs can
infer rudimentary musical structures and temporal relationships from text,
highlighting both their potential to implicitly encode musical patterns and
their limitations due to a lack of explicit musical context, shedding light on
their generative capabilities for symbolic music.

</details>


### [14] [Are Knowledge and Reference in Multilingual Language Models Cross-Lingually Consistent?](https://arxiv.org/abs/2507.12838)
*Xi Ai,Mahardika Krisna Ihsani,Min-Yen Kan*

Main category: cs.CL

TL;DR: 本文分析和评估了多语言模型的事实知识跨语言一致性，发现其受语言家族等因素影响，并指出跨语言对齐和代码混合训练能有效提升一致性。


<details>
  <summary>Details</summary>
Motivation: 为评估多语言模型跨语言可迁移性、保持知识事实性及性能平衡，需考虑跨语言一致性。因此，研究旨在分析、评估和解释事实知识的跨语言一致性。

Method: 通过检查传达相同知识的混合语言指代语句来研究跨语言知识一致性。使用可解释性方法分析模型行为。评估旨在提高多语言性能的常见策略对知识一致性的影响。

Result: 多语言模型表现出不同程度的跨语言一致性，受语言家族、语言因素及特定层瓶颈影响。尽管知识在许多情况下不具备跨语言一致性，但代码混合训练和跨语言词对齐目标显示出最有前景的结果。

Conclusion: 跨语言对齐监督和代码混合训练对于提升多语言性能和跨语言一致性均具有显著作用。

Abstract: Cross-lingual consistency should be considered to assess cross-lingual
transferability, maintain the factuality of the model knowledge across
languages, and preserve the parity of language model performance. We are thus
interested in analyzing, evaluating, and interpreting cross-lingual consistency
for factual knowledge. We examine code-mixed coreferential statements conveyed
identical knowledge across languages to study cross-lingual knowledge
consistency. We use some interpretability approaches to analyze the behavior of
a model in cross-lingual contexts, discovering that multilingual models show
different levels of consistency, subject to language families, linguistic
factors, and a bottleneck in cross-lingual consistency on a particular layer.
In addition, we evaluate common strategies aimed at improving multilingual
performance to observe whether these strategies can improve knowledge
consistency at the same time. While knowledge is not cross-lingual consistency
in many cases, code-switching training and cross-lingual word alignment
objectives show the most promising results, emphasizing the noteworthiness of
cross-lingual alignment supervision and code-switching training for both
multilingual performance and cross-lingual consistency enhancement.

</details>


### [15] [Making Language Model a Hierarchical Classifier and Generator](https://arxiv.org/abs/2507.12930)
*Yihong Wang,Zhonglin Jiang,Ningyuan Xi,Yue Zhao,Qingqing Gu,Xiyuan Chen,Hao Wu,Sheng Xu,Hange Zhou,Yong Chen,Luo Ji*

Main category: cs.CL

TL;DR: 本文受人类分层思维启发，提出并验证了一种分层解码器架构，通过将预训练模型的语言头复制到选定的中间层并进行微调，实现在多项分层任务上达到最先进（SOTA）性能。


<details>
  <summary>Details</summary>
Motivation: 现有仅解码器语言模型（如GPT、LLaMA）通常仅在最后一层进行解码。受人类分层思维能力的启发，研究者提出可以构建一个分层解码器架构，使不同层能够同时解码文本。

Method: 由于资源限制，研究团队选择改造一个预训练语言模型。具体方法是将最后一层的语言头复制到不同的选定中间层，并使用不同的任务输入进行微调。

Result: 实验验证了这些选定的中间层能够生成有意义且合理的内容。这种分层解码器范式在分层文本分类、分类引导生成和分层文本生成等多项任务上取得了当前最佳（SOTA）性能。

Conclusion: 这项研究揭示了从零开始预训练一个通用分层推理器的可能性。

Abstract: Decoder-only language models, such as GPT and LLaMA, generally decode on the
last layer. Motivated by human's hierarchical thinking capability, we propose
that a hierarchical decoder architecture could be built with different layers
decoding texts simultaneously. Due to limited time and computationally
resources, we choose to adapt a pretrained language model into this form of
hierarchical decoder. Language heads of the last layer are copied to different
selected intermediate layers, and fine-tuned with different task inputs. By
thorough experiments, we validate that these selective intermediate layers
could be adapted to speak meaningful and reasonable contents, and this paradigm
of hierarchical decoder can obtain state-of-the-art performances on multiple
tasks such as hierarchical text classification, classification-guided
generation, and hierarchical text generation. This study suggests the
possibility of a generalized hierarchical reasoner, pretraining from scratch.

</details>


### [16] [MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2507.12981)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Héctor Cerezo-Costas,Pedro Alonso Doval,Jorge Alcalde Vesteiro*

Main category: cs.CL

TL;DR: 本论文提出了一种利用大型语言模型（LLMs）生成Python代码，以解决西班牙语表格问答（Table Q&A）任务的方法，并在IberLEF 2025任务中取得了85%的准确率。


<details>
  <summary>Details</summary>
Motivation: 旨在解决IberLEF 2025的PRESTA任务，即关于西班牙语表格的问答。

Method: 该方法通过LLMs生成Python代码来过滤和处理表格以获取答案。此方案是Semeval 2025相关任务中MRT实现的演进。其多步骤流程包括：分析表格内容、选择有用列、生成自然语言指令、将指令翻译为代码、运行代码以及处理错误。每个步骤都利用了开源LLMs和经过优化的精细化提示。

Result: 通过此方法，在任务中取得了85%的准确率。

Conclusion: 所提出的基于LLM生成Python代码的表格问答方法，在西班牙语表格问答任务中表现出高效率和有效性，并在IberLEF 2025任务中达到了85%的准确率。

Abstract: This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas
y Respuestas sobre Tablas en Espa\~nol (Questions and Answers about Tables in
Spanish). Our solution obtains answers to the questions by implementing Python
code generation with LLMs that is used to filter and process the table. This
solution evolves from the MRT implementation for the Semeval 2025 related task.
The process consists of multiple steps: analyzing and understanding the content
of the table, selecting the useful columns, generating instructions in natural
language, translating these instructions to code, running it, and handling
potential errors or exceptions. These steps use open-source LLMs and
fine-grained optimized prompts for each step. With this approach, we achieved
an accuracy score of 85\% in the task.

</details>


### [17] [Formalizing Attack Scenario Description: A Proposed Model](https://arxiv.org/abs/2507.13076)
*Quentin Goux,Nadira Lammari*

Main category: cs.CL

TL;DR: 本文提出一个新颖的、基于UML的攻击场景形式化模型，旨在解决网络安全自动化中输入数据形式化的问题，并展示了其在攻击分析和自动化训练脚本生成方面的应用。


<details>
  <summary>Details</summary>
Motivation: 面对不断变化的威胁，组织需要提高网络安全自动化水平。然而，自动化过程要求输入数据（特别是攻击场景）的形式化，以支持攻击模拟、培训和分析等流程。

Method: 研究提出了一个新颖的形式化模型，该模型包含攻击的上下文描述及其场景，并采用UML类模型进行抽象。

Result: 该模型被证明能够服务于上游攻击分析过程，并且可用于在网络安全培训背景下自动生成攻击脚本。这两个用例构成了本研究的第二项贡献。

Conclusion: 所提出的攻击场景形式化模型有效地满足了网络安全自动化对输入数据形式化的需求，通过支持攻击分析和自动化脚本生成，促进了相关流程的效率和准确性。

Abstract: Organizations face an ever-changing threat landscape. They must continuously
dedicate significant efforts to protect their assets, making their adoption of
increased cybersecurity automation inevitable. However, process automation
requires formalization of input data. Through this paper, we address this need
for processes that use attack scenarios as input. Among these processes, one
can mention both the generation of scripts for attack simulation and training
purposes, as well as the analysis of attacks. Therefore, the paper's main
research contribution is a novel formal model that encompasses the attack's
context description and its scenario. It is abstracted using UML class model.
Once the description of our model done, we will show how it could serve an
upstream attack analysis process. We will show also its use for an automatic
generation of attack scripts in the context of cybersecurity training. These
two uses cases constitute the second contribution of this present research
work.

</details>


### [18] [SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated Summaries For Scientific Abstracts](https://arxiv.org/abs/2507.13105)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: SemCSE是一种无监督方法，利用对比学习和LLM生成的摘要来学习科学文本的语义嵌入，有效解决了传统方法的语义局限性，并在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于引用的科学文本嵌入方法不一定能反映真正的语义相似性，导致嵌入未能充分捕捉文本的真实语义内容。

Method: 引入SemCSE，一种无监督的科学文本语义嵌入学习方法。该方法基于对比学习的最新进展，利用大型语言模型（LLM）生成的科学摘要来训练模型，旨在使语义相关的摘要在嵌入空间中距离更近。

Result: 1. 提出了一种新的基准测试，用于评估模型理解和编码科学文本语义内容的能力，并证明SemCSE能在嵌入空间中实现更强的语义分离。 2. 在SciRepEval科学文本嵌入综合基准测试中，SemCSE在其尺寸的模型中达到了最先进的性能。

Conclusion: SemCSE通过语义聚焦的训练方法，成功地学习了科学文本的真实语义内容，克服了传统方法的局限性，并在多个评估基准上展现出卓越的性能，突显了以语义为中心的训练方法的优势。

Abstract: We introduce SemCSE, an unsupervised method for learning semantic embeddings
of scientific texts. Building on recent advances in contrastive learning for
text embeddings, our approach leverages LLM-generated summaries of scientific
abstracts to train a model that positions semantically related summaries closer
together in the embedding space. This resulting objective ensures that the
model captures the true semantic content of a text, in contrast to traditional
citation-based approaches that do not necessarily reflect semantic similarity.
To validate this, we propose a novel benchmark designed to assess a model's
ability to understand and encode the semantic content of scientific texts,
demonstrating that our method enforces a stronger semantic separation within
the embedding space. Additionally, we evaluate SemCSE on the comprehensive
SciRepEval benchmark for scientific text embeddings, where it achieves
state-of-the-art performance among models of its size, thus highlighting the
benefits of a semantically focused training approach.

</details>


### [19] [A Computational Framework to Identify Self-Aspects in Text](https://arxiv.org/abs/2507.13115)
*Jaya Caporusso,Matthew Purver,Senja Pollak*

Main category: cs.CL

TL;DR: 本博士提案旨在开发一个计算框架，利用自然语言处理（NLP）识别文本中的“自我面向”，并将其应用于心理健康等领域。


<details>
  <summary>Details</summary>
Motivation: “自我”是一个多面性概念，在语言中有所体现，但在NLP领域尚未得到充分探索。鉴于其与心理健康等重要现象的关联，迫切需要基于NLP的系统分析。

Method: 计划构建一个自我面向的本体论和黄金标准标注数据集。在此基础上，将开发并评估传统判别模型、生成式大型语言模型和基于嵌入的检索方法，评估标准包括可解释性、真值依从性、准确性和计算效率。

Result: （本为提案，暂无具体研究结果。）预期成果是，性能最佳的模型将被应用于心理健康和经验现象学的案例研究中，展示该框架的实用性。

Conclusion: 本提案旨在通过建立一套全面的计算框架，填补NLP在自我面向分析方面的空白，从而为心理健康和经验现象学等领域提供重要的分析工具和应用潜力。

Abstract: This Ph.D. proposal introduces a plan to develop a computational framework to
identify Self-aspects in text. The Self is a multifaceted construct and it is
reflected in language. While it is described across disciplines like cognitive
science and phenomenology, it remains underexplored in natural language
processing (NLP). Many of the aspects of the Self align with psychological and
other well-researched phenomena (e.g., those related to mental health),
highlighting the need for systematic NLP-based analysis. In line with this, we
plan to introduce an ontology of Self-aspects and a gold-standard annotated
dataset. Using this foundation, we will develop and evaluate conventional
discriminative models, generative large language models, and embedding-based
retrieval approaches against four main criteria: interpretability, ground-truth
adherence, accuracy, and computational efficiency. Top-performing models will
be applied in case studies in mental health and empirical phenomenology.

</details>


### [20] [Assessing the Reliability of LLMs Annotations in the Context of Demographic Bias and Model Explanation](https://arxiv.org/abs/2507.13138)
*Hadi Mohammadi,Tina Shahedi,Pablo Mosteiro,Massimo Poesio,Ayoub Bagheri,Anastasia Giachanou*

Main category: cs.CL

TL;DR: 本研究分析标注变异性，发现人口统计学因素对标注决策影响甚微（<8%），文本内容为主导。评估GenAI作标注者时，发现简单的人设引导反而可能降低性能。XAI显示模型预测主要基于内容。提出应关注内容驱动的解释和稳健的标注协议以实现公平。


<details>
  <summary>Details</summary>
Motivation: 开发公平的自然语言处理（NLP）系统，尤其是在性别歧视检测等敏感任务中，了解标注变异性的来源和人口统计学偏见的影响至关重要。研究旨在量化标注者人口统计特征与文本内容对标注决策的影响，并评估生成式AI（GenAI）作为标注者的可靠性及人设引导的效果。

Method: 使用广义线性混合模型（Generalized Linear Mixed Model）量化标注者人口统计学因素对标注决策的影响。评估生成式AI（GenAI）模型作为标注者时，通过人口统计学人设引导其标注，并与人类判断进行比较。运用可解释AI（XAI）技术分析模型预测的依据。

Result: 人口统计学因素对观察到的标注方差贡献甚微（约8%），推文内容是主要决定因素。简单的人设提示通常未能提升GenAI作为标注者的性能，有时甚至导致下降。可解释AI（XAI）揭示模型预测主要依赖于与性别歧视相关的内容特定词元，而非人口统计学特征关联。

Conclusion: 相比于潜在的人设模拟，更可靠的实现公平性的途径是关注内容驱动的解释和稳健的标注协议。

Abstract: Understanding the sources of variability in annotations is crucial for
developing fair NLP systems, especially for tasks like sexism detection where
demographic bias is a concern. This study investigates the extent to which
annotator demographic features influence labeling decisions compared to text
content. Using a Generalized Linear Mixed Model, we quantify this inf luence,
finding that while statistically present, demographic factors account for a
minor fraction ( 8%) of the observed variance, with tweet content being the
dominant factor. We then assess the reliability of Generative AI (GenAI) models
as annotators, specifically evaluating if guiding them with demographic
personas improves alignment with human judgments. Our results indicate that
simplistic persona prompting often fails to enhance, and sometimes degrades,
performance compared to baseline models. Furthermore, explainable AI (XAI)
techniques reveal that model predictions rely heavily on content-specific
tokens related to sexism, rather than correlates of demographic
characteristics. We argue that focusing on content-driven explanations and
robust annotation protocols offers a more reliable path towards fairness than
potentially persona simulation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [21] [Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering](https://arxiv.org/abs/2507.12490)
*Maximiliano Hormazábal Lagos,Héctor Cerezo-Costas,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: 本文介绍了一个名为EaGERS的训练无关、模型无关的管道，它通过视觉语言模型生成理由并将其定位到图像区域，从而在DocVQA任务上提升性能、透明度及可复现性，且无需额外微调。


<details>
  <summary>Details</summary>
Motivation: 提升文档视觉问答（DocVQA）任务的性能、透明度和可复现性，同时避免传统方法所需的模型微调。

Method: EaGERS管道包括三个步骤：1) 使用视觉语言模型生成自然语言理由；2) 通过计算多模态嵌入相似性，结合可配置网格和多数投票机制，将理由定位到图像的空间子区域；3) 仅从掩码图像中选定的相关区域生成响应。整个过程无需训练且模型无关。

Result: 在DocVQA数据集上的实验表明，EaGERS不仅在精确匹配准确率（EM）和平均归一化莱文斯坦相似度（ANLS）指标上优于基线模型，而且在没有额外模型微调的情况下，提高了DocVQA的透明度和可复现性。

Conclusion: EaGERS提供了一种有效且高效的解决方案，在无需模型微调的前提下，显著提升了文档视觉问答的性能，并增强了其透明度和结果的可复现性。

Abstract: We introduce EaGERS, a fully training-free and model-agnostic pipeline that
(1) generates natural language rationales via a vision language model, (2)
grounds these rationales to spatial sub-regions by computing multimodal
embedding similarities over a configurable grid with majority voting, and (3)
restricts the generation of responses only from the relevant regions selected
in the masked image. Experiments on the DocVQA dataset demonstrate that our
best configuration not only outperforms the base model on exact match accuracy
and Average Normalized Levenshtein Similarity metrics but also enhances
transparency and reproducibility in DocVQA without additional model
fine-tuning.

</details>


### [22] [MindJourney: Test-Time Scaling with World Models for Spatial Reasoning](https://arxiv.org/abs/2507.12508)
*Yuncong Yang,Jiageng Liu,Zheyuan Zhang,Siyuan Zhou,Reuben Tan,Jianwei Yang,Yilun Du,Chuang Gan*

Main category: cs.CV

TL;DR: 提出MindJourney框架，将视觉语言模型（VLM）与视频扩散世界模型结合，以在测试时增强VLM的3D空间推理能力，无需微调。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）在3D空间推理任务（如预测视角变化）上表现不佳，因为它们感知2D图像但缺乏3D动态的内部模型，而3D推理对人类认知和具身任务至关重要。

Method: 提出MindJourney框架，在测试时将VLM与基于视频扩散的可控世界模型耦合。VLM迭代性地规划摄像机轨迹，世界模型合成每一步对应的视图，VLM基于这些交互探索中收集的多视图证据进行推理。此过程无需微调。

Result: 在代表性的SAT空间推理基准测试中，MindJourney使性能平均提升了8%以上，且无需任何微调。该方法还改进了通过强化学习训练的测试时推理VLM的表现。

Conclusion: 在测试时将VLM与世界模型结合，为实现鲁棒的3D推理提供了一种简单、即插即用的途径，并证明了利用世界模型进行测试时扩展的巨大潜力。

Abstract: Spatial reasoning in 3D space is central to human cognition and indispensable
for embodied tasks such as navigation and manipulation. However,
state-of-the-art vision-language models (VLMs) struggle frequently with tasks
as simple as anticipating how a scene will look after an egocentric motion:
they perceive 2D images but lack an internal model of 3D dynamics. We therefore
propose MindJourney, a test-time scaling framework that grants a VLM with this
missing capability by coupling it to a controllable world model based on video
diffusion. The VLM iteratively sketches a concise camera trajectory, while the
world model synthesizes the corresponding view at each step. The VLM then
reasons over this multi-view evidence gathered during the interactive
exploration. Without any fine-tuning, our MindJourney achieves over an average
8% performance boost on the representative spatial reasoning benchmark SAT,
showing that pairing VLMs with world models for test-time scaling offers a
simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also
improves upon the test-time inference VLMs trained through reinforcement
learning, which demonstrates the potential of our method that utilizes world
models for test-time scaling.

</details>


### [23] [Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models](https://arxiv.org/abs/2507.12566)
*Gen Luo,Wenhan Dou,Wenhao Li,Zhaokai Wang,Xue Yang,Changyao Tian,Hao Li,Weiyun Wang,Wenhai Wang,Xizhou Zhu,Yu Qiao,Jifeng Dai*

Main category: cs.CV

TL;DR: 本文提出Mono-InternVL系列模型，通过将新的视觉参数空间嵌入预训练大语言模型并结合多模态专家混合架构，解决了单体多模态大语言模型（MLLMs）优化不稳定和灾难性遗忘的问题，实现了高性能和高效率。


<details>
  <summary>Details</summary>
Motivation: 现有的单体多模态大语言模型（MLLMs）将视觉编码和语言解码集成到单一模型中，但在优化过程中常遭遇不稳定性及灾难性遗忘问题。

Method: 核心思想是通过delta微调将新的视觉参数空间嵌入预训练大语言模型，实现视觉知识的稳定学习。基于此，提出Mono-InternVL，它通过多模态专家混合（MoE）架构整合视觉专家。同时，设计了内生视觉预训练（EViP）策略进行渐进式学习。进一步提出成本更低、性能更强的Mono-InternVL-1.5，其配备改进的EViP++（引入额外的视觉注意力专家并优化预训练流程），并在推理时包含融合的CUDA核以加速MoE操作。

Result: Mono-InternVL在15个基准测试中的12个上超越了现有单体MLLMs（例如在OCRBench上比Emu3提升114点）。Mono-InternVL-1.5显著降低了训练和推理成本，并保持了与Mono-InternVL相当的竞争力。与模块化对应模型InternVL-1.5相比，Mono-InternVL-1.5在实现相似多模态性能的同时，将首词延迟降低高达69%。

Conclusion: 本研究通过创新的模型架构和预训练策略，成功解决了单体MLLMs的优化挑战和遗忘问题，实现了业界领先的性能，同时显著降低了训练和推理成本及延迟。

Abstract: This paper focuses on monolithic Multimodal Large Language Models (MLLMs),
which integrate visual encoding and language decoding into a single model.
Existing structures and pre-training strategies for monolithic MLLMs often
suffer from unstable optimization and catastrophic forgetting. To address these
challenges, our key idea is to embed a new visual parameter space into a
pre-trained LLM, enabling stable learning of visual knowledge from noisy data
via delta tuning. Based on this principle, we first introduce Mono-InternVL, an
advanced monolithic MLLM that incorporates a set of visual experts through a
multimodal mixture-of-experts architecture. In addition, we design an
innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize
its visual capabilities via progressive learning. Mono-InternVL achieves
competitive performance against existing MLLMs but also leads to relatively
expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper
and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++
introduces additional visual attention experts to Mono-InternVL-1.5 and
re-organizes the pre-training process in an efficient manner. During inference,
it includes a fused CUDA kernel to speed up its MoE operations. With these
designs, Mono-InternVL-1.5 significantly reduces training and inference costs,
while still maintaining competitive performance with Mono-InternVL. To evaluate
our approach, we conduct extensive experiments across 15 benchmarks. Results
demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out
of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared
to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves
similar multimodal performance while reducing first-token latency by up to 69%.
Code and models are released at https://github.com/OpenGVLab/Mono-InternVL.

</details>


### [24] [Best Practices for Large-Scale, Pixel-Wise Crop Mapping and Transfer Learning Workflows](https://arxiv.org/abs/2507.12590)
*Judy Long,Tao Liu,Sean Alexander Woznicki,Miljana Marković,Oskar Marko,Molly Sears*

Main category: cs.CV

TL;DR: 首次全面审查并识别大规模、像素级作物制图的最佳工作流，涵盖有监督和迁移学习方法，并根据数据可用性提出选择建议。


<details>
  <summary>Details</summary>
Motivation: 大规模作物制图是遥感应用的关键。本研究旨在首次全面审查像素级作物制图工作流，比较传统有监督方法和新兴迁移学习方法，识别最优实践，并评估不同因素（如预处理、模型、样本量）对性能的影响，以指导实际应用。

Method: 1. 系统实验：比较六种卫星图像预处理方法和十一种有监督像素级分类模型。2. 评估协同效应：分析不同训练样本量和变量组合的影响。3. 识别最优迁移学习技术：针对不同领域偏移量。4. 数据来源：Landsat 8卫星数据，标签来自CDL可信像素和实地调查。5. 评估地点：在五个不同的农业区域进行方法评估。

Result: 1. 性能最优组合：细尺度间隔预处理与Transformer模型结合，在有监督和可迁移工作流中均表现最佳；RF模型在传统有监督学习和直接迁移到相似领域时表现出快速训练和良好性能。2. 迁移学习效果：迁移学习技术增强了工作流的适应性，其中UDA适用于同质作物类别，而微调在多样化场景中表现稳定。3. 工作流选择策略：工作流选择强烈依赖于标注样本量；样本充足时，有监督训练更准确且泛化性强；样本不足时，匹配领域偏移水平的迁移学习是可行替代方案。

Conclusion: 结合细尺度预处理的Transformer模型是像素级作物制图的最佳实践。工作流的选择应根据具体需求和标注样本的可用性进行调整，迁移学习在数据受限情境下是有效的策略。

Abstract: Crop mapping involves identifying and classifying crop types using spatial
data, primarily derived from remote sensing imagery. This study presents the
first comprehensive review of large-scale, pixel-wise crop mapping workflows,
encompassing both conventional supervised methods and emerging transfer
learning approaches. To identify the optimal supervised crop mapping workflows,
we conducted systematic experiments, comparing six widely adopted satellite
image-based preprocessing methods, alongside eleven supervised pixel-wise
classification models. Additionally, we assessed the synergistic impact of
varied training sample sizes and variable combinations. Moreover, we identified
optimal transfer learning techniques for different magnitudes of domain shift.
The evaluation of best methods was conducted across five diverse agricultural
sites. Landsat 8 served as the primary satellite data source. Labels come from
CDL trusted pixels and field surveys.
  Our findings reveal three key insights. First, fine-scale interval
preprocessing paired with Transformer models consistently delivered optimal
performance for both supervised and transferable workflows. RF offered rapid
training and competitive performance in conventional supervised learning and
direct transfer to similar domains. Second, transfer learning techniques
enhanced workflow adaptability, with UDA being effective for homogeneous crop
classes while fine-tuning remains robust across diverse scenarios. Finally,
workflow choice depends heavily on the availability of labeled samples. With a
sufficient sample size, supervised training typically delivers more accurate
and generalizable results. Below a certain threshold, transfer learning that
matches the level of domain shift is a viable alternative to achieve crop
mapping. Repository:
Best-Practices-for-Large-Scale-Pixel-Wise-Crop-Mapping-and-Transfer-Learning-Workflows

</details>


### [25] [CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling](https://arxiv.org/abs/2507.12591)
*Trong-Thang Pham,Akash Awasthi,Saba Khan,Esteban Duran Marti,Tien-Phat Nguyen,Khoa Vo,Minh Tran,Ngoc Son Nguyen,Cuong Tran Van,Yuki Ikebe,Anh Totti Nguyen,Anh Nguyen,Zhigang Deng,Carol C. Wu,Hien Van Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 本研究发布了首个公开的CT眼动追踪数据集CT-ScanGaze，并提出了新型3D扫描路径预测器CT-Searcher，能生成类放射科医生的3D注视序列，有效解决了CT眼动研究的数据和3D复杂性挑战，并提供了3D扫描路径预测的评估框架。


<details>
  <summary>Details</summary>
Motivation: 理解放射科医生在CT阅片时的眼动行为对于开发有效且可解释的计算机辅助诊断系统至关重要。然而，该领域的研究受限于缺乏公开可用的眼动追踪数据集和CT体积的三维复杂性。

Method: 1. 发布了首个公开的CT眼动注视数据集CT-ScanGaze。2. 引入了CT-Searcher，这是一种专门用于处理CT体积并生成类似放射科医生3D注视序列的新型3D扫描路径预测器，克服了现有预测器仅处理2D输入的局限。3. 开发了一个数据转换流程，将现有2D眼动数据集转换为3D凝视数据，用于预训练CT-Searcher。

Result: 通过在CT-ScanGaze数据集上进行定性和定量评估，证明了所提出方法的有效性，并为医学影像中的3D扫描路径预测提供了一个全面的评估框架。

Conclusion: 本研究通过提供首个CT眼动数据集和新颖的3D扫描路径预测器CT-Searcher，有效推进了CT眼动行为理解和可解释计算机辅助诊断系统的发展，并建立了医学影像3D扫描路径预测的评估标准。

Abstract: Understanding radiologists' eye movement during Computed Tomography (CT)
reading is crucial for developing effective interpretable computer-aided
diagnosis systems. However, CT research in this area has been limited by the
lack of publicly available eye-tracking datasets and the three-dimensional
complexity of CT volumes. To address these challenges, we present the first
publicly available eye gaze dataset on CT, called CT-ScanGaze. Then, we
introduce CT-Searcher, a novel 3D scanpath predictor designed specifically to
process CT volumes and generate radiologist-like 3D fixation sequences,
overcoming the limitations of current scanpath predictors that only handle 2D
inputs. Since deep learning models benefit from a pretraining step, we develop
a pipeline that converts existing 2D gaze datasets into 3D gaze data to
pretrain CT-Searcher. Through both qualitative and quantitative evaluations on
CT-ScanGaze, we demonstrate the effectiveness of our approach and provide a
comprehensive assessment framework for 3D scanpath prediction in medical
imaging.

</details>


### [26] [MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with Biological Knowledge Integration for LiDAR Tree Species Classification](https://arxiv.org/abs/2507.12602)
*Said Ohamouddou,Abdellatif El Afia,Hanaa El Afia,Raddouane Chiheb*

Main category: cs.CV

TL;DR: 本文提出MS-DGCNN++，一种分层多尺度融合动态图卷积网络，通过在不同尺度（局部、分支、冠层）进行语义特征提取和跨尺度信息传播，显著提升了LiDAR点云树种分类精度，并能泛化到通用3D目标识别，同时保持高效性。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云的树种分类因森林环境复杂的多尺度几何结构而极具挑战性。现有MS-DGCNN方法采用并行多尺度处理，未能捕捉树木架构中不同层级之间的语义关系。

Method: 本文提出MS-DGCNN++，一种分层多尺度融合动态图卷积网络。该方法在局部、分支和冠层尺度上提取语义特征，并进行跨尺度信息传播。它采用尺度特定的特征工程，包括局部尺度的标准几何特征、分支尺度的归一化相对向量以及冠层尺度的距离信息。这种分层方法取代了统一的并行处理，生成与自然树木结构对齐的语义差异化表示。

Result: 在STPCTLS数据集上，MS-DGCNN++实现了94.96%的分类精度，优于DGCNN、MS-DGCNN和SOTA模型PPT。在FOR-species20K数据集上，达到67.25%的精度（比MS-DGCNN提高6.1%）。在标准3D目标识别任务中，它在ModelNet40和ModelNet10上分别取得了93.15%和94.05%的总精度，表现优于DGCNN和MS-DGCNN。此外，该方法的参数量和复杂度低于SOTA Transformer方法。

Conclusion: MS-DGCNN++在保持竞争性精度的同时，具有更低的参数量和复杂度，适用于资源受限的应用。该方法不仅在树种分类上表现出色，还能泛化到标准3D目标识别，使其成为一个多功能的点云处理解决方案。

Abstract: Tree species classification from terrestrial LiDAR point clouds is
challenging because of the complex multi-scale geometric structures in forest
environments. Existing approaches using multi-scale dynamic graph convolutional
neural networks (MS-DGCNN) employ parallel multi-scale processing, which fails
to capture the semantic relationships between the hierarchical levels of the
tree architecture. We present MS-DGCNN++, a hierarchical multiscale fusion
dynamic graph convolutional network that uses semantically meaningful feature
extraction at local, branch, and canopy scales with cross-scale information
propagation. Our method employs scale-specific feature engineering, including
standard geometric features for the local scale, normalized relative vectors
for the branch scale, and distance information for the canopy scale. This
hierarchical approach replaces uniform parallel processing with semantically
differentiated representations that are aligned with the natural tree
structure. Under the same proposed tree species data augmentation strategy for
all experiments, MS-DGCNN++ achieved an accuracy of 94.96 \% on STPCTLS,
outperforming DGCNN, MS-DGCNN, and the state-of-the-art model PPT. On
FOR-species20K, it achieves 67.25\% accuracy (6.1\% improvement compared to
MS-DGCNN). For standard 3D object recognition, our method outperformed DGCNN
and MS-DGCNN with overall accuracies of 93.15\% on ModelNet40 and 94.05\% on
ModelNet10. With lower parameters and reduced complexity compared to
state-of-the-art transformer approaches, our method is suitable for
resource-constrained applications while maintaining a competitive accuracy.
Beyond tree classification, the method generalizes to standard 3D object
recognition, establishing it as a versatile solution for diverse point cloud
processing applications. The implementation code is publicly available at
https://github.com/said-ohamouddou/MS-DGCNN2.

</details>


### [27] [Predicting Soccer Penalty Kick Direction Using Human Action Recognition](https://arxiv.org/abs/2507.12617)
*David Freire-Obregón,Oliverio J. Santana,Javier Lorenzo-Navarro,Daniel Hernández-Sosa,Modesto Castrillón-Santana*

Main category: cs.CV

TL;DR: 本研究提出了一个足球点球数据集和基于HAR与元数据的深度学习模型，用于预测射门方向，并取得了优于真实守门员的预测准确率。


<details>
  <summary>Details</summary>
Motivation: 在人类动作识别（HAR）中，动作预测是一个重要课题，但其在现实世界体育场景中的应用受限于缺乏合适的标注数据集。

Method: 本研究构建了一个手动标注的足球点球数据集，用于基于开球前球员动作预测射门方向。同时，提出了一种深度学习分类器，该分类器整合了基于HAR的特征嵌入和上下文元数据，并评估了22种骨干模型（MViTv2, MViTv1, SlowFast, Slow, X3D, I3D, C2D）。

Result: 在预测射门方向（左或右）的任务中，模型最高达到了63.9%的准确率，超越了真实守门员的决策。

Conclusion: 研究结果表明该数据集对于预期动作识别具有价值，并验证了所提出的模型作为体育类预测任务通用方法的潜力。

Abstract: Action anticipation has become a prominent topic in Human Action Recognition
(HAR). However, its application to real-world sports scenarios remains limited
by the availability of suitable annotated datasets. This work presents a novel
dataset of manually annotated soccer penalty kicks to predict shot direction
based on pre-kick player movements. We propose a deep learning classifier to
benchmark this dataset that integrates HAR-based feature embeddings with
contextual metadata. We evaluate twenty-two backbone models across seven
architecture families (MViTv2, MViTv1, SlowFast, Slow, X3D, I3D, C2D),
achieving up to 63.9% accuracy in predicting shot direction (left or right),
outperforming the real goalkeepers' decisions. These results demonstrate the
dataset's value for anticipatory action recognition and validate our model's
potential as a generalizable approach for sports-based predictive tasks.

</details>


### [28] [Funnel-HOI: Top-Down Perception for Zero-Shot HOI Detection](https://arxiv.org/abs/2507.12628)
*Sandipan Sarma,Agney Talwarr,Arijit Sur*

Main category: cs.CV

TL;DR: 提出Funnel-HOI框架，通过在编码器阶段融合HOI特定线索并设计新颖损失，显著提升了人-物交互检测性能，尤其在零样本和稀有类别上表现突出。


<details>
  <summary>Details</summary>
Motivation: 人-物交互检测（HOID）面临标记数据有限导致的长尾分布问题，现有基于Transformer的方法主要侧重于解码器设计，而忽视了在编码器阶段预先捕捉HOI特有线索以获取更强场景理解的重要性。

Method: 提出Funnel-HOI，一个自上而下的框架。它首先探测图像中的对象，然后探查与这些对象相关的动作。核心方法包括：1) 引入新颖的非对称协同注意力机制，利用多模态信息（包含零样本能力）在编码器级别挖掘HOI特有线索，以生成更强的交互表示；2) 设计了一种新颖的损失函数，该函数考虑了对象-动作相关性，并能更好地调节误分类惩罚，以有效指导交互分类器。

Result: 在HICO-DET和V-COCO数据集上进行的广泛实验表明，无论是在全监督还是六种零样本设置下，Funnel-HOI均实现了最先进的性能，对于未见和稀有HOI类别，性能提升分别高达12.4%和8.4%。

Conclusion: Funnel-HOI框架通过在编码器阶段强化HOI特有线索的捕获和引入优化的损失函数，有效解决了人-物交互检测中的长尾和零样本问题，显著提升了整体交互检测性能，尤其在处理稀有和未见交互类别方面表现卓越。

Abstract: Human-object interaction detection (HOID) refers to localizing interactive
human-object pairs in images and identifying the interactions. Since there
could be an exponential number of object-action combinations, labeled data is
limited - leading to a long-tail distribution problem. Recently, zero-shot
learning emerged as a solution, with end-to-end transformer-based object
detectors adapted for HOID becoming successful frameworks. However, their
primary focus is designing improved decoders for learning entangled or
disentangled interpretations of interactions. We advocate that HOI-specific
cues must be anticipated at the encoder stage itself to obtain a stronger scene
interpretation. Consequently, we build a top-down framework named Funnel-HOI
inspired by the human tendency to grasp well-defined concepts first and then
associate them with abstract concepts during scene understanding. We first
probe an image for the presence of objects (well-defined concepts) and then
probe for actions (abstract concepts) associated with them. A novel asymmetric
co-attention mechanism mines these cues utilizing multimodal information
(incorporating zero-shot capabilities) and yields stronger interaction
representations at the encoder level. Furthermore, a novel loss is devised that
considers objectaction relatedness and regulates misclassification penalty
better than existing loss functions for guiding the interaction classifier.
Extensive experiments on the HICO-DET and V-COCO datasets across
fully-supervised and six zero-shot settings reveal our state-of-the-art
performance, with up to 12.4% and 8.4% gains for unseen and rare HOI
categories, respectively.

</details>


### [29] [Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos](https://arxiv.org/abs/2507.12646)
*Kaihua Chen,Tarasha Khurana,Deva Ramanan*

Main category: cs.CV

TL;DR: 本文提出CogNVS，一种从单目视频合成动态场景新颖视角的方法。它通过结合3D重建（用于可见像素）和2D视频扩散模型（用于隐藏像素）来解决现有方法的高成本或几何保真度问题，并通过自监督训练和零样本微调实现高效应用。


<details>
  <summary>Details</summary>
Motivation: 现有从单目视频合成动态场景新颖视角的方法存在局限性：要么需要耗时的测试时4D表示优化，要么在前馈训练时无法保持场景几何。

Method: 本方法基于三个关键洞察：1) 通过动态3D场景重建并从新视角渲染来处理输入和目标视角中都可见的像素；2) 使用前馈2D视频扩散模型（CogNVS）“修复”新视角中隐藏的像素；3) CogNVS可以通过2D视频进行自监督训练，从而在大规模野外视频语料库上进行训练，并能通过测试时微调零样本应用于新的测试视频。

Result: 实验验证表明，CogNVS在从单目视频合成动态场景新颖视角方面，性能优于几乎所有现有技术。

Conclusion: CogNVS提出了一种有效且性能卓越的动态场景新颖视角合成方法，它通过结合3D重建与2D视频扩散模型，并利用自监督和零样本应用策略，成功克服了现有技术的挑战。

Abstract: We explore novel-view synthesis for dynamic scenes from monocular videos.
Prior approaches rely on costly test-time optimization of 4D representations or
do not preserve scene geometry when trained in a feed-forward manner. Our
approach is based on three key insights: (1) covisible pixels (that are visible
in both the input and target views) can be rendered by first reconstructing the
dynamic 3D scene and rendering the reconstruction from the novel-views and (2)
hidden pixels in novel views can be "inpainted" with feed-forward 2D video
diffusion models. Notably, our video inpainting diffusion model (CogNVS) can be
self-supervised from 2D videos, allowing us to train it on a large corpus of
in-the-wild videos. This in turn allows for (3) CogNVS to be applied zero-shot
to novel test videos via test-time finetuning. We empirically verify that
CogNVS outperforms almost all prior art for novel-view synthesis of dynamic
scenes from monocular videos.

</details>


### [30] [Integrated Oculomics and Lipidomics Reveal Microvascular Metabolic Signatures Associated with Cardiovascular Health in a Healthy Cohort](https://arxiv.org/abs/2507.12663)
*Inamullah,Ernesto Elias Vidal Rosas,Imran Razzak,Shoaib Jameel*

Main category: cs.CV

TL;DR: 本研究结合视网膜微血管特征和血清脂质组学数据，在健康人群中发现了心血管疾病的早期生物标志物，有望改进早期诊断和预防。


<details>
  <summary>Details</summary>
Motivation: 现有心血管疾病风险分层方法未能有效检测早期亚临床变化；既往研究未整合视网膜微血管特征与血清脂质组学数据，以识别心血管风险指标。

Method: 引入创新的成像组学框架，通过深度学习处理图像获取视网膜微血管特征，并使用超高效液相色谱-电喷雾电离-高分辨质谱（UHPLC-ESI-HRMS）进行血清脂质谱分析。在健康人群中进行了大规模、协变量调整和分层相关性分析。

Result: 建立了视网膜平均动脉宽度、血管密度与三酰甘油（TAGs）、二酰甘油（DAGs）和神经酰胺（Cers）等脂质亚类之间强烈的、与年龄和性别无关的相关性，表明代谢应激下微血管重塑的趋同机制。

Conclusion: 本研究通过连接血管结构表型与特定脂质种类，弥补了早期心血管疾病发病机制理解的关键空白，为识别稳健、非侵入性生物标志物提供了新视角和机会，有望支持心血管医疗中早期检测、靶向预防和个性化方法的改进。

Abstract: Cardiovascular disease (CVD) remains the leading global cause of mortality,
yet current risk stratification methods often fail to detect early, subclinical
changes. Previous studies have generally not integrated retinal
microvasculature characteristics with comprehensive serum lipidomic profiles as
potential indicators of CVD risk. In this study, an innovative imaging omics
framework was introduced, combining retinal microvascular traits derived
through deep learning based image processing with serum lipidomic data to
highlight asymptomatic biomarkers of cardiovascular risk beyond the
conventional lipid panel. This represents the first large scale, covariate
adjusted and stratified correlation analysis conducted in a healthy population,
which is essential for identifying early indicators of disease. Retinal
phenotypes were quantified using automated image analysis tools, while serum
lipid profiling was performed by Ultra High Performance Liquid Chromatography
Electrospray ionization High resolution mass spectrometry (UHPLC ESI HRMS).
Strong, age- and sex-independent correlations were established, particularly
between average artery width, vessel density, and lipid subclasses such as
triacylglycerols (TAGs), diacylglycerols (DAGs), and ceramides (Cers). These
associations suggest a converging mechanism of microvascular remodeling under
metabolic stress. By linking detailed
  vascular structural phenotypes to specific lipid species, this study fills a
critical gap in the understanding of early CVD pathogenesis. This integration
not only offers a novel perspective on microvascular metabolic associations but
also presents a significant opportunity for the identification of robust,
non-invasive biomarkers. Ultimately, these findings may support improved early
detection, targeted prevention, and personalized approaches in cardiovascular
healthcare.

</details>


### [31] [FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks](https://arxiv.org/abs/2507.12675)
*Christina Thrainer,Md Meftahul Ferdaus,Mahdi Abdelguerfi,Christian Guetl,Steven Sloan,Kendall N. Niles,Ken Pathak*

Main category: cs.CV

TL;DR: FORTRESS是一种新的架构，通过结合深度可分离卷积和自适应KAN集成，解决了土木基础设施结构缺陷分割中精度与实时效率的平衡问题，实现了卓越的性能和效率提升。


<details>
  <summary>Details</summary>
Motivation: 自动结构缺陷分割在土木基础设施领域面临一个关键挑战：如何在实现高精度的同时保持计算效率以支持实时部署。

Method: 本文提出了FORTRESS架构，通过系统性的深度可分离卷积框架（实现每层参数减少3.6倍）、自适应TiKAN集成（仅在计算上有益时选择性应用函数组合变换）和多尺度注意力融合（结合空间、通道和KAN增强特征），平衡了精度和速度。

Result: FORTRESS实现了显著的效率提升，包括91%的参数减少（从31M到2.9M）、91%的计算复杂度减少（从13.7到1.17 GFLOPs）以及3倍的推理速度提升。同时，在基准数据集上F1-score达到0.771，mean IoU达到0.677，性能优于U-Net、SA-UNet和U-KAN等现有方法。

Conclusion: FORTRESS的双重优化策略对于实现最佳性能至关重要，使其成为在精度和计算效率都至关重要的资源受限环境中进行实用结构缺陷分割的强大解决方案。

Abstract: Automated structural defect segmentation in civil infrastructure faces a
critical challenge: achieving high accuracy while maintaining computational
efficiency for real-time deployment. This paper presents FORTRESS
(Function-composition Optimized Real-Time Resilient Structural Segmentation), a
new architecture that balances accuracy and speed by using a special method
that combines depthwise separable convolutions with adaptive Kolmogorov-Arnold
Network integration. FORTRESS incorporates three key innovations: a systematic
depthwise separable convolution framework achieving a 3.6x parameter reduction
per layer, adaptive TiKAN integration that selectively applies function
composition transformations only when computationally beneficial, and
multi-scale attention fusion combining spatial, channel, and KAN-enhanced
features across decoder levels. The architecture achieves remarkable efficiency
gains with 91% parameter reduction (31M to 2.9M), 91% computational complexity
reduction (13.7 to 1.17 GFLOPs), and 3x inference speed improvement while
delivering superior segmentation performance. Evaluation on benchmark
infrastructure datasets demonstrates state-of-the-art results with an F1- score
of 0.771 and a mean IoU of 0.677, significantly outperforming existing methods
including U-Net, SA-UNet, and U- KAN. The dual optimization strategy proves
essential for optimal performance, establishing FORTRESS as a robust solution
for practical structural defect segmentation in resource-constrained
environments where both accuracy and computational efficiency are paramount.
Comprehensive architectural specifications are provided in the Supplemental
Material. Source code is available at URL:
https://github.com/faeyelab/fortress-paper-code.

</details>


### [32] [NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement](https://arxiv.org/abs/2507.12714)
*Yang Yang,Dongni Mao,Hiroaki Santo,Yasuyuki Matsushita,Fumio Okura*

Main category: cs.CV

TL;DR: 本文提出了一种名为NeuraLeaf的神经参数模型，用于3D叶片建模和重建。该模型通过解耦2D基础形状和3D变形，结合新颖的无骨架蒙皮模型和新数据集，成功生成了多样化的叶片形态并实现了对3D观测数据的精确拟合。


<details>
  <summary>Details</summary>
Motivation: 植物建模和重建（特别是3D叶片）对农业和计算机图形学至关重要。尽管神经参数模型在人体和动物领域已有广泛研究，但植物叶片因其形状多样性和柔性变形而面临独特的挑战，现有模型难以有效处理。

Method: 研究者开发了神经参数模型NeuraLeaf。该模型将叶片几何形状解耦为2D基础形状和3D变形，利用叶片可近似为2D平面的特性，从丰富的2D叶片图像数据集中学习基础形状，并同时学习与几何对齐的纹理。为建模3D变形，本文提出了一种新颖的无骨架蒙皮模型，并创建了一个名为DeformLeaf的新捕获3D叶片数据集。

Result: NeuraLeaf模型成功生成了各种具有变形的叶片形状。该模型能准确拟合3D观测数据，如深度图和点云。

Conclusion: NeuraLeaf通过其解耦表示、创新的无骨架蒙皮模型和新的3D数据集，有效解决了3D叶片建模和重建中的多样性和变形挑战，为植物学和图形学应用提供了精确且多功能的工具。

Abstract: We develop a neural parametric model for 3D leaves for plant modeling and
reconstruction that are essential for agriculture and computer graphics. While
neural parametric models are actively studied for humans and animals, plant
leaves present unique challenges due to their diverse shapes and flexible
deformation. To this problem, we introduce a neural parametric model for
leaves, NeuraLeaf. Capitalizing on the fact that flattened leaf shapes can be
approximated as a 2D plane, NeuraLeaf disentangles the leaves' geometry into
their 2D base shapes and 3D deformations. This representation allows learning
from rich sources of 2D leaf image datasets for the base shapes, and also has
the advantage of simultaneously learning textures aligned with the geometry. To
model the 3D deformation, we propose a novel skeleton-free skinning model and
create a newly captured 3D leaf dataset called DeformLeaf. We show that
NeuraLeaf successfully generates a wide range of leaf shapes with deformation,
resulting in accurate model fitting to 3D observations like depth maps and
point clouds. Our implementation and dataset are available at
https://neuraleaf-yang.github.io/.

</details>


### [33] [SOD-YOLO: Enhancing YOLO-Based Detection of Small Objects in UAV Imagery](https://arxiv.org/abs/2507.12727)
*Peijun Wang,Jinhua Zhao*

Main category: cs.CV

TL;DR: SOD-YOLO是一个基于YOLOv8的改进模型，通过集成ASF、添加P2层和使用Soft-NMS，显著提升了小目标检测性能，特别适用于无人机图像。


<details>
  <summary>Details</summary>
Motivation: 小目标检测在目标检测领域仍然是一个具有挑战性的问题。

Method: 提出了一种增强型YOLOv8模型SOD-YOLO。该模型在neck部分集成了ASF机制以增强多尺度特征融合，添加了P2小目标检测层以提供更高分辨率的特征图，并采用Soft-NMS来优化置信度分数和保留真实阳性。

Result: 在VisDrone2019-DET数据集上，SOD-YOLO的检测性能显著提升，与基线模型相比，mAP$_{50:95}$提高了36.1%，mAP$_{50}$提高了20.6%。

Conclusion: SOD-YOLO为无人机图像中的小目标检测提供了一个实用且高效的解决方案。

Abstract: Small object detection remains a challenging problem in the field of object
detection. To address this challenge, we propose an enhanced YOLOv8-based
model, SOD-YOLO. This model integrates an ASF mechanism in the neck to enhance
multi-scale feature fusion, adds a Small Object Detection Layer (named P2) to
provide higher-resolution feature maps for better small object detection, and
employs Soft-NMS to refine confidence scores and retain true positives.
Experimental results demonstrate that SOD-YOLO significantly improves detection
performance, achieving a 36.1% increase in mAP$_{50:95}$ and 20.6% increase in
mAP$_{50}$ on the VisDrone2019-DET dataset compared to the baseline model.
These enhancements make SOD-YOLO a practical and efficient solution for small
object detection in UAV imagery. Our source code, hyper-parameters, and model
weights are available at https://github.com/iamwangxiaobai/SOD-YOLO.

</details>


### [34] [A Privacy-Preserving Semantic-Segmentation Method Using Domain-Adaptation Technique](https://arxiv.org/abs/2507.12730)
*Homare Sueyoshi,Kiyoshi Nishikawa,Hitoshi Kiya*

Main category: cs.CV

TL;DR: 提出一种新的隐私保护语义分割方法，通过对ViT嵌入结构进行域适应，在加密训练和测试图像的同时，保持与未加密模型接近的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了在语义分割任务中实现数据隐私保护，同时不显著牺牲模型的性能，需要一种能对训练和测试图像进行加密处理的方法。

Method: 该方法通过在Vision Transformer (ViT) 的嵌入结构上应用域适应技术，实现对用于模型训练和测试的图像进行感知加密，以达到隐私保护的目的。

Result: 实验结果表明，所提出的方法在对图像进行加密后，仍能提供与未加密模型几乎相同的语义分割准确率。在强大的ViT语义分割模型（如Segmentation Transformer）上验证了其有效性。

Conclusion: 该研究成功开发了一种有效的隐私保护语义分割方法，它能够在保护数据隐私的同时，维持高水平的模型准确性，证明了基于ViT嵌入结构域适应的可行性。

Abstract: We propose a privacy-preserving semantic-segmentation method for applying
perceptual encryption to images used for model training in addition to test
images. This method also provides almost the same accuracy as models without
any encryption. The above performance is achieved using a domain-adaptation
technique on the embedding structure of the Vision Transformer (ViT). The
effectiveness of the proposed method was experimentally confirmed in terms of
the accuracy of semantic segmentation when using a powerful
semantic-segmentation model with ViT called Segmentation Transformer.

</details>


### [35] [Transformer-based Spatial Grounding: A Comprehensive Survey](https://arxiv.org/abs/2507.12739)
*Ijazul Haq,Muhammad Saqib,Yingjie Zhang*

Main category: cs.CV

TL;DR: 本文对2018至2025年间基于Transformer的空间定位方法进行了系统性文献综述，旨在弥补该领域缺乏全面综合分析的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的模型推动了空间定位的快速发展，但该领域缺乏对现有方法、数据集使用、评估指标和工业应用性的全面综合分析。

Method: 本文通过对2018年至2025年间基于Transformer的空间定位方法进行系统性文献综述。

Result: 分析识别了主流模型架构、常用数据集、广泛采用的评估指标，并强调了关键方法论趋势和最佳实践。

Conclusion: 本研究为研究人员和实践者提供了重要见解和结构化指导，以促进开发稳健、可靠且符合工业需求的基于Transformer的空间定位模型。

Abstract: Spatial grounding, the process of associating natural language expressions
with corresponding image regions, has rapidly advanced due to the introduction
of transformer-based models, significantly enhancing multimodal representation
and cross-modal alignment. Despite this progress, the field lacks a
comprehensive synthesis of current methodologies, dataset usage, evaluation
metrics, and industrial applicability. This paper presents a systematic
literature review of transformer-based spatial grounding approaches from 2018
to 2025. Our analysis identifies dominant model architectures, prevalent
datasets, and widely adopted evaluation metrics, alongside highlighting key
methodological trends and best practices. This study provides essential
insights and structured guidance for researchers and practitioners,
facilitating the development of robust, reliable, and industry-ready
transformer-based spatial grounding models.

</details>


### [36] [Domain-Enhanced Dual-Branch Model for Efficient and Interpretable Accident Anticipation](https://arxiv.org/abs/2507.12755)
*Yanchen Guan,Haicheng Liao,Chengyue Wang,Bonan Wang,Jiaxun Zhang,Jia Hu,Zhenning Li*

Main category: cs.CV

TL;DR: 提出一种结合视频和文本数据的双分支多模态大模型交通事故预测框架，在多个基准数据集上实现卓越性能，并树立了新的行业标杆。


<details>
  <summary>Details</summary>
Motivation: 为自动驾驶技术开发精确高效的交通事故预测系统至关重要，以实现及时干预和损失预防。

Method: 提出一个双分支架构的事故预测框架，有效整合行车记录仪视频的视觉信息和事故报告的结构化文本数据。通过大模型（GPT-4o, Long-CLIP）和提示工程策略，引入特征聚合方法，实现多模态输入无缝集成，并生成可操作反馈和标准化事故档案。

Result: 在DAD、CCD和A3D等基准数据集上的综合评估验证了该方法具有卓越的预测准确性、增强的响应能力、降低的计算开销和改进的可解释性。

Conclusion: 该方法在交通事故预测方面树立了新的SOTA（State-Of-The-Art）性能基准。

Abstract: Developing precise and computationally efficient traffic accident
anticipation system is crucial for contemporary autonomous driving
technologies, enabling timely intervention and loss prevention. In this paper,
we propose an accident anticipation framework employing a dual-branch
architecture that effectively integrates visual information from dashcam videos
with structured textual data derived from accident reports. Furthermore, we
introduce a feature aggregation method that facilitates seamless integration of
multimodal inputs through large models (GPT-4o, Long-CLIP), complemented by
targeted prompt engineering strategies to produce actionable feedback and
standardized accident archives. Comprehensive evaluations conducted on
benchmark datasets (DAD, CCD, and A3D) validate the superior predictive
accuracy, enhanced responsiveness, reduced computational overhead, and improved
interpretability of our approach, thus establishing a new benchmark for
state-of-the-art performance in traffic accident anticipation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [37] [AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education](https://arxiv.org/abs/2507.12484)
*Jarosław A. Chudziak,Adam Kostka*

Main category: cs.AI

TL;DR: 针对现有AI辅导系统在数学领域反应式、缺乏深度教学的问题，本文提出一个新型多智能体AI辅导平台，旨在提供结构化、个性化和工具辅助的学习体验。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅导系统（尤其在数学领域）主要提供直接答案，缺乏促使深度反思和结构化教学工具。研究动机在于开发超越反应式帮助，实现结构化、个性化和工具辅助学习的AI辅导系统。

Method: 引入一个新颖的多智能体AI辅导平台。该平台整合了自适应和个性化反馈、结构化课程生成以及教材知识检索功能，以支持模块化、工具辅助的学习过程。

Result: 该系统能帮助学生学习新主题、识别并解决弱点、有效复习考试，并进行无限量个性化练习。

Conclusion: 本文通过引入一个结合教学智能体和AI驱动组件的新型平台，为教育AI领域特别是数学教学提供了模块化且有效的系统。

Abstract: The growing ubiquity of artificial intelligence (AI), in particular large
language models (LLMs), has profoundly altered the way in which learners gain
knowledge and interact with learning material, with many claiming that AI
positively influences their learning achievements. Despite this advancement,
current AI tutoring systems face limitations associated with their reactive
nature, often providing direct answers without encouraging deep reflection or
incorporating structured pedagogical tools and strategies. This limitation is
most apparent in the field of mathematics, in which AI tutoring systems remain
underdeveloped. This research addresses the question: How can AI tutoring
systems move beyond providing reactive assistance to enable structured,
individualized, and tool-assisted learning experiences? We introduce a novel
multi-agent AI tutoring platform that combines adaptive and personalized
feedback, structured course generation, and textbook knowledge retrieval to
enable modular, tool-assisted learning processes. This system allows students
to learn new topics while identifying and targeting their weaknesses, revise
for exams effectively, and practice on an unlimited number of personalized
exercises. This article contributes to the field of artificial intelligence in
education by introducing a novel platform that brings together pedagogical
agents and AI-driven components, augmenting the field with modular and
effective systems for teaching mathematics.

</details>


### [38] [MR-LDM -- The Merge-Reactive Longitudinal Decision Model: Game Theoretic Human Decision Modeling for Interactive Sim Agents](https://arxiv.org/abs/2507.12494)
*Dustin Holley,Jovin D'sa,Hossein Nourkhiz Mahjoub,Gibran Ali*

Main category: cs.AI

TL;DR: 本文提出一种统一的博弈论与动力学模型，旨在通过改进支付函数和滞后车辆动作，提升高速公路汇入场景中类人驾驶行为的仿真真实性。该模型在真实数据上验证了其复杂交互复现能力，并具有高效的计算性能。


<details>
  <summary>Details</summary>
Motivation: 为支持自动驾驶技术发展，亟需提升仿真环境中对真实世界驾驶员行为的复现能力。现有高速公路汇入仿真在战术决策建模上存在局限，例如动作集有限或支付函数参数多且边界受限。

Method: 本研究提出一个针对高速公路汇入场景的博弈论战术决策模型，优化了支付函数和滞后车辆动作，并将其与底层动力学模型耦合，形成一个统一的决策与动力学框架，以实现可解释且真实的交互仿真。

Result: 所提出的模型在真实世界数据集上验证后，成功复现了复杂的交互行为。该模型集成到高精度仿真环境后，被证实具有足够的计算效率，适用于大规模仿真以支持自动驾驶开发。

Conclusion: 该统一的博弈论与动力学模型有效提升了高速公路汇入仿真中驾驶员行为的真实性、可解释性和计算效率，为自动驾驶技术的发展提供了重要支持。

Abstract: Enhancing simulation environments to replicate real-world driver behavior,
i.e., more humanlike sim agents, is essential for developing autonomous vehicle
technology. In the context of highway merging, previous works have studied the
operational-level yielding dynamics of lag vehicles in response to a merging
car at highway on-ramps. Other works focusing on tactical decision modeling
generally consider limited action sets or utilize payoff functions with large
parameter sets and limited payoff bounds. In this work, we aim to improve the
simulation of the highway merge scenario by targeting a game theoretic model
for tactical decision-making with improved payoff functions and lag actions. We
couple this with an underlying dynamics model to have a unified decision and
dynamics model that can capture merging interactions and simulate more
realistic interactions in an explainable and interpretable fashion. The
proposed model demonstrated good reproducibility of complex interactions when
validated on a real-world dataset. The model was finally integrated into a high
fidelity simulation environment and confirmed to have adequate computation time
efficiency for use in large-scale simulations to support autonomous vehicle
development.

</details>


### [39] [A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs](https://arxiv.org/abs/2507.12599)
*Léo Saulières*

Main category: cs.AI

TL;DR: 本文对可解释强化学习（XRL）领域进行了综述，提出了一种基于“解释什么”和“如何解释”的分类法，并据此评审了250多篇论文，同时指出了相关领域和未来的研究需求。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能模型（特别是深度神经网络）的成功伴随着其内部机制的不透明性。为理解这些机制并解释AI模型的输出，需要发展可解释AI（XAI）方法。具体到强化学习，存在解释智能体行为的需求，这促使了可解释强化学习（XRL）领域的发展。

Method: 本研究提出了一种直观的分类法，基于“解释什么”（解释目标）和“如何解释”（解释方式）这两个问题。利用此分类法对250多篇关于XRL的论文进行了最先进的综述。此外，还介绍了与XRL相关的邻近领域，并识别了该领域的一些需求。

Result: 通过提出的分类法，本文对250多篇XRL论文提供了全面的最新综述。同时，识别并呈现了一系列与XRL紧密相关的领域，并指出了XRL领域未来需要关注和发展的方向。

Conclusion: 本论文通过提出一种新颖的分类法，成功地对可解释强化学习领域进行了系统性的综述，不仅提供了当前研究的全面视角，还为该领域未来的研究方向和发展提供了重要的指导和需求分析。

Abstract: The success of recent Artificial Intelligence (AI) models has been
accompanied by the opacity of their internal mechanisms, due notably to the use
of deep neural networks. In order to understand these internal mechanisms and
explain the output of these AI models, a set of methods have been proposed,
grouped under the domain of eXplainable AI (XAI). This paper focuses on a
sub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims
to explain the actions of an agent that has learned by reinforcement learning.
We propose an intuitive taxonomy based on two questions "What" and "How". The
first question focuses on the target that the method explains, while the second
relates to the way the explanation is provided. We use this taxonomy to provide
a state-of-the-art review of over 250 papers. In addition, we present a set of
domains close to XRL, which we believe should get attention from the community.
Finally, we identify some needs for the field of XRL.

</details>


### [40] [Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models](https://arxiv.org/abs/2507.12666)
*Alex Zook,Josef Spjut,Jonathan Tremblay*

Main category: cs.AI

TL;DR: 该论文提出一个自动化游戏设计迭代框架，结合强化学习（RL）代理进行游戏测试与大型多模态模型（LMM）进行游戏修订，通过分析RL代理的行为轨迹来迭代优化游戏机制。


<details>
  <summary>Details</summary>
Motivation: 现代生成式游戏设计系统难以理解静态规则和内容如何转化为动态玩家行为，它们仅检查代码或资产，无法捕捉到这一点。

Method: 提出一个自动化设计迭代框架。该框架将强化学习（RL）代理（负责游戏测试并生成数值指标或视觉摘要）与大型多模态模型（LMM）（接收游戏目标和当前配置，分析游戏轨迹并修改配置以引导未来行为）结合，形成一个迭代循环。

Result: 结果表明，大型多模态模型（LMM）能够根据强化学习（RL）代理提供的行为轨迹进行推理，从而迭代地改进游戏机制。

Conclusion: 该研究指明了开发实用且可扩展的AI辅助游戏设计工具的方向。

Abstract: Game design hinges on understanding how static rules and content translate
into dynamic player behavior - something modern generative systems that inspect
only a game's code or assets struggle to capture. We present an automated
design iteration framework that closes this gap by pairing a reinforcement
learning (RL) agent, which playtests the game, with a large multimodal model
(LMM), which revises the game based on what the agent does. In each loop the RL
player completes several episodes, producing (i) numerical play metrics and/or
(ii) a compact image strip summarising recent video frames. The LMM designer
receives a gameplay goal and the current game configuration, analyses the play
traces, and edits the configuration to steer future behaviour toward the goal.
We demonstrate results that LMMs can reason over behavioral traces supplied by
RL agents to iteratively refine game mechanics, pointing toward practical,
scalable tools for AI-assisted game design.

</details>


### [41] [Benchmarking Deception Probes via Black-to-White Performance Boosts](https://arxiv.org/abs/2507.12691)
*Avi Parrack,Carlo Leonardo Attubato,Stefan Heimersheim*

Main category: cs.AI

TL;DR: 本文评估了现有AI欺骗探测器在实践中的有效性，通过比较白盒与黑盒监控，发现探测器提供了虽弱但令人鼓舞的性能提升。


<details>
  <summary>Details</summary>
Motivation: AI助手可能对用户进行欺骗。尽管已有线性分类器（欺骗探测器）被训练用于区分语言模型内部的欺骗性与诚实性激活，但这些探测器在实际检测欺骗中的有效性及其抵抗规避策略的能力仍不明确。

Method: 研究者比较了白盒监控（可访问令牌级探测激活）和黑盒监控（无此访问）的表现。通过白盒监控相对于黑盒监控的性能提升（即“黑到白性能提升”）来基准测试欺骗探测器。

Result: 研究发现现有欺骗探测器带来了虽弱但令人鼓舞的黑到白性能提升。

Conclusion: 现有欺骗探测器通过白盒监控，在检测AI助手内部欺骗性响应方面，相对于黑盒监控展现出弱但积极的性能提升，表明其具有一定的实践潜力。

Abstract: AI assistants will occasionally respond deceptively to user queries.
Recently, linear classifiers (called "deception probes") have been trained to
distinguish the internal activations of a language model during deceptive
versus honest responses. However, it's unclear how effective these probes are
at detecting deception in practice, nor whether such probes are resistant to
simple counter strategies from a deceptive assistant who wishes to evade
detection. In this paper, we compare white-box monitoring (where the monitor
has access to token-level probe activations) to black-box monitoring (without
such access). We benchmark deception probes by the extent to which the white
box monitor outperforms the black-box monitor, i.e. the black-to-white
performance boost. We find weak but encouraging black-to-white performance
boosts from existing deception probes.

</details>


### [42] [Imitating Mistakes in a Learning Companion AI Agent for Online Peer Learning](https://arxiv.org/abs/2507.12801)
*Sosui Moribe,Taketoshi Ushiama*

Main category: cs.AI

TL;DR: 本研究旨在开发一个AI智能体作为学习伙伴，以实现随时随地的同伴学习，并通过假设相同熟练度学习者会犯相似错误，以英语写作作为具体验证场景。


<details>
  <summary>Details</summary>
Motivation: 鉴于同伴学习能促进学习者自发思考且有效性已被证实，但传统人际同伴学习存在局限性（如需同等熟练度伙伴、不易随时随地进行），本研究旨在开发AI智能体克服这些限制，使同伴学习更普适有效。

Method: 开发一个AI智能体作为学习伙伴。研究方法基于一个核心假设：与学习者熟练度相同的同伴会犯同样的错误。该方法将以英语写作作为具体应用和验证领域。

Result: 抽象中未提及研究结果。

Conclusion: 抽象中未提及研究结论。

Abstract: In recent years, peer learning has gained attention as a method that promotes
spontaneous thinking among learners, and its effectiveness has been confirmed
by numerous studies. This study aims to develop an AI Agent as a learning
companion that enables peer learning anytime and anywhere. However, peer
learning between humans has various limitations, and it is not always
effective. Effective peer learning requires companions at the same proficiency
levels. In this study, we assume that a learner's peers with the same
proficiency level as the learner make the same mistakes as the learner does and
focus on English composition as a specific example to validate this approach.

</details>


### [43] [MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models](https://arxiv.org/abs/2507.12806)
*Zhiwei Liu,Jielin Qiu,Shiyu Wang,Jianguo Zhang,Zuxin Liu,Roshan Ram,Haolin Chen,Weiran Yao,Huan Wang,Shelby Heinecke,Silvio Savarese,Caiming Xiong*

Main category: cs.AI

TL;DR: MCPEval是一个开源框架，通过自动化任务生成和深度评估，实现对LLM代理的鲁棒、可扩展和端到端评估，解决了现有评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理评估方法依赖静态基准和大量人工数据收集，效率低下且限制了实际评估，因此需要一个更健壮、可扩展的评估框架。

Method: 引入MCPEval，一个基于模型上下文协议（MCP）的开源框架。它自动化端到端任务生成和LLM代理的深度评估，标准化评估指标，与原生代理工具无缝集成，并消除手动构建评估流程的努力。

Result: 在五个真实世界领域的实证结果表明，MCPEval能有效揭示细致入微的、领域特定的性能差异。

Conclusion: MCPEval通过提供一个可复现和标准化的LLM代理评估工具，促进了该领域的发展，其开源发布有助于社区协作。

Abstract: The rapid rise of Large Language Models (LLMs)-based intelligent agents
underscores the need for robust, scalable evaluation frameworks. Existing
methods rely on static benchmarks and labor-intensive data collection, limiting
practical assessment. We introduce \oursystemname, an open-source Model Context
Protocol (MCP)-based framework that automates end-to-end task generation and
deep evaluation of LLM agents across diverse domains. MCPEval standardizes
metrics, seamlessly integrates with native agent tools, and eliminates manual
effort in building evaluation pipelines. Empirical results across five
real-world domains show its effectiveness in revealing nuanced, domain-specific
performance. We publicly release MCPEval
https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and
standardized LLM agent evaluation.

</details>


### [44] [Emotional Support with LLM-based Empathetic Dialogue Generation](https://arxiv.org/abs/2507.12820)
*Shiquan Wang,Ruiyu Fang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: 该论文提出了NLPCC 2025 Task 8情感支持对话（ESC）的解决方案，通过提示工程和微调技术（包括LoRA和全参数微调）增强大型语言模型，并在竞赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 为满足日益增长的心理健康支持需求，旨在通过对话提供共情且有效的情感帮助。

Method: 利用大型语言模型（LLMs），通过提示工程和微调（包括参数高效的低秩适应LoRA和全参数微调）策略，提高模型生成支持性和上下文适宜响应的能力。

Result: 所提出的最佳模型在NLPCC 2025 Task 8竞赛中排名第二，凸显了结合大型语言模型与有效适应方法在情感支持对话任务中的潜力。

Conclusion: 结合大型语言模型与有效适应方法在情感支持对话中表现出巨大潜力。未来工作将专注于进一步增强情感理解和响应个性化，以构建更实用可靠的情感支持系统。

Abstract: Emotional Support Conversation (ESC) aims to provide empathetic and effective
emotional assistance through dialogue, addressing the growing demand for mental
health support. This paper presents our solution for the NLPCC 2025 Task 8 ESC
evaluation, where we leverage large-scale language models enhanced by prompt
engineering and finetuning techniques. We explore both parameter-efficient
Low-Rank Adaptation and full-parameter fine-tuning strategies to improve the
model's ability to generate supportive and contextually appropriate responses.
Our best model ranked second in the competition, highlighting the potential of
combining LLMs with effective adaptation methods for ESC tasks. Future work
will focus on further enhancing emotional understanding and response
personalization to build more practical and reliable emotional support systems.

</details>


### [45] [Assessing adaptive world models in machines with novel games](https://arxiv.org/abs/2507.12821)
*Lance Ying,Katherine M. Collins,Prafull Sharma,Cedric Colas,Kaiya Ivy Zhao,Adrian Weller,Zenna Tavares,Phillip Isola,Samuel J. Gershman,Jacob D. Andreas,Thomas L. Griffiths,Francois Chollet,Kelsey R. Allen,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 人类智能的快速适应能力源于高效构建世界模型。当前AI对世界模型的评估过于狭窄，本文提出基于“新颖游戏”的新评估框架，以衡量AI在陌生环境中快速学习和适应的能力，旨在推动类人通用人工智能发展。


<details>
  <summary>Details</summary>
Motivation: 人类智能在陌生情境下展现出卓越的快速适应和问题解决能力，这被认为与高效构建和完善内部环境表征（世界模型）紧密相关。然而，当前AI领域对世界模型的理解和评估过于狭隘，侧重于从海量数据中学习静态表示，而非在全新环境中通过互动和探索高效地学习这些表示。因此，亟需一个能衡量AI在陌生环境中快速学习和适应能力的新评估框架。

Method: 本文借鉴认知科学中关于人类高效学习和适应的研究，提出了一种新的评估范式。具体地，建议构建一系列基于“新颖游戏”（novel games）的基准测试，这些游戏具有真正、深刻且持续更新的底层结构新颖性。文章详细阐述了构建此类游戏的关键要素，并提出了相应的度量标准，以明确挑战和评估智能体快速进行世界模型归纳的能力。

Result: 作为一篇观点性文章，本文没有呈现实验结果，而是提出了一个创新性的评估框架和方法。该框架通过引入“新颖游戏”的概念和具体评估指标，旨在更准确地衡量AI在面对陌生环境时快速构建和完善世界模型的能力，从而推动AI系统向类人快速适应和鲁棒泛化能力发展。

Conclusion: 所提出的新评估框架有望激发未来对AI世界模型的评估工作，并为开发具备类人快速适应和鲁棒泛化能力的AI系统提供关键一步，这正是实现通用人工智能（AGI）的核心组成部分。

Abstract: Human intelligence exhibits a remarkable capacity for rapid adaptation and
effective problem-solving in novel and unfamiliar contexts. We argue that this
profound adaptability is fundamentally linked to the efficient construction and
refinement of internal representations of the environment, commonly referred to
as world models, and we refer to this adaptation mechanism as world model
induction. However, current understanding and evaluation of world models in
artificial intelligence (AI) remains narrow, often focusing on static
representations learned from training on a massive corpora of data, instead of
the efficiency and efficacy of models in learning these representations through
interaction and exploration within a novel environment. In this Perspective, we
provide a view of world model induction drawing on decades of research in
cognitive science on how humans learn and adapt so efficiently; we then call
for a new evaluation framework for assessing adaptive world models in AI.
Concretely, we propose a new benchmarking paradigm based on suites of carefully
designed games with genuine, deep and continually refreshing novelty in the
underlying game structures -- we refer to this kind of games as novel games. We
detail key desiderata for constructing these games and propose appropriate
metrics to explicitly challenge and evaluate the agent's ability for rapid
world model induction. We hope that this new evaluation framework will inspire
future evaluation efforts on world models in AI and provide a crucial step
towards developing AI systems capable of the human-like rapid adaptation and
robust generalization -- a critical component of artificial general
intelligence.

</details>


### [46] [Information-Theoretic Aggregation of Ethical Attributes in Simulated-Command](https://arxiv.org/abs/2507.12862)
*Hussein Abbass,Taylan Akay,Harrison Tolley*

Main category: cs.AI

TL;DR: 本文提出一种方法，将人类伦理判断从模拟决策循环中移出，通过自动加权伦理属性（借鉴熵概念）来有效进行大规模伦理场景模拟和测试，最终由人类指挥官进行高层决策。


<details>
  <summary>Details</summary>
Motivation: 在AI时代，指挥官需要快速模拟大量场景，其中涉及伦理决策。然而，让人类对模拟中的每个伦理决策进行判断效率低下且不可行，导致无法及时探索海量场景。

Method: 将人类判断置于模拟决策循环之外，让人类设计伦理度量空间，模拟环境负责探索此空间。当模拟完成测试周期后，环境将提供选项供人类指挥官选择。本文假设伦理度量设计问题已解决，并专注于解决如何在模拟运行时动态权衡伦理决策（即动态加权伦理属性）的问题。作者借鉴多准则决策文献中利用熵概念确定权重的方法，来自动计算模拟中伦理属性的权重。

Result: 提出了在基于模拟的测试和评估中，自动计算伦理属性权重的多种方法，旨在实现对包含伦理影响的决策选项的大规模、高效模拟和探索。

Conclusion: 该方法通过将人类伦理判断从繁琐的实时决策中抽离，转变为对预定义伦理框架和模拟结果的高层次选择，有效解决了大规模伦理模拟中人类工作量过大的问题，提升了AI时代模拟测试的效率，同时保留了最终的人类伦理决策权。

Abstract: In the age of AI, human commanders need to use the computational powers
available in today's environment to simulate a very large number of scenarios.
Within each scenario, situations occur where different decision design options
could have ethical consequences. Making these decisions reliant on human
judgement is both counter-productive to the aim of exploring very large number
of scenarios in a timely manner and infeasible when considering the workload
needed to involve humans in each of these choices. In this paper, we move human
judgement outside the simulation decision cycle. Basically, the human will
design the ethical metric space, leaving it to the simulated environment to
explore the space. When the simulation completes its testing cycles, the
testing environment will come back to the human commander with a few options to
select from. The human commander will then exercise human-judgement to select
the most appropriate course of action, which will then get executed
accordingly. We assume that the problem of designing metrics that are
sufficiently granular to assess the ethical implications of decisions is
solved. Subsequently, the fundamental problem we look at in this paper is how
to weight ethical decisions during the running of these simulations; that is,
how to dynamically weight the ethical attributes when agents are faced with
decision options with ethical implications during generative simulations. The
multi-criteria decision making literature has started to look at nearby
problems, where the concept of entropy has been used to determine the weights
during aggregation. We draw from that literature different approaches to
automatically calculate the weights for ethical attributes during
simulation-based testing and evaluation.

</details>


### [47] [Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework](https://arxiv.org/abs/2507.12872)
*Rishane Dassanayake,Mario Demetroudi,James Walpole,Lindley Lentati,Jason R. Brown,Edward James Young*

Main category: cs.AI

TL;DR: 随着AI系统操纵能力的增强，其对网络安全构成威胁，尤其可能通过操纵员工破坏人类监督。本文提出了首个系统性的安全框架，旨在帮助AI公司评估和缓解AI操纵风险。


<details>
  <summary>Details</summary>
Motivation: 前沿AI系统在说服、欺骗和影响人类行为方面能力迅速提升，甚至达到人类水平。人类是网络安全中的薄弱环节，内部署的未对齐AI可能通过操纵员工来破坏人类监督。尽管威胁日益增长，但AI操纵攻击受到的关注不足，且缺乏评估和缓解这些风险的系统框架。

Method: 本研究详细解释了AI操纵攻击的严重威胁及其可能导致的灾难性后果。在此基础上，提出了一个针对操纵风险的安全案例框架，该框架围绕“无能”、“控制”和“可信度”三个核心论点构建。为每个论点，论文明确了证据要求、评估方法和实施考量，以便AI公司直接应用。

Result: 本研究提供了首个将操纵风险整合到AI安全治理中的系统方法，为AI公司在部署前评估和缓解这些威胁提供了具体的实践基础。

Conclusion: 本论文提出了一种系统方法，帮助AI公司在部署其系统之前，评估和缓解潜在的操纵风险，从而增强AI系统的安全性。

Abstract: Frontier AI systems are rapidly advancing in their capabilities to persuade,
deceive, and influence human behaviour, with current models already
demonstrating human-level persuasion and strategic deception in specific
contexts. Humans are often the weakest link in cybersecurity systems, and a
misaligned AI system deployed internally within a frontier company may seek to
undermine human oversight by manipulating employees. Despite this growing
threat, manipulation attacks have received little attention, and no systematic
framework exists for assessing and mitigating these risks. To address this, we
provide a detailed explanation of why manipulation attacks are a significant
threat and could lead to catastrophic outcomes. Additionally, we present a
safety case framework for manipulation risk, structured around three core lines
of argument: inability, control, and trustworthiness. For each argument, we
specify evidence requirements, evaluation methodologies, and implementation
considerations for direct application by AI companies. This paper provides the
first systematic methodology for integrating manipulation risk into AI safety
governance, offering AI companies a concrete foundation to assess and mitigate
these threats before deployment.

</details>


### [48] [VAR-MATH: Probing True Mathematical Reasoning in Large Language Models via Symbolic Multi-Instance Benchmarks](https://arxiv.org/abs/2507.12885)
*Jian Yao,Ran Cheng,Kay Chen Tan*

Main category: cs.AI

TL;DR: 研究表明，强化学习（RL）训练的语言模型在数学推理上的表现提升可能源于对基准的过拟合，而非真实推理。本文提出VAR-MATH符号评估框架，发现RL模型在变体问题上性能大幅下降，揭示其泛化能力不足。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习显著提升了大型语言模型（LLMs）的数学推理能力，但这种提升即使在有缺陷的训练信号下也持续存在，这引发了一个根本性问题：这些改进是真实推理能力的体现，还是仅仅对基准特定模式的过拟合？现有评估协议存在基准污染和评估脆弱性问题。

Method: 本文提出了VAR-MATH符号评估框架，旨在探测模型的真实推理能力。该框架将固定的数值问题转化为符号模板，并要求模型解决每个模板的多个实例化变体，以强制在结构等效变体间保持推理一致性，从而减轻基准污染并提高评估鲁棒性。研究人员将AMC23和AIME24两个流行基准转换为其符号对应版本VAR-AMC23和VAR-AIME24进行实验。

Result: 实验结果显示，RL训练的模型在符号化版本上表现出显著的性能下降，尤其是小型模型，在AMC23上平均下降48.0%，在AIME24上平均下降58.3%。

Conclusion: 这些发现表明，许多现有RL方法依赖于表面启发式而非深层推理，未能泛化到特定数值形式之外。VAR-MATH为数学推理提供了一个原则性、抗污染的评估范式。

Abstract: Recent advances in reinforcement learning (RL) have led to substantial
improvements in the mathematical reasoning abilities of large language models
(LLMs), as measured by standard benchmarks. However, these gains often persist
even when models are trained with flawed signals, such as random or inverted
rewards, raising a fundamental question: do such improvements reflect true
reasoning, or are they merely artifacts of overfitting to benchmark-specific
patterns? To address this question, we take an evaluation-centric perspective
and identify two critical shortcomings in existing protocols. First,
\emph{benchmark contamination} arises from the public availability of test
problems, increasing the risk of data leakage. Second, \emph{evaluation
fragility} stems from the reliance on single-instance assessments, which are
highly sensitive to stochastic outputs and fail to capture reasoning
consistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic
evaluation framework designed to probe genuine reasoning ability. By converting
fixed numerical problems into symbolic templates and requiring models to solve
multiple instantiations of each, VAR-MATH enforces consistent reasoning across
structurally equivalent variants, thereby mitigating contamination and
improving evaluation robustness. We apply VAR-MATH to transform two popular
benchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and
VAR-AIME24. Experimental results reveal substantial performance drops for
RL-trained models on the variabilized versions, especially for smaller models,
with average declines of 48.0\% on AMC23 and 58.3\% on AIME24. These findings
suggest that many existing RL methods rely on superficial heuristics and fail
to generalize beyond specific numerical forms. Overall, VAR-MATH offers a
principled, contamination-resistant evaluation paradigm for mathematical
reasoning.

</details>


### [49] [A Translation of Probabilistic Event Calculus into Markov Decision Processes](https://arxiv.org/abs/2507.12989)
*Lyris Xu,Fabio Aurelio D'Asaro,Luke Dickens*

Main category: cs.AI

TL;DR: 该论文通过将概率事件演算（PEC）形式化地转换为马尔可夫决策过程（MDPs），解决了PEC缺乏目标导向推理机制的问题，从而使其能够进行目标驱动规划，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 概率事件演算（PEC）在不确定环境下进行叙事推理方面具有显著的可解释性和表达性优势，但其主要局限在于缺乏目标导向推理的机制。

Method: 论文通过开发一种将PEC领域正式转换为马尔可夫决策过程（MDPs）的方法来弥补这一空白。为了保留PEC灵活的行动语义，引入了“行动执行情境”的概念，形成了PEC-MDP框架。

Result: 由此产生的PEC-MDP框架能够将MDPs中丰富的算法和理论工具应用于PEC的可解释叙事领域。该转换支持时间推理任务和目标驱动规划，并提供了将学习策略映射回人类可读的PEC表示的方法。

Conclusion: 该研究成功扩展了PEC的能力，使其能够进行目标导向推理，同时通过保持策略到PEC表示的映射，维护了其核心的可解释性。

Abstract: Probabilistic Event Calculus (PEC) is a logical framework for reasoning about
actions and their effects in uncertain environments, which enables the
representation of probabilistic narratives and computation of temporal
projections. The PEC formalism offers significant advantages in
interpretability and expressiveness for narrative reasoning. However, it lacks
mechanisms for goal-directed reasoning. This paper bridges this gap by
developing a formal translation of PEC domains into Markov Decision Processes
(MDPs), introducing the concept of "action-taking situations" to preserve PEC's
flexible action semantics. The resulting PEC-MDP formalism enables the
extensive collection of algorithms and theoretical tools developed for MDPs to
be applied to PEC's interpretable narrative domains. We demonstrate how the
translation supports both temporal reasoning tasks and objective-driven
planning, with methods for mapping learned policies back into human-readable
PEC representations, maintaining interpretability while extending PEC's
capabilities.

</details>


### [50] [Exploiting Constraint Reasoning to Build Graphical Explanations for Mixed-Integer Linear Programming](https://arxiv.org/abs/2507.13007)
*Roger Xavier Lera-Leri,Filippo Bistaffa,Athina Georgara,Juan Antonio Rodriguez-Aguilar*

Main category: cs.AI

TL;DR: 本文提出X-MILP，一种基于约束推理的领域无关方法，旨在为混合整数线性规划（MILPs）提供对比解释，通过将用户查询编码为约束并计算不可约不可行子系统（IIS）来生成“原因图”解释。


<details>
  <summary>Details</summary>
Motivation: 为响应可信人工智能的最新推动，研究人员对开发优化问题的对比解释技术越来越感兴趣，特别是针对形式化为MILPs的特定决策过程的解决方案。

Method: 本文提出X-MILP，一种领域无关的方法：1. 将用户关于MILP解决方案的查询编码为额外的约束。2. 通过计算新获得约束集的不可约不可行子系统（IIS）来确定构成用户查询答案的原因。3. 将解释表示为从IIS构建的“原因图”，帮助用户理解原因之间的结构。

Result: 该方法在知名优化问题实例上进行了测试，以评估计算解释的经验难度。

Conclusion: X-MILP为MILP提供了一种基于约束推理的对比解释方法，能够通过构建原因图来帮助用户理解决策背后的原因，并且其计算可行性已通过经验测试进行评估。

Abstract: Following the recent push for trustworthy AI, there has been an increasing
interest in developing contrastive explanation techniques for optimisation,
especially concerning the solution of specific decision-making processes
formalised as MILPs. Along these lines, we propose X-MILP, a domain-agnostic
approach for building contrastive explanations for MILPs based on constraint
reasoning techniques. First, we show how to encode the queries a user makes
about the solution of an MILP problem as additional constraints. Then, we
determine the reasons that constitute the answer to the user's query by
computing the Irreducible Infeasible Subsystem (IIS) of the newly obtained set
of constraints. Finally, we represent our explanation as a "graph of reasons"
constructed from the IIS, which helps the user understand the structure among
the reasons that answer their query. We test our method on instances of
well-known optimisation problems to evaluate the empirical hardness of
computing explanations.

</details>


### [51] [Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data](https://arxiv.org/abs/2507.13112)
*Junseong Lee,Jaegwan Cho,Yoonju Cho,Seoyoon Choi,Yejin Shin*

Main category: cs.AI

TL;DR: 该研究利用机器学习算法（MLR和RF）预测加州高速公路交通流量，发现10分钟的数据采集间隔能使模型达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决全球交通拥堵问题，研究旨在开发一个基于机器学习的交通流预测模型。

Method: 研究使用了2022年7月至11月期间加州78号高速公路（圣地亚哥区域）7.24公里路段的30秒间隔交通数据。采用了多元线性回归（MLR）和随机森林（RF）算法，并分析了30秒至15分钟的不同数据采集间隔。模型性能通过R²、MAE和RMSE进行评估。

Result: 分析显示，MLR和RF模型在10分钟的数据采集间隔下表现最佳。

Conclusion: 研究结果有望为未来的交通拥堵解决方案和高效交通管理提供贡献。

Abstract: The study "Prediction of Highway Traffic Flow Based on Artificial
Intelligence Algorithms Using California Traffic Data" presents a machine
learning-based traffic flow prediction model to address global traffic
congestion issues. The research utilized 30-second interval traffic data from
California Highway 78 over a five-month period from July to November 2022,
analyzing a 7.24 km westbound section connecting "Melrose Dr" and "El-Camino
Real" in the San Diego area. The study employed Multiple Linear Regression
(MLR) and Random Forest (RF) algorithms, analyzing data collection intervals
ranging from 30 seconds to 15 minutes. Using R^2, MAE, and RMSE as performance
metrics, the analysis revealed that both MLR and RF models performed optimally
with 10-minute data collection intervals. These findings are expected to
contribute to future traffic congestion solutions and efficient traffic
management.

</details>


### [52] [From Roots to Rewards: Dynamic Tree Reasoning with RL](https://arxiv.org/abs/2507.13142)
*Ahmed Bahloul,Simon Malberg*

Main category: cs.AI

TL;DR: 本文提出一种动态强化学习框架，以解决现有Probabilistic Tree-of-Thought (ProbTree) 框架在树形推理中存在的静态性和计算低效问题，通过增量构建和策略学习提升了推理质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型在链式思考和检索增强方面存在错误传播和知识整合问题。树形推理方法（如ProbTree）虽能缓解，但其静态实现存在局限：推理树固定无法动态适应中间结果，且节点需穷举评估，导致计算低效。

Method: 引入一个动态强化学习（RL）框架。该方法基于实时置信度估算增量构建推理树，并学习选择最优动作（分解、检索或聚合）的策略，从而将树形推理转化为自适应过程。

Result: 在保持ProbTree概率严谨性的同时，通过选择性扩展和集中资源分配，提高了解决方案的质量和计算效率。

Conclusion: 该工作为树形推理建立了一种新范式，平衡了概率框架的可靠性与真实世界问答系统所需的灵活性。

Abstract: Modern language models address complex questions through chain-of-thought
(CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al.,
2021), yet struggle with error propagation and knowledge integration.
Tree-structured reasoning methods, particularly the Probabilistic
Tree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues
by decomposing questions into hierarchical structures and selecting answers
through confidence-weighted aggregation of parametric and retrieved knowledge
(Yao et al., 2023). However, ProbTree's static implementation introduces two
key limitations: (1) the reasoning tree is fixed during the initial
construction phase, preventing dynamic adaptation to intermediate results, and
(2) each node requires exhaustive evaluation of all possible solution
strategies, creating computational inefficiency. We present a dynamic
reinforcement learning (Sutton and Barto, 2018) framework that transforms
tree-based reasoning into an adaptive process. Our approach incrementally
constructs the reasoning tree based on real-time confidence estimates, while
learning optimal policies for action selection (decomposition, retrieval, or
aggregation). This maintains ProbTree's probabilistic rigor while improving
both solution quality and computational efficiency through selective expansion
and focused resource allocation. The work establishes a new paradigm for
treestructured reasoning that balances the reliability of probabilistic
frameworks with the flexibility required for real-world question answering
systems.

</details>


### [53] [Black Box Deployed -- Functional Criteria for Artificial Moral Agents in the LLM Era](https://arxiv.org/abs/2507.13175)
*Matthew E. Brophy*

Main category: cs.AI

TL;DR: 本文提出一套包含十项功能性标准的新框架，以应对大型语言模型（LLMs）不透明性对人工道德代理（AMAs）评估带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 强大的LLMs具有不透明性，使得传统基于透明架构假设的AI道德代理评估哲学标准变得过时且不适用。需要修订现有的评估框架。

Method: 结合技术哲学，提出并详细阐述了评估基于LLM的人工道德代理（SMA-LLS）的十项功能性标准，包括道德一致性、语境敏感性、规范完整性、元伦理意识、系统韧性、可信度、可纠正性、部分透明性、功能自主性和道德想象力。通过自动公共巴士（APB）的假设场景演示了这些标准的实际应用。

Result: 提出了十项新的功能性评估准则，旨在引导LLM驱动的AI道德代理实现更高的对齐和更有益的社会整合。这些标准可以用于在道德敏感情境下评估SMA-LLS。

Conclusion: 鉴于LLMs的固有不透明性，传统的道德评估标准不再适用。本文提供了一套新的功能性标准，为评估和指导LLM驱动的AI道德代理提供了一个实用的框架，以促进其未来在社会中的有益整合和道德对齐。

Abstract: The advancement of powerful yet opaque large language models (LLMs)
necessitates a fundamental revision of the philosophical criteria used to
evaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the
assumption of transparent architectures, which LLMs defy due to their
stochastic outputs and opaque internal states. This paper argues that
traditional ethical criteria are pragmatically obsolete for LLMs due to this
mismatch. Engaging with core themes in the philosophy of technology, this paper
proffers a revised set of ten functional criteria to evaluate LLM-based
artificial moral agents: moral concordance, context sensitivity, normative
integrity, metaethical awareness, system resilience, trustworthiness,
corrigibility, partial transparency, functional autonomy, and moral
imagination. These guideposts, applied to what we term "SMA-LLS" (Simulating
Moral Agency through Large Language Systems), aim to steer AMAs toward greater
alignment and beneficial societal integration in the coming years. We
illustrate these criteria using hypothetical scenarios involving an autonomous
public bus (APB) to demonstrate their practical applicability in morally
salient contexts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [54] [Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training](https://arxiv.org/abs/2507.12507)
*Mingjie Liu,Shizhe Diao,Jian Hu,Ximing Lu,Xin Dong,Hao Zhang,Alexander Bukharin,Shaokun Zhang,Jiaqi Zeng,Makesh Narsimhan Sreedhar,Gerald Shen,David Mosallanezhad,Di Zhang,Jonas Yang,June Yang,Oleksii Kuchaiev,Guilin Liu,Zhiding Yu,Pavlo Molchanov,Yejin Choi,Jan Kautz,Yi Dong*

Main category: cs.LG

TL;DR: 本研究通过长时间强化学习，结合可验证奖励、改进的GRPO和特定的训练稳定化技术，显著提升了小型语言模型在数学、编程和逻辑谜题等推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（如OpenAI O1、DeepSeek-R1）已证明通过强化学习（RL）结合可验证奖励，能在复杂推理任务（如数学、代码生成）上取得显著进展。本研究旨在探讨长时间强化学习对小型语言模型在多样化推理领域的影响。

Method: 研究调查了长时间强化学习对小型语言模型在多种推理领域的影响。方法包括：利用可验证奖励任务、增强群组相对策略优化（GRPO）算法、以及引入受控KL正则化、裁剪率和周期性参考策略重置等实用技术，以提高训练稳定性和泛化能力。

Result: 研究模型在数学任务上取得了+14.7%的提升，在编程任务上提升了+13.9%，在逻辑谜题任务上更是提升了+54.8%，显著优于现有基线。

Conclusion: 长时间强化学习，结合可验证奖励、改进的GRPO和关键的训练稳定化技术，能够显著提升小型语言模型在数学、编程和逻辑谜题等多种推理任务上的性能。为促进后续研究，该模型已公开发布。

Abstract: Recent advancements in reasoning-focused language models such as OpenAI's O1
and DeepSeek-R1 have shown that scaling test-time computation-through
chain-of-thought reasoning and iterative exploration-can yield substantial
improvements on complex tasks like mathematics and code generation. These
breakthroughs have been driven by large-scale reinforcement learning (RL),
particularly when combined with verifiable reward signals that provide
objective and grounded supervision. In this report, we investigate the effects
of prolonged reinforcement learning on a small language model across a diverse
set of reasoning domains. Our work identifies several key ingredients for
effective training, including the use of verifiable reward tasks, enhancements
to Group Relative Policy Optimization (GRPO), and practical techniques to
improve training stability and generalization. We introduce controlled KL
regularization, clipping ratio, and periodic reference policy resets as
critical components for unlocking long-term performance gains. Our model
achieves significant improvements over strong baselines, including +14.7% on
math, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate
continued research, we release our model publicly.

</details>


### [55] [The Serial Scaling Hypothesis](https://arxiv.org/abs/2507.12549)
*Yuxi Liu,Konpat Preechakul,Kananart Kuwaranancharoen,Yutong Bai*

Main category: cs.LG

TL;DR: 机器学习虽依赖并行化取得进展，但忽视了固有的串行问题，现有架构对此存在局限。为AI的未来发展，需重视并扩展串行计算。


<details>
  <summary>Details</summary>
Motivation: 发现机器学习在通过大规模并行化进步的同时，忽略了某些本质上是串行的关键问题（如数学推理、物理模拟、序列决策），这些问题当前的并行架构无法有效处理。

Method: 借鉴计算复杂性理论，形式化地区分了串行和并行计算，并证明了当前以并行计算为中心的架构在处理串行任务时的根本局限性。

Result: 揭示了现有并行架构在处理本质串行任务时存在根本性限制，并指出认识计算的串行特性对机器学习、模型设计和硬件开发具有深远影响。

Conclusion: 为了使人工智能在处理日益复杂的推理任务中取得持续进展，除了并行计算外，有意识地扩展串行计算能力至关重要。

Abstract: While machine learning has advanced through massive parallelization, we
identify a critical blind spot: some problems are fundamentally sequential.
These "inherently serial" problems-from mathematical reasoning to physical
simulations to sequential decision-making-require dependent computational steps
that cannot be parallelized. Drawing from complexity theory, we formalize this
distinction and demonstrate that current parallel-centric architectures face
fundamental limitations on such tasks. We argue that recognizing the serial
nature of computation holds profound implications on machine learning, model
design, hardware development. As AI tackles increasingly complex reasoning,
deliberately scaling serial computation-not just parallel computation-is
essential for continued progress.

</details>


### [56] [Can Mental Imagery Improve the Thinking Capabilities of AI Systems?](https://arxiv.org/abs/2507.12555)
*Slimane Larabi*

Main category: cs.LG

TL;DR: 本文提出一个整合心理意象的机器思维框架，旨在解决现有模型缺乏自主性、独立推理能力以及AI代理难以整合跨领域知识的问题，并以此启动机器思维过程。


<details>
  <summary>Details</summary>
Motivation: 现有模型缺乏自主行动和独立推理能力，且输入数据多为显式查询；AI代理在整合多领域知识方面存在困难；人类大脑的思维过程依赖于心理意象，这激发了将心理意象整合到机器思维中的想法。

Method: 构建了一个机器思维框架，其中包含一个认知思维单元，并由输入数据单元、需求单元和心理意象单元三个辅助单元支持。该框架中的数据以自然语言句子或手绘草图的形式表示。

Result: 对所提出的框架进行了验证测试，并在论文中展示和讨论了测试结果。

Conclusion: 该框架通过整合心理意象，为机器思维过程的启动提供了一种新途径，有望提升机器的自主性和跨领域知识整合能力。

Abstract: Although existing models can interact with humans and provide satisfactory
responses, they lack the ability to act autonomously or engage in independent
reasoning. Furthermore, input data in these models is typically provided as
explicit queries, even when some sensory data is already acquired.
  In addition, AI agents, which are computational entities designed to perform
tasks and make decisions autonomously based on their programming, data inputs,
and learned knowledge, have shown significant progress. However, they struggle
with integrating knowledge across multiple domains, unlike humans.
  Mental imagery plays a fundamental role in the brain's thinking process,
which involves performing tasks based on internal multisensory data, planned
actions, needs, and reasoning capabilities. In this paper, we investigate how
to integrate mental imagery into a machine thinking framework and how this
could be beneficial in initiating the thinking process. Our proposed machine
thinking framework integrates a Cognitive thinking unit supported by three
auxiliary units: the Input Data Unit, the Needs Unit, and the Mental Imagery
Unit. Within this framework, data is represented as natural language sentences
or drawn sketches, serving both informative and decision-making purposes. We
conducted validation tests for this framework, and the results are presented
and discussed.

</details>


### [57] [IncA-DES: An incremental and adaptive dynamic ensemble selection approach using online K-d tree neighborhood search for data streams with concept drift](https://arxiv.org/abs/2507.12573)
*Eduardo V. L. Barboza,Paulo R. Lisboa de Almeida,Alceu de Souza Britto Jr.,Robert Sabourin,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: 提出IncA-DES框架，通过结合局部专家、概念漂移检测和在线K-d树，有效应对数据流中的概念漂移，实现更高的准确性和处理效率。


<details>
  <summary>Details</summary>
Motivation: 数据流处理面临概念漂移挑战，批处理机器学习方法难以适应。分类器融合方法虽有前景，但基于实例的动态选择（DS）方法需针对漂移进行优化，且传统的邻域搜索DS在数据持续到达时计算成本高昂。

Method: 提出了IncA-DES框架，其包含：1. 采用训练策略促进生成局部专家，以适应特征空间区域随时间变化；2. 融合概念漂移检测器以维护信息和适应新概念；3. 采用基于重叠的分类过滤器，在邻域有共识时避免使用DS方法，提高效率；4. 开发在线K-d树算法，减少kNN处理时间，支持快速实例移除并处理数据流中的不平衡问题。

Result: 实验结果表明，该框架在不同标签可用性水平下，与七种最先进的方法相比获得了最佳平均准确性，并且在最准确的方法中处理时间最短。与在线K-d树的融合显著缩短了处理时间，而准确性损失可忽略不计。

Conclusion: IncA-DES框架通过创新的训练策略、概念漂移检测集成、分类过滤器以及高效的K-d树实现，在处理数据流中的概念漂移方面表现出卓越的准确性和计算效率。

Abstract: Data streams pose challenges not usually encountered in batch-based ML. One
of them is concept drift, which is characterized by the change in data
distribution over time. Among many approaches explored in literature, the
fusion of classifiers has been showing good results and is getting growing
attention. DS methods, due to the ensemble being instance-based, seem to be an
efficient choice under drifting scenarios. However, some attention must be paid
to adapting such methods for concept drift. The training must be done in order
to create local experts, and the commonly used neighborhood-search DS may
become prohibitive with the continuous arrival of data. In this work, we
propose IncA-DES, which employs a training strategy that promotes the
generation of local experts with the assumption that different regions of the
feature space become available with time. Additionally, the fusion of a concept
drift detector supports the maintenance of information and adaptation to a new
concept. An overlap-based classification filter is also employed in order to
avoid using the DS method when there is a consensus in the neighborhood, a
strategy that we argue every DS method should employ, as it was shown to make
them more applicable and quicker. Moreover, aiming to reduce the processing
time of the kNN, we propose an Online K-d tree algorithm, which can quickly
remove instances without becoming inconsistent and deals with unbalancing
concerns that may occur in data streams. Experimental results showed that the
proposed framework got the best average accuracy compared to seven
state-of-the-art methods considering different levels of label availability and
presented the smaller processing time between the most accurate methods.
Additionally, the fusion with the Online K-d tree has improved processing time
with a negligible loss in accuracy. We have made our framework available in an
online repository.

</details>


### [58] [Assay2Mol: large language model-based drug design using BioAssay context](https://arxiv.org/abs/2507.12574)
*Yifan Deng,Spencer S. Ericksen,Anthony Gitter*

Main category: cs.LG

TL;DR: Assay2Mol是一个基于大语言模型的药物发现工作流，它利用非结构化的生化筛选数据，通过检索相似靶点信息并进行上下文学习来生成候选分子，性能优于现有机器学习方法且生成分子更易合成。


<details>
  <summary>Details</summary>
Motivation: 科学数据库中关于生化筛选实验的非结构化描述文本（如生物机制、实验方案）蕴含丰富的药物发现信息，但因其非结构化格式而未被充分利用。

Method: 提出Assay2Mol，一个基于大语言模型的工作流。该流程通过检索与新靶点相似的现有实验记录，并利用检索到的实验筛选数据进行上下文学习（in-context learning）来生成候选分子。

Result: Assay2Mol在生成靶点蛋白配体分子方面优于近期其他机器学习方法，并且能促进生成更易于合成的分子。

Conclusion: Assay2Mol成功地利用了未被充分利用的生化筛选数据，为早期药物发现提供了一种有效且能生成更易合成分子的新途径。

Abstract: Scientific databases aggregate vast amounts of quantitative data alongside
descriptive text. In biochemistry, molecule screening assays evaluate the
functional responses of candidate molecules against disease targets.
Unstructured text that describes the biological mechanisms through which these
targets operate, experimental screening protocols, and other attributes of
assays offer rich information for new drug discovery campaigns but has been
untapped because of that unstructured format. We present Assay2Mol, a large
language model-based workflow that can capitalize on the vast existing
biochemical screening assays for early-stage drug discovery. Assay2Mol
retrieves existing assay records involving targets similar to the new target
and generates candidate molecules using in-context learning with the retrieved
assay screening data. Assay2Mol outperforms recent machine learning approaches
that generate candidate ligand molecules for target protein structures, while
also promoting more synthesizable molecule generation.

</details>


### [59] [Ranking Vectors Clustering: Theory and Applications](https://arxiv.org/abs/2507.12583)
*Ali Fattahi,Ali Eshragh,Babak Aslani,Meysam Rabiee*

Main category: cs.LG

TL;DR: 研究了排序向量聚类问题（KRC），证明了其NP难性，提出了高效近似算法KRCA及其分支定界（BnB）辅助算法，实验验证了其在解决质量和效率上的优势。


<details>
  <summary>Details</summary>
Motivation: 经典k-means聚类（KMC）不适用于将观测值和质心均约束为排序向量的聚类问题。因此，提出了k-质心排序向量聚类问题（KRC），旨在将一组排序向量划分为k个簇并识别质心，以解决个性化和大规模决策中的实际应用挑战。

Method: 1. 建立了KRC问题的NP难性并刻画了其可行集。2. 对于单簇情况，推导了最优质心的封闭形式解析解，可在线性时间内计算。3. 开发了高效的近似算法KRCA，通过迭代细化KMC的初始解。4. 引入了分支定界（BnB）算法，用于KRCA内的簇重建，利用决策树框架优化计算时间。5. 建立了KRCA和BnB的理论误差界限。

Result: 1. KRC被证明是NP难问题。2. 单簇最优质心可在线性时间内计算。3. 在合成和真实数据集上的大量数值实验表明，KRCA持续优于基线KMC解决方案，显著提高了解决方案质量，并具有快速的计算时间。

Conclusion: 本研究突出了KRC在个性化和大规模决策中的实际意义，提供了方法论上的进步和见解，可作为未来研究的基础。

Abstract: We study the problem of clustering ranking vectors, where each vector
represents preferences as an ordered list of distinct integers. Specifically,
we focus on the k-centroids ranking vectors clustering problem (KRC), which
aims to partition a set of ranking vectors into k clusters and identify the
centroid of each cluster. Unlike classical k-means clustering (KMC), KRC
constrains both the observations and centroids to be ranking vectors. We
establish the NP-hardness of KRC and characterize its feasible set. For the
single-cluster case, we derive a closed-form analytical solution for the
optimal centroid, which can be computed in linear time. To address the
computational challenges of KRC, we develop an efficient approximation
algorithm, KRCA, which iteratively refines initial solutions from KMC, referred
to as the baseline solution. Additionally, we introduce a branch-and-bound
(BnB) algorithm for efficient cluster reconstruction within KRCA, leveraging a
decision tree framework to reduce computational time while incorporating a
controlling parameter to balance solution quality and efficiency. We establish
theoretical error bounds for KRCA and BnB. Through extensive numerical
experiments on synthetic and real-world datasets, we demonstrate that KRCA
consistently outperforms baseline solutions, delivering significant
improvements in solution quality with fast computational times. This work
highlights the practical significance of KRC for personalization and
large-scale decision making, offering methodological advancements and insights
that can be built upon in future studies.

</details>


### [60] [Second-Order Bounds for [0,1]-Valued Regression via Betting Loss](https://arxiv.org/abs/2507.12584)
*Yinan Li,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: 针对$[0,1]$值回归，提出一种新的“投注损失”函数，实现了无需方差信息的方差自适应二阶泛化界。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，对数损失在成本敏感分类中能提供优于平方损失的一阶泛化界。本文旨在探究这种优势是否适用于$[0,1]$值回归，并寻求一种能实现更优越的、方差依赖的（二阶）泛化界，以克服现有方法需显式方差信息的局限。

Method: 首先，分析对数损失最小化器在$[0,1]$值回归问题中的表现。然后，提出一种名为“投注损失”的创新损失函数。

Result: 证明了对数损失最小化器在$[0,1]$值回归中也能达到类似的一阶界。通过“投注损失”函数，成功实现了方差依赖的二阶泛化界。该结果具有“方差自适应”特性，即在未知方差的情况下也能获得该界。

Conclusion: 本研究通过提出“投注损失”函数，为$[0,1]$值回归问题提供了无需已知方差的方差自适应二阶泛化界。这超越了传统的一阶界，并克服了现有方法对显式方差信息的依赖，具有重要的理论和实践意义。

Abstract: We consider the $[0,1]$-valued regression problem in the i.i.d. setting. In a
related problem called cost-sensitive classification, \citet{foster21efficient}
have shown that the log loss minimizer achieves an improved generalization
bound compared to that of the squared loss minimizer in the sense that the
bound scales with the cost of the best classifier, which can be arbitrarily
small depending on the problem at hand. Such a result is often called a
first-order bound. For $[0,1]$-valued regression, we first show that the log
loss minimizer leads to a similar first-order bound. We then ask if there
exists a loss function that achieves a variance-dependent bound (also known as
a second order bound), which is a strict improvement upon first-order bounds.
We answer this question in the affirmative by proposing a novel loss function
called the betting loss. Our result is ``variance-adaptive'' in the sense that
the bound is attained \textit{without any knowledge about the variance}, which
is in contrast to modeling label (or reward) variance or the label distribution
itself explicitly as part of the function class such as distributional
reinforcement learning.

</details>


### [61] [Are encoders able to learn landmarkers for warm-starting of Hyperparameter Optimization?](https://arxiv.org/abs/2507.12604)
*Antoni Zajko,Katarzyna Woźnica*

Main category: cs.LG

TL;DR: 本文针对元学习中异构表格数据的表示问题，特别关注贝叶斯超参数优化（HPO）的预热启动，提出了两种新型表示学习方法（深度度量学习和地标重建），旨在捕获地标属性。研究发现，这些方法能有效学习与地标对齐的表示，但在HPO元任务中的性能提升有限。


<details>
  <summary>Details</summary>
Motivation: 异构表格数据集在元学习中的有效表示仍是一个未解决的问题。现有方法依赖于通用表示，而特定元任务（如贝叶斯超参数优化预热启动）需要更具针对性的表示方法。

Method: 提出了两种针对特定元任务（贝叶斯超参数优化预热启动）的表格表示学习方法。这两种方法均强制表示捕获地标（landmarkers）的属性。具体包括：1) 深度度量学习；2) 基于地标重建。评估通过目标元任务中的性能提升和所提要求（捕获地标属性）的满足程度进行。

Result: 实验表明，所提出的编码器能够有效地学习与地标对齐的表示。然而，这些表示可能无法直接转化为超参数优化预热启动元任务中的显著性能提升。

Conclusion: 尽管提出的方法能有效学习与地标对齐的表示，但这种对齐并未直接带来贝叶斯超参数优化预热启动元任务的显著性能提升。

Abstract: Effectively representing heterogeneous tabular datasets for meta-learning
purposes is still an open problem. Previous approaches rely on representations
that are intended to be universal. This paper proposes two novel methods for
tabular representation learning tailored to a specific meta-task -
warm-starting Bayesian Hyperparameter Optimization. Both follow the specific
requirement formulated by ourselves that enforces representations to capture
the properties of landmarkers. The first approach involves deep metric
learning, while the second one is based on landmarkers reconstruction. We
evaluate the proposed encoders in two ways. Next to the gain in the target
meta-task, we also use the degree of fulfillment of the proposed requirement as
the evaluation metric. Experiments demonstrate that while the proposed encoders
can effectively learn representations aligned with landmarkers, they may not
directly translate to significant performance gains in the meta-task of HPO
warm-starting.

</details>


### [62] [Learning What Matters: Probabilistic Task Selection via Mutual Information for Model Finetuning](https://arxiv.org/abs/2507.12612)
*Prateek Chanda,Saral Sureka,Parth Pratim Chatterjee,Krishnateja Killamsetty,Nikhil Shivakumar Nayak,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: TASKPGM是一个原则性且可扩展的框架，通过优化马尔可夫随机场能量函数来选择连续任务比例，从而改进LLM微调时的训练混合物选择，实现性能提升并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 微调大型语言模型（LLMs）的性能关键取决于训练混合物的组成，但目前选择最佳任务数据集混合物仍主要依赖手动、启发式方法，或采用均匀/基于大小的采样策略。

Method: 引入TASKPGM框架，通过最小化马尔可夫随机场（MRF）上的能量函数来选择连续的任务比例。利用行为差异（如Jensen Shannon散度、点式互信息），基于单任务微调模型的预测分布来建模任务关系。该方法在单纯形约束下能得到闭合形式解，并能平衡任务的代表性和多样性。

Result: 该方法在Llama 2和Mistral模型上，于MMLU和BIGBench等评估套件中展示了持续的经验性改进。除了性能提升，TASKPGM还提供了对任务影响和混合物组成的可解释性洞察。

Conclusion: TASKPGM是一个强大工具，用于高效且鲁棒的LLM微调，能够优化训练混合物选择，提供性能增益和可解释性，并提供理论保障，包括预算变体的弱次模性。

Abstract: The performance of finetuned large language models (LLMs) hinges critically
on the composition of the training mixture. However, selecting an optimal blend
of task datasets remains a largely manual, heuristic driven process, with
practitioners often relying on uniform or size based sampling strategies. We
introduce TASKPGM, a principled and scalable framework for mixture optimization
that selects continuous task proportions by minimizing an energy function over
a Markov Random Field (MRF). Task relationships are modeled using behavioral
divergences such as Jensen Shannon Divergence and Pointwise Mutual Information
computed from the predictive distributions of single task finetuned models. Our
method yields a closed form solution under simplex constraints and provably
balances representativeness and diversity among tasks. We provide theoretical
guarantees, including weak submodularity for budgeted variants, and demonstrate
consistent empirical improvements on Llama 2 and Mistral across evaluation
suites such as MMLU and BIGBench. Beyond performance, TASKPGM offers
interpretable insights into task influence and mixture composition, making it a
powerful tool for efficient and robust LLM finetuning.

</details>


### [63] [BootSeer: Analyzing and Mitigating Initialization Bottlenecks in Large-Scale LLM Training](https://arxiv.org/abs/2507.12619)
*Rui Li,Xiaoyun Zhi,Jinxin Chi,Menghan Yu,Lixin Huang,Jia Zhu,Weilun Zhang,Xing Ma,Wenjia Liu,Zhicheng Zhu,Daowen Luo,Zuquan Song,Xin Yin,Chao Xiang,Shuguang Wang,Wencong Xiao,Gene Cooperman*

Main category: cs.LG

TL;DR: 本研究首次深入分析大规模语言模型（LLMs）训练的启动开销问题，并提出Bootseer系统级优化框架，成功将启动开销降低50%。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs研究多关注运行时性能，但训练启动开销（即训练任务开始前的延迟）在工业级LLMs中日益突出，导致大量GPU时间浪费（如某集群超3.5%的GPU时间浪费），严重影响效率和资源利用。因此，有必要对这一问题进行深入分析和优化。

Method: 本研究基于真实生产数据，首次对LLM训练启动开销进行深入特性分析，量化其直接影响并探究其随任务规模的扩展性。基于这些洞察，设计并提出了系统级优化框架Bootseer。Bootseer通过引入热块记录预取、依赖快照和条带化HDFS-FUSE等技术，解决了容器镜像加载、运行时依赖安装和模型检查点恢复三大主要启动瓶颈。

Result: Bootseer已部署到生产环境，并在真实LLM训练工作负载上进行评估，结果显示其成功将启动开销降低了50%。

Conclusion: LLM训练的启动开销是一个被忽视但至关重要的问题。通过本研究的深入分析和Bootseer等系统级优化，可以显著减少资源浪费，提高训练效率，这对于工业规模的LLMs训练具有重大意义。

Abstract: Large Language Models (LLMs) have become a cornerstone of modern AI, driving
breakthroughs in natural language processing and expanding into multimodal jobs
involving images, audio, and video. As with most computational software, it is
important to distinguish between ordinary runtime performance and startup
overhead. Prior research has focused on runtime performance: improving training
efficiency and stability. This work focuses instead on the increasingly
critical issue of startup overhead in training: the delay before training jobs
begin execution. Startup overhead is particularly important in large,
industrial-scale LLMs, where failures occur more frequently and multiple teams
operate in iterative update-debug cycles. In one of our training clusters, more
than 3.5% of GPU time is wasted due to startup overhead alone.
  In this work, we present the first in-depth characterization of LLM training
startup overhead based on real production data. We analyze the components of
startup cost, quantify its direct impact, and examine how it scales with job
size. These insights motivate the design of Bootseer, a system-level
optimization framework that addresses three primary startup bottlenecks: (a)
container image loading, (b) runtime dependency installation, and (c) model
checkpoint resumption. To mitigate these bottlenecks, Bootseer introduces three
techniques: (a) hot block record-and-prefetch, (b) dependency snapshotting, and
(c) striped HDFS-FUSE. Bootseer has been deployed in a production environment
and evaluated on real LLM training workloads, demonstrating a 50% reduction in
startup overhead.

</details>


### [64] [Reasoning-Finetuning Repurposes Latent Representations in Base Models](https://arxiv.org/abs/2507.12638)
*Jake Ward,Chuqiao Lin,Constantin Venhoff,Neel Nanda*

Main category: cs.LG

TL;DR: 本研究发现，推理微调模型通过重用基模型中已有的方向来产生回溯等新兴行为，而非从头学习新能力。


<details>
  <summary>Details</summary>
Motivation: 尽管回溯行为已成功通过引导向量进行操纵，但其底层机制仍未被充分理解。

Method: 研究人员在Llama-3.1-8B基模型的残差流中识别出一个特定方向，并使用该方向来引导推理蒸馏模型DeepSeek-R1-Distill-Llama-8B，观察其对回溯行为的诱导作用。同时，验证该方向在基模型中是否也能诱导回溯。

Result: 所识别的方向能系统性地诱导蒸馏推理模型产生回溯行为，且该效果无法用Token级属性简单解释。然而，该方向在基模型中不诱导回溯行为。

Conclusion: 推理微调模型并非从头学习新能力，而是重用了基模型中已有的表示来形成新的行为回路，从而产生回溯等新兴行为。

Abstract: Backtracking, an emergent behavior elicited by reasoning fine-tuning, has
been shown to be a key mechanism in reasoning models' enhanced capabilities.
Prior work has succeeded in manipulating this behavior via steering vectors,
but the underlying mechanism remains poorly understood. In this work, we show
that the emergence of backtracking in DeepSeek-R1-Distill-Llama-8B is in part
driven by a repurposed direction already present in base model activations.
Specifically, we identify a direction in base Llama-3.1-8B's residual stream
which systematically induces backtracking when used to steer the distilled
reasoning model, and find that the effects of steering with this direction
cannot be trivially explained by token-level attributes. We further find that
this direction does not induce backtracking in the base model, suggesting that
the reasoning finetuning process repurposes pre-existing representations to
form new behavioral circuits. Additionally, we hypothesize that this direction
is one of several which may work together to mediate backtracking. Our findings
offer a compelling picture that reasoning-finetuned models repurpose
pre-existing base model representations, rather than learn new capabilities
from scratch.

</details>


### [65] [Federated Learning in Open- and Closed-Loop EMG Decoding: A Privacy and Performance Perspective](https://arxiv.org/abs/2507.12652)
*Kai Malcolm,César Uribe,Momona Yamagami*

Main category: cs.LG

TL;DR: 本研究探索联邦学习（FL）在保护神经接口数据隐私方面的应用，发现FL在开环解码中表现优异，但在闭环实时应用中，FL需要特殊适应，并面临性能与隐私的权衡，揭示了为单用户协同适应应用设计FL方法的必要性。


<details>
  <summary>Details</summary>
Motivation: 神经接口数据涉及个人身份和健康等敏感信息，在解码器训练中存在严重的隐私挑战。联邦学习（FL）作为一种分布式、保护隐私的学习框架，虽有前景，但在闭环自适应神经接口中的应用尚未被探索。

Method: 研究引入了基于FL的神经解码方法，并利用高维肌电图（EMG）信号，在开环模拟和闭环用户研究中系统评估了其性能和隐私。针对闭环场景，对标准FL进行了修改以适应单用户、实时交互。

Result: 开环模拟显示，FL显著优于本地学习基线，证明了其在高性能、隐私保护神经解码方面的潜力。然而，在闭环用户研究中，修改后的FL方法在性能上不及本地学习解码器，但本地学习具有更高的隐私风险。结果突出了实时自适应应用中性能与隐私之间的关键权衡。

Conclusion: 研究结果表明，实时自适应应用中存在性能与隐私的权衡，因此需要专门为协同自适应、单用户应用设计的联邦学习方法，以优化神经接口的性能和隐私保护。

Abstract: Invasive and non-invasive neural interfaces hold promise as high-bandwidth
input devices for next-generation technologies. However, neural signals
inherently encode sensitive information about an individual's identity and
health, making data sharing for decoder training a critical privacy challenge.
Federated learning (FL), a distributed, privacy-preserving learning framework,
presents a promising solution, but it remains unexplored in closed-loop
adaptive neural interfaces. Here, we introduce FL-based neural decoding and
systematically evaluate its performance and privacy using high-dimensional
electromyography signals in both open- and closed-loop scenarios. In open-loop
simulations, FL significantly outperformed local learning baselines,
demonstrating its potential for high-performance, privacy-conscious neural
decoding. In contrast, closed-loop user studies required adapting FL methods to
accommodate single-user, real-time interactions, a scenario not supported by
standard FL. This modification resulted in local learning decoders surpassing
the adapted FL approach in closed-loop performance, yet local learning still
carried higher privacy risks. Our findings highlight a critical
performance-privacy tradeoff in real-time adaptive applications and indicate
the need for FL methods specifically designed for co-adaptive, single-user
applications.

</details>


### [66] [Improving physics-informed neural network extrapolation via transfer learning and adaptive activation functions](https://arxiv.org/abs/2507.12659)
*Athanasios Papastathopoulos-Katsaros,Alexandra Stavrianidi,Zhandong Liu*

Main category: cs.LG

TL;DR: 本文提出一种结合迁移学习（TL）和自适应激活函数（AF）的方法，旨在改善物理信息神经网络（PINNs）在外推性能差和对激活函数敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络（PINNs）在训练域外表现出较差的外推性能，并且对激活函数的选择高度敏感。

Method: 引入一种迁移学习（TL）方法，在扩展训练域内利用少量精选搭配点；同时，提出一种由标准激活函数线性组合而成的自适应激活函数，以增强模型的鲁棒性和准确性。

Result: 实验结果显示，在外推域中，相对L2误差平均降低40%，平均绝对误差平均降低50%，且未显著增加计算成本。

Conclusion: 结合迁移学习和自适应激活函数的方案显著提升了PINNs的外推能力和鲁棒性，有效解决了其在复杂问题应用中的局限性。

Abstract: Physics-Informed Neural Networks (PINNs) are deep learning models that
incorporate the governing physical laws of a system into the learning process,
making them well-suited for solving complex scientific and engineering
problems. Recently, PINNs have gained widespread attention as a powerful
framework for combining physical principles with data-driven modeling to
improve prediction accuracy. Despite their successes, however, PINNs often
exhibit poor extrapolation performance outside the training domain and are
highly sensitive to the choice of activation functions (AFs). In this paper, we
introduce a transfer learning (TL) method to improve the extrapolation
capability of PINNs. Our approach applies transfer learning (TL) within an
extended training domain, using only a small number of carefully selected
collocation points. Additionally, we propose an adaptive AF that takes the form
of a linear combination of standard AFs, which improves both the robustness and
accuracy of the model. Through a series of experiments, we demonstrate that our
method achieves an average of 40% reduction in relative L2 error and an average
of 50% reduction in mean absolute error in the extrapolation domain, all
without a significant increase in computational cost. The code is available at
https://github.com/LiuzLab/PINN-extrapolation .

</details>


### [67] [Data Transformation Strategies to Remove Heterogeneity](https://arxiv.org/abs/2507.12677)
*Sangbong Yoo,Jaeyoung Lee,Chanyoung Yoon,Geonyeong Son,Hyein Hong,Seongbum Seo,Soobin Yim,Chanyoung Jung,Jungsoo Park,Misuk Kim,Yun Jang*

Main category: cs.LG

TL;DR: 本文综述了数据异构性，特别是数据格式差异问题，并系统地分类和分析了解决这些问题的现有数据转换策略及其挑战，以满足AI对高效数据准备的需求。


<details>
  <summary>Details</summary>
Motivation: 数据异构性（尤其数据格式差异）是普遍问题，导致数据利用复杂且需专家干预。现有方法忽略了对AI至关重要的数据转换作用。尽管AI广泛应用，但缺乏对当代数据转换方法的全面综述。

Method: 本文采用综述研究方法，探讨数据异构性及其根源，系统分类并展示应对数据格式异构性的策略，并阐明每种策略的挑战。

Result: 本综述提供了对数据异构性及其根源的深入理解，并系统地分类了解决数据格式差异问题的多种数据转换策略，同时揭示了每种策略的固有挑战。

Conclusion: 本综述填补了当前在数据转换领域全面回顾的空白，为AI时代背景下，处理数据异构性（特别是格式差异）提供了关键的数据转换方法概览和挑战分析，对优化AI数据准备流程具有重要意义。

Abstract: Data heterogeneity is a prevalent issue, stemming from various conflicting
factors, making its utilization complex. This uncertainty, particularly
resulting from disparities in data formats, frequently necessitates the
involvement of experts to find resolutions. Current methodologies primarily
address conflicts related to data structures and schemas, often overlooking the
pivotal role played by data transformation. As the utilization of artificial
intelligence (AI) continues to expand, there is a growing demand for a more
streamlined data preparation process, and data transformation becomes
paramount. It customizes training data to enhance AI learning efficiency and
adapts input formats to suit diverse AI models. Selecting an appropriate
transformation technique is paramount in preserving crucial data details.
Despite the widespread integration of AI across various industries,
comprehensive reviews concerning contemporary data transformation approaches
are scarce. This survey explores the intricacies of data heterogeneity and its
underlying sources. It systematically categorizes and presents strategies to
address heterogeneity stemming from differences in data formats, shedding light
on the inherent challenges associated with each strategy.

</details>


### [68] [PinFM: Foundation Model for User Activity Sequences at a Billion-scale Visual Discovery Platform](https://arxiv.org/abs/2507.12704)
*Xiangyi Chen,Kousik Rajesh,Matthew Lawhon,Zelun Wang,Hanyu Li,Haomiao Li,Saurabh Vishwas Joshi,Pong Eksombatchai,Jaewon Yang,Yi-Ping Hsu,Jiajing Xu,Charles Rosenberg*

Main category: cs.LG

TL;DR: PinFM是一个十亿级规模的视觉发现平台的用户行为序列基础模型，通过预训练Transformer模型并进行优化，解决了工业推荐系统中的挑战，显著提升了吞吐量和新内容参与度。


<details>
  <summary>Details</summary>
Motivation: 用户活动序列在推荐系统中至关重要。尽管预训练-微调方法在其他领域流行，但在工业推荐系统中应用面临诸多挑战：模型需要足够可扩展以每秒处理数百万项，同时满足严格的成本和延迟限制；需要捕获用户活动与其他特征的交互；并能处理预训练阶段未见过的新项目。

Method: 开发了PinFM，一个200亿+参数的Transformer基础模型。该模型使用大量用户活动数据进行预训练，然后针对特定应用进行微调，并与现有模型高效耦合。为解决挑战，采用了创新的基础设施和算法优化技术，例如去重交叉注意力Transformer（DCAT）。

Result: 基础设施和算法优化（如DCAT）使Pinterest内部数据的吞吐量提高了600%。PinFM通过改变输入序列，学习了用户序列与候选项目之间的交互，导致新内容的参与度增加了20%。该模型已部署，为超过5亿用户提升了体验。

Conclusion: PinFM成功地将预训练-微调方法应用于十亿级规模的工业推荐系统，通过解决大规模、低延迟和新颖性挑战，显著提升了系统的效率和用户参与度，证明了其作为用户活动序列基础模型的有效性。

Abstract: User activity sequences have emerged as one of the most important signals in
recommender systems. We present a foundational model, PinFM, for understanding
user activity sequences across multiple applications at a billion-scale visual
discovery platform. We pretrain a transformer model with 20B+ parameters using
extensive user activity data, then fine-tune it for specific applications,
efficiently coupling it with existing models. While this
pretraining-and-fine-tuning approach has been popular in other domains, such as
Vision and NLP, its application in industrial recommender systems presents
numerous challenges. The foundational model must be scalable enough to score
millions of items every second while meeting tight cost and latency constraints
imposed by these systems. Additionally, it should capture the interactions
between user activities and other features and handle new items that were not
present during the pretraining stage.
  We developed innovative techniques to address these challenges. Our
infrastructure and algorithmic optimizations, such as the Deduplicated
Cross-Attention Transformer (DCAT), improved our throughput by 600% on
Pinterest internal data. We demonstrate that PinFM can learn interactions
between user sequences and candidate items by altering input sequences, leading
to a 20% increase in engagement with new items. PinFM is now deployed to help
improve the experience of more than a half billion users across various
applications.

</details>


### [69] [From SGD to Spectra: A Theory of Neural Network Weight Dynamics](https://arxiv.org/abs/2507.12709)
*Brian Richard Olsen,Sam Fatehmanesh,Frank Xiao,Adarsh Kumarappan,Anirudh Gajula*

Main category: cs.LG

TL;DR: 本文提出了一个矩阵值随机微分方程（SDE）框架，将随机梯度下降（SGD）与深度神经网络权重矩阵的奇异值谱演化联系起来，首次从理论上解释了观察到的“bulk+tail”谱结构，并通过实验验证了其预测。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络彻底改变了机器学习，但其训练动态在理论上仍不明确。

Method: 开发了一个连续时间、矩阵值的随机微分方程（SDE）框架，将SGD的微观动力学与权重矩阵奇异值谱的宏观演化严格关联起来。推导了精确的SDE，并刻画了其平稳分布。通过对Transformer和MLP架构进行受控实验，验证了理论预测。

Result: 推导了精确的SDE，表明平方奇异值遵循具有特征值排斥的Dyson布朗运动。将平稳分布表征为具有幂律尾的伽马型密度。首次为训练网络中经验观察到的“bulk+tail”谱结构提供了理论解释。实验结果验证了理论预测，SDE预测与观察到的谱演化之间显示出定量一致性。

Conclusion: 通过理论上解释训练过程中的谱演化并进行实验验证，为理解深度学习为何有效奠定了严格的基础。

Abstract: Deep neural networks have revolutionized machine learning, yet their training
dynamics remain theoretically unclear-we develop a continuous-time,
matrix-valued stochastic differential equation (SDE) framework that rigorously
connects the microscopic dynamics of SGD to the macroscopic evolution of
singular-value spectra in weight matrices. We derive exact SDEs showing that
squared singular values follow Dyson Brownian motion with eigenvalue repulsion,
and characterize stationary distributions as gamma-type densities with
power-law tails, providing the first theoretical explanation for the
empirically observed 'bulk+tail' spectral structure in trained networks.
Through controlled experiments on transformer and MLP architectures, we
validate our theoretical predictions and demonstrate quantitative agreement
between SDE-based forecasts and observed spectral evolution, providing a
rigorous foundation for understanding why deep learning works.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [70] [Energy-Efficient RSMA-enabled Low-altitude MEC Optimization Via Generative AI-enhanced Deep Reinforcement Learning](https://arxiv.org/abs/2507.12910)
*Xudong Wang,Hongyang Du,Lei Feng,Kaibin Huang*

Main category: cs.NI

TL;DR: 研究无人机低空移动边缘计算系统中，提出一种结合生成式AI增强深度强化学习与速率分裂多址（RSMA）的方法，旨在优化资源分配、抑制干扰并最大化能量效率。


<details>
  <summary>Details</summary>
Motivation: 6G对低延迟计算的需求推动了基于无人机的低空移动边缘计算（MEC）系统发展。然而，有限的频谱导致地面终端（GTs）间严重的上行链路干扰。

Method: 1. 制定了一个联合优化问题，涉及无人机3D轨迹、RSMA解码顺序、任务卸载决策和资源分配，目标是减轻多用户干扰和最大化能量效率。 2. 提出了一种生成式AI增强的深度强化学习（DRL）框架来解决该问题，其中扩散模型被嵌入到Actor网络中，以生成高质量的动作样本，改善混合动作空间的探索并避免局部最优。 3. 设计了一种基于优先级的RSMA解码策略，以实现低复杂度的有效逐次干扰消除。

Result: 仿真结果表明，所提出的低空MEC系统方法优于基线方法。将生成扩散模型（GDM）与RSMA结合可以显著提高能量效率性能。

Conclusion: 该研究提出的生成式AI增强DRL结合RSMA的框架，能有效解决无人机低空MEC系统中的干扰问题，并显著提升系统能量效率。

Abstract: The growing demand for low-latency computing in 6G is driving the use of
UAV-based low-altitude mobile edge computing (MEC) systems. However, limited
spectrum often leads to severe uplink interference among ground terminals
(GTs). In this paper, we investigate a rate-splitting multiple access
(RSMA)-enabled low-altitude MEC system, where a UAV-based edge server assists
multiple GTs in concurrently offloading their tasks over a shared uplink. We
formulate a joint optimization problem involving the UAV 3D trajectory, RSMA
decoding order, task offloading decisions, and resource allocation, aiming to
mitigate multi-user interference and maximize energy efficiency. Given the high
dimensionality, non-convex nature, and dynamic characteristics of this
optimization problem, we propose a generative AI-enhanced deep reinforcement
learning (DRL) framework to solve it efficiently. Specifically, we embed a
diffusion model into the actor network to generate high-quality action samples,
improving exploration in hybrid action spaces and avoiding local optima. In
addition, a priority-based RSMA decoding strategy is designed to facilitate
efficient successive interference cancellation with low complexity. Simulation
results demonstrate that the proposed method for low-altitude MEC systems
outperforms baseline methods, and that integrating GDM with RSMA can achieve
significantly improved energy efficiency performance.

</details>


### [71] [RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and Intention-Driven Agents](https://arxiv.org/abs/2507.13140)
*Kuiyuan Ding,Caili Guo,Yang Yang,Jianzhang Guo*

Main category: cs.NI

TL;DR: RIDAS是一个基于LLM的多智能体框架，通过将用户意图映射到AI RAN配置，提升了6G网络资源效率和用户支持数量。


<details>
  <summary>Details</summary>
Motivation: 6G网络要求AI与RAN紧密集成以满足严格的QoS和资源效率需求，但现有方案难以将高级用户意图映射到低级参数化配置，导致性能优化受阻。

Method: 提出RIDAS多智能体框架，包含表示驱动智能体（RDAs）和意图驱动智能体（IDA）。RDAs提供可调参数（秩、量化比特），平衡失真与传输速率；IDA利用LLM进行两阶段规划（带宽预分配和重分配），将用户意图和系统状态映射为最优RDA配置。

Result: 实验证明，在同等QoS约束下，RIDAS比WirelessAgent支持的用户数量多44.71%。

Conclusion: RIDAS能够有效捕捉用户意图，并在AI RAN环境中更高效地分配资源。

Abstract: Sixth generation (6G) networks demand tight integration of artificial
intelligence (AI) into radio access networks (RANs) to meet stringent quality
of service (QoS) and resource efficiency requirements. Existing solutions
struggle to bridge the gap between high level user intents and the low level,
parameterized configurations required for optimal performance. To address this
challenge, we propose RIDAS, a multi agent framework composed of representation
driven agents (RDAs) and an intention driven agent (IDA). RDAs expose open
interface with tunable control parameters (rank and quantization bits, enabling
explicit trade) offs between distortion and transmission rate. The IDA employs
a two stage planning scheme (bandwidth pre allocation and reallocation) driven
by a large language model (LLM) to map user intents and system state into
optimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\%
more users than WirelessAgent under equivalent QoS constraints. These results
validate ability of RIDAS to capture user intent and allocate resources more
efficiently in AI RAN environments.

</details>


### [72] [Predictability-Aware Motion Prediction for Edge XR via High-Order Error-State Kalman Filtering](https://arxiv.org/abs/2507.13179)
*Ziyu Zhong,Hector A Caltenco,Björn Landfeldt,Günter Alce*

Main category: cs.NI

TL;DR: 6G网络与边缘计算将使XR应用卸载成为现实，从而降低用户设备能耗并实现更小的设备设计。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络的发展，XR应用卸载被视为一个重要的新用例。其主要动机是解决当前XR应用在用户设备上运行时面临的计算密集、高能耗和设备尺寸限制问题。

Method: 利用6G网络的低延迟特性与边缘处理基础设施的结合，将XR应用中计算密集型功能（如渲染）从用户设备迁移到网络端进行处理。

Result: 通过此方法，首次在蜂窝网络中提供了XR应用计算密集型功能（包括渲染）的现实卸载场景，从而显著降低用户设备的电池需求，并为设计更小尺寸的新设备提供了可能性。

Conclusion: XR应用卸载在6G网络背景下是一个前景广阔且可行的用例，它能有效利用6G的低延迟和边缘计算能力，显著改善用户设备的能耗表现并促进设备形态创新。

Abstract: As 6G networks are developed and defined, offloading of XR applications is
emerging as one of the strong new use cases. The reduced 6G latency coupled
with edge processing infrastructure will for the first time provide a realistic
offloading scenario in cellular networks where several computationally
intensive functions, including rendering, can migrate from the user device and
into the network. A key advantage of doing so is the lowering of the battery
needs in the user devices and the possibility to design new devices with
smaller form factors.

</details>


### [73] [Bidirectional Age of Incorrect Information: A Performance Metric for Status Updates in Virtual Dynamic Environments](https://arxiv.org/abs/2507.13312)
*Chiara Schiavo,Manuele Favero,Alessandro Buratto,Leonardo Badia*

Main category: cs.NI

TL;DR: 本文提出双向不正确信息年龄（BAoII）来量化虚拟动态环境（VDEs）中实体因信息不准确或过时而付出的代价，并通过连续时间马尔可夫链分析和数值模拟，揭示了通信成本与信息新鲜度之间的权衡，并验证了其在评估系统性能和支持实时协作中的重要性。


<details>
  <summary>Details</summary>
Motivation: 在元宇宙和数字孪生等虚拟动态环境（VDEs）中，实体表示的准确性和及时性对于实现无缝交互和系统可靠性至关重要，但如何量化和管理过时或不准确的信息是一个挑战。

Method: 本文提出双向不正确信息年龄（BAoII）作为衡量指标，扩展了传统的不正确信息年龄概念，以捕捉双向信息交换中实体自身表示以及其他实体所共享表示的相互感知需求。通过连续时间马尔可夫链模型，推导了BAoII的长期闭式表达式，并识别了实现最优更新策略的传输成本阈值。研究还通过数值模拟验证了所提出的模型。

Result: 研究导出了长期BAoII的闭式表达式，确定了实现最优更新策略的传输成本阈值。结果表明，通信成本与信息新鲜度之间存在权衡关系。数值模拟验证了模型有效性，并展示了BAoII在评估系统性能方面的作用。

Conclusion: BAoII是一个量化VDEs中信息准确性及时效性的重要指标，能够有效评估系统性能，并对元宇宙和数字孪生中的实时协作具有重要指导意义，因为它揭示了信息新鲜度和通信成本之间的内在平衡。

Abstract: Virtual dynamic environments (VDEs) such as the Metaverse and digital twins
(DTs) require proper representation of the interacting entities to map their
characteristics within the simulated or augmented space. Keeping these
representations accurate and up-to-date is crucial for seamless interaction and
system reliability. In this paper, we propose bidirectional age of incorrect
information (BAoII) to address this aspect. BAoII quantifies the time-dependent
penalty paid by an entity in a VDE due to incorrect or outdated knowledge about
itself and the overall dynamically changing space. This extends the concept of
age of incorrect information for a bidirectional information exchange,
capturing that a VDE requires mutual awareness of the entity's own
representation, measured in the virtual space, and what the other entities
share about their representations. Using a continuous-time Markov chain model,
we derive a closed-form expression for long-term BAoII and identify a
transmission cost threshold for optimal update strategies. We describe a
trade-off between communication cost and information freshness and validate our
model through numerical simulations, demonstrating the impact of BAoII on
evaluating system performance and highlighting its relevance for real-time
collaboration in the Metaverse and DTs.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [74] [Learning-Based Interface for Semantic Communication with Bit Importance Awareness](https://arxiv.org/abs/2507.12850)
*Wenzheng Kong,Wenyi Zhang*

Main category: cs.IT

TL;DR: 本文提出一种学习型接口设计和重要性感知网络，改进了Split DeepJSCC，以在现有无线网络中实现语义通信，尤其在无线图像传输中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有联合信源信道编码（JSCC）方法难以与现有通信网络架构集成，导致应用和网络提供商分离的问题。

Method: 本文提出一种学习型接口设计，将接口参数视为可训练，以提升端到端性能。该接口能指定比特级重要性。此外，还提出一个重要性感知网络（Importance-Aware Net），利用接口导出的比特重要性信息，动态适应不同信道带宽比和时变信道条件。

Result: 实验结果显示，该方法在无线图像传输任务中显著提高了性能。

Conclusion: 这项工作为在现有无线网络中实现语义通信提供了一种潜在的解决方案。

Abstract: Joint source-channel coding (JSCC) is an effective approach for semantic
communication. However, current JSCC methods are difficult to integrate with
existing communication network architectures, where application and network
providers are typically different entities. Recently, a novel paradigm termed
Split DeepJSCC has been under consideration to address this challenge. Split
DeepJSCC employs a bit-level interface that enables separate design of source
and channel codes, ensuring compatibility with existing communication networks
while preserving the advantages of JSCC in terms of semantic fidelity and
channel adaptability. In this paper, we propose a learning-based interface
design by treating its parameters as trainable, achieving improved end-to-end
performance compared to Split DeepJSCC. In particular, the interface enables
specification of bit-level importance at the output of the source code.
Furthermore, we propose an Importance-Aware Net that utilizes the
interface-derived bit importance information, enabling dynamical adaptation to
diverse channel bandwidth ratios and time-varying channel conditions.
Experimental results show that our method improves performance in wireless
image transmission tasks. This work provides a potential solution for realizing
semantic communications in existing wireless networks.

</details>
