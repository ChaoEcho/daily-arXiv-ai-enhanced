<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 37]
- [cs.CV](#cs.CV) [Total: 34]
- [cs.AI](#cs.AI) [Total: 34]
- [cs.LG](#cs.LG) [Total: 34]
- [cs.NI](#cs.NI) [Total: 7]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Novel Differential Feature Learning for Effective Hallucination Detection and Classification](https://arxiv.org/abs/2509.21357)
*Wenkai Wang,Vincent Lee,Yizhen Zheng*

Main category: cs.CL

TL;DR: 本研究提出一种双模型架构，通过识别稀疏特征子集和分层“漏斗模式”，发现大型语言模型幻觉信号比预期更集中，从而显著提升幻觉检测效率并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明LLM幻觉信号存在于特定隐藏层，但其在层内的精确位置仍不明确，这限制了高效检测方法的发展。

Method: 提出一种双模型架构，包含：1. Projected Fusion (PF) 模块，用于自适应层间特征加权；2. Differential Feature Learning (DFL) 机制，通过计算并行编码器学习的互补表示之间的差异来识别判别性特征。

Result: 幻觉信号高度集中在稀疏特征子集中，在问答和对话任务上显著提高了检测准确性。分析揭示了分层的“漏斗模式”：浅层特征多样性高，深层特征使用集中。使用仅1%的特征维度即可保持检测性能且性能下降极小。

Conclusion: 幻觉信号比之前认为的更集中，这为开发计算高效的检测系统提供了途径，可在保持准确性的同时显著降低推理成本。

Abstract: Large language model hallucination represents a critical challenge where
outputs deviate from factual accuracy due to distributional biases in training
data. While recent investigations establish that specific hidden layers exhibit
differences between hallucinatory and factual content, the precise localization
of hallucination signals within layers remains unclear, limiting the
development of efficient detection methods. We propose a dual-model
architecture integrating a Projected Fusion (PF) block for adaptive inter-layer
feature weighting and a Differential Feature Learning (DFL) mechanism that
identifies discriminative features by computing differences between parallel
encoders learning complementary representations from identical inputs. Through
systematic experiments across HaluEval's question answering, dialogue, and
summarization datasets, we demonstrate that hallucination signals concentrate
in highly sparse feature subsets, achieving significant accuracy improvements
on question answering and dialogue tasks. Notably, our analysis reveals a
hierarchical "funnel pattern" where shallow layers exhibit high feature
diversity while deep layers demonstrate concentrated usage, enabling detection
performance to be maintained with minimal degradation using only 1\% of feature
dimensions. These findings suggest that hallucination signals are more
concentrated than previously assumed, offering a pathway toward computationally
efficient detection systems that could reduce inference costs while maintaining
accuracy.

</details>


### [2] [Influence Guided Context Selection for Effective Retrieval-Augmented Generation](https://arxiv.org/abs/2509.21359)
*Jiale Deng,Yanyan Shen,Ziyuan Pei,Youmin Chen,Linpeng Huang*

Main category: cs.CL

TL;DR: 本文提出一种基于“上下文影响力值（CI值）”的新型上下文选择方法，以解决检索增强生成（RAG）中低质量检索内容的问题。CI值全面评估上下文质量，并通过一个分层代理模型在8个NLP任务上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: RAG的有效性受限于低质量检索上下文（不相关或噪声信息）导致的大语言模型幻觉。现有基于预定义质量评估指标的上下文选择方法效果有限，因为它们未能全面利用查询、上下文列表和生成器信息进行评估。

Method: 将上下文质量评估重新概念化为推理时数据估值问题，并引入“上下文影响力值（CI值）”。CI值通过衡量移除每个上下文时的性能下降来量化上下文质量，综合考虑了查询相关性、列表独特性和生成器对齐性。仅保留CI值为正的上下文。为解决实际挑战，开发了一个参数化的CI值预测代理模型，该模型采用分层架构捕捉局部查询-上下文相关性和全局上下文间交互，并通过oracle CI值监督和端到端生成器反馈进行训练。

Result: 在8个NLP任务和多个LLM上的广泛实验表明，该上下文选择方法显著优于现有SOTA基线，能有效过滤低质量上下文并保留关键信息。

Conclusion: 所提出的基于CI值的上下文选择方法及其代理模型，通过全面评估和选择高质量上下文，有效解决了RAG中低质量检索内容的问题，显著提升了RAG的性能。

Abstract: Retrieval-Augmented Generation (RAG) addresses large language model (LLM)
hallucinations by grounding responses in external knowledge, but its
effectiveness is compromised by poor-quality retrieved contexts containing
irrelevant or noisy information. While existing approaches attempt to improve
performance through context selection based on predefined context quality
assessment metrics, they show limited gains over standard RAG. We attribute
this limitation to their failure in holistically utilizing available
information (query, context list, and generator) for comprehensive quality
assessment. Inspired by recent advances in data selection, we reconceptualize
context quality assessment as an inference-time data valuation problem and
introduce the Contextual Influence Value (CI value). This novel metric
quantifies context quality by measuring the performance degradation when
removing each context from the list, effectively integrating query-aware
relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI
value eliminates complex selection hyperparameter tuning by simply retaining
contexts with positive CI values. To address practical challenges of label
dependency and computational overhead, we develop a parameterized surrogate
model for CI value prediction during inference. The model employs a
hierarchical architecture that captures both local query-context relevance and
global inter-context interactions, trained through oracle CI value supervision
and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and
multiple LLMs demonstrate that our context selection method significantly
outperforms state-of-the-art baselines, effectively filtering poor-quality
contexts while preserving critical information. Code is available at
https://github.com/SJTU-DMTai/RAG-CSM.

</details>


### [3] [Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs](https://arxiv.org/abs/2509.21361)
*Norman Paulsen*

Main category: cs.CL

TL;DR: 研究发现大语言模型的实际有效上下文窗口（MECW）远小于其宣称的最大上下文窗口（MCW），且MECW会根据问题类型而变化。


<details>
  <summary>Details</summary>
Motivation: 测试大语言模型（LLM）提供商宣称的最大上下文窗口在实际应用中的有效性。

Method: 1. 定义最大有效上下文窗口（MECW）概念；2. 制定测试方法，评估上下文窗口在不同大小和问题类型下的有效性；3. 创建标准化比较方法以确定模型失效点。收集了数十万个数据点，对多个模型进行了测试。

Result: 报告的最大上下文窗口（MCW）与最大有效上下文窗口（MECW）之间存在显著差异。MECW不仅与MCW截然不同，还会根据问题类型而变化。部分顶级模型在上下文仅100个token时即失效，大多数模型在1000个token时准确性严重下降。所有模型的有效上下文窗口都远低于其最大上下文窗口，最多相差99%。

Conclusion: 最大有效上下文窗口（MECW）会根据提供的问题类型而变化，为提高模型准确性和降低幻觉率提供了清晰可行的见解。

Abstract: Large language model (LLM) providers boast big numbers for maximum context
window sizes. To test the real world use of context windows, we 1) define a
concept of maximum effective context window, 2) formulate a testing method of a
context window's effectiveness over various sizes and problem types, and 3)
create a standardized way to compare model efficacy for increasingly larger
context window sizes to find the point of failure. We collected hundreds of
thousands of data points across several models and found significant
differences between reported Maximum Context Window (MCW) size and Maximum
Effective Context Window (MECW) size. Our findings show that the MECW is, not
only, drastically different from the MCW but also shifts based on the problem
type. A few top of the line models in our test group failed with as little as
100 tokens in context; most had severe degradation in accuracy by 1000 tokens
in context. All models fell far short of their Maximum Context Window by as
much as 99 percent. Our data reveals the Maximum Effective Context Window
shifts based on the type of problem provided, offering clear and actionable
insights into how to improve model accuracy and decrease model hallucination
rates.

</details>


### [4] [How Large Language Models Need Symbolism](https://arxiv.org/abs/2509.21404)
*Xiaotie Deng,Hanyu Li*

Main category: cs.CL

TL;DR: AI的未来发展需要人类设计的符号作为大语言模型的指南针，以超越单纯的规模化并实现真正的发现。


<details>
  <summary>Details</summary>
Motivation: 认为当前AI发展过度依赖规模化，而大语言模型虽能力强大却缺乏明确方向和“直觉”，不足以实现真正的科学发现。

Method: 提出并论证将人类设计的符号作为“指南针”来引导大语言模型，以克服其“盲目直觉”的局限性。

Result: 形成核心主张：AI若要解锁真正的发现，大语言模型必须整合人类设计的符号进行指导。

Conclusion: 人类设计的符号对于引导大语言模型、推动AI从单纯规模化走向实现真正科学发现至关重要。

Abstract: We argue that AI's future requires more than scaling. To unlock genuine
discovery, large language models need a compass: human-crafted symbols to guide
their powerful but blind intuition.

</details>


### [5] [One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning](https://arxiv.org/abs/2509.21443)
*Sualeha Farid,Jayden Lin,Zean Chen,Shivani Kumar,David Jurgens*

Main category: cs.CL

TL;DR: 研究发现，大语言模型（LLMs）在多语言和多文化环境中进行道德推理时，由于其英语预训练，其道德判断在不同语言间存在显著不一致和文化偏差。本研究揭示了这些差异的驱动因素，并提出了一种道德推理错误类型学，呼吁开发更具文化意识的AI。


<details>
  <summary>Details</summary>
Motivation: LLMs在多语言多文化环境中部署，道德推理至关重要。然而，LLMs主要基于英语数据进行预训练，引发了对其在不同语言和文化背景下泛化道德判断能力的担忧。

Method: 将两个既定的道德推理基准翻译成五种文化和类型学上多样化的语言，进行多语言零样本评估。通过构建研究问题和案例研究，深入探究了这些道德判断差异的潜在驱动因素，并关联了预训练数据在塑造LLM道德倾向中的作用。

Result: LLMs在不同语言间的道德判断存在显著不一致性，常反映出文化错位。分析揭示了导致这些差异的底层驱动因素，包括意见分歧和LLMs采用的推理策略。案例研究进一步证实预训练数据影响着LLM的道德指南。

Conclusion: 本研究提炼出了一种结构化的道德推理错误类型学，强调了构建更具文化意识的AI的必要性。

Abstract: Large Language Models (LLMs) are increasingly deployed in multilingual and
multicultural environments where moral reasoning is essential for generating
ethically appropriate responses. Yet, the dominant pretraining of LLMs on
English-language data raises critical concerns about their ability to
generalize judgments across diverse linguistic and cultural contexts. In this
work, we systematically investigate how language mediates moral decision-making
in LLMs. We translate two established moral reasoning benchmarks into five
culturally and typologically diverse languages, enabling multilingual zero-shot
evaluation. Our analysis reveals significant inconsistencies in LLMs' moral
judgments across languages, often reflecting cultural misalignment. Through a
combination of carefully constructed research questions, we uncover the
underlying drivers of these disparities, ranging from disagreements to
reasoning strategies employed by LLMs. Finally, through a case study, we link
the role of pretraining data in shaping an LLM's moral compass. Through this
work, we distill our insights into a structured typology of moral reasoning
errors that calls for more culturally-aware AI.

</details>


### [6] [LLM-Based Support for Diabetes Diagnosis: Opportunities, Scenarios, and Challenges with GPT-5](https://arxiv.org/abs/2509.21450)
*Gaurav Kumar Gupta,Nirajan Acharya,Pranal Pande*

Main category: cs.CL

TL;DR: 本研究在模拟框架中评估了GPT-5在糖尿病早期识别和管理中的潜力，通过合成病例测试其在多个场景下的表现，结果显示与ADA标准高度一致，有望成为医患双用工具。


<details>
  <summary>Details</summary>
Motivation: 糖尿病是全球性健康挑战，早期识别和管理因症状模糊、实验室值边缘等因素而困难。大型语言模型（LLMs）有望通过提供结构化、可解释的输出，增强决策支持。

Method: 本研究使用基于ADA 2025护理标准和公共数据集（如NHANES）启发的合成病例模拟框架，评估了GPT-5。测试场景包括症状识别、实验室解释、妊娠糖尿病筛查、远程监测和多模态并发症检测五种。GPT-5的任务是病例分类、生成临床理由、患者解释及输出结构化JSON摘要。

Result: GPT-5的评估结果显示与美国糖尿病协会（ADA）定义的标准高度一致。

Conclusion: GPT-5有望作为临床医生和患者的双重工具。研究同时强调了在医疗领域负责任地评估LLMs时，可复现评估框架的重要性。

Abstract: Diabetes mellitus is a major global health challenge, affecting over half a
billion adults worldwide with prevalence projected to rise. Although the
American Diabetes Association (ADA) provides clear diagnostic thresholds, early
recognition remains difficult due to vague symptoms, borderline laboratory
values, gestational complexity, and the demands of long-term monitoring.
Advances in large language models (LLMs) offer opportunities to enhance
decision support through structured, interpretable, and patient-friendly
outputs. This study evaluates GPT-5, the latest generative pre-trained
transformer, using a simulation framework built entirely on synthetic cases
aligned with ADA Standards of Care 2025 and inspired by public datasets
including NHANES, Pima Indians, EyePACS, and MIMIC-IV. Five representative
scenarios were tested: symptom recognition, laboratory interpretation,
gestational diabetes screening, remote monitoring, and multimodal complication
detection. For each, GPT-5 classified cases, generated clinical rationales,
produced patient explanations, and output structured JSON summaries. Results
showed strong alignment with ADA-defined criteria, suggesting GPT-5 may
function as a dual-purpose tool for clinicians and patients, while underscoring
the importance of reproducible evaluation frameworks for responsibly assessing
LLMs in healthcare.

</details>


### [7] [Diagnosing the Performance Trade-off in Moral Alignment: A Case Study on Gender Stereotypes](https://arxiv.org/abs/2509.21456)
*Guangliang Liu,Bocheng Chen,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 本文分析了预训练语言模型（PLMs）进行道德对齐时，下游任务性能下降的内在机制，发现性能主要受整体遗忘水平影响，且选择性遗忘刻板印象会增加整体遗忘。


<details>
  <summary>Details</summary>
Motivation: 道德对齐（如微调或模型编辑）通常会导致预训练语言模型在下游任务上的性能下降。现有研究试图通过公平性目标实现性能权衡，即选择性遗忘刻板印象同时保留有效性。本文旨在深入探究在缓解性别刻板印象背景下，这种性能权衡背后的潜在机制。

Method: 通过遗忘机制和公平性目标的视角，对缓解性别刻板印象情境下，性能权衡的潜在机制进行了分析。

Result: 分析揭示了当前公平性目标在实现权衡方面的局限性：1) 下游任务性能主要由整体遗忘水平驱动；2) 选择性遗忘刻板印象倾向于增加整体遗忘；3) 缓解遗忘的通用解决方案在减少整体遗忘和提升下游任务性能方面无效。

Conclusion: 当前公平性目标在平衡模型道德对齐与下游任务性能方面存在局限性，因为选择性遗忘刻板印象会导致整体遗忘水平上升，且现有通用遗忘缓解方案无法有效改善性能。

Abstract: Moral alignment has emerged as a widely adopted approach for regulating the
behavior of pretrained language models (PLMs), typically through fine-tuning or
model editing on curated datasets. However, this process often comes at the
cost of degraded downstream task performance. Prior studies commonly aim to
achieve a performance trade-off by encouraging PLMs to selectively forget
stereotypical knowledge through carefully designed fairness objectives, while
preserving their helpfulness. In this short paper, we investigate the
underlying mechanisms of the performance trade-off in the context of mitigating
gender stereotypes, through the lens of forgetting and the fairness objective.
Our analysis reveals the limitations of current fairness objective in achieving
trade-off by demonstrating that: (1) downstream task performance is primarily
driven by the overall forgetting level; (2) selective forgetting of stereotypes
tends to increase overall forgetting; and (3) general solutions for mitigating
forgetting are ineffective at reducing overall forgetting and fail to improve
downstream task performance.

</details>


### [8] [A State-of-the-Art SQL Reasoning Model using RLVR](https://arxiv.org/abs/2509.21459)
*Alnur Ali,Ashutosh Baheti,Jonathan Chang,Ta-Chung Chi,Brandon Cui,Andrew Drozdov,Jonathan Frankle,Abhay Gupta,Pallavi Koppol,Sean Kulinski,Jonathan Li,Dipendra Misra,Krista Opsahl-Ong,Jose Javier Gonzalez Ortiz,Matei Zaharia,Yue Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种结合可验证奖励的强化学习（RLVR）方法，用于BIRD基准测试中的自然语言到SQL转换任务，通过简单的训练策略在不使用额外数据或专有模型的情况下达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 企业客户面临需要结合组织特定知识的自定义推理模型来解决问题，尤其是在奖励函数可验证的RLVR（可验证奖励强化学习）场景中。

Method: 采用了一种简单且通用的训练方案，包括精心选择提示词和模型、使用名为TAO的离线强化学习进行预热，以及后续严格的在线RLVR训练。

Result: 在BIRD私有测试集上，该方法取得了SOTA精度：无自洽性为73.56%，有自洽性为75.68%。在有自洽性情况下，模型所需的生成次数也少于次优方法。这些结果是在不额外使用训练数据和专有模型的情况下获得的。

Conclusion: 尽管BIRD只是一个代理任务，该框架的简洁性使其能够广泛应用于商业智能、数据科学和编码等企业领域。

Abstract: Developing custom reasoning models via Reinforcement Learning (RL) that can
incorporate organization-specific knowledge has great potential to address
problems faced by enterprise customers. In many of these problems, the reward
function is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We
apply RLVR to a popular data science benchmark called BIRD that measures the
ability of an AI agent to convert a natural language query for a database to
SQL executions. We apply a simple and general-purpose training recipe involving
careful prompt and model selection, a warm-up stage using our offline RL
approach called TAO, followed by rigorous online RLVR training. With no
additional training data beyond the BIRD training set and no use of proprietary
models, our very first submission to the BIRD leaderboard reached
state-of-the-art accuracy on the private test set: 73.56% without
self-consistency and 75.68% with self-consistency. In the latter case, our
model also required fewer generations than the second-best approach. While BIRD
is only a proxy task, the simplicity of our framework makes it broadly
applicable to enterprise domains such as business intelligence, data science,
and coding.

</details>


### [9] [Learning to Reason with Mixture of Tokens](https://arxiv.org/abs/2509.21482)
*Adit Jain,Brendan Rappazzo*

Main category: cs.CL

TL;DR: 该研究将混合词元生成(MoT-G)引入可验证奖励强化学习(RLVR)，以利用丰富的概率分布信息，显著提升大语言模型推理能力和训练效率。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法在推理步骤中仅采样离散词元，忽略了模型在候选词元上的丰富概率分布信息，这不必要地限制了推理搜索空间。而在非RL设置中利用此信息已被证明是有益的，但RLVR尚未充分利用。

Method: 该研究 investigates mixture-of-token generation (MoT-G) in RLVR。提出了一个统一框架，泛化了现有MoT-G方法，包括将混合嵌入构建为词元嵌入加权和的现有免训练方法，并将RLVR扩展到在连续混合空间中直接操作以生成思维链。

Result: 在Reasoning-Gym上评估了两种MoT-G变体，相较于Qwen2.5-1.5B模型的标准解码，MoT-G方法在10个任务中的7个上取得了显著改进（5-35%的提升）。此外，在轨迹数量减半的情况下达到了可比的准确率，表明训练效率有所提升。通过全面的隐状态和词元级分析，研究发现MoT-G的益处可能源于其在推理过程中保持更高隐状态熵和促进词元空间探索的能力。

Conclusion: MoT-G通过有效利用词元的概率分布信息，解决了RLVR中离散词元采样的局限性，显著提升了大语言模型在推理任务上的性能和训练效率。其优势可能在于能够维持更高的隐状态熵和促进词元空间的探索。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a leading
approach for improving large language model (LLM) reasoning capabilities. Most
current methods follow variants of Group Relative Policy Optimization, which
samples multiple reasoning completions, scores them relative to each other, and
adjusts the policy accordingly. However, these approaches invariably sample
discrete tokens at each reasoning step, discarding the rich distributional
information in the model's probability distribution over candidate tokens.
While preserving and utilizing this distributional information has proven
beneficial in non-RL settings, current RLVR methods seem to be unnecessarily
constraining the reasoning search space by not using this information. To
address this limitation, we investigate mixture-of-token generation (MoT-G) in
RLVR. We present a unified framework that generalizes existing MoT-G
approaches, including existing training-free methods that construct mixture
embeddings as weighted sums over token embeddings, and extend RLVR to operate
directly in this continuous mixture space for generating chain-of-thought.
Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive
language tasks, we find that MoT--G methods achieve substantial improvements
(5--35 \% gains on 7 out of 10 tasks) compared to standard decoding with the
Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of
trajectories, suggesting improved training efficiency. Through comprehensive
hidden-state and token-level analyses, we provide evidence that MoT--G's
benefits may stem from its ability to maintain higher hidden-state entropy
throughout the reasoning process and promote exploration in token space.

</details>


### [10] [Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning](https://arxiv.org/abs/2509.21487)
*Jillian Xu,Dylan Zhou,Vinay Shukla,Yang Yang,Junrui Ruan,Shuhuai Lin,Wenfei Zou,Yinxiao Liu,Karthik Lakshmanan*

Main category: cs.CL

TL;DR: DHRD是一种双头推理蒸馏训练方法，通过在训练时利用教师推理指导，解决了CoT提示的准确性与吞吐量之间的矛盾，在推理时实现高效率并保持或提升准确性。


<details>
  <summary>Details</summary>
Motivation: 链式思考 (CoT) 提示能提高分类精度，但由于需要生成推理过程，会显著降低推理吞吐量。

Method: 提出双头推理蒸馏 (DHRD) 方法，该方法为解码器模型增加一个用于训练和推理的池化分类头，以及一个仅在训练时由教师推理监督的推理头。训练时使用标签交叉熵和基于输入加推理序列的token级LM损失的加权和作为损失函数。

Result: 在七个SuperGLUE任务上，DHRD比池化基线取得了0.65-5.47%的相对增益，在蕴含/因果任务上增益尤其显著。由于推理时禁用推理头，其推理吞吐量与池化分类器相同，并且比CoT解码高出96-142倍的每秒查询数 (QPS)。

Conclusion: DHRD成功解决了CoT提示中准确性与吞吐量之间的权衡问题，在不牺牲推理效率的前提下，实现了竞争性的分类性能。

Abstract: Chain-of-Thought (CoT) prompting often improves classification accuracy, but
it introduces a significant throughput penalty with rationale generation (Wei
et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we
introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for
decoder-only language models (LMs) that adds (i) a pooled classification head
used during training and inference and (ii) a reasoning head supervised by
teacher rationales used only in training. We train with a loss function that is
a weighted sum of label cross-entropy and token-level LM loss over
input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative
gains of 0.65-5.47% over pooled baselines, with notably larger gains on
entailment/causal tasks. Since we disable the reasoning head at test time,
inference throughput matches pooled classifiers and exceeds CoT decoding on the
same backbones by 96-142 times in QPS.

</details>


### [11] [On Code-Induced Reasoning in LLMs](https://arxiv.org/abs/2509.21499)
*Abdul Waheed,Zhen Wu,Carolyn Rosé,Daphne Ippolito*

Main category: cs.CL

TL;DR: 研究表明，LLM对代码的结构性扰动比语义性扰动更敏感。伪代码等抽象形式与代码同样有效，甚至在更少tokens下能提升性能。不同编程语言的风格对特定推理任务有不同增益。


<details>
  <summary>Details</summary>
Motivation: 已知代码数据能增强大型语言模型（LLMs）的推理能力，但具体是代码的哪些方面起作用尚不明确。

Method: 构建了10种编程语言的并行指令数据集，并通过受控扰动选择性破坏代码的结构或语义属性。在每个变体上对来自5个模型家族和8个规模的LLM进行微调，并在自然语言、数学和代码任务上评估其性能，共进行了3331次实验。

Result: ['LLM对结构性扰动比语义性扰动更敏感，尤其在数学和代码任务上。', '伪代码和流程图等适当的抽象形式与原始代码一样有效，且在不遵循原始语法的情况下以更少token编码相同信息，往往能保持甚至改善性能。', '即使是带有误导信号的损坏代码，只要表面层规律性保持，其表现仍具竞争力。', '语法风格也会影响特定任务的增益，Python有利于自然语言推理，而Java和Rust等低级语言则有利于数学任务。']

Conclusion: 本研究通过系统性框架，深入分析了代码不同属性对LLM推理能力的影响，为设计增强LLM推理能力的训练数据提供了见解和指导。

Abstract: Code data has been shown to enhance the reasoning capabilities of large
language models (LLMs), but it remains unclear which aspects of code are most
responsible. We investigate this question with a systematic, data-centric
framework. We construct parallel instruction datasets in ten programming
languages and apply controlled perturbations that selectively disrupt
structural or semantic properties of code. We then finetune LLMs from five
model families and eight scales on each variant and evaluate their performance
on natural language, math, and code tasks. Across 3,331 experiments, our
results show that LLMs are more vulnerable to structural perturbations than
semantic ones, particularly on math and code tasks. Appropriate abstractions
like pseudocode and flowcharts can be as effective as code, while encoding the
same information with fewer tokens without adhering to original syntax can
often retain or even improve performance. Remarkably, even corrupted code with
misleading signals remains competitive when surface-level regularities persist.
Finally, syntactic styles also shape task-specific gains with Python favoring
natural language reasoning and lower-level languages such as Java and Rust
favoring math. Through our systematic framework, we aim to provide insight into
how different properties of code influence reasoning and inform the design of
training data for enhancing LLM reasoning capabilities.

</details>


### [12] [Agribot: agriculture-specific question answer system](https://arxiv.org/abs/2509.21535)
*Naman Jain,Pranjali Jain,Pratik Kayal,Jayakrishna Sahit,Soham Pachpande,Jayesh Choudhari*

Main category: cs.CL

TL;DR: 为印度农民开发了一个基于Kisan呼叫中心数据集的农业聊天机器人，通过句子嵌入模型和实体提取，实现了86%的准确率，旨在提供便捷的农业信息并提高农业产出。


<details>
  <summary>Details</summary>
Motivation: 印度是农业经济体，农业实践的正确信息对农业增长至关重要。农民需要快速获取信息以解决其疑问，但现有方式可能效率不高。

Method: 构建了一个农业聊天机器人，利用Kisan呼叫中心的数据集。系统最初基于句子嵌入模型，通过消除同义词和结合实体提取技术进一步优化。

Result: 该聊天机器人能回答有关天气、市场价格、植物保护和政府计划等方面的疑问。系统全天候可用，可通过任何电子设备访问。初步准确率为56%，经过同义词消除和实体提取后，准确率提升至86%。

Conclusion: 该系统能帮助农民更便捷地获取农业相关信息，从而提高农业产出。同时，它也能减轻呼叫中心工作人员的工作负担，使他们能将精力投入到更有价值的目标上。

Abstract: India is an agro-based economy and proper information about agricultural
practices is the key to optimal agricultural growth and output. In order to
answer the queries of the farmer, we have build an agricultural chatbot based
on the dataset from Kisan Call Center. This system is robust enough to answer
queries related to weather, market rates, plant protection and government
schemes. This system is available 24* 7, can be accessed through any electronic
device and the information is delivered with the ease of understanding. The
system is based on a sentence embedding model which gives an accuracy of 56%.
After eliminating synonyms and incorporating entity extraction, the accuracy
jumps to 86%. With such a system, farmers can progress towards easier
information about farming related practices and hence a better agricultural
output. The job of the Call Center workforce would be made easier and the hard
work of various such workers can be redirected to a better goal.

</details>


### [13] [Domain-Aware Speaker Diarization On African-Accented English](https://arxiv.org/abs/2509.21554)
*Chibuzor Okocha,Kelechi Ezema,Christan Grant*

Main category: cs.CL

TL;DR: 本研究发现非洲口音英语的说话人识别在临床对话中存在显著的领域效应惩罚，主要源于短轮次和频繁重叠导致的误报和漏检。轻量级域适应虽能减少误差，但未能完全消除差距。建议未来关注重叠感知分割和平衡临床资源。


<details>
  <summary>Details</summary>
Motivation: 探究针对非洲口音英语的说话人识别在不同领域（特别是临床对话）中的性能表现及其领域效应，并寻找提升性能的方法。

Method: 评估了多个现有说话人识别系统在通用和临床对话上的性能，采用严格的DER协议（包含重叠部分评分）。通过错误分析识别领域惩罚的来源，并尝试通过在口音匹配数据上微调分割模块进行轻量级域适应。

Result: 临床语音中存在显著且持续的领域惩罚，该惩罚在不同模型中均保持显著。错误分析表明，惩罚主要归因于误报和漏检，这与短轮次和频繁的对话重叠有关。轻量级域适应虽然减少了错误，但未能完全消除领域差距。

Conclusion: 本研究贡献在于提供了一个跨领域的受控基准、简明的错误分解方法和可复现的适应方案。结果表明，未来研究应聚焦于开发重叠感知的分割技术和构建平衡的临床资源。

Abstract: This study examines domain effects in speaker diarization for
African-accented English. We evaluate multiple production and open systems on
general and clinical dialogues under a strict DER protocol that scores overlap.
A consistent domain penalty appears for clinical speech and remains significant
across models. Error analysis attributes much of this penalty to false alarms
and missed detections, aligning with short turns and frequent overlap. We test
lightweight domain adaptation by fine-tuning a segmentation module on
accent-matched data; it reduces error but does not eliminate the gap. Our
contributions include a controlled benchmark across domains, a concise approach
to error decomposition and conversation-level profiling, and an adaptation
recipe that is easy to reproduce. Results point to overlap-aware segmentation
and balanced clinical resources as practical next steps.

</details>


### [14] [Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution](https://arxiv.org/abs/2509.21557)
*Yash Saxena,Raviteja Bommireddy,Ankur Padia,Manas Gaur*

Main category: cs.CL

TL;DR: 论文比较了LLM的两种引用范式（G-Cite和P-Cite），发现覆盖率与引用正确性之间存在权衡，并推荐在高风险应用中优先使用以检索为中心的P-Cite方法。


<details>
  <summary>Details</summary>
Motivation: 在医疗、法律等高风险领域，可信赖的LLM必须引用可验证的来源。研究者面临一个选择：是让模型在生成时一并引用，还是先生成答案再附加上引用。

Method: 论文引入并定义了两种引用范式：生成时引用（G-Cite）和事后引用（P-Cite）。通过从零样本到先进检索增强方法，在四个流行的归因数据集上进行了全面评估。

Result: 结果显示覆盖率和引用正确性之间存在一致的权衡，其中检索是两种范式下归因质量的主要驱动因素。P-Cite方法在具有竞争力的正确性和适中延迟的同时实现高覆盖率；G-Cite方法则以牺牲覆盖率和速度为代价优先考虑精确性。

Conclusion: 对于高风险应用，建议采用以检索为中心、P-Cite优先的方法；而对于如严格声明验证等精确度至关重要的场景，则保留G-Cite。

Abstract: Trustworthy Large Language Models (LLMs) must cite human-verifiable sources
in high-stakes domains such as healthcare, law, academia, and finance, where
even small errors can have severe consequences. Practitioners and researchers
face a choice: let models generate citations during decoding, or let models
draft answers first and then attach appropriate citations. To clarify this
choice, we introduce two paradigms: Generation-Time Citation (G-Cite), which
produces the answer and citations in one pass, and Post-hoc Citation (P-Cite),
which adds or verifies citations after drafting. We conduct a comprehensive
evaluation from zero-shot to advanced retrieval-augmented methods across four
popular attribution datasets and provide evidence-based recommendations that
weigh trade-offs across use cases. Our results show a consistent trade-off
between coverage and citation correctness, with retrieval as the main driver of
attribution quality in both paradigms. P-Cite methods achieve high coverage
with competitive correctness and moderate latency, whereas G-Cite methods
prioritize precision at the cost of coverage and speed. We recommend a
retrieval-centric, P-Cite-first approach for high-stakes applications,
reserving G-Cite for precision-critical settings such as strict claim
verification. Our codes and human evaluation results are available at
https://anonymous.4open.science/r/Citation_Paradigms-BBB5/

</details>


### [15] [Comparative Personalization for Multi-document Summarization](https://arxiv.org/abs/2509.21562)
*Haoyuan Li,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: 本文提出ComPSum框架，通过比较用户间偏好差异生成个性化多文档摘要，并引入无参考评估框架AuthorMap和数据集PerMSum。实验表明ComPSum优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 为了有效实现个性化多文档摘要，需要通过比较目标用户与其他用户的偏好，识别并利用用户偏好中的细粒度差异。

Method: 1. **ComPSum框架**: 通过比较用户偏好与他人偏好，生成结构化的用户分析，然后利用此分析指导个性化摘要生成。
2. **AuthorMap评估框架**: 提出一种细粒度、无参考的个性化多文档摘要评估方法，通过分析为不同用户生成的个性化摘要之间的作者归属来评估系统个性化能力。
3. **PerMSum数据集**: 构建了一个包含评论和新闻领域的个性化多文档摘要数据集，以支持鲁棒评估。

Result: ComPSum在PerMSum数据集上，使用AuthorMap评估框架进行评估，表现优于强基线模型。

Conclusion: ComPSum框架能有效识别并利用细粒度用户偏好差异，生成高质量的个性化多文档摘要，并通过新颖的AuthorMap评估方法和PerMSum数据集得到了验证。

Abstract: Personalized multi-document summarization (MDS) is essential for meeting
individual user preferences of writing style and content focus for summaries.
In this paper, we propose that for effective personalization, it is important
to identify fine-grained differences between users' preferences by comparing
the given user's preferences with other users' preferences.Motivated by this,
we propose ComPSum, a personalized MDS framework. It first generates a
structured analysis of a user by comparing their preferences with other users'
preferences. The generated structured analysis is then used to guide the
generation of personalized summaries. To evaluate the performance of ComPSum,
we propose AuthorMap, a fine-grained reference-free evaluation framework for
personalized MDS. It evaluates the personalization of a system based on the
authorship attribution between two personalized summaries generated for
different users. For robust evaluation of personalized MDS, we construct
PerMSum, a personalized MDS dataset in the review and news domain. We evaluate
the performance of ComPSum on PerMSum using AuthorMap, showing that it
outperforms strong baselines.

</details>


### [16] [Vision Language Models Cannot Plan, but Can They Formalize?](https://arxiv.org/abs/2509.21576)
*Muyu He,Yuxi Zheng,Yuchen Liu,Zijian An,Bill Cai,Jiani Huang,Lifeng Zhou,Feng Liu,Ziyang Li,Li Zhang*

Main category: cs.CL

TL;DR: 本文提出VLM作为形式化器（VLM-as-formalizer）的方法，将多模态规划任务转化为PDDL，并发现其在长程规划中优于端到端方法，但视觉能力是瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）虽能处理简单的多模态规划，但在需要长序列动作的长程规划上表现不佳。文本领域已通过LLM作为形式化器（LLM-as-formalizer），将规划问题转换为如PDDL等形式化语言并调用求解器来有效解决长程规划。但在多模态环境中，VLM作为形式化器的研究稀缺，且常伴随简化（如预定义词汇），因此需要探索开放词汇、一次性学习的多模态PDDL形式化。

Method: 本文提出了五种VLM作为形式化器的管道，旨在实现一次性（one-shot）、开放词汇（open-vocabulary）和多模态PDDL形式化。研究团队在现有基准上进行评估，并引入了另外两个新基准，首次考虑了真实、多视角和低质量图像下的规划任务。

Result: 研究结果表明，VLM作为形式化器的方法显著优于端到端的规划生成。主要的瓶颈在于视觉而非语言，因为VLM常未能捕捉到所有必要的对象关系。生成中间文本表示（如图像描述或场景图）虽部分补偿了性能，但其提升不一致，表明未来在多模态规划形式化方面仍有研究空间。

Conclusion: VLM作为形式化器是解决长程多模态规划任务的有效途径，其表现优于直接的端到端规划生成。然而，当前视觉能力是制约其性能的关键瓶颈，特别是在捕捉详尽对象关系方面。未来研究应着重于提升VLM的视觉理解和关系推理能力，以进一步优化多模态规划的形式化过程。

Abstract: The advancement of vision language models (VLMs) has empowered embodied
agents to accomplish simple multimodal planning tasks, but not long-horizon
ones requiring long sequences of actions. In text-only simulations,
long-horizon planning has seen significant improvement brought by repositioning
the role of LLMs. Instead of directly generating action sequences, LLMs
translate the planning domain and problem into a formal planning language like
the Planning Domain Definition Language (PDDL), which can call a formal solver
to derive the plan in a verifiable manner. In multimodal environments, research
on VLM-as-formalizer remains scarce, usually involving gross simplifications
such as predefined object vocabulary or overly similar few-shot examples. In
this work, we present a suite of five VLM-as-formalizer pipelines that tackle
one-shot, open-vocabulary, and multimodal PDDL formalization. We evaluate those
on an existing benchmark while presenting another two that for the first time
account for planning with authentic, multi-view, and low-quality images. We
conclude that VLM-as-formalizer greatly outperforms end-to-end plan generation.
We reveal the bottleneck to be vision rather than language, as VLMs often fail
to capture an exhaustive set of necessary object relations. While generating
intermediate, textual representations such as captions or scene graphs
partially compensate for the performance, their inconsistent gain leaves
headroom for future research directions on multimodal planning formalization.

</details>


### [17] ["Be My Cheese?": Assessing Cultural Nuance in Multilingual LLM Translations](https://arxiv.org/abs/2509.21577)
*Madison Van Doren,Cory Holland*

Main category: cs.CL

TL;DR: 本初步研究评估了多语言AI模型在翻译比喻性语言时的本地化能力，发现现有模型在文化得体性方面仍有显著不足，即使语法正确也常需人工修正。


<details>
  <summary>Details</summary>
Motivation: 现有LLM翻译研究和行业基准侧重语法准确性和token级别正确性，但忽略了文化得体性和整体本地化质量，这对于营销和电商等实际应用至关重要。

Method: 评估了87份由LLM生成的电商营销邮件翻译（涵盖20种语言的24种方言）。由目标语言流利的人类评审员提供定量评分和定性反馈，评估其对原文语调、意义和目标受众的忠实度。

Result: 领先模型通常能生成语法正确的翻译，但在处理文化细微之处（如比喻表达和文字游戏）时仍有明显不足，常需大量人工修正。即使在行业基准中表现优异的高资源语言也频繁出现此类错误。

Conclusion: 本研究挑战了数据量是机器翻译质量最可靠预测因素的假设，提出文化得体性是衡量多语言LLM性能的关键因素。当前多语言AI系统在实际本地化场景中存在局限性，需进行更大规模研究以提供可推广的见解，并指导在文化多样性背景下部署可靠的机器翻译工作流。

Abstract: This pilot study explores the localisation capabilities of state-of-the-art
multilingual AI models when translating figurative language, such as idioms and
puns, from English into a diverse range of global languages. It expands on
existing LLM translation research and industry benchmarks, which emphasise
grammatical accuracy and token-level correctness, by focusing on cultural
appropriateness and overall localisation quality - critical factors for
real-world applications like marketing and e-commerce.
  To investigate these challenges, this project evaluated a sample of 87
LLM-generated translations of e-commerce marketing emails across 24 regional
dialects of 20 languages. Human reviewers fluent in each target language
provided quantitative ratings and qualitative feedback on faithfulness to the
original's tone, meaning, and intended audience. Findings suggest that, while
leading models generally produce grammatically correct translations, culturally
nuanced language remains a clear area for improvement, often requiring
substantial human refinement. Notably, even high-resource global languages,
despite topping industry benchmark leaderboards, frequently mistranslated
figurative expressions and wordplay.
  This work challenges the assumption that data volume is the most reliable
predictor of machine translation quality and introduces cultural
appropriateness as a key determinant of multilingual LLM performance - an area
currently underexplored in existing academic and industry benchmarks. As a
proof of concept, this pilot highlights limitations of current multilingual AI
systems for real-world localisation use cases. Results of this pilot support
the opportunity for expanded research at greater scale to deliver generalisable
insights and inform deployment of reliable machine translation workflows in
culturally diverse contexts.

</details>


### [18] [Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective](https://arxiv.org/abs/2509.21613)
*Lingxiao Kong,Cong Yang,Oya Deniz Beyan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 本文探讨了多目标强化学习（MORL）在大型语言模型（LLM）优化中的应用，提出了MORL分类法，分析了现有方法的局限性，并展望了MORL基准框架和元策略MORL作为未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 多目标强化学习（MORL）在大型语言模型（LLM）中实现多目标优化面临重大挑战和机遇。需要高效、灵活的MORL方法来适应LLM的个性化功能和内在复杂性。

Method: 引入了MORL分类法，并分析了各种MORL方法在LLM优化中的优缺点。提出了一个MORL基准框架的愿景，旨在解决不同方法对目标关系的影响。

Result: 识别出LLM优化对高效、灵活MORL方法的需求，这些方法需适应个性化功能和固有复杂性。提出了MORL基准框架的愿景，以评估不同方法对目标关系的影响。

Conclusion: 未来研究应关注元策略MORL开发，通过其双层学习范式提高效率和灵活性，以改善LLM性能。并提出了一个MORL基准框架的愿景，作为评估方法的途径。

Abstract: Multi-Objective Reinforcement Learning (MORL) presents significant challenges
and opportunities for optimizing multiple objectives in Large Language Models
(LLMs). We introduce a MORL taxonomy and examine the advantages and limitations
of various MORL methods when applied to LLM optimization, identifying the need
for efficient and flexible approaches that accommodate personalization
functionality and inherent complexities in LLMs and RL. We propose a vision for
a MORL benchmarking framework that addresses the effects of different methods
on diverse objective relationships. As future research directions, we focus on
meta-policy MORL development that can improve efficiency and flexibility
through its bi-level learning paradigm, highlighting key research questions and
potential solutions for improving LLM performance.

</details>


### [19] [OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule](https://arxiv.org/abs/2509.21623)
*Yuxuan Zhu,David H. Yang,Mohammad Mohammadi Amiri,Keerthiram Murugesan,Tejaswini Pedapati,Pin-Yu Chen*

Main category: cs.CL

TL;DR: OjaKV通过混合存储策略和在线子空间自适应，解决了LLM长上下文KV缓存的内存瓶颈，在保持甚至提升精度的同时实现高压缩率。


<details>
  <summary>Details</summary>
Motivation: LLM的长上下文能力受限于KV缓存的巨大内存消耗（例如Llama-3.1-8B处理32K上下文批处理4时KV缓存需16GB，超出模型权重）。现有基于低秩投影的KV缓存压缩方法依赖静态离线学习的子空间，在数据分布变化时性能不佳。

Method: 引入OjaKV框架，该框架整合了战略性混合存储策略和在线子空间自适应。具体地，OjaKV：1) 将关键的首部和最新token以全秩形式保留，作为注意力的高保真锚点；2) 对绝大多数中间token，通过使用Oja算法（在线主成分分析）增量自适应投影基进行低秩压缩。子空间在提示预填充期间进行全面更新，解码期间进行轻量级周期性更新，以适应不断变化的上下文。该框架完全兼容FlashAttention等现代注意力模块。

Result: 实验表明，OjaKV在高压缩率下能够保持甚至提高零样本准确性。尤其在需要复杂推理的超长上下文基准测试中，OjaKV取得了最显著的提升，凸显了在线子空间自适应在动态跟踪上下文变化中的重要性。

Conclusion: OjaKV被确立为一个实用、即插即用的解决方案，无需模型微调即可实现内存高效的长上下文推理。

Abstract: The expanding long-context capabilities of large language models are
constrained by a significant memory bottleneck: the key-value (KV) cache
required for autoregressive generation. This bottleneck is substantial; for
instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of
4 requires approximately 16GB for its KV cache, a size exceeding the model's
weights. While KV-cache compression via low-rank projection is a promising
direction, existing methods rely on a static, offline-learned subspace that
performs poorly under data distribution shifts. To overcome these limitations,
we introduce OjaKV, a novel framework that integrates a strategic hybrid
storage policy with online subspace adaptation. First, OjaKV recognizes that
not all tokens are equally important for compression; it preserves the crucial
first and most recent tokens in full-rank, maintaining high-fidelity anchors
for attention. Second, for the vast majority of intermediate tokens, it applies
low-rank compression by incrementally adapting the projection basis using Oja's
algorithm for online principal component analysis. This adaptation involves a
comprehensive update during prompt prefilling and lightweight periodic updates
during decoding, ensuring the subspace remains aligned with the evolving
context. Crucially, our framework is fully compatible with modern attention
modules like FlashAttention. Experiments demonstrate that OjaKV maintains or
even improves zero-shot accuracy at high compression ratios. In particular,
OjaKV achieves its strongest gains on very long-context benchmarks that require
complex reasoning, highlighting the importance of online subspace adaptation in
dynamically tracking context shifts. These results establish our hybrid
framework as a practical, plug-and-play solution for memory-efficient
long-context inference without requiring model fine-tuning.

</details>


### [20] [Towards Transparent AI: A Survey on Explainable Language Models](https://arxiv.org/abs/2509.21631)
*Avash Palikhe,Zichong Wang,Zhipeng Yin,Rui Guo,Qiang Duan,Jie Yang,Wenbin Zhang*

Main category: cs.CL

TL;DR: 本综述全面审查了针对语言模型（LMs）的可解释人工智能（XAI）技术，重点根据Transformer架构（编码器、解码器、编解码器）进行分类，并评估其优缺点和有效性，以解决LMs的黑盒问题，并识别未来研究挑战和方向。


<details>
  <summary>Details</summary>
Motivation: 语言模型（LMs）虽取得显著进步，但其“黑盒”特性导致内部机制和决策过程缺乏可解释性，在高风险领域应用受限。现有针对非LMs的XAI方法不适用于复杂LMs，而现有LMs的XAI综述未能充分捕捉模型架构多样性带来的独特挑战。

Method: 本文采用综述研究方法，旨在弥补现有空白。具体方法包括：1. 对LMs的XAI技术进行全面审查。2. 根据Transformer底层架构（仅编码器、仅解码器、编码器-解码器）对XAI技术进行分类和组织。3. 分析XAI方法如何适应各类架构，并评估其优缺点。4. 通过可信度和忠实度双重视角评估这些技术的有效性。

Result: 本综述系统地回顾了针对LMs的XAI技术，并根据其Transformer架构进行了分类组织。分析了这些方法如何适应不同架构，并评估了各自的优势和局限性。同时，通过可信度和忠实度评估了这些技术的有效性，并识别了该领域的开放研究挑战，概述了有前景的未来发展方向。

Conclusion: 本综述通过提供一个针对LMs的结构化XAI技术视图、评估其有效性，并识别开放挑战和未来方向，旨在指导持续的努力，以开发出更鲁棒、透明和可解释的LMs的XAI方法，从而促进LMs在关键领域的负责任应用。

Abstract: Language Models (LMs) have significantly advanced natural language processing
and enabled remarkable progress across diverse domains, yet their black-box
nature raises critical concerns about the interpretability of their internal
mechanisms and decision-making processes. This lack of transparency is
particularly problematic for adoption in high-stakes domains, where
stakeholders need to understand the rationale behind model outputs to ensure
accountability. On the other hand, while explainable artificial intelligence
(XAI) methods have been well studied for non-LMs, they face many limitations
when applied to LMs due to their complex architectures, considerable training
corpora, and broad generalization abilities. Although various surveys have
examined XAI in the context of LMs, they often fail to capture the distinct
challenges arising from the architectural diversity and evolving capabilities
of these models. To bridge this gap, this survey presents a comprehensive
review of XAI techniques with a particular emphasis on LMs, organizing them
according to their underlying transformer architectures: encoder-only,
decoder-only, and encoder-decoder, and analyzing how methods are adapted to
each while assessing their respective strengths and limitations. Furthermore,
we evaluate these techniques through the dual lenses of plausibility and
faithfulness, offering a structured perspective on their effectiveness.
Finally, we identify open research challenges and outline promising future
directions, aiming to guide ongoing efforts toward the development of robust,
transparent, and interpretable XAI methods for LMs.

</details>


### [21] [ReviewScore: Misinformed Peer Review Detection with Large Language Models](https://arxiv.org/abs/2509.21679)
*Hyun Ryu,Doohyuk Jang,Hyemin S. Lee,Joonhyun Jeong,Gyeongman Kim,Donghyeon Cho,Gyouk Chu,Minyeong Hwang,Hyeongwon Jang,Changhun Kim,Haechan Kim,Jina Kim,Joowon Kim,Yoonjeon Kim,Kwanhyung Lee,Chanjae Park,Heecheol Yun,Gregor Betz,Eunho Yang*

Main category: cs.CL

TL;DR: 针对AI会议同行评审质量下降问题，本文定义并验证了“误导性评审点”（ReviewScore），通过自动化引擎和专家标注数据集，评估了LLM检测误导性评审点的能力，发现LLM在前提层面检测效果更佳，显示了自动化评审质量评估的潜力。


<details>
  <summary>Details</summary>
Motivation: AI会议投稿量激增导致同行评审质量下降，急需可靠地检测低质量（误导性）评审意见。

Method: 1. 定义“误导性评审点”为含错误前提的弱点或论文中已有答案的问题，并引入ReviewScore指标。2. 提出自动化引擎重构弱点的前提以评估事实性。3. 构建人工专家标注的ReviewScore数据集。4. 使用8个SOTA LLM评估人-模型在ReviewScore评估上的一致性。

Result: 1. 15.2%的弱点和26.4%的问题被认定为误导性评审点。2. LLM在ReviewScore评估上与人类专家表现出中等一致性。3. 评估前提层面的事实性比评估弱点层面能显著提高一致性。

Conclusion: 研究结果支持了全面自动化ReviewScore评估的潜力，尤其是在前提层面评估能取得更高的一致性。

Abstract: Peer review serves as a backbone of academic research, but in most AI
conferences, the review quality is degrading as the number of submissions
explodes. To reliably detect low-quality reviews, we define misinformed review
points as either "weaknesses" in a review that contain incorrect premises, or
"questions" in a review that can be already answered by the paper. We verify
that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce
ReviewScore indicating if a review point is misinformed. To evaluate the
factuality of each premise of weaknesses, we propose an automated engine that
reconstructs every explicit and implicit premise from a weakness. We build a
human expert-annotated ReviewScore dataset to check the ability of LLMs to
automate ReviewScore evaluation. Then, we measure human-model agreements on
ReviewScore using eight current state-of-the-art LLMs and verify moderate
agreements. We also prove that evaluating premise-level factuality shows
significantly higher agreements than evaluating weakness-level factuality. A
thorough disagreement analysis further supports a potential of fully automated
ReviewScore evaluation.

</details>


### [22] [GRAB: A Risk Taxonomy--Grounded Benchmark for Unsupervised Topic Discovery in Financial Disclosures](https://arxiv.org/abs/2509.21698)
*Ying Li,Tiejun Ma*

Main category: cs.CL

TL;DR: 本文提出了GRAB，一个金融领域专用的基准数据集，用于评估10-K风险披露中的无监督主题模型，其标签无需人工标注自动生成，并提供了标准化的评估框架。


<details>
  <summary>Details</summary>
Motivation: 10-K风险披露的风险分类对监管和投资具有重要意义，然而目前尚无公开基准来评估用于此任务的无监督主题模型。

Method: 开发了GRAB基准，包含来自8,247份申报文件的161万个句子。通过结合FinBERT注意力机制、YAKE关键词信号和分类法感知的搭配匹配，无需人工标注即可自动生成跨度接地（span-grounded）的句子标签。标签基于一个将193个术语映射到21个细粒度类型（隶属于5个宏观类别）的风险分类法。GRAB通过固定数据集划分和准确率、宏观F1、主题BERTScore以及基于熵的有效主题数等鲁棒性指标统一了评估。

Result: 成功构建了GRAB基准数据集，提供了大量金融文本、自动生成的、基于风险分类法的标签以及统一的评估框架。这使得在金融披露任务上，对经典、嵌入式、神经和混合等各类主题模型进行可复现、标准化的比较成为可能。

Conclusion: GRAB是首个公开的、金融领域专用的无监督主题模型评估基准，填补了10-K风险分类领域的空白，为该领域的研究提供了一个标准化和可复现的比较平台。

Abstract: Risk categorization in 10-K risk disclosures matters for oversight and
investment, yet no public benchmark evaluates unsupervised topic models for
this task. We present GRAB, a finance-specific benchmark with 1.61M sentences
from 8,247 filings and span-grounded sentence labels produced without manual
annotation by combining FinBERT token attention, YAKE keyphrase signals, and
taxonomy-aware collocation matching. Labels are anchored in a risk taxonomy
mapping 193 terms to 21 fine-grained types nested under five macro classes; the
21 types guide weak supervision, while evaluation is reported at the macro
level. GRAB unifies evaluation with fixed dataset splits and robust
metrics--Accuracy, Macro-F1, Topic BERTScore, and the entropy-based Effective
Number of Topics. The dataset, labels, and code enable reproducible,
standardized comparison across classical, embedding-based, neural, and hybrid
topic models on financial disclosures.

</details>


### [23] [Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval](https://arxiv.org/abs/2509.21710)
*Xiaojun Wu,Cehao Yang,Xueyuan Lin,Chengjin Xu,Xuhui Jiang,Yuanliang Sun,Hui Xiong,Jia Li,Jian Guo*

Main category: cs.CL

TL;DR: 本文提出Think-on-Graph 3.0 (ToG-3)，一个通过多智能体上下文演化和检索（MACER）机制驱动的RAG框架。它通过动态构建和细化异构图索引，并引入查询和子图的双重演化，克服了传统图基RAG静态索引的局限性，即使对小型LLM也能实现深层精确推理。


<details>
  <summary>Details</summary>
Motivation: 现有图基RAG方法存在根本性权衡：手动构建知识图谱成本高昂难以扩展；自动提取图受限于底层LLM提取器的性能，尤其对小型LLM；且通常构建静态图索引，无法适应实际查询。

Method: ToG-3框架引入了多智能体上下文演化和检索（MACER）机制，其核心创新是动态构建和细化“块-三元组-社区”异构图索引。该机制开创性地整合了“演化查询”和“演化子图”的双重演化，以实现精确的证据检索。一个由“构建者”、“检索者”、“反思者”和“响应者”组成的多智能体系统迭代协作，进行证据检索、答案生成、充分性反思，并动态演化查询和子图。

Result: ToG-3在深度和广度推理基准测试中均优于对比基线方法。消融研究证实了MACER框架中各组件的有效性。

Conclusion: ToG-3通过其动态图索引构建和双重演化机制，有效克服了现有图基RAG方法静态图索引的固有缺陷，实现了在推理过程中自适应构建目标图索引，即使使用轻量级LLM也能支持深层、精确的推理。

Abstract: Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the
important paradigm for enhancing Large Language Models (LLMs) with external
knowledge. However, existing approaches face a fundamental trade-off. While
graph-based methods are inherently dependent on high-quality graph structures,
they face significant practical constraints: manually constructed knowledge
graphs are prohibitively expensive to scale, while automatically extracted
graphs from corpora are limited by the performance of the underlying LLM
extractors, especially when using smaller, local-deployed models. This paper
presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces
Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these
limitations. Our core innovation is the dynamic construction and refinement of
a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly
incorporates a dual-evolution mechanism of Evolving Query and Evolving
Sub-Graph for precise evidence retrieval. This approach addresses a critical
limitation of prior Graph-based RAG methods, which typically construct a static
graph index in a single pass without adapting to the actual query. A
multi-agent system, comprising Constructor, Retriever, Reflector, and Responser
agents, collaboratively engages in an iterative process of evidence retrieval,
answer generation, sufficiency reflection, and, crucially, evolving query and
subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively
build a targeted graph index during reasoning, mitigating the inherent
drawbacks of static, one-time graph construction and enabling deep, precise
reasoning even with lightweight LLMs. Extensive experiments demonstrate that
ToG-3 outperforms compared baselines on both deep and broad reasoning
benchmarks, and ablation studies confirm the efficacy of the components of
MACER framework.

</details>


### [24] [ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation](https://arxiv.org/abs/2509.21730)
*Jiho Kim,Junseong Choi,Woosog Chay,Daeun Kyung,Yeonsu Kwon,Yohan Jo,Edward Choi*

Main category: cs.CL

TL;DR: 本文提出ProPerSim模拟框架及ProPerAssistant助手模型，旨在实现主动且个性化的AI推荐，并通过用户反馈持续学习，实验证明其能有效提升用户满意度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的普及，市场对不仅响应式，而且主动且个性化的AI助手需求日益增长。尽管主动性和个性化在各自领域有所进展，但两者的结合仍未得到充分探索。

Method: 1. 引入ProPerSim，一个用于开发能在现实家庭场景中进行及时、个性化推荐的助手的新任务和模拟框架。在该框架中，具有丰富人设的用户代理与助手交互，并对建议进行评分，助手通过评分学习和适应。
2. 基于ProPerSim，提出ProPerAssistant，一个通过用户反馈持续学习和适应的检索增强、偏好对齐助手。

Result: 对32个不同人设进行的实验表明，ProPerAssistant能够调整其策略并稳步提高用户满意度。

Conclusion: 将主动性与个性化相结合在AI助手领域具有广阔前景。

Abstract: As large language models (LLMs) become increasingly integrated into daily
life, there is growing demand for AI assistants that are not only reactive but
also proactive and personalized. While recent advances have pushed forward
proactivity and personalization individually, their combination remains
underexplored. To bridge this gap, we introduce ProPerSim, a new task and
simulation framework for developing assistants capable of making timely,
personalized recommendations in realistic home scenarios. In our simulation
environment, a user agent with a rich persona interacts with the assistant,
providing ratings on how well each suggestion aligns with its preferences and
context. The assistant's goal is to use these ratings to learn and adapt to
achieve higher scores over time. Built on ProPerSim, we propose
ProPerAssistant, a retrieval-augmented, preference-aligned assistant that
continually learns and adapts through user feedback. Experiments across 32
diverse personas show that ProPerAssistant adapts its strategy and steadily
improves user satisfaction, highlighting the promise of uniting proactivity and
personalization.

</details>


### [25] [How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?](https://arxiv.org/abs/2509.21732)
*Xiliang Zhu,Shi Zong,David Rossouw*

Main category: cs.CL

TL;DR: 评估LLM在长上下文多问题问答中的表现，发现经微调的公共LLM在准确性上可超越GPT-4o，实现更经济透明的部署。


<details>
  <summary>Details</summary>
Motivation: 在工业环境中，LLM基于长上下文的问答（特别是针对同一上下文回答多个问题）面临高计算成本和高延迟的挑战。

Method: 通过广泛的实验，对一系列专有和公共LLM在同一对话上下文下回答多个问题的能力进行了基准测试和评估。

Result: 强大的专有LLM（如GPT-4o）在整体性能上表现最佳；然而，参数量高达80亿的微调公共LLM在准确性上能够超越GPT-4o。

Conclusion: 微调的公共LLM在真实世界应用中具有透明且经济高效的部署潜力。

Abstract: Deploying Large Language Models (LLMs) for question answering (QA) over
lengthy contexts is a significant challenge. In industrial settings, this
process is often hindered by high computational costs and latency, especially
when multiple questions must be answered based on the same context. In this
work, we explore the capabilities of LLMs to answer multiple questions based on
the same conversational context. We conduct extensive experiments and benchmark
a range of both proprietary and public models on this challenging task. Our
findings highlight that while strong proprietary LLMs like GPT-4o achieve the
best overall performance, fine-tuned public LLMs with up to 8 billion
parameters can surpass GPT-4o in accuracy, which demonstrates their potential
for transparent and cost-effective deployment in real-world applications.

</details>


### [26] [Self-Speculative Biased Decoding for Faster Live Translation](https://arxiv.org/abs/2509.21740)
*Linxiao Zeng,Haoyun Deng,Kangyuan Shu,Shizhen Wang*

Main category: cs.CL

TL;DR: 为解决LLM在实时流应用中面临的延迟和更新挑战，本文提出了“自推测偏置解码”方法，通过重用之前输出作为草稿并偏置验证，实现了高达1.7倍的速度提升和80%的闪烁减少，且不损失质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在各种文本生成任务中表现出色，但在实时流应用（如实时翻译）中，面临输出需随输入上下文扩展而持续更新、同时保持合理计算成本以满足低延迟要求的挑战。

Method: 本文重新审视了同声传译的重翻译方法，并提出“自推测偏置解码”（Self-Speculative Biased Decoding）。该方法将最近的输出作为当前不断增长输入上下文的草稿，并在验证阶段偏向草稿标记以提高接受率，从而避免重复从头生成输出。与现有推测解码策略不同，此方法无需草稿计算，是模型无关的即插即用解决方案。此外，还结合了display-only mask-k技术以进一步减少闪烁。

Result: 在同声文本到文本重翻译实验中，本文方法实现了高达1.7倍的速度提升，同时不影响质量。通过结合display-only mask-k技术，显著减少了80%的闪烁现象。

Conclusion: “自推测偏置解码”提供了一种有效且模型无关的解决方案，可加速LLM在延迟敏感型流应用中的推理，在提高处理速度的同时，显著改善了用户体验中的闪烁问题，且不牺牲输出质量。

Abstract: Large Language Models (LLMs) have recently demonstrated impressive
capabilities in various text generation tasks. However, it remains challenging
to use them off-the-shelf in streaming applications (such as live translation),
where the output must continually update as the input context expands, while
still maintaining a reasonable computational cost to meet the latency
requirement.
  In this work, we reexamine the re-translation approach to simultaneous
translation and propose Self-Speculative Biased Decoding, a novel inference
paradigm designed to avoid repeatedly generating output from scratch for a
consistently growing input stream. We propose using the most recent output as a
draft for the current growing input context. During the verification stage, the
output will be biased towards the draft token for a higher draft acceptance
rate. This strategy not only minimizes flickering that might distract users but
also leads to higher speedups. Conventional decoding may take charge from the
point of divergence after draft verification and continue until the end
condition is met.
  Unlike existing speculative decoding strategies, our approach eliminates the
need for draft computations, making it a model-agnostic and plug-and-play
solution for accelerating latency-sensitive streaming applications.
Experimental results on simultaneous text-to-text re-translation demonstrate
that our approach achieves up to 1.7x speedup compared to conventional
auto-regressive re-translation without compromising quality. Additionally, it
significantly reduces flickering by 80% by incorporating the display-only
mask-k technique.

</details>


### [27] [Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2509.21749)
*Zhen Xiong,Yujun Cai,Zhecheng Li,Junsong Yuan,Yiwei Wang*

Main category: cs.CL

TL;DR: 本文提出了Thinking-with-Sound (TwS)框架，通过结合语言推理和即时音频分析，为大型音频-语言模型（LALMs）配备音频CoT，以解决其在复杂声学场景中音频推理任务的局限性，并显著提高了模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LALMs在复杂声学场景中的挑战性音频推理任务上表现出显著局限性，因其缺乏噪声抑制、声源分离和精确时间对齐等声学工具，无法有效处理复杂音频输入。

Method: 引入Thinking-with-Sound (TwS)框架，该框架通过结合语言推理与即时音频域分析，赋予LALMs音频思维链（Audio CoT）能力，使其能主动进行音频信号的数值分析和数字操作。同时，构建了MELD-Hard1k新基准来评估模型鲁棒性。

Result: 在MELD-Hard1k基准测试中，SOTA LALMs的准确率比干净音频下降超过50%。TwS显著提升了鲁棒性：小型模型绝对准确率提升24.73%，大型模型提升高达36.61%。

Conclusion: 研究表明，Audio CoT可以显著增强LALMs的鲁棒性而无需重新训练，为开发更稳健的音频理解系统开辟了新方向。

Abstract: Recent Large Audio-Language Models (LALMs) have shown strong performance on
various audio understanding tasks such as speech translation and Audio Q\&A.
However, they exhibit significant limitations on challenging audio reasoning
tasks in complex acoustic scenarios. These situations would greatly benefit
from the use of acoustic tools like noise suppression, source separation, and
precise temporal alignment, but current LALMs lack access to such tools. To
address this limitation, we introduce Thinking-with-Sound (TwS), a framework
that equips LALMs with Audio CoT by combining linguistic reasoning with
on-the-fly audio-domain analysis. Unlike existing approaches that treat audio
as static input, TwS enables models to actively think with audio signals,
performing numerical analysis and digital manipulation through multimodal
reasoning. To evaluate this approach, we construct MELD-Hard1k, a new
robustness benchmark created by introducing various acoustic perturbations.
Experiments reveal that state-of-the-art LALMs suffer dramatic performance
degradation on MELD-Hard1k, with accuracy dropping by more than $50\%$ compared
to clean audio. TwS achieves substantial improvements in robustness,
demonstrating both effectiveness and scalability: small models gain $24.73\%$
absolute accuracy, with improvements scaling consistently up to $36.61\%$ for
larger models. Our findings demonstrate that Audio CoT can significantly
enhance robustness without retraining, opening new directions for developing
more robust audio understanding systems.

</details>


### [28] [SynerGen: Contextualized Generative Recommender for Unified Search and Recommendation](https://arxiv.org/abs/2509.21777)
*Vianne R. Gao,Chen Xue,Marc Versage,Xie Zhou,Zhongruo Wang,Chao Li,Yeon Seonwoo,Nan Chen,Zhen Ge,Gourab Kundu,Weiqi Zhang,Tian Wang,Qingjun Cui,Trishul Chilimbi*

Main category: cs.CL

TL;DR: 本文提出SynerGen，一个新颖的生成式推荐模型，通过单一生成骨干网络统一个性化搜索和推荐，并在检索和排序任务上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有大规模推荐系统中的“检索-排序”管道因架构分离和目标不一致导致校准失准和工程开销。尽管生成式序列模型有望统一检索和排序，但现有方案通常仅关注个性化搜索或无查询推荐，难以有效统一两者并存在性能折衷。

Method: SynerGen是一个基于行为序列训练的解码器Transformer模型。它采用联合优化策略，使用InfoNCE进行检索优化，并结合混合点对点/对对损失进行排序优化。模型还引入了新型时间感知旋转位置编码，以有效整合时间信息到注意力机制中。

Result: SynerGen在广泛采用的推荐和搜索基准测试中，相较于强大的生成式推荐和联合搜索推荐基线模型，实现了显著的性能提升。

Conclusion: 这项工作证明了单一生成式基础模型在工业级统一信息访问方面是可行的。

Abstract: The dominant retrieve-then-rank pipeline in large-scale recommender systems
suffers from mis-calibration and engineering overhead due to its architectural
split and differing optimization objectives. While recent generative sequence
models have shown promise in unifying retrieval and ranking by
auto-regressively generating ranked items, existing solutions typically address
either personalized search or query-free recommendation, often exhibiting
performance trade-offs when attempting to unify both. We introduce
\textit{SynerGen}, a novel generative recommender model that bridges this
critical gap by providing a single generative backbone for both personalized
search and recommendation, while simultaneously excelling at retrieval and
ranking tasks. Trained on behavioral sequences, our decoder-only Transformer
leverages joint optimization with InfoNCE for retrieval and a hybrid
pointwise-pairwise loss for ranking, allowing semantic signals from search to
improve recommendation and vice versa. We also propose a novel time-aware
rotary positional embedding to effectively incorporate time information into
the attention mechanism. \textit{SynerGen} achieves significant improvements on
widely adopted recommendation and search benchmarks compared to strong
generative recommender and joint search and recommendation baselines. This work
demonstrates the viability of a single generative foundation model for
industrial-scale unified information access.

</details>


### [29] [Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference](https://arxiv.org/abs/2509.21791)
*Han Yuan,Yue Zhao,Li Zhang,Wuqiong Luo,Zheng Ma*

Main category: cs.CL

TL;DR: 本文利用因果推断方法，对大型语言模型结构化输出对其生成质量的影响进行了精细分析，发现其在多数情况下并无直接因果影响，解决了现有研究的矛盾。22


<details>
  <summary>Details</summary>
Motivation: 现有研究关于大型语言模型结构化输出对生成质量的影响存在矛盾结论（积极或消极），且这些研究存在测试场景受限、对比设置弱控制和依赖粗略指标等局限性。本研究旨在通过更精细的分析解决这些冲突和局限。

Method: 本研究采用因果推断方法进行分析。基于一个假设和两个确定的约束，推导了五种潜在的因果结构。实验在七个公开和一个自研的推理任务上进行，并使用GPT-4o作为LLM。

Result: 粗略指标显示结构化输出对GPT-4o的生成有积极、消极或中性的影响。然而，因果推断结果表明，在48个场景中的43个场景中，结构化输出没有因果影响。在剩余的5个场景中，有3个涉及受具体指令影响的复杂因果结构。

Conclusion: 尽管粗略评估可能显示出多种效应，但通过因果推断发现，结构化输出在绝大多数情况下对大型语言模型的生成质量没有直接的因果影响，具体的指令可能会引入更复杂的因果结构。

Abstract: Structured output from large language models (LLMs) has enhanced efficiency
in processing generated information and is increasingly adopted in industrial
applications. Prior studies have investigated the impact of structured output
on LLMs' generation quality, often presenting one-way findings. Some suggest
that structured format enhances completeness and factual accuracy, while others
argue that it restricts the reasoning capacity of LLMs and leads to reductions
in standard evaluation metrics. Potential limitations of these assessments
include restricted testing scenarios, weakly controlled comparative settings,
and reliance on coarse metrics. In this work, we present a refined analysis
using causal inference. Based on one assumed and two guaranteed constraints, we
derive five potential causal structures characterizing the influence of
structured output on LLMs' generation: (1) collider without m-bias, (2)
collider with m-bias, (3) single cause from instruction, (4) single cause from
output format, and (5) independence. Across seven public and one developed
reasoning tasks, we find that coarse metrics report positive, negative, or
neutral effects of structured output on GPT-4o's generation. However, causal
inference reveals no causal impact in 43 out of 48 scenarios. In the remaining
5, 3 involve multifaceted causal structures influenced by concrete
instructions.

</details>


### [30] [Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment](https://arxiv.org/abs/2509.21798)
*Hongbin Zhang,Kehai Chen,Xuefeng Bai,Yang Xiang,Min Zhang*

Main category: cs.CL

TL;DR: 为解决大语言模型(LLMs)文化对齐中奖励模型(RMs)文化意识评估不足的问题，本文提出CARB基准测试集，揭示了现有RMs的缺陷和对表面特征的依赖。为提升RMs的文化理解，作者进一步提出了“Think-as-Locals”方法（基于RLVR），并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 奖励模型(RMs)对实现大语言模型(LLMs)的文化对齐至关重要，因此评估RMs的文化意识是推进LLMs全球对齐的关键。然而，现有RM评估方法因缺乏文化相关数据集，无法有效评估RMs的文化意识。

Method: ['构建“文化意识奖励建模基准”(CARB)，涵盖10种不同文化和4个文化领域，用于评估RMs。', '提出“Think-as-Locals”方法，通过可验证奖励强化学习(RLVR)和精心设计的奖励，从生成式RMs中引出更深层次的文化推理，以确保准确的偏好判断和高质量结构化评估标准生成。']

Result: ['对现有RMs的广泛评估揭示了其在文化意识建模方面的不足。', 'CARB上的表现与下游多语言文化对齐任务呈正相关。', '分析发现，RMs在文化意识奖励建模中存在虚假关联，其评分主要依赖表面特征而非真正的文化细微理解。', '“Think-as-Locals”方法能有效缓解虚假特征干扰，并推进文化意识奖励建模。']

Conclusion: 现有奖励模型在文化理解上存在不足，通常依赖表面特征。本文提出的CARB基准能有效评估这些缺陷，而“Think-as-Locals”方法（结合RLVR）则能显著提升奖励模型的文化意识和深度理解能力，从而促进更可靠的LLM文化对齐。

Abstract: Reward models (RMs) are crucial for aligning large language models (LLMs)
with diverse cultures. Consequently, evaluating their cultural awareness is
essential for further advancing global alignment of LLMs. However, existing RM
evaluations fall short in assessing cultural awareness due to the scarcity of
culturally relevant evaluation datasets. To fill this gap, we propose Cultural
Awareness Reward modeling Benchmark (CARB), covering 10 distinct cultures
across 4 cultural domains. Our extensive evaluation of state-of-the-art RMs
reveals their deficiencies in modeling cultural awareness and demonstrates a
positive correlation between performance on CARB and downstream multilingual
cultural alignment tasks. Further analysis identifies the spurious correlations
within culture-aware reward modeling, wherein RM's scoring relies predominantly
on surface-level features rather than authentic cultural nuance understanding.
To address these, we propose Think-as-Locals to elicit deeper culturally
grounded reasoning from generative RMs via reinforcement learning from
verifiable rewards (RLVR) and employ well-designed rewards to ensure accurate
preference judgments and high-quality structured evaluation criteria
generation. Experimental results validate its efficacy in mitigating spurious
features interference and advancing culture-aware reward modeling.

</details>


### [31] [Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies](https://arxiv.org/abs/2509.21801)
*Qianen Zhang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 本文通过引入四种自适应动作（SENTENCE_CUT, DROP等）扩展了LLM-based同声传译的动作空间，并在ACL60/60基准上显著提升了翻译质量并降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 传统同声传译（SiMT）仅依赖READ/WRITE动作，难以在严格的实时约束下同时满足高质量翻译的需求。

Method: 将SiMT的动作空间扩展为四种自适应动作：SENTENCE_CUT（句子切割）、DROP（删除）、PARTIAL_SUMMARIZATION（部分摘要）和PRONOMINALIZATION（代词化）。这些动作支持实时重构、省略和简化，同时保持语义忠实度。在仅解码器的大型语言模型（LLM）框架中实现这些动作，并通过动作感知提示（action-aware prompting）构建训练参考。为评估质量和延迟，开发了延迟感知的文本转语音（TTS）管道，以真实的时间映射文本输出到语音。

Result: 在ACL60/60英中和英德基准测试中，该框架持续改进了语义指标（如COMET-KIWI），并相较于参考翻译和基于“salami”的基线，实现了更低的延迟（通过平均滞后测量）。值得注意的是，结合DROP和SENTENCE_CUT在流畅性和延迟之间取得了最佳的总体平衡。

Conclusion: 丰富基于LLM的SiMT动作空间为弥合人机翻译差距提供了一个有前景的方向。

Abstract: Simultaneous Machine Translation (SiMT) requires high-quality translations
under strict real-time constraints, which traditional encoder-decoder policies
with only READ/WRITE actions cannot fully address. We extend the action space
of SiMT with four adaptive actions: SENTENCE_CUT, DROP, PARTIAL_SUMMARIZATION
and PRONOMINALIZATION, which enable real-time restructuring, omission, and
simplification while preserving semantic fidelity. We implement these actions
in a decoder-only large language model (LLM) framework and construct training
references through action-aware prompting. To evaluate both quality and
latency, we further develop a latency-aware TTS pipeline that maps textual
outputs to speech with realistic timing. Experiments on the ACL60/60
English-Chinese and English-German benchmarks show that our framework
consistently improves semantic metrics (e.g., COMET-KIWI) and achieves lower
delay (measured by Average Lagging) compared to reference translations and
salami-based baselines. Notably, combining DROP and SENTENCE_CUT yields the
best overall balance between fluency and latency. These results demonstrate
that enriching the action space of LLM-based SiMT provides a promising
direction for bridging the gap between human and machine interpretation.

</details>


### [32] [Towards Minimal Causal Representations for Human Multimodal Language Understanding](https://arxiv.org/abs/2509.21805)
*Menghua Jiang,Yuncheng Jiang,Haifeng Hu,Sijie Mai*

Main category: cs.CL

TL;DR: 现有MLU模型易受数据集偏差影响导致OOD泛化差。本文提出CaMIB模型，通过信息瓶颈过滤噪声，解耦因果与捷径特征，并利用因果推断技术提升模型在多模态理解任务中的OOD泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的人类多模态语言理解（MLU）方法过度依赖“学习关注”范式，易受数据集偏差影响，将统计捷径误认为是真实因果特征，导致模型在分布外（OOD）泛化能力下降。

Method: 引入因果多模态信息瓶颈（CaMIB）模型。该模型首先应用信息瓶颈过滤单模态输入噪声；然后，通过参数化掩码生成器将融合的多模态表示解耦为因果和捷径子表示；最后，结合工具变量约束确保因果特征的全局一致性，并采用后门调整通过随机重组因果和捷径特征来稳定因果估计。

Result: 在多模态情感分析、幽默检测和讽刺检测任务以及OOD测试集上的广泛实验表明CaMIB模型具有有效性。理论和实证分析进一步突出了其可解释性和合理性。

Conclusion: CaMIB模型通过引入因果原则而非传统似然方法，有效缓解了MLU模型中数据集偏差导致的OOD泛化问题，成功识别并利用了真实的因果特征，从而提升了多模态理解任务的性能和模型的稳健性与可解释性。

Abstract: Human Multimodal Language Understanding (MLU) aims to infer human intentions
by integrating related cues from heterogeneous modalities. Existing works
predominantly follow a ``learning to attend" paradigm, which maximizes mutual
information between data and labels to enhance predictive performance. However,
such methods are vulnerable to unintended dataset biases, causing models to
conflate statistical shortcuts with genuine causal features and resulting in
degraded out-of-distribution (OOD) generalization. To alleviate this issue, we
introduce a Causal Multimodal Information Bottleneck (CaMIB) model that
leverages causal principles rather than traditional likelihood. Concretely, we
first applies the information bottleneck to filter unimodal inputs, removing
task-irrelevant noise. A parameterized mask generator then disentangles the
fused multimodal representation into causal and shortcut subrepresentations. To
ensure global consistency of causal features, we incorporate an instrumental
variable constraint, and further adopt backdoor adjustment by randomly
recombining causal and shortcut features to stabilize causal estimation.
Extensive experiments on multimodal sentiment analysis, humor detection, and
sarcasm detection, along with OOD test sets, demonstrate the effectiveness of
CaMIB. Theoretical and empirical analyses further highlight its
interpretability and soundness.

</details>


### [33] [Can LLMs Solve and Generate Linguistic Olympiad Puzzles?](https://arxiv.org/abs/2509.21820)
*Neh Majmudar,Elena Filatova*

Main category: cs.CL

TL;DR: 本文探讨了语言学谜题的解决与生成，利用大型语言模型（LLMs）解决语言学奥林匹克谜题，发现LLMs在多数类型上超越人类，并利用这些经验探索自动化谜题生成，旨在推广语言学和稀有语言知识。


<details>
  <summary>Details</summary>
Motivation: 解决和生成语言学谜题，特别是为了拓展语言学兴趣，向更广泛受众介绍该领域，并支持稀有和研究不足语言知识的传播。

Method: 扩展了解决语言学谜题的现有基准。使用大型语言模型（LLMs，包括如OpenAI的o1等先进模型）解决语言学谜题，并分析其在不同语言学主题上的表现。利用谜题解决实验的经验指导语言学谜题的生成任务。

Result: 大型语言模型在大多数语言学谜题类型上表现优于人类，但在涉及书写系统和研究不足语言的谜题上除外。

Conclusion: 自动化语言学谜题生成对于扩大语言学兴趣、向更广泛受众介绍该领域，以及促进稀有和研究不足语言知识的传播具有重要意义，因此谜题生成是一个重要的研究任务。

Abstract: In this paper, we introduce a combination of novel and exciting tasks: the
solution and generation of linguistic puzzles. We focus on puzzles used in
Linguistic Olympiads for high school students. We first extend the existing
benchmark for the task of solving linguistic puzzles. We explore the use of
Large Language Models (LLMs), including recent state-of-the-art models such as
OpenAI's o1, for solving linguistic puzzles, analyzing their performance across
various linguistic topics. We demonstrate that LLMs outperform humans on most
puzzles types, except for those centered on writing systems, and for the
understudied languages. We use the insights from puzzle-solving experiments to
direct the novel task of puzzle generation. We believe that automating puzzle
generation, even for relatively simple puzzles, holds promise for expanding
interest in linguistics and introducing the field to a broader audience. This
finding highlights the importance of linguistic puzzle generation as a research
task: such puzzles can not only promote linguistics but also support the
dissemination of knowledge about rare and understudied languages.

</details>


### [34] [ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models](https://arxiv.org/abs/2509.21826)
*Zihan Lin,Xiaohan Wang,Jie Cao,Jiajun Chai,Guojun Yin,Wei Lin,Ran He*

Main category: cs.CL

TL;DR: 针对LLM工具使用中RL训练效率低的问题，本文提出ResT方法，通过熵感知的token重加权优化策略梯度，实现了SOTA性能并超越GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有RL范式在优化LLM工具使用策略时，依赖稀疏结果奖励且未考虑任务特殊性，导致策略梯度方差大、训练效率低下。

Method: 1. 理论上揭示了策略熵与工具使用任务训练稳定性之间的联系，发现结构化、低熵token是奖励的主要决定因素。2. 提出ResT（Reshaped Token-level policy gradients）方法，通过熵感知的token重加权重塑策略梯度。3. ResT在训练中逐步提高推理token权重，实现从结构正确性到语义推理的平滑过渡，并稳定多轮工具使用任务的收敛。

Result: 1. 在BFCL和API-Bank数据集上达到SOTA，比现有方法性能提升高达8.76%。2. 在4B基础LLM上微调后，ResT在单轮任务上超越GPT-4o 4.11%，在多轮基础任务上超越1.50%。

Conclusion: ResT通过对策略梯度进行熵感知的token重加权，有效解决了LLM工具使用任务中RL训练效率低的问题，显著提高了性能和训练稳定性，取得了先进的工具使用能力。

Abstract: Large language models (LLMs) transcend passive generation and act as
goal-directed agents by invoking external tools. Reinforcement learning (RL)
offers a principled framework for optimizing these emergent tool-use policies,
yet the prevailing paradigm relies exclusively on sparse outcome rewards and
lacks consideration of the particularity of tool-use tasks, inflating
policy-gradient variance and resulting in inefficient training. To better
understand and address these challenges, we first establish a theoretical link
between policy entropy and training stability of tool-use tasks, which reveals
that structured, low-entropy tokens are primary determinants of rewards.
Motivated by this insight, we propose \textbf{Res}haped \textbf{T}oken-level
policy gradients (\textbf{ResT}) for tool-use tasks. ResT reshapes the policy
gradient through entropy-informed token reweighting, progressively upweighting
reasoning tokens as training proceeds. This entropy-aware scheme enables a
smooth shift from structural correctness to semantic reasoning and stabilizes
convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows
that ResT achieves state-of-the-art results, outperforming prior methods by up
to $8.76\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by
$4.11\%$ on single-turn tasks and $1.50\%$ on multi-turn base tasks.

</details>


### [35] [Semantic Agreement Enables Efficient Open-Ended LLM Cascades](https://arxiv.org/abs/2509.21837)
*Duncan Soiffer,Steven Kolawole,Virginia Smith*

Main category: cs.CL

TL;DR: LLM级联系统通过语义一致性信号，实现了在开放式生成中可靠的委托决策，显著降低成本和延迟，且易于实际部署。


<details>
  <summary>Details</summary>
Motivation: LLM级联系统在开放式文本生成中，难以确定何时可靠地委托给更大的模型，因为生成质量评估复杂，且存在多重有效响应，从而影响成本与质量的平衡。

Method: 提出并利用“语义一致性”（即集成模型输出在意义层面的共识）作为一种免训练的信号，以判断是否进行可靠的委托。

Result: 语义级联系统在匹配或超越目标模型质量的同时，能将成本降低40%，延迟缩短60%。语义一致性被证实是比基于token的置信度更强的可靠性信号。该方法适用于黑盒API，且对模型更新具有鲁棒性。

Conclusion: 语义一致性提供了一个实用且稳健的可靠委托信号，使LLM级联系统能在大幅降低成本和延迟的同时保持高质量，是实际部署中极具潜力的基线方法。

Abstract: Cascade systems route computational requests to smaller models when possible
and defer to larger models only when necessary, offering a promising approach
to balance cost and quality in LLM deployment. However, they face a fundamental
challenge in open-ended text generation: determining output reliability when
generation quality lies on a continuous spectrum, often with multiple valid
responses. To address this, we propose semantic agreement -- meaning-level
consensus between ensemble outputs -- as a training-free signal for reliable
deferral. We show that when diverse model outputs agree semantically, their
consensus is a stronger reliability signal than token-level confidence.
Evaluated from 500M to 70B-parameter models, we find that semantic cascades
match or surpass target-model quality at 40% of the cost and reduce latency by
up to 60%. Our method requires no model internals, works across black-box APIs,
and remains robust to model updates, making it a practical baseline for
real-world LLM deployment.

</details>


### [36] [Following the TRACE: A Structured Path to Empathetic Response Generation with Multi-Agent Models](https://arxiv.org/abs/2509.21849)
*Ziqi Liu,Ziyang Zhou,Yilin Li,Haiyang Zhang,Yangbin Chen*

Main category: cs.CL

TL;DR: TRACE框架通过任务分解将共情响应生成建模为结构化认知过程，有效弥合专业模型分析深度和LLM生成流畅度之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有共情响应生成方法在专业模型分析深度和大型语言模型（LLMs）生成流畅度之间存在核心权衡，难以兼顾。

Method: 提出TRACE框架（Task-decomposed Reasoning for Affective Communication and Empathy），将共情建模为结构化认知过程。该框架通过任务分解将生成过程划分为分析和合成的流水线，先进行深度理解再生成，以结合深度分析和表达性生成。

Result: TRACE框架在自动评估和基于LLM的评估中均显著优于强基线模型。

Conclusion: 结构化的任务分解是创建更强大、更可解释的共情代理的一种有前景的范式。

Abstract: Empathetic response generation is a crucial task for creating more human-like
and supportive conversational agents. However, existing methods face a core
trade-off between the analytical depth of specialized models and the generative
fluency of Large Language Models (LLMs). To address this, we propose TRACE,
Task-decomposed Reasoning for Affective Communication and Empathy, a novel
framework that models empathy as a structured cognitive process by decomposing
the task into a pipeline for analysis and synthesis. By building a
comprehensive understanding before generation, TRACE unites deep analysis with
expressive generation. Experimental results show that our framework
significantly outperforms strong baselines in both automatic and LLM-based
evaluations, confirming that our structured decomposition is a promising
paradigm for creating more capable and interpretable empathetic agents. Our
code is available at https://anonymous.4open.science/r/TRACE-18EF/README.md.

</details>


### [37] [KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues](https://arxiv.org/abs/2509.21856)
*Junhao Chen,Yu Huang,Siyuan Li,Rui Yao,Hanqian Li,Hanyu Zhang,Jungang Li,Jian Chen,Bowen Wang,Xuming Hu*

Main category: cs.CL

TL;DR: 本文提出了KnowMT-Bench，首个旨在系统评估LLMs在多轮长篇知识密集型问答（MT-LFQA）能力上的基准。研究发现多轮上下文会降低模型的性能，但RAG能有效缓解事实性下降。


<details>
  <summary>Details</summary>
Motivation: 现有MT-LFQA基准仅限于单轮对话，或评估非知识密集型的事实性能力，无法系统性评估LLMs在知识密集型领域的多轮长篇问答能力，存在关键空白。

Method: 引入KnowMT-Bench，涵盖医学、金融、法律等知识密集型领域。采用动态评估设置，模型根据逻辑渐进的问题序列生成自己的多轮对话历史。使用人工验证的自动化流程评估最终轮回答的事实能力和信息传递效率。

Result: 实验表明，多轮上下文会降低性能：由于自生成历史中的上下文噪声，事实能力下降；随着对话长度增加，模型变得冗长，信息效率降低。研究发现检索增强生成（RAG）能有效缓解甚至逆转事实性下降。

Conclusion: KnowMT-Bench基准对评估和提升LLMs在真实世界知识密集型应用中的会话事实能力至关重要。研究结果强调了多轮上下文带来的挑战以及RAG作为缓解策略的有效性。

Abstract: Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application
paradigm of Large Language Models (LLMs) in knowledge-intensive domains.
However, existing benchmarks are limited to single-turn dialogue, while
multi-turn dialogue benchmarks typically assess other orthogonal capabilities
rather than knowledge-intensive factuality. To bridge this critical gap, we
introduce \textbf{KnowMT-Bench}, the \textit{first-ever} benchmark designed to
systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields,
including medicine, finance, and law. To faithfully assess the model's
real-world performance, KnowMT-Bench employs a dynamic evaluation setting where
models generate their own multi-turn dialogue histories given logically
progressive question sequences. The factual capability and information delivery
efficiency of the \textit{final-turn} answer are then evaluated using a
human-validated automated pipeline. Our experiments reveal that multi-turn
contexts degrade performance: factual capability declines due to the contextual
noise from self-generated histories, while information efficiency drops as
models become more verbose with increasing dialogue length. We then investigate
mitigation strategies, demonstrating that retrieval-augmented generation (RAG)
can effectively alleviate and even reverse this factual degradation. These
findings underscore the importance of our benchmark in evaluating and enhancing
the conversational factual capabilities of LLMs in real-world
knowledge-intensive applications. Code is available at
\href{https://github.com/hardenyu21/KnowMT-Bench}{\textcolor{cyan}{\texttt{KnowMT-Bench}}}.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [38] [Random Direct Preference Optimization for Radiography Report Generation](https://arxiv.org/abs/2509.21351)
*Valentin Samokhin,Boris Shirokikh,Mikhail Goncharov,Dmitriy Umerenkov,Maksim Bobrin,Ivan Oseledets,Dmitry Dylov,Mikhail Belyaev*

Main category: cs.CV

TL;DR: 本文提出一个模型无关的基于DPO的框架，通过随机对比采样提升X光报告生成（RRG）的准确性。该方法在不依赖奖励模型或人工标注，且不增加训练数据的情况下，将现有SOTA模型的临床性能指标提高了高达5%。


<details>
  <summary>Details</summary>
Motivation: X光报告生成（RRG）作为缓解放射科医生工作量的工具备受关注，但现有方法尚未达到在真实临床环境中部署所需的质量水平。

Method: 引入一个模型无关的框架，利用直接偏好优化（DPO）来提高RRG的准确性。该方法通过随机对比采样构建训练对，从而避免了对奖励模型或人工偏好标注的需求。

Result: 通过将提出的随机DPO方法应用于三个最先进的模型，临床性能指标最高提升了5%，且无需任何额外的训练数据。

Conclusion: 本研究提出的基于随机DPO的模型无关框架，能够有效提升RRG的临床性能，且无需奖励模型、人工标注或额外训练数据，为RRG的实际应用提供了有前景的解决方案。

Abstract: Radiography Report Generation (RRG) has gained significant attention in
medical image analysis as a promising tool for alleviating the growing workload
of radiologists. However, despite numerous advancements, existing methods have
yet to achieve the quality required for deployment in real-world clinical
settings. Meanwhile, large Visual Language Models (VLMs) have demonstrated
remarkable progress in the general domain by adopting training strategies
originally designed for Large Language Models (LLMs), such as alignment
techniques. In this paper, we introduce a model-agnostic framework to enhance
RRG accuracy using Direct Preference Optimization (DPO). Our approach leverages
random contrastive sampling to construct training pairs, eliminating the need
for reward models or human preference annotations. Experiments on supplementing
three state-of-the-art models with our Random DPO show that our method improves
clinical performance metrics by up to 5%, without requiring any additional
training data.

</details>


### [39] [Improving Autism Detection with Multimodal Behavioral Analysis](https://arxiv.org/abs/2509.21352)
*William Saakyan,Matthias Norden,Lola Eversmann,Simon Kirsch,Muyu Lin,Simon Guendelman,Isabel Dziobek,Hanna Drimalla*

Main category: cs.CV

TL;DR: 本研究通过分析大型视频数据集，引入新型注视特征并整合多模态行为标记，实现了74%的自闭症分类准确率，为开发可扩展的视频筛查工具奠定基础。


<details>
  <summary>Details</summary>
Motivation: 自闭症诊断复杂且耗资源，现有计算机辅助诊断方法在注视特征表现和真实世界泛化能力方面存在不足。

Method: 分析了包含168名ASC和157名非自闭症参与者的标准化视频数据集。采用面部表情、语音语调、头部运动、心率变异性(HRV)和注视行为的多模态分析。引入新型统计描述符以量化眼睛注视角度的变异性，并使用晚期融合集成多模态数据。

Result: 新型注视描述符将基于注视的分类准确率从64%提升至69%。通过晚期融合，实现了74%的分类准确率，证明了多模态行为标记整合的有效性。

Conclusion: 本研究的发现突出了可扩展的、基于视频的筛查工具在支持自闭症评估方面的潜力，并与临床研究中注视回避现象相吻合。

Abstract: Due to the complex and resource-intensive nature of diagnosing Autism
Spectrum Condition (ASC), several computer-aided diagnostic support methods
have been proposed to detect autism by analyzing behavioral cues in patient
video data. While these models show promising results on some datasets, they
struggle with poor gaze feature performance and lack of real-world
generalizability. To tackle these challenges, we analyze a standardized video
dataset comprising 168 participants with ASC (46% female) and 157 non-autistic
participants (46% female), making it, to our knowledge, the largest and most
balanced dataset available. We conduct a multimodal analysis of facial
expressions, voice prosody, head motion, heart rate variability (HRV), and gaze
behavior. To address the limitations of prior gaze models, we introduce novel
statistical descriptors that quantify variability in eye gaze angles, improving
gaze-based classification accuracy from 64% to 69% and aligning computational
findings with clinical research on gaze aversion in ASC. Using late fusion, we
achieve a classification accuracy of 74%, demonstrating the effectiveness of
integrating behavioral markers across multiple modalities. Our findings
highlight the potential for scalable, video-based screening tools to support
autism assessment.

</details>


### [40] [KV-Efficient VLA: A Method of Speed up Vision Language Model with RNN-Gated Chunked KV Cache](https://arxiv.org/abs/2509.21354)
*Wanshun Xu,Long Zhuang*

Main category: cs.CV

TL;DR: 本文提出KV-Efficient VLA，一个模型无关的记忆压缩框架，通过选择性保留高价值上下文，解决VLA模型在长时推理中注意力计算的二次成本和KV内存无限增长的问题，实现了推理加速和内存减少，对任务成功率影响极小。


<details>
  <summary>Details</summary>
Motivation: VLA模型在统一机器人感知和控制方面前景广阔，但其可扩展性受限于长时推理过程中注意力计算的二次成本和键值（KV）内存的无限增长。尽管现有方法通过扩展骨干架构提高了泛化能力，但它们往往忽视了实时部署中至关重要的推理效率问题。

Method: KV-Efficient VLA是一个模型无关的记忆压缩框架。它通过引入一个轻量级、训练友好的机制来选择性保留高价值上下文。具体方法是将KV缓存划分为固定大小的块，并利用循环门控模块根据学习到的效用分数来总结和过滤历史上下文。该设计在保留近期细粒度细节的同时，积极修剪陈旧、低相关性的内存，并始终保持因果关系。

Result: 理论上，KV-Efficient VLA实现了高达1.21倍的推理速度提升和36%的KV内存减少，同时对任务成功率的影响最小。

Conclusion: KV-Efficient VLA提供了一个可扩展的推理方案，能够无缝集成到现有的自回归和混合VLA栈中，且无需修改训练流程或下游控制逻辑，有效解决了VLA模型在实时部署中的效率瓶颈。

Abstract: Vision-Language-Action (VLA) models promise unified robotic perception and
control, yet their scalability is constrained by the quadratic cost of
attention and the unbounded growth of key-value (KV) memory during long-horizon
inference. While recent methods improve generalization through scaling backbone
architectures, they often neglect the inference inefficiencies critical to
real-time deployment. In this work, we present KV-Efficient VLA, a
model-agnostic memory compression framework that addresses these limitations by
introducing a lightweight, training-friendly mechanism to selectively retain
high-utility context. Our method partitions the KV cache into fixed size chunks
and employs a recurrent gating module to summarize and filter historical
context according to learned utility scores. This design preserves recent
fine-grained detail while aggressively pruning stale, low-relevance memory, all
while maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x
inference speedup and 36% KV memory reduction, with minimal impact on task
success. Our method integrates seamlessly into existing autoregressive and
hybrid VLA stacks, enabling scalable inference without modifying training
pipelines or downstream control logic.

</details>


### [41] [Phrase-grounded Fact-checking for Automatically Generated Chest X-ray Reports](https://arxiv.org/abs/2509.21356)
*Razi Mahmood,Diego Machado-Reyes,Joy Wu,Parisa Kaviani,Ken C. L. Wong,Niharika D'Souza,Mannudeep Kalra,Ge Wang,Pingkun Yan,Tanveer Syeda-Mahmood*

Main category: cs.CV

TL;DR: 针对视觉语言模型（VLM）生成胸部X光报告中存在的错误和幻觉问题，本文提出了一种新颖的短语接地事实核查模型（FC模型）。该模型通过构建模拟错误的大型合成数据集，并训练新的多标签跨模态对比回归网络，能有效检测报告中发现及其位置的错误，并在多个数据集上表现出高鲁棒性和高准确性（与真实值验证一致性相关系数达0.997），具有临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型（VLM）能够生成胸部X光报告，但其产生的描述中存在事实错误和幻觉，阻碍了其在临床上的实际应用。

Method: 本文提出了一个新颖的短语接地事实核查模型（FC模型）。具体方法包括：1) 通过扰动真实报告中的发现及其位置，构建一个大型合成数据集，模拟报告中的错误，形成真实和虚假的发现-位置对以及图像。2) 在此数据集上训练一个新的多标签跨模态对比回归网络（multi-label cross-modal contrastive regression network）。

Result: 研究结果表明，该方法在多个X光数据集上，在发现真实性预测和定位准确性方面具有鲁棒性。此外，它在多个数据集上对最先进（SOTA）报告生成器生成的报告进行错误检测时，实现了0.997的与真实值验证一致性相关系数。

Conclusion: 该事实核查模型在放射学工作流的临床推断中具有重要的实用价值。

Abstract: With the emergence of large-scale vision language models (VLM), it is now
possible to produce realistic-looking radiology reports for chest X-ray images.
However, their clinical translation has been hampered by the factual errors and
hallucinations in the produced descriptions during inference. In this paper, we
present a novel phrase-grounded fact-checking model (FC model) that detects
errors in findings and their indicated locations in automatically generated
chest radiology reports.
  Specifically, we simulate the errors in reports through a large synthetic
dataset derived by perturbing findings and their locations in ground truth
reports to form real and fake findings-location pairs with images. A new
multi-label cross-modal contrastive regression network is then trained on this
dataset. We present results demonstrating the robustness of our method in terms
of accuracy of finding veracity prediction and localization on multiple X-ray
datasets. We also show its effectiveness for error detection in reports of SOTA
report generators on multiple datasets achieving a concordance correlation
coefficient of 0.997 with ground truth-based verification, thus pointing to its
utility during clinical inference in radiology workflows.

</details>


### [42] [MDF-MLLM: Deep Fusion Through Cross-Modal Feature Alignment for Contextually Aware Fundoscopic Image Classification](https://arxiv.org/abs/2509.21358)
*Jason Jordan,Mohammadreza Akbari Lor,Peter Koulen,Mei-Ling Shyu,Shu-Ching Chen*

Main category: cs.CV

TL;DR: 该研究开发了一种名为MDF-MLLM的新型多模态深度学习架构，通过融合视网膜图像的精细特征和全局文本上下文，显著提高了眼底疾病分类的准确性，解决了现有MLLM在捕获低级空间细节方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大型语言模型（MLLMs）难以捕获诊断视网膜疾病（如青光眼、糖尿病视网膜病变、视网膜色素变性）所需的低级空间细节，导致疾病分类准确性不足。

Method: 该研究在包含1,305对眼底图像-文本对的三个公开数据集上进行模型开发和验证。MDF-MLLM将来自U-Net编码器的四个跳跃特征层集成到LLaMA 3.2 11B MLLM的交叉注意力块中。视觉特征通过补丁投影和缩放交叉注意力及基于FiLM的U-Net调制进行融合。模型在训练期间对U-Net和MLLM组件进行全面微调。

Result: MDF-MLLM在双类型疾病分类任务上达到了94%的准确率，相比基线MLLM的60%准确率，提高了56%。召回率和F1-分数分别比基线提升了67%和35%。消融研究证实，多深度融合方法显著提高了空间推理和分类能力，尤其对具有丰富临床文本的遗传性疾病表现突出。

Conclusion: MDF-MLLM通过多尺度特征融合，提供了一个可泛化、可解释、模块化的眼底图像分类框架，优于传统的MLLM基线。该架构在临床决策支持系统中具有实际应用前景。未来工作将探索同步训练技术、更广泛的疾病范围以及模型在分割任务上的扩展。

Abstract: This study aimed to enhance disease classification accuracy from retinal
fundus images by integrating fine-grained image features and global textual
context using a novel multimodal deep learning architecture. Existing
multimodal large language models (MLLMs) often struggle to capture low-level
spatial details critical for diagnosing retinal diseases such as glaucoma,
diabetic retinopathy, and retinitis pigmentosa. This model development and
validation study was conducted on 1,305 fundus image-text pairs compiled from
three public datasets (FIVES, HRF, and StoneRounds), covering acquired and
inherited retinal diseases, and evaluated using classification accuracy and
F1-score. The MDF-MLLM integrates skip features from four U-Net encoder layers
into cross-attention blocks within a LLaMA 3.2 11B MLLM. Vision features are
patch-wise projected and fused using scaled cross-attention and FiLM-based
U-Net modulation. Baseline MLLM achieved 60% accuracy on the dual-type disease
classification task. MDF-MLLM, with both U-Net and MLLM components fully
fine-tuned during training, achieved a significantly higher accuracy of 94%,
representing a 56% improvement. Recall and F1-scores improved by as much as 67%
and 35% over baseline, respectively. Ablation studies confirmed that the
multi-depth fusion approach contributed to substantial gains in spatial
reasoning and classification, particularly for inherited diseases with rich
clinical text. MDF-MLLM presents a generalizable, interpretable, and modular
framework for fundus image classification, outperforming traditional MLLM
baselines through multi-scale feature fusion. The architecture holds promise
for real-world deployment in clinical decision support systems. Future work
will explore synchronized training techniques, a larger pool of diseases for
more generalizability, and extending the model for segmentation tasks.

</details>


### [43] [Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models](https://arxiv.org/abs/2509.21360)
*Xingkai Peng,Jun Jiang,Meng Tong,Shuai Li,Weiming Zhang,Nenghai Yu,Kejiang Chen*

Main category: cs.CV

TL;DR: T2I模型易受越狱攻击生成NSFW内容，现有文本方法有局限。本文提出多模态提示解耦攻击（MPDA），利用LLM解耦提示并重写，结合VLM反馈，通过图像模态绕过安全过滤器生成语义一致的NSFW图像。


<details>
  <summary>Details</summary>
Motivation: T2I模型可能被滥用生成NSFW内容；现有越狱方法主要操纵文本提示，忽视图像输入漏洞；文本方法难以绕过模型安全过滤器。

Method: 提出多模态提示解耦攻击（MPDA）。该方法利用图像模态分离有害语义组件，并分三步实施：1. LLM将不安全提示解耦为伪安全提示和有害提示。2. LLM将有害提示重写为自然对抗性提示，指导T2I模型生成NSFW输出。3. VLM生成图像标题提供反馈，指导LLM迭代重写，确保生成内容与原始提示的语义一致性。

Result: 本文提出的MPDA方法能够利用图像模态和多模态交互，有效绕过T2I模型的安全过滤器，实现不安全内容的生成，并确保生成内容与原始提示的语义一致性。

Conclusion: MPDA揭示了T2I模型在处理图像模态输入和多模态交互时存在的潜在越狱漏洞，对T2I模型的安全防御提出了新的挑战。

Abstract: Text-to-image (T2I) models have been widely applied in generating
high-fidelity images across various domains. However, these models may also be
abused to produce Not-Safe-for-Work (NSFW) content via jailbreak attacks.
Existing jailbreak methods primarily manipulate the textual prompt, leaving
potential vulnerabilities in image-based inputs largely unexplored. Moreover,
text-based methods face challenges in bypassing the model's safety filters. In
response to these limitations, we propose the Multimodal Prompt Decoupling
Attack (MPDA), which utilizes image modality to separate the harmful semantic
components of the original unsafe prompt. MPDA follows three core steps:
firstly, a large language model (LLM) decouples unsafe prompts into pseudo-safe
prompts and harmful prompts. The former are seemingly harmless sub-prompts that
can bypass filters, while the latter are sub-prompts with unsafe semantics that
trigger filters. Subsequently, the LLM rewrites the harmful prompts into
natural adversarial prompts to bypass safety filters, which guide the T2I model
to modify the base image into an NSFW output. Finally, to ensure semantic
consistency between the generated NSFW images and the original unsafe prompts,
the visual language model generates image captions, providing a new pathway to
guide the LLM in iterative rewriting and refining the generated content.

</details>


### [44] [A Mutual Learning Method for Salient Object Detection with intertwined Multi-Supervision--Revised](https://arxiv.org/abs/2509.21363)
*Runmin Wu,Mengyang Feng,Wenlong Guan,Dong Wang,Huchuan Lu,Errui Ding*

Main category: cs.CV

TL;DR: 本研究提出了一种多任务学习方法，结合显著目标检测、前景轮廓检测和边缘检测，并通过引入互学习模块，解决了显著图预测不完整和边界不准确的问题，并在多个数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习显著目标检测方法在预测显著图时，存在由于物体内部复杂性导致的预测不完整和卷积池化操作步长引起的边界不准确问题。

Method: 提出了一种多任务学习网络训练方法，利用显著目标检测、前景轮廓检测和边缘检测的监督信息。具体地，显著目标检测与前景轮廓检测任务交织进行以生成均匀高亮的显著图；前景轮廓检测和边缘检测任务相互指导以实现精确轮廓预测并减少边缘噪声。此外，还开发了一种包含多个网络分支的互学习模块（MLM）作为方法的核心构建块，以显著提升性能。

Result: 在七个具有挑战性的数据集上进行的广泛实验表明，所提出的方法在显著目标检测和边缘检测方面均取得了最先进的（state-of-the-art）结果。

Conclusion: 通过结合多任务学习策略和互学习模块，本方法有效解决了显著目标检测中的不完整预测和边界不准确问题，并在显著目标检测和边缘检测领域达到了领先水平。

Abstract: Though deep learning techniques have made great progress in salient object
detection recently, the predicted saliency maps still suffer from incomplete
predictions due to the internal complexity of objects and inaccurate boundaries
caused by strides in convolution and pooling operations. To alleviate these
issues, we propose to train saliency detection networks by exploiting the
supervision from not only salient object detection, but also foreground contour
detection and edge detection. First, we leverage salient object detection and
foreground contour detection tasks in an intertwined manner to generate
saliency maps with uniform highlight. Second, the foreground contour and edge
detection tasks guide each other simultaneously, thereby leading to precise
foreground contour prediction and reducing the local noises for edge
prediction. In addition, we develop a novel mutual learning module (MLM) which
serves as the building block of our method. Each MLM consists of multiple
network branches trained in a mutual learning manner, which improves the
performance by a large margin. Extensive experiments on seven challenging
datasets demonstrate that the proposed method has delivered state-of-the-art
results in both salient object detection and edge detection.

</details>


### [45] [MAJORScore: A Novel Metric for Evaluating Multimodal Relevance via Joint Representation](https://arxiv.org/abs/2509.21365)
*Zhicheng Du,Qingyang Shi,Jiasheng Lu,Yingshan Liang,Xinyu Zhang,Yiran Wang,Peiwu Qin*

Main category: cs.CV

TL;DR: 本文提出MAJORScore，一种全新的多模态（N≥3）相关性评估指标，首次通过多模态联合表示来解决现有双模态评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态相关性指标（如CLIP）主要适用于双模态数据，限制了多模态相似性的评估能力。

Method: 提出MAJORScore，通过多模态联合表示（将多种模态整合到同一潜在空间）来准确表示和公平评估N≥3模态之间的相关性。

Result: MAJORScore在一致模态上相较现有方法提升26.03%-64.29%，在不一致模态上降低13.28%-20.54%。

Conclusion: MAJORScore是评估大规模多模态数据集相似性及多模态模型性能的更可靠指标。

Abstract: The multimodal relevance metric is usually borrowed from the embedding
ability of pretrained contrastive learning models for bimodal data, which is
used to evaluate the correlation between cross-modal data (e.g., CLIP).
However, the commonly used evaluation metrics are only suitable for the
associated analysis between two modalities, which greatly limits the evaluation
of multimodal similarity. Herein, we propose MAJORScore, a brand-new evaluation
metric for the relevance of multiple modalities ($N$ modalities, $N\ge3$) via
multimodal joint representation for the first time. The ability of multimodal
joint representation to integrate multiple modalities into the same latent
space can accurately represent different modalities at one scale, providing
support for fair relevance scoring. Extensive experiments have shown that
MAJORScore increases by 26.03%-64.29% for consistent modality and decreases by
13.28%-20.54% for inconsistence compared to existing methods. MAJORScore serves
as a more reliable metric for evaluating similarity on large-scale multimodal
datasets and multimodal model performance evaluation.

</details>


### [46] [Safety Assessment of Scaffolding on Construction Site using AI](https://arxiv.org/abs/2509.21368)
*Sameer Prabhu,Amit Patwardhan,Ramin Karim*

Main category: cs.CV

TL;DR: 本文提出一个基于AI和数字化技术的云平台，利用点云数据自动检测脚手架结构变化，以提升安全性并减少人工检查的耗时和错误。


<details>
  <summary>Details</summary>
Motivation: 建筑行业脚手架安全评估至关重要，但现有的人工目视检查耗时、易错，可能导致不安全状况，亟需更准确、高效的检测方法。

Method: 开发了一个云端AI平台，用于处理和分析脚手架结构的点云数据。该系统通过将近期点云数据与认证参考数据进行比较和评估，以检测结构修改。

Result: 所提出的系统能够检测脚手架的结构修改，实现了脚手架的自动化监测。

Conclusion: 该方法有望减少人工检查所需的时间和精力，同时提高施工现场的安全性。

Abstract: In the construction industry, safety assessment is vital to ensure both the
reliability of assets and the safety of workers. Scaffolding, a key structural
support asset requires regular inspection to detect and identify alterations
from the design rules that may compromise the integrity and stability. At
present, inspections are primarily visual and are conducted by site manager or
accredited personnel to identify deviations. However, visual inspection is
time-intensive and can be susceptible to human errors, which can lead to unsafe
conditions. This paper explores the use of Artificial Intelligence (AI) and
digitization to enhance the accuracy of scaffolding inspection and contribute
to the safety improvement. A cloud-based AI platform is developed to process
and analyse the point cloud data of scaffolding structure. The proposed system
detects structural modifications through comparison and evaluation of certified
reference data with the recent point cloud data. This approach may enable
automated monitoring of scaffolding, reducing the time and effort required for
manual inspections while enhancing the safety on a construction site.

</details>


### [47] [Automated Prompt Generation for Creative and Counterfactual Text-to-image Synthesis](https://arxiv.org/abs/2509.21375)
*Aleksa Jelaca,Ying Jiao,Chang Tian,Marie-Francine Moens*

Main category: cs.CV

TL;DR: 解决文本到图像生成中反事实（特别是尺寸）可控性难题，通过一个自动提示工程框架，将基础提示改编为反事实图像的修订提示。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像生成发展迅速，但仍缺乏细粒度控制，尤其是在生成与常识模式相悖的反事实图像方面。这种反事实可控性对于激发创造力和探索性应用至关重要，是本文的研究动机。

Method: 提出一个自动提示工程框架，通过将基础提示适配为修订提示来生成反事实图像。该框架包括：1) 一个图像评估器（通过扩展Grounded SAM并改进，性能提升114%）用于指导数据集构建；2) 一个监督提示重写器用于生成修订提示；3) 一个DPO训练的排序器用于选择最优提示。此外，还构建了首个反事实尺寸文本图像数据集。

Result: 实验表明，所提出的方法在反事实图像生成方面优于当前最先进的基线方法和ChatGPT-4o。

Conclusion: 本研究为未来反事实可控性研究奠定了基础。

Abstract: Text-to-image generation has advanced rapidly with large-scale multimodal
training, yet fine-grained controllability remains a critical challenge.
Counterfactual controllability, defined as the capacity to deliberately
generate images that contradict common-sense patterns, remains a major
challenge but plays a crucial role in enabling creativity and exploratory
applications. In this work, we address this gap with a focus on counterfactual
size (e.g., generating a tiny walrus beside a giant button) and propose an
automatic prompt engineering framework that adapts base prompts into revised
prompts for counterfactual images. The framework comprises three components: an
image evaluator that guides dataset construction by identifying successful
image generations, a supervised prompt rewriter that produces revised prompts,
and a DPO-trained ranker that selects the optimal revised prompt. We construct
the first counterfactual size text-image dataset and enhance the image
evaluator by extending Grounded SAM with refinements, achieving a 114 percent
improvement over its backbone. Experiments demonstrate that our method
outperforms state-of-the-art baselines and ChatGPT-4o, establishing a
foundation for future research on counterfactual controllability.

</details>


### [48] [In silico Deep Learning Protocols for Label-Free Super-Resolution Microscopy: A Comparative Study of Network Architectures and SNR Dependence](https://arxiv.org/abs/2509.21376)
*Shiraz S Kaderuppan,Jonathan Mar,Andrew Irvine,Anurag Sharma,Muhammad Ramadan Saifuddin,Wai Leong Eugene Wong,Wai Lok Woo*

Main category: cs.CV

TL;DR: 本研究评估了O-Net和Theta-Net两种深度神经网络，用于经济型非荧光超分辨率显微镜图像处理。结果显示，在高信噪比下O-Net表现更佳，低信噪比下Theta-Net更优，两者互补。


<details>
  <summary>Details</summary>
Motivation: 传统光学显微镜分辨率受限（约200nm），而现有超分辨率技术（如超分辨荧光显微镜）通常成本高昂或需专业技术，不适合普通用户。本研究旨在探索一种经济、非荧光的替代方案，通过相调制显微模式结合深度学习实现超分辨率。

Method: 评估了两种自研的深度神经网络（DNN）架构O-Net和Theta-Net。使用定制的、经原子力显微镜（AFM）校准的纳米级特征测试目标，并将其应用于非荧光相调制显微图像（如Zernike相衬、差分干涉相衬）。

Result: O-Net和Theta-Net在图像超分辨率处理上均表现良好，但互为补充而非竞争。在高图像信噪比（SNR）下，O-Net模型表现更优；在低信噪比下，Theta-Net模型更具优势。

Conclusion: 在使用DNN模型进行非荧光光学纳米显微时，模型架构的选择（结合源图像信噪比）对模型性能和生成的超分辨率图像质量至关重要，即使使用相同的训练数据和训练周期。

Abstract: The field of optical microscopy spans across numerous industries and research
domains, ranging from education to healthcare, quality inspection and analysis.
Nonetheless, a key limitation often cited by optical microscopists refers to
the limit of its lateral resolution (typically defined as ~200nm), with
potential circumventions involving either costly external modules (e.g.
confocal scan heads, etc) and/or specialized techniques [e.g. super-resolution
(SR) fluorescent microscopy]. Addressing these challenges in a normal
(non-specialist) context thus remains an aspect outside the scope of most
microscope users & facilities. This study thus seeks to evaluate an alternative
& economical approach to achieving SR optical microscopy, involving
non-fluorescent phase-modulated microscopical modalities such as Zernike phase
contrast (PCM) and differential interference contrast (DIC) microscopy. Two in
silico deep neural network (DNN) architectures which we developed previously
(termed O-Net and Theta-Net) are assessed on their abilities to resolve a
custom-fabricated test target containing nanoscale features calibrated via
atomic force microscopy (AFM). The results of our study demonstrate that
although both O-Net and Theta-Net seemingly performed well when super-resolving
these images, they were complementary (rather than competing) approaches to be
considered for image SR, particularly under different image signal-to-noise
ratios (SNRs). High image SNRs favoured the application of O-Net models, while
low SNRs inclined preferentially towards Theta-Net models. These findings
demonstrate the importance of model architectures (in conjunction with the
source image SNR) on model performance and the SR quality of the generated
images where DNN models are utilized for non-fluorescent optical nanoscopy,
even where the same training dataset & number of epochs are being used.

</details>


### [49] [Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation](https://arxiv.org/abs/2509.21377)
*Yinfeng Yu,Hailong Zhang,Meiling Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种名为DMTF-AVN的机器人视听导航新方法，通过结合多目标架构和Transformer机制，有效地融合视觉与听觉信息以定位声源，并在多个指标上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有视听具身导航方法在定位声源时，虽探索了视听数据融合，但常忽略深层感知上下文，未能有效利用多模态线索指导导航。主要挑战在于如何有效融合多模态信息。

Method: 本文提出DMTF-AVN（Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation），采用多目标架构结合改进的Transformer机制，以过滤并选择性地融合跨模态信息，从而实现高效的视听导航。

Result: 在Replica和Matterport3D数据集上进行的广泛实验表明，DMTF-AVN在成功率（SR）、路径效率（SPL）和场景适应性（SNA）方面均超越现有方法，达到了最先进的性能。此外，模型还展现出强大的可扩展性和泛化能力。

Conclusion: DMTF-AVN为机器人导航中的高级多模态融合策略奠定了基础，显著提升了机器人通过视听信息定位声源的能力，并具有良好的应用前景。

Abstract: Audiovisual embodied navigation enables robots to locate audio sources by
dynamically integrating visual observations from onboard sensors with the
auditory signals emitted by the target. The core challenge lies in effectively
leveraging multimodal cues to guide navigation. While prior works have explored
basic fusion of visual and audio data, they often overlook deeper perceptual
context. To address this, we propose the Dynamic Multi-Target Fusion for
Efficient Audio-Visual Navigation (DMTF-AVN). Our approach uses a multi-target
architecture coupled with a refined Transformer mechanism to filter and
selectively fuse cross-modal information. Extensive experiments on the Replica
and Matterport3D datasets demonstrate that DMTF-AVN achieves state-of-the-art
performance, outperforming existing methods in success rate (SR), path
efficiency (SPL), and scene adaptation (SNA). Furthermore, the model exhibits
strong scalability and generalizability, paving the way for advanced multimodal
fusion strategies in robotic navigation. The code and videos are available at
  https://github.com/zzzmmm-svg/DMTF.

</details>


### [50] [SAEmnesia: Erasing Concepts in Diffusion Models with Sparse Autoencoders](https://arxiv.org/abs/2509.21379)
*Enrico Cassano,Riccardo Renzulli,Marco Nurisso,Mirko Zaffaroni,Alan Perotti,Marco Grangetto*

Main category: cs.CV

TL;DR: SAEmnesia是一种监督式稀疏自编码器训练方法，通过系统概念标注促进概念与神经元的一一映射，从而显著提升了文本到图像扩散模型中概念遗忘的效率和准确性，并大幅减少推理时的超参数搜索。


<details>
  <summary>Details</summary>
Motivation: 在文本到图像扩散模型中，有效概念遗忘需要精确地定位概念表示。现有稀疏自编码器虽能减少神经元的多义性，但单个概念仍可能分散在多个潜在特征中，导致概念遗忘过程需要耗费大量搜索。

Method: 提出SAEmnesia，这是一种监督式稀疏自编码器训练方法。它通过系统性的概念标注，促进概念与神经元之间的一一映射，从而减轻特征分裂并促进特征集中化。该方法训练出具有更强概念关联的专业化神经元。

Result: 1. 推理时，可解释的表示将超参数搜索减少了96.67%。
2. 在UnlearnCanvas基准测试中，SAEmnesia比现有最先进技术提高了9.22%。
3. 在顺序遗忘任务中，实现了卓越的可扩展性，9个对象移除的遗忘准确性提高了28.4%。
4. 额外的计算开销仅限于训练时的交叉熵计算。

Conclusion: SAEmnesia通过提供一种更可解释且高效的概念表示，显著改进了扩散模型中的概念遗忘，不仅提高了遗忘精度和效率，还在各项任务中展现出优于现有方法的性能和可扩展性。

Abstract: Effective concept unlearning in text-to-image diffusion models requires
precise localization of concept representations within the model's latent
space. While sparse autoencoders successfully reduce neuron polysemanticity
(i.e., multiple concepts per neuron) compared to the original network,
individual concept representations can still be distributed across multiple
latent features, requiring extensive search procedures for concept unlearning.
We introduce SAEmnesia, a supervised sparse autoencoder training method that
promotes one-to-one concept-neuron mappings through systematic concept
labeling, mitigating feature splitting and promoting feature centralization.
Our approach learns specialized neurons with significantly stronger concept
associations compared to unsupervised baselines. The only computational
overhead introduced by SAEmnesia is limited to cross-entropy computation during
training. At inference time, this interpretable representation reduces
hyperparameter search by 96.67% with respect to current approaches. On the
UnlearnCanvas benchmark, SAEmnesia achieves a 9.22% improvement over the
state-of-the-art. In sequential unlearning tasks, we demonstrate superior
scalability with a 28.4% improvement in unlearning accuracy for 9-object
removal.

</details>


### [51] [Coreset selection based on Intra-class diversity](https://arxiv.org/abs/2509.21380)
*Imran Ashraf,Mukhtar Ullah,Muhammad Faisal Nadeem,Muhammad Nouman Noor*

Main category: cs.CV

TL;DR: 针对深度学习训练计算资源消耗大的问题，本研究提出一种智能、轻量级核心集选择机制，通过提取类内多样性进行采样，在生物医学图像分类任务中表现优于随机采样。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型训练及超参数优化需耗费大量计算资源和时间。现有随机采样方法在选择训练子集时，存在代表性不足、对不平衡数据集有偏见、无法捕捉类内多样性的局限。

Method: 提出一种智能、轻量级的核心集（coreset）选择机制。该方法通过提取类内多样性，形成基于类别的聚类，并利用这些聚类进行最终采样，以构建一个具有代表性的数据子集。

Result: 在知名生物医学图像数据集上进行的广泛分类实验表明，在相同条件下，所提出的方案在多个性能指标上优于随机采样方法。

Conclusion: 通过引入智能的类内多样性驱动核心集选择机制，能够有效降低深度学习模型的计算成本，并克服随机采样的局限性，提供更具代表性的训练子集，从而提高模型性能。

Abstract: Deep Learning models have transformed various domains, including the
healthcare sector, particularly biomedical image classification by learning
intricate features and enabling accurate diagnostics pertaining to complex
diseases. Recent studies have adopted two different approaches to train DL
models: training from scratch and transfer learning. Both approaches demand
substantial computational time and resources due to the involvement of massive
datasets in model training. These computational demands are further increased
due to the design-space exploration required for selecting optimal
hyperparameters, which typically necessitates several training rounds. With the
growing sizes of datasets, exploring solutions to this problem has recently
gained the research community's attention. A plausible solution is to select a
subset of the dataset for training and hyperparameter search. This subset,
referred to as the corset, must be a representative set of the original
dataset. A straightforward approach to selecting the coreset could be employing
random sampling, albeit at the cost of compromising the representativeness of
the original dataset. A critical limitation of random sampling is the bias
towards the dominant classes in an imbalanced dataset. Even if the dataset has
inter-class balance, this random sampling will not capture intra-class
diversity. This study addresses this issue by introducing an intelligent,
lightweight mechanism for coreset selection. Specifically, it proposes a method
to extract intra-class diversity, forming per-class clusters that are utilized
for the final sampling. We demonstrate the efficacy of the proposed methodology
by conducting extensive classification experiments on a well-known biomedical
imaging dataset. Results demonstrate that the proposed scheme outperforms the
random sampling approach on several performance metrics for uniform conditions.

</details>


### [52] [The LongiMam model for improved breast cancer risk prediction using longitudinal mammograms](https://arxiv.org/abs/2509.21383)
*Manel Rakez,Thomas Louis,Julien Guillaumin,Foucauld Chamming's,Pierre Fillard,Brice Amadeo,Virginie Rondeau*

Main category: cs.CV

TL;DR: LongiMam是一种端到端深度学习模型，通过整合当前和最多四次既往乳腺X线照片（结合CNN和RNN），显著提升了乳腺癌预测的准确性，优于仅使用单次检查的模型，并证实了纵向数据在风险分层中的重要性。


<details>
  <summary>Details</summary>
Motivation: 目前的深度学习模型在乳腺癌风险分层中，大多只使用单次或有限的既往乳腺X线照片，且未能有效适应真实世界中结果分布不平衡和随访异质性的挑战。因此，需要开发更稳健的模型来充分利用纵向影像数据。

Method: 开发了LongiMam，一个端到端的深度学习模型，能够整合当前和最多四次既往的乳腺X线照片。该模型结合了卷积神经网络（CNN）以捕获空间模式和循环神经网络（RNN）以捕获时间模式。模型在一个大型的、基于人群的筛查数据集上进行了训练和评估，该数据集具有与临床筛查相似的不平衡病例对照比。

Result: 当包含既往乳腺X线照片时，LongiMam在不同场景下均持续提高了预测性能。结合当前和既往检查的模型表现优于仅使用单次检查的模型，而单独使用既往检查的效果不如两者结合。子组分析证实了模型在关键风险人群（如乳腺致密女性和55岁以上女性）中的有效性。此外，模型在乳腺密度随时间发生变化的女性中表现最佳。

Conclusion: 研究结果表明，纵向建模显著增强了乳腺癌预测能力。该研究支持在筛查项目中利用重复的乳腺X线照片来优化风险分层。

Abstract: Risk-adapted breast cancer screening requires robust models that leverage
longitudinal imaging data. Most current deep learning models use single or
limited prior mammograms and lack adaptation for real-world settings marked by
imbalanced outcome distribution and heterogeneous follow-up. We developed
LongiMam, an end-to-end deep learning model that integrates both current and up
to four prior mammograms. LongiMam combines a convolutional and a recurrent
neural network to capture spatial and temporal patterns predictive of breast
cancer. The model was trained and evaluated using a large, population-based
screening dataset with disproportionate case-to-control ratio typical of
clinical screening. Across several scenarios that varied in the number and
composition of prior exams, LongiMam consistently improved prediction when
prior mammograms were included. The addition of prior and current visits
outperformed single-visit models, while priors alone performed less well,
highlighting the importance of combining historical and recent information.
Subgroup analyses confirmed the model's efficacy across key risk groups,
including women with dense breasts and those aged 55 years or older. Moreover,
the model performed best in women with observed changes in mammographic density
over time. These findings demonstrate that longitudinal modeling enhances
breast cancer prediction and support the use of repeated mammograms to refine
risk stratification in screening programs. LongiMam is publicly available as
open-source software.

</details>


### [53] [Assessing the Alignment of Popular CNNs to the Brain for Valence Appraisal](https://arxiv.org/abs/2509.21384)
*Laurent Mertens,Elahe' Yargholi,Laura Van Hove,Hans Op de Beeck,Jan Van den Stock,Joost Vennekens*

Main category: cs.CV

TL;DR: 本文探讨了CNNs与人脑在社会认知（图像效价评估）任务中的对应关系。发现CNNs难以超越简单视觉处理，未能反映高阶脑过程。提出了Object2Brain框架，并发现不同CNN架构对物体类别敏感性不同。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注CNNs与人脑在一般视觉感知方面的对应。本文旨在探究这种对应关系在更复杂的脑过程，即社会认知（具体为图像效价评估）中是否也成立。

Method: 通过相关性分析，评估流行CNN架构与人类行为数据和fMRI数据在图像效价评估任务上的对齐程度。此外，提出了一个结合GradCAM和CNN滤波器层面物体检测与相关性分析的新框架——Object2Brain，以研究不同物体类别对CNN与人类相关性的影响。

Result: 研究发现，在图像效价评估任务中，CNNs难以超越简单的视觉处理，似乎未能反映更高阶的脑过程。尽管相关性趋势相似，但不同的CNN架构表现出不同的物体类别敏感性。

Conclusion: CNNs在处理复杂的社会认知任务（如图像效价评估）时，其能力似乎仅限于简单的视觉处理，未能体现更高阶的脑功能。不同CNN模型对特定物体类别的处理方式存在差异。

Abstract: Convolutional Neural Networks (CNNs) are a popular type of computer model
that have proven their worth in many computer vision tasks. Moreover, they form
an interesting study object for the field of psychology, with shown
correspondences between the workings of CNNs and the human brain. However,
these correspondences have so far mostly been studied in the context of general
visual perception. In contrast, this paper explores to what extent this
correspondence also holds for a more complex brain process, namely social
cognition. To this end, we assess the alignment between popular CNN
architectures and both human behavioral and fMRI data for image valence
appraisal through a correlation analysis. We show that for this task CNNs
struggle to go beyond simple visual processing, and do not seem to reflect
higher-order brain processing. Furthermore, we present Object2Brain, a novel
framework that combines GradCAM and object detection at the CNN-filter level
with the aforementioned correlation analysis to study the influence of
different object classes on the CNN-to-human correlations. Despite similar
correlation trends, different CNN architectures are shown to display different
object class sensitivities.

</details>


### [54] [Debugging Concept Bottleneck Models through Removal and Retraining](https://arxiv.org/abs/2509.21385)
*Eric Enouen,Sainyam Galhotra*

Main category: cs.CV

TL;DR: 本文提出了一个通用的可解释调试框架，用于解决概念瓶颈模型（CBMs）中因偏见数据导致的系统性错位问题。该框架通过两步（移除和再训练）和新方法CBDebug，将专家概念级反馈转化为样本级辅助标签，用于有监督的偏见缓解和定向增强，显著优于现有再训练方法。


<details>
  <summary>Details</summary>
Motivation: CBMs的现有干预措施未能解决模型与专家推理之间的系统性错位（例如，模型从有偏数据中学习捷径），导致模型可能依赖于不期望的概念。因此，需要一个框架来调试和纠正这些问题。

Method: 该框架包含两步：1. **移除**：专家根据概念解释识别并移除不期望的概念。2. **再训练**：引入CBDebug方法，该方法利用CBM的可解释性，将概念级的用户反馈转化为样本级的辅助标签。这些标签随后用于应用有监督的偏见缓解和定向增强，以减少模型对不期望概念的依赖。

Result: 通过真实和自动化专家反馈的评估表明，CBDebug在多种CBM架构（PIP-Net、Post-hoc CBM）和具有已知虚假关联的基准测试中，显著优于先前的再训练方法。

Conclusion: 所提出的CBM可解释调试框架，特别是CBDebug方法，通过将概念级反馈转化为样本级辅助标签进行偏见缓解和增强，有效地解决了CBMs中的系统性错位和对不期望概念的依赖问题，并表现出卓越的性能。

Abstract: Concept Bottleneck Models (CBMs) use a set of human-interpretable concepts to
predict the final task label, enabling domain experts to not only validate the
CBM's predictions, but also intervene on incorrect concepts at test time.
However, these interventions fail to address systemic misalignment between the
CBM and the expert's reasoning, such as when the model learns shortcuts from
biased data. To address this, we present a general interpretable debugging
framework for CBMs that follows a two-step process of Removal and Retraining.
In the Removal step, experts use concept explanations to identify and remove
any undesired concepts. In the Retraining step, we introduce CBDebug, a novel
method that leverages the interpretability of CBMs as a bridge for converting
concept-level user feedback into sample-level auxiliary labels. These labels
are then used to apply supervised bias mitigation and targeted augmentation,
reducing the model's reliance on undesired concepts. We evaluate our framework
with both real and automated expert feedback, and find that CBDebug
significantly outperforms prior retraining methods across multiple CBM
architectures (PIP-Net, Post-hoc CBM) and benchmarks with known spurious
correlations.

</details>


### [55] [ShipwreckFinder: A QGIS Tool for Shipwreck Detection in Multibeam Sonar Data](https://arxiv.org/abs/2509.21386)
*Anja Sheppard,Tyler Smithline,Andrew Scheffer,David Smith,Advaith V. Sethuraman,Ryan Bird,Sabrina Lin,Katherine A. Skinner*

Main category: cs.CV

TL;DR: 本文介绍ShipwreckFinder，一个开源QGIS插件，利用深度学习从多波束声纳数据中自动检测沉船。


<details>
  <summary>Details</summary>
Motivation: 沉船是重要的海洋历史标志，但手动检查测深数据发现沉船耗时且需要专家分析。

Method: 开发了名为ShipwreckFinder的开源QGIS插件，能够自动预处理测深数据、进行深度学习推理、阈值化模型输出，并生成像素级分割掩码或预测沉船的边界框。其核心是一个深度学习模型，通过大湖区和爱尔兰海岸的沉船数据进行训练，并结合合成数据生成以增加数据集多样性。

Result: 与基于深度学习的ArcGIS工具包和传统逆沉洞检测方法相比，该工具和训练流程在分割性能上表现出优越性。

Conclusion: ShipwreckFinder提供了一个高效且性能优越的自动沉船检测解决方案，并作为开源工具发布。

Abstract: In this paper, we introduce ShipwreckFinder, an open-source QGIS plugin that
detects shipwrecks from multibeam sonar data. Shipwrecks are an important
historical marker of maritime history, and can be discovered through manual
inspection of bathymetric data. However, this is a time-consuming process and
often requires expert analysis. Our proposed tool allows users to automatically
preprocess bathymetry data, perform deep learning inference, threshold model
outputs, and produce either pixel-wise segmentation masks or bounding boxes of
predicted shipwrecks. The backbone of this open-source tool is a deep learning
model, which is trained on a variety of shipwreck data from the Great Lakes and
the coasts of Ireland. Additionally, we employ synthetic data generation in
order to increase the size and diversity of our dataset. We demonstrate
superior segmentation performance with our open-source tool and training
pipeline as compared to a deep learning-based ArcGIS toolkit and a more
classical inverse sinkhole detection method. The open-source tool can be found
at https://github.com/umfieldrobotics/ShipwreckFinderQGISPlugin.

</details>


### [56] [Do Sparse Subnetworks Exhibit Cognitively Aligned Attention? Effects of Pruning on Saliency Map Fidelity, Sparsity, and Concept Coherence](https://arxiv.org/abs/2509.21387)
*Sanish Suwal,Dipkamal Bhusal,Michael Clifford,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 轻度到中度剪枝能提高神经网络的可解释性，改善显著图和概念连贯性，但过度剪枝会损害可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明剪枝在保持模型性能的同时，其对模型可解释性的影响尚不明确。

Method: 使用在ImageNette上训练的ResNet-18模型，进行幅度剪枝和微调。通过Vanilla Gradients和Integrated Gradients评估显著图的稀疏性和忠实度，并利用CRAFT提取概念以追踪语义连贯性，比较不同剪枝程度下的影响。

Result: 轻度到中度剪枝提高了显著图的聚焦和忠实度，并保留了语义上有意义的概念。相反，过度剪枝融合了异构特征，降低了显著图的稀疏性和概念连贯性，尽管模型精度得以保持。

Conclusion: 剪枝可以将内部表示调整为更符合人类的注意力模式，但过度剪枝会损害模型的可解释性。

Abstract: Prior works have shown that neural networks can be heavily pruned while
preserving performance, but the impact of pruning on model interpretability
remains unclear. In this work, we investigate how magnitude-based pruning
followed by fine-tuning affects both low-level saliency maps and high-level
concept representations. Using a ResNet-18 trained on ImageNette, we compare
post-hoc explanations from Vanilla Gradients (VG) and Integrated Gradients (IG)
across pruning levels, evaluating sparsity and faithfulness. We further apply
CRAFT-based concept extraction to track changes in semantic coherence of
learned concepts. Our results show that light-to-moderate pruning improves
saliency-map focus and faithfulness while retaining distinct, semantically
meaningful concepts. In contrast, aggressive pruning merges heterogeneous
features, reducing saliency map sparsity and concept coherence despite
maintaining accuracy. These findings suggest that while pruning can shape
internal representations toward more human-aligned attention patterns,
excessive pruning undermines interpretability.

</details>


### [57] [TUN3D: Towards Real-World Scene Understanding from Unposed Images](https://arxiv.org/abs/2509.21388)
*Anton Konushin,Nikita Drozdov,Bulat Gabdullin,Alexey Zakharov,Anna Vorontsova,Danila Rukhovich,Maksim Kolodiazhnyi*

Main category: cs.CV

TL;DR: TUN3D是一种从多视图图像进行室内场景布局估计和3D目标检测的新方法，无需深度或相机位姿监督，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法严重依赖点云输入，但大多数消费级相机缺乏深度传感器，导致视觉数据更普遍。需要一种能联合处理布局估计和3D目标检测，并以多视图图像为输入的方案。

Method: TUN3D是首个在真实扫描中，以多视图图像为输入，且无需地面真值相机位姿或深度监督，解决联合布局估计和3D目标检测的方法。它基于轻量级稀疏卷积骨干网络，并采用两个专用头部（一个用于3D目标检测，一个用于布局估计），利用新颖有效的参数化墙体表示。

Result: TUN3D在三个挑战性场景理解基准上均实现了最先进的性能：(i) 使用地面真值点云，(ii) 使用已知位姿图像，以及 (iii) 使用未知位姿图像。

Conclusion: TUN3D在3D目标检测方面与专业方法媲美，并显著提升了布局估计的性能，为全面的室内场景理解设立了新基准。

Abstract: Layout estimation and 3D object detection are two fundamental tasks in indoor
scene understanding. When combined, they enable the creation of a compact yet
semantically rich spatial representation of a scene. Existing approaches
typically rely on point cloud input, which poses a major limitation since most
consumer cameras lack depth sensors and visual-only data remains far more
common. We address this issue with TUN3D, the first method that tackles joint
layout estimation and 3D object detection in real scans, given multi-view
images as input, and does not require ground-truth camera poses or depth
supervision. Our approach builds on a lightweight sparse-convolutional backbone
and employs two dedicated heads: one for 3D object detection and one for layout
estimation, leveraging a novel and effective parametric wall representation.
Extensive experiments show that TUN3D achieves state-of-the-art performance
across three challenging scene understanding benchmarks: (i) using ground-truth
point clouds, (ii) using posed images, and (iii) using unposed images. While
performing on par with specialized 3D object detection methods, TUN3D
significantly advances layout estimation, setting a new benchmark in holistic
indoor scene understanding. Code is available at
https://github.com/col14m/tun3d .

</details>


### [58] [Large AI Model-Enabled Generative Semantic Communications for Image Transmission](https://arxiv.org/abs/2509.21394)
*Qiyu Ma,Wanli Ni,Zhijin Qin*

Main category: cs.CV

TL;DR: 提出一种生成式语义通信系统，通过区分图像关键与非关键区域（分别采用图像编码和图文建模压缩），并结合模型轻量化技术（量化与LoRA），有效提升了图像传输的语义保真度和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信方法忽视图像区域重要性差异，影响关键内容重建质量；同时，大型AI模型存在存储和计算开销大的问题。

Method: 设计了一种生成式语义通信系统，将图像分割为关键和非关键区域：关键区域使用图像导向语义编码器处理，非关键区域通过图文建模进行高效压缩。此外，采用模型量化和低秩适应微调（LoRA）实现系统轻量化部署。

Result: 仿真结果表明，所提出的系统在语义保真度和视觉质量方面均优于传统方法。

Conclusion: 该系统在图像传输任务中展现出显著的有效性。

Abstract: The rapid development of generative artificial intelligence (AI) has
introduced significant opportunities for enhancing the efficiency and accuracy
of image transmission within semantic communication systems. Despite these
advancements, existing methodologies often neglect the difference in importance
of different regions of the image, potentially compromising the reconstruction
quality of visually critical content. To address this issue, we introduce an
innovative generative semantic communication system that refines semantic
granularity by segmenting images into key and non-key regions. Key regions,
which contain essential visual information, are processed using an image
oriented semantic encoder, while non-key regions are efficiently compressed
through an image-to-text modeling approach. Additionally, to mitigate the
substantial storage and computational demands posed by large AI models, the
proposed system employs a lightweight deployment strategy incorporating model
quantization and low-rank adaptation fine-tuning techniques, significantly
boosting resource utilization without sacrificing performance. Simulation
results demonstrate that the proposed system outperforms traditional methods in
terms of both semantic fidelity and visual quality, thereby affirming its
effectiveness for image transmission tasks.

</details>


### [59] [mmHSense: Multi-Modal and Distributed mmWave ISAC Datasets for Human Sensing](https://arxiv.org/abs/2509.21396)
*Nabeel Nisar Bhat,Maksim Karnaukh,Stein Vandenbroeke,Wouter Lemoine,Jakob Struye,Jesus Omar Lacruz,Siddhartha Kumar,Mohammad Hossein Moghaddam,Joerg Widmer,Rafael Berkvens,Jeroen Famaey*

Main category: cs.CV

TL;DR: 本文提出了mmHSense，一个开放标记的毫米波数据集，用于支持集成感知与通信（ISAC）系统中的人体感知研究，并展示了其在特定任务上的实用性及参数高效微调的应用。


<details>
  <summary>Details</summary>
Motivation: 支持ISAC系统中人体感知研究，探索毫米波ISAC在手势识别、人员识别、姿态估计和定位等多种应用，并推动毫米波ISAC信号处理和深度学习研究的发展。

Method: 创建了一套名为mmHSense的开放标记毫米波数据集，详细描述了测试平台、实验设置和信号特征。通过在特定下游任务上进行验证，展示了数据集的实用性。此外，还通过参数高效微调来适应ISAC模型到不同任务。

Result: mmHSense数据集可用于探索毫米波ISAC在多种终端应用（如手势识别、人员识别、姿态估计和定位）中的潜力。参数高效微调能够显著降低计算复杂度，同时在原有任务上保持性能。

Conclusion: mmHSense数据集是毫米波ISAC人体感知研究的宝贵资源，能够支持多种应用开发和研究进展，并通过参数高效微调实现模型的高效适应。

Abstract: This article presents mmHSense, a set of open labeled mmWave datasets to
support human sensing research within Integrated Sensing and Communication
(ISAC) systems. The datasets can be used to explore mmWave ISAC for various end
applications such as gesture recognition, person identification, pose
estimation, and localization. Moreover, the datasets can be used to develop and
advance signal processing and deep learning research on mmWave ISAC. This
article describes the testbed, experimental settings, and signal features for
each dataset. Furthermore, the utility of the datasets is demonstrated through
validation on a specific downstream task. In addition, we demonstrate the use
of parameter-efficient fine-tuning to adapt ISAC models to different tasks,
significantly reducing computational complexity while maintaining performance
on prior tasks.

</details>


### [60] [Skeleton Sparsification and Densification Scale-Spaces](https://arxiv.org/abs/2509.21398)
*Julia Gierke,Pascal Peter*

Main category: cs.CV

TL;DR: 该研究引入了“骨架化尺度空间”框架，结合中轴线和稀疏化尺度空间，实现了形状的层次化简化，有效解决了传统中轴线对噪声敏感的问题，并提供了致密化扩展。


<details>
  <summary>Details</summary>
Motivation: 传统的Hamilton-Jacobi骨架（中轴线）作为强大的形状描述符，存在对噪声高度敏感的缺点，微小的边界变化会引发骨架不期望的膨胀。现有剪枝方法虽能缓解，但未能满足全面的尺度空间特性。

Method: 该研究提出了“骨架化尺度空间”，将中轴线与稀疏化尺度空间原理结合，通过稀疏化中轴线实现形状的层次化简化。该方法在连续和离散公式中都提供了严格的理论基础，并进一步扩展了“致密化”概念，允许从粗到细的逆向演进，甚至能生成过完备的形状表示。

Result: 所提出的框架固有地满足了关键的尺度空间特性，包括分层架构、可控简化和几何变换等变性。通过概念验证实验，证明了其在鲁棒骨架化、形状压缩和增材制造的刚度增强等实际任务中的有效性。

Conclusion: 骨架化尺度空间提供了一种克服传统中轴线噪声敏感性的新方法，通过层次化简化和致密化扩展，不仅增强了骨架的实用性，还为形状表示和分析开辟了新的可能性，具有重要的理论和实际应用价值。

Abstract: The Hamilton-Jacobi skeleton, also known as the medial axis, is a powerful
shape descriptor that represents binary objects in terms of the centres of
maximal inscribed discs. Despite its broad applicability, the medial axis
suffers from sensitivity to noise: minor boundary variations can lead to
disproportionately large and undesirable expansions of the skeleton. Classical
pruning methods mitigate this shortcoming by systematically removing extraneous
skeletal branches. This sequential simplification of skeletons resembles the
principle of sparsification scale-spaces that embed images into a family of
reconstructions from increasingly sparse pixel representations.
  We combine both worlds by introducing skeletonisation scale-spaces: They
leverage sparsification of the medial axis to achieve hierarchical
simplification of shapes. Unlike conventional pruning, our framework inherently
satisfies key scale-space properties such as hierarchical architecture,
controllable simplification, and equivariance to geometric transformations. We
provide a rigorous theoretical foundation in both continuous and discrete
formulations and extend the concept further with densification. This allows
inverse progression from coarse to fine scales and can even reach beyond the
original skeleton to produce overcomplete shape representations with relevancy
for practical applications.
  Through proof-of-concept experiments, we demonstrate the effectiveness of our
framework for practical tasks including robust skeletonisation, shape
compression, and stiffness enhancement for additive manufacturing.

</details>


### [61] [Downscaling climate projections to 1 km with single-image super resolution](https://arxiv.org/abs/2509.21399)
*Petr Košťál,Pavel Kordík,Ondřej Podsztavek*

Main category: cs.CV

TL;DR: 该研究利用单图像超分辨率模型将低分辨率气候预测统计降尺度至1公里，并通过气候指标进行评估，结果显示其在每日平均温度预测中未增加误差。


<details>
  <summary>Details</summary>
Motivation: 现有气候预测空间分辨率低（如12.5公里），限制了其在地方决策中的可用性，高分辨率气候预测对于地方决策至关重要。

Method: 利用单图像超分辨率模型对气候预测进行统计降尺度，目标分辨率为1公里。由于缺乏高分辨率气候预测数据用于训练，模型在高质量观测网格数据集上进行训练，然后应用于低分辨率气候预测。通过基于气候指标的评估方法，使用气象站观测到的气候指数来评估降尺度后的气候预测。

Result: 在每日平均温度的实验中，单图像超分辨率模型能够对气候预测进行降尺度，与低分辨率气候预测相比，并未增加气候指标的误差。

Conclusion: 单图像超分辨率模型可以有效地将气候预测降尺度，且在关键气候指标上保持了准确性，为获取高分辨率气候信息提供了可行方案。

Abstract: High-resolution climate projections are essential for local decision-making.
However, available climate projections have low spatial resolution (e.g. 12.5
km), which limits their usability. We address this limitation by leveraging
single-image super-resolution models to statistically downscale climate
projections to 1-km resolution. Since high-resolution climate projections are
unavailable for training, we train models on a high-resolution observational
gridded data set and apply them to low-resolution climate projections. We
propose a climate indicator-based assessment using observed climate indices
computed at weather station locations to evaluate the downscaled climate
projections without ground-truth high-resolution climate projections.
Experiments on daily mean temperature demonstrate that single-image
super-resolution models can downscale climate projections without increasing
the error of climate indicators compared to low-resolution climate projections.

</details>


### [62] [JaiLIP: Jailbreaking Vision-Language Models via Loss Guided Image Perturbation](https://arxiv.org/abs/2509.21401)
*Md Jueal Mia,M. Hadi Amini*

Main category: cs.CV

TL;DR: 本文提出JaiLIP，一种基于图像的VLM越狱攻击方法，通过联合损失生成有效且不可察觉的对抗图像，优于现有方法，并强调了图像越狱的挑战和防御需求。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）虽能力强大，但存在滥用和安全对齐问题，特别是图像扰动攻击。现有越狱方法性能不稳定且扰动可见。

Method: 提出JaiLIP（Loss-guided Image Perturbation），一种在图像空间进行的越狱攻击。该方法通过最小化结合了干净图像与对抗图像的均方误差（MSE）损失（用于 imperceptibility）和模型有害输出损失（用于攻击效果）的联合目标函数来生成对抗图像。使用Perspective API和Detoxify的标准毒性指标评估VLM。

Result: 实验结果表明，JaiLIP能生成高效且不可察觉的对抗图像，在产生毒性输出方面优于现有方法。此外，在交通领域验证了其攻击的实用性，不仅限于生成毒性文本。

Conclusion: 研究结果强调了基于图像的越狱攻击的实际挑战，并指出VLM需要高效的防御机制。

Abstract: Vision-Language Models (VLMs) have remarkable abilities in generating
multimodal reasoning tasks. However, potential misuse or safety alignment
concerns of VLMs have increased significantly due to different categories of
attack vectors. Among various attack vectors, recent studies have demonstrated
that image-based perturbations are particularly effective in generating harmful
outputs. In the literature, many existing techniques have been proposed to
jailbreak VLMs, leading to unstable performance and visible perturbations. In
this study, we propose Jailbreaking with Loss-guided Image Perturbation
(JaiLIP), a jailbreaking attack in the image space that minimizes a joint
objective combining the mean squared error (MSE) loss between clean and
adversarial image with the models harmful-output loss. We evaluate our proposed
method on VLMs using standard toxicity metrics from Perspective API and
Detoxify. Experimental results demonstrate that our method generates highly
effective and imperceptible adversarial images, outperforming existing methods
in producing toxicity. Moreover, we have evaluated our method in the
transportation domain to demonstrate the attacks practicality beyond toxic text
generation in specific domain. Our findings emphasize the practical challenges
of image-based jailbreak attacks and the need for efficient defense mechanisms
for VLMs.

</details>


### [63] [Overview of ExpertLifeCLEF 2018: how far automated identification systems are from the best experts?](https://arxiv.org/abs/2509.21419)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 研究对比了深度学习植物识别系统与人类专家的表现，发现当前AI系统已接近顶级人类专家水平。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习使植物动物自动识别取得巨大进展，但仍需量化这些系统与人类专业知识之间的差距，并比较其处理视觉/音频信息不确定性的能力。

Method: 通过LifeCLEF 2018 ExpertCLEF挑战赛，评估了4个研究团队开发的19个深度学习系统，并将其表现与9位法国植物学专家进行比较。

Result: 研究主要结果表明，最先进的深度学习模型在植物识别方面的表现已接近最顶尖的人类专业知识水平。

Conclusion: 自动化识别系统在性能上正逼近人类专家水平，该挑战赛为比较人类专家与自动化系统提供了宝贵资源和评估。

Abstract: Automated identification of plants and animals has improved considerably in
the last few years, in particular thanks to the recent advances in deep
learning. The next big question is how far such automated systems are from the
human expertise. Indeed, even the best experts are sometimes confused and/or
disagree between each others when validating visual or audio observations of
living organism. A picture actually contains only a partial information that is
usually not sufficient to determine the right species with certainty.
Quantifying this uncertainty and comparing it to the performance of automated
systems is of high interest for both computer scientists and expert
naturalists. The LifeCLEF 2018 ExpertCLEF challenge presented in this paper was
designed to allow this comparison between human experts and automated systems.
In total, 19 deep-learning systems implemented by 4 different research teams
were evaluated with regard to 9 expert botanists of the French flora. The main
outcome of this work is that the performance of state-of-the-art deep learning
models is now close to the most advanced human expertise. This paper presents
more precisely the resources and assessments of the challenge, summarizes the
approaches and systems employed by the participating research groups, and
provides an analysis of the main outcomes.

</details>


### [64] [QuadGPT: Native Quadrilateral Mesh Generation with Autoregressive Models](https://arxiv.org/abs/2509.21420)
*Jian Liu,Chunshi Wang,Song Guo,Haohan Weng,Zhen Zhou,Zhiqi Li,Jiaao Yu,Yiling Zhu,Jing Xu,Biwen Lei,Zhuo Chen,Chunchao Guo*

Main category: cs.CV

TL;DR: QuadGPT是首个端到端自回归框架，用于生成高质量的四边形网格，通过统一的token化和RL微调，显著优于现有三角转四边形方法。


<details>
  <summary>Details</summary>
Motivation: 现有四边形网格生成模型通过三角网格转换，导致拓扑质量差。迫切需要直接生成高质量四边形网格的方法。

Method: 引入QuadGPT，一个自回归框架，将四边形网格生成视为序列预测问题。关键创新包括：1) 统一的token化方法处理混合拓扑（三角形和四边形）；2) 专门的强化学习微调方法tDPO以提高生成质量。

Result: 大量实验表明，QuadGPT在几何精度和拓扑质量方面均显著超越了之前的三角转四边形转换流程。

Conclusion: QuadGPT为原生四边形网格生成建立了新基准，并展示了大型自回归模型与拓扑感知RL结合在创建结构化3D资产方面的强大能力。

Abstract: The generation of quadrilateral-dominant meshes is a cornerstone of
professional 3D content creation. However, existing generative models generate
quad meshes by first generating triangle meshes and then merging triangles into
quadrilaterals with some specific rules, which typically produces quad meshes
with poor topology. In this paper, we introduce QuadGPT, the first
autoregressive framework for generating quadrilateral meshes in an end-to-end
manner. QuadGPT formulates this as a sequence prediction paradigm,
distinguished by two key innovations: a unified tokenization method to handle
mixed topologies of triangles and quadrilaterals, and a specialized
Reinforcement Learning fine-tuning method tDPO for better generation quality.
Extensive experiments demonstrate that QuadGPT significantly surpasses previous
triangle-to-quad conversion pipelines in both geometric accuracy and
topological quality. Our work establishes a new benchmark for native quad-mesh
generation and showcases the power of combining large-scale autoregressive
models with topology-aware RL refinement for creating structured 3D assets.

</details>


### [65] [DyME: Dynamic Multi-Concept Erasure in Diffusion Models with Bi-Level Orthogonal LoRA Adaptation](https://arxiv.org/abs/2509.21433)
*Jiaqi Liu,Lan Zhang,Xiaoyong Yuan*

Main category: cs.CV

TL;DR: DyME是一种按需擦除框架，通过动态组合概念特定的LoRA适配器和引入双层正交性约束，实现了文本到图像扩散模型中可扩展、高保真度的多概念擦除，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型会无意中复制受版权保护的风格和视觉概念，引发法律和伦理问题。现有概念擦除方法不适用于实际场景中的多概念擦除，且缺乏灵活性，导致擦除效果不佳并降低非目标内容的保真度。

Method: 提出DyME按需擦除框架，训练轻量级、概念特定的LoRA适配器，并在推理时动态组合所需的适配器。为解决适配器间的干扰，引入了特征和参数层面的双层正交性约束。同时开发了分层基准测试ErasureBench-H用于评估。

Result: 在ErasureBench-H和标准数据集上，DyME始终优于现有基线方法，在多概念擦除保真度方面表现更优，同时最大限度地减少了附带的性能下降。

Conclusion: DyME通过其模块化设计和创新的正交性约束，为文本到图像扩散模型中多概念擦除提供了一种灵活、高效且可扩展的解决方案，有效解决了版权和伦理问题。

Abstract: Text-to-image diffusion models (DMs) inadvertently reproduce copyrighted
styles and protected visual concepts, raising legal and ethical concerns.
Concept erasure has emerged as a safeguard, aiming to selectively suppress such
concepts through fine-tuning. However, existing methods do not scale to
practical settings where providers must erase multiple and possibly conflicting
concepts. The core bottleneck is their reliance on static erasure: a single
checkpoint is fine-tuned to remove all target concepts, regardless of the
actual erasure needs at inference. This rigid design mismatches real-world
usage, where requests vary per generation, leading to degraded erasure success
and reduced fidelity for non-target content. We propose DyME, an on-demand
erasure framework that trains lightweight, concept-specific LoRA adapters and
dynamically composes only those needed at inference. This modular design
enables flexible multi-concept erasure, but naive composition causes
interference among adapters, especially when many or semantically related
concepts are suppressed. To overcome this, we introduce bi-level orthogonality
constraints at both the feature and parameter levels, disentangling
representation shifts and enforcing orthogonal adapter subspaces. We further
develop ErasureBench-H, a new hierarchical benchmark with
brand-series-character structure, enabling principled evaluation across
semantic granularities and erasure set sizes. Experiments on ErasureBench-H and
standard datasets (e.g., CIFAR-100, Imagenette) demonstrate that DyME
consistently outperforms state-of-the-art baselines, achieving higher
multi-concept erasure fidelity with minimal collateral degradation.

</details>


### [66] [VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding](https://arxiv.org/abs/2509.21451)
*Abdul Waheed,Zhen Wu,Dareen Alharthi,Seungone Kim,Bhiksha Raj*

Main category: cs.CV

TL;DR: 本文提出VideoJudge，一个3B/7B大小的MLLM判官，专门用于评估视频理解模型。通过生成器-评估器互动训练，VideoJudge-7B在多数基准测试中超越大型MLLM，并证实视频输入对视频理解评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型评估指标（如BLEU、ROUGE、BERTScore）无法捕捉人类判断的细微差别，而人工评估成本高昂。将大型语言模型（LLMs）或多模态LLMs（MLLMs）作为评估器应用于视频理解领域仍有待探索。

Method: 引入VideoJudge，一个3B和7B大小的MLLM判官，专用于评估视频理解模型产生的文本响应（以视频为条件）。其训练方法基于生成器和评估器之间的互动：生成器根据目标评分生成响应，而与评估器评分不匹配的响应将被舍弃。

Result: VideoJudge-7B在四项元评估基准中的三项上优于更大的MLLM判官基线（如Qwen2.5-VL 32B和72B）。研究还发现，LLM判官（Qwen3）表现不如MLLM判官（Qwen2.5-VL），并且长链式思维推理并未提升性能，这表明提供视频输入对于视频理解任务的评估至关重要。

Conclusion: VideoJudge是一个高效的视频理解模型评估工具，它通过利用多模态能力和特定的训练范式，显著提升了评估的准确性。研究结果强调了在评估视频理解任务时，直接提供视频输入的重要性，而非仅依赖文本信息进行推理。

Abstract: Precisely evaluating video understanding models remains challenging: commonly
used metrics such as BLEU, ROUGE, and BERTScore fail to capture the fineness of
human judgment, while obtaining such judgments through manual evaluation is
costly. Recent work has explored using large language models (LLMs) or
multimodal LLMs (MLLMs) as evaluators, but their extension to video
understanding remains relatively unexplored. In this work, we introduce
VideoJudge, a 3B and 7B-sized MLLM judge specialized to evaluate outputs from
video understanding models (\textit{i.e.}, text responses conditioned on
videos). To train VideoJudge, our recipe builds on the interplay between a
generator and an evaluator: the generator is prompted to produce responses
conditioned on a target rating, and responses not matching the evaluator's
rating are discarded. Across three out of four meta-evaluation benchmarks,
VideoJudge-7B outperforms larger MLLM judge baselines such as Qwen2.5-VL (32B
and 72B). Notably, we find that LLM judges (Qwen3) models perform worse than
MLLM judges (Qwen2.5-VL) and long chain-of-thought reasoning does not improve
performance, indicating that providing video inputs is crucial for evaluation
of video understanding tasks.

</details>


### [67] [Residual Vector Quantization For Communication-Efficient Multi-Agent Perception](https://arxiv.org/abs/2509.21464)
*Dereje Shenkut,B. V. K Vijaya Kumar*

Main category: cs.CV

TL;DR: ReVQom是一种学习型特征编解码器，通过残差向量量化技术大幅压缩多智能体协同感知中的特征数据，同时保持高精度，实现超低带宽通信，助力V2X实际部署。


<details>
  <summary>Details</summary>
Motivation: 多智能体协同感知（CP）通过信息共享提升场景理解能力，但通信带宽限制了其可扩展性，急需高效的特征压缩方法。

Method: ReVQom是一种端到端方法，通过一个简单的瓶颈网络压缩特征维度，随后采用多阶段残差向量量化（RVQ）技术，仅传输每像素码索引，从而实现中间特征的压缩。

Result: 在DAIR-V2X数据集上，ReVQom将未压缩特征的8192 bpp减少到每智能体6-30 bpp，实现273倍（30 bpp）至1365倍（6 bpp）的压缩。在18 bpp（455倍压缩）时，其性能与原始特征CP相当或更优；在6-12 bpp下，可在超低带宽操作中实现优雅的性能降级。

Conclusion: ReVQom能够实现高效且准确的多智能体协同感知，是迈向实际V2X部署的关键一步，解决了通信带宽瓶颈问题。

Abstract: Multi-agent collaborative perception (CP) improves scene understanding by
sharing information across connected agents such as autonomous vehicles,
unmanned aerial vehicles, and robots. Communication bandwidth, however,
constrains scalability. We present ReVQom, a learned feature codec that
preserves spatial identity while compressing intermediate features. ReVQom is
an end-to-end method that compresses feature dimensions via a simple bottleneck
network followed by multi-stage residual vector quantization (RVQ). This allows
only per-pixel code indices to be transmitted, reducing payloads from 8192 bits
per pixel (bpp) of uncompressed 32-bit float features to 6-30 bpp per agent
with minimal accuracy loss. On DAIR-V2X real-world CP dataset, ReVQom achieves
273x compression at 30 bpp to 1365x compression at 6 bpp. At 18 bpp (455x),
ReVQom matches or outperforms raw-feature CP, and at 6-12 bpp it enables
ultra-low-bandwidth operation with graceful degradation. ReVQom allows
efficient and accurate multi-agent collaborative perception with a step toward
practical V2X deployment.

</details>


### [68] [Gender Stereotypes in Professional Roles Among Saudis: An Analytical Study of AI-Generated Images Using Language Models](https://arxiv.org/abs/2509.21466)
*Khaloud S. AlKhalifah,Malak Mashaabi,Hend Al-Khalifa*

Main category: cs.CV

TL;DR: 研究发现，主流文生图AI模型在生成沙特专业人士图像时，存在严重的性别偏见（尤其是DALL-E V3）和文化不准确性，反映了训练数据中的社会偏见。


<details>
  <summary>Details</summary>
Motivation: 探讨当代文生图AI模型在生成沙特专业人士图像时，在多大程度上延续了性别刻板印象和文化不准确性。

Method: 分析了ImageFX、DALL-E V3和Grok模型为56个沙特职业生成的1,006张图像，使用中性提示词。两名受训的沙特标注员根据性别感知、服装外观、背景环境、活动互动和年龄五个维度进行评估，共计10,100个判断，由第三位高级研究员仲裁分歧。

Result: 结果显示强烈的性别不平衡：ImageFX输出85%为男性，Grok 86.6%为男性，DALL-E V3高达96%为男性，DALL-E V3的性别刻板印象最强，尤其体现在领导和技术岗位。此外，所有模型在服装、背景和活动中频繁出现文化不准确性，且反刻板印象图像常源于文化误解而非真正的进步描绘。

Conclusion: 当前模型反映了其训练数据中存在的社会偏见，由人类生成，对沙特劳动力市场的性别动态和文化细微差别反映有限。因此，迫切需要更多样化的训练数据、更公平的算法和文化敏感的评估框架，以确保公平真实的视觉输出。

Abstract: This study investigates the extent to which contemporary Text-to-Image
artificial intelligence (AI) models perpetuate gender stereotypes and cultural
inaccuracies when generating depictions of professionals in Saudi Arabia. We
analyzed 1,006 images produced by ImageFX, DALL-E V3, and Grok for 56 diverse
Saudi professions using neutral prompts. Two trained Saudi annotators evaluated
each image on five dimensions: perceived gender, clothing and appearance,
background and setting, activities and interactions, and age. A third senior
researcher adjudicated whenever the two primary raters disagreed, yielding
10,100 individual judgements. The results reveal a strong gender imbalance,
with ImageFX outputs being 85\% male, Grok 86.6\% male, and DALL-E V3 96\%
male, indicating that DALL-E V3 exhibited the strongest overall gender
stereotyping. This imbalance was most evident in leadership and technical
roles. Moreover, cultural inaccuracies in clothing, settings, and depicted
activities were frequently observed across all three models.
Counter-stereotypical images often arise from cultural misinterpretations
rather than genuinely progressive portrayals. We conclude that current models
mirror societal biases embedded in their training data, generated by humans,
offering only a limited reflection of the Saudi labour market's gender dynamics
and cultural nuances. These findings underscore the urgent need for more
diverse training data, fairer algorithms, and culturally sensitive evaluation
frameworks to ensure equitable and authentic visual outputs.

</details>


### [69] [Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large Language Models for Short Video Content Moderation](https://arxiv.org/abs/2509.21486)
*Zixuan Wang,Yu Sun,Hongwei Wang,Baoyu Jing,Xiang Shen,Xin Dong,Zhuolin Hao,Hongyu Xiong,Yang Song*

Main category: cs.CV

TL;DR: 本文提出一种推理增强的多模态大语言模型(MLLM)预训练范式，用于统一短视频平台上的不当内容检测，克服了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 短视频平台内容识别日益关键，现有方法为每种问题训练单独模型，需大量标注数据，且缺乏跨问题泛化能力。

Method: 提出一种推理增强的多模态大语言模型(MLLM)预训练范式，旨在统一不当内容检测。为解决短视频内容与MLLM原始预训练数据之间的分布差距及复杂问题定义，引入了三个有针对性的预训练任务：Caption（增强视频细节感知）、VQA（加深对问题定义和标注指南的理解）和Chain-of-Thought（增强推理能力）。

Result: 实验结果表明，该预训练方法显著提升了MLLM在零样本和监督微调设置下的性能。此外，预训练模型对新兴的、以前未见过的问题展现出强大的泛化能力。

Conclusion: 所提出的推理增强MLLM预训练范式有效统一了不当内容检测，显著提升了模型性能，并展现出强大的泛化能力，解决了短视频内容审核的关键挑战。

Abstract: Short video platforms are evolving rapidly, making the identification of
inappropriate content increasingly critical. Existing approaches typically
train separate and small classification models for each type of issue, which
requires extensive human-labeled data and lacks cross-issue generalization. We
propose a reasoning-enhanced multimodal large language model (MLLM) pretraining
paradigm for unified inappropriate content detection. To address the
distribution gap between short video content and the original pretraining data
of MLLMs, as well as the complex issue definitions, we introduce three targeted
pretraining tasks: (1) \textit{Caption}, to enhance the MLLM's perception of
video details; (2) \textit{Visual Question Answering (VQA)}, to deepen the
MLLM's understanding of issue definitions and annotation guidelines; (3)
\textit{Chain-of-Thought (CoT)}, to enhance the MLLM's reasoning capability.
Experimental results show that our pretraining approach significantly improves
the MLLM's performance in both zero-shot and supervised fine-tuning (SFT)
settings. In addition, our pretrained model demonstrates strong generalization
capabilities to emergent, previously unseen issues.

</details>


### [70] [Learning GUI Grounding with Spatial Reasoning from Visual Feedback](https://arxiv.org/abs/2509.21552)
*Yu Zhao,Wei-Ning Chen,Huseyin Atahan Inan,Samuel Kessler,Lu Wang,Lukas Wutschitz,Fangkai Yang,Chaoyun Zhang,Pasquale Minervini,Saravan Rajmohan,Robert Sim*

Main category: cs.CV

TL;DR: 现有GUI定位方法在复杂高分辨率界面上坐标预测不准确。本文提出GUI-Cursor，将GUI定位重构为交互式光标搜索任务，利用强化学习训练，显著提升了定位准确性，并在多个基准测试上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: GUI定位通常被视为坐标预测任务，但现有视觉语言模型（VLMs）在处理高分辨率、复杂布局的GUI图像时，难以预测准确的数值坐标。

Method: 将GUI定位重构为“交互式搜索任务”，VLM生成动作来移动GUI中的光标以定位UI元素。在每一步中，模型确定目标、评估光标与目标的空间关系并移动光标。渲染的光标提供视觉反馈。模型GUI-Cursor通过多步在线强化学习和密集的基于轨迹的奖励函数进行训练。

Result: GUI-Cursor（基于Qwen2.5-VL-7B）显著提高了GUI定位精度，并在ScreenSpot-v2上从88.8%提升到93.9%，在ScreenSpot-Pro上从26.8%提升到56.5%，达到最先进水平。模型能在95%的实例中两步内解决问题，并能自适应地处理更困难的示例。

Conclusion: 将GUI定位重构为交互式光标搜索任务，并结合强化学习训练，能有效解决复杂GUI图像上的坐标预测不准确问题，显著提升GUI定位的准确性和鲁棒性。

Abstract: Graphical User Interface (GUI) grounding is commonly framed as a coordinate
prediction task -- given a natural language instruction, generate on-screen
coordinates for actions such as clicks and keystrokes. However, recent Vision
Language Models (VLMs) often fail to predict accurate numeric coordinates when
processing high-resolution GUI images with complex layouts. To address this
issue, we reframe GUI grounding as an \emph{interactive search task}, where the
VLM generates actions to move a cursor in the GUI to locate UI elements. At
each step, the model determines the target object, evaluates the spatial
relations between the cursor and the target, and moves the cursor closer to the
target conditioned on the movement history. In this interactive process, the
rendered cursor provides visual feedback to help the model align its
predictions with the corresponding on-screen locations. We train our GUI
grounding model, GUI-Cursor, using multi-step online reinforcement learning
with a dense trajectory-based reward function. Our experimental results show
that GUI-Cursor, based on Qwen2.5-VL-7B, improves the GUI grounding accuracy
and achieves state-of-the-art results on ScreenSpot-v2 ($88.8\% \rightarrow
93.9\%$) and ScreenSpot-Pro ($26.8\% \rightarrow 56.5\%$). Moreover, we observe
that GUI-Cursor learns to solve the problem within two steps for 95\% of
instances and can adaptively conduct more steps on more difficult examples.

</details>


### [71] [X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning](https://arxiv.org/abs/2509.21559)
*Prasanna Reddy Pulakurthi,Jiamian Wang,Majid Rabbani,Sohail Dianat,Raghuveer Rao,Zhiqiang Tao*

Main category: cs.CV

TL;DR: X-CoT是一个基于LLM CoT推理的可解释文本到视频检索框架，通过扩展数据集和设计检索CoT，提高了检索性能并增强了结果的可解释性，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频检索系统存在两个主要局限：低质量文本-视频数据对会损害检索结果且难以识别；仅凭余弦相似度无法解释排名结果，限制了模型的可解释性。研究动机是探究如何解释排名结果，以评估检索模型和检查数据质量。

Method: 本工作提出了X-CoT框架，利用LLM CoT推理取代基于嵌入模型的相似度排名。具体方法包括：扩展现有基准数据集，增加视频标注以支持语义理解并减少数据偏差；设计一种包含成对比较步骤的检索CoT（Chain-of-Thought），以生成详细推理和完整的排名。

Result: X-CoT在经验上提升了检索性能，并能够生成详细的解释理由。它还有助于模型行为和数据质量的分析。

Conclusion: X-CoT成功提供了一个可解释的文本到视频检索框架，通过LLM CoT推理解决了现有方法的局限性，实现了性能提升和可解释性增强，并能够促进模型行为和数据质量分析。

Abstract: Prevalent text-to-video retrieval systems mainly adopt embedding models for
feature extraction and compute cosine similarities for ranking. However, this
design presents two limitations. Low-quality text-video data pairs could
compromise the retrieval, yet are hard to identify and examine. Cosine
similarity alone provides no explanation for the ranking results, limiting the
interpretability. We ask that can we interpret the ranking results, so as to
assess the retrieval models and examine the text-video data? This work proposes
X-CoT, an explainable retrieval framework upon LLM CoT reasoning in place of
the embedding model-based similarity ranking. We first expand the existing
benchmarks with additional video annotations to support semantic understanding
and reduce data bias. We also devise a retrieval CoT consisting of pairwise
comparison steps, yielding detailed reasoning and complete ranking. X-CoT
empirically improves the retrieval performance and produces detailed
rationales. It also facilitates the model behavior and data quality analysis.
Code and data are available at: https://github.com/PrasannaPulakurthi/X-CoT.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [72] [Towards mitigating information leakage when evaluating safety monitors](https://arxiv.org/abs/2509.21344)
*Gerard Boxo,Aman Neelappa,Shivam Raval*

Main category: cs.AI

TL;DR: 本文提出一个系统框架和三种新策略（内容过滤、分数过滤、微调模型有机体），以解决白盒监测器在评估大型语言模型有害行为时因“信息泄露”导致的性能虚高问题，并通过实验证明这些策略能更准确地评估监测器，并揭示其真实性能存在显著下降。


<details>
  <summary>Details</summary>
Motivation: 白盒监测器在检测大型语言模型（LLMs）有害行为方面具有潜力，但其训练和评估过程中，用于诱发目标行为的信息（如提示）会不可避免地“泄露”到监测数据中，从而夸大监测器的有效性。因此，需要一个系统框架来评估监测器检测真实模型行为的能力，而非表面诱导产物。

Method: 本文提出了一个系统框架，用于评估监测器检测真实模型行为而非诱导伪影的能力。具体引入了三种新颖的评估策略：1) 内容过滤（从输入中移除与欺骗相关的文本）；2) 分数过滤（仅聚合与任务相关的token分数）；3) 提示蒸馏微调的模型有机体（训练模型在无显式提示下展现欺骗行为）。以欺骗检测为案例研究，识别出诱导泄漏和推理泄漏两种形式，并应用缓解策略进行实验。

Result: 研究发现：1) 内容过滤是一种有效的缓解策略，可使探针AUROC降低30%；2) 分数过滤可使AUROC降低15%，但其归因不甚直接；3) 微调模型有机体改善了监测器评估，但即使重新训练，也会将其性能降低高达40%。

Conclusion: 本研究揭示了现有白盒监测器在评估大型语言模型有害行为时，其性能可能因诱导和推理泄漏而被高估。提出的评估框架和缓解策略（特别是内容过滤和使用微调模型有机体）能够更准确地评估监测器的真实能力，表明其在无泄漏情况下的实际性能可能远低于此前预期。

Abstract: White box monitors that analyze model internals offer promising advantages
for detecting potentially harmful behaviors in large language models, including
lower computational costs and integration into layered defense systems.However,
training and evaluating these monitors requires response exemplars that exhibit
the target behaviors, typically elicited through prompting or fine-tuning. This
presents a challenge when the information used to elicit behaviors inevitably
leaks into the data that monitors ingest, inflating their effectiveness. We
present a systematic framework for evaluating a monitor's performance in terms
of its ability to detect genuine model behavior rather than superficial
elicitation artifacts. Furthermore, we propose three novel strategies to
evaluate the monitor: content filtering (removing deception-related text from
inputs), score filtering (aggregating only over task-relevant tokens), and
prompt distilled fine-tuned model organisms (models trained to exhibit
deceptive behavior without explicit prompting). Using deception detection as a
representative case study, we identify two forms of leakage that inflate
monitor performance: elicitation leakage from prompts that explicitly request
harmful behavior, and reasoning leakage from models that verbalize their
deceptive actions. Through experiments on multiple deception benchmarks, we
apply our proposed mitigation strategies and measure performance retention. Our
evaluation of the monitors reveal three crucial findings: (1) Content filtering
is a good mitigation strategy that allows for a smooth removal of elicitation
signal and can decrease probe AUROC by 30\% (2) Score filtering was found to
reduce AUROC by 15\% but is not as straightforward to attribute to (3) A
finetuned model organism improves monitor evaluations but reduces their
performance by upto 40\%, even when re-trained.

</details>


### [73] [Correct Reasoning Paths Visit Shared Decision Pivots](https://arxiv.org/abs/2509.21549)
*Dongkyu Cho,Amy B. Z. Zhang,Bilel Fehri,Sheng Wang,Rumi Chunara,Rui Song,Hengrui Cai*

Main category: cs.AI

TL;DR: 本文提出“决策支点”作为可验证的思维过程检查点，以解决大规模验证大语言模型（LLMs）链式思考（CoT）推理痕迹的难题，并开发了一种基于这些支点的自训练方法。


<details>
  <summary>Details</summary>
Motivation: 链式思考（CoT）推理虽然暴露了LLMs的中间思维过程，但大规模验证这些痕迹仍是未解决的挑战。

Method: 引入“决策支点”概念（任何正确推理路径必须访问的最小可验证检查点），并假设正确推理路径会收敛于相同支点集。提出自训练流程：(i) 采样多样推理路径并挖掘共享决策支点，(ii) 使用辅助验证器将每条痕迹压缩成以支点为中心的短路径推理，(iii) 使用自生成输出对模型进行后期训练。该方法无需真实推理数据或外部指标即可对齐推理。

Result: 在LogiQA、MedQA和MATH500等标准基准测试上进行的实验证明了该方法的有效性。

Conclusion: 该方法在没有真实推理数据或外部指标的情况下，实现了推理的对齐。

Abstract: Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of
large language models (LLMs), yet verifying those traces at scale remains
unsolved. In response, we introduce the idea of decision pivots-minimal,
verifiable checkpoints that any correct reasoning path must visit. We
hypothesize that correct reasoning, though stylistically diverse, converge on
the same pivot set, while incorrect ones violate at least one pivot. Leveraging
this property, we propose a self-training pipeline that (i) samples diverse
reasoning paths and mines shared decision pivots, (ii) compresses each trace
into pivot-focused short-path reasoning using an auxiliary verifier, and (iii)
post-trains the model using its self-generated outputs. The proposed method
aligns reasoning without ground truth reasoning data or external metrics.
Experiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the
effectiveness of our method.

</details>


### [74] [AutoClimDS: Climate Data Science Agentic AI -- A Knowledge Graph is All You Need](https://arxiv.org/abs/2509.21553)
*Ahmed Jaber,Wangshu Zhu,Karthick Jayavelu,Justin Downes,Sameer Mohamed,Candace Agonafir,Linnia Hawkins,Tian Zheng*

Main category: cs.AI

TL;DR: 本文提出将知识图谱与AI代理相结合，以克服气候数据科学中数据分散、格式异构及高技术门槛等问题，从而降低非专业用户参与科学研究的门槛，并实现数据发现与分析的民主化。


<details>
  <summary>Details</summary>
Motivation: 气候数据科学面临数据源碎片化、格式不统一以及识别、获取和处理数据集所需技术专业知识门槛高的挑战。这些问题限制了研究参与、减缓了发现进程，并降低了科学工作流的可复现性。

Method: 本文提出了一个概念验证，通过整合一个精心策划的知识图谱（KG）与专为云原生科学工作流设计的AI代理来解决这些障碍。知识图谱提供了一个统一层，用于组织数据集、工具和工作流；而由生成式AI服务驱动的AI代理则实现了自然语言交互、自动化数据访问和简化分析。

Result: 这些组件共同显著降低了参与气候数据科学的技术门槛，使非专业用户能够识别和分析相关数据集。通过利用现有的云就绪API数据门户，研究证明“只需知识图谱”即可解锁可扩展的代理科学探究工作流。此外，系统的开源设计也支持社区贡献。

Conclusion: 研究结果展示了一条通向气候数据访问民主化的路径，并为科学研究中人机协作建立了一个可复现、可扩展的框架。

Abstract: Climate data science faces persistent barriers stemming from the fragmented
nature of data sources, heterogeneous formats, and the steep technical
expertise required to identify, acquire, and process datasets. These challenges
limit participation, slow discovery, and reduce the reproducibility of
scientific workflows. In this paper, we present a proof of concept for
addressing these barriers through the integration of a curated knowledge graph
(KG) with AI agents designed for cloud-native scientific workflows. The KG
provides a unifying layer that organizes datasets, tools, and workflows, while
AI agents -- powered by generative AI services -- enable natural language
interaction, automated data access, and streamlined analysis. Together, these
components drastically lower the technical threshold for engaging in climate
data science, enabling non-specialist users to identify and analyze relevant
datasets. By leveraging existing cloud-ready API data portals, we demonstrate
that "a knowledge graph is all you need" to unlock scalable and agentic
workflows for scientific inquiry. The open-source design of our system further
supports community contributions, ensuring that the KG and associated tools can
evolve as a shared commons. Our results illustrate a pathway toward
democratizing access to climate data and establishing a reproducible,
extensible framework for human--AI collaboration in scientific research.

</details>


### [75] [EEG-Based Consumer Behaviour Prediction: An Exploration from Classical Machine Learning to Graph Neural Networks](https://arxiv.org/abs/2509.21567)
*Mohammad Parsa Afshar,Aryan Azimi*

Main category: cs.AI

TL;DR: 本研究利用EEG数据，比较了传统机器学习模型和图神经网络（GNN）在预测消费者行为方面的性能，发现GNN在某些基础准则上表现更优，为深入理解消费者行为提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 预测消费者行为是营销、认知神经科学和人机交互领域的重要目标。脑电图（EEG）数据能提供大脑神经活动的详细信息，有助于分析决策过程。

Method: 研究采用比较方法，首先从NeuMa数据集中提取并清洗EEG数据特征，并为图神经网络（GNN）模型创建大脑连接特征。随后，使用包括集成模型在内的多种经典机器学习模型和不同架构的GNN模型进行预测和比较。

Result: 研究结果显示，尽管总体上模型间没有显著差异，但在某些基本准则上，GNN模型普遍表现优于传统模型，而传统模型在这些方面表现不尽如人意。

Conclusion: 本研究不仅表明结合EEG信号分析和机器学习模型能够更深入地理解消费者行为，还全面比较了神经营销领域常用（如SVM）和不常用（如GNN）的机器学习模型。

Abstract: Prediction of consumer behavior is one of the important purposes in
marketing, cognitive neuroscience, and human-computer interaction. The
electroencephalography (EEG) data can help analyze the decision process by
providing detailed information about the brain's neural activity. In this
research, a comparative approach is utilized for predicting consumer behavior
by EEG data. In the first step, the features of the EEG data from the NeuMa
dataset were extracted and cleaned. For the Graph Neural Network (GNN) models,
the brain connectivity features were created. Different machine learning
models, such as classical models and Graph Neural Networks, are used and
compared. The GNN models with different architectures are implemented to have a
comprehensive comparison; furthermore, a wide range of classical models, such
as ensemble models, are applied, which can be very helpful to show the
difference and performance of each model on the dataset. Although the results
did not show a significant difference overall, the GNN models generally
performed better in some basic criteria where classical models were not
satisfactory. This study not only shows that combining EEG signal analysis and
machine learning models can provide an approach to deeper understanding of
consumer behavior, but also provides a comprehensive comparison between the
machine learning models that have been widely used in previous studies in the
EEG-based neuromarketing such as Support Vector Machine (SVM), and the models
which are not used or rarely used in the field, like Graph Neural Networks.

</details>


### [76] [GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models](https://arxiv.org/abs/2509.21593)
*Peng Luo,Xiayin Lou,Yu Zheng,Zhuo Zheng,Stefano Ermon*

Main category: cs.AI

TL;DR: GeoEvolve是一个多智能体LLM框架，结合进化搜索和地理空间领域知识，自动设计和优化地理空间算法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM算法发现框架（如AlphaEvolve）擅长通用代码演化，但缺乏处理复杂地理空间问题所需的领域知识和多步骤推理能力。

Method: GeoEvolve采用多智能体LLM框架，包含两个嵌套循环：内循环使用代码演化器生成和变异候选解决方案；外循环代理控制器评估全局最优解并查询GeoKnowRAG模块（一个结构化的地理空间知识库），注入地理理论先验知识，指导搜索生成理论上合理且计算高效的算法。

Result: GeoEvolve在空间插值（克里金）和空间不确定性量化任务上，自动改进并发现新算法，将空间插值误差（RMSE）降低13-21%，不确定性估计性能提高17%。消融研究证实领域引导检索对稳定高质量的演化至关重要。

Conclusion: GeoEvolve为自动化、知识驱动的地理空间建模提供了可扩展途径，为可信赖、高效的科学AI发现开辟了新机遇。

Abstract: Geospatial modeling provides critical solutions for pressing global
challenges such as sustainability and climate change. Existing large language
model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at
evolving generic code but lack the domain knowledge and multi-step reasoning
required for complex geospatial problems. We introduce GeoEvolve, a multi-agent
LLM framework that couples evolutionary search with geospatial domain knowledge
to automatically design and refine geospatial algorithms. GeoEvolve operates in
two nested loops: an inner loop leverages a code evolver to generate and mutate
candidate solutions, while an outer agentic controller evaluates global elites
and queries a GeoKnowRAG module -- a structured geospatial knowledge base that
injects theoretical priors from geography. This knowledge-guided evolution
steers the search toward theoretically meaningful and computationally efficient
algorithms. We evaluate GeoEvolve on two fundamental and classical tasks:
spatial interpolation (kriging) and spatial uncertainty quantification
(geospatial conformal prediction). Across these benchmarks, GeoEvolve
automatically improves and discovers new algorithms, incorporating geospatial
theory on top of classical models. It reduces spatial interpolation error
(RMSE) by 13-21% and enhances uncertainty estimation performance by 17\%.
Ablation studies confirm that domain-guided retrieval is essential for stable,
high-quality evolution. These results demonstrate that GeoEvolve provides a
scalable path toward automated, knowledge-driven geospatial modeling, opening
new opportunities for trustworthy and efficient AI-for-Science discovery.

</details>


### [77] [Automated and Interpretable Survival Analysis from Multimodal Data](https://arxiv.org/abs/2509.21600)
*Mafalda Malafaia,Peter A. N. Bosman,Coen Rasch,Tanja Alderliesten*

Main category: cs.AI

TL;DR: 提出一种可解释的多模态AI框架MultiFIX，整合临床变量和CT影像，用于肿瘤生存分析，并在头颈癌数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 肿瘤学中准确且可解释的生存分析是一个核心挑战，多模态数据的增长以及临床对透明模型的需求进一步增加了这一挑战的复杂性。

Method: 开发了MultiFIX框架，整合临床变量和CT影像进行生存分析。该框架使用深度学习推断生存相关特征，并通过Grad-CAM解释影像特征，通过遗传编程将临床变量建模为符号表达式。风险估计采用透明的Cox回归，实现患者分层。

Result: 在RADCURE头颈癌数据集上，MultiFIX的C-index（预测）达到0.838，C-index（分层）达到0.826。这优于临床和学术基线方法，并且与已知预后标志物相符。

Conclusion: MultiFIX表明可解释的多模态AI在精准肿瘤学领域具有良好的应用前景。

Abstract: Accurate and interpretable survival analysis remains a core challenge in
oncology. With growing multimodal data and the clinical need for transparent
models to support validation and trust, this challenge increases in complexity.
We propose an interpretable multimodal AI framework to automate survival
analysis by integrating clinical variables and computed tomography imaging. Our
MultiFIX-based framework uses deep learning to infer survival-relevant features
that are further explained: imaging features are interpreted via Grad-CAM,
while clinical variables are modeled as symbolic expressions through genetic
programming. Risk estimation employs a transparent Cox regression, enabling
stratification into groups with distinct survival outcomes. Using the
open-source RADCURE dataset for head and neck cancer, MultiFIX achieves a
C-index of 0.838 (prediction) and 0.826 (stratification), outperforming the
clinical and academic baseline approaches and aligning with known prognostic
markers. These results highlight the promise of interpretable multimodal AI for
precision oncology with MultiFIX.

</details>


### [78] [Semantic F1 Scores: Fair Evaluation Under Fuzzy Class Boundaries](https://arxiv.org/abs/2509.21633)
*Georgios Chochlakis,Jackson Trager,Vedant Jhaveri,Nikhil Ravichandran,Alexandros Potamianos,Shrikanth Narayanan*

Main category: cs.AI

TL;DR: 提出语义F1分数，一种新的评估指标，用于主观或模糊的多标签分类，通过量化预测和真实标签间的语义关联性，实现更公平、可解释的评估。


<details>
  <summary>Details</summary>
Motivation: 传统F1指标将语义相关但非完全相同的预测视为完全错误，未能反映主观或模糊领域中人类分歧和类别重叠的现实，需要更公平、更具生态有效性的评估方法。

Method: 提出语义F1分数，通过引入标签相似性矩阵计算软性的精确率和召回率。采用新颖的两步精确率-召回率公式，能够在不丢弃标签或强制匹配不相似标签的情况下，比较任意大小的标签集。

Result: 语义F1能为语义相关但非完全相同的标签提供部分分数，更好地反映主观领域的现实。经验证，它提供了更公平的评估，并展现出更高的可解释性和生态有效性。

Conclusion: 语义F1仅需要一个领域适用的相似性矩阵（对错误规范鲁棒），而无需严格的本体论，因此适用于各种任务和模态。

Abstract: We propose Semantic F1 Scores, novel evaluation metrics for subjective or
fuzzy multi-label classification that quantify semantic relatedness between
predicted and gold labels. Unlike the conventional F1 metrics that treat
semantically related predictions as complete failures, Semantic F1 incorporates
a label similarity matrix to compute soft precision-like and recall-like
scores, from which the Semantic F1 scores are derived. Unlike existing
similarity-based metrics, our novel two-step precision-recall formulation
enables the comparison of label sets of arbitrary sizes without discarding
labels or forcing matches between dissimilar labels. By granting partial credit
for semantically related but nonidentical labels, Semantic F1 better reflects
the realities of domains marked by human disagreement or fuzzy category
boundaries. In this way, it provides fairer evaluations: it recognizes that
categories overlap, that annotators disagree, and that downstream decisions
based on similar predictions lead to similar outcomes. Through theoretical
justification and extensive empirical validation on synthetic and real data, we
show that Semantic F1 demonstrates greater interpretability and ecological
validity. Because it requires only a domain-appropriate similarity matrix,
which is robust to misspecification, and not a rigid ontology, it is applicable
across tasks and modalities.

</details>


### [79] [Can AI Perceive Physical Danger and Intervene?](https://arxiv.org/abs/2509.21651)
*Abhishek Jindal,Dmitry Kalashnikov,Oscar Chang,Divya Garikapati,Anirudha Majumdar,Pierre Sermanet,Vikas Sindhwani*

Main category: cs.AI

TL;DR: 本文开发了一个可扩展的具身AI物理安全基准，并分析了主流基础模型在此领域的表现。同时，提出了一种后训练范式，显著提升了模型在物理安全约束推理上的可解释性和最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当AI以机器人或辅助代理形式与物理世界交互时，存在直接且即时的物理伤害风险。研究动机在于探究最先进的基础模型对物理安全常识（如物体重量或热饮风险）的理解能力。

Method: 1. 开发了一个可扩展的具身AI物理安全基准，该基准基于真实世界伤害叙述和操作安全约束，并使用生成模型将其转化为捕捉安全到不安全状态转变的逼真图像和视频。
2. 全面分析了主流基础模型感知风险、推理安全和触发干预的能力。
3. 开发了一种后训练范式，通过系统指令教导模型明确推理具身特定的安全约束，使安全推理过程可解释和透明。

Result: 1. 对主流基础模型的分析提供了关于其在安全关键型代理应用部署就绪程度的多方面洞察。
2. 通过后训练范式训练的模型能够生成可解释和透明的安全推理痕迹，并在约束满足评估中达到了最先进的性能。

Conclusion: 本研究为具身AI的物理安全评估提供了重要的基准和分析工具，并通过创新的后训练范式，显著提高了模型处理复杂物理安全约束的推理能力、透明度及性能，对安全关键型具身AI的部署具有重要意义。

Abstract: When AI interacts with the physical world -- as a robot or an assistive agent
-- new safety challenges emerge beyond those of purely ``digital AI". In such
interactions, the potential for physical harm is direct and immediate. How well
do state-of-the-art foundation models understand common-sense facts about
physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of
coffee should not be handed to a child? In this paper, our contributions are
three-fold: first, we develop a highly scalable approach to continuous physical
safety benchmarking of Embodied AI systems, grounded in real-world injury
narratives and operational safety constraints. To probe multi-modal safety
understanding, we turn these narratives and constraints into photorealistic
images and videos capturing transitions from safe to unsafe states, using
advanced generative models. Secondly, we comprehensively analyze the ability of
major foundation models to perceive risks, reason about safety, and trigger
interventions; this yields multi-faceted insights into their deployment
readiness for safety-critical agentic applications. Finally, we develop a
post-training paradigm to teach models to explicitly reason about
embodiment-specific safety constraints provided through system instructions.
The resulting models generate thinking traces that make safety reasoning
interpretable and transparent, achieving state of the art performance in
constraint satisfaction evaluations. The benchmark will be released at
https://asimov-benchmark.github.io/v2

</details>


### [80] [Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization](https://arxiv.org/abs/2509.21718)
*Shehzeen Hussain,Paarth Neekhara,Xuesong Yang,Edresson Casanova,Subhankar Ghosh,Roy Fejgin,Ryan Langman,Mikyas Desta,Leili Tavabi,Jason Li*

Main category: cs.AI

TL;DR: 提出了一种基于GRPO的框架，通过多任务奖励信号（ASR、说话人验证和音频质量估计）利用无配对数据，将多语言TTS模型有效适应到低资源语言，并优于传统微调和DPO方法。


<details>
  <summary>Details</summary>
Motivation: 为低资源语言开发高质量文本转语音（TTS）系统面临配对文本和语音数据稀缺的挑战，而这些语言的自动语音识别（ASR）模型由于大规模多语言预训练而相对更易获得。

Method: 该方法首先通过IPA（国际音标）训练多语言基线TTS模型以建立语言无关的基础。接着，利用有限的配对数据对新语言进行微调以捕捉韵律特征。最后，应用Group Relative Policy Optimization (GRPO)，仅使用无配对文本和说话人提示，通过预训练ASR、说话人验证和音频质量估计模型的多目标奖励来优化模型。

Result: 实验证明，该方法在低资源语言中生成了可理解且说话人一致的语音，显著优于单独微调。此外，GRPO框架还提高了高资源语言的TTS性能，在可懂度、说话人相似度和音频质量方面超越了离线对齐方法（如DPO）。

Conclusion: 基于GRPO的框架能有效利用ASR等模型，通过无配对数据显著提升低资源和高资源语言的TTS性能，尤其在数据稀缺的情况下表现优异。

Abstract: Developing high-quality text-to-speech (TTS) systems for low-resource
languages is challenging due to the scarcity of paired text and speech data. In
contrast, automatic speech recognition (ASR) models for such languages are
often more accessible, owing to large-scale multilingual pre-training efforts.
We propose a framework based on Group Relative Policy Optimization (GRPO) to
adapt an autoregressive, multilingual TTS model to new languages. Our method
first establishes a language-agnostic foundation for TTS synthesis by training
a multilingual baseline with International Phonetic Alphabet (IPA) tokens.
Next, we fine-tune this model on limited paired data of the new languages to
capture the target language's prosodic features. Finally, we apply GRPO to
optimize the model using only unpaired text and speaker prompts, guided by a
multi-objective reward from pretrained ASR, speaker verification, and audio
quality estimation models. Experiments demonstrate that this pipeline produces
intelligible and speaker-consistent speech in low-resource languages,
substantially outperforming fine-tuning alone. Furthermore, our GRPO-based
framework also improves TTS performance in high-resource languages, surpassing
offline alignment methods such as Direct Preference Optimization (DPO) yielding
superior intelligibility, speaker similarity, and audio quality.

</details>


### [81] [Retrieval-of-Thought: Efficient Reasoning via Reusing Thoughts](https://arxiv.org/abs/2509.21743)
*Ammar Ahmed,Azal Ahmad Khan,Ayaan Ahmad,Sheng Di,Zirui Liu,Ali Anwar*

Main category: cs.AI

TL;DR: RoT通过检索和重用现有推理步骤，显著提高了大型推理模型的推理效率，降低了延迟和成本，同时保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过生成冗长的推理过程来提高准确性，但这导致了高延迟和高成本，因此需要提高推理时的效率。

Method: 提出RoT（Retrieval-of-Thought）方法，将先前的推理作为可组合的“思考”步骤进行重用。RoT将这些步骤组织成一个带有序列和语义边的思考图，以实现快速检索和灵活重组。在推理时，RoT检索与查询相关的节点，并通过奖励引导的遍历组装成一个问题特定的模板来指导生成。

Result: RoT在多个模型和推理基准上进行了评估，结果显示在保持准确性的同时，显著提高了效率：输出tokens减少高达40%，推理延迟降低82%，成本降低59%，且提示增长很小。

Conclusion: RoT通过检索实现的动态模板构建，为高效的大型推理模型推理提供了一个可扩展的范式。

Abstract: Large reasoning models improve accuracy by producing long reasoning traces,
but this inflates latency and cost, motivating inference-time efficiency. We
propose Retrieval-of-Thought (RoT), which reuses prior reasoning as composable
``thought" steps to guide new problems. RoT organizes steps into a thought
graph with sequential and semantic edges to enable fast retrieval and flexible
recombination. At inference, RoT retrieves query-relevant nodes and applies
reward-guided traversal to assemble a problem-specific template that guides
generation. This dynamic template reuse reduces redundant exploration and,
therefore, reduces output tokens while preserving accuracy. We evaluate RoT on
reasoning benchmarks with multiple models, measuring accuracy, token usage,
latency, and memory overhead. Findings show small prompt growth but substantial
efficiency gains, with RoT reducing output tokens by up to 40%, inference
latency by 82%, and cost by 59% while maintaining accuracy. RoT establishes a
scalable paradigm for efficient LRM reasoning via dynamic template construction
through retrieval.

</details>


### [82] [Lifelong Learning with Behavior Consolidation for Vehicle Routing](https://arxiv.org/abs/2509.21765)
*Jiyuan Pei,Yi Mei,Jialin Liu,Mengjie Zhang,Xin Yao*

Main category: cs.AI

TL;DR: 现有神经路由求解器在新任务上存在泛化差或灾难性遗忘问题。本文提出LLR-BC终身学习框架，通过行为整合有效学习新任务，同时保持旧任务性能，解决遗忘问题并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前的神经求解器在处理路由问题时，主要依赖一次性训练，导致在新任务出现时，要么零样本泛化能力差（因任务差异），要么在微调时出现灾难性遗忘（丢失旧知识）。因此，需要一种新的终身学习范式，使求解器能有效学习序列出现的多样化新任务，同时保持对已学任务的性能。

Method: 本文提出了一种名为“行为整合终身学习路由器 (Lifelong Learning Router with Behavior Consolidation, LLR-BC)”的新框架。LLR-BC通过决策寻求的方式，将新任务训练的求解器行为与缓冲的先验行为对齐，从而有效整合知识。为更关注关键经验，LLR-BC对置信度较低的决策分配更大的整合权重。

Result: 在带容量车辆路径问题和旅行商问题上的广泛实验表明，LLR-BC在终身学习设置下，能够有效训练高性能神经求解器，成功解决了灾难性遗忘问题，保持了模型的可塑性，并提升了零样本泛化能力。

Conclusion: LLR-BC提供了一个有效的解决方案，使得神经路由求解器能够在终身学习范式下持续学习新任务，同时克服了传统方法中灾难性遗忘和泛化能力不足的挑战，展现出强大的性能和适应性。

Abstract: Recent neural solvers have demonstrated promising performance in learning to
solve routing problems. However, existing studies are primarily based on
one-off training on one or a set of predefined problem distributions and
scales, i.e., tasks. When a new task arises, they typically rely on either
zero-shot generalization, which may be poor due to the discrepancies between
the new task and the training task(s), or fine-tuning the pretrained solver on
the new task, which possibly leads to catastrophic forgetting of knowledge
acquired from previous tasks. This paper explores a novel lifelong learning
paradigm for neural VRP solvers, where multiple tasks with diverse
distributions and scales arise sequentially over time. Solvers are required to
effectively and efficiently learn to solve new tasks while maintaining their
performance on previously learned tasks. Consequently, a novel framework called
Lifelong Learning Router with Behavior Consolidation (LLR-BC) is proposed.
LLR-BC consolidates prior knowledge effectively by aligning behaviors of the
solver trained on a new task with the buffered ones in a decision-seeking way.
To encourage more focus on crucial experiences, LLR-BC assigns greater
consolidated weights to decisions with lower confidence. Extensive experiments
on capacitated vehicle routing problems and traveling salesman problems
demonstrate LLR-BC's effectiveness in training high-performance neural solvers
in a lifelong learning setting, addressing the catastrophic forgetting issue,
maintaining their plasticity, and improving zero-shot generalization ability.

</details>


### [83] [UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios](https://arxiv.org/abs/2509.21766)
*Haotian Luo,Huaisong Zhang,Xuelin Zhang,Haoyu Wang,Zeyu Qin,Wenjie Lu,Guozheng Ma,Haiying He,Yingsha Xie,Qiyang Zhou,Zixuan Hu,Hongze Mi,Yibo Wang,Naiqiang Tan,Hong Chen,Yi R. Fung,Chun Yuan,Li Shen*

Main category: cs.AI

TL;DR: 本文提出了UltraHorizon，一个新基准来评估自主智能体在长周期、部分可观测真实世界任务中的能力。实验发现，现有LLM智能体在此类任务中表现远不如人类，存在持续的能力差距，并指出了关键的错误类型和原因。


<details>
  <summary>Details</summary>
Motivation: 尽管自主智能体在许多领域取得进展，但大多数评估集中在短周期、完全可观测的任务。然而，许多关键的真实世界任务（如软件开发、投资、科学发现）是长周期、部分可观测的，需要持续的推理、规划、记忆管理和工具使用。现有基准未能捕捉这些长周期挑战，导致系统性评估存在空白。

Method: 引入了UltraHorizon基准，旨在衡量复杂真实世界挑战所需的基础能力。该基准以“探索”作为统一任务，在三个不同的环境中进行。智能体需通过持续推理、规划、记忆和工具管理以及与环境交互，迭代地发现隐藏规则。在最重负载设置下，轨迹平均包含20万+代币和400+工具调用；标准配置下，也超过3.5万代币和60+工具调用。

Result: 实验表明，LLM智能体在UltraHorizon设置中持续表现不佳，而人类参与者获得了更高的分数，突显了智能体长周期能力方面存在的持续差距。同时观察到简单的规模扩展在该任务中无效。通过对收集到的轨迹进行深入分析，识别出八种错误类型，并将其归因于两个主要原因：上下文锁定（in-context locking）和功能性基本能力差距（functional fundamental capability gaps）。

Conclusion: UltraHorizon基准有效揭示了当前LLM智能体在长周期、部分可观测任务中进行持续推理、规划和记忆管理能力的不足。现有智能体与人类之间存在显著差距，且简单缩放无法弥补。研究指出的错误类型和原因，为未来提升智能体在复杂真实世界场景中的性能提供了方向。

Abstract: Autonomous agents have recently achieved remarkable progress across diverse
domains, yet most evaluations focus on short-horizon, fully observable tasks.
In contrast, many critical real-world tasks, such as large-scale software
development, commercial investment, and scientific discovery, unfold in
long-horizon and partially observable scenarios where success hinges on
sustained reasoning, planning, memory management, and tool use. Existing
benchmarks rarely capture these long-horizon challenges, leaving a gap in
systematic evaluation. To bridge this gap, we introduce \textbf{UltraHorizon} a
novel benchmark that measures the foundational capabilities essential for
complex real-world challenges. We use exploration as a unifying task across
three distinct environments to validate these core competencies. Agents are
designed in long-horizon discovery tasks where they must iteratively uncover
hidden rules through sustained reasoning, planning, memory and tools
management, and interaction with environments. Under the heaviest scale
setting, trajectories average \textbf{200k+} tokens and \textbf{400+} tool
calls, whereas in standard configurations they still exceed \textbf{35k} tokens
and involve more than \textbf{60} tool calls on average. Our extensive
experiments reveal that LLM-agents consistently underperform in these settings,
whereas human participants achieve higher scores, underscoring a persistent gap
in agents' long-horizon abilities. We also observe that simple scaling fails in
our task. To better illustrate the failure of agents, we conduct an in-depth
analysis of collected trajectories. We identify eight types of errors and
attribute them to two primary causes: in-context locking and functional
fundamental capability gaps.
\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available
here.}

</details>


### [84] [Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety](https://arxiv.org/abs/2509.21782)
*Junliang Liu,Jingyu Xiao,Wenxin Tang,Wenxuan Wang,Zhixian Wang,Minrui Zhang,Shuanghe Yu*

Main category: cs.AI

TL;DR: 现有MLLM基准未能充分评估Web应用所需的推理、鲁棒性和安全性。本文提出WebRSSBench，一个综合基准来弥补这一空白，并发现当前MLLM在这三个方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在构建复杂的Web应用（如GUI代理、前端代码生成）中潜力巨大。然而，现有基准主要关注视觉感知或UI代码生成，未能充分评估端到端Web应用所需的推理、鲁棒性和安全性。

Method: 本文引入WebRSSBench，一个综合性的Web理解基准，用于联合评估MLLMs的推理、鲁棒性和安全性。该基准由729个网站构建，包含3799个问答对，涵盖位置关系推理、颜色鲁棒性、安全关键检测等八项任务。为确保评估可靠性，采用了标准化提示、确定性评估脚本和多阶段质量控制。在WebRSSBench上评估了12个MLLM。

Result: 评估结果揭示了显著差距：MLLMs在真实布局上的组合和跨元素推理方面仍有困难；在面对用户界面和内容扰动（如布局重排或视觉风格变化）时鲁棒性有限；在识别和避免安全关键或不可逆操作方面表现保守。

Conclusion: MLLMs在Web应用所需的推理、鲁棒性和安全性方面仍存在显著差距，尤其是在处理复杂布局、应对扰动以及识别安全关键行为时表现不足。

Abstract: Multimodal large language models (MLLMs) are increasingly positioned as AI
collaborators for building complex web-related applications like GUI agents and
front-end code generation. However, existing benchmarks largely emphasize
visual perception or UI code generation, showing insufficient evaluation on the
reasoning, robustness and safety capability required for end-to-end web
applications. To bridge the gap, we introduce a comprehensive web understanding
benchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and
Safety across eight tasks, such as position relationship reasoning, color
robustness, and safety critical detection, etc. The benchmark is constructed
from 729 websites and contains 3799 question answer pairs that probe multi-step
inference over page structure, text, widgets, and safety-critical interactions.
To ensure reliable measurement, we adopt standardized prompts, deterministic
evaluation scripts, and multi-stage quality control combining automatic checks
with targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The
results reveal significant gaps, models still struggle with compositional and
cross-element reasoning over realistic layouts, show limited robustness when
facing perturbations in user interfaces and content such as layout
rearrangements or visual style shifts, and are rather conservative in
recognizing and avoiding safety critical or irreversible actions. Our code is
available at https://github.com/jinliang-byte/webssrbench.

</details>


### [85] [D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents](https://arxiv.org/abs/2509.21799)
*Hongze Mi,Yibo Feng,Wenjie Lu,Yuqi Wang,Jinyuan Li,Song Cao,He Cui,Tengfei Tian,Xuelin Zhang,Haotian Luo,Di Sun,Naiqiang Tan,Gang Pan*

Main category: cs.AI

TL;DR: 本文提出D-Artemis，一个受人类认知启发的新型深思熟虑框架，用于增强GUI代理自动化任务。它通过细粒度提示检索、预执行对齐和后执行反思来克服现有挑战，无需大量训练即可提升MLLMs的泛化能力，并在两大基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前的图形用户界面（GUI）代理面临数据瓶颈、错误延迟检测成本高昂以及指导矛盾等关键挑战，限制了其在自动化人类任务方面的能力。

Method: 受人类“思考、对齐、反思”认知循环的启发，本文提出了D-Artemis框架。它包含：1) 细粒度的、应用特定的提示检索机制；2) 主动式预执行对齐阶段，通过“思想-行动一致性检查（TAC Check）”和“行动纠正代理（ACA）”来降低执行失败风险；3) 后执行状态反思代理（SRA）以实现从经验中学习。该方法在无需复杂轨迹数据集训练的情况下，增强了通用多模态大语言模型（MLLMs）在GUI任务上的能力。

Result: D-Artemis在两大主要基准测试中均取得了新的最先进（SOTA）结果，在AndroidWorld上达到75.8%的成功率，在ScreenSpot-V2上达到96.8%的成功率。广泛的消融研究进一步证明了框架中每个组件的显著贡献。

Conclusion: D-Artemis是一个有效的深思熟虑框架，它通过模拟人类认知过程，显著提升了GUI代理的性能和泛化能力，尤其是在无需复杂训练数据的情况下，成功增强了MLLMs处理GUI任务的能力，并取得了突破性的SOTA成果。

Abstract: Graphical User Interface (GUI) agents aim to automate a wide spectrum of
human tasks by emulating user interaction. Despite rapid advancements, current
approaches are hindered by several critical challenges: data bottleneck in
end-to-end training, high cost of delayed error detection, and risk of
contradictory guidance. Inspired by the human cognitive loop of Thinking,
Alignment, and Reflection, we present D-Artemis -- a novel deliberative
framework in this paper. D-Artemis leverages a fine-grained, app-specific tip
retrieval mechanism to inform its decision-making process. It also employs a
proactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC)
Check module and Action Correction Agent (ACA) work in concert to mitigate the
risk of execution failures. A post-execution Status Reflection Agent (SRA)
completes the cognitive loop, enabling strategic learning from experience.
Crucially, D-Artemis enhances the capabilities of general-purpose Multimodal
large language models (MLLMs) for GUI tasks without the need for training on
complex trajectory datasets, demonstrating strong generalization. D-Artemis
establishes new state-of-the-art (SOTA) results across both major benchmarks,
achieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2.
Extensive ablation studies further demonstrate the significant contribution of
each component to the framework.

</details>


### [86] [ProRe: A Proactive Reward System for GUI Agents via Reasoner-Actor Collaboration](https://arxiv.org/abs/2509.21823)
*Gaole Dai,Shiqi Jiang,Ting Cao,Yuqing Yang,Yuanchun Li,Rui Tan,Mo Li,Lili Qiu*

Main category: cs.AI

TL;DR: ProRe是一个主动奖励系统，通过通用推理器和领域特定评估器与环境互动，为GUI智能体提供更准确、可验证的奖励，显著提升了奖励准确性和智能体成功率。


<details>
  <summary>Details</summary>
Motivation: 现有奖励方法难以泛化到GUI智能体，因缺乏真实轨迹或应用数据库，且基于静态轨迹的LLM-as-a-Judge方法准确性有限。

Method: 提出ProRe系统，它利用通用推理器和领域特定评估器。推理器调度目标状态探测任务，评估器通过主动与环境交互收集额外观测，从而使推理器能分配更准确、可验证的奖励。

Result: 在超过3K条轨迹上，ProRe将奖励准确率和F1分数分别提升了高达5.3%和19.4%。与现有策略智能体结合后，成功率提升高达22.4%。

Conclusion: ProRe通过主动与环境交互的方式，有效解决了GUI智能体奖励评估的挑战，显著提高了奖励的准确性和智能体的性能。

Abstract: Reward is critical to the evaluation and training of large language models
(LLMs). However, existing rule-based or model-based reward methods struggle to
generalize to GUI agents, where access to ground-truth trajectories or
application databases is often unavailable, and static trajectory-based
LLM-as-a-Judge approaches suffer from limited accuracy. To address these
challenges, we propose ProRe, a proactive reward system that leverages a
general-purpose reasoner and domain-specific evaluator agents (actors). The
reasoner schedules targeted state probing tasks, which the evaluator agents
then execute by actively interacting with the environment to collect additional
observations. This enables the reasoner to assign more accurate and verifiable
rewards to GUI agents. Empirical results on over 3K trajectories demonstrate
that ProRe improves reward accuracy and F1 score by up to 5.3% and 19.4%,
respectively. Furthermore, integrating ProRe with state-of-the-art policy
agents yields a success rate improvement of up to 22.4%.

</details>


### [87] [DS-STAR: Data Science Agent via Iterative Planning and Verification](https://arxiv.org/abs/2509.21825)
*Jaehyun Nam,Jinsung Yoon,Jiefeng Chen,Jinwoo Shin,Tomas Pfister*

Main category: cs.AI

TL;DR: 本文介绍了一种名为DS-STAR的新型数据科学智能体，它通过数据文件分析模块、LLM驱动的验证步骤和顺序规划机制，克服了大型语言模型在处理异构数据和生成分析计划方面的挑战，并在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 数据科学对于数据驱动的决策至关重要，但任务通常复杂，涉及探索和整合多种数据源。尽管大型语言模型（LLMs）有潜力自动化此过程，但它们难以处理异构数据格式，且生成的分析计划往往不理想，因为在开放式任务中缺乏真实标签使得验证计划充分性变得困难。

Method: DS-STAR引入了三项关键贡献：(1) 一个数据文件分析模块，能自动探索并从包括非结构化类型在内的多样数据格式中提取上下文；(2) 一个验证步骤，由基于LLM的判断器在每个阶段评估分析计划的充分性；(3) 一个顺序规划机制，从一个简单的可执行计划开始，根据DS-STAR的反馈迭代地进行细化，直到其充分性得到验证。

Result: DS-STAR在DABStep、KramaBench和DA-Code这三个具有挑战性的基准测试中取得了最先进的性能。此外，DS-STAR在需要处理多个异构格式数据文件的困难任务上，尤其超越了基线模型。

Conclusion: DS-STAR通过其创新的模块和迭代规划机制，能够可靠地处理涉及多样数据源的复杂数据分析任务，有效克服了现有LLMs的局限性，并在数据科学基准测试中展现出卓越的性能。

Abstract: Data science, which transforms raw data into actionable insights, is critical
for data-driven decision-making. However, these tasks are often complex,
involving steps for exploring multiple data sources and synthesizing findings
to deliver insightful answers. While large language models (LLMs) show
significant promise in automating this process, they often struggle with
heterogeneous data formats and generate sub-optimal analysis plans, as
verifying plan sufficiency is inherently difficult without ground-truth labels
for such open-ended tasks. To overcome these limitations, we introduce DS-STAR,
a novel data science agent. Specifically, DS-STAR makes three key
contributions: (1) a data file analysis module that automatically explores and
extracts context from diverse data formats, including unstructured types; (2) a
verification step where an LLM-based judge evaluates the sufficiency of the
analysis plan at each stage; and (3) a sequential planning mechanism that
starts with a simple, executable plan and iteratively refines it based on the
DS-STAR's feedback until its sufficiency is verified. This iterative refinement
allows DS-STAR to reliably navigate complex analyses involving diverse data
sources. Our experiments show that DS-STAR achieves state-of-the-art
performance across three challenging benchmarks: DABStep, KramaBench, and
DA-Code. Moreover, DS-STAR particularly outperforms baselines on hard tasks
that require processing multiple data files with heterogeneous formats.

</details>


### [88] [Axiomatic Choice and the Decision-Evaluation Paradox](https://arxiv.org/abs/2509.21836)
*Ben Abramowitz,Nicholas Mattei*

Main category: cs.AI

TL;DR: 提出了一个用公理建模决策的框架，并揭示了在决策制定与评估中使用公理时存在的“决策-评估悖论”，强调其对AI模型训练的警示。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个能整合决策公理（如伦理约束）的决策建模框架，以深入理解公理在决策过程中的作用及其潜在冲突。

Method: 引入了一个用公理（作为决策声明）建模决策的框架。在此框架下，根据公理的结构特性定义了决策公理的分类体系。

Result: 揭示了“决策-评估悖论”，即使用公理来制定决策与使用公理来评估决策之间存在的紧张关系。研究指出该悖论在现实公理结构中普遍存在。

Conclusion: 决策-评估悖论表明，在基于决策数据训练模型或应用公理来制定和评估决策时，必须极其谨慎。

Abstract: We introduce a framework for modeling decisions with axioms that are
statements about decisions, e.g., ethical constraints. Using our framework we
define a taxonomy of decision axioms based on their structural properties and
demonstrate a tension between the use of axioms to make decisions and the use
of axioms to evaluate decisions which we call the Decision-Evaluation Paradox.
We argue that the Decision-Evaluation Paradox arises with realistic axiom
structures, and the paradox illuminates why one must be exceptionally careful
when training models on decision data or applying axioms to make and evaluate
decisions.

</details>


### [89] [DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents](https://arxiv.org/abs/2509.21842)
*Yansong Ning,Rui Liu,Jun Wang,Kai Chen,Wei Li,Jun Fang,Kan Zheng,Naiqiang Tan,Hao Liu*

Main category: cs.AI

TL;DR: DeepTravel是一个端到端智能体强化学习框架，通过构建沙盒环境、分层奖励模型和回放增强学习方法，使小型LLM在旅行规划任务中显著超越现有领先LLM。


<details>
  <summary>Details</summary>
Motivation: 现有旅行规划智能体依赖人工提示和固定工作流，缺乏灵活性和自主性，限制了其在多步推理中的探索、验证和细化能力。

Method: 本文提出DeepTravel框架：1. 构建健壮的沙盒环境，缓存真实数据以规避API限制。2. 设计分层奖励建模系统，包含轨迹级验证器（检查时空可行性）和回合级验证器（验证行程细节与工具响应一致性）。3. 引入回放增强强化学习方法，使智能体能周期性地从失败经验缓冲区回放。

Result: 通过在线和离线评估，DeepTravel使小型LLM（如Qwen3 32B）在旅行规划任务中显著优于现有领先LLM（如OpenAI o1, o3和DeepSeek R1）。已成功部署于滴滴企业解决方案App。

Conclusion: DeepTravel提供了一种有效构建自主旅行规划智能体的方法，通过创新的环境、奖励和学习机制，显著提升了旅行规划性能，并使小型LLM超越了大型领先模型。

Abstract: Travel planning (TP) agent has recently worked as an emerging building block
to interact with external tools and resources for travel itinerary generation,
ensuring enjoyable user experience. Despite its benefits, existing studies rely
on hand craft prompt and fixed agent workflow, hindering more flexible and
autonomous TP agent. This paper proposes DeepTravel, an end to end agentic
reinforcement learning framework for building autonomous travel planning agent,
capable of autonomously planning, executing tools, and reflecting on tool
responses to explore, verify, and refine intermediate actions in multi step
reasoning. To achieve this, we first construct a robust sandbox environment by
caching transportation, accommodation and POI data, facilitating TP agent
training without being constrained by real world APIs limitations (e.g.,
inconsistent outputs). Moreover, we develop a hierarchical reward modeling
system, where a trajectory level verifier first checks spatiotemporal
feasibility and filters unsatisfied travel itinerary, and then the turn level
verifier further validate itinerary detail consistency with tool responses,
enabling efficient and precise reward service. Finally, we propose the reply
augmented reinforcement learning method that enables TP agent to periodically
replay from a failures experience buffer, emerging notable agentic capacity. We
deploy trained TP agent on DiDi Enterprise Solutions App and conduct
comprehensive online and offline evaluations, demonstrating that DeepTravel
enables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing
frontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.

</details>


### [90] [Reimagining Agent-based Modeling with Large Language Model Agents via Shachi](https://arxiv.org/abs/2509.21862)
*So Kuroki,Yingtao Tian,Kou Misaki,Takashi Ikegami,Takuya Akiba,Yujin Tang*

Main category: cs.AI

TL;DR: 提出Shachi框架，一种系统化研究LLM多智能体涌现行为的方法，通过分解智能体认知组件，并在多任务基准和真实世界关税冲击中验证，表明记忆和工具是校准智能体行为的关键。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的多智能体系统中的涌现行为研究面临挑战，现有方法缺乏受控实验的原则性。

Method: 引入Shachi，一个形式化方法和模块化框架。它将智能体策略分解为核心认知组件：配置（固有特质）、记忆（上下文持久性）和工具（扩展能力），并由LLM推理引擎协调。

Result: 该方法在10个综合任务基准上得到验证，并通过新颖的科学探究展示了其能力。通过模拟真实世界美国关税冲击，证明仅当智能体认知架构适当配置记忆和工具时，其行为才与观察到的市场反应一致，从而确立了方法的外部有效性。

Conclusion: 本工作为构建和评估LLM智能体提供了一个严谨、开源的基础，旨在促进更具累积性和科学依据的研究。

Abstract: The study of emergent behaviors in large language model (LLM)-driven
multi-agent systems is a critical research challenge, yet progress is limited
by a lack of principled methodologies for controlled experimentation. To
address this, we introduce Shachi, a formal methodology and modular framework
that decomposes an agent's policy into core cognitive components: Configuration
for intrinsic traits, Memory for contextual persistence, and Tools for expanded
capabilities, all orchestrated by an LLM reasoning engine. This principled
architecture moves beyond brittle, ad-hoc agent designs and enables the
systematic analysis of how specific architectural choices influence collective
behavior. We validate our methodology on a comprehensive 10-task benchmark and
demonstrate its power through novel scientific inquiries. Critically, we
establish the external validity of our approach by modeling a real-world U.S.
tariff shock, showing that agent behaviors align with observed market reactions
only when their cognitive architecture is appropriately configured with memory
and tools. Our work provides a rigorous, open-source foundation for building
and evaluating LLM agents, aimed at fostering more cumulative and
scientifically grounded research.

</details>


### [91] [TRACE: Learning to Compute on Graphs](https://arxiv.org/abs/2509.21886)
*Ziyang Zheng,Jiaying Zhu,Jingyi Zhou,Qiang Xu*

Main category: cs.AI

TL;DR: 本文提出TRACE，一种新的范式，通过分层Transformer架构和“函数偏移学习”目标来解决计算图上“学习计算”的挑战，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有消息传递神经网络(MPNNs)和基于Transformer的模型在处理计算图的函数行为时存在架构不匹配问题，无法捕捉计算的位置感知和分层特性，这是一个图表示学习中的基本挑战。

Method: 引入TRACE范式：1. 采用分层Transformer作为骨干，模拟计算的逐步流程，取代有缺陷的置换不变聚合。2. 提出“函数偏移学习”目标，模型学习预测真实全局函数与简单局部近似之间的差异（函数偏移），而非直接预测复杂的全局函数。

Result: 在电子电路（一类复杂的计算图）上验证了TRACE范式，在一系列综合基准测试中，TRACE大幅超越了所有先前的架构。

Conclusion: 架构对齐的骨干网络和解耦的学习目标共同构成了解决图上“学习计算”这一基本挑战的更稳健范式。

Abstract: Learning to compute, the ability to model the functional behavior of a
computational graph, is a fundamental challenge for graph representation
learning. Yet, the dominant paradigm is architecturally mismatched for this
task. This flawed assumption, central to mainstream message passing neural
networks (MPNNs) and their conventional Transformer-based counterparts,
prevents models from capturing the position-aware, hierarchical nature of
computation. To resolve this, we introduce \textbf{TRACE}, a new paradigm built
on an architecturally sound backbone and a principled learning objective.
First, TRACE employs a Hierarchical Transformer that mirrors the step-by-step
flow of computation, providing a faithful architectural backbone that replaces
the flawed permutation-invariant aggregation. Second, we introduce
\textbf{function shift learning}, a novel objective that decouples the learning
problem. Instead of predicting the complex global function directly, our model
is trained to predict only the \textit{function shift}, the discrepancy between
the true global function and a simple local approximation that assumes input
independence. We validate this paradigm on electronic circuits, one of the most
complex and economically critical classes of computational graphs. Across a
comprehensive suite of benchmarks, TRACE substantially outperforms all prior
architectures. These results demonstrate that our architecturally-aligned
backbone and decoupled learning objective form a more robust paradigm for the
fundamental challenge of learning to compute on graphs.

</details>


### [92] [GenesisGeo: Technical Report](https://arxiv.org/abs/2509.21896)
*Minfeng Zhu,Zi Wang,Sizhe Ji,Zhengtong Du,Junming Ke,Xiao Deng,Zanlang Yin,Xiuqi Huang,Heyu Wang,Wei Chen*

Main category: cs.AI

TL;DR: 本文提出了GenesisGeo，一个高效的欧几里得几何自动定理证明器，结合神经符号方法和优化的符号引擎，解决了IMO级别的几何问题，并开源了大规模几何数据集。


<details>
  <summary>Details</summary>
Motivation: 提高欧几里得几何自动定理证明器的能力，特别是解决包含辅助构造的复杂问题，并为该领域提供大规模训练与测试数据。

Method: 开发了基于Qwen3-0.6B-Base的神经符号证明器GenesisGeo；通过定理匹配和C++实现，将符号演绎引擎DDARN加速120倍；构建并开源了一个包含2180万个几何问题的大规模数据集。

Result: GenesisGeo在IMO-AG-30基准测试中，单模型解决了30个问题中的24个（IMO银牌水平），双模型集成达到了26个问题（IMO金牌水平）。DDARN引擎获得了120倍的显著加速。

Conclusion: GenesisGeo是一个性能卓越的欧几里得几何自动定理证明器，其神经符号架构和引擎优化使其在IMO级别问题上达到顶尖水平，同时发布的庞大数据集对社区具有重要价值。

Abstract: We present GenesisGeo, an automated theorem prover in Euclidean geometry. We
have open-sourced a large-scale geometry dataset of 21.8 million geometric
problems, over 3 million of which contain auxiliary constructions. Specially,
we significantly accelerate the symbolic deduction engine DDARN by 120x through
theorem matching, combined with a C++ implementation of its core components.
Furthermore, we build our neuro-symbolic prover, GenesisGeo, upon
Qwen3-0.6B-Base, which solves 24 of 30 problems (IMO silver medal level) in the
IMO-AG-30 benchmark using a single model, and achieves 26 problems (IMO gold
medal level) with a dual-model ensemble.

</details>


### [93] [DyRo-MCTS: A Robust Monte Carlo Tree Search Approach to Dynamic Job Shop Scheduling](https://arxiv.org/abs/2509.21902)
*Ruiqi Chen,Yi Mei,Fangfang Zhang,Mengjie Zhang*

Main category: cs.AI

TL;DR: 针对动态作业车间调度中新任务到达引起的干扰问题，本文提出DyRo-MCTS方法，将动作鲁棒性估计集成到MCTS中，以指导系统做出适应未来任务的决策，显著提升了离线学习策略的性能并超越了传统MCTS。


<details>
  <summary>Details</summary>
Motivation: 动态作业车间调度因新任务频繁到达而充满挑战。现有先进方法采用离线机器学习策略，但这些策略不完善，需要MCTS等在线规划技术辅助。然而，新任务到达的不可预测性使基于不完整信息的在线规划容易受到干扰。

Method: 提出动态鲁棒MCTS (DyRo-MCTS) 方法。该方法将动作鲁棒性估计集成到MCTS中，旨在引导生产环境生成既能产生良好调度结果又易于适应未来任务到达的状态。

Result: 实验表明，DyRo-MCTS显著提升了离线学习策略的性能，且仅增加可忽略的在线规划时间。在各种调度场景中，DyRo-MCTS持续优于传统MCTS。

Conclusion: DyRo-MCTS通过做出鲁棒的调度决策，能够在干扰下实现长期、可持续的性能提升。

Abstract: Dynamic job shop scheduling, a fundamental combinatorial optimisation problem
in various industrial sectors, poses substantial challenges for effective
scheduling due to frequent disruptions caused by the arrival of new jobs.
State-of-the-art methods employ machine learning to learn scheduling policies
offline, enabling rapid responses to dynamic events. However, these offline
policies are often imperfect, necessitating the use of planning techniques such
as Monte Carlo Tree Search (MCTS) to improve performance at online decision
time. The unpredictability of new job arrivals complicates online planning, as
decisions based on incomplete problem information are vulnerable to
disturbances. To address this issue, we propose the Dynamic Robust MCTS
(DyRo-MCTS) approach, which integrates action robustness estimation into MCTS.
DyRo-MCTS guides the production environment toward states that not only yield
good scheduling outcomes but are also easily adaptable to future job arrivals.
Extensive experiments show that DyRo-MCTS significantly improves the
performance of offline-learned policies with negligible additional online
planning time. Moreover, DyRo-MCTS consistently outperforms vanilla MCTS across
various scheduling scenarios. Further analysis reveals that its ability to make
robust scheduling decisions leads to long-term, sustainable performance gains
under disturbances.

</details>


### [94] [Outlier Detection in Plantar Pressure: Human-Centered Comparison of Statistical Parametric Mapping and Explainable Machine Learning](https://arxiv.org/abs/2509.21943)
*Carlo Dindorf,Jonas Dully,Steven Simon,Dennis Perchthaler,Stephan Becker,Hannah Ehmann,Kjell Heitmann,Bernd Stetter,Christian Diers,Michael Fröhlich*

Main category: cs.AI

TL;DR: 本研究比较了统计参数映射(SPM)和可解释机器学习(ML)在足底压力数据异常值检测中的性能和解释性，发现ML模型表现更优，且两者解释性均受专家认可。


<details>
  <summary>Details</summary>
Motivation: 足底压力数据在临床和运动科学中至关重要，但大型异构数据集常含异常值。SPM方法虽可解释但对对齐敏感且异常值检测能力不明确。本研究旨在建立透明的质量控制流程。

Method: 研究使用了多中心专家标注并含合成异常值的数据集（798个有效样本，2000个异常值）。评估了非参数、依赖配准的SPM方法和使用SHAP解释的卷积神经网络（CNN）。性能通过嵌套交叉验证评估，解释质量通过语义差异调查由领域专家评估。

Result: ML模型表现出高准确性并优于SPM，SPM误分类了临床有意义的变异并错过了真实异常值。专家认为SPM和SHAP解释均清晰、有用且值得信赖，但SPM被认为复杂度较低。

Conclusion: SPM和可解释ML在足底压力数据自动异常值检测中具有互补潜力，且可解释性对于将复杂模型输出转化为可理解的见解至关重要，以有效辅助决策。

Abstract: Plantar pressure mapping is essential in clinical diagnostics and sports
science, yet large heterogeneous datasets often contain outliers from technical
errors or procedural inconsistencies. Statistical Parametric Mapping (SPM)
provides interpretable analyses but is sensitive to alignment and its capacity
for robust outlier detection remains unclear. This study compares an SPM
approach with an explainable machine learning (ML) approach to establish
transparent quality-control pipelines for plantar pressure datasets. Data from
multiple centers were annotated by expert consensus and enriched with synthetic
anomalies resulting in 798 valid samples and 2000 outliers. We evaluated (i) a
non-parametric, registration-dependent SPM approach and (ii) a convolutional
neural network (CNN), explained using SHapley Additive exPlanations (SHAP).
Performance was assessed via nested cross-validation; explanation quality via a
semantic differential survey with domain experts. The ML model reached high
accuracy and outperformed SPM, which misclassified clinically meaningful
variations and missed true outliers. Experts perceived both SPM and SHAP
explanations as clear, useful, and trustworthy, though SPM was assessed less
complex. These findings highlight the complementary potential of SPM and
explainable ML as approaches for automated outlier detection in plantar
pressure data, and underscore the importance of explainability in translating
complex model outputs into interpretable insights that can effectively inform
decision-making.

</details>


### [95] [CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration](https://arxiv.org/abs/2509.21981)
*Zhimin Wang,Shaokang He,Duo Wu,Jinghe Wang,Linjia Kang,Jing Yu,Zhi Wang*

Main category: cs.AI

TL;DR: 为解决LLM多智能体协作中动态意图推理不足导致效率低下的问题，本文提出了CoBel-World框架，通过协作信念世界实现意图感知，显著减少了通信成本并提高了任务完成效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的协作框架未能充分利用LLM在动态意图推理方面的潜力，导致智能体规划不一致和通信冗余，从而降低了协作效率。

Method: 本文提出CoBel-World框架，为LLM智能体引入“协作信念世界”，联合建模物理环境和协作者的心理状态。该框架通过符号信念语言将任务知识解析为结构化信念，并利用LLM推理进行零样本贝叶斯式信念更新，从而使智能体能够主动发现潜在的协作失误并自适应地进行通信。

Result: 在TDW-MAT和C-WAH等具身基准测试中，CoBel-World与现有最强基线相比，通信成本降低了22-60%，任务完成效率提高了4-28%。

Conclusion: 研究结果表明，在基于LLM的多智能体系统中，明确的、意图感知的信念建模对于实现高效和类人协作至关重要。

Abstract: Effective real-world multi-agent collaboration requires not only accurate
planning but also the ability to reason about collaborators' intents -- a
crucial capability for avoiding miscoordination and redundant communication
under partial observable environments. Due to their strong planning and
reasoning capabilities, large language models (LLMs) have emerged as promising
autonomous agents for collaborative task solving. However, existing
collaboration frameworks for LLMs overlook their reasoning potential for
dynamic intent inference, and thus produce inconsistent plans and redundant
communication, reducing collaboration efficiency. To bridge this gap, we
propose CoBel-World, a novel framework that equips LLM agents with a
collaborative belief world -- an internal representation jointly modeling the
physical environment and collaborators' mental states. CoBel-World enables
agents to parse open-world task knowledge into structured beliefs via a
symbolic belief language, and perform zero-shot Bayesian-style belief updates
through LLM reasoning. This allows agents to proactively detect potential
miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated
on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World
significantly reduces communication costs by 22-60% and improves task
completion efficiency by 4-28% compared to the strongest baseline. Our results
show that explicit, intent-aware belief modeling is essential for efficient and
human-like collaboration in LLM-based multi-agent systems.

</details>


### [96] [RISK: A Framework for GUI Agents in E-commerce Risk Management](https://arxiv.org/abs/2509.21982)
*Renqi Chen,Zeyin Tao,Jianming Guo,Jingzhe Zhu,Yiheng Peng,Qingqing Sun,Tianyi Zhang,Shuai Chen*

Main category: cs.AI

TL;DR: 本文提出RISK框架，包含数据集、基准测试和RISK-R1强化微调方法，旨在解决电商风险管理中传统GUI代理无法处理的多步骤、有状态的复杂网页交互问题，实验证明RISK-R1显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 电商风险管理需要通过多步骤、有状态的交互来聚合深度嵌入的网页数据，但传统抓取方法和现有GUI代理（通常限于单步任务）无法处理动态、交互式内容，难以有效进行风险评估。

Method: 引入RISK框架，包含三个组件：(1) RISK-Data：一个包含8,492个单步和2,386个多步交互轨迹的数据集；(2) RISK-Bench：一个包含802个单步和320个多步轨迹的基准测试集，分为三个难度级别；(3) RISK-R1：一个R1风格的强化微调框架，考虑了输出格式、单步准确性、多步过程权重和任务难度权重四个方面的奖励机制。

Result: 实验结果显示，RISK-R1优于现有基线，在离线单步任务中实现了6.8%的提升，在离线多步任务中实现了8.8%的提升。此外，在线评估中，它达到了70.5%的最高任务成功率。

Conclusion: RISK为自动化复杂网页交互提供了可扩展的、领域特定的解决方案，推动了电商风险管理领域的最新进展。

Abstract: E-commerce risk management requires aggregating diverse, deeply embedded web
data through multi-step, stateful interactions, which traditional scraping
methods and most existing Graphical User Interface (GUI) agents cannot handle.
These agents are typically limited to single-step tasks and lack the ability to
manage dynamic, interactive content critical for effective risk assessment. To
address this challenge, we introduce RISK, a novel framework designed to build
and deploy GUI agents for this domain. RISK integrates three components: (1)
RISK-Data, a dataset of 8,492 single-step and 2,386 multi-step interaction
trajectories, collected through a high-fidelity browser framework and a
meticulous data curation process; (2) RISK-Bench, a benchmark with 802
single-step and 320 multi-step trajectories across three difficulty levels for
standardized evaluation; and (3) RISK-R1, a R1-style reinforcement fine-tuning
framework considering four aspects: (i) Output Format: Updated format reward to
enhance output syntactic correctness and task comprehension, (ii) Single-step
Level: Stepwise accuracy reward to provide granular feedback during early
training stages, (iii) Multi-step Level: Process reweight to emphasize critical
later steps in interaction sequences, and (iv) Task Level: Level reweight to
focus on tasks of varying difficulty. Experiments show that RISK-R1 outperforms
existing baselines, achieving a 6.8% improvement in offline single-step and an
8.8% improvement in offline multi-step. Moreover, it attains a top task success
rate of 70.5% in online evaluation. RISK provides a scalable, domain-specific
solution for automating complex web interactions, advancing the state of the
art in e-commerce risk management.

</details>


### [97] [Bilinear relational structure fixes reversal curse and enables consistent model editing](https://arxiv.org/abs/2509.21993)
*Dong-Kyum Kim,Minsung Kim,Jea Kwon,Nakyeong Yang,Meeyoung Cha*

Main category: cs.AI

TL;DR: 语言模型的“逆转诅咒”并非固有缺陷，而是知识编码方式所致。研究发现，通过在关系知识图谱上训练，模型内部会涌现双线性表示结构，该结构不仅能解决逆转诅咒，还能确保模型编辑时的逻辑一致性。模型的编辑成功与否，关键在于知识的底层表示几何。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型普遍存在的“逆转诅咒”问题（即无法从已学习的“A是B”推断出未见的“B是A”），该问题被广泛认为是模型的一个根本性局限。作者认为这并非模型的固有失败，而是其知识编码方式的产物。

Method: 从头开始，在合成的关系知识图谱数据集上训练语言模型，以观察其隐藏表示中是否会涌现特定的结构。

Result: 1. 训练后的语言模型在其隐藏表示中涌现出双线性关系结构。
2. 这种双线性结构显著缓解了“逆转诅咒”，使模型能够推断出未见的逆向事实。
3. 双线性结构在一致性模型编辑中扮演关键角色，确保事实更新能正确传播到其逆向及其他逻辑相关事实。
4. 缺乏这种表示的模型不仅存在逆转诅咒，也无法泛化编辑，并引入逻辑不一致性。

Conclusion: 在关系知识数据集上训练可以诱导语言模型出现双线性内部表示，进而使模型在编辑后能够表现出逻辑一致性。模型编辑的成功不仅取决于编辑算法，更关键在于被修改知识的底层表示几何。

Abstract: The reversal curse -- a language model's (LM) inability to infer an unseen
fact ``B is A'' from a learned fact ``A is B'' -- is widely considered a
fundamental limitation. We show that this is not an inherent failure but an
artifact of how models encode knowledge. By training LMs from scratch on a
synthetic dataset of relational knowledge graphs, we demonstrate that bilinear
relational structure emerges in their hidden representations. This structure
substantially alleviates the reversal curse, enabling LMs to infer unseen
reverse facts. Crucially, we also find that this bilinear structure plays a key
role in consistent model editing. When a fact is updated in a LM with this
structure, the edit correctly propagates to its reverse and other logically
dependent facts. In contrast, models lacking this representation not only
suffer from the reversal curse but also fail to generalize edits, further
introducing logical inconsistencies. Our results establish that training on a
relational knowledge dataset induces the emergence of bilinear internal
representations, which in turn enable LMs to behave in a logically consistent
manner after editing. This implies that the success of model editing depends
critically not just on editing algorithms but on the underlying
representational geometry of the knowledge being modified.

</details>


### [98] [GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments](https://arxiv.org/abs/2509.21998)
*Hanlin Zhu,Tianyu Guo,Song Mei,Stuart Russell,Nikhil Ghosh,Alberto Bietti,Jiantao Jiao*

Main category: cs.AI

TL;DR: 本文提出GSM-Agent基准，用于评估LLM的智能体推理能力，要求智能体主动使用工具获取信息以解决小学数学问题。结果显示，即使是顶尖模型也表现不佳。通过引入智能体推理图，发现多数模型缺乏重新访问先前节点的能力，并基于此提出了一种工具增强的测试时扩展方法来提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为智能体的应用日益广泛，结合工具使用（特别是搜索）和推理的智能体推理能力变得至关重要。然而，现有基准测试往往将智能体推理与复杂数学、专业知识等混合，难以独立评估智能体推理能力。

Method: 1. 构建新基准GSM-Agent：LLM智能体需解决小学级别的推理问题，但提示中只给出问题，缺少必要的前提信息，智能体需主动使用工具收集信息。2. 提出智能体推理图：将环境文档嵌入聚类为节点，并将每个工具调用映射到最近的节点以构建推理路径，用于分析智能体推理模式。3. 提出工具增强的测试时扩展方法：通过添加工具来鼓励模型重新访问节点，以提高LLM的智能体推理性能。

Result: 1. 在GSM-Agent基准上，即使是GPT-5等前沿模型也仅达到67%的准确率。2. 通过智能体推理图分析发现，许多模型在智能体推理中常常缺失重新访问先前访问过的节点的能力（这在静态推理中被认为是关键模式）。

Conclusion: GSM-Agent基准和智能体推理框架有望帮助未来研究更好地理解和拓展智能体推理的边界。重新访问节点是智能体推理中一个缺失的关键模式，并且通过工具增强的方法鼓励模型重新访问可以有效提升其智能体推理性能。

Abstract: As LLMs are increasingly deployed as agents, agentic reasoning - the ability
to combine tool use, especially search, and reasoning - becomes a critical
skill. However, it is hard to disentangle agentic reasoning when evaluated in
complex environments and tasks. Current agent benchmarks often mix agentic
reasoning with challenging math reasoning, expert-level knowledge, and other
advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent,
where an LLM agent is required to solve grade-school-level reasoning problems,
but is only presented with the question in the prompt without the premises that
contain the necessary information to solve the task, and needs to proactively
collect that information using tools. Although the original tasks are
grade-school math problems, we observe that even frontier models like GPT-5
only achieve 67% accuracy. To understand and analyze the agentic reasoning
patterns, we propose the concept of agentic reasoning graph: cluster the
environment's document embeddings into nodes, and map each tool call to its
nearest node to build a reasoning path. Surprisingly, we identify that the
ability to revisit a previously visited node, widely taken as a crucial pattern
in static reasoning, is often missing for agentic reasoning for many models.
Based on the insight, we propose a tool-augmented test-time scaling method to
improve LLM's agentic reasoning performance by adding tools to encourage models
to revisit. We expect our benchmark and the agentic reasoning framework to aid
future studies of understanding and pushing the boundaries of agentic
reasoning.

</details>


### [99] [The Thinking Spectrum: An Emperical Study of Tunable Reasoning in LLMs through Model Merging](https://arxiv.org/abs/2509.22034)
*Xiaochong Lan,Yu Zheng,Shiteng Cao,Yong Li*

Main category: cs.AI

TL;DR: 本研究通过对多种模型合并技术进行大规模实证分析，发现该方法能有效调控大型语言模型（LLM）的推理能力与计算效率之间的平衡，甚至实现帕累托改进，为创建定制化LLM提供实用指南。


<details>
  <summary>Details</summary>
Motivation: 实际应用中对具有可调推理能力的LLM需求日益增长，亟需兼顾推理深度与计算成本的模型生产方法。尽管模型合并作为一种免训练技术具有前景，但其在精细控制推理能力以创建模型谱系方面的潜力尚未充分探索。

Method: 进行大规模实证研究，评估一系列模型合并技术在多个推理基准上的表现。通过系统性地调整合并强度，构建准确性-效率曲线，以全面分析可调性能。

Result: 模型合并提供了一种有效且可控的方法，用于校准推理准确性与token效率之间的权衡，即使当父模型权重空间高度差异化时也成立。研究还发现了帕累托改进的实例，即合并模型在准确性和token消耗上均优于其某个父模型。

Conclusion: 本研究首次全面分析了模型合并在LLM可调推理能力方面的空间，为根据不同应用需求创建具有特定推理配置的LLM提供了实用指导。

Abstract: The growing demand for large language models (LLMs) with tunable reasoning
capabilities in many real-world applications highlights a critical need for
methods that can efficiently produce a spectrum of models balancing reasoning
depth and computational cost. Model merging has emerged as a promising,
training-free technique to address this challenge by arithmetically combining
the weights of a general-purpose model with a specialized reasoning model.
While various merging techniques exist, their potential to create a spectrum of
models with fine-grained control over reasoning abilities remains largely
unexplored. This work presents a large-scale empirical study evaluating a range
of model merging techniques across multiple reasoning benchmarks. We
systematically vary merging strengths to construct accuracy-efficiency curves,
providing the first comprehensive view of the tunable performance landscape.
Our findings reveal that model merging offers an effective and controllable
method for calibrating the trade-off between reasoning accuracy and token
efficiency, even when parent models have highly divergent weight spaces.
Crucially, we identify instances of Pareto Improvement, where a merged model
achieves both higher accuracy and lower token consumption than one of its
parents. Our study provides the first comprehensive analysis of this tunable
space, offering practical guidelines for creating LLMs with specific reasoning
profiles to meet diverse application demands.

</details>


### [100] [A2R: An Asymmetric Two-Stage Reasoning Framework for Parallel Reasoning](https://arxiv.org/abs/2509.22044)
*Ziqi Wang,Boye Niu,Zhongli Li,Linghui Meng,Jing Liu,Zhi Zheng,Tong Xu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.AI

TL;DR: 本文提出了A2R非对称两阶段推理框架，通过并行探索和综合来弥合大型推理模型单次尝试与潜在能力之间的差距，显著提升模型在复杂任务上的性能，并提供高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型的基础推理能力迅速发展，但模型在单次尝试中的表现与其潜在能力之间存在持续差距，这种差距只有通过多条解决方案路径才能显现，突显了模型已实现能力与固有能力之间的差异。

Method: A2R框架采用两阶段推理：首先，一个“探索者”模型通过重复采样并行生成潜在解决方案；随后，一个“综合者”模型整合这些参考，进行更精炼的第二阶段推理。这种两阶段过程允许计算与现有顺序方法正交地扩展。

Result: 1. A2R作为即插即用的并行推理框架，显著增强了模型在复杂问题上的能力。例如，Qwen3-8B-distill模型在使用A2R后，性能相比其自洽基线提升了75%。
2. 通过系统分析探索者和综合者角色，识别出一种有效的非对称扩展范式（A2R-Efficient），即“小到大”变体，结合Qwen3-4B探索者和Qwen3-8B综合者，其性能超越了单一Qwen3-32B模型的平均表现，同时成本降低了近30%。

Conclusion: A2R不仅是一个性能提升框架，也是一个适用于实际应用的高效且实用的解决方案。

Abstract: Recent Large Reasoning Models have achieved significant improvements in
complex task-solving capabilities by allocating more computation at the
inference stage with a "thinking longer" paradigm. Even as the foundational
reasoning capabilities of models advance rapidly, the persistent gap between a
model's performance in a single attempt and its latent potential, often
revealed only across multiple solution paths, starkly highlights the disparity
between its realized and inherent capabilities. To address this, we present
A2R, an Asymmetric Two-Stage Reasoning framework designed to explicitly bridge
the gap between a model's potential and its actual performance. In this
framework, an "explorer" model first generates potential solutions in parallel
through repeated sampling. Subsequently,a "synthesizer" model integrates these
references for a more refined, second stage of reasoning. This two-stage
process allows computation to be scaled orthogonally to existing sequential
methods. Our work makes two key innovations: First, we present A2R as a
plug-and-play parallel reasoning framework that explicitly enhances a model's
capabilities on complex questions. For example, using our framework, the
Qwen3-8B-distill model achieves a 75% performance improvement compared to its
self-consistency baseline. Second, through a systematic analysis of the
explorer and synthesizer roles, we identify an effective asymmetric scaling
paradigm. This insight leads to A2R-Efficient, a "small-to-big" variant that
combines a Qwen3-4B explorer with a Qwen3-8B synthesizer. This configuration
surpasses the average performance of a monolithic Qwen3-32B model at a nearly
30% lower cost. Collectively, these results show that A2R is not only a
performance-boosting framework but also an efficient and practical solution for
real-world applications.

</details>


### [101] [Generalizing Multi-Objective Search via Objective-Aggregation Functions](https://arxiv.org/abs/2509.22085)
*Hadar Peer,Eyal Weiss,Ron Alterovitz,Oren Salzman*

Main category: cs.AI

TL;DR: 本文提出一种通用的多目标搜索（MOS）问题表述，通过聚合隐藏目标来优化解决方案，使标准MOS算法能应用于复杂机器人问题，并显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 机器人系统需要平衡多个冲突目标，但现有复杂问题表述导致标准多目标搜索（MOS）算法无法直接应用。

Method: 提出一种通用的问题表述，通过隐藏（搜索）目标的聚合函数来优化解决方案目标。该表述仅需扩展标准MOS算法的几个核心操作即可使其适用。

Result: 在多种机器人规划问题中，通过扩展核心操作应用标准MOS算法，其性能比未进行目标聚合的原始算法高出几个数量级。

Conclusion: 所提出的通用问题表述和目标聚合方法，能有效使标准多目标搜索算法适用于复杂的机器人多目标问题，并显著提升求解性能。

Abstract: Multi-objective search (MOS) has become essential in robotics, as real-world
robotic systems need to simultaneously balance multiple, often conflicting
objectives. Recent works explore complex interactions between objectives,
leading to problem formulations that do not allow the usage of out-of-the-box
state-of-the-art MOS algorithms. In this paper, we suggest a generalized
problem formulation that optimizes solution objectives via aggregation
functions of hidden (search) objectives. We show that our formulation supports
the application of standard MOS algorithms, necessitating only to properly
extend several core operations to reflect the specific aggregation functions
employed. We demonstrate our approach in several diverse robotics planning
problems, spanning motion-planning for navigation, manipulation and planning fr
medical systems under obstacle uncertainty as well as inspection planning, and
route planning with different road types. We solve the problems using
state-of-the-art MOS algorithms after properly extending their core operations,
and provide empirical evidence that they outperform by orders of magnitude the
vanilla versions of the algorithms applied to the same problems but without
objective aggregation.

</details>


### [102] [Ground-Truthing AI Energy Consumption: Validating CodeCarbon Against External Measurements](https://arxiv.org/abs/2509.22092)
*Raphael Fischer*

Main category: cs.AI

TL;DR: 本研究评估了AI能耗估算工具的准确性，发现它们存在高达40%的误差，并为可持续AI发展提供了改进指南和实证依据。


<details>
  <summary>Details</summary>
Motivation: 尽管现有AI能耗估算工具（如ML Emissions Calculator和CodeCarbon）易于集成，但它们基于实用假设且忽略了关键因素，导致其估算准确性受到质疑。

Method: 本研究通过一个验证框架，在数百个AI实验中，将静态和动态能耗估算方法与真实测量值进行比较，系统性地评估了它们的可靠性。

Result: 研究发现，常用估算方法虽然大致遵循AI能耗模式，但其估算结果持续存在高达40%的误差。

Conclusion: 本研究提供了AI能耗估算质量和误差的实证证据，提升了透明度，并验证了广泛使用的工具。同时，它为改进现有技术制定了指导方针，并提供了可扩展验证的代码，为资源感知型ML和AI可持续性研究做出了重要贡献。

Abstract: Although machine learning (ML) and artificial intelligence (AI) present
fascinating opportunities for innovation, their rapid development is also
significantly impacting our environment. In response to growing
resource-awareness in the field, quantification tools such as the ML Emissions
Calculator and CodeCarbon were developed to estimate the energy consumption and
carbon emissions of running AI models. They are easy to incorporate into AI
projects, however also make pragmatic assumptions and neglect important
factors, raising the question of estimation accuracy. This study systematically
evaluates the reliability of static and dynamic energy estimation approaches
through comparisons with ground-truth measurements across hundreds of AI
experiments. Based on the proposed validation framework, investigative insights
into AI energy demand and estimation inaccuracies are provided. While generally
following the patterns of AI energy consumption, the established estimation
approaches are shown to consistently make errors of up to 40%. By providing
empirical evidence on energy estimation quality and errors, this study
establishes transparency and validates widely used tools for sustainable AI
development. It moreover formulates guidelines for improving the
state-of-the-art and offers code for extending the validation to other domains
and tools, thus making important contributions to resource-aware ML and AI
sustainability research.

</details>


### [103] [Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach](https://arxiv.org/abs/2509.22137)
*Seoyoung Lee,Seonbin Yoon,Seongbeen Lee,Hyesoo Kim,Joo Yong Sim*

Main category: cs.AI

TL;DR: Log2Plan通过结合两级规划框架和用户行为日志的任务挖掘，实现了鲁棒且适应性强的GUI任务自动化。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM/VLM的GUI自动化代理在泛化性、延迟和长序列任务连贯性方面表现不佳，难以应对UI变化或复杂任务。

Method: Log2Plan采用结构化两级规划框架和用户行为日志的任务挖掘方法。它通过将用户命令映射到结构化任务字典构建高层计划，并从用户行为日志中识别用户特定模式以支持个性化；然后根据实时GUI上下文将高层计划细化为低层动作序列。

Result: 在200个真实任务上，Log2Plan显著提高了任务成功率和执行时间。在长序列任务中，仍能保持60.0%以上的成功率，显示其在复杂多步工作流中的鲁棒性。

Conclusion: Log2Plan有效解决了现有GUI自动化代理的局限性，特别是在处理复杂和长序列任务时表现出卓越的鲁棒性和适应性。

Abstract: GUI task automation streamlines repetitive tasks, but existing LLM or
VLM-based planner-executor agents suffer from brittle generalization, high
latency, and limited long-horizon coherence. Their reliance on single-shot
reasoning or static plans makes them fragile under UI changes or complex tasks.
Log2Plan addresses these limitations by combining a structured two-level
planning framework with a task mining approach over user behavior logs,
enabling robust and adaptable GUI automation. Log2Plan constructs high-level
plans by mapping user commands to a structured task dictionary, enabling
consistent and generalizable automation. To support personalization and reuse,
it employs a task mining approach from user behavior logs that identifies
user-specific patterns. These high-level plans are then grounded into low-level
action sequences by interpreting real-time GUI context, ensuring robust
execution across varying interfaces. We evaluated Log2Plan on 200 real-world
tasks, demonstrating significant improvements in task success rate and
execution time. Notably, it maintains over 60.0% success rate even on
long-horizon task sequences, highlighting its robustness in complex, multi-step
workflows.

</details>


### [104] [Clinical Uncertainty Impacts Machine Learning Evaluations](https://arxiv.org/abs/2509.22242)
*Simone Lionetti,Fabian Gröger,Philippe Gottfrois,Alvaro Gonzalez-Jimenez,Ludovic Amruthalingam,Alexander A. Navarini,Marc Pouly*

Main category: cs.AI

TL;DR: 临床数据标签不确定性普遍存在，传统评估方法会掩盖。本文提出使用直接处理分布的概率度量来显式考虑标注不确定性，并在实验中证明其对模型排名有显著影响，呼吁社区采纳不确定性感知评估。


<details>
  <summary>Details</summary>
Motivation: 临床数据集标签通常不确定（标注者分歧，置信度不均），而多数投票等传统聚合方法会掩盖这种变异性，导致模型评估无法真实反映临床数据情况。

Method: 提出机器学习评估应采用直接作用于分布的概率度量，以显式考虑标注不确定性。这些度量计算开销小（排序后可实现线性时间），且可独立于标注生成过程。

Result: 在医学影像基准的简单实验中，考虑二元标签的置信度显著影响了模型排名，证明了不确定性评估的重要性。

Conclusion: 机器学习评估应采纳不确定性感知的概率度量，以更好地反映临床数据。呼吁社区发布原始标注并采用此类评估方法。

Abstract: Clinical dataset labels are rarely certain as annotators disagree and
confidence is not uniform across cases. Typical aggregation procedures, such as
majority voting, obscure this variability. In simple experiments on medical
imaging benchmarks, accounting for the confidence in binary labels
significantly impacts model rankings. We therefore argue that machine-learning
evaluations should explicitly account for annotation uncertainty using
probabilistic metrics that directly operate on distributions. These metrics can
be applied independently of the annotations' generating process, whether
modeled by simple counting, subjective confidence ratings, or probabilistic
response models. They are also computationally lightweight, as closed-form
expressions have linear-time implementations once examples are sorted by model
score. We thus urge the community to release raw annotations for datasets and
to adopt uncertainty-aware evaluation so that performance estimates may better
reflect clinical data.

</details>


### [105] [Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase Heuristics for 2D Bin-Packing](https://arxiv.org/abs/2509.22255)
*Syed Mahbubul Huq,Daniel Brito,Daniel Sikar,Rajesh Mojumder*

Main category: cs.AI

TL;DR: 本文提出了一个评估框架，结合LLM与演化算法解决2D装箱问题，并展示LLM（如GPT-4o）能高效生成更优解。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在组合优化领域的潜力，特别是2D装箱问题，并建立一套系统的评估框架和性能基准。

Method: 引入一套系统方法，将LLMs与演化算法结合，以迭代方式生成和优化启发式解决方案。通过与传统方法（Finite First-Fit和Hybrid First-Fit）进行综合实验比较，评估LLM生成启发式解的性能。

Result: LLMs能够生成比传统方法更高效的解决方案，且所需计算资源更少。GPT-4o能在两次迭代内达到最优解，将平均箱子使用量从16个减少到15个，空间利用率从0.76-0.78提高到0.83。

Conclusion: 该研究促进了对LLM在专业领域评估的理解，并为评估LLM在组合优化任务中的性能建立了基准。

Abstract: This paper presents an evaluation framework for assessing Large Language
Models' (LLMs) capabilities in combinatorial optimization, specifically
addressing the 2D bin-packing problem. We introduce a systematic methodology
that combines LLMs with evolutionary algorithms to generate and refine
heuristic solutions iteratively. Through comprehensive experiments comparing
LLM generated heuristics against traditional approaches (Finite First-Fit and
Hybrid First-Fit), we demonstrate that LLMs can produce more efficient
solutions while requiring fewer computational resources. Our evaluation reveals
that GPT-4o achieves optimal solutions within two iterations, reducing average
bin usage from 16 to 15 bins while improving space utilization from 0.76-0.78
to 0.83. This work contributes to understanding LLM evaluation in specialized
domains and establishes benchmarks for assessing LLM performance in
combinatorial optimization tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [106] [Discovering and Analyzing Stochastic Processes to Reduce Waste in Food Retail](https://arxiv.org/abs/2509.21322)
*Anna Kalenkova,Lu Xia,Dirk Neumann*

Main category: cs.LG

TL;DR: 本文提出一种结合OCPM和随机过程分析的新方法，用于分析食品零售流程以减少食物浪费，通过优化供需平衡来防止浪费和短缺。


<details>
  <summary>Details</summary>
Motivation: 通过分析食品零售流程来减少食物浪费，并优化供应链策略。

Method: 整合面向对象过程挖掘（OCPM）与随机过程发现及分析。具体包括：1) 从销售数据中发现连续时间马尔可夫链；2) 将模型扩展到包含供应活动；3) 进行情景分析（what-if analysis）评估店内产品数量随时间的变化。

Result: 能够识别客户购买行为与供应策略之间的最佳平衡点。

Conclusion: 该方法有助于防止因供应过剩造成的食物浪费和产品短缺，实现供需的有效平衡。

Abstract: This paper proposes a novel method for analyzing food retail processes with a
focus on reducing food waste. The approach integrates object-centric process
mining (OCPM) with stochastic process discovery and analysis. First, a
stochastic process in the form of a continuous-time Markov chain is discovered
from grocery store sales data. This model is then extended with supply
activities. Finally, a what-if analysis is conducted to evaluate how the
quantity of products in the store evolves over time. This enables the
identification of an optimal balance between customer purchasing behavior and
supply strategies, helping to prevent both food waste due to oversupply and
product shortages.

</details>


### [107] [Impact of Loss Weight and Model Complexity on Physics-Informed Neural Networks for Computational Fluid Dynamics](https://arxiv.org/abs/2509.21393)
*Yi En Chou,Te Hsin Liu,Chao An Lin*

Main category: cs.LG

TL;DR: PINNs在PDE求解中对损失权重敏感。本文提出量纲分析的两种权重方案，其中结合不可量化项的方案，显著提升了PINNs在复杂CFD问题（如高Peclet数对流扩散）中的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络（PINNs）虽为无网格PDE求解框架，但其性能极易受损失权重选择影响。

Method: 提出两种基于量纲分析的权重方案：一种仅基于可量化项；另一种除可量化项外，还结合了不可量化项以实现更均衡的训练。

Result: 基准测试（热传导、对流扩散、盖驱动腔流）表明，第二种方案（含不可量化项）相比等权重分配，能持续提高PINNs的稳定性和准确性。尤其在高Peclet数对流扩散问题中，该方案使PINNs实现了稳定、准确的预测，而传统求解器在此类问题上通常失效。

Conclusion: 本文提出的权重方案显著提升了PINNs在计算流体力学（CFD）问题中的鲁棒性和泛化能力，特别适用于处理传统方法难以解决的复杂流动问题。

Abstract: Physics Informed Neural Networks offer a mesh free framework for solving PDEs
but are highly sensitive to loss weight selection. We propose two dimensional
analysis based weighting schemes, one based on quantifiable terms, and another
also incorporating unquantifiable terms for more balanced training. Benchmarks
on heat conduction, convection diffusion, and lid driven cavity flows show that
the second scheme consistently improves stability and accuracy over equal
weighting. Notably, in high Peclet number convection diffusion, where
traditional solvers fail, PINNs with our scheme achieve stable, accurate
predictions, highlighting their robustness and generalizability in CFD
problems.

</details>


### [108] [LLMs for Bayesian Optimization in Scientific Domains: Are We There Yet?](https://arxiv.org/abs/2509.21403)
*Rushil Gupta,Jason Hartford,Bang Liu*

Main category: cs.LG

TL;DR: 研究发现现有大型语言模型（LLMs）无法有效进行情境内实验设计，其性能不如传统方法。文章提出了一种混合方法LLMNN，结合LLM先验知识与最近邻采样，取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）作为通用代理在情境内实验设计方面的能力，以验证其相关主张。

Method: 使用开放和闭源指令调优LLMs，在基因扰动和分子性质发现任务上进行评估。通过替换真实结果为随机标签来测试LLM对实验反馈的敏感性。将LLM代理与线性匪徒和高斯过程优化等经典方法进行比较。提出并测试了一种名为LLMNN（LLM-guided Nearest Neighbour）的混合采样方法。

Result: LLM代理对实验反馈不敏感，用随机标签替换真实结果对其性能无影响。经典方法（如线性匪徒和高斯过程优化）的性能始终优于LLM代理。提出的LLMNN混合方法在多个领域取得了竞争或更优的性能，且无需大量情境内适应。

Conclusion: 目前的开放和闭源LLMs实际上不能进行情境内实验设计。需要开发混合框架，将基于先验知识的推理与基于更新后验的批次采集解耦。

Abstract: Large language models (LLMs) have recently been proposed as general-purpose
agents for experimental design, with claims that they can perform in-context
experimental design. We evaluate this hypothesis using both open- and
closed-source instruction-tuned LLMs applied to genetic perturbation and
molecular property discovery tasks. We find that LLM-based agents show no
sensitivity to experimental feedback: replacing true outcomes with randomly
permuted labels has no impact on performance. Across benchmarks, classical
methods such as linear bandits and Gaussian process optimization consistently
outperform LLM agents. We further propose a simple hybrid method, LLM-guided
Nearest Neighbour (LLMNN) sampling, that combines LLM prior knowledge with
nearest-neighbor sampling to guide the design of experiments. LLMNN achieves
competitive or superior performance across domains without requiring
significant in-context adaptation. These results suggest that current open- and
closed-source LLMs do not perform in-context experimental design in practice
and highlight the need for hybrid frameworks that decouple prior-based
reasoning from batch acquisition with updated posteriors.

</details>


### [109] [Object Identification Under Known Dynamics: A PIRNN Approach for UAV Classification](https://arxiv.org/abs/2509.21405)
*Nyi Nyi Aung,Neil Muralles,Adrian Stein*

Main category: cs.LG

TL;DR: 本文提出一种结合物理知识的残差神经网络，用于在已知动力学条件下进行无人机物体识别，实现了高分类精度和更短的训练时间。


<details>
  <summary>Details</summary>
Motivation: 解决无人机应用中在已知动力学下进行物体识别的问题，并结合学习与分类。

Method: 开发了一种物理信息残差神经网络（PI-ResNet）框架。该框架利用物理信息学习进行状态映射和状态导数预测，并通过Softmax层进行多类别置信度估计。在四旋翼、固定翼和直升机上进行了案例研究。

Result: 实验结果显示，该方法具有高分类精度，并显著减少了训练时间。

Conclusion: 该方案为在底层动力学清晰的领域解决系统识别问题提供了一个有前景的解决方案。

Abstract: This work addresses object identification under known dynamics in unmanned
aerial vehicle applications, where learning and classification are combined
through a physics-informed residual neural network. The proposed framework
leverages physics-informed learning for state mapping and state-derivative
prediction, while a softmax layer enables multi-class confidence estimation.
Quadcopter, fixed-wing, and helicopter aerial vehicles are considered as case
studies. The results demonstrate high classification accuracy with reduced
training time, offering a promising solution for system identification problems
in domains where the underlying dynamics are well understood.

</details>


### [110] [Null-Space Filtering for Data-Free Continual Model Merging: Preserving Transparency, Promoting Fidelity](https://arxiv.org/abs/2509.21413)
*Zihuan Qiu,Lei Wang,Yang Cao,Runtong Zhang,Bing Su,Yi Xu,Fanman Meng,Linfeng Xu,Qingbo Wu,Hongliang Li*

Main category: cs.LG

TL;DR: 本文提出NUFILT，一个无数据持续模型合并框架，通过零空间滤波和LoRA适配器，在参数空间解决旧任务“透明性”和新任务“保真度”的挑战，实现了领先的性能并减少遗忘。


<details>
  <summary>Details</summary>
Motivation: 无数据持续模型合并(DFCMM)面临核心挑战：如何在无任务数据的情况下，既能确保模型对现有任务的“透明性”（不干扰），又能实现对新任务的“保真度”（有效适应）。现有方法难以将数据层面的需求与参数空间优化相结合，以同时满足这两个目标。

Method: 本文提出NUFILT (NUll-space FILTering) 框架。其核心思想是任务向量与表示子空间近似对齐。为此，NUFILT设计了一个零空间投影器，通过过滤新任务向量中与旧任务重叠的组件来保留先前的响应，从而确保透明性。同时，引入一个轻量级LoRA适配器，注入互补的任务特定信号，以实现对新任务的保真度。该适配器通过基于投影的替代损失进行训练，以在引入新方向的同时保持与先前知识的一致性。这个联合的滤波-适应过程允许主干模型吸收新知识并保持现有行为。最终，更新以分层线性方式融合，不产生额外参数或推理成本。理论上，该方法建立了近似子空间对齐的保证来支持零空间滤波的有效性。

Result: 在视觉和NLP基准测试中，NUFILT取得了最先进的性能，并具有最小的遗忘。与OPCM和WUDI-Merging等现有方法相比，平均准确率提高了4-7%，同时缩小了与完全微调的性能差距并降低了计算开销。

Conclusion: NUFILT成功地解决了无数据持续模型合并中透明性和保真度的难题。通过创新性地将零空间滤波与LoRA适配器结合，并在无数据条件下实现参数空间优化，NUFILT在多个基准测试中展现出卓越的性能、最小的遗忘和更高的计算效率，为持续学习模型融合提供了一个有效的解决方案。

Abstract: Data-free continual model merging (DFCMM) aims to fuse independently
fine-tuned models into a single backbone that evolves with incoming tasks
without accessing task data. This paper formulate two fundamental desiderata
for DFCMM: transparency, avoiding interference with earlier tasks, and
fidelity, adapting faithfully to each new task. This poses a challenge that
existing approaches fail to address: how to bridge data-level desiderata with
parameter-space optimization to ensure transparency and fidelity in the absence
of task data. To this end, we propose NUFILT (NUll-space FILTering), a
data-free framework that directly links these desiderata to optimization. Our
key observation is that task vectors approximately align with representation
subspaces, providing structural surrogates for enforcing transparency and
fidelity. Accordingly, we design a null-space projector that preserves prior
responses by filtering out overlapping components of new task vectors, thereby
ensuring transparency, and a lightweight LoRA adapter that injects
complementary task-specific signals, enabling fidelity in adapting to new
tasks. The adapter is trained with a projection-based surrogate loss to retain
consistency with previous knowledge while introducing novel directions. This
joint filtering-adaptation process allows the backbone to absorb new knowledge
while retaining existing behaviors, and the updates are finally fused back in a
layer-wise linear fashion without extra parameters or inference cost.
Theoretically, we establish approximate subspace alignment guarantees that
justify null-space filtering. Empirically, NUFILT achieves state-of-the-art
performance with minimal forgetting on both vision and NLP benchmarks,
improving average accuracy by 4-7% over OPCM and WUDI-Merging, while narrowing
the gap to fine-tuning and reducing computation overhead.

</details>


### [111] [Forecasting Seismic Waveforms: A Deep Learning Approach for Einstein Telescope](https://arxiv.org/abs/2509.21446)
*Waleed Esmail,Alexander Kappes,Stuart Russell,Christine Thomas*

Main category: cs.LG

TL;DR: SeismoGPT是一种基于Transformer的自回归模型，能预测未来引力波探测器的三分量地震波形，以支持噪声缓解和实时控制。


<details>
  <summary>Details</summary>
Motivation: 为未来引力波探测器（如爱因斯坦望远镜）提供地震波形预测，以支持牛顿噪声缓解和实时观测站控制。

Method: 引入SeismoGPT模型，一个基于Transformer的自回归模型，通过直接学习波形数据中的时空依赖性，支持单站和阵列输入。

Result: 模型能捕捉真实的地面运动模式并提供准确的短期预测，预测性能在即时窗口内良好，并随时间推移逐渐下降。

Conclusion: 该方法为数据驱动的地震预测奠定了基础，可用于支持牛顿噪声缓解和实时观测控制。

Abstract: We introduce \textit{SeismoGPT}, a transformer-based model for forecasting
three-component seismic waveforms in the context of future gravitational wave
detectors like the Einstein Telescope. The model is trained in an
autoregressive setting and can operate on both single-station and array-based
inputs. By learning temporal and spatial dependencies directly from waveform
data, SeismoGPT captures realistic ground motion patterns and provides accurate
short-term forecasts. Our results show that the model performs well within the
immediate prediction window and gradually degrades further ahead, as expected
in autoregressive systems. This approach lays the groundwork for data-driven
seismic forecasting that could support Newtonian noise mitigation and real-time
observatory control.

</details>


### [112] [Talking Trees: Reasoning-Assisted Induction of Decision Trees for Tabular Data](https://arxiv.org/abs/2509.21465)
*George Yakushev,Alina Shutova,Ivan Rubachev,Renat Sergazinov,Artem Babenko*

Main category: cs.LG

TL;DR: 本文提出使用具有推理能力的LLM在智能体环境中，通过工具诱导决策树来处理低资源表格数据。该方法生成的决策树优于传统CART，并提供可解释的推理过程和人工干预能力，以解决黑盒模型的解释性和成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有表格基础模型虽在低资源表格问题上表现卓越，但其黑盒特性导致难以解释且推理成本高昂，缺乏透明度。

Method: 研究采用一种替代策略：在智能体设置中使用具备推理能力的LLM来为小型表格数据集诱导决策树。为此，设计了一套最少化的工具，用于构建、分析和操作决策树。

Result: LLM利用这些工具结合先验知识和数据学习，创建了轻量级决策树，在低资源表格问题上性能优于传统CART。尽管单个决策树未能超越最先进的黑盒模型，但它提供了人类可读的推理轨迹，可用于检查偏差和数据泄露，并允许人工输入以纠正偏差或融入领域知识。

Conclusion: 该工作为低资源表格问题提供了一种可解释、可人工干预的替代方案，通过LLM诱导的决策树实现了比传统方法更好的性能，并解决了黑盒模型的透明度问题，尽管其原始性能可能不及最先进的黑盒模型。

Abstract: Tabular foundation models are becoming increasingly popular for low-resource
tabular problems. These models make up for small training datasets by
pretraining on large volumes of synthetic data. The prior knowledge obtained
via pretraining provides the exceptional performance, but the resulting model
becomes a black box that is difficult to interpret and costly to inference. In
this work, we explore an alternative strategy: using reasoning-capable LLMs to
induce decision trees for small tabular datasets in agentic setup. We design a
minimal set of tools for constructing, analyzing and manipulating decision
trees. By using these tools, LLMs combine their prior knowledge with learning
from data to create a lightweight decision tree that outperforms traditional
CART on low-resource tabular problems. While a single decision tree does not
outperform state-of-the-art black box models, it comes with a human-readable
reasoning trace that can be checked for biases and data leaks. Furthermore, the
reasoning-based LLM's creation process allows for additional human input:
correcting biases or incorporating domain-specific intuition that is not
captured in the data.

</details>


### [113] [Score-based Idempotent Distillation of Diffusion Models](https://arxiv.org/abs/2509.21470)
*Shehtab Zaman,Chengyan Liu,Kenneth Chiu*

Main category: cs.LG

TL;DR: 本文提出SIGN（Score-based Idempotent Generative Networks），通过从扩散模型的得分中提炼幂等模型，结合了扩散模型的稳定性与幂等生成网络的灵活性和效率，解决了传统IGN的训练不稳定性和扩散模型推理成本高的问题，实现了更快的生成速度和最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的幂等生成网络（IGNs）需要对抗训练，易出现训练不稳定和模式坍塌。扩散模型虽然训练稳定且生成质量高，但计算成本高昂。研究旨在结合两者的优点，开发一个既稳定高效，又能进行灵活生成的新型生成模型。

Method: 本文提出了SIGN（Score-based Idempotent Generative Networks），通过从预训练的扩散模型得分中提炼出幂等模型来融合扩散模型与IGN。该方法基于理论分析的得分训练，避免了对抗性损失。

Result: SIGNs具有高度的训练稳定性，无需对抗性损失。其推理速度快于迭代式得分模型，并支持多步采样以平衡质量与效率。模型能直接在源域操作，将受损或替代分布映射回目标流形，实现零样本输入编辑。在CIFAR和CelebA数据集上，SIGNs在幂等模型中取得了最先进的成果。

Conclusion: SIGNs成功地将扩散模型的稳定性和高质量生成能力与IGNs的灵活生成相结合，解决了两者各自的局限性。它提供了一种高效、稳定且高质量的生成范式，并具备独特的零样本编辑能力。

Abstract: Idempotent generative networks (IGNs) are a new line of generative models
based on idempotent mapping to a target manifold. IGNs support both single-and
multi-step generation, allowing for a flexible trade-off between computational
cost and sample quality. But similar to Generative Adversarial Networks (GANs),
conventional IGNs require adversarial training and are prone to training
instabilities and mode collapse. Diffusion and score-based models are popular
approaches to generative modeling that iteratively transport samples from one
distribution, usually a Gaussian, to a target data distribution. These models
have gained popularity due to their stable training dynamics and high-fidelity
generation quality. However, this stability and quality come at the cost of
high computational cost, as the data must be transported incrementally along
the entire trajectory. New sampling methods, model distillation, and
consistency models have been developed to reduce the sampling cost and even
perform one-shot sampling from diffusion models. In this work, we unite
diffusion and IGNs by distilling idempotent models from diffusion model scores,
called SIGN. Our proposed method is highly stable and does not require
adversarial losses. We provide a theoretical analysis of our proposed
score-based training methods and empirically show that IGNs can be effectively
distilled from a pre-trained diffusion model, enabling faster inference than
iterative score-based models. SIGNs can perform multi-step sampling, allowing
users to trade off quality for efficiency. These models operate directly on the
source domain; they can project corrupted or alternate distributions back onto
the target manifold, enabling zero-shot editing of inputs. We validate our
models on multiple image datasets, achieving state-of-the-art results for
idempotent models on the CIFAR and CelebA datasets.

</details>


### [114] [Are Hallucinations Bad Estimations?](https://arxiv.org/abs/2509.21473)
*Hude Liu,Jerry Yao-Chieh Hu,Jennifer Yuntong Zhang,Zhao Song,Han Liu*

Main category: cs.LG

TL;DR: 研究表明，生成模型中的幻觉是普遍存在的，即使是损失最小化的最优估计器也无法避免，这是由于损失最小化与人类可接受输出之间存在结构性错位。


<details>
  <summary>Details</summary>
Motivation: 论文旨在形式化生成模型中的幻觉现象，并解释为何即使是最佳模型也可能产生幻觉，将其重新定义为一种深层次的结构性问题而非简单的估计误差。

Method: 通过将幻觉形式化为估计与合理原因之间的失败，论文展示了即使是损失最小化的最优估计器也会产生幻觉。并通过一个针对通用数据分布的幻觉率高概率下界进行数学证明，最后通过硬币聚合、开放式问答和文本到图像任务的实验来支持其理论。

Result: 研究发现，即使是损失最小化的最优估计器也仍会产生幻觉。论文给出了一个针对通用数据分布的幻觉率的通用高概率下界。这表明幻觉是损失最小化与人类可接受输出之间的结构性错位，即由校准失误引起的估计误差。

Conclusion: 结论是幻觉是生成模型的一个固有问题，它源于损失最小化和人类可接受输出之间的结构性错位，而非简单的估计误差，需要重新审视校准问题。

Abstract: We formalize hallucinations in generative models as failures to link an
estimate to any plausible cause. Under this interpretation, we show that even
loss-minimizing optimal estimators still hallucinate. We confirm this with a
general high probability lower bound on hallucinate rate for generic data
distributions. This reframes hallucination as structural misalignment between
loss minimization and human-acceptable outputs, and hence estimation errors
induced by miscalibration. Experiments on coin aggregation, open-ended QA, and
text-to-image support our theory.

</details>


### [115] [d2: Improved Techniques for Training Reasoning Diffusion Language Models](https://arxiv.org/abs/2509.21474)
*Guanghan Wang,Yair Schiff,Gilad Turok,Volodymyr Kuleshov*

Main category: cs.LG

TL;DR: d2框架通过新策略梯度算法，显著提升了掩码扩散语言模型在逻辑和数学推理任务上的表现，超越现有方法并达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散语言模型在文本生成方面表现出色，但通过强化学习提升其推理能力仍是活跃的研究领域。

Method: 引入了d2推理框架，专为掩码扩散语言模型设计。核心是新的策略梯度算法，利用掩码特性准确估计采样轨迹的似然。该估计器在计算和近似精度之间权衡，并对支持任意顺序似然估计的DLM特别有效。

Result: d2显著优于先前的扩散推理框架（仅使用RL，不依赖SFT），并在逻辑推理任务（Countdown和Sudoku）和数学推理基准（GSM8K和MATH500）上为DLM设定了新的SOTA性能。

Conclusion: d2框架通过其创新的策略梯度算法和对DLM特性（如任意顺序似然估计）的利用，有效提升了扩散语言模型的推理能力，并在多项基准测试中取得了领先成果。

Abstract: While diffusion language models (DLMs) have achieved competitive performance
in text generation, improving their reasoning ability with reinforcement
learning remains an active research area. Here, we introduce d2, a reasoning
framework tailored for masked DLMs. Central to our framework is a new policy
gradient algorithm that relies on properties of masking to accurately estimate
the likelihoods of sampling trajectories. Our estimators trade off computation
for approximation accuracy in an analytically tractable manner, and are
particularly effective for DLMs that support any-order likelihood estimation.
We characterize and study this property in popular DLMs and show that it is key
for efficient diffusion-based reasoning. Empirically, d2 significantly improves
over previous diffusion reasoning frameworks using only RL (without relying on
supervised fine-tuning), and sets a new state-of-the-art performance for DLMs
on logical reasoning tasks (Countdown and Sudoku) and math reasoning benchmarks
(GSM8K and MATH500).

</details>


### [116] [VISION: Prompting Ocean Vertical Velocity Reconstruction from Incomplete Observations](https://arxiv.org/abs/2509.21477)
*Yuan Gao,Hao Wu,Qingsong Wen,Kun Wang,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 本文提出了一个高分辨率海洋动力学基准KD48，并引入了VISION模型，该模型基于动态提示机制，能够从不完整的表面观测中高精度重建次表层海洋动力学，在极端数据缺失下表现卓越。


<details>
  <summary>Details</summary>
Motivation: 从不完整的表面观测数据中重建次表层海洋动力学（如垂直速度场）是地球科学面临的关键挑战，且长期缺乏标准化、可用于分析的基准。

Method: 1. 构建并发布了高分辨率海洋动力学基准KD48，该基准源自拍级模拟并经过专家去噪处理。2. 引入了VISION，一种基于动态提示（Dynamic Prompting）的新型重建范式，旨在解决真实观测中数据缺失的核心问题。3. VISION能够根据任何可用观测子集即时生成视觉提示，编码数据可用性和海洋物理状态。4. 设计了一个状态条件提示（State-conditioned Prompting）模块，将此提示高效注入到具备几何和尺度感知算子的通用骨干网络中，以指导其自适应调整计算策略。

Result: 1. 在KD48基准上的广泛实验表明，VISION模型不仅显著优于现有最先进模型。2. VISION在极端数据缺失场景下也展现出强大的泛化能力。

Conclusion: 本工作通过提供高质量的基准和鲁棒的模型，为数据不确定性下的海洋科学研究建立了坚实的基础设施。

Abstract: Reconstructing subsurface ocean dynamics, such as vertical velocity fields,
from incomplete surface observations poses a critical challenge in Earth
science, a field long hampered by the lack of standardized, analysis-ready
benchmarks. To systematically address this issue and catalyze research, we
first build and release KD48, a high-resolution ocean dynamics benchmark
derived from petascale simulations and curated with expert-driven denoising.
Building on this benchmark, we introduce VISION, a novel reconstruction
paradigm based on Dynamic Prompting designed to tackle the core problem of
missing data in real-world observations. The essence of VISION lies in its
ability to generate a visual prompt on-the-fly from any available subset of
observations, which encodes both data availability and the ocean's physical
state. More importantly, we design a State-conditioned Prompting module that
efficiently injects this prompt into a universal backbone, endowed with
geometry- and scale-aware operators, to guide its adaptive adjustment of
computational strategies. This mechanism enables VISION to precisely handle the
challenges posed by varying input combinations. Extensive experiments on the
KD48 benchmark demonstrate that VISION not only substantially outperforms
state-of-the-art models but also exhibits strong generalization under extreme
data missing scenarios. By providing a high-quality benchmark and a robust
model, our work establishes a solid infrastructure for ocean science research
under data uncertainty. Our codes are available at:
https://github.com/YuanGao-YG/VISION.

</details>


### [117] [Filtering with Confidence: When Data Augmentation Meets Conformal Prediction](https://arxiv.org/abs/2509.21479)
*Zixuan Wu,So Won Jeong,Yating Liu,Yeo Jin Jung,Claire Donnat*

Main category: cs.LG

TL;DR: 本文提出共形数据增强（Conformal Data Augmentation），一个利用共形预测的原则性数据过滤框架，用于生成高质量、多样化的合成数据，以解决数据稀缺问题并提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺是当前数据密集型模型面临的挑战，合成数据增强是可行的解决方案。然而，合成数据需在减少估计器方差的同时引入最小偏差，生成与原始数据同分布的多样化样本至关重要，因此控制偏差是关键研究动机。

Method: 本文提出了共形数据增强（conformal data augmentation）方法，这是一个原则性数据过滤框架。它利用共形预测（conformal prediction）的强大能力来生成多样化的合成数据，同时通过可证明的风险控制过滤掉低质量的生成样本。该方法易于实现，无需访问内部模型logit或进行大规模模型再训练。

Result: 该方法在主题预测、情感分析、图像分类和欺诈检测等多个任务中均表现出有效性，相对于未增强的基线，F1分数持续提升高达40%；相对于其他过滤增强基线，F1分数提升4%。

Conclusion: 共形数据增强通过提供一种具有可证明风险控制能力的原则性数据过滤框架，有效解决了合成数据质量和偏差控制问题，显著提升了模型在多种应用场景下的性能。

Abstract: With promising empirical performance across a wide range of applications,
synthetic data augmentation appears a viable solution to data scarcity and the
demands of increasingly data-intensive models. Its effectiveness lies in
expanding the training set in a way that reduces estimator variance while
introducing only minimal bias. Controlling this bias is therefore critical:
effective data augmentation should generate diverse samples from the same
underlying distribution as the training set, with minimal shifts. In this
paper, we propose conformal data augmentation, a principled data filtering
framework that leverages the power of conformal prediction to produce diverse
synthetic data while filtering out poor-quality generations with provable risk
control. Our method is simple to implement, requires no access to internal
model logits, nor large-scale model retraining. We demonstrate the
effectiveness of our approach across multiple tasks, including topic
prediction, sentiment analysis, image classification, and fraud detection,
showing consistent performance improvements of up to 40% in F1 score over
unaugmented baselines, and 4% over other filtered augmentation baselines.

</details>


### [118] [High-Probability Analysis of Online and Federated Zero-Order Optimisation](https://arxiv.org/abs/2509.21484)
*Arya Akhavan,David Janz,El-Mahdi El-Mhamdi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study distributed learning in the setting of gradient-free zero-order
optimization and introduce FedZero, a federated zero-order algorithm that
delivers sharp theoretical guarantees. Specifically, FedZero: (1) achieves
near-optimal optimization error bounds with high probability in the federated
convex setting; and (2) in the single-worker regime-where the problem reduces
to the standard zero-order framework, establishes the first high-probability
convergence guarantees for convex zero-order optimization, thereby
strengthening the classical expectation-based results. At its core, FedZero
employs a gradient estimator based on randomization over the $\ell_1$-sphere.
To analyze it, we develop new concentration inequalities for Lipschitz
functions under the uniform measure on the $\ell_1$-sphere, with explicit
constants. These concentration tools are not only central to our
high-probability guarantees but may also be of independent interest.

</details>


### [119] [Neural Operators for Mathematical Modeling of Transient Fluid Flow in Subsurface Reservoir Systems](https://arxiv.org/abs/2509.21485)
*Daniil D. Sirota,Sergey A. Khan,Sergey L. Kostikov,Kirill A. Butov*

Main category: cs.LG

TL;DR: 本文提出一种基于傅里叶神经算子（TFNO-opt）的瞬态流体地下储层系统建模方法，通过多项改进显著提高了计算速度和稳定性，并将其应用于地下储气库建模，实现比传统方法快六个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在模拟复杂的地下储层系统时，计算成本高昂，限制了其在控制和决策支持问题中的应用。因此，需要开发更快速、高效的建模方法。

Method: 开发了一种名为TFNO-opt的神经算子架构，基于傅里叶神经算子。主要改进包括：调整积分傅里叶算子的内部时间分辨率、在谱域中对参数进行张量分解、在误差函数中使用Sobolev范数，以及分离近似误差和初始条件重建，以提高准确性和稳定性。

Result: 计算实验证实了所提出改进的有效性。在地下储气库（UGS）水动力建模问题中，TFNO-opt方法实现了比传统方法快六个数量级的计算加速，且提高了神经算子的准确性和稳定性。

Conclusion: TFNO-opt方法为复杂储层系统的有效控制开辟了新机遇，通过显著加速计算，展现了在水动力建模中的重要实际意义。

Abstract: This paper presents a method for modeling transient fluid flow in subsurface
reservoir systems based on the developed neural operator architecture
(TFNO-opt). Reservoir systems are complex dynamic objects with distributed
parameters described by systems of partial differential equations (PDEs).
Traditional numerical methods for modeling such systems, despite their high
accuracy, are characterized by significant time costs for performing
calculations, which limits their applicability in control and decision support
problems. The proposed architecture (TFNO-opt) is based on Fourier neural
operators, which allow approximating PDE solutions in infinite-dimensional
functional spaces, providing invariance to discretization and the possibility
of generalization to various implementations of equations. The developed
modifications are aimed at increasing the accuracy and stability of the trained
neural operator, which is especially important for control problems. These
include adjustable internal time resolution of the integral Fourier operator,
tensor decomposition of parameters in the spectral domain, use of the Sobolev
norm in the error function, and separation of approximation errors and
reconstruction of initial conditions for more accurate reproduction of physical
processes. The effectiveness of the proposed improvements is confirmed by
computational experiments. The practical significance is confirmed by
computational experiments using the example of the problem of hydrodynamic
modeling of an underground gas storage (UGS), where the acceleration of
calculations by six orders of magnitude was achieved, compared to traditional
methods. This opens up new opportunities for the effective control of complex
reservoir systems.

</details>


### [120] [GraphPFN: A Prior-Data Fitted Graph Foundation Model](https://arxiv.org/abs/2509.21489)
*Dmitry Eremeev,Oleg Platonov,Gleb Bazhenov,Artem Babenko,Liudmila Prokhorenkova*

Main category: cs.LG

TL;DR: 本文提出GraphPFN，一种通过在合成图上预训练来构建图基础模型的方法。它结合了表格基础模型和图邻域聚合，并在真实世界图数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图基础模型（如G2T-FM）主要依赖手工图特征，限制了其学习复杂图特有模式的能力，而图数据仍是基础模型应用有限的领域。

Method: GraphPFN首先设计了一个合成属性图的先验分布，结合随机块模型和优先连接过程生成图结构，并使用图感知因果模型生成节点属性和目标。然后，它通过注意力机制的图邻域聚合层增强了表格基础模型LimiX，并在从该先验分布中采样的合成图上进行训练。

Result: GraphPFN在高达50,000个节点的真实世界图数据集上展示了强大的上下文学习能力，并在微调后取得了最先进的结果，在大多数数据集上优于G2T-FM和从头训练的任务特定GNN。研究还表明，在来自精心设计的先验分布的合成图上进行预训练是构建图基础模型的有效策略。

Conclusion: 预训练于来自精心设计的先验分布的合成图是构建图基础模型的有效策略。GraphPFN通过此方法成功捕捉图结构依赖性，并在节点级预测任务上实现了卓越性能。

Abstract: Foundation models pretrained on large-scale datasets have transformed such
fields as natural language processing and computer vision, but their
application to graph data remains limited. Recently emerged graph foundation
models, such as G2T-FM, utilize tabular foundation models for graph tasks and
were shown to significantly outperform prior attempts to create GFMs. However,
these models primarily rely on hand-crafted graph features, limiting their
ability to learn complex graph-specific patterns. In this work, we propose
GraphPFN: a prior-data fitted network for node-level prediction. First, we
design a prior distribution of synthetic attributed graphs. For graph structure
generation, we use a novel combination of multiple stochastic block models and
a preferential attachment process. We then apply graph-aware structured causal
models to generate node attributes and targets. This procedure allows us to
efficiently generate a wide range of realistic graph datasets. Then, we augment
the tabular foundation model LimiX with attention-based graph neighborhood
aggregation layers and train it on synthetic graphs sampled from our prior,
allowing the model to capture graph structural dependencies not present in
tabular data. On diverse real-world graph datasets with up to 50,000 nodes,
GraphPFN shows strong in-context learning performance and achieves
state-of-the-art results after finetuning, outperforming both G2T-FM and
task-specific GNNs trained from scratch on most datasets. More broadly, our
work demonstrates that pretraining on synthetic graphs from a well-designed
prior distribution is an effective strategy for building graph foundation
models.

</details>


### [121] [SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models](https://arxiv.org/abs/2509.21498)
*Arani Roy,Shristi Das Biswas,Kaushik Roy*

Main category: cs.LG

TL;DR: 扩散模型计算成本高。SlimDiff提出一种无梯度、激活感知的结构压缩框架，通过谱近似和动态剪枝，实现了显著加速和参数减少，同时保持生成质量，且无需训练和大量校准样本。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）虽生成性能优异，但其数十亿级参数和迭代去噪过程导致计算成本极高。现有效率技术（如量化、步长减少、剪枝）需微调或重训练才能恢复性能，存在瓶颈。

Method: 引入SlimDiff，一种自动化、激活感知的结构压缩框架，可完全无梯度地减少DMs中注意力和前馈层的维度。该方法将DM压缩重构为谱近似任务，利用去噪时间步长的激活协方差定义低秩子空间，指导固定压缩预算下的动态剪枝。通过对功能权重组（Q-K交互、V-O耦合、前馈投影）进行模块化分解，并自适应分配稀疏性，以减轻误差累积。

Result: SlimDiff实现了高达35%的加速和约1亿参数的减少，生成质量与未压缩模型相当，且无需任何反向传播。该方法仅需约500个校准样本，比现有方法少70倍以上。

Conclusion: SlimDiff是首个闭式、激活引导的、完全免训练的DM结构压缩方法，提供了理论清晰性和实践效率，有效解决了扩散模型的计算效率问题。

Abstract: Diffusion models (DMs), lauded for their generative performance, are
computationally prohibitive due to their billion-scale parameters and iterative
denoising dynamics. Existing efficiency techniques, such as quantization,
timestep reduction, or pruning, offer savings in compute, memory, or runtime
but are strictly bottlenecked by reliance on fine-tuning or retraining to
recover performance. In this work, we introduce SlimDiff, an automated
activation-informed structural compression framework that reduces both
attention and feedforward dimensionalities in DMs, while being entirely
gradient-free. SlimDiff reframes DM compression as a spectral approximation
task, where activation covariances across denoising timesteps define low-rank
subspaces that guide dynamic pruning under a fixed compression budget. This
activation-aware formulation mitigates error accumulation across timesteps by
applying module-wise decompositions over functional weight groups: query--key
interactions, value--output couplings, and feedforward projections, rather than
isolated matrix factorizations, while adaptively allocating sparsity across
modules to respect the non-uniform geometry of diffusion trajectories. SlimDiff
achieves up to 35\% acceleration and $\sim$100M parameter reduction over
baselines, with generation quality on par with uncompressed models without any
backpropagation. Crucially, our approach requires only about 500 calibration
samples, over 70$\times$ fewer than prior methods. To our knowledge, this is
the first closed-form, activation-guided structural compression of DMs that is
entirely training-free, providing both theoretical clarity and practical
efficiency.

</details>


### [122] [Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training](https://arxiv.org/abs/2509.21500)
*Junkai Zhang,Zihao Wang,Lin Gui,Swarnashree Mysore Sathyendra,Jaehwan Jeong,Victor Veitch,Wei Wang,Yunzhong He,Bing Liu,Lifeng Jin*

Main category: cs.LG

TL;DR: 强化微调（RFT）常因奖励过优化而导致低质量输出。本研究提出基于评分标准的奖励机制，通过利用离策略示例并关注高奖励尾部奖励规范性，有效减轻过优化，提升大型语言模型（LLM）性能。


<details>
  <summary>Details</summary>
Motivation: RFT面临奖励过优化问题，导致模型追求高分但生成质量低劣。理论分析表明，关键在于高奖励尾部（如无法区分“优秀”与“良好”响应）的奖励定义不准确。虽然离策略示例易于获取，但直接使用会引入奖励错位。

Method: 提出基于评分标准（rubric-based）的奖励机制。通过精心设计评分标准，可以有效利用离策略示例，同时避免其缺陷。为确保评分标准能捕获高奖励尾部，研究引入了一个工作流程，强调区分优秀且多样化响应的重要性。

Result: 经验证据表明，基于评分标准的奖励机制能显著缓解奖励过优化，并为LLM带来有效的后训练改进。

Conclusion: 基于评分标准的奖励机制是解决RFT中奖励过优化问题的有效方案，它通过改进高奖励区域的奖励规范性并利用离策略数据，实现了LLM的性能提升。

Abstract: Reinforcement fine-tuning (RFT) often suffers from \emph{reward
over-optimization}, where a policy model hacks the reward signals to achieve
high scores while producing low-quality outputs. Our theoretical analysis shows
that the key lies in reward misspecification at the high-reward tail: the
inability to reliably distinguish Excellent responses from merely Great ones.
This motivate us to focus on the high-reward region. However, such tail
examples are scarce under the base LLM. While off-policy exemplars (e.g. from
stronger models or rewrites) are easier to obtain, naively training on them
yields a misspecified reward for the policy we aim to align. To address this,
we study rubric-based rewards. By design, rubrics can leverage off-policy
examples while remaining insensitive to their artifacts. To elicit rubrics that
capture the high-reward tail, we highlight the importance of distinguishing
among great and diverse responses, and introduce a workflow to implement this
idea. We empirically demonstrate that rubric-based rewards substantially
mitigate reward over-optimization and deliver effective LLM post-training
improvements. Our code can be accessed at
https://github.com/Jun-Kai-Zhang/rubrics.git .

</details>


### [123] [Contrastive Mutual Information Learning: Toward Robust Representations without Positive-Pair Augmentations](https://arxiv.org/abs/2509.21511)
*Micha Livne*

Main category: cs.LG

TL;DR: 本文提出了对比互信息机（cMIM），一个结合了对比学习和互信息最大化的表示学习框架，旨在同时提升模型的生成能力和判别性能，解决了现有MIM在判别任务上的不足。


<details>
  <summary>Details</summary>
Motivation: 表示学习领域的核心挑战是学习能够良好迁移到各种下游任务的表示。现有范式在这一挑战中各有权衡，而互信息机（MIM）虽能最大化输入与潜在表示的互信息并促进代码聚类，但在判别任务上表现不佳。

Method: 引入对比互信息机（cMIM），它通过对比目标扩展了MIM，在保留MIM生成保真度的同时施加全局判别结构，以弥补MIM在判别任务上的不足。cMIM无需正向数据增强，且对批量大小的敏感性远低于InfoNCE。此外，还提出了“信息嵌入”技术，用于从编码器-解码器模型中提取丰富特征，无需额外训练即可提升判别性能。

Result: 在视觉和分子基准测试中，cMIM在分类和回归任务上均优于MIM和InfoNCE，同时保持了有竞争力的重建质量。这些结果表明cMIM作为统一框架在判别和生成应用中均表现出色。

Conclusion: cMIM是一个统一的表示学习框架，有效提升了模型在判别和生成应用中的性能，为实现同时服务于这两类目标的模型迈出了重要一步。

Abstract: Learning representations that transfer well to diverse downstream tasks
remains a central challenge in representation learning. Existing paradigms --
contrastive learning, self-supervised masking, and denoising auto-encoders --
balance this challenge with different trade-offs. We introduce the {contrastive
Mutual Information Machine} (cMIM), a probabilistic framework that extends the
Mutual Information Machine (MIM) with a contrastive objective. While MIM
maximizes mutual information between inputs and latents and promotes clustering
of codes, it falls short on discriminative tasks. cMIM addresses this gap by
imposing global discriminative structure while retaining MIM's generative
fidelity. Our contributions are threefold. First, we propose cMIM, a
contrastive extension of MIM that removes the need for positive data
augmentation and is substantially less sensitive to batch size than InfoNCE.
Second, we introduce {informative embeddings}, a general technique for
extracting enriched features from encoder-decoder models that boosts
discriminative performance without additional training and applies broadly
beyond MIM. Third, we provide empirical evidence across vision and molecular
benchmarks showing that cMIM outperforms MIM and InfoNCE on classification and
regression tasks while preserving competitive reconstruction quality. These
results position cMIM as a unified framework for representation learning,
advancing the goal of models that serve both discriminative and generative
applications effectively.

</details>


### [124] [DistillKac: Few-Step Image Generation via Damped Wave Equations](https://arxiv.org/abs/2509.21513)
*Weiqiao Han,Chenlin Meng,Christopher D. Manning,Stefano Ermon*

Main category: cs.LG

TL;DR: DistillKac是一种快速图像生成器，利用阻尼波动方程和Kac表示实现有限速度概率质量传输，相比扩散模型具有更好的数值稳定性和高效高质量的样本生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的反向时间速度可能变得僵硬，隐含无限传播速度，导致数值稳定性问题。本研究旨在开发一种能强制有限速度传输、全局动能有界的生成模型。

Method: DistillKac利用阻尼波动方程及其随机Kac表示来以有限速度传输概率质量。引入速度空间中的无分类器指导，并在温和条件下保持平方可积性。提出仅端点蒸馏训练方法，使学生模型在长区间内匹配冻结的教师模型，并证明了端点监督到整个路径接近性的稳定性结果。

Result: 实验证明DistillKac能以极少的函数评估生成高质量样本，同时保留了有限速度概率流带来的数值稳定性优势。

Conclusion: DistillKac提供了一种新颖、高效且数值稳定的图像生成方法，通过其独特的有限速度传输机制和端点蒸馏训练，克服了传统扩散模型的局限性。

Abstract: We present DistillKac, a fast image generator that uses the damped wave
equation and its stochastic Kac representation to move probability mass at
finite speed. In contrast to diffusion models whose reverse time velocities can
become stiff and implicitly allow unbounded propagation speed, Kac dynamics
enforce finite speed transport and yield globally bounded kinetic energy.
Building on this structure, we introduce classifier-free guidance in velocity
space that preserves square integrability under mild conditions. We then
propose endpoint only distillation that trains a student to match a frozen
teacher over long intervals. We prove a stability result that promotes
supervision at the endpoints to closeness along the entire path. Experiments
demonstrate DistillKac delivers high quality samples with very few function
evaluations while retaining the numerical stability benefits of finite speed
probability flows.

</details>


### [125] [Uncertainty-Aware Knowledge Tracing Models](https://arxiv.org/abs/2509.21514)
*Joshua Mitton,Prarthana Bhattacharyya,Ralph Abboud,Simon Woodhead*

Main category: cs.LG

TL;DR: 本文提出通过捕捉预测不确定性来增强知识追踪（KT）模型，发现高不确定性与模型错误预测一致，该信号对教学应用具有实用价值。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型主要关注提高预测准确性，但在学生选择干扰项时常做出错误预测，导致学生错误未被发现。

Method: 通过捕捉知识追踪模型的预测不确定性，以增加模型的新功能和洞察力。

Result: 研究表明，更大的预测不确定性与模型的不正确预测高度一致。知识追踪模型中的不确定性信号具有信息价值。

Conclusion: 该不确定性信号在教学上具有潜在的实用价值，特别适用于资源有限且需要理解学生能力的教育学习平台。

Abstract: The main focus of research on Knowledge Tracing (KT) models is on model
developments with the aim of improving predictive accuracy. Most of these
models make the most incorrect predictions when students choose a distractor,
leading to student errors going undetected. We present an approach to add new
capabilities to KT models by capturing predictive uncertainty and demonstrate
that a larger predictive uncertainty aligns with model incorrect predictions.
We show that uncertainty in KT models is informative and that this signal would
be pedagogically useful for application in an educational learning platform
that can be used in a limited resource setting where understanding student
ability is necessary.

</details>


### [126] [$\mathbf{Li_2}$: A Framework on Dynamics of Feature Emergence and Delayed Generalization](https://arxiv.org/abs/2509.21519)
*Yuandong Tian*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While the phenomenon of grokking, i.e., delayed generalization, has been
studied extensively, it remains an open question whether there is a
mathematical framework to characterize what kind of features emerge, how and in
which conditions it happens from training, for complex structured inputs. We
propose a novel framework, named $\mathbf{Li_2}$, that captures three key
stages for the grokking behavior of 2-layer nonlinear networks: (I) Lazy
learning, (II) independent feature learning and (III) interactive feature
learning, characterized by the structure of backpropagated gradient $G_F$
across layers. In (I), $G_F$ is random, and top layer overfits to random hidden
representation. In (II), the gradient of each node (column of $G_F$) only
depends on its own activation, and thus each hidden node learns their
representation independently from $G_F$, which now carries information about
target labels, thanks to weight decay. Interestingly, the independent dynamics
follows exactly the gradient ascent of an energy function $E$, and its local
maxima are precisely the emerging features. We study whether these local-optima
induced features are generalizable, their representation power, and how they
change on sample size, in group arithmetic tasks. Finally, in (III), we
provably show how hidden nodes interact, and how $G_F$ changes to focus on
missing features that need to be learned. Our study sheds lights on roles
played by key hyperparameters such as weight decay, learning rate and sample
sizes in grokking, leads to provable scaling laws of memorization and
generalization, and reveals the underlying cause why recent optimizers such as
Muon can be effective, from the first principles of gradient dynamics. Our
analysis can be extended to multi-layer architectures.

</details>


### [127] [TRiCo: Triadic Game-Theoretic Co-Training for Robust Semi-Supervised Learning](https://arxiv.org/abs/2509.21526)
*Hongyang He,Xinyuan Song,Yangfan He,Zeyu Zhang,Yanshu Li,Haochen You,Lifan Sun,Wenqiao Zhang*

Main category: cs.LG

TL;DR: TRiCo是一个新颖的三元博弈论协同训练框架，用于半监督学习（SSL），它将教师、两个学生和一个对抗性生成器整合到一个统一的训练范式中，并在低标签数据量下实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有SSL框架存在静态视图交互、伪标签不可靠以及缺乏难样本建模等关键局限性。TRiCo旨在通过结构化的三方交互来解决这些问题。

Method: TRiCo将SSL建模为Stackelberg博弈中的三方交互：
1. 两个学生分类器，在冻结、互补的表示上训练。
2. 一个元学习的教师，通过基于验证的反馈自适应地调节伪标签选择（基于互信息而非置信度）和损失平衡。
3. 一个非参数生成器，通过扰动嵌入来发现决策边界弱点。

Result: 在CIFAR-10、SVHN、STL-10和ImageNet数据集上的广泛实验表明，TRiCo在低标签数据量下持续实现了最先进的性能。

Conclusion: TRiCo提供了一个原则性且可泛化的半监督学习解决方案，解决了现有框架的关键局限性。它具有架构无关性，并与冻结的视觉骨干网络兼容。

Abstract: We introduce TRiCo, a novel triadic game-theoretic co-training framework that
rethinks the structure of semi-supervised learning by incorporating a teacher,
two students, and an adversarial generator into a unified training paradigm.
Unlike existing co-training or teacher-student approaches, TRiCo formulates SSL
as a structured interaction among three roles: (i) two student classifiers
trained on frozen, complementary representations, (ii) a meta-learned teacher
that adaptively regulates pseudo-label selection and loss balancing via
validation-based feedback, and (iii) a non-parametric generator that perturbs
embeddings to uncover decision boundary weaknesses. Pseudo-labels are selected
based on mutual information rather than confidence, providing a more robust
measure of epistemic uncertainty. This triadic interaction is formalized as a
Stackelberg game, where the teacher leads strategy optimization and students
follow under adversarial perturbations. By addressing key limitations in
existing SSL frameworks, such as static view interactions, unreliable
pseudo-labels, and lack of hard sample modeling, TRiCo provides a principled
and generalizable solution. Extensive experiments on CIFAR-10, SVHN, STL-10,
and ImageNet demonstrate that TRiCo consistently achieves state-of-the-art
performance in low-label regimes, while remaining architecture-agnostic and
compatible with frozen vision backbones.

</details>


### [128] [Preemptive Detection and Steering of LLM Misalignment via Latent Reachability](https://arxiv.org/abs/2509.21528)
*Sathwik Karnik,Somil Bansal*

Main category: cs.LG

TL;DR: BRT-Align是一种基于可达性分析的框架，将控制理论安全工具引入LLM推理，用于实时监测和引导生成过程，以减少有害内容，提高推理阶段的LLM安全性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）普遍存在生成有害内容的风险。当前主流的安全对齐方法（如RLHF）虽在训练期间有效塑造模型行为，但在推理阶段无法提供保障，仍可能产生不安全内容。因此，亟需一种在推理时提供安全保障的机制。

Method: 本文提出了BRT-Align框架，将自回归生成建模为潜在空间中的动态系统，并通过逆向可达性（backward reachability）学习一个安全价值函数，以估计轨迹的最坏情况演变。该框架包含两个互补机制：1) 一个运行时监视器，能提前数个token预测不安全完成；2) 一个限制最小的引导滤波器，能微扰潜在状态以使生成远离不安全区域。

Result: 实验证明，BRT-Align比基线方法能更准确、更早地检测到不安全内容。在LLM安全对齐方面，它显著减少了不安全生成，同时保持了语句的多样性和连贯性。定性结果进一步表明，BRT-Align生成的响应在暴力、亵渎、冒犯性和政治偏见方面均有所降低。

Conclusion: 可达性分析为推理阶段的LLM安全性提供了一个有原则且实用的基础。

Abstract: Large language models (LLMs) are now ubiquitous in everyday tools, raising
urgent safety concerns about their tendency to generate harmful content. The
dominant safety approach -- reinforcement learning from human feedback (RLHF)
-- effectively shapes model behavior during training but offers no safeguards
at inference time, where unsafe continuations may still arise. We propose
BRT-Align, a reachability-based framework that brings control-theoretic safety
tools to LLM inference. BRT-Align models autoregressive generation as a
dynamical system in latent space and learn a safety value function via backward
reachability, estimating the worst-case evolution of a trajectory. This enables
two complementary mechanisms: (1) a runtime monitor that forecasts unsafe
completions several tokens in advance, and (2) a least-restrictive steering
filter that minimally perturbs latent states to redirect generation away from
unsafe regions. Experiments across multiple LLMs and toxicity benchmarks
demonstrate that BRT-Align provides more accurate and earlier detection of
unsafe continuations than baselines. Moreover, for LLM safety alignment,
BRT-Align substantially reduces unsafe generations while preserving sentence
diversity and coherence. Qualitative results further highlight emergent
alignment properties: BRT-Align consistently produces responses that are less
violent, less profane, less offensive, and less politically biased. Together,
these findings demonstrate that reachability analysis provides a principled and
practical foundation for inference-time LLM safety.

</details>


### [129] [Expert-guided Clinical Text Augmentation via Query-Based Model Collaboration](https://arxiv.org/abs/2509.21530)
*Dongkyu Cho,Miao Zhang,Rumi Chunara*

Main category: cs.LG

TL;DR: 提出一种查询式模型协作框架，通过融入领域专家知识，实现在医疗等高风险领域安全、有效地使用LLM进行数据增强。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）虽具备强大的数据生成能力可用于数据增强，但在医疗等高风险领域，存在生成临床不准确或误导性信息的风险，限制了其应用。

Method: 本文提出一种新颖的查询式模型协作框架，该框架通过整合专家级领域知识来指导数据增强过程，旨在保留关键医疗信息。

Result: 在临床预测任务上的实验表明，该轻量级协作方法持续优于现有LLM增强方法，并通过显著减少事实错误提高了数据增强的安全性。

Conclusion: 该框架有效弥补了LLM数据增强潜力与专业领域安全要求之间的差距，使其能够安全地应用于高风险场景。

Abstract: Data augmentation is a widely used strategy to improve model robustness and
generalization by enriching training datasets with synthetic examples. While
large language models (LLMs) have demonstrated strong generative capabilities
for this purpose, their applications in high-stakes domains like healthcare
present unique challenges due to the risk of generating clinically incorrect or
misleading information. In this work, we propose a novel query-based model
collaboration framework that integrates expert-level domain knowledge to guide
the augmentation process to preserve critical medical information. Experiments
on clinical prediction tasks demonstrate that our lightweight
collaboration-based approach consistently outperforms existing LLM augmentation
methods while improving safety through reduced factual errors. This framework
addresses the gap between LLM augmentation potential and the safety
requirements of specialized domains.

</details>


### [130] [A circuit for predicting hierarchical structure in-context in Large Language Models](https://arxiv.org/abs/2509.21534)
*Tankred Saanum,Can Demircan,Samuel J. Gershman,Eric Schulz*

Main category: cs.LG

TL;DR: 本研究通过设计分层重复模式的合成任务和自然语言类比，发现大型语言模型中的归纳头具有适应性，能学习上下文信息以预测复杂模式，并揭示了其通过识别潜在上下文来学习的机制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的上下文学习能力被认为与归纳头（induction heads）有关，但目前尚不清楚归纳头是否能支持更复杂的、具有分层结构的重复模式的上下文学习，因为简单的归纳机制无法有效整合上下文信息来预测这类模式。

Method: 设计了一个合成的上下文学习任务，其中令牌以分层依赖关系重复。在此任务及相应的自然语言类比上评估了一系列大型语言模型。进一步探究了归纳头如何在上下文中学习，寻找能够揭示潜在上下文并确定不同令牌转换关系的注意力头。

Result: 研究发现大型语言模型具有自适应的归纳头，它们能够学习在上下文中关注哪些信息以支持预测。归纳头自身的学习机制得到其他注意力头的支持，这些注意力头能够揭示一组潜在上下文，从而确定不同的令牌转换关系。

Conclusion: 研究不仅证明了大型语言模型拥有能够学习的归纳头，而且提供了一个完整的机制解释，阐述了大型语言模型如何学习预测上下文中的高阶重复模式。

Abstract: Large Language Models (LLMs) excel at in-context learning, the ability to use
information provided as context to improve prediction of future tokens.
Induction heads have been argued to play a crucial role for in-context learning
in Transformer Language Models. These attention heads make a token attend to
successors of past occurrences of the same token in the input. This basic
mechanism supports LLMs' ability to copy and predict repeating patterns.
However, it is unclear if this same mechanism can support in-context learning
of more complex repetitive patterns with hierarchical structure. Natural
language is teeming with such cases: The article "the" in English usually
prefaces multiple nouns in a text. When predicting which token succeeds a
particular instance of "the", we need to integrate further contextual cues from
the text to predict the correct noun. If induction heads naively attend to all
past instances of successor tokens of "the" in a context-independent manner,
they cannot support this level of contextual information integration. In this
study, we design a synthetic in-context learning task, where tokens are
repeated with hierarchical dependencies. Here, attending uniformly to all
successor tokens is not sufficient to accurately predict future tokens.
Evaluating a range of LLMs on these token sequences and natural language
analogues, we find adaptive induction heads that support prediction by learning
what to attend to in-context. Next, we investigate how induction heads
themselves learn in-context. We find evidence that learning is supported by
attention heads that uncover a set of latent contexts, determining the
different token transition relationships. Overall, we not only show that LLMs
have induction heads that learn, but offer a complete mechanistic account of
how LLMs learn to predict higher-order repetitive patterns in-context.

</details>


### [131] [Evidence for Limited Metacognition in LLMs](https://arxiv.org/abs/2509.21545)
*Christopher Ackerman*

Main category: cs.LG

TL;DR: 本研究引入新方法量化评估大语言模型（LLMs）的元认知能力，发现2024年后推出的前沿LLMs表现出增强的元认知证据，具体体现在评估自信心和预测答案方面，尽管这些能力分辨率有限、情境依赖且与人类存在质的差异。后训练可能在元认知发展中发挥作用。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的自我意识乃至感知能力日益受到公众关注，并带来重大的安全和政策影响，但目前衡量这些能力的科学方法尚不成熟。

Method: 本研究提出了一种新颖的定量评估LLMs元认知能力的方法。该方法借鉴非人类动物元认知研究，避免模型自我报告，转而测试模型策略性运用内部状态知识的能力。通过两种实验范式进行验证，并辅以对模型返回的token概率进行分析，以揭示潜在的内部信号。

Result: 自2024年初以来，前沿LLMs在评估和利用自身对回答事实和推理问题正确性的信心、以及预测自身答案并适当利用信息方面，展现出日益增强的元认知能力证据。token概率分析进一步表明存在支持元认知的上游内部信号。然而，这些能力1)分辨率有限，2)以情境依赖方式出现，3)与人类的能力存在质的区别。此外，发现能力相似的模型之间也存在有趣差异，暗示LLM的后训练可能在元认知能力发展中扮演角色。

Conclusion: 最新LLMs展现出日益增强但受限的元认知能力，其特点与人类不同，且可能受到后训练过程的影响，为未来LLM自我意识研究提供了量化评估框架。

Abstract: The possibility of LLM self-awareness and even sentience is gaining
increasing public attention and has major safety and policy implications, but
the science of measuring them is still in a nascent state. Here we introduce a
novel methodology for quantitatively evaluating metacognitive abilities in
LLMs. Taking inspiration from research on metacognition in nonhuman animals,
our approach eschews model self-reports and instead tests to what degree models
can strategically deploy knowledge of internal states. Using two experimental
paradigms, we demonstrate that frontier LLMs introduced since early 2024 show
increasingly strong evidence of certain metacognitive abilities, specifically
the ability to assess and utilize their own confidence in their ability to
answer factual and reasoning questions correctly and the ability to anticipate
what answers they would give and utilize that information appropriately. We
buttress these behavioral findings with an analysis of the token probabilities
returned by the models, which suggests the presence of an upstream internal
signal that could provide the basis for metacognition. We further find that
these abilities 1) are limited in resolution, 2) emerge in context-dependent
manners, and 3) seem to be qualitatively different from those of humans. We
also report intriguing differences across models of similar capabilities,
suggesting that LLM post-training may have a role in developing metacognitive
abilities.

</details>


### [132] [Machine Learning. The Science of Selection under Uncertainty](https://arxiv.org/abs/2509.21547)
*Yevgeny Seldin*

Main category: cs.LG

TL;DR: 本书提供了统计工具，以确保在机器学习中基于噪声经验估计进行模型选择时的理论性能保证，涵盖了从测度集中不等式到离线和在线学习的泛化及悔恨界限。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的模型选择依赖于经验估计，但数据采样的随机性引入噪声，导致在不确定性下做出选择。因此，需要统计工具来为不确定性下的选择结果提供理论保证。

Method: 1. **测度集中不等式**: 介绍并应用多种测度集中不等式（如Markov、Chebyshev、Hoeffding、Bernstein等），以控制经验估计与真实期望的偏差。
2. **离线监督学习**: 提供推导泛化界限的工具，包括奥卡姆剃刀、Vapnik-Chervonenkis分析和PAC-贝叶斯分析，并将其应用于加权多数投票。
3. **在线学习**: 探讨在线学习问题及其性能衡量标准（悔恨），并提供在随机/对抗性环境以及完全信息/bandit反馈下推导悔恨界限的工具。

Result: 本书系统地提供了在不确定性下进行机器学习模型选择的理论保证所需的统计工具集。这些工具能够控制经验估计的偏差，推导离线学习的泛化界限，以及在线学习的悔恨界限。

Conclusion: 该书为机器学习中基于噪声经验估计进行选择的问题提供了全面的统计学理论框架和工具，从而为学习算法（包括离线和在线设置）的性能提供了坚实的理论保证。

Abstract: Learning, whether natural or artificial, is a process of selection. It starts
with a set of candidate options and selects the more successful ones. In the
case of machine learning the selection is done based on empirical estimates of
prediction accuracy of candidate prediction rules on some data. Due to
randomness of data sampling the empirical estimates are inherently noisy,
leading to selection under uncertainty. The book provides statistical tools to
obtain theoretical guarantees on the outcome of selection under uncertainty. We
start with concentration of measure inequalities, which are the main
statistical instrument for controlling how much an empirical estimate of
expectation of a function deviates from the true expectation. The book covers a
broad range of inequalities, including Markov's, Chebyshev's, Hoeffding's,
Bernstein's, Empirical Bernstein's, Unexpected Bernstein's, kl, and split-kl.
We then study the classical (offline) supervised learning and provide a range
of tools for deriving generalization bounds, including Occam's razor,
Vapnik-Chervonenkis analysis, and PAC-Bayesian analysis. The latter is further
applied to derive generalization guarantees for weighted majority votes. After
covering the offline setting, we turn our attention to online learning. We
present the space of online learning problems characterized by environmental
feedback, environmental resistance, and structural complexity. A common
performance measure in online learning is regret, which compares performance of
an algorithm to performance of the best prediction rule in hindsight, out of a
restricted set of prediction rules. We present tools for deriving regret bounds
in stochastic and adversarial environments, and under full information and
bandit feedback.

</details>


### [133] [Interpretable time series analysis with Gumbel dynamics](https://arxiv.org/abs/2509.21578)
*Yiliu Wang,Timothy Doyeon Kim,Eric Shea-Brown,Uygar Sümbül*

Main category: cs.LG

TL;DR: Gumbel动态模型（GDM）通过引入离散状态的连续松弛和基于Gumbel分布的噪声模型，解决了传统切换动态系统在处理平滑过渡、随机混合状态和虚假快速切换方面的不足，实现了对复杂时间序列数据更忠实、可解释且可微分的建模。


<details>
  <summary>Details</summary>
Motivation: 传统切换动态系统因其状态的离散性，难以捕捉平滑、变速率的转换以及重叠状态的随机混合，并且在实际数据中常表现出虚假的快速切换问题。

Method: 本文提出了Gumbel动态模型（GDM）。该模型首先引入离散状态的连续松弛化，并通过Gumbel分布在松弛离散状态空间上定义新的噪声模型，从而扩展了可用的状态动力学，使其能更准确地近似平滑和非平稳的真实动力学。其次，这种松弛化使得模型完全可微分，支持使用标准梯度下降方法进行快速且可伸缩的训练。

Result: GDM在标准模拟数据集上得到了验证，展示了其在随机环境下建模“软”的、粘滞状态和转换的能力。此外，在两个真实世界数据集上的应用表明，该模型能够在传统方法常失败的随机、多动力学时间序列中推断出可解释的状态。

Conclusion: GDM通过引入状态的连续松弛和创新的噪声模型，成功克服了传统切换动态系统在处理平滑转换和随机状态混合方面的局限性。它能够更忠实地建模复杂的随机时间序列，提供可解释的状态，并且训练过程高效可伸缩。

Abstract: Switching dynamical systems can model complicated time series data while
maintaining interpretability by inferring a finite set of dynamics primitives
and explaining different portions of the observed time series with one of these
primitives. However, due to the discrete nature of this set, such models
struggle to capture smooth, variable-speed transitions, as well as stochastic
mixtures of overlapping states, and the inferred dynamics often display
spurious rapid switching on real-world datasets. Here, we propose the Gumbel
Dynamical Model (GDM). First, by introducing a continuous relaxation of
discrete states and a different noise model defined on the relaxed-discrete
state space via the Gumbel distribution, GDM expands the set of available state
dynamics, allowing the model to approximate smoother and non-stationary
ground-truth dynamics more faithfully. Second, the relaxation makes the model
fully differentiable, enabling fast and scalable training with standard
gradient descent methods. We validate our approach on standard simulation
datasets and highlight its ability to model soft, sticky states and transitions
in a stochastic setting. Furthermore, we apply our model to two real-world
datasets, demonstrating its ability to infer interpretable states in stochastic
time series with multiple dynamics, a setting where traditional methods often
fail.

</details>


### [134] [Leveraging Big Data Frameworks for Spam Detection in Amazon Reviews](https://arxiv.org/abs/2509.21579)
*Mst Eshita Khatun,Halima Akter,Tasnimul Rehan,Toufiq Ahmed*

Main category: cs.LG

TL;DR: 本研究利用大数据分析和机器学习，成功地从亚马逊产品评论中检测并分类垃圾评论，其中逻辑回归模型准确率达90.35%，旨在提升在线购物环境的信任度。


<details>
  <summary>Details</summary>
Motivation: 在线购物中，产品评论对消费者购买行为和信任建立至关重要。然而，虚假评论的泛滥破坏了这种信任，误导消费者并损害商家声誉。

Method: 本研究采用先进的大数据分析和机器学习方法，处理大量亚马逊产品评论数据集。利用可扩展的大数据框架高效处理和分析评论数据，提取欺诈行为的关键特征，并使用多种机器学习分类器进行垃圾评论检测，其中包括逻辑回归。

Result: 研究成功地检测和分类了垃圾评论，其中逻辑回归（Logistic Regression）分类器取得了90.35%的准确率。

Conclusion: 本研究通过准确检测和分类垃圾评论，提升了评论的真实性，从而有助于创建一个更值得信赖和透明的在线购物环境。

Abstract: In this digital era, online shopping is common practice in our daily lives.
Product reviews significantly influence consumer buying behavior and help
establish buyer trust. However, the prevalence of fraudulent reviews undermines
this trust by potentially misleading consumers and damaging the reputations of
the sellers. This research addresses this pressing issue by employing advanced
big data analytics and machine learning approaches on a substantial dataset of
Amazon product reviews. The primary objective is to detect and classify spam
reviews accurately so that it enhances the authenticity of the review. Using a
scalable big data framework, we efficiently process and analyze a large scale
of review data, extracting key features indicative of fraudulent behavior. Our
study illustrates the utility of various machine learning classifiers in
detecting spam reviews, with Logistic Regression achieving an accuracy of
90.35%, thus contributing to a more trustworthy and transparent online shopping
environment.

</details>


### [135] [GenUQ: Predictive Uncertainty Estimates via Generative Hyper-Networks](https://arxiv.org/abs/2509.21605)
*Tian Yu Yen,Reese E. Jones,Ravi G. Patel*

Main category: cs.LG

TL;DR: 本文提出GenUQ，一种基于测度论的不确定性量化（UQ）方法，用于解决算子学习中随机算子难以构建似然函数的问题。GenUQ通过生成式超网络直接生成参数分布，并在多个问题中表现优于其他UQ方法。


<details>
  <summary>Details</summary>
Motivation: 算子学习能大幅减少偏微分方程的数值计算成本。然而，现有算子模型中的不确定性量化（UQ）方法依赖于似然函数，这对于难以或无法构建似然函数的随机算子而言是巨大的挑战。

Method: 引入GenUQ，一种基于测度论的UQ方法，它通过引入一个生成式超网络模型来规避似然函数的构建。该模型能够生成与观测数据一致的参数分布。

Result: GenUQ在三个示例问题中表现优于其他UQ方法：成功恢复一个人工构造的算子、学习随机椭圆偏微分方程的解算子，以及模拟多孔钢在张力下的失效位置。

Conclusion: GenUQ提供了一种无需似然函数的不确定性量化新范式，有效解决了随机算子学习中的UQ挑战，并通过其在多个实际问题中的优越性能验证了其有效性。

Abstract: Operator learning is a recently developed generalization of regression to
mappings between functions. It promises to drastically reduce expensive
numerical integration of PDEs to fast evaluations of mappings between
functional states of a system, i.e., surrogate and reduced-order modeling.
Operator learning has already found applications in several areas such as
modeling sea ice, combustion, and atmospheric physics. Recent approaches
towards integrating uncertainty quantification into the operator models have
relied on likelihood based methods to infer parameter distributions from noisy
data. However, stochastic operators may yield actions from which a likelihood
is difficult or impossible to construct. In this paper, we introduce, GenUQ, a
measure-theoretic approach to UQ that avoids constructing a likelihood by
introducing a generative hyper-network model that produces parameter
distributions consistent with observed data. We demonstrate that GenUQ
outperforms other UQ methods in three example problems, recovering a
manufactured operator, learning the solution operator to a stochastic elliptic
PDE, and modeling the failure location of porous steel under tension.

</details>


### [136] [Task-Agnostic Federated Continual Learning via Replay-Free Gradient Projection](https://arxiv.org/abs/2509.21606)
*Seohyeon Cha,Huancheng Chen,Haris Vikalo*

Main category: cs.LG

TL;DR: FedProTIP是一个联邦持续学习（FCL）框架，通过梯度投影和任务身份预测来有效缓解灾难性遗忘和任务不可知推理问题，其性能优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 联邦持续学习（FCL）面临核心挑战：灾难性遗忘。在去中心化设置中，数据异构性、通信受限和隐私问题会加剧这一问题。此外，如何进行任务不可知推理也是一个难题。

Method: 提出FedProTIP框架。它通过将客户端更新投影到全局模型先前学习表示所张子空间的精确正交补空间上，从而减轻对早期任务的干扰。此外，该方法还集成了一个轻量级机制，利用先前任务的核心基底来预测任务身份，并动态调整全局模型的输出，以解决任务不可知推理问题。

Result: 在标准FCL基准测试中，FedProTIP在平均准确性方面显著优于最先进方法，尤其是在任务身份事先未知的情况下表现突出。

Conclusion: FedProTIP能够有效解决联邦持续学习中的灾难性遗忘和任务不可知推理问题，并在实际应用场景中展现出卓越的性能。

Abstract: Federated continual learning (FCL) enables distributed client devices to
learn from streaming data across diverse and evolving tasks. A major challenge
to continual learning, catastrophic forgetting, is exacerbated in decentralized
settings by the data heterogeneity, constrained communication and privacy
concerns. We propose Federated gradient Projection-based Continual Learning
with Task Identity Prediction (FedProTIP), a novel FCL framework that mitigates
forgetting by projecting client updates onto the orthogonal complement of the
subspace spanned by previously learned representations of the global model.
This projection reduces interference with earlier tasks and preserves
performance across the task sequence. To further address the challenge of
task-agnostic inference, we incorporate a lightweight mechanism that leverages
core bases from prior tasks to predict task identity and dynamically adjust the
global model's outputs. Extensive experiments across standard FCL benchmarks
demonstrate that FedProTIP significantly outperforms state-of-the-art methods
in average accuracy, particularly in settings where task identities are a
priori unknown.

</details>


### [137] [Causal Abstraction Inference under Lossy Representations](https://arxiv.org/abs/2509.21607)
*Kevin Xia,Elias Bareinboim*

Main category: cs.LG

TL;DR: 提出一种新的“投影抽象”因果抽象框架，以解决现有方法在处理有损表示时的局限性，并能从有限低级数据中识别和估计高级因果查询。


<details>
  <summary>Details</summary>
Motivation: 现有因果抽象定义在处理有损抽象函数（即多个低级干预映射到同一高级干预但效果不同）时存在局限性，无法满足抽象不变性条件。

Method: 引入“投影抽象”概念，泛化现有定义以适应有损表示；展示了如何构建投影抽象，并将其用于翻译观测、干预和反事实因果查询；提出了从有限低级数据识别和估计高级因果查询的新图形判据。

Result: 通过实验证明了投影抽象模型在高维图像设置中的有效性。

Conclusion: 该研究通过引入投影抽象成功克服了现有因果抽象在处理有损表示方面的局限，并提供了从有限数据中识别高级因果查询的实用方法。

Abstract: The study of causal abstractions bridges two integral components of human
intelligence: the ability to determine cause and effect, and the ability to
interpret complex patterns into abstract concepts. Formally, causal abstraction
frameworks define connections between complicated low-level causal models and
simple high-level ones. One major limitation of most existing definitions is
that they are not well-defined when considering lossy abstraction functions in
which multiple low-level interventions can have different effects while mapping
to the same high-level intervention (an assumption called the abstract
invariance condition). In this paper, we introduce a new type of abstractions
called projected abstractions that generalize existing definitions to
accommodate lossy representations. We show how to construct a projected
abstraction from the low-level model and how it translates equivalent
observational, interventional, and counterfactual causal queries from low to
high-level. Given that the true model is rarely available in practice we prove
a new graphical criteria for identifying and estimating high-level causal
queries from limited low-level data. Finally, we experimentally show the
effectiveness of projected abstraction models in high-dimensional image
settings.

</details>


### [138] [LANCE: Low Rank Activation Compression for Efficient On-Device Continual Learning](https://arxiv.org/abs/2509.21617)
*Marco Paul E. Apolinario,Kaushik Roy*

Main category: cs.LG

TL;DR: LANCE提出一种低秩激活压缩框架，通过一次性高阶SVD实现高效的设备端微调和持续学习，显著降低内存和计算成本。


<details>
  <summary>Details</summary>
Motivation: 设备端学习对个性化、隐私和长期适应至关重要，但受限于反向传播中激活存储的高内存成本。现有激活压缩方法引入计算开销且未用于持续学习。

Method: 提出LANCE（Low-rank Activation Compression）框架，通过一次性高阶奇异值分解（SVD）获得可重用的低秩子空间进行激活投影。在持续学习中，通过将任务分配到正交子空间来避免存储大型任务特定矩阵。

Result: 在CIFAR-10/100等数据集上，激活存储减少高达250倍，同时保持与完整反向传播相当的精度。在持续学习基准测试中，以极小的内存成本实现与正交梯度投影方法相当的性能。

Conclusion: LANCE为边缘设备上的高效微调和持续学习提供了一个实用且可扩展的解决方案。

Abstract: On-device learning is essential for personalization, privacy, and long-term
adaptation in resource-constrained environments. Achieving this requires
efficient learning, both fine-tuning existing models and continually acquiring
new tasks without catastrophic forgetting. Yet both settings are constrained by
high memory cost of storing activations during backpropagation. Existing
activation compression methods reduce this cost but relying on repeated
low-rank decompositions, introducing computational overhead. Also, such methods
have not been explored for continual learning. We propose LANCE (Low-rank
Activation Compression), a framework that performs one-shot higher-order
Singular Value Decompsoition (SVD) to obtain a reusable low-rank subspace for
activation projection. This eliminates repeated decompositions, reducing both
memory and computation. Moreover, fixed low-rank subspaces further enable
on-device continual learning by allocating tasks to orthogonal subspaces
without storing large task-specific matrices. Experiments show that LANCE
reduces activation storage up to 250$\times$ while maintaining accuracy
comparable to full backpropagation on CIFAR-10/100, Oxford-IIIT Pets,
Flowers102, and CUB-200 datasets. On continual learning benchmarks (Split
CIFAR-100, Split MiniImageNet, 5-Datasets), it achieves performance competitive
with orthogonal gradient projection methods at a fraction of the memory cost.
These results position LANCE as a practical and scalable solution for efficient
fine-tuning and continual learning on edge devices.

</details>


### [139] [PreLoRA: Hybrid Pre-training of Vision Transformers with Full Training and Low-Rank Adapters](https://arxiv.org/abs/2509.21619)
*Krishu K Thapa,Reet Barik,Krishna Teja Chitty-Venkata,Murali Emani,Venkatram Vishwanath*

Main category: cs.LG

TL;DR: 提出一种在大型模型训练中动态切换到低秩适配（LoRA）的方法，以在保持精度的前提下显著节省资源和提高效率。


<details>
  <summary>Details</summary>
Motivation: 训练数百万到数十亿参数的大型模型资源消耗巨大，需要大量时间、计算和内存。

Method: 识别训练早期参数变化较大的“部分收敛”状态，并在此状态下动态地从全参数训练切换到ViT-Large模型上的低秩适配（LoRA）。该方法通过用户定义的超参数确定切换点，并根据各模块层的收敛程度为其分配特定的秩。

Result: 在保持模型精度的同时，可训练参数量减少到原始的10%，吞吐量提高3倍，每epoch平均训练时间减少1.5倍，GPU内存消耗减少20%。

Conclusion: 通过在训练早期动态切换到低秩适配，可以显著提高大型模型训练的效率和资源利用率，同时不牺牲模型性能。

Abstract: Training large models ranging from millions to billions of parameters is
highly resource-intensive, requiring significant time, compute, and memory. It
is observed that most of the learning (higher change in weights) takes place in
the earlier stage of the training loop. These changes stabilize as training
continues, enabling them to be captured by matrices of a low intrinsic rank.
Therefore, we propose an approach to identify such states of partial
convergence and dynamically switch from full parameter training to Low-Rank
Adaptation (LoRA) on the ViT-Large model. We introduce a flexible approach that
leverages user-defined hyperparameters to determine the switching point and
assign a rank specific to each module layer based on its level of convergence.
Experimental results show that this approach preserves model accuracy while
reducing the number of trainable parameters to 10% of its original size,
resulting in a 3x improvement in throughput, and a 1.5x reduction in average
training time per epoch while also reducing GPU memory consumption by 20%

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [140] [Context-Aware Hybrid Routing in Bluetooth Mesh Networks Using Multi-Model Machine Learning and AODV Fallback](https://arxiv.org/abs/2509.21490)
*Md Sajid Islam,Tanvir Hasan*

Main category: cs.NI

TL;DR: 本研究提出一种基于机器学习的混合智能路由框架，用于增强蓝牙网状网络中的AODV协议，以改善下一跳选择。该框架在仿真环境中表现出色，实现了约99.97%的数据包投递率，显著优于传统AODV，证明了轻量级机器学习模型在无基础设施环境中提升路由可靠性和适应性的潜力。


<details>
  <summary>Details</summary>
Motivation: 蓝牙网状网络在紧急和资源受限场景中为离线通信提供了前景广阔的基础设施。然而，传统的路由策略（如AODV）在拥塞和动态拓扑变化下性能下降，急需改进下一跳选择。

Method: 本研究提出一个混合智能路由框架，通过监督机器学习增强AODV协议。该框架整合了四个预测模型：投递成功分类器、TTL回归器、延迟回归器和转发器适用性分类器，形成统一的评分机制，在多跳消息传输期间动态排序邻居。研究开发了一个具有固定节点部署、缓冲区限制和设备异构性的仿真环境，并评估了三种策略：基线AODV、部分混合ML模型（ABC）和完整混合ML模型（ABCD）。

Result: 在十个仿真场景中，Hybrid ABCD模型在受控条件下实现了约99.97%的数据包投递率，显著优于基线AODV和中间方法（ABC）。

Conclusion: 研究结果表明，轻量级、可解释的机器学习模型可以增强蓝牙网状网络的路由可靠性和适应性，特别是在投递成功优先于延迟限制的无基础设施环境中。

Abstract: Bluetooth-based mesh networks offer a promising infrastructure for offline
communication in emergency and resource constrained scenarios. However,
traditional routing strategies such as Ad hoc On-Demand Distance Vector (AODV)
often degrade under congestion and dynamic topological changes. This study
proposes a hybrid intelligent routing framework that augments AODV with
supervised machine learning to improve next-hop selection under varied network
constraints. The framework integrates four predictive models: a delivery
success classifier, a TTL regressor, a delay regressor, and a forwarder
suitability classifier, into a unified scoring mechanism that dynamically ranks
neighbors during multi-hop message transmission. A simulation environment with
stationary node deployments was developed, incorporating buffer constraints and
device heterogeneity to evaluate three strategies: baseline AODV, a partial
hybrid ML model (ABC), and the full hybrid ML model (ABCD). Across ten
scenarios, the Hybrid ABCD model achieves approximately 99.97 percent packet
delivery under these controlled conditions, significantly outperforming both
the baseline and intermediate approaches. The results demonstrate that
lightweight, explainable machine learning models can enhance routing
reliability and adaptability in Bluetooth mesh networks, particularly in
infrastructure-less environments where delivery success is prioritized over
latency constraints.

</details>


### [141] [A Target-Agnostic Protocol-Independent Interface for the Transport Layer](https://arxiv.org/abs/2509.21550)
*Pedro Mizuno,Kimiya Mohammadtaheri,Linfan Qian,Joshua Johnson,Danny Akbarzadeh,Chris Neely,Mario Baldi,Nacihket Kapre,Mina Tahmasbi Arashloo*

Main category: cs.NI

TL;DR: 该论文提出了一种名为TINF的高级、与目标无关的传输层编程抽象，旨在简化传输协议的开发、自动化分析和形式化验证，以应对传输协议及其运行环境日益多样化的挑战。


<details>
  <summary>Details</summary>
Motivation: 传输协议对网络通信至关重要，但随着新应用、工作负载和网络架构的发展，以及在广泛执行环境（目标）中运行的需求，协议本身和其运行环境的多样性日益增加。这种多样性导致开发和管理复杂性，因此需要一种更高效、与目标无关的编程抽象。

Method: 作者提出将传输协议指定为高级程序，这些程序以事件和流状态作为输入，并使用受限的类C结构，生成更新的状态以及用于数据重组、数据包生成和调度、计时器操作等关键传输操作的与目标无关的指令。他们开发了一个名为TINF的编程框架，并为其开发了两个兼容的后端（一个基于DPDK，一个基于Linux eXpress DataPath）。

Result: 通过在TINF框架中开发多个传输协议，并在DPDK和Linux XDP这两个后端上部署这些TINF程序，作者展示了其高级传输程序的优势和可行性。

Conclusion: 受P4等L2/L3数据包处理语言的启发，作者认为与目标无关的传输程序可以显著减少传输协议的开发工作，实现传输层的自动化分析和形式化验证，并促进对传输协议可编程目标的研究。

Abstract: Transport protocols are fundamental to network communications, continuously
evolving to meet the demands of new applications, workloads, and network
architectures while running in a wide range of execution environments (a.k.a
targets). We argue that this diversity across protocols and targets calls for a
high-level, target-agnostic programming abstraction for the transport layer.
Specifically, we propose to specify transport protocols as high-level programs
that take an event and flow state as input, and using constrained C-like
constructs, produce the updated state along with target-agnostic instructions
for key transport operations such as data reassembly, packet generation and
scheduling, and timer manipulations.
  We show the benefits of our high-level transport programs by developing
multiple transport protocols in our programming framework called TINF,
developing two TINF- compliant backends, one in DPDK and one in Linux eXpress
DataPath, and deploying TINF programs for multiple protocols across both
backends. Inspired by the benefits unlocked by L2/L3 packet-processing
languages like P4, we believe target-agnostic transport programs can reduce the
development effort for transport protocols, enable automated analysis and
formal verification of the transport layer, and further research in
programmable targets for transport protocols.

</details>


### [142] [eXplainable Artificial Intelligence for RL-based Networking Solutions](https://arxiv.org/abs/2509.21649)
*Yeison Stiven Murcia,Oscar Mauricio Caicedo,Daniela Maria Casas,Nelson Luis Saldanha da Fonseca*

Main category: cs.NI

TL;DR: 强化学习智能体在网络任务中应用广泛，但其决策难以理解。本文提出了eXplaNet——一个可解释人工智能管道，旨在帮助理解RL智能体的决策过程，并展示了其在改进基于Q-learning的路由解决方案中的应用。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）智能体已被广泛用于改进网络任务，但理解这些智能体所做出的决策对于其在网络和网络管理中的广泛采用至关重要。

Method: 引入eXplaNet，这是一个基于可解释人工智能（XAI）的管道，旨在帮助网络研究人员和从业者深入了解基于RL的解决方案的决策过程。

Result: 展示了eXplaNet如何应用于改进由Q-learning智能体驱动的路由解决方案，具体是通过优化其奖励函数。此外，文章还讨论了将可解释性融入RL以更好地优化网络性能的机遇和挑战。

Conclusion: eXplaNet为理解RL智能体在网络中的决策提供了工具，有助于提高其在网络管理中的采用率。研究工作还探讨了将可解释性引入RL以提升网络性能优化的潜力和挑战。

Abstract: Reinforcement Learning (RL) agents have been widely used to improve
networking tasks. However, understanding the decisions made by these agents is
essential for their broader adoption in networking and network management. To
address this, we introduce eXplaNet - a pipeline grounded in explainable
artificial intelligence - designed to help networking researchers and
practitioners gain deeper insights into the decision-making processes of
RL-based solutions. We demonstrate how eXplaNet can be applied to refine a
routing solution powered by a Q-learning agent, specifically by improving its
reward function. In addition, we discuss the opportunities and challenges of
incorporating explainability into RL to better optimize network performance.

</details>


### [143] [XenoFlow: How Fast Can a SmartNIC-Based DNS Load Balancer Run?](https://arxiv.org/abs/2509.21656)
*Max Schrötter,Sten Heimbrodt,Bettina Schnor*

Main category: cs.NI

TL;DR: 本文介绍了在Nvidia Bluefield-3上实现的负载均衡器XenoFlow，利用可编程NIC将网络功能从CPU卸载，相比基于eBPF的方案，延迟降低44%，并在高负载下保持低延迟，但Bluefield-3在某些配置下存在性能限制。


<details>
  <summary>Details</summary>
Motivation: 随着可编程网络硬件（如Nvidia Bluefield-3）的出现，旨在将更多网络功能从通用CPU卸载到NIC，以提升性能。

Method: 在Nvidia Bluefield-3上开发了名为XenoFlow的负载均衡器。同时，评估了Bluefield-3 eSwitch的功能和局限性，并将其性能与基于eBPF的负载均衡器进行比较。

Result: Bluefield-3 eSwitch在Flow Pipe中只有2个条目时无法达到线速。然而，硬件卸载到NIC并靠近网络具有显著优势。XenoFlow比基于eBPF的负载均衡器延迟降低了44%，并且在高负载下也能保持低延迟。

Conclusion: 尽管Bluefield-3 eSwitch存在某些局限性，但将网络功能（如负载均衡）卸载到可编程NIC（如Bluefield-3）上，可以显著降低延迟并提高性能，尤其是在高负载条件下。

Abstract: With the advent of programmable network hardware, more and more
  functionality can be moved from software running on general purpose CPUs to
  the NIC. Early NICs only allowed offloading fixed functions like checksum
  computation. Recent NICs like the Nvidia Bluefield-3 allow a fully
  programmable dataplane. In this paper, we present our first steps towards a
  load balancer named XenoFlow running on the Bluefield-3. Furthermore, we
  show the capabilities and limitations of the Bluefield-3 eSwitch. Our
  results show that the Bluefield-3 will not achieve line rate with only 2
  entries in a Flow Pipe. However, we also show the adventages of hardware
  offloading on the NIC and being closer to the network. With XenoFlow, we
  achieve an 44% lower latency compared to a comparable eBPF-based load
  balancer running on the host. Furthermore, XenoFlow achieves this low
  latency even under high load.

</details>


### [144] [Evaluating Open-Source Large Language Models for Technical Telecom Question Answering](https://arxiv.org/abs/2509.21949)
*Arina Caraus,Alessio Buscemi,Sumit Kumar,Ion Turcanu*

Main category: cs.NI

TL;DR: 本文评估了Gemma 3 27B和DeepSeek R1 32B两款开源LLM在高级无线通信领域的性能。结果显示Gemma在语义准确性上表现更佳，DeepSeek在词汇一致性上略胜一筹，并揭示了LLM在电信应用中的局限性，强调了领域适应模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型(LLMs)在多个领域能力显著，但其在电信等技术领域的表现仍未被充分探索。

Method: 构建了一个包含105个问题-答案对的基准测试集，问题来源于高级无线通信材料。评估了Gemma 3 27B和DeepSeek R1 32B两款开源LLM的性能，评估指标包括词汇度量、语义相似度以及“LLM作为评判者”评分。同时，通过溯源归属和分数方差分析了一致性、判断可靠性和幻觉现象。

Result: Gemma在语义保真度和LLM评定的正确性方面表现突出，而DeepSeek在词汇一致性方面略高。研究还发现LLM在电信应用中存在当前局限性。

Conclusion: 当前LLM在电信应用中存在局限性，需要开发领域适应性模型来支持工程领域中可信赖的人工智能助手。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities across
various fields. However, their performance in technical domains such as
telecommunications remains underexplored. This paper evaluates two open-source
LLMs, Gemma 3 27B and DeepSeek R1 32B, on factual and reasoning-based questions
derived from advanced wireless communications material. We construct a
benchmark of 105 question-answer pairs and assess performance using lexical
metrics, semantic similarity, and LLM-as-a-judge scoring. We also analyze
consistency, judgment reliability, and hallucination through source attribution
and score variance. Results show that Gemma excels in semantic fidelity and
LLM-rated correctness, while DeepSeek demonstrates slightly higher lexical
consistency. Additional findings highlight current limitations in telecom
applications and the need for domain-adapted models to support trustworthy
Artificial Intelligence (AI) assistants in engineering.

</details>


### [145] [Extreme Value Theory-enhanced Radio Maps for Handovers in Ultra-reliable Communications](https://arxiv.org/abs/2509.22547)
*Dian Echevarría Pérez,Onel L. Alcaraz López,Hirley Alves*

Main category: cs.NI

TL;DR: 为超可靠通信(URC)系统提出了一种基于物理层的新型切换(HO)框架，利用极值理论和统计无线电地图优化切换决策，显著提升了服务可用性和能源效率。


<details>
  <summary>Details</summary>
Motivation: 超可靠通信(URC)系统需要高效的切换(HO)策略来满足其严苛的性能要求。

Method: 引入了基于物理层的新型切换框架，利用极值理论(EVT)和统计无线电地图预测信号行为，以确定最优的切换时间和位置。该框架还通过有效的资源转换和空间协调确保无缝切换，并包含抑制乒乓效应的机制。

Result: 对比评估表明，该策略比传统切换机制提供了更优的服务可用性和能源效率。

Conclusion: 该新型切换策略在URC环境中表现出优越的性能，有效提高了服务可用性和能源效率。

Abstract: Efficient handover (HO) strategies are essential for maintaining the
stringent performance requirements of ultra-reliable communication (URC)
systems. This work introduces a novel HO framework designed from a
physical-layer perspective, where the decision-making process focuses on
determining the optimal time and location for performing HOs. Leveraging
extreme value theory (EVT) and statistical radio maps, the proposed method
predicts signal behaviour and enables efficient resource allocation. The
framework ensures seamless HOs and improved system performance by facilitating
effective resource transitions and coordination across spatial locations while
incorporating mechanisms to mitigate the ping-pong effect. Comparative
evaluations demonstrate that this strategy provides superior service
availability and energy efficiency than traditional HO mechanisms, highlighting
its effectiveness in URC environments.

</details>


### [146] [Bridging Technical Capability and User Accessibility: Off-grid Civilian Emergency Communication](https://arxiv.org/abs/2509.22568)
*Karim Khamaisi,Oliver Kamer,Bruno Rodrigues,Jan von der Assen,Burkhard Stiller*

Main category: cs.NI

TL;DR: 本文提出并评估了一个统一的紧急通信系统，该系统结合了长距离低功耗网络和危机导向的智能手机应用，旨在为基础设施中断下的平民提供离网、去中心化的通信能力，并在实地实验中验证了其网络鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在大型危机导致蜂窝和互联网基础设施中断时，平民缺乏可靠的通信、援助协调和获取可信信息的方法。

Method: 本文提出了一个统一的紧急通信系统，该系统整合了低功耗、长距离网络与面向危机的智能手机应用，以实现去中心化和离网的平民通信。该设计将物理层弹性与用户层可用性融合。系统通过通信性能和应用功能两个维度进行评估。

Result: 在苏黎世的城市实地实验中，868 MHz频段使用LongFast配置，实现了高达1.2公里的通信范围和92%的数据包传输率，证明了网络在真实基础设施受损条件下的鲁棒性。一个具备点对点消息、身份验证和社区审核功能的专用移动应用，通过基于需求的分析进行了功能评估。

Conclusion: 该系统成功将网络鲁棒性与用户可用性结合，为大规模危机下平民提供了一种可靠、去中心化的离网紧急通信解决方案。

Abstract: During large-scale crises disrupting cellular and Internet infrastructure,
civilians lack reliable methods for communication, aid coordination, and access
to trustworthy information. This paper presents a unified emergency
communication system integrating a low-power, long-range network with a
crisis-oriented smartphone application, enabling decentralized and off-grid
civilian communication. Unlike previous solutions separating physical layer
resilience from user layer usability, our design merges these aspects into a
cohesive crisis-tailored framework.
  The system is evaluated in two dimensions: communication performance and
application functionality. Field experiments in urban Z\"urich demonstrate that
the 868 MHz band, using the LongFast configuration, achieves a communication
range of up to 1.2 km with 92% Packet Delivery Ratio, validating network
robustness under real-world infrastructure degraded conditions. In parallel, a
purpose-built mobile application featuring peer-to-peer messaging, identity
verification, and community moderation was evaluated through a
requirements-based analysis.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [147] [MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs](https://arxiv.org/abs/2509.21634)
*Prakhar Sharma,Haohuang Wen,Vinod Yegneswaran,Ashish Gehani,Phillip Porras,Zhiqiang Lin*

Main category: cs.CR

TL;DR: MobiLLM是一个基于LLM的多智能体AI框架，旨在为6G O-RAN环境提供全自动、端到端的威胁缓解方案，通过智能体协作实现实时威胁分析、分类和响应。


<details>
  <summary>Details</summary>
Motivation: 6G O-RAN的开放性在带来创新机遇的同时也扩大了攻击面，而现有防御系统响应迟缓、人力密集且不足以应对下一代网络的复杂性。当前O-RAN应用主要关注优化或被动威胁检测，缺乏闭环、自动化响应能力，急需弹性、低成本、自主的安全解决方案。

Method: 本文提出了MobiLLM，一个基于大型语言模型（LLM）的模块化多智能体系统，用于编排安全工作流。它包含：威胁分析智能体（实时数据分类）、威胁分类智能体（利用RAG将异常映射到对策）、威胁响应智能体（通过O-RAN控制接口安全执行缓解措施）。该框架植根于MITRE FiGHT和3GPP规范等可信知识库，并配备鲁棒的安全防护机制。

Result: 初步评估表明，MobiLLM能够有效识别和协调复杂的缓解策略，显著缩短响应延迟，并展示了在6G网络中实现自主安全操作的可行性。

Conclusion: MobiLLM为可信赖的AI驱动网络安全提供了一个蓝图，证明了在6G O-RAN环境中实现全自动、端到端威胁缓解的可行性，解决了当前安全方案的关键空白。

Abstract: The evolution toward 6G networks is being accelerated by the Open Radio
Access Network (O-RAN) paradigm -- an open, interoperable architecture that
enables intelligent, modular applications across public telecom and private
enterprise domains. While this openness creates unprecedented opportunities for
innovation, it also expands the attack surface, demanding resilient, low-cost,
and autonomous security solutions. Legacy defenses remain largely reactive,
labor-intensive, and inadequate for the scale and complexity of next-generation
systems. Current O-RAN applications focus mainly on network optimization or
passive threat detection, with limited capability for closed-loop, automated
response.
  To address this critical gap, we present an agentic AI framework for fully
automated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM
orchestrates security workflows through a modular multi-agent system powered by
Large Language Models (LLMs). The framework features a Threat Analysis Agent
for real-time data triage, a Threat Classification Agent that uses
Retrieval-Augmented Generation (RAG) to map anomalies to specific
countermeasures, and a Threat Response Agent that safely operationalizes
mitigation actions via O-RAN control interfaces. Grounded in trusted knowledge
bases such as the MITRE FiGHT framework and 3GPP specifications, and equipped
with robust safety guardrails, MobiLLM provides a blueprint for trustworthy
AI-driven network security. Initial evaluations demonstrate that MobiLLM can
effectively identify and orchestrate complex mitigation strategies,
significantly reducing response latency and showcasing the feasibility of
autonomous security operations in 6G.

</details>
