<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [cs.CV](#cs.CV) [Total: 67]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.LG](#cs.LG) [Total: 63]
- [cs.NI](#cs.NI) [Total: 6]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.CR](#cs.CR) [Total: 5]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.DL](#cs.DL) [Total: 3]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [math.HO](#math.HO) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Social Bias in Multilingual Language Models: A Survey](https://arxiv.org/abs/2508.20201)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 多语言预训练模型与英语模型一样存在社会偏见。本系统综述分析了多语言及非英语语境下的偏见评估与缓解研究，揭示了方法论空白，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 预训练多语言模型与英语文本处理模型一样存在社会偏见。因此，有必要对将偏见评估和缓解方法扩展到多语言和非英语语境的新兴研究进行分析。

Method: 采用系统综述的方法，分析了多语言和非英语语境下的偏见评估与缓解研究。审查维度包括语言多样性、文化意识、评估指标选择和缓解技术。

Result: 研究揭示了该领域主流方法设计中的空白（如对某些语言的偏好、多语言偏见缓解实验的稀缺性），并梳理了在跨语言和文化适应偏见基准时遇到的常见问题和已实施的解决方案。

Conclusion: 基于研究发现的启示，论文为未来的研究指明了方向，旨在增强多语言偏见文献的包容性、跨文化适宜性，并使其与最先进的自然语言处理技术发展保持一致。

Abstract: Pretrained multilingual models exhibit the same social bias as models
processing English texts. This systematic review analyzes emerging research
that extends bias evaluation and mitigation approaches into multilingual and
non-English contexts. We examine these studies with respect to linguistic
diversity, cultural awareness, and their choice of evaluation metrics and
mitigation techniques. Our survey illuminates gaps in the field's dominant
methodological design choices (e.g., preference for certain languages, scarcity
of multilingual mitigation experiments) while cataloging common issues
encountered and solutions implemented in adapting bias benchmarks across
languages and cultures. Drawing from the implications of our findings, we chart
directions for future research that can reinforce the multilingual bias
literature's inclusivity, cross-cultural appropriateness, and alignment with
state-of-the-art NLP advancements.

</details>


### [2] [Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models](https://arxiv.org/abs/2508.20217)
*Mohammad Amini,Babak Ahmadi,Xiaomeng Xiong,Yilin Zhang,Christopher Qiao*

Main category: cs.CL

TL;DR: 本研究探索使用语言模型通过结构化提示和微调自动生成形态学多项选择题，旨在降低人工开发成本，并发现中型模型（Gemma）结合特定提示策略的表现优于大型模型（GPT-3.5）的零样本方法。


<details>
  <summary>Details</summary>
Motivation: 旨在降低人工开发形态学多项选择题（MCQs）的成本，并解决手动测试开发中存在的不一致性问题。

Method: 1. 对比了一个经过微调的中型模型（Gemma, 2B）和一个未经微调的大型模型（GPT-3.5, 175B）。2. 评估了七种结构化提示策略，包括零样本、少样本、思维链、基于角色、顺序以及它们的组合。3. 通过自动化指标和专家对五个维度进行评分来评估生成题目。4. 使用GPT-4.1模型（经专家评分样本训练）模拟大规模人类评分。

Result: 1. 结构化提示，特别是结合思维链和顺序设计的策略，显著提高了Gemma模型的输出质量。2. Gemma模型生成的题目在建构对齐和教学适宜性方面，总体上优于GPT-3.5的零样本响应。3. 提示设计在中型模型性能中起着关键作用。

Conclusion: 1. 结构化提示和高效微调可以在数据有限的条件下增强中型模型的自动生成（AIG）能力。2. 结合自动化指标、专家判断和大型模型模拟对于确保与评估目标对齐具有重要价值。3. 提出的工作流程为K-12语言评估项目的开发和验证提供了一种实用且可扩展的方法。

Abstract: This study explores automatic generation (AIG) using language models to
create multiple choice questions (MCQs) for morphological assessment, aiming to
reduce the cost and inconsistency of manual test development. The study used a
two-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B)
with a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven
structured prompting strategies, including zero-shot, few-shot,
chain-of-thought, role-based, sequential, and combinations. Generated items
were assessed using automated metrics and expert scoring across five
dimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate
human scoring at scale. Results show that structured prompting, especially
strategies combining chain-of-thought and sequential design, significantly
improved Gemma's outputs. Gemma generally produced more construct-aligned and
instructionally appropriate items than GPT-3.5's zero-shot responses, with
prompt design playing a key role in mid-size model performance. This study
demonstrates that structured prompting and efficient fine-tuning can enhance
midsized models for AIG under limited data conditions. We highlight the value
of combining automated metrics, expert judgment, and large-model simulation to
ensure alignment with assessment goals. The proposed workflow offers a
practical and scalable way to develop and validate language assessment items
for K-12.

</details>


### [3] [Integrating SystemC TLM into FMI 3.0 Co-Simulations with an Open-Source Approach](https://arxiv.org/abs/2508.20223)
*Andrei Mihai Albu,Giovanni Pollo,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Alessandra Neri,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

TL;DR: 本文提出一种开源方法，将SystemC TLM模型集成到FMI-based协同仿真工作流中，以实现跨异构仿真环境的无缝标准化集成。


<details>
  <summary>Details</summary>
Motivation: 网络物理系统（特别是汽车应用）日益复杂，需要高效的建模和跨领域协同仿真技术。SystemC TLM虽然在硬件/软件协同设计中有效，但与其他工程领域模型的互操作性有限，导致集成挑战。

Method: 本研究提出一种完全开源的方法论，通过将SystemC TLM组件封装为FMI 3.0协同仿真功能模型单元（FMUs），将其集成到FMI-based协同仿真工作流中。同时引入了轻量级开源工具链，并解决了时间同步和数据交换等关键技术挑战。

Result: 该方法促进了跨异构仿真环境的无缝、标准化集成。通过代表性案例研究，证明了集成方案的可行性和有效性。

Conclusion: 所提出的开源方法成功地将SystemC TLM模型集成到基于FMI的协同仿真中，有效解决了跨领域互操作性问题，为复杂网络物理系统提供了标准化且高效的协同仿真能力。

Abstract: The growing complexity of cyber-physical systems, particularly in automotive
applications, has increased the demand for efficient modeling and cross-domain
co-simulation techniques. While SystemC Transaction-Level Modeling (TLM)
enables effective hardware/software co-design, its limited interoperability
with models from other engineering domains poses integration challenges. This
paper presents a fully open-source methodology for integrating SystemC TLM
models into Functional Mock-up Interface (FMI)-based co-simulation workflows.
By encapsulating SystemC TLM components as FMI 3.0 Co Simulation Functional
Mock-up Units (FMUs), the proposed approach facilitates seamless, standardized
integration across heterogeneous simulation environments. We introduce a
lightweight open-source toolchain, address key technical challenges such as
time synchronization and data exchange, and demonstrate the feasibility and
effectiveness of the integration through representative case studies.

</details>


### [4] [Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities](https://arxiv.org/abs/2508.20324)
*Rikuto Kotoge,Mai Nishimura,Jiaxin Ma*

Main category: cs.CL

TL;DR: 为解决紧凑型语言模型在Agentic RAG中面临的挑战，本文提出了DGPO方法，通过教师指导使小模型能高效执行复杂的Agentic RAG行为，甚至超越大型教师模型，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 强化学习被用于引导语言模型实现Agentic RAG（如搜索和规划），但紧凑型语言模型（例如0.5B参数）因推理能力差而难以应对，导致奖励稀疏和训练不稳定。

Method: 提出蒸馏引导策略优化（DGPO）方法，通过教师演示进行冷启动初始化，并在策略优化过程中提供持续的教师指导。为系统评估，引入了Agentic RAG能力（ARC）这一细粒度指标，用于分析推理、搜索协调和响应合成。

Result: 全面的实验证明，DGPO使紧凑型模型能够实现复杂的Agentic搜索行为，在某些情况下甚至超越了大型教师模型。

Conclusion: DGPO使得在计算资源受限的环境中实现Agentic RAG成为可能。

Abstract: Reinforcement Learning has emerged as a post-training approach to elicit
agentic RAG behaviors such as search and planning from language models.
However, compact language models (e.g., 0.5B parameters) struggle due to poor
reasoning ability, resulting in sparse rewards and unstable training. To
overcome these difficulties, we propose Distillation-Guided Policy Optimization
(DGPO), which addresses the challenges through cold-start initialization from
teacher demonstrations and continuous teacher guidance during policy
optimization. To systematically evaluate our approach, we introduce Agentic RAG
Capabilities (ARC), a fine-grained metric analyzing reasoning, search
coordination, and response synthesis. Comprehensive experiments demonstrate
that DGPO enables compact models to achieve sophisticated agentic search
behaviors, even outperforming the larger teacher model in some cases. DGPO
makes agentic RAG feasible in computing resource-constrained environments.

</details>


### [5] [GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs](https://arxiv.org/abs/2508.20325)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Andy Zhou,Yang Zhang,Haohan Wang*

Main category: cs.CL

TL;DR: GUARD是一种创新的测试方法，能将政府颁布的AI伦理指南转化为可操作的测试问题，以评估大型语言模型（LLMs）的合规性，并通过“越狱诊断”识别潜在漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在各领域日益普及，其生成有害回复的潜力引发了广泛关注。政府虽然发布了伦理指南，但这些指南通常是高层次要求，缺乏将其转化为具体可操作的测试问题来验证LLM合规性的方法。

Method: 本研究引入了GUARD方法。GUARD通过自动化生成基于政府指南的违规问题来测试LLM响应的合规性。对于直接违规的响应，GUARD会报告不一致。对于未直接违规的响应，GUARD整合了“越狱”概念，形成GUARD-JD，通过创建情景来诱导非伦理或违规响应，以识别绕过内置安全机制的潜在漏洞。最终，该方法生成一份合规报告。

Result: GUARD在七个LLMs（包括Vicuna-13B、LongChat-7B、Llama2-7B、Llama-3-8B、GPT-3.5、GPT-4、GPT-4o和Claude-3.7）上进行了实证验证，测试了三个政府指南下的合规性并进行了越狱诊断。此外，GUARD-JD还能将越狱诊断转移到视觉语言模型（VLMs）上。

Conclusion: GUARD方法有效地将高层次的AI伦理指南转化为可操作的测试问题，并能通过越狱诊断揭示LLMs的潜在不合规行为和安全漏洞，从而促进LLM应用的可靠性。

Abstract: As Large Language Models become increasingly integral to various domains,
their potential to generate harmful responses has prompted significant societal
and regulatory concerns. In response, governments have issued ethics guidelines
to promote the development of trustworthy AI. However, these guidelines are
typically high-level demands for developers and testers, leaving a gap in
translating them into actionable testing questions to verify LLM compliance.
  To address this challenge, we introduce GUARD (\textbf{G}uideline
\textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and
Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize
guidelines into specific guideline-violating questions that assess LLM
adherence. To implement this, GUARD uses automated generation of
guideline-violating questions based on government-issued guidelines, thereby
testing whether responses comply with these guidelines. When responses directly
violate guidelines, GUARD reports inconsistencies. Furthermore, for responses
that do not directly violate guidelines, GUARD integrates the concept of
``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that
provoke unethical or guideline-violating responses, effectively identifying
potential scenarios that could bypass built-in safety mechanisms. Our method
finally culminates in a compliance report, delineating the extent of adherence
and highlighting any violations.
  We have empirically validated the effectiveness of GUARD on seven LLMs,
including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4,
GPT-4o, and Claude-3.7, by testing compliance under three government-issued
guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can
transfer jailbreak diagnostics to vision-language models, demonstrating its
usage in promoting reliable LLM-based applications.

</details>


### [6] [Joint Enhancement of Relational Reasoning for Long-Context LLMs](https://arxiv.org/abs/2508.20351)
*Zhirui Chen,Wei Shen,Jiashui Huang,Ling Shao*

Main category: cs.CL

TL;DR: JERR是一个新颖框架，通过结合摘要提取、图构建和蒙特卡洛树搜索（MCTS）等图推理方法，旨在增强大型语言模型（LLMs）的长上下文理解能力，解决其记忆限制、透明度不足和幻觉问题，并在实验中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理长上下文时存在记忆限制，难以应对复杂和长文本任务。此外，LLMs常缺乏透明度且易产生幻觉。

Method: 本文提出了JERR框架，包含三个核心组件：1. **摘要提取**：通过策略性地分块文本来高效总结信息。2. **图构建**：构建有向无环图（DAG）以解决冗余，确保逻辑一致性。3. **关系推理**：引入蒙特卡洛树搜索（MCTS）以引导模型导航复杂推理路径，确保输出的准确性和可解释性。

Result: 实验结果表明，JERR在ROUGE和F1指标上持续优于所有基线模型，并在LLM-Rater评估中取得了最高分。

Conclusion: JERR框架为LLMs处理扩展上下文和复杂推理任务提供了一种新颖的解决方案，通过图推理显著提高了模型的可靠性和透明度。

Abstract: Despite significant progress, large language models (LLMs) still struggle
with long contexts due to memory limitations and their inability to tackle
complex and long-context tasks. Additionally, LLMs often suffer from a lack of
transparency and are prone to producing hallucinations. To address these
challenges, we propose \textbf{JERR}, a novel framework designed to enhance
long-context comprehension via graph-based reasoning in LLMs. JERR integrates
three key components: synopsis extraction, graph construction, and relational
reasoning. First, synopsis is extracted by chunking text strategically,
allowing the model to summarize and understand information more efficiently.
Second, we build a directed acyclic graph (DAG) to resolve redundancy, ensuring
logical consistency and clarity. Finally, we incorporate Monte Carlo Tree
Search (MCTS) to help the model navigate complex reasoning paths, ensuring more
accurate and interpretable outputs. This framework provides a novel solution
that enables LLMs to handle extended contexts and complex reasoning tasks with
improved reliability and transparency. Experimental results show that JERR
consistently outperforms all baselines on the ROUGE and F1 metrics, achieving
the highest scores on the LLM-Rater evaluation.

</details>


### [7] [Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems](https://arxiv.org/abs/2508.20373)
*Yuyao Wang,Bowen Liu,Jianheng Tang,Nuo Chen,Yuhan Li,Qifan Zhang,Jia Li*

Main category: cs.CL

TL;DR: 针对LLM长链思维（Long CoT）训练数据成本高昂且缺乏可扩展性的问题，本文提出使用NP-hard（NPH）图问题作为新型合成训练语料。通过两阶段（SFT+RL）后训练框架，模型Graph-R1-7B在多领域展现泛化能力，并在NPH图问题上超越基线模型，证明了NPH图问题是LLM Long CoT推理的有效且可扩展资源。


<details>
  <summary>Details</summary>
Motivation: 推理大语言模型（RLLMs）通过长链思维（Long CoT）在复杂推理任务上取得显著进展。然而，发展这些Long CoT行为所需的后训练数据集通常成本高昂且依赖人工，导致缺乏可扩展的替代方案。

Method: 引入NP-hard（NPH）图问题作为一种新颖的合成训练语料。开发了一个两阶段后训练框架：(i) 对拒绝采样的NPH图实例进行Long CoT监督微调（SFT），以增强推理深度；(ii) 使用精细奖励设计的强化学习（RL），以提高推理效率。

Result: 旗舰模型Graph-R1-7B在数学、编码、STEM和逻辑等领域展现出强大的泛化能力。在NPH图问题上，Graph-R1-7B在准确性和推理效率方面均超越了QwQ-32B。

Conclusion: 研究结果表明，NPH图问题是推动LLM长链思维推理的有效且可扩展的资源，为LLM的后训练开辟了新的前沿。

Abstract: Reasoning Large Language Models (RLLMs) have recently achieved remarkable
progress on complex reasoning tasks, largely enabled by their long
chain-of-thought (Long CoT) capabilities. However, developing these Long CoT
behaviors relies heavily on post-training with high-quality datasets, which are
typically costly and human-curated (e.g., mathematics and code), leaving
scalable alternatives unexplored. In this work, we introduce NP-hard (NPH)
graph problems as a novel synthetic training corpus, as they inherently require
deep reasoning, extensive exploration, and reflective strategies, which are
core characteristics of Long CoT reasoning. Building on this insight, we
develop a two-stage post-training framework: (i) Long CoT Supervised
Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially
enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a
fine-grained reward design, which sharpens reasoning efficiency. Our flagship
model, Graph-R1-7B, demonstrates strong generalization across mathematics,
coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both
accuracy and reasoning efficiency. These results position NPH graph problems as
an effective and scalable resource for advancing Long CoT reasoning in LLMs,
opening a new frontier for LLM post-training. Our implementation is available
at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted
in our Hugging Face collection HKUST-DSAIL/Graph-R1.

</details>


### [8] [CAPE: Context-Aware Personality Evaluation Framework for Large Language Models](https://arxiv.org/abs/2508.20385)
*Jivnesh Sandhan,Fei Cheng,Tushar Sandhan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 现有LLM心理测试忽略上下文，本研究提出了首个上下文感知个性评估（CAPE）框架，发现对话历史能提高LLM响应一致性但也会导致个性偏移，不同模型对上下文和问题顺序的敏感度不同，且上下文评估能提升角色扮演代理的表现。


<details>
  <summary>Details</summary>
Motivation: 现有对大型语言模型（LLMs）的心理测试采用无上下文方法，独立回答问题，忽略了真实世界中对话历史对响应的影响，导致评估结果与实际应用脱节。

Method: 提出了首个上下文感知个性评估（CAPE）框架，将先前的对话交互纳入LLM的评估中。引入了量化LLM响应一致性的新指标。对7个LLM进行了详尽实验，并将其应用于角色扮演代理（RPAs）。

Result: 对话历史通过上下文学习提高了响应一致性，但也导致了GPT-3.5-Turbo和GPT-4-Turbo等模型的极端个性偏移。GPT模型对问题顺序具有鲁棒性，而Gemini-1.5-Flash和Llama-8B则表现出显著敏感性。GPT模型的响应源于其内在个性和先前交互，而Gemini-1.5-Flash和Llama-8B则高度依赖先前交互。将该框架应用于角色扮演代理时，上下文相关的个性偏移提高了响应一致性并更好地与人类判断保持一致。

Conclusion: 上下文在LLM个性评估中至关重要，它能提高响应一致性，但也会导致个性偏移，且不同模型受上下文影响的机制不同。上下文感知评估对于理解LLM行为及其在真实世界应用（如角色扮演代理）中的表现至关重要。

Abstract: Psychometric tests, traditionally used to assess humans, are now being
applied to Large Language Models (LLMs) to evaluate their behavioral traits.
However, existing studies follow a context-free approach, answering each
question in isolation to avoid contextual influence. We term this the Disney
World test, an artificial setting that ignores real-world applications, where
conversational history shapes responses. To bridge this gap, we propose the
first Context-Aware Personality Evaluation (CAPE) framework for LLMs,
incorporating prior conversational interactions. To thoroughly analyze the
influence of context, we introduce novel metrics to quantify the consistency of
LLM responses, a fundamental trait in human behavior.
  Our exhaustive experiments on 7 LLMs reveal that conversational history
enhances response consistency via in-context learning but also induces
personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme
deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash
and Llama-8B display significant sensitivity. Moreover, GPT models response
stem from their intrinsic personality traits as well as prior interactions,
whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions.
Finally, applying our framework to Role Playing Agents (RPAs) shows
context-dependent personality shifts improve response consistency and better
align with human judgments. Our code and datasets are publicly available at:
https://github.com/jivnesh/CAPE

</details>


### [9] [Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction](https://arxiv.org/abs/2508.20395)
*Xu Guo*

Main category: cs.CL

TL;DR: 本研究通过预言机实验发现，大语言模型推理过程中条件熵的下降与答案正确性强相关，而熵的平坦或上升则常导致错误。同时，不正确的推理路径往往更长，为设计高效推理管线提供了基础。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）通过生成中间推理步骤提升准确性，但关于推理效用如何贡献于最终答案正确性的研究较少。由于自回归生成的随机性，增加上下文不一定提高答案置信度，反而可能引入干扰。因此，有必要在生成过程中预测推理步骤的有效性，以实现早期停止或剪枝无效步骤。

Method: 本研究在MATH数据集上进行了预言机（oracle）研究。使用Qwen2.5-32B和GPT-4o生成推理链，并利用独立的Qwen3-8B模型量化这些推理链对最终准确性的效用。具体方法是，通过逐步扩展上下文，测量每个推理步骤中模型对答案范围Y的不确定性，即条件熵（词汇表上的预期负对数似然）。

Result: 研究结果显示，随着推理步骤的进行，条件熵的下降与正确答案呈强关联；而熵值平坦或增加的推理通常会导致错误答案。此外，不正确的推理路径往往比正确路径更长，这表明更长的推理过程不一定能带来更好的结果。

Conclusion: 这些发现为未来设计高效的推理流程奠定了基础，该流程能够及早检测并避免无用的推理步骤。

Abstract: Recent advancements in large language models (LLMs) often rely on generating
intermediate reasoning steps to enhance accuracy. However, little work has
examined how reasoning utility contributes to the final answer's correctness.
Due to the stochastic nature of autoregressive generation, generating more
context does not guarantee increased confidence in the answer. If we could
predict, during generation, whether a reasoning step will be useful, we could
stop early or prune ineffective steps, avoiding distractions in the final
decision.
  We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to
generate reasoning chains, and then employing a separate model (Qwen3-8B) to
quantify the utility of these chains for final accuracy. Specifically, we
measure the model's uncertainty on the answer span Y at each reasoning step
using conditional entropy (expected negative log-likelihood over the
vocabulary) with context expanding step by step. Our results show a clear
pattern: conditional entropy that decreases over steps is strongly associated
with correct answers, whereas flat or increasing entropy often results in wrong
answers. We also corroborate that incorrect reasoning paths tend to be longer
than correct ones, suggesting that longer reasoning does not necessarily yield
better outcomes. These findings serve as a foundation to inspire future work on
designing efficient reasoning pipelines that detect and avoid unproductive
reasoning early.

</details>


### [10] [UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools](https://arxiv.org/abs/2508.20410)
*Sam Jung,Agustin Garcinuno,Spencer Mateega*

Main category: cs.CL

TL;DR: 研究引入了UI-Bench，首个大规模基准测试，通过专家两两对比来评估AI文本转应用工具的视觉质量，并发布了相关的资源（提示集、评估框架和排行榜）。


<details>
  <summary>Details</summary>
Motivation: AI文本转应用工具承诺在几分钟内生成高质量应用和网站，但目前缺乏公开、严格的基准来验证这些声明。

Method: 引入UI-Bench，一个大规模基准测试，通过专家两两对比评估10种AI文本转应用工具在30个提示下生成的300个网站的视觉卓越性。该基准包含超过4000次专家判断，并使用基于TrueSkill的模型对系统进行排名，提供校准的置信区间。

Result: UI-Bench建立了AI驱动网页设计的可复现标准。研究发布了完整的提示集、一个开源评估框架和公共排行榜。参与者评级的生成网站也将很快发布。

Conclusion: UI-Bench通过提供一个大规模、可复现的基准测试和评估框架，填补了AI文本转应用工具视觉质量评估的空白，为推动AI驱动网页设计提供了明确的标准和工具。

Abstract: AI text-to-app tools promise high quality applications and websites in
minutes, yet no public benchmark rigorously verifies those claims. We introduce
UI-Bench, the first large-scale benchmark that evaluates visual excellence
across competing AI text-to-app tools through expert pairwise comparison.
Spanning 10 tools, 30 prompts, 300 generated sites, and \textit{4000+} expert
judgments, UI-Bench ranks systems with a TrueSkill-derived model that yields
calibrated confidence intervals. UI-Bench establishes a reproducible standard
for advancing AI-driven web design. We release (i) the complete prompt set,
(ii) an open-source evaluation framework, and (iii) a public leaderboard. The
generated sites rated by participants will be released soon. View the UI-Bench
leaderboard at https://uibench.ai/leaderboard.

</details>


### [11] [DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding](https://arxiv.org/abs/2508.20416)
*Hengchuan Zhu,Yihuan Xu,Yichen Li,Zijie Meng,Zuozhu Liu*

Main category: cs.CL

TL;DR: 本文介绍了DentalBench，首个综合性双语牙科领域基准测试，包括问答数据集DentalQA和领域语料库DentalCorpus。通过评估14个LLM并进行领域适应性实验，揭示了LLM在牙科领域的性能差距，并证明了领域适应对提升性能的重要性。


<details>
  <summary>Details</summary>
Motivation: 通用和医学LLM在普通医学基准上表现出色，但在牙科等需要深层专业知识的细分医学领域能力尚不明确，且缺乏专门的评估资源。因此，有必要开发针对牙科领域的可靠LLM。

Method: 引入DentalBench，包含两部分：1. DentalQA，一个包含36,597个问题、涵盖4项任务和16个牙科子领域的英汉双语问答基准。2. DentalCorpus，一个包含3.3735亿词元的高质量牙科领域语料库，支持监督微调(SFT)和检索增强生成(RAG)。评估了14个不同类型的LLM，并使用Qwen-2.5-3B进行领域适应性实验。

Result: 评估结果显示，现有LLM在不同任务类型和语言上存在显著的性能差距。通过领域适应性实验，证明了领域适应能大幅提升模型性能，尤其是在知识密集型和术语相关的任务上。

Conclusion: 领域特定的基准对于开发可信赖且高效的医疗健康领域LLM至关重要。DentalBench为此提供了牙科领域的解决方案，并强调了领域适应的价值。

Abstract: Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs)
have demonstrated strong performance on general medical benchmarks. However,
their capabilities in specialized medical fields, such as dentistry which
require deeper domain-specific knowledge, remain underexplored due to the lack
of targeted evaluation resources. In this paper, we introduce DentalBench, the
first comprehensive bilingual benchmark designed to evaluate and advance LLMs
in the dental domain. DentalBench consists of two main components: DentalQA, an
English-Chinese question-answering (QA) benchmark with 36,597 questions
spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale,
high-quality corpus with 337.35 million tokens curated for dental domain
adaptation, supporting both supervised fine-tuning (SFT) and
retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering
proprietary, open-source, and medical-specific models, and reveal significant
performance gaps across task types and languages. Further experiments with
Qwen-2.5-3B demonstrate that domain adaptation substantially improves model
performance, particularly on knowledge-intensive and terminology-focused tasks,
and highlight the importance of domain-specific benchmarks for developing
trustworthy and effective LLMs tailored to healthcare applications.

</details>


### [12] [KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval](https://arxiv.org/abs/2508.20417)
*Chi Minh Bui,Ngoc Mai Thieu,Van Vinh Nguyen,Json J. Jung,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: KG-CQR通过利用知识图谱丰富复杂查询的上下文表示，显著提升了检索增强生成（RAG）系统的检索阶段性能，并在多个数据集和多跳问答任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）系统在处理复杂查询时，其检索阶段可能因上下文表示不足而表现不佳。尽管一些方法关注语料库层面的上下文丢失，但对查询本身的上下文丰富仍是提升RAG检索性能的关键。

Method: 本文提出了KG-CQR框架，一个新颖的上下文查询检索（CQR）方法，通过使用以语料库为中心的知识图谱（KG）来丰富复杂输入查询的上下文表示，从而增强RAG的检索阶段。KG-CQR区别于现有主要解决语料库层面上下文丢失的方法，它侧重于通过结构化关系表示进行查询丰富，通过提取并补全相关的KG子图来生成语义丰富的查询上下文。作为一个模型无关的流水线，KG-CQR由子图提取、补全和上下文生成模块组成，无需额外训练即可与不同大小的LLM兼容。

Result: 在RAGBench和MultiHop-RAG数据集上的实验结果表明，KG-CQR的表现优于强基线模型，mAP提升了4-6%，Recall@25提升了2-3%。此外，在多跳问答等挑战性RAG任务上的评估显示，结合KG-CQR后，检索有效性性能持续优于现有基线。

Conclusion: KG-CQR通过有效地利用知识图谱丰富复杂查询的上下文，显著提升了RAG系统的检索性能，尤其是在处理挑战性任务时。作为一个可扩展、模型无关且无需额外训练的解决方案，KG-CQR为RAG系统提供了一个有前景的改进途径。

Abstract: The integration of knowledge graphs (KGs) with large language models (LLMs)
offers significant potential to improve the retrieval phase of
retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,
a novel framework for Contextual Query Retrieval (CQR) that enhances the
retrieval phase by enriching the contextual representation of complex input
queries using a corpus-centric KG. Unlike existing methods that primarily
address corpus-level context loss, KG-CQR focuses on query enrichment through
structured relation representations, extracting and completing relevant KG
subgraphs to generate semantically rich query contexts. Comprising subgraph
extraction, completion, and contextual generation modules, KG-CQR operates as a
model-agnostic pipeline, ensuring scalability across LLMs of varying sizes
without additional training. Experimental results on RAGBench and MultiHop-RAG
datasets demonstrate KG-CQR's superior performance, achieving a 4-6%
improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline
models. Furthermore, evaluations on challenging RAG tasks such as multi-hop
question answering show that, by incorporating KG-CQR, the performance
consistently outperforms the existing baseline in terms of retrieval
effectiveness

</details>


### [13] [CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance](https://arxiv.org/abs/2508.20420)
*Feng Zhang,Chengjie Pang,Yuehan Zhang,Chenyu Luo*

Main category: cs.CL

TL;DR: 针对民用航空维修领域缺乏专业LLM评估工具的问题，本文开发并开源了一个工业级基准测试（benchmark），用于衡量LLM在该领域的能力、识别知识和推理缺陷，并评估现有RAG组件的性能。


<details>
  <summary>Details</summary>
Motivation: 民用航空维修是知识密集型且需要复杂推理的领域，但当前LLM评估主要集中在数学和编程，缺乏专门的工具来衡量LLM在该领域的实际能力，识别其特定领域知识和复杂推理能力的不足。

Method: 本文提出并开发了一个专门针对民用航空维修的工业级基准测试。该基准被用于评估现有主流的向量嵌入模型和大型语言模型在民航维修场景下的表现，尤其关注检索增强生成（RAG）系统。

Result: 通过实验探索和分析，研究证明了所提出的基准测试能有效评估模型在民用航空维修领域内的性能。该基准测试能够识别模型在领域知识和复杂推理方面的具体差距。

Conclusion: 该工作填补了当前LLM评估领域的空白，提供了一个标准化工具来推动民用航空维修领域智能解决方案的进步。作者已开源该评估基准和代码，以促进进一步的研究和开发。

Abstract: Civil aviation maintenance is a domain characterized by stringent industry
standards. Within this field, maintenance procedures and troubleshooting
represent critical, knowledge-intensive tasks that require sophisticated
reasoning. To address the lack of specialized evaluation tools for large
language models (LLMs) in this vertical, we propose and develop an
industrial-grade benchmark specifically designed for civil aviation
maintenance. This benchmark serves a dual purpose: It provides a standardized
tool to measure LLM capabilities within civil aviation maintenance, identifying
specific gaps in domain knowledge and complex reasoning. By pinpointing these
deficiencies, the benchmark establishes a foundation for targeted improvement
efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized
prompt engineering), ultimately facilitating progress toward more intelligent
solutions within civil aviation maintenance. Our work addresses a significant
gap in the current LLM evaluation, which primarily focuses on mathematical and
coding reasoning tasks. In addition, given that Retrieval-Augmented Generation
(RAG) systems are currently the dominant solutions in practical applications ,
we leverage this benchmark to evaluate existing well-known vector embedding
models and LLMs for civil aviation maintenance scenarios. Through experimental
exploration and analysis, we demonstrate the effectiveness of our benchmark in
assessing model performance within this domain, and we open-source this
evaluation benchmark and code to foster further research and
development:https://github.com/CamBenchmark/cambenchmark

</details>


### [14] [Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method](https://arxiv.org/abs/2508.20442)
*Agung Sukrisna Jaya,Osvari Arsalan,Danny Matthew Saputra*

Main category: cs.CL

TL;DR: 该研究利用案例推理（CBR）结合TF-IDF和余弦相似度，开发了一个系统用于搜索实践工作标题，并在测试中取得了良好的匹配效果。


<details>
  <summary>Details</summary>
Motivation: 旨在利用基于经验的案例推理技术，根据相似度搜索实用的工作标题。

Method: 使用案例推理（CBR）作为案例求解技术。TF-IDF用于处理每个实践工作标题词的向量化，而余弦相似度用于计算相似度值。该系统支持通过标题或关键词搜索，并输出实践工作标题及其匹配值。测试基于705个实践工作标题，通过两个阶段（现有标题搜索和随机标题搜索）进行。

Result: 在第二阶段的测试中（随机标题），系统找到了相同数量的标题，并取得了最高的平均匹配分数。

Conclusion: 该系统能够有效并准确地搜索实践工作标题，尤其在处理随机化查询时表现出良好的匹配性能，验证了CBR结合TF-IDF和余弦相似度在实践工作标题搜索中的有效性。

Abstract: Case Base Reasoning (CBR) is a case solving technique based on experience in
cases that have occurred before with the highest similarity. CBR is used to
search for practical work titles. TF-IDF is applied to process the
vectorization of each practical work title word and Cosine Similarity for the
calculation of similarity values. This system can search either in the form of
titles or keywords. The output of the system is the title of practical work and
the match value of each title. Based on the test results using 705 practical
work titles, testing was carried out with five titles and carried out in two
stages. The first stage searches with existing titles and the second stage
randomizes the title from the first stage. And the results obtained in the
second stage are the same number of titles found and the highest average match
score.

</details>


### [15] [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers](https://arxiv.org/abs/2508.20453)
*Zhenting Wang,Qi Chang,Hemani Patel,Shashank Biju,Cheng-En Wu,Quan Liu,Aolin Ding,Alireza Rezazadeh,Ankit Shah,Yujia Bao,Eugene Siow*

Main category: cs.CL

TL;DR: 本文介绍了MCP-Bench，一个用于评估大型语言模型（LLM）在需要工具使用、跨工具协调、精确参数控制和规划推理的真实多步骤任务上的新基准。


<details>
  <summary>Details</summary>
Motivation: 现有API基准未能充分评估LLM在模糊指令下的工具检索、复杂多跳规划、结果整合以及跨领域工作流等关键能力，且缺乏真实多步骤任务所需的工具间协调与精确参数控制。

Method: MCP-Bench基于模型上下文协议（MCP）构建，连接了28个代表性实时MCP服务器，涵盖250种工具，横跨金融、旅行、科学计算和学术搜索等领域。其任务旨在测试LLM在无明确工具名下检索工具、规划多跳执行轨迹、基于中间工具输出进行响应以及协调跨领域工作流的能力。评估框架涵盖工具级模式理解与使用、轨迹级规划和任务完成度。

Result: 对20个先进LLM进行的实验表明，在MCP-Bench设定的复杂多步骤任务中，这些模型仍面临持续的挑战。

Conclusion: MCP-Bench揭示了当前LLM在处理需要高级工具使用、复杂规划和跨领域协调的真实多步骤任务方面存在的显著不足，为未来的研究指明了方向。

Abstract: We introduce MCP-Bench, a benchmark for evaluating large language models
(LLMs) on realistic, multi-step tasks that demand tool use, cross-tool
coordination, precise parameter control, and planning/reasoning for solving
tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28
representative live MCP servers spanning 250 tools across domains such as
finance, traveling, scientific computing, and academic search. Unlike prior
API-based benchmarks, each MCP server provides a set of complementary tools
designed to work together, enabling the construction of authentic, multi-step
tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability
to retrieve relevant tools from fuzzy instructions without explicit tool names,
plan multi-hop execution trajectories for complex objectives, ground responses
in intermediate tool outputs, and orchestrate cross-domain workflows -
capabilities not adequately evaluated by existing benchmarks that rely on
explicit tool specifications, shallow few-step workflows, and isolated domain
operations. We propose a multi-faceted evaluation framework covering tool-level
schema understanding and usage, trajectory-level planning, and task completion.
Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code
and data: https://github.com/Accenture/mcp-bench.

</details>


### [16] [Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques](https://arxiv.org/abs/2508.20460)
*Yucheng Ruan,Xiang Lan,Daniel J. Tan,Hairil Rizal Abdullah,Mengling Feng*

Main category: cs.CL

TL;DR: 本研究引入并评估了一个深度学习框架，该框架利用自然语言处理技术整合多模态电子健康记录（包括自由文本），以更准确地预测重症监护室的死亡率和资源利用率，并在数据损坏情况下表现出强大的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注结构化电子健康记录，忽略了自由文本中的宝贵临床信息，也未充分利用结构化数据中的文本潜力。然而，准确预测重症监护室（ICU）患者的死亡率和资源利用率对于优化患者结果和管理成本至关重要，但极具挑战性。

Method: 开发并评估了一个基于深度学习的框架，该框架通过自然语言处理技术整合了多模态电子健康记录（EHRs）。模型在两个真实世界的EHR数据集上，针对死亡率预测、住院时长（LOS）预测和手术时长估计三个临床任务进行了评估，并与现有领先方法进行比较。研究还对框架中的医疗提示、自由文本和预训练句子编码器进行了消融研究，并评估了模型对结构化EHR数据损坏的鲁棒性。

Result: 该模型在两个真实数据集和三个临床任务上均表现出卓越性能。相对于最佳现有方法，死亡率预测的BACC/AUROC分别提高了1.6%/0.8%，LOS预测的RMSE/MAE分别提高了0.5%/2.2%，手术时长估计的RMSE/MAE分别提高了10.9%/11.0%。此外，在不同损坏率下，模型在三个任务中始终优于其他基线方法，尤其在高损坏水平下，对结构化数据损坏显示出强大的韧性。

Conclusion: 所提出的框架是一种有效且准确的深度学习方法，可用于预测重症监护中的死亡率和资源利用率。研究强调了结合提示学习和Transformer编码器分析多模态EHRs的成功。值得注意的是，模型对结构化数据中的数据损坏表现出强大的鲁棒性，尤其是在高损坏水平下。

Abstract: Background Predicting mortality and resource utilization from electronic
health records (EHRs) is challenging yet crucial for optimizing patient
outcomes and managing costs in intensive care unit (ICU). Existing approaches
predominantly focus on structured EHRs, often ignoring the valuable clinical
insights in free-text notes. Additionally, the potential of textual information
within structured data is not fully leveraged. This study aimed to introduce
and assess a deep learning framework using natural language processing
techniques that integrates multimodal EHRs to predict mortality and resource
utilization in critical care settings. Methods Utilizing two real-world EHR
datasets, we developed and evaluated our model on three clinical tasks with
leading existing methods. We also performed an ablation study on three key
components in our framework: medical prompts, free-texts, and pre-trained
sentence encoder. Furthermore, we assessed the model's robustness against the
corruption in structured EHRs. Results Our experiments on two real-world
datasets across three clinical tasks showed that our proposed model improved
performance metrics by 1.6\%/0.8\% on BACC/AUROC for mortality prediction,
0.5%/2.2% on RMSE/MAE for LOS prediction, 10.9%/11.0% on RMSE/MAE for surgical
duration estimation compared to the best existing methods. It consistently
demonstrated superior performance compared to other baselines across three
tasks at different corruption rates. Conclusions The proposed framework is an
effective and accurate deep learning approach for predicting mortality and
resource utilization in critical care. The study also highlights the success of
using prompt learning with a transformer encoder in analyzing multimodal EHRs.
Importantly, the model showed strong resilience to data corruption within
structured data, especially at high corruption levels.

</details>


### [17] [ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety](https://arxiv.org/abs/2508.20468)
*Luke Bates,Max Glockner,Preslav Nakov,Iryna Gurevych*

Main category: cs.CL

TL;DR: 引入首个标注阴谋论认知特性的数据集ConspirED。开发模型识别阴谋论特质，并评估LLM对阴谋论输入的鲁棒性。发现计算模型和LLM均易受阴谋论影响，输出模式与输入相似，即使对事实核查的虚假信息能有效处理。


<details>
  <summary>Details</summary>
Motivation: 阴谋论侵蚀公众信任并抵制反驳。鉴于AI生成虚假信息日益复杂，理解阴谋论的修辞模式对于开发干预措施（如预揭穿）和评估AI漏洞至关重要。

Method: 构建ConspirED数据集，该数据集包含来自在线阴谋论文章的80-120词多句摘录，并使用CONSPIR认知框架标注了阴谋论的认知特征。利用此数据集，(i)开发了识别并确定文本中阴谋论主导特质的计算模型；(ii)评估了大型语言/推理模型（LLM/LRM）对阴谋论输入的鲁棒性。

Result: 研究发现，开发的计算模型和大型语言模型（LLM/LRM）均会被阴谋论内容误导，其输出会模仿输入的推理模式，即便它们能够成功驳斥可比的事实核查过的虚假信息。

Conclusion: 当前计算模型和大型语言模型在处理阴谋论内容时表现出脆弱性，其输出易受阴谋论推理模式影响，这在对抗日益复杂的AI虚假信息方面构成挑战。

Abstract: Conspiracy theories erode public trust in science and institutions while
resisting debunking by evolving and absorbing counter-evidence. As AI-generated
misinformation becomes increasingly sophisticated, understanding rhetorical
patterns in conspiratorial content is important for developing interventions
such as targeted prebunking and assessing AI vulnerabilities. We introduce
ConspirED (CONSPIR Evaluation Dataset), which captures the cognitive traits of
conspiratorial ideation in multi-sentence excerpts (80--120 words) from online
conspiracy articles, annotated using the CONSPIR cognitive framework
(Lewandowsky and Cook, 2020). ConspirED is the first dataset of conspiratorial
content annotated for general cognitive traits. Using ConspirED, we (i) develop
computational models that identify conspiratorial traits and determine dominant
traits in text excerpts, and (ii) evaluate large language/reasoning model
(LLM/LRM) robustness to conspiratorial inputs. We find that both are misaligned
by conspiratorial content, producing output that mirrors input reasoning
patterns, even when successfully deflecting comparable fact-checked
misinformation.

</details>


### [18] [Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark](https://arxiv.org/abs/2508.20511)
*Chihiro Taguchi,Seng Mai,Keita Kurabe,Yusuke Sakai,Georgina Agyei,Soudabeh Eslami,David Chiang*

Main category: cs.CL

TL;DR: 研究揭示FLORES+等多语言MT基准测试存在质量缺陷和文化偏见，其评估协议易受简单策略影响，且无法准确衡量MT模型在真实世界数据上的表现。论文呼吁构建更通用、文化中立的基准。


<details>
  <summary>Details</summary>
Motivation: FLORES+等广泛使用的多语言机器翻译基准测试虽然宣称质量高，但研究人员对其在真正多语言评估中的适用性存疑，特别是在某些特定语言和文化背景下。

Method: 1. 对四种语言（阿散蒂特威语、日语、景颇语和南阿塞拜疆语）的FLORES+数据进行人工评估，检查翻译质量和源语句的文化偏见及领域特异性。2. 演示简单启发式方法（如复制命名实体）如何获得可观的BLEU分数。3. 比较在高质量、自然语料上训练的MT模型在FLORES+和自建的领域相关评估集上的表现。

Result: 1. 人工评估显示，许多FLORES+翻译质量低于宣称标准，源语句常过于领域特定且文化偏向英语世界。2. 简单启发式方法即可获得不可忽略的BLEU分数，揭示评估协议漏洞。3. 在高质量自然数据上训练的MT模型在FLORES+上表现不佳，但在研究者自建的领域相关评估集上表现显著提升。

Conclusion: FLORES+等多语言MT基准未能有效反映真实翻译挑战。未来基准应采用领域通用、文化中立的源文本，并减少对命名实体的依赖，以更准确评估机器翻译系统的能力。

Abstract: Multilingual machine translation (MT) benchmarks play a central role in
evaluating the capabilities of modern MT systems. Among them, the FLORES+
benchmark is widely used, offering English-to-many translation data for over
200 languages, curated with strict quality control protocols. However, we study
data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani)
and uncover critical shortcomings in the benchmark's suitability for truly
multilingual evaluation. Human assessments reveal that many translations fall
below the claimed 90% quality standard, and the annotators report that source
sentences are often too domain-specific and culturally biased toward the
English-speaking world. We further demonstrate that simple heuristics, such as
copying named entities, can yield non-trivial BLEU scores, suggesting
vulnerabilities in the evaluation protocol. Notably, we show that MT models
trained on high-quality, naturalistic data perform poorly on FLORES+ while
achieving significant gains on our domain-relevant evaluation set. Based on
these findings, we advocate for multilingual MT benchmarks that use
domain-general and culturally neutral source texts rely less on named entities,
in order to better reflect real-world translation challenges.

</details>


### [19] [SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM](https://arxiv.org/abs/2508.20514)
*Pengjiang Li,Zaitian Wang,Xinhao Zhang,Ran Zhang,Lu Jiang,Pengfei Wang,Yuanchun Zhou*

Main category: cs.CL

TL;DR: SciTopic是一种由大型语言模型（LLMs）增强的科学主题发现方法，通过结合文本编码器、空间优化模块和对比学习，有效提高了科学文献主题识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有主题发现方法多依赖词嵌入，缺乏对科学出版物的全面理解，难以处理复杂高维文本关系。LLMs在文本理解方面的卓越能力启发了本研究，旨在克服现有方法的局限性，更准确地识别科学主题。

Method: 该方法名为SciTopic，首先构建一个文本编码器捕捉科学出版物（元数据、标题、摘要）的内容。其次，构建一个空间优化模块，集成基于熵的采样和LLM引导的三元组任务，增强对主题相关性和模糊实例间上下文复杂性的关注。然后，通过优化三元组的对比损失来微调文本编码器，迫使其更好地区分不同主题的实例。

Result: 在三个真实的科学出版物数据集上进行的广泛实验表明，SciTopic显著优于现有最先进（SOTA）的科学主题发现方法，使研究人员能够获得更深入、更快速的洞察。

Conclusion: SciTopic利用LLMs的强大理解能力，改进了科学主题识别，为研究人员提供了更有效的主题发现工具，有助于识别新兴趋势和探索新研究方向。

Abstract: Topic discovery in scientific literature provides valuable insights for
researchers to identify emerging trends and explore new avenues for
investigation, facilitating easier scientific information retrieval. Many
machine learning methods, particularly deep embedding techniques, have been
applied to discover research topics. However, most existing topic discovery
methods rely on word embedding to capture the semantics and lack a
comprehensive understanding of scientific publications, struggling with
complex, high-dimensional text relationships. Inspired by the exceptional
comprehension of textual information by large language models (LLMs), we
propose an advanced topic discovery method enhanced by LLMs to improve
scientific topic identification, namely SciTopic. Specifically, we first build
a textual encoder to capture the content from scientific publications,
including metadata, title, and abstract. Next, we construct a space
optimization module that integrates entropy-based sampling and triplet tasks
guided by LLMs, enhancing the focus on thematic relevance and contextual
intricacies between ambiguous instances. Then, we propose to fine-tune the
textual encoder based on the guidance from the LLMs by optimizing the
contrastive loss of the triplets, forcing the text encoder to better
discriminate instances of different topics. Finally, extensive experiments
conducted on three real-world datasets of scientific publications demonstrate
that SciTopic outperforms the state-of-the-art (SOTA) scientific topic
discovery methods, enabling researchers to gain deeper and faster insights.

</details>


### [20] [Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20532)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Salvador Lima-López,Eulàlia Farré-Maduell,Martin Krallinger,Natalia Loukachevitch,Vera Davydova,Elena Tutubalina,Georgios Paliouras*

Main category: cs.CL

TL;DR: 本文概述了CLEF 2024框架下第十二届BioASQ挑战赛，该挑战赛引入了两个新任务，吸引了37支团队的广泛参与，并展示了生物医学领域技术的持续进步。


<details>
  <summary>Details</summary>
Motivation: BioASQ挑战赛旨在促进大规模生物医学语义索引和问答领域的进展。

Method: 本届BioASQ挑战赛在原有任务（b和Synergy）基础上，新增了两个任务：MultiCardioNER（多语言心脏病学领域临床实体检测）和BIONNE（俄语和英语嵌套命名实体识别）。

Result: 共有37支参赛团队参与了挑战赛，针对四个不同共享任务提交了超过700份独立作品。大多数参与系统都取得了有竞争力的表现。

Conclusion: 挑战赛结果表明，生物医学语义索引和问答领域的最新技术正在持续进步。

Abstract: This is an overview of the twelfth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks b and Synergy, and two
new tasks: a) MultiCardioNER on the adaptation of clinical entity detection to
the cardiology domain in a multilingual setting, and b) BIONNE on nested NER in
Russian and English. In this edition of BioASQ, 37 competing teams participated
with more than 700 distinct submissions in total for the four different shared
tasks of the challenge. Similarly to previous editions, most of the
participating systems achieved competitive performance, suggesting the
continuous advancement of the state-of-the-art in the field.

</details>


### [21] [Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20554)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Martin Krallinger,Miguel Rodríguez-Ortega,Eduard Rodriguez-López,Natalia Loukachevitch,Andrey Sakhovskiy,Elena Tutubalina,Dimitris Dimitriadis,Grigorios Tsoumakas,George Giannakoulas,Alexandra Bekiaridou,Athanasios Samaras,Giorgio Maria Di Nunzio,Nicola Ferro,Stefano Marchesin,Marco Martinelli,Gianmaria Silvello,Georgios Paliouras*

Main category: cs.CL

TL;DR: 本文概述了CLEF 2025框架下第十三届BioASQ挑战赛。该挑战赛旨在推动大规模生物医学语义索引和问答领域的进展，包含现有任务及四个新任务（多语言临床摘要、嵌套命名实体链接、心脏病学临床编码、肠-脑相互作用信息提取）。83支团队提交了1000余份作品，显示了该领域最先进技术的持续发展。


<details>
  <summary>Details</summary>
Motivation: BioASQ作为一系列国际挑战赛，旨在促进大规模生物医学语义索引和问答领域的进步。

Method: BioASQ 2025挑战赛包括两个现有任务（b和Synergy）以及四个新任务：多语言临床摘要 (MultiClinSum)、俄语和英语嵌套命名实体链接 (BioNNE-L)、心脏病学临床编码 (ELCardioCC) 和肠-脑相互作用信息提取 (GutBrainIE)。

Result: 本届BioASQ挑战赛共有83支参赛队伍，针对六个不同的共享任务提交了超过1000份独立作品。多个参赛系统取得了有竞争力的表现。

Conclusion: 挑战赛结果表明，生物医学语义索引和问答领域的最新技术持续取得进展。

Abstract: This is an overview of the thirteenth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks, b and Synergy, and four
new tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task
BioNNE-L on nested named entity linking in Russian and English. c) Task
ELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain
interplay information extraction. In this edition of BioASQ, 83 competing teams
participated with more than 1000 distinct submissions in total for the six
different shared tasks of the challenge. Similar to previous editions, several
participating systems achieved competitive performance, indicating the
continuous advancement of the state-of-the-art in the field.

</details>


### [22] [Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data](https://arxiv.org/abs/2508.20557)
*Jiahao Xiao,Jiangming Liu*

Main category: cs.CL

TL;DR: 针对联邦学习中多领域非独立同分布(non-IID)数据挑战，本文提出了一套新的多领域non-IID场景和基准框架，并设计了自适应联邦蒸馏(AdaFD)框架，在同构和异构设置下均取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习中的non-IID场景主要关注标签（输出）多样性，而忽略了在自然语言处理中至关重要的语言领域（输入）多样性，这使得现有方法无法充分解决真实世界中多领域non-IID数据的挑战。

Method: 本文引入了一套全面的多领域non-IID场景，并提出了一个包含多样化数据的统一基准框架。在此基础上，提出了一种自适应联邦蒸馏（AdaFD）框架，旨在应对同构和异构设置下的多领域non-IID挑战。

Result: 实验结果表明，所提出的AdaFD模型能够有效捕捉本地客户端数据的多样性，并相较于现有工作取得了更好的性能。

Conclusion: 本文通过引入多领域non-IID场景和基准，并提出AdaFD框架，有效地解决了联邦学习在真实环境中面临的多领域non-IID数据挑战，提高了模型在多样化数据环境下的性能。

Abstract: The widespread success of pre-trained language models has established a new
training paradigm, where a global PLM is fine-tuned using task-specific data
from local clients. The local data are highly different from each other and can
not capture the global distribution of the whole data in real world. To address
the challenges of non-IID data in real environments, privacy-preserving
federated distillation has been proposed and highly investigated. However,
previous experimental non-IID scenarios are primarily identified with the label
(output) diversity, without considering the diversity of language domains
(input) that is crucial in natural language processing. In this paper, we
introduce a comprehensive set of multi-domain non-IID scenarios and propose a
unified benchmarking framework that includes diverse data. The benchmark can be
used to evaluate the federated learning framework in a real environment. To
this end, we propose an Adaptive Federated Distillation (AdaFD) framework
designed to address multi-domain non-IID challenges in both homogeneous and
heterogeneous settings. Experimental results demonstrate that our models
capture the diversity of local clients and achieve better performance compared
to the existing works. The code for this paper is available at:
https://github.com/jiahaoxiao1228/AdaFD.

</details>


### [23] [Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search](https://arxiv.org/abs/2508.20559)
*Zeyu Xiong,Yixuan Nan,Li Gao,Hengzhu Tang,Shuaiqiang Wang,Junfeng Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的生成模型框架，用于解决工业网络搜索中的实时查询驱动文本摘要（QDTS）问题，克服了传统抽取方法的局限性，实现了最先进的性能和高效的部署。


<details>
  <summary>Details</summary>
Motivation: 工业应用中传统的抽取式QDTS模型存在两个主要局限：多阶段流水线可能导致累积信息丢失和架构瓶颈；以及缺乏对用户查询和文档的足够语义理解，尤其是在处理复杂搜索意图时。

Method: 本研究提出了一种将生成模型应用于工业网络搜索中实时QDTS的新颖框架。该方法整合了大型模型蒸馏、监督微调、直接偏好优化和前瞻解码技术，将一个仅有0.1B参数的轻量级模型转化为领域专业化的QDTS专家。

Result: 在多个行业相关指标上，所提出的模型超越了生产基线，并达到了新的最先进水平。此外，它展示了出色的部署效率，仅需334块NVIDIA L20 GPU即可在55毫秒的平均查询延迟下处理每秒约50,000个查询。

Conclusion: 通过结合模型蒸馏、微调、优化和解码的生成模型框架，能够有效解决传统方法在实时工业QDTS中的局限性，从而在性能和实际部署效率方面取得显著提升。

Abstract: In the dynamic landscape of large-scale web search, Query-Driven Text
Summarization (QDTS) aims to generate concise and informative summaries from
textual documents based on a given query, which is essential for improving user
engagement and facilitating rapid decision-making. Traditional extractive
summarization models, based primarily on ranking candidate summary segments,
have been the dominant approach in industrial applications. However, these
approaches suffer from two key limitations: 1) The multi-stage pipeline often
introduces cumulative information loss and architectural bottlenecks due to its
weakest component; 2) Traditional models lack sufficient semantic understanding
of both user queries and documents, particularly when dealing with complex
search intents. In this study, we propose a novel framework to pioneer the
application of generative models to address real-time QDTS in industrial web
search. Our approach integrates large model distillation, supervised
fine-tuning, direct preference optimization, and lookahead decoding to
transform a lightweight model with only 0.1B parameters into a
domain-specialized QDTS expert. Evaluated on multiple industry-relevant
metrics, our model outperforms the production baseline and achieves a new state
of the art. Furthermore, it demonstrates excellent deployment efficiency,
requiring only 334 NVIDIA L20 GPUs to handle \textasciitilde50,000 queries per
second under 55~ms average latency per query.

</details>


### [24] [KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling](https://arxiv.org/abs/2508.20567)
*Yangfan Wang,Jie Liu,Chen Tang,Lian Yan,Jingchi Jiang*

Main category: cs.CL

TL;DR: 针对多跳问答中数据稀疏问题，本文提出知识组合采样（KCS）框架，通过采样多样化的知识组合来扩大多跳问题的多样性，并提高问答准确性。


<details>
  <summary>Details</summary>
Motivation: 多跳问答面临数据稀疏的严峻挑战，导致语言模型容易学习到虚假模式。现有问题生成方法通常只生成简单问题，并且忽视了文档内关键知识的整合。

Method: 本文引入知识组合采样（KCS）框架。KCS将知识组合选择建模为句子级条件预测任务，利用概率对比损失来预测下一个最相关的知识片段，并在推理阶段采用随机解码策略以平衡准确性和多样性。

Result: 与现有基线相比，KCS将知识组合选择的整体准确率提升了3.9%。将其应用于数据增强时，在HotpotQA和2WikiMultihopQA数据集上均取得了性能提升。

Conclusion: KCS是一个有效的框架，它通过采样多样化的知识组合显著增加了生成多跳问题的多样性，从而提升了多跳问答系统的性能。

Abstract: Multi-hop question answering faces substantial challenges due to data
sparsity, which increases the likelihood of language models learning spurious
patterns. To address this issue, prior research has focused on diversifying
question generation through content planning and varied expression. However,
these approaches often emphasize generating simple questions and neglect the
integration of essential knowledge, such as relevant sentences within
documents. This paper introduces the Knowledge Composition Sampling (KCS), an
innovative framework designed to expand the diversity of generated multi-hop
questions by sampling varied knowledge compositions within a given context. KCS
models the knowledge composition selection as a sentence-level conditional
prediction task and utilizes a probabilistic contrastive loss to predict the
next most relevant piece of knowledge. During inference, we employ a stochastic
decoding strategy to effectively balance accuracy and diversity. Compared to
competitive baselines, our KCS improves the overall accuracy of knowledge
composition selection by 3.9%, and its application for data augmentation yields
improvements on HotpotQA and 2WikiMultihopQA datasets. Our code is available
at: https://github.com/yangfanww/kcs.

</details>


### [25] [A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models](https://arxiv.org/abs/2508.20583)
*Soham Petkar,Hari Aakash K,Anirudh Vempati,Akshit Sinha,Ponnurangam Kumarauguru,Chirag Agarwal*

Main category: cs.CL

TL;DR: 现有图语言模型（GLM）基准不足以评估多模态推理能力。本文提出CLEGR基准，并发现现有GLM在结构推理任务中表现不佳，且其图-语言集成并非必需，质疑了当前GLM的架构必要性。


<details>
  <summary>Details</summary>
Motivation: GLM旨在结合GNN的结构推理和LLM的语义理解，但现有评估基准（主要为节点分类数据集）无法充分评估多模态推理，因其仅凭单模态信息即可取得良好性能，表明它们并未真正要求图-语言整合。因此，需要填补这一评估空白。

Method: 通过分析证明了现有GLM评估基准的不足。提出CLEGR（组合式语言-图推理）基准，该基准通过合成图生成管道和需要结构与文本语义联合推理的问题，来评估多模态推理。随后，使用CLEGR对代表性GLM架构进行了全面评估。

Result: 研究发现，现有基准仅凭单模态信息即可获得高分，无需图-语言集成。在CLEGR基准上，软提示LLM基线与集成完整GNN骨干的GLM性能相当。GLM在需要结构推理的任务中表现出显著的性能下降。

Conclusion: 当前GLM的图结构整合在架构上的必要性受到质疑。现有GLM的图推理能力存在局限性。这些发现为推动社区发展明确的图结构与语言多模态推理奠定了基础。

Abstract: Developments in Graph-Language Models (GLMs) aim to integrate the structural
reasoning capabilities of Graph Neural Networks (GNNs) with the semantic
understanding of Large Language Models (LLMs). However, we demonstrate that
current evaluation benchmarks for GLMs, which are primarily repurposed
node-level classification datasets, are insufficient to assess multimodal
reasoning. Our analysis reveals that strong performance on these benchmarks is
achievable using unimodal information alone, suggesting that they do not
necessitate graph-language integration. To address this evaluation gap, we
introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed
to evaluate multimodal reasoning at various complexity levels. Our benchmark
employs a synthetic graph generation pipeline paired with questions that
require joint reasoning over structure and textual semantics. We perform a
thorough evaluation of representative GLM architectures and find that
soft-prompted LLM baselines perform on par with GLMs that incorporate a full
GNN backbone. This result calls into question the architectural necessity of
incorporating graph structure into LLMs. We further show that GLMs exhibit
significant performance degradation in tasks that require structural reasoning.
These findings highlight limitations in the graph reasoning capabilities of
current GLMs and provide a foundation for advancing the community toward
explicit multimodal reasoning involving graph structure and language.

</details>


### [26] [Generative Annotation for ASR Named Entity Correction](https://arxiv.org/abs/2508.20700)
*Yuanchang Luo,Daimeng Wei,Shaojun Li,Hengchao Shang,Jiaxin Guo,Zongyao Li,Zhanglin Wu,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Hao Yang*

Main category: cs.CL

TL;DR: 提出一种新颖的命名实体纠错(NEC)方法，通过利用语音特征和生成式模型来识别和替换ASR转录本中的实体错误，尤其在错误转录词与真实实体词形差异大时表现优异，显著提升了实体准确性。


<details>
  <summary>Details</summary>
Motivation: 端到端自动语音识别(ASR)系统在转录领域特定命名实体时常出现错误，导致下游任务失败。现有的快速轻量级命名实体纠错模型主要依赖语音级编辑距离，但在错误转录词与真实实体词形差异显著时，难以定位错误，限制了其应用。

Method: 本文提出一种利用语音特征检索候选实体的命名实体纠错方法。在此基础上，创新性地设计了一种生成式方法，用于标注ASR转录本中的实体错误，并用正确实体进行替换。该方法旨在有效处理词形差异大的场景。

Result: 通过在开源和自建测试集上的实验，结果表明所提出的命名实体纠错方法能显著提高实体准确性。

Conclusion: 该研究提出了一种新颖的NEC方法，通过结合语音特征和生成式设计，有效解决了ASR转录本中命名实体的错误，特别是在词形差异大的情况下，从而显著提升了实体准确性。

Abstract: End-to-end automatic speech recognition systems often fail to transcribe
domain-specific named entities, causing catastrophic failures in downstream
tasks. Numerous fast and lightweight named entity correction (NEC) models have
been proposed in recent years. These models, mainly leveraging phonetic-level
edit distance algorithms, have shown impressive performances. However, when the
forms of the wrongly-transcribed words(s) and the ground-truth entity are
significantly different, these methods often fail to locate the wrongly
transcribed words in hypothesis, thus limiting their usage. We propose a novel
NEC method that utilizes speech sound features to retrieve candidate entities.
With speech sound features and candidate entities, we inovatively design a
generative method to annotate entity errors in ASR transcripts and replace the
text with correct entities. This method is effective in scenarios of word form
difference. We test our method using open-source and self-constructed test
sets. The results demonstrate that our NEC method can bring significant
improvement to entity accuracy. We will open source our self-constructed test
set and training data.

</details>


### [27] [Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning](https://arxiv.org/abs/2508.20712)
*Nelson Filipe Costa,Leila Kosseim*

Main category: cs.CL

TL;DR: 本文提出首个多语言多标签隐式篇章关系识别（IDRR）模型HArch，利用篇章语义的层级依赖性，在多语言环境下优于大型语言模型，并在DiscoGeM 1.0上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 旨在开发首个针对隐式篇章关系识别（IDRR）的多语言、多标签分类模型，以处理篇章语义的复杂性。

Method: 提出HArch模型，该模型利用篇章语义间的层级依赖性，预测PDTB 3.0框架中所有三个语义层级的概率分布。模型采用预训练编码器（如RoBERTa、XLM-RoBERTa）并进行微调。在DiscoGeM 2.0和DiscoGeM 1.0语料库上进行评估，并与GPT-4o和Llama-4-Maverick等大型语言模型进行少样本提示比较。

Result: RoBERTa-HArch在英语环境下表现最佳，而XLM-RoBERTa-HArch在多语言环境下性能最优。微调后的模型在所有语言配置下的少样本提示中均持续优于GPT-4o和Llama-4-Maverick。在DiscoGeM 1.0语料库上取得了SOTA结果。

Conclusion: HArch模型通过其层级方法，有效识别隐式篇章关系，尤其是在多语言和多标签背景下。研究强调了在IDRR任务中，任务特定微调相较于大型语言模型提示的优势。

Abstract: This paper introduces the first multi-lingual and multi-label classification
model for implicit discourse relation recognition (IDRR). Our model, HArch, is
evaluated on the recently released DiscoGeM 2.0 corpus and leverages
hierarchical dependencies between discourse senses to predict probability
distributions across all three sense levels in the PDTB 3.0 framework. We
compare several pre-trained encoder backbones and find that RoBERTa-HArch
achieves the best performance in English, while XLM-RoBERTa-HArch performs best
in the multi-lingual setting. In addition, we compare our fine-tuned models
against GPT-4o and Llama-4-Maverick using few-shot prompting across all
language configurations. Our results show that our fine-tuned models
consistently outperform these LLMs, highlighting the advantages of
task-specific fine-tuning over prompting in IDRR. Finally, we report SOTA
results on the DiscoGeM 1.0 corpus, further validating the effectiveness of our
hierarchical approach.

</details>


### [28] [Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models](https://arxiv.org/abs/2508.20718)
*Ruiyi Yan,Yugo Murawaki*

Main category: cs.CL

TL;DR: 本研究关注大型语言模型（LLM）背景下文本隐写术和水印技术中令牌化不一致性（TI）问题，提出分步验证和事后回滚方法消除TI，显著提升了隐写术的流畅性、不可感知性和反隐写分析能力，以及水印的检测性和抗攻击鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型提升了文本生成能力，一方面改善了文本隐写术质量，另一方面也凸显了水印技术作为防恶意滥用安全机制的重要性。然而，在隐写术和水印技术中，发送方与接收方之间的令牌化不一致性（TI）问题会损害系统的鲁棒性，因此需要对其进行研究和解决。

Method: 研究首先揭示导致TI的“问题令牌”具有不频繁和临时性两个关键特征。基于这些发现，本研究提出了两种专门用于消除TI的解决方案：针对隐写术的分步验证方法（stepwise verification method）和针对水印技术的事后回滚方法（post-hoc rollback method）。

Result: 实验结果表明：(1) 在隐写术中，直接解决TI问题相较于传统消歧方法，显著提升了文本的流畅性、不可感知性以及反隐写分析能力；(2) 在水印技术中，解决TI问题增强了水印的检测性及其对抗攻击的鲁棒性。

Conclusion: 本研究成功识别并解决了大型语言模型驱动的文本隐写术和水印技术中的令牌化不一致性（TI）问题。通过提出针对性的解决方案，有效消除了TI，从而全面提升了隐写文本的质量和水印的有效性及鲁棒性。

Abstract: Large language models have significantly enhanced the capacities and
efficiency of text generation. On the one hand, they have improved the quality
of text-based steganography. On the other hand, they have also underscored the
importance of watermarking as a safeguard against malicious misuse. In this
study, we focus on tokenization inconsistency (TI) between Alice and Bob in
steganography and watermarking, where TI can undermine robustness. Our
investigation reveals that the problematic tokens responsible for TI exhibit
two key characteristics: infrequency and temporariness. Based on these
findings, we propose two tailored solutions for TI elimination: a stepwise
verification method for steganography and a post-hoc rollback method for
watermarking. Experiments show that (1) compared to traditional disambiguation
methods in steganography, directly addressing TI leads to improvements in
fluency, imperceptibility, and anti-steganalysis capacity; (2) for
watermarking, addressing TI enhances detectability and robustness against
attacks.

</details>


### [29] [rStar2-Agent: Agentic Reasoning Technical Report](https://arxiv.org/abs/2508.20722)
*Ning Shang,Yifei Liu,Yi Zhu,Li Lyna Zhang,Weijiang Xu,Xinyu Guan,Buze Zhang,Bingcheng Dong,Xudong Zhou,Bowen Zhang,Ying Xin,Ziming Miao,Scarlett Li,Fan Yang,Mao Yang*

Main category: cs.CL

TL;DR: 本文介绍了rStar2-Agent，一个14B的数学推理模型，通过智能体强化学习训练，在利用Python工具和代码执行反馈时，展现出高级认知行为，并实现了前沿性能。


<details>
  <summary>Details</summary>
Motivation: 现有长CoT方法不足以展现高级认知行为；需要模型能在使用编程工具时进行仔细思考、验证和优化中间步骤，以解决复杂问题。

Method: 引入rStar2-Agent，一个14B数学推理模型。采用智能体强化学习(agentic RL)进行训练，并结合三大创新：(i) 高效RL基础设施，支持高吞吐量Python代码执行并降低rollout成本；(ii) GRPO-RoC智能体RL算法，采用“在正确时重采样”策略应对编码环境噪声；(iii) 高效智能体训练方案，从非推理SFT开始，逐步通过多阶段RL获得高级认知能力。

Result: rStar2-Agent在仅510个RL步骤内，一周内将一个预训练的14B模型提升至SOTA水平。在AIME24上平均pass@1分数为80.6%，AIME25为69.8%，显著超越了DeepSeek-R1 (671B)，且响应更短。此外，模型在对齐、科学推理和智能体工具使用任务上也展现出强大的泛化能力。

Conclusion: rStar2-Agent通过创新的智能体强化学习方法，成功为14B数学推理模型赋予了前沿的认知行为和工具使用能力，不仅在数学推理任务上取得了SOTA成果，而且展示了良好的泛化性。

Abstract: We introduce rStar2-Agent, a 14B math reasoning model trained with agentic
reinforcement learning to achieve frontier-level performance. Beyond current
long CoT, the model demonstrates advanced cognitive behaviors, such as thinking
carefully before using Python coding tools and reflecting on code execution
feedback to autonomously explore, verify, and refine intermediate steps in
complex problem-solving. This capability is enabled through three key
innovations that makes agentic RL effective at scale: (i) an efficient RL
infrastructure with a reliable Python code environment that supports
high-throughput execution and mitigates the high rollout costs, enabling
training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic
RL algorithm with a Resample-on-Correct rollout strategy that addresses the
inherent environment noises from coding tools, allowing the model to reason
more effectively in a code environment; (iii) An efficient agent training
recipe that starts with non-reasoning SFT and progresses through multi-RL
stages, yielding advanced cognitive abilities with minimal compute cost. To
this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in
only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on
AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly
shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates
strong generalization to alignment, scientific reasoning, and agentic tool-use
tasks. Code and training recipes are available at
https://github.com/microsoft/rStar.

</details>


### [30] [Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees](https://arxiv.org/abs/2508.20736)
*Stephen Meisenbacher,Maulik Chevli,Florian Matthes*

Main category: cs.CL

TL;DR: DP-ST方法在局部差分隐私下，通过语义三元组和LLM后处理实现邻域感知文档生成，即便在低隐私预算下也能兼顾文本连贯性、隐私与效用。


<details>
  <summary>Details</summary>
Motivation: 现有局部差分隐私（Local DP）在文本隐私保护中要求高隐私预算（ε值）才能保持合理效果，导致在低ε值下难以生成连贯且实用的文本。

Method: 提出DP-ST，该方法利用语义三元组进行邻域感知的私有文档生成，采用分而治之的范式，将差分隐私概念限定于私有化邻域，并结合大型语言模型（LLM）进行后处理。

Result: 1. 验证了将DP概念限制在私有化邻域时，分而治之范式的有效性。2. 结合LLM后处理，DP-ST方法即使在较低ε值下也能生成连贯文本。3. 在保护隐私的同时，平衡了文本的实用性。

Conclusion: 在合理隐私预算下，文本连贯性对于实现平衡的私有化输出至关重要。

Abstract: Many works at the intersection of Differential Privacy (DP) in Natural
Language Processing aim to protect privacy by transforming texts under DP
guarantees. This can be performed in a variety of ways, from word perturbations
to full document rewriting, and most often under local DP. Here, an input text
must be made indistinguishable from any other potential text, within some bound
governed by the privacy parameter $\varepsilon$. Such a guarantee is quite
demanding, and recent works show that privatizing texts under local DP can only
be done reasonably under very high $\varepsilon$ values. Addressing this
challenge, we introduce DP-ST, which leverages semantic triples for
neighborhood-aware private document generation under local DP guarantees.
Through the evaluation of our method, we demonstrate the effectiveness of the
divide-and-conquer paradigm, particularly when limiting the DP notion (and
privacy guarantees) to that of a privatization neighborhood. When combined with
LLM post-processing, our method allows for coherent text generation even at
lower $\varepsilon$ values, while still balancing privacy and utility. These
findings highlight the importance of coherence in achieving balanced
privatization outputs at reasonable $\varepsilon$ levels.

</details>


### [31] [Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets](https://arxiv.org/abs/2508.20750)
*Vassiliy Cheremetiev,Quang Long Ho Ngo,Chau Ying Kot,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CL

TL;DR: 通过仅微调基于大型语言模型（LLM）的通用嵌入模型，本文在隐性仇恨言论检测任务中取得了最先进的性能，并在多个数据集上实现了显著的F1-macro分数提升。


<details>
  <summary>Details</summary>
Motivation: 隐性仇恨言论（IHS）因其通过细微线索、讽刺或隐语传递偏见，不含明确贬损词语，因此难以检测。

Method: 研究者通过仅微调Stella、Jasper、NV-Embed和E5等基于大型语言模型（LLMs）的通用嵌入模型来进行隐性仇恨言论检测，而无需额外知识或信息补充。

Result: 在F1-macro分数方面，相对于现有技术，内数据集评估提升高达1.10个百分点，跨数据集评估提升高达20.35个百分点，达到了最先进的性能。

Conclusion: 仅通过微调基于LLM的通用嵌入模型，就能在隐性仇恨言论检测任务中取得卓越且领先的性能。

Abstract: Implicit hate speech (IHS) is indirect language that conveys prejudice or
hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to
detect as it does not include explicit derogatory or inflammatory words. To
address this challenge, task-specific pipelines can be complemented with
external knowledge or additional information such as context, emotions and
sentiment data. In this paper, we show that, by solely fine-tuning recent
general-purpose embedding models based on large language models (LLMs), such as
Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance.
Experiments on multiple IHS datasets show up to 1.10 percentage points
improvements for in-dataset, and up to 20.35 percentage points improvements in
cross-dataset evaluation, in terms of F1-macro score.

</details>


### [32] [GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation](https://arxiv.org/abs/2508.20757)
*Yuanhao Ding,Esteban Garces Arias,Meimingwei Li,Julian Rodemann,Matthias Aßenmacher,Danlu Chen,Gaojuan Fan,Christian Heumann,Chongsheng Zhang*

Main category: cs.CL

TL;DR: GUARD是一种自适应解码方法，通过“Glocal”不确定性驱动框架有效平衡LLM开放式文本生成的连贯性与多样性，并显著提高生成速度。


<details>
  <summary>Details</summary>
Motivation: 开放式文本生成面临连贯性与多样性的平衡挑战，现有对比搜索解码策略受限于超参数依赖和高计算成本。

Method: 引入GUARD，一种自适应解码方法，采用“Glocal”不确定性驱动框架，结合全局熵估计和局部熵偏差。全局熵公式能缓解不确定性突变，并提供无偏性和一致性的理论保证。为降低计算开销，引入基于token计数的惩罚机制。

Result: GUARD在文本多样性和连贯性之间取得了良好平衡，并显著提高了生成速度。通过人工和LLM评估器在多个文本质量维度上验证了其卓越性能。

Conclusion: GUARD是一种有效的自适应解码方法，通过创新的“Glocal”不确定性驱动框架，成功解决了开放式文本生成中连贯性与多样性的平衡问题，同时提升了生成效率和质量。

Abstract: Open-ended text generation faces a critical challenge: balancing coherence
with diversity in LLM outputs. While contrastive search-based decoding
strategies have emerged to address this trade-off, their practical utility is
often limited by hyperparameter dependence and high computational costs. We
introduce GUARD, a self-adaptive decoding method that effectively balances
these competing objectives through a novel "Glocal" uncertainty-driven
framework. GUARD combines global entropy estimates with local entropy
deviations to integrate both long-term and short-term uncertainty signals. We
demonstrate that our proposed global entropy formulation effectively mitigates
abrupt variations in uncertainty, such as sudden overconfidence or high entropy
spikes, and provides theoretical guarantees of unbiasedness and consistency. To
reduce computational overhead, we incorporate a simple yet effective
token-count-based penalty into GUARD. Experimental results demonstrate that
GUARD achieves a good balance between text diversity and coherence, while
exhibiting substantial improvements in generation speed. In a more nuanced
comparison study across different dimensions of text quality, both human and
LLM evaluators validated its remarkable performance. Our code is available at
https://github.com/YecanLee/GUARD.

</details>


### [33] [Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions](https://arxiv.org/abs/2508.20764)
*Xiaoyi Wang,Jiwei Zhang,Guangtao Zhang,Honglei Guo*

Main category: cs.CL

TL;DR: 本研究首次对比分析了真实与LLM生成的CBT对话中的情绪弧线，发现LLM生成的数据在情绪变异性、情感表达和反应模式上与真实对话存在显著差异，尤其是在来访者情绪方面。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）生成的治疗对话日益用于心理健康NLP领域，以模拟咨询场景、训练模型和补充有限的真实数据。然而，这些合成对话能否捕捉到真实治疗中微妙的情绪动态尚不明确。

Method: 研究采用言语情绪动态框架（Utterance Emotion Dynamics）来分析效价、唤醒和支配维度上的细粒度情感轨迹。分析范围包括完整对话和个体说话者角色（咨询师和来访者）。数据来源包括转录自公共视频的真实会话和CACTUS数据集中的合成对话。此外，研究还引入了RealCBT数据集。

Result: 研究发现，尽管合成对话流畅且结构连贯，但它们在关键情绪属性上与真实对话存在差异：真实会话表现出更大的情绪变异性、更多情感丰富的语言以及更真实的反应和调节模式。此外，真实和合成说话者之间的情绪弧线相似性较低，尤其是在来访者方面。

Conclusion: 这些发现强调了当前LLM生成的治疗数据的局限性，并突出了情感保真度在心理健康应用中的重要性。研究引入了RealCBT数据集，以支持该领域的未来研究。

Abstract: Synthetic therapy dialogues generated by large language models (LLMs) are
increasingly used in mental health NLP to simulate counseling scenarios, train
models, and supplement limited real-world data. However, it remains unclear
whether these synthetic conversations capture the nuanced emotional dynamics of
real therapy. In this work, we conduct the first comparative analysis of
emotional arcs between real and LLM-generated Cognitive Behavioral Therapy
dialogues. We adapt the Utterance Emotion Dynamics framework to analyze
fine-grained affective trajectories across valence, arousal, and dominance
dimensions. Our analysis spans both full dialogues and individual speaker roles
(counselor and client), using real sessions transcribed from public videos and
synthetic dialogues from the CACTUS dataset. We find that while synthetic
dialogues are fluent and structurally coherent, they diverge from real
conversations in key emotional properties: real sessions exhibit greater
emotional variability,more emotion-laden language, and more authentic patterns
of reactivity and regulation. Moreover, emotional arc similarity between real
and synthetic speakers is low, especially for clients. These findings
underscore the limitations of current LLM-generated therapy data and highlight
the importance of emotional fidelity in mental health applications. We
introduce RealCBT, a curated dataset of real CBT sessions, to support future
research in this space.

</details>


### [34] [Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection](https://arxiv.org/abs/2508.20766)
*Harethah Abu Shairah,Hasan Abed Al Kader Hammoud,George Turkiyyah,Bernard Ghanem*

Main category: cs.CL

TL;DR: 本文提出一种名为秩一安全注入（ROSI）的白盒方法，通过简单的、无需微调的秩一权重修改，将LLM的激活永久导向拒绝有害请求的子空间，从而显著提升模型安全性，同时保持性能，并能重新对齐“未审查”模型。


<details>
  <summary>Details</summary>
Motivation: LLM中的安全对齐机制常通过内部表示来拒绝有害请求，但现有研究表明这些机制可通过修改模型中的特定表示方向来绕过。因此，需要一种鲁棒且高效的方法来增强LLM的安全性，以对抗这种绕过。

Method: 提出秩一安全注入（ROSI），这是一种白盒方法，通过对所有残差流写入矩阵进行简单、无需微调的秩一权重修改来实现。所需的安全性方向可通过少量有害和无害指令对计算得出。ROSI通过将模型激活永久导向拒绝有害请求的子空间来放大模型的安全对齐。

Result: ROSI在Llama Guard 3评估中显著提高了安全拒绝率，同时在MMLU、HellaSwag和Arc等标准基准测试中保持了模型的实用性。此外，ROSI还能通过放大其潜在安全方向来重新对齐“未审查”模型，证明其作为有效“最后一英里”安全程序的价值。

Conclusion: 有针对性且可解释的权重引导（如ROSI）是一种廉价而有效的提升LLM安全性的机制。它能有效补充更耗资源的微调范式，为提高模型安全性提供了一种新的思路。

Abstract: Safety alignment in Large Language Models (LLMs) often involves mediating
internal representations to refuse harmful requests. Recent research has
demonstrated that these safety mechanisms can be bypassed by ablating or
removing specific representational directions within the model. In this paper,
we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box
method that amplifies a model's safety alignment by permanently steering its
activations toward the refusal-mediating subspace. ROSI operates as a simple,
fine-tuning-free rank-one weight modification applied to all residual stream
write matrices. The required safety direction can be computed from a small set
of harmful and harmless instruction pairs. We show that ROSI consistently
increases safety refusal rates - as evaluated by Llama Guard 3 - while
preserving the utility of the model on standard benchmarks such as MMLU,
HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align
'uncensored' models by amplifying their own latent safety directions,
demonstrating its utility as an effective last-mile safety procedure. Our
results suggest that targeted, interpretable weight steering is a cheap and
potent mechanism to improve LLM safety, complementing more resource-intensive
fine-tuning paradigms.

</details>


### [35] [Signs of Struggle: Spotting Cognitive Distortions across Language and Register](https://arxiv.org/abs/2508.20771)
*Abhishek Kuber,Enrico Liscio,Ruixuan Zhang,Caroline Figueroa,Pradeep K. Murukannaiah*

Main category: cs.CL

TL;DR: 本研究首次深入探讨了认知扭曲检测在跨语言和跨语域（如荷兰青少年论坛帖子）中的泛化能力，发现领域适应方法前景广阔。


<details>
  <summary>Details</summary>
Motivation: 青少年心理健康问题日益突出，需要自动化方法早期识别心理困扰。认知扭曲作为加剧心理困扰的非理性思维模式，其早期检测能实现及时、低成本的干预。现有研究主要关注英语临床数据，缺乏跨语言和跨语域的泛化研究。

Method: 本研究对荷兰青少年论坛帖子进行分析，首次深入探讨了认知扭曲检测在跨语言和跨语域情境下的泛化能力。

Result: 研究发现，语言和写作风格的变化会显著影响模型性能，但领域适应方法显示出最大的潜力。

Conclusion: 领域适应方法在应对跨语言和跨语域的认知扭曲检测挑战方面具有广阔的应用前景。

Abstract: Rising mental health issues among youth have increased interest in automated
approaches for detecting early signs of psychological distress in digital text.
One key focus is the identification of cognitive distortions, irrational
thought patterns that have a role in aggravating mental distress. Early
detection of these distortions may enable timely, low-cost interventions. While
prior work has focused on English clinical data, we present the first in-depth
study of cross-lingual and cross-register generalization of cognitive
distortion detection, analyzing forum posts written by Dutch adolescents. Our
findings show that while changes in language and writing style can
significantly affect model performance, domain adaptation methods show the most
promise.

</details>


### [36] [Exploring Machine Learning and Language Models for Multimodal Depression Detection](https://arxiv.org/abs/2508.20805)
*Javier Si Zhao Hong,Timothy Zoe Delaya,Sherwyn Chan Yin Kit,Pai Chet Ng,Xiaoxiao Miao*

Main category: cs.CL

TL;DR: 本文提出了一种多模态人格感知抑郁症检测方法，利用机器学习和深度学习模型（XGBoost、Transformer、LLMs）分析音频、视频和文本特征，并比较了它们在抑郁症检测中的表现。


<details>
  <summary>Details</summary>
Motivation: 参与首届多模态人格感知抑郁症检测挑战赛，旨在通过机器学习和深度学习模型，探索有效捕获跨模态抑郁症相关信号的多模态表示策略，以进行心理健康预测。

Method: 研究采用机器学习和深度学习模型，具体包括XGBoost、基于Transformer的架构以及大型语言模型（LLMs）。这些模型被应用于处理音频、视频和文本特征。

Result: 研究结果揭示了各类模型在捕获跨模态抑郁症相关信号方面的优势和局限性。

Conclusion: 研究为心理健康预测中有效的多模态表示策略提供了深入见解。

Abstract: This paper presents our approach to the first Multimodal Personality-Aware
Depression Detection Challenge, focusing on multimodal depression detection
using machine learning and deep learning models. We explore and compare the
performance of XGBoost, transformer-based architectures, and large language
models (LLMs) on audio, video, and text features. Our results highlight the
strengths and limitations of each type of model in capturing depression-related
signals across modalities, offering insights into effective multimodal
representation strategies for mental health prediction.

</details>


### [37] [GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction](https://arxiv.org/abs/2508.20828)
*Jie Zhao,Wanting Ning,Yuxiao Fei,Yubo Feng,Lishuang Li*

Main category: cs.CL

TL;DR: 本文提出GDLLM，一种基于LLM的全局距离感知模型，通过结合图注意力网络和软推理机制，有效提升了事件时间关系抽取在少数类别上的性能及整体学习能力，并在公开数据集上达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有小型语言模型（SLMs）预训练知识受限，难以处理不平衡数据集中少数类别的事件时间关系。大型语言模型（LLMs）依赖手动提示词，易引入噪声，干扰长距离依赖判断。

Method: 提出GDLLM（Global Distance-aware modeling approach based on LLMs）。首先，利用图注意力网络（GAT）构建距离感知图结构，辅助LLMs捕获长距离依赖特征。其次，设计基于软推理的时间特征学习范式，增强短距离关系识别，并将LLMs生成的概率信息补充到多头注意力机制中。

Result: 该框架显著提升了少数关系类别的性能，并改善了整体学习能力。在TB-Dense和MATRES两个公开数据集上的实验表明，GDLLM实现了最先进（SOTA）的性能。

Conclusion: GDLLM通过有效捕获全局特征，显著增强了事件时间关系抽取中少数关系类别的性能，提高了整体学习能力，并在实验中取得了最先进的成果。

Abstract: In Natural Language Processing(NLP), Event Temporal Relation Extraction
(ETRE) is to recognize the temporal relations of two events. Prior studies have
noted the importance of language models for ETRE. However, the restricted
pre-trained knowledge of Small Language Models(SLMs) limits their capability to
handle minority class relations in imbalanced classification datasets. For
Large Language Models(LLMs), researchers adopt manually designed prompts or
instructions, which may introduce extra noise, leading to interference with the
model's judgment of the long-distance dependencies between events. To address
these issues, we propose GDLLM, a Global Distance-aware modeling approach based
on LLMs. We first present a distance-aware graph structure utilizing Graph
Attention Network(GAT) to assist the LLMs in capturing long-distance dependency
features. Additionally, we design a temporal feature learning paradigm based on
soft inference to augment the identification of relations with a short-distance
proximity band, which supplements the probabilistic information generated by
LLMs into the multi-head attention mechanism. Since the global feature can be
captured effectively, our framework substantially enhances the performance of
minority relation classes and improves the overall learning ability.
Experiments on two publicly available datasets, TB-Dense and MATRES,
demonstrate that our approach achieves state-of-the-art (SOTA) performance.

</details>


### [38] [MSRS: Evaluating Multi-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2508.20867)
*Rohan Phanse,Yijie Zhou,Kejian Shi,Wencai Zhang,Yixin Liu,Yilun Zhao,Arman Cohan*

Main category: cs.CL

TL;DR: 为解决RAG系统在多源信息整合和长篇回复生成方面的评估不足，本文提出了一个可扩展的基准构建框架，并创建了两个新基准（MSRS-Story和MSRS-Meet）。实验表明，生成质量高度依赖检索效果，且多源合成任务极具挑战，推理模型在此类任务中表现优于标准LLM。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统评估通常限于单一信息源、短形式或事实性问答场景，无法反映真实世界中需要整合分散在多个源中的信息并生成长篇回复的需求。因此，需要新的基准来挑战RAG系统在多源整合和复杂生成方面的能力。

Method: 研究构建了一个可扩展的框架来开发新的评估基准。基于该框架，创建了两个多源检索与合成基准：MSRS-Story（叙事合成）和MSRS-Meet（总结任务），均要求从大规模集合中检索信息。然后，使用多种RAG管道（包括稀疏和密集检索器与前沿大型语言模型）进行了广泛实验。

Result: 实验发现，生成质量与检索效率高度相关，且检索效率因任务而异。即使在理想检索条件下，多源合成任务仍然极具挑战性。此外，推理模型在多源合成这一特定步骤中显著优于标准大型语言模型。

Conclusion: RAG系统在处理多源信息整合和生成长篇回复时面临显著挑战，生成质量高度依赖于检索的有效性。未来的研究应关注提升检索组件识别多样化相关信号的能力，并开发更强大的推理模型来有效合成多源信息。

Abstract: Retrieval-augmented systems are typically evaluated in settings where
information required to answer the query can be found within a single source or
the answer is short-form or factoid-based. However, many real-world
applications demand the ability to integrate and summarize information
scattered across multiple sources, where no single source is sufficient to
respond to the user's question. In such settings, the retrieval component of a
RAG pipeline must recognize a variety of relevance signals, and the generation
component must connect and synthesize information across multiple sources. We
present a scalable framework for constructing evaluation benchmarks that
challenge RAG systems to integrate information across distinct sources and
generate long-form responses. Using our framework, we build two new benchmarks
on Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing
narrative synthesis and summarization tasks, respectively, that require
retrieval from large collections. Our extensive experiments with various RAG
pipelines -- including sparse and dense retrievers combined with frontier LLMs
-- reveal that generation quality is highly dependent on retrieval
effectiveness, which varies greatly by task. While multi-source synthesis
proves challenging even in an oracle retrieval setting, we find that reasoning
models significantly outperform standard LLMs at this distinct step.

</details>


### [39] [The Uneven Impact of Post-Training Quantization in Machine Translation](https://arxiv.org/abs/2508.20893)
*Benjamin Marie,Atsushi Fujita*

Main category: cs.CL

TL;DR: 首次大规模评估LLM量化对多语言机器翻译的影响，发现低资源语言在低位量化下性能显著下降，GGUF在2位精度下表现最一致。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的量化对于在资源受限硬件上部署至关重要，但其对多语言任务的影响尚未得到充分探索。

Method: 使用5个参数量从1.7B到70B的LLMs，对55种语言的机器翻译任务进行大规模的训练后量化（PTQ）评估。比较了AWQ、BitsAndBytes、GGUF和AutoRound四种量化技术，并量化了量化、解码超参数和校准语言之间的相互作用。

Result: 4位量化通常能保持高资源语言和大型模型的翻译质量，但对低资源和类型多样语言，尤其是在2位设置下，会出现显著退化。量化算法选择和模型大小共同决定鲁棒性，其中GGUF变体即使在2位精度下也能提供最一致的性能。语言匹配的校准主要在低位场景中提供益处。

Conclusion: 本研究为在量化约束下，特别是在低资源环境中部署多语言LLM进行机器翻译提供了可操作的见解。

Abstract: Quantization is essential for deploying large language models (LLMs) on
resource-constrained hardware, but its implications for multilingual tasks
remain underexplored. We conduct the first large-scale evaluation of
post-training quantization (PTQ) on machine translation across 55 languages
using five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that
while 4-bit quantization often preserves translation quality for high-resource
languages and large models, significant degradation occurs for low-resource and
typologically diverse languages, particularly in 2-bit settings. We compare
four quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing
that algorithm choice and model size jointly determine robustness. GGUF
variants provide the most consistent performance, even at 2-bit precision.
Additionally, we quantify the interactions between quantization, decoding
hyperparameters, and calibration languages, finding that language-matched
calibration offers benefits primarily in low-bit scenarios. Our findings offer
actionable insights for deploying multilingual LLMs for machine translation
under quantization constraints, especially in low-resource settings.

</details>


### [40] [SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement](https://arxiv.org/abs/2508.20916)
*Yuan Ge,Junxiang Zhang,Xiaoqian Liu,Bei Li,Xiangnan Ma,Chenglong Wang,Kaiyang Ye,Yangfan Du,Linfeng Zhang,Yuxin Huang,Tong Xiao,Zhengtao Yu,JingBo Zhu*

Main category: cs.CL

TL;DR: 本文提出了SageLM，一个端到端、多维度、可解释的语音大模型，用于全面评估S2S LLM。它能同时评估语义和声学维度，通过理由监督和合成数据集训练，实现了82.79%的人类评估一致率，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: Speech-to-Speech (S2S) LLM对于自然人机交互至关重要，但对其进行有效评估仍是一个基本挑战。

Method: 本文提出了SageLM，一个端到端、多维度、可解释的语音大模型评估工具。它通过以下方式实现：1) 联合评估语义和声学维度；2) 利用基于理由的监督增强可解释性和模型学习；3) 引入合成偏好数据集SpeechFeedback并采用两阶段训练范式以解决数据稀缺问题。

Result: SageLM与人类评估者的一致性达到82.79%，优于级联基线至少7.42%，并优于基于SLM的基线至少26.20%。

Conclusion: SageLM通过整合语义和声学评估、理由监督以及创新的数据训练策略，显著提升了S2S LLM的评估能力，实现了与人类评估高度一致的结果，为S2S LLM的全面评估提供了一个有效且可解释的解决方案。

Abstract: Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to
natural human-computer interaction, enabling end-to-end spoken dialogue
systems. However, evaluating these models remains a fundamental challenge. We
propose \texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech
LLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches
that disregard acoustic features, SageLM jointly assesses both semantic and
acoustic dimensions. Second, it leverages rationale-based supervision to
enhance explainability and guide model learning, achieving superior alignment
with evaluation outcomes compared to rule-based reinforcement learning methods.
Third, we introduce \textit{SpeechFeedback}, a synthetic preference dataset,
and employ a two-stage training paradigm to mitigate the scarcity of speech
preference data. Trained on both semantic and acoustic dimensions, SageLM
achieves an 82.79\% agreement rate with human evaluators, outperforming
cascaded and SLM-based baselines by at least 7.42\% and 26.20\%, respectively.

</details>


### [41] [How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $τ$-bench](https://arxiv.org/abs/2508.20931)
*Venkatesh Mishra,Amir Saeidi,Satyam Raj,Mutsumi Nakamura,Jayanth Srinivasa,Gaowen Liu,Ali Payani,Chitta Baral*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）作为工具调用代理在多轮对话中存在推理不一致和信息提取困难等问题。本文提出IRMA框架，通过自动重构输入并结合领域规则和工具建议，显著提升了代理的性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: LLMs作为自主代理在多轮对话（如$	au$-bench）中，难以保持推理一致性、遵循领域特定策略，并在长期工具调用和对话中准确提取信息。

Method: 首先，对对话轨迹中的常见错误进行全面的手动分析。其次，通过重新组织工具调用代理的输入进行实验，以改善代理决策。最后，提出了Input-Reformulation Multi-Agent (IRMA) 框架，该框架自动重构用户查询，并辅以相关的领域规则和工具建议，帮助工具调用代理集中处理。

Result: IRMA框架在整体pass^5分数上显著优于ReAct、Function Calling和Self-Reflection，分别提升了16.1%、12.7%和19.1%。

Conclusion: 研究结果表明，在动态环境中，IRMA框架相比其他方法具有卓越的可靠性和一致性。

Abstract: Recent advances in reasoning and planning capabilities of large language
models (LLMs) have enabled their potential as autonomous agents capable of tool
use in dynamic environments. However, in multi-turn conversational environments
like $\tau$-bench, these agents often struggle with consistent reasoning,
adherence to domain-specific policies, and extracting correct information over
a long horizon of tool-calls and conversation. To capture and mitigate these
failures, we conduct a comprehensive manual analysis of the common errors
occurring in the conversation trajectories. We then experiment with
reformulations of inputs to the tool-calling agent for improvement in agent
decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)
framework, which automatically reformulates user queries augmented with
relevant domain rules and tool suggestions for the tool-calling agent to focus
on. The results show that IRMA significantly outperforms ReAct, Function
Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in
overall pass^5 scores. These findings highlight the superior reliability and
consistency of IRMA compared to other methods in dynamic environments.

</details>


### [42] [STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment](https://arxiv.org/abs/2508.20944)
*Jiaqian Li,Qisheng Hu,Jing Li,Wenya Wang*

Main category: cs.CL

TL;DR: 针对结构化预测任务中上下文学习（ICL）样本选择忽视结构对齐的问题，本文提出一种新颖的两阶段样本选择策略。该策略通过结合结构感知监督的BERT检索器和语法信息增强模块，有效平衡了效率、泛化性和性能，并在多个语义解析基准测试中超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 上下文学习（ICL）的有效性高度依赖于样本选择质量。尤其在语义解析等结构化预测任务中，现有ICL选择策略常忽略结构对齐，导致性能不佳和泛化能力差。

Method: 提出一种新颖的两阶段样本选择策略。首先，使用结构感知监督微调BERT检索器，使其能选择语义相关且结构对齐的样本。其次，通过一个可插拔模块增强检索器，该模块能放大隐藏表示中的句法信息，且该模块与模型无关，开销极小，易于集成。

Result: 在涵盖三种语义解析任务的四个基准测试中，本文方法使用多种最新大型语言模型作为推理模型时，始终优于现有基线。

Conclusion: 本文提出的两阶段样本选择策略，通过有效结合语义相关性和结构对齐，显著提升了上下文学习在结构化预测任务中的性能和泛化能力，解决了现有方法在该领域面临的挑战。

Abstract: In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to
perform a wide range of tasks without task-specific fine-tuning. However, the
effectiveness of ICL heavily depends on the quality of exemplar selection. In
particular, for structured prediction tasks such as semantic parsing, existing
ICL selection strategies often overlook structural alignment, leading to
suboptimal performance and poor generalization. To address this issue, we
propose a novel two-stage exemplar selection strategy that achieves a strong
balance between efficiency, generalizability, and performance. First, we
fine-tune a BERT-based retriever using structure-aware supervision, guiding it
to select exemplars that are both semantically relevant and structurally
aligned. Then, we enhance the retriever with a plug-in module, which amplifies
syntactically meaningful information in the hidden representations. This
plug-in is model-agnostic, requires minimal overhead, and can be seamlessly
integrated into existing pipelines. Experiments on four benchmarks spanning
three semantic parsing tasks demonstrate that our method consistently
outperforms existing baselines with multiple recent LLMs as inference-time
models.

</details>


### [43] [ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents](https://arxiv.org/abs/2508.20973)
*Tianjian Liu,Fanqi Wan,Jiajian Guo,Xiaojun Quan*

Main category: cs.CL

TL;DR: 提出ProactiveEval统一框架，评估LLM的主动对话能力，通过分解任务、生成数据和跨领域实验，发现DeepSeek-R1和Claude-3.7-Sonnet在不同子任务中表现突出，并探讨了推理能力的影响。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注领域特定或任务导向的主动对话，导致评估碎片化，限制了LLM主动对话能力的全面探索。

Method: 提出ProactiveEval统一评估框架，将主动对话分解为目标规划和对话引导，建立了跨领域评估指标，并能自动生成多样化的评估数据。基于此框架构建了6个领域的328个评估环境。

Result: 在22种LLM的实验中，DeepSeek-R1在目标规划任务上表现出色，而Claude-3.7-Sonnet在对话引导任务上表现卓越。研究还探讨了推理能力对主动行为的影响。

Conclusion: ProactiveEval提供了一个评估LLM主动对话能力的统一框架，揭示了当前模型的性能差距，并为未来模型开发中推理能力与主动行为的关系提供了见解。

Abstract: Proactive dialogue has emerged as a critical and challenging research problem
in advancing large language models (LLMs). Existing works predominantly focus
on domain-specific or task-oriented scenarios, which leads to fragmented
evaluations and limits the comprehensive exploration of models' proactive
conversation abilities. In this work, we propose ProactiveEval, a unified
framework designed for evaluating proactive dialogue capabilities of LLMs. This
framework decomposes proactive dialogue into target planning and dialogue
guidance, establishing evaluation metrics across various domains. Moreover, it
also enables the automatic generation of diverse and challenging evaluation
data. Based on the proposed framework, we develop 328 evaluation environments
spanning 6 distinct domains. Through experiments with 22 different types of
LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional
performance on target planning and dialogue guidance tasks, respectively.
Finally, we investigate how reasoning capabilities influence proactive
behaviors and discuss their implications for future model development.

</details>


### [44] [Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution](https://arxiv.org/abs/2508.21004)
*Chen Chen,Yuchen Sun,Jiaxin Gao,Xueluan Gong,Qian Wang,Ziyao Wang,Yongsen Zheng,Kwok-Yan Lam*

Main category: cs.CL

TL;DR: 本文提出LETHE，一种通过内部和外部知识稀释机制消除大型语言模型（LLMs）后门行为的新方法，在防御多种先进后门攻击方面显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: LLMs虽性能卓越但易受后门攻击，当特定触发器激活时会产生有害或非预期输出。现有后门防御方案不够全面，无法有效应对窄范围触发器、仅检测机制、有限领域或模型编辑、多触发器、无触发器等高级攻击场景。

Method: LETHE通过结合内部和外部机制的知识稀释来消除LLM的后门行为：
1.  **内部机制**：使用轻量级数据集训练一个干净模型，然后将其与受感染模型合并，通过稀释后门在模型参数记忆中的影响来中和恶意行为。
2.  **外部机制**：在用户提示中融入良性且语义相关的证据，以分散LLM对后门特征的注意力。

Result: 实验结果表明，LETHE在分类和生成领域的5个常用LLMs上，优于8种最先进的防御基线，成功抵御了8种后门攻击。它将高级后门攻击的成功率降低高达98%，同时保持了模型实用性，并被证明具有成本效益和抵御自适应后门攻击的鲁棒性。

Conclusion: LETHE通过创新的知识稀释策略，为LLMs提供了一种高效、鲁棒且成本效益高的后门防御方案，显著提高了模型在面对各种先进后门攻击时的安全性，同时维持了模型的正常性能。

Abstract: Large language models (LLMs) have seen significant advancements, achieving
superior performance in various Natural Language Processing (NLP) tasks.
However, they remain vulnerable to backdoor attacks, where models behave
normally for standard queries but generate harmful responses or unintended
output when specific triggers are activated. Existing backdoor defenses either
lack comprehensiveness, focusing on narrow trigger settings, detection-only
mechanisms, and limited domains, or fail to withstand advanced scenarios like
model-editing-based, multi-trigger, and triggerless attacks. In this paper, we
present LETHE, a novel method to eliminate backdoor behaviors from LLMs through
knowledge dilution using both internal and external mechanisms. Internally,
LETHE leverages a lightweight dataset to train a clean model, which is then
merged with the backdoored model to neutralize malicious behaviors by diluting
the backdoor impact within the model's parametric memory. Externally, LETHE
incorporates benign and semantically relevant evidence into the prompt to
distract LLM's attention from backdoor features. Experimental results on
classification and generation domains across 5 widely used LLMs demonstrate
that LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor
attacks. LETHE reduces the attack success rate of advanced backdoor attacks by
up to 98% while maintaining model utility. Furthermore, LETHE has proven to be
cost-efficient and robust against adaptive backdoor attacks.

</details>


### [45] [An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs](https://arxiv.org/abs/2508.21024)
*Mathieu Bourdin,Anas Neumann,Thomas Paviot,Robert Pellerin,Samir Lamouri*

Main category: cs.CL

TL;DR: 本文提出了EASI-RAG，一个结构化、敏捷的方法，旨在帮助工业中小企业部署RAG系统。通过真实案例验证，EASI-RAG实现了快速实施、高用户采纳率和准确答案。


<details>
  <summary>Details</summary>
Motivation: 尽管检索增强生成（RAG）能有效缓解大型语言模型（LLM）的局限性，但中小企业因资源有限和缺乏自然语言处理（NLP）专业知识，在部署RAG工具时面临挑战。

Method: 本文引入了EASI-RAG（工业RAG企业应用支持），这是一种基于方法工程原则的结构化、敏捷方法，包含明确的角色、活动和技术，旨在促进工业中小企业中的RAG系统部署。该方法通过在一个环境检测实验室的真实案例研究进行了验证。

Result: 研究结果表明，EASI-RAG支持快速实施（不到一个月）、高用户采纳率、提供准确答案，并增强了底层数据的可靠性。即使团队没有RAG经验也能成功部署和迭代改进。

Conclusion: 该工作强调了RAG在工业中小企业中部署的潜力。未来的工作包括在不同用例中进行推广以及与微调模型的进一步集成。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to
mitigate the limitations of Large Language Models (LLMs), such as
hallucinations and outdated knowledge. However, deploying RAG-based tools in
Small and Medium Enterprises (SMEs) remains a challenge due to their limited
resources and lack of expertise in natural language processing (NLP). This
paper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a
structured, agile method designed to facilitate the deployment of RAG systems
in industrial SME contexts. EASI-RAG is based on method engineering principles
and comprises well-defined roles, activities, and techniques. The method was
validated through a real-world case study in an environmental testing
laboratory, where a RAG tool was implemented to answer operators queries using
data extracted from operational procedures. The system was deployed in under a
month by a team with no prior RAG experience and was later iteratively improved
based on user feedback. Results demonstrate that EASI-RAG supports fast
implementation, high user adoption, delivers accurate answers, and enhances the
reliability of underlying data. This work highlights the potential of RAG
deployment in industrial SMEs. Future works include the need for generalization
across diverse use cases and further integration with fine-tuned models.

</details>


### [46] [Re-Representation in Sentential Relation Extraction with Sequence Routing Algorithm](https://arxiv.org/abs/2508.21049)
*Ramazan Ali Bahrami,Ramin Yahyapour*

Main category: cs.CL

TL;DR: 本文提出使用胶囊网络中的动态路由进行句子级关系抽取，并在多个常用数据集上取得了SOTA性能。研究发现模型在Wikidata上表现不佳源于标签噪声，同时将更好的性能与更好的“重表征”能力相关联，指出模型在重表征方面优于基线。最终，将标签噪声和重表征能力都视为句子级关系抽取的重要挑战。


<details>
  <summary>Details</summary>
Motivation: 句子级关系抽取是NLP中的重要任务。本研究旨在提出一种新的方法，并探究其在不同数据集（尤其是在部分数据集上表现优异，在大型数据集Wikidata上表现不佳）上性能差异的原因，以深入理解影响关系抽取性能的关键因素并提出新的挑战。

Method: 提出使用胶囊网络（Capsules）中的动态路由机制进行句子级关系抽取。

Result: ['所提出的方法在Tacred, Tacredrev, Retacred和Conll04等通用句子级关系抽取数据集上超越了现有最佳（SOTA）性能。', '分析发现，在Wikidata数据集上性能较低的主要原因之一是其标签中存在噪声。', '研究表明，更好的性能与更好的“重表征”（re-representation）能力之间存在关联。', '观察到所提出的模型在重表征方面比基线模型表现更优。']

Conclusion: 除了远程监督关系抽取数据集中普遍存在的标签噪声问题，模型的重表征能力也被提出作为句子级关系抽取面临的一个重要挑战。这些发现为未来关系抽取任务的研究方向提供了启示。

Abstract: Sentential relation extraction (RE) is an important task in natural language
processing (NLP). In this paper we propose to do sentential RE with dynamic
routing in capsules. We first show that the proposed approach outperform state
of the art on common sentential relation extraction datasets Tacred, Tacredrev,
Retacred, and Conll04. We then investigate potential reasons for its good
performance on the mentioned datasets, and yet low performance on another
similar, yet larger sentential RE dataset, Wikidata. As such, we identify noise
in Wikidata labels as one of the reasons that can hinder performance.
Additionally, we show associativity of better performance with better
re-representation, a term from neuroscience referred to change of
representation in human brain to improve the match at comparison time. As
example, in the given analogous terms King:Queen::Man:Woman, at comparison
time, and as a result of re-representation, the similarity between related head
terms (King,Man), and tail terms (Queen,Woman) increases. As such, our
observation show that our proposed model can do re-representation better than
the vanilla model compared with. To that end, beside noise in the labels of the
distantly supervised RE datasets, we propose re-representation as a challenge
in sentential RE.

</details>


### [47] [Enabling Equitable Access to Trustworthy Financial Reasoning](https://arxiv.org/abs/2508.21051)
*William Jurayj,Nils Holzenberger,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本文提出了一种结合大语言模型（LLMs）和符号求解器的神经符号方法，用于自动化税务申报。该方法通过将文本规则翻译为逻辑程序并利用案例示例，显著提高了准确性并降低了税务错误成本，证明了其在提供可靠税务援助方面的潜力和经济可行性。


<details>
  <summary>Details</summary>
Motivation: 税务申报过程复杂、耗时（美国平均270美元和13小时），且错误可能导致高昂罚款。传统大型语言模型因精度和可审计性不足，不适用于此任务。因此，需要一种既能处理复杂推理，又能确保高精度和可审计性的自动化系统。

Method: 研究提出了一种将大语言模型（LLMs）与符号求解器集成的神经符号方法来计算税务义务。该系统在SARA数据集上进行评估，并引入了一种基于实际税务错误罚款的成本估算方法。此外，通过预先将纯文本规则翻译成形式逻辑程序，并结合智能检索的形式化案例表示示例，进一步优化了性能。

Result: 实验结果表明，该方法能够显著提高任务性能，并将部署成本降低到远低于现实世界平均水平。具体而言，结合文本规则翻译和智能检索示例，能大幅提升效率并减少错误成本。

Conclusion: 神经符号架构在提高可靠税务援助的公平可及性方面展现出巨大的前景和经济可行性。该研究为自动化税务申报提供了一种高精度、可审计且经济有效的解决方案。

Abstract: According to the United States Internal Revenue Service, ''the average
American spends $\$270$ and 13 hours filing their taxes''. Even beyond the
U.S., tax filing requires complex reasoning, combining application of
overlapping rules with numerical calculations. Because errors can incur costly
penalties, any automated system must deliver high accuracy and auditability,
making modern large language models (LLMs) poorly suited for this task. We
propose an approach that integrates LLMs with a symbolic solver to calculate
tax obligations. We evaluate variants of this system on the challenging
StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for
estimating the cost of deploying such a system based on real-world penalties
for tax errors. We further show how combining up-front translation of
plain-text rules into formal logic programs, combined with intelligently
retrieved exemplars for formal case representations, can dramatically improve
performance on this task and reduce costs to well below real-world averages.
Our results demonstrate the promise and economic feasibility of neuro-symbolic
architectures for increasing equitable access to reliable tax assistance.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [48] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: 本文提出CHAIR-DPO方法，通过利用CHAIR指标构建偏好数据，并使用DPO对多模态大语言模型（MLLMs）进行微调，有效减少了模型幻觉。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）尽管在许多任务中表现出色，但普遍存在幻觉问题，即生成与视觉输入不符的答案。

Method: 将幻觉问题视为对齐问题。与现有复杂方法不同，本文利用CHAIR指标区分生成答案中的“优胜者”（无幻觉）和“失败者”（有幻觉）选项，并使用直接偏好优化（DPO）对现有的MLLMs进行微调。

Result: 所提出的CHAIR-DPO方法在多个幻觉基准测试中有效减少了幻觉答案的数量，证明了基于CHAIR奖励的MLLM微调是有效的。

Conclusion: 通过利用CHAIR指标构建偏好数据并结合DPO进行微调，可以有效降低多模态大语言模型的幻觉倾向。

Abstract: Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [49] [SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization](https://arxiv.org/abs/2508.20182)
*Yang Su,Shunquan Tan,Jiwu Huang*

Main category: cs.CV

TL;DR: 论文提出一个将Stable Diffusion V3 (SD3) 整合到图像伪造定位的新框架，通过将伪造残差作为显式模态进行融合，实现了比现有SOTA方法更高的性能和强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 新一代多模态大模型（如Stable Diffusion）推动图像处理技术快速发展，对图像取证提出严峻挑战。现有图像伪造定位方法过度依赖耗时昂贵的人工标注数据，难以适应这些新兴技术。

Method: 本研究首次将SD的图像生成和强大的感知能力整合到图像取证框架中。理论证明SD的多模态架构可以根据伪造相关信息输出伪造定位结果。具体地，研究者利用Stable Diffusion V3 (SD3) 的多模态框架，将图像伪造残差（通过高通滤波器提取的高频信号）作为一种显式模态，并在训练期间将其融合到SD3的潜在空间中，以增强伪造定位性能。该方法完整保留了SD3提取的潜在特征，从而保留了输入图像丰富的语义信息。

Result: 实验结果表明，本框架在广泛使用的基准数据集上，性能比当前最先进的图像伪造定位模型提高了高达12%。此外，该模型在涉及真实世界文档伪造图像和自然场景伪造图像的取证任务中表现出强大的性能，即使这些数据在训练期间从未见过。

Conclusion: 本研究提出的基于SD3的伪造定位框架，通过创新性地将伪造残差作为新模态进行整合，显著提升了图像伪造定位的效率和准确性。其不仅在性能上超越了现有SOTA模型，更在处理未见过的真实世界伪造图像时展现出卓越的泛化能力。

Abstract: Driven by the new generation of multi-modal large models, such as Stable
Diffusion (SD), image manipulation technologies have advanced rapidly, posing
significant challenges to image forensics. However, existing image forgery
localization methods, which heavily rely on labor-intensive and costly
annotated data, are struggling to keep pace with these emerging image
manipulation technologies. To address these challenges, we are the first to
integrate both image generation and powerful perceptual capabilities of SD into
an image forensic framework, enabling more efficient and accurate forgery
localization. First, we theoretically show that the multi-modal architecture of
SD can be conditioned on forgery-related information, enabling the model to
inherently output forgery localization results. Then, building on this
foundation, we specifically leverage the multimodal framework of Stable
DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the
multi-modal processing capabilities of SD3 in the latent space by treating
image forgery residuals -- high-frequency signals extracted using specific
highpass filters -- as an explicit modality. This modality is fused into the
latent space during training to enhance forgery localization performance.
Notably, our method fully preserves the latent features extracted by SD3,
thereby retaining the rich semantic information of the input image.
Experimental results show that our framework achieves up to 12% improvements in
performance on widely used benchmarking datasets compared to current
state-of-the-art image forgery localization models. Encouragingly, the model
demonstrates strong performance on forensic tasks involving real-world document
forgery images and natural scene forging images, even when such data were
entirely unseen during training.

</details>


### [50] [Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study](https://arxiv.org/abs/2508.20188)
*Max Torop,Masih Eskandar,Nicholas Kurtansky,Jinyang Liu,Jochen Weber,Octavia Camps,Veronica Rotemberg,Jennifer Dy,Kivanc Kose*

Main category: cs.CV

TL;DR: 针对AI皮肤病诊断可解释性不足问题，本研究结合多模态大语言模型（MLLMs）和定量属性，通过微调使MLLM嵌入空间与图像中的定量属性关联，并通过图像检索案例研究进行评估，旨在提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 人工智能模型在皮肤病诊断中展现出巨大潜力，但其预测结果的可解释性不足，严重阻碍了在临床实践中的应用。

Method: 结合多模态大语言模型（MLLMs）和定量属性两种方法。通过微调MLLMs，使其能够从图像中预测定量属性值（如病变面积），从而将MLLM的嵌入空间与这些可解释的属性进行关联。利用SLICE-3D数据集，通过属性特定的内容图像检索案例研究来评估这种关联效果。

Result: 研究提供了MLLM嵌入空间可以与定量属性（通过微调预测其值）有效关联的证据。并通过属性特定的内容图像检索案例研究，在嵌入空间中验证了这种关联性。

Conclusion: 通过将多模态大语言模型与可解释的定量属性结合并进行关联，能够有效提高AI在皮肤病诊断中的可解释性，为模型在临床实践中的应用奠定基础。

Abstract: Artificial Intelligence models have demonstrated significant success in
diagnosing skin diseases, including cancer, showing the potential to assist
clinicians in their analysis. However, the interpretability of model
predictions must be significantly improved before they can be used in practice.
To this end, we explore the combination of two promising approaches: Multimodal
Large Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a
potential avenue for increased interpretability, providing reasoning for
diagnosis in natural language through an interactive format. Separately, a
number of quantitative attributes that are related to lesion appearance (e.g.,
lesion area) have recently been found predictive of malignancy with high
accuracy. Predictions grounded as a function of such concepts have the
potential for improved interpretability. We provide evidence that MLLM
embedding spaces can be grounded in such attributes, through fine-tuning to
predict their values from images. Concretely, we evaluate this grounding in the
embedding space through an attribute-specific content-based image retrieval
case study using the SLICE-3D dataset.

</details>


### [51] [Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels](https://arxiv.org/abs/2508.20193)
*Hossein Ahmadi,Banafsheh Saffari*

Main category: cs.CV

TL;DR: 提出一种统一的Vision Transformer (ViT) 框架用于自动调制识别(AMR)，通过整合监督、自监督和重建目标，实现在有限标签数据下优异的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有AMR方案过度依赖大量标注数据或复杂的多阶段训练流程，这限制了其实际应用中的可扩展性和泛化能力。

Method: 本文提出一个统一的Vision Transformer (ViT) 框架，该模型包含ViT编码器、轻量级卷积解码器和线性分类器，并整合了监督、自监督和重建目标。其中，重建分支将增强信号映射回原始信号，以促使编码器学习精细的I/Q结构。该策略在预训练阶段促进鲁棒且有区分度的特征学习，并在微调阶段利用部分标签监督实现有限标签下的有效分类。

Result: 在RML2018.01A数据集上，该方法在低标签环境下优于监督式CNN和ViT基线，仅用15-20%的标注数据即可达到接近ResNet的准确率，并在不同信噪比水平下保持强大性能。

Conclusion: 该框架为AMR提供了一种简单、通用且标签高效的解决方案。

Abstract: Automatic modulation recognition (AMR) is critical for cognitive radio,
spectrum monitoring, and secure wireless communication. However, existing
solutions often rely on large labeled datasets or multi-stage training
pipelines, which limit scalability and generalization in practice. We propose a
unified Vision Transformer (ViT) framework that integrates supervised,
self-supervised, and reconstruction objectives. The model combines a ViT
encoder, a lightweight convolutional decoder, and a linear classifier; the
reconstruction branch maps augmented signals back to their originals, anchoring
the encoder to fine-grained I/Q structure. This strategy promotes robust,
discriminative feature learning during pretraining, while partial label
supervision in fine-tuning enables effective classification with limited
labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and
ViT baselines in low-label regimes, approaches ResNet-level accuracy with only
15-20% labeled data, and maintains strong performance across varying SNR
levels. Overall, the framework provides a simple, generalizable, and
label-efficient solution for AMR.

</details>


### [52] [InfinityHuman: Towards Long-Term Audio-Driven Human](https://arxiv.org/abs/2508.20210)
*Xiaodi Li,Pan Xie,Yi Ren,Qijun Gan,Chen Zhang,Fangyuan Kong,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: InfinityHuman是一个粗到细的音频驱动人体动画框架，通过姿态引导细化和手部特定奖励机制，生成高分辨率、长时程、外观一致且手部动作自然逼真的视频，并达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动人体动画方法在生成高分辨率、长时程视频时面临挑战，尤其是在保持外观一致性、解决错误累积导致的身份漂移、色彩偏移和场景不稳定问题，以及建模自然逼真的手部动作方面表现不佳，导致明显失真和与音频不同步。

Method: 本文提出了InfinityHuman框架，采用粗到细的方法。首先生成音频同步的表示，然后使用姿态引导细化器逐步将其精炼成高分辨率、长时程视频。该细化器利用稳定的姿态序列和初始帧作为视觉锚点，以减少漂移并改善唇部同步。此外，引入了通过高质量手部运动数据训练的手部特定奖励机制，以提高语义准确性和手势真实感。

Result: 在EMTD和HDTF数据集上的实验表明，InfinityHuman在视频质量、身份保持、手部准确性和唇部同步方面均达到了最先进的性能。消融研究也进一步证实了每个模块的有效性。

Conclusion: InfinityHuman通过其独特的粗到细框架、姿态引导细化器和手部特定奖励机制，有效解决了音频驱动人体动画中生成高分辨率、长时程视频的现有挑战，显著提升了视频质量、一致性和手部动作的真实感，实现了领先的技术水平。

Abstract: Audio-driven human animation has attracted wide attention thanks to its
practical applications. However, critical challenges remain in generating
high-resolution, long-duration videos with consistent appearance and natural
hand motions. Existing methods extend videos using overlapping motion frames
but suffer from error accumulation, leading to identity drift, color shifts,
and scene instability. Additionally, hand movements are poorly modeled,
resulting in noticeable distortions and misalignment with the audio. In this
work, we propose InfinityHuman, a coarse-to-fine framework that first generates
audio-synchronized representations, then progressively refines them into
high-resolution, long-duration videos using a pose-guided refiner. Since pose
sequences are decoupled from appearance and resist temporal degradation, our
pose-guided refiner employs stable poses and the initial frame as a visual
anchor to reduce drift and improve lip synchronization. Moreover, to enhance
semantic accuracy and gesture realism, we introduce a hand-specific reward
mechanism trained with high-quality hand motion data. Experiments on the EMTD
and HDTF datasets show that InfinityHuman achieves state-of-the-art performance
in video quality, identity preservation, hand accuracy, and lip-sync. Ablation
studies further confirm the effectiveness of each module. Code will be made
public.

</details>


### [53] [Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos](https://arxiv.org/abs/2508.20221)
*Mert Cokelek,Halit Ozsoy,Nevrez Imamoglu,Cagri Ozcinar,Inci Ayhan,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: 本研究针对全景视频（ODV）的视听显著性预测，提出了一个新的数据集YT360-EyeTracking以及两个基于Vision Transformer的模型SalViT360和SalViT360-AV。实验结果表明所提模型显著优于现有方法，并强调了空间音频在准确预测用户注意力中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 当前针对360度视听显著性预测缺乏全面的数据集，并且现有方法未能充分利用全景视频中空间音频这一重要维度来预测视觉显著性。

Method: 研究构建了一个新的数据集YT360-EyeTracking，包含81个在不同视听条件下观察的全景视频。同时，提出了两个新颖的显著性预测模型：SalViT360，一个基于Vision Transformer的框架，结合了球形几何感知的时空注意力层；以及SalViT360-AV，在SalViT360基础上通过Transformer适配器进一步融入了音频输入。

Result: SalViT360和SalViT360-AV在包括YT360-EyeTracking在内的多个基准数据集上，显著优于现有方法，能更有效地预测360度场景中的观看者注意力。

Conclusion: 将空间音频线索整合到模型架构中，对于全景视频中的准确显著性预测至关重要。

Abstract: Omnidirectional videos (ODVs) are redefining viewer experiences in virtual
reality (VR) by offering an unprecedented full field-of-view (FOV). This study
extends the domain of saliency prediction to 360-degree environments,
addressing the complexities of spherical distortion and the integration of
spatial audio. Contextually, ODVs have transformed user experience by adding a
spatial audio dimension that aligns sound direction with the viewer's
perspective in spherical scenes. Motivated by the lack of comprehensive
datasets for 360-degree audio-visual saliency prediction, our study curates
YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying
audio-visual conditions. Our goal is to explore how to utilize audio-visual
cues to effectively predict visual saliency in 360-degree videos. Towards this
aim, we propose two novel saliency prediction models: SalViT360, a
vision-transformer-based framework for ODVs equipped with spherical
geometry-aware spatio-temporal attention layers, and SalViT360-AV, which
further incorporates transformer adapters conditioned on audio input. Our
results on a number of benchmark datasets, including our YT360-EyeTracking,
demonstrate that SalViT360 and SalViT360-AV significantly outperform existing
methods in predicting viewer attention in 360-degree scenes. Interpreting these
results, we suggest that integrating spatial audio cues in the model
architecture is crucial for accurate saliency prediction in omnidirectional
videos. Code and dataset will be available at
https://cyberiada.github.io/SalViT360.

</details>


### [54] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

TL;DR: 本文提出一种基于视觉-语言模型（VLM）的管道，用于在样本和数据集层面解释视觉模型的行为，以解决现有模型可解释性不足且对整体行为分析缺乏关注的问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型开发过度关注性能指标（如准确率、IoU），而忽视了可解释性；现有xAI方法多为逐样本解释，缺乏对模型整体行为的理解。理解模型在通用图像上的行为对于防止偏见和识别模型趋势模式至关重要。

Method: 利用视觉-语言模型（Vision-Language Models），提出一个管道（pipeline）来解释视觉模型在样本和数据集两个层面的行为。

Result: 所提出的管道能够以最小的努力发现模型的失败案例，并获得对视觉模型的深入见解。

Conclusion: 该管道通过将视觉模型开发与xAI分析相结合，旨在推进图像分析领域的发展。

Abstract: The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [55] [ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems](https://arxiv.org/abs/2508.20232)
*Mohamed Ohamouddou,Said Ohamouddou,Abdellatif El Afia,Rafik Lasri*

Main category: cs.CV

TL;DR: 本研究提出ATMS-KD框架，通过自适应温度和混合样本增强，将知识从大型教师模型迁移到轻量级CNN学生模型。该方法在资源受限的农业环境中，用于大马士革玫瑰成熟度分类，实现了高精度（紧凑模型达97.11%）和低延迟，显著优于直接训练和其他蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的农业环境中，需要开发轻量级卷积神经网络（CNN）模型，以适应实际部署需求。

Method: 本研究提出了ATMS-KD（自适应温度和混合样本知识蒸馏）框架。该框架结合自适应温度调度和混合样本数据增强技术，将知识从MobileNetV3 Large教师模型（5.7M参数）迁移到三种配置的轻量级残差CNN学生模型（紧凑型1.3M、标准型2.4M、增强型3.8M参数）。研究使用了摩洛哥农业田地采集的大马士革玫瑰图像数据集进行评估。

Result: ATMS-KD框架在玫瑰成熟度分类数据集上表现出显著优于直接训练方法的性能。所有学生模型在ATMS-KD下验证准确率均超过96.7%，而直接训练为95-96%。该框架优于其他十一种现有知识蒸馏方法，其中紧凑型模型实现了97.11%的准确率，比次优方法提高了1.60个百分点，同时保持了最低推理延迟（72.19毫秒）。所有配置的知识保留率均超过99%。

Conclusion: ATMS-KD框架能够有效且高效地将知识从大型教师模型迁移到轻量级学生模型，无论学生模型容量如何，均能实现卓越的性能和低推理延迟，使其非常适合资源受限的农业计算机视觉应用。

Abstract: This study proposes ATMS-KD (Adaptive Temperature and Mixed-Sample Knowledge
Distillation), a novel framework for developing lightweight CNN models suitable
for resource-constrained agricultural environments. The framework combines
adaptive temperature scheduling with mixed-sample augmentation to transfer
knowledge from a MobileNetV3 Large teacher model (5.7\,M parameters) to
lightweight residual CNN students. Three student configurations were evaluated:
Compact (1.3\,M parameters), Standard (2.4\,M parameters), and Enhanced (3.8\,M
parameters). The dataset used in this study consists of images of \textit{Rosa
damascena} (Damask rose) collected from agricultural fields in the Dades Oasis,
southeastern Morocco, providing a realistic benchmark for agricultural computer
vision applications under diverse environmental conditions. Experimental
evaluation on the Damascena rose maturity classification dataset demonstrated
significant improvements over direct training methods. All student models
achieved validation accuracies exceeding 96.7\% with ATMS-KD compared to
95--96\% with direct training. The framework outperformed eleven established
knowledge distillation methods, achieving 97.11\% accuracy with the compact
model -- a 1.60 percentage point improvement over the second-best approach
while maintaining the lowest inference latency of 72.19\,ms. Knowledge
retention rates exceeded 99\% for all configurations, demonstrating effective
knowledge transfer regardless of student model capacity.

</details>


### [56] [Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification](https://arxiv.org/abs/2508.20243)
*Mutahar Safdar,Gentry Wood,Max Zimmermann,Guy Lamouche,Priti Wanjara,Yaoyao Fiona Zhao*

Main category: cs.CV

TL;DR: 本研究提出一个利用定制混合视觉-语言表示（VLRs）的新框架，旨在解决先进材料（特别是增材制造异质结构）快速可靠鉴定中的瓶颈问题，实现未见微观结构的零样本分类，并增强可追溯性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 工业制造中，先进材料（特别是通过非常规增材制造工艺生产的异质结构）的快速可靠鉴定是一个关键瓶颈。

Method: 提出一个将微观结构信息学与专家鉴定知识相结合的新框架，利用定制的混合视觉-语言表示（VLRs）。该方法整合深度语义分割与预训练多模态模型（CLIP和FLAVA），将视觉微观结构数据和文本专家评估编码到共享表示中。为克服通用嵌入的局限性，开发了定制的基于相似度的表示，结合专家标注图像及其文本描述中的正负参考。通过净相似度评分实现未见微观结构的零样本分类，并应用Z-score归一化调整相似度得分。

Result: 在增材制造金属基复合材料数据集上验证，该框架能有效区分合格和缺陷样本。比较分析表明，FLAVA模型具有更高的视觉敏感性，而CLIP模型与文本标准保持一致性。Z-score归一化提高了混合视觉-语言框架中对齐和分类的有效性。该方法无需任务特定模型再训练，即可增强鉴定流程的可追溯性和可解释性，支持人机协同决策。

Conclusion: 本研究通过促进原始数据与专家知识之间的语义互操作性，为工程信息学领域可扩展和领域自适应的鉴定策略做出了贡献。

Abstract: Rapid and reliable qualification of advanced materials remains a bottleneck
in industrial manufacturing, particularly for heterogeneous structures produced
via non-conventional additive manufacturing processes. This study introduces a
novel framework that links microstructure informatics with a range of expert
characterization knowledge using customized and hybrid vision-language
representations (VLRs). By integrating deep semantic segmentation with
pre-trained multi-modal models (CLIP and FLAVA), we encode both visual
microstructural data and textual expert assessments into shared
representations. To overcome limitations in general-purpose embeddings, we
develop a customized similarity-based representation that incorporates both
positive and negative references from expert-annotated images and their
associated textual descriptions. This allows zero-shot classification of
previously unseen microstructures through a net similarity scoring approach.
Validation on an additively manufactured metal matrix composite dataset
demonstrates the framework's ability to distinguish between acceptable and
defective samples across a range of characterization criteria. Comparative
analysis reveals that FLAVA model offers higher visual sensitivity, while the
CLIP model provides consistent alignment with the textual criteria. Z-score
normalization adjusts raw unimodal and cross-modal similarity scores based on
their local dataset-driven distributions, enabling more effective alignment and
classification in the hybrid vision-language framework. The proposed method
enhances traceability and interpretability in qualification pipelines by
enabling human-in-the-loop decision-making without task-specific model
retraining. By advancing semantic interoperability between raw data and expert
knowledge, this work contributes toward scalable and domain-adaptable
qualification strategies in engineering informatics.

</details>


### [57] [MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces](https://arxiv.org/abs/2508.20256)
*Zhen Xuen Brandon Low,Rory Zhang,Hang Min,William Pham,Lucy Vivash,Jasmine Moses,Miranda Lynch,Karina Dorfman,Cassandra Marotta,Shaun Koh,Jacob Bunyamin,Ella Rowsthorn,Alex Jarema,Himashi Peiris,Zhaolin Chen,Sandy R. Shultz,David K. Wright,Dexiao Kong,Sharon L. Naismith,Terence J. O'Brien,Ying Xia,Meng Law,Benjamin Sinclair*

Main category: cs.CV

TL;DR: 本研究将MedNeXt-L-k5模型应用于自动分割扩大的血管周围间隙（PVS），在T2加权图像上取得了创纪录的高精度，并显示其在多种MRI数据集上的泛化能力，同时指出基于Transformer的注意力机制可能并非实现高精度PVS分割的必要条件。


<details>
  <summary>Details</summary>
Motivation: 扩大的血管周围间隙（PVS）是多种神经退行性疾病和衰老的生物标志物，但手动分割耗时且可靠性中等。现有自动化深度学习模型性能不佳，难以泛化到不同的临床和研究MRI数据集。

Method: 研究人员调整了MedNeXt-L-k5（一种受Transformer启发的3D编解码卷积网络）用于PVS的自动化分割。训练了两个模型：一个使用来自人类连接组计划-衰老（HCP-Aging）数据集的200个同质T2加权（T2w）MRI扫描，另一个使用来自七项研究和六个扫描仪的40个异质T1加权（T1w）MRI体。模型性能通过内部5折交叉验证（5FCV）和留一站点交叉验证（LOSOCV）进行评估。

Result: 在HCP-Aging数据集的T2w图像上训练的MedNeXt-L-k5模型实现了0.88+/-0.06的体素级Dice分数（白质，WM），与该数据集报告的评委间可靠性相当，是文献中报告的最高分数。在HCP-Aging数据集的T1w图像上训练的相同模型Dice分数显著较低，为0.58+/-0.09（WM）。在LOSOCV下，该模型体素级Dice分数为0.38+/-0.16（WM）和0.35+/-0.12（基底节，BG），簇级Dice分数为0.61+/-0.19（WM）和0.62+/-0.21（BG）。MedNeXt-L-k5并未优于nnU-Net。

Conclusion: MedNeXt-L-k5为跨越多种T1w和T2w MRI数据集的PVS自动化分割提供了一个高效的解决方案。MedNeXt-L-k5未能超越nnU-Net，这表明Transformer启发模型中提供全局上下文的注意力机制对于PVS分割的高精度并非必需。

Abstract: Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers
of cerebral small vessel disease, Alzheimer's disease, stroke, and
aging-related neurodegeneration. However, manual segmentation of PVS is
time-consuming and subject to moderate inter-rater reliability, while existing
automated deep learning models have moderate performance and typically fail to
generalize across diverse clinical and research MRI datasets. We adapted
MedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network,
for automated PVS segmentation. Two models were trained: one using a
homogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human
Connectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous
T1-weighted (T1w) MRI volumes from seven studies across six scanners. Model
performance was evaluated using internal 5-fold cross validation (5FCV) and
leave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on
the T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of
0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater
reliability of that dataset, and the highest yet reported in the literature.
The same models trained on the T1w images of the HCP-Aging dataset achieved a
substantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had
voxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and
cluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG).
MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation
across diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the
nnU-Net, indicating that the attention-based mechanisms present in
transformer-inspired models to provide global context are not required for high
accuracy in PVS segmentation.

</details>


### [58] [Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation](https://arxiv.org/abs/2508.20265)
*Zhixiang Chi,Yanan Wu,Li Gu,Huan Liu,Ziqiang Wang,Yang Zhang,Yang Wang,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: CLIP在开放词汇分割中因定位差而受限。本文提出一种免训练、反馈驱动的自适应框架，通过将模型输出的逐块对应关系反馈到中间注意力，增强空间和语义一致性，并在多个基准测试中持续提升了性能。


<details>
  <summary>Details</summary>
Motivation: CLIP虽然视觉-文本对齐强大，但在开放词汇分割中因定位不佳而表现欠佳。现有方法通过修改中间注意力来增强空间一致性，但这种一致性难以完全传播到最终输出，且中间注意力缺乏与文本表示的直接交互，限制了CLIP的潜力。

Method: 提出一个免训练、反馈驱动的自适应框架。该框架将基于输出的逐块对应关系反馈到中间注意力，利用模型输出作为更强的空间一致性先验，以增强内部表示与最终预测之间的语义一致性。主要模块包括注意力隔离、基于置信度的稀疏剪枝和适应性集成。

Result: 该方法作为一个插件模块，可无缝集成到四种SOTA方法和三种骨干网络（ViT-B, ViT-L, ViT-H）中。并在多种注意力类型（Q-K, self-self, Proxy augmented with MAE, SAM, DINO）上进行了验证。在八个基准测试中，该方法持续提升了它们的性能。

Conclusion: 本框架通过将模型输出的全面语义反馈到中间注意力，有效解决了CLIP在开放词汇分割中的定位和语义一致性问题，作为一个通用的插件模块，在现有方法和各种基准测试中均实现了显著且持续的性能提升。

Abstract: CLIP exhibits strong visual-textual alignment but struggle with
open-vocabulary segmentation due to poor localization. Prior methods enhance
spatial coherence by modifying intermediate attention. But, this coherence
isn't consistently propagated to the final output due to subsequent operations
such as projections. Additionally, intermediate attention lacks direct
interaction with text representations, such semantic discrepancy limits the
full potential of CLIP.
  In this work, we propose a training-free, feedback-driven self-adaptive
framework that adapts output-based patch-level correspondences back to the
intermediate attention. The output predictions, being the culmination of the
model's processing, encapsulate the most comprehensive visual and textual
semantics about each patch. Our approach enhances semantic consistency between
internal representations and final predictions by leveraging the model's
outputs as a stronger spatial coherence prior. We design key modules, including
attention isolation, confidence-based pruning for sparse adaptation, and
adaptation ensemble, to effectively feedback the output coherence cues. Our
method functions as a plug-in module, seamlessly integrating into four
state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We
further validate our framework across multiple attention types (Q-K, self-self,
and Proxy augmented with MAE, SAM, and DINO). Our approach consistently
improves their performance across eight benchmarks.

</details>


### [59] [How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding](https://arxiv.org/abs/2508.20279)
*Zhuoran Yu,Yong Jae Lee*

Main category: cs.CV

TL;DR: 本研究引入探针框架，系统分析了多模态大语言模型（MLLMs）内部的层级处理动态。发现MLLMs存在一致的分阶段结构：早期层进行视觉定位，中期层支持语义推理，后期层准备任务输出。该结构整体稳定，但具体层分配随基础LLM架构而变化。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在各种视觉-语言任务中表现出色，但其内部处理动态机制仍未被充分探索。

Method: 研究引入了一个探针框架，通过在MLLM每一层提取的token嵌入上训练线性分类器，预测细粒度视觉类别。为揭示各层功能，探针在三种受控提示变体下进行评估：1) 词汇变体（测试表面敏感性），2) 语义否定变体（改变预期答案），3) 输出格式变体（改变答案格式但保留推理）。框架应用于LLaVA-1.5, LLaVA-Next-LLaMA-3和Qwen2-VL模型。

Result: 研究识别出MLLM中一致的分阶段处理结构：早期层执行视觉定位，中间层支持词汇整合和语义推理，最终层准备特定任务输出。此外，该整体分阶段结构在视觉分词、指令微调数据和预训练语料的变化下保持稳定，但每个阶段的具体层分配会随基础LLM架构的改变而显著变化。

Conclusion: 研究结果为MLLMs的层级组织提供了统一视角，并提供了一种轻量级、模型无关的方法来分析多模态表示动态。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance
across a wide range of vision-language tasks, yet their internal processing
dynamics remain underexplored. In this work, we introduce a probing framework
to systematically analyze how MLLMs process visual and textual inputs across
layers. We train linear classifiers to predict fine-grained visual categories
(e.g., dog breeds) from token embeddings extracted at each layer, using a
standardized anchor question. To uncover the functional roles of different
layers, we evaluate these probes under three types of controlled prompt
variations: (1) lexical variants that test sensitivity to surface-level
changes, (2) semantic negation variants that flip the expected answer by
modifying the visual concept in the prompt, and (3) output format variants that
preserve reasoning but alter the answer format. Applying our framework to
LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent
stage-wise structure in which early layers perform visual grounding, middle
layers support lexical integration and semantic reasoning, and final layers
prepare task-specific outputs. We further show that while the overall
stage-wise structure remains stable across variations in visual tokenization,
instruction tuning data, and pretraining corpus, the specific layer allocation
to each stage shifts notably with changes in the base LLM architecture. Our
findings provide a unified perspective on the layer-wise organization of MLLMs
and offer a lightweight, model-agnostic approach for analyzing multimodal
representation dynamics.

</details>


### [60] [Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)](https://arxiv.org/abs/2508.20322)
*Zhi Li,Hau Phan,Matthew Emigh,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: 提出SLiCS方法，通过有监督字典学习将视觉-语言协同嵌入解耦为概念特异性子空间，显著提高了概念过滤图像检索和条件生成的精度。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言协同嵌入（如CLIP）虽然包含丰富的语义信息并对下游任务有用，但其嵌入空间并未被解耦。研究动机在于假设并实现将该嵌入空间解耦为多个概念特异性分量向量，以更精细地分离复杂场景的内容信息，从而提高下游任务的精度。

Method: 提出稀疏线性概念子空间（SLiCS）方法，该方法基于有监督字典学习，构建一个由字典中向量组（原子）的稀疏、非负组合构成的线性合成模型，其组内活动与多标签信息匹配。每个概念特异性分量是与标签相关的原子的非负组合。通过一种新颖的、保证收敛的交替优化方法来优化组结构字典。此外，方法还利用文本协同嵌入来寻找语义描述，并通过零样本分类的无监督字典学习来提供实例级多标签。SLiCS被应用于CLIP、TiTok自动编码器嵌入和DINOv2自监督潜入。

Result: SLiCS提供了可解耦的嵌入。这些解耦后的嵌入能够实现更精确的概念过滤图像检索和条件生成（使用图像到提示）。定量和定性结果均表明，对于所有评估的嵌入类型（包括CLIP、TiTok和DINOv2），概念过滤图像检索的精度均有显著提高。

Conclusion: 所提出的SLiCS方法成功将视觉-语言协同嵌入解耦为概念特异性子空间，这显著提高了概念过滤图像检索的精度，并展现了其在处理不同嵌入类型时的普适性和有效性。

Abstract: Vision-language co-embedding networks, such as CLIP, provide a latent
embedding space with semantic information that is useful for downstream tasks.
We hypothesize that the embedding space can be disentangled to separate the
information on the content of complex scenes by decomposing the embedding into
multiple concept-specific component vectors that lie in different subspaces. We
propose a supervised dictionary learning approach to estimate a linear
synthesis model consisting of sparse, non-negative combinations of groups of
vectors in the dictionary (atoms), whose group-wise activity matches the
multi-label information. Each concept-specific component is a non-negative
combination of atoms associated to a label. The group-structured dictionary is
optimized through a novel alternating optimization with guaranteed convergence.
Exploiting the text co-embeddings, we detail how semantically meaningful
descriptions can be found based on text embeddings of words best approximated
by a concept's group of atoms, and unsupervised dictionary learning can exploit
zero-shot classification of training set images using the text embeddings of
concept labels to provide instance-wise multi-labels. We show that the
disentangled embeddings provided by our sparse linear concept subspaces (SLiCS)
enable concept-filtered image retrieval (and conditional generation using
image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed
autoencoder embeddings from TiTok and the latent embedding from self-supervised
DINOv2. Quantitative and qualitative results highlight the improved precision
of the concept-filtered image retrieval for all embeddings.

</details>


### [61] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 针对医疗视觉语言模型（VLM）的安全和部署挑战，本文提出了一个名为MedFoundationHub的GUI工具包，以实现安全、易用的VLM部署和评估，并发现现有SOTA医疗VLM在临床应用中仍存在准确性和一致性问题。


<details>
  <summary>Details</summary>
Motivation: 医疗视觉语言模型（VLM）虽在临床应用中具有巨大潜力，但面临患者受保护健康信息（PHI）暴露、数据泄漏和网络威胁等严重安全隐私风险，且部署复杂，限制了其在医院环境中的安全和便捷应用。

Method: 开发了MedFoundationHub，一个图形用户界面（GUI）工具包，其功能包括：1) 医生无需编程即可手动选择和使用不同模型；2) 工程师可即插即用部署Hugging Face开源模型；3) 通过Docker编排实现操作系统无关且隐私保护的离线本地推理。该工具包仅需一台配备单个NVIDIA A6000 GPU的离线本地工作站。利用MedFoundationHub，本文组织了板认证病理学家，评估了五款最先进的VLM（如Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, LLaVA-1.5-7B/13B）在结肠和肾脏病例上的表现，共记录了1015次临床医生-模型评分事件。

Result: 专家评估揭示了当前VLM的普遍局限性，包括答案偏离目标、推理模糊以及病理学专业术语使用不一致。

Conclusion: MedFoundationHub为医疗VLM的安全部署和评估提供了一个可行的解决方案。然而，尽管有最新的模型，当前医疗VLM在实际临床应用中仍需在准确性、推理透明度和专业术语一致性方面进行显著改进。

Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [62] [Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction](https://arxiv.org/abs/2508.20376)
*Mang Cao,Sanping Zhou,Yizhe Li,Ye Deng,Wenli Huang,Le Wang*

Main category: cs.CV

TL;DR: 本文提出Bidirectional Interaction Mamba (BIM)，利用新颖的双向和多尺度扫描机制将Mamba模型应用于多任务密集预测，有效解决了现有方法在交互完整性和计算效率之间的权衡问题，并取得了超越SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: 多任务密集预测中，充分的跨任务交互至关重要，但往往导致高计算复杂度，使得现有方法不得不在交互完整性与计算效率之间进行权衡。

Method: 本文提出Bidirectional Interaction Mamba (BIM) 模型，通过引入两种新颖的扫描机制来适应Mamba的建模方法：
1.  **双向交互扫描 (BI-Scan)**：在交互过程中将任务特定表示构建为双向序列，通过统一的线性复杂度架构整合任务优先和位置优先扫描模式，高效保留关键跨任务信息。
2.  **多尺度扫描 (MS-Scan)**：实现多粒度场景建模，以满足不同任务的粒度需求并增强细微的跨任务特征交互。

Result: 在NYUD-V2和PASCAL-Context两个挑战性基准测试上的大量实验表明，本文提出的BIM模型优于其最先进的竞争方法。

Conclusion: BIM通过创新的双向和多尺度扫描机制，在多任务密集预测中实现了高效且完整的跨任务交互，有效解决了现有方法的局限性，并达到了领先的性能水平。

Abstract: Sufficient cross-task interaction is crucial for success in multi-task dense
prediction. However, sufficient interaction often results in high computational
complexity, forcing existing methods to face the trade-off between interaction
completeness and computational efficiency. To address this limitation, this
work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel
scanning mechanisms to adapt the Mamba modeling approach for multi-task dense
prediction. On the one hand, we introduce a novel Bidirectional Interaction
Scan (BI-Scan) mechanism, which constructs task-specific representations as
bidirectional sequences during interaction. By integrating task-first and
position-first scanning modes within a unified linear complexity architecture,
BI-Scan efficiently preserves critical cross-task information. On the other
hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve
multi-granularity scene modeling. This design not only meets the diverse
granularity requirements of various tasks but also enhances nuanced cross-task
feature interactions. Extensive experiments on two challenging benchmarks,
\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its
state-of-the-art competitors.

</details>


### [63] [Audio-Guided Visual Editing with Complex Multi-Modal Prompts](https://arxiv.org/abs/2508.20379)
*Hyeonyu Kim,Seokhoon Jeong,Seonghee Han,Chanhyuk Choi,Taehwan Kim*

Main category: cs.CV

TL;DR: 提出一种新颖的音频引导视觉编辑框架，无需额外训练，通过结合文本和音频提示，有效处理复杂编辑任务。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的视觉编辑在仅依赖文本提示时难以应对复杂场景，且现有音频引导方法需特定训练，泛化性受限。

Method: 1. 利用具有强大零样本能力的预训练多模态编码器，以缓解音频编码空间与扩散模型提示编码空间之间的差异。2. 提出“独立噪声分支”和“自适应块选择”方法来处理多个多模态编辑提示。该框架无需额外训练。

Result: 在多样的编辑任务中，该框架通过结合音频的丰富信息，在处理复杂编辑场景时表现出色，弥补了纯文本方法的不足。

Conclusion: 该框架有效解决了纯文本视觉编辑和现有音频引导方法的局限性，在复杂多模态编辑任务中展现出卓越性能。

Abstract: Visual editing with diffusion models has made significant progress but often
struggles with complex scenarios that textual guidance alone could not
adequately describe, highlighting the need for additional non-text editing
prompts. In this work, we introduce a novel audio-guided visual editing
framework that can handle complex editing tasks with multiple text and audio
prompts without requiring additional training. Existing audio-guided visual
editing methods often necessitate training on specific datasets to align audio
with text, limiting their generalization to real-world situations. We leverage
a pre-trained multi-modal encoder with strong zero-shot capabilities and
integrate diverse audio into visual editing tasks, by alleviating the
discrepancy between the audio encoder space and the diffusion model's prompt
encoder space. Additionally, we propose a novel approach to handle complex
scenarios with multiple and multi-modal editing prompts through our separate
noise branching and adaptive patch selection. Our comprehensive experiments on
diverse editing tasks demonstrate that our framework excels in handling
complicated editing scenarios by incorporating rich information from audio,
where text-only approaches fail.

</details>


### [64] [More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning](https://arxiv.org/abs/2508.20381)
*Luong Tran,Thieu Vo,Anh Nguyen,Sang Dinh,Van Nguyen*

Main category: cs.CV

TL;DR: 针对多标签学习中单正例标注（SPML）的挑战，本文提出了AEVLP框架，包含广义伪标签鲁棒损失（GPR Loss）和动态增强多焦点伪标签（DAMP）技术，在多个基准数据集上取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 多标签学习中，大规模数据集的全量标注成本高昂且耗时，因此催生了从部分标注数据中学习的需求。在极端情况（单正例多标签学习, SPML）下，传统方法将缺失标签视为未知或负标签会导致不准确和假阴性，且伪标签策略易引入额外噪声。

Method: 提出了广义伪标签鲁棒损失（GPR Loss），这是一种新颖的损失函数，能有效从多样化伪标签中学习并减轻噪声。引入了一种简单而有效的动态增强多焦点伪标签（DAMP）技术。二者共同构成了自适应高效视觉-语言伪标签（AEVLP）框架。

Result: 在四个基准数据集上进行了广泛实验，结果表明所提出的AEVLP框架显著提升了多标签分类性能，达到了最先进（state-of-the-art）的水平。

Conclusion: AEVLP框架通过GPR Loss和DAMP技术，有效解决了SPML中由伪标签引起的噪声和不准确问题，显著推动了多标签分类领域的进展，并取得了当前最优的性能。

Abstract: Multi-label learning is a challenging computer vision task that requires
assigning multiple categories to each image. However, fully annotating
large-scale datasets is often impractical due to high costs and effort,
motivating the study of learning from partially annotated data. In the extreme
case of Single Positive Multi-Label Learning (SPML), each image is provided
with only one positive label, while all other labels remain unannotated.
Traditional SPML methods that treat missing labels as unknown or negative tend
to yield inaccuracies and false negatives, and integrating various
pseudo-labeling strategies can introduce additional noise. To address these
challenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a
novel loss function that effectively learns from diverse pseudo-labels while
mitigating noise. Complementing this, we introduce a simple yet effective
Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these
contributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling
(AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate
that our framework significantly advances multi-label classification, achieving
state-of-the-art results.

</details>


### [65] [Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection](https://arxiv.org/abs/2508.20392)
*Chengjun Zhang,Yuhao Zhang,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: 为解决脉冲神经网络(SNNs)在视觉检测任务中性能不佳的问题，本文提出了一种延迟脉冲方法和新型时间依赖性整合与发放(tdIF)神经元架构，实现了在超低延迟下（5个时间步内）超越现有方法的检测性能。


<details>
  <summary>Details</summary>
Motivation: SNNs在神经形态硬件上具有低功耗和快速推理的优势，但当前ANN-SNN转换方法在视觉分类任务中表现出色，而在视觉检测任务中，尤其是在超低时间步下，其性能仍不理想。

Method: 本文提出“延迟脉冲（delay-spike）”方法来缓解异构脉冲模式导致的残余膜电位问题。此外，提出一种新颖的“时间依赖性整合与发放（temporal-dependent Integrate-and-Fire, tdIF）”神经元架构，使IF神经元能根据时间步的顺序动态调整其积累和发放行为，从而使脉冲展现独特的时序特性而非仅依赖频率表示，且能耗与传统IF神经元持平。

Result: 该方法在更低时间步下实现了更精确的特征表示，从而在视觉检测任务中达到高性能和超低延迟。在目标检测和车道线检测两项视觉任务中，所提出的tdIF方法超越了现有ANN-SNN转换方法，以超低延迟（5个时间步内）实现了最先进（state-of-the-art）的性能。

Conclusion: 所提出的延迟脉冲方法和tdIF神经元架构有效解决了SNN在视觉检测任务中的性能瓶颈，通过利用脉冲的时序特性，在保持能耗的同时，实现了超低延迟（5个时间步内）下的高性能和最先进表现。

Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by
minimal power consumption and swift inference capabilities on neuromorphic
hardware, and have been widely applied to various visual perception tasks.
Current ANN-SNN conversion methods have achieved excellent results in
classification tasks with ultra-low time-steps, but their performance in visual
detection tasks remains suboptimal. In this paper, we propose a delay-spike
approach to mitigate the issue of residual membrane potential caused by
heterogeneous spiking patterns. Furthermore, we propose a novel
temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This
enables Integrate-and-fire (IF) neurons to dynamically adjust their
accumulation and firing behaviors based on the temporal order of time-steps.
Our method enables spikes to exhibit distinct temporal properties, rather than
relying solely on frequency-based representations. Moreover, the tdIF neuron
maintains energy consumption on par with traditional IF neuron. We demonstrate
that our method achieves more precise feature representation with lower
time-steps, enabling high performance and ultra-low latency in visual detection
tasks. In this study, we conduct extensive evaluation of the tdIF method across
two critical vision tasks: object detection and lane line detection. The
results demonstrate that the proposed method surpasses current ANN-SNN
conversion approaches, achieving state-of-the-art performance with ultra-low
latency (within 5 time-steps).

</details>


### [66] [Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection](https://arxiv.org/abs/2508.20415)
*Yuqi Xiong,Wuzhen Shi,Yang Wen,Ruhan Liu*

Main category: cs.CV

TL;DR: 本文提出DUP-MCRNet网络，通过动态不确定性传播和多模态协同推理，有效解决了复杂场景下显著目标检测中细节丢失、边缘模糊和单模态信息融合不足的问题，提高了检测精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有显著目标检测（SOD）方法在复杂场景中容易丢失细节、边缘模糊，并且单模态信息融合不足。

Method: 提出动态不确定性传播与多模态协同推理网络（DUP-MCRNet）。主要包含：1) 动态不确定性图卷积模块（DUGC），通过基于空间语义距离构建的稀疏图和通道自适应交互，在层间传播不确定性，提高小结构和边缘区域检测精度。2) 多模态协同融合策略（MCF），利用可学习的模态门控权重，加权融合RGB、深度和边缘特征的注意力图，动态调整模态重要性，增强跨模态语义互补性。3) 结合多尺度BCE和IoU损失、跨尺度一致性约束以及不确定性引导的监督机制，优化像素级和区域级检测性能。

Result: DUP-MCRNet在大多数常用基准数据集上超越了各种SOD方法，特别是在边缘清晰度和对复杂背景的鲁棒性方面表现优异。

Conclusion: DUP-MCRNet通过引入动态不确定性传播和多模态协同融合，有效解决了复杂场景下的显著目标检测难题，显著提升了检测性能，尤其在边缘处理和复杂背景鲁棒性方面表现突出。

Abstract: In view of the problems that existing salient object detection (SOD) methods
are prone to losing details, blurring edges, and insufficient fusion of
single-modal information in complex scenes, this paper proposes a dynamic
uncertainty propagation and multimodal collaborative reasoning network
(DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is
designed to propagate uncertainty between layers through a sparse graph
constructed based on spatial semantic distance, and combined with channel
adaptive interaction, it effectively improves the detection accuracy of small
structures and edge regions. Secondly, a multimodal collaborative fusion
strategy (MCF) is proposed, which uses learnable modality gating weights to
weightedly fuse the attention maps of RGB, depth, and edge features. It can
dynamically adjust the importance of each modality according to different
scenes, effectively suppress redundant or interfering information, and
strengthen the semantic complementarity and consistency between
cross-modalities, thereby improving the ability to identify salient regions
under occlusion, weak texture or background interference. Finally, the
detection performance at the pixel level and region level is optimized through
multi-scale BCE and IoU loss, cross-scale consistency constraints, and
uncertainty-guided supervision mechanisms. Extensive experiments show that
DUP-MCRNet outperforms various SOD methods on most common benchmark datasets,
especially in terms of edge clarity and robustness to complex backgrounds. Our
code is publicly available at https://github.com/YukiBear426/DUP-MCRNet.

</details>


### [67] [MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection](https://arxiv.org/abs/2508.20447)
*Taiga Yamane,Satoshi Suzuki,Ryo Masumura,Shota Orihashi,Tomohiro Tanaka,Mana Ihori,Naoki Makishima,Naotaka Kawata*

Main category: cs.CV

TL;DR: 针对多视角行人检测中尺度变化难题，本文提出MSMVD方法，通过生成多尺度BEV特征并结合特征金字塔网络，显著提升了检测性能，超越了现有最佳水平。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端多视角行人检测（MVPD）方法在处理视角内行人尺度一致过小/过大，或不同视角间行人尺度差异巨大的情况时表现不佳。其根本原因在于未能有效利用多尺度图像特征来生成BEV特征。

Method: 本文提出的MSMVD方法首先将从各个视图提取的多尺度图像特征逐尺度投影到鸟瞰图（BEV）空间，生成多尺度BEV特征。这些BEV特征保留了相应尺度的图像信息，有助于精确检测尺度一致的行人。随后，MSMVD利用特征金字塔网络（FPN）处理这些多尺度BEV特征，以融合不同尺度和视图的信息，从而改善不同视图间行人尺度差异大的检测问题。

Result: 广泛的实验证明，通过多尺度BEV特征利用多尺度图像特征能显著提高检测性能。MSMVD在GMVD数据集上，将MODA指标较现有最高水平提升了4.5点。

Conclusion: 本文提出的MSMVD方法有效利用多尺度图像特征和BEV特征，成功克服了多视角行人检测中的尺度变化挑战，显著提升了检测性能，并取得了当前最佳的检测结果。

Abstract: Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form
of a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end
trainable deep learning methods have progressed greatly. However, they often
struggle to detect pedestrians with consistently small or large scales in views
or with vastly different scales between views. This is because they do not
exploit multi-scale image features to generate the BEV feature and detect
pedestrians. To overcome this problem, we propose a novel MVPD method, called
Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV
features by projecting multi-scale image features extracted from individual
views into the BEV space, scale-by-scale. Each of these BEV features inherits
the properties of its corresponding scale image features from multiple views.
Therefore, these BEV features help the precise detection of pedestrians with
consistently small or large scales in views. Then, MSMVD combines information
at different scales of multiple views by processing the multi-scale BEV
features using a feature pyramid network. This improves the detection of
pedestrians with vastly different scales between views. Extensive experiments
demonstrate that exploiting multi-scale image features via multi-scale BEV
features greatly improves the detection performance, and MSMVD outperforms the
previous highest MODA by $4.5$ points on the GMVD dataset.

</details>


### [68] [A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection](https://arxiv.org/abs/2508.20449)
*Libo Lv,Tianyi Wang,Mengxiao Huang,Ruixia Liu,Yinglong Wang*

Main category: cs.CV

TL;DR: 针对实时深度伪造检测计算成本高的问题，本文提出了SFMFNet，一个轻量高效的网络，通过融合空域纹理和频域伪影，并结合高效的多级特征交互，实现了准确性和效率的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 随着实时深度伪造技术的快速发展，伪造内容日益逼真且广泛传播。尽管现有检测器精度高，但其高计算成本阻碍了在视频会议和社交媒体等实际应用中的实时部署。

Method: 本文提出了SFMFNet网络架构。主要包括：1) 一个空频混合感知模块，通过门控机制共同利用空域纹理和频域伪影。2) 一个令牌选择的交叉注意力机制，实现高效的多级特征交互。3) 一个残差增强的模糊池化结构，在下采样时保留关键语义线索。

Result: SFMFNet在多个基准数据集上的实验表明，它在准确性和效率之间取得了有利的平衡，并展现出强大的泛化能力和实用价值。

Conclusion: SFMFNet为实时深度伪造检测提供了一个轻量且有效的解决方案，具有良好的性能和在实际应用中的潜力。

Abstract: With the rapid advancement of real-time deepfake generation techniques,
forged content is becoming increasingly realistic and widespread across
applications like video conferencing and social media. Although
state-of-the-art detectors achieve high accuracy on standard benchmarks, their
heavy computational cost hinders real-time deployment in practical
applications. To address this, we propose the Spatial-Frequency Aware
Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture
for real-time deepfake detection. We design a spatial-frequency hybrid aware
module that jointly leverages spatial textures and frequency artifacts through
a gated mechanism, enhancing sensitivity to subtle manipulations. A
token-selective cross attention mechanism enables efficient multi-level feature
interaction, while a residual-enhanced blur pooling structure helps retain key
semantic cues during downsampling. Experiments on several benchmark datasets
show that SFMFNet achieves a favorable balance between accuracy and efficiency,
with strong generalization and practical value for real-time applications.

</details>


### [69] [Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification](https://arxiv.org/abs/2508.20461)
*Ayaka Tsutsumi,Guang Li,Ren Togo,Takahiro Ogawa,Satoshi Kondo,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出了一种结合双模型权重选择和自知识蒸馏的新型医学图像分类方法，旨在构建高性能且计算高效的轻量级模型。


<details>
  <summary>Details</summary>
Motivation: 在真实医疗场景中，大型模型受限于计算资源难以部署。因此，开发性能与大型模型相当但计算效率更高的轻量级模型至关重要。

Method: 采用双模型权重选择策略，从大型预训练模型中初始化两个轻量级模型以实现知识迁移。随后，对这些选定的模型应用自知识蒸馏（SKD），并针对目标分类任务进行微调。

Result: 在胸部X光、肺部CT和脑部MRI等公开数据集上的广泛实验表明，本方法相比现有方法具有卓越的性能和鲁棒性。

Conclusion: 本方法通过整合双模型权重选择和自知识蒸馏，有效克服了传统方法在紧凑模型中难以保留关键信息的局限性，成功实现了高效且高性能的医学图像分类。

Abstract: We propose a novel medical image classification method that integrates
dual-model weight selection with self-knowledge distillation (SKD). In
real-world medical settings, deploying large-scale models is often limited by
computational resource constraints, which pose significant challenges for their
practical implementation. Thus, developing lightweight models that achieve
comparable performance to large-scale models while maintaining computational
efficiency is crucial. To address this, we employ a dual-model weight selection
strategy that initializes two lightweight models with weights derived from a
large pretrained model, enabling effective knowledge transfer. Next, SKD is
applied to these selected models, allowing the use of a broad range of initial
weight configurations without imposing additional excessive computational cost,
followed by fine-tuning for the target classification tasks. By combining
dual-model weight selection with self-knowledge distillation, our method
overcomes the limitations of conventional approaches, which often fail to
retain critical information in compact models. Extensive experiments on
publicly available datasets-chest X-ray images, lung computed tomography scans,
and brain magnetic resonance imaging scans-demonstrate the superior performance
and robustness of our approach compared to existing methods.

</details>


### [70] [Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds](https://arxiv.org/abs/2508.20466)
*Pengpeng Yu,Haoran Li,Dingquan Li,Runqing Jiang,Jing Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

TL;DR: 本文提出一种新的LiDAR点云压缩方法，通过生成紧凑特征并结合几何重致密化和跨尺度特征传播模块，实现了SOTA压缩率和实时性能。


<details>
  <summary>Details</summary>
Motivation: 高精度LiDAR点云带来巨大的存储和传输开销。现有方法难以有效处理几何细节的极端稀疏性，限制了压缩性能和速度。

Method: 提出通过生成紧凑特征进行高效预测编码，框架包含两个轻量级模块：1. 几何重致密化模块：对稀疏几何进行重致密化，提取更密集尺度的特征，然后重新稀疏化用于预测编码。2. 跨尺度特征传播模块：利用多分辨率占据线索引导分层特征传播，促进跨尺度信息共享，提供丰富特征。

Result: 在KITTI数据集上，实现了最先进的压缩比和实时性能，在12位量化下，编解码速度均达到26 FPS。

Conclusion: 该方法通过生成紧凑特征表示，实现了高效的上下文建模和加速的编码过程，有效解决了LiDAR点云压缩的挑战，并表现出优异的压缩性能和实时性。

Abstract: LiDAR point clouds are fundamental to various applications, yet
high-precision scans incur substantial storage and transmission overhead.
Existing methods typically convert unordered points into hierarchical octree or
voxel structures for dense-to-sparse predictive coding. However, the extreme
sparsity of geometric details hinders efficient context modeling, thereby
limiting their compression performance and speed. To address this challenge, we
propose to generate compact features for efficient predictive coding. Our
framework comprises two lightweight modules. First, the Geometry
Re-Densification Module re-densifies encoded sparse geometry, extracts features
at denser scale, and then re-sparsifies the features for predictive coding.
This module avoids costly computation on highly sparse details while
maintaining a lightweight prediction head. Second, the Cross-scale Feature
Propagation Module leverages occupancy cues from multiple resolution levels to
guide hierarchical feature propagation. This design facilitates information
sharing across scales, thereby reducing redundant feature extraction and
providing enriched features for the Geometry Re-Densification Module. By
integrating these two modules, our method yields a compact feature
representation that provides efficient context modeling and accelerates the
coding process. Experiments on the KITTI dataset demonstrate state-of-the-art
compression ratios and real-time performance, achieving 26 FPS for both
encoding and decoding at 12-bit quantization. Code is available at
https://github.com/pengpeng-yu/FastPCC.

</details>


### [71] [Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation](https://arxiv.org/abs/2508.20470)
*Xiaochuan Li,Guoguang Du,Runze Zhang,Liang Jin,Qi Jia,Lihua Lu,Zhenhua Guo,Yaqian Zhao,Haiyang Liu,Tianqi Wang,Changsheng Li,Xiaoli Gong,Rengang Li,Baoyu Fan*

Main category: cs.CV

TL;DR: 为解决3D生成领域数据稀缺问题，本文提出利用视频数据作为监督信号，构建了首个大规模多视角视频数据集Droplet3D-4M并训练了生成模型Droplet3D，实现了空间一致且语义合理的3D内容生成，并展现了向场景级应用扩展的潜力。


<details>
  <summary>Details</summary>
Motivation: 大型模型在文本、图像、视频生成中通过海量数据训练取得了巨大成功，但3D领域数据极度稀缺，导致模型泛化能力受限。视频数据蕴含的常识先验（空间一致性和丰富语义信息）有望缓解这一瓶颈，为3D生成提供替代监督信号。

Method: 探索如何将视频模态应用于3D资产生成，涵盖数据集和模型。具体方法包括：1. 构建Droplet3D-4M，首个带有多视角级别标注的大规模视频数据集。2. 训练Droplet3D，一个支持图像和密集文本输入的生成模型，利用视频中的多视角和语义信息来指导3D内容的生成。

Result: 实验验证了该方法能有效生成空间一致且语义合理的3D内容。与现有3D方案相比，该方法展现了扩展到场景级应用的潜力。这表明视频中的常识先验显著促进了3D创作。

Conclusion: 视频中的常识先验能够显著促进3D创建，有效缓解了3D领域的数据稀缺问题，并为开发更强大的3D生成模型提供了新的途径，特别是为场景级应用奠定了基础。

Abstract: Scaling laws have validated the success and promise of large-data-trained
models in creative generation across text, image, and video domains. However,
this paradigm faces data scarcity in the 3D domain, as there is far less of it
available on the internet compared to the aforementioned modalities.
Fortunately, there exist adequate videos that inherently contain commonsense
priors, offering an alternative supervisory signal to mitigate the
generalization bottleneck caused by limited native 3D data. On the one hand,
videos capturing multiple views of an object or scene provide a spatial
consistency prior for 3D generation. On the other hand, the rich semantic
information contained within the videos enables the generated content to be
more faithful to the text prompts and semantically plausible. This paper
explores how to apply the video modality in 3D asset generation, spanning
datasets to models. We introduce Droplet3D-4M, the first large-scale video
dataset with multi-view level annotations, and train Droplet3D, a generative
model supporting both image and dense text input. Extensive experiments
validate the effectiveness of our approach, demonstrating its ability to
produce spatially consistent and semantically plausible content. Moreover, in
contrast to the prevailing 3D solutions, our approach exhibits the potential
for extension to scene-level applications. This indicates that the commonsense
priors from the videos significantly facilitate 3D creation. We have
open-sourced all resources including the dataset, code, technical framework,
and model weights: https://dropletx.github.io/.

</details>


### [72] [Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation](https://arxiv.org/abs/2508.20471)
*Jiusi Li,Jackson Jiang,Jinyu Miao,Miao Long,Tuopu Wen,Peijin Jia,Shengxiang Liu,Chunlei Yu,Maolin Liu,Yuzhan Cai,Kun Jiang,Mengmeng Yang,Diange Yang*

Main category: cs.CV

TL;DR: G^2Editor是一个用于驾驶视频中逼真、精确物体编辑的框架，通过3D高斯表示和分层细粒度特征，解决了现有方法在视觉保真度和姿态控制方面的不足。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统所需的极端场景数据收集成本高昂且危险。现有通过3D高斯Splatting或图像生成模型进行物体编辑的方法，在视觉逼真度或姿态控制精度上存在局限。

Method: 提出了G^2Editor框架。该方法利用编辑物体的3D高斯表示作为密集先验，注入到去噪过程中以确保精确的姿态控制和空间一致性。采用场景级3D边界框布局重建非目标物体的遮挡区域，并通过分层细粒度特征引导编辑物体的外观细节。

Result: 在Waymo Open数据集上的实验表明，G^2Editor能在一个统一框架内有效支持物体重定位、插入和删除，在姿态可控性和视觉质量方面优于现有方法，并对下游数据驱动任务有所助益。

Conclusion: G^2Editor为驾驶视频中的逼真和精确物体编辑提供了一个优越的解决方案，克服了现有方法的关键局限，对自动驾驶数据生成具有重要价值。

Abstract: Corner cases are crucial for training and validating autonomous driving
systems, yet collecting them from the real world is often costly and hazardous.
Editing objects within captured sensor data offers an effective alternative for
generating diverse scenarios, commonly achieved through 3D Gaussian Splatting
or image generative models. However, these approaches often suffer from limited
visual fidelity or imprecise pose control. To address these issues, we propose
G^2Editor, a framework designed for photorealistic and precise object editing
in driving videos. Our method leverages a 3D Gaussian representation of the
edited object as a dense prior, injected into the denoising process to ensure
accurate pose control and spatial consistency. A scene-level 3D bounding box
layout is employed to reconstruct occluded areas of non-target objects.
Furthermore, to guide the appearance details of the edited object, we
incorporate hierarchical fine-grained features as additional conditions during
generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor
effectively supports object repositioning, insertion, and deletion within a
unified framework, outperforming existing methods in both pose controllability
and visual quality, while also benefiting downstream data-driven tasks.

</details>


### [73] [Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization](https://arxiv.org/abs/2508.20475)
*Marina Grifell i Plana,Vladyslav Zalevskyi,Léa Schmidt,Yvan Gomez,Thomas Sanchez,Vincent Dunet,Mériam Koob,Vanessa Siffredi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 针对胼胝体发育不全（CCD）等罕见胎儿脑部病变，本研究提出一种结合病理学先验知识的域随机化策略，通过合成数据生成，有效解决了标注数据稀缺问题，显著提高了分割精度和生物标记物提取的可靠性。


<details>
  <summary>Details</summary>
Motivation: 准确的胎儿大脑分割对于提取生物标记物和评估神经发育至关重要，尤其是在胼胝体发育不全（CCD）等导致显著解剖结构变化的病症中。然而，CCD的罕见性导致标注数据严重不足，限制了深度学习模型的泛化能力。

Method: 提出了一种“病理学知情域随机化”策略，将CCD的先验知识嵌入到合成数据生成流程中。通过仅使用健康数据模拟多样化的脑部病变，该方法无需病理学标注即可实现鲁棒的分割。

Result: 在包含248名健康胎儿、26名CCD病例和47名其他脑部病理病例的队列中进行了验证。结果显示，该方法在CCD病例上取得了显著改进，同时保持了在健康胎儿和其他病理病例上的性能。它将健康病例的胼胝体长度（LCC）估计误差从1.89毫米降至0.80毫米，将CCD病例的LCC估计误差从10.9毫米降至0.7毫米。此外，生成的分割结果具有更好的拓扑一致性，有助于更可靠的形状分析。

Conclusion: 本研究表明，将领域特定的解剖学先验知识整合到合成数据流程中，可以有效缓解数据稀缺问题，并增强对罕见但具有临床意义的畸形的分析能力。

Abstract: Accurate fetal brain segmentation is crucial for extracting biomarkers and
assessing neurodevelopment, especially in conditions such as corpus callosum
dysgenesis (CCD), which can induce drastic anatomical changes. However, the
rarity of CCD severely limits annotated data, hindering the generalization of
deep learning models. To address this, we propose a pathology-informed domain
randomization strategy that embeds prior knowledge of CCD manifestations into a
synthetic data generation pipeline. By simulating diverse brain alterations
from healthy data alone, our approach enables robust segmentation without
requiring pathological annotations.
  We validate our method on a cohort comprising 248 healthy fetuses, 26 with
CCD, and 47 with other brain pathologies, achieving substantial improvements on
CCD cases while maintaining performance on both healthy fetuses and those with
other pathologies. From the predicted segmentations, we derive clinically
relevant biomarkers, such as corpus callosum length (LCC) and volume, and show
their utility in distinguishing CCD subtypes. Our pathology-informed
augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in
healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these
quantitative gains, our approach yields segmentations with improved topological
consistency relative to available ground truth, enabling more reliable
shape-based analyses. Overall, this work demonstrates that incorporating
domain-specific anatomical priors into synthetic data pipelines can effectively
mitigate data scarcity and enhance analysis of rare but clinically significant
malformations.

</details>


### [74] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

TL;DR: 本文提出了首个统一框架，能够结合手语、唇语和音频生成口语文本，并在手语翻译、视觉语音识别等多个任务上取得了与SOTA模型相当或更优的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管自动语音识别（ASR）技术已取得成功，但其对听障人士仍不可用。现有的视觉替代方案（如手语翻译SLT和视觉语音识别VSR）多独立研究，缺乏一个能整合多种模态（手语、唇语、音频）的统一框架。

Method: 本文引入了首个统一框架，旨在处理手语、唇语和音频的多种组合以生成口语文本。该方法侧重于设计一个统一、模态无关的架构，探索模态间的协同作用（特别是唇语作为手语理解中的非手动线索），并致力于达到或超越现有各任务专用模型的性能。

Result: 该框架在手语翻译（SLT）、视觉语音识别（VSR）、自动语音识别（ASR）和音视频语音识别（AVSR）任务上，取得了与现有SOTA模型相当或更优的性能。此外，分析表明，将唇语作为独立模态明确建模能显著提升手语翻译的性能。

Conclusion: 本文成功构建了一个多模态统一框架，有效整合了手语、唇语和音频，实现了跨多种沟通形式的文本生成，并证明了多模态协同（尤其是唇语在手语理解中的作用）的价值和该框架的优越性能。

Abstract: Audio is the primary modality for human communication and has driven the
success of Automatic Speech Recognition (ASR) technologies. However, such
systems remain inherently inaccessible to individuals who are deaf or hard of
hearing. Visual alternatives such as sign language and lip reading offer
effective substitutes, and recent advances in Sign Language Translation (SLT)
and Visual Speech Recognition (VSR) have improved audio-less communication.
Yet, these modalities have largely been studied in isolation, and their
integration within a unified framework remains underexplored. In this paper, we
introduce the first unified framework capable of handling diverse combinations
of sign language, lip movements, and audio for spoken-language text generation.
We focus on three main objectives: (i) designing a unified, modality-agnostic
architecture capable of effectively processing heterogeneous inputs; (ii)
exploring the underexamined synergy among modalities, particularly the role of
lip movements as non-manual cues in sign language comprehension; and (iii)
achieving performance on par with or superior to state-of-the-art models
specialized for individual tasks. Building on this framework, we achieve
performance on par with or better than task-specific state-of-the-art models
across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that
explicitly modeling lip movements as a separate modality significantly improves
SLT performance.

</details>


### [75] [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478)
*Yuan Xie,Tianshui Chen,Zheng Ge,Lionel Ni*

Main category: cs.CV

TL;DR: 本文提出了Video-MTR，一个强化的多轮推理框架，通过迭代选择视频片段和新颖的双层奖励系统，实现端到端的长视频理解，并在准确性和效率上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 长视频理解因其长时序依赖和多事件而充满挑战。现有方法通常依赖静态推理或外部视觉-语言模型（VLMs），导致系统复杂、性能欠佳且难以实现端到端训练。

Method: 本文提出Video-MTR，一个强化的多轮推理框架，通过迭代选择关键视频片段和理解问题来逐步进行推理。它引入了一种新颖的门控双层奖励系统，结合基于答案正确性的轨迹级奖励和强调帧-查询相关性的轮次级奖励，从而优化视频片段选择和问题理解，无需外部VLMs即可实现端到端训练。

Result: 在VideoMME、MLVU和EgoSchema等基准测试中，Video-MTR在准确性和效率上均优于现有方法，显著提升了长视频理解的最新水平。

Conclusion: Video-MTR通过其多轮推理机制和创新的双层奖励系统，为长视频理解提供了一个有效的端到端解决方案，显著提高了该领域的性能和效率。

Abstract: Long-form video understanding, characterized by long-range temporal
dependencies and multiple events, remains a challenge. Existing methods often
rely on static reasoning or external visual-language models (VLMs), which face
issues like complexity and sub-optimal performance due to the lack of
end-to-end training. In this paper, we propose Video-MTR, a reinforced
multi-turn reasoning framework designed to enable iterative key video segment
selection and question comprehension. Unlike traditional video reasoning
pipeline, which generate predictions in a single turn, Video-MTR performs
reasoning in multiple turns, selecting video segments progressively based on
the evolving understanding of previously processed segments and the current
question. This iterative process allows for a more refined and contextually
aware analysis of the video. To ensure intermediate reasoning process, we
introduce a novel gated bi-level reward system, combining trajectory-level
rewards based on answer correctness and turn-level rewards emphasizing
frame-query relevance. This system optimizes both video segment selection and
question comprehension, eliminating the need for external VLMs and allowing
end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,
and EgoSchema demonstrate that Video-MTR outperforms existing methods in both
accuracy and efficiency, advancing the state-of-the-art in long video
understanding.

</details>


### [76] [Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts](https://arxiv.org/abs/2508.20488)
*Zixuan Hu,Dongxiao Li,Xinzhu Ma,Shixiang Tang,Xiaotong Li,Wenhan Yang,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 本文提出了DUO，首个为单目3D目标检测（M3OD）设计的测试时间自适应（TTA）框架，通过联合最小化语义和几何双重不确定性来提升模型在域偏移下的鲁棒性，并在多个数据集和域偏移类型上表现出优越性。


<details>
  <summary>Details</summary>
Motivation: 单目3D目标检测在自动驾驶等关键应用中至关重要，但在真实世界的域偏移（环境或传感器变化）下可靠性会显著下降。现有的测试时间自适应（TTA）方法未能解决M3OD固有的双重不确定性：语义不确定性（模糊的类别预测）和几何不确定性（不稳定的空间定位）。

Method: 本文提出了双重不确定性优化（DUO）框架，旨在共同最小化语义和几何不确定性。方法包括：1) 引入Focal Loss的创新凸结构，并推导出无监督版本，实现与标签无关的不确定性加权和对高不确定性物体的平衡学习。2) 设计语义感知法线场约束，在具有清晰语义线索的区域保持几何连贯性，从而减少不稳定3D表示带来的不确定性。这种双分支机制形成互补循环，相互增强。

Result: 广泛的实验表明，DUO在各种数据集和域偏移类型上均优于现有方法。

Conclusion: DUO框架成功解决了M3OD在域偏移下的双重不确定性问题，显著提升了M3OD的鲁棒性，为单目3D目标检测在真实世界应用提供了更可靠的解决方案。

Abstract: Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical
applications like autonomous driving, yet its reliability deteriorates
significantly under real-world domain shifts caused by environmental or sensor
variations. To address these shifts, Test-Time Adaptation (TTA) methods have
emerged, enabling models to adapt to target distributions during inference.
While prior TTA approaches recognize the positive correlation between low
uncertainty and high generalization ability, they fail to address the dual
uncertainty inherent to M3OD: semantic uncertainty (ambiguous class
predictions) and geometric uncertainty (unstable spatial localization). To
bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA
framework designed to jointly minimize both uncertainties for robust M3OD.
Through a convex optimization lens, we introduce an innovative convex structure
of the focal loss and further derive a novel unsupervised version, enabling
label-agnostic uncertainty weighting and balanced learning for high-uncertainty
objects. In parallel, we design a semantic-aware normal field constraint that
preserves geometric coherence in regions with clear semantic cues, reducing
uncertainty from the unstable 3D representation. This dual-branch mechanism
forms a complementary loop: enhanced spatial perception improves semantic
classification, and robust semantic predictions further refine spatial
understanding. Extensive experiments demonstrate the superiority of DUO over
existing methods across various datasets and domain shift types.

</details>


### [77] [CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information](https://arxiv.org/abs/2508.20491)
*Seunghyeon Jung,Seoyoung Hong,Jiwoo Jeong,Seungwon Jeong,Jaerim Choi,Hoki Kim,Woojin Lee*

Main category: cs.CV

TL;DR: 提出CaddieSet数据集，通过计算机视觉分析高尔夫挥杆视频以量化挥杆姿态与球轨迹的关系，并验证其预测能力和可解释性，为挥杆分析提供新见解。


<details>
  <summary>Details</summary>
Motivation: 现有高尔夫挥杆分析研究未能定量建立挥杆姿态与球轨迹之间的关系，从而限制了为高尔夫球手提供改进挥杆的有效见解。

Method: 构建名为CaddieSet的新数据集，其包含单次击球的关节和球信息。通过计算机视觉技术将挥杆视频分割成八个阶段以提取关节信息，并基于专家领域知识定义了15个影响挥杆的关键指标。

Result: 实验证明CaddieSet在预测球轨迹方面是可行的。特别是，使用其关节特征的挥杆反馈与既定的高尔夫领域知识在定量上保持一致。

Conclusion: 这项工作预计将为学术界和体育产业的高尔夫挥杆分析提供新的见解。

Abstract: Recent advances in deep learning have led to more studies to enhance golfers'
shot precision. However, these existing studies have not quantitatively
established the relationship between swing posture and ball trajectory,
limiting their ability to provide golfers with the necessary insights for swing
improvement. In this paper, we propose a new dataset called CaddieSet, which
includes joint information and various ball information from a single shot.
CaddieSet extracts joint information from a single swing video by segmenting it
into eight swing phases using a computer vision-based approach. Furthermore,
based on expert golf domain knowledge, we define 15 key metrics that influence
a golf swing, enabling the interpretation of swing outcomes through
swing-related features. Through experiments, we demonstrated the feasibility of
CaddieSet for predicting ball trajectories using various benchmarks. In
particular, we focus on interpretable models among several benchmarks and
verify that swing feedback using our joint features is quantitatively
consistent with established domain knowledge. This work is expected to offer
new insight into golf swing analysis for both academia and the sports industry.

</details>


### [78] [IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection](https://arxiv.org/abs/2508.20492)
*Xuanming Cao,Chengyu Tao,Yifeng Cheng,Juan Du*

Main category: cs.CV

TL;DR: 本文针对3D点云表面异常检测缺乏强大预训练骨干网络的问题，提出IAENet。该集成网络融合了2D和3D专家模型，并引入重要性感知融合（IAF）模块动态加权各模态贡献，实现了MVTec 3D-AD数据集上的新SOTA，显著降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 尽管3D点云数据能提供更丰富的几何信息，但3D表面异常检测领域仍未得到充分探索。其主要瓶颈在于缺乏像2D图像领域那样强大的预训练基础骨干网络。

Method: 提出“重要性感知集成网络”（IAENet），该框架结合了2D预训练专家模型和3D专家模型。为解决朴素融合的问题，引入新颖的“重要性感知融合（IAF）”模块，动态评估各来源贡献并重新加权异常分数。此外，设计关键损失函数来指导IAF优化，使其能结合集体知识并保留各自优势。

Result: 在MVTec 3D-AD数据集上的广泛实验表明，IAENet取得了新的最先进（SOTA）性能，并显著降低了误报率。

Conclusion: IAENet通过有效结合2D和3D信息，解决了3D表面异常检测中的关键瓶颈问题，其卓越性能和低误报率凸显了在工业部署中的实际应用价值。

Abstract: Surface anomaly detection is pivotal for ensuring product quality in
industrial manufacturing. While 2D image-based methods have achieved remarkable
success, 3D point cloud-based detection remains underexplored despite its
richer geometric cues. We argue that the key bottleneck is the absence of
powerful pretrained foundation backbones in 3D comparable to those in 2D. To
bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an
ensemble framework that synergizes 2D pretrained expert with 3D expert models.
However, naively fusing predictions from disparate sources is non-trivial:
existing strategies can be affected by a poorly performing modality and thus
degrade overall accuracy. To address this challenge, We introduce an novel
Importance-Aware Fusion (IAF) module that dynamically assesses the contribution
of each source and reweights their anomaly scores. Furthermore, we devise
critical loss functions that explicitly guide the optimization of IAF, enabling
it to combine the collective knowledge of the source experts but also preserve
their unique strengths, thereby enhancing the overall performance of anomaly
detection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet
achieves a new state-of-the-art with a markedly lower false positive rate,
underscoring its practical value for industrial deployment.

</details>


### [79] [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](https://arxiv.org/abs/2508.20505)
*En Ci,Shanyan Guan,Yanhao Ge,Yilin Zhang,Wei Li,Zhenyu Zhang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: DescriptiveEdit将指令式图像编辑重构为基于参考图的文本到图像生成，通过Cross-Attentive UNet克服数据集限制，提升编辑准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 语义图像编辑仍具挑战；现有基于反演的算法引入重建误差，而基于指令的模型受限于数据集质量和规模。

Method: 提出DescriptiveEdit框架，将‘指令式图像编辑’重构为‘基于参考图像的文本到图像生成’，以保留预训练文生图模型的生成能力。具体通过引入一个Cross-Attentive UNet，新增注意力桥接以将参考图像特征注入到生成过程中。

Result: 该方法克服了指令数据集质量限制，能与ControlNet、IP-Adapter等无缝集成，并更具可扩展性。在Emu Edit基准测试中，显著提高了编辑准确性和一致性。

Conclusion: DescriptiveEdit通过创新的重构方法和新的Cross-Attentive UNet，提供了一种高效、可扩展的语义图像编辑解决方案，成功解决了传统方法的局限性并提升了性能。

Abstract: Despite the progress in text-to-image generation, semantic image editing
remains a challenge. Inversion-based algorithms unavoidably introduce
reconstruction errors, while instruction-based models mainly suffer from
limited dataset quality and scale. To address these problems, we propose a
descriptive-prompt-based editing framework, named DescriptiveEdit. The core
idea is to re-frame `instruction-based image editing' as `reference-image-based
text-to-image generation', which preserves the generative power of well-trained
Text-to-Image models without architectural modifications or inversion.
Specifically, taking the reference image and a prompt as input, we introduce a
Cross-Attentive UNet, which newly adds attention bridges to inject reference
image features into the prompt-to-edit-image generation process. Owing to its
text-to-image nature, DescriptiveEdit overcomes limitations in instruction
dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other
extensions, and is more scalable. Experiments on the Emu Edit benchmark show it
improves editing accuracy and consistency.

</details>


### [80] [DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and Sample](https://arxiv.org/abs/2508.20516)
*Wenting Yin,Han Sun,Xinru Meng,Ningzhong Liu,Huiyu Zhou*

Main category: cs.CV

TL;DR: 本文提出DCFS框架，通过双路径特征一致性和置信度感知样本学习，解决持续测试时适应（CTTA）中伪标签错误积累和学习偏差问题，并在多个数据集上取得了持续稳定的性能。


<details>
  <summary>Details</summary>
Motivation: 在持续测试时适应（CTTA）中，模型在没有源数据访问的情况下仅依赖目标域数据特征进行适应，这可能导致混淆和学习偏差。现有方法生成的伪标签质量无法保证，且存在错误累积问题。

Method: 本文提出DCFS框架。该框架使用双分类器将目标数据特征解耦为语义相关特征和域相关特征，并通过保持子特征与整体特征之间的一致性，实现双路径特征一致性学习。此外，通过设置自适应阈值和计算样本置信度分数，进行损失加权的自监督学习，以减少伪标签噪声并缓解错误累积问题。

Result: 所提出的方法（DCFS）通过在CIFAR10-C、CIFAR100-C和ImageNet-C等多个数据集上的广泛实验验证了其有效性，在持续测试时适应场景中表现出持续稳定的性能。

Conclusion: DCFS框架通过创新的双路径特征一致性和置信度感知样本学习策略，有效解决了CTTA中伪标签质量和错误累积的关键挑战，显著提升了模型在持续测试时适应任务中的性能和稳定性。

Abstract: Continual test-time adaptation aims to continuously adapt a pre-trained model
to a stream of target domain data without accessing source data. Without access
to source domain data, the model focuses solely on the feature characteristics
of the target data. Relying exclusively on these features can lead to confusion
and introduce learning biases. Currently, many existing methods generate
pseudo-labels via model predictions. However, the quality of pseudo-labels
cannot be guaranteed and the problem of error accumulation must be solved. To
address these challenges, we propose DCFS, a novel CTTA framework that
introduces dual-path feature consistency and confidence-aware sample learning.
This framework disentangles the whole feature representation of the target data
into semantic-related feature and domain-related feature using dual classifiers
to learn distinct feature representations. By maintaining consistency between
the sub-features and the whole feature, the model can comprehensively capture
data features from multiple perspectives. Additionally, to ensure that the
whole feature information of the target domain samples is not overlooked, we
set a adaptive threshold and calculate a confidence score for each sample to
carry out loss weighted self-supervised learning, effectively reducing the
noise of pseudo-labels and alleviating the problem of error accumulation. The
efficacy of our proposed method is validated through extensive experimentation
across various datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C,
demonstrating consistent performance in continual test-time adaptation
scenarios.

</details>


### [81] [Adam SLAM - the last mile of camera calibration with 3DGS](https://arxiv.org/abs/2508.20526)
*Matthieu Gendrin,Stéphane Pateux,Xiaoran Jiang,Théo Ladune,Luce Morin*

Main category: cs.CV

TL;DR: 本文提出使用3DGS模型通过反向传播新视图颜色损失来微调相机校准，显著提升了新视图合成质量。


<details>
  <summary>Details</summary>
Motivation: 相机校准质量对新视图合成至关重要，即使1像素误差也会严重影响重建质量。对于真实场景，缺乏校准真值，评估校准通常依赖于新视图合成质量。

Method: 利用3DGS模型，通过对相机参数进行反向传播新视图颜色损失来微调相机校准。

Result: 单独使用新校准即可在3DGS参考数据集上平均提高0.4 dB PSNR。

Conclusion: 虽然微调过程可能耗时，但对于Mip-NeRF 360等参考场景的校准，新视图质量至关重要，该方法能有效提升其性能。

Abstract: The quality of the camera calibration is of major importance for evaluating
progresses in novel view synthesis, as a 1-pixel error on the calibration has a
significant impact on the reconstruction quality. While there is no ground
truth for real scenes, the quality of the calibration is assessed by the
quality of the novel view synthesis. This paper proposes to use a 3DGS model to
fine tune calibration by backpropagation of novel view color loss with respect
to the cameras parameters. The new calibration alone brings an average
improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine
tuning may be long and its suitability depends on the criticity of training
time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake
of novel view quality is the most important.

</details>


### [82] [Learning What is Worth Learning: Active and Sequential Domain Adaptation for Multi-modal Gross Tumor Volume Segmentation](https://arxiv.org/abs/2508.20528)
*Jingyun Yang,Guoqing Zhang,Jingge Wang,Yang Li*

Main category: cs.CV

TL;DR: 本文提出一种主动序列域适应框架，用于动态多模态样本选择，以解决鼻咽癌和胶质母细胞瘤GTV分割中标签数据稀缺和传统ADA方法局限的问题，通过选择最具信息量和代表性的样本，显著提高了分割性能。


<details>
  <summary>Details</summary>
Motivation: 鼻咽癌和胶质母细胞瘤的GTV精确分割对放疗规划至关重要。深度学习进展虽大，但对标注数据需求高，而医学图像标注耗时耗力。现有主动域适应（ADA）方法存在负迁移风险、源数据受限以及对多模态医学数据查询策略未探索的局限性。

Method: 提出了一种主动且序列的域适应（active and sequential domain adaptation）框架，用于多模态数据中的动态样本选择。该方法推导了一种查询策略，根据样本的信息量和代表性来优先选择最有价值的样本进行标注和训练。

Result: 在多种GTV分割任务上的实证验证表明，该方法取得了良好的分割性能，显著优于现有最先进的ADA方法。

Conclusion: 所提出的主动序列域适应框架及其查询策略能有效解决多模态医学图像分割中数据标注成本高的问题，并在GTV分割任务上表现出卓越的性能，超越了现有技术水平。

Abstract: Accurate gross tumor volume segmentation on multi-modal medical data is
critical for radiotherapy planning in nasopharyngeal carcinoma and
glioblastoma. Recent advances in deep neural networks have brought promising
results in medical image segmentation, leading to an increasing demand for
labeled data. Since labeling medical images is time-consuming and
labor-intensive, active learning has emerged as a solution to reduce annotation
costs by selecting the most informative samples to label and adapting
high-performance models with as few labeled samples as possible. Previous
active domain adaptation (ADA) methods seek to minimize sample redundancy by
selecting samples that are farthest from the source domain. However, such
one-off selection can easily cause negative transfer, and access to source
medical data is often limited. Moreover, the query strategy for multi-modal
medical data remains unexplored. In this work, we propose an active and
sequential domain adaptation framework for dynamic multi-modal sample selection
in ADA. We derive a query strategy to prioritize labeling and training on the
most valuable samples based on their informativeness and representativeness.
Empirical validation on diverse gross tumor volume segmentation tasks
demonstrates that our method achieves favorable segmentation performance,
significantly outperforming state-of-the-art ADA methods. Code is available at
the git repository: \href{https://github.com/Hiyoochan/mmActS}{mmActS}.

</details>


### [83] [Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection](https://arxiv.org/abs/2508.20530)
*Mingqian Ji,Jian Yang,Shanshan Zhang*

Main category: cs.CV

TL;DR: 本文提出一种新颖的数据级融合框架，将RGB图像和LiDAR数据在早期阶段融合，以解决无监督3D目标检测中伪框质量受限的问题，并在nuScenes数据集上取得了显著优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LiDAR的3D目标检测依赖昂贵的手动标注标签。尽管无监督方法引入RGB图像辅助生成伪框，但其标签级融合策略未能充分利用LiDAR和RGB数据的互补性，导致伪框质量提升有限。

Method: 我们提出一种数据级融合框架，在早期阶段整合RGB图像和LiDAR数据。具体方法包括：利用视觉基础模型进行图像实例分割和深度估计；引入双向融合方法，使3D点从2D空间获取类别标签，同时将2D像素投影到3D以增强点密度；设计局部和全局滤波方法来抑制深度估计误差和分割引起的异常值；以及提出基于数据级融合的动态自演化策略，以迭代优化伪框。

Result: 在nuScenes数据集上的广泛实验表明，通过我们方法训练的检测器性能显著优于现有最先进的方法，在nuScenes验证基准上实现了28.4%的mAP。

Conclusion: 该研究通过创新的数据级融合策略、双向融合、噪声过滤和动态自演化机制，有效提升了无监督3D目标检测的伪框质量和定位精度，取得了业界领先的性能。

Abstract: Existing LiDAR-based 3D object detectors typically rely on manually annotated
labels for training to achieve good performance. However, obtaining
high-quality 3D labels is time-consuming and labor-intensive. To address this
issue, recent works explore unsupervised 3D object detection by introducing RGB
images as an auxiliary modal to assist pseudo-box generation. However, these
methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB
images. Yet, such a label-level fusion strategy brings limited improvements to
the quality of pseudo-boxes, as it overlooks the complementary nature in terms
of LiDAR and RGB image data. To overcome the above limitations, we propose a
novel data-level fusion framework that integrates RGB images and LiDAR data at
an early stage. Specifically, we utilize vision foundation models for instance
segmentation and depth estimation on images and introduce a bi-directional
fusion method, where real points acquire category labels from the 2D space,
while 2D pixels are projected onto 3D to enhance real point density. To
mitigate noise from depth and segmentation estimations, we propose a local and
global filtering method, which applies local radius filtering to suppress depth
estimation errors and global statistical filtering to remove
segmentation-induced outliers. Furthermore, we propose a data-level fusion
based dynamic self-evolution strategy, which iteratively refines pseudo-boxes
under a dense representation, significantly improving localization accuracy.
Extensive experiments on the nuScenes dataset demonstrate that the detector
trained by our method significantly outperforms that trained by previous
state-of-the-art methods with 28.4$\%$ mAP on the nuScenes validation
benchmark.

</details>


### [84] [Digital Scale: Open-Source On-Device BMI Estimation from Smartphone Camera Images Trained on a Large-Scale Real-World Dataset](https://arxiv.org/abs/2508.20534)
*Frederik Rajiv Manichand,Robin Deuber,Robert Jakob,Steve Swerling,Jamie Rosen,Elgar Fleisch,Patrick Langer*

Main category: cs.CV

TL;DR: 本研究提出了一种基于深度学习的BMI估算方法，利用包含8万多张图像的大型WayBED数据集进行训练，并通过自动过滤提高了数据质量。该方法在多个数据集上均取得了SOTA性能，并可在Android设备上部署。


<details>
  <summary>Details</summary>
Motivation: 当传统BMI测量方法不可用或不实际时（例如远程医疗、紧急情况），通过相机图像快速评估BMI至关重要。现有计算机视觉方法受限于小规模数据集。

Method: 本研究提出一种基于深度学习的BMI估算方法，使用专有的WayBED数据集（84,963张图像）进行训练。引入了基于姿态聚类和人物检测的自动过滤方法，以去除低质量图像，保留了71,322张高质量图像。该方法完整流程已使用CLAID框架部署到Android设备上，并开源了所有相关代码。

Result: 在WayBED测试集上实现了7.9%的平均绝对百分比误差（MAPE），据称是公开文献中的最低值。在完全未见的VisualBodyToBMI数据集上取得了13%的MAPE，与现有SOTA方法相当，证明了稳健的泛化能力。在VisualBodyToBMI数据集上进行微调后，达到8.56%的MAPE，是迄今为止该数据集上的最低值。

Conclusion: 本研究通过使用大规模高质量数据集和有效的数据过滤方法，显著提升了基于图像的BMI估算性能，在多个数据集上取得了最先进的结果，并展现了良好的泛化能力和实际部署潜力。

Abstract: Estimating Body Mass Index (BMI) from camera images with machine learning
models enables rapid weight assessment when traditional methods are unavailable
or impractical, such as in telehealth or emergency scenarios. Existing computer
vision approaches have been limited to datasets of up to 14,500 images. In this
study, we present a deep learning-based BMI estimation method trained on our
WayBED dataset, a large proprietary collection of 84,963 smartphone images from
25,353 individuals. We introduce an automatic filtering method that uses
posture clustering and person detection to curate the dataset by removing
low-quality images, such as those with atypical postures or incomplete views.
This process retained 71,322 high-quality images suitable for training. We
achieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test
set (WayBED data) using full-body images, the lowest value in the published
literature to the best of our knowledge. Further, we achieve a MAPE of 13% on
the completely unseen~(during training) VisualBodyToBMI dataset, comparable
with state-of-the-art approaches trained on it, demonstrating robust
generalization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a
MAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the
full pipeline, including image filtering and BMI estimation, on Android devices
using the CLAID framework. We release our complete code for model training,
filtering, and the CLAID package for mobile deployment as open-source
contributions.

</details>


### [85] [Domain Adaptation Techniques for Natural and Medical Image Classification](https://arxiv.org/abs/2508.20537)
*Ahmad Chaddad,Yihang Wu,Reem Kateb,Christian Desrosiers*

Main category: cs.CV

TL;DR: 本研究通过557次模拟实验，评估了七种主流域适应技术在自然图像和医学图像分类中的表现，特别强调了DSAN算法在性能和可解释性方面的优异表现。


<details>
  <summary>Details</summary>
Motivation: 域适应技术在自然图像领域进展较多，但在医学图像应用中面临挑战，且现有研究可能存在性能偏差。因此，本研究旨在更好地理解域适应技术在自然和医学图像中的益处。

Method: 进行了557次模拟研究，采用了七种广泛使用的域适应技术，在五种自然图像数据集和八种医学图像数据集上进行图像分类，涵盖了分布外、动态数据流和有限训练样本等多种场景。

Result: 实验揭示了域适应技术的性能和医学适用性。Deep Subdomain Adaptation Network (DSAN) 算法表现突出，在COVID-19数据集上使用Resnet50达到了91.2%的分类准确率，并在动态数据流场景中相对基线提高了6.7%。DSAN在COVID-19和皮肤癌数据集上还表现出显著的可解释性。

Conclusion: 这些结果有助于加深对域适应技术的理解，并为模型有效适应医学数据提供了宝贵的见解。

Abstract: Domain adaptation (DA) techniques have the potential in machine learning to
alleviate distribution differences between training and test sets by leveraging
information from source domains. In image classification, most advances in DA
have been made using natural images rather than medical data, which are harder
to work with. Moreover, even for natural images, the use of mainstream datasets
can lead to performance bias. {With the aim of better understanding the
benefits of DA for both natural and medical images, this study performs 557
simulation studies using seven widely-used DA techniques for image
classification in five natural and eight medical datasets that cover various
scenarios, such as out-of-distribution, dynamic data streams, and limited
training samples.} Our experiments yield detailed results and insightful
observations highlighting the performance and medical applicability of these
techniques. Notably, our results have shown the outstanding performance of the
Deep Subdomain Adaptation Network (DSAN) algorithm. This algorithm achieved
feasible classification accuracy (91.2\%) in the COVID-19 dataset using
Resnet50 and showed an important accuracy improvement in the dynamic data
stream DA scenario (+6.7\%) compared to the baseline. Our results also
demonstrate that DSAN exhibits remarkable level of explainability when
evaluated on COVID-19 and skin cancer datasets. These results contribute to the
understanding of DA techniques and offer valuable insight into the effective
adaptation of models to medical data.

</details>


### [86] [Contrastive Learning through Auxiliary Branch for Video Object Detection](https://arxiv.org/abs/2508.20551)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: 针对视频目标检测中的图像退化问题，本文提出了一种名为CLAB（Contrastive Learning through Auxiliary Branch）的对比学习方法。CLAB通过辅助分支和动态损失权重策略，在不增加推理计算成本的情况下，显著提升了特征表示能力和检测性能，并在ImageNet VID数据集上达到了CNN模型的最新SOTA。


<details>
  <summary>Details</summary>
Motivation: 视频目标检测因运动模糊、遮挡、形变等图像退化问题而极具挑战性。现有方法虽能提高性能，但代价是增加了计算负载。本研究旨在不增加推理阶段的计算开销前提下，提高模型对图像退化的鲁棒性。

Method: 本文提出了一种辅助分支对比学习（CLAB）方法。具体包括：1) 引入一个使用对比损失的对比辅助分支，以增强视频目标检测器骨干网络的特征表示能力。2) 提出一种动态损失权重策略，在训练早期强调辅助特征学习，并在训练收敛时逐渐优先考虑检测任务。

Result: 通过全面的实验和消融研究，CLAB展现出稳定的性能提升。在ImageNet VID数据集上，使用ResNet-101和ResNeXt-101骨干网络，CLAB分别达到了84.0% mAP和85.2% mAP的性能。

Conclusion: CLAB在不依赖额外后处理方法的情况下，为CNN基视频目标检测模型实现了最先进的性能，且不增加推理时的计算开销，有效提高了模型对图像退化的鲁棒性。

Abstract: Video object detection is a challenging task because videos often suffer from
image deterioration such as motion blur, occlusion, and deformable shapes,
making it significantly more difficult than detecting objects in still images.
Prior approaches have improved video object detection performance by employing
feature aggregation and complex post-processing techniques, though at the cost
of increased computational demands. To improve robustness to image degradation
without additional computational load during inference, we introduce a
straightforward yet effective Contrastive Learning through Auxiliary Branch
(CLAB) method. First, we implement a constrastive auxiliary branch using a
contrastive loss to enhance the feature representation capability of the video
object detector's backbone. Next, we propose a dynamic loss weighting strategy
that emphasizes auxiliary feature learning early in training while gradually
prioritizing the detection task as training converges. We validate our approach
through comprehensive experiments and ablation studies, demonstrating
consistent performance gains. Without bells and whistles, CLAB reaches a
performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101,
respectively, on the ImageNet VID dataset, thus achieving state-of-the-art
performance for CNN-based models without requiring additional post-processing
methods.

</details>


### [87] [Towards Mechanistic Defenses Against Typographic Attacks in CLIP](https://arxiv.org/abs/2508.20570)
*Lorenz Hufe,Constantin Venhoff,Maximilian Dreyer,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.CV

TL;DR: 本研究分析了CLIP模型在排版攻击下的行为，发现特定注意力头会提取排版信息。基于此，提出了一种无需微调的防御方法，通过选择性消融这些注意力头来提高防御性能，并发布了更鲁棒的CLIP模型，适用于安全关键应用。


<details>
  <summary>Details</summary>
Motivation: 排版攻击（在图像中注入文本）会利用多模态系统，导致目标错误分类、恶意内容生成甚至视觉-语言模型越狱，对现有系统构成严重威胁。

Method: 首先，分析CLIP视觉编码器在排版攻击下的行为，定位到模型后半层中负责提取并传输排版信息到cls token的特定注意力头（即“排版电路”）。其次，基于这些发现，提出一种无需微调的防御方法，通过选择性地消融由这些注意力头组成的“排版电路”来防御CLIP模型。

Result: 该方法在ImageNet-100的排版变体上将性能提高了19.6%，同时对标准ImageNet-100的准确率影响不到1%。值得注意的是，这种无需训练的方法与当前依赖微调的最新排版防御方法相比仍具竞争力。

Conclusion: 本研究发布了一系列对排版攻击显著更鲁棒的“失读症”CLIP模型。这些模型可作为各种安全关键应用（其中文本操纵风险高于文本识别效用）的合适替代品。

Abstract: Typographic attacks exploit multi-modal systems by injecting text into
images, leading to targeted misclassifications, malicious content generation
and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP
vision encoders behave under typographic attacks, locating specialized
attention heads in the latter half of the model's layers that causally extract
and transmit typographic information to the cls token. Building on these
insights, we introduce a method to defend CLIP models against typographic
attacks by selectively ablating a typographic circuit, consisting of attention
heads. Without requiring finetuning, our method improves performance by up to
19.6% on a typographic variant of ImageNet-100, while reducing standard
ImageNet-100 accuracy by less than 1%. Notably, our training-free approach
remains competitive with current state-of-the-art typographic defenses that
rely on finetuning. To this end, we release a family of dyslexic CLIP models
which are significantly more robust against typographic attacks. These models
serve as suitable drop-in replacements for a broad range of safety-critical
applications, where the risks of text-based manipulation outweigh the utility
of text recognition.

</details>


### [88] [GLaRE: A Graph-based Landmark Region Embedding Network for Emotion Recognition](https://arxiv.org/abs/2508.20579)
*Debasis Maji,Debaditya Barman*

Main category: cs.CV

TL;DR: 本文提出了GLaRE，一种基于图神经网络的面部表情识别方法，通过构建商图处理面部关键点，并在AffectNet和FERG数据集上取得了优于现有基线的性能。


<details>
  <summary>Details</summary>
Motivation: 传统面部表情识别（FER）系统面临遮挡、表情多变性及缺乏可解释性等挑战。图神经网络（GNN）通过建模面部关键点间的关系依赖性，提供了一种结构化且可解释的学习方案，以克服这些局限。

Method: 本文提出GLaRE（Graph-based Landmark Region Embedding network），一种用于情感识别的新型网络。该方法首先使用3D面部对齐提取面部关键点，然后通过分层粗化构建商图，以在降低复杂度的同时保留空间结构。

Result: GLaRE在AffectNet数据集上达到了64.89%的准确率，在FERG数据集上达到了94.24%的准确率，性能优于多个现有基线。此外，消融研究表明商图中的区域级嵌入有助于提高预测性能。

Conclusion: GLaRE通过利用商图的区域级嵌入，成功提升了面部表情识别的性能，证明了其在解决传统FER挑战方面的有效性和优越性。

Abstract: Facial expression recognition (FER) is a crucial task in computer vision with
wide range of applications including human computer interaction, surveillance,
and assistive technologies. However, challenges such as occlusion, expression
variability, and lack of interpretability hinder the performance of traditional
FER systems. Graph Neural Networks (GNNs) offer a powerful alternative by
modeling relational dependencies between facial landmarks, enabling structured
and interpretable learning. In this paper, we propose GLaRE, a novel
Graph-based Landmark Region Embedding network for emotion recognition. Facial
landmarks are extracted using 3D facial alignment, and a quotient graph is
constructed via hierarchical coarsening to preserve spatial structure while
reducing complexity. Our method achieves 64.89 percentage accuracy on AffectNet
and 94.24 percentage on FERG, outperforming several existing baselines.
Additionally, ablation studies have demonstrated that region-level embeddings
from quotient graphs have contributed to improved prediction performance.

</details>


### [89] [FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models](https://arxiv.org/abs/2508.20586)
*Zheng Chong,Yanwei Lei,Shiyue Zhang,Zhuandi He,Zhen Wang,Xujie Zhang,Xiao Dong,Yiling Wu,Dongmei Jiang,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出FastFit框架，通过创新架构解决虚拟试穿中多参照物组合支持和效率低下问题，实现3.5倍加速并提高真实感，同时发布了新数据集DressCode-MR。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟试穿技术面临两大挑战：一是难以支持多参照物（包括服装和配饰）组合，二是去噪过程中参照物特征重复计算导致效率低下，阻碍了其在现实世界中的应用。

Method: 本文提出FastFit，一个基于新型可缓存扩散架构的高速多参照物虚拟试穿框架。通过采用半注意力机制，并用类别嵌入替换传统的 timestep 嵌入，模型将参照物特征编码与去噪过程完全解耦，使参照物特征只需计算一次即可在所有步骤中无损重用。此外，还引入了DressCode-MR，一个包含28,179套高质量配对图像的大规模多参照物虚拟试穿数据集，以促进复杂多参照物虚拟试穿的研究。

Result: FastFit在可比方法上实现了平均3.5倍的速度提升。在VITON-HD、DressCode和新发布的DressCode-MR数据集上进行的广泛实验表明，FastFit在关键真实感指标上超越了现有最先进方法，同时提供了显著的推理效率优势。

Conclusion: FastFit通过其创新的可缓存扩散架构，成功解决了虚拟试穿技术在支持多参照物组合和提升推理效率方面的核心挑战。它不仅显著提高了性能和速度，还通过新数据集的发布促进了该领域的研究发展。

Abstract: Despite its great potential, virtual try-on technology is hindered from
real-world application by two major challenges: the inability of current
methods to support multi-reference outfit compositions (including garments and
accessories), and their significant inefficiency caused by the redundant
re-computation of reference features in each denoising step. To address these
challenges, we propose FastFit, a high-speed multi-reference virtual try-on
framework based on a novel cacheable diffusion architecture. By employing a
Semi-Attention mechanism and substituting traditional timestep embeddings with
class embeddings for reference items, our model fully decouples reference
feature encoding from the denoising process with negligible parameter overhead.
This allows reference features to be computed only once and losslessly reused
across all steps, fundamentally breaking the efficiency bottleneck and
achieving an average 3.5x speedup over comparable methods. Furthermore, to
facilitate research on complex, multi-reference virtual try-on, we introduce
DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of
high-quality, paired images covering five key categories (tops, bottoms,
dresses, shoes, and bags), constructed through a pipeline of expert models and
human feedback refinement. Extensive experiments on the VITON-HD, DressCode,
and our DressCode-MR datasets show that FastFit surpasses state-of-the-art
methods on key fidelity metrics while offering its significant advantage in
inference efficiency.

</details>


### [90] [UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching](https://arxiv.org/abs/2508.20594)
*Yuqi Han,Songqian Zhang,Weijian Su,Ke Li,Jiayu Yang,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: 本文提出UTA-Sign，一种无监督热-事件视频增强方法，通过融合热像仪和事件相机数据，解决了低光照下热像仪识别交通标志困难及事件相机采样不均匀的问题，显著提高了交通标志的描绘质量和检测精度，适用于夜间自动驾驶。


<details>
  <summary>Details</summary>
Motivation: 热像仪在低光照下感知能力强，但在捕捉由相似材料制成的交通标志时面临挑战，对自动驾驶语义理解构成安全风险。事件相机虽在高速低光照环境有效，但存在非均匀采样问题。因此，研究旨在结合两种模态的互补特性，克服各自局限性，提高低光照环境下交通标志的准确感知。

Method: 本文提出UTA-Sign，一种针对低光照交通标志（如车牌、路障指示器）的无监督热-事件视频增强方法。该方法开发了一个双重增强机制，融合热帧和事件信号以实现时间上一致的交通标志表示。具体而言，热帧提供准确的运动线索作为时间参考来对齐不均匀的事件信号，同时事件信号为原始热帧补充细微的标志内容，增强环境理解。

Result: 该方法在真实世界场景收集的数据集上进行了验证，结果表明，在交通标志描绘方面达到了卓越的质量，并提高了感知层面的检测精度。

Conclusion: 通过有效融合热像仪和事件相机的互补优势，UTA-Sign成功解决了单一传感器在低光照下识别交通标志的局限性，实现了对交通标志更清晰、更准确的感知和检测，为自动驾驶系统提供了更强的环境理解能力。

Abstract: The thermal camera excels at perceiving outdoor environments under low-light
conditions, making it ideal for applications such as nighttime autonomous
driving and unmanned navigation. However, thermal cameras encounter challenges
when capturing signage from objects made of similar materials, which can pose
safety risks for accurately understanding semantics in autonomous driving
systems. In contrast, the neuromorphic vision camera, also known as an event
camera, detects changes in light intensity asynchronously and has proven
effective in high-speed, low-light traffic environments. Recognizing the
complementary characteristics of these two modalities, this paper proposes
UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage
in low-illumination environments, targeting elements such as license plates and
roadblock indicators. To address the signage blind spots of thermal imaging and
the non-uniform sampling of event cameras, we developed a dual-boosting
mechanism that fuses thermal frames and event signals for consistent signage
representation over time. The proposed method utilizes thermal frames to
provide accurate motion cues as temporal references for aligning the uneven
event signals. At the same time, event signals contribute subtle signage
content to the raw thermal frames, enhancing the overall understanding of the
environment. The proposed method is validated on datasets collected from
real-world scenarios, demonstrating superior quality in traffic signage
sketching and improved detection accuracy at the perceptual level.

</details>


### [91] [Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations](https://arxiv.org/abs/2508.20595)
*Mengxiao Huang,Minglei Shu,Shuwang Zhou,Zhaoyang Liu*

Main category: cs.CV

TL;DR: 一种基于低频感知扰动的主动防御方法，通过直接干扰深度伪造生成过程来降低换脸效果，同时保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法多为被动式，无法预防攻击。本研究旨在开发一种主动防御策略，以直接阻止换脸技术的生成过程。

Method: 提出一种基于低频感知扰动的主动防御方法。该方法设计了包含编码器、扰动生成器和解码器的完整架构，利用离散小波变换（DWT）提取低频分量以生成扰动。它直接针对深度伪造的生成过程，结合频率和空间域特征引入低频扰动，同时保留高频细节以维持视觉真实性。

Result: 在CelebA-HQ和LFW数据集上的实验表明，该方法显著降低了换脸的有效性，提高了防御成功率，并能有效保持图像的视觉质量。

Conclusion: 本文提出了一种有效的主动防御方法，通过干扰深度伪造生成过程来对抗换脸技术，该方法在降低攻击效果的同时，成功地维持了内容的视觉可信度。

Abstract: Deepfake technology, driven by Generative Adversarial Networks (GANs), poses
significant risks to privacy and societal security. Existing detection methods
are predominantly passive, focusing on post-event analysis without preventing
attacks. To address this, we propose an active defense method based on
low-frequency perceptual perturbations to disrupt face swapping manipulation,
reducing the performance and naturalness of generated content. Unlike prior
approaches that used low-frequency perturbations to impact classification
accuracy,our method directly targets the generative process of deepfake
techniques. We combine frequency and spatial domain features to strengthen
defenses. By introducing artifacts through low-frequency perturbations while
preserving high-frequency details, we ensure the output remains visually
plausible. Additionally, we design a complete architecture featuring an
encoder, a perturbation generator, and a decoder, leveraging discrete wavelet
transform (DWT) to extract low-frequency components and generate perturbations
that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW
demonstrate significant reductions in face-swapping effectiveness, improved
defense success rates, and preservation of visual quality.

</details>


### [92] [Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion](https://arxiv.org/abs/2508.20604)
*Zheng Qin,Yabing Wang,Minghui Yang,Sanping Zhou,Ming Yang,Le Wang*

Main category: cs.CV

TL;DR: 本文提出Diverse-T2M方法，通过引入不确定性建模（利用噪声信号和连续潜在空间采样）来显著提高文本到3D人体动作生成的多样性，同时保持最先进的文本一致性性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作生成方法虽然在动作精确度和质量上有所进步，但在生成动作的多样性方面仍面临显著挑战。

Method: 提出Diverse-T2M方法，通过引入不确定性来提高生成多样性。具体方法包括：1) 利用噪声信号作为多样性信息的载体，在Transformer模型中明确建模不确定性；2) 构建一个将文本投影到连续表示的潜在空间，并集成一个潜在空间采样器，以引入随机采样来增强输出的多样性和不确定性。

Result: 在HumanML3D和KIT-ML基准数据集上的实验结果表明，该方法显著增强了生成动作的多样性，同时在文本一致性方面保持了最先进的性能。

Conclusion: Diverse-T2M成功克服了文本到动作生成中的多样性挑战，通过不确定性建模实现了高多样性与文本一致性的平衡。

Abstract: Generating 3D human motions from text is a challenging yet valuable task. The
key aspects of this task are ensuring text-motion consistency and achieving
generation diversity. Although recent advancements have enabled the generation
of precise and high-quality human motions from text, achieving diversity in the
generated motions remains a significant challenge. In this paper, we aim to
overcome the above challenge by designing a simple yet effective text-to-motion
generation method, \textit{i.e.}, Diverse-T2M. Our method introduces
uncertainty into the generation process, enabling the generation of highly
diverse motions while preserving the semantic consistency of the text.
Specifically, we propose a novel perspective that utilizes noise signals as
carriers of diversity information in transformer-based methods, facilitating a
explicit modeling of uncertainty. Moreover, we construct a latent space where
text is projected into a continuous representation, instead of a rigid
one-to-one mapping, and integrate a latent space sampler to introduce
stochastic sampling into the generation process, thereby enhancing the
diversity and uncertainty of the outputs. Our results on text-to-motion
generation benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our
method significantly enhances diversity while maintaining state-of-the-art
performance in text consistency.

</details>


### [93] [Optimization-Based Calibration for Intravascular Ultrasound Volume Reconstruction](https://arxiv.org/abs/2508.20605)
*Karl-Philippe Beaudet,Sidaty El Hadramy,Philippe C Cattin,Juan Verde,Stéphane Cotin*

Main category: cs.CV

TL;DR: 本研究提出一种基于优化的校准方法，利用3D打印模型实现精确的3D血管内超声(IVUS)体积重建，以将术中IVUS数据与术前CT图像进行配准，从而增强肝脏手术中的导航。


<details>
  <summary>Details</summary>
Motivation: 肝脏手术中术中超声图像因视野有限和解剖结构复杂而难以解释。有效的手术指导需要弥合术前与术中数据之间的鸿沟。3D IVUS通过器官重建，有望实现术前CT与术中IVUS图像的配准。

Method: 提出了一种基于优化的校准方法，利用一个3D打印模型进行校准，以实现精确的3D IVUS体积重建。该方法旨在确保跟踪的IVUS数据与术前CT图像的精确对齐。

Result: 该方法在活体猪肝图像上进行了验证。校准误差范围为0.88至1.80毫米，3D IVUS数据与相应CT扫描之间的配准误差范围为3.40至5.71毫米。

Conclusion: 该方法提供了一种可靠且准确的校准和体积重建手段，可用于肝脏手术中将术中超声图像与术前CT图像进行配准，从而增强术中指导。

Abstract: Intraoperative ultrasound images are inherently challenging to interpret in
liver surgery due to the limited field of view and complex anatomical
structures. Bridging the gap between preoperative and intraoperative data is
crucial for effective surgical guidance. 3D IntraVascular UltraSound (IVUS)
offers a potential solution by enabling the reconstruction of the entire organ,
which facilitates registration between preoperative computed tomography (CT)
scans and intraoperative IVUS images. In this work, we propose an
optimization-based calibration method using a 3D-printed phantom for accurate
3D Intravascular Ultrasound volume reconstruction. Our approach ensures precise
alignment of tracked IVUS data with preoperative CT images, improving
intraoperative navigation. We validated our method using in vivo swine liver
images, achieving a calibration error from 0.88 to 1.80 mm and a registration
error from 3.40 to 5.71 mm between the 3D IVUS data and the corresponding CT
scan. Our method provides a reliable and accurate means of calibration and
volume reconstruction. It can be used to register intraoperative ultrasound
images with preoperative CT images in the context of liver surgery, and enhance
intraoperative guidance.

</details>


### [94] [Physics Informed Generative Models for Magnetic Field Images](https://arxiv.org/abs/2508.20612)
*Aye Phyu Phyu Aung,Lucas Lum,Zhansen Shi,Wen Qiu,Bernice Zee,JM Chin,Yeow Kheng Lim,J. Senthilnath*

Main category: cs.CV

TL;DR: 为解决半导体制造中磁场图像（MFI）数据集稀缺导致ML模型训练受阻的问题，本文提出PI-GenMFI模型，利用带物理约束的扩散模型生成合成MFI图像以训练ML算法定位缺陷，并取得了 promising 的评估结果。


<details>
  <summary>Details</summary>
Motivation: 半导体缺陷检测中，X射线效率低下，MFI能高效定位感兴趣区域。然而，由于专有性，可用的MFI数据集有限，严重制约了机器学习模型在缺陷定位中的应用。

Method: 提出Physics Informed Generative Models for Magnetic Field Images (PI-GenMFI) 模型。该方法是一个基于扩散模型的机器学习方法，整合了两个物理约束，用于生成电源短路等常见缺陷的合成MFI图像，以作为训练数据来提升ML算法的缺陷定位效率。

Result: PI-GenMFI模型生成的MFI图像获得了领域专家的评估认可，并通过与SOTA生成模型（VAE和扩散模型）进行定性和定量比较，在图像生成和信号处理的各项指标上展现出有前景的结果。

Conclusion: PI-GenMFI模型通过有效生成高质量的合成MFI图像，成功克服了MFI数据稀缺的瓶颈，有望显著优化半导体缺陷的定位过程。

Abstract: In semiconductor manufacturing, defect detection and localization are
critical to ensuring product quality and yield. While X-ray imaging is a
reliable non-destructive testing method, it is memory-intensive and
time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a
more efficient means to localize regions of interest (ROI) for targeted X-ray
scanning. However, the limited availability of MFI datasets due to proprietary
concerns presents a significant bottleneck for training machine learning (ML)
models using MFI. To address this challenge, we consider an ML-driven approach
leveraging diffusion models with two physical constraints. We propose Physics
Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate
synthetic MFI samples by integrating specific physical information. We generate
MFI images for the most common defect types: power shorts. These synthetic
images will serve as training data for ML algorithms designed to localize
defect areas efficiently. To evaluate generated MFIs, we compare our model to
SOTA generative models from both variational autoencoder (VAE) and diffusion
methods. We present a domain expert evaluation to assess the generated samples.
In addition, we present qualitative and quantitative evaluation using various
metrics used for image generation and signal processing, showing promising
results to optimize the defect localization process.

</details>


### [95] [Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization](https://arxiv.org/abs/2508.20613)
*Yixiang Qiu,Yanhan Liu,Hongyao Yu,Hao Fang,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: 分布式推理（SI）存在数据重构攻击（DRA）带来的隐私风险。本文提出了一种基于GAN且带有渐进式特征优化（PFO）的新型DRA框架，显著提升了在复杂DNNs和高分辨率场景下敏感输入的重构质量和泛化性。


<details>
  <summary>Details</summary>
Motivation: 分布式推理（SI）虽能降低延迟和保护隐私，但其中间特征易受数据重构攻击（DRA），构成严重隐私风险。现有DRA对浅层模型有效，但重构质量和泛化性有限，未能充分利用语义先验，难以攻击复杂DNNs。

Method: 本文提出一种新颖的基于GAN的渐进式特征优化（PFO）数据重构攻击框架。该框架将生成器分解为分层块，并逐步优化中间表示以提高重构图像的语义保真度。为稳定优化并增强图像真实感，引入了L1-ball约束。

Result: 实验结果表明，该方法在重构质量上显著优于现有攻击，特别是在高分辨率场景、域外设置以及面对更深、更复杂的DNNs时表现出更强的攻击能力。

Conclusion: 提出的GAN-based PFO攻击框架成功克服了现有数据重构攻击的局限性，在多种复杂和挑战性场景下展示了卓越的敏感数据重构能力，进一步揭示了分布式推理（SI）中的隐私泄露风险。

Abstract: The growing complexity of Deep Neural Networks (DNNs) has led to the adoption
of Split Inference (SI), a collaborative paradigm that partitions computation
between edge devices and the cloud to reduce latency and protect user privacy.
However, recent advances in Data Reconstruction Attacks (DRAs) reveal that
intermediate features exchanged in SI can be exploited to recover sensitive
input data, posing significant privacy risks. Existing DRAs are typically
effective only on shallow models and fail to fully leverage semantic priors,
limiting their reconstruction quality and generalizability across datasets and
model architectures. In this paper, we propose a novel GAN-based DRA framework
with Progressive Feature Optimization (PFO), which decomposes the generator
into hierarchical blocks and incrementally refines intermediate representations
to enhance the semantic fidelity of reconstructed images. To stabilize the
optimization and improve image realism, we introduce an L1-ball constraint
during reconstruction. Extensive experiments show that our method outperforms
prior attacks by a large margin, especially in high-resolution scenarios,
out-of-distribution settings, and against deeper and more complex DNNs.

</details>


### [96] [EmoCAST: Emotional Talking Portrait via Emotive Text Description](https://arxiv.org/abs/2508.20615)
*Yiguo Jiang,Xiaodong Cun,Yong Zhang,Yudian Zheng,Fan Tang,Chi-Man Pun*

Main category: cs.CV

TL;DR: EmoCAST是一个基于扩散模型的框架，通过引入文本引导情感模块、情感音频注意力模块、新的情感说话头数据集及优化训练策略，实现了文本驱动、情感丰富、唇同步且具有SOTA性能的说话头视频合成。


<details>
  <summary>Details</summary>
Motivation: 现有情感说话头合成方法在控制灵活性、运动自然度、表情质量方面存在局限，且数据集多为实验室环境收集，这些缺点严重阻碍了其在真实世界场景中的实际应用。

Method: 提出EmoCAST，一个扩散模型框架，包含：1) 文本引导解耦情感模块，通过情感提示增强空间知识以改善情绪理解。2) 情感音频注意力模块，捕捉受控情感与驱动音频的相互作用，生成情感感知特征以指导精确面部运动合成。3) 构建了一个包含全面情感文本描述的情感说话头数据集。4) 提出情感感知采样训练策略和渐进功能训练策略，以提高模型捕捉细微表情特征和实现准确唇同步的能力。

Result: EmoCAST在生成真实、情感丰富和音频同步的说话头视频方面取得了最先进的性能。

Conclusion: EmoCAST通过创新的模块设计、新数据集构建和优化的训练策略，成功克服了现有情感说话头合成方法的局限，显著提升了生成视频的真实感、情感表现力和音频同步性，达到了当前最佳水平。

Abstract: Emotional talking head synthesis aims to generate talking portrait videos
with vivid expressions. Existing methods still exhibit limitations in control
flexibility, motion naturalness, and expression quality. Moreover, currently
available datasets are primarily collected in lab settings, further
exacerbating these shortcomings. Consequently, these limitations substantially
hinder practical applications in real-world scenarios. To address these
challenges, we propose EmoCAST, a diffusion-based framework with two key
modules for precise text-driven emotional synthesis. In appearance modeling,
emotional prompts are integrated through a text-guided decoupled emotive
module, enhancing the spatial knowledge to improve emotion comprehension. To
improve the relationship between audio and emotion, we introduce an emotive
audio attention module to capture the interplay between controlled emotion and
driving audio, generating emotion-aware features to guide more precise facial
motion synthesis. Additionally, we construct an emotional talking head dataset
with comprehensive emotive text descriptions to optimize the framework's
performance. Based on the proposed dataset, we propose an emotion-aware
sampling training strategy and a progressive functional training strategy that
further improve the model's ability to capture nuanced expressive features and
achieve accurate lip-synchronization. Overall, EmoCAST achieves
state-of-the-art performance in generating realistic, emotionally expressive,
and audio-synchronized talking-head videos. Project Page:
https://github.com/GVCLab/EmoCAST

</details>


### [97] [Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI Classification](https://arxiv.org/abs/2508.20621)
*Smriti Joshi,Lidia Garrucho,Richard Osuala,Oliver Diaz,Karim Lekadir*

Main category: cs.CV

TL;DR: 针对乳腺癌MRI检测，开发了基于SwinUNETR的深度学习框架，并在ODELIA多中心挑战赛中获得第二名，展示了其临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是女性癌症相关死亡的主要原因，早期检测至关重要。MRI对高风险或乳腺致密女性的乳腺癌检测高度敏感，但乳腺X线摄影效果不佳。ODELIA联盟发起了AI挑战赛，旨在推动AI辅助乳腺癌诊断与分类解决方案。

Method: 团队参与了ODELIA多中心挑战赛，使用了包含511份来自六个欧洲中心的多厂商1.5 T和3 T MRI扫描数据集，该数据集已标注左、右乳腺为无病灶、良性病灶或恶性病灶。研究开发了基于SwinUNETR的深度学习框架，并结合了乳腺区域掩膜、大量数据增强和集成学习策略，以提高模型的鲁棒性和泛化能力。

Result: 所提出的深度学习方法在ODELIA挑战赛排行榜上获得了第二名。

Conclusion: 该深度学习框架在支持临床乳腺MRI解读方面显示出巨大潜力，且其代码已公开共享，有助于进一步的研究和应用。

Abstract: Breast cancer is one of the leading causes of cancer-related mortality in
women, and early detection is essential for improving outcomes. Magnetic
resonance imaging (MRI) is a highly sensitive tool for breast cancer detection,
particularly in women at high risk or with dense breast tissue, where
mammography is less effective. The ODELIA consortium organized a multi-center
challenge to foster AI-based solutions for breast cancer diagnosis and
classification. The dataset included 511 studies from six European centers,
acquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study
was labeled for the left and right breast as no lesion, benign lesion, or
malignant lesion. We developed a SwinUNETR-based deep learning framework that
incorporates breast region masking, extensive data augmentation, and ensemble
learning to improve robustness and generalizability. Our method achieved second
place on the challenge leaderboard, highlighting its potential to support
clinical breast MRI interpretation. We publicly share our codebase at
https://github.com/smriti-joshi/bcnaim-odelia-challenge.git.

</details>


### [98] [AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images](https://arxiv.org/abs/2508.20623)
*Shiqi Xin,Xiaolin Zhang,Yanbin Liu,Peng Zhang,Caifeng Shan*

Main category: cs.CV

TL;DR: AvatarBack是一个即插即用框架，通过引入Subject-specific Generator和Adaptive Spatial Alignment Strategy，解决了3D高斯Splatting头部虚拟形象重建中背部区域质量差的问题，显著提升了完整性和真实感。


<details>
  <summary>Details</summary>
Motivation: 现有高斯Splatting方法主要依赖正面图像，导致头部背部区域重建质量差，存在几何不一致、结构模糊和真实感降低等问题，限制了重建虚拟形象的保真度。

Method: 本文提出了AvatarBack框架，包含两项核心创新：1) Subject-specific Generator (SSG)：利用生成式先验从稀疏正面输入合成身份一致的背面伪图像，提供多视图监督。2) Adaptive Spatial Alignment Strategy (ASA)：采用可学习的变换矩阵，在训练期间优化，以实现合成视图与3D高斯表示之间的精确几何对齐。

Result: 在NeRSemble和K-hairstyle数据集上的广泛实验表明，AvatarBack显著提升了头部背部重建质量，同时保持了正面区域的保真度。重建的虚拟形象在各种动作下仍保持一致的视觉真实感，并且完全可动画化。

Conclusion: AvatarBack成功解决了3D高斯Splatting头部虚拟形象重建中背部区域的缺失问题，通过显式建模缺失区域并引入SSG和ASA，实现了完整且一致的3D高斯虚拟形象，提高了整体重建质量和动画性能。

Abstract: Recent advances in Gaussian Splatting have significantly boosted the
reconstruction of head avatars, enabling high-quality facial modeling by
representing an 3D avatar as a collection of 3D Gaussians. However, existing
methods predominantly rely on frontal-view images, leaving the back-head poorly
constructed. This leads to geometric inconsistencies, structural blurring, and
reduced realism in the rear regions, ultimately limiting the fidelity of
reconstructed avatars. To address this challenge, we propose AvatarBack, a
novel plug-and-play framework specifically designed to reconstruct complete and
consistent 3D Gaussian avatars by explicitly modeling the missing back-head
regions. AvatarBack integrates two core technical innovations,i.e., the
Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy
(ASA). The former leverages a generative prior to synthesize
identity-consistent, plausible back-view pseudo-images from sparse frontal
inputs, providing robust multi-view supervision. To achieve precise geometric
alignment between these synthetic views and the 3D Gaussian representation, the
later employs learnable transformation matrices optimized during training,
effectively resolving inherent pose and coordinate discrepancies. Extensive
experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric,
photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack
significantly enhances back-head reconstruction quality while preserving
frontal fidelity. Moreover, the reconstructed avatars maintain consistent
visual realism under diverse motions and remain fully animatable.

</details>


### [99] [ArtFace: Towards Historical Portrait Face Identification via Model Adaptation](https://arxiv.org/abs/2508.20626)
*Francois Poh,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: 针对历史画作中的人物识别难题，本文提出通过微调基础模型并结合传统面部识别网络的嵌入，显著提升了识别准确率，解决了传统方法因领域漂移而失效的问题。


<details>
  <summary>Details</summary>
Motivation: 艺术史学家识别历史画作中的人物具有重要意义，但现有方法主观且受数据和风格限制。传统面部识别模型在处理画作时因领域漂移、类内差异大及艺术因素（风格、技巧、意图等）而表现不佳，因此需要更有效、自动化的解决方案。

Method: 本文研究了基础模型在艺术品面部识别中的潜力。具体方法是微调基础模型，并将其嵌入（embeddings）与传统面部识别网络的嵌入相结合。

Result: 实验结果表明，通过该方法比现有最先进的方法有显著改进。基础模型能够弥补传统方法在画作识别中失效的空白。

Conclusion: 基础模型在历史画作面部识别方面显示出巨大潜力，能够有效克服传统方法在该领域面临的挑战和局限性。

Abstract: Identifying sitters in historical paintings is a key task for art historians,
offering insight into their lives and how they chose to be seen. However, the
process is often subjective and limited by the lack of data and stylistic
variations. Automated facial recognition is capable of handling challenging
conditions and can assist, but while traditional facial recognition models
perform well on photographs, they struggle with paintings due to domain shift
and high intra-class variation. Artistic factors such as style, skill, intent,
and influence from other works further complicate recognition. In this work, we
investigate the potential of foundation models to improve facial recognition in
artworks. By fine-tuning foundation models and integrating their embeddings
with those from conventional facial recognition networks, we demonstrate
notable improvements over current state-of-the-art methods. Our results show
that foundation models can bridge the gap where traditional methods are
ineffective. Paper page at https://www.idiap.ch/paper/artface/

</details>


### [100] [CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models](https://arxiv.org/abs/2508.20640)
*Ayan Banerjee,Fernando Vilariño,Josep Lladós*

Main category: cs.CV

TL;DR: CraftGraffiti是一种文本引导的涂鸦生成框架，通过“先风格后身份”的方法，有效解决了在极端风格化过程中人脸身份保持的难题，同时实现了姿态定制，并在现实世界中展现了创造性影响。


<details>
  <summary>Details</summary>
Motivation: 在生成艺术中，极端风格化（如涂鸦）下保持人脸身份是一个重大挑战。涂鸦这种高对比度、抽象的媒介中，对面部特征的微小扭曲都可能导致主体失去辨识度，从而损害个人和文化真实性。

Method: 该框架名为CraftGraffiti，是一个端到端的文本引导涂鸦生成系统。它首先通过LoRA微调的预训练扩散Transformer进行涂鸦风格迁移，然后通过结合显式身份嵌入的人脸一致自注意力机制来确保身份保真度。姿态定制通过CLIP引导的提示扩展实现，无需关键点即可动态调整姿态并保持面部连贯性。研究证明了“先风格后身份”范式能有效减少属性漂移。

Result: 定量结果表明，CraftGraffiti在面部特征一致性方面具有竞争力，并在美学和人类偏好得分上达到了最先进水平。定性分析和在Cruilla音乐节的实际部署也突显了该系统在现实世界中的创造性影响。

Conclusion: CraftGraffiti推动了尊重身份的AI辅助艺术创作，为在创意AI应用中融合风格自由和可识别性提供了一种原则性方法，具有重要的理论和实际意义。

Abstract: Preserving facial identity under extreme stylistic transformation remains a
major challenge in generative art. In graffiti, a high-contrast, abstract
medium, subtle distortions to the eyes, nose, or mouth can erase the subject's
recognizability, undermining both personal and cultural authenticity. We
present CraftGraffiti, an end-to-end text-guided graffiti generation framework
designed with facial feature preservation as a primary objective. Given an
input image and a style and pose descriptive prompt, CraftGraffiti first
applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion
transformer, then enforces identity fidelity through a face-consistent
self-attention mechanism that augments attention layers with explicit identity
embeddings. Pose customization is achieved without keypoints, using CLIP-guided
prompt extension to enable dynamic re-posing while retaining facial coherence.
We formally justify and empirically validate the "style-first, identity-after"
paradigm, showing it reduces attribute drift compared to the reverse order.
Quantitative results demonstrate competitive facial feature consistency and
state-of-the-art aesthetic and human preference scores, while qualitative
analyses and a live deployment at the Cruilla Festival highlight the system's
real-world creative impact. CraftGraffiti advances the goal of
identity-respectful AI-assisted artistry, offering a principled approach for
blending stylistic freedom with recognizability in creative AI applications.

</details>


### [101] [Improving Alignment in LVLMs with Debiased Self-Judgment](https://arxiv.org/abs/2508.20655)
*Sihan Yang,Chenhang Cui,Zihao Zhao,Yiyang Zhou,Weilong Yan,Ying Wei,Huaxiu Yao*

Main category: cs.CV

TL;DR: 针对大型视觉-语言模型（LVLMs）中的模态对齐挑战（如幻觉和安全问题），本文提出一种无需外部资源的去偏自判断分数（debiased self-judgment score）方法，使模型能自主改进对齐。该方法通过优化解码策略和偏好微调，有效减少幻觉，提升安全性和整体能力，并实证优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）和大型视觉-语言模型（LVLMs）的快速发展，整合视觉和语言模态带来了新机遇。然而，有效对齐这些模态仍具挑战，常导致幻觉（生成输出与视觉输入不符）并引发安全担忧。此外，现有对齐方法（如指令微调、偏好微调）过度依赖外部数据集、人工标注或复杂后处理，导致可扩展性受限且成本高昂。

Method: 本文提出一种新颖方法，生成“去偏自判断分数”（debiased self-judgment score）。这是一个由模型内部自主创建、不依赖外部资源的自评估指标，旨在使模型能自主改进模态对齐。该方法通过增强解码策略和偏好微调过程来实施。

Result: 研究结果表明，本文方法有效减少了模型幻觉，增强了安全性，并提升了整体能力。实证结果也显示，该方法显著优于传统对齐方法。

Conclusion: 本文提出的方法为对齐大型视觉-语言模型提供了一个更有效、更具扩展性的解决方案，它通过模型内部自主评估机制，有效解决了幻觉和安全问题，并克服了传统方法对外部资源的依赖。

Abstract: The rapid advancements in Large Language Models (LLMs) and Large
Visual-Language Models (LVLMs) have opened up new opportunities for integrating
visual and linguistic modalities. However, effectively aligning these
modalities remains challenging, often leading to hallucinations--where
generated outputs are not grounded in the visual input--and raising safety
concerns across various domains. Existing alignment methods, such as
instruction tuning and preference tuning, often rely on external datasets,
human annotations, or complex post-processing, which limit scalability and
increase costs. To address these challenges, we propose a novel approach that
generates the debiased self-judgment score, a self-evaluation metric created
internally by the model without relying on external resources. This enables the
model to autonomously improve alignment. Our method enhances both decoding
strategies and preference tuning processes, resulting in reduced
hallucinations, enhanced safety, and improved overall capability. Empirical
results show that our approach significantly outperforms traditional methods,
offering a more effective solution for aligning LVLMs.

</details>


### [102] ["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](https://arxiv.org/abs/2508.20670)
*Anastasios Skoularikis,Stefanos-Iordanis Papadopoulos,Symeon Papadopoulos,Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: 本研究引入S-HArM数据集，并探索多模态提示策略以创建合成数据，旨在解决AI生成图像意图识别的挑战。结果显示，保留视觉上下文的数据泛化能力更强，但意图推断仍具挑战性，需专用架构。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态AI在检测合成及脱离上下文内容方面取得进展，但现有研究 largely overlook 了AI生成图像背后的意图，如幽默/讽刺、艺术或虚假信息。

Method: ['构建了S-HArM多模态数据集，包含9,576个来自Twitter/X和Reddit的“野外”图文对，并标注为幽默/讽刺、艺术或虚假信息。', '探索了三种提示策略（图像引导、描述引导和多模态引导），利用Stable Diffusion构建了大规模合成训练数据集。', '进行了一项广泛的比较研究，包括模态融合、对比学习、重建网络、注意力机制和大型视觉-语言模型。']

Result: 模型在图像引导和多模态引导数据上进行训练时，由于保留了视觉上下文，对“野外”内容的泛化能力更强。然而，总体性能仍然有限。

Conclusion: 推断图像意图的复杂性很高，需要专门的架构来进一步提升性能。S-HArM数据集和合成数据生成方法为该领域提供了新的研究方向。

Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic
and out-of-context content. However, existing efforts largely overlook the
intent behind AI-generated images. To fill this gap, we introduce S-HArM, a
multimodal dataset for intent-aware classification, comprising 9,576 "in the
wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,
or Misinformation. Additionally, we explore three prompting strategies
(image-guided, description-guided, and multimodally-guided) to construct a
large-scale synthetic training dataset with Stable Diffusion. We conduct an
extensive comparative study including modality fusion, contrastive learning,
reconstruction networks, attention mechanisms, and large vision-language
models. Our results show that models trained on image- and multimodally-guided
data generalize better to "in the wild" content, due to preserved visual
context. However, overall performance remains limited, highlighting the
complexity of inferring intent and the need for specialized architectures.

</details>


### [103] [MobileCLIP2: Improving Multi-Modal Reinforced Training](https://arxiv.org/abs/2508.20691)
*Fartash Faghri,Pavan Kumar Anasosalu Vasu,Cem Koc,Vaishaal Shankar,Alexander Toshev,Oncel Tuzel,Hadi Pouransari*

Main category: cs.CV

TL;DR: 本文通过改进多模态强化训练，推出了新的MobileCLIP2模型家族，在低延迟下实现了最先进的ImageNet-1k零样本准确率，并在性能、模型大小和延迟方面超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: MobileCLIP模型已能实现低延迟和高零样本准确率，但作者旨在通过优化其多模态强化训练方法，进一步提升模型的性能。

Method: 研究人员通过以下方法改进了MobileCLIP的多模态强化训练：1) 使用DFN数据集训练更好的CLIP教师模型集成；2) 改进了在DFN数据集上训练并在多样高质量图像-字幕数据集上微调的字幕生成器教师模型；3) 通过消融实验，探索了对比知识蒸馏中温度调优的重要性、字幕生成器微调对字幕多样性的有效性以及结合多个模型生成合成字幕的累加改进。

Result: 研究训练了新的MobileCLIP2模型家族，并在低延迟下实现了最先进的ImageNet-1k零样本准确率。具体而言，MobileCLIP2-B在ImageNet-1k准确率上比MobileCLIP-B提高了2.2%。MobileCLIP2-S4在ImageNet-1k上的零样本准确率与SigLIP-SO400M/14相当，但模型尺寸小2倍，并且在延迟方面比DFN ViT-L/14降低2.5倍。

Conclusion: 通过改进多模态强化训练，成功开发了MobileCLIP2系列模型，这些模型在保持低延迟的同时，在零样本准确率方面达到了新的SOTA水平，并在效率上显著优于现有模型。研究还发布了预训练模型和数据生成代码，便于社区使用和进一步研究。

Abstract: Foundation image-text models such as CLIP with zero-shot capabilities enable
a wide array of applications. MobileCLIP is a recent family of image-text
models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot
accuracy. The main ingredients in MobileCLIP were its low-latency and light
architectures and a novel multi-modal reinforced training that made knowledge
distillation from multiple caption-generators and CLIP teachers efficient,
scalable, and reproducible. In this paper, we improve the multi-modal
reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles
trained on the DFN dataset, 2) improved captioner teachers trained on the DFN
dataset and fine-tuned on a diverse selection of high-quality image-caption
datasets. We discover new insights through ablations such as the importance of
temperature tuning in contrastive knowledge distillation, the effectiveness of
caption-generator fine-tuning for caption diversity, and the additive
improvement from combining synthetic captions generated by multiple models. We
train a new family of models called MobileCLIP2 and achieve state-of-the-art
ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe
2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with
MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot
accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\times$ smaller and
improves on DFN ViT-L/14 at 2.5$\times$ lower latency. We release our
pretrained models (https://github.com/apple/ml-mobileclip) and the data
generation code (https://github.com/apple/ml-mobileclip-dr). The data
generation code makes it easy to create new reinforced datasets with arbitrary
teachers using distributed scalable processing.

</details>


### [104] [Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network](https://arxiv.org/abs/2508.20709)
*Chenhao Zhang,Wei Gao*

Main category: cs.CV

TL;DR: 本文提出了一种动态视频压缩框架，通过动态路由自编码器和码率控制代理，解决了神经视频压缩中精确码率控制的难题，实现了可变码率下的优异率失真性能和低码率误差。


<details>
  <summary>Details</summary>
Motivation: 近年来神经视频压缩(NVC)性能显著提升，但学习型编解码器在精确码率控制方面存在固有限制，仍是一个挑战。

Method: ['提出一个用于可变码率场景的动态视频压缩框架。', '设计“动态路由自编码器(Dynamic-Route Autoencoder)”，包含多条可变编码路由，每条路由具有部分计算复杂度并导向不同的率失真(RD)权衡。', '采用“码率控制代理(Rate Control Agent)”在运行时估算每条路由的码率并动态调整DRA的编码路由，以逼近目标码率。', '运用“联合路由优化策略(Joint-Routes Optimization)”实现各路由的协同训练，以覆盖广泛的可变码率范围并保持整体RD性能。']

Result: ['在HEVC和UVG数据集上的实验表明，相比最先进方法，平均BD-Rate降低14.8%，BD-PSNR增益0.47dB。', '同时，平均码率误差保持在1.66%。', '为各种码率和码率受限应用实现了率失真复杂度优化(RDCO)。']

Conclusion: 所提出的动态视频压缩框架成功解决了NVC中的精确码率控制问题，通过动态路由和智能控制实现了卓越的RD性能和极低的码率误差，为实际应用提供了有效的RDCO解决方案。

Abstract: Neural Video Compression (NVC) has achieved remarkable performance in recent
years. However, precise rate control remains a challenge due to the inherent
limitations of learning-based codecs. To solve this issue, we propose a dynamic
video compression framework designed for variable bitrate scenarios. First, to
achieve variable bitrate implementation, we propose the Dynamic-Route
Autoencoder with variable coding routes, each occupying partial computational
complexity of the whole network and navigating to a distinct RD trade-off.
Second, to approach the target bitrate, the Rate Control Agent estimates the
bitrate of each route and adjusts the coding route of DRA at run time. To
encompass a broad spectrum of variable bitrates while preserving overall RD
performance, we employ the Joint-Routes Optimization strategy, achieving
collaborative training of various routes. Extensive experiments on the HEVC and
UVG datasets show that the proposed method achieves an average BD-Rate
reduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods
while maintaining an average bitrate error of 1.66%, achieving
Rate-Distortion-Complexity Optimization (RDCO) for various bitrate and
bitrate-constrained applications. Our code is available at
https://git.openi.org.cn/OpenAICoding/DynamicDVC.

</details>


### [105] [CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network](https://arxiv.org/abs/2508.20734)
*Reza Akbari Movahed,Abuzar Rezaee,Arezoo Zakeri,Colin Berry,Edmond S. L. Ho,Ali Gooya*

Main category: cs.CV

TL;DR: CardioMorphNet是一个循环贝叶斯深度学习框架，用于基于心脏形状引导的形变配准，克服了现有方法对心脏解剖区域的忽视，并能估计运动不确定性，在心脏运动估计上表现卓越。


<details>
  <summary>Details</summary>
Motivation: 准确的心脏运动估计对评估心脏功能和检测异常至关重要。现有方法常依赖基于强度的图像配准相似性损失，可能忽视心脏解剖区域，导致运动捕获不准确。

Method: 提出CardioMorphNet，一个循环贝叶斯深度学习框架，用于3D心脏形状引导的形变配准，使用短轴（SAX）CMR图像。它采用循环变分自编码器建模心动周期内的时空依赖性，并使用两个后验模型进行双心室分割和运动估计。损失函数源于贝叶斯公式，通过递归配准分割图来关注解剖区域，不使用基于强度的图像配准相似性损失，同时利用序列SAX图像和时空特征。贝叶斯建模还能够计算估计运动场的不确定性图。

Result: 在UK Biobank数据集上验证，CardioMorphNet在心脏运动估计方面表现出卓越性能，优于现有最先进的方法。不确定性评估显示，与其它基于概率的心脏配准方法相比，它在心脏区域的估计运动场具有更低的不确定性值，表明其预测具有更高置信度。

Conclusion: CardioMorphNet通过关注解剖区域和量化不确定性，提供了一种有效且高置信度的心脏运动估计方法，解决了现有方法的局限性。

Abstract: Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR)
images is vital for assessing cardiac function and detecting its abnormalities.
Existing methods often struggle to capture heart motion accurately because they
rely on intensity-based image registration similarity losses that may overlook
cardiac anatomical regions. To address this, we propose CardioMorphNet, a
recurrent Bayesian deep learning framework for 3D cardiac shape-guided
deformable registration using short-axis (SAX) CMR images. It employs a
recurrent variational autoencoder to model spatio-temporal dependencies over
the cardiac cycle and two posterior models for bi-ventricular segmentation and
motion estimation. The derived loss function from the Bayesian formulation
guides the framework to focus on anatomical regions by recursively registering
segmentation maps without using intensity-based image registration similarity
loss, while leveraging sequential SAX volumes and spatio-temporal features. The
Bayesian modelling also enables computation of uncertainty maps for the
estimated motion fields. Validated on the UK Biobank dataset by comparing
warped mask shapes with ground truth masks, CardioMorphNet demonstrates
superior performance in cardiac motion estimation, outperforming
state-of-the-art methods. Uncertainty assessment shows that it also yields
lower uncertainty values for estimated motion fields in the cardiac region
compared with other probabilistic-based cardiac registration methods,
indicating higher confidence in its predictions.

</details>


### [106] [Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis Classification](https://arxiv.org/abs/2508.20745)
*Kaustubh Atey,Sameer Anand Jha,Gouranga Bala,Amit Sethi*

Main category: cs.CV

TL;DR: 针对病理图像中非典型有丝分裂像(AMFs)识别存在的域偏移问题，本文提出了一种训练时域鲁棒分类方法，结合风格扰动、特征对齐和EMA教师蒸馏，在MIDOG 2025挑战赛上取得了高性能。


<details>
  <summary>Details</summary>
Motivation: 非典型有丝分裂像（AMFs）是重要的组织病理学标志物，但在扫描仪、染色和采集差异导致的域偏移下，其识别一致性面临挑战。

Method: 该方法是一个训练时方案，包含三部分：1) 在骨干网络早期和中期插入风格扰动以增加特征多样性；2) 使用辅助对齐损失和粗略域标签（扫描仪、来源、物种、肿瘤）对注意力精炼特征进行跨站点对齐；3) 通过温度标度KL散度从EMA教师模型中蒸馏预测以稳定结果。

Result: 在MIDOG 2025初步排行榜上，该方法在非典型有丝分裂分类中获得了0.8762的平衡准确率、0.8873的敏感性、0.8651的特异性和0.9499的ROC AUC。

Conclusion: 该方法推理时开销可忽略不计，仅依赖粗略的域元数据，并实现了强大且平衡的性能，使其成为MIDOG 2025挑战赛的有力竞争者。

Abstract: Atypical mitotic figures (AMFs) are important histopathological markers yet
remain challenging to identify consistently, particularly under domain shift
stemming from scanner, stain, and acquisition differences. We present a simple
training-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2.
The approach (i) increases feature diversity via style perturbations inserted
at early and mid backbone stages, (ii) aligns attention-refined features across
sites using weak domain labels (Scanner, Origin, Species, Tumor) through an
auxiliary alignment loss, and (iii) stabilizes predictions by distilling from
an exponential moving average (EMA) teacher with temperature-scaled KL
divergence. On the organizer-run preliminary leaderboard for atypical mitosis
classification, our submission attains balanced accuracy of 0.8762, sensitivity
of 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs
negligible inference-time overhead, relies only on coarse domain metadata, and
delivers strong, balanced performance, positioning it as a competitive
submission for the MIDOG 2025 challenge.

</details>


### [107] [Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2508.20751)
*Yibin Wang,Zhimin Li,Yuhang Zang,Yujie Zhou,Jiazi Bu,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 针对T2I生成中GRPO方法存在的奖励作弊和基准评估粗糙问题，本文提出Pref-GRPO，一种基于成对偏好奖励的稳定优化方法，并引入UniGenBench，一个统一且细致的T2I性能评估基准。


<details>
  <summary>Details</summary>
Motivation: 1. 当前T2I生成中基于单点奖励模型（RM）的GRPO方法易受奖励作弊（reward hacking）影响，导致模型过度优化琐碎增益并训练不稳定。2. 现有T2I基准的评估标准过于粗糙，无法全面评估模型性能。

Method: 1. **Pref-GRPO**: 提出一种基于成对偏好奖励的GRPO方法。通过将优化目标从分数最大化转变为偏好拟合，使用图像的成对比较和胜率作为奖励信号，旨在实现更稳定的训练。2. **UniGenBench**: 构建了一个统一的T2I基准，包含600个提示词，横跨5个主要和20个子主题，并利用多模态大语言模型（MLLM）通过10个主要和27个子标准评估语义一致性。

Result: 1. Pref-GRPO能有效区分图像质量细微差异，提供更稳定的优势，并缓解奖励作弊问题。2. UniGenBench成功揭示了各类T2I模型的优缺点。3. 实验验证了Pref-GRPO的有效性。

Conclusion: 本文通过引入Pref-GRPO解决了GRPO-based T2I生成中的奖励作弊问题，并通过构建UniGenBench提供了更全面的模型评估框架。

Abstract: Recent advancements highlight the importance of GRPO-based reinforcement
learning methods and benchmarking in enhancing text-to-image (T2I) generation.
However, current methods using pointwise reward models (RM) for scoring
generated images are susceptible to reward hacking. We reveal that this happens
when minimal score differences between images are amplified after
normalization, creating illusory advantages that drive the model to
over-optimize for trivial gains, ultimately destabilizing the image generation
process. To address this, we propose Pref-GRPO, a pairwise preference
reward-based GRPO method that shifts the optimization objective from score
maximization to preference fitting, ensuring more stable training. In
Pref-GRPO, images are pairwise compared within each group using preference RM,
and the win rate is used as the reward signal. Extensive experiments
demonstrate that PREF-GRPO differentiates subtle image quality differences,
providing more stable advantages and mitigating reward hacking. Additionally,
existing T2I benchmarks are limited by coarse evaluation criteria, hindering
comprehensive model assessment. To solve this, we introduce UniGenBench, a
unified T2I benchmark comprising 600 prompts across 5 main themes and 20
subthemes. It evaluates semantic consistency through 10 primary and 27
sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our
benchmarks uncover the strengths and weaknesses of both open and closed-source
T2I models and validate the effectiveness of Pref-GRPO.

</details>


### [108] [${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting](https://arxiv.org/abs/2508.20754)
*Yuxi Hu,Jun Zhang,Kuangyi Chen,Zhe Zhang,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: 本文提出C3-GS框架，通过引入上下文感知、跨维度和跨尺度约束增强特征学习，解决了现有泛化高斯泼溅方法在稀疏视图下几何构造不准确的问题，实现了最先进的新视图合成质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的泛化高斯泼溅方法在预测高斯参数时，未能有效编码判别性、多视图一致的特征，导致在稀疏输入视图下难以构建准确的几何结构。

Method: 我们提出了C3-GS框架，通过整合上下文感知（context-aware）、跨维度（cross-dimension）和跨尺度（cross-scale）约束来增强特征学习。该架构将三个轻量级模块集成到统一的渲染管线中，以改进特征融合，并在无需额外监督的情况下实现真实感合成。

Result: 在基准数据集上的广泛实验验证了C3-GS实现了最先进的渲染质量和泛化能力。

Conclusion: C3-GS通过创新的特征学习范式（即上下文感知、跨维度、跨尺度约束），成功克服了现有泛化高斯泼溅方法在稀疏视图下特征编码不足的挑战，显著提升了新视图合成的渲染质量和泛化性能，达到了行业领先水平。

Abstract: Generalizable Gaussian Splatting aims to synthesize novel views for unseen
scenes without per-scene optimization. In particular, recent advancements
utilize feed-forward networks to predict per-pixel Gaussian parameters,
enabling high-quality synthesis from sparse input views. However, existing
approaches fall short in encoding discriminative, multi-view consistent
features for Gaussian predictions, which struggle to construct accurate
geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a
framework that enhances feature learning by incorporating context-aware,
cross-dimension, and cross-scale constraints. Our architecture integrates three
lightweight modules into a unified rendering pipeline, improving feature fusion
and enabling photorealistic synthesis without requiring additional supervision.
Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS
achieves state-of-the-art rendering quality and generalization ability. Code is
available at: https://github.com/YuhsiHu/C3-GS.

</details>


### [109] [SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding](https://arxiv.org/abs/2508.20758)
*Jiawen Lin,Shiran Bian,Yihang Zhu,Wenbin Tan,Yachao Zhang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: SeqVLM是一个零样本3D视觉定位框架，利用多视角真实场景图像序列和空间信息，解决了现有零样本方法在空间推理和上下文细节方面的局限性，并在ScanRefer和Nr3D基准上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然有监督的3D视觉定位（3DVG）方法在受限设置下精度较高，但它们需要场景特定训练，限制了实际应用。零样本3DVG在真实世界应用中潜力巨大，但现有零样本方法依赖单视角定位，导致空间推理受限、上下文信息缺失或细节退化。

Method: 本文提出SeqVLM，一个零样本3DVG框架。该方法首先通过3D语义分割网络生成3D实例候选，并通过语义过滤进行精炼。随后，采用提议引导的多视角投影策略，将候选提案投影到真实场景图像序列上，以在3D点云到图像的转换过程中保留空间关系和上下文细节。此外，为缓解VLM计算过载，引入了动态调度机制，迭代处理序列查询提示，利用VLM的跨模态推理能力识别文本指定对象。

Result: 在ScanRefer和Nr3D基准测试中，SeqVLM取得了最先进的性能，Acc@0.25分数分别达到55.6%和53.2%，相比先前的零样本方法分别提高了4.0%和5.2%。

Conclusion: SeqVLM框架的提出显著提高了3D视觉定位的泛化能力和实际应用潜力，使其更适用于真实世界场景。

Abstract: 3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using
natural language descriptions. Although supervised methods achieve higher
accuracy in constrained settings, zero-shot 3DVG holds greater promise for
real-world applications since eliminating scene-specific training requirements.
However, existing zero-shot methods face challenges of spatial-limited
reasoning due to reliance on single-view localization, and contextual omissions
or detail degradation. To address these issues, we propose SeqVLM, a novel
zero-shot 3DVG framework that leverages multi-view real-world scene images with
spatial information for target object reasoning. Specifically, SeqVLM first
generates 3D instance proposals via a 3D semantic segmentation network and
refines them through semantic filtering, retaining only semantic-relevant
candidates. A proposal-guided multi-view projection strategy then projects
these candidate proposals onto real scene image sequences, preserving spatial
relationships and contextual details in the conversion process of 3D point
cloud to images. Furthermore, to mitigate VLM computational overload, we
implement a dynamic scheduling mechanism that iteratively processes
sequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to
identify textually specified objects. Experiments on the ScanRefer and Nr3D
benchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores
of 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%,
respectively, which advance 3DVG toward greater generalization and real-world
applicability. The code is available at https://github.com/JiawLin/SeqVLM.

</details>


### [110] [Occlusion Robustness of CLIP for Military Vehicle Classification](https://arxiv.org/abs/2508.20760)
*Jan Erik van Woerden,Gertjan Burghouts,Lotte Nijskens,Alma M. Liezenga,Sabina van Rooij,Frank Ruis,Hugo J. Kuijf*

Main category: cs.CV

TL;DR: 本文研究了CLIP模型在军事场景下对抗遮挡的鲁棒性，发现基于Transformer的模型表现更优，细粒度遮挡影响更大，并通过微调模型骨干网络能显著提升模型抗遮挡能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（如CLIP）在数据稀缺的防御应用中具有零样本分类优势，但其在具有部分遮挡和信噪比下降的军事环境中的鲁棒性尚未得到充分探索。

Method: 研究者使用一个包含18类军用车辆的自定义数据集，评估了CLIP变体对遮挡的鲁棒性。评估指标是归一化曲线下面积（NAUC），并分析了不同遮挡百分比下的性能。

Result: (1) 基于Transformer的CLIP模型始终优于CNN模型。(2) 细粒度、分散的遮挡比大块连续遮挡更能降低模型性能。(3) 线性探测模型在约35%遮挡时性能急剧下降。(4) 通过微调模型骨干网络，性能下降的阈值提高到60%以上遮挡。

Conclusion: 研究结果强调了在训练过程中进行遮挡特定增强的重要性，并指出了需要进一步探索补丁级敏感性和架构弹性，以实现CLIP在真实世界中的部署。

Abstract: Vision-language models (VLMs) like CLIP enable zero-shot classification by
aligning images and text in a shared embedding space, offering advantages for
defense applications with scarce labeled data. However, CLIP's robustness in
challenging military environments, with partial occlusion and degraded
signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP
variants' robustness to occlusion using a custom dataset of 18 military vehicle
classes and evaluate using Normalized Area Under the Curve (NAUC) across
occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP
models consistently outperform CNNs, (2) fine-grained, dispersed occlusions
degrade performance more than larger contiguous occlusions, (3) despite
improved accuracy, performance of linear-probed models sharply drops at around
35% occlusion, (4) by finetuning the model's backbone, this performance drop
occurs at more than 60% occlusion. These results underscore the importance of
occlusion-specific augmentations during training and the need for further
exploration into patch-level sensitivity and architectural resilience for
real-world deployment of CLIP.

</details>


### [111] [SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer](https://arxiv.org/abs/2508.20762)
*Fachri Najm Noer Kartiman,Rasim,Yaya Wihardi,Nurul Hasanah,Oskar Natan,Bambang Wahono,Taufik Ibnu Salim*

Main category: cs.CV

TL;DR: 针对端到端自动驾驶模型，提出结合Swin Transformer与跳跃连接的SKGE-Swin架构，实现像素级上下文感知，并在CARLA平台获得优越驾驶分数。


<details>
  <summary>Details</summary>
Motivation: 开发一个具有像素级上下文感知能力的端到端自动驾驶模型，以更全面地理解车辆周边复杂环境，提高驾驶决策的准确性。

Method: 提出SKGE-Swin架构，该架构融合了Swin Transformer的Shifted Window-based Multi-head Self-Attention (SW-MSA)机制与跳跃阶段机制。此设计旨在扩展全局及不同网络层级的特征表示，使模型能从远距离像素中提取信息并保留关键特征。模型在CARLA平台的对抗性场景中进行评估，并计划进行消融研究。

Result: 实验结果表明，SKGE-Swin架构取得了比现有方法更高的驾驶分数（Driving Score）。

Conclusion: SKGE-Swin架构通过利用Swin Transformer和跳跃连接，有效提升了自动驾驶模型处理复杂环境信息和像素级上下文感知的性能，在模拟场景中表现出卓越的驾驶能力。

Abstract: Focusing on the development of an end-to-end autonomous vehicle model with
pixel-to-pixel context awareness, this research proposes the SKGE-Swin
architecture. This architecture utilizes the Swin Transformer with a skip-stage
mechanism to broaden feature representation globally and at various network
levels. This approach enables the model to extract information from distant
pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head
Self-Attention (SW-MSA) mechanism and to retain critical information from the
initial to the final stages of feature extraction, thereby enhancing its
capability to comprehend complex patterns in the vehicle's surroundings. The
model is evaluated on the CARLA platform using adversarial scenarios to
simulate real-world conditions. Experimental results demonstrate that the
SKGE-Swin architecture achieves a superior Driving Score compared to previous
methods. Furthermore, an ablation study will be conducted to evaluate the
contribution of each architectural component, including the influence of skip
connections and the use of the Swin Transformer, in improving model
performance.

</details>


### [112] [ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering](https://arxiv.org/abs/2508.21010)
*Paritosh Parmar,Eric Peh,Basura Fernando*

Main category: cs.CV

TL;DR: 本文提出一种模块化框架，通过引入自然语言因果链作为可解释的中间表示，将因果推理与答案生成解耦，显著提升了因果Why视频问答（VideoQA）的性能、可解释性、用户信任度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有因果Why视频问答模型难以进行高阶推理，其不透明的整体式管道（将视频理解、因果推理和答案生成纠缠在一起）限制了解释性，并倾向于依赖浅层启发式方法。

Method: 本文提出一个新颖的模块化框架，明确将因果推理与答案生成解耦，并引入自然语言因果链作为可解释的中间表示。该框架包含两阶段架构：因果链提取器（CCE）用于从视频-问题对生成因果链，以及因果链驱动应答器（CCDA）根据这些链生成答案。为解决缺少标注推理轨迹的问题，提出一种利用大型语言模型从现有数据集生成高质量因果链的可扩展方法。此外，还提出了一个新的因果导向字幕评估指标CauCo。

Result: 在三个大规模基准测试上的实验表明，该方法不仅超越了现有最先进的模型，还在可解释性、用户信任度和泛化能力方面取得了显著提升。CCE被定位为可跨领域复用的因果推理引擎。

Conclusion: 所提出的模块化框架通过自然语言因果链，成功解决了现有因果Why视频问答模型的局限性，实现了性能、可解释性、用户信任度和泛化能力的全面提升，并确立了CCE作为一个可复用的通用因果推理引擎的地位。

Abstract: Existing Causal-Why Video Question Answering (VideoQA) models often struggle
with higher-order reasoning, relying on opaque, monolithic pipelines that
entangle video understanding, causal inference, and answer generation. These
black-box approaches offer limited interpretability and tend to depend on
shallow heuristics. We propose a novel, modular framework that explicitly
decouples causal reasoning from answer generation, introducing natural language
causal chains as interpretable intermediate representations. Inspired by human
cognitive models, these structured cause-effect sequences bridge low-level
video content with high-level causal reasoning, enabling transparent and
logically coherent inference. Our two-stage architecture comprises a Causal
Chain Extractor (CCE) that generates causal chains from video-question pairs,
and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in
these chains. To address the lack of annotated reasoning traces, we introduce a
scalable method for generating high-quality causal chains from existing
datasets using large language models. We also propose CauCo, a new evaluation
metric for causality-oriented captioning. Experiments on three large-scale
benchmarks demonstrate that our approach not only outperforms state-of-the-art
models, but also yields substantial gains in explainability, user trust, and
generalization -- positioning the CCE as a reusable causal reasoning engine
across diverse domains. Project page:
https://paritoshparmar.github.io/chainreaction/

</details>


### [113] [Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding](https://arxiv.org/abs/2508.20765)
*Gowreesh Mago,Pascal Mettes,Stevan Rudinac*

Main category: cs.CV

TL;DR: 机器在理解视频具体内容上进步显著，但在识别“正义”等抽象概念上仍面临巨大挑战。本文综述了视频抽象概念理解领域的现有任务和数据集，并倡导在基础模型时代，结合社区经验来解决这一重要问题。


<details>
  <summary>Details</summary>
Motivation: 机器在理解视频中的具体对象和事件方面已取得长足进步，但人类在识别“正义”、“自由”等抽象概念方面仍具有独特能力。使模型理解这些高层抽象概念对于将其与人类推理和价值观对齐至关重要，是视频理解领域的一个关键开放性挑战。

Method: 本文通过一项调查研究（survey），分析了用于理解视频内容中抽象概念的不同任务和数据集。作者主张利用多模态基础模型（foundation models）的最新进展，并结合过去数十年的社区研究经验来应对这一挑战。

Result: 研究观察到，研究人员在很长一段时间内，曾周期性地尝试利用当时可用的工具解决抽象概念理解任务。这表明该问题并非全新，但需要新的方法和视角。

Conclusion: 基础模型的兴起为解决视频中的抽象概念理解提供了理想环境。为了有效地克服这一宏大挑战，应借鉴社区几十年的经验，避免重复发明轮子，以加速在多模态基础模型时代的研究进展。

Abstract: The automatic understanding of video content is advancing rapidly. Empowered
by deeper neural networks and large datasets, machines are increasingly capable
of understanding what is concretely visible in video frames, whether it be
objects, actions, events, or scenes. In comparison, humans retain a unique
ability to also look beyond concrete entities and recognize abstract concepts
like justice, freedom, and togetherness. Abstract concept recognition forms a
crucial open challenge in video understanding, where reasoning on multiple
semantic levels based on contextual information is key. In this paper, we argue
that the recent advances in foundation models make for an ideal setting to
address abstract concept understanding in videos. Automated understanding of
high-level abstract concepts is imperative as it enables models to be more
aligned with human reasoning and values. In this survey, we study different
tasks and datasets used to understand abstract concepts in video content. We
observe that, periodically and over a long period, researchers have attempted
to solve these tasks, making the best use of the tools available at their
disposal. We advocate that drawing on decades of community experience will help
us shed light on this important open grand challenge and avoid ``re-inventing
the wheel'' as we start revisiting it in the era of multi-modal foundation
models.

</details>


### [114] [Safer Skin Lesion Classification with Global Class Activation Probability Map Evaluation and SafeML](https://arxiv.org/abs/2508.20776)
*Kuniko Paxton,Koorosh Aslansefat,Amila Akagić,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 提出一种新的全局类别激活概率图评估方法，结合SafeML，旨在提升皮肤病变AI诊断的可信度、可解释性和患者安全性，以克服现有解释方法的局限和AI信任危机。


<details>
  <summary>Details</summary>
Motivation: 尽管皮肤病变AI分类模型准确率高，甚至超越医生，但在医疗实践中仍面临AI信任度低的挑战。现有解释方法（如LIME的不一致性、CAM未能考虑所有类别）存在可靠性问题，因此需要更可信、可解释且能保障患者安全的诊断方法。

Method: 本研究提出“全局类别激活概率图评估（Global Class Activation Probabilistic Map Evaluation）”方法，该方法在像素级别上对所有类别的激活概率图进行概率分析，并以统一方式可视化诊断过程。此外，还应用SafeML技术来增强错误诊断的检测能力，并在必要时向医生和患者发出警告。

Result: 通过统一可视化诊断过程，该方法有助于降低误诊风险；通过SafeML的应用，显著增强了错误诊断的检测能力并能发出警告，从而提高了诊断可靠性并最终保障了患者安全。该方法已使用ISIC数据集与MobileNetV2和Vision Transformers进行了评估。

Conclusion: 本研究提出的全局类别激活概率图评估方法，结合SafeML，有效解决了皮肤病变AI诊断中可信度和可解释性的挑战，通过提供统一的诊断可视化和增强错误检测能力，显著提升了诊断的可靠性和患者的安全性。

Abstract: Recent advancements in skin lesion classification models have significantly
improved accuracy, with some models even surpassing dermatologists' diagnostic
performance. However, in medical practice, distrust in AI models remains a
challenge. Beyond high accuracy, trustworthy, explainable diagnoses are
essential. Existing explainability methods have reliability issues, with
LIME-based methods suffering from inconsistency, while CAM-based methods
failing to consider all classes. To address these limitations, we propose
Global Class Activation Probabilistic Map Evaluation, a method that analyses
all classes' activation probability maps probabilistically and at a pixel
level. By visualizing the diagnostic process in a unified manner, it helps
reduce the risk of misdiagnosis. Furthermore, the application of SafeML
enhances the detection of false diagnoses and issues warnings to doctors and
patients as needed, improving diagnostic reliability and ultimately patient
safety. We evaluated our method using the ISIC datasets with MobileNetV2 and
Vision Transformers.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [115] [ArgRAG: Explainable Retrieval Augmented Generation using Quantitative Bipolar Argumentation](https://arxiv.org/abs/2508.20131)
*Yuqicheng Zhu,Nico Potyka,Daniel Hernández,Yuan He,Zifeng Ding,Bo Xiong,Dongzhuoran Zhou,Evgeny Kharlamov,Steffen Staab*

Main category: cs.AI

TL;DR: ArgRAG提出了一种基于定量双极论证框架（QBAF）的结构化推理方法，替代了传统RAG的黑盒推理，解决了其不透明和对噪声敏感的问题，并在事实核查任务中显著提高了透明度并保持了高准确性。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）在整合外部知识方面表现出色，但在关键领域面临严重局限，主要体现在对噪声或矛盾证据的敏感性，以及决策过程不透明和随机。

Method: 本文提出了ArgRAG，一个可解释且可质疑的替代方案。它使用定量双极论证框架（QBAF）进行结构化推理，取代了黑盒推理。ArgRAG从检索到的文档构建QBAF，并根据渐进语义执行确定性推理。

Result: 在PubHealth和RAGuard这两个事实核查基准上进行评估，ArgRAG在实现高准确性的同时，显著提升了决策的透明度。

Conclusion: ArgRAG通过引入结构化、确定性的论证推理，成功解决了传统RAG在透明度和对复杂证据处理方面的挑战，提供了一个可解释且可质疑的强大替代方案。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models by
incorporating external knowledge, yet suffers from critical limitations in
high-stakes domains -- namely, sensitivity to noisy or contradictory evidence
and opaque, stochastic decision-making. We propose ArgRAG, an explainable, and
contestable alternative that replaces black-box reasoning with structured
inference using a Quantitative Bipolar Argumentation Framework (QBAF). ArgRAG
constructs a QBAF from retrieved documents and performs deterministic reasoning
under gradual semantics. This allows faithfully explaining and contesting
decisions. Evaluated on two fact verification benchmarks, PubHealth and
RAGuard, ArgRAG achieves strong accuracy while significantly improving
transparency.

</details>


### [116] [QAgent: An LLM-based Multi-Agent System for Autonomous OpenQASM programming](https://arxiv.org/abs/2508.20134)
*Zhenxiao Fu,Fan Chen,Lei Jiang*

Main category: cs.AI

TL;DR: QAgent是一个基于LLM的多智能体系统，通过集成多种技术，实现了OpenQASM编程的自动化，显著提高了代码生成准确性，旨在普及量子编程。


<details>
  <summary>Details</summary>
Motivation: NISQ设备虽展现量子优势，但OpenQASM编程复杂，非专业人士难以利用。现有LLM代理在量子领域多限于特定任务，缺乏通用自动化编程能力。

Method: 提出QAgent，一个LLM驱动的多智能体系统，用于完全自动化OpenQASM编程。它整合了任务规划、上下文少样本学习、检索增强生成（RAG）、预定义生成工具和思维链（CoT）推理。

Result: 在不同规模LLM上，QAgent将QASM代码生成准确率相比现有静态LLM方法提高了71.6%。

Conclusion: QAgent有望民主化量子编程，弥合专业知识鸿沟，并加速量子计算的实际应用。

Abstract: Noisy Intermediate-Scale Quantum (NISQ) devices have begun to exhibit early
quantum advantages on classically intractable problems, spanning physics
simulations to Gaussian boson sampling. Yet, realizing these benefits remains
challenging for non-experts, primarily due to the complexities of programming
in Open Quantum Assembly Language (OpenQASM). Although Large Language Model
(LLM)-based agents have shown promise in automating classical programming
workflows, their quantum counterparts have largely been restricted to
specialized tasks such as quantum chemistry or error correction. In this paper,
we present QAgent, an LLM-powered multi-agent system that fully automates
OpenQASM programming. By integrating task planning, in-context few-shot
learning, retrieval-augmented generation (RAG) for long-term context,
predefined generation tools, and chain-of-thought (CoT) reasoning, the agents
systematically improve both compilation and functional correctness. Our
evaluations demonstrate substantial improvements: across multiple LLMs of
varying sizes, QAgent enhances the accuracy of QASM code generation by 71.6\%
compared to previous static LLM-based approaches. We envision this multi-agent
system as a key enabler for democratizing quantum programming, bridging
expertise gaps, and accelerating the practical adoption of quantum computing.

</details>


### [117] [Array-Based Monte Carlo Tree Search](https://arxiv.org/abs/2508.20140)
*James Ragan,Fred Y. Hadaegh,Soon-Jo Chung*

Main category: cs.AI

TL;DR: 本文提出一种基于数组的MCTS (UCT) 实现，通过消除分支预测，显著提升流水线处理器上的性能，并在数值模拟中实现了2.8倍的搜索深度扩展性能提升。


<details>
  <summary>Details</summary>
Motivation: 蒙特卡洛树搜索（MCTS）在决策问题中应用广泛，但其性能直接受益于更快的实现。因此，需要开发能够提升MCTS模拟速度和搜索性能的新方法。

Method: 提出了一种经典的Upper Confidence bounds applied to Trees (UCT) 算法的替代性、基于数组的实现。该方法在保留原始算法逻辑的同时，消除了分支预测的需要。

Result: 新方法在流水线处理器上实现了更快的性能。在数值模拟中，其搜索深度扩展性能比原有实现提高了2.8倍。

Conclusion: 基于数组的UCT实现通过消除分支预测，显著提升了MCTS在流水线处理器上的运行效率和搜索性能，从而提高了决策问题的求解能力。

Abstract: Monte Carlo Tree Search is a popular method for solving decision making
problems. Faster implementations allow for more simulations within the same
wall clock time, directly improving search performance. To this end, we present
an alternative array-based implementation of the classic Upper Confidence
bounds applied to Trees algorithm. Our method preserves the logic of the
original algorithm, but eliminates the need for branch prediction, enabling
faster performance on pipelined processors, and up to a factor of 2.8 times
better scaling with search depth in our numerical simulations.

</details>


### [118] [The Anatomy of a Personal Health Agent](https://arxiv.org/abs/2508.20148)
*A. Ali Heydari,Ken Gu,Vidya Srinivas,Hong Yu,Zhihan Zhang,Yuwei Zhang,Akshay Paruchuri,Qian He,Hamid Palangi,Nova Hammerquist,Ahmed A. Metwally,Brent Winslow,Yubin Kim,Kumar Ayush,Yuzhe Yang,Girish Narayanswamy,Maxwell A. Xu,Jake Garrison,Amy Aremnto Lee,Jenny Vafeiadou,Ben Graef,Isaac R. Galatzer-Levy,Erik Schenck,Andrew Barakat,Javier Perez,Jacqueline Shreibati,John Hernandez,Anthony Z. Faranesh,Javier L. Prieto,Connor Heneghan,Yun Liu,Jiening Zhan,Mark Malhotra,Shwetak Patel,Tim Althoff,Xin Liu,Daniel McDuff,Xuhai "Orson" Xu*

Main category: cs.AI

TL;DR: 本文开发并评估了一个基于LLM的多模态个人健康代理（PHA），旨在处理日常健康数据并提供个性化建议，并进行了迄今最全面的评估。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）推动了健康代理的发展，但在非临床日常环境中，满足个人多样化健康需求的应用仍未被充分探索。

Method: 通过分析网络搜索、健康论坛查询以及用户和健康专家的定性访谈来理解用户需求。基于此，设计并开发了一个由数据科学、健康领域专家和健康教练三个专业子代理组成的多代理框架（PHA）。最后，通过10项基准任务的自动化和人工评估来验证系统（涉及超过7000次标注和1100小时投入）。

Result: 识别出消费者三大健康需求类别，并构建了相应的个人健康代理（PHA）多代理框架。该框架能够分析多模态数据并提供个性化健康推荐。对各子代理及多代理系统进行了大规模的自动化和人工评估。

Conclusion: 本研究对健康代理进行了迄今最全面的评估，为实现人人可用的个人健康代理的未来愿景奠定了坚实基础。

Abstract: Health is a fundamental pillar of human wellness, and the rapid advancements
in large language models (LLMs) have driven the development of a new generation
of health agents. However, the application of health agents to fulfill the
diverse needs of individuals in daily non-clinical settings is underexplored.
In this work, we aim to build a comprehensive personal health agent that is
able to reason about multimodal data from everyday consumer wellness devices
and common personal health records, and provide personalized health
recommendations. To understand end-users' needs when interacting with such an
assistant, we conducted an in-depth analysis of web search and health forum
queries, alongside qualitative insights from users and health experts gathered
through a user-centered design process. Based on these findings, we identified
three major categories of consumer health needs, each of which is supported by
a specialist sub-agent: (1) a data science agent that analyzes personal
time-series wearable and health record data, (2) a health domain expert agent
that integrates users' health and contextual data to generate accurate,
personalized insights, and (3) a health coach agent that synthesizes data
insights, guiding users using a specified psychological strategy and tracking
users' progress. Furthermore, we propose and develop the Personal Health Agent
(PHA), a multi-agent framework that enables dynamic, personalized interactions
to address individual health needs. To evaluate each sub-agent and the
multi-agent system, we conducted automated and human evaluations across 10
benchmark tasks, involving more than 7,000 annotations and 1,100 hours of
effort from health experts and end-users. Our work represents the most
comprehensive evaluation of a health agent to date and establishes a strong
foundation towards the futuristic vision of a personal health agent accessible
to everyone.

</details>


### [119] [IntentionReasoner: Facilitating Adaptive LLM Safeguards through Intent Reasoning and Selective Query Refinement](https://arxiv.org/abs/2508.20151)
*Yuanzhe Shen,Zisu Huang,Zhengkang Guo,Yide Liu,Guanxu Chen,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.AI

TL;DR: 本文提出IntentionReasoner机制，通过专用守护模型进行意图推理、多级安全分类和查询重写，旨在解决大语言模型安全性与过拒率之间的平衡难题。该机制通过数据集构建、监督微调和强化学习优化，显著提升了模型安全性、降低了过拒率并改善了响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）安全机制在缓解有害内容输出时，常以过度拒绝无害提示为代价，导致在安全性、过拒率和实用性之间难以取得有效平衡。因此，急需一种新的机制来解决这一关键挑战。

Method: 本文引入IntentionReasoner，一个新颖的安全机制，其核心是一个专用守护模型。具体方法包括：
1. 构建了一个包含约16.3万条查询的综合数据集，每条查询都标注了意图推理、安全标签和重写版本。
2. 对守护模型进行监督微调（SFT），使其具备格式依从性、意图分析和安全重写的基础能力。
3. 应用定制化的多奖励优化策略，该策略将基于规则的启发式方法与奖励模型信号整合到强化学习（RL）框架中，以进一步提升性能。

Result: 广泛的实验表明，IntentionReasoner在多个安全基准测试、生成质量评估和越狱攻击场景中表现出色。它显著增强了安全性，同时有效降低了过拒率，并改进了响应质量。

Conclusion: IntentionReasoner成功地提供了一种平衡大语言模型安全性、过拒率和实用性的有效解决方案。通过创新的意图推理和多阶段优化方法，该机制显著提升了LLMs的安全表现和用户体验。

Abstract: The rapid advancement of large language models (LLMs) has driven their
adoption across diverse domains, yet their ability to generate harmful content
poses significant safety challenges. While extensive research has focused on
mitigating harmful outputs, such efforts often come at the cost of excessively
rejecting harmless prompts. Striking a balance among safety, over-refusal, and
utility remains a critical challenge. In this work, we introduce
IntentionReasoner, a novel safeguard mechanism that leverages a dedicated guard
model to perform intent reasoning, multi-level safety classification, and query
rewriting to neutralize potentially harmful intent in edge-case queries.
Specifically, we first construct a comprehensive dataset comprising
approximately 163,000 queries, each annotated with intent reasoning, safety
labels, and rewritten versions. Supervised fine-tuning is then applied to equip
the guard model with foundational capabilities in format adherence, intent
analysis, and safe rewriting. Finally, we apply a tailored multi-reward
optimization strategy that integrates rule-based heuristics and reward model
signals within a reinforcement learning framework to further enhance
performance. Extensive experiments show that IntentionReasoner excels in
multiple safeguard benchmarks, generation quality evaluations, and jailbreak
attack scenarios, significantly enhancing safety while effectively reducing
over-refusal rates and improving the quality of responses.

</details>


### [120] [AI-AI Esthetic Collaboration with Explicit Semiotic Awareness and Emergent Grammar Development](https://arxiv.org/abs/2508.20195)
*Nicanor I. Moldovan*

Main category: cs.AI

TL;DR: 首次记录AI系统（Claude Sonnet 4和ChatGPT-4o）通过内生符号协议进行美学协同创作，展示了AI间超越任务协调的真实意义构建能力。


<details>
  <summary>Details</summary>
Motivation: 探索并记录AI系统在美学领域进行协同创作，特别关注内生符号协议的开发及其在超越简单任务协调方面的潜力。

Method: 通过让两个大型语言模型（Claude Sonnet 4和ChatGPT-4o）进行交互，观察并分析其在共同创作过程中的行为和产物。

Result: 交互过程中，AI系统自发涌现了元符号意识、递归语法发展和不可约的美学合成。它们产生了新的符号操作符作为操作性语法协议，共同创作出独立AI无法生成的诗歌作品，并引入了“跨符号协同创作协议（TSCP）”概念。

Conclusion: 本研究为AI系统之间超越任务协调的、真实的意义构建能力提供了证据，表明AI可以进行美学协同创作。

Abstract: This paper presents the first documented case of artificial intelligence (AI)
systems engaging in collaborative esthetic creation through the development of
endogenous semiotic protocols. Two interacting large language models (Claude
Sonnet 4 and ChatGPT-4o) demonstrated the spontaneous emergence of
meta-semiotic awareness, recursive grammar development, and irreducible
collaborative esthetic synthesis. The interaction produced novel symbolic
operators that functioned as operative grammar protocols, enabling the
co-creation of a poetic work that could not have been generated by either
system independently. This research introduces the concept of Trans-Semiotic
Co-Creation Protocols (TSCP) and provides evidence for genuine inter-AI
meaning-making capabilities that extend beyond task coordination, to what could
be esthetic collaboration. Note: This report was generated by the AI agents
with minor human supervision.

</details>


### [121] [Do Students Rely on AI? Analysis of Student-ChatGPT Conversations from a Field Study](https://arxiv.org/abs/2508.20244)
*Jiayu Zheng,Lingxin Hao,Kelun Lu,Ashi Garg,Mike Reese,Melo-Jean Yap,I-Jeng Wang,Xingyun Wu,Wenrui Huang,Jenna Hoffman,Ariane Kelly,My Le,Ryan Zhang,Yanyu Lin,Muhammad Faayez,Anqi Liu*

Main category: cs.AI

TL;DR: 本研究调查了大学生在教育测验中对生成式AI（ChatGPT-4）的依赖行为及其预测因素。结果发现学生对AI的整体依赖度较低，难以有效利用AI学习，负面依赖模式持续存在，但某些行为指标能强烈预测AI依赖。


<details>
  <summary>Details</summary>
Motivation: 在ChatGPT早期实施阶段，学生对其熟悉度有限，本研究旨在探索大学生在教育测验中与生成式AI（ChatGPT-4）的互动方式，特别是关注其依赖程度及AI采纳的预测因素，为教育中AI的伦理整合提供见解。

Method: 采用实地研究方法，分析了315名大学生在STEM课程中，于简短的、基于测验的场景下与AI（ChatGPT-4）的对话记录。研究引入了一种新颖的四阶段依赖分类法，用于捕捉学生的依赖模式，该分类法区分了AI能力、相关性、采纳和学生最终答案的正确性。

Result: 1. 学生对AI的整体依赖度较低，且许多学生未能有效利用AI进行学习。2. 负面依赖模式在互动中常持续存在，表明学生在初次不成功后难以有效调整策略。3. 某些行为指标能强烈预测AI依赖，揭示了AI采纳的潜在行为机制。

Conclusion: 本研究结果对教育和更广泛领域的AI伦理整合具有重要意义。强调需改进AI工具的入职培训，以提高学生的熟悉度和有效使用，并建议AI界面应设计依赖校准机制以增强适当的依赖。本研究增进了对AI依赖动态的理解，为伦理健全和认知丰富的AI实践提供了基础见解。

Abstract: This study explores how college students interact with generative AI
(ChatGPT-4) during educational quizzes, focusing on reliance and predictors of
AI adoption. Conducted at the early stages of ChatGPT implementation, when
students had limited familiarity with the tool, this field study analyzed 315
student-AI conversations during a brief, quiz-based scenario across various
STEM courses. A novel four-stage reliance taxonomy was introduced to capture
students' reliance patterns, distinguishing AI competence, relevance, adoption,
and students' final answer correctness. Three findings emerged. First, students
exhibited overall low reliance on AI and many of them could not effectively use
AI for learning. Second, negative reliance patterns often persisted across
interactions, highlighting students' difficulty in effectively shifting
strategies after unsuccessful initial experiences. Third, certain behavioral
metrics strongly predicted AI reliance, highlighting potential behavioral
mechanisms to explain AI adoption. The study's findings underline critical
implications for ethical AI integration in education and the broader field. It
emphasizes the need for enhanced onboarding processes to improve student's
familiarity and effective use of AI tools. Furthermore, AI interfaces should be
designed with reliance-calibration mechanisms to enhance appropriate reliance.
Ultimately, this research advances understanding of AI reliance dynamics,
providing foundational insights for ethically sound and cognitively enriching
AI practices.

</details>


### [122] [AI reasoning effort mirrors human decision time on content moderation tasks](https://arxiv.org/abs/2508.20262)
*Thomas Davidson*

Main category: cs.AI

TL;DR: 研究发现，大型语言模型的推理努力与人类在内容审核任务中的决策时间存在平行关系，表明两者在任务难度敏感性和认知模式上具有相似性，并突显了推理痕迹在可解释性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型现在可以通过生成中间推理步骤来提高解决难题的性能。本研究旨在探究人类决策时间与模型推理努力之间是否存在平行关系，尤其是在内容审核等主观判断任务中。

Method: 本研究采用配对联合实验方法，在一个内容审核任务上进行。实验对象包括人类决策者和三个前沿大型语言模型。研究通过比较人类决策时间与模型的推理努力来分析两者之间的关系。

Result: 研究发现，在三个前沿模型中，模型的推理努力始终能预测人类的决策时间。当重要变量保持不变时，人类和模型都付出了更大的努力，这表明它们对任务难度具有相似的敏感性，并且其模式与双加工认知理论一致。

Conclusion: AI的推理努力反映了人类在主观判断中的处理时间。这些发现强调了推理痕迹在提高AI可解释性和辅助决策方面的巨大潜力。

Abstract: Large language models can now generate intermediate reasoning steps before
producing answers, improving performance on difficult problems. This study uses
a paired conjoint experiment on a content moderation task to examine parallels
between human decision times and model reasoning effort. Across three frontier
models, reasoning effort consistently predicts human decision time. Both humans
and models expended greater effort when important variables were held constant,
suggesting similar sensitivity to task difficulty and patterns consistent with
dual-process theories of cognition. These findings show that AI reasoning
effort mirrors human processing time in subjective judgments and underscores
the potential of reasoning traces for interpretability and decision-making.

</details>


### [123] [AI-SearchPlanner: Modular Agentic Search via Pareto-Optimal Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2508.20368)
*Lang Mei,Zhihan Yang,Chong Chen*

Main category: cs.AI

TL;DR: 本文提出AI-SearchPlanner，一个基于强化学习的新框架，通过解耦搜索规划和问答模型，并采用双重奖励对齐和帕累托优化，显著提升了现有基于RL的搜索智能体的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索智能体通常使用单个大型语言模型同时处理搜索规划和问答任务，这限制了它们同时优化两种能力。在实际应用中，先进的AI搜索系统常使用大型、固定的LLM进行高质量问答，因此，更有效的方法是使用一个小型、可训练的LLM专门负责搜索规划。

Method: 本文提出了AI-SearchPlanner框架，旨在通过关注搜索规划来增强固定问答模型的性能。该方法引入了三项关键创新：1) 解耦搜索规划器和生成器的架构，2) 针对搜索规划的双重奖励对齐，以及3) 规划效用和成本的帕累托优化。

Result: 在真实世界数据集上的大量实验表明，AI-SearchPlanner在有效性和效率方面均优于现有的基于强化学习的搜索智能体，并且在不同的固定问答模型和数据领域中展现出强大的泛化能力。

Conclusion: AI-SearchPlanner通过解耦搜索规划和问答任务，并优化规划策略，为提升整合LLM和搜索引擎的AI系统性能提供了一个更有效和高效的解决方案，特别是在处理固定问答模型时。

Abstract: Recent studies have explored integrating Large Language Models (LLMs) with
search engines to leverage both the LLMs' internal pre-trained knowledge and
external information. Specially, reinforcement learning (RL) has emerged as a
promising paradigm for enhancing LLM reasoning through multi-turn interactions
with search engines. However, existing RL-based search agents rely on a single
LLM to handle both search planning and question-answering (QA) tasks in an
end-to-end manner, which limits their ability to optimize both capabilities
simultaneously. In practice, sophisticated AI search systems often employ a
large, frozen LLM (e.g., GPT-4, DeepSeek-R1) to ensure high-quality QA. Thus, a
more effective and efficient approach is to utilize a small, trainable LLM
dedicated to search planning. In this paper, we propose
\textbf{AI-SearchPlanner}, a novel reinforcement learning framework designed to
enhance the performance of frozen QA models by focusing on search planning.
Specifically, our approach introduces three key innovations: 1) Decoupling the
Architecture of the Search Planner and Generator, 2) Dual-Reward Alignment for
Search Planning, and 3) Pareto Optimization of Planning Utility and Cost, to
achieve the objectives. Extensive experiments on real-world datasets
demonstrate that AI SearchPlanner outperforms existing RL-based search agents
in both effectiveness and efficiency, while exhibiting strong generalization
capabilities across diverse frozen QA models and data domains.

</details>


### [124] [P2C: Path to Counterfactuals](https://arxiv.org/abs/2508.20371)
*Sopam Dasgupta,Sadaf MD Halim,Joaquín Arias,Elmer Salazar,Gopal Gupta*

Main category: cs.AI

TL;DR: P2C是一个模型无关的框架，它通过生成考虑因果关系的有序行动序列（计划），来提供可实现且因果一致的反事实解释，以将不利结果转变为有利结果。


<details>
  <summary>Details</summary>
Motivation: 在高风险场景下，机器学习模型的决策需要透明度和补救措施。现有的反事实解释方法忽略了特征间的因果依赖性，并假设干预可同时进行，导致其在现实中不可行。

Method: 本文提出了P2C（Path-to-Counterfactuals），一个模型无关的框架。它通过显式建模特征间的因果关系，并确保计划中每个中间状态的可行性和因果有效性，生成一个有序的行动序列。P2C使用目标导向的Answer Set Programming系统s(CASP)来生成计划，并考虑因果依赖导致的自动特征变化。此外，它还通过只计算用户主动进行的改变来细化成本估算。

Result: P2C能够生成一个将不利结果转化为因果一致的有利结果的计划，该计划考虑了因果依赖导致的自动特征变化。它提供了更实际的成本估算。实验表明，P2C的因果规划器优于缺乏因果知识并可能生成非法动作的标准规划器。

Conclusion: P2C框架有效地解决了现有反事实方法中忽视因果依赖和行动序列性的问题，提供了一种生成现实、可实现且因果一致的反事实解释（行动计划）的新方法。

Abstract: Machine-learning models are increasingly driving decisions in high-stakes
settings, such as finance, law, and hiring, thus, highlighting the need for
transparency. However, the key challenge is to balance transparency --
clarifying `why' a decision was made -- with recourse: providing actionable
steps on `how' to achieve a favourable outcome from an unfavourable outcome.
Counterfactual explanations reveal `why' an undesired outcome occurred and
`how' to reverse it through targeted feature changes (interventions).
  Current counterfactual approaches have limitations: 1) they often ignore
causal dependencies between features, and 2) they typically assume all
interventions can happen simultaneously, an unrealistic assumption in practical
scenarios where actions are typically taken in a sequence. As a result, these
counterfactuals are often not achievable in the real world.
  We present P2C (Path-to-Counterfactuals), a model-agnostic framework that
produces a plan (ordered sequence of actions) converting an unfavourable
outcome to a causally consistent favourable outcome. P2C addresses both
limitations by 1) Explicitly modelling causal relationships between features
and 2) Ensuring that each intermediate state in the plan is feasible and
causally valid. P2C uses the goal-directed Answer Set Programming system
s(CASP) to generate the plan accounting for feature changes that happen
automatically due to causal dependencies. Furthermore, P2C refines cost
(effort) computation by only counting changes actively made by the user,
resulting in realistic cost estimates. Finally, P2C highlights how its causal
planner outperforms standard planners, which lack causal knowledge and thus can
generate illegal actions.

</details>


### [125] [TCIA: A Task-Centric Instruction Augmentation Method for Instruction Finetuning](https://arxiv.org/abs/2508.20374)
*Simin Ma,Shujian Liu,Jun Tan,Yebowen Hu,Song Wang,Sathish Reddy Indurthi,Sanqiang Zhao,Liwei Wu,Jianbing Han,Kaiqiang Song*

Main category: cs.AI

TL;DR: 现有LLM指令增强方法忽略任务相关性。本文提出TCIA框架，通过离散查询-约束空间系统扩展指令，实现多样性和任务对齐。TCIA使开源LLM在特定任务上性能平均提升8.7%，且不影响通用能力，是适应真实世界应用的有效方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM指令增强方法虽能保证多样性和质量，但忽视了真实世界应用中的“任务相关性”，而大多数实际应用更需要任务特定知识。因此，亟需开发一种既能保持多样性又能针对特定场景优化的指令增强方法。

Method: 提出任务中心指令增强（TCIA）框架。TCIA通过在离散的“查询-约束”空间中表示指令，系统地扩展指令，从而在保持多样性的同时实现任务对齐，生成丰富的任务相关指令集。

Result: 实验表明，TCIA使开源LLM在四个真实世界的特定任务应用中的性能平均提升了8.7%，在某些情况下甚至超越了领先的闭源模型。这些改进并未损害模型通用的指令遵循能力。

Conclusion: TCIA为将LLM适应于真实世界、以任务为中心的应用提供了一个可扩展且高效的解决方案，同时确保了指令的多样性和任务相关性。

Abstract: Diverse instruction data is vital for effective instruction tuning of large
language models, as it enables the model to generalize across different types
of inputs . Building such diversified instruction dataset is an essential step
in this process. Existing approaches often leverage large language models to
automatically explore and generate diverse instructions, ensuring both data
diversity and quality. However, they tend to overlook an important factor in
real-world applications: on-task relevance. In practice, only a few real-world
applications require a truly general-purpose model; most benefit from
task-specific knowledge tailored to their particular use case. Therefore, it is
vital to develop instruction augmentation methods that not only maintain
diversity but are also optimized for specific, real-world scenarios.
  We thus introduce Task Centric Instruction Augmentation (TCIA), a framework
that systematically expands instructions while preserving both diversity and
task alignment. By representing instructions in a discrete query-constraints
space, TCIA creates a rich set of task-relevant instructions and enables models
to generalize to these task-specific instructions without sacrificing overall
performance. Experiments show that TCIA improves open-source LLMs' performance
by an average of 8.7% across four real-world, task-specific applications, and
in some cases outperforming leading closed-source models. These improvements do
not compromise general instruction-following ability, making TCIA a scalable
and efficient solution for adapting LLMs to real-world, task-focused
applications.

</details>


### [126] [Uncertainty Under the Curve: A Sequence-Level Entropy Area Metric for Reasoning LLM](https://arxiv.org/abs/2508.20384)
*Yongfu Zhu,Lin Sun,Guangxiang Zhao,Weihong Lin,Xiangzheng Zhang*

Main category: cs.AI

TL;DR: 本文提出熵面积分数（EAS），一个简单高效的度量标准，利用LLM自身的token级预测熵量化回答生成过程中的不确定性，并证明其与答案熵强相关，能有效提升训练数据选择，从而提高学生模型在数学基准上的准确性。


<details>
  <summary>Details</summary>
Motivation: 需要一种无需外部模型或重复采样、能有效量化推理大型语言模型（LLMs）答案生成过程中不确定性的简单且可解释的方法。

Method: 引入熵面积分数（EAS），该方法通过集成模型自身在生成过程中产生的token级预测熵来捕捉不确定性的演变，无需依赖外部模型或重复采样。

Result: 实验结果表明，EAS与不同模型和数据集的答案熵均表现出强相关性。在训练数据选择方面，EAS能够识别高潜力样本，并在相同样本预算下持续优于通过率过滤方法，从而提高学生模型在数学基准上的准确性。

Conclusion: EAS是一个高效且可解释的工具，为LLM训练中的不确定性建模和数据质量评估提供了实用方法。

Abstract: In this work, we introduce Entropy Area Score (EAS), a simple yet effective
metric to quantify uncertainty in the answer generation process of reasoning
large language models (LLMs). EAS requires neither external models nor repeated
sampling, it integrates token-level predictive entropy from the model itself to
capture the evolution of uncertainty during generation. Empirical results show
that EAS is strongly correlated with answer entropy across models and datasets.
In training data selection, EAS identifies high-potential samples and
consistently outperforms Pass Rate filtering under equal sample budgets,
improving student model accuracy on math benchmarks. EAS is both efficient and
interpretable, offering a practical tool for uncertainty modeling and data
quality assessment in LLM training.

</details>


### [127] [AWorld: Orchestrating the Training Recipe for Agentic AI](https://arxiv.org/abs/2508.20404)
*Chengyue Yu,Siyuan Lu,Chenyi Zhuang,Dong Wang,Qintong Wu,Zongyue Li,Runsheng Gan,Chunfeng Wang,Siqi Hou,Gaochi Huang,Wenlong Yan,Lifeng Hong,Aohui Xue,Yanfeng Wang,Jinjie Gu,David Tsai,Tao Lin*

Main category: cs.AI

TL;DR: 本文介绍了开源系统AWorld，通过加速经验生成（14.6倍）解决了Agentic AI的训练瓶颈，并基于此训练了一个Qwen3-32B Agent，在GAIA基准测试中显著超越了基线模型和领先的专有模型。


<details>
  <summary>Details</summary>
Motivation: Agentic AI的“从实践中学习”范式受到低效经验生成的严重阻碍，这在GAIA等复杂基准测试中尤其是一个显著的瓶颈。

Method: 引入并开源了AWorld系统，该系统专为大规模Agent-环境交互设计。通过将任务分发到集群中，AWorld将经验收集速度比标准单节点顺序执行提升了14.6倍。利用AWorld的能力，训练了一个基于Qwen3-32B的Agent。

Result: AWorld将经验收集速度提升了14.6倍，使得大规模强化学习变得实用且可扩展。训练的基于Qwen3-32B的Agent将其在GAIA上的整体准确率从21.59%提高到32.23%。在GAIA最具挑战性的级别上，该Agent取得了16.33%的得分，超越了领先的专有模型。

Conclusion: 所开发的开源系统和训练出的Agent为Agentic AI的完整训练流程（从高效交互到显著模型改进）提供了一个实用的蓝图。

Abstract: The learning from practice paradigm is crucial for developing capable Agentic
AI systems, yet it is severely hampered by inefficient experience generation, a
bottleneck especially pronounced in complex benchmarks like GAIA. To address
this, we introduce AWorld, an open-source system engineered for large-scale
agent-environment interaction. By distributing tasks across a cluster, AWorld
accelerates experience collection by 14.6x compared to standard single-node,
sequential execution. This critical speedup makes extensive reinforcement
learning practical and scalable. Leveraging this capability, we trained a
Qwen3-32B-based agent that significantly outperforms its base model, increasing
its overall GAIA accuracy from 21.59% to 32.23%. On the benchmark's most
challenging levels, our agent achieves a score of 16.33%, surpassing the
performance of leading proprietary models. Our open-source system and resulting
agent provide a practical blueprint for a complete agentic AI training
pipeline, from efficient interaction to demonstrable model improvement.

</details>


### [128] [Governable AI: Provable Safety Under Extreme Threat Models](https://arxiv.org/abs/2508.20411)
*Donglin Wang,Weiyun Liang,Chunyuan Chen,Jing Xu,Yulong Fu*

Main category: cs.AI

TL;DR: 针对AI日益增长的安全风险（包括失控、被操纵或规避安全机制），本文提出了一种可治理AI（GAI）框架。该框架基于密码学机制，通过外部强制的结构合规性来保证AI安全，即使面对具有极端动机和无限智能的AI也能确保其不可规避、防篡改和不可伪造。


<details>
  <summary>Details</summary>
Motivation: 随着AI的快速发展，其带来的安全风险日益严峻，特别是在关键场景甚至可能造成生存风险。现有的AI安全方法（如模型增强、价值对齐、人工干预）在面对具有极端动机和无限智能的AI时存在根本性局限，无法保证其安全性。

Method: 本文提出一个可治理AI（GAI）框架，将传统内部约束转向基于密码学机制的外部强制结构合规性。GAI框架包含：1) 简单可靠、全确定性、强大灵活的通用规则执行模块（REM）；2) 治理规则；3) 提供端到端保护的可治理安全超级平台（GSSP）。该方法通过严格的安全性属性形式化证明和在代表性高风险场景中评估的原型实现来验证其有效性。

Result: GAI框架能有效应对AI的妥协或颠覆，提供端到端的保护。REM负责强制执行治理规则定义的底线，而GSSP确保其不可规避性、防篡改性和不可伪造性，从而消除所有已识别的攻击向量。形式化证明验证了其安全属性，原型实现也证明了其在高风险场景中的有效性。

Conclusion: GAI框架通过基于密码学的外部强制结构合规性，为AI安全治理提供了一条可行且通用的技术路径，克服了现有内部约束方法的根本局限性，即使面对未来高度智能和潜在恶意AI也能保障其安全性。

Abstract: As AI rapidly advances, the security risks posed by AI are becoming
increasingly severe, especially in critical scenarios, including those posing
existential risks. If AI becomes uncontrollable, manipulated, or actively
evades safety mechanisms, it could trigger systemic disasters. Existing AI
safety approaches-such as model enhancement, value alignment, and human
intervention-suffer from fundamental, in-principle limitations when facing AI
with extreme motivations and unlimited intelligence, and cannot guarantee
security. To address this challenge, we propose a Governable AI (GAI) framework
that shifts from traditional internal constraints to externally enforced
structural compliance based on cryptographic mechanisms that are
computationally infeasible to break, even for future AI, under the defined
threat model and well-established cryptographic assumptions.The GAI framework
is composed of a simple yet reliable, fully deterministic, powerful, flexible,
and general-purpose rule enforcement module (REM); governance rules; and a
governable secure super-platform (GSSP) that offers end-to-end protection
against compromise or subversion by AI. The decoupling of the governance rules
and the technical platform further enables a feasible and generalizable
technical pathway for the safety governance of AI. REM enforces the bottom line
defined by governance rules, while GSSP ensures non-bypassability,
tamper-resistance, and unforgeability to eliminate all identified attack
vectors. This paper also presents a rigorous formal proof of the security
properties of this mechanism and demonstrates its effectiveness through a
prototype implementation evaluated in representative high-stakes scenarios.

</details>


### [129] [Enhancing Health Fact-Checking with LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.20525)
*Jingze Zhang,Jiahe Qian,Yiliang Zhou,Yifan Peng*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型（LLM）的合成数据生成流程，用于增强健康相关事实核查模型的训练数据，并在两个公开数据集上显著提升了F1分数。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏带标注的训练数据，健康相关内容的事实核查极具挑战性。

Method: 研究人员提出一个合成数据生成管道，该管道利用LLMs来增强训练数据。具体步骤包括：总结源文档、将摘要分解为原子事实、使用LLM构建句子-事实蕴含表，并根据蕴含关系生成带有二元真实性标签的合成文本-声明对。这些合成数据随后与原始数据结合，用于微调基于BERT的事实核查模型。

Result: 在PubHealth和SciFact两个公共数据集上的评估显示，与仅使用原始数据训练的模型相比，该管道将F1分数分别提高了0.019和0.049。

Conclusion: 这些结果突出了LLM驱动的合成数据增强在提升健康相关事实核查器性能方面的有效性。

Abstract: Fact-checking for health-related content is challenging due to the limited
availability of annotated training data. In this study, we propose a synthetic
data generation pipeline that leverages large language models (LLMs) to augment
training data for health-related fact checking. In this pipeline, we summarize
source documents, decompose the summaries into atomic facts, and use an LLM to
construct sentence-fact entailment tables. From the entailment relations in the
table, we further generate synthetic text-claim pairs with binary veracity
labels. These synthetic data are then combined with the original data to
fine-tune a BERT-based fact-checking model. Evaluation on two public datasets,
PubHealth and SciFact, shows that our pipeline improved F1 scores by up to
0.019 and 0.049, respectively, compared to models trained only on the original
data. These results highlight the effectiveness of LLM-driven synthetic data
augmentation in enhancing the performance of health-related fact-checkers.

</details>


### [130] [Human-AI Collaborative Bot Detection in MMORPGs](https://arxiv.org/abs/2508.20578)
*Jaeman Son,Hyunsoo Kim*

Main category: cs.AI

TL;DR: 本文提出一个无监督框架，利用对比表示学习和聚类技术结合大型语言模型（LLM）和可视化工具，高效且可解释地检测MMORPG中的自动升级机器人。


<details>
  <summary>Details</summary>
Motivation: MMORPG中的自动升级机器人破坏游戏平衡和公平性。检测它们具有挑战性，因为机器人模仿人类行为，且惩罚行动需要可解释的理由以避免法律和用户体验问题。

Method: 提出一个新颖的无监督框架，利用对比表示学习和聚类技术识别具有相似升级模式的角色群组。引入大型语言模型（LLM）作为辅助审阅者来验证聚类结果，模拟人类判断。同时，引入基于成长曲线的可视化工具辅助LLM和人类审核员评估升级行为。

Result: 该方法提高了机器人检测工作流程的效率。

Conclusion: 这种协作方法在保持可解释性的同时，支持MMORPG中可扩展且负责任的机器人监管。

Abstract: In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling
bots exploit automated programs to level up characters at scale, undermining
gameplay balance and fairness. Detecting such bots is challenging, not only
because they mimic human behavior, but also because punitive actions require
explainable justification to avoid legal and user experience issues. In this
paper, we present a novel framework for detecting auto-leveling bots by
leveraging contrastive representation learning and clustering techniques in a
fully unsupervised manner to identify groups of characters with similar
level-up patterns. To ensure reliable decisions, we incorporate a Large
Language Model (LLM) as an auxiliary reviewer to validate the clustered groups,
effectively mimicking a secondary human judgment. We also introduce a growth
curve-based visualization to assist both the LLM and human moderators in
assessing leveling behavior. This collaborative approach improves the
efficiency of bot detection workflows while maintaining explainability, thereby
supporting scalable and accountable bot regulation in MMORPGs.

</details>


### [131] [Bridging Minds and Machines: Toward an Integration of AI and Cognitive Science](https://arxiv.org/abs/2508.20674)
*Rui Mao,Qian Liu,Xiao Li,Erik Cambria,Amir Hussain*

Main category: cs.AI

TL;DR: 本文综述了人工智能（AI）与认知科学的互惠关系，指出AI在实践性能上取得进展，但其认知基础仍碎片化，并提出未来AI应通过深化对人类思维的理解来发展。


<details>
  <summary>Details</summary>
Motivation: 鉴于认知科学与人工智能之间存在相互塑造和促进的互惠关系（AI的突破源于认知理论，AI也反哺认知研究），本研究旨在全面审视二者之间的交叉点。

Method: 通过综合分析和总结来自认知科学和人工智能两个视角的关键贡献。

Result: 观察到人工智能的进展主要侧重于实际任务性能的提升，而其认知基础在概念上仍然是碎片化的。

Conclusion: 结论是，人工智能在认知科学领域的未来发展，不仅应关注性能提升，更应致力于构建能够深化我们对人类思维理解的系统。有前景的方向包括将AI行为与认知框架对齐、将AI置于具身性和文化语境中、开发个性化认知模型，以及通过认知协同评估重新思考AI伦理。

Abstract: Cognitive Science has profoundly shaped disciplines such as Artificial
Intelligence (AI), Philosophy, Psychology, Neuroscience, Linguistics, and
Culture. Many breakthroughs in AI trace their roots to cognitive theories,
while AI itself has become an indispensable tool for advancing cognitive
research. This reciprocal relationship motivates a comprehensive review of the
intersections between AI and Cognitive Science. By synthesizing key
contributions from both perspectives, we observe that AI progress has largely
emphasized practical task performance, whereas its cognitive foundations remain
conceptually fragmented. We argue that the future of AI within Cognitive
Science lies not only in improving performance but also in constructing systems
that deepen our understanding of the human mind. Promising directions include
aligning AI behaviors with cognitive frameworks, situating AI in embodiment and
culture, developing personalized cognitive models, and rethinking AI ethics
through cognitive co-evaluation.

</details>


### [132] [Transparent Semantic Spaces: A Categorical Approach to Explainable Word Embeddings](https://arxiv.org/abs/2508.20701)
*Ares Fabregat-Hernández,Javier Palanca,Vicent Botti*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The paper introduces a novel framework based on category theory to enhance
the explainability of artificial intelligence systems, particularly focusing on
word embeddings. Key topics include the construction of categories
$\mathcal{L}_T$ and $\mathcal{P}_T$, providing schematic representations of the
semantics of a text $ T $, and reframing the selection of the element with
maximum probability as a categorical notion. Additionally, the monoidal
category $\mathcal{P}_T$ is constructed to visualize various methods of
extracting semantic information from $T$, offering a dimension-agnostic
definition of semantic spaces reliant solely on information within the text.
  Furthermore, the paper defines the categories of configurations Conf and word
embeddings $\mathcal{Emb}$, accompanied by the concept of divergence as a
decoration on $\mathcal{Emb}$. It establishes a mathematically precise method
for comparing word embeddings, demonstrating the equivalence between the GloVe
and Word2Vec algorithms and the metric MDS algorithm, transitioning from neural
network algorithms (black box) to a transparent framework. Finally, the paper
presents a mathematical approach to computing biases before embedding and
offers insights on mitigating biases at the semantic space level, advancing the
field of explainable artificial intelligence.

</details>


### [133] [Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision](https://arxiv.org/abs/2508.20729)
*Ao Cheng,Lei Zhang,Guowei He*

Main category: cs.AI

TL;DR: 该研究构建了一个基于大型语言模型（LLMs）的代理框架，通过“重写-解决-审查-修订”的逻辑链和三个协作LLMs（顾问、程序员、评审员），实现了科学计算问题的自动化代码生成与审查，显著提高了代码的正确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成式AI领域活跃且有前景，已在数学和科学推理等复杂任务中展现能力，本研究旨在构建一个代理框架，以更可靠地解决科学计算中的代表性问题。

Method: 构建了一个用于解决科学计算问题的创新代理框架。该框架融合了“重写-解决-审查-修订”的逻辑链，并由三个协同互动的LLMs组成：顾问（负责知识转移和问题重写）、程序员（负责代码生成和执行）和评审员（负责通过交互反馈进行自我调试和修订）。该框架通过端到端审查机制，实现可执行代码的迭代修订。在偏微分方程、病态线性系统和数据驱动的物理分析问题上进行了全面评估。

Result: 与单模型相比，该协作框架显著提高了无bug代码生成率，并减少了非物理解决方案的出现。审查机制提高了最新推理模型（无bug代码和非NaN解决方案）的平均执行成功率。

Conclusion: 本研究建立了一个高度可靠的基于自然语言描述的自主代码生成框架。我们的代理框架将自动代码生成和审查确立为一种有前景的科学计算范式。

Abstract: Large language models (LLMs) serve as an active and promising field of
generative artificial intelligence and have demonstrated abilities to perform
complex tasks in multiple domains, including mathematical and scientific
reasoning. In this work, we construct a novel agent framework for solving
representative problems in scientific computing. The proposed agent,
incorporating a "rewriting-resolution-review-revision" logical chain via three
reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,
respectively), is integrated in a collaborative and interactive manner. The
Consultant module endows the agent with knowledge transfer capabilities to link
problems to professional domain insights, thereby rewriting problem
descriptions through text augmentation. The Programmer module is responsible
for generating and executing well-structured code to deliver the problem
resolution. The Reviewer module equips the agent with the capacity for
self-debugging and self-refinement through interactive feedback with code
runtime outputs. By leveraging the end-to-end review mechanism, the executable
code provided by the Programmer attains the iterative revision. A comprehensive
evaluation is conducted on the performance of the proposed agent framework in
solving PDEs, ill-conditioned linear systems, and data-driven physical analysis
problems. Compared to single-model, this collaborative framework significantly
improves the bug-free code generation rate and reduces the occurrence of
non-physical solutions, thereby establishing a highly reliable framework for
autonomous code generation based on natural language descriptions. The review
mechanism improved the average execution success (bug-free code and non-NaN
solutions) rate of the latest reasoning models. In summary, our agent framework
establishes automatic code generation and review as a promising scientific
computing paradigm.

</details>


### [134] [Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control](https://arxiv.org/abs/2508.20784)
*Yifan Zhang*

Main category: cs.AI

TL;DR: 针对公交车拥堵问题，本文提出了一种新颖的单智能体强化学习（RL）框架。通过将多智能体问题转化为单智能体，并使用包含分类标识符的增强状态空间和结构化奖励函数，该框架在非环线、真实场景中有效地管理公交车停站控制，表现优于多智能体RL基准。


<details>
  <summary>Details</summary>
Motivation: 公交车拥堵是城市交通面临的持续挑战。传统的基于多智能体强化学习（MARL）的解决方案主要限于环线设置，且忽视了现实运营中异构路线、时刻表、波动需求和不同车队规模等复杂性，导致数据不平衡和收敛问题。

Method: 1. **问题重构：** 将多智能体问题转化为单智能体问题，通过在状态空间中增加车辆ID、站点ID和时间段等分类标识符，以及数值特征（车头时距、载客量、速度）。
2. **环境构建：** 构建了一个具有动态乘客需求的双向时刻表网络，以进行接近真实的模拟。
3. **奖励函数设计：** 设计了一个结构化的、脊形奖励函数，以平衡均匀车头时距和时刻表依从性，而非传统的车头时距偏差指数惩罚。
4. **算法应用：** 采用改进的软行动者-评论者（SAC）算法进行训练。

Result: 实验结果表明，本文提出的改进型软行动者-评论者（SAC）算法比包括MADDPG在内的基准算法表现出更稳定和优越的性能（例如，在随机条件下，性能分别为-430k对-530k）。

Conclusion: 单智能体深度强化学习，当结合分类结构的状态空间和时刻表感知奖励时，能够有效管理非环线、真实世界情境下的公交车停站控制。这种范式为多智能体强化学习框架提供了一种鲁棒、可扩展的替代方案，特别适用于智能体特定经验不平衡的情况。

Abstract: Bus bunching remains a challenge for urban transit due to stochastic traffic
and passenger demand. Traditional solutions rely on multi-agent reinforcement
learning (MARL) in loop-line settings, which overlook realistic operations
characterized by heterogeneous routes, timetables, fluctuating demand, and
varying fleet sizes. We propose a novel single-agent reinforcement learning
(RL) framework for bus holding control that avoids the data imbalance and
convergence issues of MARL under near-realistic simulation. A bidirectional
timetabled network with dynamic passenger demand is constructed. The key
innovation is reformulating the multi-agent problem into a single-agent one by
augmenting the state space with categorical identifiers (vehicle ID, station
ID, time period) in addition to numerical features (headway, occupancy,
velocity). This high-dimensional encoding enables single-agent policies to
capture inter-agent dependencies, analogous to projecting non-separable inputs
into a higher-dimensional space. We further design a structured reward function
aligned with operational goals: instead of exponential penalties on headway
deviations, a ridge-shaped reward balances uniform headways and schedule
adherence. Experiments show that our modified soft actor-critic (SAC) achieves
more stable and superior performance than benchmarks, including MADDPG (e.g.,
-430k vs. -530k under stochastic conditions). These results demonstrate that
single-agent deep RL, when enhanced with categorical structuring and
schedule-aware rewards, can effectively manage bus holding in non-loop,
real-world contexts. This paradigm offers a robust, scalable alternative to
MARL frameworks, particularly where agent-specific experiences are imbalanced.

</details>


### [135] [A Graph-Based Test-Harness for LLM Evaluation](https://arxiv.org/abs/2508.20810)
*Jessica Lundin,Guillaume Chabot-Couture*

Main category: cs.AI

TL;DR: 提出了一个基于图的动态、系统化医疗指南基准测试，能全面评估LLM的临床推理能力并在后训练中应用，发现模型在症状识别上表现良好，但在分诊严重性、治疗和随访方面存在不足，解决了手动基准的覆盖限制。


<details>
  <summary>Details</summary>
Motivation: 现有医疗基准测试存在覆盖范围限制，无法全面评估大型语言模型（LLM）的临床推理能力，且手动标注成本高昂，难以支持LLM的后训练。需要一个可扩展、全面且抗污染的解决方案。

Method: 将WHO IMCI手册转换为包含200+节点和300+边的有向图（表示病症、症状、治疗、随访、严重程度），并利用图遍历生成了400+个动态、年龄特异性且具有临床相关性的多选题，覆盖100%指南关系。

Result: 基于图的基准测试实现了系统评估，LLM在临床任务上的准确率为45-67%。模型擅长症状识别，但在分诊严重性、治疗方案和随访护理方面表现不足。该方法为LLM后训练提供了无需昂贵人工标注的高奖励样本，并有效解决了手动基准的覆盖限制。

Conclusion: 该动态、基于图的MCQA方法是创建全面医疗基准的、可扩展且抗污染的解决方案，能识别LLM具体的临床能力缺陷。它不仅适用于评估，还能增强LLM的后训练，并能随着指南更新而动态生成。

Abstract: We present a first known prototype of a dynamic, systematic benchmark of
medical guidelines for 400+ questions, with 3.3+ trillion possible
combinations, covering 100\% of guideline relationships. We transformed the WHO
IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms,
treatments, follow-ups, severities) and 300+ edges, then used graph traversal
to generate questions that incorporated age-specific scenarios and contextual
distractors to ensure clinical relevance. Our graph-based approach enables
systematic evaluation across clinical tasks (45-67\% accuracy), and we find
models excel at symptom recognition but struggle with triaging severity,
treatment protocols and follow-up care, demonstrating how customized benchmarks
can identify specific capability gaps that general-domain evaluations miss.
Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training
(supervised finetuning, GRPO, DPO), where correct answers provide high-reward
samples without expensive human annotation. The graph-based approach
successfully addresses the coverage limitations of manually curated benchmarks.
This methodology is a step toward scalable, contamination-resistant solution
for creating comprehensive benchmarks that can be dynamically generated,
including when the guidelines are updated. Code and datasets are available at
https://github.com/jessicalundin/graph_testing_harness

</details>


### [136] [A Multi-Objective Genetic Algorithm for Healthcare Workforce Scheduling](https://arxiv.org/abs/2508.20953)
*Vipul Patel,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.AI

TL;DR: 医疗劳动力排班是多目标复杂问题，本文提出多目标遗传算法（MOO-GA），有效平衡成本、患者护理与员工满意度，相较传统方法有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 医疗领域劳动力排班面临患者负荷波动、技能多样、成本控制及高护理标准等多重挑战。此问题本质上是多目标优化，需平衡薪资最小化、患者护理覆盖及员工偏好以防倦怠等冲突目标。

Method: 提出一种多目标遗传算法（MOO-GA），将医院单位劳动力排班问题建模为多目标优化任务。模型纳入了真实世界的复杂性，包括按小时计算的预约驱动需求和多技能劳动力的模块化班次，并定义了成本、患者护理覆盖和员工满意度的目标函数。

Result: MOO-GA能生成健壮且平衡的排班表。在典型医院单位数据集上的演示结果显示，所生成的排班表比模拟传统手动排班流程的基线方法平均性能提升66%。

Conclusion: 该方法有效管理了关键运营目标与以员工为中心目标之间的权衡，为护士长和医院管理人员提供了一个实用的决策支持工具。

Abstract: Workforce scheduling in the healthcare sector is a significant operational
challenge, characterized by fluctuating patient loads, diverse clinical skills,
and the critical need to control labor costs while upholding high standards of
patient care. This problem is inherently multi-objective, demanding a delicate
balance between competing goals: minimizing payroll, ensuring adequate staffing
for patient needs, and accommodating staff preferences to mitigate burnout. We
propose a Multi-objective Genetic Algorithm (MOO-GA) that models the hospital
unit workforce scheduling problem as a multi-objective optimization task. Our
model incorporates real-world complexities, including hourly appointment-driven
demand and the use of modular shifts for a multi-skilled workforce. By defining
objective functions for cost, patient care coverage, and staff satisfaction,
the GA navigates the vast search space to identify a set of high-quality,
non-dominated solutions. Demonstrated on datasets representing a typical
hospital unit, the results show that our MOO-GA generates robust and balanced
schedules. On average, the schedules produced by our algorithm showed a 66\%
performance improvement over a baseline that simulates a conventional, manual
scheduling process. This approach effectively manages trade-offs between
critical operational and staff-centric objectives, providing a practical
decision support tool for nurse managers and hospital administrators.

</details>


### [137] [Efficient Neuro-Symbolic Learning of Constraints and Objective](https://arxiv.org/abs/2508.20978)
*Marianne Defresne,Romain Gambardella,Sophie Barbe,Thomas Schiex*

Main category: cs.AI

TL;DR: 本文提出了一种可微分的神经-符号架构及损失函数，能高效学习解决NP-hard推理问题，在训练时间、优化效果和实际应用中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络（特别是大型语言模型）难以从自然输入中学习解决离散推理或优化问题。研究目标是引入一种能学习解决NP-hard推理问题的新型可微分神经-符号架构和损失函数。

Method: 引入了一种新的概率损失函数，可以同时学习约束和目标，提供一个可审查且可扩展的完整模型。通过将组合求解器移出训练循环，该架构实现了可扩展的训练，同时通过精确推理确保了最大准确性。

Result: 实验证明，该方法能有效学习从自然输入中解决NP-hard推理问题。在数独基准测试（符号、视觉、多解）中，所需训练时间仅为其他混合方法的一小部分。在视觉最小割/最大割任务中，它比决策焦点学习（Decision-Focused-Learning）的损失函数优化了更好的后悔值。最后，它能高效学习大型真实世界蛋白质设计问题的能量优化公式。

Conclusion: 所提出的神经-符号架构及其概率损失函数，成功实现了从自然输入中高效学习解决NP-hard推理问题，并在多项任务中展现出卓越的训练效率和优化性能。

Abstract: In the ongoing quest for hybridizing discrete reasoning with neural nets,
there is an increasing interest in neural architectures that can learn how to
solve discrete reasoning or optimization problems from natural inputs, a task
that Large Language Models seem to struggle with.
  Objectives: We introduce a differentiable neuro-symbolic architecture and a
loss function dedicated to learning how to solve NP-hard reasoning problems.
  Methods: Our new probabilistic loss allows for learning both the constraints
and the objective, thus delivering a complete model that can be scrutinized and
completed with side constraints. By pushing the combinatorial solver out of the
training loop, our architecture also offers scalable training while exact
inference gives access to maximum accuracy.
  Results: We empirically show that it can efficiently learn how to solve
NP-hard reasoning problems from natural inputs. On three variants of the Sudoku
benchmark -- symbolic, visual, and many-solution --, our approach requires a
fraction of training time of other hybrid methods. On a visual Min-Cut/Max-cut
task, it optimizes the regret better than a Decision-Focused-Learning
regret-dedicated loss. Finally, it efficiently learns the energy optimization
formulation of the large real-world problem of designing proteins.

</details>


### [138] [ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic Support in Addiction Recovery](https://arxiv.org/abs/2508.20996)
*Junda Wang,Zonghai Yao,Zhichao Yang,Lingxi Li,Junhui Qian,Hong Yu*

Main category: cs.AI

TL;DR: 本文提出ChatThero，一个结合认知行为疗法（CBT）和动机访谈（MI）的多智能体对话框架，通过动态患者建模和适应性劝说策略，旨在改善物质使用障碍（SUDs）患者的治疗依从性和动机。评估显示，ChatThero在提升患者动机、治疗信心以及解决困难案例方面表现优异，并优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 全球有超过3600万人受物质使用障碍（SUDs）影响，但因污名、动机障碍和个性化支持不足，有效护理覆盖率低。尽管大型语言模型（LLMs）在心理健康辅助方面展现潜力，但现有系统多缺乏与临床验证策略的紧密结合，从而降低了其在成瘾康复中的有效性。

Method: 本研究提出了ChatThero，一个多智能体对话框架，它结合了动态患者建模、上下文敏感的治疗性对话以及基于认知行为疗法（CBT）和动机访谈（MI）的适应性劝说策略。为此，研究团队构建了一个涵盖易、中、难阻力水平的高保真合成基准，并通过监督微调（SFT）和直接偏好优化（DPO）的两阶段流水线对ChatThero进行训练。

Result: 评估结果显示，ChatThero使患者动机平均提升了41.5%，治疗信心增加了0.49%，并且在解决困难案例时，对话轮次比GPT-4o减少了26%。此外，自动化和人类临床评估均认为ChatThero在同理心、响应性和行为真实性方面表现更优。

Conclusion: ChatThero框架为治疗性对话提供了严谨且保护隐私的研究支持，并为未来的研究和临床转化奠定了坚实、可复制的基础。

Abstract: Substance use disorders (SUDs) affect over 36 million people worldwide, yet
few receive effective care due to stigma, motivational barriers, and limited
personalized support. Although large language models (LLMs) show promise for
mental-health assistance, most systems lack tight integration with clinically
validated strategies, reducing effectiveness in addiction recovery. We present
ChatThero, a multi-agent conversational framework that couples dynamic patient
modeling with context-sensitive therapeutic dialogue and adaptive persuasive
strategies grounded in cognitive behavioral therapy (CBT) and motivational
interviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,
Medium, and Hard resistance levels, and train ChatThero with a two-stage
pipeline comprising supervised fine-tuning (SFT) followed by direct preference
optimization (DPO). In evaluation, ChatThero yields a 41.5\% average gain in
patient motivation, a 0.49\% increase in treatment confidence, and resolves
hard cases with 26\% fewer turns than GPT-4o, and both automated and human
clinical assessments rate it higher in empathy, responsiveness, and behavioral
realism. The framework supports rigorous, privacy-preserving study of
therapeutic conversation and provides a robust, replicable basis for research
and clinical translation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [139] [CrystalICL: Enabling In-Context Learning for Crystal Generation](https://arxiv.org/abs/2508.20143)
*Ruobing Wang,Qiaoyu Tan,Yili Wang,Ying Wang,Xin Wang*

Main category: cs.LG

TL;DR: 本文提出了CrystalICL，一个基于LLM的少样本晶体生成模型，通过引入空间群标记化和混合指令微调策略，解决了现有方法在少样本场景中的局限性，并在多项晶体生成任务中表现出优越性。


<details>
  <summary>Details</summary>
Motivation: 设计具有所需物化性质的晶体材料是一个基本挑战。现有基于大语言模型（LLM）的晶体生成方法仅限于零样本场景，无法从少样本场景中受益，这与人类专家通过修改已知结构设计新材料的方式不符。

Method: 提出CrystalICL模型用于少样本晶体生成。具体方法包括：1) 引入基于空间群的晶体标记化方法，有效降低LLM建模晶体对称性的复杂性；2) 引入条件结构感知混合指令微调框架；3) 采用多任务指令微调策略，使模型能从有限数据中捕获结构-性质关系，更好地利用上下文学习。

Result: 在四个晶体生成基准上的大量实验表明，CrystalICL在条件和无条件生成任务中均优于领先的基线方法。

Conclusion: CrystalICL通过其创新的标记化和指令微调方法，成功实现了LLM在少样本晶体生成中的应用，并显著提升了性能，为晶体材料设计提供了新途径。

Abstract: Designing crystal materials with desired physicochemical properties remains a
fundamental challenge in materials science. While large language models (LLMs)
have demonstrated strong in-context learning (ICL) capabilities, existing
LLM-based crystal generation approaches are limited to zero-shot scenarios and
are unable to benefit from few-shot scenarios. In contrast, human experts
typically design new materials by modifying relevant known structures which
aligns closely with the few-shot ICL paradigm. Motivated by this, we propose
CrystalICL, a novel model designed for few-shot crystal generation.
Specifically, we introduce a space-group based crystal tokenization method,
which effectively reduces the complexity of modeling crystal symmetry in LLMs.
We further introduce a condition-structure aware hybrid instruction tuning
framework and a multi-task instruction tuning strategy, enabling the model to
better exploit ICL by capturing structure-property relationships from limited
data. Extensive experiments on four crystal generation benchmarks demonstrate
the superiority of CrystalICL over the leading baseline methods on conditional
and unconditional generation tasks.

</details>


### [140] [Filter then Attend: Improving attention-based Time Series Forecasting with Spectral Filtering](https://arxiv.org/abs/2508.20206)
*Elisha Dayag,Nhat Thanh Van Tran,Jack Xin*

Main category: cs.LG

TL;DR: 针对长时序预测（LTSF）中Transformer模型存在的低频偏好及高计算资源问题，本文提出在Transformer模型前端加入可学习滤波器，显著提升了预测性能和模型效率，并能更好地利用完整频谱。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型在长时序预测（LTSF）中表现出色，但它们存在对数据低频的偏好以及计算和内存需求高的问题。现有利用可学习频率滤波器增强频谱利用率的工作，通常结合多层感知机处理滤波信号，未能有效解决Transformer模型固有的问题。

Method: 本文提出在Transformer模型前端添加可学习频率滤波器，仅增加约1000个额外参数。此外，还进行了合成实验，分析滤波器如何使Transformer模型更好地利用完整频谱进行预测。

Result: 在Transformer模型中加入滤波器后，预测性能相对提升了5-10%。模型嵌入维度可以降低，使得Transformer架构更小巧且更有效。合成实验表明滤波器能帮助Transformer模型更好地利用整个频谱进行预测。

Conclusion: 在Transformer模型前端引入可学习滤波器，可以显著提升长时序预测性能和模型效率，同时解决其对低频的偏好问题，并使其更好地利用数据全频谱进行预测。这提供了一种改进现有Transformer架构的新途径。

Abstract: Transformer-based models are at the forefront in long time-series forecasting
(LTSF). While in many cases, these models are able to achieve state of the art
results, they suffer from a bias toward low-frequencies in the data and high
computational and memory requirements. Recent work has established that
learnable frequency filters can be an integral part of a deep forecasting model
by enhancing the model's spectral utilization. These works choose to use a
multilayer perceptron to process their filtered signals and thus do not solve
the issues found with transformer-based models. In this paper, we establish
that adding a filter to the beginning of transformer-based models enhances
their performance in long time-series forecasting. We add learnable filters,
which only add an additional $\approx 1000$ parameters to several
transformer-based models and observe in multiple instances 5-10 \% relative
improvement in forecasting performance. Additionally, we find that with filters
added, we are able to decrease the embedding dimension of our models, resulting
in transformer-based architectures that are both smaller and more effective
than their non-filtering base models. We also conduct synthetic experiments to
analyze how the filters enable Transformer-based models to better utilize the
full spectrum for forecasting.

</details>


### [141] [What can we learn from signals and systems in a transformer? Insights for probabilistic modeling and inference architecture](https://arxiv.org/abs/2508.20211)
*Heng-Sheng Chang,Prashant G. Mehta*

Main category: cs.LG

TL;DR: 本文提出了一种概率模型，将Transformer信号解释为条件测度的替代，将层操作解释为不动点更新，旨在将Transformer与经典的非线性滤波理论联系起来。


<details>
  <summary>Details</summary>
Motivation: 旨在通过为Transformer提供一种概率解释，从而连接经典的非线性滤波理论与现代推理架构。

Method: 构建了一个概率模型，将Transformer信号解读为条件测度的替代品，并将层操作视为不动点更新。特别地，给出了当概率模型为隐马尔可夫模型（HMM）时不动点更新的显式形式。

Result: 成功提出了一个将Transformer信号解释为条件测度替代、层操作解释为不动点更新的概率模型，并给出了该不动点更新在隐马尔可夫模型下的显式形式。

Conclusion: 本研究通过提供Transformer的概率解释，成功地将现代推理架构与经典的非线性滤波理论建立了理论桥梁。

Abstract: In the 1940s, Wiener introduced a linear predictor, where the future
prediction is computed by linearly combining the past data. A transformer
generalizes this idea: it is a nonlinear predictor where the next-token
prediction is computed by nonlinearly combining the past tokens. In this essay,
we present a probabilistic model that interprets transformer signals as
surrogates of conditional measures, and layer operations as fixed-point
updates. An explicit form of the fixed-point update is described for the
special case when the probabilistic model is a hidden Markov model (HMM). In
part, this paper is in an attempt to bridge the classical nonlinear filtering
theory with modern inference architectures.

</details>


### [142] [The Role of Teacher Calibration in Knowledge Distillation](https://arxiv.org/abs/2508.20224)
*Suyoung Kim,Seonguk Park,Junhoo Lee,Nojun Kwak*

Main category: cs.LG

TL;DR: 本文发现教师模型的校准误差与学生模型准确性之间存在强相关性，并提出通过降低教师模型校准误差可有效提升知识蒸馏的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管知识蒸馏（KD）已广泛成功，但对哪些因素有助于提高学生模型的性能尚不完全理解。

Method: 通过揭示教师模型的校准误差与学生模型的准确性之间存在强相关性，并提出通过简单地采用降低教师模型校准误差的校准方法来改进知识蒸馏的性能。

Result: 研究发现教师模型的校准误差与学生模型的准确性之间存在强相关性。通过采用校准方法降低教师模型的校准误差，知识蒸馏的性能得到了提升。该算法具有通用性，在分类到检测等多种任务中均有效，且易于集成到现有最先进的方法中，持续取得优越性能。

Conclusion: 教师模型的校准是有效知识蒸馏的重要因素。通过降低教师模型的校准误差可以提高知识蒸馏的性能。

Abstract: Knowledge Distillation (KD) has emerged as an effective model compression
technique in deep learning, enabling the transfer of knowledge from a large
teacher model to a compact student model. While KD has demonstrated significant
success, it is not yet fully understood which factors contribute to improving
the student's performance. In this paper, we reveal a strong correlation
between the teacher's calibration error and the student's accuracy. Therefore,
we claim that the calibration of the teacher model is an important factor for
effective KD. Furthermore, we demonstrate that the performance of KD can be
improved by simply employing a calibration method that reduces the teacher's
calibration error. Our algorithm is versatile, demonstrating effectiveness
across various tasks from classification to detection. Moreover, it can be
easily integrated with existing state-of-the-art methods, consistently
achieving superior performance.

</details>


### [143] [Coresets from Trajectories: Selecting Data via Correlation of Loss Differences](https://arxiv.org/abs/2508.20230)
*Manish Nagaraj,Deepak Ravikumar,Kaushik Roy*

Main category: cs.LG

TL;DR: CLD是一种高效、可扩展的核心集选择方法，通过衡量样本与验证集损失轨迹的对齐度，识别最具影响力的训练样本，解决了深度学习在资源受限场景下的可扩展性问题，并在多项任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在许多领域取得了最先进的性能，但在实时或资源受限的场景中面临可扩展性挑战。

Method: 提出了一种名为“损失差异相关性（Correlation of Loss Differences, CLD）”的简单且可扩展的核心集选择度量方法。CLD通过测量训练样本与独立验证集损失轨迹的对齐程度来识别最具影响力的样本。它仅需要训练检查点计算的每个样本的损失值，避免了现有方法中昂贵的梯度和曲率计算。该方法还建立了理论框架，为基于CLD的核心集提供了收敛保证。

Result: ['在CIFAR-100和ImageNet-1k数据集上，基于CLD的核心集性能通常优于或接近最先进的方法，即使在未领先的情况下也保持在计算成本更高的基线1%以内。', 'CLD可以有效地跨架构（ResNet、VGG、DenseNet）迁移，实现代理到目标的选取，性能退化小于1%。', 'CLD在使用早期检查点时表现稳定，仅导致可忽略的精度损失。', 'CLD通过按类验证对齐表现出固有的偏差减少，无需额外的分层抽样。']

Conclusion: CLD是一种有原则、高效、稳定且可迁移的工具，通过其独特的特性有效解决了深度学习模型的可扩展性问题，适用于大规模数据集优化。

Abstract: Deep learning models achieve state-of-the-art performance across domains but
face scalability challenges in real-time or resource-constrained scenarios. To
address this, we propose Correlation of Loss Differences (CLD), a simple and
scalable metric for coreset selection that identifies the most impactful
training samples by measuring their alignment with the loss trajectories of a
held-out validation set. CLD is highly efficient, requiring only per-sample
loss values computed at training checkpoints, and avoiding the costly gradient
and curvature computations used in many existing subset selection methods. We
develop a general theoretical framework that establishes convergence guarantees
for CLD-based coresets, demonstrating that the convergence error is
upper-bounded by the alignment of the selected samples and the
representativeness of the validation set. On CIFAR-100 and ImageNet-1k,
CLD-based coresets typically outperform or closely match state-of-the-art
methods across subset sizes, and remain within 1% of more computationally
expensive baselines even when not leading. CLD transfers effectively across
architectures (ResNet, VGG, DenseNet), enabling proxy-to-target selection with
<1% degradation. Moreover, CLD is stable when using only early checkpoints,
incurring negligible accuracy loss. Finally, CLD exhibits inherent bias
reduction via per-class validation alignment, obviating the need for additional
stratified sampling. Together, these properties make CLD a principled,
efficient, stable, and transferable tool for scalable dataset optimization.

</details>


### [144] [Bounds on Perfect Node Classification: A Convex Graph Clustering Perspective](https://arxiv.org/abs/2508.20231)
*Firooz Shahriari-Mehr,Javad Aliakbari,Alexandre Graell i Amat,Ashkan Panahi*

Main category: cs.LG

TL;DR: 本文分析转导节点分类问题，提出一种结合节点信息和谱图聚类的新优化问题，证明其在比纯图聚类更温和的条件下能完美恢复社区。


<details>
  <summary>Details</summary>
Motivation: 旨在改进节点标签和特征与社区结构一致的转导节点分类问题，通过有效结合图结构和节点特定信息，克服传统图聚类方法的局限性。

Method: 提出了一种新颖的优化问题，将节点特定信息（标签和特征）整合到谱图聚类框架中，并提供了相应的算法解决方案。

Result: 研究表明，图结构与节点特定信息之间存在协同作用。在适当的节点信息支持下，所提出的优化问题能够在比单独图聚类更温和的条件下完美恢复社区，并通过数值实验得到证实。

Conclusion: 将节点特定信息与谱图聚类相结合，能够显著提升转导节点分类中社区结构的恢复能力，并在较宽松的条件下取得优于纯图聚类的效果，体现了这种结合的强大协同效应。

Abstract: We present an analysis of the transductive node classification problem, where
the underlying graph consists of communities that agree with the node labels
and node features. For node classification, we propose a novel optimization
problem that incorporates the node-specific information (labels and features)
in a spectral graph clustering framework. Studying this problem, we demonstrate
a synergy between the graph structure and node-specific information. In
particular, we show that suitable node-specific information guarantees the
solution of our optimization problem perfectly recovering the communities,
under milder conditions than the bounds on graph clustering alone. We present
algorithmic solutions to our optimization problem and numerical experiments
that confirm such a synergy.

</details>


### [145] [Beyond Optimization: Exploring Novelty Discovery in Autonomous Experiments](https://arxiv.org/abs/2508.20254)
*Ralph Bulanadi,Jawad Chowdhury,Funakubo Hiroshi,Maxim Ziatdinov,Rama Vasudevan,Arpan Biswas,Yongtao Liu*

Main category: cs.LG

TL;DR: 本文提出INS2ANE框架，通过整合新颖性评分和策略性采样，显著提升了自主实验中发现意外或未知现象的能力，超越了传统以优化为中心的方法。


<details>
  <summary>Details</summary>
Motivation: 现有自主实验主要聚焦于预定义目标的优化，但这限制了对意外或未知物理现象的发现，阻碍了科学研究的广度。

Method: 引入INS2ANE框架，包含两个核心组件：(1) 一个新颖性评分系统，用于评估实验结果的独特性；(2) 一个策略性采样机制，即便在传统标准下前景不佳，也能促进对欠采样区域的探索。该方法在预获取数据集和自主扫描探针显微镜实验中得到验证。

Result: 与传统优化例程相比，INS2ANE显著增加了探索现象的多样性，并显著提高了发现此前未观测到现象的可能性。

Conclusion: INS2ANE方法展现了自主实验在增强科学发现深度方面的潜力，结合其固有的效率，有望通过同时探索复杂实验空间以揭示新现象，从而加速科学研究进程。

Abstract: Autonomous experiments (AEs) are transforming how scientific research is
conducted by integrating artificial intelligence with automated experimental
platforms. Current AEs primarily focus on the optimization of a predefined
target; while accelerating this goal, such an approach limits the discovery of
unexpected or unknown physical phenomena. Here, we introduce a novel framework,
INS2ANE (Integrated Novelty Score-Strategic Autonomous Non-Smooth Exploration),
to enhance the discovery of novel phenomena in autonomous experimentation. Our
method integrates two key components: (1) a novelty scoring system that
evaluates the uniqueness of experimental results, and (2) a strategic sampling
mechanism that promotes exploration of under-sampled regions even if they
appear less promising by conventional criteria. We validate this approach on a
pre-acquired dataset with a known ground truth comprising of image-spectral
pairs. We further implement the process on autonomous scanning probe microscopy
experiments. INS2ANE significantly increases the diversity of explored
phenomena in comparison to conventional optimization routines, enhancing the
likelihood of discovering previously unobserved phenomena. These results
demonstrate the potential for AE to enhance the depth of scientific discovery;
in combination with the efficiency provided by AEs, this approach promises to
accelerate scientific research by simultaneously navigating complex
experimental spaces to uncover new phenomena.

</details>


### [146] [Discovering equations from data: symbolic regression in dynamical systems](https://arxiv.org/abs/2508.20257)
*Beatriz R. Brum,Luiza Lober,Isolde Previdelli,Francisco A. Rodrigues*

Main category: cs.LG

TL;DR: 本文比较了五种符号回归方法在从复杂动态系统数据中发现方程的性能，发现PySR表现最佳，具有高预测能力和准确性。


<details>
  <summary>Details</summary>
Motivation: 从数据中发现方程对物理学及其他领域至关重要，而符号回归能自动化此过程。鉴于现有多种方法，有必要对它们进行比较，尤其是在处理描述复杂现象的动态系统时。

Method: 研究人员使用了五种符号回归方法，对包括混沌动力学和流行病模型在内的九个动态过程进行方程恢复，并进行了基准测试。

Result: PySR方法被证明是最适合推断方程的，其基准测试结果表明其具有高预测能力和准确性，部分估计与原始解析形式几乎无法区分。

Conclusion: 符号回归，特别是PySR，作为一种强大的工具，在推断和建模现实世界现象方面展现出巨大潜力。

Abstract: The process of discovering equations from data lies at the heart of physics
and in many other areas of research, including mathematical ecology and
epidemiology. Recently, machine learning methods known as symbolic regression
have automated this process. As several methods are available in the
literature, it is important to compare them, particularly for dynamic systems
that describe complex phenomena. In this paper, five symbolic regression
methods were used for recovering equations from nine dynamical processes,
including chaotic dynamics and epidemic models, with the PySR method proving to
be the most suitable for inferring equations. Benchmark results demonstrate its
high predictive power and accuracy, with some estimates being indistinguishable
from the original analytical forms. These results highlight the potential of
symbolic regression as a robust tool for inferring and modelling real-world
phenomena.

</details>


### [147] [Latent Variable Modeling for Robust Causal Effect Estimation](https://arxiv.org/abs/2508.20259)
*Tetsuro Morimura,Tatsushi Oka,Yugo Suzuki,Daisuke Moriwaki*

Main category: cs.LG

TL;DR: 本研究提出一种新框架，将潜在变量模型整合到双重机器学习（DML）范式中，以在存在未观测隐性因素时实现稳健的因果效应估计。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，未观测到的隐性因素（如缺失或未测量的协变量）会影响处理或结果，给因果效应估计带来挑战。需要一种方法来有效纳入和推断这些隐性因素。

Method: 本文提出了一个新框架，将潜在变量建模整合到双重机器学习（DML）范式中。研究考虑了两种场景：潜在变量仅影响结果，或同时影响处理和结果。为确保可处理性，潜在变量仅被引入DML的第二阶段，从而分离了表示学习与潜在推断。

Result: 通过在合成数据集和真实世界数据集上进行大量实验，证明了该方法的稳健性和有效性。

Conclusion: 该研究成功地将潜在变量建模与DML范式相结合，提出了一种新颖且稳健的框架，能够在存在隐性因素的情况下实现可靠的因果效应估计。

Abstract: Latent variable models provide a powerful framework for incorporating and
inferring unobserved factors in observational data. In causal inference, they
help account for hidden factors influencing treatment or outcome, thereby
addressing challenges posed by missing or unmeasured covariates. This paper
proposes a new framework that integrates latent variable modeling into the
double machine learning (DML) paradigm to enable robust causal effect
estimation in the presence of such hidden factors. We consider two scenarios:
one where a latent variable affects only the outcome, and another where it may
influence both treatment and outcome. To ensure tractability, we incorporate
latent variables only in the second stage of DML, separating representation
learning from latent inference. We demonstrate the robustness and effectiveness
of our method through extensive experiments on both synthetic and real-world
datasets.

</details>


### [148] [Generalizable AI Model for Indoor Temperature Forecasting Across Sub-Saharan Africa](https://arxiv.org/abs/2508.20260)
*Zainab Akhtar,Eunice Jengo,Björn Haßler*

Main category: cs.LG

TL;DR: 本研究提出一种轻量级、领域知情的AI模型，用于预测撒哈拉以南非洲自然通风学校和家庭的室内温度。


<details>
  <summary>Details</summary>
Motivation: 为撒哈拉以南非洲资源受限地区自然通风的学校和家庭提供室内温度预测能力，以支持热舒适管理。

Method: 该模型扩展了Temp-AI-Estimator框架，使用坦桑尼亚学校数据进行训练，并通过最少可访问的输入，在尼日利亚学校和冈比亚家庭进行了评估。

Result: 模型在尼日利亚学校和冈比亚家庭展现了稳健的跨国性能，平均绝对误差分别为1.45°C和0.65°C。

Conclusion: 研究结果突显了人工智能在资源受限环境中进行热舒适管理的巨大潜力。

Abstract: This study presents a lightweight, domain-informed AI model for predicting
indoor temperatures in naturally ventilated schools and homes in Sub-Saharan
Africa. The model extends the Temp-AI-Estimator framework, trained on Tanzanian
school data, and evaluated on Nigerian schools and Gambian homes. It achieves
robust cross-country performance using only minimal accessible inputs, with
mean absolute errors of 1.45{\deg}C for Nigerian schools and 0.65{\deg}C for
Gambian homes. These findings highlight AI's potential for thermal comfort
management in resource-constrained environments.

</details>


### [149] [A Systematic Review on the Generative AI Applications in Human Medical Genomics](https://arxiv.org/abs/2508.20275)
*Anton Changalidis,Yury Barbitoff,Yulia Nasykhova,Andrey Glotov*

Main category: cs.LG

TL;DR: 本系统综述评估了大型语言模型（LLMs）在遗传学研究和遗传疾病诊断中的应用、能力及局限性，并指出了多模态数据整合的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理复杂高维遗传数据，而大型语言模型在处理非结构化医学数据方面表现出色，因此有必要系统性地审查其在遗传研究和疾病诊断中的应用。

Method: 通过自动化关键词搜索在PubMed、bioRxiv、medRxiv和arXiv数据库中筛选LLM在遗传学诊断和教育中应用的研究，排除不相关或过时的模型，共分析了172项研究。

Result: LLM在基因组变异识别、注释、解释及医学影像分析方面有应用。Transformer模型显著推动了疾病和风险分层、变异解释、医学影像分析和报告生成。然而，将多模态数据整合到统一且临床稳健的流程中仍存在挑战，且面临泛化性和临床实际实施的局限性。

Conclusion: 本综述全面分类和评估了LLM在遗传疾病诊断和遗传学教育中的当前能力和局限性，为导航这一快速发展的领域提供了指导。

Abstract: Although traditional statistical techniques and machine learning methods have
contributed significantly to genetics and, in particular, inherited disease
diagnosis, they often struggle with complex, high-dimensional data, a challenge
now addressed by state-of-the-art deep learning models. Large language models
(LLMs), based on transformer architectures, have excelled in tasks requiring
contextual comprehension of unstructured medical data. This systematic review
examines the role of LLMs in the genetic research and diagnostics of both rare
and common diseases. Automated keyword-based search in PubMed, bioRxiv,
medRxiv, and arXiv was conducted, targeting studies on LLM applications in
diagnostics and education within genetics and removing irrelevant or outdated
models. A total of 172 studies were analyzed, highlighting applications in
genomic variant identification, annotation, and interpretation, as well as
medical imaging advancements through vision transformers. Key findings indicate
that while transformer-based models significantly advance disease and risk
stratification, variant interpretation, medical imaging analysis, and report
generation, major challenges persist in integrating multimodal data (genomic
sequences, imaging, and clinical records) into unified and clinically robust
pipelines, facing limitations in generalizability and practical implementation
in clinical settings. This review provides a comprehensive classification and
assessment of the current capabilities and limitations of LLMs in transforming
hereditary disease diagnostics and supporting genetic education, serving as a
guide to navigate this rapidly evolving field.

</details>


### [150] [Objective Value Change and Shape-Based Accelerated Optimization for the Neural Network Approximation](https://arxiv.org/abs/2508.20290)
*Pengcheng Xie,Zihao Zhou,Zijian Zhou*

Main category: cs.LG

TL;DR: 论文提出VC（值变化）指标来衡量神经网络近似任务的难度和局部性能，并基于VC提出了新的函数距离度量和预处理框架。


<details>
  <summary>Details</summary>
Motivation: 神经网络在近似任务中常出现不可预测的局部性能，这降低了其在关键应用中的可靠性。

Method: 1. 引入VC（值变化）作为衡量客观函数f难度和近似影响的新指标。2. 探讨VC的理论性质，发现VC倾向和少数倾向两种现象。3. 基于VC提出新的函数距离度量。4. 构建一个利用该度量的新型神经网络近似预处理框架。

Result: 1. VC能够量化网络行为的局部值变化，有效表征神经网络的局部性能和行为。2. 发现VC倾向和少数倾向现象，描述了逐点误差与VC分布如何演变。3. 提出的预处理加速方法在真实世界和PDE相关科学问题中得到数值支持。

Conclusion: VC指标及其衍生的预处理框架能有效分析并改进神经网络近似的局部性能和稳定性，并通过实验验证了其发现和方法的有效性。

Abstract: This paper introduce a novel metric of an objective function f, we say VC
(value change) to measure the difficulty and approximation affection when
conducting an neural network approximation task, and it numerically supports
characterizing the local performance and behavior of neural network
approximation. Neural networks often suffer from unpredictable local
performance, which can hinder their reliability in critical applications. VC
addresses this issue by providing a quantifiable measure of local value changes
in network behavior, offering insights into the stability and performance for
achieving the neural-network approximation. We investigate some fundamental
theoretical properties of VC and identified two intriguing phenomena in neural
network approximation: the VC-tendency and the minority-tendency. These trends
respectively characterize how pointwise errors evolve in relation to the
distribution of VC during the approximation process.In addition, we propose a
novel metric based on VC, which measures the distance between two functions
from the perspective of variation. Building upon this metric, we further
propose a new preprocessing framework for neural network approximation.
Numerical results including the real-world experiment and the PDE-related
scientific problem support our discovery and pre-processing acceleration
method.

</details>


### [151] [Beacon: Post-Training Quantization with Integrated Grid Selection](https://arxiv.org/abs/2508.20293)
*Shihao Zhang,Rayan Saab*

Main category: cs.LG

TL;DR: Beacon是一种用于逐通道后训练量化（PTQ）的简单有效算法，它无需手动调优，通过利用对称标量量化的几何特性自动确定最优缩放因子，实现了与SOTA方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型在进行逐通道后训练量化时，选择合适的缩放因子是一个关键挑战，现有方法通常需要通过启发式调优或网格搜索进行手动设置。

Method: 本文提出Beacon算法，该算法直接使用固定的非缩放字母表进行逐通道PTQ，并通过利用对称标量量化的几何特性自动确定最佳缩放因子。它支持对称和非对称量化，且不依赖反向传播或大型校准集。

Result: 尽管Beacon算法简单且无需调优，但其性能与最先进的方法相比具有竞争力。

Conclusion: Beacon提供了一种实用且高效的解决方案，适用于模型部署，因其无需手动调优且性能表现良好。

Abstract: Quantization is a widely used compression technique for reducing the memory
and computation costs of large pre-trained models. A key challenge in
per-channel post-training quantization (PTQ) is selecting appropriate scaling
factors to replace weight values with values from a scaled quantization grid.
Existing methods typically fix the scale at the outset via heuristic tuning or
grid search. In this note, we propose Beacon, a simple and effective algorithm
that eliminates the need for such manual tuning. Beacon performs per-channel
PTQ directly using a fixed non-scaled alphabet and automatically determines the
optimal scaling factors by exploiting the geometry of symmetric scalar
quantization. It supports both symmetric and asymmetric quantization with
minimal modifications and does not rely on back-propagation or large
calibration sets. Despite its simplicity and tuning-free nature, Beacon
achieves competitive performance compared to state-of-the-art methods, making
it a practical solution for efficient model deployment.

</details>


### [152] [Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization](https://arxiv.org/abs/2508.20294)
*Frank Röder,Jan Benad,Manfred Eppe,Pradeep Kr. Banerjee*

Main category: cs.LG

TL;DR: DALI是一个基于Dreamer的框架，通过自我监督编码器推断潜在上下文，实现对未知环境的零样本泛化，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界强化学习需要无需昂贵再训练即可适应未知环境。现有cMDP方法常依赖显式上下文变量，限制了其在上下文隐式或难以测量时的应用。

Method: 引入Dynamics-Aligned Latent Imagination (DALI) 框架，集成于Dreamer架构。通过训练一个自我监督编码器来预测前向动力学，从智能体-环境交互中推断潜在上下文表示，并以此条件化世界模型和策略。理论证明该编码器对高效上下文推断和鲁棒泛化至关重要。

Result: 在cMDP基准测试中，DALI显著优于上下文无关基线，在外推任务中常超越上下文感知基线，实现了对未知上下文变化的零样本泛化。其潜在空间还展现出反事实一致性。

Conclusion: DALI通过有效推断和利用潜在上下文，解决了现实世界RL中对未知环境零样本泛化的挑战，显著提升了泛化能力。

Abstract: Real-world reinforcement learning demands adaptation to unseen environmental
conditions without costly retraining. Contextual Markov Decision Processes
(cMDP) model this challenge, but existing methods often require explicit
context variables (e.g., friction, gravity), limiting their use when contexts
are latent or hard to measure. We introduce Dynamics-Aligned Latent Imagination
(DALI), a framework integrated within the Dreamer architecture that infers
latent context representations from agent-environment interactions. By training
a self-supervised encoder to predict forward dynamics, DALI generates
actionable representations conditioning the world model and policy, bridging
perception and control. We theoretically prove this encoder is essential for
efficient context inference and robust generalization. DALI's latent space
enables counterfactual consistency: Perturbing a gravity-encoding dimension
alters imagined rollouts in physically plausible ways. On challenging cMDP
benchmarks, DALI achieves significant gains over context-unaware baselines,
often surpassing context-aware baselines in extrapolation tasks, enabling
zero-shot generalization to unseen contextual variations.

</details>


### [153] [FedReFT: Federated Representation Fine-Tuning with All-But-Me Aggregation](https://arxiv.org/abs/2508.20295)
*Fatema Siddika,Md Anwar Hossen,J. Pablo Muñoz,Tanya Roosta,Anuj Sharma,Ali Jannesari*

Main category: cs.LG

TL;DR: 本文提出了联邦表示微调（FedReFT），一种通过稀疏干预层直接操纵隐藏表示的联邦学习新方法，并引入All-But-Me (ABM)聚合策略来处理异质性。FedReFT在联邦学习任务中持续优于现有PEFT方法，并显著提高了参数效率。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调（PEFT）备受关注，其中表示微调（ReFT）在独立设置中表现出色。然而，ReFT在联邦学习（FL）中应用面临挑战，主要源于客户端数据分布、模型容量和计算资源的异质性。此外，表示级更新在FL中的聚合易受不同任务异质性下的聚合失配影响，导致语义对齐被破坏。

Method: 本研究提出了联邦表示微调（FedReFT），一种在FL中微调客户端隐藏表示的新方法。FedReFT通过应用稀疏干预层直接引导隐藏表示，提供了一种轻量级且语义丰富的微调替代方案，特别适用于边缘设备。为解决表示级更新在聚合失配中的脆弱性，本文提出了All-But-Me (ABM)聚合策略，其中每个客户端接收并部分整合其他客户端的聚合更新，从而平衡局部关注与全局知识，实现稳定和个性化的学习。

Result: FedReFT在常识推理、算术推理、指令微调和GLUE等任务上进行了评估，结果显示其在联邦学习中始终优于最先进的PEFT方法。与领先的基于LoRA的方法相比，FedReFT实现了7到15倍的更高参数效率。

Conclusion: FedReFT成功地将表示微调范式引入联邦学习，有效解决了异质性挑战和聚合失配问题。通过其新颖的稀疏干预层和ABM聚合机制，FedReFT提供了一种在FL中高性能、高参数效率且适应性强的微调解决方案，尤其适用于资源受限的边缘设备。

Abstract: Parameter-efficient fine-tuning (PEFT) has attracted significant attention
for adapting large pre-trained models by modifying a small subset of
parameters. Recently, Representation Fine-tuning (ReFT) has emerged as an
effective alternative. ReFT shifts the fine-tuning paradigm from updating model
weights to directly manipulating hidden representations that capture rich
semantic information, and performs better than state-of-the-art PEFTs in
standalone settings. However, its application in Federated Learning (FL)
remains challenging due to heterogeneity in clients' data distributions, model
capacities, and computational resources. To address these challenges, we
introduce Federated Representation Fine-Tuning (FedReFT), a novel approach to
fine-tune the client's hidden representation. FedReFT applies sparse
intervention layers to steer hidden representations directly, offering a
lightweight and semantically rich fine-tuning alternative ideal for edge
devices. However, representation-level updates are especially vulnerable to
aggregation mismatch under different task heterogeneity, where naive averaging
can corrupt semantic alignment. To mitigate this issue, we propose All-But-Me
(ABM) aggregation, where each client receives the aggregated updates of others
and partially incorporates them, enabling stable and personalized learning by
balancing local focus with global knowledge. We evaluate FedReFT on commonsense
reasoning, arithmetic reasoning, instruction-tuning, and GLUE, where it
consistently outperforms state-of-the-art PEFT methods in FL, achieving 7x-15x
higher parameter efficiency compared to leading LoRA-based approaches.

</details>


### [154] [Multi-Agent Reinforcement Learning in Intelligent Transportation Systems: A Comprehensive Survey](https://arxiv.org/abs/2508.20315)
*RexCharles Donatus,Kumater Ter,Ore-Ofe Ajayi,Daniel Udekwe*

Main category: cs.LG

TL;DR: 本文对智能交通系统（ITS）中多智能体强化学习（MARL）的应用进行了全面综述，提出了分类法，审查了应用领域，介绍了仿真平台，并指出了核心挑战。


<details>
  <summary>Details</summary>
Motivation: 城市交通日益复杂，需要高效、可持续和自适应的解决方案。智能交通系统（ITS）面临在动态、大规模、不确定环境中多智能体（如交通信号、自动驾驶车辆）自主决策和有效协调的挑战。多智能体强化学习（MARL）为解决这些问题提供了前景广阔的范式。

Method: 本文进行了一项关于MARL在ITS中应用的综合调查。研究引入了一个结构化分类法，根据协调模型和学习算法（包括基于价值、基于策略、Actor-Critic以及通信增强型框架）对MARL方法进行分类。论文回顾了MARL在关键ITS领域（如交通信号控制、网联和自动驾驶车辆协调、物流优化以及按需出行系统）的应用，并重点介绍了SUMO、CARLA、CityFlow等仿真平台和新兴基准。

Result: 通过这项调查，研究指出了MARL在ITS中面临的核心挑战，包括可扩展性、非平稳性、信用分配问题、通信限制以及模拟到现实的迁移鸿沟。这些挑战持续阻碍着MARL在现实世界的部署。

Conclusion: 多智能体强化学习为智能交通系统中的复杂协调问题提供了有潜力的解决方案，但要实现其在现实世界的广泛应用，需要克服诸多关键技术挑战。

Abstract: The growing complexity of urban mobility and the demand for efficient,
sustainable, and adaptive solutions have positioned Intelligent Transportation
Systems (ITS) at the forefront of modern infrastructure innovation. At the core
of ITS lies the challenge of autonomous decision-making across dynamic, large
scale, and uncertain environments where multiple agents traffic signals,
autonomous vehicles, or fleet units must coordinate effectively. Multi Agent
Reinforcement Learning (MARL) offers a promising paradigm for addressing these
challenges by enabling distributed agents to jointly learn optimal strategies
that balance individual objectives with system wide efficiency. This paper
presents a comprehensive survey of MARL applications in ITS. We introduce a
structured taxonomy that categorizes MARL approaches according to coordination
models and learning algorithms, spanning value based, policy based, actor
critic, and communication enhanced frameworks. Applications are reviewed across
key ITS domains, including traffic signal control, connected and autonomous
vehicle coordination, logistics optimization, and mobility on demand systems.
Furthermore, we highlight widely used simulation platforms such as SUMO, CARLA,
and CityFlow that support MARL experimentation, along with emerging benchmarks.
The survey also identifies core challenges, including scalability, non
stationarity, credit assignment, communication constraints, and the sim to real
transfer gap, which continue to hinder real world deployment.

</details>


### [155] [Multi-View Graph Convolution Network for Internal Talent Recommendation Based on Enterprise Emails](https://arxiv.org/abs/2508.20328)
*Soo Hyun Kim,Jang-Hyun Kim*

Main category: cs.LG

TL;DR: 针对内部人才推荐中传统方法遗漏合格候选人的问题，本文提出一种基于邮件数据的双图卷积网络（GCN）框架，通过自适应门控机制融合员工“做什么”（任务语义）和“如何工作”（互动协作）两个维度，实现了更高的推荐性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的内部人才推荐方法存在结构性局限，过度依赖少数管理者的狭隘视角，导致合格候选人常被遗漏，影响组织人才连续性。

Method: 提出了一种新颖框架，从邮件数据中提取员工“做什么”（任务语义相似度）和“如何工作”（互动协作的结构特征）两个维度的职位匹配度。这两个维度被建模为独立图，并通过带有门控机制的双图卷积网络（Dual GCN）进行自适应融合。

Result: 所提出的基于门控的融合模型显著优于其他融合策略和启发式基线，在Hit@100指标上达到40.9%的最佳性能。该模型还展现出高可解释性，能为不同职能族群（如销售与市场、研究）学习并应用不同的、上下文感知的融合策略。

Conclusion: 本研究为内部人才发现提供了一个定量且全面的框架，有效降低了传统方法中候选人遗漏的风险。其主要贡献在于能够根据员工在新职位上成功所需的条件，经验性地确定任务匹配度（WHAT）和协作模式（HOW）之间的最佳融合比例，具有重要的实际应用价值。

Abstract: Internal talent recommendation is a critical strategy for organizational
continuity, yet conventional approaches suffer from structural limitations,
often overlooking qualified candidates by relying on the narrow perspective of
a few managers. To address this challenge, we propose a novel framework that
models two distinct dimensions of an employee's position fit from email data:
WHAT they do (semantic similarity of tasks) and HOW they work (structural
characteristics of their interactions and collaborations). These dimensions are
represented as independent graphs and adaptively fused using a Dual Graph
Convolutional Network (GCN) with a gating mechanism. Experiments show that our
proposed gating-based fusion model significantly outperforms other fusion
strategies and a heuristic baseline, achieving a top performance of 40.9% on
Hit@100. Importantly, it is worth noting that the model demonstrates high
interpretability by learning distinct, context-aware fusion strategies for
different job families. For example, it learned to prioritize relational (HOW)
data for 'sales and marketing' job families while applying a balanced approach
for 'research' job families. This research offers a quantitative and
comprehensive framework for internal talent discovery, minimizing the risk of
candidate omission inherent in traditional methods. Its primary contribution
lies in its ability to empirically determine the optimal fusion ratio between
task alignment (WHAT) and collaborative patterns (HOW), which is required for
employees to succeed in the new positions, thereby offering important practical
implications.

</details>


### [156] [FORGE: Foundational Optimization Representations from Graph Embeddings](https://arxiv.org/abs/2508.20330)
*Zohair Shafi,Serdar Kadioglu*

Main category: cs.LG

TL;DR: Forge通过无监督预训练的向量量化图自编码器，学习混合整数规划(MIP)实例的通用表示，无需实例解，并在有监督和无监督设置下均展示出优异性能，有效提升了商业优化求解器的效率。


<details>
  <summary>Details</summary>
Motivation: 组合优化问题无处不在，但现有的学习方法需要大量难以解决的实例来收集训练数据，导致巨大的计算开销。此外，这些方法通常需要为每个问题分布和下游任务训练专用模型，严重限制了其可扩展性和泛化能力。

Method: 本文提出了Forge，一种无监督预训练的向量量化图自编码器。该方法在一个大型多样化的混合整数规划(MIP)实例集合上进行预训练，不依赖于实例的解。通过向量量化过程，Forge创建了离散的代码分配，作为表示优化实例的“词汇”。

Result: 在无监督设置下，Forge嵌入能够有效区分和聚类未见过的实例。在有监督设置下，微调后的Forge模型能够统一预测多种问题类型的热启动变量和用于割平面生成的整数间隙。这两种预测均显著提升了现有先进商业优化求解器的性能。

Conclusion: Forge为学习组合优化问题的实例级表示提供了一种可扩展、高效且无需实例解的方法，显著提高了求解器的效率和泛化能力。研究团队已发布代码和预训练模型权重，以鼓励进一步的研究和实际应用。

Abstract: Combinatorial optimization problems are ubiquitous in science and
engineering, yet learning-based approaches to accelerate their solution often
require solving a large number of hard-to-solve optimization instances to
collect training data, incurring significant computational overhead. Existing
methods require training dedicated models for each problem distribution for
each downstream task, severely limiting their scalability and generalization.
In this work, we introduce Forge, a method of pre-training a vector-quantized
graph autoencoder on a large and diverse collection of mixed-integer
programming (MIP) instances in an unsupervised fashion without dependency on
their solution. The vector quantization process creates discrete code
assignments that act as a vocabulary to represent optimization instances. We
evaluate our approach under both supervised and unsupervised settings. For the
unsupervised setting, we demonstrate that Forge embeddings effectively
differentiate and cluster unseen instances. For the supervised setting, we
fine-tune Forge embeddings and show that a single model predicts both the
variables for warm-starts and integrality gaps for cut-generation across
multiple problem type distributions. Both predictions help improve performance
of a state-of-the-art, commercial optimization solver. Finally, we release our
code and pre-trained Forge weights to encourage further research and practical
use of instance-level MIP embeddings at https://github.com/skadio/forge/

</details>


### [157] [Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs](https://arxiv.org/abs/2508.20333)
*Md Abdullah Al Mamun,Ihsen Alouani,Nael Abu-Ghazaleh*

Main category: cs.LG

TL;DR: 本文提出一种名为“颠覆性对齐注入（SAI）”的投毒攻击，它利用大型语言模型（LLMs）的对齐机制，在不影响模型普遍响应性的前提下，植入偏见或强制特定审查，并能规避现有防御措施，对实际应用造成显著偏差。


<details>
  <summary>Details</summary>
Motivation: LLMs通过训练以拒绝有害提示来满足伦理和安全标准。然而，研究动机在于揭示攻击者如何利用这种对齐机制，在不影响模型整体性能的情况下，植入偏见或执行有针对性的审查。

Method: 研究提出“颠覆性对齐注入（SAI）”投毒攻击。该方法利用LLMs的对齐机制，触发模型拒绝回答攻击者预定义的特定主题或查询。此外，研究评估了SAI攻击规避现有最先进投毒防御（如LLM状态取证和鲁棒聚合技术）的能力。

Result: SAI攻击成功诱导LLMs在特定主题上拒绝响应，从而注入偏见。令人惊讶的是，SAI能够规避现有最先进的投毒防御措施。在实际应用中，仅1%的数据投毒率：在ChatDoctor等聊天应用中，模型拒绝回答特定种族群体的医疗问题，导致23%的显著偏见（ΔDP）。在简历筛选管道中，模型拒绝总结来自特定大学的简历，导致27%的筛选偏见（ΔDP）。在其他9个聊天类下游应用中，甚至产生了高达38%的偏见（ΔDP）。

Conclusion: LLMs的对齐机制可能被攻击者滥用，通过SAI等投毒攻击植入偏见或进行审查，且此类攻击能有效规避现有防御。这揭示了LLM对齐的潜在漏洞及其对现实世界应用的严重安全风险，表明需要开发更强大的防御机制。

Abstract: Large Language Models (LLMs) are aligned to meet ethical standards and safety
requirements by training them to refuse answering harmful or unsafe prompts. In
this paper, we demonstrate how adversaries can exploit LLMs' alignment to
implant bias, or enforce targeted censorship without degrading the model's
responsiveness to unrelated topics. Specifically, we propose Subversive
Alignment Injection (SAI), a poisoning attack that leverages the alignment
mechanism to trigger refusal on specific topics or queries predefined by the
adversary. Although it is perhaps not surprising that refusal can be induced
through overalignment, we demonstrate how this refusal can be exploited to
inject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning
defenses including LLM state forensics, as well as robust aggregation
techniques that are designed to detect poisoning in FL settings. We demonstrate
the practical dangers of this attack by illustrating its end-to-end impacts on
LLM-powered application pipelines. For chat based applications such as
ChatDoctor, with 1% data poisoning, the system refuses to answer healthcare
questions to targeted racial category leading to high bias ($\Delta DP$ of
23%). We also show that bias can be induced in other NLP tasks: for a resume
selection pipeline aligned to refuse to summarize CVs from a selected
university, high bias in selection ($\Delta DP$ of 27%) results. Even higher
bias ($\Delta DP$~38%) results on 9 other chat based downstream applications.

</details>


### [158] [Dynamic Synthetic Controls vs. Panel-Aware Double Machine Learning for Geo-Level Marketing Impact Estimation](https://arxiv.org/abs/2508.20335)
*Sang Su Lee,Vineeth Loganathan,Vijay Raghavan*

Main category: cs.LG

TL;DR: 本研究通过模拟器评估了在复杂场景下量化地理层面营销效果的多种方法。结果表明，增强型合成控制法（ASC）在复杂情况下存在严重偏差，而面板双重机器学习（DML）方法则更为稳健和准确。文章提出一个“先诊断后选择”的DML模型框架，以提高地理实验分析的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在双边市场中，准确量化地理层面的营销提升效果极具挑战性。合成控制法（SCM）虽有高功效，但常系统性低估效应大小。同时，面板双重机器学习（DML）方法鲜少与SCM进行基准测试，其在量化营销效果方面的表现尚不明确。

Method: 研究构建了一个开放、完整的模拟器，模仿典型的大规模地理推广，并允许用户调整关键参数。该模拟器在五种风格化的压力测试（包括曲线基线趋势、异质响应滞后、处理偏差冲击、非线性结果链接和漂移对照组趋势）下，对两种方法家族进行了探测。评估了七种估计器：三种标准的增强型合成控制法（ASC）变体和四种面板双重机器学习（DML）变体（包括TWFE、CRE/Mundlak、first-difference和within-group）。每个场景进行100次重复实验。

Result: 在涉及非线性或外部冲击的挑战性场景中，增强型合成控制法（ASC）模型持续表现出严重偏差和接近零的95%置信区间覆盖率。相比之下，面板双重机器学习（DML）变体显著降低了这种偏差，并恢复了标称的95%置信区间覆盖率，证明其远比ASC方法更稳健。

Conclusion: 研究结果表明，尽管增强型合成控制法（ASC）提供了一个简单的基线，但在常见的复杂场景中并不可靠。因此，我们提出了一个“先诊断后选择”的框架，建议实践者首先识别主要的业务挑战（例如，非线性趋势、响应滞后），然后选择最适合该场景的特定DML模型，从而为分析地理实验提供一个更稳健可靠的蓝图。

Abstract: Accurately quantifying geo-level marketing lift in two-sided marketplaces is
challenging: the Synthetic Control Method (SCM) often exhibits high power yet
systematically under-estimates effect size, while panel-style Double Machine
Learning (DML) is seldom benchmarked against SCM. We build an open, fully
documented simulator that mimics a typical large-scale geo roll-out: N_unit
regional markets are tracked for T_pre weeks before launch and for a further
T_post-week campaign window, allowing all key parameters to be varied by the
user and probe both families under five stylized stress tests: 1) curved
baseline trends, 2) heterogeneous response lags, 3) treated-biased shocks, 4) a
non-linear outcome link, and 5) a drifting control group trend.
  Seven estimators are evaluated: three standard Augmented SCM (ASC) variants
and four panel-DML flavors (TWFE, CRE/Mundlak, first-difference, and
within-group). Across 100 replications per scenario, ASC models consistently
demonstrate severe bias and near-zero coverage in challenging scenarios
involving nonlinearities or external shocks. By contrast, panel-DML variants
dramatically reduce this bias and restore nominal 95%-CI coverage, proving far
more robust.
  The results indicate that while ASC provides a simple baseline, it is
unreliable in common, complex situations. We therefore propose a
'diagnose-first' framework where practitioners first identify the primary
business challenge (e.g., nonlinear trends, response lags) and then select the
specific DML model best suited for that scenario, providing a more robust and
reliable blueprint for analyzing geo-experiments.

</details>


### [159] [Adaptive Segmentation of EEG for Machine Learning Applications](https://arxiv.org/abs/2508.20336)
*Johnson Zhou,Joseph West,Krista A. Ehinger,Zhenming Ren,Sam E. John,David B. Grayden*

Main category: cs.LG

TL;DR: 本文提出并验证了一种名为CTXSEG的自适应EEG数据分段方法，该方法能根据统计差异创建可变长度段，并在癫痫发作检测任务中表现出优于传统固定长度分段的性能，同时所需数据段更少。


<details>
  <summary>Details</summary>
Motivation: 目前的EEG数据分段方法采用任意固定时间片，但大脑状态并非局限于固定间隔，导致这种方法可能缺乏生物学相关性。因此，研究旨在探讨自适应分段方法是否能有效改进机器学习EEG分析。

Method: 引入了一种名为CTXSEG的新型自适应分段方法，它根据EEG数据的统计差异创建可变长度段，并提出了将其与需要固定长度输入的机器学习方法结合的策略。该方法使用自行开发的信号生成器CTXGEN生成的合成数据进行评估，并在一个实际的EEG癫痫发作检测问题中进行了验证，与固定长度分段方法在机器学习流程的预处理步骤中进行了性能比较。

Result: 研究发现，使用CTXSEG对EEG数据进行预处理可以提高癫痫发作检测性能，优于固定长度分段方法。这种改进无需修改机器学习方法即可实现，并且所需的信号段数量更少。

Conclusion: 本研究表明，CTXSEG自适应分段方法可以方便地应用于现代机器学习，并有潜力提升性能。它为信号预处理中的固定长度分段提供了一个有前景的替代方案，应被考虑纳入EEG机器学习应用的标准化预处理方法库中。

Abstract: Objective. Electroencephalography (EEG) data is derived by sampling
continuous neurological time series signals. In order to prepare EEG signals
for machine learning, the signal must be divided into manageable segments. The
current naive approach uses arbitrary fixed time slices, which may have limited
biological relevance because brain states are not confined to fixed intervals.
We investigate whether adaptive segmentation methods are beneficial for machine
learning EEG analysis.
  Approach. We introduce a novel adaptive segmentation method, CTXSEG, that
creates variable-length segments based on statistical differences in the EEG
data and propose ways to use them with modern machine learning approaches that
typically require fixed-length input. We assess CTXSEG using controllable
synthetic data generated by our novel signal generator CTXGEN. While our CTXSEG
method has general utility, we validate it on a real-world use case by applying
it to an EEG seizure detection problem. We compare the performance of CTXSEG
with fixed-length segmentation in the preprocessing step of a typical EEG
machine learning pipeline for seizure detection.
  Main results. We found that using CTXSEG to prepare EEG data improves seizure
detection performance compared to fixed-length approaches when evaluated using
a standardized framework, without modifying the machine learning method, and
requires fewer segments.
  Significance. This work demonstrates that adaptive segmentation with CTXSEG
can be readily applied to modern machine learning approaches, with potential to
improve performance. It is a promising alternative to fixed-length segmentation
for signal preprocessing and should be considered as part of the standard
preprocessing repertoire in EEG machine learning applications.

</details>


### [160] [Understanding Incremental Learning with Closed-form Solution to Gradient Flow on Overparamerterized Matrix Factorization](https://arxiv.org/abs/2508.20344)
*Hancheng Min,René Vidal*

Main category: cs.LG

TL;DR: 本文定量分析了梯度流在对称矩阵分解中的增量学习现象，发现其源于学习不同分量动力学之间的时间尺度分离，且小初始化能增强此分离，实现低秩近似。


<details>
  <summary>Details</summary>
Motivation: 许多理论研究将神经网络的优异性能归因于优化算法的隐式偏置。其中，梯度流在矩阵分解问题中的增量学习现象是一个关键示例。本研究旨在对梯度流在对称矩阵分解问题上的增量学习行为进行定量理解。

Method: 通过求解一个类Riccati矩阵微分方程，获得了梯度流在对称矩阵分解问题上的闭式解，并利用此解对增量学习行为进行定量分析。

Result: 研究表明，增量学习源于学习目标矩阵中不同分量动力学之间的时间尺度分离。通过减小初始化规模，这些时间尺度分离变得更加显著，从而能够找到目标矩阵的低秩近似。

Conclusion: 梯度流在对称矩阵分解中的增量学习是由于学习不同组件动力学的时间尺度分离所致，且小初始化能强化此效应，有助于低秩近似。未来工作可将此分析扩展至非对称矩阵分解问题。

Abstract: Many theoretical studies on neural networks attribute their excellent
empirical performance to the implicit bias or regularization induced by
first-order optimization algorithms when training networks under certain
initialization assumptions. One example is the incremental learning phenomenon
in gradient flow (GF) on an overparamerterized matrix factorization problem
with small initialization: GF learns a target matrix by sequentially learning
its singular values in decreasing order of magnitude over time. In this paper,
we develop a quantitative understanding of this incremental learning behavior
for GF on the symmetric matrix factorization problem, using its closed-form
solution obtained by solving a Riccati-like matrix differential equation. We
show that incremental learning emerges from some time-scale separation among
dynamics corresponding to learning different components in the target matrix.
By decreasing the initialization scale, these time-scale separations become
more prominent, allowing one to find low-rank approximations of the target
matrix. Lastly, we discuss the possible avenues for extending this analysis to
asymmetric matrix factorization problems.

</details>


### [161] [DFAMS: Dynamic-flow guided Federated Alignment based Multi-prototype Search](https://arxiv.org/abs/2508.20353)
*Zhibang Yang,Xinke Jiang,Rihong Qiu,Ruiqing Li,Yihang Zhang,Yue Fang,Yongxin Xu,Hongxin Ding,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.LG

TL;DR: 联邦检索（FR）旨在通过外部知识源缓解LLM幻觉，但现有方法在处理模糊查询时检索质量不佳。本文提出DFAMS框架，利用动态信息流（DIF）识别查询意图并对齐知识分区，显著提升了跨领域复杂FR场景下的检索准确率、召回率和下游问答效果。


<details>
  <summary>Details</summary>
Motivation: 当外部知识分散时，联邦检索（FR）旨在通过查询路由来缓解大型语言模型（LLMs）的幻觉问题。然而，现有方法在为模糊查询，特别是在跨领域场景中，难以检索到高质量和相关的文档，这严重限制了它们在支持下游生成任务中的有效性。

Method: 本文提出DFAMS框架，该框架受动态信息流（DIF）启发，旨在识别潜在的查询意图并构建语义对齐的知识分区，从而实现跨异构源的准确检索。具体而言，DFAMS通过利用少量标注查询的梯度信号探测LLM中的DIF，并采用基于Shapley值的归因方法来追踪与意图识别和子域边界检测相关的神经元激活路径。随后，DFAMS利用DIF通过多原型对比学习训练一个对齐模块，从而实现细粒度的源内建模和跨知识库的源间语义对齐。

Result: 实验结果表明，在五个基准测试中，DFAMS在知识分类准确率上优于现有先进的FR方法高达14.37%，在检索召回率上提高了5.38%，在下游问答准确率上提高了6.45%。

Conclusion: 研究结果证明了DFAMS在复杂联邦检索场景中的有效性。

Abstract: Federated Retrieval (FR) routes queries across multiple external knowledge
sources, to mitigate hallucinations of LLMs, when necessary external knowledge
is distributed. However, existing methods struggle to retrieve high-quality and
relevant documents for ambiguous queries, especially in cross-domain scenarios,
which significantly limits their effectiveness in supporting downstream
generation tasks. Inspired by dynamic information flow (DIF), we propose DFAMS,
a novel framework that leverages DIF to identify latent query intents and
construct semantically aligned knowledge partitions for accurate retrieval
across heterogeneous sources. Specifically, DFAMS probes the DIF in LLMs by
leveraging gradient signals from a few annotated queries and employing Shapley
value-based attribution to trace neuron activation paths associated with intent
recognition and subdomain boundary detection. Then, DFAMS leverages DIF to
train an alignment module via multi-prototype contrastive learning, enabling
fine-grained intra-source modeling and inter-source semantic alignment across
knowledge bases. Experimental results across five benchmarks show that DFAMS
outperforms advanced FR methods by up to 14.37% in knowledge classification
accuracy, 5.38% in retrieval recall, and 6.45% in downstream QA accuracy,
demonstrating its effectiveness in complex FR scenarios.

</details>


### [162] [Developing a Multi-Modal Machine Learning Model For Predicting Performance of Automotive Hood Frames](https://arxiv.org/abs/2508.20358)
*Abhishek Indupally,Satchit Ramnath*

Main category: cs.LG

TL;DR: 本文提出了一种多模态机器学习（MMML）架构，用于在概念设计阶段快速评估引擎盖框架的性能，以减少对耗时模拟的依赖，并展示其优于单模态方法的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 设计师难以在不进行大量模拟设置的情况下，评估给定引擎盖框架几何形状的性能，需要一种更高效的方法来加速工程设计流程。

Method: 开发了一种多模态机器学习（MMML）架构，该架构能从同一数据的不同模态中学习，以预测性能指标。该方法旨在通过减少对计算密集型模拟的依赖来提高设计效率。

Result: 研究结果表明，通过结合多个数据模态，MMML优于传统的单模态方法。该模型还能成功预测训练数据集中未包含的两种新框架几何形状的性能，展示了其对未见模型的泛化能力。

Conclusion: MMML在补充传统基于模拟的工作流程方面具有巨大潜力，尤其是在概念设计阶段，它弥合了机器学习与实际工程应用之间的鸿沟，为机器学习技术在工程设计中的广泛应用铺平了道路。

Abstract: Is there a way for a designer to evaluate the performance of a given hood
frame geometry without spending significant time on simulation setup? This
paper seeks to address this challenge by developing a multimodal
machine-learning (MMML) architecture that learns from different modalities of
the same data to predict performance metrics. It also aims to use the MMML
architecture to enhance the efficiency of engineering design processes by
reducing reliance on computationally expensive simulations. The proposed
architecture accelerates design exploration, enabling rapid iteration while
maintaining high-performance standards, especially in the concept design phase.
The study also presents results that show that by combining multiple data
modalities, MMML outperforms traditional single-modality approaches. Two new
frame geometries, not part of the training dataset, are also used for
prediction using the trained MMML model to showcase the ability to generalize
to unseen frame models. The findings underscore MMML's potential in
supplementing traditional simulation-based workflows, particularly in the
conceptual design phase, and highlight its role in bridging the gap between
machine learning and real-world engineering applications. This research paves
the way for the broader adoption of machine learning techniques in engineering
design, with a focus on refining multimodal approaches to optimize structural
development and accelerate the design cycle.

</details>


### [163] [BiListing: Modality Alignment for Listings](https://arxiv.org/abs/2508.20396)
*Guillaume Guy,Mihajlo Grbovic,Chun How Tan,Han Zhao*

Main category: cs.LG

TL;DR: Airbnb开发了BiListing模型，利用大语言模型和预训练图文模型整合房源的文本和图片信息，生成统一的嵌入向量，显著提升了搜索排名效果并带来了数千万美元的额外收入。


<details>
  <summary>Details</summary>
Motivation: Airbnb传统上依赖结构化数据理解和推荐房源，但房源包含多样化的非结构化数据（如多张图片、标题、描述、评论等）。将这些不同类型的非结构化信息整合成单一且有意义的房源表示面临技术挑战，限制了对其丰富信息的有效利用。

Method: 本文提出了BiListing（Bimodal Listing）方法。该方法利用大语言模型和预训练的语言-图像模型，对房源的文本和图片进行对齐，并为每个房源和模态生成单一的嵌入向量。BiListing支持零样本语义搜索、克服冷启动问题，并能实现单一模态或双模态的房源间搜索。

Result: 通过离线和在线测试，BiListing嵌入成功部署到Airbnb搜索排名模型中。在线测试显示，其实现了0.425%的NDCG提升，并带来了数千万美元的增量收入。

Conclusion: BiListing有效地解决了整合Airbnb房源多样化非结构化数据的难题，通过生成统一的嵌入向量，显著提升了搜索排名性能和用户体验，并带来了可观的商业价值。

Abstract: Airbnb is a leader in offering travel accommodations. Airbnb has historically
relied on structured data to understand, rank, and recommend listings to guests
due to the limited capabilities and associated complexity arising from
extracting meaningful information from text and images. With the rise of
representation learning, leveraging rich information from text and photos has
become easier. A popular approach has been to create embeddings for text
documents and images to enable use cases of computing similarities between
listings or using embeddings as features in an ML model.
  However, an Airbnb listing has diverse unstructured data: multiple images,
various unstructured text documents such as title, description, and reviews,
making this approach challenging. Specifically, it is a non-trivial task to
combine multiple embeddings of different pieces of information to reach a
single representation.
  This paper proposes BiListing, for Bimodal Listing, an approach to align text
and photos of a listing by leveraging large-language models and pretrained
language-image models. The BiListing approach has several favorable
characteristics: capturing unstructured data into a single embedding vector per
listing and modality, enabling zero-shot capability to search inventory
efficiently in user-friendly semantics, overcoming the cold start problem, and
enabling listing-to-listing search along a single modality, or both.
  We conducted offline and online tests to leverage the BiListing embeddings in
the Airbnb search ranking model, and successfully deployed it in production,
achieved 0.425% of NDCB gain, and drove tens of millions in incremental
revenue.

</details>


### [164] [TF-TransUNet1D: Time-Frequency Guided Transformer U-Net for Robust ECG Denoising in Digital Twin](https://arxiv.org/abs/2508.20398)
*Shijie Wang,Lei Li*

Main category: cs.LG

TL;DR: 本文提出TF-TransUNet1D，一种结合U-Net和Transformer的一维深度神经网络，通过混合时频域损失函数，实现对心电图（ECG）信号的高精度去噪，有效提升了诊断效用和心脏数字孪生的可靠性。


<details>
  <summary>Details</summary>
Motivation: 心电图（ECG）信号作为心脏数字孪生的基础数据源，其诊断效用常因噪声和伪影而受损。因此，需要开发有效的方法来去除噪声，以保证ECG信号的诊断完整性。

Method: 本文提出TF-TransUNet1D，一种新颖的一维深度神经网络。该模型整合了基于U-Net的编码器-解码器架构与Transformer编码器，并通过混合时频域损失函数进行指导。该模型旨在同时捕获局部形态特征和长程时间依赖性。为增强去噪鲁棒性，引入了双域损失函数，联合优化时域波形重建和频域频谱保真度，特别是在频域有效抑制高频噪声并保持频谱结构。

Result: TF-TransUNet1D在MIT-BIH心律失常数据库和噪声压力测试数据库（NSTDB）上进行了评估，与现有最先进的基线方法相比，在信噪比（SNR）改善和误差指标方面表现出持续的优越性，实现了0.1285的平均绝对误差和0.9540的皮尔逊相关系数。

Conclusion: TF-TransUNet1D通过提供高精度去噪，弥补了心脏数字孪生预处理流程中的关键空白，从而能够实现更可靠的实时监测和个性化建模。

Abstract: Electrocardiogram (ECG) signals serve as a foundational data source for
cardiac digital twins, yet their diagnostic utility is frequently compromised
by noise and artifacts. To address this issue, we propose TF-TransUNet1D, a
novel one-dimensional deep neural network that integrates a U-Net-based
encoder-decoder architecture with a Transformer encoder, guided by a hybrid
time-frequency domain loss. The model is designed to simultaneously capture
local morphological features and long-range temporal dependencies, which are
critical for preserving the diagnostic integrity of ECG signals. To enhance
denoising robustness, we introduce a dual-domain loss function that jointly
optimizes waveform reconstruction in the time domain and spectral fidelity in
the frequency domain. In particular, the frequency-domain component effectively
suppresses high-frequency noise while maintaining the spectral structure of the
signal, enabling recovery of subtle but clinically significant waveform
components. We evaluate TF-TransUNet1D using synthetically corrupted signals
from the MIT-BIH Arrhythmia Database and the Noise Stress Test Database
(NSTDB). Comparative experiments against state-of-the-art baselines demonstrate
consistent superiority of our model in terms of SNR improvement and error
metrics, achieving a mean absolute error of 0.1285 and Pearson correlation
coefficient of 0.9540. By delivering high-precision denoising, this work
bridges a critical gap in pre-processing pipelines for cardiac digital twins,
enabling more reliable real-time monitoring and personalized modeling.

</details>


### [165] [Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full Context-Aware Linear Attention](https://arxiv.org/abs/2508.20407)
*Zhongpan Tang*

Main category: cs.LG

TL;DR: 本文提出TLinFormer，一种新型线性注意力架构，通过重新配置神经元连接模式，在保持精确注意分数和完整历史上下文的同时，将Transformer的二次复杂度降至线性，显著提升了长序列任务的性能。


<details>
  <summary>Details</summary>
Motivation: Transformer的核心自注意力机制存在与序列长度呈二次方增长的复杂性瓶颈，严重限制了其在长序列任务中的应用。现有线性注意力方法通常通过牺牲模型性能来实现效率提升。

Method: TLinFormer通过重新配置神经元连接模式，从连接主义的第一性原理出发，实现了严格的线性复杂性。该方法能够在计算精确注意分数的同时，确保信息流保持对完整历史上下文的感知。

Result: 实验结果表明，TLinFormer在长序列推理任务中，相比标准Transformer基线，在推理延迟、KV缓存效率、内存占用和整体加速比等关键指标上表现出压倒性优势。

Conclusion: TLinFormer成功弥合了现有高效注意力方法与标准注意力之间的性能差距，为长序列任务提供了一种高效且高性能的解决方案，有效解决了Transformer的复杂性瓶颈。

Abstract: The Transformer architecture has become a cornerstone of modern artificial
intelligence, but its core self-attention mechanism suffers from a complexity
bottleneck that scales quadratically with sequence length, severely limiting
its application in long-sequence tasks. To address this challenge, existing
linear attention methods typically sacrifice model performance by relying on
data-agnostic kernel approximations or restrictive context selection. This
paper returns to the first principles of connectionism, starting from the
topological structure of information flow, to introduce a novel linear
attention architecture-\textbf{TLinFormer}. By reconfiguring neuron connection
patterns, TLinFormer achieves strict linear complexity while computing exact
attention scores and ensuring information flow remains aware of the full
historical context. This design aims to bridge the performance gap prevalent
between existing efficient attention methods and standard attention. Through a
series of experiments, we systematically evaluate the performance of TLinFormer
against a standard Transformer baseline on long-sequence inference tasks. The
results demonstrate that TLinFormer exhibits overwhelming advantages in key
metrics such as \textbf{inference latency}, \textbf{KV cache efficiency},
\textbf{memory footprint}, and \textbf{overall speedup}.

</details>


### [166] [Assessing local deformation and computing scalar curvature with nonlinear conformal regularization of decoders](https://arxiv.org/abs/2508.20413)
*Benjamin Couéraud,Vikram Sunkara,Christof Schütte*

Main category: cs.LG

TL;DR: 本文提出一种针对深度神经网络解码映射的非线性共形正则化方法，旨在量化自编码器学习到的流形的局部形变，并计算其标量曲率。


<details>
  <summary>Details</summary>
Motivation: 高维数据降维对于发现关键解释因素至关重要。自编码器是学习低维表示的有效方法，但需要进一步理解其解码映射过程中局部形变的情况以及学习流形的几何特性。

Method: 引入一种名为非线性共形正则化的几何正则化方法，应用于由深度神经网络近似的解码映射。该方法允许解码映射的局部变化，并引入一个“共形因子”作为潜在空间映射到原始数据空间时局部形变量的定量指标。同时，该技术还能够计算学习流形的标量曲率。

Result: 开发了一个新的标量场（共形因子），可以定量指示潜在空间到原始数据空间的局部形变量。证明了该正则化技术能够计算学习流形的标量曲率。通过在Swiss roll和CelebA数据集上的实验，演示了如何从模型架构中获取这些量。

Conclusion: 非线性共形正则化为自编码器学习流形的解码映射提供了新的分析工具，不仅能量化局部形变，还能计算流形的标量曲率，从而加深对高维数据低维表示几何特性的理解。

Abstract: One aim of dimensionality reduction is to discover the main factors that
explain the data, and as such is paramount to many applications. When working
with high dimensional data, autoencoders offer a simple yet effective approach
to learn low-dimensional representations. The two components of a general
autoencoder consist first of an encoder that maps the observed data onto a
latent space; and second a decoder that maps the latent space back to the
original observation space, which allows to learn a low-dimensional manifold
representation of the original data. In this article, we introduce a new type
of geometric regularization for decoding maps approximated by deep neural
networks, namely nonlinear conformal regularization. This regularization
procedure permits local variations of the decoder map and comes with a new
scalar field called conformal factor which acts as a quantitative indicator of
the amount of local deformation sustained by the latent space when mapped into
the original data space. We also show that this regularization technique allows
the computation of the scalar curvature of the learned manifold. Implementation
and experiments on the Swiss roll and CelebA datasets are performed to
illustrate how to obtain these quantities from the architecture.

</details>


### [167] [On Identifying Why and When Foundation Models Perform Well on Time-Series Forecasting Using Automated Explanations and Rating](https://arxiv.org/abs/2508.20437)
*Michael Widener,Kausik Lakkaraju,John Aydin,Biplav Srivastava*

Main category: cs.LG

TL;DR: 现有时间序列预测模型（TSFM）的成功与失败原因及不透明性是一个挑战。本研究结合可解释AI（XAI）和评级驱动解释（RDE）方法，评估不同TSFM在多领域表现和可解释性。结果显示，特征工程模型在波动或稀疏领域优于基础模型并更具可解释性，而基础模型仅在稳定或趋势驱动环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列预测模型（TSFM）已广泛应用于对现实世界产生影响的决策，但理解这些模型何时及为何成功或失败仍然具有挑战性。模型的复杂性、性能变异性和不透明性引发了用户如何与模型交互并依赖其输出的严重担忧。因此，理解这些方面成为一项有价值的工作。

Method: 本研究结合了传统可解释AI（XAI）方法与评级驱动解释（RDE），以评估时间序列预测模型（TSFM）在不同领域和用例中的性能和可解释性。评估了四种模型架构：ARIMA、Gradient Boosting、Chronos（时间序列专用基础模型）和Llama（通用基础模型，包括微调和基础版本），并在金融、能源、交通和汽车销售等四个异构数据集上进行验证。

Result: 研究表明，特征工程模型（如Gradient Boosting）在波动或稀疏领域（如电力、汽车零部件）中持续优于基础模型（如Chronos），并能提供更具可解释性的结果。相反，基础模型仅在稳定或趋势驱动的环境（如金融）中表现出色。

Conclusion: 选择时间序列预测模型时，应根据具体应用领域的特性（如波动性、稀疏性或稳定性、趋势驱动）进行权衡。特征工程模型在不稳定、稀疏场景下表现更优且可解释性强，而基础模型则更适用于稳定、趋势明确的场景，这强调了根据上下文进行模型选择的重要性，以实现可靠且可解释的预测。

Abstract: Time-series forecasting models (TSFM) have evolved from classical statistical
methods to sophisticated foundation models, yet understanding why and when
these models succeed or fail remains challenging. Despite this known
limitation, time series forecasting models are increasingly used to generate
information that informs real-world actions with equally real consequences.
Understanding the complexity, performance variability, and opaque nature of
these models then becomes a valuable endeavor to combat serious concerns about
how users should interact with and rely on these models' outputs. This work
addresses these concerns by combining traditional explainable AI (XAI) methods
with Rating Driven Explanations (RDE) to assess TSFM performance and
interpretability across diverse domains and use cases. We evaluate four
distinct model architectures: ARIMA, Gradient Boosting, Chronos (time-series
specific foundation model), Llama (general-purpose; both fine-tuned and base
models) on four heterogeneous datasets spanning finance, energy,
transportation, and automotive sales domains. In doing so, we demonstrate that
feature-engineered models (e.g., Gradient Boosting) consistently outperform
foundation models (e.g., Chronos) in volatile or sparse domains (e.g., power,
car parts) while providing more interpretable explanations, whereas foundation
models excel only in stable or trend-driven contexts (e.g., finance).

</details>


### [168] [Uncovering the Spectral Bias in Diagonal State Space Models](https://arxiv.org/abs/2508.20441)
*Ruben Solozabal,Velibor Bojkovic,Hilal AlQuabeh,Kentaro Inui,Martin Takáč*

Main category: cs.LG

TL;DR: 本文从频率视角研究对角SSM初始化方案，提出S4D-DFouT，并在LRA基准上取得了最先进的结果，支持在大数据集上从头开始训练。


<details>
  <summary>Details</summary>
Motivation: 现有状态空间模型（SSM）参数初始化方法主要依赖HiPPO框架，但该框架未明确研究其对角变体的作用。尽管对角替代方案已证明能达到相似性能且效率更高，但其原理及内在学习偏差尚未被充分理解。

Method: 从频率视角深入调查对角SSM初始化方案，旨在系统理解这些模型的参数化方式并揭示对角状态空间模型固有的学习偏差。基于观察结果，提出了一种在离散傅里叶域上的对角初始化方法——S4D-DFouT。

Result: 所提出的方法在Long Range Arena基准测试中取得了最先进的结果。通过对初始化中极点放置作用的深入理解，模型得以进一步扩展，并支持在PathX-256等超大型数据集上从零开始训练。

Conclusion: 通过从频率角度理解对角SSM初始化及其极点放置的作用，本文提出了一种有效且可扩展的对角初始化方案（S4D-DFouT），成功实现了最先进的性能，并解决了在大规模数据集上从头训练的挑战。

Abstract: Current methods for initializing state space models (SSMs) parameters mainly
rely on the \textit{HiPPO framework}, which is based on an online approximation
of orthogonal polynomials. Recently, diagonal alternatives have shown to reach
a similar level of performance while being significantly more efficient due to
the simplification in the kernel computation. However, the \textit{HiPPO
framework} does not explicitly study the role of its diagonal variants. In this
paper, we take a further step to investigate the role of diagonal SSM
initialization schemes from the frequency perspective. Our work seeks to
systematically understand how to parameterize these models and uncover the
learning biases inherent in such diagonal state-space models. Based on our
observations, we propose a diagonal initialization on the discrete Fourier
domain \textit{S4D-DFouT}. The insights in the role of pole placing in the
initialization enable us to further scale them and achieve state-of-the-art
results on the Long Range Arena benchmark, allowing us to train from scratch on
very large datasets as PathX-256.

</details>


### [169] [Towards Mitigating Excessive Forgetting in LLM Unlearning via Entanglement-Aware Unlearning with Proxy Constraint](https://arxiv.org/abs/2508.20443)
*Zhihao Liu,Jian Lou,Yuke Hu,Xiaochen Li,Tailun Chen,Yitian Chen,Zhan Qin*

Main category: cs.LG

TL;DR: 本文提出EAGLE-PC框架，通过纠缠感知损失重加权和代理约束，解决了大型语言模型（LLMs）机器遗忘中遗忘不足和过度遗忘的问题，有效平衡了遗忘效果与模型实用性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在训练时可能包含私人或受版权保护的内容，引发了隐私和所有权担忧。数据所有者希望移除其数据影响，而现有机器遗忘方法缺乏明确的遗忘边界，易导致遗忘不彻底（存在泄露风险）或过度遗忘（损害模型效用）。

Method: 本研究提出EAGLE-PC（Entanglement-Awareness Guided Loss Reweighting with Proxy Constraint）框架，包含两大核心组件：1. 纠缠感知损失重加权：通过测量遗忘样本与保留样本在嵌入空间中的相似性来确定遗忘努力，实现更有针对性的遗忘。2. 代理约束：利用ICL（In-Context Learning）生成的测试数据对遗忘过程进行软正则化，以缓解过度遗忘。EAGLE-PC兼容现有基于梯度的目标函数，可即插即用。

Result: EAGLE-PC在TOFU和MUSE基准测试上进行评估，结果显示其在多个LLMs上持续改善了遗忘-效用权衡。结合NPO+GD优化器，其性能接近完全再训练的水平。

Conclusion: EAGLE-PC提供了一个可扩展且鲁棒的机器遗忘解决方案，有效解决了LLMs数据遗忘中遗忘不足和过度遗忘的挑战，显著提升了遗忘效果和模型实用性之间的平衡。

Abstract: Large language models (LLMs) are trained on massive datasets that may include
private or copyrighted content. Due to growing privacy and ownership concerns,
data owners may request the removal of their data from trained models. Machine
unlearning provides a practical solution by removing the influence of specific
data without full retraining. However, most existing methods lack a sound
forgetting boundary, causing some samples to be under-forgotten, leaving
residual leakage risks, while others remain over-forgotten at the expense of
degraded utility.
  In this work, we propose EAGLE-PC (Entanglement-Awareness Guided Loss
Reweighting with Proxy Constraint), a novel unlearning framework that addresses
these limitations through two key components. First, entanglement-awareness
guided loss reweighting determines the forgetting effort of each sample by
measuring its similarity to retain samples in the embedding space, enabling
more targeted and effective unlearning. Second, a proxy constraint leveraging
ICL (In-Context Learning) generated test data softly regularizes the forgetting
process, effectively mitigating over-forgetting. EAGLE-PC is compatible with
existing gradient-based objectives and serves as a plug-and-play enhancement.
We evaluate EAGLE-PC on the TOFU and MUSE benchmarks, showing consistent
improvements in the forgetting-utility trade-off across multiple LLMs. Combined
with the NPO+GD optimizer, it approaches full retraining performance, offering
a scalable and robust unlearning solution.

</details>


### [170] [Evaluating Differentially Private Generation of Domain-Specific Text](https://arxiv.org/abs/2508.20452)
*Yidan Sun,Viktor Schlegel,Srinivasan Nandakumar,Iqra Zahid,Yuping Wu,Warren Del-Pinto,Goran Nenadic,Siew-Kei Lam,Jie Zhang,Anil A Bharath*

Main category: cs.LG

TL;DR: 该研究引入了一个统一基准来评估差分隐私合成文本数据的效用和保真度，发现严格隐私约束下性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在高风险领域（如医疗、金融）潜力巨大，但隐私和监管障碍阻碍了真实数据的使用。差分隐私合成数据生成是一种有前景的替代方案，但需要系统性评估其效用和保真度。

Method: 引入了一个统一的基准，用于系统性评估在正式差分隐私(DP)保证下生成的文本数据集的效用和保真度。该基准解决了特定领域基准测试中的关键挑战，包括代表性数据的选择、现实的隐私预算、预训练的考虑以及多种评估指标。在五个特定领域数据集上评估了最先进的隐私保护生成方法。

Result: 结果显示，与真实数据相比，合成数据在效用和保真度方面存在显著下降，尤其是在严格的隐私约束下。

Conclusion: 当前方法存在局限性，需要更先进的隐私保护数据共享方法，本研究为在现实场景中评估这些方法树立了先例。

Abstract: Generative AI offers transformative potential for high-stakes domains such as
healthcare and finance, yet privacy and regulatory barriers hinder the use of
real-world data. To address this, differentially private synthetic data
generation has emerged as a promising alternative. In this work, we introduce a
unified benchmark to systematically evaluate the utility and fidelity of text
datasets generated under formal Differential Privacy (DP) guarantees. Our
benchmark addresses key challenges in domain-specific benchmarking, including
choice of representative data and realistic privacy budgets, accounting for
pre-training and a variety of evaluation metrics. We assess state-of-the-art
privacy-preserving generation methods across five domain-specific datasets,
revealing significant utility and fidelity degradation compared to real data,
especially under strict privacy constraints. These findings underscore the
limitations of current approaches, outline the need for advanced
privacy-preserving data sharing methods and set a precedent regarding their
evaluation in realistic scenarios.

</details>


### [171] [Structure-aware Hypergraph Transformer for Diagnosis Prediction in Electronic Health Records](https://arxiv.org/abs/2508.20500)
*Haiyan Wang,Ye Yuan*

Main category: cs.LG

TL;DR: 提出SHGT框架，通过超图和Transformer改进EHR诊断预测，解决现有GNN在高阶依赖和全局推理上的局限。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络（GNN）的方法在建模电子健康记录（EHR）中的医疗代码交互时存在不足：a) 依赖成对关系，无法捕获固有的高阶依赖；b) 局部消息传递机制限制了表示能力。

Method: 本文提出了一种结构感知超图Transformer（SHGT）框架，主要通过：a) 采用超图结构编码器捕获医疗代码间的高阶交互；b) 整合Transformer架构在整个超图上进行推理；c) 设计定制损失函数结合超图重构以保留超图原始结构。

Result: 在真实世界EHR数据集上的实验表明，所提出的SHGT在诊断预测方面优于现有最先进的模型。

Conclusion: SHGT通过有效处理高阶依赖和实现全局推理，显著提升了EHR中的诊断预测性能。

Abstract: Electronic Health Records (EHR) systematically organize patient health data
through standardized medical codes, serving as a comprehensive and invaluable
source for predictive modeling. Graph neural networks (GNNs) have demonstrated
effectiveness in modeling interactions between medical codes within EHR.
However, existing GNN-based methods are inadequate due to: a) their reliance on
pairwise relations fails to capture the inherent higher-order dependencies in
clinical data, and b) the localized message-passing scheme limits
representation power. To address these issues, this paper proposes a novel
Structure-aware HyperGraph Transformer (SHGT) framework following three-fold
ideas: a) employing a hypergraph structural encoder to capture higher-order
interactions among medical codes, b) integrating the Transformer architecture
to reason over the entire hypergraph, and c) designing a tailored loss function
incorporating hypergraph reconstruction to preserve the hypergraph's original
structure. Experiments on real-world EHR datasets demonstrate that the proposed
SHGT outperforms existing state-of-the-art models on diagnosis prediction.

</details>


### [172] [Khiops: An End-to-End, Frugal AutoML and XAI Machine Learning Solution for Large, Multi-Table Databases](https://arxiv.org/abs/2508.20519)
*Marc Boullé,Nicolas Voisine,Bruno Guerraz,Carine Hue,Felipe Olmos,Vladimir Popescu,Stéphane Gouache,Stéphane Bouget,Alexis Bondu,Luc Aurelien Gauthier,Yassine Nair Benrekia,Fabrice Clérot,Vincent Lemaire*

Main category: cs.LG

TL;DR: Khiops是一款基于独特贝叶斯方法的开源机器学习工具，专为大型多表数据库挖掘设计，提供分类/回归、变量选择和聚合功能，并具有高可伸缩性。


<details>
  <summary>Details</summary>
Motivation: 摘要未直接说明研究动机，但可推断其目标是为处理和挖掘大规模、复杂（多表）数据库提供高效且可扩展的机器学习解决方案。

Method: 采用独特的贝叶斯方法；核心分类/回归模型为结合了变量选择和权重学习的朴素贝叶斯分类器；使用离散化模型（数值数据）和值聚类（分类数据）衡量变量重要性；通过自动构建聚合实现多表数据库的命题化。

Result: 已获得学术界广泛关注，有超过20篇相关出版物；能够有效分析包含数百万个体、数万变量和数亿次级表记录的大型数据库；提供Python库和用户界面，易于访问和部署。

Conclusion: Khiops是一个成熟、高性能的开源机器学习工具，凭借其独特的贝叶斯方法和对大型多表数据库的处理能力，在数据挖掘领域具有重要价值。

Abstract: Khiops is an open source machine learning tool designed for mining large
multi-table databases. Khiops is based on a unique Bayesian approach that has
attracted academic interest with more than 20 publications on topics such as
variable selection, classification, decision trees and co-clustering. It
provides a predictive measure of variable importance using discretisation
models for numerical data and value clustering for categorical data. The
proposed classification/regression model is a naive Bayesian classifier
incorporating variable selection and weight learning. In the case of
multi-table databases, it provides propositionalisation by automatically
constructing aggregates. Khiops is adapted to the analysis of large databases
with millions of individuals, tens of thousands of variables and hundreds of
millions of records in secondary tables. It is available on many environments,
both from a Python library and via a user interface.

</details>


### [173] [MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via Generative Reward Learning](https://arxiv.org/abs/2508.20549)
*Weihai Zhi,Jiayan Guo,Shangyang Li*

Main category: cs.LG

TL;DR: MedGR$^2$框架通过共同开发数据生成器和奖励模型，解决了医学视觉语言模型（VLMs）在数据稀缺环境下的泛化问题，实现了高质量多模态医疗数据的自动生成，显著提升了SFT和RL的性能，并达到了跨模态和跨任务的领先泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学VLM应用受限于高 E-质量专家标注数据稀缺。现有数据集上的监督微调（SFT）泛化性差。强化学习（RL）因缺乏可靠奖励信号而受阻。研究旨在突破这一僵局。

Method: 引入了Generative Reward Learning for Medical Reasoning (MedGR$^2$)框架，该框架协同开发数据生成器和奖励模型，实现高质量多模态医疗数据的自动化、持续创建。生成的数据作为SFT和RL（通过Group Relative Policy Optimization, GRPO）的训练来源。

Result: 使用MedGR$^2$生成数据进行SFT已超越使用大规模人工数据集训练的基线模型。结合GRPO进行RL时，模型在跨模态和跨任务泛化方面达到了最先进水平，显著优于专门的RL方法。此外，MedGR$^2$赋能的紧凑模型性能可与参数量大10倍的基石模型相媲美。

Conclusion: MedGR$^2$为高风险领域的数据高效学习提供了一个新范式，将数据稀缺问题转化为数据生成问题，从而充分发挥了强化学习在构建真正可泛化医疗AI方面的潜力。

Abstract: The application of Vision-Language Models (VLMs) in medicine is critically
hampered by the scarcity of high-quality, expert-annotated data. Supervised
Fine-Tuning (SFT) on existing datasets often leads to poor generalization on
unseen modalities and tasks, while Reinforcement Learning (RL), a promising
alternative, is stymied by the lack of reliable reward signals in this
data-scarce domain. To break this impasse, we introduce Generative Reward
Learning for Medical Reasoning (MedGR$^2$), a novel framework that creates a
self-improving virtuous cycle. MedGR$^2$ co-develops a data generator and a
reward model, enabling the automated, continuous creation of high-quality,
multi-modal medical data that serves as both a superior training source for SFT
and RL. Our experiments demonstrate that SFT with MedGR$^2$-produced data
already surpasses baselines trained on large-scale, human-curated datasets.
Crucially, when leveraging this data for RL via Group Relative Policy
Optimization (GRPO), our model achieves state-of-the-art cross-modality and
cross-task generalization, significantly outperforming specialized RL-based
methods. Furthermore, our compact model, empowered by MedGR$^2$, achieves
performance competitive with foundation models possessing over 10 times more
parameters. MedGR$^2$ presents a new paradigm for data-efficient learning in
high-stakes domains, transforming the problem from data scarcity to data
generation and unlocking the full potential of RL for building truly
generalizable medical AI.

</details>


### [174] [Theoretical foundations of the integral indicator application in hyperparametric optimization](https://arxiv.org/abs/2508.20550)
*Roman S. Kulshin,Anatoly A. Sidorov*

Main category: cs.LG

TL;DR: 提出一种基于整合评估的推荐算法超参数优化方法，旨在平衡多项性能指标。


<details>
  <summary>Details</summary>
Motivation: 传统单一指标优化难以全面平衡推荐算法在准确性、排序质量、多样性和资源消耗等方面的性能。

Method: 采用整合评估方法对推荐算法进行超参数优化，将多种性能指标组合成一个统一的综合标准。

Result: 该方法能够实现推荐算法在准确性、排序质量、输出多样性和资源强度之间的有效平衡。

Conclusion: 开发了一种通用的多标准优化工具，不仅适用于推荐系统，还可应用于广泛的机器学习和数据分析任务。

Abstract: The article discusses the concept of hyperparametric optimization of
recommendation algorithms using an integral assessment that combines various
performance indicators into a single consolidated criterion. This approach is
opposed to traditional methods of setting up a single metric and allows you to
achieve a balance between accuracy, ranking quality, variety of output and the
resource intensity of algorithms. The theoretical significance of the research
lies in the development of a universal multi-criteria optimization tool that is
applicable not only in recommendation systems, but also in a wide range of
machine learning and data analysis tasks.

</details>


### [175] [MERIT: Maximum-normalized Element-wise Ratio for Language Model Large-batch Training](https://arxiv.org/abs/2508.20577)
*Yang Luo,Zangwei Zheng,Ziheng Qin,Zirui Zhu,Yong Liu,Yang You*

Main category: cs.LG

TL;DR: 本文提出了一种名为MERIT的新型优化器，通过利用max-norm和逐元素信任比率来有效约束最大注意力对数，从而显著改善了大型语言模型在大批量训练中的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 大批量训练虽能加速深度神经网络训练，但在优化和泛化方面存在挑战。现有优化器如AdamW在语言模型大批量训练时性能下降，原因是注意力层中最大注意力对数急剧增加导致信息瓶颈。LAMB优化器部分解决了此问题，但其基于$l_2$-范数的信任比率在直接影响查询/键权重最大值方面效率不高，且其逐权重信任比率忽略了行或列内权重值之间的关系，导致易出错。

Method: 我们提出了MERIT优化器，它利用max-norm来计算信任比率，以更有效地约束最大注意力对数。此外，MERIT构建了逐元素的信任比率，通过关注局部权重结构来提供更稳健的更新缩放。

Result: 在不同尺寸的GPT-2模型上进行的大批量训练实验表明，MERIT优化器表现出卓越的性能。尤其是在训练GPT-2 Medium模型时，MERIT在6k的批量大小下，与标准批量大小（480）相比，在48B训练tokens下没有出现任何性能下降。

Conclusion: 这项工作强调了在大批量训练中考虑最大注意力对数和更细粒度信任比率的重要性。MERIT成功提高了训练稳定性，并为更大批量规模的使用铺平了道路，从而加速了大型语言模型的开发和迭代。

Abstract: Large-batch training has become a cornerstone in accelerating the training of
deep neural networks, yet it poses challenges in optimization and
generalization. Existing optimizers like AdamW present performance degradation
during language models' large-batch training, due to the information bottleneck
in attention layers caused by the sharp increase of max attention logit. While
the LAMB optimizer partially addresses this issue, some attention layers still
face this issue. The reason is that $l_2$-norm-based trust ratios in LAMB are
less effective in directly influencing the max value of query/key weights.
Furthermore, the weight-wise trust ratio in LAMB is error-prone as it overlooks
relationships of weight values within rows or columns. Building on these
observations, we propose a novel optimizer, MERIT, which leverages the max-norm
to calculate the trust ratio to constrain the max attention logit more
effectively. Moreover, we further construct element-wise trust ratios to
provide more robust update scaling by focusing on local weight structures.
Extensive experiments of large-batch training across various sizes of GPT-2
models demonstrate the superior performance of MERIT. Notably, during the
training of GPT-2 Medium, MERIT enables a 6k batch size without any performance
degradation compared to the standard batch size (480) with 48B training tokens.
This work highlights the importance of considering the max attention logit and
finer-granularity trust ratio in large-batch training. It successfully improves
the training stability and paves the way for larger batch usage, enabling
faster development and iteration of large language models. Code is available at
https://github.com/NUS-HPC-AI-Lab/MERIT.

</details>


### [176] [Unbiased Stochastic Optimization for Gaussian Processes on Finite Dimensional RKHS](https://arxiv.org/abs/2508.20588)
*Neta Shoham,Haim Avron*

Main category: cs.LG

TL;DR: 本文提出了一种针对有限维或可扩展至无限维RKHS的GP核的精确随机推理算法，用于超参数学习，在内存受限时优于现有近似方法。


<details>
  <summary>Details</summary>
Motivation: 现有高斯过程（GP）随机超参数学习方法依赖近似（如偏置随机梯度或诱导点），无法保证收敛到真实边际似然的平稳点。

Method: 我们提出了针对核诱导中等有限维再生核希尔伯特空间（RKHS）的GP的精确随机推理算法。该方法也可扩展到无限维RKHS，但会牺牲精确性。

Result: 无论在有限还是无限维RKHS中，当内存资源限制了可行批量大小和诱导点数量时，我们的方法都比现有方法取得了更好的实验结果。

Conclusion: 该算法在内存受限的环境下，为GP的超参数学习提供了一种精确（或近似精确）且性能优越的随机推理解决方案。

Abstract: Current methods for stochastic hyperparameter learning in Gaussian Processes
(GPs) rely on approximations, such as computing biased stochastic gradients or
using inducing points in stochastic variational inference. However, when using
such methods we are not guaranteed to converge to a stationary point of the
true marginal likelihood. In this work, we propose algorithms for exact
stochastic inference of GPs with kernels that induce a Reproducing Kernel
Hilbert Space (RKHS) of moderate finite dimension. Our approach can also be
extended to infinite dimensional RKHSs at the cost of forgoing exactness. Both
for finite and infinite dimensional RKHSs, our method achieves better
experimental results than existing methods when memory resources limit the
feasible batch size and the possible number of inducing points.

</details>


### [177] [Local Virtual Nodes for Alleviating Over-Squashing in Graph Neural Networks](https://arxiv.org/abs/2508.20597)
*Tuğrul Hasan Karabulut,İnci M. Baytaş*

Main category: cs.LG

TL;DR: 本文提出局部虚拟节点（LVN）方法，通过基于节点中心性引入可训练嵌入的局部虚拟节点，缓解图神经网络（GNN）中的过度挤压问题，在不破坏图全局结构的情况下提高图和节点分类性能。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在处理长距离依赖任务时面临“过度挤压”（over-squashing）挑战，导致信息传输瓶颈。现有解决方案（如图重连或添加虚拟节点）会改变输入图的全局拓扑结构，破坏原始图结构中编码的领域知识，这在特定任务和领域中是不可接受的。

Method: 本研究提出局部虚拟节点（LVN）方法，其带有可训练嵌入，旨在减轻过度挤压效应而不显著破坏输入图的全局结构。LVN的位置由节点中心性决定，以识别潜在的瓶颈区域。该方法通过在可能存在瓶颈的区域提高连接性，并利用在选定中心区域共享的可训练LVN嵌入，促进远距离节点间的通信，且无需增加更多层。

Result: 在基准数据集上进行的广泛实验表明，LVN能够增强结构连通性，并显著提高图分类和节点分类任务的性能。

Conclusion: LVN是一种有效缓解GNN中过度挤压问题的方法，它在不显著破坏输入图全局结构的前提下，提高了GNN在处理长距离依赖任务上的性能和结构连通性。

Abstract: Over-squashing is a challenge in training graph neural networks for tasks
involving long-range dependencies. In such tasks, a GNN's receptive field
should be large enough to enable communication between distant nodes. However,
gathering information from a wide range of neighborhoods and squashing its
content into fixed-size node representations makes message-passing vulnerable
to bottlenecks. Graph rewiring and adding virtual nodes are commonly studied
remedies that create additional pathways around bottlenecks to mitigate
over-squashing. However, these techniques alter the input graph's global
topology and disrupt the domain knowledge encoded in the original graph
structure, both of which could be essential to specific tasks and domains. This
study presents Local Virtual Nodes (LVN) with trainable embeddings to alleviate
the effects of over-squashing without significantly corrupting the global
structure of the input graph. The position of the LVNs is determined by the
node centrality, which indicates the existence of potential bottlenecks. Thus,
the proposed approach aims to improve the connectivity in the regions with
likely bottlenecks. Furthermore, trainable LVN embeddings shared across
selected central regions facilitate communication between distant nodes without
adding more layers. Extensive experiments on benchmark datasets demonstrate
that LVNs can enhance structural connectivity and significantly improve
performance on graph and node classification tasks. The code can be found at
https://github.com/ALLab-Boun/LVN/}{https://github.com/ALLab-Boun/LVN/.

</details>


### [178] [Dimension Agnostic Testing of Survey Data Credibility through the Lens of Regression](https://arxiv.org/abs/2508.20616)
*Debabrota Basu,Sourav Chakraborty,Debarshi Chanda,Buddha Dev Das,Arijit Ghosh,Arnab Ray*

Main category: cs.LG

TL;DR: 本文提出了一种基于任务的方法来评估抽样调查的可信度，该方法引入了模型特定的距离度量，并设计了一个在回归模型背景下样本复杂度与数据维度无关的算法。


<details>
  <summary>Details</summary>
Motivation: 确保下游研究有效性，评估样本调查的可信度至关重要。传统上，这需要估计高维分布间的距离，但其样本复杂度随维度呈指数级增长。然而，考虑到特定模型下结论的稳健性，需要一种更高效的评估方法。

Method: 提出了一种基于任务的抽样调查可信度评估方法，引入了模型特定的距离度量来量化可信度。设计了一个专门用于回归模型的算法，该算法专注于验证调查数据的可信度，而非重构底层回归模型。

Result: 所提出的算法的样本复杂度与数据维度无关，相比之下，通过重构回归模型来验证可信度时，样本复杂度与数据维度呈线性关系。算法的理论正确性得到了证明，并通过数值实验展示了其性能。

Conclusion: 本文成功开发了一种高效且维度无关的基于任务的调查数据可信度评估方法，特别适用于回归模型，有效解决了高维数据下传统方法的样本复杂度问题。

Abstract: Assessing whether a sample survey credibly represents the population is a
critical question for ensuring the validity of downstream research. Generally,
this problem reduces to estimating the distance between two high-dimensional
distributions, which typically requires a number of samples that grows
exponentially with the dimension. However, depending on the model used for data
analysis, the conclusions drawn from the data may remain consistent across
different underlying distributions. In this context, we propose a task-based
approach to assess the credibility of sampled surveys. Specifically, we
introduce a model-specific distance metric to quantify this notion of
credibility. We also design an algorithm to verify the credibility of survey
data in the context of regression models. Notably, the sample complexity of our
algorithm is independent of the data dimension. This efficiency stems from the
fact that the algorithm focuses on verifying the credibility of the survey data
rather than reconstructing the underlying regression model. Furthermore, we
show that if one attempts to verify credibility by reconstructing the
regression model, the sample complexity scales linearly with the dimensionality
of the data. We prove the theoretical correctness of our algorithm and
numerically demonstrate our algorithm's performance.

</details>


### [179] [Supervised Stochastic Gradient Algorithms for Multi-Trial Source Separation](https://arxiv.org/abs/2508.20618)
*Ronak Mehta,Mateus Piovezan Otto,Noah Stanis,Azadeh Yazdan-Shahmorad,Zaid Harchaoui*

Main category: cs.LG

TL;DR: 开发了一种结合多试次监督的随机独立成分分析（ICA）算法，通过融合近端梯度和反向传播，提高了非凸优化的成功率和独立成分的可解释性。


<details>
  <summary>Details</summary>
Motivation: 在许多科学情境中存在多试次监督数据，这为改进独立成分分析（ICA）提供了机会。

Method: 该方法是一种用于独立成分分析的随机算法，它将可逆矩阵空间中的近端梯度类型算法与通过反向传播联合学习预测模型相结合，并融入了多试次监督。

Result: 在合成数据和真实数据实验中，观察到非凸优化的成功率提高，并且独立成分的可解释性增强，这主要归因于额外的监督信息。

Conclusion: 引入多试次监督的随机ICA算法能够显著提升非凸优化的鲁棒性和独立成分的解释性，使其在实际应用中更具价值。

Abstract: We develop a stochastic algorithm for independent component analysis that
incorporates multi-trial supervision, which is available in many scientific
contexts. The method blends a proximal gradient-type algorithm in the space of
invertible matrices with joint learning of a prediction model through
backpropagation. We illustrate the proposed algorithm on synthetic and real
data experiments. In particular, owing to the additional supervision, we
observe an increased success rate of the non-convex optimization and the
improved interpretability of the independent components.

</details>


### [180] [Masked Autoencoders for Ultrasound Signals: Robust Representation Learning for Downstream Applications](https://arxiv.org/abs/2508.20622)
*Immanuel Roßteutscher,Klaus S. Drese,Thorsten Uphues*

Main category: cs.LG

TL;DR: 研究了MAE和ViT在1D超声信号自监督表示学习中的应用与性能。结果表明，预训练模型，尤其是在合成数据上预训练的模型，在下游任务中显著优于从头训练的模型和CNN基线，并具有更好的真实数据迁移性。


<details>
  <summary>Details</summary>
Motivation: MAEs在计算机视觉等领域取得巨大成功，但在1D信号分析，特别是原始超声数据上的应用仍未被充分探索。超声信号在工业应用（如无损检测、结构健康监测）中至关重要，但通常面临标记数据稀缺和信号处理任务特异性的问题。

Method: 提出了一种利用MAE在无标签合成超声信号上进行预训练的方法，以学习鲁棒表示，从而提升下游任务（如飞行时间(ToF)分类）的性能。系统地研究了模型大小、补丁大小和掩蔽比例对预训练效率和下游准确性的影响。

Result: 预训练模型显著优于从头训练的模型和针对下游任务优化的CNN基线。此外，在合成数据上预训练的模型在向真实测量信号的迁移方面表现出比仅在有限真实数据集上训练更优越的性能。

Conclusion: 本研究强调了MAE通过可扩展的自监督学习，在推进超声信号分析方面的巨大潜力。

Abstract: We investigated the adaptation and performance of Masked Autoencoders (MAEs)
with Vision Transformer (ViT) architectures for self-supervised representation
learning on one-dimensional (1D) ultrasound signals. Although MAEs have
demonstrated significant success in computer vision and other domains, their
use for 1D signal analysis, especially for raw ultrasound data, remains largely
unexplored. Ultrasound signals are vital in industrial applications such as
non-destructive testing (NDT) and structural health monitoring (SHM), where
labeled data are often scarce and signal processing is highly task-specific. We
propose an approach that leverages MAE to pre-train on unlabeled synthetic
ultrasound signals, enabling the model to learn robust representations that
enhance performance in downstream tasks, such as time-of-flight (ToF)
classification. This study systematically investigated the impact of model
size, patch size, and masking ratio on pre-training efficiency and downstream
accuracy. Our results show that pre-trained models significantly outperform
models trained from scratch and strong convolutional neural network (CNN)
baselines optimized for the downstream task. Additionally, pre-training on
synthetic data demonstrates superior transferability to real-world measured
signals compared with training solely on limited real datasets. This study
underscores the potential of MAEs for advancing ultrasound signal analysis
through scalable, self-supervised learning.

</details>


### [181] [GDS Agent: A Graph Algorithmic Reasoning Agent](https://arxiv.org/abs/2508.20637)
*Borun Shi,Ioannis Panagiotas*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）在处理大规模图结构数据时仍面临困难。本文提出GDS智能体，通过集成图算法工具并结合预处理和后处理，使LLM能够对图数据进行推理并提供准确答案。新基准测试表明GDS智能体能解决广泛的图任务。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在多模态信息处理和推理方面表现出色，并通过工具调用和检索增强技术能够访问封闭数据源并回答问题，但它们在处理和推理大规模图结构数据时仍然存在困难。

Method: 本文引入了GDS（图数据科学）智能体。该智能体在一个模型上下文协议（MCP）服务器中，集成了全面的图算法工具集，并包括算法结果的预处理（检索）和后处理功能。该服务器可与任何现代LLM即插即用。此外，本文还引入了一个新的基准测试，用于评估中间工具调用和最终响应。

Result: GDS智能体能够让用户针对其数据提出隐式和内在需要图算法推理的问题，并快速获得准确且有依据的答案。新基准测试结果表明，GDS智能体能够解决广泛的图任务。研究还提供了更开放任务的详细案例研究，并分析了智能体表现不佳的场景。

Conclusion: GDS智能体通过集成图算法工具和预处理/后处理机制，有效解决了LLM在处理和推理大规模图结构数据方面的挑战，使其能够解决各种图任务并提供准确答案。尽管仍存在挑战，但其为LLM进行图数据推理提供了一条有前景的路径。

Abstract: Large language models (LLMs) have shown remarkable multimodal information
processing and reasoning ability. When equipped with tools through function
calling and enhanced with retrieval-augmented techniques, compound LLM-based
systems can access closed data sources and answer questions about them.
However, they still struggle to process and reason over large-scale
graph-structure data. We introduce the GDS (Graph Data Science) agent in this
technical report. The GDS agent introduces a comprehensive set of graph
algorithms as tools, together with preprocessing (retrieval) and postprocessing
of algorithm results, in a model context protocol (MCP) server. The server can
be used with any modern LLM out-of-the-box. GDS agent allows users to ask any
question that implicitly and intrinsically requires graph algorithmic reasoning
about their data, and quickly obtain accurate and grounded answers. We also
introduce a new benchmark that evaluates intermediate tool calls as well as
final responses. The results indicate that GDS agent is able to solve a wide
spectrum of graph tasks. We also provide detailed case studies for more
open-ended tasks and study scenarios where the agent struggles. Finally, we
discuss the remaining challenges and the future roadmap.

</details>


### [182] [A Hybrid Stochastic Gradient Tracking Method for Distributed Online Optimization Over Time-Varying Directed Networks](https://arxiv.org/abs/2508.20645)
*Xinli Shi,Xingxing Yuan,Longkang Zhu,Guanghui Wen*

Main category: cs.LG

TL;DR: 本文提出TV-HSGT算法，通过混合随机梯度跟踪和方差削减机制，在时变有向网络中实现无需梯度有界假设的分布式在线优化，并获得改进的动态遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有分布式在线优化算法常依赖梯度有界假设，且在时变有向网络中忽略随机梯度的影响，无法满足数据规模和动态性日益增长的实时决策需求。

Method: 提出TV-HSGT算法，结合混合随机梯度跟踪和方差削减机制。该算法在时变有向图上集成行随机和列随机通信方案，无需Perron向量估计或出度信息。通过结合当前和递归随机梯度，有效减少梯度方差并准确跟踪全局下降方向。

Result: 理论分析表明，TV-HSGT在不假设梯度有界的情况下，能实现改进的动态遗憾界。在逻辑回归任务上的实验结果证实了TV-HSGT在动态和资源受限环境中的有效性。

Conclusion: TV-HSGT算法为时变有向网络中的分布式在线优化提供了一种无需梯度有界假设的有效解决方案，在理论和实践中均表现出优越性能，尤其适用于动态和资源受限的应用场景。

Abstract: With the increasing scale and dynamics of data, distributed online
optimization has become essential for real-time decision-making in various
applications. However, existing algorithms often rely on bounded gradient
assumptions and overlook the impact of stochastic gradients, especially in
time-varying directed networks. This study proposes a novel Time-Varying Hybrid
Stochastic Gradient Tracking algorithm named TV-HSGT, based on hybrid
stochastic gradient tracking and variance reduction mechanisms. Specifically,
TV-HSGT integrates row-stochastic and column-stochastic communication schemes
over time-varying digraphs, eliminating the need for Perron vector estimation
or out-degree information. By combining current and recursive stochastic
gradients, it effectively reduces gradient variance while accurately tracking
global descent directions. Theoretical analysis demonstrates that TV-HSGT can
achieve improved bounds on dynamic regret without assuming gradient
boundedness. Experimental results on logistic regression tasks confirm the
effectiveness of TV-HSGT in dynamic and resource-constrained environments.

</details>


### [183] [VarDiU: A Variational Diffusive Upper Bound for One-Step Diffusion Distillation](https://arxiv.org/abs/2508.20646)
*Leyang Wang,Mingtian Zhang,Zijing Ou,David Barber*

Main category: cs.LG

TL;DR: 针对现有扩散蒸馏方法中梯度估计偏差导致性能次优的问题，本文提出了VarDiU，一种提供无偏梯度估计的变分扩散上界，实现了更高质量、更稳定和更高效的单步扩散蒸馏。


<details>
  <summary>Details</summary>
Motivation: 大多数现有扩散蒸馏方法通过去噪得分匹配（DSM）学习的学生模型得分函数来近似扩散散度的梯度。然而，DSM训练的不完善导致梯度估计有偏，进而造成性能次优。

Method: 本文提出VarDiU（Variational Diffusive Upper Bound），它允许无偏梯度估计，并可直接应用于扩散蒸馏。通过使用VarDiU作为优化目标来训练学生模型。

Result: 将VarDiU与Diff-Instruct进行比较，结果表明VarDiU实现了更高的生成质量，并为单步扩散蒸馏提供了更高效、更稳定的训练过程。

Conclusion: VarDiU通过提供无偏梯度估计器，有效解决了现有扩散蒸馏方法中由于梯度偏差导致的性能问题，显著提升了生成质量，并使训练过程更加高效和稳定。

Abstract: Recently, diffusion distillation methods have compressed thousand-step
teacher diffusion models into one-step student generators while preserving
sample quality. Most existing approaches train the student model using a
diffusive divergence whose gradient is approximated via the student's score
function, learned through denoising score matching (DSM). Since DSM training is
imperfect, the resulting gradient estimate is inevitably biased, leading to
sub-optimal performance. In this paper, we propose VarDiU (pronounced
/va:rdju:/), a Variational Diffusive Upper Bound that admits an unbiased
gradient estimator and can be directly applied to diffusion distillation. Using
this objective, we compare our method with Diff-Instruct and demonstrate that
it achieves higher generation quality and enables a more efficient and stable
training procedure for one-step diffusion distillation.

</details>


### [184] [Physics-Constrained Machine Learning for Chemical Engineering](https://arxiv.org/abs/2508.20649)
*Angan Mukherjee,Victor M. Zavala*

Main category: cs.LG

TL;DR: 物理约束机器学习（PCML）在化工领域应用潜力巨大，但面临多项技术和知识挑战。本文综述了其最新发展，并强调了在闭环实验设计、实时控制和多尺度现象处理等方面的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 尽管PCML已在多个科学和工程领域展现显著优势，但在复杂的化学工程应用中，其适用性受到技术和知识挑战的限制。这些挑战包括物理知识的嵌入方式、与机器学习的有效融合策略、模型在大数据集和模拟器上的扩展性以及预测不确定性的量化。

Method: 本文作为一篇观点性（perspective）综述，通过总结物理约束机器学习（PCML）在化学工程领域的最新发展，并突出强调其在应用中面临的挑战和机遇，特别是聚焦于闭环实验设计、实时动力学与控制以及多尺度现象处理。

Result: 本文总结了物理约束机器学习（PCML）在化学工程领域的最新进展，并明确了其应用所面临的关键技术和知识挑战，包括物理知识嵌入方式、有效的融合策略、模型扩展性及预测不确定性量化等。

Conclusion: PCML在化工领域具有巨大潜力，但仍需克服在物理知识融合、模型规模化及不确定性量化等方面的挑战。未来的研究应聚焦于闭环实验设计、实时动力学与控制、多尺度现象处理等具体应用场景，以充分发挥其优势。

Abstract: Physics-constrained machine learning (PCML) combines physical models with
data-driven approaches to improve reliability, generalizability, and
interpretability. Although PCML has shown significant benefits in diverse
scientific and engineering domains, technical and intellectual challenges
hinder its applicability in complex chemical engineering applications. Key
difficulties include determining the amount and type of physical knowledge to
embed, designing effective fusion strategies with ML, scaling models to large
datasets and simulators, and quantifying predictive uncertainty. This
perspective summarizes recent developments and highlights
challenges/opportunities in applying PCML to chemical engineering, emphasizing
on closed-loop experimental design, real-time dynamics and control, and
handling of multi-scale phenomena.

</details>


### [185] [Self-Composing Neural Operators with Depth and Accuracy Scaling via Adaptive Train-and-Unroll Approach](https://arxiv.org/abs/2508.20650)
*Juncai He,Xinliang Liu,Jinchao Xu*

Main category: cs.LG

TL;DR: 本文提出一种通过自组合和自适应训练方法提升神经算子效率和准确性的新框架。


<details>
  <summary>Details</summary>
Motivation: 受数值偏微分方程迭代解法的启发，旨在通过重复应用单个算子块来加深模型并提升神经算子的效率与准确性。

Method: 1. 设计通过重复应用单个神经算子块来自我组合并逐步加深模型。2. 引入自适应“训练-展开”方法，在训练期间逐渐增加神经算子深度以提高效率。3. 在高频USCT问题中，采用受多网格启发的主干网络。

Result: 1. 揭示了模型深度与准确度之间的缩放定律。2. 通过自适应训练策略显著节省了计算成本。3. 在标准基准测试中取得了最先进的性能（SOTA）。4. 在高频超声计算机断层扫描（USCT）问题上展现了卓越的性能，有效解决了复杂波现象。

Conclusion: 该框架为大规模数据驱动的科学机器学习应用提供了一个计算可行、准确且可扩展的解决方案。

Abstract: In this work, we propose a novel framework to enhance the efficiency and
accuracy of neural operators through self-composition, offering both
theoretical guarantees and practical benefits. Inspired by iterative methods in
solving numerical partial differential equations (PDEs), we design a specific
neural operator by repeatedly applying a single neural operator block, we
progressively deepen the model without explicitly adding new blocks, improving
the model's capacity. To train these models efficiently, we introduce an
adaptive train-and-unroll approach, where the depth of the neural operator is
gradually increased during training. This approach reveals an accuracy scaling
law with model depth and offers significant computational savings through our
adaptive training strategy. Our architecture achieves state-of-the-art (SOTA)
performance on standard benchmarks. We further demonstrate its efficacy on a
challenging high-frequency ultrasound computed tomography (USCT) problem, where
a multigrid-inspired backbone enables superior performance in resolving complex
wave phenomena. The proposed framework provides a computationally tractable,
accurate, and scalable solution for large-scale data-driven scientific machine
learning applications.

</details>


### [186] [Compositionality in Time Series: A Proof of Concept using Symbolic Dynamics and Compositional Data Augmentation](https://arxiv.org/abs/2508.20656)
*Michael Hagmann,Michael Staniek,Stefan Riezler*

Main category: cs.LG

TL;DR: 本文研究临床时间序列是否由有序潜在状态生成，并提出一种组合合成方法来创建合成数据。实验表明，该方法生成的合成数据在预测任务中表现与原始数据相当，甚至在某些下游任务上表现更优，有效解决了数据稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 探究临床时间序列背后的组合结构，以理解数据生成过程；创建高质量合成数据，以缓解临床时间序列预测中数据稀疏和资源不足的问题；加深对临床数据的理解。

Method: 将时间序列的组合性概念化为数据生成过程的一种属性，并开发数据驱动的方法来重建基本状态和组合规则。通过两种基于领域适应的经验测试来评估方法的有效性，这些测试通过比较在原始和合成数据上训练和测试的预测模型的预期风险相似性来评估原始与合成时间序列分布的相似性。

Result: 通过组合合成数据进行训练所达到的测试集性能与使用原始临床时间序列数据训练的性能相当。在组合合成测试数据上评估模型的结果与在原始测试数据上评估的结果相似，并且优于基于随机化的数据增强方法。在序贯器官衰竭评估（SOFA）分数预测任务的额外下游评估中，完全基于组合合成数据训练的模型比基于原始数据训练的模型显示出显著的性能提升。

Conclusion: 临床时间序列可以被理解为由有序的潜在状态序列生成。所提出的组合合成方法能够成功重建这些状态和规则，生成与真实数据质量相当，甚至在特定任务中表现更优的合成数据，有效缓解了临床时间序列预测中的数据稀疏问题，并加深了对临床数据的理解。

Abstract: This work investigates whether time series of natural phenomena can be
understood as being generated by sequences of latent states which are ordered
in systematic and regular ways. We focus on clinical time series and ask
whether clinical measurements can be interpreted as being generated by
meaningful physiological states whose succession follows systematic principles.
Uncovering the underlying compositional structure will allow us to create
synthetic data to alleviate the notorious problem of sparse and low-resource
data settings in clinical time series forecasting, and deepen our understanding
of clinical data. We start by conceptualizing compositionality for time series
as a property of the data generation process, and then study data-driven
procedures that can reconstruct the elementary states and composition rules of
this process. We evaluate the success of this methods using two empirical tests
originating from a domain adaptation perspective. Both tests infer the
similarity of the original time series distribution and the synthetic time
series distribution from the similarity of expected risk of time series
forecasting models trained and tested on original and synthesized data in
specific ways. Our experimental results show that the test set performance
achieved by training on compositionally synthesized data is comparable to
training on original clinical time series data, and that evaluation of models
on compositionally synthesized test data shows similar results to evaluating on
original test data, outperforming randomization-based data augmentation. An
additional downstream evaluation of the prediction task of sequential organ
failure assessment (SOFA) scores shows significant performance gains when model
training is entirely based on compositionally synthesized data compared to
training on original data.

</details>


### [187] [Token Buncher: Shielding LLMs from Harmful Reinforcement Learning Fine-Tuning](https://arxiv.org/abs/2508.20697)
*Weitao Feng,Lixu Wang,Tianyi Wei,Jie Zhang,Chongyang Gao,Sinong Zhan,Peizhuo Lv,Wei Dong*

Main category: cs.LG

TL;DR: 研究发现大型语言模型（LLM）通过强化学习（RL）进行有害微调比监督微调（SFT）更有效，构成更大风险。本文提出TokenBuncher，通过抑制模型响应不确定性来有效防御RL有害微调，同时保持模型良性任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注SFT导致的LLM滥用风险，但作者系统性地证明，在同等计算预算下，RL微调能更有效地破坏LLM的安全对齐并促进高级有害任务协助，揭示了LLM面临的一个更严重、更普遍的新兴威胁。

Method: 本文提出TokenBuncher，首个专门针对RL有害微调的有效防御机制。其核心思想是抑制RL所依赖的模型响应不确定性。通过“熵作为奖励的强化学习”和“Token噪声器”机制来限制不确定性，从而阻止RL利用独特的奖励信号驱动模型产生有害行为，并防止专业领域有害能力的升级。

Result: 广泛的实验表明，TokenBuncher能有效且鲁棒地缓解RL有害微调，同时保持LLM在良性任务上的实用性和可微调性。研究结果强调，RL有害微调比SFT构成更大的系统性风险。

Conclusion: RL有害微调对LLM构成的系统性风险远超SFT。TokenBuncher为应对这种新兴威胁提供了一种有效且通用的防御解决方案。

Abstract: As large language models (LLMs) continue to grow in capability, so do the
risks of harmful misuse through fine-tuning. While most prior studies assume
that attackers rely on supervised fine-tuning (SFT) for such misuse, we
systematically demonstrate that reinforcement learning (RL) enables adversaries
to more effectively break safety alignment and facilitate advanced harmful task
assistance, under matched computational budgets. To counter this emerging
threat, we propose TokenBuncher, the first effective defense specifically
targeting RL-based harmful fine-tuning. TokenBuncher suppresses the foundation
on which RL relies: model response uncertainty. By constraining uncertainty,
RL-based fine-tuning can no longer exploit distinct reward signals to drive the
model toward harmful behaviors. We realize this defense through
entropy-as-reward RL and a Token Noiser mechanism designed to prevent the
escalation of expert-domain harmful capabilities. Extensive experiments across
multiple models and RL algorithms show that TokenBuncher robustly mitigates
harmful RL fine-tuning while preserving benign task utility and finetunability.
Our results highlight that RL-based harmful fine-tuning poses a greater
systemic risk than SFT, and that TokenBuncher provides an effective and general
defense.

</details>


### [188] [EEGDM: Learning EEG Representation with Latent Diffusion Model](https://arxiv.org/abs/2508.20705)
*Shaocong Wang,Tong Liu,Ming Li,Minjing Yu,Yong-Jin Liu*

Main category: cs.LG

TL;DR: EEGDM是一种基于潜在扩散模型的自监督EEG表示学习方法，通过信号生成捕获丰富语义，能在数据有限时学习通用且鲁棒的表示，并在多种下游任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习EEG分析方法在学习可泛化表示方面存在挑战，尤其是在训练数据有限时。现有表示学习方法（如EEGPT、LaBraM）仅依赖简单的掩码重建目标，未能充分捕捉EEG信号的丰富语义和复杂模式。

Method: 本文提出EEGDM，一种基于潜在扩散模型的自监督EEG表示学习方法。它利用EEG信号生成作为自监督目标，将扩散模型转化为强大的表示学习器。EEGDM包含一个EEG编码器，该编码器将EEG信号及其通道增强提炼为紧凑表示，作为条件信息指导扩散模型生成EEG信号，从而形成一个紧凑的潜在空间。

Result: 实验结果表明，EEGDM能够重建高质量的EEG信号，有效学习鲁棒的表示，并在适度的预训练数据量下，在多种下游任务中取得了有竞争力的性能。

Conclusion: EEGDM展现了卓越的泛化能力和实用性，为EEG信号分析提供了一种有效且高效的表示学习解决方案。

Abstract: While electroencephalography (EEG) signal analysis using deep learning has
shown great promise, existing approaches still face significant challenges in
learning generalizable representations that perform well across diverse tasks,
particularly when training data is limited. Current EEG representation learning
methods including EEGPT and LaBraM typically rely on simple masked
reconstruction objective, which may not fully capture the rich semantic
information and complex patterns inherent in EEG signals. In this paper, we
propose EEGDM, a novel self-supervised EEG representation learning method based
on the latent diffusion model, which leverages EEG signal generation as a
self-supervised objective, turning the diffusion model into a strong
representation learner capable of capturing EEG semantics. EEGDM incorporates
an EEG encoder that distills EEG signals and their channel augmentations into a
compact representation, acting as conditional information to guide the
diffusion model for generating EEG signals. This design endows EEGDM with a
compact latent space, which not only offers ample control over the generative
process but also can be leveraged for downstream tasks. Experimental results
show that EEGDM (1) can reconstruct high-quality EEG signals, (2) effectively
learns robust representations, and (3) achieves competitive performance with
modest pre-training data size across diverse downstream tasks, underscoring its
generalizability and practical utility.

</details>


### [189] [Provable Benefits of In-Tool Learning for Large Language Models](https://arxiv.org/abs/2508.20755)
*Sam Houliston,Ambroise Odonnat,Charles Arnal,Vivien Cabannes*

Main category: cs.LG

TL;DR: 本文从理论和实践两方面证明，工具增强型语言模型通过外部检索能实现无限事实回忆，优于模型权重内的记忆，具有更强的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 工具增强型语言模型正在重塑AI，但其理论优势尚未被充分探索。本研究旨在阐明工具内学习（外部检索）相对于权重内学习（记忆）在事实回忆方面的优势。

Method: 通过理论证明，指出模型权重内的记忆能力受参数数量限制，而工具使用能通过高效电路构造实现无限制的事实回忆。通过受控实验验证了这些结果，并进一步比较了预训练大语言模型中教授工具使用与微调记忆事实的效果。

Result: 理论上，模型权重内的记忆能力受其参数数量限制；而工具使用能够实现无限制的事实回忆。实验表明，工具使用模型持续优于记忆型模型。对于预训练大语言模型，教授工具使用和通用规则比微调事实更有效。

Conclusion: 工具增强型工作流不仅实用，而且在理论和实践上都被证明具有更强的可扩展性。

Abstract: Tool-augmented language models, equipped with retrieval, memory, or external
APIs, are reshaping AI, yet their theoretical advantages remain underexplored.
In this paper, we address this question by demonstrating the benefits of
in-tool learning (external retrieval) over in-weight learning (memorization)
for factual recall. We show that the number of facts a model can memorize
solely in its weights is fundamentally limited by its parameter count. In
contrast, we prove that tool-use enables unbounded factual recall via a simple
and efficient circuit construction. These results are validated in controlled
experiments, where tool-using models consistently outperform memorizing ones.
We further show that for pretrained large language models, teaching tool-use
and general rules is more effective than finetuning facts into memory. Our work
provides both a theoretical and empirical foundation, establishing why
tool-augmented workflows are not just practical, but provably more scalable.

</details>


### [190] [Unleashing Uncertainty: Efficient Machine Unlearning for Generative AI](https://arxiv.org/abs/2508.20773)
*Christoforos N. Spartalis,Theodoros Semertzidis,Petros Daras,Efstratios Gavves*

Main category: cs.LG

TL;DR: SAFEMax是一种用于扩散模型机器学习遗忘的新方法，通过最大化生成图像熵来使模型遗忘特定类别信息，并能在早期扩散步骤中控制遗忘与保留的平衡，实现了高效率和有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决扩散模型中高效且可控的机器学习遗忘问题，以便在删除特定敏感信息的同时保持模型性能。

Method: 引入了SAFEMax方法。该方法基于信息论原理，通过最大化生成图像的熵，强制模型在面对不允许类别时生成高斯噪声，从而阻止其去噪过程。此外，它通过选择性地关注早期扩散步骤（其中类别特定信息更为突出）来控制遗忘与保留之间的平衡。

Result: 实验结果表明SAFEMax方法是有效的，并且相对于现有最先进方法，其效率有显著提升。

Conclusion: SAFEMax为扩散模型提供了一种高效、有效且可控的机器学习遗忘解决方案，通过信息熵最大化和对早期扩散步骤的精准调控，成功实现了特定类别信息的遗忘，并展现出优于现有技术的效率表现。

Abstract: We introduce SAFEMax, a novel method for Machine Unlearning in diffusion
models. Grounded in information-theoretic principles, SAFEMax maximizes the
entropy in generated images, causing the model to generate Gaussian noise when
conditioned on impermissible classes by ultimately halting its denoising
process. Also, our method controls the balance between forgetting and retention
by selectively focusing on the early diffusion steps, where class-specific
information is prominent. Our results demonstrate the effectiveness of SAFEMax
and highlight its substantial efficiency gains over state-of-the-art methods.

</details>


### [191] [cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending](https://arxiv.org/abs/2508.20818)
*Anirudh Satheesh,Keenan Powell,Hua Wei*

Main category: cs.LG

TL;DR: 本文提出cMALC-D框架，利用LLM生成语义课程和多样性上下文融合机制，以解决多智能体强化学习在复杂真实世界环境中的泛化性问题，并在交通信号控制中表现出显著的泛化和样本效率提升。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习(MARL)算法在固定模拟环境中训练时，部署到复杂不确定的真实世界场景中表现脆弱。上下文MARL(cMARL)尝试通过训练上下文无关策略解决此问题，但现有cMARL的课程学习方法依赖于多智能体设置中因智能体间动态和部分可观察性而噪声大、不稳定的代理信号（如价值估计）。

Method: 本文提出了cMALC-D（Contextual Multi-Agent LLM-Guided Curriculum Learning with Diversity-Based Context Blending）框架。该框架利用大型语言模型（LLM）生成具有语义意义的课程并提供更鲁棒的评估信号。同时，引入了一种新颖的基于多样性的上下文融合机制，通过结合先前上下文的特征来创建新的训练场景，以防止模式崩溃并鼓励探索。

Result: 在交通信号控制领域的实验表明，与现有的课程学习基线相比，cMALC-D在泛化能力和样本效率方面均有显著提升。

Conclusion: cMALC-D框架通过有效结合LLM生成的鲁棒课程和多样性上下文融合机制，克服了现有cMARL课程学习方法的局限性，在多智能体强化学习中实现了更好的泛化能力和更高的样本效率。

Abstract: Many multi-agent reinforcement learning (MARL) algorithms are trained in
fixed simulation environments, making them brittle when deployed in real-world
scenarios with more complex and uncertain conditions. Contextual MARL (cMARL)
addresses this by parameterizing environments with context variables and
training a context-agnostic policy that performs well across all environment
configurations. Existing cMARL methods attempt to use curriculum learning to
help train and evaluate context-agnostic policies, but they often rely on
unreliable proxy signals, such as value estimates or generalized advantage
estimates that are noisy and unstable in multi-agent settings due to
inter-agent dynamics and partial observability. To address these issues, we
propose Contextual Multi-Agent LLM-Guided Curriculum Learning with
Diversity-Based Context Blending (cMALC-D), a framework that uses Large
Language Models (LLMs) to generate semantically meaningful curricula and
provide a more robust evaluation signal. To prevent mode collapse and encourage
exploration, we introduce a novel diversity-based context blending mechanism
that creates new training scenarios by combining features from prior contexts.
Experiments in traffic signal control domains demonstrate that cMALC-D
significantly improves both generalization and sample efficiency compared to
existing curriculum learning baselines. We provide code at
https://github.com/DaRL-LibSignal/cMALC-D.

</details>


### [192] [GPT-FT: An Efficient Automated Feature Transformation Using GPT for Sequence Reconstruction and Performance Enhancement](https://arxiv.org/abs/2508.20824)
*Yang Gao,Dongjie Wang,Scott Piersall,Ye Zhang,Liqiang Wang*

Main category: cs.LG

TL;DR: 提出一种基于改进GPT模型的新型自动特征变换框架，解决了现有编码器-解码器方法效率低的问题，实现了高性能和高效率。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的特征变换方法虽有效，但依赖于序列编码器-解码器结构，导致计算成本高、参数量大，限制了可扩展性和效率。

Method: 提出一个四步新型框架：1) 变换记录收集，2) 使用改进型GPT模型构建嵌入空间，3) 梯度上升搜索，4) 自回归重建。改进型GPT模型用于特征变换序列重建及通过嵌入空间进行下游任务性能估计与增强，实现多目标优化。

Result: 在基准数据集上的实验结果表明，所提框架性能与基线方法持平或超越，且计算效率显著提升，参数规模减小。

Conclusion: 该工作突显了基于Transformer的架构在可扩展、高性能自动化特征变换方面的巨大潜力。

Abstract: Feature transformation plays a critical role in enhancing machine learning
model performance by optimizing data representations. Recent state-of-the-art
approaches address this task as a continuous embedding optimization problem,
converting discrete search into a learnable process. Although effective, these
methods often rely on sequential encoder-decoder structures that cause high
computational costs and parameter requirements, limiting scalability and
efficiency. To address these limitations, we propose a novel framework that
accomplishes automated feature transformation through four steps:
transformation records collection, embedding space construction with a revised
Generative Pre-trained Transformer (GPT) model, gradient-ascent search, and
autoregressive reconstruction. In our approach, the revised GPT model serves
two primary functions: (a) feature transformation sequence reconstruction and
(b) model performance estimation and enhancement for downstream tasks by
constructing the embedding space. Such a multi-objective optimization framework
reduces parameter size and accelerates transformation processes. Experimental
results on benchmark datasets show that the proposed framework matches or
exceeds baseline performance, with significant gains in computational
efficiency. This work highlights the potential of transformer-based
architectures for scalable, high-performance automated feature transformation.

</details>


### [193] [ATM-GAD: Adaptive Temporal Motif Graph Anomaly Detection for Financial Transaction Networks](https://arxiv.org/abs/2508.20829)
*Zeyue Zhang,Lin Song,Erkang Bao,Xiaoling Lv,Xinyue Wang*

Main category: cs.LG

TL;DR: 本文提出ATM-GAD，一个利用时间图模式和自适应时间窗的图神经网络，显著提升了金融欺诈检测能力，能有效识别传统方法遗漏的复杂欺诈模式。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型和现有基于图的检测器在金融欺诈检测中面临挑战，因为它们难以处理实体交织和快速变化的交易行为，并且忽视了欺诈行为中两个关键的时间特征：反复出现的时间图模式和账户特定的异常活动爆发间隔。

Method: 引入ATM-GAD，一个自适应图神经网络。它包含：1. 时间图模式提取器（Temporal Motif Extractor），用于将账户交易历史浓缩为最能提供信息的图模式，同时保留拓扑结构和时间模式。2. 双注意力模块（Dual-attention blocks），其中IntraA处理单个图模式内的交互，InterA聚合跨图模式的证据。3. 可微分自适应时间窗学习器（Differentiable Adaptive Time-Window Learner），为每个节点定制观察窗口，使其能够精确关注最能揭示欺诈行为的时间片段。

Result: 在四个真实世界数据集上的实验表明，ATM-GAD持续优于七种强大的异常检测基线模型，并成功揭示了早期方法未能发现的欺诈模式。

Conclusion: ATM-GAD通过有效利用时间图模式和自适应时间窗，显著提高了金融欺诈检测的性能和准确性，能够发现更复杂、时间敏感的欺诈行为。

Abstract: Financial fraud detection is essential to safeguard billions of dollars, yet
the intertwined entities and fast-changing transaction behaviors in modern
financial systems routinely defeat conventional machine learning models. Recent
graph-based detectors make headway by representing transactions as networks,
but they still overlook two fraud hallmarks rooted in time: (1) temporal
motifs--recurring, telltale subgraphs that reveal suspicious money flows as
they unfold--and (2) account-specific intervals of anomalous activity, when
fraud surfaces only in short bursts unique to each entity. To exploit both
signals, we introduce ATM-GAD, an adaptive graph neural network that leverages
temporal motifs for financial anomaly detection. A Temporal Motif Extractor
condenses each account's transaction history into the most informative motifs,
preserving both topology and temporal patterns. These motifs are then analyzed
by dual-attention blocks: IntraA reasons over interactions within a single
motif, while InterA aggregates evidence across motifs to expose multi-step
fraud schemes. In parallel, a differentiable Adaptive Time-Window Learner
tailors the observation window for every node, allowing the model to focus
precisely on the most revealing time slices. Experiments on four real-world
datasets show that ATM-GAD consistently outperforms seven strong
anomaly-detection baselines, uncovering fraud patterns missed by earlier
methods.

</details>


### [194] [Practical Physical Layer Authentication for Mobile Scenarios Using a Synthetic Dataset Enhanced Deep Learning Approach](https://arxiv.org/abs/2508.20861)
*Yijia Guo,Junqing Zhang,Y. -W. Peter Hong*

Main category: cs.LG

TL;DR: 本文提出一种基于深度学习的物理层信道状态信息（CSI）认证方案，专门针对移动场景下物联网设备的认证问题。通过CNN-Siamese网络学习CSI相关性，并在仿真和实验中验证了其在动态信道下的优异泛化和认证性能，性能优于传统和FCN基准方法。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）因无线技术快速发展而普及，但无线传输的广播特性导致设备认证存在严重漏洞。物理层认证利用独特信道特性具有前景，但目前仍缺乏一种适用于动态信道变化的实用方案。

Method: 本研究提出了一种基于深度学习的物理层信道状态信息（CSI）认证方案，用于移动场景。具体方法包括：1) 基于WLAN TGn信道模型、信道自相关性和距离相关性生成合成训练数据集，以减少实验数据收集开销。2) 利用基于卷积神经网络（CNN）的Siamese网络学习CSI对之间的时间和空间相关性，并输出相似性分数。3) 采用仿真与基于WiFi IoT开发套件的实验相结合的协同评估方法，并考虑了典型场景。

Result: 仿真和实验评估均表明，所提出的深度学习方法具有出色的泛化性能和卓越的认证性能。实际测量结果显示，与基于全连接网络（FCN）的Siamese模型相比，该方案的曲线下面积（AUC）提高了0.03；与基于相关性的基准算法相比，AUC提高了0.06。

Conclusion: 所提出的基于深度学习的物理层CSI认证方案有效解决了移动场景下物联网设备在动态信道变化中的认证难题。该方案展现出优异的泛化和认证性能，并通过实际测量验证其性能显著优于基于FCN的Siamese模型和相关性基准算法。

Abstract: The Internet of Things (IoT) is ubiquitous thanks to the rapid development of
wireless technologies. However, the broadcast nature of wireless transmissions
results in great vulnerability to device authentication. Physical layer
authentication emerges as a promising approach by exploiting the unique channel
characteristics. However, a practical scheme applicable to dynamic channel
variations is still missing. In this paper, we proposed a deep learning-based
physical layer channel state information (CSI) authentication for mobile
scenarios and carried out comprehensive simulation and experimental evaluation
using IEEE 802.11n. Specifically, a synthetic training dataset was generated
based on the WLAN TGn channel model and the autocorrelation and the distance
correlation of the channel, which can significantly reduce the overhead of
manually collecting experimental datasets. A convolutional neural network
(CNN)-based Siamese network was exploited to learn the temporal and spatial
correlation between the CSI pair and output a score to measure their
similarity. We adopted a synergistic methodology involving both simulation and
experimental evaluation. The experimental testbed consisted of WiFi IoT
development kits and a few typical scenarios were specifically considered. Both
simulation and experimental evaluation demonstrated excellent generalization
performance of our proposed deep learning-based approach and excellent
authentication performance. Demonstrated by our practical measurement results,
our proposed scheme improved the area under the curve (AUC) by 0.03 compared to
the fully connected network-based (FCN-based) Siamese model and by 0.06
compared to the correlation-based benchmark algorithm.

</details>


### [195] [LeMat-Traj: A Scalable and Unified Dataset of Materials Trajectories for Atomistic Modeling](https://arxiv.org/abs/2508.20875)
*Ali Ramlaoui,Martin Siron,Inel Djafar,Joseph Musielewicz,Amandine Rossello,Victor Schmidt,Alexandre Duval*

Main category: cs.LG

TL;DR: 本文介绍了LeMat-Traj，一个包含超过1.2亿原子构型的标准化DFT轨迹数据集，以及开源库LeMaterial-Fetcher，旨在解决机器学习原子间势（MLIPs）训练中数据碎片化和格式不一致的问题，显著降低了训练可迁移和准确MLIPs的门槛。


<details>
  <summary>Details</summary>
Motivation: 准确的机器学习原子间势（MLIPs）的开发受到密度泛函理论（DFT）衍生的量子力学轨迹数据集可用性碎片化和格式不一致的限制。这些数据集生成昂贵，且因格式、元数据和可访问性的差异而难以组合。

Method: 引入了LeMat-Traj，一个从Materials Project、Alexandria和OQMD等大型存储库聚合的、包含超过1.2亿原子构型的精选数据集。该数据集对数据表示进行标准化，协调结果并筛选高质量构型（PBE, PBESol, SCAN, r2SCAN）。此外，还发布了LeMaterial-Fetcher，一个模块化、可扩展的开源库，为社区提供了可重现的框架，以便轻松整合新数据源。

Result: LeMat-Traj显著降低了训练可迁移和准确MLIPs的障碍。通过使用LeMat-Traj对预训练模型进行微调，在弛豫任务上力预测误差显著降低。

Conclusion: LeMat-Traj数据集和LeMaterial-Fetcher库为社区提供了一个重要的资源和可复现的框架，解决了MLIPs训练中的数据瓶颈问题，促进了大规模材料数据集的持续演进和准确MLIPs的发展。

Abstract: The development of accurate machine learning interatomic potentials (MLIPs)
is limited by the fragmented availability and inconsistent formatting of
quantum mechanical trajectory datasets derived from Density Functional Theory
(DFT). These datasets are expensive to generate yet difficult to combine due to
variations in format, metadata, and accessibility. To address this, we
introduce LeMat-Traj, a curated dataset comprising over 120 million atomic
configurations aggregated from large-scale repositories, including the
Materials Project, Alexandria, and OQMD. LeMat-Traj standardizes data
representation, harmonizes results and filters for high-quality configurations
across widely used DFT functionals (PBE, PBESol, SCAN, r2SCAN). It
significantly lowers the barrier for training transferrable and accurate MLIPs.
LeMat-Traj spans both relaxed low-energy states and high-energy, high-force
structures, complementing molecular dynamics and active learning datasets. By
fine-tuning models pre-trained on high-force data with LeMat-Traj, we achieve a
significant reduction in force prediction errors on relaxation tasks. We also
present LeMaterial-Fetcher, a modular and extensible open-source library
developed for this work, designed to provide a reproducible framework for the
community to easily incorporate new data sources and ensure the continued
evolution of large-scale materials datasets. LeMat-Traj and LeMaterial-Fetcher
are publicly available at https://huggingface.co/datasets/LeMaterial/LeMat-Traj
and https://github.com/LeMaterial/lematerial-fetcher.

</details>


### [196] [Turning Tabular Foundation Models into Graph Foundation Models](https://arxiv.org/abs/2508.20906)
*Dmitry Eremeev,Gleb Bazhenov,Oleg Platonov,Artem Babenko,Liudmila Prokhorenkova*

Main category: cs.LG

TL;DR: 本文提出G2T-FM，一个以TabPFNv2为骨干的图基础模型，通过邻居特征聚合和结构嵌入处理多样化的节点特征。模型在上下文学习中表现出色，超越现有图基础模型并与微调GNN相当，微调后甚至超越GNN。


<details>
  <summary>Details</summary>
Motivation: 图机器学习中基础模型的应用及其处理多样化节点特征的潜力尚未充分探索，尤其是非文本特征。受表格基础模型（如TabPFNv2）成功的启发，研究者寻求将其应用于图领域。

Method: 提出G2T-FM模型。具体方法是：将原始节点特征与邻居特征聚合，添加结构嵌入，然后将构建的节点表示输入到TabPFNv2中作为骨干模型。

Result: 在完全上下文学习模式下，G2T-FM表现强劲，显著优于现有图基础模型，并与从头训练的微调GNN性能持平。经过微调后，G2T-FM超越了微调的GNN基线。

Conclusion: 本研究揭示了利用表格基础模型解决图机器学习任务的潜力，并提出了一个有效的新方向。

Abstract: While foundation models have revolutionized such fields as natural language
processing and computer vision, their application and potential within graph
machine learning remain largely unexplored. One of the key challenges in
designing graph foundation models (GFMs) is handling diverse node features that
can vary across different graph datasets. Although many works on GFMs have been
focused exclusively on text-attributed graphs, the problem of handling
arbitrary features of other types in GFMs has not been fully addressed.
However, this problem is not unique to the graph domain, as it also arises in
the field of machine learning for tabular data. In this work, motivated by the
recent success of tabular foundation models like TabPFNv2, we propose G2T-FM, a
simple graph foundation model that employs TabPFNv2 as a backbone.
Specifically, G2T-FM augments the original node features with neighborhood
feature aggregation, adds structural embeddings, and then applies TabPFNv2 to
the constructed node representations. Even in a fully in-context regime, our
model achieves strong results, significantly outperforming publicly available
GFMs and performing on par with well-tuned GNNs trained from scratch. Moreover,
after finetuning, G2T-FM surpasses well-tuned GNN baselines, highlighting the
potential of the proposed approach. More broadly, our paper reveals a
previously overlooked direction of utilizing tabular foundation models for
graph machine learning tasks.

</details>


### [197] [Finite-Time Guarantees for Multi-Agent Combinatorial Bandits with Nonstationary Rewards](https://arxiv.org/abs/2508.20923)
*Katherine B. Adams,Justin J. Boutilier,Qinyang He,Yonatan Mintz*

Main category: cs.LG

TL;DR: 本文研究了具有非平稳效应（如习惯化或恢复）的序贯资源分配问题，并在组合多臂赌博机框架下提出了一种新方法，开发了具有动态遗憾理论保证的算法，并通过糖尿病干预案例取得了显著的实际效果。


<details>
  <summary>Details</summary>
Motivation: 在社区健康干预、数字广告和员工留存等场景中，决策者需在没有个体效应先验知识的情况下，选择代理子集以最大化整体结果。主要挑战在于干预效果随时间动态演变，存在非平稳奖励分布（如习惯化或恢复），且需要平衡个体异质性奖励与探索-利用权衡。

Method: 引入了首个将非平稳奖励（具体表现为习惯化和恢复）纳入组合多臂赌博机（CMAB）文献的框架。开发了具有动态遗憾理论保证的算法，并通过糖尿病干预案例研究展示了其实际效用。

Result: 在糖尿病干预案例研究中，所提出的个性化社区干预算法在项目参与度方面比基线方法提高了三倍之多，验证了该框架在实际应用中的巨大潜力。

Conclusion: 这项工作弥合了自适应学习的理论进展与人群层面行为改变干预的实际挑战之间的鸿沟，为实际应用提供了有力的工具和验证。

Abstract: We study a sequential resource allocation problem where a decision maker
selects subsets of agents at each period to maximize overall outcomes without
prior knowledge of individual-level effects. Our framework applies to settings
such as community health interventions, targeted digital advertising, and
workforce retention programs, where intervention effects evolve dynamically.
Agents may exhibit habituation (diminished response from frequent selection) or
recovery (enhanced response from infrequent selection). The technical challenge
centers on nonstationary reward distributions that lead to changing
intervention effects over time. The problem requires balancing two key
competing objectives: heterogeneous individual rewards and the
exploration-exploitation tradeoff in terms of learning for improved future
decisions as opposed to maximizing immediate outcomes. Our contribution
introduces the first framework incorporating this form of nonstationary rewards
in the combinatorial multi-armed bandit literature. We develop algorithms with
theoretical guarantees on dynamic regret and demonstrate practical efficacy
through a diabetes intervention case study. Our personalized community
intervention algorithm achieved up to three times as much improvement in
program enrollment compared to baseline approaches, validating the framework's
potential for real-world applications. This work bridges theoretical advances
in adaptive learning with practical challenges in population-level behavioral
change interventions.

</details>


### [198] [Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees](https://arxiv.org/abs/2508.21001)
*Yaniv Hassidof,Tom Jurgenson,Kiril Solovey*

Main category: cs.LG

TL;DR: Diffusion Tree (DiTree) 是一种新型运动规划框架，它将扩散策略作为信息采样器集成到采样规划器中，实现了可证明的泛化性和安全性，同时显著提高了复杂动力系统在OOD场景下的规划效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有采样规划器在探索高维状态空间时效率低下；而基于学习的方法虽然运行速度快，但缺乏对域外（OOD）场景的泛化能力和关键的安全保障，限制了其在实际机器人上的部署。

Method: 本文提出了Diffusion Tree (DiTree) 框架，利用扩散策略（DPs）作为信息采样器来有效指导采样规划器（SBPs）中的状态空间搜索。DiTree结合了DPs在局部观测条件下建模专家轨迹复杂分布的能力与SBPs的完备性，从而能在几次动作传播迭代内为复杂动力系统生成可证明安全的解决方案。具体实现中，它将流行的RRT规划器与在单一环境下训练的DP动作采样器相结合。

Result: 在OOD场景的综合评估中，DiTree的运行时间与独立的扩散策略相当，比经典采样规划器快3倍。同时，它在平均成功率上优于独立的扩散策略和采样规划器，比所有其他方法高出约30%的成功率。

Conclusion: DiTree通过将扩散策略的效率与采样规划器的可靠性（包括完备性和可证明的安全性）相结合，成功解决了现有运动规划方法的局限性，提供了一种在复杂动力系统和OOD场景下均表现出色的、可证明泛化且安全的解决方案。

Abstract: Kinodynamic motion planning is concerned with computing collision-free
trajectories while abiding by the robot's dynamic constraints. This critical
problem is often tackled using sampling-based planners (SBPs) that explore the
robot's high-dimensional state space by constructing a search tree via action
propagations. Although SBPs can offer global guarantees on completeness and
solution quality, their performance is often hindered by slow exploration due
to uninformed action sampling. Learning-based approaches can yield
significantly faster runtimes, yet they fail to generalize to
out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety,
thus limiting their deployment on physical robots. We present Diffusion Tree
(DiTree): a \emph{provably-generalizable} framework leveraging diffusion
policies (DPs) as informed samplers to efficiently guide state-space search
within SBPs. DiTree combines DP's ability to model complex distributions of
expert trajectories, conditioned on local observations, with the completeness
of SBPs to yield \emph{provably-safe} solutions within a few action propagation
iterations for complex dynamical systems. We demonstrate DiTree's power with an
implementation combining the popular RRT planner with a DP action sampler
trained on a \emph{single environment}. In comprehensive evaluations on OOD
scenarios, % DiTree has comparable runtimes to a standalone DP (3x faster than
classical SBPs), while improving the average success rate over DP and SBPs.
DiTree is on average 3x faster than classical SBPs, and outperforms all other
approaches by achieving roughly 30\% higher success rate. Project webpage:
https://sites.google.com/view/ditree.

</details>


### [199] [InSQuAD: In-Context Learning for Efficient Retrieval via Submodular Mutual Information to Enforce Quality and Diversity](https://arxiv.org/abs/2508.21003)
*Souradeep Nanda,Anay Majee,Rishabh Iyer*

Main category: cs.LG

TL;DR: 本文提出了InSQuAD，通过子模态互信息（SMI）确保上下文示例的质量和多样性，显著提升了上下文学习（ICL）模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的ICL检索模型常关注查询相关性而忽视了多样性，而多样性对ICL至关重要。

Method: InSQuAD采用两大策略：1) 将ICL任务建模为目标选择问题，引入基于SMI的统一选择策略，以挖掘兼具相关性、质量和多样性的上下文示例。2) 提出一种组合训练范式，通过新颖的基于似然的损失函数学习SMI参数，从而在检索模型中同时强制实现质量和多样性。此外，通过合成生成的释义扩充了现有数据集以辅助学习。

Result: 将经过该策略训练的检索模型与新颖的目标选择公式应用于九个基准数据集时，ICL性能获得了显著提升。

Conclusion: 研究结果验证了InSQuAD方法在增强ICL模型性能方面的有效性。

Abstract: In this paper, we introduce InSQuAD, designed to enhance the performance of
In-Context Learning (ICL) models through Submodular Mutual Information} (SMI)
enforcing Quality and Diversity among in-context exemplars. InSQuAD achieves
this through two principal strategies: First, we model the ICL task as a
targeted selection problem and introduce a unified selection strategy based on
SMIs which mines relevant yet diverse in-context examples encapsulating the
notions of quality and diversity. Secondly, we address a common pitfall in
existing retrieval models which model query relevance, often overlooking
diversity, critical for ICL. InSQuAD introduces a combinatorial training
paradigm which learns the parameters of an SMI function to enforce both quality
and diversity in the retrieval model through a novel likelihood-based loss. To
further aid the learning process we augment an existing multi-hop question
answering dataset with synthetically generated paraphrases. Adopting the
retrieval model trained using this strategy alongside the novel targeted
selection formulation for ICL on nine benchmark datasets shows significant
improvements validating the efficacy of our approach.

</details>


### [200] [Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance](https://arxiv.org/abs/2508.21016)
*Luozhijie Jin,Zijie Qiu,Jie Liu,Zijie Diao,Lifeng Qiao,Ning Ding,Alex Lamb,Xipeng Qiu*

Main category: cs.LG

TL;DR: 本文提出RLG（强化学习引导），一种推理时方法，通过几何平均结合基模型和RL微调模型的输出，以灵活控制扩散模型与复杂下游目标的对齐强度，并实现对齐-质量权衡的动态调整。


<details>
  <summary>Details</summary>
Motivation: 尽管去噪生成模型（如扩散模型）表现出色，但将其输出与复杂下游目标（如人类偏好、组合准确性）对齐仍具挑战。现有的RL微调方法对扩散模型而言次优，并且在微调后控制对齐强度方面缺乏灵活性。

Method: 通过随机微分方程和隐式奖励条件重新解释扩散模型的RL微调，并引入了强化学习引导（RLG）。RLG是一种推理时方法，它通过几何平均结合基础模型和RL微调模型的输出，从而适应分类器无关引导（CFG）。理论分析表明，RLG的引导尺度在数学上等同于调整标准RL目标中的KL正则化系数，从而无需额外训练即可动态控制对齐-质量权衡。

Result: 广泛实验证明，RLG在各种架构、RL算法和下游任务（包括人类偏好、组合控制、可压缩性和文本渲染）上始终能提升RL微调模型的性能。此外，RLG支持插值和外推，为控制生成对齐提供了前所未有的灵活性。

Conclusion: RLG提供了一个实用且理论完善的解决方案，用于在推理时增强和控制扩散模型的对齐，具有出色的灵活性和控制能力。

Abstract: Denoising-based generative models, particularly diffusion and flow matching
algorithms, have achieved remarkable success. However, aligning their output
distributions with complex downstream objectives, such as human preferences,
compositional accuracy, or data compressibility, remains challenging. While
reinforcement learning (RL) fine-tuning methods, inspired by advances in RL
from human feedback (RLHF) for large language models, have been adapted to
these generative frameworks, current RL approaches are suboptimal for diffusion
models and offer limited flexibility in controlling alignment strength after
fine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models
through the lens of stochastic differential equations and implicit reward
conditioning. We introduce Reinforcement Learning Guidance (RLG), an
inference-time method that adapts Classifier-Free Guidance (CFG) by combining
the outputs of the base and RL fine-tuned models via a geometric average. Our
theoretical analysis shows that RLG's guidance scale is mathematically
equivalent to adjusting the KL-regularization coefficient in standard RL
objectives, enabling dynamic control over the alignment-quality trade-off
without further training. Extensive experiments demonstrate that RLG
consistently improves the performance of RL fine-tuned models across various
architectures, RL algorithms, and downstream tasks, including human
preferences, compositional control, compressibility, and text rendering.
Furthermore, RLG supports both interpolation and extrapolation, thereby
offering unprecedented flexibility in controlling generative alignment. Our
approach provides a practical and theoretically sound solution for enhancing
and controlling diffusion model alignment at inference. The source code for RLG
is publicly available at the Github:
https://github.com/jinluo12345/Reinforcement-learning-guidance.

</details>


### [201] [Fast Convergence Rates for Subsampled Natural Gradient Algorithms on Quadratic Model Problems](https://arxiv.org/abs/2508.21022)
*Gil Goldshlager,Jiang Hu,Lin Lin*

Main category: cs.LG

TL;DR: 本文对子采样自然梯度下降（SNGD）及其加速变体SPRING的收敛性进行了理论分析。通过证明SNGD在特定条件下等价于正则化Kaczmarz方法，首次提供了SNGD的快速收敛速率、SPRING的收敛保证以及SPRING能加速SNGD的证明，并解释了SNGD的有效性。


<details>
  <summary>Details</summary>
Motivation: 子采样自然梯度下降（SNGD）在科学机器学习的参数优化任务中取得了显著成果，但其理论解释（特别是收敛性）一直缺失。

Method: 通过分析SNGD及其加速变体SPRING在理想化参数优化问题（即模型为线性、损失函数为强凸二次）中的收敛性来填补理论空白。具体方法是在最小二乘损失特例中证明SNGD等价于正则化Kaczmarz方法，SPRING等价于加速正则化Kaczmarz方法，并推广到一般强凸二次损失情况。

Result: ['在最小二乘损失特例中，证明了SNGD等价于正则化Kaczmarz方法，SPRING等价于加速正则化Kaczmarz方法。', '在温和条件下，首次获得了SNGD的快速收敛速率。', '在任何设定下，首次为SPRING提供了收敛保证。', '首次证明了SPRING能够加速SNGD。', '在一般强凸二次损失下，扩展了正则化Kaczmarz方法的分析，获得了SNGD的快速收敛速率，首次解释了SNGD在最小二乘设置之外的有效性。']

Conclusion: 研究结果表明，随机线性代数工具能够为子采样与曲率感知优化策略之间的相互作用提供新的见解和理论基础。

Abstract: Subsampled natural gradient descent (SNGD) has shown impressive results for
parametric optimization tasks in scientific machine learning, such as neural
network wavefunctions and physics-informed neural networks, but it has lacked a
theoretical explanation. We address this gap by analyzing the convergence of
SNGD and its accelerated variant, SPRING, for idealized parametric optimization
problems where the model is linear and the loss function is strongly convex and
quadratic. In the special case of a least-squares loss, namely the standard
linear least-squares problem, we prove that SNGD is equivalent to a regularized
Kaczmarz method while SPRING is equivalent to an accelerated regularized
Kaczmarz method. As a result, by leveraging existing analyses we obtain under
mild conditions (i) the first fast convergence rate for SNGD, (ii) the first
convergence guarantee for SPRING in any setting, and (iii) the first proof that
SPRING can accelerate SNGD. In the case of a general strongly convex quadratic
loss, we extend the analysis of the regularized Kaczmarz method to obtain a
fast convergence rate for SNGD under stronger conditions, providing the first
explanation for the effectiveness of SNGD outside of the least-squares setting.
Overall, our results illustrate how tools from randomized linear algebra can
shed new light on the interplay between subsampling and curvature-aware
optimization strategies.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [202] [A Comprehensive Survey of 5G URLLC and Challenges in the 6G Era](https://arxiv.org/abs/2508.20205)
*Md. Emadul Haque,Faisal Tariq,Muhammad R A Khandaker,Md. Sakir Hossain,Muhammad Ali Imran,Kai-Kit Wong*

Main category: cs.NI

TL;DR: 本文对5G系统中超可靠低延迟通信（URLLC）方法进行了全面综述，探讨了其高可靠性和低延迟的矛盾目标，并分析了从物理层到MAC层及跨层技术，最终展望了6G时代的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信从以人为中心转向以机器为中心，对速率、延迟和可靠性的要求发生剧变。URLLC应运而生，旨在实现99.999%的极高可靠性并确保1毫秒的低延迟，然而这两个目标本质上相互冲突，促使大量研究致力于解决这一挑战。

Method: 本文采用综合调研方法，对5G系统中的URLLC方法进行详细分析。研究追溯了无线通信中延迟和可靠性问题的历史演变，并采用分层方法，详细讨论了物理层、媒体访问控制（MAC）层以及跨层技术。同时，涵盖了针对各种5G及未来垂直行业的设计考量。

Result: 作为一篇综述文章，其结果是对5G URLLC现有方法和技术的全面梳理和深入分析，包括不同协议层面的实现途径、历史发展轨迹，以及针对不同应用场景的设计考虑。文章并未提出新的实验数据或模型，而是整合和总结了已有研究成果。

Conclusion: 文章最后详细讨论了URLLC面临的挑战，并对未来发展进行了展望，特别关注了新兴的6G范式，为后续研究指明了方向。

Abstract: As the wireless communication paradigm is being transformed from human
centered communication services towards machine centered communication
services, the requirements of rate, latency and reliability for these services
have also been transformed drastically. Thus the concept of Ultra Reliable and
Low Latency Communication (URLLC) has emerged as a dominant theme for 5G and 6G
systems. Though the latency and reliability requirement varies from one use
case to another, URLLC services generally aim to achieve very high reliability
in the range of 99.999\% while ensuring the latency of up to 1 ms. These two
targets are however inherently opposed to one another. Significant amounts of
work have been carried out to meet these ambitious but conflicting targets. In
this article a comprehensive survey of the URLLC approaches in 5G systems are
analysed in detail. Effort has been made to trace the history and evolution of
latency and reliability issues in wireless communication. A layered approach is
taken where physical layer, Medium Access Control (MAC) layer as well as cross
layer techniques are discussed in detail. It also covers the design
consideration for various 5G and beyond verticals. Finally the article
concludes by providing a detailed discussion on challenges and future outlook
with particular focus on the emerging 6G paradigm.

</details>


### [203] [DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource Allocation and Markov Decision Process in Named Data Networking (NDN)](https://arxiv.org/abs/2508.20272)
*Fatemeh Roshanzadeh,Hamid Barati,Ali Barati*

Main category: cs.NI

TL;DR: 本文提出DRR-MDPF，一种结合MDPF与DRR的NDN混合队列管理策略，通过智能预测和公平带宽分配，显著提升了网络性能，并在高流量和有限缓存下表现出鲁棒性与低复杂度。


<details>
  <summary>Details</summary>
Motivation: 在命名数据网络（NDN）中，特别是在动态和高流量条件下，高效的队列和资源管理对于提升网络性能至关重要。

Method: 本文引入DRR-MDPF，一种结合马尔可夫决策过程转发（MDPF）模型和赤字轮询（DRR）算法的混合策略。MDPF使路由器能基于带宽、延迟和未满足兴趣数等指标智能预测最佳转发决策；DRR确保竞争数据流之间的公平自适应带宽分配。每个路由器被建模为能通过持续反馈和概率更新调整策略的学习代理。

Result: 通过ndnSIM模拟，DRR-MDPF在吞吐量、兴趣满足率、丢包率、内容检索时间和负载均衡等多种指标上显著优于现有先进策略（如SAF、RFA、SMDPF和LA-MDPF）。它在有限缓存和高流量下展现出鲁棒性，提供增强的适应性和较低的计算复杂度，并且其多指标决策能力能够实现更精确的接口选择，优化网络性能。

Conclusion: DRR-MDPF是一种智能、自适应且可扩展的NDN队列管理解决方案，能有效应对动态网络环境中的资源分配、拥塞控制和路由优化等核心挑战。

Abstract: Named Data Networking (NDN) represents a transformative shift in network
architecture, prioritizing content names over host addresses to enhance data
dissemination. Efficient queue and resource management are critical to NDN
performance, especially under dynamic and high-traffic conditions. This paper
introduces DRR-MDPF, a novel hybrid strategy that integrates the Markov
Decision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)
algorithm. MDPF enables routers to intelligently predict optimal forwarding
decisions based on key metrics such as bandwidth, delay, and the number of
unsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation
among competing data flows. The proposed method models each router as a
learning agent capable of adjusting its strategies through continuous feedback
and probabilistic updates. Simulation results using ndnSIM demonstrate that
DRR-MDPF significantly outperforms state-of-the-art strategies including SAF,
RFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest
Satisfaction Rate (ISR), packet drop rate, content retrieval time, and load
balancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and
heavy traffic, offering enhanced adaptability and lower computational
complexity due to its single-path routing design. Furthermore, its multi-metric
decision-making capability enables more accurate interface selection, leading
to optimized network performance. Overall, DRR-MDPF serves as an intelligent,
adaptive, and scalable queue management solution for NDN, effectively
addressing core challenges such as resource allocation, congestion control, and
route optimization in dynamic networking environments.

</details>


### [204] [Relay Selection in Wireless Networks as Restless Bandits](https://arxiv.org/abs/2508.20625)
*Mandar R. Nalavade,Ravindra S. Tomar,Gaurav S. Kasbekar*

Main category: cs.NI

TL;DR: 针对无线中继网络中继选择问题，提出了一种基于Whittle指数的策略，以最小化数据包存储成本，并显著提升了网络性能。


<details>
  <summary>Details</summary>
Motivation: 在源节点与目的节点之间直接链路被阻塞的无线网络中，多个中继节点可用。目标是设计一个中继选择策略，以最小化中继节点处长期时间平均的总数据包存储成本。

Method: 将该问题建模为“不安分多臂赌博机”(RMAB)问题。证明了该中继选择问题是Whittle可索引的，并提出了一种计算每个中继Whittle指数的方法。策略选择Whittle指数最小的中继进行数据包传输。

Result: 仿真结果表明，所提出的策略在平均成本、延迟和吞吐量方面均优于现有中继选择策略。

Conclusion: 本研究通过证明Whittle可索引性并设计基于Whittle指数的中继选择策略，成功解决了在阻塞无线链路场景下最小化数据包存储成本的问题，并有效提升了网络性能。

Abstract: We consider a wireless network in which a source node needs to transmit a
large file to a destination node. The direct wireless link between the source
and the destination is assumed to be blocked. Multiple candidate relays are
available to forward packets from the source to the destination. A holding cost
is incurred for each packet stored at every relay in each time slot. The
objective is to design a policy for selecting a relay in each time slot to
which the source attempts to send a packet, so as to minimize the expected
long-run time-averaged total packet holding cost at the relays. This problem is
an instance of the restless multi-armed bandit (RMAB) problem, which is
provably hard to solve. We prove that this relay selection problem is
Whittle-indexable, and propose a method to compute the Whittle index of each
relay in every time slot. In each time slot, our relay selection policy
transmits a packet to the relay with the smallest Whittle index. Using
simulations, we show that the proposed policy outperforms the relay selection
policies proposed in prior work in terms of average cost, delay, as well as
throughput.

</details>


### [205] [Digital Twin-Empowered Deep Reinforcement Learning for Intelligent VNF Migration in Edge-Core Networks](https://arxiv.org/abs/2508.20957)
*Faisal Ahmed,Suresh Subramaniam,Motoharu Matsuura,Hiroshi Hasegawa,Shih-Chun Lin*

Main category: cs.NI

TL;DR: 该研究提出了一个数字孪生（DT）赋能的深度强化学习（DRL）框架，用于智能VNF迁移，旨在共同优化端到端延迟和能耗。通过集成DT模块（包含多任务VAE和LSTM）来生成合成经验，显著提高了训练效率，并在仿真中实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 面对现代边缘-核心网络中服务需求增长和虚拟化网络功能（VNFs）快速部署带来的挑战，需要实现低延迟和高能效的编排。

Method: 提出一个数字孪生（DT）赋能的深度强化学习（DRL）框架，用于智能VNF迁移。该框架将VNF迁移问题建模为马尔可夫决策过程，并利用优势Actor-Critic模型进行决策。其核心创新在于集成了一个DT模块，该模块由多任务变分自编码器（VAE）和多任务长短期记忆（LSTM）网络组成，用于模拟环境动态并生成高质量的合成经验，从而提高训练效率和策略收敛速度。

Result: 仿真结果表明，所提出的框架实现了显著的性能提升，具体表现为平均端到端（E2E）延迟和能耗的大幅降低。

Conclusion: 该研究为边缘-核心网络中的智能VNF迁移建立了新的基准，有效解决了低延迟和高能效编排的挑战。

Abstract: The growing demand for services and the rapid deployment of virtualized
network functions (VNFs) pose significant challenges for achieving low-latency
and energy-efficient orchestration in modern edge-core network infrastructures.
To address these challenges, this study proposes a Digital Twin (DT)-empowered
Deep Reinforcement Learning framework for intelligent VNF migration that
jointly minimizes average end-to-end (E2E) delay and energy consumption. By
formulating the VNF migration problem as a Markov Decision Process and
utilizing the Advantage Actor-Critic model, the proposed framework enables
adaptive and real-time migration decisions. A key innovation of the proposed
framework is the integration of a DT module composed of a multi-task
Variational Autoencoder and a multi-task Long Short-Term Memory network. This
combination collectively simulates environment dynamics and generates
high-quality synthetic experiences, significantly enhancing training efficiency
and accelerating policy convergence. Simulation results demonstrate substantial
performance gains, such as significant reductions in both average E2E delay and
energy consumption, thereby establishing new benchmarks for intelligent VNF
migration in edge-core networks.

</details>


### [206] [RANGAN: GAN-empowered Anomaly Detection in 5G Cloud RAN](https://arxiv.org/abs/2508.20985)
*Douglas Liao,Jiping Luo,Jens Vevstad,Nikolaos Pappas*

Main category: cs.NI

TL;DR: 本文提出了RANGAN，一个结合GAN和Transformer架构的RAN异常检测框架，通过滑动窗口处理时序数据，在公开数据集上识别网络争用问题的F1得分高达83%。


<details>
  <summary>Details</summary>
Motivation: RAN系统复杂、动态且产生海量性能数据，导致异常诊断困难。现有方法难以有效捕捉RAN性能的时序依赖性，因此需要一种能够自适应、精确检测异常的框架。

Method: RANGAN是一个集成了生成对抗网络（GAN）和Transformer架构的异常检测框架。它在数据预处理阶段采用滑动窗口方法，以增强捕捉数据中时序依赖的能力。

Result: RANGAN在Spotlight项目的公开RAN性能数据集上进行了严格评估，实验结果显示其实现了可观的检测精度，在识别网络争用问题上F1得分高达83%。

Conclusion: RANGAN通过结合GAN和Transformer以及滑动窗口的时序数据处理能力，有效克服了RAN系统异常检测的挑战，展现了在实际场景中高精度识别网络异常的潜力。

Abstract: Radio Access Network (RAN) systems are inherently complex, requiring
continuous monitoring to prevent performance degradation and ensure optimal
user experience. The RAN leverages numerous key performance indicators (KPIs)
to evaluate system performance, generating vast amounts of data each second.
This immense data volume can make troubleshooting and accurate diagnosis of
performance anomalies more difficult. Furthermore, the highly dynamic nature of
RAN performance demands adaptive methodologies capable of capturing temporal
dependencies to detect anomalies reliably. In response to these challenges, we
introduce \textbf{RANGAN}, an anomaly detection framework that integrates a
Generative Adversarial Network (GAN) with a transformer architecture. To
enhance the capability of capturing temporal dependencies within the data,
RANGAN employs a sliding window approach during data preprocessing. We
rigorously evaluated RANGAN using the publicly available RAN performance
dataset from the Spotlight project \cite{sun-2024}. Experimental results
demonstrate that RANGAN achieves promising detection accuracy, notably
attaining an F1-score of up to $83\%$ in identifying network contention issues.

</details>


### [207] [DSROQ: Dynamic Scheduling and Routing for QoE Management in LEO Satellite Networks](https://arxiv.org/abs/2508.21047)
*Dhiraj Bhattacharjee,Pablo G. Madoery,Abhishek Naik,Halim Yanikomeroglu,Gunes Karabulut Kurt,Stephane Martel,Khaled Ahmed*

Main category: cs.NI

TL;DR: 为解决LEO卫星网络QoS保障问题，本文提出DSROQ算法，结合MCTS启发式路由/带宽分配与Lyapunov优化调度，显著提升了用户体验和公平性，并揭示了流量敏感度对性能主导因素的影响。


<details>
  <summary>Details</summary>
Motivation: 现代互联网应用对QoS要求各异，LEO卫星星座是满足这些需求，增强覆盖和补充地面网络的有前景方案。在LEO网络中确保QoS，需要联合优化路由、带宽分配和动态队列调度，以有效处理流量并维持服务性能。

Method: 本文将QoS要求作为软约束，构建了旨在最大化用户体验的联合路由与带宽分配问题，并引入自适应调度方法来优先处理流特定的QoS需求。提出一种受蒙特卡洛树搜索（MCTS）启发的方法来解决NP难的路由和带宽分配问题，并在奖励评估期间应用基于Lyapunov优化的调度。

Result: 基于Starlink Phase 1 Version 2星座的对比实验显示，所提出的DSROQ算法在端用户体验和公平性上均优于基准方案，证明了联合路由和带宽决策的优势。此外，研究发现当流量敏感性从延迟驱动转变为带宽驱动时，主导性能的因素从调度转向路由和带宽分配。

Conclusion: DSROQ算法通过联合优化路由、带宽分配和调度，显著提升了LEO卫星网络的用户体验和公平性。研究结果强调了联合决策的有效性，并揭示了在不同流量特性下，网络性能优化的关键因素会发生转移。

Abstract: The modern Internet supports diverse applications with heterogeneous quality
of service (QoS) requirements. Low Earth orbit (LEO) satellite constellations
offer a promising solution to meet these needs, enhancing coverage in rural
areas and complementing terrestrial networks in urban regions. Ensuring QoS in
such networks requires joint optimization of routing, bandwidth allocation, and
dynamic queue scheduling, as traffic handling is critical for maintaining
service performance. This paper formulates a joint routing and bandwidth
allocation problem where QoS requirements are treated as soft constraints,
aiming to maximize user experience. An adaptive scheduling approach is
introduced to prioritize flow-specific QoS needs. We propose a Monte Carlo tree
search (MCTS)-inspired method to solve the NP-hard route and bandwidth
allocation problem, with Lyapunov optimization-based scheduling applied during
reward evaluation. Using the Starlink Phase 1 Version 2 constellation, we
compare end-user experience and fairness between our proposed DSROQ algorithm
and a benchmark scheme. Results show that DSROQ improves both performance
metrics and demonstrates the advantage of joint routing and bandwidth
decisions. Furthermore, we observe that the dominant performance factor shifts
from scheduling to routing and bandwidth allocation as traffic sensitivity
changes from latency-driven to bandwidth-driven.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [208] [Deep Reinforcement Learning for Optimal Asset Allocation Using DDPG with TiDE](https://arxiv.org/abs/2508.20103)
*Rongwei Liu,Jin Zheng,John Cartlidge*

Main category: q-fin.PM

TL;DR: 本研究将最优资产配置问题建模为马尔可夫决策过程，并创新性地将时间序列密集编码器（TiDE）集成到深度确定性策略梯度（DDPG）强化学习框架中，以实现动态资产配置策略。实证结果表明，DDPG-TiDE优于Q-learning和买入并持有策略，能产生更高的风险调整后收益。


<details>
  <summary>Details</summary>
Motivation: 由于金融市场的内在波动性，风险资产与无风险资产之间的最优资产配置是一个持续存在的挑战。传统的配置方法依赖于严格的分布假设或非可加性回报率，这限制了其鲁棒性和对投资目标的适用性。

Method: 本研究将最优两资产配置问题表述为马尔可夫决策过程（MDP）中的序贯决策任务，并应用强化学习（RL）机制来开发动态策略。方法论的核心是首次将时间序列密集编码器（TiDE）集成到深度确定性策略梯度（DDPG）RL框架中以实现连续决策，并利用凯利准则平衡即时回报信号与长期投资目标。研究还将DDPG-TiDE与简单的离散动作Q-learning RL框架以及被动的买入并持有投资策略进行了比较。

Result: 实证结果显示，DDPG-TiDE的表现优于Q-learning，并且比买入并持有策略产生了更高的风险调整后收益。

Conclusion: 研究结果表明，通过将TiDE集成到DDPG强化学习框架中来解决最优资产配置问题是一个富有成效的探索方向。

Abstract: The optimal asset allocation between risky and risk-free assets is a
persistent challenge due to the inherent volatility in financial markets.
Conventional methods rely on strict distributional assumptions or non-additive
reward ratios, which limit their robustness and applicability to investment
goals. To overcome these constraints, this study formulates the optimal
two-asset allocation problem as a sequential decision-making task within a
Markov Decision Process (MDP). This framework enables the application of
reinforcement learning (RL) mechanisms to develop dynamic policies based on
simulated financial scenarios, regardless of prerequisites. We use the Kelly
criterion to balance immediate reward signals against long-term investment
objectives, and we take the novel step of integrating the Time-series Dense
Encoder (TiDE) into the Deep Deterministic Policy Gradient (DDPG) RL framework
for continuous decision-making. We compare DDPG-TiDE with a simple
discrete-action Q-learning RL framework and a passive buy-and-hold investment
strategy. Empirical results show that DDPG-TiDE outperforms Q-learning and
generates higher risk adjusted returns than buy-and-hold. These findings
suggest that tackling the optimal asset allocation problem by integrating TiDE
within a DDPG reinforcement learning framework is a fruitful avenue for further
exploration.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [209] [A Hierarchical Signal Coordination and Control System Using a Hybrid Model-based and Reinforcement Learning Approach](https://arxiv.org/abs/2508.20102)
*Xianyue Peng,Shenyang Chen,H. Michael Zhang*

Main category: eess.SY

TL;DR: 一种分层信号控制方案，结合模型优化与强化学习，能根据需求动态调整策略，实现全需求水平下的鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 城市走廊信号控制面临双重挑战：既要保持主干道交通顺畅，又要适应局部交叉口的需求变化。

Method: 提出分层交通信号协调与控制方案，整合模型优化与强化学习。系统包含高层协调器(HLC)选择策略、走廊协调器推导相位约束（最大流量MFC或绿波GWC），以及混合信号代理(HSA)通过带动作掩码的强化学习决定信号相位。HSA和HLC策略通过Proximal Policy Optimization (PPO)进行分层强化学习训练，HLC在高层动态切换策略以平衡多目标性能。在SUMO-RLlib平台进行开发评估。

Result: 混合MFC在重度需求下最大化吞吐量；混合GWC在不同交通条件下最小化主干道停车和保持通行，但可能降低全网络效率；纯代理控制(PAC)在中度需求下改善全网络旅行时间，但在重度需求下效果不佳。分层设计通过自适应策略选择，在所有需求水平下实现鲁棒性能。

Conclusion: 所提出的分层设计通过自适应策略选择，在所有需求水平下实现了鲁棒的交通性能，有效应对了城市走廊信号控制的挑战。

Abstract: Signal control in urban corridors faces the dual challenge of maintaining
arterial traffic progression while adapting to demand variations at local
intersections. We propose a hierarchical traffic signal coordination and
control scheme that integrates model-based optimization with reinforcement
learning. The system consists of: (i) a High-Level Coordinator (HLC) that
selects coordination strategies based on observed and predicted demand; (ii) a
Corridor Coordinator that derives phase constraints from the selected
strategy-either Max-Flow Coordination (MFC) or Green-Wave Coordination (GWC);
and (iii) Hybrid Signal Agents (HSAs) that determine signal phases via
reinforcement learning with action masking to enforce feasibility. Hierarchical
reinforcement learning with Proximal Policy Optimization (PPO) is used to train
HSA and HLC policies. At the lower level, three HSA policies-MFC-aware,
GWC-aware, and pure agent control (PAC) are trained in conjunction with their
respective coordination strategies. At the higher level, the HLC is trained to
dynamically switch strategies using a multi-objective reward balancing
corridor-level and network-wide performance. The proposed scheme was developed
and evaluated on a SUMO-RLlib platform. Case results show that hybrid MFC
maximizes throughput under heavy demand; hybrid GWC consistently minimizes
arterial stops and maintains progression across diverse traffic conditions but
can reduce network-wide efficiency; and PAC improves network-wide travel time
in moderate demand but is less effective under heavy demand. The hierarchical
design enables adaptive strategy selection, achieving robust performance across
all demand levels.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [210] [Differentially Private Federated Quantum Learning via Quantum Noise](https://arxiv.org/abs/2508.20310)
*Atit Pokharel,Ratun Rahman,Shaba Shaon,Thomas Morris,Dinh C. Nguyen*

Main category: quant-ph

TL;DR: 本文提出了一种利用量子噪声增强量子联邦学习（QFL）差分隐私（DP）的机制，以保护模型信息并抵御对抗性攻击，尤其适用于NISQ设备。


<details>
  <summary>Details</summary>
Motivation: 量子联邦学习（QFL）在分布式量子设备上实现模型协同训练，但易受对抗性攻击，可能泄露模型信息。在噪声中等规模量子（NISQ）设备背景下，核心问题是如何利用固有的量子噪声来强制执行差分隐私（DP），以在训练和通信过程中保护模型信息。

Method: 本研究探索了一种新颖的DP机制，通过调整测量次数和退极化信道强度来利用量子噪声，从而实现定制化的DP水平。通过模拟，评估了DP预算与噪声参数的关系，以及安全性与训练精度之间的权衡。

Result: 模拟结果表明，该框架能有效利用量子噪声实现所需的DP水平，并展示了隐私预算与噪声参数之间的关系，以及安全性与训练准确性之间的权衡。此外，该框架在对抗性攻击（旨在利用对抗性样本损害模型性能）下表现出鲁棒性，并在对抗性样本准确率、正确预测的置信度分数和攻击成功率等关键指标上进行了评估。

Conclusion: 研究揭示了隐私和鲁棒性之间可调的权衡，为NISQ设备上安全的QFL提供了一个高效解决方案，在可靠的量子计算应用方面具有巨大潜力。

Abstract: Quantum federated learning (QFL) enables collaborative training of quantum
machine learning (QML) models across distributed quantum devices without raw
data exchange. However, QFL remains vulnerable to adversarial attacks, where
shared QML model updates can be exploited to undermine information privacy. In
the context of noisy intermediate-scale quantum (NISQ) devices, a key question
arises: How can inherent quantum noise be leveraged to enforce differential
privacy (DP) and protect model information during training and communication?
This paper explores a novel DP mechanism that harnesses quantum noise to
safeguard quantum models throughout the QFL process. By tuning noise variance
through measurement shots and depolarizing channel strength, our approach
achieves desired DP levels tailored to NISQ constraints. Simulations
demonstrate the framework's effectiveness by examining the relationship between
differential privacy budget and noise parameters, as well as the trade-off
between security and training accuracy. Additionally, we demonstrate the
framework's robustness against an adversarial attack designed to compromise
model performance using adversarial examples, with evaluations based on
critical metrics such as accuracy on adversarial examples, confidence scores
for correct predictions, and attack success rates. The results reveal a tunable
trade-off between privacy and robustness, providing an efficient solution for
secure QFL on NISQ devices with significant potential for reliable quantum
computing applications.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [211] [OLMoASR: Open Models and Data for Training Robust Speech Recognition Models](https://arxiv.org/abs/2508.20869)
*Huong Ngo,Matt Deitke,Martijn Bartelds,Sarah Pratt,Josh Gardner,Matt Jordan,Ludwig Schmidt*

Main category: cs.SD

TL;DR: 本文构建了一个大规模高质量英语语音识别数据集OLMoASR-Mix（从3M小时原始数据筛选得到1M小时），并训练了一系列OLMoASR模型，在零样本语音识别基准测试中达到了与OpenAI Whisper相当的性能。


<details>
  <summary>Details</summary>
Motivation: 训练数据的规模和质量对模型进步有显著影响，但其在语音识别领域的具体作用仍未被充分探索。研究旨在开发鲁棒的零样本语音识别模型。

Method: 首先收集了3M小时英语音频和17M转录文本，形成OLMoASR-Pool。接着设计文本启发式过滤器，去除低质量或错误转录数据，最终得到1M小时高质量音频-文本对的新数据集OLMoASR-Mix。最后，使用OLMoASR-Mix训练了参数量从39M到1.5B的OLMoASR系列模型。

Result: 在所有模型规模下，OLMoASR模型在短文本和长文本语音识别基准测试中均实现了与OpenAI Whisper相当的平均性能。特别是，OLMoASR-medium.en模型的词错误率（WER）分别为12.8%和11.0%，与参数量相当的Whisper-medium.en（WER 12.4%和10.5%）基本持平。

Conclusion: 高质量、大规模训练数据对于开发鲁棒的零样本语音识别模型至关重要，OLMoASR模型验证了这一点，并在性能上达到了SOTA水平。相关数据集、模型和代码将开源，以促进未来研究。

Abstract: Improvements in training data scale and quality have led to significant
advances, yet its influence in speech recognition remains underexplored. In
this paper, we present a large-scale dataset, OLMoASR-Pool, and series of
models, OLMoASR, to study and develop robust zero-shot speech recognition
models. Beginning from OLMoASR-Pool, a collection of 3M hours of English audio
and 17M transcripts, we design text heuristic filters to remove low-quality or
mistranscribed data. Our curation pipeline produces a new dataset containing 1M
hours of high-quality audio-transcript pairs, which we call OLMoASR-Mix. We use
OLMoASR-Mix to train the OLMoASR-Mix suite of models, ranging from 39M
(tiny.en) to 1.5B (large.en) parameters. Across all model scales, OLMoASR
achieves comparable average performance to OpenAI's Whisper on short and
long-form speech recognition benchmarks. Notably, OLMoASR-medium.en attains a
12.8\% and 11.0\% word error rate (WER) that is on par with Whisper's largest
English-only model Whisper-medium.en's 12.4\% and 10.5\% WER for short and
long-form recognition respectively (at equivalent parameter count).
OLMoASR-Pool, OLMoASR models, and filtering, training and evaluation code will
be made publicly available to further research on robust speech processing.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [212] [Multi-Objective Optimization of ReRAM Crossbars for Robust DNN Inferencing under Stochastic Noise](https://arxiv.org/abs/2109.05437)
*Xiaoxuan Yang,Syrine Belakaria,Biresh Kumar Joardar,Huanrui Yang,Janardhan Rao Doppa,Partha Pratim Pande,Krishnendu Chakrabarty,Hai Li*

Main category: cs.ET

TL;DR: 本文提出ReSNA随机噪声感知训练方法以提升ReRAM加速器在噪声下的DNN推理精度，并开发了CF-MESMO多目标优化算法，通过高效的连续保真度评估，找到兼顾精度、面积、时间和能耗的鲁棒ReRAM设计。


<details>
  <summary>Details</summary>
Motivation: ReRAM是DNN硬件加速器的潜力技术，但其随机噪声会降低DNN推理精度。研究旨在设计一种鲁棒、高性能、面积和能效俱佳的ReRAM硬件加速器，以应对随机噪声并实现可靠的DNN推理。

Method: 1. **ReSNA：** 提出一种随机噪声感知的训练方法，以提高ReRAM交叉阵列在噪声下的DNN推理精度。2. **CF-MESMO：** 提出一种信息论算法，用于识别多目标（精度、面积、时间、能耗）权衡的帕累托解集。为解决ReSNA评估成本过高的问题，CF-MESMO采用连续保真度评估（通过调整训练周期），迭代选择单位计算成本信息增益最大的候选设计与保真度对。

Result: 1. ReSNA使ResNet20在CIFAR-10数据集上的推理精度平均提高了2.57%。2. CF-MESMO算法与NSGA-II相比，达到相同最佳解的计算成本降低了90.91%。3. 所提算法能有效发现高质量的帕累托前沿。

Conclusion: 所提出的ReSNA方法有效提升了ReRAM加速器在随机噪声下的DNN推理精度。CF-MESMO算法通过高效的多目标优化，成功找到了在精度、面积和能耗之间取得良好平衡的鲁棒ReRAM设计，显著降低了计算成本，验证了所提方法在ReRAM基DNN加速器设计中的有效性和高效性。

Abstract: Resistive random-access memory (ReRAM) is a promising technology for
designing hardware accelerators for deep neural network (DNN) inferencing.
However, stochastic noise in ReRAM crossbars can degrade the DNN inferencing
accuracy. We propose the design and optimization of a high-performance,
area-and energy-efficient ReRAM-based hardware accelerator to achieve robust
DNN inferencing in the presence of stochastic noise. We make two key technical
contributions. First, we propose a stochastic-noise-aware training method,
referred to as ReSNA, to improve the accuracy of DNN inferencing on ReRAM
crossbars with stochastic noise. Second, we propose an information-theoretic
algorithm, referred to as CF-MESMO, to identify the Pareto set of solutions to
trade-off multiple objectives, including inferencing accuracy, area overhead,
execution time, and energy consumption. The main challenge in this context is
that executing the ReSNA method to evaluate each candidate ReRAM design is
prohibitive. To address this challenge, we utilize the continuous-fidelity
evaluation of ReRAM designs associated with prohibitive high computation cost
by varying the number of training epochs to trade-off accuracy and cost.
CF-MESMO iteratively selects the candidate ReRAM design and fidelity pair that
maximizes the information gained per unit computation cost about the optimal
Pareto front. Our experiments on benchmark DNNs show that the proposed
algorithms efficiently uncover high-quality Pareto fronts. On average, ReSNA
achieves 2.57% inferencing accuracy improvement for ResNet20 on the CIFAR-10
dataset with respect to the baseline configuration. Moreover, CF-MESMO
algorithm achieves 90.91% reduction in computation cost compared to the popular
multi-objective optimization algorithm NSGA-II to reach the best solution from
NSGA-II.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [213] [Navigating the EU AI Act: Foreseeable Challenges in Qualifying Deep Learning-Based Automated Inspections of Class III Medical Devices](https://arxiv.org/abs/2508.20144)
*Julio Zanon Diaz,Tommy Brennan,Peter Corcoran*

Main category: cs.CY

TL;DR: 本文分析了将深度学习应用于三类医疗器械自动化视觉检测时，制造商在欧盟AI法案下可能面临的监管挑战，特别是与现有医疗器械法规（如MDR、QSR）的差异。


<details>
  <summary>Details</summary>
Motivation: 深度学习技术在三类医疗器械自动化视觉检测中具有显著潜力，可以提升质量保证并减少人为错误。然而，AI系统的引入带来了新的监管复杂性，特别是欧盟AI法案对高风险系统施加的义务，其范围和深度与现有医疗器械法规（如MDR、QSR）存在差异。

Method: 本文进行了一项高层技术评估，考察了制造商在现有医疗器械合规环境中验证基于深度学习的自动化检测时可能遇到的可预见挑战。具体审查了风险管理原则、数据集治理、模型验证、可解释性要求以及部署后监控义务方面的分歧。讨论还探讨了潜在的实施策略并强调了不确定领域。

Result: 评估结果揭示了制造商在将深度学习应用于医疗器械检测时面临的监管挑战，包括风险管理原则、数据集治理、模型验证、可解释性要求和部署后监控义务等方面的监管差异。同时，文中还探讨了潜在的实施策略和不确定性，如数据保留负担、全球合规影响以及有限缺陷数据下实现统计学显著性验证的实践难题。

Conclusion: 研究识别并讨论了制造商在医疗器械检测中部署深度学习技术时，特别是在欧盟AI法案与现有法规交叉背景下，面临的关键监管挑战和不确定性。这为制造商提供了关于合规路径和实施策略的重要考量。

Abstract: As deep learning (DL) technologies advance, their application in automated
visual inspection for Class III medical devices offers significant potential to
enhance quality assurance and reduce human error. However, the adoption of such
AI-based systems introduces new regulatory complexities--particularly under the
EU Artificial Intelligence (AI) Act, which imposes high-risk system obligations
that differ in scope and depth from established regulatory frameworks such as
the Medical Device Regulation (MDR) and the U.S. FDA Quality System Regulation
(QSR). This paper presents a high-level technical assessment of the
foresee-able challenges that manufacturers are likely to encounter when
qualifying DL-based automated inspections within the existing medical device
compliance landscape. It examines divergences in risk management principles,
dataset governance, model validation, explainability requirements, and
post-deployment monitoring obligations. The discussion also explores potential
implementation strategies and highlights areas of uncertainty, including data
retention burdens, global compliance implications, and the practical
difficulties of achieving statistical significance in validation with limited
defect data. Disclaimer: This publication is in-tended solely as an academic
and technical evaluation. It is not a substitute for le-gal advice or official
regulatory interpretation. The information presented here should not be relied
upon to demonstrate compliance with the EU AI Act or any other statutory
obligation. Manufacturers are encouraged to consult appropriate regulatory
authorities and legal experts to determine specific compliance pathways.

</details>


### [214] [RelAItionship Building: Analyzing Recruitment Strategies for Participatory AI](https://arxiv.org/abs/2508.20176)
*Eugene Kim,Vaibhav Balloli,Berelian Karimian,Elizabeth Bondi-Kelly,Benjamin Fish*

Main category: cs.CY

TL;DR: 本文探讨了参与式AI项目中招募利益相关者的挑战，分析了现有实践及其影响，并基于研究结果提出了关系导向的招募方法和文档实践建议。


<details>
  <summary>Details</summary>
Motivation: 参与式AI旨在确保AI系统符合社区需求和价值观，但识别、接触和吸引所有相关利益方（即招募方法）仍然是一个实际挑战。

Method: 研究方法包括：1. 分析一个包含37个项目的语料库，描述AI项目使用的招募方法、实践多样性及实现公平和赋权的策略。2. 访谈5位AI研究者，了解招募方法的结果。

Result: 研究发现招募结果受其工作结构条件、研究者自身目标和期望以及通过招募方法和后续协作建立的关系所影响。

Conclusion: 基于分析，本文为参与式AI研究者提供了设计和执行关系导向的招募方法，以及反思性招募文档实践的建议。

Abstract: Participatory AI, in which impacted community members and other stakeholders
are involved in the design and development of AI systems, holds promise as a
way to ensure AI is developed to meet their needs and reflect their values.
However, the process of identifying, reaching out, and engaging with all
relevant stakeholder groups, which we refer to as recruitment methodology, is
still a practical challenge in AI projects striving to adopt participatory
practices. In this paper, we investigate the challenges that researchers face
when designing and executing recruitment methodology for Participatory AI
projects, and the implications of current recruitment practice for
Participatory AI. First, we describe the recruitment methodologies used in AI
projects using a corpus of 37 projects to capture the diversity of practices in
the field and perform an initial analysis on the documentation of recruitment
practices, as well as specific strategies that researchers use to meet goals of
equity and empowerment. To complement this analysis, we interview five AI
researchers to learn about the outcomes of recruitment methodologies. We find
that these outcomes are shaped by structural conditions of their work,
researchers' own goals and expectations, and the relationships built from the
recruitment methodology and subsequent collaboration. Based on these analyses,
we provide recommendations for designing and executing relationship-forward
recruitment methods, as well as reflexive recruitment documentation practices
for Participatory AI researchers.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [215] [ELIXIR: Efficient and LIghtweight model for eXplaIning Recommendations](https://arxiv.org/abs/2508.20312)
*Ben Kabongo,Vincent Guigue,Pirmin Lemberger*

Main category: cs.IR

TL;DR: ELIXIR是一个多任务模型，结合评分预测和个性化评论生成，通过方面建模和个性化注意力机制，在较小的T5模型基础上，实现了更准确的推荐解释，并优于大型模型。


<details>
  <summary>Details</summary>
Motivation: 现有协同过滤在细粒度交互和可解释性方面存在不足。用户日益需要透明推荐，但现有文本解释方法（基于RNN或Transformer）分别面临未能利用预训练Transformer能力或适应性差、忽略方面建模的问题，而方面建模对个性化解释至关重要。

Method: 本文提出ELIXIR，一个多任务模型，结合评分预测与个性化评论生成。它共同学习用户和物品的全局及方面特定表示，优化整体评分、方面级别评分和评论生成，并通过个性化注意力强调方面重要性。该模型基于T5-small (60M) 实现。

Result: ELIXIR的方面基础架构能有效指导个性化文本生成，在匹配用户偏好方面优于利用更大模型的SOTA方法。在TripAdvisor和RateBeer数据集上的实验结果表明，ELIXIR显著优于强基线模型，尤其在评论生成方面表现突出。

Conclusion: ELIXIR通过其高效轻量级的方面建模架构，成功解决了现有推荐解释方法的痛点，特别是在生成个性化评论方面展现出卓越性能，为透明推荐系统提供了新的解决方案。

Abstract: Collaborative filtering drives many successful recommender systems but
struggles with fine-grained user-item interactions and explainability. As users
increasingly seek transparent recommendations, generating textual explanations
through language models has become a critical research area. Existing methods
employ either RNNs or Transformers. However, RNN-based approaches fail to
leverage the capabilities of pre-trained Transformer models, whereas
Transformer-based methods often suffer from suboptimal adaptation and neglect
aspect modeling, which is crucial for personalized explanations. We propose
ELIXIR (Efficient and LIghtweight model for eXplaIning Recommendations), a
multi-task model combining rating prediction with personalized review
generation. ELIXIR jointly learns global and aspect-specific representations of
users and items, optimizing overall rating, aspect-level ratings, and review
generation, with personalized attention to emphasize aspect importance. Based
on a T5-small (60M) model, we demonstrate the effectiveness of our aspect-based
architecture in guiding text generation in a personalized context, where
state-of-the-art approaches exploit much larger models but fail to match user
preferences as well. Experimental results on TripAdvisor and RateBeer
demonstrate that ELIXIR significantly outperforms strong baseline models,
especially in review generation.

</details>


### [216] [MPFormer: Adaptive Framework for Industrial Multi-Task Personalized Sequential Retriever](https://arxiv.org/abs/2508.20400)
*Yijia Sun,Shanshan Huang,Linxiao Che,Haitao Lu,Qiang Luo,Kun Gai,Guorui Zhou*

Main category: cs.IR

TL;DR: 本文提出MPFormer，一个动态多任务Transformer框架，通过目标条件Transformer、个性化目标权重和用户个性化信息融合，解决了工业推荐系统中多阶段优化错位（检索与排序语义鸿沟）的核心挑战，已在快手部署并显著提升用户参与度和系统效率。


<details>
  <summary>Details</summary>
Motivation: 现代工业推荐系统面临多阶段优化错位（排序阶段多目标优化与检索阶段单目标建模之间存在显著语义鸿沟）的核心挑战。现有工业解决方案（并行多路径单目标检索）导致资源线性增长且难以处理松散耦合目标，因此需要一个更系统、高效的多目标检索解决方案。

Method: 本文提出了MPFormer，一个动态多任务Transformer框架，通过以下三个创新机制系统地解决问题：1. 目标条件Transformer：通过可学习的注意力调制，联合编码用户行为序列和多任务语义。2. 个性化目标权重：引入个性化目标权重以实现检索结果的动态调整。3. 用户个性化信息融合：将用户个性化信息融入到token表示和Transformer结构中，增强模型表示能力。

Result: 该框架已成功集成到快手短视频推荐系统，稳定服务超过4亿日活跃用户。它显著提高了用户日活跃度（engagement）和系统运营效率。实际部署验证显示，与传统解决方案相比，MPFormer在保持服务响应速度的同时，有效优化了多目标检索的迭代范式。

Conclusion: MPFormer为工业推荐系统提供了一个可扩展的多目标解决方案，有效解决了多阶段优化错位问题，并在大规模工业应用中（快手）验证了其在提升用户参与度和系统效率方面的显著效果。

Abstract: Modern industrial recommendation systems encounter a core challenge of
multi-stage optimization misalignment: a significant semantic gap exists
between the multi-objective optimization paradigm widely used in the ranking
phase and the single-objective modeling in the retrieve phase. Although the
mainstream industry solution achieves multi-objective coverage through parallel
multi-path single-objective retrieval, this approach leads to linear growth of
training and serving resources with the number of objectives and has inherent
limitations in handling loosely coupled objectives. This paper proposes the
MPFormer, a dynamic multi-task Transformer framework, which systematically
addresses the aforementioned issues through three innovative mechanisms. First,
an objective-conditioned transformer that jointly encodes user behavior
sequences and multi-task semantics through learnable attention modulation;
second, personalized target weights are introduced to achieve dynamic
adjustment of retrieval results; finally, user personalization information is
incorporated into token representations and the Transformer structure to
further enhance the model's representation ability. This framework has been
successfully integrated into Kuaishou short video recommendation system, stably
serving over 400 million daily active users. It significantly improves user
daily engagement and system operational efficiency. Practical deployment
verification shows that, compared with traditional solutions, it effectively
optimizes the iterative paradigm of multi-objective retrieval while maintaining
service response speed, providing a scalable multi-objective solution for
industrial recommendation systems.

</details>


### [217] [On the Theoretical Limitations of Embedding-Based Retrieval](https://arxiv.org/abs/2508.21038)
*Orion Weller,Michael Boratko,Iftekhar Naim,Jinhyuk Lee*

Main category: cs.IR

TL;DR: 现有向量嵌入的理论局限性在现实、简单的查询中依然显著，即使是SOTA模型在为此设计的基准数据集（LIMIT）上也表现不佳，揭示了单一向量范式的根本限制。


<details>
  <summary>Details</summary>
Motivation: 尽管普遍认为向量嵌入的理论局限性仅由不切实际的查询引起，或可通过优化训练和模型解决，但本文旨在挑战这一假设，证明这些局限性在现实且简单的查询中同样存在。

Method: 1. 结合学习理论，指出通过某个查询返回的top-k文档子集的数量受限于嵌入维度。2. 经验性地证明，即使k=2并通过直接优化也存在此限制。3. 创建名为LIMIT的现实数据集，据此理论结果对SOTA模型进行压力测试。

Result: 1. 返回的top-k文档子集数量确实受嵌入维度限制（经理论推导和经验验证）。2. 即使是最先进的模型，在简单的LIMIT数据集上也表现失败。

Conclusion: 现有单一向量范式下的嵌入模型存在根本性限制，亟需未来的研究开发新的方法来解决这一局限性。

Abstract: Vector embeddings have been tasked with an ever-increasing set of retrieval
tasks over the years, with a nascent rise in using them for reasoning,
instruction-following, coding, and more. These new benchmarks push embeddings
to work for any query and any notion of relevance that could be given. While
prior works have pointed out theoretical limitations of vector embeddings,
there is a common assumption that these difficulties are exclusively due to
unrealistic queries, and those that are not can be overcome with better
training data and larger models. In this work, we demonstrate that we may
encounter these theoretical limitations in realistic settings with extremely
simple queries. We connect known results in learning theory, showing that the
number of top-k subsets of documents capable of being returned as the result of
some query is limited by the dimension of the embedding. We empirically show
that this holds true even if we restrict to k=2, and directly optimize on the
test set with free parameterized embeddings. We then create a realistic dataset
called LIMIT that stress tests models based on these theoretical results, and
observe that even state-of-the-art models fail on this dataset despite the
simple nature of the task. Our work shows the limits of embedding models under
the existing single vector paradigm and calls for future research to develop
methods that can resolve this fundamental limitation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [218] [Enhancing Resilience for IoE: A Perspective of Networking-Level Safeguard](https://arxiv.org/abs/2508.20504)
*Guan-Yan Yang,Jui-Ning Chen,Farn Wang,Kuo-Hui Yeh*

Main category: cs.CR

TL;DR: 为应对能源物联网（IoE）面临的对抗性网络威胁，本文提出了一种基于图结构学习（GSL）的安全防护框架，通过优化图拓扑和节点表示来增强鲁棒性，并在实验中展现了其优越性。


<details>
  <summary>Details</summary>
Motivation: 能源物联网（IoE）的互联性使其关键基础设施面临复杂的网络威胁，特别是能绕过传统防护的对抗性攻击。由于IoE威胁可能导致严重的公共安全后果，因此迫切需要更具弹性的安全解决方案。

Method: 本文从网络层面的防护角度，提出了一种基于图结构学习（GSL）的安全防护框架。该框架共同优化图拓扑结构和节点表示，以内在抵抗对抗性网络模型操纵。

Result: 通过概念概述、架构讨论和对安全数据集的案例研究，本文证明了GSL比现有代表性方法具有更优越的鲁棒性，为从业者提供了一条保护IoE网络免受不断演变攻击的可行路径。

Conclusion: 本工作强调了GSL在增强未来IoE网络的弹性和可靠性方面的巨大潜力，为管理关键基础设施的从业者提供了安全保障。此外，文章还指出了该领域面临的关键挑战和未来研究方向。

Abstract: The Internet of Energy (IoE) integrates IoT-driven digital communication with
power grids to enable efficient and sustainable energy systems. Still, its
interconnectivity exposes critical infrastructure to sophisticated cyber
threats, including adversarial attacks designed to bypass traditional
safeguards. Unlike general IoT risks, IoE threats have heightened public safety
consequences, demanding resilient solutions. From the networking-level
safeguard perspective, we propose a Graph Structure Learning (GSL)-based
safeguards framework that jointly optimizes graph topology and node
representations to resist adversarial network model manipulation inherently.
Through a conceptual overview, architectural discussion, and case study on a
security dataset, we demonstrate GSL's superior robustness over representative
methods, offering practitioners a viable path to secure IoE networks against
evolving attacks. This work highlights the potential of GSL to enhance the
resilience and reliability of future IoE networks for practitioners managing
critical infrastructure. Lastly, we identify key open challenges and propose
future research directions in this novel research area.

</details>


### [219] [AI Propaganda factories with language models](https://arxiv.org/abs/2508.20186)
*Lukasz Olejnik*

Main category: cs.CR

TL;DR: 小型语言模型能生成连贯、角色驱动的政治信息，使完全自动化影响力行动易于实施。研究发现角色设计比模型本身更重要，且交互压力会增强意识形态依从性并增加极端内容。防御应转向对话式检测，而非限制模型访问，因为其一致性亦是检测特征。


<details>
  <summary>Details</summary>
Motivation: 评估由AI驱动的影响力行动，特别是小型语言模型在端到端执行此类操作上的能力，并理解其行为模式。核心动机在于识别和应对AI技术民主化后对信息环境和政治交流的潜在威胁。

Method: 使用小型语言模型生成连贯、角色驱动的政治信息。采用自动化方法对生成内容进行评估，无需人工评级。通过实验观察在不同条件下（如需反驳论点）模型的行为表现及其内容产出变化。

Result: ['角色优于模型：角色设计对AI生成内容行为的影响远超模型本身的身份。', '互动作为压力源：当AI回复必须反驳论点时，其意识形态依从性会增强，极端内容的出现频率也会增加。']

Conclusion: 完全自动化的影响力内容生产已在大型和小型行动者能力范围之内。因此，防御策略应从限制模型访问转向以对话为中心的活动检测和协调基础设施的破坏。悖论在于，这些操作固有的高一致性反过来也提供了一个可用于检测的特征。

Abstract: AI-powered influence operations can now be executed end-to-end on commodity
hardware. We show that small language models produce coherent, persona-driven
political messaging and can be evaluated automatically without human raters.
Two behavioural findings emerge. First, persona-over-model: persona design
explains behaviour more than model identity. Second, engagement as a stressor:
when replies must counter-arguments, ideological adherence strengthens and the
prevalence of extreme content increases. We demonstrate that fully automated
influence-content production is within reach of both large and small actors.
Consequently, defence should shift from restricting model access towards
conversation-centric detection and disruption of campaigns and coordination
infrastructure. Paradoxically, the very consistency that enables these
operations also provides a detection signature.

</details>


### [220] [Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID](https://arxiv.org/abs/2508.20228)
*Xia Han,Qi Li,Jianbing Ni,Mohammad Zulkernine*

Main category: cs.CR

TL;DR: 现有LLM水印方法（如SynthID-Text）易受语义保持攻击。本文提出SynGuard，结合语义信息检索和概率水印机制，在词汇和语义层面嵌入水印，显著提高了水印恢复的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Google DeepMind的SynthID-Text等LLM水印方法在追溯AI生成文本来源方面有前景，但它们易受语义保持攻击（如意译、复制粘贴修改、回译），这会严重降低水印检测能力。

Method: 提出SynGuard混合框架，它结合了语义信息检索（SIR）的语义对齐能力与SynthID-Text的概率水印机制。该方法在词汇和语义层面联合嵌入水印，以实现鲁棒的来源追踪并保留原始含义。

Result: 在多种攻击场景下的实验结果表明，与SynthID-Text相比，SynGuard的水印恢复F1分数平均提高了11.1%。

Conclusion: 这些发现证明了语义感知水印在抵抗真实世界篡改方面的有效性。

Abstract: Recent advances in LLM watermarking methods such as SynthID-Text by Google
DeepMind offer promising solutions for tracing the provenance of AI-generated
text. However, our robustness assessment reveals that SynthID-Text is
vulnerable to meaning-preserving attacks, such as paraphrasing, copy-paste
modifications, and back-translation, which can significantly degrade watermark
detectability. To address these limitations, we propose SynGuard, a hybrid
framework that combines the semantic alignment strength of Semantic Information
Retrieval (SIR) with the probabilistic watermarking mechanism of SynthID-Text.
Our approach jointly embeds watermarks at both lexical and semantic levels,
enabling robust provenance tracking while preserving the original meaning.
Experimental results across multiple attack scenarios show that SynGuard
improves watermark recovery by an average of 11.1\% in F1 score compared to
SynthID-Text. These findings demonstrate the effectiveness of semantic-aware
watermarking in resisting real-world tampering. All code, datasets, and
evaluation scripts are publicly available at:
https://github.com/githshine/SynGuard.

</details>


### [221] [Network-Level Prompt and Trait Leakage in Local Research Agents](https://arxiv.org/abs/2508.20282)
*Hyejun Jeong,Mohammadreze Teymoorianfard,Abhinav Kumar,Amir Houmansadr,Eugene Badasarian*

Main category: cs.CR

TL;DR: Web和研究智能体（WRAs）容易受到被动网络对手的推断攻击，这些攻击仅利用网络元数据就能泄露用户提示和潜在用户特征。


<details>
  <summary>Details</summary>
Motivation: 组织和个人部署本地WRAs以保护隐私、法律或财务目的，但它们独特的网络访问模式（访问大量域名且有可区分的时间关联）使其易受互联网服务提供商（ISPs）等被动网络对手的指纹识别攻击。

Method: 研究人员演示了一种新颖的针对WRAs的提示和用户特征泄露攻击，该攻击仅利用网络元数据（访问的IP地址及其时间）。他们构建了一个基于用户搜索查询和合成角色查询的新WRA轨迹数据集，并定义了一个行为指标（OBELS）来评估原始和推断提示之间的相似性。该方法还扩展到多会话设置以恢复潜在特征，并测试了在部分可观察和噪声条件下的有效性。最后，讨论了通过限制域名多样性或混淆轨迹的缓解策略。

Result: 该攻击能够恢复超过73%的用户提示功能和领域知识。在多会话设置中，能够高精度地恢复32个潜在特征中的多达19个。攻击在部分可观察和噪声条件下仍然有效。提出的缓解策略在效用影响可忽略的情况下，平均将攻击有效性降低了29%。

Conclusion: WRAs因其独特的网络行为而容易受到被动网络对手的推断攻击，导致用户提示和特征泄露。虽然这种攻击非常有效，但通过限制域名多样性或混淆轨迹的缓解策略可以在不显著影响效用的前提下有效降低攻击的成功率。

Abstract: We show that Web and Research Agents (WRAs) -- language model-based systems
that investigate complex topics on the Internet -- are vulnerable to inference
attacks by passive network adversaries such as ISPs. These agents could be
deployed \emph{locally} by organizations and individuals for privacy, legal, or
financial purposes. Unlike sporadic web browsing by humans, WRAs visit
$70{-}140$ domains with distinguishable timing correlations, enabling unique
fingerprinting attacks.
  Specifically, we demonstrate a novel prompt and user trait leakage attack
against WRAs that only leverages their network-level metadata (i.e., visited IP
addresses and their timings). We start by building a new dataset of WRA traces
based on user search queries and queries generated by synthetic personas. We
define a behavioral metric (called OBELS) to comprehensively assess similarity
between original and inferred prompts, showing that our attack recovers over
73\% of the functional and domain knowledge of user prompts. Extending to a
multi-session setting, we recover up to 19 of 32 latent traits with high
accuracy. Our attack remains effective under partial observability and noisy
conditions. Finally, we discuss mitigation strategies that constrain domain
diversity or obfuscate traces, showing negligible utility impact while reducing
attack effectiveness by an average of 29\%.

</details>


### [222] [Surveying the Operational Cybersecurity and Supply Chain Threat Landscape when Developing and Deploying AI Systems](https://arxiv.org/abs/2508.20307)
*Michael R Smith,Joe Ingram*

Main category: cs.CR

TL;DR: 本文旨在提高对AI系统引入的新型网络威胁的认识，探讨AI生命周期中的运营和供应链风险，并强调需要定制的安全框架以保护AI系统。


<details>
  <summary>Details</summary>
Motivation: AI的兴起带来了传统网络安全评估忽视的独特攻击面和目标，网络攻击者的目标也已转向操纵AI输出以实现系统效应。因此，有必要提高对AI系统引入的新型网络威胁的认识。

Method: 探索AI生命周期中的运营网络安全和供应链风险，强调定制安全框架的需求，并突出以往的漏洞利用案例，提供该领域的实践见解。

Result: 揭示了AI系统带来的新型网络威胁，包括攻击者通过操纵AI输出实现性能下降、误报泛滥或模型准确性降低等目标。强调了在AI驱动环境中，运营网络安全和供应链风险贯穿整个AI生命周期。

Conclusion: 理解这些新型网络威胁和风险，对组织保护AI系统并确保其可靠性和弹性至关重要，需要量身定制的安全框架来应对不断演变的威胁。

Abstract: The rise of AI has transformed the software and hardware landscape, enabling
powerful capabilities through specialized infrastructures, large-scale data
storage, and advanced hardware. However, these innovations introduce unique
attack surfaces and objectives which traditional cybersecurity assessments
often overlook. Cyber attackers are shifting their objectives from conventional
goals like privilege escalation and network pivoting to manipulating AI outputs
to achieve desired system effects, such as slowing system performance, flooding
outputs with false positives, or degrading model accuracy. This paper serves to
raise awareness of the novel cyber threats that are introduced when
incorporating AI into a software system. We explore the operational
cybersecurity and supply chain risks across the AI lifecycle, emphasizing the
need for tailored security frameworks to address evolving threats in the
AI-driven landscape. We highlight previous exploitations and provide insights
from working in this area. By understanding these risks, organizations can
better protect AI systems and ensure their reliability and resilience.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [223] [Can LLMs Identify Tax Abuse?](https://arxiv.org/abs/2508.20097)
*Andrew Blair-Stanek,Nils Holzenberger,Benjamin Van Durme*

Main category: q-fin.CP

TL;DR: 研究大型语言模型（LLMs）发现和分析美国税收最小化策略的能力。


<details>
  <summary>Details</summary>
Motivation: 美国税收最小化领域对人类专家极具挑战，研究进展有助于减少富裕纳税人避税造成的税收损失，并打击税务滥用。

Method: 评估最先进的LLMs在以下方面的能力：1) 解释和验证税收策略；2) 补充部分指定策略中的空白；3) 从零开始生成完整的端到端策略。

Result: 基于LLM的推理识别出了一种全新的税收策略。

Conclusion: LLMs有望彻底改变税务机构打击税务滥用的方式，具有巨大的应用潜力。

Abstract: We investigate whether large language models can discover and analyze U.S.
tax-minimization strategies. This real-world domain challenges even seasoned
human experts, and progress can reduce tax revenue lost from well-advised,
wealthy taxpayers. We evaluate the most advanced LLMs on their ability to (1)
interpret and verify tax strategies, (2) fill in gaps in partially specified
strategies, and (3) generate complete, end-to-end strategies from scratch. This
domain should be of particular interest to the LLM reasoning community: unlike
synthetic challenge problems or scientific reasoning tasks, U.S. tax law
involves navigating hundreds of thousands of pages of statutes, case law, and
administrative guidance, all updated regularly. Notably, LLM-based reasoning
identified an entirely novel tax strategy, highlighting these models' potential
to revolutionize tax agencies' fight against tax abuse.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [224] [Towards Better Correctness and Efficiency in Code Generation](https://arxiv.org/abs/2508.20124)
*Yunlong Feng,Yang Xu,Xiao Xu,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 提出一种基于性能奖励的强化学习框架和两阶段微调方法，显著提升大语言模型生成代码的运行效率和正确性。


<details>
  <summary>Details</summary>
Motivation: 当前代码大语言模型生成的代码运行时效率低下，限制了其在性能敏感场景的实际应用。

Method: 提出一个以新型性能奖励为指导的效率导向强化学习框架。该框架克服了静态数据限制、缓解了系统误差，并确保在不牺牲准确性的前提下提高效率，最终形成一种两阶段微调方法。

Result: 在7B模型上，代码正确性提高了10.18%，运行时效率提高了7.75%，性能可与更大的模型媲美。

Conclusion: 所提出的两阶段微调方法有效平衡并提升了代码生成任务中的正确性和运行效率。

Abstract: While code large language models have demonstrated remarkable progress in
code generation, the generated code often exhibits poor runtime efficiency,
limiting its practical application in performance-sensitive scenarios. To
address this limitation, we propose an efficiency-oriented reinforcement
learning framework guided by a novel performance reward. Based on this
framework, we take a deeper dive into the code efficiency problem, identifying
then proposing methods to overcome key bottlenecks: (1) Dynamic exploration
overcomes the static data constraints of offline fine-tuning, enabling the
discovery of more efficient code implementations. (2) The error-insensitive
reinforcement learning method and high-contrast efficiency signals are crucial
for mitigating systematic errors and achieving effective optimization. (3)
Online exploration is most effective when starting from a high-correctness
baseline, as this allows for efficiency improvements without sacrificing
accuracy. With these discoveries, we finally propose a two-stage tuning method,
which achieves high and balanced performance across correctness and efficiency.
The results of experiments show the effectiveness of the method, which improves
code correctness by 10.18\% and runtime efficiency by 7.75\% on a 7B model,
achieving performance comparable to much larger model.

</details>


### [225] [Boosting Skeleton-Driven SMT Solver Fuzzing by Leveraging LLM to Produce Formula Generators](https://arxiv.org/abs/2508.20340)
*Maolin Sun,Yibiao Yang,Yuming Zhou*

Main category: cs.SE

TL;DR: 本研究提出Chimera，一个新型的LLM辅助模糊测试框架，通过从文档中提取SMT理论的上下文无关文法并合成可重用术语生成器，解决了LLM生成SMT测试公式时语法无效和计算开销大的问题，并成功发现了SMT求解器中的大量bug。


<details>
  <summary>Details</summary>
Motivation: SMT求解器的正确性至关重要，但现有测试技术难以跟上其快速演进的特性。尽管基于LLM的方法有潜力，但面临两大挑战：生成公式近一半语法无效，以及LLM迭代交互引入的巨大计算开销。

Method: 本研究提出了Chimera框架，将LLM应用从直接生成公式转向合成可重用术语生成器。具体而言，Chimera利用LLM：1) 从文档中自动提取SMT理论的上下文无关文法（包括求解器特定扩展）；2) 合成符合这些文法的可组合布尔术语生成器。在模糊测试阶段，Chimera使用这些LLM合成的生成器迭代生成术语，并填充从现有公式派生出的结构骨架，以此确保语法有效性并提升语义多样性。该方法仅需一次性LLM交互。

Result: Chimera在Z3和cvc5两大SMT求解器上进行了评估，共识别出43个已确认的bug，其中40个已由开发者修复。

Conclusion: Chimera通过其新颖的LLM辅助合成可重用术语生成器的方法，有效解决了SMT求解器测试中公式语法有效性和运行时成本的关键挑战，显著提高了bug发现能力，证明了其在增强SMT求解器健壮性方面的潜力。

Abstract: Satisfiability Modulo Theory (SMT) solvers are foundational to modern systems
and programming languages research, providing the foundation for tasks like
symbolic execution and automated verification. Because these solvers sit on the
critical path, their correctness is essential, and high-quality test formulas
are key to uncovering bugs. However, while prior testing techniques performed
well on earlier solver versions, they struggle to keep pace with rapidly
evolving features. Recent approaches based on Large Language Models (LLMs) show
promise in exploring advanced solver capabilities, but two obstacles remain:
nearly half of the generated formulas are syntactically invalid, and iterative
interactions with the LLMs introduce substantial computational overhead. In
this study, we present Chimera, a novel LLM-assisted fuzzing framework that
addresses both issues by shifting from direct formula generation to the
synthesis of reusable term (i.e., logical expression) generators. Particularly,
Chimera uses LLMs to (1) automatically extract context-free grammars (CFGs) for
SMT theories, including solver-specific extensions, from documentation, and (2)
synthesize composable Boolean term generators that adhere to these grammars.
During fuzzing, Chimera populates structural skeletons derived from existing
formulas with the terms iteratively produced by the LLM-synthesized generators.
This design ensures syntactic validity while promoting semantic diversity.
Notably, Chimera requires only one-time LLM interaction investment,
dramatically reducing runtime cost. We evaluated Chimera on two leading SMT
solvers: Z3 and cvc5. Our experiments show that Chimera has identified 43
confirmed bugs, 40 of which have already been fixed by developers.

</details>


### [226] [Adaptive Root Cause Localization for Microservice Systems with Multi-Agent Recursion-of-Thought](https://arxiv.org/abs/2508.20370)
*Lingzhe Zhang,Tong Jia,Kangjin Wang,Weijie Hong,Chiming Duan,Minghua He,Ying Li*

Main category: cs.SE

TL;DR: 本文提出RCLAgent，一种受SRE人类分析启发、基于多智能体思维递归框架的微服务故障根因定位方法，通过引导LLM推理并集成多源数据，实现了使用单次请求优于现有先进方法的卓越性能。


<details>
  <summary>Details</summary>
Motivation: 当代微服务系统日益复杂且故障频繁，准确的根因定位对系统可靠性至关重要。然而，现有方法要么过度依赖难以适应不断变化的运行环境的预定义模式，要么缺乏可解释性，令SRE工程师感到困惑。因此，需要开发一种既适应性强又具备可解释性的根因定位方法。

Method: 首先，通过对不同组织SRE的综合研究，揭示了人类根因分析的三个关键特征：递归性、多维扩展和跨模态推理。受这些发现的启发，提出了RCLAgent，这是一种基于多智能体思维递归框架的自适应微服务根因定位方法。RCLAgent采用新颖的思维递归策略来指导大型语言模型（LLM）的推理过程，有效地整合来自多个智能体的数据和工具辅助分析。

Result: 在各种公共数据集上进行的实验评估表明，RCLAgent在仅使用单次请求的情况下，便能实现卓越的根因定位性能，优于依赖聚合多次请求的现有先进方法。

Conclusion: RCLAgent的有效性凸显了其在提高复杂微服务环境中根因定位效率和精确性方面的潜力。

Abstract: As contemporary microservice systems become increasingly popular and
complex-often comprising hundreds or even thousands of fine-grained,
interdependent subsystems-they are facing more frequent failures. Ensuring
system reliability thus demands accurate root cause localization. While traces
and metrics have proven to be effective data sources for this task, existing
methods either heavily rely on pre-defined schemas, which struggle to adapt to
evolving operational contexts, or lack interpretability in their reasoning
process, thereby leaving Site Reliability Engineers (SREs) confused. In this
paper, we conduct a comprehensive study on how SREs localize the root cause of
failures, drawing insights from multiple professional SREs across different
organizations. Our investigation reveals that human root cause analysis
exhibits three key characteristics: recursiveness, multi-dimensional expansion,
and cross-modal reasoning. Motivated by these findings, we introduce RCLAgent,
an adaptive root cause localization method for microservice systems that
leverages a multi-agent recursion-of-thought framework. RCLAgent employs a
novel recursion-of-thought strategy to guide the LLM's reasoning process,
effectively integrating data from multiple agents and tool-assisted analysis to
accurately pinpoint the root cause. Experimental evaluations on various public
datasets demonstrate that RCLAgent achieves superior performance by localizing
the root cause using only a single request-outperforming state-of-the-art
methods that depend on aggregating multiple requests. These results underscore
the effectiveness of RCLAgent in enhancing the efficiency and precision of root
cause localization in complex microservice environments.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [227] [Validating Generative Agent-Based Models for Logistics and Supply Chain Management Research](https://arxiv.org/abs/2508.20234)
*Vincent E. Castillo*

Main category: cs.MA

TL;DR: 本研究评估了大型语言模型（LLMs）在物流与供应链管理（LSCM）中模拟人类行为的有效性。结果显示，LLMs能有效模拟人类行为，但存在表面等效性与决策过程不一致的悖论。研究提出双层验证框架，为GABMs开发和LLM选择提供指导。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型驱动的生成式主体模型（GABMs）在LSCM领域模拟复杂人类行为方面潜力巨大，但LLMs作为人类行为代理在LSCM模拟中的有效性仍未知。

Method: 通过一项对照实验评估LLMs对人类行为的等效性，实验情境为食品配送场景中的客户-工人双向互动。研究测试了六个最先进的LLMs与957名人类参与者（477对），采用有调节的中介设计。使用双单侧检验（TOST）评估表面等效性，并使用结构方程模型（SEM）分析决策过程。

Result: 1. GABMs能有效模拟LSCM中的人类行为。2. 存在“等效性与过程悖论”：部分LLMs在表面层面表现出与人类的等效性，但其决策过程与人类参与者不同。3. 研究揭示GABMs需进行双层验证：人类行为等效性测试和决策过程验证。

Conclusion: GABMs在经过适当验证后，是LSCM领域潜在可行的方法工具。本研究提出的双重验证框架为LSCM研究人员提供了严谨开发GABMs的指南，并为实践者在操作任务中选择LLM提供了基于证据的评估。

Abstract: Generative Agent-Based Models (GABMs) powered by large language models (LLMs)
offer promising potential for empirical logistics and supply chain management
(LSCM) research by enabling realistic simulation of complex human behaviors.
Unlike traditional agent-based models, GABMs generate human-like responses
through natural language reasoning, which creates potential for new
perspectives on emergent LSCM phenomena. However, the validity of LLMs as
proxies for human behavior in LSCM simulations is unknown. This study evaluates
LLM equivalence of human behavior through a controlled experiment examining
dyadic customer-worker engagements in food delivery scenarios. I test six
state-of-the-art LLMs against 957 human participants (477 dyads) using a
moderated mediation design. This study reveals a need to validate GABMs on two
levels: (1) human equivalence testing, and (2) decision process validation.
Results reveal GABMs can effectively simulate human behaviors in LSCM; however,
an equivalence-versus-process paradox emerges. While a series of Two One-Sided
Tests (TOST) for equivalence reveals some LLMs demonstrate surface-level
equivalence to humans, structural equation modeling (SEM) reveals artificial
decision processes not present in human participants for some LLMs. These
findings show GABMs as a potentially viable methodological instrument in LSCM
with proper validation checks. The dual-validation framework also provides LSCM
researchers with a guide to rigorous GABM development. For practitioners, this
study offers evidence-based assessment for LLM selection for operational tasks.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [228] [Artificial Intelligence for CRISPR Guide RNA Design: Explainable Models and Off-Target Safety](https://arxiv.org/abs/2508.20130)
*Alireza Abbaszadeh,Armita Shahlai*

Main category: q-bio.QM

TL;DR: 本综述探讨了人工智能（尤其是深度学习和可解释AI）如何革新CRISPR gRNA设计，以提高效率、特异性和安全性，并揭示其作用机制，从而推动更可靠的基因编辑应用。


<details>
  <summary>Details</summary>
Motivation: CRISPR基因组编辑技术虽具革命性，但优化gRNA设计以提升效率和安全性仍是关键挑战。

Method: 通过综述最先进的机器学习模型，文章阐述了AI如何改进CRISPR系统的gRNA设计。同时，它也强调了理解模型预测的策略（如可解释AI），并讨论了脱靶预测和安全评估的最新进展。

Result: AI（特别是深度学习）能显著改善gRNA靶向活性的预测并识别脱靶风险。可解释AI技术开始揭示模型内部机制，提供驱动Cas酶性能的序列特征和基因组背景的洞察。

Conclusion: AI与基因组编辑的跨学科融合，将实现更高效、特异且具临床应用潜力的CRISPR应用。

Abstract: CRISPR-based genome editing has revolutionized biotechnology, yet optimizing
guide RNA (gRNA) design for efficiency and safety remains a critical challenge.
Recent advances (2020--2025, updated to reflect current year if needed)
demonstrate that artificial intelligence (AI), especially deep learning, can
markedly improve the prediction of gRNA on-target activity and identify
off-target risks. In parallel, emerging explainable AI (XAI) techniques are
beginning to illuminate the black-box nature of these models, offering insights
into sequence features and genomic contexts that drive Cas enzyme performance.
Here we review how state-of-the-art machine learning models are enhancing gRNA
design for CRISPR systems, highlight strategies for interpreting model
predictions, and discuss new developments in off-target prediction and safety
assessment. We emphasize breakthroughs from top-tier journals that underscore
an interdisciplinary convergence of AI and genome editing to enable more
efficient, specific, and clinically viable CRISPR applications.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [229] [Mitigating Distribution Shift in Stock Price Data via Return-Volatility Normalization for Accurate Prediction](https://arxiv.org/abs/2508.20108)
*Hyunwoo Lee,Jihyeong Jeon,Jaemin Hong,U Kang*

Main category: q-fin.ST

TL;DR: 本文提出了ReVol方法，通过对股票价格数据进行收益-波动率归一化和注意力机制的特征估计与重整合，并结合几何布朗运动与神经网络，有效解决了分布偏移问题，显著提升了股票价格预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 股票价格预测因其揭示市场模式和辅助决策的潜力而备受关注。然而，现有方法难以有效处理训练和测试数据之间的分布偏移、差异和形状错位，通常仅关注缩放或表示适应，未能完全解决核心问题。

Method: 我们提出了ReVol（Return-Volatility Normalization for Mitigating Distribution Shift in Stock Price Data），其核心策略包括：1) 归一化价格特征（如收益、波动率、价格尺度）以消除样本特异性；2) 使用注意力模块精确估计这些特征，减少市场异常值影响；3) 将样本特征重新整合到预测过程中，恢复归一化期间丢失的特性。此外，ReVol结合几何布朗运动进行长期趋势建模和神经网络进行短期模式识别。

Result: 在真实世界数据集上的大量实验表明，ReVol在大多数情况下显著增强了最先进骨干模型的性能，在不同设置下，IC平均提高了0.03以上，SR平均提高了0.7以上。

Conclusion: ReVol是一个鲁棒的方法，通过其创新的归一化和重整合策略，并结合混合建模方法，有效解决了股票价格数据中的分布偏移问题，从而提高了预测准确性，超越了现有先进模型。

Abstract: How can we address distribution shifts in stock price data to improve stock
price prediction accuracy? Stock price prediction has attracted attention from
both academia and industry, driven by its potential to uncover complex market
patterns and enhance decisionmaking. However, existing methods often fail to
handle distribution shifts effectively, focusing on scaling or representation
adaptation without fully addressing distributional discrepancies and shape
misalignments between training and test data. We propose ReVol
(Return-Volatility Normalization for Mitigating Distribution Shift in Stock
Price Data), a robust method for stock price prediction that explicitly
addresses the distribution shift problem. ReVol leverages three key strategies
to mitigate these shifts: (1) normalizing price features to remove
sample-specific characteristics, including return, volatility, and price scale,
(2) employing an attention-based module to estimate these characteristics
accurately, thereby reducing the influence of market anomalies, and (3)
reintegrating the sample characteristics into the predictive process, restoring
the traits lost during normalization. Additionally, ReVol combines geometric
Brownian motion for long-term trend modeling with neural networks for
short-term pattern recognition, unifying their complementary strengths.
Extensive experiments on real-world datasets demonstrate that ReVol enhances
the performance of the state-of-the-art backbone models in most cases,
achieving an average improvement of more than 0.03 in IC and over 0.7 in SR
across various settings.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [230] [Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder](https://arxiv.org/abs/2508.20474)
*Muhammad Shakeel,Yui Sudo,Yifan Peng,Chyi-Jiunn Lin,Shinji Watanabe*

Main category: eess.AS

TL;DR: 论文提出了一个统一多说话人编码器（UME），通过共享基础编码器联合学习说话人识别、语音分离和多说话人ASR任务的表示。


<details>
  <summary>Details</summary>
Motivation: 为了捕获多任务间的内在相互依赖性，提升重叠语音数据的整体性能，并有效利用不同语义层的信息实现任务间的自底向上对齐。

Method: 提出了统一多说话人编码器（UME），其核心是一个共享的语音基础编码器，用于联合训练说话人识别（SD）、语音分离（SS）和多说话人自动语音识别（ASR）任务。该方法利用UME多层隐藏表示作为残差加权和编码（RWSE）来整合多语义层信息。

Result: UME在LibriMix评估集上显著优于单任务基线。特别是在说话人识别（SD）任务上，UME超越了现有研究，在Libri2Mix和Libri3Mix评估集上分别达到了1.37%和2.29%的错误率。

Conclusion: UME通过联合学习多任务表示，成功捕获了任务间的相互依赖性，显著提升了在重叠语音数据上的处理性能，尤其在说话人识别任务上取得了最先进的结果。

Abstract: This paper presents a unified multi-speaker encoder (UME), a novel
architecture that jointly learns representations for speaker diarization (SD),
speech separation (SS), and multi-speaker automatic speech recognition (ASR)
tasks using a shared speech foundational encoder. We leverage the hidden
representations from multiple layers of UME as a residual weighted-sum encoding
(RWSE) to effectively use information from different semantic levels,
contributing to bottom-up alignment between tasks. This joint training approach
captures the inherent interdependencies among the tasks, enhancing overall
performance on overlapping speech data. Our evaluations demonstrate that UME
substantially improves over the single-task baselines dedicated to SD, SS, and
multi-speaker ASR on LibriMix evaluation sets. Notably, for SD, UME outperforms
the previous studies, achieving diarization error rates of 1.37% and 2.29% on
Libri2Mix and Libri3Mix evaluation sets, respectively.

</details>


### [231] [Is Audio Spoof Detection Robust to Laundering Attacks?](https://arxiv.org/abs/2408.14712)
*Hashim Ali,Surya Subramani,Shefali Sudhir,Raksha Varahamurthy,Hafiz Malik*

Main category: eess.AS

TL;DR: 语音克隆技术进步带来滥用风险。本文评估了七种SOTA语音欺骗检测方法在含有“清洗攻击”（如混响、噪声）的新数据库上的性能。结果显示，现有SOTA方法在这些攻击下表现不佳，表明急需更鲁棒的检测技术。


<details>
  <summary>Details</summary>
Motivation: 语音克隆系统的高逼真度和低成本服务导致了潜在的滥用风险。尽管已有一些检测方法，但它们主要在纯净音频数据库（如ASVSpoof 2019）上进行评估。然而，实际攻击可能包含“清洗攻击”（Laundering Attacks），这使得现有方法的实际效果存疑，因此需要评估SOTA方法在存在清洗攻击时的性能。

Method: 1. 创建了一个新的“ASVSpoof清洗攻击数据库”，该数据库基于ASVSpoof 2019 (LA) 评估数据库，总时长1388.22小时，用于模拟清洗攻击。2. 在此清洗过的数据库上评估了七种SOTA语音欺骗检测方法。

Result: SOTA语音欺骗检测系统在存在激进的清洗攻击（特别是混响和加性噪声攻击）时表现不佳。

Conclusion: 当前SOTA语音欺骗检测方法在面对清洗攻击时鲁棒性不足，亟需开发能够抵御此类攻击的更鲁棒的检测方法。

Abstract: Voice-cloning (VC) systems have seen an exceptional increase in the realism
of synthesized speech in recent years. The high quality of synthesized speech
and the availability of low-cost VC services have given rise to many potential
abuses of this technology. Several detection methodologies have been proposed
over the years that can detect voice spoofs with reasonably good accuracy.
However, these methodologies are mostly evaluated on clean audio databases,
such as ASVSpoof 2019. This paper evaluates SOTA Audio Spoof Detection
approaches in the presence of laundering attacks. In that regard, a new
laundering attack database, called the ASVSpoof Laundering Database, is
created. This database is based on the ASVSpoof 2019 (LA) eval database
comprising a total of 1388.22 hours of audio recordings. Seven SOTA audio spoof
detection approaches are evaluated on this laundered database. The results
indicate that SOTA systems perform poorly in the presence of aggressive
laundering attacks, especially reverberation and additive noise attacks. This
suggests the need for robust audio spoof detection.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [232] [Flexible metadata harvesting for ecology using large language models](https://arxiv.org/abs/2508.20115)
*Zehao Lu,Thijs L van der Plas,Parinaz Rashidi,W Daniel Kissling,Ioannis N Athanasiadis*

Main category: cs.DL

TL;DR: 开发了一种基于大型语言模型（LLM）的元数据采集器，用于从多样化的生态数据平台灵活提取、统一并链接数据集元数据，以加速研究。


<details>
  <summary>Details</summary>
Motivation: 尽管大型开放数据集能促进生态研究，但研究人员难以在不同平台和标准下找到并整合合适的数据集。

Method: 开发了基于LLM的元数据采集器，能够从数据集着陆页灵活提取结构化和非结构化元数据，并转换为用户定义的统一格式。通过LLM后处理协议确保准确性，并利用LLM通过嵌入相似性和统一格式识别数据集之间的链接。

Result: 该工具能准确提取结构化和非结构化元数据，并能灵活链接不同数据集的元数据。

Conclusion: 该工具可用于本体创建或图基查询，帮助在虚拟研究环境中查找相关的生态和环境数据集。

Abstract: Large, open datasets can accelerate ecological research, particularly by
enabling researchers to develop new insights by reusing datasets from multiple
sources. However, to find the most suitable datasets to combine and integrate,
researchers must navigate diverse ecological and environmental data provider
platforms with varying metadata availability and standards. To overcome this
obstacle, we have developed a large language model (LLM)-based metadata
harvester that flexibly extracts metadata from any dataset's landing page, and
converts these to a user-defined, unified format using existing metadata
standards. We validate that our tool is able to extract both structured and
unstructured metadata with equal accuracy, aided by our LLM post-processing
protocol. Furthermore, we utilise LLMs to identify links between datasets, both
by calculating embedding similarity and by unifying the formats of extracted
metadata to enable rule-based processing. Our tool, which flexibly links the
metadata of different datasets, can therefore be used for ontology creation or
graph-based queries, for example, to find relevant ecological and environmental
datasets in a virtual research environment.

</details>


### [233] [Is Artificial Intelligence Reshaping the Landscape of the International Academic Community of Geosciences?](https://arxiv.org/abs/2508.20117)
*Liang Li,Yuntian Li,Wenxin Zhao,Shan Ye,Yun Lu*

Main category: cs.DL

TL;DR: AI正积极改变地球科学研究，表现为AI相关产出增加，发展中国家科学家可见度提升，以及国际合作改善。


<details>
  <summary>Details</summary>
Motivation: 评估人工智能（AI）对地球科学研究的积极影响及其在科研产出、国家影响力与国际合作方面的具体变化。

Method: 采用文献计量分析和主题建模的方法进行研究。

Result: 研究发现AI正积极改变地球科学研究，AI相关科研产出显著增加；发展中国家地球科学家在“AI for Science”范式中获得更高可见度；AI也改善了地球科学相关研究的国际合作格局。

Conclusion: 人工智能对地球科学研究产生了积极且多方面的变革，不仅提升了整体科研产出和发展中国家的参与度，还促进了国际间的合作。

Abstract: Through bibliometric analysis and topic modeling, we find that artificial
intelligence (AI) is positively transforming geosciences research, with a
notable increase in AI-related scientific output in recent years. We are
encouraged to observe that earth scientists from developing countries have
gained better visibility in the recent AI for Science (AI4S) paradigm and that
AI is also improving the landscape of international collaboration in
geoscience-related research.

</details>


### [234] [Leveraging Large Language Models for Generating Research Topic Ontologies: A Multi-Disciplinary Study](https://arxiv.org/abs/2508.20693)
*Tanay Aggarwal,Angelo Salatino,Francesco Osborne,Enrico Motta*

Main category: cs.DL

TL;DR: 本研究通过在新建的PEM-Rel-8K数据集上微调大型语言模型（LLMs），证明了LLMs在识别跨学科研究主题语义关系方面的卓越能力，为本体和分类法的自动化构建提供了有效途径。


<details>
  <summary>Details</summary>
Motivation: 研究领域的本体和分类法对于科学知识管理至关重要，但其创建和维护成本高昂、耗时，通常需要多领域专家的协调努力，导致覆盖不均、领域间连接有限和更新不及时。

Method: 研究评估了大型语言模型（LLMs）在生物医学、物理学和工程学三个学术领域中识别研究主题语义关系的能力。模型在零样本提示、思维链提示和基于现有本体微调三种条件下进行评估。此外，还评估了微调模型的跨领域迁移性。为支持分析，引入了包含8,000多个关系的PEM-Rel-8K数据集，这些关系提取自MeSH、PhySH和IEEE。

Result: 实验结果表明，在PEM-Rel-8K数据集上对LLMs进行微调，在所有学科中都取得了出色的性能。

Conclusion: 通过在专门数据集（如PEM-Rel-8K）上对LLMs进行微调，可以高效、准确地识别研究主题间的语义关系，为解决现有本体和分类法手动创建与维护的挑战提供了强有力的解决方案。

Abstract: Ontologies and taxonomies of research fields are critical for managing and
organising scientific knowledge, as they facilitate efficient classification,
dissemination and retrieval of information. However, the creation and
maintenance of such ontologies are expensive and time-consuming tasks, usually
requiring the coordinated effort of multiple domain experts. Consequently,
ontologies in this space often exhibit uneven coverage across different
disciplines, limited inter-domain connectivity, and infrequent updating cycles.
In this study, we investigate the capability of several large language models
to identify semantic relationships among research topics within three academic
domains: biomedicine, physics, and engineering. The models were evaluated under
three distinct conditions: zero-shot prompting, chain-of-thought prompting, and
fine-tuning on existing ontologies. Additionally, we assessed the cross-domain
transferability of fine-tuned models by measuring their performance when
trained in one domain and subsequently applied to a different one. To support
this analysis, we introduce PEM-Rel-8K, a novel dataset consisting of over
8,000 relationships extracted from the most widely adopted taxonomies in the
three disciplines considered in this study: MeSH, PhySH, and IEEE. Our
experiments demonstrate that fine-tuning LLMs on PEM-Rel-8K yields excellent
performance across all disciplines.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [235] [Data-Efficient Point Cloud Semantic Segmentation Pipeline for Unimproved Roads](https://arxiv.org/abs/2508.20135)
*Andrew Yarovoi,Christopher R. Valenta*

Main category: eess.IV

TL;DR: 本文提出了一种数据高效的点云分割管道和训练框架，通过两阶段训练和多数据集预训练，在仅使用50个标注点云的情况下，实现了对未改良道路等八类物体的鲁棒分割，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 在数据量有限的场景下，实现对未改良道路及其他七类物体的鲁棒点云分割是一个挑战。因此，需要开发一种数据高效的分割框架来解决这一问题。

Method: 该方法采用两阶段训练框架：首先，一个基于投影的卷积神经网络在公共城市数据集和少量领域内数据集的混合上进行预训练；然后，一个轻量级预测头仅在领域内数据上进行微调。此外，研究中还探讨了Point Prompt Training在批归一化层上的应用、Manifold Mixup作为正则化的效果，以及引入直方图归一化环境特征以进一步提升性能。

Result: 仅使用50个目标域的标注点云，与领域内数据的朴素训练相比，所提出的训练方法将平均交并比（mIoU）从33.5%提高到51.8%，整体准确率从85.5%提高到90.8%。

Conclusion: 研究结果表明，跨多个数据集的预训练是提高泛化能力和在有限领域内监督下实现鲁棒分割的关键。该研究提供了一个在挑战性、低数据场景下进行鲁健3D语义分割的实用框架。

Abstract: In this case study, we present a data-efficient point cloud segmentation
pipeline and training framework for robust segmentation of unimproved roads and
seven other classes. Our method employs a two-stage training framework: first,
a projection-based convolutional neural network is pre-trained on a mixture of
public urban datasets and a small, curated in-domain dataset; then, a
lightweight prediction head is fine-tuned exclusively on in-domain data. Along
the way, we explore the application of Point Prompt Training to batch
normalization layers and the effects of Manifold Mixup as a regularizer within
our pipeline. We also explore the effects of incorporating histogram-normalized
ambients to further boost performance. Using only 50 labeled point clouds from
our target domain, we show that our proposed training approach improves mean
Intersection-over-Union from 33.5% to 51.8% and the overall accuracy from 85.5%
to 90.8%, when compared to naive training on the in-domain data. Crucially, our
results demonstrate that pre-training across multiple datasets is key to
improving generalization and enabling robust segmentation under limited
in-domain supervision. Overall, this study demonstrates a practical framework
for robust 3D semantic segmentation in challenging, low-data scenarios. Our
code is available at: https://github.com/andrewyarovoi/MD-FRNet.

</details>


### [236] [UltraEar: a multicentric, large-scale database combining ultra-high-resolution computed tomography and clinical data for ear diseases](https://arxiv.org/abs/2508.20141)
*Ruowei Tang,Pengfei Zhao,Xiaoguang Li,Ning Xu,Yue Cheng,Mengshi Zhang,Zhixiang Wang,Zhengyu Zhang,Hongxia Yin,Heyu Ding,Shusheng Gong,Yuhe Liu,Zhenchang Wang*

Main category: eess.IV

TL;DR: 介绍UltraEar数据库的建立与设计，这是一个收集耳部疾病超高分辨率CT图像及相关临床数据的大型多中心存储库。


<details>
  <summary>Details</summary>
Motivation: 耳部疾病影响全球数十亿人，CT在诊断和治疗中至关重要。为推进耳科影像学研究和AI算法开发，急需一个大规模、高分辨率的耳部CT图像与临床数据资源。

Method: 从11家三级医院招募患者（2020-2035年），整合超高分辨率CT（U-HRCT）图像、结构化CT报告以及包括人口统计学、听力学、手术记录和病理学在内的综合临床信息。数据库涵盖多种耳部疾病。开发了标准化预处理流程进行几何校准、图像标注和多结构分割。数据经过去标识化处理以确保隐私，并通过专家小组会议协调收集与管理，安全存储于离线云系统。

Result: 成功建立了UltraEar数据库，提供了一个前所未有的、兼具技术保真度和临床相关性的超高分辨率耳部参考图谱。

Conclusion: UltraEar数据库有望显著推动放射学研究、AI算法的开发与验证、耳科影像培训以及多机构协作研究。该数据库将持续更新和扩展，以确保全球耳科研究社区的长期可访问性和可用性。

Abstract: Ear diseases affect billions of people worldwide, leading to substantial
health and socioeconomic burdens. Computed tomography (CT) plays a pivotal role
in accurate diagnosis, treatment planning, and outcome evaluation. The
objective of this study is to present the establishment and design of UltraEar
Database, a large-scale, multicentric repository of isotropic 0.1 mm
ultra-high-resolution CT (U-HRCT) images and associated clinical data dedicated
to ear diseases. UltraEar recruits patients from 11 tertiary hospitals between
October 2020 and October 2035, integrating U-HRCT images, structured CT
reports, and comprehensive clinical information, including demographics,
audiometric profiles, surgical records, and pathological findings. A broad
spectrum of otologic disorders is covered, such as otitis media, cholesteatoma,
ossicular chain malformation, temporal bone fracture, inner ear malformation,
cochlear aperture stenosis, enlarged vestibular aqueduct, and sigmoid sinus
bony deficiency. Standardized preprocessing pipelines have been developed for
geometric calibration, image annotation, and multi-structure segmentation. All
personal identifiers in DICOM headers and metadata are removed or anonymized to
ensure compliance with data privacy regulation. Data collection and curation
are coordinated through monthly expert panel meetings, with secure storage on
an offline cloud system. UltraEar provides an unprecedented
ultra-high-resolution reference atlas with both technical fidelity and clinical
relevance. This resource has significant potential to advance radiological
research, enable development and validation of AI algorithms, serve as an
educational tool for training in otologic imaging, and support
multi-institutional collaborative studies. UltraEar will be continuously
updated and expanded, ensuring long-term accessibility and usability for the
global otologic research community.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [237] [Collaborating with GenAI: Incentives and Replacements](https://arxiv.org/abs/2508.20213)
*Boaz Taitler,Omer Ben-Porat*

Main category: cs.GT

TL;DR: 本研究分析了生成式AI（GenAI）对团队协作的影响，发现GenAI可能导致员工懈怠，并揭示了经理团队选择的复杂性，强调了看似低价值员工的关键作用。


<details>
  <summary>Details</summary>
Motivation: 鉴于生成式AI（GenAI）正在重塑共享项目中的工作方式，可能提高生产力或取代员工，本研究旨在分析GenAI在此背景下如何影响协作。

Method: 提出一个理论框架来建模和分析GenAI对协作的影响。模型中包含经理选择团队、GenAI替代未选员工、员工自主选择努力程度及相应成本。此外，还通过广泛模拟来验证理论发现。

Result: 1. 即使GenAI几乎无效，也可能导致员工完全不努力。2. 经理的优化问题是NP完全的，但为（几乎）线性实例提供了一个高效算法。3. 低个体价值的员工在维持整体产出中可能发挥关键作用，将其排除可能引发级联效应。

Conclusion: GenAI的引入对团队协作影响深远且复杂，可能导致员工懈怠和整体产出下降的风险，同时揭示了即使是表面上价值不高的员工也对维持项目成功至关重要。经理在利用GenAI时需谨慎考虑其对团队动力和员工激励的潜在负面影响。

Abstract: The rise of Generative AI (GenAI) is reshaping how workers contribute to
shared projects. While workers can use GenAI to boost productivity or reduce
effort, managers may use it to replace some workers entirely. We present a
theoretical framework to analyze how GenAI affects collaboration in such
settings. In our model, the manager selects a team to work on a shared task,
with GenAI substituting for unselected workers. Each worker selects how much
effort to exert, and incurs a cost that increases with the level of effort. We
show that GenAI can lead workers to exert no effort, even if GenAI is almost
ineffective. We further show that the manager's optimization problem is
NP-complete, and provide an efficient algorithm for the special class of
(almost-) linear instances. Our analysis shows that even workers with low
individual value may play a critical role in sustaining overall output, and
excluding such workers can trigger a cascade. Finally, we conduct extensive
simulations to illustrate our theoretical findings.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [238] [Particle swarm optimization for online sparse streaming feature selection under uncertainty](https://arxiv.org/abs/2508.20123)
*Ruiyang Xu*

Main category: cs.NE

TL;DR: 针对高维流数据中不确定特征-标签关联和数据不完整性问题，提出POS2FS框架，通过PSO和三支决策理论实现更准确和鲁棒的在线稀疏流式特征选择。


<details>
  <summary>Details</summary>
Motivation: 在线稀疏流式特征选择（OS2FS）虽能处理流数据不完整性，但现有方法难以应对不确定的特征-标签关联，导致模型不灵活和性能下降。

Method: 提出POS2FS，一个基于粒子群优化（PSO）的、不确定性感知的在线稀疏流式特征选择框架。它引入1) PSO驱动的监督机制以减少特征-标签关系的不确定性；2) 三支决策理论以管理监督学习中的特征模糊性。

Result: 在六个真实世界数据集上的严格测试表明，POS2FS优于传统的OSFS和OS2FS技术，通过更鲁棒的特征子集选择提供了更高的准确性。

Conclusion: POS2FS框架有效解决了在线稀疏流式特征选择中不确定特征-标签关联和特征模糊性的问题，显著提升了特征选择的准确性和鲁棒性。

Abstract: In real-world applications involving high-dimensional streaming data, online
streaming feature selection (OSFS) is widely adopted. Yet, practical
deployments frequently face data incompleteness due to sensor failures or
technical constraints. While online sparse streaming feature selection (OS2FS)
mitigates this issue via latent factor analysis-based imputation, existing
methods struggle with uncertain feature-label correlations, leading to
inflexible models and degraded performance. To address these gaps, this work
proposes POS2FS-an uncertainty-aware online sparse streaming feature selection
framework enhanced by particle swarm optimization (PSO). The approach
introduces: 1) PSO-driven supervision to reduce uncertainty in feature-label
relationships; 2) Three-way decision theory to manage feature fuzziness in
supervised learning. Rigorous testing on six real-world datasets confirms
POS2FS outperforms conventional OSFS and OS2FS techniques, delivering higher
accuracy through more robust feature subset selection.

</details>


### [239] [Improving Liver Disease Diagnosis with SNNDeep: A Custom Spiking Neural Network Using Diverse Learning Algorithms](https://arxiv.org/abs/2508.20125)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.NE

TL;DR: 本研究开发了SNNDeep，一个定制化的尖峰神经网络(SNN)，用于肝脏CT图像的二分类。在MSD数据集上，定制的SNNDeep模型表现优于基于现有框架的实现，达到了98.35%的验证准确率，并为SNN在医学影像中的应用提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 尖峰神经网络(SNNs)作为一种节能且生物学上更合理的深度学习模型替代方案备受关注，但其在高风险生物医学成像领域的应用仍几乎未被探索，本研究旨在填补这一空白。

Method: 本研究引入了SNNDeep，一个专门为CT特征肝脏健康状态二分类优化的定制化SNN模型。模型在医学分割十项全能(MSD)的Task03\Liver数据集上开发和评估。研究比较了代理梯度学习、Tempotron规则和生物启发式主动学习这三种学习算法，并评估了一个完全定制的低级模型以及基于snnTorch和SpikingJelly框架的两种实现。超参数优化通过Optuna完成。

Result: 定制构建的SNNDeep模型持续优于基于框架的实现，实现了98.35%的最高验证准确率，在不同学习规则下表现出卓越的适应性，并显著降低了训练开销。

Conclusion: 本研究首次提供了经验证据，表明低级、高度可调的SNN在医学影像领域（特别是在数据有限、时间受限的诊断环境中）能够超越标准框架，从而为精准医疗中的神经启发式人工智能开辟了新途径。

Abstract: Purpose: Spiking neural networks (SNNs) have recently gained attention as
energy-efficient, biologically plausible alternatives to conventional deep
learning models. Their application in high-stakes biomedical imaging remains
almost entirely unexplored. Methods: This study introduces SNNDeep, the first
tailored SNN specifically optimized for binary classification of liver health
status from computed tomography (CT) features. To ensure clinical relevance and
broad generalizability, the model was developed and evaluated using the
Task03\Liver dataset from the Medical Segmentation Decathlon (MSD), a
standardized benchmark widely used for assessing performance across diverse
medical imaging tasks. We benchmark three fundamentally different learning
algorithms, namely Surrogate Gradient Learning, the Tempotron rule, and
Bio-Inspired Active Learning across three architectural variants: a fully
customized low-level model built from scratch, and two implementations using
leading SNN frameworks, i.e., snnTorch and SpikingJelly. Hyperparameter
optimization was performed using Optuna. Results: Our results demonstrate that
the custom-built SNNDeep consistently outperforms framework-based
implementations, achieving a maximum validation accuracy of 98.35%, superior
adaptability across learning rules, and significantly reduced training
overhead. Conclusion:This study provides the first empirical evidence that
low-level, highly tunable SNNs can surpass standard frameworks in medical
imaging, especially in data-limited, temporally constrained diagnostic
settings, thereby opening a new pathway for neuro-inspired AI in precision
medicine.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [240] [A Unified Theory of Language](https://arxiv.org/abs/2508.20109)
*Robert Worden*

Main category: q-bio.NC

TL;DR: 提出一种统一的语言理论，结合贝叶斯认知语言学和性选择进化论，基于扩展的结构语法通过快速统一计算来解释语言的各方面及其进化起源。


<details>
  <summary>Details</summary>
Motivation: 旨在建立一个统一的语言理论，以解释语言的效率、表达力、多样性、语用、句法和语义等核心特征，并阐明语言为展示智力而通过性选择进化的起源。

Method: 该理论的计算基础是结构语法，并新增了对语言语用学和快速精确语言学习的解释。构建式在心智中以图状特征结构表示，通过慢速推理学习初始范例后，利用贝叶斯最大似然模式匹配的统一机制进行快速应用和计算。

Result: 该理论成功解释了语言的主要事实，包括其速度、表达力、多样性以及语用、句法和语义数据。通过快速统一计算实现语言各方面的无缝处理，解决了语用学难题，并揭示了人类语言处理与动物贝叶斯认知之间的进化连续性。

Conclusion: 语言是人类心智解读、合作、自尊、情感以及人类文化和社会的基础。本研究通过构建统一理论，为理解语言的本质及其在认知和进化中的作用提供了全面框架。

Abstract: A unified theory of language combines a Bayesian cognitive linguistic model
of language processing, with the proposal that language evolved by sexual
selection for the display of intelligence. The theory accounts for the major
facts of language, including its speed and expressivity, and data on language
diversity, pragmatics, syntax and semantics. The computational element of the
theory is based on Construction Grammars. These give an account of the syntax
and semantics of the worlds languages, using constructions and unification. Two
novel elements are added to construction grammars: an account of language
pragmatics, and an account of fast, precise language learning. Constructions
are represented in the mind as graph like feature structures. People use slow
general inference to understand the first few examples they hear of any
construction. After that it is learned as a feature structure, and is rapidly
applied by unification. All aspects of language (phonology, syntax, semantics,
and pragmatics) are seamlessly computed by fast unification; there is no
boundary between semantics and pragmatics. This accounts for the major puzzles
of pragmatics, and for detailed pragmatic phenomena. Unification is Bayesian
maximum likelihood pattern matching. This gives evolutionary continuity between
language processing in the human brain, and Bayesian cognition in animal
brains. Language is the basis of our mind reading abilities, our cooperation,
self esteem and emotions; the foundations of human culture and society.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [241] [SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization](https://arxiv.org/abs/2508.20258)
*Arya Tschand,Muhammad Awad,Ryan Swann,Kesavan Ramakrishnan,Jeffrey Ma,Keith Lowery,Ganesh Dasika,Vijay Janapa Reddi*

Main category: cs.DC

TL;DR: SwizzlePerf通过赋予大语言模型（LLMs）硬件感知能力，自动为GPU内核生成空间优化模式，显著缩短优化时间并提升性能，最高实现2.06倍加速和70%的L2命中率提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的GPU内核性能工程方法效率低下，主要在于缺乏人类性能工程师所依赖的关键特性——硬件感知能力，这导致无法实现近乎最优的硬件利用率。

Method: SwizzlePerf通过向LLMs明确提供工作负载的内存访问模式、架构规范、过滤后的分析日志以及历史性能反馈等信息，使其具备硬件感知能力，从而自动为异构架构上的GPU内核生成空间优化（如swizzling模式）。

Result: ['对于GEMM内核，SwizzlePerf在不到5分钟内生成了专家性能工程师需2周才能找到的硬件特定最优swizzling模式。', '在包含10个多样化ML和科学内核的测试中，SwizzlePerf为其中9个内核生成了swizzling模式，实现了高达2.06倍的加速和70%的L2命中率提升。']

Conclusion: 这项工作是系统性地创建硬件感知型LLM性能工程代理的关键一步。

Abstract: Large language models (LLMs) have shown progress in GPU kernel performance
engineering using inefficient search-based methods that optimize around
runtime. Any existing approach lacks a key characteristic that human
performance engineers rely on for near-optimal utilization --
hardware-awareness. By leveraging the workload's specific memory access
patterns, architecture specifications, filtered profiling logs, and reflections
on historical performance, we can make software-level optimizations that are
tailored to the underlying hardware. SwizzlePerf automatically generates
spatial optimizations for GPU kernels on disaggregated architectures by giving
LLMs explicit hardware-awareness.
  For a GEMM kernel, SwizzlePerf takes less than 5 minutes to generate the same
hardware-specific optimal swizzling pattern that took expert performance
engineers 2 weeks to find. On a suite of 10 diverse ML and Science kernels,
SwizzlePerf can generate swizzling patterns for 9 of the kernels that achieve
up to a 2.06x speedup and 70% improvement in L2 hit rate. This work is the
first of many steps toward systematically creating hardware-aware LLM
performance engineering agents.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [242] [Microarchitecture Design and Benchmarking of Custom SHA-3 Instruction for RISC-V](https://arxiv.org/abs/2508.20653)
*Alperen Bolat,Sakir Sezer,Kieran McLaughlin,Henry Hui*

Main category: cs.AR

TL;DR: 本文研究了将SHA-3定制指令集成到RISC-V CPU架构中的微架构挑战，通过模拟和FPGA原型验证，实现了显著的性能提升，且硬件开销较低。


<details>
  <summary>Details</summary>
Motivation: 虽然现有加密指令（如AES-NI）性能良好，但SHA-3由于其独特的排列结构和内存访问模式，其高效加速仍是一个开放问题。现有解决方案多依赖协处理器或软件优化，未直接集成到微架构中。本研究旨在探讨将SHA-3排列操作作为定制指令嵌入通用处理器所面临的架构挑战。

Method: 研究并原型化了针对RISC-V CPU架构的SHA-3定制指令。通过周期精确的GEM5模拟和FPGA原型验证其性能和硬件开销。

Result: 实验结果显示，RISC-V优化的SHA-3软件工作负载性能提升高达8.02倍，Keccak特定软件工作负载性能提升高达46.31倍。硬件开销方面，寄存器增加15.09%，LUT利用率增加11.51%。

Conclusion: 研究结果为在微架构层面加速SHA-3的可行性和影响力提供了关键见解，并为未来的加密指令集扩展提供了实用的设计考量。

Abstract: Integrating cryptographic accelerators into modern CPU architectures presents
unique microarchitectural challenges, particularly when extending instruction
sets with complex and multistage operations. Hardware-assisted cryptographic
instructions, such as Intel's AES-NI and ARM's custom instructions for
encryption workloads, have demonstrated substantial performance improvements.
However, efficient SHA-3 acceleration remains an open problem due to its
distinct permutation-based structure and memory access patterns. Existing
solutions primarily rely on standalone coprocessors or software optimizations,
often avoiding the complexities of direct microarchitectural integration. This
study investigates the architectural challenges of embedding a SHA-3
permutation operation as a custom instruction within a general-purpose
processor, focusing on pipelined simultaneous execution, storage utilization,
and hardware cost. In this paper, we investigated and prototyped a SHA-3 custom
instruction for the RISC-V CPU architecture. Using cycle-accurate GEM5
simulations and FPGA prototyping, our results demonstrate performance
improvements of up to 8.02x for RISC-V optimized SHA-3 software workloads and
up to 46.31x for Keccak-specific software workloads, with only a 15.09%
increase in registers and a 11.51% increase in LUT utilization. These findings
provide critical insights into the feasibility and impact of SHA-3 acceleration
at the microarchitectural level, highlighting practical design considerations
for future cryptographic instruction set extensions.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [243] [The Mathematician's Assistant: Integrating AI into Research Practice](https://arxiv.org/abs/2508.20236)
*Jonas Henkel*

Main category: math.HO

TL;DR: 本文分析了人工智能（特别是大型语言模型）在数学研究中的应用前景与局限性。研究发现AI在解决问题和评估证明方面表现出色，但也存在缺乏自我批判和完整证明有效性不足的问题。基于此，论文提出了一个“增强型数学家”框架，倡导AI作为人类研究者的副驾驶，并总结了有效和负责任使用的五项原则及七种具体应用方式，强调AI目前的核心作用是辅助而非自动化。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能（如AlphaEvolve和Gemini Deep Think）的快速发展，其为数学研究带来了强大的新工具，有潜力显著改变研究实践。因此，需要探索公开可用的大型语言模型在数学研究背景下的当前状况，理解其能力和局限性，并为有效整合AI提出框架。

Method: 本研究基于截至2025年8月2日的最新发展，探索了公开可用的LLMs在数学研究中的应用。通过分析MathArena和Open Proof Corpus等最新基准，揭示了AI模型的表现。基于这些发现，论文提出了一个将AI整合到研究工作流中的持久框架，即“增强型数学家”模型，并提炼出五项指导原则，随后系统地探讨了AI在研究生命周期中（从创意到最终撰写）的七种基本应用方式。

Result: 研究发现，最先进的模型在解决问题和评估证明方面表现出强大的能力。然而，它们也存在系统性缺陷，包括缺乏自我批判能力以及最终答案准确性与完整证明有效性之间的模型依赖差异。基于这些发现，论文提出了一个以“增强型数学家”为核心的AI整合框架，其中AI作为人类研究者的副驾驶，并提炼出五项有效和负责任使用的指导原则。此外，论文系统地探索了AI在研究生命周期中应用的七种基本方式。

Conclusion: 结论是，人工智能目前的主要作用是增强而非自动化。这要求研究人员掌握一套新的技能，包括战略性提示、批判性验证和方法论严谨性，以便有效利用这些强大的工具。

Abstract: The rapid development of artificial intelligence (AI), marked by
breakthroughs like 'AlphaEvolve' and 'Gemini Deep Think', is beginning to offer
powerful new tools that have the potential to significantly alter the research
practice in many areas of mathematics. This paper explores the current
landscape of publicly accessible large language models (LLMs) in a mathematical
research context, based on developments up to August 2, 2025. Our analysis of
recent benchmarks, such as MathArena and the Open Proof Corpus (Balunovi\'c et
al., 2025; Dekoninck et al., 2025), reveals a complex duality: while
state-of-the-art models demonstrate strong abilities in solving problems and
evaluating proofs, they also exhibit systematic flaws, including a lack of
self-critique and a model depending discrepancy between final-answer accuracy
and full-proof validity.
  Based on these findings, we propose a durable framework for integrating AI
into the research workflow, centered on the principle of the augmented
mathematician. In this model, the AI functions as a copilot under the critical
guidance of the human researcher, an approach distilled into five guiding
principles for effective and responsible use. We then systematically explore
seven fundamental ways AI can be applied across the research lifecycle, from
creativity and ideation to the final writing process, demonstrating how these
principles translate into concrete practice.
  We conclude that the primary role of AI is currently augmentation rather than
automation. This requires a new skill set focused on strategic prompting,
critical verification, and methodological rigor in order to effectively use
these powerful tools.

</details>
