{"id": "2508.05130", "pdf": "https://arxiv.org/pdf/2508.05130", "abs": "https://arxiv.org/abs/2508.05130", "authors": ["Ali Raza", "Muhammad Farhan Khan", "Zeeshan Alam", "Muhammad Saad", "Ilyas Saleem", "Muhammad Ahmed Mohsin", "Muhammad Ali Jamshed"], "title": "TeraRIS NOMA-MIMO Communications for 6G and Beyond Industrial Networks", "categories": ["cs.NI", "eess.SP"], "comment": "Accepted at PIMRC", "summary": "This paper presents a joint framework that integrates reconfigurable\nintelligent surfaces (RISs) with Terahertz (THz) communications and\nnon-orthogonal multiple access (NOMA) to enhance smart industrial\ncommunications. The proposed system leverages the advantages of RIS and THz\nbands to improve spectral efficiency, coverage, and reliability key\nrequirements for industrial automation and real-time communications in future\n6G networks and beyond. Within this framework, two power allocation strategies\nare investigated: the first optimally distributes power between near and far\nindustrial nodes, and the second prioritizes network demands to enhance system\nperformance further. A performance evaluation is conducted to compare the sum\nrate and outage probability against a fixed power allocation scheme. Our scheme\nachieves up to a 23% sum rate gain over fixed PA at 30 dBm. Simulation results\nvalidate the theoretical analysis, demonstrating the effectiveness and\nrobustness of the RIS-assisted NOMA MIMO framework for THz enabled industrial\ncommunications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u3001\u592a\u8d6b\u5179\uff08THz\uff09\u901a\u4fe1\u548c\u975e\u6b63\u4ea4\u591a\u5740\uff08NOMA\uff09\u7684\u8054\u5408\u6846\u67b6\uff0c\u65e8\u5728\u589e\u5f3a\u667a\u80fd\u5de5\u4e1a\u901a\u4fe1\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u529f\u7387\u5206\u914d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u548c\u901f\u7387\u548c\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u4e3a\u6ee1\u8db3\u672a\u67656G\u7f51\u7edc\u53ca\u66f4\u8fdc\u671f\u5de5\u4e1a\u81ea\u52a8\u5316\u548c\u5b9e\u65f6\u901a\u4fe1\u5bf9\u9891\u8c31\u6548\u7387\u3001\u8986\u76d6\u8303\u56f4\u548c\u53ef\u9760\u6027\u7684\u5173\u952e\u9700\u6c42\uff0c\u5229\u7528RIS\u548cTHz\u9891\u6bb5\u7684\u4f18\u52bf\u6765\u63d0\u5347\u901a\u4fe1\u6027\u80fd\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u96c6\u6210RIS\u3001THz\u901a\u4fe1\u548cNOMA\u7684\u8054\u5408\u6846\u67b6\uff0c\u5e76\u7814\u7a76\u4e86\u4e24\u79cd\u529f\u7387\u5206\u914d\u7b56\u7565\uff1a\u4e00\u662f\u4f18\u5316\u8fd1\u8fdc\u5de5\u4e1a\u8282\u70b9\u95f4\u7684\u529f\u7387\u5206\u914d\uff0c\u4e8c\u662f\u57fa\u4e8e\u7f51\u7edc\u9700\u6c42\u8fdb\u884c\u4f18\u5148\u7ea7\u5206\u914d\u3002\u901a\u8fc7\u5bf9\u6bd4\u56fa\u5b9a\u529f\u7387\u5206\u914d\u65b9\u6848\uff0c\u8bc4\u4f30\u4e86\u548c\u901f\u7387\u548c\u4e2d\u65ad\u6982\u7387\u6027\u80fd\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6848\u572830 dBm\u529f\u7387\u4e0b\uff0c\u548c\u901f\u7387\u6bd4\u56fa\u5b9a\u529f\u7387\u5206\u914d\u65b9\u6848\u63d0\u9ad8\u4e8623%\u3002\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684RIS\u8f85\u52a9NOMA MIMO\u6846\u67b6\u5bf9\u4e8eTHz\u5de5\u4e1a\u901a\u4fe1\u662f\u6709\u6548\u4e14\u9c81\u68d2\u7684\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2508.05249", "pdf": "https://arxiv.org/pdf/2508.05249", "abs": "https://arxiv.org/abs/2508.05249", "authors": ["Jos\u00e9 Ruela", "Ivan Cojocaru", "Andr\u00e9 Coelho", "Rui Campos", "Manuel Ricardo"], "title": "Modular Design and Experimental Evaluation of 5G Mobile Cell Architectures Based on Overlay and Integrated Models", "categories": ["cs.NI"], "comment": null, "summary": "This paper presents the concept, architectural design, and performance\nevaluation of a 5G Mobile Cell (MC) used to provide 5G wireless connectivity to\nUser Equipment (UE) in areas with limited fixed 5G infrastructures or subject\nto adverse radio conditions. We consider two main approaches to MC design: an\noverlay model, where the MC obtains backhaul connectivity from a 5G overlay\nnetwork, and an Integrated Access and Backhaul (IAB)-based model, discussing\ntheir protocol stacks and architectural implications. In order to validate the\nMC's performance, we employ an emulation-based testbed using the\nOpenAirInterface (OAI) implementation, considering different MC positions. The\nresults validate the MC concept and demonstrate that MC positioning\nsignificantly influences network performance. This paper has the potential to\naid network operators and service providers in selecting and deploying MC\narchitectures for temporary coverage extension and capacity reinforcement in\ndifferent environments, including seaports, industrial scenarios, and public\nsafety.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e865G\u79fb\u52a8\u8702\u7a9d(MC)\u7684\u6982\u5ff5\u3001\u67b6\u6784\u8bbe\u8ba1\u4e0e\u6027\u80fd\u8bc4\u4f30\uff0c\u65e8\u5728\u4e3a\u57fa\u7840\u8bbe\u65bd\u4e0d\u8db3\u6216\u65e0\u7ebf\u6761\u4ef6\u6076\u52a3\u533a\u57df\u63d0\u4f9b5G\u8fde\u63a5\u3002\u7814\u7a76\u4e86\u8986\u76d6\u548cIAB\u4e24\u79cd\u6a21\u578b\uff0c\u5e76\u5229\u7528OAI\u4eff\u771f\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660eMC\u5b9a\u4f4d\u5bf9\u7f51\u7edc\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u5728\u56fa\u5b9a5G\u57fa\u7840\u8bbe\u65bd\u6709\u9650\u6216\u65e0\u7ebf\u7535\u6761\u4ef6\u6076\u52a3\u7684\u533a\u57df\uff0c\u4e3a\u7528\u6237\u8bbe\u5907(UE)\u63d0\u4f9b5G\u65e0\u7ebf\u8fde\u63a5\u3002", "method": "\u63d0\u51fa\u5e76\u63a2\u8ba8\u4e86\u4e24\u79cd5G\u79fb\u52a8\u8702\u7a9d(MC)\u8bbe\u8ba1\u65b9\u6cd5\uff1a\u8986\u76d6\u6a21\u578b\uff08\u4ece5G\u8986\u76d6\u7f51\u7edc\u83b7\u53d6\u56de\u7a0b\uff09\u548c\u57fa\u4e8e\u96c6\u6210\u63a5\u5165\u4e0e\u56de\u7a0b(IAB)\u7684\u6a21\u578b\u3002\u901a\u8fc7\u4f7f\u7528OpenAirInterface (OAI)\u5b9e\u73b0\u7684\u4eff\u771f\u6d4b\u8bd5\u5e73\u53f0\u8fdb\u884c\u6027\u80fd\u9a8c\u8bc1\uff0c\u5e76\u8003\u8651\u4e86\u4e0d\u540c\u7684MC\u90e8\u7f72\u4f4d\u7f6e\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u9a8c\u8bc1\u4e865G\u79fb\u52a8\u8702\u7a9d(MC)\u7684\u6982\u5ff5\uff0c\u5e76\u8bc1\u660eMC\u7684\u5b9a\u4f4d\u5bf9\u7f51\u7edc\u6027\u80fd\u5177\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u6709\u6f5c\u529b\u5e2e\u52a9\u7f51\u7edc\u8fd0\u8425\u5546\u548c\u670d\u52a1\u63d0\u4f9b\u5546\u5728\u591a\u79cd\u73af\u5883\u4e2d\uff08\u5982\u6e2f\u53e3\u3001\u5de5\u4e1a\u548c\u516c\u5171\u5b89\u5168\uff09\uff0c\u9009\u62e9\u548c\u90e8\u7f72MC\u67b6\u6784\u4ee5\u5b9e\u73b0\u4e34\u65f6\u8986\u76d6\u6269\u5c55\u548c\u5bb9\u91cf\u589e\u5f3a\u3002"}}
{"id": "2508.04967", "pdf": "https://arxiv.org/pdf/2508.04967", "abs": "https://arxiv.org/abs/2508.04967", "authors": ["Yuan Li", "Chen Zhang", "Hao Zhang", "Tao Huang", "Yunjie Liu"], "title": "A Design for an Early Quantum Network", "categories": ["quant-ph", "cs.NI"], "comment": null, "summary": "With the rapid advancement of quantum information technology, quantum\nnetworks have become essential for supporting diverse applications, which often\nhave stringent demands for key metrics such as fidelity and request completion\ntime. In this work, we propose a design for early-stage quantum networks that\nis compatible with the three existing quantum repeater technologies. The design\naims to maximize the ability of the network to accommodate the diverse needs of\nquantum applications, even under conditions of limited quantum resources and\nsuboptimal network performance. We have also described the required identifiers\nin the quantum network and the specific process for implementing quantum\nrequests. To assess the feasibility of our design, we conduct simulations based\non discrete-event modeling of quantum networks. The simulations consider\nvarious types of noise and imperfect parameters that might exist in early-stage\nnetworks. We analyze the impact of these parameters on the fidelity of the\ngenerated entangled states and the request completion time. Furthermore, we\ninvestigated additional decisions that the central controller can make beyond\npath selection, such as the choice of cutoff time and the allocation of network\nresources to requests.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e9\u671f\u91cf\u5b50\u7f51\u7edc\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4eff\u771f\u8bc4\u4f30\u5176\u5728\u6709\u9650\u8d44\u6e90\u548c\u975e\u7406\u60f3\u6761\u4ef6\u4e0b\u9002\u5e94\u591a\u6837\u5316\u5e94\u7528\u9700\u6c42\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u968f\u7740\u91cf\u5b50\u4fe1\u606f\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u91cf\u5b50\u7f51\u7edc\u5bf9\u652f\u6301\u591a\u6837\u5316\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8fd9\u4e9b\u5e94\u7528\u5bf9\u7ea0\u7f20\u6001\u4fdd\u771f\u5ea6\u3001\u8bf7\u6c42\u5b8c\u6210\u65f6\u95f4\u7b49\u5173\u952e\u6307\u6807\u6709\u4e25\u683c\u8981\u6c42\uff0c\u4e14\u65e9\u671f\u7f51\u7edc\u9762\u4e34\u8d44\u6e90\u6709\u9650\u548c\u6027\u80fd\u6b20\u4f73\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u517c\u5bb9\u4e09\u79cd\u73b0\u6709\u91cf\u5b50\u4e2d\u7ee7\u5668\u6280\u672f\u7684\u65e9\u671f\u91cf\u5b50\u7f51\u7edc\u8bbe\u8ba1\uff0c\u5e76\u8be6\u7ec6\u63cf\u8ff0\u4e86\u6240\u9700\u7684\u7f51\u7edc\u6807\u8bc6\u7b26\u548c\u91cf\u5b50\u8bf7\u6c42\u7684\u5b9e\u73b0\u8fc7\u7a0b\u3002\u901a\u8fc7\u57fa\u4e8e\u79bb\u6563\u4e8b\u4ef6\u6a21\u578b\u7684\u91cf\u5b50\u7f51\u7edc\u4eff\u771f\u8bc4\u4f30\u8bbe\u8ba1\u7684\u53ef\u884c\u6027\uff0c\u4eff\u771f\u4e2d\u8003\u8651\u4e86\u65e9\u671f\u7f51\u7edc\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u5404\u79cd\u566a\u58f0\u548c\u4e0d\u5b8c\u5584\u53c2\u6570\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u4e2d\u592e\u63a7\u5236\u5668\u9664\u8def\u5f84\u9009\u62e9\u5916\u7684\u5176\u4ed6\u51b3\u7b56\uff0c\u5982\u622a\u6b62\u65f6\u95f4\u7684\u9009\u62e9\u548c\u7f51\u7edc\u8d44\u6e90\u7684\u5206\u914d\u3002", "result": "\u5206\u6790\u4e86\u5404\u79cd\u566a\u58f0\u548c\u4e0d\u5b8c\u5584\u53c2\u6570\u5bf9\u751f\u6210\u7ea0\u7f20\u6001\u4fdd\u771f\u5ea6\u53ca\u8bf7\u6c42\u5b8c\u6210\u65f6\u95f4\u7684\u5f71\u54cd\u3002\u7814\u7a76\u4e86\u4e2d\u592e\u63a7\u5236\u5668\u5728\u622a\u6b62\u65f6\u95f4\u9009\u62e9\u548c\u7f51\u7edc\u8d44\u6e90\u5206\u914d\u7b49\u65b9\u9762\u7684\u989d\u5916\u51b3\u7b56\u53ca\u5176\u6f5c\u5728\u5f71\u54cd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bbe\u8ba1\u9002\u7528\u4e8e\u65e9\u671f\u91cf\u5b50\u7f51\u7edc\uff0c\u5373\u4f7f\u5728\u6709\u9650\u91cf\u5b50\u8d44\u6e90\u548c\u6b21\u4f18\u7f51\u7edc\u6027\u80fd\u6761\u4ef6\u4e0b\uff0c\u4e5f\u80fd\u6700\u5927\u9650\u5ea6\u5730\u6ee1\u8db3\u91cf\u5b50\u5e94\u7528\u7684\u591a\u6837\u5316\u9700\u6c42\uff0c\u5176\u53ef\u884c\u6027\u5df2\u901a\u8fc7\u8003\u8651\u566a\u58f0\u548c\u4e0d\u5b8c\u5584\u53c2\u6570\u7684\u4eff\u771f\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2508.04795", "pdf": "https://arxiv.org/pdf/2508.04795", "abs": "https://arxiv.org/abs/2508.04795", "authors": ["Thomas Thebaud", "Yen-Ju Lu", "Matthew Wiesner", "Peter Viechnicki", "Najim Dehak"], "title": "Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted in the 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop", "summary": "In dialogue transcription pipelines, Large Language Models (LLMs) are\nfrequently employed in post-processing to improve grammar, punctuation, and\nreadability. We explore a complementary post-processing step: enriching\ntranscribed dialogues by adding metadata tags for speaker characteristics such\nas age, gender, and emotion. Some of the tags are global to the entire\ndialogue, while some are time-variant. Our approach couples frozen audio\nfoundation models, such as Whisper or WavLM, with a frozen LLAMA language model\nto infer these speaker attributes, without requiring task-specific fine-tuning\nof either model. Using lightweight, efficient connectors to bridge audio and\nlanguage representations, we achieve competitive performance on speaker\nprofiling tasks while preserving modularity and speed. Additionally, we\ndemonstrate that a frozen LLAMA model can compare x-vectors directly, achieving\nan Equal Error Rate of 8.8% in some scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528\u51bb\u7ed3\u7684\u97f3\u9891\u57fa\u7840\u6a21\u578b\u548c\u51bb\u7ed3\u7684LLAMA\u8bed\u8a00\u6a21\u578b\uff08\u65e0\u9700\u5fae\u8c03\uff09\u6765\u4e3a\u5bf9\u8bdd\u8f6c\u5f55\u672c\u6dfb\u52a0\u8bf4\u8bdd\u8005\u5143\u6570\u636e\uff08\u5982\u5e74\u9f84\u3001\u6027\u522b\u3001\u60c5\u611f\uff09\u7684\u540e\u5904\u7406\u65b9\u6cd5\u3002", "motivation": "\u5728\u5bf9\u8bdd\u8f6c\u5f55\u6d41\u7a0b\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e38\u7528\u4e8e\u6539\u5584\u6587\u672c\u7684\u8bed\u6cd5\u3001\u6807\u70b9\u548c\u53ef\u8bfb\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u4e2a\u8865\u5145\u6027\u7684\u540e\u5904\u7406\u6b65\u9aa4\uff0c\u5373\u901a\u8fc7\u6dfb\u52a0\u8bf4\u8bdd\u8005\u7279\u5f81\uff08\u5982\u5e74\u9f84\u3001\u6027\u522b\u3001\u60c5\u611f\uff09\u7684\u5143\u6570\u636e\u6807\u7b7e\u6765\u4e30\u5bcc\u8f6c\u5f55\u5bf9\u8bdd\uff0c\u4ee5\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u4fe1\u606f\u3002", "method": "\u8be5\u65b9\u6cd5\u5c06\u51bb\u7ed3\u7684\u97f3\u9891\u57fa\u7840\u6a21\u578b\uff08\u5982Whisper\u6216WavLM\uff09\u4e0e\u51bb\u7ed3\u7684LLAMA\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u4ee5\u63a8\u65ad\u8bf4\u8bdd\u8005\u5c5e\u6027\uff0c\u4e14\u65e0\u9700\u5bf9\u4efb\u4e00\u6a21\u578b\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\u3002\u901a\u8fc7\u4f7f\u7528\u8f7b\u91cf\u3001\u9ad8\u6548\u7684\u8fde\u63a5\u5668\u6765\u6865\u63a5\u97f3\u9891\u548c\u8bed\u8a00\u8868\u793a\u3002\u6b64\u5916\uff0c\u8fd8\u5c55\u793a\u4e86\u51bb\u7ed3\u7684LLAMA\u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u6bd4\u8f83x-vectors\u3002", "result": "\u5728\u8bf4\u8bdd\u8005\u5206\u6790\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u5757\u5316\u548c\u5904\u7406\u901f\u5ea6\u3002\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\uff0c\u51bb\u7ed3\u7684LLAMA\u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u6bd4\u8f83x-vectors\uff0c\u5e76\u5b9e\u73b0\u4e868.8%\u7684\u7b49\u9519\u8bef\u7387\uff08EER\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u4e00\u4e2a\u7531\u51bb\u7ed3\u97f3\u9891\u6a21\u578b\u548c\u51bb\u7ed3\u8bed\u8a00\u6a21\u578b\u7ec4\u6210\u7684\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\uff0c\u53ef\u4ee5\u6709\u6548\u4e14\u53ef\u884c\u5730\u4e30\u5bcc\u5bf9\u8bdd\u8f6c\u5f55\u672c\u7684\u8bf4\u8bdd\u8005\u5143\u6570\u636e\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3001\u6a21\u5757\u5316\u548c\u901f\u5ea6\u3002\u6b64\u5916\uff0c\u51bb\u7ed3\u7684LLAMA\u6a21\u578b\u5728\u76f4\u63a5\u6bd4\u8f83\u97f3\u9891\u5d4c\u5165\u65b9\u9762\u4e5f\u663e\u793a\u51fa\u6f5c\u529b\u3002"}}
{"id": "2508.04796", "pdf": "https://arxiv.org/pdf/2508.04796", "abs": "https://arxiv.org/abs/2508.04796", "authors": ["Negar Foroutan", "Clara Meister", "Debjit Paul", "Joel Niklaus", "Sina Ahmadi", "Antoine Bosselut", "Rico Sennrich"], "title": "Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Tokenization is the first -- and often least scrutinized -- step of most NLP\npipelines. Standard algorithms for learning tokenizers rely on frequency-based\nobjectives, which favor languages dominant in the training data and\nconsequently leave lower-resource languages with tokenizations that are\ndisproportionately longer, morphologically implausible, or even riddled with\n<UNK> placeholders. This phenomenon ultimately amplifies computational and\nfinancial inequalities between users from different language backgrounds. To\nremedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of\nthe widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes\nthe compression gain of the currently worst-compressed language, trading a\nsmall amount of global compression for cross-lingual parity. We find\nempirically that Parity-aware BPE leads to more equitable token counts across\nlanguages, with negligible impact on global compression rate and no substantial\neffect on language-model performance in downstream tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aParity-aware BPE\u7684\u65b0\u578b\u5206\u8bcd\u7b97\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u5206\u8bcd\u5668\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4e0d\u516c\u5e73\u73b0\u8c61\uff0c\u901a\u8fc7\u4f18\u5148\u63d0\u5347\u6700\u5dee\u538b\u7f29\u8bed\u8a00\u7684\u6027\u80fd\u6765\u5e73\u8861\u8de8\u8bed\u8a00\u5206\u8bcd\u8ba1\u6570\uff0c\u4e14\u5bf9\u5168\u5c40\u538b\u7f29\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u5f71\u54cd\u751a\u5fae\u3002", "motivation": "\u5927\u591a\u6570NLP\u6d41\u6c34\u7ebf\u7684\u9996\u8981\u6b65\u9aa4\u2014\u2014\u5206\u8bcd\u2014\u2014\u901a\u5e38\u672a\u53d7\u5145\u5206\u5ba1\u89c6\u3002\u6807\u51c6\u7684\u5206\u8bcd\u7b97\u6cd5\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u9891\u7387\u7684\u76ee\u6807\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u504f\u5411\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u4f18\u52bf\u8bed\u8a00\uff0c\u5bfc\u81f4\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5206\u8bcd\u7ed3\u679c\u8fc7\u957f\u3001\u4e0d\u5408\u7406\uff0c\u751a\u81f3\u5145\u6ee1\u672a\u77e5\u7b26\u53f7\uff0c\u4ece\u800c\u52a0\u5267\u4e86\u4e0d\u540c\u8bed\u8a00\u80cc\u666f\u7528\u6237\u95f4\u7684\u8ba1\u7b97\u548c\u7ecf\u6d4e\u4e0d\u5e73\u7b49\u3002", "method": "\u5f15\u5165\u4e86\u201cParity-aware Byte Pair Encoding (BPE)\u201d\u7b97\u6cd5\uff0c\u5b83\u662f\u5e7f\u6cdb\u4f7f\u7528\u7684BPE\u7b97\u6cd5\u7684\u4e00\u79cd\u53d8\u4f53\u3002\u5728\u6bcf\u4e2a\u5408\u5e76\u6b65\u9aa4\u4e2d\uff0cParity-aware BPE\u90fd\u6700\u5927\u5316\u5f53\u524d\u538b\u7f29\u6548\u679c\u6700\u5dee\u8bed\u8a00\u7684\u538b\u7f29\u589e\u76ca\uff0c\u4ee5\u6b64\u727a\u7272\u5c11\u91cf\u5168\u5c40\u538b\u7f29\u6765\u6362\u53d6\u8de8\u8bed\u8a00\u7684\u516c\u5e73\u6027\u3002", "result": "\u7ecf\u9a8c\u6027\u7814\u7a76\u8868\u660e\uff0cParity-aware BPE\u5b9e\u73b0\u4e86\u66f4\u516c\u5e73\u7684\u8de8\u8bed\u8a00\u5206\u8bcd\u8ba1\u6570\uff0c\u5bf9\u5168\u5c40\u538b\u7f29\u7387\u7684\u5f71\u54cd\u53ef\u5ffd\u7565\u4e0d\u8ba1\uff0c\u4e14\u5bf9\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u6ca1\u6709\u5b9e\u8d28\u6027\u5f71\u54cd\u3002", "conclusion": "Parity-aware BPE\u6210\u529f\u5730\u89e3\u51b3\u4e86\u4f20\u7edf\u5206\u8bcd\u5668\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4e0d\u516c\u5e73\u95ee\u9898\uff0c\u5728\u4e0d\u663e\u8457\u5f71\u54cd\u6574\u4f53\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u4e0d\u540c\u8bed\u8a00\u95f4\u7684\u5206\u8bcd\u516c\u5e73\u6027\u3002"}}
{"id": "2508.04714", "pdf": "https://arxiv.org/pdf/2508.04714", "abs": "https://arxiv.org/abs/2508.04714", "authors": ["Chitranshu Harbola", "Anupam Purwar"], "title": "Prescriptive Agents based on Rag for Automated Maintenance (PARAM)", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "eess.SP"], "comment": null, "summary": "Industrial machinery maintenance requires timely intervention to prevent\ncatastrophic failures and optimize operational efficiency. This paper presents\nan integrated Large Language Model (LLM)-based intelligent system for\nprescriptive maintenance that extends beyond traditional anomaly detection to\nprovide actionable maintenance recommendations. Building upon our prior LAMP\nframework for numerical data analysis, we develop a comprehensive solution that\ncombines bearing vibration frequency analysis with multi agentic generation for\nintelligent maintenance planning. Our approach serializes bearing vibration\ndata (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM\nprocessing, enabling few-shot anomaly detection with high accuracy. The system\nclassifies fault types (inner race, outer race, ball/roller, cage faults) and\nassesses severity levels. A multi-agentic component processes maintenance\nmanuals using vector embeddings and semantic search, while also conducting web\nsearches to retrieve comprehensive procedural knowledge and access up-to-date\nmaintenance practices for more accurate and in-depth recommendations. The\nGemini model then generates structured maintenance recommendations includes\nimmediate actions, inspection checklists, corrective measures, parts\nrequirements, and timeline specifications. Experimental validation in bearing\nvibration datasets demonstrates effective anomaly detection and contextually\nrelevant maintenance guidance. The system successfully bridges the gap between\ncondition monitoring and actionable maintenance planning, providing industrial\npractitioners with intelligent decision support. This work advances the\napplication of LLMs in industrial maintenance, offering a scalable framework\nfor prescriptive maintenance across machinery components and industrial\nsectors.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u8f74\u627f\u632f\u52a8\u6570\u636e\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u5206\u6790\uff0c\u5e76\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u77e5\u8bc6\u68c0\u7d22\uff0c\u4e3a\u5de5\u4e1a\u673a\u68b0\u63d0\u4f9b\u7cbe\u51c6\u7684\u6545\u969c\u68c0\u6d4b\u548c\u53ef\u64cd\u4f5c\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u5efa\u8bae\u3002", "motivation": "\u5de5\u4e1a\u673a\u68b0\u7ef4\u62a4\u9700\u8981\u53ca\u65f6\u5e72\u9884\u4ee5\u9632\u6b62\u707e\u96be\u6027\u6545\u969c\u5e76\u4f18\u5316\u8fd0\u884c\u6548\u7387\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u9650\u4e8e\u5f02\u5e38\u68c0\u6d4b\uff0c\u96be\u4ee5\u63d0\u4f9b\u5177\u4f53\u53ef\u884c\u7684\u7ef4\u62a4\u5efa\u8bae\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u8d85\u8d8a\u4f20\u7edf\u68c0\u6d4b\u3001\u63d0\u4f9b\u884c\u52a8\u6307\u5357\u7684\u667a\u80fd\u7ef4\u62a4\u7cfb\u7edf\u3002", "method": "\u8be5\u7cfb\u7edf\u5c06\u8f74\u627f\u632f\u52a8\u9891\u7387\u6570\u636e\uff08BPFO, BPFI, BSF, FTF\uff09\u5e8f\u5217\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\uff0c\u5229\u7528LLM\uff08Gemini\u6a21\u578b\uff09\u8fdb\u884c\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u3001\u6545\u969c\u7c7b\u578b\u5206\u7c7b\u548c\u4e25\u91cd\u6027\u8bc4\u4f30\u3002\u7cfb\u7edf\u5305\u542b\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7ec4\u4ef6\uff0c\u901a\u8fc7\u5411\u91cf\u5d4c\u5165\u548c\u8bed\u4e49\u641c\u7d22\u5904\u7406\u7ef4\u62a4\u624b\u518c\uff0c\u5e76\u8fdb\u884c\u7f51\u7edc\u641c\u7d22\u4ee5\u83b7\u53d6\u6700\u65b0\u7684\u7ef4\u62a4\u77e5\u8bc6\u3002\u6700\u7ec8\uff0cLLM\u751f\u6210\u7ed3\u6784\u5316\u7684\u7ef4\u62a4\u5efa\u8bae\uff0c\u5305\u62ec\u5373\u65f6\u884c\u52a8\u3001\u68c0\u67e5\u6e05\u5355\u3001\u7ea0\u6b63\u63aa\u65bd\u3001\u96f6\u4ef6\u9700\u6c42\u548c\u65f6\u95f4\u8868\u3002", "result": "\u5728\u8f74\u627f\u632f\u52a8\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\uff0c\u5e76\u63d0\u4f9b\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7ef4\u62a4\u6307\u5bfc\u3002\u5b83\u6210\u529f\u5f25\u5408\u4e86\u72b6\u6001\u76d1\u6d4b\u4e0e\u53ef\u64cd\u4f5c\u7ef4\u62a4\u89c4\u5212\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5de5\u4e1a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u667a\u80fd\u51b3\u7b56\u652f\u6301\uff0c\u63a8\u8fdb\u4e86LLM\u5728\u5de5\u4e1a\u7ef4\u62a4\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u5404\u7c7b\u673a\u68b0\u7ec4\u4ef6\u548c\u5de5\u4e1a\u90e8\u95e8\u3002"}}
{"id": "2508.04797", "pdf": "https://arxiv.org/pdf/2508.04797", "abs": "https://arxiv.org/abs/2508.04797", "authors": ["Mohab Kishawy", "Ali Abdellatif Hussein", "Jun Chen"], "title": "RetinexDual: Retinex-based Dual Nature Approach for Generalized Ultra-High-Definition Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Advancements in image sensing have elevated the importance of\nUltra-High-Definition Image Restoration (UHD IR). Traditional methods, such as\nextreme downsampling or transformation from the spatial to the frequency\ndomain, encounter significant drawbacks: downsampling induces irreversible\ninformation loss in UHD images, while our frequency analysis reveals that pure\nfrequency-domain approaches are ineffective for spatially confined image\nartifacts, primarily due to the loss of degradation locality. To overcome these\nlimitations, we present RetinexDual, a novel Retinex theory-based framework\ndesigned for generalized UHD IR tasks. RetinexDual leverages two complementary\nsub-networks: the Scale-Attentive maMBA (SAMBA) and the Frequency Illumination\nAdaptor (FIA). SAMBA, responsible for correcting the reflectance component,\nutilizes a coarse-to-fine mechanism to overcome the causal modeling of mamba,\nwhich effectively reduces artifacts and restores intricate details. On the\nother hand, FIA ensures precise correction of color and illumination\ndistortions by operating in the frequency domain and leveraging the global\ncontext provided by it. Evaluating RetinexDual on four UHD IR tasks, namely\nderaining, deblurring, dehazing, and Low-Light Image Enhancement (LLIE), shows\nthat it outperforms recent methods qualitatively and quantitatively. Ablation\nstudies demonstrate the importance of employing distinct designs for each\nbranch in RetinexDual, as well as the effectiveness of its various components.", "AI": {"tldr": "RetinexDual\u662f\u4e00\u79cd\u65b0\u578b\u57fa\u4e8eRetinex\u7406\u8bba\u7684\u8d85\u9ad8\u6e05\u56fe\u50cf\u6062\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u57df\u7684SAMBA\u548c\u9891\u7387\u57df\u7684FIA\u4e24\u4e2a\u4e92\u8865\u5b50\u7f51\u7edc\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4fe1\u606f\u4e22\u5931\u548c\u5c40\u90e8\u6027\u95ee\u9898\uff0c\u5728\u591a\u9879\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u8d85\u9ad8\u6e05\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\uff08\u5982\u6781\u7aef\u4e0b\u91c7\u6837\u6216\u7eaf\u9891\u7387\u57df\u53d8\u6362\uff09\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff1a\u4e0b\u91c7\u6837\u5bfc\u81f4\u4e0d\u53ef\u9006\u4fe1\u606f\u4e22\u5931\uff0c\u800c\u7eaf\u9891\u7387\u57df\u65b9\u6cd5\u56e0\u4e22\u5931\u964d\u89e3\u5c40\u90e8\u6027\u5bf9\u7a7a\u95f4\u53d7\u9650\u7684\u56fe\u50cf\u4f2a\u5f71\u65e0\u6548\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRetinexDual\u6846\u67b6\uff0c\u4e00\u4e2a\u57fa\u4e8eRetinex\u7406\u8bba\u7684\u901a\u7528\u8d85\u9ad8\u6e05\u56fe\u50cf\u6062\u590d\u65b9\u6848\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u5b50\u7f51\u7edc\uff1aScale-Attentive maMBA (SAMBA) \u548c Frequency Illumination Adaptor (FIA)\u3002SAMBA\u8d1f\u8d23\u6821\u6b63\u53cd\u5c04\u5206\u91cf\uff0c\u91c7\u7528\u4ece\u7c97\u5230\u7cbe\u7684\u673a\u5236\u5904\u7406\u7ec6\u8282\uff1bFIA\u5219\u5728\u9891\u7387\u57df\u5de5\u4f5c\uff0c\u5229\u7528\u5168\u5c40\u4e0a\u4e0b\u6587\u7cbe\u786e\u6821\u6b63\u989c\u8272\u548c\u5149\u7167\u7578\u53d8\u3002", "result": "RetinexDual\u5728\u53bb\u96e8\u3001\u53bb\u6a21\u7cca\u3001\u53bb\u96fe\u548c\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u56db\u79cd\u8d85\u9ad8\u6e05\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5b9a\u6027\u548c\u5b9a\u91cf\u6027\u80fd\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u65b0\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86RetinexDual\u4e2d\u5404\u5206\u652f\u72ec\u7acb\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u53ca\u5176\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002", "conclusion": "RetinexDual\u901a\u8fc7\u72ec\u7279\u5730\u7ed3\u5408\u7a7a\u95f4\u57df\u548c\u9891\u7387\u57df\u7684\u5904\u7406\u80fd\u529b\uff0c\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edf\u8d85\u9ad8\u6e05\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6062\u590d\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u4e3a\u5e7f\u4e49\u8d85\u9ad8\u6e05\u56fe\u50cf\u6062\u590d\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04731", "pdf": "https://arxiv.org/pdf/2508.04731", "abs": "https://arxiv.org/abs/2508.04731", "authors": ["Shruti Saxena", "Arijit Khan", "Joydeep Chandra"], "title": "NAEx: A Plug-and-Play Framework for Explaining Network Alignment", "categories": ["cs.LG", "cs.IR", "cs.SI"], "comment": null, "summary": "Network alignment (NA) identifies corresponding nodes across multiple\nnetworks, with applications in domains like social networks, co-authorship, and\nbiology. Despite advances in alignment models, their interpretability remains\nlimited, making it difficult to understand alignment decisions and posing\nchallenges in building trust, particularly in high-stakes domains. To address\nthis, we introduce NAEx, a plug-and-play, model-agnostic framework that\nexplains alignment models by identifying key subgraphs and features influencing\npredictions. NAEx addresses the key challenge of preserving the joint\ncross-network dependencies on alignment decisions by: (1) jointly\nparameterizing graph structures and feature spaces through learnable edge and\nfeature masks, and (2) introducing an optimization objective that ensures\nexplanations are both faithful to the original predictions and enable\nmeaningful comparisons of structural and feature-based similarities between\nnetworks. NAEx is an inductive framework that efficiently generates NA\nexplanations for previously unseen data. We introduce evaluation metrics\ntailored to alignment explainability and demonstrate NAEx's effectiveness and\nefficiency on benchmark datasets by integrating it with four representative NA\nmodels.", "AI": {"tldr": "NAEx\u662f\u4e00\u4e2a\u53ef\u63d2\u62d4\u3001\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u5b50\u56fe\u548c\u7279\u5f81\u6765\u89e3\u91ca\u7f51\u7edc\u5bf9\u9f50\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u7f51\u7edc\u5bf9\u9f50\u6a21\u578b\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u5176\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u96be\u4ee5\u7406\u89e3\u5bf9\u9f50\u51b3\u7b56\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u8fd9\u963b\u788d\u4e86\u4fe1\u4efb\u7684\u5efa\u7acb\u3002", "method": "\u5f15\u5165NAEx\u6846\u67b6\uff0c\u4e00\u4e2a\u53ef\u63d2\u62d4\u3001\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u5b50\u56fe\u548c\u7279\u5f81\u6765\u89e3\u91ca\u5bf9\u9f50\u6a21\u578b\u3002\u5176\u6838\u5fc3\u5728\u4e8e\uff1a1) \u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u8fb9\u548c\u7279\u5f81\u63a9\u7801\u8054\u5408\u53c2\u6570\u5316\u56fe\u7ed3\u6784\u548c\u7279\u5f81\u7a7a\u95f4\uff1b2) \u5f15\u5165\u4f18\u5316\u76ee\u6807\u4ee5\u786e\u4fdd\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\uff0c\u5e76\u4fc3\u8fdb\u7ed3\u6784\u548c\u7279\u5f81\u76f8\u4f3c\u6027\u7684\u6709\u6548\u6bd4\u8f83\u3002NAEx\u662f\u4e00\u4e2a\u5f52\u7eb3\u6846\u67b6\uff0c\u80fd\u9ad8\u6548\u751f\u6210\u672a\u89c1\u6570\u636e\u7684\u89e3\u91ca\u3002", "result": "\u5f15\u5165\u4e86\u9488\u5bf9\u5bf9\u9f50\u53ef\u89e3\u91ca\u6027\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u4e0e\u56db\u79cd\u4ee3\u8868\u6027\u7f51\u7edc\u5bf9\u9f50\u6a21\u578b\u96c6\u6210\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86NAEx\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u3002", "conclusion": "NAEx\u6846\u67b6\u6210\u529f\u63d0\u5347\u4e86\u7f51\u7edc\u5bf9\u9f50\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002"}}
{"id": "2508.04814", "pdf": "https://arxiv.org/pdf/2508.04814", "abs": "https://arxiv.org/abs/2508.04814", "authors": ["David Sasu", "Natalie Schluter"], "title": "Pitch Accent Detection improves Pretrained Automatic Speech Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "We show the performance of Automatic Speech Recognition (ASR) systems that\nuse semi-supervised speech representations can be boosted by a complimentary\npitch accent detection module, by introducing a joint ASR and pitch accent\ndetection model. The pitch accent detection component of our model achieves a\nsignificant improvement on the state-of-the-art for the task, closing the gap\nin F1-score by 41%. Additionally, the ASR performance in joint training\ndecreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With\nthese results, we show the importance of extending pretrained speech models to\nretain or re-learn important prosodic cues such as pitch accent.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u8054\u5408ASR\u548c\u91cd\u97f3\u68c0\u6d4b\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u91cd\u97f3\u68c0\u6d4b\u6a21\u5757\u53ef\u4ee5\u663e\u8457\u63d0\u5347ASR\u6027\u80fd\uff0c\u5e76\u5f3a\u8c03\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u4fdd\u7559\u97f5\u5f8b\u7ebf\u7d22\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u65e8\u5728\u63d0\u5347\u4f7f\u7528\u534a\u76d1\u7763\u8bed\u97f3\u8868\u793a\u7684\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u7d22\u4e92\u8865\u7684\u91cd\u97f3\u68c0\u6d4b\u6a21\u5757\u5bf9\u5176\u7684\u589e\u76ca\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u8054\u5408ASR\u548c\u91cd\u97f3\u68c0\u6d4b\u6a21\u578b\u3002", "result": "\u91cd\u97f3\u68c0\u6d4b\u7ec4\u4ef6\u5728F1\u5206\u6570\u4e0a\u53d6\u5f97\u4e8641%\u7684\u663e\u8457\u63d0\u5347\uff0c\u63a5\u8fd1\u4e86\u73b0\u6709\u6700\u4f73\u6c34\u5e73\u3002\u8054\u5408\u8bad\u7ec3\u4f7fASR\u5728LibriSpeech\u6570\u636e\u96c6\u4e0a\u7684\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u964d\u4f4e\u4e8628.3%\uff08\u5728\u6709\u9650\u8d44\u6e90\u5fae\u8c03\u4e0b\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6269\u5c55\u9884\u8bad\u7ec3\u8bed\u97f3\u6a21\u578b\u4ee5\u4fdd\u7559\u6216\u91cd\u65b0\u5b66\u4e60\u91cd\u8981\u7684\u97f5\u5f8b\u7ebf\u7d22\uff08\u5982\u91cd\u97f3\uff09\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.04719", "pdf": "https://arxiv.org/pdf/2508.04719", "abs": "https://arxiv.org/abs/2508.04719", "authors": ["Amulya Bhattaram", "Justin Chung", "Stanley Chung", "Ranit Gupta", "Janani Ramamoorthy", "Kartikeya Gullapalli", "Diana Marculescu", "Dimitrios Stamoulis"], "title": "GeoFlow: Agentic Workflow Automation for Geospatial Tasks", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to ACM SIGSPATIAL 2025", "summary": "We present GeoFlow, a method that automatically generates agentic workflows\nfor geospatial tasks. Unlike prior work that focuses on reasoning decomposition\nand leaves API selection implicit, our method provides each agent with detailed\ntool-calling objectives to guide geospatial API invocation at runtime. GeoFlow\nincreases agentic success by 6.8% and reduces token usage by up to fourfold\nacross major LLM families compared to state-of-the-art approaches.", "AI": {"tldr": "GeoFlow\u662f\u4e00\u79cd\u4e3a\u5730\u7406\u7a7a\u95f4\u4efb\u52a1\u81ea\u52a8\u751f\u6210\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u660e\u786e\u7684\u5de5\u5177\u8c03\u7528\u76ee\u6807\uff0c\u6709\u6548\u63d0\u5347\u667a\u80fd\u4f53\u6210\u529f\u7387\u5e76\u964d\u4f4etoken\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u63a8\u7406\u5206\u89e3\uff0c\u4f46\u5730\u7406\u7a7a\u95f4API\u9009\u62e9\u4e0d\u591f\u660e\u786e\uff0c\u9650\u5236\u4e86\u667a\u80fd\u4f53\u5728\u5730\u7406\u7a7a\u95f4\u4efb\u52a1\u4e2d\u7684\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "method": "GeoFlow\u4e3a\u6bcf\u4e2a\u667a\u80fd\u4f53\u63d0\u4f9b\u8be6\u7ec6\u7684\u5de5\u5177\u8c03\u7528\u76ee\u6807\uff0c\u4ee5\u5728\u8fd0\u884c\u65f6\u660e\u786e\u6307\u5bfc\u5730\u7406\u7a7a\u95f4API\u7684\u8c03\u7528\uff0c\u4ece\u800c\u81ea\u52a8\u751f\u6210\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0cGeoFlow\u5c06\u667a\u80fd\u4f53\u6210\u529f\u7387\u63d0\u9ad8\u4e866.8%\uff0c\u5e76\u5728\u4e3b\u8981LLM\u5bb6\u65cf\u4e2d\u5c06token\u4f7f\u7528\u91cf\u51cf\u5c11\u4e86\u591a\u8fbe\u56db\u500d\u3002", "conclusion": "GeoFlow\u901a\u8fc7\u5176\u72ec\u7279\u7684\u5de5\u5177\u8c03\u7528\u76ee\u6807\u5f15\u5bfc\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5730\u7406\u7a7a\u95f4\u4efb\u52a1\u4e2d\u667a\u80fd\u4f53\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2508.04801", "pdf": "https://arxiv.org/pdf/2508.04801", "abs": "https://arxiv.org/abs/2508.04801", "authors": ["Trong-Thuan Nguyen", "Viet-Tham Huynh", "Thao Thi Phuong Dao", "Ha Nguyen Thi", "Tien To Vu Thuy", "Uyen Hanh Tran", "Tam V. Nguyen", "Thanh Dinh Le", "Minh-Triet Tran"], "title": "ACM Multimedia Grand Challenge on ENT Endoscopy Analysis", "categories": ["cs.CV"], "comment": null, "summary": "Automated analysis of endoscopic imagery is a critical yet underdeveloped\ncomponent of ENT (ear, nose, and throat) care, hindered by variability in\ndevices and operators, subtle and localized findings, and fine-grained\ndistinctions such as laterality and vocal-fold state. In addition to\nclassification, clinicians require reliable retrieval of similar cases, both\nvisually and through concise textual descriptions. These capabilities are\nrarely supported by existing public benchmarks. To this end, we introduce\nENTRep, the ACM Multimedia 2025 Grand Challenge on ENT endoscopy analysis,\nwhich integrates fine-grained anatomical classification with image-to-image and\ntext-to-image retrieval under bilingual (Vietnamese and English) clinical\nsupervision. Specifically, the dataset comprises expert-annotated images,\nlabeled for anatomical region and normal or abnormal status, and accompanied by\ndual-language narrative descriptions. In addition, we define three benchmark\ntasks, standardize the submission protocol, and evaluate performance on public\nand private test splits using server-side scoring. Moreover, we report results\nfrom the top-performing teams and provide an insight discussion.", "AI": {"tldr": "\u5f15\u5165\u4e00\u4e2a\u7528\u4e8e\u8033\u9f3b\u5589\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u6790\u7684\u65b0\u578b\u53cc\u8bed\u57fa\u51c6\u6311\u6218\u8d5bENTRep\uff0c\u4ee5\u4fc3\u8fdb\u7cbe\u7ec6\u5206\u7c7b\u548c\u68c0\u7d22\u80fd\u529b\u3002", "motivation": "\u8033\u9f3b\u5589\uff08ENT\uff09\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u81ea\u52a8\u5316\u5206\u6790\u76ee\u524d\u5c1a\u4e0d\u6210\u719f\uff0c\u9762\u4e34\u8bbe\u5907\u3001\u64cd\u4f5c\u8005\u5dee\u5f02\u3001\u7ec6\u5fae\u53d1\u73b0\u53ca\u7cbe\u7ec6\u533a\u5206\uff08\u5982\u5de6\u53f3\u4fa7\u3001\u58f0\u5e26\u72b6\u6001\uff09\u7b49\u6311\u6218\u3002\u4e34\u5e8a\u533b\u751f\u9700\u8981\u53ef\u9760\u7684\u75c5\u4f8b\u5206\u7c7b\u4e0e\u68c0\u7d22\u529f\u80fd\uff0c\u4f46\u73b0\u6709\u516c\u5171\u57fa\u51c6\u65e0\u6cd5\u5145\u5206\u652f\u6301\u8fd9\u4e9b\u80fd\u529b\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86ACM Multimedia 2025 ENT\u5185\u7aa5\u955c\u5206\u6790\u5927\u6311\u6218\u8d5bENTRep\uff0c\u8be5\u6311\u6218\u6574\u5408\u4e86\u7cbe\u7ec6\u89e3\u5256\u5206\u7c7b\u3001\u56fe\u50cf\u5230\u56fe\u50cf\u68c0\u7d22\u548c\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\uff0c\u5e76\u7531\u53cc\u8bed\uff08\u8d8a\u5357\u8bed\u548c\u82f1\u8bed\uff09\u4e34\u5e8a\u4e13\u5bb6\u76d1\u7763\u3002\u4e3a\u6b64\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4e13\u5bb6\u6807\u6ce8\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u56fe\u50cf\u88ab\u6807\u8bb0\u4e86\u89e3\u5256\u533a\u57df\u3001\u6b63\u5e38/\u5f02\u5e38\u72b6\u6001\uff0c\u5e76\u9644\u6709\u53cc\u8bed\u53d9\u8ff0\u6027\u63cf\u8ff0\u3002\u540c\u65f6\uff0c\u5b9a\u4e49\u4e86\u4e09\u9879\u57fa\u51c6\u4efb\u52a1\uff0c\u6807\u51c6\u5316\u4e86\u63d0\u4ea4\u534f\u8bae\uff0c\u5e76\u91c7\u7528\u670d\u52a1\u5668\u7aef\u8bc4\u5206\u5bf9\u516c\u5f00\u548c\u79c1\u6709\u6d4b\u8bd5\u96c6\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u62a5\u544a\u4e86\u8868\u73b0\u6700\u4f73\u53c2\u8d5b\u56e2\u961f\u7684\u7ed3\u679c\uff0c\u5e76\u63d0\u4f9b\u4e86\u6df1\u5165\u7684\u8ba8\u8bba\u3002", "conclusion": "ENTRep\u4f5c\u4e3a\u4e00\u4e2a\u5168\u9762\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\uff0c\u65e8\u5728\u63a8\u52a8\u8033\u9f3b\u5589\u5185\u7aa5\u955c\u56fe\u50cf\u7684\u81ea\u52a8\u5316\u7cbe\u7ec6\u5206\u6790\uff0c\u5e76\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u5206\u7c7b\u548c\u68c0\u7d22\u80fd\u529b\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2508.04732", "pdf": "https://arxiv.org/pdf/2508.04732", "abs": "https://arxiv.org/abs/2508.04732", "authors": ["Xiaoqi Dong", "Xiangyu Zhou", "Nicholas Evans", "Yujia Lin"], "title": "LumiGen: An LVLM-Enhanced Iterative Framework for Fine-Grained Text-to-Image Generation", "categories": ["cs.LG", "cs.GR"], "comment": null, "summary": "Text-to-Image (T2I) generation has made significant advancements with\ndiffusion models, yet challenges persist in handling complex instructions,\nensuring fine-grained content control, and maintaining deep semantic\nconsistency. Existing T2I models often struggle with tasks like accurate text\nrendering, precise pose generation, or intricate compositional coherence.\nConcurrently, Vision-Language Models (LVLMs) have demonstrated powerful\ncapabilities in cross-modal understanding and instruction following. We propose\nLumiGen, a novel LVLM-enhanced iterative framework designed to elevate T2I\nmodel performance, particularly in areas requiring fine-grained control,\nthrough a closed-loop, LVLM-driven feedback mechanism. LumiGen comprises an\nIntelligent Prompt Parsing & Augmentation (IPPA) module for proactive prompt\nenhancement and an Iterative Visual Feedback & Refinement (IVFR) module, which\nacts as a \"visual critic\" to iteratively correct and optimize generated images.\nEvaluated on the challenging LongBench-T2I Benchmark, LumiGen achieves a\nsuperior average score of 3.08, outperforming state-of-the-art baselines.\nNotably, our framework demonstrates significant improvements in critical\ndimensions such as text rendering and pose expression, validating the\neffectiveness of LVLM integration for more controllable and higher-quality\nimage generation.", "AI": {"tldr": "\u63d0\u51faLumiGen\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(LVLM)\u7684\u8fed\u4ee3\u53cd\u9988\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u6587\u751f\u56fe(T2I)\u6a21\u578b\u5728\u7cbe\u7ec6\u63a7\u5236\u3001\u6587\u672c\u6e32\u67d3\u548c\u59ff\u6001\u751f\u6210\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u751f\u56fe(T2I)\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u6307\u4ee4\u3001\u7cbe\u7ec6\u5185\u5bb9\u63a7\u5236\u548c\u6df1\u5c42\u8bed\u4e49\u4e00\u81f4\u6027\uff08\u5982\u51c6\u786e\u6587\u672c\u6e32\u67d3\u3001\u7cbe\u786e\u59ff\u6001\u751f\u6210\u548c\u590d\u6742\u6784\u56fe\u8fde\u8d2f\u6027\uff09\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(LVLMs)\u5728\u8de8\u6a21\u6001\u7406\u89e3\u548c\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\u3002", "method": "\u63d0\u51faLumiGen\uff0c\u4e00\u4e2aLVLM\u589e\u5f3a\u578b\u8fed\u4ee3\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u95ed\u73af\u3001LVLM\u9a71\u52a8\u7684\u53cd\u9988\u673a\u5236\u63d0\u5347T2I\u6027\u80fd\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a\u667a\u80fd\u63d0\u793a\u89e3\u6790\u4e0e\u589e\u5f3a(IPPA)\u6a21\u5757\uff0c\u7528\u4e8e\u4e3b\u52a8\u4f18\u5316\u63d0\u793a\u8bcd\uff1b\u4ee5\u53ca\u8fed\u4ee3\u89c6\u89c9\u53cd\u9988\u4e0e\u4f18\u5316(IVFR)\u6a21\u5757\uff0c\u4f5c\u4e3a\u201c\u89c6\u89c9\u8bc4\u8bba\u5458\u201d\u8fed\u4ee3\u6821\u6b63\u548c\u4f18\u5316\u751f\u6210\u56fe\u50cf\u3002", "result": "\u5728LongBench-T2I\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLumiGen\u53d6\u5f973.08\u7684\u5353\u8d8a\u5e73\u5747\u5206\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\u3002\u8be5\u6846\u67b6\u5728\u6587\u672c\u6e32\u67d3\u548c\u59ff\u6001\u8868\u8fbe\u7b49\u5173\u952e\u7ef4\u5ea6\u4e0a\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u9a8c\u8bc1\u4e86LVLM\u96c6\u6210\u5728\u5b9e\u73b0\u66f4\u53ef\u63a7\u3001\u66f4\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u63d0\u5347\u6587\u751f\u56fe\u6a21\u578b\u7684\u7cbe\u7ec6\u63a7\u5236\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.04826", "pdf": "https://arxiv.org/pdf/2508.04826", "abs": "https://arxiv.org/abs/2508.04826", "authors": ["Tommaso Tosato", "Saskia Helbling", "Yorguin-Jose Mantilla-Ramos", "Mahmood Hegazy", "Alberto Tosato", "David John Lemay", "Irina Rish", "Guillaume Dumas"], "title": "Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models require consistent behavioral patterns for safe\ndeployment, yet their personality-like traits remain poorly understood. We\npresent PERSIST (PERsonality Stability in Synthetic Text), a comprehensive\nevaluation framework testing 25+ open-source models (1B-671B parameters) across\n500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted\npersonality instruments, we systematically vary question order, paraphrasing,\npersonas, and reasoning modes. Our findings challenge fundamental deployment\nassumptions: (1) Even 400B+ models exhibit substantial response variability (SD\n> 0.4); (2) Minor prompt reordering alone shifts personality measurements by up\nto 20%; (3) Interventions expected to stabilize behavior, such as\nchain-of-thought reasoning, detailed personas instruction, inclusion of\nconversation history, can paradoxically increase variability; (4) LLM-adapted\ninstruments show equal instability to human-centric versions, confirming\narchitectural rather than translational limitations. This persistent\ninstability across scales and mitigation strategies suggests current LLMs lack\nthe foundations for genuine behavioral consistency. For safety-critical\napplications requiring predictable behavior, these findings indicate that\npersonality-based alignment strategies may be fundamentally inadequate.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5373\u4f7f\u5728\u89c4\u6a21\u548c\u5e72\u9884\u63aa\u65bd\u4e0b\uff0c\u5176\u884c\u4e3a\uff08\u4eba\u683c\uff09\u7a33\u5b9a\u6027\u4ecd\u7136\u4e0d\u8db3\uff0c\u63d0\u793a\u57fa\u4e8e\u4eba\u683c\u7684\u5bf9\u9f50\u7b56\u7565\u5bf9\u5b89\u5168\u5173\u952e\u5e94\u7528\u53ef\u80fd\u65e0\u6548\u3002", "motivation": "\u4e3a\u786e\u4fdd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u90e8\u7f72\uff0c\u5176\u884c\u4e3a\u6a21\u5f0f\u9700\u8981\u4fdd\u6301\u4e00\u81f4\u6027\uff0c\u4f46\u76ee\u524d\u5bf9\u8fd9\u4e9b\u6a21\u578b\u7c7b\u4eba\u683c\u7279\u5f81\u7684\u7a33\u5b9a\u6027\u77e5\u4e4b\u751a\u5c11\u3002", "method": "\u63d0\u51fa\u4e86PERSIST\uff08\u6587\u672c\u4e2d\u4eba\u683c\u7a33\u5b9a\u6027\uff09\u8bc4\u4f30\u6846\u67b6\uff0c\u5bf925+\u4e2a\u5f00\u6e90\u6a21\u578b\uff081B-671B\u53c2\u6570\uff09\u8fdb\u884c\u4e8650\u4e07+\u54cd\u5e94\u7684\u5168\u9762\u6d4b\u8bd5\u3002\u7814\u7a76\u91c7\u7528\u4e86\u4f20\u7edf\uff08BFI-44, SD3\uff09\u548c\u65b0\u9896\u7684LLM\u9002\u914d\u4eba\u683c\u91cf\u8868\uff0c\u5e76\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u4e86\u95ee\u9898\u987a\u5e8f\u3001\u590d\u8ff0\u3001\u89d2\u8272\u548c\u63a8\u7406\u6a21\u5f0f\u3002", "result": "1. \u5373\u4f7f400B+\u7684\u6a21\u578b\u4e5f\u8868\u73b0\u51fa\u663e\u8457\u7684\u54cd\u5e94\u53d8\u5f02\u6027\uff08\u6807\u51c6\u5dee>0.4\uff09\u30022. \u4ec5\u5fae\u5c0f\u7684\u63d0\u793a\u8bcd\u91cd\u6392\u5c31\u80fd\u4f7f\u4eba\u683c\u6d4b\u91cf\u7ed3\u679c\u504f\u79fb\u9ad8\u8fbe20%\u30023. \u65e8\u5728\u7a33\u5b9a\u884c\u4e3a\u7684\u5e72\u9884\u63aa\u65bd\uff08\u5982\u601d\u7ef4\u94fe\u63a8\u7406\u3001\u8be6\u7ec6\u89d2\u8272\u6307\u4ee4\u3001\u5305\u542b\u5bf9\u8bdd\u5386\u53f2\uff09\u53cd\u800c\u53ef\u80fd\u589e\u52a0\u53d8\u5f02\u6027\u30024. LLM\u9002\u914d\u7684\u4eba\u683c\u6d4b\u91cf\u5de5\u5177\u4e0e\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u7248\u672c\u8868\u73b0\u51fa\u540c\u7b49\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u8bc1\u5b9e\u4e86\u8fd9\u662f\u67b6\u6784\u800c\u975e\u8f6c\u6362\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8fd9\u79cd\u8de8\u89c4\u6a21\u548c\u7f13\u89e3\u7b56\u7565\u7684\u6301\u7eed\u4e0d\u7a33\u5b9a\u6027\u8868\u660e\uff0c\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u771f\u6b63\u7684\u884c\u4e3a\u4e00\u81f4\u6027\u57fa\u7840\u3002\u5bf9\u4e8e\u9700\u8981\u53ef\u9884\u6d4b\u884c\u4e3a\u7684\u5b89\u5168\u5173\u952e\u5e94\u7528\uff0c\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\u57fa\u4e8e\u4eba\u683c\u7684\u5bf9\u9f50\u7b56\u7565\u53ef\u80fd\u4ece\u6839\u672c\u4e0a\u662f\u4e0d\u8db3\u7684\u3002"}}
{"id": "2508.04720", "pdf": "https://arxiv.org/pdf/2508.04720", "abs": "https://arxiv.org/abs/2508.04720", "authors": ["Yingjie Zhou", "Jiezhang Cao", "Farong Wen", "Li Xu", "Yanwei Jiang", "Jun Jia", "Ronghui Li", "Xiaohong Liu", "Yu Zhou", "Xiongkuo Min", "Jie Guo", "Zicheng Zhang", "Guangtao Zhai"], "title": "Who is a Better Player: LLM against LLM", "categories": ["cs.AI"], "comment": null, "summary": "Adversarial board games, as a paradigmatic domain of strategic reasoning and\nintelligence, have long served as both a popular competitive activity and a\nbenchmark for evaluating artificial intelligence (AI) systems. Building on this\nfoundation, we propose an adversarial benchmarking framework to assess the\ncomprehensive performance of Large Language Models (LLMs) through board games\ncompetition, compensating the limitation of data dependency of the mainstream\nQuestion-and-Answer (Q&A) based benchmark method. We introduce Qi Town, a\nspecialized evaluation platform that supports 5 widely played games and\ninvolves 20 LLM-driven players. The platform employs both the Elo rating system\nand a novel Performance Loop Graph (PLG) to quantitatively evaluate the\ntechnical capabilities of LLMs, while also capturing Positive Sentiment Score\n(PSS) throughout gameplay to assess mental fitness. The evaluation is\nstructured as a round-robin tournament, enabling systematic comparison across\nplayers. Experimental results indicate that, despite technical differences,\nmost LLMs remain optimistic about winning and losing, demonstrating greater\nadaptability to high-stress adversarial environments than humans. On the other\nhand, the complex relationship between cyclic wins and losses in PLGs exposes\nthe instability of LLMs' skill play during games, warranting further\nexplanation and exploration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5bf9\u6297\u6027\u68cb\u76d8\u6e38\u620f\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7efc\u5408\u6027\u80fd\uff0c\u5f25\u8865\u4e86\u4f20\u7edf\u95ee\u7b54\u57fa\u51c6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4e3b\u6d41\u7684\u57fa\u4e8e\u95ee\u7b54\uff08Q&A\uff09\u7684LLM\u57fa\u51c6\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u4f9d\u8d56\u6027\u9650\u5236\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5bf9\u6297\u6027\u68cb\u76d8\u6e38\u620f\u6bd4\u8d5b\uff0c\u66f4\u5168\u9762\u3001\u7efc\u5408\u5730\u8bc4\u4f30LLMs\u7684\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86\u201c\u68cb\u9547\u201d\uff08Qi Town\uff09\u8bc4\u4f30\u5e73\u53f0\uff0c\u652f\u63015\u6b3e\u68cb\u76d8\u6e38\u620f\u548c20\u4e2aLLM\u9a71\u52a8\u7684\u73a9\u5bb6\u3002\u91c7\u7528Elo\u8bc4\u5206\u7cfb\u7edf\u548c\u65b0\u9896\u7684\u6027\u80fd\u5faa\u73af\u56fe\uff08PLG\uff09\u6765\u91cf\u5316LLM\u7684\u6280\u672f\u80fd\u529b\uff0c\u540c\u65f6\u901a\u8fc7\u79ef\u6781\u60c5\u7eea\u5206\u6570\uff08PSS\uff09\u8bc4\u4f30\u5fc3\u7406\u7d20\u8d28\u3002\u8bc4\u4f30\u91c7\u7528\u5faa\u73af\u8d5b\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cLLMs\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u666e\u904d\u4fdd\u6301\u4e50\u89c2\uff0c\u5e76\u8868\u73b0\u51fa\u6bd4\u4eba\u7c7b\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u3002\u7136\u800c\uff0cPLG\u63ed\u793a\u4e86LLMs\u5728\u6e38\u620f\u8fc7\u7a0b\u4e2d\u6280\u80fd\u8868\u73b0\u7684\u4e0d\u7a33\u5b9a\u6027\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u89e3\u91ca\u548c\u63a2\u7d22\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63ed\u793a\u4e86LLMs\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u7684\u4f18\u52bf\u548c\u4e0d\u8db3\uff0c\u4e3aLLM\u7684\u7efc\u5408\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002\u540c\u65f6\uff0cLLM\u6280\u80fd\u8868\u73b0\u7684\u4e0d\u7a33\u5b9a\u6027\u662f\u672a\u6765\u7814\u7a76\u7684\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2508.04816", "pdf": "https://arxiv.org/pdf/2508.04816", "abs": "https://arxiv.org/abs/2508.04816", "authors": ["Sriram Mandalika", "Lalitha V"], "title": "CoMAD: A Multiple-Teacher Self-Supervised Distillation Framework", "categories": ["cs.CV", "cs.AI"], "comment": "8 Pages, 2 Figures", "summary": "Numerous self-supervised learning paradigms, such as contrastive learning and\nmasked image modeling, learn powerful representations from unlabeled data but\nare typically pretrained in isolation, overlooking complementary insights and\nyielding large models that are impractical for resource-constrained deployment.\nTo overcome these challenges, we introduce Consensus-oriented Masked\nDistillation (CoMAD), a lightweight, parameter-free framework that unifies\nknowledge from multiple current state-of-the-art self-supervised Vision\nTransformers into a compact student network. CoMAD distills from three\npretrained ViT-Base teachers, MAE, MoCo v3, and iBOT, each offering distinct\nsemantic and contextual priors. Rather than naively averaging teacher outputs,\nwe apply asymmetric masking: the student sees only 25 percent of patches while\neach teacher receives a progressively lighter, unique mask, forcing the student\nto interpolate missing features under richer contexts. Teacher embeddings are\naligned to the student's space via a linear adapter and layer normalization,\nthen fused through our joint consensus gating, which weights each token by\ncombining cosine affinity with inter-teacher agreement. The student is trained\nwith dual-level KL divergence on visible tokens and reconstructed feature maps,\ncapturing both local and global structure. On ImageNet-1K, CoMAD's ViT-Tiny\nachieves 75.4 percent Top-1, an increment of 0.4 percent over the previous\nstate-of-the-art. In dense-prediction transfers, it attains 47.3 percent mIoU\non ADE20K, and 44.5 percent box average precision and 40.5 percent mask average\nprecision on MS-COCO, establishing a new state-of-the-art in compact SSL\ndistillation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCoMAD\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u65e0\u53c2\u6570\u7684\u6846\u67b6\uff0c\u65e8\u5728\u5c06\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u81ea\u76d1\u7763Vision Transformer\u7684\u77e5\u8bc6\u7edf\u4e00\u5230\u7d27\u51d1\u7684\u5b66\u751f\u7f51\u7edc\u4e2d\uff0c\u901a\u8fc7\u4e0d\u5bf9\u79f0\u63a9\u7801\u548c\u8054\u5408\u5171\u8bc6\u95e8\u63a7\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\uff08\u5982\u5bf9\u6bd4\u5b66\u4e60\u548c\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\uff09\u901a\u5e38\u72ec\u7acb\u9884\u8bad\u7ec3\uff0c\u5ffd\u89c6\u4e86\u4e92\u8865\u7684\u89c1\u89e3\uff0c\u5e76\u4ea7\u751f\u5927\u578b\u6a21\u578b\uff0c\u4e0d\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u90e8\u7f72\u3002", "method": "CoMAD\u4eceMAE\u3001MoCo v3\u548ciBOT\u4e09\u4e2a\u9884\u8bad\u7ec3\u7684ViT-Base\u6559\u5e08\u6a21\u578b\u4e2d\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u3002\u91c7\u7528\u4e0d\u5bf9\u79f0\u63a9\u7801\u7b56\u7565\uff0c\u5b66\u751f\u7f51\u7edc\u4ec5\u770b\u523025%\u7684\u56fe\u50cf\u5757\uff0c\u800c\u6bcf\u4e2a\u6559\u5e08\u63a5\u6536\u4e0d\u540c\u7a0b\u5ea6\u7684\u8f7b\u91cf\u7ea7\u72ec\u7279\u63a9\u7801\u3002\u6559\u5e08\u5d4c\u5165\u901a\u8fc7\u7ebf\u6027\u9002\u914d\u5668\u548c\u5c42\u5f52\u4e00\u5316\u4e0e\u5b66\u751f\u7a7a\u95f4\u5bf9\u9f50\u3002\u901a\u8fc7\u7ed3\u5408\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u6559\u5e08\u95f4\u4e00\u81f4\u6027\u7684\u8054\u5408\u5171\u8bc6\u95e8\u63a7\u6765\u878d\u5408\u6559\u5e08\u8f93\u51fa\u3002\u5b66\u751f\u6a21\u578b\u901a\u8fc7\u5728\u53ef\u89c1\u4ee4\u724c\u548c\u91cd\u5efa\u7279\u5f81\u56fe\u4e0a\u7684\u53cc\u7ea7KL\u6563\u5ea6\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728ImageNet-1K\u4e0a\uff0cCoMAD\u7684ViT-Tiny\u6a21\u578b\u8fbe\u5230\u4e8675.4%\u7684Top-1\u51c6\u786e\u7387\uff0c\u6bd4\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u63d0\u9ad8\u4e860.4%\u3002\u5728\u5bc6\u96c6\u9884\u6d4b\u8fc1\u79fb\u4efb\u52a1\u4e2d\uff0c\u5728ADE20K\u4e0a\u5b9e\u73b0\u4e8647.3%\u7684mIoU\uff0c\u5728MS-COCO\u4e0a\u5b9e\u73b0\u4e8644.5%\u7684box\u5e73\u5747\u7cbe\u5ea6\u548c40.5%\u7684mask\u5e73\u5747\u7cbe\u5ea6\uff0c\u5728\u7d27\u51d1\u578bSSL\u84b8\u998f\u9886\u57df\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "CoMAD\u6210\u529f\u5730\u5c06\u6765\u81ea\u591a\u4e2a\u81ea\u76d1\u7763ViT\u6559\u5e08\u7684\u77e5\u8bc6\u7edf\u4e00\u5230\u4e00\u4e2a\u7d27\u51d1\u7684\u5b66\u751f\u7f51\u7edc\u4e2d\uff0c\u514b\u670d\u4e86\u73b0\u6709\u5927\u578b\u6a21\u578b\u90e8\u7f72\u53d7\u9650\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u89c6\u89c9\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86SOTA\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7d27\u51d1SSL\u84b8\u998f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.04740", "pdf": "https://arxiv.org/pdf/2508.04740", "abs": "https://arxiv.org/abs/2508.04740", "authors": ["Youran Zhou", "Mohamed Reda Bouadjenek", "Sunil Aryal"], "title": "MissMecha: An All-in-One Python Package for Studying Missing Data Mechanisms", "categories": ["cs.LG", "cs.MS"], "comment": null, "summary": "Incomplete data is a persistent challenge in real-world datasets, often\ngoverned by complex and unobservable missing mechanisms. Simulating missingness\nhas become a standard approach for understanding its impact on learning and\nanalysis. However, existing tools are fragmented, mechanism-limited, and\ntypically focus only on numerical variables, overlooking the heterogeneous\nnature of real-world tabular data. We present MissMecha, an open-source Python\ntoolkit for simulating, visualizing, and evaluating missing data under MCAR,\nMAR, and MNAR assumptions. MissMecha supports both numerical and categorical\nfeatures, enabling mechanism-aware studies across mixed-type tabular datasets.\nIt includes visual diagnostics, MCAR testing utilities, and type-aware\nimputation evaluation metrics. Designed to support data quality research,\nbenchmarking, and education,MissMecha offers a unified platform for researchers\nand practitioners working with incomplete data.", "AI": {"tldr": "MissMecha\u662f\u4e00\u4e2a\u5f00\u6e90Python\u5de5\u5177\u5305\uff0c\u65e8\u5728\u6a21\u62df\u3001\u53ef\u89c6\u5316\u548c\u8bc4\u4f30\u4e0d\u540c\u7f3a\u5931\u673a\u5236\u4e0b\u7684\u6df7\u5408\u7c7b\u578b\u8868\u683c\u6570\u636e\u4e2d\u7684\u4e0d\u5b8c\u6574\u6570\u636e\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u4e0d\u5b8c\u6574\u6570\u636e\u662f\u6301\u7eed\u6027\u6311\u6218\uff0c\u73b0\u6709\u6a21\u62df\u5de5\u5177\u788e\u7247\u5316\u3001\u673a\u5236\u53d7\u9650\u4e14\u4ec5\u5173\u6ce8\u6570\u503c\u53d8\u91cf\uff0c\u5ffd\u7565\u4e86\u5f02\u6784\u8868\u683c\u6570\u636e\u7279\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u5f00\u6e90Python\u5de5\u5177\u5305MissMecha\uff0c\u652f\u6301MCAR\u3001MAR\u548cMNAR\u5047\u8bbe\u4e0b\u7684\u7f3a\u5931\u6570\u636e\u6a21\u62df\u3001\u53ef\u89c6\u5316\u548c\u8bc4\u4f30\uff0c\u53ef\u5904\u7406\u6570\u503c\u548c\u5206\u7c7b\u7279\u5f81\uff0c\u5e76\u5305\u542b\u89c6\u89c9\u8bca\u65ad\u3001MCAR\u6d4b\u8bd5\u548c\u7c7b\u578b\u611f\u77e5\u63d2\u8865\u8bc4\u4f30\u6307\u6807\u3002", "result": "MissMecha\u80fd\u591f\u652f\u6301\u6df7\u5408\u7c7b\u578b\u8868\u683c\u6570\u636e\u7684\u673a\u5236\u611f\u77e5\u7814\u7a76\uff0c\u63d0\u4f9b\u89c6\u89c9\u8bca\u65ad\u3001MCAR\u6d4b\u8bd5\u5b9e\u7528\u7a0b\u5e8f\u548c\u7c7b\u578b\u611f\u77e5\u7684\u63d2\u8865\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "MissMecha\u4e3a\u5904\u7406\u4e0d\u5b8c\u6574\u6570\u636e\u7684\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u5e73\u53f0\uff0c\u65e8\u5728\u652f\u6301\u6570\u636e\u8d28\u91cf\u7814\u7a76\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u6559\u80b2\u3002"}}
{"id": "2508.04903", "pdf": "https://arxiv.org/pdf/2508.04903", "abs": "https://arxiv.org/abs/2508.04903", "authors": ["Jun Liu", "Zhenglun Kong", "Changdi Yang", "Fan Yang", "Tianqi Li", "Peiyan Dong", "Joannah Nanjekye", "Hao Tang", "Geng Yuan", "Wei Niu", "Wenbin Zhang", "Pu Zhao", "Xue Lin", "Dong Huang", "Yanzhi Wang"], "title": "RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent large language model (LLM) systems have shown strong potential in\ncomplex reasoning and collaborative decision-making tasks. However, most\nexisting coordination schemes rely on static or full-context routing\nstrategies, which lead to excessive token consumption, redundant memory\nexposure, and limited adaptability across interaction rounds. We introduce\nRCR-Router, a modular and role-aware context routing framework designed to\nenable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge,\nthis is the first routing approach that dynamically selects semantically\nrelevant memory subsets for each agent based on its role and task stage, while\nadhering to a strict token budget. A lightweight scoring policy guides memory\nselection, and agent outputs are iteratively integrated into a shared memory\nstore to facilitate progressive context refinement. To better evaluate model\nbehavior, we further propose an Answer Quality Score metric that captures\nLLM-generated explanations beyond standard QA accuracy. Experiments on three\nmulti-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate\nthat RCR-Router reduces token usage (up to 30%) while improving or maintaining\nanswer quality. These results highlight the importance of structured memory\nrouting and output-aware evaluation in advancing scalable multi-agent LLM\nsystems.", "AI": {"tldr": "\u63d0\u51faRCR-Router\uff0c\u4e00\u4e2a\u52a8\u6001\u4e0a\u4e0b\u6587\u8def\u7531\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u4e2d\u9ad8\u6548\u534f\u4f5c\uff0c\u663e\u8457\u51cf\u5c11token\u6d88\u8017\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53LLM\u534f\u4f5c\u65b9\u6848\u4f9d\u8d56\u9759\u6001\u6216\u5168\u4e0a\u4e0b\u6587\u8def\u7531\uff0c\u5bfc\u81f4token\u6d88\u8017\u8fc7\u591a\u3001\u8bb0\u5fc6\u5197\u4f59\u66b4\u9732\u53ca\u4ea4\u4e92\u8f6e\u6b21\u9002\u5e94\u6027\u5dee\u3002", "method": "\u5f15\u5165RCR-Router\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bc4\u5206\u7b56\u7565\uff0c\u57fa\u4e8e\u667a\u80fd\u4f53\u89d2\u8272\u548c\u4efb\u52a1\u9636\u6bb5\u52a8\u6001\u9009\u62e9\u8bed\u4e49\u76f8\u5173\u7684\u8bb0\u5fc6\u5b50\u96c6\uff0c\u4e25\u683c\u63a7\u5236token\u9884\u7b97\uff0c\u5e76\u5c06\u667a\u80fd\u4f53\u8f93\u51fa\u8fed\u4ee3\u6574\u5408\u5230\u5171\u4eab\u8bb0\u5fc6\u4e2d\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u201c\u7b54\u6848\u8d28\u91cf\u5206\u6570\u201d\u8bc4\u4f30\u6307\u6807\u4ee5\u66f4\u5168\u9762\u8861\u91cf\u6a21\u578b\u8868\u73b0\u3002", "result": "\u5728\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRCR-Router\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u7b54\u6848\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5c06token\u4f7f\u7528\u91cf\u964d\u4f4e\u4e86\u9ad8\u8fbe30%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u7ed3\u6784\u5316\u8bb0\u5fc6\u8def\u7531\u548c\u8f93\u51fa\u611f\u77e5\u8bc4\u4f30\u5bf9\u4fc3\u8fdb\u53ef\u6269\u5c55\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u53d1\u5c55\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.04846", "pdf": "https://arxiv.org/pdf/2508.04846", "abs": "https://arxiv.org/abs/2508.04846", "authors": ["Mahdi Nazari Ashani", "Ali Asghar Alesheikh", "Saba Kazemi", "Kimya Kheirkhah", "Yasin Mohammadi", "Fatemeh Rezaie", "Amir Mahdi Manafi", "Hedieh Zarkesh"], "title": "Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Autonomous web-based geographical information systems (AWebGIS) aim to\nperform geospatial operations from natural language input, providing intuitive,\nintelligent, and hands-free interaction. However, most current solutions rely\non cloud-based large language models (LLMs), which require continuous internet\naccess and raise users' privacy and scalability issues due to centralized\nserver processing. This study compares three approaches to enabling AWebGIS:\n(1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2)\na semi-automated offline method using classical machine learning classifiers\nsuch as support vector machine and random forest; and (3) a fully autonomous\noffline (client-side) method based on a fine-tuned small language model (SLM),\nspecifically T5-small model, executed in the client's web browser. The third\napproach, which leverages SLMs, achieved the highest accuracy among all\nmethods, with an exact matching accuracy of 0.93, Levenshtein similarity of\n0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L\nscores of 0.98. Crucially, this client-side computation strategy reduces the\nload on backend servers by offloading processing to the user's device,\neliminating the need for server-based inference. These results highlight the\nfeasibility of browser-executable models for AWebGIS solutions.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u79bb\u7ebf\u81ea\u4e3b\u7f51\u7edc\u5730\u7406\u4fe1\u606f\u7cfb\u7edf (AWebGIS) \u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\uff0c\u53d1\u73b0\u57fa\u4e8e\u5ba2\u6237\u7aef\u5c0f\u578b\u8bed\u8a00\u6a21\u578b (SLM) \u7684\u65b9\u6848\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5f53\u524dAWebGIS\u89e3\u51b3\u65b9\u6848\u8fc7\u5ea6\u4f9d\u8d56\u57fa\u4e8e\u4e91\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM)\uff0c\u5bfc\u81f4\u9700\u8981\u6301\u7eed\u7f51\u7edc\u8fde\u63a5\u3001\u7528\u6237\u9690\u79c1\u95ee\u9898\u548c\u4e2d\u5fc3\u5316\u670d\u52a1\u5668\u5904\u7406\u5e26\u6765\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cdAWebGIS\u5b9e\u73b0\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u4e91LLM\uff08\u5982Cohere\uff09\u7684\u5168\u81ea\u52a8\u5728\u7ebf\u65b9\u6cd5\uff1b2) \u4f7f\u7528\u652f\u6301\u5411\u91cf\u673a\u548c\u968f\u673a\u68ee\u6797\u7b49\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u7684\u534a\u81ea\u52a8\u79bb\u7ebf\u65b9\u6cd5\uff1b3) \u57fa\u4e8e\u5ba2\u6237\u7aef\u6d4f\u89c8\u5668\u4e2d\u8fd0\u884c\u7684\u5fae\u8c03\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08T5-small\uff09\u7684\u5168\u81ea\u52a8\u79bb\u7ebf\u65b9\u6cd5\u3002", "result": "\u57fa\u4e8eSLM\u7684\u7b2c\u4e09\u79cd\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u5b9e\u73b0\u4e860.93\u7684\u7cbe\u786e\u5339\u914d\u51c6\u786e\u7387\u30010.99\u7684Levenshtein\u76f8\u4f3c\u5ea6\u4ee5\u53ca0.98\u7684ROUGE-1\u548cROUGE-L\u5206\u6570\u3002\u8be5\u5ba2\u6237\u7aef\u8ba1\u7b97\u7b56\u7565\u901a\u8fc7\u5c06\u5904\u7406\u8d1f\u8f7d\u8f6c\u79fb\u5230\u7528\u6237\u8bbe\u5907\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u540e\u7aef\u670d\u52a1\u5668\u7684\u538b\u529b\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\u6d4f\u89c8\u5668\u53ef\u6267\u884c\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u4e8eAWebGIS\u89e3\u51b3\u65b9\u6848\u662f\u53ef\u884c\u4e14\u9ad8\u6548\u7684\uff0c\u80fd\u591f\u89e3\u51b3\u5f53\u524d\u4e91\u7aefLLM\u5e26\u6765\u7684\u9690\u79c1\u3001\u53ef\u6269\u5c55\u6027\u548c\u7f51\u7edc\u4f9d\u8d56\u95ee\u9898\u3002"}}
{"id": "2508.04818", "pdf": "https://arxiv.org/pdf/2508.04818", "abs": "https://arxiv.org/abs/2508.04818", "authors": ["Mehrdad Moradi", "Marco Grasso", "Bianca Maria Colosimo", "Kamran Paynabar"], "title": "Single-Step Reconstruction-Free Anomaly Detection and Segmentation via Diffusion Models", "categories": ["cs.CV", "eess.IV", "stat.ML", "62H35, 68T07, 62M40, 68T45", "I.2.6; I.2.10; I.4.6; I.4.8; I.5.1; I.5.4"], "comment": "9 pages, 8 figures, 2 tables. Submitted to an IEEE conference", "summary": "Generative models have demonstrated significant success in anomaly detection\nand segmentation over the past decade. Recently, diffusion models have emerged\nas a powerful alternative, outperforming previous approaches such as GANs and\nVAEs. In typical diffusion-based anomaly detection, a model is trained on\nnormal data, and during inference, anomalous images are perturbed to a\npredefined intermediate step in the forward diffusion process. The\ncorresponding normal image is then reconstructed through iterative reverse\nsampling.\n  However, reconstruction-based approaches present three major challenges: (1)\nthe reconstruction process is computationally expensive due to multiple\nsampling steps, making real-time applications impractical; (2) for complex or\nsubtle patterns, the reconstructed image may correspond to a different normal\npattern rather than the original input; and (3) Choosing an appropriate\nintermediate noise level is challenging because it is application-dependent and\noften assumes prior knowledge of anomalies, an assumption that does not hold in\nunsupervised settings.\n  We introduce Reconstruction-free Anomaly Detection with Attention-based\ndiffusion models in Real-time (RADAR), which overcomes the limitations of\nreconstruction-based anomaly detection. Unlike current SOTA methods that\nreconstruct the input image, RADAR directly produces anomaly maps from the\ndiffusion model, improving both detection accuracy and computational\nefficiency. We evaluate RADAR on real-world 3D-printed material and the\nMVTec-AD dataset. Our approach surpasses state-of-the-art diffusion-based and\nstatistical machine learning models across all key metrics, including accuracy,\nprecision, recall, and F1 score. Specifically, RADAR improves F1 score by 7% on\nMVTec-AD and 13% on the 3D-printed material dataset compared to the next best\nmodel.\n  Code available at: https://github.com/mehrdadmoradi124/RADAR", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRADAR\uff0c\u4e00\u79cd\u65e0\u91cd\u5efa\u3001\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u548c\u5206\u5272\u3002\u5b83\u76f4\u63a5\u4ece\u6269\u6563\u6a21\u578b\u751f\u6210\u5f02\u5e38\u56fe\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u91cd\u5efa\u65b9\u6cd5\u7684\u8ba1\u7b97\u6602\u8d35\u3001\u91cd\u5efa\u4e0d\u51c6\u786e\u548c\u566a\u58f0\u6c34\u5e73\u9009\u62e9\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3001\u91cd\u5efa\u53ef\u80fd\u4e0d\u51c6\u786e\u4e14\u4e2d\u95f4\u566a\u58f0\u6c34\u5e73\u9009\u62e9\u56f0\u96be\u7684\u91cd\u5efa\u8fc7\u7a0b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5b9e\u65f6\u5e94\u7528\u548c\u65e0\u76d1\u7763\u573a\u666f\u4e0b\u7684\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRADAR\uff08Reconstruction-free Anomaly Detection with Attention-based diffusion models in Real-time\uff09\u7684\u65b0\u65b9\u6cd5\u3002RADAR\u4e0d\u91cd\u5efa\u8f93\u5165\u56fe\u50cf\uff0c\u800c\u662f\u5229\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u6269\u6563\u6a21\u578b\u76f4\u63a5\u751f\u6210\u5f02\u5e38\u56fe\uff0c\u4ee5\u6b64\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "RADAR\u5728\u771f\u5b9e\u4e16\u754c3D\u6253\u5370\u6750\u6599\u548cMVTec-AD\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5b83\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u7b49\u6240\u6709\u5173\u952e\u6307\u6807\u4e0a\u5747\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u6269\u6563\u548c\u7edf\u8ba1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002\u5177\u4f53\u800c\u8a00\uff0cRADAR\u5728MVTec-AD\u6570\u636e\u96c6\u4e0aF1\u5206\u6570\u63d0\u9ad8\u4e867%\uff0c\u57283D\u6253\u5370\u6750\u6599\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e8613%\u3002", "conclusion": "RADAR\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u65e0\u91cd\u5efa\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4f18\u8d8a\u6027\u3002"}}
