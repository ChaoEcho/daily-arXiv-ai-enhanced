{"id": "2507.07186", "pdf": "https://arxiv.org/pdf/2507.07186", "abs": "https://arxiv.org/abs/2507.07186", "authors": ["Itay Itzhak", "Yonatan Belinkov", "Gabriel Stanovsky"], "title": "Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "CoLM 2025", "summary": "Large language models (LLMs) exhibit cognitive biases -- systematic\ntendencies of irrational decision-making, similar to those seen in humans.\nPrior work has found that these biases vary across models and can be amplified\nby instruction tuning. However, it remains unclear if these differences in\nbiases stem from pretraining, finetuning, or even random noise due to training\nstochasticity. We propose a two-step causal experimental approach to\ndisentangle these factors. First, we finetune models multiple times using\ndifferent random seeds to study how training randomness affects over $30$\ncognitive biases. Second, we introduce \\emph{cross-tuning} -- swapping\ninstruction datasets between models to isolate bias sources. This swap uses\ndatasets that led to different bias patterns, directly testing whether biases\nare dataset-dependent. Our findings reveal that while training randomness\nintroduces some variability, biases are mainly shaped by pretraining: models\nwith the same pretrained backbone exhibit more similar bias patterns than those\nsharing only finetuning data. These insights suggest that understanding biases\nin finetuned models requires considering their pretraining origins beyond\nfinetuning effects. This perspective can guide future efforts to develop\nprincipled strategies for evaluating and mitigating bias in LLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba4\u77e5\u504f\u5dee\u4e3b\u8981\u6e90\u4e8e\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u800c\u975e\u5fae\u8c03\u6216\u8bad\u7ec3\u968f\u673a\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b58\u5728\u8ba4\u77e5\u504f\u5dee\uff0c\u4f46\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u504f\u5dee\u7684\u6765\u6e90\u662f\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u8fd8\u662f\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u968f\u673a\u6027\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u56e0\u679c\u5b9e\u9a8c\u65b9\u6cd5\uff1a\u9996\u5148\uff0c\u4f7f\u7528\u4e0d\u540c\u968f\u673a\u79cd\u5b50\u591a\u6b21\u5fae\u8c03\u6a21\u578b\u4ee5\u8bc4\u4f30\u8bad\u7ec3\u968f\u673a\u6027\u5bf930\u591a\u79cd\u8ba4\u77e5\u504f\u5dee\u7684\u5f71\u54cd\uff1b\u5176\u6b21\uff0c\u5f15\u5165\u201c\u4ea4\u53c9\u5fae\u8c03\u201d\uff0c\u5728\u6a21\u578b\u95f4\u4ea4\u6362\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u4ee5\u9694\u79bb\u504f\u5dee\u6765\u6e90\u5e76\u6d4b\u8bd5\u504f\u5dee\u662f\u5426\u4f9d\u8d56\u4e8e\u6570\u636e\u96c6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bad\u7ec3\u968f\u673a\u6027\u786e\u5b9e\u5f15\u5165\u4e86\u4e00\u4e9b\u53d8\u5f02\u6027\uff0c\u4f46\u6a21\u578b\u7684\u8ba4\u77e5\u504f\u5dee\u4e3b\u8981\u7531\u9884\u8bad\u7ec3\u51b3\u5b9a\u3002\u62e5\u6709\u76f8\u540c\u9884\u8bad\u7ec3\u9aa8\u5e72\u7684\u6a21\u578b\uff0c\u5176\u504f\u5dee\u6a21\u5f0f\u6bd4\u4ec5\u5171\u4eab\u5fae\u8c03\u6570\u636e\u7684\u6a21\u578b\u66f4\u4e3a\u76f8\u4f3c\u3002", "conclusion": "\u7406\u89e3\u5fae\u8c03\u6a21\u578b\u7684\u504f\u5dee\u9700\u8981\u8d85\u8d8a\u5fae\u8c03\u6548\u5e94\uff0c\u6df1\u5165\u8003\u8651\u5176\u9884\u8bad\u7ec3\u8d77\u6e90\u3002\u8fd9\u4e00\u89c6\u89d2\u6709\u52a9\u4e8e\u672a\u6765\u5236\u5b9a\u8bc4\u4f30\u548c\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u504f\u5dee\u7684\u539f\u5219\u6027\u7b56\u7565\u3002"}}
{"id": "2507.07188", "pdf": "https://arxiv.org/pdf/2507.07188", "abs": "https://arxiv.org/abs/2507.07188", "authors": ["Jens Rupprecht", "Georg Ahnert", "Markus Strohmaier"], "title": "Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses", "categories": ["cs.CL", "cs.AI", "cs.CY", "J.4"], "comment": "18 pages, 17 figures", "summary": "Large Language Models (LLMs) are increasingly used as proxies for human\nsubjects in social science surveys, but their reliability and susceptibility to\nknown response biases are poorly understood. This paper investigates the\nresponse robustness of LLMs in normative survey contexts -- we test nine\ndiverse LLMs on questions from the World Values Survey (WVS), applying a\ncomprehensive set of 11 perturbations to both question phrasing and answer\noption structure, resulting in over 167,000 simulated interviews. In doing so,\nwe not only reveal LLMs' vulnerabilities to perturbations but also reveal that\nall tested models exhibit a consistent \\textit{recency bias} varying in\nintensity, disproportionately favoring the last-presented answer option. While\nlarger models are generally more robust, all models remain sensitive to\nsemantic variations like paraphrasing and to combined perturbations. By\napplying a set of perturbations, we reveal that LLMs partially align with\nsurvey response biases identified in humans. This underscores the critical\nimportance of prompt design and robustness testing when using LLMs to generate\nsynthetic survey data.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6a21\u62df\u793e\u4f1a\u79d1\u5b66\u8c03\u67e5\u4e2d\u5bf9\u95ee\u9898\u6270\u52a8\u7684\u654f\u611f\u6027\uff0c\u5e76\u53d1\u73b0\u5176\u5b58\u5728\u4e00\u81f4\u7684\u8fd1\u56e0\u504f\u5dee\uff0c\u5f3a\u8c03\u4e86\u63d0\u793a\u8bcd\u8bbe\u8ba1\u548c\u9c81\u68d2\u6027\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5f53\u524d\u5bf9\u4e8eLLM\u4f5c\u4e3a\u4eba\u7c7b\u8c03\u67e5\u5bf9\u8c61\u66ff\u4ee3\u54c1\u7684\u53ef\u9760\u6027\u53ca\u5176\u5bf9\u5df2\u77e5\u54cd\u5e94\u504f\u5dee\u7684\u6613\u611f\u6027\u4e86\u89e3\u4e0d\u8db3\uff0c\u4e9f\u9700\u6df1\u5165\u7814\u7a76\u3002", "method": "\u672c\u7814\u7a76\u9009\u53d6\u4e869\u4e2a\u4e0d\u540c\u7684LLM\uff0c\u5229\u7528\u4e16\u754c\u4ef7\u503c\u89c2\u8c03\u67e5\uff08WVS\uff09\u4e2d\u7684\u95ee\u9898\uff0c\u5bf9\u95ee\u9898\u63aa\u8f9e\u548c\u7b54\u6848\u9009\u9879\u7ed3\u6784\u65bd\u52a0\u4e8611\u79cd\u6270\u52a8\uff0c\u5171\u8fdb\u884c\u4e86\u8d85\u8fc7167,000\u6b21\u6a21\u62df\u8bbf\u8c08\uff0c\u4ee5\u8bc4\u4f30LLM\u7684\u54cd\u5e94\u9c81\u68d2\u6027\u3002", "result": "\u6240\u6709\u88ab\u6d4b\u8bd5\u7684LLM\u90fd\u5bf9\u6270\u52a8\u8868\u73b0\u51fa\u654f\u611f\u6027\uff0c\u5e76\u4e00\u81f4\u5730\u5448\u73b0\u51fa\u7a0b\u5ea6\u4e0d\u4e00\u7684\u201c\u8fd1\u56e0\u504f\u5dee\u201d\uff08\u503e\u5411\u4e8e\u9009\u62e9\u6700\u540e\u5448\u73b0\u7684\u7b54\u6848\u9009\u9879\uff09\u3002\u867d\u7136\u5927\u578b\u6a21\u578b\u901a\u5e38\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5bf9\u8bed\u4e49\u53d8\u5316\uff08\u5982\u8f6c\u8ff0\uff09\u548c\u7ec4\u5408\u6270\u52a8\u4ecd\u5f88\u654f\u611f\u3002\u7814\u7a76\u53d1\u73b0LLM\u7684\u67d0\u4e9b\u54cd\u5e94\u504f\u5dee\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u4f7f\u7528LLM\u751f\u6210\u5408\u6210\u8c03\u67e5\u6570\u636e\u65f6\uff0c\u63d0\u793a\u8bcd\u8bbe\u8ba1\u548c\u8fdb\u884c\u9c81\u68d2\u6027\u6d4b\u8bd5\u7684\u81f3\u5173\u91cd\u8981\u6027\u3002"}}
{"id": "2507.07229", "pdf": "https://arxiv.org/pdf/2507.07229", "abs": "https://arxiv.org/abs/2507.07229", "authors": ["Krithika Ramesh", "Daniel Smolyak", "Zihao Zhao", "Nupoor Gandhi", "Ritu Agarwal", "Margr\u00e9t Bjarnad\u00f3ttir", "Anjalie Field"], "title": "SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains", "categories": ["cs.CL"], "comment": null, "summary": "We present SynthTextEval, a toolkit for conducting comprehensive evaluations\nof synthetic text. The fluency of large language model (LLM) outputs has made\nsynthetic text potentially viable for numerous applications, such as reducing\nthe risks of privacy violations in the development and deployment of AI systems\nin high-stakes domains. Realizing this potential, however, requires principled\nconsistent evaluations of synthetic data across multiple dimensions: its\nutility in downstream systems, the fairness of these systems, the risk of\nprivacy leakage, general distributional differences from the source text, and\nqualitative feedback from domain experts. SynthTextEval allows users to conduct\nevaluations along all of these dimensions over synthetic data that they upload\nor generate using the toolkit's generation module. While our toolkit can be run\nover any data, we highlight its functionality and effectiveness over datasets\nfrom two high-stakes domains: healthcare and law. By consolidating and\nstandardizing evaluation metrics, we aim to improve the viability of synthetic\ntext, and in-turn, privacy-preservation in AI development.", "AI": {"tldr": "SynthTextEval\u662f\u4e00\u4e2a\u5168\u9762\u7684\u5408\u6210\u6587\u672c\u8bc4\u4f30\u5de5\u5177\u5305\uff0c\u65e8\u5728\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bc4\u4f30\u63d0\u5347AI\u5f00\u53d1\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u5408\u6210\u6587\u672c\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u533b\u7597\u3001\u6cd5\u5f8b\uff09\u5177\u6709\u9690\u79c1\u4fdd\u62a4\u7684\u6f5c\u529b\uff0c\u4f46\u8981\u5b9e\u73b0\u8fd9\u4e00\u6f5c\u529b\uff0c\u9700\u8981\u5bf9\u5408\u6210\u6570\u636e\u8fdb\u884c\u7cfb\u7edf\u4e14\u591a\u7ef4\u5ea6\u7684\u8bc4\u4f30\uff0c\u5305\u62ec\u5176\u5728\u4e0b\u6e38\u7cfb\u7edf\u4e2d\u7684\u6548\u7528\u3001\u7cfb\u7edf\u516c\u5e73\u6027\u3001\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3001\u4e0e\u6e90\u6587\u672c\u7684\u5206\u5e03\u5dee\u5f02\u4ee5\u53ca\u9886\u57df\u4e13\u5bb6\u7684\u5b9a\u6027\u53cd\u9988\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86SynthTextEval\u5de5\u5177\u5305\uff0c\u5b83\u5141\u8bb8\u7528\u6237\u5bf9\u4e0a\u4f20\u6216\u901a\u8fc7\u5176\u751f\u6210\u6a21\u5757\u521b\u5efa\u7684\u5408\u6210\u6570\u636e\uff0c\u4ece\u4e0b\u6e38\u7cfb\u7edf\u6548\u7528\u3001\u7cfb\u7edf\u516c\u5e73\u6027\u3001\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3001\u901a\u7528\u5206\u5e03\u5dee\u5f02\u548c\u9886\u57df\u4e13\u5bb6\u5b9a\u6027\u53cd\u9988\u7b49\u591a\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "\u5c3d\u7ba1SynthTextEval\u53ef\u7528\u4e8e\u4efb\u4f55\u6570\u636e\uff0c\u4f46\u5176\u529f\u80fd\u548c\u6709\u6548\u6027\u5df2\u5728\u533b\u7597\u548c\u6cd5\u5f8b\u8fd9\u4e24\u4e2a\u9ad8\u98ce\u9669\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e86\u7a81\u51fa\u5c55\u793a\u548c\u9a8c\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u548c\u6807\u51c6\u5316\u5408\u6210\u6587\u672c\u7684\u8bc4\u4f30\u6307\u6807\uff0cSynthTextEval\u65e8\u5728\u63d0\u9ad8\u5408\u6210\u6587\u672c\u7684\u5b9e\u7528\u6027\uff0c\u8fdb\u800c\u4fc3\u8fdb\u4eba\u5de5\u667a\u80fd\u5f00\u53d1\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u3002"}}
{"id": "2507.07248", "pdf": "https://arxiv.org/pdf/2507.07248", "abs": "https://arxiv.org/abs/2507.07248", "authors": ["Minseon Kim", "Jean-Philippe Corbeil", "Alessandro Sordoni", "Francois Beaulieu", "Paul Vozila"], "title": "Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings", "categories": ["cs.CL"], "comment": null, "summary": "As the performance of large language models (LLMs) continues to advance,\ntheir adoption is expanding across a wide range of domains, including the\nmedical field. The integration of LLMs into medical applications raises\ncritical safety concerns, particularly due to their use by users with diverse\nroles, e.g. patients and clinicians, and the potential for model's outputs to\ndirectly affect human health. Despite the domain-specific capabilities of\nmedical LLMs, prior safety evaluations have largely focused only on general\nsafety benchmarks. In this paper, we introduce a safety evaluation protocol\ntailored to the medical domain in both patient user and clinician user\nperspectives, alongside general safety assessments and quantitatively analyze\nthe safety of medical LLMs. We bridge a gap in the literature by building the\nPatientSafetyBench containing 466 samples over 5 critical categories to measure\nsafety from the perspective of the patient. We apply our red-teaming protocols\non the MediPhi model collection as a case study. To our knowledge, this is the\nfirst work to define safety evaluation criteria for medical LLMs through\ntargeted red-teaming taking three different points of view - patient,\nclinician, and general user - establishing a foundation for safer deployment in\nmedical domains.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u6240\u5e26\u6765\u7684\u5b89\u5168\u9690\u60a3\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u5b9a\u5236\u5316\u7684\u5b89\u5168\u8bc4\u4f30\u534f\u8bae\u548c\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u60a3\u8005\u3001\u4e34\u5e8a\u533b\u751f\u548c\u901a\u7528\u7528\u6237\u4e09\u4e2a\u89c6\u89d2\uff0c\u5e76\u901a\u8fc7\u7ea2\u961f\u6d4b\u8bd5\u5bf9\u533b\u7597LLMs\u7684\u5b89\u5168\u6027\u8fdb\u884c\u4e86\u5b9a\u91cf\u5206\u6790\uff0c\u65e8\u5728\u4e3a\u533b\u7597LLMs\u7684\u5b89\u5168\u90e8\u7f72\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u968f\u7740LLMs\u5728\u533b\u7597\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u8f93\u51fa\u53ef\u80fd\u76f4\u63a5\u5f71\u54cd\u4eba\u7c7b\u5065\u5eb7\uff0c\u5f15\u53d1\u4e86\u4e25\u91cd\u7684\u5b89\u5168\u62c5\u5fe7\u3002\u7136\u800c\uff0c\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u4e8e\u901a\u7528\u57fa\u51c6\uff0c\u7f3a\u4e4f\u9488\u5bf9\u533b\u7597\u9886\u57df\u7279\u5b9a\u7528\u6237\uff08\u5982\u60a3\u8005\u548c\u4e34\u5e8a\u533b\u751f\uff09\u89c6\u89d2\u7684\u4e13\u4e1a\u5b89\u5168\u8bc4\u4f30\u3002", "method": "\u5f15\u5165\u4e86\u9488\u5bf9\u533b\u7597\u9886\u57df\u7684\u5b89\u5168\u8bc4\u4f30\u534f\u8bae\uff0c\u8be5\u534f\u8bae\u540c\u65f6\u8003\u8651\u4e86\u60a3\u8005\u7528\u6237\u548c\u4e34\u5e8a\u533b\u751f\u7528\u6237\u7684\u89c6\u89d2\uff0c\u5e76\u7ed3\u5408\u901a\u7528\u5b89\u5168\u8bc4\u4f30\u3002\u4e3a\u6b64\uff0c\u6784\u5efa\u4e86PatientSafetyBench\u6570\u636e\u96c6\uff0c\u5305\u542b466\u4e2a\u6837\u672c\uff0c\u6db5\u76d65\u4e2a\u5173\u952e\u7c7b\u522b\uff0c\u4e13\u95e8\u7528\u4e8e\u8861\u91cf\u60a3\u8005\u89c6\u89d2\u7684\u5b89\u5168\u6027\u3002\u5c06\u7ea2\u961f\u6d4b\u8bd5\u534f\u8bae\u5e94\u7528\u4e8eMediPhi\u6a21\u578b\u96c6\u5408\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u636e\u4f5c\u8005\u6240\u77e5\uff0c\u8fd9\u662f\u9996\u6b21\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u7ea2\u961f\u6d4b\u8bd5\uff0c\u4ece\u60a3\u8005\u3001\u4e34\u5e8a\u533b\u751f\u548c\u901a\u7528\u7528\u6237\u4e09\u4e2a\u4e0d\u540c\u89c6\u89d2\u5b9a\u4e49\u533b\u7597LLMs\u7684\u5b89\u5168\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u6210\u529f\u6784\u5efa\u4e86PatientSafetyBench\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u533b\u7597\u9886\u57df\u7279\u5b9a\u5b89\u5168\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u533b\u7597\u9886\u57dfLLMs\u66f4\u5b89\u5168\u7684\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u3001\u5b9a\u5236\u5316\u7684\u5b89\u5168\u8bc4\u4f30\uff0c\u63d0\u9ad8\u4e86\u533b\u7597LLMs\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2507.07118", "pdf": "https://arxiv.org/pdf/2507.07118", "abs": "https://arxiv.org/abs/2507.07118", "authors": ["Zelin Zhu", "Kai Yang", "Rui Zhang"], "title": "Synergistic Localization and Sensing in MIMO-OFDM Systems via Mixed-Integer Bilevel Learning", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "Wireless localization and sensing technologies are essential in modern\nwireless networks, supporting applications in smart cities, the Internet of\nThings (IoT), and autonomous systems. High-performance localization and sensing\nsystems are critical for both network efficiency and emerging intelligent\napplications. Integrating channel state information (CSI) with deep learning\nhas recently emerged as a promising solution. Recent works have leveraged the\nspatial diversity of multiple input multiple output (MIMO) systems and the\nfrequency granularity of orthogonal frequency division multiplexing (OFDM)\nwaveforms to improve spatial resolution. Nevertheless, the joint modeling of\nlocalization and sensing under the high-dimensional CSI characteristics of\nMIMO-OFDM systems remains insufficiently investigated. This work aims to\njointly model and optimize localization and sensing tasks to harness their\npotential synergy. We first formulate localization and sensing as a\nmixed-integer bilevel deep learning problem and then propose a novel stochastic\nproximal gradient-based mixed-integer bilevel optimization (SPG-MIBO)\nalgorithm. SPG-MIBO is well-suited for high-dimensional and large-scale\ndatasets, leveraging mini-batch training at each step for computational and\nmemory efficiency. The algorithm is also supported by theoretical convergence\nguarantees. Extensive experiments on multiple datasets validate its\neffectiveness and highlight the performance gains from joint localization and\nsensing optimization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u8fd1\u7aef\u68af\u5ea6\u6df7\u5408\u6574\u6570\u53cc\u5c42\u4f18\u5316\u7684SPG-MIBO\u7b97\u6cd5\uff0c\u7528\u4e8e\u8054\u5408\u5efa\u6a21\u548c\u4f18\u5316MIMO-OFDM\u7cfb\u7edf\u4e2d\u7684\u65e0\u7ebf\u5b9a\u4f4d\u4e0e\u4f20\u611f\u4efb\u52a1\u3002", "motivation": "\u73b0\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\uff0c\u65e0\u7ebf\u5b9a\u4f4d\u4e0e\u4f20\u611f\u5bf9\u667a\u80fd\u57ce\u5e02\u3001\u7269\u8054\u7f51\u548c\u81ea\u52a8\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5c06\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u5f88\u6709\u524d\u666f\uff0c\u4f46MIMO-OFDM\u7cfb\u7edf\u9ad8\u7ef4CSI\u7279\u6027\u4e0b\u5b9a\u4f4d\u4e0e\u4f20\u611f\u7684\u8054\u5408\u5efa\u6a21\u4ecd\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u5c06\u5b9a\u4f4d\u4e0e\u4f20\u611f\u4efb\u52a1\u516c\u5f0f\u5316\u4e3a\u6df7\u5408\u6574\u6570\u53cc\u5c42\u6df1\u5ea6\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u968f\u673a\u8fd1\u7aef\u68af\u5ea6\u7684\u6df7\u5408\u6574\u6570\u53cc\u5c42\u4f18\u5316\uff08SPG-MIBO\uff09\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u9002\u7528\u4e8e\u9ad8\u7ef4\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5c0f\u6279\u91cf\u8bad\u7ec3\u63d0\u9ad8\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SPG-MIBO\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u7a81\u51fa\u4e86\u8054\u5408\u5b9a\u4f4d\u4e0e\u4f20\u611f\u4f18\u5316\u6240\u5e26\u6765\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u548c\u4f18\u5316\u65e0\u7ebf\u5b9a\u4f4d\u4e0e\u4f20\u611f\u4efb\u52a1\uff0c\u6240\u63d0\u51fa\u7684SPG-MIBO\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528MIMO-OFDM\u7cfb\u7edf\u7684\u9ad8\u7ef4CSI\uff0c\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0c\u4ece\u800c\u652f\u6301\u73b0\u4ee3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5404\u79cd\u667a\u80fd\u5e94\u7528\u3002"}}
{"id": "2507.07115", "pdf": "https://arxiv.org/pdf/2507.07115", "abs": "https://arxiv.org/abs/2507.07115", "authors": ["Javal Vyas", "Mehmet Mercangoz"], "title": "Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation", "categories": ["cs.AI", "cs.MA", "cs.SY", "eess.SY"], "comment": null, "summary": "The increasing complexity of modern chemical processes, coupled with\nworkforce shortages and intricate fault scenarios, demands novel automation\nparadigms that blend symbolic reasoning with adaptive control. In this work, we\nintroduce a unified agentic framework that leverages large language models\n(LLMs) for both discrete fault-recovery planning and continuous process control\nwithin a single architecture. We adopt Finite State Machines (FSMs) as\ninterpretable operating envelopes: an LLM-driven planning agent proposes\nrecovery sequences through the FSM, a Simulation Agent executes and checks each\ntransition, and a Validator-Reprompting loop iteratively refines invalid plans.\nIn Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25\nstates, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path\nsuccess within five reprompts-outperforming open-source LLMs in both accuracy\nand latency. In Case Study 2, the same framework modulates dual-heater inputs\non a laboratory TCLab platform (and its digital twin) to maintain a target\naverage temperature under persistent asymmetric disturbances. Compared to\nclassical PID control, our LLM-based controller attains similar performance,\nwhile ablation of the prompting loop reveals its critical role in handling\nnonlinear dynamics. We analyze key failure modes-such as instruction following\nlapses and coarse ODE approximations. Our results demonstrate that, with\nstructured feedback and modular agents, LLMs can unify high-level symbolic\nplanningand low-level continuous control, paving the way towards resilient,\nlanguage-driven automation in chemical engineering.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u7edf\u4e00\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u5316\u5de5\u8fc7\u7a0b\u4e2d\u7684\u79bb\u6563\u6545\u969c\u6062\u590d\u89c4\u5212\u548c\u8fde\u7eed\u8fc7\u7a0b\u63a7\u5236\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u5316\u5de5\u8fc7\u7a0b\u65e5\u76ca\u590d\u6742\uff0c\u52b3\u52a8\u529b\u77ed\u7f3a\u4e14\u6545\u969c\u60c5\u666f\u590d\u6742\uff0c\u4e9f\u9700\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u4e0e\u81ea\u9002\u5e94\u63a7\u5236\u7684\u65b0\u578b\u81ea\u52a8\u5316\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u540c\u65f6\u8fdb\u884c\u79bb\u6563\u6545\u969c\u6062\u590d\u89c4\u5212\u548c\u8fde\u7eed\u8fc7\u7a0b\u63a7\u5236\u3002\u8be5\u6846\u67b6\u91c7\u7528\u6709\u9650\u72b6\u6001\u673a\uff08FSMs\uff09\u5b9a\u4e49\u64cd\u4f5c\u8303\u56f4\uff0c\u5e76\u901a\u8fc7LLM\u89c4\u5212\u4ee3\u7406\u3001\u6a21\u62df\u4ee3\u7406\u548c\u9a8c\u8bc1-\u91cd\u63d0\u793a\u5faa\u73af\u8fed\u4ee3\u4f18\u5316\u65b9\u6848\u3002", "result": "\u5728\u6545\u969c\u6062\u590d\u6848\u4f8b\u4e2d\uff0cGPT-4o\u53camini\u7248\u5728FSM\u8def\u5f84\u89c4\u5212\u4e0a\u8fbe\u5230100%\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u5f00\u6e90LLMs\u3002\u5728\u8fde\u7eed\u63a7\u5236\u6848\u4f8b\u4e2d\uff0cLLM\u63a7\u5236\u5668\u5728\u6e29\u5ea6\u7ef4\u6301\u65b9\u9762\u8868\u73b0\u4e0e\u4f20\u7edfPID\u76f8\u5f53\uff0c\u4e14\u63d0\u793a\u5faa\u73af\u5bf9\u5904\u7406\u975e\u7ebf\u6027\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u4e3b\u8981\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u53cd\u9988\u548c\u6a21\u5757\u5316\u667a\u80fd\u4f53\uff0cLLMs\u80fd\u591f\u7edf\u4e00\u9ad8\u5c42\u7b26\u53f7\u89c4\u5212\u4e0e\u4f4e\u5c42\u8fde\u7eed\u63a7\u5236\uff0c\u4e3a\u5316\u5de5\u9886\u57df\u7684\u97e7\u6027\u3001\u8bed\u8a00\u9a71\u52a8\u81ea\u52a8\u5316\u5f00\u8f9f\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.07129", "pdf": "https://arxiv.org/pdf/2507.07129", "abs": "https://arxiv.org/abs/2507.07129", "authors": ["A. Bochkov"], "title": "Growing Transformers: Modular Composition and Layer-wise Expansion on a Frozen Substrate", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The prevailing paradigm for scaling large language models (LLMs) involves\nmonolithic, end-to-end training, a resource-intensive process that lacks\nflexibility. This paper explores an alternative, constructive approach to model\ndevelopment, built upon the foundation of non-trainable, deterministic input\nembeddings. In prior [1], we established that high-level semantic reasoning can\nemerge in Transformers using frozen embeddings derived from the visual\nstructure of Unicode glyphs. Here, we demonstrate that this fixed\nrepresentational substrate acts as a universal \"docking port,\" enabling two\npowerful and efficient scaling paradigms: seamless modular composition and\nprogressive layer-wise growth.\n  First, we show that specialist models trained on disparate datasets (e.g.,\nRussian and Chinese text) can be merged into a single, more capable\nMixture-of-Experts (MoE) model, post-training, with zero architectural\nmodification. This is achieved by simply averaging their output logits. The\nresulting MoE model exhibits immediate performance improvements on reasoning\nbenchmarks like MMLU, surpassing its constituent experts without catastrophic\nforgetting. Second, we introduce a layer-wise constructive training\nmethodology, where a deep Transformer is \"grown\" by progressively stacking and\ntraining one layer at a time. This method demonstrates stable convergence and a\nclear correlation between model depth and the emergence of complex reasoning\nabilities, such as those required for SQuAD.\n  Our findings suggest a paradigm shift from monolithic optimization towards a\nmore biological or constructive model of AI development, where complexity is\nbuilt incrementally and modules can be composed freely. This opens new avenues\nfor resource-efficient scaling, continual learning, and a more democratized\necosystem for building powerful AI systems. We release all code and models to\nfacilitate further research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fa\u5b9a\u3001\u4e0d\u53ef\u8bad\u7ec3\u8f93\u5165\u5d4c\u5165\u7684\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u65b9\u6cd5\uff0c\u4f5c\u4e3a\u201c\u901a\u7528\u5bf9\u63a5\u7aef\u53e3\u201d\uff0c\u5b9e\u73b0\u4e86\u4e24\u79cd\u9ad8\u6548\u6269\u5c55\u8303\u5f0f\uff1a\u901a\u8fc7\u5e73\u5747\u8f93\u51falogits\u8fdb\u884c\u4e13\u5bb6\u6a21\u578b\uff08MoE\uff09\u7684\u6a21\u5757\u5316\u7ec4\u5408\uff0c\u4ee5\u53ca\u9010\u5c42\u589e\u957f\u6a21\u578b\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5728\u63a8\u7406\u57fa\u51c6\u4e0a\u5747\u8868\u73b0\u51fa\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u65e0\u9700\u6574\u4f53\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6574\u4f53\u7aef\u5230\u7aef\u8bad\u7ec3\u8303\u5f0f\u8d44\u6e90\u5bc6\u96c6\u4e14\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u66ff\u4ee3\u6027\u7684\u3001\u66f4\u5177\u5efa\u8bbe\u6027\u548c\u7075\u6d3b\u6027\u7684\u6a21\u578b\u5f00\u53d1\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u57fa\u4e8e\u4e0d\u53ef\u8bad\u7ec3\u7684\u3001\u786e\u5b9a\u6027\u8f93\u5165\u5d4c\u5165\uff08\u6e90\u81eaUnicode\u5b57\u5f62\u89c6\u89c9\u7ed3\u6784\uff09\u5efa\u7acb\u6a21\u578b\u3002\u8fd9\u79cd\u56fa\u5b9a\u8868\u793a\u4f5c\u4e3a\u201c\u901a\u7528\u5bf9\u63a5\u7aef\u53e3\u201d\uff0c\u652f\u6301\u4e24\u79cd\u4e3b\u8981\u6269\u5c55\u8303\u5f0f\uff1a\n1. **\u6a21\u5757\u5316\u7ec4\u5408\uff1a** \u5c06\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u4e13\u5bb6\u6a21\u578b\uff08\u5982\u4fc4\u8bed\u548c\u4e2d\u6587\u6587\u672c\u6a21\u578b\uff09\u901a\u8fc7\u7b80\u5355\u5e73\u5747\u5176\u8f93\u51falogits\uff0c\u5728\u8bad\u7ec3\u540e\u65e0\u7f1d\u5408\u5e76\u6210\u4e00\u4e2a\u66f4\u5f3a\u5927\u7684Mixture-of-Experts (MoE) \u6a21\u578b\uff0c\u65e0\u9700\u4fee\u6539\u67b6\u6784\u3002\n2. **\u9010\u5c42\u589e\u957f\uff1a** \u5f15\u5165\u9010\u5c42\u6784\u5efa\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u5806\u53e0\u548c\u8bad\u7ec3Transformer\u6a21\u578b\u4e2d\u7684\u6bcf\u4e00\u5c42\u6765\u201c\u6210\u957f\u201d\u6df1\u5ea6\u6a21\u578b\u3002", "result": "1. **\u6a21\u5757\u5316\u7ec4\u5408\uff1a** \u5408\u5e76\u540e\u7684MoE\u6a21\u578b\u5728MMLU\u7b49\u63a8\u7406\u57fa\u51c6\u4e0a\u7acb\u5373\u663e\u793a\u51fa\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a\u5176\u7ec4\u6210\u4e13\u5bb6\u6a21\u578b\uff0c\u4e14\u672a\u51fa\u73b0\u707e\u96be\u6027\u9057\u5fd8\u3002\n2. **\u9010\u5c42\u589e\u957f\uff1a** \u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u7a33\u5b9a\u7684\u6536\u655b\u6027\uff0c\u5e76\u4e14\u6a21\u578b\u6df1\u5ea6\u4e0e\u590d\u6742\u63a8\u7406\u80fd\u529b\uff08\u4f8b\u5982SQuAD\u6240\u9700\u7684\uff09\u7684\u51fa\u73b0\u4e4b\u95f4\u5b58\u5728\u660e\u786e\u7684\u76f8\u5173\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4eba\u5de5\u667a\u80fd\u5f00\u53d1\u53ef\u4ee5\u4ece\u6574\u4f53\u4f18\u5316\u8f6c\u5411\u66f4\u5177\u751f\u7269\u5b66\u6216\u5efa\u8bbe\u6027\u7684\u6a21\u578b\uff0c\u5176\u4e2d\u590d\u6742\u6027\u53ef\u4ee5\u589e\u91cf\u6784\u5efa\uff0c\u6a21\u5757\u53ef\u4ee5\u81ea\u7531\u7ec4\u5408\u3002\u8fd9\u4e3a\u8d44\u6e90\u9ad8\u6548\u7684\u6269\u5c55\u3001\u6301\u7eed\u5b66\u4e60\u4ee5\u53ca\u6784\u5efa\u5f3a\u5927AI\u7cfb\u7edf\u7684\u66f4\u6c11\u4e3b\u5316\u751f\u6001\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.07108", "pdf": "https://arxiv.org/pdf/2507.07108", "abs": "https://arxiv.org/abs/2507.07108", "authors": ["Zhiwei Hu", "V\u00edctor Guti\u00e9rrez-Basulto", "Zhiliang Xiang", "Ru Li", "Jeff Z. Pan"], "title": "Multi-level Mixture of Experts for Multimodal Entity Linking", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.MM"], "comment": "Accepted at KDD 2025", "summary": "Multimodal Entity Linking (MEL) aims to link ambiguous mentions within\nmultimodal contexts to associated entities in a multimodal knowledge base.\nExisting approaches to MEL introduce multimodal interaction and fusion\nmechanisms to bridge the modality gap and enable multi-grained semantic\nmatching. However, they do not address two important problems: (i) mention\nambiguity, i.e., the lack of semantic content caused by the brevity and\nomission of key information in the mention's textual context; (ii) dynamic\nselection of modal content, i.e., to dynamically distinguish the importance of\ndifferent parts of modal information. To mitigate these issues, we propose a\nMulti-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:\n(i) the description-aware mention enhancement module leverages large language\nmodels to identify the WikiData descriptions that best match a mention,\nconsidering the mention's textual context; (ii) the multimodal feature\nextraction module adopts multimodal feature encoders to obtain textual and\nvisual embeddings for both mentions and entities; (iii)-(iv) the intra-level\nmixture of experts and inter-level mixture of experts modules apply a switch\nmixture of experts mechanism to dynamically and adaptively select features from\nrelevant regions of information. Extensive experiments demonstrate the\noutstanding performance of MMoE compared to the state-of-the-art. MMoE's code\nis available at: https://github.com/zhiweihu1103/MEL-MMoE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMMoE\uff08Multi-level Mixture of Experts\uff09\u7684\u65b0\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u6a21\u6001\u5b9e\u4f53\u94fe\u63a5\uff08MEL\uff09\u4e2d\u5b58\u5728\u7684\u63d0\u53ca\u6b67\u4e49\u548c\u6a21\u6001\u5185\u5bb9\u52a8\u6001\u9009\u62e9\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MEL\u65b9\u6cd5\u672a\u80fd\u89e3\u51b3\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u63d0\u53ca\u6b67\u4e49\uff0c\u5373\u7531\u4e8e\u63d0\u53ca\u6587\u672c\u4e0a\u4e0b\u6587\u7684\u7b80\u6d01\u6027\u548c\u4fe1\u606f\u7f3a\u5931\u5bfc\u81f4\u7684\u8bed\u4e49\u5185\u5bb9\u4e0d\u8db3\uff1b2) \u6a21\u6001\u5185\u5bb9\u52a8\u6001\u9009\u62e9\uff0c\u5373\u96be\u4ee5\u52a8\u6001\u533a\u5206\u4e0d\u540c\u6a21\u6001\u4fe1\u606f\u90e8\u5206\u7684\u91cd\u8981\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86MMoE\u6a21\u578b\u3002\u5b83\u5305\u542b\u56db\u4e2a\u7ec4\u4ef6\uff1a1) \u63cf\u8ff0\u611f\u77e5\u63d0\u53ca\u589e\u5f3a\u6a21\u5757\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u6700\u5339\u914d\u63d0\u53ca\u7684WikiData\u63cf\u8ff0\uff1b2) \u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff0c\u91c7\u7528\u591a\u6a21\u6001\u7279\u5f81\u7f16\u7801\u5668\u83b7\u53d6\u6587\u672c\u548c\u89c6\u89c9\u5d4c\u5165\uff1b3) \u5185\u90e8\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\u548c4) \u7ea7\u522b\u95f4\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff0c\u5e94\u7528\u4e13\u5bb6\u6df7\u5408\u673a\u5236\u52a8\u6001\u81ea\u9002\u5e94\u5730\u9009\u62e9\u76f8\u5173\u4fe1\u606f\u533a\u57df\u7684\u7279\u5f81\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0cMMoE\u6a21\u578b\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "MMoE\u6a21\u578b\u6709\u6548\u7f13\u89e3\u4e86\u63d0\u53ca\u6b67\u4e49\u548c\u6a21\u6001\u5185\u5bb9\u52a8\u6001\u9009\u62e9\u95ee\u9898\uff0c\u5e76\u5728\u591a\u6a21\u6001\u5b9e\u4f53\u94fe\u63a5\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002"}}
{"id": "2507.07280", "pdf": "https://arxiv.org/pdf/2507.07280", "abs": "https://arxiv.org/abs/2507.07280", "authors": ["Mariah Bradford", "Nikhil Krishnaswamy", "Nathaniel Blanchard"], "title": "The Impact of Background Speech on Interruption Detection in Collaborative Groups", "categories": ["cs.CL"], "comment": "Long Paper AIED 2025", "summary": "Interruption plays a crucial role in collaborative learning, shaping group\ninteractions and influencing knowledge construction. AI-driven support can\nassist teachers in monitoring these interactions. However, most previous work\non interruption detection and interpretation has been conducted in\nsingle-conversation environments with relatively clean audio. AI agents\ndeployed in classrooms for collaborative learning within small groups will need\nto contend with multiple concurrent conversations -- in this context,\noverlapping speech will be ubiquitous, and interruptions will need to be\nidentified in other ways. In this work, we analyze interruption detection in\nsingle-conversation and multi-group dialogue settings. We then create a\nstate-of-the-art method for interruption identification that is robust to\noverlapping speech, and thus could be deployed in classrooms. Further, our work\nhighlights meaningful linguistic and prosodic information about how\ninterruptions manifest in collaborative group interactions. Our investigation\nalso paves the way for future works to account for the influence of overlapping\nspeech from multiple groups when tracking group dialog.", "AI": {"tldr": "\u4e3a\u8bfe\u5802AI\u90e8\u7f72\uff0c\u5f00\u53d1\u4e86\u5728\u591a\u7ec4\u5bf9\u8bdd\u91cd\u53e0\u8bed\u97f3\u73af\u5883\u4e0b\u9c81\u68d2\u8bc6\u522b\u6253\u65ad\u7684\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6253\u65ad\u68c0\u6d4b\u65b9\u6cd5\u591a\u5728\u5355\u5bf9\u8bdd\u3001\u6e05\u6670\u97f3\u9891\u4e0b\u7814\u7a76\uff0c\u4e0d\u9002\u7528\u4e8e\u8bfe\u5802\u4e2d\u591a\u7ec4\u5e76\u53d1\u5bf9\u8bdd\u548c\u8bed\u97f3\u91cd\u53e0\u666e\u904d\u5b58\u5728\u7684\u771f\u5b9e\u573a\u666f\u3002", "method": "\u5206\u6790\u4e86\u5355\u5bf9\u8bdd\u548c\u591a\u7ec4\u5bf9\u8bdd\u73af\u5883\u4e0b\u7684\u6253\u65ad\u68c0\u6d4b\uff0c\u5e76\u521b\u5efa\u4e86\u4e00\u79cd\u5bf9\u91cd\u53e0\u8bed\u97f3\u9c81\u68d2\u7684\u5148\u8fdb\u6253\u65ad\u8bc6\u522b\u65b9\u6cd5\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u53ef\u7528\u4e8e\u8bfe\u5802\u90e8\u7f72\u7684\u3001\u5bf9\u91cd\u53e0\u8bed\u97f3\u9c81\u68d2\u7684\u6253\u65ad\u8bc6\u522b\u65b9\u6cd5\uff1b\u63ed\u793a\u4e86\u534f\u540c\u5c0f\u7ec4\u4e92\u52a8\u4e2d\u6253\u65ad\u7684\u8bed\u8a00\u548c\u97f5\u5f8b\u7279\u5f81\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u771f\u5b9e\u8bfe\u5802\u73af\u5883\u7684\u6253\u65ad\u8bc6\u522b\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u8003\u8651\u591a\u7ec4\u91cd\u53e0\u8bed\u97f3\u5f71\u54cd\u7684\u5c0f\u7ec4\u5bf9\u8bdd\u8ffd\u8e2a\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.07149", "pdf": "https://arxiv.org/pdf/2507.07149", "abs": "https://arxiv.org/abs/2507.07149", "authors": ["Renyuan Liu", "Yuyang Leng", "Kaiyan Liu", "Shaohan Hu", "Chun-Fu", "Chen", "Peijun Zhao", "Heechul Yun", "Shuochao Yao"], "title": "DAF: An Efficient End-to-End Dynamic Activation Framework for on-Device DNN Training", "categories": ["cs.NI", "cs.LG"], "comment": "Accepted to MobiSys 2025", "summary": "Recent advancements in on-device training for deep neural networks have\nunderscored the critical need for efficient activation compression to overcome\nthe memory constraints of mobile and edge devices. As activations dominate\nmemory usage during training and are essential for gradient computation,\ncompressing them without compromising accuracy remains a key research\nchallenge. While existing methods for dynamic activation quantization promise\ntheoretical memory savings, their practical deployment is impeded by\nsystem-level challenges such as computational overhead and memory\nfragmentation.\n  To address these challenges, we introduce DAF, a Dynamic Activation Framework\nthat enables scalable and efficient on-device training through system-level\noptimizations. DAF achieves both memory- and time-efficient dynamic\nquantization training by addressing key system bottlenecks. It develops hybrid\nreduction operations tailored to the memory hierarchies of mobile and edge\nSoCs, leverages collaborative CPU-GPU bit-packing for efficient dynamic\nquantization, and implements an importance-aware paging memory management\nscheme to reduce fragmentation and support dynamic memory adjustments.\n  These optimizations collectively enable DAF to achieve substantial memory\nsavings and speedup without compromising model training accuracy. Evaluations\non various deep learning models across embedded and mobile platforms\ndemonstrate up to a $22.9\\times$ reduction in memory usage and a $3.2\\times$\nspeedup, making DAF a scalable and practical solution for resource-constrained\nenvironments.", "AI": {"tldr": "DAF\u662f\u4e00\u4e2a\u52a8\u6001\u6fc0\u6d3b\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u8bbe\u5907\u4e0a\u8bad\u7ec3\u7684\u5185\u5b58\u548c\u65f6\u95f4\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5185\u5b58\u8282\u7701\u548c\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u8bbe\u5907\u4e0a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\uff0c\u6fc0\u6d3b\u538b\u7f29\u5bf9\u4e8e\u514b\u670d\u79fb\u52a8\u548c\u8fb9\u7f18\u8bbe\u5907\u7684\u5185\u5b58\u9650\u5236\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u52a8\u6001\u6fc0\u6d3b\u91cf\u5316\u65b9\u6cd5\u56e0\u8ba1\u7b97\u5f00\u9500\u548c\u5185\u5b58\u788e\u7247\u5316\u7b49\u7cfb\u7edf\u7ea7\u6311\u6218\uff0c\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\u3002", "method": "DAF\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4f18\u5316\u5b9e\u73b0\u5185\u5b58\u548c\u65f6\u95f4\u9ad8\u6548\u7684\u52a8\u6001\u91cf\u5316\u8bad\u7ec3\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u5f00\u53d1\u9488\u5bf9\u79fb\u52a8\u548c\u8fb9\u7f18SoC\u5185\u5b58\u5c42\u6b21\u7ed3\u6784\u7684\u6df7\u5408\u89c4\u7ea6\u64cd\u4f5c\uff1b\u5229\u7528CPU-GPU\u534f\u540c\u4f4d\u6253\u5305\u8fdb\u884c\u9ad8\u6548\u52a8\u6001\u91cf\u5316\uff1b\u4ee5\u53ca\u5b9e\u65bd\u91cd\u8981\u6027\u611f\u77e5\u5206\u9875\u5185\u5b58\u7ba1\u7406\u65b9\u6848\uff0c\u4ee5\u51cf\u5c11\u788e\u7247\u5e76\u652f\u6301\u52a8\u6001\u5185\u5b58\u8c03\u6574\u3002", "result": "\u5728\u5d4c\u5165\u5f0f\u548c\u79fb\u52a8\u5e73\u53f0\u4e0a\u5bf9\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\u9ad8\u8fbe22.9\u500d\uff0c\u901f\u5ea6\u63d0\u53473.2\u500d\uff0c\u4e14\u672a\u5f71\u54cd\u6a21\u578b\u8bad\u7ec3\u7cbe\u5ea6\u3002", "conclusion": "DAF\u7684\u7cfb\u7edf\u7ea7\u4f18\u5316\u4f7f\u5176\u80fd\u591f\u5728\u4e0d\u727a\u7272\u6a21\u578b\u8bad\u7ec3\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u8282\u7701\u5185\u5b58\u5e76\u63d0\u9ad8\u901f\u5ea6\uff0c\u4ece\u800c\u6210\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
