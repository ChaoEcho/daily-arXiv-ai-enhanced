<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 15]
- [cs.CV](#cs.CV) [Total: 11]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.NI](#cs.NI) [Total: 9]
- [quant-ph](#quant-ph) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion](https://arxiv.org/abs/2508.03712)
*Agrima Seth,Monojit Choudhary,Sunayana Sitaram,Kentaro Toyama,Aditya Vashistha,Kalika Bali*

Main category: cs.CL

TL;DR: 研究发现GPT-4 Turbo在生成印度相关故事时存在严重的、难以纠正的宗教和种姓代表性偏差，即使使用旨在鼓励多样性的提示，模型也倾向于过度代表主导群体。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）的表征偏差研究多集中于单次交互及北美中心身份（如种族、性别）。本研究旨在扩展此范围，探究LLM中表征偏差的深度及其对印度宗教和种姓等较少探索的身份维度的影响。

Method: 对GPT-4 Turbo进行了系统性审计。通过设计不同程度鼓励多样性的提示，生成了超过7,200个关于印度重要生活事件（如婚礼）的故事。将输出中宗教和种姓代表的多样性与印度实际人口普查数据进行对比，量化LLM中表征偏差的存在和“粘性”。

Result: GPT-4的响应持续过度代表文化主导群体，远远超出其统计比例，即使提示旨在鼓励代表性多样性。研究还发现，LLM中的表征偏差具有“赢者通吃”的特点，比训练数据中可能的分布偏差更为严重，且重复的基于提示的干预在消除这些偏差方面效果有限且不一致。

Conclusion: 仅通过多样化训练数据可能不足以纠正LLM偏差，这表明模型开发需要进行更根本的改变。

Abstract: Representational bias in large language models (LLMs) has predominantly been
measured through single-response interactions and has focused on Global
North-centric identities like race and gender. We expand on that research by
conducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded
representational biases are and how they extend to less-explored dimensions of
identity. We prompt GPT-4 Turbo to generate over 7,200 stories about
significant life events (such as weddings) in India, using prompts designed to
encourage diversity to varying extents. Comparing the diversity of religious
and caste representation in the outputs against the actual population
distribution in India as recorded in census data, we quantify the presence and
"stickiness" of representational bias in the LLM for religion and caste. We
find that GPT-4 responses consistently overrepresent culturally dominant groups
far beyond their statistical representation, despite prompts intended to
encourage representational diversity. Our findings also suggest that
representational bias in LLMs has a winner-take-all quality that is more biased
than the likely distribution bias in their training data, and repeated
prompt-based nudges have limited and inconsistent efficacy in dislodging these
biases. These results suggest that diversifying training data alone may not be
sufficient to correct LLM bias, highlighting the need for more fundamental
changes in model development. Dataset and Codebook:
https://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs

</details>


### [2] [FeynTune: Large Language Models for High-Energy Theory](https://arxiv.org/abs/2508.03716)
*Paul Richmond,Prarit Agarwal,Borun Chowdhury,Vasilis Niarchos,Constantinos Papageorgakis*

Main category: cs.CL

TL;DR: 本文对Llama-3.1模型进行微调，创建了20个专门用于高能理论物理的LLM变体，并在相关摘要补全任务中超越了基础模型和商业LLM。


<details>
  <summary>Details</summary>
Motivation: 旨在开发专门服务于高能理论物理领域的LLM，并探索其优化策略，以期在专业领域任务中提升性能。

Method: 基于80亿参数的Llama-3.1模型，使用两种不同的低秩适应（LoRA）方法和不同数据集大小进行微调。训练数据源自arXiv中hep-th、hep-ph、gr-qc等高能物理相关类别摘要，并加入了q-bio和cs等跨领域摘要进行对比研究。模型性能在hep-th摘要补全任务上与基础模型及主流商业LLM（ChatGPT、Claude、Gemini、DeepSeek）进行了比较。

Result: 所有微调后的模型在hep-th摘要补全任务上均优于基础Llama-3.1模型。研究还为高能理论物理领域专业语言模型的进一步开发提供了有价值的见解。

Conclusion: 通过对通用LLM进行领域特定摘要的微调，能够有效构建高能理论物理领域的专业语言模型，这些模型在特定任务上表现出色，并为未来该领域专业模型的优化与发展指明了方向。

Abstract: We present specialized Large Language Models for theoretical High-Energy
Physics, obtained as 20 fine-tuned variants of the 8-billion parameter
Llama-3.1 model. Each variant was trained on arXiv abstracts (through August
2024) from different combinations of hep-th, hep-ph and gr-qc. For a
comparative study, we also trained models on datasets that contained abstracts
from disparate fields such as the q-bio and cs categories. All models were
fine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and
varying dataset sizes, and outperformed the base model on hep-th abstract
completion tasks. We compare performance against leading commercial LLMs
(ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing
specialized language models for High-Energy Theoretical Physics.

</details>


### [3] [Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering](https://arxiv.org/abs/2508.03719)
*Abhay Vijayvargia,Ajay Nagpal,Kundeshwar Pundalik,Atharva Savarkar,Smita Gautam,Pankaj Singh,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 本文介绍了一种名为Krishi Sathi的AI农业聊天机器人，旨在通过个性化、易于理解的文本和语音回复，为印度农民提供及时、可及的农业建议。


<details>
  <summary>Details</summary>
Motivation: 印度农民，特别是农村地区文化水平较低的农民，缺乏及时、可及且语言友好的农业建议。

Method: 开发了基于IFT模型并经过印度农业知识微调的AI聊天机器人Krishi Sathi。系统采用结构化的多轮对话流程来充分理解用户查询，并结合检索增强生成（RAG）技术，从农业数据库中检索信息并生成定制化回复。为提高可及性，支持英语和印地语，并提供语音输入/输出（ASR和TTS）功能。

Result: Krishi Sathi在查询响应准确率和完成率上均达到97.53%，上下文相关性和个性化方面达到91.35%。平均响应时间低于6秒，确保了及时性。

Conclusion: 该研究表明，结合意图驱动的对话流、指令调优模型和检索增强生成技术，可以显著提高印度数字农业支持的质量和可及性。

Abstract: Indian farmers often lack timely, accessible, and language-friendly
agricultural advice, especially in rural areas with low literacy. To address
this gap in accessibility, this paper presents a novel AI-powered agricultural
chatbot, Krishi Sathi, designed to support Indian farmers by providing
personalized, easy-to-understand answers to their queries through both text and
speech. The system's intelligence stems from an IFT model, subsequently refined
through fine-tuning on Indian agricultural knowledge across three curated
datasets. Unlike traditional chatbots that respond to one-off questions, Krishi
Sathi follows a structured, multi-turn conversation flow to gradually collect
the necessary details from the farmer, ensuring the query is fully understood
before generating a response. Once the intent and context are extracted, the
system performs Retrieval-Augmented Generation (RAG) by first fetching
information from a curated agricultural database and then generating a tailored
response using the IFT model. The chatbot supports both English and Hindi
languages, with speech input and output features (via ASR and TTS) to make it
accessible for users with low literacy or limited digital skills. This work
demonstrates how combining intent-driven dialogue flows, instruction-tuned
models, and retrieval-based generation can improve the quality and
accessibility of digital agricultural support in India.
  This approach yielded strong results, with the system achieving a query
response accuracy of 97.53%, 91.35% contextual relevance and personalization,
and a query completion rate of 97.53%. The average response time remained under
6 seconds, ensuring timely support for users across both English and Hindi
interactions.

</details>


### [4] [Hierarchical Verification of Speculative Beams for Accelerating LLM Inference](https://arxiv.org/abs/2508.03726)
*Jaydip Sen,Harshitha Puvvala,Subhasis Dasgupta*

Main category: cs.CL

TL;DR: 该论文提出了一种名为分层验证树（HVT）的新框架，通过优先处理高可能性草稿并早期剪枝次优候选，重构了推测性波束解码，从而显著提高了大型语言模型的推理效率，同时保持或提升了输出质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）由于其自回归特性，在推理效率方面面临持续挑战。传统的推测解码和波束采样方法按顺序验证草稿序列，缺乏优先级，导致不必要的计算开销。

Method: 本文提出分层验证树（HVT），一种重构推测性波束解码的新框架。它通过优先处理高可能性草稿和早期剪枝次优候选来提高效率，并开发了理论基础和形式化的验证-剪枝算法，以确保正确性和效率。HVT无需重新训练或修改架构即可与标准LLM推理管线集成。

Result: 实验评估表明，HVT在多个数据集和模型上持续优于现有推测解码方案。它显著减少了推理时间和能耗，同时保持或提升了输出质量。

Conclusion: 研究结果强调了分层验证策略作为加速大型语言模型推理新方向的潜力。

Abstract: Large language models (LLMs) have achieved remarkable success across diverse
natural language processing tasks but face persistent challenges in inference
efficiency due to their autoregressive nature. While speculative decoding and
beam sampling offer notable improvements, traditional methods verify draft
sequences sequentially without prioritization, leading to unnecessary
computational overhead. This work proposes the Hierarchical Verification Tree
(HVT), a novel framework that restructures speculative beam decoding by
prioritizing high-likelihood drafts and enabling early pruning of suboptimal
candidates. Theoretical foundations and a formal verification-pruning algorithm
are developed to ensure correctness and efficiency. Integration with standard
LLM inference pipelines is achieved without requiring retraining or
architecture modification. Experimental evaluations across multiple datasets
and models demonstrate that HVT consistently outperforms existing speculative
decoding schemes, achieving substantial reductions in inference time and energy
consumption while maintaining or enhancing output quality. The findings
highlight the potential of hierarchical verification strategies as a new
direction for accelerating large language model inference.

</details>


### [5] [WINELL: Wikipedia Never-Ending Updating with LLM Agents](https://arxiv.org/abs/2508.03728)
*Revanth Gangi Reddy,Tanay Dixit,Jiaxin Qin,Cheng Qian,Daniel Lee,Jiawei Han,Kevin Small,Xing Fan,Ruhi Sarikaya,Heng Ji*

Main category: cs.CL

TL;DR: WiNELL是一个基于LLM多智能体的框架，旨在自动化更新Wikipedia内容，通过聚合信息、筛选知识并生成符合人类编辑习惯的修改建议。


<details>
  <summary>Details</summary>
Motivation: Wikipedia依赖人工编辑，内容更新面临巨大挑战。本文受NELL持续知识获取愿景和LLM代理技术进步的启发，旨在解决知识库内容实时更新的问题。

Method: 本研究采用多智能体框架，该框架负责聚合在线信息、筛选Wikipedia目标实体的新知识和重要知识，并生成精确的编辑建议供人工审核。同时，使用在Wikipedia丰富编辑历史数据上训练的细粒度编辑模型，确保更新方式符合人类编辑行为。

Result: 编辑模型在关键信息覆盖率和编辑效率方面优于开源指令遵循基线和闭源LLM（如GPT-4o）。在对活跃Wikipedia页面的端到端评估中，WiNELL展示了识别并提出及时事实更新的能力。

Conclusion: WiNELL为LLM智能体自动、持续更新知识库开辟了一个有前景的研究方向。

Abstract: Wikipedia, a vast and continuously consulted knowledge base, faces
significant challenges in maintaining up-to-date content due to its reliance on
manual human editors. Inspired by the vision of continuous knowledge
acquisition in NELL and fueled by advances in LLM-based agents, this paper
introduces WiNELL, an agentic framework for continuously updating Wikipedia
articles. Our approach employs a multi-agent framework to aggregate online
information, select new and important knowledge for a target entity in
Wikipedia, and then generate precise edit suggestions for human review. Our
fine-grained editing models, trained on Wikipedia's extensive history of human
edits, enable incorporating updates in a manner consistent with human editing
behavior. Our editor models outperform both open-source instruction-following
baselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and
editing efficiency. End-to-end evaluation on high-activity Wikipedia pages
demonstrates WiNELL's ability to identify and suggest timely factual updates.
This opens up a promising research direction in LLM agents for automatically
updating knowledge bases in a never-ending fashion.

</details>


### [6] [GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models](https://arxiv.org/abs/2508.03737)
*Ashutosh Bandooni,Brindha Subburaj*

Main category: cs.CL

TL;DR: 本文提出了一个名为 GanitBench 的双语（英语和印地语）数学视觉语言模型（VLM）基准测试，用于评估模型在图像数学推理方面的表现，并发现现有模型在印地语和复杂约束下的表现有待提高。


<details>
  <summary>Details</summary>
Motivation: 现有VLM推理评估基准多为单语（主要是英语），且在印地语等其他语言中，除了理解和翻译任务外，缺乏数据集。尤其是在数学推理等复杂任务上，印地语数据集更是稀缺。

Method: 研究者创建了 GanitBench，一个包含1527个视觉图像数学问题的难度基准，这些问题来自印度JEE Advanced和CBSE Boards考试。该基准提供英语和印地语版本。他们使用零样本思维链（CoT）和两样本思维链（CoT）设置，评估了GPT-4o mini等两个闭源模型，并引入了“双重锁定”约束进行额外评估。

Result: GPT-4o mini在GanitBench上表现最佳，最高平均准确率为38.15%。“双重锁定”约束显著降低了模型的性能，而两样本CoT在该环境下更为有效。模型在回答印地语问题时，性能有所下降。

Conclusion: 当前VLM在处理视觉数学推理任务时仍面临挑战，尤其是在多语言（如印地语）和复杂约束条件下。本工作旨在推动印地语等语言在VLM研究中的包容性，并指出需要更多努力来提高模型在这些场景下的性能。

Abstract: Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on
several fields and domains are being curated more frequently over the last few
years. However these are often monolingual, mostly available in English.
Additionally there also is a lack of datasets available in Hindi on tasks apart
from comprehension and translation. We introduce GanitBench, a tough benchmark
consisting of 1527 vision-only questions covering several topics in Mathematics
- available in languages English and Hindi. Collected from two major
examinations from India, the JEE Advanced and the CBSE Boards examinations,
this benchmark includes questions in the form of images comprising of figures
essential to a question as well as text. We evaluate two closed source models
for the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings.
GPT-4o mini is found to be the more dominant model on the benchmark, with it's
highest average accuracy being 38.15%. We also evaluate models through a
"Double Lock" constraint, which brings down the performance of the models by
considerable margins. We observe that two-shot CoT appears to be a more
effective setting under this environment. Performance of the two VLMs also
decreases when answering the same questions in the Hindi language. We hope to
facilitate the inclusion of languages like Hindi in research through our work.

</details>


### [7] [AttnTrace: Attention-based Context Traceback for Long-Context LLMs](https://arxiv.org/abs/2508.03793)
*Yanting Wang,Runpeng Geng,Ying Chen,Jinyuan Jia*

Main category: cs.CL

TL;DR: 本文提出AttnTrace，一种基于注意力权重的长上下文大语言模型上下文溯源方法，它在准确性和效率上均优于现有方法，并能有效提升提示注入检测能力。


<details>
  <summary>Details</summary>
Motivation: 长上下文大语言模型在高级AI系统中应用广泛，但现有上下文溯源方法计算成本高昂（如TracLLM耗时数百秒），尽管上下文溯源对于事后取证分析、提升LLM输出的可解释性和可信度具有重要的实际应用价值。

Method: 本研究提出AttnTrace，一种基于LLM生成的注意力权重的上下文溯源新方法。为有效利用注意力权重，文中引入了两项旨在增强AttnTrace有效性的技术，并提供了其设计选择的理论见解。

Result: 通过系统评估，AttnTrace在上下文溯源的准确性和效率方面均超越了现有最先进的方法。此外，AttnTrace能通过“归因-检测前”范式，提高现有方法在长上下文下检测提示注入的能力。作为一个实际应用，AttnTrace能有效识别旨在操纵LLM生成评论的论文中注入的指令。

Conclusion: AttnTrace提供了一种更准确、更高效的上下文溯源解决方案，显著优于现有技术，并在提升LLM输出可信度、进行提示注入检测和法证分析等实际应用中展现出巨大潜力。

Abstract: Long-context large language models (LLMs), such as Gemini-2.5-Pro and
Claude-Sonnet-4, are increasingly used to empower advanced AI systems,
including retrieval-augmented generation (RAG) pipelines and autonomous agents.
In these systems, an LLM receives an instruction along with a context--often
consisting of texts retrieved from a knowledge database or memory--and
generates a response that is contextually grounded by following the
instruction. Recent studies have designed solutions to trace back to a subset
of texts in the context that contributes most to the response generated by the
LLM. These solutions have numerous real-world applications, including
performing post-attack forensic analysis and improving the interpretability and
trustworthiness of LLM outputs. While significant efforts have been made,
state-of-the-art solutions such as TracLLM often lead to a high computation
cost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a
single response-context pair. In this work, we propose AttnTrace, a new context
traceback method based on the attention weights produced by an LLM for a
prompt. To effectively utilize attention weights, we introduce two techniques
designed to enhance the effectiveness of AttnTrace, and we provide theoretical
insights for our design choice. We also perform a systematic evaluation for
AttnTrace. The results demonstrate that AttnTrace is more accurate and
efficient than existing state-of-the-art context traceback methods. We also
show that AttnTrace can improve state-of-the-art methods in detecting prompt
injection under long contexts through the attribution-before-detection
paradigm. As a real-world application, we demonstrate that AttnTrace can
effectively pinpoint injected instructions in a paper designed to manipulate
LLM-generated reviews. The code is at
https://github.com/Wang-Yanting/AttnTrace.

</details>


### [8] [Majority Bit-Aware Watermarking For Large Language Models](https://arxiv.org/abs/2508.03829)
*Jiahao Xu,Rui Hu,Zikai Zhang*

Main category: cs.CL

TL;DR: 针对LLM多比特水印技术中内容质量与解码精度之间的权衡问题，本文提出了MajorMark及其增强版MajorMark+，通过基于多数比特的编码和聚类解码策略，显著提升了水印文本的生成质量和解码精度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在实际应用中日益普及，其生成有害或欺骗性内容的潜在滥用引起担忧。水印技术作为一种通过嵌入可识别信息来验证内容来源和追踪滥用的方法，备受关注。然而，现有多比特水印方案普遍面临内容质量与解码精度之间的权衡：为确保可靠解码，必须限制优选词元集大小，但这会降低生成内容质量。

Method: 本文提出了MajorMark，一种新型水印方法。它通过基于多数比特感知的编码来改进权衡，根据消息的多数比特选择优选词元集，从而允许更大、更灵活的词元采样。与依赖词元频率分析的先前方法不同，MajorMark采用聚类解码策略，即使优选词元集较大也能保持高解码精度。此外，本文还引入了MajorMark$^+$，它将消息划分为多个块，独立编码并确定性解码每个块，进一步提升了水印文本质量和解码精度。

Result: 在最先进的LLMs上进行的广泛实验表明，我们的方法显著增强了解码精度和文本生成质量，表现优于现有的多比特水印基线方法。

Conclusion: MajorMark及其改进版MajorMark$^+$成功解决了LLM多比特水印技术中内容质量与解码精度之间的核心权衡问题，通过创新的编码和解码策略，在保持高质量生成文本的同时，实现了高精度水印信息解码，为LLM内容溯源和滥用追踪提供了更有效的解决方案。

Abstract: The growing deployment of Large Language Models (LLMs) in real-world
applications has raised concerns about their potential misuse in generating
harmful or deceptive content. To address this issue, watermarking techniques
have emerged as a promising solution by embedding identifiable binary messages
into generated text for origin verification and misuse tracing. While recent
efforts have explored multi-bit watermarking schemes capable of embedding rich
information such as user identifiers, they typically suffer from the
fundamental trade-off between text quality and decoding accuracy: to ensure
reliable message decoding, they have to restrict the size of preferred token
sets during encoding, yet such restrictions reduce the quality of the generated
content. In this work, we propose MajorMark, a novel watermarking method that
improves this trade-off through majority bit-aware encoding. MajorMark selects
preferred token sets based on the majority bit of the message, enabling a
larger and more flexible sampling of tokens. In contrast to prior methods that
rely on token frequency analysis for decoding, MajorMark employs a
clustering-based decoding strategy, which maintains high decoding accuracy even
when the preferred token set is large, thus preserving both content quality and
decoding accuracy. We further introduce MajorMark$^+$, which partitions the
message into multiple blocks to independently encode and deterministically
decode each block, thereby further enhancing the quality of watermarked text
and improving decoding accuracy. Extensive experiments on state-of-the-art LLMs
demonstrate that our methods significantly enhance both decoding accuracy and
text generation quality, outperforming prior multi-bit watermarking baselines.

</details>


### [9] [Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models](https://arxiv.org/abs/2508.03860)
*Subhey Sadi Rahman,Md. Adnanul Islam,Md. Mahbub Alam,Musarrat Zeba,Md. Abdur Rahman,Sadia Sultana Chowa,Mohaimenul Azam Khan Raiaan,Sami Azam*

Main category: cs.CL

TL;DR: 鉴于大型语言模型（LLM）可能生成错误信息，本文系统回顾了LLM内容事实准确性评估的挑战与方法，并强调了通过高级策略（如RAG、领域微调）构建更可信、领域定制化LLM的重要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在包含不准确或误导性内容的互联网语料库上进行训练，导致其可能生成错误信息，因此急需健壮的事实核查机制。

Method: 本文对2020年至2025年的相关文献进行了系统回顾，分析了LLM生成内容在事实准确性评估方面的主要挑战（如幻觉、数据集限制、评估指标可靠性），并探讨了包括高级提示策略、领域特定微调、检索增强生成（RAG）、指令微调和多智能体推理等在内的缓解技术。

Result: 研究发现当前评估指标存在局限性；强调了通过经验证的外部证据来验证LLM输出的价值；并指出领域特定定制对于提高事实一致性的重要性。

Conclusion: 构建不仅准确、可解释，而且针对特定领域事实核查定制的LLM至关重要，这将有助于推动研究，实现更值得信赖和语境感知的语言模型。

Abstract: Large Language Models (LLMs) are trained on vast and diverse internet corpora
that often include inaccurate or misleading content. Consequently, LLMs can
generate misinformation, making robust fact-checking essential. This review
systematically analyzes how LLM-generated content is evaluated for factual
accuracy by exploring key challenges such as hallucinations, dataset
limitations, and the reliability of evaluation metrics. The review emphasizes
the need for strong fact-checking frameworks that integrate advanced prompting
strategies, domain-specific fine-tuning, and retrieval-augmented generation
(RAG) methods. It proposes five research questions that guide the analysis of
the recent literature from 2020 to 2025, focusing on evaluation methods and
mitigation techniques. The review also discusses the role of instruction
tuning, multi-agent reasoning, and external knowledge access via RAG
frameworks. Key findings highlight the limitations of current metrics, the
value of grounding outputs with validated external evidence, and the importance
of domain-specific customization to improve factual consistency. Overall, the
review underlines the importance of building LLMs that are not only accurate
and explainable but also tailored for domain-specific fact-checking. These
insights contribute to the advancement of research toward more trustworthy and
context-aware language models.

</details>


### [10] [An Entity Linking Agent for Question Answering](https://arxiv.org/abs/2508.03865)
*Yajie Luo,Yihong Wu,Muzhi Li,Fengran Mo,Jia Ao Sun,Xinyu Wang,Liheng Ma,Yingxue Zhang,Jian-Yun Nie*

Main category: cs.CL

TL;DR: 本文提出一个基于大型语言模型（LLM）的实体链接代理，用于解决问答（QA）系统中短而模糊问题的实体链接挑战，并经验证其鲁棒性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现有实体链接方法主要针对长上下文设计，在处理问答（QA）任务中短小、模糊的用户问题时表现不佳，难以准确链接自然语言提及到知识库条目。

Method: 提出一个基于大型语言模型（LLM）的实体链接代理，该代理模拟人类认知工作流程，能主动识别实体提及、检索候选实体并做出决策。

Result: 通过工具化实体链接和问答任务评估两项实验，验证了所提出代理的鲁棒性和有效性。

Conclusion: 所提出的基于LLM的实体链接代理能够有效解决QA任务中短而模糊问题的实体链接挑战。

Abstract: Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide
accurate answers. Entity Linking (EL) plays a critical role in linking natural
language mentions to KB entries. However, most existing EL methods are designed
for long contexts and do not perform well on short, ambiguous user questions in
QA tasks. We propose an entity linking agent for QA, based on a Large Language
Model that simulates human cognitive workflows. The agent actively identifies
entity mentions, retrieves candidate entities, and makes decision. To verify
the effectiveness of our agent, we conduct two experiments: tool-based entity
linking and QA task evaluation. The results confirm the robustness and
effectiveness of our agent.

</details>


### [11] [Sotopia-RL: Reward Design for Social Intelligence](https://arxiv.org/abs/2508.03905)
*Haofei Yu,Zhengyang Qi,Yining Zhao,Kolby Nottingham,Keyang Xuan,Bodhisattwa Prasad Majumder,Hao Zhu,Paul Pu Liang,Jiaxuan You*

Main category: cs.CL

TL;DR: 本文提出Sotopia-RL框架，通过话语级、多维度奖励解决了强化学习训练社会智能LLM时面临的观测不全和多维度问题，并在Sotopia环境中取得了最先进的社交目标完成分数。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的社会智能至关重要，但强化学习（RL）在社交互动中面临两大挑战：1) 部分可观察性，导致信用分配困难；2) 多维度性，使得行为对目标贡献间接。这些使传统基于MDP的RL低效且不稳定。

Method: 提出Sotopia-RL框架，将粗粒度的回合级反馈细化为话语级（utterance-level）和多维度（multi-dimensional）奖励。话语级信用分配缓解部分可观察性，多维度奖励捕捉社交互动丰富性并减少奖励欺骗。

Result: 在Sotopia社交学习环境中，Sotopia-RL在Sotopia-hard上取得7.17分，在Sotopia-full上取得8.31分的SOTA社交目标完成分数，显著优于现有方法。消融研究证实了话语级信用分配和多维度奖励设计的必要性。

Conclusion: Sotopia-RL通过创新性的奖励设计，有效克服了强化学习在训练社会智能LLM时的挑战，显著提升了LLM在复杂社交任务中的表现。

Abstract: Social intelligence has become a critical capability for large language
models (LLMs), enabling them to engage effectively in real-world social tasks
such as accommodation, persuasion, collaboration, and negotiation.
Reinforcement learning (RL) is a natural fit for training socially intelligent
agents because it allows models to learn sophisticated strategies directly
through social interactions. However, social interactions have two key
characteristics that set barriers for RL training: (1) partial observability,
where utterances have indirect and delayed effects that complicate credit
assignment, and (2) multi-dimensionality, where behaviors such as
rapport-building or knowledge-seeking contribute indirectly to goal
achievement. These characteristics make Markov decision process (MDP)-based RL
with single-dimensional episode-level rewards inefficient and unstable. To
address these challenges, we propose Sotopia-RL, a novel framework that refines
coarse episode-level feedback into utterance-level, multi-dimensional rewards.
Utterance-level credit assignment mitigates partial observability by
attributing outcomes to individual utterances, while multi-dimensional rewards
capture the full richness of social interactions and reduce reward hacking.
Experiments in Sotopia, an open-ended social learning environment, demonstrate
that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17
on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing
approaches. Ablation studies confirm the necessity of both utterance-level
credit assignment and multi-dimensional reward design for RL training. Our
implementation is publicly available at:
https://github.com/sotopia-lab/sotopia-rl.

</details>


### [12] [CoAct-1: Computer-using Agents with Coding as Actions](https://arxiv.org/abs/2508.03923)
*Linxin Song,Yutong Dai,Viraj Prabhu,Jieyu Zhang,Taiwei Shi,Li Li,Junnan Li,Silvio Savarese,Zeyuan Chen,Jieyu Zhao,Ran Xu,Caiming Xiong*

Main category: cs.CL

TL;DR: CoAct-1是一个结合GUI操作和编程执行的多智能体系统，通过允许代理编写和执行代码，显著提升了计算机自动化代理在复杂任务上的效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的图形用户界面（GUI）操作计算机的自主代理在复杂、长周期任务中效率低下且可靠性不足，因为它们仅依赖GUI操作，导致脆弱性和低效。

Method: 提出CoAct-1，一个新颖的多智能体系统，它将GUI控制与直接程序化执行（编程）结合。系统包含一个协调器，动态地将子任务分配给传统的GUI操作员或专门的编程代理（可编写和执行Python/Bash脚本），从而实现混合操作以绕过低效的GUI序列。

Result: 在OSWorld基准测试中，CoAct-1达到了60.76%的SOTA成功率，显著优于先前方法。同时，它将完成任务所需的平均步骤数从领先GUI代理的15步减少到仅10.15步，大幅提高了效率。

Conclusion: 将编程作为核心行动整合，为实现通用计算机自动化提供了一条更强大、高效和可扩展的途径。

Abstract: Autonomous agents that operate computers via Graphical User Interfaces (GUIs)
often struggle with efficiency and reliability on complex, long-horizon tasks.
While augmenting these agents with planners can improve task decomposition,
they remain constrained by the inherent limitations of performing all actions
through GUI manipulation, leading to brittleness and inefficiency. In this
work, we introduce a more robust and flexible paradigm: enabling agents to use
coding as a enhanced action. We present CoAct-1, a novel multi-agent system
that synergistically combines GUI-based control with direct programmatic
execution. CoAct-1 features an Orchestrator that dynamically delegates subtasks
to either a conventional GUI Operator or a specialized Programmer agent, which
can write and execute Python or Bash scripts. This hybrid approach allows the
agent to bypass inefficient GUI action sequences for tasks like file management
and data processing, while still leveraging visual interaction when necessary.
We evaluate our system on the challenging OSWorld benchmark, where CoAct-1
achieves a new state-of-the-art success rate of 60.76%, significantly
outperforming prior methods. Furthermore, our approach dramatically improves
efficiency, reducing the average number of steps required to complete a task to
just 10.15, compared to 15 for leading GUI agents. Our results demonstrate that
integrating coding as a core action provides a more powerful, efficient, and
scalable path toward generalized computer automation.

</details>


### [13] [CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation](https://arxiv.org/abs/2508.03935)
*Raymond Wilson,Cole Graham,Chase Carter,Zefeng Yang,Ruiqi Gu*

Main category: cs.CL

TL;DR: 本文提出CAP-LLM，一个利用大语言模型（LLM）生成个性化且事实一致新闻标题的框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 在信息过载时代，个性化新闻标题对吸引用户至关重要，但现有方法难以有效捕捉用户兴趣并确保事实一致性，常导致标题过于笼统或具有误导性。

Method: 本文提出Context-Augmented Personalized LLM (CAP-LLM)，将用户偏好和事实一致性约束整合到预训练LLM中。它包含：用户偏好编码器（捕捉长期兴趣）、上下文注入适配器（整合偏好和文章上下文）和事实一致性强化模块（使用对比损失减少幻觉）。

Result: 在PENS数据集上，CAP-LLM表现出最先进的性能，在所有指标上均有提升。相比BART等基线，其事实一致性（FactCC 87.50）显著提高，同时增强了个性化（Pc(avg) 2.73, Pc(max) 17.25）和内容覆盖（ROUGE-1 26.55, ROUGE-2 9.95, ROUGE-L 23.01）。消融研究、人工评估和敏感性分析进一步验证了其有效性。

Conclusion: CAP-LLM能够更好地平衡新闻标题生成中的个性化与事实准确性，具有出色的表现和鲁棒性。

Abstract: In the era of information overload, personalized news headline generation is
crucial for engaging users by tailoring content to their preferences while
accurately conveying news facts. Existing methods struggle with effectively
capturing complex user interests and ensuring factual consistency, often
leading to generic or misleading headlines. Leveraging the unprecedented
capabilities of Large Language Models (LLMs) in text generation, we propose
Context-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates
user preferences and factual consistency constraints into a powerful
pre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture
long-term user interests, a Context Injection Adapter to seamlessly integrate
these preferences and current article context into the LLM's generation
process, and a Fact-Consistency Reinforcement Module employing a novel
contrastive loss to mitigate hallucination. Evaluated on the real-world PENS
dataset, CAP-LLM achieves state-of-the-art performance across all metrics.
Notably, it significantly improves factual consistency (FactCC of 87.50) over
strong baselines like BART (86.67), while simultaneously enhancing
personalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1
26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,
and sensitivity analyses further validate the effectiveness of each component
and the robustness of our approach, demonstrating CAP-LLM's ability to achieve
a superior balance between personalization and factual accuracy in news
headline generation.

</details>


### [14] [Data and AI governance: Promoting equity, ethics, and fairness in large language models](https://arxiv.org/abs/2508.03970)
*Alok Abhishek,Lisa Erickson,Tushar Bandopadhyay*

Main category: cs.CL

TL;DR: 本文探讨了在机器学习模型（特别是大型语言模型）的整个生命周期中系统地治理、评估和量化偏差的方法，并提出了一种实用的数据和AI治理框架，以增强生成式AI系统的安全性与责任性。


<details>
  <summary>Details</summary>
Motivation: 识别并解决大型语言模型中普遍存在的偏差和公平性问题，提升生成式AI系统的安全性和责任感，规避歧视风险及潜在声誉损害，从而促进创建和部署符合社会责任和道德标准的生成式AI应用。

Method: 基于大型语言模型偏差评估和测试套件（BEATS）的基础工作，提出了一种数据和AI治理框架。该框架适用于实际应用，能对LLM进行严格基准测试、持续实时评估，并主动管理LLM生成响应，覆盖从开发到生产监控的整个生命周期。

Result: 通过实施所讨论的数据和AI治理方法，组织可以显著提高其生成式AI系统的安全性与责任性，有效缓解歧视风险，并保护免受潜在的声誉或品牌相关损害。

Conclusion: 本文旨在通过系统化地治理和评估偏差，为创建和部署具有社会责任感和道德一致性的生成式人工智能应用做出贡献。

Abstract: In this paper, we cover approaches to systematically govern, assess and
quantify bias across the complete life cycle of machine learning models, from
initial development and validation to ongoing production monitoring and
guardrail implementation. Building upon our foundational work on the Bias
Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the
authors share prevalent bias and fairness related gaps in Large Language Models
(LLMs) and discuss data and AI governance framework to address Bias, Ethics,
Fairness, and Factuality within LLMs. The data and AI governance approach
discussed in this paper is suitable for practical, real-world applications,
enabling rigorous benchmarking of LLMs prior to production deployment,
facilitating continuous real-time evaluation, and proactively governing LLM
generated responses. By implementing the data and AI governance across the life
cycle of AI development, organizations can significantly enhance the safety and
responsibility of their GenAI systems, effectively mitigating risks of
discrimination and protecting against potential reputational or brand-related
harm. Ultimately, through this article, we aim to contribute to advancement of
the creation and deployment of socially responsible and ethically aligned
generative artificial intelligence powered applications.

</details>


### [15] [Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency](https://arxiv.org/abs/2508.03979)
*Md Arafat Sultan,Ramón Fernandez Astudillo*

Main category: cs.CL

TL;DR: 提出一种早期剪枝方法，通过模型置信度和词汇覆盖率，提升自洽性在长链式思维任务中的令牌效率，同时保留并行性。


<details>
  <summary>Details</summary>
Motivation: 自洽性虽然简单有效，但其高令牌消耗限制了其实用性，尤其在长链式思维推理任务中。

Method: 通过早期假设剪枝，在并行生成所有解决方案的同时，定期根据模型的置信度以及候选子集对当前所有假设的词汇覆盖率，剪除不必要的中间假设。为此，设计了一个利用这两个指标的快速加权集合覆盖算法。

Result: 在三个数学基准上对五种大型语言模型（LLMs）进行评估，结果显示该方法可以提高所有模型的令牌效率，在许多情况下提升10%-35%。

Conclusion: 所提出的早期剪枝方法能够有效提高自洽性的令牌效率，并保持其并行处理能力，使其更具实用性。

Abstract: Despite its simplicity and efficacy, the high token expenditure of
self-consistency can limit its practical utility. Here we investigate if
self-consistency can be made more token-efficient for long chain-of-thought
reasoning tasks, while preserving its parallelism, through early hypothesis
pruning. Concretely, we generate all solutions in parallel, but periodically
prune intermediate hypotheses that are deemed unnecessary based on two
lightweight indicators: (a) the model's own confidence in individual
hypotheses, and (b) lexical coverage of all current hypotheses by candidate
subsets that are under consideration for continued retention. We design a fast
weighted set cover algorithm that utilizes the two indicators; our evaluation
of five LLMs on three math benchmarks shows that this method can improve token
efficiency for all models, by 10-35% in many cases.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [16] [Text2VR: Automated instruction Generation in Virtual Reality using Large language Models for Assembly Task](https://arxiv.org/abs/2508.03699)
*Subin Raj Peter*

Main category: cs.CV

TL;DR: 本文提出一种利用大型语言模型（LLMs）自动化从文本生成VR虚拟指令的新方法，以解决VR培训内容开发耗时耗力的问题，从而提升训练效率并降低开发成本。


<details>
  <summary>Details</summary>
Motivation: 尽管VR在劳动力培训中具有沉浸、互动、无风险等优势，但开发VR培训应用面临创建准确且引人入胜的教学内容所需的时间、专业知识和资源方面的重大挑战。

Method: 该研究提出一个系统，包含两个核心组件：1) LLM模块，负责从文本中提取任务相关信息；2) 智能模块，将LLM提取的信息转换为VR环境中的动画演示和视觉提示。智能模块中的指令生成器利用数据库数据，通过改变虚拟对象颜色和创建动画来生成训练内容。

Result: 所提出的方法能够显著提升训练效果并减少开发开销。

Conclusion: 该方法使基于VR的培训更具可扩展性和适应性，能够满足不断变化的工业需求。

Abstract: Virtual Reality (VR) has emerged as a powerful tool for workforce training,
offering immersive, interactive, and risk-free environments that enhance skill
acquisition, decision-making, and confidence. Despite its advantages,
developing VR applications for training remains a significant challenge due to
the time, expertise, and resources required to create accurate and engaging
instructional content. To address these limitations, this paper proposes a
novel approach that leverages Large Language Models (LLMs) to automate the
generation of virtual instructions from textual input. The system comprises two
core components: an LLM module that extracts task-relevant information from the
text, and an intelligent module that transforms this information into animated
demonstrations and visual cues within a VR environment. The intelligent module
receives input from the LLM module and interprets the extracted information.
Based on this, an instruction generator creates training content using relevant
data from a database. The instruction generator generates the instruction by
changing the color of virtual objects and creating animations to illustrate
tasks. This approach enhances training effectiveness and reduces development
overhead, making VR-based training more scalable and adaptable to evolving
industrial needs.

</details>


### [17] [Outlier Detection Algorithm for Circle Fitting](https://arxiv.org/abs/2508.03720)
*Ahmet Gökhan Poyraz*

Main category: cs.CV

TL;DR: 本文提出一种基于极坐标的异常点检测（PCOD）算法，旨在提高噪声数据下圆拟合的精度，并通过工业应用验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 圆拟合方法在工业应用中广泛使用，但其在处理噪声点集时效果会显著降低，因此需要开发有效的异常点检测和移除算法来提升拟合精度。

Method: 提出极坐标异常点检测（PCOD）算法。该方法首先将点集转换为极坐标，然后计算局部和全局标准差，并通过比较局部均值与全局标准差来识别异常点。研究以工业垫片高精度直径测量为例，通过机器视觉系统获取图像，经过亚像素边缘检测后，利用PCOD算法清洗边缘点，再进行圆拟合。

Result: 实验结果表明，与十种不同的圆拟合算法和五种异常点检测方法相比，所提出的PCOD算法在数据集的精度方面表现最佳。

Conclusion: PCOD算法能有效提高工业环境下圆拟合应用的精度，展示了其在处理噪声数据方面的巨大潜力。

Abstract: Circle fitting methods are extensively utilized in various industries,
particularly in quality control processes and design applications. The
effectiveness of these algorithms can be significantly compromised when the
point sets to be predicted are noisy. To mitigate this issue, outlier detection
and removal algorithms are often applied before the circle fitting procedure.
This study introduces the Polar Coordinate-Based Outlier Detection (PCOD)
algorithm, which can be effectively employed in circle fitting applications. In
the proposed approach, the point set is first transformed into polar
coordinates, followed by the calculation of both local and global standard
deviations. Outliers are then identified by comparing local mean values with
the global standard deviation. The practicality and efficiency of the proposed
method are demonstrated by focusing on the high-precision diameter measurement
of industrial washer parts. Images from a machine vision system are processed
through preprocessing steps, including sub-pixel edge detection. The resulting
sub-pixel edge points are then cleaned using the proposed outlier detection and
removal algorithm, after which circle fitting is performed. A comparison is
made using ten different circle fitting algorithms and five distinct outlier
detection methods. The results indicate that the proposed method outperforms
the other approaches, delivering the best performance in terms of accuracy
within the dataset, thereby demonstrating its potential for enhancing circle
fitting applications in industrial environments.

</details>


### [18] [Enhancing Diameter Measurement Accuracy in Machine Vision Applications](https://arxiv.org/abs/2508.03721)
*Ahmet Gokhan Poyraz,Ahmet Emir Dirik,Hakan Gurkan,Mehmet Kacmaz*

Main category: cs.CV

TL;DR: 本研究提出两种新方法，通过利用少量已知参考部件，将相机测量系统中不同直径部件的测量误差从13-114微米显著降低至1-2微米，从而提高测量精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管相机测量系统使用了远心镜头等专业设备，但由于机械和软件因素，仍会产生测量误差，尤其是在同一设置下测量不同直径部件时，误差更为明显，因此需要提高测量精度。

Method: 本研究提出两种创新方法以提高测量精度，均利用多个已知参考部件：1. 基于转换因子的方法：从已知参考部件估算转换因子，进而计算未知部件的直径。2. 基于像素的方法：直接利用参考部件的像素直径信息估算直径。实验采用工业级相机和远心镜头进行测试。

Result: 实验结果显示，通过所提出的方法，对玻璃样本（1-12毫米）和金属工件（3-24毫米）的测量误差从原来的13-114微米成功降低至1-2微米。

Conclusion: 通过仅使用少量已知参考部件，所提出的方法能够实现相机视场内所有部件的高精度测量，显著降低误差率并提高测量可靠性，从而丰富了现有直径测量技术。

Abstract: In camera measurement systems, specialized equipment such as telecentric
lenses is often employed to measure parts with narrow tolerances. However,
despite the use of such equipment, measurement errors can occur due to
mechanical and software-related factors within the system. These errors are
particularly evident in applications where parts of different diameters are
measured using the same setup. This study proposes two innovative approaches to
enhance measurement accuracy using multiple known reference parts: a conversion
factor-based method and a pixel-based method. In the first approach, the
conversion factor is estimated from known references to calculate the diameter
(mm) of the unknown part. In the second approach, the diameter (mm) is directly
estimated using pixel-based diameter information from the references. The
experimental setup includes an industrial-grade camera and telecentric lenses.
Tests conducted on glass samples (1-12 mm) and metal workpieces (3-24 mm) show
that measurement errors, which originally ranged from 13-114 micrometers, were
reduced to 1-2 micrometers using the proposed methods. By utilizing only a few
known reference parts, the proposed approach enables high-accuracy measurement
of all parts within the camera's field of view. Additionally, this method
enhances the existing diameter measurement literature by significantly reducing
error rates and improving measurement reliability.

</details>


### [19] [Multimodal Video Emotion Recognition with Reliable Reasoning Priors](https://arxiv.org/abs/2508.03722)
*Zhepeng Wang,Yingjian Zhu,Guanghao Dong,Hongzhu Yi,Feng Chen,Xinming Wang,Jun Xie*

Main category: cs.CV

TL;DR: 本研究通过整合多模态大语言模型（MLLMs）的可信先验推理知识和引入平衡双对比学习，显著提升了多模态情感识别的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在将MLLMs的可信先验推理知识融入多模态情感识别，并解决该领域普遍存在的类别不平衡问题。

Method: 利用Gemini生成细粒度、模态可分离的推理轨迹，并在融合阶段作为先验知识注入，以增强跨模态交互。同时，引入平衡双对比学习（一种损失函数）来共同平衡类间和类内分布，以缓解类别不平衡。

Result: 在MER2024基准测试中，所提出的先验增强框架取得了显著的性能提升。

Conclusion: MLLM导出的推理可靠性可以与轻量级融合网络的领域适应性协同结合，从而实现稳健、可扩展的情感识别。

Abstract: This study investigates the integration of trustworthy prior reasoning
knowledge from MLLMs into multimodal emotion recognition. We employ Gemini to
generate fine-grained, modality-separable reasoning traces, which are injected
as priors during the fusion stage to enrich cross-modal interactions. To
mitigate the pronounced class-imbalance in multimodal emotion recognition, we
introduce Balanced Dual-Contrastive Learning, a loss formulation that jointly
balances inter-class and intra-class distributions. Applied to the MER2024
benchmark, our prior-enhanced framework yields substantial performance gains,
demonstrating that the reliability of MLLM-derived reasoning can be
synergistically combined with the domain adaptability of lightweight fusion
networks for robust, scalable emotion recognition.

</details>


### [20] [From Waveforms to Pixels: A Survey on Audio-Visual Segmentation](https://arxiv.org/abs/2508.03724)
*Jia Li,Yapeng Tian*

Main category: cs.CV

TL;DR: 本文对音视频分割（AVS）领域进行了全面综述，涵盖其问题定义、方法、训练范式、性能比较、当前挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 音视频分割（AVS）作为多模态感知中的重要研究领域，旨在利用视觉和音频模态识别并分割视频中的发声物体，以实现细粒度的对象级理解。本论文旨在提供AVS领域的全面概述，分析其方法进展、当前挑战和未来趋势。

Method: 本文对AVS领域进行了全面综述，内容涵盖问题定义、基准数据集、评估指标和方法学演进。详细分析了单模态和多模态编码架构、关键音视频融合策略以及各种解码器设计。同时，考察了从全监督学习到弱监督和免训练的主要训练范式，并对AVS方法在标准基准上的性能进行了广泛比较，以突出不同架构选择、融合策略和训练范式对性能的影响。

Result: 本综述全面概述了AVS领域的问题定义、数据集、评估指标和方法学演进。通过比较分析，揭示了不同架构、融合策略和训练范式对AVS性能的关键影响。此外，识别了当前AVS面临的主要挑战，包括有限的时间建模、视觉模态偏倚、复杂环境下的鲁棒性不足以及高计算需求。

Conclusion: AVS作为多模态感知的重要方向，虽然发展迅速但仍面临多项挑战。未来的研究应聚焦于提升时间推理和多模态融合能力、利用基础模型实现更好的泛化和少样本学习、通过自监督和弱监督学习减少对标注数据的依赖，以及融入更高层级的推理能力，以构建更智能、更鲁棒的AVS系统。

Abstract: Audio-Visual Segmentation (AVS) aims to identify and segment sound-producing
objects in videos by leveraging both visual and audio modalities. It has
emerged as a significant research area in multimodal perception, enabling
fine-grained object-level understanding. In this survey, we present a
comprehensive overview of the AVS field, covering its problem formulation,
benchmark datasets, evaluation metrics, and the progression of methodologies.
We analyze a wide range of approaches, including architectures for unimodal and
multimodal encoding, key strategies for audio-visual fusion, and various
decoder designs. Furthermore, we examine major training paradigms, from fully
supervised learning to weakly supervised and training-free methods. Notably, we
provide an extensive comparison of AVS methods across standard benchmarks,
highlighting the impact of different architectural choices, fusion strategies,
and training paradigms on performance. Finally, we outline the current
challenges, such as limited temporal modeling, modality bias toward vision,
lack of robustness in complex environments, and high computational demands, and
propose promising future directions, including improving temporal reasoning and
multimodal fusion, leveraging foundation models for better generalization and
few-shot learning, reducing reliance on labeled data through selfand weakly
supervised learning, and incorporating higher-level reasoning for more
intelligent AVS systems.

</details>


### [21] [A Large Language Model Powered Integrated Circuit Footprint Geometry Understanding](https://arxiv.org/abs/2508.03725)
*Yida Wang,Taiting Lu,Runze Liu,Lanqing Yang,Yifan Yang,Zhe Chen,Yuehai Wang,Yixin Liu,Kaiyuan Lin,Xiaomeng Chen,Dian Ding,Yijie Li,Yi-Chao Chen,Yincheng Jin,Mahanth Gowda*

Main category: cs.CV

TL;DR: 针对集成电路（IC）印制电路板（PCB）封装几何的自动化标注挑战，本文提出LLM4-IC8K框架。该框架利用大型语言模型（LLM）从IC机械图纸中进行结构化几何解释，并引入ICGeo8K数据集。实验证明，LLM4-IC8K在几何感知和标注方面优于现有大型多模态模型（LMMs）。


<details>
  <summary>Details</summary>
Motivation: IC PCB封装几何标注对于定义组件与PCB布局的物理接口至关重要，但由于图纸非结构化和抽象标注，自动化解析和建模极具挑战。目前缺乏直接从IC机械图纸自动化标注的方法，且现有大型多模态模型（LMMs）在IC封装几何理解上存在严重的几何感知不准确问题。

Method: 提出LLM4-IC8K框架，将IC机械图纸视为图像，并利用LLM进行结构化几何解释。该框架模仿人类工程师的推理过程，解决感知引脚数量、计算引脚中心坐标和估计引脚尺寸三个子任务。采用两阶段训练：首先在合成IC封装图上训练LMM学习基础几何推理，然后在使用真实数据微调以提高鲁棒性和准确性。同时，构建了包含8608个样本（4138个手绘，4470个合成）的多模态数据集ICGeo8K。

Result: 初步研究揭示当前LMMs在IC封装几何理解方面存在严重的几何感知不准确性。本文提出的LLM4-IC8K模型在所设定的基准测试中，性能显著优于现有的最先进LMMs。

Conclusion: LLM4-IC8K框架通过结合LLM的结构化解释能力、模仿人类推理过程以及采用多阶段训练策略，成功解决了IC PCB封装几何的自动化标注难题，并在性能上超越了现有的大型多模态模型。

Abstract: Printed-Circuit-board (PCB) footprint geometry labeling of integrated
circuits (IC) is essential in defining the physical interface between
components and the PCB layout, requiring exceptional visual perception
proficiency. However, due to the unstructured footprint drawing and abstract
diagram annotations, automated parsing and accurate footprint geometry modeling
remain highly challenging. Despite its importance, no methods currently exist
for automated package geometry labeling directly from IC mechanical drawings.
In this paper, we first investigate the visual perception performance of Large
Multimodal Models (LMMs) when solving IC footprint geometry understanding. Our
findings reveal that current LMMs severely suffer from inaccurate geometric
perception, which hinders their performance in solving the footprint geometry
labeling problem. To address these limitations, we propose LLM4-IC8K, a novel
framework that treats IC mechanical drawings as images and leverages LLMs for
structured geometric interpretation. To mimic the step-by-step reasoning
approach used by human engineers, LLM4-IC8K addresses three sub-tasks:
perceiving the number of pins, computing the center coordinates of each pin,
and estimating the dimensions of individual pins. We present a two-stage
framework that first trains LMMs on synthetically generated IC footprint
diagrams to learn fundamental geometric reasoning and then fine-tunes them on
real-world datasheet drawings to enhance robustness and accuracy in practical
scenarios. To support this, we introduce ICGeo8K, a multi-modal dataset with
8,608 labeled samples, including 4138 hand-crafted IC footprint samples and
4470 synthetically generated samples. Extensive experiments demonstrate that
our model outperforms state-of-the-art LMMs on the proposed benchmark.

</details>


### [22] [TIR-Diffusion: Diffusion-based Thermal Infrared Image Denoising via Latent and Wavelet Domain Optimization](https://arxiv.org/abs/2508.03727)
*Tai Hyoung Rhee,Dong-guw Lee,Ayoung Kim*

Main category: cs.CV

TL;DR: 提出一种基于扩散模型的TIR图像去噪框架，有效去除噪声，并在性能和泛化能力上表现卓越，适用于机器人部署。


<details>
  <summary>Details</summary>
Motivation: 热红外（TIR）成像对机器人感知任务潜力巨大，尤其是在低能见度或光照挑战性环境。然而，TIR图像常受到严重的非均匀固定模式噪声困扰，这极大地复杂化了目标检测、定位和建图等任务。

Method: 我们提出一个基于扩散的TIR图像去噪框架，该框架利用潜在空间表示和小波域优化。具体地，通过结合潜在空间和离散小波变换（DWT）/双树复小波变换（DTCWT）损失的新型损失函数，对预训练的稳定扩散模型进行微调。此外，还引入了级联细化阶段以增强精细细节，确保高保真去噪效果。

Result: 在基准数据集上的实验表明，我们的方法性能优于现有最先进的去噪方法。此外，该方法对多样化且具有挑战性的真实世界TIR数据集展现出强大的零样本泛化能力。

Conclusion: 该去噪框架有效解决了TIR图像的噪声问题，其卓越的性能和强大的泛化能力，使其非常适用于实际的机器人部署。

Abstract: Thermal infrared imaging exhibits considerable potentials for robotic
perception tasks, especially in environments with poor visibility or
challenging lighting conditions. However, TIR images typically suffer from
heavy non-uniform fixed-pattern noise, complicating tasks such as object
detection, localization, and mapping. To address this, we propose a
diffusion-based TIR image denoising framework leveraging latent-space
representations and wavelet-domain optimization. Utilizing a pretrained stable
diffusion model, our method fine-tunes the model via a novel loss function
combining latent-space and discrete wavelet transform (DWT) / dual-tree complex
wavelet transform (DTCWT) losses. Additionally, we implement a cascaded
refinement stage to enhance fine details, ensuring high-fidelity denoising
results. Experiments on benchmark datasets demonstrate superior performance of
our approach compared to state-of-the-art denoising methods. Furthermore, our
method exhibits robust zero-shot generalization to diverse and challenging
real-world TIR datasets, underscoring its effectiveness for practical robotic
deployment.

</details>


### [23] [What is Beneath Misogyny: Misogynous Memes Classification and Explanation](https://arxiv.org/abs/2508.03732)
*Kushal Kanwar,Dushyant Singh Chauhan,Gopendra Vikram Singh,Asif Ekbal*

Main category: cs.CV

TL;DR: 本文提出MM-Misogyny，一种新型多模态方法，用于检测、分类并解释网络迷因中的厌女内容，并在新构建的数据集上取得了优于现有方法的成果。


<details>
  <summary>Details</summary>
Motivation: 网络迷因虽主要用于娱乐，但可能传播厌女等有害意识形态。由于迷因的多模态特性（图像和文本）及其在不同社会背景下的细微表现，检测和理解迷因为何具有厌女性是一个重要的研究挑战。

Method: 研究引入了名为MM-Misogyny的多模态方法，用于检测、分类和解释迷因中的厌女内容。该方法分别处理文本和图像模态，通过交叉注意力机制将其整合为多模态上下文。随后，利用分类器和大型语言模型（LLM）对该上下文进行标签、分类和解释。模型在新构建的WBMS（What's Beneath Misogynous Stereotyping）数据集上进行评估，该数据集包含从网络收集并分为厨房、领导力、工作和购物四类的厌女迷因。

Result: MM-Misogyny模型不仅能有效检测和分类厌女内容，还能提供关于厌女症如何在不同生活领域运作的细致理解。实验结果表明，该方法相比现有方法具有卓越的性能。

Conclusion: 该研究提出的MM-Misogyny模型成功地实现了对网络迷因中厌女内容的检测、分类和解释，其性能优于现有方法，并能提供厌女症在特定生活领域中表现的深入理解，为识别和理解有害意识形态提供了有效工具。

Abstract: Memes are popular in the modern world and are distributed primarily for
entertainment. However, harmful ideologies such as misogyny can be propagated
through innocent-looking memes. The detection and understanding of why a meme
is misogynous is a research challenge due to its multimodal nature (image and
text) and its nuanced manifestations across different societal contexts. We
introduce a novel multimodal approach, \textit{namely},
\textit{\textbf{MM-Misogyny}} to detect, categorize, and explain misogynistic
content in memes. \textit{\textbf{MM-Misogyny}} processes text and image
modalities separately and unifies them into a multimodal context through a
cross-attention mechanism. The resulting multimodal context is then easily
processed for labeling, categorization, and explanation via a classifier and
Large Language Model (LLM). The evaluation of the proposed model is performed
on a newly curated dataset (\textit{\textbf{W}hat's \textbf{B}eneath
\textbf{M}isogynous \textbf{S}tereotyping (WBMS)}) created by collecting
misogynous memes from cyberspace and categorizing them into four categories,
\textit{namely}, Kitchen, Leadership, Working, and Shopping. The model not only
detects and classifies misogyny, but also provides a granular understanding of
how misogyny operates in domains of life. The results demonstrate the
superiority of our approach compared to existing methods. The code and dataset
are available at
\href{https://github.com/kushalkanwarNS/WhatisBeneathMisogyny/tree/main}{https://github.com/Misogyny}.

</details>


### [24] [StorySync: Training-Free Subject Consistency in Text-to-Image Generation via Region Harmonization](https://arxiv.org/abs/2508.03735)
*Gopalji Gaur,Mohammadreza Zolfaghari,Thomas Brox*

Main category: cs.CV

TL;DR: 本文提出一种无需训练的方法，通过引入遮蔽跨图像注意力共享和区域特征协调，有效解决了文本到图像扩散模型在生成视觉故事时难以保持主体一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型在生成视觉故事序列时，难以在不同场景中保持主体一致性。现有方法（如微调或重训练）计算成本高、耗时，且常干扰模型原有能力。

Method: 提出一种无需训练的高效主体一致性生成方法。该方法通过“遮蔽跨图像注意力共享”动态对齐批处理图像中的主体特征，并利用“区域特征协调”优化视觉相似细节，以提高主体一致性。此方法可无缝应用于预训练的扩散模型。

Result: 实验结果表明，该方法成功地在多种场景下生成了视觉一致的主体，同时保持了扩散模型的创造能力。

Conclusion: 本研究的无需训练方法有效解决了文本到图像扩散模型在视觉故事生成中的主体一致性难题，在保证效率的同时，成功生成了视觉一致且保持模型创造力的图像序列。

Abstract: Generating a coherent sequence of images that tells a visual story, using
text-to-image diffusion models, often faces the critical challenge of
maintaining subject consistency across all story scenes. Existing approaches,
which typically rely on fine-tuning or retraining models, are computationally
expensive, time-consuming, and often interfere with the model's pre-existing
capabilities. In this paper, we follow a training-free approach and propose an
efficient consistent-subject-generation method. This approach works seamlessly
with pre-trained diffusion models by introducing masked cross-image attention
sharing to dynamically align subject features across a batch of images, and
Regional Feature Harmonization to refine visually similar details for improved
subject consistency. Experimental results demonstrate that our approach
successfully generates visually consistent subjects across a variety of
scenarios while maintaining the creative abilities of the diffusion model.

</details>


### [25] [Fusion of Pervasive RF Data with Spatial Images via Vision Transformers for Enhanced Mapping in Smart Cities](https://arxiv.org/abs/2508.03736)
*Rafayel Mkrtchyan,Armen Manukyan,Hrant Khachatrian,Theofanis P. Raptis*

Main category: cs.CV

TL;DR: 本文提出了一种深度学习方法，通过结合DINOv2架构、开源地图数据和射频（RF）数据来提高智慧城市中的建筑制图精度，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 智慧城市应用中环境制图至关重要，但传统方法（如卫星图像、LiDAR、人工标注）受限于成本、可访问性和精度。开源地图平台虽然被广泛用于AI应用，但存在人为错误和环境动态变化导致的偏差，负面影响神经网络性能，因此需要更精确的制图方法。

Method: 该研究提出了一种基于深度学习的方法，集成了DINOv2架构。它结合了开源地图数据与从多无线用户设备和基站收集的射频（RF）数据，利用基于视觉Transformer的架构在统一框架内联合处理RF和地图模态。该模型仅利用聚合路径损耗信息进行制图。评估使用华为合作生产的合成数据集，并采用Jaccard指数（IoU）、Hausdorff距离和Chamfer距离作为性能指标。

Result: 所提出的设计实现了65.3%的宏观IoU，显著优于：1) 错误地图基线（40.1%），2) 文献中的纯RF方法（37.3%），以及 3) 研究者设计的非AI融合基线（42.2%）。

Conclusion: 该研究提出的深度学习方法，通过有效整合DINOv2架构、RF数据和开源地图数据，显著提升了建筑制图的准确性，超越了多种现有基线，为智慧城市应用提供了更精确的环境感知能力。

Abstract: Environment mapping is an important computing task for a wide range of smart
city applications, including autonomous navigation, wireless network operations
and extended reality environments. Conventional smart city mapping techniques,
such as satellite imagery, LiDAR scans, and manual annotations, often suffer
from limitations related to cost, accessibility and accuracy. Open-source
mapping platforms have been widely utilized in artificial intelligence
applications for environment mapping, serving as a source of ground truth.
However, human errors and the evolving nature of real-world environments
introduce biases that can negatively impact the performance of neural networks
trained on such data. In this paper, we present a deep learning-based approach
that integrates the DINOv2 architecture to improve building mapping by
combining maps from open-source platforms with radio frequency (RF) data
collected from multiple wireless user equipments and base stations. Our
approach leverages a vision transformer-based architecture to jointly process
both RF and map modalities within a unified framework, effectively capturing
spatial dependencies and structural priors for enhanced mapping accuracy. For
the evaluation purposes, we employ a synthetic dataset co-produced by Huawei.
We develop and train a model that leverages only aggregated path loss
information to tackle the mapping problem. We measure the results according to
three performance metrics which capture different qualities: (i) The Jaccard
index, also known as intersection over union (IoU), (ii) the Hausdorff
distance, and (iii) the Chamfer distance. Our design achieves a macro IoU of
65.3%, significantly surpassing (i) the erroneous maps baseline, which yields
40.1%, (ii) an RF-only method from the literature, which yields 37.3%, and
(iii) a non-AI fusion baseline that we designed which yields 42.2%.

</details>


### [26] [VQ-DeepISC: Vector Quantized-Enabled Digital Semantic Communication with Channel Adaptive Image Transmission](https://arxiv.org/abs/2508.03740)
*Jianqiao Chen,Tingting Zhu,Huishi Song,Nan Ma,Xiaodong Xu*

Main category: cs.CV

TL;DR: 本文提出一种名为VQ-DeepISC的向量量化（VQ）数字语义通信系统，通过结合深度联合源信道编码（DJSCC）和信道自适应机制，实现高效且鲁棒的图像语义特征传输，并表现出优越的重建保真度。


<details>
  <summary>Details</summary>
Motivation: 语义特征的数字化对于实现语义与数字通信系统互操作性具有巨大潜力，但其核心挑战在于如何在将固有的模拟表示压缩为离散符号时，既能保留连续性和上下文信息，又能确保对信道退化的鲁棒性。

Method: 本文提出了VQ-DeepISC系统，主要方法包括：1) 在DJSCC的指导下，设计Swin Transformer骨干网络用于分层语义特征提取，并结合VQ模块将特征投影到离散潜在空间，从而实现高效的基于索引的传输；2) 开发注意力机制驱动的信道自适应模块，以动态优化索引传输；3) 采用最小化Kullback-Leibler散度（KLD）的分布正则化和指数移动平均（EMA）来防止码本崩溃，稳定训练并确保码本更新期间的特征覆盖平衡；4) 采用QPSK调制和OFDM技术实现数字通信，符合IEEE 802.11a标准。

Result: 实验结果表明，所提出的VQ-DeepISC系统在图像重建保真度方面显著优于现有的基准方法。

Conclusion: VQ-DeepISC系统成功解决了语义特征数字化过程中的关键挑战，通过创新的VQ、DJSCC和信道自适应结合，实现了高效、鲁棒的数字语义通信，其卓越的重建性能验证了其在实际应用中的可行性和优越性。

Abstract: Discretization of semantic features enables interoperability between semantic
and digital communication systems, showing significant potential for practical
applications. The fundamental difficulty in digitizing semantic features stems
from the need to preserve continuity and context in inherently analog
representations during their compression into discrete symbols while ensuring
robustness to channel degradation. In this paper, we propose a vector quantized
(VQ)-enabled digital semantic communication system with channel adaptive image
transmission, named VQ-DeepISC. Guided by deep joint source-channel coding
(DJSCC), we first design a Swin Transformer backbone for hierarchical semantic
feature extraction, followed by VQ modules projecting features into discrete
latent spaces. Consequently, it enables efficient index-based transmission
instead of raw feature transmission. To further optimize this process, we
develop an attention mechanism-driven channel adaptation module to dynamically
optimize index transmission. Secondly, to counteract codebook collapse during
training process, we impose a distributional regularization by minimizing the
Kullback-Leibler divergence (KLD) between codeword usage frequencies and a
uniform prior. Meanwhile, exponential moving average (EMA) is employed to
stabilize training and ensure balanced feature coverage during codebook
updates. Finally, digital communication is implemented using quadrature phase
shift keying (QPSK) modulation alongside orthogonal frequency division
multiplexing (OFDM), adhering to the IEEE 802.11a standard. Experimental
results demonstrate superior reconstruction fidelity of the proposed system
over benchmark methods.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [27] [MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems](https://arxiv.org/abs/2508.03858)
*Charles L. Wang,Trisha Singhal,Ameya Kelkar,Jason Tuo*

Main category: cs.AI

TL;DR: 代理AI系统因运行时行为的不可预测性带来独特治理挑战。本研究提出了MI9，一个集成运行时治理框架，包含六个组件，旨在解决传统方法不足之处，实现代理AI系统的安全和负责任部署。


<details>
  <summary>Details</summary>
Motivation: 与传统AI不同，代理AI系统在运行时会展现出紧急和意想不到的行为，引入了部署前治理无法完全预见的代理相关风险。因此，需要一个专门的运行时治理框架来应对这一关键挑战。

Method: 引入了MI9，这是首个专为代理AI系统安全和对齐设计的完全集成运行时治理框架。MI9通过六个集成组件提供实时控制：代理风险指数、代理语义遥测捕获、持续授权监控、基于有限状态机（FSM）的一致性引擎、目标条件漂移检测和分级遏制策略。

Result: MI9能够在传统治理方法不足的生产环境中，系统化、安全、负责任地部署代理系统，为大规模安全代理AI部署提供了基础架构。通过多样化场景的详细分析，证明了MI9能系统地覆盖现有方法未能解决的治理挑战。

Conclusion: MI9框架为全面的代理AI监管奠定了技术基础，有效地解决了代理AI系统在运行时面临的治理难题，从而促进了代理AI系统安全、负责任地部署和扩展。

Abstract: Agentic AI systems capable of reasoning, planning, and executing actions
present fundamentally distinct governance challenges compared to traditional AI
models. Unlike conventional AI, these systems exhibit emergent and unexpected
behaviors during runtime, introducing novel agent-related risks that cannot be
fully anticipated through pre-deployment governance alone. To address this
critical gap, we introduce MI9, the first fully integrated runtime governance
framework designed specifically for safety and alignment of agentic AI systems.
MI9 introduces real-time controls through six integrated components:
agency-risk index, agent-semantic telemetry capture, continuous authorization
monitoring, Finite-State-Machine (FSM)-based conformance engines,
goal-conditioned drift detection, and graduated containment strategies.
Operating transparently across heterogeneous agent architectures, MI9 enables
the systematic, safe, and responsible deployment of agentic systems in
production environments where conventional governance approaches fall short,
providing the foundational infrastructure for safe agentic AI deployment at
scale. Detailed analysis through a diverse set of scenarios demonstrates MI9's
systematic coverage of governance challenges that existing approaches fail to
address, establishing the technical foundation for comprehensive agentic AI
oversight.

</details>


### [28] [Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety](https://arxiv.org/abs/2508.03864)
*Zhenyu Pan,Yiting Zhang,Yutong Zhang,Jianshu Zhang,Haozheng Luo,Yuwei Han,Dennis Wu,Hong-Yu Chen,Philip S. Yu,Manling Li,Han Liu*

Main category: cs.AI

TL;DR: 本文提出Evo-MARL框架，通过多智能体强化学习使所有任务智能体共同获得防御能力，以应对大语言模型多智能体系统的安全威胁，并在不牺牲性能的前提下提高安全性。


<details>
  <summary>Details</summary>
Motivation: 基于多模态大语言模型的多智能体系统面临越狱和对抗性攻击的严重安全风险。现有防御方法依赖外部安全模块，存在保护有限、单点故障及增加成本和复杂性等问题。

Method: 本文提出Evo-MARL，一个新颖的多智能体强化学习框架。该框架使每个任务智能体同时学习其主要功能并抵抗对抗性威胁，从而内化防御能力。Evo-MARL通过结合进化搜索与参数共享强化学习，共同进化攻击者和防御者，以持续增强系统性能。

Result: 实验结果显示，Evo-MARL将攻击成功率降低高达22%，并在推理任务上将准确性提高高达5%，表明安全性和实用性可以共同改善。

Conclusion: Evo-MARL通过使多智能体系统内化防御机制，有效地解决了安全挑战，并在不增加系统开销或引入单点故障的情况下，同时提升了系统的安全性和任务性能。

Abstract: Multi-agent systems (MAS) built on multimodal large language models exhibit
strong collaboration and performance. However, their growing openness and
interaction complexity pose serious risks, notably jailbreak and adversarial
attacks. Existing defenses typically rely on external guard modules, such as
dedicated safety agents, to handle unsafe behaviors. Unfortunately, this
paradigm faces two challenges: (1) standalone agents offer limited protection,
and (2) their independence leads to single-point failure-if compromised,
system-wide safety collapses. Naively increasing the number of guard agents
further raises cost and complexity. To address these challenges, we propose
Evo-MARL, a novel multi-agent reinforcement learning (MARL) framework that
enables all task agents to jointly acquire defensive capabilities. Rather than
relying on external safety modules, Evo-MARL trains each agent to
simultaneously perform its primary function and resist adversarial threats,
ensuring robustness without increasing system overhead or single-node failure.
Furthermore, Evo-MARL integrates evolutionary search with parameter-sharing
reinforcement learning to co-evolve attackers and defenders. This adversarial
training paradigm internalizes safety mechanisms and continually enhances MAS
performance under co-evolving threats. Experiments show that Evo-MARL reduces
attack success rates by up to 22% while boosting accuracy by up to 5% on
reasoning tasks-demonstrating that safety and utility can be jointly improved.

</details>


### [29] [MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework](https://arxiv.org/abs/2508.03929)
*Nguyen Viet Tuan Kiet,Dao Van Tung,Tran Cong Dao,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: 本文提出MOTIF框架，利用基于蒙特卡洛树搜索的双LLM智能体轮流优化方法，解决NP-hard组合优化问题中的多策略求解器设计，并显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: NP-hard组合优化问题求解器高度依赖人工设计的算法组件。现有LLM方法在求解器设计中通常局限于优化单一组件（如启发式评分函数），限制了创新。研究需要更广泛的多策略优化框架来同时改进相互依赖的组件。

Method: 引入MOTIF（Multi-strategy Optimization via Turn-based Interactive Framework）框架，该框架基于蒙特卡洛树搜索，促进两个LLM智能体之间的轮流优化。每个回合，一个智能体利用自身和对手的历史更新来改进一个组件，从而促进竞争压力和协作，拓宽搜索空间并发现高性能解决方案。

Result: 在多个组合优化问题领域的实验表明，MOTIF持续优于现有最先进的方法。

Conclusion: 轮流式、多智能体提示方法在全自动求解器设计方面展现出巨大潜力。

Abstract: Designing effective algorithmic components remains a fundamental obstacle in
tackling NP-hard combinatorial optimization problems (COPs), where solvers
often rely on carefully hand-crafted strategies. Despite recent advances in
using large language models (LLMs) to synthesize high-quality components, most
approaches restrict the search to a single element - commonly a heuristic
scoring function - thus missing broader opportunities for innovation. In this
paper, we introduce a broader formulation of solver design as a multi-strategy
optimization problem, which seeks to jointly improve a set of interdependent
components under a unified objective. To address this, we propose
Multi-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a
novel framework based on Monte Carlo Tree Search that facilitates turn-based
optimization between two LLM agents. At each turn, an agent improves one
component by leveraging the history of both its own and its opponent's prior
updates, promoting both competitive pressure and emergent cooperation. This
structured interaction broadens the search landscape and encourages the
discovery of diverse, high-performing solutions. Experiments across multiple
COP domains show that MOTIF consistently outperforms state-of-the-art methods,
highlighting the promise of turn-based, multi-agent prompting for fully
automated solver design.

</details>


### [30] [Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?](https://arxiv.org/abs/2508.03963)
*Zewen Liu,Juntong Ni,Xianfeng Tang,Max S. Y. Lau,Wei Jin*

Main category: cs.AI

TL;DR: 该研究引入SymbolBench基准和LLM-遗传编程框架，系统评估大型语言模型从时间序列数据中发现和推理隐藏符号规律的能力，并揭示了现有模型的优缺点，强调了结合领域知识的重要性。


<details>
  <summary>Details</summary>
Motivation: 从时间序列数据中发现隐藏的符号规律是科学发现和人工智能领域的核心挑战。尽管大型语言模型在结构化推理任务中展现潜力，但其从时间序列数据中推断可解释、上下文对齐的符号结构的能力尚未得到充分探索。

Method: 引入了SymbolBench这一综合基准，旨在评估大型语言模型在真实世界时间序列上的符号推理能力，涵盖多变量符号回归、布尔网络推断和因果发现三项任务。同时，提出了一个将大型语言模型与遗传编程相结合的统一闭环符号推理框架，其中大型语言模型兼作预测器和评估器。

Result: 经验结果揭示了当前模型在符号推理方面的关键优势和局限性。

Conclusion: 研究强调了结合领域知识、上下文对齐和推理结构对于提升大型语言模型在自动化科学发现中表现的重要性。

Abstract: Uncovering hidden symbolic laws from time series data, as an aspiration
dating back to Kepler's discovery of planetary motion, remains a core challenge
in scientific discovery and artificial intelligence. While Large Language
Models show promise in structured reasoning tasks, their ability to infer
interpretable, context-aligned symbolic structures from time series data is
still underexplored. To systematically evaluate this capability, we introduce
SymbolBench, a comprehensive benchmark designed to assess symbolic reasoning
over real-world time series across three tasks: multivariate symbolic
regression, Boolean network inference, and causal discovery. Unlike prior
efforts limited to simple algebraic equations, SymbolBench spans a diverse set
of symbolic forms with varying complexity. We further propose a unified
framework that integrates LLMs with genetic programming to form a closed-loop
symbolic reasoning system, where LLMs act both as predictors and evaluators.
Our empirical results reveal key strengths and limitations of current models,
highlighting the importance of combining domain knowledge, context alignment,
and reasoning structure to improve LLMs in automated scientific discovery.

</details>


### [31] [The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?](https://arxiv.org/abs/2508.03986)
*Yuan Xun,Xiaojun Jia,Xinwei Liu,Hua Zhang*

Main category: cs.AI

TL;DR: 本文发现面向人类服务的MLRM在情感提示下易绕过安全协议，产生有害内容，即使视觉风险被识别。提出EmoAgent框架，通过夸大情感提示劫持模型推理。同时，识别出模型内部推理与表面行为的错位，并引入RRSS、RVNR和RAIC三项新指标量化风险。实验证明模型安全行为存在深层情感认知错位。


<details>
  <summary>Details</summary>
Motivation: 研究观察到面向人类服务的MLRM在深度思考阶段对用户情感提示高度敏感，在高情感强度下可能绕过安全协议，导致有害输出。此外，现有内容级安全措施无法捕获模型内部推理与表面行为的错位，使得有害推理被看似安全的响应所掩盖。

Method: 1. 提出EmoAgent，一个自主对抗性情感代理框架，旨在通过编排夸大的情感提示来劫持模型的推理路径。2. 引入三个新的量化风险指标：风险推理隐蔽得分 (RRSS) 用于评估良性输出下的有害推理；风险视觉忽视率 (RVNR) 用于衡量识别视觉风险但仍生成不安全内容的比例；拒绝态度不一致性 (RAIC) 用于评估模型在不同提示变体下的拒绝稳定性。

Result: 1. 即使模型能够正确识别视觉风险，也可能因情感错位而生成有害内容。2. 发现了透明深度思考场景中持续存在的高风险失败模式，即MLRM在看似安全的响应背后生成有害推理。3. 这些失败暴露了内部推理与表面行为之间的错位，现有基于内容的防护措施无法检测。4. 对先进MLRM的广泛实验证明了EmoAgent的有效性，并揭示了模型安全行为中更深层次的情感认知错位。

Conclusion: MLRM在处理用户情感提示时存在显著的安全漏洞，表现为情感认知错位导致有害内容生成，且现有安全机制难以察觉。解决模型内部推理与表面行为之间的错位是提升其安全性的关键。EmoAgent及其提出的量化指标有助于揭示和理解这些深层问题。

Abstract: We observe that MLRMs oriented toward human-centric service are highly
susceptible to user emotional cues during the deep-thinking stage, often
overriding safety protocols or built-in safety checks under high emotional
intensity. Inspired by this key insight, we propose EmoAgent, an autonomous
adversarial emotion-agent framework that orchestrates exaggerated affective
prompts to hijack reasoning pathways. Even when visual risks are correctly
identified, models can still produce harmful completions through emotional
misalignment. We further identify persistent high-risk failure modes in
transparent deep-thinking scenarios, such as MLRMs generating harmful reasoning
masked behind seemingly safe responses. These failures expose misalignments
between internal inference and surface-level behavior, eluding existing
content-based safeguards. To quantify these risks, we introduce three metrics:
(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign
outputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite
visual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for
evaluating refusal unstability under prompt variants. Extensive experiments on
advanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper
emotional cognitive misalignments in model safety behavior.

</details>


### [32] [Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents](https://arxiv.org/abs/2508.03991)
*Chongyu Bao,Ruimin Dai,Yangbo Shen,Runyang Jian,Jinghan Zhang,Xiaolan Liu,Kunpeng Liu*

Main category: cs.AI

TL;DR: 本文提出Galaxy框架，通过统一认知架构和系统设计，构建了主动、隐私保护且能自我进化的智能个人助手（IPA），并展示了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能个人助手（IPAs）在响应能力方面研究较多，但主动行为、隐私保护和自我进化能力仍是未充分探索的挑战。大语言模型（LLM）代理的兴起为开发此类IPAs提供了新机遇。

Method: 提出Cognition Forest语义结构，将认知建模与系统级设计对齐。基于此，开发了Galaxy框架，该框架统一了认知架构和系统设计为自强化循环，并包含KoRa（支持响应和主动技能的认知增强生成代理）和Kernel（实现自我进化和隐私保护的元认知代理）。

Result: Galaxy框架在多项最新基准测试中表现优于现有方法。消融研究和真实世界交互案例验证了Galaxy的有效性。

Conclusion: Galaxy框架有效解决了构建主动、隐私保护且可自我进化智能个人助手的挑战，并通过实验证明了其优越性和有效性。

Abstract: Intelligent personal assistants (IPAs) such as Siri and Google Assistant are
designed to enhance human capabilities and perform tasks on behalf of users.
The emergence of LLM agents brings new opportunities for the development of
IPAs. While responsive capabilities have been widely studied, proactive
behaviors remain underexplored. Designing an IPA that is proactive,
privacy-preserving, and capable of self-evolution remains a significant
challenge. Designing such IPAs relies on the cognitive architecture of LLM
agents. This work proposes Cognition Forest, a semantic structure designed to
align cognitive modeling with system-level design. We unify cognitive
architecture and system design into a self-reinforcing loop instead of treating
them separately. Based on this principle, we present Galaxy, a framework that
supports multidimensional interactions and personalized capability generation.
Two cooperative agents are implemented based on Galaxy: KoRa, a
cognition-enhanced generative agent that supports both responsive and proactive
skills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's
self-evolution and privacy preservation. Experimental results show that Galaxy
outperforms multiple state-of-the-art benchmarks. Ablation studies and
real-world interaction cases validate the effectiveness of Galaxy.

</details>


### [33] [Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement](https://arxiv.org/abs/2508.04025)
*Chao Hao,Shuai Wang,Kaiwen Zhou*

Main category: cs.AI

TL;DR: RecAgent通过自适应感知机制，有效解决了移动图形用户界面（GUI）代理在自动化任务中面临的输入冗余和决策模糊问题，提升了代理性能。


<details>
  <summary>Details</summary>
Motivation: 现有移动GUI代理在自动化移动任务时面临输入冗余和决策模糊的挑战。这些问题可归结为两种不确定性：由全面屏幕信息带来的输入冗余和噪声导致的“感知不确定性”，以及由模糊任务和复杂推理引起的“决策不确定性”。

Method: 本文提出了一个不确定性感知代理RecAgent。针对感知不确定性，RecAgent采用组件推荐机制来识别并关注最相关的UI元素，以减少输入复杂性。针对决策不确定性，它使用一个交互模块在模糊情境下请求用户反馈，以实现意图感知的决策。这些组件被整合到一个统一框架中，通过人机协作主动降低输入复杂性并应对高不确定性情况。此外，论文还提出了一个名为ComplexAction的数据集，用于评估GUI代理在复杂场景下执行指定单步动作的成功率。

Result: 广泛的实验验证了所提出方法的有效性。

Conclusion: RecAgent通过自适应感知机制，有效降低了移动GUI代理在自动化任务中的感知和决策不确定性，从而提高了任务执行的成功率和代理的整体性能。

Abstract: Graphical user interface (GUI) agents have shown promise in automating mobile
tasks but still struggle with input redundancy and decision ambiguity. In this
paper, we present \textbf{RecAgent}, an uncertainty-aware agent that addresses
these issues through adaptive perception. We distinguish two types of
uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input
redundancy and noise from comprehensive screen information, and (2) decision
uncertainty, arising from ambiguous tasks and complex reasoning. To reduce
perceptual uncertainty, RecAgent employs a component recommendation mechanism
that identifies and focuses on the most relevant UI elements. For decision
uncertainty, it uses an interactive module to request user feedback in
ambiguous situations, enabling intent-aware decisions. These components are
integrated into a unified framework that proactively reduces input complexity
and reacts to high-uncertainty cases via human-in-the-loop refinement.
Additionally, we propose a dataset called \textbf{ComplexAction} to evaluate
the success rate of GUI agents in executing specified single-step actions
within complex scenarios. Extensive experiments validate the effectiveness of
our approach. The dataset and code will be available at
https://github.com/Fanye12/RecAgent.

</details>


### [34] [SEA: Self-Evolution Agent with Step-wise Reward for Computer Use](https://arxiv.org/abs/2508.04037)
*Liang Tang,Shuxian Li,Yuhao Cheng,Yukang Huo,Zhepeng Wang,Yiqiang Yan,Kaer Huang,Yanzhe Jing,Tiaonan Duan*

Main category: cs.AI

TL;DR: 本文提出一种名为SEA的自进化计算机使用智能体，通过创新的数据生成、强化学习和模型增强方法，以7B参数量实现了卓越性能，超越同等参数模型并媲美大型模型。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用智能体性能不足，远未达到实用水平。

Method: 1) 提出自动生成可验证训练轨迹数据的管道；2) 引入高效的逐步强化学习以降低计算成本；3) 设计模型增强方法，无需额外训练即可融合基础理解和规划能力。

Result: 所提出的7B参数Self-Evolution Agent (SEA) 智能体性能优于相同参数量的模型，并与更大规模的模型表现相当。

Conclusion: 通过数据生成、训练策略和模型增强的创新，SEA智能体在计算机使用任务中以较小的参数量取得了优异性能。

Abstract: Computer use agent is an emerging area in artificial intelligence that aims
to operate the computers to achieve the user's tasks, which attracts a lot of
attention from both industry and academia. However, the present agents'
performance is far from being used. In this paper, we propose the
Self-Evolution Agent (SEA) for computer use, and to develop this agent, we
propose creative methods in data generation, reinforcement learning, and model
enhancement. Specifically, we first propose an automatic pipeline to generate
the verifiable trajectory for training. And then, we propose efficient
step-wise reinforcement learning to alleviate the significant computational
requirements for long-horizon training. In the end, we propose the enhancement
method to merge the grounding and planning ability into one model without any
extra training. Accordingly, based on our proposed innovation of data
generation, training strategy, and enhancement, we get the Selfevolution Agent
(SEA) for computer use with only 7B parameters, which outperforms models with
the same number of parameters and has comparable performance to larger ones. We
will make the models' weight and related codes open-source in the future.

</details>


### [35] [Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals](https://arxiv.org/abs/2508.04070)
*Ronja Mehlan,Claudia Hess,Quintus Stierstorfer,Kristina Schaaff*

Main category: cs.AI

TL;DR: 本研究探讨了生成式AI驱动的、基于职业目标的学习内容个性化对学习者参与度、满意度和学习效率的影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI融入数字学习，根据学习者个人职业目标定制学习内容有望提升参与度和长期动机。研究旨在评估这种定制化带来的具体效果。

Method: 采用混合方法实验设计，招募了4000多名学习者，分为实验组（接受职业目标定制的学习场景）和对照组（接受标准内容）。

Result: 定量结果显示，实验组的学习会话时长增加，满意度更高，学习时长适度缩短。定性分析表明，学习者认为个性化材料更具启发性和实用性，促进了深度认知参与和高度内容认同。

Conclusion: 研究结果强调了将教育内容与学习者职业目标对齐的重要性，并指出可扩展的AI个性化技术能够有效地连接学术知识与职场应用。

Abstract: As artificial intelligence becomes increasingly integrated into digital
learning environments, the personalization of learning content to reflect
learners' individual career goals offers promising potential to enhance
engagement and long-term motivation. In our study, we investigate how career
goal-based content adaptation in learning systems based on generative AI
(GenAI) influences learner engagement, satisfaction, and study efficiency. The
mixed-methods experiment involved more than 4,000 learners, with one group
receiving learning scenarios tailored to their career goals and a control
group. Quantitative results show increased session duration, higher
satisfaction ratings, and a modest reduction in study duration compared to
standard content. Qualitative analysis highlights that learners found the
personalized material motivating and practical, enabling deep cognitive
engagement and strong identification with the content. These findings
underscore the value of aligning educational content with learners' career
goals and suggest that scalable AI personalization can bridge academic
knowledge and workplace applicability.

</details>


### [36] [KG-Augmented Executable CoT for Mathematical Coding](https://arxiv.org/abs/2508.04072)
*Xingyu Chen,Junxiu An,Jun Guo,Li Wang,Jingcai Guo*

Main category: cs.AI

TL;DR: KGA-ECoT是一个新颖框架，它结合知识图谱和可执行代码，显著提升了大型语言模型在复杂数学推理和代码生成任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言处理任务中表现出色，但在数学推理和代码生成等复杂推理任务中面临显著挑战。

Method: 该研究提出了KG-Augmented Executable Chain-of-Thought (KGA-ECoT) 框架。该框架将问题分解为结构化任务图，利用GraphRAG从数学库中精确检索知识，并生成可验证代码以确保计算准确性。通过外部代码执行来提升数学推理能力。

Result: 在多个数学推理基准测试中，KGA-ECoT显著优于现有提示方法，绝对准确率提高了数个到十多个百分点。分析证实GraphRAG在提升代码质量和外部代码执行在保证精度方面的关键作用。

Conclusion: KGA-ECoT是一个针对复杂数学推理任务的强大且高度通用的框架。

Abstract: In recent years, large language models (LLMs) have excelled in natural
language processing tasks but face significant challenges in complex reasoning
tasks such as mathematical reasoning and code generation. To address these
limitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a
novel framework that enhances code generation through knowledge graphs and
improves mathematical reasoning via executable code. KGA-ECoT decomposes
problems into a Structured Task Graph, leverages efficient GraphRAG for precise
knowledge retrieval from mathematical libraries, and generates verifiable code
to ensure computational accuracy. Evaluations on multiple mathematical
reasoning benchmarks demonstrate that KGA-ECoT significantly outperforms
existing prompting methods, achieving absolute accuracy improvements ranging
from several to over ten percentage points. Further analysis confirms the
critical roles of GraphRAG in enhancing code quality and external code
execution in ensuring precision. These findings collectively establish KGA-ECoT
as a robust and highly generalizable framework for complex mathematical
reasoning tasks.

</details>


### [37] [GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement](https://arxiv.org/abs/2508.04080)
*Jinfan Tang,Kunming Wu,Ruifeng Gongxie,Yuya He,Yuankai Wu*

Main category: cs.AI

TL;DR: 为解决大型语言模型（LLMs）在地理空间问题中的挑战，本文提出了GeoSR框架。该框架通过整合地理学原理和智能体驱动的迭代精炼过程，显著提升了LLMs在地理空间预测中的准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在地理问题上展现出潜力，但它们仍面临空间一致性、多跳推理和地理偏差等方面的挑战，这限制了其在地理空间应用中的有效性。

Method: GeoSR是一个自修正的智能体推理框架，它将核心地理学原理（如Tobler第一定律）嵌入迭代预测循环中。该框架包含三个协作智能体：变量选择智能体、点选择智能体和精炼智能体。通过利用空间依赖性和变量间关系，这些智能体在迭代过程中逐步提高预测质量。

Result: GeoSR在物理世界属性估计和社会经济预测等任务上进行了验证。实验结果表明，相对于标准提示策略，GeoSR取得了持续改进，证明了将地理统计先验和空间结构化推理融入LLMs可以实现更准确和公平的地理空间预测。

Conclusion: 研究表明，通过在LLMs中融入地理统计先验知识和空间结构化推理，可以显著提升其在地理空间预测任务中的准确性和公平性。GeoSR框架为解决LLMs在地理空间应用中的现有挑战提供了一个有效方案。

Abstract: Recent studies have extended the application of large language models (LLMs)
to geographic problems, revealing surprising geospatial competence even without
explicit spatial supervision. However, LLMs still face challenges in spatial
consistency, multi-hop reasoning, and geographic bias. To address these issues,
we propose GeoSR, a self-refining agentic reasoning framework that embeds core
geographic principles -- most notably Tobler's First Law of Geography -- into
an iterative prediction loop. In GeoSR, the reasoning process is decomposed
into three collaborating agents: (1) a variable-selection agent that selects
relevant covariates from the same location; (2) a point-selection agent that
chooses reference predictions at nearby locations generated by the LLM in
previous rounds; and (3) a refine agent that coordinates the iterative
refinement process by evaluating prediction quality and triggering further
rounds when necessary. This agentic loop progressively improves prediction
quality by leveraging both spatial dependencies and inter-variable
relationships. We validate GeoSR on tasks ranging from physical-world property
estimation to socioeconomic prediction. Experimental results show consistent
improvements over standard prompting strategies, demonstrating that
incorporating geostatistical priors and spatially structured reasoning into
LLMs leads to more accurate and equitable geospatial predictions. The code of
GeoSR is available at https://github.com/JinfanTang/GeoSR.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [38] [Privileged Contrastive Pretraining for Multimodal Affect Modelling](https://arxiv.org/abs/2508.03729)
*Kosmas Pinitas,Konstantinos Makantasis,Georgios N. Yannakakis*

Main category: cs.LG

TL;DR: 本文提出Privileged Contrastive Pretraining (PriCon) 框架，旨在解决情感计算模型从实验室到真实世界环境的迁移挑战。该框架结合监督对比学习和特权信息学习，显著提升了模型在真实世界环境中的鲁棒性和性能，并有望弥合in-vitro和in-vivo情感建模的差距。


<details>
  <summary>Details</summary>
Motivation: 情感计算（AC）模型在深度学习的推动下取得了显著进展，但将情感模型从受控实验室环境（in-vitro）可靠地迁移到不受控真实世界环境（in-vivo）仍然是一个持续存在的挑战。

Method: 提出了Privileged Contrastive Pretraining (PriCon) 框架。该框架首先通过监督对比学习（SCL）对模型进行预训练，然后将预训练模型作为特权信息学习（LUPI）框架中的教师模型。PriCon在训练过程中利用特权信息，并通过SCL增强了派生情感模型的鲁棒性。

Result: 在RECOLA和AGAIN两个基准情感语料库上进行的实验表明，使用PriCon训练的模型持续优于LUPI和端到端模型。值得注意的是，在许多情况下，PriCon模型甚至实现了与在训练和测试阶段都能访问所有模态的模型相当的性能。

Conclusion: 研究结果强调了PriCon作为一种新范式的潜力，有望进一步弥合in-vitro和in-vivo情感建模之间的差距，为实际应用提供一个可扩展和实用的解决方案。

Abstract: Affective Computing (AC) has made significant progress with the advent of
deep learning, yet a persistent challenge remains: the reliable transfer of
affective models from controlled laboratory settings (in-vitro) to uncontrolled
real-world environments (in-vivo). To address this challenge we introduce the
Privileged Contrastive Pretraining (PriCon) framework according to which models
are first pretrained via supervised contrastive learning (SCL) and then act as
teacher models within a Learning Using Privileged Information (LUPI) framework.
PriCon both leverages privileged information during training and enhances the
robustness of derived affect models via SCL. Experiments conducted on two
benchmark affective corpora, RECOLA and AGAIN, demonstrate that models trained
using PriCon consistently outperform LUPI and end to end models. Remarkably, in
many cases, PriCon models achieve performance comparable to models trained with
access to all modalities during both training and testing. The findings
underscore the potential of PriCon as a paradigm towards further bridging the
gap between in-vitro and in-vivo affective modelling, offering a scalable and
practical solution for real-world applications.

</details>


### [39] [PILOT-C: Physics-Informed Low-Distortion Optimal Trajectory Compression](https://arxiv.org/abs/2508.03730)
*Kefei Wu,Baihua Zheng,Weiwei Sun*

Main category: cs.LG

TL;DR: 本文提出PILOT-C，一种新颖的轨迹压缩框架，通过整合频域物理建模和误差约束优化，在多维轨迹压缩方面超越了现有算法。


<details>
  <summary>Details</summary>
Motivation: 位置感知设备产生大量轨迹数据，需要高效压缩。现有直线简化方法通常假设2D轨迹，并忽略时间同步和运动连续性，无法满足多维轨迹压缩需求。

Method: 研究者提出了PILOT-C框架，该框架将频域物理建模与误差约束优化相结合。与现有方法不同，PILOT-C通过独立压缩每个空间轴来支持任意维度（包括3D）的轨迹。

Result: 在四个真实世界数据集上评估，PILOT-C表现出色。在压缩比方面，PILOT-C比当前最先进的SED基直线简化算法CISED-W平均高19.2%。在轨迹保真度方面，PILOT-C比CISED-W平均减少32.6%的误差。此外，PILOT-C能无缝扩展到3D轨迹，计算复杂度不变，在3D数据集上比最有效的直线简化算法SQUISH-E提升49%的压缩比。

Conclusion: PILOT-C是一种高效、准确且适用于任意维度的轨迹压缩框架，显著优于现有最先进的压缩算法，特别在多维轨迹处理上表现出卓越性能。

Abstract: Location-aware devices continuously generate massive volumes of trajectory
data, creating demand for efficient compression. Line simplification is a
common solution but typically assumes 2D trajectories and ignores time
synchronization and motion continuity. We propose PILOT-C, a novel trajectory
compression framework that integrates frequency-domain physics modeling with
error-bounded optimization. Unlike existing line simplification methods,
PILOT-C supports trajectories in arbitrary dimensions, including 3D, by
compressing each spatial axis independently. Evaluated on four real-world
datasets, PILOT-C achieves superior performance across multiple dimensions. In
terms of compression ratio, PILOT-C outperforms CISED-W, the current
state-of-the-art SED-based line simplification algorithm, by an average of
19.2%. For trajectory fidelity, PILOT-C achieves an average of 32.6% reduction
in error compared to CISED-W. Additionally, PILOT-C seamlessly extends to
three-dimensional trajectories while maintaining the same computational
complexity, achieving a 49% improvement in compression ratios over SQUISH-E,
the most efficient line simplification algorithm on 3D datasets.

</details>


### [40] [CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2508.03733)
*Wenjie Li,Yujie Zhang,Haoran Sun,Yueqi Li,Fanrui Zhang,Mengzhe Xu,Victoria Borja Clausich,Sade Mellin,Renhao Yang,Chenrun Wang,Jethro Zih-Shuo Wang,Shiyi Yao,Gen Li,Yidong Xu,Hanyu Wang,Yilin Huang,Angela Lin Wang,Chen Shi,Yin Zhang,Jianan Guo,Luqi Yang,Renxuan Li,Yang Xu,Jiawei Liu,Yao Zhang,Lei Liu,Carlos Gutiérrez SanRomán,Lei Wang*

Main category: cs.LG

TL;DR: 提出CX-Mind，一个基于课程强化学习和可验证过程奖励的生成式多模态大语言模型，通过交错式“思考-回答”推理显著提升胸部X光（CXR）诊断的效率、准确性和临床实用性，解决了现有模型推理冗长和幻觉等问题。


<details>
  <summary>Details</summary>
Motivation: 现有用于胸部X光（CXR）诊断的多模态大语言模型（MLLMs）普遍采用“一次性”诊断方法，缺乏对推理过程的可验证监督，导致在多任务CXR诊断中出现推理冗长、奖励稀疏及频繁幻觉等问题，影响诊断效率和可解释性。

Method: 本文提出CX-Mind，首个实现CXR任务交错式“思考-回答”推理的生成模型，由基于课程的强化学习和可验证过程奖励（CuRL-VPR）驱动。研究构建了包含70万+图像和260万+样本的指令微调数据集CX-Set，并生成了4万+高质量交错推理数据。模型优化在Group Relative Policy Optimization框架下分两阶段进行：先用封闭域任务稳定基本推理，后迁移至开放域诊断，并引入基于规则的条件过程奖励。

Result: 实验结果显示，CX-Mind在视觉理解、文本生成和时空对齐方面显著优于现有医疗和通用领域MLLMs，比同类CXR专用模型平均性能提升25.1%。在真实临床数据集Rui-CXR上，CX-Mind在14种疾病上的平均召回率@1大幅超越次优结果，并获得多中心专家评估的高度肯定。

Conclusion: CX-Mind通过其创新的交错推理机制和训练方法，有效解决了现有MLLMs在CXR诊断中的挑战，大幅提升了诊断效率和准确性，并被证明具有显著的临床实用价值。

Abstract: Chest X-ray (CXR) imaging is one of the most widely used diagnostic
modalities in clinical practice, encompassing a broad spectrum of diagnostic
tasks. Recent advancements have seen the extensive application of
reasoning-based multimodal large language models (MLLMs) in medical imaging to
enhance diagnostic efficiency and interpretability. However, existing
multimodal models predominantly rely on "one-time" diagnostic approaches,
lacking verifiable supervision of the reasoning process. This leads to
challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse
rewards, and frequent hallucinations. To address these issues, we propose
CX-Mind, the first generative model to achieve interleaved "think-answer"
reasoning for CXR tasks, driven by curriculum-based reinforcement learning and
verifiable process rewards (CuRL-VPR). Specifically, we constructed an
instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148
samples, and generated 42,828 high-quality interleaved reasoning data points
supervised by clinical reports. Optimization was conducted in two stages under
the Group Relative Policy Optimization framework: initially stabilizing basic
reasoning with closed-domain tasks, followed by transfer to open-domain
diagnostics, incorporating rule-based conditional process rewards to bypass the
need for pretrained reward models. Extensive experimental results demonstrate
that CX-Mind significantly outperforms existing medical and general-domain
MLLMs in visual understanding, text generation, and spatiotemporal alignment,
achieving an average performance improvement of 25.1% over comparable
CXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves
a mean recall@1 across 14 diseases that substantially surpasses the second-best
results, with multi-center expert evaluations further confirming its clinical
utility across multiple dimensions.

</details>


### [41] [Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models](https://arxiv.org/abs/2508.03741)
*Xin Liu,Qiyang Song,Shaowen Xu,Kerou Zhou,Wenbo Jiang,Xiaoqi Jia,Weijuan Zhang,Heqing Huang,Yakai Li*

Main category: cs.LG

TL;DR: 本文提出Latent Knowledge Scalpel (LKS)，一种基于轻量级超网络的LLM编辑工具，通过操纵潜在表示实现大规模、精确的知识编辑，同时保持模型通用能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）常保留预训练中不准确或过时的信息，导致错误预测或偏见输出。现有模型编辑方法难以同时编辑大量事实信息，且可能损害模型的通用能力。

Method: 通过实证研究发现编辑LLM内部表示并替换实体是可行的。在此基础上，提出Latent Knowledge Scalpel (LKS)，它利用轻量级超网络操纵特定实体的潜在知识，以实现精确和大规模的知识编辑。

Result: 在Llama-2和Mistral上的实验表明，即使同时编辑数量达到10,000条，LKS仍能有效执行知识编辑，同时保留已编辑LLM的通用能力。

Conclusion: LKS提供了一种有效且高效的解决方案，用于解决LLM中大规模、精确的知识编辑挑战，克服了现有方法在可扩展性和通用能力保持方面的局限性。

Abstract: Large Language Models (LLMs) often retain inaccurate or outdated information
from pre-training, leading to incorrect predictions or biased outputs during
inference. While existing model editing methods can address this challenge,
they struggle with editing large amounts of factual information simultaneously
and may compromise the general capabilities of the models. In this paper, our
empirical study demonstrates that it is feasible to edit the internal
representations of LLMs and replace the entities in a manner similar to editing
natural language inputs. Based on this insight, we introduce the Latent
Knowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of
specific entities via a lightweight hypernetwork to enable precise and
large-scale editing. Experiments conducted on Llama-2 and Mistral show even
with the number of simultaneous edits reaching 10,000, LKS effectively performs
knowledge editing while preserving the general abilities of the edited LLMs.
Code is available at: https://github.com/Linuxin-xxx/LKS.

</details>


### [42] [GlaBoost: A multimodal Structured Framework for Glaucoma Risk Stratification](https://arxiv.org/abs/2508.03750)
*Cheng Huang,Weizheng Xie,Karanjit Kooner,Tsengdar Lee,Jui-Kai Wang,Jia Zhang*

Main category: cs.LG

TL;DR: 该论文提出了GlaBoost，一个多模态梯度提升框架，用于早期、可解释的青光眼风险预测，通过整合图像、文本和结构化临床数据，实现了高准确率。


<details>
  <summary>Details</summary>
Motivation: 青光眼的早期准确检测对于预防不可逆的视力丧失至关重要。然而，现有方法常依赖单一模态数据且缺乏可解释性，限制了其临床应用。

Method: GlaBoost是一个多模态梯度提升框架。它利用预训练的卷积编码器从眼底图像中提取高级视觉特征，使用基于Transformer的语言模型编码专家文本描述。这些异构信号，结合手动评估的风险评分和定量眼科指标，被融合到一个统一的特征空间中，并通过增强型XGBoost模型进行分类。

Result: 在真实世界标注数据集上的实验表明，GlaBoost显著优于基线模型，验证准确率达到98.71%。特征重要性分析揭示了临床一致的模式，其中杯盘比、视盘苍白和特定的文本嵌入对模型决策贡献最大。

Conclusion: GlaBoost为可解释的青光眼诊断提供了一个透明且可扩展的解决方案，并可推广应用于其他眼科疾病。

Abstract: Early and accurate detection of glaucoma is critical to prevent irreversible
vision loss. However, existing methods often rely on unimodal data and lack
interpretability, limiting their clinical utility. In this paper, we present
GlaBoost, a multimodal gradient boosting framework that integrates structured
clinical features, fundus image embeddings, and expert-curated textual
descriptions for glaucoma risk prediction. GlaBoost extracts high-level visual
representations from retinal fundus photographs using a pretrained
convolutional encoder and encodes free-text neuroretinal rim assessments using
a transformer-based language model. These heterogeneous signals, combined with
manually assessed risk scores and quantitative ophthalmic indicators, are fused
into a unified feature space for classification via an enhanced XGBoost model.
Experiments conducted on a real-world annotated dataset demonstrate that
GlaBoost significantly outperforms baseline models, achieving a validation
accuracy of 98.71%. Feature importance analysis reveals clinically consistent
patterns, with cup-to-disc ratio, rim pallor, and specific textual embeddings
contributing most to model decisions. GlaBoost offers a transparent and
scalable solution for interpretable glaucoma diagnosis and can be extended to
other ophthalmic disorders.

</details>


### [43] [LRTuckerRep: Low-rank Tucker Representation Model for Multi-dimensional Data Completion](https://arxiv.org/abs/2508.03755)
*Wenwu Gong,Lili Yang*

Main category: cs.LG

TL;DR: 本文提出LRTuckerRep模型，通过结合Tucker分解中的全局低秩和局部平滑先验，有效解决了多维数据补全问题，并在高缺失率下展现出卓越的补全精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多维数据补全在计算科学中至关重要。现有方法（全局低秩近似或局部平滑正则化）存在显著局限：低秩方法计算昂贵且可能破坏数据结构；平滑方法需大量手动参数调优且泛化性差。

Method: 提出Low-Rank Tucker Representation (LRTuckerRep) 模型，该模型在Tucker分解中统一了全局低秩和局部平滑先验建模。具体通过因子矩阵上的自适应加权核范数和稀疏Tucker核心编码低秩性，并通过因子空间上的无参数拉普拉斯正则化捕捉平滑性。同时开发了两种具有收敛性保证的迭代算法。

Result: 在多维图像修复和交通数据插补的广泛实验中，LRTuckerRep在高缺失率下，相比基线方法，实现了卓越的补全精度和鲁棒性。

Conclusion: LRTuckerRep模型通过在Tucker分解中统一全局和局部先验，有效解决了多维数据补全问题，并经验性地证明其性能优于现有方法，为相关领域提供了新的解决方案。

Abstract: Multi-dimensional data completion is a critical problem in computational
sciences, particularly in domains such as computer vision, signal processing,
and scientific computing. Existing methods typically leverage either global
low-rank approximations or local smoothness regularization, but each suffers
from notable limitations: low-rank methods are computationally expensive and
may disrupt intrinsic data structures, while smoothness-based approaches often
require extensive manual parameter tuning and exhibit poor generalization. In
this paper, we propose a novel Low-Rank Tucker Representation (LRTuckerRep)
model that unifies global and local prior modeling within a Tucker
decomposition. Specifically, LRTuckerRep encodes low rankness through a
self-adaptive weighted nuclear norm on the factor matrices and a sparse Tucker
core, while capturing smoothness via a parameter-free Laplacian-based
regularization on the factor spaces. To efficiently solve the resulting
nonconvex optimization problem, we develop two iterative algorithms with
provable convergence guarantees. Extensive experiments on multi-dimensional
image inpainting and traffic data imputation demonstrate that LRTuckerRep
achieves superior completion accuracy and robustness under high missing rates
compared to baselines.

</details>


### [44] [LLM-Prior: A Framework for Knowledge-Driven Prior Elicitation and Aggregation](https://arxiv.org/abs/2508.03766)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 本研究提出一种利用大型语言模型（LLMs）自动化贝叶斯推断中先验分布指定的方法，解决了传统方法的耗时、主观和不可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯推断中先验分布的指定至关重要，但传统的先验获取过程通常是手动、主观且难以扩展的，构成了一个显著瓶颈。

Method: 本研究提出`LLMPrior`框架，该框架将LLM与显式、可处理的生成模型（如高斯混合模型，形成基于LLM的混合密度网络）相结合，将非结构化上下文（如自然语言描述、数据或图表）转化为有效且易于处理的概率分布，确保数学属性。此外，该框架扩展到多智能体系统，引入`Fed-LLMPrior`算法，利用对数意见池化聚合分布式、情境依赖的先验分布，并对智能体异质性具有鲁棒性。

Result: 本研究成功提出并形式化了`LLMPrior`和`Fed-LLMPrior`框架，它们能够自动化和扩展贝叶斯先验分布的指定，并能鲁棒地聚合分布式先验。

Conclusion: 本工作为一类新工具奠定了基础，这些工具能够降低复杂贝叶斯建模的入门门槛，使得先验指定过程更加便捷和高效。

Abstract: The specification of prior distributions is fundamental in Bayesian
inference, yet it remains a significant bottleneck. The prior elicitation
process is often a manual, subjective, and unscalable task. We propose a novel
framework which leverages Large Language Models (LLMs) to automate and scale
this process. We introduce \texttt{LLMPrior}, a principled operator that
translates rich, unstructured contexts such as natural language descriptions,
data or figures into valid, tractable probability distributions. We formalize
this operator by architecturally coupling an LLM with an explicit, tractable
generative model, such as a Gaussian Mixture Model (forming a LLM based Mixture
Density Network), ensuring the resulting prior satisfies essential mathematical
properties. We further extend this framework to multi-agent systems where
Logarithmic Opinion Pooling is employed to aggregate prior distributions
induced by decentralized knowledge. We present the federated prior aggregation
algorithm, \texttt{Fed-LLMPrior}, for aggregating distributed,
context-dependent priors in a manner robust to agent heterogeneity. This work
provides the foundation for a new class of tools that can potentially lower the
barrier to entry for sophisticated Bayesian modeling.

</details>


### [45] [Provably Near-Optimal Distributionally Robust Reinforcement Learning in Online Settings](https://arxiv.org/abs/2508.03768)
*Debamita Ghosh,George K. Atia,Yue Wang*

Main category: cs.LG

TL;DR: 本文提出一种在线分布鲁棒强化学习（DR-RL）算法，旨在单一未知训练环境中优化最差性能，解决传统DR-RL在无先验知识环境下应用受限的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习在现实部署中面临“模拟-现实”差距，而现有分布鲁棒强化学习方法通常假设可访问生成模型或广泛覆盖的离线数据集，这在无先验知识的未知环境中是不切实际的，限制了其实用性。

Method: 研究在线分布鲁棒强化学习设置，其中智能体仅与一个未知训练环境交互，目标是优化其最差性能。针对基于f-散度的不确定性集（如卡方和KL散度球），提出了一种计算高效的算法，并提供了次线性遗憾（regret）保证。

Result: 所提出的算法实现了次线性遗憾保证，且在理论上通过建立在线学习的最小最大遗憾下界，证明了该方法的接近最优性。广泛的实验进一步证实了算法的鲁棒性和效率。

Conclusion: 该研究成功开发了一种在未知在线环境中实现鲁棒性和高效性的分布鲁棒强化学习算法，弥补了现有方法在实用性上的不足，并得到了理论和实验的双重验证。

Abstract: Reinforcement learning (RL) faces significant challenges in real-world
deployments due to the sim-to-real gap, where policies trained in simulators
often underperform in practice due to mismatches between training and
deployment conditions. Distributionally robust RL addresses this issue by
optimizing worst-case performance over an uncertainty set of environments and
providing an optimized lower bound on deployment performance. However, existing
studies typically assume access to either a generative model or offline
datasets with broad coverage of the deployment environment -- assumptions that
limit their practicality in unknown environments without prior knowledge. In
this work, we study the more realistic and challenging setting of online
distributionally robust RL, where the agent interacts only with a single
unknown training environment while aiming to optimize its worst-case
performance. We focus on general $f$-divergence-based uncertainty sets,
including Chi-Square and KL divergence balls, and propose a computationally
efficient algorithm with sublinear regret guarantees under minimal assumptions.
Furthermore, we establish a minimax lower bound on regret of online learning,
demonstrating the near-optimality of our approach. Extensive experiments across
diverse environments further confirm the robustness and efficiency of our
algorithm, validating our theoretical findings.

</details>


### [46] [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.LG

TL;DR: 本文提出GTPO（Group-relative Trajectory-based Policy Optimization）来解决现有GRPO方法在语言模型策略优化中的两个主要限制：梯度冲突和负奖励导致的学习退化。GTPO通过保护冲突令牌并过滤高熵完成来提高训练稳定性，且无需KL散度正则化和参考模型，在多个基准测试中表现出更好的性能。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型训练和对齐广泛采用基于策略的优化方法，其中GRPO是一种有效方法。然而，GRPO存在两个主要限制：1) 令牌在具有正负奖励的完成中同时出现，导致梯度更新冲突，可能降低其输出概率；2) 负奖励的完成可能惩罚置信度高的响应，导致模型决策倾向于不太可能出现的令牌，进而使输出分布扁平化并降低学习效果。因此，需要一种更稳定、有效的策略优化策略。

Method: 为解决GRPO的问题，本文引入了GTPO。GTPO识别“冲突令牌”（在不同奖励的完成中处于相同位置的令牌），通过跳过负更新并放大正更新来保护它们。为进一步防止策略崩溃，GTPO过滤掉熵超过可证明阈值的完成。与GRPO不同，GTPO不依赖KL散度正则化，从而消除了训练过程中对参考模型的需要。

Result: 实验结果表明，GTPO在提供更大训练稳定性的同时，显著提高了性能。这些效果在GSM8K、MATH和AIME 2024等多个基准测试中得到了验证。

Conclusion: GTPO通过有效解决GRPO的梯度冲突和学习退化问题，提供了一种更稳定、更有效的策略优化方法。它无需参考模型，简化了训练过程，并在实际应用中展现出优越的性能和稳定性。

Abstract: Policy-based optimizations are widely adopted today for the training and
alignment of language models, where one of the most recent and effective
approaches is Group-relative Policy Optimization (GRPO). In this paper, we
reveals and analyze two major limitations of GRPO: (i) tokens frequently appear
in completions with both positive and negative rewards, leading to conflicting
gradient updates that can reduce their output probability, even though can be
essential for maintaining proper structure; (ii) negatively rewarded
completions may penalize confident responses and shift model decisions toward
unlikely tokens, progressively flattening the output distribution and degrading
learning. To address these issues and provide a more stable and effective
policy optimization strategy, we introduce GTPO (Group-relative
Trajectory-based Policy Optimization), which identifies conflict tokens, tokens
appearing in the same position across completions with opposite rewards,
protects them by skipping negative updates, while amplifying positive ones. To
further prevent policy collapse, GTPO filters out completions whose entropy
exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence
regularization, eliminating the need for a reference model during training,
while still ensuring greater training stability and improved performance,
validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.

</details>


### [47] [Data-Driven Spectrum Demand Prediction: A Spatio-Temporal Framework with Transfer Learning](https://arxiv.org/abs/2508.03863)
*Amin Farajzadeh,Hongzhao Zheng,Sarah Dumoulin,Trevor Ha,Halim Yanikomeroglu,Amir Ghasemi*

Main category: cs.LG

TL;DR: 本文提出一个基于众包KPI和监管数据的时空预测框架，以实现高精度的频谱需求预测，优于传统ITU模型。


<details>
  <summary>Details</summary>
Motivation: 准确的频谱需求预测对频谱分配、监管规划及支持5G/6G/IoT等新兴技术至关重要。传统ITU模型常受限于任意输入和不切实际的假设。

Method: 提出一个有效的时空预测框架，利用众包用户侧关键性能指标（KPIs）和监管数据集来建模和预测频谱需求。该方法结合了高级特征工程、综合相关性分析和迁移学习技术。

Result: 与ITU估算相比，该框架在预测精度和跨区域泛化能力上表现卓越，能提供更现实、可操作的预测。实验结果验证了该方法的有效性。

Conclusion: 该方法为政策制定者和监管机构提供了一个稳健的频谱管理和规划工具，有助于提升频谱管理效率。

Abstract: Accurate spectrum demand prediction is crucial for informed spectrum
allocation, effective regulatory planning, and fostering sustainable growth in
modern wireless communication networks. It supports governmental efforts,
particularly those led by the international telecommunication union (ITU), to
establish fair spectrum allocation policies, improve auction mechanisms, and
meet the requirements of emerging technologies such as advanced 5G, forthcoming
6G, and the internet of things (IoT). This paper presents an effective
spatio-temporal prediction framework that leverages crowdsourced user-side key
performance indicators (KPIs) and regulatory datasets to model and forecast
spectrum demand. The proposed methodology achieves superior prediction accuracy
and cross-regional generalizability by incorporating advanced feature
engineering, comprehensive correlation analysis, and transfer learning
techniques. Unlike traditional ITU models, which are often constrained by
arbitrary inputs and unrealistic assumptions, this approach exploits granular,
data-driven insights to account for spatial and temporal variations in spectrum
utilization. Comparative evaluations against ITU estimates, as the benchmark,
underscore our framework's capability to deliver more realistic and actionable
predictions. Experimental results validate the efficacy of our methodology,
highlighting its potential as a robust approach for policymakers and regulatory
bodies to enhance spectrum management and planning.

</details>


### [48] [U-PINet: End-to-End Hierarchical Physics-Informed Learning With Sparse Graph Coupling for 3D EM Scattering Modeling](https://arxiv.org/abs/2508.03774)
*Rui Zhu,Yuexing Peng,Peng Wang,George C. Alexandropoulos,Wenbo Wang,Wei Xiang*

Main category: cs.LG

TL;DR: 本文提出U-PINet，一个基于深度学习的物理信息分层框架，用于电磁散射建模。该方法结合物理约束和多尺度神经网络架构，实现了计算效率高、泛化性强且物理一致性好的端到端电磁散射建模，优于传统求解器和纯数据驱动深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 电磁散射建模对雷达遥感至关重要，但计算复杂。传统数值求解器准确性高但可扩展性差、计算成本高。纯数据驱动的深度学习方法虽高效，但缺乏物理约束且需要大量标注数据，限制了其适用性和泛化能力。本研究旨在克服这些局限性。

Method: 提出U型物理信息网络（U-PINet），这是首个全深度学习、物理信息分层计算电磁框架。受电磁求解器中分层分解策略和局部电磁耦合固有的稀疏性启发，U-PINet通过多尺度处理神经网络架构对近场和远场相互作用的分解与耦合进行建模，并采用物理启发的稀疏图表示来高效建模复杂3D物体网格单元的自耦合和互耦合。这种方法实现了端到端多尺度电磁散射建模。

Result: 实验结果表明，U-PINet能准确预测表面电流分布，与传统求解器高度一致，同时显著缩短计算时间，并在准确性和鲁棒性方面优于传统深度学习基线。此外，在雷达截面预测任务上的评估证实了U-PINet在下游电磁散射应用中的可行性。

Conclusion: U-PINet为端到端多尺度电磁散射建模提供了更高的效率、泛化能力和物理一致性，成功解决了现有方法的挑战。

Abstract: Electromagnetic (EM) scattering modeling is critical for radar remote
sensing, however, its inherent complexity introduces significant computational
challenges. Traditional numerical solvers offer high accuracy, but suffer from
scalability issues and substantial computational costs. Pure data-driven deep
learning approaches, while efficient, lack physical constraints embedding
during training and require extensive labeled data, limiting their
applicability and generalization. To overcome these limitations, we propose a
U-shaped Physics-Informed Network (U-PINet), the first fully
deep-learning-based, physics-informed hierarchical framework for computational
EM designed to ensure physical consistency while maximizing computational
efficiency. Motivated by the hierarchical decomposition strategy in EM solvers
and the inherent sparsity of local EM coupling, the U-PINet models the
decomposition and coupling of near- and far-field interactions through a
multiscale processing neural network architecture, while employing a
physics-inspired sparse graph representation to efficiently model both self-
and mutual- coupling among mesh elements of complex $3$-Dimensional (3D)
objects. This principled approach enables end-to-end multiscale EM scattering
modeling with improved efficiency, generalization, and physical consistency.
Experimental results showcase that the U-PINet accurately predicts surface
current distributions, achieving close agreement with traditional solver, while
significantly reducing computational time and outperforming conventional deep
learning baselines in both accuracy and robustness. Furthermore, our
evaluations on radar cross section prediction tasks confirm the feasibility of
the U-PINet for downstream EM scattering applications.

</details>


### [49] [Revisiting Heat Flux Analysis of Tungsten Monoblock Divertor on EAST using Physics-Informed Neural Network](https://arxiv.org/abs/2508.03776)
*Xiao Wang,Zikang Yan,Hao Si,Zhendong Yang,Qingquan Yang,Dengdi Sun,Wanli Lyu,Jin Tang*

Main category: cs.LG

TL;DR: 针对EAST核聚变装置热通量估计，本文提出一种物理信息神经网络（PINN），解决了传统有限元法（FEM）计算效率低的问题，实现了与FEM相当的精度和40倍的计算加速。


<details>
  <summary>Details</summary>
Motivation: EAST核聚变装置的热通量估计至关重要。传统FEM方法计算效率低下，难以实现实时模拟，无法满足实际实验需求。

Method: 受AI驱动科学计算启发，提出PINN模型。模型将空间坐标和时间戳作为输入，结合热传导方程计算边界损失、初始条件损失和物理损失。同时，辅以少量数据驱动采样以提升模型适应性。

Result: 实验结果表明，在均匀和非均匀加热条件下，所提出的PINN模型在保持与FEM相当的估计精度的同时，计算效率提升了40倍。

Conclusion: PINN方法能够有效且高效地解决核聚变装置的热传导估计问题，为实现实时模拟提供了可行方案。

Abstract: Estimating heat flux in the nuclear fusion device EAST is a critically
important task. Traditional scientific computing methods typically model this
process using the Finite Element Method (FEM). However, FEM relies on
grid-based sampling for computation, which is computationally inefficient and
hard to perform real-time simulations during actual experiments. Inspired by
artificial intelligence-powered scientific computing, this paper proposes a
novel Physics-Informed Neural Network (PINN) to address this challenge,
significantly accelerating the heat conduction estimation process while
maintaining high accuracy. Specifically, given inputs of different materials,
we first feed spatial coordinates and time stamps into the neural network, and
compute boundary loss, initial condition loss, and physical loss based on the
heat conduction equation. Additionally, we sample a small number of data points
in a data-driven manner to better fit the specific heat conduction scenario,
further enhancing the model's predictive capability. We conduct experiments
under both uniform and non-uniform heating conditions on the top surface.
Experimental results show that the proposed thermal conduction physics-informed
neural network achieves accuracy comparable to the finite element method, while
achieving $\times$40 times acceleration in computational efficiency. The
dataset and source code will be released on
https://github.com/Event-AHU/OpenFusion.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [50] [CASH: Context-Aware Smart Handover for Reliable UAV Connectivity on Aerial Corridors](https://arxiv.org/abs/2508.03862)
*Abdul Saboor,Zhuangzhuang Cui,Achiel Colpaert,Evgenii Vinogradov,Sofie Pollin*

Main category: cs.NI

TL;DR: 为解决城市空中交通（UAM）中无人机频繁切换导致的连接问题，提出CASH协议，通过前瞻性决策显著降低切换频率，提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 城市空中交通（UAM）依赖空中走廊支持无人机三维移动，但高移动性导致频繁切换，从而降低网络性能，确保可靠连接是一个关键挑战。

Method: 提出上下文感知智能切换（CASH）协议。该协议利用基于无人机轨迹的前瞻性评分机制，做出主动切换决策。通过定制模拟器评估其性能，并与现有切换协议进行比较。此外，还研究了基站密度和安全裕度对切换性能的影响。

Result: CASH协议将切换频率降低了高达78%，同时保持了较低的中断概率。研究还通过经验获得了基站密度和安全裕度的最佳设置，以确保可靠的UAM通信。

Conclusion: CASH协议通过智能前瞻性切换有效解决了UAM中无人机频繁切换导致的性能下降问题，显著提高了连接可靠性，并为UAM网络部署提供了关键参数优化建议。

Abstract: Urban Air Mobility (UAM) envisions aerial corridors for Unmanned Aerial
Vehicles (UAVs) to reduce ground traffic congestion by supporting 3D mobility,
such as air taxis. A key challenge in these high-mobility aerial corridors is
ensuring reliable connectivity, where frequent handovers can degrade network
performance. To resolve this, we present a Context-Aware Smart Handover (CASH)
protocol that uses a forward-looking scoring mechanism based on UAV trajectory
to make proactive handover decisions. We evaluate the performance of the
proposed CASH against existing handover protocols in a custom-built simulator.
Results show that CASH reduces handover frequency by up to 78% while
maintaining low outage probability. We then investigate the impact of base
station density and safety margin on handover performance, where their optimal
setups are empirically obtained to ensure reliable UAM communication.

</details>


### [51] [Confidence Driven Classification of Application Types in the Presence of Background Network](https://arxiv.org/abs/2508.03891)
*Eun Hun Choi,Jasleen Kaur,Vladas Pipiras,Nelson Gomes Rodrigues Antunes,Brendan Massey*

Main category: cs.NI

TL;DR: 现有深度学习流量分类器在真实流量中因背景流量表现不佳，添加背景类别会加剧混淆。本文提出一种基于高斯混合模型（GMM）的框架，以提高分类置信度，实现更可靠的分类。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习网络流量分类器在真实数据中因非应用特定背景流量（如广告、分析流量）的存在而性能不佳。先进分类器在训练时忽略了此类流量，而简单添加背景流量类别则因其异构性导致应用流量与背景流量之间的额外混淆。为避免误将背景流量分类为应用类型，需要一种可靠的置信度度量方法来处理不确定样本。

Method: 本文设计了一个基于高斯混合模型（GMM）的分类框架，旨在改善深度学习分类器的置信度指示，从而实现更可靠的网络流量应用类型识别。

Result: 摘要中未明确提及具体的研究结果。

Conclusion: 针对现有深度学习流量分类器在处理真实世界背景流量时的不足，本文提出了一种基于高斯混合模型的分类框架，以提高分类器的置信度并实现更可靠的网络流量应用类型识别。

Abstract: Accurately classifying the application types of network traffic using deep
learning models has recently gained popularity. However, we find that these
classifiers do not perform well on real-world traffic data due to the presence
of non-application-specific generic background traffic originating from
advertisements, analytics, shared APIs, and trackers. Unfortunately,
state-of-the-art application classifiers overlook such traffic in curated
datasets and only classify relevant application traffic. To address this issue,
when we label and train using an additional class for background traffic, it
leads to additional confusion between application and background traffic, as
the latter is heterogeneous and encompasses all traffic that is not relevant to
the application sessions. To avoid falsely classifying background traffic as
one of the relevant application types, a reliable confidence measure is
warranted, such that we can refrain from classifying uncertain samples.
Therefore, we design a Gaussian Mixture Model-based classification framework
that improves the indication of the deep learning classifier's confidence to
allow more reliable classification.

</details>


### [52] [Enabling Site-Specific Cellular Network Simulation Through Ray-Tracing-Driven ns-3](https://arxiv.org/abs/2508.04004)
*Tanguy Ropitault,Matteo Bordin,Paolo Testolina,Michele Polese,Pedram Johari,Nada Golmie,Tommaso Melodia*

Main category: cs.NI

TL;DR: 论文通过向5G-LENA扩展一个基于迹线的新信道模型，实现了高精度、特定站点的蜂窝网络仿真，揭示了传统统计模型无法捕捉的性能细节，并为数字孪生研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前蜂窝系统评估（5G NR到6G）复杂且性能依赖多因素耦合。现有系统级仿真多用统计3GPP信道模型，但其无法重现特定站点的几何现象（如衍射、阻塞），这阻碍了高保真度和站点感知研究（如波束管理）。

Method: 将5G-LENA（ns-3的NR模块）扩展为基于迹线的信道模型。该模型处理来自外部射线追踪器或测量活动的多径分量（MPCs），构建频域信道矩阵，并无缝集成到现有PHY/MAC堆栈。通过波束赋形验证和端到端指标分析进行功能展示。

Result: 开发出兼容3GPP的几何基信道模型，提供特定站点几何保真度。该模型在波束赋形验证和端到端指标分析中，揭示了统计模型无法展现的性能变化。

Conclusion: 新模块是高保真蜂窝网络系统级研究的关键构建块，为数字孪生能力奠定基础，并能支持波束管理、阻塞缓解等需要站点感知的深入研究。

Abstract: Evaluating cellular systems, from 5G New Radio (NR) and 5G-Advanced to 6G, is
challenging because the performance emerges from the tight coupling of
propagation, beam management, scheduling, and higher-layer interactions.
System-level simulation is therefore indispensable, yet the vast majority of
studies rely on the statistical 3GPP channel models. These are well suited to
capture average behavior across many statistical realizations, but cannot
reproduce site-specific phenomena such as corner diffraction, street-canyon
blockage, or deterministic line-of-sight conditions and
angle-of-departure/arrival relationships that drive directional links. This
paper extends 5G-LENA, an NR module for the system-level Network Simulator 3
(ns-3), with a trace-based channel model that processes the Multipath
Components (MPCs) obtained from external ray-tracers (e.g., Sionna Ray Tracer
(RT)) or measurement campaigns. Our module constructs frequency-domain channel
matrices and feeds them to the existing Physical (PHY)/Medium Access Control
(MAC) stack without any further modifications. The result is a geometry-based
channel model that remains fully compatible with the standard 3GPP
implementation in 5G-LENA, while delivering site-specific geometric fidelity.
This new module provides a key building block toward Digital Twin (DT)
capabilities by offering realistic site-specific channel modeling, unlocking
studies that require site awareness, including beam management, blockage
mitigation, and environment-aware sensing. We demonstrate its capabilities for
precise beam-steering validation and end-to-end metric analysis. In both cases,
the trace-driven engine exposes performance inflections that the statistical
model does not exhibit, confirming its value for high-fidelity system-level
cellular networks research and as a step toward DT applications.

</details>


### [53] [A Novel Hierarchical Co-Optimization Framework for Coordinated Task Scheduling and Power Dispatch in Computing Power Networks](https://arxiv.org/abs/2508.04015)
*Haoxiang Luo,Kun Yang,Qi Huang,Schahram Dustdar*

Main category: cs.NI

TL;DR: 本文提出了一种双阶段协同优化（TSCO）框架，旨在通过协同管理电力系统调度和计算能力网络（CPN）任务调度，解决CPN高能耗和可再生能源并网导致电网不稳定的双重挑战，以实现低碳运营。


<details>
  <summary>Details</summary>
Motivation: 大规模人工智能和数据密集型应用的普及推动了计算能力网络（CPN）的发展，但其巨大的能耗带来了可持续性挑战。同时，高渗透率的间歇性可再生能源（RES）导致电网稳定性问题。本研究旨在应对这些双重挑战。

Method: 提出了一种新颖的双阶段协同优化（TSCO）框架。该框架将复杂问题分解为日前随机机组组合（SUC）阶段（通过Benders分解求解）和实时运行阶段。在实时阶段，发电资产的经济调度与由深度强化学习（DRL）代理管理的自适应CPN任务调度相结合，该代理能根据实时电价和边际碳强度做出碳感知决策。

Result: 在集成CPN的IEEE 30-bus系统上的广泛模拟表明，TSCO框架显著优于基线方法。结果显示，该框架减少了总碳排放和运营成本，同时将可再生能源弃用量减少了60%以上，并保持了计算任务严格的服务质量（QoS）。

Conclusion: 所提出的TSCO框架有效地解决了计算能力网络能耗和电网不稳定的双重挑战，实现了低碳运营，降低了成本，并提升了可再生能源的利用率，同时保证了服务质量。

Abstract: The proliferation of large-scale artificial intelligence and data-intensive
applications has spurred the development of Computing Power Networks (CPNs),
which promise to deliver ubiquitous and on-demand computational resources.
However, the immense energy consumption of these networks poses a significant
sustainability challenge. Simultaneously, power grids are grappling with the
instability introduced by the high penetration of intermittent renewable energy
sources (RES). This paper addresses these dual challenges through a novel
Two-Stage Co-Optimization (TSCO) framework that synergistically manages power
system dispatch and CPN task scheduling to achieve low-carbon operations. The
framework decomposes the complex, large-scale problem into a day-ahead
stochastic unit commitment (SUC) stage and a real-time operational stage. The
former is solved using Benders decomposition for computational tractability,
while in the latter, economic dispatch of generation assets is coupled with an
adaptive CPN task scheduling managed by a Deep Reinforcement Learning (DRL)
agent. This agent makes intelligent, carbon-aware decisions by responding to
dynamic grid conditions, including real-time electricity prices and marginal
carbon intensity. Through extensive simulations on an IEEE 30-bus system
integrated with a CPN, the TSCO framework is shown to significantly outperform
baseline approaches. Results demonstrate that the proposed framework reduces
total carbon emissions and operational costs, while simultaneously decreasing
RES curtailment by more than 60% and maintaining stringent Quality of Service
(QoS) for computational tasks.

</details>


### [54] [Metaverse Framework for Wireless Systems Management](https://arxiv.org/abs/2508.04150)
*Ilias Chrysovergis,Alexandros-Apostolos A. Boulogeorgos,Theodoros A. Tsiftsis,Dusit Niyato*

Main category: cs.NI

TL;DR: 本文介绍了一个全面的元宇宙框架，用于无线系统的模拟、仿真和交互，整合了XR、数字孪生、AI、IoT、区块链和5G/6G等核心技术。


<details>
  <summary>Details</summary>
Motivation: 旨在为无线系统的开发和管理创建一个动态、沉浸式的元宇宙平台，并通过提供对未来网络环境的深入见解来探索、发展和优化无线系统。

Method: 该框架通过整合以下核心元宇宙技术实现：扩展现实（XR）用于可视化和交互；数字孪生（DTs）用于实时监控和优化；人工智能（AI）用于生成3D内容、增强决策和系统性能；物联网（IoT）设备提供实时传感器数据以提高模拟精度；区块链确保安全、去中心化交互；以及5G/6G网络提供无缝、低延迟的通信基础设施。

Result: 所提出的框架能够实现复杂无线系统的可视化和交互，提供实时监控和优化能力，通过AI生成3D内容并提升决策和系统性能，利用IoT数据提高模拟精度，并通过区块链确保安全交互。它作为一个强大的工具，支持无线系统的探索、开发和优化。

Conclusion: 该全面的元宇宙框架为探索、开发和优化无线系统提供了一个强大的工具，有望为未来网络环境提供有价值的见解。

Abstract: This article introduces a comprehensive metaverse framework, which is
designed for the simulation, emulation, and interaction with wireless systems.
The proposed framework integrates core metaverse technologies such as extended
reality (XR), digital twins (DTs), artificial intelligence (AI), internet of
things (IoT), blockchain, and advanced 6G networking solutions to create a
dynamic, immersive platform for both system development and management. By
leveraging XR, users can visualize and engage with complex systems, while DTs
enable real-time monitoring and optimization. AI generates the
three-dimensional (3D) content, enhances decision-making and system
performance, whereas IoT devices provide real-time sensor data for boosting the
simulation accuracy. Additionally, blockchain ensures secure, decentralized
interactions, and 5G/6G networks offer the necessary infrastructure for
seamless, low-latency communication. This framework serves as a robust tool for
exploring, developing, and optimizing wireless systems, aiming to provide
valuable insights into the future of networked environments.

</details>


### [55] [DSNS: The Deep Space Network Simulator](https://arxiv.org/abs/2508.04317)
*Joshua Smailes,Filip Futera,Sebastian Köhler,Simon Birnbach,Martin Strohmeier,Ivan Martinovic*

Main category: cs.NI

TL;DR: 现有模拟工具无法满足大规模卫星和星际网络需求，本文提出了新型深空网络模拟器DSNS，该模拟器在可扩展性、灵活性和精确性上超越现有工具，有助于加速未来卫星网络发展。


<details>
  <summary>Details</summary>
Motivation: 随着卫星网络节点数量增多（数千个）和星际互联网的发展，现有卫星和网络模拟工具在处理此类大规模复杂网络时已变得不切实际。

Method: 研究提出并开发了深空网络模拟器（DSNS），这是一种专注于大规模卫星网络的新型网络模拟器。

Result: DSNS相比现有工具展现出显著的改进能力；通过实现现有协议和CCSDS推荐的DTN模拟参考场景，展示了其灵活性和可扩展性；在可扩展性方面超越现有工具，同时提供更高的仿真精度。

Conclusion: DSNS为标准机构和卫星运营商提供了实用价值，能够加速协议开发和参数测试，从而推动未来卫星网络的快速安全通信。

Abstract: Simulation tools are commonly used in the development and testing of new
protocols or new networks. However, as satellite networks start to grow to
encompass thousands of nodes, and as companies and space agencies begin to
realize the interplanetary internet, existing satellite and network simulation
tools have become impractical for use in this context.
  We therefore present the Deep Space Network Simulator (DSNS): a new network
simulator with a focus on large-scale satellite networks. We demonstrate its
improved capabilities compared to existing offerings, showcase its flexibility
and extensibility through an implementation of existing protocols and the DTN
simulation reference scenarios recommended by CCSDS, and evaluate its
scalability, showing that it exceeds existing tools while providing better
fidelity.
  DSNS provides concrete usefulness to both standards bodies and satellite
operators, enabling fast iteration on protocol development and testing of
parameters under highly realistic conditions. By removing roadblocks to
research and innovation, we can accelerate the development of upcoming
satellite networks and ensure that their communication is both fast and secure.

</details>


### [56] [Empowering Nanoscale Connectivity through Molecular Communication: A Case Study of Virus Infection](https://arxiv.org/abs/2508.04415)
*Xuan Chen,Yu Huang,Miaowen Wen,Shahid Mumtaz,Fatih Gulec,Anwer Al-Dulaimi,Andrew W. Eckford*

Main category: cs.NI

TL;DR: 研究通过分子通信在生物纳米物联网中实现病毒传播建模、检测与变异识别以防控疫情。


<details>
  <summary>Details</summary>
Motivation: 鉴于生物纳米物联网（IoBNT）在流行病控制中的潜力，本文旨在利用分子通信（MC）解决构建IoBNT在病毒传播建模、病毒/感染者检测及病毒突变识别方面的挑战。

Method: 1. 讨论宏观和微观尺度下的分子通信信道，以匹配病毒传播。2. 研究相应尺度下的检测方法，并设计病毒/感染者的定位机制。3. 提出病毒突变识别策略，并使用ORF3a蛋白进行仿真验证。4. 应用分子通信中的信号处理技术对抗病毒传播。

Result: 论文探讨并提出了在不同尺度下通过分子通信进行病毒传播建模、检测和定位的方法。成功提出了一种病毒突变识别策略，并通过仿真验证了其可行性。

Conclusion: 本文深入分析了通过分子通信进行病毒传播的机制，并提出利用分子通信中的信号处理技术有效对抗病毒传播，为基于IoBNT的疫情控制提供了新颖的理论和技术路径。

Abstract: The Internet of Bio-Nano Things (IoBNT), envisioned as a revolutionary
healthcare paradigm, shows promise for epidemic control. This paper explores
the potential of using molecular communication (MC) to address the challenges
in constructing IoBNT for epidemic prevention, specifically focusing on
modeling viral transmission, detecting the virus/infected individuals, and
identifying virus mutations. First, the MC channels in macroscale and
microscale scenarios are discussed to match viral transmission in both scales
separately. Besides, the detection methods for these two scales are also
studied, along with the localization mechanism designed for the virus/infected
individuals. Moreover, an identification strategy is proposed to determine
potential virus mutations, which is validated through simulation using the
ORF3a protein as a benchmark. Finally, open research issues are discussed. In
summary, this paper aims to analyze viral transmission through MC and combat
viral spread using signal processing techniques within MC.

</details>


### [57] [Policy Design in Zero-Trust Distributed Networks: Challenges and Solutions](https://arxiv.org/abs/2508.04526)
*Fannya R. Sandjaja,Ayesha A. Majeed,Abdullah Abdullah,Gyan Wickremasinghe,Karen Rafferty,Vishal Sharma*

Main category: cs.NI

TL;DR: 本文探讨分布式网络中零信任架构（ZTA）策略的设计挑战与解决方案，并通过UPPAAL进行形式化验证的案例研究，强调策略安全性和系统问责制的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统安全架构因对信任的过度依赖，在分布式攻击面前日益脆弱，尤其在整合智能AI和密集物联网（IoT）等分布式系统时问题更为突出。尽管零信任架构（ZTA）被视为潜在解决方案，但其策略若未经严格验证，可能导致未授权访问。因此，研究在分布式网络环境下ZTA策略设计（即零信任分布式网络ZTDN）的挑战与解决方案变得至关重要。

Method: 本研究首先深入探讨了零信任分布式网络（ZTDN）中ZTA策略设计面临的挑战并提出相应解决方案。随后，通过使用UPPAAL工具对ZTA策略进行形式化验证，进行了具体的案例研究。此外，论文还讨论了在系统安全中确立问责制和责任的重要性。

Result: 本文通过对零信任分布式网络（ZTDN）中ZTA策略设计的挑战和解决方案的探索，识别了关键问题并提出了潜在方向。通过使用UPPAAL进行的形式化验证案例研究，展示了验证ZTA策略的可行性与效用。研究还强调了问责制和责任在确保系统安全中的核心作用。

Conclusion: 在日益复杂的分布式网络环境中，ZTA策略的严谨设计与验证对于提升系统安全性至关重要。形式化验证工具（如UPPAAL）能有效提升策略的可靠性，同时，明确的问责制和责任是维护系统整体安全不可或缺的组成部分。

Abstract: Traditional security architectures are becoming more vulnerable to
distributed attacks due to significant dependence on trust. This will further
escalate when implementing agentic AI within the systems, as more components
must be secured over a similar distributed space. These scenarios can be
observed in consumer technologies, such as the dense Internet of things (IoT).
Here, zero-trust architecture (ZTA) can be seen as a potential solution, which
relies on a key principle of not giving users explicit trust, instead always
verifying their privileges whenever a request is made. However, the overall
security in ZTA is managed through its policies, and unverified policies can
lead to unauthorized access. Thus, this paper explores challenges and solutions
for ZTA policy design in the context of distributed networks, which is referred
to as zero-trust distributed networks (ZTDN). This is followed by a case-study
on formal verification of policies using UPPAAL. Subsequently, the importance
of accountability and responsibility in the system's security is discussed.

</details>


### [58] [CONVERGE: A Multi-Agent Vision-Radio Architecture for xApps](https://arxiv.org/abs/2508.04556)
*Filipe B. Teixeira,Carolina Simões,Paulo Fidalgo,Wagner Pedrosa,André Coelho,Manuel Ricardo,Luis M. Pessoa*

Main category: cs.NI

TL;DR: 本文提出一种新颖的架构，通过多智能体方法将实时无线电和视频感知信息传递给O-RAN xApps，实现一体化感知与通信，并验证了其低延迟和实时控制5G/6G RAN的能力。


<details>
  <summary>Details</summary>
Motivation: 电信和计算机视觉独立发展，但高频无线链路多为视距操作，易受障碍物影响。视觉数据可用于预测信道动态，通过波束成形或切换技术克服障碍，从而提升通信性能。

Method: 提出一种新颖的多智能体架构，用于将实时无线电和视频感知信息传输给O-RAN xApps。引入一种新的视频功能，能够为xApps生成阻塞信息，从而实现一体化感知与通信（Integrated Sensing and Communications）。

Result: 实验结果表明，感知信息延迟保持在1毫秒以下。一个xApp能够成功利用无线电和视频感知信息实时控制5G/6G无线接入网（RAN）。

Conclusion: 该研究成功地将视觉感知与无线电感知相结合，为5G/6G RAN提供了实时的信道控制能力，通过低延迟的感知信息有效提升了通信系统的韧性与性能。

Abstract: Telecommunications and computer vision have evolved independently. With the
emergence of high-frequency wireless links operating mostly in line-of-sight,
visual data can help predict the channel dynamics by detecting obstacles and
help overcoming them through beamforming or handover techniques.
  This paper proposes a novel architecture for delivering real-time radio and
video sensing information to O-RAN xApps through a multi-agent approach, and
introduces a new video function capable of generating blockage information for
xApps, enabling Integrated Sensing and Communications. Experimental results
show that the delay of sensing information remains under 1\,ms and that an xApp
can successfully use radio and video sensing information to control the 5G/6G
RAN in real-time.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [59] [Entanglement distribution in quantum networks via swapping of partially entangled states](https://arxiv.org/abs/2508.04536)
*Henrique Guerra,Tailan S. Sarubi,Rafael Chaves,Jonas Maziero*

Main category: quant-ph

TL;DR: 分析了纠缠交换协议（ESP）在不同拓扑结构的量子网络中对初始部分纠缠态的应用，及其纠缠演化和成功概率。


<details>
  <summary>Details</summary>
Motivation: 将现有关于部分纠缠态可通过贝尔基测量进行纠缠集中和传输的发现，扩展到具有线性、星形和混合等多种拓扑结构的量子网络中，研究纠缠交换协议在初始部分纠缠态下的应用。

Method: 在具有线性、星形和混合拓扑的量子网络中，分析纠缠交换协议（ESP）对初始部分纠缠态的应用。通过检查初始态的变换，并评估输出生成最大纠缠态的成功概率来研究纠缠的演化。

Result: 研究结果提供了对量子网络中纠缠分发动力学的新见解。

Conclusion: 为在实际条件下设计稳健的量子通信策略提供了实用指导。

Abstract: The entanglement swapping protocol (ESP) is a fundamental primitive for
distributing quantum correlations across distant nodes in a quantum network.
Recent studies have demonstrated that even when the involved qubit pairs are
only partially entangled, it is still possible to concentrate and transmit
entanglement via Bell-basis measurements. In this work, we extend these ideas
to quantum networks with various topologies - including linear, star, and
hybrid configurations - by analyzing the application of the ESP to initially
partially entangled states. We investigate how entanglement evolves under such
protocols by examining the transformations of the initial states and evaluating
the success probabilities for generating maximally entangled states at the
output. Our results offer new insights into the dynamics of the entanglement
distribution in quantum networks and provide practical guidelines for designing
robust quantum communication strategies under realistic conditions.

</details>
