{"id": "2510.06901", "pdf": "https://arxiv.org/pdf/2510.06901", "abs": "https://arxiv.org/abs/2510.06901", "authors": ["Fangzhou Zhao", "Yao Sun", "Jianglin Lan", "Lan Zhang", "Xuesong Liu", "Muhammad Ali Imran"], "title": "Adaptive Semantic Communication for UAV/UGV Cooperative Path Planning", "categories": ["cs.NI"], "comment": null, "summary": "Effective path planning is fundamental to the coordination of unmanned aerial\nvehicles (UAVs) and unmanned ground vehicles (UGVs) systems, particularly in\napplications such as surveillance, navigation, and emergency response.\nCombining UAVs' broad field of view with UGVs' ground-level operational\ncapability greatly improve the likelihood of successfully achieving task\nobjectives such as locating victims, monitoring target areas, or navigating\nhazardous terrain. In complex environments, UAVs need to provide precise\nenvironmental perception information for UGVs to optimize their routing policy.\nHowever, due to severe interference and non-line-of-sight conditions, wireless\ncommunication is often unstable in such complex environments, making it\ndifficult to support timely and accurate path planning for UAV-UGV\ncoordination. To this end, this paper proposes a semantic communication\n(SemCom) framework to enhance UAV/UGV cooperative path planning under\nunreliable wireless conditions. Unlike traditional methods that transmit raw\ndata, SemCom transmits only the key information for path planning, reducing\ntransmission volume without sacrificing accuracy. The proposed framework is\ndeveloped by defining key semantics for path planning and designing a\ntransceiver for meeting the requirements of UAV-UGV cooperative path planning.\nSimulation results show that, compared to conventional SemCom transceivers, the\nproposed transceiver significantly reduces data transmission volume while\nmaintaining path planning accuracy, thereby enhancing system collaboration\nefficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u4ee5\u5728\u4e0d\u53ef\u9760\u65e0\u7ebf\u6761\u4ef6\u4e0b\u589e\u5f3a\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u7684\u534f\u540c\u8def\u5f84\u89c4\u5212\u6548\u7387\uff0c\u901a\u8fc7\u4f20\u8f93\u5173\u952e\u8bed\u4e49\u4fe1\u606f\u51cf\u5c11\u6570\u636e\u91cf\u5e76\u4fdd\u6301\u89c4\u5212\u7cbe\u5ea6\u3002", "motivation": "\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u5730\u9762\u8f66\u8f86\uff08UAV-UGV\uff09\u7cfb\u7edf\u5728\u76d1\u63a7\u3001\u5bfc\u822a\u548c\u5e94\u6025\u54cd\u5e94\u7b49\u4efb\u52a1\u4e2d\uff0c\u6709\u6548\u7684\u8def\u5f84\u89c4\u5212\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\uff0c\u65e0\u7ebf\u901a\u4fe1\u901a\u5e38\u4e0d\u7a33\u5b9a\uff0c\u96be\u4ee5\u652f\u6301\u53ca\u65f6\u51c6\u786e\u7684\u8def\u5f84\u89c4\u5212\uff0c\u4ece\u800c\u963b\u788d\u4e86\u7cfb\u7edf\u534f\u4f5c\u6548\u7387\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u8bed\u4e49\u901a\u4fe1\uff08SemCom\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4e3a\u8def\u5f84\u89c4\u5212\u5b9a\u4e49\u5173\u952e\u8bed\u4e49\u5e76\u8bbe\u8ba1\u4e00\u4e2a\u6ee1\u8db3UAV-UGV\u534f\u540c\u8def\u5f84\u89c4\u5212\u9700\u6c42\u7684\u6536\u53d1\u5668\u3002\u8be5\u65b9\u6cd5\u533a\u522b\u4e8e\u4f20\u7edf\u539f\u59cb\u6570\u636e\u4f20\u8f93\uff0c\u53ea\u4f20\u8f93\u8def\u5f84\u89c4\u5212\u7684\u5173\u952e\u4fe1\u606f\u4ee5\u51cf\u5c11\u4f20\u8f93\u91cf\u800c\u4e0d\u727a\u7272\u7cbe\u5ea6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edfSemCom\u6536\u53d1\u5668\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u6536\u53d1\u5668\u663e\u8457\u51cf\u5c11\u4e86\u6570\u636e\u4f20\u8f93\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8def\u5f84\u89c4\u5212\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u4fe1\u606f\u4f20\u8f93\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u4e0d\u53ef\u9760\u65e0\u7ebf\u6761\u4ef6\u4e0b\u65e0\u4eba\u673a/\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u7684\u7cfb\u7edf\u534f\u4f5c\u6548\u7387\u548c\u8def\u5f84\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2510.06916", "pdf": "https://arxiv.org/pdf/2510.06916", "abs": "https://arxiv.org/abs/2510.06916", "authors": ["Fangzhou Zhao", "Yao Sun", "Jianglin Lan", "Muhammad Ali Imran"], "title": "Dynamic Control Aware Semantic Communication Enabled Image Transmission for Lunar Landing", "categories": ["cs.NI"], "comment": null, "summary": "The primary challenge in autonomous lunar landing missions lies in the\nunreliable local control system, which has limited capacity to handle\nhigh-dynamic conditions, severely affecting landing precision and safety.\nRecent advancements in lunar satellite communication make it possible to\nestablish a wireless link between lunar orbit satellites and the lunar lander.\nThis enables satellites to run high-performance autonomous landing algorithms,\nimproving landing accuracy while reducing the lander's computational and\nstorage load. Nevertheless, traditional communication paradigms are not\ndirectly applicable due to significant temperature fluctuations on the lunar\nsurface, intense solar radiation, and severe interference caused by lunar dust\non hardware. The emerging technique of semantic communication (SemCom) offers\nsignificant advantages in robustness and resource efficiency, particularly\nunder harsh channel conditions. In this paper, we introduce a novel SemCom\nframework for transmitting images from the lander to satellites operating the\nremote landing control system. The proposed encoder-decoder dynamically adjusts\nthe transmission strategy based on real-time feedback from the lander's control\nalgorithm, ensuring the accurate delivery of critical image features and\nenhancing control reliability. We provide a rigorous theoretical analysis of\nthe conditions that improve the accuracy of the control algorithm and reduce\nend-to-end transmission time under the proposed framework. Simulation results\ndemonstrate that our SemCom method significantly enhances autonomous landing\nperformance compared to traditional communication methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u521b\u65b0\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u7528\u4e8e\u6708\u7403\u7740\u9646\u5668\u4e0e\u8f68\u9053\u536b\u661f\u95f4\u7684\u56fe\u50cf\u4f20\u8f93\uff0c\u65e8\u5728\u514b\u670d\u6076\u52a3\u6708\u7403\u73af\u5883\u4e0b\u7684\u901a\u4fe1\u6311\u6218\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4f20\u8f93\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u6708\u7403\u81ea\u4e3b\u7740\u9646\u7684\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u6708\u7403\u81ea\u4e3b\u7740\u9646\u4efb\u52a1\u9762\u4e34\u672c\u5730\u63a7\u5236\u7cfb\u7edf\u4e0d\u53ef\u9760\u3001\u5904\u7406\u9ad8\u52a8\u6001\u6761\u4ef6\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u4e14\u6708\u7403\u8868\u9762\u6076\u52a3\u73af\u5883\uff08\u6781\u7aef\u6e29\u5dee\u3001\u5f3a\u592a\u9633\u8f90\u5c04\u3001\u6708\u5c18\u5e72\u6270\uff09\u4f7f\u4f20\u7edf\u901a\u4fe1\u8303\u5f0f\u5931\u6548\u3002\u6025\u9700\u4e00\u79cd\u9c81\u68d2\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u901a\u4fe1\u65b9\u6848\uff0c\u4ee5\u652f\u6301\u536b\u661f\u7aef\u9ad8\u6027\u80fd\u81ea\u4e3b\u7740\u9646\u7b97\u6cd5\u5e76\u51cf\u8f7b\u7740\u9646\u5668\u8ba1\u7b97\u8d1f\u8377\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u8bed\u4e49\u901a\u4fe1\uff08SemCom\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7740\u9646\u5668\u5411\u64cd\u4f5c\u8fdc\u7a0b\u7740\u9646\u63a7\u5236\u7cfb\u7edf\u7684\u536b\u661f\u4f20\u8f93\u56fe\u50cf\u3002\u8be5\u6846\u67b6\u7684\u6838\u5fc3\u662f\u4e00\u4e2a\u7f16\u7801\u5668-\u89e3\u7801\u5668\uff0c\u5b83\u6839\u636e\u7740\u9646\u5668\u63a7\u5236\u7b97\u6cd5\u7684\u5b9e\u65f6\u53cd\u9988\u52a8\u6001\u8c03\u6574\u4f20\u8f93\u7b56\u7565\uff0c\u4ee5\u786e\u4fdd\u5173\u952e\u56fe\u50cf\u7279\u5f81\u7684\u51c6\u786e\u4f20\u9012\u548c\u63a7\u5236\u53ef\u9760\u6027\u3002\u8bba\u6587\u8fd8\u63d0\u4f9b\u4e86\u5173\u4e8e\u63d0\u5347\u63a7\u5236\u7b97\u6cd5\u7cbe\u5ea6\u548c\u51cf\u5c11\u7aef\u5230\u7aef\u4f20\u8f93\u65f6\u95f4\u7684\u7406\u8bba\u5206\u6790\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u901a\u4fe1\u65b9\u6cd5\u76f8\u6bd4\uff0c\u672c\u6587\u63d0\u51fa\u7684\u8bed\u4e49\u901a\u4fe1\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u81ea\u4e3b\u7740\u9646\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bed\u4e49\u901a\u4fe1\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u589e\u5f3a\u6708\u7403\u81ea\u4e3b\u7740\u9646\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u672a\u6765\u6708\u7403\u63a2\u7d22\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u9014\u5f84\u3002"}}
{"id": "2510.07292", "pdf": "https://arxiv.org/pdf/2510.07292", "abs": "https://arxiv.org/abs/2510.07292", "authors": ["Tiago Silva", "Ant\u00f3nio Grilo"], "title": "A Genetic Algorithm Approach to Anti-Jamming UAV Swarm Behavior", "categories": ["cs.NI", "cs.SY", "eess.SY"], "comment": "8 pages, conference paper", "summary": "In recent years, Unmanned Aerial Vehicles (UAVs) have brought a new true\nrevolution to military tactics. While UAVs already constitute an advantage when\noperating alone, multi-UAV swarms expand the available possibilities, allowing\nthe UAVs to collaborate and support each other as a team to carry out a given\ntask. This entails the capability to exchange information related with\nsituation awareness and action coordination by means of a suitable wireless\ncommunication technology. In such scenario, the adversary is expected to\ndisrupt communications by jamming the communication channel. The latter becomes\nthe Achilles heel of the swarm. While anti-jamming techniques constitute a well\ncovered topic in the literature, the use of intelligent swarm behaviors to\nleverage those techniques is still an open research issue.\n  This paper explores the use of Genetic Algorithms (GAs) to jointly optimize\nUAV swarm formation, beam-steering antennas and traffic routing in order to\nmitigate the effect of jamming in the main coordination channel, under the\nassumption that a more robust and low data rate channel is used for formation\nmanagement signaling. Simulation results show the effectiveness of proposed\napproach. However, the significant computational cost paves the way for further\nresearch.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u65e0\u4eba\u673a\u8702\u7fa4\u7f16\u961f\u3001\u6ce2\u675f\u8d4b\u5f62\u5929\u7ebf\u548c\u8def\u7531\uff0c\u4ee5\u51cf\u8f7b\u4e3b\u534f\u8c03\u4fe1\u9053\u53d7\u5230\u7684\u5e72\u6270\uff0c\u4eff\u771f\u663e\u793a\u5176\u6709\u6548\u6027\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8f83\u9ad8\u3002", "motivation": "\u591a\u65e0\u4eba\u673a\u8702\u7fa4\u5728\u519b\u4e8b\u6218\u672f\u4e2d\u5177\u6709\u9769\u547d\u6027\u6f5c\u529b\uff0c\u4f46\u5176\u5173\u952e\u7684\u65e0\u7ebf\u901a\u4fe1\u6613\u53d7\u654c\u65b9\u5e72\u6270\uff0c\u6210\u4e3a\u8702\u7fa4\u7684\u201c\u963f\u5580\u7409\u65af\u4e4b\u8e35\u201d\u3002\u73b0\u6709\u53cd\u5e72\u6270\u6280\u672f\u5c1a\u672a\u5145\u5206\u5229\u7528\u667a\u80fd\u8702\u7fa4\u884c\u4e3a\u6765\u589e\u5f3a\u6548\u679c\u3002", "method": "\u7814\u7a76\u91c7\u7528\u9057\u4f20\u7b97\u6cd5\uff08GAs\uff09\u6765\u8054\u5408\u4f18\u5316\u65e0\u4eba\u673a\u8702\u7fa4\u7684\u7f16\u961f\u3001\u6ce2\u675f\u8d4b\u5f62\u5929\u7ebf\u4ee5\u53ca\u6d41\u91cf\u8def\u7531\uff0c\u4ee5\u51cf\u8f7b\u4e3b\u534f\u8c03\u4fe1\u9053\u4e2d\u7684\u5e72\u6270\u5f71\u54cd\u3002\u5047\u8bbe\u7f16\u961f\u7ba1\u7406\u4fe1\u53f7\u4f7f\u7528\u4e00\u4e2a\u66f4\u9c81\u68d2\u7684\u4f4e\u6570\u636e\u901f\u7387\u4fe1\u9053\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u8f7b\u4e3b\u534f\u8c03\u4fe1\u9053\u4e2d\u7684\u5e72\u6270\u6548\u5e94\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51cf\u8f7b\u65e0\u4eba\u673a\u8702\u7fa4\u901a\u4fe1\u5e72\u6270\u65b9\u9762\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u4f46\u5176\u663e\u8457\u7684\u8ba1\u7b97\u6210\u672c\u8868\u660e\u9700\u8981\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u4ee5\u4f18\u5316\u548c\u6539\u8fdb\u3002"}}
{"id": "2510.06530", "pdf": "https://arxiv.org/pdf/2510.06530", "abs": "https://arxiv.org/abs/2510.06530", "authors": ["Thusitha Dayaratne", "Ngoc Duy Pham", "Viet Vo", "Shangqi Lai", "Sharif Abuadbba", "Hajime Suzuki", "Xingliang Yuan", "Carsten Rudolph"], "title": "From Description to Detection: LLM based Extendable O-RAN Compliant Blind DoS Detection in 5G and Beyond", "categories": ["cs.CR", "cs.ET", "cs.LG", "cs.NI"], "comment": null, "summary": "The quality and experience of mobile communication have significantly\nimproved with the introduction of 5G, and these improvements are expected to\ncontinue beyond the 5G era. However, vulnerabilities in control-plane\nprotocols, such as Radio Resource Control (RRC) and Non-Access Stratum (NAS),\npose significant security threats, such as Blind Denial of Service (DoS)\nattacks. Despite the availability of existing anomaly detection methods that\nleverage rule-based systems or traditional machine learning methods, these\nmethods have several limitations, including the need for extensive training\ndata, predefined rules, and limited explainability. Addressing these\nchallenges, we propose a novel anomaly detection framework that leverages the\ncapabilities of Large Language Models (LLMs) in zero-shot mode with unordered\ndata and short natural language attack descriptions within the Open Radio\nAccess Network (O-RAN) architecture. We analyse robustness to prompt variation,\ndemonstrate the practicality of automating the attack descriptions and show\nthat detection quality relies on the semantic completeness of the description\nrather than its phrasing or length. We utilise an RRC/NAS dataset to evaluate\nthe solution and provide an extensive comparison of open-source and proprietary\nLLM implementations to demonstrate superior performance in attack detection. We\nfurther validate the practicality of our framework within O-RAN's real-time\nconstraints, illustrating its potential for detecting other Layer-3 attacks.", "AI": {"tldr": "\u9488\u5bf95G\u63a7\u5236\u5e73\u9762\u534f\u8bae\uff08RRC/NAS\uff09\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u96f6\u6837\u672c\u80fd\u529b\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u5728O-RAN\u67b6\u6784\u4e0b\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u653b\u51fb\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u68c0\u6d4b\u6027\u80fd\u548c\u5b9e\u65f6\u5b9e\u7528\u6027\u3002", "motivation": "5G\u79fb\u52a8\u901a\u4fe1\u7684RRC\u548cNAS\u7b49\u63a7\u5236\u5e73\u9762\u534f\u8bae\u5b58\u5728\u663e\u8457\u5b89\u5168\u6f0f\u6d1e\uff0c\u5982\u76f2\u62d2\u7edd\u670d\u52a1\uff08DoS\uff09\u653b\u51fb\u3002\u73b0\u6709\u57fa\u4e8e\u89c4\u5219\u6216\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u5927\u3001\u89c4\u5219\u9884\u5b9a\u4e49\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7b49\u5c40\u9650\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u96f6\u6837\u672c\u6a21\u5f0f\u80fd\u529b\uff0c\u5728Open Radio Access Network (O-RAN) \u67b6\u6784\u4e2d\u5904\u7406\u65e0\u5e8f\u6570\u636e\u548c\u7b80\u77ed\u7684\u81ea\u7136\u8bed\u8a00\u653b\u51fb\u63cf\u8ff0\u3002\u8be5\u6846\u67b6\u901a\u8fc7RRC/NAS\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5bf9\u5f00\u6e90\u548c\u4e13\u6709LLM\u5b9e\u73b0\u8fdb\u884c\u4e86\u5e7f\u6cdb\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u5206\u6790\u4e86\u6846\u67b6\u5bf9\u63d0\u793a\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u5c55\u793a\u4e86\u81ea\u52a8\u5316\u653b\u51fb\u63cf\u8ff0\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u53d1\u73b0\u68c0\u6d4b\u8d28\u91cf\u53d6\u51b3\u4e8e\u63cf\u8ff0\u7684\u8bed\u4e49\u5b8c\u6574\u6027\u800c\u975e\u63aa\u8f9e\u6216\u957f\u5ea6\u3002\u8be5\u6846\u67b6\u5728\u653b\u51fb\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u5728O-RAN\u7684\u5b9e\u65f6\u7ea6\u675f\u4e0b\u9a8c\u8bc1\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684LLM\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\u57285G O-RAN\u73af\u5883\u4e2d\u68c0\u6d4bRRC/NAS\u653b\u51fb\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\uff0c\u5e76\u5177\u6709\u68c0\u6d4b\u5176\u4ed6\u4e09\u5c42\u653b\u51fb\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.06239", "pdf": "https://arxiv.org/pdf/2510.06239", "abs": "https://arxiv.org/abs/2510.06239", "authors": ["Pranav Gupta"], "title": "OpenStaxQA: A multilingual dataset based on open-source college textbooks", "categories": ["cs.CL"], "comment": null, "summary": "We present OpenStaxQA, an evaluation benchmark specific to college-level\neducational applications based on 43 open-source college textbooks in English,\nSpanish, and Polish, available under a permissive Creative Commons license. We\nfinetune and evaluate large language models (LLMs) with approximately 7 billion\nparameters on this dataset using quantized low rank adapters (QLoRa).\nAdditionally we also perform a zero-shot evaluation on the AI2 reasoning\nchallenge dev dataset in order to check if OpenStaxQA can lead to an improved\nperformance on other tasks. We also discuss broader impacts relevant to\ndatasets such as OpenStaxQA.", "AI": {"tldr": "\u63d0\u51faOpenStaxQA\uff0c\u4e00\u4e2a\u57fa\u4e8e43\u672c\u591a\u8bed\u8a00\u5927\u5b66\u6559\u6750\u7684\u6559\u80b2\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u5fae\u8c03\u548c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u4e3a\u5927\u5b66\u7ea7\u522b\u6559\u80b2\u5e94\u7528\u521b\u5efa\u4e00\u4e2a\u4e13\u95e8\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u8003\u5bdf\u6a21\u578b\u5728\u6b64\u57fa\u51c6\u4e0a\u5fae\u8c03\u540e\u5bf9\u5176\u4ed6\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86OpenStaxQA\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u548c\u6ce2\u5170\u8bed\u768443\u672c\u5f00\u6e90\u5927\u5b66\u6559\u6750\uff1b\u4f7f\u7528\u91cf\u5316\u4f4e\u79e9\u9002\u914d\u5668\uff08QLoRa\uff09\u5bf9\u7ea670\u4ebf\u53c2\u6570\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u548c\u8bc4\u4f30\uff1b\u5728AI2\u63a8\u7406\u6311\u6218\u5f00\u53d1\u96c6\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\u4ee5\u6d4b\u8bd5\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u6458\u8981\u4e2d\u672a\u63d0\u4f9b\u5177\u4f53\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u7ed3\u679c\u6216\u6cdb\u5316\u6027\u80fd\u6570\u636e\u3002", "conclusion": "OpenStaxQA\u4e3a\u5927\u5b66\u7ea7\u522b\u7684\u6559\u80b2\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u8bed\u8a00\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u6709\u52a9\u4e8e\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u8868\u73b0\u53ca\u5176\u66f4\u5e7f\u6cdb\u7684\u5f71\u54cd\u3002"}}
{"id": "2510.06261", "pdf": "https://arxiv.org/pdf/2510.06261", "abs": "https://arxiv.org/abs/2510.06261", "authors": ["Zhanke Zhou", "Chentao Cao", "Xiao Feng", "Xuan Li", "Zongze Li", "Xiangyu Lu", "Jiangchao Yao", "Weikai Huang", "Linrui Xu", "Tian Cheng", "Guanyu Jiang", "Yiming Zheng", "Brando Miranda", "Tongliang Liu", "Sanmi Koyejo", "Masashi Sugiyama", "Bo Han"], "title": "AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Ongoing project", "summary": "We present AlphaApollo, a self-evolving agentic reasoning system that aims to\naddress two bottlenecks in foundation model (FM) reasoning-limited\nmodel-intrinsic capacity and unreliable test-time iteration. AlphaApollo\norchestrates multiple models with professional tools to enable deliberate,\nverifiable reasoning. It couples (i) a computation tool (Python with numerical\nand symbolic libraries) and (ii) a retrieval tool (task-relevant external\ninformation) to execute exact calculations and ground decisions. The system\nfurther supports multi-round, multi-model solution evolution via a shared state\nmap that records candidates, executable checks, and feedback for iterative\nrefinement. In evaluations on AIME 2024/2025 across multiple models,\nAlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32\nfor Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for\nLlama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool\ncalls are successfully executed, with consistent outperformance of non-tool\nbaselines, thereby lifting the capability ceiling of FMs. More empirical\nresults and implementation details will be updated at\nhttps://github.com/tmlr-group/AlphaApollo.", "AI": {"tldr": "AlphaApollo\u662f\u4e00\u4e2a\u81ea\u6f14\u8fdb\u7684\u667a\u80fd\u4f53\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6a21\u578b\u4e0e\u4e13\u4e1a\u5de5\u5177\uff08\u8ba1\u7b97\u548c\u68c0\u7d22\uff09\u5e76\u652f\u6301\u591a\u8f6e\u8fed\u4ee3\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u5176\u5185\u5728\u80fd\u529b\u9650\u5236\u548c\u8fed\u4ee3\u4e0d\u53ef\u9760\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u63a8\u7406\u4e2d\u7684\u4e24\u4e2a\u4e3b\u8981\u74f6\u9888\uff1a\u6709\u9650\u7684\u6a21\u578b\u5185\u5728\u80fd\u529b\u548c\u4e0d\u53ef\u9760\u7684\u6d4b\u8bd5\u65f6\u8fed\u4ee3\u3002", "method": "\u63d0\u51faAlphaApollo\uff0c\u4e00\u4e2a\u81ea\u6f14\u8fdb\u7684\u667a\u80fd\u4f53\u63a8\u7406\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5b9e\u73b0\uff1a\n1. \u534f\u8c03\u591a\u4e2a\u6a21\u578b\u4e0e\u4e13\u4e1a\u5de5\u5177\u8fdb\u884c\u534f\u4f5c\uff0c\u4ee5\u5b9e\u73b0\u6df1\u601d\u719f\u8651\u4e14\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u3002\n2. \u6574\u5408\u8ba1\u7b97\u5de5\u5177\uff08Python\u53ca\u6570\u503c/\u7b26\u53f7\u5e93\uff09\u548c\u68c0\u7d22\u5de5\u5177\uff08\u4efb\u52a1\u76f8\u5173\u5916\u90e8\u4fe1\u606f\uff09\uff0c\u7528\u4e8e\u7cbe\u786e\u8ba1\u7b97\u548c\u51b3\u7b56\u4f9d\u636e\u3002\n3. \u5229\u7528\u5171\u4eab\u72b6\u6001\u56fe\u652f\u6301\u591a\u8f6e\u3001\u591a\u6a21\u578b\u7684\u89e3\u51b3\u65b9\u6848\u6f14\u8fdb\uff0c\u8bb0\u5f55\u5019\u9009\u65b9\u6848\u3001\u53ef\u6267\u884c\u68c0\u67e5\u548c\u53cd\u9988\u4ee5\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "1. \u5728AIME 2024/2025\u7684\u8bc4\u4f30\u4e2d\uff0cAlphaApollo\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\uff1a\n   - \u5bf9\u4e8eQwen2.5-14B-Instruct\uff0cAverage@32\u63d0\u53475.15%\uff0cPass@32\u63d0\u534723.34%\u3002\n   - \u5bf9\u4e8eLlama-3.3-70B-Instruct\uff0cAverage@32\u63d0\u53478.91%\uff0cPass@32\u63d0\u534726.67%\u3002\n2. \u5de5\u5177\u4f7f\u7528\u5206\u6790\u663e\u793a\uff0c\u8d85\u8fc780%\u7684\u5de5\u5177\u8c03\u7528\u6210\u529f\u6267\u884c\uff0c\u4e14\u6301\u7eed\u4f18\u4e8e\u975e\u5de5\u5177\u57fa\u7ebf\u3002", "conclusion": "AlphaApollo\u901a\u8fc7\u6709\u6548\u5730\u5229\u7528\u5de5\u5177\uff0c\u6210\u529f\u514b\u670d\u4e86\u57fa\u7840\u6a21\u578b\u7684\u5185\u5728\u80fd\u529b\u548c\u8fed\u4ee3\u53ef\u9760\u6027\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u63a8\u7406\u80fd\u529b\u4e0a\u9650\u3002"}}
{"id": "2510.06229", "pdf": "https://arxiv.org/pdf/2510.06229", "abs": "https://arxiv.org/abs/2510.06229", "authors": ["Josh Hunter", "John McDermid", "Simon Burton", "Poppy Fynes", "Mia Dempster"], "title": "Milestone Determination for Autonomous Railway Operation", "categories": ["cs.CV", "cs.LG"], "comment": "Paper submitted and partially accepted to ICART 2025, paper is 8\n  pages and has 1 figure, 2 tables", "summary": "In the field of railway automation, one of the key challenges has been the\ndevelopment of effective computer vision systems due to the limited\navailability of high-quality, sequential data. Traditional datasets are\nrestricted in scope, lacking the spatio temporal context necessary for\nreal-time decision-making, while alternative solutions introduce issues related\nto realism and applicability. By focusing on route-specific, contextually\nrelevant cues, we can generate rich, sequential datasets that align more\nclosely with real-world operational logic. The concept of milestone\ndetermination allows for the development of targeted, rule-based models that\nsimplify the learning process by eliminating the need for generalized\nrecognition of dynamic components, focusing instead on the critical decision\npoints along a route. We argue that this approach provides a practical\nframework for training vision agents in controlled, predictable environments,\nfacilitating safer and more efficient machine learning systems for railway\nautomation.", "AI": {"tldr": "\u9488\u5bf9\u94c1\u8def\u81ea\u52a8\u5316\u89c6\u89c9\u7cfb\u7edf\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u91cc\u7a0b\u7891\u786e\u5b9a\u548c\u8def\u7ebf\u7279\u5b9a\u7ebf\u7d22\u7684\u65b9\u6cd5\uff0c\u7b80\u5316\u6a21\u578b\u8bad\u7ec3\u5e76\u63d0\u5347\u7cfb\u7edf\u5b89\u5168\u6027\u4e0e\u6548\u7387\u3002", "motivation": "\u94c1\u8def\u81ea\u52a8\u5316\u9886\u57df\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u9762\u4e34\u9ad8\u8d28\u91cf\u65f6\u5e8f\u6570\u636e\u532e\u4e4f\u7684\u6311\u6218\uff0c\u73b0\u6709\u6570\u636e\u96c6\u4e0d\u5177\u5907\u5b9e\u65f6\u51b3\u7b56\u6240\u9700\u7684\u65f6\u7a7a\u4e0a\u4e0b\u6587\uff0c\u66ff\u4ee3\u65b9\u6848\u7f3a\u4e4f\u771f\u5b9e\u6027\u4e0e\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u5173\u6ce8\u8def\u7ebf\u7279\u5b9a\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7ebf\u7d22\u751f\u6210\u4e30\u5bcc\u65f6\u5e8f\u6570\u636e\u96c6\u3002\u5229\u7528\u201c\u91cc\u7a0b\u7891\u786e\u5b9a\u201d\u6982\u5ff5\uff0c\u5f00\u53d1\u9488\u5bf9\u6027\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u6a21\u578b\uff0c\u805a\u7126\u5173\u952e\u51b3\u7b56\u70b9\u800c\u975e\u6cdb\u5316\u8bc6\u522b\u52a8\u6001\u7ec4\u4ef6\uff0c\u4ece\u800c\u7b80\u5316\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u53d7\u63a7\u3001\u53ef\u9884\u6d4b\u73af\u5883\u4e2d\u8bad\u7ec3\u94c1\u8def\u81ea\u52a8\u5316\u89c6\u89c9\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u6846\u67b6\u3002", "conclusion": "\u6b64\u65b9\u6cd5\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u5b89\u5168\u3001\u66f4\u9ad8\u6548\u7684\u94c1\u8def\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u3002"}}
{"id": "2510.06267", "pdf": "https://arxiv.org/pdf/2510.06267", "abs": "https://arxiv.org/abs/2510.06267", "authors": ["Khartik Uppalapati", "Shakeel Abdulkareem", "Bora Yimenicioglu"], "title": "RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases", "categories": ["cs.LG", "cs.AI", "I.2.6; H.2.8; J.3"], "comment": "6 pages, 2 figures, 2 tables. Submitted to IEEE International\n  Conference on Data Science and Advanced Analytics (DSAA)", "summary": "We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion\nframework that generates realistic yet privacy-preserving synthetic\nelectronic-health-record (EHR) trajectories for ultra-rare diseases.\nRareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human\nPhenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA\nAdverse Event Reporting System (FAERS) into a heterogeneous knowledge graph\ncomprising approximately 8 M typed edges. Meta-path scores extracted from this\n8-million-edge KG modulate the per-token noise schedule in the forward\nstochastic differential equation, steering generation toward biologically\nplausible lab-medication-adverse-event co-occurrences while retaining\nscore-based diffusion model stability. The reverse denoiser then produces\ntimestamped sequences of lab-code, medication-code, and adverse-event-flag\ntriples that contain no protected health information. On simulated\nultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean\nDiscrepancy by 40 percent relative to an unguided diffusion baseline and by\ngreater than 60 percent versus GAN counterparts, without sacrificing downstream\npredictive utility. A black-box membership-inference evaluation using the\nDOMIAS attacker yields AUROC approximately 0.53, well below the 0.55\nsafe-release threshold and substantially better than the approximately 0.61\nplus or minus 0.03 observed for non-KG baselines, demonstrating strong\nresistance to re-identification. These results suggest that integrating\nbiomedical knowledge graphs directly into diffusion noise schedules can\nsimultaneously enhance fidelity and privacy, enabling safer data sharing for\nrare-disease research.", "AI": {"tldr": "\u63d0\u51faRareGraph-Synth\uff0c\u4e00\u4e2a\u77e5\u8bc6\u5f15\u5bfc\u7684\u8fde\u7eed\u65f6\u95f4\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9488\u5bf9\u8d85\u7f55\u89c1\u75be\u75c5\u7684\u771f\u5b9e\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u5408\u6210\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u3002", "motivation": "\u4e3a\u8d85\u7f55\u89c1\u75be\u75c5\u7814\u7a76\u63d0\u4f9b\u771f\u5b9e\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u5408\u6210\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\uff0c\u4ee5\u5e94\u5bf9\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u6311\u6218\u3002", "method": "RareGraph-Synth\u901a\u8fc7\u6574\u5408Orphanet/Orphadata\u3001HPO\u3001GARD\u3001PrimeKG\u548cFAERS\u7b49\u4e94\u4e2a\u516c\u5171\u8d44\u6e90\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7ea6800\u4e07\u6761\u8fb9\u7684\u5f02\u6784\u77e5\u8bc6\u56fe\u8c31\u3002\u8be5\u6846\u67b6\u5229\u7528\u4ece\u77e5\u8bc6\u56fe\u8c31\u4e2d\u63d0\u53d6\u7684\u5143\u8def\u5f84\u5206\u6570\u6765\u8c03\u8282\u524d\u5411\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u4e2d\u7684\u6bcf\u4ee4\u724c\u566a\u97f3\u8c03\u5ea6\uff0c\u5f15\u5bfc\u751f\u6210\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u5b9e\u9a8c\u5ba4-\u836f\u7269-\u4e0d\u826f\u4e8b\u4ef6\u5171\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u6269\u6563\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u3002\u53cd\u5411\u53bb\u566a\u5668\u751f\u6210\u65e0\u53d7\u4fdd\u62a4\u5065\u5eb7\u4fe1\u606f\u7684\u3001\u5e26\u6709\u65f6\u95f4\u6233\u7684\u5b9e\u9a8c\u5ba4\u4ee3\u7801\u3001\u836f\u7269\u4ee3\u7801\u548c\u4e0d\u826f\u4e8b\u4ef6\u6807\u5fd7\u4e09\u5143\u7ec4\u5e8f\u5217\u3002", "result": "\u5728\u6a21\u62df\u8d85\u7f55\u89c1\u75be\u75c5\u961f\u5217\u4e0a\uff0cRareGraph-Synth\u7684\u5206\u7c7b\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u6bd4\u65e0\u5f15\u5bfc\u6269\u6563\u57fa\u7ebf\u964d\u4f4e40%\uff0c\u6bd4GANs\u964d\u4f4e\u8d85\u8fc760%\uff0c\u4e14\u4e0d\u727a\u7272\u4e0b\u6e38\u9884\u6d4b\u6548\u7528\u3002\u901a\u8fc7DOMIAS\u653b\u51fb\u8005\u7684\u9ed1\u76d2\u6210\u5458\u63a8\u65ad\u8bc4\u4f30\uff0cAUROC\u7ea6\u4e3a0.53\uff0c\u8fdc\u4f4e\u4e8e0.55\u7684\u5b89\u5168\u53d1\u5e03\u9608\u503c\uff0c\u663e\u8457\u4f18\u4e8e\u975e\u77e5\u8bc6\u56fe\u8c31\u57fa\u7ebf\u7684\u7ea60.61\u00b10.03\uff0c\u8868\u660e\u5176\u5f3a\u5927\u7684\u518d\u8bc6\u522b\u62b5\u6297\u80fd\u529b\u3002", "conclusion": "\u5c06\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u76f4\u63a5\u6574\u5408\u5230\u6269\u6563\u6a21\u578b\u7684\u566a\u97f3\u8c03\u5ea6\u4e2d\uff0c\u80fd\u591f\u540c\u65f6\u589e\u5f3a\u5408\u6210\u6570\u636e\u7684\u771f\u5b9e\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\uff0c\u4ece\u800c\u4e3a\u7f55\u89c1\u75be\u75c5\u7814\u7a76\u5b9e\u73b0\u66f4\u5b89\u5168\u7684\u6570\u636e\u5171\u4eab\u3002"}}
{"id": "2510.06737", "pdf": "https://arxiv.org/pdf/2510.06737", "abs": "https://arxiv.org/abs/2510.06737", "authors": ["Iftach Yakar", "Michael Ben-Or"], "title": "Advantages of Global Entanglement-Distillation Policies in Quantum Repeater Chains", "categories": ["quant-ph", "cs.NI"], "comment": null, "summary": "Quantum repeaters are essential for achieving long-distance quantum\ncommunication due to photon loss, which grows exponentially with the channel\ndistance. Current quantum repeater generations use entanglement distillation\nprotocols, where the decision of when to perform distillation depends on either\nlocal or global knowledge. Recent approaches for quantum repeaters, such as\nMantri et al. (arXiv:2409.06152), consider using deterministic local decision\npolicies for entanglement distillation. We ask whether global deterministic\npolicies outperform local ones in terms of communication rate. We simulate\nequidistant repeater chains, assisted by two-way classical communication, and\ncompare local and global policies for distillation decisions, spanning large\ndistances and varying network and hardware parameters. Our findings show that\nglobal deterministic policies consistently outperform these local ones, and in\nsome cases, determine whether secret communication is possible. For large\nrepeater chains ($N>512$), global policies improve SKR by two orders of\nmagnitude. These results suggest that local distillation decisions in quantum\nrepeater chains may not be optimal, and may inform future protocol design.", "AI": {"tldr": "\u5728\u91cf\u5b50\u4e2d\u7ee7\u5668\u4e2d\uff0c\u5168\u5c40\u786e\u5b9a\u6027\u7ea0\u7f20\u84b8\u998f\u7b56\u7565\u5728\u901a\u4fe1\u901f\u7387\u4e0a\u4f18\u4e8e\u5c40\u90e8\u7b56\u7565\uff0c\u5c24\u5176\u5728\u957f\u8ddd\u79bb\u94fe\u4e2d\u80fd\u663e\u8457\u63d0\u9ad8\u5bc6\u94a5\u751f\u6210\u7387\u3002", "motivation": "\u91cf\u5b50\u4e2d\u7ee7\u5668\u5bf9\u4e8e\u5b9e\u73b0\u957f\u8ddd\u79bb\u91cf\u5b50\u901a\u4fe1\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5149\u5b50\u635f\u8017\u968f\u8ddd\u79bb\u5448\u6307\u6570\u589e\u957f\u3002\u5f53\u524d\u4e2d\u7ee7\u5668\u4f7f\u7528\u7ea0\u7f20\u84b8\u998f\u534f\u8bae\uff0c\u5176\u51b3\u7b56\u4f9d\u8d56\u4e8e\u5c40\u90e8\u6216\u5168\u5c40\u4fe1\u606f\u3002\u9274\u4e8e\u8fd1\u671f\u7814\u7a76\u503e\u5411\u4e8e\u4f7f\u7528\u5c40\u90e8\u786e\u5b9a\u6027\u51b3\u7b56\u7b56\u7565\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5168\u5c40\u786e\u5b9a\u6027\u7b56\u7565\u662f\u5426\u80fd\u5728\u901a\u4fe1\u901f\u7387\u65b9\u9762\u8d85\u8d8a\u5c40\u90e8\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u7b49\u8ddd\u4e2d\u7ee7\u5668\u94fe\uff0c\u5e76\u5728\u53cc\u5411\u7ecf\u5178\u901a\u4fe1\u7684\u8f85\u52a9\u4e0b\uff0c\u6bd4\u8f83\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u7ea0\u7f20\u84b8\u998f\u51b3\u7b56\u7b56\u7565\u3002\u6a21\u62df\u6db5\u76d6\u4e86\u5e7f\u6cdb\u7684\u8ddd\u79bb\u4ee5\u53ca\u53d8\u5316\u7684\u7f51\u7edc\u548c\u786c\u4ef6\u53c2\u6570\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5168\u5c40\u786e\u5b9a\u6027\u7b56\u7565\u59cb\u7ec8\u4f18\u4e8e\u5c40\u90e8\u7b56\u7565\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u51b3\u5b9a\u4e86\u79d8\u5bc6\u901a\u4fe1\u662f\u5426\u53ef\u884c\u3002\u5bf9\u4e8e\u5927\u578b\u4e2d\u7ee7\u5668\u94fe\uff08N>512\uff09\uff0c\u5168\u5c40\u7b56\u7565\u80fd\u5c06\u79d8\u5bc6\u5bc6\u94a5\u751f\u6210\u7387\uff08SKR\uff09\u63d0\u9ad8\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u91cf\u5b50\u4e2d\u7ee7\u5668\u94fe\u4e2d\u7684\u5c40\u90e8\u84b8\u998f\u51b3\u7b56\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u5e76\u5e94\u4e3a\u672a\u6765\u7684\u534f\u8bae\u8bbe\u8ba1\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2510.06240", "pdf": "https://arxiv.org/pdf/2510.06240", "abs": "https://arxiv.org/abs/2510.06240", "authors": ["Jiqun Pan", "Zhenke Duan", "Jiani Tu", "Anzhi Cheng", "Yanqing Wang"], "title": "Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "41 pages, 12 figures, 6 tables", "summary": "Industrial question-answering (QA) systems require higher safety and\nreliability than general-purpose dialogue models, as errors in high-risk\nscenarios such as equipment fault diagnosis can have severe consequences.\nAlthough multi-agent large language models enhance reasoning depth, they suffer\nfrom uncontrolled iterations and unverifiable outputs, and conventional\ndistillation methods struggle to transfer collaborative reasoning capabilities\nto lightweight, deployable student models. To address these challenges, we\npropose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our\napproach formulates distillation as a Markov Decision Process and incorporates\na knowledge graph as a verifiable structured prior to enrich state\nrepresentation and ensure convergence. By integrating collaborative reasoning\nwith knowledge grounding, KG-MASD generates high-confidence instruction-tuning\ndata and jointly distills reasoning depth and verifiability into compact\nstudent models suitable for edge deployment. Experiments on an industrial QA\ndataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent\nover baselines and significantly enhances reliability, enabling trustworthy AI\ndeployment in safety-critical industrial scenarios. Code and data are available\nat https://github.com/erwinmsmith/KG-MAD/.", "AI": {"tldr": "\u9488\u5bf9\u5de5\u4e1aQA\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u6311\u6218\uff0c\u63d0\u51faKG-MASD\u65b9\u6cd5\uff0c\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u591a\u667a\u80fd\u4f53\u84b8\u998f\uff0c\u63d0\u5347\u6a21\u578b\u7cbe\u5ea6\u548c\u53ef\u4fe1\u8d56\u6027\uff0c\u5b9e\u73b0\u7d27\u51d1\u6a21\u578b\u90e8\u7f72\u3002", "motivation": "\u5de5\u4e1aQA\u7cfb\u7edf\uff08\u5982\u8bbe\u5907\u6545\u969c\u8bca\u65ad\uff09\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u5bf9\u53ef\u9760\u6027\u8981\u6c42\u9ad8\uff0c\u73b0\u6709\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u5b58\u5728\u8fed\u4ee3\u5931\u63a7\u548c\u8f93\u51fa\u4e0d\u53ef\u9a8c\u8bc1\u95ee\u9898\uff1b\u4f20\u7edf\u84b8\u998f\u65b9\u6cd5\u96be\u4ee5\u5c06\u534f\u4f5c\u63a8\u7406\u80fd\u529b\u6709\u6548\u8fc1\u79fb\u5230\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u77e5\u8bc6\u56fe\u8c31\u5f15\u5bfc\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u84b8\u998f\uff08KG-MASD\uff09\u3002\u5c06\u84b8\u998f\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u53ef\u9a8c\u8bc1\u7684\u7ed3\u6784\u5316\u5148\u9a8c\uff0c\u4e30\u5bcc\u72b6\u6001\u8868\u793a\u5e76\u786e\u4fdd\u6536\u655b\u3002\u901a\u8fc7\u6574\u5408\u534f\u4f5c\u63a8\u7406\u548c\u77e5\u8bc6 grounding\uff0c\u751f\u6210\u9ad8\u7f6e\u4fe1\u5ea6\u6307\u4ee4\u5fae\u8c03\u6570\u636e\uff0c\u5c06\u63a8\u7406\u6df1\u5ea6\u548c\u53ef\u9a8c\u8bc1\u6027\u84b8\u998f\u5230\u7d27\u51d1\u7684\u5b66\u751f\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u5de5\u4e1aQA\u6570\u636e\u96c6\u4e0a\uff0cKG-MASD\u7684\u51c6\u786e\u7387\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u63d0\u9ad8\u4e862.4%\u81f320.1%\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u53ef\u9760\u6027\u3002", "conclusion": "KG-MASD\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1aQA\u7cfb\u7edf\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u53ef\u4fe1\u8d56AI\u7684\u90e8\u7f72\uff0c\u63d0\u9ad8\u4e86\u5de5\u4e1aQA\u7684\u51c6\u786e\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u3002"}}
{"id": "2510.06274", "pdf": "https://arxiv.org/pdf/2510.06274", "abs": "https://arxiv.org/abs/2510.06274", "authors": ["Mohammad Mahdi Samiei Paqaleh", "Arash Marioriyad", "Arman Tahmasebi-Zadeh", "Mohamadreza Fereydooni", "Mahdi Ghaznavai", "Mahdieh Soleymani Baghshah"], "title": "Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Recent progress has pushed AI frontiers from pattern recognition tasks toward\nproblems that require step by step, System2 style reasoning, especially with\nlarge language models. Yet, unlike learning, where generalization and out of\ndistribution (OoD) evaluation concepts are well formalized, there is no clear,\nconsistent definition or metric for reasoning ability. We propose Complexity\nOut of Distribution (Complexity OoD) generalization as a framework and problem\nsetting to define and measure reasoning. A model exhibits Complexity OoD\ngeneralization when it maintains performance on test instances whose minimal\nrequired solution complexity, either representational (richer solution\nstructure) or computational (more reasoning steps/program length), exceeds that\nof all training examples. We formalize complexity via solution description\nKolmogorov complexity and operational proxies (e.g., object/relation counts;\nreasoning step counts), clarifying how Complexity OoD differs from length and\ncompositional OoD. This lens unifies learning and reasoning: many cases\nsolvable with System1 like processing at low complexity become System2 like\nunder complexity pressure, while System2 can be viewed as generalization over\nsolution structures. We translate this perspective into practice with\nrecommendations for operationalizing Complexity OoD across the stack:\nincorporating complexity into benchmark and evaluation metric design,\nrethinking supervision to target solution traces, seeking and designing\ninductive biases for Complexity OoD generalization, addressing learning to\nreason spillovers such as spurious shortcuts, semantic robustness, catastrophic\nforgetting, and step wise calibration. Because Complexity OoD cannot be solved\nby scaling data alone, progress toward robust reasoning will require\narchitectures and training regimes that explicitly model and allocate\ncomputation with respect to complexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u201c\u590d\u6742\u5ea6\u5206\u5e03\u5916\u6cdb\u5316\uff08Complexity OoD\uff09\u201d\u6846\u67b6\uff0c\u65e8\u5728\u5b9a\u4e49\u548c\u8861\u91cfAI\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u9762\u5bf9\u9700\u8981\u6bd4\u8bad\u7ec3\u6570\u636e\u66f4\u9ad8\u89e3\u51b3\u590d\u6742\u5ea6\u7684\u4efb\u52a1\u65f6\u3002", "motivation": "\u968f\u7740AI\u4ece\u6a21\u5f0f\u8bc6\u522b\u8f6c\u5411\u9700\u8981System2\u5f0f\u63a8\u7406\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u76ee\u524d\u5bf9\u63a8\u7406\u80fd\u529b\u7684\u6cdb\u5316\u548c\u5206\u5e03\u5916\u8bc4\u4f30\u7f3a\u4e4f\u6e05\u6670\u4e00\u81f4\u7684\u5b9a\u4e49\u548c\u8861\u91cf\u6807\u51c6\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51faComplexity OoD\u6cdb\u5316\u6846\u67b6\uff0c\u5c06\u5176\u5b9a\u4e49\u4e3a\u6a21\u578b\u5728\u6d4b\u8bd5\u5b9e\u4f8b\u7684\u6700\u5c0f\u6240\u9700\u89e3\u51b3\u65b9\u6848\u590d\u6742\u5ea6\uff08\u8868\u793a\u6216\u8ba1\u7b97\uff09\u8d85\u51fa\u6240\u6709\u8bad\u7ec3\u6837\u672c\u65f6\uff0c\u4ecd\u80fd\u4fdd\u6301\u6027\u80fd\u3002\u901a\u8fc7\u67ef\u5c14\u83ab\u54e5\u6d1b\u592b\u590d\u6742\u5ea6\u548c\u64cd\u4f5c\u4ee3\u7406\uff08\u5982\u5bf9\u8c61/\u5173\u7cfb\u8ba1\u6570\u3001\u63a8\u7406\u6b65\u6570\uff09\u6765\u5f62\u5f0f\u5316\u590d\u6742\u5ea6\uff0c\u5e76\u9610\u660e\u5176\u4e0e\u957f\u5ea6\u548c\u7ec4\u5408\u6027OoD\u7684\u533a\u522b\u3002\u8be5\u6846\u67b6\u7edf\u4e00\u4e86\u5b66\u4e60\u548c\u63a8\u7406\uff0c\u5e76\u5c06System2\u89c6\u4e3a\u5bf9\u89e3\u51b3\u65b9\u6848\u7ed3\u6784\u7684\u6cdb\u5316\u3002", "result": "\u8be5\u6846\u67b6\u4e3a\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u5b9a\u4e49\u548c\u8861\u91cf\u6807\u51c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u5728\u57fa\u51c6\u8bbe\u8ba1\u3001\u76d1\u7763\u65b9\u5f0f\u3001\u5f52\u7eb3\u504f\u7f6e\u7b49\u65b9\u9762\u64cd\u4f5c\u5316Complexity OoD\u7684\u5b9e\u8df5\u5efa\u8bae\u3002\u7814\u7a76\u6307\u51fa\uff0c\u5355\u9760\u6570\u636e\u6269\u5c55\u65e0\u6cd5\u89e3\u51b3Complexity OoD\u95ee\u9898\u3002", "conclusion": "\u4e3a\u4e86\u5b9e\u73b0\u9c81\u68d2\u63a8\u7406\uff0cAI\u9700\u8981\u660e\u786e\u5efa\u6a21\u5e76\u6839\u636e\u590d\u6742\u5ea6\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u673a\u5236\uff0c\u800c\u975e\u4ec5\u4ec5\u4f9d\u9760\u6269\u5927\u6570\u636e\u89c4\u6a21\u3002"}}
{"id": "2510.06231", "pdf": "https://arxiv.org/pdf/2510.06231", "abs": "https://arxiv.org/abs/2510.06231", "authors": ["Mingzhe Zheng", "Dingjie Song", "Guanyu Zhou", "Jun You", "Jiahao Zhan", "Xuran Ma", "Xinyuan Song", "Ser-Nam Lim", "Qifeng Chen", "Harry Yang"], "title": "CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation", "categories": ["cs.CV", "cs.CL"], "comment": "24 pages, 9 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating highly structured texts. However, while exhibiting a high degree of\nstructural organization, movie scripts demand an additional layer of nuanced\nstorytelling and emotional depth-the 'soul' of compelling cinema-that LLMs\noften fail to capture. To investigate this deficiency, we first curated\nCML-Dataset, a dataset comprising (summary, content) pairs for Cinematic Markup\nLanguage (CML), where 'content' consists of segments from esteemed,\nhigh-quality movie scripts and 'summary' is a concise description of the\ncontent. Through an in-depth analysis of the intrinsic multi-shot continuity\nand narrative structures within these authentic scripts, we identified three\npivotal dimensions for quality assessment: Dialogue Coherence (DC), Character\nConsistency (CC), and Plot Reasonableness (PR). Informed by these findings, we\npropose the CML-Bench, featuring quantitative metrics across these dimensions.\nCML-Bench effectively assigns high scores to well-crafted, human-written\nscripts while concurrently pinpointing the weaknesses in screenplays generated\nby LLMs. To further validate our benchmark, we introduce CML-Instruction, a\nprompting strategy with detailed instructions on character dialogue and event\nlogic, to guide LLMs to generate more structured and cinematically sound\nscripts. Extensive experiments validate the effectiveness of our benchmark and\ndemonstrate that LLMs guided by CML-Instruction generate higher-quality\nscreenplays, with results aligned with human preferences.", "AI": {"tldr": "LLMs\u5728\u751f\u6210\u7535\u5f71\u5267\u672c\u65f6\u7f3a\u4e4f\u201c\u7075\u9b42\u201d\uff0c\u672c\u6587\u901a\u8fc7\u6784\u5efaCML-Dataset\u3001\u63d0\u51fa\u57fa\u4e8e\u5bf9\u8bdd\u8fde\u8d2f\u6027\u3001\u89d2\u8272\u4e00\u81f4\u6027\u548c\u60c5\u8282\u5408\u7406\u6027\u7684CML-Bench\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u5f15\u5165CML-Instruction\u63d0\u793a\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u751f\u6210\u5267\u672c\u7684\u8d28\u91cf\uff0c\u4f7f\u5176\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u7ed3\u6784\u5316\u6587\u672c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7535\u5f71\u5267\u672c\u521b\u4f5c\u4e2d\uff0c\u9664\u4e86\u7ed3\u6784\u7ec4\u7ec7\u5916\uff0c\u8fd8\u9700\u6355\u6349\u7ec6\u81f4\u5165\u5fae\u7684\u53d9\u4e8b\u548c\u60c5\u611f\u6df1\u5ea6\uff08\u5373\u201c\u7535\u5f71\u7684\u7075\u9b42\u201d\uff09\uff0c\u800c\u8fd9\u6b63\u662fLLMs\u76ee\u524d\u672a\u80fd\u6709\u6548\u5b9e\u73b0\u7684\u90e8\u5206\u3002", "method": ["\u6784\u5efa\u4e86CML-Dataset\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u9ad8\u8d28\u91cf\u7535\u5f71\u5267\u672c\u7247\u6bb5\uff08\u5185\u5bb9\uff09\u53ca\u5176\u7b80\u6d01\u6458\u8981\u7684\uff08\u6458\u8981\uff0c\u5185\u5bb9\uff09\u5bf9\u6570\u636e\u96c6\u3002", "\u901a\u8fc7\u6df1\u5165\u5206\u6790\u771f\u5b9e\u5267\u672c\u7684\u591a\u955c\u5934\u8fde\u7eed\u6027\u548c\u53d9\u4e8b\u7ed3\u6784\uff0c\u786e\u5b9a\u4e86\u8bc4\u4f30\u5267\u672c\u8d28\u91cf\u7684\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a\u5bf9\u8bdd\u8fde\u8d2f\u6027\uff08DC\uff09\u3001\u89d2\u8272\u4e00\u81f4\u6027\uff08CC\uff09\u548c\u60c5\u8282\u5408\u7406\u6027\uff08PR\uff09\u3002", "\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86CML-Bench\uff0c\u4e00\u4e2a\u5305\u542b\u4e0a\u8ff0\u7ef4\u5ea6\u91cf\u5316\u6307\u6807\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u4eba\u7c7b\u521b\u4f5c\u7684\u5267\u672c\u5e76\u6307\u51faLLM\u751f\u6210\u5267\u672c\u7684\u5f31\u70b9\u3002", "\u5f15\u5165\u4e86CML-Instruction\uff0c\u4e00\u79cd\u5305\u542b\u8be6\u7ec6\u89d2\u8272\u5bf9\u8bdd\u548c\u4e8b\u4ef6\u903b\u8f91\u6307\u4ee4\u7684\u63d0\u793a\u7b56\u7565\uff0c\u65e8\u5728\u6307\u5bfcLLMs\u751f\u6210\u66f4\u5177\u7ed3\u6784\u6027\u548c\u7535\u5f71\u611f\u7684\u5267\u672c\u3002"], "result": ["CML-Bench\u80fd\u591f\u6709\u6548\u4e3a\u7cbe\u5fc3\u7f16\u5199\u7684\u4eba\u7c7b\u5267\u672c\u6253\u51fa\u9ad8\u5206\uff0c\u5e76\u540c\u65f6\u7cbe\u51c6\u8bc6\u522bLLMs\u751f\u6210\u5267\u672c\u4e2d\u7684\u5f31\u70b9\u3002", "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CML-Bench\u8bc4\u4f30\u57fa\u51c6\u7684\u6709\u6548\u6027\u3002", "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728CML-Instruction\u6307\u5bfc\u4e0b\uff0cLLMs\u80fd\u591f\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u5267\u672c\uff0c\u4e14\u5176\u7ed3\u679c\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u3002"], "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u63d0\u51faCML-Bench\u8bc4\u4f30\u57fa\u51c6\u548cCML-Instruction\u63d0\u793a\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLMs\u5728\u751f\u6210\u7535\u5f71\u5267\u672c\u65f6\u7f3a\u4e4f\u53d9\u4e8b\u6df1\u5ea6\u548c\u60c5\u611f\u6355\u6349\u80fd\u529b\u7684\u4e0d\u8db3\uff0c\u4f7fLLMs\u80fd\u591f\u751f\u6210\u66f4\u5177\u7ed3\u6784\u5316\u3001\u66f4\u7b26\u5408\u7535\u5f71\u611f\u4e14\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u7684\u9ad8\u8d28\u91cf\u5267\u672c\u3002"}}
{"id": "2510.06270", "pdf": "https://arxiv.org/pdf/2510.06270", "abs": "https://arxiv.org/abs/2510.06270", "authors": ["Nian Ran", "Zhongzheng Li", "Yue Wang", "Qingsong Ran", "Xiaoyuan Zhang", "Shikun Feng", "Richard Allmendinger", "Xiaoguang Zhao"], "title": "MCCE: A Framework for Multi-LLM Collaborative Co-Evolution", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multi-objective discrete optimization problems, such as molecular design,\npose significant challenges due to their vast and unstructured combinatorial\nspaces. Traditional evolutionary algorithms often get trapped in local optima,\nwhile expert knowledge can provide crucial guidance for accelerating\nconvergence. Large language models (LLMs) offer powerful priors and reasoning\nability, making them natural optimizers when expert knowledge matters. However,\nclosed-source LLMs, though strong in exploration, cannot update their\nparameters and thus cannot internalize experience. Conversely, smaller open\nmodels can be continually fine-tuned but lack broad knowledge and reasoning\nstrength. We introduce Multi-LLM Collaborative Co-evolution (MCCE), a hybrid\nframework that unites a frozen closed-source LLM with a lightweight trainable\nmodel. The system maintains a trajectory memory of past search processes; the\nsmall model is progressively refined via reinforcement learning, with the two\nmodels jointly supporting and complementing each other in global exploration.\nUnlike model distillation, this process enhances the capabilities of both\nmodels through mutual inspiration. Experiments on multi-objective drug design\nbenchmarks show that MCCE achieves state-of-the-art Pareto front quality and\nconsistently outperforms baselines. These results highlight a new paradigm for\nenabling continual evolution in hybrid LLM systems, combining knowledge-driven\nexploration with experience-driven learning.", "AI": {"tldr": "MCCE\u662f\u4e00\u4e2a\u6df7\u5408LLM\u6846\u67b6\uff0c\u7ed3\u5408\u51bb\u7ed3\u95ed\u6e90LLM\u548c\u53ef\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u901a\u8fc7\u534f\u540c\u6f14\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u591a\u76ee\u6807\u79bb\u6563\u4f18\u5316\u95ee\u9898\uff0c\u5728\u836f\u7269\u8bbe\u8ba1\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u591a\u76ee\u6807\u79bb\u6563\u4f18\u5316\uff08\u5982\u5206\u5b50\u8bbe\u8ba1\uff09\u56e0\u5de8\u5927\u7ec4\u5408\u7a7a\u95f4\u800c\u5177\u6311\u6218\u3002\u4f20\u7edf\u8fdb\u5316\u7b97\u6cd5\u6613\u9677\u5c40\u90e8\u6700\u4f18\u3002LLM\u867d\u6709\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u95ed\u6e90LLM\u65e0\u6cd5\u5185\u5316\u7ecf\u9a8c\uff0c\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\u53c8\u7f3a\u4e4f\u5e7f\u535a\u77e5\u8bc6\uff0c\u9700\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u89e3\u51b3\u5f53\u524dLLM\u4f18\u5316\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faMulti-LLM Collaborative Co-evolution (MCCE)\u6846\u67b6\uff0c\u7ed3\u5408\u4e00\u4e2a\u51bb\u7ed3\u7684\u95ed\u6e90LLM\u548c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u6a21\u578b\u3002\u7cfb\u7edf\u7ef4\u62a4\u641c\u7d22\u8f68\u8ff9\u8bb0\u5fc6\uff0c\u5c0f\u578b\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9010\u6b65\u4f18\u5316\u3002\u4e24\u4e2a\u6a21\u578b\u5728\u5168\u5c40\u63a2\u7d22\u4e2d\u76f8\u4e92\u652f\u6301\u3001\u4e92\u8865\uff0c\u5e76\u901a\u8fc7\u76f8\u4e92\u542f\u53d1\u63d0\u5347\u80fd\u529b\u3002", "result": "\u5728\u591a\u76ee\u6807\u836f\u7269\u8bbe\u8ba1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMCCE\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u8d28\u91cf\uff0c\u5e76\u6301\u7eed\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u7ed3\u679c\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408LLM\u7cfb\u7edf\u5b9e\u73b0\u6301\u7eed\u8fdb\u5316\u7684\u65b0\u8303\u5f0f\uff0c\u6709\u6548\u7ed3\u5408\u4e86\u77e5\u8bc6\u9a71\u52a8\u7684\u63a2\u7d22\u548c\u7ecf\u9a8c\u9a71\u52a8\u7684\u5b66\u4e60\u3002"}}
{"id": "2510.06884", "pdf": "https://arxiv.org/pdf/2510.06884", "abs": "https://arxiv.org/abs/2510.06884", "authors": ["Rahul Gulia", "Amlan Ganguly", "Michael E. Kuhl", "Ehsan Rashedi", "Clark Hochgraf"], "title": "Memory-Augmented Generative AI for Real-time Wireless Prediction in Dynamic Industrial Environments", "categories": ["eess.SP", "cs.NI"], "comment": null, "summary": "Accurate and real-time prediction of wireless channel conditions,\nparticularly the Signal-to-Interference-plus-Noise Ratio (SINR), is a\nfoundational requirement for enabling Ultra-Reliable Low-Latency Communication\n(URLLC) in highly dynamic Industry 4.0 environments. Traditional physics-based\nor statistical models fail to cope with the spatio-temporal complexities\nintroduced by mobile obstacles and transient interference inherent to smart\nwarehouses. To address this, we introduce Evo-WISVA (Evolutionary Wireless\nInfrastructure for Smart Warehouse using VAE), a novel synergistic deep\nlearning architecture that functions as a lightweight 2D predictive digital\ntwin of the radio environment. Evo-WISVA integrates a memory-augmented\nVariational Autoencoder (VAE) featuring an Attention-driven Latent Memory\nModule (LMM) for robust, context-aware spatial feature extraction, with a\nConvolutional Long Short-Term Memory (ConvLSTM) network for precise temporal\nforecasting and sequential refinement. The entire pipeline is optimized\nend-to-end via a joint loss function, ensuring optimal feature alignment\nbetween the generative and predictive components. Rigorous experimental\nevaluation conducted on a high-fidelity ns-3-generated industrial warehouse\ndataset demonstrates that Evo-WISVA significantly surpasses state-of-the-art\nbaselines, achieving up to a 47.6\\% reduction in average reconstruction error.\nCrucially, the model exhibits exceptional generalization capacity to unseen\nenvironments with vastly increased dynamic complexity (up to ten simultaneously\nmoving obstacles) while maintaining amortized computational efficiency\nessential for real-time deployment. Evo-WISVA establishes a foundational\ntechnology for proactive wireless resource management, enabling autonomous\noptimization and advancing the realization of predictive digital twins in\nindustrial communication networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faEvo-WISVA\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u4f5c\u4e3a\u65e0\u7ebf\u7535\u73af\u5883\u7684\u9884\u6d4b\u6570\u5b57\u5b6a\u751f\uff0c\u65e8\u5728\u4e3a\u52a8\u6001\u5de5\u4e1a4.0\u73af\u5883\u63d0\u4f9b\u65e0\u7ebf\u4fe1\u9053\u6761\u4ef6\u7684\u5b9e\u65f6\u7cbe\u51c6\u9884\u6d4b\uff0c\u5176\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5728\u9ad8\u5ea6\u52a8\u6001\u7684\u5de5\u4e1a4.0\u73af\u5883\u4e2d\uff0c\u5b9e\u73b0\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1\uff08URLLC\uff09\u9700\u8981\u5bf9\u65e0\u7ebf\u4fe1\u9053\u6761\u4ef6\uff08\u7279\u522b\u662fSINR\uff09\u8fdb\u884c\u51c6\u786e\u3001\u5b9e\u65f6\u7684\u9884\u6d4b\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7269\u7406\u6216\u7edf\u8ba1\u6a21\u578b\u96be\u4ee5\u5e94\u5bf9\u667a\u80fd\u4ed3\u5e93\u4e2d\u79fb\u52a8\u969c\u788d\u7269\u548c\u77ac\u6001\u5e72\u6270\u5e26\u6765\u7684\u590d\u6742\u65f6\u7a7a\u53d8\u5316\u3002", "method": "\u5f15\u5165Evo-WISVA\uff0c\u4e00\u79cd\u534f\u540c\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u4f5c\u4e3a\u8f7b\u91cf\u7ea72D\u9884\u6d4b\u6570\u5b57\u5b6a\u751f\u3002\u5b83\u878d\u5408\u4e86\uff1a1) \u5e26\u6709\u6ce8\u610f\u529b\u9a71\u52a8\u6f5c\u5728\u8bb0\u5fc6\u6a21\u5757\uff08LMM\uff09\u7684\u8bb0\u5fc6\u589e\u5f3a\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\uff0c\u7528\u4e8e\u9c81\u68d2\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\uff1b2) \u5377\u79ef\u957f\u77ed\u671f\u8bb0\u5fc6\uff08ConvLSTM\uff09\u7f51\u7edc\uff0c\u7528\u4e8e\u7cbe\u786e\u7684\u65f6\u95f4\u9884\u6d4b\u548c\u5e8f\u5217\u7ec6\u5316\u3002\u6574\u4e2a\u6d41\u7a0b\u901a\u8fc7\u8054\u5408\u635f\u5931\u51fd\u6570\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "\u5728ns-3\u751f\u6210\u7684\u9ad8\u4fdd\u771f\u5de5\u4e1a\u4ed3\u5e93\u6570\u636e\u96c6\u4e0a\uff0cEvo-WISVA\u7684\u5e73\u5747\u91cd\u5efa\u8bef\u5dee\u964d\u4f4e\u9ad8\u8fbe47.6%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u3002\u8be5\u6a21\u578b\u5bf9\u5177\u6709\u6781\u9ad8\u52a8\u6001\u590d\u6742\u6027\uff08\u591a\u8fbe\u5341\u4e2a\u540c\u65f6\u79fb\u52a8\u969c\u788d\u7269\uff09\u7684\u672a\u77e5\u73af\u5883\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u90e8\u7f72\u6240\u9700\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "Evo-WISVA\u4e3a\u524d\u77bb\u6027\u65e0\u7ebf\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u4e00\u9879\u57fa\u7840\u6280\u672f\uff0c\u80fd\u591f\u5b9e\u73b0\u81ea\u4e3b\u4f18\u5316\uff0c\u5e76\u63a8\u52a8\u4e86\u5de5\u4e1a\u901a\u4fe1\u7f51\u7edc\u4e2d\u9884\u6d4b\u6570\u5b57\u5b6a\u751f\u7684\u5b9e\u73b0\u3002"}}
{"id": "2510.06242", "pdf": "https://arxiv.org/pdf/2510.06242", "abs": "https://arxiv.org/abs/2510.06242", "authors": ["Subin An", "Yugyeong Ji", "Junyoung Kim", "Heejin Kook", "Yang Lu", "Josh Seltzer"], "title": "Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP Industry Track", "summary": "Open-ended survey responses provide valuable insights in marketing research,\nbut low-quality responses not only burden researchers with manual filtering but\nalso risk leading to misleading conclusions, underscoring the need for\neffective evaluation. Existing automatic evaluation methods target\nLLM-generated text and inadequately assess human-written responses with their\ndistinct characteristics. To address such characteristics, we propose a\ntwo-stage evaluation framework specifically designed for human survey\nresponses. First, gibberish filtering removes nonsensical responses. Then,\nthree dimensions-effort, relevance, and completeness-are evaluated using LLM\ncapabilities, grounded in empirical analysis of real-world survey data.\nValidation on English and Korean datasets shows that our framework not only\noutperforms existing metrics but also demonstrates high practical applicability\nfor real-world applications such as response quality prediction and response\nrejection, showing strong correlations with expert assessment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528LLM\u80fd\u529b\u6709\u6548\u8bc4\u4f30\u4eba\u7c7b\u64b0\u5199\u7684\u5f00\u653e\u5f0f\u8c03\u67e5\u95ee\u5377\u7b54\u590d\u8d28\u91cf\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u5f00\u653e\u5f0f\u8c03\u67e5\u95ee\u5377\u7b54\u590d\u867d\u80fd\u63d0\u4f9b\u5b9d\u8d35\u89c1\u89e3\uff0c\u4f46\u4f4e\u8d28\u91cf\u56de\u590d\u4f1a\u589e\u52a0\u4eba\u5de5\u7b5b\u9009\u8d1f\u62c5\u5e76\u53ef\u80fd\u5bfc\u81f4\u8bef\u5bfc\u6027\u7ed3\u8bba\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002\u73b0\u6709\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9LLM\u751f\u6210\u6587\u672c\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u4eba\u7c7b\u64b0\u5199\u7684\u3001\u5177\u6709\u72ec\u7279\u7279\u5f81\u7684\u56de\u590d\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u4eba\u7c7b\u8c03\u67e5\u7b54\u590d\u7684\u4e24\u9636\u6bb5\u8bc4\u4f30\u6846\u67b6\uff1a\u9996\u5148\u8fdb\u884c\u4e71\u7801\u8fc7\u6ee4\u4ee5\u53bb\u9664\u65e0\u610f\u4e49\u56de\u590d\uff1b\u7136\u540e\uff0c\u57fa\u4e8e\u771f\u5b9e\u8c03\u67e5\u6570\u636e\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u5229\u7528LLM\u80fd\u529b\u4ece\u52aa\u529b\u7a0b\u5ea6\u3001\u76f8\u5173\u6027\u548c\u5b8c\u6574\u6027\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u56de\u590d\u8d28\u91cf\u3002", "result": "\u5728\u82f1\u8bed\u548c\u97e9\u8bed\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4e0d\u4ec5\u4f18\u4e8e\u73b0\u6709\u8bc4\u4f30\u6307\u6807\uff0c\u800c\u4e14\u5728\u56de\u590d\u8d28\u91cf\u9884\u6d4b\u548c\u56de\u590d\u62d2\u7edd\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u9ad8\u5b9e\u7528\u6027\uff0c\u5e76\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u7ed3\u679c\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "\u8be5\u4e24\u9636\u6bb5\u8bc4\u4f30\u6846\u67b6\u80fd\u591f\u6709\u6548\u4e14\u51c6\u786e\u5730\u8bc4\u4f30\u4eba\u7c7b\u64b0\u5199\u7684\u5f00\u653e\u5f0f\u8c03\u67e5\u95ee\u5377\u7b54\u590d\u7684\u8d28\u91cf\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5e02\u573a\u7814\u7a76\u4e2d\u7b54\u590d\u8d28\u91cf\u63a7\u5236\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2510.06288", "pdf": "https://arxiv.org/pdf/2510.06288", "abs": "https://arxiv.org/abs/2510.06288", "authors": ["Raj Ghugare", "Catherine Ji", "Kathryn Wantlin", "Jin Schofield", "Benjamin Eysenbach"], "title": "BuilderBench -- A benchmark for generalist agents", "categories": ["cs.AI", "cs.LG"], "comment": "Project page: https://rajghugare19.github.io/builderbench and Code:\n  https://github.com/rajghugare19/builderbench", "summary": "Today's AI models learn primarily through mimicry and sharpening, so it is\nnot surprising that they struggle to solve problems beyond the limits set by\nexisting data. To solve novel problems, agents should acquire skills for\nexploring and learning through experience. Finding a scalable learning\nmechanism for developing agents that learn through interaction remains a major\nopen problem. In this work, we introduce BuilderBench, a benchmark to\naccelerate research into agent pre-training that centers open-ended\nexploration. BuilderBench requires agents to learn how to build any structure\nusing blocks. BuilderBench is equipped with $(1)$ a hardware accelerated\nsimulator of a robotic agent interacting with various physical blocks, and\n$(2)$ a task-suite with over 42 diverse target structures that are carefully\ncurated to test an understanding of physics, mathematics, and long-horizon\nplanning. During training, agents have to explore and learn general principles\nabout the environment without any external supervision. During evaluation,\nagents have to build the unseen target structures from the task suite. Solving\nthese tasks requires a sort of \\emph{embodied reasoning} that is not reflected\nin words but rather in actions, experimenting with different strategies and\npiecing them together. Our experiments show that many of these tasks challenge\nthe current iteration of algorithms. Hence, we also provide a ``training\nwheels'' protocol, in which agents are trained and evaluated to build a single\ntarget structure from the task suite. Finally, we provide single-file\nimplementations of six different algorithms as a reference point for\nresearchers.", "AI": {"tldr": "\u5f15\u5165BuilderBench\uff0c\u4e00\u4e2a\u7528\u4e8e\u63a8\u52a8\u667a\u80fd\u4f53\u901a\u8fc7\u5f00\u653e\u5f0f\u63a2\u7d22\u5b66\u4e60\u6784\u5efa\u7ed3\u6784\u7684\u57fa\u51c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709AI\u96be\u4ee5\u5e94\u5bf9\u65b0\u95ee\u9898\u5e76\u4fc3\u8fdb\u5177\u8eab\u63a8\u7406\u7814\u7a76\uff0c\u73b0\u6709\u7b97\u6cd5\u5728\u6b64\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709AI\u6a21\u578b\u4e3b\u8981\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\uff0c\u96be\u4ee5\u89e3\u51b3\u73b0\u6709\u6570\u636e\u8303\u56f4\u4e4b\u5916\u7684\u65b0\u95ee\u9898\u3002\u9700\u8981\u5f00\u53d1\u80fd\u901a\u8fc7\u4ea4\u4e92\u63a2\u7d22\u548c\u7ecf\u9a8c\u5b66\u4e60\u7684\u667a\u80fd\u4f53\uff0c\u4f46\u7f3a\u4e4f\u53ef\u6269\u5c55\u7684\u5b66\u4e60\u673a\u5236\u3002", "method": "\u63d0\u51faBuilderBench\u57fa\u51c6\uff0c\u5305\u542b\u4e00\u4e2a\u786c\u4ef6\u52a0\u901f\u7684\u673a\u5668\u4eba\u6a21\u62df\u5668\u548c42\u4e2a\u6d4b\u8bd5\u7269\u7406\u3001\u6570\u5b66\u53ca\u957f\u7a0b\u89c4\u5212\u80fd\u529b\u7684\u6784\u5efa\u4efb\u52a1\u3002\u667a\u80fd\u4f53\u5728\u65e0\u76d1\u7763\u4e0b\u63a2\u7d22\u5e76\u5b66\u4e60\u73af\u5883\u539f\u7406\uff0c\u7136\u540e\u6784\u5efa\u672a\u89c1\u8fc7\u7684\u76ee\u6807\u7ed3\u6784\u3002\u540c\u65f6\u63d0\u4f9b\u201c\u8bad\u7ec3\u8f6e\u201d\u534f\u8bae\u548c\u516d\u79cd\u7b97\u6cd5\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBuilderBench\u4e2d\u7684\u8bb8\u591a\u4efb\u52a1\u5bf9\u5f53\u524d\u7b97\u6cd5\u6784\u6210\u6311\u6218\u3002", "conclusion": "BuilderBench\u4e3a\u7814\u7a76\u667a\u80fd\u4f53\u5f00\u653e\u5f0f\u63a2\u7d22\u548c\u5177\u8eab\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5e76\u63ed\u793a\u4e86\u73b0\u6709\u7b97\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002"}}
{"id": "2510.06233", "pdf": "https://arxiv.org/pdf/2510.06233", "abs": "https://arxiv.org/abs/2510.06233", "authors": ["Haoyang Zhang", "Zhou Yang", "Yucai Pang"], "title": "User to Video: A Model for Spammer Detection Inspired by Video Classification Technology", "categories": ["cs.CV"], "comment": "Accepted by International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "This article is inspired by video classification technology. If the user\nbehavior subspace is viewed as a frame image, consecutive frame images are\nviewed as a video. Following this novel idea, a model for spammer detection\nbased on user videoization, called UVSD, is proposed. Firstly, a user2piexl\nalgorithm for user pixelization is proposed. Considering the adversarial\nbehavior of user stances, the user is viewed as a pixel, and the stance is\nquantified as the pixel's RGB. Secondly, a behavior2image algorithm is proposed\nfor transforming user behavior subspace into frame images. Low-rank dense\nvectorization of subspace user relations is performed using representation\nlearning, while cutting and diffusion algorithms are introduced to complete the\nframe imageization. Finally, user behavior videos are constructed based on\ntemporal features. Subsequently, a video classification algorithm is combined\nto identify the spammers. Experiments using publicly available datasets, i.e.,\nWEIBO and TWITTER, show an advantage of the UVSD model over state-of-the-art\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u884c\u4e3a\u89c6\u9891\u5316\u7684UVSD\u6a21\u578b\uff0c\u5c06\u7528\u6237\u884c\u4e3a\u8f6c\u5316\u4e3a\u89c6\u9891\u5e76\u7ed3\u5408\u89c6\u9891\u5206\u7c7b\u6280\u672f\uff0c\u6709\u6548\u68c0\u6d4b\u5783\u573e\u90ae\u4ef6\u53d1\u9001\u8005\u3002", "motivation": "\u53d7\u89c6\u9891\u5206\u7c7b\u6280\u672f\u542f\u53d1\uff0c\u5c06\u7528\u6237\u884c\u4e3a\u5b50\u7a7a\u95f4\u89c6\u4e3a\u5e27\u56fe\u50cf\uff0c\u8fde\u7eed\u5e27\u56fe\u50cf\u89c6\u4e3a\u89c6\u9891\uff0c\u4ee5\u65b0\u9896\u89c6\u89d2\u89e3\u51b3\u5783\u573e\u90ae\u4ef6\u53d1\u9001\u8005\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u63d0\u51faUVSD\u6a21\u578b\u3002\u9996\u5148\uff0c\u901a\u8fc7user2piexl\u7b97\u6cd5\u5c06\u7528\u6237\u50cf\u7d20\u5316\uff0c\u7528\u6237\u7acb\u573a\u91cf\u5316\u4e3a\u50cf\u7d20RGB\u3002\u5176\u6b21\uff0c\u5229\u7528behavior2image\u7b97\u6cd5\u5c06\u7528\u6237\u884c\u4e3a\u5b50\u7a7a\u95f4\u8f6c\u5316\u4e3a\u5e27\u56fe\u50cf\uff0c\u6d89\u53ca\u8868\u793a\u5b66\u4e60\u3001\u526a\u5207\u548c\u6269\u6563\u7b97\u6cd5\u3002\u6700\u540e\uff0c\u57fa\u4e8e\u65f6\u95f4\u7279\u5f81\u6784\u5efa\u7528\u6237\u884c\u4e3a\u89c6\u9891\uff0c\u5e76\u7ed3\u5408\u89c6\u9891\u5206\u7c7b\u7b97\u6cd5\u8bc6\u522b\u5783\u573e\u90ae\u4ef6\u53d1\u9001\u8005\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6WEIBO\u548cTWITTER\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUVSD\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "UVSD\u6a21\u578b\u901a\u8fc7\u5c06\u7528\u6237\u884c\u4e3a\u89c6\u9891\u5316\u5e76\u7ed3\u5408\u89c6\u9891\u5206\u7c7b\u6280\u672f\uff0c\u4e3a\u5783\u573e\u90ae\u4ef6\u53d1\u9001\u8005\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u521b\u65b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2510.06278", "pdf": "https://arxiv.org/pdf/2510.06278", "abs": "https://arxiv.org/abs/2510.06278", "authors": ["M. Sajid", "Mushir Akhtar", "A. Quadir", "M. Tanveer"], "title": "RVFL-X: A Novel Randomized Network Based on Complex Transformed Real-Valued Tabular Datasets", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advancements in neural networks, supported by foundational theoretical\ninsights, emphasize the superior representational power of complex numbers.\nHowever, their adoption in randomized neural networks (RNNs) has been limited\ndue to the lack of effective methods for transforming real-valued tabular\ndatasets into complex-valued representations. To address this limitation, we\npropose two methods for generating complex-valued representations from\nreal-valued datasets: a natural transformation and an autoencoder-driven\nmethod. Building on these mechanisms, we propose RVFL-X, a complex-valued\nextension of the random vector functional link (RVFL) network. RVFL-X\nintegrates complex transformations into real-valued datasets while maintaining\nthe simplicity and efficiency of the original RVFL architecture. By leveraging\ncomplex components such as input, weights, and activation functions, RVFL-X\nprocesses complex representations and produces real-valued outputs.\nComprehensive evaluations on 80 real-valued UCI datasets demonstrate that\nRVFL-X consistently outperforms both the original RVFL and state-of-the-art\n(SOTA) RNN variants, showcasing its robustness and effectiveness across diverse\napplication domains.", "AI": {"tldr": "\u9488\u5bf9\u968f\u673a\u795e\u7ecf\u7f51\u7edc\u4e2d\u590d\u6570\u8868\u793a\u8f6c\u6362\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u5b9e\u503c\u5230\u590d\u503c\u7684\u6570\u636e\u8f6c\u6362\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86\u590d\u503c\u968f\u673a\u5411\u91cf\u51fd\u6570\u8fde\u63a5\u7f51\u7edcRVFL-X\u3002\u5b9e\u9a8c\u8bc1\u660eRVFL-X\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u590d\u6570\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u8868\u793a\u80fd\u529b\uff0c\u4f46\u5728\u968f\u673a\u795e\u7ecf\u7f51\u7edc\uff08RNNs\uff09\u4e2d\uff0c\u7531\u4e8e\u7f3a\u4e4f\u6709\u6548\u7684\u65b9\u6cd5\u5c06\u5b9e\u503c\u8868\u683c\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u590d\u503c\u8868\u793a\uff0c\u5176\u5e94\u7528\u53d7\u5230\u4e86\u9650\u5236\u3002", "method": "1. \u63d0\u51fa\u4e24\u79cd\u5c06\u5b9e\u503c\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u590d\u503c\u8868\u793a\u7684\u65b9\u6cd5\uff1a\u81ea\u7136\u53d8\u6362\u548c\u81ea\u7f16\u7801\u5668\u9a71\u52a8\u7684\u65b9\u6cd5\u30022. \u57fa\u4e8e\u8fd9\u4e9b\u8f6c\u6362\u673a\u5236\uff0c\u63d0\u51faRVFL-X\uff0c\u5b83\u662f\u968f\u673a\u5411\u91cf\u51fd\u6570\u8fde\u63a5\uff08RVFL\uff09\u7f51\u7edc\u7684\u590d\u503c\u6269\u5c55\u3002RVFL-X\u6574\u5408\u4e86\u590d\u6570\u53d8\u6362\u3001\u590d\u6570\u8f93\u5165\u3001\u6743\u91cd\u548c\u6fc0\u6d3b\u51fd\u6570\uff0c\u4ee5\u5904\u7406\u590d\u6570\u8868\u793a\u5e76\u751f\u6210\u5b9e\u503c\u8f93\u51fa\u3002", "result": "\u572880\u4e2a\u5b9e\u503cUCI\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cRVFL-X\u59cb\u7ec8\u4f18\u4e8e\u539f\u59cbRVFL\u7f51\u7edc\u548c\u73b0\u6709\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u968f\u673a\u795e\u7ecf\u7f51\u7edc\u53d8\u4f53\u3002", "conclusion": "RVFL-X\u5c55\u73b0\u4e86\u5176\u5728\u591a\u6837\u5316\u5e94\u7528\u9886\u57df\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5c06\u590d\u6570\u96c6\u6210\u5230\u968f\u673a\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.07109", "pdf": "https://arxiv.org/pdf/2510.07109", "abs": "https://arxiv.org/abs/2510.07109", "authors": ["Guan-Yan Yang", "Farn Wang", "Kuo-Hui Yeh"], "title": "GNN-enhanced Traffic Anomaly Detection for Next-Generation SDN-Enabled Consumer Electronics", "categories": ["cs.CR", "cs.LG", "cs.NI", "C.2.0; C.2.1; C.2.3; C.2.5; I.2.6; K.6.5"], "comment": "This paper has been accepted for publication in IEEE Transactions on\n  Consumer Electronics. 10 pages, 6 figures", "summary": "Consumer electronics (CE) connected to the Internet of Things are susceptible\nto various attacks, including DDoS and web-based threats, which can compromise\ntheir functionality and facilitate remote hijacking. These vulnerabilities\nallow attackers to exploit CE for broader system attacks while enabling the\npropagation of malicious code across the CE network, resulting in device\nfailures. Existing deep learning-based traffic anomaly detection systems\nexhibit high accuracy in traditional network environments but are often overly\ncomplex and reliant on static infrastructure, necessitating manual\nconfiguration and management. To address these limitations, we propose a\nscalable network model that integrates Software-defined Networking (SDN) and\nCompute First Networking (CFN) for next-generation CE networks. In this network\nmodel, we propose a Graph Neural Networks-based Network Anomaly Detection\nframework (GNN-NAD) that integrates SDN-based CE networks and enables the CFN\narchitecture. GNN-NAD uniquely fuses a static, vulnerability-aware attack graph\nwith dynamic traffic features, providing a holistic view of network security.\nThe core of the framework is a GNN model (GSAGE) for graph representation\nlearning, followed by a Random Forest (RF) classifier. This design (GSAGE+RF)\ndemonstrates superior performance compared to existing feature selection\nmethods. Experimental evaluations on CE environment reveal that GNN-NAD\nachieves superior metrics in accuracy, recall, precision, and F1 score, even\nwith small sample sizes, exceeding the performance of current network anomaly\ndetection methods. This work advances the security and efficiency of\nnext-generation intelligent CE networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGNN-NAD\uff0c\u4e00\u4e2a\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u7f51\u7edc\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u4e0b\u4e00\u4ee3\u7269\u8054\u7f51\u6d88\u8d39\u7535\u5b50\uff08CE\uff09\u7f51\u7edc\u3002\u5b83\u6574\u5408SDN\u548cCFN\u67b6\u6784\uff0c\u901a\u8fc7GSAGE\u6a21\u578b\u8fdb\u884c\u56fe\u8868\u793a\u5b66\u4e60\u5e76\u7ed3\u5408\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\uff0c\u5728CE\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7269\u8054\u7f51\u6d88\u8d39\u7535\u5b50\u8bbe\u5907\u6613\u53d7DDoS\u3001\u7f51\u7edc\u653b\u51fb\u548c\u8fdc\u7a0b\u52ab\u6301\u7b49\u5a01\u80c1\uff0c\u5bfc\u81f4\u529f\u80fd\u53d7\u635f\u548c\u6076\u610f\u4ee3\u7801\u4f20\u64ad\u3002\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u68c0\u6d4b\u7cfb\u7edf\u590d\u6742\u4e14\u4f9d\u8d56\u9759\u6001\u57fa\u7840\u8bbe\u65bd\uff0c\u4e0d\u9002\u7528\u4e8e\u52a8\u6001CE\u7f51\u7edc\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u4e0b\u4e00\u4ee3CE\u7f51\u7edc\u5f02\u5e38\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408SDN\u548cCFN\u7684\u53ef\u6269\u5c55\u7f51\u7edc\u6a21\u578b\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bbe\u8ba1\u4e86GNN-NAD\u6846\u67b6\u3002GNN-NAD\u5c06\u9759\u6001\u7684\u3001\u6f0f\u6d1e\u611f\u77e5\u7684\u653b\u51fb\u56fe\u4e0e\u52a8\u6001\u6d41\u91cf\u7279\u5f81\u878d\u5408\uff0c\u91c7\u7528\u4e00\u4e2a\u6838\u5fc3\u7684GNN\u6a21\u578b\uff08GSAGE\uff09\u8fdb\u884c\u56fe\u8868\u793a\u5b66\u4e60\uff0c\u5e76\u7ed3\u5408\u968f\u673a\u68ee\u6797\uff08RF\uff09\u5206\u7c7b\u5668\uff08GSAGE+RF\uff09\u6765\u68c0\u6d4b\u7f51\u7edc\u5f02\u5e38\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cGNN-NAD\uff08GSAGE+RF\uff09\u8bbe\u8ba1\u5728\u51c6\u786e\u6027\u3001\u53ec\u56de\u7387\u3001\u7cbe\u786e\u5ea6\u548cF1\u5206\u6570\u65b9\u9762\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u5373\u4f7f\u5728\u5c0f\u6837\u672c\u91cf\u4e0b\u4e5f\u8d85\u8d8a\u4e86\u73b0\u6709\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u548c\u5f53\u524d\u7684\u7f51\u7edc\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728CE\u73af\u5883\u4e2d\u3002", "conclusion": "GNN-NAD\u6846\u67b6\u6709\u6548\u5730\u63d0\u9ad8\u4e86\u4e0b\u4e00\u4ee3\u667a\u80fd\u6d88\u8d39\u7535\u5b50\u7f51\u7edc\u7684\u5b89\u5168\u6027\u4e0e\u6548\u7387\uff0c\u4e3a\u89e3\u51b3CE\u8bbe\u5907\u9762\u4e34\u7684\u590d\u6742\u7f51\u7edc\u653b\u51fb\u63d0\u4f9b\u4e86\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.06243", "pdf": "https://arxiv.org/pdf/2510.06243", "abs": "https://arxiv.org/abs/2510.06243", "authors": ["Qihua Dong", "Luis Figueroa", "Handong Zhao", "Kushal Kafle", "Jason Kuen", "Zhihong Ding", "Scott Cohen", "Yun Fu"], "title": "CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "MLLM, Referring Expression Segmentation", "summary": "Referring Expression Comprehension and Segmentation are critical tasks for\nassessing the integration of language understanding and image comprehension,\nserving as benchmarks for Multimodal Large Language Models (MLLMs)\ncapabilities. To address these challenges, we propose a new strategy, CoT\nReferring, which enhances model reasoning across modalities through a\nstructured, chain-of-thought training data structure. Our approach\nsystematically parses textual structures to a sequential referring step, where\nin each step it identifies relationships and ensures consistent reference\nalignment, thereby improving accuracy in complex query scenarios. We\nrestructure the training data to enforce a new output form, providing new\nannotations for existing datasets and compiling an evaluation benchmark from\nexisting resources. This benchmark is designed explicitly for complex referring\ncases. We also integrate detection and segmentation capabilities into a unified\nMLLM framework, training it with a novel adaptive weighted loss to optimize\nperformance. Experimental results on our curated benchmark and RefCOCO/+/g\ndemonstrate the effectiveness of our approach, with a notable increase of 2.5%+\nover baseline models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCoT Referring\u7b56\u7565\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u94fe\u5f0f\u601d\u8003\u8bad\u7ec3\u6570\u636e\u548c\u7edf\u4e00\u7684MLLM\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4e0e\u5206\u5272\u662f\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u8bed\u8a00\u4e0e\u56fe\u50cf\u6574\u5408\u80fd\u529b\u7684\u5173\u952e\u4efb\u52a1\u3002\u5f53\u524d\u5728\u590d\u6742\u67e5\u8be2\u573a\u666f\u4e0b\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51faCoT Referring\u7b56\u7565\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u94fe\u5f0f\u601d\u8003\u8bad\u7ec3\u6570\u636e\u7ed3\u6784\u589e\u5f3a\u6a21\u578b\u8de8\u6a21\u6001\u63a8\u7406\u3002\u8be5\u65b9\u6cd5\u5c06\u6587\u672c\u7ed3\u6784\u7cfb\u7edf\u89e3\u6790\u4e3a\u987a\u5e8f\u6307\u4ee3\u6b65\u9aa4\uff0c\u8bc6\u522b\u5173\u7cfb\u5e76\u786e\u4fdd\u5f15\u7528\u4e00\u81f4\u6027\u3002\u91cd\u6784\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u4f9b\u65b0\u6807\u6ce8\uff0c\u5e76\u4ece\u73b0\u6709\u8d44\u6e90\u7f16\u8bd1\u4e86\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u6307\u4ee3\u60c5\u51b5\u7684\u8bc4\u4f30\u57fa\u51c6\u3002\u6b64\u5916\uff0c\u5c06\u68c0\u6d4b\u548c\u5206\u5272\u80fd\u529b\u6574\u5408\u5230\u7edf\u4e00\u7684MLLM\u6846\u67b6\u4e2d\uff0c\u5e76\u91c7\u7528\u65b0\u9896\u7684\u81ea\u9002\u5e94\u52a0\u6743\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u81ea\u5efa\u57fa\u51c6\u548cRefCOCO/+/g\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u6709\u6548\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u63d0\u53472.5%\u4ee5\u4e0a\u3002", "conclusion": "CoT Referring\u7b56\u7565\u901a\u8fc7\u589e\u5f3a\u6a21\u578b\u7684\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\u548c\u7edf\u4e00\u7684\u68c0\u6d4b\u5206\u5272\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4e0e\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728\u590d\u6742\u67e5\u8be2\u573a\u666f\u4e2d\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2510.06302", "pdf": "https://arxiv.org/pdf/2510.06302", "abs": "https://arxiv.org/abs/2510.06302", "authors": ["Ksenija Lace", "Marite Kirikova"], "title": "Requirements for Game-Based Learning Design Framework for Information System Integration in the Context of Post-Merger Integration", "categories": ["cs.AI"], "comment": null, "summary": "Post-merger integration states unique challenges for professionals\nresponsible for information system integration aimed on alignment and\ncombination diverse system architectures of merging organizations. Although the\ntheoretical and practical guidance exists for post-merger integration on the\nbusiness level, there is a significant gap in training for information system\nintegration in this context. In prior research specific methods AMILI (Support\nmethod for informed decision identification) and AMILP (Support method for\ninformed decision-making) were introduced for the support of information system\nintegration decisions in the post-merger integration. But during the practical\napplication was reported high learning curve and low learner motivation. This\npaper explores how game-based learning design can address these limitations by\ntransforming static method training into engaging learning experience. The\nstudy analyzes foundational learning theories, cognitive load and motivation\nmodels, and serious game design frameworks to identify the essential\nrequirements for a game-based learning design framework tailored to information\nsystem integration in post-merger integration. Requirements are structured in\ntwo components: the transformation process and resulting learning experience.\nThe paper concludes with a plan for developing and evaluating the proposed\nframework through iterative design and real-world validation.", "AI": {"tldr": "\u9488\u5bf9\u5e76\u8d2d\u540e\u4fe1\u606f\u7cfb\u7edf\u96c6\u6210\u8bad\u7ec3\u4e2d\u9ad8\u5b66\u4e60\u66f2\u7ebf\u548c\u4f4e\u52a8\u673a\u95ee\u9898\uff0c\u672c\u6587\u63a2\u7d22\u6e38\u620f\u5316\u5b66\u4e60\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5206\u6790\u76f8\u5173\u7406\u8bba\u548c\u6e38\u620f\u6846\u67b6\uff0c\u63d0\u51fa\u4e86\u6784\u5efa\u6e38\u620f\u5316\u5b66\u4e60\u6846\u67b6\u7684\u5173\u952e\u8bbe\u8ba1\u8981\u6c42\u3002", "motivation": "\u5e76\u8d2d\u540e\u4fe1\u606f\u7cfb\u7edf\u96c6\u6210\u5bf9\u4e13\u4e1a\u4eba\u5458\u6784\u6210\u72ec\u7279\u6311\u6218\uff0c\u73b0\u6709\u51b3\u7b56\u652f\u6301\u65b9\u6cd5\uff08AMILI\u3001AMILP\uff09\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u5b66\u4e60\u66f2\u7ebf\u9ad8\u548c\u5b66\u4e60\u8005\u52a8\u673a\u4f4e\u7684\u95ee\u9898\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u57f9\u8bad\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u57fa\u7840\u5b66\u4e60\u7406\u8bba\u3001\u8ba4\u77e5\u8d1f\u8377\u4e0e\u52a8\u673a\u6a21\u578b\u4ee5\u53ca\u4e25\u8083\u6e38\u620f\u8bbe\u8ba1\u6846\u67b6\uff0c\u65e8\u5728\u8bc6\u522b\u5e76\u63d0\u51fa\u9002\u7528\u4e8e\u5e76\u8d2d\u540e\u4fe1\u606f\u7cfb\u7edf\u96c6\u6210\u7684\u6e38\u620f\u5316\u5b66\u4e60\u8bbe\u8ba1\u6846\u67b6\u7684\u5fc5\u8981\u8981\u6c42\u3002\u8fd9\u4e9b\u8981\u6c42\u88ab\u7ed3\u6784\u5316\u4e3a\u8f6c\u5316\u8fc7\u7a0b\u548c\u5b66\u4e60\u4f53\u9a8c\u4e24\u4e2a\u7ec4\u6210\u90e8\u5206\u3002", "result": "\u8bc6\u522b\u5e76\u7ed3\u6784\u5316\u4e86\u9488\u5bf9\u5e76\u8d2d\u540e\u4fe1\u606f\u7cfb\u7edf\u96c6\u6210\u573a\u666f\u7684\u6e38\u620f\u5316\u5b66\u4e60\u8bbe\u8ba1\u6846\u67b6\u7684\u5fc5\u8981\u8981\u6c42\uff0c\u6db5\u76d6\u8f6c\u5316\u8fc7\u7a0b\u548c\u5b66\u4e60\u4f53\u9a8c\u4e24\u4e2a\u65b9\u9762\u3002", "conclusion": "\u8bba\u6587\u89c4\u5212\u4e86\u672a\u6765\u901a\u8fc7\u8fed\u4ee3\u8bbe\u8ba1\u548c\u5b9e\u9645\u9a8c\u8bc1\u6765\u5f00\u53d1\u548c\u8bc4\u4f30\u6240\u63d0\u51fa\u7684\u6e38\u620f\u5316\u5b66\u4e60\u6846\u67b6\u7684\u8ba1\u5212\u3002"}}
{"id": "2510.06238", "pdf": "https://arxiv.org/pdf/2510.06238", "abs": "https://arxiv.org/abs/2510.06238", "authors": ["Sagar Lekhak", "Emmett J. Ientilucci", "Dimah Dera", "Susmita Ghosh"], "title": "Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout", "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.OT"], "comment": "This work has been accepted and presented at IGARSS 2025 and will\n  appear in the IEEE IGARSS 2025 proceedings", "summary": "Detecting surface landmines and unexploded ordnances (UXOs) using deep\nlearning has shown promise in humanitarian demining. However, deterministic\nneural networks can be vulnerable to noisy conditions and adversarial attacks,\nleading to missed detection or misclassification. This study introduces the\nidea of uncertainty quantification through Monte Carlo (MC) Dropout, integrated\ninto a fine-tuned ResNet-50 architecture for surface landmine and UXO\nclassification, which was tested on a simulated dataset. Integrating the MC\nDropout approach helps quantify epistemic uncertainty, providing an additional\nmetric for prediction reliability, which could be helpful to make more informed\ndecisions in demining operations. Experimental results on clean, adversarially\nperturbed, and noisy test images demonstrate the model's ability to flag\nunreliable predictions under challenging conditions. This proof-of-concept\nstudy highlights the need for uncertainty quantification in demining, raises\nawareness about the vulnerability of existing neural networks in demining to\nadversarial threats, and emphasizes the importance of developing more robust\nand reliable models for practical applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u8499\u7279\u5361\u6d1b\uff08MC\uff09Dropout\u96c6\u6210\u5230ResNet-50\u67b6\u6784\u4e2d\uff0c\u7528\u4e8e\u8868\u9762\u5730\u96f7\u548c\u672a\u7206\u5f39\u836f\u7684\u5206\u7c7b\uff0c\u4ee5\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u566a\u58f0\u548c\u5bf9\u6297\u6027\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u786e\u5b9a\u6027\u795e\u7ecf\u7f51\u7edc\u5728\u8868\u9762\u5730\u96f7\u548c\u672a\u7206\u5f39\u836f\u68c0\u6d4b\u4e2d\uff0c\u6613\u53d7\u566a\u58f0\u6761\u4ef6\u548c\u5bf9\u6297\u6027\u653b\u51fb\u5f71\u54cd\uff0c\u5bfc\u81f4\u6f0f\u68c0\u6216\u8bef\u5206\u7c7b\uff0c\u8fd9\u5728\u4eba\u9053\u4e3b\u4e49\u6392\u96f7\u4e2d\u662f\u4e25\u91cd\u7684\u95ee\u9898\u3002", "method": "\u5c06\u8499\u7279\u5361\u6d1b\uff08MC\uff09Dropout\u65b9\u6cd5\u96c6\u6210\u5230\u7ecf\u8fc7\u5fae\u8c03\u7684ResNet-50\u67b6\u6784\u4e2d\uff0c\u7528\u4e8e\u8868\u9762\u5730\u96f7\u548c\u672a\u7206\u5f39\u836f\u7684\u5206\u7c7b\uff0c\u901a\u8fc7\u91cf\u5316\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u4f9b\u989d\u5916\u7684\u9884\u6d4b\u53ef\u9760\u6027\u6307\u6807\u3002\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5e72\u51c0\u3001\u53d7\u5bf9\u6297\u6027\u6270\u52a8\u548c\u566a\u58f0\u7684\u6d4b\u8bd5\u56fe\u50cf\u4e0a\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u6807\u8bb0\u51fa\u4e0d\u53ef\u9760\u7684\u9884\u6d4b\uff0c\u5728\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u63d0\u5347\u4e86\u9884\u6d4b\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u672c\u6982\u5ff5\u9a8c\u8bc1\u7814\u7a76\u5f3a\u8c03\u4e86\u6392\u96f7\u4e2d\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u5fc5\u8981\u6027\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u5728\u5bf9\u6297\u6027\u5a01\u80c1\u4e0b\u7684\u8106\u5f31\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u53ef\u9760\u6a21\u578b\u7684\u7d27\u8feb\u6027\uff0c\u4ee5\u5b9e\u73b0\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2510.06284", "pdf": "https://arxiv.org/pdf/2510.06284", "abs": "https://arxiv.org/abs/2510.06284", "authors": ["Anne Dranowski", "Yura Kabkov", "Daniel Tubbenhauer"], "title": "On knot detection via picture recognition", "categories": ["cs.LG", "cs.CV", "math.GT", "Primary: 57K10, 68T07, secondary: 57K14, 68T45"], "comment": "21 pages, many figures, comments welcome", "summary": "Our goal is to one day take a photo of a knot and have a phone automatically\nrecognize it. In this expository work, we explain a strategy to approximate\nthis goal, using a mixture of modern machine learning methods (in particular\nconvolutional neural networks and transformers for image recognition) and\ntraditional algorithms (to compute quantum invariants like the Jones\npolynomial). We present simple baselines that predict crossing number directly\nfrom images, showing that even lightweight CNN and transformer architectures\ncan recover meaningful structural information. The longer-term aim is to\ncombine these perception modules with symbolic reconstruction into planar\ndiagram (PD) codes, enabling downstream invariant computation for robust knot\nclassification. This two-stage approach highlights the complementarity between\nmachine learning, which handles noisy visual data, and invariants, which\nenforce rigorous topological distinctions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u4f20\u7edf\u7b97\u6cd5\u7684\u7b56\u7565\uff0c\u65e8\u5728\u5b9e\u73b0\u901a\u8fc7\u624b\u673a\u7167\u7247\u81ea\u52a8\u8bc6\u522b\u548c\u5206\u7c7b\u7ed3\u7684\u76ee\u6807\u3002", "motivation": "\u6700\u7ec8\u76ee\u6807\u662f\u8ba9\u624b\u673a\u80fd\u901a\u8fc7\u7167\u7247\u81ea\u52a8\u8bc6\u522b\u7ed3\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\uff08\u7279\u522b\u662f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548cTransformer\u8fdb\u884c\u56fe\u50cf\u8bc6\u522b\uff09\u548c\u4f20\u7edf\u7b97\u6cd5\uff08\u7528\u4e8e\u8ba1\u7b97Jones\u591a\u9879\u5f0f\u7b49\u91cf\u5b50\u4e0d\u53d8\u91cf\uff09\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u5c06\u611f\u77e5\u6a21\u5757\u4e0e\u5e73\u9762\u56fe\uff08PD\uff09\u4ee3\u7801\u7684\u7b26\u53f7\u91cd\u5efa\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e0b\u6e38\u4e0d\u53d8\u91cf\u8ba1\u7b97\u3002", "result": "\u63d0\u51fa\u4e86\u7b80\u5355\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5373\u4f7f\u662f\u8f7b\u91cf\u7ea7\u7684CNN\u548cTransformer\u67b6\u6784\u4e5f\u80fd\u76f4\u63a5\u4ece\u56fe\u50cf\u4e2d\u9884\u6d4b\u4ea4\u53c9\u6570\uff0c\u5e76\u6062\u590d\u6709\u610f\u4e49\u7684\u7ed3\u6784\u4fe1\u606f\u3002", "conclusion": "\u8be5\u4e24\u9636\u6bb5\u65b9\u6cd5\u7a81\u663e\u4e86\u673a\u5668\u5b66\u4e60\uff08\u5904\u7406\u5608\u6742\u89c6\u89c9\u6570\u636e\uff09\u4e0e\u4e0d\u53d8\u91cf\uff08\u5f3a\u5236\u4e25\u683c\u62d3\u6251\u533a\u5206\uff09\u4e4b\u95f4\u7684\u4e92\u8865\u6027\uff0c\u5bf9\u4e8e\u9c81\u68d2\u7684\u7ed3\u5206\u7c7b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2510.06244", "pdf": "https://arxiv.org/pdf/2510.06244", "abs": "https://arxiv.org/abs/2510.06244", "authors": ["Nouman Ahmed", "Ronin Wu", "Victor Botev"], "title": "Evaluating Embedding Frameworks for Scientific Domain", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Finding an optimal word representation algorithm is particularly important in\nterms of domain specific data, as the same word can have different meanings and\nhence, different representations depending on the domain and context. While\nGenerative AI and transformer architecture does a great job at generating\ncontextualized embeddings for any given work, they are quite time and compute\nextensive, especially if we were to pre-train such a model from scratch. In\nthis work, we focus on the scientific domain and finding the optimal word\nrepresentation algorithm along with the tokenization method that could be used\nto represent words in the scientific domain. The goal of this research is two\nfold: 1) finding the optimal word representation and tokenization methods that\ncan be used in downstream scientific domain NLP tasks, and 2) building a\ncomprehensive evaluation suite that could be used to evaluate various word\nrepresentation and tokenization algorithms (even as new ones are introduced) in\nthe scientific domain. To this end, we build an evaluation suite consisting of\nseveral downstream tasks and relevant datasets for each task. Furthermore, we\nuse the constructed evaluation suite to test various word representation and\ntokenization algorithms.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u79d1\u5b66\u9886\u57df\u5bfb\u627e\u6700\u4f18\u8bcd\u8868\u793a\u548c\u5206\u8bcd\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u5957\u4ef6\u4ee5\u6d4b\u8bd5\u548c\u6bd4\u8f83\u5404\u79cd\u7b97\u6cd5\u3002", "motivation": "\u9886\u57df\u7279\u5b9a\u6570\u636e\u4e2d\u8bcd\u6c47\u542b\u4e49\u591a\u53d8\uff0c\u9700\u8981\u6700\u4f18\u8bcd\u8868\u793a\u3002\u5c3d\u7ba1\u751f\u6210\u5f0fAI\u548cTransformer\u6a21\u578b\u80fd\u751f\u6210\u8bed\u5883\u5316\u5d4c\u5165\uff0c\u4f46\u4ece\u5934\u9884\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5728\u79d1\u5b66\u9886\u57df\u627e\u5230\u9ad8\u6548\u4e14\u4f18\u5316\u7684\u8bcd\u8868\u793a\u548c\u5206\u8bcd\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u5305\u542b\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u53ca\u76f8\u5e94\u6570\u636e\u96c6\u7684\u7efc\u5408\u8bc4\u4f30\u5957\u4ef6\uff0c\u5e76\u5229\u7528\u6b64\u5957\u4ef6\u6d4b\u8bd5\u5404\u79cd\u8bcd\u8868\u793a\u548c\u5206\u8bcd\u7b97\u6cd5\u3002", "result": "\u6458\u8981\u4e2d\u672a\u63d0\u4f9b\u5177\u4f53\u7684\u7814\u7a76\u7ed3\u679c\u3002", "conclusion": "\u6458\u8981\u4e3b\u8981\u9610\u8ff0\u7814\u7a76\u52a8\u673a\u3001\u76ee\u6807\u548c\u65b9\u6cd5\uff0c\u5c1a\u672a\u5f97\u51fa\u7814\u7a76\u7ed3\u8bba\u3002"}}
{"id": "2510.06307", "pdf": "https://arxiv.org/pdf/2510.06307", "abs": "https://arxiv.org/abs/2510.06307", "authors": ["Wentao Deng", "Jiahuan Pei", "Zhiwei Xu", "Zhaochun Ren", "Zhumin Chen", "Pengjie Ren"], "title": "Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks", "categories": ["cs.AI"], "comment": "This paper has been accepted by NeurIPS 2025", "summary": "A multi-agent system (MAS) enhances its capacity to solve complex natural\nlanguage processing (NLP) tasks through collaboration among multiple agents,\nwhere consensus-seeking serves as a fundamental mechanism. However, existing\nconsensus-seeking approaches typically rely on voting mechanisms to judge\nconsensus, overlooking contradictions in system-internal beliefs that\ndestabilize the consensus. Moreover, these methods often involve agents\nupdating their results through indiscriminate collaboration with every other\nagent. Such uniform interaction fails to identify the optimal collaborators for\neach agent, hindering the emergence of a stable consensus. To address these\nchallenges, we provide a theoretical framework for selecting optimal\ncollaborators that maximize consensus stability. Based on the theorems, we\npropose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate\nstable consensus via selecting optimal collaborators and calibrating the\nconsensus judgment by system-internal beliefs. Experimental results on the MATH\nand MMLU benchmark datasets demonstrate that the proposed BCCS framework\noutperforms the best existing results by 2.23% and 3.95% of accuracy on\nchallenging tasks, respectively. Our code and data are available at\nhttps://github.com/dengwentao99/BCCS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4fe1\u5ff5\u6821\u51c6\u5171\u8bc6\u5bfb\u6c42\uff08BCCS\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u5408\u4f5c\u8005\u9009\u62e9\u548c\u5185\u90e8\u4fe1\u5ff5\u6821\u51c6\u6765\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742NLP\u4efb\u52a1\u4e2d\u7684\u5171\u8bc6\u7a33\u5b9a\u6027\uff0c\u5e76\u5728MATH\u548cMMLU\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5171\u8bc6\u5bfb\u6c42\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u6295\u7968\u673a\u5236\uff0c\u5ffd\u7565\u4e86\u7cfb\u7edf\u5185\u90e8\u4fe1\u5ff5\u77db\u76fe\u5bfc\u81f4\u7684\u5171\u8bc6\u4e0d\u7a33\u5b9a\u6027\u3002\u6b64\u5916\uff0c\u65e0\u5dee\u522b\u7684\u667a\u80fd\u4f53\u95f4\u534f\u4f5c\u672a\u80fd\u6709\u6548\u8bc6\u522b\u6700\u4f18\u5408\u4f5c\u8005\uff0c\u963b\u788d\u4e86\u7a33\u5b9a\u5171\u8bc6\u7684\u5f62\u6210\u3002", "method": "\u4f5c\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u9009\u62e9\u80fd\u591f\u6700\u5927\u5316\u5171\u8bc6\u7a33\u5b9a\u6027\u7684\u6700\u4f18\u5408\u4f5c\u8005\u3002\u5728\u6b64\u7406\u8bba\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4fe1\u5ff5\u6821\u51c6\u5171\u8bc6\u5bfb\u6c42\uff08BCCS\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6700\u4f18\u5408\u4f5c\u8005\u548c\u5229\u7528\u7cfb\u7edf\u5185\u90e8\u4fe1\u5ff5\u6821\u51c6\u5171\u8bc6\u5224\u65ad\uff0c\u4ee5\u4fc3\u8fdb\u7a33\u5b9a\u5171\u8bc6\u3002", "result": "\u5728MATH\u548cMMLU\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684BCCS\u6846\u67b6\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e0a\uff0c\u51c6\u786e\u7387\u5206\u522b\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u7ed3\u679c2.23%\u548c3.95%\u3002", "conclusion": "BCCS\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u73b0\u6709\u5171\u8bc6\u5bfb\u6c42\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742NLP\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u7a33\u5b9a\u3001\u66f4\u9ad8\u6548\u7684\u5171\u8bc6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u9014\u5f84\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2510.06241", "pdf": "https://arxiv.org/pdf/2510.06241", "abs": "https://arxiv.org/abs/2510.06241", "authors": ["Anselm W. Stark", "Marc Ilic", "Ali Mokhtari", "Pooya Mohammadi Kazaj", "Christoph Graeni", "Isaac Shiri"], "title": "multimodars: A Rust-powered toolkit for multi-modality cardiac image fusion and registration", "categories": ["cs.CV", "physics.med-ph"], "comment": null, "summary": "Combining complementary imaging modalities is critical to build reliable 3D\ncoronary models: intravascular imaging gives sub-millimetre resolution but\nlimited whole-vessel context, while CCTA supplies 3D geometry but suffers from\nlimited spatial resolution and artefacts (e.g., blooming). Prior work\ndemonstrated intravascular/CCTA fusion, yet no open, flexible toolkit is\ntailored for multi-state analysis (rest/stress, pre-/post-stenting) while\noffering deterministic behaviour, high performance, and easy pipeline\nintegration. multimodars addresses this gap with deterministic alignment\nalgorithms, a compact NumPy-centred data model, and an optimised Rust backend\nsuitable for scalable, reproducible experiments. The package accepts CSV/NumPy\ninputs including data formats produced by the AIVUS-CAA software", "AI": {"tldr": "\u201cmultimodars\u201d\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u786e\u5b9a\u6027\u7b97\u6cd5\u878d\u5408\u8840\u7ba1\u5185\u6210\u50cf\u4e0eCCTA\u6570\u636e\uff0c\u7528\u4e8e\u6784\u5efa\u53ef\u9760\u76843D\u51a0\u72b6\u52a8\u8109\u6a21\u578b\uff0c\u652f\u6301\u591a\u72b6\u6001\u5206\u6790\uff0c\u5e76\u63d0\u4f9b\u9ad8\u6027\u80fd\u548c\u6613\u96c6\u6210\u6027\u3002", "motivation": "\u6784\u5efa\u53ef\u9760\u76843D\u51a0\u72b6\u52a8\u8109\u6a21\u578b\u9700\u8981\u7ed3\u5408\u8840\u7ba1\u5185\u6210\u50cf\uff08\u9ad8\u5206\u8fa8\u7387\uff0c\u4f46\u7f3a\u4e4f\u6574\u4f53\u80cc\u666f\uff09\u548cCCTA\uff08\u63d0\u4f9b3D\u51e0\u4f55\uff0c\u4f46\u5206\u8fa8\u7387\u6709\u9650\u4e14\u6709\u4f2a\u5f71\uff09\u3002\u73b0\u6709\u878d\u5408\u65b9\u6cd5\u7f3a\u4e4f\u4e00\u4e2a\u5f00\u653e\u3001\u7075\u6d3b\u3001\u652f\u6301\u591a\u72b6\u6001\u5206\u6790\uff08\u5982\u9759\u606f/\u5e94\u6fc0\u3001\u652f\u67b6\u524d/\u540e\uff09\u3001\u4e14\u5177\u6709\u786e\u5b9a\u6027\u3001\u9ad8\u6027\u80fd\u548c\u6613\u96c6\u6210\u7279\u6027\u7684\u5de5\u5177\u5305\u3002", "method": "\u5f00\u53d1\u4e86\u201cmultimodars\u201d\u5de5\u5177\u5305\uff0c\u91c7\u7528\u786e\u5b9a\u6027\u5bf9\u9f50\u7b97\u6cd5\uff0c\u4ee5NumPy\u4e3a\u4e2d\u5fc3\u7684\u7d27\u51d1\u6570\u636e\u6a21\u578b\uff0c\u4ee5\u53ca\u4f18\u5316\u7684Rust\u540e\u7aef\uff0c\u9002\u7528\u4e8e\u53ef\u6269\u5c55\u548c\u53ef\u91cd\u73b0\u7684\u5b9e\u9a8c\u3002\u8be5\u5de5\u5177\u5305\u53ef\u63a5\u53d7CSV/NumPy\u8f93\u5165\uff0c\u5305\u62ecAIVUS-CAA\u8f6f\u4ef6\u751f\u6210\u7684\u6570\u636e\u683c\u5f0f\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u5f25\u8865\u73b0\u6709\u7a7a\u767d\u7684\u5de5\u5177\u5305\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5206\u8fa8\u7387\u8840\u7ba1\u5185\u6210\u50cf\u4e0eCCTA\u6570\u636e\u7684\u786e\u5b9a\u6027\u878d\u5408\uff0c\u5e76\u4e3a\u591a\u72b6\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u3001\u53ef\u6269\u5c55\u548c\u53ef\u91cd\u73b0\u7684\u5b9e\u9a8c\u5e73\u53f0\u3002", "conclusion": "\u201cmultimodars\u201d\u4e3a\u6784\u5efa\u53ef\u9760\u76843D\u51a0\u72b6\u52a8\u8109\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6025\u9700\u7684\u5f00\u653e\u3001\u7075\u6d3b\u3001\u786e\u5b9a\u6027\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u590d\u6742\u7684\u4e34\u5e8a\u591a\u72b6\u6001\u5206\u6790\uff0c\u514b\u670d\u4e86\u73b0\u6709\u5de5\u5177\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.06291", "pdf": "https://arxiv.org/pdf/2510.06291", "abs": "https://arxiv.org/abs/2510.06291", "authors": ["Zhiyang Zhang", "Ningcong Chen", "Xin Zhang", "Yanhua Li", "Shen Su", "Hui Lu", "Jun Luo"], "title": "Traj-Transformer: Diffusion Models with Transformer for GPS Trajectory Generation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The widespread use of GPS devices has driven advances in spatiotemporal data\nmining, enabling machine learning models to simulate human decision making and\ngenerate realistic trajectories, addressing both data collection costs and\nprivacy concerns. Recent studies have shown the promise of diffusion models for\nhigh-quality trajectory generation. However, most existing methods rely on\nconvolution based architectures (e.g. UNet) to predict noise during the\ndiffusion process, which often results in notable deviations and the loss of\nfine-grained street-level details due to limited model capacity. In this paper,\nwe propose Trajectory Transformer, a novel model that employs a transformer\nbackbone for both conditional information embedding and noise prediction. We\nexplore two GPS coordinate embedding strategies, location embedding and\nlongitude-latitude embedding, and analyze model performance at different\nscales. Experiments on two real-world datasets demonstrate that Trajectory\nTransformer significantly enhances generation quality and effectively\nalleviates the deviation issues observed in prior approaches.", "AI": {"tldr": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u8f68\u8ff9\u751f\u6210\u4e2d\u56e0\u5377\u79ef\u67b6\u6784\u5bfc\u81f4\u7ec6\u8282\u4e22\u5931\u548c\u504f\u5dee\u3002\u672c\u6587\u63d0\u51faTrajectory Transformer\uff0c\u5229\u7528Transformer\u9aa8\u5e72\u7f51\u7edc\u63d0\u5347\u8f68\u8ff9\u751f\u6210\u8d28\u91cf\uff0c\u6709\u6548\u51cf\u5c11\u504f\u5dee\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u8f68\u8ff9\u751f\u6210\u65f6\uff0c\u591a\u4f9d\u8d56\u5377\u79ef\u67b6\u6784\uff08\u5982UNet\uff09\u9884\u6d4b\u566a\u58f0\uff0c\u56e0\u6a21\u578b\u5bb9\u91cf\u9650\u5236\uff0c\u5e38\u5bfc\u81f4\u663e\u8457\u504f\u5dee\u5e76\u4e22\u5931\u7cbe\u7ec6\u8857\u666f\u7ec6\u8282\u3002", "method": "\u63d0\u51faTrajectory Transformer\u6a21\u578b\uff0c\u91c7\u7528Transformer\u9aa8\u5e72\u7f51\u7edc\u8fdb\u884c\u6761\u4ef6\u4fe1\u606f\u5d4c\u5165\u548c\u566a\u58f0\u9884\u6d4b\u3002\u63a2\u7d22\u4e86\u4f4d\u7f6e\u5d4c\u5165\u548c\u7ecf\u7eac\u5ea6\u5d4c\u5165\u4e24\u79cdGPS\u5750\u6807\u5d4c\u5165\u7b56\u7565\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u5c3a\u5ea6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTrajectory Transformer\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u6709\u6548\u7f13\u89e3\u4e86\u5148\u524d\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u504f\u5dee\u95ee\u9898\u3002", "conclusion": "Trajectory Transformer\u901a\u8fc7\u5f15\u5165Transformer\u67b6\u6784\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u8f68\u8ff9\u751f\u6210\u4e2d\u5b58\u5728\u7684\u7ec6\u8282\u4e22\u5931\u548c\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2510.06249", "pdf": "https://arxiv.org/pdf/2510.06249", "abs": "https://arxiv.org/abs/2510.06249", "authors": ["Toshiki Nakai", "Ravi Kiran Chikkala", "Lena Sophie Oberkircher", "Nicholas Jennings", "Natalia Skachkova", "Tatiana Anikina", "Jesujoba Oluwadara Alabi"], "title": "TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B", "categories": ["cs.CL", "cs.AI"], "comment": "It is work in progress", "summary": "The 2025 Multimodal Models for Low-Resource Contexts and Social Impact\n(MMLoSo) Language Challenge addresses one of India's most pressing linguistic\ngaps: the lack of resources for its diverse low-resource languages (LRLs). In\nthis study, we investigate whether enforcing cross-lingual similarity in\nspecific internal layers of a decoder-only multilingual large language model\n(LLM) can improve translation quality from LRL to high-resource language (HRL).\nSpecifically, we combine Centered Kernel Alignment (CKA), a similarity metric\nthat encourages representations of different languages to align, with REPINA, a\nregularization method that constrains parameter updates to remain close to the\npretrained model, into a joint method we call TRepLiNa. In this research\nproject, we experiment with zero-shot, few-shot, and fine-tuning settings using\nAya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari,\nSantali, Bhili) with Hindi/English pivots. Our results show that aligning\nmid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach\nto improving LRL translation, especially in data-scarce settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faTRepLiNa\uff08\u7ed3\u5408CKA\u4e0eREPINA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u591a\u8bed\u8a00LLM\uff08Aya-23 8B\uff09\u5185\u90e8\u5c42\u8fdb\u884c\u8de8\u8bed\u8a00\u76f8\u4f3c\u6027\u5bf9\u9f50\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08LRL\uff09\u7684\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u4f4e\u6210\u672c\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5370\u5ea6\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08LRLs\uff09\u7684\u8d44\u6e90\u532e\u4e4f\u95ee\u9898\uff0c\u5e76\u63a2\u7a76\u901a\u8fc7\u5728\u89e3\u7801\u5668-only\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7279\u5b9a\u5185\u90e8\u5c42\u4e2d\u5f3a\u5236\u6267\u884c\u8de8\u8bed\u8a00\u76f8\u4f3c\u6027\uff0c\u662f\u5426\u80fd\u63d0\u9ad8\u4eceLRL\u5230\u9ad8\u8d44\u6e90\u8bed\u8a00\uff08HRL\uff09\u7684\u7ffb\u8bd1\u8d28\u91cf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aTRepLiNa\u7684\u8054\u5408\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u65e8\u5728\u9f13\u52b1\u4e0d\u540c\u8bed\u8a00\u8868\u793a\u5bf9\u9f50\u7684\u76f8\u4f3c\u6027\u5ea6\u91cfCKA\uff08Centered Kernel Alignment\uff09\u548c\u9650\u5236\u53c2\u6570\u66f4\u65b0\u9760\u8fd1\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6b63\u5219\u5316\u65b9\u6cd5REPINA\u3002\u5b9e\u9a8c\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u8fdb\u884c\uff0c\u4f7f\u7528Aya-23 8B\u6a21\u578b\u7ed3\u5408QLoRA\u6280\u672f\uff0c\u9488\u5bf9MMLoSo\u5171\u4eab\u4efb\u52a1\u7684\u8bed\u8a00\u5bf9\uff08\u8499\u8fbe\u91cc\u8bed\u3001\u6851\u5854\u5229\u8bed\u3001\u6bd4\u91cc\u8bed\uff09\u4e0e\u5370\u5730\u8bed/\u82f1\u8bed\u652f\u70b9\u8fdb\u884c\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528TRepLiNa\uff08CKA+REPINA\uff09\u5bf9\u4e2d\u95f4\u5c42\u8fdb\u884c\u5bf9\u9f50\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u7684\u573a\u666f\u4e0b\u6548\u679c\u663e\u8457\u3002", "conclusion": "TRepLiNa\uff08CKA+REPINA\uff09\u901a\u8fc7\u5bf9\u9f50\u591a\u8bed\u8a00LLM\u7684\u4e2d\u95f4\u5c42\uff0c\u4e3a\u6539\u5584\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u3001\u4f4e\u6210\u672c\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6570\u636e\u53d7\u9650\u7684\u73af\u5883\u3002"}}
{"id": "2510.06410", "pdf": "https://arxiv.org/pdf/2510.06410", "abs": "https://arxiv.org/abs/2510.06410", "authors": ["Aochong Oliver Li", "Tanya Goyal"], "title": "Off-Trajectory Reasoning: Can LLMs Collaborate on Reasoning Trajectory?", "categories": ["cs.AI"], "comment": null, "summary": "Reasoning LLMs are trained to verbalize their reasoning process, yielding\nstrong gains on complex tasks. This transparency also opens a promising\ndirection: multiple reasoners can directly collaborate on each other's thinking\nwithin a shared trajectory, yielding better inference efficiency and\nexploration. A key prerequisite, however, is the ability to assess the\nusefulness and build on another model's partial thinking -- we call this\noff-trajectory reasoning. Our paper investigates a critical question: can\nstandard solo-reasoning training pipelines deliver desired off-trajectory\nbehaviors? We propose twin tests that capture the two extremes of the\noff-trajectory spectrum, namely Recoverability, which tests whether LLMs can\nbacktrack from \"distractions\" induced by misleading reasoning traces, and\nGuidability, which tests their ability to build upon correct reasoning from\nstronger collaborators. Our study evaluates 15 open-weight LLMs (1.5B-32B) and\nreveals a counterintuitive finding -- \"stronger\" LLMs on benchmarks are often\nmore fragile under distraction. Moreover, all models tested fail to effectively\nleverage guiding steps from collaborators on problems beyond their inherent\ncapabilities with solve rates remaining under 9.2%. Finally, we conduct control\nstudies to isolate the effects of three factors in post-training on these\nbehaviors: the choice of distillation teacher, the use of RL, and data\nselection strategy. Our results provide actionable insights for training\nnatively strong reasoning collaborators; e.g., we find that suboptimal\nrecoverability behaviors of teacher models are transferred to distilled\nstudents even if the distillation trajectories are correct. Taken together,\nthis work lays the groundwork for evaluating multi-model collaborations in\nshared reasoning trajectories and highlights the limitations of off-the-shelf\nreasoning LLMs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6807\u51c6LLM\u8bad\u7ec3\u65b9\u6cd5\u80fd\u5426\u4f7f\u5176\u5177\u5907\u201c\u8131\u8f68\u63a8\u7406\u201d\u80fd\u529b\uff08\u5373\u8bc4\u4f30\u5e76\u5229\u7528\u5176\u4ed6\u6a21\u578b\u7684\u601d\u8003\uff09\u3002\u901a\u8fc7\u63d0\u51faRecoverability\u548cGuidability\u201c\u53cc\u751f\u6d4b\u8bd5\u201d\uff0c\u53d1\u73b0\u5f53\u524d\u5f3a\u5927\u7684LLM\u5728\u9762\u5bf9\u5e72\u6270\u65f6\u66f4\u8106\u5f31\uff0c\u4e14\u65e0\u6cd5\u6709\u6548\u5229\u7528\u5408\u4f5c\u8005\u7684\u6307\u5bfc\uff0c\u63ed\u793a\u4e86\u73b0\u6709LLM\u5728\u591a\u6a21\u578b\u534f\u4f5c\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u63a8\u7406\u8fc7\u7a0b\u7684\u900f\u660e\u5316\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002\u8fd9\u79cd\u900f\u660e\u6027\u4e5f\u4e3a\u591a\u6a21\u578b\u76f4\u63a5\u534f\u4f5c\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002\u7136\u800c\uff0c\u5173\u952e\u524d\u63d0\u662fLLM\u9700\u8981\u5177\u5907\u8bc4\u4f30\u5e76\u57fa\u4e8e\u5176\u4ed6\u6a21\u578b\u90e8\u5206\u601d\u8003\u7684\u80fd\u529b\uff0c\u5373\u201c\u8131\u8f68\u63a8\u7406\u201d\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u6807\u51c6\u5355\u4f53\u63a8\u7406\u8bad\u7ec3\u7ba1\u9053\u662f\u5426\u80fd\u57f9\u517b\u51fa\u6240\u9700\u7684\u8131\u8f68\u884c\u4e3a\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u8861\u91cf\u8131\u8f68\u63a8\u7406\u7684\u201c\u53cc\u751f\u6d4b\u8bd5\u201d\uff1aRecoverability\uff08\u6062\u590d\u80fd\u529b\uff09\uff0c\u6d4b\u8bd5LLM\u4ece\u8bef\u5bfc\u6027\u63a8\u7406\u8f68\u8ff9\u4e2d\u56de\u6eaf\u7684\u80fd\u529b\uff1bGuidability\uff08\u5f15\u5bfc\u80fd\u529b\uff09\uff0c\u6d4b\u8bd5LLM\u57fa\u4e8e\u66f4\u5f3a\u5408\u4f5c\u8005\u6b63\u786e\u63a8\u7406\u8fdb\u884c\u6784\u5efa\u7684\u80fd\u529b\u3002\u7814\u7a76\u8bc4\u4f30\u4e8615\u4e2a\u5f00\u6e90LLM\uff081.5B-32B\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u9694\u79bb\u4e86\u84b8\u998f\u6559\u5e08\u6a21\u578b\u9009\u62e9\u3001\u5f3a\u5316\u5b66\u4e60\u4f7f\u7528\u548c\u6570\u636e\u9009\u62e9\u7b56\u7565\u7b49\u540e\u8bad\u7ec3\u56e0\u7d20\u5bf9\u8fd9\u4e9b\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u4e00\u4e2a\u53cd\u76f4\u89c9\u7684\u7ed3\u679c\uff1a\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u201c\u66f4\u5f3a\u201d\u7684LLM\u5728\u9762\u5bf9\u5e72\u6270\u65f6\u5f80\u5f80\u66f4\u8106\u5f31\uff08Recoverability\u8f83\u5dee\uff09\u3002\u6240\u6709\u88ab\u6d4b\u8bd5\u7684\u6a21\u578b\u90fd\u65e0\u6cd5\u6709\u6548\u5229\u7528\u5408\u4f5c\u8005\u63d0\u4f9b\u7684\u5f15\u5bfc\u6b65\u9aa4\u6765\u89e3\u51b3\u8d85\u51fa\u5176\u56fa\u6709\u80fd\u529b\u7684\u95ee\u9898\uff0c\u89e3\u51b3\u7387\u4fdd\u6301\u57289.2%\u4ee5\u4e0b\uff08Guidability\u8f83\u5dee\uff09\u3002\u63a7\u5236\u7814\u7a76\u8868\u660e\uff0c\u6559\u5e08\u6a21\u578b\u6b21\u4f18\u7684\u6062\u590d\u80fd\u529b\u884c\u4e3a\u4f1a\u88ab\u4f20\u9012\u7ed9\u901a\u8fc7\u84b8\u998f\u8bad\u7ec3\u7684\u5b66\u751f\u6a21\u578b\uff0c\u5373\u4f7f\u84b8\u998f\u8f68\u8ff9\u662f\u6b63\u786e\u7684\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8bc4\u4f30\u5171\u4eab\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u591a\u6a21\u578b\u534f\u4f5c\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u5f3a\u8c03\u4e86\u73b0\u6210\u63a8\u7406LLM\u5728\u4f5c\u4e3a\u6709\u6548\u534f\u4f5c\u4f19\u4f34\u65b9\u9762\u7684\u5c40\u5c40\u9650\u6027\u3002\u7814\u7a76\u7ed3\u679c\u4e5f\u4e3a\u8bad\u7ec3\u539f\u751f\u5f3a\u5927\u7684\u63a8\u7406\u534f\u4f5c\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u4f8b\u5982\uff0c\u63d0\u793a\u4e86\u84b8\u998f\u8bad\u7ec3\u4e2d\u6559\u5e08\u6a21\u578b\u9009\u62e9\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.06251", "pdf": "https://arxiv.org/pdf/2510.06251", "abs": "https://arxiv.org/abs/2510.06251", "authors": ["Ieva Bagdonaviciute", "Vibhav Vineet"], "title": "Does Physics Knowledge Emerge in Frontier Models?", "categories": ["cs.CV"], "comment": "8 pages, 7 figures. Preprint", "summary": "Leading Vision-Language Models (VLMs) show strong results in visual\nperception and general reasoning, but their ability to understand and predict\nphysical dynamics remains unclear. We benchmark six frontier VLMs on three\nphysical simulation datasets - CLEVRER, Physion, and Physion++ - where the\nevaluation tasks test whether a model can predict outcomes or hypothesize about\nalternative situations. To probe deeper, we design diagnostic subtests that\nisolate perception (objects, colors, occluders) from physics reasoning (motion\nprediction, spatial relations). Intuitively, stronger diagnostic performance\nshould support higher evaluation accuracy. Yet our analysis reveals weak\ncorrelations: models that excel at perception or physics reasoning do not\nconsistently perform better on predictive or counterfactual evaluation. This\ncounterintuitive gap exposes a central limitation of current VLMs: perceptual\nand physics skills remain fragmented and fail to combine into causal\nunderstanding, underscoring the need for architectures that bind perception and\nreasoning more tightly.", "AI": {"tldr": "\u5f53\u524d\u9886\u5148\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u7269\u7406\u52a8\u6001\u7406\u89e3\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5176\u611f\u77e5\u548c\u7269\u7406\u63a8\u7406\u80fd\u529b\u672a\u80fd\u6709\u6548\u7ed3\u5408\u4ee5\u5b9e\u73b0\u56e0\u679c\u7406\u89e3\u3002", "motivation": "\u5c3d\u7ba1\u9886\u5148\u7684VLMs\u5728\u89c6\u89c9\u611f\u77e5\u548c\u901a\u7528\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u7406\u89e3\u548c\u9884\u6d4b\u7269\u7406\u52a8\u6001\u7684\u80fd\u529b\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u516d\u4e2a\u524d\u6cbfVLM\u5728\u4e09\u4e2a\u7269\u7406\u6a21\u62df\u6570\u636e\u96c6\uff08CLEVRER, Physion, Physion++\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u4efb\u52a1\u5305\u62ec\u7ed3\u679c\u9884\u6d4b\u548c\u53cd\u4e8b\u5b9e\u5047\u8bbe\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86\u8bca\u65ad\u6027\u5b50\u6d4b\u8bd5\uff0c\u4ee5\u533a\u5206\u611f\u77e5\uff08\u7269\u4f53\u3001\u989c\u8272\u3001\u906e\u6321\u7269\uff09\u548c\u7269\u7406\u63a8\u7406\uff08\u8fd0\u52a8\u9884\u6d4b\u3001\u7a7a\u95f4\u5173\u7cfb\uff09\u80fd\u529b\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u8bca\u65ad\u6027\u6027\u80fd\u4e0e\u8bc4\u4f30\u51c6\u786e\u5ea6\u4e4b\u95f4\u7684\u5f31\u76f8\u5173\u6027\uff1a\u5728\u611f\u77e5\u6216\u7269\u7406\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u79c0\u7684\u6a21\u578b\uff0c\u5728\u9884\u6d4b\u6216\u53cd\u4e8b\u5b9e\u8bc4\u4f30\u4e2d\u5e76\u672a\u6301\u7eed\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u8fd9\u4e00\u53cd\u76f4\u89c9\u7684\u5dee\u8ddd\u63ed\u793a\u4e86\u5f53\u524dVLM\u7684\u6838\u5fc3\u5c40\u9650\u6027\uff1a\u611f\u77e5\u548c\u7269\u7406\u6280\u80fd\u4ecd\u7136\u662f\u788e\u7247\u5316\u7684\uff0c\u672a\u80fd\u7ed3\u5408\u6210\u56e0\u679c\u7406\u89e3\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u66f4\u7d27\u5bc6\u7ed3\u5408\u611f\u77e5\u548c\u63a8\u7406\u7684\u67b6\u6784\u3002"}}
{"id": "2510.06293", "pdf": "https://arxiv.org/pdf/2510.06293", "abs": "https://arxiv.org/abs/2510.06293", "authors": ["Cristian Meo", "Varun Sarathchandran", "Avijit Majhi", "Shao Hung", "Carlo Saccardi", "Ruben Imhoff", "Roberto Deidda", "Remko Uijlenhoet", "Justin Dauwels"], "title": "BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Predicting precipitation maps is a highly complex spatiotemporal modeling\ntask, critical for mitigating the impacts of extreme weather events. Short-term\nprecipitation forecasting, or nowcasting, requires models that are not only\naccurate but also computationally efficient for real-time applications. Current\nmethods, such as token-based autoregressive models, often suffer from flawed\ninductive biases and slow inference, while diffusion models can be\ncomputationally intensive. To address these limitations, we introduce BlockGPT,\na generative autoregressive transformer using batched tokenization (Block)\nmethod that predicts full two-dimensional fields (frames) at each time step.\nConceived as a model-agnostic paradigm for video prediction, BlockGPT\nfactorizes space-time by using self-attention within each frame and causal\nattention across frames; in this work, we instantiate it for precipitation\nnowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI\n(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines\nincluding token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)\nmodels. The results show that BlockGPT achieves superior accuracy, event\nlocalization as measured by categorical metrics, and inference speeds up to 31x\nfaster than comparable baselines.", "AI": {"tldr": "BlockGPT\u662f\u4e00\u79cd\u65b0\u578b\u751f\u6210\u5f0f\u81ea\u56de\u5f52Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u6279\u5904\u7406\u5206\u8bcd\u65b9\u6cd5\u9884\u6d4b\u5b8c\u6574\u7684\u4e8c\u7ef4\u964d\u6c34\u573a\uff0c\u5b9e\u73b0\u4e86\u5728\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u4e2d\u5353\u8d8a\u7684\u51c6\u786e\u6027\u548c\u9ad8\u8fbe31\u500d\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u77ed\u671f\u964d\u6c34\u9884\u62a5\uff08\u4e34\u8fd1\u9884\u62a5\uff09\u5bf9\u51cf\u8f7b\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u5f71\u54cd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u57fa\u4e8etoken\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff09\u5b58\u5728\u5f52\u7eb3\u504f\u5dee\u548c\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u800c\u6269\u6563\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u5bf9\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u7684\u8981\u6c42\u3002", "method": "\u672c\u6587\u63d0\u51faBlockGPT\uff0c\u4e00\u79cd\u751f\u6210\u5f0f\u81ea\u56de\u5f52Transformer\u6a21\u578b\uff0c\u5229\u7528\u6279\u5904\u7406\u5206\u8bcd\uff08Block\uff09\u65b9\u6cd5\u5728\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9884\u6d4b\u5b8c\u6574\u7684\u4e8c\u7ef4\u573a\uff08\u5e27\uff09\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u5e27\u5185\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u8de8\u5e27\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u6765\u5206\u89e3\u65f6\u7a7a\u3002\u7814\u7a76\u5c06BlockGPT\u5b9e\u4f8b\u5316\u7528\u4e8e\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\uff0c\u5e76\u5728KNMI\uff08\u8377\u5170\uff09\u548cSEVIR\uff08\u7f8e\u56fd\uff09\u4e24\u4e2a\u964d\u6c34\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4e0eNowcastingGPT\uff08\u57fa\u4e8etoken\uff09\u548cDiffCast+Phydnet\uff08\u57fa\u4e8e\u6269\u6563\uff09\u7b49\u73b0\u6709SOTA\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "BlockGPT\u5728KNMI\u548cSEVIR\u6570\u636e\u96c6\u4e0a\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\u76f8\u6bd4\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u51c6\u786e\u6027\u3001\u4e8b\u4ef6\u5b9a\u4f4d\u80fd\u529b\uff08\u901a\u8fc7\u5206\u7c7b\u6307\u6807\u8861\u91cf\uff09\uff0c\u5e76\u4e14\u63a8\u7406\u901f\u5ea6\u6bd4\u53ef\u6bd4\u57fa\u7ebf\u6a21\u578b\u5feb31\u500d\u3002", "conclusion": "BlockGPT\u6709\u6548\u514b\u670d\u4e86\u73b0\u6709\u964d\u6c34\u4e34\u8fd1\u9884\u62a5\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6027\u80fd\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u5747\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002"}}
{"id": "2510.06250", "pdf": "https://arxiv.org/pdf/2510.06250", "abs": "https://arxiv.org/abs/2510.06250", "authors": ["Bharti Meena", "Joanna Skubisz", "Harshit Rajgarhia", "Nand Dave", "Kiran Ganesh", "Shivali Dalmia", "Abhishek Mukherji", "Vasudevan Sundarababu", "Olga Pospelova"], "title": "Scalable multilingual PII annotation for responsible AI in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) gain wider adoption, ensuring their reliable\nhandling of Personally Identifiable Information (PII) across diverse regulatory\ncontexts has become essential. This work introduces a scalable multilingual\ndata curation framework designed for high-quality PII annotation across 13\nunderrepresented locales, covering approximately 336 locale-specific PII types.\nOur phased, human-in-the-loop annotation methodology combines linguistic\nexpertise with rigorous quality assurance, leading to substantial improvements\nin recall and false positive rates from pilot, training, and production phases.\nBy leveraging inter-annotator agreement metrics and root-cause analysis, the\nframework systematically uncovers and resolves annotation inconsistencies,\nresulting in high-fidelity datasets suitable for supervised LLM fine-tuning.\nBeyond reporting empirical gains, we highlight common annotator challenges in\nmultilingual PII labeling and demonstrate how iterative, analytics-driven\npipelines can enhance both annotation quality and downstream model reliability.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u591a\u8bed\u8a00PII\u6570\u636e\u6807\u6ce8\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u548c\u8fed\u4ee3\u5206\u6790\uff0c\u63d0\u5347\u4e86LLM\u5904\u7406\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5b83\u4eec\u5728\u4e0d\u540c\u6cd5\u89c4\u73af\u5883\u4e0b\u53ef\u9760\u5904\u7406\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\uff08PII\uff09\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u591a\u8bed\u8a00\u6570\u636e\u6807\u6ce8\u6846\u67b6\uff0c\u65e8\u5728\u4e3a13\u4e2a\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u533a\u57df\uff08\u6db5\u76d6\u7ea6336\u79cd\u672c\u5730\u5316PII\u7c7b\u578b\uff09\u63d0\u4f9b\u9ad8\u8d28\u91cfPII\u6807\u6ce8\u3002\u91c7\u7528\u5206\u9636\u6bb5\u3001\u4eba\u673a\u534f\u4f5c\u7684\u6807\u6ce8\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bed\u8a00\u4e13\u5bb6\u77e5\u8bc6\u4e0e\u4e25\u683c\u8d28\u68c0\uff0c\u5e76\u901a\u8fc7\u6807\u6ce8\u8005\u4e00\u81f4\u6027\u6307\u6807\u548c\u6839\u56e0\u5206\u6790\u6765\u7cfb\u7edf\u6027\u5730\u53d1\u73b0\u548c\u89e3\u51b3\u6807\u6ce8\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "result": "\u6807\u6ce8\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff0c\u53ec\u56de\u7387\u548c\u8bef\u62a5\u7387\u5728\u8bd5\u70b9\u3001\u8bad\u7ec3\u548c\u751f\u4ea7\u9636\u6bb5\u5747\u5f97\u5230\u5b9e\u8d28\u6027\u6539\u5584\u3002\u6210\u529f\u751f\u6210\u4e86\u9002\u7528\u4e8e\u76d1\u7763\u5f0fLLM\u5fae\u8c03\u7684\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\u3002\u540c\u65f6\uff0c\u4e5f\u63ed\u793a\u4e86\u591a\u8bed\u8a00PII\u6807\u6ce8\u4e2d\u7684\u5e38\u89c1\u6311\u6218\u3002", "conclusion": "\u8fed\u4ee3\u7684\u3001\u5206\u6790\u9a71\u52a8\u7684\u6807\u6ce8\u6d41\u7a0b\u80fd\u591f\u6709\u6548\u63d0\u5347\u6807\u6ce8\u8d28\u91cf\u53ca\u4e0b\u6e38\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2510.06433", "pdf": "https://arxiv.org/pdf/2510.06433", "abs": "https://arxiv.org/abs/2510.06433", "authors": ["Aryan Singh Dalal", "Yinglun Zhang", "Duru Do\u011fan", "Atalay Mert \u0130leri", "Hande K\u00fc\u00e7\u00fck McGinty"], "title": "Flavonoid Fusion: Creating a Knowledge Graph to Unveil the Interplay Between Food and Health", "categories": ["cs.AI"], "comment": null, "summary": "The focus on \"food as medicine\" is gaining traction in the field of health\nand several studies conducted in the past few years discussed this aspect of\nfood in the literature. However, very little research has been done on\nrepresenting the relationship between food and health in a standardized,\nmachine-readable format using a semantic web that can help us leverage this\nknowledge effectively. To address this gap, this study aims to create a\nknowledge graph to link food and health through the knowledge graph's ability\nto combine information from various platforms focusing on flavonoid contents of\nfood found in the USDA databases and cancer connections found in the\nliterature. We looked closely at these relationships using KNARM methodology\nand represented them in machine-operable format. The proposed knowledge graph\nserves as an example for researchers, enabling them to explore the complex\ninterplay between dietary choices and disease management. Future work for this\nstudy involves expanding the scope of the knowledge graph by capturing nuances,\nadding more related data, and performing inferences on the acquired knowledge\nto uncover hidden relationships.", "AI": {"tldr": "\u672c\u7814\u7a76\u521b\u5efa\u4e86\u4e00\u4e2a\u77e5\u8bc6\u56fe\u8c31\uff0c\u5c06\u98df\u7269\uff08\u5c24\u5176\u662f\u9ec4\u916e\u7c7b\u5316\u5408\u7269\uff09\u4e0e\u5065\u5eb7\uff08\u764c\u75c7\u5173\u8054\uff09\u8054\u7cfb\u8d77\u6765\uff0c\u4ee5\u6807\u51c6\u5316\u3001\u673a\u5668\u53ef\u8bfb\u7684\u683c\u5f0f\u5448\u73b0\u98df\u7269\u4e0e\u5065\u5eb7\u7684\u5173\u7cfb\u3002", "motivation": "\u5c3d\u7ba1\u201c\u98df\u7269\u5373\u836f\u7269\u201d\u7684\u7406\u5ff5\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5c06\u98df\u7269\u4e0e\u5065\u5eb7\u5173\u7cfb\u4ee5\u6807\u51c6\u5316\u3001\u673a\u5668\u53ef\u8bfb\u7684\u8bed\u4e49\u7f51\u7edc\u683c\u5f0f\u8868\u793a\u7684\u7814\u7a76\uff0c\u4ee5\u6709\u6548\u5229\u7528\u73b0\u6709\u77e5\u8bc6\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u7ed3\u5408USDA\u6570\u636e\u5e93\u4e2d\u7684\u98df\u7269\u9ec4\u916e\u7c7b\u5316\u5408\u7269\u542b\u91cf\u548c\u6587\u732e\u4e2d\u7684\u764c\u75c7\u5173\u8054\u4fe1\u606f\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u77e5\u8bc6\u56fe\u8c31\u3002\u7814\u7a76\u91c7\u7528\u4e86KNARM\u65b9\u6cd5\u8bba\uff0c\u4ee5\u673a\u5668\u53ef\u64cd\u4f5c\u7684\u683c\u5f0f\u8868\u793a\u8fd9\u4e9b\u5173\u7cfb\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u98df\u7269\u4e0e\u5065\u5eb7\uff08\u98df\u7269\u4e2d\u7684\u9ec4\u916e\u7c7b\u5316\u5408\u7269\u4e0e\u764c\u75c7\uff09\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u4ee5\u673a\u5668\u53ef\u64cd\u4f5c\u7684\u683c\u5f0f\u5448\u73b0\u3002\u8be5\u56fe\u8c31\u4e3a\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u996e\u98df\u9009\u62e9\u4e0e\u75be\u75c5\u7ba1\u7406\u4e4b\u95f4\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u8303\u4f8b\u3002", "conclusion": "\u672c\u7814\u7a76\u5f25\u8865\u4e86\u98df\u7269\u4e0e\u5065\u5eb7\u5173\u7cfb\u6807\u51c6\u5316\u3001\u673a\u5668\u53ef\u8bfb\u8868\u793a\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5de5\u5177\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u66f4\u597d\u5730\u7406\u89e3\u996e\u98df\u4e0e\u75be\u75c5\u4e4b\u95f4\u7684\u8054\u7cfb\u3002"}}
{"id": "2510.06254", "pdf": "https://arxiv.org/pdf/2510.06254", "abs": "https://arxiv.org/abs/2510.06254", "authors": ["Xiaochen Zhao", "Chengting Yu", "Kairong Yu", "Lei Liu", "Aili Wang"], "title": "Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training", "categories": ["cs.CV"], "comment": null, "summary": "Spiking Neural Networks (SNNs) exhibit exceptional energy efficiency on\nneuromorphic hardware due to their sparse activation patterns. However,\nconventional training methods based on surrogate gradients and Backpropagation\nThrough Time (BPTT) not only lag behind Artificial Neural Networks (ANNs) in\nperformance, but also incur significant computational and memory overheads that\ngrow linearly with the temporal dimension. To enable high-performance SNN\ntraining under limited computational resources, we propose an enhanced\nself-distillation framework, jointly optimized with rate-based backpropagation.\nSpecifically, the firing rates of intermediate SNN layers are projected onto\nlightweight ANN branches, and high-quality knowledge generated by the model\nitself is used to optimize substructures through the ANN pathways. Unlike\ntraditional self-distillation paradigms, we observe that low-quality\nself-generated knowledge may hinder convergence. To address this, we decouple\nthe teacher signal into reliable and unreliable components, ensuring that only\nreliable knowledge is used to guide the optimization of the model. Extensive\nexperiments on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate that\nour method reduces training complexity while achieving high-performance SNN\ntraining. Our code is available at\nhttps://github.com/Intelli-Chip-Lab/enhanced-self-distillation-framework-for-snn.", "AI": {"tldr": "\u9488\u5bf9SNN\u4f20\u7edf\u8bad\u7ec3\u65b9\u6cd5\u6027\u80fd\u4e0d\u8db3\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u901f\u7387\u53cd\u5411\u4f20\u64ad\u7684\u589e\u5f3a\u578b\u81ea\u84b8\u998f\u6846\u67b6\u3002\u901a\u8fc7\u5c06SNN\u5c42\u53d1\u653e\u7387\u6620\u5c04\u5230ANN\u5206\u652f\u5e76\u89e3\u8026\u6559\u5e08\u4fe1\u53f7\u4ee5\u786e\u4fdd\u53ea\u4f7f\u7528\u53ef\u9760\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u5728\u964d\u4f4e\u8bad\u7ec3\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u9ad8\u6027\u80fdSNN\u7684\u8bad\u7ec3\u3002", "motivation": "SNN\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\u80fd\u6548\u9ad8\uff0c\u4f46\u4f20\u7edf\u8bad\u7ec3\u65b9\u6cd5\uff08\u57fa\u4e8e\u66ff\u4ee3\u68af\u5ea6\u548cBPTT\uff09\u4e0d\u4ec5\u6027\u80fd\u843d\u540e\u4e8eANN\uff0c\u800c\u4e14\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u968f\u65f6\u95f4\u7ef4\u5ea6\u7ebf\u6027\u589e\u957f\u3002\u7814\u7a76\u52a8\u673a\u662f\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fdSNN\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578b\u81ea\u84b8\u998f\u6846\u67b6\uff0c\u4e0e\u57fa\u4e8e\u901f\u7387\u7684\u53cd\u5411\u4f20\u64ad\u8054\u5408\u4f18\u5316\u3002\u5177\u4f53\u5730\uff0c\u5c06SNN\u4e2d\u95f4\u5c42\u7684\u8109\u51b2\u53d1\u653e\u7387\u6295\u5f71\u5230\u8f7b\u91cf\u7ea7ANN\u5206\u652f\uff0c\u5e76\u5229\u7528\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u77e5\u8bc6\u901a\u8fc7ANN\u8def\u5f84\u4f18\u5316\u5b50\u7ed3\u6784\u3002\u4e3a\u907f\u514d\u4f4e\u8d28\u91cf\u81ea\u751f\u6210\u77e5\u8bc6\u963b\u788d\u6536\u655b\uff0c\u5c06\u6559\u5e08\u4fe1\u53f7\u89e3\u8026\u4e3a\u53ef\u9760\u548c\u4e0d\u53ef\u9760\u4e24\u90e8\u5206\uff0c\u4ec5\u4f7f\u7528\u53ef\u9760\u77e5\u8bc6\u6307\u5bfc\u6a21\u578b\u4f18\u5316\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u3001CIFAR10-DVS\u548cImageNet\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u8bad\u7ec3\u590d\u6742\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fdSNN\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u51fa\u589e\u5f3a\u578b\u81ea\u84b8\u998f\u6846\u67b6\u5e76\u7ed3\u5408\u53ef\u9760\u77e5\u8bc6\u89e3\u8026\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfSNN\u8bad\u7ec3\u7684\u6027\u80fd\u74f6\u9888\u548c\u8d44\u6e90\u6d88\u8017\u95ee\u9898\uff0c\u4e3a\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fdSNN\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2510.06303", "pdf": "https://arxiv.org/pdf/2510.06303", "abs": "https://arxiv.org/abs/2510.06303", "authors": ["Shuang Cheng", "Yihan Bian", "Dawei Liu", "Yuhua Jiang", "Yihao Liu", "Linfeng Zhang", "Wenhai Wang", "Qipeng Guo", "Kai Chen", "Biqing Qi", "Bowen Zhou"], "title": "SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation", "categories": ["cs.LG", "cs.AI"], "comment": "Technical report. 39 pages, including 14 pages of appendix", "summary": "We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies\nthe training efficiency of autoregressive models with the parallel inference\ncapability of diffusion. Instead of costly end-to-end diffusion training, SDAR\nperforms a lightweight paradigm conversion that transforms a well-trained\nautoregressive (AR) model into a blockwise diffusion model through brief,\ndata-efficient adaptation. During inference, SDAR generates sequences\nautoregressively across blocks for global coherence while decoding all tokens\nwithin each block in parallel via a discrete diffusion process. Extensive\nexperiments show that AR models remain substantially more compute-efficient\nthan masked diffusion models, providing a strong foundation for adaptation.\nBuilding on this insight, SDAR achieves efficient AR-to-diffusion conversion\nwith minimal cost, preserving AR-level performance while enabling parallel\ngeneration. Scaling studies across dense and Mixture-of-Experts architectures\nconfirm that SDAR scales without compromise: larger models exhibit stronger\nrobustness to block size and decoding thresholds, yielding greater speedups\nwithout accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning\nand domain adaptability. Our 30B MoE model surpasses its AR counterpart on\nchallenging scientific reasoning benchmarks such as GPQA and ChemBench, and\ngains further improvements under test-time scaling methods like majority voting\nand pass@k. Together, these results establish SDAR as a practical paradigm that\ncombines the strengths of autoregression and diffusion for scalable,\nhigh-throughput reasoning.", "AI": {"tldr": "SDAR\u662f\u4e00\u79cd\u534f\u540c\u6269\u6563-\u81ea\u56de\u5f52\u8303\u5f0f\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8f6c\u6362\u5c06\u9884\u8bad\u7ec3\u81ea\u56de\u5f52\u6a21\u578b\u53d8\u4e3a\u5206\u5757\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u4e0e\u6269\u6563\u6a21\u578b\u7684\u5e76\u884c\u63a8\u7406\u80fd\u529b\u7ed3\u5408\uff0c\u5e76\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u901f\u5ea6\u3001\u63a8\u7406\u80fd\u529b\u548c\u9886\u57df\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u9ad8\u4f46\u63a8\u7406\u5e76\u884c\u6027\u5dee\uff0c\u800c\u6269\u6563\u6a21\u578b\u867d\u80fd\u5e76\u884c\u63a8\u7406\u4f46\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\u3002\u7814\u7a76\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u5b9e\u73b0\u65e2\u9ad8\u6548\u8bad\u7ec3\u53c8\u80fd\u5e76\u884c\u63a8\u7406\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faSDAR\u8303\u5f0f\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u3001\u6570\u636e\u9ad8\u6548\u7684\u9002\u5e94\u6027\u8f6c\u6362\uff0c\u5c06\u5df2\u8bad\u7ec3\u7684\u81ea\u56de\u5f52\u6a21\u578b\u8f6c\u5316\u4e3a\u5206\u5757\u6269\u6563\u6a21\u578b\u3002\u63a8\u7406\u65f6\uff0c\u6a21\u578b\u5728\u5757\u95f4\u8fdb\u884c\u81ea\u56de\u5f52\u751f\u6210\u4ee5\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5728\u6bcf\u4e2a\u5757\u5185\u901a\u8fc7\u79bb\u6563\u6269\u6563\u8fc7\u7a0b\u5e76\u884c\u89e3\u7801\u6240\u6709token\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u81ea\u56de\u5f52\u6a21\u578b\u6bd4\u63a9\u7801\u6269\u6563\u6a21\u578b\u66f4\u8ba1\u7b97\u9ad8\u6548\u3002SDAR\u4ee5\u6781\u5c0f\u6210\u672c\u5b9e\u73b0AR\u5230\u6269\u6563\u7684\u8f6c\u6362\uff0c\u4fdd\u6301\u4e86AR\u7ea7\u522b\u7684\u6027\u80fd\u5e76\u5b9e\u73b0\u4e86\u5e76\u884c\u751f\u6210\u3002\u5927\u89c4\u6a21\u6a21\u578b\u7814\u7a76\u8bc1\u5b9eSDAR\u53ef\u6269\u5c55\uff0c\u5927\u578b\u6a21\u578b\u5bf9\u5757\u5927\u5c0f\u548c\u89e3\u7801\u9608\u503c\u66f4\u9c81\u68d2\uff0c\u5b9e\u73b0\u66f4\u9ad8\u52a0\u901f\u6bd4\u800c\u4e0d\u635f\u5931\u7cbe\u5ea6\u3002SDAR\u8fd8\u589e\u5f3a\u4e86\u63a8\u7406\u548c\u9886\u57df\u9002\u5e94\u6027\uff0c\u517630B MoE\u6a21\u578b\u5728\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\uff08\u5982GPQA\u548cChemBench\uff09\u4e0a\u8d85\u8d8a\u4e86\u5176AR\u5bf9\u5e94\u6a21\u578b\uff0c\u5e76\u5728\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u4e0b\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "SDAR\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u8303\u5f0f\uff0c\u6210\u529f\u7ed3\u5408\u4e86\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u9ad8\u541e\u5410\u91cf\u7684\u63a8\u7406\uff0c\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u5728\u63a8\u7406\u548c\u591a\u9886\u57df\u9002\u5e94\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002"}}
{"id": "2510.06262", "pdf": "https://arxiv.org/pdf/2510.06262", "abs": "https://arxiv.org/abs/2510.06262", "authors": ["Aryan Kumar Singh", "Janvi Singh"], "title": "Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "4 pages, 4 figures", "summary": "This dataset provides responses to a standardized, bilingual (English-Hindi)\nPrakriti Assessment Questionnaire designed to evaluate the physical,\nphysiological, and psychological characteristics of individuals according to\nclassical Ayurvedic principles. The questionnaire consists of 24\nmultiple-choice items covering body features, appetite, sleep patterns, energy\nlevels, and temperament. It was developed following AYUSH/CCRAS guidelines to\nensure comprehensive and accurate data collection. All questions are mandatory\nand neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha)\nare hidden from participants. Data were collected via a Google Forms\ndeployment, enabling automated scoring of responses to map individual traits to\ndosha-specific scores. The resulting dataset provides a structured platform for\nresearch in computational intelligence, Ayurvedic studies, and personalized\nhealth analytics, supporting analysis of trait distributions, correlations, and\npredictive modeling. It can also serve as a reference for future Prakriti-based\nstudies and the development of intelligent health applications.", "AI": {"tldr": "\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u3001\u53cc\u8bed\u7684\uff08\u82f1\u8bed-\u5370\u5730\u8bed\uff09Prakriti\u8bc4\u4f30\u95ee\u5377\u7684\u54cd\u5e94\u6570\u636e\uff0c\u7528\u4e8e\u6839\u636e\u963f\u80b2\u5420\u9640\u539f\u5219\u8bc4\u4f30\u4e2a\u4f53\u7684\u8eab\u4f53\u3001\u751f\u7406\u548c\u5fc3\u7406\u7279\u5f81\u3002", "motivation": "\u6839\u636e\u7ecf\u5178\u7684\u963f\u80b2\u5420\u9640\u539f\u7406\uff0c\u8bc4\u4f30\u4e2a\u4f53\u7684\u8eab\u4f53\u3001\u751f\u7406\u548c\u5fc3\u7406\u7279\u5f81\uff0c\u5e76\u4e3a\u8ba1\u7b97\u667a\u80fd\u3001\u963f\u80b2\u5420\u9640\u7814\u7a76\u548c\u4e2a\u6027\u5316\u5065\u5eb7\u5206\u6790\u63d0\u4f9b\u7ed3\u6784\u5316\u6570\u636e\u5e73\u53f0\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b24\u4e2a\u591a\u9009\u9898\u7684\u6807\u51c6\u5316\u53cc\u8bed\uff08\u82f1\u8bed-\u5370\u5730\u8bed\uff09Prakriti\u8bc4\u4f30\u95ee\u5377\uff0c\u9075\u5faaAYUSH/CCRAS\u6307\u5357\u3002\u95ee\u5377\u901a\u8fc7Google Forms\u90e8\u7f72\uff0c\u5e76\u9690\u85cfDosha\u6807\u7b7e\u4ee5\u51cf\u5c11\u504f\u89c1\uff0c\u5b9e\u73b0\u54cd\u5e94\u7684\u81ea\u52a8\u8bc4\u5206\uff0c\u4ee5\u5c06\u4e2a\u4f53\u7279\u8d28\u6620\u5c04\u5230\u7279\u5b9aDosha\u5f97\u5206\u3002", "result": "\u751f\u6210\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4e2a\u4f53\u7279\u8d28\u5230Dosha\uff08Vata, Pitta, Kapha\uff09\u7279\u5b9a\u5206\u6570\u7684\u6620\u5c04\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u8ba1\u7b97\u667a\u80fd\u3001\u963f\u80b2\u5420\u9640\u7814\u7a76\u548c\u4e2a\u6027\u5316\u5065\u5eb7\u5206\u6790\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5e73\u53f0\uff0c\u652f\u6301\u7279\u8d28\u5206\u5e03\u3001\u76f8\u5173\u6027\u548c\u9884\u6d4b\u6a21\u578b\u7684\u5206\u6790\uff0c\u5e76\u53ef\u4f5c\u4e3a\u672a\u6765Prakriti\u76f8\u5173\u7814\u7a76\u548c\u667a\u80fd\u5065\u5eb7\u5e94\u7528\u5f00\u53d1\u7684\u53c2\u8003\u3002"}}
{"id": "2510.06475", "pdf": "https://arxiv.org/pdf/2510.06475", "abs": "https://arxiv.org/abs/2510.06475", "authors": ["Yitao Long", "Yuru Jiang", "Hongjun Liu", "Yilun Zhao", "Jingchen Sun", "Yiqiu Shen", "Chen Zhao", "Arman Cohan", "Dennis Shasha"], "title": "PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "This work investigates the reasoning and planning capabilities of foundation\nmodels and their scalability in complex, dynamic environments. We introduce\nPuzzlePlex, a benchmark designed to assess these capabilities through a diverse\nset of puzzles. PuzzlePlex consists of 15 types of puzzles, including\ndeterministic and stochastic games of varying difficulty, as well as\nsingle-player and two-player scenarios. The PuzzlePlex framework provides a\ncomprehensive environment for each game, and supports extensibility to generate\nmore challenging instances as foundation models evolve. Additionally, we\nimplement customized game-playing strategies for comparison. Building on this\nbenchmark, we develop fine-grained metrics to measure performance and conduct\nan in-depth analysis of frontier foundation models across two settings:\ninstruction-based and code-based. Furthermore, we systematically investigate\ntheir scaling limits. Our findings show that reasoning models outperform others\nin instruction-based settings, while code-based execution presents greater\nchallenges but offers a scalable and efficient alternative. PuzzlePlex enables\ntargeted evaluation and guides future improvements in reasoning, planning, and\ngeneralization for foundation models.", "AI": {"tldr": "\u672c\u6587\u5f15\u5165\u4e86PuzzlePlex\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\uff0c\u5e76\u5206\u6790\u4e86\u5176\u6027\u80fd\u548c\u6269\u5c55\u6027\u3002", "motivation": "\u7814\u7a76\u57fa\u7840\u6a21\u578b\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u63a8\u7406\u3001\u89c4\u5212\u80fd\u529b\u53ca\u5176\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5f15\u5165\u4e86\u5305\u542b15\u79cd\u8c1c\u9898\u7c7b\u578b\u7684PuzzlePlex\u57fa\u51c6\uff1b\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6e38\u620f\u73af\u5883\u5e76\u652f\u6301\u6269\u5c55\u6027\uff1b\u5b9e\u73b0\u4e86\u5b9a\u5236\u5316\u7684\u6e38\u620f\u7b56\u7565\u8fdb\u884c\u6bd4\u8f83\uff1b\u5f00\u53d1\u4e86\u7ec6\u7c92\u5ea6\u6307\u6807\uff1b\u5728\u57fa\u4e8e\u6307\u4ee4\u548c\u57fa\u4e8e\u4ee3\u7801\u7684\u8bbe\u7f6e\u4e2d\u6df1\u5165\u5206\u6790\u4e86\u524d\u6cbf\u57fa\u7840\u6a21\u578b\uff1b\u7cfb\u7edf\u6027\u5730\u8c03\u67e5\u4e86\u5176\u6269\u5c55\u9650\u5236\u3002", "result": "\u63a8\u7406\u6a21\u578b\u5728\u57fa\u4e8e\u6307\u4ee4\u7684\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff1b\u57fa\u4e8e\u4ee3\u7801\u7684\u6267\u884c\u5e26\u6765\u4e86\u66f4\u5927\u6311\u6218\uff0c\u4f46\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "PuzzlePlex\u652f\u6301\u6709\u9488\u5bf9\u6027\u7684\u8bc4\u4f30\uff0c\u5e76\u6307\u5bfc\u672a\u6765\u57fa\u7840\u6a21\u578b\u5728\u63a8\u7406\u3001\u89c4\u5212\u548c\u6cdb\u5316\u65b9\u9762\u7684\u6539\u8fdb\u3002"}}
{"id": "2510.06260", "pdf": "https://arxiv.org/pdf/2510.06260", "abs": "https://arxiv.org/abs/2510.06260", "authors": ["Sher Khan", "Raz Muhammad", "Adil Hussain", "Muhammad Sajjad", "Muhammad Rashid"], "title": "Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Cutaneous malignancies demand early detection for favorable outcomes, yet\ncurrent diagnostics suffer from inter-observer variability and access\ndisparities. While AI shows promise, existing dermatological systems are\nlimited by homogeneous architectures, dataset biases across skin tones, and\nfragmented approaches that treat natural language processing as separate\npost-hoc explanations rather than integral to clinical decision-making. We\nintroduce a unified framework that fundamentally reimagines AI integration for\ndermatological diagnostics through two synergistic innovations. First, a\npurposefully heterogeneous ensemble of architecturally diverse convolutional\nneural networks provides complementary diagnostic perspectives, with an\nintrinsic uncertainty mechanism flagging discordant cases for specialist review\n-- mimicking clinical best practices. Second, we embed large language model\ncapabilities directly into the diagnostic workflow, transforming classification\noutputs into clinically meaningful assessments that simultaneously fulfill\nmedical documentation requirements and deliver patient-centered education. This\nseamless integration generates structured reports featuring precise lesion\ncharacterization, accessible diagnostic reasoning, and actionable monitoring\nguidance -- empowering patients to recognize early warning signs between\nvisits. By addressing both diagnostic reliability and communication barriers\nwithin a single cohesive system, our approach bridges the critical\ntranslational gap that has prevented previous AI implementations from achieving\nclinical impact. The framework represents a significant advancement toward\ndeployable dermatological AI that enhances diagnostic precision while actively\nsupporting the continuum of care from initial detection through patient\neducation, ultimately improving early intervention rates for skin lesions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684AI\u6846\u67b6\uff0c\u7528\u4e8e\u76ae\u80a4\u75c5\u8bca\u65ad\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5f02\u6784\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u8bca\u65ad\u80fd\u529b\uff08\u5305\u542b\u4e0d\u786e\u5b9a\u6027\u673a\u5236\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4e34\u5e8a\u6c9f\u901a\u4e0e\u6559\u80b2\u529f\u80fd\uff0c\u65e8\u5728\u63d0\u9ad8\u8bca\u65ad\u7cbe\u5ea6\u3001\u51cf\u5c11\u8bef\u8bca\uff0c\u5e76\u4f18\u5316\u60a3\u8005\u6559\u80b2\u548c\u533b\u60a3\u6c9f\u901a\uff0c\u586b\u8865\u4e86AI\u5728\u76ae\u80a4\u79d1\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u7a7a\u767d\u3002", "motivation": "\u76ae\u80a4\u6076\u6027\u80bf\u7624\u9700\u65e9\u671f\u53d1\u73b0\uff0c\u4f46\u73b0\u6709\u8bca\u65ad\u65b9\u6cd5\u5b58\u5728\u89c2\u5bdf\u8005\u95f4\u5dee\u5f02\u5927\u3001\u83b7\u53d6\u4e0d\u4fbf\u7b49\u95ee\u9898\u3002\u5f53\u524dAI\u7cfb\u7edf\u53d7\u9650\u4e8e\u67b6\u6784\u540c\u8d28\u6027\u3001\u6570\u636e\u96c6\u80a4\u8272\u504f\u5dee\u53caNLP\u4f5c\u4e3a\u540e\u5904\u7406\u800c\u975e\u6838\u5fc3\u7684\u788e\u7247\u5316\u5e94\u7528\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8bca\u65ad\u53ef\u9760\u6027\u4f4e\u548c\u6c9f\u901a\u969c\u788d\uff0c\u5c06AI\u6df1\u5ea6\u6574\u5408\u5230\u4e34\u5e8a\u51b3\u7b56\u4e2d\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u521b\u65b0\uff1a1. \u4e00\u4e2a\u7531\u67b6\u6784\u591a\u6837\u5316\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7ec4\u6210\u7684\u5f02\u6784\u96c6\u6210\u6a21\u578b\uff0c\u63d0\u4f9b\u4e92\u8865\u7684\u8bca\u65ad\u89c6\u89d2\uff0c\u5e76\u5185\u7f6e\u4e0d\u786e\u5b9a\u6027\u673a\u5236\uff0c\u7528\u4e8e\u6807\u8bb0\u4e0d\u4e00\u81f4\u75c5\u4f8b\u4f9b\u4e13\u5bb6\u5ba1\u67e5\u30022. \u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u76f4\u63a5\u5d4c\u5165\u8bca\u65ad\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5c06\u5206\u7c7b\u8f93\u51fa\u8f6c\u5316\u4e3a\u4e34\u5e8a\u6709\u610f\u4e49\u7684\u8bc4\u4f30\uff0c\u540c\u65f6\u6ee1\u8db3\u533b\u7597\u6587\u4ef6\u8981\u6c42\u5e76\u63d0\u4f9b\u4ee5\u60a3\u8005\u4e3a\u4e2d\u5fc3\u7684\u6559\u80b2\u3002\u6700\u7ec8\u751f\u6210\u7ed3\u6784\u5316\u62a5\u544a\uff0c\u5305\u62ec\u7cbe\u786e\u75c5\u7076\u7279\u5f81\u3001\u53ef\u7406\u89e3\u7684\u8bca\u65ad\u63a8\u7406\u548c\u53ef\u64cd\u4f5c\u7684\u76d1\u6d4b\u6307\u5bfc\u3002", "result": "\u8be5\u6846\u67b6\u751f\u6210\u7ed3\u6784\u5316\u62a5\u544a\uff0c\u7cbe\u786e\u63cf\u8ff0\u75c5\u7076\u3001\u63d0\u4f9b\u6613\u61c2\u7684\u8bca\u65ad\u63a8\u7406\u548c\u53ef\u64cd\u4f5c\u7684\u76d1\u6d4b\u6307\u5bfc\uff0c\u8d4b\u80fd\u60a3\u8005\u8bc6\u522b\u65e9\u671f\u9884\u8b66\u4fe1\u53f7\u3002\u901a\u8fc7\u5728\u4e00\u4e2a\u8fde\u8d2f\u7cfb\u7edf\u4e2d\u89e3\u51b3\u8bca\u65ad\u53ef\u9760\u6027\u548c\u6c9f\u901a\u969c\u788d\uff0c\u5f25\u5408\u4e86\u963b\u788d\u6b64\u524dAI\u5b9e\u65bd\u4ea7\u751f\u4e34\u5e8a\u5f71\u54cd\u7684\u5173\u952e\u8f6c\u5316\u9e3f\u6c9f\u3002\u663e\u8457\u63d0\u5347\u4e86\u90e8\u7f72\u578b\u76ae\u80a4\u75c5AI\u7684\u8bca\u65ad\u7cbe\u5ea6\uff0c\u5e76\u79ef\u6781\u652f\u6301\u4ece\u65e9\u671f\u68c0\u6d4b\u5230\u60a3\u8005\u6559\u80b2\u7684\u8fde\u7eed\u62a4\u7406\uff0c\u6700\u7ec8\u63d0\u9ad8\u4e86\u76ae\u80a4\u75c5\u53d8\u7684\u65e9\u671f\u5e72\u9884\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u662f\u53ef\u90e8\u7f72\u7684\u76ae\u80a4\u75c5AI\u9886\u57df\u7684\u91cd\u5927\u8fdb\u5c55\uff0c\u5b83\u5728\u63d0\u9ad8\u8bca\u65ad\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u79ef\u6781\u652f\u6301\u4ece\u521d\u59cb\u68c0\u6d4b\u5230\u60a3\u8005\u6559\u80b2\u7684\u8fde\u7eed\u62a4\u7406\uff0c\u6700\u7ec8\u6539\u5584\u4e86\u76ae\u80a4\u75c5\u53d8\u7684\u65e9\u671f\u5e72\u9884\u7387\u3002"}}
{"id": "2510.06349", "pdf": "https://arxiv.org/pdf/2510.06349", "abs": "https://arxiv.org/abs/2510.06349", "authors": ["Moein E. Samadi", "Andreas Schuppert"], "title": "Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": null, "summary": "Foundation models have rapidly advanced AI, raising the question of whether\ntheir decisions will ultimately surpass human strategies in real-world domains.\nThe exponential, and possibly super-exponential, pace of AI development makes\nsuch analysis elusive. Nevertheless, many application areas that matter for\ndaily life and society show only modest gains so far; a prominent case is\ndiagnosing and treating dynamically evolving disease in intensive care.\n  The common challenge is adapting complex systems to dynamic environments.\nEffective strategies must optimize outcomes in systems composed of strongly\ninteracting functions while avoiding shared side effects; this requires\nreliable, self-adaptive modeling. These tasks align with building digital twins\nof highly complex systems whose mechanisms are not fully or quantitatively\nunderstood. It is therefore essential to develop methods for self-adapting AI\nmodels with minimal data and limited mechanistic knowledge. As this challenge\nextends beyond medicine, AI should demonstrate clear superiority in these\nsettings before assuming broader decision-making roles.\n  We identify the curse of dimensionality as a fundamental barrier to efficient\nself-adaptation and argue that monolithic foundation models face conceptual\nlimits in overcoming it. As an alternative, we propose a decentralized\narchitecture of interacting small agent networks (SANs). We focus on agents\nrepresenting the specialized substructure of the system, where each agent\ncovers only a subset of the full system functions. Drawing on mathematical\nresults on the learning behavior of SANs and evidence from existing\napplications, we argue that swarm-learning in diverse swarms can enable\nself-adaptive SANs to deliver superior decision-making in dynamic environments\ncompared with monolithic foundation models, though at the cost of reduced\nreproducibility in detail.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u57fa\u7840\u6a21\u578b\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u5e76\u8bba\u8bc1\u4e86\u53bb\u4e2d\u5fc3\u5316\u5c0f\u667a\u80fd\u4f53\u7f51\u7edc\uff08SANs\uff09\u53ca\u5176\u7fa4\u5b66\u4e60\u6a21\u5f0f\uff0c\u8ba4\u4e3a\u5176\u5728\u52a8\u6001\u51b3\u7b56\u4e2d\u4f18\u4e8e\u5355\u4f53\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u4ee3\u4ef7\u662f\u7ec6\u8282\u53ef\u590d\u73b0\u6027\u964d\u4f4e\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u53d6\u5f97\u4e86\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u52a8\u6001\u573a\u666f\uff08\u5982\u91cd\u75c7\u76d1\u62a4\uff09\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\u662f\u5426\u80fd\u6700\u7ec8\u8d85\u8d8a\u4eba\u7c7b\u7b56\u7565\u5b58\u7591\u3002AI\u5728\u8fd9\u4e9b\u5173\u952e\u5e94\u7528\u9886\u57df\u8fdb\u5c55\u6709\u9650\uff0c\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u4f7f\u590d\u6742\u7cfb\u7edf\u9002\u5e94\u52a8\u6001\u73af\u5883\uff0c\u5728\u6570\u636e\u548c\u673a\u5236\u77e5\u8bc6\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u81ea\u6211\u9002\u5e94\u5efa\u6a21\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u7684\u4ea4\u4e92\u5f0f\u5c0f\u667a\u80fd\u4f53\u7f51\u7edc\uff08SANs\uff09\u67b6\u6784\uff0c\u4ee5\u6b64\u514b\u670d\u7ef4\u5ea6\u8bc5\u5492\u3002\u6bcf\u4e2a\u667a\u80fd\u4f53\u8986\u76d6\u7cfb\u7edf\u7684\u4e00\u90e8\u5206\u4e13\u4e1a\u529f\u80fd\u3002\u901a\u8fc7\u591a\u6837\u5316\u7fa4\u4f53\u4e2d\u7684\u7fa4\u5b66\u4e60\uff08swarm-learning\uff09\u6765\u4f7fSANs\u5b9e\u73b0\u81ea\u6211\u9002\u5e94\u3002", "result": "\u57fa\u4e8e\u6570\u5b66\u7406\u8bba\u548c\u73b0\u6709\u5e94\u7528\u8bc1\u636e\uff0c\u5177\u6709\u7fa4\u5b66\u4e60\u80fd\u529b\u7684\u81ea\u9002\u5e94SANs\u80fd\u5728\u52a8\u6001\u73af\u5883\u4e2d\u63d0\u4f9b\u4f18\u4e8e\u5355\u4f53\u57fa\u7840\u6a21\u578b\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u5c3d\u7ba1\u5728\u7ec6\u8282\u53ef\u590d\u73b0\u6027\u65b9\u9762\u6709\u6240\u964d\u4f4e\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u7684SANs\u53ca\u5176\u7fa4\u5b66\u4e60\u8303\u5f0f\u662f\u514b\u670d\u201c\u7ef4\u5ea6\u8bc5\u5492\u201d\u5e76\u8d85\u8d8a\u5355\u4f53\u57fa\u7840\u6a21\u578b\u5728\u52a8\u6001\u590d\u6742\u73af\u5883\u5c40\u9650\u6027\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\u3002AI\u5728\u627f\u62c5\u66f4\u5e7f\u6cdb\u51b3\u7b56\u89d2\u8272\u524d\uff0c\u9700\u5148\u5728\u8fd9\u4e9b\u7279\u5b9a\u8bbe\u7f6e\u4e2d\u8bc1\u660e\u5176\u660e\u786e\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.06263", "pdf": "https://arxiv.org/pdf/2510.06263", "abs": "https://arxiv.org/abs/2510.06263", "authors": ["Jiajun Wu", "Swaleh Zaidi", "Braden Teitge", "Henry Leung", "Jiayu Zhou", "Jessalyn Holodinsky", "Steve Drew"], "title": "Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at the IEEE Annual Congress on Artificial Intelligence of\n  Things (IEEE AIoT) 2025", "summary": "Electronic health records (EHRs) contain extensive unstructured clinical data\nthat can overwhelm emergency physicians trying to identify critical\ninformation. We present a two-stage summarization system that runs entirely on\nembedded devices, enabling offline clinical summarization while preserving\npatient privacy. In our approach, a dual-device architecture first retrieves\nrelevant patient record sections using the Jetson Nano-R (Retrieve), then\ngenerates a structured summary on another Jetson Nano-S (Summarize),\ncommunicating via a lightweight socket link. The summarization output is\ntwo-fold: (1) a fixed-format list of critical findings, and (2) a\ncontext-specific narrative focused on the clinician's query. The retrieval\nstage uses locally stored EHRs, splits long notes into semantically coherent\nsections, and searches for the most relevant sections per query. The generation\nstage uses a locally hosted small language model (SLM) to produce the summary\nfrom the retrieved text, operating within the constraints of two NVIDIA Jetson\ndevices. We first benchmarked six open-source SLMs under 7B parameters to\nidentify viable models. We incorporated an LLM-as-Judge evaluation mechanism to\nassess summary quality in terms of factual accuracy, completeness, and clarity.\nPreliminary results on MIMIC-IV and de-identified real EHRs demonstrate that\nour fully offline system can effectively produce useful summaries in under 30\nseconds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5d4c\u5165\u5f0f\u8bbe\u5907\u7684\u4e24\u9636\u6bb5\u6458\u8981\u7cfb\u7edf\uff0c\u53ef\u5728\u79bb\u7ebf\u73af\u5883\u4e0b\u5904\u7406\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHRs\uff09\uff0c\u4e3a\u6025\u8bca\u533b\u751f\u5feb\u901f\u751f\u6210\u7ed3\u6784\u5316\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u4e34\u5e8a\u6458\u8981\uff0c\u540c\u65f6\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u5927\u91cf\u975e\u7ed3\u6784\u5316\u4e34\u5e8a\u6570\u636e\u4f7f\u6025\u8bca\u533b\u751f\u96be\u4ee5\u5feb\u901f\u8bc6\u522b\u5173\u952e\u4fe1\u606f\u3002\u73b0\u6709\u7cfb\u7edf\u53ef\u80fd\u65e0\u6cd5\u6ee1\u8db3\u9690\u79c1\u4fdd\u62a4\u548c\u79bb\u7ebf\u64cd\u4f5c\u7684\u9700\u6c42\uff0c\u5bfc\u81f4\u533b\u751f\u5de5\u4f5c\u8d1f\u62c5\u8fc7\u91cd\u3002", "method": "\u8be5\u7cfb\u7edf\u91c7\u7528\u53cc\u8bbe\u5907\u67b6\u6784\uff1aJetson Nano-R\u8d1f\u8d23\u68c0\u7d22\uff08Retrieve\uff09\u76f8\u5173\u75c5\u5386\u7247\u6bb5\uff0cJetson Nano-S\u8d1f\u8d23\u751f\u6210\uff08Summarize\uff09\u6458\u8981\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5957\u63a5\u5b57\u901a\u4fe1\u3002\u68c0\u7d22\u9636\u6bb5\u5904\u7406\u672c\u5730EHRs\uff0c\u5c06\u957f\u7b14\u8bb0\u5206\u5272\u5e76\u641c\u7d22\u6700\u76f8\u5173\u90e8\u5206\uff1b\u751f\u6210\u9636\u6bb5\u4f7f\u7528\u672c\u5730\u6258\u7ba1\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u751f\u6210\u6458\u8981\uff0c\u5305\u62ec\u56fa\u5b9a\u683c\u5f0f\u7684\u5173\u952e\u53d1\u73b0\u5217\u8868\u548c\u57fa\u4e8e\u67e5\u8be2\u7684\u53d9\u8ff0\u3002\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e86\u516d\u4e2a7B\u53c2\u6570\u4ee5\u4e0b\u7684\u5f00\u6e90SLM\uff0c\u5e76\u4f7f\u7528LLM-as-Judge\u673a\u5236\u8bc4\u4f30\u6458\u8981\u7684\u51c6\u786e\u6027\u3001\u5b8c\u6574\u6027\u548c\u6e05\u6670\u5ea6\u3002", "result": "\u5728MIMIC-IV\u548c\u53bb\u8bc6\u522b\u5316\u7684\u771f\u5b9eEHRs\u4e0a\u7684\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u5b8c\u5168\u79bb\u7ebf\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u751f\u6210\u6709\u7528\u7684\u6458\u8981\uff0c\u4e14\u572830\u79d2\u5185\u5b8c\u6210\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u79bb\u7ebf\u3001\u5d4c\u5165\u5f0f\u6458\u8981\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u3001\u51c6\u786e\u5730\u4e3a\u6025\u8bca\u533b\u751f\u63d0\u4f9b\u5173\u952e\u4e34\u5e8a\u4fe1\u606f\uff0c\u540c\u65f6\u786e\u4fdd\u60a3\u8005\u9690\u79c1\uff0c\u663e\u8457\u63d0\u5347\u4e86EHR\u6570\u636e\u5229\u7528\u6548\u7387\u3002"}}
{"id": "2510.06534", "pdf": "https://arxiv.org/pdf/2510.06534", "abs": "https://arxiv.org/abs/2510.06534", "authors": ["Jiahe Jin", "Abhijay Paladugu", "Chenyan Xiong"], "title": "Beneficial Reasoning Behaviors in Agentic Search and Effective Post-training to Obtain Them", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Agentic search leverages large language models (LLMs) to interpret complex\nuser information needs and execute a multi-step process of planning, searching,\nand synthesizing information to provide answers. This paradigm introduces\nunique challenges for LLMs' reasoning and agentic capabilities when interacting\nwith retrieval systems and the broader web. In this paper, we propose a\nreasoning-driven LLM-based pipeline to study effective reasoning behavior\npatterns in agentic search. Using this pipeline, we analyze successful agentic\nsearch trajectories and identify four beneficial reasoning behaviors:\nInformation Verification, Authority Evaluation, Adaptive Search, and Error\nRecovery. Based on these findings, we propose a technique called Behavior\nPriming to train more effective agentic search models. It synthesizes agentic\nsearch trajectories that exhibit these four behaviors and integrates them into\nthe agentic search model through supervised fine-tuning (SFT), followed by\nstandard reinforcement learning (RL). Experiments on three benchmarks (GAIA,\nWebWalker, and HLE) demonstrate that behavior priming yields over 35% gains in\nLlama3.2-3B and Qwen3-1.7B compared to directly training agentic search models\nwith RL. Crucially, we demonstrate that the desired reasoning behaviors in the\nSFT data, rather than the correctness of the final answer, is the critical\nfactor for achieving strong final performance after RL: fine-tuning on\ntrajectories with desirable reasoning behaviors but incorrect answers leads to\nbetter performance than fine-tuning on trajectories with correct answers. Our\nanalysis further reveals the underlying mechanism: the introduced reasoning\nbehaviors endow models with more effective exploration (higher pass@k and\nentropy) and test-time scaling (longer trajectories) capabilities, providing a\nstrong foundation for RL. Our code will be released as open source.", "AI": {"tldr": "\u4ee3\u7406\u5f0f\u641c\u7d22\u4e2d\uff0c\u672c\u6587\u63d0\u51faBehavior Priming\u6280\u672f\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002\u7814\u7a76\u53d1\u73b0\uff0cSFT\u6570\u636e\u4e2d\u597d\u7684\u63a8\u7406\u884c\u4e3a\u6bd4\u7b54\u6848\u6b63\u786e\u6027\u66f4\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4ece\u800c\u4f7fLLM\u5728\u4ee3\u7406\u5f0f\u641c\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4ee3\u7406\u5f0f\u641c\u7d22\uff08Agentic search\uff09\u5229\u7528LLM\u89e3\u91ca\u590d\u6742\u7528\u6237\u9700\u6c42\u5e76\u6267\u884c\u591a\u6b65\u89c4\u5212\u3001\u641c\u7d22\u548c\u4fe1\u606f\u5408\u6210\u3002\u7136\u800c\uff0cLLM\u5728\u4e0e\u68c0\u7d22\u7cfb\u7edf\u548c\u7f51\u9875\u4ea4\u4e92\u65f6\uff0c\u5176\u63a8\u7406\u548c\u4ee3\u7406\u80fd\u529b\u9762\u4e34\u72ec\u7279\u7684\u6311\u6218\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u63d0\u5347\u5176\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u4e2a\u63a8\u7406\u9a71\u52a8\u7684LLM\u7ba1\u9053\uff0c\u7528\u4e8e\u5206\u6790\u4ee3\u7406\u5f0f\u641c\u7d22\u4e2d\u6709\u6548\u7684\u63a8\u7406\u884c\u4e3a\u6a21\u5f0f\u30022. \u901a\u8fc7\u5206\u6790\u6210\u529f\u7684\u4ee3\u7406\u5f0f\u641c\u7d22\u8f68\u8ff9\uff0c\u8bc6\u522b\u51fa\u56db\u79cd\u6709\u76ca\u7684\u63a8\u7406\u884c\u4e3a\uff1a\u4fe1\u606f\u9a8c\u8bc1\u3001\u6743\u5a01\u8bc4\u4f30\u3001\u81ea\u9002\u5e94\u641c\u7d22\u548c\u9519\u8bef\u6062\u590d\u30023. \u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u63d0\u51fa\u201c\u884c\u4e3a\u9884\u8bad\u7ec3\u201d\uff08Behavior Priming\uff09\u6280\u672f\uff0c\u901a\u8fc7\u5408\u6210\u5305\u542b\u8fd9\u4e9b\u884c\u4e3a\u7684\u4ee3\u7406\u5f0f\u641c\u7d22\u8f68\u8ff9\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u968f\u540e\u7ed3\u5408\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6765\u8bad\u7ec3\u66f4\u6709\u6548\u7684\u4ee3\u7406\u5f0f\u641c\u7d22\u6a21\u578b\u3002", "result": "1. \u5728GAIA\u3001WebWalker\u548cHLE\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u884c\u4e3a\u9884\u8bad\u7ec3\uff08Behavior Priming\uff09\u4f7fLlama3.2-3B\u548cQwen3-1.7B\u7684\u6027\u80fd\u76f8\u6bd4\u76f4\u63a5\u4f7f\u7528RL\u8bad\u7ec3\u63d0\u9ad8\u4e8635%\u4ee5\u4e0a\u30022. \u5173\u952e\u53d1\u73b0\u662f\uff0cSFT\u6570\u636e\u4e2d\u671f\u671b\u7684\u63a8\u7406\u884c\u4e3a\uff08\u800c\u975e\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u6027\uff09\u662fRL\u540e\u5b9e\u73b0\u5f3a\u5927\u6700\u7ec8\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff1a\u5373\u4f7f\u5fae\u8c03\u6570\u636e\u4e2d\u7684\u63a8\u7406\u884c\u4e3a\u826f\u597d\u4f46\u7b54\u6848\u4e0d\u6b63\u786e\uff0c\u4e5f\u80fd\u6bd4\u5fae\u8c03\u6570\u636e\u7b54\u6848\u6b63\u786e\u4f46\u63a8\u7406\u884c\u4e3a\u4e0d\u4f73\u7684\u60c5\u51b5\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u30023. \u5206\u6790\u63ed\u793a\uff0c\u5f15\u5165\u7684\u63a8\u7406\u884c\u4e3a\u8d4b\u4e88\u6a21\u578b\u66f4\u6709\u6548\u7684\u63a2\u7d22\u80fd\u529b\uff08\u66f4\u9ad8\u7684pass@k\u548c\u71b5\uff09\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\u80fd\u529b\uff08\u66f4\u957f\u7684\u8f68\u8ff9\uff09\uff0c\u4e3aRL\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "conclusion": "1. \u6709\u6548\u7684\u63a8\u7406\u884c\u4e3a\uff08\u4fe1\u606f\u9a8c\u8bc1\u3001\u6743\u5a01\u8bc4\u4f30\u3001\u81ea\u9002\u5e94\u641c\u7d22\u3001\u9519\u8bef\u6062\u590d\uff09\u5bf9\u4e8e\u63d0\u5347\u4ee3\u7406\u5f0f\u641c\u7d22LLM\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u30022. \u884c\u4e3a\u9884\u8bad\u7ec3\uff08Behavior Priming\uff09\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u5728SFT\u9636\u6bb5\u5173\u6ce8\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u4ee3\u7406\u5f0f\u641c\u7d22\u6a21\u578b\u7684\u6548\u7387\u30023. SFT\u6570\u636e\u4e2d\u63a8\u7406\u884c\u4e3a\u7684\u8d28\u91cf\u6bd4\u6700\u7ec8\u7b54\u6848\u7684\u6b63\u786e\u6027\u66f4\u80fd\u51b3\u5b9a\u6a21\u578b\u5728RL\u540e\u7684\u6700\u7ec8\u6027\u80fd\uff0c\u56e0\u4e3a\u826f\u597d\u7684\u63a8\u7406\u884c\u4e3a\u80fd\u589e\u5f3a\u6a21\u578b\u7684\u63a2\u7d22\u548c\u6269\u5c55\u80fd\u529b\u3002"}}
{"id": "2510.06273", "pdf": "https://arxiv.org/pdf/2510.06273", "abs": "https://arxiv.org/abs/2510.06273", "authors": ["Divyansh Srivastava", "Andrzej Niedzielski"], "title": "Vision Transformer for Transient Noise Classification", "categories": ["cs.CV", "astro-ph.IM", "cs.LG", "gr-qc"], "comment": "9 pages, 4 figures", "summary": "Transient noise (glitches) in LIGO data hinders the detection of\ngravitational waves (GW). The Gravity Spy project has categorized these noise\nevents into various classes. With the O3 run, there is the inclusion of two\nadditional noise classes and thus a need to train new models for effective\nclassification. We aim to classify glitches in LIGO data into 22 existing\nclasses from the first run plus 2 additional noise classes from O3a using the\nVision Transformer (ViT) model. We train a pre-trained Vision Transformer\n(ViT-B/32) model on a combined dataset consisting of the Gravity Spy dataset\nwith the additional two classes from the LIGO O3a run. We achieve a\nclassification efficiency of 92.26%, demonstrating the potential of Vision\nTransformer to improve the accuracy of gravitational wave detection by\neffectively distinguishing transient noise.\n  Key words: gravitational waves --vision transformer --machine learning", "AI": {"tldr": "\u4e3aLIGO\u6570\u636e\u4e2d\u768424\u79cd\u77ac\u6001\u566a\u58f0\uff08glitch\uff09\u8bbe\u8ba1\u4e86\u4e00\u4e2aVision Transformer\u5206\u7c7b\u6a21\u578b\uff0c\u5b9e\u73b0\u4e8692.26%\u7684\u5206\u7c7b\u6548\u7387\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u5f15\u529b\u6ce2\u63a2\u6d4b\u7cbe\u5ea6\u3002", "motivation": "LIGO\u6570\u636e\u4e2d\u7684\u77ac\u6001\u566a\u58f0\uff08glitch\uff09\u4e25\u91cd\u5e72\u6270\u5f15\u529b\u6ce2\u63a2\u6d4b\u3002\u968f\u7740O3a\u8fd0\u884c\u65b0\u589e\u4e24\u4e2a\u566a\u58f0\u7c7b\u522b\uff0c\u73b0\u6709\u5206\u7c7b\u6a21\u578b\u4e0d\u8db3\uff0c\u4e9f\u9700\u5f00\u53d1\u65b0\u7684\u6709\u6548\u5206\u7c7b\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u5f15\u529b\u6ce2\u63a2\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684Vision Transformer (ViT-B/32) \u6a21\u578b\uff0c\u5728\u4e00\u4e2a\u5305\u542bGravity Spy\u6570\u636e\u96c6\uff0822\u7c7b\uff09\u548cLIGO O3a\u65b0\u589e\u4e24\u7c7b\u566a\u58f0\u7684\u7ec4\u5408\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e8\u5728\u5bf9\u603b\u517124\u79cd\u566a\u58f0\u7c7b\u522b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u6a21\u578b\u572824\u79cd\u77ac\u6001\u566a\u58f0\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e8692.26%\u7684\u5206\u7c7b\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u8868\u660eVision Transformer\u6a21\u578b\u5728\u6709\u6548\u533a\u5206\u77ac\u6001\u566a\u58f0\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u6709\u671b\u663e\u8457\u63d0\u9ad8\u5f15\u529b\u6ce2\u63a2\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
