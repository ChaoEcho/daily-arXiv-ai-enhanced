<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 17]
- [cs.CV](#cs.CV) [Total: 20]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 17]
- [cs.NI](#cs.NI) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MultiStream-LLM: Bridging Modalities for Robust Sign Language Translation](https://arxiv.org/abs/2509.00030)
*Marshall Thomas,Edward Fish,Richard Bowden*

Main category: cs.CL

TL;DR: 现有手语翻译模型难以处理高速手指拼写和异步面部线索。本文提出MultiStream-LLM，一个模块化框架，通过分离并使用专门预测器处理不同模态（连续手语、手指拼写、唇读），再通过Transformer融合并输入LLM进行生成，在How2Sign基准测试和手指拼写数据集上达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管无光泽手语翻译（SLT）有所进展，但单一同构的端到端模型在识别高速手指拼写和整合异步非手动面部线索这两个关键方面表现不佳，导致在翻译姓名、地点和技术术语等关键信息时性能低下。

Method: 引入MultiStream-LLM，一个模块化框架。该方法采用独立的、专门的预测器分别处理连续手语、手指拼写和唇读。每个专家网络首先将其特定模态解码为token序列。然后，一个轻量级Transformer融合这些并行流以解决时间错位，并将合并后的表示传递给大型语言模型（LLM）进行最终句子生成。

Result: 在How2Sign基准测试上取得了23.5的BLEU-4分数，创造了新的最先进记录。在具有挑战性的ChicagoFSWildPlus手指拼写数据集上，实现了73.2%的字母准确率。

Conclusion: 通过在融合之前隔离并解决不同的识别任务，本文提出的多专家方法为实现稳健、高保真手语翻译提供了一条更强大和有效的途径，验证了其核心假设。

Abstract: Despite progress in gloss-free Sign Language Translation (SLT), monolithic
end-to-end models consistently fail on two critical components of natural
signing: the precise recognition of high-speed fingerspelling and the
integration of asynchronous non-manual cues from the face. Recent progress in
Automated Sign Language Translation with Large Language Models has side stepped
this challenge, forcing a single network to learn these simultaneously
resulting in poor performance when tasked with translating crucial information
such as names,places, and technical terms. We introduce MultiStream-LLM, a
modular framework designed to overcome these limitations. Our approach employs
separate, specialized predictors for continuous signing, fingerspelling, and
lipreading. Each expert network first decodes its specific modality into a
sequence of tokens. These parallel streams are then fused by a lightweight
transformer that resolves temporal misalignments before passing the combined
representation to a Large Language Model (LLM) for final sentence generation.
Our method establishes a new state-of-the-art on the How2Sign benchmark with a
BLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging
ChicagoFSWildPlus fingerspelling dataset. These results validate our core
hypothesis: by isolating and solving distinct recogni tion tasks before fusion,
our multi-expert approach provides a more powerful and effective pathway to
robust, high-fidelity sign language translation.

</details>


### [2] [Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis](https://arxiv.org/abs/2509.00038)
*Teo Susnjak*

Main category: cs.CL

TL;DR: 本文提出一种将声明式提示优化技术应用于系统性文献综述（SLR）自动化领域的新方法，旨在解决现有LLM辅助SLR中提示脆弱、可重复性差的问题，从而构建可验证且透明的LLM流水线。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在加速系统性文献综述（SLR）方面潜力巨大，但现有方法依赖于脆弱且手动制作的提示，这损害了可靠性和可重复性，从而削弱了对LLM辅助证据综合的科学信心。

Method: 该研究将通用的声明式提示优化（declarative prompt optimisation）技术应用于SLR自动化领域。具体方法是提出一个结构化、领域特定的框架，该框架将任务声明、测试套件和自动化提示调整嵌入到可重复的SLR工作流程中，并提供具体蓝图和可操作代码示例。

Result: 通过此方法，研究人员能够构建可验证的LLM流水线，使其与证据综合中既定的透明度和严谨性原则保持一致。该研究为SLR流水线提供了此类方法的新颖应用。

Conclusion: 本研究通过引入声明式提示优化框架，为LLM辅助的系统性文献综述提供了一种解决现有方法中提示脆弱性问题的新途径，从而提升了LLM在证据综合中的可靠性、可重复性和科学信心。

Abstract: Large language models (LLMs) offer significant potential to accelerate
systematic literature reviews (SLRs), yet current approaches often rely on
brittle, manually crafted prompts that compromise reliability and
reproducibility. This fragility undermines scientific confidence in
LLM-assisted evidence synthesis. In response, this work adapts recent advances
in declarative prompt optimisation, developed for general-purpose LLM
applications, and demonstrates their applicability to the domain of SLR
automation. This research proposes a structured, domain-specific framework that
embeds task declarations, test suites, and automated prompt tuning into a
reproducible SLR workflow. These emerging methods are translated into a
concrete blueprint with working code examples, enabling researchers to
construct verifiable LLM pipelines that align with established principles of
transparency and rigour in evidence synthesis. This is a novel application of
such approaches to SLR pipelines.

</details>


### [3] [What Are Research Hypotheses?](https://arxiv.org/abs/2509.00185)
*Jian Wu,Sarah Rajtmajer*

Main category: cs.CL

TL;DR: 本文分析了自然语言理解(NLU)领域中“假设”一词定义的多样性与偏离传统科学定义的现象，并强调了清晰定义假设的重要性，以促进机器可解释的学术记录。


<details>
  <summary>Details</summary>
Motivation: 在自然语言理解(NLU)任务中，“假设”一词的解释已偏离传统科学定义，且在NLU文献内部也存在定义上的差异和不一致性。

Method: 本文概述并描绘了“假设”的各种定义，尤其辨别了近期NLU任务中定义的细微差别。

Result: 通过分析，揭示了NLU任务中“假设”定义的复杂性、多样性和不一致性。

Conclusion: 明确和良好定义的假设对于构建机器可解释的学术记录至关重要。

Abstract: Over the past decades, alongside advancements in natural language processing,
significant attention has been paid to training models to automatically
extract, understand, test, and generate hypotheses in open and scientific
domains. However, interpretations of the term \emph{hypothesis} for various
natural language understanding (NLU) tasks have migrated from traditional
definitions in the natural, social, and formal sciences. Even within NLU, we
observe differences defining hypotheses across literature. In this paper, we
overview and delineate various definitions of hypothesis. Especially, we
discern the nuances of definitions across recently published NLU tasks. We
highlight the importance of well-structured and well-defined hypotheses,
particularly as we move toward a machine-interpretable scholarly record.

</details>


### [4] [Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics](https://arxiv.org/abs/2509.00190)
*Sheldon Yu,Yuxin Xiong,Junda Wu,Xintong Li,Tong Yu,Xiang Chen,Ritwik Sinha,Jingbo Shang,Julian McAuley*

Main category: cs.CL

TL;DR: CoT推理的可解释性不足，尤其在高级语义层面。本文提出一种状态感知转换框架，通过谱分析和马尔可夫链将CoT轨迹抽象为结构化潜在动态，以增强其解释性并支持多维分析。


<details>
  <summary>Details</summary>
Motivation: 现有CoT推理的可解释性有限，侧重于局部token层面归因，而高层语义角色及推理步骤间的转换机制未被充分探索。

Method: 引入一个状态感知转换框架，将CoT轨迹抽象为结构化潜在动态。具体地，通过对token级嵌入进行谱分析将每个推理步骤聚类成语义连贯的潜在状态；接着，将这些潜在状态的演进建模为马尔可夫链，以刻画推理的全局结构。

Result: 该抽象框架支持一系列分析，包括语义角色识别、时间模式可视化和一致性评估。

Conclusion: 本框架通过提供结构化且可解释的推理过程视角，显著提升了CoT推理的可解释性。

Abstract: Recent advances in chain-of-thought (CoT) prompting have enabled large
language models (LLMs) to perform multi-step reasoning. However, the
explainability of such reasoning remains limited, with prior work primarily
focusing on local token-level attribution, such that the high-level semantic
roles of reasoning steps and their transitions remain underexplored. In this
paper, we introduce a state-aware transition framework that abstracts CoT
trajectories into structured latent dynamics. Specifically, to capture the
evolving semantics of CoT reasoning, each reasoning step is represented via
spectral analysis of token-level embeddings and clustered into semantically
coherent latent states. To characterize the global structure of reasoning, we
model their progression as a Markov chain, yielding a structured and
interpretable view of the reasoning process. This abstraction supports a range
of analyses, including semantic role identification, temporal pattern
visualization, and consistency evaluation.

</details>


### [5] [The Rarity Blind Spot: A Framework for Evaluating Statistical Reasoning in LLMs](https://arxiv.org/abs/2509.00245)
*Seiji Maekawa,Hayate Iso,Nikita Bhutani*

Main category: cs.CL

TL;DR: 本文引入了“独特性特征挖掘（DFM）”新任务和“DiFBench”基准测试框架，用于评估大型语言模型（LLMs）在文档集合中识别全局独特特征的能力，发现现有LLMs在统计推理和稀有性检测方面存在显著局限。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准侧重于信息检索或摘要，未能评估模型识别文档集中全局独特性特征的能力，而这对于决策制定（如候选人选择、产品差异化）至关重要，需要统计推理而非简单检索。

Method: 引入了“独特性特征挖掘（DFM）”任务，要求模型分析10-40份文档集合，识别全局范围内稀有（出现频率低于10%）的特征。创建了可配置的“DiFBench”基准测试框架，用于系统评估此能力。使用DiFBench对十个先进的LLMs进行了大规模评估。

Result: 研究发现，通用型模型与推理增强型模型之间存在显著的性能差距。所有模型在任务复杂度和文档数量增加时，性能均大幅下降。一个常见失败模式是将频繁特征误识别为独特性特征。

Conclusion: 这些发现揭示了当前LLMs在执行细粒度统计推理和稀有性检测方面的核心局限性。

Abstract: Effective decision-making often relies on identifying what makes each
candidate distinctive. While existing benchmarks for LLMs emphasize retrieving
or summarizing information relevant to a given query, they do not evaluate a
model's ability to identify globally distinctive features across a set of
documents. We introduce Distinctive Feature Mining (DFM), a new task that
challenges models to analyze a small-to-medium collection (10-40 documents) and
surface features that are rare in the global context (e.g., appearing in less
than 10% of documents). This setting mirrors real-world scenarios such as
candidate selection or product differentiation, where statistical reasoning,
not retrieval, is key. To enable systematic evaluation of this capability, we
present DiFBench, a configurable benchmark creation framework with controllable
parameters such as document set size and distinctiveness thresholds. Using
DiFBench, we perform a large-scale assessment of distinctive feature mining
across ten state-of-the-art LLMs. Our findings reveal a significant performance
gap between general-purpose and reasoning-enhanced models. All models, however,
substantially degrade as the task complexity and document count increase. We
also find that a common failure mode is misidentifying frequent features as
distinctive. These insights reveal core limitations in contemporary LLMs'
abilities to perform fine-grained, statistical reasoning and rarity detection.

</details>


### [6] [The Differential Meaning of Models: A Framework for Analyzing the Structural Consequences of Semantic Modeling Decisions](https://arxiv.org/abs/2509.00248)
*Zachary K. Stine,James E. Deitrick*

Main category: cs.CL

TL;DR: 本文提出了一个基于C. S. Peirce符号学理论的通用理论框架，用于描述和比较各种人类意义构建模型，并将模型及其决策视为符号。


<details>
  <summary>Details</summary>
Motivation: 现有的人类意义构建模型方法众多，但领域内缺乏一个通用的理论框架，无法以统一的方式描述和比较这些不同类型的建模实践。

Method: 提出一个基于C. S. Peirce符号学理论的通用框架。该框架将模型视为测量潜在符号几何体，并将其理解为关于符号数据背后复杂符号能动性的假设。此外，当模型价值无法通过性能代理指标直接衡量时，该框架建议通过与其他模型的对比来关系性地理解模型，从而形成一个模型语义理论，其中模型及其建模决策本身被视为符号。

Result: 提出了一个能够统一描述和比较人类意义构建模型的通用理论框架，并在此基础上发展了一个模型语义理论，将模型视为符号。通过简短的实证例子，展示了该框架的实际应用，并探讨了其所启用的基础性问题和未来研究方向。

Conclusion: 该框架提供了一种描述、比较和解释各种人类意义构建模型的连贯方式，将模型及其建模决策本身视为符号，为理解这些复杂系统提供了新的视角，并开启了未来的研究方向。

Abstract: The proliferation of methods for modeling of human meaning-making constitutes
a powerful class of instruments for the analysis of complex semiotic systems.
However, the field lacks a general theoretical framework for describing these
modeling practices across various model types in an apples-to-apples way. In
this paper, we propose such a framework grounded in the semiotic theory of C.
S. Peirce. We argue that such models measure latent symbol geometries, which
can be understood as hypotheses about the complex of semiotic agencies
underlying a symbolic dataset. Further, we argue that in contexts where a
model's value cannot be straightforwardly captured by proxy measures of
performance, models can instead be understood relationally, so that the
particular interpretive lens of a model becomes visible through its contrast
with other models. This forms the basis of a theory of model semantics in which
models, and the modeling decisions that constitute them, are themselves treated
as signs. In addition to proposing the framework, we illustrate its empirical
use with a few brief examples and consider foundational questions and future
directions enabled by the framework.

</details>


### [7] [The Temporal Game: A New Perspective on Temporal Relation Extraction](https://arxiv.org/abs/2509.00250)
*Hugo Sousa,Ricardo Campos,Alípio Jorge*

Main category: cs.CL

TL;DR: 本文介绍Temporal Game，一种将时间关系提取任务转化为交互式游戏的新颖方法，通过点对点比较实现更灵活、细粒度的标注，并为强化学习代理训练奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统的区间级时间关系标注不够灵活和细粒度，且未充分利用交互式方法潜力；研究旨在提供一种更高效、更灵活的标注方法，并为未来训练强化学习代理进行时间标注提供基础。

Method: 该方法将时间关系提取任务视为一个交互式游戏，将区间级关系分解为时间实体起点和终点之间的点对点比较。玩家在每一步分类一个单一的时间点关系，系统应用时间闭包来推断额外关系并强制保持一致性。这种基于点的方法同时支持区间和瞬时实体。

Result: 开发了一个演示系统，包含游戏模式（用户在TempEval-3数据集上标注并获得评分反馈）和标注模式（允许标注自定义文档并导出时间线）。该系统作为研究工具和标注界面，已公开发布并开源。

Conclusion: Temporal Game提供了一种创新的、更细粒度和灵活的时间标注方法，解决了现有方法的局限性，并为训练强化学习代理提供了基础。其公开可用和开源的演示系统将促进时间推理和标注领域的进一步研究和社区发展。

Abstract: In this paper we demo the Temporal Game, a novel approach to temporal
relation extraction that casts the task as an interactive game. Instead of
directly annotating interval-level relations, our approach decomposes them into
point-wise comparisons between the start and end points of temporal entities.
At each step, players classify a single point relation, and the system applies
temporal closure to infer additional relations and enforce consistency. This
point-based strategy naturally supports both interval and instant entities,
enabling more fine-grained and flexible annotation than any previous approach.
The Temporal Game also lays the groundwork for training reinforcement learning
agents, by treating temporal annotation as a sequential decision-making task.
To showcase this potential, the demo presented in this paper includes a Game
mode, in which users annotate texts from the TempEval-3 dataset and receive
feedback based on a scoring system, and an Annotation mode, that allows custom
documents to be annotated and resulting timeline to be exported. Therefore,
this demo serves both as a research tool and an annotation interface. The demo
is publicly available at https://temporal-game.inesctec.pt, and the source code
is open-sourced to foster further research and community-driven development in
temporal reasoning and annotation.

</details>


### [8] [Exploring Reasoning-Infused Text Embedding with Large Language Models for Zero-Shot Dense Retrieval](https://arxiv.org/abs/2509.00276)
*Yuxiang Liu,Tian Wang,Gourab Kundu,Tianyu Cao,Guang Cheng,Zhen Ge,Jianshu Chen,Qingjun Cui,Trishul Chilimbi*

Main category: cs.CL

TL;DR: 提出RITE方法，通过生成中间推理文本，将逻辑推理集成到生成式LLM的文本嵌入过程中，显著提升了推理密集型检索任务的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer编码器模型难以处理需推理的复杂查询；尽管解码器LLM具有强推理能力，但现有LLM嵌入方法未充分利用其推理优势。

Method: 提出推理注入文本嵌入（RITE），利用生成式LLM，在计算嵌入前生成中间推理文本，将逻辑推理融入文本嵌入过程，以增加表示的推理深度。

Result: 在推理密集型检索基准BRIGHT上的实验结果表明，RITE显著提升了跨领域零样本检索性能。

Conclusion: 将推理能力融入嵌入过程是有效且可行的，RITE的成功验证了这一点。

Abstract: Transformer-based models such as BERT and E5 have significantly advanced text
embedding by capturing rich contextual representations. However, many complex
real-world queries require sophisticated reasoning to retrieve relevant
documents beyond surface-level lexical matching, where encoder-only retrievers
often fall short. Decoder-only large language models (LLMs), known for their
strong reasoning capabilities, offer a promising alternative. Despite this
potential, existing LLM-based embedding methods primarily focus on contextual
representation and do not fully exploit the reasoning strength of LLMs. To
bridge this gap, we propose Reasoning-Infused Text Embedding (RITE), a simple
but effective approach that integrates logical reasoning into the text
embedding process using generative LLMs. RITE builds upon existing language
model embedding techniques by generating intermediate reasoning texts in the
token space before computing embeddings, thereby enriching representations with
inferential depth. Experimental results on BRIGHT, a reasoning-intensive
retrieval benchmark, demonstrate that RITE significantly enhances zero-shot
retrieval performance across diverse domains, underscoring the effectiveness of
incorporating reasoning into the embedding process.

</details>


### [9] [OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews](https://arxiv.org/abs/2509.00285)
*Mir Tafseer Nayeem,Davood Rafiei*

Main category: cs.CL

TL;DR: 本研究提出OpinioRAG框架，结合RAG和LLMs，从海量用户评论中高效生成个性化观点摘要，并引入新型无参考验证指标和大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理海量用户评论的观点摘要生成时，存在可扩展性差或生成通用摘要无法满足个性化需求的问题。

Method: 1. 提出OpinioRAG框架，利用RAG（检索增强生成）证据检索与大型语言模型（LLMs）结合，高效生成定制化摘要。 2. 提出针对情感丰富领域的新型无参考验证指标，用于细粒度、上下文敏感地评估事实一致性。 3. 构建首个大规模长篇用户评论数据集，包含专家摘要和人工标注查询。

Result: 通过广泛实验，识别了关键挑战，提供了改进系统的可行见解，并为未来研究铺平了道路。

Conclusion: OpinioRAG被定位为一个强大的框架，能够大规模生成准确、相关且结构化的观点摘要。

Abstract: We study the problem of opinion highlights generation from large volumes of
user reviews, often exceeding thousands per entity, where existing methods
either fail to scale or produce generic, one-size-fits-all summaries that
overlook personalized needs. To tackle this, we introduce OpinioRAG, a
scalable, training-free framework that combines RAG-based evidence retrieval
with LLMs to efficiently produce tailored summaries. Additionally, we propose
novel reference-free verification metrics designed for sentiment-rich domains,
where accurately capturing opinions and sentiment alignment is essential. These
metrics offer a fine-grained, context-sensitive assessment of factual
consistency. To facilitate evaluation, we contribute the first large-scale
dataset of long-form user reviews, comprising entities with over a thousand
reviews each, paired with unbiased expert summaries and manually annotated
queries. Through extensive experiments, we identify key challenges, provide
actionable insights into improving systems, pave the way for future research,
and position OpinioRAG as a robust framework for generating accurate, relevant,
and structured summaries at scale.

</details>


### [10] [Wage Sentiment Indices Derived from Survey Comments via Large Language Models](https://arxiv.org/abs/2509.00290)
*Taihei Sone*

Main category: cs.CL

TL;DR: 本研究提出并构建了一个基于大型语言模型(LLM)的工资情绪指数(WSI)，用于预测日本的工资动态，并发现其性能显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能为经济文本分析创造了新机遇，旨在利用LLM开发一种更及时、有效的工具来监测和预测经济指标（如工资），以辅助政府和央行的经济政策制定。

Method: 该研究构建了工资情绪指数（WSI），利用大型语言模型（LLMs）对日本内阁府的“经济观察者调查”（EWS）数据进行分析。WSI扩展了此前价格情绪指数（PSI）的框架，并为确保可扩展性和适应性，开发了一个数据架构，允许整合报纸和社交媒体等额外数据源。

Result: 实验结果表明，基于LLM构建的WSI模型在预测工资动态方面，显著优于基线方法和预训练模型。

Conclusion: 研究结果强调了LLM驱动的情绪指数在提升政府和中央银行经济政策设计的及时性和有效性方面的巨大潜力。

Abstract: The emergence of generative Artificial Intelligence (AI) has created new
opportunities for economic text analysis. This study proposes a Wage Sentiment
Index (WSI) constructed with Large Language Models (LLMs) to forecast wage
dynamics in Japan. The analysis is based on the Economy Watchers Survey (EWS),
a monthly survey conducted by the Cabinet Office of Japan that captures
real-time economic assessments from workers in industries highly sensitive to
business conditions. The WSI extends the framework of the Price Sentiment Index
(PSI) used in prior studies, adapting it specifically to wage related
sentiment. To ensure scalability and adaptability, a data architecture is also
developed that enables integration of additional sources such as newspapers and
social media. Experimental results demonstrate that WSI models based on LLMs
significantly outperform both baseline approaches and pretrained models. These
findings highlight the potential of LLM-driven sentiment indices to enhance the
timeliness and effectiveness of economic policy design by governments and
central banks.

</details>


### [11] [Balanced Actor Initialization: Stable RLHF Training of Distillation-Based Reasoning Models](https://arxiv.org/abs/2509.00309)
*Chen Zheng,Yiyuan Ma,Yuan Yang,Deyi Liu,Jing Liu,Zuquan Song,Yuxin Song,Cheng Ren,Hang Zhu,Xin Liu,Yiyuan Ma,Siyuan Qiao,Xun Zhou,Liang Xiang,Yonghui Wu*

Main category: cs.CL

TL;DR: 本研究提出平衡Actor初始化 (BAI) 方法，旨在解决将RLHF应用于蒸馏训练模型时出现的序列长度崩溃和奖励曲棍球杆曲线等不稳定性问题，从而实现稳定训练并结合蒸馏效率与RLHF对齐。


<details>
  <summary>Details</summary>
Motivation: 尽管指令微调/RLHF和蒸馏推理微调在大型语言模型中独立有效，但将RLHF应用于经过蒸馏训练的模型（第三范式）存在显著挑战，表现为序列长度崩溃和奖励曲棍球杆曲线，严重损害模型的对齐和推理能力。

Method: 提出平衡Actor初始化 (Balanced Actor Initialization, BAI)。这是一种两阶段加权模型合并方法：首先合并指令遵循模型和蒸馏推理微调模型，然后将此中间模型与预训练模型进一步结合，以保留基础知识。

Result: BAI解决了序列长度崩溃问题，缓解了奖励曲棍球杆曲线，并在训练过程中实现了序列长度的持续改进。分析表明，平衡的合并比例在训练稳定性与推理能力保留之间达到了最佳权衡。

Conclusion: 本工作为第三范式下的稳定训练提供了有效解决方案，从而能够构建结合蒸馏效率和RLHF对齐的更强大的推理模型。

Abstract: The development of alignment and reasoning capabilities in large language
models has seen remarkable progress through two paradigms: instruction tuning
and reinforcement learning from human feedback (RLHF) alignment paradigm, and
distillation-based reasoning fine-tuning paradigm. While both approaches prove
effective independently, the third paradigm of applying RLHF to
distillation-trained models presents significant challenges. Our investigation
reveals two critical phenomena that emerge in this paradigm: Sequence Length
Collapse, where language generation dramatically reduces during early RLHF
training, and the Reward Hockey Stick Curve, featuring severe reward score
drops followed by gradual recovery. These instabilities fundamentally
compromise the model's alignment and reasoning capabilities. To address these
challenges, we propose Balanced Actor Initialization (BAI), a two-stage
weighted model merging approach. BAI first merges instruction-following and
distillation-based reasoning fine-tuned models, then further combines this
intermediate model with the pretrained model to preserve foundational
knowledge. Through comprehensive experiments across diverse benchmarks and
detailed analysis of training experiments, we demonstrate that BAI resolves
Sequence Length Collapse, mitigates the Reward Hockey Stick Curve, and enables
continuous sequence length improvement during training. Additionally, our
analysis reveals that balanced merging ratios achieve optimal trade-offs
between training stability and reasoning capability preservation. Our work
provides the effective solution for stable training in this third paradigm,
enabling more capable reasoning models that combine distillation efficiency
with RLHF alignment.

</details>


### [12] [GIER: Gap-Driven Self-Refinement for Large Language Models](https://arxiv.org/abs/2509.00325)
*Rinku Dewri*

Main category: cs.CL

TL;DR: GIER是一个通用框架，通过自我反思和基于概念质量标准的迭代修订来改进大型语言模型（LLM）的输出。它利用自然语言描述的推理差距，提升了多种推理任务中LLM的输出质量（如推理质量、基础和推理对齐），且不影响任务准确性。


<details>
  <summary>Details</summary>
Motivation: 现有提示策略（如演示、示例、思维链）未能有效利用LLM的自我反思能力来系统性地改进输出质量。研究旨在开发一种通用框架，使LLM能基于抽象的概念质量标准，通过迭代修订来填补推理差距，从而提升其输出质量。

Method: 引入GIER (Gap-driven Iterative Enhancement of Responses) 框架。该方法不依赖于示例或思维链，而是利用自然语言描述的“推理差距”，引导LLM迭代地批判并精炼自己的输出，以更好地满足预设的概念质量标准。

Result: GIER在三个推理密集型任务（SciFact, PrivacyQA, e-SNLI）和四种LLM（GPT-4.1, GPT-4o Mini, Gemini 1.5 Pro, Llama 3.3 70B）上进行了评估。结果显示，GIER显著提高了输出的“推理质量、基础和推理对齐”，同时没有降低任务的准确性。

Conclusion: 研究表明，LLM不仅能够理解抽象的概念性差距，还能将其转化为具体的推理改进。GIER框架提供了一种有效且通用的方法，通过引导LLM进行自我反思和基于差距的迭代修订来提升其推理能力和输出质量。

Abstract: We introduce GIER (Gap-driven Iterative Enhancement of Responses), a general
framework for improving large language model (LLM) outputs through
self-reflection and revision based on conceptual quality criteria. Unlike
prompting strategies that rely on demonstrations, examples, or chain-of-thought
templates, GIER utilizes natural language descriptions of reasoning gaps, and
prompts a model to iteratively critique and refine its own outputs to better
satisfy these criteria. Across three reasoning-intensive tasks (SciFact,
PrivacyQA, and e-SNLI) and four LLMs (GPT-4.1, GPT-4o Mini, Gemini 1.5 Pro, and
Llama 3.3 70B), GIER improves rationale quality, grounding, and reasoning
alignment without degrading task accuracy. Our analysis demonstrates that
models can not only interpret abstract conceptual gaps but also translate them
into concrete reasoning improvements.

</details>


### [13] [Open Data Synthesis For Deep Research](https://arxiv.org/abs/2509.00375)
*Ziyi Xia,Kun Luo,Hongjin Qian,Zheng Liu*

Main category: cs.CL

TL;DR: 本文提出InfoSeek，一个可扩展的框架，通过双代理系统从网页构建研究树来合成复杂的深度研究任务（HCSPs）。InfoSeek训练出的模型在挑战性基准上显著优于现有基线、甚至更大的模型和轻量级商业API。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）需要处理深度研究任务，这涉及问题分解、多步骤推理和信息综合。现有基准未能捕捉这种复杂性，而现有合成数据集存在快捷推理、知识泄露或结构深度不足等问题，亟需新的基准来弥补这一空白。

Method: 将深度研究任务形式化为分层约束满足问题（HCSPs）。引入InfoSeek框架，通过双代理系统从大规模网页递归构建研究树，将其转化为需要遍历完整层级的自然语言问题。该框架可扩展生成超过5万个训练样本和测试集，并生成拒绝采样的推理轨迹。同时，InfoSeek保留了中间步骤和检索标签等元信息，以支持高级优化策略。

Result: 在InfoSeek上训练的模型持续优于强基线。在挑战性基准BrowseComp-Plus上，经过InfoSeek优化的3B LLMs超越了更大的32B模型和轻量级商业API（如Gemini2.5-Flash），并取得了与更强API（如Gemini2.5-Pro）相当的性能。

Conclusion: InfoSeek通过提供可扩展的复杂深度研究任务合成框架，有效地解决了现有基准的不足。它能显著提升LLMs在这些任务上的表现，使小型模型也能与大型模型和商业API媲美，且其元信息保留功能支持进一步的高级优化。

Abstract: Large language models (LLMs) are increasingly expected to go beyond simple
factual queries toward Deep Research-tasks that require decomposing questions
into sub-problems, coordinating multi-step reasoning, and synthesizing evidence
from diverse sources. We formalize Deep Research tasks with verifiable answers
as Hierarchical Constraint Satisfaction Problems (HCSPs), which are
fundamentally different from single-constraint, multi-hop, or flat CSP
formulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA)
fail to capture this complexity, while recent synthetic datasets often
introduce shortcut reasoning, knowledge leakage, or lack sufficient structural
depth. To address this gap, we introduce InfoSeek, a scalable framework for
synthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to
recursively build a Research Tree from large-scale webpages, blurring
intermediate nodes into valid sub-problems, and converting these trees into
natural language questions that require traversing the full hierarchy. It also
enables rapid scaling, yielding over 50K training examples, a curated test set,
and reasoning trajectories generated via reject sampling. Experiments show that
models trained on InfoSeek consistently outperform strong baselines. On a
challenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass
much larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash),
while achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro).
By preserving meta-information such as intermediate steps and retrieval labels,
InfoSeek further supports advanced optimization strategies, including compound
reward design and trajectory-level exploration. We provide our codes and
datasets in \href{https://github.com/VectorSpaceLab/InfoSeek}{this repository}.

</details>


### [14] [GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction](https://arxiv.org/abs/2509.00388)
*Xuelin Li,Xiangqi Jin,Linfeng Zhang*

Main category: cs.CL

TL;DR: GraphKV是一个图基的KV缓存管理框架，通过动态更新令牌重要性来优化LLM在长序列处理中的KV缓存压缩，解决了传统方法的静态性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）处理长文本序列时，KV缓存管理效率低下且受限于内存。传统KV淘汰策略（如基于注意力分数的top-k选择）采用静态启发式方法，无法捕获推理过程中令牌之间动态演变的隐式依赖关系。

Method: 提出GraphKV，一个图基的KV缓存压缩令牌选择框架。将令牌建模为带有重要性分数的节点，边代表它们之间的相似性关系。通过“衰减信号传播机制”在图上传播信息，动态更新令牌的重要性，以自适应保留最具上下文意义的令牌。

Result: GraphKV能够重新定义KV缓存压缩中的令牌选择，实现动态、自适应地保留最具上下文意义的令牌。它还可以作为即插即用组件，无缝集成到现有KV缓存淘汰方法（如SnapKV和PyramidKV）中。

Conclusion: GraphKV通过引入动态图机制，克服了传统KV缓存淘汰策略的静态性限制，实现了对LLM长序列处理中上下文关键令牌的自适应保留，并可灵活应用于现有方案，从而提高了KV缓存管理的效率。

Abstract: Efficient Key-Value (KV) cache management is essential for processing long
text sequences in large language models (LLMs), where memory constraints often
limit performance. Conventional KV eviction strategies, such as top-k selection
based on attention scores, depend on static heuristics that fail to capture the
evolving implicit dependencies among tokens during inference. To overcome this,
we propose GraphKV, a graph-based framework that redefines token selection for
KV cache compression. In GraphKV, tokens are modeled as nodes with importance
scores, and edges represent their similarity relationships. Through a
decay-signal-propagation mechanism, token importance is dynamically updated by
propagating information across the graph, enabling adaptive retention of the
most contextually significant tokens. GraphKV can be seamlessly utilized in
existing KV cache eviction methods such as SnapKV and PyramidKV in a
plug-and-play manner. Codes will be released on Github.

</details>


### [15] [The Resurgence of GCG Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2509.00391)
*Yuting Tan,Xuying Li,Zhuo Li,Huizhen Shu,Peikang Hu*

Main category: cs.CL

TL;DR: 本研究系统评估了梯度对抗性提示算法GCG及其变体T-GCG在不同规模开源LLM上的越狱能力。主要发现包括：攻击成功率随模型规模增大而降低，编码任务比安全任务更易受攻击，且基于前缀的评估方法会显著高估攻击效果。


<details>
  <summary>Details</summary>
Motivation: 鉴于梯度对抗性提示（如GCG）已成为越狱大型语言模型（LLM）的强大方法，本研究旨在对GCG及其模拟退火增强变体T-GCG在不同规模开源LLM上的有效性和局限性进行系统性评估。

Method: 研究使用了Qwen2.5-0.5B、LLaMA-3.2-1B和GPT-OSS-20B等开源LLM，在安全导向提示（AdvBench）和推理密集型编码提示上评估了攻击有效性。评估方法对比了基于前缀的启发式评估与GPT-4o语义判断的严格评估。

Result: 1. 攻击成功率（ASR）随模型规模增大而降低，这反映了大型模型损失景观的复杂性。2. 基于前缀的启发式评估方法显著高估了攻击效果，而GPT-4o语义判断提供了更严格和真实的评估。3. 编码相关提示词比对抗性安全提示词更易受攻击，表明推理本身可以被利用作为攻击向量。4. T-GCG（模拟退火增强）能使对抗性搜索多样化，并在前缀评估下获得有竞争力的ASR，但在语义判断下其益处有限。

Conclusion: 这些发现揭示了GCG的可扩展性限制，暴露了推理任务中被忽视的漏洞，并促使研究人员进一步开发受模拟退火启发的新策略，以实现更稳健的对抗性评估。

Abstract: Gradient-based adversarial prompting, such as the Greedy Coordinate Gradient
(GCG) algorithm, has emerged as a powerful method for jailbreaking large
language models (LLMs). In this paper, we present a systematic appraisal of GCG
and its annealing-augmented variant, T-GCG, across open-source LLMs of varying
scales. Using Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B, we evaluate attack
effectiveness on both safety-oriented prompts (AdvBench) and
reasoning-intensive coding prompts. Our study reveals three key findings: (1)
attack success rates (ASR) decrease with model size, reflecting the increasing
complexity and non-convexity of larger models' loss landscapes; (2)
prefix-based heuristics substantially overestimate attack effectiveness
compared to GPT-4o semantic judgments, which provide a stricter and more
realistic evaluation; and (3) coding-related prompts are significantly more
vulnerable than adversarial safety prompts, suggesting that reasoning itself
can be exploited as an attack vector. In addition, preliminary results with
T-GCG show that simulated annealing can diversify adversarial search and
achieve competitive ASR under prefix evaluation, though its benefits under
semantic judgment remain limited. Together, these findings highlight the
scalability limits of GCG, expose overlooked vulnerabilities in reasoning
tasks, and motivate further development of annealing-inspired strategies for
more robust adversarial evaluation.

</details>


### [16] [MedSEBA: Synthesizing Evidence-Based Answers Grounded in Evolving Medical Literature](https://arxiv.org/abs/2509.00414)
*Juraj Vladika,Florian Matthes*

Main category: cs.CL

TL;DR: MedSEBA是一个AI驱动的交互式系统，利用大型语言模型从PubMed动态检索并整合可信医学研究，为医学问题提供基于证据的答案，并展示研究共识随时间演变的情况。


<details>
  <summary>Details</summary>
Motivation: 在数字时代，人们难以区分网上医学信息的可靠性；每年发表数百万医学研究，研究人员难以追踪最新发现和其可能存在的不同结论；传统搜索工具无法反映这些不断演变的研究结果。

Method: 开发了MedSEBA，一个交互式AI系统，它利用大型语言模型生成连贯且富有表达力的答案，并通过从PubMed检索的可信医学研究来支撑这些答案。答案包含可追溯到具体研究的关键点和论据，并提供相关研究支持或驳斥特定医学主张的程度概述，以及研究共识随时间演变的视觉化呈现。

Result: 用户研究表明，医学专家和普通用户都认为该系统易于使用、有帮助，且提供的答案值得信赖和信息丰富。这使得该系统非常适合日常健康咨询和高级研究洞察。

Conclusion: MedSEBA通过提供基于证据、可追溯且能展示研究共识演变的答案，有效解决了在线医疗信息可靠性低和研究追踪困难的挑战，为医学专业人士和公众提供了有价值的工具。

Abstract: In the digital age, people often turn to the Internet in search of medical
advice and recommendations. With the increasing volume of online content, it
has become difficult to distinguish reliable sources from misleading
information. Similarly, millions of medical studies are published every year,
making it challenging for researchers to keep track of the latest scientific
findings. These evolving studies can reach differing conclusions, which is not
reflected in traditional search tools. To address these challenges, we
introduce MedSEBA, an interactive AI-powered system for synthesizing
evidence-based answers to medical questions. It utilizes the power of Large
Language Models to generate coherent and expressive answers, but grounds them
in trustworthy medical studies dynamically retrieved from the research database
PubMed. The answers consist of key points and arguments, which can be traced
back to respective studies. Notably, the platform also provides an overview of
the extent to which the most relevant studies support or refute the given
medical claim, and a visualization of how the research consensus evolved
through time. Our user study revealed that medical experts and lay users find
the system usable and helpful, and the provided answers trustworthy and
informative. This makes the system well-suited for both everyday health
questions and advanced research insights.

</details>


### [17] [The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang](https://arxiv.org/abs/2509.00425)
*Fenghua Liu,Yulong Chen,Yixuan Liu,Zhujun Jin,Solomon Tsai,Ming Zhong*

Main category: cs.CL

TL;DR: 本研究通过让大型语言模型 (LLMs) 学习一种新构造语言Camlang，并进行认知科学范式下的元语言演绎学习测试。结果显示，LLMs在Camlang上的表现远低于人类，表明其缺乏系统性的语法掌握和真正的元语言能力，而主要依赖浅层词汇匹配。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多基准测试中表现出色，但其成功是否反映了真正的推理能力而非模式匹配仍不明确。从认知科学角度，测试模型能否通过元语言演绎学习掌握一门陌生语言，是衡量其能否内化语法系统的有效方式，这在现有基准测试中尚未充分探索。

Method: 研究引入了Camlang，一种具有自然但未被验证特征组合的新构造语言，并为其提供语法书和双语词典，模拟人类二语学习过程。通过将CommonsenseQA改编为Camlang-CSQA-v0任务，评估LLMs（包括GPT-5）在此任务上的表现。同时，进行人类实验建立基线，并对模型成功案例进行人工验证，以区分浅层词汇对齐和系统性语法掌握。

Result: GPT-5在英语任务中准确率达98%，但在Camlang中仅为47%，远低于人类的87%。其他最先进的推理LLMs表现更差。人工验证发现，模型大部分成功归因于浅层词汇对齐，GPT-5虽显示出有限的元语言意识，但未展现出与人类相当的系统性语法掌握。

Conclusion: Camlang提供了一个认知科学基础的评估范式，揭示了当前LLMs与人类元语言能力之间存在的根本性差距。LLMs在处理需要系统性语法掌握和元语言推理的陌生语言时，仍远未达到人类水平。

Abstract: Large Language Models (LLMs) achieve gold-medal performance across many
benchmarks, yet it remains unclear whether such success reflects genuine
reasoning or pattern matching. From a cognitive science perspective, an
informative test is whether models can master an unfamiliar language through
explicit metalinguistic deductive learning, a paradigm where human learners can
reliably internalise grammatical systems through metalinguistic reasoning. We
address this question with Camlang, a novel constructed language that exhibits
naturalistic yet unattested feature combinations. Camlang consists of two
explicit resources, a grammar book and a bilingual dictionary, which mirror
adult second-language learning via explicit grammar rules and lexical lookup,
and enable us to disentangle errors in morpho-syntax, lexical semantics, and
sentence-level reasoning. Human experiments show that these resources are
sufficient for participants to acquire Camlang and successfully solve Camlang
tasks. To operationalise evaluation, we adapt CommonsenseQA into Camlang,
creating Camlang-CSQA-v0, the first task in a broader suite where solving
questions requires applying grammar rules and lexical mappings. Experimental
results show that GPT-5 achieves 98\% EM accuracy in English but only 47\% in
Camlang, far below human performance at 87\%, while other state-of-the-art
reasoning LLMs perform even worse. Human verification further reveals that most
model successes stem from shallow lexical alignment while GPT-5 shows emerging
metalinguistic awareness to a limited extent but not systematic grammatical
mastery as humans. Camlang establishes a cognitively grounded evaluation
paradigm that exposes fundamental gaps between current models and human
metalinguistic competence.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [18] [Deep Learning-Driven Multimodal Detection and Movement Analysis of Objects in Culinary](https://arxiv.org/abs/2509.00033)
*Tahoshin Alam Ishat*

Main category: cs.CV

TL;DR: 该研究结合YOLOv8、LSTM和ASR模型提取数据，输入至LLM（TinyLLaMa）以预测食谱并生成分步烹饪指南，展示计算机视觉在日常任务中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个鲁棒且特定任务的系统，使其在复杂和挑战性环境中表现最佳。同时，验证计算机视觉在日常活动（如厨房工作）中的广泛应用和无限扩展性。

Method: 该研究探索并微调了现有模型，包括YOLOv8分割模型、基于手部点运动序列训练的LSTM模型以及ASR（Whisper-base）模型。这些模型被用于提取足够数据，以供大型语言模型（TinyLLaMa）预测食谱并生成分步烹饪指南。所有任务特定数据均由作者自行收集。

Result: 成功构建了一个系统，能够通过多模态数据输入（计算机视觉、手势、语音）使LLM（TinyLLaMa）预测食谱并生成详细的烹饪步骤指南。这证明了计算机视觉在日常活动（如厨房工作）中的有效扩展和广泛应用。

Conclusion: 本研究有效展示了计算机视觉在结合其他AI模型后，在处理日常复杂任务（如厨房烹饪）方面的巨大潜力，并为将此技术扩展到更多日常关键任务铺平了道路。

Abstract: This is a research exploring existing models and fine tuning them to combine
a YOLOv8 segmentation model, a LSTM model trained on hand point motion sequence
and a ASR (whisper-base) to extract enough data for a LLM (TinyLLaMa) to
predict the recipe and generate text creating a step by step guide for the
cooking procedure. All the data were gathered by the author for a robust task
specific system to perform best in complex and challenging environments proving
the extension and endless application of computer vision in daily activities
such as kitchen work. This work extends the field for many more crucial task of
our day to day life.

</details>


### [19] [AMMKD: Adaptive Multimodal Multi-teacher Distillation for Lightweight Vision-Language Models](https://arxiv.org/abs/2509.00039)
*Yuqi Li,Chuanguang Yang,Junhao Dong,Zhengtao Yao,Haoyan Xu,Zeyu Dong,Hansheng Zeng,Zhulin An,Yingli Tian*

Main category: cs.CV

TL;DR: 针对大型视觉语言预训练模型在移动设备上部署受限的问题，本文提出了AMMKD框架，通过多模态特征融合、多教师知识蒸馏和自适应优化，实现了轻量级且高性能的图像-文本检索模型，显著降低了模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言预训练（VLP）模型在图像-文本检索任务上表现出色，但由于模型庞大和计算复杂性高，其在移动设备上的部署受到限制。因此，需要开发轻量级而有效的检索模型。

Method: 本文提出自适应多模态多教师知识蒸馏（AMMKD）框架。具体方法包括：1) 使用特征融合网络提取并融合图像和文本模态的判别性特征。2) 设计多教师知识蒸馏框架，利用两个预训练的CLIP教师模型进行蒸馏，并通过预计算并存储文本特征作为类别向量来解耦模态以提高效率。3) 应用KL散度进行概率分布匹配，以更好地对齐教师和学生的输出。4) 设计自适应动态加权方案，将多教师蒸馏视为多目标优化问题，通过利用梯度空间多样性动态调整每个教师的影响，以减少冲突并引导学生学习。

Result: 在三个基准数据集上进行的广泛实验表明，AMMKD在显著降低模型复杂度的同时，取得了优越的性能，验证了其有效性和灵活性。

Conclusion: AMMKD是一个有效且灵活的框架，能够为图像-文本检索任务提供轻量级而高性能的模型，解决了大型VLP模型在移动设备部署上的挑战。

Abstract: The success of large-scale visual language pretraining (VLP) models has
driven widespread adoption of image-text retrieval tasks. However, their
deployment on mobile devices remains limited due to large model sizes and
computational complexity. We propose Adaptive Multi-Modal Multi-Teacher
Knowledge Distillation (AMMKD), a novel framework that integrates multi-modal
feature fusion, multi-teacher distillation, and adaptive optimization to
deliver lightweight yet effective retrieval models. Specifically, our method
begins with a feature fusion network that extracts and merges discriminative
features from both the image and text modalities. To reduce model parameters
and further improve performance, we design a multi-teacher knowledge
distillation framework to pre-train two CLIP teacher models. We decouple
modalities by pre-computing and storing text features as class vectors via the
teacher text encoder to enhance efficiency. To better align teacher and student
outputs, we apply KL scatter for probability distribution matching. Finally, we
design an adaptive dynamic weighting scheme that treats multi-teacher
distillation as a multi-objective optimization problem. By leveraging gradient
space diversity, we dynamically adjust the influence of each teacher, reducing
conflicts and guiding the student toward more optimal learning directions.
Extensive experiments on three benchmark datasets demonstrate that AMMKD
achieves superior performance while significantly reducing model complexity,
validating its effectiveness and flexibility.

</details>


### [20] [ARTPS: Depth-Enhanced Hybrid Anomaly Detection and Learnable Curiosity Score for Autonomous Rover Target Prioritization](https://arxiv.org/abs/2509.00042)
*Poyraz Baydemir*

Main category: cs.CV

TL;DR: 提出ARTPS，一种结合深度估计、异常检测和可学习好奇心评分的混合AI系统，用于行星表面自主探索，并实现了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 为行星表面自主探索开发一个高效的目标优先级排序系统，以提高探索效率和应对复杂环境的能力。

Method: 开发了ARTPS（自主漫游车目标优先级系统），一个混合AI系统。该系统整合了使用Vision Transformers的单目深度估计、多组件异常检测，以及一个平衡已知价值、异常信号、深度方差和表面粗糙度的加权好奇心评分。通过消融研究来分析各组件的贡献。

Result: 在火星探测器数据集上，该系统实现了0.94的AUROC、0.89的AUPRC和0.87的F1分数，达到最先进的性能。混合融合方法在保持高检测灵敏度的同时，将假阳性减少了23%。

Conclusion: ARTPS的混合融合方法显著提高了目标优先级排序的准确性，有效减少了假阳性，并在多样化的行星地形中展现出强大的探测能力和鲁棒性。

Abstract: We present ARTPS (Autonomous Rover Target Prioritization System), a novel
hybrid AI system that combines depth estimation, anomaly detection, and
learnable curiosity scoring for autonomous exploration of planetary surfaces.
Our approach integrates monocular depth estimation using Vision Transformers
with multi-component anomaly detection and a weighted curiosity score that
balances known value, anomaly signals, depth variance, and surface roughness.
The system achieves state-of-the-art performance with AUROC of 0.94, AUPRC of
0.89, and F1-Score of 0.87 on Mars rover datasets. We demonstrate significant
improvements in target prioritization accuracy through ablation studies and
provide comprehensive analysis of component contributions. The hybrid fusion
approach reduces false positives by 23% while maintaining high detection
sensitivity across diverse terrain types.

</details>


### [21] [Performance is not All You Need: Sustainability Considerations for Algorithms](https://arxiv.org/abs/2509.00045)
*Xiang Li,Chong Zhang,Hongpeng Wang,Shreyank Narayana Gowda,Yushi Li,Xiaobo Jin*

Main category: cs.CV

TL;DR: 针对深度学习高能耗问题，本文提出一个创新的二维可持续性评估系统，包含FMS和ASC两个指标，旨在量化平衡算法性能与能耗。该系统通过多模态任务验证了其通用性，为绿色AI研究和行业能效标准制定提供了量化依据和方法论支持。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型训练产生大量碳排放，核心挑战在于如何在算法性能和能源消耗之间取得平衡，缺乏有效的量化评估方法。

Method: 本文提出一个创新的二维可持续性评估系统，包含两个量化指标：1) 可持续调和平均值（FMS），通过调和平均数整合累积能耗和性能参数，揭示单位能耗下的算法表现；2) 可持续性曲线下面积（ASC），构建性能-功耗曲线，表征算法全周期的能效特性。为验证通用性，研究在图像分类、分割、姿态估计等多种多模态任务以及批量和在线学习中构建了基准。

Result: 实验结果表明，该系统能够为跨任务算法评估提供量化依据。

Conclusion: 该可持续性评估系统能有效推动绿色AI研究从理论走向实践，并为行业建立算法能效标准提供方法论支持。

Abstract: This work focuses on the high carbon emissions generated by deep learning
model training, specifically addressing the core challenge of balancing
algorithm performance and energy consumption. It proposes an innovative
two-dimensional sustainability evaluation system. Different from the
traditional single performance-oriented evaluation paradigm, this study
pioneered two quantitative indicators that integrate energy efficiency ratio
and accuracy: the sustainable harmonic mean (FMS) integrates accumulated energy
consumption and performance parameters through the harmonic mean to reveal the
algorithm performance under unit energy consumption; the area under the
sustainability curve (ASC) constructs a performance-power consumption curve to
characterize the energy efficiency characteristics of the algorithm throughout
the cycle. To verify the universality of the indicator system, the study
constructed benchmarks in various multimodal tasks, including image
classification, segmentation, pose estimation, and batch and online learning.
Experiments demonstrate that the system can provide a quantitative basis for
evaluating cross-task algorithms and promote the transition of green AI
research from theory to practice. Our sustainability evaluation framework code
can be found here, providing methodological support for the industry to
establish algorithm energy efficiency standards.

</details>


### [22] [MESTI-MEGANet: Micro-expression Spatio-Temporal Image and Micro-expression Gradient Attention Networks for Micro-expression Recognition](https://arxiv.org/abs/2509.00056)
*Luu Tu Nguyen,Vu Tram Anh Khuong,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: 本文提出了一种新型微表情时空图像（MESTI）输入模态和微表情梯度注意力网络（MEGANet），结合两者显著提升了微表情识别（MER）的性能，并在CASMEII和SAMM数据集上取得了迄今为止最高的准确率，建立了新的基准。


<details>
  <summary>Details</summary>
Motivation: 微表情识别（MER）因其细微和短暂的特性而极具挑战性。传统的输入模态（如Apex Frame、光流、动态图像）未能有效捕捉这些短暂的面部运动，导致识别性能不佳。

Method: 本文引入了微表情时空图像（MESTI），一种将视频序列转换为单一图像，同时保留微动作核心特征的新型动态输入模态。此外，还提出了微表情梯度注意力网络（MEGANet），该网络包含一个新颖的梯度注意力模块，用于增强微表情的细粒度运动特征提取。通过结合MESTI和MEGANet来建立更有效的MER方法。

Result: 通过广泛实验证明，MESTI在与现有输入模态进行比较时，在三种CNN架构（VGG19、ResNet50和EfficientNetB0）下均表现出更优越的性能。将MESTI应用于已发布的MER网络输入，能带来一致性的性能提升。MEGANet（无论结合MESTI还是动态图像）在CASMEII和SAMM数据集上均取得了最先进（SOTA）的识别结果。MEGANet与MESTI的组合达到了迄今为止报告的最高准确率，为微表情识别设定了新基准。

Conclusion: MESTI作为一种卓越的输入模态，以及MEGANet作为一种先进的识别网络，其潜力得到了充分证实。两者的结合为开发更有效的微表情识别系统铺平了道路，在各种应用中具有广阔前景。

Abstract: Micro-expression recognition (MER) is a challenging task due to the subtle
and fleeting nature of micro-expressions. Traditional input modalities, such as
Apex Frame, Optical Flow, and Dynamic Image, often fail to adequately capture
these brief facial movements, resulting in suboptimal performance. In this
study, we introduce the Micro-expression Spatio-Temporal Image (MESTI), a novel
dynamic input modality that transforms a video sequence into a single image
while preserving the essential characteristics of micro-movements.
Additionally, we present the Micro-expression Gradient Attention Network
(MEGANet), which incorporates a novel Gradient Attention block to enhance the
extraction of fine-grained motion features from micro-expressions. By combining
MESTI and MEGANet, we aim to establish a more effective approach to MER.
Extensive experiments were conducted to evaluate the effectiveness of MESTI,
comparing it with existing input modalities across three CNN architectures
(VGG19, ResNet50, and EfficientNetB0). Moreover, we demonstrate that replacing
the input of previously published MER networks with MESTI leads to consistent
performance improvements. The performance of MEGANet, both with MESTI and
Dynamic Image, is also evaluated, showing that our proposed network achieves
state-of-the-art results on the CASMEII and SAMM datasets. The combination of
MEGANet and MESTI achieves the highest accuracy reported to date, setting a new
benchmark for micro-expression recognition. These findings underscore the
potential of MESTI as a superior input modality and MEGANet as an advanced
recognition network, paving the way for more effective MER systems in a variety
of applications.

</details>


### [23] [Scaffold Diffusion: Sparse Multi-Category Voxel Structure Generation with Discrete Diffusion](https://arxiv.org/abs/2509.00062)
*Justin Jung*

Main category: cs.CV

TL;DR: 本文提出Scaffold Diffusion，一个利用离散扩散语言模型生成稀疏多类别3D体素结构的生成模型，解决了高稀疏度下的生成难题，并展现了其在生成逼真连贯结构方面的优越性。


<details>
  <summary>Details</summary>
Motivation: 生成逼真的稀疏多类别3D体素结构面临挑战，主要原因在于体素结构的内存占用呈立方增长，以及稀疏性导致的显著类别不平衡问题。

Method: 引入Scaffold Diffusion生成模型，该模型将体素视为tokens，并利用离散扩散语言模型来生成3D体素结构。文章证明离散扩散语言模型可以从文本等固有的序列领域扩展到生成空间连贯的3D结构。

Result: 在3D-Craft数据集的Minecraft房屋结构上进行评估，结果显示Scaffold Diffusion模型能够生成逼真且连贯的结构，即使在数据稀疏度超过98%的情况下也表现出色，优于现有基线和自回归方法。

Conclusion: 研究结果突出表明，离散扩散是一个有前景的3D稀疏体素生成建模框架。

Abstract: Generating realistic sparse multi-category 3D voxel structures is difficult
due to the cubic memory scaling of voxel structures and moreover the
significant class imbalance caused by sparsity. We introduce Scaffold
Diffusion, a generative model designed for sparse multi-category 3D voxel
structures. By treating voxels as tokens, Scaffold Diffusion uses a discrete
diffusion language model to generate 3D voxel structures. We show that discrete
diffusion language models can be extended beyond inherently sequential domains
such as text to generate spatially coherent 3D structures. We evaluate on
Minecraft house structures from the 3D-Craft dataset and demonstrate that,
unlike prior baselines and an auto-regressive formulation, Scaffold Diffusion
produces realistic and coherent structures even when trained on data with over
98% sparsity. We provide an interactive viewer where readers can visualize
generated samples and the generation process. Our results highlight discrete
diffusion as a promising framework for 3D sparse voxel generative modeling.

</details>


### [24] [Dual-Stage Global and Local Feature Framework for Image Dehazing](https://arxiv.org/abs/2509.00108)
*Anas M. Ali,Anis Koubaa,Bilel Benjdira*

Main category: cs.CV

TL;DR: 本文提出了一种名为SGLC的新型框架，通过结合全局和局部特征，有效解决了高分辨率图像去雾的挑战，显著提升了去雾性能并可应用于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前去雾模型在高分辨率图像上表现不佳，常需降采样或分块处理导致性能下降。主要原因是难以在高分辨率下有效结合全局上下文信息与局部精细细节。

Method: 提出Streamlined Global and Local Features Combinator (SGLC) 框架，包含两部分：全局特征生成器(GFG)负责初步去雾和场景理解，以及局部特征增强器(LFE)用于细化局部细节和像素级特征。该方法可与任意去雾网络结合（如Uformer）。

Result: 在高分辨率数据集上的实验结果表明，SGLC显著提升了峰值信噪比(PSNR)，证明其在高分辨率图像去雾方面的强大能力。其模型无关性使其能增强任何去雾网络。

Conclusion: SGLC框架通过有效地融合场景级线索和细节信息，显著改善了高分辨率环境中的图像去雾视觉保真度，为高分辨率去雾提供了鲁棒且通用的解决方案。

Abstract: Addressing the challenge of removing atmospheric fog or haze from digital
images, known as image dehazing, has recently gained significant traction in
the computer vision community. Although contemporary dehazing models have
demonstrated promising performance, few have thoroughly investigated
high-resolution imagery. In such scenarios, practitioners often resort to
downsampling the input image or processing it in smaller patches, which leads
to a notable performance degradation. This drop is primarily linked to the
difficulty of effectively combining global contextual information with
localized, fine-grained details as the spatial resolution grows. In this
chapter, we propose a novel framework, termed the Streamlined Global and Local
Features Combinator (SGLC), to bridge this gap and enable robust dehazing for
high-resolution inputs. Our approach is composed of two principal components:
the Global Features Generator (GFG) and the Local Features Enhancer (LFE). The
GFG produces an initial dehazed output by focusing on broad contextual
understanding of the scene. Subsequently, the LFE refines this preliminary
output by enhancing localized details and pixel-level features, thereby
capturing the interplay between global appearance and local structure. To
evaluate the effectiveness of SGLC, we integrated it with the Uformer
architecture, a state-of-the-art dehazing model. Experimental results on
high-resolution datasets reveal a considerable improvement in peak
signal-to-noise ratio (PSNR) when employing SGLC, indicating its potency in
addressing haze in large-scale imagery. Moreover, the SGLC design is
model-agnostic, allowing any dehazing network to be augmented with the proposed
global-and-local feature fusion mechanism. Through this strategy, practitioners
can harness both scene-level cues and granular details, significantly improving
visual fidelity in high-resolution environments.

</details>


### [25] [Self-supervised large-scale kidney abnormality detection in drug safety assessment studies](https://arxiv.org/abs/2509.00131)
*Ivan Slootweg,Natalia P. García-De-La-Puente,Geert Litjens,Salma Dammak*

Main category: cs.CV

TL;DR: 研究开发并评估了首个大规模自监督肾脏毒理病理异常检测模型，旨在提高药物安全评估的效率。


<details>
  <summary>Details</summary>
Motivation: 药物临床前开发中的肾脏异常检测过程耗时且成本高昂，需要人工检查数千张全切片图像（其中大部分是正常的），以发现指示毒性作用的细微变化。

Method: 该研究利用UNI基础模型（FM）提取特征，并首先探索了使用简单k-近邻分类器在这些特征上的性能。随后，将自监督方法应用于相同的特征，构建了一个用于肾脏毒理病理学异常检测的模型，该模型使用了来自158种化合物的药物安全评估研究数据。

Result: 结果显示，单独使用UNI基础模型生成的特征结合简单k-近邻分类器时，其性能接近随机，表明FM特征本身不足以检测异常。然而，将自监督方法应用于相同特征后，模型性能优于随机，取得了0.62的接收者操作特征曲线下面积（AUC）和89%的阴性预测值（NPV）。

Conclusion: 该自监督模型在进一步开发后，有望用于药物安全评估研究中排除正常切片，从而显著降低药物开发的相关成本和时间。

Abstract: Kidney abnormality detection is required for all preclinical drug
development. It involves a time-consuming and costly examination of hundreds to
thousands of whole-slide images per drug safety study, most of which are
normal, to detect any subtle changes indicating toxic effects. In this study,
we present the first large-scale self-supervised abnormality detection model
for kidney toxicologic pathology, spanning drug safety assessment studies from
158 compounds. We explore the complexity of kidney abnormality detection on
this scale using features extracted from the UNI foundation model (FM) and show
that a simple k-nearest neighbor classifier on these features performs at
chance, demonstrating that the FM-generated features alone are insufficient for
detecting abnormalities. We then demonstrate that a self-supervised method
applied to the same features can achieve better-than-chance performance, with
an area under the receiver operating characteristic curve of 0.62 and a
negative predictive value of 89%. With further development, such a model can be
used to rule out normal slides in drug safety assessment studies, reducing the
costs and time associated with drug development.

</details>


### [26] [Waste-Bench: A Comprehensive Benchmark for Evaluating VLLMs in Cluttered Environments](https://arxiv.org/abs/2509.00176)
*Muhammad Ali,Salman Khan*

Main category: cs.CV

TL;DR: VLLMs在标准图像上表现出色，但在复杂、杂乱且包含变形物体的环境中尚未充分探索。本文引入一个针对真实世界垃圾分类的新数据集和评估方法，旨在评估VLLM在此类挑战性环境下的鲁棒性，并指出VLLM需进一步提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管VLLMs在标准自然图像上表现出色，但其在复杂环境、杂乱数据集以及包含变形物体的场景（如真实世界垃圾分类）中的能力和鲁棒性尚未得到充分探索。

Method: 引入了一个针对真实世界垃圾分类的新型数据集，其特点是复杂环境和变形物体。同时，提出了一种深入的评估方法，用于严格评估VLLMs的鲁棒性和准确性。

Result: 引入的数据集和全面的分析为VLLMs在挑战性条件下的性能提供了有价值的见解。

Conclusion: 研究结果强调VLLMs在复杂环境中表现更好需要进一步提升鲁棒性。该数据集和实验代码将公开发布。

Abstract: Recent advancements in Large Language Models (LLMs) have paved the way for
Vision Large Language Models (VLLMs) capable of performing a wide range of
visual understanding tasks. While LLMs have demonstrated impressive performance
on standard natural images, their capabilities have not been thoroughly
explored in cluttered datasets where there is complex environment having
deformed shaped objects. In this work, we introduce a novel dataset
specifically designed for waste classification in real-world scenarios,
characterized by complex environments and deformed shaped objects. Along with
this dataset, we present an in-depth evaluation approach to rigorously assess
the robustness and accuracy of VLLMs. The introduced dataset and comprehensive
analysis provide valuable insights into the performance of VLLMs under
challenging conditions. Our findings highlight the critical need for further
advancements in VLLM's robustness to perform better in complex environments.
The dataset and code for our experiments will be made publicly available.

</details>


### [27] [Category-level Text-to-Image Retrieval Improved: Bridging the Domain Gap with Diffusion Models and Vision Encoders](https://arxiv.org/abs/2509.00177)
*Faizan Farooq Khan,Vladan Stojnić,Zakaria Laskar,Mohamed Elhoseiny,Giorgos Tolias*

Main category: cs.CV

TL;DR: 本文提出一种两步法进行语义类别文本到图像检索：首先利用生成扩散模型将文本查询转换为视觉查询，然后通过视觉模型计算图像间相似度，并引入聚合网络融合多模态分数，显著优于仅依赖文本查询的方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如CLIP）在文本到图像检索中，将文本和图像映射到表示空间中距离较远的区域，限制了检索性能，尤其是在处理指定或描述语义类别的查询时存在模态鸿沟。

Method: 本研究提出一种两步法来弥合模态鸿沟：1. 使用生成扩散模型将文本查询转换为视觉查询。2. 使用视觉模型评估图像到图像的相似性。此外，还引入了一个聚合网络，用于将多个生成的图像组合成单个向量表示，并融合跨两种查询模态的相似性分数。该方法利用了视觉编码器、视觉语言模型和文本到图像生成模型的最新进展。

Result: 通过广泛的评估，该方法在文本到图像检索方面始终优于仅依赖文本查询的传统方法。

Conclusion: 所提出的结合了文本到视觉生成和图像到图像相似度计算的两步法，辅以聚合网络，能有效弥合文本与图像之间的模态鸿沟，显著提升了语义类别文本到图像检索的性能。

Abstract: This work explores text-to-image retrieval for queries that specify or
describe a semantic category. While vision-and-language models (VLMs) like CLIP
offer a straightforward open-vocabulary solution, they map text and images to
distant regions in the representation space, limiting retrieval performance. To
bridge this modality gap, we propose a two-step approach. First, we transform
the text query into a visual query using a generative diffusion model. Then, we
estimate image-to-image similarity with a vision model. Additionally, we
introduce an aggregation network that combines multiple generated images into a
single vector representation and fuses similarity scores across both query
modalities. Our approach leverages advancements in vision encoders, VLMs, and
text-to-image generation models. Extensive evaluations show that it
consistently outperforms retrieval methods relying solely on text queries.
Source code is available at: https://github.com/faixan-khan/cletir

</details>


### [28] [Safe-LLaVA: A Privacy-Preserving Vision-Language Dataset and Benchmark for Biometric Safety](https://arxiv.org/abs/2509.00192)
*Younggun Kim,Sirnam Swetha,Fazil Kagdi,Mubarak Shah*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLMs）存在生物特征信息泄露问题，本文提出了PRISM基准测试和Safe-LLaVA数据集来评估和缓解这一隐私风险，并证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在未被明确要求的情况下，仍会推断并泄露敏感的生物特征属性（如种族、性别、年龄等），这在实际应用和敏感领域中引发了严重隐私担忧。目前缺乏公开的综合性数据集或基准来评估或缓解MLLMs中的生物特征泄露问题。

Method: 本文引入了PRISM基准测试，用于评估MLLMs在(1)拒绝生物特征相关查询和(2)在通用响应中隐式生物特征泄露方面的表现。此外，对广泛使用的LLaVA数据集进行了详细审计，发现了预训练和指令数据中存在的大量生物特征泄露。为解决此问题，构建了Safe-LLaVA数据集，这是首个通过系统性移除LLaVA数据集中显式和隐式生物特征信息而构建的隐私保护MLLM训练数据集。

Result: 通过PRISM评估发现，MLLMs普遍存在不同属性的生物特征信息泄露问题，凸显了详细的隐私侵犯。在Safe-LLaVA数据集上微调的模型显著减少了生物特征信息泄露。

Conclusion: Safe-LLaVA数据集和PRISM基准测试共同为MLLMs的隐私对齐开发和评估设定了新标准。这两个资源已公开可用，以促进隐私保护的MLLM发展。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in vision-language tasks. However, these models often infer and
reveal sensitive biometric attributes - such as race, gender, age, body weight,
and eye color - even when such information is not explicitly requested. This
raises critical concerns, particularly in real-world applications and
socially-sensitive domains. Despite increasing awareness, no publicly available
dataset or benchmark exists to comprehensively evaluate or mitigate biometric
leakage in MLLMs. To address this gap, we introduce PRISM (Privacy-aware
Evaluation of Responses in Sensitive Modalities), a new benchmark designed to
assess MLLMs on two fronts: (1) refuse biometric-related queries and (2)
implicit biometric leakage in general responses while maintaining semantic
faithfulness. Further, we conduct a detailed audit of the widely used LLaVA
datasets and uncover extensive biometric leakage across pretraining and
instruction data. To address this, we present Safe-LLaVA dataset, the first
privacy-preserving MLLM training dataset constructed by systematically removing
explicit and implicit biometric information from LLaVA dataset. Our evaluations
on PRISM reveal biometric leakages across MLLMs for different attributes,
highlighting the detailed privacy-violations. We also fine-tune a model on
Safe-LLaVA dataset and show that it substantially reduces the biometric
leakages. Together, Safe-LLaVA & PRISM set a new standard for privacy-aligned
development and evaluation of MLLMs. The Safe-LLaVA dataset & PRISM benchmark
are publicly available at https://huggingface.co/datasets/kyh9191/Safe-LLaVA,
and the source code is available at
https://github.com/Kimyounggun99/Safe-LLaVA.git.

</details>


### [29] [Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment](https://arxiv.org/abs/2509.00210)
*Jinzhou Tang,Jusheng zhang,Sidi Liu,Waikit Xiu,Qinhan Lv,Xiying Li*

Main category: cs.CV

TL;DR: 本文提出VEME方法，通过跨模态对齐和以自我为中心的经验世界模型，显著提升了具身智能在未知动态环境中的时空推理和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在具身智能的复杂任务中，难以实现类人推理，尤其在未知动态环境下的时空推理、适应性以及对精细时空线索和物理世界的理解不足，限制了任务型导航和具身问答等应用。

Method: 本文提出VEME，一种新颖的跨模态对齐方法，通过学习以自我为中心、以经验为中心的世界模型来增强未见场景的泛化能力。该框架包含三个核心组件：1) 结合物体、空间表征和视觉语义与时空线索的跨模态对齐框架，以增强VLM的上下文学习；2) 由世界嵌入激活的动态隐式认知地图，用于任务相关的几何-语义记忆召回；3) 基于指令的导航与推理框架，利用具身先验进行长期规划和高效探索。

Result: 在VSI-Bench和VLN-CE基准测试中，VEME方法相比传统方法，在准确性和探索效率方面实现了1%-3%的提升。

Conclusion: 通过嵌入几何感知的时空情景经验，VEME方法显著改善了模型在动态环境中的推理和规划能力，有效解决了具身智能在复杂未知环境中的关键挑战。

Abstract: Achieving human-like reasoning in deep learning models for complex tasks in
unknown environments remains a critical challenge in embodied intelligence.
While advanced vision-language models (VLMs) excel in static scene
understanding, their limitations in spatio-temporal reasoning and adaptation to
dynamic, open-set tasks like task-oriented navigation and embodied question
answering (EQA) persist due to inadequate modeling of fine-grained
spatio-temporal cues and physical world comprehension. To address this, we
propose VEME, a novel cross-modal alignment method that enhances generalization
in unseen scenes by learning an ego-centric, experience-centered world model.
Our framework integrates three key components: (1) a cross-modal alignment
framework bridging objects, spatial representations, and visual semantics with
spatio-temporal cues to enhance VLM in-context learning; (2) a dynamic,
implicit cognitive map activated by world embedding to enable task-relevant
geometric-semantic memory recall; and (3) an instruction-based navigation and
reasoning framework leveraging embodied priors for long-term planning and
efficient exploration. By embedding geometry-aware spatio-temporal episodic
experiences, our method significantly improves reasoning and planning in
dynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate
1%-3% accuracy and exploration efficiency improvement compared to traditional
approaches.

</details>


### [30] [Multimodal Deep Learning for Phyllodes Tumor Classification from Ultrasound and Clinical Data](https://arxiv.org/abs/2509.00213)
*Farhan Fuad Abir,Abigail Elliott Daly,Kyle Anderman,Tolga Ozmen,Laura J. Brattain*

Main category: cs.CV

TL;DR: 提出一个多模态深度学习框架，结合乳腺超声图像和临床数据，显著提高了叶状肿瘤的术前诊断准确性，旨在减少不必要的手术切除。


<details>
  <summary>Details</summary>
Motivation: 叶状肿瘤术前难以与良性纤维腺瘤区分，导致大量不必要的手术切除，亟需改进诊断准确性。

Method: 开发了一个双分支深度学习框架，融合来自81名患者的乳腺超声图像和结构化临床数据。采用类别平衡采样和分层5折交叉验证来处理类别不平衡和数据泄漏问题。

Result: 该多模态方法在区分良性与交界/恶性叶状肿瘤方面优于单模态基线。其中，ConvNeXt和ResNet18在多模态设置下表现最佳，AUC-ROC分别为0.9427和0.9349，F1-score分别为0.6720和0.7294。

Conclusion: 该研究表明，多模态AI作为一种非侵入性诊断工具，在乳腺肿瘤管理中具有巨大潜力，有助于减少不必要的活检并改善临床决策。

Abstract: Phyllodes tumors (PTs) are rare fibroepithelial breast lesions that are
difficult to classify preoperatively due to their radiological similarity to
benign fibroadenomas. This often leads to unnecessary surgical excisions. To
address this, we propose a multimodal deep learning framework that integrates
breast ultrasound (BUS) images with structured clinical data to improve
diagnostic accuracy. We developed a dual-branch neural network that extracts
and fuses features from ultrasound images and patient metadata from 81 subjects
with confirmed PTs. Class-aware sampling and subject-stratified 5-fold
cross-validation were applied to prevent class imbalance and data leakage. The
results show that our proposed multimodal method outperforms unimodal baselines
in classifying benign versus borderline/malignant PTs. Among six image
encoders, ConvNeXt and ResNet18 achieved the best performance in the multimodal
setting, with AUC-ROC scores of 0.9427 and 0.9349, and F1-scores of 0.6720 and
0.7294, respectively. This study demonstrates the potential of multimodal AI to
serve as a non-invasive diagnostic tool, reducing unnecessary biopsies and
improving clinical decision-making in breast tumor management.

</details>


### [31] [GraViT: Transfer Learning with Vision Transformers and MLP-Mixer for Strong Gravitational Lens Discovery](https://arxiv.org/abs/2509.00226)
*René Parlange,Juan C. Cuevas-Tello,Octavio Valenzuela,Omar de J. Cabrera-Rosas,Tomás Verdugo,Anupreeta More,Anton T. Jaelani*

Main category: cs.CV

TL;DR: 该研究引入了GraViT，一个基于PyTorch的引力透镜自动检测管道，利用预训练的Vision Transformer和MLP-Mixer模型及迁移学习，以应对LSST海量数据挑战，并提供了强引力透镜可检测性的见解。


<details>
  <summary>Details</summary>
Motivation: 引力透镜是探测暗物质属性和推断宇宙学参数的重要手段。即将到来的LSST调查预计将发现约10^5个引力透镜，这迫切需要自动化的分类器。

Method: 本研究引入了GraViT，一个利用最先进的Vision Transformer (ViT) 模型和MLP-Mixer进行广泛预训练的PyTorch管道。通过考察数据质量、模型架构、训练策略和集成预测，评估了迁移学习对分类性能的影响。该研究复现了之前神经网络系统比较的实验，并使用HOLISMOKES VI和SuGOHI X数据集对十种架构进行了微调，将其与卷积基线进行了基准测试，并分析了复杂性和推理时间。

Result: 该研究提供了在通用测试样本上强引力透镜可检测性的见解。通过与卷积基线对比，讨论了不同模型架构的复杂性和推理时间。

Conclusion: GraViT作为一种基于迁移学习和先进Vision Transformer模型的自动化分类器，在引力透镜检测方面展现了潜力，并为理解强引力透镜的可检测性提供了重要洞察。

Abstract: Gravitational lensing offers a powerful probe into the properties of dark
matter and is crucial to infer cosmological parameters. The Legacy Survey of
Space and Time (LSST) is predicted to find O(10^5) gravitational lenses over
the next decade, demanding automated classifiers. In this work, we introduce
GraViT, a PyTorch pipeline for gravitational lens detection that leverages
extensive pretraining of state-of-the-art Vision Transformer (ViT) models and
MLP-Mixer. We assess the impact of transfer learning on classification
performance by examining data quality (source and sample size), model
architecture (selection and fine-tuning), training strategies (augmentation,
normalization, and optimization), and ensemble predictions. This study
reproduces the experiments in a previous systematic comparison of neural
networks and provides insights into the detectability of strong gravitational
lenses on that common test sample. We fine-tune ten architectures using
datasets from HOLISMOKES VI and SuGOHI X, and benchmark them against
convolutional baselines, discussing complexity and inference-time analysis.

</details>


### [32] [A High-Accuracy Fast Hough Transform with Linear-Log-Cubed Computational Complexity for Arbitrary-Shaped Images](https://arxiv.org/abs/2509.00231)
*Danil Kazimirov,Dmitry Nikolaev*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The Hough transform (HT) is a fundamental tool across various domains, from
classical image analysis to neural networks and tomography. Two key aspects of
the algorithms for computing the HT are their computational complexity and
accuracy - the latter often defined as the error of approximation of continuous
lines by discrete ones within the image region. The fast HT (FHT) algorithms
with optimal linearithmic complexity - such as the Brady-Yong algorithm for
power-of-two-sized images - are well established. Generalizations like $FHT2DT$
extend this efficiency to arbitrary image sizes, but with reduced accuracy that
worsens with scale. Conversely, accurate HT algorithms achieve constant-bounded
error but require near-cubic computational cost. This paper introduces $FHT2SP$
algorithm - a fast and highly accurate HT algorithm. It builds on our
development of Brady's superpixel concept, extending it to arbitrary shapes
beyond the original power-of-two square constraint, and integrates it into the
$FHT2DT$ algorithm. With an appropriate choice of the superpixel's size, for an
image of shape $w \times h$, the $FHT2SP$ algorithm achieves near-optimal
computational complexity $\mathcal{O}(wh \ln^3 w)$, while keeping the
approximation error bounded by a constant independent of image size, and
controllable via a meta-parameter. We provide theoretical and experimental
analyses of the algorithm's complexity and accuracy.

</details>


### [33] [Generative AI for Industrial Contour Detection: A Language-Guided Vision System](https://arxiv.org/abs/2509.00284)
*Liang Gong,Tommy,Wang,Sara Chaker,Yanchen Dong,Fouad Bousetouane,Brenden Morton,Mark Mendez*

Main category: cs.CV

TL;DR: 该论文提出了一种语言引导的生成式视觉系统，用于工业制造中的残余轮廓检测，旨在实现CAD级别精度，并通过VLM指导的工作流克服传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统工业计算机视觉系统在噪声、材料多样性和不受控成像条件下，难以实现精确的轮廓检测，限制了经典边缘检测器和手工管道的有效性。

Method: 系统分为三阶段：数据采集与预处理；使用条件GAN进行轮廓生成；以及通过视觉-语言模型（VLM）进行多模态轮廓细化，其中人工参与（human-in-the-loop）制定标准化提示词，并通过图像-文本引导合成应用。在细化阶段，基准测试了包括Google Gemini 2.0 Flash和OpenAI GPT-image-1在内的多个VLM。

Result: 在FabTrack数据集上，所提出的系统提高了轮廓保真度，增强了边缘连续性和几何对齐，并减少了手动描迹。在细化阶段，GPT-image-1在结构精度和感知质量方面均优于Gemini 2.0 Flash。

Conclusion: 这些发现表明，VLM引导的生成式工作流有望推动工业计算机视觉超越传统管道的局限性。

Abstract: Industrial computer vision systems often struggle with noise, material
variability, and uncontrolled imaging conditions, limiting the effectiveness of
classical edge detectors and handcrafted pipelines. In this work, we present a
language-guided generative vision system for remnant contour detection in
manufacturing, designed to achieve CAD-level precision. The system is organized
into three stages: data acquisition and preprocessing, contour generation using
a conditional GAN, and multimodal contour refinement through vision-language
modeling, where standardized prompts are crafted in a human-in-the-loop process
and applied through image-text guided synthesis. On proprietary FabTrack
datasets, the proposed system improved contour fidelity, enhancing edge
continuity and geometric alignment while reducing manual tracing. For the
refinement stage, we benchmarked several vision-language models, including
Google's Gemini 2.0 Flash, OpenAI's GPT-image-1 integrated within a VLM-guided
workflow, and open-source baselines. Under standardized conditions, GPT-image-1
consistently outperformed Gemini 2.0 Flash in both structural accuracy and
perceptual quality. These findings demonstrate the promise of VLM-guided
generative workflows for advancing industrial computer vision beyond the
limitations of classical pipelines.

</details>


### [34] [Language-Aware Information Maximization for Transductive Few-Shot CLIP](https://arxiv.org/abs/2509.00305)
*Ghassen Baklouti,Maxime Zanella,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: 开发了一种基于信息论和PEFT的新型转导式小样本CLIP方法LIMO，在视觉-语言模型（VLM）的转导式小样本学习任务中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 转导式小样本学习在视觉模型中研究丰富，但在视觉-语言模型（VLMs）背景下仍处于早期，现有方法较少，迫切需要为VLM量身定制的转导式小样本方法。

Method: 该研究提出了一种名为LIMO（Language-aware Information MaximizatiOn）的损失函数，该函数整合了三个互补项：视觉输入与文本类别描述之间的互信息、惩罚网络概率输出偏离文本驱动零样本预测的KL散度，以及基于标注样本的标准交叉熵损失。此外，探索并应用了参数高效微调（PEFT）策略。

Result: LIMO在转导式小样本CLIP任务上显著优于最新的同类方法，并相对最佳归纳式方法取得了显著提升。研究还发现，之前被忽视的PEFT策略能大幅提升性能。

Conclusion: 该研究通过LIMO损失函数和对PEFT策略的有效利用，成功推动了VLM在转导式小样本学习领域的发展，确立了新的性能标杆，并揭示了PEFT在该设置中的巨大潜力。

Abstract: Transductive few-shot learning has triggered an abundant literature focusing
on vision-only models, but is still at a nascent stage within the recent
context of foundational vision-language models (VLMs). Only a few recent
methods addressed the problem, pointing to the potential of tranduction in VLMs
and to the need for VLM-tailored methods. Building on this momentum, we
leverage information-theoretic concepts and recent progress in
parameter-efficient fine-tuning (PEFT), developing a highly competitive
transductive few-shot CLIP method. Specifically, we introduce a novel
Language-aware Information MaximizatiOn (LIMO) loss integrating three
complementary terms: (i) the mutual information between the vision inputs and
the textual class descriptions; (ii) a Kullback-Leibler (KL) divergence
penalizing deviation of the network's probabilistic outputs from the
text-driven zero-shot predictions; and (iii) a standard cross-entropy loss
based on the labeled shots. Furthermore, we challenge the commonly followed
fine-tuning practices in the context of transductive few-shot learning, and
explore PEFT strategies, completely overlooked in this context. Surprisingly,
we observe substantial boosts in performances, which points to the potential of
adapting a subset of the model's parameters in the transductive few-shot
setting. We report comprehensive evaluations, which show that LIMO outperforms
the very recent transductive few-shot CLIP methods by a large margin and yields
significant gains over the best-performing inductive methods. Our code is
publicly available at:\[
\href{https://github.com/ghassenbaklouti/LIMO}{\text{here}} \]

</details>


### [35] [MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification](https://arxiv.org/abs/2509.00311)
*Hikmat Khan,Syed Farhan Alam Zaidi,Pir Masoom Shah,Kiruthika Balakrishnan,Rabia Khan,Muhammad Waqas,Jia Wu*

Main category: cs.CV

TL;DR: MorphGen是一种基于监督对比学习的方法，通过整合核分割掩模，明确建模细胞核形态和空间组织，以学习对病理图像域偏移和图像损坏具有鲁棒性的癌症表征，从而提高计算组织病理学中的域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 计算组织病理学中的域泛化能力受全玻片图像（WSIs）异质性（由组织制备、染色和成像条件引起）的阻碍。与机器学习系统不同，病理学家依赖域不变的形态学特征（如细胞核异型性、结构异型性）。本研究假设，明确建模生物学上鲁棒的细胞核形态和空间组织，将使学习到的癌症表征对域偏移具有弹性。

Method: 提出MorphGen方法，它在监督对比学习框架内整合组织病理图像、增强技术和核分割掩模。通过对齐图像和核掩模的潜在表征，MorphGen优先考虑细胞核和形态异型性、空间组织等诊断特征，而非染色伪影和域特异性特征。为进一步增强分布外（OOD）鲁棒性，该方法还结合了随机权重平均（SWA），以将优化引向更平坦的最小值。

Result: 注意力图分析显示，MorphGen主要依赖细胞核形态、细胞组成和肿瘤或正常区域内的空间细胞组织进行最终分类。研究证明，所学表征对图像损坏（如染色伪影）和对抗性攻击具有弹性，不仅展示了OOD泛化能力，还解决了当前数字病理深度学习系统中的关键漏洞。

Conclusion: MorphGen通过明确利用生物学上鲁棒的细胞核形态和空间组织，有效提高了计算组织病理学的域泛化能力和对图像损坏与对抗性攻击的鲁棒性，从而提升了数字病理深度学习系统的可靠性。

Abstract: Domain generalization in computational histopathology is hindered by
heterogeneity in whole slide images (WSIs), caused by variations in tissue
preparation, staining, and imaging conditions across institutions. Unlike
machine learning systems, pathologists rely on domain-invariant morphological
cues such as nuclear atypia (enlargement, irregular contours, hyperchromasia,
chromatin texture, spatial disorganization), structural atypia (abnormal
architecture and gland formation), and overall morphological atypia that remain
diagnostic across diverse settings. Motivated by this, we hypothesize that
explicitly modeling biologically robust nuclear morphology and spatial
organization will enable the learning of cancer representations that are
resilient to domain shifts. We propose MorphGen (Morphology-Guided
Generalization), a method that integrates histopathology images, augmentations,
and nuclear segmentation masks within a supervised contrastive learning
framework. By aligning latent representations of images and nuclear masks,
MorphGen prioritizes diagnostic features such as nuclear and morphological
atypia and spatial organization over staining artifacts and domain-specific
features. To further enhance out-of-distribution robustness, we incorporate
stochastic weight averaging (SWA), steering optimization toward flatter minima.
Attention map analyses revealed that MorphGen primarily relies on nuclear
morphology, cellular composition, and spatial cell organization within tumors
or normal regions for final classification. Finally, we demonstrate resilience
of the learned representations to image corruptions (such as staining
artifacts) and adversarial attacks, showcasing not only OOD generalization but
also addressing critical vulnerabilities in current deep learning systems for
digital pathology. Code, datasets, and trained models are available at:
https://github.com/hikmatkhan/MorphGen

</details>


### [36] [Towards Adaptive Visual Token Pruning for Large Multimodal Models](https://arxiv.org/abs/2509.00320)
*Hao Zhang,Mengsi Lyu,Chenrui He,Yulong Ao,Yonghua Lin*

Main category: cs.CV

TL;DR: 本文提出一种针对大型多模态模型（LMMs）的视觉token剪枝策略，通过保留跨模态对齐和模态内信息多样性，显著减少token数量（88.9%）并提升推理速度（56.7%），同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）将视觉输入编码为密集token序列，与文本token合并后导致token数量大幅增加，从而显著提升推理时的计算和内存成本。现有token剪枝方法存在校准成本高或重要性度量次优的问题，导致保留冗余token。

Method: 分析视觉和文本token的冗余差异，并提出仅对视觉token进行剪枝。具体策略包括：1) 基于互信息移除与文本token语义未对齐的视觉token，以保留跨模态对齐；2) 通过最大化嵌入空间中的预期成对距离，利用贪婪算法剪枝冗余视觉token，以提高保留token的表征质量和模态内多样性。

Result: 该方法在LLaVA-1.5-7B和LLaVA-NEXT-7B等模型上，在保持强大性能的同时，实现了88.9%的token减少量，并将推理速度提升了56.7%。

Conclusion: 通过显式保留跨模态对齐和模态内信息多样性，本文提出的视觉token剪枝策略能有效降低大型多模态模型（LMMs）的计算成本，显著提升推理速度，同时维持性能。

Abstract: Large Multimodal Models (LMMs) have achieved significant success across
various tasks. These models usually encode visual inputs into dense token
sequences, which are then concatenated with textual tokens and jointly
processed by a language model. However, the increased token count substantially
raises computational and memory costs during inference. Token pruning has
emerged as a promising approach to address this issue. Existing token pruning
methods often rely on costly calibration or suboptimal importance metrics,
leading to redundant retained tokens. In this paper, we analyze the redundancy
differences between visual and textual tokens and propose pruning exclusively
on visual tokens. Based on this, we propose a visual token pruning strategy
that explicitly preserves both cross-modal alignment and intra-modal
informational diversity. We introduce a mutual information-based token pruning
strategy that removes visual tokens semantically misaligned with textual
tokens, effectively preserving the alignment between the visual and textual
modalities. To further improve the representational quality of the retained
tokens, we additionally prune redundant visual tokens by maximizing the
expected pairwise distances in the embedding space, which is solved efficiently
with a greedy algorithm. Extensive experiments demonstrate that our method
maintains strong performance while reducing tokens by 88.9% on models such as
LLaVA-1.5-7B and LLaVA-NEXT-7B, resulting in a 56.7% improvement in inference
speed.

</details>


### [37] [CryptoFace: End-to-End Encrypted Face Recognition](https://arxiv.org/abs/2509.00332)
*Wei Ao,Vishnu Naresh Boddeti*

Main category: cs.CV

TL;DR: CryptoFace是首个基于全同态加密（FHE）的端到端加密人脸识别系统，通过创新的浅层补丁卷积网络，在确保隐私的同时，显著提高了推理速度和识别精度。


<details>
  <summary>Details</summary>
Motivation: 人脸识别技术广泛应用于认证、安全和个性化服务，但其敏感的生物特征数据存在严重的隐私泄露风险，亟需一种能安全处理人脸数据的方案。

Method: 本文提出了CryptoFace，一个利用全同态加密（FHE）实现端到端加密的人脸识别系统。它通过引入浅层补丁卷积网络的混合体，支持基于补丁的高维张量处理，从而减少乘法深度并降低推理延迟。通过并行FHE评估，系统实现了接近分辨率无关的延迟，确保人脸数据（特征提取、存储、匹配）在整个过程中不被暴露。

Result: 在标准人脸识别基准测试中，CryptoFace与现有最先进的FHE神经网络相比，显著加快了推理速度，并提高了验证准确性。

Conclusion: CryptoFace为需要强大且可证明安全保障的加密人脸识别系统提供了有效的解决方案，将推动安全人脸识别技术的发展。

Abstract: Face recognition is central to many authentication, security, and
personalized applications. Yet, it suffers from significant privacy risks,
particularly arising from unauthorized access to sensitive biometric data. This
paper introduces CryptoFace, the first end-to-end encrypted face recognition
system with fully homomorphic encryption (FHE). It enables secure processing of
facial data across all stages of a face-recognition process--feature
extraction, storage, and matching--without exposing raw images or features. We
introduce a mixture of shallow patch convolutional networks to support
higher-dimensional tensors via patch-based processing while reducing the
multiplicative depth and, thus, inference latency. Parallel FHE evaluation of
these networks ensures near-resolution-independent latency. On standard face
recognition benchmarks, CryptoFace significantly accelerates inference and
increases verification accuracy compared to the state-of-the-art FHE neural
networks adapted for face recognition. CryptoFace will facilitate secure face
recognition systems requiring robust and provable security. The code is
available at https://github.com/human-analysis/CryptoFace.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [38] [A Comparative Study of Controllability, Explainability, and Performance in Dysfluency Detection Models](https://arxiv.org/abs/2509.00058)
*Eric Zhang,Li Wei,Sarah Chen,Michael Wang*

Main category: cs.AI

TL;DR: 本文系统比较了四种口吃检测模型，评估其性能、可控性和可解释性，发现UDM在准确性和临床可解释性上平衡最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管口吃检测模型性能持续提升，但临床应用不仅需要准确性，还需模型具有可控性和可解释性，现有模型在此方面各有侧重。

Method: 对YOLO-Stutter、FluentNet、UDM和SSDM四种代表性口吃检测方法进行了系统性比较分析，评估维度包括性能、可控性和可解释性。评估通过多数据集和临床专家评审完成。

Result: 研究发现YOLO-Stutter和FluentNet高效简单但透明度有限；UDM在准确性和临床可解释性之间取得了最佳平衡；SSDM虽有前景但在实验中未能完全复现。

Conclusion: 该分析揭示了不同方法间的权衡，并为临床可行的口吃建模指明了未来发展方向。同时，提供了各方法的实现细节和实际部署考量。

Abstract: Recent advances in dysfluency detection have introduced a variety of modeling
paradigms, ranging from lightweight object-detection inspired networks
(YOLOStutter) to modular interpretable frameworks (UDM). While performance on
benchmark datasets continues to improve, clinical adoption requires more than
accuracy: models must be controllable and explainable. In this paper, we
present a systematic comparative analysis of four representative
approaches--YOLO-Stutter, FluentNet, UDM, and SSDM--along three dimensions:
performance, controllability, and explainability. Through comprehensive
evaluation on multiple datasets and expert clinician assessment, we find that
YOLO-Stutter and FluentNet provide efficiency and simplicity, but with limited
transparency; UDM achieves the best balance of accuracy and clinical
interpretability; and SSDM, while promising, could not be fully reproduced in
our experiments. Our analysis highlights the trade-offs among competing
approaches and identifies future directions for clinically viable dysfluency
modeling. We also provide detailed implementation insights and practical
deployment considerations for each approach.

</details>


### [39] [Beyond Memorization: Reasoning-Driven Synthesis as a Mitigation Strategy Against Benchmark Contamination](https://arxiv.org/abs/2509.00072)
*Terry Jingchen Zhang,Gopal Dev,Ning Wang,Nicole Ni,Wenyuan Jiang,Yinya Huang,Bernhard Schölkopf,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: LLM基准评估面临数据污染质疑，本研究通过arXiv论文生成多步推理QA，发现前沿模型在知识截止期附近无显著性能衰减，表明此方法能有效对抗基准污染。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试面临数据污染质疑，难以区分模型的真实推理能力与单纯记忆，这使得LLM能力评估的有效性受到挑战。

Method: 1. 构建了一个可无限扩展的框架，直接从arXiv论文合成研究级多步问答（QA）题。2. 利用出版物的时间结构，通过模型在知识截止日期后的性能衰减来检测潜在的数据污染。3. 评估了4个前沿LLM（每个家族2个不同知识截止日期的模型），使用从20,277篇arXiv论文中生成的1,643个多步推理问题，这些问题时间跨度26个月，覆盖所有截止日期前后至少6个月。4. 与先前报告使用公共数据直接检索问题出现显著性能衰减的纵向研究进行了对比分析。

Result: 结果一致表明，对于各种规模、开发者和发布日期的模型，在知识截止日期附近均未出现显著的性能衰减。

Conclusion: 1. 研究者推断，其合成管道所需的多步推理问题提供了超越浅层记忆的更高复杂性，有效起到了缓解基准测试数据污染的作用。2. 呼吁一种范式转变，即优先采用推理驱动的合成方法来构建基准，而非简单地定期收集新发布的问题。3. 已开源代码和数据集以支持可重复性。

Abstract: Capability evaluation of large language models (LLMs) is increasingly
shadowed by rising concerns of data contamination that cast doubts on whether
static benchmarks measure genuine reasoning or mere memorization. We present an
empirical study using an infinitely scalable framework to synthesize
research-level QA directly from arXiv papers, harnessing the natural temporal
structure of research publications where performance decay after knowledge
cutoffs may indicate potential contamination. We evaluated 4 frontier model
represented by 2 models of different knowledge cutoff dates per family on 1,643
multi-step reasoning questions synthesized from 20,277 arXiv papers stratified
over 26 months, covering at least 6 months before and after all cutoff dates.
Our results consistently showed a lack of significant performance decay near
knowledge cutoff dates for models of various sizes, developers, and release
dates. We further performed a comparative analysis with previous longitudinal
studies that reported significant post-cutoff performance decay using directly
retrieved questions based on public data. we hypothesize that the multi-step
reasoning required by our synthesis pipeline offered additional complexity that
goes deeper than shallow memorization, which effectively serves a mitigation
strategy against benchmark contamination. We fully open source our code and
dataset to aid reproducibility and advocate for a paradigm shift that
prioritize reasoning-driven synthesis to construct benchmarks over simply
collecting newly released questions periodically.

</details>


### [40] [Language and Experience: A Computational Model of Social Learning in Complex Tasks](https://arxiv.org/abs/2509.00074)
*Cédric Colas,Tracey Mills,Ben Prystawski,Michael Henry Tessler,Noah Goodman,Jacob Andreas,Joshua Tenenbaum*

Main category: cs.AI

TL;DR: 论文提出一个计算框架，通过将预训练语言模型转化为概率模型，使AI能结合语言指导和直接经验进行社会学习。在视频游戏实验中，该方法加速了人类和模型的学习，并实现了人机间的知识迁移，为构建人机协作学习系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何整合语言指导与直接经验以实现安全快速学习，及其对人类发展的核心作用。探索AI系统如何也能有效结合这两种知识来源。

Method: 提出了一个计算框架，将社会学习建模为基于结构化、可执行世界模型的联合概率推理，利用感觉运动和语言数据。将预训练语言模型（PLM）转化为一个概率模型，模拟人类基于信念分享建议的方式，使AI代理能生成建议并在贝叶斯推理中将语言输入作为证据。通过在10款视频游戏中的行为实验和模拟进行验证。

Result: 语言指导能塑造探索行为，并通过减少危险交互和加速关键发现，显著加速人类和模型的学习过程。通过迭代学习实验，揭示了知识如何在代际间积累。成功实现了人类与模型之间的知识迁移。

Conclusion: 该研究揭示了结构化、兼容语言的表征能够促进有效的人机协作学习。所提出的框架为理解人类社会学习机制以及设计具有类似能力的AI系统提供了见解。

Abstract: The ability to combine linguistic guidance from others with direct experience
is central to human development, enabling safe and rapid learning in new
environments. How do people integrate these two sources of knowledge, and how
might AI systems? We present a computational framework that models social
learning as joint probabilistic inference over structured, executable world
models given sensorimotor and linguistic data. We make this possible by turning
a pretrained language model into a probabilistic model of how humans share
advice conditioned on their beliefs, allowing our agents both to generate
advice for others and to interpret linguistic input as evidence during Bayesian
inference. Using behavioral experiments and simulations across 10 video games,
we show how linguistic guidance can shape exploration and accelerate learning
by reducing risky interactions and speeding up key discoveries in both humans
and models. We further explore how knowledge can accumulate across generations
through iterated learning experiments and demonstrate successful knowledge
transfer between humans and models -- revealing how structured,
language-compatible representations might enable human-machine collaborative
learning.

</details>


### [41] [Entropy-Guided Loop: Achieving Reasoning through Uncertainty-Aware Generation](https://arxiv.org/abs/2509.00079)
*Andrew G. A. Correa,Ana C. H de Matos*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级的熵引导优化循环，通过利用token级不确定性触发精炼，使小型模型在大幅降低成本的同时，达到接近大型推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然性能优越，但成本高昂且延迟大。研究旨在寻找一种能在保证质量的同时降低成本和延迟的方法。

Method: 提出“熵引导优化”方法，这是一个轻量级的测试时循环。该方法提取对数概率，计算前k个备选项的香农熵，并结合困惑度、最大token熵和低置信度token数量使用OR逻辑触发器。它将包含token、置信度、备选项和上下文的紧凑不确定性报告反馈给模型，以指导纠正性编辑。

Result: 在推理、数学和代码生成等任务上，使用该循环的小型模型能够达到参考推理模型95%的质量，而成本仅为其约三分之一。该方法对大约31%的响应进行了选择性优化，并将准确性比单次推理提高了16个百分点。

Conclusion: 这种不确定性感知循环在单次推理和昂贵的推理链之间提供了一个有效的折中方案，对于同时注重质量和成本的生产部署具有实际应用价值。

Abstract: Reasoning models often outperform smaller models but at 3--5$\times$ higher
cost and added latency. We present entropy-guided refinement: a lightweight,
test-time loop that uses token-level uncertainty to trigger a single, targeted
refinement pass. We extract logprobs, compute Shannon entropy on top-$k$
alternatives, and apply a simple OR-logic trigger over perplexity, maximum
token entropy, and low-confidence-token count. Unlike approaches that use
entropy only for measurement or decoding, we pass a compact uncertainty report
(tokens, confidences, alternatives, context) back to the model to guide
corrective edits. On representative technical queries across reasoning,
mathematics, and code generation tasks, a small model with our loop approaches
95\% of a reference reasoning model's quality at approximately one-third of the
cost. The method achieves selective refinement on ~31\% of responses while
improving accuracy by 16 percentage points over single-pass inference. We
demonstrate that this uncertainty-aware loop provides an effective middle
ground between single-pass inference and expensive reasoning chains, making it
practical for production deployments where both quality and cost matter.

</details>


### [42] [Wrong Face, Wrong Move: The Social Dynamics of Emotion Misperception in Agent-Based Models](https://arxiv.org/abs/2509.00080)
*David Freire-Obregón*

Main category: cs.AI

TL;DR: 研究不同情绪识别准确度对人工代理社会行为、情绪和空间组织的影响，发现低准确度导致不信任、情绪解体和社会混乱，而高准确度则形成稳定情绪群落并抵抗干扰。即使是误感知也足以导致社会瓦解。


<details>
  <summary>Details</summary>
Motivation: 人类识别和回应他人情绪的能力是理解社会行为的基础。本研究旨在通过人工代理，探讨感知准确性对群体涌现情绪和空间行为的影响。

Method: 构建人工代理，赋予其不同准确度（低、中、高）的情绪分类器。代理在二维环形网格上局部交流，根据其分类器感知邻居情绪，并向感知到的积极情绪移动，远离消极情绪。代理对的是感知而非真实情绪。在同质、异质群体和重复情绪冲击场景下进行了一系列实验。

Result: 研究发现，低准确度分类器显著导致信任度下降、情绪解体为悲伤以及社会组织混乱。相反，高准确度分类器使代理形成坚固的情绪群落并对情绪干扰表现出韧性。即使在情绪中立的场景中，误感知也足以引发隔离和凝聚力瓦解。

Conclusion: 情绪识别中的偏见或不准确性可能显著扭曲社会过程并破坏情绪整合。

Abstract: The ability of humans to detect and respond to others' emotions is
fundamental to understanding social behavior. Here, agents are instantiated
with emotion classifiers of varying accuracy to study the impact of perceptual
accuracy on emergent emotional and spatial behavior. Agents are visually
represented with face photos from the KDEF database and endowed with one of
three classifiers trained on the JAFFE (poor), CK+ (medium), or KDEF (high)
datasets. Agents communicate locally on a 2D toroidal lattice, perceiving
neighbors' emotional state based on their classifier and responding with
movement toward perceived positive emotions and away from perceived negative
emotions. Note that the agents respond to perceived, instead of ground-truth,
emotions, introducing systematic misperception and frustration. A battery of
experiments is carried out on homogeneous and heterogeneous populations and
scenarios with repeated emotional shocks. Results show that low-accuracy
classifiers on the part of the agent reliably result in diminished trust,
emotional disintegration into sadness, and disordered social organization. By
contrast, the agent that develops high accuracy develops hardy emotional
clusters and resilience to emotional disruptions. Even in emotionally neutral
scenarios, misperception is enough to generate segregation and disintegration
of cohesion. These findings underscore the fact that biases or imprecision in
emotion recognition may significantly warp social processes and disrupt
emotional integration.

</details>


### [43] [Ensemble Debates with Local Large Language Models for AI Alignment](https://arxiv.org/abs/2509.00091)
*Ephraiem Sarabamoun*

Main category: cs.AI

TL;DR: 本研究发现，本地开源集成辩论模型相比单一模型能显著提升大型语言模型在价值对齐推理方面的能力，特别是在推理深度和论证质量方面，并提供了可复现的评估基础。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型（LLMs）在关键决策中的日益重要作用，确保其与人类价值观对齐至关重要。目前对专有API的依赖限制了研究的复现性和广泛参与，因此需要探索开源且可复现的方法来提升LLM的对齐推理能力。

Method: 本研究探讨了本地开源集成辩论是否能提高大型语言模型的对齐导向推理能力。通过在15个场景和5种集成配置下进行150场辩论，研究人员使用7点量表评估了集成模型相对于单一模型基线的性能。

Result: 集成模型在7点量表上的总体表现优于单一模型基线（3.48 vs. 3.13），在推理深度（+19.4%）和论证质量（+34.1%）方面取得了最大增益。在真实性（+1.25分）和人类增强（+0.80分）方面，改进最为显著。

Conclusion: 本地开源集成辩论是一种有效且可复现的方法，能够提升大型语言模型的对齐导向推理能力。研究者提供的代码、提示和辩论数据集为基于集成模型的对齐评估奠定了可访问且可复现的基础。

Abstract: As large language models (LLMs) take on greater roles in high-stakes
decisions, alignment with human values is essential. Reliance on proprietary
APIs limits reproducibility and broad participation. We study whether local
open-source ensemble debates can improve alignmentoriented reasoning. Across
150 debates spanning 15 scenarios and five ensemble configurations, ensembles
outperform single-model baselines on a 7-point rubric (overall: 3.48 vs. 3.13),
with the largest gains in reasoning depth (+19.4%) and argument quality
(+34.1%). Improvements are strongest for truthfulness (+1.25 points) and human
enhancement (+0.80). We provide code, prompts, and a debate data set, providing
an accessible and reproducible foundation for ensemble-based alignment
evaluation.

</details>


### [44] [MODE: Mixture of Document Experts for RAG](https://arxiv.org/abs/2509.00100)
*Rahul Anand*

Main category: cs.AI

TL;DR: MODE是一种轻量级RAG替代方案，通过聚类和路由检索，为小型领域特定语料库提供高效、准确的答案，无需大型向量数据库和重排。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）方法常依赖大型向量数据库和交叉编码器，对于小型、领域特定的语料库来说过于复杂和资源密集。

Method: 本文提出MODE（Mixture of Document Experts），用“聚类与路由”检索替代细粒度近邻搜索。文档被嵌入、聚类成语义连贯的组，并由缓存的质心表示。查询时，系统路由到最佳质心，仅在这些集群内检索上下文，从而无需外部向量数据库和重排，同时保持低延迟。

Result: 在HotpotQA和SQuAD（100-500个分块）上，MODE的答案质量与密集检索基线相当或更优，并缩短了端到端检索时间。消融实验表明，集群粒度和多集群路由控制召回率/准确率权衡，更紧密的集群能提高下游准确性。

Conclusion: MODE为小型和中型语料库提供了一种实用的解决方案，在这些场景中，简洁性、速度和主题聚焦至关重要。

Abstract: Retrieval-Augmented Generation (RAG) often relies on large vector databases
and cross-encoders tuned for large-scale corpora, which can be excessive for
small, domain-specific collections. We present MODE (Mixture of Document
Experts), a lightweight alternative that replaces fine-grained nearest-neighbor
search with cluster-and-route retrieval. Documents are embedded, grouped into
semantically coherent clusters, and represented by cached centroids. At query
time, we route to the top centroid(s) and retrieve context only within those
clusters, eliminating external vector-database infrastructure and reranking
while keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,
MODE matches or exceeds a dense-retrieval baseline in answer quality while
reducing end-to-end retrieval time. Ablations show that cluster granularity and
multi-cluster routing control the recall/precision trade-off, and that tighter
clusters improve downstream accuracy. MODE offers a practical recipe for small
and medium corpora where simplicity, speed, and topical focus matter.

</details>


### [45] [Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems](https://arxiv.org/abs/2509.00115)
*Manish Shukla*

Main category: cs.AI

TL;DR: 本文提出了自适应多维度监控（AMDM）算法，用于在关键领域监控代理AI，通过归一化异构指标和联合异常检测，显著降低了异常检测延迟和误报率。


<details>
  <summary>Details</summary>
Motivation: 早前关于代理AI的框架缺乏算法实现和实证支持。当前对代理AI的评估主要集中于技术指标，忽视了以人为中心和经济维度，无法满足其在高风险领域的监控需求。

Method: 1. 系统回顾了84篇论文，发现现有评估主导指标为能力指标（83%），而人机或经济维度关注不足（30%）。2. 形式化了自适应多维度监控（AMDM）算法，该算法对异构指标进行归一化，应用每轴指数加权移动平均阈值，并通过马哈拉诺比斯距离进行联合异常检测。3. 进行了模拟和真实世界实验。

Result: 1. AMDM在模拟目标漂移中将异常检测延迟从12.3秒缩短至5.6秒。2. 相比静态阈值，AMDM将误报率从4.5%降低至0.9%。3. 提供了比较表、ROC/PR曲线，并重新分析案例研究以揭示缺失指标。

Conclusion: AMDM算法有效解决了代理AI监控的空白，通过多维度方法显著提升了异常检测性能（更低延迟、更少误报），弥补了当前评估实践的不足。

Abstract: Agentic artificial intelligence (AI) -- multi-agent systems that combine
large language models with external tools and autonomous planning -- are
rapidly transitioning from research laboratories into high-stakes domains. Our
earlier "Basic" paper introduced a five-axis framework and proposed preliminary
metrics such as goal drift and harm reduction but did not provide an
algorithmic instantiation or empirical evidence. This "Advanced" sequel fills
that gap. First, we revisit recent benchmarks and industrial deployments to
show that technical metrics still dominate evaluations: a systematic review of
84 papers from 2023--2025 found that 83% report capability metrics while only
30% consider human-centred or economic axes [2]. Second, we formalise an
Adaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises
heterogeneous metrics, applies per-axis exponentially weighted moving-average
thresholds and performs joint anomaly detection via the Mahalanobis distance.
Third, we conduct simulations and real-world experiments. AMDM cuts
anomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and
reduces false-positive rates from 4.5% to 0.9% compared with static thresholds.
We present a comparison table and ROC/PR curves, and we reanalyse case studies
to surface missing metrics. Code, data and a reproducibility checklist
accompany this paper to facilitate replication.

</details>


### [46] [Know When to Explore: Difficulty-Aware Certainty as a Guide for LLM Reinforcement Learning](https://arxiv.org/abs/2509.00125)
*Ang Li,Zhihang Yuan,Yang Zhang,Shouda Liu,Yisen Wang*

Main category: cs.AI

TL;DR: 现有RLVF在LLM推理中依赖稀疏奖励，DACE算法利用模型自确信度感知任务难度，动态调整探索与利用策略，在数学推理任务中显著提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: RLVF依赖稀疏、结果导向的奖励，无法为LLM的推理过程提供细粒度指导，导致学习效率低下，难以区分解决方案质量，也无法有效从不同类型的失败中学习。

Method: 提出难度感知确信度引导探索(DACE)算法。该算法在线评估任务难度（基于策略成功率），并利用此信号调制内在奖励：对于困难任务，通过惩罚高确信度来鼓励探索；对于简单任务，通过奖励高确信度来提高学习效率。

Result: 在AIME和MATH等数学推理基准测试中，DACE显著优于强基线模型。DACE训练的模型不仅实现了更高的准确性，而且在扩展测试时计算资源时，表现出更强的鲁棒性。

Conclusion: DACE的自适应方法在不牺牲精度的前提下，促进了有效的探索，从而验证了其能有效提升LLM的推理能力。

Abstract: Reinforcement Learning with Verifiable Feedback (RLVF) has become a key
technique for enhancing the reasoning abilities of Large Language Models
(LLMs). However, its reliance on sparse, outcome based rewards, which only
indicate if a final answer is correct or not, fails to provide granular
guidance on the reasoning process itself. This limitation hinders efficient
learning, as the model cannot distinguish between high quality and inefficient
solutions, nor can it learn effectively from different types of failures. To
address this, we observe that an LLMs self-certainty often correlates with task
difficulty and solution quality. We introduce Difficulty Aware Certainty guided
Exploration (DACE), a novel RL algorithm that leverages this insight to
dynamically balance the exploration exploitation trade-off. DACE assesses task
difficulty online based on the policys success rate. It then uses this signal
to modulate an intrinsic reward: for difficult tasks where the model is
struggling, DACE encourages exploration by penalizing high certainty; for
easier tasks, it encourages learning efficiency by rewarding high certainty.
Experiments on challenging mathematical reasoning benchmarks (AIME, MATH) show
that DACE significantly outperforms strong baselines. The DACE-trained models
not only achieve higher accuracy but also demonstrate more robust performance
when scaling test-time compute, validating that our adaptive approach fosters
effective exploration without sacrificing precision.

</details>


### [47] [Optimizing Health Coverage in Ethiopia: A Learning-augmented Approach and Persistent Proportionality Under an Online Budget](https://arxiv.org/abs/2509.00135)
*Davin Choo,Yohai Trabelsi,Fentabil Getnet,Samson Warkaye Lamma,Wondesen Nigatu,Kasahun Sime,Lisa Matay,Milind Tambe,Stéphane Verguet*

Main category: cs.AI

TL;DR: 本文提出了一款名为HARP的优化工具及两种算法，旨在帮助埃塞俄比亚卫生部在预算有限和不确定的情况下，优化医疗点建设优先级，以最大化人口覆盖率并满足区域目标，并已验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚卫生部为实现联合国可持续发展目标3，正在加强医疗点以扩大基本医疗服务。然而，由于预算有限和优先事项竞争，每年能实施的卫生系统强化工作仅占一小部分，因此需要一个优化框架来指导各地区的优先排序。

Method: 开发了健康可及资源规划器（HARP）工具，该工具基于序贯设施规划的决策支持优化框架，旨在预算不确定性下最大化人口覆盖率，并满足各区域特定比例目标。提出了两种算法：一种是学习增强方法，用于改进单步专家建议；另一种是多步规划的贪婪算法，两者均具有强大的最坏情况近似估计。

Result: 通过与埃塞俄比亚公共卫生研究所和卫生部合作，在三个地区的不同规划场景下，成功验证了本文方法的经验有效性。

Conclusion: 本文提出的HARP工具及优化算法，为埃塞俄比亚在预算不确定性下优化医疗点建设、扩大医疗服务覆盖提供了有效且经过实证验证的决策支持框架。

Abstract: As part of nationwide efforts aligned with the United Nations' Sustainable
Development Goal 3 on Universal Health Coverage, Ethiopia's Ministry of Health
is strengthening health posts to expand access to essential healthcare
services. However, only a fraction of this health system strengthening effort
can be implemented each year due to limited budgets and other competing
priorities, thus the need for an optimization framework to guide prioritization
across the regions of Ethiopia. In this paper, we develop a tool, Health Access
Resource Planner (HARP), based on a principled decision-support optimization
framework for sequential facility planning that aims to maximize population
coverage under budget uncertainty while satisfying region-specific
proportionality targets at every time step. We then propose two algorithms: (i)
a learning-augmented approach that improves upon expert recommendations at any
single-step; and (ii) a greedy algorithm for multi-step planning, both with
strong worst-case approximation estimation. In collaboration with the Ethiopian
Public Health Institute and Ministry of Health, we demonstrated the empirical
efficacy of our method on three regions across various planning scenarios.

</details>


### [48] [Virtual Group Knowledge and Group Belief in Topological Evidence Models (Extended Version)](https://arxiv.org/abs/2509.00184)
*Alexandru Baltag,Malvin Gattinger,Djanira Gomes*

Main category: cs.AI

TL;DR: 在多智能体证据模型中，本文研究了群组知识和信念，对相关逻辑进行了完全公理化和可判定性证明，并扩展了动态证据共享操作符。


<details>
  <summary>Details</summary>
Motivation: 将基于证据的个体信念和可错知识的拓扑语义扩展到群组，以在多智能体证据模型中形式化和理解（虚拟）群组知识和群组信念的概念。

Method: 1. 将基于证据的个体信念和可错知识的拓扑语义扩展到多智能体证据模型中的群组。2. 对“硬”和“软”群组证据逻辑及其重要的子集（群组知识和群组信念逻辑）进行完全公理化和可判定性证明。3. 引入动态证据共享操作符扩展这些语言，并对相应的逻辑进行完全公理化。

Result: 1. 得到了“硬”和“软”群组证据逻辑的完全公理化和可判定性。2. 得到了群组知识和群组信念逻辑的完全公理化和可判定性。3. 得到了扩展了动态证据共享操作符的相应逻辑的完全公理化。4. 证明了这些动态逻辑与其静态基础具有共表达性。

Conclusion: 本研究为多智能体证据模型中的群组知识和群组信念提供了坚实的逻辑框架，包括其动态性，并通过完全公理化和可判定性证明，深化了对复杂群体认知现象的理解。

Abstract: We study notions of (virtual) group knowledge and group belief within
multi-agent evidence models, obtained by extending the topological semantics of
evidence-based belief and fallible knowledge from individuals to groups. We
completely axiomatize and show the decidability of the logic of ("hard" and
"soft") group evidence, and do the same for an especially interesting fragment
of it: the logic of group knowledge and group belief. We also extend these
languages with dynamic evidence-sharing operators, and completely axiomatize
the corresponding logics, showing that they are co-expressive with their static
bases.

</details>


### [49] [HiVA: Self-organized Hierarchical Variable Agent via Goal-driven Semantic-Topological Evolution](https://arxiv.org/abs/2509.00189)
*Jinzhou Tang,Jusheng Zhang,Qinhan Lv,Sidi Liu,Jing Yang,Chengpei Tang,Keze Wang*

Main category: cs.AI

TL;DR: HiVA是一种基于自组织图和STEV算法的自主代理框架，解决了现有框架在可重用性和灵活性之间的权衡，显著提升了多任务精度和资源效率。


<details>
  <summary>Details</summary>
Motivation: 现有自主代理范式存在缺陷：固定工作流需手动重新配置，而反应式循环无法将推理进展转化为可迁移结构，限制了LLM在复杂任务中的应用和AGI发展。

Method: 引入分层可变代理(HiVA)框架，将代理工作流建模为自组织图。采用语义拓扑演化(STEV)算法，通过文本梯度优化混合语义拓扑空间。迭代过程包括多臂赌博机驱动的正向路由、环境反馈诊断梯度生成，以及语义与拓扑的协同更新。

Result: 在对话、编程、长文本问答、数学和代理基准测试中，任务准确率提高了5-10%，资源效率也优于现有基线。

Conclusion: HiVA框架在自主任务执行中表现出显著有效性，通过优化工作流的自组织能力，克服了现有方法的局限，提升了性能和效率。

Abstract: Autonomous agents play a crucial role in advancing Artificial General
Intelligence, enabling problem decomposition and tool orchestration through
Large Language Models (LLMs). However, existing paradigms face a critical
trade-off. On one hand, reusable fixed workflows require manual reconfiguration
upon environmental changes; on the other hand, flexible reactive loops fail to
distill reasoning progress into transferable structures. We introduce
Hierarchical Variable Agent (HiVA), a novel framework modeling agentic
workflows as self-organized graphs with the Semantic-Topological Evolution
(STEV) algorithm, which optimizes hybrid semantic-topological spaces using
textual gradients as discrete-domain surrogates for backpropagation. The
iterative process comprises Multi-Armed Bandit-infused forward routing,
diagnostic gradient generation from environmental feedback, and coordinated
updates that co-evolve individual semantics and topology for collective
optimization in unknown environments. Experiments on dialogue, coding,
Long-context Q&A, mathematical, and agentic benchmarks demonstrate improvements
of 5-10% in task accuracy and enhanced resource efficiency over existing
baselines, establishing HiVA's effectiveness in autonomous task execution.

</details>


### [50] [Universal Deep Research: Bring Your Own Model and Strategy](https://arxiv.org/abs/2509.00244)
*Peter Belcak,Pavlo Molchanov*

Main category: cs.AI

TL;DR: UDR是一个通用的智能体系统，可封装任意语言模型，使用户能够自定义深度研究策略，无需训练或微调，解决了现有系统硬编码策略的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度研究智能体系统都采用硬编码的特定研究策略和固定工具，缺乏灵活性和用户自定义能力。

Method: 提出通用深度研究（UDR）系统，它作为一个通用型智能体系统，可封装任何语言模型，使用户能够创建、编辑和完善完全自定义的深度研究策略，且无需额外训练或微调。为展示其通用性，UDR配备了示例性的最小、扩展和密集研究策略，并提供了用户界面以方便实验。

Result: UDR系统展示了其通用性，能够支持和执行多种自定义研究策略（如最小、扩展、密集），并通过用户界面方便了系统的实验和验证。

Conclusion: UDR成功提供了一个灵活且可定制的深度研究智能体解决方案，使用户能够自主设计和调整研究流程，克服了现有系统策略僵化的局限性。

Abstract: Deep research tools are among the most impactful and most commonly
encountered agentic systems today. We observe, however, that each deep research
agent introduced so far is hard-coded to carry out a particular research
strategy using a fixed choice of tools. We introduce Universal Deep Research
(UDR), a generalist agentic system that wraps around any language model and
enables the user to create, edit, and refine their own entirely custom deep
research strategies without any need for additional training or finetuning. To
showcase the generality of our system, we equip UDR with example minimal,
expansive, and intensive research strategies, and provide a user interface to
facilitate experimentation with the system.

</details>


### [51] [Instruction-Level Weight Shaping: A Framework for Self-Improving AI Agents](https://arxiv.org/abs/2509.00251)
*Rimom Costa*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) are fluent but largely static after
pre-training; new or shifting knowledge is typically added with
retrieval-augmented generation (RAG) or fine-tuning. RAG raises latency and
engineering overhead and often fails to integrate facts; prompt engineering is
brittle and can conflict with prior knowledge; fine-tuning is costly and risks
catastrophic forgetting. We propose Instruction-Level Weight Shaping (ILWS):
curated system instructions act as external, auditable pseudo-parameters
updated after each session via reflection and user feedback. A Reflection
Engine inspects conversation traces, diagnoses reasoning successes and
failures, and proposes typed deltas $\Delta K=(\Delta S,\Delta U,\Delta T)$
over instructions, user preferences, and tools. Deltas are version-controlled,
evaluated with a sliding window of 1-5 star ratings, auto-repaired on first
failure, and rolled back on repeated failure. When an edit budget crosses a
threshold, the agent compiles a rating-weighted synthetic set and distills
matured instruction-space gains into parameters, converting prompt-space
improvements into weight-space without downtime. ILWS makes explicit the
low-rank shaping induced by context in transformer blocks, preserves
governance, and removes per-call retrieval. In enterprise support it increased
throughput 2.4-5.0x and cut audited hallucinations by about 80% versus a frozen
baseline. In an Adobe Commerce Cloud proof of concept "L0 Support", it achieved
4-5x more tickets per hour and about 80% lower time per ticket, with autonomous
instruction updates and optional tool synthesis. Because ILWS operates at the
instruction layer until controlled distillation, it generalizes to dynamic
domains (legal, medical, engineering) requiring adaptive reasoning, tool
creation, and low-latency deployment.

</details>


### [52] [SHERPA: A Model-Driven Framework for Large Language Model Execution](https://arxiv.org/abs/2509.00272)
*Boqi Chen,Kua Chen,José Antonio Hernández López,Gunter Mussbacher,Dániel Varró,Amir Feizpour*

Main category: cs.AI

TL;DR: 本文提出SHERPA框架，通过将领域特定最佳实践融入分层状态机，实现对大型语言模型（LLMs）行为的精细控制，从而显著提高LLMs在代码生成、类名生成和问答等复杂任务上的输出质量。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs能力强大，但在需要领域特定最佳实践的复杂任务上，它们缺乏结构化推理能力，且这些实践往往在训练数据中缺失。现有多步提示方法（如CoT）缺乏通用机制来精细控制LLM行为。

Method: 提出SHERPA框架，该框架通过将领域特定最佳实践明确地整合到分层状态机中，来改进LLMs在复杂任务上的表现。SHERPA利用状态机结构化LLM的执行过程，并通过规则或机器学习（包括LLMs）驱动的决策，实现对LLM行为的精细控制。

Result: SHERPA框架适用于代码生成、类名生成和问答等多种任务，并能复现并进一步提高现有方法的性能。系统评估表明，整合精心设计的状态机能显著提高LLM输出的质量，尤其对于具有良好人类最佳实践但LLM训练数据缺乏的复杂任务效果更佳。

Conclusion: 通过将精心设计的状态机融入LLM的执行流程，并结合领域特定最佳实践，可以显著提升LLM在复杂任务上的输出质量和控制能力，特别是在训练数据不足但有清晰人类最佳实践的场景下表现突出。

Abstract: Recently, large language models (LLMs) have achieved widespread application
across various fields. Despite their impressive capabilities, LLMs suffer from
a lack of structured reasoning ability, particularly for complex tasks
requiring domain-specific best practices, which are often unavailable in the
training data. Although multi-step prompting methods incorporating human best
practices, such as chain-of-thought and tree-of-thought, have gained
popularity, they lack a general mechanism to control LLM behavior. In this
paper, we propose SHERPA, a model-driven framework to improve the LLM
performance on complex tasks by explicitly incorporating domain-specific best
practices into hierarchical state machines. By structuring the LLM execution
processes using state machines, SHERPA enables more fine-grained control over
their behavior via rules or decisions driven by machine learning-based
approaches, including LLMs. We show that SHERPA is applicable to a wide variety
of tasks-specifically, code generation, class name generation, and question
answering-replicating previously proposed approaches while further improving
the performance. We demonstrate the effectiveness of SHERPA for the
aforementioned tasks using various LLMs. Our systematic evaluation compares
different state machine configurations against baseline approaches without
state machines. Results show that integrating well-designed state machines
significantly improves the quality of LLM outputs, and is particularly
beneficial for complex tasks with well-established human best practices but
lacking data used for training LLMs.

</details>


### [53] [SIGMUS: Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces](https://arxiv.org/abs/2509.00287)
*Brian Wang,Mani Srivastava*

Main category: cs.AI

TL;DR: 本文提出了SIGMUS系统，利用大型语言模型（LLMs）将城市多模态传感器数据语义整合到知识图中，以识别和分析城市事件，克服数据碎片化和人工推理的局限性。


<details>
  <summary>Details</summary>
Motivation: 现代城市传感器产生大量碎片化的多模态数据，难以整合以识别和推理城市事件（如紧急情况、文化活动、自然灾害）及其因果关系，预测未来事件的规模和强度，且目前高度依赖人工推理来建立数据与事件之间的联系。

Method: 开发了SIGMUS（Semantic Integration for Knowledge Graphs in Multimodal Urban Spaces）系统。该系统使用大型语言模型（LLMs）生成必要的“世界知识”，以识别城市事件与不同模态数据之间的关系，从而组织与事件相关的证据和观测结果，并将其表示为知识图谱，避免依赖人工编码规则。

Result: SIGMUS系统能够在新文章文本、闭路电视图像、空气质量、天气和交通测量这5种不同数据源与同时同地发生的事件之间建立合理的关联。

Conclusion: SIGMUS系统通过利用LLMs有效整合了多样化的城市数据，并将其组织成知识图谱，成功实现了城市事件的自动化分析和关系识别，克服了传统人工数据集成和规则依赖的挑战。

Abstract: Modern urban spaces are equipped with an increasingly diverse set of sensors,
all producing an abundance of multimodal data. Such multimodal data can be used
to identify and reason about important incidents occurring in urban landscapes,
such as major emergencies, cultural and social events, as well as natural
disasters. However, such data may be fragmented over several sources and
difficult to integrate due to the reliance on human-driven reasoning for
identifying relationships between the multimodal data corresponding to an
incident, as well as understanding the different components which define an
incident. Such relationships and components are critical to identifying the
causes of such incidents, as well as producing forecasting the scale and
intensity of future incidents as they begin to develop. In this work, we create
SIGMUS, a system for Semantic Integration for Knowledge Graphs in Multimodal
Urban Spaces. SIGMUS uses Large Language Models (LLMs) to produce the necessary
world knowledge for identifying relationships between incidents occurring in
urban spaces and data from different modalities, allowing us to organize
evidence and observations relevant to an incident without relying and
human-encoded rules for relating multimodal sensory data with incidents. This
organized knowledge is represented as a knowledge graph, organizing incidents,
observations, and much more. We find that our system is able to produce
reasonable connections between 5 different data sources (new article text, CCTV
images, air quality, weather, and traffic measurements) and relevant incidents
occurring at the same time and location.

</details>


### [54] [NEWSAGENT: Benchmarking Multimodal Agents as Journalists with Real-World Newswriting Tasks](https://arxiv.org/abs/2509.00446)
*Yen-Che Chien,Kuang-Da Wang,Wei-Yao Wang,Wen-Chih Peng*

Main category: cs.AI

TL;DR: 本文提出了NEWSAGENT，一个用于评估自主智能体从多模态网络数据生成新闻文章的基准测试，发现智能体在事实检索方面表现良好，但在规划和叙事整合方面仍有欠缺。


<details>
  <summary>Details</summary>
Motivation: 尽管行业内的自主数字智能体在结构化任务中展现出潜力，但它们在提高多模态网络数据生产力方面的能力尚不明确。新闻采写是一个需要迭代规划、解读和上下文推理的复杂任务，且常面临信息缺失，这使其成为研究智能体能力提升的理想领域。

Method: 研究者引入了NEWSAGENT基准，用于评估智能体如何自动搜索、选择、编辑和重构原始内容以形成新闻文章。智能体需根据写作指令和一手资料，识别叙事视角、发出关键词查询、检索历史背景并生成完整文章。该基准包含6000个来自真实新闻的人工验证示例，并将多模态内容转换为文本以兼容主流模型。研究者评估了开源和闭源大型语言模型在常用智能体框架下的表现。

Result: 评估结果表明，智能体能够有效检索相关事实，但在新闻内容的规划和叙事整合方面仍存在挑战。

Conclusion: NEWSAGENT提供了一个真实的测试平台，有助于迭代和评估智能体在多模态网络数据处理方面实现实际生产力的能力。

Abstract: Recent advances in autonomous digital agents from industry (e.g., Manus AI
and Gemini's research mode) highlight potential for structured tasks by
autonomous decision-making and task decomposition; however, it remains unclear
to what extent the agent-based systems can improve multimodal web data
productivity. We study this in the realm of journalism, which requires
iterative planning, interpretation, and contextual reasoning from multimodal
raw contents to form a well structured news. We introduce NEWSAGENT, a
benchmark for evaluating how agents can automatically search available raw
contents, select desired information, and edit and rephrase to form a news
article by accessing core journalistic functions. Given a writing instruction
and firsthand data as how a journalist initiates a news draft, agents are
tasked to identify narrative perspectives, issue keyword-based queries,
retrieve historical background, and generate complete articles. Unlike typical
summarization or retrieval tasks, essential context is not directly available
and must be actively discovered, reflecting the information gaps faced in
real-world news writing. NEWSAGENT includes 6k human-verified examples derived
from real news, with multimodal contents converted to text for broad model
compatibility. We evaluate open- and closed-sourced LLMs with commonly-used
agentic frameworks on NEWSAGENT, which shows that agents are capable of
retrieving relevant facts but struggling with planning and narrative
integration. We believe that NEWSAGENT serves a realistic testbed for iterating
and evaluating agent capabilities in terms of multimodal web data manipulation
to real-world productivity.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [55] [Diagnosing Psychiatric Patients: Can Large Language and Machine Learning Models Perform Effectively in Emergency Cases?](https://arxiv.org/abs/2509.00026)
*Abu Shad Ahammed,Sayeri Mukherjee,Roman Obermaisser*

Main category: cs.LG

TL;DR: 本研究利用德国救援站的急诊精神病患者行为数据，评估传统机器学习和大型语言模型（如Llama 3.1）在紧急情况下诊断精神障碍的有效性。


<details>
  <summary>Details</summary>
Motivation: 精神障碍因缺乏可见症状常被误判和诊断不足，尤其在紧急情况下，准确识别至关重要但极具挑战性，可能危及患者生命。

Method: 研究收集了来自德国救援站的急诊精神病患者数据。使用包括Llama 3.1在内的多种机器学习模型，分析患者行为模式，旨在评估这些模型的预测能力是否能有效识别患有精神障碍的患者。

Result: 抽象中未明确给出具体研究结果，仅表明研究旨在评估机器学习模型（包括Llama 3.1）的预测能力是否能作为一种高效工具，用于识别患有不健康精神障碍的急诊患者，尤其是在救援案例中。

Conclusion: 该研究旨在通过结合传统机器学习和大型语言模型，利用行为模式数据，为精神障碍患者提供诊断评估，以期在紧急情况下开发出一种高效的识别工具，改善诊断困境。

Abstract: Mental disorders are clinically significant patterns of behavior that are
associated with stress and/or impairment in social, occupational, or family
activities. People suffering from such disorders are often misjudged and poorly
diagnosed due to a lack of visible symptoms compared to other health
complications. During emergency situations, identifying psychiatric issues is
that's why challenging but highly required to save patients. In this paper, we
have conducted research on how traditional machine learning and large language
models (LLM) can assess these psychiatric patients based on their behavioral
patterns to provide a diagnostic assessment. Data from emergency psychiatric
patients were collected from a rescue station in Germany. Various machine
learning models, including Llama 3.1, were used with rescue patient data to
assess if the predictive capabilities of the models can serve as an efficient
tool for identifying patients with unhealthy mental disorders, especially in
rescue cases.

</details>


### [56] [Mitigating Data Exfiltration Attacks through Layer-Wise Learning Rate Decay Fine-Tuning](https://arxiv.org/abs/2509.00027)
*Elie Thellier,Huiyu Li,Nicholas Ayache,Hervé Delingette*

Main category: cs.LG

TL;DR: 本文提出一种防御策略，通过在模型导出时微调扰动模型参数，有效阻止数据湖中敏感医疗数据的窃取攻击，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 数据湖在敏感医疗数据集上训练机器学习模型时，存在严重的隐私风险，可能导致受保护健康信息泄露。现有研究表明，攻击者可通过在模型参数中嵌入潜在表示或诱导记忆来窃取训练数据，并重建高保真医疗图像，构成严重的隐私威胁。

Method: 提出一种简单有效的缓解策略：在模型导出时，通过使用衰减的逐层学习率进行微调，扰动模型参数，以破坏嵌入数据，同时不降低任务性能。

Result: 在DermaMNIST、ChestMNIST和MIMIC-CXR数据集上的评估表明，该方法能保持任务性能，有效干扰最先进的数据窃取攻击，优于现有防御措施，并使窃取的数据无法用于训练。

Conclusion: 本研究为数据湖训练模型和集中式联邦学习中的数据泄露提供了一种实用的防御方案。对消融实验和自适应攻击的讨论也揭示了未来的挑战和研究方向。

Abstract: Data lakes enable the training of powerful machine learning models on
sensitive, high-value medical datasets, but also introduce serious privacy
risks due to potential leakage of protected health information. Recent studies
show adversaries can exfiltrate training data by embedding latent
representations into model parameters or inducing memorization via multi-task
learning. These attacks disguise themselves as benign utility models while
enabling reconstruction of high-fidelity medical images, posing severe privacy
threats with legal and ethical implications. In this work, we propose a simple
yet effective mitigation strategy that perturbs model parameters at export time
through fine-tuning with a decaying layer-wise learning rate to corrupt
embedded data without degrading task performance. Evaluations on DermaMNIST,
ChestMNIST, and MIMIC-CXR show that our approach maintains utility task
performance, effectively disrupts state-of-the-art exfiltration attacks,
outperforms prior defenses, and renders exfiltrated data unusable for training.
Ablations and discussions on adaptive attacks highlight challenges and future
directions. Our findings offer a practical defense against data leakage in data
lake-trained models and centralized federated learning.

</details>


### [57] [ZeroQAT: Your Quantization-aware Training but Efficient](https://arxiv.org/abs/2509.00031)
*Qitao Tan,Xiaoying Song,Jin Lu,Guoming Li,Jun Liu,Lingzi Hong,Caiwen Ding,Jundong Li,Xiaoming Zhai,Shaoyi Huang,Wei Niu,Geng Yuan*

Main category: cs.LG

TL;DR: ZeroQAT是一个基于零阶优化的QAT框架，它通过前向梯度估计避免反向传播，实现LLM低比特量化的高精度和高效率。


<details>
  <summary>Details</summary>
Motivation: 现有低比特PTQ方法因累积误差和目标不匹配导致精度下降；而QAT虽精确但反向传播带来高昂的计算、时间、内存成本，限制了其实用性。

Method: 提出ZeroQAT框架，利用仅前向的零阶梯度估计来避免反向传播，从而降低计算和内存开销。ZeroQAT还联合学习量化权重、权重裁剪阈值和等效变换，以减轻量化误差并处理激活异常值。

Result: 实验表明，ZeroQAT在保持QAT精度的同时，达到了PTQ的效率。

Conclusion: ZeroQAT为LLM的高质量低比特量化提供了一种实用的解决方案。

Abstract: Quantization is an effective technique to reduce the deployment cost of large
language models (LLMs), and post-training quantization (PTQ) has been widely
studied due to its efficiency. However, existing low-bit PTQ methods suffer
from accuracy degradation because their layer-wise optimization introduces
cumulative error propagation and misalignment between local reconstruction
objectives and downstream performance. While quantization-aware training (QAT)
provides a principled solution, its reliance on backpropagation incurs
prohibitive data, time, and memory costs, limiting its practicality. To address
these challenges, we propose ZeroQAT, a zeroth-order optimization-based QAT
framework. ZeroQAT leverages forward-only gradient estimation to eliminate the
need for backpropagation, significantly reducing computational and memory
overhead while retaining the benefits of end-to-end optimization. Moreover,
ZeroQAT jointly learns quantized weights, weight clipping thresholds, and
equivalent transformations to mitigate quantization error and handle activation
outliers. Experiments demonstrate that ZeroQAT achieves the efficiency of PTQ
while retaining the accuracy of QAT, offering a practical solution for
high-quality low-bit quantization of LLMs.

</details>


### [58] [Industrial Steel Slag Flow Data Loading Method for Deep Learning Applications](https://arxiv.org/abs/2509.00034)
*Mert Sehri,Ana Cardoso,Francisco de Assis Boldt,Patrick Dumond*

Main category: cs.LG

TL;DR: 本研究提出一种基于振动数据的跨域诊断方法，利用混合深度学习模型实现高精度炉渣流态监测，提高钢铁制造效率。


<details>
  <summary>Details</summary>
Motivation: 钢水铸造过程中，炉渣流动污染易导致经济损失，因此准确检测炉渣流态至关重要。

Method: 提出一种基于振动数据的跨域诊断方法。使用结合一维卷积神经网络（1D-CNN）和长短期记忆网络（LSTM）的混合深度学习模型，处理加速计的原始时域振动信号。结合均方根（RMS）预处理和选择性嵌入数据加载策略，并在16个不同域的真实跨域数据集上进行性能评估。

Result: 结果表明，结合RMS预处理和选择性嵌入数据加载策略的混合1D-CNN-LSTM架构实现了鲁棒的分类精度，优于传统模型和加载技术。最高测试准确率达99.10% ± 0.30%，显示出该方法的泛化能力和工业应用价值。

Conclusion: 本研究为实时炉渣流态监测提供了一种实用且可扩展的解决方案，有助于提高钢铁制造的可靠性和运行效率。

Abstract: Steel casting processes are vulnerable to financial losses due to slag flow
contamination, making accurate slag flow condition detection essential. This
study introduces a novel cross-domain diagnostic method using vibration data
collected from an industrial steel foundry to identify various stages of slag
flow. A hybrid deep learning model combining one-dimensional convolutional
neural networks and long short-term memory layers is implemented, tested, and
benchmarked against a standard one-dimensional convolutional neural network.
The proposed method processes raw time-domain vibration signals from
accelerometers and evaluates performance across 16 distinct domains using a
realistic cross-domain dataset split. Results show that the hybrid
convolutional neural network and long short-term memory architecture, when
combined with root mean square preprocessing and a selective embedding data
loading strategy, achieves robust classification accuracy, outperforming
traditional models and loading techniques. The highest test accuracy of 99.10
+/- 0.30 demonstrates the method's capability for generalization and industrial
relevance. This work presents a practical and scalable solution for real-time
slag flow monitoring, contributing to improved reliability and operational
efficiency in steel manufacturing.

</details>


### [59] [Transfer Learning for Minimum Operating Voltage Prediction in Advanced Technology Nodes: Leveraging Legacy Data and Silicon Odometer Sensing](https://arxiv.org/abs/2509.00035)
*Yuxuan Yin,Rebecca Chen,Boxun Xu,Chen He,Peng Li*

Main category: cs.LG

TL;DR: 提出一种迁移学习框架，结合芯片里程表传感器数据，实现5nm先进节点最小工作电压($V_{min}$)的准确预测。


<details>
  <summary>Details</summary>
Motivation: 在先进技术节点（如5nm），由于训练数据有限以及工艺变异与$V_{min}$的复杂关系，开发准确的$V_{min}$预测模型极具挑战性。

Method: 提出一种新颖的迁移学习框架，利用16nm技术节点的大量遗留数据来预测5nm节点的$V_{min}$。关键创新是整合了从片上硅里程表传感器数据中提取的输入特征，以精细表征局部工艺变异。

Result: 通过整合芯片里程表传感器数据，该方法显著提高了$V_{min}$的预测精度，尤其是在5nm节点。

Conclusion: 所提出的迁移学习框架结合传感器数据，有效解决了先进节点$V_{min}$预测中的数据稀缺和复杂性问题，显著提升了预测精度，对半导体制造的能效和可靠性至关重要。

Abstract: Accurate prediction of chip performance is critical for ensuring energy
efficiency and reliability in semiconductor manufacturing. However, developing
minimum operating voltage ($V_{min}$) prediction models at advanced technology
nodes is challenging due to limited training data and the complex relationship
between process variations and $V_{min}$. To address these issues, we propose a
novel transfer learning framework that leverages abundant legacy data from the
16nm technology node to enable accurate $V_{min}$ prediction at the advanced
5nm node. A key innovation of our approach is the integration of input features
derived from on-chip silicon odometer sensor data, which provide fine-grained
characterization of localized process variations -- an essential factor at the
5nm node -- resulting in significantly improved prediction accuracy.

</details>


### [60] [A-FloPS: Accelerating Diffusion Sampling with Adaptive Flow Path Sampler](https://arxiv.org/abs/2509.00036)
*Cheng Jin,Zhenyu Xiao,Yuantao Gu*

Main category: cs.LG

TL;DR: A-FloPS是一种无需训练的框架，通过将扩散模型的采样轨迹重新参数化为流匹配形式并引入自适应速度分解，显著加速了扩散模型的生成过程，同时提高了样本质量和效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然在生成性能上表现卓越，但其迭代采样过程导致计算成本高昂。现有无需训练的加速方法主要优化逆时ODE的数值求解器，但受限于底层采样轨迹的低效性。

Method: A-FloPS将预训练扩散模型的采样轨迹重新参数化为流匹配形式，并通过自适应速度分解进行增强。该重参数化将扩散分数解析地映射到与流兼容的速度，从而在不重新训练的情况下产生易于积分的轨迹。自适应机制进一步将速度场分解为线性漂移项和残差分量，并积极抑制残差分量的时间变化，以在极低NFE（函数评估次数）下恢复高阶积分的准确性优势。

Result: 在条件图像生成和文本到图像合成方面，A-FloPS持续优于最先进的无需训练采样器，无论是在样本质量还是效率上。值得注意的是，A-FloPS在仅有5次函数评估的情况下，实现了显著更低的FID，并生成了更锐利、更连贯的图像。自适应机制也改进了原生的基于流的生成模型。

Conclusion: A-FloPS被定位为一种多功能且有效的解决方案，可实现高质量、低延迟的生成建模。

Abstract: Diffusion models deliver state-of-the-art generative performance across
diverse modalities but remain computationally expensive due to their inherently
iterative sampling process. Existing training-free acceleration methods
typically improve numerical solvers for the reverse-time ODE, yet their
effectiveness is fundamentally constrained by the inefficiency of the
underlying sampling trajectories. We propose A-FloPS (Adaptive Flow Path
Sampler), a principled, training-free framework that reparameterizes the
sampling trajectory of any pre-trained diffusion model into a flow-matching
form and augments it with an adaptive velocity decomposition. The
reparameterization analytically maps diffusion scores to flow-compatible
velocities, yielding integration-friendly trajectories without retraining. The
adaptive mechanism further factorizes the velocity field into a linear drift
term and a residual component whose temporal variation is actively suppressed,
restoring the accuracy benefits of high-order integration even in extremely
low-NFE regimes. Extensive experiments on conditional image generation and
text-to-image synthesis show that A-FloPS consistently outperforms
state-of-the-art training-free samplers in both sample quality and efficiency.
Notably, with as few as $5$ function evaluations, A-FloPS achieves
substantially lower FID and generates sharper, more coherent images. The
adaptive mechanism also improves native flow-based generative models,
underscoring its generality. These results position A-FloPS as a versatile and
effective solution for high-quality, low-latency generative modeling.

</details>


### [61] [Exploring and Reshaping the Weight Distribution in LLM](https://arxiv.org/abs/2509.00046)
*Chunming Ye,Songzhou Li,Xu Xu*

Main category: cs.LG

TL;DR: 本文研究大型语言模型不同层间权重分布的相关性，发现其余弦距离呈幂律分布。基于此，提出一种重塑LoRA初始化权重的方法，实验表明该方法能提升LoRA训练性能。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型不同层权重分布之间的相关性，并分析这些相关性对LoRA训练效果的潜在影响。

Method: 1. 提取模型（如自注意力层、MLP层）的权重矩阵（如Query-projection、down-projection），进行奇异值分解（SVD）。2. 计算并分析奇异值矩阵间的余弦距离，发现其具有幂律分布特征。3. 基于距离分析结果，提出描述不同模型分布特性的定性方法。4. 设计结合高斯过程和帕累托分布函数的数据生成器，模拟生成符合特定分布特征的权重数据。5. 利用这些分布特征和生成方法，重塑LoRA的初始化权重进行训练。

Result: 1. 模型中不同层权重之间的余弦距离展现出显著的幂律分布特征。2. 在不改变模型结构或训练过程的前提下，所提出的方法在LoRA训练性能上取得了一定提升。

Conclusion: 通过理解和利用大型模型层间权重分布的幂律特性，并优化LoRA初始化权重，可以有效提升LoRA的训练表现。

Abstract: The performance of Large Language Models is influenced by their
characteristics such as architecture, model sizes, decoding methods and so on.
Due to differences in structure or function, the weights in different layers of
large models have varying distributions. This paper explores the correlations
between different types of layers in terms of weights distribution and studies
the potential impact of these correlations on LoRA training effectiveness.
Firstly, the study reveals that in the model the cosine distances between
weights of different layers manifest power-law distribution. We extract
Query-projection, down-projection and other weight matrices from the
self-attention layers and MLP layers, calculate the singular values of the
matrices using singular value decomposition, and organize a certain number of
singular values into matrices according to projection's type. By analyzing the
probability distribution of the cosine distances between these matrices, it is
found that the cosine distances values between them have distinct power-law
distribution characteristics. Secondly, based on the results of distance
calculations and analysis across different layers of model, a qualitative
method is proposed to describe the distribution characteristics of different
models. Next, to construct weights that align with the distribution
characteristics, a data generator is designed using a combination of Gaussian
process and Pareto distribution functions. The generator is used to simulate
the generation of data that aligns with specific distribution characteristics.
Finally, based on the aforementioned distribution characteristics and data
generation method, the weights in LoRA initialization are reshaped for
training. Experimental results indicate that, without altering the model
structure or training process, this method achieves a certain improvement in
the performance of LoRA training.

</details>


### [62] [Teaching AI to Remember: Insights from Brain-Inspired Replay in Continual Learning](https://arxiv.org/abs/2509.00047)
*Jina Kim*

Main category: cs.LG

TL;DR: 研究了在持续学习中，受大脑启发的内部重放机制如何缓解灾难性遗忘。结果表明，内部重放能显著减少遗忘（尤其结合SI时），但会牺牲初始任务准确性，并导致潜在空间表征重叠，揭示了当前脑启发方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 人工神经网络在持续学习中面临灾难性遗忘的挑战，即学习新任务时会丢失先前学习的知识，需要有效的机制来解决此问题。

Method: 受人脑记忆巩固启发，深入研究了通过重新激活先前经验潜在表征的内部重放机制。在CIFAR-100数据集的类别增量设置下，单独及结合突触智能（SI）评估其有效性。进一步通过对数似然分布、重构误差、轮廓分数和UMAP投影进行分析。

Result: 内部重放显著缓解了遗忘，尤其与SI结合时效果更佳，但代价是初始任务准确性降低，这凸显了记忆稳定性与学习可塑性之间的权衡。分析还发现，内部重放增加了潜在空间中的表征重叠，可能限制了任务特异性区分。

Conclusion: 研究结果突出了当前脑启发式方法在持续学习中的局限性，并为未来平衡知识保留与适应性的持续学习系统发展提供了方向。

Abstract: Artificial neural networks (ANNs) continue to face challenges in continual
learning, particularly due to catastrophic forgetting, the loss of previously
learned knowledge when acquiring new tasks. Inspired by memory consolidation in
the human brain, we investigate the internal replay mechanism proposed
by~\citep{brain_inspired_replay1}, which reactivates latent representations of
prior experiences during learning. As internal replay was identified as the
most influential component among the brain-inspired mechanisms in their
framework, it serves as the central focus of our in-depth investigation. Using
the CIFAR-100 dataset in a class-incremental setting, we evaluate the
effectiveness of internal replay, both in isolation and in combination with
Synaptic Intelligence (SI). Our experiments show that internal replay
significantly mitigates forgetting, especially when paired with SI, but at the
cost of reduced initial task accuracy, highlighting a trade-off between memory
stability and learning plasticity. Further analyses using log-likelihood
distributions, reconstruction errors, silhouette scores, and UMAP projections
reveal that internal replay increases representational overlap in latent space,
potentially limiting task-specific differentiation. These results underscore
the limitations of current brain-inspired methods and suggest future directions
for balancing retention and adaptability in continual learning systems.

</details>


### [63] [Adaptive Physics-Informed Neural Networks with Multi-Category Feature Engineering for Hydrogen Sorption Prediction in Clays, Shales, and Coals](https://arxiv.org/abs/2509.00049)
*Mohammad Nooraiepour,Mohammad Masoudi,Zezhang Song,Helge Hellevang*

Main category: cs.LG

TL;DR: 提出了一种结合多类别特征工程的自适应物理信息神经网络（PINN）框架，显著提高了粘土、页岩和煤中氢吸附预测的准确性和效率，并支持不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 准确预测粘土、页岩和煤中的氢吸附对于地下储氢、天然氢勘探和放射性废物处置至关重要。然而，传统实验方法耗时、易出错且难以捕捉地质异质性。

Method: 本研究引入了一个自适应物理信息神经网络（PINN）框架，该框架结合了多类别特征工程。它整合了经典等温线模型与热力学约束以确保物理一致性，同时利用了深度学习的灵活性。模型使用包含155个样本（50个粘土、60个页岩、45个煤）的综合数据集，并采用了七个类别的多类别特征工程。PINN模型采用带有多头注意力的深度残差网络，并通过自适应损失函数和蒙特卡洛dropout进行优化，以实现不确定性量化。

Result: 通过K折交叉验证和超参数优化，模型实现了高预测准确性（R2 = 0.979, RMSE = 0.045 mol/kg），并在复杂性增加15倍的情况下仍使收敛速度加快了67%。该框架在粘土矿物（R2 = 0.981）、页岩（R2 = 0.971）和煤（R2 = 0.978）中均表现出稳健的岩性特异性性能，可靠性评分保持在85-91%。可解释性分析表明氢吸附容量主导预测，且86.7%的特征对存在强相互作用，证实了非线性建模的必要性。

Conclusion: 该自适应物理信息框架能够加速场地筛选，并通过稳健的不确定性量化支持风险知情的决策。

Abstract: Accurate prediction of hydrogen sorption in clays, shales, and coals is vital
for advancing underground hydrogen storage, natural hydrogen exploration, and
radioactive waste containment. Traditional experimental methods, while
foundational, are time-consuming, error-prone, and limited in capturing
geological heterogeneity. This study introduces an adaptive physics-informed
neural network (PINN) framework with multi-category feature engineering to
enhance hydrogen sorption prediction. The framework integrates classical
isotherm models with thermodynamic constraints to ensure physical consistency
while leveraging deep learning flexibility. A comprehensive dataset consisting
of 155 samples, which includes 50 clays, 60 shales, and 45 coals, was employed,
incorporating diverse compositional properties and experimental conditions.
Multi-category feature engineering across seven categories captured complex
sorption dynamics. The PINN employs deep residual networks with multi-head
attention, optimized via adaptive loss functions and Monte Carlo dropout for
uncertainty quantification. K-fold cross-validation and hyperparameter
optimization achieve significant accuracy (R2 = 0.979, RMSE = 0.045 mol per kg)
with 67% faster convergence despite 15-fold increased complexity. The framework
demonstrates robust lithology-specific performance across clay minerals (R2 =
0.981), shales (R2 = 0.971), and coals (R2 = 0.978), maintaining 85-91%
reliability scores. Interpretability analysis via SHAP, accumulated local
effects, and Friedman's H-statistics reveal that hydrogen adsorption capacity
dominates predictions, while 86.7% of feature pairs exhibit strong
interactions, validating the necessity of non-linear modeling approaches. This
adaptive physics-informed framework accelerates site screening and enables
risk-informed decision-making through robust uncertainty quantification.

</details>


### [64] [Applying Deep Learning to Anomaly Detection of Russian Satellite Activity for Indications Prior to Military Activity](https://arxiv.org/abs/2509.00050)
*David Kurtenbach,Megan Manly,Zach Metzinger*

Main category: cs.LG

TL;DR: 本研究利用深度学习技术分析俄军在乌克兰入侵前后太空资产（RSO）的异常活动，旨在识别可作为未来冲突中军事侵略行为指示和预警（I&W）的发现。研究发现俄军RSO活动存在统计学上的显著异常，并能细化到单个轨道元素。


<details>
  <summary>Details</summary>
Motivation: 评估俄罗斯拥有的RSO在乌克兰入侵前的活动是否能作为未来军事冲突中侵略行为的指示和预警。通过分析异常活动，理解可能的战术和程序，以识别俄罗斯RSO生活/行为模式（PoL/PoB）中统计学上显著的变化。

Method: 研究采用统计和深度学习方法来评估异常活动。深度学习模型包括Isolation Forest (IF)、传统自编码器 (AE)、变分自编码器 (VAE)、Kolmogorov Arnold Network (KAN) 和一种新颖的基于锚点损失的自编码器 (Anchor AE)。每个模型都基于五年的数据样本建立轨道活动基线。主要调查期聚焦于2022年2月24日入侵前的六个月，并对入侵后的活跃作战期间的RSO活动进行额外分析。为捕获每个RSO的独特特性，为每个观测到的太空物体训练了单独的模型。自编码器模型根据超过阈值西格玛的重建误差来识别异常。研究优先考虑模型结果的可解释性，因此评估了单个六个轨道元素的异常行为，而非将输入数据作为单一整体观测进行分析。

Result: 研究结果表明，不仅发现了俄罗斯RSO活动中统计学上显著的异常，而且将这些异常发现详细至单个轨道元素。

Conclusion: 深度学习技术能够成功识别俄罗斯RSO活动中统计学上显著的异常，这些发现（尤其是个体轨道层面的异常）可作为未来冲突中侵略性军事行为的指示和预警。

Abstract: We apply deep learning techniques for anomaly detection to analyze activity
of Russian-owned resident space objects (RSO) prior to the Ukraine invasion and
assess the results for any findings that can be used as indications and
warnings (I&W) of aggressive military behavior for future conflicts. Through
analysis of anomalous activity, an understanding of possible tactics and
procedures can be established to assess the existence of statistically
significant changes in Russian RSO pattern of life/pattern of behavior
(PoL/PoB) using publicly available two-line element (TLE) data. This research
looks at statistical and deep learning approaches to assess anomalous activity.
The deep learning methods assessed are isolation forest (IF), traditional
autoencoder (AE), variational autoencoder (VAE), Kolmogorov Arnold Network
(KAN), and a novel anchor-loss based autoencoder (Anchor AE). Each model is
used to establish a baseline of on-orbit activity based on a five-year data
sample. The primary investigation period focuses on the six months leading up
to the invasion date of February 24, 2022. Additional analysis looks at RSO
activity during an active combat period by sampling TLE data after the invasion
date. The deep learning autoencoder models identify anomalies based on
reconstruction errors that surpass a threshold sigma. To capture the nuance and
unique characteristics of each RSO an individual model was trained for each
observed space object. The research made an effort to prioritize explainability
and interpretability of the model results thus each observation was assessed
for anomalous behavior of the individual six orbital elements versus analyzing
the input data as a single monolithic observation. The results demonstrate not
only statistically significant anomalies of Russian RSO activity but also
details anomalous findings to the individual orbital element.

</details>


### [65] [From Data to Decision: A Multi-Stage Framework for Class Imbalance Mitigation in Optical Network Failure Analysis](https://arxiv.org/abs/2509.00057)
*Yousuf Moiz Ali,Jaroslaw E. Prilepsky,Nicola Sambo,Joao Pedro,Mohammad M. Hosseini,Antonio Napoli,Sergei K. Turitsyn,Pedro Freire*

Main category: cs.LG

TL;DR: 本文比较了预处理、内部处理和后处理方法在缓解光网络故障管理中类别不平衡问题上的效果。结果显示，后处理在故障检测中表现最佳，而生成式AI在故障识别中性能突出。最佳方法选择取决于类别重叠和延迟需求。


<details>
  <summary>Details</summary>
Motivation: 机器学习在光网络故障管理中受到广泛关注，但严重的类别不平衡是一个重大挑战，且后处理方法在此领域的探索较少。

Method: 使用实验数据集，直接比较了预处理、内部处理和后处理方法，以缓解故障检测和识别中的类别不平衡问题。

Result: 故障检测中，后处理方法（特别是阈值调整）F1分数提升最高（达15.3%），随机欠采样推理速度最快。故障识别中，生成式AI方法性能提升最显著（达24.2%），而后处理影响有限。在类别重叠且延迟关键时，SMOTE等过采样方法最有效；无延迟约束时，元学习效果最佳。在低重叠场景中，生成式AI方法性能最高且推理时间最短。

Conclusion: 针对光网络故障管理中的类别不平衡问题，不同处理方法在故障检测和识别任务中表现各异。后处理在故障检测中表现突出，生成式AI在故障识别中效果显著。方法的选择应根据具体场景（如类别重叠程度、延迟要求）进行优化。

Abstract: Machine learning-based failure management in optical networks has gained
significant attention in recent years. However, severe class imbalance, where
normal instances vastly outnumber failure cases, remains a considerable
challenge. While pre- and in-processing techniques have been widely studied,
post-processing methods are largely unexplored. In this work, we present a
direct comparison of pre-, in-, and post-processing approaches for class
imbalance mitigation in failure detection and identification using an
experimental dataset. For failure detection, post-processing
methods-particularly Threshold Adjustment-achieve the highest F1 score
improvement (up to 15.3%), while Random Under-Sampling provides the fastest
inference. In failure identification, GenAI methods deliver the most
substantial performance gains (up to 24.2%), whereas post-processing shows
limited impact in multi-class settings. When class overlap is present and
latency is critical, over-sampling methods such as the SMOTE are most
effective; without latency constraints, Meta-Learning yields the best results.
In low-overlap scenarios, Generative AI approaches provide the highest
performance with minimal inference time.

</details>


### [66] [T-MLP: Tailed Multi-Layer Perceptron for Level-of-Detail Signal Representation](https://arxiv.org/abs/2509.00066)
*Chuanxiang Yang,Yuanfeng Zhou,Guangshun Wei,Siyu Ren,Yuan Liu,Junhui Hou,Wenping Wang*

Main category: cs.LG

TL;DR: 提出了一种名为T-MLP的新型神经网络架构，通过在MLP中添加多个输出分支实现多尺度细节层次（LoD）信号表示，并在多种任务中超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 细节层次（LoD）表示对于高效建模和传输图像、3D形状等各类信号至关重要，但广泛使用的多层感知机（MLP）本质上是单尺度操作，缺乏对LoD的原生支持。

Method: 引入了Tailed Multi-Layer Perceptron (T-MLP)，通过在MLP的隐藏层附加多个输出分支（即“尾巴”），实现多深度直接监督。设计了特定的损失函数和训练策略，使每个隐藏层能有效学习特定LoD的目标信号，从而实现多尺度建模。

Result: 广泛的实验结果表明，T-MLP在各种信号表示任务中，性能优于其他神经LoD基线方法。

Conclusion: T-MLP提供了一种新颖且高效的神经网络架构，能够原生支持多尺度的细节层次（LoD）信号表示。

Abstract: Level-of-detail (LoD) representation is critical for efficiently modeling and
transmitting various types of signals, such as images and 3D shapes. In this
work, we present a novel neural architecture that supports LoD signal
representation. Our architecture is based on an elaborate modification of the
widely used Multi-Layer Perceptron (MLP), which inherently operates at a single
scale and therefore lacks native support for LoD. Specifically, we introduce
the Tailed Multi-Layer Perceptron (T-MLP) that extends the MLP by attaching
multiple output branches, also called tails, to its hidden layers, enabling
direct supervision at multiple depths. Our loss formulation and training
strategy allow each hidden layer to effectively learn a target signal at a
specific LoD, thus enabling multi-scale modeling. Extensive experimental
results show that our T-MLP outperforms other neural LoD baselines across a
variety of signal representation tasks.

</details>


### [67] [AnomalyExplainer Explainable AI for LLM-based anomaly detection using BERTViz and Captum](https://arxiv.org/abs/2509.00069)
*Prasasthy Balasubramanian,Dumindu Kankanamge,Ekaterina Gilman,Mourad Oussalah*

Main category: cs.LG

TL;DR: 本研究提出一个可解释的AI框架，用于网络安全异常检测，结合RoBERTa、可视化工具和自然语言解释，实现了高准确率并提升了用户理解。


<details>
  <summary>Details</summary>
Motivation: 对话式AI和大型语言模型（LLMs）在网络安全领域应用广泛，但仍面临误报、模型管理复杂和对可解释AI（XAI）信任不足等挑战，需要更透明有效的异常检测方案。

Method: 开发了一个结合异常检测和高质量解释的框架，利用BERTViz和Captum等可视化工具，并结合基于注意力输出的自然语言报告。通过在LogHub的HDFS数据集上，将RoBERTa与Falcon-7B、DeBERTa和Mistral-7B进行比较分析，并收集了用户反馈。

Result: RoBERTa在异常检测中表现出色，准确率达99.6%，优于Falcon-7B和DeBERTa，并比Mistral-7B更具灵活性。用户反馈证实了聊天机器人的易用性，并提高了对异常的理解。

Conclusion: 所开发的框架通过提供高准确度、可解释的异常检测，有效增强了网络安全工作流程，减少了人工工作量并提升了安全分析师的理解。

Abstract: Conversational AI and Large Language Models (LLMs) have become powerful tools
across domains, including cybersecurity, where they help detect threats early
and improve response times. However, challenges such as false positives and
complex model management still limit trust. Although Explainable AI (XAI) aims
to make AI decisions more transparent, many security analysts remain uncertain
about its usefulness. This study presents a framework that detects anomalies
and provides high-quality explanations through visual tools BERTViz and Captum,
combined with natural language reports based on attention outputs. This reduces
manual effort and speeds up remediation. Our comparative analysis showed that
RoBERTa offers high accuracy (99.6 %) and strong anomaly detection,
outperforming Falcon-7B and DeBERTa, as well as exhibiting better flexibility
than large-scale Mistral-7B on the HDFS dataset from LogHub. User feedback
confirms the chatbot's ease of use and improved understanding of anomalies,
demonstrating the ability of the developed framework to strengthen
cybersecurity workflows.

</details>


### [68] [SynCircuit: Automated Generation of New Synthetic RTL Circuits Can Enable Big Data in Circuits](https://arxiv.org/abs/2509.00071)
*Shang Liu,Jing Wang,Wenji Fang,Zhiyao Xie*

Main category: cs.LG

TL;DR: 本文提出SynCircuit，一个三步框架，通过定制扩散模型、约束细化和MCTS优化，生成具有有效功能的HDL格式合成电路数据，以解决AI辅助IC设计中的数据稀缺瓶颈，并提升下游ML模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助IC设计方法潜力巨大，但电路设计数据（尤其公共领域数据）极其有限，已成为开发这些方法的首要瓶颈。

Method: SynCircuit采用三步创新框架自动生成合成数据：1) 提出定制的扩散生成模型解决有向循环图（DCG）生成任务。2) 通过细化初始图生成输出来强制执行电路约束，确保电路有效性。3) 利用蒙特卡洛树搜索（MCTS）方法进一步优化生成图中的逻辑冗余。

Result: 实验结果表明，所提出的SynCircuit能够生成更真实的合成电路，并提升下游电路设计任务中机器学习模型的性能。

Conclusion: SynCircuit首次尝试生成具有有效功能的HDL格式合成电路，成功缓解了AI辅助IC设计领域的数据短缺问题，并被证明能增强相关机器学习模型的表现。

Abstract: In recent years, AI-assisted IC design methods have demonstrated great
potential, but the availability of circuit design data is extremely limited,
especially in the public domain. The lack of circuit data has become the
primary bottleneck in developing AI-assisted IC design methods. In this work,
we make the first attempt, SynCircuit, to generate new synthetic circuits with
valid functionalities in the HDL format. SynCircuit automatically generates
synthetic data using a framework with three innovative steps: 1) We propose a
customized diffusion-based generative model to resolve the Directed Cyclic
Graph (DCG) generation task, which has not been well explored in the AI
community. 2) To ensure our circuit is valid, we enforce the circuit
constraints by refining the initial graph generation outputs. 3) The Monte
Carlo tree search (MCTS) method further optimizes the logic redundancy in the
generated graph. Experimental results demonstrate that our proposed SynCircuit
can generate more realistic synthetic circuits and enhance ML model performance
in downstream circuit design tasks.

</details>


### [69] [Mitigating Clinician Information Overload: Generative AI for Integrated EHR and RPM Data Analysis](https://arxiv.org/abs/2509.00073)
*Ankit Shetgaonkar,Dipen Pradhan,Lakshit Arora,Sanjay Surendranath Girija,Shashank Kapoor,Aman Raj*

Main category: cs.LG

TL;DR: 本文概述了生成式AI（特别是LLMs）在医疗领域中，如何利用RPM和EHR数据来解决临床医生信息过载问题，提升临床效率，并探讨了其应用潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 临床医生面临来自远程患者监测(RPM)和电子健康记录(EHR)相结合的庞大且异构数据带来的巨大挑战，导致信息过载，急需利用AI技术来解析复杂数据，提升临床洞察力和效率。

Method: 本文通过对生成式AI（尤其是LLMs）在医疗领域的背景、能力、需求和应用进行全面概述。具体方法包括：探讨LLM驱动的应用如何改善患者数据导航和提供临床决策支持；讨论简化工作流程和个性化护理的机遇；分析数据整合、质量、隐私、AI输出验证、偏见缓解及临床接受度等关键挑战。

Result: LLM驱动的应用能够增强纵向患者数据导航，并通过自然语言对话提供可操作的临床决策支持。本文识别了简化临床医生工作流程和实现个性化护理的机遇，并阐明了实施GenAI的关键挑战，如数据集成复杂性、数据质量、患者隐私、AI输出临床安全性验证、偏见缓解以及临床接受度。

Conclusion: 生成式AI，特别是大型语言模型，在解读复杂的医疗数据和提高临床效率方面具有强大潜力。本文首次系统总结了GenAI技术在管理由RPM/EHR数据复杂性引起的临床医生数据过载方面的应用。尽管存在巨大机遇，但成功部署GenAI仍需解决一系列关键挑战。

Abstract: Generative Artificial Intelligence (GenAI), particularly Large Language
Models (LLMs), offer powerful capabilities for interpreting the complex data
landscape in healthcare. In this paper, we present a comprehensive overview of
the capabilities, requirements and applications of GenAI for deriving clinical
insights and improving clinical efficiency. We first provide some background on
the forms and sources of patient data, namely real-time Remote Patient
Monitoring (RPM) streams and traditional Electronic Health Records (EHRs). The
sheer volume and heterogeneity of this combined data present significant
challenges to clinicians and contribute to information overload. In addition,
we explore the potential of LLM-powered applications for improving clinical
efficiency. These applications can enhance navigation of longitudinal patient
data and provide actionable clinical decision support through natural language
dialogue. We discuss the opportunities this presents for streamlining clinician
workflows and personalizing care, alongside critical challenges such as data
integration complexity, ensuring data quality and RPM data reliability,
maintaining patient privacy, validating AI outputs for clinical safety,
mitigating bias, and ensuring clinical acceptance. We believe this work
represents the first summarization of GenAI techniques for managing clinician
data overload due to combined RPM / EHR data complexities.

</details>


### [70] [Experimental Assessment of a Multi-Class AI/ML Architecture for Real-Time Characterization of Cyber Events in a Live Research Reactor](https://arxiv.org/abs/2509.00076)
*Zachery Dahm,Konstantinos Vasili,Vasileios Theos,Konstantinos Gkouliaras,William Richards,True Miller,Brian Jowers,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 本文提出并验证了一种多层AI/ML架构，通过整合核反应堆的IT和OT数据流，以区分网络安全事件和其他运行异常。研究在真实反应堆条件下进行，结果表明AI/ML在核网络安全领域具有巨大潜力，但仍需解决数据同步和复杂事件区分的挑战。


<details>
  <summary>Details</summary>
Motivation: 核工业对AI/ML的应用兴趣日益增加，以实现异常识别、故障预测和优化。然而，在实际运行的核反应堆中，AI/ML工具在网络安全方面的可行性和适用性研究尚有限。

Method: 引入了一个多层AI/ML架构，该架构整合了信息技术（IT）和操作技术（OT）数据流。利用普渡大学的PUR-1研究堆，在一个包含多重并发虚假数据注入和拒绝服务攻击的代表性用例中进行了演示和验证。该用例包含14种系统状态和超过1380万个多变量运行及IT数据点。

Result: 研究表明AI/ML能够区分正常、异常和网络安全相关事件，即使在拒绝服务攻击等挑战性条件下也表现出色。结合OT和IT数据提高了分类准确性，但在某些网络事件中，数据同步和收集面临挑战。

Conclusion: AI/ML在核网络安全方面展现出巨大潜力。研究结果也强调，需要进一步完善复杂事件的区分能力和多类别架构，尤其是在数据同步和处理方面。

Abstract: There is increased interest in applying Artificial Intelligence and Machine
Learning (AI/ML) within the nuclear industry and nuclear engineering community.
Effective implementation of AI/ML could offer benefits to the nuclear domain,
including enhanced identification of anomalies, anticipation of system
failures, and operational schedule optimization. However, limited work has been
done to investigate the feasibility and applicability of AI/ML tools in a
functioning nuclear reactor. Here, we go beyond the development of a single
model and introduce a multi-layered AI/ML architecture that integrates both
information technology and operational technology data streams to identify,
characterize, and differentiate (i) among diverse cybersecurity events and (ii)
between cyber events and other operational anomalies. Leveraging Purdue
Universitys research reactor, PUR-1, we demonstrate this architecture through a
representative use case that includes multiple concurrent false data injections
and denial-of-service attacks of increasing complexity under realistic reactor
conditions. The use case includes 14 system states (1 normal, 13 abnormal) and
over 13.8 million multi-variate operational and information technology data
points. The study demonstrated the capability of AI/ML to distinguish between
normal, abnormal, and cybersecurity-related events, even under challenging
conditions such as denial-of-service attacks. Combining operational and
information technology data improved classification accuracy but posed
challenges related to synchronization and collection during certain cyber
events. While results indicate significant promise for AI/ML in nuclear
cybersecurity, the findings also highlight the need for further refinement in
handling complex event differentiation and multi-class architectures.

</details>


### [71] [Data Cartography for Detecting Memorization Hotspots and Guiding Data Interventions in Generative Models](https://arxiv.org/abs/2509.00083)
*Laksh Patel,Neel Shanbhag*

Main category: cs.LG

TL;DR: 本文提出Generative Data Cartography (GenDataCarto)框架，通过评估训练样本的难度和记忆度来指导数据干预，有效降低生成模型的记忆风险和数据泄露，同时对模型性能影响极小。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型存在过拟合问题，易于无意中记忆训练数据中的稀有样本。这不仅可能导致对抗性攻击下的数据泄露，还会虚高模型在基准测试上的表现。

Method: 研究者提出了Generative Data Cartography (GenDataCarto)框架。该框架为每个预训练样本分配一个难度分数（通过早期epoch损失衡量）和一个记忆度分数（通过“遗忘事件”的频率衡量）。随后，将样本划分为四个象限，以指导有针对性的数据剪枝、增权或降权。理论上，证明了所提出的记忆度分数在平滑性假设下是经典影响的下限，并且对高记忆度热点进行降权能够通过统一稳定性界限，可证明地降低泛化差距。

Result: 实验结果表明，GenDataCarto在仅剪枝10%数据的情况下，将合成金丝雀（synthetic canary）提取成功率降低了40%以上，而验证集困惑度仅增加了不到0.5%。

Conclusion: 这些结果表明，有原则的数据干预措施能够显著减轻生成模型的数据泄露问题，且对模型的生成性能影响极小。

Abstract: Modern generative models risk overfitting and unintentionally memorizing rare
training examples, which can be extracted by adversaries or inflate benchmark
performance. We propose Generative Data Cartography (GenDataCarto), a
data-centric framework that assigns each pretraining sample a difficulty score
(early-epoch loss) and a memorization score (frequency of ``forget events''),
then partitions examples into four quadrants to guide targeted pruning and
up-/down-weighting. We prove that our memorization score lower-bounds classical
influence under smoothness assumptions and that down-weighting
high-memorization hotspots provably decreases the generalization gap via
uniform stability bounds. Empirically, GenDataCarto reduces synthetic canary
extraction success by over 40\% at just 10\% data pruning, while increasing
validation perplexity by less than 0.5\%. These results demonstrate that
principled data interventions can dramatically mitigate leakage with minimal
cost to generative performance.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [72] [VOTA: Parallelizing 6G-RAN Experimentation with Virtualized Over-The-Air Workloads](https://arxiv.org/abs/2509.00130)
*Chang Liu,T. D. Khoa Le,Rahul Saini,Kishor C. Joshi,George Exarchakos*

Main category: cs.NI

TL;DR: VOTA是一个开源的、纯软件测试平台扩展方法，通过实时虚拟化和频率调谐，在共享无线测试平台上最大化并行实验，同时控制干扰，实现接近专用资源的效果并增加共享机会。


<details>
  <summary>Details</summary>
Motivation: 无线实验研究中，测试平台共享普遍存在，但其主要缺点是实验不便，研究人员需要延迟实验或忍受计算和射频干扰，这会损害实验的保真度。

Method: 本文提出了VOTA，一个开源的、纯软件的测试平台扩展方法，它利用实时虚拟化和频率调谐来最大化并行实验，同时控制干扰。

Result: 通过在一个32核主机上并行运行两个对干扰敏感的6G用例（MIMO iDFT/DFT卸载和O-RAN DoS攻击），VOTA展示了其能力：在允许2.67倍更多共享机会的同时，实现了类似专用资源的实验结果。

Conclusion: VOTA能够有效解决共享测试平台中的干扰问题，显著提高并行实验的数量和效率，同时保持实验结果的保真度。

Abstract: Testbed sharing, a practice in which different researchers concurrently
develop independent use cases on top of the same testbed, is ubiquitous in
wireless experimental research. Its key drawback is experimental inconvenience:
one must delay experiments or tolerate compute and RF interference that harms
experimental fidelity. In this paper, we propose \textbf{VOTA}, an open-source,
software-only testbed scaling method that leverages real-time virtualization
and frequency tuning to maximize parallel experiments while controlling
interference. In a demonstration of two interference-sensitive 6G use cases --
\textit{MIMO iDFT/DFT Offloading} and \textit{O-RAN DoS Attack} -- running
side-by-side on a 32-core host, we showcase VOTA capabilities:
\textbf{dedicated-like} results while allowing \textbf{2.67$\times$} more
sharing opportunities.

</details>


### [73] [Intelligent Spectrum Management in Satellite Communications](https://arxiv.org/abs/2509.00286)
*Rakshitha De Silva,Shiva Raj Pokhrel,Jonathan Kua,Sithamparanathan Kandeepan*

Main category: cs.NI

TL;DR: 本综述探讨了如何通过认知无线电/卫星（CogSat）和动态频谱管理（DSM），特别是利用人工智能/机器学习技术，解决卫星通信（SatCom）日益增长的频谱稀缺问题，以实现全球连接性的增强。


<details>
  <summary>Details</summary>
Motivation: 现代卫星通信网络对高带宽服务的需求不断扩大，以及巨型卫星星座的激增，暴露出传统独占式卫星频谱分配方法的局限性，导致频谱稀缺问题日益突出，需要更有效的管理方案。

Method: 本文采用综述研究方法，探讨将智能动态频谱管理（DSM）技术适应于卫星通信（SatCom），以实现认知卫星（CogSat）网络。具体方法包括：讨论相关法规和标准化的贡献与障碍，深入分析DSM技术，评估并分类用于DSM的先进AI/ML方法，探讨其运行韧性与鲁棒性，并研究关键性能评估指标。

Result: 本综述探索了智能DSM在SatCom中的适应性，讨论了在法规和标准化方面的贡献与障碍，深入分析了DSM技术以实现CogSat网络。此外，本文广泛评估和分类了用于DSM的AI/ML方法，并探讨了这些集成的运行韧性和鲁棒性，同时考察了关键性能评估指标。

Conclusion: 智能动态频谱管理，特别是结合AI/ML技术，为解决卫星通信的频谱稀缺问题提供了一个有前景的解决方案。通过认知卫星（CogSat）网络，可以构建可持续、可扩展的卫星通信网络，从而增强全球连接性。本综述还明确了监管框架、网络架构和智能频谱管理领域的开放挑战和未来研究方向。

Abstract: Satellite Communication (SatCom) networks represent a fundamental pillar in
modern global connectivity, facilitating reliable service and extensive
coverage across a plethora of applications. The expanding demand for
high-bandwidth services and the proliferation of mega satellite constellations
highlight the limitations of traditional exclusive satellite spectrum
allocation approaches. Cognitive Radio (CR) leading to Cognitive Satellite
(CogSat) networks through Dynamic Spectrum Management (DSM), which enables the
dynamic adaptability of radio equipment to environmental conditions for optimal
performance, presents a promising solution for the emerging spectrum scarcity.
In this survey, we explore the adaptation of intelligent DSM methodologies to
SatCom, leveraging satellite network integrations. We discuss contributions and
hurdles in regulations and standardizations in realizing intelligent DSM in
SatCom, and deep dive into DSM techniques, which enable CogSat networks.
Furthermore, we extensively evaluate and categorize state-of-the-art Artificial
Intelligence (AI)/Machine Learning (ML) methods leveraged for DSM while
exploring operational resilience and robustness of such integrations. In
addition, performance evaluation metrics critical for adaptive resource
management and system optimization in CogSat networks are thoroughly
investigated. This survey also identifies open challenges and outlines future
research directions in regulatory frameworks, network architectures, and
intelligent spectrum management, paving the way for sustainable and scalable
SatCom networks for enhanced global connectivity.

</details>


### [74] [SpliDT: Partitioned Decision Trees for Scalable Stateful Inference at Line Rate](https://arxiv.org/abs/2509.00397)
*Murayyiam Parvez,Annus Zulfiqar,Roman Beltiukov,Shir Landau Feibish,Walter Willinger,Arpit Gupta,Muhammad Shahbaz*

Main category: cs.NI

TL;DR: SPLIDT通过分区推理和资源复用，显著提升了数据平面决策树的特征使用量、准确性和可伸缩性，克服了现有方法的硬件限制。


<details>
  <summary>Details</summary>
Motivation: 机器学习（特别是决策树）在可编程数据平面中的应用受限于需要预先计算所有输入特征，导致模型只能依赖少量固定特征，这严重限制了在严格硬件资源约束下的模型准确性和可伸缩性。

Method: 本文提出了SPLIDT系统，通过在数据包滑动窗口上启用分区推理来重新思考数据平面中的决策树部署。其关键创新包括：1) 为决策树的独立子树分配不同的、可变的特征集，并将其分组到分区中；2) 利用带内控制通道（通过循环回灌）以线速跨分区复用数据平面资源（包括有状态寄存器和匹配键）。SPLIDT还包含一个自定义训练和设计空间探索（DSE）框架，用于联合优化特征分配、树分区和决策树模型深度。

Result: 在多个真实世界数据集上的评估表明，SPLIDT在保持与现有系统相同低检测时间（TTD）的同时，实现了更高的准确性，并支持比现有方法（如NetBeacon和Leo）多达5倍的状态特征。它能够扩展到数百万流，且循环回灌开销极小（<0.05%）。

Conclusion: SPLIDT通过其创新的分区推理和资源复用机制，成功克服了数据平面决策树在特征使用和硬件资源方面的限制，显著提升了模型的准确性、特征利用率和可伸缩性，为数据平面中的实时机器学习应用提供了更强大的解决方案。

Abstract: Machine learning (ML) is increasingly being deployed in programmable data
planes (switches and SmartNICs) to enable real-time traffic analysis, security
monitoring, and in-network decision-making. Decision trees (DTs) are
particularly well-suited for these tasks due to their interpretability and
compatibility with data-plane architectures, i.e., match-action tables (MATs).
However, existing in-network DT implementations are constrained by the need to
compute all input features upfront, forcing models to rely on a small, fixed
set of features per flow. This significantly limits model accuracy and
scalability under stringent hardware resource constraints.
  We present SPLIDT, a system that rethinks DT deployment in the data plane by
enabling partitioned inference over sliding windows of packets. SPLIDT
introduces two key innovations: (1) it assigns distinct, variable feature sets
to individual sub-trees of a DT, grouped into partitions, and (2) it leverages
an in-band control channel (via recirculation) to reuse data-plane resources
(both stateful registers and match keys) across partitions at line rate. These
insights allow SPLIDT to scale the number of stateful features a model can use
without exceeding hardware limits. To support this architecture, SPLIDT
incorporates a custom training and design-space exploration (DSE) framework
that jointly optimizes feature allocation, tree partitioning, and DT model
depth. Evaluation across multiple real-world datasets shows that SPLIDT
achieves higher accuracy while supporting up to 5x more stateful features than
prior approaches (e.g., NetBeacon and Leo). It maintains the same low
time-to-detection (TTD) as these systems, while scaling to millions of flows
with minimal recirculation overhead (<0.05%).

</details>


### [75] [Interference Between FM Cell Sites and CDMA Cell Sites](https://arxiv.org/abs/2509.00567)
*P. Kumar*

Main category: cs.NI

TL;DR: 本论文主要探讨电信领域中，FM蜂窝基站对CDMA蜂窝基站造成的干扰问题。


<details>
  <summary>Details</summary>
Motivation: 干扰是当前电信领域的主要问题。其中，FM蜂窝基站对CDMA蜂窝基站的干扰是一种常见且突出的挑战。

Method: 本文通过讨论干扰的类型并呈现相关的观察结果，来分析FM与CDMA蜂窝基站之间的干扰。

Result: 摘要中未具体给出研究结果，仅指出论文将讨论干扰类型及在干扰过程中获得的各种观察发现。

Conclusion: 摘要中未提供研究结论。

Abstract: Interference is the major problem now days in telecommunication sector. One
type of interference which is very common now days is FM Cell sites
interference between CDMA Cell sites. Which are the types of interference and
various observations during this interference is discussed below in this paper.

</details>


### [76] [SmartFLow: A Communication-Efficient SDN Framework for Cross-Silo Federated Learning](https://arxiv.org/abs/2509.00603)
*Osama Abu Hamdan,Hao Che,Engin Arslan,Md Arifuzzaman*

Main category: cs.NI

TL;DR: 本文提出SmartFLow，一个基于SDN的框架，通过动态调整路由路径来有效减少联邦学习中的网络拥塞，显著提升通信效率和参数同步速度。


<details>
  <summary>Details</summary>
Motivation: 在跨筒仓联邦学习中，客户端与中心服务器频繁交换模型权重，使得训练时间对网络性能高度敏感。然而，传统路由方法未能有效防止拥塞，导致通信延迟增加和训练时间延长。

Method: 提出SmartFLow，一个基于软件定义网络（SDN）的框架，旨在增强跨筒仓联邦学习中的通信效率。SmartFLow通过动态调整路由路径以响应变化的网络条件，从而减少拥塞并提高同步效率。

Result: 实验结果表明，与最短路径路由相比，SmartFLow将参数同步时间减少高达47%；与容量感知路由相比，减少高达41%。此外，它具有极低的计算开销，并能有效扩展到50个客户端的网络。

Conclusion: SmartFLow通过SDN动态路由显著提升了联邦学习的通信效率，有效缩短了参数同步时间，并展现出低计算开销和良好的可扩展性，证明了其在实际联邦学习部署中的实用性。

Abstract: Cross-silo Federated Learning (FL) enables multiple institutions to
collaboratively train machine learning models while preserving data privacy. In
such settings, clients repeatedly exchange model weights with a central server,
making the overall training time highly sensitive to network performance.
However, conventional routing methods often fail to prevent congestion, leading
to increased communication latency and prolonged training. Software-Defined
Networking (SDN), which provides centralized and programmable control over
network resources, offers a promising way to address this limitation. To this
end, we propose SmartFLow, an SDN-based framework designed to enhance
communication efficiency in cross-silo FL. SmartFLow dynamically adjusts
routing paths in response to changing network conditions, thereby reducing
congestion and improving synchronization efficiency. Experimental results show
that SmartFLow decreases parameter synchronization time by up to 47% compared
to shortest-path routing and 41% compared to capacity-aware routing.
Furthermore, it achieves these gains with minimal computational overhead and
scales effectively to networks of up to 50 clients, demonstrating its
practicality for real-world FL deployments.

</details>


### [77] [FLEET: A Federated Learning Emulation and Evaluation Testbed for Holistic Research](https://arxiv.org/abs/2509.00621)
*Osama Abu Hamdan,Hao Che,Engin Arslan,Md Arifuzzaman*

Main category: cs.NI

TL;DR: 为解决现有联邦学习评估工具缺乏真实性，本文提出了FLEET，一个集成学习组件与网络仿真器的新型测试平台，用于在真实网络条件下全面评估联邦学习系统。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习评估工具未能有效模拟真实操作条件，导致联邦学习 (FL) 算法理论设计与实际性能之间存在显著差距，尤其在算法效率、客户端异构性及网络基础设施动态性方面。

Method: 引入了联邦学习仿真与评估测试平台 (FLEET)。该平台通过整合通用、框架无关的学习组件和高保真网络仿真器，提供可扩展和可配置的环境，支持多样机器学习框架、可定制的真实网络拓扑和动态背景流量生成，并收集关联算法结果与详细网络统计数据的整体指标。

Result: FLEET使得研究人员能够系统地调查网络约束（如有限带宽、高延迟和丢包）如何影响联邦学习算法的收敛性和效率。

Conclusion: FLEET为研究社区提供了一个强大的工具，旨在弥合联邦学习算法理论与真实网络条件之间的差距，促进联邦学习系统的整体性与可重现评估。

Abstract: Federated Learning (FL) presents a robust paradigm for privacy-preserving,
decentralized machine learning. However, a significant gap persists between the
theoretical design of FL algorithms and their practical performance, largely
because existing evaluation tools often fail to model realistic operational
conditions. Many testbeds oversimplify the critical dynamics among algorithmic
efficiency, client-level heterogeneity, and continuously evolving network
infrastructure. To address this challenge, we introduce the Federated Learning
Emulation and Evaluation Testbed (FLEET). This comprehensive platform provides
a scalable and configurable environment by integrating a versatile,
framework-agnostic learning component with a high-fidelity network emulator.
FLEET supports diverse machine learning frameworks, customizable real-world
network topologies, and dynamic background traffic generation. The testbed
collects holistic metrics that correlate algorithmic outcomes with detailed
network statistics. By unifying the entire experiment configuration, FLEET
enables researchers to systematically investigate how network constraints, such
as limited bandwidth, high latency, and packet loss, affect the convergence and
efficiency of FL algorithms. This work provides the research community with a
robust tool to bridge the gap between algorithmic theory and real-world network
conditions, promoting the holistic and reproducible evaluation of federated
learning systems.

</details>


### [78] [Unsupervised Dataset Cleaning Framework for Encrypted Traffic Classification](https://arxiv.org/abs/2509.00701)
*Kun Qiu,Ying Wang,Baoqian Li,Wenjun Zhu*

Main category: cs.NI

TL;DR: 针对加密移动流量分类中机器学习对数据清洗的需求，本文提出了一种无监督的自动清洗框架，其性能与手动清洗相当，仅导致2%~2.5%的分类准确率下降，为ML-based分类提供高效预处理。


<details>
  <summary>Details</summary>
Motivation: 随着移动设备普及，加密技术广泛应用于移动应用，导致传统流量分类方法（如DPI）无法识别加密流量。尽管机器学习是解决加密流量分类的有效方案，但其依赖高质量数据，而现有手动数据清洗过程成本高且耗时。

Method: 本文提出了一种无监督框架，能够自动清洗加密移动流量数据，去除不适用于训练的流量（例如无关协议、后台活动、控制平面消息和长时间会话）。

Result: 在真实世界数据集上的评估结果显示，与手动清洗相比，该框架仅导致分类准确率下降2%~2.5%。

Conclusion: 研究结果表明，本文提出的无监督自动清洗方法为基于机器学习的加密流量分类提供了一个高效且有效的预处理步骤。

Abstract: Traffic classification, a technique for assigning network flows to predefined
categories, has been widely deployed in enterprise and carrier networks. With
the massive adoption of mobile devices, encryption is increasingly used in
mobile applications to address privacy concerns. Consequently, traditional
methods such as Deep Packet Inspection (DPI) fail to distinguish encrypted
traffic. To tackle this challenge, Artificial Intelligence (AI), in particular
Machine Learning (ML), has emerged as a promising solution for encrypted
traffic classification. A crucial prerequisite for any ML-based approach is
traffic data cleaning, which removes flows that are not useful for training
(e.g., irrelevant protocols, background activity, control-plane messages, and
long-lived sessions). Existing cleaning solutions depend on manual inspection
of every captured packet, making the process both costly and time-consuming. In
this poster, we present an unsupervised framework that automatically cleans
encrypted mobile traffic. Evaluation on real-world datasets shows that our
framework incurs only a 2%~2.5% reduction in classification accuracy compared
with manual cleaning. These results demonstrate that our method offers an
efficient and effective preprocessing step for ML-based encrypted traffic
classification.

</details>


### [79] [ReWeave: Traffic Engineering with Robust Path Weaving for Localized Link Failure Recover](https://arxiv.org/abs/2509.00708)
*Jingyi Guan,Kun Qiu,Jin Zhao*

Main category: cs.NI

TL;DR: 本文提出ReWeave，一种可扩展高效的链路级流量工程方案，通过为每条链路配备紧凑的邻近备份路径，利用SRv6实现故障链路两端路由器的本地化动态重路由，无需控制器干预，在大型骨干网络中显著优于现有故障恢复方案，降低了链路利用率并保持了快速响应速度。


<details>
  <summary>Details</summary>
Motivation: ISP网络中链路故障频繁，对流量工程（TE）构成严峻挑战。现有TE方案存在不足：要么通过脆弱的静态路径重路由导致性能下降，要么预计算大量故障场景下的备份路由引入高开销并限制可扩展性。因此，需要一种有效的故障恢复机制，在有限开销下提供足够的路径多样性，以确保网络运行的鲁棒性和高性能。

Method: 本文提出了ReWeave，一种可扩展高效的链路级流量工程方案，通过为每条链路配备一套紧凑的、仅基于邻近路径的备份路径来实现本地化重路由。当检测到故障时，只有故障链路两端的路由器会利用基于SRv6的绕行路径动态重路由流量，无需控制器干预或全路径重新计算。

Result: 在大型骨干网络上的评估结果表明，ReWeave在链路故障场景中优于现有TE方案。与HARP（集中式控制和动态流量重新分配的最新故障恢复方案）相比，ReWeave将平均最大链路利用率降低了10.5%~20.1%，将最差情况下的利用率降低了29.5%~40.9%。与Flexile（为多故障场景预计算路由的保护方案）相比，ReWeave在90%的故障情况下实现了相似的低丢包率，同时保持了与最快的基于路由器的本地重路由方案相当的响应速度。

Conclusion: ReWeave提供了一种可扩展且高效的链路级流量工程解决方案，通过本地化重路由实现快速故障恢复，显著提升了网络在链路故障场景下的性能，降低了链路利用率，并保持了快速响应，有效解决了ISP网络中流量工程面临的挑战。

Abstract: Link failures occur frequently in Internet Service Provider (ISP) networks
and pose significant challenges for Traffic Engineering (TE). Existing TE
schemes either reroute traffic over vulnerable static paths, leading to
performance degradation, or precompute backup routes for a broad range of
failure scenarios, which introduces high overhead and limits scalability.
Hence, an effective failure recovery mechanism is required to offer sufficient
path diversity under constrained overhead, thereby ensuring robust and
performant network operation. This paper presents ReWeave, a scalable and
efficient link-level TE scheme that enables localized rerouting by equipping
each link with a compact set of adjacent-only backup paths. Upon detecting a
failure, only the routers at both ends of the failed link reroute traffic
dynamically using SRv6-based detours, without controller intervention or
full-path recomputation. Evaluation results on large-scale backbone networks
demonstrate that ReWeave outperforms existing TE schemes in link failure
scenarios. Compared to HARP, the state-of-the-art failure recovery scheme based
on centralized control and dynamic traffic reallocation, our approach reduces
the average maximum link utilization by 10.5%~20.1%, and lowers the worst-case
utilization by 29.5%~40.9%. When compared with Flexile, a protection-based
scheme that precomputes routes for multi-failure scenarios, ReWeave achieves a
similarly low packet loss rate in 90% of failure cases, while maintaining a
response speed comparable to the fastest router-based local rerouting schemes.

</details>


### [80] [A Modular and Scalable Simulator for Connected-UAVs Communication in 5G Networks](https://arxiv.org/abs/2509.00868)
*Yong Su,Yiyi Chen,Shenghong Yi,Hui Feng,Yuedong Xu,Wang Xiang,Bo Hu*

Main category: cs.NI

TL;DR: 本文开发了一个基于MATLAB的模块化仿真平台，用于研究蜂窝连接无人机系统中的传输协议和切换策略，以应对频繁切换和协议效率低下等挑战。


<details>
  <summary>Details</summary>
Motivation: 蜂窝连接无人机系统面临频繁切换和传统传输协议效率低下等挑战，需要一个专用平台来深入研究这些问题。

Method: 开发了一个基于MATLAB的模块化、可扩展仿真平台。该平台支持5G NR节点部署、可定制的无人机移动模型、多网络接口扩展、多种传输协议（如TCP, UDP, QUIC）以及评估传统和学习型切换策略的切换管理模块。

Result: 成功构建了一个能够灵活模拟和评估不同传输协议及切换策略对无人机通信性能影响的仿真平台。

Conclusion: 该平台可作为蜂窝连接无人机系统中高级传输策略开发和评估的测试平台。

Abstract: Cellular-connected UAV systems have enabled a wide range of low-altitude
aerial services. However, these systems still face many challenges, such as
frequent handovers and the inefficiency of traditional transport protocols. To
better study these issues, we develop a modular and scalable simulation
platform specifically designed for UAVs communication leveraging the research
ecology in wireless communication of MATLAB. The platform supports flexible 5G
NR node deployment, customizable UAVs mobility models, and
multi-network-interface extensions. It also supports multiple transport
protocols including TCP, UDP, QUIC, etc., allowing to investigate how different
transport protocols affect UAVs communication performance.In addition, the
platform includes a handover management module, enabling the evaluation of both
traditional and learning-based handover strategies. Our platform can serve as a
testbed for the development and evaluation of advanced transmission strategies
in cellular-connected UAV systems.

</details>


### [81] [Efficient Multichannel Rendezvous Algorithms without Global Channel Enumeration](https://arxiv.org/abs/2509.00885)
*Yi-Chia Cheng,Cheng-Shang Chang*

Main category: cs.NI

TL;DR: 针对物联网无全局信道枚举的多信道会合问题，本文提出基于LSH的低复杂度算法，实现高效且鲁棒的设备发现，性能媲美现有技术。


<details>
  <summary>Details</summary>
Motivation: 物联网邻居发现中的多信道会合问题面临挑战，尤其是在信道仅有局部L位标识符而无全局枚举系统的场景下，现有方案难以适用，亟需创新算法。

Method: 本文提出了一系列基于局部敏感哈希（LSH）的低复杂度多信道会合算法。具体包括：针对同步和异步设置的LC-LSH及LC-LSH4算法；以及为保证异步设置MTTR有界而融入多集增强模块时钟和准随机技术的ASYM-LC-LSH4和QR-LC-LSH4算法。这些算法受分布式系统一致性哈希启发。

Result: 所提算法显著降低了实现复杂性，同时在期望会合时间（ETTR）性能上与需全局信道枚举的先进方法相当。仿真表明，在同步和异步环境下，即使没有全局信道枚举系统，其性能也与现有LSH算法相当。

Conclusion: 本文提出的基于LSH的多信道会合算法，在缺乏全局信道枚举的物联网环境中，有效解决了邻居发现问题，实现了低复杂度、高性能和鲁棒性，为相关应用提供了有效解决方案。

Abstract: The multichannel rendezvous problem (MRP) is a critical challenge for
neighbor discovery in IoT applications, requiring two users to find each other
by hopping among available channels over time. This paper addresses the MRP in
scenarios where a global channel enumeration system is unavailable. To tackle
this challenge, we propose a suite of low-complexity multichannel rendezvous
algorithms based on locality-sensitive hashing (LSH), tailored for environments
where channel labels are unique L-bit identifiers rather than globally
coordinated indices. Inspired by consistent hashing techniques in distributed
systems, we develop the LC-LSH and LC-LSH4 algorithms for synchronous and
asynchronous settings, respectively. These algorithms significantly reduce
implementation complexity while maintaining expected time-to-rendezvous (ETTR)
performance comparable to state-of-the-art methods that require global channel
enumeration. To ensure bounded maximum time-to-rendezvous (MTTR) in the
asynchronous setting, we further introduce the ASYM-LC-LSH4 and QR-LC-LSH4
algorithms by embedding multiset-enhanced modular clock and quasi-random
techniques into our framework. Extensive simulations demonstrate that the
proposed algorithms achieve performance comparable to state-of-the-art LSH
algorithms in both synchronous and asynchronous settings, even without a global
channel enumeration system.

</details>


### [82] [BUBBLE-BLUE a multihop private network based on Bluetooth](https://arxiv.org/abs/2509.00967)
*Nadjib Achir,Philippe Jacquet*

Main category: cs.NI

TL;DR: BUBBLE-BLUE (BB) 项目旨在利用智能手机的蓝牙功能，构建私有、脱离运营商的自主通信网络，并基于动态连通支配集（CDS）进行路由，展示了其网络特性及路由性能的仿真结果。


<details>
  <summary>Details</summary>
Motivation: 创建基于智能手机蓝牙的私有通信气泡，实现用户间无需依赖运营商网络（数据或蜂窝）的自主通信，目标是构建一种基于智能手机的地面STARLINK网络。

Method: 路由策略基于动态连通支配集（Connected Dominant Sets, CDS）。

Result: 论文介绍了BB网络的具体特征，并展示了其路由性能的一些仿真结果。

Conclusion: 该研究提出了一个基于智能手机蓝牙的自主通信网络方案，并提供了初步的网络功能及路由性能评估。

Abstract: The BUBBLE-BLUE (BB) project aims to create private Bluetooth bubbles on top
of smartphones and to create a kind of terrestrial STARLINK network based on
users smartphones.. In each private bubble, participants will be able to
communicate autonomously, without recourse to private operator networks,
neither data nor cellular, relying solely on the Bluetooth technology of
smartphones. The routing strategy is based on dynamic Connected Dominant Sets
(CDS). We present the specific features of a BB network as well as some
simulation results on their routing performance.

</details>


### [83] [Quantum-based QoE Optimization in Advanced Cellular Networks: Integration and Cloud Gaming Use Case](https://arxiv.org/abs/2509.01008)
*Fatma Chaouech,Javier Villegas,António Pereira,Carlos Baena,Sergio Fortes,Raquel Barco,Dominic Gribben,Mohammad Dib,Alba Villarino,Aser Cortines,Román Orús*

Main category: cs.NI

TL;DR: 本研究探索了量子机器学习（QML）和量子启发（QI）技术在优化5G及未来电信网络端到端服务中的应用，特别是在提升用户体验质量（QoE）方面。结果显示，QML模型在估计精度上与经典机器学习模型相当或更优，同时显著缩短了推理和加载时间，在高维数据处理上展现出更大潜力。


<details>
  <summary>Details</summary>
Motivation: 优化5G及未来电信网络的端到端（E2E）服务是重要挑战，尤其是在提升用户体验质量（QoE）方面。研究旨在探索QML和QI技术在此领域的应用潜力，并与经典机器学习（ML）方法进行性能比较，以克服现有方法的局限性。

Method: 本研究采用了一个结合量子与经典计算的混合框架，旨在利用QML和QI的优势，同时避免完全依赖量子硬件。该框架包含：1) 一个基于用户指标、服务设置和蜂窝配置来估计预期QoE的估算器；2) 一个利用估算结果选择最佳蜂窝和服务配置的优化器。该方法特别应用于云游戏服务的网络配置优化，并通过估算器的准确性、模型加载和推理时间，以及优化器的求解时间和方案得分进行评估。

Result: 研究结果表明，QML模型在估计精度方面与经典ML模型相当或更优，同时显著减少了模型的推理和加载时间。此外，对于更高维度的数据（即更复杂的问题），QML模型展现出实现更好性能的潜力。

Conclusion: 研究结果证明了QML在推动网络优化方面的巨大潜力。尽管如此，数据可用性以及量子与经典ML之间集成复杂性等挑战被确定为未来的研究方向。

Abstract: This work explores the integration of Quantum Machine Learning (QML) and
Quantum-Inspired (QI) techniques for optimizing end-to-end (E2E) network
services in telecommunication systems, particularly focusing on 5G networks and
beyond. The application of QML and QI algorithms is investigated, comparing
their performance with classical Machine Learning (ML) approaches. The present
study employs a hybrid framework combining quantum and classical computing
leveraging the strengths of QML and QI, without the penalty of quantum hardware
availability. This is particularized for the optimization of the Quality of
Experience (QoE) over cellular networks. The framework comprises an estimator
for obtaining the expected QoE based on user metrics, service settings, and
cell configuration, and an optimizer that uses the estimation to choose the
best cell and service configuration. Although the approach is applicable to any
QoE-based network management, its implementation is particularized for the
optimization of network configurations for Cloud Gaming services. Then, it is
evaluated via performance metrics such as accuracy and model loading and
inference times for the estimator, and time to solution and solution score for
the optimizer. The results indicate that QML models achieve similar or superior
accuracy to classical ML models for estimation, while decreasing inference and
loading times. Furthermore, potential for better performance is observed for
higher-dimensional data, highlighting promising results for higher complexity
problems. Thus, the results demonstrate the promising potential of QML in
advancing network optimization, although challenges related to data
availability and integration complexities between quantum and classical ML are
identified as future research lines.

</details>


### [84] [Modeling and Analysis of Coexistence Between MLO NSTR-based Wi-Fi 7 and Legacy Wi-Fi](https://arxiv.org/abs/2509.01201)
*Suhwan Jung,Seokwoo Choi,Youngkeun Yoon,Ho-kyung Son,Hyoil Kim*

Main category: cs.NI

TL;DR: 本文提出马尔可夫链模型分析Wi-Fi 7 MLO与传统Wi-Fi共存性能，并通过ns-3仿真验证。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi 7引入MLO以提升性能，但现有方法忽视了MLO的关键方面，缺乏符合标准的分析框架来研究其与传统Wi-Fi设备的共存性能。

Method: 提出一套新颖的马尔可夫链模型，分别用于建模AP和非AP多链路设备的MLO操作及其多链路退避行为。基于此推导出饱和流量下的传输和碰撞概率，并进一步推导出Wi-Fi 7与传统Wi-Fi共存场景下各类设备的吞吐量闭合表达式。通过开发基于ns-3的模拟器（实现STR和NSTR MLO操作）来验证模型准确性。

Result: 提出的分析模型能够准确估计每设备的吞吐量性能，并揭示了WLAN间共存场景的动态特性。

Conclusion: 该分析模型能准确评估Wi-Fi 7 MLO与传统Wi-Fi的共存性能，有助于深入理解其动态交互。

Abstract: Wi-Fi 7 introduces Multi-link operation (MLO) to enhance throughput and
latency performance compared to legacy Wi-Fi standards. MLO enables
simultaneous transmission and reception through multiple links, departing from
conventional single-link operations (SLO). To fully exploit MLO's potential, it
is essential to investigate Wi-Fi 7's coexistence performance with legacy Wi-Fi
devices. Existing approaches, however, have overlooked some crucial aspects of
MLO, necessitating the development of a standards-compliant analytical
framework to model the actual channel access mechanism of MLO. Therefore, this
paper tries to fill the gap by proposing a set of novel Markov chains (MC) to
accurately model the MLO operation aligned with multi-link backoff behaviors
specified by the standard. Specifically, we design two separate MCs for AP and
non-AP multi-link devices (MLD) respectively, based on which transmit and
collision probabilities are derived under the saturated traffic condition.
Then, we also derive closed-form expressions for the throughput of various
device types in the coexistence scenario between Wi-Fi 7 and legacy Wi-Fi,
including AP MLD, non- AP MLD, and legacy devices. To validate the accuracy of
our proposed models, we developed an ns-3 based simulator by implementing both
STR(simultaneous transmission and reception) and NSTR(non-STR) based MLO
operations. Our ns-3 based extensive simulations have demonstrated that the
proposed analytic model provides accurate estimates on the per device
throughput performance, while also revealing the dynamics of inter-WLAN
coexistence scenarios.

</details>
