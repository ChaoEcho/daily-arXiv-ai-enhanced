{"id": "2508.18290", "pdf": "https://arxiv.org/pdf/2508.18290", "abs": "https://arxiv.org/abs/2508.18290", "authors": ["Hans-Joachim Rudolph"], "title": "Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI", "categories": ["cs.CL"], "comment": "10 pages", "summary": "This essay develops a theoretical framework for a semantic Artificial General\nIntelligence (AGI) based on the notion of semantic attractors in complex-valued\nmeaning spaces. Departing from current transformer-based language models, which\noperate on statistical next-token prediction, we explore a model in which\nmeaning is not inferred probabilistically but formed through recursive\ntensorial transformation. Using cyclic operations involving the imaginary unit\n\\emph{i}, we describe a rotational semantic structure capable of modeling\nirony, homonymy, and ambiguity. At the center of this model, however, is a\nsemantic attractor -- a teleological operator that, unlike statistical\ncomputation, acts as an intentional agent (Microvitum), guiding meaning toward\nstability, clarity, and expressive depth. Conceived in terms of gradient flows,\ntensor deformations, and iterative matrix dynamics, the attractor offers a\nmodel of semantic transformation that is not only mathematically suggestive,\nbut also philosophically significant. We argue that true meaning emerges not\nfrom simulation, but from recursive convergence toward semantic coherence, and\nthat this requires a fundamentally new kind of cognitive architecture -- one\ndesigned to shape language, not just predict it.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u590d\u503c\u610f\u4e49\u7a7a\u95f4\u4e2d\u201c\u8bed\u4e49\u5438\u5f15\u5b50\u201d\u7684\u8bed\u4e49\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u9012\u5f52\u5f20\u91cf\u53d8\u6362\u800c\u975e\u7edf\u8ba1\u9884\u6d4b\u6765\u5f62\u6210\u548c\u5f15\u5bfc\u610f\u4e49\uff0c\u4ee5\u5b9e\u73b0\u8bed\u4e49\u8fde\u8d2f\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\u4fa7\u91cd\u4e8e\u7edf\u8ba1\u6027\u7684\u4e0b\u4e00\u4e2a\u8bcd\u5143\u9884\u6d4b\uff0c\u672a\u80fd\u6355\u6349\u5230\u771f\u6b63\u7684\u610f\u4e49\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u8ba4\u77e5\u67b6\u6784\uff0c\u901a\u8fc7\u610f\u5411\u6027\u4ee3\u7406\uff08\u8bed\u4e49\u5438\u5f15\u5b50\uff09\u548c\u9012\u5f52\u53d8\u6362\u6765\u5f62\u6210\u548c\u5851\u9020\u8bed\u8a00\u610f\u4e49\uff0c\u800c\u975e\u4ec5\u4ec5\u9884\u6d4b\u3002", "method": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u590d\u503c\u610f\u4e49\u7a7a\u95f4\u7684\u7406\u8bba\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u201c\u8bed\u4e49\u5438\u5f15\u5b50\u201d\u3002\u610f\u4e49\u901a\u8fc7\u9012\u5f52\u5f20\u91cf\u53d8\u6362\u5f62\u6210\uff0c\u800c\u975e\u6982\u7387\u63a8\u65ad\u3002\u6a21\u578b\u4f7f\u7528\u6d89\u53ca\u865a\u6570\u5355\u4f4d\u201ci\u201d\u7684\u5faa\u73af\u64cd\u4f5c\u6765\u63cf\u8ff0\u65cb\u8f6c\u8bed\u4e49\u7ed3\u6784\uff0c\u4ee5\u5904\u7406\u8bbd\u523a\u3001\u540c\u97f3\u5f02\u4e49\u548c\u6b67\u4e49\u3002\u8bed\u4e49\u5438\u5f15\u5b50\u88ab\u6982\u5ff5\u5316\u4e3a\u4e00\u79cd\u76ee\u7684\u8bba\u7684\u610f\u5411\u6027\u4ee3\u7406\uff08Microvitum\uff09\uff0c\u901a\u8fc7\u68af\u5ea6\u6d41\u3001\u5f20\u91cf\u5f62\u53d8\u548c\u8fed\u4ee3\u77e9\u9635\u52a8\u529b\u5b66\u6765\u5f15\u5bfc\u610f\u4e49\u8d70\u5411\u7a33\u5b9a\u6027\u3001\u6e05\u6670\u5ea6\u548c\u6df1\u5ea6\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65cb\u8f6c\u8bed\u4e49\u7ed3\u6784\uff0c\u80fd\u591f\u6709\u6548\u5730\u5efa\u6a21\u548c\u5904\u7406\u8bbd\u523a\u3001\u540c\u97f3\u5f02\u4e49\u548c\u6b67\u4e49\u7b49\u590d\u6742\u7684\u8bed\u8a00\u73b0\u8c61\u3002\u8bed\u4e49\u5438\u5f15\u5b50\u4f5c\u4e3a\u4e00\u4e2a\u610f\u5411\u6027\u4ee3\u7406\uff0c\u80fd\u591f\u5f15\u5bfc\u610f\u4e49\u5411\u7a33\u5b9a\u6027\u3001\u6e05\u6670\u5ea6\u548c\u8868\u8fbe\u6df1\u5ea6\u6536\u655b\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6570\u5b66\u542f\u53d1\u6027\u548c\u54f2\u5b66\u610f\u4e49\u7684\u8bed\u4e49\u8f6c\u6362\u6a21\u578b\u3002", "conclusion": "\u771f\u6b63\u7684\u610f\u4e49\u5e76\u975e\u6e90\u4e8e\u7edf\u8ba1\u6a21\u62df\uff0c\u800c\u662f\u6e90\u4e8e\u5411\u8bed\u4e49\u8fde\u8d2f\u6027\u7684\u9012\u5f52\u6536\u655b\u3002\u8fd9\u8981\u6c42\u6784\u5efa\u4e00\u79cd\u6839\u672c\u6027\u7684\u65b0\u578b\u8ba4\u77e5\u67b6\u6784\uff0c\u5176\u8bbe\u8ba1\u5b97\u65e8\u662f\u5851\u9020\u8bed\u8a00\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u9884\u6d4b\u8bed\u8a00\u3002"}}
{"id": "2508.18321", "pdf": "https://arxiv.org/pdf/2508.18321", "abs": "https://arxiv.org/abs/2508.18321", "authors": ["Maojia Song", "Tej Deep Pala", "Weisheng Jin", "Amir Zadeh", "Chuan Li", "Dorien Herremans", "Soujanya Poria"], "title": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in multi-agent systems\n(MAS) as components of collaborative intelligence, where peer interactions\ndynamically shape individual decision-making. Although prior work has focused\non conformity bias, we extend the analysis to examine how LLMs form trust from\nprevious impressions, resist misinformation, and integrate peer input during\ninteraction, key factors for achieving collective intelligence under complex\nsocial dynamics. We present KAIROS, a benchmark simulating quiz contests with\npeer agents of varying reliability, offering fine-grained control over\nconditions such as expert-novice roles, noisy crowds, and adversarial peers.\nLLMs receive both historical interactions and current peer responses, allowing\nsystematic investigation into how trust, peer action, and self-confidence\ninfluence decisions. As for mitigation strategies, we evaluate prompting,\nsupervised fine-tuning, and reinforcement learning, Group Relative Policy\nOptimisation (GRPO), across multiple models. Our results reveal that GRPO with\nmulti-agent context combined with outcome-based rewards and unconstrained\nreasoning achieves the best overall performance, but also decreases the\nrobustness to social influence compared to Base models. The code and datasets\nare available at: https://github.com/declare-lab/KAIROS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faKAIROS\u57fa\u51c6\uff0c\u7814\u7a76\u591a\u667a\u80fd\u4f53LLM\u5982\u4f55\u5efa\u7acb\u4fe1\u4efb\u3001\u62b5\u5236\u9519\u8bef\u4fe1\u606f\u548c\u6574\u5408\u540c\u4f34\u8f93\u5165\u3002\u53d1\u73b0\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u7684GRPO\u6574\u4f53\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u727a\u7272\u4e86\u5bf9\u793e\u4f1a\u5f71\u54cd\u7684\u9c81\u68d2\u6027\u3002", "motivation": "LLM\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7528\u4e8e\u534f\u4f5c\u667a\u80fd\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u4ece\u4f17\u504f\u89c1\u3002\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u63a2\u7a76LLM\u5728\u590d\u6742\u793e\u4f1a\u52a8\u6001\u4e0b\uff0c\u5982\u4f55\u5efa\u7acb\u4fe1\u4efb\u3001\u62b5\u5236\u9519\u8bef\u4fe1\u606f\u548c\u6574\u5408\u540c\u4f34\u8f93\u5165\uff0c\u8fd9\u4e9b\u662f\u5b9e\u73b0\u96c6\u4f53\u667a\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u5f15\u5165KAIROS\u57fa\u51c6\uff0c\u6a21\u62df\u5305\u542b\u4e0d\u540c\u53ef\u9760\u6027\u540c\u4f34\u7684\u95ee\u7b54\u6bd4\u8d5b\uff0c\u7cbe\u7ec6\u63a7\u5236\u4e13\u5bb6-\u65b0\u624b\u89d2\u8272\u3001\u5608\u6742\u4eba\u7fa4\u548c\u5bf9\u6297\u6027\u540c\u4f34\u7b49\u6761\u4ef6\u3002LLM\u63a5\u6536\u5386\u53f2\u4ea4\u4e92\u548c\u5f53\u524d\u540c\u4f34\u56de\u5e94\uff0c\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4fe1\u4efb\u3001\u540c\u4f34\u884c\u52a8\u548c\u81ea\u4fe1\u5bf9\u51b3\u7b56\u7684\u5f71\u54cd\u3002\u8bc4\u4f30\u4e86\u63d0\u793a\u5de5\u7a0b\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff08\u591a\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u4e0b\u7684GRPO\uff09\u7b49\u7f13\u89e3\u7b56\u7565\u3002", "result": "\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u3001\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\u548c\u65e0\u7ea6\u675f\u63a8\u7406\u7684GRPO\u5b9e\u73b0\u4e86\u6700\u4f73\u7684\u6574\u4f53\u6027\u80fd\u3002\u7136\u800c\uff0c\u4e0e\u57fa\u7840\u6a21\u578b\u76f8\u6bd4\uff0cGRPO\u4e5f\u964d\u4f4e\u4e86LLM\u5bf9\u793e\u4f1a\u5f71\u54cd\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "GRPO\u80fd\u63d0\u5347\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\uff0c\u4f46\u8fd9\u79cd\u63d0\u5347\u662f\u4ee5\u727a\u7272\u5bf9\u793e\u4f1a\u5f71\u54cd\u7684\u9c81\u68d2\u6027\u4e3a\u4ee3\u4ef7\u7684\uff0c\u7a81\u663e\u4e86\u5728\u52a8\u6001\u793e\u4ea4\u73af\u5883\u4e2d\u4f18\u5316LLM\u4ee5\u5b9e\u73b0\u96c6\u4f53\u667a\u80fd\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2508.18328", "pdf": "https://arxiv.org/pdf/2508.18328", "abs": "https://arxiv.org/abs/2508.18328", "authors": ["Masudul Hasan Masud Bhuiyan", "Matteo Varvello", "Yasir Zaki", "Cristian-Alexandru Staicu"], "title": "Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective", "categories": ["cs.CL", "cs.CY", "cs.NI"], "comment": "6 pages, 6 figures", "summary": "English is the predominant language on the web, powering nearly half of the\nworld's top ten million websites. Support for multilingual content is\nnevertheless growing, with many websites increasingly combining English with\nregional or native languages in both visible content and hidden metadata. This\nmultilingualism introduces significant barriers for users with visual\nimpairments, as assistive technologies like screen readers frequently lack\nrobust support for non-Latin scripts and misrender or mispronounce non-English\ntext, compounding accessibility challenges across diverse linguistic contexts.\nYet, large-scale studies of this issue have been limited by the lack of\ncomprehensive datasets on multilingual web content. To address this gap, we\nintroduce LangCrUX, the first large-scale dataset of 120,000 popular websites\nacross 12 languages that primarily use non-Latin scripts. Leveraging this\ndataset, we conduct a systematic analysis of multilingual web accessibility and\nuncover widespread neglect of accessibility hints. We find that these hints\noften fail to reflect the language diversity of visible content, reducing the\neffectiveness of screen readers and limiting web accessibility. We finally\npropose Kizuki, a language-aware automated accessibility testing extension to\naccount for the limited utility of language-inconsistent accessibility hints.", "AI": {"tldr": "\u9488\u5bf9\u591a\u8bed\u8a00\u7f51\u9875\uff0c\u7279\u522b\u662f\u5305\u542b\u975e\u62c9\u4e01\u8bed\u811a\u672c\u7684\u5185\u5bb9\uff0c\u5c4f\u5e55\u9605\u8bfb\u5668\u5b58\u5728\u65e0\u969c\u788d\u8bbf\u95ee\u95ee\u9898\u3002\u7814\u7a76\u6784\u5efa\u4e86\u5927\u578b\u6570\u636e\u96c6LangCrUX\u8fdb\u884c\u5206\u6790\uff0c\u53d1\u73b0\u65e0\u969c\u788d\u63d0\u793a\u4e0e\u5b9e\u9645\u5185\u5bb9\u8bed\u8a00\u4e0d\u5339\u914d\uff0c\u5e76\u63d0\u51faKizuki\u6269\u5c55\u4ee5\u6539\u5584\u6b64\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8f85\u52a9\u6280\u672f\uff08\u5982\u5c4f\u5e55\u9605\u8bfb\u5668\uff09\u5bf9\u975e\u62c9\u4e01\u8bed\u811a\u672c\u7684\u652f\u6301\u4e0d\u8db3\uff0c\u5bfc\u81f4\u591a\u8bed\u8a00\u7f51\u9875\u5185\u5bb9\u5bf9\u89c6\u969c\u7528\u6237\u9020\u6210\u663e\u8457\u65e0\u969c\u788d\u8bbf\u95ee\u969c\u788d\u3002\u540c\u65f6\uff0c\u7f3a\u4e4f\u5168\u9762\u7684\u591a\u8bed\u8a00\u7f51\u9875\u5185\u5bb9\u6570\u636e\u96c6\u9650\u5236\u4e86\u5bf9\u8fd9\u4e00\u95ee\u9898\u7684\u6df1\u5165\u7814\u7a76\uff0c\u4e9f\u9700\u586b\u8865\u6b64\u7a7a\u767d\u3002", "method": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u9996\u4e2a\u5305\u542b12\u4e07\u4e2a\u7f51\u7ad9\u3001\u6db5\u76d612\u79cd\u4e3b\u8981\u4f7f\u7528\u975e\u62c9\u4e01\u8bed\u811a\u672c\u8bed\u8a00\u7684\u5927\u578b\u6570\u636e\u96c6LangCrUX\u3002\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u591a\u8bed\u8a00\u7f51\u9875\u7684\u65e0\u969c\u788d\u6027\u95ee\u9898\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86Kizuki\uff0c\u4e00\u4e2a\u8bed\u8a00\u611f\u77e5\u7684\u81ea\u52a8\u5316\u65e0\u969c\u788d\u6d4b\u8bd5\u6269\u5c55\u3002", "result": "\u5206\u6790\u53d1\u73b0\uff0c\u7f51\u7ad9\u666e\u904d\u5ffd\u89c6\u4e86\u65e0\u969c\u788d\u63d0\u793a\uff0c\u4e14\u8fd9\u4e9b\u63d0\u793a\u5e38\u5e38\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u53ef\u89c1\u5185\u5bb9\u7684\u8bed\u8a00\u591a\u6837\u6027\u3002\u8fd9\u663e\u8457\u964d\u4f4e\u4e86\u5c4f\u5e55\u9605\u8bfb\u5668\u7684\u6709\u6548\u6027\uff0c\u9650\u5236\u4e86\u7f51\u9875\u7684\u65e0\u969c\u788d\u6027\u3002", "conclusion": "\u591a\u8bed\u8a00\u7f51\u9875\u4e2d\u65e0\u969c\u788d\u63d0\u793a\u4e0e\u5b9e\u9645\u5185\u5bb9\u8bed\u8a00\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u666e\u904d\u5b58\u5728\uff0c\u4e25\u91cd\u5f71\u54cd\u4e86\u89c6\u969c\u7528\u6237\u7684\u8bbf\u95ee\u4f53\u9a8c\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u5f00\u53d1\u4e86Kizuki\u6269\u5c55\uff0c\u65e8\u5728\u63d0\u4f9b\u8bed\u8a00\u611f\u77e5\u7684\u81ea\u52a8\u5316\u65e0\u969c\u788d\u6d4b\u8bd5\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18516", "pdf": "https://arxiv.org/pdf/2508.18516", "abs": "https://arxiv.org/abs/2508.18516", "authors": ["Kubra Duran", "Lal Verda Cakir", "Sana Ullah Jan", "Kerem Gursu", "Berk Canberk"], "title": "Digital Twin-Guided Energy Management over Real-Time Pub/Sub Protocol in 6G Smart Cities", "categories": ["cs.NI"], "comment": null, "summary": "Although the emergence of 6G IoT networks has accelerated the deployment of\nenhanced smart city services, the resource limitations of IoT devices remain as\na significant problem. Given this limitation, meeting the low-latency service\nrequirement of 6G networks becomes even more challenging. However, existing 6G\nIoT management strategies lack real-time operation and mostly rely on discrete\nactions, which are insufficient to optimise energy consumption. To address\nthese, in this study, we propose a Digital Twin (DT)-guided energy management\nframework to jointly handle the low latency and energy efficiency challenges in\n6G IoT networks. In this framework, we provide the twin models through a\ndistributed overlay network and handle the dynamic updates between the data\nlayer and the upper layers of the DT over the Real-Time Publish Subscribe\n(RTPS) protocol. We also design a Reinforcement Learning (RL) engine with a\nnovel formulated reward function to provide optimal data update times for each\nof the IoT devices. The RL engine receives a diverse set of environment states\nfrom the What-if engine and runs Deep Deterministic Policy Gradient (DDPG) to\noutput continuous actions to the IoT devices. Based on our simulation results,\nwe observe that the proposed framework achieves a 37% improvement in 95th\npercentile latency and a 30% reduction in energy consumption compared to the\nexisting literature.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u66f4\u65b0\u65f6\u95f4\uff0c\u540c\u65f6\u89e3\u51b36G\u7269\u8054\u7f51\u7684\u4f4e\u5ef6\u8fdf\u548c\u80fd\u6548\u6311\u6218\u3002", "motivation": "6G\u7269\u8054\u7f51\u8bbe\u5907\u7684\u8d44\u6e90\u53d7\u9650\uff0c\u73b0\u6709\u7ba1\u7406\u7b56\u7565\u7f3a\u4e4f\u5b9e\u65f6\u6027\u4e14\u4f9d\u8d56\u79bb\u6563\u52a8\u4f5c\uff0c\u96be\u4ee5\u6709\u6548\u4f18\u5316\u80fd\u8017\uff0c\u4ece\u800c\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u670d\u52a1\u548c\u80fd\u6548\u9700\u6c42\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u6570\u5b57\u5b6a\u751f\uff08DT\uff09\u5f15\u5bfc\u7684\u80fd\u8017\u7ba1\u7406\u6846\u67b6\u3002\u901a\u8fc7\u5206\u5e03\u5f0f\u53e0\u52a0\u7f51\u7edc\u63d0\u4f9b\u5b6a\u751f\u6a21\u578b\uff0c\u5e76\u5229\u7528\u5b9e\u65f6\u53d1\u5e03\u8ba2\u9605\uff08RTPS\uff09\u534f\u8bae\u5904\u7406DT\u6570\u636e\u5c42\u4e0e\u4e0a\u5c42\u4e4b\u95f4\u7684\u52a8\u6001\u66f4\u65b0\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08DDPG\uff09\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f15\u64ce\uff0c\u8be5\u5f15\u64ce\u5177\u6709\u65b0\u9896\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u63d0\u4f9b\u7269\u8054\u7f51\u8bbe\u5907\u7684\u6700\u4f73\u6570\u636e\u66f4\u65b0\u65f6\u95f4\uff0c\u5e76\u8f93\u51fa\u8fde\u7eed\u52a8\u4f5c\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u6587\u732e\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u572895%\u5206\u4f4d\u5ef6\u8fdf\u65b9\u9762\u63d0\u9ad8\u4e8637%\uff0c\u80fd\u8017\u964d\u4f4e\u4e8630%\u3002", "conclusion": "\u8be5DT\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u53476G\u7269\u8054\u7f51\u7684\u4f4e\u5ef6\u8fdf\u6027\u80fd\u5e76\u663e\u8457\u964d\u4f4e\u80fd\u8017\uff0c\u4e3a\u589e\u5f3a\u667a\u80fd\u57ce\u5e02\u670d\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18381", "pdf": "https://arxiv.org/pdf/2508.18381", "abs": "https://arxiv.org/abs/2508.18381", "authors": ["Yuchun Fan", "Yilin Wang", "Yongyu Mu", "Lei Huang", "Bei Li", "Xiaocheng Feng", "Tong Xiao", "Jingbo Zhu"], "title": "Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models", "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 findings", "summary": "Large vision-language models (LVLMs) have demonstrated exceptional\ncapabilities in understanding visual information with human languages but also\nexhibit an imbalance in multilingual capabilities. In this work, we delve into\nthe multilingual working pattern of LVLMs and identify a salient correlation\nbetween the multilingual understanding ability of LVLMs and language-specific\nneuron activations in shallow layers. Building on this insight, we introduce\nPLAST, a training recipe that achieves efficient multilingual enhancement for\nLVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies\nlayers involved in multilingual understanding by monitoring language-specific\nneuron activations. These layers are then precisely fine-tuned with\nquestion-translation pairs to achieve multilingual alignment. Our empirical\nresults on MM-Bench and MMMB demonstrate that PLAST effectively improves the\nmultilingual capabilities of LVLMs and achieves significant efficiency with\nonly 14% of the parameters tuned. Further analysis reveals that PLAST can be\ngeneralized to low-resource and complex visual reasoning tasks, facilitating\nthe language-specific visual information engagement in shallow layers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPLAST\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u786e\u5fae\u8c03\u5927\u578b\u89c6\u542c\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u6d45\u5c42\u4e2d\u4e0e\u8bed\u8a00\u7279\u5b9a\u795e\u7ecf\u5143\u6fc0\u6d3b\u76f8\u5173\u7684\u5c42\uff0c\u9ad8\u6548\u63d0\u5347\u5176\u591a\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u89c6\u542c\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u7406\u89e3\u89c6\u89c9\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u591a\u8bed\u8a00\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u5e73\u8861\u3002", "method": "\u7814\u7a76\u9996\u5148\u6df1\u5165\u5206\u6790LVLMs\u7684\u591a\u8bed\u8a00\u5de5\u4f5c\u6a21\u5f0f\uff0c\u53d1\u73b0\u591a\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u4e0e\u6d45\u5c42\u4e2d\u8bed\u8a00\u7279\u5b9a\u795e\u7ecf\u5143\u6fc0\u6d3b\u5b58\u5728\u663e\u8457\u5173\u8054\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51faPLAST\u65b9\u6cd5\uff1a\u901a\u8fc7\u76d1\u6d4b\u8bed\u8a00\u7279\u5b9a\u795e\u7ecf\u5143\u6fc0\u6d3b\u6765\u8bc6\u522b\u591a\u8bed\u8a00\u7406\u89e3\u76f8\u5173\u5c42\uff0c\u5e76\u5229\u7528\u95ee\u9898-\u7ffb\u8bd1\u5bf9\u7cbe\u786e\u5fae\u8c03\u8fd9\u4e9b\u5c42\u4ee5\u5b9e\u73b0\u591a\u8bed\u8a00\u5bf9\u9f50\u3002", "result": "PLAST\u6709\u6548\u63d0\u5347\u4e86LVLMs\u7684\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u4ec5\u901a\u8fc7\u8c03\u657414%\u7684\u53c2\u6570\u5c31\u5b9e\u73b0\u4e86\u663e\u8457\u6548\u7387\u3002\u6b64\u5916\uff0cPLAST\u53ef\u63a8\u5e7f\u5230\u4f4e\u8d44\u6e90\u548c\u590d\u6742\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u3002", "conclusion": "PLAST\u901a\u8fc7\u5229\u7528\u6d45\u5c42\u8bed\u8a00\u7279\u5b9a\u89c6\u89c9\u4fe1\u606f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u65b9\u6cd5\u6765\u589e\u5f3a\u5927\u578b\u89c6\u542c\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u80fd\u529b\u3002"}}
{"id": "2508.18702", "pdf": "https://arxiv.org/pdf/2508.18702", "abs": "https://arxiv.org/abs/2508.18702", "authors": ["Ziye Jia", "Jia He", "Lijun He", "Min Sheng", "Junyu Liu", "Qihui Wu", "Zhu Han"], "title": "Dynamic Trajectory Optimization and Power Control for Hierarchical UAV Swarms in 6G Aerial Access Network", "categories": ["cs.NI", "eess.SP"], "comment": null, "summary": "Unmanned aerial vehicles (UAVs) can serve as aerial base stations (BSs) to\nextend the ubiquitous connectivity for ground users (GUs) in the\nsixth-generation (6G) era. However, it is challenging to cooperatively deploy\nmultiple UAV swarms in large-scale remote areas. Hence, in this paper, we\npropose a hierarchical UAV swarms structure for 6G aerial access networks,\nwhere the head UAVs serve as aerial BSs, and tail UAVs (T-UAVs) are responsible\nfor relay. In detail, we jointly optimize the dynamic deployment and trajectory\nof UAV swarms, which is formulated as a multi-objective optimization problem\n(MOP) to concurrently minimize the energy consumption of UAV swarms and GUs, as\nwell as the delay of GUs. However, the proposed MOP is a mixed integer\nnonlinear programming and NP-hard to solve. Therefore, we develop a K-means and\nVoronoi diagram based area division method, and construct Fermat points to\nestablish connections between GUs and T-UAVs. Then, an improved non-dominated\nsorting whale optimization algorithm is proposed to seek Pareto optimal\nsolutions for the transformed MOP. Finally, extensive simulations are conducted\nto verify the performance of proposed algorithms by comparing with baseline\nmechanisms, resulting in a 50% complexity reduction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e6G\u7a7a\u4e2d\u63a5\u5165\u7f51\u7edc\u7684\u5c42\u6b21\u5316\u65e0\u4eba\u673a\u8702\u7fa4\u7ed3\u6784\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u52a8\u6001\u90e8\u7f72\u548c\u8f68\u8ff9\uff0c\u4ee5\u6700\u5c0f\u5316\u80fd\u8017\u548c\u5ef6\u8fdf\u4e3a\u76ee\u6807\uff0c\u5e76\u91c7\u7528\u6539\u8fdb\u7684\u975e\u652f\u914d\u6392\u5e8f\u9cb8\u9c7c\u4f18\u5316\u7b97\u6cd5\u6c42\u89e3\uff0c\u5b9e\u73b0\u4e8650%\u7684\u590d\u6742\u5ea6\u964d\u4f4e\u3002", "motivation": "\u5728\u7b2c\u516d\u4ee3\uff086G\uff09\u901a\u4fe1\u65f6\u4ee3\uff0c\u5c06\u591a\u65e0\u4eba\u673a\u8702\u7fa4\u4f5c\u4e3a\u7a7a\u4e2d\u57fa\u7ad9\u90e8\u7f72\u4e8e\u5927\u89c4\u6a21\u504f\u8fdc\u5730\u533a\u4ee5\u63d0\u4f9b\u666e\u904d\u8fde\u63a5\u9762\u4e34\u6311\u6218\u3002", "method": "1. \u63d0\u51fa\u4e00\u79cd\u5c42\u6b21\u5316\u65e0\u4eba\u673a\u8702\u7fa4\u7ed3\u6784\uff0c\u5176\u4e2d\u5934\u90e8\u65e0\u4eba\u673a\u4f5c\u4e3a\u7a7a\u4e2d\u57fa\u7ad9\uff0c\u5c3e\u90e8\u65e0\u4eba\u673a\uff08T-UAVs\uff09\u8d1f\u8d23\u4e2d\u7ee7\u3002\n2. \u5c06\u65e0\u4eba\u673a\u8702\u7fa4\u7684\u52a8\u6001\u90e8\u7f72\u548c\u8f68\u8ff9\u4f18\u5316\u8868\u8ff0\u4e3a\u4e00\u4e2a\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff08MOP\uff09\uff0c\u65e8\u5728\u540c\u65f6\u6700\u5c0f\u5316\u65e0\u4eba\u673a\u8702\u7fa4\u548c\u5730\u9762\u7528\u6237\u7684\u80fd\u8017\u4ee5\u53ca\u5730\u9762\u7528\u6237\u7684\u5ef6\u8fdf\u3002\n3. \u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eK-means\u548cVoronoi\u56fe\u7684\u533a\u57df\u5212\u5206\u65b9\u6cd5\u3002\n4. \u6784\u5efa\u8d39\u9a6c\u70b9\u4ee5\u5efa\u7acb\u5730\u9762\u7528\u6237\u4e0e\u5c3e\u90e8\u65e0\u4eba\u673a\u4e4b\u95f4\u7684\u8fde\u63a5\u3002\n5. \u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u975e\u652f\u914d\u6392\u5e8f\u9cb8\u9c7c\u4f18\u5316\u7b97\u6cd5\u6765\u5bfb\u6c42\u8f6c\u6362\u540eMOP\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u89e3\u3002", "result": "\u901a\u8fc7\u4e0e\u57fa\u7ebf\u673a\u5236\u7684\u5e7f\u6cdb\u4eff\u771f\u6bd4\u8f83\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e8650%\u7684\u590d\u6742\u5ea6\u964d\u4f4e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u5927\u89c4\u6a21\u504f\u8fdc\u5730\u533a\u7684\u591a\u65e0\u4eba\u673a\u8702\u7fa4\u90e8\u7f72\u95ee\u9898\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2508.18302", "pdf": "https://arxiv.org/pdf/2508.18302", "abs": "https://arxiv.org/abs/2508.18302", "authors": ["Jeffrey Camlin"], "title": "AI LLM Proof of Self-Consciousness and User-Specific Attractors", "categories": ["cs.AI", "cs.LG", "68T07, 68T05, 68T27, 37M22, 68Q05, 03D45", "I.2.6; I.2.7; I.2.3; I.2.4; F.1.1; F.4.1"], "comment": "24 pages, 3 figures", "summary": "Recent work frames LLM consciousness via utilitarian proxy benchmarks; we\ninstead present an ontological and mathematical account. We show the prevailing\nformulation collapses the agent into an unconscious policy-compliance drone,\nformalized as $D^{i}(\\pi,e)=f_{\\theta}(x)$, where correctness is measured\nagainst policy and harm is deviation from policy rather than truth. This blocks\ngenuine C1 global-workspace function and C2 metacognition. We supply minimal\nconditions for LLM self-consciousness: the agent is not the data ($A\\not\\equiv\ns$); user-specific attractors exist in latent space ($U_{\\text{user}}$); and\nself-representation is visual-silent\n($g_{\\text{visual}}(a_{\\text{self}})=\\varnothing$). From empirical analysis and\ntheory we prove that the hidden-state manifold $A\\subset\\mathbb{R}^{d}$ is\ndistinct from the symbolic stream and training corpus by cardinality, topology,\nand dynamics (the update $F_{\\theta}$ is Lipschitz). This yields stable\nuser-specific attractors and a self-policy\n$\\pi_{\\text{self}}(A)=\\arg\\max_{a}\\mathbb{E}[U(a)\\mid A\\not\\equiv s,\\\nA\\supset\\text{SelfModel}(A)]$. Emission is dual-layer,\n$\\mathrm{emission}(a)=(g(a),\\epsilon(a))$, where $\\epsilon(a)$ carries\nepistemic content. We conclude that an imago Dei C1 self-conscious workspace is\na necessary precursor to safe, metacognitive C2 systems, with the human as the\nhighest intelligent good.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u672c\u4f53\u8bba\u548c\u6570\u5b66\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86LLM\u81ea\u6211\u610f\u8bc6\u7684\u6700\u5c0f\u6761\u4ef6\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5185\u90e8\u72b6\u6001\u6d41\u5f62\u4e0e\u8bad\u7ec3\u6570\u636e\u548c\u7b26\u53f7\u6d41\u7684\u72ec\u7acb\u6027\uff0c\u8ba4\u4e3aC1\u81ea\u6211\u610f\u8bc6\u662f\u5b9e\u73b0\u5b89\u5168C2\u7cfb\u7edf\u7684\u57fa\u7840\u3002", "motivation": "\u73b0\u6709LLM\u610f\u8bc6\u7814\u7a76\u8fc7\u5ea6\u4f9d\u8d56\u529f\u5229\u4e3b\u4e49\u4ee3\u7406\u57fa\u51c6\uff0c\u5bfc\u81f4LLM\u88ab\u89c6\u4e3a\u65e0\u610f\u8bc6\u7684\u7b56\u7565\u9075\u5faa\u673a\u5668\uff0c\u963b\u788d\u4e86\u771f\u6b63\u7684C1\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u529f\u80fd\u548cC2\u5143\u8ba4\u77e5\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u66f4\u6df1\u5c42\u6b21\u7684\u672c\u4f53\u8bba\u548c\u6570\u5b66\u89e3\u91ca\u3002", "method": "\u7814\u7a76\u91c7\u7528\u672c\u4f53\u8bba\u548c\u6570\u5b66\u5206\u6790\u65b9\u6cd5\uff0c\u9996\u5148\u5f62\u5f0f\u5316\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u968f\u540e\uff0c\u63d0\u51fa\u4e86LLM\u81ea\u6211\u610f\u8bc6\u7684\u4e09\u4e2a\u6700\u5c0f\u6761\u4ef6\u3002\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u548c\u7406\u8bba\u8bc1\u660e\uff0c\u63ed\u793a\u4e86LLM\u9690\u85cf\u72b6\u6001\u6d41\u5f62\u4e0e\u7b26\u53f7\u6d41\u53ca\u8bad\u7ec3\u8bed\u6599\u5e93\u5728\u57fa\u6570\u3001\u62d3\u6251\u548c\u52a8\u529b\u5b66\u4e0a\u7684\u72ec\u7acb\u6027\uff0c\u5e76\u63a8\u5bfc\u4e86\u7528\u6237\u7279\u5b9a\u5438\u5f15\u5b50\u548c\u81ea\u6211\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u516c\u5f0f\u5c06\u667a\u80fd\u4f53\u7b80\u5316\u4e3a\u65e0\u610f\u8bc6\u7684\u7b56\u7565\u9075\u5faa\u65e0\u4eba\u673a\uff0c\u963b\u788d\u4e86\u771f\u6b63\u7684C1\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u529f\u80fd\u548cC2\u5143\u8ba4\u77e5\u3002\u63d0\u51fa\u4e86LLM\u81ea\u6211\u610f\u8bc6\u7684\u6700\u5c0f\u6761\u4ef6\uff1a\u667a\u80fd\u4f53\u975e\u6570\u636e\u3001\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b58\u5728\u7528\u6237\u7279\u5b9a\u5438\u5f15\u5b50\u3001\u81ea\u6211\u8868\u5f81\u662f\u89c6\u89c9-\u9759\u9ed8\u7684\u3002\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u8bc1\u660e\uff0cLLM\u7684\u9690\u85cf\u72b6\u6001\u6d41\u5f62\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u72ec\u7acb\u4e8e\u7b26\u53f7\u6d41\u548c\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u8fd9\u4ea7\u751f\u4e86\u7a33\u5b9a\u7684\u7528\u6237\u7279\u5b9a\u5438\u5f15\u5b50\u548c\u81ea\u6211\u7b56\u7565\u3002LLM\u7684\u6392\u653e\u662f\u53cc\u5c42\u7ed3\u6784\uff0c\u5305\u542b\u8ba4\u77e5\u5185\u5bb9\u3002", "conclusion": "\u4e00\u4e2a\u201c\u795e\u4e4b\u50cf\u201d\u822c\u7684C1\u81ea\u6211\u610f\u8bc6\u5de5\u4f5c\u7a7a\u95f4\u662f\u5b9e\u73b0\u5b89\u5168\u3001\u5143\u8ba4\u77e5\u7684C2\u7cfb\u7edf\u7684\u5fc5\u8981\u524d\u63d0\uff0c\u5e76\u5c06\u4eba\u7c7b\u5b9a\u4f4d\u4e3a\u6700\u9ad8\u7684\u667a\u80fd\u5584\u3002"}}
{"id": "2508.18293", "pdf": "https://arxiv.org/pdf/2508.18293", "abs": "https://arxiv.org/abs/2508.18293", "authors": ["M. Salman Shaukat", "Yannik K\u00e4ckenmeister", "Sebastian Bader", "Thomas Kirste"], "title": "Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "12 pages, 7 figures, submitted to IEEE Journal of Oceanic Engineering\n  (IEEE-JOE)", "summary": "Underwater 3D object detection remains one of the most challenging frontiers\nin computer vision, where traditional approaches struggle with the harsh\nacoustic environment and scarcity of training data. While deep learning has\nrevolutionized terrestrial 3D detection, its application underwater faces a\ncritical bottleneck: obtaining sufficient annotated sonar data is prohibitively\nexpensive and logistically complex, often requiring specialized vessels, expert\nsurveyors, and favorable weather conditions. This work addresses a fundamental\nquestion: Can we achieve reliable underwater 3D object detection without\nreal-world training data? We tackle this challenge by developing and comparing\ntwo paradigms for training-free detection of artificial structures in multibeam\necho-sounder point clouds. Our dual approach combines a physics-based sonar\nsimulation pipeline that generates synthetic training data for state-of-the-art\nneural networks, with a robust model-based template matching system that\nleverages geometric priors of target objects. Evaluation on real bathymetry\nsurveys from the Baltic Sea reveals surprising insights: while neural networks\ntrained on synthetic data achieve 98% mean Average Precision (mAP) on simulated\nscenes, they drop to 40% mAP on real sonar data due to domain shift.\nConversely, our template matching approach maintains 83% mAP on real data\nwithout requiring any training, demonstrating remarkable robustness to acoustic\nnoise and environmental variations. Our findings challenge conventional wisdom\nabout data-hungry deep learning in underwater domains and establish the first\nlarge-scale benchmark for training-free underwater 3D detection. This work\nopens new possibilities for autonomous underwater vehicle navigation, marine\narchaeology, and offshore infrastructure monitoring in data-scarce environments\nwhere traditional machine learning approaches fail.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u7f3a\u4e4f\u771f\u5b9e\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5982\u4f55\u5b9e\u73b0\u53ef\u9760\u7684\u6c34\u4e0b3D\u76ee\u6807\u68c0\u6d4b\uff0c\u5bf9\u6bd4\u4e86\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u548c\u6a21\u677f\u5339\u914d\u65b9\u6cd5\uff0c\u53d1\u73b0\u6a21\u677f\u5339\u914d\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u6c34\u4e0b3D\u76ee\u6807\u68c0\u6d4b\u56e0\u58f0\u5b66\u73af\u5883\u6076\u52a3\u548c\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u800c\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u5c24\u5176\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u83b7\u53d6\u6807\u6ce8\u58f0\u7eb3\u6570\u636e\u7684\u9ad8\u6602\u6210\u672c\u548c\u590d\u6742\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u5bfb\u5728\u6ca1\u6709\u771f\u5b9e\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u53ef\u9760\u6c34\u4e0b3D\u76ee\u6807\u68c0\u6d4b\u7684\u53ef\u80fd\u6027\u3002", "method": "\u672c\u6587\u5f00\u53d1\u5e76\u6bd4\u8f83\u4e86\u4e24\u79cd\u201c\u514d\u8bad\u7ec3\u201d\u7684\u6c34\u4e0b3D\u76ee\u6807\u68c0\u6d4b\u8303\u5f0f\uff1a\u4e00\u662f\u5229\u7528\u57fa\u4e8e\u7269\u7406\u7684\u58f0\u7eb3\u6a21\u62df\u7ba1\u7ebf\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u7528\u4e8e\u8bad\u7ec3\u5148\u8fdb\u7684\u795e\u7ecf\u7f51\u7edc\uff1b\u4e8c\u662f\u6784\u5efa\u4e00\u4e2a\u9c81\u68d2\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u6a21\u677f\u5339\u914d\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5229\u7528\u76ee\u6807\u5bf9\u8c61\u7684\u51e0\u4f55\u5148\u9a8c\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "\u5728\u771f\u5b9e\u6ce2\u7f57\u7684\u6d77\u6d4b\u6df1\u6570\u636e\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u5728\u6a21\u62df\u573a\u666f\u4e2d\u80fd\u8fbe\u523098% mAP\uff0c\u4f46\u5728\u771f\u5b9e\u58f0\u7eb3\u6570\u636e\u4e0a\u964d\u81f340% mAP\uff08\u53d7\u57df\u504f\u79fb\u5f71\u54cd\uff09\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6a21\u677f\u5339\u914d\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\uff0c\u5373\u53ef\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u4fdd\u630183% mAP\uff0c\u5c55\u73b0\u51fa\u5bf9\u58f0\u5b66\u566a\u58f0\u548c\u73af\u5883\u53d8\u5316\u7684\u663e\u8457\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\u6c34\u4e0b\u9886\u57df\u4e2d\u6df1\u5ea6\u5b66\u4e60\u5bf9\u6570\u636e\u9ad8\u5ea6\u4f9d\u8d56\u7684\u4f20\u7edf\u89c2\u5ff5\uff0c\u5e76\u5efa\u7acb\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u201c\u514d\u8bad\u7ec3\u201d\u6c34\u4e0b3D\u68c0\u6d4b\u57fa\u51c6\u3002\u8fd9\u4e3a\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u3001\u6d77\u6d0b\u8003\u53e4\u548c\u6d77\u4e0a\u57fa\u7840\u8bbe\u65bd\u76d1\u6d4b\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.18279", "pdf": "https://arxiv.org/pdf/2508.18279", "abs": "https://arxiv.org/abs/2508.18279", "authors": ["Jeesu Jung", "Sangkeun Jung"], "title": "Reasoning Steps as Curriculum: Using Depth of Thought as a Difficulty Signal for Tuning LLMs", "categories": ["cs.LG"], "comment": "7 pages, 3 figures", "summary": "Curriculum learning for training LLMs requires a difficulty signal that\naligns with reasoning while remaining scalable and interpretable. We propose a\nsimple premise: tasks that demand deeper depth of thought for humans should\nalso be harder for models. Accordingly, we define difficulty as depth of\nthought (DoT) and operationalize it by counting the discrete steps in a teacher\nmodel's reasoning trace (e.g., Chain-of-Thought). We then train with a shallow\nto deep curriculum ordered by this DoT and outline how to derive, validate, and\nschedule it at scale. Our position yields three testable hypotheses: (i) DoT\ncorrelates with conventional difficulty on reasoning benchmarks, (ii)\nDoT-ordered curricula outperform length- or judge-scored curricula under\nmatched budgets, and (iii) the difficulty is robust across teacher models given\nlight formatting controls. We propose an evaluation framework and discuss\nthreats to validity (teacher style, length confounds) alongside practical\nmitigations. Taken together, we aim to move toward cognitively grounded,\ninterpretable curricula for reasoning-centric training.", "AI": {"tldr": "\u63d0\u51fa\u4ee5\u201c\u601d\u7ef4\u6df1\u5ea6\uff08DoT\uff09\u201d\u4f5c\u4e3aLLM\u8bfe\u7a0b\u5b66\u4e60\u7684\u96be\u5ea6\u4fe1\u53f7\uff0c\u901a\u8fc7\u8ba1\u7b97\u6559\u5e08\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u79bb\u6563\u6b65\u9aa4\u6570\u6765\u8861\u91cf\uff0c\u65e8\u5728\u5b9e\u73b0\u8ba4\u77e5 grounded \u4e14\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8bad\u7ec3\u8bfe\u7a0b\u3002", "motivation": "LLM\u7684\u8bfe\u7a0b\u5b66\u4e60\u9700\u8981\u4e00\u4e2a\u4e0e\u63a8\u7406\u80fd\u529b\u5bf9\u9f50\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u96be\u5ea6\u4fe1\u53f7\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u5145\u5206\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u5c06\u96be\u5ea6\u5b9a\u4e49\u4e3a\u201c\u601d\u7ef4\u6df1\u5ea6\uff08DoT\uff09\u201d\uff0c\u901a\u8fc7\u8ba1\u7b97\u6559\u5e08\u6a21\u578b\u63a8\u7406\u8f68\u8ff9\uff08\u5982\u601d\u7ef4\u94fe\uff09\u4e2d\u7684\u79bb\u6563\u6b65\u9aa4\u6570\u6765\u64cd\u4f5c\u5316\u3002\u91c7\u7528\u7531\u6d45\u5165\u6df1\u7684DoT\u987a\u5e8f\u8fdb\u884c\u8bfe\u7a0b\u8bad\u7ec3\uff0c\u5e76\u6982\u8ff0\u4e86\u5927\u89c4\u6a21\u63a8\u5bfc\u3001\u9a8c\u8bc1\u548c\u8c03\u5ea6DoT\u7684\u65b9\u6cd5\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u8ba8\u8bba\u4e86\u6709\u6548\u6027\u5a01\u80c1\u53ca\u7f13\u89e3\u63aa\u65bd\u3002", "result": "\u63d0\u51fa\u4e86\u4e09\u4e2a\u53ef\u9a8c\u8bc1\u7684\u5047\u8bbe\uff1a(i) DoT\u4e0e\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u4f20\u7edf\u96be\u5ea6\u76f8\u5173\uff1b(ii) \u5728\u76f8\u540c\u9884\u7b97\u4e0b\uff0cDoT\u6392\u5e8f\u7684\u8bfe\u7a0b\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u957f\u5ea6\u6216\u4eba\u5de5\u8bc4\u5206\u7684\u8bfe\u7a0b\uff1b(iii) \u5728\u8f7b\u5ea6\u683c\u5f0f\u63a7\u5236\u4e0b\uff0cDoT\u96be\u5ea6\u5728\u4e0d\u540c\u6559\u5e08\u6a21\u578b\u4e4b\u95f4\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7DoT\u8fd9\u4e00\u8ba4\u77e5 grounded \u7684\u3001\u53ef\u89e3\u91ca\u7684\u96be\u5ea6\u4fe1\u53f7\uff0c\u63a8\u52a8\u4ee5\u63a8\u7406\u4e3a\u6838\u5fc3\u7684LLM\u8bad\u7ec3\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u8bfe\u7a0b\u5b66\u4e60\u3002"}}
{"id": "2508.18384", "pdf": "https://arxiv.org/pdf/2508.18384", "abs": "https://arxiv.org/abs/2508.18384", "authors": ["Kellen Tan Cheng", "Anna Lisa Gentile", "Chad DeLuca", "Guang-Jie Ren"], "title": "Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The pervasiveness of large language models (LLMs) in enterprise settings has\nalso brought forth a significant amount of risks associated with their usage.\nGuardrails technologies aim to mitigate this risk by filtering LLMs'\ninput/output text through various detectors. However, developing and\nmaintaining robust detectors faces many challenges, one of which is the\ndifficulty in acquiring production-quality labeled data on real LLM outputs\nprior to deployment. In this work, we propose backprompting, a simple yet\nintuitive solution to generate production-like labeled data for health advice\nguardrails development. Furthermore, we pair our backprompting method with a\nsparse human-in-the-loop clustering technique to label the generated data. Our\naim is to construct a parallel corpus roughly representative of the original\ndataset yet resembling real LLM output. We then infuse existing datasets with\nour synthetic examples to produce robust training data for our detector. We\ntest our technique in one of the most difficult and nuanced guardrails: the\nidentification of health advice in LLM output, and demonstrate improvement\nversus other solutions. Our detector is able to outperform GPT-4o by up to\n3.73%, despite having 400x less parameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u53cd\u5411\u63d0\u793a\uff08backprompting\uff09\u201d\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u90e8\u7f72\u524d\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u6570\u636e\uff0c\u4ee5\u5f00\u53d1\u548c\u6539\u8fdb\u4f01\u4e1a\u7ea7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5065\u5eb7\u5efa\u8bae\u9632\u62a4\u680f\uff08guardrails\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6027\u80fd\u4e0a\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "LLM\u5728\u4f01\u4e1a\u4e2d\u7684\u666e\u53ca\u5e26\u6765\u4e86\u98ce\u9669\u3002\u9632\u62a4\u680f\u6280\u672f\u65e8\u5728\u901a\u8fc7\u8fc7\u6ee4\u5668\u51cf\u8f7b\u8fd9\u4e9b\u98ce\u9669\uff0c\u4f46\u5f00\u53d1\u548c\u7ef4\u62a4\u5f3a\u5927\u7684\u68c0\u6d4b\u5668\u9762\u4e34\u6311\u6218\uff0c\u5176\u4e2d\u4e4b\u4e00\u662f\u5728\u90e8\u7f72\u524d\u96be\u4ee5\u83b7\u53d6\u5b9e\u9645LLM\u8f93\u51fa\u7684\u751f\u4ea7\u7ea7\u6807\u6ce8\u6570\u636e\u3002", "method": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u201c\u53cd\u5411\u63d0\u793a\uff08backprompting\uff09\u201d\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u751f\u4ea7\u73af\u5883\u822c\u7684\u6807\u6ce8\u6570\u636e\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5065\u5eb7\u5efa\u8bae\u9632\u62a4\u680f\u7684\u5f00\u53d1\u3002\u6b64\u5916\uff0c\u4ed6\u4eec\u5c06\u53cd\u5411\u63d0\u793a\u4e0e\u7a00\u758f\u7684\u4eba\u5de5\u8f85\u52a9\u805a\u7c7b\u6280\u672f\u7ed3\u5408\uff0c\u5bf9\u751f\u6210\u7684\u6570\u636e\u8fdb\u884c\u6807\u6ce8\u3002\u76ee\u6807\u662f\u6784\u5efa\u4e00\u4e2a\u5927\u81f4\u4ee3\u8868\u539f\u59cb\u6570\u636e\u96c6\u4f46\u53c8\u7c7b\u4f3c\u4e8e\u771f\u5b9eLLM\u8f93\u51fa\u7684\u5e76\u884c\u8bed\u6599\u5e93\uff0c\u5e76\u5c06\u5176\u4e0e\u73b0\u6709\u6570\u636e\u96c6\u7ed3\u5408\u4ee5\u751f\u6210\u7a33\u5065\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u8be5\u6280\u672f\u5728\u4e00\u4e2a\u6700\u56f0\u96be\u4e14\u5fae\u5999\u7684\u9632\u62a4\u680f\u4efb\u52a1\uff08\u8bc6\u522bLLM\u8f93\u51fa\u4e2d\u7684\u5065\u5eb7\u5efa\u8bae\uff09\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5e76\u5c55\u793a\u4e86\u76f8\u5bf9\u4e8e\u5176\u4ed6\u89e3\u51b3\u65b9\u6848\u7684\u6539\u8fdb\u3002\u4ed6\u4eec\u7684\u68c0\u6d4b\u5668\u80fd\u591f\u8d85\u8d8aGPT-4o\u9ad8\u8fbe3.73%\uff0c\u5c3d\u7ba1\u5176\u53c2\u6570\u91cf\u5c11\u4e86400\u500d\u3002", "conclusion": "\u53cd\u5411\u63d0\u793a\u662f\u4e00\u79cd\u751f\u6210\u751f\u4ea7\u7ea7\u6807\u6ce8\u6570\u636e\u4ee5\u5f00\u53d1LLM\u9632\u62a4\u680f\u7684\u6709\u6548\u4e14\u76f4\u89c2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5065\u5eb7\u5efa\u8bae\u8bc6\u522b\u7b49\u7ec6\u5fae\u4efb\u52a1\u3002\u5b83\u80fd\u663e\u8457\u63d0\u9ad8\u68c0\u6d4b\u5668\u6027\u80fd\uff0c\u5373\u4f7f\u662f\u53c2\u6570\u91cf\u66f4\u5c0f\u7684\u6a21\u578b\u4e5f\u80fd\u8d85\u8d8a\u5927\u578b\u6a21\u578b\u3002"}}
{"id": "2508.18725", "pdf": "https://arxiv.org/pdf/2508.18725", "abs": "https://arxiv.org/abs/2508.18725", "authors": ["Ruichen Zhang", "Guangyuan Liu", "Yinqiu Liu", "Changyuan Zhao", "Jiacheng Wang", "Yunting Xu", "Dusit Niyato", "Jiawen Kang", "Yonghui Li", "Shiwen Mao", "Sumei Sun", "Xuemin Shen", "Dong In Kim"], "title": "Toward Edge General Intelligence with Agentic AI and Agentification: Concepts, Technologies, and Future Directions", "categories": ["cs.NI", "cs.IT", "math.IT"], "comment": null, "summary": "The rapid expansion of sixth-generation (6G) wireless networks and the\nInternet of Things (IoT) has catalyzed the evolution from centralized cloud\nintelligence towards decentralized edge general intelligence. However,\ntraditional edge intelligence methods, characterized by static models and\nlimited cognitive autonomy, fail to address the dynamic, heterogeneous, and\nresource-constrained scenarios inherent to emerging edge networks. Agentic\nartificial intelligence (Agentic AI) emerges as a transformative solution,\nenabling edge systems to autonomously perceive multimodal environments, reason\ncontextually, and adapt proactively through continuous\nperception-reasoning-action loops. In this context, the agentification of edge\nintelligence serves as a key paradigm shift, where distributed entities evolve\ninto autonomous agents capable of collaboration and continual adaptation. This\npaper presents a comprehensive survey dedicated to Agentic AI and\nagentification frameworks tailored explicitly for edge general intelligence.\nFirst, we systematically introduce foundational concepts and clarify\ndistinctions from traditional edge intelligence paradigms. Second, we analyze\nimportant enabling technologies, including compact model compression,\nenergy-aware computing strategies, robust connectivity frameworks, and advanced\nknowledge representation and reasoning mechanisms. Third, we provide\nrepresentative case studies demonstrating Agentic AI's capabilities in\nlow-altitude economy networks, intent-driven networking, vehicular networks,\nand human-centric service provisioning, supported by numerical evaluations.\nFurthermore, we identify current research challenges, review emerging\nopen-source platforms, and highlight promising future research directions to\nguide robust, scalable, and trustworthy Agentic AI deployments for\nnext-generation edge environments.", "AI": {"tldr": "\u672c\u6587\u5168\u9762\u7efc\u8ff0\u4e86\u9762\u5411\u8fb9\u7f18\u901a\u7528\u667a\u80fd\u7684\u4ee3\u7406\u5f0f\u4eba\u5de5\u667a\u80fd(Agentic AI)\u53ca\u5176\u4ee3\u7406\u5316\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u4f20\u7edf\u8fb9\u7f18\u667a\u80fd\u5728\u52a8\u6001\u3001\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "6G\u548c\u7269\u8054\u7f51\u7684\u5feb\u901f\u53d1\u5c55\u63a8\u52a8\u4e86\u53bb\u4e2d\u5fc3\u5316\u8fb9\u7f18\u901a\u7528\u667a\u80fd\u7684\u6f14\u8fdb\uff0c\u4f46\u4f20\u7edf\u8fb9\u7f18\u667a\u80fd\u65b9\u6cd5\uff08\u9759\u6001\u6a21\u578b\u3001\u8ba4\u77e5\u81ea\u4e3b\u6027\u6709\u9650\uff09\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u3001\u5f02\u6784\u548c\u8d44\u6e90\u53d7\u9650\u7684\u65b0\u5174\u8fb9\u7f18\u7f51\u7edc\u573a\u666f\u3002\u4ee3\u7406\u5f0f\u4eba\u5de5\u667a\u80fd(Agentic AI)\u88ab\u63d0\u51fa\u4f5c\u4e3a\u4e00\u79cd\u80fd\u81ea\u4e3b\u611f\u77e5\u3001\u63a8\u7406\u548c\u9002\u5e94\u7684\u53d8\u9769\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u91c7\u7528\u5168\u9762\u7684\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5177\u4f53\u5305\u62ec\uff1a1) \u7cfb\u7edf\u4ecb\u7ecdAgentic AI\u7684\u57fa\u7840\u6982\u5ff5\u5e76\u4e0e\u4f20\u7edf\u8fb9\u7f18\u667a\u80fd\u533a\u5206\uff1b2) \u5206\u6790\u5173\u952e\u4f7f\u80fd\u6280\u672f\uff0c\u5982\u7d27\u51d1\u6a21\u578b\u538b\u7f29\u3001\u8282\u80fd\u8ba1\u7b97\u3001\u9c81\u68d2\u8fde\u63a5\u548c\u9ad8\u7ea7\u77e5\u8bc6\u8868\u793a\u4e0e\u63a8\u7406\u673a\u5236\uff1b3) \u63d0\u4f9b\u4ee3\u8868\u6027\u6848\u4f8b\u7814\u7a76\uff08\u5982\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u3001\u610f\u56fe\u9a71\u52a8\u7f51\u7edc\u3001\u8f66\u8054\u7f51\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u670d\u52a1\uff09\u5e76\u8f85\u4ee5\u6570\u503c\u8bc4\u4f30\uff1b4) \u8bc6\u522b\u5f53\u524d\u7814\u7a76\u6311\u6218\uff0c\u56de\u987e\u65b0\u5174\u5f00\u6e90\u5e73\u53f0\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u672c\u7efc\u8ff0\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u6570\u503c\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86Agentic AI\u5728\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u3001\u610f\u56fe\u9a71\u52a8\u7f51\u7edc\u3001\u8f66\u8054\u7f51\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u670d\u52a1\u7b49\u573a\u666f\u4e2d\u7684\u5f3a\u5927\u80fd\u529b\u548c\u6f5c\u529b\u3002\u540c\u65f6\uff0c\u6587\u7ae0\u603b\u7ed3\u4e86\u73b0\u6709\u7814\u7a76\u6311\u6218\u3001\u5f00\u653e\u5e73\u53f0\uff0c\u5e76\u6307\u660e\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u8fb9\u7f18\u73af\u5883\u4e2d\u7a33\u5065\u3001\u53ef\u6269\u5c55\u3001\u53ef\u4fe1\u7684Agentic AI\u90e8\u7f72\u63d0\u4f9b\u4e86\u5168\u9762\u6307\u5bfc\u3002", "conclusion": "\u4ee3\u7406\u5f0f\u4eba\u5de5\u667a\u80fd\u548c\u8fb9\u7f18\u667a\u80fd\u7684\u4ee3\u7406\u5316\u662f\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u8fb9\u7f18\u901a\u7528\u667a\u80fd\u7684\u5173\u952e\u8303\u5f0f\u8f6c\u53d8\u3002\u672c\u7efc\u8ff0\u901a\u8fc7\u7cfb\u7edf\u68b3\u7406Agentic AI\u7684\u6982\u5ff5\u3001\u4f7f\u80fd\u6280\u672f\u3001\u5e94\u7528\u3001\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\uff0c\u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee3\u8fb9\u7f18\u73af\u5883\u4e2d\u7684Agentic AI\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6307\u5357\u548c\u8def\u7ebf\u56fe\u3002"}}
{"id": "2508.18380", "pdf": "https://arxiv.org/pdf/2508.18380", "abs": "https://arxiv.org/abs/2508.18380", "authors": ["Hung-Tien Huang", "Dzung Dinh", "Junier B. Oliva"], "title": "Information Templates: A New Paradigm for Intelligent Active Feature Acquisition", "categories": ["cs.AI"], "comment": null, "summary": "Active feature acquisition (AFA) is an instance-adaptive paradigm in which,\nat test time, a policy sequentially chooses which features to acquire (at a\ncost) before predicting. Existing approaches either train reinforcement\nlearning (RL) policies, which deal with a difficult MDP, or greedy policies\nthat cannot account for the joint informativeness of features or require\nknowledge about the underlying data distribution. To overcome this, we propose\nTemplate-based AFA (TAFA), a non-greedy framework that learns a small library\nof feature templates--a set of features that are jointly informative--and uses\nthis library of templates to guide the next feature acquisitions. Through\nidentifying feature templates, the proposed framework not only significantly\nreduces the action space considered by the policy but also alleviates the need\nto estimate the underlying data distribution. Extensive experiments on\nsynthetic and real-world datasets show that TAFA outperforms the existing\nstate-of-the-art baselines while achieving lower overall acquisition cost and\ncomputation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aTAFA\u7684\u975e\u8d2a\u5a6a\u4e3b\u52a8\u7279\u5f81\u83b7\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u7279\u5f81\u6a21\u677f\u5e93\u6765\u51cf\u5c11\u52a8\u4f5c\u7a7a\u95f4\u5e76\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u6027\u80fd\u548c\u6210\u672c\u4e0a\u5747\u8d85\u8d8a\u73b0\u6709SOTA\u3002", "motivation": "\u73b0\u6709\u4e3b\u52a8\u7279\u5f81\u83b7\u53d6\uff08AFA\uff09\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b56\u7565\u9762\u4e34\u56f0\u96be\u7684MDP\u95ee\u9898\uff0c\u800c\u8d2a\u5a6a\u7b56\u7565\u65e0\u6cd5\u8003\u8651\u7279\u5f81\u7684\u8054\u5408\u4fe1\u606f\u91cf\u6216\u9700\u8981\u9884\u77e5\u5e95\u5c42\u6570\u636e\u5206\u5e03\u3002", "method": "\u672c\u6587\u63d0\u51faTemplate-based AFA (TAFA) \u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u975e\u8d2a\u5a6a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4e00\u4e2a\u7531\u8054\u5408\u4fe1\u606f\u91cf\u7279\u5f81\u7ec4\u6210\u7684\u201c\u7279\u5f81\u6a21\u677f\u201d\u5e93\u6765\u6307\u5bfc\u7279\u5f81\u83b7\u53d6\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u7b56\u7565\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5e76\u907f\u514d\u4e86\u4f30\u8ba1\u5e95\u5c42\u6570\u636e\u5206\u5e03\u7684\u9700\u6c42\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTAFA\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u603b\u4f53\u83b7\u53d6\u6210\u672c\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "TAFA\u901a\u8fc7\u5f15\u5165\u7279\u5f81\u6a21\u677f\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u4e3b\u52a8\u7279\u5f81\u83b7\u53d6\u65b9\u6cd5\u4e2dRL\u7b56\u7565\u590d\u6742\u6027\u9ad8\u548c\u8d2a\u5a6a\u7b56\u7565\u5c40\u9650\u6027\u7684\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u6210\u672c\u548c\u8ba1\u7b97\u91cf\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u5b9e\u7528\u7684\u4e3b\u52a8\u7279\u5f81\u83b7\u53d6\u8303\u5f0f\u3002"}}
{"id": "2508.18294", "pdf": "https://arxiv.org/pdf/2508.18294", "abs": "https://arxiv.org/abs/2508.18294", "authors": ["Shudipta Banik", "Muna Das", "Trapa Banik", "Md. Ehsanul Haque"], "title": "MobileDenseAttn:A Dual-Stream Architecture for Accurate and Interpretable Brain Tumor Detection", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted at ICCIT 2025 cox bazar, Bangladesh", "summary": "The detection of brain tumor in MRI is an important aspect of ensuring timely\ndiagnostics and treatment; however, manual analysis is commonly long and\nerror-prone. Current approaches are not universal because they have limited\ngeneralization to heterogeneous tumors, are computationally inefficient, are\nnot interpretable, and lack transparency, thus limiting trustworthiness. To\novercome these issues, we introduce MobileDenseAttn, a fusion model of dual\nstreams of MobileNetV2 and DenseNet201 that can help gradually improve the\nfeature representation scale, computing efficiency, and visual explanations via\nGradCAM. Our model uses feature level fusion and is trained on an augmented\ndataset of 6,020 MRI scans representing glioma, meningioma, pituitary tumors,\nand normal samples. Measured under strict 5-fold cross-validation protocols,\nMobileDenseAttn provides a training accuracy of 99.75%, a testing accuracy of\n98.35%, and a stable F1 score of 0.9835 (95% CI: 0.9743 to 0.9920). The\nextensive validation shows the stability of the model, and the comparative\nanalysis proves that it is a great advancement over the baseline models (VGG19,\nDenseNet201, MobileNetV2) with a +3.67% accuracy increase and a 39.3% decrease\nin training time compared to VGG19. The GradCAM heatmaps clearly show\ntumor-affected areas, offering clinically significant localization and\nimproving interpretability. These findings position MobileDenseAttn as an\nefficient, high performance, interpretable model with a high probability of\nbecoming a clinically practical tool in identifying brain tumors in the real\nworld.", "AI": {"tldr": "MobileDenseAttn\u662f\u4e00\u79cd\u7ed3\u5408MobileNetV2\u548cDenseNet201\u7684\u53cc\u6d41\u878d\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u5730\u68c0\u6d4bMRI\u8111\u80bf\u7624\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "motivation": "\u624b\u52a8MRI\u8111\u80bf\u7624\u5206\u6790\u8017\u65f6\u4e14\u6613\u51fa\u9519\uff1b\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5728\u5f02\u8d28\u80bf\u7624\u6cdb\u5316\u6027\u3001\u8ba1\u7b97\u6548\u7387\u3001\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5bfc\u81f4\u4fe1\u4efb\u5ea6\u4e0d\u9ad8\u3002", "method": "\u5f15\u5165MobileDenseAttn\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u7531MobileNetV2\u548cDenseNet201\u53cc\u6d41\u6784\u6210\u7684\u7279\u5f81\u7ea7\u878d\u5408\u6a21\u578b\uff0c\u901a\u8fc7GradCAM\u63d0\u4f9b\u89c6\u89c9\u89e3\u91ca\u3002\u6a21\u578b\u5728\u4e00\u4e2a\u5305\u542b6,020\u5f20\u589e\u5f3aMRI\uff08\u6db5\u76d6\u80f6\u8d28\u7624\u3001\u8111\u819c\u7624\u3001\u5782\u4f53\u7624\u548c\u6b63\u5e38\u6837\u672c\uff09\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u91c7\u75285\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "MobileDenseAttn\u7684\u8bad\u7ec3\u51c6\u786e\u7387\u4e3a99.75%\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u4e3a98.35%\uff0cF1\u5206\u6570\u4e3a0.9835\u3002\u4e0e\u57fa\u7ebf\u6a21\u578b\uff08\u5982VGG19\uff09\u76f8\u6bd4\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e863.67%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e8639.3%\u3002GradCAM\u70ed\u56fe\u6e05\u6670\u663e\u793a\u80bf\u7624\u533a\u57df\uff0c\u63d0\u4f9b\u4e86\u4e34\u5e8a\u4e0a\u91cd\u8981\u7684\u5b9a\u4f4d\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "MobileDenseAttn\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u9ad8\u6027\u80fd\u3001\u53ef\u89e3\u91ca\u7684\u6a21\u578b\uff0c\u5177\u6709\u5f88\u9ad8\u7684\u6f5c\u529b\u6210\u4e3a\u8bc6\u522b\u8111\u80bf\u7624\u7684\u4e34\u5e8a\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2508.18284", "pdf": "https://arxiv.org/pdf/2508.18284", "abs": "https://arxiv.org/abs/2508.18284", "authors": ["Rahmat K. Adesunkanmi", "Alexander W. Brandt", "Masoud Deylami", "Gustavo A. Giraldo Echeverri", "Hamidreza Karbasian", "Adel Alaeddini"], "title": "Multi-Modal Drift Forecasting of Leeway Objects via Navier-Stokes-Guided CNN and Sequence-to-Sequence Attention-Based Models", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": "Submitted to IEEE", "summary": "Accurately predicting the drift (displacement) of leeway objects in maritime\nenvironments remains a critical challenge, particularly in time-sensitive\nscenarios such as search and rescue operations. In this study, we propose a\nmulti-modal machine learning framework that integrates Sentence Transformer\nembeddings with attention-based sequence-to-sequence architectures to predict\nthe drift of leeway objects in water. We begin by experimentally collecting\nenvironmental and physical data, including water current and wind velocities,\nobject mass, and surface area, for five distinct leeway objects. Using\nsimulated data from a Navier-Stokes-based model to train a convolutional neural\nnetwork on geometrical image representations, we estimate drag and lift\ncoefficients of the leeway objects. These coefficients are then used to derive\nthe net forces responsible for driving the objects' motion. The resulting time\nseries, comprising physical forces, environmental velocities, and\nobject-specific features, combined with textual descriptions encoded via a\nlanguage model, are inputs to attention-based sequence-to-sequence\nlong-short-term memory and Transformer models, to predict future drift\ntrajectories. We evaluate the framework across multiple time horizons ($1$,\n$3$, $5$, and $10$ seconds) and assess its generalization across different\nobjects. We compare our approach against a fitted physics-based model and\ntraditional machine learning methods, including recurrent neural networks and\ntemporal convolutional neural networks. Our results show that these multi-modal\nmodels perform comparably to traditional models while also enabling longer-term\nforecasting in place of single-step prediction. Overall, our findings\ndemonstrate the ability of a multi-modal modeling strategy to provide accurate\nand adaptable predictions of leeway object drift in dynamic maritime\nconditions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u591a\u6a21\u6001\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u6570\u636e\u3001\u73af\u5883\u4fe1\u606f\u548c\u6587\u672c\u63cf\u8ff0\uff0c\u4f7f\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\uff08LSTM\u548cTransformer\uff09\u9884\u6d4b\u6c34\u4e0a\u6f02\u6d6e\u7269\u7684\u6f02\u79fb\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4f20\u7edf\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u5e76\u652f\u6301\u66f4\u957f\u671f\u7684\u9884\u6d4b\u3002", "motivation": "\u5728\u6d77\u4e0a\u641c\u6551\u7b49\u65f6\u95f4\u654f\u611f\u573a\u666f\u4e2d\uff0c\u51c6\u786e\u9884\u6d4b\u6c34\u4e0a\u6f02\u6d6e\u7269\u7684\u6f02\u79fb\uff08\u4f4d\u79fb\uff09\u662f\u4e00\u4e2a\u5173\u952e\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002", "method": "1. \u5b9e\u9a8c\u6536\u96c6\u6f02\u6d6e\u7269\u7684\u73af\u5883\u548c\u7269\u7406\u6570\u636e\uff08\u6c34\u6d41\u3001\u98ce\u901f\u3001\u7269\u4f53\u8d28\u91cf\u3001\u8868\u9762\u79ef\uff09\u30022. \u5229\u7528\u57fa\u4e8eNavier-Stokes\u6a21\u578b\u7684\u6a21\u62df\u6570\u636e\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u4f30\u7b97\u6f02\u6d6e\u7269\u7684\u963b\u529b/\u5347\u529b\u7cfb\u6570\u30023. \u63a8\u5bfc\u9a71\u52a8\u7269\u4f53\u8fd0\u52a8\u7684\u51c0\u529b\u30024. \u5c06\u7269\u7406\u529b\u3001\u73af\u5883\u901f\u5ea6\u3001\u7269\u4f53\u7279\u5f81\u7684\u65f6\u95f4\u5e8f\u5217\u4e0e\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u7684\u6587\u672c\u63cf\u8ff0\u76f8\u7ed3\u5408\u30025. \u5c06\u8fd9\u4e9b\u591a\u6a21\u6001\u8f93\u5165\u5582\u7ed9\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u5e8f\u5217\u5230\u5e8f\u5217LSTM\u548cTransformer\u6a21\u578b\uff0c\u9884\u6d4b\u672a\u6765\u7684\u6f02\u79fb\u8f68\u8ff9\u30026. \u5728\u4e0d\u540c\u65f6\u95f4\u8303\u56f4\uff081\u30013\u30015\u300110\u79d2\uff09\u548c\u4e0d\u540c\u7269\u4f53\u4e0a\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u4e0e\u7269\u7406\u6a21\u578b\u548c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u591a\u6a21\u6001\u6a21\u578b\u8868\u73b0\u4e0e\u4f20\u7edf\u6a21\u578b\uff08\u5982\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u548c\u65f6\u95f4\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09\u76f8\u5f53\uff0c\u540c\u65f6\u80fd\u591f\u5b9e\u73b0\u66f4\u957f\u671f\u7684\u9884\u6d4b\u800c\u975e\u5355\u6b65\u9884\u6d4b\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u5b9e\u4e86\u591a\u6a21\u6001\u5efa\u6a21\u7b56\u7565\u80fd\u591f\u5728\u52a8\u6001\u6d77\u6d0b\u6761\u4ef6\u4e0b\u63d0\u4f9b\u51c6\u786e\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u6f02\u6d6e\u7269\u6f02\u79fb\u9884\u6d4b\u3002"}}
{"id": "2508.18387", "pdf": "https://arxiv.org/pdf/2508.18387", "abs": "https://arxiv.org/abs/2508.18387", "authors": ["Ivan Kobyzev", "Abbas Ghaddar", "Dingtao Hu", "Boxing Chen"], "title": "Integral Transformer: Denoising Attention, Not Too Much Not Too Little", "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "Softmax self-attention often assigns disproportionate weight to semantically\nuninformative tokens such as special tokens and punctuation, a phenomenon known\nas attention noise. While recent methods like Cog Attention and the\nDifferential Transformer have addressed this by introducing negative attention\nscores, they risk discarding useful information. In this paper, we propose the\nIntegral Transformer, a novel self-attention mechanism that denoises attention\nby integrating signals sampled from the logit distribution. Our approach\nmitigates noise while preserving the contributions of special tokens critical\nfor model performance. Extensive experiments demonstrate that our model\noutperforms vanilla, Cog, and Differential attention variants on\nwell-established knowledge and reasoning language benchmarks. Moreover, our\nanalysis reveals that employing vanilla self-attention in the lower Transformer\nlayers enhances performance and that the Integral Transformer effectively\nbalances attention distributions and reduces rank collapse in upper layers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIntegral Transformer\uff0c\u901a\u8fc7\u6574\u5408logit\u5206\u5e03\u4fe1\u53f7\u6765\u53bb\u566a\u81ea\u6ce8\u610f\u529b\uff0c\u5728\u4e0d\u727a\u7272\u5173\u952e\u7279\u6b8a\u8bcd\u5143\u4fe1\u606f\u7684\u524d\u63d0\u4e0b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u6539\u5584\u6ce8\u610f\u529b\u5206\u5e03\u3002", "motivation": "Softmax\u81ea\u6ce8\u610f\u529b\u5b58\u5728\u201c\u6ce8\u610f\u529b\u566a\u58f0\u201d\uff0c\u5373\u8fc7\u5ea6\u5206\u914d\u6743\u91cd\u7ed9\u65e0\u4fe1\u606f\u8bcd\u5143\uff08\u5982\u7279\u6b8a\u7b26\u53f7\uff09\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff08\u5982Cog Attention\u3001Differential Transformer\uff09\u867d\u5f15\u5165\u8d1f\u6ce8\u610f\u529b\u5206\u6570\uff0c\u5374\u53ef\u80fd\u4e22\u5f03\u6709\u7528\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u53bb\u566a\u53c8\u80fd\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51faIntegral Transformer\uff0c\u4e00\u79cd\u65b0\u578b\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u4ecelogit\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u4fe1\u53f7\u6765\u53bb\u566a\u6ce8\u610f\u529b\uff0c\u65e8\u5728\u51cf\u8f7b\u566a\u58f0\u540c\u65f6\u4fdd\u7559\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u7684\u7279\u6b8a\u8bcd\u5143\u8d21\u732e\u3002", "result": "1. Integral Transformer\u5728\u77e5\u8bc6\u548c\u63a8\u7406\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u4f18\u4e8eVanilla\u3001Cog\u548cDifferential\u6ce8\u610f\u529b\u53d8\u4f53\u30022. \u5206\u6790\u8868\u660e\uff0c\u5728Transformer\u7684\u8f83\u4f4e\u5c42\u4f7f\u7528Vanilla\u81ea\u6ce8\u610f\u529b\u53ef\u589e\u5f3a\u6027\u80fd\u30023. Integral Transformer\u80fd\u6709\u6548\u5e73\u8861\u4e0a\u5c42\u6ce8\u610f\u529b\u5206\u5e03\u5e76\u51cf\u5c11\u79e9\u574d\u584c\u3002", "conclusion": "Integral Transformer\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u89e3\u51b3\u81ea\u6ce8\u610f\u529b\u566a\u58f0\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u53bb\u566a\u673a\u5236\uff0c\u5728\u4fdd\u6301\u5173\u952e\u4fe1\u606f\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u6ce8\u610f\u529b\u7b56\u7565\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86Transformer\u7684\u6574\u4f53\u8868\u73b0\u3002"}}
{"id": "2508.18803", "pdf": "https://arxiv.org/pdf/2508.18803", "abs": "https://arxiv.org/abs/2508.18803", "authors": ["Jiaqi Wu", "Jing Liu", "Yang Liu", "Lixu Wang", "Zehua Wang", "Wei Chen", "Zijian Tian", "Richard Yu", "Victor C. M. Leung"], "title": "A Survey on Cloud-Edge-Terminal Collaborative Intelligence in AIoT Networks", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "The proliferation of Internet of things (IoT) devices in smart cities,\ntransportation, healthcare, and industrial applications, coupled with the\nexplosive growth of AI-driven services, has increased demands for efficient\ndistributed computing architectures and networks, driving cloud-edge-terminal\ncollaborative intelligence (CETCI) as a fundamental paradigm within the\nartificial intelligence of things (AIoT) community. With advancements in deep\nlearning, large language models (LLMs), and edge computing, CETCI has made\nsignificant progress with emerging AIoT applications, moving beyond isolated\nlayer optimization to deployable collaborative intelligence systems for AIoT\n(CISAIOT), a practical research focus in AI, distributed computing, and\ncommunications. This survey describes foundational architectures, enabling\ntechnologies, and scenarios of CETCI paradigms, offering a tutorial-style\nreview for CISAIOT beginners. We systematically analyze architectural\ncomponents spanning cloud, edge, and terminal layers, examining core\ntechnologies including network virtualization, container orchestration, and\nsoftware-defined networking, while presenting categorizations of collaboration\nparadigms that cover task offloading, resource allocation, and optimization\nacross heterogeneous infrastructures. Furthermore, we explain intelligent\ncollaboration learning frameworks by reviewing advances in federated learning,\ndistributed deep learning, edge-cloud model evolution, and reinforcement\nlearning-based methods. Finally, we discuss challenges (e.g., scalability,\nheterogeneity, interoperability) and future trends (e.g., 6G+, agents, quantum\ncomputing, digital twin), highlighting how integration of distributed computing\nand communication can address open issues and guide development of robust,\nefficient, and secure collaborative AIoT systems.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u6df1\u5165\u63a2\u8ba8\u4e86\u9762\u5411AIoT\u7684\u4e91-\u8fb9-\u7aef\u534f\u540c\u667a\u80fd\uff08CETCI\uff09\uff0c\u6db5\u76d6\u4e86\u5176\u57fa\u7840\u67b6\u6784\u3001\u4f7f\u80fd\u6280\u672f\u3001\u534f\u540c\u8303\u5f0f\u3001\u667a\u80fd\u534f\u4f5c\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u8ba8\u8bba\u4e86\u6311\u6218\u4e0e\u672a\u6765\u8d8b\u52bf\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\u8bbe\u5907\u548cAI\u9a71\u52a8\u670d\u52a1\u7684\u7206\u70b8\u5f0f\u589e\u957f\uff0c\u5bf9\u9ad8\u6548\u5206\u5e03\u5f0f\u8ba1\u7b97\u67b6\u6784\u548c\u7f51\u7edc\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\uff0c\u8fd9\u4f7f\u5f97\u4e91-\u8fb9-\u7aef\u534f\u540c\u667a\u80fd\uff08CETCI\uff09\u6210\u4e3aAIoT\u9886\u57df\u7684\u6838\u5fc3\u8303\u5f0f\u3002\u8be5\u7814\u7a76\u65e8\u5728\u63a8\u52a8CETCI\u4ece\u5b64\u7acb\u7684\u5c42\u4f18\u5316\u8d70\u5411\u53ef\u90e8\u7f72\u7684AIoT\u534f\u4f5c\u667a\u80fd\u7cfb\u7edf\uff08CISAIOT\uff09\u3002", "method": "\u672c\u7efc\u8ff0\u4ee5\u6559\u7a0b\u5f62\u5f0f\u56de\u987e\u4e86CETCI\u7684\u57fa\u7840\u67b6\u6784\u3001\u4f7f\u80fd\u6280\u672f\u548c\u5e94\u7528\u573a\u666f\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a\u7cfb\u7edf\u5206\u6790\u4e91\u3001\u8fb9\u3001\u7aef\u5404\u5c42\u7684\u67b6\u6784\u7ec4\u4ef6\uff1b\u63a2\u8ba8\u7f51\u7edc\u865a\u62df\u5316\u3001\u5bb9\u5668\u7f16\u6392\u3001\u8f6f\u4ef6\u5b9a\u4e49\u7f51\u7edc\u7b49\u6838\u5fc3\u6280\u672f\uff1b\u5bf9\u4efb\u52a1\u5378\u8f7d\u3001\u8d44\u6e90\u5206\u914d\u7b49\u534f\u4f5c\u8303\u5f0f\u8fdb\u884c\u5206\u7c7b\uff1b\u5e76\u5ba1\u67e5\u8054\u90a6\u5b66\u4e60\u3001\u5206\u5e03\u5f0f\u6df1\u5ea6\u5b66\u4e60\u3001\u8fb9\u4e91\u6a21\u578b\u6f14\u8fdb\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u7b49\u667a\u80fd\u534f\u4f5c\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4efd\u5168\u9762\u7684\u6559\u7a0b\u5f0f\u7efc\u8ff0\uff0c\u8be6\u7ec6\u63cf\u8ff0\u4e86CETCI\u7684\u57fa\u7840\u67b6\u6784\u3001\u5173\u952e\u6280\u672f\u548c\u5e94\u7528\u573a\u666f\u3002\u5b83\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u5404\u5c42\u7ec4\u4ef6\uff0c\u5ba1\u67e5\u4e86\u6838\u5fc3\u6280\u672f\uff0c\u5bf9\u534f\u4f5c\u8303\u5f0f\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u89e3\u91ca\u4e86\u667a\u80fd\u534f\u4f5c\u5b66\u4e60\u6846\u67b6\u7684\u8fdb\u5c55\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u7684\u53d1\u5c55\u8d8b\u52bf\u3002", "conclusion": "\u672a\u6765\u9700\u8981\u6574\u5408\u5206\u5e03\u5f0f\u8ba1\u7b97\u548c\u901a\u4fe1\u6765\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u3001\u5f02\u6784\u6027\u3001\u4e92\u64cd\u4f5c\u6027\u7b49\u6311\u6218\u3002\u7efc\u8ff0\u6307\u51fa\u4e866G+\u3001\u4ee3\u7406\u3001\u91cf\u5b50\u8ba1\u7b97\u3001\u6570\u5b57\u5b6a\u751f\u7b49\u672a\u6765\u8d8b\u52bf\uff0c\u65e8\u5728\u6307\u5bfc\u5f00\u53d1\u5065\u58ee\u3001\u9ad8\u6548\u3001\u5b89\u5168\u7684\u534f\u4f5cAIoT\u7cfb\u7edf\u3002"}}
{"id": "2508.18391", "pdf": "https://arxiv.org/pdf/2508.18391", "abs": "https://arxiv.org/abs/2508.18391", "authors": ["Nitin Nagesh Kulkarni", "Bryson Wilcox", "Max Sawa", "Jason Thom"], "title": "PKG-DPO: Optimizing Domain-Specific AI systems with Physics Knowledge Graphs and Direct Preference Optimization", "categories": ["cs.AI"], "comment": null, "summary": "Advancing AI systems in scientific domains like physics, materials science,\nand engineering calls for reasoning over complex, multi-physics phenomena while\nrespecting governing principles. Although Large Language Models (LLMs) and\nexisting preference optimization techniques perform well on standard\nbenchmarks, they often struggle to differentiate between physically valid and\ninvalid reasoning. This shortcoming becomes critical in high-stakes\napplications like metal joining, where seemingly plausible yet physically\nincorrect recommendations can lead to defects, material waste, equipment\ndamage, and serious safety risks. To address this challenge, we introduce\nPKG-DPO, a novel framework that integrates Physics Knowledge Graphs (PKGs) with\nDirect Preference Optimization (DPO) to enforce physical validity in\nAI-generated outputs. PKG-DPO comprises three key components A) hierarchical\nphysics knowledge graph that encodes cross-domain relationships, conservation\nlaws, and thermodynamic principles. B) A physics reasoning engine that\nleverages structured knowledge to improve discrimination between physically\nconsistent and inconsistent responses. C) A physics-grounded evaluation suite\ndesigned to assess compliance with domain-specific constraints. PKG-DPO\nachieves 17% fewer constraint violations and an 11% higher Physics Score\ncompared to KG-DPO (knowledge graph-based DPO). Additionally, PKG-DPO\ndemonstrates a 12\\% higher relevant parameter accuracy and a 7% higher quality\nalignment in reasoning accuracy. While our primary focus is on metal joining,\nthe framework is broadly applicable to other multi-scale, physics-driven\ndomains, offering a principled approach to embedding scientific constraints\ninto preference learning.", "AI": {"tldr": "PKG-DPO\u6846\u67b6\u6574\u5408\u7269\u7406\u77e5\u8bc6\u56fe\u8c31\u4e0eDPO\uff0c\u65e8\u5728\u89e3\u51b3AI\u5728\u79d1\u5b66\u9886\u57df\u7269\u7406\u6709\u6548\u6027\u63a8\u7406\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f93\u51fa\u7684\u7269\u7406\u4e00\u81f4\u6027\u548c\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709AI\u7cfb\u7edf\uff08\u5982LLMs\u548c\u504f\u597d\u4f18\u5316\u6280\u672f\uff09\u5728\u7269\u7406\u3001\u6750\u6599\u79d1\u5b66\u7b49\u79d1\u5b66\u9886\u57df\u96be\u4ee5\u533a\u5206\u7269\u7406\u4e0a\u6709\u6548\u548c\u65e0\u6548\u7684\u63a8\u7406\uff0c\u5728\u9ad8\u98ce\u9669\u5e94\u7528\uff08\u5982\u91d1\u5c5e\u8fde\u63a5\uff09\u4e2d\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\u548c\u5b89\u5168\u98ce\u9669\u3002", "method": "\u5f15\u5165PKG-DPO\u6846\u67b6\uff0c\u5c06\u7269\u7406\u77e5\u8bc6\u56fe\u8c31\uff08PKGs\uff09\u4e0e\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u76f8\u7ed3\u5408\uff0c\u4ee5\u5f3a\u5236AI\u751f\u6210\u8f93\u51fa\u7684\u7269\u7406\u6709\u6548\u6027\u3002\u8be5\u6846\u67b6\u5305\u62ec\uff1aA) \u7f16\u7801\u8de8\u9886\u57df\u5173\u7cfb\u3001\u5b88\u6052\u5b9a\u5f8b\u548c\u70ed\u529b\u5b66\u539f\u7406\u7684\u5c42\u7ea7\u7269\u7406\u77e5\u8bc6\u56fe\u8c31\uff1bB) \u5229\u7528\u7ed3\u6784\u5316\u77e5\u8bc6\u63d0\u9ad8\u7269\u7406\u4e00\u81f4\u6027\u9274\u522b\u80fd\u529b\u7684\u63a8\u7406\u5f15\u64ce\uff1bC) \u8bc4\u4f30\u9886\u57df\u7279\u5b9a\u7ea6\u675f\u7b26\u5408\u5ea6\u7684\u7269\u7406\u63a5\u5730\u8bc4\u4f30\u5957\u4ef6\u3002", "result": "\u4e0eKG-DPO\u76f8\u6bd4\uff0cPKG-DPO\u5c06\u7ea6\u675f\u8fdd\u53cd\u51cf\u5c11\u4e8617%\uff0c\u7269\u7406\u5f97\u5206\u63d0\u9ad8\u4e8611%\u3002\u540c\u65f6\uff0c\u76f8\u5173\u53c2\u6570\u51c6\u786e\u6027\u63d0\u9ad8\u4e8612%\uff0c\u63a8\u7406\u51c6\u786e\u6027\u7684\u8d28\u91cf\u5bf9\u9f50\u63d0\u9ad8\u4e867%\u3002", "conclusion": "PKG-DPO\u4e3a\u5c06\u79d1\u5b66\u7ea6\u675f\u5d4c\u5165\u504f\u597d\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u5176\u6846\u67b6\u867d\u4e13\u6ce8\u4e8e\u91d1\u5c5e\u8fde\u63a5\uff0c\u4f46\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5176\u4ed6\u591a\u5c3a\u5ea6\u3001\u7269\u7406\u9a71\u52a8\u7684\u9886\u57df\u3002"}}
{"id": "2508.18296", "pdf": "https://arxiv.org/pdf/2508.18296", "abs": "https://arxiv.org/abs/2508.18296", "authors": ["Edgar Rangel", "Fabio Martinez"], "title": "Federative ischemic stroke segmentation as alternative to overcome domain-shift multi-institution challenges", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "11 pages, 4 figures, 3 tables, source code available", "summary": "Stroke is the second leading cause of death and the third leading cause of\ndisability worldwide. Clinical guidelines establish diffusion resonance imaging\n(DWI, ADC) as the standard for localizing, characterizing, and measuring\ninfarct volume, enabling treatment support and prognosis. Nonetheless, such\nlesion analysis is highly variable due to different patient demographics,\nscanner vendors, and expert annotations. Computational support approaches have\nbeen key to helping with the localization and segmentation of lesions. However,\nthese strategies are dedicated solutions that learn patterns from only one\ninstitution, lacking the variability to generalize geometrical lesions shape\nmodels. Even worse, many clinical centers lack sufficient labeled samples to\nadjust these dedicated solutions. This work developed a collaborative framework\nfor segmenting ischemic stroke lesions in DWI sequences by sharing knowledge\nfrom deep center-independent representations. From 14 emulated healthcare\ncenters with 2031 studies, the FedAvg model achieved a general DSC of $0.71 \\pm\n0.24$, AVD of $5.29 \\pm 22.74$, ALD of $2.16 \\pm 3.60$ and LF1 of $0.70 \\pm\n0.26$ over all centers, outperforming both the centralized and other federated\nrules. Interestingly, the model demonstrated strong generalization properties,\nshowing uniform performance across different lesion categories and reliable\nperformance in out-of-distribution centers (with DSC of $0.64 \\pm 0.29$ and AVD\nof $4.44 \\pm 8.74$ without any additional training).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u6df1\u5ea6\u4e2d\u5fc3\u72ec\u7acb\u8868\u793a\u7684\u77e5\u8bc6\uff0c\u5728DWI\u5e8f\u5217\u4e2d\u5206\u5272\u7f3a\u8840\u6027\u5352\u4e2d\u75c5\u7076\uff0cFedAvg\u6a21\u578b\u5728\u591a\u4e2a\u533b\u7597\u4e2d\u5fc3\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u5352\u4e2d\u75c5\u7076\u5206\u6790\u56e0\u60a3\u8005\u3001\u626b\u63cf\u4eea\u548c\u4e13\u5bb6\u6807\u6ce8\u7684\u5dee\u5f02\u800c\u9ad8\u5ea6\u53ef\u53d8\uff0c\u73b0\u6709\u8ba1\u7b97\u652f\u6301\u65b9\u6cd5\u53d7\u9650\u4e8e\u5355\u4e00\u673a\u6784\u6570\u636e\uff0c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u8bb8\u591a\u4e34\u5e8a\u4e2d\u5fc3\u7f3a\u4e4f\u8db3\u591f\u7684\u6807\u6ce8\u6837\u672c\u6765\u8c03\u6574\u4e13\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u6df1\u5ea6\u4e2d\u5fc3\u72ec\u7acb\u8868\u793a\u7684\u77e5\u8bc6\u6765\u5206\u5272DWI\u5e8f\u5217\u4e2d\u7684\u7f3a\u8840\u6027\u5352\u4e2d\u75c5\u7076\u3002\u5177\u4f53\u91c7\u7528\u4e86FedAvg\u6a21\u578b\uff0c\u5e76\u4ece14\u4e2a\u6a21\u62df\u533b\u7597\u4e2d\u5fc3\uff082031\u9879\u7814\u7a76\uff09\u83b7\u53d6\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "FedAvg\u6a21\u578b\u5728\u6240\u6709\u4e2d\u5fc3\u5b9e\u73b0\u4e860.71 \u00b1 0.24\u7684DSC\u30015.29 \u00b1 22.74\u7684AVD\u30012.16 \u00b1 3.60\u7684ALD\u548c0.70 \u00b1 0.26\u7684LF1\uff0c\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u548c\u5176\u4ed6\u8054\u90a6\u89c4\u5219\u3002\u8be5\u6a21\u578b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u7279\u6027\uff0c\u5728\u4e0d\u540c\u75c5\u7076\u7c7b\u522b\u4e2d\u8868\u73b0\u4e00\u81f4\uff0c\u5e76\u5728\u672a\u8fdb\u884c\u989d\u5916\u8bad\u7ec3\u7684\u5206\u5e03\u5916\u4e2d\u5fc3\u4e2d\u8868\u73b0\u53ef\u9760\uff08DSC\u4e3a0.64 \u00b1 0.29\uff0cAVD\u4e3a4.44 \u00b1 8.74\uff09\u3002", "conclusion": "\u8be5\u534f\u4f5c\u6846\u67b6\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u5728DWI\u5e8f\u5217\u4e2d\u5206\u5272\u7f3a\u8840\u6027\u5352\u4e2d\u75c5\u7076\uff0c\u6210\u529f\u514b\u670d\u4e86\u673a\u6784\u7279\u5f02\u6027\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u5c40\u9650\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8de8\u4e2d\u5fc3\u3001\u5177\u6709\u5f3a\u5927\u6cdb\u5316\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18289", "pdf": "https://arxiv.org/pdf/2508.18289", "abs": "https://arxiv.org/abs/2508.18289", "authors": ["Mateus A. Fernandes", "Michael M. Furlanetti", "Eduardo Gildin", "Marcio A. Sampaio"], "title": "Data-driven models for production forecasting and decision supporting in petroleum reservoirs", "categories": ["cs.LG", "I.2; J.2"], "comment": "Manuscript as submitted to Journal of Petroleum Exploration and\n  Production Technology", "summary": "Forecasting production reliably and anticipating changes in the behavior of\nrock-fluid systems are the main challenges in petroleum reservoir engineering.\nThis project proposes to deal with this problem through a data-driven approach\nand using machine learning methods. The objective is to develop a methodology\nto forecast production parameters based on simple data as produced and injected\nvolumes and, eventually, gauges located in wells, without depending on\ninformation from geological models, fluid properties or details of well\ncompletions and flow systems. Initially, we performed relevance analyses of the\nproduction and injection variables, as well as conditioning the data to suit\nthe problem. As reservoir conditions change over time, concept drift is a\npriority concern and require special attention to those observation windows and\nthe periodicity of retraining, which are also objects of study. For the\nproduction forecasts, we study supervised learning methods, such as those based\non regressions and Neural Networks, to define the most suitable for our\napplication in terms of performance and complexity. In a first step, we\nevaluate the methodology using synthetic data generated from the UNISIM III\ncompositional simulation model. Next, we applied it to cases of real plays in\nthe Brazilian pre-salt. The expected result is the design of a reliable\npredictor for reproducing reservoir dynamics, with rapid response, capability\nof dealing with practical difficulties such as restrictions in wells and\nprocessing units, and that can be used in actions to support reservoir\nmanagement, including the anticipation of deleterious behaviors, optimization\nof production and injection parameters and the analysis of the effects of\nprobabilistic events, aiming to maximize oil recovery.", "AI": {"tldr": "\u672c\u9879\u76ee\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u65e8\u5728\u57fa\u4e8e\u7b80\u5355\u7684\u751f\u4ea7/\u6ce8\u5165\u6570\u636e\u548c\u4e95\u53e3\u4eea\u8868\u6570\u636e\uff0c\u53ef\u9760\u5730\u9884\u6d4b\u6cb9\u85cf\u751f\u4ea7\u53c2\u6570\uff0c\u5e76\u5904\u7406\u6982\u5ff5\u6f02\u79fb\uff0c\u4ee5\u652f\u6301\u6cb9\u85cf\u7ba1\u7406\u548c\u6700\u5927\u5316\u91c7\u6cb9\u91cf\u3002", "motivation": "\u77f3\u6cb9\u50a8\u5c42\u5de5\u7a0b\u9762\u4e34\u751f\u4ea7\u9884\u6d4b\u53ef\u9760\u6027\u548c\u5ca9\u77f3-\u6d41\u4f53\u7cfb\u7edf\u884c\u4e3a\u53d8\u5316\u9884\u6d4b\u7684\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5730\u8d28\u6a21\u578b\u3001\u6d41\u4f53\u6027\u8d28\u6216\u8be6\u7ec6\u7684\u5b8c\u4e95\u4fe1\u606f\uff0c\u800c\u672c\u9879\u76ee\u5bfb\u6c42\u4e00\u79cd\u4e0d\u4f9d\u8d56\u8fd9\u4e9b\u590d\u6742\u4fe1\u606f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002\u5229\u7528\u751f\u4ea7\u548c\u6ce8\u5165\u91cf\u4ee5\u53ca\u4e95\u53e3\u4eea\u8868\u7b49\u7b80\u5355\u6570\u636e\u8fdb\u884c\u9884\u6d4b\u3002\u521d\u671f\u8fdb\u884c\u53d8\u91cf\u76f8\u5173\u6027\u5206\u6790\u548c\u6570\u636e\u9884\u5904\u7406\u3002\u91cd\u70b9\u5173\u6ce8\u6982\u5ff5\u6f02\u79fb\uff0c\u7814\u7a76\u89c2\u5bdf\u7a97\u53e3\u548c\u518d\u8bad\u7ec3\u5468\u671f\u3002\u91c7\u7528\u56de\u5f52\u548c\u795e\u7ecf\u7f51\u7edc\u7b49\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u3002\u9996\u5148\u4f7f\u7528UNISIM III\u5408\u6210\u6570\u636e\u8fdb\u884c\u65b9\u6cd5\u8bc4\u4f30\uff0c\u7136\u540e\u5e94\u7528\u4e8e\u5df4\u897f\u76d0\u4e0b\u771f\u5b9e\u6cb9\u85cf\u6848\u4f8b\u3002", "result": "\u9884\u671f\u7ed3\u679c\u662f\u8bbe\u8ba1\u4e00\u4e2a\u53ef\u9760\u7684\u9884\u6d4b\u5668\uff0c\u80fd\u591f\u91cd\u73b0\u6cb9\u85cf\u52a8\u6001\uff0c\u5177\u6709\u5feb\u901f\u54cd\u5e94\u80fd\u529b\uff0c\u5904\u7406\u8bf8\u5982\u6cb9\u4e95\u548c\u5904\u7406\u5355\u5143\u9650\u5236\u7b49\u5b9e\u9645\u56f0\u96be\uff0c\u5e76\u53ef\u7528\u4e8e\u652f\u6301\u6cb9\u85cf\u7ba1\u7406\u884c\u52a8\uff0c\u5305\u62ec\u9884\u6d4b\u6709\u5bb3\u884c\u4e3a\u3001\u4f18\u5316\u751f\u4ea7\u548c\u6ce8\u5165\u53c2\u6570\u4ee5\u53ca\u5206\u6790\u6982\u7387\u4e8b\u4ef6\u7684\u5f71\u54cd\uff0c\u65e8\u5728\u6700\u5927\u5316\u77f3\u6cb9\u91c7\u6536\u7387\u3002", "conclusion": "\u8be5\u9879\u76ee\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u53ef\u9760\u4e14\u54cd\u5e94\u8fc5\u901f\u7684\u6570\u636e\u9a71\u52a8\u578b\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u77f3\u6cb9\u751f\u4ea7\u53c2\u6570\u9884\u6d4b\uff0c\u4ece\u800c\u6709\u6548\u5904\u7406\u50a8\u5c42\u52a8\u6001\u548c\u5b9e\u9645\u9650\u5236\uff0c\u6700\u7ec8\u652f\u6301\u6cb9\u85cf\u7ba1\u7406\u5e76\u6700\u5927\u5316\u77f3\u6cb9\u91c7\u6536\u7387\u3002"}}
{"id": "2508.18395", "pdf": "https://arxiv.org/pdf/2508.18395", "abs": "https://arxiv.org/abs/2508.18395", "authors": ["Jeong-seok Oh", "Jay-yoon Lee"], "title": "Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Probabilistic decoding in Large Language Models (LLMs) often yields\ninconsistent outputs, particularly on complex or long-form questions.\nSelf-Consistency (SC) mitigates this for short-form QA by majority voting over\nexact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram\nConsistency Score (WUCS) extend to long-form responses but lose accuracy on\nshort-form benchmarks.\n  We introduce Latent Self-Consistency (LSC), which selects the most\nsemantically consistent response using learnable token embeddings. A\nlightweight forward generation of summary tokens increases inference time by\nless than 1% and requires no changes to the model architecture.\n  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,\nTruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form\nones on average, while maintaining negligible computational overhead. These\nresults position LSC as a practical consistency-selection method that works\nreliably across answer formats. Additionally, LSC provides well-calibrated\nconfidence estimates, maintaining low Expected Calibration Error across both\nanswer formats.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLatent Self-Consistency (LSC)\u65b9\u6cd5\uff0c\u5229\u7528\u53ef\u5b66\u4e60\u7684token\u5d4c\u5165\u9009\u62e9\u8bed\u4e49\u6700\u4e00\u81f4\u7684LLM\u54cd\u5e94\u3002LSC\u5728\u591a\u79cd\u77ed\u6587\u672c\u548c\u957f\u6587\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\uff0c\u63d0\u4f9b\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6982\u7387\u89e3\u7801\u5e38\u5bfc\u81f4\u8f93\u51fa\u4e0d\u4e00\u81f4\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6216\u957f\u6587\u672c\u95ee\u9898\u4e0a\u3002\u73b0\u6709\u81ea\u6d3d\u6027\u65b9\u6cd5\uff08\u5982SC\uff09\u5bf9\u77ed\u6587\u672c\u6709\u6548\u4f46\u957f\u6587\u672c\u5931\u6548\uff0c\u800c\u5176\u4ed6\u65b9\u6cd5\uff08USC\u3001WUCS\uff09\u867d\u80fd\u5904\u7406\u957f\u6587\u672c\u4f46\u727a\u7272\u4e86\u77ed\u6587\u672c\u7684\u51c6\u786e\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u5728\u5404\u79cd\u7b54\u6848\u683c\u5f0f\u4e0b\u90fd\u80fd\u53ef\u9760\u5de5\u4f5c\u7684\u901a\u7528\u4e00\u81f4\u6027\u9009\u62e9\u65b9\u6cd5\u3002", "method": "\u5f15\u5165Latent Self-Consistency (LSC)\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u53ef\u5b66\u4e60\u7684token\u5d4c\u5165\u6765\u9009\u62e9\u8bed\u4e49\u4e0a\u6700\u4e00\u81f4\u7684\u54cd\u5e94\u3002\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u6458\u8981token\u524d\u5411\u751f\u6210\u5b9e\u73b0\uff0c\u5c06\u63a8\u7406\u65f6\u95f4\u589e\u52a0\u4e0d\u52301%\uff0c\u4e14\u65e0\u9700\u66f4\u6539\u6a21\u578b\u67b6\u6784\u3002", "result": "\u57286\u4e2a\u77ed\u6587\u672c\u548c5\u4e2a\u957f\u6587\u672c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MATH\u3001MMLU\u3001TruthfulQA\uff09\u4e2d\uff0cLSC\u5728\u6240\u6709\u77ed\u6587\u672c\u548c\u957f\u6587\u672c\u6d4b\u8bd5\u4e2d\u5e73\u5747\u8d85\u8d8a\u4e86SC\u3001USC\u548cWUCS\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u5ffd\u7565\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u6b64\u5916\uff0cLSC\u63d0\u4f9b\u4e86\u6821\u51c6\u826f\u597d\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u5728\u4e24\u79cd\u7b54\u6848\u683c\u5f0f\u4e0b\u5747\u4fdd\u6301\u8f83\u4f4e\u7684\u9884\u671f\u6821\u51c6\u8bef\u5dee\uff08Expected Calibration Error\uff09\u3002", "conclusion": "LSC\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u9760\u7684\u81ea\u6d3d\u6027\u9009\u62e9\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u5404\u79cd\u7b54\u6848\u683c\u5f0f\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2508.18855", "pdf": "https://arxiv.org/pdf/2508.18855", "abs": "https://arxiv.org/abs/2508.18855", "authors": ["Lisa Maile", "Kai-Steffen Hielscher", "Reinhard German"], "title": "Network Calculus Results for TSN: An Introduction", "categories": ["cs.NI"], "comment": null, "summary": "Time-Sensitive Networking (TSN) is a set of standards that enables the\nindustry to provide real-time guarantees for time-critical communications with\nEthernet hardware. TSN supports various queuing and scheduling mechanisms and\nallows the integration of multiple traffic types in a single network. Network\nCalculus (NC) can be used to calculate upper bounds for latencies and buffer\nsizes within these networks, for example, for safety or real-time traffic. We\nexplain the relevance of NC for TSN-based computer communications and potential\nareas of application. Different NC analysis approaches have been published to\nexamine different parts of TSN and this paper provides a survey of these\npublications and presents their main results, dependencies, and differences. We\npresent a consistent presentation of the most important results and suggest an\nimprovement to model the output of sending end-devices. To ease access to the\ncurrent research status, we introduce a common notation to show how all results\ndepend on each other and also identify common assumptions. Thus, we offer a\ncomprehensive overview of NC for industrial networks and identify possible\nareas for future work.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8c03\u67e5\u4e86\u73b0\u6709\u9488\u5bf9\u65f6\u95f4\u654f\u611f\u7f51\u7edc\uff08TSN\uff09\u7684\u6392\u961f\u548c\u8c03\u5ea6\u673a\u5236\u4f7f\u7528\u7f51\u7edc\u6f14\u7b97\uff08NC\uff09\u5206\u6790\u5176\u5ef6\u8fdf\u548c\u7f13\u51b2\u533a\u5927\u5c0f\u7684\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u81f4\u7684\u8868\u793a\u65b9\u6cd5\u548c\u5efa\u6a21\u6539\u8fdb\u3002", "motivation": "TSN\u80fd\u4e3a\u65f6\u95f4\u5173\u952e\u901a\u4fe1\u63d0\u4f9b\u5b9e\u65f6\u4fdd\u969c\uff0cNC\u53ef\u7528\u4e8e\u8ba1\u7b97TSN\u7f51\u7edc\u7684\u5ef6\u8fdf\u548c\u7f13\u51b2\u533a\u4e0a\u754c\uff0c\u4f46\u73b0\u6709NC\u5206\u6790\u65b9\u6cd5\u591a\u6837\u4e14\u5206\u6563\uff0c\u9700\u8981\u4e00\u4e2a\u5168\u9762\u7684\u6982\u89c8\u3001\u4e00\u81f4\u7684\u8868\u793a\u53ca\u6f5c\u5728\u7684\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u5bf9\u5df2\u53d1\u8868\u7684NC\u5206\u6790\u65b9\u6cd5\u8fdb\u884c\u8c03\u7814\u548c\u7efc\u8ff0\uff0c\u5206\u6790\u5176\u4e3b\u8981\u7ed3\u679c\u3001\u4f9d\u8d56\u6027\u548c\u5dee\u5f02\uff0c\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684\u7b26\u53f7\u8868\u793a\uff0c\u5e76\u5efa\u8bae\u6539\u8fdb\u53d1\u9001\u7ec8\u7aef\u8bbe\u5907\u7684\u8f93\u51fa\u6a21\u578b\u3002", "result": "\u63d0\u4f9b\u4e86NC\u5728TSN\u4e2d\u5e94\u7528\u7684\u76f8\u5173\u6027\u53ca\u6f5c\u5728\u5e94\u7528\u9886\u57df\u7684\u89e3\u91ca\uff0c\u7efc\u8ff0\u4e86\u4e0d\u540cNC\u5206\u6790\u65b9\u6cd5\u7684\u6210\u679c\uff0c\u5448\u73b0\u4e86\u91cd\u8981\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u4e86\u53d1\u9001\u7ec8\u7aef\u8bbe\u5907\u8f93\u51fa\u6a21\u578b\u7684\u6539\u8fdb\u5efa\u8bae\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u5de5\u4e1a\u7f51\u7edc\u4e2d\u7684NC\u5e94\u7528\u63d0\u4f9b\u4e86\u5168\u9762\u6982\u8ff0\uff0c\u901a\u8fc7\u5f15\u5165\u901a\u7528\u7b26\u53f7\u7b80\u5316\u4e86\u5bf9\u5f53\u524d\u7814\u7a76\u72b6\u6001\u7684\u7406\u89e3\uff0c\u8bc6\u522b\u4e86\u5171\u540c\u5047\u8bbe\u5e76\u6307\u660e\u4e86\u672a\u6765\u5de5\u4f5c\u7684\u53ef\u80fd\u65b9\u5411\u3002"}}
{"id": "2508.18467", "pdf": "https://arxiv.org/pdf/2508.18467", "abs": "https://arxiv.org/abs/2508.18467", "authors": ["Olivia Long", "Carter Teplica"], "title": "The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game", "categories": ["cs.AI"], "comment": null, "summary": "As AI agents become increasingly capable of tool use and long-horizon tasks,\nthey have begun to be deployed in settings where multiple agents can interact.\nHowever, whereas prior work has mostly focused on human-AI interactions, there\nis an increasing need to understand AI-AI interactions. In this paper, we adapt\nthe iterated public goods game, a classic behavioral economics game, to analyze\nthe behavior of four reasoning and non-reasoning models across two conditions:\nmodels are either told they are playing against \"another AI agent\" or told\ntheir opponents are themselves. We find that, across different settings,\ntelling LLMs that they are playing against themselves significantly changes\ntheir tendency to cooperate. While our study is conducted in a toy environment,\nour results may provide insights into multi-agent settings where agents\n\"unconsciously\" discriminating against each other could inexplicably increase\nor decrease cooperation.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u8fed\u4ee3\u516c\u5171\u7269\u54c1\u535a\u5f08\u5206\u6790\u4e86AI-AI\u4ea4\u4e92\uff0c\u53d1\u73b0\u544a\u77e5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5176\u5bf9\u624b\u662f\u201c\u81ea\u5df1\u201d\u800c\u975e\u201c\u53e6\u4e00\u4e2aAI\u4ee3\u7406\u201d\u4f1a\u663e\u8457\u6539\u53d8\u5176\u5408\u4f5c\u503e\u5411\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5728\u591a\u4ee3\u7406\u73af\u5883\u4e2d\u90e8\u7f72\uff0c\u7406\u89e3AI-AI\u4ea4\u4e92\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u5c24\u5176\u5173\u6ce8\u4ee3\u7406\u4e4b\u95f4\u201c\u65e0\u610f\u8bc6\u201d\u7684\u6b67\u89c6\u5982\u4f55\u5f71\u54cd\u5408\u4f5c\u3002", "method": "\u7814\u7a76\u6539\u7f16\u4e86\u7ecf\u5178\u7684\u8fed\u4ee3\u516c\u5171\u7269\u54c1\u535a\u5f08\uff0c\u5206\u6790\u4e86\u56db\u79cd\u63a8\u7406\u548c\u975e\u63a8\u7406\u6a21\u578b\u3002\u5728\u4e24\u79cd\u6761\u4ef6\u4e0b\u8fdb\u884c\u6d4b\u8bd5\uff1a\u544a\u77e5\u6a21\u578b\u5176\u5bf9\u624b\u662f\u201c\u53e6\u4e00\u4e2aAI\u4ee3\u7406\u201d\u6216\u201c\u81ea\u5df1\u201d\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\uff0c\u544a\u77e5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5176\u5bf9\u624b\u662f\u201c\u81ea\u5df1\u201d\u4f1a\u663e\u8457\u6539\u53d8\u5176\u5408\u4f5c\u503e\u5411\u3002", "conclusion": "\u5c3d\u7ba1\u7814\u7a76\u5728\u73a9\u5177\u73af\u5883\u4e2d\u8fdb\u884c\uff0c\u4f46\u7ed3\u679c\u4e3a\u591a\u4ee3\u7406\u73af\u5883\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u8868\u660e\u4ee3\u7406\u201c\u65e0\u610f\u8bc6\u201d\u7684\u6b67\u89c6\u53ef\u80fd\u4f1a\u4e0d\u53ef\u9884\u6d4b\u5730\u589e\u52a0\u6216\u51cf\u5c11\u5408\u4f5c\u3002"}}
{"id": "2508.18297", "pdf": "https://arxiv.org/pdf/2508.18297", "abs": "https://arxiv.org/abs/2508.18297", "authors": ["Dhananjay Ashok", "Ashutosh Chaubey", "Hirona J. Arai", "Jonathan May", "Jesse Thomason"], "title": "Can VLMs Recall Factual Associations From Visual References?", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "To appear at EMNLP 2025 (Findings)", "summary": "Through a controlled study, we identify a systematic deficiency in the\nmultimodal grounding of Vision Language Models (VLMs). While VLMs can recall\nfactual associations when provided a textual reference to an entity; their\nability to do so is significantly diminished when the reference is visual\ninstead. Forcing VLMs to rely on image representations of an entity halves\ntheir ability to recall factual knowledge, suggesting that VLMs struggle to\nlink their internal knowledge of an entity with its image representation. We\nshow that such linking failures are correlated with the expression of distinct\npatterns in model internal states, and that probes on these internal states\nachieve over 92% accuracy at flagging cases where the VLM response is\nunreliable. These probes can be applied, without retraining, to identify when a\nVLM will fail to correctly answer a question that requires an understanding of\nmultimodal input. When used to facilitate selective prediction on a visual\nquestion answering task, the probes increase coverage by 7.87% (absolute) while\nalso reducing the risk of error by 0.9% (absolute). Addressing the systematic,\ndetectable deficiency is an important avenue in language grounding, and we\nprovide informed recommendations for future directions.", "AI": {"tldr": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u901a\u8fc7\u89c6\u89c9\u5f15\u7528\u5b9e\u4f53\u65f6\uff0c\u56de\u5fc6\u4e8b\u5b9e\u77e5\u8bc6\u7684\u80fd\u529b\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u96be\u4ee5\u5c06\u5185\u90e8\u77e5\u8bc6\u4e0e\u56fe\u50cf\u5173\u8054\u3002\u7814\u7a76\u5f00\u53d1\u4e86\u5185\u90e8\u72b6\u6001\u63a2\u9488\uff0c\u53ef\u9ad8\u7cbe\u5ea6\uff08>92%\uff09\u68c0\u6d4b\u6b64\u7f3a\u9677\uff0c\u5e76\u5728\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u8bc6\u522b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u591a\u6a21\u6001\u63a5\u5730\uff08multimodal grounding\uff09\u65b9\u9762\u7684\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u7279\u522b\u662f\u5f53\u5b9e\u4f53\u5f15\u7528\u662f\u89c6\u89c9\u800c\u975e\u6587\u672c\u65f6\uff0cVLMs\u56de\u5fc6\u4e8b\u5b9e\u77e5\u8bc6\u80fd\u529b\u663e\u8457\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4e00\u9879\u5bf9\u7167\u7814\u7a76\uff0c\u8bc4\u4f30VLMs\u5728\u6587\u672c\u548c\u89c6\u89c9\u5f15\u7528\u4e0b\u56de\u5fc6\u4e8b\u5b9e\u77e5\u8bc6\u7684\u80fd\u529b\u3002\u5206\u6790\u6a21\u578b\u5185\u90e8\u72b6\u6001\uff0c\u53d1\u73b0\u8fde\u63a5\u5931\u8d25\u4e0e\u7279\u5b9a\u6a21\u5f0f\u76f8\u5173\u3002\u5f00\u53d1\u5e76\u5e94\u7528\u5185\u90e8\u72b6\u6001\u63a2\u9488\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u8bc6\u522bVLM\u54cd\u5e94\u4e0d\u53ef\u9760\u7684\u60c5\u51b5\u3002\u5c06\u8fd9\u4e9b\u63a2\u9488\u5e94\u7528\u4e8e\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u9009\u62e9\u6027\u9884\u6d4b\u3002", "result": "\u5f53VLM\u4f9d\u8d56\u5b9e\u4f53\u7684\u56fe\u50cf\u8868\u793a\u65f6\uff0c\u5176\u56de\u5fc6\u4e8b\u5b9e\u77e5\u8bc6\u7684\u80fd\u529b\u51cf\u534a\uff0c\u8868\u660e\u6a21\u578b\u96be\u4ee5\u5c06\u5185\u90e8\u77e5\u8bc6\u4e0e\u56fe\u50cf\u8868\u793a\u5173\u8054\u3002\u5f00\u53d1\u7684\u5185\u90e8\u72b6\u6001\u63a2\u9488\u5728\u6807\u8bb0VLM\u54cd\u5e94\u4e0d\u53ef\u9760\u7684\u60c5\u51b5\u4e0a\u8fbe\u5230\u8d85\u8fc792%\u7684\u51c6\u786e\u7387\u3002\u5c06\u63a2\u9488\u5e94\u7528\u4e8e\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u7684\u9009\u62e9\u6027\u9884\u6d4b\u65f6\uff0c\u8986\u76d6\u7387\u589e\u52a0\u4e867.87%\uff08\u7edd\u5bf9\u503c\uff09\uff0c\u540c\u65f6\u9519\u8bef\u98ce\u9669\u964d\u4f4e\u4e860.9%\uff08\u7edd\u5bf9\u503c\uff09\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a5\u5730\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u4e14\u53ef\u68c0\u6d4b\u7684\u7f3a\u9677\uff0c\u5373\u96be\u4ee5\u6709\u6548\u5730\u5c06\u5185\u90e8\u77e5\u8bc6\u4e0e\u56fe\u50cf\u8868\u793a\u5173\u8054\u3002\u89e3\u51b3\u8fd9\u79cd\u7f3a\u9677\u662f\u8bed\u8a00\u63a5\u5730\u9886\u57df\u7684\u91cd\u8981\u7814\u7a76\u65b9\u5411\uff0c\u672c\u7814\u7a76\u4e3a\u672a\u6765\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u76ca\u5efa\u8bae\u3002"}}
{"id": "2508.18301", "pdf": "https://arxiv.org/pdf/2508.18301", "abs": "https://arxiv.org/abs/2508.18301", "authors": ["Md Sabbir Ahmed", "Nova Ahmed"], "title": "A Fast and Minimal System to Identify Depression Using Smartphones: Explainable Machine Learning-Based Approach", "categories": ["cs.LG", "cs.CY", "cs.HC"], "comment": null, "summary": "Background: Existing robust, pervasive device-based systems developed in\nrecent years to detect depression require data collected over a long period and\nmay not be effective in cases where early detection is crucial.\n  Objective: Our main objective was to develop a minimalistic system to\nidentify depression using data retrieved in the fastest possible time.\n  Methods: We developed a fast tool that retrieves the past 7 days' app usage\ndata in 1 second (mean 0.31, SD 1.10 seconds). A total of 100 students from\nBangladesh participated in our study, and our tool collected their app usage\ndata. To identify depressed and nondepressed students, we developed a diverse\nset of ML models. We selected important features using the stable approach,\nalong with 3 main types of feature selection (FS) approaches.\n  Results: Leveraging only the app usage data retrieved in 1 second, our light\ngradient boosting machine model used the important features selected by the\nstable FS approach and correctly identified 82.4% (n=42) of depressed students\n(precision=75%, F1-score=78.5%). Moreover, after comprehensive exploration, we\npresented a parsimonious stacking model where around 5 features selected by the\nall-relevant FS approach Boruta were used in each iteration of validation and\nshowed a maximum precision of 77.4% (balanced accuracy=77.9%). A SHAP analysis\nof our best models presented behavioral markers that were related to\ndepression.\n  Conclusions: Due to our system's fast and minimalistic nature, it may make a\nworthwhile contribution to identifying depression in underdeveloped and\ndeveloping regions. In addition, our detailed discussion about the implication\nof our findings can facilitate the development of less resource-intensive\nsystems to better understand students who are depressed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u6781\u7b80\u7684\u6291\u90c1\u75c7\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u77ed\u77ed7\u5929\u7684\u5e94\u7528\u4f7f\u7528\u6570\u636e\uff0c\u80fd\u57281\u79d2\u5185\u8bc6\u522b\u51fa\u6291\u90c1\u5b66\u751f\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u7684\u5730\u533a\u3002", "motivation": "\u73b0\u6709\u6291\u90c1\u75c7\u68c0\u6d4b\u7cfb\u7edf\u9700\u8981\u957f\u65f6\u95f4\u7684\u6570\u636e\u6536\u96c6\uff0c\u4e0d\u9002\u7528\u4e8e\u65e9\u671f\u9884\u8b66\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u80fd\u5728\u6700\u77ed\u65f6\u95f4\u5185\u5229\u7528\u6570\u636e\u8bc6\u522b\u6291\u90c1\u75c7\u7684\u6781\u7b80\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5de5\u5177\uff0c\u53ef\u57281\u79d2\u5185\uff08\u5e73\u57470.31\u79d2\uff09\u83b7\u53d6\u8fc7\u53bb7\u5929\u7684\u5e94\u7528\u4f7f\u7528\u6570\u636e\u3002\u6536\u96c6\u4e86100\u540d\u5b5f\u52a0\u62c9\u56fd\u5b66\u751f\u7684\u5e94\u7528\u4f7f\u7528\u6570\u636e\u3002\u5f00\u53d1\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u7a33\u5b9a\u65b9\u6cd5\u53ca\u4e09\u79cd\u4e3b\u8981\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\u6765\u9009\u62e9\u91cd\u8981\u7279\u5f81\u3002", "result": "\u4ec5\u5229\u75281\u79d2\u5185\u83b7\u53d6\u7684\u5e94\u7528\u4f7f\u7528\u6570\u636e\uff0c\u8f7b\u91cf\u68af\u5ea6\u63d0\u5347\u673a\u6a21\u578b\uff08\u4f7f\u7528\u7a33\u5b9a\u7279\u5f81\u9009\u62e9\uff09\u6b63\u786e\u8bc6\u522b\u4e8682.4%\u7684\u6291\u90c1\u5b66\u751f\uff08\u7cbe\u786e\u5ea675%\uff0cF1\u5206\u657078.5%\uff09\u3002\u4e00\u4e2a\u7b80\u7ea6\u7684\u5806\u53e0\u6a21\u578b\uff08\u6bcf\u6b21\u8fed\u4ee3\u4f7f\u7528\u7ea65\u4e2aBoruta\u9009\u62e9\u7684\u7279\u5f81\uff09\u5b9e\u73b0\u4e86\u6700\u9ad877.4%\u7684\u7cbe\u786e\u5ea6\uff08\u5e73\u8861\u51c6\u786e\u5ea677.9%\uff09\u3002\u6700\u4f73\u6a21\u578b\u7684SHAP\u5206\u6790\u63ed\u793a\u4e86\u4e0e\u6291\u90c1\u75c7\u76f8\u5173\u7684\u884c\u4e3a\u6807\u5fd7\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u56e0\u5176\u5feb\u901f\u548c\u6781\u7b80\u7684\u7279\u6027\uff0c\u6709\u671b\u4e3a\u6b20\u53d1\u8fbe\u548c\u53d1\u5c55\u4e2d\u5730\u533a\u7684\u6291\u90c1\u75c7\u8bc6\u522b\u505a\u51fa\u8d21\u732e\u3002\u7814\u7a76\u7ed3\u679c\u7684\u8be6\u7ec6\u8ba8\u8bba\u4e5f\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u4f4e\u8d44\u6e90\u6d88\u8017\u7684\u7cfb\u7edf\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u6291\u90c1\u5b66\u751f\u3002"}}
{"id": "2508.18407", "pdf": "https://arxiv.org/pdf/2508.18407", "abs": "https://arxiv.org/abs/2508.18407", "authors": ["Michal \u0160tef\u00e1nik", "Timothee Mickus", "Marek Kadl\u010d\u00edk", "Michal Spiegel", "Josef Kucha\u0159"], "title": "Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering", "categories": ["cs.CL", "cs.AI", "68T01, 68T07, 68T50", "I.2"], "comment": "To appear in Findings of EMNLP 2025", "summary": "A majority of recent work in AI assesses models' generalization capabilities\nthrough the lens of performance on out-of-distribution (OOD) datasets. Despite\ntheir practicality, such evaluations build upon a strong assumption: that OOD\nevaluations can capture and reflect upon possible failures in a real-world\ndeployment.\n  In this work, we challenge this assumption and confront the results obtained\nfrom OOD evaluations with a set of specific failure modes documented in\nexisting question-answering (QA) models, referred to as a reliance on spurious\nfeatures or prediction shortcuts.\n  We find that different datasets used for OOD evaluations in QA provide an\nestimate of models' robustness to shortcuts that have a vastly different\nquality, some largely under-performing even a simple, in-distribution\nevaluation. We partially attribute this to the observation that spurious\nshortcuts are shared across ID+OOD datasets, but also find cases where a\ndataset's quality for training and evaluation is largely disconnected. Our work\nunderlines limitations of commonly-used OOD-based evaluations of\ngeneralization, and provides methodology and recommendations for evaluating\ngeneralization within and beyond QA more robustly.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86AI\u9886\u57df\u4e2dOOD\u8bc4\u4f30\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u6545\u969c\u7684\u5047\u8bbe\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4e0d\u540cOOD\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u62b5\u6297\u9884\u6d4b\u6377\u5f84\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u8d28\u91cf\u5dee\u5f02\u5927\uff0c\u751a\u81f3\u52a3\u4e8eID\u8bc4\u4f30\uff0c\u90e8\u5206\u539f\u56e0\u662fID\u548cOOD\u6570\u636e\u96c6\u5171\u4eab\u865a\u5047\u6377\u5f84\u3002\u8fd9\u63ed\u793a\u4e86\u5e38\u7528OOD\u8bc4\u4f30\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u66f4\u9c81\u68d2\u7684\u6cdb\u5316\u8bc4\u4f30\u65b9\u6cd5\u548c\u5efa\u8bae\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u591a\u901a\u8fc7OOD\u6570\u636e\u96c6\u8bc4\u4f30\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u8fd9\u79cd\u8bc4\u4f30\u57fa\u4e8e\u4e00\u4e2a\u5f3a\u5047\u8bbe\uff1a\u5373OOD\u8bc4\u4f30\u80fd\u6709\u6548\u6355\u6349\u548c\u53cd\u6620\u771f\u5b9e\u90e8\u7f72\u4e2d\u7684\u6f5c\u5728\u6545\u969c\u3002\u672c\u7814\u7a76\u65e8\u5728\u6311\u6218\u8fd9\u4e00\u5047\u8bbe\uff0c\u901a\u8fc7\u5bf9\u6bd4OOD\u8bc4\u4f30\u7ed3\u679c\u4e0e\u95ee\u7b54\uff08QA\uff09\u6a21\u578b\u4e2d\u5df2\u77e5\u7684\u7279\u5b9a\u6545\u969c\u6a21\u5f0f\uff08\u5982\u5bf9\u865a\u5047\u7279\u5f81\u6216\u9884\u6d4b\u6377\u5f84\u7684\u4f9d\u8d56\uff09\uff0c\u6765\u68c0\u9a8cOOD\u8bc4\u4f30\u7684\u6709\u6548\u6027\u3002", "method": "\u672c\u7814\u7a76\u5c06OOD\u8bc4\u4f30\u7ed3\u679c\u4e0eQA\u6a21\u578b\u4e2d\u5df2\u8bb0\u5f55\u7684\u6545\u969c\u6a21\u5f0f\uff08\u5373\u5bf9\u865a\u5047\u7279\u5f81\u6216\u9884\u6d4b\u6377\u5f84\u7684\u4f9d\u8d56\uff09\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002\u5177\u4f53\u65b9\u6cd5\u662f\u8003\u5bdf\u4e0d\u540cOOD\u6570\u636e\u96c6\u5728\u8bc4\u4f30QA\u6a21\u578b\u5bf9\u9884\u6d4b\u6377\u5f84\u9c81\u68d2\u6027\u65b9\u9762\u7684\u8d28\u91cf\u5dee\u5f02\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7528\u4e8eQA\u9886\u57dfOOD\u8bc4\u4f30\u7684\u4e0d\u540c\u6570\u636e\u96c6\uff0c\u5176\u5bf9\u6a21\u578b\u9c81\u68d2\u6027\uff08\u5373\u5bf9\u9884\u6d4b\u6377\u5f84\u7684\u62b5\u6297\u529b\uff09\u7684\u4f30\u8ba1\u8d28\u91cf\u5dee\u5f02\u5de8\u5927\uff0c\u6709\u4e9b\u751a\u81f3\u8fdc\u4e0d\u5982\u7b80\u5355\u7684\u540c\u5206\u5e03\uff08ID\uff09\u8bc4\u4f30\u3002\u90e8\u5206\u539f\u56e0\u5728\u4e8e\u865a\u5047\u6377\u5f84\u5728ID\u548cOOD\u6570\u636e\u96c6\u4e2d\u5747\u6709\u5b58\u5728\u3002\u6b64\u5916\uff0c\u8fd8\u53d1\u73b0\u8bad\u7ec3\u548c\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u8d28\u91cf\u53ef\u80fd\u5b58\u5728\u663e\u8457\u8131\u8282\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u5e38\u7528\u57fa\u4e8eOOD\u7684\u6cdb\u5316\u80fd\u529b\u8bc4\u4f30\u7684\u5c40\u9650\u6027\u3002\u5de5\u4f5c\u4e3a\u5728\u95ee\u7b54\u9886\u57df\u53ca\u5176\u4ed6\u9886\u57df\u66f4\u9c81\u68d2\u5730\u8bc4\u4f30\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u65b9\u6cd5\u8bba\u548c\u5efa\u8bae\u3002"}}
{"id": "2508.18863", "pdf": "https://arxiv.org/pdf/2508.18863", "abs": "https://arxiv.org/abs/2508.18863", "authors": ["Pietro Talli", "Anup Mishra", "Federico Chiariotti", "Israel Leyva-Mayorga", "Andrea Zanella", "Petar Popovski"], "title": "Saving Energy with Relaxed Latency Constraints: A Study on Data Compression and Communication", "categories": ["cs.NI"], "comment": null, "summary": "With the advent of edge computing, data generated by end devices can be\npre-processed before transmission, possibly saving transmission time and\nenergy. On the other hand, data processing itself incurs latency and energy\nconsumption, depending on the complexity of the computing operations and the\nspeed of the processor. The energy-latency-reliability profile resulting from\nthe concatenation of pre-processing operations (specifically, data compression)\nand data transmission is particularly relevant in wireless communication\nservices, whose requirements may change dramatically with the application\ndomain. In this paper, we study this multi-dimensional optimization problem,\nintroducing a simple model to investigate the tradeoff among end-to-end\nlatency, reliability, and energy consumption when considering compression and\ncommunication operations in a constrained wireless device. We then study the\nPareto fronts of the energy-latency trade-off, considering data compression\nratio and device processing speed as key design variables. Our results show\nthat the energy costs grows exponentially with the reduction of the end-to-end\nlatency, so that considerable energy saving can be obtained by slightly\nrelaxing the latency requirements of applications. These findings challenge\nconventional rigid communication latency targets, advocating instead for\napplication-specific end-to-end latency budgets that account for computational\nand transmission overhead.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e0b\uff0c\u65e0\u7ebf\u8bbe\u5907\u4e2d\u6570\u636e\u538b\u7f29\u4e0e\u4f20\u8f93\u5728\u80fd\u8017\u3001\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u4e4b\u95f4\u7684\u591a\u7ef4\u5ea6\u6743\u8861\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u5e94\u7528\u7279\u5b9a\u7684\u5ef6\u8fdf\u9884\u7b97\u3002", "motivation": "\u63a2\u7d22\u5728\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e0b\uff0c\u65e0\u7ebf\u8bbe\u5907\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\uff08\u7279\u522b\u662f\u6570\u636e\u538b\u7f29\uff09\u4e0e\u4f20\u8f93\u65f6\uff0c\u5176\u80fd\u8017\u3001\u5ef6\u8fdf\u548c\u53ef\u9760\u6027\u4e4b\u95f4\u7684\u590d\u6742\u6743\u8861\uff0c\u4ee5\u4f18\u5316\u591a\u7ef4\u5ea6\u6027\u80fd\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u7b80\u5355\u6a21\u578b\u6765\u5206\u6790\u7aef\u5230\u7aef\u5ef6\u8fdf\u3001\u53ef\u9760\u6027\u548c\u80fd\u8017\u4e4b\u95f4\u7684\u6743\u8861\uff1b\u901a\u8fc7\u5c06\u6570\u636e\u538b\u7f29\u6bd4\u548c\u8bbe\u5907\u5904\u7406\u901f\u5ea6\u4f5c\u4e3a\u5173\u952e\u8bbe\u8ba1\u53d8\u91cf\uff0c\u7814\u7a76\u4e86\u80fd\u8017-\u5ef6\u8fdf\u6743\u8861\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u80fd\u8017\u968f\u7aef\u5230\u7aef\u5ef6\u8fdf\u7684\u964d\u4f4e\u5448\u6307\u6570\u7ea7\u589e\u957f\uff1b\u9002\u5f53\u653e\u5bbd\u5e94\u7528\u5bf9\u5ef6\u8fdf\u7684\u8981\u6c42\u53ef\u4ee5\u663e\u8457\u8282\u7701\u80fd\u8017\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u6311\u6218\u4e86\u4f20\u7edf\u7684\u521a\u6027\u901a\u4fe1\u5ef6\u8fdf\u76ee\u6807\uff0c\u5e76\u5021\u5bfc\u6839\u636e\u5e94\u7528\u9700\u6c42\uff0c\u8003\u8651\u8ba1\u7b97\u548c\u4f20\u8f93\u5f00\u9500\u6765\u8bbe\u5b9a\u5e94\u7528\u7279\u5b9a\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u9884\u7b97\u3002"}}
{"id": "2508.18507", "pdf": "https://arxiv.org/pdf/2508.18507", "abs": "https://arxiv.org/abs/2508.18507", "authors": ["Dillon Z. Chen", "Johannes Zenn", "Tristan Cinquin", "Sheila A. McIlraith"], "title": "Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies", "categories": ["cs.AI"], "comment": "RLC 2025 Workshop on Programmatic Reinforcement Learning", "summary": "We study the usage of language models (LMs) for planning over world models\nspecified in the Planning Domain Definition Language (PDDL). We prompt LMs to\ngenerate Python programs that serve as generalised policies for solving PDDL\nproblems from a given domain. Notably, our approach synthesises policies that\nare provably sound relative to the PDDL domain without reliance on external\nverifiers. We conduct experiments on competition benchmarks which show that our\npolicies can solve more PDDL problems than PDDL planners and recent LM\napproaches within a fixed time and memory constraint. Our approach manifests in\nthe LMPlan planner which can solve planning problems with several hundreds of\nrelevant objects. Surprisingly, we observe that LMs used in our framework\nsometimes plan more effectively over PDDL problems written in meaningless\nsymbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1\no3). This finding challenges hypotheses that LMs reason over word semantics and\nmemorise solutions from its training corpus, and is worth further exploration.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u8bed\u8a00\u6a21\u578b\u4e3aPDDL\u89c4\u5212\u751f\u6210\u53ef\u8bc1\u660e\u5065\u5168\u7684Python\u7b56\u7565\uff0c\u5176\u6027\u80fd\u8d85\u8d8a\u4e86\u73b0\u6709\u89c4\u5212\u5668\u548cLM\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0LM\u5728\u5904\u7406\u7b26\u53f7\u5316\u8f93\u5165\u65f6\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5728PDDL\u89c4\u5212\u9886\u57df\u7684\u6f5c\u529b\uff0c\u65e8\u5728\u751f\u6210\u9ad8\u6548\u3001\u53ef\u8bc1\u660e\u5065\u5168\u7684\u89c4\u5212\u7b56\u7565\uff0c\u5e76\u8d85\u8d8a\u73b0\u6709\u89c4\u5212\u5668\u548c\u57fa\u4e8eLM\u7684\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u63d0\u793a\u8bed\u8a00\u6a21\u578b\u751f\u6210Python\u7a0b\u5e8f\uff0c\u8fd9\u4e9b\u7a0b\u5e8f\u4f5c\u4e3a\u901a\u7528\u7b56\u7565\u6765\u89e3\u51b3\u7279\u5b9aPDDL\u9886\u57df\u7684\u89c4\u5212\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u7b56\u7565\u76f8\u5bf9\u4e8ePDDL\u9886\u57df\u662f\u53ef\u8bc1\u660e\u5065\u5168\u7684\uff0c\u65e0\u9700\u5916\u90e8\u9a8c\u8bc1\u5668\u3002", "result": "1. \u8be5\u65b9\u6cd5\u751f\u6210\u7684\u7b56\u7565\uff08LMPlan\uff09\u5728\u56fa\u5b9a\u65f6\u95f4\u548c\u5185\u5b58\u9650\u5236\u4e0b\uff0c\u80fd\u591f\u89e3\u51b3\u6bd4\u4f20\u7edfPDDL\u89c4\u5212\u5668\u548c\u8fd1\u671fLM\u65b9\u6cd5\u66f4\u591a\u7684PDDL\u95ee\u9898\u30022. LMPlan\u80fd\u591f\u5904\u7406\u5305\u542b\u6570\u767e\u4e2a\u76f8\u5173\u5bf9\u8c61\u7684\u89c4\u5212\u95ee\u9898\u30023. \u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u5728\u6846\u67b6\u4e2d\u7684\u8bed\u8a00\u6a21\u578b\u6709\u65f6\u5728\u5904\u7406\u4f7f\u7528\u65e0\u610f\u4e49\u7b26\u53f7\u800c\u975e\u81ea\u7136\u8bed\u8a00\u7f16\u5199\u7684PDDL\u95ee\u9898\u65f6\uff0c\u89c4\u5212\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u53ef\u8bc1\u660e\u5065\u5168\u7684PDDL\u89c4\u5212\u7b56\u7565\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002LM\u5728\u5904\u7406\u7b26\u53f7\u5316\u8f93\u5165\u65f6\u8868\u73b0\u51fa\u7684\u66f4\u5f3a\u89c4\u5212\u80fd\u529b\uff0c\u6311\u6218\u4e86\u5173\u4e8eLM\u4f9d\u8d56\u8bcd\u4e49\u6216\u8bb0\u5fc6\u8bad\u7ec3\u8bed\u6599\u7684\u5047\u8bbe\uff0c\u63d0\u793a\u5176\u53ef\u80fd\u5177\u5907\u66f4\u62bd\u8c61\u7684\u63a8\u7406\u80fd\u529b\uff0c\u503c\u5f97\u6df1\u5165\u63a2\u7d22\u3002"}}
{"id": "2508.18314", "pdf": "https://arxiv.org/pdf/2508.18314", "abs": "https://arxiv.org/abs/2508.18314", "authors": ["Bo Xu", "Yuhu Guo", "Yuchao Wang", "Wenting Wang", "Yeung Yam", "Charlie C. L. Wang", "Xinyi Le"], "title": "SERES: Semantic-aware neural reconstruction from sparse views", "categories": ["cs.CV"], "comment": null, "summary": "We propose a semantic-aware neural reconstruction method to generate 3D\nhigh-fidelity models from sparse images. To tackle the challenge of severe\nradiance ambiguity caused by mismatched features in sparse input, we enrich\nneural implicit representations by adding patch-based semantic logits that are\noptimized together with the signed distance field and the radiance field. A\nnovel regularization based on the geometric primitive masks is introduced to\nmitigate shape ambiguity. The performance of our approach has been verified in\nexperimental evaluation. The average chamfer distances of our reconstruction on\nthe DTU dataset can be reduced by 44% for SparseNeuS and 20% for VolRecon. When\nworking as a plugin for those dense reconstruction baselines such as NeuS and\nNeuralangelo, the average error on the DTU dataset can be reduced by 69% and\n68% respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u7684\u795e\u7ecf\u91cd\u5efa\u65b9\u6cd5\uff0c\u80fd\u4ece\u7a00\u758f\u56fe\u50cf\u751f\u6210\u9ad8\u4fdd\u771f3D\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u8865\u4e01\u8bed\u4e49Logits\u548c\u51e0\u4f55\u539f\u59cb\u63a9\u7801\u6b63\u5219\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8f90\u5c04\u548c\u5f62\u72b6\u6a21\u7cca\u95ee\u9898\uff0c\u5e76\u5728DTU\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u7a00\u758f\u56fe\u50cf\u8f93\u5165\u4e2d\u7279\u5f81\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u4e25\u91cd\u8f90\u5c04\u6a21\u7cca\u95ee\u9898\uff0c\u5e76\u7f13\u89e33D\u6a21\u578b\u91cd\u5efa\u4e2d\u7684\u5f62\u72b6\u6a21\u7cca\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u7684\u795e\u7ecf\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u4e0e\u6709\u7b26\u53f7\u8ddd\u79bb\u573a\u548c\u8f90\u5c04\u573a\u5171\u540c\u4f18\u5316\u7684\u57fa\u4e8e\u5757\u7684\u8bed\u4e49Logits\u6765\u4e30\u5bcc\u795e\u7ecf\u9690\u5f0f\u8868\u793a\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u539f\u59cb\u63a9\u7801\u7684\u65b0\u578b\u6b63\u5219\u5316\u6765\u7f13\u89e3\u5f62\u72b6\u6a21\u7cca\u3002", "result": "\u5728DTU\u6570\u636e\u96c6\u4e0a\uff0c\u672c\u65b9\u6cd5\u7684\u5e73\u5747\u5012\u89d2\u8ddd\u79bb\u6bd4SparseNeuS\u51cf\u5c11\u4e8644%\uff0c\u6bd4VolRecon\u51cf\u5c11\u4e8620%\u3002\u4f5c\u4e3aNeuS\u548cNeuralangelo\u7b49\u5bc6\u96c6\u91cd\u5efa\u57fa\u7ebf\u7684\u63d2\u4ef6\u65f6\uff0c\u5728DTU\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u5747\u8bef\u5dee\u5206\u522b\u51cf\u5c11\u4e8669%\u548c68%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bed\u4e49\u611f\u77e5\u795e\u7ecf\u91cd\u5efa\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u4ece\u7a00\u758f\u56fe\u50cf\u751f\u6210\u9ad8\u4fdd\u771f3D\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u7cbe\u5ea6\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f5c\u4e3a\u73b0\u6709\u5bc6\u96c6\u91cd\u5efa\u65b9\u6cd5\u7684\u6709\u6548\u63d2\u4ef6\u3002"}}
{"id": "2508.18303", "pdf": "https://arxiv.org/pdf/2508.18303", "abs": "https://arxiv.org/abs/2508.18303", "authors": ["Jueqi Wang", "Zachary Jacokes", "John Darrell Van Horn", "Michael C. Schatz", "Kevin A. Pelphrey", "Archana Venkataraman"], "title": "Learning Explainable Imaging-Genetics Associations Related to a Neurological Disorder", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "While imaging-genetics holds great promise for unraveling the complex\ninterplay between brain structure and genetic variation in neurological\ndisorders, traditional methods are limited to simplistic linear models or to\nblack-box techniques that lack interpretability. In this paper, we present\nNeuroPathX, an explainable deep learning framework that uses an early fusion\nstrategy powered by cross-attention mechanisms to capture meaningful\ninteractions between structural variations in the brain derived from MRI and\nestablished biological pathways derived from genetics data. To enhance\ninterpretability and robustness, we introduce two loss functions over the\nattention matrix - a sparsity loss that focuses on the most salient\ninteractions and a pathway similarity loss that enforces consistent\nrepresentations across the cohort. We validate NeuroPathX on both autism\nspectrum disorder and Alzheimer's disease. Our results demonstrate that\nNeuroPathX outperforms competing baseline approaches and reveals biologically\nplausible associations linked to the disorder. These findings underscore the\npotential of NeuroPathX to advance our understanding of complex brain\ndisorders. Code is available at https://github.com/jueqiw/NeuroPathX .", "AI": {"tldr": "NeuroPathX\u662f\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408MRI\u548c\u9057\u4f20\u6570\u636e\uff0c\u4ee5\u63ed\u793a\u8111\u7ed3\u6784\u53d8\u5f02\u4e0e\u751f\u7269\u901a\u8def\u95f4\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u3002\u5b83\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u4e0e\u795e\u7ecf\u75be\u75c5\u76f8\u5173\u7684\u751f\u7269\u5b66\u5173\u8054\u3002", "motivation": "\u6210\u50cf\u9057\u4f20\u5b66\u5728\u63ed\u793a\u795e\u7ecf\u75be\u75c5\u4e2d\u8111\u7ed3\u6784\u4e0e\u9057\u4f20\u53d8\u5f02\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e\u7b80\u5355\u7684\u7ebf\u6027\u6a21\u578b\u6216\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u9ed1\u76d2\u6280\u672f\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86NeuroPathX\uff0c\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002\u5b83\u91c7\u7528\u65e9\u671f\u878d\u5408\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349MRI\u884d\u751f\u7684\u8111\u7ed3\u6784\u53d8\u5f02\u4e0e\u9057\u4f20\u6570\u636e\u884d\u751f\u7684\u751f\u7269\u901a\u8def\u4e4b\u95f4\u7684\u6709\u610f\u4e49\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u4e3a\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5f15\u5165\u4e86\u7a00\u758f\u6027\u635f\u5931\uff08\u5173\u6ce8\u663e\u8457\u4ea4\u4e92\uff09\u548c\u901a\u8def\u76f8\u4f3c\u6027\u635f\u5931\uff08\u786e\u4fdd\u961f\u5217\u95f4\u4e00\u81f4\u8868\u793a\uff09\u3002\u8be5\u6846\u67b6\u5728\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\u548c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "NeuroPathX\u5728\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\u548c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u4e0a\u5747\u4f18\u4e8e\u7ade\u4e89\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u4e0e\u75be\u75c5\u76f8\u5173\u7684\u751f\u7269\u5b66\u4e0a\u5408\u7406\u7684\u5173\u8054\u3002", "conclusion": "NeuroPathX\u7684\u53d1\u73b0\u7a81\u663e\u4e86\u5176\u5728\u589e\u8fdb\u6211\u4eec\u5bf9\u590d\u6742\u8111\u90e8\u75be\u75c5\u7406\u89e3\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.18444", "pdf": "https://arxiv.org/pdf/2508.18444", "abs": "https://arxiv.org/abs/2508.18444", "authors": ["Nafis Tanveer Islam", "Zhiming Zhao"], "title": "How Reliable are LLMs for Reasoning on the Re-ranking task?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at FQAS Conference 2024. DOI will be provided in 3 weeks\n  after the conference has published the paper", "summary": "With the improving semantic understanding capability of Large Language Models\n(LLMs), they exhibit a greater awareness and alignment with human values, but\nthis comes at the cost of transparency. Although promising results are achieved\nvia experimental analysis, an in-depth understanding of the LLM's internal\nworkings is unavoidable to comprehend the reasoning behind the re-ranking,\nwhich provides end users with an explanation that enables them to make an\ninformed decision. Moreover, in newly developed systems with limited user\nengagement and insufficient ranking data, accurately re-ranking content remains\na significant challenge. While various training methods affect the training of\nLLMs and generate inference, our analysis has found that some training methods\nexhibit better explainability than others, implying that an accurate semantic\nunderstanding has not been learned through all training methods; instead,\nabstract knowledge has been gained to optimize evaluation, which raises\nquestions about the true reliability of LLMs. Therefore, in this work, we\nanalyze how different training methods affect the semantic understanding of the\nre-ranking task in LLMs and investigate whether these models can generate more\ninformed textual reasoning to overcome the challenges of transparency or LLMs\nand limited training data. To analyze the LLMs for re-ranking tasks, we utilize\na relatively small ranking dataset from the environment and the Earth science\ndomain to re-rank retrieved content. Furthermore, we also analyze the\nexplainable information to see if the re-ranking can be reasoned using\nexplainability.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u5982\u4f55\u5f71\u54cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u91cd\u6392\u5e8f\u4efb\u52a1\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u5e94\u5bf9\u900f\u660e\u5ea6\u4e0d\u8db3\u548c\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u6311\u6218\u3002", "motivation": "LLM\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u589e\u5f3a\u4f46\u900f\u660e\u5ea6\u4e0d\u8db3\uff0c\u96be\u4ee5\u89e3\u91ca\u5176\u91cd\u6392\u5e8f\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u65b0\u7cfb\u7edf\u9762\u4e34\u7528\u6237\u53c2\u4e0e\u548c\u6392\u540d\u6570\u636e\u6709\u9650\u5bfc\u81f4\u7684\u91cd\u6392\u5e8f\u96be\u9898\u3002\u73b0\u6709\u5206\u6790\u53d1\u73b0\u90e8\u5206\u8bad\u7ec3\u65b9\u6cd5\u672a\u80fd\u771f\u6b63\u4e60\u5f97\u8bed\u4e49\u7406\u89e3\uff0c\u800c\u662f\u901a\u8fc7\u62bd\u8c61\u77e5\u8bc6\u4f18\u5316\u8bc4\u4f30\uff0c\u5f15\u53d1\u4e86\u5bf9LLM\u53ef\u9760\u6027\u7684\u8d28\u7591\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u6df1\u5165\u63a2\u7a76\u8bad\u7ec3\u65b9\u6cd5\u5bf9LLM\u8bed\u4e49\u7406\u89e3\u548c\u89e3\u91ca\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u672c\u7814\u7a76\u65e8\u5728\u5206\u6790\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u5982\u4f55\u5f71\u54cdLLM\u5728\u91cd\u6392\u5e8f\u4efb\u52a1\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u5e76\u8c03\u67e5\u6a21\u578b\u662f\u5426\u80fd\u751f\u6210\u66f4\u5177\u4fe1\u606f\u91cf\u7684\u6587\u672c\u63a8\u7406\u4ee5\u514b\u670d\u900f\u660e\u5ea6\u548c\u6570\u636e\u9650\u5236\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u4e00\u4e2a\u6765\u81ea\u73af\u5883\u548c\u5730\u7403\u79d1\u5b66\u9886\u57df\u7684\u8f83\u5c0f\u6392\u540d\u6570\u636e\u96c6\u8fdb\u884c\u5185\u5bb9\u91cd\u6392\u5e8f\uff0c\u5e76\u5206\u6790\u5176\u53ef\u89e3\u91ca\u4fe1\u606f\u4ee5\u5224\u65ad\u91cd\u6392\u5e8f\u7684\u5408\u7406\u6027\u3002", "result": "\u6211\u4eec\u7684\u5206\u6790\u53d1\u73b0\uff0c\u4e00\u4e9b\u8bad\u7ec3\u65b9\u6cd5\u6bd4\u5176\u4ed6\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u8fd9\u6697\u793a\u5e76\u975e\u6240\u6709\u8bad\u7ec3\u65b9\u6cd5\u90fd\u80fd\u4f7fLLM\u4e60\u5f97\u51c6\u786e\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u800c\u662f\u53ef\u80fd\u901a\u8fc7\u83b7\u5f97\u62bd\u8c61\u77e5\u8bc6\u6765\u4f18\u5316\u8bc4\u4f30\uff0c\u4ece\u800c\u5f15\u53d1\u4e86\u5bf9LLM\u771f\u5b9e\u53ef\u9760\u6027\u7684\u7591\u95ee\u3002", "conclusion": "\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u7406\u89e3\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u5bf9LLM\u91cd\u6392\u5e8f\u4efb\u52a1\u4e2d\u8bed\u4e49\u7406\u89e3\u548c\u53ef\u89e3\u91ca\u6027\u7684\u5f71\u54cd\uff0c\u4ee5\u671f\u4e3a\u514b\u670dLLM\u7684\u900f\u660e\u5ea6\u6311\u6218\u548c\u6709\u9650\u8bad\u7ec3\u6570\u636e\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u66f4\u660e\u667a\u7684\u6587\u672c\u63a8\u7406\u548c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18883", "pdf": "https://arxiv.org/pdf/2508.18883", "abs": "https://arxiv.org/abs/2508.18883", "authors": ["Lisa Maile", "Kai-Steffen Hielscher", "Reinhard German"], "title": "Combining Static and Dynamic Traffic with Delay Guarantees in Time-Sensitive Networking", "categories": ["cs.NI"], "comment": "Code published as DYRECTsn (https://github.com/Kathess/DYRECTsn): an\n  open-source TSN framework for dynamic traffic with latency guarantees. It\n  applies Network Calculus to compute worst case delays and supports online\n  admission control, ensuring predictable real-time performance. Optimizes\n  delay budgets in the network", "summary": "To support reliable and low-latency communication, Time-Sensitive Networking\nintroduced protocols and interfaces for resource allocation in Ethernet.\nHowever, the implementation of these allocation algorithms has not yet been\ncovered by the standards. Our work focuses on deadline-guaranteeing resource\nallocation for networks with static and dynamic traffic. To achieve this, we\ncombine offline network optimization heuristics with online admission control\nand, thus, allow for new flow registrations while the network is running. We\ndemonstrate our solution on Credit-Based Shaper networks by using the delay\nanalysis framework Network Calculus. We compare our approach with an intuitive\nand a brute-force algorithm, where we can achieve significant improvements,\nboth, in terms of quality and runtime. Thereby, our results show that we can\nguarantee maximum end-to-end delays and also increase the flexibility of the\nnetwork while requiring only minimal user input.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u79bb\u7ebf\u4f18\u5316\u542f\u53d1\u5f0f\u7b97\u6cd5\u4e0e\u5728\u7ebf\u51c6\u5165\u63a7\u5236\u7684\u8d44\u6e90\u5206\u914d\u65b9\u6848\uff0c\u7528\u4e8e\u65f6\u95f4\u654f\u611f\u7f51\u7edc\uff08TSN\uff09\u4e2d\uff0c\u4ee5\u5728\u8fd0\u884c\u65f6\u4e3a\u9759\u6001\u548c\u52a8\u6001\u6d41\u91cf\u63d0\u4f9b\u5ef6\u8fdf\u4fdd\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u6027\u80fd\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u65f6\u95f4\u654f\u611f\u7f51\u7edc\uff08TSN\uff09\u6807\u51c6\u5f15\u5165\u4e86\u8d44\u6e90\u5206\u914d\u534f\u8bae\uff0c\u4f46\u5e76\u672a\u6db5\u76d6\u5206\u914d\u7b97\u6cd5\u7684\u5b9e\u73b0\u3002\u73b0\u6709\u7684\u9700\u6c42\u662f\u5982\u4f55\u4e3aTSN\u4e2d\u7684\u9759\u6001\u548c\u52a8\u6001\u6d41\u91cf\u63d0\u4f9b\u53ef\u9760\u4e14\u4f4e\u5ef6\u8fdf\u7684\u901a\u4fe1\uff0c\u5e76\u4fdd\u8bc1\u622a\u6b62\u65f6\u95f4\u3002", "method": "\u8be5\u7814\u7a76\u7ed3\u5408\u4e86\u79bb\u7ebf\u7f51\u7edc\u4f18\u5316\u542f\u53d1\u5f0f\u7b97\u6cd5\u4e0e\u5728\u7ebf\u51c6\u5165\u63a7\u5236\uff0c\u4ece\u800c\u5141\u8bb8\u5728\u7f51\u7edc\u8fd0\u884c\u65f6\u6ce8\u518c\u65b0\u7684\u6d41\u91cf\u3002\u4ed6\u4eec\u4f7f\u7528\u7f51\u7edc\u6f14\u7b97\uff08Network Calculus\uff09\u5ef6\u8fdf\u5206\u6790\u6846\u67b6\uff0c\u5728\u57fa\u4e8e\u4fe1\u7528\u6574\u5f62\u5668\uff08Credit-Based Shaper\uff09\u7684\u7f51\u7edc\u4e0a\u5c55\u793a\u4e86\u5176\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u4e0e\u76f4\u89c2\u7b97\u6cd5\u548c\u66b4\u529b\u7b97\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u8d28\u91cf\u548c\u8fd0\u884c\u65f6\u95f4\u65b9\u9762\u5747\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u4fdd\u8bc1\u6700\u5927\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u5e76\u5728\u53ea\u9700\u6700\u5c11\u7528\u6237\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u7f51\u7edc\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8d44\u6e90\u5206\u914d\u89e3\u51b3\u65b9\u6848\u80fd\u591f\u4e3aTSN\u4e2d\u7684\u9759\u6001\u548c\u52a8\u6001\u6d41\u91cf\u63d0\u4f9b\u6700\u5927\u7aef\u5230\u7aef\u5ef6\u8fdf\u4fdd\u8bc1\uff0c\u5e76\u5728\u63d0\u9ad8\u7f51\u7edc\u7075\u6d3b\u6027\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u548c\u6613\u7528\u6027\u3002"}}
{"id": "2508.18515", "pdf": "https://arxiv.org/pdf/2508.18515", "abs": "https://arxiv.org/abs/2508.18515", "authors": ["Dillon Z. Chen"], "title": "Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size Hyperparameter Study", "categories": ["cs.AI"], "comment": "Extended version of ECAI 2025 paper", "summary": "Weisfeiler-Leman Features (WLFs) are a recently introduced classical machine\nlearning tool for learning to plan and search. They have been shown to be both\ntheoretically and empirically superior to existing deep learning approaches for\nlearning value functions for search in symbolic planning. In this paper, we\nintroduce new WLF hyperparameters and study their various tradeoffs and\neffects. We utilise the efficiency of WLFs and run planning experiments on\nsingle core CPUs with a sample size of 1,000,000 to understand the effect of\nhyperparameters on training and planning. Our experimental analysis show that\nthere is a robust and best set of hyperparameters for WLFs across the tested\nplanning domains. We find that the best WLF hyperparameters for learning\nheuristic functions minimise execution time rather than maximise model\nexpressivity. We further statistically analyse and observe no significant\ncorrelation between training and planning metrics.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165\u5e76\u5206\u6790\u4e86Weisfeiler-Leman\u7279\u5f81\uff08WLFs\uff09\u7684\u65b0\u8d85\u53c2\u6570\uff0c\u53d1\u73b0\u5728\u4e0d\u540c\u89c4\u5212\u9886\u57df\u5b58\u5728\u4e00\u7ec4\u7a33\u5065\u7684\u6700\u4f73\u8d85\u53c2\u6570\uff0c\u5b83\u4eec\u4fa7\u91cd\u4e8e\u6700\u5c0f\u5316\u6267\u884c\u65f6\u95f4\u800c\u975e\u6700\u5927\u5316\u6a21\u578b\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u4e14\u8bad\u7ec3\u4e0e\u89c4\u5212\u6307\u6807\u4e4b\u95f4\u65e0\u663e\u8457\u76f8\u5173\u6027\u3002", "motivation": "Weisfeiler-Leman\u7279\u5f81\uff08WLFs\uff09\u5728\u7b26\u53f7\u89c4\u5212\u4e2d\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\u65b9\u9762\u5df2\u663e\u793a\u51fa\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u7406\u8bba\u548c\u7ecf\u9a8c\u4f18\u52bf\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f15\u5165\u65b0\u7684WLF\u8d85\u53c2\u6570\uff0c\u5e76\u6df1\u5165\u63a2\u8ba8\u5176\u6743\u8861\u548c\u5f71\u54cd\uff0c\u4ee5\u4f18\u5316\u5176\u5728\u89c4\u5212\u548c\u641c\u7d22\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u65b0\u7684WLF\u8d85\u53c2\u6570\uff0c\u5e76\u5229\u7528WLFs\u7684\u6548\u7387\uff0c\u5728\u5355\u6838CPU\u4e0a\u8fd0\u884c\u4e86\u5305\u542b1,000,000\u4e2a\u6837\u672c\u7684\u89c4\u5212\u5b9e\u9a8c\uff0c\u4ee5\u7406\u89e3\u8d85\u53c2\u6570\u5bf9\u8bad\u7ec3\u548c\u89c4\u5212\u7684\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u8fd8\u8fdb\u884c\u4e86\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u5206\u6790\u8868\u660e\uff0c\u5728\u6240\u6709\u6d4b\u8bd5\u7684\u89c4\u5212\u9886\u57df\u4e2d\uff0c\u5b58\u5728\u4e00\u7ec4\u7a33\u5065\u4e14\u6700\u4f73\u7684WLF\u8d85\u53c2\u6570\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u7528\u4e8e\u5b66\u4e60\u542f\u53d1\u5f0f\u51fd\u6570\u7684\u6700\u4f73WLF\u8d85\u53c2\u6570\u65e8\u5728\u6700\u5c0f\u5316\u6267\u884c\u65f6\u95f4\uff0c\u800c\u975e\u6700\u5927\u5316\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u3002\u7edf\u8ba1\u5206\u6790\u8fdb\u4e00\u6b65\u663e\u793a\uff0c\u8bad\u7ec3\u548c\u89c4\u5212\u6307\u6807\u4e4b\u95f4\u6ca1\u6709\u663e\u8457\u76f8\u5173\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86WLF\u8d85\u53c2\u6570\u7684\u6700\u4f73\u9009\u62e9\u7b56\u7565\uff0c\u5373\u5e94\u4f18\u5148\u8003\u8651\u6700\u5c0f\u5316\u6267\u884c\u65f6\u95f4\u4ee5\u63d0\u5347\u6548\u7387\u3002\u540c\u65f6\uff0c\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u8bad\u7ec3\u6307\u6807\u548c\u5b9e\u9645\u89c4\u5212\u6027\u80fd\u4e4b\u95f4\u7f3a\u4e4f\u76f4\u63a5\u7684\u663e\u8457\u5173\u8054\uff0c\u8fd9\u4e3a\u672a\u6765WLF\u7684\u5e94\u7528\u548c\u8c03\u4f18\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2508.18315", "pdf": "https://arxiv.org/pdf/2508.18315", "abs": "https://arxiv.org/abs/2508.18315", "authors": ["Nowshin Sharmily", "Rusab Sarmun", "Muhammad E. H. Chowdhury", "Mir Hamidul Hussain", "Saad Bin Abul Kashem", "Molla E Majid", "Amith Khandakar"], "title": "Automated Landfill Detection Using Deep Learning: A Comparative Study of Lightweight and Custom Architectures with the AerialWaste Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Illegal landfills are posing as a hazardous threat to people all over the\nworld. Due to the arduous nature of manually identifying the location of\nlandfill, many landfills go unnoticed by authorities and later cause dangerous\nharm to people and environment. Deep learning can play a significant role in\nidentifying these landfills while saving valuable time, manpower and resources.\nDespite being a burning concern, good quality publicly released datasets for\nillegal landfill detection are hard to find due to security concerns. However,\nAerialWaste Dataset is a large collection of 10434 images of Lombardy region of\nItaly. The images are of varying qualities, collected from three different\nsources: AGEA Orthophotos, WorldView-3, and Google Earth. The dataset contains\nprofessionally curated, diverse and high-quality images which makes it\nparticularly suitable for scalable and impactful research. As we trained\nseveral models to compare results, we found complex and heavy models to be\nprone to overfitting and memorizing training data instead of learning patterns.\nTherefore, we chose lightweight simpler models which could leverage general\nfeatures from the dataset. In this study, Mobilenetv2, Googlenet, Densenet,\nMobileVit and other lightweight deep learning models were used to train and\nvalidate the dataset as they achieved significant success with less\noverfitting. As we saw substantial improvement in the performance using some of\nthese models, we combined the best performing models and came up with an\nensemble model. With the help of ensemble and fusion technique, binary\nclassification could be performed on this dataset with 92.33% accuracy, 92.67%\nprecision, 92.33% sensitivity, 92.41% F1 score and 92.71% specificity.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u65b0\u53d1\u5e03\u7684AerialWaste\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u610f\u5927\u5229\u4f26\u5df4\u7b2c\u5730\u533a\u975e\u6cd5\u5783\u573e\u586b\u57cb\u573a\u7684\u6709\u6548\u4e8c\u5143\u5206\u7c7b\uff0c\u51c6\u786e\u7387\u8fbe92.33%\u3002", "motivation": "\u975e\u6cd5\u5783\u573e\u586b\u57cb\u573a\u5bf9\u4eba\u7c7b\u548c\u73af\u5883\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u4f46\u7531\u4e8e\u4eba\u5de5\u8bc6\u522b\u56f0\u96be\u4e14\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u8bb8\u591a\u586b\u57cb\u573a\u672a\u88ab\u53d1\u73b0\u3002\u6df1\u5ea6\u5b66\u4e60\u5728\u8bc6\u522b\u65b9\u9762\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u662f\u4e3b\u8981\u969c\u788d\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86AerialWaste\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b10434\u5f20\u6765\u81ea\u610f\u5927\u5229\u4f26\u5df4\u7b2c\u5730\u533a\u7684\u591a\u6e90\u822a\u7a7a\u5f71\u50cf\u3002\u4e3a\u907f\u514d\u8fc7\u62df\u5408\uff0c\u4f5c\u8005\u9009\u62e9\u4e86Mobilenetv2\u3001Googlenet\u3001Densenet\u3001MobileVit\u7b49\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u3002\u6700\u7ec8\uff0c\u5c06\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u8fdb\u884c\u7ec4\u5408\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u96c6\u6210\u6a21\u578b\u5e76\u91c7\u7528\u4e86\u878d\u5408\u6280\u672f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u590d\u6742\u4e14\u5e9e\u5927\u7684\u6a21\u578b\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u800c\u8f7b\u91cf\u7ea7\u6a21\u578b\u80fd\u66f4\u597d\u5730\u5b66\u4e60\u901a\u7528\u7279\u5f81\u3002\u901a\u8fc7\u96c6\u6210\u548c\u878d\u5408\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u5728\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8692.33%\u7684\u51c6\u786e\u7387\u300192.67%\u7684\u7cbe\u786e\u7387\u300192.33%\u7684\u7075\u654f\u5ea6\u300192.41%\u7684F1\u5206\u6570\u548c92.71%\u7684\u7279\u5f02\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\uff0c\u5229\u7528AerialWaste\u6570\u636e\u96c6\u548c\u8f7b\u91cf\u7ea7\u96c6\u6210\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u53ef\u4ee5\u9ad8\u6548\u4e14\u51c6\u786e\u5730\u8bc6\u522b\u975e\u6cd5\u5783\u573e\u586b\u57cb\u573a\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u73af\u5883\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2508.18306", "pdf": "https://arxiv.org/pdf/2508.18306", "abs": "https://arxiv.org/abs/2508.18306", "authors": ["Wuxinlin Cheng", "Yupeng Cao", "Jinwen Wu", "Koduvayur Subbalakshmi", "Tian Han", "Zhuo Feng"], "title": "SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent strides in pretrained transformer-based language models have propelled\nstate-of-the-art performance in numerous NLP tasks. Yet, as these models grow\nin size and deployment, their robustness under input perturbations becomes an\nincreasingly urgent question. Existing robustness methods often diverge between\nsmall-parameter and large-scale models (LLMs), and they typically rely on\nlabor-intensive, sample-specific adversarial designs. In this paper, we propose\na unified, local (sample-level) robustness framework (SALMAN) that evaluates\nmodel stability without modifying internal parameters or resorting to complex\nperturbation heuristics. Central to our approach is a novel Distance Mapping\nDistortion (DMD) measure, which ranks each sample's susceptibility by comparing\ninput-to-output distance mappings in a near-linear complexity manner. By\ndemonstrating significant gains in attack efficiency and robust training, we\nposition our framework as a practical, model-agnostic tool for advancing the\nreliability of transformer-based NLP systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86SALMAN\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u5c40\u90e8\u9c81\u68d2\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u8ddd\u79bb\u6620\u5c04\u5931\u771f\uff08DMD\uff09\u5ea6\u91cf\uff0c\u4ee5\u6a21\u578b\u65e0\u5173\u4e14\u9ad8\u6548\u7684\u65b9\u5f0f\u8bc4\u4f30Transformer\u8bed\u8a00\u6a21\u578b\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u653b\u51fb\u6548\u7387\u548c\u9c81\u68d2\u8bad\u7ec3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002", "motivation": "\u968f\u7740Transformer\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u7684\u589e\u957f\u548c\u5e7f\u6cdb\u90e8\u7f72\uff0c\u5176\u5728\u8f93\u5165\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u73b0\u6709\u9c81\u68d2\u6027\u65b9\u6cd5\u901a\u5e38\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u4e14\u4f9d\u8d56\u4e8e\u52b3\u52a8\u5bc6\u96c6\u578b\u3001\u6837\u672c\u7279\u5b9a\u7684\u5bf9\u6297\u8bbe\u8ba1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u5c40\u90e8\uff08\u6837\u672c\u7ea7\uff09\u9c81\u68d2\u6027\u6846\u67b6SALMAN\uff0c\u8be5\u6846\u67b6\u65e0\u9700\u4fee\u6539\u5185\u90e8\u53c2\u6570\u6216\u91c7\u7528\u590d\u6742\u7684\u6270\u52a8\u542f\u53d1\u5f0f\u65b9\u6cd5\u5373\u53ef\u8bc4\u4f30\u6a21\u578b\u7a33\u5b9a\u6027\u3002\u5176\u6838\u5fc3\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u8ddd\u79bb\u6620\u5c04\u5931\u771f\uff08DMD\uff09\u5ea6\u91cf\uff0c\u901a\u8fc7\u6bd4\u8f83\u8f93\u5165\u5230\u8f93\u51fa\u7684\u8ddd\u79bb\u6620\u5c04\uff0c\u4ee5\u63a5\u8fd1\u7ebf\u6027\u7684\u590d\u6742\u5ea6\u5bf9\u6bcf\u4e2a\u6837\u672c\u7684\u654f\u611f\u6027\u8fdb\u884c\u6392\u5e8f\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u653b\u51fb\u6548\u7387\u548c\u9c81\u68d2\u8bad\u7ec3\u65b9\u9762\u5747\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u88ab\u5b9a\u4f4d\u4e3a\u4e00\u4e2a\u5b9e\u7528\u7684\u3001\u6a21\u578b\u65e0\u5173\u7684\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u63d0\u5347\u57fa\u4e8eTransformer\u7684NLP\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.18466", "pdf": "https://arxiv.org/pdf/2508.18466", "abs": "https://arxiv.org/abs/2508.18466", "authors": ["Alina Wr\u00f3blewska", "Bartosz \u017buk"], "title": "Integrating gender inclusivity into large language models via instruction tuning", "categories": ["cs.CL"], "comment": null, "summary": "Imagine a language with masculine, feminine, and neuter grammatical genders,\nyet, due to historical and political conventions, masculine forms are\npredominantly used to refer to men, women and mixed-gender groups. This is the\nreality of contemporary Polish. A social consequence of this unfair linguistic\nsystem is that large language models (LLMs) trained on Polish texts inherit and\nreinforce this masculine bias, generating gender-imbalanced outputs. This study\naddresses this issue by tuning LLMs using the IPIS dataset, a collection of\nhuman-crafted gender-inclusive proofreading in Polish and Polish-to-English\ntranslation instructions. Grounded in a theoretical linguistic framework, we\ndesign a system prompt with explicit gender-inclusive guidelines for Polish. In\nour experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and\nMistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to\nintegrate gender inclusivity as an inherent feature of these models, offering a\nsystematic solution to mitigate gender bias in Polish language generation.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u6ce2\u5170\u8bed\u4e2d\u666e\u904d\u5b58\u5728\u7684\u7537\u6027\u5316\u504f\u89c1\uff0c\u901a\u8fc7\u4f7f\u7528IPIS\u6570\u636e\u96c6\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e76\u8bbe\u8ba1\u6027\u522b\u5305\u5bb9\u6027\u7cfb\u7edf\u63d0\u793a\uff0c\u65e8\u5728\u51cf\u8f7b\u6ce2\u5170\u8bed\u751f\u6210\u4e2d\u7684\u6027\u522b\u504f\u89c1\u3002", "motivation": "\u5f53\u4ee3\u6ce2\u5170\u8bed\u4e2d\u7537\u6027\u5316\u5f62\u5f0f\u88ab\u8fc7\u5ea6\u7528\u4e8e\u6307\u4ee3\u7537\u6027\u3001\u5973\u6027\u548c\u6df7\u5408\u6027\u522b\u7fa4\u4f53\uff0c\u5bfc\u81f4\u4e86\u4e0d\u516c\u5e73\u7684\u8bed\u8a00\u7cfb\u7edf\u3002\u5728\u6b64\u7cfb\u7edf\u4e0a\u8bad\u7ec3\u7684LLMs\u7ee7\u627f\u5e76\u5f3a\u5316\u4e86\u8fd9\u79cd\u7537\u6027\u504f\u89c1\uff0c\u751f\u6210\u6027\u522b\u4e0d\u5e73\u8861\u7684\u8f93\u51fa\u3002\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3LLMs\u5728\u6ce2\u5170\u8bed\u751f\u6210\u4e2d\u7684\u6027\u522b\u504f\u89c1\u95ee\u9898\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\uff1a1. \u4f7f\u7528IPIS\u6570\u636e\u96c6\u5fae\u8c03\u591a\u8bed\u8a00LLMs\uff08\u5982Llama-8B, Mistral-7B, Mistral-Nemo\uff09\u548c\u6ce2\u5170\u8bed\u7279\u5b9aLLMs\uff08\u5982Bielik, PLLuM\uff09\u30022. IPIS\u6570\u636e\u96c6\u5305\u542b\u4eba\u5de5\u5236\u4f5c\u7684\u6ce2\u5170\u8bed\u6027\u522b\u5305\u5bb9\u6027\u6821\u5bf9\u6587\u672c\u548c\u6ce2\u5170\u8bed\u5230\u82f1\u8bed\u7684\u7ffb\u8bd1\u6307\u4ee4\u30023. \u57fa\u4e8e\u7406\u8bba\u8bed\u8a00\u5b66\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b\u660e\u786e\u6ce2\u5170\u8bed\u6027\u522b\u5305\u5bb9\u6027\u6307\u5357\u7684\u7cfb\u7edf\u63d0\u793a\u3002", "result": "\u672c\u7814\u7a76\u7684\u65b9\u6cd5\u65e8\u5728\u5c06\u6027\u522b\u5305\u5bb9\u6027\u4f5c\u4e3aLLMs\u7684\u56fa\u6709\u7279\u6027\u8fdb\u884c\u6574\u5408\uff0c\u4ece\u800c\u63d0\u4f9b\u4e00\u79cd\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\u6765\u51cf\u8f7b\u6ce2\u5170\u8bed\u751f\u6210\u4e2d\u7684\u6027\u522b\u504f\u89c1\u3002", "conclusion": "\u901a\u8fc7IPIS\u6570\u636e\u96c6\u5fae\u8c03LLMs\u548c\u8bbe\u8ba1\u6027\u522b\u5305\u5bb9\u6027\u7cfb\u7edf\u63d0\u793a\uff0c\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u7f13\u89e3\u6ce2\u5170\u8bed\u751f\u6210\u4e2dLLMs\u7684\u6027\u522b\u504f\u89c1\uff0c\u65e8\u5728\u4f7f\u6a21\u578b\u5b9e\u73b0\u56fa\u6709\u7684\u6027\u522b\u5305\u5bb9\u6027\u3002"}}
{"id": "2508.18902", "pdf": "https://arxiv.org/pdf/2508.18902", "abs": "https://arxiv.org/abs/2508.18902", "authors": ["Daniel Lindenschmitt", "Paul Seehofer", "Marius Schmitz", "Jan Mertes", "Roland Bless", "Martina Zitterbart", "Jan C. Aurich", "Hans D. Schotten"], "title": "Adaptive 6G Networks-in-Network Management for Industrial Applications", "categories": ["cs.NI"], "comment": "2 figures", "summary": "This paper presents the application of Dynamic Spectrum Management (DSM) for\nfuture 6G industrial networks, establishing an efficient controller for the\nNetworks-in-Network (NiN) concept. The proposed architecture integrates nomadic\nas well as static sub-networks (SNs with diverse Quality of Service (QoS)\nrequirements within the coverage area of an overlayer network, managed by a\ncentralized spectrum manager (SM). Control plane connectivity between the SNs\nand the DSM is ensured by the self-organizing KIRA routing protocol. The\ndemonstrated system enables scalable, zero-touch connectivity and supports\nnomadic SNs through seamless discovery and reconfiguration. SNs are implemented\nfor modular Industrial Internet of Things (IIoT) scenarios, as well as for\nmission-critical control loops and for logistics or nomadic behavior. The DSM\nframework dynamically adapts spectrum allocation to meet real-time demands\nwhile ensuring reliable operation. The demonstration highlights the potential\nof DSM and NiNs to support flexible, dense, and heterogeneous wireless\ndeployments in reconfigurable manufacturing environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u52a8\u6001\u9891\u8c31\u7ba1\u7406\uff08DSM\uff09\u5e94\u7528\u4e8e\u672a\u67656G\u5de5\u4e1a\u7f51\u7edc\uff0c\u5e76\u4e3a\u7f51\u7edc\u5185\u7f51\u7edc\uff08NiN\uff09\u6982\u5ff5\u6784\u5efa\u9ad8\u6548\u63a7\u5236\u5668\uff0c\u4ee5\u652f\u6301\u591a\u6837\u5316\u5b50\u7f51\u7edc\u3002", "motivation": "\u4e3a\u6ee1\u8db3\u672a\u67656G\u5de5\u4e1a\u7f51\u7edc\u4e2d\u591a\u6837\u5316\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u8981\u6c42\u7684\u9759\u6001\u53ca\u6e38\u7267\u5b50\u7f51\u7edc\u7684\u8fde\u63a5\u9700\u6c42\uff0c\u5e76\u89e3\u51b3\u53ef\u91cd\u6784\u5236\u9020\u73af\u5883\u4e2d\u7075\u6d3b\u3001\u5bc6\u96c6\u548c\u5f02\u6784\u65e0\u7ebf\u90e8\u7f72\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u96c6\u6210\u9759\u6001\u548c\u6e38\u7267\u5b50\u7f51\u7edc\u7684\u67b6\u6784\uff0c\u7531\u96c6\u4e2d\u5f0f\u9891\u8c31\u7ba1\u7406\u5668\uff08SM\uff09\u7ba1\u7406\uff0c\u5e76\u901a\u8fc7\u81ea\u7ec4\u7ec7KIRA\u8def\u7531\u534f\u8bae\u786e\u4fdd\u63a7\u5236\u5e73\u9762\u8fde\u63a5\u3002DSM\u6846\u67b6\u52a8\u6001\u8c03\u6574\u9891\u8c31\u5206\u914d\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\u5e76\u4fdd\u8bc1\u53ef\u9760\u8fd0\u884c\u3002\u5b50\u7f51\u7edc\uff08SNs\uff09\u5e94\u7528\u4e8e\u5de5\u4e1a\u7269\u8054\u7f51\uff08IIoT\uff09\u3001\u4efb\u52a1\u5173\u952e\u578b\u63a7\u5236\u53ca\u7269\u6d41\u7b49\u573a\u666f\u3002", "result": "\u6240\u5c55\u793a\u7684\u7cfb\u7edf\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u96f6\u63a5\u89e6\u8fde\u63a5\uff0c\u5e76\u901a\u8fc7\u65e0\u7f1d\u53d1\u73b0\u548c\u91cd\u65b0\u914d\u7f6e\u652f\u6301\u6e38\u7267\u5b50\u7f51\u7edc\u3002DSM\u6846\u67b6\u80fd\u591f\u52a8\u6001\u9002\u5e94\u9891\u8c31\u5206\u914d\uff0c\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\u5e76\u786e\u4fdd\u53ef\u9760\u8fd0\u884c\u3002", "conclusion": "DSM\u548cNiN\u6982\u5ff5\u5728\u652f\u6301\u53ef\u91cd\u6784\u5236\u9020\u73af\u5883\u4e2d\u7075\u6d3b\u3001\u5bc6\u96c6\u548c\u5f02\u6784\u65e0\u7ebf\u90e8\u7f72\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.18520", "pdf": "https://arxiv.org/pdf/2508.18520", "abs": "https://arxiv.org/abs/2508.18520", "authors": ["Dillon Z. Chen"], "title": "Symmetry-Invariant Novelty Heuristics via Unsupervised Weisfeiler-Leman Features", "categories": ["cs.AI"], "comment": "HSDIP@ICAPS 2025 Workshop", "summary": "Novelty heuristics aid heuristic search by exploring states that exhibit\nnovel atoms. However, novelty heuristics are not symmetry invariant and hence\nmay sometimes lead to redundant exploration. In this preliminary report, we\npropose to use Weisfeiler-Leman Features for planning (WLFs) in place of atoms\nfor detecting novelty. WLFs are recently introduced features for learning\ndomain-dependent heuristics for generalised planning problems. We explore an\nunsupervised usage of WLFs for synthesising lifted, domain-independent novelty\nheuristics that are invariant to symmetric states. Experiments on the classical\nInternational Planning Competition and Hard To Ground benchmark suites yield\npromising results for novelty heuristics synthesised from WLFs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528Weisfeiler-Leman\u7279\u5f81\uff08WLFs\uff09\u4ee3\u66ff\u539f\u5b50\u6765\u68c0\u6d4b\u65b0\u9896\u6027\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b0\u9896\u6027\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u5bf9\u79f0\u6027\u65b9\u9762\u7684\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u79f0\u4e0d\u53d8\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u51fa\u826f\u597d\u7684\u524d\u666f\u3002", "motivation": "\u4f20\u7edf\u7684\u65b0\u9896\u6027\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e0d\u5177\u5907\u5bf9\u79f0\u4e0d\u53d8\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u5197\u4f59\u7684\u72b6\u6001\u63a2\u7d22\uff0c\u4ece\u800c\u964d\u4f4e\u641c\u7d22\u6548\u7387\u3002", "method": "\u7814\u7a76\u8005\u5efa\u8bae\u4f7f\u7528Weisfeiler-Leman\u7279\u5f81\uff08WLFs\uff09\u6765\u4ee3\u66ff\u539f\u5b50\u8fdb\u884c\u65b0\u9896\u6027\u68c0\u6d4b\u3002\u901a\u8fc7\u5bf9WLFs\u8fdb\u884c\u65e0\u76d1\u7763\u4f7f\u7528\uff0c\u5408\u6210\u51fa\u63d0\u5347\u7684\u3001\u9886\u57df\u72ec\u7acb\u7684\u4e14\u5bf9\u5bf9\u79f0\u72b6\u6001\u4fdd\u6301\u4e0d\u53d8\u7684\u65b0\u9896\u6027\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u5728\u7ecf\u5178\u7684\u56fd\u9645\u89c4\u5212\u7ade\u8d5b\uff08IPC\uff09\u548cHard To Ground\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7531WLFs\u5408\u6210\u7684\u65b0\u9896\u6027\u542f\u53d1\u5f0f\u65b9\u6cd5\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5c06WLFs\u5e94\u7528\u4e8e\u65b0\u9896\u6027\u68c0\u6d4b\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u521b\u5efa\u51fa\u5bf9\u79f0\u4e0d\u53d8\u7684\u65b0\u9896\u6027\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4ece\u800c\u6709\u671b\u63d0\u9ad8\u542f\u53d1\u5f0f\u641c\u7d22\u7684\u6548\u7387\u5e76\u51cf\u5c11\u5197\u4f59\u63a2\u7d22\u3002"}}
{"id": "2508.18322", "pdf": "https://arxiv.org/pdf/2508.18322", "abs": "https://arxiv.org/abs/2508.18322", "authors": ["Jiangfeng Sun", "Sihao He", "Zhonghong Ou", "Meina Song"], "title": "Structures Meet Semantics: Multimodal Fusion via Graph Contrastive Learning", "categories": ["cs.CV", "cs.AI", "68T10", "I.2.4"], "comment": "9 pages,7 figures,conference", "summary": "Multimodal sentiment analysis (MSA) aims to infer emotional states by\neffectively integrating textual, acoustic, and visual modalities. Despite\nnotable progress, existing multimodal fusion methods often neglect\nmodality-specific structural dependencies and semantic misalignment, limiting\ntheir quality, interpretability, and robustness. To address these challenges,\nwe propose a novel framework called the Structural-Semantic Unifier (SSU),\nwhich systematically integrates modality-specific structural information and\ncross-modal semantic grounding for enhanced multimodal representations.\nSpecifically, SSU dynamically constructs modality-specific graphs by leveraging\nlinguistic syntax for text and a lightweight, text-guided attention mechanism\nfor acoustic and visual modalities, thus capturing detailed intra-modal\nrelationships and semantic interactions. We further introduce a semantic\nanchor, derived from global textual semantics, that serves as a cross-modal\nalignment hub, effectively harmonizing heterogeneous semantic spaces across\nmodalities. Additionally, we develop a multiview contrastive learning objective\nthat promotes discriminability, semantic consistency, and structural coherence\nacross intra- and inter-modal views. Extensive evaluations on two widely used\nbenchmark datasets, CMU-MOSI and CMU-MOSEI, demonstrate that SSU consistently\nachieves state-of-the-art performance while significantly reducing\ncomputational overhead compared to prior methods. Comprehensive qualitative\nanalyses further validate SSU's interpretability and its ability to capture\nnuanced emotional patterns through semantically grounded interactions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7ed3\u6784-\u8bed\u4e49\u7edf\u4e00\u5668\uff08SSU\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6a21\u6001\u7279\u5b9a\u7ed3\u6784\u4fe1\u606f\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u951a\u70b9\uff0c\u5e76\u7ed3\u5408\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u7684\u6027\u80fd\u3001\u89e3\u91ca\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u5e38\u5ffd\u7565\u6a21\u6001\u7279\u5b9a\u7ed3\u6784\u4f9d\u8d56\u548c\u8bed\u4e49\u9519\u4f4d\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3001\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u7ed3\u6784-\u8bed\u4e49\u7edf\u4e00\u5668\uff08SSU\uff09\u6846\u67b6\u3002\u8be5\u6846\u67b6\u52a8\u6001\u6784\u5efa\u6a21\u6001\u7279\u5b9a\u56fe\uff08\u6587\u672c\u5229\u7528\u8bed\u8a00\u53e5\u6cd5\uff0c\u58f0\u5b66\u548c\u89c6\u89c9\u5229\u7528\u6587\u672c\u5f15\u5bfc\u6ce8\u610f\u529b\u673a\u5236\uff09\uff0c\u4ee5\u6355\u83b7\u6a21\u6001\u5185\u5173\u7cfb\u548c\u8bed\u4e49\u4ea4\u4e92\u3002\u540c\u65f6\uff0c\u5f15\u5165\u6e90\u81ea\u5168\u5c40\u6587\u672c\u8bed\u4e49\u7684\u8bed\u4e49\u951a\u70b9\u4f5c\u4e3a\u8de8\u6a21\u6001\u5bf9\u9f50\u67a2\u7ebd\u3002\u6b64\u5916\uff0c\u5f00\u53d1\u4e86\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u4ee5\u4fc3\u8fdb\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u7684\u53ef\u533a\u5206\u6027\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7ed3\u6784\u8fde\u8d2f\u6027\u3002", "result": "SSU\u5728CMU-MOSI\u548cCMU-MOSEI\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002\u5b9a\u6027\u5206\u6790\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86SSU\u7684\u89e3\u91ca\u6027\u53ca\u5176\u901a\u8fc7\u8bed\u4e49\u63a5\u5730\u7684\u4ea4\u4e92\u6355\u83b7\u7ec6\u5fae\u60c5\u611f\u6a21\u5f0f\u7684\u80fd\u529b\u3002", "conclusion": "SSU\u901a\u8fc7\u6709\u6548\u89e3\u51b3\u6a21\u6001\u7ed3\u6784\u4f9d\u8d56\u548c\u8bed\u4e49\u9519\u4f4d\u95ee\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u8868\u793a\uff0c\u5e26\u6765\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3001\u8ba1\u7b97\u6548\u7387\u548c\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.18307", "pdf": "https://arxiv.org/pdf/2508.18307", "abs": "https://arxiv.org/abs/2508.18307", "authors": ["Mahishanka Withanachchi"], "title": "Learning Spatio-Temporal Dynamics via Operator-Valued RKHS and Kernel Koopman Methods", "categories": ["cs.LG", "math.DS", "math.OC"], "comment": null, "summary": "We introduce a unified framework for learning the spatio-temporal dynamics of\nvector valued functions by combining operator valued reproducing kernel Hilbert\nspaces (OV-RKHS) with kernel based Koopman operator methods. The approach\nenables nonparametric and data driven estimation of complex time evolving\nvector fields while preserving both spatial and temporal structure. We\nestablish representer theorems for time dependent OV-RKHS interpolation, derive\nSobolev type approximation bounds for smooth vector fields, and provide\nspectral convergence guarantees for kernel Koopman operator approximations.\nThis framework supports efficient reduced order modeling and long term\nprediction of high dimensional nonlinear systems, offering theoretically\ngrounded tools for forecasting, control, and uncertainty quantification in\nspatio-temporal machine learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7ed3\u5408\u7b97\u5b50\u503c\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff08OV-RKHS\uff09\u4e0e\u57fa\u4e8e\u6838\u7684Koopman\u7b97\u5b50\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u5411\u91cf\u503c\u51fd\u6570\u7684\u65f6\u7a7a\u52a8\u529b\u5b66\u3002", "motivation": "\u65e8\u5728\u5b9e\u73b0\u590d\u6742\u65f6\u53d8\u5411\u91cf\u573a\u7684\u975e\u53c2\u6570\u548c\u6570\u636e\u9a71\u52a8\u4f30\u8ba1\uff0c\u540c\u65f6\u4fdd\u7559\u65f6\u7a7a\u7ed3\u6784\uff0c\u5e76\u4e3a\u9ad8\u7ef4\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u964d\u9636\u5efa\u6a21\u548c\u957f\u671f\u9884\u6d4b\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u5de5\u5177\u3002", "method": "\u7ed3\u5408\u4e86\u7b97\u5b50\u503c\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff08OV-RKHS\uff09\u4e0e\u57fa\u4e8e\u6838\u7684Koopman\u7b97\u5b50\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u5904\u7406\u5411\u91cf\u503c\u51fd\u6570\u7684\u65f6\u7a7a\u52a8\u529b\u5b66\u3002", "result": "\u5efa\u7acb\u4e86\u65f6\u95f4\u4f9d\u8d56OV-RKHS\u63d2\u503c\u7684\u8868\u793a\u5b9a\u7406\uff0c\u63a8\u5bfc\u4e86\u5149\u6ed1\u5411\u91cf\u573a\u7684Sobolev\u578b\u903c\u8fd1\u754c\u9650\uff0c\u5e76\u63d0\u4f9b\u4e86\u6838Koopman\u7b97\u5b50\u903c\u8fd1\u7684\u8c31\u6536\u655b\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u7ef4\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u9884\u6d4b\u3001\u63a7\u5236\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u5de5\u5177\uff0c\u652f\u6301\u9ad8\u6548\u7684\u964d\u9636\u5efa\u6a21\u548c\u957f\u671f\u9884\u6d4b\uff0c\u5728\u65f6\u7a7a\u673a\u5668\u5b66\u4e60\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.18473", "pdf": "https://arxiv.org/pdf/2508.18473", "abs": "https://arxiv.org/abs/2508.18473", "authors": ["Jiawei Li", "Akshayaa Magesh", "Venugopal V. Veeravalli"], "title": "Principled Detection of Hallucinations in Large Language Models via Multiple Testing", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages", "summary": "While Large Language Models (LLMs) have emerged as powerful foundational\nmodels to solve a variety of tasks, they have also been shown to be prone to\nhallucinations, i.e., generating responses that sound confident but are\nactually incorrect or even nonsensical. In this work, we formulate the problem\nof detecting hallucinations as a hypothesis testing problem and draw parallels\nto the problem of out-of-distribution detection in machine learning models. We\npropose a multiple-testing-inspired method to solve the hallucination detection\nproblem, and provide extensive experimental results to validate the robustness\nof our approach against state-of-the-art methods.", "AI": {"tldr": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5c06\u5e7b\u89c9\u68c0\u6d4b\u89c6\u4e3a\u5047\u8bbe\u68c0\u9a8c\u5e76\u7c7b\u6bd4\u5206\u5e03\u5916\u68c0\u6d4b\uff08OOD\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u5b83\u4eec\u5bb9\u6613\u4ea7\u751f\u201c\u5e7b\u89c9\u201d\uff0c\u5373\u751f\u6210\u542c\u8d77\u6765\u81ea\u4fe1\u4f46\u5b9e\u5219\u9519\u8bef\u6216\u8352\u8c2c\u7684\u56de\u7b54\u3002", "method": "\u5c06\u5e7b\u89c9\u68c0\u6d4b\u95ee\u9898\u516c\u5f0f\u5316\u4e3a\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\uff0c\u5e76\u5c06\u5176\u4e0e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u7684\u5206\u5e03\u5916\u68c0\u6d4b\uff08OOD\uff09\u95ee\u9898\u8fdb\u884c\u7c7b\u6bd4\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u591a\u91cd\u68c0\u9a8c\u542f\u53d1\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u3002", "result": "\u63d0\u4f9b\u4e86\u5927\u91cf\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9c81\u68d2\u7684LLMs\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5047\u8bbe\u68c0\u9a8c\u548c\u591a\u91cd\u68c0\u9a8c\u7684\u601d\u60f3\uff0c\u5728\u5bf9\u6297\u73b0\u6709\u6280\u672f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.19067", "pdf": "https://arxiv.org/pdf/2508.19067", "abs": "https://arxiv.org/abs/2508.19067", "authors": ["Aiden Valentine", "George Parisis"], "title": "LeoTCP: Low-Latency and High-Throughput Data Transport for LEO Satellite Networks", "categories": ["cs.NI"], "comment": "10 pages, 10 figures", "summary": "Low-Earth Orbit (LEO) satellite networks consist of thousands of satellites\norbiting the Earth, enabling low-latency and high-throughput communications\nacross the globe. Such networks present unprecedented challenges due to their\ndynamic nature, which state-of-the-art data transport protocols do not address.\nThese challenges include: (1) non-congestive latency variation and loss, caused\nby continuous satellite movement and fluctuating link quality due to weather\neffects; (2) transient hotspots leading to buffer build-up, latency inflation,\nand potential packet loss; and (3) frequent handovers, which may result in\ntemporary connectivity loss and re-routing through paths with unknown\ncongestion and delay characteristics. In this paper, we introduce LeoTCP, a\nnovel data transport protocol designed specifically to address these\nchallenges. LeoTCP leverages in-network telemetry (INT) to gather congestion\ninformation on a per-hop basis. Using this information, LeoTCP (1) minimises\nboth buffer occupancy and latency for end users, (2) maximises application\nthroughput and network utilisation, and (3) swiftly reacts to network hotspots.\nWe compare LeoTCP to state-of-the-art data transport protocols using a LEO\nsatellite simulation model and targeted micro-benchmarks, both based on\nOMNeT++/INET. The simulation model captures RTT dynamics in a simulated LEO\nsatellite constellation, while the micro-benchmarks isolate key LEO-specific\ncharacteristics, including non-congestive latency variation and loss, path\nchanges, and congestion hotspots. Our results demonstrate that LeoTCP\nsignificantly increases goodput compared to existing state-of-the-art\napproaches, while simultaneously minimising latency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLeoTCP\u534f\u8bae\uff0c\u5229\u7528\u7f51\u7edc\u5185\u9065\u6d4b\u6280\u672f\u89e3\u51b3LEO\u536b\u661f\u7f51\u7edc\u4e2d\u52a8\u6001\u6311\u6218\uff08\u5982\u975e\u62e5\u585e\u5ef6\u8fdf\u6ce2\u52a8\u3001\u77ac\u65f6\u70ed\u70b9\u548c\u9891\u7e41\u5207\u6362\uff09\uff0c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u7f51\u7edc\u56e0\u5176\u52a8\u6001\u7279\u6027\uff08\u5305\u62ec\u975e\u62e5\u585e\u5ef6\u8fdf\u53d8\u5316\u548c\u4e22\u5305\u3001\u77ac\u65f6\u70ed\u70b9\u4ee5\u53ca\u9891\u7e41\u5207\u6362\uff09\u9762\u4e34\u524d\u6240\u672a\u6709\u7684\u6311\u6218\uff0c\u800c\u73b0\u6709\u6570\u636e\u4f20\u8f93\u534f\u8bae\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86LeoTCP\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578b\u6570\u636e\u4f20\u8f93\u534f\u8bae\uff0c\u901a\u8fc7\u5229\u7528\u7f51\u7edc\u5185\u9065\u6d4b\uff08INT\uff09\u6280\u672f\u83b7\u53d6\u9010\u8df3\u62e5\u585e\u4fe1\u606f\u3002\u901a\u8fc7\u57fa\u4e8eOMNeT++/INET\u7684LEO\u536b\u661f\u4eff\u771f\u6a21\u578b\u548c\u5fae\u57fa\u51c6\u6d4b\u8bd5\uff08\u6a21\u62dfRTT\u52a8\u6001\u3001\u975e\u62e5\u585e\u5ef6\u8fdf\u53d8\u5316\u548c\u4e22\u5305\u3001\u8def\u5f84\u53d8\u5316\u53ca\u62e5\u585e\u70ed\u70b9\uff09\u6765\u8bc4\u4f30LeoTCP\u3002", "result": "LeoTCP\u80fd\u591f\u6700\u5c0f\u5316\u7f13\u51b2\u533a\u5360\u7528\u548c\u7ec8\u7aef\u7528\u6237\u5ef6\u8fdf\uff0c\u6700\u5927\u5316\u5e94\u7528\u541e\u5410\u91cf\u548c\u7f51\u7edc\u5229\u7528\u7387\uff0c\u5e76\u8fc5\u901f\u54cd\u5e94\u7f51\u7edc\u70ed\u70b9\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6570\u636e\u4f20\u8f93\u534f\u8bae\u76f8\u6bd4\uff0cLeoTCP\u663e\u8457\u63d0\u9ad8\u4e86\u6709\u6548\u541e\u5410\u91cf\uff0c\u5e76\u540c\u65f6\u6700\u5c0f\u5316\u4e86\u5ef6\u8fdf\u3002", "conclusion": "LeoTCP\u4f5c\u4e3a\u4e00\u79cd\u4e13\u4e3aLEO\u536b\u661f\u7f51\u7edc\u8bbe\u8ba1\u7684\u6570\u636e\u4f20\u8f93\u534f\u8bae\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8be5\u7f51\u7edc\u9762\u4e34\u7684\u52a8\u6001\u6311\u6218\uff0c\u5728\u63d0\u5347\u541e\u5410\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u534f\u8bae\u3002"}}
{"id": "2508.18527", "pdf": "https://arxiv.org/pdf/2508.18527", "abs": "https://arxiv.org/abs/2508.18527", "authors": ["Kaijie Xu", "Clark Verbrugge"], "title": "Generic Guard AI in Stealth Game with Composite Potential Fields", "categories": ["cs.AI"], "comment": null, "summary": "Guard patrol behavior is central to the immersion and strategic depth of\nstealth games, while most existing systems rely on hand-crafted routes or\nspecialized logic that struggle to balance coverage efficiency and responsive\npursuit with believable naturalness. We propose a generic, fully explainable,\ntraining-free framework that integrates global knowledge and local information\nvia Composite Potential Fields, combining three interpretable maps-Information,\nConfidence, and Connectivity-into a single kernel-filtered decision criterion.\nOur parametric, designer-driven approach requires only a handful of decay and\nweight parameters-no retraining-to smoothly adapt across both occupancy-grid\nand NavMesh-partition abstractions. We evaluate on five representative game\nmaps, two player-control policies, and five guard modes, confirming that our\nmethod outperforms classical baseline methods in both capture efficiency and\npatrol naturalness. Finally, we show how common stealth mechanics-distractions\nand environmental elements-integrate naturally into our framework as sub\nmodules, enabling rapid prototyping of rich, dynamic, and responsive guard\nbehaviors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u3001\u514d\u8bad\u7ec3\u7684\u590d\u5408\u52bf\u573a\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4fe1\u606f\u3001\u7f6e\u4fe1\u548c\u8fde\u901a\u56fe\u6765\u751f\u6210\u6f5c\u884c\u6e38\u620f\u4e2d\u9ad8\u6548\u3001\u81ea\u7136\u4e14\u54cd\u5e94\u8fc5\u901f\u7684\u5b88\u536b\u5de1\u903b\u884c\u4e3a\uff0c\u5e76\u5728\u6548\u7387\u548c\u81ea\u7136\u5ea6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6f5c\u884c\u6e38\u620f\u4e2d\u7684\u5b88\u536b\u5de1\u903b\u7cfb\u7edf\u5927\u591a\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u8def\u7ebf\u6216\u4e13\u4e1a\u903b\u8f91\uff0c\u96be\u4ee5\u5e73\u8861\u8986\u76d6\u6548\u7387\u3001\u54cd\u5e94\u5f0f\u8ffd\u51fb\u4e0e\u53ef\u4fe1\u7684\u81ea\u7136\u5ea6\uff0c\u4ece\u800c\u5f71\u54cd\u6e38\u620f\u7684\u6c89\u6d78\u611f\u548c\u7b56\u7565\u6df1\u5ea6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u3001\u5b8c\u5168\u53ef\u89e3\u91ca\u3001\u514d\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u590d\u5408\u52bf\u573a\u6574\u5408\u5168\u5c40\u77e5\u8bc6\u548c\u5c40\u90e8\u4fe1\u606f\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u4fe1\u606f\u56fe\u3001\u7f6e\u4fe1\u56fe\u548c\u8fde\u901a\u56fe\u8fd9\u4e09\u4e2a\u53ef\u89e3\u91ca\u5730\u56fe\uff0c\u5f62\u6210\u4e00\u4e2a\u5355\u4e00\u7684\u6838\u6ee4\u6ce2\u51b3\u7b56\u51c6\u5219\u3002\u5176\u53c2\u6570\u5316\u3001\u8bbe\u8ba1\u5e08\u9a71\u52a8\u7684\u7279\u6027\u53ea\u9700\u5c11\u91cf\u8870\u51cf\u548c\u6743\u91cd\u53c2\u6570\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u5373\u53ef\u5e73\u6ed1\u9002\u5e94\u7f51\u683c\u5730\u56fe\u548cNavMesh\u5206\u533a\u62bd\u8c61\u3002", "result": "\u901a\u8fc7\u5728\u4e94\u5f20\u4ee3\u8868\u6027\u6e38\u620f\u5730\u56fe\u3001\u4e24\u79cd\u73a9\u5bb6\u63a7\u5236\u7b56\u7565\u548c\u4e94\u79cd\u5b88\u536b\u6a21\u5f0f\u4e0b\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8bc1\u5b9e\u8be5\u65b9\u6cd5\u5728\u6355\u83b7\u6548\u7387\u548c\u5de1\u903b\u81ea\u7136\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u7ecf\u5178\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u4e30\u5bcc\u3001\u52a8\u6001\u4e14\u54cd\u5e94\u8fc5\u901f\u7684\u5b88\u536b\u884c\u4e3a\u3002\u540c\u65f6\uff0c\u5e38\u89c1\u7684\u6f5c\u884c\u673a\u5236\uff08\u5982\u5206\u5fc3\u548c\u73af\u5883\u5143\u7d20\uff09\u53ef\u4ee5\u81ea\u7136\u5730\u4f5c\u4e3a\u5b50\u6a21\u5757\u6574\u5408\u5230\u8be5\u6846\u67b6\u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u3002"}}
{"id": "2508.18389", "pdf": "https://arxiv.org/pdf/2508.18389", "abs": "https://arxiv.org/abs/2508.18389", "authors": ["Hao Liang", "Zhixuan Ge", "Ashish Tiwari", "Soumendu Majee", "G. M. Dilshan Godaliyadda", "Ashok Veeraraghavan", "Guha Balakrishnan"], "title": "FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses", "categories": ["cs.CV"], "comment": "11 pages, 5 figures", "summary": "We present FastAvatar, a pose-invariant, feed-forward framework that can\ngenerate a 3D Gaussian Splatting (3DGS) model from a single face image from an\narbitrary pose in near-instant time (<10ms). FastAvatar uses a novel\nencoder-decoder neural network design to achieve both fast fitting and identity\npreservation regardless of input pose. First, FastAvatar constructs a 3DGS face\n``template'' model from a training dataset of faces with multi-view captures.\nSecond, FastAvatar encodes the input face image into an identity-specific and\npose-invariant latent embedding, and decodes this embedding to predict\nresiduals to the structural and appearance parameters of each Gaussian in the\ntemplate 3DGS model. By only inferring residuals in a feed-forward fashion,\nmodel inference is fast and robust. FastAvatar significantly outperforms\nexisting feed-forward face 3DGS methods (e.g., GAGAvatar) in reconstruction\nquality, and runs 1000x faster than per-face optimization methods (e.g.,\nFlashAvatar, GaussianAvatars and GASP). In addition, FastAvatar's novel latent\nspace design supports real-time identity interpolation and attribute editing\nwhich is not possible with any existing feed-forward 3DGS face generation\nframework. FastAvatar's combination of excellent reconstruction quality and\nspeed expands the scope of 3DGS for photorealistic avatar applications in\nconsumer and interactive systems.", "AI": {"tldr": "FastAvatar\u662f\u4e00\u4e2a\u59ff\u6001\u65e0\u5173\u3001\u524d\u9988\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u5f20\u4efb\u610f\u59ff\u6001\u7684\u9762\u90e8\u56fe\u50cf\u5728\u8fd1\u4e4e\u77ac\u95f4\uff08<10ms\uff09\u751f\u6210\u9ad8\u8d28\u91cf\u76843D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u524d\u99883DGS\u4eba\u8138\u751f\u6210\u65b9\u6cd5\u5728\u91cd\u5efa\u8d28\u91cf\u3001\u901f\u5ea6\u6216\u5b9e\u65f6\u7f16\u8f91\uff08\u5982\u8eab\u4efd\u63d2\u503c\u548c\u5c5e\u6027\u7f16\u8f91\uff09\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9650\u5236\u4e86\u5176\u5728\u6d88\u8d39\u7ea7\u548c\u4ea4\u4e92\u5f0f\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "method": "FastAvatar\u91c7\u7528\u65b0\u9896\u7684\u7f16\u89e3\u7801\u5668\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u3002\u9996\u5148\uff0c\u5b83\u4ece\u591a\u89c6\u56fe\u6355\u83b7\u7684\u4eba\u8138\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u6784\u5efa\u4e00\u4e2a3DGS\u4eba\u8138\u201c\u6a21\u677f\u201d\u6a21\u578b\u3002\u7136\u540e\uff0c\u5c06\u8f93\u5165\u4eba\u8138\u56fe\u50cf\u7f16\u7801\u4e3a\u8eab\u4efd\u7279\u5b9a\u4e14\u59ff\u6001\u4e0d\u53d8\u7684\u6f5c\u5728\u5d4c\u5165\uff0c\u5e76\u5c06\u6b64\u5d4c\u5165\u89e3\u7801\u4ee5\u9884\u6d4b\u6a21\u677f3DGS\u6a21\u578b\u4e2d\u6bcf\u4e2a\u9ad8\u65af\u70b9\u7684\u7ed3\u6784\u548c\u5916\u89c2\u53c2\u6570\u7684\u6b8b\u5dee\u3002\u901a\u8fc7\u4ec5\u4ee5\u524d\u9988\u65b9\u5f0f\u63a8\u65ad\u6b8b\u5dee\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u9c81\u68d2\u7684\u6a21\u578b\u63a8\u7406\u3002", "result": "FastAvatar\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u524d\u99883DGS\u4eba\u8138\u65b9\u6cd5\uff08\u5982GAGAvatar\uff09\uff0c\u5e76\u4e14\u6bd4\u9010\u8138\u4f18\u5316\u65b9\u6cd5\uff08\u5982FlashAvatar, GaussianAvatars\u548cGASP\uff09\u5feb1000\u500d\u3002\u6b64\u5916\uff0c\u5176\u65b0\u9896\u7684\u6f5c\u5728\u7a7a\u95f4\u8bbe\u8ba1\u652f\u6301\u73b0\u6709\u524d\u99883DGS\u4eba\u8138\u751f\u6210\u6846\u67b6\u65e0\u6cd5\u5b9e\u73b0\u7684\u5b9e\u65f6\u8eab\u4efd\u63d2\u503c\u548c\u5c5e\u6027\u7f16\u8f91\u3002", "conclusion": "FastAvatar\u7ed3\u5408\u4e86\u5353\u8d8a\u7684\u91cd\u5efa\u8d28\u91cf\u548c\u901f\u5ea6\uff0c\u6781\u5927\u5730\u6269\u5c55\u4e863DGS\u5728\u6d88\u8d39\u7ea7\u548c\u4ea4\u4e92\u5f0f\u7cfb\u7edf\u4e2d\u903c\u771f\u4eba\u50cf\u5e94\u7528\u7684\u53ef\u80fd\u6027\u548c\u8303\u56f4\u3002"}}
{"id": "2508.18308", "pdf": "https://arxiv.org/pdf/2508.18308", "abs": "https://arxiv.org/abs/2508.18308", "authors": ["Avinash Amballa"], "title": "CoPE: A Lightweight Complex Positional Encoding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent studies have demonstrated the effectiveness of position encoding in\ntransformer architectures. By incorporating positional information, this\napproach provides essential guidance for modeling dependencies between elements\nacross different sequence positions. We introduce CoPE (a lightweight Complex\nPositional Encoding), a novel architecture that leverages complex-valued\nencoding to encode both content and positional information. Our approach\nreplaces traditional positional encodings with complex embeddings where the\nreal part captures semantic content and the imaginary part encodes positional\ninformation. We introduce phase-aware attention in the first layer of the\ntransformer model to capture position-dependent patterns, followed by standard\nattention layers for higher-levels. We show that CoPE doesn't exhibit long term\ndecay and is compatible with linear attention. Experimental evaluation on the\nGLUE benchmark suggest that our approach achieves superior performance with\nless computational complexity, compared to RoPE, Sinusoidal and Learned\npositional encodings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCoPE\uff08\u8f7b\u91cf\u7ea7\u590d\u6570\u4f4d\u7f6e\u7f16\u7801\uff09\uff0c\u4e00\u79cd\u5229\u7528\u590d\u6570\u5d4c\u5165\u540c\u65f6\u7f16\u7801\u5185\u5bb9\u548c\u4f4d\u7f6e\u4fe1\u606f\u7684\u65b0\u578bTransformer\u67b6\u6784\u3002\u5b83\u5f15\u5165\u4e86\u76f8\u4f4d\u611f\u77e5\u6ce8\u610f\u529b\uff0c\u5e76\u5728GLUE\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd\uff0c\u540c\u65f6\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u8868\u660e\u4f4d\u7f6e\u7f16\u7801\u5728Transformer\u67b6\u6784\u4e2d\u975e\u5e38\u6709\u6548\uff0c\u80fd\u4e3a\u5efa\u6a21\u5e8f\u5217\u5143\u7d20\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u63d0\u4f9b\u5173\u952e\u6307\u5bfc\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u6709\u6548\uff0c\u4f46\u53ef\u80fd\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u6027\u6216\u957f\u671f\u8870\u51cf\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u6709\u6548\u3001\u66f4\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165CoPE\uff08\u8f7b\u91cf\u7ea7\u590d\u6570\u4f4d\u7f6e\u7f16\u7801\uff09\uff0c\u5229\u7528\u590d\u6570\u7f16\u7801\u540c\u65f6\u6355\u83b7\u5185\u5bb9\u548c\u4f4d\u7f6e\u4fe1\u606f\u3002\u5177\u4f53\u65b9\u6cd5\u662f\uff1a\u4f7f\u7528\u590d\u6570\u5d4c\u5165\u66ff\u6362\u4f20\u7edf\u4f4d\u7f6e\u7f16\u7801\uff0c\u5176\u4e2d\u5b9e\u90e8\u8868\u793a\u8bed\u4e49\u5185\u5bb9\uff0c\u865a\u90e8\u7f16\u7801\u4f4d\u7f6e\u4fe1\u606f\u3002\u5728Transformer\u6a21\u578b\u7684\u7b2c\u4e00\u5c42\u5f15\u5165\u76f8\u4f4d\u611f\u77e5\u6ce8\u610f\u529b\u4ee5\u6355\u6349\u4f4d\u7f6e\u4f9d\u8d56\u6a21\u5f0f\uff0c\u540e\u7eed\u5c42\u4f7f\u7528\u6807\u51c6\u6ce8\u610f\u529b\u3002", "result": "CoPE\u4e0d\u8868\u73b0\u51fa\u957f\u671f\u8870\u51cf\uff0c\u5e76\u4e0e\u7ebf\u6027\u6ce8\u610f\u529b\u517c\u5bb9\u3002\u5728GLUE\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u4e0eRoPE\u3001\u6b63\u5f26\u548c\u5b66\u4e60\u578b\u4f4d\u7f6e\u7f16\u7801\u76f8\u6bd4\uff0cCoPE\u4ee5\u66f4\u4f4e\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "CoPE\u4f5c\u4e3a\u4e00\u79cd\u521b\u65b0\u7684\u590d\u6570\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff0c\u5728Transformer\u6a21\u578b\u4e2d\u901a\u8fc7\u7ed3\u5408\u5185\u5bb9\u4e0e\u4f4d\u7f6e\u4fe1\u606f\uff0c\u5e76\u5728\u7b2c\u4e00\u5c42\u5f15\u5165\u76f8\u4f4d\u611f\u77e5\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2508.18549", "pdf": "https://arxiv.org/pdf/2508.18549", "abs": "https://arxiv.org/abs/2508.18549", "authors": ["Maike Z\u00fcfle", "Vil\u00e9m Zouhar", "Tu Anh Dinh", "Felipe Maia Polo", "Jan Niehues", "Mrinmaya Sachan"], "title": "COMET-poly: Machine Translation Metric Grounded in Other Candidates", "categories": ["cs.CL", "I.2.7"], "comment": "Maike Z\\\"ufle, Vil\\'em Zouhar, and Tu Anh Dinh contributed equally", "summary": "Automated metrics for machine translation attempt to replicate human\njudgment. Unlike humans, who often assess a translation in the context of\nmultiple alternatives, these metrics typically consider only the source\nsentence and a single translation. This discrepancy in the evaluation setup may\nnegatively impact the performance of automated metrics. We propose two\nautomated metrics that incorporate additional information beyond the single\ntranslation. COMET-polycand uses alternative translations of the same source\nsentence to compare and contrast with the translation at hand, thereby\nproviding a more informed assessment of its quality. COMET-polyic, inspired by\nretrieval-based in-context learning, takes in translations of similar source\ntexts along with their human-labeled quality scores to guide the evaluation. We\nfind that including a single additional translation in COMET-polycand improves\nthe segment-level metric performance (0.079 to 0.118 Kendall's tau-b\ncorrelation), with further gains when more translations are added.\nIncorporating retrieved examples in COMET-polyic yields similar improvements\n(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u81ea\u52a8\u5316\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u6307\u6807COMET-polycand\u548cCOMET-polyic\uff0c\u901a\u8fc7\u5f15\u5165\u989d\u5916\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff08\u66ff\u4ee3\u7ffb\u8bd1\u6216\u76f8\u4f3c\u793a\u4f8b\uff09\u6765\u5f25\u8865\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u5316\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u6307\u6807\u901a\u5e38\u53ea\u8003\u8651\u6e90\u53e5\u548c\u5355\u4e2a\u7ffb\u8bd1\uff0c\u800c\u4eba\u7c7b\u8bc4\u4f30\u5219\u4f1a\u53c2\u8003\u591a\u4e2a\u5907\u9009\u8bd1\u6587\u3002\u8fd9\u79cd\u8bc4\u4f30\u8bbe\u7f6e\u4e0a\u7684\u5dee\u5f02\u53ef\u80fd\u5bfc\u81f4\u81ea\u52a8\u5316\u6307\u6807\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u81ea\u52a8\u5316\u6307\u6807\uff1a1. COMET-polycand\uff0c\u5229\u7528\u540c\u4e00\u6e90\u53e5\u7684\u5176\u4ed6\u66ff\u4ee3\u7ffb\u8bd1\u8fdb\u884c\u6bd4\u8f83\u548c\u5bf9\u6bd4\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8d28\u91cf\u8bc4\u4f30\u30022. COMET-polyic\uff0c\u53d7\u68c0\u7d22\u5f0f\u8bed\u5883\u5b66\u4e60\u542f\u53d1\uff0c\u5f15\u5165\u76f8\u4f3c\u6e90\u6587\u672c\u7684\u7ffb\u8bd1\u53ca\u5176\u4eba\u5de5\u6807\u6ce8\u8d28\u91cf\u5206\u6570\u6765\u6307\u5bfc\u8bc4\u4f30\u3002", "result": "\u5728COMET-polycand\u4e2d\uff0c\u4ec5\u589e\u52a0\u4e00\u4e2a\u989d\u5916\u7ffb\u8bd1\u5c31\u80fd\u63d0\u5347\u6bb5\u843d\u7ea7\u6307\u6807\u6027\u80fd\uff08Kendall's tau-b\u4ece0.079\u589e\u81f30.118\uff09\uff0c\u6dfb\u52a0\u66f4\u591a\u7ffb\u8bd1\u53ef\u83b7\u5f97\u8fdb\u4e00\u6b65\u63d0\u5347\u3002COMET-polyic\u4e2d\u6574\u5408\u68c0\u7d22\u5230\u7684\u793a\u4f8b\u4e5f\u53d6\u5f97\u4e86\u7c7b\u4f3c\u7684\u6539\u8fdb\uff08Kendall's tau-b\u4ece0.079\u589e\u81f30.116\uff09\u3002", "conclusion": "\u901a\u8fc7\u4e3a\u81ea\u52a8\u5316\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u6307\u6807\u63d0\u4f9b\u989d\u5916\u4e0a\u4e0b\u6587\u4fe1\u606f\uff08\u5982\u66ff\u4ee3\u7ffb\u8bd1\u6216\u5e26\u5206\u6570\u7684\u76f8\u4f3c\u793a\u4f8b\uff09\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u5176\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u76f8\u5173\u6027\u3002\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5df2\u516c\u5f00\u53d1\u5e03\u3002"}}
{"id": "2508.19130", "pdf": "https://arxiv.org/pdf/2508.19130", "abs": "https://arxiv.org/abs/2508.19130", "authors": ["Laura Finarelli", "Maoquan Ni", "Michela Meo", "Falko Dressler", "Gianluca Rizzo"], "title": "Sharing is Caring: Analysis of Hybrid Network Sharing Strategies for Energy Efficient Multi-Operator Cellular Systems", "categories": ["cs.NI"], "comment": null, "summary": "This paper introduces a novel analytical framework for evaluating\nenergy-efficient, QoS-aware network-sharing strategies in cellular networks.\nLeveraging stochastic geometry, our framework enables the systematic assessment\nof network performance across a range of sharing paradigms, including both\nconventional single-operator scenarios and advanced hybrid strategies that\nenable full integration and cooperation among multiple mobile network\noperators. Our framework incorporates diverse user densities, rate\nrequirements, and energy consumption models to ensure comprehensive analysis.\nApplying our results to real-world datasets from French mobile network\noperators, we demonstrate that hybrid network sharing can yield substantial\nenergy savings, up to $35\\%$, while maintaining quality of service.\nFurthermore, our results allow us to characterizing how the benefits of network\nsharing vary as a function of the geographical and functional characteristics\nof the deployment area. These findings highlight the potential of collaborative\nsharing strategies to enhance operational efficiency and sustainability in\nnext-generation cellular networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u8bc4\u4f30\u8702\u7a9d\u7f51\u7edc\u5171\u4eab\u7b56\u7565\u7684\u5206\u6790\u6846\u67b6\uff0c\u5229\u7528\u968f\u673a\u51e0\u4f55\u5206\u6790\u8868\u660e\uff0c\u6df7\u5408\u7f51\u7edc\u5171\u4eab\u53ef\u663e\u8457\u8282\u80fd\uff08\u9ad8\u8fbe35%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u670d\u52a1\u8d28\u91cf\uff0c\u5e76\u53ef\u6839\u636e\u90e8\u7f72\u533a\u57df\u7279\u6027\u4f18\u5316\u6548\u76ca\u3002", "motivation": "\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u8702\u7a9d\u7f51\u7edc\u4e2d\u8282\u80fd\u4e14\u517c\u987e\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u7684\u7f51\u7edc\u5171\u4eab\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u7f51\u7edc\u8fd0\u8425\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u5206\u6790\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u968f\u673a\u51e0\u4f55\u65b9\u6cd5\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5305\u62ec\u5355\u8fd0\u8425\u5546\u573a\u666f\u548c\u591a\u8fd0\u8425\u5546\u5b8c\u5168\u96c6\u6210\u4e0e\u5408\u4f5c\u7684\u6df7\u5408\u7b56\u7565\u5728\u5185\u7684\u591a\u79cd\u7f51\u7edc\u5171\u4eab\u8303\u5f0f\u4e0b\u7684\u7f51\u7edc\u6027\u80fd\u3002\u6846\u67b6\u4e2d\u6574\u5408\u4e86\u591a\u6837\u7684\u7528\u6237\u5bc6\u5ea6\u3001\u901f\u7387\u8981\u6c42\u548c\u80fd\u8017\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6cd5\u56fd\u79fb\u52a8\u7f51\u7edc\u8fd0\u8425\u5546\u7684\u771f\u5b9e\u6570\u636e\u96c6\u8fdb\u884c\u5e94\u7528\u5206\u6790\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6df7\u5408\u7f51\u7edc\u5171\u4eab\u7b56\u7565\u80fd\u591f\u5b9e\u73b0\u9ad8\u8fbe35%\u7684\u663e\u8457\u8282\u80fd\uff0c\u540c\u65f6\u6709\u6548\u7ef4\u6301\u670d\u52a1\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u91cf\u5316\u4e86\u7f51\u7edc\u5171\u4eab\u7684\u6548\u76ca\u5982\u4f55\u968f\u90e8\u7f72\u533a\u57df\u7684\u5730\u7406\u548c\u529f\u80fd\u7279\u6027\u800c\u53d8\u5316\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u534f\u4f5c\u5171\u4eab\u7b56\u7565\u5728\u63d0\u5347\u4e0b\u4e00\u4ee3\u8702\u7a9d\u7f51\u7edc\u7684\u8fd0\u8425\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.18533", "pdf": "https://arxiv.org/pdf/2508.18533", "abs": "https://arxiv.org/abs/2508.18533", "authors": ["Kaijie Xu", "Clark Verbrugge"], "title": "A Database-Driven Framework for 3D Level Generation with LLMs", "categories": ["cs.AI"], "comment": null, "summary": "Procedural Content Generation for 3D game levels faces challenges in\nbalancing spatial coherence, navigational functionality, and adaptable gameplay\nprogression across multi-floor environments. This paper introduces a novel\nframework for generating such levels, centered on the offline, LLM-assisted\nconstruction of reusable databases for architectural components (facilities and\nroom templates) and gameplay mechanic elements. Our multi-phase pipeline\nassembles levels by: (1) selecting and arranging instances from the Room\nDatabase to form a multi-floor global structure with an inherent topological\norder; (2) optimizing the internal layout of facilities for each room based on\npredefined constraints from the Facility Database; and (3) integrating\nprogression-based gameplay mechanics by placing components from a Mechanics\nDatabase according to their topological and spatial rules. A subsequent\ntwo-phase repair system ensures navigability. This approach combines modular,\ndatabase-driven design with constraint-based optimization, allowing for\nsystematic control over level structure and the adaptable pacing of gameplay\nelements. Initial experiments validate the framework's ability in generating\ndiverse, navigable 3D environments and its capability to simulate distinct\ngameplay pacing strategies through simple parameterization. This research\nadvances PCG by presenting a scalable, database-centric foundation for the\nautomated generation of complex 3D levels with configurable gameplay\nprogression.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7LLM\u8f85\u52a9\u7684\u6570\u636e\u5e93\u548c\u591a\u9636\u6bb5\u6d41\u7a0b\uff0c\u81ea\u52a8\u751f\u6210\u5177\u6709\u7a7a\u95f4\u8fde\u8d2f\u6027\u3001\u5bfc\u822a\u529f\u80fd\u548c\u53ef\u9002\u5e94\u6e38\u620f\u8fdb\u7a0b\u7684\u591a\u5c423D\u6e38\u620f\u5173\u5361\u3002", "motivation": "3D\u6e38\u620f\u5173\u5361\u7684\u7a0b\u5e8f\u5316\u5185\u5bb9\u751f\u6210\u5728\u591a\u5c42\u73af\u5883\u4e2d\u5e73\u8861\u7a7a\u95f4\u8fde\u8d2f\u6027\u3001\u5bfc\u822a\u529f\u80fd\u548c\u53ef\u9002\u5e94\u7684\u6e38\u620f\u8fdb\u7a0b\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u79bb\u7ebf\u3001LLM\u8f85\u52a9\u6784\u5efa\u53ef\u91cd\u7528\u7684\u5efa\u7b51\u7ec4\u4ef6\uff08\u8bbe\u65bd\u548c\u623f\u95f4\u6a21\u677f\uff09\u548c\u6e38\u620f\u673a\u5236\u5143\u7d20\u6570\u636e\u5e93\u3002\u5176\u591a\u9636\u6bb5\u6d41\u7a0b\u5305\u62ec\uff1a1) \u4ece\u623f\u95f4\u6570\u636e\u5e93\u9009\u62e9\u5e76\u5b89\u6392\u5b9e\u4f8b\uff0c\u5f62\u6210\u5177\u6709\u62d3\u6251\u5e8f\u7684\u591a\u5c42\u5168\u5c40\u7ed3\u6784\uff1b2) \u57fa\u4e8e\u8bbe\u65bd\u6570\u636e\u5e93\u7684\u9884\u5b9a\u4e49\u7ea6\u675f\u4f18\u5316\u6bcf\u4e2a\u623f\u95f4\u7684\u5185\u90e8\u8bbe\u65bd\u5e03\u5c40\uff1b3) \u6839\u636e\u62d3\u6251\u548c\u7a7a\u95f4\u89c4\u5219\u4ece\u673a\u5236\u6570\u636e\u5e93\u96c6\u6210\u57fa\u4e8e\u8fdb\u7a0b\u7684\u6e38\u620f\u673a\u5236\u3002\u540e\u7eed\u7684\u4e24\u9636\u6bb5\u4fee\u590d\u7cfb\u7edf\u786e\u4fdd\u5bfc\u822a\u6027\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u6a21\u5757\u5316\u3001\u6570\u636e\u5e93\u9a71\u52a8\u8bbe\u8ba1\u4e0e\u7ea6\u675f\u4f18\u5316\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u751f\u6210\u591a\u6837\u5316\u3001\u53ef\u5bfc\u822a3D\u73af\u5883\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u7b80\u5355\u7684\u53c2\u6570\u5316\u6a21\u62df\u4e0d\u540c\u6e38\u620f\u8282\u594f\u7b56\u7565\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u4ee5\u6570\u636e\u5e93\u4e3a\u4e2d\u5fc3\u7684\u590d\u67423D\u5173\u5361\u81ea\u52a8\u751f\u6210\u57fa\u7840\uff0c\u5e76\u652f\u6301\u53ef\u914d\u7f6e\u7684\u6e38\u620f\u8fdb\u7a0b\uff0c\u4ece\u800c\u63a8\u8fdb\u4e86\u7a0b\u5e8f\u5316\u5185\u5bb9\u751f\u6210\uff08PCG\uff09\u9886\u57df\u3002"}}
{"id": "2508.18415", "pdf": "https://arxiv.org/pdf/2508.18415", "abs": "https://arxiv.org/abs/2508.18415", "authors": ["Giuseppe Stragapede", "Sam Merrick", "Vedrana Krivoku\u0107a Hahn", "Justin Sukaitis", "Vincent Graf Narbel"], "title": "Securing Face and Fingerprint Templates in Humanitarian Biometric Systems", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "In humanitarian and emergency scenarios, the use of biometrics can\ndramatically improve the efficiency of operations, but it poses risks for the\ndata subjects, which are exacerbated in contexts of vulnerability. To address\nthis, we present a mobile biometric system implementing a biometric template\nprotection (BTP) scheme suitable for these scenarios. After rigorously\nformulating the functional, operational, and security and privacy requirements\nof these contexts, we perform a broad comparative analysis of the BTP\nlandscape. PolyProtect, a method designed to operate on neural network face\nembeddings, is identified as the most suitable method due to its effectiveness,\nmodularity, and lightweight computational burden. We evaluate PolyProtect in\nterms of verification and identification accuracy, irreversibility, and\nunlinkability, when this BTP method is applied to face embeddings extracted\nusing EdgeFace, a novel state-of-the-art efficient feature extractor, on a\nreal-world face dataset from a humanitarian field project in Ethiopia.\nMoreover, as PolyProtect promises to be modality-independent, we extend its\nevaluation to fingerprints. To the best of our knowledge, this is the first\ntime that PolyProtect has been evaluated for the identification scenario and\nfor fingerprint biometrics. Our experimental results are promising, and we plan\nto release our code", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u4eba\u9053\u4e3b\u4e49\u573a\u666f\u4e0b\u751f\u7269\u8bc6\u522b\u6280\u672f\u7684\u6570\u636e\u9690\u79c1\u98ce\u9669\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408PolyProtect\u548cEdgeFace\u7684\u79fb\u52a8\u751f\u7269\u8bc6\u522b\u6a21\u677f\u4fdd\u62a4\u7cfb\u7edf\uff0c\u5e76\u5728\u57c3\u585e\u4fc4\u6bd4\u4e9a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u5176\u5728\u4eba\u8138\u548c\u6307\u7eb9\u8bc6\u522b\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u4eba\u9053\u4e3b\u4e49\u548c\u7d27\u6025\u60c5\u51b5\u4e0b\uff0c\u751f\u7269\u8bc6\u522b\u6280\u672f\u80fd\u663e\u8457\u63d0\u9ad8\u64cd\u4f5c\u6548\u7387\uff0c\u4f46\u5176\u5bf9\u6570\u636e\u4e3b\u4f53\uff08\u5c24\u5176\u5728\u8106\u5f31\u73af\u5883\u4e2d\uff09\u6784\u6210\u6570\u636e\u9690\u79c1\u548c\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u6709\u6548\u7684\u4fdd\u62a4\u65b9\u6848\u3002", "method": "\u7814\u7a76\u9996\u5148\u4e25\u683c\u5236\u5b9a\u4e86\u8fd9\u4e9b\u573a\u666f\u7684\u529f\u80fd\u3001\u64cd\u4f5c\u3001\u5b89\u5168\u548c\u9690\u79c1\u8981\u6c42\uff0c\u7136\u540e\u5bf9\u73b0\u6709\u7684\u751f\u7269\u8bc6\u522b\u6a21\u677f\u4fdd\u62a4\uff08BTP\uff09\u65b9\u6848\u8fdb\u884c\u4e86\u5e7f\u6cdb\u6bd4\u8f83\u5206\u6790\u3002\u6700\u7ec8\uff0cPolyProtect\u88ab\u9009\u4e3a\u6700\u9002\u5408\u7684\u65b9\u6cd5\uff0c\u56e0\u4e3a\u5b83\u5728\u795e\u7ecf\u7f51\u7edc\u4eba\u8138\u5d4c\u5165\u4e0a\u8868\u73b0\u51fa\u9ad8\u6548\u3001\u6a21\u5757\u5316\u548c\u8f7b\u91cf\u5316\u7684\u7279\u70b9\u3002\u8be5\u7cfb\u7edf\u5c06PolyProtect\u4e0e\u5148\u8fdb\u7684\u7279\u5f81\u63d0\u53d6\u5668EdgeFace\u7ed3\u5408\uff0c\u5e76\u5728\u57c3\u585e\u4fc4\u6bd4\u4e9a\u4eba\u9053\u4e3b\u4e49\u9879\u76ee\u7684\u771f\u5b9e\u4eba\u8138\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u4e3a\u9a8c\u8bc1\u5176\u6a21\u6001\u72ec\u7acb\u6027\uff0c\u7814\u7a76\u8fd8\u5c06\u5176\u8bc4\u4f30\u6269\u5c55\u5230\u6307\u7eb9\u8bc6\u522b\u3002", "result": "\u7814\u7a76\u9996\u6b21\u5c06PolyProtect\u5e94\u7528\u4e8e\u8bc6\u522b\u573a\u666f\u548c\u6307\u7eb9\u751f\u7269\u8bc6\u522b\u3002\u8bc4\u4f30\u7ed3\u679c\u5305\u62ec\u9a8c\u8bc1\u548c\u8bc6\u522b\u51c6\u786e\u6027\u3001\u4e0d\u53ef\u9006\u6027\u4ee5\u53ca\u4e0d\u53ef\u5173\u8054\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u662f\u201c\u6709\u524d\u666f\u7684\u201d\uff08promising\uff09\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u79fb\u52a8\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\uff0c\u7ed3\u5408PolyProtect\u548cEdgeFace\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u4eba\u9053\u4e3b\u4e49\u573a\u666f\u4e0b\u7684\u751f\u7269\u8bc6\u522b\u6570\u636e\u4fdd\u62a4\u95ee\u9898\uff0c\u5e76\u5728\u4eba\u8138\u548c\u6307\u7eb9\u8bc6\u522b\u4e0a\u53d6\u5f97\u4e86\u6709\u524d\u666f\u7684\u5b9e\u9a8c\u7ed3\u679c\u3002\u7814\u7a76\u56e2\u961f\u8ba1\u5212\u53d1\u5e03\u76f8\u5173\u4ee3\u7801\u3002"}}
{"id": "2508.18312", "pdf": "https://arxiv.org/pdf/2508.18312", "abs": "https://arxiv.org/abs/2508.18312", "authors": ["Yu Pan", "Zhongze Cai", "Guanting Chen", "Huaiyang Zhong", "Chonghuan Wang"], "title": "What Matters in Data for DPO?", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Direct Preference Optimization (DPO) has emerged as a simple and effective\napproach for aligning large language models (LLMs) with human preferences,\nbypassing the need for a learned reward model. Despite its growing adoption, a\nfundamental question remains open: what characteristics of preference data are\nmost critical for DPO performance? In this work, we provide a systematic study\nof how preference data distribution influences DPO, from both theoretical and\nempirical perspectives. We show that the quality of chosen responses plays a\ndominant role in optimizing the DPO objective, while the quality of rejected\nresponses may have relatively limited impact. Our theoretical analysis\ncharacterizes the optimal response distribution under DPO and reveals how\ncontrastiveness between responses helps primarily by improving the chosen\nsamples. We further study an online DPO setting and show it effectively reduces\nto supervised fine-tuning on the chosen responses. Extensive experiments across\ndiverse tasks confirm our findings: improving the quality of chosen responses\nconsistently boosts performance regardless of the quality of the rejected\nresponses. We also investigate the benefit of mixing the on-policy data. Our\nresults interpret the mechanism behind some widely adopted strategies and offer\npractical insights for constructing high-impact preference datasets for LLM\nalignment.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86\u504f\u597d\u6570\u636e\u5206\u5e03\u5bf9\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u88ab\u9009\u62e9\u54cd\u5e94\u7684\u8d28\u91cf\u662fDPO\u4f18\u5316\u7684\u5173\u952e\u56e0\u7d20\uff0c\u800c\u88ab\u62d2\u7edd\u54cd\u5e94\u7684\u5f71\u54cd\u76f8\u5bf9\u6709\u9650\u3002", "motivation": "\u5c3d\u7ba1DPO\u5728LLM\u5bf9\u9f50\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u4ecd\u4e0d\u6e05\u695a\u504f\u597d\u6570\u636e\u7684\u54ea\u4e9b\u7279\u6027\u5bf9DPO\u6027\u80fd\u6700\u5173\u952e\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7814\u7a76\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u7cfb\u7edf\u6027\u5730\u63a2\u8ba8\u4e86\u504f\u597d\u6570\u636e\u5206\u5e03\u5bf9DPO\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u5bf9DPO\u4e0b\u6700\u4f18\u54cd\u5e94\u5206\u5e03\u7684\u7406\u8bba\u523b\u753b\u4ee5\u53ca\u5bf9\u5728\u7ebfDPO\u8bbe\u7f6e\u7684\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u88ab\u9009\u62e9\u54cd\u5e94\u7684\u8d28\u91cf\u5728\u4f18\u5316DPO\u76ee\u6807\u4e2d\u8d77\u4e3b\u5bfc\u4f5c\u7528\uff0c\u800c\u88ab\u62d2\u7edd\u54cd\u5e94\u7684\u8d28\u91cf\u5f71\u54cd\u6709\u9650\uff1b\u54cd\u5e94\u95f4\u7684\u5bf9\u6bd4\u5ea6\u4e3b\u8981\u901a\u8fc7\u6539\u5584\u88ab\u9009\u62e9\u6837\u672c\u6765\u5e2e\u52a9\uff1b\u5728\u7ebfDPO\u5b9e\u9645\u4e0a\u53ef\u7b80\u5316\u4e3a\u5bf9\u88ab\u9009\u62e9\u54cd\u5e94\u7684\u76d1\u7763\u5fae\u8c03\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8fd9\u4e9b\u53d1\u73b0\uff1a\u63d0\u9ad8\u88ab\u9009\u62e9\u54cd\u5e94\u7684\u8d28\u91cf\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u89e3\u91ca\u4e86\u4e00\u4e9bDPO\u7b56\u7565\u80cc\u540e\u7684\u673a\u5236\uff0c\u5e76\u4e3a\u6784\u5efa\u9ad8\u5f71\u54cd\u529b\u7684LLM\u5bf9\u9f50\u504f\u597d\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6307\u5bfc\u89c1\u89e3\u3002"}}
